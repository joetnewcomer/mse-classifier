,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,About Null vector orthogonal or Parallel to any other vector,About Null vector orthogonal or Parallel to any other vector,,"I have one basic doubt regarding Null vector. By definition of Dot product if $\vec{a}$ is any vector and $\vec{b}$ is Null vector then its obvious that $$\vec{a}\cdot\vec{b}=0 \tag{1}$$ that is a Null vector is Orthogonal to any vector. Similarly By definition of cross product if $\vec{a}$ is any vector and $\vec{b}$ is Null vector then its obvious that $$\vec{a} \times\vec{b}=\vec0 \tag{2}$$ that is a Null vector is parallel to any vector. But by definition of null vector, Null vector is a vector with zero magnitude and no Specific direction (Points in different directions viewing it as circle with zero radius). So practically how can $(1)$ and $(2)$ be true?","I have one basic doubt regarding Null vector. By definition of Dot product if $\vec{a}$ is any vector and $\vec{b}$ is Null vector then its obvious that $$\vec{a}\cdot\vec{b}=0 \tag{1}$$ that is a Null vector is Orthogonal to any vector. Similarly By definition of cross product if $\vec{a}$ is any vector and $\vec{b}$ is Null vector then its obvious that $$\vec{a} \times\vec{b}=\vec0 \tag{2}$$ that is a Null vector is parallel to any vector. But by definition of null vector, Null vector is a vector with zero magnitude and no Specific direction (Points in different directions viewing it as circle with zero radius). So practically how can $(1)$ and $(2)$ be true?",,"['linear-algebra', 'vectors']"
1,Why does the product of adjugates equal an adjugate of the product?,Why does the product of adjugates equal an adjugate of the product?,,"How can I show that $\mathrm{adj} (AB) = \mathrm{adj}(B)\ \mathrm{adj}(A)$? It is obvious if determinants are non-zero, but if any of the matrices are singular, I just don't get it. UPD. I've just tried to simply expand (i,j)-th element in both sides, but it seems to be (unnecessarily) complicated and I couldn't figure out the proof.","How can I show that $\mathrm{adj} (AB) = \mathrm{adj}(B)\ \mathrm{adj}(A)$? It is obvious if determinants are non-zero, but if any of the matrices are singular, I just don't get it. UPD. I've just tried to simply expand (i,j)-th element in both sides, but it seems to be (unnecessarily) complicated and I couldn't figure out the proof.",,"['linear-algebra', 'matrices', 'products']"
2,find two different generalized inverse of the given matrix,find two different generalized inverse of the given matrix,,"Definition: For a given matrix $A_{m\times n}$, a matrix $G_{n\times m}$ is said to be a generalized inverse of $A$, if it satisfies $$AGA=A.$$ Question: Find two different generalized inverse of the given matrix $$\begin{pmatrix}  1 & 0 &-1 & 2\\2 & 0 &-2 & 4 \\-1 & 1 & 1 & 3\\ -2 & 2 & 2 & 6  \end{pmatrix}$$ Work done: Since the echelon form of the matrix is, $$ \left(\begin{array}{rrrr} 1 & 0 & -1 & 2 \\ 0 & 1 & 0 & 5 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right)$$ rank is 2. since there are two distinct $2\times 2$ minors, one of the generalized inverse is,  $$\left(\begin{array}{rrrr} 0 & 0 & 0 & 0 \\ \frac 1 2  &0 & 0 & 0 \\ \frac 1 2 &  1  & 0 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right)$$ and the other one is, $$\left(\begin{array}{rrrr} 0 & 0 & 0 & 0 \\ 0 &0 & \frac 3 {10} & -\frac 4{10} \\ 0&  0  & \frac 1 {10} & \frac 2 {10} \\ 0 & 0 & 0 & 0 \end{array}\right)$$ Luckily we get two different solutions, But if the question is to find 5 different generalized inverses, How to do that? As we know there are plenty of generalized inverses are there for this given matrix, different possible ways are welcome. Thanks in advance.","Definition: For a given matrix $A_{m\times n}$, a matrix $G_{n\times m}$ is said to be a generalized inverse of $A$, if it satisfies $$AGA=A.$$ Question: Find two different generalized inverse of the given matrix $$\begin{pmatrix}  1 & 0 &-1 & 2\\2 & 0 &-2 & 4 \\-1 & 1 & 1 & 3\\ -2 & 2 & 2 & 6  \end{pmatrix}$$ Work done: Since the echelon form of the matrix is, $$ \left(\begin{array}{rrrr} 1 & 0 & -1 & 2 \\ 0 & 1 & 0 & 5 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right)$$ rank is 2. since there are two distinct $2\times 2$ minors, one of the generalized inverse is,  $$\left(\begin{array}{rrrr} 0 & 0 & 0 & 0 \\ \frac 1 2  &0 & 0 & 0 \\ \frac 1 2 &  1  & 0 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right)$$ and the other one is, $$\left(\begin{array}{rrrr} 0 & 0 & 0 & 0 \\ 0 &0 & \frac 3 {10} & -\frac 4{10} \\ 0&  0  & \frac 1 {10} & \frac 2 {10} \\ 0 & 0 & 0 & 0 \end{array}\right)$$ Luckily we get two different solutions, But if the question is to find 5 different generalized inverses, How to do that? As we know there are plenty of generalized inverses are there for this given matrix, different possible ways are welcome. Thanks in advance.",,"['linear-algebra', 'matrices', 'pseudoinverse']"
3,Finding the orthogonal projection of a vector on the null space of a linear transformation,Finding the orthogonal projection of a vector on the null space of a linear transformation,,I want to find the orthogonal projection of a vector $v \in \mathbb{R}^n$ onto the subspace of solutions to the linear equation $Ax=0$. Is there a more straightforward way than determining an orthonormal basis to the null space of $A$ and then projecting $v$ onto each basis element? Thanks,I want to find the orthogonal projection of a vector $v \in \mathbb{R}^n$ onto the subspace of solutions to the linear equation $Ax=0$. Is there a more straightforward way than determining an orthonormal basis to the null space of $A$ and then projecting $v$ onto each basis element? Thanks,,"['linear-algebra', 'matrices', 'linear-transformations']"
4,Minimal polynomial is irreducible if the only $T$-invariant subspaces of $V$ are $V$ and $0$,Minimal polynomial is irreducible if the only -invariant subspaces of  are  and,T V V 0,"Let $V$ be a finite dimensional vector space over $F$ and $T:V \rightarrow V$ linear. Suppose the only $T$-invariant subspace of $V$ are $V$ and $0$, show that the minimal polynomial $q_T$ is irreducible over $F[t]$. Attempt: Suppose not and write $q_T=p_1^{e_1}\ldots p_r^{e_r}$ with $p_i$ monic irreducible polynomials. By primary decomposition theorem, we know that $V=\ker(p_1^{e_1}(T)) \oplus \ldots \oplus \ker(p_r^{e_r}(T))$. Since $\ker(p_i^{e_i}(T))$ is $T$-invariant, by assumption, $\ker(p_j^{e_j}(T))=V$ for some $1 \leq j \leq r$. So $p_j^{e_j}=0 \Rightarrow p_j=0$, a contradiction. Is my proof correct?","Let $V$ be a finite dimensional vector space over $F$ and $T:V \rightarrow V$ linear. Suppose the only $T$-invariant subspace of $V$ are $V$ and $0$, show that the minimal polynomial $q_T$ is irreducible over $F[t]$. Attempt: Suppose not and write $q_T=p_1^{e_1}\ldots p_r^{e_r}$ with $p_i$ monic irreducible polynomials. By primary decomposition theorem, we know that $V=\ker(p_1^{e_1}(T)) \oplus \ldots \oplus \ker(p_r^{e_r}(T))$. Since $\ker(p_i^{e_i}(T))$ is $T$-invariant, by assumption, $\ker(p_j^{e_j}(T))=V$ for some $1 \leq j \leq r$. So $p_j^{e_j}=0 \Rightarrow p_j=0$, a contradiction. Is my proof correct?",,"['linear-algebra', 'proof-verification']"
5,Involutions of full matrix ring $M_n(R)$,Involutions of full matrix ring,M_n(R),"Hellow, I want to describe all involutions of full matrix ring over field and all involutions of matrix polynomial ring. Is it true or false that every involution of the full matrix ring $T = M_n(R)$ over field $R$ has the follwing form $$ A \to C^{-1}A^TC, $$ for all $A\in M_n(R)$ and some fixed matrix $C$? What can we say about involutions of the matrix-polynomial ring $T[x]$?","Hellow, I want to describe all involutions of full matrix ring over field and all involutions of matrix polynomial ring. Is it true or false that every involution of the full matrix ring $T = M_n(R)$ over field $R$ has the follwing form $$ A \to C^{-1}A^TC, $$ for all $A\in M_n(R)$ and some fixed matrix $C$? What can we say about involutions of the matrix-polynomial ring $T[x]$?",,"['linear-algebra', 'abstract-algebra']"
6,What is the easiest proof you know for the Jordan Canonical Form,What is the easiest proof you know for the Jordan Canonical Form,,"I read numerous demonstration of the existence of the Jordan Canonical Form, but all of them involve more than 2 pages of demonstration with numerous lemmas in between. I'm writing some notes for some students, but the subject is only tangentially related to Jordan Normal Form and so I was wondering if anybody knew a simple 1-page demonstration of the existence of this form! If the demonstration is original I will find a way to cite you :)","I read numerous demonstration of the existence of the Jordan Canonical Form, but all of them involve more than 2 pages of demonstration with numerous lemmas in between. I'm writing some notes for some students, but the subject is only tangentially related to Jordan Normal Form and so I was wondering if anybody knew a simple 1-page demonstration of the existence of this form! If the demonstration is original I will find a way to cite you :)",,"['linear-algebra', 'soft-question', 'jordan-normal-form']"
7,Triangularization of matrices over algebraically closed field,Triangularization of matrices over algebraically closed field,,"A friend of mine is studying physics in first semester and for his next assignment, he has to prove the following theorem: Let $V$ be a finite dimensional vector space over an algebraically closed field $K$ . Further, let $f: V \to V$ be an endomorphism. Then there exists a basis $B$ of $V$ , such that $\mbox{Mat}_{B,B}(f)$ is an upper triangular matrix. Now this theorem really stumbles me, because I know two proofs of it but they are way beyond first semester. They have only introduced elementary matrix/basis manipulation, basis change theorems and they know theorems about the existence of eigenvectors and eigenvalues ( $K$ is algebraically closed so there has to exist an eigenvector). Is there a way to prove this theorem just with the mentioned work tools? Thanks for your help!","A friend of mine is studying physics in first semester and for his next assignment, he has to prove the following theorem: Let be a finite dimensional vector space over an algebraically closed field . Further, let be an endomorphism. Then there exists a basis of , such that is an upper triangular matrix. Now this theorem really stumbles me, because I know two proofs of it but they are way beyond first semester. They have only introduced elementary matrix/basis manipulation, basis change theorems and they know theorems about the existence of eigenvectors and eigenvalues ( is algebraically closed so there has to exist an eigenvector). Is there a way to prove this theorem just with the mentioned work tools? Thanks for your help!","V K f: V \to V B V \mbox{Mat}_{B,B}(f) K","['linear-algebra', 'abstract-algebra', 'matrices', 'matrix-decomposition', 'triangularization']"
8,Is it possible to prove the Fundamental Theorem of Algebra for all polynomials of degree $n \le 4$?,Is it possible to prove the Fundamental Theorem of Algebra for all polynomials of degree ?,n \le 4,"Recenly I've been wondering whether it's possible to prove the FTA for all polynomials of degree $n \le 4$ without utilizing advanced maths but, at most, basic linear algebra (concepts such as eigenvectors, eigenvalues, determinants etc.). I've tried to give this some thought but I've been only able to make very naïve and futile statements such as ""all real numbers can be represented as complex numbers "" and ""some quadratics and quartics, such as $p(x)=x^2 + 1$ and $p(x)=x^4 -1$, contain complex solutions"". Now, I would be really greatful, if it is indeed possible to prove the FTA for all polynomials with degree $n \le 4$, if someone could provide a reference of some sort.","Recenly I've been wondering whether it's possible to prove the FTA for all polynomials of degree $n \le 4$ without utilizing advanced maths but, at most, basic linear algebra (concepts such as eigenvectors, eigenvalues, determinants etc.). I've tried to give this some thought but I've been only able to make very naïve and futile statements such as ""all real numbers can be represented as complex numbers "" and ""some quadratics and quartics, such as $p(x)=x^2 + 1$ and $p(x)=x^4 -1$, contain complex solutions"". Now, I would be really greatful, if it is indeed possible to prove the FTA for all polynomials with degree $n \le 4$, if someone could provide a reference of some sort.",,['linear-algebra']
9,General definition of angle/ rotation,General definition of angle/ rotation,,"It is well known that in the Euclidean plane a rotation about the origin can be computed with the formula $$R_{\theta}(x,y) = \big(\cos(\theta)x-\sin(\theta)y, \sin(\theta)x+\cos(\theta)y\big)$$ It is somewhat well known that in the hyperbolic (Minkowski) plane a hyperbolic rotation (Lorentz boost) about the origin can be computed with the formula $$HR_{\phi}(t,x) = \big(\cosh(\phi)t - \sinh(\phi)x, -\sinh(\phi)t+\cosh(\phi)x\big)$$ I'm well aware that given an inner product we can define the angle between two vectors $u,v$ by $\cos(\theta) = \dfrac{\langle u, v\rangle}{\|u\|\|v\|}$.  But the hyperbolic plane (thought of as a vector space) isn't an inner product space.  As far as I know the difference between the Euclidean plane and the hyperbolic plane is that they are equipped with different quadratic forms .  In the Euclidean plane that quadratic form can be used to define an inner product, but not in the hyperbolic plane as  it's not positive definite. This leads me to think that there's a generalization of the formula for angles (or rotations as one seems to be expressible in terms of the other)  for quadratic spaces .  Does anyone know of a way of extending the concept of angle/ rotation to general quadratic spaces?","It is well known that in the Euclidean plane a rotation about the origin can be computed with the formula $$R_{\theta}(x,y) = \big(\cos(\theta)x-\sin(\theta)y, \sin(\theta)x+\cos(\theta)y\big)$$ It is somewhat well known that in the hyperbolic (Minkowski) plane a hyperbolic rotation (Lorentz boost) about the origin can be computed with the formula $$HR_{\phi}(t,x) = \big(\cosh(\phi)t - \sinh(\phi)x, -\sinh(\phi)t+\cosh(\phi)x\big)$$ I'm well aware that given an inner product we can define the angle between two vectors $u,v$ by $\cos(\theta) = \dfrac{\langle u, v\rangle}{\|u\|\|v\|}$.  But the hyperbolic plane (thought of as a vector space) isn't an inner product space.  As far as I know the difference between the Euclidean plane and the hyperbolic plane is that they are equipped with different quadratic forms .  In the Euclidean plane that quadratic form can be used to define an inner product, but not in the hyperbolic plane as  it's not positive definite. This leads me to think that there's a generalization of the formula for angles (or rotations as one seems to be expressible in terms of the other)  for quadratic spaces .  Does anyone know of a way of extending the concept of angle/ rotation to general quadratic spaces?",,"['linear-algebra', 'vector-spaces', 'rotations', 'quadratic-forms']"
10,partial solving of ellipse from 5 points,partial solving of ellipse from 5 points,,"From 5 points on an ellipse I can get the ellipse characteristics (center, radii, angle) by solving a $5\times5$ system (the ellipse equation applied on each point). But this is costly when called billion times per second, plus in my case I only want the ellipse center. -> Is there a cheaper way to get the ellipse center only (either geometric, algebraic or numeric), without solving the full $5\times5$ system ? NB: for now (see end part of here ) I am using an iterative solution finding the 2 most distant points, i.e. the main axis, and taking the middle. But it is still costly, and of course inelegant. EDIT 1: if it helps, I could also provide the tangents at points. EDIT 2: note that the full ellipse equation is not a quadratic form (since not centred at (0,0)).","From 5 points on an ellipse I can get the ellipse characteristics (center, radii, angle) by solving a $5\times5$ system (the ellipse equation applied on each point). But this is costly when called billion times per second, plus in my case I only want the ellipse center. -> Is there a cheaper way to get the ellipse center only (either geometric, algebraic or numeric), without solving the full $5\times5$ system ? NB: for now (see end part of here ) I am using an iterative solution finding the 2 most distant points, i.e. the main axis, and taking the middle. But it is still costly, and of course inelegant. EDIT 1: if it helps, I could also provide the tangents at points. EDIT 2: note that the full ellipse equation is not a quadratic form (since not centred at (0,0)).",,"['linear-algebra', 'geometry', 'algebraic-geometry', 'conic-sections']"
11,Basis for the intersection of two integer lattices,Basis for the intersection of two integer lattices,,"If $B_1$ and $B_2$ are the bases of two integer lattices $L_1$ and $L_2$, i.e. $L_1=\{B_1n:n\in\mathbb Z^d\}$ and $L_2=\{B_2n:n\in\mathbb Z^d\}$, is there an easy way to determine a basis for $L_1\cap L_2$? Answers of the form ""Plug the matrices into a computer and ask for Hermite Normal Form, etc"" are perfectly acceptable as this is a practical problem and the matrices of integers $B_1$ and $B_2$ are known, but I need some algorithmic way because the procedure will be repeated many times.","If $B_1$ and $B_2$ are the bases of two integer lattices $L_1$ and $L_2$, i.e. $L_1=\{B_1n:n\in\mathbb Z^d\}$ and $L_2=\{B_2n:n\in\mathbb Z^d\}$, is there an easy way to determine a basis for $L_1\cap L_2$? Answers of the form ""Plug the matrices into a computer and ask for Hermite Normal Form, etc"" are perfectly acceptable as this is a practical problem and the matrices of integers $B_1$ and $B_2$ are known, but I need some algorithmic way because the procedure will be repeated many times.",,"['linear-algebra', 'integer-lattices']"
12,"$\text{Hom}_k(M,N)\cong M^*\otimes_k N$ as Hopf-algebra modules.",as Hopf-algebra modules.,"\text{Hom}_k(M,N)\cong M^*\otimes_k N","I'm reading Representations and Cohomology by D.J. Benson. At the beginning of the third chapter the following is explained: Let $\Lambda$ be a bialgebra over $R$ and $M,N$ left $\Lambda$-modules. We make $M\otimes_R N$ into a $\Lambda$-module as follows: If $$\Delta(\lambda)=\sum_i \mu_i\otimes \nu_i$$ then  $$\lambda(m\otimes n)=\sum_i \mu_i(m)\otimes \nu_i(n).$$ We can also make $R$ into a $\Lambda$-module via $\lambda(r)=\varepsilon(\lambda)r$. If $\Lambda$ is a Hopf-algebra over $R$ and $M,N$ are left $\Lambda$-modules, then we make $\text{Hom}_R(M,N)$ into a $\Lambda$-module as follows: If $$\Delta(\lambda)=\sum_i \mu_i\otimes \nu_i$$ and $\phi\in \text{Hom}_R(M,N)$, then  $$\lambda(\phi)(m)=\sum_i\mu_i(\phi(\eta(\nu_i)(m))),$$ where $\eta$ means the antipode of $\Lambda$. We write $M^*=\text{Hom}_R(M,R)$. Note that we are viewing $M^*$ as a left $\Lambda$-module. Because of the antipode $\eta$, we can regard right $\Lambda$-modules as left $\Lambda$-modules via $\lambda m = m\eta(\lambda)$ and vice-versa. Now, suppose that $R=k$ is a field and $\Lambda$ is a cocommutative Hopf-algebra. Suppose that $M,N$ are two left $\Lambda$-modules that are finite dimensional as $k$-vector spaces, then the natural vector space isomorphism $$\text{Hom}_k(M,N)\cong M^*\otimes_kN$$ is a $\Lambda$-module isomorphism. I don't understand why this last statement is true. I know that the natural bijection is given by  $$f:M^*\otimes_k N\rightarrow \text{Hom}_k(M,N):\phi\otimes w\mapsto \phi(\cdot)w.$$ I only have to show that this map is a $\Lambda$-module morphism. So let $\lambda\in \Lambda$ such that $\Delta(\lambda)=\sum_i\mu_i\otimes \nu_i=\sum_i\nu_i\otimes \mu_i$ (cocommutativity), and we calculate \begin{eqnarray*}  (\lambda\cdot f(\phi\otimes w))(v) &=& \sum_i \mu_i(f(\phi\otimes w)(\eta(\nu_i)(v)))\\ &=& \sum_i \mu_i(\phi(\eta(\nu_i)(v))w),\\ f(\lambda\cdot (\phi\otimes w))(v) &=& f(\sum_i (\mu_i\phi)\otimes (\nu_i w))(v)\\ &=& \sum_i (\mu_i\phi)(v)\nu_i w. \end{eqnarray*} By cocommutativity $\mu_i$ and $\nu_i$ are interchangeable in the calculation, however if I want to proceed, we need to consider $\Delta(\mu_i)$ to calculate $\mu_i\phi$ since $\mu_i\in \Lambda$ and $\phi\in \text{Hom}_k(M,k)$ (which we gave a $\Lambda$-module structure.) Now this leads to nowhere, so how do I see that this really is a $\Lambda$-module morphism? Thank you in advance!","I'm reading Representations and Cohomology by D.J. Benson. At the beginning of the third chapter the following is explained: Let $\Lambda$ be a bialgebra over $R$ and $M,N$ left $\Lambda$-modules. We make $M\otimes_R N$ into a $\Lambda$-module as follows: If $$\Delta(\lambda)=\sum_i \mu_i\otimes \nu_i$$ then  $$\lambda(m\otimes n)=\sum_i \mu_i(m)\otimes \nu_i(n).$$ We can also make $R$ into a $\Lambda$-module via $\lambda(r)=\varepsilon(\lambda)r$. If $\Lambda$ is a Hopf-algebra over $R$ and $M,N$ are left $\Lambda$-modules, then we make $\text{Hom}_R(M,N)$ into a $\Lambda$-module as follows: If $$\Delta(\lambda)=\sum_i \mu_i\otimes \nu_i$$ and $\phi\in \text{Hom}_R(M,N)$, then  $$\lambda(\phi)(m)=\sum_i\mu_i(\phi(\eta(\nu_i)(m))),$$ where $\eta$ means the antipode of $\Lambda$. We write $M^*=\text{Hom}_R(M,R)$. Note that we are viewing $M^*$ as a left $\Lambda$-module. Because of the antipode $\eta$, we can regard right $\Lambda$-modules as left $\Lambda$-modules via $\lambda m = m\eta(\lambda)$ and vice-versa. Now, suppose that $R=k$ is a field and $\Lambda$ is a cocommutative Hopf-algebra. Suppose that $M,N$ are two left $\Lambda$-modules that are finite dimensional as $k$-vector spaces, then the natural vector space isomorphism $$\text{Hom}_k(M,N)\cong M^*\otimes_kN$$ is a $\Lambda$-module isomorphism. I don't understand why this last statement is true. I know that the natural bijection is given by  $$f:M^*\otimes_k N\rightarrow \text{Hom}_k(M,N):\phi\otimes w\mapsto \phi(\cdot)w.$$ I only have to show that this map is a $\Lambda$-module morphism. So let $\lambda\in \Lambda$ such that $\Delta(\lambda)=\sum_i\mu_i\otimes \nu_i=\sum_i\nu_i\otimes \mu_i$ (cocommutativity), and we calculate \begin{eqnarray*}  (\lambda\cdot f(\phi\otimes w))(v) &=& \sum_i \mu_i(f(\phi\otimes w)(\eta(\nu_i)(v)))\\ &=& \sum_i \mu_i(\phi(\eta(\nu_i)(v))w),\\ f(\lambda\cdot (\phi\otimes w))(v) &=& f(\sum_i (\mu_i\phi)\otimes (\nu_i w))(v)\\ &=& \sum_i (\mu_i\phi)(v)\nu_i w. \end{eqnarray*} By cocommutativity $\mu_i$ and $\nu_i$ are interchangeable in the calculation, however if I want to proceed, we need to consider $\Delta(\mu_i)$ to calculate $\mu_i\phi$ since $\mu_i\in \Lambda$ and $\phi\in \text{Hom}_k(M,k)$ (which we gave a $\Lambda$-module structure.) Now this leads to nowhere, so how do I see that this really is a $\Lambda$-module morphism? Thank you in advance!",,"['linear-algebra', 'proof-verification', 'modules', 'representation-theory', 'hopf-algebras']"
13,Prove that a polytope is closed,Prove that a polytope is closed,,"Let the polytope defined by $$S:=co \left\{ x_1,x_2,...,x_k \right\}$$ where $x_1,x_2,...,x_k \in \mathbb{R^n}$ and $co \left \{... \right \}$ is the convex Hull. Prove that S i closed. I tried the following. I want to show that $S=cl(S)$ I've proved that for all set $S \subseteq cl(S)$. Now I want to prove that $cl(S)\subseteq S$. I know that $cl(S) = int(S) \cup bd(S)$ So, taking $x \in cl(S)$. If $x \in int(S)$ then its clear that $x \in S$. If $x \in bd(S)$ is not clear but intuitively the border is the convex combination between $x_i$ and $x_j$ (in pairs) and $i,j=1,2,...,k$. I don't know how to rite it formally and how to prove that my intuition about the border is actually $bd(S)$","Let the polytope defined by $$S:=co \left\{ x_1,x_2,...,x_k \right\}$$ where $x_1,x_2,...,x_k \in \mathbb{R^n}$ and $co \left \{... \right \}$ is the convex Hull. Prove that S i closed. I tried the following. I want to show that $S=cl(S)$ I've proved that for all set $S \subseteq cl(S)$. Now I want to prove that $cl(S)\subseteq S$. I know that $cl(S) = int(S) \cup bd(S)$ So, taking $x \in cl(S)$. If $x \in int(S)$ then its clear that $x \in S$. If $x \in bd(S)$ is not clear but intuitively the border is the convex combination between $x_i$ and $x_j$ (in pairs) and $i,j=1,2,...,k$. I don't know how to rite it formally and how to prove that my intuition about the border is actually $bd(S)$",,"['linear-algebra', 'convex-analysis', 'convex-optimization']"
14,Why do complex eigenvalues/eigenvectors cause rotation?,Why do complex eigenvalues/eigenvectors cause rotation?,,"I am trying to understand the intuition behind eigenvalues/eigenvectors through the lens of repeated matrix multiplication: Given a $2\times2$ matrix $M$ and $2D$ vector $v$, multiplying $v$ repeatedly  with $M$ causes the result ($M^n v$) to gravitate towards one of the eigenspaces of $M$ because: $$M^n v = M^n(\alpha x_1 + \beta x_2) = (\alpha \lambda_1^n x_1 + \beta \lambda_2^n x_2)$$ where $x_1$ and $x_2$ are eigenvectors of $M$ and $\lambda_1$ and $\lambda_2$ the corresponding eigenvalues. As $n$ gets larger $M^n v$ will gravitate towards either $\alpha \lambda_1^n x_1$ or $\beta \lambda_2^n x_2$, whichever has the dominant eigenvalue. assuming: $v = \alpha x_1 + \beta x_2$ So the above is a way to connect the abstract concept of eigenvalue/eigenvector to something concrete: what happens when you apply a matrix over and over to a vector. However, the intuition breaks down for me with complex eigenvectors. I know repeated multiplication by a matrix with complex eigenvectors causes the result to either spiral outwards or inwards. Is there simple math such as above to see why? Edit: I know similar questions have been asked before, but I ask in the context of repeated matrix multiplication","I am trying to understand the intuition behind eigenvalues/eigenvectors through the lens of repeated matrix multiplication: Given a $2\times2$ matrix $M$ and $2D$ vector $v$, multiplying $v$ repeatedly  with $M$ causes the result ($M^n v$) to gravitate towards one of the eigenspaces of $M$ because: $$M^n v = M^n(\alpha x_1 + \beta x_2) = (\alpha \lambda_1^n x_1 + \beta \lambda_2^n x_2)$$ where $x_1$ and $x_2$ are eigenvectors of $M$ and $\lambda_1$ and $\lambda_2$ the corresponding eigenvalues. As $n$ gets larger $M^n v$ will gravitate towards either $\alpha \lambda_1^n x_1$ or $\beta \lambda_2^n x_2$, whichever has the dominant eigenvalue. assuming: $v = \alpha x_1 + \beta x_2$ So the above is a way to connect the abstract concept of eigenvalue/eigenvector to something concrete: what happens when you apply a matrix over and over to a vector. However, the intuition breaks down for me with complex eigenvectors. I know repeated multiplication by a matrix with complex eigenvectors causes the result to either spiral outwards or inwards. Is there simple math such as above to see why? Edit: I know similar questions have been asked before, but I ask in the context of repeated matrix multiplication",,"['linear-algebra', 'eigenvalues-eigenvectors']"
15,Is it a legal operation to invert both sides of a linear algebra equation?,Is it a legal operation to invert both sides of a linear algebra equation?,,"I'm studying Stang's linear algebra book. One of the questions is: $$AB = C, \text{ find a formula for } A^{-1}$$ I solved this as: $$(AB)^{-1} = C^{-1}$$ $$B^{-1}A^{-1} = C^{-1}$$ $$A^{-1} = BC^{-1}$$ This is correct according to answer at the back of the book, however I couldn't find a reference to say taking the inverse of both sides is allowable, or if instead I had to do it with some method of multiplication.","I'm studying Stang's linear algebra book. One of the questions is: $$AB = C, \text{ find a formula for } A^{-1}$$ I solved this as: $$(AB)^{-1} = C^{-1}$$ $$B^{-1}A^{-1} = C^{-1}$$ $$A^{-1} = BC^{-1}$$ This is correct according to answer at the back of the book, however I couldn't find a reference to say taking the inverse of both sides is allowable, or if instead I had to do it with some method of multiplication.",,"['linear-algebra', 'matrices']"
16,For which orthogonal matrices does the matrix exponential converge?,For which orthogonal matrices does the matrix exponential converge?,,"Part (a) For which 2×2 orthogonal matrices A does $\large e^A=I+\frac{A^1}{1!}+\frac{A^2}{2!}+…$ converge? Part(b) For what A does the series converge to an orthogonal matrix? My work: Let A be 2x2 and orthogonal.  Then $A^tA = AA^t = I$ and so this implies that A is normal.  Over the ground field = $C$, A is then orthogonally / unitarily diagonalizable. We can write $A=QDQ^*$, where Q is unitary and D is diagonal with the eigenvalues of A on the diagonal.  Also, since A is assumed to be orthogonal, then the modulus of each eigenvalue is 1. Now $$e^A=I+\frac{A^1}{1!}+\frac{A^2}{2!}+…$$ $$ e^A=I+\frac{QDQ^*}{1!}+\frac{QD^2Q^*}{2!}+…$$ $$ e^A= Q(I+\frac{D}{1!}+\frac{D^2}{2!}+…)Q^*$$ $$ e^A= Qe^DQ^*$$ Where $e^D$ is again diagonal. What can I say from here?  I know that online sources such as Wikipedia and Wolfram just state without any proof or extended discussions that the matrix exponential is well-defined and converges for any square matrix.  If this is stated as a fact without proof, then it seems a little strange that I am working on a problem statement that asks ""for which orthogonal 2x2 matrices A does $e^A$ converge"".  Is there an important point that I am overlooking?  Or can I really just state that the matrix exponential converges for any square matrix A, hence it is well-defined and converges for any 2x2 orthogonal matrix A? Any suggestions and hints for how to finish part (a) and how to start on part (b) are welcome. Thanks,","Part (a) For which 2×2 orthogonal matrices A does $\large e^A=I+\frac{A^1}{1!}+\frac{A^2}{2!}+…$ converge? Part(b) For what A does the series converge to an orthogonal matrix? My work: Let A be 2x2 and orthogonal.  Then $A^tA = AA^t = I$ and so this implies that A is normal.  Over the ground field = $C$, A is then orthogonally / unitarily diagonalizable. We can write $A=QDQ^*$, where Q is unitary and D is diagonal with the eigenvalues of A on the diagonal.  Also, since A is assumed to be orthogonal, then the modulus of each eigenvalue is 1. Now $$e^A=I+\frac{A^1}{1!}+\frac{A^2}{2!}+…$$ $$ e^A=I+\frac{QDQ^*}{1!}+\frac{QD^2Q^*}{2!}+…$$ $$ e^A= Q(I+\frac{D}{1!}+\frac{D^2}{2!}+…)Q^*$$ $$ e^A= Qe^DQ^*$$ Where $e^D$ is again diagonal. What can I say from here?  I know that online sources such as Wikipedia and Wolfram just state without any proof or extended discussions that the matrix exponential is well-defined and converges for any square matrix.  If this is stated as a fact without proof, then it seems a little strange that I am working on a problem statement that asks ""for which orthogonal 2x2 matrices A does $e^A$ converge"".  Is there an important point that I am overlooking?  Or can I really just state that the matrix exponential converges for any square matrix A, hence it is well-defined and converges for any 2x2 orthogonal matrix A? Any suggestions and hints for how to finish part (a) and how to start on part (b) are welcome. Thanks,",,"['linear-algebra', 'matrices', 'exponential-function', 'exponentiation', 'matrix-equations']"
17,Understanding part of the proof of Spectral Theorem for symmetric matrices,Understanding part of the proof of Spectral Theorem for symmetric matrices,,"I'm reading a textbook where the Spectral Theorem for symmetric matrices is proven. I understand almost everything about the proof except for one thing. The theorem is stated as follows: Theorem: Let $A \in \mathbb{R}^{n \times n}$. Then $A$ is orthogonally diagonalizable if and only if $A$ is symmetric. The first implication is easy. The converse is proven by induction by the author. Here is part of the proof: We want to prove that for any symmetric matrix $A$, there is an   orthogonal matrix $P$ and a diagonal matrix $D$ such that $P^T AP = D$. We prove this by induction. Any $1 \times 1$ symmetric matrix is   already diagonal, so we can take $P = I$ and the basic step is proven. Now assume the theorem holds for $(n -1) \times (n-1)$ symmetric   matrices, with $n \geq 2$. Then we now prove it also holds for $n$. So   let $A$ be an $ n \times n$ symmetric matrix. We know that $A$ has   only real eigenvalues (he concludes this on the basis of a preceding   theorem). Let $\lambda_1$ be any eigenvalue of $A$, and let $v_1$ be   the corresponding eigenvector which satisfies $||v_1 || = 1 $. Then we   can extend the set $\left\{v_1 \right\}$ to a basis $\left\{ v_1, x_1, x_2, \ldots, x_n \right\}$ of $\mathbb{R}^n$. We can then use the   Gram-Schmidt process to transform into an orthonormal basis $B = \left\{v_1, v_2, \ldots, v_n \right\}$ of $\mathbb{R}^n$. Let $P$ be the matrix whose columns are the vectors in $B$, with the   first column being $v_1$. Then $P$ is orthogonal because its column   vectors are all orthonormal. Now $P^T A P = P^{-1} AP$ represents the   linear transformation $T: x \mapsto Ax $ in the basis $B$. But we know   that the first column of $P^T AP$ will be the coordinate vector of   $T(v_1)$ with respect to the basis $B$. Now, $T(v_1) = Av_1 = \lambda_1 v_1$, so this coordinate vector is \begin{align*} \begin{pmatrix} \lambda_1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}. \end{align*} It follows that... He then shows $P^T A P$ is diagonal by making use of induction hypothesis on a smaller block matrix. But here is what I don't understand. He says $P^T A P$ represents the linear transformation $T: x \mapsto Ax$. What does he mean here? Does he mean the linear transformation $L_A : \mathbb{R}^n \rightarrow \mathbb{R}^n$ ? This doesn't seem right to me, since the matrixrepresentation of $L_A$ is just $A$. Also, what he says after that doesn't really make sense to me, i.e. that the first column $P^T A P$ is the coordinate vector $T(v_1)$ with respect to $B$. Maybe someone can clarify this, or provide an example?","I'm reading a textbook where the Spectral Theorem for symmetric matrices is proven. I understand almost everything about the proof except for one thing. The theorem is stated as follows: Theorem: Let $A \in \mathbb{R}^{n \times n}$. Then $A$ is orthogonally diagonalizable if and only if $A$ is symmetric. The first implication is easy. The converse is proven by induction by the author. Here is part of the proof: We want to prove that for any symmetric matrix $A$, there is an   orthogonal matrix $P$ and a diagonal matrix $D$ such that $P^T AP = D$. We prove this by induction. Any $1 \times 1$ symmetric matrix is   already diagonal, so we can take $P = I$ and the basic step is proven. Now assume the theorem holds for $(n -1) \times (n-1)$ symmetric   matrices, with $n \geq 2$. Then we now prove it also holds for $n$. So   let $A$ be an $ n \times n$ symmetric matrix. We know that $A$ has   only real eigenvalues (he concludes this on the basis of a preceding   theorem). Let $\lambda_1$ be any eigenvalue of $A$, and let $v_1$ be   the corresponding eigenvector which satisfies $||v_1 || = 1 $. Then we   can extend the set $\left\{v_1 \right\}$ to a basis $\left\{ v_1, x_1, x_2, \ldots, x_n \right\}$ of $\mathbb{R}^n$. We can then use the   Gram-Schmidt process to transform into an orthonormal basis $B = \left\{v_1, v_2, \ldots, v_n \right\}$ of $\mathbb{R}^n$. Let $P$ be the matrix whose columns are the vectors in $B$, with the   first column being $v_1$. Then $P$ is orthogonal because its column   vectors are all orthonormal. Now $P^T A P = P^{-1} AP$ represents the   linear transformation $T: x \mapsto Ax $ in the basis $B$. But we know   that the first column of $P^T AP$ will be the coordinate vector of   $T(v_1)$ with respect to the basis $B$. Now, $T(v_1) = Av_1 = \lambda_1 v_1$, so this coordinate vector is \begin{align*} \begin{pmatrix} \lambda_1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}. \end{align*} It follows that... He then shows $P^T A P$ is diagonal by making use of induction hypothesis on a smaller block matrix. But here is what I don't understand. He says $P^T A P$ represents the linear transformation $T: x \mapsto Ax$. What does he mean here? Does he mean the linear transformation $L_A : \mathbb{R}^n \rightarrow \mathbb{R}^n$ ? This doesn't seem right to me, since the matrixrepresentation of $L_A$ is just $A$. Also, what he says after that doesn't really make sense to me, i.e. that the first column $P^T A P$ is the coordinate vector $T(v_1)$ with respect to $B$. Maybe someone can clarify this, or provide an example?",,"['linear-algebra', 'diagonalization', 'orthogonality']"
18,Uniqueness of Unitary Similarity Transform,Uniqueness of Unitary Similarity Transform,,"Let's say I have a Hermitian matrix $H$ that is diagonalized by a unitary matrix $U$: $D = U^{\dagger} H U$, where $D$ is diagonal. How unique is $U$? If I stick on an overall phase factor by $U \rightarrow e^{i\phi}U$, then $U$ still diagonalizes $H$. Do I have more options than that, or is $U$ uniquely defined up to a phase?","Let's say I have a Hermitian matrix $H$ that is diagonalized by a unitary matrix $U$: $D = U^{\dagger} H U$, where $D$ is diagonal. How unique is $U$? If I stick on an overall phase factor by $U \rightarrow e^{i\phi}U$, then $U$ still diagonalizes $H$. Do I have more options than that, or is $U$ uniquely defined up to a phase?",,['linear-algebra']
19,Linear Algebra: System of Equations,Linear Algebra: System of Equations,,"Consider a finite sequence $x_i \in (0,1)$ for $i=1,\ldots, n$ and define $y_i=\dfrac{\Pi_{j=1}^n x_j }{x_i}$. I solved this system for $x$ in terms of $y$ and got $$x_i=\dfrac{\left(\Pi_{j=1}^n y_j \right)^\frac{1}{n-1}}{y_i}.$$ Now pick some $m<n$. Is there a simple way of solving $(x_1,\ldots x_m, y_{m+1},\ldots, y_n)$ in terms of $(y_1,\ldots y_m, x_{m+1},\ldots, x_n)$? If we take logs, this is a linear system. My only idea on how to solve is to write the system for $n=2,3$ and solve it by brute-force to see if some pattern emerges and if so, make a conjecture and prove it... But perhaps someone can easily see some clever trick I am missing. This is not homework. It is for some lemma I need in my paper.","Consider a finite sequence $x_i \in (0,1)$ for $i=1,\ldots, n$ and define $y_i=\dfrac{\Pi_{j=1}^n x_j }{x_i}$. I solved this system for $x$ in terms of $y$ and got $$x_i=\dfrac{\left(\Pi_{j=1}^n y_j \right)^\frac{1}{n-1}}{y_i}.$$ Now pick some $m<n$. Is there a simple way of solving $(x_1,\ldots x_m, y_{m+1},\ldots, y_n)$ in terms of $(y_1,\ldots y_m, x_{m+1},\ldots, x_n)$? If we take logs, this is a linear system. My only idea on how to solve is to write the system for $n=2,3$ and solve it by brute-force to see if some pattern emerges and if so, make a conjecture and prove it... But perhaps someone can easily see some clever trick I am missing. This is not homework. It is for some lemma I need in my paper.",,['linear-algebra']
20,Vector Space: is there a general definition of annihilator beyond the dual space?,Vector Space: is there a general definition of annihilator beyond the dual space?,,"The definitions I see for the annihilator of a subset S of a vector space V over a field F is the subset $S^0$ of the dual space $V^*$given by $S^0 = \{\varphi\in V^\star:\varphi(S)=0\}$, and the annihilator can be seen to be a subspace. My question is whether this is ever extended to a more general situation ? The dual space corresponds to $Hom(V, F)$ the set of linear transformations from V to F. So if instead of $Hom(V, F)$ one considers $Hom(V, W)$, the set of linear transformations from V to a space W (over the same field) it seems there should be a corresponding subspace of $Hom(V, W)$ which maps the subset S of V to the zero vector in W. I'd appreciate any feedback on whether this is in fact the case and the terminology that applies.","The definitions I see for the annihilator of a subset S of a vector space V over a field F is the subset $S^0$ of the dual space $V^*$given by $S^0 = \{\varphi\in V^\star:\varphi(S)=0\}$, and the annihilator can be seen to be a subspace. My question is whether this is ever extended to a more general situation ? The dual space corresponds to $Hom(V, F)$ the set of linear transformations from V to F. So if instead of $Hom(V, F)$ one considers $Hom(V, W)$, the set of linear transformations from V to a space W (over the same field) it seems there should be a corresponding subspace of $Hom(V, W)$ which maps the subset S of V to the zero vector in W. I'd appreciate any feedback on whether this is in fact the case and the terminology that applies.",,"['linear-algebra', 'vector-spaces', 'definition']"
21,Diagonalization of a symmetric matrix over algebraically closed field,Diagonalization of a symmetric matrix over algebraically closed field,,Let $k$ be an algebraically closed field. Let $A$ be an $n \times n$ symmetric matrix with entries in $k$. Does it then follow that there exist eigenvectors of $A$ which form an orthonormal basis of $k^n$? Thanks!,Let $k$ be an algebraically closed field. Let $A$ be an $n \times n$ symmetric matrix with entries in $k$. Does it then follow that there exist eigenvectors of $A$ which form an orthonormal basis of $k^n$? Thanks!,,"['linear-algebra', 'quadratic-forms', 'bilinear-form']"
22,How to prove the both identity (matrix),How to prove the both identity (matrix),,"I read a paper, and the paper use the following identities (that hold true in any ring) $(I+AB)^{-1}A = A(I+BA)^{-1}$ $(I+AB)^{-1} = I - A(I+BA)^{-1}B$ Any way to prove this? How to open the term $(I+AB)^{-1}$ ?","I read a paper, and the paper use the following identities (that hold true in any ring) $(I+AB)^{-1}A = A(I+BA)^{-1}$ $(I+AB)^{-1} = I - A(I+BA)^{-1}B$ Any way to prove this? How to open the term $(I+AB)^{-1}$ ?",,"['linear-algebra', 'matrices', 'control-theory']"
23,Why is it easy to calculate $\operatorname{rank}(A)=n$?,Why is it easy to calculate ?,\operatorname{rank}(A)=n,"when I read a paper with matrices methods, and found a difficult problem. Define matrix $A=(a_{jk})_{n\times n}$,where    $$a_{jk}=\begin{cases} j+k\cdot i&j<k\\ k+j\cdot i&j>k\\ 2(j+k\cdot i)& j=k \end{cases}$$   where $i^2=-1$. The author say it is easy to calculate that $\operatorname{rank}(A)=n$. I have found that for $n\le 5$ it is true, but for general $n$, I can't prove it. $$A=P+iQ$$ $$P=\begin{bmatrix} 2&1&1&\cdots&1\\ 1&4&2&\cdots& 2\\ 1&2&6&\cdots& 3\\ \cdots&\cdots&\cdots&\cdots&\cdots\\ 1&2&3&\cdots& 2n \end{bmatrix},Q=\begin{bmatrix}  2&2&3&\cdots& n\\ 2&4&3&\cdots &n\\ 3&3&6&\cdots& n\\ \cdots&\cdots&\cdots&\cdots&\cdots\\ n&n&n&\cdots& 2n\end{bmatrix}$$ define $$J=\begin{bmatrix} 1&0&\cdots &0\\ -1&1&\cdots& 0\\ \cdots&\cdots&\cdots&\cdots\\ 0&\cdots&-1&1 \end{bmatrix}$$ then we have $$JPJ^T=J^TQJ=\begin{bmatrix} 2&-2&\cdots&0\\ -2&4&-3&\cdots\\ \cdots&\cdots&\cdots&\cdots\\ 0&\cdots&-(n-1)&2n \end{bmatrix}$$ and $$\begin{align*}A^HA&=(P-iQ)(P+iQ)=P^2+Q^2+i(PQ-QP)=\\&=\binom{p}{Q}^T\cdot\begin{bmatrix} I& iI\\ -iI & I  \end{bmatrix} \binom{P}{Q}\end{align*}$$","when I read a paper with matrices methods, and found a difficult problem. Define matrix $A=(a_{jk})_{n\times n}$,where    $$a_{jk}=\begin{cases} j+k\cdot i&j<k\\ k+j\cdot i&j>k\\ 2(j+k\cdot i)& j=k \end{cases}$$   where $i^2=-1$. The author say it is easy to calculate that $\operatorname{rank}(A)=n$. I have found that for $n\le 5$ it is true, but for general $n$, I can't prove it. $$A=P+iQ$$ $$P=\begin{bmatrix} 2&1&1&\cdots&1\\ 1&4&2&\cdots& 2\\ 1&2&6&\cdots& 3\\ \cdots&\cdots&\cdots&\cdots&\cdots\\ 1&2&3&\cdots& 2n \end{bmatrix},Q=\begin{bmatrix}  2&2&3&\cdots& n\\ 2&4&3&\cdots &n\\ 3&3&6&\cdots& n\\ \cdots&\cdots&\cdots&\cdots&\cdots\\ n&n&n&\cdots& 2n\end{bmatrix}$$ define $$J=\begin{bmatrix} 1&0&\cdots &0\\ -1&1&\cdots& 0\\ \cdots&\cdots&\cdots&\cdots\\ 0&\cdots&-1&1 \end{bmatrix}$$ then we have $$JPJ^T=J^TQJ=\begin{bmatrix} 2&-2&\cdots&0\\ -2&4&-3&\cdots\\ \cdots&\cdots&\cdots&\cdots\\ 0&\cdots&-(n-1)&2n \end{bmatrix}$$ and $$\begin{align*}A^HA&=(P-iQ)(P+iQ)=P^2+Q^2+i(PQ-QP)=\\&=\binom{p}{Q}^T\cdot\begin{bmatrix} I& iI\\ -iI & I  \end{bmatrix} \binom{P}{Q}\end{align*}$$",,"['linear-algebra', 'matrices']"
24,"Proving if $A$ is an $n\times n$ positive semi-definite matrix, A is Hermitian with non-negative eigenvalues.","Proving if  is an  positive semi-definite matrix, A is Hermitian with non-negative eigenvalues.",A n\times n,"I have a test on Monday and the professor hinted that this question might be relevant to the exam, unfortunately, I'm at a loss. As the title states, I would like to prove that if $A$ is an $n\times n$ positive semi-definite matrix then $A$ is Hermitian with non-negative eigenvalues. I would love to be able to work through this and I hope someone can lend a hand. Thank you very much.","I have a test on Monday and the professor hinted that this question might be relevant to the exam, unfortunately, I'm at a loss. As the title states, I would like to prove that if $A$ is an $n\times n$ positive semi-definite matrix then $A$ is Hermitian with non-negative eigenvalues. I would love to be able to work through this and I hope someone can lend a hand. Thank you very much.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
25,Every skew-symmetric matrix has a non-negative determinant,Every skew-symmetric matrix has a non-negative determinant,,"Let $A$ be a skew-symmetric $n\times n$-matrix over the real numbers. Show that $\det A$ is nonnegative. I'm breaking this up into the even case and odd case (if $A$ is an $n\times n$ skew-symmetric matrix). So when $n$ is odd, we have: $\det(A)=\det(A^T)=\det(-A)=(-1)^n\det(A)\Rightarrow \det(A)=-\det(A)\Rightarrow \det(A) = 0$ So $\det(A)$ is non-negative when $n$ is odd. When $n$ is even, we have: $\det(A)=\det(A^T)=\det(-A)=(-1)^n\det(A)\Rightarrow \det(A)=\det(A)$ But why can't $\det(A)$ be negative in this case?","Let $A$ be a skew-symmetric $n\times n$-matrix over the real numbers. Show that $\det A$ is nonnegative. I'm breaking this up into the even case and odd case (if $A$ is an $n\times n$ skew-symmetric matrix). So when $n$ is odd, we have: $\det(A)=\det(A^T)=\det(-A)=(-1)^n\det(A)\Rightarrow \det(A)=-\det(A)\Rightarrow \det(A) = 0$ So $\det(A)$ is non-negative when $n$ is odd. When $n$ is even, we have: $\det(A)=\det(A^T)=\det(-A)=(-1)^n\det(A)\Rightarrow \det(A)=\det(A)$ But why can't $\det(A)$ be negative in this case?",,"['linear-algebra', 'matrices', 'determinant', 'skew-symmetric-matrices']"
26,Most elementary proof that a determinant is divisible by $m$,Most elementary proof that a determinant is divisible by,m,"So a challenge problem states that you have an $n \times n$ matrix, where each entry is an integer between $0$ and $9$, and when each row is read as a base-10 number the number is divisible by a common factor $m$. The problem is to prove that the determinant of the matrix is also divisible by $m$. So far I have two proofs: If $m$ is a power of a prime, then we can consider the field ${\mathbb F}_m$ and the vector $(10^{n-1}, 10^{n-2}, \ldots, 1)$ is a non-zero vector in the kernel of the matrix over ${\mathbb F}_m$ so the determinant is $0$ over the field by general linear algebra. Or, for arbitrary $m$, we can add $10^{n-1}$ times the first column and $10^{n-2}$ times the second column and so forth and add it all to the last column without changing the determinant, and then we get the the last column is all numbers divisible by $m$ so the determinant is divisible by $m$ by expanding the determinant along the last column. However are there any more elementary proofs, say that just use the basic full expansion formula of the determinant, without using so many properties of the determinant?","So a challenge problem states that you have an $n \times n$ matrix, where each entry is an integer between $0$ and $9$, and when each row is read as a base-10 number the number is divisible by a common factor $m$. The problem is to prove that the determinant of the matrix is also divisible by $m$. So far I have two proofs: If $m$ is a power of a prime, then we can consider the field ${\mathbb F}_m$ and the vector $(10^{n-1}, 10^{n-2}, \ldots, 1)$ is a non-zero vector in the kernel of the matrix over ${\mathbb F}_m$ so the determinant is $0$ over the field by general linear algebra. Or, for arbitrary $m$, we can add $10^{n-1}$ times the first column and $10^{n-2}$ times the second column and so forth and add it all to the last column without changing the determinant, and then we get the the last column is all numbers divisible by $m$ so the determinant is divisible by $m$ by expanding the determinant along the last column. However are there any more elementary proofs, say that just use the basic full expansion formula of the determinant, without using so many properties of the determinant?",,"['linear-algebra', 'abstract-algebra', 'number-theory', 'determinant']"
27,"3-sigma Ellipse, why axis length scales with square root of eigenvalues of covariance-matrix","3-sigma Ellipse, why axis length scales with square root of eigenvalues of covariance-matrix",,"This is my first post on math.stackexchange and i am not a mathematician, but i took some undergrad math courses and some grad mathematical modelling courses, so i come with a basic understanding of math and computed a lot of eigenvectors and eigenvalues already... I hope my notation can be understood easily. I am trying to describe a two dimensional scatter plot by 3 parameter, which define the shape of an ellipse (i.e. length of major-axis, length of minor-axis and the angle between the major-axis and the ordinate of a cartesian coordinate system. The reason for this is, the scatter-plot changes over time and i want to track that change by tracking the change of those 3 parameters, i.e. the shape of the ellipse. I do not need a confidence-region or anything like this, just the parameters of an ellipse, fitting to my data. All i have given (and need) is the x- and y-coordinates of the scatter plot. I understand that the eigenvectors of the covariance-matrix give me orientation of the major- and minor-axis, which gives me the angle between major-axis and ordinate (just as in a principal component analysis). What I do not understand is, how do I get the length of the major- and minor-axis from the covariance matrix. I found two different approaches: Similar post on math.stackexchange linking to this source code on Wikipedia . They do a Cholesky-Decomposition of the Covariance matrix and transform a unit-circle with it. I cannot follow this mathematically, nor do i know why this would be correct. Similar post on stackoverflow They define the width and the height (i.e. length of the major- and minor-axis) as 2 * nstd * np.sqrt(vals), where nstd is denotes the n-th standard deviation which. In my case this would be 3. I can follow the second approach better, used the python code and the result looks great! But i do not understand why the length of the axis scales with the square root of the eigenvalues. I think that there must be a simple relation between the eigenvalue of a covariance matrix and the standard deviation of the projected data to the ellipse-axis, which i just cannot find in the literature and which i cannot derive myself. Can anyone enlighten me on this or help me to find something to read, such i can understand this better. Best, Nras. Update: I got the Textbook ""Circular Statistics in Biology"" from Batchelet, 1981. On pp.268-274 they calculate the semi-axis and the inclination angle for an example with $\sigma_1=3$ and $\sigma_2=2$ and with the correlation coefficient $\rho=0.4$. This yields the covariance matrix $$\begin{align*}   Cov & =\left[ {\begin{array}{cc}    \sigma_1^2 & \rho\sigma_1\sigma_2\\    \rho\sigma_1\sigma_2 & \sigma_2^2\\   \end{array} } \right] \\ &= \left[ {\begin{array}{cc}    9.0 & 2.4\\    2.4 & 4.0\\   \end{array} } \right] \end{align*}$$ and go through the equations 14.4.8 to 14.4.11 and find the results $$a=3.157, b=1.742, \Theta=21.9^\circ$$ The result just magically appears, as i cannot find the referenced equations, they stop after 14.4.7, right before this example and the chapter finishes after the example. What a pity! Running the Code in python, where the semi-axis length is defined by the square root of the eigenvalue, i obtain the same result: import numpy cov = numpy.array([[9., 2.4], [2.4, 4.]]) eigenvals, eigenvecs = numpy.linalg.eig(cov) numpy.sqrt(eigenvals) numpy.rad2deg(numpy.arccos(eigenvecs[0, 0])) The output is as follows: array([ 3.1568251,  1.7419688]) 21.915430336046288 So there must be something that explains why the ellipse semi-axis length scales with the square root of the eigenvalue. I now know that it is true, but still do not know why. Update2: The referenced equations really do not exist, but the calculation is sketched in the previous chapter 13.8. Summary: Let the Distributions 1 and 2 be centered at $\mu_1$ and $\mu_2$, respectively and and let  $$ A=\sigma_2^2,\quad B=-\rho\sigma_1\sigma_2,\quad C=\sigma_1^2,\quad D=(1-\rho^2)\sigma_1^2\sigma_2^2$$ then the Ellipse follows the equation $$ A(x-\mu_1)^2 + 2B(x-\mu_1)(y-\mu_2) + C(y-\mu_2)^2 = D$$ From that equation the semi-axis length and inclination of the major-axis is computed as follows: Let $$R=[(A-C)^2 + 4B^2]^{1/2}$$ then the semi-major-axis $a$ and the semi-minor-axis $b$ can be computed as $$b, a = \left(\dfrac{2D}{A+C\pm R}\right)^{1/2}$$ and for the inclination $$\Theta = \arctan\left(\dfrac{2B}{A-C-R}\right)$$ Substituting in the values from the example yields the exact same solution as the approach with the square root of the eigenvalues and the inclination which can be obtained from the arccos of the eigenvector. From here it cannot be fare to show that $a=\sqrt{\lambda_1}$ and $b=\sqrt{\lambda_2}$ if $\lambda_i$ are the eigenvalues of the covariance matrix.","This is my first post on math.stackexchange and i am not a mathematician, but i took some undergrad math courses and some grad mathematical modelling courses, so i come with a basic understanding of math and computed a lot of eigenvectors and eigenvalues already... I hope my notation can be understood easily. I am trying to describe a two dimensional scatter plot by 3 parameter, which define the shape of an ellipse (i.e. length of major-axis, length of minor-axis and the angle between the major-axis and the ordinate of a cartesian coordinate system. The reason for this is, the scatter-plot changes over time and i want to track that change by tracking the change of those 3 parameters, i.e. the shape of the ellipse. I do not need a confidence-region or anything like this, just the parameters of an ellipse, fitting to my data. All i have given (and need) is the x- and y-coordinates of the scatter plot. I understand that the eigenvectors of the covariance-matrix give me orientation of the major- and minor-axis, which gives me the angle between major-axis and ordinate (just as in a principal component analysis). What I do not understand is, how do I get the length of the major- and minor-axis from the covariance matrix. I found two different approaches: Similar post on math.stackexchange linking to this source code on Wikipedia . They do a Cholesky-Decomposition of the Covariance matrix and transform a unit-circle with it. I cannot follow this mathematically, nor do i know why this would be correct. Similar post on stackoverflow They define the width and the height (i.e. length of the major- and minor-axis) as 2 * nstd * np.sqrt(vals), where nstd is denotes the n-th standard deviation which. In my case this would be 3. I can follow the second approach better, used the python code and the result looks great! But i do not understand why the length of the axis scales with the square root of the eigenvalues. I think that there must be a simple relation between the eigenvalue of a covariance matrix and the standard deviation of the projected data to the ellipse-axis, which i just cannot find in the literature and which i cannot derive myself. Can anyone enlighten me on this or help me to find something to read, such i can understand this better. Best, Nras. Update: I got the Textbook ""Circular Statistics in Biology"" from Batchelet, 1981. On pp.268-274 they calculate the semi-axis and the inclination angle for an example with $\sigma_1=3$ and $\sigma_2=2$ and with the correlation coefficient $\rho=0.4$. This yields the covariance matrix $$\begin{align*}   Cov & =\left[ {\begin{array}{cc}    \sigma_1^2 & \rho\sigma_1\sigma_2\\    \rho\sigma_1\sigma_2 & \sigma_2^2\\   \end{array} } \right] \\ &= \left[ {\begin{array}{cc}    9.0 & 2.4\\    2.4 & 4.0\\   \end{array} } \right] \end{align*}$$ and go through the equations 14.4.8 to 14.4.11 and find the results $$a=3.157, b=1.742, \Theta=21.9^\circ$$ The result just magically appears, as i cannot find the referenced equations, they stop after 14.4.7, right before this example and the chapter finishes after the example. What a pity! Running the Code in python, where the semi-axis length is defined by the square root of the eigenvalue, i obtain the same result: import numpy cov = numpy.array([[9., 2.4], [2.4, 4.]]) eigenvals, eigenvecs = numpy.linalg.eig(cov) numpy.sqrt(eigenvals) numpy.rad2deg(numpy.arccos(eigenvecs[0, 0])) The output is as follows: array([ 3.1568251,  1.7419688]) 21.915430336046288 So there must be something that explains why the ellipse semi-axis length scales with the square root of the eigenvalue. I now know that it is true, but still do not know why. Update2: The referenced equations really do not exist, but the calculation is sketched in the previous chapter 13.8. Summary: Let the Distributions 1 and 2 be centered at $\mu_1$ and $\mu_2$, respectively and and let  $$ A=\sigma_2^2,\quad B=-\rho\sigma_1\sigma_2,\quad C=\sigma_1^2,\quad D=(1-\rho^2)\sigma_1^2\sigma_2^2$$ then the Ellipse follows the equation $$ A(x-\mu_1)^2 + 2B(x-\mu_1)(y-\mu_2) + C(y-\mu_2)^2 = D$$ From that equation the semi-axis length and inclination of the major-axis is computed as follows: Let $$R=[(A-C)^2 + 4B^2]^{1/2}$$ then the semi-major-axis $a$ and the semi-minor-axis $b$ can be computed as $$b, a = \left(\dfrac{2D}{A+C\pm R}\right)^{1/2}$$ and for the inclination $$\Theta = \arctan\left(\dfrac{2B}{A-C-R}\right)$$ Substituting in the values from the example yields the exact same solution as the approach with the square root of the eigenvalues and the inclination which can be obtained from the arccos of the eigenvector. From here it cannot be fare to show that $a=\sqrt{\lambda_1}$ and $b=\sqrt{\lambda_2}$ if $\lambda_i$ are the eigenvalues of the covariance matrix.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'covariance']"
28,Proving result on spectral radius,Proving result on spectral radius,,"How do I prove that $$\rho(A)=\inf\limits_{\text{operator norms}}\|A\|,$$ $\rho$ being the spectral radius, $A$ being a complex $n\times n$ matrix and operator norms being induced from vector norms by defining $\|A\|=\min\limits_{\|x\|=1}\frac{\|Ax\|}{\|x\|}$? I have proved that $\rho(A)\leq\|A\|$ for any induced norm $\|\cdot\|$, so $\leq$ holds in the above equality. How do I prove $\geq$?","How do I prove that $$\rho(A)=\inf\limits_{\text{operator norms}}\|A\|,$$ $\rho$ being the spectral radius, $A$ being a complex $n\times n$ matrix and operator norms being induced from vector norms by defining $\|A\|=\min\limits_{\|x\|=1}\frac{\|Ax\|}{\|x\|}$? I have proved that $\rho(A)\leq\|A\|$ for any induced norm $\|\cdot\|$, so $\leq$ holds in the above equality. How do I prove $\geq$?",,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms', 'spectral-radius']"
29,How can I prove that the span of a subspace and its orthogonal complement is the whole vector space?,How can I prove that the span of a subspace and its orthogonal complement is the whole vector space?,,"The book Linear and Geometric Algebra explains the following theorem in a way that I haven't been able to figure out: If $\mathbf{A}$ and $\mathbf{B}$ are subspaces of a vector space $\mathbf{B}$ then the set of all combinations $\mathbf{a} + \mathbf{b}$ such that, $\mathbf{a} \in \mathbf{A}$ and $\mathbf{b} \in \mathbf{B}$ is called the span of $\mathbf{A}$ and $\mathbf{B}$, written as $\text{span}(\mathbf{A}, \mathbf{B})$. Furthermore let $\mathbf{U}^{\bot}$ be the subspace consisting of all vectors orthogonal to a subspace $\mathbf{U}$, in the sense that $\mathbf{u} \in \mathbf{U}^{\top}$ if and only if, $\mathbf{u} \perp \mathbf{v}$ for all vectors $\mathbf{v} \in \mathbf{U}$. I have of course been able to prove that $\mathbf{U}^{\bot}$ is indeed a subspace for all subspaces $\mathbf{U}$, if this turns out to be useful. The theorem I want to prove is: if $\mathbf{U}$ is a subspace of $\mathbf{V}$ then  $\text{span}(\mathbf{U}, \mathbf{U}^{\perp}) = \mathbf{V}$. The book mentioned above proves it as follows: If $\text{span}(\mathbf{U}, \mathbf{U}^{\perp}) \neq \mathbf{V}$ then there is a nonzero $\mathbf{u} \perp \text{span}(\mathbf{U}, \mathbf{U}^{\perp})$ because any orthonormal basis for a subspace of an inner product space can be extended into an orthonormal basis for the entire inner product space. In particular $\mathbf{u} \perp \mathbf{U}$, i.e. $\mathbf{u} \in \mathbf{U}^{\perp}$, a contradiction. I understand how an orthonormal basis for a subspace of an inner product space can be extended into an orthonormal basis for the whole inner product space essentially using Gram-Schmidt orthogonalisation. I don't understand how this process allows you to go from, $\text{span}(\mathbf{U}, \mathbf{U}^{\perp}) \neq \mathbf{V}$ to $\exists \mathbf{u} \in \mathbf{U} : \mathbf{u} \perp \text{span}(\mathbf{U}, \mathbf{U}^{\perp})$. So my question would be how does this implication work?","The book Linear and Geometric Algebra explains the following theorem in a way that I haven't been able to figure out: If $\mathbf{A}$ and $\mathbf{B}$ are subspaces of a vector space $\mathbf{B}$ then the set of all combinations $\mathbf{a} + \mathbf{b}$ such that, $\mathbf{a} \in \mathbf{A}$ and $\mathbf{b} \in \mathbf{B}$ is called the span of $\mathbf{A}$ and $\mathbf{B}$, written as $\text{span}(\mathbf{A}, \mathbf{B})$. Furthermore let $\mathbf{U}^{\bot}$ be the subspace consisting of all vectors orthogonal to a subspace $\mathbf{U}$, in the sense that $\mathbf{u} \in \mathbf{U}^{\top}$ if and only if, $\mathbf{u} \perp \mathbf{v}$ for all vectors $\mathbf{v} \in \mathbf{U}$. I have of course been able to prove that $\mathbf{U}^{\bot}$ is indeed a subspace for all subspaces $\mathbf{U}$, if this turns out to be useful. The theorem I want to prove is: if $\mathbf{U}$ is a subspace of $\mathbf{V}$ then  $\text{span}(\mathbf{U}, \mathbf{U}^{\perp}) = \mathbf{V}$. The book mentioned above proves it as follows: If $\text{span}(\mathbf{U}, \mathbf{U}^{\perp}) \neq \mathbf{V}$ then there is a nonzero $\mathbf{u} \perp \text{span}(\mathbf{U}, \mathbf{U}^{\perp})$ because any orthonormal basis for a subspace of an inner product space can be extended into an orthonormal basis for the entire inner product space. In particular $\mathbf{u} \perp \mathbf{U}$, i.e. $\mathbf{u} \in \mathbf{U}^{\perp}$, a contradiction. I understand how an orthonormal basis for a subspace of an inner product space can be extended into an orthonormal basis for the whole inner product space essentially using Gram-Schmidt orthogonalisation. I don't understand how this process allows you to go from, $\text{span}(\mathbf{U}, \mathbf{U}^{\perp}) \neq \mathbf{V}$ to $\exists \mathbf{u} \in \mathbf{U} : \mathbf{u} \perp \text{span}(\mathbf{U}, \mathbf{U}^{\perp})$. So my question would be how does this implication work?",,"['linear-algebra', 'inner-products', 'orthonormal', 'orthogonality']"
30,How to find adjoint of linear operator T on inner product space V,How to find adjoint of linear operator T on inner product space V,,"Let $V$ be an inner product space and $T$ a linear operator with $T(\alpha) = (\alpha,\beta)\gamma$ for fixed elements $\beta,\gamma \in V$. I now that $T$ is linear operator. How we can show that adjoint of $T$ ($T^*$) exist and what is it?","Let $V$ be an inner product space and $T$ a linear operator with $T(\alpha) = (\alpha,\beta)\gamma$ for fixed elements $\beta,\gamma \in V$. I now that $T$ is linear operator. How we can show that adjoint of $T$ ($T^*$) exist and what is it?",,['linear-algebra']
31,Does an isotropic vector always exist for an indefinite quadratic forms?,Does an isotropic vector always exist for an indefinite quadratic forms?,,"I have some problem reading some paper. In that paper, the author proved that a quadratic form $Q$ has an isotropic vector(of course, nonzero) by showing $Q$ has both nonnegative and nonpositive values. Does an isotropic vector always exist for an indefinite quadratic forms? If true, why?","I have some problem reading some paper. In that paper, the author proved that a quadratic form $Q$ has an isotropic vector(of course, nonzero) by showing $Q$ has both nonnegative and nonpositive values. Does an isotropic vector always exist for an indefinite quadratic forms? If true, why?",,"['linear-algebra', 'quadratic-forms']"
32,"Solve the System of Equations in Real $x$,$y$ and $z$","Solve the System of Equations in Real , and",x y z,"Solve for $x$,$y$ and $z$ $\in $ $\mathbb{R}$ if $$\begin{align} x^2+x-1=y \\                    y^2+y-1=z\\ z^2+z-1=x \end{align}$$ My Try: if  $x=y=z$ then the two triplets $(1,1,1)$ and $(-1,-1,-1)$ are the Solutions. if $x \ne y \ne z$ Then we have $$\begin{align} x(x+1)=y+1 \\                    y(y+1)=z+1\\ z(z+1)=x+1 \end{align}$$ Multiplying all we get  $$xyz=1 \tag{1}$$ and adding all we get $$x^2+y^2+z^2=3 \tag{2}$$ Now from Original Equations $$\begin{align} x^2=y+1-x\\                 y^2=z+1-y\\                 z^2=x+1-z \end{align}$$ Multiplying all and Using $(1)$ we get $$(y+1-x)(z+1-y)(x+1-z)=1 $$    $\implies$ $$xy+yz+zx-3=(x-y)(y-z)(z-x) \tag{3}$$ I am unable to proceed further..","Solve for $x$,$y$ and $z$ $\in $ $\mathbb{R}$ if $$\begin{align} x^2+x-1=y \\                    y^2+y-1=z\\ z^2+z-1=x \end{align}$$ My Try: if  $x=y=z$ then the two triplets $(1,1,1)$ and $(-1,-1,-1)$ are the Solutions. if $x \ne y \ne z$ Then we have $$\begin{align} x(x+1)=y+1 \\                    y(y+1)=z+1\\ z(z+1)=x+1 \end{align}$$ Multiplying all we get  $$xyz=1 \tag{1}$$ and adding all we get $$x^2+y^2+z^2=3 \tag{2}$$ Now from Original Equations $$\begin{align} x^2=y+1-x\\                 y^2=z+1-y\\                 z^2=x+1-z \end{align}$$ Multiplying all and Using $(1)$ we get $$(y+1-x)(z+1-y)(x+1-z)=1 $$    $\implies$ $$xy+yz+zx-3=(x-y)(y-z)(z-x) \tag{3}$$ I am unable to proceed further..",,"['linear-algebra', 'systems-of-equations', 'real-numbers']"
33,"When is $R \, A^{-1} \, R^t$ invertible?",When is  invertible?,"R \, A^{-1} \, R^t","In the context of a Gaussian model, I came across a matrix product $R \, A^{-1} \, R^t$ where $R$ is a $m \times n$ rectangular matrix and as implied $A$ is $n \times n$ and invertible. On which properties of $R$ does the existence of $(R \, A^{-1} \, R^t)^{-1}$ depend?","In the context of a Gaussian model, I came across a matrix product $R \, A^{-1} \, R^t$ where $R$ is a $m \times n$ rectangular matrix and as implied $A$ is $n \times n$ and invertible. On which properties of $R$ does the existence of $(R \, A^{-1} \, R^t)^{-1}$ depend?",,['linear-algebra']
34,Singular Values/l2-norm of Pseudo-inverse,Singular Values/l2-norm of Pseudo-inverse,,"I am trying to prove, given a matrix $A=\lbrack\frac{A_1}{A_2}\rbrack\in C^{m\times n}$, with $A_1\in C^{n\times n}$ non-singular, that: $||A^+||_2\leq||A_1^{-1}||_2$ ($||\cdot||_2$ is the induced $\ell_2$ norm, $(\cdot)^+$ is the Moore-Penrose pseudoinverse.) Supposed to be simple but I'm having trouble relating $A$'s singular values to $A_1$'s. Any ideas?","I am trying to prove, given a matrix $A=\lbrack\frac{A_1}{A_2}\rbrack\in C^{m\times n}$, with $A_1\in C^{n\times n}$ non-singular, that: $||A^+||_2\leq||A_1^{-1}||_2$ ($||\cdot||_2$ is the induced $\ell_2$ norm, $(\cdot)^+$ is the Moore-Penrose pseudoinverse.) Supposed to be simple but I'm having trouble relating $A$'s singular values to $A_1$'s. Any ideas?",,['linear-algebra']
35,Proof that eigenvector corresponding to simple eigenvalue is continuous,Proof that eigenvector corresponding to simple eigenvalue is continuous,,"Let $\lambda$ be a simple eigenvalue of $A \in L(C^n)$ and let $x$ be the corresponding eigenvector. Then for $E \in L(C^n)$,  $A+E$ has an eigenvalue $\lambda(E)$ and an eigenvector $x(E)$ such that as $E\longrightarrow 0$, $\lambda(E) \longrightarrow \lambda$ and $x(E) \longrightarrow x$. This is 3.1.3 (Chapter 3, page 46) from Numerical Analysis: a second course by J. Ortega. I am using the book  for self study. The continuity of the eigenvalue is clear, but I do not understand the proof for the eigenvector bit. Could someone help me understand what is going on ? I looked at Continuity of a simple eigenvalue and its corresponding eigenvector but that does not answer my question. I would like to have a proof that does not invoke implicit/inverse function theorem.","Let $\lambda$ be a simple eigenvalue of $A \in L(C^n)$ and let $x$ be the corresponding eigenvector. Then for $E \in L(C^n)$,  $A+E$ has an eigenvalue $\lambda(E)$ and an eigenvector $x(E)$ such that as $E\longrightarrow 0$, $\lambda(E) \longrightarrow \lambda$ and $x(E) \longrightarrow x$. This is 3.1.3 (Chapter 3, page 46) from Numerical Analysis: a second course by J. Ortega. I am using the book  for self study. The continuity of the eigenvalue is clear, but I do not understand the proof for the eigenvector bit. Could someone help me understand what is going on ? I looked at Continuity of a simple eigenvalue and its corresponding eigenvector but that does not answer my question. I would like to have a proof that does not invoke implicit/inverse function theorem.",,"['linear-algebra', 'continuity', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
36,Axiomatization of angle measuring in real vector spaces,Axiomatization of angle measuring in real vector spaces,,"In linear algebra / analytic geometry it is common to define the angle between two vectors $u,v \in V$ of an euclidean vector space $V$ by $\angle (u,v) := \arccos \frac{(u,v)}{\|u\| \cdot \|v\|}$. Is there a general theory analogous to measure theory that abstracts from this concrete mapping by allowing an angle measure to be any function which satisfies the most important properties of the above one? I am thinking of a definition like this: Let $V$ be a real vector space. A mapping $\angle : V \times V \to \mathbb{R}$ is called an angle measure, if it satisfies the following axioms for all $u,v \in V$ and $\lambda, \mu \in \mathbb{R}^+$: $\angle(u,v) = \angle (v,u)$ $\angle(\lambda u, \mu v) = \angle(u,v)$ $\angle(u,w) + \angle(w,v) = \angle(u,v)$ where $w = \lambda u + \mu v$ $\angle(u,v) \leq \angle(u,w) + \angle(w,v)$ for all $w$ ... Is there a finite set of such axioms that characterizes the usual angle measures coming from inner products (in finite dimensional spaces)? Are there interesting examples of angle measures coming not from inner products?","In linear algebra / analytic geometry it is common to define the angle between two vectors $u,v \in V$ of an euclidean vector space $V$ by $\angle (u,v) := \arccos \frac{(u,v)}{\|u\| \cdot \|v\|}$. Is there a general theory analogous to measure theory that abstracts from this concrete mapping by allowing an angle measure to be any function which satisfies the most important properties of the above one? I am thinking of a definition like this: Let $V$ be a real vector space. A mapping $\angle : V \times V \to \mathbb{R}$ is called an angle measure, if it satisfies the following axioms for all $u,v \in V$ and $\lambda, \mu \in \mathbb{R}^+$: $\angle(u,v) = \angle (v,u)$ $\angle(\lambda u, \mu v) = \angle(u,v)$ $\angle(u,w) + \angle(w,v) = \angle(u,v)$ where $w = \lambda u + \mu v$ $\angle(u,v) \leq \angle(u,w) + \angle(w,v)$ for all $w$ ... Is there a finite set of such axioms that characterizes the usual angle measures coming from inner products (in finite dimensional spaces)? Are there interesting examples of angle measures coming not from inner products?",,"['linear-algebra', 'geometry', 'vector-spaces']"
37,Average Line of a Set of Lines,Average Line of a Set of Lines,,"Suppose we have 10 lines in an x-y plane.  The lines are somewhat clustered together, and going more or less in the same direction. The data I have for these lines is their line equation: $$y = a + bx$$ I'm wondering how one can come up with an ""average line"" for the set. Does it make sense to take the average of all the $a$ values (the y-intercept) and take the average of all the $b$ values (the slope) and use those two together to arrive at an average line equation? Thoughts and comments appreciated.","Suppose we have 10 lines in an x-y plane.  The lines are somewhat clustered together, and going more or less in the same direction. The data I have for these lines is their line equation: $$y = a + bx$$ I'm wondering how one can come up with an ""average line"" for the set. Does it make sense to take the average of all the $a$ values (the y-intercept) and take the average of all the $b$ values (the slope) and use those two together to arrive at an average line equation? Thoughts and comments appreciated.",,['linear-algebra']
38,Why is this formula positive definite?,Why is this formula positive definite?,,"I have a formula $$A(I+GQ)^{-1}(G+GQG)(I+QG)^{-1}A^{\mathrm T}+G$$ where $A,Q,G,I\in\mathbb R^{n\times n}$, $A$ nonsingular, $G$ positive semi-definite, $Q$ positive definite, $I$ the identity matrix, and $(A,G)$ controllable. It is obvious that both the first and second terms are positive semi-definite, but why this formula is positive definite in fact? Thanks in advance.","I have a formula $$A(I+GQ)^{-1}(G+GQG)(I+QG)^{-1}A^{\mathrm T}+G$$ where $A,Q,G,I\in\mathbb R^{n\times n}$, $A$ nonsingular, $G$ positive semi-definite, $Q$ positive definite, $I$ the identity matrix, and $(A,G)$ controllable. It is obvious that both the first and second terms are positive semi-definite, but why this formula is positive definite in fact? Thanks in advance.",,['linear-algebra']
39,Lower bound on the smallest eigenvalue,Lower bound on the smallest eigenvalue,,"Recently I encountered a lower bound on the minimum eigenvalue of positive Hermitian matrices in ( Lower bound on smallest eigenvalue of (symmetric positive-definite) matrix ). The lower bound is stated as: $$ \lambda_{min} \gt \sqrt{\frac{||A||_F^2-n||A||_E^2}{n(1-||A||_E^2/|det(A)|^{2/n})}} $$ My question is if this bound exists in the first place, and if it does, is it only for real matrices or does it include complex ones too. Also, what is the difference between the Frobenius (F) and the Euclidean norm (E) here? The referenced page is not clear either. I would really appreciate your help.  Thank you.","Recently I encountered a lower bound on the minimum eigenvalue of positive Hermitian matrices in ( Lower bound on smallest eigenvalue of (symmetric positive-definite) matrix ). The lower bound is stated as: $$ \lambda_{min} \gt \sqrt{\frac{||A||_F^2-n||A||_E^2}{n(1-||A||_E^2/|det(A)|^{2/n})}} $$ My question is if this bound exists in the first place, and if it does, is it only for real matrices or does it include complex ones too. Also, what is the difference between the Frobenius (F) and the Euclidean norm (E) here? The referenced page is not clear either. I would really appreciate your help.  Thank you.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
40,Adjacent basic solutions and adjacent bases,Adjacent basic solutions and adjacent bases,,"I'm reading chapter 2, entitled geometry of linear programming, of the following book D. Bertsimas, J. N. Tsitsiklis, Introduction to Linear Optimization, Athena Scientific, 1997 I'm having some difficulty with the concept of adjacency as in adjacent basic solutions and adjacent bases . I'll start by collecting a few definitions and results pertaining to adjacency. The page numbers refer to the aforementioned book. Here are the definition related to adjacent basic feasible solutions and edges. Definition 1 (Adjacent basic solutions; p. 53) Two distinct basic solutions to a set of linear constraints in $\mathbb{R}^n$ are said to be adjacent if we can find $n - 1$ linearly independent constraints that are active at both of them. Definition 2 (Edge; p. 53) If two adjacent basic solutions are also feasible, then the line segment that joins them is called an edge of the feasible set. and then we have the following theorem that characterizes edges Theorem 3 (Edges joining adjacent vertices; Exercise 2.15, p. 78) Consider the polyhedron $P = \{x \in \mathbb{R}^n \mid a_i'x\geq b_i, i = 1, \dots, m\}$ . Suppose that $u$ and $v$ are distinct basic feasible solutions that satisfy $a_i'u = a_i'v = b_i$ , $i = 1, \dots, n - 1$ , and that the vectors $a_1, \dots, a_{n - 1}$ are linearly independent. (In particular, $u$ and $v$ are adjacent.) Let $L = \{\lambda u + (1-\lambda)v \mid 0\leq \lambda \leq 1\}$ be the segment that joins $u$ and $v$ . Then $L = \{z \in P \mid a_i' z = b_i, i = 1, \dots, n - 1\}$ . If we are dealing with a standard polyhedra then concept of a basis comes into the play and we get the following definition for two adjacent bases. Definition 4 (Adjacent bases; p. 56) For standard form problems, we say that two bases are adjacent if they share all but one basic column. The link between adjacent bases and adjacent basic solutions are given by the following theorem. Theorem 5 (Equivalence of adjacent basic solutions and adjacent bases; p. 56) a) Adjacent basic solutions can always be obtained from two adjacent bases. b) Conversely, if two adjacent bases lead to distinct basic solutions, then the latter are adjacent. Here's what I don't understand. Adjacent vertices, like basic solutions, are defined in terms of the way the polyhedron is represented: different representations may yield different basic solutions. However, the property of being a feasible basic solution was shown (Theorem 2.3, p. 50) to be independent of the representation used. Is adjacency of vertices also independent of the representation used? Conversely, is it possible that, given two vertices $u$ and $v$ , they are adjacent under one representation, but not adjacent under a different representation? It seems intuitive that if two feasible basic solutions are adjacent, then the edge between them lies in a hyperplane whose intersection with the polyhedron consists exactly of the points along the edge and furthermore, the hyperplane separates the space into two halves, only one of which intersects with the polyhedron. However, I fall short of proving this formally. I don't see how to prove Theorem 5. It was not proved in the book, but rather dismissed with the phrase: ""it is not hard to check"".","I'm reading chapter 2, entitled geometry of linear programming, of the following book D. Bertsimas, J. N. Tsitsiklis, Introduction to Linear Optimization, Athena Scientific, 1997 I'm having some difficulty with the concept of adjacency as in adjacent basic solutions and adjacent bases . I'll start by collecting a few definitions and results pertaining to adjacency. The page numbers refer to the aforementioned book. Here are the definition related to adjacent basic feasible solutions and edges. Definition 1 (Adjacent basic solutions; p. 53) Two distinct basic solutions to a set of linear constraints in are said to be adjacent if we can find linearly independent constraints that are active at both of them. Definition 2 (Edge; p. 53) If two adjacent basic solutions are also feasible, then the line segment that joins them is called an edge of the feasible set. and then we have the following theorem that characterizes edges Theorem 3 (Edges joining adjacent vertices; Exercise 2.15, p. 78) Consider the polyhedron . Suppose that and are distinct basic feasible solutions that satisfy , , and that the vectors are linearly independent. (In particular, and are adjacent.) Let be the segment that joins and . Then . If we are dealing with a standard polyhedra then concept of a basis comes into the play and we get the following definition for two adjacent bases. Definition 4 (Adjacent bases; p. 56) For standard form problems, we say that two bases are adjacent if they share all but one basic column. The link between adjacent bases and adjacent basic solutions are given by the following theorem. Theorem 5 (Equivalence of adjacent basic solutions and adjacent bases; p. 56) a) Adjacent basic solutions can always be obtained from two adjacent bases. b) Conversely, if two adjacent bases lead to distinct basic solutions, then the latter are adjacent. Here's what I don't understand. Adjacent vertices, like basic solutions, are defined in terms of the way the polyhedron is represented: different representations may yield different basic solutions. However, the property of being a feasible basic solution was shown (Theorem 2.3, p. 50) to be independent of the representation used. Is adjacency of vertices also independent of the representation used? Conversely, is it possible that, given two vertices and , they are adjacent under one representation, but not adjacent under a different representation? It seems intuitive that if two feasible basic solutions are adjacent, then the edge between them lies in a hyperplane whose intersection with the polyhedron consists exactly of the points along the edge and furthermore, the hyperplane separates the space into two halves, only one of which intersects with the polyhedron. However, I fall short of proving this formally. I don't see how to prove Theorem 5. It was not proved in the book, but rather dismissed with the phrase: ""it is not hard to check"".","\mathbb{R}^n n - 1 P = \{x \in \mathbb{R}^n \mid a_i'x\geq b_i, i = 1, \dots, m\} u v a_i'u = a_i'v = b_i i = 1, \dots, n - 1 a_1, \dots, a_{n - 1} u v L = \{\lambda u + (1-\lambda)v \mid 0\leq \lambda \leq 1\} u v L = \{z \in P \mid a_i' z = b_i, i = 1, \dots, n - 1\} u v","['linear-algebra', 'linear-programming', 'polyhedra']"
41,Commuting matrices and simultaneous diagonalizability,Commuting matrices and simultaneous diagonalizability,,"It is a known fact from linear algebra that if a set of matrices is pairwise commutable then they are simultaneously diagonalizable. A problem in the book I am currently studying asks to prove this claim using representation theory. Specifically If $G = \{M_1,\ldots,M_k\}$ is an abelian subgroup of $\rm{GL}_d(\mathbb{C}).$ How can one use representation theory to show that the matrices in $G$ are simultaneously diagonalizable?","It is a known fact from linear algebra that if a set of matrices is pairwise commutable then they are simultaneously diagonalizable. A problem in the book I am currently studying asks to prove this claim using representation theory. Specifically If $G = \{M_1,\ldots,M_k\}$ is an abelian subgroup of $\rm{GL}_d(\mathbb{C}).$ How can one use representation theory to show that the matrices in $G$ are simultaneously diagonalizable?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'representation-theory']"
42,"Intuition - If $Ax = b$ has infinitely many solutions, why can't $Ax = c$ have only one solution? [Strang P165 3.4. 22]","Intuition - If  has infinitely many solutions, why can't  have only one solution? [Strang P165 3.4. 22]",Ax = b Ax = c,"If $\mathbf{Ax = b}$ has infinitely many solutions, why is it impossible for $\mathbf{Ax = c}$ (where $\mathbf{c}$ is a new right side) to have only one solution? Proof : Take two solutions of $\mathbf{Ax = b} :$ $\mathbf{Ax_1 = b}$ and $\mathbf{Ax_2 = b} \implies \mathbf{\color{green}{A(x_1 - x_2) = 0}}$. Thus, if $\mathbf{Ax_0 = c}$ then add the homogenous solution in green to $\mathbf{x_0}$: $\mathbf{Ax_0 + \color{green}{A(x_1 - x_2)} = c + \color{green}{0} }$ so $\mathbf{x_0}$ is not unique. If $\mathbf{c}$ is not in $colspace(A)$, then no solution to $\mathbf{Ax = c}$. $\Large{{1.}}$ I accept the proof but I don't perceive the intuition? Would someone please explain/uncloak it? I recollect: If $\mathbf{c}$ is not in $colspace(A)$, then no solution to $\mathbf{Ax = c}$. $\Large{{2.}}$ How would you divine/previse the strategy of subtracting two solutions to $\mathbf{Ax = b} $ and then adding this homogeneous solution to $\mathbf{Ax = c}$  ? $\Large{{3.}}$ Does the above prove that $\mathbf{Ax = c}$ has infinitely many solutions also, beyond $\mathbf{x_0}$ and $\mathbf{x_0 + \color{green}{(x_1 - x_2)}}$?","If $\mathbf{Ax = b}$ has infinitely many solutions, why is it impossible for $\mathbf{Ax = c}$ (where $\mathbf{c}$ is a new right side) to have only one solution? Proof : Take two solutions of $\mathbf{Ax = b} :$ $\mathbf{Ax_1 = b}$ and $\mathbf{Ax_2 = b} \implies \mathbf{\color{green}{A(x_1 - x_2) = 0}}$. Thus, if $\mathbf{Ax_0 = c}$ then add the homogenous solution in green to $\mathbf{x_0}$: $\mathbf{Ax_0 + \color{green}{A(x_1 - x_2)} = c + \color{green}{0} }$ so $\mathbf{x_0}$ is not unique. If $\mathbf{c}$ is not in $colspace(A)$, then no solution to $\mathbf{Ax = c}$. $\Large{{1.}}$ I accept the proof but I don't perceive the intuition? Would someone please explain/uncloak it? I recollect: If $\mathbf{c}$ is not in $colspace(A)$, then no solution to $\mathbf{Ax = c}$. $\Large{{2.}}$ How would you divine/previse the strategy of subtracting two solutions to $\mathbf{Ax = b} $ and then adding this homogeneous solution to $\mathbf{Ax = c}$  ? $\Large{{3.}}$ Does the above prove that $\mathbf{Ax = c}$ has infinitely many solutions also, beyond $\mathbf{x_0}$ and $\mathbf{x_0 + \color{green}{(x_1 - x_2)}}$?",,['linear-algebra']
43,Can a unitary matrix be constructed from any doubly stochastic matrix?,Can a unitary matrix be constructed from any doubly stochastic matrix?,,"Here is a question that came up while I was thinking about the foundations of quantum mechanics: Consider a unitary $n\times n$ complex matrix $U$, with elements $u_{ij}$.  We know that the rows and columns of such a matrix must form orthonormal bases of $\mathbb{C}^n$. It follows that the matrix $P$, with elements $p_{ij} = |u_{ij}|^2$, must be doubly stochastic, i.e. $\sum_i p_{ij} = \sum_j p_{ij} = 1$, and every $p_{ij}\ge 0$. The question is, is the reverse also true? That is, for any arbitrary doubly stochastic matrix $P = (p_{ij})$, does there exist a unitary matrix $U$ with elements $u_{ij}$ such that $|u_{ij}|^2 = p_{ij}$? If so, is there a systematic way to construct such a unitary matrix, given $P$? (I realise the solution will not be unique.) If not, is there some other property, besides being doubly stochastic, that $P$ must have in order to make this conjecture true?","Here is a question that came up while I was thinking about the foundations of quantum mechanics: Consider a unitary $n\times n$ complex matrix $U$, with elements $u_{ij}$.  We know that the rows and columns of such a matrix must form orthonormal bases of $\mathbb{C}^n$. It follows that the matrix $P$, with elements $p_{ij} = |u_{ij}|^2$, must be doubly stochastic, i.e. $\sum_i p_{ij} = \sum_j p_{ij} = 1$, and every $p_{ij}\ge 0$. The question is, is the reverse also true? That is, for any arbitrary doubly stochastic matrix $P = (p_{ij})$, does there exist a unitary matrix $U$ with elements $u_{ij}$ such that $|u_{ij}|^2 = p_{ij}$? If so, is there a systematic way to construct such a unitary matrix, given $P$? (I realise the solution will not be unique.) If not, is there some other property, besides being doubly stochastic, that $P$ must have in order to make this conjecture true?",,"['linear-algebra', 'quantum-mechanics']"
44,Algebraically-nice general solution for last step of Gaussian elimination to Smith Normal Form?,Algebraically-nice general solution for last step of Gaussian elimination to Smith Normal Form?,,"My question takes a little bit of preamble: it concerns a well-known and solved problem, but I am looking for a solution with a particularly nice property. $\newcommand{\matrix}[4]{\left( \begin{array}{cc} #1 & #2 \\ #3 & #4 \end{array} \right)}  \DeclareMathOperator{\lcm}{lcm}$In using Guassian elimination to put a matrix into Smith normal form over $\mathbb{Z}$ (or, more generally, some PID ), the last step is to make sure that successive diagonal entries divide each other.  Solving this reduces to the following problem (with all matrices over $\mathbb{Z}$): given a diagonal matrix $M = \left( \begin{array}{cc} a & 0 \\ 0 & b \end{array} \right)$, find invertible matrices $L$, $R$ such that  $$ M = L \left( \begin{array}{cc} \gcd(a,b) & 0 \\ 0 & \lcm(a,b) \end{array} \right) R $$ This is of course well-known and not hard to do.  For instance, using the Euclidean algorithm to find $(x,y)$ such that $ax + by = d = \gcd(a,b)$, one can define $$ L = \left( \begin{array}{cc} a/d & -y \\ b/d & x \end{array} \right),  \quad R = \left( \begin{array}{cc} 1-yb/d & yb/d \\ -1 & 1 \end{array} \right)$$ or alternatively $$ L = \left( \begin{array}{cc} a/d & -1 \\ 1-xa/d & x \end{array} \right), \quad R = \left( \begin{array}{cc} 1-yb/d & b/d \\ -y & 1 \end{array} \right).$$ However, in the special case where $a$ divides $b$, we know that $M$ is already as desired and so we could simply take $L = R = I$.  My question is: can we find a general algebraic solution like the ones above (i.e. algebraic definitions of $L$, $R$ in terms of the integers $(x,y,a/d,b/d)$), but with the additional property that when $a$ divides $b$ (and so $x=1$, $y=0$, $a/d=1$), the solution yields $L = R = I$? Roughly: can we find an algebraic solution which only does something non-trivial if it needs to?","My question takes a little bit of preamble: it concerns a well-known and solved problem, but I am looking for a solution with a particularly nice property. $\newcommand{\matrix}[4]{\left( \begin{array}{cc} #1 & #2 \\ #3 & #4 \end{array} \right)}  \DeclareMathOperator{\lcm}{lcm}$In using Guassian elimination to put a matrix into Smith normal form over $\mathbb{Z}$ (or, more generally, some PID ), the last step is to make sure that successive diagonal entries divide each other.  Solving this reduces to the following problem (with all matrices over $\mathbb{Z}$): given a diagonal matrix $M = \left( \begin{array}{cc} a & 0 \\ 0 & b \end{array} \right)$, find invertible matrices $L$, $R$ such that  $$ M = L \left( \begin{array}{cc} \gcd(a,b) & 0 \\ 0 & \lcm(a,b) \end{array} \right) R $$ This is of course well-known and not hard to do.  For instance, using the Euclidean algorithm to find $(x,y)$ such that $ax + by = d = \gcd(a,b)$, one can define $$ L = \left( \begin{array}{cc} a/d & -y \\ b/d & x \end{array} \right),  \quad R = \left( \begin{array}{cc} 1-yb/d & yb/d \\ -1 & 1 \end{array} \right)$$ or alternatively $$ L = \left( \begin{array}{cc} a/d & -1 \\ 1-xa/d & x \end{array} \right), \quad R = \left( \begin{array}{cc} 1-yb/d & b/d \\ -y & 1 \end{array} \right).$$ However, in the special case where $a$ divides $b$, we know that $M$ is already as desired and so we could simply take $L = R = I$.  My question is: can we find a general algebraic solution like the ones above (i.e. algebraic definitions of $L$, $R$ in terms of the integers $(x,y,a/d,b/d)$), but with the additional property that when $a$ divides $b$ (and so $x=1$, $y=0$, $a/d=1$), the solution yields $L = R = I$? Roughly: can we find an algebraic solution which only does something non-trivial if it needs to?",,"['linear-algebra', 'matrices', 'gaussian-elimination']"
45,How to prove the positive-definiteness of a symmetric Toeplitz matrix like this?,How to prove the positive-definiteness of a symmetric Toeplitz matrix like this?,,"Define a symmetric Toeplitz matrix by $$\begin{pmatrix}c_1 & c_2 & c_3 & \cdots & c_n\\c_2 & c_1 & c_2 & \cdots & c_{n-1}\\c_3 & c_2 & c_1 & \cdots &c_{n-2}\\ \vdots & \vdots & \vdots & \ddots & \vdots\\c_n & c_{n-1} & c_{n-2} & \cdots & c_1\end{pmatrix},$$  for $$c_1 = 1, \quad c_{k+1}=\frac{1}{2}\left((k+1)^{2-\alpha}-2 k^{2-\alpha}+(k-1)^{2-\alpha}\right) \quad (k>0),$$ where $\alpha \in (0, 1)$. How to prove that this matrix is positive-definite? Thanks a lot.","Define a symmetric Toeplitz matrix by $$\begin{pmatrix}c_1 & c_2 & c_3 & \cdots & c_n\\c_2 & c_1 & c_2 & \cdots & c_{n-1}\\c_3 & c_2 & c_1 & \cdots &c_{n-2}\\ \vdots & \vdots & \vdots & \ddots & \vdots\\c_n & c_{n-1} & c_{n-2} & \cdots & c_1\end{pmatrix},$$  for $$c_1 = 1, \quad c_{k+1}=\frac{1}{2}\left((k+1)^{2-\alpha}-2 k^{2-\alpha}+(k-1)^{2-\alpha}\right) \quad (k>0),$$ where $\alpha \in (0, 1)$. How to prove that this matrix is positive-definite? Thanks a lot.",,"['linear-algebra', 'matrices']"
46,show that the characteristic polynomial of this matrix has negative coefficients,show that the characteristic polynomial of this matrix has negative coefficients,,"Let $n\geq 2$,  $A$ be the $n\times n$ matrix $A=(a_{ij})$ where $a_{ij}=\max(i,j)$. Can anybody show that the characteristic polynomial $P(x)=\det(xI-A)$ has all its coefficients negative except the leading one? I have checked this for $2 \leq n \leq 20$.","Let $n\geq 2$,  $A$ be the $n\times n$ matrix $A=(a_{ij})$ where $a_{ij}=\max(i,j)$. Can anybody show that the characteristic polynomial $P(x)=\det(xI-A)$ has all its coefficients negative except the leading one? I have checked this for $2 \leq n \leq 20$.",,"['linear-algebra', 'matrices']"
47,"Prove that if $(v_1,\ldots,v_n)$ spans $V$, then so does the list $(v_1-v_2,v_2-v_3,\ldots,v_{n-1}-v_n,v_n).$","Prove that if  spans , then so does the list","(v_1,\ldots,v_n) V (v_1-v_2,v_2-v_3,\ldots,v_{n-1}-v_n,v_n).","Prove that if $(v_1,\ldots,v_n)$ spans $V$, then so does the list $(v_1-v_2,v_2-v_3,\ldots,v_{n-1}-v_n,v_n).$ Proof: Suppose that $V = \text{span } (v_1,\ldots,v_n).$ Then for any $v \in V$, there exist $a_1,\ldots,a_n \in \mathbb{F}$ such that \begin{align}     v &= a_1v_1+\cdots +a_nv_n  \\   &= a_1v_1-a_1v_2 + a_1v_2+a_2v_2+a_3v_3+\cdots+ a_nv_n  \\   &= a_1v_1-a_1v_2+a_1v_2-a_1v_3+a_2v_2-a_2v_3+a_1v_3+a_2v_3+a_3v_3+a_4v_4+\cdots+a_nv_n \\   &= a_1(v_1-v_2)+(a_1+a_2)(v_2-v_3)+(a_1+a_2+a_3)v_3+a_4v_4+\cdots+a_nv_n \\   &=\sum_{i=1}^{n-1} \left[\left(\sum_{k=1}^{i}a_k\right)(v_i-v_{i+1})\right] + a_nv_n \in \text{span}(v_1-v_2,v_2-v_3,\ldots,v_{n-1}-v_n,v_n). \end{align} Now let $u \in \text{span}(v_1-v_2,v_2-v_3,\ldots,v_{n-1}-v_n,v_n).$ Then there exist $b_1,\ldots,b_n \in \mathbb{F}$ such that \begin{align}     u &= b_1(v_1-v_2)+ b_2(v_2-v_3)+\cdots +b_{n-1}(v_{n-1}-v_n) + b_nv_n \\   &= b_1v_1 + (b_2-b_1)v_2+(b_3-b_2)v_3+\cdots+(b_n-b_{n-1})v_n \in \text{span}(v_1,\ldots,v_n)  \end{align} completing the proof. $\hspace{125mm} \Box$","Prove that if $(v_1,\ldots,v_n)$ spans $V$, then so does the list $(v_1-v_2,v_2-v_3,\ldots,v_{n-1}-v_n,v_n).$ Proof: Suppose that $V = \text{span } (v_1,\ldots,v_n).$ Then for any $v \in V$, there exist $a_1,\ldots,a_n \in \mathbb{F}$ such that \begin{align}     v &= a_1v_1+\cdots +a_nv_n  \\   &= a_1v_1-a_1v_2 + a_1v_2+a_2v_2+a_3v_3+\cdots+ a_nv_n  \\   &= a_1v_1-a_1v_2+a_1v_2-a_1v_3+a_2v_2-a_2v_3+a_1v_3+a_2v_3+a_3v_3+a_4v_4+\cdots+a_nv_n \\   &= a_1(v_1-v_2)+(a_1+a_2)(v_2-v_3)+(a_1+a_2+a_3)v_3+a_4v_4+\cdots+a_nv_n \\   &=\sum_{i=1}^{n-1} \left[\left(\sum_{k=1}^{i}a_k\right)(v_i-v_{i+1})\right] + a_nv_n \in \text{span}(v_1-v_2,v_2-v_3,\ldots,v_{n-1}-v_n,v_n). \end{align} Now let $u \in \text{span}(v_1-v_2,v_2-v_3,\ldots,v_{n-1}-v_n,v_n).$ Then there exist $b_1,\ldots,b_n \in \mathbb{F}$ such that \begin{align}     u &= b_1(v_1-v_2)+ b_2(v_2-v_3)+\cdots +b_{n-1}(v_{n-1}-v_n) + b_nv_n \\   &= b_1v_1 + (b_2-b_1)v_2+(b_3-b_2)v_3+\cdots+(b_n-b_{n-1})v_n \in \text{span}(v_1,\ldots,v_n)  \end{align} completing the proof. $\hspace{125mm} \Box$",,"['linear-algebra', 'proof-verification']"
48,Determinants of block matrices with commuting blocks,Determinants of block matrices with commuting blocks,,"On Wikipedia, I read that $$\det \begin{pmatrix} A & B\\B& A\end{pmatrix} = \det(A+B) \, \det(A-B)$$ if $A$ and $B$ commute. Does this hold even if $ A $ and $ B$ are not invertible? I spent a short amount of time trying to come up with a counter example, but it worked for the noninvertible matrices I picked. I'm curious if this is in fact true. Does anyone have a proof of it?","On Wikipedia, I read that if and commute. Does this hold even if and are not invertible? I spent a short amount of time trying to come up with a counter example, but it worked for the noninvertible matrices I picked. I'm curious if this is in fact true. Does anyone have a proof of it?","\det \begin{pmatrix} A & B\\B& A\end{pmatrix} = \det(A+B) \, \det(A-B) A B  A   B","['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
49,Prove that the determinant of this matrix can be represented as a polynomial.,Prove that the determinant of this matrix can be represented as a polynomial.,,"Let $A=(A_{ij})$ be a square matrix of order $n$. Verify that the determinant of the matrix $\left( \begin{array}{ccc} a_{11}+x & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22}+x & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn}+x \end{array} \right)$, can be represented as the polynomial $x^n+a_1x^{n-1}+a_2x^{n-2}+\cdots+a_{n-1}x+a_n$, where each coefficient $a_k$ is the sum of the minors of order $k$ of the matrix $A$. I tried to use the definition of determinant by cofactor expansion but it's very long, I was wondering if there's a shorter way to show this.","Let $A=(A_{ij})$ be a square matrix of order $n$. Verify that the determinant of the matrix $\left( \begin{array}{ccc} a_{11}+x & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22}+x & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn}+x \end{array} \right)$, can be represented as the polynomial $x^n+a_1x^{n-1}+a_2x^{n-2}+\cdots+a_{n-1}x+a_n$, where each coefficient $a_k$ is the sum of the minors of order $k$ of the matrix $A$. I tried to use the definition of determinant by cofactor expansion but it's very long, I was wondering if there's a shorter way to show this.",,['linear-algebra']
50,On minimizing matrix norm,On minimizing matrix norm,,"Given $m\geq n \geq k$ and $A \in M^{m\times n}(\mathbb{C})$, the problem asks to evaluate $$\inf_{\textrm{rank}(B)\leq k}||A-B||$$ and gives condition on $A$ making the above minimum is taken by some unique $B$. Here $||\cdot||$ denotes standard spectral norm of a matrix, i.e. the norm of a linear functional defined by regarding matrix as linear transform between Euclidean spaces with canonical Euclidean norm. It is quite clear if we apply Singular Value Decomposition to $A$, we will find $$\inf_{\mathrm{rank}(B)\leq k}||A-B||\leq \sigma_{k+1}$$ where $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_n\geq0$ denotes the singular value of $A$, and define $\sigma_{n+1}=0$. But I am not sure if the inequality is actually equality since I do not see an approach to discuss how spectral radius changes by perturbing with a matrix of given rank.","Given $m\geq n \geq k$ and $A \in M^{m\times n}(\mathbb{C})$, the problem asks to evaluate $$\inf_{\textrm{rank}(B)\leq k}||A-B||$$ and gives condition on $A$ making the above minimum is taken by some unique $B$. Here $||\cdot||$ denotes standard spectral norm of a matrix, i.e. the norm of a linear functional defined by regarding matrix as linear transform between Euclidean spaces with canonical Euclidean norm. It is quite clear if we apply Singular Value Decomposition to $A$, we will find $$\inf_{\mathrm{rank}(B)\leq k}||A-B||\leq \sigma_{k+1}$$ where $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_n\geq0$ denotes the singular value of $A$, and define $\sigma_{n+1}=0$. But I am not sure if the inequality is actually equality since I do not see an approach to discuss how spectral radius changes by perturbing with a matrix of given rank.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
51,Proving that a Particular Set Is a Vector Space,Proving that a Particular Set Is a Vector Space,,"Let $V$ be the set of all differentiable real-valued functions defined on $\mathbb R$. Show that $V$ is a vector space under addition and scalar multiplication, defined by   $$(f+g)(t) = f(t) + g(t),\quad (cf)(t) = c[ f(t)],$$ where $f, g \in V$, $c \in F$. Since addition defined as such is one-to-one, and since $f(t) + g(t)$ is differentiable,  $f+g$ is unique and belongs to $V$. By similar argument, $cf$ is unique and belongs to $V$. My question has to do with the eight properties of the field $F$. They are so obvious that I have to question if I did it right. For example, $$f+g = f(t) + g(t) = g(t) + f(t) = g+f.$$ Now, how do I justify that $f(t) + g(t) = g(t) + f(t)$? I do not think that continuity is sufficient, because the proof that $$\lim_{t \to c} f(t) + g(t) = \lim_{t \to c} g(t) + f(t)$$ already assumes that $f(t) + g(t) = g(t) + f(t)$.","Let $V$ be the set of all differentiable real-valued functions defined on $\mathbb R$. Show that $V$ is a vector space under addition and scalar multiplication, defined by   $$(f+g)(t) = f(t) + g(t),\quad (cf)(t) = c[ f(t)],$$ where $f, g \in V$, $c \in F$. Since addition defined as such is one-to-one, and since $f(t) + g(t)$ is differentiable,  $f+g$ is unique and belongs to $V$. By similar argument, $cf$ is unique and belongs to $V$. My question has to do with the eight properties of the field $F$. They are so obvious that I have to question if I did it right. For example, $$f+g = f(t) + g(t) = g(t) + f(t) = g+f.$$ Now, how do I justify that $f(t) + g(t) = g(t) + f(t)$? I do not think that continuity is sufficient, because the proof that $$\lim_{t \to c} f(t) + g(t) = \lim_{t \to c} g(t) + f(t)$$ already assumes that $f(t) + g(t) = g(t) + f(t)$.",,"['linear-algebra', 'vector-spaces']"
52,"Can a non-square matrix be called ""invertible""?","Can a non-square matrix be called ""invertible""?",,"Let $R$ be a commutative ring with $1 \neq 0$. It is known that $R^{n} \hookrightarrow R^{m}$ implies $n \leq m$ and $R^{n} \twoheadrightarrow R^{m}$ implies $n \geq m$, and I might use this without mentioning (though I don't think I did). Isomorphisms are $R$-module maps if not mentioned otherwise. Let $m \geq n$ and consider $R^{n} = R^{n} \times 0 \times \cdots \times 0 \subseteq R^{m}$. We have $\text{Hom}_{R}(R^{n}, R^{m}) \simeq M_{m \times n}(R)$ via $\phi \mapsto M(\phi) = (c_{ij})$ where $\phi(e_{j}) = \sum_{i=1}^{m}c_{ij}e_{i}$. Take any $R$-linearly independent $y_{1}, \cdots, y_{n} \subseteq R^{m}$ and define an $m \times n$ matrix $A = (a_{ij})$ via $y_{j} = b_{1j}e_{1} + \cdots + b_{mj}e_{m}$. If we understand $e_{j}$ with $1 \leq j \leq n$ as $n \times 1$ column matrix in $R^{n}$ when we multiply an $m \times n$ matrix to it on its left, then $Ae_{j} = y_{j}$ for $1 \leq j \leq n$. Thus, we can understand this matrix as an injection $A : R^{n} \hookrightarrow R^{m}$. By definition $A$ is an $m \times n$ matrix, but if what I wrote is correct, then $A|_{R^{n}}$ must be an isomorphism $R^{n} \simeq \bigoplus_{j=1}^{n} Ry_{j}$. I am not trying to find an invertible matrix to $A$, but is it okay to think of it as an embedding like how I described?","Let $R$ be a commutative ring with $1 \neq 0$. It is known that $R^{n} \hookrightarrow R^{m}$ implies $n \leq m$ and $R^{n} \twoheadrightarrow R^{m}$ implies $n \geq m$, and I might use this without mentioning (though I don't think I did). Isomorphisms are $R$-module maps if not mentioned otherwise. Let $m \geq n$ and consider $R^{n} = R^{n} \times 0 \times \cdots \times 0 \subseteq R^{m}$. We have $\text{Hom}_{R}(R^{n}, R^{m}) \simeq M_{m \times n}(R)$ via $\phi \mapsto M(\phi) = (c_{ij})$ where $\phi(e_{j}) = \sum_{i=1}^{m}c_{ij}e_{i}$. Take any $R$-linearly independent $y_{1}, \cdots, y_{n} \subseteq R^{m}$ and define an $m \times n$ matrix $A = (a_{ij})$ via $y_{j} = b_{1j}e_{1} + \cdots + b_{mj}e_{m}$. If we understand $e_{j}$ with $1 \leq j \leq n$ as $n \times 1$ column matrix in $R^{n}$ when we multiply an $m \times n$ matrix to it on its left, then $Ae_{j} = y_{j}$ for $1 \leq j \leq n$. Thus, we can understand this matrix as an injection $A : R^{n} \hookrightarrow R^{m}$. By definition $A$ is an $m \times n$ matrix, but if what I wrote is correct, then $A|_{R^{n}}$ must be an isomorphism $R^{n} \simeq \bigoplus_{j=1}^{n} Ry_{j}$. I am not trying to find an invertible matrix to $A$, but is it okay to think of it as an embedding like how I described?",,"['linear-algebra', 'abstract-algebra', 'commutative-algebra']"
53,Bijective map preserving inner products is linear,Bijective map preserving inner products is linear,,"The question comes from Kaplansky's book Linear Algebra and Geometry on page 96 exercise 2 Let $V$ be a non-singular inner product space of characteristic $\neq2$. Let $T$ be a one-to-one map of $V$ onto itself, sending $0$ to $0$ and satisfying $(x-y, x-y) = (Tx - Ty, Tx - Ty)$ for all $x,y \in V$. Prove that $T$ is orthogonal. That $T$ preserves inner products is easy, but I don't know how to prove that it is linear. Any help would be appreciated.","The question comes from Kaplansky's book Linear Algebra and Geometry on page 96 exercise 2 Let $V$ be a non-singular inner product space of characteristic $\neq2$. Let $T$ be a one-to-one map of $V$ onto itself, sending $0$ to $0$ and satisfying $(x-y, x-y) = (Tx - Ty, Tx - Ty)$ for all $x,y \in V$. Prove that $T$ is orthogonal. That $T$ preserves inner products is easy, but I don't know how to prove that it is linear. Any help would be appreciated.",,['linear-algebra']
54,"If $\dim V=v$ and $\dim(\ker T)=n$, prove that $T$ has at most $v-n+1$ distinct eigenvalues","If  and , prove that  has at most  distinct eigenvalues",\dim V=v \dim(\ker T)=n T v-n+1,"Let $T:V\to V$ be a linear operator. If $\dim V=v$ and $\dim(\ker T)=n$, prove that $T$ has at most $v-n+1$ distinct eigenvalues. I have been working on this proof for a few days and I am not sure what direction to really go with it? I feel like starting with the rank nullity theorem is correct and relating that to the sum of eigenspaces may be my next move. Though I cant think of how to bring these two ideas together to create a fluid proof? Thank you for your help...","Let $T:V\to V$ be a linear operator. If $\dim V=v$ and $\dim(\ker T)=n$, prove that $T$ has at most $v-n+1$ distinct eigenvalues. I have been working on this proof for a few days and I am not sure what direction to really go with it? I feel like starting with the rank nullity theorem is correct and relating that to the sum of eigenspaces may be my next move. Though I cant think of how to bring these two ideas together to create a fluid proof? Thank you for your help...",,['linear-algebra']
55,Conceptual questions about cross product rules,Conceptual questions about cross product rules,,I am studying cross products and I just cannot get my head around the materials. Can someone explain to me why... a) The cross product $\vec{a}\times \vec{b}$ is normal to $\vec{a}$ and $\vec{b}$. Where did this rule come from? b) $|\vec{a}\times \vec{b}| = |\vec{a}||\vec{b}| \sin\theta$ is area of a parallelogram c) $|\vec{a} \cdot (\vec{b} \times \vec{c})|$ is the volume of a parallelepiped. These rules look very strange to me. I just don't see how someone could have come up with them out of thin air. There has to be some reasoning and/or insight I'm missing.,I am studying cross products and I just cannot get my head around the materials. Can someone explain to me why... a) The cross product $\vec{a}\times \vec{b}$ is normal to $\vec{a}$ and $\vec{b}$. Where did this rule come from? b) $|\vec{a}\times \vec{b}| = |\vec{a}||\vec{b}| \sin\theta$ is area of a parallelogram c) $|\vec{a} \cdot (\vec{b} \times \vec{c})|$ is the volume of a parallelepiped. These rules look very strange to me. I just don't see how someone could have come up with them out of thin air. There has to be some reasoning and/or insight I'm missing.,,"['linear-algebra', 'algebra-precalculus']"
56,"Properties of trace $0$ matrices: similarity, invertibility, relation to commutators","Properties of trace  matrices: similarity, invertibility, relation to commutators",0,"$1.$ Are trace $0$ matrices always of the form $AB-BA$? $2.$ Is a trace $0$ matrix over the complex field always similar to a matrix with $0$ as a diagonal element? $3.$ Is a trace $0$ matrices over any field always similar to a matrix with $0$ as a diagonal element? $4.$ Is a trace $0$ matrix not invertible if it is upper triangular.? I solved one problem in hoffman kunze saying : $W$ be the span of $n\times n$ matrices over the field $F$ and $W_0$ be the subspace spanned by the matrices $C$ where $C=AB-BA$. Then we proved there that $W_0$ is the exactly subspace of matrices which have trace $0$, so from this result can we say $1$ is true?","$1.$ Are trace $0$ matrices always of the form $AB-BA$? $2.$ Is a trace $0$ matrix over the complex field always similar to a matrix with $0$ as a diagonal element? $3.$ Is a trace $0$ matrices over any field always similar to a matrix with $0$ as a diagonal element? $4.$ Is a trace $0$ matrix not invertible if it is upper triangular.? I solved one problem in hoffman kunze saying : $W$ be the span of $n\times n$ matrices over the field $F$ and $W_0$ be the subspace spanned by the matrices $C$ where $C=AB-BA$. Then we proved there that $W_0$ is the exactly subspace of matrices which have trace $0$, so from this result can we say $1$ is true?",,"['linear-algebra', 'matrices', 'trace']"
57,"How to show that a valid inner product on V is defined with the formula $[x, y] = \langle Ax, Ay\rangle $?",How to show that a valid inner product on V is defined with the formula ?,"[x, y] = \langle Ax, Ay\rangle ","Let $A \in L(V,W)$ be an injection and $W$ an inner product space with the inner product $\langle \cdot,\cdot\rangle $. Prove that a valid inner product on $V$ is defined with the formula $[x, y] = \langle Ax, Ay\rangle $ $L(V, W)$ = The set of all linear mappings (linear operators) from V to W To prove this, if I am correct, I need to show that the four properties of an inner products space apply on this formula: 1. $\langle x, y \rangle  = \overline{\langle y,x\rangle }$ 2. $\langle \alpha x, y\rangle  = \alpha\langle x,y\rangle $ 3. $\langle x+y,z\rangle  = \langle x,z\rangle  + \langle y,z\rangle $ 4. $\langle x,x\rangle \space \ge 0 \space \space \forall x$ 4.' $\langle x,x\rangle \space = 0 \Longleftrightarrow x=0$ 4. $ [x, x] = \langle Ax, Ax\rangle , Ax \in W$, and since $W$ is an inner product space, $\langle Ax, Ax\rangle \space \ge 0$ implies $[x, x] \ge 0$. 4.' Since A is an injection: $Ax = 0 \implies x = 0$ and since $W$ is an inner product space and $Ax \in W \implies \langle  Ax, Ax \rangle  = 0 \implies Ax = 0 \implies [x,x] = 0 \Longleftrightarrow x=0$ 3. $[x+y,z] = \langle A(x+y), Az\rangle  = A$ is linear $= \langle Ax + Ay, Az\rangle = Ax, Ay, Az \in W$ and $W$ is an inner product space $= \langle Ax, Az\rangle  + \langle Ay, Az\rangle  = [x,z] + [y,z]$ 2. $[\alpha x, y] = \langle A(\alpha x), Ay\rangle  = A$ is linear $ = \langle \alpha (Ax), Ay \rangle  = W$ is an inner product space $ = \alpha \langle Ax, Ay\rangle  = \alpha [x, y]$ 1. $[x, y] = \langle  Ax, Ay \rangle  = W$ is an inner product space $= \overline{\langle Ay, Ax\rangle } = \overline{[y, x]}$ But this seems to me a little to easy, did I maybe conclude something that can't be concluded so easily or maybe is my approach to prove this completely wrong?","Let $A \in L(V,W)$ be an injection and $W$ an inner product space with the inner product $\langle \cdot,\cdot\rangle $. Prove that a valid inner product on $V$ is defined with the formula $[x, y] = \langle Ax, Ay\rangle $ $L(V, W)$ = The set of all linear mappings (linear operators) from V to W To prove this, if I am correct, I need to show that the four properties of an inner products space apply on this formula: 1. $\langle x, y \rangle  = \overline{\langle y,x\rangle }$ 2. $\langle \alpha x, y\rangle  = \alpha\langle x,y\rangle $ 3. $\langle x+y,z\rangle  = \langle x,z\rangle  + \langle y,z\rangle $ 4. $\langle x,x\rangle \space \ge 0 \space \space \forall x$ 4.' $\langle x,x\rangle \space = 0 \Longleftrightarrow x=0$ 4. $ [x, x] = \langle Ax, Ax\rangle , Ax \in W$, and since $W$ is an inner product space, $\langle Ax, Ax\rangle \space \ge 0$ implies $[x, x] \ge 0$. 4.' Since A is an injection: $Ax = 0 \implies x = 0$ and since $W$ is an inner product space and $Ax \in W \implies \langle  Ax, Ax \rangle  = 0 \implies Ax = 0 \implies [x,x] = 0 \Longleftrightarrow x=0$ 3. $[x+y,z] = \langle A(x+y), Az\rangle  = A$ is linear $= \langle Ax + Ay, Az\rangle = Ax, Ay, Az \in W$ and $W$ is an inner product space $= \langle Ax, Az\rangle  + \langle Ay, Az\rangle  = [x,z] + [y,z]$ 2. $[\alpha x, y] = \langle A(\alpha x), Ay\rangle  = A$ is linear $ = \langle \alpha (Ax), Ay \rangle  = W$ is an inner product space $ = \alpha \langle Ax, Ay\rangle  = \alpha [x, y]$ 1. $[x, y] = \langle  Ax, Ay \rangle  = W$ is an inner product space $= \overline{\langle Ay, Ax\rangle } = \overline{[y, x]}$ But this seems to me a little to easy, did I maybe conclude something that can't be concluded so easily or maybe is my approach to prove this completely wrong?",,['linear-algebra']
58,Vectors Definition,Vectors Definition,,"What is basically a vector ? I am not a starter in learning vectors , have used them in physics and mathematics and done with 1st year of my BS in physics . But what is a vector basically ? Is it anything that has direction and magnitude ? or is it something that obeys vector laws , but this statement seems kind of circular ? How can we go about doing geometry using vectors ? and represent shapes in euclidean geometry using vectors ?","What is basically a vector ? I am not a starter in learning vectors , have used them in physics and mathematics and done with 1st year of my BS in physics . But what is a vector basically ? Is it anything that has direction and magnitude ? or is it something that obeys vector laws , but this statement seems kind of circular ? How can we go about doing geometry using vectors ? and represent shapes in euclidean geometry using vectors ?",,['linear-algebra']
59,A basis of the symmetric power consisting of powers,A basis of the symmetric power consisting of powers,,"Let $V$ be a complex vector space of dimension $n$. Denote by $v_1\odot\cdots\odot v_k$ the image of $v_1\otimes\cdots\otimes v_k$ in the symmetric power $\newcommand{\Sym}{\mathrm{Sym}}\Sym^k(V)$. It is well-known that the Elements $v^{\odot k}$  for $v\in V$ generate this space (see, for instance, this answer on math.se ), so they must contain a basis. In other words, let $N=\binom{n+k-1}k$, then there must be $v_1,\ldots,v_N\in V$ with  $$\Sym^k V = \mathbb Cv_1^{\odot k} \oplus \cdots \oplus \mathbb C v_N^{\odot k}.$$ I am looking for an explicit description of such a basis. Is such a description known? Is there maybe even a ""nice"" or somewhat ""natural"" choice for the $v_i$? As pointed out by Martin Brandenburg, I certainly allow choosing a basis $x_1,\ldots,x_n\in V$ and expressing the $v_i$ as linear combinations of these.","Let $V$ be a complex vector space of dimension $n$. Denote by $v_1\odot\cdots\odot v_k$ the image of $v_1\otimes\cdots\otimes v_k$ in the symmetric power $\newcommand{\Sym}{\mathrm{Sym}}\Sym^k(V)$. It is well-known that the Elements $v^{\odot k}$  for $v\in V$ generate this space (see, for instance, this answer on math.se ), so they must contain a basis. In other words, let $N=\binom{n+k-1}k$, then there must be $v_1,\ldots,v_N\in V$ with  $$\Sym^k V = \mathbb Cv_1^{\odot k} \oplus \cdots \oplus \mathbb C v_N^{\odot k}.$$ I am looking for an explicit description of such a basis. Is such a description known? Is there maybe even a ""nice"" or somewhat ""natural"" choice for the $v_i$? As pointed out by Martin Brandenburg, I certainly allow choosing a basis $x_1,\ldots,x_n\in V$ and expressing the $v_i$ as linear combinations of these.",,"['linear-algebra', 'abstract-algebra', 'polynomials', 'multilinear-algebra']"
60,Reproducing Kernel Hilbert Spaces for Dummies,Reproducing Kernel Hilbert Spaces for Dummies,,"I am in the middle of some machine learning paper that states that for function $f$,  imposing the norm constraint, $\|f \|=1$,  corresponds to an orthogonal projection onto the direction selected in reproducing kernel Hilbert space. I do not get this. I am lacking solid background in RKHS, so can anyone shortly tell me what it tries to say? How is an equality bounded norm  related to orthogonality? Do I need to study functional analysis?","I am in the middle of some machine learning paper that states that for function $f$,  imposing the norm constraint, $\|f \|=1$,  corresponds to an orthogonal projection onto the direction selected in reproducing kernel Hilbert space. I do not get this. I am lacking solid background in RKHS, so can anyone shortly tell me what it tries to say? How is an equality bounded norm  related to orthogonality? Do I need to study functional analysis?",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces', 'machine-learning']"
61,Connection between Galois trace and matrix trace,Connection between Galois trace and matrix trace,,"$\newcommand{\Tr}{\operatorname{Tr}}$ I am having trouble seeing the connection between the two kinds of trace. For a finite extension $K$ of a field $\mathbb F$ of degree $n$ , with $\alpha \in K$ , we defined the Galois trace as $$ \Tr(\alpha) = \sum_{\sigma \in \mathrm{Gal}(K/F)} \sigma(\alpha) $$ We also showed that $\Tr(\alpha) = -a_{n-1}$ , where $m_{\alpha, F}(x) = \displaystyle\sum_{i=0}^na_ix^i$ is the minimal polynomial for $\alpha$ in $F[x]$ . We have also shown that the minimal polynomial for $\alpha$ , $m_{\alpha, F}(x)$ and the minimal polynomial for left multiplication by $\alpha$ , $T_\alpha(g) = \alpha g$ for $g \in K$ , denoted by $m_{T_\alpha}$ , are in fact the same polynomial. My question is: how can I now see that these two traces are equivalent: one is the sum of the Galois conjugates, and one is the sum of the diagonal entries of the $n \times n$ matrix representation of $T_\alpha$ .","I am having trouble seeing the connection between the two kinds of trace. For a finite extension of a field of degree , with , we defined the Galois trace as We also showed that , where is the minimal polynomial for in . We have also shown that the minimal polynomial for , and the minimal polynomial for left multiplication by , for , denoted by , are in fact the same polynomial. My question is: how can I now see that these two traces are equivalent: one is the sum of the Galois conjugates, and one is the sum of the diagonal entries of the matrix representation of .","\newcommand{\Tr}{\operatorname{Tr}} K \mathbb F n \alpha \in K 
\Tr(\alpha) = \sum_{\sigma \in \mathrm{Gal}(K/F)} \sigma(\alpha)
 \Tr(\alpha) = -a_{n-1} m_{\alpha, F}(x) = \displaystyle\sum_{i=0}^na_ix^i \alpha F[x] \alpha m_{\alpha, F}(x) \alpha T_\alpha(g) = \alpha g g \in K m_{T_\alpha} n \times n T_\alpha","['linear-algebra', 'polynomials', 'galois-theory']"
62,Is the polar set of convex polytope also a polytope?,Is the polar set of convex polytope also a polytope?,,"Let $P$ be a convex polytope. How can I prove that the polar set of $P$ (lets call it $P^*$) is polytope? where $P^*=\{x\in\mathcal R^n:\forall v\in P, |\langle x,v\rangle|\le1\}$ . $Thanks$","Let $P$ be a convex polytope. How can I prove that the polar set of $P$ (lets call it $P^*$) is polytope? where $P^*=\{x\in\mathcal R^n:\forall v\in P, |\langle x,v\rangle|\le1\}$ . $Thanks$",,"['linear-algebra', 'convex-analysis', 'normed-spaces', 'convex-geometry', 'polytopes']"
63,Is there any connection between this matrices,Is there any connection between this matrices,,"Matrices I discuss are all $N\times N$ hermitian matrices. Define two positive (semi)definite matrices $H_1$ and $H_2$. Define the following matrices  \begin{align} P_1&=H_1+(I+H_2)^{-1} \\ P_2&=(I+H_1)^{-1}+H_2 \end{align} I was just curious if there are any connections between them. It can be from any perspective, eigenvalues, rank, eigenbasis, simultaneous diagonalization or any such concept. Even special cases are welcome, for instance, say they are rank-one matrices, Does it make any difference?","Matrices I discuss are all $N\times N$ hermitian matrices. Define two positive (semi)definite matrices $H_1$ and $H_2$. Define the following matrices  \begin{align} P_1&=H_1+(I+H_2)^{-1} \\ P_2&=(I+H_1)^{-1}+H_2 \end{align} I was just curious if there are any connections between them. It can be from any perspective, eigenvalues, rank, eigenbasis, simultaneous diagonalization or any such concept. Even special cases are welcome, for instance, say they are rank-one matrices, Does it make any difference?",,"['linear-algebra', 'matrices']"
64,"In terms of complexity, is there a quicker way of checking if a matrix is nonsingular than computing the determinant?","In terms of complexity, is there a quicker way of checking if a matrix is nonsingular than computing the determinant?",,"To repeat the question, let $A$ be a square matrix. We wish to determine if $A$ is nonsingular, that is, invertible. One way is compute its determinant and check if it is nonzero. However, if $A$ is invertible, calculating its determinant gives us strictly more information that knowing that it is nonzero. Although the naive complexity for calculating the determinant is $O(n!)$, faster $O(n^3)$ algorithms exist. It does not seem unreasonable to suppose that their might be an algorithm that checks for nonsingularity that has strictly lower complexity than the fastest algorithms known for computing the determinant. Is such an algorithm known to exist? Can such an algorithm exist? (I am quite ignorant in these areas of computational complexity) I am primarily interested in the case where $A$ is a real or complex matrix, but the case of rational matrices, or matrices over finite fields are also of interest. Feel free to discuss matrices over an arbitrary ring if you think it can clarify this discussion. Thanks in advance for any insight you can spare!","To repeat the question, let $A$ be a square matrix. We wish to determine if $A$ is nonsingular, that is, invertible. One way is compute its determinant and check if it is nonzero. However, if $A$ is invertible, calculating its determinant gives us strictly more information that knowing that it is nonzero. Although the naive complexity for calculating the determinant is $O(n!)$, faster $O(n^3)$ algorithms exist. It does not seem unreasonable to suppose that their might be an algorithm that checks for nonsingularity that has strictly lower complexity than the fastest algorithms known for computing the determinant. Is such an algorithm known to exist? Can such an algorithm exist? (I am quite ignorant in these areas of computational complexity) I am primarily interested in the case where $A$ is a real or complex matrix, but the case of rational matrices, or matrices over finite fields are also of interest. Feel free to discuss matrices over an arbitrary ring if you think it can clarify this discussion. Thanks in advance for any insight you can spare!",,"['linear-algebra', 'computational-complexity', 'numerical-linear-algebra']"
65,inner product and adjoint operator,inner product and adjoint operator,,"This is a problem I found in Schaum's Outlines: Linear Algebra, and I was wondering if someone knew how to solve it.  I began using integration by parts, but that approach did not lead to any conclusions. Let V be the space of all infinitely-differentiable functions on R which are periodic of period h>0  [i.e., f(x+h) = f(x) for all x in R].  Define an inner product on V by $$\langle f,g\rangle =\int_{-h}^hf(x)g(x)dx$$ Let $\alpha(f)=f'$.  Find $\alpha^*$. I know that the adjoint implies the relationship $\langle\alpha(f),g\rangle= \langle f,\alpha^*(g)\rangle$ . Thank you.","This is a problem I found in Schaum's Outlines: Linear Algebra, and I was wondering if someone knew how to solve it.  I began using integration by parts, but that approach did not lead to any conclusions. Let V be the space of all infinitely-differentiable functions on R which are periodic of period h>0  [i.e., f(x+h) = f(x) for all x in R].  Define an inner product on V by $$\langle f,g\rangle =\int_{-h}^hf(x)g(x)dx$$ Let $\alpha(f)=f'$.  Find $\alpha^*$. I know that the adjoint implies the relationship $\langle\alpha(f),g\rangle= \langle f,\alpha^*(g)\rangle$ . Thank you.",,"['linear-algebra', 'inner-products', 'adjoint-operators']"
66,Oblique projection matrix - proof definition is correct.,Oblique projection matrix - proof definition is correct.,,"The oblique projection matrix on $\text{range}(X)$ orthogonal to $\text{range}(Y)$ is given by $P = X (Y^\top X)^\dagger Y^\top$. Prove that the above definition is right, i.e. it holds that  $Xw^\star = Py$ where $w^\star = \text{argmin}_{w: (Xw - y)^\top Y = 0} \|Xw - y\|_2$. Edit : originally, the formula above erroneously had a min instead of an argmin. I do recognize this has the feel of very standard material, but for some reason I can't find it. It's OK if you could just name a reference. I don't know if this makes a huge difference, but I need the case where the matrix $Y^\top X$ need not be invertible (hence the pseudo-inverse). Ok, I do accept that calling the matrix $P$ an oblique projection requires that $\text{range}(X)$ and $\text{range}(Y)^\perp$ are complementary subspaces. Even given this, $Y^\top X$ may still not be invertible (consider $X=Y$ and $X$ has linearly dependent columns). Also, if we forget the geometry, it is still a legitimate question to ask, in terms of pure algebra, if $Xw^\star = Py$ for the given definition of $w^\star$, for any matrices $X$, $Y$. I am interested in this case because the question comes from an application where the matrices involved are estimated from samples and the task is to show that even when the inverse doesn't exist, the formula $X (Y^\top X)^\dagger Y^\top$ still solves the optimization as mentioned above. I have realized that the optimization given above may not be the obvious way to ask the question - the solution $X w^\star$ is already defined by just the fact that $(Xw^\star - y)^\top Y$ has to vanish, even without any optimization. Now, I'm trying to prove the following things suggested by Marc van Leeuwen below. $Pw = w$ for $w \in \text{range}(X)$. This is equivalent to saying $\forall b. PXb = Xb$, or $X(Y^\top X)^\dagger (X^\top Y) = X$. But where do I go from here? I can't just cancel the terms because $(Y^\top X)^\dagger (X^\top Y)$ isn't necessarily the identity. $Pw=0$ for $w \perp \text{range}(Y)$. This is obvious since $Y^\top w = 0$. Also, I don't see how 1 and 2 should imply the result.","The oblique projection matrix on $\text{range}(X)$ orthogonal to $\text{range}(Y)$ is given by $P = X (Y^\top X)^\dagger Y^\top$. Prove that the above definition is right, i.e. it holds that  $Xw^\star = Py$ where $w^\star = \text{argmin}_{w: (Xw - y)^\top Y = 0} \|Xw - y\|_2$. Edit : originally, the formula above erroneously had a min instead of an argmin. I do recognize this has the feel of very standard material, but for some reason I can't find it. It's OK if you could just name a reference. I don't know if this makes a huge difference, but I need the case where the matrix $Y^\top X$ need not be invertible (hence the pseudo-inverse). Ok, I do accept that calling the matrix $P$ an oblique projection requires that $\text{range}(X)$ and $\text{range}(Y)^\perp$ are complementary subspaces. Even given this, $Y^\top X$ may still not be invertible (consider $X=Y$ and $X$ has linearly dependent columns). Also, if we forget the geometry, it is still a legitimate question to ask, in terms of pure algebra, if $Xw^\star = Py$ for the given definition of $w^\star$, for any matrices $X$, $Y$. I am interested in this case because the question comes from an application where the matrices involved are estimated from samples and the task is to show that even when the inverse doesn't exist, the formula $X (Y^\top X)^\dagger Y^\top$ still solves the optimization as mentioned above. I have realized that the optimization given above may not be the obvious way to ask the question - the solution $X w^\star$ is already defined by just the fact that $(Xw^\star - y)^\top Y$ has to vanish, even without any optimization. Now, I'm trying to prove the following things suggested by Marc van Leeuwen below. $Pw = w$ for $w \in \text{range}(X)$. This is equivalent to saying $\forall b. PXb = Xb$, or $X(Y^\top X)^\dagger (X^\top Y) = X$. But where do I go from here? I can't just cancel the terms because $(Y^\top X)^\dagger (X^\top Y)$ isn't necessarily the identity. $Pw=0$ for $w \perp \text{range}(Y)$. This is obvious since $Y^\top w = 0$. Also, I don't see how 1 and 2 should imply the result.",,"['linear-algebra', 'matrices', 'projection-matrices']"
67,Finding solutions for a linear system of equations,Finding solutions for a linear system of equations,,"Got the following problem where I can't find a way to solve: Knowing $\begin{pmatrix}5\\ 3\\ 6\end{pmatrix}$ is the unique solution for the system  $Ax=\begin{pmatrix}2\\1\\1\end{pmatrix}$, with $A \in \mathbb{R}^{3\times3}$ and $B=\begin{pmatrix} 1 & 2 & 1 & 2 \\  1 & 0 & 4 & -1 \\  1 & 3 & -3 & 6 \end{pmatrix}$ Find all solutions for $ABx=\begin{pmatrix}2\\1\\1\end{pmatrix}$ What I've tried: The problem says that $Ax=b$ got unique solution, so I've tried by getting rid of $A$ by using the inverse matrix but it doesn't work sice I don't know $A$. Since the constant matrix is $\begin{pmatrix}2\\1\\1\end{pmatrix}$ for both systems, I've tried $ABx = Ax$ but that also doesn't work for me. Thanks in advance for your help and sorry for my bad English. Lucas","Got the following problem where I can't find a way to solve: Knowing $\begin{pmatrix}5\\ 3\\ 6\end{pmatrix}$ is the unique solution for the system  $Ax=\begin{pmatrix}2\\1\\1\end{pmatrix}$, with $A \in \mathbb{R}^{3\times3}$ and $B=\begin{pmatrix} 1 & 2 & 1 & 2 \\  1 & 0 & 4 & -1 \\  1 & 3 & -3 & 6 \end{pmatrix}$ Find all solutions for $ABx=\begin{pmatrix}2\\1\\1\end{pmatrix}$ What I've tried: The problem says that $Ax=b$ got unique solution, so I've tried by getting rid of $A$ by using the inverse matrix but it doesn't work sice I don't know $A$. Since the constant matrix is $\begin{pmatrix}2\\1\\1\end{pmatrix}$ for both systems, I've tried $ABx = Ax$ but that also doesn't work for me. Thanks in advance for your help and sorry for my bad English. Lucas",,['linear-algebra']
68,Solving for specific entries in a Lyapunov Equation,Solving for specific entries in a Lyapunov Equation,,"Let $A$ be a $2n\times 2n$ real matrix with the following structure  \begin{equation} A = \left(\begin{matrix} 0 & -I \\ K & S \end{matrix}\right) \end{equation} with all sub-matrices of size $n\times n$: $I$ is the identity matrix, $K$ is symmetric positive definite and $S$ is diagonal but singular. I am interested in the (numerical) solution of the  continuous time Lyapunov equation for the $2n\times 2n$ matrix R: \begin{equation}  R A^\text{T} + A R = \left(\begin{matrix} 0 & 0 \\ 0 & \Gamma \end{matrix}\right) \end{equation} where $\Gamma$ is a diagonal and singular $n\times n$ real matrix. However, I only need a few elements of $R$. More specifically, writing \begin{equation} R = \left(\begin{matrix} X & C \\ C^\text{T} & V \end{matrix}\right) \end{equation} all I really want are the diagonal entries of $V$ (also $n\times n$). Is there anyway I could reduce this problem to some other (probably Sylvester) equation for $V$ or, better yet, only it's diagonal? I don't really know how to approach this problem.","Let $A$ be a $2n\times 2n$ real matrix with the following structure  \begin{equation} A = \left(\begin{matrix} 0 & -I \\ K & S \end{matrix}\right) \end{equation} with all sub-matrices of size $n\times n$: $I$ is the identity matrix, $K$ is symmetric positive definite and $S$ is diagonal but singular. I am interested in the (numerical) solution of the  continuous time Lyapunov equation for the $2n\times 2n$ matrix R: \begin{equation}  R A^\text{T} + A R = \left(\begin{matrix} 0 & 0 \\ 0 & \Gamma \end{matrix}\right) \end{equation} where $\Gamma$ is a diagonal and singular $n\times n$ real matrix. However, I only need a few elements of $R$. More specifically, writing \begin{equation} R = \left(\begin{matrix} X & C \\ C^\text{T} & V \end{matrix}\right) \end{equation} all I really want are the diagonal entries of $V$ (also $n\times n$). Is there anyway I could reduce this problem to some other (probably Sylvester) equation for $V$ or, better yet, only it's diagonal? I don't really know how to approach this problem.",,"['linear-algebra', 'numerical-linear-algebra', 'control-theory']"
69,Block Determinants,Block Determinants,,"This is a nice question I recently found in Golan's book. Problem: Let $A,B,C,D$ be $n\times n$ matrices over $\mathbb{R}$ with $n\ge 2$, and let $M$ be the $2n\times 2n$ matrix \begin{bmatrix}   A & B \\   C & D\\  \end{bmatrix} If all of the ""formal determinants"" $AD-BC$, $AD-CB$, $DA-CB$, and $DA-BC$ are nonsingular, is $M$ necessarily nonsingular? If $M$ is nonsingular, must all of the formal determinants also be nonsingular?","This is a nice question I recently found in Golan's book. Problem: Let $A,B,C,D$ be $n\times n$ matrices over $\mathbb{R}$ with $n\ge 2$, and let $M$ be the $2n\times 2n$ matrix \begin{bmatrix}   A & B \\   C & D\\  \end{bmatrix} If all of the ""formal determinants"" $AD-BC$, $AD-CB$, $DA-CB$, and $DA-BC$ are nonsingular, is $M$ necessarily nonsingular? If $M$ is nonsingular, must all of the formal determinants also be nonsingular?",,"['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
70,Eigenvalues of block matrices,Eigenvalues of block matrices,,"Let $K$ be a field of characteristic 0, and consider the following block matrix $$M=\left(\begin{array}{cc} A & B\\ -B&D\end{array}\right),$$ where each block is an $n\times n$ matrix with coefficients in $K$. I am looking for a relation between the eigenvalues of $M$ and those of $A$ and $D$. Context : Here, I'm assuming that $M$ is invertible and semisimple. I was wondering if there is a way to show that both $A$ and $D$ are invertible and semisimple as well. Moreover, we can also assume that $B$ is $2\times2$ and has one of the following forms: $$\left\{\left(\begin{array}{cc} 1 & 0\\ 0&1\end{array}\right),\left(\begin{array}{cc} 1 & 0\\ 0&0\end{array}\right),\left(\begin{array}{cc} 0 & 0\\ 0&0\end{array}\right)\right\}.$$","Let $K$ be a field of characteristic 0, and consider the following block matrix $$M=\left(\begin{array}{cc} A & B\\ -B&D\end{array}\right),$$ where each block is an $n\times n$ matrix with coefficients in $K$. I am looking for a relation between the eigenvalues of $M$ and those of $A$ and $D$. Context : Here, I'm assuming that $M$ is invertible and semisimple. I was wondering if there is a way to show that both $A$ and $D$ are invertible and semisimple as well. Moreover, we can also assume that $B$ is $2\times2$ and has one of the following forms: $$\left\{\left(\begin{array}{cc} 1 & 0\\ 0&1\end{array}\right),\left(\begin{array}{cc} 1 & 0\\ 0&0\end{array}\right),\left(\begin{array}{cc} 0 & 0\\ 0&0\end{array}\right)\right\}.$$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
71,Kernel of the Tensor Product of a Linear Map with Itself,Kernel of the Tensor Product of a Linear Map with Itself,,"For two vector spaces, $V$ and $W$, and a map $f: V \to W$, it is clear that:  $$ \ker(f) \otimes V + V \otimes \ker(f) \subseteq \ker(f \otimes f). $$ Does the opposite inclusion hold? If so, I'd like a proof, and if not, a counterexample. Bascially, given an element an element $\sum_i a_i \otimes b_i \in V \otimes V$, for which it holds that  $$ \sum_i f(a_i) \otimes f(b_i) = 0 $$ can we show that $\sum_i a_i \otimes b_i \in \ker(f) \otimes V + V \otimes  \ker(f)$?","For two vector spaces, $V$ and $W$, and a map $f: V \to W$, it is clear that:  $$ \ker(f) \otimes V + V \otimes \ker(f) \subseteq \ker(f \otimes f). $$ Does the opposite inclusion hold? If so, I'd like a proof, and if not, a counterexample. Bascially, given an element an element $\sum_i a_i \otimes b_i \in V \otimes V$, for which it holds that  $$ \sum_i f(a_i) \otimes f(b_i) = 0 $$ can we show that $\sum_i a_i \otimes b_i \in \ker(f) \otimes V + V \otimes  \ker(f)$?",,"['linear-algebra', 'tensor-products', 'multilinear-algebra']"
72,"Torsion subgroup of $SL(n,\mathbb Z)$",Torsion subgroup of,"SL(n,\mathbb Z)","Let $G$ be subgroup of $SL(n,Z)$ such that for any $g\in G$ there exists integer $m\geq1$ $g^m=1$. Show that there exists $N\geq1$ such that for any $g\in G$ , $g^N=1$ I know $m$-th root of unity is the eigenvalue of elements, any element is diagonalizable matrix over complex number but I don't know how to use facts? any suggestion?","Let $G$ be subgroup of $SL(n,Z)$ such that for any $g\in G$ there exists integer $m\geq1$ $g^m=1$. Show that there exists $N\geq1$ such that for any $g\in G$ , $g^N=1$ I know $m$-th root of unity is the eigenvalue of elements, any element is diagonalizable matrix over complex number but I don't know how to use facts? any suggestion?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'matrices']"
73,The use of Gershgorin Circle Theorem,The use of Gershgorin Circle Theorem,,"In order to estimate the eigenvalues of a real symmetric $n\times n$ matrix, I intend to use the Gershgorin Circle Theorem. Unfortunately, the examples one might find on the internet are a bit confusing; What would be the mathematical formula for deriving the eigenvalue estimates? I understand that certain disks are formed, each centered at the diagonal entry, with the radius equal to the summation of absolute values of the associated off-diagonal row entries. (The example from http://en.wikipedia.org/wiki/Gershgorin_circle_theorem is clear with the disks, but not with the eigenvalue estimation) Which steps to take from this point to get the estimates on the eigenvalues? From Theorem 2.1 in http://buzzard.ups.edu/courses/2007spring/projects/brakkenthal-paper.pdf one could understand the eigenvalue $ranges$, but the example 2.3 from the above paper gives concrete eigenvalue estimates (some of which are negative). I would appreciate if someone explains this.","In order to estimate the eigenvalues of a real symmetric $n\times n$ matrix, I intend to use the Gershgorin Circle Theorem. Unfortunately, the examples one might find on the internet are a bit confusing; What would be the mathematical formula for deriving the eigenvalue estimates? I understand that certain disks are formed, each centered at the diagonal entry, with the radius equal to the summation of absolute values of the associated off-diagonal row entries. (The example from http://en.wikipedia.org/wiki/Gershgorin_circle_theorem is clear with the disks, but not with the eigenvalue estimation) Which steps to take from this point to get the estimates on the eigenvalues? From Theorem 2.1 in http://buzzard.ups.edu/courses/2007spring/projects/brakkenthal-paper.pdf one could understand the eigenvalue $ranges$, but the example 2.3 from the above paper gives concrete eigenvalue estimates (some of which are negative). I would appreciate if someone explains this.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'gershgorin-sets']"
74,Vector dimension of a set of functions,Vector dimension of a set of functions,,"Let $F$ be a field and $S$ an infinite set. Set $V=\{f:S \rightarrow F\}$ endowed with the vector space structure that results from the pointwise operations of $F$. It is easy to prove that $|S| \leq \dim V$, since the functions $\delta_s$ defined by $\delta_s(s)=1$ and $\delta_s(t)=0$ for all $t \neq s$ are linearly independent. Is it true that $|S| < \dim V$? If so how could it be proved? I tried some kind of Cantor-like diagonal argument, but nothing worked so far.","Let $F$ be a field and $S$ an infinite set. Set $V=\{f:S \rightarrow F\}$ endowed with the vector space structure that results from the pointwise operations of $F$. It is easy to prove that $|S| \leq \dim V$, since the functions $\delta_s$ defined by $\delta_s(s)=1$ and $\delta_s(t)=0$ for all $t \neq s$ are linearly independent. Is it true that $|S| < \dim V$? If so how could it be proved? I tried some kind of Cantor-like diagonal argument, but nothing worked so far.",,"['linear-algebra', 'vector-spaces']"
75,Full-rank condition for product of two matrices,Full-rank condition for product of two matrices,,"Given two matrices $A_{m\times n}$ and $B_{n\times p}$, what is the sufficient and necessary condition for $AB$ to have full rank? I know $r(AB)=r(B)-\dim N(A) \cap R(B)$, so is it true the above iff $\dim N(A)\cap R(B)=0$? But seems incorrect if $m\lt p$. Please help.","Given two matrices $A_{m\times n}$ and $B_{n\times p}$, what is the sufficient and necessary condition for $AB$ to have full rank? I know $r(AB)=r(B)-\dim N(A) \cap R(B)$, so is it true the above iff $\dim N(A)\cap R(B)=0$? But seems incorrect if $m\lt p$. Please help.",,"['linear-algebra', 'matrices']"
76,Bases of spaces of continuous functions,Bases of spaces of continuous functions,,"What would the basis of a space of continuous functions defined over a closed interval $[a,b]$ be? Also, what would the basis for a similar space with the additional constraints that $f$ is continuously differentiable and $f(a) = 0$? I reckon these 2 spaces are not isomorphic... Is anything in $\mathbb R^N$ potentially isomorphic to the continuous functions? Thanks. Added: If the question above is not well-defined, perhaps a more explicit question might be, is the set of continuous functions on [a,b] isomorphic to $\mathbb R^n$ for some $n\in \mathbb N$? Or more generally, if given a vector space, how do I determine whether this set is isomorphic to the set of continuous functions on [a,b] ? Thanks again.","What would the basis of a space of continuous functions defined over a closed interval $[a,b]$ be? Also, what would the basis for a similar space with the additional constraints that $f$ is continuously differentiable and $f(a) = 0$? I reckon these 2 spaces are not isomorphic... Is anything in $\mathbb R^N$ potentially isomorphic to the continuous functions? Thanks. Added: If the question above is not well-defined, perhaps a more explicit question might be, is the set of continuous functions on [a,b] isomorphic to $\mathbb R^n$ for some $n\in \mathbb N$? Or more generally, if given a vector space, how do I determine whether this set is isomorphic to the set of continuous functions on [a,b] ? Thanks again.",,"['linear-algebra', 'functional-analysis']"
77,Similarity between special matrices and special complex numbers,Similarity between special matrices and special complex numbers,,"From Wikipedia : It is occasionally useful (but sometimes misleading) to think of the   relationships of different kinds of normal matrices as analogous to   the relationships between different kinds of complex numbers: Invertible matrices are analogous to non-zero complex numbers Unitary matrices are analogous to complex numbers whose absolute   value is 1 Hermitian matrices are analogous to real numbers Hermitian positive definite matrices are analogous to positive real   numbers Skew Hermitian matrices are analogous to purely imaginary numbers I also happen to see that the number analogy of each kind of special matrices also agree with their eigenvalues: eigenvalues of invertible matrices are non-zero complex numbers eigenvalues of Unitary matrices are complex numbers whose absolute value is 1 eigenvalues of Hermitian matrices are real numbers eigenvalues of Hermitian positive definite matrices are positive real numbers eigenvalues of Skew Hermitian matrices are purely imaginary numbers I wonder if the relation between the eigenvalues of special matrices and their number analogy is just coincidence, or there are something inherent, fundamental and more than analogy? if there are other ways than eigenvalues by which the special matrices and special numbers are similar to each other? (I actually don't quite understand what Wikipedia means by analogy. The example in terms of eigenvalues is just my guess. The author may have other things in mind and there may be other possibilities.) if there are other kinds of special matrices not mentioned in the list have similar analogy to special numbers? (Hermitian negative definite is too trivial to mention.) if there are some relevant references? (It seems that this kind of analogy is mentioned in Halmos ""Linear Algebra"", but cannot find where it is.) Thanks and regards!","From Wikipedia : It is occasionally useful (but sometimes misleading) to think of the   relationships of different kinds of normal matrices as analogous to   the relationships between different kinds of complex numbers: Invertible matrices are analogous to non-zero complex numbers Unitary matrices are analogous to complex numbers whose absolute   value is 1 Hermitian matrices are analogous to real numbers Hermitian positive definite matrices are analogous to positive real   numbers Skew Hermitian matrices are analogous to purely imaginary numbers I also happen to see that the number analogy of each kind of special matrices also agree with their eigenvalues: eigenvalues of invertible matrices are non-zero complex numbers eigenvalues of Unitary matrices are complex numbers whose absolute value is 1 eigenvalues of Hermitian matrices are real numbers eigenvalues of Hermitian positive definite matrices are positive real numbers eigenvalues of Skew Hermitian matrices are purely imaginary numbers I wonder if the relation between the eigenvalues of special matrices and their number analogy is just coincidence, or there are something inherent, fundamental and more than analogy? if there are other ways than eigenvalues by which the special matrices and special numbers are similar to each other? (I actually don't quite understand what Wikipedia means by analogy. The example in terms of eigenvalues is just my guess. The author may have other things in mind and there may be other possibilities.) if there are other kinds of special matrices not mentioned in the list have similar analogy to special numbers? (Hermitian negative definite is too trivial to mention.) if there are some relevant references? (It seems that this kind of analogy is mentioned in Halmos ""Linear Algebra"", but cannot find where it is.) Thanks and regards!",,"['linear-algebra', 'matrices']"
78,"Prove that the matrix $(I-xx^\top)(I-3 \mathrm{Diag}(xx^\top))(I-xx^\top)$ has at most one negative eigenvalue, for $x$ a unit vector","Prove that the matrix  has at most one negative eigenvalue, for  a unit vector",(I-xx^\top)(I-3 \mathrm{Diag}(xx^\top))(I-xx^\top) x,"Let $x \in \mathbb{R}^n$ be a unit vector, i.e., $\|x\|=1$ .  Show that the following matrix $A$ has at most one negative eigenvalue: $$A:=(I-xx^\top)(I-3 \mathrm{Diag}(xx^\top))(I-xx^\top).$$ $\mathrm{Diag}(xx^\top)$ is the diagonal matrix whose $i$ -th diagonal entry is $x_i^2$ .  ( $\mathrm{Diag}$ extracts the diagonal of a matrix.) Some of my thoughts/attempts: The inner matrix $I-3 \mathrm{Diag}(xx^\top)$ clearly has at most 2 negative eigenvalues, as $\|x\|^2=1$ . The matrix $(I-xx^\top)$ is of course the orthogonal projector to the orthogonal complement of $x$ .  So we can add anything of the form $z x^\top + x z^\top$ to the inner matrix without changing $A$ . I believe it suffices to show that there is some $y \in \mathbb{R}^n$ such that $A + y y^\top \succeq 0$ -- see this MSE post .  So it suffices to show that there exists a $y$ such that $$z^\top (I-3 \mathrm{Diag}(xx^\top) + y y^\top) z \geq 0$$ for all $z$ orthogonal to $x$ . Maybe we can somehow appeal to Sylvester's law of inertia -- also not clear though.","Let be a unit vector, i.e., .  Show that the following matrix has at most one negative eigenvalue: is the diagonal matrix whose -th diagonal entry is .  ( extracts the diagonal of a matrix.) Some of my thoughts/attempts: The inner matrix clearly has at most 2 negative eigenvalues, as . The matrix is of course the orthogonal projector to the orthogonal complement of .  So we can add anything of the form to the inner matrix without changing . I believe it suffices to show that there is some such that -- see this MSE post .  So it suffices to show that there exists a such that for all orthogonal to . Maybe we can somehow appeal to Sylvester's law of inertia -- also not clear though.",x \in \mathbb{R}^n \|x\|=1 A A:=(I-xx^\top)(I-3 \mathrm{Diag}(xx^\top))(I-xx^\top). \mathrm{Diag}(xx^\top) i x_i^2 \mathrm{Diag} I-3 \mathrm{Diag}(xx^\top) \|x\|^2=1 (I-xx^\top) x z x^\top + x z^\top A y \in \mathbb{R}^n A + y y^\top \succeq 0 y z^\top (I-3 \mathrm{Diag}(xx^\top) + y y^\top) z \geq 0 z x,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-semidefinite']"
79,How to determine if a subspace of $ \mathbb{R}^n $ has an integer basis,How to determine if a subspace of  has an integer basis, \mathbb{R}^n ,"Let $ W $ be a sub vector space of $ \mathbb{R}^n $ . How can we determine if $ W $ admits an integer basis? This is equivalent to asking how to determine if $ W \cap \mathbb{Z}^n $ spans $ W $ . Obviously if $ W=\mathbb{R}^n $ then there is always an integer basis. For example the standard basis $ e_1,\dots e_n $ , just the columns of the identity matrix. $ W $ a $ 1 $ dimensional subspace of $ \mathbb{R}^2 $ is the first nontrivial case. In this case $$ W=Span \{ (a,b) \}  $$ and $ W $ has an integer basis if and only if $ a/b $ or $ b/a $ (check both in case $ a $ or $ b $ is $ 0 $ ) is rational. Is there some general criterion that allows us to look at certain spanning sets for $ W $ and perhaps look at ratios of coefficients like this to solve the problem? Update: Just to reiterate the correct answer given below, to determine if an integer basis exists just take any set of $ k $ vectors spanning $ W $ , put them as the rows of a matrix, then find RREF of that matrix and then $ W $ has an integer basis if and only if that RREF has all rational entries.","Let be a sub vector space of . How can we determine if admits an integer basis? This is equivalent to asking how to determine if spans . Obviously if then there is always an integer basis. For example the standard basis , just the columns of the identity matrix. a dimensional subspace of is the first nontrivial case. In this case and has an integer basis if and only if or (check both in case or is ) is rational. Is there some general criterion that allows us to look at certain spanning sets for and perhaps look at ratios of coefficients like this to solve the problem? Update: Just to reiterate the correct answer given below, to determine if an integer basis exists just take any set of vectors spanning , put them as the rows of a matrix, then find RREF of that matrix and then has an integer basis if and only if that RREF has all rational entries."," W   \mathbb{R}^n   W   W \cap \mathbb{Z}^n   W   W=\mathbb{R}^n   e_1,\dots e_n   W   1   \mathbb{R}^2  
W=Span \{ (a,b) \} 
  W   a/b   b/a   a   b   0   W   k   W   W ",['linear-algebra']
80,Prove $\frac{Ax}{ \Vert Ax \Vert}$ is a contraction mapping,Prove  is a contraction mapping,\frac{Ax}{ \Vert Ax \Vert},"I am going through a proof of the Perron-Frobenius theorem which uses the Banach fixed point theorem. The author first asks the reader to consider the space $$ X = \left\{ x \in  \mathbb{R}^{d} : \Vert x\Vert^{2} = 1 \text{ and } x_{i} \geq 0 \right\} $$ Then consider the matrix $A$ with strictly positive entries. We define the map: $$ T(x) = \frac{Ax}{\Vert Ax \Vert} $$ The authors then argue that, since T is a contraction mapping with respect to the geodesic sphere distance, the Banach fixed point Theorem can be applied. This results in a unique fixed point satisfying the conditions of the Theorem. That is, there exists only one eigenvector with strictly positive entries (Proving that the corresponding eigenvalue is maximal is done in the latter part of the proof). I understand the overall idea of the proof but am struggling to work out why $T$ is a contraction mapping. The spherical distance is given by: $$ d(x, y) = \cos^{-1}(x^{\top} y) $$ Thus we need to prove that $$ \cos^{-1}\left(\frac{x^{\top}A^{T}Ay}{\Vert Ax\Vert \Vert Ay \Vert}\right) \leq k\cos^{-1}(x^\top y) $$ for some $0 < k < 1$ . One observation I tried using is that $\cos^{-1}$ monotonically decreasing on the domain $[-1, 1]$ . Thus being able to show $$ \cos^{-1}\left(\frac{x^{\top}A^{T}Ay}{\Vert Ax\Vert \Vert Ay \Vert}\right) \leq \cos^{-1}(x^\top y) $$ is equivalent to showing that $$ \frac{x^{\top}A^{T}Ay}{\Vert Ax\Vert \Vert Ay \Vert} \geq x^\top y $$ My plan was then to use basic facts about operator norms to prove this inequality but I couldn't make any progress. EDIT: This question is based on the following lecture notes .","I am going through a proof of the Perron-Frobenius theorem which uses the Banach fixed point theorem. The author first asks the reader to consider the space Then consider the matrix with strictly positive entries. We define the map: The authors then argue that, since T is a contraction mapping with respect to the geodesic sphere distance, the Banach fixed point Theorem can be applied. This results in a unique fixed point satisfying the conditions of the Theorem. That is, there exists only one eigenvector with strictly positive entries (Proving that the corresponding eigenvalue is maximal is done in the latter part of the proof). I understand the overall idea of the proof but am struggling to work out why is a contraction mapping. The spherical distance is given by: Thus we need to prove that for some . One observation I tried using is that monotonically decreasing on the domain . Thus being able to show is equivalent to showing that My plan was then to use basic facts about operator norms to prove this inequality but I couldn't make any progress. EDIT: This question is based on the following lecture notes .","
X = \left\{ x \in  \mathbb{R}^{d} : \Vert x\Vert^{2} = 1 \text{ and } x_{i} \geq 0 \right\}
 A 
T(x) = \frac{Ax}{\Vert Ax \Vert}
 T 
d(x, y) = \cos^{-1}(x^{\top} y)
 
\cos^{-1}\left(\frac{x^{\top}A^{T}Ay}{\Vert Ax\Vert \Vert Ay \Vert}\right) \leq k\cos^{-1}(x^\top y)
 0 < k < 1 \cos^{-1} [-1, 1] 
\cos^{-1}\left(\frac{x^{\top}A^{T}Ay}{\Vert Ax\Vert \Vert Ay \Vert}\right) \leq \cos^{-1}(x^\top y)
 
\frac{x^{\top}A^{T}Ay}{\Vert Ax\Vert \Vert Ay \Vert} \geq x^\top y
","['linear-algebra', 'functional-analysis']"
81,Connected components of $\{A \in M_n(\mathbb{R}) \mid A^2 \neq 0\}$,Connected components of,\{A \in M_n(\mathbb{R}) \mid A^2 \neq 0\},How many connected components does the space $\{A \in M_n(\mathbb{R}) \mid A^2 \neq 0\}$ have? I tried writing out the conditions that arise from $A^2 \neq 0$ explicitly for a general $A \in M_n(\mathbb{R})$ however I do not understand how the surfaces that they generate intersect in the space.,How many connected components does the space have? I tried writing out the conditions that arise from explicitly for a general however I do not understand how the surfaces that they generate intersect in the space.,\{A \in M_n(\mathbb{R}) \mid A^2 \neq 0\} A^2 \neq 0 A \in M_n(\mathbb{R}),"['linear-algebra', 'general-topology']"
82,"if $ A^2 = 2I $, then $ A + I, A - I $ are invertible","if , then  are invertible"," A^2 = 2I   A + I, A - I ","Let $ A $ be a matrix of order $ n \times n $ and assume $ A^2 = 2I $ . I want to prove than $ A + I $ and $ A - I $ are invertible. Is my proof valid? $ A^2 = 2I \Rightarrow A^2 - I = I => (A - I)(A + I) = I $ Therefore, $ A - I, A + I  $ are invertible","Let be a matrix of order and assume . I want to prove than and are invertible. Is my proof valid? Therefore, are invertible"," A   n \times n   A^2 = 2I   A + I   A - I   A^2 = 2I \Rightarrow A^2 - I = I => (A - I)(A + I) = I   A - I, A + I  ","['linear-algebra', 'matrices', 'solution-verification']"
83,Eigenvalue bounded by sum of row and column,Eigenvalue bounded by sum of row and column,,"Let $A =(a_{ij})$ be a matrix and $R_i=\sum_j |a_{ij}|$ and $C_j=\sum_i |a_{ij}|$ Let $\lambda$ be an eigen value of $A$ . Prove that there exists a $k$ such that $$|\lambda| \leq \sqrt{R_k C_k}$$ Any hints? Edit: What I tried so far we have $AX=\lambda X$ where $||X||=  X^tX=1$ $$|\lambda|^2 = X^tA^tAX = \sum_i\sum_j x_i(A^tA)_{i,j}x_j = \sum_p\sum_ix_ia_{ip}\sum_jx_ja_{pj} \leq \sum_p\sqrt{\sum_i|x_i|^2|a_{ip}|}\sqrt{R_p}\sqrt{\sum_j|x_j|^2|a_{pj}|}\sqrt{C_p} \text{     (Cauchy schwartz)}$$ Continuing: Let $R_kC_k=\max_p{R_pL_p}$ $$\begin{align} |\lambda|^2 &\leq \sqrt{R_kC_k} \sum_p\sqrt{\sum_i|x_i|^2|a_{ip}|}\sqrt{\sum_j|x_j|^2|a_{pj}|} \\  &\leq \sqrt{R_kC_k}\sqrt{\sum_p\sum_i|x_i|^2|a_{ip}|\sum_p\sum_j|x_j|^2|a_{pj}|}\\ &=\sqrt{R_kC_k}\sqrt{\sum_i|x_i|^2\sum_p|a_{ip}|\sum_j|x_j|^2\sum_p|a_{pj}|} \\ &\leq... \end{align}$$ I'm still stuck here...",Let be a matrix and and Let be an eigen value of . Prove that there exists a such that Any hints? Edit: What I tried so far we have where Continuing: Let I'm still stuck here...,"A =(a_{ij}) R_i=\sum_j |a_{ij}| C_j=\sum_i |a_{ij}| \lambda A k |\lambda| \leq \sqrt{R_k C_k} AX=\lambda X ||X||=  X^tX=1 |\lambda|^2 = X^tA^tAX = \sum_i\sum_j x_i(A^tA)_{i,j}x_j = \sum_p\sum_ix_ia_{ip}\sum_jx_ja_{pj} \leq \sum_p\sqrt{\sum_i|x_i|^2|a_{ip}|}\sqrt{R_p}\sqrt{\sum_j|x_j|^2|a_{pj}|}\sqrt{C_p} \text{     (Cauchy schwartz)} R_kC_k=\max_p{R_pL_p} \begin{align} |\lambda|^2 &\leq \sqrt{R_kC_k} \sum_p\sqrt{\sum_i|x_i|^2|a_{ip}|}\sqrt{\sum_j|x_j|^2|a_{pj}|} \\
 &\leq \sqrt{R_kC_k}\sqrt{\sum_p\sum_i|x_i|^2|a_{ip}|\sum_p\sum_j|x_j|^2|a_{pj}|}\\ &=\sqrt{R_kC_k}\sqrt{\sum_i|x_i|^2\sum_p|a_{ip}|\sum_j|x_j|^2\sum_p|a_{pj}|} \\
&\leq...
\end{align}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
84,Is every Zariski-closed real matrix Lie group the joint stabilizer of some list of mixed tensors?,Is every Zariski-closed real matrix Lie group the joint stabilizer of some list of mixed tensors?,,"Consider a finite-dimensional real vector space $V$ , and an embedded real Lie subgroup $G \subset \mathrm{GL}_\mathbb{R}(V)$ . In what follows, $V^*$ denotes the real dual vector space of $V$ . Def: Let us say $G$ has property $\mathcal{P}$ if there exist some finitely many $\alpha_1,\ldots,\alpha_k$ in the mixed real tensor algebra $TV$ (including tensor factors of both $V$ and $V^*$ ) of $V$ , such that $G= \bigcap_{j=1}^{k}\mathrm{Stab}_{\mathrm{GL}_{\mathbb{R}}(V)}(\alpha_j)$ . Here we use the induced action of $\mathrm{GL}(V)$ on the mixed tensor algebra $TV$ . My question is whether every Zariski-closed Lie subgroup of $\mathrm{GL}_{\mathbb{R}}(V)$ has property $\mathcal{P}$ ? (Here we use the Zariski topology on $\mathrm{End}_{\mathbb{R}}(V)$ coming from real polynomials, e.g. from a choice of basis, or just in the multilinear sense.) Examples having property $\mathcal{P}$ : For $V = \mathbb{R}^n$ , for $G = \mathrm{O}(n,\mathbb{R})$ , we can use $\alpha \in V^* \otimes V^*$ given by the standard real-bilinear inner product. For $V = \mathbb{R}^n$ , for $G = \mathrm{SL}(n,\mathbb{R})$ , we can use $\alpha \in V^{\otimes n}$ the standard volume form. For $V = \mathbb{R}^{2n}$ , for $G = \mathrm{GL}(n,\mathbb{C}) \subset \mathrm{GL}(2n,\mathbb{R})$ , we can use $\alpha = J = \left(\begin{smallmatrix} 0 & -1 \\ 1 & 0 \end{smallmatrix}\right)  \in V \otimes_\mathbb{R} V^*$ . For $V = \mathbb{R}^{2n}$ , for $G = U(n)$ , if I'm not mistaken, we can use both $\alpha = J = \left(\begin{smallmatrix} 0 & -1 \\ 1 & 0 \end{smallmatrix}\right)  \in V \otimes_\mathbb{R} V^*$ , and the $\beta_1,\beta_2 \in V^* \otimes_\mathbb{R} V^* $ which correspond to the real and imaginary parts of the standard Hermitian inner product $\mathbb{C}^{n} \times \mathbb{C}^{n} \rightarrow \mathbb{C}$ but treated as a real-bilinear map. For $V = \mathbb{R}^n$ and $G = \mathbb{R}^\times 1 \subset \mathrm{GL}(n,\mathbb{R})$ the nonzero multiples of the identity, we should be able to take any $\alpha_1,\ldots,\alpha_{n^2} \in V \otimes_\mathbb{R} V^* \simeq \mathrm{M}(n,\mathbb{R})$ such that the $\alpha$ 's form an $\mathbb{R}$ -linear basis of $\mathrm{M}(n,\mathbb{R})$ . (This uses the fact that only the scalar multiples of the identity commute with all other square matrices.) Lemma: If $G$ has property $\mathcal{P}$ , then $G \subset \mathrm{GL}_{\mathbb{R}}(V)$ is Zariski-closed; i.e. it is the intersection of $\mathrm{GL}_{\mathbb{R}}(V)$ with a Zariski-closed subset (a real polynomial-vanishing subset) of $\mathrm{End}_{\mathbb{R}}(V)$ . Proof: After picking bases, each ""stabilizing equation"" $g \cdot \alpha_j = \alpha_j$ can be converted into a polynomial equation in the entries of $g$ and $g^{-1}$ , and the $g^{-1}$ factors can be multiplied out to get a polynomial in just $g$ 's entries; thus $G$ is the vanishing set in $\mathrm{GL}_{\mathbb{R}}(V)$ of a collection of real polynomials of $\mathrm{End}_{\mathbb{R}}(V)$ . Rmk: For instance, as pointed out in the comments below, $\mathrm{GL}_+(n,\mathbb{R})$ is not Zariski-closed in $\mathrm{GL}_(n,\mathbb{R})$ , hence $\mathrm{GL}_+(n,\mathbb{R})$ cannot have property $\mathcal{P}$ . My question is: does every Zariski-closed, real Lie subgroup of $\mathrm{GL}_{\mathbb{R}}(V)$ have property $\mathcal{P}$ ? (I've updated the question to reflect the suggestions noticing that every $G$ having property $\mathcal{P}$ is Zariski-closed; now I'm wondering about the converse.)","Consider a finite-dimensional real vector space , and an embedded real Lie subgroup . In what follows, denotes the real dual vector space of . Def: Let us say has property if there exist some finitely many in the mixed real tensor algebra (including tensor factors of both and ) of , such that . Here we use the induced action of on the mixed tensor algebra . My question is whether every Zariski-closed Lie subgroup of has property ? (Here we use the Zariski topology on coming from real polynomials, e.g. from a choice of basis, or just in the multilinear sense.) Examples having property : For , for , we can use given by the standard real-bilinear inner product. For , for , we can use the standard volume form. For , for , we can use . For , for , if I'm not mistaken, we can use both , and the which correspond to the real and imaginary parts of the standard Hermitian inner product but treated as a real-bilinear map. For and the nonzero multiples of the identity, we should be able to take any such that the 's form an -linear basis of . (This uses the fact that only the scalar multiples of the identity commute with all other square matrices.) Lemma: If has property , then is Zariski-closed; i.e. it is the intersection of with a Zariski-closed subset (a real polynomial-vanishing subset) of . Proof: After picking bases, each ""stabilizing equation"" can be converted into a polynomial equation in the entries of and , and the factors can be multiplied out to get a polynomial in just 's entries; thus is the vanishing set in of a collection of real polynomials of . Rmk: For instance, as pointed out in the comments below, is not Zariski-closed in , hence cannot have property . My question is: does every Zariski-closed, real Lie subgroup of have property ? (I've updated the question to reflect the suggestions noticing that every having property is Zariski-closed; now I'm wondering about the converse.)","V G \subset \mathrm{GL}_\mathbb{R}(V) V^* V G \mathcal{P} \alpha_1,\ldots,\alpha_k TV V V^* V G= \bigcap_{j=1}^{k}\mathrm{Stab}_{\mathrm{GL}_{\mathbb{R}}(V)}(\alpha_j) \mathrm{GL}(V) TV \mathrm{GL}_{\mathbb{R}}(V) \mathcal{P} \mathrm{End}_{\mathbb{R}}(V) \mathcal{P} V = \mathbb{R}^n G = \mathrm{O}(n,\mathbb{R}) \alpha \in V^* \otimes V^* V = \mathbb{R}^n G = \mathrm{SL}(n,\mathbb{R}) \alpha \in V^{\otimes n} V = \mathbb{R}^{2n} G = \mathrm{GL}(n,\mathbb{C}) \subset \mathrm{GL}(2n,\mathbb{R}) \alpha = J = \left(\begin{smallmatrix} 0 & -1 \\ 1 & 0 \end{smallmatrix}\right)  \in V \otimes_\mathbb{R} V^* V = \mathbb{R}^{2n} G = U(n) \alpha = J = \left(\begin{smallmatrix} 0 & -1 \\ 1 & 0 \end{smallmatrix}\right)  \in V \otimes_\mathbb{R} V^* \beta_1,\beta_2 \in V^* \otimes_\mathbb{R} V^*
 \mathbb{C}^{n} \times \mathbb{C}^{n} \rightarrow \mathbb{C} V = \mathbb{R}^n G = \mathbb{R}^\times 1 \subset \mathrm{GL}(n,\mathbb{R}) \alpha_1,\ldots,\alpha_{n^2} \in V \otimes_\mathbb{R} V^* \simeq \mathrm{M}(n,\mathbb{R}) \alpha \mathbb{R} \mathrm{M}(n,\mathbb{R}) G \mathcal{P} G \subset \mathrm{GL}_{\mathbb{R}}(V) \mathrm{GL}_{\mathbb{R}}(V) \mathrm{End}_{\mathbb{R}}(V) g \cdot \alpha_j = \alpha_j g g^{-1} g^{-1} g G \mathrm{GL}_{\mathbb{R}}(V) \mathrm{End}_{\mathbb{R}}(V) \mathrm{GL}_+(n,\mathbb{R}) \mathrm{GL}_(n,\mathbb{R}) \mathrm{GL}_+(n,\mathbb{R}) \mathcal{P} \mathrm{GL}_{\mathbb{R}}(V) \mathcal{P} G \mathcal{P}","['linear-algebra', 'matrices', 'algebraic-geometry', 'representation-theory', 'lie-groups']"
85,Proving the identity $e^{J_nA} = cos(A) + J_nsin(A)$ where $J_n^2=-I_n$,Proving the identity  where,e^{J_nA} = cos(A) + J_nsin(A) J_n^2=-I_n,"My professor has told me that the following identity is true: $$e^{J_nA} = \cos(A) + J_n\sin(A)$$ where $e^A$ , $\sin(A)$ , and $\cos(A)$ are all matrix functions defined by their Taylor series, $A$ is a $n\times n$ matrix, and $$J_n^2=-I_n$$ I know one possible value of $J_2$ here: $$J_2 = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$$ As a sidenote, n must be even if we want $J_n$ to have only real entries. I am having trouble deriving the identity. I know that the proof for $e^{it}=\cos(t)+i\sin(t)$ involved separating the Taylor series for sin and cos depending on the behavior of $i^n$ . I tried a similar method for the matrix exponential and got here. $$e^{J_nA}=I + \frac{J_nA}{1} + \frac{J_nAJ_nA}{2!} + \frac{J_nAJ_nAJ_nA}{3!}+\dotsb $$ I would like to write it as: $$e^{J_nA}=I + \frac{J_nA}{1} + \frac{-A^2}{2!} + \frac{-J_nA^3}{1}...$$ The identity only works if $J_n$ and $A$ commute, though I can't see how this can be true. It could work if $J_n$ and $A$ are simultaneously diagonalizable, but that would impose restrictions on what matrix $A$ is. Is there a way to prove the identity? Is the identity just not true? I know there are multiple possible values for $J_n$ . Would I need to impose restrictions on what the value of $J_n$ is to get the identity to work? Thank you","My professor has told me that the following identity is true: where , , and are all matrix functions defined by their Taylor series, is a matrix, and I know one possible value of here: As a sidenote, n must be even if we want to have only real entries. I am having trouble deriving the identity. I know that the proof for involved separating the Taylor series for sin and cos depending on the behavior of . I tried a similar method for the matrix exponential and got here. I would like to write it as: The identity only works if and commute, though I can't see how this can be true. It could work if and are simultaneously diagonalizable, but that would impose restrictions on what matrix is. Is there a way to prove the identity? Is the identity just not true? I know there are multiple possible values for . Would I need to impose restrictions on what the value of is to get the identity to work? Thank you",e^{J_nA} = \cos(A) + J_n\sin(A) e^A \sin(A) \cos(A) A n\times n J_n^2=-I_n J_2 J_2 = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix} J_n e^{it}=\cos(t)+i\sin(t) i^n e^{J_nA}=I + \frac{J_nA}{1} + \frac{J_nAJ_nA}{2!} + \frac{J_nAJ_nAJ_nA}{3!}+\dotsb  e^{J_nA}=I + \frac{J_nA}{1} + \frac{-A^2}{2!} + \frac{-J_nA^3}{1}... J_n A J_n A A J_n J_n,"['linear-algebra', 'matrix-exponential']"
86,Difficulty understanding a proposition in Fulton and Harris regarding $\mathrm{Sp}_{2n}(\mathbb{C})$,Difficulty understanding a proposition in Fulton and Harris regarding,\mathrm{Sp}_{2n}(\mathbb{C}),"In Fulton & Harris, Proposition 23.1 (computing the fundamental groups of classical complex Lie groups) and the Exercises after it, specifically in dealing with $\mathrm{Sp}_{2n}(\mathbb{C})$ , the book seems to suggest that the following two submanifolds are diffeomorphic to each other: $$ M = \left\{ ((x_1,x_2),(y_1,y_2)) \in \mathbb{C}^{2n}\times\mathbb{C}^{2n} : x_1^Ty_2 - x_2^Ty_1 = 1 \right\} , \\ M' = \left\{ ((x_1,x_2),(y_1,y_2)) \in \mathbb{C}^{2n}\times\mathbb{C}^{2n} : x_1^Tx_1 + x_2^Tx_2 + y_1^Ty_1 + y_2^Ty_2 = 1 \right\} . $$ In the book these are written as $$ M = \{ (v,w) \in \mathbb{C}^{2n}\times \mathbb{C}^{2n} : Q(v,w) = 1\}, \\ M' = \{ z \in \mathbb{C}^{4n} : z^T z = 1\} , $$ where $Q$ is an alternating non-degenerate quadratic form on $\mathbb{C}^{2n} \times \mathbb{C}^{2n}$ , which I've put in ""standard form"" by picking a symplectic basis. My guess is, $M$ and $M'$ are shown diffeomorphic by showing there is a change-of-basis on $\mathbb{C}^{4n}$ which takes the $M$ expression to the $M'$ one. However, I'm not sure how to find such a change-of-basis; how do I complete the argument here that $M$ and $M'$ are diffeomorphic? Perhaps I'm missing something? Any help would be much appreciated! Edit: as Stephen has pointed out in the comments, $(v,w) \mapsto v^Tv + w^Tw$ is not at all a bilinear form (as I'd incorrectly written earlier).","In Fulton & Harris, Proposition 23.1 (computing the fundamental groups of classical complex Lie groups) and the Exercises after it, specifically in dealing with , the book seems to suggest that the following two submanifolds are diffeomorphic to each other: In the book these are written as where is an alternating non-degenerate quadratic form on , which I've put in ""standard form"" by picking a symplectic basis. My guess is, and are shown diffeomorphic by showing there is a change-of-basis on which takes the expression to the one. However, I'm not sure how to find such a change-of-basis; how do I complete the argument here that and are diffeomorphic? Perhaps I'm missing something? Any help would be much appreciated! Edit: as Stephen has pointed out in the comments, is not at all a bilinear form (as I'd incorrectly written earlier).","\mathrm{Sp}_{2n}(\mathbb{C}) 
M = \left\{ ((x_1,x_2),(y_1,y_2)) \in \mathbb{C}^{2n}\times\mathbb{C}^{2n}
: x_1^Ty_2 - x_2^Ty_1 = 1 \right\} ,
\\
M' = \left\{ ((x_1,x_2),(y_1,y_2)) \in \mathbb{C}^{2n}\times\mathbb{C}^{2n}
: x_1^Tx_1 + x_2^Tx_2 + y_1^Ty_1 + y_2^Ty_2 = 1 \right\} .
 
M = \{ (v,w) \in \mathbb{C}^{2n}\times \mathbb{C}^{2n} : Q(v,w) = 1\},
\\
M' = \{ z \in \mathbb{C}^{4n} : z^T z = 1\} ,
 Q \mathbb{C}^{2n} \times \mathbb{C}^{2n} M M' \mathbb{C}^{4n} M M' M M' (v,w) \mapsto v^Tv + w^Tw","['linear-algebra', 'representation-theory', 'lie-groups', 'symplectic-linear-algebra']"
87,"Trace of product of commutators in $\operatorname{SL}(2,\mathbb{C})$",Trace of product of commutators in,"\operatorname{SL}(2,\mathbb{C})","Let $A,B\in\operatorname{SL}(2,\mathbb{C})$ and define $x=\operatorname{tr}(A)$ , $y=\operatorname{tr}(B)$ , $z=\operatorname{tr}(AB)$ . It is know that these satisfy the equation $$\operatorname{tr}([A,B])=x^2+y^2+z^2-xyz-2,$$ where the commutator $[A,B]=ABA^{-1}B^{-1}$ is defined in the group setting. For $A,B,C,D\in\operatorname{SL}(2,\mathbb{C})$ can one obtain a similar formula for $\operatorname{tr}([A,B][C,D])$ in terms of $x=\operatorname{tr}(A)$ , $y=\operatorname{tr}(B)$ , $z=\operatorname{tr}(AB)$ , $r=\operatorname{tr}(C)$ , $s=\operatorname{tr}(D)$ , $t=\operatorname{tr}(CD)$ ? Edit: I think such an equation will also need to include the variables $u=\operatorname{tr}(AC)$ , $v=\operatorname{tr}(BD)$ , $w=\operatorname{tr}(AD)$ , $\omega=\operatorname{tr}(BC)$ (so that its level sets are $8$ dimensional surfaces).","Let and define , , . It is know that these satisfy the equation where the commutator is defined in the group setting. For can one obtain a similar formula for in terms of , , , , , ? Edit: I think such an equation will also need to include the variables , , , (so that its level sets are dimensional surfaces).","A,B\in\operatorname{SL}(2,\mathbb{C}) x=\operatorname{tr}(A) y=\operatorname{tr}(B) z=\operatorname{tr}(AB) \operatorname{tr}([A,B])=x^2+y^2+z^2-xyz-2, [A,B]=ABA^{-1}B^{-1} A,B,C,D\in\operatorname{SL}(2,\mathbb{C}) \operatorname{tr}([A,B][C,D]) x=\operatorname{tr}(A) y=\operatorname{tr}(B) z=\operatorname{tr}(AB) r=\operatorname{tr}(C) s=\operatorname{tr}(D) t=\operatorname{tr}(CD) u=\operatorname{tr}(AC) v=\operatorname{tr}(BD) w=\operatorname{tr}(AD) \omega=\operatorname{tr}(BC) 8","['linear-algebra', 'matrices', 'matrix-equations', 'trace']"
88,What is the direct sum of linear space and its dual space?,What is the direct sum of linear space and its dual space?,,"For finite-dimensional linear space $L$ and its dual space $L^*$ , take as basis of $L$ the set of vectors $\{e\}$ , and as the basis of the dual space the dual basis $\{e^*\}$ . Given $y\in L^*,x\in L$ , denote $y(x)$ by the bracket $[x,y]$ . I wish to show that $[x,y]$ is a bilinear form in $K=L \oplus L^* $ , the direct sum of the linear space and its dual space. I thought this was an interesting path to take, because the theory of bilinear forms is nicely developed, but I haven't found any information about it. I'm also not sure that it's 'true' (i.e. that it generalizes in a useful way that maintains the bilinear property), because I'm not sure what the direct sum of these spaces is, and how it would play into the bilinear form described. It's clear that the linear space and its dual are isomorphic, so you could represent them by $n$ -tuples, $x=(\xi_1,\xi_2,...,\xi_n)$ , $y=(\eta_1,\eta_2,...,\eta_n)$ , where $[x,y]$ is defined by $$ [x,y] = \eta_1\xi_1+\eta_2\xi_2+...+\eta_n\xi_n $$ But then we almost fall into the trap of thinking both $x$ and $y$ are in the same basis, which they are not. In the space $K$ , we have to take $$x=\xi_1e_1+\xi_2e_2+...+\xi_ne_n+0e^*_1+0e^*_2+...+0e^*_n$$ $$y=0e_1+0e_2+...+0e_n+\eta_1e^*_1+\eta_2e^*_2+...+\eta_ne^*_n$$ I'm just not sure how to interpret general vectors $z$ , and how to generalize the 'bilinear form' $[x,y]$ (quotes because it may not be a bilinear form for general vectors). Then there's the question of what $[x,x]$ or $[y,y]$ should mean. The whole of it comes down to the models underlying the vector spaces, so in order for this concept to be useful at all, the models need to generalize appropriately. I don't see any issue with the linear spaces themselves, apart from a partially defined mapping $[x,y]$ causing issues. I'm worried I'm going in the wrong direction though. Help! --Edit-- By a bilinear form in a linear space $K$ over a field $\mathcal{F}$ is meant a mapping $A:K\times K\rightarrow\mathcal{F}$ such that $A(x,y)$ for $x,y\in K$ is linear in either argument when the other is fixed.","For finite-dimensional linear space and its dual space , take as basis of the set of vectors , and as the basis of the dual space the dual basis . Given , denote by the bracket . I wish to show that is a bilinear form in , the direct sum of the linear space and its dual space. I thought this was an interesting path to take, because the theory of bilinear forms is nicely developed, but I haven't found any information about it. I'm also not sure that it's 'true' (i.e. that it generalizes in a useful way that maintains the bilinear property), because I'm not sure what the direct sum of these spaces is, and how it would play into the bilinear form described. It's clear that the linear space and its dual are isomorphic, so you could represent them by -tuples, , , where is defined by But then we almost fall into the trap of thinking both and are in the same basis, which they are not. In the space , we have to take I'm just not sure how to interpret general vectors , and how to generalize the 'bilinear form' (quotes because it may not be a bilinear form for general vectors). Then there's the question of what or should mean. The whole of it comes down to the models underlying the vector spaces, so in order for this concept to be useful at all, the models need to generalize appropriately. I don't see any issue with the linear spaces themselves, apart from a partially defined mapping causing issues. I'm worried I'm going in the wrong direction though. Help! --Edit-- By a bilinear form in a linear space over a field is meant a mapping such that for is linear in either argument when the other is fixed.","L L^* L \{e\} \{e^*\} y\in L^*,x\in L y(x) [x,y] [x,y] K=L \oplus L^*  n x=(\xi_1,\xi_2,...,\xi_n) y=(\eta_1,\eta_2,...,\eta_n) [x,y] 
[x,y] = \eta_1\xi_1+\eta_2\xi_2+...+\eta_n\xi_n
 x y K x=\xi_1e_1+\xi_2e_2+...+\xi_ne_n+0e^*_1+0e^*_2+...+0e^*_n y=0e_1+0e_2+...+0e_n+\eta_1e^*_1+\eta_2e^*_2+...+\eta_ne^*_n z [x,y] [x,x] [y,y] [x,y] K \mathcal{F} A:K\times K\rightarrow\mathcal{F} A(x,y) x,y\in K","['linear-algebra', 'bilinear-form', 'dual-spaces']"
89,Why the number of linearly independent columns of a matrix doesn't change by if we apply Row operations,Why the number of linearly independent columns of a matrix doesn't change by if we apply Row operations,,Can anyone please tell me why the number of linearly independent columns of a matrix doesn't change even if we apply row operations on the matrix? The column space does change by row operations but I don't understand why the number of independent columns remains the same.,Can anyone please tell me why the number of linearly independent columns of a matrix doesn't change even if we apply row operations on the matrix? The column space does change by row operations but I don't understand why the number of independent columns remains the same.,,"['linear-algebra', 'matrices', 'gaussian-elimination']"
90,Finding all matrices that satisfy a 'wrong property',Finding all matrices that satisfy a 'wrong property',,"I was reminded of a very usual highschool algebra question of a 'wrong property' where students usually just 'distribute squares' like: $(x+y)^2 = x^2+y^2$ . Clearly this is wrong and that if we solve this equation anyway, then we arrive at $2xy = 0.$ This means that the solution set for which this 'wrong property' holds is when you have any real number $x$ and $y=0$ , or both are zero. Coming to linear algebra, we have a quite similar common misconception that $(\mathbf{A} + \mathbf{B})^{-1} = \mathbf{A}^{-1} + \mathbf{B}^{-1}$ . My question is, can we find all $2\times 2$ matrices that satisfy this equation? I tried solving this, but it is proving a bit difficult, so I made some assumptions like let $\mathbf{A}$ be the identity matrix $\mathbf{I}_2$ , can we find all $\mathbf{B}$ that satisfies this 'wrong property'?","I was reminded of a very usual highschool algebra question of a 'wrong property' where students usually just 'distribute squares' like: . Clearly this is wrong and that if we solve this equation anyway, then we arrive at This means that the solution set for which this 'wrong property' holds is when you have any real number and , or both are zero. Coming to linear algebra, we have a quite similar common misconception that . My question is, can we find all matrices that satisfy this equation? I tried solving this, but it is proving a bit difficult, so I made some assumptions like let be the identity matrix , can we find all that satisfies this 'wrong property'?",(x+y)^2 = x^2+y^2 2xy = 0. x y=0 (\mathbf{A} + \mathbf{B})^{-1} = \mathbf{A}^{-1} + \mathbf{B}^{-1} 2\times 2 \mathbf{A} \mathbf{I}_2 \mathbf{B},"['linear-algebra', 'matrices', 'soft-question', 'inverse']"
91,How do I find $\operatorname{det} T_Q$?,How do I find ?,\operatorname{det} T_Q,"Let $S$ be the space of all $n \times n$ real skew symmetric matrices and let $Q$ be a real orthogonal matrix. Consider the map $T_Q: S \to S$ defined by $$T_Q(X) = QXQ^T.$$ Find $\operatorname{det} T_Q$ . I thought about diagonalizing $Q$ , but I don't think we know it is real diagonalizable. I can show it is an isometry using the Hilbert-Schmidt inner product, but I can't really relate it to the determinant of $Q$ (I've seen posts here that say the determinant should be $\operatorname{det}Q^{n-1}$ ). So all I know is that $\operatorname{det} T_Q = \pm1$ . How would I find $\det T_Q$ ?","Let be the space of all real skew symmetric matrices and let be a real orthogonal matrix. Consider the map defined by Find . I thought about diagonalizing , but I don't think we know it is real diagonalizable. I can show it is an isometry using the Hilbert-Schmidt inner product, but I can't really relate it to the determinant of (I've seen posts here that say the determinant should be ). So all I know is that . How would I find ?",S n \times n Q T_Q: S \to S T_Q(X) = QXQ^T. \operatorname{det} T_Q Q Q \operatorname{det}Q^{n-1} \operatorname{det} T_Q = \pm1 \det T_Q,"['linear-algebra', 'linear-transformations', 'determinant', 'orthogonal-matrices', 'skew-symmetric-matrices']"
92,How to know whether a set of points can be rotated to lie in positive orthant?,How to know whether a set of points can be rotated to lie in positive orthant?,,"I have a set of points in N-dimensional space. I want to find out whether they could, in principle, be rotated to lie solely in the positive orthant of space. Is there a property of these points that would guarantee this rotation exists? If this question proves too difficult, perhaps another related one will be easier. I have the linear dynamical system and starting point that generated these points. Is there a property of linear dynamical systems which, if satisfied, ensure that under some rotation the trajectory will lie solely in the positive orthant? To give an idea of the kind of thing I'm looking for (if it exists!): in 2D if the dot products between the set of points are all positive you can guarantee that there's some rotation that could rotate the set of points into the positive quadrant, and if a dot product is negative you know there is no such rotation. This is a nice easy test I can apply. However, this approach doesn't generalise (in 3D points can have all positive dot products, but cannot be rotated into the positive octant), can you think of one that will generalise? Later edit - a few things I've been considering: People study positive linear systems: linear dynamical systems, $\dot{\boldsymbol{x}}(t) = \boldsymbol{Ax}(t)$ , in which the trajectory, $\boldsymbol{x}(t)$ , never leaves the positive orthant. It turns out if the matrix $\boldsymbol{A}$ is Metzler, meaning all off-diagonal elements are non-negative, then a trajectory starting in the positive orthant will stay there. This is related to the Perron-Frobenius Theorem which I don't know so much about. So it seems another way of framing my question would be: you have your linear dynamical system in an arbitrary basis. Can you tell whether the dynamics matrix $\boldsymbol{A}$ is Metzler under some rotation? The Perron-Frobenius theorem talks about rotation invariant things like eigenvalue spectra which gives me hope that this could be a constructive route, but I haven't been able to fit things together yet... Maybe the clever people on this site will be able to! (Which is similar to this old question: Converse of Perron Frobenius Theorem: Necessary and Sufficient Conditions for positivity (or non negativity) )","I have a set of points in N-dimensional space. I want to find out whether they could, in principle, be rotated to lie solely in the positive orthant of space. Is there a property of these points that would guarantee this rotation exists? If this question proves too difficult, perhaps another related one will be easier. I have the linear dynamical system and starting point that generated these points. Is there a property of linear dynamical systems which, if satisfied, ensure that under some rotation the trajectory will lie solely in the positive orthant? To give an idea of the kind of thing I'm looking for (if it exists!): in 2D if the dot products between the set of points are all positive you can guarantee that there's some rotation that could rotate the set of points into the positive quadrant, and if a dot product is negative you know there is no such rotation. This is a nice easy test I can apply. However, this approach doesn't generalise (in 3D points can have all positive dot products, but cannot be rotated into the positive octant), can you think of one that will generalise? Later edit - a few things I've been considering: People study positive linear systems: linear dynamical systems, , in which the trajectory, , never leaves the positive orthant. It turns out if the matrix is Metzler, meaning all off-diagonal elements are non-negative, then a trajectory starting in the positive orthant will stay there. This is related to the Perron-Frobenius Theorem which I don't know so much about. So it seems another way of framing my question would be: you have your linear dynamical system in an arbitrary basis. Can you tell whether the dynamics matrix is Metzler under some rotation? The Perron-Frobenius theorem talks about rotation invariant things like eigenvalue spectra which gives me hope that this could be a constructive route, but I haven't been able to fit things together yet... Maybe the clever people on this site will be able to! (Which is similar to this old question: Converse of Perron Frobenius Theorem: Necessary and Sufficient Conditions for positivity (or non negativity) )",\dot{\boldsymbol{x}}(t) = \boldsymbol{Ax}(t) \boldsymbol{x}(t) \boldsymbol{A} \boldsymbol{A},"['linear-algebra', 'vectors', 'linear-transformations', 'dynamical-systems', 'rotations']"
93,Kreyszig's definition of finite dimensional vector spae,Kreyszig's definition of finite dimensional vector spae,,"In his book on functional analysis Kreyszig gives the following definition: Definition. A vector space $X$ is said to be finite dimensional if there is a positive integer $n$ such that $X$ contains a linearly independent set of $n$ vectors whereas any set of $n+1$ or more vectors of $X$ is linearly dependent. $n$ is called the dimension of $X$ , written $n=\text{dim } X$ . By definition, $X=\{0\}$ is finite dimensional and $\text{dim } X=0$ . If $X$ is not finite dimensional, it is said to be infinite dimensional. If $\text{dim } X=n$ , a linearly independent $n$ -tuple of vectors of $X$ is called a basis for $X$ . He then proves the following basic theorem Theorem. Let $X$ be an $n$ dimensional vector space. Then any proper subspace $Y$ of $X$ has dimension less than $n$ . Proof. If $n=0$ , then $X=\{0\}$ and has no proper subspace. If $\text{dim } Y=0$ , then $Y=\{0\}$ , and $X \neq Y$ implies $\text{dim } X\geq 1$ . Clearly, $\text{dim } Y\leq \text{dim } X=n$ . If $\text{dim } Y$ were $n$ , then $Y$ would have a basis of $n$ elements, which would also be a basis for $X$ since $\text{dim } X=n$ , so that $X=Y$ . This shows that any linearly independent set of vectors in $Y$ must have fewer than $n$ elements, and $\text{dim } Y<n$ . Questions: Why is the last sentence in the proof necessary? It seems like a contradiction has already been reached at this point. Am not sure the statement $\text{dim } Y\leq \text{dim } X$ is so clear given his definitions. We cannot have $\text{dim } Y=m>n$ , but how can we rule out the case of $\text{dim } Y=\infty$ ? We idea to rule out $\text{dim } Y=\infty$ is the following argument: If $\text{dim } Y=\infty$ , then in particular $\text{dim } Y\neq n$ . Hence either there exist a set of $n+1$ linearly independent vectors in $Y$ or every set of $n$ vectors in $Y$ is linearly dependent. By definition of $\text{dim } X=n$ it must be that the latter holds. Now we repeat the argument to obtain that every set of $n-1$ vectors in $Y$ is linearly dependent. After $n$ steps we reach the conclusion that $Y=\{0\}$ , contradiction. Is this correct?","In his book on functional analysis Kreyszig gives the following definition: Definition. A vector space is said to be finite dimensional if there is a positive integer such that contains a linearly independent set of vectors whereas any set of or more vectors of is linearly dependent. is called the dimension of , written . By definition, is finite dimensional and . If is not finite dimensional, it is said to be infinite dimensional. If , a linearly independent -tuple of vectors of is called a basis for . He then proves the following basic theorem Theorem. Let be an dimensional vector space. Then any proper subspace of has dimension less than . Proof. If , then and has no proper subspace. If , then , and implies . Clearly, . If were , then would have a basis of elements, which would also be a basis for since , so that . This shows that any linearly independent set of vectors in must have fewer than elements, and . Questions: Why is the last sentence in the proof necessary? It seems like a contradiction has already been reached at this point. Am not sure the statement is so clear given his definitions. We cannot have , but how can we rule out the case of ? We idea to rule out is the following argument: If , then in particular . Hence either there exist a set of linearly independent vectors in or every set of vectors in is linearly dependent. By definition of it must be that the latter holds. Now we repeat the argument to obtain that every set of vectors in is linearly dependent. After steps we reach the conclusion that , contradiction. Is this correct?",X n X n n+1 X n X n=\text{dim } X X=\{0\} \text{dim } X=0 X \text{dim } X=n n X X X n Y X n n=0 X=\{0\} \text{dim } Y=0 Y=\{0\} X \neq Y \text{dim } X\geq 1 \text{dim } Y\leq \text{dim } X=n \text{dim } Y n Y n X \text{dim } X=n X=Y Y n \text{dim } Y<n \text{dim } Y\leq \text{dim } X \text{dim } Y=m>n \text{dim } Y=\infty \text{dim } Y=\infty \text{dim } Y=\infty \text{dim } Y\neq n n+1 Y n Y \text{dim } X=n n-1 Y n Y=\{0\},"['linear-algebra', 'vector-spaces', 'solution-verification', 'hamel-basis']"
94,Spectral decomposition theorem,Spectral decomposition theorem,,"I'm reviewing the spectral decomposition theorem. If $Y$ is a symmetric matrix, then $Y$ can be decomposed as $Y=Q\Lambda Q'$ , where the columns of $Q$ are the eigenvectors of $Y$ and $\Lambda$ is a diagonal matrix with the diagonal composed of the eigenvalues ​​of $Y$ . From some part of this theorem can it be concluded that $Q$ is an orthogonal matrix? I know that the eigenvectors of $Y$ are orthogonal, so can it be concluded that the matrix $Q$ is orthogonal?","I'm reviewing the spectral decomposition theorem. If is a symmetric matrix, then can be decomposed as , where the columns of are the eigenvectors of and is a diagonal matrix with the diagonal composed of the eigenvalues ​​of . From some part of this theorem can it be concluded that is an orthogonal matrix? I know that the eigenvectors of are orthogonal, so can it be concluded that the matrix is orthogonal?",Y Y Y=Q\Lambda Q' Q Y \Lambda Y Q Y Q,"['linear-algebra', 'matrix-decomposition']"
95,Intuitive explanation for inverse of a permutation matrix,Intuitive explanation for inverse of a permutation matrix,,"Today in lecture we learned that the transpose of a permutation matrix is the inverse of the permutation matrix. Meaning, $$P^{T}P = I$$ I can work out the math by matrix multiplication but I'd prefer a deeper, more intuitive understanding. What I have so far in my head is: We know that the matrix $P$ will swap rows when we apply it to a matrix, let's say $A$ . Then $PA$ will swap the $i^{th}$ row of A with the $j^{th}$ of $A$ . This then means that $P^{T}(PA)$ must swap our new $i^{th}$ row with the new $j^{th}$ row so we can have our original $A$ matrix back. Why is this always true? More specifically why does $P^{T}$ swap back out rows...?","Today in lecture we learned that the transpose of a permutation matrix is the inverse of the permutation matrix. Meaning, I can work out the math by matrix multiplication but I'd prefer a deeper, more intuitive understanding. What I have so far in my head is: We know that the matrix will swap rows when we apply it to a matrix, let's say . Then will swap the row of A with the of . This then means that must swap our new row with the new row so we can have our original matrix back. Why is this always true? More specifically why does swap back out rows...?",P^{T}P = I P A PA i^{th} j^{th} A P^{T}(PA) i^{th} j^{th} A P^{T},"['linear-algebra', 'matrices', 'inverse', 'intuition', 'permutation-matrices']"
96,Why do sometimes care for where vectors originate from and sometimes not? and exactly how many kinds of vectors are there?,Why do sometimes care for where vectors originate from and sometimes not? and exactly how many kinds of vectors are there?,,"When I did linear algebra in high-school, it wasn't of much importance where the vectors originated from and for me this is a really hard concept to grasp. It's like no matter where the two vectors are pivoted in 3-d space, their dot product is invariant. Like, we don't even define an origin when talking about vectors... it's like they're freely floating in space.  Why can we do this as in why do we not need to regard origin when we speak of vectors? Is the vector attached to some object? like does it not matter where the 'tail' is. Some more context This question arose mainly when I was learning about plotting vector fields, in that, I had to associate each point with a vector so definitely here the vectors origin is relevant but not in the previous case, why? I had also come across this problem when studying physics, see this post . The person answering the post says that the cross product gives an axial vector. So I wonder how many types of vectors are there? Does this mean that regular 'vector' that we learned of has many 'cousin-forms'? How many takes types of vectors are there? how do we distinguish between these kinds of vectors?","When I did linear algebra in high-school, it wasn't of much importance where the vectors originated from and for me this is a really hard concept to grasp. It's like no matter where the two vectors are pivoted in 3-d space, their dot product is invariant. Like, we don't even define an origin when talking about vectors... it's like they're freely floating in space.  Why can we do this as in why do we not need to regard origin when we speak of vectors? Is the vector attached to some object? like does it not matter where the 'tail' is. Some more context This question arose mainly when I was learning about plotting vector fields, in that, I had to associate each point with a vector so definitely here the vectors origin is relevant but not in the previous case, why? I had also come across this problem when studying physics, see this post . The person answering the post says that the cross product gives an axial vector. So I wonder how many types of vectors are there? Does this mean that regular 'vector' that we learned of has many 'cousin-forms'? How many takes types of vectors are there? how do we distinguish between these kinds of vectors?",,"['linear-algebra', 'soft-question', 'vectors']"
97,The angle between two subspaces of $\mathbb R^n$ can not be too small,The angle between two subspaces of  can not be too small,\mathbb R^n,"Notations. Let $A$ and $B$ be two subspaces of $\mathbb R^{2k}$ of dimensions $k$ . Let's call $p_A^\perp$ the orthogonal projection onto $A$ . Let's define two quantities: $$\psi_1(A,B)=\inf_{b\in B\setminus\{0\}} \vert\sin\widehat{(b,p_A^\perp(b))}\vert$$ and $$\psi_\infty(A,B)=\sup_{b\in B\setminus\{0\}} \vert\sin\widehat{(b,p_A^\perp(b))}\vert.$$ The question. Assume that it exists a constant $c$ such that $\psi_\infty(A,B)\geqslant c$ ; There exist $b_1,\ldots,b_k\in B$ linearly independent such that $\widehat{(b_i,b_j)}\geqslant \pi/4$ if $i\ne j$ , and $$\forall j\in\{1,\ldots,k\},\quad \vert\sin\widehat{(b_j,p_A^\perp(b_j))}\vert \geqslant c.$$ Let's also assume that $A\cap B=\{0\}$ . Can we prove that for all $c$ sufficiently small, $$\psi_1(A,B)\geqslant c'$$ where $c'$ is a constant depending on $c$ ? Remarks. This question is kind of linked to this other question . Any hints or references would be much appreciated.","Notations. Let and be two subspaces of of dimensions . Let's call the orthogonal projection onto . Let's define two quantities: and The question. Assume that it exists a constant such that ; There exist linearly independent such that if , and Let's also assume that . Can we prove that for all sufficiently small, where is a constant depending on ? Remarks. This question is kind of linked to this other question . Any hints or references would be much appreciated.","A B \mathbb R^{2k} k p_A^\perp A \psi_1(A,B)=\inf_{b\in B\setminus\{0\}} \vert\sin\widehat{(b,p_A^\perp(b))}\vert \psi_\infty(A,B)=\sup_{b\in B\setminus\{0\}} \vert\sin\widehat{(b,p_A^\perp(b))}\vert. c \psi_\infty(A,B)\geqslant c b_1,\ldots,b_k\in B \widehat{(b_i,b_j)}\geqslant \pi/4 i\ne j \forall j\in\{1,\ldots,k\},\quad \vert\sin\widehat{(b_j,p_A^\perp(b_j))}\vert \geqslant c. A\cap B=\{0\} c \psi_1(A,B)\geqslant c' c' c","['linear-algebra', 'geometry', 'vector-spaces', 'angle']"
98,"Unable to invert matrix on Galois Field, even though the matrix should be invertible by construction","Unable to invert matrix on Galois Field, even though the matrix should be invertible by construction",,"We are trying to implement a general file recovery algorithm using Galois Fields. We have implemented the operations for Galois Fields GF(2^8) succesfully, but we're are running into a problem for the case with 4 data drives and 4 parity drives. More specifically, the case where data drives 0, 1, and 2, and parity drive 2 are missing. Following the implementation in this paper , we construct an 8 by 4 matrix: 1   0   0   0  0   1   0   0 0   0   1   0 0   0   0   1 1   1   1   1 1   2   3   4 1^2 2^2 3^2 4^2 1^3 2^3 3^3 4^3 Since datadrives 0, 1, and 2, and parity drive 2 are compromised, we omit the rows 0, 1, 2, and 6. That leaves 0   0   0   1         0  0  0  1  1   1   1   1     =   1  1  1  1 1   2   3   4         1  2  3  4 1^3 2^3 3^3 4^3       1  8 15 64 Let's call this matrix A'. If D is the data vector, and E' is the vector corresponding to the data on the drives that did not fail, we should be able to solve A' D = E' for D. D = A'^{-1} A' D = A'^{-1} E' However, matrix A' doesn't seem to be invertible. See also this code snippet at sageMath: SageMath even shows that the matrix can be simplified to A' = [0 0 0 1] [1 1 1 1] [1 0 1 0] [1 0 1 0] which is clearly not invertible. Somehow the parity rows [1^1, 2^1, 3^1, 4^1] and [1^3, 2^3, 3^3, 4^3] are linearly dependent.  We thought these rows were constructed to be independent, even with the operations on GF(2^8) Clearly, we're missing something, but at this point, we're not sure what's happening here. UPDATE 1 So it turns out the generator matrix in that paper was completely wrong. Following the suggestion by Jyrki in the comments here , we constructed the desired matrix starting from a variant of the Vandermonde matrix. This variant is of the form: $$G = \left( \begin{array}{ cccc }   \alpha_0^0 & \alpha_0^1 & .. & \alpha_0^d \\   \alpha_1^0 & \alpha_1^1 & .. & \alpha_1^d \\   .. & .. & .. & .. & .. \\   \alpha_{d+p}^0 & \alpha_{d+p}^1 & .. & \alpha_{d+p}^d \\ \end{array} \right)$$ where the $d$ is the number of data bits, $p$ is the number of parity bits, and $\alpha_i$ are chosen elements of order $2^8$ in $GF(2^8)$ . These alpha can be constructed by taking the Galois Field element with 2 as its integer representation, and raising it to a power that shares no prime factors with $2^8 - 1$ . So: $\alpha_0 = 2^1$ , $\alpha_1 = 2^2$ , $\alpha_2 = 2^4$ (skip $2^3$ because 3 has a prime in common with $2^8-1$ ), $\alpha_3 = 2^7$ (skip $2^5$ and $2^6$ because 5 and 6 share prime factors with $2^8-1$ ), $\alpha_4 = 2^8$ , etc I must admit that I don't understand this argument completely, but it works for now. For the top $d \times d$ block (let's call it $P$ ), we compute $P^{-1}$ and calculate $GP^{-1}$ to get it in the desired form. We also tried the matrix on described in this paper , which uses the same base $\alpha$ , which also works as far as we can tell. We have also looked at multiple other software implementations, where they appear to determine the parity block in one go (so without calculating $GP^{-1}$ . The code however is very difficult to understand. Could there be a standard procedure to immediately generate that bottom block or do these software packages only work for a specific order of the field and polynomial?","We are trying to implement a general file recovery algorithm using Galois Fields. We have implemented the operations for Galois Fields GF(2^8) succesfully, but we're are running into a problem for the case with 4 data drives and 4 parity drives. More specifically, the case where data drives 0, 1, and 2, and parity drive 2 are missing. Following the implementation in this paper , we construct an 8 by 4 matrix: 1   0   0   0  0   1   0   0 0   0   1   0 0   0   0   1 1   1   1   1 1   2   3   4 1^2 2^2 3^2 4^2 1^3 2^3 3^3 4^3 Since datadrives 0, 1, and 2, and parity drive 2 are compromised, we omit the rows 0, 1, 2, and 6. That leaves 0   0   0   1         0  0  0  1  1   1   1   1     =   1  1  1  1 1   2   3   4         1  2  3  4 1^3 2^3 3^3 4^3       1  8 15 64 Let's call this matrix A'. If D is the data vector, and E' is the vector corresponding to the data on the drives that did not fail, we should be able to solve A' D = E' for D. D = A'^{-1} A' D = A'^{-1} E' However, matrix A' doesn't seem to be invertible. See also this code snippet at sageMath: SageMath even shows that the matrix can be simplified to A' = [0 0 0 1] [1 1 1 1] [1 0 1 0] [1 0 1 0] which is clearly not invertible. Somehow the parity rows [1^1, 2^1, 3^1, 4^1] and [1^3, 2^3, 3^3, 4^3] are linearly dependent.  We thought these rows were constructed to be independent, even with the operations on GF(2^8) Clearly, we're missing something, but at this point, we're not sure what's happening here. UPDATE 1 So it turns out the generator matrix in that paper was completely wrong. Following the suggestion by Jyrki in the comments here , we constructed the desired matrix starting from a variant of the Vandermonde matrix. This variant is of the form: where the is the number of data bits, is the number of parity bits, and are chosen elements of order in . These alpha can be constructed by taking the Galois Field element with 2 as its integer representation, and raising it to a power that shares no prime factors with . So: , , (skip because 3 has a prime in common with ), (skip and because 5 and 6 share prime factors with ), , etc I must admit that I don't understand this argument completely, but it works for now. For the top block (let's call it ), we compute and calculate to get it in the desired form. We also tried the matrix on described in this paper , which uses the same base , which also works as far as we can tell. We have also looked at multiple other software implementations, where they appear to determine the parity block in one go (so without calculating . The code however is very difficult to understand. Could there be a standard procedure to immediately generate that bottom block or do these software packages only work for a specific order of the field and polynomial?","G = \left( \begin{array}{ cccc }
  \alpha_0^0 & \alpha_0^1 & .. & \alpha_0^d \\
  \alpha_1^0 & \alpha_1^1 & .. & \alpha_1^d \\
  .. & .. & .. & .. & .. \\
  \alpha_{d+p}^0 & \alpha_{d+p}^1 & .. & \alpha_{d+p}^d \\
\end{array} \right) d p \alpha_i 2^8 GF(2^8) 2^8 - 1 \alpha_0 = 2^1 \alpha_1 = 2^2 \alpha_2 = 2^4 2^3 2^8-1 \alpha_3 = 2^7 2^5 2^6 2^8-1 \alpha_4 = 2^8 d \times d P P^{-1} GP^{-1} \alpha GP^{-1}","['linear-algebra', 'galois-theory', 'finite-fields']"
99,Number of non-zero entries in $A$ and $AA^T$,Number of non-zero entries in  and,A AA^T,Let $A$ be a matrix with each entry $0$ or $1$ such that each row contains at most $r$ $1$ 's and each row of $AA^T$ contains at most $z$ non-zero entries. Can we somehow bound from above the number of $1$ 's on each column of $A$ in terms of $r$ and $z$ only? I get lost in the use of the matrix multiplication calculations. (The bound need not be tight but should be expressible as a simple function of $r$ and $z$ .) Any help appreciated!,Let be a matrix with each entry or such that each row contains at most 's and each row of contains at most non-zero entries. Can we somehow bound from above the number of 's on each column of in terms of and only? I get lost in the use of the matrix multiplication calculations. (The bound need not be tight but should be expressible as a simple function of and .) Any help appreciated!,A 0 1 r 1 AA^T z 1 A r z r z,"['linear-algebra', 'combinatorics']"
