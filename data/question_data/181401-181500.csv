,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Multivariable Taylor series convergence,Multivariable Taylor series convergence,,"Let us have a Taylor series expansion for multivariable function $$ f(x_1,x_2,\ldots,x_N)=\sum_{n=0}^{\infty}{\sum_{j_1+j_2+\ldots +j_N=n}{\dfrac{1}{j_1!j_2!\ldots j_N!}\dfrac{\partial^{n}f}{\partial x_1^{j_1}\partial x_2^{j_2}\ldots \partial x_N^{j_N}}\Bigg|_{x=0}x_1^{j_1}x_2^{j_2}\ldots x_N^{j_N}}}, $$ where in the second sum summation goes over all possible combinations of non-negative integer solutions of the equation $j_1+j_2+\ldots + j_N=n$ . In particular I’m interested in the expansion of the function $$ f(x_1,x_2,\ldots,x_N)=\exp\left\{\dfrac{1}{2}D_{st}{x_s}{x_t}\right\}, $$ where $D_{st}x_sx_t$ means sum $\sum_{s=1}^{N}{\sum_{t=1}^{N}{D_{st}x_sx_t}}$ , because that is a moment-generating function for multivariable normal distribution with a covariance matrix $D$ . I need to prove that in the case of such a function it’s Taylor expansion converges to this function for every possible vector $(x_1,x_2,\ldots,x_N)\in \mathbb{R}^{N}$ . I know that for a function of one variable its Taylor series $$ \sum_{i=0}^{n}{\dfrac{f^{(i)}(0)}{i!}x^{i}}+R_{n}(x) $$ converges to it if and only if its remainder term $R_n(x)$ has a limit $$ \lim_{n\rightarrow \infty}{R_n(x)}=0, $$ and for any given interval $x\in (-R,R)$ this is true if, for example, $$ \left|f^{(n)}(\xi)\right|\leq M $$ for any $n$ and $\xi\in(-R,R)$ . I’m interested if there some way to prove that for multivariable function in question its series converges to it on all $\mathbb{R}^N$ so that its series unambiguously defines it. This question has stemmed out of that The theorem inverse to Isserlis' theorem .","Let us have a Taylor series expansion for multivariable function where in the second sum summation goes over all possible combinations of non-negative integer solutions of the equation . In particular I’m interested in the expansion of the function where means sum , because that is a moment-generating function for multivariable normal distribution with a covariance matrix . I need to prove that in the case of such a function it’s Taylor expansion converges to this function for every possible vector . I know that for a function of one variable its Taylor series converges to it if and only if its remainder term has a limit and for any given interval this is true if, for example, for any and . I’m interested if there some way to prove that for multivariable function in question its series converges to it on all so that its series unambiguously defines it. This question has stemmed out of that The theorem inverse to Isserlis' theorem .","
f(x_1,x_2,\ldots,x_N)=\sum_{n=0}^{\infty}{\sum_{j_1+j_2+\ldots +j_N=n}{\dfrac{1}{j_1!j_2!\ldots j_N!}\dfrac{\partial^{n}f}{\partial x_1^{j_1}\partial x_2^{j_2}\ldots \partial x_N^{j_N}}\Bigg|_{x=0}x_1^{j_1}x_2^{j_2}\ldots x_N^{j_N}}},
 j_1+j_2+\ldots + j_N=n 
f(x_1,x_2,\ldots,x_N)=\exp\left\{\dfrac{1}{2}D_{st}{x_s}{x_t}\right\},
 D_{st}x_sx_t \sum_{s=1}^{N}{\sum_{t=1}^{N}{D_{st}x_sx_t}} D (x_1,x_2,\ldots,x_N)\in \mathbb{R}^{N} 
\sum_{i=0}^{n}{\dfrac{f^{(i)}(0)}{i!}x^{i}}+R_{n}(x)
 R_n(x) 
\lim_{n\rightarrow \infty}{R_n(x)}=0,
 x\in (-R,R) 
\left|f^{(n)}(\xi)\right|\leq M
 n \xi\in(-R,R) \mathbb{R}^N","['multivariable-calculus', 'power-series', 'taylor-expansion']"
1,"if $f$ with continuous partial derivatives at $(x_0,y_0)$. Then there exists a unit vector $\vec{u}$ which $D_\vec{u} (f)(x_0,y_0)=0$",if  with continuous partial derivatives at . Then there exists a unit vector  which,"f (x_0,y_0) \vec{u} D_\vec{u} (f)(x_0,y_0)=0","Prove or disprove: let $f$ be a function with continuous partial derivatives at $(x_0,y_0)$. Then there exists a unit vector $\vec{u}$ for which $D_\vec{u} (f)(x_0,y_0)=0$ It feels like it's not true, but I'm new to this material and don't really understand it.","Prove or disprove: let $f$ be a function with continuous partial derivatives at $(x_0,y_0)$. Then there exists a unit vector $\vec{u}$ for which $D_\vec{u} (f)(x_0,y_0)=0$ It feels like it's not true, but I'm new to this material and don't really understand it.",,['multivariable-calculus']
2,How to avoid ambiguity defining continuity / differentiability of multivariable function,How to avoid ambiguity defining continuity / differentiability of multivariable function,,"A function $f(x):\mathbb R^n \rightarrow\mathbb R^1$ is continuous on $\mathbb R^n\setminus \{0\}$ and the limit $\lim_{x\rightarrow0} f(x)$ exists (in a sense is bounded) in any direction to origin. Is this function continuous in the origin, or in order to be continuous there it has to have the same limit value there in all directions? How to differentiate such two cases defining function continuity? Same question arises concerning differentiability. I appreciate any explanations.","A function $f(x):\mathbb R^n \rightarrow\mathbb R^1$ is continuous on $\mathbb R^n\setminus \{0\}$ and the limit $\lim_{x\rightarrow0} f(x)$ exists (in a sense is bounded) in any direction to origin. Is this function continuous in the origin, or in order to be continuous there it has to have the same limit value there in all directions? How to differentiate such two cases defining function continuity? Same question arises concerning differentiability. I appreciate any explanations.",,"['multivariable-calculus', 'continuity']"
3,Chain rule and implicit functions problem,Chain rule and implicit functions problem,,"I'm trying to solve a problem involving the implicit function theorem. I've come to a point where I must derive this implicit function with respect to it's coordinates and don't know how to proceed. Here is the problem: Prove that exists an implicit function $\varphi$ around the point $0$ with $\varphi (0) = 0$ such that $F(\varphi(y),y)=0$ $F: \Bbb R^2 \rightarrow \Bbb R$ , $F(x,y)= x^3 - x(y+1)+y^2$ Express $\varphi ' (y)$ in terms of $y$ and $\varphi(y)$ Find the Taylor polynomial of second degree of $g$ around the point $p=(1,-1)$ with $g(x.y)=\varphi(y+x^2)+xe^{y+1}$ The hypothesis for the implicit function theorem are true for $F$ around the point. And $F_x(0,0)\neq0$ . So the function exists. I have $\varphi'(y)=\frac{\varphi(y)-2y}{3 \varphi(y)^2-y-1}$ I want to find the equation for $g$ 's taylor polynomial but I'm confused on how to derive with respect to what in $g$ . $g_x(x,y)=\varphi'(y+x^2)2x+e^{1+y}$ and $g_y(x,y)=\varphi'(y+x^2)+xe^{1+y}$ Could anyone help me with this? Thanks. Update: As I have $\varphi'$ I only need to understand how to get the second order partial derivatives of $g$ .","I'm trying to solve a problem involving the implicit function theorem. I've come to a point where I must derive this implicit function with respect to it's coordinates and don't know how to proceed. Here is the problem: Prove that exists an implicit function around the point with such that , Express in terms of and Find the Taylor polynomial of second degree of around the point with The hypothesis for the implicit function theorem are true for around the point. And . So the function exists. I have I want to find the equation for 's taylor polynomial but I'm confused on how to derive with respect to what in . and Could anyone help me with this? Thanks. Update: As I have I only need to understand how to get the second order partial derivatives of .","\varphi 0 \varphi (0) = 0 F(\varphi(y),y)=0 F: \Bbb R^2 \rightarrow \Bbb R F(x,y)= x^3 - x(y+1)+y^2 \varphi ' (y) y \varphi(y) g p=(1,-1) g(x.y)=\varphi(y+x^2)+xe^{y+1} F F_x(0,0)\neq0 \varphi'(y)=\frac{\varphi(y)-2y}{3 \varphi(y)^2-y-1} g g g_x(x,y)=\varphi'(y+x^2)2x+e^{1+y} g_y(x,y)=\varphi'(y+x^2)+xe^{1+y} \varphi' g","['multivariable-calculus', 'derivatives', 'partial-derivative', 'implicit-function-theorem']"
4,"Universal differential identities, part 2","Universal differential identities, part 2",,"This is a follow-up of this question . Let $$F(f,f_{x_1},f_{x_2},\dots,f_{x_n},f_{x_1x_2},f_{x_1x_3}...)=0$$ be an identity relating some finite number of derivatives of a map $f:\mathbb{R}^n \to \mathbb{R}$. Suppose it is satisfied by all smooth maps. Must $F$ be the trivial identity, i.e the zero map? (Note we already take into account the equality of mixed derivatives, i.e for each pair $x_i,x_j$ we have as an argument only one of   the expressions $f_{x_ix_j} \,,\,f_{x_jx_i}$). The previous question was about the special case where $F$ was a polynomial identity. The point of the current generalization is to rule out the existence of non-trivial universal identities such as $\sin(f_x)=f_y$. (This particular identity is represented by $F(x_1,x_2,x_3)=\sin x_2-x_3$).","This is a follow-up of this question . Let $$F(f,f_{x_1},f_{x_2},\dots,f_{x_n},f_{x_1x_2},f_{x_1x_3}...)=0$$ be an identity relating some finite number of derivatives of a map $f:\mathbb{R}^n \to \mathbb{R}$. Suppose it is satisfied by all smooth maps. Must $F$ be the trivial identity, i.e the zero map? (Note we already take into account the equality of mixed derivatives, i.e for each pair $x_i,x_j$ we have as an argument only one of   the expressions $f_{x_ix_j} \,,\,f_{x_jx_i}$). The previous question was about the special case where $F$ was a polynomial identity. The point of the current generalization is to rule out the existence of non-trivial universal identities such as $\sin(f_x)=f_y$. (This particular identity is represented by $F(x_1,x_2,x_3)=\sin x_2-x_3$).",,"['real-analysis', 'multivariable-calculus', 'differential-operators']"
5,Green's Theorem in polar coordinates - what am I doing wrong?,Green's Theorem in polar coordinates - what am I doing wrong?,,"Let $$\vec F = M\hat i + N\hat j = (3x^2y^2)\hat i + (2x^2 + 2x^3y)\hat j$$ be a vector field, and C be the counterclockwise circle centered at $(a,0)$, with a radius of $a$. Find $\oint_C M\,dx + N\,dy$. Applying Green's Theorem:  $$\oint_C M\,dx + N\,dy = \iint_R N_x-M_y\,dA = 4\iint_R x\,dx$$ Using center of mass with uniform density in place of direct computation: $$4\iint_R x\,dx=4\frac{\iint_R x\,dx}{\iint_R\,dx}\iint_R\,dx = 4 \bar x *Area(R) = 4\pi a^3$$ The problem I'm having is when I try to do the double integral by direct computation in polar coordinates. I've checked the intermediate steps with an integral calculator to make sure there wasn't a simple calculation error, which there wasn't, so I'm guessing something is wrong with the setup. The first thing I did was get an expression for $x$ and $y$ in terms of $\theta$. I did this by noting that, for any ray coming from the origin, it will intersect the circle and form an isosceles triangle, where the first leg of length $a$ is coincidental with the $x$ axis, and the second leg, call it $l$, is the line segment of length $a$ going through the circle's origin and the point of intersection. From this, the angle between the two legs should be $\pi-2\theta$, since there are two equivalent $\theta$s, and the sum of the angles in a triangle is $\pi$ radians. The angle $\alpha$ between $l$ and the $x$ axis is $\pi - (\pi - 2\theta)$, since the angle between the two legs of the isosceles triangle plus the angle between $l$ and the $x$ axis should be $\pi$. This makes it easy to get an expression for $x$ and $y$ using vectors. If the circle were at the origin, then we would have $\vec r = (a\,cos(\alpha),a\,sin(\alpha)=(a\,cos(2\theta), a\,sin(2\theta)$, to which we add the offset vector $(a,0)$ to obtain $\vec r = (a(1+cos(2\theta), a\,sin(2\theta)$. I did this another way to confirm, by using the law of cosines, and it seems correct from that standpoint as well. If the angle between the two sides of the isosceles triangle is $\pi - 2\theta$, then $r$ should be $\sqrt{2a^2(1-cos(\pi - 2\theta)}$, or equivalently, $\sqrt{2a^2(1+cos(2\theta))}$, since $cos(\pi-x)=-cos(x)$. Then, the unit vector extending in the direction of the ray from the origin is $\hat u = (cos(\theta), sin(\theta))$, and the ray should be $r \hat u$. I graphed both of these representations for the $x$ coordinate, namely, $x=a(1+cos(2\theta))$ and $x=cos(\theta) \sqrt{2a^2(1+cos(2\theta))}$, and they are equivalent on $-\frac{\pi}{2} \le x \le \frac{\pi}{2}$. Finally, I set up the integral: $$\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \int_0^{\sqrt{2a^2(1+cos(2\theta))}}r(4a(1+cos(2\theta)))\,dr\,d\theta$$ Evaluating the inner integral leaves: $$\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} 4a^3(1+cos(2\theta))^2\,d\theta = 6 \pi a^3$$ Interestingly, if I evaluate this integral without the square, i.e., $$\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} 4a^3(1+cos(2\theta))\,d\theta$$ then I get the correct answer, although I have no clue at this point how I would get such an expression. Any insight would be much appreciated.","Let $$\vec F = M\hat i + N\hat j = (3x^2y^2)\hat i + (2x^2 + 2x^3y)\hat j$$ be a vector field, and C be the counterclockwise circle centered at $(a,0)$, with a radius of $a$. Find $\oint_C M\,dx + N\,dy$. Applying Green's Theorem:  $$\oint_C M\,dx + N\,dy = \iint_R N_x-M_y\,dA = 4\iint_R x\,dx$$ Using center of mass with uniform density in place of direct computation: $$4\iint_R x\,dx=4\frac{\iint_R x\,dx}{\iint_R\,dx}\iint_R\,dx = 4 \bar x *Area(R) = 4\pi a^3$$ The problem I'm having is when I try to do the double integral by direct computation in polar coordinates. I've checked the intermediate steps with an integral calculator to make sure there wasn't a simple calculation error, which there wasn't, so I'm guessing something is wrong with the setup. The first thing I did was get an expression for $x$ and $y$ in terms of $\theta$. I did this by noting that, for any ray coming from the origin, it will intersect the circle and form an isosceles triangle, where the first leg of length $a$ is coincidental with the $x$ axis, and the second leg, call it $l$, is the line segment of length $a$ going through the circle's origin and the point of intersection. From this, the angle between the two legs should be $\pi-2\theta$, since there are two equivalent $\theta$s, and the sum of the angles in a triangle is $\pi$ radians. The angle $\alpha$ between $l$ and the $x$ axis is $\pi - (\pi - 2\theta)$, since the angle between the two legs of the isosceles triangle plus the angle between $l$ and the $x$ axis should be $\pi$. This makes it easy to get an expression for $x$ and $y$ using vectors. If the circle were at the origin, then we would have $\vec r = (a\,cos(\alpha),a\,sin(\alpha)=(a\,cos(2\theta), a\,sin(2\theta)$, to which we add the offset vector $(a,0)$ to obtain $\vec r = (a(1+cos(2\theta), a\,sin(2\theta)$. I did this another way to confirm, by using the law of cosines, and it seems correct from that standpoint as well. If the angle between the two sides of the isosceles triangle is $\pi - 2\theta$, then $r$ should be $\sqrt{2a^2(1-cos(\pi - 2\theta)}$, or equivalently, $\sqrt{2a^2(1+cos(2\theta))}$, since $cos(\pi-x)=-cos(x)$. Then, the unit vector extending in the direction of the ray from the origin is $\hat u = (cos(\theta), sin(\theta))$, and the ray should be $r \hat u$. I graphed both of these representations for the $x$ coordinate, namely, $x=a(1+cos(2\theta))$ and $x=cos(\theta) \sqrt{2a^2(1+cos(2\theta))}$, and they are equivalent on $-\frac{\pi}{2} \le x \le \frac{\pi}{2}$. Finally, I set up the integral: $$\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \int_0^{\sqrt{2a^2(1+cos(2\theta))}}r(4a(1+cos(2\theta)))\,dr\,d\theta$$ Evaluating the inner integral leaves: $$\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} 4a^3(1+cos(2\theta))^2\,d\theta = 6 \pi a^3$$ Interestingly, if I evaluate this integral without the square, i.e., $$\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} 4a^3(1+cos(2\theta))\,d\theta$$ then I get the correct answer, although I have no clue at this point how I would get such an expression. Any insight would be much appreciated.",,"['integration', 'multivariable-calculus', 'polar-coordinates', 'greens-theorem']"
6,Understanding the mathematics behind backpropagation,Understanding the mathematics behind backpropagation,,"I have just finished watching a lecture from Patrick Winston's AI course . In an attempt to understand the mathematics behind back-propagation, I have formulated a simplistic neural network as follows: Task Given an input $x \in \{0, 1\}$, train a simple neural network to mimic the identity function, that is $f(x) = x$. Definitions $x$ = the input $T$ = the target function; in this case $T(x) = x$ $y$ = the desired output; $y = T(x)$ $A$ = the activation function; I'll use $A(x) = \frac{1}{1 + e^{-x}}$ $\hat{T}$ = the feed-forward function $\hat{y}$ = the output of the network; $\hat{y} = \hat{T}(x)$ $E$ = the error function; I'll use $E(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^2$ $w$ = a weight $\in [0, 1]$ $\alpha$ = the learning rate Note that $\hat{T}(x) = A(wx)$ in this very simplistic example. Formulating back-propagation via gradient descent In a particular iteration $i$ of this network, let $x_i \in \{0, 1\}$ and $\hat{y}_i = \hat{T}(x_i)$. Now, the weight $w_i$ of this network needs to be adjusted such that $E(\hat{y}_i, y_i) > E(\hat{y}_{i+1}, y_{i+1})$. So, $\frac{\partial{E}}{\partial{w_i}}$ will give the rate of change function of $E$ with respect to $w_i$. Computing this partial: \begin{align} \tag{1}\frac{\partial{E}}{\partial{w_i}} &= \frac{\partial{E}}{\partial{\hat{y}_i}} \frac{\partial{\hat{y}_i}}{\partial{w}_i} + \frac{\partial{E}}{\partial{y_i}} \frac{\partial{y_i}}{\partial{w}_i}\\ \tag{2}&= (\hat{y}_i - y_i) \cdot \frac{\partial{\hat{y}_i}}{\partial{w}_i} - (\hat{y}_i - y_i) \cdot 0\\ \tag{3}&= (\hat{y}_i - y_i) \cdot \frac{\partial{\hat{y}_i}}{\partial{w}_i}\\ \tag{4}&= (\hat{y}_i - y_i) \cdot \frac{\partial{\hat{T}}}{\partial{w}_i}\\ \tag{5}&= (\hat{y}_i - y_i) \cdot \frac{\partial{A(w_ix_i)}}{\partial{w}_i}\\ \tag{6}&= (\hat{y}_i - y_i) \cdot x_i \cdot (1 - A(w_ix_i)) \cdot A(w_ix_i) \end{align} The confusion How does finding $\frac{\partial{E}}{\partial{w_i}}$ help in the gradient descent algorithm in this case? I only understand that the gradient of a function at a point returns a vector pointing in the direction of the greatest incline. How can the weight $w_i$ be updated in such a manner?","I have just finished watching a lecture from Patrick Winston's AI course . In an attempt to understand the mathematics behind back-propagation, I have formulated a simplistic neural network as follows: Task Given an input $x \in \{0, 1\}$, train a simple neural network to mimic the identity function, that is $f(x) = x$. Definitions $x$ = the input $T$ = the target function; in this case $T(x) = x$ $y$ = the desired output; $y = T(x)$ $A$ = the activation function; I'll use $A(x) = \frac{1}{1 + e^{-x}}$ $\hat{T}$ = the feed-forward function $\hat{y}$ = the output of the network; $\hat{y} = \hat{T}(x)$ $E$ = the error function; I'll use $E(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^2$ $w$ = a weight $\in [0, 1]$ $\alpha$ = the learning rate Note that $\hat{T}(x) = A(wx)$ in this very simplistic example. Formulating back-propagation via gradient descent In a particular iteration $i$ of this network, let $x_i \in \{0, 1\}$ and $\hat{y}_i = \hat{T}(x_i)$. Now, the weight $w_i$ of this network needs to be adjusted such that $E(\hat{y}_i, y_i) > E(\hat{y}_{i+1}, y_{i+1})$. So, $\frac{\partial{E}}{\partial{w_i}}$ will give the rate of change function of $E$ with respect to $w_i$. Computing this partial: \begin{align} \tag{1}\frac{\partial{E}}{\partial{w_i}} &= \frac{\partial{E}}{\partial{\hat{y}_i}} \frac{\partial{\hat{y}_i}}{\partial{w}_i} + \frac{\partial{E}}{\partial{y_i}} \frac{\partial{y_i}}{\partial{w}_i}\\ \tag{2}&= (\hat{y}_i - y_i) \cdot \frac{\partial{\hat{y}_i}}{\partial{w}_i} - (\hat{y}_i - y_i) \cdot 0\\ \tag{3}&= (\hat{y}_i - y_i) \cdot \frac{\partial{\hat{y}_i}}{\partial{w}_i}\\ \tag{4}&= (\hat{y}_i - y_i) \cdot \frac{\partial{\hat{T}}}{\partial{w}_i}\\ \tag{5}&= (\hat{y}_i - y_i) \cdot \frac{\partial{A(w_ix_i)}}{\partial{w}_i}\\ \tag{6}&= (\hat{y}_i - y_i) \cdot x_i \cdot (1 - A(w_ix_i)) \cdot A(w_ix_i) \end{align} The confusion How does finding $\frac{\partial{E}}{\partial{w_i}}$ help in the gradient descent algorithm in this case? I only understand that the gradient of a function at a point returns a vector pointing in the direction of the greatest incline. How can the weight $w_i$ be updated in such a manner?",,"['multivariable-calculus', 'neural-networks', 'gradient-descent']"
7,Normal vector to a regular star shaped region,Normal vector to a regular star shaped region,,"Let $U\subset \mathbb{R}^N$ be a star-shaped region (with respect to the origin $0$) with $C^1$ bundary. Can you give a detailed proof of the following fact? if $x\in\partial U$ then for each $\varepsilon >0$ there exists $\delta >0$ such that $|y-x|<\delta$ and $y\in\bar{U}$ imply    $$\nu(x)\cdot\frac{y-x}{|y-x|}\le\varepsilon.$$ We say that $\Omega \subset \mathbb{R}^N$ has $C^k$ boundary if, for all $x \in \partial \Omega$, there exists a neighborhood $O$ of $x$ and a map $T:Q \to O$ which is bijective, of class $C^k$ and with inverse of class $C^k$ and such that $$T(Q_+) = O \cap \Omega$$ $$T(Q_0)= O \cap \partial \Omega,$$ where we have denoted  $$Q=\{ (x',x_N) \in \mathbb{R}^{N-1}\times \mathbb{R}, |x'|< 1, |x_N|< 1\}$$ $$Q_+=\{ x \in Q: x_n >0\}$$ $$Q_0=\{ x \in Q: x_n =0\}$$","Let $U\subset \mathbb{R}^N$ be a star-shaped region (with respect to the origin $0$) with $C^1$ bundary. Can you give a detailed proof of the following fact? if $x\in\partial U$ then for each $\varepsilon >0$ there exists $\delta >0$ such that $|y-x|<\delta$ and $y\in\bar{U}$ imply    $$\nu(x)\cdot\frac{y-x}{|y-x|}\le\varepsilon.$$ We say that $\Omega \subset \mathbb{R}^N$ has $C^k$ boundary if, for all $x \in \partial \Omega$, there exists a neighborhood $O$ of $x$ and a map $T:Q \to O$ which is bijective, of class $C^k$ and with inverse of class $C^k$ and such that $$T(Q_+) = O \cap \Omega$$ $$T(Q_0)= O \cap \partial \Omega,$$ where we have denoted  $$Q=\{ (x',x_N) \in \mathbb{R}^{N-1}\times \mathbb{R}, |x'|< 1, |x_N|< 1\}$$ $$Q_+=\{ x \in Q: x_n >0\}$$ $$Q_0=\{ x \in Q: x_n =0\}$$",,"['calculus', 'real-analysis']"
8,Area of surface using polar coordinates,Area of surface using polar coordinates,,"Calculate the area of the surface $z=x+y$ that is inside the cylinder $x^2+y^4 = 4$. I was able to find the correct answer by calculating the normal vector (using cross product) at each point on the surface parametrized: $$ \vec{n} = (-r)\vec{i}+(-r)\vec{j}+(r)\vec{k} $$ And then I used polar coordinates to integrate the domain of the parametrized surface: $$ \int_0^{2\pi}\int_0^2 ||\vec{n}|| \text{ } dr \text{ } d\theta = \sqrt{3}\int_0^{2\pi}\int_0^2  r\text{ } dr \text{ } d\theta = 4\pi\sqrt{3} $$ But after that I was asking myself why changing $drd\theta$ to the infinetesimal element of area $r dr d\theta$ is not giving me the correct answer. If I do that, I'll get my integral to be: $$ \sqrt{3}\int_0^{2\pi}\int_0^2  r^2\text{ } dr \text{ } d\theta \neq 4\pi\sqrt{3} $$ When we change our double integral from cartesian coordinates to polar coordinates, the infinetesimal element of area is changed to $dA = rdrd\theta$. But when I do that I get the wrong answer?! What am I doing that is not correct? Thanks!","Calculate the area of the surface $z=x+y$ that is inside the cylinder $x^2+y^4 = 4$. I was able to find the correct answer by calculating the normal vector (using cross product) at each point on the surface parametrized: $$ \vec{n} = (-r)\vec{i}+(-r)\vec{j}+(r)\vec{k} $$ And then I used polar coordinates to integrate the domain of the parametrized surface: $$ \int_0^{2\pi}\int_0^2 ||\vec{n}|| \text{ } dr \text{ } d\theta = \sqrt{3}\int_0^{2\pi}\int_0^2  r\text{ } dr \text{ } d\theta = 4\pi\sqrt{3} $$ But after that I was asking myself why changing $drd\theta$ to the infinetesimal element of area $r dr d\theta$ is not giving me the correct answer. If I do that, I'll get my integral to be: $$ \sqrt{3}\int_0^{2\pi}\int_0^2  r^2\text{ } dr \text{ } d\theta \neq 4\pi\sqrt{3} $$ When we change our double integral from cartesian coordinates to polar coordinates, the infinetesimal element of area is changed to $dA = rdrd\theta$. But when I do that I get the wrong answer?! What am I doing that is not correct? Thanks!",,"['multivariable-calculus', 'polar-coordinates', 'surface-integrals']"
9,"How do I solve $\int_{C}\frac{-y\,dx+(x-1)\,dy}{(x-1)^2+y^2} $?",How do I solve ?,"\int_{C}\frac{-y\,dx+(x-1)\,dy}{(x-1)^2+y^2} ","If $$\gamma=(2\cos(t)+4\cos^2(t) , 2\sin(t)+4\sin(t)\cos(t))$$ and $C$ is a path defined by $\gamma$ when $\frac{-2\pi}{3} \le t\le \frac{4\pi}{3}$. calculate: $$\int_{C}\frac{-y\,dx+(x-1)\,dy}{(x-1)^2+y^2} $$ first of all, I was given a hint that $\gamma$ can be changed to polar coordinates, but it seems to me already in these coordinates. am I missing something? second, for calculating the integral I thought to use that: $$\int_{C}f\,dr=\int_{0}^1f(\gamma(t)\cdot\gamma'(t)\,dt$$ but with current parametrization it is too messy. any ideas how to approach this question? edit: I draw the function using matlab: https://i.sstatic.net/WkzFI.png","If $$\gamma=(2\cos(t)+4\cos^2(t) , 2\sin(t)+4\sin(t)\cos(t))$$ and $C$ is a path defined by $\gamma$ when $\frac{-2\pi}{3} \le t\le \frac{4\pi}{3}$. calculate: $$\int_{C}\frac{-y\,dx+(x-1)\,dy}{(x-1)^2+y^2} $$ first of all, I was given a hint that $\gamma$ can be changed to polar coordinates, but it seems to me already in these coordinates. am I missing something? second, for calculating the integral I thought to use that: $$\int_{C}f\,dr=\int_{0}^1f(\gamma(t)\cdot\gamma'(t)\,dt$$ but with current parametrization it is too messy. any ideas how to approach this question? edit: I draw the function using matlab: https://i.sstatic.net/WkzFI.png",,"['integration', 'multivariable-calculus', 'line-integrals']"
10,Jacobian of a Matrix Product,Jacobian of a Matrix Product,,"I have a vector $x$ and I'm trying to find $\frac{dy}{dx}$, where $y=x^{T}x$. I believe the answer should be $2x^{T}$ but I'm trying to understand why. Does the product rule apply to matrices/vectors being multiplied? The issue I run into is the following: If I use the product rule blindly (which I don't know if I'm allowed to do), I get: $\frac{dy}{dx} = \frac{dx^{T}}{dx}x+x^{T}\frac{dx}{dx}$ $\frac{dx^{T}}{dx} = \frac{dx}{dx} = I$ So I get: $\frac{dy}{dx} = x+x^{T}$ But this makes no sense since the dimensions of the vectors don't match. If one of them were transposed then I get 2x and everything would be fine. Any help would be greatly appreciated. Thanks!","I have a vector $x$ and I'm trying to find $\frac{dy}{dx}$, where $y=x^{T}x$. I believe the answer should be $2x^{T}$ but I'm trying to understand why. Does the product rule apply to matrices/vectors being multiplied? The issue I run into is the following: If I use the product rule blindly (which I don't know if I'm allowed to do), I get: $\frac{dy}{dx} = \frac{dx^{T}}{dx}x+x^{T}\frac{dx}{dx}$ $\frac{dx^{T}}{dx} = \frac{dx}{dx} = I$ So I get: $\frac{dy}{dx} = x+x^{T}$ But this makes no sense since the dimensions of the vectors don't match. If one of them were transposed then I get 2x and everything would be fine. Any help would be greatly appreciated. Thanks!",,"['linear-algebra', 'multivariable-calculus', 'vector-analysis', 'matrix-calculus', 'multivalued-functions']"
11,Calculus and Geometry Word Problem,Calculus and Geometry Word Problem,,"The Chemist's Dilemma: Mary, the chemist, is making a solution in her laboratory, pouring ChemA into ChemB. Mary pours 1 mg of ChemA per second into an inverted 60º cone of vertical length .5 meters containing 10 g of ChemB for 1 minute; however, she does not notice a hole at the bottom of the cone that lets 1 mg of the solution in the cone out every second. When Mary's finished, what percentage of the solution is ChemB? [Assume that ChemA and ChemB mix instantaneously and fully on touch] In my attempts to solve this problem, I've attempted to construct an integral that would include a function determining the current percentages of ChemA and ChemB in the solution so I could calculate how much of each would be left after losing the 1 mg; however, I soon realized that function, which seemed fairly necessary to me, was actually just the original integral I was trying to find. I didn't know what to do at this recursion so I stopped there, but if anyone cares to help me (I came up with this putzing around and would just love to know the answer), it'd be greatly appreciated.","The Chemist's Dilemma: Mary, the chemist, is making a solution in her laboratory, pouring ChemA into ChemB. Mary pours 1 mg of ChemA per second into an inverted 60º cone of vertical length .5 meters containing 10 g of ChemB for 1 minute; however, she does not notice a hole at the bottom of the cone that lets 1 mg of the solution in the cone out every second. When Mary's finished, what percentage of the solution is ChemB? [Assume that ChemA and ChemB mix instantaneously and fully on touch] In my attempts to solve this problem, I've attempted to construct an integral that would include a function determining the current percentages of ChemA and ChemB in the solution so I could calculate how much of each would be left after losing the 1 mg; however, I soon realized that function, which seemed fairly necessary to me, was actually just the original integral I was trying to find. I didn't know what to do at this recursion so I stopped there, but if anyone cares to help me (I came up with this putzing around and would just love to know the answer), it'd be greatly appreciated.",,"['calculus', 'geometry', 'multivariable-calculus']"
12,Problematic definition of pullbacks of covector fields in Spivak's Calculus on Manifolds,Problematic definition of pullbacks of covector fields in Spivak's Calculus on Manifolds,,"This is a follow-up question to a recent one: What is a $k$-form on $[0,1]^k$ in Spivak's Calculus on Manifolds? In Spivak's Calculus on Manifolds , he defines the integration on chains as follows: where $A$ is some subset of ${\bf R}^n$. In this definition, the pullback $c^*\omega$ is very confusing for me. Let me write down how Spivak developed the definitions. Suppose $f:{\bf R}^n\to{\bf R}^m$ is a differentiable function and $\omega$ is a $k$-form on ${\bf R}^m$. Then $f^*\omega$ is defined as a $k$-form on ${\bf R}^n$ by $$ (f^*\omega)_p=f^*(\omega_{f(p)}) $$ which means if $v_1,\cdots,v_k\in{\bf R}^n_p$, then  $$ (f^*\omega)_p(v_1,\cdots,v_k)=\omega_{f(p)}(df_p(v_1),\cdots, df_p(v_k)). $$ Here I have changed Spivak's notation $(f^*\omega)(p)$ to $(f^*\omega)_p$ to make the notation ""clearer"". On the other hand, a singular $k$-cube $c$ in $A\subset{\bf R}^n$ is defined as a continuous function $c:[0,1]^k\to A.$ (In fact, Spivak only defines the $k$-cube for $k=n$). Here comes my question : The domain of $c$, namely $[0,1]^k$, is a closed subset of ${\bf R}^k$. one cannot even talk about $dc_p$ for $p$ on the boundary of $[0,1]^k$. How should I make sense of the notation $c^*\omega$ in the quoted definition of integration on chains?","This is a follow-up question to a recent one: What is a $k$-form on $[0,1]^k$ in Spivak's Calculus on Manifolds? In Spivak's Calculus on Manifolds , he defines the integration on chains as follows: where $A$ is some subset of ${\bf R}^n$. In this definition, the pullback $c^*\omega$ is very confusing for me. Let me write down how Spivak developed the definitions. Suppose $f:{\bf R}^n\to{\bf R}^m$ is a differentiable function and $\omega$ is a $k$-form on ${\bf R}^m$. Then $f^*\omega$ is defined as a $k$-form on ${\bf R}^n$ by $$ (f^*\omega)_p=f^*(\omega_{f(p)}) $$ which means if $v_1,\cdots,v_k\in{\bf R}^n_p$, then  $$ (f^*\omega)_p(v_1,\cdots,v_k)=\omega_{f(p)}(df_p(v_1),\cdots, df_p(v_k)). $$ Here I have changed Spivak's notation $(f^*\omega)(p)$ to $(f^*\omega)_p$ to make the notation ""clearer"". On the other hand, a singular $k$-cube $c$ in $A\subset{\bf R}^n$ is defined as a continuous function $c:[0,1]^k\to A.$ (In fact, Spivak only defines the $k$-cube for $k=n$). Here comes my question : The domain of $c$, namely $[0,1]^k$, is a closed subset of ${\bf R}^k$. one cannot even talk about $dc_p$ for $p$ on the boundary of $[0,1]^k$. How should I make sense of the notation $c^*\omega$ in the quoted definition of integration on chains?",,['multivariable-calculus']
13,The integral for gravitational potential of a uniform planet,The integral for gravitational potential of a uniform planet,,"The gravitational potential for a uniform planet is given by the integral $$V(X,Y,Z)= \int \frac{G\rho(x,y,z)}{\xi(x-X, y-Y, z-Z)}dxdydz$$ where: $G$ is the gravitation constant $\xi$ is distance from the center equal to  $\sqrt{X^2+Y^2+Z^2}$ $(x,y,z)$ is the center of the planet at $(0,0,0)$ $(X,Y,Z)$ are the coordinates of a point $P$ outside the planet. The gravitational potential to be calculated is the force that is felt by an object at $P$ $\space$ My question is in one of the steps our professor provides in coming up with this integral. Since the shape is a sphere, I know that the Volume of a sphere in polar coordinates is: $$V=\int r^2\sin\theta drd\theta d\phi$$ This next step is where I need the clarification. The notes say: Now we are going to make a clever change in coordinates, replacing $\theta$ with $R$. A famous triangular relation says that $$R^2=r^2 +\xi^2+2r\xi \cos\theta,$$ where $\xi$ is the distance from point P to the center of the planet. The differential keeping r fixed is $$2RdR - 2r\xi \sin\theta d\theta.$$  so, in the volume element replace $$\sin\theta d\theta \rightarrow \frac{R}{r\xi}dR$$ Hence $$\frac{r^2\sin\theta drd\theta d\phi}{R}=\frac{1}{\xi}rdrdRd\phi$$ My main concern is how he arrived to the differential above, and why he replaces $sin\theta d\theta$.  If you have any info on the triangulation, that would be helpful but not necessary.","The gravitational potential for a uniform planet is given by the integral $$V(X,Y,Z)= \int \frac{G\rho(x,y,z)}{\xi(x-X, y-Y, z-Z)}dxdydz$$ where: $G$ is the gravitation constant $\xi$ is distance from the center equal to  $\sqrt{X^2+Y^2+Z^2}$ $(x,y,z)$ is the center of the planet at $(0,0,0)$ $(X,Y,Z)$ are the coordinates of a point $P$ outside the planet. The gravitational potential to be calculated is the force that is felt by an object at $P$ $\space$ My question is in one of the steps our professor provides in coming up with this integral. Since the shape is a sphere, I know that the Volume of a sphere in polar coordinates is: $$V=\int r^2\sin\theta drd\theta d\phi$$ This next step is where I need the clarification. The notes say: Now we are going to make a clever change in coordinates, replacing $\theta$ with $R$. A famous triangular relation says that $$R^2=r^2 +\xi^2+2r\xi \cos\theta,$$ where $\xi$ is the distance from point P to the center of the planet. The differential keeping r fixed is $$2RdR - 2r\xi \sin\theta d\theta.$$  so, in the volume element replace $$\sin\theta d\theta \rightarrow \frac{R}{r\xi}dR$$ Hence $$\frac{r^2\sin\theta drd\theta d\phi}{R}=\frac{1}{\xi}rdrdRd\phi$$ My main concern is how he arrived to the differential above, and why he replaces $sin\theta d\theta$.  If you have any info on the triangulation, that would be helpful but not necessary.",,"['integration', 'multivariable-calculus', 'mathematical-physics', 'polar-coordinates', 'multiple-integral']"
14,Derivative of map restricted to subset,Derivative of map restricted to subset,,"I was recently doing an exercise and in the exercise I was given the map $\phi: \mathbb{R}^4 \rightarrow \mathbb{R}^2$ defined by $\phi(x,y,s,t) = (x^2 + y, x^2 + y^2 + s^2 + t^2 + y)$. Let $S = \phi^{-1}(0,1)$. I wanted to show that $S$ was a submanifold of $\mathbb{R}^4$ and so I needed to show that the differential $d\phi$ was surjective at every point $(x,y,s,t) \in S$. What I did was calculate its differential as $$ d\phi = \begin{pmatrix} 2x & 1 & 0 & 0 \\ 2x & 2y + 1 & 2s & 2t \end{pmatrix} $$ Now, let's consider the map $\phi: S \rightarrow \mathbb{R}^2$. Then $\phi(x,y,s,t) = (0,1)$ on $S$. So if I naively calculate the differential here, I get the differential is the zero matrix, which obviously does not agree with my previous calculation. I kind of figure that this is because when originally calculating the differential, $(x,y,s,t)$ ranged over all of $\mathbb{R}^4$, but for the map $\phi: S \rightarrow \mathbb{R}^2$, the points $(x,y,s,t)$ are restricted to only points on $S$. However, shouldn't $d\phi = d(\phi|_S)$ because calculating the differential shouldn't depend on the path taken by the points $(x,y,s,t)$, and so even when restricting my points to $S$, I should get $d\phi = d(\phi|_S)$? I hope my question is clear. EDIT: I can see that a matrix representation of the differential is the Jacobian of $\phi|_S$ in local coordinates. Since these local coordinates obviously cannot be standard coordinates on $\mathbb{R}^4$, I can't just differentiate $\phi$ then restrict the derivative to $S$. I believe this answers my own question, but I will leave this open for further discussion in the case that the more knowledgeable posters would like to contribute.","I was recently doing an exercise and in the exercise I was given the map $\phi: \mathbb{R}^4 \rightarrow \mathbb{R}^2$ defined by $\phi(x,y,s,t) = (x^2 + y, x^2 + y^2 + s^2 + t^2 + y)$. Let $S = \phi^{-1}(0,1)$. I wanted to show that $S$ was a submanifold of $\mathbb{R}^4$ and so I needed to show that the differential $d\phi$ was surjective at every point $(x,y,s,t) \in S$. What I did was calculate its differential as $$ d\phi = \begin{pmatrix} 2x & 1 & 0 & 0 \\ 2x & 2y + 1 & 2s & 2t \end{pmatrix} $$ Now, let's consider the map $\phi: S \rightarrow \mathbb{R}^2$. Then $\phi(x,y,s,t) = (0,1)$ on $S$. So if I naively calculate the differential here, I get the differential is the zero matrix, which obviously does not agree with my previous calculation. I kind of figure that this is because when originally calculating the differential, $(x,y,s,t)$ ranged over all of $\mathbb{R}^4$, but for the map $\phi: S \rightarrow \mathbb{R}^2$, the points $(x,y,s,t)$ are restricted to only points on $S$. However, shouldn't $d\phi = d(\phi|_S)$ because calculating the differential shouldn't depend on the path taken by the points $(x,y,s,t)$, and so even when restricting my points to $S$, I should get $d\phi = d(\phi|_S)$? I hope my question is clear. EDIT: I can see that a matrix representation of the differential is the Jacobian of $\phi|_S$ in local coordinates. Since these local coordinates obviously cannot be standard coordinates on $\mathbb{R}^4$, I can't just differentiate $\phi$ then restrict the derivative to $S$. I believe this answers my own question, but I will leave this open for further discussion in the case that the more knowledgeable posters would like to contribute.",,"['multivariable-calculus', 'differential-geometry', 'differential-topology']"
15,Exchanging and dividing the order of double summations,Exchanging and dividing the order of double summations,,"I want to divide the following equation into two independent parts. $$ S_n=\sum_{i=0}^{n-1}\sum_{j=0}^{i-1}\alpha^{i-j-1}\omega_{j}, \qquad S_0=0 $$ Here, $\omega_{j}$ is a Gaussian process with zero mean and unit variance. I want to derive the variance of $S_n$. First, I tried to divide these two summations like, $$ \frac{1}{\alpha}\sum_{i=0}^{n-1}\alpha^{i}\sum_{j=0}^{i-1}\alpha^{-j}\omega_{j} $$ I'm wondering if I can simply just divide like above or not. Also, I tried to put specific $i$ and $j$ from $0$. Is is possible to calculate $$ \sum_{j=0}^{-1}\alpha^{-j} $$ which is the situation that $i=0$ in double summation. I appreciate any help that you can provide.","I want to divide the following equation into two independent parts. $$ S_n=\sum_{i=0}^{n-1}\sum_{j=0}^{i-1}\alpha^{i-j-1}\omega_{j}, \qquad S_0=0 $$ Here, $\omega_{j}$ is a Gaussian process with zero mean and unit variance. I want to derive the variance of $S_n$. First, I tried to divide these two summations like, $$ \frac{1}{\alpha}\sum_{i=0}^{n-1}\alpha^{i}\sum_{j=0}^{i-1}\alpha^{-j}\omega_{j} $$ I'm wondering if I can simply just divide like above or not. Also, I tried to put specific $i$ and $j$ from $0$. Is is possible to calculate $$ \sum_{j=0}^{-1}\alpha^{-j} $$ which is the situation that $i=0$ in double summation. I appreciate any help that you can provide.",,"['probability', 'multivariable-calculus', 'summation', 'variance']"
16,Find the absolute max and min of f subject to the given constraint,Find the absolute max and min of f subject to the given constraint,,"$f(x)=4x^4+y^4$ subject to the constraint $x^2+y^2=1$ My attempt at it was by finding partial derivitives: $$f_x=16x^3 =0$$ $$f_y=4y^3=0$$ this means that we have an interior point $(0,0)$ Now Let $x=\cos(t)$, $y=\sin(t)$ $$f(\cos(t),\sin(t))=4\cos^4t+\sin^4t$$ $$<=> 3\cos^4t+1$$ we can now note that $f$ has max when $t=0,\pi$, or $2\pi $ and min when $t= \pi/t +k\pi)$ this means that we have the following potential points  $f(1,0),f(-1,0), f(0,1), f(0,-1), f(0,0)$ $$f(0,0)=0$$ $$f(1,0)=4$$ $$f(0,1)=1$$ Max =4 Min = 0 Anyone can help verifying my answer, because i tried the lagrange way and it didnt work","$f(x)=4x^4+y^4$ subject to the constraint $x^2+y^2=1$ My attempt at it was by finding partial derivitives: $$f_x=16x^3 =0$$ $$f_y=4y^3=0$$ this means that we have an interior point $(0,0)$ Now Let $x=\cos(t)$, $y=\sin(t)$ $$f(\cos(t),\sin(t))=4\cos^4t+\sin^4t$$ $$<=> 3\cos^4t+1$$ we can now note that $f$ has max when $t=0,\pi$, or $2\pi $ and min when $t= \pi/t +k\pi)$ this means that we have the following potential points  $f(1,0),f(-1,0), f(0,1), f(0,-1), f(0,0)$ $$f(0,0)=0$$ $$f(1,0)=4$$ $$f(0,1)=1$$ Max =4 Min = 0 Anyone can help verifying my answer, because i tried the lagrange way and it didnt work",,"['multivariable-calculus', 'optimization']"
17,Calculation of $\int_{\mathbb R^n} e^{it|\xi + \frac{x}{2t}|^2} d \xi$ [closed],Calculation of  [closed],\int_{\mathbb R^n} e^{it|\xi + \frac{x}{2t}|^2} d \xi,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $t > 0$ and $x \in \mathbb R^n$. I want to derive the following integral value $$ \int_{\mathbb R^n} \exp\left(it\left|\xi +\frac{x}{2t}\right|^2\right) d \xi $$ How can I calculate this?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $t > 0$ and $x \in \mathbb R^n$. I want to derive the following integral value $$ \int_{\mathbb R^n} \exp\left(it\left|\xi +\frac{x}{2t}\right|^2\right) d \xi $$ How can I calculate this?",,"['calculus', 'integration', 'complex-analysis', 'multivariable-calculus', 'multivalued-functions']"
18,Exact vs Total differential,Exact vs Total differential,,I'm not sure I understand the difference between the two. Is it correct to claim that a total differential is always exact but not vice versa? Thanks,I'm not sure I understand the difference between the two. Is it correct to claim that a total differential is always exact but not vice versa? Thanks,,['multivariable-calculus']
19,"If $f'(0)$ doesn't have eigenvalue $1$, then there is a neighborhood where $f(x)\neq x$","If  doesn't have eigenvalue , then there is a neighborhood where",f'(0) 1 f(x)\neq x,"Let $f:\mathbb{R}^m\to\mathbb{R}^m$, differentiable, with $f(0) = 0$.   If the linear transformation $f'(0)$ has no eigenvalue $1$, prove that   there exists a neighborhood $V$ of $0$ in $\mathbb{R}^m$ such that   $f(x)\neq x$ for all $x\in V-\{0\}$ My book solves it by the following: Since the operator $f'(0)$ has no fixed point in the compact   $S^{n-1}$, there exists $\epsilon>0$ such that $|u|=1\implies  |f'(0)\cdot u -u|\ge \epsilon$. Being $f$ differentiable, with $f(0) =  0$, we have $f(x) = f'(0)\cdot x + p(x)\cdot |x| = |x|\left(f'(0)\cdot  \frac{x}{|x|}+p(x)\right)$ and exists $\delta >0 $ such that   $0<|x|<\delta \implies |p(x)|<\epsilon$ (why? - 1) . So, if   $0<|x|<\delta$, then $|f(x)-x|\ge |x|\left[\left|f'(x)\cdot  \frac{x}{|x|}-\frac{x}{|x|}\right|-|p(x)|\right]>0$ and then $f(x)\neq  x$ (why? - 2) Also, what the compact $S^{n-1}$ have to do with all of this?","Let $f:\mathbb{R}^m\to\mathbb{R}^m$, differentiable, with $f(0) = 0$.   If the linear transformation $f'(0)$ has no eigenvalue $1$, prove that   there exists a neighborhood $V$ of $0$ in $\mathbb{R}^m$ such that   $f(x)\neq x$ for all $x\in V-\{0\}$ My book solves it by the following: Since the operator $f'(0)$ has no fixed point in the compact   $S^{n-1}$, there exists $\epsilon>0$ such that $|u|=1\implies  |f'(0)\cdot u -u|\ge \epsilon$. Being $f$ differentiable, with $f(0) =  0$, we have $f(x) = f'(0)\cdot x + p(x)\cdot |x| = |x|\left(f'(0)\cdot  \frac{x}{|x|}+p(x)\right)$ and exists $\delta >0 $ such that   $0<|x|<\delta \implies |p(x)|<\epsilon$ (why? - 1) . So, if   $0<|x|<\delta$, then $|f(x)-x|\ge |x|\left[\left|f'(x)\cdot  \frac{x}{|x|}-\frac{x}{|x|}\right|-|p(x)|\right]>0$ and then $f(x)\neq  x$ (why? - 2) Also, what the compact $S^{n-1}$ have to do with all of this?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives']"
20,Find all points where the function is differentiable and calculate its derivative,Find all points where the function is differentiable and calculate its derivative,,"I need help with this: I need to find all points where this function is differentiable: $f:\mathbb{R}^{3}\rightarrow \mathbb{R}, \begin{pmatrix}x\\ y\\ z\end{pmatrix} \mapsto e^x \sin (z) + z \cos\bigl( \sqrt{x^2 + y^2 +1}\,\bigr).$ After that I must calculate the derivatives at that points. This is a part of an exercise in my analysis class. It is the first part of a larger exercise but i need to understand this first to do the rest. I know so far that to calculate the derivatives i need to show that the function is continous partial differentiable. But I don't know how to find all these point and do it for them. Can you give me a hint?","I need help with this: I need to find all points where this function is differentiable: $f:\mathbb{R}^{3}\rightarrow \mathbb{R}, \begin{pmatrix}x\\ y\\ z\end{pmatrix} \mapsto e^x \sin (z) + z \cos\bigl( \sqrt{x^2 + y^2 +1}\,\bigr).$ After that I must calculate the derivatives at that points. This is a part of an exercise in my analysis class. It is the first part of a larger exercise but i need to understand this first to do the rest. I know so far that to calculate the derivatives i need to show that the function is continous partial differentiable. But I don't know how to find all these point and do it for them. Can you give me a hint?",,['multivariable-calculus']
21,"Redefining $f(x,y) = x \cdot y \cdot \sin(1/x)\cdot \sin(1/y)$",Redefining,"f(x,y) = x \cdot y \cdot \sin(1/x)\cdot \sin(1/y)","I'm analysing this function from my calculus class: $$f(x,y) = x \cdot y \cdot \sin(1/x)\cdot \sin(1/y)$$ I want to know if it's possible to redefine this function to all of $\Bbb R^2$ so it's continuous. Will it suffice to make it equal to 0 along both axis?","I'm analysing this function from my calculus class: $$f(x,y) = x \cdot y \cdot \sin(1/x)\cdot \sin(1/y)$$ I want to know if it's possible to redefine this function to all of $\Bbb R^2$ so it's continuous. Will it suffice to make it equal to 0 along both axis?",,"['limits', 'multivariable-calculus', 'continuity', 'analytic-continuation']"
22,Compute the gradient of a vector,Compute the gradient of a vector,,"I have to compute the following expression: $$ \frac{ \mathrm{d} (\mathbf{x}- \mathbf{\mu})^T \Sigma^{-1} (\mathbf{x}- \mathbf{\mu})}{\mathrm{d} \mathbf{\mu}} $$ where $\mathbf{x}$ and $\mathbf{\mu}$ are a column vectors, $\Sigma^{-1}$ is a matrix. I tried to do it component-wise and decomposing the matrix product in sum of products: $$ \frac{ \mathrm{d} \sum_k (\sum_j (\mathbf{x}_j - \mathbf{\mu}_j) {\Sigma^{-1}}_{j})_k (\mathbf{x}_k - \mathbf{\mu}_k)}{\mathrm{d} \mathbf{\mu}} $$ and then selecting only one component of $\mathbf{\mu}$: $$ \frac{ \mathrm{d} \sum_k (\sum_j (\mathbf{x}_j - \mathbf{\mu}_j) {\Sigma^{-1}}_{j})_k (\mathbf{x}_k - \mathbf{\mu}_k)}{\mathrm{d} \mathbf{\mu}_k}  =  \mathbf{\mu}_k (\sum_j (\mathbf{x}_j - \mathbf{\mu}_j) {\Sigma^{-1}}_{j})_k - (\mathbf{x}_k - \mathbf{\mu}_k) {\mathbf{\mu}_k \Sigma^{-1}}_k $$ What is the best way to compute this derivation? Could you show the passages?","I have to compute the following expression: $$ \frac{ \mathrm{d} (\mathbf{x}- \mathbf{\mu})^T \Sigma^{-1} (\mathbf{x}- \mathbf{\mu})}{\mathrm{d} \mathbf{\mu}} $$ where $\mathbf{x}$ and $\mathbf{\mu}$ are a column vectors, $\Sigma^{-1}$ is a matrix. I tried to do it component-wise and decomposing the matrix product in sum of products: $$ \frac{ \mathrm{d} \sum_k (\sum_j (\mathbf{x}_j - \mathbf{\mu}_j) {\Sigma^{-1}}_{j})_k (\mathbf{x}_k - \mathbf{\mu}_k)}{\mathrm{d} \mathbf{\mu}} $$ and then selecting only one component of $\mathbf{\mu}$: $$ \frac{ \mathrm{d} \sum_k (\sum_j (\mathbf{x}_j - \mathbf{\mu}_j) {\Sigma^{-1}}_{j})_k (\mathbf{x}_k - \mathbf{\mu}_k)}{\mathrm{d} \mathbf{\mu}_k}  =  \mathbf{\mu}_k (\sum_j (\mathbf{x}_j - \mathbf{\mu}_j) {\Sigma^{-1}}_{j})_k - (\mathbf{x}_k - \mathbf{\mu}_k) {\mathbf{\mu}_k \Sigma^{-1}}_k $$ What is the best way to compute this derivation? Could you show the passages?",,['multivariable-calculus']
23,"Is $f(x,y)=\frac{\sin\sqrt[3]{x^3+y^3}}{\sqrt[5]{x^5+y^5}}$ uniformly continuous or not",Is  uniformly continuous or not,"f(x,y)=\frac{\sin\sqrt[3]{x^3+y^3}}{\sqrt[5]{x^5+y^5}}","Find out if function $$f(x,y)=\frac{\sin\sqrt[3]{x^3+y^3}}{\sqrt[5]{x^5+y^5}}$$ is uniformly continous or not in area $D=\{0<x^2+y^2<1\}$. I found out that we have no $\lim\limits_{x,y\to 0}f(x,y)$, because $$\lim\limits_{x,y\to 0}\frac{\sin\sqrt[3]{x^3+y^3}}{\sqrt[5]{x^5+y^5}}=\lim\limits_{\rho\to0}\frac{\sqrt[3]{\rho^3\sin^3\alpha+\rho^3\cos^3\alpha}}{\sqrt[5]{\rho^5\sin^5\alpha+\rho^5\cos^5\alpha}}=\lim\limits_{\rho\to0}\frac{\sqrt[3]{\sin^3\alpha+\cos^3\alpha}}{\sqrt[5]{\sin^5\alpha+\cos^5\alpha}}$$ hence we can't use The Uniform Continuity Theorem as we can't determ $f(0,0)$. Function doesn't have bounded partial derivatives, so I think it's not uniformly continous, but I don't know how to show that","Find out if function $$f(x,y)=\frac{\sin\sqrt[3]{x^3+y^3}}{\sqrt[5]{x^5+y^5}}$$ is uniformly continous or not in area $D=\{0<x^2+y^2<1\}$. I found out that we have no $\lim\limits_{x,y\to 0}f(x,y)$, because $$\lim\limits_{x,y\to 0}\frac{\sin\sqrt[3]{x^3+y^3}}{\sqrt[5]{x^5+y^5}}=\lim\limits_{\rho\to0}\frac{\sqrt[3]{\rho^3\sin^3\alpha+\rho^3\cos^3\alpha}}{\sqrt[5]{\rho^5\sin^5\alpha+\rho^5\cos^5\alpha}}=\lim\limits_{\rho\to0}\frac{\sqrt[3]{\sin^3\alpha+\cos^3\alpha}}{\sqrt[5]{\sin^5\alpha+\cos^5\alpha}}$$ hence we can't use The Uniform Continuity Theorem as we can't determ $f(0,0)$. Function doesn't have bounded partial derivatives, so I think it's not uniformly continous, but I don't know how to show that",,"['calculus', 'limits', 'multivariable-calculus', 'continuity', 'uniform-continuity']"
24,finding maximum and minimum of a multivariable function in restricted domains,finding maximum and minimum of a multivariable function in restricted domains,,"Find the maximum and minimum values of $f(x,y)=xy-2x$ on the rectangle $-1\leq x \leq1 $ and $0\leq y \leq 1$ . I don't understand the approach. The solution manual suggests that the critical point is not inside my domain so maximum and minimum values of $f$ must be on one of the four boundary points. I don't understand how we get to this conclusion.",Find the maximum and minimum values of on the rectangle and . I don't understand the approach. The solution manual suggests that the critical point is not inside my domain so maximum and minimum values of must be on one of the four boundary points. I don't understand how we get to this conclusion.,"f(x,y)=xy-2x -1\leq x \leq1  0\leq y \leq 1 f",['multivariable-calculus']
25,"Disprove the existence of $\lim_{(x,y,z) \to (0,0,0)} \frac{x+y+z}{x^2y^2z^2}$",Disprove the existence of,"\lim_{(x,y,z) \to (0,0,0)} \frac{x+y+z}{x^2y^2z^2}","I need to prove that the following limit does not exist in $\mathbb{R^3}$, but I cannot seem to find the solution to this problem.  The limit is as follows: \begin{equation}  \lim_{(x,y,z) \to (0,0,0)} \frac{x+y+z}{x^2y^2z^2} \end{equation} Help is greatly appreciated!","I need to prove that the following limit does not exist in $\mathbb{R^3}$, but I cannot seem to find the solution to this problem.  The limit is as follows: \begin{equation}  \lim_{(x,y,z) \to (0,0,0)} \frac{x+y+z}{x^2y^2z^2} \end{equation} Help is greatly appreciated!",,['real-analysis']
26,"Multivariable proof of $lim_{(x,y)\to(a,b)} \frac{sin(f(x,y))}{f(x,y)}=1$",Multivariable proof of,"lim_{(x,y)\to(a,b)} \frac{sin(f(x,y))}{f(x,y)}=1","I know the typical result when it's the limit in one variable, but I can't find a multivariable epsilon-delta proof to the following problem: Let $f:B_r(a,b) \rightarrow \Bbb R$ such that $f$ is defined over $B_r(a,b)-\{(a,b)\}$ and $lim_{(x,y)\to(a,b)} f(x,y) = 0$. Prove that   $$lim_{(x,y)\to(a,b)} \frac{sin(f(x,y))}{f(x,y)}=1$$ $B_r(a,b)$ is the ball with radius $r$ and center $(a,b)$ I basically don't know how to take the known limit to an extra dimension. Where should I start? Also if you know of a book where I can find the proof or another website where it's explained it'll be helpful. Thanks a lot.","I know the typical result when it's the limit in one variable, but I can't find a multivariable epsilon-delta proof to the following problem: Let $f:B_r(a,b) \rightarrow \Bbb R$ such that $f$ is defined over $B_r(a,b)-\{(a,b)\}$ and $lim_{(x,y)\to(a,b)} f(x,y) = 0$. Prove that   $$lim_{(x,y)\to(a,b)} \frac{sin(f(x,y))}{f(x,y)}=1$$ $B_r(a,b)$ is the ball with radius $r$ and center $(a,b)$ I basically don't know how to take the known limit to an extra dimension. Where should I start? Also if you know of a book where I can find the proof or another website where it's explained it'll be helpful. Thanks a lot.",,"['multivariable-calculus', 'trigonometry', 'proof-writing', 'proof-explanation', 'epsilon-delta']"
27,How to find the direction vector of a ball falling off an ellipsoid?,How to find the direction vector of a ball falling off an ellipsoid?,,"A tiny ball is placed in top of an ellipsoid $3x^2+2y^2+z^2=9$ at $(1,1,2)$. Find the three-dimensional vector $\underline u$ in whose direction the ball will start moving after the ball is released. I feel this problem involves usage of gradients but not sure how to tackle it. EDIT the solution shouldn't use physics knowledge and has to be based on directional derivatives and/or gradients. EDIT 1 I've finally come up with the ""no physics solution"" however it is different from the accepted answer, I'd appreciate if other members confirm if the accepted answer is correct. One potential flaw with the accepted answer is that it's not using the $9$ from the original equation $3x^2+2y^2+z^2+\mathbf{9}=0$. Anyway this is my take: The $xy$ direction in which the ball will fall is $-\nabla f(1,1)$. $f_x=-\frac{3x}{\sqrt{9-3x^2-2y^2}}\stackrel{we.plug.in.x=1}{=}-\frac{3}{2}$. Similarly, $f_y=-1$ therefore $-\nabla f(1,1)=\langle 3/2,1 \rangle$. Let the 3d vector we're after be $d=\langle 3/2, 1, a \rangle$. Notice that $d$ is perpendicular to the normal vector of the tangential plane $n=\langle 6x,4x,-1 \rangle=\langle 6,4,-1 \rangle$ so $d\cdot n=0$ therefore $a=13$ so the final result is $d=\langle \frac{3}{2}, 1,13\rangle$.","A tiny ball is placed in top of an ellipsoid $3x^2+2y^2+z^2=9$ at $(1,1,2)$. Find the three-dimensional vector $\underline u$ in whose direction the ball will start moving after the ball is released. I feel this problem involves usage of gradients but not sure how to tackle it. EDIT the solution shouldn't use physics knowledge and has to be based on directional derivatives and/or gradients. EDIT 1 I've finally come up with the ""no physics solution"" however it is different from the accepted answer, I'd appreciate if other members confirm if the accepted answer is correct. One potential flaw with the accepted answer is that it's not using the $9$ from the original equation $3x^2+2y^2+z^2+\mathbf{9}=0$. Anyway this is my take: The $xy$ direction in which the ball will fall is $-\nabla f(1,1)$. $f_x=-\frac{3x}{\sqrt{9-3x^2-2y^2}}\stackrel{we.plug.in.x=1}{=}-\frac{3}{2}$. Similarly, $f_y=-1$ therefore $-\nabla f(1,1)=\langle 3/2,1 \rangle$. Let the 3d vector we're after be $d=\langle 3/2, 1, a \rangle$. Notice that $d$ is perpendicular to the normal vector of the tangential plane $n=\langle 6x,4x,-1 \rangle=\langle 6,4,-1 \rangle$ so $d\cdot n=0$ therefore $a=13$ so the final result is $d=\langle \frac{3}{2}, 1,13\rangle$.",,"['multivariable-calculus', 'vectors', 'partial-derivative']"
28,Change of variables in Einstein summation,Change of variables in Einstein summation,,"This should be trivial, but I am not able to work it out. How is the following equality true? $$ \frac{\partial}{\partial p^2} = \frac{1}{2p^2}p^\mu\frac{\partial}{\partial p^\mu} \,,$$ where $ p^2 = p^\mu p_\mu$. Basically, it involves calculating the partial derivative $$\frac{\partial p^\mu}{\partial p^2} = \Big(\frac{\partial p^2}{\partial p^\mu}\Big)^{-1}=\frac{1}{2}(p^\mu)^{-1} \,,$$ and showing that $$ \boxed{\color{blue}{(p^\mu)^{-1} = \frac{p^\mu}{p^2}}} \,. $$ I am not sure if what I have done above is rigorous at all. NOTE: I am not sure if the argument about multiplying and dividing $(p^\mu)^{-1}$ by a factor of $p^\mu$ works rigorously because there would be an implicit summation in the denominator.","This should be trivial, but I am not able to work it out. How is the following equality true? $$ \frac{\partial}{\partial p^2} = \frac{1}{2p^2}p^\mu\frac{\partial}{\partial p^\mu} \,,$$ where $ p^2 = p^\mu p_\mu$. Basically, it involves calculating the partial derivative $$\frac{\partial p^\mu}{\partial p^2} = \Big(\frac{\partial p^2}{\partial p^\mu}\Big)^{-1}=\frac{1}{2}(p^\mu)^{-1} \,,$$ and showing that $$ \boxed{\color{blue}{(p^\mu)^{-1} = \frac{p^\mu}{p^2}}} \,. $$ I am not sure if what I have done above is rigorous at all. NOTE: I am not sure if the argument about multiplying and dividing $(p^\mu)^{-1}$ by a factor of $p^\mu$ works rigorously because there would be an implicit summation in the denominator.",,"['multivariable-calculus', 'proof-verification']"
29,Simplifying $\lim_{h\to 0} f'(\frac{(f(a)-f(a+h))(a^2+ah)}{f(a)(a+h)-f(a+h)a)})=\frac{-f(a)}{a}$,Simplifying,\lim_{h\to 0} f'(\frac{(f(a)-f(a+h))(a^2+ah)}{f(a)(a+h)-f(a+h)a)})=\frac{-f(a)}{a},"I'm working on a personal for fun project that you can look at the broader question of here if you feel the need for context, but the gist of it is that I've reached a blockade where the only thing left for me to solve is:  $$\lim_{h\to 0} f'(\frac{(f(a)-f(a+h))(a^2+ah)}{f(a)(a+h)-f(a+h)a)})=\frac{-f(a)}{a}$$ honestly the entire thing is a mess, an easy way to start would probably to move to $$\lim_{h\to 0}f(\frac{(f(a)-f(a+h))(a^2+ah)}{f(a)(a+h)-f(a+h)a)})=\int_0^x \frac{-f(a)}{a} \,da$$ but from there I would need to simplify all of $\frac{(f(a)-f(a+h))(a^2+ah)}{f(a)(a+h)-f(a+h)a)}$, but any way of doing that is beyond me. If it is impossible to outright simplify it all, any simplification would be appreciated. Please excuse me for any errors that I make, or stupid questions, I am basically swinging blind at this point and learning as I go.","I'm working on a personal for fun project that you can look at the broader question of here if you feel the need for context, but the gist of it is that I've reached a blockade where the only thing left for me to solve is:  $$\lim_{h\to 0} f'(\frac{(f(a)-f(a+h))(a^2+ah)}{f(a)(a+h)-f(a+h)a)})=\frac{-f(a)}{a}$$ honestly the entire thing is a mess, an easy way to start would probably to move to $$\lim_{h\to 0}f(\frac{(f(a)-f(a+h))(a^2+ah)}{f(a)(a+h)-f(a+h)a)})=\int_0^x \frac{-f(a)}{a} \,da$$ but from there I would need to simplify all of $\frac{(f(a)-f(a+h))(a^2+ah)}{f(a)(a+h)-f(a+h)a)}$, but any way of doing that is beyond me. If it is impossible to outright simplify it all, any simplification would be appreciated. Please excuse me for any errors that I make, or stupid questions, I am basically swinging blind at this point and learning as I go.",,"['calculus', 'algebra-precalculus', 'multivariable-calculus']"
30,Inverse Function Theorem proof: f is injective,Inverse Function Theorem proof: f is injective,,"I am trying to prove the Inverse Function Theorem from the Implicit Function Theorem for Banach spaces. My attempt so far is as follows: Let $f:\mathbf{X}\to \mathbf{Y}$ be a $\mathcal{C}^k$ function between Banach spaces, and let $x^*\in\mathbf{X}$ and $y^*:=f(x^*)$ be such that the Fréchet derivative $\mathrm{d}\,f(x^*):\mathbf{X}\to\mathbf{Y}$ is bounded with bounded inverse. Consider $F:\mathbf{X}\times\mathbf{Y}\to\mathbf{Y}$ given by $F(x,y):=f(x)-y$ . Then the partial Fréchet derivative $\partial_xF(x^*,y^*)\equiv \mathrm{d}\,f(x^*)$ is bounded with a bounded inverse, so $F$ satisfies the hypotheses of the Implicit Function Theorem. Hence, there are open sets $U\subseteq\mathbf{X}$ and $V\subseteq\mathbf{Y}$ containing $x^*$ and $y^*$ , respectively, and a $\mathcal{C}^k$ function $g:V \to U$ such that $F(g(y), y) = 0$ for all $y\in V$ , i.e. $f(g(y)) = y$ . The above shows that $g$ is a right inverse of $f$ , but I want to be able to conclude, further, that $g$ is a left inverse of $f$ on $U$ , i.e. $g(f(x)) = x$ for all $x \in U$ . I know this to be equivalent to showing that the restriction $f|_U$ is injective (possibly by choosing a smaller neighbourhood of $x^*$ in $U$ ), but I am not sure how to do this. I would greatly appreciate any hints!","I am trying to prove the Inverse Function Theorem from the Implicit Function Theorem for Banach spaces. My attempt so far is as follows: Let be a function between Banach spaces, and let and be such that the Fréchet derivative is bounded with bounded inverse. Consider given by . Then the partial Fréchet derivative is bounded with a bounded inverse, so satisfies the hypotheses of the Implicit Function Theorem. Hence, there are open sets and containing and , respectively, and a function such that for all , i.e. . The above shows that is a right inverse of , but I want to be able to conclude, further, that is a left inverse of on , i.e. for all . I know this to be equivalent to showing that the restriction is injective (possibly by choosing a smaller neighbourhood of in ), but I am not sure how to do this. I would greatly appreciate any hints!","f:\mathbf{X}\to \mathbf{Y} \mathcal{C}^k x^*\in\mathbf{X} y^*:=f(x^*) \mathrm{d}\,f(x^*):\mathbf{X}\to\mathbf{Y} F:\mathbf{X}\times\mathbf{Y}\to\mathbf{Y} F(x,y):=f(x)-y \partial_xF(x^*,y^*)\equiv \mathrm{d}\,f(x^*) F U\subseteq\mathbf{X} V\subseteq\mathbf{Y} x^* y^* \mathcal{C}^k g:V \to U F(g(y), y) = 0 y\in V f(g(y)) = y g f g f U g(f(x)) = x x \in U f|_U x^* U","['functional-analysis', 'multivariable-calculus', 'implicit-function-theorem', 'frechet-derivative', 'inverse-function-theorem']"
31,Euler equations for the incompressible fluids,Euler equations for the incompressible fluids,,"Incompressible fluid with constant density ρ fills the three-dimensional domain below the free surface $z = η(r)$ in cylindrical polar coordinates. The flow is axisymmetric and steady, and the only non-zero velocity component is $u_θ$. Gravity acts upon the fluid. The fluid in $r\lt a$ rotates rigidly about the z-axis with angular velocity $\Omega$ and the fluid $r \ge a$ is irrotational. Use the radial and vertical components of the Euler equations to show that the pressure $p$ in the region $z < η$, $r < a$ satisfies $$\frac{p}{\rho} = \frac{1}{2}{Ω^2}{r^2} −gz + \text{constant}$$ and find the constant. $$\frac{∂u_r}{∂t}+\left(u_r\frac{∂}{∂r}+u_z\frac{∂}{∂z}\right)u_r-\frac{(u_θ)^2}{r}+\frac{1}{\rho}\frac{∂p}{∂r}=0 $$ $$\frac{∂u_θ}{∂t}+\left(u_r\frac{∂}{∂r}+u_z\frac{∂}{∂z}\right)u_θ+\frac{u_θu_r} {r}=0 $$ $$\frac{∂u_z}{∂t}+\left(u_r\frac{∂}{∂r}+u_z\frac{∂}{∂z}\right)u_z+\frac{1}{\rho}\frac{∂p}{∂z}=-g $$ I have the Euler equation in cylindrical coordinate. Then how should I figure out the $\Omega$. The next question is to show that the free surface position in $ r<a $ is  $$ η=\frac{\Omega^2a^2}{g}(\frac{r^2}{2a^2}-1) $$ if it is helpful. Thank you so much!","Incompressible fluid with constant density ρ fills the three-dimensional domain below the free surface $z = η(r)$ in cylindrical polar coordinates. The flow is axisymmetric and steady, and the only non-zero velocity component is $u_θ$. Gravity acts upon the fluid. The fluid in $r\lt a$ rotates rigidly about the z-axis with angular velocity $\Omega$ and the fluid $r \ge a$ is irrotational. Use the radial and vertical components of the Euler equations to show that the pressure $p$ in the region $z < η$, $r < a$ satisfies $$\frac{p}{\rho} = \frac{1}{2}{Ω^2}{r^2} −gz + \text{constant}$$ and find the constant. $$\frac{∂u_r}{∂t}+\left(u_r\frac{∂}{∂r}+u_z\frac{∂}{∂z}\right)u_r-\frac{(u_θ)^2}{r}+\frac{1}{\rho}\frac{∂p}{∂r}=0 $$ $$\frac{∂u_θ}{∂t}+\left(u_r\frac{∂}{∂r}+u_z\frac{∂}{∂z}\right)u_θ+\frac{u_θu_r} {r}=0 $$ $$\frac{∂u_z}{∂t}+\left(u_r\frac{∂}{∂r}+u_z\frac{∂}{∂z}\right)u_z+\frac{1}{\rho}\frac{∂p}{∂z}=-g $$ I have the Euler equation in cylindrical coordinate. Then how should I figure out the $\Omega$. The next question is to show that the free surface position in $ r<a $ is  $$ η=\frac{\Omega^2a^2}{g}(\frac{r^2}{2a^2}-1) $$ if it is helpful. Thank you so much!",,"['multivariable-calculus', 'partial-differential-equations', 'mathematical-physics', 'fluid-dynamics']"
32,To find function satisfying given partial derivatives,To find function satisfying given partial derivatives,,"Let $F_1,F_2:\mathbb{R^2} \to  \mathbb{R}$ be the functions $ F_1(x_1,x_2)={\frac{-x_2}{x_1^2+x_2^2}}$ and $ F_2(x_1,x_2)={\frac{x_1}{x_1^2+x_2^2}}$. Which of the following is correct? ${\frac{\partial F_1}{\partial x_2}}$ =${\frac{\partial F_2}{\partial x_1}}$ There exists a function $f: \mathbb{R^2}\setminus\{(0,0)\}\to \mathbb{R}$ such that ${\frac{\partial f}{\partial x_1}}=F_1$ and ${\frac{\partial f}{\partial x_2}}=F_2$ There exists no function $f:\mathbb{R^2}\setminus\{(0,0)\}\to \mathbb{R}$ such that ${\frac{\partial f}{\partial x_1}}=F_1$ and ${\frac{\partial f}{\partial x_2}}=F_2$ There exists a function $f:D\to \mathbb{R}$ where $D$ is the open disc of radius $1$ centred at $(2,0)$, which satisfies  ${\frac{\partial f}{\partial x_1}}=F_1$ and ${\frac{\partial f}{\partial x_2}}=F_2$ on $D$. I got first option. 2 $\to$ suppose such function exist then ${\frac{\partial f}{\partial x_1}}=F_1$ gives $f=-\tan^{-1} \left(\frac{x_1}{x_2}\right) +V(x_2)$ where $V(x_2)$ is some function of $x_2$  and ${\frac{\partial f}{\partial x_2}}=F_2$ gives $V'(x_2)=0$ hence $V(x_2)=C$, a constant . hence such function $f=-\tan^{-1}\left(\frac{x_1}{x_2}\right) +C$ exists. 3 $\to$ not true how to conclude for 4 is my explanation for 3 is perfect?","Let $F_1,F_2:\mathbb{R^2} \to  \mathbb{R}$ be the functions $ F_1(x_1,x_2)={\frac{-x_2}{x_1^2+x_2^2}}$ and $ F_2(x_1,x_2)={\frac{x_1}{x_1^2+x_2^2}}$. Which of the following is correct? ${\frac{\partial F_1}{\partial x_2}}$ =${\frac{\partial F_2}{\partial x_1}}$ There exists a function $f: \mathbb{R^2}\setminus\{(0,0)\}\to \mathbb{R}$ such that ${\frac{\partial f}{\partial x_1}}=F_1$ and ${\frac{\partial f}{\partial x_2}}=F_2$ There exists no function $f:\mathbb{R^2}\setminus\{(0,0)\}\to \mathbb{R}$ such that ${\frac{\partial f}{\partial x_1}}=F_1$ and ${\frac{\partial f}{\partial x_2}}=F_2$ There exists a function $f:D\to \mathbb{R}$ where $D$ is the open disc of radius $1$ centred at $(2,0)$, which satisfies  ${\frac{\partial f}{\partial x_1}}=F_1$ and ${\frac{\partial f}{\partial x_2}}=F_2$ on $D$. I got first option. 2 $\to$ suppose such function exist then ${\frac{\partial f}{\partial x_1}}=F_1$ gives $f=-\tan^{-1} \left(\frac{x_1}{x_2}\right) +V(x_2)$ where $V(x_2)$ is some function of $x_2$  and ${\frac{\partial f}{\partial x_2}}=F_2$ gives $V'(x_2)=0$ hence $V(x_2)=C$, a constant . hence such function $f=-\tan^{-1}\left(\frac{x_1}{x_2}\right) +C$ exists. 3 $\to$ not true how to conclude for 4 is my explanation for 3 is perfect?",,"['real-analysis', 'multivariable-calculus', 'partial-derivative']"
33,"$\int_\Omega |\nabla u |^2-2u^2\sin{x} \, dx$ - first variation, natural boundary conditions and solution for","- first variation, natural boundary conditions and solution for","\int_\Omega |\nabla u |^2-2u^2\sin{x} \, dx","I am taking calculus of variations course and I am stuck with the following problem: I need to find first varation , Euler equation, natural boundary conditions and solution for $I[u]=\int_\Omega (|\nabla u |^2-2u^2\sin{x} )\, dx$. First of all I am comfortable with problems like this one but for ""ordinary"", more straightforward functionals. E.g. I can solve the same problem if $I[u]=\int_0^1(y'')^2+2x\, dx + x^2(0)$ without any issues. But I never met integrals like this - it looks like it is ""ordinary"" integral but it is defined on a domain $\Omega$. Moreover, we do not have any information not only about boundaries of $\Omega$, but also even about dimension of $\Omega$! I guess it is either does not depend on the dimension or if it depends on it please suppose that the dimension is 2. But still, there is no information about the boundary of $\Omega$. I understand the general idea about first steps. We calculate the first variation by definition and use it to obtain Euler Equation and natural boundary conditions. $$\begin{align} 0 =\frac{d}{dt}\Bigg[ I[u+th]\Bigg ]_{t=0} = \frac{d}{dt}\Bigg [\int_\Omega (|\nabla (u+th) |^2-2(u+th)^2\sin{x} )\, dx\Bigg ]_{t=0} =\\ = \frac{d}{dt}\Bigg [\int_\Omega (\sum_i{(u'_i+th'_i)^2}-2(u^2+2thu+t^2h^2)\sin{x} )\, dx\Bigg ]_{t=0} =\\ =2\cdot\int_\Omega \sum_i{u'_ih'_i}-2hu\sin{x} \, dx= -4\cdot\int_\Omega \ hu\sin{x}\, dx + 2\cdot\int_\Omega \ \sum_i{u'_ih'_i}\, dx \end{align}$$ What one is trying do do next with an ""ordinary"" functional is the following: We should represent first variation with an integral having the form $\int Eh \, dx$ (where E is Euler equation) and another terms, which have the form $h(x_j)B_j$ (where $B_j$ will give us natural boundary conditions). In ""ordinary"" case at this moment we use integration by parts in order to remove terms with $h'$ and obtain everything else. But I do see how to do it here. I guess that I have to modify the last summand $$\int_\Omega \ \sum_i{u'_ih'_i}\, dx$$ with some kind of multidimensional integration by parts, but I do not see how to do it. Moreover, I do not see how to solve the problem because I do not see the form of Euler equation and natural boundary conditions. I do not even feel sure the solution (by ""solution"" I mean explicit form of $u$) can be found. Please help me to understand this. Thanks a lot for your time and help! Update: Gio67 advised to use Divergence theorem I can move forward a bit: $$\begin{align} = -2\cdot\int_\Omega \ h(2u\sin{x} + \Delta u)\, dx + 2\cdot\int_{\partial\Omega} \ h\sum_i{u'_i\nu_i}\, ds \end{align}$$ Can you please advice what to do with the last summand and how to get Euler equation and natural boundary conditions?","I am taking calculus of variations course and I am stuck with the following problem: I need to find first varation , Euler equation, natural boundary conditions and solution for $I[u]=\int_\Omega (|\nabla u |^2-2u^2\sin{x} )\, dx$. First of all I am comfortable with problems like this one but for ""ordinary"", more straightforward functionals. E.g. I can solve the same problem if $I[u]=\int_0^1(y'')^2+2x\, dx + x^2(0)$ without any issues. But I never met integrals like this - it looks like it is ""ordinary"" integral but it is defined on a domain $\Omega$. Moreover, we do not have any information not only about boundaries of $\Omega$, but also even about dimension of $\Omega$! I guess it is either does not depend on the dimension or if it depends on it please suppose that the dimension is 2. But still, there is no information about the boundary of $\Omega$. I understand the general idea about first steps. We calculate the first variation by definition and use it to obtain Euler Equation and natural boundary conditions. $$\begin{align} 0 =\frac{d}{dt}\Bigg[ I[u+th]\Bigg ]_{t=0} = \frac{d}{dt}\Bigg [\int_\Omega (|\nabla (u+th) |^2-2(u+th)^2\sin{x} )\, dx\Bigg ]_{t=0} =\\ = \frac{d}{dt}\Bigg [\int_\Omega (\sum_i{(u'_i+th'_i)^2}-2(u^2+2thu+t^2h^2)\sin{x} )\, dx\Bigg ]_{t=0} =\\ =2\cdot\int_\Omega \sum_i{u'_ih'_i}-2hu\sin{x} \, dx= -4\cdot\int_\Omega \ hu\sin{x}\, dx + 2\cdot\int_\Omega \ \sum_i{u'_ih'_i}\, dx \end{align}$$ What one is trying do do next with an ""ordinary"" functional is the following: We should represent first variation with an integral having the form $\int Eh \, dx$ (where E is Euler equation) and another terms, which have the form $h(x_j)B_j$ (where $B_j$ will give us natural boundary conditions). In ""ordinary"" case at this moment we use integration by parts in order to remove terms with $h'$ and obtain everything else. But I do see how to do it here. I guess that I have to modify the last summand $$\int_\Omega \ \sum_i{u'_ih'_i}\, dx$$ with some kind of multidimensional integration by parts, but I do not see how to do it. Moreover, I do not see how to solve the problem because I do not see the form of Euler equation and natural boundary conditions. I do not even feel sure the solution (by ""solution"" I mean explicit form of $u$) can be found. Please help me to understand this. Thanks a lot for your time and help! Update: Gio67 advised to use Divergence theorem I can move forward a bit: $$\begin{align} = -2\cdot\int_\Omega \ h(2u\sin{x} + \Delta u)\, dx + 2\cdot\int_{\partial\Omega} \ h\sum_i{u'_i\nu_i}\, ds \end{align}$$ Can you please advice what to do with the last summand and how to get Euler equation and natural boundary conditions?",,"['calculus', 'integration', 'multivariable-calculus', 'calculus-of-variations']"
34,In Trouble by the Sunny Side of Mercury,In Trouble by the Sunny Side of Mercury,,"I have the following homework problem, and have been able to work out parts a) and b), but not part c). Captain Ralph is in trouble near the sunny side of Mercury. The temperature of the ship's hull when he is at location $(x, y, z)$ will be given by $T (x, y, z) = e^{−x^2 − 2y^2 − 3z^2}$, where $x$, $y$, and $z$ are measured in meters. He is currently at $(1, 1, 1)$. a) In what direction should he proceed in order to decrease the temperature most rapidly? I got: $\left(2e^{-6},4e^{-6},6e^{-6}\right)$ b) If the ship travels at $e^9$ meters per second, how fast (in degrees per second) will the temperature decrease if he proceeds in that direction? I got: $\sqrt{56}e^3$ c) Unfortunately, the metal of the hull will crack if cooled at a rate greater than $\sqrt{{{17}}}e^3$ degrees per second. Describe the set of possible directions in which he may proceed to bring the temperature down at no more than that rate. I'm lost here. I found this other math.slackexchange post , but I'm lost following him. I've tried recreating his steps to no avail for my own problem. I also found this Caltech homework . The problem I'm doing is exactly the same as their last problem (just different numbers), I don't understand how they got to their answer. Also, I am submitting to a computer, so even though the answer might be correct, it could be that the answer isn't the one they're looking for. I submitted to my instructors for more verification on this problem last night, but have still not heard back from them. Any clarification/help would be much appreciated! Edit: I also tried $\sqrt{56}e^3\cos \left(\theta \right)$, which wasn't accepted either. I still haven't heard back from either of the course professors. Edit 2: I found this website where they go through and solve for theta. Additionally, my answer needs to be something in the form of this (whatever it means, please explain it to me if you understand it). $$\{ai+bj+ck \mid a^2+b^2+c^2=1, 0<[answer]\leq \sqrt{17}e^3\}$$","I have the following homework problem, and have been able to work out parts a) and b), but not part c). Captain Ralph is in trouble near the sunny side of Mercury. The temperature of the ship's hull when he is at location $(x, y, z)$ will be given by $T (x, y, z) = e^{−x^2 − 2y^2 − 3z^2}$, where $x$, $y$, and $z$ are measured in meters. He is currently at $(1, 1, 1)$. a) In what direction should he proceed in order to decrease the temperature most rapidly? I got: $\left(2e^{-6},4e^{-6},6e^{-6}\right)$ b) If the ship travels at $e^9$ meters per second, how fast (in degrees per second) will the temperature decrease if he proceeds in that direction? I got: $\sqrt{56}e^3$ c) Unfortunately, the metal of the hull will crack if cooled at a rate greater than $\sqrt{{{17}}}e^3$ degrees per second. Describe the set of possible directions in which he may proceed to bring the temperature down at no more than that rate. I'm lost here. I found this other math.slackexchange post , but I'm lost following him. I've tried recreating his steps to no avail for my own problem. I also found this Caltech homework . The problem I'm doing is exactly the same as their last problem (just different numbers), I don't understand how they got to their answer. Also, I am submitting to a computer, so even though the answer might be correct, it could be that the answer isn't the one they're looking for. I submitted to my instructors for more verification on this problem last night, but have still not heard back from them. Any clarification/help would be much appreciated! Edit: I also tried $\sqrt{56}e^3\cos \left(\theta \right)$, which wasn't accepted either. I still haven't heard back from either of the course professors. Edit 2: I found this website where they go through and solve for theta. Additionally, my answer needs to be something in the form of this (whatever it means, please explain it to me if you understand it). $$\{ai+bj+ck \mid a^2+b^2+c^2=1, 0<[answer]\leq \sqrt{17}e^3\}$$",,"['calculus', 'multivariable-calculus', 'vectors']"
35,The assumptions of the substitution theorem in double integral,The assumptions of the substitution theorem in double integral,,"Let  $$ \mathrm{T}:  \left\{   \begin{array}{l}     u=u(x,y) \\     v=v(x,y)   \end{array} \right. $$ be a change of variables substitution, where $u$ and $v$ have continues partial derivative in an open set $D$. In addition we assume that the Jacobian  $$ \frac{\partial(x,y)}{\partial(u,v)} $$  of $T$ does not vanish in $D$ and that $f(x,y)$ is integlable in the image $T(D)$. Under these condition I know that the formula of substitution in double integral is  $$ \iint_{\mathrm{T(D)}}f(x,y)\,dxdy =\iint_{\mathrm{D}}f(x(u,v),y(u,v))\left|\frac{\partial(x,y)}{\partial(u,v)}\right|\,dudv $$ Am I missing any condition on the boundary of the set $D$? What should we assume further on the set $D$? Should it be connected? bounded? Thanks!","Let  $$ \mathrm{T}:  \left\{   \begin{array}{l}     u=u(x,y) \\     v=v(x,y)   \end{array} \right. $$ be a change of variables substitution, where $u$ and $v$ have continues partial derivative in an open set $D$. In addition we assume that the Jacobian  $$ \frac{\partial(x,y)}{\partial(u,v)} $$  of $T$ does not vanish in $D$ and that $f(x,y)$ is integlable in the image $T(D)$. Under these condition I know that the formula of substitution in double integral is  $$ \iint_{\mathrm{T(D)}}f(x,y)\,dxdy =\iint_{\mathrm{D}}f(x(u,v),y(u,v))\left|\frac{\partial(x,y)}{\partial(u,v)}\right|\,dudv $$ Am I missing any condition on the boundary of the set $D$? What should we assume further on the set $D$? Should it be connected? bounded? Thanks!",,['multivariable-calculus']
36,integrate $\int_D e^{x^2+3y^2}$,integrate,\int_D e^{x^2+3y^2},"Evaluate $\int_D e^{x^2+3y^2}$, where $D$ is the region bounded in the first quadrant by the lines $y=0, y=x, x^2+3y^2=1$. My method is as follows, and I am not sure if it is correct. Let $u=x, v=\sqrt{3}y$. Then $D$ becomes bounded by $v=0, \frac{\sqrt{3}}{3}v=u, u^2+v^2=1$. $u=r\cos\theta, v=r\sin\theta$, so $\int_D e^{x^2+3y^2} = \int_D e^{r^2}r = \int^{\pi/6}_0e^{r^2}r$ Is this correct? If not, can you tell me where I got this wrong? Thank you.","Evaluate $\int_D e^{x^2+3y^2}$, where $D$ is the region bounded in the first quadrant by the lines $y=0, y=x, x^2+3y^2=1$. My method is as follows, and I am not sure if it is correct. Let $u=x, v=\sqrt{3}y$. Then $D$ becomes bounded by $v=0, \frac{\sqrt{3}}{3}v=u, u^2+v^2=1$. $u=r\cos\theta, v=r\sin\theta$, so $\int_D e^{x^2+3y^2} = \int_D e^{r^2}r = \int^{\pi/6}_0e^{r^2}r$ Is this correct? If not, can you tell me where I got this wrong? Thank you.",,['calculus']
37,Laplacian of a potential. Correct use of Index gymnastics and notation?,Laplacian of a potential. Correct use of Index gymnastics and notation?,,"Currently working my way through Gravity Newtonian, Post-Newtonian, Relativistic - Poisson and Will (2014) and I've stumbled upon a potential issue with my memory of vector calc in index notation. Context I've got an equation which takes the following form $$ \rho ( \partial_t \mathbf{v} + \mathbf{v} \cdot \nabla \mathbf{v}) = \rho \nabla U - \nabla p, \tag{1} \label{eq:maineq} $$ where mass density, pressure, velocity vector field and Newtonian gravitational potential for a fluid element are given by $\rho, p, \mathbf{v}$ and $U$ respectively. Usual notation is used i.e. $\partial_t\equiv\partial/\partial t$ etc. etc. Now, in terms of a fluid element comprised of a continuous matter distribution, the Newtonian gravitational potential satisfies Poisson's equation, namely $$ \nabla^2U = -4\pi G \rho. \tag{2} $$ We can express the first term on the RHS of Eq. (\ref{eq:maineq}) as the following $$ \rho \partial_j U =  -\frac{1}{4 \pi G} \nabla^2 U \partial_j U. $$ where $\partial_j U \equiv \nabla U$. The steps that follow is where the issue lies. Question We have $ -\frac{1}{4 \pi G} \nabla^2 U \partial_j U$ which can be expressed as the following $$ -\frac{1}{4 \pi G} (\partial_k\partial_k U) \partial_j U. \tag{3} \label{eq:laplacegrad}$$ In light of the comments below can someone explain the steps that Poisson and Will have taken to arrive at the following: $$\rho \partial_j U = -\frac{1}{4 \pi G} \partial_k  \left( \partial_j U \partial_k U - \frac{1}{2} \delta^{jk} \partial_i U \partial_i U \right). \tag{4} \label{eq:derive}$$ With regards the rules of the site. To see my (incorrect) attempt, it can be seen in a previously edited version of the question. See below for answer.","Currently working my way through Gravity Newtonian, Post-Newtonian, Relativistic - Poisson and Will (2014) and I've stumbled upon a potential issue with my memory of vector calc in index notation. Context I've got an equation which takes the following form $$ \rho ( \partial_t \mathbf{v} + \mathbf{v} \cdot \nabla \mathbf{v}) = \rho \nabla U - \nabla p, \tag{1} \label{eq:maineq} $$ where mass density, pressure, velocity vector field and Newtonian gravitational potential for a fluid element are given by $\rho, p, \mathbf{v}$ and $U$ respectively. Usual notation is used i.e. $\partial_t\equiv\partial/\partial t$ etc. etc. Now, in terms of a fluid element comprised of a continuous matter distribution, the Newtonian gravitational potential satisfies Poisson's equation, namely $$ \nabla^2U = -4\pi G \rho. \tag{2} $$ We can express the first term on the RHS of Eq. (\ref{eq:maineq}) as the following $$ \rho \partial_j U =  -\frac{1}{4 \pi G} \nabla^2 U \partial_j U. $$ where $\partial_j U \equiv \nabla U$. The steps that follow is where the issue lies. Question We have $ -\frac{1}{4 \pi G} \nabla^2 U \partial_j U$ which can be expressed as the following $$ -\frac{1}{4 \pi G} (\partial_k\partial_k U) \partial_j U. \tag{3} \label{eq:laplacegrad}$$ In light of the comments below can someone explain the steps that Poisson and Will have taken to arrive at the following: $$\rho \partial_j U = -\frac{1}{4 \pi G} \partial_k  \left( \partial_j U \partial_k U - \frac{1}{2} \delta^{jk} \partial_i U \partial_i U \right). \tag{4} \label{eq:derive}$$ With regards the rules of the site. To see my (incorrect) attempt, it can be seen in a previously edited version of the question. See below for answer.",,"['multivariable-calculus', 'vector-analysis', 'laplacian']"
38,Intersection with paraboloid,Intersection with paraboloid,,"This problem feels really easy but I've been having a really hard time with it. I'm given an equation of a paraboloid $z=x^2+4y^2$ and told that an unknown plane, perpendicular to the $xy$ plane has a point $(2,1,8)$ in common with the paraboloid. The intersection between the plane and the paraboloid is a parabola with slope $0$ at the given point. I'm told to find the equation of the plane. I've tried using the gradient vector but I found out that my approach is wrong. I tried to explicitly find the intersection between a plane with an equation $y=ax+d$ and the paraboloid equation, then differentiate it once to find out what $a$ and $d$ are so that the slope in $(2,1,8)$ is $0$, looking at the graphs in Mathematica it seems that I've got it wrong with both approaches. Looking for any suggestions on this, I'm really lost. Edit: some information on the gradient approach. I calculated $\nabla z(x,y)=(2x,8y)$, then substituted $x$ and $y$ for $2$ and $1$ respectively. This should be perpendicular to the level curve $8=x^2+4y^2$, if I'm thinking correctly. Therefore, I can define a plane using $\nabla z(2,1)=(4,8)$ and using the fact that we know the plane is orthogonal to $xy$, therefore we use $(4,8,0)$ as the normal vector to the plane. So, by my chain of thought, the plane equation should be $4x+8y+d=0$, substituting $x$ and $y$ for $2$ and $1$ we get $d=-16$. Unless I messed up my Mathematica plot lots of times, this isn't right...","This problem feels really easy but I've been having a really hard time with it. I'm given an equation of a paraboloid $z=x^2+4y^2$ and told that an unknown plane, perpendicular to the $xy$ plane has a point $(2,1,8)$ in common with the paraboloid. The intersection between the plane and the paraboloid is a parabola with slope $0$ at the given point. I'm told to find the equation of the plane. I've tried using the gradient vector but I found out that my approach is wrong. I tried to explicitly find the intersection between a plane with an equation $y=ax+d$ and the paraboloid equation, then differentiate it once to find out what $a$ and $d$ are so that the slope in $(2,1,8)$ is $0$, looking at the graphs in Mathematica it seems that I've got it wrong with both approaches. Looking for any suggestions on this, I'm really lost. Edit: some information on the gradient approach. I calculated $\nabla z(x,y)=(2x,8y)$, then substituted $x$ and $y$ for $2$ and $1$ respectively. This should be perpendicular to the level curve $8=x^2+4y^2$, if I'm thinking correctly. Therefore, I can define a plane using $\nabla z(2,1)=(4,8)$ and using the fact that we know the plane is orthogonal to $xy$, therefore we use $(4,8,0)$ as the normal vector to the plane. So, by my chain of thought, the plane equation should be $4x+8y+d=0$, substituting $x$ and $y$ for $2$ and $1$ we get $d=-16$. Unless I messed up my Mathematica plot lots of times, this isn't right...",,['multivariable-calculus']
39,Exterior derivative of 2-forms and divergence,Exterior derivative of 2-forms and divergence,,"I'm working through A Geometric Approach to Differential Forms . The deriative of a $2$-form $\omega$ in $\mathbb{R}^3$, denoted $d\omega$ and operating on vectors $U, V, W \in T_p \mathbb{R}^3$, is defined as $$d\omega(U, V, W) = \nabla_U \omega(V, W) - \nabla_V \omega(U, W) + \nabla_W \omega(U, V)$$ where $\nabla_U \omega(V, W)$ denotes the directional derivative of $\omega(V, W)$ in the direction $U$. Suppose that we have a $2$-form $\omega = f(x, y, z) \ dx \wedge dy + g(x, y, z) \ dy \wedge dz + h(x, y, z) \ dx \wedge dz$. One way to calculate $d\omega$ is to realize that it must have the form $d\omega = a(x, y, z) \ dx \wedge dy \wedge dz$. Geometrically, we can think of $d\omega$ as taking three vectors, calculating the signed volume of the parallelogram formed by those three vectors, and then applying a scaling factor $a(x, y, z)$. So one way to determine $a(x, y, z)$ is to use the above definition of $d\omega(U, V, W)$ to see how it acts on vectors corresponding to a parallelogram of signed volume $1$. If I take $U = (1, 0, 0)$, $V = (0, 1, 0)$, and $W = (0, 0, 1)$, and I go through the computations, I arrive at $$d\omega = \left( \frac{\partial g}{\partial x} - \frac{\partial h}{\partial y} + \frac{\partial f}{\partial z} \right) \ dx \wedge dy \wedge dz.$$ This seems to be right as far as I can tell. In particular, the second term is given by $\nabla_V \omega(U, W) = \partial h / \partial y$, with the negative sign coming from the alternating signs in the definition of $d\omega(U, V, W)$. Explicitly, my calculation is $$\nabla_V \omega(U, W) = \nabla \left( \underbrace{f \begin{vmatrix} 1 & 0 \\ 0 & 0 \end{vmatrix}}_0 + \underbrace{g \begin{vmatrix} 0 & 0 \\ 0 & 1 \end{vmatrix}}_0 + \underbrace{h \begin{vmatrix} 1 & 0 \\ 0 & 1 \end{vmatrix}}_h \right) \cdot V = \frac{\partial h}{\partial y}.$$ Furthermore, I know that there's a connection between the exterior derivative of a $2$-form in $\mathbb{R}^3$ and the operation of the divergence. From, e.g., the Wikipedia description , it seems that divergence is $$(g, h, f) \mapsto \frac{\partial g}{\partial x} + \frac{\partial h}{\partial y} + \frac{\partial f}{\partial z}$$ where I've ordered $f, g, h$ for consistency with the representation of $\omega$. The difference here compared to my expression for $d\omega$ is that the term $\partial h / \partial y$ is positive rather than negative. Whence the difference?","I'm working through A Geometric Approach to Differential Forms . The deriative of a $2$-form $\omega$ in $\mathbb{R}^3$, denoted $d\omega$ and operating on vectors $U, V, W \in T_p \mathbb{R}^3$, is defined as $$d\omega(U, V, W) = \nabla_U \omega(V, W) - \nabla_V \omega(U, W) + \nabla_W \omega(U, V)$$ where $\nabla_U \omega(V, W)$ denotes the directional derivative of $\omega(V, W)$ in the direction $U$. Suppose that we have a $2$-form $\omega = f(x, y, z) \ dx \wedge dy + g(x, y, z) \ dy \wedge dz + h(x, y, z) \ dx \wedge dz$. One way to calculate $d\omega$ is to realize that it must have the form $d\omega = a(x, y, z) \ dx \wedge dy \wedge dz$. Geometrically, we can think of $d\omega$ as taking three vectors, calculating the signed volume of the parallelogram formed by those three vectors, and then applying a scaling factor $a(x, y, z)$. So one way to determine $a(x, y, z)$ is to use the above definition of $d\omega(U, V, W)$ to see how it acts on vectors corresponding to a parallelogram of signed volume $1$. If I take $U = (1, 0, 0)$, $V = (0, 1, 0)$, and $W = (0, 0, 1)$, and I go through the computations, I arrive at $$d\omega = \left( \frac{\partial g}{\partial x} - \frac{\partial h}{\partial y} + \frac{\partial f}{\partial z} \right) \ dx \wedge dy \wedge dz.$$ This seems to be right as far as I can tell. In particular, the second term is given by $\nabla_V \omega(U, W) = \partial h / \partial y$, with the negative sign coming from the alternating signs in the definition of $d\omega(U, V, W)$. Explicitly, my calculation is $$\nabla_V \omega(U, W) = \nabla \left( \underbrace{f \begin{vmatrix} 1 & 0 \\ 0 & 0 \end{vmatrix}}_0 + \underbrace{g \begin{vmatrix} 0 & 0 \\ 0 & 1 \end{vmatrix}}_0 + \underbrace{h \begin{vmatrix} 1 & 0 \\ 0 & 1 \end{vmatrix}}_h \right) \cdot V = \frac{\partial h}{\partial y}.$$ Furthermore, I know that there's a connection between the exterior derivative of a $2$-form in $\mathbb{R}^3$ and the operation of the divergence. From, e.g., the Wikipedia description , it seems that divergence is $$(g, h, f) \mapsto \frac{\partial g}{\partial x} + \frac{\partial h}{\partial y} + \frac{\partial f}{\partial z}$$ where I've ordered $f, g, h$ for consistency with the representation of $\omega$. The difference here compared to my expression for $d\omega$ is that the term $\partial h / \partial y$ is positive rather than negative. Whence the difference?",,"['multivariable-calculus', 'vector-analysis', 'differential-forms', 'divergence-operator']"
40,Disappearing conservative field with zero divergence: is it zero in higher dimensions?,Disappearing conservative field with zero divergence: is it zero in higher dimensions?,,"I have a vector field, which I know is conservative (it is the divergence of a scalar field). It has divergence zero, and it disappears at infinite distance. The dimensionality of the problem is arbitrary. I am working in a Euclidean space. If I understand correctly, then in two or three dimensions, this vector field is necessarily zero. This is because any vector-valued function is fully determined by its divergence (=0), its curl (=0) and its value on the boundary (->0). Does this finding extend to higher dimensions? Is a disappearing conservative field with zero divergence still necessarily zero?","I have a vector field, which I know is conservative (it is the divergence of a scalar field). It has divergence zero, and it disappears at infinite distance. The dimensionality of the problem is arbitrary. I am working in a Euclidean space. If I understand correctly, then in two or three dimensions, this vector field is necessarily zero. This is because any vector-valued function is fully determined by its divergence (=0), its curl (=0) and its value on the boundary (->0). Does this finding extend to higher dimensions? Is a disappearing conservative field with zero divergence still necessarily zero?",,['multivariable-calculus']
41,Why does the partial of $f: \Delta \to \mathbb{R}^2$ fail to exist?,Why does the partial of  fail to exist?,f: \Delta \to \mathbb{R}^2,"Given $f: \Delta = \{x = (x_1,x_2)| x_1+x_2 = 1, x_1 \geq 0, x_2 \geq 0\} \to \mathbb{R}^2$ Claim: $\dfrac{\partial f_i}{ \partial x_j}$ doesn't exist. This statement is true. Can anyone please provide a way to justify this statement? I have a feeling that this is due to compactness of the domain of $f$, which comes into conflict with the formal definition of the partial derivative. But I do not know if we could extend the definition of partial derivative for $f$ defined on compact sets. Further, I can produce large amount of examples that seems to contradict the claim: Example: Consider $f(x_1,x_2) = \begin{bmatrix} x_1 \\ 2 x_2 \end{bmatrix} = \begin{bmatrix} f_1 \\ f_2 \end{bmatrix}$, then clearly $\dfrac{\partial f_1}{\partial x_1} = 1, \dfrac{\partial f_1}{\partial x_2} = 0$, ditto the other partials. Can anyone please assist!","Given $f: \Delta = \{x = (x_1,x_2)| x_1+x_2 = 1, x_1 \geq 0, x_2 \geq 0\} \to \mathbb{R}^2$ Claim: $\dfrac{\partial f_i}{ \partial x_j}$ doesn't exist. This statement is true. Can anyone please provide a way to justify this statement? I have a feeling that this is due to compactness of the domain of $f$, which comes into conflict with the formal definition of the partial derivative. But I do not know if we could extend the definition of partial derivative for $f$ defined on compact sets. Further, I can produce large amount of examples that seems to contradict the claim: Example: Consider $f(x_1,x_2) = \begin{bmatrix} x_1 \\ 2 x_2 \end{bmatrix} = \begin{bmatrix} f_1 \\ f_2 \end{bmatrix}$, then clearly $\dfrac{\partial f_1}{\partial x_1} = 1, \dfrac{\partial f_1}{\partial x_2} = 0$, ditto the other partials. Can anyone please assist!",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'definition', 'jacobian']"
42,Find double derivative using implicit derivation - mulitvariable calculus,Find double derivative using implicit derivation - mulitvariable calculus,,"Short question: $z(x,y)$ is a function that is implicitly defined by the equation $$4x+3y+5z+4\cos(4z)+3=0$$ in the neigbourhood of the point $p=(-5\pi/16, 1/3, \pi/4)$ I am going to calculate $\partial$ in $p$. $F$ differentiated with respect to $x$ is $4$ and with respect to $y$ is $3$. Differentiated with respect to $z$ is $5-16\sin(4z)$ Then we get $$\frac{\partial z}{\partial x}=-4/5$$ and $$\frac{\partial z}{\partial y}=-3/5 $$ by plugging in $z=\pi/4$ But now I don't know how to calculate $\frac{\partial^2z}{\partial x \partial y}$. Can somebody help me?","Short question: $z(x,y)$ is a function that is implicitly defined by the equation $$4x+3y+5z+4\cos(4z)+3=0$$ in the neigbourhood of the point $p=(-5\pi/16, 1/3, \pi/4)$ I am going to calculate $\partial$ in $p$. $F$ differentiated with respect to $x$ is $4$ and with respect to $y$ is $3$. Differentiated with respect to $z$ is $5-16\sin(4z)$ Then we get $$\frac{\partial z}{\partial x}=-4/5$$ and $$\frac{\partial z}{\partial y}=-3/5 $$ by plugging in $z=\pi/4$ But now I don't know how to calculate $\frac{\partial^2z}{\partial x \partial y}$. Can somebody help me?",,"['multivariable-calculus', 'derivatives', 'implicit-differentiation', 'implicit-function-theorem']"
43,A question about Frechet derivative,A question about Frechet derivative,,"Let $X,Y$ be real Banach spaces. Define $F: X \times Y \rightarrow \mathbb{R}$ as a functional mapping $\left(u,t\right) \mapsto F\left(u,t\right)$ and $F_u\left(u,t\right), F_t\left(u,t\right)$ be partial Frechet derivatives of $F.$ I want to know what are the relationships between $DF\left(u,t\right)$ and $ F_u\left(u,t\right), F_t\left(u,t\right)$. Can you show me? By the Riesz representation theorem, it can be represented in the form of an inner product; denoting the representing element by \begin{equation} F_u\left(u,t\right)h=\langle \text{grad}F\left(u,t\right),h \rangle \end{equation} and  $$F_t\left(u,t\right)l=\langle \text{grad}F\left(u,t\right),l \rangle$$  and  $$DF\left(u,t\right)w=\langle \text{grad}F\left(u,t\right),w \rangle$$. I don't understand what $F\left(u,t\right)$ in above identities mean does?","Let $X,Y$ be real Banach spaces. Define $F: X \times Y \rightarrow \mathbb{R}$ as a functional mapping $\left(u,t\right) \mapsto F\left(u,t\right)$ and $F_u\left(u,t\right), F_t\left(u,t\right)$ be partial Frechet derivatives of $F.$ I want to know what are the relationships between $DF\left(u,t\right)$ and $ F_u\left(u,t\right), F_t\left(u,t\right)$. Can you show me? By the Riesz representation theorem, it can be represented in the form of an inner product; denoting the representing element by \begin{equation} F_u\left(u,t\right)h=\langle \text{grad}F\left(u,t\right),h \rangle \end{equation} and  $$F_t\left(u,t\right)l=\langle \text{grad}F\left(u,t\right),l \rangle$$  and  $$DF\left(u,t\right)w=\langle \text{grad}F\left(u,t\right),w \rangle$$. I don't understand what $F\left(u,t\right)$ in above identities mean does?",,"['calculus', 'multivariable-calculus', 'partial-derivative', 'calculus-of-variations', 'frechet-derivative']"
44,Proof of Implicit Function Theorem: special case,Proof of Implicit Function Theorem: special case,,"I was reading this pdf online on the Implicit Function Theorem (pg 18-19). I did not understand the last line : Why "" continuity of $f$ and uniqueness of $y$ implies continuity of $g$ ""?","I was reading this pdf online on the Implicit Function Theorem (pg 18-19). I did not understand the last line : Why "" continuity of $f$ and uniqueness of $y$ implies continuity of $g$ ""?",,"['multivariable-calculus', 'derivatives', 'manifolds', 'proof-explanation', 'implicit-function-theorem']"
45,Applying stokes' theorem to evaluate the double integral for the hemisphere with radius 2 for $y \geq 0$ and oriented in the $+y$ direction.,Applying stokes' theorem to evaluate the double integral for the hemisphere with radius 2 for  and oriented in the  direction.,y \geq 0 +y,"A question on my homework asks: Use Stokes' Theorem to evaluate $\int\int_{S} (curl\vec{F}) * {n} \space dS$ where S is defined as the hemisphere $x^2 + y^2 + z^2 = 4$ for $y \geq 0$ and oriented in the positive y direction. They've also given the vector field $\vec{F} = \space <ze^y, xcos(y), xzsin(y) > $ So I started by drawing the surface, which is pretty straightforward. I'm gonna be integrating over the xz plane, where the curve is defined by $x^2 + z^2 = 4$, i.e. a circle with radius of $2$. According to my book, the orientation of the curve is consistent with that of the surface if you imagine a person walking around the curve and 1) their head points in the same direction as the normal vector to the surface, and 2) the surface is to their left at each point along the curve. I parametrized the curve as follows: $\vec{r(t)} = \space <2cos(t), 0, 2sin(t)>$ I did everything else right--I plugged in the values for x, y, and z into the vector field and properly set up the integral. In the end, I got an answer of $-4\pi$, but this was wrong. I then tried entering $4\pi$, and that was apparently the correct answer. This seems to imply that while I did everything else right, I had to set up the limits as $t$ going from $2\pi$ to $0$ as opposed to $0$ to $2\pi$. But this doesn't make sense to me, since that forces the surface to be to the right of the imaginary person as he walks along the curve. What did I do wrong?","A question on my homework asks: Use Stokes' Theorem to evaluate $\int\int_{S} (curl\vec{F}) * {n} \space dS$ where S is defined as the hemisphere $x^2 + y^2 + z^2 = 4$ for $y \geq 0$ and oriented in the positive y direction. They've also given the vector field $\vec{F} = \space <ze^y, xcos(y), xzsin(y) > $ So I started by drawing the surface, which is pretty straightforward. I'm gonna be integrating over the xz plane, where the curve is defined by $x^2 + z^2 = 4$, i.e. a circle with radius of $2$. According to my book, the orientation of the curve is consistent with that of the surface if you imagine a person walking around the curve and 1) their head points in the same direction as the normal vector to the surface, and 2) the surface is to their left at each point along the curve. I parametrized the curve as follows: $\vec{r(t)} = \space <2cos(t), 0, 2sin(t)>$ I did everything else right--I plugged in the values for x, y, and z into the vector field and properly set up the integral. In the end, I got an answer of $-4\pi$, but this was wrong. I then tried entering $4\pi$, and that was apparently the correct answer. This seems to imply that while I did everything else right, I had to set up the limits as $t$ going from $2\pi$ to $0$ as opposed to $0$ to $2\pi$. But this doesn't make sense to me, since that forces the surface to be to the right of the imaginary person as he walks along the curve. What did I do wrong?",,"['multivariable-calculus', 'stokes-theorem']"
46,Compute Flux (Divergence Theorem),Compute Flux (Divergence Theorem),,"Given that $\textbf{F} =  \langle x\cos^2z,  y\sin^2z,  \sqrt{x^2+y^2}\:z \rangle $ and let E be the solid cone above the $xy$-plane and inside z = $1 -\sqrt{x^2+y^2}.$ I'm trying to use the divergence theorem to compute the flux. $$\iint_{D} \textbf{F} \cdot \textbf{N} \: dS = \iiint_E \nabla \cdot \textbf{F}\:d\textbf{V}$$ Attempt: $$\text{div}\textbf{F} \:=\: \nabla \cdot \textbf{F}  =\cos^2z + \sin^2z + \sqrt{x^2+y^2} \: = z + r \: $$ Since solid cone is above the $xy$-plane, $\text{z} \ge 0$ and $z = 1-\sqrt{x^2+y^2} = 1-r$. Hence $ 0 \le \text{z} \le 1-r$. Is this bound right for z? \begin{align*} \iiint_E \nabla \cdot \textbf{F}\:d\textbf{V} &= \int_0^{2\pi} \int_0^{1} \int_0^{1-r} (z+r) r\: dz\: dr\: d\theta \\ &= 2\pi \int_0^{1} \left(\frac{z^2r}{2} + r^2z\right)\bigg\rvert_0^{1-r} dr \\ &= 2\pi \cdot \frac{1}{2} \int_0^1 r-r^3\, dr \\ &= \pi \left(\frac{r^2}{2}-\frac{r^4}{4}\right)\bigg\rvert_0^1 \\ &= \frac{\pi}{4}. \end{align*} The answer given was $\frac{\pi}{2}.\:$  Perhaps I'm doing something wrong. One thing that I'm not sure of is picturing the surface that they're looking for. Also, not sure if my bounds are right. Where am I going wrong here? Would appreciate some help! Thank you.","Given that $\textbf{F} =  \langle x\cos^2z,  y\sin^2z,  \sqrt{x^2+y^2}\:z \rangle $ and let E be the solid cone above the $xy$-plane and inside z = $1 -\sqrt{x^2+y^2}.$ I'm trying to use the divergence theorem to compute the flux. $$\iint_{D} \textbf{F} \cdot \textbf{N} \: dS = \iiint_E \nabla \cdot \textbf{F}\:d\textbf{V}$$ Attempt: $$\text{div}\textbf{F} \:=\: \nabla \cdot \textbf{F}  =\cos^2z + \sin^2z + \sqrt{x^2+y^2} \: = z + r \: $$ Since solid cone is above the $xy$-plane, $\text{z} \ge 0$ and $z = 1-\sqrt{x^2+y^2} = 1-r$. Hence $ 0 \le \text{z} \le 1-r$. Is this bound right for z? \begin{align*} \iiint_E \nabla \cdot \textbf{F}\:d\textbf{V} &= \int_0^{2\pi} \int_0^{1} \int_0^{1-r} (z+r) r\: dz\: dr\: d\theta \\ &= 2\pi \int_0^{1} \left(\frac{z^2r}{2} + r^2z\right)\bigg\rvert_0^{1-r} dr \\ &= 2\pi \cdot \frac{1}{2} \int_0^1 r-r^3\, dr \\ &= \pi \left(\frac{r^2}{2}-\frac{r^4}{4}\right)\bigg\rvert_0^1 \\ &= \frac{\pi}{4}. \end{align*} The answer given was $\frac{\pi}{2}.\:$  Perhaps I'm doing something wrong. One thing that I'm not sure of is picturing the surface that they're looking for. Also, not sure if my bounds are right. Where am I going wrong here? Would appreciate some help! Thank you.",,"['calculus', 'multivariable-calculus', 'surface-integrals']"
47,Computing line integral (Stokes Theorem),Computing line integral (Stokes Theorem),,"Given that $\textbf{F} =  \langle z,x,y \rangle$, The plane $ z=2x+2y-1$ and the paraboloid $ z= x^2 +y^2$ intersect in a closed curve. I'm trying to use stokes theorem to find the line integral. Attempt: We know that Stokes Theorem is given by: $\iint_S (\nabla \times \textbf{F}) \cdot \textbf{N}\: d\textbf{S} $ $\nabla \times \textbf{F} = \vec{\imath} + \vec{\jmath} + \vec{k}  = \langle 1 , 1, 1\rangle$ Now, the problem I have is parameterizing. This is what I did. $$\vec{r} = \langle x,y,2x+2y-1\rangle$$ $R_x \times R_y = 2\vec{\imath} - 2\vec{\jmath} + 0\vec{k} $ Now, $\iint_S \langle 1 , 1, 1\rangle \cdot \langle 2 , -2, 0\rangle \: d\textbf{S} = 0 $. But this is not right! Perhaps what I'm struggling most is getting the parameterization right for the surface. Also would I have to convert to polar coordinates to complete the integral? Any help would be appreciated! Thank you","Given that $\textbf{F} =  \langle z,x,y \rangle$, The plane $ z=2x+2y-1$ and the paraboloid $ z= x^2 +y^2$ intersect in a closed curve. I'm trying to use stokes theorem to find the line integral. Attempt: We know that Stokes Theorem is given by: $\iint_S (\nabla \times \textbf{F}) \cdot \textbf{N}\: d\textbf{S} $ $\nabla \times \textbf{F} = \vec{\imath} + \vec{\jmath} + \vec{k}  = \langle 1 , 1, 1\rangle$ Now, the problem I have is parameterizing. This is what I did. $$\vec{r} = \langle x,y,2x+2y-1\rangle$$ $R_x \times R_y = 2\vec{\imath} - 2\vec{\jmath} + 0\vec{k} $ Now, $\iint_S \langle 1 , 1, 1\rangle \cdot \langle 2 , -2, 0\rangle \: d\textbf{S} = 0 $. But this is not right! Perhaps what I'm struggling most is getting the parameterization right for the surface. Also would I have to convert to polar coordinates to complete the integral? Any help would be appreciated! Thank you",,"['calculus', 'multivariable-calculus', 'parametrization', 'stokes-theorem']"
48,3 variable measurements of a box question,3 variable measurements of a box question,,"A box is to be constructed with a total volume of 600 $cm^3$, but the sides have different costs. The top, bottom, left and right sides cost 5 dollars per $cm^2$, but the front and back cost 3 dollars per $cm^2 . What are the dimensions of the box that have the correct volume, but minimize cost? I know that this is an xyz question since the dimensions of a box means 3 dimension (height, width, & length). But I'm not sure how to go about answering this question to  find the separate costs. (x, y, z)","A box is to be constructed with a total volume of 600 $cm^3$, but the sides have different costs. The top, bottom, left and right sides cost 5 dollars per $cm^2$, but the front and back cost 3 dollars per $cm^2 . What are the dimensions of the box that have the correct volume, but minimize cost? I know that this is an xyz question since the dimensions of a box means 3 dimension (height, width, & length). But I'm not sure how to go about answering this question to  find the separate costs. (x, y, z)",,"['calculus', 'multivariable-calculus']"
49,Finding flux over a surface (parameterization),Finding flux over a surface (parameterization),,"I'm trying to evaluate $\iint_D \langle x,y,-2\rangle \textbf{n} \cdot \textbf{dS} $, where $D$ is given by $z=1- x^2 - y^2$, $x^2+y^2 \leq 1$, oriented up. Now, I'm trying to find a parameterization for D but I'm struggling to do so. Graphically, we have a paraboloid with maximum $z=1$ and a cylinder with radius $\leq 1$. So that means I'm trying to parameterize an area that is like a hemisphere. Can I use spherical coordinates then such that: $$r(\phi,\theta) = \langle r\sin(\phi)\cos(\theta), r\sin(\phi) \sin(\theta), r\cos(\phi), \quad  0\leq \phi \leq \frac \pi 2, \quad 0 \leq \theta \leq 2\pi $$ Then I can find the flux using the formula  $\iint\textbf{F}\cdot(r_\phi \times r_\theta) \, dA $. P.S. I'm weak at parameterizing..","I'm trying to evaluate $\iint_D \langle x,y,-2\rangle \textbf{n} \cdot \textbf{dS} $, where $D$ is given by $z=1- x^2 - y^2$, $x^2+y^2 \leq 1$, oriented up. Now, I'm trying to find a parameterization for D but I'm struggling to do so. Graphically, we have a paraboloid with maximum $z=1$ and a cylinder with radius $\leq 1$. So that means I'm trying to parameterize an area that is like a hemisphere. Can I use spherical coordinates then such that: $$r(\phi,\theta) = \langle r\sin(\phi)\cos(\theta), r\sin(\phi) \sin(\theta), r\cos(\phi), \quad  0\leq \phi \leq \frac \pi 2, \quad 0 \leq \theta \leq 2\pi $$ Then I can find the flux using the formula  $\iint\textbf{F}\cdot(r_\phi \times r_\theta) \, dA $. P.S. I'm weak at parameterizing..",,"['multivariable-calculus', 'surface-integrals']"
50,About the method of Lagrange multipliers to extremize a function,About the method of Lagrange multipliers to extremize a function,,"Suppose that by the using the method of Lagrange multipliers to extremize the function $f(x,y)$ subject to the constraint $g(x,y)$, we find that $f(x,y)$ has only one critical point.  How can we deduce the critical point is minumum or maximum?","Suppose that by the using the method of Lagrange multipliers to extremize the function $f(x,y)$ subject to the constraint $g(x,y)$, we find that $f(x,y)$ has only one critical point.  How can we deduce the critical point is minumum or maximum?",,['multivariable-calculus']
51,"Multivariate Caluclus, harmonic, Divergence Theorem, Flux","Multivariate Caluclus, harmonic, Divergence Theorem, Flux",,"Facts that would be helpful to the proof: I honestly have no idea how to begin/proceed since we rarely look at proofs or abstract idea ""near the origin"". We have only done a lot of parameterizations and flux integrals, etc. Any help or sketch of proof would be appreciated!! :)","Facts that would be helpful to the proof: I honestly have no idea how to begin/proceed since we rarely look at proofs or abstract idea ""near the origin"". We have only done a lot of parameterizations and flux integrals, etc. Any help or sketch of proof would be appreciated!! :)",,"['calculus', 'real-analysis', 'multivariable-calculus', 'divergence-operator', 'multivalued-functions']"
52,How can I find critical points of the multivariate polynomial?,How can I find critical points of the multivariate polynomial?,,"I need to find the critical points of the multivariate polynomial and types of critical points $f(x_1,x_2,x_3,x_4)=x_2x_3+x_3x_4+x_1x_2+x_1x_2x_3x_4$ What are soft to find the critical points and how I can classification the critical points. If taken: $$\frac{\partial f}{\partial x_1}=0,\frac{\partial f}{\partial x_2}=0,\frac{\partial f}{\partial x_3}=0,\frac{\partial f}{\partial x_4}=0,$$ then we get: $$x_2+x_2x_3x_4=0$$ $$x_3+x_1+x_1x_3x_4=0$$ $$x_2+x_4+x_1x_2x_4=0$$ $$x_3+x_1x_2x_3=0$$ Do you use a program to solve equations or is there a better way? If there is a set of solutions. How do you critical points ? Thanks for the help.",I need to find the critical points of the multivariate polynomial and types of critical points What are soft to find the critical points and how I can classification the critical points. If taken: then we get: Do you use a program to solve equations or is there a better way? If there is a set of solutions. How do you critical points ? Thanks for the help.,"f(x_1,x_2,x_3,x_4)=x_2x_3+x_3x_4+x_1x_2+x_1x_2x_3x_4 \frac{\partial f}{\partial x_1}=0,\frac{\partial f}{\partial x_2}=0,\frac{\partial f}{\partial x_3}=0,\frac{\partial f}{\partial x_4}=0, x_2+x_2x_3x_4=0 x_3+x_1+x_1x_3x_4=0 x_2+x_4+x_1x_2x_4=0 x_3+x_1x_2x_3=0","['multivariable-calculus', 'optimization', 'systems-of-equations', 'maxima-minima']"
53,Mean Value Theorem for Convex derivatives,Mean Value Theorem for Convex derivatives,,"Let $f:\mathbb R^n\to\mathbb R^m$ be differentiable. Assume that the set of   derivatives $$\{f'(x)\in L(\mathbb R^n,\mathbb R^m):x\in [a,b]\} \text{ is convex.}$$ Prove that there exist a $θ$ in $[a,b]$ such that $f(b)−f(a)=f'(θ)(b−a)$. This is problem number $17$ from chapter $5$ of "" Real mathematical analysis "" by Charles Chapman Pugh [Springer (February 19, 2010)]. If $f$ is $C^1$, there is a straightforward proof. Could anyone help to prove or disprove this statement without $C^1$ condition?","Let $f:\mathbb R^n\to\mathbb R^m$ be differentiable. Assume that the set of   derivatives $$\{f'(x)\in L(\mathbb R^n,\mathbb R^m):x\in [a,b]\} \text{ is convex.}$$ Prove that there exist a $θ$ in $[a,b]$ such that $f(b)−f(a)=f'(θ)(b−a)$. This is problem number $17$ from chapter $5$ of "" Real mathematical analysis "" by Charles Chapman Pugh [Springer (February 19, 2010)]. If $f$ is $C^1$, there is a straightforward proof. Could anyone help to prove or disprove this statement without $C^1$ condition?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'convex-analysis', 'examples-counterexamples']"
54,"How do I find the maximum and minimum points of f(x,y) on a unit circle?","How do I find the maximum and minimum points of f(x,y) on a unit circle?",,"I'm supposed to find the maximum and minimum values of $f(x,y) = xy^2$ on the circle $x^2 + y^2 = 1$. My work so far: $\bigtriangledown f = (y^2 ,xy)$ Let $g(x,y) = x^2 + y^2 $ $\bigtriangledown g = (2x,2y)$ $\bigtriangledown f = \lambda \bigtriangledown g$ and I'm not really sure where to go from here.","I'm supposed to find the maximum and minimum values of $f(x,y) = xy^2$ on the circle $x^2 + y^2 = 1$. My work so far: $\bigtriangledown f = (y^2 ,xy)$ Let $g(x,y) = x^2 + y^2 $ $\bigtriangledown g = (2x,2y)$ $\bigtriangledown f = \lambda \bigtriangledown g$ and I'm not really sure where to go from here.",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
55,Find the critical point of the function,Find the critical point of the function,,"Consider the following function. $g(x, y)=e^{−8x^2−6y^2+24y}$ (a) Find the critical point of g. (b) Using your critical point in (a), find the value of D(a, b) from the Second Partials test that is used to classify the critical point. (c) Use the Second Partials test to classify the critical point from (a). For C, the options are either Saddle Point, Relative Minimum, Relative Maximum, or Inconclusive I could really used some help finding the critical point. I separated them into $f_x$ and $f_y$ and set them equal to 0 and got (24,0). I'm not sure if this answer is right and what I am supposed to do to find b.","Consider the following function. $g(x, y)=e^{−8x^2−6y^2+24y}$ (a) Find the critical point of g. (b) Using your critical point in (a), find the value of D(a, b) from the Second Partials test that is used to classify the critical point. (c) Use the Second Partials test to classify the critical point from (a). For C, the options are either Saddle Point, Relative Minimum, Relative Maximum, or Inconclusive I could really used some help finding the critical point. I separated them into $f_x$ and $f_y$ and set them equal to 0 and got (24,0). I'm not sure if this answer is right and what I am supposed to do to find b.",,"['calculus', 'multivariable-calculus']"
56,Using Lagrange Multipliers to determine the point on a surface nearest to P,Using Lagrange Multipliers to determine the point on a surface nearest to P,,"I'm attempting to figure this problem out. I would appreciate some guidance on how to get the answer. Thanks. Consider the surface defined as $S: x^2+y^2+z^2 = 8$. If we have a $P = (0,1,1)$, use Lagrange multipliers to determine the point on $S$ nearest to $P$.","I'm attempting to figure this problem out. I would appreciate some guidance on how to get the answer. Thanks. Consider the surface defined as $S: x^2+y^2+z^2 = 8$. If we have a $P = (0,1,1)$, use Lagrange multipliers to determine the point on $S$ nearest to $P$.",,"['calculus', 'multivariable-calculus', 'surfaces', 'lagrange-multiplier']"
57,"Is this a composition of two functions of random variables, or two independent functions?","Is this a composition of two functions of random variables, or two independent functions?",,"I'm reviewing some topics that were tricky for me in the past, and this problem from Grimmett and Stirzaker (4.7.1) came up: Let $X,Y,Z$ be independent and uniformly distributed on $[0,1]$. Find the joint density function of $XY$ and $Z^2$, and show that $P(XY < Z^2) = \frac{5}{9}$. I'm not clever with these types of problems, so I just stick with the change of variables, but I don't know if I'm preparing the right statements to evaluate $P(XY < Z^2)$ (as the second part of the problem is clearly just a self-test): Let $u = xy, v = x, w = z^2 \implies t_1(v) = v, t_2(u,v) = \frac{u}{v}, t_3(w) = \sqrt{w}$. Compute Jacobian: \begin{equation}      \left| \begin{matrix} 0 & \frac{1}{v} & 0 \\                            1 & -\frac{u}{v^2} & 0 \\                            0 & 0 & \frac{1}{2\sqrt{w}}      \end{matrix} \right| = -\frac{1}{2v\sqrt{w}}    \end{equation} Stating the joint density function: \begin{equation}    f_{U, V, W} (u, v, w) = f(t_1(v), t_2(u,v), t_3(w))|J(v,w)|    \end{equation} (in the domain of the variables) \begin{equation}    f_{U, W} (u, w) = f(x, \frac{u}{x}, \sqrt{w}) \left| \frac{1}{2x\sqrt{w}}\right|    \end{equation} (for $w \geq 0$) Assuming I have that right, I think $P(XY < Z^2)$ can be phrased as \begin{equation}    P(XY < Z^2) = F_{U} (u=w) = \int_{-\infty}^w \int_{-\infty}^\infty f(x, \frac{u}{x}) \left| \frac{1}{x}\right| dxdu    \end{equation} (for $w \geq 0$), since $Z^2$ is independent of $XY$. I don't have any confidence in what I did in (3) and (4), however (despite the $XY$ function being derived as an example in the same text), and I'm not sure how to come up with a PDF to substitute in the last integrand to compute with. Am I on the right track?","I'm reviewing some topics that were tricky for me in the past, and this problem from Grimmett and Stirzaker (4.7.1) came up: Let $X,Y,Z$ be independent and uniformly distributed on $[0,1]$. Find the joint density function of $XY$ and $Z^2$, and show that $P(XY < Z^2) = \frac{5}{9}$. I'm not clever with these types of problems, so I just stick with the change of variables, but I don't know if I'm preparing the right statements to evaluate $P(XY < Z^2)$ (as the second part of the problem is clearly just a self-test): Let $u = xy, v = x, w = z^2 \implies t_1(v) = v, t_2(u,v) = \frac{u}{v}, t_3(w) = \sqrt{w}$. Compute Jacobian: \begin{equation}      \left| \begin{matrix} 0 & \frac{1}{v} & 0 \\                            1 & -\frac{u}{v^2} & 0 \\                            0 & 0 & \frac{1}{2\sqrt{w}}      \end{matrix} \right| = -\frac{1}{2v\sqrt{w}}    \end{equation} Stating the joint density function: \begin{equation}    f_{U, V, W} (u, v, w) = f(t_1(v), t_2(u,v), t_3(w))|J(v,w)|    \end{equation} (in the domain of the variables) \begin{equation}    f_{U, W} (u, w) = f(x, \frac{u}{x}, \sqrt{w}) \left| \frac{1}{2x\sqrt{w}}\right|    \end{equation} (for $w \geq 0$) Assuming I have that right, I think $P(XY < Z^2)$ can be phrased as \begin{equation}    P(XY < Z^2) = F_{U} (u=w) = \int_{-\infty}^w \int_{-\infty}^\infty f(x, \frac{u}{x}) \left| \frac{1}{x}\right| dxdu    \end{equation} (for $w \geq 0$), since $Z^2$ is independent of $XY$. I don't have any confidence in what I did in (3) and (4), however (despite the $XY$ function being derived as an example in the same text), and I'm not sure how to come up with a PDF to substitute in the last integrand to compute with. Am I on the right track?",,"['multivariable-calculus', 'random-variables', 'jacobian']"
58,Linear transformation and derivative,Linear transformation and derivative,,Let $f:\mathbb R^n \rightarrow \mathbb R^m$ be linear. Find $D^2f(\mathbf x_0)$. I do not really know how to start from here. We are not given any more details to solve the derivative.,Let $f:\mathbb R^n \rightarrow \mathbb R^m$ be linear. Find $D^2f(\mathbf x_0)$. I do not really know how to start from here. We are not given any more details to solve the derivative.,,"['calculus', 'real-analysis']"
59,What are the methods to manipulate a gradient?,What are the methods to manipulate a gradient?,,"Vector Calc has been a while, and I can't seem to find anything about this online. How can I solve for D in an equation like so: $$\nabla\cdot D = \rho_v$$ How do I ""move"" the gradient to the other side?","Vector Calc has been a while, and I can't seem to find anything about this online. How can I solve for D in an equation like so: $$\nabla\cdot D = \rho_v$$ How do I ""move"" the gradient to the other side?",,"['multivariable-calculus', 'vector-analysis']"
60,Why does tangent vector to a curve always point in direction of increasing t?,Why does tangent vector to a curve always point in direction of increasing t?,,"I understand that the tangent vector is defined by the equation: $$\lim_{h\to0} \frac{r(t+h) - r(t)}{h}$$ I know that the right and left hand limits are positive (even when $h < 0$, the negative denominator will produce a positive quotient). I know that this then means the tangent vector points in the positive direction. What I don't understand is how the positive direction necessarily means the direction of increasing t? I guess I'm confused as how the positive or negative direction of the vector would correlate to increasing or decreasing values of t.","I understand that the tangent vector is defined by the equation: $$\lim_{h\to0} \frac{r(t+h) - r(t)}{h}$$ I know that the right and left hand limits are positive (even when $h < 0$, the negative denominator will produce a positive quotient). I know that this then means the tangent vector points in the positive direction. What I don't understand is how the positive direction necessarily means the direction of increasing t? I guess I'm confused as how the positive or negative direction of the vector would correlate to increasing or decreasing values of t.",,"['limits', 'multivariable-calculus']"
61,Reconciliating the definitions of derivative for a curve in $\mathbb{R}^n$,Reconciliating the definitions of derivative for a curve in,\mathbb{R}^n,"It seems to me that two very different definitions of derivative permeate the literature: in introductory calculus, the derivative $f'(x)$ of a (differentiable) function $f:U\subseteq\mathbb{R}\rightarrow\mathbb{R}$ at a point $x\in\mathbb{R}$ is a real number . When one moves on to multivariable calculus, the derivative $Df(x)$ of a function $f:U\subseteq\mathbb{R}^m\rightarrow\mathbb{R}^n$ at a point $x\in\mathbb{R}^m$ is a linear mapping $Df(x):\mathbb{R}^m\rightarrow\mathbb{R}^n$. However, when one talk about curves $\gamma:\mathbb{R}\rightarrow\mathbb{R}^n$, we are already in the domain of multivariable calculus, so the derivative $\gamma'(x)=D\gamma(x)$ of $\gamma$ at a point $x\in\mathbb{R}$ should be a linear mapping $D\gamma(x):\mathbb{R}\rightarrow\mathbb{R}^n$. But usually I see people using $\gamma'(x)$ as an element of $\mathbb{R}^n$. How to reconcile those? My guess is that $\mathcal{L}(\mathbb{R},\mathbb{R}^n)\simeq M_{1\times n}(\mathbb{R})\simeq\mathbb{R}^n$, so people just identify $\gamma'(x)$ with its tuple in $\mathbb{R}^n$, just like sometimes one speaks of a matrix as a linear map or a linear map as being a matrix.","It seems to me that two very different definitions of derivative permeate the literature: in introductory calculus, the derivative $f'(x)$ of a (differentiable) function $f:U\subseteq\mathbb{R}\rightarrow\mathbb{R}$ at a point $x\in\mathbb{R}$ is a real number . When one moves on to multivariable calculus, the derivative $Df(x)$ of a function $f:U\subseteq\mathbb{R}^m\rightarrow\mathbb{R}^n$ at a point $x\in\mathbb{R}^m$ is a linear mapping $Df(x):\mathbb{R}^m\rightarrow\mathbb{R}^n$. However, when one talk about curves $\gamma:\mathbb{R}\rightarrow\mathbb{R}^n$, we are already in the domain of multivariable calculus, so the derivative $\gamma'(x)=D\gamma(x)$ of $\gamma$ at a point $x\in\mathbb{R}$ should be a linear mapping $D\gamma(x):\mathbb{R}\rightarrow\mathbb{R}^n$. But usually I see people using $\gamma'(x)$ as an element of $\mathbb{R}^n$. How to reconcile those? My guess is that $\mathcal{L}(\mathbb{R},\mathbb{R}^n)\simeq M_{1\times n}(\mathbb{R})\simeq\mathbb{R}^n$, so people just identify $\gamma'(x)$ with its tuple in $\mathbb{R}^n$, just like sometimes one speaks of a matrix as a linear map or a linear map as being a matrix.",,"['multivariable-calculus', 'differential-geometry', 'curves']"
62,Different Curvatures Obtained Depending on the Form of the Curvature Equation,Different Curvatures Obtained Depending on the Form of the Curvature Equation,,"Thank you ahead of time for taking a look at this. My problem is as follows: We have two forms of the equation for the curvature of a space curve $\mathbf{r}(t)$ given by \begin{equation} \kappa(t) = \frac{|\mathbf{T'(t)}|}{|\mathbf{r'(t)}|} \end{equation} and \begin{equation} \kappa(t) = \frac{|\mathbf{r'}(t)\times\mathbf{r''}(t)|}{|\mathbf{r'(t)}|^3} \end{equation} All authors I have been able to find equate the two forms, indicating that both will give the same curvature of a space curve. However, for some space curves, I obtain different forms of the curvature depending on which form of the $\kappa$ equation I use. Take the example $y = 5e^x$. We parameterize this as \begin{equation*} \mathbf{r}(t)=\ <t,\ 5e^t> \end{equation*} and using the first form of the curvature formula obtain $|\mathbf{T'}(t)|= 5e^t$ and $|\mathbf{r'}(t)|= \sqrt{1+25e^{2t}}$ which gives \begin{equation*} \kappa(t) = \frac{5e^t}{\sqrt{1+25e^{2t}}} \end{equation*} yet when I use the second curvature formula I obtain \begin{equation*} \kappa(t) = \frac{5e^t}{(1+25e^{2t})^{3/2}} \end{equation*} I have been puzzling at this for a while now and have not been able to ascertain as to why the two forms of the equation should give different answers.","Thank you ahead of time for taking a look at this. My problem is as follows: We have two forms of the equation for the curvature of a space curve $\mathbf{r}(t)$ given by \begin{equation} \kappa(t) = \frac{|\mathbf{T'(t)}|}{|\mathbf{r'(t)}|} \end{equation} and \begin{equation} \kappa(t) = \frac{|\mathbf{r'}(t)\times\mathbf{r''}(t)|}{|\mathbf{r'(t)}|^3} \end{equation} All authors I have been able to find equate the two forms, indicating that both will give the same curvature of a space curve. However, for some space curves, I obtain different forms of the curvature depending on which form of the $\kappa$ equation I use. Take the example $y = 5e^x$. We parameterize this as \begin{equation*} \mathbf{r}(t)=\ <t,\ 5e^t> \end{equation*} and using the first form of the curvature formula obtain $|\mathbf{T'}(t)|= 5e^t$ and $|\mathbf{r'}(t)|= \sqrt{1+25e^{2t}}$ which gives \begin{equation*} \kappa(t) = \frac{5e^t}{\sqrt{1+25e^{2t}}} \end{equation*} yet when I use the second curvature formula I obtain \begin{equation*} \kappa(t) = \frac{5e^t}{(1+25e^{2t})^{3/2}} \end{equation*} I have been puzzling at this for a while now and have not been able to ascertain as to why the two forms of the equation should give different answers.",,"['calculus', 'multivariable-calculus', 'curvature']"
63,Is the Laplace operator w.r.t. all coordinates or not?,Is the Laplace operator w.r.t. all coordinates or not?,,"I have the equation \begin{align} \begin{cases}     u_t(\boldsymbol{x},t)-\nabla^2u(\boldsymbol{x},t) = 0, &\boldsymbol{x}\in D, & t>0, \\     u(\boldsymbol{x},t)=0, & \boldsymbol{x}\in\partial D, & t>0, \\     u(\boldsymbol{x},0)=u_0(\boldsymbol{x}), &\boldsymbol{x}\in D, \end{cases} \end{align} where $\boldsymbol{x}=(x_1,x_2,x_3)$. Does the Laplace operator acts on the vector $\boldsymbol{x}$ AND $t$? I.e.  \begin{align} \nabla^2 u(\boldsymbol{x},t)=\frac{\partial^2 u}{\partial x_1^2}+\frac{\partial^2 u}{\partial x_2^2}+ \frac{\partial^2 u}{\partial x_3^2}+\frac{\partial^2 u}{\partial t^2} \end{align} or just \begin{align} \nabla^2 u(\boldsymbol{x},t)=\frac{\partial^2 u}{\partial x_1^2}+\frac{\partial^2 u}{\partial x_2^2}+ \frac{\partial^2 u}{\partial x_3^2} \end{align} If the second one is correct, why is it so?","I have the equation \begin{align} \begin{cases}     u_t(\boldsymbol{x},t)-\nabla^2u(\boldsymbol{x},t) = 0, &\boldsymbol{x}\in D, & t>0, \\     u(\boldsymbol{x},t)=0, & \boldsymbol{x}\in\partial D, & t>0, \\     u(\boldsymbol{x},0)=u_0(\boldsymbol{x}), &\boldsymbol{x}\in D, \end{cases} \end{align} where $\boldsymbol{x}=(x_1,x_2,x_3)$. Does the Laplace operator acts on the vector $\boldsymbol{x}$ AND $t$? I.e.  \begin{align} \nabla^2 u(\boldsymbol{x},t)=\frac{\partial^2 u}{\partial x_1^2}+\frac{\partial^2 u}{\partial x_2^2}+ \frac{\partial^2 u}{\partial x_3^2}+\frac{\partial^2 u}{\partial t^2} \end{align} or just \begin{align} \nabla^2 u(\boldsymbol{x},t)=\frac{\partial^2 u}{\partial x_1^2}+\frac{\partial^2 u}{\partial x_2^2}+ \frac{\partial^2 u}{\partial x_3^2} \end{align} If the second one is correct, why is it so?",,"['multivariable-calculus', 'partial-differential-equations', 'vector-analysis']"
64,Why is there a difference between coordinate variables and vector components when transforming coordinates?,Why is there a difference between coordinate variables and vector components when transforming coordinates?,,"My textbook has a table of coordinate transformations. Why are the coordinate variables and the vector components different? For example, for the conversion between spherical to cartesian for ""z"", it is equivalent to $R\cos(\theta)$, which I understand. However, it is also said that the vector component of ""z"" is $A_R\cos(\theta)-A_\theta\sin(\theta)$ Why is this the case that they are different?","My textbook has a table of coordinate transformations. Why are the coordinate variables and the vector components different? For example, for the conversion between spherical to cartesian for ""z"", it is equivalent to $R\cos(\theta)$, which I understand. However, it is also said that the vector component of ""z"" is $A_R\cos(\theta)-A_\theta\sin(\theta)$ Why is this the case that they are different?",,"['multivariable-calculus', 'vector-spaces', 'vectors', 'vector-analysis', 'coordinate-systems']"
65,"Max and Min$ f(x,y)=x^3-12xy+8y^3$",Max and Min," f(x,y)=x^3-12xy+8y^3","First I solved $f_y=0$ then plugged in my variable into $f_x$ to get an output and then plugged that output back into $f_y$ to get a point and did this again for $f_x$ I think I screwed up on the $(-7,\sqrt{3})$ $f_y=-12x+24y^2$ $-12x+24y^2=0$ $-12x=24y^2$ $x=2y^2$ $f_x(2y^2,y)=3(2y^2)^2-12y$ $3(4y^4)-12y=0$ $12y^4 - 36y=0$ $12y(y^3-3=0) \to y=0$ or $y=\sqrt{3}$ $f_y(x,0)=-12x=0 \to (0,0)$ $f_y(x,\sqrt{3}) = -12x+24(3)$ $f_y(x.sqrt{3})= -12x-84=0$ $f_y(x,\sqrt{3})=-12x=84$ $x=-7 \to (-7,\sqrt{3})$ $f_y=-12x+24y^2=0$ $-12x=24y^2$ $x=2y^2$ $f_x(2y^2,y)=3(2y^2)^2-12y=0$ $12y^4-12y=0$ $12y(y^3-1)=0$ $y=0$ or $y=1$ $f_y(x,o)= -12x=0 \to x=0$ $f_y(x,1)=-12x+24=0$ $f_y(x,1)=x=2$ Critical points: $(2,-1),(0,0),(-7,\sqrt{3})$ $D=(6x)(48y)-(-12)^2$ $D(0,0)=-144 so (0,0) is a saddle point $D(,2,-1)= Min D(-7,\sqrt{3})= Saddle point","First I solved $f_y=0$ then plugged in my variable into $f_x$ to get an output and then plugged that output back into $f_y$ to get a point and did this again for $f_x$ I think I screwed up on the $(-7,\sqrt{3})$ $f_y=-12x+24y^2$ $-12x+24y^2=0$ $-12x=24y^2$ $x=2y^2$ $f_x(2y^2,y)=3(2y^2)^2-12y$ $3(4y^4)-12y=0$ $12y^4 - 36y=0$ $12y(y^3-3=0) \to y=0$ or $y=\sqrt{3}$ $f_y(x,0)=-12x=0 \to (0,0)$ $f_y(x,\sqrt{3}) = -12x+24(3)$ $f_y(x.sqrt{3})= -12x-84=0$ $f_y(x,\sqrt{3})=-12x=84$ $x=-7 \to (-7,\sqrt{3})$ $f_y=-12x+24y^2=0$ $-12x=24y^2$ $x=2y^2$ $f_x(2y^2,y)=3(2y^2)^2-12y=0$ $12y^4-12y=0$ $12y(y^3-1)=0$ $y=0$ or $y=1$ $f_y(x,o)= -12x=0 \to x=0$ $f_y(x,1)=-12x+24=0$ $f_y(x,1)=x=2$ Critical points: $(2,-1),(0,0),(-7,\sqrt{3})$ $D=(6x)(48y)-(-12)^2$ $D(0,0)=-144 so (0,0) is a saddle point $D(,2,-1)= Min D(-7,\sqrt{3})= Saddle point",,"['calculus', 'multivariable-calculus']"
66,Equation of tangent plane.,Equation of tangent plane.,,"I have trouble understanding the proof of tangent plane: let $z=f(x,y)$ and $z_0=f(x_0,y_0)$, for $x=x_0$ the tangent line to $z$ at $(x_0,y_0,z_0) $ is along $(0,1,\frac{\partial f}{\partial y}(x_0,y_0))$. How did we get the above result ?","I have trouble understanding the proof of tangent plane: let $z=f(x,y)$ and $z_0=f(x_0,y_0)$, for $x=x_0$ the tangent line to $z$ at $(x_0,y_0,z_0) $ is along $(0,1,\frac{\partial f}{\partial y}(x_0,y_0))$. How did we get the above result ?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'tangent-line']"
67,given derivative to get the original function,given derivative to get the original function,,"Let $f : \mathbb R^n → \mathbb R^m$ be a differentiable function such that $f (\mathbf 0) = \mathbf 0$ and $D f (\mathbf x)(\mathbf h) = 2⟨\mathbf x, \mathbf h⟩$ for any $\mathbf x, \mathbf h ∈ \mathbb R^n$. Show that $f (\mathbf x) = ⟨\mathbf x, \mathbf x⟩$. How should I use the theorem that $f : U ⊆ \mathbb R^n → \mathbb R^m$ with $U$ open and connected. If $D f (\mathbf x) = O$ for every $x ∈ U$, then $f$ is constant to prove this? Thanks.","Let $f : \mathbb R^n → \mathbb R^m$ be a differentiable function such that $f (\mathbf 0) = \mathbf 0$ and $D f (\mathbf x)(\mathbf h) = 2⟨\mathbf x, \mathbf h⟩$ for any $\mathbf x, \mathbf h ∈ \mathbb R^n$. Show that $f (\mathbf x) = ⟨\mathbf x, \mathbf x⟩$. How should I use the theorem that $f : U ⊆ \mathbb R^n → \mathbb R^m$ with $U$ open and connected. If $D f (\mathbf x) = O$ for every $x ∈ U$, then $f$ is constant to prove this? Thanks.",,['real-analysis']
68,Showing $\int_C \ln |x - y| dy = 0$ where $C$ is the unit circle?,Showing  where  is the unit circle?,\int_C \ln |x - y| dy = 0 C,"I saw it mentioned in a proof that $$\int_C \ln |x - y| dy = 0,$$ where $x, y \in \mathbb{R}^2$ and they are both on the unit circle $C$. I'm trying to figure out the calculation but I'm not sure how to show it. First of all I switch to polars and get $$\int_0^{2\pi} \ln \sqrt{(\cos \theta_x - \cos\theta_y)^2 - (\sin \theta_x - \sin\theta_y)^2} d\theta_y \\ = \frac{1}{2}\int_0^{2\pi} \ln (\cos^2 \theta_x - 2 \cos \theta_x \cos \theta_y + \cos^2 \theta_y + \sin^2 \theta_x - 2 \sin \theta_x \sin \theta_y + \sin^2 \theta_y) d\theta_y \\ = \frac{1}{2}\int_0^{2\pi} \ln (2 - 2 \cos \theta_x \cos \theta_y - 2 \sin \theta_x \sin \theta_y) d\theta_y \\ = \frac{1}{2}\int_0^{2\pi} \ln 2 d\theta_y  + \frac{1}{2}\int_0^{2\pi} \ln (1 - (\cos \theta_x \cos \theta_y + \sin \theta_x \sin \theta_y) d\theta_y$$ What steps are needed to get this expression equal to zero?","I saw it mentioned in a proof that $$\int_C \ln |x - y| dy = 0,$$ where $x, y \in \mathbb{R}^2$ and they are both on the unit circle $C$. I'm trying to figure out the calculation but I'm not sure how to show it. First of all I switch to polars and get $$\int_0^{2\pi} \ln \sqrt{(\cos \theta_x - \cos\theta_y)^2 - (\sin \theta_x - \sin\theta_y)^2} d\theta_y \\ = \frac{1}{2}\int_0^{2\pi} \ln (\cos^2 \theta_x - 2 \cos \theta_x \cos \theta_y + \cos^2 \theta_y + \sin^2 \theta_x - 2 \sin \theta_x \sin \theta_y + \sin^2 \theta_y) d\theta_y \\ = \frac{1}{2}\int_0^{2\pi} \ln (2 - 2 \cos \theta_x \cos \theta_y - 2 \sin \theta_x \sin \theta_y) d\theta_y \\ = \frac{1}{2}\int_0^{2\pi} \ln 2 d\theta_y  + \frac{1}{2}\int_0^{2\pi} \ln (1 - (\cos \theta_x \cos \theta_y + \sin \theta_x \sin \theta_y) d\theta_y$$ What steps are needed to get this expression equal to zero?",,"['integration', 'multivariable-calculus', 'circles', 'polar-coordinates']"
69,Arc length integral for the curve $e^{2x+2y}=x-y$.,Arc length integral for the curve .,e^{2x+2y}=x-y,Arc length integral for the curve $$e^{2x+2y}=x-y$$ So I tried isolating $y$ or $x$ and got stuck with a $0 = 0$ at the end of it. I tried to also use partial derivatives. Am I going in the right direction?,Arc length integral for the curve $$e^{2x+2y}=x-y$$ So I tried isolating $y$ or $x$ and got stuck with a $0 = 0$ at the end of it. I tried to also use partial derivatives. Am I going in the right direction?,,['multivariable-calculus']
70,Double Integral bounds,Double Integral bounds,,"I am attempting a question that asks me to evaluate $$ \iint_D (x^2+y^2) dx dy  $$ where $D$ is the finite region in the positive quadrant bounded by the curves  $$ x^2 - y^2 = \pm 1, \quad xy = \frac{1}{2}. $$ So, I thought of using the transformation $x^2 - y^2 = u$ and $xy=v$, and computed the Jacobian and all that and get to the integral $$ \iint_D \frac{1}{v} du dv. $$ But I can't get the bounds to work. I thought $u$ would be between $-1$ and $1$, and $v$ would be between $0$ and $\frac{1}{2}$. However then the integral is not finite. Thoughts?","I am attempting a question that asks me to evaluate $$ \iint_D (x^2+y^2) dx dy  $$ where $D$ is the finite region in the positive quadrant bounded by the curves  $$ x^2 - y^2 = \pm 1, \quad xy = \frac{1}{2}. $$ So, I thought of using the transformation $x^2 - y^2 = u$ and $xy=v$, and computed the Jacobian and all that and get to the integral $$ \iint_D \frac{1}{v} du dv. $$ But I can't get the bounds to work. I thought $u$ would be between $-1$ and $1$, and $v$ would be between $0$ and $\frac{1}{2}$. However then the integral is not finite. Thoughts?",,"['calculus', 'integration', 'multivariable-calculus']"
71,How do I prove this function is differentiable?,How do I prove this function is differentiable?,,"$$f(x) = \begin{cases} \frac{ \langle Mx, x \rangle}{\| x \|}, & \text{for } x \neq 0 \\\\ 0, & \text{for } x = 0 \end{cases}$$ $M$ is a symmetric matrix and $x \in \mathbb{R^n}$ Is there a particular property from linear algebra I need to keep in mind?","$$f(x) = \begin{cases} \frac{ \langle Mx, x \rangle}{\| x \|}, & \text{for } x \neq 0 \\\\ 0, & \text{for } x = 0 \end{cases}$$ $M$ is a symmetric matrix and $x \in \mathbb{R^n}$ Is there a particular property from linear algebra I need to keep in mind?",,"['analysis', 'multivariable-calculus', 'derivatives']"
72,Gateaux and Frechet derivatives on vector valued functions,Gateaux and Frechet derivatives on vector valued functions,,"If $f:\mathbb{R}\to\mathbb{X}$ is a function from the real numbers to any normed vector space (finite or infinite dimension), and $f$ is Gateaux differentiable, is $f$ necessarily Frechet differentiable?","If $f:\mathbb{R}\to\mathbb{X}$ is a function from the real numbers to any normed vector space (finite or infinite dimension), and $f$ is Gateaux differentiable, is $f$ necessarily Frechet differentiable?",,"['real-analysis', 'functional-analysis', 'multivariable-calculus', 'gateaux-derivative', 'frechet-derivative']"
73,How to find the maximal volume of a rectangular box given a fixed surface area?,How to find the maximal volume of a rectangular box given a fixed surface area?,,"I am asked to find the maximal volume of a rectangular box with a fixed surface area of $150$. I would like to solve this problem using gradients/Lagrange multipliers. I have done a bit of work so far but I'm not sure if I am on the right track or how to complete the problem. I know that this box should be a cube, but I would like to show this without assuming it is true. Here is what I have so far: $Volume=V(l,w,h)=lwh$ $Surface Area=S(l,w,h)=2(lw+hw+hl)$. $\triangledown V(l,w,h)= \lambda  \triangledown S(l,w,h)$ $\triangledown V(l,w,h)=(wh)\hat{i} + (lh)\hat{j} + (wl)\hat{k}$ $\triangledown S(l,w,h)=(2w+2h)\hat{i} + (2l+2h)\hat{j} + (2w+2l)\hat{k}$ So putting this all together gives me: $(wh)\hat{i} + (lh)\hat{j} + (wl)\hat{k} =\lambda [(2w+2h)\hat{i} + (2l+2h)\hat{j} + (2w+2l)\hat{k}]$ And at this point I am not sure how to conclude that $w=h=l$ . Additionally, I see that I can look at the system of equations: $wh=\lambda2(w+h)$ $lh=\lambda2(l+h)$ $wl=\lambda2(w+l)$ $2(lh+wh+lw)=150$ But I am unsure of how to solve these.","I am asked to find the maximal volume of a rectangular box with a fixed surface area of $150$. I would like to solve this problem using gradients/Lagrange multipliers. I have done a bit of work so far but I'm not sure if I am on the right track or how to complete the problem. I know that this box should be a cube, but I would like to show this without assuming it is true. Here is what I have so far: $Volume=V(l,w,h)=lwh$ $Surface Area=S(l,w,h)=2(lw+hw+hl)$. $\triangledown V(l,w,h)= \lambda  \triangledown S(l,w,h)$ $\triangledown V(l,w,h)=(wh)\hat{i} + (lh)\hat{j} + (wl)\hat{k}$ $\triangledown S(l,w,h)=(2w+2h)\hat{i} + (2l+2h)\hat{j} + (2w+2l)\hat{k}$ So putting this all together gives me: $(wh)\hat{i} + (lh)\hat{j} + (wl)\hat{k} =\lambda [(2w+2h)\hat{i} + (2l+2h)\hat{j} + (2w+2l)\hat{k}]$ And at this point I am not sure how to conclude that $w=h=l$ . Additionally, I see that I can look at the system of equations: $wh=\lambda2(w+h)$ $lh=\lambda2(l+h)$ $wl=\lambda2(w+l)$ $2(lh+wh+lw)=150$ But I am unsure of how to solve these.",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
74,"Find the PDF of G(X,Y)=$(X,\frac{9}{(X+Y)^{2}})$ Transformation given the pdf of $f_{XY}(x,y)$","Find the PDF of G(X,Y)= Transformation given the pdf of","(X,\frac{9}{(X+Y)^{2}}) f_{XY}(x,y)","So I made up a problem for myself, and am sort of confused on how to get started.  if you work out the full solution I will be sad! This is a deviation from a book problem in Casella and Burger 4.4 where I've made the problem a bit harder. and am stuck. so here is the joint pdf $f(x,y)=\frac{1}{4}(x+2y)$ for $0<y<1$ and $0<x<2$ and 0 other wise. My question has to do with transformations of multi-variable probability densities Let $G(X,Y)=(g_{1}(X,Y),g_{2}(X,Y))= (X,\frac{9}{(X+Y)^{2}})$ be a random variable $(X,Y) \to (X,V)$ Now assume that G is is invertible and now we find the inverse on this interval. doing some algebra returns H(U,V)= (U,$\frac{3}{\sqrt{V}}-U)$.  a little more algebra to find the Jacobian returns $|J| = \frac{3}{2}v^{-\frac{3}{2}}$ Now finding the joint pdf under the transformation  \begin{equation} \begin{split} f_{UV}(u,v) &=\frac{1}{4}(u + 2(\frac{3}{\sqrt{v}} - u))\cdot \frac{3}{2}v^{-\frac{3}{2}} \\  &= \frac{3}{8}(\frac{6}{v^{2}}- \frac{u}{v^{\frac{3}{2}}}) \end{split} \end{equation} now assuming I escaped an algebra mistake, is the set up that I've done correct? essentially the conceptual idea is that given a joint PDF of X,Y I looked at a transformation of G, found the inverse and the Jacobian. and now the joint pdf $f_{UV}(u,v)$ is joint pdf of X,Y under the map of G ? thank you all for any help","So I made up a problem for myself, and am sort of confused on how to get started.  if you work out the full solution I will be sad! This is a deviation from a book problem in Casella and Burger 4.4 where I've made the problem a bit harder. and am stuck. so here is the joint pdf $f(x,y)=\frac{1}{4}(x+2y)$ for $0<y<1$ and $0<x<2$ and 0 other wise. My question has to do with transformations of multi-variable probability densities Let $G(X,Y)=(g_{1}(X,Y),g_{2}(X,Y))= (X,\frac{9}{(X+Y)^{2}})$ be a random variable $(X,Y) \to (X,V)$ Now assume that G is is invertible and now we find the inverse on this interval. doing some algebra returns H(U,V)= (U,$\frac{3}{\sqrt{V}}-U)$.  a little more algebra to find the Jacobian returns $|J| = \frac{3}{2}v^{-\frac{3}{2}}$ Now finding the joint pdf under the transformation  \begin{equation} \begin{split} f_{UV}(u,v) &=\frac{1}{4}(u + 2(\frac{3}{\sqrt{v}} - u))\cdot \frac{3}{2}v^{-\frac{3}{2}} \\  &= \frac{3}{8}(\frac{6}{v^{2}}- \frac{u}{v^{\frac{3}{2}}}) \end{split} \end{equation} now assuming I escaped an algebra mistake, is the set up that I've done correct? essentially the conceptual idea is that given a joint PDF of X,Y I looked at a transformation of G, found the inverse and the Jacobian. and now the joint pdf $f_{UV}(u,v)$ is joint pdf of X,Y under the map of G ? thank you all for any help",,"['probability', 'multivariable-calculus', 'probability-distributions']"
75,Polar coordinates; Jacobian and order of the variables,Polar coordinates; Jacobian and order of the variables,,"I'm trying to understand the Jacobian a bit. I know the general form of the transformation with two variables: $$\int\int_Ag(x,y)\,\mathrm dx\,\mathrm dy=\int\int_{T(A)}g\big(x(u,v),y(u,v)\big)\lvert J(u,n)\rvert \,\mathrm du\,\mathrm dv.$$ Say we'd like to change to polar coordinates; $$\int\int_Ag(x,y)\,\mathrm dx\,\mathrm dy=\int\int_{T(A)}g\big(x(r,\theta),y(r,\theta)\big)\lvert J(r,\theta)\rvert \,\mathrm dr\,\mathrm d\theta.$$ How do we know the order of $(r,\theta)$? Could it also have been $(\theta,r)$? This would yield to the same Jacobian determinant but with the opposite sign.","I'm trying to understand the Jacobian a bit. I know the general form of the transformation with two variables: $$\int\int_Ag(x,y)\,\mathrm dx\,\mathrm dy=\int\int_{T(A)}g\big(x(u,v),y(u,v)\big)\lvert J(u,n)\rvert \,\mathrm du\,\mathrm dv.$$ Say we'd like to change to polar coordinates; $$\int\int_Ag(x,y)\,\mathrm dx\,\mathrm dy=\int\int_{T(A)}g\big(x(r,\theta),y(r,\theta)\big)\lvert J(r,\theta)\rvert \,\mathrm dr\,\mathrm d\theta.$$ How do we know the order of $(r,\theta)$? Could it also have been $(\theta,r)$? This would yield to the same Jacobian determinant but with the opposite sign.",,"['multivariable-calculus', 'polar-coordinates', 'jacobian']"
76,"$\mathbb{R}$ such that $f(x)=e^{||x||^2-2<a,x>}$. Find $Df(x)$ and local max and min.",such that . Find  and local max and min.,"\mathbb{R} f(x)=e^{||x||^2-2<a,x>} Df(x)","Let $a$ element of $\mathbb{R^n}$, a $f$ functiom from $\mathbb{R^n}$ to $\mathbb{R}$ such that $$f(x)=e^{||x||^2-2<a,x>}$$. Find $Df(x)$ and local max and min. My solution: First of all I want to get $Df(x)$ soo the function $f$ sholud be written as a composition of functions. But than I see that $||x||^2=<x,x>$, so our function can be written as $f(x)=e^{<x,x>-2<a,x>}=e^{<x-2a,x>}$. Now our function looks a little better. Let's $s(x)=<x-2a,x>$ s is linnar operator so we know how $Ds$ looks. $$Df(x)(h)=Des(x)[Ds(x)(h)]=e^{<x-2a,x>}*<h-2a,h>$$ My question is it this ok for first part of task?","Let $a$ element of $\mathbb{R^n}$, a $f$ functiom from $\mathbb{R^n}$ to $\mathbb{R}$ such that $$f(x)=e^{||x||^2-2<a,x>}$$. Find $Df(x)$ and local max and min. My solution: First of all I want to get $Df(x)$ soo the function $f$ sholud be written as a composition of functions. But than I see that $||x||^2=<x,x>$, so our function can be written as $f(x)=e^{<x,x>-2<a,x>}=e^{<x-2a,x>}$. Now our function looks a little better. Let's $s(x)=<x-2a,x>$ s is linnar operator so we know how $Ds$ looks. $$Df(x)(h)=Des(x)[Ds(x)(h)]=e^{<x-2a,x>}*<h-2a,h>$$ My question is it this ok for first part of task?",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'maxima-minima']"
77,Volume of frustum cut by an inclined plane at distance h,Volume of frustum cut by an inclined plane at distance h,,"If i have frustum and its top is cut by an inclined plane at angle $\alpha$, such that it makes an ellipse. The height is $h$ (at the axis of obliquely truncated frustum). How can i use triple integral to determine its volume, and coordinates of its geometric center. I will be thankful. I can determine the volume of elliptical cone by using parametric equation however i am confused to obtain parametric equations for right circular cone cut by inclined plane. The radius of the bottom surface is $R$ and top surface is $r$, as shown figure below. Frustum Figure","If i have frustum and its top is cut by an inclined plane at angle $\alpha$, such that it makes an ellipse. The height is $h$ (at the axis of obliquely truncated frustum). How can i use triple integral to determine its volume, and coordinates of its geometric center. I will be thankful. I can determine the volume of elliptical cone by using parametric equation however i am confused to obtain parametric equations for right circular cone cut by inclined plane. The radius of the bottom surface is $R$ and top surface is $r$, as shown figure below. Frustum Figure",,"['calculus', 'integration', 'geometry', 'multivariable-calculus', 'discrete-mathematics']"
78,Surface integral via divergence theorem and Stokes' theorem,Surface integral via divergence theorem and Stokes' theorem,,"I want to compute $\iint_{\Sigma}(\nabla\times F)\cdot \omega dS$ where $\Sigma:=\{(x,y,z)\in\mathbb{R}^3:\sqrt{x^2+y^2}=z, 2+\frac{x}{4}\leq z\leq 4\}$, $F=(y,z,x)$ and $\omega (-3,0,3)=\frac{\sqrt{2}}{2}(1,0,1)$. 1)  Applying the divergence theorem I got $\iint_{\Sigma}(\nabla\times F)\cdot \omega dS=\iiint_{V(\Sigma)}\nabla\cdot (\nabla\times F)dV-\iint_{z=4\cap cone}(\nabla\times F)\cdot \omega dS-\iint_{z=2+x/2\cap cone}(\nabla\times F)\cdot \omega dS=0-0-(-16\pi )=16\pi$ 2) To check the result I've also applied Stokes's theorem taking as C the circumference $\sqrt{x^2+y^2}=4$ and I get $\oint_C F\cdot dr=\int_{0}^{2\pi}(4\sin (t),4,4\cos (t))\cdot (-4\sin (t), 4\cos (t),0)dt- \iint_{z=2+x/2\cap cone} (\nabla\times F)\cdot \omega dS =16\pi -0=16\pi$ So, is what I did correct?","I want to compute $\iint_{\Sigma}(\nabla\times F)\cdot \omega dS$ where $\Sigma:=\{(x,y,z)\in\mathbb{R}^3:\sqrt{x^2+y^2}=z, 2+\frac{x}{4}\leq z\leq 4\}$, $F=(y,z,x)$ and $\omega (-3,0,3)=\frac{\sqrt{2}}{2}(1,0,1)$. 1)  Applying the divergence theorem I got $\iint_{\Sigma}(\nabla\times F)\cdot \omega dS=\iiint_{V(\Sigma)}\nabla\cdot (\nabla\times F)dV-\iint_{z=4\cap cone}(\nabla\times F)\cdot \omega dS-\iint_{z=2+x/2\cap cone}(\nabla\times F)\cdot \omega dS=0-0-(-16\pi )=16\pi$ 2) To check the result I've also applied Stokes's theorem taking as C the circumference $\sqrt{x^2+y^2}=4$ and I get $\oint_C F\cdot dr=\int_{0}^{2\pi}(4\sin (t),4,4\cos (t))\cdot (-4\sin (t), 4\cos (t),0)dt- \iint_{z=2+x/2\cap cone} (\nabla\times F)\cdot \omega dS =16\pi -0=16\pi$ So, is what I did correct?",,"['multivariable-calculus', 'surface-integrals', 'stokes-theorem', 'divergence-operator']"
79,"Finding extreme values of $f(x,y,z)=x^2+2y^2+3z^2$ on unit sphere $x^2+y^2+z^2=1$",Finding extreme values of  on unit sphere,"f(x,y,z)=x^2+2y^2+3z^2 x^2+y^2+z^2=1","I defined $G(x)=x^2+y^2+z^2-1$, such that the gradient of G $\nabla G=(2x, 2y, 2z)$, so that $\nabla f=(2x, 4y, 6z)=\lambda (2x,2y,2z)$. The conclusion I drew was that the only possible value was either $\lambda=0$, or y and z were both 0 but x can be anything. This seems incorrect to me, but I'm not sure how to proceed.","I defined $G(x)=x^2+y^2+z^2-1$, such that the gradient of G $\nabla G=(2x, 2y, 2z)$, so that $\nabla f=(2x, 4y, 6z)=\lambda (2x,2y,2z)$. The conclusion I drew was that the only possible value was either $\lambda=0$, or y and z were both 0 but x can be anything. This seems incorrect to me, but I'm not sure how to proceed.",,"['multivariable-calculus', 'optimization']"
80,"Function $f = (f_1,\dots,f_n)$ s.t. $[f_1(x),\dots,f_n(x)]$ goes from $[1,0,\dots,0]$ to $[0,\dots,0,1]$ in a smooth way",Function  s.t.  goes from  to  in a smooth way,"f = (f_1,\dots,f_n) [f_1(x),\dots,f_n(x)] [1,0,\dots,0] [0,\dots,0,1]","Question I would like to have a function: $$ f: \mathbb{R} \rightarrow \left\{x \in \mathbb{R}^n \mid \sum_{i=1}^n x_i = 1\right\}:x \mapsto (f_1(x),\dots,f_n(x)) $$ s.t. $f(0) = (1,0,\dots,0)$ and $f(1) = (0,\dots,0,1)$. This is of course very easy to find but I would like to have these such that the conversion happens in a smooth way. What I mean by that is that I would like to have the following extra properties: $f_1(1/2) = \dots = f_n(1/2) = 1/n$ $f_1(x)$ is descending $f_n(x)$ is ascending If we define the function $g_x(k) := f_k(x)$ and we draw the graphs of $g_x(k)$ for $x$ going from $0$ to $1$ this should look like a bit like a wave (idea further explained in next two points) On $]0,1/2[$ we have $f_1(x) > \dots > f_n(x)$ and on $]1/2,1[$ we have $f_1(x) < \dots < f_n(x)$ Example method For example for $n = 2$ we can define: $$ f(x) := (x, 1-x) $$ now for $n  = 3$ we can write: $$ f(x) := (x,f_2(x),1-x) $$ but then we have a problem as we need to have $f_2(x) = 0$, therefore we need to let $f_1(x)$ descend to $1/3$ on $[0,1/2]$ and $f_3(x)$ ascend to $1/3$ on $[0,1/2]$ we define: $$ f_1(x) :=\begin{cases}  (1 - \frac{4}{3} \cdot x) & \mbox{ if } x \in [0,1/2]\\ \frac{2}{3}(1 -  x) & \mbox{ if } x \in [1/2,1] \end{cases} $$ and $$ f_3(x) :=\begin{cases}  \frac{2}{3} \cdot x & \mbox{ if } x \in [0,1/2]\\ \frac{4}{3} x - \frac{1}{3} & \mbox{ if } x \in [1/2,1] \end{cases} $$ then we automatically find for $f_2(x)$: $$ f_2(x) = \begin{cases} \frac{2}{3} x & \mbox{ if } x \in [0,1/2]\\ \frac{2}{3}(1 -  x) & \mbox{ if } x \in [1/2,1] \end{cases} $$ Now we can continue for $n = 4$, the logical way to continue would be to define $f_1(x)$ to be $(1 - \frac{3}{2} x)$ on $[0,1/2]$ and $f_4(x) := \frac{1}{2} x$ on $[0,\frac{1}{2}]$. We could then use $f_2 = f_3$ and solve the equation $f_1 + 2 f_2 + f_4 = 1$ on $[0,\frac{1}{2}]$. But this does not satisfy the last property and doesn't look enough like a wave to me. Reason For My Question The Reason I ask this is because I would like to have a sequence of Markov Chains which goes from perfect correlation (i.e. transition matrix the unity matrix) to some other Markov Chain which is strongly negatively correlated.","Question I would like to have a function: $$ f: \mathbb{R} \rightarrow \left\{x \in \mathbb{R}^n \mid \sum_{i=1}^n x_i = 1\right\}:x \mapsto (f_1(x),\dots,f_n(x)) $$ s.t. $f(0) = (1,0,\dots,0)$ and $f(1) = (0,\dots,0,1)$. This is of course very easy to find but I would like to have these such that the conversion happens in a smooth way. What I mean by that is that I would like to have the following extra properties: $f_1(1/2) = \dots = f_n(1/2) = 1/n$ $f_1(x)$ is descending $f_n(x)$ is ascending If we define the function $g_x(k) := f_k(x)$ and we draw the graphs of $g_x(k)$ for $x$ going from $0$ to $1$ this should look like a bit like a wave (idea further explained in next two points) On $]0,1/2[$ we have $f_1(x) > \dots > f_n(x)$ and on $]1/2,1[$ we have $f_1(x) < \dots < f_n(x)$ Example method For example for $n = 2$ we can define: $$ f(x) := (x, 1-x) $$ now for $n  = 3$ we can write: $$ f(x) := (x,f_2(x),1-x) $$ but then we have a problem as we need to have $f_2(x) = 0$, therefore we need to let $f_1(x)$ descend to $1/3$ on $[0,1/2]$ and $f_3(x)$ ascend to $1/3$ on $[0,1/2]$ we define: $$ f_1(x) :=\begin{cases}  (1 - \frac{4}{3} \cdot x) & \mbox{ if } x \in [0,1/2]\\ \frac{2}{3}(1 -  x) & \mbox{ if } x \in [1/2,1] \end{cases} $$ and $$ f_3(x) :=\begin{cases}  \frac{2}{3} \cdot x & \mbox{ if } x \in [0,1/2]\\ \frac{4}{3} x - \frac{1}{3} & \mbox{ if } x \in [1/2,1] \end{cases} $$ then we automatically find for $f_2(x)$: $$ f_2(x) = \begin{cases} \frac{2}{3} x & \mbox{ if } x \in [0,1/2]\\ \frac{2}{3}(1 -  x) & \mbox{ if } x \in [1/2,1] \end{cases} $$ Now we can continue for $n = 4$, the logical way to continue would be to define $f_1(x)$ to be $(1 - \frac{3}{2} x)$ on $[0,1/2]$ and $f_4(x) := \frac{1}{2} x$ on $[0,\frac{1}{2}]$. We could then use $f_2 = f_3$ and solve the equation $f_1 + 2 f_2 + f_4 = 1$ on $[0,\frac{1}{2}]$. But this does not satisfy the last property and doesn't look enough like a wave to me. Reason For My Question The Reason I ask this is because I would like to have a sequence of Markov Chains which goes from perfect correlation (i.e. transition matrix the unity matrix) to some other Markov Chain which is strongly negatively correlated.",,"['linear-algebra', 'functions', 'multivariable-calculus', 'markov-chains']"
81,Classifying extrema via Taylor's Theorem,Classifying extrema via Taylor's Theorem,,"Let $f(x,y) = 4x^3y - 4xy^3$. This has first derivatives $$f_x = 12x^2y - 4y^3$$ and $$f_y = 4x^3 - 12xy^2$$ from which we can conclude that there is an extremum at $(0,0)$. Upon attempting to classify this extremum we obtain a discriminant of $D = 0$ and have to proceed by using Taylor's Theorem. My problem is that even to the fourth order and above I keep getting zeroes everywhere and am not learning anything about the function. Is there a point at which I should just stop and conclude that I can't say anything about the extreme point or am I doing something wrong? I'm expanding $$f(\varepsilon \cos\theta, \varepsilon\sin\theta) = \sum_{k=0}^{\infty}\frac{1}{k!}\left( \varepsilon\cos\theta\frac{\partial}{\partial x} + \varepsilon\sin\theta\frac{\partial}{\partial y}\right)^kf\bigg\vert_{(0,0)}$$ and getting zeroes at all orders.","Let $f(x,y) = 4x^3y - 4xy^3$. This has first derivatives $$f_x = 12x^2y - 4y^3$$ and $$f_y = 4x^3 - 12xy^2$$ from which we can conclude that there is an extremum at $(0,0)$. Upon attempting to classify this extremum we obtain a discriminant of $D = 0$ and have to proceed by using Taylor's Theorem. My problem is that even to the fourth order and above I keep getting zeroes everywhere and am not learning anything about the function. Is there a point at which I should just stop and conclude that I can't say anything about the extreme point or am I doing something wrong? I'm expanding $$f(\varepsilon \cos\theta, \varepsilon\sin\theta) = \sum_{k=0}^{\infty}\frac{1}{k!}\left( \varepsilon\cos\theta\frac{\partial}{\partial x} + \varepsilon\sin\theta\frac{\partial}{\partial y}\right)^kf\bigg\vert_{(0,0)}$$ and getting zeroes at all orders.",,"['multivariable-calculus', 'derivatives', 'taylor-expansion', 'maxima-minima']"
82,Prove that the norm of an integrable vector function is integrable,Prove that the norm of an integrable vector function is integrable,,"This result seems basic but I couldn't find a proof anywhere. Suppose $\mathbf{f}:[a,b]\to\mathbb R^n$ is Riemann integrable. Let $\|\cdot\|$ be a norm on $\mathbb R^n$. I want to show that $\|\mathbf f(x)\|$ is Riemann integrable as a function $[a,b]\to\mathbb R$. Is this result true? How can I prove it? Thanks in advance! I know how to prove this result for specific norms like $\|\cdot\|_1$ and $\|\cdot\|_2$, but I don't know how to prove it for a general norm. btw I'm using the defintion that $\mathbf{f}:[a,b]\to\mathbb R^n$ is Riemann integrable if each of its components $f_i$ are Riemann integrable. Then the integral would be $\int\mathbf{f}(x)dx=(\int f_1(x)dx,\cdots,\int f_n(x)dx)$.","This result seems basic but I couldn't find a proof anywhere. Suppose $\mathbf{f}:[a,b]\to\mathbb R^n$ is Riemann integrable. Let $\|\cdot\|$ be a norm on $\mathbb R^n$. I want to show that $\|\mathbf f(x)\|$ is Riemann integrable as a function $[a,b]\to\mathbb R$. Is this result true? How can I prove it? Thanks in advance! I know how to prove this result for specific norms like $\|\cdot\|_1$ and $\|\cdot\|_2$, but I don't know how to prove it for a general norm. btw I'm using the defintion that $\mathbf{f}:[a,b]\to\mathbb R^n$ is Riemann integrable if each of its components $f_i$ are Riemann integrable. Then the integral would be $\int\mathbf{f}(x)dx=(\int f_1(x)dx,\cdots,\int f_n(x)dx)$.",,"['integration', 'analysis', 'multivariable-calculus', 'normed-spaces']"
83,Does partial derivative and partial evaluation commute?,Does partial derivative and partial evaluation commute?,,"If $f$ is a function from $\mathbb{R}^2$ to $\mathbb{R}$, not necessarily well-behaved. Then does the equation $$\frac{\partial f(t,u)}{\partial t}\Big|_{u=0}=\frac{\partial f(t,0)}{\partial t}$$ hold? Which means that the l.h.s. is defined if and only if the r.h.s is defined, and their values are equal in that case.","If $f$ is a function from $\mathbb{R}^2$ to $\mathbb{R}$, not necessarily well-behaved. Then does the equation $$\frac{\partial f(t,u)}{\partial t}\Big|_{u=0}=\frac{\partial f(t,0)}{\partial t}$$ hold? Which means that the l.h.s. is defined if and only if the r.h.s is defined, and their values are equal in that case.",,"['calculus', 'multivariable-calculus']"
84,Compact set with measure zero has volume zero,Compact set with measure zero has volume zero,,"First , the definitions I work with - A set $A$ has measure zero if for any $ϵ>0$ there are open sets ${S_i},i∈\mathbb{N}$ such that  $A\subset \bigcup ^∞ _{i=0}S_i  $  and $∑^∞_{i=0}vol(S_i)<ϵ$. A set $A$ has a volume zero if the there is such a finite cover. -- I need to prove that compact set $A \subset \mathbb{R}^n$ has measure zero iff it has volume zero. My attempt- If $A$ has measure zero, then there exsits such countable cover.Also, $A$ is compact,so there is a finite cover for the mentioned above cover,which is the required cover for volume zero. If $A$ has a volume zero, then there is a finite cover, which is countable, and we are done. Is it correct???","First , the definitions I work with - A set $A$ has measure zero if for any $ϵ>0$ there are open sets ${S_i},i∈\mathbb{N}$ such that  $A\subset \bigcup ^∞ _{i=0}S_i  $  and $∑^∞_{i=0}vol(S_i)<ϵ$. A set $A$ has a volume zero if the there is such a finite cover. -- I need to prove that compact set $A \subset \mathbb{R}^n$ has measure zero iff it has volume zero. My attempt- If $A$ has measure zero, then there exsits such countable cover.Also, $A$ is compact,so there is a finite cover for the mentioned above cover,which is the required cover for volume zero. If $A$ has a volume zero, then there is a finite cover, which is countable, and we are done. Is it correct???",,"['calculus', 'integration', 'measure-theory', 'multivariable-calculus', 'lebesgue-measure']"
85,If $F$ is conservative.... does $\mathrm{curl}(F)$ really equal zero?,If  is conservative.... does  really equal zero?,F \mathrm{curl}(F),"My notes say that if $F$ is conservative (i.e. $F=\nabla f$) then $\text{curl }(F)=0$.  But I feel this is not quite right. There is a theorem that says that if $f(x,y,z)$ has continuous second order partial derivatives then $\text{curl }(\nabla f)=0$. (I'm fine with this theorem, it makes sense.) Obviously the 'proof' for my question is  $$\text{curl }(F)=\text{curl }(\nabla f)=0$$ But I don't see why it is necessarily true that $f$ has continuous second order partial derivatives.  Is there something I am missing?","My notes say that if $F$ is conservative (i.e. $F=\nabla f$) then $\text{curl }(F)=0$.  But I feel this is not quite right. There is a theorem that says that if $f(x,y,z)$ has continuous second order partial derivatives then $\text{curl }(\nabla f)=0$. (I'm fine with this theorem, it makes sense.) Obviously the 'proof' for my question is  $$\text{curl }(F)=\text{curl }(\nabla f)=0$$ But I don't see why it is necessarily true that $f$ has continuous second order partial derivatives.  Is there something I am missing?",,"['multivariable-calculus', 'proof-explanation']"
86,Solving a system of linear differential equations with repeated eigen values,Solving a system of linear differential equations with repeated eigen values,,"I have this problem where to solve the system, $$x'=4x+y-z$$ $$y'=2x+5y-2z$$ $$z'=x+y+2z$$ using a linear algebraic solution. I have found the eigen values of the  $$\begin{bmatrix}         4 & 1 & -1 \\         2 & 5 & -2 \\         1 & 1 & 2 \\         \end{bmatrix}$$ as 3,3 and 5. When evaluating the corresponding eigen vectors for 3, the following occurs. $$(A-3I)x=0$$ $$\begin{bmatrix}         1 & 1 & -1 \\         2 & 2 & -2 \\         1 & 1 & -1 \\         \end{bmatrix} \begin{bmatrix} x_1\\ x_2\\ x_3 \end{bmatrix} = 0$$ We can say ok and $x_3=x_1+x_2$ and then a set of eigen vectors which are not multiples of each other are formed. As the next step, I have to find $\rho$ such that $(A-3I)\rho = \eta_{\lambda =3}$. There I'm getting nowhere because of the ambiguity of $\eta_{\lambda=3}$. I am new to this eigen things. Am I doing something terribly wrong or what?","I have this problem where to solve the system, $$x'=4x+y-z$$ $$y'=2x+5y-2z$$ $$z'=x+y+2z$$ using a linear algebraic solution. I have found the eigen values of the  $$\begin{bmatrix}         4 & 1 & -1 \\         2 & 5 & -2 \\         1 & 1 & 2 \\         \end{bmatrix}$$ as 3,3 and 5. When evaluating the corresponding eigen vectors for 3, the following occurs. $$(A-3I)x=0$$ $$\begin{bmatrix}         1 & 1 & -1 \\         2 & 2 & -2 \\         1 & 1 & -1 \\         \end{bmatrix} \begin{bmatrix} x_1\\ x_2\\ x_3 \end{bmatrix} = 0$$ We can say ok and $x_3=x_1+x_2$ and then a set of eigen vectors which are not multiples of each other are formed. As the next step, I have to find $\rho$ such that $(A-3I)\rho = \eta_{\lambda =3}$. There I'm getting nowhere because of the ambiguity of $\eta_{\lambda=3}$. I am new to this eigen things. Am I doing something terribly wrong or what?",,"['linear-algebra', 'ordinary-differential-equations', 'multivariable-calculus', 'eigenvalues-eigenvectors']"
87,Problem on differentiation of multivariable function,Problem on differentiation of multivariable function,,"The question is to find the value of $f$ differentiated partially with respect to $x$ (say $f_x$) at $(0,0)$ And the find the value of $f$ differentiated partially with respect to $y$ (say $f_y$) at $(0,0$) I find $f_x(0,0) =1$  But I am unable to find $f_y(0,0)$ Please help NOTE: Consider $(x,y)\in \mathbb R^2$ in the image.","The question is to find the value of $f$ differentiated partially with respect to $x$ (say $f_x$) at $(0,0)$ And the find the value of $f$ differentiated partially with respect to $y$ (say $f_y$) at $(0,0$) I find $f_x(0,0) =1$  But I am unable to find $f_y(0,0)$ Please help NOTE: Consider $(x,y)\in \mathbb R^2$ in the image.",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
88,Understanding when a vector is parallel vs. perpendicular?,Understanding when a vector is parallel vs. perpendicular?,,"I am having some difficulty with this concept. I am studying a problem: Find an equation for the plane that passes through the point $P = (1, 2, 3)$   and contains the line $L$ given by the parametric equation   $x(t) = 1 − 3t,\, y(t) = 3$, and $z(t) = 6 + 2t$. The solution states that $v = \left< 3, 0, −2 \right>$ is parallel to $L$ and hence the plane. $Q = (1, 3, 6)$ is in the plane and hence $PQ = \left< 0, 1, 3 \right>$ is parallel to the plane. $n = v \times PQ = \left< −2, 9, −3 \right>$ is perpendicular to the plane. Therefore, the plane is $−2x + 9y − 3z = −2 \cdot 1 + 9 \cdot 2 − 3 \cdot 3 = 7$. Could someone help me with this reasoning? I understand $v$ is parallel to $L$ because it is a scaled version of the normal to $L$, where the normal is $\left< -3, 0, 2 \right>$. I understand why $Q$ is in the plane. Why is $PQ$ parallel to the plane? Why do we know that $v \times PQ$ will be perpendicular to the plane? Is this because the cross product of two vectors parallel to a plane will be perpendicular to the plane? Thank you for any clarification or help in advance! Just trying to get a better hold on this concept.","I am having some difficulty with this concept. I am studying a problem: Find an equation for the plane that passes through the point $P = (1, 2, 3)$   and contains the line $L$ given by the parametric equation   $x(t) = 1 − 3t,\, y(t) = 3$, and $z(t) = 6 + 2t$. The solution states that $v = \left< 3, 0, −2 \right>$ is parallel to $L$ and hence the plane. $Q = (1, 3, 6)$ is in the plane and hence $PQ = \left< 0, 1, 3 \right>$ is parallel to the plane. $n = v \times PQ = \left< −2, 9, −3 \right>$ is perpendicular to the plane. Therefore, the plane is $−2x + 9y − 3z = −2 \cdot 1 + 9 \cdot 2 − 3 \cdot 3 = 7$. Could someone help me with this reasoning? I understand $v$ is parallel to $L$ because it is a scaled version of the normal to $L$, where the normal is $\left< -3, 0, 2 \right>$. I understand why $Q$ is in the plane. Why is $PQ$ parallel to the plane? Why do we know that $v \times PQ$ will be perpendicular to the plane? Is this because the cross product of two vectors parallel to a plane will be perpendicular to the plane? Thank you for any clarification or help in advance! Just trying to get a better hold on this concept.",,"['multivariable-calculus', 'vectors', 'vector-analysis', 'plane-geometry']"
89,"Choosing $a$ such that equation $f(x, a)=g(x)$ has only one solution with respect to $x$",Choosing  such that equation  has only one solution with respect to,"a f(x, a)=g(x) x","Let $f(x, a)$ and $g(x)$ are given functions. We want to find all values of $a$ such that equation $f(x, a)-g(x)=0$ has only one solution with respect to $x$. For example: $a \cdot \log_{a}x=x$ or $ax^4 - e^x=0$","Let $f(x, a)$ and $g(x)$ are given functions. We want to find all values of $a$ such that equation $f(x, a)-g(x)=0$ has only one solution with respect to $x$. For example: $a \cdot \log_{a}x=x$ or $ax^4 - e^x=0$",,"['calculus', 'multivariable-calculus']"
90,computing flux integral,computing flux integral,,"just given this question. compute the flux out of the unit circle, C. $$F(x,y)=\langle x+2y,3x+4y\rangle $$ i am not sure on how to solve this. Usually the flux would include Z function. please help!","just given this question. compute the flux out of the unit circle, C. $$F(x,y)=\langle x+2y,3x+4y\rangle $$ i am not sure on how to solve this. Usually the flux would include Z function. please help!",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
91,Is non-differentiable having the derivative a contradiction?,Is non-differentiable having the derivative a contradiction?,,"I'm reading this book and on page 104 they define what means a function being differentiable: Afterwards they give the following example: Is it not a contradiction? Following the definition $f$ is differentiable at $a$ if there is $D_f(a)$. In the example, the derivative $D_f(0,0)$ does exist at $(0,0)$. What am I missing?","I'm reading this book and on page 104 they define what means a function being differentiable: Afterwards they give the following example: Is it not a contradiction? Following the definition $f$ is differentiable at $a$ if there is $D_f(a)$. In the example, the derivative $D_f(0,0)$ does exist at $(0,0)$. What am I missing?",,"['analysis', 'multivariable-calculus', 'definition']"
92,"Prove or disprove that $\lim_{(x,y)\to(-2,1)} \frac{(1-\cos{((y-1)^2x)})(x+2)}{(x+2)^4+(y-1)^4}=0$",Prove or disprove that,"\lim_{(x,y)\to(-2,1)} \frac{(1-\cos{((y-1)^2x)})(x+2)}{(x+2)^4+(y-1)^4}=0","I have to prove or disprove that this limit is $0$: $$\lim_{(x,y)\to(-2,1)} \frac{(1-\cos{((y-1)^2x)})(x+2)}{(x+2)^4+(y-1)^4}$$ (Wolfram Alpha says it doesn't exist) As a ""clue"" it is suggested that I find $$\lim_{t\to0}\frac{1-\cos{t}}{t^2}$$ ,which is $\frac{1}{2}$, but I really don't know how this might help me. I tried approching $(-2,1)$ along $x=-2$ to transform it into the second limit, but that $x$ inside the $\cos$ is very annoying. I also tried using the $\epsilon$ - $\delta$ definition and find an $\epsilon$, such that no $\delta$ exists, but again I've never used this before, so I get nowhere.","I have to prove or disprove that this limit is $0$: $$\lim_{(x,y)\to(-2,1)} \frac{(1-\cos{((y-1)^2x)})(x+2)}{(x+2)^4+(y-1)^4}$$ (Wolfram Alpha says it doesn't exist) As a ""clue"" it is suggested that I find $$\lim_{t\to0}\frac{1-\cos{t}}{t^2}$$ ,which is $\frac{1}{2}$, but I really don't know how this might help me. I tried approching $(-2,1)$ along $x=-2$ to transform it into the second limit, but that $x$ inside the $\cos$ is very annoying. I also tried using the $\epsilon$ - $\delta$ definition and find an $\epsilon$, such that no $\delta$ exists, but again I've never used this before, so I get nowhere.",,"['limits', 'multivariable-calculus']"
93,Representing a differential equation as a gradient of a function,Representing a differential equation as a gradient of a function,,"Suppose I have a linear dynamical system that is described by the following differential equation $$\dot{\bf{x}} = \bf{A}\bf{x}$$ I wish to represent the right-hand side of this system as a gradient of a function $U({\bf x})$ . Now, if $\bf{A}$ is symmetric, I can construct a quadratic form of $\bf{A}$ , so that: $$ U({\bf x}) = \frac12 \bf{x}^\top \bf{A} \bf{x} $$ and $$ \dot{{\bf x}} = {\bf I} \dfrac{dU}{d{\bf x}} $$ where $\bf I$ is identity matrix of a proper order. However, how to construct $U({\bf x})$ when $\bf{A}$ is not symmetric? Note that for the problem I am solving I can't have $\bf I$ something else rather than identity. One idea I was trying to explore was decomposing $\bf{A}$ as a sum of symmetric and skew-symmetric matrices, but it led me nowhere.","Suppose I have a linear dynamical system that is described by the following differential equation I wish to represent the right-hand side of this system as a gradient of a function . Now, if is symmetric, I can construct a quadratic form of , so that: and where is identity matrix of a proper order. However, how to construct when is not symmetric? Note that for the problem I am solving I can't have something else rather than identity. One idea I was trying to explore was decomposing as a sum of symmetric and skew-symmetric matrices, but it led me nowhere.",\dot{\bf{x}} = \bf{A}\bf{x} U({\bf x}) \bf{A} \bf{A}  U({\bf x}) = \frac12 \bf{x}^\top \bf{A} \bf{x}   \dot{{\bf x}} = {\bf I} \dfrac{dU}{d{\bf x}}  \bf I U({\bf x}) \bf{A} \bf I \bf{A},"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'multivariable-calculus', 'gradient-flows']"
94,Intersection points of two polar curves,Intersection points of two polar curves,,"I was asked to find the area inside $r=3cos\theta$ and outside $r=1+cos\theta$ (see figure) My question is, how do i find the intersection points, I was taught to make $1+cos\theta = 3cos\theta$ and solving it we get $\theta=\pi/3$ and $\theta =5\pi/3$, but as you can see the curves meet at the pole as well, how do I find this point? (I do not need this for the área but I'm just curious)","I was asked to find the area inside $r=3cos\theta$ and outside $r=1+cos\theta$ (see figure) My question is, how do i find the intersection points, I was taught to make $1+cos\theta = 3cos\theta$ and solving it we get $\theta=\pi/3$ and $\theta =5\pi/3$, but as you can see the curves meet at the pole as well, how do I find this point? (I do not need this for the área but I'm just curious)",,"['multivariable-calculus', 'polar-coordinates']"
95,Compute a triple Integral,Compute a triple Integral,,"Compute the triple integral $\int\int\int_{D}fdV$ , where $f(\vec{r})= xy$ and $D$ is the tetrahedron: $D=(\vec{r}\in\mathbb{R^3}, 0 < y < x, |z| < 1 − x)$ This question has puzzled me for a number of hours now so if anyone could help me out that would be appreciated.","Compute the triple integral $\int\int\int_{D}fdV$ , where $f(\vec{r})= xy$ and $D$ is the tetrahedron: $D=(\vec{r}\in\mathbb{R^3}, 0 < y < x, |z| < 1 − x)$ This question has puzzled me for a number of hours now so if anyone could help me out that would be appreciated.",,['integration']
96,"If $\int_{R^2} f(x,y)dxdy$ exists, must $\int_R f(a,y)dy$ exist?","If  exists, must  exist?","\int_{R^2} f(x,y)dxdy \int_R f(a,y)dy","Let $f$ be a smooth real function on $R^2$ such that    $$\int_{R^2} f dxdy$$ exists. Let $a\in R$. Must $\int_R f(a,y) dy$ exist? I believe this is true, but I don't know how to prove it.","Let $f$ be a smooth real function on $R^2$ such that    $$\int_{R^2} f dxdy$$ exists. Let $a\in R$. Must $\int_R f(a,y) dy$ exist? I believe this is true, but I don't know how to prove it.",,['multivariable-calculus']
97,"Conversion from Cartesian to spherical coordinates, calculation of volume by triple integration","Conversion from Cartesian to spherical coordinates, calculation of volume by triple integration",,"Compute volume of the function $(x^{2}+y^{2})^{2}+z^{4}=y$ Attempted solution: $y$ is the sum of a square and a fourth power, clearly $y$ is positive. since the object consists of positive y, the object will occupy 2 quadrants in its proj. on xy plane and it will occupy 2 quadrants on zy plane. the object will have azimuth and co-latitude and distance: $0\le\phi\le\pi,\\ 0\le \theta\le\pi\\ \rho\ge0 $ Now to convert directly to spherical coordinates is my problem, as I understand it, it is easier to convert from Cartesian to cylindrical, and then from cylindrical to spherical. the conversion from Cartesian to cylindrical is as follows: $$x=rcos\theta,\\y=rsin\theta,\\z=z$$ Thus the function $(x^2+y^2)^2+z^4=y$ becomes $$(r^2cos^2\theta+r^2sin^2\theta)^2+z^4=rsin\theta$$ $$(r^2)^2*(1)+z^4=rsin\theta\\r^4+z^4=rsin\theta$$ Now To convert from cylindrical to spherical we may use: $$r=\rho sin\phi,\\\theta=\theta,\\z=\rho cos\phi$$ We get  $$\rho^4 sin^4\phi+\rho^4cos^4\phi=\rho sin\theta sin\theta\\\rho^3(sin^4\phi+cos^4\phi)=sin^2\theta\\\rho=\sqrt[3]{\frac{sin^2\theta}{sin^4\phi+cos^4\phi}}$$ the largest value of $sin^nu$ and $cos^nu$ is 1, thus the largest value of $\rho$ is $\sqrt[3]{\frac{1}{2}}$ Thus the limits of integration are $$0\le \phi \le\pi,\\0\le\theta\le\pi.\\0\le\rho\le\sqrt[3]{\frac{1}{2}}$$ What knowledge I would like mathstack's to share; Is this an okay method to convert to spherical coordinates? Am I missing an easier way to convert directly from Cartesian to spherical coordinates? How do I set up the integral, since I want to integrate with respect to Rho, Theta and Phi? please DO NOT solve the triple integral, that would be missing the point. Thanks! refer to this plot: My attempt at integrating: volume of the blob is: $$\int\int\int_A\:dV\\=\int_0^{\sqrt[3]{1/2}}\int_0^\pi\int_0^\pi\:\rho^2 \:sin\phi\: d\theta\: d\phi\: d\rho$$ Check back I will attempt to evaluate $$=\int_0^{\sqrt[3]{1/2}}\int_0^\pi\:\rho^2 \:[-cos\phi]^{\pi}_{0}\: d\phi\: d\rho$$ $$=\int_0^{\sqrt[3]{1/2}}\int_0^\pi\:\rho^2 \:[-cos\pi+cos(0)]\: d\phi\: d\rho$$ $$=\int_0^{\sqrt[3]{1/2}}\int_0^\pi\:2*\rho^2\: d\phi\: d\rho$$ $$=2*\int_0^{\sqrt[3]{1/2}}\int_0^\pi\:\rho^2\: d\phi\: d\rho=2*\int_0^{\sqrt[3]{1/2}}\:[\phi*\rho^2]^\pi_0\: d\rho=2\pi*\int_0^{\sqrt[3]{1/2}}\:\rho^2\:d\rho$$ $$=2\pi*[\phi^3/3]^{\sqrt[3]{1/2}}_0\\=2\pi*\frac{1/2}{3}=\pi/3$$ Thus the volume bound by the surface is $\pi/3$ If anyone would care to check my evaluation that would be greatly appreciated!","Compute volume of the function $(x^{2}+y^{2})^{2}+z^{4}=y$ Attempted solution: $y$ is the sum of a square and a fourth power, clearly $y$ is positive. since the object consists of positive y, the object will occupy 2 quadrants in its proj. on xy plane and it will occupy 2 quadrants on zy plane. the object will have azimuth and co-latitude and distance: $0\le\phi\le\pi,\\ 0\le \theta\le\pi\\ \rho\ge0 $ Now to convert directly to spherical coordinates is my problem, as I understand it, it is easier to convert from Cartesian to cylindrical, and then from cylindrical to spherical. the conversion from Cartesian to cylindrical is as follows: $$x=rcos\theta,\\y=rsin\theta,\\z=z$$ Thus the function $(x^2+y^2)^2+z^4=y$ becomes $$(r^2cos^2\theta+r^2sin^2\theta)^2+z^4=rsin\theta$$ $$(r^2)^2*(1)+z^4=rsin\theta\\r^4+z^4=rsin\theta$$ Now To convert from cylindrical to spherical we may use: $$r=\rho sin\phi,\\\theta=\theta,\\z=\rho cos\phi$$ We get  $$\rho^4 sin^4\phi+\rho^4cos^4\phi=\rho sin\theta sin\theta\\\rho^3(sin^4\phi+cos^4\phi)=sin^2\theta\\\rho=\sqrt[3]{\frac{sin^2\theta}{sin^4\phi+cos^4\phi}}$$ the largest value of $sin^nu$ and $cos^nu$ is 1, thus the largest value of $\rho$ is $\sqrt[3]{\frac{1}{2}}$ Thus the limits of integration are $$0\le \phi \le\pi,\\0\le\theta\le\pi.\\0\le\rho\le\sqrt[3]{\frac{1}{2}}$$ What knowledge I would like mathstack's to share; Is this an okay method to convert to spherical coordinates? Am I missing an easier way to convert directly from Cartesian to spherical coordinates? How do I set up the integral, since I want to integrate with respect to Rho, Theta and Phi? please DO NOT solve the triple integral, that would be missing the point. Thanks! refer to this plot: My attempt at integrating: volume of the blob is: $$\int\int\int_A\:dV\\=\int_0^{\sqrt[3]{1/2}}\int_0^\pi\int_0^\pi\:\rho^2 \:sin\phi\: d\theta\: d\phi\: d\rho$$ Check back I will attempt to evaluate $$=\int_0^{\sqrt[3]{1/2}}\int_0^\pi\:\rho^2 \:[-cos\phi]^{\pi}_{0}\: d\phi\: d\rho$$ $$=\int_0^{\sqrt[3]{1/2}}\int_0^\pi\:\rho^2 \:[-cos\pi+cos(0)]\: d\phi\: d\rho$$ $$=\int_0^{\sqrt[3]{1/2}}\int_0^\pi\:2*\rho^2\: d\phi\: d\rho$$ $$=2*\int_0^{\sqrt[3]{1/2}}\int_0^\pi\:\rho^2\: d\phi\: d\rho=2*\int_0^{\sqrt[3]{1/2}}\:[\phi*\rho^2]^\pi_0\: d\rho=2\pi*\int_0^{\sqrt[3]{1/2}}\:\rho^2\:d\rho$$ $$=2\pi*[\phi^3/3]^{\sqrt[3]{1/2}}_0\\=2\pi*\frac{1/2}{3}=\pi/3$$ Thus the volume bound by the surface is $\pi/3$ If anyone would care to check my evaluation that would be greatly appreciated!",,"['integration', 'multivariable-calculus', 'volume']"
98,Inconclusive second derivative test rigorous proof,Inconclusive second derivative test rigorous proof,,"I need to find and identify stationary points of the following function: $f(x,y) = x^4 + 2x^2y^2 - y^4 - 2x^2 + 3$ Second derivative test appears to be inconclusive (Hessian equal to zero) at the point $(0,0)$. By plotting this appears to be a maximum. How does one prove this rigorously?","I need to find and identify stationary points of the following function: $f(x,y) = x^4 + 2x^2y^2 - y^4 - 2x^2 + 3$ Second derivative test appears to be inconclusive (Hessian equal to zero) at the point $(0,0)$. By plotting this appears to be a maximum. How does one prove this rigorously?",,"['functions', 'multivariable-calculus']"
99,Determine if a set of equations has unique solution,Determine if a set of equations has unique solution,,"Determine if the following set of equations has unique solution of the   form $g(z)=(x,y)$ in the neighbourhood of the origin. $$\begin{cases}  xyz+\sin(xyz)=0 \\ x+y+z=0  \end{cases}$$ The answer is negative. In fact, there exists infinitely many solutions in the neighbourhood of the origin. Let $F:ℝ^3\rightarrow ℝ^2, F(x,y,z)=(xyz+\sin(xyz),x+y+z).$ If we consider the function $F$, we notice that $F(\epsilon,-\epsilon,0)=(0,0)$ $∀\epsilon>0$. Because we can choose $\epsilon$ to be arbitrarily small,  $\nexists{R>0}$ such that n-hood of the origin $B(0,R)$ would contain a unique solution $(x,y,z)=(g(z),z)$ such that $F(x,y,z)=0$. Thus the set of equations doesn't have a unique solution in any n-hood of the origin. Is this correct? Are there alternative methods?","Determine if the following set of equations has unique solution of the   form $g(z)=(x,y)$ in the neighbourhood of the origin. $$\begin{cases}  xyz+\sin(xyz)=0 \\ x+y+z=0  \end{cases}$$ The answer is negative. In fact, there exists infinitely many solutions in the neighbourhood of the origin. Let $F:ℝ^3\rightarrow ℝ^2, F(x,y,z)=(xyz+\sin(xyz),x+y+z).$ If we consider the function $F$, we notice that $F(\epsilon,-\epsilon,0)=(0,0)$ $∀\epsilon>0$. Because we can choose $\epsilon$ to be arbitrarily small,  $\nexists{R>0}$ such that n-hood of the origin $B(0,R)$ would contain a unique solution $(x,y,z)=(g(z),z)$ such that $F(x,y,z)=0$. Thus the set of equations doesn't have a unique solution in any n-hood of the origin. Is this correct? Are there alternative methods?",,"['linear-algebra', 'multivariable-calculus']"
