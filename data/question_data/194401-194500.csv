,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Derivative with respect to a matrix,Derivative with respect to a matrix,,How do we start with the matrix differentiation of this kind of equation? $$ V = \big[ y_t - Cx_t \big]^T R^{-1} \big[y_t - Cx_t \big] $$ here $x_t$ and $y_t$ are vectors and $C$ and $R$ are matrices. $R$ is a covariance matrix (symmetric). How do we find the derivative of $V$ with respect of matrix $C$?,How do we start with the matrix differentiation of this kind of equation? $$ V = \big[ y_t - Cx_t \big]^T R^{-1} \big[y_t - Cx_t \big] $$ here $x_t$ and $y_t$ are vectors and $C$ and $R$ are matrices. $R$ is a covariance matrix (symmetric). How do we find the derivative of $V$ with respect of matrix $C$?,,"['linear-algebra', 'matrices', 'derivatives']"
1,"If $f(x_0+x)=P(x)+O(x^n)$, is $f$ $m<n$ times differentiable at $x_0$?","If , is   times differentiable at ?",f(x_0+x)=P(x)+O(x^n) f m<n x_0,"Let $f : \mathbb{R} \to \mathbb{R}$ be a real function and $x_0 \in \mathbb{R}$ be a real number. Suppose that there exists a polynomial $P \in \mathbb{R}[X]$ such that $f(x_0+x)=P(x)+ \underset{x \to 0}{O} (x^n)$ with $n> \text{deg}(P)$. Is it true that for $m<n$, $f^{(m)}(x_0)$ exists? (Of course, it is obvious for $m=1$.) If so, we can notice that $f^{(m)}(x_0)=P^{(m)}(x_0)$.","Let $f : \mathbb{R} \to \mathbb{R}$ be a real function and $x_0 \in \mathbb{R}$ be a real number. Suppose that there exists a polynomial $P \in \mathbb{R}[X]$ such that $f(x_0+x)=P(x)+ \underset{x \to 0}{O} (x^n)$ with $n> \text{deg}(P)$. Is it true that for $m<n$, $f^{(m)}(x_0)$ exists? (Of course, it is obvious for $m=1$.) If so, we can notice that $f^{(m)}(x_0)=P^{(m)}(x_0)$.",,"['real-analysis', 'derivatives', 'asymptotics']"
2,For what value is the local minimum the largest?,For what value is the local minimum the largest?,,"If  $f(x)=e^x-kx$ for $k>0$, find the values of $k$ for which the local minimum at $x=\ln(k)$ is the largest. I found the derivative, which is $e^{x} - k$, and when I set that to $0$ I got $-e^{x}=k$. I'm not really sure if this is useful information and if it is, I am not sure how to use it to answer the questions, so I would appreciate any tips on where to go next! Thanks!","If  $f(x)=e^x-kx$ for $k>0$, find the values of $k$ for which the local minimum at $x=\ln(k)$ is the largest. I found the derivative, which is $e^{x} - k$, and when I set that to $0$ I got $-e^{x}=k$. I'm not really sure if this is useful information and if it is, I am not sure how to use it to answer the questions, so I would appreciate any tips on where to go next! Thanks!",,"['calculus', 'derivatives']"
3,Calculus trig derivative?,Calculus trig derivative?,,How would I evaluate the following two derivatives. $g(t)=\sin^2(x)+\cos^2(t)$ For this derivative I know $\sin^2+\cos^2(t)=1$ so there derivative of $1$ is $0$ For my second question I have to find the second derivative of $h(t)=\sec(3t)$ at $t=\pi$ I found the first derivative as $3\sec(3t)\tan(3t)$ The second derivative I got $3[\sec(3t)(3\sec^2(3t))+\tan(3t)(3)\sec(3t)\tan(3t)]$ But how would I apply the $\pi$ to the $t$,How would I evaluate the following two derivatives. $g(t)=\sin^2(x)+\cos^2(t)$ For this derivative I know $\sin^2+\cos^2(t)=1$ so there derivative of $1$ is $0$ For my second question I have to find the second derivative of $h(t)=\sec(3t)$ at $t=\pi$ I found the first derivative as $3\sec(3t)\tan(3t)$ The second derivative I got $3[\sec(3t)(3\sec^2(3t))+\tan(3t)(3)\sec(3t)\tan(3t)]$ But how would I apply the $\pi$ to the $t$,,"['calculus', 'derivatives']"
4,Laplacian in $\mathbb{R}^n$ expressed with Christoffel symbols,Laplacian in  expressed with Christoffel symbols,\mathbb{R}^n,"I thought I'd try my luck with another question, so here it goes: I have to show that if $(y^1,...,y^n):\mathbb R^n \rightarrow \mathbb R^n$ is a diffeomorphism and $f\in C^\infty(\mathbb R^n)$. Then the Laplacian in $\mathbb R^n$ $\nabla_{\mathbb R^n} = \sum_{j=1}^n(\frac{\partial}{\partial x^j})^2$  on the composite function $f$ is $\nabla_{\mathbb R^n} f(y^1(x^1,...,x^n),...y^n(x^1,...x^n)) = (\widetilde{\nabla}f)(y^1(x^1,...,x^n),...,y^n(x^1,...,x^n))$ where $\widetilde{\nabla} = \sum_{i,j=1}^n g^{ij}((\frac{\partial}{\partial y^i} \frac{\partial}{\partial y^j}) - \sum_{k=1}^n \Gamma_{ij}^k \frac{\partial}{\partial y^k})$ and $g^{ij}=\sum_{l=1}^n \frac{\partial y^i}{\partial x^l}\frac{\partial y^j}{\partial x^l}$ and  $\Gamma_{ij}^k = \sum_{l=1}^n \frac{\partial y^k}{\partial x^l} \frac{\partial ^2 x^l}{\partial y^i \partial y^j}$ are the Christoffel symbols So far I've tried to just write out the Laplacian but I got stuck on the second derivative. My Calculations: $\nabla_{\mathbb R^n} f(y^1(x^1,...,x^n),...y^n(x^1,...x^n)) = \sum_{j=1}^n (\frac{\partial}{\partial x^j})^2(f) = \sum_{j=1}^n \frac{\partial}{\partial x^j}(\frac{\partial f}{\partial y^j}\frac{\partial y^j}{\partial x^j}) = \sum_{j=1}^n \frac{\partial ^2 f}{\partial (y^j)^2}(\frac{\partial y^j}{\partial x^j})^2 + \frac{\partial f}{\partial y^j}\frac{\partial ^x y^j}{\partial (x^j)^2}$ but then I'm not quite sure what I should do next. Can anyone help me with this? Thanks in advance!","I thought I'd try my luck with another question, so here it goes: I have to show that if $(y^1,...,y^n):\mathbb R^n \rightarrow \mathbb R^n$ is a diffeomorphism and $f\in C^\infty(\mathbb R^n)$. Then the Laplacian in $\mathbb R^n$ $\nabla_{\mathbb R^n} = \sum_{j=1}^n(\frac{\partial}{\partial x^j})^2$  on the composite function $f$ is $\nabla_{\mathbb R^n} f(y^1(x^1,...,x^n),...y^n(x^1,...x^n)) = (\widetilde{\nabla}f)(y^1(x^1,...,x^n),...,y^n(x^1,...,x^n))$ where $\widetilde{\nabla} = \sum_{i,j=1}^n g^{ij}((\frac{\partial}{\partial y^i} \frac{\partial}{\partial y^j}) - \sum_{k=1}^n \Gamma_{ij}^k \frac{\partial}{\partial y^k})$ and $g^{ij}=\sum_{l=1}^n \frac{\partial y^i}{\partial x^l}\frac{\partial y^j}{\partial x^l}$ and  $\Gamma_{ij}^k = \sum_{l=1}^n \frac{\partial y^k}{\partial x^l} \frac{\partial ^2 x^l}{\partial y^i \partial y^j}$ are the Christoffel symbols So far I've tried to just write out the Laplacian but I got stuck on the second derivative. My Calculations: $\nabla_{\mathbb R^n} f(y^1(x^1,...,x^n),...y^n(x^1,...x^n)) = \sum_{j=1}^n (\frac{\partial}{\partial x^j})^2(f) = \sum_{j=1}^n \frac{\partial}{\partial x^j}(\frac{\partial f}{\partial y^j}\frac{\partial y^j}{\partial x^j}) = \sum_{j=1}^n \frac{\partial ^2 f}{\partial (y^j)^2}(\frac{\partial y^j}{\partial x^j})^2 + \frac{\partial f}{\partial y^j}\frac{\partial ^x y^j}{\partial (x^j)^2}$ but then I'm not quite sure what I should do next. Can anyone help me with this? Thanks in advance!",,"['differential-geometry', 'derivatives']"
5,The derivative of characterestic polynomial?,The derivative of characterestic polynomial?,,Let $A\in M_{n}(R)$ and $f(x)$ be the characterestic polynomial of $A$. Is it true that $f'(x)=\sum_{i=1}^{^{n}}\sum_{j=1}^{n}\det(xI-A(i\mid j))$ which $A(i\mid j)$ is a submatrix of $A$ obtained by the cancelation the $i$th row and $j$th colomn?,Let $A\in M_{n}(R)$ and $f(x)$ be the characterestic polynomial of $A$. Is it true that $f'(x)=\sum_{i=1}^{^{n}}\sum_{j=1}^{n}\det(xI-A(i\mid j))$ which $A(i\mid j)$ is a submatrix of $A$ obtained by the cancelation the $i$th row and $j$th colomn?,,"['linear-algebra', 'matrices', 'polynomials', 'derivatives', 'determinant']"
6,$\delta_{ij}$ and $\delta_{ji}$: relation and meaning,and : relation and meaning,\delta_{ij} \delta_{ji},What's the relation between $\delta_{ij}$ and $\delta_{ji}$? What about their mathematical and physical meanings? Thank you!,What's the relation between $\delta_{ij}$ and $\delta_{ji}$? What about their mathematical and physical meanings? Thank you!,,"['analysis', 'derivatives']"
7,"If the mean value theorem always gives a $c \in (0,\infty) $ such that $f'(c) > 0$ is there an interval starting from $0$ such that $f' >0$",If the mean value theorem always gives a  such that  is there an interval starting from  such that,"c \in (0,\infty)  f'(c) > 0 0 f' >0","Assume that $f$ is a differentiable function on $[0,\infty]$ and $f(0) = 0$.  Furthermore, assume that  for all $x \in (0,\infty)$ there is a $c \in (0,x)$  such that $$ 0 < f'(c) = \frac{f(x) - f(0)}{x} = \frac{f(x)}{x} $$ Must there exist a point $p \in (0,\infty)$ such that for all $x \in (0,p)$ we have  $f'(x) > 0$? This is not a book or homework problem, this is a a question of my own.  So far I have tried a contradiction, assume that for all $p \in (0, \infty)$ there is at least one $x \in(0,p)$ such that $f'(x) \leq 0$. Consider such a point $x$, now, by Darboux's Theorem there should be a point $x_0 \in (0,x)$ such that $f'(x_0) = 0$. Let $p = x_0$, there must be another point $x$ such that $f'(x) \leq 0$ , and hence, another point $x_1$ such that $f'(x_1) = 0$ (it is of course trivial that $x_1 < x_0$). As this process keeps repeating, there should be a infinite sequence of points $x_0,x_1,x_2,...$ converging to $0$ (the sequence is bounded below by $0$ by definition so the Monotone Convergence Theorem should guarantee convergence to $0$ right?) such that $f'=0$ at any point in this sequence. And here I'm stuck. Any convergent sequence in $R$ is Cauchy could that help at all? Or is this the entirely wrong approach? This is my first semester of Advanced Calculus, so please try not to laugh too hard if this question is stupid. Thanks!","Assume that $f$ is a differentiable function on $[0,\infty]$ and $f(0) = 0$.  Furthermore, assume that  for all $x \in (0,\infty)$ there is a $c \in (0,x)$  such that $$ 0 < f'(c) = \frac{f(x) - f(0)}{x} = \frac{f(x)}{x} $$ Must there exist a point $p \in (0,\infty)$ such that for all $x \in (0,p)$ we have  $f'(x) > 0$? This is not a book or homework problem, this is a a question of my own.  So far I have tried a contradiction, assume that for all $p \in (0, \infty)$ there is at least one $x \in(0,p)$ such that $f'(x) \leq 0$. Consider such a point $x$, now, by Darboux's Theorem there should be a point $x_0 \in (0,x)$ such that $f'(x_0) = 0$. Let $p = x_0$, there must be another point $x$ such that $f'(x) \leq 0$ , and hence, another point $x_1$ such that $f'(x_1) = 0$ (it is of course trivial that $x_1 < x_0$). As this process keeps repeating, there should be a infinite sequence of points $x_0,x_1,x_2,...$ converging to $0$ (the sequence is bounded below by $0$ by definition so the Monotone Convergence Theorem should guarantee convergence to $0$ right?) such that $f'=0$ at any point in this sequence. And here I'm stuck. Any convergent sequence in $R$ is Cauchy could that help at all? Or is this the entirely wrong approach? This is my first semester of Advanced Calculus, so please try not to laugh too hard if this question is stupid. Thanks!",,"['calculus', 'real-analysis', 'derivatives']"
8,Proof of 2nd Derivative of a Sum of a Geometric Series,Proof of 2nd Derivative of a Sum of a Geometric Series,,"I am trying to prove how $$g''(r)=\sum\limits_{k=2}^\infty ak(k-1)r^{k-2}=0+0+2a+6ar+\cdots=\dfrac{2a}{(1-r)^3}=2a(1-r)^{-3}$$ or $\sum ak(k-1)r^(k-1) = 2a(1-r)^{-3}$. I don't know what I am doing wrong and am at my wits end. The attempt at a solution (The index of the summation is always $k=2$ to infinity) \begin{align*} \sum ak(k-1)r^{k-1} &=a \sum k(k-1)r^{k-2}\\ &=a \sum (r^k)''\\ &=a \left(\frac{r^2}{1-r}\right)'' \end{align*} From this point I get a mess, and the incorrect answer. The thing I have a problem is I think $\sum (r^k)''$ when $k$ is from $2$ to infinity is $r^2/(1-r)$, since the first term in this sequence is $r^2$. I don't think that is correct though.","I am trying to prove how $$g''(r)=\sum\limits_{k=2}^\infty ak(k-1)r^{k-2}=0+0+2a+6ar+\cdots=\dfrac{2a}{(1-r)^3}=2a(1-r)^{-3}$$ or $\sum ak(k-1)r^(k-1) = 2a(1-r)^{-3}$. I don't know what I am doing wrong and am at my wits end. The attempt at a solution (The index of the summation is always $k=2$ to infinity) \begin{align*} \sum ak(k-1)r^{k-1} &=a \sum k(k-1)r^{k-2}\\ &=a \sum (r^k)''\\ &=a \left(\frac{r^2}{1-r}\right)'' \end{align*} From this point I get a mess, and the incorrect answer. The thing I have a problem is I think $\sum (r^k)''$ when $k$ is from $2$ to infinity is $r^2/(1-r)$, since the first term in this sequence is $r^2$. I don't think that is correct though.",,"['sequences-and-series', 'derivatives']"
9,Rates of increase of the angle of inclination and of the distance from the object,Rates of increase of the angle of inclination and of the distance from the object,,"A hot air balloon leaves the ground at a point that is horizontal distance 190 metres from an observer and rises vertically upwards. The observer notes that the rate of increase of the angle of inclination between the horizontal and the vertical and the observer where line of sight (hyp) is a constant 0.045 radians per minute. What is the rate of increase in metres per minute of the distance between the balloon and the observer when the balloon is 180 metres above the ground? To solve it I visualised a right angled triangle with angle 0.045 radians, a base (adj) of 190 and opposite of 180m (position of balloon) so I'm guessing an element of trig is involved. I denoted the rate of increase in metres per minute of the distance ds/dt but I don't know if s is the hypotenuse and what to do to calculate ds/dt. Can someone please break it down for me? Edit i did but i hit a brick wall. If i call the side opposite to θ x then tanθ=x/170 and dx/dθ=170sec $^2$ θ (which can be expressed as cos)  but I don't know what to do next","A hot air balloon leaves the ground at a point that is horizontal distance 190 metres from an observer and rises vertically upwards. The observer notes that the rate of increase of the angle of inclination between the horizontal and the vertical and the observer where line of sight (hyp) is a constant 0.045 radians per minute. What is the rate of increase in metres per minute of the distance between the balloon and the observer when the balloon is 180 metres above the ground? To solve it I visualised a right angled triangle with angle 0.045 radians, a base (adj) of 190 and opposite of 180m (position of balloon) so I'm guessing an element of trig is involved. I denoted the rate of increase in metres per minute of the distance ds/dt but I don't know if s is the hypotenuse and what to do to calculate ds/dt. Can someone please break it down for me? Edit i did but i hit a brick wall. If i call the side opposite to θ x then tanθ=x/170 and dx/dθ=170sec θ (which can be expressed as cos)  but I don't know what to do next",^2,"['calculus', 'trigonometry', 'derivatives']"
10,Partial derivatives chain rule,Partial derivatives chain rule,,Let $ w=\frac{x^2}{y} $ with $ x=e^{-u^2}u $ and $ y=e^{-u^2}v $ $ \frac{\partial w}{\partial x} = (x^2)'\frac{1}{y} = \frac{2x}{y} $ $ \frac{\partial w}{\partial y} = x^2(\frac{1}{y})' = -\frac{x^2}{y^2} $ $ \frac{\partial y}{\partial u} = (e^{-u^2})'v = e^{-u^2}(-u^2)'v = -2e^{-u^2}uv $ $ \frac{\partial y}{\partial v} = e^{-u^2}(v)' = e^{-u^2} $ $ \frac{\partial x}{\partial u} = (e^{-u^2}u)' = (e^{-u^2})'u+u'e^{-u^2} = e^{-u^2}(-u^2)'u+e^{-u^2} = -2e^{-u^2}u^2 + e^{-u^2} = e^{-u^2}(1-2u^2) $ $ \frac{\partial x}{\partial v} = 0 $ $ \frac{\partial w}{\partial u} = \frac{\partial w}{\partial x}\frac{\partial x}{\partial u} + \frac{\partial w}{\partial y}\frac{\partial y}{\partial u} = \frac{2x}{y}e^{-u^2}(1-2u^2) +(-\frac{x^2}{y^2})(-2e^{-u^2}uv) = \frac{2e^{-u^2}u}{e^{-u^2}v}e^{-u^2}(1-2u^2) + 2e^{-u^2}uv\frac{(e^{-u^2}u)^2}{(e^{-u^2}v)^2} = \frac{2e^{-u^2}u}{v}-\frac{4e^{-u^2}u^3}{v} + \frac{2e^{-u^2}u^3}{v} = \frac{2e^{-u^2}u}{v} - \frac{2e^{-u^2}u^3}{v} = \frac{2e^{-u^2}u}{v}(1-u^2) $ $ \frac{\partial w}{\partial v} = \frac{\partial w}{\partial x}\frac{\partial x}{\partial v} + \frac{\partial w}{\partial y}\frac{\partial y}{\partial v} = - \frac{x^2}{y^2}e^{-u^2} = - \frac{(e^{-u^2}u)^2}{(e^{-u^2}v)^2} e^{-u^2} = - \frac{e^{-u^2}u^2}{v^2} $ Is my solution correct? I am not sure if I understood chain rule according to partial derivatives.,Let $ w=\frac{x^2}{y} $ with $ x=e^{-u^2}u $ and $ y=e^{-u^2}v $ $ \frac{\partial w}{\partial x} = (x^2)'\frac{1}{y} = \frac{2x}{y} $ $ \frac{\partial w}{\partial y} = x^2(\frac{1}{y})' = -\frac{x^2}{y^2} $ $ \frac{\partial y}{\partial u} = (e^{-u^2})'v = e^{-u^2}(-u^2)'v = -2e^{-u^2}uv $ $ \frac{\partial y}{\partial v} = e^{-u^2}(v)' = e^{-u^2} $ $ \frac{\partial x}{\partial u} = (e^{-u^2}u)' = (e^{-u^2})'u+u'e^{-u^2} = e^{-u^2}(-u^2)'u+e^{-u^2} = -2e^{-u^2}u^2 + e^{-u^2} = e^{-u^2}(1-2u^2) $ $ \frac{\partial x}{\partial v} = 0 $ $ \frac{\partial w}{\partial u} = \frac{\partial w}{\partial x}\frac{\partial x}{\partial u} + \frac{\partial w}{\partial y}\frac{\partial y}{\partial u} = \frac{2x}{y}e^{-u^2}(1-2u^2) +(-\frac{x^2}{y^2})(-2e^{-u^2}uv) = \frac{2e^{-u^2}u}{e^{-u^2}v}e^{-u^2}(1-2u^2) + 2e^{-u^2}uv\frac{(e^{-u^2}u)^2}{(e^{-u^2}v)^2} = \frac{2e^{-u^2}u}{v}-\frac{4e^{-u^2}u^3}{v} + \frac{2e^{-u^2}u^3}{v} = \frac{2e^{-u^2}u}{v} - \frac{2e^{-u^2}u^3}{v} = \frac{2e^{-u^2}u}{v}(1-u^2) $ $ \frac{\partial w}{\partial v} = \frac{\partial w}{\partial x}\frac{\partial x}{\partial v} + \frac{\partial w}{\partial y}\frac{\partial y}{\partial v} = - \frac{x^2}{y^2}e^{-u^2} = - \frac{(e^{-u^2}u)^2}{(e^{-u^2}v)^2} e^{-u^2} = - \frac{e^{-u^2}u^2}{v^2} $ Is my solution correct? I am not sure if I understood chain rule according to partial derivatives.,,['derivatives']
11,"Differentiability of $(x,y)\mapsto|x|\cdot y$",Differentiability of,"(x,y)\mapsto|x|\cdot y","Check the differentiability of the function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ of two variables given by the formula $$f(x)=|x_1|\cdot x_2$$ I still have problems with this. I started by trying to count partial derivatives: $\displaystyle\frac{\partial f}{\partial x_1}(x)=\lim_{h\to 0}\frac{f(x_1+h,x_2)-f(x_1,x_2)}{h}=\lim_{h\to 0}\frac{|x_1+h|x_2-|x_1|x_2}{h}$, so I think the problems can be at the points: $(0,x_2)$, where $x_2\neq 0$, because then we have that this limit is equal to $\displaystyle\lim_{h\to 0}\frac{|h|x_2}{h}$ which doesn't exist (left and right limits are not equal). $\displaystyle\frac{\partial f}{\partial x_2}(x)=\lim_{h\to 0}\frac{f(x_1,x_2+h)-f(x_1,x_2)}{h}=\lim_{h \to 0}\frac{|x_1|(x_2+h)-|x_1|x_2}{h}=|x_1|$, so I think we haven't any problems here, this partial derivative always exists. But what exactly can we deduce from these speculations about partial derivatives? I've also tried to proudly find the differential of this function. I was taught that the function is differentiable at the point $x$ iff there exists (if there exists, there is only one) a linear mapping $L$ such that  $(*)\displaystyle\lim_{h\to 0}\frac{f(x+h)-f(x)-L(h)}{\|h\|}=0$. Then we say that $Df(x)=L$ is a differential of function $f$ at the point $x$. For example consider function $g:\mathbb{R}^2\rightarrow \mathbb{R}, \ g(x)=x_1\cdot x_2$. We can find differential of this function by looking at the increment of this function: $g(x+h)-g(x)=(x_1+h_1)(x_2+h_2)-x_1x_2=x_1h_2+x_2h_1+h_1h_2$ . Then the candidate for $Df(x)$ is linear part of this increment: $L(h)=x_1h_2+x_2h_1$. When we check $(*)$ it appears that indeed it is a desired differential. But in my example: $f(x+h)-f(x)=|x_1+h_1|(x_2+h_2)-|x_1|x_2$ I'm confused, it seems hard. Do I have to consider a few cases depending on a sign of $x_1,  \ x_2$ ? Can anybody make it clear for me? It is really important to me to finally understand this topic.","Check the differentiability of the function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ of two variables given by the formula $$f(x)=|x_1|\cdot x_2$$ I still have problems with this. I started by trying to count partial derivatives: $\displaystyle\frac{\partial f}{\partial x_1}(x)=\lim_{h\to 0}\frac{f(x_1+h,x_2)-f(x_1,x_2)}{h}=\lim_{h\to 0}\frac{|x_1+h|x_2-|x_1|x_2}{h}$, so I think the problems can be at the points: $(0,x_2)$, where $x_2\neq 0$, because then we have that this limit is equal to $\displaystyle\lim_{h\to 0}\frac{|h|x_2}{h}$ which doesn't exist (left and right limits are not equal). $\displaystyle\frac{\partial f}{\partial x_2}(x)=\lim_{h\to 0}\frac{f(x_1,x_2+h)-f(x_1,x_2)}{h}=\lim_{h \to 0}\frac{|x_1|(x_2+h)-|x_1|x_2}{h}=|x_1|$, so I think we haven't any problems here, this partial derivative always exists. But what exactly can we deduce from these speculations about partial derivatives? I've also tried to proudly find the differential of this function. I was taught that the function is differentiable at the point $x$ iff there exists (if there exists, there is only one) a linear mapping $L$ such that  $(*)\displaystyle\lim_{h\to 0}\frac{f(x+h)-f(x)-L(h)}{\|h\|}=0$. Then we say that $Df(x)=L$ is a differential of function $f$ at the point $x$. For example consider function $g:\mathbb{R}^2\rightarrow \mathbb{R}, \ g(x)=x_1\cdot x_2$. We can find differential of this function by looking at the increment of this function: $g(x+h)-g(x)=(x_1+h_1)(x_2+h_2)-x_1x_2=x_1h_2+x_2h_1+h_1h_2$ . Then the candidate for $Df(x)$ is linear part of this increment: $L(h)=x_1h_2+x_2h_1$. When we check $(*)$ it appears that indeed it is a desired differential. But in my example: $f(x+h)-f(x)=|x_1+h_1|(x_2+h_2)-|x_1|x_2$ I'm confused, it seems hard. Do I have to consider a few cases depending on a sign of $x_1,  \ x_2$ ? Can anybody make it clear for me? It is really important to me to finally understand this topic.",,"['real-analysis', 'derivatives']"
12,Please help with derivative question,Please help with derivative question,,"Here's my question: A metal bar is heated to a certain temperature and then the heat source is removed. At time t minutes after the heat source is removed, the temperature, x degrees Celcius, of the metal bar is given by $x = \dfrac{280}{1+0.02t}$ At what rate is the temperature decreasing 100 minutes after the removal of the heat source? I'm guessing the chain rule needs to be employed; as in $\dfrac{dx}{dt} = \dfrac{dx}{du}\dfrac{du}{dt}$ but couldn't figure out exactly how? Help appreciated, thanks!","Here's my question: A metal bar is heated to a certain temperature and then the heat source is removed. At time t minutes after the heat source is removed, the temperature, x degrees Celcius, of the metal bar is given by $x = \dfrac{280}{1+0.02t}$ At what rate is the temperature decreasing 100 minutes after the removal of the heat source? I'm guessing the chain rule needs to be employed; as in $\dfrac{dx}{dt} = \dfrac{dx}{du}\dfrac{du}{dt}$ but couldn't figure out exactly how? Help appreciated, thanks!",,"['calculus', 'derivatives']"
13,"does dx equals to (x+h)-x? If it is, why isn't it explained like this?? [duplicate]","does dx equals to (x+h)-x? If it is, why isn't it explained like this?? [duplicate]",,"This question already has answers here : What actually is a differential? (3 answers) Closed 20 days ago . The community reviewed whether to reopen this question 19 days ago and left it closed: Original close reason(s) were not resolved While I was looking at a question regarding derivatives, I suddenly got enlightened when I realized, $dx=\lim_{h\rightarrow 0} (x+h)-(x)$ I noticed this while considering on the equation $\frac{df(x)}{dx}=\lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{(x+h)-(x)}$ Up until now, i am surprised that I've never realized this before because now every differential equation and integrals make sense now. For example, dx at the end of the integral is actually meaning the width of the rectangle over the x axis. And also, I think it is much easier to grasp the differential equations or integral when it is shown like this. I wonder, is there anything wrong about this equation, if not, why isn't it taught in this way, Instead differentials and integrals are treated entirely different things, and we just try to deal with ""dx"" by not having a single idea about what that actually means.","This question already has answers here : What actually is a differential? (3 answers) Closed 20 days ago . The community reviewed whether to reopen this question 19 days ago and left it closed: Original close reason(s) were not resolved While I was looking at a question regarding derivatives, I suddenly got enlightened when I realized, I noticed this while considering on the equation Up until now, i am surprised that I've never realized this before because now every differential equation and integrals make sense now. For example, dx at the end of the integral is actually meaning the width of the rectangle over the x axis. And also, I think it is much easier to grasp the differential equations or integral when it is shown like this. I wonder, is there anything wrong about this equation, if not, why isn't it taught in this way, Instead differentials and integrals are treated entirely different things, and we just try to deal with ""dx"" by not having a single idea about what that actually means.",dx=\lim_{h\rightarrow 0} (x+h)-(x) \frac{df(x)}{dx}=\lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{(x+h)-(x)},"['integration', 'derivatives', 'notation', 'education', 'differential']"
14,Can the gradient of a scalar field be expressed as a surface integral per unit volume?,Can the gradient of a scalar field be expressed as a surface integral per unit volume?,,"This question was migrated from Physics Stack Exchange because it can be answered on Mathematics Stack Exchange. Migrated 21 days ago . I am familiar with the usual equation for the gradient of a scalar field $\varphi$ $$ \nabla \phi = \frac{\partial \phi}{\partial x} \mathbf{i} + \frac{\partial \phi}{\partial y} \mathbf{j} + \frac{\partial \phi}{\partial z} \mathbf{k}. $$ In Butkov, 'Mathematical Physics', page 33, he has an equation for grad $\varphi$ in terms of the limit of a surface integral of $\varphi$ per unit volume. $$ \nabla \phi = \lim_{\Delta V \to 0} \frac{1}{\Delta V} \iint \phi d\mathbf{S}. $$ I do not understand this version. Is it even correct? If so, can someone explain how it agrees with the usual equation. Edited: changed it to read 'surface integral'","This question was migrated from Physics Stack Exchange because it can be answered on Mathematics Stack Exchange. Migrated 21 days ago . I am familiar with the usual equation for the gradient of a scalar field In Butkov, 'Mathematical Physics', page 33, he has an equation for grad in terms of the limit of a surface integral of per unit volume. I do not understand this version. Is it even correct? If so, can someone explain how it agrees with the usual equation. Edited: changed it to read 'surface integral'","\varphi 
\nabla \phi = \frac{\partial \phi}{\partial x} \mathbf{i} + \frac{\partial \phi}{\partial y} \mathbf{j} + \frac{\partial \phi}{\partial z} \mathbf{k}.
 \varphi \varphi 
\nabla \phi = \lim_{\Delta V \to 0} \frac{1}{\Delta V} \iint \phi d\mathbf{S}.
","['differential-geometry', 'derivatives', 'vector-fields']"
15,Working of the Chain Rule in Calculus,Working of the Chain Rule in Calculus,,"I came across a proof of the chain rule in a book called ""Calculus Made Easy"" by Silvanus P. Thompson. It said that the rule works because we essentially multiply and divide by another small change in another function (usually represented as du for a change in function u(x) ). I.e. $$\frac {dy}{dx}=\frac {dy}{du}.\frac {du}{dx}$$ But when I tried to confirm this with my Physics teacher, he said that we can't actually explain it in that way because, du separated from dy or dx has no meaning as $\frac {d}{dx}$ is considered as a whole entity or an operator rather than a fraction . Please help me out with a clear explanation regarding this. Also, please say whether there is any more direct approach other than the indirect (I suppose, it's indirect because we make up an intermediate function) chain rule to differentiate function compositions such as $$y=e^{sin x}$$ P.S. I'm just a beginner in the context of Calculus.","I came across a proof of the chain rule in a book called ""Calculus Made Easy"" by Silvanus P. Thompson. It said that the rule works because we essentially multiply and divide by another small change in another function (usually represented as du for a change in function u(x) ). I.e. But when I tried to confirm this with my Physics teacher, he said that we can't actually explain it in that way because, du separated from dy or dx has no meaning as is considered as a whole entity or an operator rather than a fraction . Please help me out with a clear explanation regarding this. Also, please say whether there is any more direct approach other than the indirect (I suppose, it's indirect because we make up an intermediate function) chain rule to differentiate function compositions such as P.S. I'm just a beginner in the context of Calculus.",\frac {dy}{dx}=\frac {dy}{du}.\frac {du}{dx} \frac {d}{dx} y=e^{sin x},"['derivatives', 'calculus']"
16,Extenstion of Caputo's fractional derivative to distribution.,Extenstion of Caputo's fractional derivative to distribution.,,"Let us start with the definitions that $\frac{d}{dx}\theta(x)=\delta(x)$ where $\theta(x)$ and $\delta(x)$ are Heaviside and delta functions. Now, with the definition: $^c D^\alpha f(t)=\frac{1}{\Gamma(n-\alpha)}\int_0^t\frac{\frac{d^n}{dx^n}f(x)}{(t-x)^{\alpha-n+1}}\mathrm d x$ I want to understand what is the half-derivative of the $\theta(x)$ such that applying another half derivative on the result gives the delta function. For a simple function like say $f(t)=sin(x)$ one can look at the Taylor expansion and for each power of x, the formula gives the half derivative to be $$^c D^\frac12 x^p=\frac{p x^{p-\frac{1}{2}} \Gamma (p)}{\Gamma \left(p+\frac{1}{2}\right)}$$ such that application of another half derivative gives the same result as the full derivative i.e. $px^{p-1}$ . However, for the Heaviside theta function, the first application gives $$^c D^\frac12 \theta(x)=\frac{2 \theta (x)-1}{\sqrt{\pi } \sqrt{x}}$$ and now I am usure how to show that the second application gives the Delta function.","Let us start with the definitions that where and are Heaviside and delta functions. Now, with the definition: I want to understand what is the half-derivative of the such that applying another half derivative on the result gives the delta function. For a simple function like say one can look at the Taylor expansion and for each power of x, the formula gives the half derivative to be such that application of another half derivative gives the same result as the full derivative i.e. . However, for the Heaviside theta function, the first application gives and now I am usure how to show that the second application gives the Delta function.",\frac{d}{dx}\theta(x)=\delta(x) \theta(x) \delta(x) ^c D^\alpha f(t)=\frac{1}{\Gamma(n-\alpha)}\int_0^t\frac{\frac{d^n}{dx^n}f(x)}{(t-x)^{\alpha-n+1}}\mathrm d x \theta(x) f(t)=sin(x) ^c D^\frac12 x^p=\frac{p x^{p-\frac{1}{2}} \Gamma (p)}{\Gamma \left(p+\frac{1}{2}\right)} px^{p-1} ^c D^\frac12 \theta(x)=\frac{2 \theta (x)-1}{\sqrt{\pi } \sqrt{x}},"['real-analysis', 'calculus', 'derivatives', 'fractional-calculus']"
17,Extrema of derivate are where tangent crosses the curve.,Extrema of derivate are where tangent crosses the curve.,,"In this article https://www.jstor.org/stable/2310782 i found this proposition: Let $f$ be a differentiable function defined on an open interval $(a, b)$ containing the point $x_0$ . Let: (B) There exists an open interval $I\subset (a, b)$ , $x_0\in I$ , such that on $I$ the function $f'$ attains a (local) maximum or minimum at $x_0$ . (C) There exists an open interval $I\subset (a, b)$ , $x_0\in I$ such that on I we have $T\ge f$ on one side of $x_0$ and $T\le f$ on the other side. Here $T$ is the tangent to the graph of $f$ at the point $x_0$ . Then (B) implies (C). I ask help for proof thats if $x_0$ is local minimum of $f'$ (i.e. $\exists \delta>0$ : $f'(x)\ge f'(x_0)$ for $x_0-\delta<x<x_0+\delta$ ), then $f\le T$ in $(x_0-\delta,x_0)$ and $f\ge T$ in $(x_0,x_0+\delta)$ . $[T(x)=f(x_0)+f'(x_0)(x-x_0)]$ . Ps. On Wikipedia ( https://en.wikipedia.org/wiki/Inflection_point ) i found that ' If all extrema of $f'$ are isolated (that is, in some neighborhood, x is the one and only point at which f' has a (local) minimum or maximum), then an inflection point is a point on the graph of f at which the tangent crosses the curve.'","In this article https://www.jstor.org/stable/2310782 i found this proposition: Let be a differentiable function defined on an open interval containing the point . Let: (B) There exists an open interval , , such that on the function attains a (local) maximum or minimum at . (C) There exists an open interval , such that on I we have on one side of and on the other side. Here is the tangent to the graph of at the point . Then (B) implies (C). I ask help for proof thats if is local minimum of (i.e. : for ), then in and in . . Ps. On Wikipedia ( https://en.wikipedia.org/wiki/Inflection_point ) i found that ' If all extrema of are isolated (that is, in some neighborhood, x is the one and only point at which f' has a (local) minimum or maximum), then an inflection point is a point on the graph of f at which the tangent crosses the curve.'","f (a, b) x_0 I\subset (a, b) x_0\in I I f' x_0 I\subset (a, b) x_0\in I T\ge f x_0 T\le f T f x_0 x_0 f' \exists \delta>0 f'(x)\ge f'(x_0) x_0-\delta<x<x_0+\delta f\le T (x_0-\delta,x_0) f\ge T (x_0,x_0+\delta) [T(x)=f(x_0)+f'(x_0)(x-x_0)] f'","['real-analysis', 'derivatives', 'maxima-minima', 'tangent-line', 'inflection-point']"
18,"Given $ye^{-{xe^{y-2x}}} = 2xe^{-x},$ where $x>\frac{1}{\sqrt{2}}, y<\sqrt{2}$. Show that $y=y(x)$ decreases.",Given  where . Show that  decreases.,"ye^{-{xe^{y-2x}}} = 2xe^{-x}, x>\frac{1}{\sqrt{2}}, y<\sqrt{2} y=y(x)","Suppose $y$ is defined by the following implicit equation: $ye^{-{xe^{y-2x}}} = 2xe^{-x},$ where $x,y\geq 0.$ I want to show that $y$ decreases as $x$ increases, when $x>\frac{1}{\sqrt{2}}$ and $y<\sqrt{2}$ . Here is my work: Suppose by contradiction that there exist some $x_1, x_2> \frac{1}{\sqrt{2}}$ such that $x_1<x_2 \implies y_1<y_2.$ From the above relation, we have $$2x_1 = y_1e^{x_1(1-e^{y_1-2x_1})} \tag{1}$$ and $$2x_2 = y_2e^{x_2(1-e^{y_2-2x_2})} .\tag{2}$$ Subtracting this results in the following: $$2(x_2-x_1) = y_2e^{x_2(1-e^{y_2-2x_2})}- y_1e^{x_1(1-e^{y_1-2x_1})}.$$ From here, I am not sure how to proceed. The goal is to arrive at the contradiction $y_1>y_2$ with the assumption but getting this inequality seems somewhat challenging. Following @jean's comment: Take the natural logarithm on both sides of $(1)$ and $(2)$ , and subtract: \begin{align*}\ln{x_2}-\ln{x_1} &= \ln{y_2}-\ln{y_1} + x_2(1-e^{y_2-2x_2})- x_1(1-e^{y_1-2x_1})\\ & = \ln{y_2}-\ln{y_1} + (x_2-x_1) + (e^{y_1-2x_1}-e^{y_2-2x_2}). \end{align*} This looks much better compared to the exponential expression.","Suppose is defined by the following implicit equation: where I want to show that decreases as increases, when and . Here is my work: Suppose by contradiction that there exist some such that From the above relation, we have and Subtracting this results in the following: From here, I am not sure how to proceed. The goal is to arrive at the contradiction with the assumption but getting this inequality seems somewhat challenging. Following @jean's comment: Take the natural logarithm on both sides of and , and subtract: This looks much better compared to the exponential expression.","y ye^{-{xe^{y-2x}}} = 2xe^{-x}, x,y\geq 0. y x x>\frac{1}{\sqrt{2}} y<\sqrt{2} x_1, x_2> \frac{1}{\sqrt{2}} x_1<x_2 \implies y_1<y_2. 2x_1 = y_1e^{x_1(1-e^{y_1-2x_1})} \tag{1} 2x_2 = y_2e^{x_2(1-e^{y_2-2x_2})} .\tag{2} 2(x_2-x_1) = y_2e^{x_2(1-e^{y_2-2x_2})}- y_1e^{x_1(1-e^{y_1-2x_1})}. y_1>y_2 (1) (2) \begin{align*}\ln{x_2}-\ln{x_1} &= \ln{y_2}-\ln{y_1} + x_2(1-e^{y_2-2x_2})- x_1(1-e^{y_1-2x_1})\\
& = \ln{y_2}-\ln{y_1} + (x_2-x_1) + (e^{y_1-2x_1}-e^{y_2-2x_2}).
\end{align*}","['calculus', 'derivatives']"
19,Question about theorem 9.21 in Rudin's PMA,Question about theorem 9.21 in Rudin's PMA,,"The theorem in question is: Now, the part that I have a problem is the following highlighted text: Theorem (5.10) is I don't understand what function plays the role of $f$ at (5.10). What i've tried is the function $$g(t)=f(x+v_{j-1}+th_je_j)$$ But its derivative depends on the deriative of $f$ which I don't know it exists yet.","The theorem in question is: Now, the part that I have a problem is the following highlighted text: Theorem (5.10) is I don't understand what function plays the role of at (5.10). What i've tried is the function But its derivative depends on the deriative of which I don't know it exists yet.",f g(t)=f(x+v_{j-1}+th_je_j) f,"['derivatives', 'mean-value-theorem']"
20,Limits inferior of differentiable function,Limits inferior of differentiable function,,"I am asking myself if for every differentiable function $f\colon[0,\infty)\to\mathbb R$ with $f(0)=0$ and $f(x)>0$ for every $x>0$ in a neighborhood of zero, we have $$\liminf_{x\to 0^+} \frac{f(x)}{f'(x)}=0?$$ Consider $\displaystyle\lim_{\delta\to 0+} \inf\Big\{\frac{f(x)}{f'(x)}:x\in (0,\delta)\Big\}$ . As the function $f$ is continuous, the numerator goes to $0$ . Since $f$ is continuous at $0$ and $f(x)>0\ \forall x>0$ , we have by the $\varepsilon,\delta$ -criterion: $$\forall \varepsilon >0 \ \exists \delta>0 \text{ such that } \forall x \in [0,\delta) \text{ we have } |f(x)|=f(x)<\varepsilon.$$ Hence, the difference quotient $\frac{f(x)- f(0)}{x-0}=\frac{f(x)}{x}>0 \ \forall x>0$ . My thought is to use the following intuitive argument: If $f(0)=0$ and $f(x)>0$ for every $x>0$ , then $f'(x)>0$ for $x$ sufficiently close to $0$ .","I am asking myself if for every differentiable function with and for every in a neighborhood of zero, we have Consider . As the function is continuous, the numerator goes to . Since is continuous at and , we have by the -criterion: Hence, the difference quotient . My thought is to use the following intuitive argument: If and for every , then for sufficiently close to .","f\colon[0,\infty)\to\mathbb R f(0)=0 f(x)>0 x>0 \liminf_{x\to 0^+} \frac{f(x)}{f'(x)}=0? \displaystyle\lim_{\delta\to 0+} \inf\Big\{\frac{f(x)}{f'(x)}:x\in (0,\delta)\Big\} f 0 f 0 f(x)>0\ \forall x>0 \varepsilon,\delta \forall \varepsilon >0 \ \exists \delta>0 \text{ such that } \forall x \in [0,\delta) \text{ we have } |f(x)|=f(x)<\varepsilon. \frac{f(x)- f(0)}{x-0}=\frac{f(x)}{x}>0 \ \forall x>0 f(0)=0 f(x)>0 x>0 f'(x)>0 x 0","['real-analysis', 'derivatives', 'continuity', 'limsup-and-liminf']"
21,Variational Calculus Confusion,Variational Calculus Confusion,,"In the variational calculus, we find the Euler-Lagrange equation by using the following: $S[x(t) + \epsilon f(t)] = S[x(t)] + \delta S + O(\epsilon^2)$ (1) $S[x(t) + \epsilon f(t)] = S[x(t)] + \frac{d}{d\epsilon}S[x(t) + \epsilon f(t)]\Bigr|_{\epsilon=0}\epsilon + O(\epsilon^2)$ Now, I understand that in (1), the LHS's value has to be less (over all - including all orders) than the RHS's value because $x(t)$ is what minimizes the action, but I also understand that $\delta S$ must be $0$ . So, even if $\delta S = 0$ , (1) still obeys the principle such that the LHS is less than the RHS because the RHS also has $O(\epsilon^2)$ . So we say actions are equal in first order. Now, if I look at i.e $\delta S = \frac{d}{d\epsilon}S[x(t) + \epsilon f(t)]\Bigr|_{\epsilon=0}\epsilon$ , what this tells me in words is how much action changes between our paths, so since we set this to $0$ , then we're officially saying that action values are the same on both paths, even though, on true path, the value has to be less. What's going on? Can this be explained in layman's terms?","In the variational calculus, we find the Euler-Lagrange equation by using the following: (1) Now, I understand that in (1), the LHS's value has to be less (over all - including all orders) than the RHS's value because is what minimizes the action, but I also understand that must be . So, even if , (1) still obeys the principle such that the LHS is less than the RHS because the RHS also has . So we say actions are equal in first order. Now, if I look at i.e , what this tells me in words is how much action changes between our paths, so since we set this to , then we're officially saying that action values are the same on both paths, even though, on true path, the value has to be less. What's going on? Can this be explained in layman's terms?",S[x(t) + \epsilon f(t)] = S[x(t)] + \delta S + O(\epsilon^2) S[x(t) + \epsilon f(t)] = S[x(t)] + \frac{d}{d\epsilon}S[x(t) + \epsilon f(t)]\Bigr|_{\epsilon=0}\epsilon + O(\epsilon^2) x(t) \delta S 0 \delta S = 0 O(\epsilon^2) \delta S = \frac{d}{d\epsilon}S[x(t) + \epsilon f(t)]\Bigr|_{\epsilon=0}\epsilon 0,"['integration', 'derivatives', 'calculus-of-variations']"
22,Three real roots of a cubic,Three real roots of a cubic,,"Question: If the equation $z^3-mz^2+lz-k=0$ has three real roots, then necessary condition must be _______ $l=1$ $ l \neq 1$ $ m = 1$ $ m \neq 1$ I know there is a question here on stack about discriminant and all for these three distinct real roots but I do not feel myself competent to use that. However I tried differentiating the function and for real roots I ended up with $m^2 \geq 3l$","Question: If the equation has three real roots, then necessary condition must be _______ I know there is a question here on stack about discriminant and all for these three distinct real roots but I do not feel myself competent to use that. However I tried differentiating the function and for real roots I ended up with",z^3-mz^2+lz-k=0 l=1  l \neq 1  m = 1  m \neq 1 m^2 \geq 3l,"['calculus', 'algebra-precalculus', 'derivatives', 'cubics', 'roots-of-cubics']"
23,"Finding $f'(x), f(x)= \frac{4x^5+12x^3-40x}{4(x^2+5)}$",Finding,"f'(x), f(x)= \frac{4x^5+12x^3-40x}{4(x^2+5)}","I'm lost, my textbook says the answer is $3x^2-2.$ At this point in the book, they haven't taught quotient rule, but i tried it with quotient rule and I couldn't get $3x^2-2$ . The only method they've shown for differentiating fractions involved separating each term in the numerator into an individual fraction, but I'm not sure how to do that here. If anyone can see what the book's expecting students to do, I'd really appreciate if they'd show me.","I'm lost, my textbook says the answer is At this point in the book, they haven't taught quotient rule, but i tried it with quotient rule and I couldn't get . The only method they've shown for differentiating fractions involved separating each term in the numerator into an individual fraction, but I'm not sure how to do that here. If anyone can see what the book's expecting students to do, I'd really appreciate if they'd show me.",3x^2-2. 3x^2-2,"['calculus', 'derivatives']"
24,Determine the horizontal tangent to the graph of the trigonometric function $f(x) = 2x + \sqrt{3}\cos(x)$,Determine the horizontal tangent to the graph of the trigonometric function,f(x) = 2x + \sqrt{3}\cos(x),"Given $y = 2x + \sqrt{3}\cos{x}$ , find all horizontal tangents and write a general solution for x. To find horizontal tangents, we derive and equal to 0: $y' = 2 - \sqrt{3} \sin{x}$ ; $y'=0$ $2 - \sqrt{3} \sin{x} = 0$ $\sin{x}= \frac{2}{\sqrt{3}}$ $x = \arcsin(\frac{2}{\sqrt{3}})$ However, $\arcsin(\frac{2}{\sqrt{3}})$ is not a real number. Upon graphing, I found the horizontal tangents to be $x = \frac{π}{2} + πn$ where n is an integer. But I don't know how to find the x values without graphing. Any ideas? Thank you!","Given , find all horizontal tangents and write a general solution for x. To find horizontal tangents, we derive and equal to 0: ; However, is not a real number. Upon graphing, I found the horizontal tangents to be where n is an integer. But I don't know how to find the x values without graphing. Any ideas? Thank you!",y = 2x + \sqrt{3}\cos{x} y' = 2 - \sqrt{3} \sin{x} y'=0 2 - \sqrt{3} \sin{x} = 0 \sin{x}= \frac{2}{\sqrt{3}} x = \arcsin(\frac{2}{\sqrt{3}}) \arcsin(\frac{2}{\sqrt{3}}) x = \frac{π}{2} + πn,"['calculus', 'derivatives', 'trigonometry', 'complex-numbers']"
25,Can a relation have a derivative?,Can a relation have a derivative?,,"In most textbooks when they talk about derivatives, they always say ""derivative of a function"" , but my question is, can derivatives be defined for relations other than functions? Like when one value of x gives two values of y, as in a circle or a parabola? Eg: $x^2+y^2=r^2$ or $y^2=4ax$ . If they are defined for relations as well, then why is it always mentioned as derivative of a function?","In most textbooks when they talk about derivatives, they always say ""derivative of a function"" , but my question is, can derivatives be defined for relations other than functions? Like when one value of x gives two values of y, as in a circle or a parabola? Eg: or . If they are defined for relations as well, then why is it always mentioned as derivative of a function?",x^2+y^2=r^2 y^2=4ax,"['calculus', 'derivatives']"
26,Derivatives of Matrix Functions of Different Dimensions,Derivatives of Matrix Functions of Different Dimensions,,"Notation: For matrices $A,B\in\mathbb R^{n\times m}$ , we define the inner product $\langle A,B\rangle=\sum_{i,j}A_{ij}B_{ij}$ The basis vector $e_i$ is equal to $1$ at position $i$ and $0$ otherwise. Problem: Consider the function $f:\mathbb R^{n\times n}\times \mathbb R^{d\times n}\to \mathbb R$ such that $$f(X,Y)=\langle W, X-Y^\intercal Y\rangle.$$ My goal is to determine when the derivative of $f$ with respect to $X$ and $Y$ are equal to $0$ . From what I understand, we can take $\nabla_Xf=W$ . However, for $\nabla_Yf$ , we should consider the matrix $M$ labeled with $$M_{ij}=\frac{\partial f}{\partial Y_{ij}}=e_j^\intercal WY^\intercal e_i$$ (here $e_j\in\mathbb R^n$ , while $e_i\in\mathbb R^d$ ), meaning $M=WY^\intercal$ . The problem is that $M$ is of size $d\times n$ , so I'm not sure how to describe when the derivative of $f$ is equal to some matrix $Z$ . Attempt: I thought about trying to represent the problem in a way where everything is the same shape. I noticed \begin{align}     \langle W,Y^\intercal Y\rangle&=\text{Trace}(W^\intercal Y^\intercal Y)\\ &=\text{Trace}(YW^\intercal Y^\intercal)\\ &=\langle WY^\intercal, Y^\intercal\rangle\\ &=\left\langle\begin{bmatrix} 0&Y^\intercal\\ Y&0 \end{bmatrix}, \begin{bmatrix} 0&WY^\intercal\\ YW^\intercal&0 \end{bmatrix} \right\rangle, \end{align} meaning we can rewrite $f$ as $$ f(X, Y)=\left\langle\begin{bmatrix}     X&Y^\intercal\\     Y&0 \end{bmatrix}, \begin{bmatrix}     W&-WY^\intercal/2\\     -YW^\intercal/2&0 \end{bmatrix} \right\rangle $$ so I'm thinking we can represent $\nabla_{X,Y}f$ as $$\begin{bmatrix} W&-WY^\intercal/2\\ -YW^\intercal/2&0\end{bmatrix}.$$ If $Z=\begin{bmatrix} Z_1&Z_2^\intercal\\ Z_2 & Z_3 \end{bmatrix}$ , Then the derivative is equal to $Z$ when $W=Z_1$ , $YW^\intercal=-2Z_2$ (meaning $Y^\intercal=-2W^+Z_2$ ?), and $Z_3=0$ . Does this make any sense? How is this usually done?","Notation: For matrices , we define the inner product The basis vector is equal to at position and otherwise. Problem: Consider the function such that My goal is to determine when the derivative of with respect to and are equal to . From what I understand, we can take . However, for , we should consider the matrix labeled with (here , while ), meaning . The problem is that is of size , so I'm not sure how to describe when the derivative of is equal to some matrix . Attempt: I thought about trying to represent the problem in a way where everything is the same shape. I noticed meaning we can rewrite as so I'm thinking we can represent as If , Then the derivative is equal to when , (meaning ?), and . Does this make any sense? How is this usually done?","A,B\in\mathbb R^{n\times m} \langle A,B\rangle=\sum_{i,j}A_{ij}B_{ij} e_i 1 i 0 f:\mathbb R^{n\times n}\times \mathbb R^{d\times n}\to \mathbb R f(X,Y)=\langle W, X-Y^\intercal Y\rangle. f X Y 0 \nabla_Xf=W \nabla_Yf M M_{ij}=\frac{\partial f}{\partial Y_{ij}}=e_j^\intercal WY^\intercal e_i e_j\in\mathbb R^n e_i\in\mathbb R^d M=WY^\intercal M d\times n f Z \begin{align}
    \langle W,Y^\intercal Y\rangle&=\text{Trace}(W^\intercal Y^\intercal Y)\\
&=\text{Trace}(YW^\intercal Y^\intercal)\\
&=\langle WY^\intercal, Y^\intercal\rangle\\
&=\left\langle\begin{bmatrix}
0&Y^\intercal\\
Y&0
\end{bmatrix},
\begin{bmatrix}
0&WY^\intercal\\
YW^\intercal&0
\end{bmatrix}
\right\rangle,
\end{align} f 
f(X, Y)=\left\langle\begin{bmatrix}
    X&Y^\intercal\\
    Y&0
\end{bmatrix},
\begin{bmatrix}
    W&-WY^\intercal/2\\
    -YW^\intercal/2&0
\end{bmatrix}
\right\rangle
 \nabla_{X,Y}f \begin{bmatrix}
W&-WY^\intercal/2\\
-YW^\intercal/2&0\end{bmatrix}. Z=\begin{bmatrix}
Z_1&Z_2^\intercal\\
Z_2 & Z_3
\end{bmatrix} Z W=Z_1 YW^\intercal=-2Z_2 Y^\intercal=-2W^+Z_2 Z_3=0","['linear-algebra', 'derivatives', 'matrix-calculus', 'matrix-analysis']"
27,"A continuous, almost nowhere differentiable function","A continuous, almost nowhere differentiable function",,"Consider the function $$f(x)=\sum _{k=0}^{\infty }\frac{\sin \left(k^2\pi x\right)}{k^2}.$$ It converges uniformly on $\mathbb{R}$ according to the Weierstrass M-test and is continuous on $\mathbb{R}$ due to the uniform limit theorem . According to my lecture notes, the termwise differentiated series $\sum_{k=1}^\infty \pi \cos (k^2\pi x)$ diverges. It doesn't state for which values of $x$ it diverges, however, using a similar argument as in this answer , I believe it diverges for all $x\in \mathbb{R}$ (the argument goes like this; whenever $\cos (k^2 \pi x)$ is small, then $k^2 x$ will be close to an odd multiple of $1/2$ , and therefor $(2k)^2 x$ will be close to an even multiple of $1/2$ and correspondingly larger). Now, my lecture notes go on to claim that $f$ in fact is differentiable at some rational points with derivative $-1/2$ . I was wondering if this is true and if yes, if it's easy to show (on an undergraduate level). Also, I'd be grateful if you could explain why the linked argument fails to show divergence on all of $\mathbb{R}$ , if my lecture notes really are correct.","Consider the function It converges uniformly on according to the Weierstrass M-test and is continuous on due to the uniform limit theorem . According to my lecture notes, the termwise differentiated series diverges. It doesn't state for which values of it diverges, however, using a similar argument as in this answer , I believe it diverges for all (the argument goes like this; whenever is small, then will be close to an odd multiple of , and therefor will be close to an even multiple of and correspondingly larger). Now, my lecture notes go on to claim that in fact is differentiable at some rational points with derivative . I was wondering if this is true and if yes, if it's easy to show (on an undergraduate level). Also, I'd be grateful if you could explain why the linked argument fails to show divergence on all of , if my lecture notes really are correct.",f(x)=\sum _{k=0}^{\infty }\frac{\sin \left(k^2\pi x\right)}{k^2}. \mathbb{R} \mathbb{R} \sum_{k=1}^\infty \pi \cos (k^2\pi x) x x\in \mathbb{R} \cos (k^2 \pi x) k^2 x 1/2 (2k)^2 x 1/2 f -1/2 \mathbb{R},"['real-analysis', 'sequences-and-series', 'derivatives', 'continuity', 'uniform-convergence']"
28,What is the derivative of $\sin^{-1}(\frac{x+\sqrt{1-x^2}}{\sqrt{2}})$ [closed],What is the derivative of  [closed],\sin^{-1}(\frac{x+\sqrt{1-x^2}}{\sqrt{2}}),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question If we substitute $x=\sin \theta$ then we get the derivative to be $\frac{1}{\sqrt{1-x^2}}$ shown in the picture below But instead of assuming $x=\sin \theta$ if we assume $x=\cos \theta$ we get the answer to be $-\frac{1}{\sqrt{1-x^2}}$ . Why the answers are different in this case? Where is the mistake here?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question If we substitute then we get the derivative to be shown in the picture below But instead of assuming if we assume we get the answer to be . Why the answers are different in this case? Where is the mistake here?",x=\sin \theta \frac{1}{\sqrt{1-x^2}} x=\sin \theta x=\cos \theta -\frac{1}{\sqrt{1-x^2}},"['calculus', 'derivatives', 'inverse-function']"
29,Partial derivative with respect to a matrix in RNN backpropagation,Partial derivative with respect to a matrix in RNN backpropagation,,"I have an issue with the following problem. I am trying to derive the gradients with respect to $x_t, h_{t-1}, W_x, W_h$ . $x_t$ is a $N*D$ vector. $h_t$ is a $N*H$ vector. $W_h$ is a $H*H$ matrix. $W_x$ is a $D*H$ matrix. The function is $h_t=tanh(h_{t-1}W_h+x_tW_x+b)=tanh(O_t)$ I am struggling with the derivative with respect to $W_h, W_x$ The answer said that the derivative with respect to $W_h$ is $h_{t-1}^T⋅dh_t⋅(1-h_t⋅h_t)$ . $dh_t$ means the derivative obtained in the previous step. Also, $dh_t⋅(1-h_t⋅h_t)$ is just inner product in my guess. $h_{t-1}^T⋅dh_t$ is done by matrix multiplication. The most confusing part is how $h_{t-1}^T$ moves to the first term when the chain rule is applied. I will appreciate if anyone could answer this question.","I have an issue with the following problem. I am trying to derive the gradients with respect to . is a vector. is a vector. is a matrix. is a matrix. The function is I am struggling with the derivative with respect to The answer said that the derivative with respect to is . means the derivative obtained in the previous step. Also, is just inner product in my guess. is done by matrix multiplication. The most confusing part is how moves to the first term when the chain rule is applied. I will appreciate if anyone could answer this question.","x_t, h_{t-1}, W_x, W_h x_t N*D h_t N*H W_h H*H W_x D*H h_t=tanh(h_{t-1}W_h+x_tW_x+b)=tanh(O_t) W_h, W_x W_h h_{t-1}^T⋅dh_t⋅(1-h_t⋅h_t) dh_t dh_t⋅(1-h_t⋅h_t) h_{t-1}^T⋅dh_t h_{t-1}^T","['linear-algebra', 'matrices', 'derivatives', 'neural-networks', 'backpropagation']"
30,Why does the TI-30X Pro calculator seem to get the derivative of $\sin(x)$ at $x=0$ wrong?,Why does the TI-30X Pro calculator seem to get the derivative of  at  wrong?,\sin(x) x=0,"I typed the derivative of $\sin(x)$ at $x=0$ into my calculator, and it outputted $0.0174...$ I was expecting the answer $1$ . What went wrong?","I typed the derivative of at into my calculator, and it outputted I was expecting the answer . What went wrong?",\sin(x) x=0 0.0174... 1,"['calculus', 'derivatives', 'calculator']"
31,Handling the differentiation of piecewise functions,Handling the differentiation of piecewise functions,,"When I differentiate $\large f(x) = x|x|$ using the product rule i get: $$\large f'(x)=x\frac{d}{dx}|x| + |x|$$ the problem is that the derivative of $\large |x|$ is not defined when $\large x=0$ : $$\large \frac{d}{dx}|x|=\begin{cases}-1,x<0\\1,x>0 \end{cases}$$ so $\large f'(x)$ becomes: $$\large f'(x)=\begin{cases}2x,x>0\\-2x, x<0\end{cases}$$ But in reality $\large f(x)$ is differentiable for $\large x=0$ You can correctly differentiate $\large f(x)$ by ""merging"" first the functions $\large |x|$ and $\large x$ into a single piecewise function: $$\large f(x)=\begin{cases}x^2,x≥0\\-x^2,x<0\end{cases}$$ $$\large f'(x)=\begin{cases}2x,x≥0\\-2x,x<0\end{cases}$$ So, how can I differentiate piecewise functions like this one and always obtain the correct domain for the derivative? Is it enough to ""merge"" the functions and then check if the breakpoints are differentiable by checking if the left and right hand derivatives are the same?","When I differentiate using the product rule i get: the problem is that the derivative of is not defined when : so becomes: But in reality is differentiable for You can correctly differentiate by ""merging"" first the functions and into a single piecewise function: So, how can I differentiate piecewise functions like this one and always obtain the correct domain for the derivative? Is it enough to ""merge"" the functions and then check if the breakpoints are differentiable by checking if the left and right hand derivatives are the same?","\large f(x) = x|x| \large f'(x)=x\frac{d}{dx}|x| + |x| \large |x| \large x=0 \large \frac{d}{dx}|x|=\begin{cases}-1,x<0\\1,x>0 \end{cases} \large f'(x) \large f'(x)=\begin{cases}2x,x>0\\-2x, x<0\end{cases} \large f(x) \large x=0 \large f(x) \large |x| \large x \large f(x)=\begin{cases}x^2,x≥0\\-x^2,x<0\end{cases} \large f'(x)=\begin{cases}2x,x≥0\\-2x,x<0\end{cases}","['calculus', 'derivatives']"
32,Does This Equation Have a Closed Form Solution?,Does This Equation Have a Closed Form Solution?,,"In a previous question I asked ( Lagrange Method With Random Variables ), someone suggested to me (in the comments) that I can use Maximum Likelihood Estimation to arrive at the same solution for an Optimization Problem involving the Lagrange Method ( https://en.wikipedia.org/wiki/Inverse-variance_weighting ). Here is the original problem: Consider a generic weighted sum $Y=\sum_i w_i X_i$ , where the weights $w_i$ are normalized such that $\sum_i w_i = 1$ . If the $X_i$ are all independent, the variance of $Y$ is given by $\text{Var}(Y) = \sum_i w_i^2 \sigma_i^2$ Now, here is my attempt to arrive at the same solution using Maximum Likelihood Estimation (as per the suggestion in the comments): Suppose $Y = \sum_{i} w_iX_i$ , where $w_i$ are the weights, $X_i$ are independent random variables, and $\sum_{i} w_i = 1$ . We want to find the optimal weights that minimize the variance of $Y$ . If $X_i$ follows a Normal Distribution with mean $\mu_i$ and variance $\sigma_i^2$ , then we can define some new random variable $Y$ and write the distribution of $Y$ as: $$Y \sim N\left(\sum_{i} w_i\mu_i, \sum_{i} w_i^2\sigma_i^2\right)$$ Now, the logarithm of the likelihood function can be written as: $$\log(L) = \log\left(\prod_{i} \frac{1}{\sqrt{2\pi\sum_{j} w_j^2\sigma_j^2}} \exp\left(-\frac{1}{2\sum_{j} w_j^2\sigma_j^2}(y - \sum_{i} w_i\mu_i)^2\right)\right)$$ $$= -\frac{1}{2}\log(2\pi) - \frac{1}{2}\log\left(\sum_{j} w_j^2\sigma_j^2\right) - \frac{1}{2\sum_{j} w_j^2\sigma_j^2}(y - \sum_{i} w_i\mu_i)^2$$ To find the maximum likelihood estimate, we differentiate (using chain rule) the log-likelihood with respect to $w_i$ and set it to zero: $$\frac{\partial}{\partial w_{i}} \log (L) = -\frac{w_i\sigma_i^2}{(\sum_j w_j^2\sigma_j^2)^2} + \frac{(\sum_j w_j^2\sigma_j^2)(y - \sum_{i} w_{i} \mu_{i})\mu_i + (y - \sum_{i} w_{i} \mu_{i})^2w_i\sigma_i^2}{(\sum_j w_j^2\sigma_j^2)^2} = 0$$ But I am not sure if the above system of likelihood equations has a closed form solution for $w_i$ . But perhaps this system of likelihood equations might have a closed form solution if all $\mu_i$ are equal to each other ? My Question: Can someone please tell me if I am doing this correctly? Can Maximum Likelihood Estimation really be used to arrive at the same optimal solutions for $w_i$ as compared to the Lagrange Method? Thanks! Note : If $Y=\sum_i w_i X_i$ , then I don't think that variance of $Y$ is given by $\text{Var}(Y) = \sum_i w_i^2 \sigma_i^2$ unless each $X_i$ has the same mean $\mu$ . This suggests that the ""objective function"" (i.e. $\text{Var}(Y)$ being optimized would actually be a completely different function with a completely different optimal solution of $w_i$ I think actually asked a question about this same point over here: Simplifying the Formulas for Weighted Means","In a previous question I asked ( Lagrange Method With Random Variables ), someone suggested to me (in the comments) that I can use Maximum Likelihood Estimation to arrive at the same solution for an Optimization Problem involving the Lagrange Method ( https://en.wikipedia.org/wiki/Inverse-variance_weighting ). Here is the original problem: Consider a generic weighted sum , where the weights are normalized such that . If the are all independent, the variance of is given by Now, here is my attempt to arrive at the same solution using Maximum Likelihood Estimation (as per the suggestion in the comments): Suppose , where are the weights, are independent random variables, and . We want to find the optimal weights that minimize the variance of . If follows a Normal Distribution with mean and variance , then we can define some new random variable and write the distribution of as: Now, the logarithm of the likelihood function can be written as: To find the maximum likelihood estimate, we differentiate (using chain rule) the log-likelihood with respect to and set it to zero: But I am not sure if the above system of likelihood equations has a closed form solution for . But perhaps this system of likelihood equations might have a closed form solution if all are equal to each other ? My Question: Can someone please tell me if I am doing this correctly? Can Maximum Likelihood Estimation really be used to arrive at the same optimal solutions for as compared to the Lagrange Method? Thanks! Note : If , then I don't think that variance of is given by unless each has the same mean . This suggests that the ""objective function"" (i.e. being optimized would actually be a completely different function with a completely different optimal solution of I think actually asked a question about this same point over here: Simplifying the Formulas for Weighted Means","Y=\sum_i w_i X_i w_i \sum_i w_i = 1 X_i Y \text{Var}(Y) = \sum_i w_i^2 \sigma_i^2 Y = \sum_{i} w_iX_i w_i X_i \sum_{i} w_i = 1 Y X_i \mu_i \sigma_i^2 Y Y Y \sim N\left(\sum_{i} w_i\mu_i, \sum_{i} w_i^2\sigma_i^2\right) \log(L) = \log\left(\prod_{i} \frac{1}{\sqrt{2\pi\sum_{j} w_j^2\sigma_j^2}} \exp\left(-\frac{1}{2\sum_{j} w_j^2\sigma_j^2}(y - \sum_{i} w_i\mu_i)^2\right)\right) = -\frac{1}{2}\log(2\pi) - \frac{1}{2}\log\left(\sum_{j} w_j^2\sigma_j^2\right) - \frac{1}{2\sum_{j} w_j^2\sigma_j^2}(y - \sum_{i} w_i\mu_i)^2 w_i \frac{\partial}{\partial w_{i}} \log (L) = -\frac{w_i\sigma_i^2}{(\sum_j w_j^2\sigma_j^2)^2} + \frac{(\sum_j w_j^2\sigma_j^2)(y - \sum_{i} w_{i} \mu_{i})\mu_i + (y - \sum_{i} w_{i} \mu_{i})^2w_i\sigma_i^2}{(\sum_j w_j^2\sigma_j^2)^2} = 0 w_i \mu_i w_i Y=\sum_i w_i X_i Y \text{Var}(Y) = \sum_i w_i^2 \sigma_i^2 X_i \mu \text{Var}(Y) w_i","['probability', 'derivatives', 'optimization', 'maximum-likelihood']"
33,"In L'Hospital rule should we specify $g(x) \ne 0$ on $(a,b)$ in the theorem to avoid division by $0$? Does $g'(x)\ne 0$ imply $g(x)\ne 0$ on $(a,b)$?",In L'Hospital rule should we specify  on  in the theorem to avoid division by ? Does  imply  on ?,"g(x) \ne 0 (a,b) 0 g'(x)\ne 0 g(x)\ne 0 (a,b)","L'Hospital rule (according to Rudin or this website ): Suppose that $f$ and $g$ are differentiable on the open interval $(a,b)$ , that $\lim_{x\to a^+} f(x) = 0$ , $\lim_{x\to a^+} g(x) = 0$ , that $g'(x) \ne 0$ on $(a,b)$ and that $\lim_{x\to a^+} \frac{f'}{g'}$ exists. Then, $$\lim_{x\to a^+} \frac{f(x)}{g(x)} = \lim_{x\to a^+} \frac{f'(x)}{g'(x)}$$ Here is a proof (the proof given in the website ): Let $f(a)=g(a)=0$ Then $f,g$ are continuous on $[a,b)$ . Let $x\in (a,b)$ . $f,g$ are continuous on $[a,x]$ and differentiable on $(a,x)$ . By the generalized MVT, we have: $$f(x) g'(c)= g(x) f'(c)$$ for a certain $c\in (a,x)$ . Hence, $$\frac{f'(c)}{g'(c)} = \frac{f(x)}{g(x)} \tag{1}$$ We conclude by taking the limit as $x$ tends to $a^+$ . My question is: why does the statement of the theorem (be it from Rudin, on from the website ) doesn't require $g(x)\ne 0$ on $(a,b)$ ? In $(1)$ we get to divide by $g(x)$ . Do we have the guarantee that the RHS denominator doesn't blow up when the LHS denominator is different from $0$ as in $(1)$ ? Thank you. Edit: From $(1)$ we have: $$g(x) = \frac{g'(c)}{f'(c)} f(x)$$ However we don't have the guarantee that $f(x)/f'(c) \ne 0$","L'Hospital rule (according to Rudin or this website ): Suppose that and are differentiable on the open interval , that , , that on and that exists. Then, Here is a proof (the proof given in the website ): Let Then are continuous on . Let . are continuous on and differentiable on . By the generalized MVT, we have: for a certain . Hence, We conclude by taking the limit as tends to . My question is: why does the statement of the theorem (be it from Rudin, on from the website ) doesn't require on ? In we get to divide by . Do we have the guarantee that the RHS denominator doesn't blow up when the LHS denominator is different from as in ? Thank you. Edit: From we have: However we don't have the guarantee that","f g (a,b) \lim_{x\to a^+} f(x) = 0 \lim_{x\to a^+} g(x) = 0 g'(x) \ne 0 (a,b) \lim_{x\to a^+} \frac{f'}{g'} \lim_{x\to a^+} \frac{f(x)}{g(x)} = \lim_{x\to a^+} \frac{f'(x)}{g'(x)} f(a)=g(a)=0 f,g [a,b) x\in (a,b) f,g [a,x] (a,x) f(x) g'(c)= g(x) f'(c) c\in (a,x) \frac{f'(c)}{g'(c)} = \frac{f(x)}{g(x)} \tag{1} x a^+ g(x)\ne 0 (a,b) (1) g(x) 0 (1) (1) g(x) = \frac{g'(c)}{f'(c)} f(x) f(x)/f'(c) \ne 0","['derivatives', 'mean-value-theorem']"
34,Does Taylor's theorem (Peano's form) generalize to other fields?,Does Taylor's theorem (Peano's form) generalize to other fields?,,"Let $\mathbb F$ be a field with a non-trivial absolute value, such as $\mathbb F_p(X)$ or $\mathbb Q$ or $\mathbb Q_p$ , and let $f:\mathbb F\to\mathbb F$ be a function. Limits, continuity, and derivatives are defined as usual: $\lim_{x\to a}f(x)=b$ means that for any real $\varepsilon>0$ there exists $\delta>0$ such that, for all $x\in\mathbb F$ where $0<|x-a|<\delta$ , $|f(x)-b|<\varepsilon$ . We say that $f$ is $n$ th-order Peano differentiable at $a$ , if there's a polynomial $\sum_{k=0}^nc_kx^k$ and a function $h$ continuous at $h(0)=0$ such that $$f(a+x)=\sum_{k=0}^nc_kx^k+x^nh(x).$$ Suppose $f$ has both ordinary derivatives and Peano derivatives up to order $n$ at $a$ . Must they be proportional, $$f^{(k)}(a)=k!\,c_k\quad?$$ Related: In characteristic $2$, can a function have a non-zero second derivative? A function may have Peano derivatives but not ordinary derivatives of order $n$ . Here's an example with $\mathbb F=\mathbb R$ (I suppose a similar example could be constructed for $\mathbb F=\mathbb Q$ using $2^{\lfloor-1/x^2\rfloor}$ on the dyadic rationals): $$f(x)=\begin{cases}e^{-1/x^2},\quad x\in\mathbb Q\setminus\{0\}; \\ 0,\quad\text{otherwise};\end{cases}$$ this satisfies $f(0+x)=0+0x+\cdots+0x^n+x^nh(x)$ where $\lim_{x\to0}h(x)=0$ . But $f''(0)$ doesn't exist, because $f'(x)$ doesn't exist in a neighbourhood of $0$ , because $f(x)$ is not continuous in a neighbourhood of $0$ . Conversely, a function may have ordinary derivatives but not Peano derivatives of order $n$ . An example with $\mathbb F=\mathbb Q_p$ is given here : $$f(x)=\begin{cases}p^{2m},\quad x=p^m+yp^{2m+1},\quad y\in\mathbb Z_p,\quad m\in\mathbb N; \\ 0,\quad\text{otherwise};\end{cases}$$ this satisfies $f'(x)=0$ for all $x$ , hence $f^{(n)}(0)=0$ for all $n>1$ . But $\lim_{x\to0}f(x)/x^2$ doesn't exist, so $f$ can't have a $2$ nd-order Peano derivative; $f(0+x)=0+0x+c_2x^2+x^2h(x)$ would imply that the limit exists as $c_2$ .","Let be a field with a non-trivial absolute value, such as or or , and let be a function. Limits, continuity, and derivatives are defined as usual: means that for any real there exists such that, for all where , . We say that is th-order Peano differentiable at , if there's a polynomial and a function continuous at such that Suppose has both ordinary derivatives and Peano derivatives up to order at . Must they be proportional, Related: In characteristic $2$, can a function have a non-zero second derivative? A function may have Peano derivatives but not ordinary derivatives of order . Here's an example with (I suppose a similar example could be constructed for using on the dyadic rationals): this satisfies where . But doesn't exist, because doesn't exist in a neighbourhood of , because is not continuous in a neighbourhood of . Conversely, a function may have ordinary derivatives but not Peano derivatives of order . An example with is given here : this satisfies for all , hence for all . But doesn't exist, so can't have a nd-order Peano derivative; would imply that the limit exists as .","\mathbb F \mathbb F_p(X) \mathbb Q \mathbb Q_p f:\mathbb F\to\mathbb F \lim_{x\to a}f(x)=b \varepsilon>0 \delta>0 x\in\mathbb F 0<|x-a|<\delta |f(x)-b|<\varepsilon f n a \sum_{k=0}^nc_kx^k h h(0)=0 f(a+x)=\sum_{k=0}^nc_kx^k+x^nh(x). f n a f^{(k)}(a)=k!\,c_k\quad? n \mathbb F=\mathbb R \mathbb F=\mathbb Q 2^{\lfloor-1/x^2\rfloor} f(x)=\begin{cases}e^{-1/x^2},\quad x\in\mathbb Q\setminus\{0\}; \\ 0,\quad\text{otherwise};\end{cases} f(0+x)=0+0x+\cdots+0x^n+x^nh(x) \lim_{x\to0}h(x)=0 f''(0) f'(x) 0 f(x) 0 n \mathbb F=\mathbb Q_p f(x)=\begin{cases}p^{2m},\quad x=p^m+yp^{2m+1},\quad y\in\mathbb Z_p,\quad m\in\mathbb N; \\ 0,\quad\text{otherwise};\end{cases} f'(x)=0 x f^{(n)}(0)=0 n>1 \lim_{x\to0}f(x)/x^2 f 2 f(0+x)=0+0x+c_2x^2+x^2h(x) c_2","['analysis', 'derivatives', 'field-theory', 'taylor-expansion', 'p-adic-number-theory']"
35,How to find all polynomials $p \in \Bbb C [x]$ such that $x^3 p'(x) = p(x)^2$?,How to find all polynomials  such that ?,p \in \Bbb C [x] x^3 p'(x) = p(x)^2,"How to find all polynomials $p \in \Bbb C [x]$ such that $x^3 p'(x) = p(x)^2$ ? Since $\operatorname{degree}(p(x)) = y$ , and $\operatorname{degree}\left(p'(x)\right) = y-1$ , I tried to deduce what degree the polynomial should have. If $\operatorname{degree}(p(x))=2$ , then $\operatorname{degree}(p(x)^2)=4$ . And $\operatorname{degree}(p'(x))=1$ , then $\operatorname{degree}(x^3 p'(x))=4$ . So the polynomial should have degree $2$ . I'm not sure if this is the right approach. How can I complete the proof from this point?","How to find all polynomials such that ? Since , and , I tried to deduce what degree the polynomial should have. If , then . And , then . So the polynomial should have degree . I'm not sure if this is the right approach. How can I complete the proof from this point?",p \in \Bbb C [x] x^3 p'(x) = p(x)^2 \operatorname{degree}(p(x)) = y \operatorname{degree}\left(p'(x)\right) = y-1 \operatorname{degree}(p(x))=2 \operatorname{degree}(p(x)^2)=4 \operatorname{degree}(p'(x))=1 \operatorname{degree}(x^3 p'(x))=4 2,"['derivatives', 'polynomials', 'complex-numbers']"
36,"Let $F:[0,1] \rightarrow \mathbb{R} $ be continuous. Suppose $F$ is differentiable on (0,1), $F(0) = 0$ and $F(x)>0$ for all $x \in (0,1)$.","Let  be continuous. Suppose  is differentiable on (0,1),  and  for all .","F:[0,1] \rightarrow \mathbb{R}  F F(0) = 0 F(x)>0 x \in (0,1)","Let $F:[0,1] \rightarrow \mathbb{R} $ be continuous. Suppose $F$ is differentiable on (0,1), $F(0) = 0$ and $F(x)>0$ for all $x \in (0,1)$ . Prove that there exists $r,s \in (0,1)$ such that $r + s = 1$ and $8\frac{F'(r)}{F(r)} = 5\frac{F'(s)}{F(s)}$ It seems that this question requires the mean-value theorem. I considered a function G(x) = F(x)F(1-x) and proved the case $\frac{F'(r)}{F(r)} = \frac{F'(s)}{F(s)}$ , but for the case involving two different constants. I have no clue. Really thanks for any help or advice.","Let be continuous. Suppose is differentiable on (0,1), and for all . Prove that there exists such that and It seems that this question requires the mean-value theorem. I considered a function G(x) = F(x)F(1-x) and proved the case , but for the case involving two different constants. I have no clue. Really thanks for any help or advice.","F:[0,1] \rightarrow \mathbb{R}  F F(0) = 0 F(x)>0 x \in (0,1) r,s \in (0,1) r + s = 1 8\frac{F'(r)}{F(r)} = 5\frac{F'(s)}{F(s)} \frac{F'(r)}{F(r)} = \frac{F'(s)}{F(s)}","['real-analysis', 'calculus', 'analysis', 'derivatives', 'mean-value-theorem']"
37,Prove that $\frac{d}{dx}\left(3x^2+4\sqrt{x}\right) = 6x+\frac{2}{\sqrt{x}}$,Prove that,\frac{d}{dx}\left(3x^2+4\sqrt{x}\right) = 6x+\frac{2}{\sqrt{x}},"Prove that $\frac{d}{dx}\left(3x^2+4\sqrt{x}\right) = 6x+\frac{2}{\sqrt{x}}$ Note 2 things here please: I KNOW the differentiation rules, I KNOW that $\frac{d}{dx}\left(3x^2+4\sqrt{x}\right) = 6x+\frac{2}{\sqrt{x}}$ , but I'm having a trouble proving this with a certain method, you see I was training on the basic method -I don't really know what they call it- to get back to the origins of calculus, which is $\frac{d}{dx}f(x) = \lim_{h \to 0}\frac {f(x+h)-f(x)}{h}$ I've done the following: $$\lim_{h \to 0}\frac {3(x+h)^2+4\sqrt{x+h}-3x^2-4\sqrt{x}}{h}$$ which is so far correct, but I'm struggling to transform this to $6x+\frac{2}{\sqrt{x}}$ , I know the problem isn't hard but I wish that someone will be kind and tell me what the next step is.","Prove that Note 2 things here please: I KNOW the differentiation rules, I KNOW that , but I'm having a trouble proving this with a certain method, you see I was training on the basic method -I don't really know what they call it- to get back to the origins of calculus, which is I've done the following: which is so far correct, but I'm struggling to transform this to , I know the problem isn't hard but I wish that someone will be kind and tell me what the next step is.",\frac{d}{dx}\left(3x^2+4\sqrt{x}\right) = 6x+\frac{2}{\sqrt{x}} \frac{d}{dx}\left(3x^2+4\sqrt{x}\right) = 6x+\frac{2}{\sqrt{x}} \frac{d}{dx}f(x) = \lim_{h \to 0}\frac {f(x+h)-f(x)}{h} \lim_{h \to 0}\frac {3(x+h)^2+4\sqrt{x+h}-3x^2-4\sqrt{x}}{h} 6x+\frac{2}{\sqrt{x}},"['calculus', 'limits', 'derivatives']"
38,Find the value of $\ln\left(\frac{d^2y}{dx^2}\right)_{x=0}$,Find the value of,\ln\left(\frac{d^2y}{dx^2}\right)_{x=0},"If $xy=e-e^y$ then find the value of $\ln\left(\frac{d^2y}{dx^2}\right)_{x=0}$ . I found out that $y+\frac{dy}{dx}x=-e^y\frac{dy}{dx}$ and also $2\frac{dy}{dx}+\frac{d^2y}{dx^2}x=-e^y\frac{dy}{dx}\frac{dy}{dx}-\frac{d^2y}{dx^2}e^y$ ,or $\frac{d^2y}{dx^2}(x+e^y)=\frac{dy}{dx}\left(-e^y\frac{dy}{dx}-2\right).$ What to do next?","If then find the value of . I found out that and also ,or What to do next?",xy=e-e^y \ln\left(\frac{d^2y}{dx^2}\right)_{x=0} y+\frac{dy}{dx}x=-e^y\frac{dy}{dx} 2\frac{dy}{dx}+\frac{d^2y}{dx^2}x=-e^y\frac{dy}{dx}\frac{dy}{dx}-\frac{d^2y}{dx^2}e^y \frac{d^2y}{dx^2}(x+e^y)=\frac{dy}{dx}\left(-e^y\frac{dy}{dx}-2\right).,"['calculus', 'derivatives']"
39,A function with weakly positive $n$-th derivative has at most $n$ roots – an inequality version of the Fundamental theorem of Algebra.,A function with weakly positive -th derivative has at most  roots – an inequality version of the Fundamental theorem of Algebra.,n n,"This is an attempt to generalize the result in [1] . Claim: Let $n\in \mathbb N$ , and let $f:\mathbb R \to \mathbb R$ be such that its $n$ -th derivative $f^{(n)}(x)\geq 0, \ \forall x\in \mathbb R$ . Then the set $R$ of roots of $f$ either consists of at most of $n$ isolated points, or it is a (non-degenerate) closed interval. Note: As argued in [1] , under the stronger assumption $f^{(n)}(x)> 0$ the case of $R$ being a closed interval can be excluded. The analogy of this claim to the fundamental theorem of algebra (restricted to real numbers) is shown in [1] . Special cases: for $n=1$ the claim states that if a function has nonnegative derivative, it has a single root or a closed interval of roots, e.g. consider the function $f(x) = \min\{x+1,0\} + \max\{x-1,0\}$ for which $R = [-1,1]$ ; for $n=2$ the condition $f^{(n)}(x)\geq 0$ implies that $f$ is (weakly) convex, and example could be the function $f(x)=x^2-1$ which has two roots, or $$   f(x) = \begin{cases} 0, &\text{ if } x\in [-1,1] \\ (|x|-1)^2, &\text{otherwise}, \end{cases} $$ in which case $R= [-1,1]$ . The reason why I think that $[a,b]\subset R$ for some $a<b$ implies that $f$ has no isolated roots is that this implies that all the derivatives of $f$ are zero on $(a,b)$ , and so (assuming that the interval $[a,b]$ is maximal among the closed intervals contained in $R$ ) the derivatives $f^{(n-1)}(x),f^{(n-2)}(x),f'(x),f(x)$ are all positive for $x>b$ as $f^{(n)}(x)\geq 0$ for $x>b$ with inequality strict at $x=b+\varepsilon$ for $\varepsilon>0$ arbitrarily small. Does the claim hold?","This is an attempt to generalize the result in [1] . Claim: Let , and let be such that its -th derivative . Then the set of roots of either consists of at most of isolated points, or it is a (non-degenerate) closed interval. Note: As argued in [1] , under the stronger assumption the case of being a closed interval can be excluded. The analogy of this claim to the fundamental theorem of algebra (restricted to real numbers) is shown in [1] . Special cases: for the claim states that if a function has nonnegative derivative, it has a single root or a closed interval of roots, e.g. consider the function for which ; for the condition implies that is (weakly) convex, and example could be the function which has two roots, or in which case . The reason why I think that for some implies that has no isolated roots is that this implies that all the derivatives of are zero on , and so (assuming that the interval is maximal among the closed intervals contained in ) the derivatives are all positive for as for with inequality strict at for arbitrarily small. Does the claim hold?","n\in \mathbb N f:\mathbb R \to \mathbb R n f^{(n)}(x)\geq 0, \ \forall x\in \mathbb R R f n f^{(n)}(x)> 0 R n=1 f(x) = \min\{x+1,0\} + \max\{x-1,0\} R = [-1,1] n=2 f^{(n)}(x)\geq 0 f f(x)=x^2-1 
  f(x) = \begin{cases}
0, &\text{ if } x\in [-1,1] \\
(|x|-1)^2, &\text{otherwise},
\end{cases}
 R= [-1,1] [a,b]\subset R a<b f f (a,b) [a,b] R f^{(n-1)}(x),f^{(n-2)}(x),f'(x),f(x) x>b f^{(n)}(x)\geq 0 x>b x=b+\varepsilon \varepsilon>0","['real-analysis', 'derivatives', 'roots', 'rolles-theorem', 'real-algebraic-geometry']"
40,"If $\delta \colon R[x_1, \dotsc, x_r] \to M$ is an $R$-derivation, then $\delta P = \sum_{i = 1}^r \frac{\partial P}{\partial x_i} dx_i$","If  is an -derivation, then","\delta \colon R[x_1, \dotsc, x_r] \to M R \delta P = \sum_{i = 1}^r \frac{\partial P}{\partial x_i} dx_i","Let $\phi \colon R \rightarrow S$ be a ring homomorphism and $M$ an $S$ -module. An $R$ -derivation $\delta \colon S \rightarrow M$ is a $R$ -module homomorphism such that: $\phi(R) \subset \ker(\delta)$ , (Leibniz’ rule:) $\delta(s s') = s \delta s' + s' \delta s$ for all $s, s' \in S$ . Then, according to the exercises on my lecture notes, if $\delta \colon R[x_1, \dotsc, x_r] \rightarrow M$ is an $R$ -derivation, then $\delta P = \sum_{i = 1}^r \frac{\partial P}{\partial x_i} dx_i$ . I’m not totally sure what they mean by $dx_i$ here, since I think they don’t have defined the operator $d$ as far as I know. I guess it should be just the same as $\delta x_i$ ? I’d like to prove this. I thought of doing it by induction: for the case $r = 1$ , it is fairly easy to see using the above properties that $\delta P = \frac{\partial P}{\partial x} \delta x$ . Then, if we write a multivariable polynomial as $\sum_{j = 0}^k a_j x^j_{n + 1}$ , where $a_j \in R[x_1, \dotsc, x_n]$ , we may assume that the result holds for $n$ and try to prove it for $n + 1$ : $$   \delta P   = \delta\Biggl( \sum_{j = 0}^k a_j x^j_{n+1} \Biggr)   = \sum_{j = 0}^k \Bigl( a_j \delta(x_{n+1}^j) + \delta(a_j) x_{n+1}^j \Bigr) $$ … where we may use that $\delta(x_{n+1}^j) = j x_{n+1}^{j-1} \delta(x_{n+1})$ and that $\delta(a_j) = \sum_{i = 1}^n \frac{\partial{a_j}}{\partial{x_i}}dx_i$ (because of the induction hypothesis), but I don’t really know where to go from there. I guess it’s a matter of manipulating expressions with polynomials in several variables, but I’m also not entirely sure what the exercise expects me to prove (as I don’t know what “ $dx_i$ ” is).","Let be a ring homomorphism and an -module. An -derivation is a -module homomorphism such that: , (Leibniz’ rule:) for all . Then, according to the exercises on my lecture notes, if is an -derivation, then . I’m not totally sure what they mean by here, since I think they don’t have defined the operator as far as I know. I guess it should be just the same as ? I’d like to prove this. I thought of doing it by induction: for the case , it is fairly easy to see using the above properties that . Then, if we write a multivariable polynomial as , where , we may assume that the result holds for and try to prove it for : … where we may use that and that (because of the induction hypothesis), but I don’t really know where to go from there. I guess it’s a matter of manipulating expressions with polynomials in several variables, but I’m also not entirely sure what the exercise expects me to prove (as I don’t know what “ ” is).","\phi \colon R \rightarrow S M S R \delta \colon S \rightarrow M R \phi(R) \subset \ker(\delta) \delta(s s') = s \delta s' + s' \delta s s, s' \in S \delta \colon R[x_1, \dotsc, x_r] \rightarrow M R \delta P = \sum_{i = 1}^r \frac{\partial P}{\partial x_i} dx_i dx_i d \delta x_i r = 1 \delta P = \frac{\partial P}{\partial x} \delta x \sum_{j = 0}^k a_j x^j_{n + 1} a_j \in R[x_1, \dotsc, x_n] n n + 1 
  \delta P
  = \delta\Biggl( \sum_{j = 0}^k a_j x^j_{n+1} \Biggr)
  = \sum_{j = 0}^k \Bigl( a_j \delta(x_{n+1}^j) + \delta(a_j) x_{n+1}^j \Bigr)
 \delta(x_{n+1}^j) = j x_{n+1}^{j-1} \delta(x_{n+1}) \delta(a_j) = \sum_{i = 1}^n \frac{\partial{a_j}}{\partial{x_i}}dx_i dx_i","['abstract-algebra', 'derivatives', 'commutative-algebra', 'modules', 'differential-algebra']"
41,"Is there any function that has a derivative almost everywhere, but where the derivative function is everywhere discontinuous?","Is there any function that has a derivative almost everywhere, but where the derivative function is everywhere discontinuous?",,"This question asks whether there exists a function that has a derivative that is discontinuous everywhere. Is there any function that has a derivative almost everywhere, but where the derivative function is everywhere discontinuous, for the definition of continuity that takes into account only the inputs where the derivative is defined?","This question asks whether there exists a function that has a derivative that is discontinuous everywhere. Is there any function that has a derivative almost everywhere, but where the derivative function is everywhere discontinuous, for the definition of continuity that takes into account only the inputs where the derivative is defined?",,"['calculus', 'limits', 'derivatives', 'continuity']"
42,Is $\Vert\cdot\Vert_{L^2}$ Fréchet differentiable as a function on the Sobolev Space $H^1(\mathbb{R})$?,Is  Fréchet differentiable as a function on the Sobolev Space ?,\Vert\cdot\Vert_{L^2} H^1(\mathbb{R}),"Consider the function $\Vert\cdot\Vert_{L^2}:H^1(\mathbb{R})\to \mathbb{R}$ and a point $0\neq f\in H^1(\mathbb{R})$ . Is $\Vert\cdot\Vert_{L^2}$ Fréchet differentiable w.r.t. to the norm $\Vert\cdot\Vert_{H^1}$ ? I.e. is there a bounded linear operator $A:H^1(\mathbb{R})\to \mathbb{R}$ , s.t. \begin{equation}      \lim_{\Vert g\Vert_{H^1}\to 0} \frac{\left\lvert  \Vert f+g\Vert_{L^2} - \Vert f\Vert_{L^2} - Ag\right\rvert}{\Vert g\Vert_{H^1}} = 0? \end{equation}","Consider the function and a point . Is Fréchet differentiable w.r.t. to the norm ? I.e. is there a bounded linear operator , s.t.","\Vert\cdot\Vert_{L^2}:H^1(\mathbb{R})\to \mathbb{R} 0\neq f\in H^1(\mathbb{R}) \Vert\cdot\Vert_{L^2} \Vert\cdot\Vert_{H^1} A:H^1(\mathbb{R})\to \mathbb{R} \begin{equation}
     \lim_{\Vert g\Vert_{H^1}\to 0} \frac{\left\lvert  \Vert f+g\Vert_{L^2} - \Vert f\Vert_{L^2} - Ag\right\rvert}{\Vert g\Vert_{H^1}} = 0?
\end{equation}","['real-analysis', 'derivatives', 'sobolev-spaces']"
43,Implicit Differentiation $x^2 y^3 = 3$,Implicit Differentiation,x^2 y^3 = 3,So I've been trying to implicitly differentiate $x^2 y^3 = 3$ which if I use product rule I get $$ -\frac{2y}{3x} $$ but when I move $x^2$ to the right hand side and differentiate I get $$ -\frac{2}{x^3 y^2}  $$ which is completely different answer and I do not get what I did wrong. Help Please.,So I've been trying to implicitly differentiate which if I use product rule I get but when I move to the right hand side and differentiate I get which is completely different answer and I do not get what I did wrong. Help Please.,"x^2 y^3 = 3 
-\frac{2y}{3x}
 x^2 
-\frac{2}{x^3 y^2} 
","['calculus', 'derivatives', 'implicit-differentiation']"
44,prove that $|x^y - y^x| > 2$,prove that,|x^y - y^x| > 2,"Prove that for any integers $x\neq y ,x,y>2, |x^y-y^x|>2$ . I know that the function $f(x) = \dfrac{\ln x}{x}$ is increasing for $x<e$ and decreasing for $x > e$ . So $|x^y-y^x| = x^y - y^x \ge 1$ if $x < y$ . I was thinking of proving something involving ratios (e.g. $f(x)/f(y)$ for $x < y$ ). I could possibly consider the case where x and y are consecutive and I could assume WLOG that $y>x$ so that it suffices to prove $x^y > y^x+2$ for $y>x>2.$ The inequality seems to be fairly weak; even for the smallest possible values of x and y, $y=4,x=3,$ we have $x^y = y^x+17.$ I think the sequence $\dfrac{x^{x+1}}{(x+1)^x} = x(1-\dfrac{1}{x+1})^x$ is an increasing function of x, and one could prove this using derivatives. Source: A PuMAC 2008 problem.","Prove that for any integers . I know that the function is increasing for and decreasing for . So if . I was thinking of proving something involving ratios (e.g. for ). I could possibly consider the case where x and y are consecutive and I could assume WLOG that so that it suffices to prove for The inequality seems to be fairly weak; even for the smallest possible values of x and y, we have I think the sequence is an increasing function of x, and one could prove this using derivatives. Source: A PuMAC 2008 problem.","x\neq y ,x,y>2, |x^y-y^x|>2 f(x) = \dfrac{\ln x}{x} x<e x > e |x^y-y^x| = x^y - y^x \ge 1 x < y f(x)/f(y) x < y y>x x^y > y^x+2 y>x>2. y=4,x=3, x^y = y^x+17. \dfrac{x^{x+1}}{(x+1)^x} = x(1-\dfrac{1}{x+1})^x","['real-analysis', 'derivatives', 'inequality', 'contest-math']"
45,Derivative of capital Pi product,Derivative of capital Pi product,,"I wanted to find the derivative of this function at $x=6$ $$y= \prod_{i=1}^{10} (x-i) = (x-1)(x-2) \cdots (x-10) $$ without expanding all of the brackets, so I used the product rule to find a pattern. However, the resulting sum tells me that the derivative is zero at every whole number which is obviously not true. I've been over my solution and I can't see how I've gone wrong. Please could someone highlight where I went wrong? Thank you in advance. \begin{align*}       \frac{\textit{d}y}{dx} &= (x-2)(x-3) \cdots (x-10) + (x-1) \frac{d}{dx} \biggl((x-2) \cdots (x-10) \biggr) \\        &= \prod_{i=2}^{10} (x-i) +  (x-1) \frac{d}{dx} \biggl(\prod_{i=2}^{10} (x-i) \biggr) \\        &= \prod_{i=2}^{10} (x-i) +  (x-1)\prod_{i=3}^{10} (x-i) + (x-1)(x-2)\frac{d}{dx} \biggl(\prod_{i=3}^{10} (x-i) \biggr) \\                 &= \prod_{i=2}^{10} (x-i) +  (x-1)\prod_{i=3}^{10} (x-i) + (x-1)(x-2)\biggl(\prod_{i=4}^{10} (x-i) \biggr) + \cdots \\                       &= \frac{y}{x-1} + \frac{y}{x-2} + \frac{y}{x-3}+\cdots + \frac{y}{x-10} \\                              &= \sum_{i=1}^{10} \biggl(\frac{y}{x-i}\biggr)     \end{align*}","I wanted to find the derivative of this function at without expanding all of the brackets, so I used the product rule to find a pattern. However, the resulting sum tells me that the derivative is zero at every whole number which is obviously not true. I've been over my solution and I can't see how I've gone wrong. Please could someone highlight where I went wrong? Thank you in advance.","x=6 y= \prod_{i=1}^{10} (x-i) = (x-1)(x-2) \cdots (x-10)  \begin{align*}
      \frac{\textit{d}y}{dx} &= (x-2)(x-3) \cdots (x-10) + (x-1) \frac{d}{dx} \biggl((x-2) \cdots (x-10) \biggr) \\
       &= \prod_{i=2}^{10} (x-i) +  (x-1) \frac{d}{dx} \biggl(\prod_{i=2}^{10} (x-i) \biggr) \\
       &= \prod_{i=2}^{10} (x-i) +  (x-1)\prod_{i=3}^{10} (x-i) + (x-1)(x-2)\frac{d}{dx} \biggl(\prod_{i=3}^{10} (x-i) \biggr) \\
                &= \prod_{i=2}^{10} (x-i) +  (x-1)\prod_{i=3}^{10} (x-i) + (x-1)(x-2)\biggl(\prod_{i=4}^{10} (x-i) \biggr) + \cdots \\
                      &= \frac{y}{x-1} + \frac{y}{x-2} + \frac{y}{x-3}+\cdots + \frac{y}{x-10} \\
                             &= \sum_{i=1}^{10} \biggl(\frac{y}{x-i}\biggr)
    \end{align*}","['calculus', 'derivatives', 'products']"
46,"Spivak, Ch. 20, Problem 9c: Is there a Typo in this question item or not?","Spivak, Ch. 20, Problem 9c: Is there a Typo in this question item or not?",,"The following problem is from Chapter 20 of Spivak's Calculus . I've asked a separate question regarding the comment at the end of item $(c)$ . In the accepted answer to that question, the author of the response claims that item $(c)$ itself has a typographical error. I had already come up with a proof for that item, however, and my question now is if such a proof is incorrect. In other words, was the author of the aforementioned answer correct in his claim. (a) Problem $7(i)$ amounts to the equation $$P_{n,a,f+g}=P_{n,a,f}+P_{n,a,g}$$ Give a more direct proof by writing $$f(x)=P_{n,a,f}(x)+R_{n,a,f}(x)\tag{1}$$ $$g(x)=P_{n,a,g}(x)+R_{n,a,g}(x)\tag{2}$$ and using the obvious fact about $R_{n,a,f}+R_{n,a,g}$ . (b) Similarly, Problem $7(ii)$ could be used to show that $$P_{n,a,fg}=[P_{n,a,f}\cdot P_{n,a,g}]_n$$ where $[P]_n$ denotes the truncation of $P$ to degree $n$ , the sum of all terms of $P$ of degree $\leq n$ [with $P$ written as a polynomial in $x-a$ ]. Again, give a more direct proof, using the obvious facts about products involving terms of the form $R_n$ . (c) Prove that if $p$ and $q$ are polynomials in $x-a$ and $\lim\limits_{x\to 0} \frac{R(x)}{(x-a)^n}=0$ then $$p(q(x)+R(x))=p(q(x))+\bar{R}(x)$$ where $$\lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=0$$ The claimed typo seems to be that $x\to a$ in the limits instead of $x\to 0$ . My attempt at a proof $$p(x)=\sum\limits_{i=1}^n a_i(x-a)^i$$ $$p(q(x)+R(x))=\sum\limits_{i=0}^n a_i(q(x)+R(x)-a)^i=\sum\limits_{i=0}^n a_i \sum\limits_{j=0}^i \binom{i}{j} (q(x)-a)^{i-j} R(x)^j$$ $$=\sum\limits_{i=0}^n a_i(q(x)-a)^i + \sum\limits_{i=0}^n a_i \sum\limits_{j=1}^i \binom{i}{j}(q(x)-a)^{i-j}R(x)^j$$ $$=p(q(x))+\bar{R(x)}$$ where $\bar{R}(x)=\sum\limits_{i=0}^n a_i \sum\limits_{j=1}^i \binom{i}{j}(q(x)-a)^{i-j}R(x)^j$ Note that $(q(x)-a)^{i-j}$ is polynomial, hence continuous everywhere and $\lim\limits_{x\to 0} (q(x)-a)^{i-j}$ exists. Thus, $\lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=\sum\limits_{i=0}^n a_i \sum\limits_{j=1}^i \binom{i}{j} \cdot \lim\limits_{x\to 0} (q(x)-a)^{i-j} \cdot \frac{R(x)^j}{(x-a)^n}=0$ Is this proof correct?","The following problem is from Chapter 20 of Spivak's Calculus . I've asked a separate question regarding the comment at the end of item . In the accepted answer to that question, the author of the response claims that item itself has a typographical error. I had already come up with a proof for that item, however, and my question now is if such a proof is incorrect. In other words, was the author of the aforementioned answer correct in his claim. (a) Problem amounts to the equation Give a more direct proof by writing and using the obvious fact about . (b) Similarly, Problem could be used to show that where denotes the truncation of to degree , the sum of all terms of of degree [with written as a polynomial in ]. Again, give a more direct proof, using the obvious facts about products involving terms of the form . (c) Prove that if and are polynomials in and then where The claimed typo seems to be that in the limits instead of . My attempt at a proof where Note that is polynomial, hence continuous everywhere and exists. Thus, Is this proof correct?","(c) (c) 7(i) P_{n,a,f+g}=P_{n,a,f}+P_{n,a,g} f(x)=P_{n,a,f}(x)+R_{n,a,f}(x)\tag{1} g(x)=P_{n,a,g}(x)+R_{n,a,g}(x)\tag{2} R_{n,a,f}+R_{n,a,g} 7(ii) P_{n,a,fg}=[P_{n,a,f}\cdot P_{n,a,g}]_n [P]_n P n P \leq n P x-a R_n p q x-a \lim\limits_{x\to 0} \frac{R(x)}{(x-a)^n}=0 p(q(x)+R(x))=p(q(x))+\bar{R}(x) \lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=0 x\to a x\to 0 p(x)=\sum\limits_{i=1}^n a_i(x-a)^i p(q(x)+R(x))=\sum\limits_{i=0}^n a_i(q(x)+R(x)-a)^i=\sum\limits_{i=0}^n a_i \sum\limits_{j=0}^i \binom{i}{j} (q(x)-a)^{i-j} R(x)^j =\sum\limits_{i=0}^n a_i(q(x)-a)^i + \sum\limits_{i=0}^n a_i \sum\limits_{j=1}^i \binom{i}{j}(q(x)-a)^{i-j}R(x)^j =p(q(x))+\bar{R(x)} \bar{R}(x)=\sum\limits_{i=0}^n a_i \sum\limits_{j=1}^i \binom{i}{j}(q(x)-a)^{i-j}R(x)^j (q(x)-a)^{i-j} \lim\limits_{x\to 0} (q(x)-a)^{i-j} \lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=\sum\limits_{i=0}^n a_i \sum\limits_{j=1}^i \binom{i}{j} \cdot \lim\limits_{x\to 0} (q(x)-a)^{i-j} \cdot \frac{R(x)^j}{(x-a)^n}=0","['calculus', 'integration', 'derivatives', 'solution-verification', 'taylor-expansion']"
47,Does the sequence of iterated derivatives of $2^x$ converge uniformly to the zero function?,Does the sequence of iterated derivatives of  converge uniformly to the zero function?,2^x,"Consider the function $2^x$ . As you keep taking derivatives of that function over and over again, it converges to the zero function, at least pointwise. My question is, does it converge uniformly to the zero function on the entire real line? I used the example of $2^x$ , but I could have used any exponential function $b^x$ for a number $b$ between $1$ and $e$ .","Consider the function . As you keep taking derivatives of that function over and over again, it converges to the zero function, at least pointwise. My question is, does it converge uniformly to the zero function on the entire real line? I used the example of , but I could have used any exponential function for a number between and .",2^x 2^x b^x b 1 e,"['real-analysis', 'derivatives']"
48,Why is the antiderivative of $\dfrac{1}{x}=\ln(x)$? [duplicate],Why is the antiderivative of ? [duplicate],\dfrac{1}{x}=\ln(x),This question already has an answer here : Justify the fact that antiderivative of $\frac{1}{x}$ is $\ln |x|$ not $\ln x$ (1 answer) Closed 1 year ago . I understand that the derivative of $\ln(x)$ is equal to $\dfrac{1}{x}$ . What I don't get is why the antiderivative of $1/x$ is $\ln(|x|)$ instead of $\ln(x)$ . Why is the absolute value sign necessary? I would like a simple explanation as I am new to calculus. Thanks!,This question already has an answer here : Justify the fact that antiderivative of $\frac{1}{x}$ is $\ln |x|$ not $\ln x$ (1 answer) Closed 1 year ago . I understand that the derivative of is equal to . What I don't get is why the antiderivative of is instead of . Why is the absolute value sign necessary? I would like a simple explanation as I am new to calculus. Thanks!,\ln(x) \dfrac{1}{x} 1/x \ln(|x|) \ln(x),"['calculus', 'derivatives']"
49,"$\alpha(x)=\int_0^x (1+t^2)^{-1}$, $\tan(x)=\alpha^{-1}(x)$, $\sin{x}=\frac{\tan{x}}{\sqrt{1+\tan{x}^2}}$. Prove $\lim\limits_{x\to\pi/2^-}\sin{x}=1$.",", , . Prove .",\alpha(x)=\int_0^x (1+t^2)^{-1} \tan(x)=\alpha^{-1}(x) \sin{x}=\frac{\tan{x}}{\sqrt{1+\tan{x}^2}} \lim\limits_{x\to\pi/2^-}\sin{x}=1,"Imagine we know nothing about trigonometric functions and we define the following function $$\alpha(x)=\int_0^x (1+t^2)^{-1}$$ It can be shown that $\alpha'(x)=(1+x^2)^{-1}>0$ , so $\alpha$ is increasing. therefore, $\alpha^{-1}$ is defined on the image of $\alpha$ which is $(-\pi/2,\pi/2)$ $\alpha$ is odd $\lim\limits_{x\to\infty} \alpha(x)$ and $\lim\limits_{x\to-\infty} \alpha(x)$ exist If we define $$\pi=2\lim\limits_{x\to\infty} \alpha(x)$$ then $\lim\limits_{x\to\infty} \alpha(x)=-\lim\limits_{x\to-\infty} \alpha(x)=\frac{\pi}{2}$ Now let's define the following functions $$\tan(x)=\alpha^{-1}(x)$$ $$\sin{x}=\frac{\tan{x}}{\sqrt{1+(\tan{x})^2}}$$ Let's say we want to show that $\lim\limits_{x\to \frac{\pi}{2}^-}\sin{x}=1$ . We will need the following result $$(\alpha^{-1})'(x)=\frac{1}{\alpha'(\alpha^{-1}(x))}=1+[\alpha^{-1}(x)]^2$$ Then $$\lim\limits_{x\to \frac{\pi}{2}^-} \sin{x}=\lim\limits_{x\to \frac{\pi}{2}^-} \frac{\tan{x}}{\sqrt{1+(\tan{x})^2}}$$ Now, the first thing I am unsure of is how to show, in the context of this problem, that $$\lim\limits_{x\to \frac{\pi}{2}^-} \tan{x}=\lim\limits_{x\to \frac{\pi}{2}^-} \alpha^{-1}(x)=\infty$$ Assuming we can show it, then $$\lim\limits_{x\to \frac{\pi}{2}^-} \sin{x}=\frac{\infty}{\infty}$$ and by L'Hopital $$=\lim\limits_{x\to \frac{\pi}{2}^-} \frac{1+[\alpha^{-1}(x)]^2}{\frac{\alpha^{-1}(x)(\alpha^{-1})'(x)}{\sqrt{1+[\alpha'(x)]^2}}}$$ $$=\lim\limits_{x\to \frac{\pi}{2}^-} \frac{1}{\sin{x}}$$ That is, we have $$\lim\limits_{x\to \frac{\pi}{2}^-} \sin{x}=\lim\limits_{x\to \frac{\pi}{2}^-} \frac{1}{\sin{x}}$$ At this point, the solution manual says ""so the limit is $\pm 1$ . What is it that allows us to conclude this latter step?","Imagine we know nothing about trigonometric functions and we define the following function It can be shown that , so is increasing. therefore, is defined on the image of which is is odd and exist If we define then Now let's define the following functions Let's say we want to show that . We will need the following result Then Now, the first thing I am unsure of is how to show, in the context of this problem, that Assuming we can show it, then and by L'Hopital That is, we have At this point, the solution manual says ""so the limit is . What is it that allows us to conclude this latter step?","\alpha(x)=\int_0^x (1+t^2)^{-1} \alpha'(x)=(1+x^2)^{-1}>0 \alpha \alpha^{-1} \alpha (-\pi/2,\pi/2) \alpha \lim\limits_{x\to\infty} \alpha(x) \lim\limits_{x\to-\infty} \alpha(x) \pi=2\lim\limits_{x\to\infty} \alpha(x) \lim\limits_{x\to\infty} \alpha(x)=-\lim\limits_{x\to-\infty} \alpha(x)=\frac{\pi}{2} \tan(x)=\alpha^{-1}(x) \sin{x}=\frac{\tan{x}}{\sqrt{1+(\tan{x})^2}} \lim\limits_{x\to \frac{\pi}{2}^-}\sin{x}=1 (\alpha^{-1})'(x)=\frac{1}{\alpha'(\alpha^{-1}(x))}=1+[\alpha^{-1}(x)]^2 \lim\limits_{x\to \frac{\pi}{2}^-} \sin{x}=\lim\limits_{x\to \frac{\pi}{2}^-} \frac{\tan{x}}{\sqrt{1+(\tan{x})^2}} \lim\limits_{x\to \frac{\pi}{2}^-} \tan{x}=\lim\limits_{x\to \frac{\pi}{2}^-} \alpha^{-1}(x)=\infty \lim\limits_{x\to \frac{\pi}{2}^-} \sin{x}=\frac{\infty}{\infty} =\lim\limits_{x\to \frac{\pi}{2}^-} \frac{1+[\alpha^{-1}(x)]^2}{\frac{\alpha^{-1}(x)(\alpha^{-1})'(x)}{\sqrt{1+[\alpha'(x)]^2}}} =\lim\limits_{x\to \frac{\pi}{2}^-} \frac{1}{\sin{x}} \lim\limits_{x\to \frac{\pi}{2}^-} \sin{x}=\lim\limits_{x\to \frac{\pi}{2}^-} \frac{1}{\sin{x}} \pm 1","['calculus', 'integration', 'limits', 'derivatives', 'proof-explanation']"
50,Calculation of the Acceleration of a Stepper Motor,Calculation of the Acceleration of a Stepper Motor,,I'm working on a stepper motor as part of my studies and while I was doing research I came across this paper I tried to walk my way through the equations and tried to find the proof for the Eq. 13. But I'm having a hard time going from Eq. 2 to Eq. 3. The author calculated the acceleration between c1 and c2. The derivation of the speed gives us the acceleration. But since we don't have the function as a variable of time I cannot calculate the derivative from the equation. Also I tried to calculate it from the change on the axes but it doesn't have the information on the time axes. This is the graph for the velocity against time The paper gives the velocity equation as: $$w = \frac{αf }{c}$$ It doesn't define the velocity as a function of time. And we need the get to the equation 3 the derivative of w. $$w' = \frac{2αf^2 (c_1-c_2) }{c_1c_2(c_1+c_2)}$$ I appreciate all your help me making this calculation. Thank you.,I'm working on a stepper motor as part of my studies and while I was doing research I came across this paper I tried to walk my way through the equations and tried to find the proof for the Eq. 13. But I'm having a hard time going from Eq. 2 to Eq. 3. The author calculated the acceleration between c1 and c2. The derivation of the speed gives us the acceleration. But since we don't have the function as a variable of time I cannot calculate the derivative from the equation. Also I tried to calculate it from the change on the axes but it doesn't have the information on the time axes. This is the graph for the velocity against time The paper gives the velocity equation as: It doesn't define the velocity as a function of time. And we need the get to the equation 3 the derivative of w. I appreciate all your help me making this calculation. Thank you.,w = \frac{αf }{c} w' = \frac{2αf^2 (c_1-c_2) }{c_1c_2(c_1+c_2)},"['calculus', 'derivatives']"
51,"Getting the area between $y$-axis, $f(x)$, $f(2-x)$ when both function is given by their subtraction?","Getting the area between -axis, ,  when both function is given by their subtraction?",y f(x) f(2-x),"For a polynomial $f(x)$ , let function $g(x) = f(x) - f(2-x)$ . $g'(x) = 24x^2 - 48x + 50$ What is the area between $y=f(x)$ , $y=f(2-x)$ , and $y$ -axis? My approach: $g(1) = f(1) - f(1) = 0$ . From $g'(x)$ , $g(x) = 8x^3 - 24x^2 + 50x + C$ . From 1 and 2, $C = -34$ . Since $f(x)$ is a cubic function, let $f(x) = ax^3 + bx^2 + cx + d$ and compare coefficients of $g$ and $f(x) - f(2-x)$ . $a = 4$ , $2b + c = 1$ . And I'm stuck. I can't see how I can get more info about $f(x)$ using these conditions and get areas out of it.","For a polynomial , let function . What is the area between , , and -axis? My approach: . From , . From 1 and 2, . Since is a cubic function, let and compare coefficients of and . , . And I'm stuck. I can't see how I can get more info about using these conditions and get areas out of it.",f(x) g(x) = f(x) - f(2-x) g'(x) = 24x^2 - 48x + 50 y=f(x) y=f(2-x) y g(1) = f(1) - f(1) = 0 g'(x) g(x) = 8x^3 - 24x^2 + 50x + C C = -34 f(x) f(x) = ax^3 + bx^2 + cx + d g f(x) - f(2-x) a = 4 2b + c = 1 f(x),"['derivatives', 'definite-integrals', 'cubics']"
52,"$f$ integrable on $[a,b]$, differentiable at $c\in (a,b)$. $F(x)=\int_a^x f$. Out of the three ways $F'$ can be discont. at $c$, which are possible?","integrable on , differentiable at . . Out of the three ways  can be discont. at , which are possible?","f [a,b] c\in (a,b) F(x)=\int_a^x f F' c","Assume a function $f$ is integrable on $[a,b]$ , differentiable at a point $c\in (a,b),$ and define $$F(x)=\int_a^x f$$ Let's investigate the theoretical conditions under which $F'$ could be discontinuous at $c$ . Since $f$ is differentiable at $c$ , this implies $f$ is continuous at $c$ . By the first fundamental theorem of calculus (FTC1) applied to $f$ on $[a,b]$ , we have that $F$ is differentiable at $c \in (a,b)$ and $F'(c)=f(c)$ . Let's recall the definition of continuity Spivak, Ch. 6, Definition: A function $f$ is continuous at $c$ if $$\lim\limits_{x\to c} f(x)=f(c)\tag{1}$$ What are the ways a function can be discontinuous at a point $c$ ? $f$ isn't defined at $c$ , hence $f(c)$ isn't defined and so $(1)$ doesn't make sense and is false $\lim\limits_{x\to c} f(x)$ does not exist, so again $(1)$ doesn't make sense and is false $f$ is defined at $c$ and the limit above does exist but it differs from $f(c)$ . Which of the above scenarios is possible in our setup of $f$ and $F$ , for $F'$ to be discontinuous at $c$ ? Here is what I came up with In the case of our function $F$ , it is differentiable wherever $f$ is continuous. If $f$ were to be continuous in an interval around $c$ , then $F$ would be differentiable and hence continuous on that interval, including at $c$ . Therefore, $f$ has to be discontinuous on every interval containing $c$ . We've established that $F'(c)=f(c)$ so $F'$ is defined at $c$ and hence option 1. above is not possible. If 3. were true, then we'd have $$\lim\limits_{x\to c} F'(x)\neq F'(c)=f(c)\tag{2}$$ Here is a line of reasoning I am unsure about Since the limit above exists, then $F'$ is defined everywhere around $c$ . Is this true? Ie, when we defined a limit $$\lim\limits_{x\to c} f(x)$$ as $$\forall \epsilon>0\ \exists \delta>0\ \forall x, |x-c|<\delta\implies |f(x)-f(c)|<\epsilon$$ Did the $\forall x$ mean literally for every single real number within $\delta$ of $c$ , or just the $x$ where $f$ is defined? if the above is indeed true, then we have the following happening (i) $F'$ defined in an interval containing $c$ (ii) since $f$ is integrable on $[a,b]$ , $F$ continuous on $[a,b]$ , and in particular at $c$ (iii) $\lim\limits_{x\to c} F'$ exists I believe the following theorem is thus applicable Spivak, Ch. 11, Theorem 7: Suppose $F$ is continuous at $c$ , $F'(x)$ exists for all $x$ in some interval containing $c$ , except possibly at $x=c$ . Suppose, moreover, that $\lim\limits_{x\to c} F'(x)$ exists. Then, $F'(c)$ exists and $F'(c)=\lim\limits_{x\to c} F'(x)$ This theorem basically says that if a function is continuous at a point, then its derivative cannot have a jump discontinuity there. Applying to $F$ , we conclude that $F'(c)=\lim\limits_{x\to c} F'(x)$ , which contradicts $(2)$ , and hence scenario 3. isn't possible. Which leaves only scenario 2., which is possible by virtue of the fact that I can come up with an example for it: $$f(x)=\begin{cases} 0, \text{ if } x \in \left (\frac{1}{n+1},\frac{1}{n} \right ] \\ x^2, \text{ if } x \in \left (\frac{1}{n},\frac{1}{n-1} \right ] \end{cases}$$ for $n\in\mathbb{N}$ . Edit For the record I thought I'd mention this is a problem from Spivak's Calculus and there is a (terse solution) that isn't very didactical. Here it is If we assume that $f$ is continuous in an interval around $c$ , then $F'$ will be continuous at $c$ , since we will have $F'(x)=f(x)$ in this interval, and differentiability of $f$ at $c$ implies continuity of $f$ at $c$ . But without this assumption $F'$ may not even be exist at all points near $c$ . For example, $f$ could be the function shown below","Assume a function is integrable on , differentiable at a point and define Let's investigate the theoretical conditions under which could be discontinuous at . Since is differentiable at , this implies is continuous at . By the first fundamental theorem of calculus (FTC1) applied to on , we have that is differentiable at and . Let's recall the definition of continuity Spivak, Ch. 6, Definition: A function is continuous at if What are the ways a function can be discontinuous at a point ? isn't defined at , hence isn't defined and so doesn't make sense and is false does not exist, so again doesn't make sense and is false is defined at and the limit above does exist but it differs from . Which of the above scenarios is possible in our setup of and , for to be discontinuous at ? Here is what I came up with In the case of our function , it is differentiable wherever is continuous. If were to be continuous in an interval around , then would be differentiable and hence continuous on that interval, including at . Therefore, has to be discontinuous on every interval containing . We've established that so is defined at and hence option 1. above is not possible. If 3. were true, then we'd have Here is a line of reasoning I am unsure about Since the limit above exists, then is defined everywhere around . Is this true? Ie, when we defined a limit as Did the mean literally for every single real number within of , or just the where is defined? if the above is indeed true, then we have the following happening (i) defined in an interval containing (ii) since is integrable on , continuous on , and in particular at (iii) exists I believe the following theorem is thus applicable Spivak, Ch. 11, Theorem 7: Suppose is continuous at , exists for all in some interval containing , except possibly at . Suppose, moreover, that exists. Then, exists and This theorem basically says that if a function is continuous at a point, then its derivative cannot have a jump discontinuity there. Applying to , we conclude that , which contradicts , and hence scenario 3. isn't possible. Which leaves only scenario 2., which is possible by virtue of the fact that I can come up with an example for it: for . Edit For the record I thought I'd mention this is a problem from Spivak's Calculus and there is a (terse solution) that isn't very didactical. Here it is If we assume that is continuous in an interval around , then will be continuous at , since we will have in this interval, and differentiability of at implies continuity of at . But without this assumption may not even be exist at all points near . For example, could be the function shown below","f [a,b] c\in (a,b), F(x)=\int_a^x f F' c f c f c f [a,b] F c \in (a,b) F'(c)=f(c) f c \lim\limits_{x\to c} f(x)=f(c)\tag{1} c f c f(c) (1) \lim\limits_{x\to c} f(x) (1) f c f(c) f F F' c F f f c F c f c F'(c)=f(c) F' c \lim\limits_{x\to c} F'(x)\neq F'(c)=f(c)\tag{2} F' c \lim\limits_{x\to c} f(x) \forall \epsilon>0\ \exists \delta>0\ \forall x, |x-c|<\delta\implies |f(x)-f(c)|<\epsilon \forall x \delta c x f F' c f [a,b] F [a,b] c \lim\limits_{x\to c} F' F c F'(x) x c x=c \lim\limits_{x\to c} F'(x) F'(c) F'(c)=\lim\limits_{x\to c} F'(x) F F'(c)=\lim\limits_{x\to c} F'(x) (2) f(x)=\begin{cases} 0, \text{ if } x \in \left (\frac{1}{n+1},\frac{1}{n} \right ] \\ x^2, \text{ if } x \in \left (\frac{1}{n},\frac{1}{n-1} \right ] \end{cases} n\in\mathbb{N} f c F' c F'(x)=f(x) f c f c F' c f","['calculus', 'integration', 'limits', 'derivatives']"
53,Characterize subdifferential of a convex function by directional derivative,Characterize subdifferential of a convex function by directional derivative,,"This thread is meant to record a question that I feel interesting during my self-study. I'm very happy to receive your suggestion and comments. See: SE blog: Answer own Question and MSE meta: Answer own Question . Let $A$ be a subset of a normed space $X$ and $f:A \to \mathbb R$ . Let $a \in \operatorname{int} A$ . For $v \in X$ , the right directional derivative $f_{+}^{\prime}(a)[v]$ , the left directional derivative $f_{-}^{\prime}(a)[v]$ , and the ( bilateral ) directional derivative $f^{\prime}(a)[v]$ are defined by: $$ \begin{aligned} f_{+}^{\prime}(a)[v] &= \lim _{t \to 0^+} \frac{f(a+t v)-f(a)}{t} \\ f_{-}^{\prime}(a)[v] &= \lim _{t \to 0^-} \frac{f(a+t v)-f(a)}{t} \\ f^{\prime}(a)[v] &= \lim _{t \to 0} \frac{f(a+t v)-f(a)}{t}. \end{aligned} $$ We say that $f$ is Gâteaux differentiable at $a$ if $f^{\prime}(a) \in X^{*}$ . The subdifferential of $f$ at $a \in A$ is the set $$ \partial f(a)=\left\{x^* \in X^* \mid f(x) - f(a) \ge \langle x^*, x-a \rangle \text { for each } x \in A\right\}. $$ The elements of $\partial f(a)$ are called subgradients of $f$ at $a$ . Theorem: Assume $A$ is open convex and $f$ convex. For $a\in A$ and $x^* \in X^*$ , the following assertions are equivalent: (i) $x^* \in \partial f(a)$ ; (ii) $x^*(v) \leq f_{+}^{\prime}(a)[v]$ for each $v \in X$ ; (iii) $f_-^{\prime}(a)[v] \leq x^*(v) \leq f_{+}^{\prime}(a)[v]$ for each $v \in X$ . As a corollary, we obtain that $\partial f(a)$ is fully determined by the values of $f$ in any neighborhood of $a$ .","This thread is meant to record a question that I feel interesting during my self-study. I'm very happy to receive your suggestion and comments. See: SE blog: Answer own Question and MSE meta: Answer own Question . Let be a subset of a normed space and . Let . For , the right directional derivative , the left directional derivative , and the ( bilateral ) directional derivative are defined by: We say that is Gâteaux differentiable at if . The subdifferential of at is the set The elements of are called subgradients of at . Theorem: Assume is open convex and convex. For and , the following assertions are equivalent: (i) ; (ii) for each ; (iii) for each . As a corollary, we obtain that is fully determined by the values of in any neighborhood of .","A X f:A \to \mathbb R a \in \operatorname{int} A v \in X f_{+}^{\prime}(a)[v] f_{-}^{\prime}(a)[v] f^{\prime}(a)[v] 
\begin{aligned}
f_{+}^{\prime}(a)[v] &= \lim _{t \to 0^+} \frac{f(a+t v)-f(a)}{t} \\
f_{-}^{\prime}(a)[v] &= \lim _{t \to 0^-} \frac{f(a+t v)-f(a)}{t} \\
f^{\prime}(a)[v] &= \lim _{t \to 0} \frac{f(a+t v)-f(a)}{t}.
\end{aligned}
 f a f^{\prime}(a) \in X^{*} f a \in A 
\partial f(a)=\left\{x^* \in X^* \mid f(x) - f(a) \ge \langle x^*, x-a \rangle \text { for each } x \in A\right\}.
 \partial f(a) f a A f a\in A x^* \in X^* x^* \in \partial f(a) x^*(v) \leq f_{+}^{\prime}(a)[v] v \in X f_-^{\prime}(a)[v] \leq x^*(v) \leq f_{+}^{\prime}(a)[v] v \in X \partial f(a) f a","['functional-analysis', 'derivatives', 'normed-spaces', 'convex-analysis']"
54,How to prove $s > \frac{11}{2}t-3t\ln t$ in this problem,How to prove  in this problem,s > \frac{11}{2}t-3t\ln t,"The problem is: Given $f(x)=\frac{\ln x}{x}$ , line $l$ is the tangent of curve $y=f(x)$ at $(t,f(t))$ ,  and intersects the curve at another point $(s,f(s))$ where $s<t$ . (1) Find the range of $t$ ; (2) (i) Prove $\ln x\le 1 +\frac{1}{e}(x-e)-\frac{1}{2e^2}(x-e)^2+\frac{1}{3e^3}(x-e)^3$ ; $\quad$ (ii) Prove $s>\frac{11}{2}t-3t\ln t$ . Now I have solved the problem (1) and (2)(i), but I can't figure out the proof of (2)(ii). In problem (1) we can get $t\in(e^{3/2},+\infty)$ , and the intersection is the null point of the function $F(x)=\frac{\ln x}{x}-\frac{1-\ln t}{t^2}(x-t)-\frac{\ln t}{t}$ . Therefore, we have: $\color{red}{\frac{\ln s}{s}-\frac{1-\ln t}{t^2}(s-t)-\frac{\ln t}{t}=0}$ , where $s<t$ and $t>e^{3/2}$ . Then we intend to prove $s>\frac{11}{2}t-3t\ln t$ . Maybe the inequality in (2)(i) is helpful for enlarging and reducing the inequality, but I can't find how to use it properly.","The problem is: Given , line is the tangent of curve at ,  and intersects the curve at another point where . (1) Find the range of ; (2) (i) Prove ; (ii) Prove . Now I have solved the problem (1) and (2)(i), but I can't figure out the proof of (2)(ii). In problem (1) we can get , and the intersection is the null point of the function . Therefore, we have: , where and . Then we intend to prove . Maybe the inequality in (2)(i) is helpful for enlarging and reducing the inequality, but I can't find how to use it properly.","f(x)=\frac{\ln x}{x} l y=f(x) (t,f(t)) (s,f(s)) s<t t \ln x\le 1 +\frac{1}{e}(x-e)-\frac{1}{2e^2}(x-e)^2+\frac{1}{3e^3}(x-e)^3 \quad s>\frac{11}{2}t-3t\ln t t\in(e^{3/2},+\infty) F(x)=\frac{\ln x}{x}-\frac{1-\ln t}{t^2}(x-t)-\frac{\ln t}{t} \color{red}{\frac{\ln s}{s}-\frac{1-\ln t}{t^2}(s-t)-\frac{\ln t}{t}=0} s<t t>e^{3/2} s>\frac{11}{2}t-3t\ln t","['derivatives', 'inequality']"
55,"Can I prove that $f$ convex $\implies$ $f$ continuous using a proof by all possible cases of discontinuity, showing contradiction in each one?","Can I prove that  convex   continuous using a proof by all possible cases of discontinuity, showing contradiction in each one?",f \implies f,"Given the following definitions of a convex function from Spivak's Calculus: Definition 1: A function $f$ is convex on an interval, if for all $a$ and $b$ in the interval, the line segment joining $(a,f(a))$ and $(b,f(b))$ lies above the graph of $f$ . Definition 2: A function $f$ is convex on an interval if for all $a, x$ , and $b$ in the interval with $a<x<b$ we have $\frac{f(x)-f(a)}{x-a}<\frac{f(b)-f(a)}{b-a}$ . First question: Does Definition 2 mean that $f$ has to be defined everywhere in an interval to be convex in that interval? Second question: Is it possible to prove that such a function is always continuous? If so, how? Here is my attempt at a proof. I'm going to outline the proofs, without actually proving using $\epsilon$ and $\delta$ proofs as would be required to be rigorous. If the general idea of each case below is sound, then I think the corresponding rigorous proofs are relatively simple. Let $f$ be convex. Assume $f$ is discontinuous at some point $a$ , ie $\lim\limits_{x \to a} f(x) \neq f(a)$ . My proof strategy is to go through each of the possible ways that $f$ could be discontinuous at $a$ and show that they violate convexity of $f$ . Removable discontinuities aren't possible if $f$ must be defined at every point in an interval to be convex. Jump discontinuities aren't possible for a convex function, due to the following two situations: In both cases above, the light red line has a slope that is larger than the slope of the blue line, and this violates convexity. Essential discontinuities aren't possible because the slope between any point $x_1<a$ and a point $a+h$ can be made arbitrarily high. Just choose an $h_1>0$ and an $h_2<0$ , let the slope between $x_1$ and $a+h_1$ be some value. Then we can always find an $a+h_2$ with a larger slope, violating convexity as in the picture below. Finally, consider the case of a point discontinuity Whether $f(a)>\lim\limits_{x \to a} f(x)$ or $f(a)<\lim\limits_{x \to a} f(x)$ , convexity is violated. In all possible cases, $f$ turns out not to be convex, a contradiction. Therefore, by proof by contradiction, we conclude that $f$ must be continuous at every point. Is this approach (proof by possible cases of discontinuity, showing contradiction in each one) correct?","Given the following definitions of a convex function from Spivak's Calculus: Definition 1: A function is convex on an interval, if for all and in the interval, the line segment joining and lies above the graph of . Definition 2: A function is convex on an interval if for all , and in the interval with we have . First question: Does Definition 2 mean that has to be defined everywhere in an interval to be convex in that interval? Second question: Is it possible to prove that such a function is always continuous? If so, how? Here is my attempt at a proof. I'm going to outline the proofs, without actually proving using and proofs as would be required to be rigorous. If the general idea of each case below is sound, then I think the corresponding rigorous proofs are relatively simple. Let be convex. Assume is discontinuous at some point , ie . My proof strategy is to go through each of the possible ways that could be discontinuous at and show that they violate convexity of . Removable discontinuities aren't possible if must be defined at every point in an interval to be convex. Jump discontinuities aren't possible for a convex function, due to the following two situations: In both cases above, the light red line has a slope that is larger than the slope of the blue line, and this violates convexity. Essential discontinuities aren't possible because the slope between any point and a point can be made arbitrarily high. Just choose an and an , let the slope between and be some value. Then we can always find an with a larger slope, violating convexity as in the picture below. Finally, consider the case of a point discontinuity Whether or , convexity is violated. In all possible cases, turns out not to be convex, a contradiction. Therefore, by proof by contradiction, we conclude that must be continuous at every point. Is this approach (proof by possible cases of discontinuity, showing contradiction in each one) correct?","f a b (a,f(a)) (b,f(b)) f f a, x b a<x<b \frac{f(x)-f(a)}{x-a}<\frac{f(b)-f(a)}{b-a} f \epsilon \delta f f a \lim\limits_{x \to a} f(x) \neq f(a) f a f f x_1<a a+h h_1>0 h_2<0 x_1 a+h_1 a+h_2 f(a)>\lim\limits_{x \to a} f(x) f(a)<\lim\limits_{x \to a} f(x) f f","['calculus', 'derivatives', 'continuity', 'solution-verification']"
56,"Spivak Calculus: $f$ satisfies $f''(x)+f'(x)g(x)-f(x)=0$, for some g. Prove that if 𝑓 is 0 at two points, then 𝑓 is 0 on the interval between them.","Spivak Calculus:  satisfies , for some g. Prove that if 𝑓 is 0 at two points, then 𝑓 is 0 on the interval between them.",f f''(x)+f'(x)g(x)-f(x)=0,"The following is a problem from chapter 11, ""Significance of the Derivative"" from Spivak's Calculus Suppose that $f$ satisfies $$f''(x)+f'(x)g(x)-f(x)=0\tag{1}$$ for some function $g$ . Prove that if $f$ is $0$ at two points, then $f$ is $0$ on the interval between them. Hint: Use Theorem 6. Here is the theorem referred to above. Theorem 6: Suppose $f''(a)$ exists. If $f$ has a local minimum at $a$ , then $f''(a) \geq 0$ ; if $f$ has a local maximum at $a$ , then $f''(a) \leq 0$ . The solution manual solution is as follows Suppose $f(a)=f(b)=0$ . If $x$ is a local maximum point of $f$ on $[a,b]$ , then $f'(x)=0$ and $f''(x) \leq 0$ ; from the equation $$f''(x)+f'(x)g(x)-f(x)=0$$ we can conclude that $f(x) \leq 0$ . Similarly, $f$ cannot have a negative local minimum on $(a,b)$ . My question regards the above proof, which seems terse to the point that I don't understand how the desired result ensues from it. Let me go through it in more words and steps. If we assume that $(1)$ is true for all $x$ , then $f$ is (twice) differentiable at all $x$ . The interval $[a,b]$ is closed, and $f$ is continuous on this interval. Therefore any max or min on $[a,b]$ must either be a critical point, one of the endpoints, or points where $f$ isn't differentiable. Suppose there is a local max in $x_1 \in [a,b]$ . Theorem 6 tells us that $f''(x_1)<0$ and $(1)$ tell us that $$f''(x_1)=f(x_1)<0\tag{2}$$ The proof above stops here and seems to imply that whatever $(2)$ means or implies is clear. It is not, however, clear to me. Could we not have a situation such as Though I can't quite finish the reasoning, it seems the situation above also leads to a contradiction. Reasoning 1 Since $x_1$ is a local max, there is an interval around it where $f$ is smaller than $f(x_1)$ . In particular, $\exists x_2, x_2 < x_1 \land f(x_2)<f(x_1)$ . The Intermediate Value Theorem tells us that there is some $x_3 \in [a,x_2]$ such that $f(x_3)=f(x_1)$ . Hence, by Rolle's Theorem, there is some $x_4 \in [x_3, x_1]$ such that $f'(x_4)=0$ . But then $(1)$ tell us that $f''(x_4)=f(x_4) \leq 0$ . So $x_4$ is a local max. I can't quite finish this reasoning, but it seems to imply that there are infinite local maxima in $(a,b)$ , at every local max $f$ is negative, and every critical point at which $f$ is negative is a local max. The reasoning when we assume there is a local min in $(a,b)$ should be analogous. Is the reasoning above on the right track, and if so how do I finish it (ie explicitly conclude that $f=0$ in $[a,b]$ )? Reasoning 2 Since $f''(x_1)<0$ , and $f'(x)<0$ in an interval around $x_1$ , $f$ is decreasing as we move to the left of $x_1$ and decreasing as we move to the left of $x_1$ . But since $f(a)=f(b)=0$ , and $f$ is continuous, it will need to increase at some point between $x_1$ and $a$ , and between $x_1$ and $b$ . But then $f'$ needs to be positive, and hence $f''$ needs to be positive so that $f'$ increases from a negative value to a positive value. But then $f$ is positive at such points by $(1)$ , which contradicts the fact that actually $f$ is negative whenever the sign of $f''$ changes because $f$ is negative and decreasing when that happens. Again, this seems to mean contradiction, which means $f$ can't have a local max at a point where $f<0$ . I can't seem to satisfy myself with the details though. The reasoning doesn't seem rigorous enough.","The following is a problem from chapter 11, ""Significance of the Derivative"" from Spivak's Calculus Suppose that satisfies for some function . Prove that if is at two points, then is on the interval between them. Hint: Use Theorem 6. Here is the theorem referred to above. Theorem 6: Suppose exists. If has a local minimum at , then ; if has a local maximum at , then . The solution manual solution is as follows Suppose . If is a local maximum point of on , then and ; from the equation we can conclude that . Similarly, cannot have a negative local minimum on . My question regards the above proof, which seems terse to the point that I don't understand how the desired result ensues from it. Let me go through it in more words and steps. If we assume that is true for all , then is (twice) differentiable at all . The interval is closed, and is continuous on this interval. Therefore any max or min on must either be a critical point, one of the endpoints, or points where isn't differentiable. Suppose there is a local max in . Theorem 6 tells us that and tell us that The proof above stops here and seems to imply that whatever means or implies is clear. It is not, however, clear to me. Could we not have a situation such as Though I can't quite finish the reasoning, it seems the situation above also leads to a contradiction. Reasoning 1 Since is a local max, there is an interval around it where is smaller than . In particular, . The Intermediate Value Theorem tells us that there is some such that . Hence, by Rolle's Theorem, there is some such that . But then tell us that . So is a local max. I can't quite finish this reasoning, but it seems to imply that there are infinite local maxima in , at every local max is negative, and every critical point at which is negative is a local max. The reasoning when we assume there is a local min in should be analogous. Is the reasoning above on the right track, and if so how do I finish it (ie explicitly conclude that in )? Reasoning 2 Since , and in an interval around , is decreasing as we move to the left of and decreasing as we move to the left of . But since , and is continuous, it will need to increase at some point between and , and between and . But then needs to be positive, and hence needs to be positive so that increases from a negative value to a positive value. But then is positive at such points by , which contradicts the fact that actually is negative whenever the sign of changes because is negative and decreasing when that happens. Again, this seems to mean contradiction, which means can't have a local max at a point where . I can't seem to satisfy myself with the details though. The reasoning doesn't seem rigorous enough.","f f''(x)+f'(x)g(x)-f(x)=0\tag{1} g f 0 f 0 f''(a) f a f''(a) \geq 0 f a f''(a) \leq 0 f(a)=f(b)=0 x f [a,b] f'(x)=0 f''(x) \leq 0 f''(x)+f'(x)g(x)-f(x)=0 f(x) \leq 0 f (a,b) (1) x f x [a,b] f [a,b] f x_1 \in [a,b] f''(x_1)<0 (1) f''(x_1)=f(x_1)<0\tag{2} (2) x_1 f f(x_1) \exists x_2, x_2 < x_1 \land f(x_2)<f(x_1) x_3 \in [a,x_2] f(x_3)=f(x_1) x_4 \in [x_3, x_1] f'(x_4)=0 (1) f''(x_4)=f(x_4) \leq 0 x_4 (a,b) f f (a,b) f=0 [a,b] f''(x_1)<0 f'(x)<0 x_1 f x_1 x_1 f(a)=f(b)=0 f x_1 a x_1 b f' f'' f' f (1) f f'' f f f<0","['calculus', 'derivatives', 'proof-explanation']"
57,The derivative of a semialgebraic map is semialgebraic,The derivative of a semialgebraic map is semialgebraic,,"Coste's notes on semialgebraic geometry have the question: If $f:U \to \mathbb{R}$ is semialgebraic, with $U$ an open semialgebraic set, then each partial derivative $\dfrac{\partial f}{\partial x_{i}}$ is semialgebraic Once, a teacher told me how to answer this question. He said to write the graph of $\dfrac{\partial f}{\partial x_{i}}$ as $$\left\{(x,y) \in U \times \mathbb{R}: \forall \varepsilon > 0, \exists \delta > 0 \left(\|t\|^{2} \geq \delta \lor \left|\left|y - \dfrac{f(p+te_{i})-f(p)}{t}\right|\right|^{2} < \varepsilon \right) \right\}$$ and use Tarski-Seidenberg to finish the proof. However, upon reviewing the proof, it is wrong, since this set is not defined using the first-order language of ordered fields. $f$ is a semialgebraic function, not a polynomial. I don't know how to proceed here, and I appreciate any help.","Coste's notes on semialgebraic geometry have the question: If is semialgebraic, with an open semialgebraic set, then each partial derivative is semialgebraic Once, a teacher told me how to answer this question. He said to write the graph of as and use Tarski-Seidenberg to finish the proof. However, upon reviewing the proof, it is wrong, since this set is not defined using the first-order language of ordered fields. is a semialgebraic function, not a polynomial. I don't know how to proceed here, and I appreciate any help.","f:U \to \mathbb{R} U \dfrac{\partial f}{\partial x_{i}} \dfrac{\partial f}{\partial x_{i}} \left\{(x,y) \in U \times \mathbb{R}: \forall \varepsilon > 0, \exists \delta > 0 \left(\|t\|^{2} \geq \delta \lor \left|\left|y - \dfrac{f(p+te_{i})-f(p)}{t}\right|\right|^{2} < \varepsilon \right) \right\} f","['derivatives', 'algebraic-geometry', 'real-algebraic-geometry', 'semialgebraic-geometry']"
58,"Spivak Calculus, Ch 10, problem 32b: Can we use Leibniz's formula to calculate $f^{(k)}(x)$ if $f(x)=\frac{1}{x^2-1}$?","Spivak Calculus, Ch 10, problem 32b: Can we use Leibniz's formula to calculate  if ?",f^{(k)}(x) f(x)=\frac{1}{x^2-1},"Spivak's Calculus, Chapter 10 on Differentiation, problem 32: What is $f^{(k)}(x)$ if a) $f(x) = \frac{1}{(x-1)^n}$ *b) $f(x)=\frac{1}{x^2-1}$ My question regards part $b)$ . $a)$ can be solved quite easily if you use the conclusion of a previous problem (30), which shows If $f(x)=x^{-n}$ for $n \in \mathbb{N}$ then $$f^{(k)}(x)=(-1)^k\frac{(n+k-1)!}{(n-1)!}x^{-n-k}\tag{1}$$ $$g(x)=f(x+1)=x^{-n}$$ $$g^{(k)}(x)=f^{(k)}(x+1)=(-1)^k\frac{(n+k-1)!}{(n-1)!}x^{-n-k}$$ $$g^{(k)}(x-1)=f^{(k)}(x)=\frac{d^{k} \frac{1}{(x-1)^n}}{dx^k}=(-1)^k\frac{(n+k-1)!}{(n-1)!}(x-1)^{-n-k}\tag{2}$$ Similarly, we can show that $$\frac{d^k \frac{1}{(x+1)^n}}{dx^k}=(-1)^k\frac{(n+k-1)!}{(n-1)!}(x+1)^{-n-k}\tag{3}$$ Now for part b). We can rewrite $f(x)=\frac{1}{x^2-1}$ in two different ways: $$f(x)=\frac{1}{x+1}\cdot \frac{1}{x-1}\tag{4}$$ $$f(x)=\frac{1}{2} \left ( \frac{1}{x-1}-\frac{1}{x+1} \right )\tag{5}$$ Spivak's solution manual differentiates $(5)$ . $$f^{(k)}(x)=\frac{1}{2} \left ( \frac{d^k \frac{1}{(x-1)^n}}{dx^k} - \frac{d^k \frac{1}{(x+1)^n}}{dx^k} \right )$$ So we can just sub in $(2)$ and $(3)$ : $$f^{(k)}(x)=\frac{1}{2}(-1)^k\frac{(n+k-1)!}{(n-1)!}((x-1)^{-n-k}-(x+1)^{-n-k})\tag{6}$$ My question is: can we apply Leibniz's formula to differentiate $(4)$ ? Leibniz's formula tells us that $$(f \cdot g)^{(n)}(x)=\sum_{k=0}^n \binom{n}{k}f^{(k)}(x) \cdot g^{(n-k)}(x)$$ If we apply this to $f(x)=\frac{1}{x+1}\cdot \frac{1}{x-1}$ then we get $$f^{(n)}(x)=\sum_{k=0}^n \left [(-1)^k \frac{k!}{0!}(x-1)^{-1-k} \right ] \left [(-1)^{n-k} \frac{(n+k)!}{0!}(x+1)^{-1-k}  \right ]$$ $$= \sum_{k=0}^n \binom{n}{k}(-1)^nk! (n+k)! (x^2-1)^{-1-k}\tag{7}$$ Differentiating the sum in $(5)$ was the easier route. But is the calculation of differentiating $(4)$ correct? Is one route better than the other?","Spivak's Calculus, Chapter 10 on Differentiation, problem 32: What is if a) *b) My question regards part . can be solved quite easily if you use the conclusion of a previous problem (30), which shows If for then Similarly, we can show that Now for part b). We can rewrite in two different ways: Spivak's solution manual differentiates . So we can just sub in and : My question is: can we apply Leibniz's formula to differentiate ? Leibniz's formula tells us that If we apply this to then we get Differentiating the sum in was the easier route. But is the calculation of differentiating correct? Is one route better than the other?",f^{(k)}(x) f(x) = \frac{1}{(x-1)^n} f(x)=\frac{1}{x^2-1} b) a) f(x)=x^{-n} n \in \mathbb{N} f^{(k)}(x)=(-1)^k\frac{(n+k-1)!}{(n-1)!}x^{-n-k}\tag{1} g(x)=f(x+1)=x^{-n} g^{(k)}(x)=f^{(k)}(x+1)=(-1)^k\frac{(n+k-1)!}{(n-1)!}x^{-n-k} g^{(k)}(x-1)=f^{(k)}(x)=\frac{d^{k} \frac{1}{(x-1)^n}}{dx^k}=(-1)^k\frac{(n+k-1)!}{(n-1)!}(x-1)^{-n-k}\tag{2} \frac{d^k \frac{1}{(x+1)^n}}{dx^k}=(-1)^k\frac{(n+k-1)!}{(n-1)!}(x+1)^{-n-k}\tag{3} f(x)=\frac{1}{x^2-1} f(x)=\frac{1}{x+1}\cdot \frac{1}{x-1}\tag{4} f(x)=\frac{1}{2} \left ( \frac{1}{x-1}-\frac{1}{x+1} \right )\tag{5} (5) f^{(k)}(x)=\frac{1}{2} \left ( \frac{d^k \frac{1}{(x-1)^n}}{dx^k} - \frac{d^k \frac{1}{(x+1)^n}}{dx^k} \right ) (2) (3) f^{(k)}(x)=\frac{1}{2}(-1)^k\frac{(n+k-1)!}{(n-1)!}((x-1)^{-n-k}-(x+1)^{-n-k})\tag{6} (4) (f \cdot g)^{(n)}(x)=\sum_{k=0}^n \binom{n}{k}f^{(k)}(x) \cdot g^{(n-k)}(x) f(x)=\frac{1}{x+1}\cdot \frac{1}{x-1} f^{(n)}(x)=\sum_{k=0}^n \left [(-1)^k \frac{k!}{0!}(x-1)^{-1-k} \right ] \left [(-1)^{n-k} \frac{(n+k)!}{0!}(x+1)^{-1-k}  \right ] = \sum_{k=0}^n \binom{n}{k}(-1)^nk! (n+k)! (x^2-1)^{-1-k}\tag{7} (5) (4),"['calculus', 'derivatives', 'solution-verification', 'factorial']"
59,"Using differentiation under the integral sign to evaluate $\int\ln(x^n+1) \,\mathrm dx$ [closed]",Using differentiation under the integral sign to evaluate  [closed],"\int\ln(x^n+1) \,\mathrm dx","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question This is my first stack exchange question, so sorry if it is not neat. So for $$\int\ln(x^n+1)\,\mathrm dx, $$ I tried doing $\ln(a(x^n + 1))$ where $a = 1$ and differentiating under the integral. So I set the integral is $Q(a)$ and get $$ Q^\prime(a)=\int 1/a\,\mathrm dx $$ because we treat $(x^n + 1)$ like a coefficient. And then $Q^\prime(a) = dQ/da=x/a$ , so $Q = x\ln(a) + C$ but setting $a = 1$ you get $Q(1) = C$ and the derivative of $C$ is $0$ , which isn't $\ln(x^n + 1)$ . So what did I do wrong?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question This is my first stack exchange question, so sorry if it is not neat. So for I tried doing where and differentiating under the integral. So I set the integral is and get because we treat like a coefficient. And then , so but setting you get and the derivative of is , which isn't . So what did I do wrong?","\int\ln(x^n+1)\,\mathrm dx,
 \ln(a(x^n + 1)) a = 1 Q(a) 
Q^\prime(a)=\int 1/a\,\mathrm dx
 (x^n + 1) Q^\prime(a) = dQ/da=x/a Q = x\ln(a) + C a = 1 Q(1) = C C 0 \ln(x^n + 1)","['calculus', 'integration', 'derivatives']"
60,Understanding a proof involving the exponential function,Understanding a proof involving the exponential function,,"In an exercise sheet from my real analysis lectures, there is a question I have been stuck on for some time. I will attach the solution from the notes in the hope that someone can 'fill in the gaps' and help me understand. Question: Suppose that $f(x)$ satisfies $f(x+y) = f(x)f(y)$ , prove that if $f$ is differentiable then either $f(x)=0$ or $f(x)=e^{ax}$ . I have attached the proof in picture form so that you can see the spots I'm getting confused at: For clarity, I do not understand how the derivative in the first part has been calculated and what might make you try to calculate the $y$ derivative at $y=0$ . Also why do we have an $a$ in $e^{ax}$ ?","In an exercise sheet from my real analysis lectures, there is a question I have been stuck on for some time. I will attach the solution from the notes in the hope that someone can 'fill in the gaps' and help me understand. Question: Suppose that satisfies , prove that if is differentiable then either or . I have attached the proof in picture form so that you can see the spots I'm getting confused at: For clarity, I do not understand how the derivative in the first part has been calculated and what might make you try to calculate the derivative at . Also why do we have an in ?",f(x) f(x+y) = f(x)f(y) f f(x)=0 f(x)=e^{ax} y y=0 a e^{ax},"['real-analysis', 'derivatives']"
61,Differentiating $y = x - \frac2x + \frac3{x^2}$,Differentiating,y = x - \frac2x + \frac3{x^2},"Another easy question for you guys. I'm differentiating the below to find the equation of the tangent at $(-3,-2)$ $$y = x - \dfrac{2}{x} + \dfrac{3}{x^2}$$ I simplified to: $$ y = x - 2x^{-1} + 3x^{-2}$$ Then differentiated to get: $$ \frac{dy}{dx} = 1 + 2x^{-2} - 6x^{-3}$$ or $$\frac{dy}{dx} = 1 + \frac{2}{x^2} - \frac{6}{x^3}$$ Placing $x = -3$ into this gives me $1$ , and placing $m=1$ into $y=mx+c$ gives me $c = 1$ . Making the simple equation: $0 = x - y + 1$ However, I'm given the answer as: $0 = 13x - 9y + 21$ Where did I go wrong, I've studied it for longer than I'm willing to admit, have I made a stupid mistake somewhere? Thanks,","Another easy question for you guys. I'm differentiating the below to find the equation of the tangent at I simplified to: Then differentiated to get: or Placing into this gives me , and placing into gives me . Making the simple equation: However, I'm given the answer as: Where did I go wrong, I've studied it for longer than I'm willing to admit, have I made a stupid mistake somewhere? Thanks,","(-3,-2) y = x - \dfrac{2}{x} + \dfrac{3}{x^2}  y = x - 2x^{-1} + 3x^{-2}  \frac{dy}{dx} = 1 + 2x^{-2} - 6x^{-3} \frac{dy}{dx} = 1 + \frac{2}{x^2} - \frac{6}{x^3} x = -3 1 m=1 y=mx+c c = 1 0 = x - y + 1 0 = 13x - 9y + 21",['derivatives']
62,Relation of trace and matrix derivatives,Relation of trace and matrix derivatives,,"I've been trying to calculate the gradients of a neural network for backpropagation and I used chain rule which didn't work because of dimensions of the matrices. Then I found that the trace function along with differantials can be used to calculate the correct derivatives, I tried and succesfully found the correct ones as below: $$A:B=tr(A^TB)$$ $$O = J(W_2^TW_1^TX)$$ $$\eqalign{\frac{\partial O}{\partial W_1} &= \nabla J : dO \cr &= \nabla J : dW_2^T(W_1^TX) + W_2^T(dW_1^TX + W_1^TdX) \cr &= \nabla J : dW_2^TW_1^TX + W_2^TdW_1^TX + W_2^TW_1^TdX \cr &= \nabla J : dW_2^TW_1^TX + \nabla J : W_2^TdW_1^TX + \nabla J : W_2^TW_1^TdX \cr &= \nabla JX^TW_1 : dW_2^T + W_2\nabla JX^T : dW_1^T + W_1W_2\nabla J : dX \cr &= (W_2\nabla JX^T : dW_1^T)^T \cr &= X\nabla J^TW_2^T: dW_1 \cr &= X\nabla J^TW_2^T}$$ I can calculate it but I am lost looking for an explaination . Why and how is trace related to matrix derivatives?","I've been trying to calculate the gradients of a neural network for backpropagation and I used chain rule which didn't work because of dimensions of the matrices. Then I found that the trace function along with differantials can be used to calculate the correct derivatives, I tried and succesfully found the correct ones as below: I can calculate it but I am lost looking for an explaination . Why and how is trace related to matrix derivatives?","A:B=tr(A^TB) O = J(W_2^TW_1^TX) \eqalign{\frac{\partial O}{\partial W_1} &= \nabla J : dO \cr
&= \nabla J : dW_2^T(W_1^TX) + W_2^T(dW_1^TX + W_1^TdX) \cr
&= \nabla J : dW_2^TW_1^TX + W_2^TdW_1^TX + W_2^TW_1^TdX \cr
&= \nabla J : dW_2^TW_1^TX + \nabla J : W_2^TdW_1^TX + \nabla J : W_2^TW_1^TdX \cr
&= \nabla JX^TW_1 : dW_2^T + W_2\nabla JX^T : dW_1^T + W_1W_2\nabla J : dX \cr
&= (W_2\nabla JX^T : dW_1^T)^T \cr
&= X\nabla J^TW_2^T: dW_1 \cr
&= X\nabla J^TW_2^T}","['calculus', 'matrices', 'derivatives', 'partial-derivative', 'matrix-calculus']"
63,Differentiable But Not Continuous Example,Differentiable But Not Continuous Example,,"In the context of maps between Banach spaces, a map $f:E\to F$ is differentiable at $x\in E $ if it is continuous at $ x $ and there exists a linear map $ T:E\to F $ such that $$\lim_{h\to0}\frac{\lVert f(x+h)-f(x)-Th\rVert}{\lVert h\rVert}=0.$$ The continuity assumption here then implies that the linear map $ T $ is bounded. Also, if one assumes in the definition that $ T $ is a bounded linear map, then $ f $ will be continuous at $x$ . I was wondering if someone had in mind an example of Banach spaces $ E, F $ , a map $f:E\to F $ , which isn't continuous at $ x\in E $ , and an unbounded linear map $ T:E\to F $ such that $$\lim_{h\to0}\frac{\lVert f(x+h)-f(x)-Th\rVert}{\lVert h\rVert}=0.$$ Such an example would motivate the necessity of including either $ f$ continuous at $ x $ or $ T $ bounded in the definition of differentiability.","In the context of maps between Banach spaces, a map is differentiable at if it is continuous at and there exists a linear map such that The continuity assumption here then implies that the linear map is bounded. Also, if one assumes in the definition that is a bounded linear map, then will be continuous at . I was wondering if someone had in mind an example of Banach spaces , a map , which isn't continuous at , and an unbounded linear map such that Such an example would motivate the necessity of including either continuous at or bounded in the definition of differentiability.","f:E\to F x\in E   x   T:E\to F  \lim_{h\to0}\frac{\lVert f(x+h)-f(x)-Th\rVert}{\lVert h\rVert}=0.  T   T   f  x  E, F  f:E\to F   x\in E   T:E\to F  \lim_{h\to0}\frac{\lVert f(x+h)-f(x)-Th\rVert}{\lVert h\rVert}=0.  f  x   T ","['functional-analysis', 'derivatives', 'continuity', 'definition', 'banach-spaces']"
64,How to construct discontinuous derivative with piecewise-polynomials?,How to construct discontinuous derivative with piecewise-polynomials?,,"Problem : Construct differentiable function with piecewise-polynomials, but it has discontinuous derivative. My Attempt At first I tried to make similar one of : $$x\to x^2\sin\left(\frac{1}{x}\right)$$ Since $x^2$ is already polynomial, I tried to imitate $\sin$ part with $$f\quad \colon\quad  x\to (-1)^n2^n\left(x-\frac{1}{2^{n+1}}\right)\left(x-\frac{1}{2^n}\right)\quad \left(\frac{1}{2^{n+1}} \le x \le \frac{1}{2^n}\right)$$ for each $n \in \mathbb{N}$ . Then $f$ is differentiable for $x\neq 0$ . And I have problems : How can I find $f$ is bounded or not? (to find $f$ is continuous or not at $x=0$ ) (I tried to bound it with $x^2$ but I dont think its valid because $x^2$ is convex but the $f$ switches convex and concave infinitely many times ) (If $f$ is continuous  at $x=0$ ) Can I construct discontinuous derivative with $f$ and something? If I can't for '2', Is there any piecewise-polynomial which has discontinuous derivative?","Problem : Construct differentiable function with piecewise-polynomials, but it has discontinuous derivative. My Attempt At first I tried to make similar one of : Since is already polynomial, I tried to imitate part with for each . Then is differentiable for . And I have problems : How can I find is bounded or not? (to find is continuous or not at ) (I tried to bound it with but I dont think its valid because is convex but the switches convex and concave infinitely many times ) (If is continuous  at ) Can I construct discontinuous derivative with and something? If I can't for '2', Is there any piecewise-polynomial which has discontinuous derivative?","x\to x^2\sin\left(\frac{1}{x}\right) x^2 \sin f\quad \colon\quad
 x\to (-1)^n2^n\left(x-\frac{1}{2^{n+1}}\right)\left(x-\frac{1}{2^n}\right)\quad \left(\frac{1}{2^{n+1}} \le x \le \frac{1}{2^n}\right) n \in \mathbb{N} f x\neq 0 f f x=0 x^2 x^2 f f x=0 f","['calculus', 'analysis', 'derivatives']"
65,Why does $\frac{(x+2)\cdot (2x+\frac{1}{x^2})-(x^2-\frac{1}{x})\cdot 1}{(x+2)^2}$ simplify to $\frac{x^4+4x^3+2x+2}{x^2(x+2)^2}$?,Why does  simplify to ?,\frac{(x+2)\cdot (2x+\frac{1}{x^2})-(x^2-\frac{1}{x})\cdot 1}{(x+2)^2} \frac{x^4+4x^3+2x+2}{x^2(x+2)^2},"I have been burdened with an unclear textbook. The exercise is to differentiate the fraction of two functions. Here it is: $$f(x)=\frac{x^2-\frac{1}{x}}{x+2}$$ I understand how to do this: $\left ( \frac{a}{b} \right )'=\frac{ba'-ab'}{b^2}$ . I have that memorised, and for step one me and the textbook agree. Here's the textbook solution: $$f'(x)=\frac{(x+2)\cdot (2x+\frac{1}{x^2})-(x^2-\frac{1}{x})\cdot 1}{(x+2)^2}=\frac{x^4+4x^3+2x+2}{x^2(x+2)^2}$$ Now here's my solution: $$f'(x)=\frac{(x+2)\cdot (2x+x^{-2})-(x^2-x^{-1})\cdot 1}{(x+2)^2}=\frac{2x^2+x^{-1}+4x+2x{-2}-x^2+x^{-1}}{x^2+4x+4}=\frac{x^2+4x+2x^{-1}+2x^{-2}}{x^2+4x+4}$$ My approach is simple: I write instances of $1/x^n$ with negative exponents, multiply everything in brackets until there's no more brackets, then I add together the terms with the same exponent and order them from high to low. I understand that brackets are necessary if you want to keep multiplication factors intact (for instance to figure out for what values of x the result is zero) but then I might as well not have done anything. The textbook clearly goes for something else entirely, and its approach I truly cannot devise. I know that these two answers are equivalent. Wolfram Alpha agrees . My question is, why would one go for the textbook notation when simplifying the immediate solution (the step me and my textbook agree on). This is a consistent problem I have with the textbook; my answers end up equivalent to the textbook answer but entirely differently notated. I feel like there's something I am missing, for when I ask Wolfram to differentiate the original fraction, it too ends up with the textbook notation . I don't have a tutor to ask; I am working my way through the book on my own in preparation for a programming course. This feels critical to me; what are the steps to reach the textbook solution, and most importantly why are those the steps to take?","I have been burdened with an unclear textbook. The exercise is to differentiate the fraction of two functions. Here it is: I understand how to do this: . I have that memorised, and for step one me and the textbook agree. Here's the textbook solution: Now here's my solution: My approach is simple: I write instances of with negative exponents, multiply everything in brackets until there's no more brackets, then I add together the terms with the same exponent and order them from high to low. I understand that brackets are necessary if you want to keep multiplication factors intact (for instance to figure out for what values of x the result is zero) but then I might as well not have done anything. The textbook clearly goes for something else entirely, and its approach I truly cannot devise. I know that these two answers are equivalent. Wolfram Alpha agrees . My question is, why would one go for the textbook notation when simplifying the immediate solution (the step me and my textbook agree on). This is a consistent problem I have with the textbook; my answers end up equivalent to the textbook answer but entirely differently notated. I feel like there's something I am missing, for when I ask Wolfram to differentiate the original fraction, it too ends up with the textbook notation . I don't have a tutor to ask; I am working my way through the book on my own in preparation for a programming course. This feels critical to me; what are the steps to reach the textbook solution, and most importantly why are those the steps to take?",f(x)=\frac{x^2-\frac{1}{x}}{x+2} \left ( \frac{a}{b} \right )'=\frac{ba'-ab'}{b^2} f'(x)=\frac{(x+2)\cdot (2x+\frac{1}{x^2})-(x^2-\frac{1}{x})\cdot 1}{(x+2)^2}=\frac{x^4+4x^3+2x+2}{x^2(x+2)^2} f'(x)=\frac{(x+2)\cdot (2x+x^{-2})-(x^2-x^{-1})\cdot 1}{(x+2)^2}=\frac{2x^2+x^{-1}+4x+2x{-2}-x^2+x^{-1}}{x^2+4x+4}=\frac{x^2+4x+2x^{-1}+2x^{-2}}{x^2+4x+4} 1/x^n,"['algebra-precalculus', 'derivatives']"
66,Interpretation of $\exp\left(\frac{d}{dx} \ln( f(x) ) \right)$?,Interpretation of ?,\exp\left(\frac{d}{dx} \ln( f(x) ) \right),"Is there any intuitive interpretation or simplification of $\exp\left(\frac{d}{dx} \ln(f(x))\right)$ ? Forms like $\phi=\exp\left(\frac{d}{dN} \ln(f(N))\right)$ are common in thermal physics/chemistry. Typically $N\gg 0$ , and $f(N)$ is the ratio of two increasing, positive-definite, ""very large"" functions ( multiplicities ). For example, $\phi(T,P,N)=\exp\left( \frac{∂}{∂N} \left( \ln Ω_{IG}(T,P,N) - \ln Ω(T,P,N) \right) \right)$ is the fugacity coefficient.","Is there any intuitive interpretation or simplification of ? Forms like are common in thermal physics/chemistry. Typically , and is the ratio of two increasing, positive-definite, ""very large"" functions ( multiplicities ). For example, is the fugacity coefficient.","\exp\left(\frac{d}{dx} \ln(f(x))\right) \phi=\exp\left(\frac{d}{dN} \ln(f(N))\right) N\gg 0 f(N) \phi(T,P,N)=\exp\left( \frac{∂}{∂N} \left( \ln Ω_{IG}(T,P,N) - \ln Ω(T,P,N) \right) \right)",['derivatives']
67,How do we find the derivative of the following function: $F(x)=\sin\Big(\int_0^x \sin\big(\int_0^y \frac{1}{1+t^4}\ dt\big)\ dy\Big)$ using FTC.,How do we find the derivative of the following function:  using FTC.,F(x)=\sin\Big(\int_0^x \sin\big(\int_0^y \frac{1}{1+t^4}\ dt\big)\ dy\Big),"Find the derivative of the following function: $F(x)=\sin\Big(\int_0^x \sin\big(\int_0^y \frac{1}{1+t^4}\ dt\big)\ dy\Big)$ I tried applying fundamental theorem of calculus directly but the the integral $\int_0^y \frac{1}{1+t^4}\ dt$ is giving me problems, I started by deriving both sides with respect to x, and by chain rule, the derivative should be f'(g(x))*g'(x). I reached this point and couldn't complete, does anyone have the key for the solution?","Find the derivative of the following function: I tried applying fundamental theorem of calculus directly but the the integral is giving me problems, I started by deriving both sides with respect to x, and by chain rule, the derivative should be f'(g(x))*g'(x). I reached this point and couldn't complete, does anyone have the key for the solution?",F(x)=\sin\Big(\int_0^x \sin\big(\int_0^y \frac{1}{1+t^4}\ dt\big)\ dy\Big) \int_0^y \frac{1}{1+t^4}\ dt,"['real-analysis', 'integration', 'analysis', 'derivatives']"
68,"If $f(0) = 0$, then, is $\mathrm{lim}_{x\to0} xf'(x) = 0$?","If , then, is ?",f(0) = 0 \mathrm{lim}_{x\to0} xf'(x) = 0,"Is the above claim true? I think that it is. I am assuming that $f$ is continuous and differentiable at $0$ . My not-so-rigorous attempt: $$ \mathrm{lim}_{x \to 0} x f'(x) = \mathrm{lim}_{x \to 0} \mathrm{lim}_{h\to0} x \frac{f(h)}{h} $$ Now choosing the approach direction $x=h$ , we see that the limit equals $0$ because $f(0)=0$ . Is the idea correct? If not, can this condition be true under some additional constraints?","Is the above claim true? I think that it is. I am assuming that is continuous and differentiable at . My not-so-rigorous attempt: Now choosing the approach direction , we see that the limit equals because . Is the idea correct? If not, can this condition be true under some additional constraints?",f 0  \mathrm{lim}_{x \to 0} x f'(x) = \mathrm{lim}_{x \to 0} \mathrm{lim}_{h\to0} x \frac{f(h)}{h}  x=h 0 f(0)=0,"['real-analysis', 'calculus', 'limits', 'derivatives']"
69,Closed-form expression of the $n$-th derivative of $f_k(x)=e^{-cx}(c(x_u-x))^k$,Closed-form expression of the -th derivative of,n f_k(x)=e^{-cx}(c(x_u-x))^k,"We have a function $$f_k(x)=e^{-cx}(c(x_u-x))^k,$$ that is defined for all $x\in[0,x_u]$ . We know that $k\in\mathbb{N}^+$ and $c>0$ is a given constant. We are interested in having a closed-form expression (if possible!) of the $n$ -th derivative of $f_k(x)$ . So far I started from $n=1$ to see if there's any pattern and I was thinking maybe we can formulate it as a series but I was not successful. I also tried using Mathematica but again, I'm not sure how to use the output there. Can anyone please give me a hint?","We have a function that is defined for all . We know that and is a given constant. We are interested in having a closed-form expression (if possible!) of the -th derivative of . So far I started from to see if there's any pattern and I was thinking maybe we can formulate it as a series but I was not successful. I also tried using Mathematica but again, I'm not sure how to use the output there. Can anyone please give me a hint?","f_k(x)=e^{-cx}(c(x_u-x))^k, x\in[0,x_u] k\in\mathbb{N}^+ c>0 n f_k(x) n=1",['calculus']
70,Confused about the meaning of a differantial map in baby do Carmo.,Confused about the meaning of a differantial map in baby do Carmo.,,"The images are from section $2-4$ of do Carmo's Differential Geometry of Curves and Surfaces . In the following discussion $T_p(S_1)$ is the tangent plane of $S_1$ (a regular surface) at $p$ (a point of such surface). Similarly with $T_{\varphi(p)}(S_2)$ . I do not understand what it means for $\varphi$ to be differentiable since it is defined as a function $S_1\rightarrow S_2$ , where $S_1$ is not an open set of $\mathbb{R}^3$ . As I understand it, the goal of proposition $2$ (the proposition following the discussion above, and which is pictured below in the post) is to define what it means for a function between two regular surfaces to be differentiable, so it makes no sense to claim that $\varphi$ is differentiable at this point of the text. In case it is needed, proposition $2$ reads as follows: What does it mean for $\varphi$ to be differentiable?","The images are from section of do Carmo's Differential Geometry of Curves and Surfaces . In the following discussion is the tangent plane of (a regular surface) at (a point of such surface). Similarly with . I do not understand what it means for to be differentiable since it is defined as a function , where is not an open set of . As I understand it, the goal of proposition (the proposition following the discussion above, and which is pictured below in the post) is to define what it means for a function between two regular surfaces to be differentiable, so it makes no sense to claim that is differentiable at this point of the text. In case it is needed, proposition reads as follows: What does it mean for to be differentiable?",2-4 T_p(S_1) S_1 p T_{\varphi(p)}(S_2) \varphi S_1\rightarrow S_2 S_1 \mathbb{R}^3 2 \varphi 2 \varphi,"['derivatives', 'differential-geometry', 'linear-transformations', 'proof-explanation', 'differential']"
71,Trouble with the Jacobian for a camera projection matrix.,Trouble with the Jacobian for a camera projection matrix.,,"I am using the Gauss Newton method to update a camera projection matrix. $$ x = \frac{PX}{P_\text{last row} X} $$ where x is a known $3\times k$ matrix, X is a known $4\times k$ matrix, and $$ P =  \begin{bmatrix} p_1 & p_2 & p_3 & p_4\\ p_5 & p_6 & p_7 & p_8\\ p_{9} & p_{10} & p_{11} & p_{12}\\ \end{bmatrix} $$ I have a loss function I want to minimize L, where $$ R(P) = \frac{PX}{P_\text{last row} X} - x $$ and unroll r into $$ r(P) =  \begin{bmatrix} R(P)_{1,1} \\ R(P)_{2,1} \\ \vdots \\ R(P)_{2,k} \\ R(P)_{3,k} \end{bmatrix} $$ then $$ L(P) = r(P)^T \cdot r(P) $$ I unroll P into a vector p $$ p =  \begin{bmatrix} p_1 \\ p_2 \\ \vdots \\ p_{11} \\ p_{12}\\ \end{bmatrix} $$ I know $$ \frac{\partial L}{\partial P} =  2 \frac{\partial r}{\partial  P} r(P) $$ and then I have my Jacobian where I want to find $$ \frac{\partial r(P)}{\partial p} = J = \begin{bmatrix} \frac{\partial r(P)_{1,1}}{\partial p_1} & \cdots & \frac{\partial r(P)_{1,1}}{\partial p_{12}}\\ \frac{\partial r(P)_{2,1}}{\partial p_1} & \cdots & \frac{\partial r(P)_{2,1}}{\partial p_{12}}\\ \frac{\partial r(P)_{1,2}}{\partial p_1} & \cdots & \frac{\partial r(P)_{1,2}}{\partial p_{12}}\\ \vdots & \ddots & \vdots\\ \frac{\partial r(P)_{2,k}}{\partial p_1} & \cdots & \frac{\partial r(P)_{2,k}}{\partial p_{12}}\\ \end{bmatrix} $$ I removed the rows that don't depend on P, every third row, because the last row of the output is always 1. $$ \frac{\partial R(P)_{1,i}}{\partial P_{1, j}} = \frac{X_{j, i}}{X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4}} = \frac{X_{j,i}}{c_i}\\ \frac{\partial R(P)_{1,i}}{\partial P_{2, j}} = 0 \\ \frac{\partial R(P)_{1,i}}{\partial P_{3, j}} = \frac{X_{j, i}(X_{1, i}P_{1, 1} + X_{2, i}P_{1, 2} + X_{3, i}P_{1, 3} + X_{4, i}P_{1, 4})}{(X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4})^2} = \frac{X_{j, i}a_i}{c_i^2}\\ \frac{\partial R(P)_{2,i}}{\partial P_{1, j}} = 0 \\ \frac{\partial R(P)_{2,i}}{\partial P_{2, j}} = \frac{X_{j, i}}{X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4}} = \frac{X_{j,i}}{c_i}\\ \frac{\partial R(P)_{2,i}}{\partial P_{3, j}} = \frac{X_{j, i}(X_{1, i}P_{2, 1} + X_{2, i}P_{2, 2} + X_{3, i}P_{2, 3} + X_{4, i}P_{2, 4})}{(X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4})^2} = \frac{X_{j, i}b_i}{c_i^2} \\ \vdots $$ Doing some substitution I get $$ J = \begin{bmatrix} \frac{X_{1,1}}{c_1} & 0 & \frac{X_{1, 1}a_1}{c_1^2} & \cdots & \frac{X_{4, 1}a_1}{c_1^2}\\ 0 & \frac{X_{1,1}}{c_1} & \frac{X_{1, 1}b_1}{c_1^2} & \cdots & \frac{X_{4, 1}b_1}{c_1^2}\\ \frac{X_{1,2}}{c_2} & 0 & \frac{X_{1, 2}a_2}{c_2^2} & \cdots & \frac{X_{4, 2}a_2}{c_2^2}\\ 0 & \frac{X_{1,2}}{c_2} & \frac{X_{1, 2}b_2}{c_2^2} & \cdots & \frac{X_{4, 2}b_2}{c_2^2}\\ \vdots & \ddots & \ddots & \ddots & \vdots\\ 0 & \frac{X_{1,k}}{c_2} & \cdots & \cdots & \frac{X_{4, k}b_k}{c_k^2}\\ \end{bmatrix} $$ But when plugging this into the Gauss-Newton Method: $$ p_{n+1} = p_n - (J^TJ)^{-1}J^Tr(P_n) $$ It just gives me wildly incorrect results. Where did I go wrong?","I am using the Gauss Newton method to update a camera projection matrix. where x is a known matrix, X is a known matrix, and I have a loss function I want to minimize L, where and unroll r into then I unroll P into a vector p I know and then I have my Jacobian where I want to find I removed the rows that don't depend on P, every third row, because the last row of the output is always 1. Doing some substitution I get But when plugging this into the Gauss-Newton Method: It just gives me wildly incorrect results. Where did I go wrong?","
x = \frac{PX}{P_\text{last row} X}
 3\times k 4\times k 
P = 
\begin{bmatrix}
p_1 & p_2 & p_3 & p_4\\
p_5 & p_6 & p_7 & p_8\\
p_{9} & p_{10} & p_{11} & p_{12}\\
\end{bmatrix}
 
R(P) = \frac{PX}{P_\text{last row} X} - x
 
r(P) = 
\begin{bmatrix}
R(P)_{1,1} \\ R(P)_{2,1} \\ \vdots \\ R(P)_{2,k} \\ R(P)_{3,k}
\end{bmatrix}
 
L(P) = r(P)^T \cdot r(P)
 
p = 
\begin{bmatrix}
p_1 \\ p_2 \\ \vdots \\ p_{11} \\ p_{12}\\
\end{bmatrix}
 
\frac{\partial L}{\partial P} =  2 \frac{\partial r}{\partial 
P} r(P)
 
\frac{\partial r(P)}{\partial p} = J =
\begin{bmatrix}
\frac{\partial r(P)_{1,1}}{\partial p_1} & \cdots & \frac{\partial r(P)_{1,1}}{\partial p_{12}}\\
\frac{\partial r(P)_{2,1}}{\partial p_1} & \cdots & \frac{\partial r(P)_{2,1}}{\partial p_{12}}\\
\frac{\partial r(P)_{1,2}}{\partial p_1} & \cdots & \frac{\partial r(P)_{1,2}}{\partial p_{12}}\\
\vdots & \ddots & \vdots\\
\frac{\partial r(P)_{2,k}}{\partial p_1} & \cdots & \frac{\partial r(P)_{2,k}}{\partial p_{12}}\\
\end{bmatrix}
 
\frac{\partial R(P)_{1,i}}{\partial P_{1, j}} = \frac{X_{j, i}}{X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4}} = \frac{X_{j,i}}{c_i}\\
\frac{\partial R(P)_{1,i}}{\partial P_{2, j}} = 0 \\
\frac{\partial R(P)_{1,i}}{\partial P_{3, j}} = \frac{X_{j, i}(X_{1, i}P_{1, 1} + X_{2, i}P_{1, 2} + X_{3, i}P_{1, 3} + X_{4, i}P_{1, 4})}{(X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4})^2} = \frac{X_{j, i}a_i}{c_i^2}\\
\frac{\partial R(P)_{2,i}}{\partial P_{1, j}} = 0 \\
\frac{\partial R(P)_{2,i}}{\partial P_{2, j}} = \frac{X_{j, i}}{X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4}} = \frac{X_{j,i}}{c_i}\\
\frac{\partial R(P)_{2,i}}{\partial P_{3, j}} = \frac{X_{j, i}(X_{1, i}P_{2, 1} + X_{2, i}P_{2, 2} + X_{3, i}P_{2, 3} + X_{4, i}P_{2, 4})}{(X_{1, i}P_{3, 1} + X_{2, i}P_{3, 2} + X_{3, i}P_{3, 3} + X_{4, i}P_{3, 4})^2} = \frac{X_{j, i}b_i}{c_i^2} \\
\vdots
 
J =
\begin{bmatrix}
\frac{X_{1,1}}{c_1} & 0 & \frac{X_{1, 1}a_1}{c_1^2} & \cdots & \frac{X_{4, 1}a_1}{c_1^2}\\
0 & \frac{X_{1,1}}{c_1} & \frac{X_{1, 1}b_1}{c_1^2} & \cdots & \frac{X_{4, 1}b_1}{c_1^2}\\
\frac{X_{1,2}}{c_2} & 0 & \frac{X_{1, 2}a_2}{c_2^2} & \cdots & \frac{X_{4, 2}a_2}{c_2^2}\\
0 & \frac{X_{1,2}}{c_2} & \frac{X_{1, 2}b_2}{c_2^2} & \cdots & \frac{X_{4, 2}b_2}{c_2^2}\\
\vdots & \ddots & \ddots & \ddots & \vdots\\
0 & \frac{X_{1,k}}{c_2} & \cdots & \cdots & \frac{X_{4, k}b_k}{c_k^2}\\
\end{bmatrix}
 
p_{n+1} = p_n - (J^TJ)^{-1}J^Tr(P_n)
","['linear-algebra', 'derivatives', 'partial-derivative', 'jacobian', 'total-variation']"
72,"Can I exchange these operations: Absolute square, limit and time derivative in my specific example involving a Dirac sequence. If yes: why?","Can I exchange these operations: Absolute square, limit and time derivative in my specific example involving a Dirac sequence. If yes: why?",,"I am rederiving some physics stuff (related to Fermi's Golden rule) so I know what the result should be. ( I am a physicist, so I lack some math training.) However to get this result I have to exchange several operations at some point and I am really unsure why I should be able to do this. The term I have is: $$ \frac{d}{dt} \left|\lim_{\epsilon \rightarrow 0} \frac{e^{\epsilon t} e^{\mathrm{i} (x -y) t}}{\epsilon + \mathrm{i}( x - y)} \right|^2 $$ If I could just reorder all the operations as much as I want, I would choose: First absolute square, then time derivative, then the limit. This would give me: $$ \lim_{\epsilon \rightarrow 0} \frac{2 \epsilon e^{2 \epsilon t}}{\epsilon^2 + (x - y)^2} = 2 \pi \delta(x-y) $$ which is what I expected to find at some point in my derivation. But why should this be allowed? Is it allowed in this case? I tried to read up on when I can exchange two limits when expressing $\frac{d}{dt}$ as a limit. However I think the problem is more special because of the Dirac distribution (?). I would be happy if someone can give advise here. Thank you very much in advance.","I am rederiving some physics stuff (related to Fermi's Golden rule) so I know what the result should be. ( I am a physicist, so I lack some math training.) However to get this result I have to exchange several operations at some point and I am really unsure why I should be able to do this. The term I have is: If I could just reorder all the operations as much as I want, I would choose: First absolute square, then time derivative, then the limit. This would give me: which is what I expected to find at some point in my derivation. But why should this be allowed? Is it allowed in this case? I tried to read up on when I can exchange two limits when expressing as a limit. However I think the problem is more special because of the Dirac distribution (?). I would be happy if someone can give advise here. Thank you very much in advance.","
\frac{d}{dt}
\left|\lim_{\epsilon \rightarrow 0} \frac{e^{\epsilon t} e^{\mathrm{i} (x -y) t}}{\epsilon + \mathrm{i}( x - y)} \right|^2
 
\lim_{\epsilon \rightarrow 0} \frac{2 \epsilon e^{2 \epsilon t}}{\epsilon^2 + (x - y)^2} = 2 \pi \delta(x-y)
 \frac{d}{dt}","['limits', 'derivatives', 'dirac-delta']"
73,Calculus differentiation — What did I do wrong?,Calculus differentiation — What did I do wrong?,,"I am learning Calculus with the book Calculus and Analytics Geometry, by George B. and Thomas, Jr., and I was doing the exercises on the Chapter 2.6 and using Wolfram Alpha to check my answer. I am told to differentiate $x(x^2+1)^{1/2}$ , so I tried using The Product Rule and The Power Rule for Rational Numbers: $\frac{d}{dt}x(x^2+1)^{1/2} = x\frac{d}{dx}\sqrt{x^2+1} + \sqrt{x^2+1}\frac{d}{dx}x = x\frac{1}{2}\frac{1}{\sqrt{x^2+1}} + \sqrt{x^2+1} = \frac{x}{2\sqrt{x^2+1}} + \frac{x^2+1}{\sqrt{x^2+1}} = \frac{x}{2\sqrt{x^2+1}} + \frac{2x^2+2}{2\sqrt{x^2+1}} = \frac{2x^2+x+2}{2\sqrt{x^2+1}}$ But Wolfram Alpha gives $\frac{2x^2 + 1}{\sqrt{x^2 + 1}}$ I am pretty sure I made a mistake, but I don't know where, sice it looks, to me, that I used the Product Rule and The Power Rule correctly. Hence my question: what did I do wrong? As a note, I don't know if this question is allowed, and my LaTeX isn't very good.","I am learning Calculus with the book Calculus and Analytics Geometry, by George B. and Thomas, Jr., and I was doing the exercises on the Chapter 2.6 and using Wolfram Alpha to check my answer. I am told to differentiate , so I tried using The Product Rule and The Power Rule for Rational Numbers: But Wolfram Alpha gives I am pretty sure I made a mistake, but I don't know where, sice it looks, to me, that I used the Product Rule and The Power Rule correctly. Hence my question: what did I do wrong? As a note, I don't know if this question is allowed, and my LaTeX isn't very good.",x(x^2+1)^{1/2} \frac{d}{dt}x(x^2+1)^{1/2} = x\frac{d}{dx}\sqrt{x^2+1} + \sqrt{x^2+1}\frac{d}{dx}x = x\frac{1}{2}\frac{1}{\sqrt{x^2+1}} + \sqrt{x^2+1} = \frac{x}{2\sqrt{x^2+1}} + \frac{x^2+1}{\sqrt{x^2+1}} = \frac{x}{2\sqrt{x^2+1}} + \frac{2x^2+2}{2\sqrt{x^2+1}} = \frac{2x^2+x+2}{2\sqrt{x^2+1}} \frac{2x^2 + 1}{\sqrt{x^2 + 1}},"['calculus', 'derivatives']"
74,"Compute derivative of $k(t)=\int^{\infty}_{-\infty} \frac{\sin{tx^2}}{1+x^4}\,\textrm dx$",Compute derivative of,"k(t)=\int^{\infty}_{-\infty} \frac{\sin{tx^2}}{1+x^4}\,\textrm dx","I'm trying to compute the derivative of $k(t) = \int^{\infty}_{-\infty} \frac{\sin{tx^2}}{1+x^4}\,\textrm dx$ . I've already showed that it exists, so here's what I've thought of so far: letting $k_n(t) = \int^n_{-n} \frac{\sin{tx^2}}{1+x^4}\,\textrm dx$ , and using the $\int^\infty_{-\infty} \frac{\textrm dx}{1+x^4}$ bound (and symmetry to write $2 \int_0^\infty$ ), I can show the sequence converges uniformly. Now, letting $h(\delta, t, x) = \frac{\sin{tx^2} - \sin{(t+\delta)x^2}}{\delta} - x^2 \cos{tx^2}$ , by considering we are in $[-n, n]$ , and using uniform continuity and MVT, we get $h(\delta, t, x) < \epsilon$ for sufficiently small delta for a fixed $t$ with $x \in[-n, n]$ . Thus, since we are in a finite interval, we have $k_n^\prime (t) = \int^n_{-n} \frac{x^2 \cos{tx^2}}{1+x^4}\,\textrm dx$ . I think I could then finish it using the bound $\int^\infty_{-\infty} \frac{\textrm dx}{x^2}$ to show the derivatives converge uniformly and then apply the uniform convergence result on derivatives. I would like to ask if my reasoning is correct and I can finish it this way or if there are any mistakes along it. Any help is appreciated!","I'm trying to compute the derivative of . I've already showed that it exists, so here's what I've thought of so far: letting , and using the bound (and symmetry to write ), I can show the sequence converges uniformly. Now, letting , by considering we are in , and using uniform continuity and MVT, we get for sufficiently small delta for a fixed with . Thus, since we are in a finite interval, we have . I think I could then finish it using the bound to show the derivatives converge uniformly and then apply the uniform convergence result on derivatives. I would like to ask if my reasoning is correct and I can finish it this way or if there are any mistakes along it. Any help is appreciated!","k(t) = \int^{\infty}_{-\infty} \frac{\sin{tx^2}}{1+x^4}\,\textrm dx k_n(t) = \int^n_{-n} \frac{\sin{tx^2}}{1+x^4}\,\textrm dx \int^\infty_{-\infty} \frac{\textrm dx}{1+x^4} 2 \int_0^\infty h(\delta, t, x) = \frac{\sin{tx^2} - \sin{(t+\delta)x^2}}{\delta} - x^2 \cos{tx^2} [-n, n] h(\delta, t, x) < \epsilon t x \in[-n, n] k_n^\prime (t) = \int^n_{-n} \frac{x^2 \cos{tx^2}}{1+x^4}\,\textrm dx \int^\infty_{-\infty} \frac{\textrm dx}{x^2}","['real-analysis', 'calculus', 'derivatives', 'solution-verification', 'leibniz-integral-rule']"
75,Jacobian matrix of $\mathbb R^3$ functions involving unit vector,Jacobian matrix of  functions involving unit vector,\mathbb R^3,"Jacobian matrix of $\mathbb R^3$ functions involving unit vector I'm learning differentiation of vector-valued functions in my analysis class now and I'm a bit stuck in the following question: Given $f:$ $\mathbb{R^3}\setminus \{0,0,0\} \to \mathbb{R^3}$ , $f(x)=x/||x||,$ for all $x \neq (0,0,0).$ Find ${(Df(p))}$ for $p > \neq (0,0,0)$ , and hence prove that $||p|| {(Df(p))}^2 =  {(Df(p))}$ . My attempt: I know that ${(Df(p))}$ will be a $3 \times 3$ matrix represented by the gradient vectors of the 3 coordinate functions. While solving for $||p|| {(Df(p))}^2$ , the only way I thought of is to do the matrix multiplication of ${(Df(p))}$ to itself multiplied by the norm of $p$ to get ${(Df(p))}$ which is a bit tedious and seems not to get me anywhere to  what I need to derive. Could anyone give me some directions on how I could approach the question (probably some definitions that I've missed?). Thanks...","Jacobian matrix of functions involving unit vector I'm learning differentiation of vector-valued functions in my analysis class now and I'm a bit stuck in the following question: Given , for all Find for , and hence prove that . My attempt: I know that will be a matrix represented by the gradient vectors of the 3 coordinate functions. While solving for , the only way I thought of is to do the matrix multiplication of to itself multiplied by the norm of to get which is a bit tedious and seems not to get me anywhere to  what I need to derive. Could anyone give me some directions on how I could approach the question (probably some definitions that I've missed?). Thanks...","\mathbb R^3 f: \mathbb{R^3}\setminus \{0,0,0\} \to \mathbb{R^3} f(x)=x/||x||, x \neq (0,0,0). {(Df(p))} p
> \neq (0,0,0) ||p|| {(Df(p))}^2 =  {(Df(p))} {(Df(p))} 3 \times 3 ||p|| {(Df(p))}^2 {(Df(p))} p {(Df(p))}","['real-analysis', 'derivatives', 'jacobian']"
76,Understanding the informal intution behind Little Oh,Understanding the informal intution behind Little Oh,,"According to user137731 in the thread linked below, we define ""little oh"" as follows: Definition : A function $f$ is called little oh of $g$ as $x\to a$ , denoted $f\in o(g)$ as $x\to a$ , if $$\lim_{x\to a}\frac {f(x)}{g(x)}=0$$ Intuitively this means that $f(x)\to 0$ as $x\to a$ ""faster"" than $g$ does. Personally, I find the intuition behind ""little oh"" more difficult than the straightforward definition of it. So I need intuition for the intuition given above. Here below is my attempt at understanding it. Does that make sense? By definition, $\forall \epsilon > 0 \ \exists \delta \ |x - a| < \delta \implies \left|\frac{f(x)}{g(x)}\right| < \epsilon $ meaning for $x \in N_\delta(a)$ , we have $\left|\frac{f(x)}{g(x)}\right| = 0$ . But that only happens if $f(x) = 0, \ g(x) \neq 0.$ Is it the sense in which $f(x)$ is faster than $g(x)$ around $0$ ? How is the derivative truly, literally the ""best linear approximation"" near a point?","According to user137731 in the thread linked below, we define ""little oh"" as follows: Definition : A function is called little oh of as , denoted as , if Intuitively this means that as ""faster"" than does. Personally, I find the intuition behind ""little oh"" more difficult than the straightforward definition of it. So I need intuition for the intuition given above. Here below is my attempt at understanding it. Does that make sense? By definition, meaning for , we have . But that only happens if Is it the sense in which is faster than around ? How is the derivative truly, literally the ""best linear approximation"" near a point?","f g x\to a f\in o(g) x\to a \lim_{x\to a}\frac {f(x)}{g(x)}=0 f(x)\to 0 x\to a g \forall \epsilon > 0 \ \exists \delta \ |x - a| < \delta \implies \left|\frac{f(x)}{g(x)}\right| < \epsilon  x \in N_\delta(a) \left|\frac{f(x)}{g(x)}\right| = 0 f(x) = 0, \ g(x) \neq 0. f(x) g(x) 0","['real-analysis', 'limits', 'derivatives']"
77,Proof verification of function that satisfies $f(xy)=f(x)+f(y)$,Proof verification of function that satisfies,f(xy)=f(x)+f(y),"A function $f:(0,\infty)\mapsto R$ satisfies the condition $f(xy)=f(x)+f(y) $ for all $x>0,y>0$ . If $f$ is differentiable at $1$ , prove that $f$ is differentiable at every $c\in (0,\infty)$ and $f'(c)=f'(1)/c$ My attempt: Consider, $\lim_{x \to c}\{\frac{f(x)-f(c)}{x-c}\}$ $=$ $\lim_{x \to c}\{\frac{2f(x)-f(xc)}{x-c}\}$ Now take $x=yc$ , then, $\lim_{y \to 1}\{\frac{2f(yc)-f(yc^2)}{(y-1)c}\}$ (*) Now $2f(yc)=2f(y)+2f(c)$ and $f(yc^2)=f(y)+f(c^2)$ Then (*) becomes, $\lim_{y \to 1}\{\frac{f(y)+2f(c)-f(c^2)}{(y-1)c}\}$ Now, $f(c^2)=2f(c)$ by definition This implies, $\lim_{y \to 1}\{\frac{f(y)}{(y-1)c}\}$ $=$ $f'(1)/c$ (since $f'(1)= \lim_{y \to 1}\{\frac{f(y)}{y-1}\}$ , as taking y=1 we get $f(x)=f(1)+f(x)$ so, $f(1)=0$ Is this correct? Thanks in advance!","A function satisfies the condition for all . If is differentiable at , prove that is differentiable at every and My attempt: Consider, Now take , then, (*) Now and Then (*) becomes, Now, by definition This implies, (since , as taking y=1 we get so, Is this correct? Thanks in advance!","f:(0,\infty)\mapsto R f(xy)=f(x)+f(y)  x>0,y>0 f 1 f c\in (0,\infty) f'(c)=f'(1)/c \lim_{x \to c}\{\frac{f(x)-f(c)}{x-c}\} = \lim_{x \to c}\{\frac{2f(x)-f(xc)}{x-c}\} x=yc \lim_{y \to 1}\{\frac{2f(yc)-f(yc^2)}{(y-1)c}\} 2f(yc)=2f(y)+2f(c) f(yc^2)=f(y)+f(c^2) \lim_{y \to 1}\{\frac{f(y)+2f(c)-f(c^2)}{(y-1)c}\} f(c^2)=2f(c) \lim_{y \to 1}\{\frac{f(y)}{(y-1)c}\} = f'(1)/c f'(1)= \lim_{y \to 1}\{\frac{f(y)}{y-1}\} f(x)=f(1)+f(x) f(1)=0","['real-analysis', 'derivatives', 'continuity', 'solution-verification']"
78,derivative of trace of (XAX) w.r.t X when X is symmetric?,derivative of trace of (XAX) w.r.t X when X is symmetric?,,"I wonder if there is any formula of $\frac{d tr(XAX)}{d X}$ when $X$ is symmetric? I understand that when $X$ is not symmetric, we have $$\frac{d tr(XAX^T)}{dX} = XA^T+XA,$$ and when $X$ is symmetric, we have $$\frac{d tr(AX)}{dX} = A+A^T-A\circ I,$$ where $\circ$ means elementwise product. However, I am not able to find $\frac{d tr(XAX)}{d X}$ when $X$ is given to be symmetric. Or any hint on the derivative of this formula is appreciated.","I wonder if there is any formula of when is symmetric? I understand that when is not symmetric, we have and when is symmetric, we have where means elementwise product. However, I am not able to find when is given to be symmetric. Or any hint on the derivative of this formula is appreciated.","\frac{d tr(XAX)}{d X} X X \frac{d tr(XAX^T)}{dX} = XA^T+XA, X \frac{d tr(AX)}{dX} = A+A^T-A\circ I, \circ \frac{d tr(XAX)}{d X} X","['derivatives', 'matrix-calculus', 'trace']"
79,Let ${f}$ be a differentiable function in $x=6$ so that $\lim_{x\to6}{\frac{x^{3}f(x)-1512}{x-6}}=262.144$. Find the values of $f(6)$ and $f(6)'$.,Let  be a differentiable function in  so that . Find the values of  and .,{f} x=6 \lim_{x\to6}{\frac{x^{3}f(x)-1512}{x-6}}=262.144 f(6) f(6)',"Let ${f}$ be a differentiable function in $x=6$ so that $$\lim_{x\to6}{\frac{x^{3}f(x)-1512}{x-6}}=262.144$$ Find the values of $f(6)$ and $f(6)'$ . The right answers are, respectively 7 and 1.21. This was a problem presented in a quiz I had this week in Calculus I. By my understanding it would suffice to do $x^{3}f(x)-1512 = 0$ since the upper part should be $0$ just as $x-6$ for the function to exist in $x=6$ . This would return $f(6)=7$ . Doing l'Hôpital's rule and deriving the function makes $f(6)'=-2.28$ ... My other problem is that using $7$ in the place of $f(x)$ does NOT give a limit of $262.144$ as it's intended. Any enlightnment would be appreciated! UPDATE: You can really find $f(6)$ by just isolating it. To get $f(6)'$ it's necessary to use the definition of derivatives when ${x\to6}$ . That is: $\lim_{x\to6}\frac{f(x)-f(6)}{x-6}$ . Multiplying both sides by $\frac{1}{x^{3}}$ back in the original limit and using $x=6$ we get to: $$\lim_{x\to6}\frac{f(x)-7}{x-6} = \frac{262.144}{6^{3}}$$ It looks exactly like f(6)'... Since $\frac{262.144}{6^{3}}=1.21$ , that's the derivative wanted. A huge thanks to the people that helped me understand it here.","Let be a differentiable function in so that Find the values of and . The right answers are, respectively 7 and 1.21. This was a problem presented in a quiz I had this week in Calculus I. By my understanding it would suffice to do since the upper part should be just as for the function to exist in . This would return . Doing l'Hôpital's rule and deriving the function makes ... My other problem is that using in the place of does NOT give a limit of as it's intended. Any enlightnment would be appreciated! UPDATE: You can really find by just isolating it. To get it's necessary to use the definition of derivatives when . That is: . Multiplying both sides by back in the original limit and using we get to: It looks exactly like f(6)'... Since , that's the derivative wanted. A huge thanks to the people that helped me understand it here.",{f} x=6 \lim_{x\to6}{\frac{x^{3}f(x)-1512}{x-6}}=262.144 f(6) f(6)' x^{3}f(x)-1512 = 0 0 x-6 x=6 f(6)=7 f(6)'=-2.28 7 f(x) 262.144 f(6) f(6)' {x\to6} \lim_{x\to6}\frac{f(x)-f(6)}{x-6} \frac{1}{x^{3}} x=6 \lim_{x\to6}\frac{f(x)-7}{x-6} = \frac{262.144}{6^{3}} \frac{262.144}{6^{3}}=1.21,"['calculus', 'limits', 'derivatives']"
80,Meaning of derivative of function by absolute value: $\frac{d}{d|x|} f(|x|)$,Meaning of derivative of function by absolute value:,\frac{d}{d|x|} f(|x|),What does the following Leibniz's notation of derivative mean? $$\frac{d}{d|x|} f(|x|)$$ $f(|x|)$ is a function of absolute value of variable x . I am OK with this notation and I know how to treat it: $$\frac{d}{dx} f(|x|)$$ What is the difference between those two?,What does the following Leibniz's notation of derivative mean? is a function of absolute value of variable x . I am OK with this notation and I know how to treat it: What is the difference between those two?,\frac{d}{d|x|} f(|x|) f(|x|) \frac{d}{dx} f(|x|),"['derivatives', 'notation']"
81,Derivative of $\frac{d \int p(x) dx}{d p(x)} $,Derivative of,\frac{d \int p(x) dx}{d p(x)} ,"Suppose $p(x)$ is a probability between 0 and 1 . It looks $$\frac{d \int p(x) dx}{d p(x)} = 1$$ but I find it a bit difficult to convince myself why? I feel influenced by the fact that $\frac{d \int p(x) dx}{dx} = p(x)$ . Can anyone provide a better understanding of the above derivative? In contrast, I find it relatively more intuitive in the case of a discrete random variable, where $$\frac{d \sum_{i} p(x_i) }{d p(x_i)} = 1$$","Suppose is a probability between 0 and 1 . It looks but I find it a bit difficult to convince myself why? I feel influenced by the fact that . Can anyone provide a better understanding of the above derivative? In contrast, I find it relatively more intuitive in the case of a discrete random variable, where",p(x) \frac{d \int p(x) dx}{d p(x)} = 1 \frac{d \int p(x) dx}{dx} = p(x) \frac{d \sum_{i} p(x_i) }{d p(x_i)} = 1,"['calculus', 'integration', 'derivatives']"
82,"$f(x)=\sum_{n=0}^{\infty} \frac{e^{-nx}}{1+n^2}$ is differentiable at $~(0, \infty)$",is differentiable at,"f(x)=\sum_{n=0}^{\infty} \frac{e^{-nx}}{1+n^2} ~(0, \infty)","Let us consider a real valued function $~~f:[0 , \infty) \to \mathbb R~~$ defined by $$f(x)=\sum_{n=0}^{\infty} \frac{e^{-nx}}{1+n^2},~~x \in [0,\infty).$$ Show that $f$ is differentiable at $~(0, \infty)~~$ but $~~\displaystyle \lim_{x \to 0+} f'(x)~$ does not exists. My attempt: By using $~M-$ test I have proved that the series of functions $~~\displaystyle \sum_{n=0}^{\infty} f_n(x)~~$ uniformly convergent to $~~f(x)~~$ as given, where $$f_n(x)=\frac{e^{-nx}}{1+n^2},~~x \in [0,\infty).$$ Then we have $$|f_n(x)| =\left|\frac{e^{-nx}}{1+n^2}\right| \leq \frac{1}{1+n^2}.$$ So uniformly convergent. Hence $~~f'(x)=\displaystyle \sum_{n=0}^{\infty} f'_n(x)=\displaystyle \sum_{n=0}^{\infty} \frac{-ne^{-nx}}{1+n^2}.$ This follows that $~~f(x)~~$ is differentiable on $~(0,\infty).$ Now notice that $$\lim_{x \to 0+}f'(x)=\lim_{x \to 0+} \sum \frac{-ne^{-nx}}{1+n^2} = \sum \left(\lim_{x \to 0+} \frac{-ne^{-nx}}{1+n^2}\right)=-\sum \frac{n}{1+n^2}.$$ Since the above series is not convergent, it yields that the limit does not exists. Is my solution is all okay? Is anything I did wrong or can be solve in much simpler way please suggest me? Thanks for your time to look in my solution.","Let us consider a real valued function defined by Show that is differentiable at but does not exists. My attempt: By using test I have proved that the series of functions uniformly convergent to as given, where Then we have So uniformly convergent. Hence This follows that is differentiable on Now notice that Since the above series is not convergent, it yields that the limit does not exists. Is my solution is all okay? Is anything I did wrong or can be solve in much simpler way please suggest me? Thanks for your time to look in my solution.","~~f:[0 , \infty) \to \mathbb R~~ f(x)=\sum_{n=0}^{\infty} \frac{e^{-nx}}{1+n^2},~~x \in [0,\infty). f ~(0, \infty)~~ ~~\displaystyle \lim_{x \to 0+} f'(x)~ ~M- ~~\displaystyle \sum_{n=0}^{\infty} f_n(x)~~ ~~f(x)~~ f_n(x)=\frac{e^{-nx}}{1+n^2},~~x \in [0,\infty). |f_n(x)| =\left|\frac{e^{-nx}}{1+n^2}\right| \leq \frac{1}{1+n^2}. ~~f'(x)=\displaystyle \sum_{n=0}^{\infty} f'_n(x)=\displaystyle \sum_{n=0}^{\infty} \frac{-ne^{-nx}}{1+n^2}. ~~f(x)~~ ~(0,\infty). \lim_{x \to 0+}f'(x)=\lim_{x \to 0+} \sum \frac{-ne^{-nx}}{1+n^2} = \sum \left(\lim_{x \to 0+} \frac{-ne^{-nx}}{1+n^2}\right)=-\sum \frac{n}{1+n^2}.","['real-analysis', 'sequences-and-series', 'derivatives', 'uniform-convergence', 'sequence-of-function']"
83,How do I find the equation of an envelope?,How do I find the equation of an envelope?,,"I read that you must solve the two equations $$g(x,y,c)=0\\\frac{\partial g}{\partial c}=0$$ for $x$ and $y$ as a function of $c$ , but how exactly do you go about doing this? The specific example I am trying to solve is where $$F(x,y,\alpha)= -t y \sin\alpha \tan{\alpha\over2} - t \sin\alpha  \left(x - r \cos\alpha - t \sin\alpha \tan{\alpha\over2}\right)$$ and find the parametric equation for the envelope ( $r$ and $t$ are constants). BTW, this is from this post if you are curious.","I read that you must solve the two equations for and as a function of , but how exactly do you go about doing this? The specific example I am trying to solve is where and find the parametric equation for the envelope ( and are constants). BTW, this is from this post if you are curious.","g(x,y,c)=0\\\frac{\partial g}{\partial c}=0 x y c F(x,y,\alpha)=
-t y \sin\alpha \tan{\alpha\over2} - t \sin\alpha 
\left(x - r \cos\alpha - t \sin\alpha \tan{\alpha\over2}\right) r t","['geometry', 'derivatives', 'partial-differential-equations', 'partial-derivative', 'envelope']"
84,Two methods of finding out $\frac{d^2y}{dx^2}$ when $\frac{dy}{dt} = 4t-2$ and $\frac{dx}{dt} = 2t + 3$,Two methods of finding out  when  and,\frac{d^2y}{dx^2} \frac{dy}{dt} = 4t-2 \frac{dx}{dt} = 2t + 3,"Find $\frac{d^2y}{dx^2}$ given that $\frac{dy}{dt} = 4t-2$ and $\frac{dx}{dt} = 2t + 3$ at $t=2$ I tried two methods for this question, both of which give me different answers. Which one is wrong and why so? Method $1$ : $\frac{dy}{dt} = 4t-2$ hence $\frac{d^2y}{dt^2} = 4$ , similarly $\frac{d^2x}{dt^2} = 2$ therefore, $\frac{d^2y}{dx^2} = 2$ Method $2$ : $\frac{dy}{dx} = \frac{4t-2}{2t+3}$ therefore $\frac{d^2y}{dx^2} = \frac{(2t+3)4 - (4t-2)2}{(2t+3)^2}.\frac{dt}{dx} = \frac{16}{(2t+3)^2}.\frac{1}{2t+3} = \frac{16}{7^3}$ I can't understand why one method is wrong while the other is right, however they have very different answers. What am I doing wrong?","Find given that and at I tried two methods for this question, both of which give me different answers. Which one is wrong and why so? Method : hence , similarly therefore, Method : therefore I can't understand why one method is wrong while the other is right, however they have very different answers. What am I doing wrong?",\frac{d^2y}{dx^2} \frac{dy}{dt} = 4t-2 \frac{dx}{dt} = 2t + 3 t=2 1 \frac{dy}{dt} = 4t-2 \frac{d^2y}{dt^2} = 4 \frac{d^2x}{dt^2} = 2 \frac{d^2y}{dx^2} = 2 2 \frac{dy}{dx} = \frac{4t-2}{2t+3} \frac{d^2y}{dx^2} = \frac{(2t+3)4 - (4t-2)2}{(2t+3)^2}.\frac{dt}{dx} = \frac{16}{(2t+3)^2}.\frac{1}{2t+3} = \frac{16}{7^3},['derivatives']
85,Is an infinite product of functions differentiable if it's components are?,Is an infinite product of functions differentiable if it's components are?,,"$$ p(\theta) = \left[\lim_{t \to0} p(t) \right] \lim_{n \to \infty}\prod_{i=0}^{n}( \frac{\cos^2 \frac{ \theta}{2^i} +1}{2})$$ I know each term in the product is differentiable but does that mean the total product will be ? I thought of doing a repeated product rule, but   I was a bit worried that maybe due to the limit, the differentiability would be effected.","I know each term in the product is differentiable but does that mean the total product will be ? I thought of doing a repeated product rule, but   I was a bit worried that maybe due to the limit, the differentiability would be effected.", p(\theta) = \left[\lim_{t \to0} p(t) \right] \lim_{n \to \infty}\prod_{i=0}^{n}( \frac{\cos^2 \frac{ \theta}{2^i} +1}{2}),['derivatives']
86,Double derivative in parametric form,Double derivative in parametric form,,"Let there be two functions expressed in the form of a parametric variable, $y=f(t)$ and $x=g(t)$ and I have find the second derivative of $y$ with respect to $x$ . To do that, I have done as shown $$\frac{d^2y}{dx^2}= \frac{d}{dt}(\frac{dy}{dt})×(\frac{dt}{dx})^2$$ $$\frac{d^2y}{dx^2} = \frac{d^2y}{dt^2} \biggm/\left(\frac{dx}{dt}\right)^2$$ But I am not getting the correct answer and I don't know what is the problem with this. I want to know if I have done something wrong in the above procedure?","Let there be two functions expressed in the form of a parametric variable, and and I have find the second derivative of with respect to . To do that, I have done as shown But I am not getting the correct answer and I don't know what is the problem with this. I want to know if I have done something wrong in the above procedure?",y=f(t) x=g(t) y x \frac{d^2y}{dx^2}= \frac{d}{dt}(\frac{dy}{dt})×(\frac{dt}{dx})^2 \frac{d^2y}{dx^2} = \frac{d^2y}{dt^2} \biggm/\left(\frac{dx}{dt}\right)^2,"['derivatives', 'parametric']"
87,"Showing $F(x+h)=F(x)+hF'(x)+\frac{h^2}{2}F''(x)+h^2\phi (h)$, not with Taylor series.","Showing , not with Taylor series.",F(x+h)=F(x)+hF'(x)+\frac{h^2}{2}F''(x)+h^2\phi (h),"The book I am studying, has this question, show $F(x+h)=F(x)+hF'(x)+\frac{h^2}{2}F''(x)+h^2\phi (h)$ where $\phi(h) \to 0$ as $h\to 0$ . It is noted that this is a Taylor expansion, but the method suggested was to use $F(x+h)-F(x)=\int_x^{x+h}F'(y)dy$ and $F'(y)=F'(x)+(y-x)F''(x)+(y-x)\psi(y-x)$ where $\psi(h) \to 0$ as $h\to 0$ . I tried it and am having trouble at the last step. \begin{align*} F(x+h)-F(x)&=\int_x^{x+h}F'(y)dy\\ &= \int_x^{x+h}F'(x)+(y-x)F''(x)+(y-x)\psi(y-x)dy\\ &=hF'(x)+\frac{h^2}{2}F''(x)+\int_x^{x+h}(y-x)\psi(y-x)dy\\&=hF'(x)+\frac{h^2}{2}F''(x)+\int_0^{h}k\psi(k)dk  \end{align*} How do I finish it off, i.e. how do I show $\int_0^{h}k\psi(k)dk=h^2\phi (h)$ . Thanks","The book I am studying, has this question, show where as . It is noted that this is a Taylor expansion, but the method suggested was to use and where as . I tried it and am having trouble at the last step. How do I finish it off, i.e. how do I show . Thanks",F(x+h)=F(x)+hF'(x)+\frac{h^2}{2}F''(x)+h^2\phi (h) \phi(h) \to 0 h\to 0 F(x+h)-F(x)=\int_x^{x+h}F'(y)dy F'(y)=F'(x)+(y-x)F''(x)+(y-x)\psi(y-x) \psi(h) \to 0 h\to 0 \begin{align*} F(x+h)-F(x)&=\int_x^{x+h}F'(y)dy\\ &= \int_x^{x+h}F'(x)+(y-x)F''(x)+(y-x)\psi(y-x)dy\\ &=hF'(x)+\frac{h^2}{2}F''(x)+\int_x^{x+h}(y-x)\psi(y-x)dy\\&=hF'(x)+\frac{h^2}{2}F''(x)+\int_0^{h}k\psi(k)dk  \end{align*} \int_0^{h}k\psi(k)dk=h^2\phi (h),"['derivatives', 'taylor-expansion']"
88,Verify nth derivative satisfies differential equation,Verify nth derivative satisfies differential equation,,"I've been working my way through  an old series of maths books (An Analytical Calculus by Maxwell) and finally got stuck on a question midway through book 2 (of 4). If anyone could help that would be great (as there are quite a few like this following). The question is in two parts and I can get the first part. I figure that the first part is to be used in the solution of the second part (but could be wrong). We haven't covered ODEs yet, so any methods using theorems from ODEs shouldn't be used. a) Prove that $$ (x+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^{n+1}~\left\{(x+1)^n(x-1)^{n+1}\right\} =(n+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^n\left\{(x+1)^{n+1}(x-1)^n\right\}  $$ --> I can do this part. b) Prove also that the function $$ \left(\frac{\mathrm d}{\mathrm d x}\right)^n~{(x+1)^{n+1}(x-1)^n} $$ Satisfies the equation: $$ (1-x^2)\left(\frac{\mathrm d}{\mathrm d x}\right)^2y-(1+x)\left(\frac{\mathrm d y}{\mathrm d x}\right)+(n+1)^2y=0 $$ --> I can't do this part. Worst case I thought I could do it via a recurrence relation between terms in a power series in the DE, and then check the function satisfies it, but it gets messy fast. I can't find any similar solved problems anywhere! The first part is solved via Leibniz: $$ (x+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^{n+1}{(x+1)^n(x-1)^{n+1}}=\left(\frac{\mathrm d}{\mathrm d x}\right)^{n+1}{(x+1)^{n+1}(x-1)^{n+1}}-(n+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^n{(x+1)^n(x-1)^{n+1}} =\left(\frac{\mathrm d}{\mathrm d x}\right)^n(n+1){(x+1)^{n+1}(x-1)^n}+\left(\frac{\mathrm d}{\mathrm d x}\right)^n(n+1){(x+1)^n(x-1)^{n+1}}-(n+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^n{(x+1)^n(x-1)^{n+1}}=\left(\frac{\mathrm d}{\mathrm d x}\right)^n(n+1){(x+1)^{n+1}(x-1)^n} $$","I've been working my way through  an old series of maths books (An Analytical Calculus by Maxwell) and finally got stuck on a question midway through book 2 (of 4). If anyone could help that would be great (as there are quite a few like this following). The question is in two parts and I can get the first part. I figure that the first part is to be used in the solution of the second part (but could be wrong). We haven't covered ODEs yet, so any methods using theorems from ODEs shouldn't be used. a) Prove that --> I can do this part. b) Prove also that the function Satisfies the equation: --> I can't do this part. Worst case I thought I could do it via a recurrence relation between terms in a power series in the DE, and then check the function satisfies it, but it gets messy fast. I can't find any similar solved problems anywhere! The first part is solved via Leibniz:","
(x+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^{n+1}~\left\{(x+1)^n(x-1)^{n+1}\right\}
=(n+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^n\left\{(x+1)^{n+1}(x-1)^n\right\} 
 
\left(\frac{\mathrm d}{\mathrm d x}\right)^n~{(x+1)^{n+1}(x-1)^n}
 
(1-x^2)\left(\frac{\mathrm d}{\mathrm d x}\right)^2y-(1+x)\left(\frac{\mathrm d y}{\mathrm d x}\right)+(n+1)^2y=0
 
(x+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^{n+1}{(x+1)^n(x-1)^{n+1}}=\left(\frac{\mathrm d}{\mathrm d x}\right)^{n+1}{(x+1)^{n+1}(x-1)^{n+1}}-(n+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^n{(x+1)^n(x-1)^{n+1}}
=\left(\frac{\mathrm d}{\mathrm d x}\right)^n(n+1){(x+1)^{n+1}(x-1)^n}+\left(\frac{\mathrm d}{\mathrm d x}\right)^n(n+1){(x+1)^n(x-1)^{n+1}}-(n+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^n{(x+1)^n(x-1)^{n+1}}=\left(\frac{\mathrm d}{\mathrm d x}\right)^n(n+1){(x+1)^{n+1}(x-1)^n}
","['calculus', 'derivatives']"
89,Different answers of same differentiation Question with two different methods,Different answers of same differentiation Question with two different methods,,"Question:- Find $\frac{dy}{dx}$ if $$\arccos\bigg({\frac{x^2-y^2}{x^2+y^2}}\bigg)=\arctan( a)$$ $$\arccos\bigg({\frac{x^2-y^2}{x^2+y^2}}\bigg)=\arctan( a)$$ $$\implies  \frac{x^2-y^2}{x^2+y^2}=\cos(\arctan(a))$$ Taking derivative on both sides w.r.t $\space x$ , we get $\frac{dy}{dx}=\frac{y}{x}$ Out of fun, I tried the substitution $y^2=x^2\cos(\theta)$ and got $$\frac{1-\cos(\theta)}{1+\cos(\theta)}=\cos(\arctan(a))$$ for $x\ne 0$ $$\implies \tan^2{\frac{\theta}{2}}=\cos(\arctan(a))$$ Differentiate both sides w.r.t $\space \theta$ , we get $$\tan{\frac{\theta}{2}}\bigg(1+\tan^2{\frac{\theta}{2}}\bigg)=0$$ $$\implies \tan{\frac{\theta}{2}}=0$$ $$\cos(\theta)=\frac{1-\tan^2{\frac{\theta}{2}}}{1+\tan^2{\frac{\theta}{2}}}=1$$ As $y^2=x^2\cos(\theta)$ ,this gives $y^2=x^2$ and $\frac{dy}{dx}=\pm 1$ Which contradicts the other method, So What's wrong with the $2^{nd}$ method?","Question:- Find if Taking derivative on both sides w.r.t , we get Out of fun, I tried the substitution and got for Differentiate both sides w.r.t , we get As ,this gives and Which contradicts the other method, So What's wrong with the method?",\frac{dy}{dx} \arccos\bigg({\frac{x^2-y^2}{x^2+y^2}}\bigg)=\arctan( a) \arccos\bigg({\frac{x^2-y^2}{x^2+y^2}}\bigg)=\arctan( a) \implies  \frac{x^2-y^2}{x^2+y^2}=\cos(\arctan(a)) \space x \frac{dy}{dx}=\frac{y}{x} y^2=x^2\cos(\theta) \frac{1-\cos(\theta)}{1+\cos(\theta)}=\cos(\arctan(a)) x\ne 0 \implies \tan^2{\frac{\theta}{2}}=\cos(\arctan(a)) \space \theta \tan{\frac{\theta}{2}}\bigg(1+\tan^2{\frac{\theta}{2}}\bigg)=0 \implies \tan{\frac{\theta}{2}}=0 \cos(\theta)=\frac{1-\tan^2{\frac{\theta}{2}}}{1+\tan^2{\frac{\theta}{2}}}=1 y^2=x^2\cos(\theta) y^2=x^2 \frac{dy}{dx}=\pm 1 2^{nd},"['calculus', 'derivatives']"
90,Derive a new equation from $m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}}$,Derive a new equation from,m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}},"$$m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}}$$ $$\implies m^2=\frac{m_0^2}{1-\frac{v^2}{c^2}}$$ $$\implies m^2c^2-m^2v^2=m_0^2c^2$$ Differentiating the equation, $$2m \;dm\;c^2-2m\;dm\;v^2-2v\;dv\;m_0^2=0$$ Our book says when we differentiate $m^2c^2-m^2v^2=m_0^2c^2$ . We will get the above equation. But, what I understand about Calculus. That I can't derive it anyway. Actually, what we differentiate here? $mass$ or, $velocity$ ? Relativistic mass equation : $$m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}}$$ Above question is a constant equation which maybe found from Lorentz Transformation. In the second line I’ve just squared both side. In third line I‘ve just moved ""something"" right to left. Then, I differentiate. But, I can't understand how they differentiate here. After writing the question, I was doing some the sum again. Relativistic mass changes over time (How fast you travel through space your mass decreases). Here $m$ is relativistic mass and $m_0$ is ""normal"" mass. So, I decided to differentiate mass $$(mc)^2-(mv)^2-(m_0c)^2=0$$ We know, $$(f(x))^n=n(f(x))^{n-1} . f' (x)$$ Then : $$2 (mc) . c - 2 (mv) .v -$$ Then, I can't differentiate anymore. $c$ is speed of light which is constant. That $m_0$ is also constant. So, I stopped there. Prove of $E=mc^2$ , $$F=\frac{dp}{dt}$$ $$=\frac{d}{dt} (mv)$$ $$= m \frac{dv}{dt} + v \frac{dm}{dt}$$ ----------------1 $$dW=F .dS$$ $$=>dK=F .dS$$ $$=>dK=[m \frac{dv}{dt} +v \frac{dm}{dt}] .dS$$ $$=m\frac{dS}{dt} .dv + v . \frac{dS}{dt} .dm$$ $$=mvdv+v^2dm$$ -------------------2 $$m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}}$$ $$\implies m^2=\frac{m_0^2}{1-\frac{v^2}{c^2}}$$ $$\implies m^2c^2-m^2v^2=m_0^2c^2$$ $$2m \;dm\;c^2-2m\;dm\;v^2-2v\;dv\;m_0^2=0$$ $$c^2dm=mvdv+v^2dm$$ ---------------3 $$dK=c^2dm$$ $$\int = \int c^2dm$$ (In LHS Integral starts from 0 and finishes at $k$ . In RHS Integral starts from $m_0$ and ends at $m$ ) $$k=c^2[m-m_0]$$ Hence, $$Total energy = k + Rest mass energy$$ $$E=c^2[m-m_0]+m_0c^2$$ $$=mc^2-m_0c^2+m_0c^2$$ $$=mc^2$$ $$E=mc^2$$","Differentiating the equation, Our book says when we differentiate . We will get the above equation. But, what I understand about Calculus. That I can't derive it anyway. Actually, what we differentiate here? or, ? Relativistic mass equation : Above question is a constant equation which maybe found from Lorentz Transformation. In the second line I’ve just squared both side. In third line I‘ve just moved ""something"" right to left. Then, I differentiate. But, I can't understand how they differentiate here. After writing the question, I was doing some the sum again. Relativistic mass changes over time (How fast you travel through space your mass decreases). Here is relativistic mass and is ""normal"" mass. So, I decided to differentiate mass We know, Then : Then, I can't differentiate anymore. is speed of light which is constant. That is also constant. So, I stopped there. Prove of , ----------------1 -------------------2 ---------------3 (In LHS Integral starts from 0 and finishes at . In RHS Integral starts from and ends at ) Hence,",m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}} \implies m^2=\frac{m_0^2}{1-\frac{v^2}{c^2}} \implies m^2c^2-m^2v^2=m_0^2c^2 2m \;dm\;c^2-2m\;dm\;v^2-2v\;dv\;m_0^2=0 m^2c^2-m^2v^2=m_0^2c^2 mass velocity m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}} m m_0 (mc)^2-(mv)^2-(m_0c)^2=0 (f(x))^n=n(f(x))^{n-1} . f' (x) 2 (mc) . c - 2 (mv) .v - c m_0 E=mc^2 F=\frac{dp}{dt} =\frac{d}{dt} (mv) = m \frac{dv}{dt} + v \frac{dm}{dt} dW=F .dS =>dK=F .dS =>dK=[m \frac{dv}{dt} +v \frac{dm}{dt}] .dS =m\frac{dS}{dt} .dv + v . \frac{dS}{dt} .dm =mvdv+v^2dm m=\frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}} \implies m^2=\frac{m_0^2}{1-\frac{v^2}{c^2}} \implies m^2c^2-m^2v^2=m_0^2c^2 2m \;dm\;c^2-2m\;dm\;v^2-2v\;dv\;m_0^2=0 c^2dm=mvdv+v^2dm dK=c^2dm \int = \int c^2dm k m_0 m k=c^2[m-m_0] Total energy = k + Rest mass energy E=c^2[m-m_0]+m_0c^2 =mc^2-m_0c^2+m_0c^2 =mc^2 E=mc^2,[]
91,Find the associated matrix to a linear transformation,Find the associated matrix to a linear transformation,,"Let $D:\mathbb{P}_3[x]\to\mathbb{P}_2[x]$ be the linear transformation $f(x)\rightarrow f'(x)$ . I.e, the derivative of $f(x)$ . $\mathbb{P}_n[x]$ denotes the vector space of real polynomials of degree less or equal to $n$ . Find the associated matrix to $D$ with respect to the bases: $$\mathcal{B}=\{1,x,x^2,x^3\}  ~~\text{and}~~  \mathcal{C}=\{1,x,x^2\}$$ Find the associated matrix to $D$ with respect to the bases $\mathcal{B}'$ of $\mathbb{P}_3[x]$ and $\mathcal{C}'$ of $\mathbb{P}_2[x]$ $$\mathcal{B}'=\{x^3+1,x^2+x,x^2-x,x^3-1\}$$ and $$\mathcal{C}'=\{\frac{1}{2}x(x+1),1-x^2,\frac{1}{2}x(x-1)\}.$$ I solved part 1. and I obtained  the associated matrix, which is $$\begin{pmatrix} 0 & 1 & 0 & 0\\ 0 & 0 & 2 & 0\\ 0 & 0 & 0 & 3 \end{pmatrix}$$ However, I couldn't solve part 2. I don't really know how to proceed. Any help will be greatly appreciated.","Let be the linear transformation . I.e, the derivative of . denotes the vector space of real polynomials of degree less or equal to . Find the associated matrix to with respect to the bases: Find the associated matrix to with respect to the bases of and of and I solved part 1. and I obtained  the associated matrix, which is However, I couldn't solve part 2. I don't really know how to proceed. Any help will be greatly appreciated.","D:\mathbb{P}_3[x]\to\mathbb{P}_2[x] f(x)\rightarrow f'(x) f(x) \mathbb{P}_n[x] n D \mathcal{B}=\{1,x,x^2,x^3\}  ~~\text{and}~~  \mathcal{C}=\{1,x,x^2\} D \mathcal{B}' \mathbb{P}_3[x] \mathcal{C}' \mathbb{P}_2[x] \mathcal{B}'=\{x^3+1,x^2+x,x^2-x,x^3-1\} \mathcal{C}'=\{\frac{1}{2}x(x+1),1-x^2,\frac{1}{2}x(x-1)\}. \begin{pmatrix}
0 & 1 & 0 & 0\\
0 & 0 & 2 & 0\\
0 & 0 & 0 & 3
\end{pmatrix}","['linear-algebra', 'matrices', 'derivatives', 'polynomials', 'linear-transformations']"
92,Why is there this connection between Euler's number and $1/x$?,Why is there this connection between Euler's number and ?,1/x,I've seen plenty of easy to understand proofs showing that $\frac{dy}{dx}\ln(x)=\frac{1}{x}$ but what I'm looking for is a way to intuitively justify why $n=-1$ is an exception to the power rule for integration (besides the fact that it results in a $\frac{1}{0}$ ).  Why is there this connection between $e$ and $1/x$ ?,I've seen plenty of easy to understand proofs showing that but what I'm looking for is a way to intuitively justify why is an exception to the power rule for integration (besides the fact that it results in a ).  Why is there this connection between and ?,\frac{dy}{dx}\ln(x)=\frac{1}{x} n=-1 \frac{1}{0} e 1/x,"['calculus', 'integration', 'derivatives', 'eulers-number-e']"
93,When does the equation $f(x)=x\cdot\sqrt{2x+6}=ax$ have 2 real solutions?,When does the equation  have 2 real solutions?,f(x)=x\cdot\sqrt{2x+6}=ax,"My question is the following: Let $f\colon[-3,\infty)\to \mathbf{R}$ with $f(x)=x\sqrt{2x+6}$ . Is there a value $a\in\mathbf{R}$ for which $f(x)=ax$ has 2 real solutions? I tried the following: With some caculations we find $f'(x)=\frac{3x+6}{\sqrt{2x+6}}$ . Since $(0,0)$ is on the line $y=ax$ , we find $f'(0)=\sqrt{6}$ and therefore a = $\sqrt{6}$ . By using the schetch of the graph I find the answer $a\in[0,\sqrt{6}$ ). I feel this is kind of cheating, since I used the graph and 'cleverly guessed' the value of $a=\sqrt{6}$ . Is there a more mathematical rigous way to solve this equation? Any help is greatly valued.","My question is the following: Let with . Is there a value for which has 2 real solutions? I tried the following: With some caculations we find . Since is on the line , we find and therefore a = . By using the schetch of the graph I find the answer ). I feel this is kind of cheating, since I used the graph and 'cleverly guessed' the value of . Is there a more mathematical rigous way to solve this equation? Any help is greatly valued.","f\colon[-3,\infty)\to \mathbf{R} f(x)=x\sqrt{2x+6} a\in\mathbf{R} f(x)=ax f'(x)=\frac{3x+6}{\sqrt{2x+6}} (0,0) y=ax f'(0)=\sqrt{6} \sqrt{6} a\in[0,\sqrt{6} a=\sqrt{6}","['real-analysis', 'calculus', 'derivatives', 'solution-verification']"
94,Theorem about series of functions,Theorem about series of functions,,"If both series $\sum_{n=1}^{\infty} f_{\mathrm{n}}(x)$ and $\sum_{n=1}^{\infty} f_{\mathrm{n}}^{\prime}(x)$ converge uniformly on I and $f_{n}^{\prime}$ is continuous for every $n \in \mathbb{N}$ , then the function $f(x) = \sum_{n=1}^{\infty}f_{n}(x)$ is differantiable on I and we have $f^{\prime}(x)=\sum_{n=1}^{\infty}f_{n}^{\prime}(x)$ My Work: We want to prove that $f^{\prime}(x) =\displaystyle \sum_{n=1}^{\infty}f_{n}^{\prime}(x)$ in other words $\displaystyle\lim_{n \to \infty} f_{n}(x) = \displaystyle\sum_{n=1}^{\infty}f_{n}(x)$ we know that $\displaystyle\sum_{n=1}^{\infty}f_{n}(x)$ converge uniformly. Then $\displaystyle\lim_{n \to \infty}f_{n}(x)=f(x)$ exist. Also we can say that from the information given in question, $f_{n}$ is differantiable because $f_{n}^{\prime}$ is continuos. And from above we can say that $\displaystyle\lim_{n \to \infty}f_{n}^{\prime}=f^{\prime}$ since $f_{n}^{\prime}$ is continuous. Therefore, that is, $f^{\prime}=\displaystyle\sum_{n=1}^{\infty}f_{n}^{\prime}$ . Does it sufficient to prove? I am not sure...","If both series and converge uniformly on I and is continuous for every , then the function is differantiable on I and we have My Work: We want to prove that in other words we know that converge uniformly. Then exist. Also we can say that from the information given in question, is differantiable because is continuos. And from above we can say that since is continuous. Therefore, that is, . Does it sufficient to prove? I am not sure...",\sum_{n=1}^{\infty} f_{\mathrm{n}}(x) \sum_{n=1}^{\infty} f_{\mathrm{n}}^{\prime}(x) f_{n}^{\prime} n \in \mathbb{N} f(x) = \sum_{n=1}^{\infty}f_{n}(x) f^{\prime}(x)=\sum_{n=1}^{\infty}f_{n}^{\prime}(x) f^{\prime}(x) =\displaystyle \sum_{n=1}^{\infty}f_{n}^{\prime}(x) \displaystyle\lim_{n \to \infty} f_{n}(x) = \displaystyle\sum_{n=1}^{\infty}f_{n}(x) \displaystyle\sum_{n=1}^{\infty}f_{n}(x) \displaystyle\lim_{n \to \infty}f_{n}(x)=f(x) f_{n} f_{n}^{\prime} \displaystyle\lim_{n \to \infty}f_{n}^{\prime}=f^{\prime} f_{n}^{\prime} f^{\prime}=\displaystyle\sum_{n=1}^{\infty}f_{n}^{\prime},"['real-analysis', 'derivatives', 'solution-verification', 'sequence-of-function']"
95,How can I find the relative minimum and maximum point as $f(x)$ is equal to zero?,How can I find the relative minimum and maximum point as  is equal to zero?,f(x),How can I find the relative minimum and maximum points of $f(x)=x^3-\frac{3}{2}x^2$ . This is what I found so far: \begin{align*}  f(x)&=x^3-\frac{3}{2}x^2  \\ f'(x)&= 3x^2-3x  \\ f''(x)&=6x-3 \end{align*} As the critical points are $0$ and $1$ .,How can I find the relative minimum and maximum points of . This is what I found so far: As the critical points are and .,"f(x)=x^3-\frac{3}{2}x^2 \begin{align*}
 f(x)&=x^3-\frac{3}{2}x^2 
\\ f'(x)&= 3x^2-3x 
\\ f''(x)&=6x-3
\end{align*} 0 1","['calculus', 'derivatives']"
96,Simple differential-difference equation [duplicate],Simple differential-difference equation [duplicate],,"This question already has answers here : When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? (5 answers) Closed 3 years ago . I was doing some exam practice when I almost surely went the wrong way in my method and ended up with the following equation: $$f'(x) = f(x) - f(x-1)$$ Although not what I was intended to find, I'm still curious to know if there are methods to solve this. We can spot the obvious solution $f(x) = Ax+B$ , but I could not find a way to prove this was the only solution, or find any other solutions. Any insight would be helpful. For context I am a first-year undergrad.","This question already has answers here : When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? (5 answers) Closed 3 years ago . I was doing some exam practice when I almost surely went the wrong way in my method and ended up with the following equation: Although not what I was intended to find, I'm still curious to know if there are methods to solve this. We can spot the obvious solution , but I could not find a way to prove this was the only solution, or find any other solutions. Any insight would be helpful. For context I am a first-year undergrad.",f'(x) = f(x) - f(x-1) f(x) = Ax+B,"['derivatives', 'recurrence-relations', 'delay-differential-equations']"
97,taylor series for inverse of error function,taylor series for inverse of error function,,"I was asked the following question: Determine the Taylor Series degree 3 around $0$ of the inverse function of $erf(x)$ . I took the first derivative of the function $erf'(x) = \frac{2}{\sqrt{\pi}} e^{-x^2}$ . When $erf(x)=0 \implies x=0$ , thus I would have that $erf'(0)=\frac{2}{\sqrt{\pi}}$ . The derivative of the inverse function is given by the $\frac{1}{erf'(0)}=\frac{\sqrt{\pi}}{2}$ and the first term of the Taylor Series is $\frac{\sqrt{\pi}}{2} x$ . How would I proceed to get the 2nd and 3rd one. For some reason, I got a diferent 3rd derivative than what appears in the solution, and I can't understand why, specifically, why a $\pi^{\frac{3}{2}}$ appears in the 3rd derivative.","I was asked the following question: Determine the Taylor Series degree 3 around of the inverse function of . I took the first derivative of the function . When , thus I would have that . The derivative of the inverse function is given by the and the first term of the Taylor Series is . How would I proceed to get the 2nd and 3rd one. For some reason, I got a diferent 3rd derivative than what appears in the solution, and I can't understand why, specifically, why a appears in the 3rd derivative.",0 erf(x) erf'(x) = \frac{2}{\sqrt{\pi}} e^{-x^2} erf(x)=0 \implies x=0 erf'(0)=\frac{2}{\sqrt{\pi}} \frac{1}{erf'(0)}=\frac{\sqrt{\pi}}{2} \frac{\sqrt{\pi}}{2} x \pi^{\frac{3}{2}},"['calculus', 'derivatives', 'inverse-function']"
98,"Let $f,g$ be differentiable and continuous functions on $[a,b]$, and let $f(a)=f(b)=0$, do we always have $c \in (a,b)$ s.t $g'(c)f(c)+f'(c)=0$?","Let  be differentiable and continuous functions on , and let , do we always have  s.t ?","f,g [a,b] f(a)=f(b)=0 c \in (a,b) g'(c)f(c)+f'(c)=0","I tried to prove this with the fact "" $f(x)g(x)=h(x)=0$ "" when $x=a,b$ . So, by Rolle's theorem, we know there exists $c \in (a,b)$ s.t $h'(c)=f'(c)g(c)+f(c)g'(c)=0$ . Then, for such $c$ , $g'(c)f(c)=-f'(c)g(c)$ , so $f'(c)+g'(c)f(c)=f'(c)(1-g(c))$ . But, if we say $f'(c)(1-g(c))=0$ is true, then this implies $f'(c)=0$ because we have no information for $g(x)$ . Here is my question: [If we know that $f(a)=f(b)=0$ , then we know there exists some $c_1 \in (a,b)$ s.t $f'(c_1)=0$ . Combining with this fact, I wonder if this $c_1$ is guaranteed to be equal to $c$ from above?]","I tried to prove this with the fact "" "" when . So, by Rolle's theorem, we know there exists s.t . Then, for such , , so . But, if we say is true, then this implies because we have no information for . Here is my question: [If we know that , then we know there exists some s.t . Combining with this fact, I wonder if this is guaranteed to be equal to from above?]","f(x)g(x)=h(x)=0 x=a,b c \in (a,b) h'(c)=f'(c)g(c)+f(c)g'(c)=0 c g'(c)f(c)=-f'(c)g(c) f'(c)+g'(c)f(c)=f'(c)(1-g(c)) f'(c)(1-g(c))=0 f'(c)=0 g(x) f(a)=f(b)=0 c_1 \in (a,b) f'(c_1)=0 c_1 c","['real-analysis', 'derivatives']"
99,Seeking an example of continuous function that has no integrable derivative,Seeking an example of continuous function that has no integrable derivative,,"I am seeking for a function $[0,1]\to \mathbb{R}$ that is continuous and has derivative almost everywhere, but this derivative is not integrable niether on sense of improper integrals. For instance, $f(x)=\sqrt{x}$ is s.t. $f'(x)=\dfrac{1}{2\sqrt{x}}$ and $f'$ is Lebesgue integrable. I am seeking an example s.t. $f'(x)$ is not Lebesgue integrable. This is the difference on the question What is an example that a function is differentiable but derivative is not Riemann integrable Thank you in advance.","I am seeking for a function that is continuous and has derivative almost everywhere, but this derivative is not integrable niether on sense of improper integrals. For instance, is s.t. and is Lebesgue integrable. I am seeking an example s.t. is not Lebesgue integrable. This is the difference on the question What is an example that a function is differentiable but derivative is not Riemann integrable Thank you in advance.","[0,1]\to \mathbb{R} f(x)=\sqrt{x} f'(x)=\dfrac{1}{2\sqrt{x}} f' f'(x)","['measure-theory', 'derivatives', 'continuity', 'examples-counterexamples']"
