,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solve $x(dy/dx) = x^4y^3 - y $ (Bernoulli's equations),Solve  (Bernoulli's equations),x(dy/dx) = x^4y^3 - y ,Use method of Bernoulli's equations to solve the equation: $$ x(dy/dx) = x^4y^3 - y $$ I don't really can understand how to use Bernoulli's equations as I had it only once and I wasn't able to understand it then. Can someone guide me through the way of solving?,Use method of Bernoulli's equations to solve the equation: $$ x(dy/dx) = x^4y^3 - y $$ I don't really can understand how to use Bernoulli's equations as I had it only once and I wasn't able to understand it then. Can someone guide me through the way of solving?,,"['calculus', 'ordinary-differential-equations']"
1,"General solution of $\frac{dy}{dx}+y=1$, $(y\neq 1)$","General solution of ,",\frac{dy}{dx}+y=1 (y\neq 1),"Solve $\frac{dy}{dx}+y=1$, $(y\neq 1)$ The general solution for this differential equation is given in my reference as $y=1+Ae^{-x}$, but is it a complete solution ? My Attempt $$ \frac{dy}{dx}=1-y\implies \frac{dy}{1-y}=dx\\ \int\frac{dy}{1-y}=\int dx\implies\int\frac{dy}{y-1}=-\int dx\\ \log|y-1|=-x+C_1\implies |y-1|=e^{-x}.e^{C_1}\\ |y-1|=A.e^{-x}\\ y=\begin{cases}1+A.e^{-x};\quad y\geq1\\ 1-A.e^{-x};\quad y<1 \end{cases} $$ right ?","Solve $\frac{dy}{dx}+y=1$, $(y\neq 1)$ The general solution for this differential equation is given in my reference as $y=1+Ae^{-x}$, but is it a complete solution ? My Attempt $$ \frac{dy}{dx}=1-y\implies \frac{dy}{1-y}=dx\\ \int\frac{dy}{1-y}=\int dx\implies\int\frac{dy}{y-1}=-\int dx\\ \log|y-1|=-x+C_1\implies |y-1|=e^{-x}.e^{C_1}\\ |y-1|=A.e^{-x}\\ y=\begin{cases}1+A.e^{-x};\quad y\geq1\\ 1-A.e^{-x};\quad y<1 \end{cases} $$ right ?",,"['integration', 'ordinary-differential-equations', 'exponential-function']"
2,Is the solution of $ u_{tt}=c^2u_{xx}+xt $ correct?,Is the solution of  correct?, u_{tt}=c^2u_{xx}+xt ,"Consider the following $ u_{tt}=c^2u_{xx}+xt,\\ u(x,0)=0,\\ u_t(x,0)=\sin (x)$ and find the solution. Solution. We have that $u(x,t)$ is given by $$u(x,t)=\frac{1}{2}(g(x+ct)+g(x-ct))+\frac{1}{2c}\int_{x-ct}^{x+ct}h(u)du+\frac{1}{2c}\iint_\Delta f(y,s)dA,\cdots\cdots (1)$$ where $\Delta $ is a triangle with vertex $(x-ct,0),(x+ct,0),(x,t)$. Thus in our case, $u(x,t)=\frac{1}{2c}\int_{x-ct}^{x+ct}\sin(u)du+\frac{1}{2c}\iint_\Delta xt \ dA.$ Using Green Theorem we have that $\frac{1}{2c}\iint_\Delta xt \ dA=\frac{1}{2}[\cos(x_0+ct)-\cos(x_0-ct)-2cu(x_0,t_0)],$ where I took a point $(x_0,t_0)$ to be able to integrate  using the theorem. and $\frac{1}{2c}\int_{x-ct}^{x+ct}\sin(u)du=\frac{1}{2}[-\cos(x_0+ct_0)+\cos(x_0-ct_0)]$ Hence the final solution is $u(x,t)=\frac{1}{2}[\cos(x_0+ct_0)-\cos(x_0-ct_0)]+\frac{1}{2}[\cos(x_0+ct)-\cos(x_0-ct)-2cu(x_0,t_0)]$ Is it correct? This is the first time I use formula (1), so if something can be improved don't hesitate to suggest. Edit Apparently the solution is wrong because I had to calculate the double integral over the area of the triangle and not the contour integral, that's what my professor said, but I don't know why, I didn't understand why. Could someone explain why is that? I still don't see why application of Green theorem it's not correct in the double integral.","Consider the following $ u_{tt}=c^2u_{xx}+xt,\\ u(x,0)=0,\\ u_t(x,0)=\sin (x)$ and find the solution. Solution. We have that $u(x,t)$ is given by $$u(x,t)=\frac{1}{2}(g(x+ct)+g(x-ct))+\frac{1}{2c}\int_{x-ct}^{x+ct}h(u)du+\frac{1}{2c}\iint_\Delta f(y,s)dA,\cdots\cdots (1)$$ where $\Delta $ is a triangle with vertex $(x-ct,0),(x+ct,0),(x,t)$. Thus in our case, $u(x,t)=\frac{1}{2c}\int_{x-ct}^{x+ct}\sin(u)du+\frac{1}{2c}\iint_\Delta xt \ dA.$ Using Green Theorem we have that $\frac{1}{2c}\iint_\Delta xt \ dA=\frac{1}{2}[\cos(x_0+ct)-\cos(x_0-ct)-2cu(x_0,t_0)],$ where I took a point $(x_0,t_0)$ to be able to integrate  using the theorem. and $\frac{1}{2c}\int_{x-ct}^{x+ct}\sin(u)du=\frac{1}{2}[-\cos(x_0+ct_0)+\cos(x_0-ct_0)]$ Hence the final solution is $u(x,t)=\frac{1}{2}[\cos(x_0+ct_0)-\cos(x_0-ct_0)]+\frac{1}{2}[\cos(x_0+ct)-\cos(x_0-ct)-2cu(x_0,t_0)]$ Is it correct? This is the first time I use formula (1), so if something can be improved don't hesitate to suggest. Edit Apparently the solution is wrong because I had to calculate the double integral over the area of the triangle and not the contour integral, that's what my professor said, but I don't know why, I didn't understand why. Could someone explain why is that? I still don't see why application of Green theorem it's not correct in the double integral.",,"['ordinary-differential-equations', 'partial-differential-equations']"
3,How to derive an equation for terminal velocity assuming air resistance is some constant multiplied by the square of velocity?,How to derive an equation for terminal velocity assuming air resistance is some constant multiplied by the square of velocity?,,"So for my latest physics homework question, I had to derive an equation for the terminal velocity of a ball falling in some gravitational field assuming that the air resistance force was equal to some constant c multiplied by $v^2.$ So first I started with the differntial equation: $\frac{dv}{dt}=-mg-cv^2$ Rearranging to get: $\frac{dv}{dt}=-\left(g+\frac{cv^2}{m}\right)$ From here I tried solving it and ended up with: $\frac{\sqrt{m}}{\sqrt{c}\sqrt{g}}\arctan \left(\frac{\sqrt{c}v}{\sqrt{g}\sqrt{m}}\right)+C=-t$ I rearranged this to get: $v\left(t\right)=\left(\frac{\sqrt{g}\sqrt{m}\tan \left(\frac{\left(-C\sqrt{c}\sqrt{g}-\sqrt{c}\sqrt{g}t\right)}{\sqrt{m}}\right)}{\sqrt{c}}\right)$ In order to calculate the terminal velocity I took the limit as t approaches infinity: $\lim _{t\to \infty }\left(\frac{\sqrt{g}\sqrt{m}\tan \:\left(\frac{\left(-C\sqrt{c}\sqrt{g}-\sqrt{c}\sqrt{g}t\right)}{\sqrt{m}}\right)}{\sqrt{c}}\right)$ This reduces to: $\frac{\sqrt{g}\sqrt{m}\tan \left(\infty \right)}{\sqrt{c}}$ The problem with this is that tan $(\infty)$ is indefinite. Where did I go wrong? Could someone please help properly solve this equation. Cheers, Gabriel.","So for my latest physics homework question, I had to derive an equation for the terminal velocity of a ball falling in some gravitational field assuming that the air resistance force was equal to some constant c multiplied by $v^2.$ So first I started with the differntial equation: $\frac{dv}{dt}=-mg-cv^2$ Rearranging to get: $\frac{dv}{dt}=-\left(g+\frac{cv^2}{m}\right)$ From here I tried solving it and ended up with: $\frac{\sqrt{m}}{\sqrt{c}\sqrt{g}}\arctan \left(\frac{\sqrt{c}v}{\sqrt{g}\sqrt{m}}\right)+C=-t$ I rearranged this to get: $v\left(t\right)=\left(\frac{\sqrt{g}\sqrt{m}\tan \left(\frac{\left(-C\sqrt{c}\sqrt{g}-\sqrt{c}\sqrt{g}t\right)}{\sqrt{m}}\right)}{\sqrt{c}}\right)$ In order to calculate the terminal velocity I took the limit as t approaches infinity: $\lim _{t\to \infty }\left(\frac{\sqrt{g}\sqrt{m}\tan \:\left(\frac{\left(-C\sqrt{c}\sqrt{g}-\sqrt{c}\sqrt{g}t\right)}{\sqrt{m}}\right)}{\sqrt{c}}\right)$ This reduces to: $\frac{\sqrt{g}\sqrt{m}\tan \left(\infty \right)}{\sqrt{c}}$ The problem with this is that tan $(\infty)$ is indefinite. Where did I go wrong? Could someone please help properly solve this equation. Cheers, Gabriel.",,"['ordinary-differential-equations', 'physics', 'infinity']"
4,Linearization of a second order differential equation,Linearization of a second order differential equation,,"Linearize the equation $$x'' = -\alpha x-\rho x'+c \sin(t)$$ It is very easy when $c=0$ giving you a  $$ x' = y $$$$ y' = -\alpha x -\rho y $$ giving you a very nice phase portrait. However, if $c$ is non-zero, the linearization should be like  $$ x' = y $$$$ y' = -\alpha x -\rho y +c\sin(t) $$ but this gives a very ugly phase portrait (the lines keep intersecting with themselves) Is this still accurate or should I further use a Jacobian to linearize the matrix? (If so, can somebody provide a small hint on how to approach the result, take any initial state for initial condition)","Linearize the equation $$x'' = -\alpha x-\rho x'+c \sin(t)$$ It is very easy when $c=0$ giving you a  $$ x' = y $$$$ y' = -\alpha x -\rho y $$ giving you a very nice phase portrait. However, if $c$ is non-zero, the linearization should be like  $$ x' = y $$$$ y' = -\alpha x -\rho y +c\sin(t) $$ but this gives a very ugly phase portrait (the lines keep intersecting with themselves) Is this still accurate or should I further use a Jacobian to linearize the matrix? (If so, can somebody provide a small hint on how to approach the result, take any initial state for initial condition)",,['ordinary-differential-equations']
5,Outer and Inner expansions of odes,Outer and Inner expansions of odes,,"I have been asked to determine the first two terms of the outer and inner expansions  of the ODE  $$\epsilon \frac{d^2y}{dx^2}+\frac{dy}{dx}+\frac{1}{1+x}y=0$$ on $0<x<1$ with $0 < \epsilon \ll 1$ with boundary conditions $y(1)=1, y(0)=0$. I've worked out the boundary layer is at $x=0$ but I cant get the right answer even for the outer perturbation. I've been told to maybe Taylor expand it but I still can't seem to get it using the expansion $y=y_0+\epsilon y_1$. The solution is meant to be $y_0=\frac{2}{1 + x}$ which I can get and then I get lost getting $$y_1=\frac{-2}{1+x}+\frac{4}{(1+x)^2}$$ Can anyone see how you can get to this? Using the expansion $y=y_0+\epsilon y_1$ I have got the equation to be $\epsilon   y_0'' + y_0' + \epsilon y_1' + \frac{1}{1+x} y_0 + \frac{1}{1+x} \epsilon y_1=0$. The peeling off order one I have the equation $y_0'+ \frac{1}{1+x} y_0=0$, which I solved using the integrating factor to get $y_0=\frac{A}{1 + x}$ then used the BC at $x=1$ to get $A=2$. Next is when I got problems getting the answer as I got order $\epsilon$ to be $y_0''+y_1'+\frac{1}{1+x} y_1=0$. I got $y_0''=\frac{4}{(1+x)^3}$, so the equation becomes $y_1'+\frac{1}{1+x} y_1=\frac{-4}{(1+x)^3}$ but this is where I am struggling?","I have been asked to determine the first two terms of the outer and inner expansions  of the ODE  $$\epsilon \frac{d^2y}{dx^2}+\frac{dy}{dx}+\frac{1}{1+x}y=0$$ on $0<x<1$ with $0 < \epsilon \ll 1$ with boundary conditions $y(1)=1, y(0)=0$. I've worked out the boundary layer is at $x=0$ but I cant get the right answer even for the outer perturbation. I've been told to maybe Taylor expand it but I still can't seem to get it using the expansion $y=y_0+\epsilon y_1$. The solution is meant to be $y_0=\frac{2}{1 + x}$ which I can get and then I get lost getting $$y_1=\frac{-2}{1+x}+\frac{4}{(1+x)^2}$$ Can anyone see how you can get to this? Using the expansion $y=y_0+\epsilon y_1$ I have got the equation to be $\epsilon   y_0'' + y_0' + \epsilon y_1' + \frac{1}{1+x} y_0 + \frac{1}{1+x} \epsilon y_1=0$. The peeling off order one I have the equation $y_0'+ \frac{1}{1+x} y_0=0$, which I solved using the integrating factor to get $y_0=\frac{A}{1 + x}$ then used the BC at $x=1$ to get $A=2$. Next is when I got problems getting the answer as I got order $\epsilon$ to be $y_0''+y_1'+\frac{1}{1+x} y_1=0$. I got $y_0''=\frac{4}{(1+x)^3}$, so the equation becomes $y_1'+\frac{1}{1+x} y_1=\frac{-4}{(1+x)^3}$ but this is where I am struggling?",,"['ordinary-differential-equations', 'taylor-expansion', 'mathematical-physics', 'boundary-value-problem']"
6,Solving $u(t)=f(t)+a\int_0^t u(s)ds$ by assuming $f$ is differentiable,Solving  by assuming  is differentiable,u(t)=f(t)+a\int_0^t u(s)ds f,"Original Let $a\in\mathbb R$ and $f\colon [0,1]\to\mathbb R$ a continuous   function. Solve the integral equation $$u(t)=f(t)+a\int_0^t  u(s)\mathrm ds,\quad t\geq 0$$ and find an explicit formula for the   solution. First assume $f$ is differentiable. Then $$u'(t)=f'(t)+au(t)$$ $$\to u'(t)-au(t)=f'(t)$$ $$\to u'(t)e^{-at}-au(t)e^{-at}=f'(t)e^{-at}$$ $$\to (u(t)e^{-at})'=f'(t)e^{-at}$$ $$\to (u(t)e^{-at})=\int f'(t)e^{-at} dt$$ $$\to u(t)=e^{at}\int f'(t)e^{-at} dt$$ $$\to u(t)=e^{at}[e^{-at}f(t)+a\int f e^{-at} dt]$$ $$\to u(t)=f(t)+ae^{at}\int f e^{-at} dt$$ Second, use the above as an Ansatz to show that the solution for when $f$ is differentiable is the same as when $f$ isn't . Is this merely coincidence? Or is there some rule like in solving integral equations with conditions of blah blah blah, the Ansatz is of when $f$ is differentiable under the conditions of blah blah blah, is right?","Original Let $a\in\mathbb R$ and $f\colon [0,1]\to\mathbb R$ a continuous   function. Solve the integral equation $$u(t)=f(t)+a\int_0^t  u(s)\mathrm ds,\quad t\geq 0$$ and find an explicit formula for the   solution. First assume $f$ is differentiable. Then $$u'(t)=f'(t)+au(t)$$ $$\to u'(t)-au(t)=f'(t)$$ $$\to u'(t)e^{-at}-au(t)e^{-at}=f'(t)e^{-at}$$ $$\to (u(t)e^{-at})'=f'(t)e^{-at}$$ $$\to (u(t)e^{-at})=\int f'(t)e^{-at} dt$$ $$\to u(t)=e^{at}\int f'(t)e^{-at} dt$$ $$\to u(t)=e^{at}[e^{-at}f(t)+a\int f e^{-at} dt]$$ $$\to u(t)=f(t)+ae^{at}\int f e^{-at} dt$$ Second, use the above as an Ansatz to show that the solution for when $f$ is differentiable is the same as when $f$ isn't . Is this merely coincidence? Or is there some rule like in solving integral equations with conditions of blah blah blah, the Ansatz is of when $f$ is differentiable under the conditions of blah blah blah, is right?",,"['calculus', 'real-analysis', 'integration', 'ordinary-differential-equations', 'derivatives']"
7,Differential equation using Laplace transform struck on inverse Laplace,Differential equation using Laplace transform struck on inverse Laplace,,I was solving this differential equation  $$ty''+2y'+ty=\cos{t}$$ with initial condition $y(0)=1$. I took Laplace on both sides and after simplifying got this differential equation in terms of $Y(s)$ $$(s^2-2)Y'(s)+4sY(s)=\frac{s}{s^2+1}+3$$ Solving this I got $$Y(s)=\frac{s^2- 3\log{(s^2+1)}+2s^3-12s}{2(s^2-2)^2}$$ What is the Laplace inverse of $Y(s)$?,I was solving this differential equation  $$ty''+2y'+ty=\cos{t}$$ with initial condition $y(0)=1$. I took Laplace on both sides and after simplifying got this differential equation in terms of $Y(s)$ $$(s^2-2)Y'(s)+4sY(s)=\frac{s}{s^2+1}+3$$ Solving this I got $$Y(s)=\frac{s^2- 3\log{(s^2+1)}+2s^3-12s}{2(s^2-2)^2}$$ What is the Laplace inverse of $Y(s)$?,,"['ordinary-differential-equations', 'inverse-laplace']"
8,"Show that $f(x)={1+\ln x\over x}, x>0$",Show that,"f(x)={1+\ln x\over x}, x>0","Let $f:(0,+\infty)\to\mathbb R$ be a function with $f(1)=1$ which is differentiable and for which it applies: $$xf(x)=1-x^2f'(x), \forall x>0$$ Show that $f(x)={1+\ln x\over x}, x>0$ Personal work: $$xf(x)=1-x^2f'(x)\iff_{x\neq0} f(x)={1-x^2f'(x)\over x}$$ We know that $f(1)=1$ , so: $$f(1)=1\iff{1-x^2f'(x)\over x}=1\iff1-x^2f'(x)=x\iff-x^2f'(x)=x-1\iff f'(x)={x-1\over -x^2}$$ That's what I got. What's the problem? The actual derivative of $f(x)$ is ${-\ln x\over x^2}$ and not $$\color{red}{x-1\over -x^2}$$","Let be a function with which is differentiable and for which it applies: Show that Personal work: We know that , so: That's what I got. What's the problem? The actual derivative of is and not","f:(0,+\infty)\to\mathbb R f(1)=1 xf(x)=1-x^2f'(x), \forall x>0 f(x)={1+\ln x\over x}, x>0 xf(x)=1-x^2f'(x)\iff_{x\neq0} f(x)={1-x^2f'(x)\over x} f(1)=1 f(1)=1\iff{1-x^2f'(x)\over x}=1\iff1-x^2f'(x)=x\iff-x^2f'(x)=x-1\iff f'(x)={x-1\over -x^2} f(x) {-\ln x\over x^2} \color{red}{x-1\over -x^2}","['ordinary-differential-equations', 'derivatives']"
9,Mechanics Question on Differential Equations,Mechanics Question on Differential Equations,,"I am going through a question in a past mechanics exam paper for a first-year undergraduate course and I was pretty stumped on this question. Question: A particle of mass 5kg moves along the x axis under the influence of two forces. Firstly,   it experiences an attractive force towards the origin O of 40 times the instantaneous distance   from O and secondly damping (or resistive force) which is 20 times the instantaneous   speed. Assume that the particle starts from 0.2m from the origin. (i) Set up the differential equation and conditions which describe the motion of the   particle. (ii) Write down the position of the particle x at time t and determine the period of the   oscillation. I managed to get a differential equation for the first part but is really confused with how to manipulate it for part 2, which is making me question the answer to the first part. I managed to get $$ \frac{dv}{dx} = \frac{8x-4v}{v}$$ for the first part using Newton's second law of motion. If anyone could help correct this or give me tips for the second part, that would be great. Thanks!","I am going through a question in a past mechanics exam paper for a first-year undergraduate course and I was pretty stumped on this question. Question: A particle of mass 5kg moves along the x axis under the influence of two forces. Firstly,   it experiences an attractive force towards the origin O of 40 times the instantaneous distance   from O and secondly damping (or resistive force) which is 20 times the instantaneous   speed. Assume that the particle starts from 0.2m from the origin. (i) Set up the differential equation and conditions which describe the motion of the   particle. (ii) Write down the position of the particle x at time t and determine the period of the   oscillation. I managed to get a differential equation for the first part but is really confused with how to manipulate it for part 2, which is making me question the answer to the first part. I managed to get $$ \frac{dv}{dx} = \frac{8x-4v}{v}$$ for the first part using Newton's second law of motion. If anyone could help correct this or give me tips for the second part, that would be great. Thanks!",,"['ordinary-differential-equations', 'physics', 'classical-mechanics']"
10,Problem of book ODE and dynamical systems gerald teschl,Problem of book ODE and dynamical systems gerald teschl,,"Problem 1. Consider again the exact model from the previous problem   and write $$ \ddot{r} = -\frac{\gamma M \epsilon^2}{(1 + \epsilon r)^2}, ~~~\epsilon = \frac{1}{R} $$ It can be shown that the solution $r(t) = r(t,\epsilon)$ to the above initial conditions is $C^{\infty}$ (with respect to both $t$ and $\epsilon$). Show that $$ r(t) = h - g\left[1 - 2\frac{h}{R} \right]\frac{t^2}{2} + \mathcal{O}\left(\frac{1}{R^4}\right), ~~~ g = \frac{\gamma M}{R^2} $$ The initial condition reads $r(0)=h$ and $\dot{r}(0)=0$ (Hint: Insert $r(t,\epsilon) = r_0(t) + r_1(t)\epsilon + r_2(t)\epsilon^2  + r_3(t)\epsilon^3 + \mathcal{O}(\epsilon^4)$    into the differential equation and collect powers of $\epsilon$. Then solve the corresponding differential equations for $r_0(t)$, $r_1(t)$, $\cdots$ and note that the initial conditions follow from $r(0, \epsilon) = h$ respectively $\dot{r}(0, \epsilon) = 0$. A rigorous justification for   this procedure will be given in Section 2.5.). Remark: $\dot{r}$ and $\ddot{r}$ are derivatives of first And Second  order. how to solve this problem following is hint? What does these ($r_0$, $r_1$, $r_2$, $r_3$) mean? the derivatives? This problem be in book book ODE and dynamical systems gerald teschl on introduction.","Problem 1. Consider again the exact model from the previous problem   and write $$ \ddot{r} = -\frac{\gamma M \epsilon^2}{(1 + \epsilon r)^2}, ~~~\epsilon = \frac{1}{R} $$ It can be shown that the solution $r(t) = r(t,\epsilon)$ to the above initial conditions is $C^{\infty}$ (with respect to both $t$ and $\epsilon$). Show that $$ r(t) = h - g\left[1 - 2\frac{h}{R} \right]\frac{t^2}{2} + \mathcal{O}\left(\frac{1}{R^4}\right), ~~~ g = \frac{\gamma M}{R^2} $$ The initial condition reads $r(0)=h$ and $\dot{r}(0)=0$ (Hint: Insert $r(t,\epsilon) = r_0(t) + r_1(t)\epsilon + r_2(t)\epsilon^2  + r_3(t)\epsilon^3 + \mathcal{O}(\epsilon^4)$    into the differential equation and collect powers of $\epsilon$. Then solve the corresponding differential equations for $r_0(t)$, $r_1(t)$, $\cdots$ and note that the initial conditions follow from $r(0, \epsilon) = h$ respectively $\dot{r}(0, \epsilon) = 0$. A rigorous justification for   this procedure will be given in Section 2.5.). Remark: $\dot{r}$ and $\ddot{r}$ are derivatives of first And Second  order. how to solve this problem following is hint? What does these ($r_0$, $r_1$, $r_2$, $r_3$) mean? the derivatives? This problem be in book book ODE and dynamical systems gerald teschl on introduction.",,"['ordinary-differential-equations', 'analysis', 'systems-of-equations']"
11,How can I solve the following ode,How can I solve the following ode,,"$$C\frac{dT}{dt}=-\sigma T(t)^{4}+(1-\alpha)Q$$ I need help to solve the above pde where $C,\alpha ,Q$ are constants, I'm really unsure on how to even start to solve it","$$C\frac{dT}{dt}=-\sigma T(t)^{4}+(1-\alpha)Q$$ I need help to solve the above pde where $C,\alpha ,Q$ are constants, I'm really unsure on how to even start to solve it",,"['ordinary-differential-equations', 'partial-derivative']"
12,Differential equation solved by $y(x) = c_1e^x + c_2xe^x + c_3x^2e^x + c_4\cos(x) + c_5\sin(x)$,Differential equation solved by,y(x) = c_1e^x + c_2xe^x + c_3x^2e^x + c_4\cos(x) + c_5\sin(x),"Consider the following solution: $$y(x) = c_1e^x + c_2xe^x + c_3x^2e^x + c_4\cos(x) + c_5\sin(x)$$   where all the $c_i$ are real constants ($c_i \in \Bbb R$).   How to find a differential equation that have the previous solution? I re-wrote the solution to the following: $$y(x)=(c_1 + c_2x + c_3x^2)e^x + c_4\cos(x) + c_5\sin(x)$$ I can see that the solution looks like: $y(x) = Q(x)e^x + a\cos(x) + b\sin(x)$ where $Q$ is a second degree polynomial, but I don't really know how to take it from here.","Consider the following solution: $$y(x) = c_1e^x + c_2xe^x + c_3x^2e^x + c_4\cos(x) + c_5\sin(x)$$   where all the $c_i$ are real constants ($c_i \in \Bbb R$).   How to find a differential equation that have the previous solution? I re-wrote the solution to the following: $$y(x)=(c_1 + c_2x + c_3x^2)e^x + c_4\cos(x) + c_5\sin(x)$$ I can see that the solution looks like: $y(x) = Q(x)e^x + a\cos(x) + b\sin(x)$ where $Q$ is a second degree polynomial, but I don't really know how to take it from here.",,"['real-analysis', 'ordinary-differential-equations']"
13,"$y''+\epsilon y'=\epsilon$, where $y(0)=1$, $y'(0)=0$",", where ,",y''+\epsilon y'=\epsilon y(0)=1 y'(0)=0,"I am trying to solve $y''+\epsilon y'=\epsilon$, where $y(0)=1$, $y'(0)=0$ using perturbation theory. Using the substitution $y=y_{0}+y_{1}\epsilon$ I got the series $y=1+\epsilon(1+\frac{x^{2}}{2})+O(\epsilon ^{2})$. However wolframalpha tells me the exact solution involves an exponential, (see http://m.wolframalpha.com/input/?i=y%22%2B0.1y%27%3D0.1 where I set $\epsilon=0.1$). Am I on the right track with the solution? I'd then like to determine the validity of the solution as $x\rightarrow\infty$ but I'm not confident with this.","I am trying to solve $y''+\epsilon y'=\epsilon$, where $y(0)=1$, $y'(0)=0$ using perturbation theory. Using the substitution $y=y_{0}+y_{1}\epsilon$ I got the series $y=1+\epsilon(1+\frac{x^{2}}{2})+O(\epsilon ^{2})$. However wolframalpha tells me the exact solution involves an exponential, (see http://m.wolframalpha.com/input/?i=y%22%2B0.1y%27%3D0.1 where I set $\epsilon=0.1$). Am I on the right track with the solution? I'd then like to determine the validity of the solution as $x\rightarrow\infty$ but I'm not confident with this.",,"['ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
14,What type of differential equation is it and how can it be solved?,What type of differential equation is it and how can it be solved?,,"The equation is: $$\frac{dA(t)}{dt}=aA(t)\cdot k^{2}(t)+b\cdot k(t)$$ If not the last part, I would use the separation of variables, but how to solve it in this form?","The equation is: $$\frac{dA(t)}{dt}=aA(t)\cdot k^{2}(t)+b\cdot k(t)$$ If not the last part, I would use the separation of variables, but how to solve it in this form?",,['ordinary-differential-equations']
15,Second Order ODE $y'' - 2y' + y = e^{2x}$ by Method of Variation of Parameters,Second Order ODE  by Method of Variation of Parameters,y'' - 2y' + y = e^{2x},"I'm having some trouble with solving this second-order ODE by the method of variation of parameters. The problem is: $y'' - 2y' + y = e^{2x}$. Here's what I've done thus far. The homogeneous component has an auxiliary equation of $r^2 - 2 r + 1$, which has a single root of $r = 1$, so our complimentary equation is $y_c = c_1 e^{x}+ c_2 xe^{x}$. So, we take $y_1 = y_2 = e^{x}$, and guess that our particular solution takes the form $y_p = u_1(x) y_1 + u_2 (x) y_2$. Differentiating in $x$ gives us  \begin{align*} y'_p = \frac{d}{dx}[u_1 e^x+u_2 xe^x] = (u_1' e^x + u_2' xe^x) + u_1e^x+u_2e^x+u_2xe^x \end{align*} Then, we can assume that $u_1' e^x + u_2' xe^x = 0$, so $y_p' = u_1e^x+u_2e^x+u_2xe^x$. Differentiating in $x$ once more and using our earlier assumption gives us \begin{align*} y''_p = (u_1' e^x + u_2'xe^x)+u_1 e^x+2u_2e^x+u'_2e^x+u_2xe^x = u_1 e^x+2u_2e^x+u'_2e^x+u_2xe^x \end{align*} Substituting back into our differential equation gives us (pre-simplification) \begin{align*} y'' - 2y' + y = u_1 e^x+2u_2e^x+u'_2e^x+u_2xe^x - 2(u_1e^x+u_2e^x+u_2xe^x) + u_1 e^x + u_2 xe^x = e^{2x} \end{align*} After simplifying, I get $2u'_2 e^x = e^{2x}$, so $2u'_2 = e^x$, $u'_2 = \frac{1}{2} e^x$, and $u_2 = \frac{1}{2} e^x$. Since $u_1'e^x + u_2'xe^x = 0$ by assumption, and $u'_2 = \frac{1}{2}e^x$, we have $u_1'+ \frac{1}{2}e^x = 0$, and $u'_1 = - \frac{1}{2}e^x$, so we get $$y_p = \Big(-\frac{1}{2}e^x\Big)e^x + \Big(\frac{1}{2}e^x\Big)xe^x = - \frac{1}{2} e^{2x} + \frac{1}{2} xe^{2x}$$ Therefore, our general solution is $$y = c_1 e^{x}+ c_2 xe^{x} + - \frac{1}{2} e^{2x} + \frac{1}{2} xe^{2x}$$ This answer, however, is not correct. The right answer should be $y = e^x(c_1 + c_2x) + e^{2x}$, but I can't figure out where I went wrong. Some help on this would be greatly appreciated.","I'm having some trouble with solving this second-order ODE by the method of variation of parameters. The problem is: $y'' - 2y' + y = e^{2x}$. Here's what I've done thus far. The homogeneous component has an auxiliary equation of $r^2 - 2 r + 1$, which has a single root of $r = 1$, so our complimentary equation is $y_c = c_1 e^{x}+ c_2 xe^{x}$. So, we take $y_1 = y_2 = e^{x}$, and guess that our particular solution takes the form $y_p = u_1(x) y_1 + u_2 (x) y_2$. Differentiating in $x$ gives us  \begin{align*} y'_p = \frac{d}{dx}[u_1 e^x+u_2 xe^x] = (u_1' e^x + u_2' xe^x) + u_1e^x+u_2e^x+u_2xe^x \end{align*} Then, we can assume that $u_1' e^x + u_2' xe^x = 0$, so $y_p' = u_1e^x+u_2e^x+u_2xe^x$. Differentiating in $x$ once more and using our earlier assumption gives us \begin{align*} y''_p = (u_1' e^x + u_2'xe^x)+u_1 e^x+2u_2e^x+u'_2e^x+u_2xe^x = u_1 e^x+2u_2e^x+u'_2e^x+u_2xe^x \end{align*} Substituting back into our differential equation gives us (pre-simplification) \begin{align*} y'' - 2y' + y = u_1 e^x+2u_2e^x+u'_2e^x+u_2xe^x - 2(u_1e^x+u_2e^x+u_2xe^x) + u_1 e^x + u_2 xe^x = e^{2x} \end{align*} After simplifying, I get $2u'_2 e^x = e^{2x}$, so $2u'_2 = e^x$, $u'_2 = \frac{1}{2} e^x$, and $u_2 = \frac{1}{2} e^x$. Since $u_1'e^x + u_2'xe^x = 0$ by assumption, and $u'_2 = \frac{1}{2}e^x$, we have $u_1'+ \frac{1}{2}e^x = 0$, and $u'_1 = - \frac{1}{2}e^x$, so we get $$y_p = \Big(-\frac{1}{2}e^x\Big)e^x + \Big(\frac{1}{2}e^x\Big)xe^x = - \frac{1}{2} e^{2x} + \frac{1}{2} xe^{2x}$$ Therefore, our general solution is $$y = c_1 e^{x}+ c_2 xe^{x} + - \frac{1}{2} e^{2x} + \frac{1}{2} xe^{2x}$$ This answer, however, is not correct. The right answer should be $y = e^x(c_1 + c_2x) + e^{2x}$, but I can't figure out where I went wrong. Some help on this would be greatly appreciated.",,[]
16,Finding a recursive relation from a differential equation.,Finding a recursive relation from a differential equation.,,We consider the following differential equation: $$(1-x)\psi'(x) - 1 = 0$$ We then define:  $\psi(x) = \sum_{n = 0}^{\infty}a_nx^n$.(power series) Plugging it into the formula we get to this step: $$\sum_{n=0}^{\infty}a_nn(x^{n-1}-x^{n}) - 1= 0$$ How can we then identify the recursive relation between the summands thus determine $a_{n+1}$ ?,We consider the following differential equation: $$(1-x)\psi'(x) - 1 = 0$$ We then define:  $\psi(x) = \sum_{n = 0}^{\infty}a_nx^n$.(power series) Plugging it into the formula we get to this step: $$\sum_{n=0}^{\infty}a_nn(x^{n-1}-x^{n}) - 1= 0$$ How can we then identify the recursive relation between the summands thus determine $a_{n+1}$ ?,,"['ordinary-differential-equations', 'summation', 'recursion']"
17,Why boundary conditions in Sturm-Liouville problem are homogeneous?,Why boundary conditions in Sturm-Liouville problem are homogeneous?,,Boundary conditions in Sturm-Liouville problem looks like this: $$ \alpha_1 y(a)+\alpha_2 y'(a)=0 $$ $$ \beta_1y(b)+\beta_2 y'(b)=0 $$ The ordinary boundary conditions for boundary-value problem looks: $$ \alpha_1 y(a)+\alpha_2 y'(a)=\gamma_1 $$ $$ \beta_1y(b)+\beta_2 y'(b)=\gamma_2 $$ Why in Sturm-Liouville conditions $\gamma_1$ and $\gamma_2$ both zeros? Does it have some hidden (or physical) sense?,Boundary conditions in Sturm-Liouville problem looks like this: $$ \alpha_1 y(a)+\alpha_2 y'(a)=0 $$ $$ \beta_1y(b)+\beta_2 y'(b)=0 $$ The ordinary boundary conditions for boundary-value problem looks: $$ \alpha_1 y(a)+\alpha_2 y'(a)=\gamma_1 $$ $$ \beta_1y(b)+\beta_2 y'(b)=\gamma_2 $$ Why in Sturm-Liouville conditions $\gamma_1$ and $\gamma_2$ both zeros? Does it have some hidden (or physical) sense?,,"['ordinary-differential-equations', 'mathematical-physics', 'boundary-value-problem', 'sturm-liouville']"
18,First-order differential equations - why is only the interval of validity with the initial value valid? [duplicate],First-order differential equations - why is only the interval of validity with the initial value valid? [duplicate],,"This question already has answers here : Why is it differential equations exist on an interval instead of a domain? (2 answers) Closed 3 years ago . I'm reading through Paul's online math notes on differential equations because I'd like to have a basic grasp on the subject (I'm interested in physics ...) There's something I don't understand about initial value problems. I get that the solution y(x) to a first-order differential equation with a given initial value constraint might not be valid for every x. But what I don't get is that the author claims that we should only see the interval of validity in which the x of the initial value constraints lays as the interval of validity. I'm getting these claims from this page , at the Example 1 section. I just see no reason for this claim and I was wondering if someone could offer a mathematical explanation of why we discard all the intervals of validity except for the one in which the initial value lays. ( I could understand it from a physical point of view ). Thanks very much in advance, Joshua","This question already has answers here : Why is it differential equations exist on an interval instead of a domain? (2 answers) Closed 3 years ago . I'm reading through Paul's online math notes on differential equations because I'd like to have a basic grasp on the subject (I'm interested in physics ...) There's something I don't understand about initial value problems. I get that the solution y(x) to a first-order differential equation with a given initial value constraint might not be valid for every x. But what I don't get is that the author claims that we should only see the interval of validity in which the x of the initial value constraints lays as the interval of validity. I'm getting these claims from this page , at the Example 1 section. I just see no reason for this claim and I was wondering if someone could offer a mathematical explanation of why we discard all the intervals of validity except for the one in which the initial value lays. ( I could understand it from a physical point of view ). Thanks very much in advance, Joshua",,"['calculus', 'ordinary-differential-equations', 'initial-value-problems']"
19,Finding solutions to a non homogeneous differential equation knowing the solutions of its homogenous counterpart,Finding solutions to a non homogeneous differential equation knowing the solutions of its homogenous counterpart,,"Let $f(x)$ and $xf(x)$ be the particular solutions of a differential   equation $$y''+R(x)y'+S(x)y=0$$ Find the solution of the differential   equation $$y''+R(x)y'+S(x)y=f(x)$$ in terms of $f(x)$. I was trying to find a solution if the form $(ax^2+bx+c)f(x)$. Now as $(bx+c)f(x)$ is a solution to the homogeneous equation, I just need to assume that $ax^2f(x)$ is a particular solution of the non homogeneous equation, and find the suitable value of $a$. But I am getting stuck. Please help.","Let $f(x)$ and $xf(x)$ be the particular solutions of a differential   equation $$y''+R(x)y'+S(x)y=0$$ Find the solution of the differential   equation $$y''+R(x)y'+S(x)y=f(x)$$ in terms of $f(x)$. I was trying to find a solution if the form $(ax^2+bx+c)f(x)$. Now as $(bx+c)f(x)$ is a solution to the homogeneous equation, I just need to assume that $ax^2f(x)$ is a particular solution of the non homogeneous equation, and find the suitable value of $a$. But I am getting stuck. Please help.",,['ordinary-differential-equations']
20,Rectification of Vector fields,Rectification of Vector fields,,"I am trying to understand how to rectify vector fields on manifolds. Namely if there is a manifold $M$ and a given vector field $X_p$ where for a $p_0$ we have $X(p_0)\neq0$ then there exists a neighborhood of $p_0$ and some other coordinate systme $y$ where $X=\frac{\partial}{\partial y_1}$.  I would like some refference to study this phenomenon, mainly excersices or examples. I have read the proof in Spivak's book but I have a hard time solving actual excersises. Also I found Arnold's book on Ordinary Differential Equations lacking of any worked examples (although I may be wrong here). For example, I would appreciate if someone could give me some on direction on rectifying the following $\mathbb{R^2}$ vector field, given on cartesian coordinates, around the point $p=(1,2)$: $$V=x_1\frac{\partial}{\partial x_1} -2x_2\frac{\partial}{\partial x_2}.$$ I would be perticularly grateful if you could illuminate the procedure using flows.","I am trying to understand how to rectify vector fields on manifolds. Namely if there is a manifold $M$ and a given vector field $X_p$ where for a $p_0$ we have $X(p_0)\neq0$ then there exists a neighborhood of $p_0$ and some other coordinate systme $y$ where $X=\frac{\partial}{\partial y_1}$.  I would like some refference to study this phenomenon, mainly excersices or examples. I have read the proof in Spivak's book but I have a hard time solving actual excersises. Also I found Arnold's book on Ordinary Differential Equations lacking of any worked examples (although I may be wrong here). For example, I would appreciate if someone could give me some on direction on rectifying the following $\mathbb{R^2}$ vector field, given on cartesian coordinates, around the point $p=(1,2)$: $$V=x_1\frac{\partial}{\partial x_1} -2x_2\frac{\partial}{\partial x_2}.$$ I would be perticularly grateful if you could illuminate the procedure using flows.",,"['ordinary-differential-equations', 'manifolds', 'coordinate-systems', 'vector-fields']"
21,"For second-order ODE, why do we only assign state variables for $\dot q, q$?","For second-order ODE, why do we only assign state variables for ?","\dot q, q","Suppose we are given an ODE of a system, for example, a mass spring damper: $$m\ddot q + \dot q + q = u$$ We may think of $q$ as position, $\dot q$ as the velocity, $\ddot q$ as the acceleration. Why is it that we only assign states to $q$ and $\dot q$, i.e., letting $x_1 = q$ and $x_2 = \dot q$, instead of letting letting $x_1 = q$ and $x_2 = \dot q$ and $x_3 = \ddot q$? Is it not that the acceleration is a ""state"" of the system? If not, then why should the other two variables be the state? I'm watching a video on Youtube where they attempt to assign state variables to a swinging pendulum, and the video says, in which I paraphrase: ""since kinetic energy and potential energy depends on position and velocity, therefore it makes sense to assign these as state variables."" I'm looking for a more precise way of stating the above quote.","Suppose we are given an ODE of a system, for example, a mass spring damper: $$m\ddot q + \dot q + q = u$$ We may think of $q$ as position, $\dot q$ as the velocity, $\ddot q$ as the acceleration. Why is it that we only assign states to $q$ and $\dot q$, i.e., letting $x_1 = q$ and $x_2 = \dot q$, instead of letting letting $x_1 = q$ and $x_2 = \dot q$ and $x_3 = \ddot q$? Is it not that the acceleration is a ""state"" of the system? If not, then why should the other two variables be the state? I'm watching a video on Youtube where they attempt to assign state variables to a swinging pendulum, and the video says, in which I paraphrase: ""since kinetic energy and potential energy depends on position and velocity, therefore it makes sense to assign these as state variables."" I'm looking for a more precise way of stating the above quote.",,"['ordinary-differential-equations', 'physics', 'mathematical-modeling', 'control-theory']"
22,Invariant: Differential Equations.,Invariant: Differential Equations.,,I came across a question. Show: $A = x^2 + y^2 $ and $B=x^2+z^2$ . Are invariants of the nonlinear system: $x'(t)=y(t)z(t) .     y'(t) = -x(t)z(t) .    z'(t) = -x(t)y(t) .  $ Now I know the solution is: $A′ = 2xx′ + 2yy′ = 2xyz − 2xyz = 0$ . and $B′ = 2xx′ + 2zz′ = 0.$ My question is: a) What is the significance of invariant solutions? b) Is there anywhere in ordinary differential equations that invariant solutions become useful? c) Does invariance have any influence on existence theory?,I came across a question. Show: $A = x^2 + y^2 $ and $B=x^2+z^2$ . Are invariants of the nonlinear system: $x'(t)=y(t)z(t) .     y'(t) = -x(t)z(t) .    z'(t) = -x(t)y(t) .  $ Now I know the solution is: $A′ = 2xx′ + 2yy′ = 2xyz − 2xyz = 0$ . and $B′ = 2xx′ + 2zz′ = 0.$ My question is: a) What is the significance of invariant solutions? b) Is there anywhere in ordinary differential equations that invariant solutions become useful? c) Does invariance have any influence on existence theory?,,['ordinary-differential-equations']
23,How to find matrix exponential $e^A$,How to find matrix exponential,e^A,"I have the matrix $$A =\begin{pmatrix} 0 & 1 \\ - 1 & 0 \end{pmatrix}$$ and I have to find $e^A$ I've found two complex-conjugate eigenvalues $\lambda_{1,2} = \pm i$ so substracting $\lambda_1 = i$ from the matrix's diagonal I got: $$A_1 = \begin{pmatrix} -i & 1 \\-1 & i \end{pmatrix}$$ and therefore. to find eigenvector I have to solve the system: $$A_1 = \begin{pmatrix} -i & 1 \\-1 & i \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$$ so the first eigenvector is $h_1 = \begin{pmatrix}1 \\ i\end{pmatrix}$  and the second one is $h_2 = \begin{pmatrix}1 \\ -i\end{pmatrix}$ so the general solution is $$x(t) = C_1e^{it}\begin{pmatrix} 1 \\i\end{pmatrix} + C_2e^{-it}\begin{pmatrix} 1 \\-i\end{pmatrix}$$ I know that now I have to solve two Cauchy's problems for the standard basis $\mathbb{R}^2$ with vectors $v_1 = (1, 0)$ and $v_2 = (0,1)$ But I do not know how to approach it for complex numbers","I have the matrix $$A =\begin{pmatrix} 0 & 1 \\ - 1 & 0 \end{pmatrix}$$ and I have to find $e^A$ I've found two complex-conjugate eigenvalues $\lambda_{1,2} = \pm i$ so substracting $\lambda_1 = i$ from the matrix's diagonal I got: $$A_1 = \begin{pmatrix} -i & 1 \\-1 & i \end{pmatrix}$$ and therefore. to find eigenvector I have to solve the system: $$A_1 = \begin{pmatrix} -i & 1 \\-1 & i \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$$ so the first eigenvector is $h_1 = \begin{pmatrix}1 \\ i\end{pmatrix}$  and the second one is $h_2 = \begin{pmatrix}1 \\ -i\end{pmatrix}$ so the general solution is $$x(t) = C_1e^{it}\begin{pmatrix} 1 \\i\end{pmatrix} + C_2e^{-it}\begin{pmatrix} 1 \\-i\end{pmatrix}$$ I know that now I have to solve two Cauchy's problems for the standard basis $\mathbb{R}^2$ with vectors $v_1 = (1, 0)$ and $v_2 = (0,1)$ But I do not know how to approach it for complex numbers",,"['matrices', 'complex-analysis', 'ordinary-differential-equations', 'matrix-exponential', 'cauchy-problem']"
24,Method of Characteristics(Advection equation with initial and boundary condition),Method of Characteristics(Advection equation with initial and boundary condition),,"Solve, using the Method of Characteristics, the equation $\frac{\partial \rho}{\partial t} + \frac{\partial \rho}{\partial x}=-\mu\rho$ for $x,t>0$ with the conditions $\rho(x,0)=f(x)$ and $\rho(0,t)=g(t)$. I'm currently trying to solve above problem using method of characteristics. I used initial condition $\rho(x,0)=f(x)$ and obtained $|\rho(x,t)|=|f(x-t)|e^{- \mu t}$. But I need to find $\rho(x,t)$ and also I don't know how to use the boundary condition $\rho(0,t)=g(t)$. I'm posting this problem here to get a hint.Thanks in advance.","Solve, using the Method of Characteristics, the equation $\frac{\partial \rho}{\partial t} + \frac{\partial \rho}{\partial x}=-\mu\rho$ for $x,t>0$ with the conditions $\rho(x,0)=f(x)$ and $\rho(0,t)=g(t)$. I'm currently trying to solve above problem using method of characteristics. I used initial condition $\rho(x,0)=f(x)$ and obtained $|\rho(x,t)|=|f(x-t)|e^{- \mu t}$. But I need to find $\rho(x,t)$ and also I don't know how to use the boundary condition $\rho(0,t)=g(t)$. I'm posting this problem here to get a hint.Thanks in advance.",,"['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-modeling', 'fluid-dynamics', 'characteristics']"
25,"Picard Iteration of $Y_n(t) = y_0 + c \int_{t_o}^t s Y_{n-1}(s) \, ds$",Picard Iteration of,"Y_n(t) = y_0 + c \int_{t_o}^t s Y_{n-1}(s) \, ds","Let $(t_0, y_0) \in \mathbb{R}$, $c \in \mathbb{R}$ and define $Y_0(t) = y_0$, $$Y_n(t) = y_0 + c \int_{t_o}^t s Y_{n-1}(s) \, ds.$$ I need to compute $Y_n(t)$ and $Y(t) = \lim_{n \rightarrow \infty} Y_n(t).$ I know that I need to compute $Y_n(t)$ by induction but I am getting stuck on coming up with a general forumla for $Y_n(t)$. So far I have that $$Y_1(t) = y_0 \left( 1+c \frac{t^2-t_o^2}{2} \right)$$ and $$Y_2(t) = y_0 \left( 1+c \frac{t^2-t_0^2}{2} - c \frac{(t^2-t_0^2)t_0^2}{4} + c^2 \frac{t^4-t_0^4}{8} \right).$$ I can compute the expression for $Y_3(t)$, and so on, but I'm not seeing a general forumla for $Y_n(t)$ so that I can use it to do the proof by induction.","Let $(t_0, y_0) \in \mathbb{R}$, $c \in \mathbb{R}$ and define $Y_0(t) = y_0$, $$Y_n(t) = y_0 + c \int_{t_o}^t s Y_{n-1}(s) \, ds.$$ I need to compute $Y_n(t)$ and $Y(t) = \lim_{n \rightarrow \infty} Y_n(t).$ I know that I need to compute $Y_n(t)$ by induction but I am getting stuck on coming up with a general forumla for $Y_n(t)$. So far I have that $$Y_1(t) = y_0 \left( 1+c \frac{t^2-t_o^2}{2} \right)$$ and $$Y_2(t) = y_0 \left( 1+c \frac{t^2-t_0^2}{2} - c \frac{(t^2-t_0^2)t_0^2}{4} + c^2 \frac{t^4-t_0^4}{8} \right).$$ I can compute the expression for $Y_3(t)$, and so on, but I'm not seeing a general forumla for $Y_n(t)$ so that I can use it to do the proof by induction.",,"['real-analysis', 'sequences-and-series', 'ordinary-differential-equations']"
26,Liapunov's Function and kind of Stability,Liapunov's Function and kind of Stability,,"Problem : Using the right Liapunov's Function V find the kind of stability of the non-linear system:   $$ x'=\sin(x+y) $$ $$y'=-\sin(x-y)$$ given the Liapunov's function: $$V=x\cdot y $$ So, I don't have any experience on this type of exercices and I don't know even how to start the exercice.I know that i have to differentiate  the liapunov's function $$V=x\cdot y$$ so i take :$$V'=Vx\cdot x'+Vy\cdot y'$$ but i don't find the reason and i don't know if it's right too. I would really appreciate a thorough solution and explanation, since I've just started working on Liapunov's functions and Liapunov's stability and I have to clear my mind on them.I have to say that my dynamical's system book doesn't have problems of these types so to proceed.Thanks in advance!","Problem : Using the right Liapunov's Function V find the kind of stability of the non-linear system:   $$ x'=\sin(x+y) $$ $$y'=-\sin(x-y)$$ given the Liapunov's function: $$V=x\cdot y $$ So, I don't have any experience on this type of exercices and I don't know even how to start the exercice.I know that i have to differentiate  the liapunov's function $$V=x\cdot y$$ so i take :$$V'=Vx\cdot x'+Vy\cdot y'$$ but i don't find the reason and i don't know if it's right too. I would really appreciate a thorough solution and explanation, since I've just started working on Liapunov's functions and Liapunov's stability and I have to clear my mind on them.I have to say that my dynamical's system book doesn't have problems of these types so to proceed.Thanks in advance!",,"['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
27,Recursive formula (IMO 2017 #3),Recursive formula (IMO 2017 #3),,"The  2017 International Math Olympiad posed a question involving a rabbit and hunter https://artofproblemsolving.com/community/c6h1480157_imo_2017_problem_3 Assume: the rabbit always moves 1 unit on a vector away from the hunter the tracker always emits a signal which is 1 unit perpendicular to that vector the hunter always moves 1 unit on a vector toward the position reported by the tracker The distance d(n) between hunter and rabbit after n rounds can be expressed recursively as $$ d(n)  = \sqrt{((d(n-1)+1)^2 +1} -1 $$ Essentially, the problem is asking whether d(1e9) can feasibly be larger than 100. With the use of computer and spreadsheet tool one can show that $$ d(n)  = \sqrt{n - 2\sqrt{n+1} +2}  $$ Question: how could one derive the non-recursive expression of d(n) without benefit of a computer spreadsheet","The  2017 International Math Olympiad posed a question involving a rabbit and hunter https://artofproblemsolving.com/community/c6h1480157_imo_2017_problem_3 Assume: the rabbit always moves 1 unit on a vector away from the hunter the tracker always emits a signal which is 1 unit perpendicular to that vector the hunter always moves 1 unit on a vector toward the position reported by the tracker The distance d(n) between hunter and rabbit after n rounds can be expressed recursively as $$ d(n)  = \sqrt{((d(n-1)+1)^2 +1} -1 $$ Essentially, the problem is asking whether d(1e9) can feasibly be larger than 100. With the use of computer and spreadsheet tool one can show that $$ d(n)  = \sqrt{n - 2\sqrt{n+1} +2}  $$ Question: how could one derive the non-recursive expression of d(n) without benefit of a computer spreadsheet",,"['ordinary-differential-equations', 'contest-math', 'recursive-algorithms']"
28,How do I use Leibniz formula to solve this difficult equation?,How do I use Leibniz formula to solve this difficult equation?,,"Suppose there exists a $y$ such that $$y \equiv \frac{d^n}{dx^n}e^{-x^2/2}$$. Prove that $$\frac{d^2y}{dx^2} + x\frac{dy}{dx} + (n+1)y = 0$$ I'm not sure where to start as Leibniz formula require at least 2 functions to begin with. There are no clear two functions in this problem. My thought process: I could possibly factor out the y in all 3 terms, but this will make the derivatives invalid, wouldn't it? So since approach 1 wouldn't work, I could try integrating the whole equation, but I wouldn't get an exponential somewhere. I can try to sub in y into the equation, but I don't know how to proceed from here.","Suppose there exists a $y$ such that $$y \equiv \frac{d^n}{dx^n}e^{-x^2/2}$$. Prove that $$\frac{d^2y}{dx^2} + x\frac{dy}{dx} + (n+1)y = 0$$ I'm not sure where to start as Leibniz formula require at least 2 functions to begin with. There are no clear two functions in this problem. My thought process: I could possibly factor out the y in all 3 terms, but this will make the derivatives invalid, wouldn't it? So since approach 1 wouldn't work, I could try integrating the whole equation, but I wouldn't get an exponential somewhere. I can try to sub in y into the equation, but I don't know how to proceed from here.",,"['calculus', 'ordinary-differential-equations']"
29,What is the intuitive reason that partial differential equations are hard to solve?,What is the intuitive reason that partial differential equations are hard to solve?,,My question is why it is difficult to find analytical solutions for these equations. Is there any intuition why numerical solutions are preferred?,My question is why it is difficult to find analytical solutions for these equations. Is there any intuition why numerical solutions are preferred?,,"['ordinary-differential-equations', 'partial-derivative']"
30,If $u(x(t))$ then what is $\frac{du}{dt}$?,If  then what is ?,u(x(t)) \frac{du}{dt},My notes say that if $u$ is a function of $x$ and $x$ is a function of $t$ then $$\frac{du}{dt}=\frac{∂u}{∂t}+\frac{∂u}{∂x}\frac{dx}{dt}$$ Let's take a simple example where $u=x$ and $x=t$. Then $u=t$ So $$\frac{du}{dt}=1$$ $$\frac{du}{dx}=1$$ $$\frac{dx}{dt}=1$$ Which doesn't agree with the above.,My notes say that if $u$ is a function of $x$ and $x$ is a function of $t$ then $$\frac{du}{dt}=\frac{∂u}{∂t}+\frac{∂u}{∂x}\frac{dx}{dt}$$ Let's take a simple example where $u=x$ and $x=t$. Then $u=t$ So $$\frac{du}{dt}=1$$ $$\frac{du}{dx}=1$$ $$\frac{dx}{dt}=1$$ Which doesn't agree with the above.,,['ordinary-differential-equations']
31,Problem on Wave Function,Problem on Wave Function,,"A particle of mass $m$ is subjected to a force $F(r) = -\nabla V(r)$ such that the wave function $\varphi(p, t)$ satisfies the momentum-space Schrödinger equation $$\left(\frac{p^2}{2m}-a\Delta^2_p\right) \varphi (p,t) = i\frac{\partial \varphi (p,t)}{ \partial t}$$ where $\hbar = 1$, $a$ is some real constant and $$\Delta^2_p \equiv \frac{\partial^2 }{ \partial p^2_x} + \frac{\partial^2}{ \partial p^2_y } + \frac{\partial^2 }{\partial^2_z} \, .$$ How do we find force $F(r) \equiv -\nabla V(r)$? We know that the coordinate and momentum representations of a wave function are related by $$\\psi (r,t) = \left(\frac {1}{2\pi}\right)^{\frac {3}{2}} \int \varphi (k,t) e^{ik\cdot r} \mathrm dk \tag {1}$$ $$\varphi (k,t) = \left(\frac {1}{2\pi}\right)^{\frac {3}{2}} \int \psi (r,t) e^{-ik\cdot r} \mathrm dr \tag {2}$$ where $k \equiv p / \hbar$ with  $Ii  = 1$.","A particle of mass $m$ is subjected to a force $F(r) = -\nabla V(r)$ such that the wave function $\varphi(p, t)$ satisfies the momentum-space Schrödinger equation $$\left(\frac{p^2}{2m}-a\Delta^2_p\right) \varphi (p,t) = i\frac{\partial \varphi (p,t)}{ \partial t}$$ where $\hbar = 1$, $a$ is some real constant and $$\Delta^2_p \equiv \frac{\partial^2 }{ \partial p^2_x} + \frac{\partial^2}{ \partial p^2_y } + \frac{\partial^2 }{\partial^2_z} \, .$$ How do we find force $F(r) \equiv -\nabla V(r)$? We know that the coordinate and momentum representations of a wave function are related by $$\\psi (r,t) = \left(\frac {1}{2\pi}\right)^{\frac {3}{2}} \int \varphi (k,t) e^{ik\cdot r} \mathrm dk \tag {1}$$ $$\varphi (k,t) = \left(\frac {1}{2\pi}\right)^{\frac {3}{2}} \int \psi (r,t) e^{-ik\cdot r} \mathrm dr \tag {2}$$ where $k \equiv p / \hbar$ with  $Ii  = 1$.",,"['integration', 'ordinary-differential-equations', 'physics', 'mathematical-physics', 'quantum-mechanics']"
32,Using partial fractions in ODE,Using partial fractions in ODE,,"I am trying to solve the following ODE via different methods as in this question : $$ \begin{align} \frac{dr}{dt} &= ar(1-r) + b (1-r)^2 \end{align} $$ The correct solution is given by: $$ r(t) = \frac{e^{at} -1}{ e^{at} - 1 + (a/b)} $$ In this part I try to use partial fractions. I have never used this method before and am quite unsure of how to proceed. My first step is to separate the equation. Here is my attempt: $$ \int \frac{1}{ar(1-r) + b(1-r)^2} = \int dt  $$ $$ = \int \bigg\{ \frac{1}{ar} + \frac{1}{a(1-r)}\bigg\}dr + 2\int \frac{1}{b(1-r)} dr $$ Then, $$ t = \frac 1 a \ln ( r(r-1)) - \frac 2b \ln (r-1) +c  $$ Taking the exponential: $$ e^{at} = r(r -1) + (r-1)^{-2a/b} $$ I am not sure if this is correct, or how to proceed? Any help is much appreciated!","I am trying to solve the following ODE via different methods as in this question : $$ \begin{align} \frac{dr}{dt} &= ar(1-r) + b (1-r)^2 \end{align} $$ The correct solution is given by: $$ r(t) = \frac{e^{at} -1}{ e^{at} - 1 + (a/b)} $$ In this part I try to use partial fractions. I have never used this method before and am quite unsure of how to proceed. My first step is to separate the equation. Here is my attempt: $$ \int \frac{1}{ar(1-r) + b(1-r)^2} = \int dt  $$ $$ = \int \bigg\{ \frac{1}{ar} + \frac{1}{a(1-r)}\bigg\}dr + 2\int \frac{1}{b(1-r)} dr $$ Then, $$ t = \frac 1 a \ln ( r(r-1)) - \frac 2b \ln (r-1) +c  $$ Taking the exponential: $$ e^{at} = r(r -1) + (r-1)^{-2a/b} $$ I am not sure if this is correct, or how to proceed? Any help is much appreciated!",,"['ordinary-differential-equations', 'proof-verification']"
33,Definition of eigenvalue for Sturm-Liouville problem.,Definition of eigenvalue for Sturm-Liouville problem.,,"I am familiar with the notion of an eigenvalue/eigenvector pair for a linear operator. Explicitly: If $V$ is a vector space over a field $K$, and $T$ is a linear operator on $V$, then we call $\lambda \in K$ an eigenvalue if there exists a non-zero vector $v \in V$ such that $Tv = \lambda v.$ I am now studying differential equations and am learning about Sturm- Liouville problems. There we define a linear differential operator  $$L[y] = -[p(x) y']' + q(x)y$$ for some functions $p(x)$ and $q(x)$, and $y$ is a function in some appropriate function space (which function space is not entirely clear, but I do not think that is too relevant to my question). We then consider the Sturm-Liouville problem: \begin{equation}\tag{1} \label{SL}  L[\phi] = \lambda r(x) \phi  \end{equation} for some function $r(x)$, subject to some boundary conditions. My confusion comes from this $r(x)$. Most sources say if $\lambda$ is some complex number for which there exists a non-trivial solution $\phi$ to \ref{SL}, then  $\lambda$ is an eigenvalue and $\phi$ the associated eigenfunction. But I would expect the problem to be \begin{equation}\tag{2} \label{expected} L[\phi] = \lambda \phi . \end{equation}  How can the notion of an eigenvalue/eigenvector pair jive with this $r(x)$? Is this just a difference in definitions for Sturm-Liouville theory? Is it a notational convenience? Because we could just define $$L[y] = \frac{1}{r(x)}\left( -[p(x) y']' + q(x)y \right),$$ and then things are how we expect them in \ref{expected}.","I am familiar with the notion of an eigenvalue/eigenvector pair for a linear operator. Explicitly: If $V$ is a vector space over a field $K$, and $T$ is a linear operator on $V$, then we call $\lambda \in K$ an eigenvalue if there exists a non-zero vector $v \in V$ such that $Tv = \lambda v.$ I am now studying differential equations and am learning about Sturm- Liouville problems. There we define a linear differential operator  $$L[y] = -[p(x) y']' + q(x)y$$ for some functions $p(x)$ and $q(x)$, and $y$ is a function in some appropriate function space (which function space is not entirely clear, but I do not think that is too relevant to my question). We then consider the Sturm-Liouville problem: \begin{equation}\tag{1} \label{SL}  L[\phi] = \lambda r(x) \phi  \end{equation} for some function $r(x)$, subject to some boundary conditions. My confusion comes from this $r(x)$. Most sources say if $\lambda$ is some complex number for which there exists a non-trivial solution $\phi$ to \ref{SL}, then  $\lambda$ is an eigenvalue and $\phi$ the associated eigenfunction. But I would expect the problem to be \begin{equation}\tag{2} \label{expected} L[\phi] = \lambda \phi . \end{equation}  How can the notion of an eigenvalue/eigenvector pair jive with this $r(x)$? Is this just a difference in definitions for Sturm-Liouville theory? Is it a notational convenience? Because we could just define $$L[y] = \frac{1}{r(x)}\left( -[p(x) y']' + q(x)y \right),$$ and then things are how we expect them in \ref{expected}.",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'eigenfunctions', 'sturm-liouville']"
34,Can polynomial be a solution to linear ordinary differential equation?,Can polynomial be a solution to linear ordinary differential equation?,,Can polynomial \begin{equation} \pi_x(t) = \sum^m_{k=0}x_kt^k \end{equation} be a solution to  a linear ordinary differential equation: \begin{equation} \dot{x}(t) = A x(t) \end{equation} where $x \in \mathbb{R}^n$ and $A \in \mathbb{R}^{n \times n}$.,Can polynomial \begin{equation} \pi_x(t) = \sum^m_{k=0}x_kt^k \end{equation} be a solution to  a linear ordinary differential equation: \begin{equation} \dot{x}(t) = A x(t) \end{equation} where $x \in \mathbb{R}^n$ and $A \in \mathbb{R}^{n \times n}$.,,"['linear-algebra', 'ordinary-differential-equations', 'polynomials']"
35,Is the travelling wave ansatz the only solution for linear PDEs?,Is the travelling wave ansatz the only solution for linear PDEs?,,"If I have a linear PDE such as $$U_t=U_{xxx}+U_{xx}+U,$$ I know that particular solutions exist of the form $U=e^{\lambda t}e^{\alpha x}$.  I heard that the total solution is the sum of such travelling waves. However can't other types of solutions exist? I often hear textbooks say they ""seek"" travelling wave solutions or that the solution is an ansatz, implying that other types exist. If not, what's the proof? Furthermore if I have coupled equations such as \begin{align} U_t&=U_{xx}+V_x+V\\ V_t&=V_{xx}+U_x, \end{align} is the solution that $U,V$ are travelling waves also the only type of solution?","If I have a linear PDE such as $$U_t=U_{xxx}+U_{xx}+U,$$ I know that particular solutions exist of the form $U=e^{\lambda t}e^{\alpha x}$.  I heard that the total solution is the sum of such travelling waves. However can't other types of solutions exist? I often hear textbooks say they ""seek"" travelling wave solutions or that the solution is an ansatz, implying that other types exist. If not, what's the proof? Furthermore if I have coupled equations such as \begin{align} U_t&=U_{xx}+V_x+V\\ V_t&=V_{xx}+U_x, \end{align} is the solution that $U,V$ are travelling waves also the only type of solution?",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'wave-equation']"
36,"Differential equation $y'(x)=\frac{-\sqrt{y(x)}}{1+x}$ with two branches, are these both valid?","Differential equation  with two branches, are these both valid?",y'(x)=\frac{-\sqrt{y(x)}}{1+x},"I have the following differential equation: $$\frac{dy}{dx} = \frac{-\sqrt{y}}{1+x} $$ by simple integration we end up with: $$y(x) = (C-\frac{\ln{1+x}}{2})^2 $$ where C is a constant. Now we do also have the condition $y(0) = 1$, which leads to $C=\pm 1$. My question is: are both branches (constants) valid? $C_1 = 1$ does obviously lead to a valid solution, but how about $C_2=-1$? $C=-1$ gives $$y_2(x) = (-1-\frac{\ln{1+x}}{2})^2 $$ which when substituted back in the differenial equation gives the correct result. However, we also have: $$y_2(x) = (-1)^2(1+\frac{\ln{1+x}}{2})^2 = (1+\frac{\ln{1+x}}{2})^2$$ which when substituted back doesn't give the right result, as the minus sign is missing. What solves this apparent contradiction?","I have the following differential equation: $$\frac{dy}{dx} = \frac{-\sqrt{y}}{1+x} $$ by simple integration we end up with: $$y(x) = (C-\frac{\ln{1+x}}{2})^2 $$ where C is a constant. Now we do also have the condition $y(0) = 1$, which leads to $C=\pm 1$. My question is: are both branches (constants) valid? $C_1 = 1$ does obviously lead to a valid solution, but how about $C_2=-1$? $C=-1$ gives $$y_2(x) = (-1-\frac{\ln{1+x}}{2})^2 $$ which when substituted back in the differenial equation gives the correct result. However, we also have: $$y_2(x) = (-1)^2(1+\frac{\ln{1+x}}{2})^2 = (1+\frac{\ln{1+x}}{2})^2$$ which when substituted back doesn't give the right result, as the minus sign is missing. What solves this apparent contradiction?",,['ordinary-differential-equations']
37,"when are the successive approximations using picard's method for solving an ODE, are the terms of the taylor expansion of the solution of the ODE","when are the successive approximations using picard's method for solving an ODE, are the terms of the taylor expansion of the solution of the ODE",,"The question is as stated in the title, as is ""when are the successive approximations using picard's method for solving an ODE, are the terms of the taylor expansion about $x=0$ of the solution of the ODE"". In this question : Solve $y'=y$, $y(0)=1$ using method of successive approximations, obtaining the power series expansions of the solution , the approximations are indeed the terms of the taylor expansion about $x=0$ of the solution. But, when solving $y' = y^2, y(0)=1$, the second approximation is not the same as the taylor expansion about $x=0$ of the solution of the ODE. first approximation = 1 second approximation = $1+x$ third approximation = $1+x+x^2+x^3/3$ Any help appreciated.","The question is as stated in the title, as is ""when are the successive approximations using picard's method for solving an ODE, are the terms of the taylor expansion about $x=0$ of the solution of the ODE"". In this question : Solve $y'=y$, $y(0)=1$ using method of successive approximations, obtaining the power series expansions of the solution , the approximations are indeed the terms of the taylor expansion about $x=0$ of the solution. But, when solving $y' = y^2, y(0)=1$, the second approximation is not the same as the taylor expansion about $x=0$ of the solution of the ODE. first approximation = 1 second approximation = $1+x$ third approximation = $1+x+x^2+x^3/3$ Any help appreciated.",,['ordinary-differential-equations']
38,"Given $y=e^{rx}$, for which values of $r$ does $y'' - 2y' - 3y = 0$?","Given , for which values of  does ?",y=e^{rx} r y'' - 2y' - 3y = 0,"I am trying to solve this problem but I am not able to do it. $y=e^{rx}$ : For what values of $r$ does $y'' - 2y' - 3y = 0$. I tried to differentiate $y$ with respect to $x$, but I get something like : $e^{rx}\cdot \frac{d}{dx}(rx)$, which does not seem to help me a lot. How I am supposed to do this ?","I am trying to solve this problem but I am not able to do it. $y=e^{rx}$ : For what values of $r$ does $y'' - 2y' - 3y = 0$. I tried to differentiate $y$ with respect to $x$, but I get something like : $e^{rx}\cdot \frac{d}{dx}(rx)$, which does not seem to help me a lot. How I am supposed to do this ?",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
39,Solve the differential equation $x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}+(x-1)y=0$,Solve the differential equation,x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}+(x-1)y=0,"$$x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}+(x-1)y=0$$ $$\sum_{n=0}^{\infty}(n+r)(n+r-1)c_nx^{n+r}+\sum_{n=0}^{\infty}(n+r)c_nx^{n+r}+\sum_{n=1}^{\infty}c_{n-1}x^{n+r}-\sum_{n=0}^{\infty}c_nx^{n+r}$$ The exponents of the equation is then, $$r^2-1=0$$ $$r_1=1,r_2=-1$$ The first solution is given by $$c_n=\frac{c_{n-1}}{1-(n+1)^2}$$ $$c_1=-\frac{c_0}{3}$$ $$c_2=-\frac{c_1}{8}$$ $$c_3=-\frac{c_2}{15}$$ $$y_1(x)=1+2\sum_{n=1}^{\infty}\frac{(-1)^n(x^n)}{(n!)(n+2)!}$$ I can't seem to find the second solution, $$r=r_2=-1$$ The recurrence formula is given by $$c_n=\frac{c_{n-1}}{1-(n-1)^2},n\neq 2$$ We then have linearly independent solution For $n>2$ $$c_3=-\frac{c_2}{3}$$ $$c_4=-\frac{c_3}{8}$$ $$c_5=-\frac{c_4}{15}$$ $$c_6=-\frac{c_5}{24}$$ I notice that the two are the same. How shall I find the linearly independent $y_2$ with log in it. Any help would be appreciated.","$$x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}+(x-1)y=0$$ $$\sum_{n=0}^{\infty}(n+r)(n+r-1)c_nx^{n+r}+\sum_{n=0}^{\infty}(n+r)c_nx^{n+r}+\sum_{n=1}^{\infty}c_{n-1}x^{n+r}-\sum_{n=0}^{\infty}c_nx^{n+r}$$ The exponents of the equation is then, $$r^2-1=0$$ $$r_1=1,r_2=-1$$ The first solution is given by $$c_n=\frac{c_{n-1}}{1-(n+1)^2}$$ $$c_1=-\frac{c_0}{3}$$ $$c_2=-\frac{c_1}{8}$$ $$c_3=-\frac{c_2}{15}$$ $$y_1(x)=1+2\sum_{n=1}^{\infty}\frac{(-1)^n(x^n)}{(n!)(n+2)!}$$ I can't seem to find the second solution, $$r=r_2=-1$$ The recurrence formula is given by $$c_n=\frac{c_{n-1}}{1-(n-1)^2},n\neq 2$$ We then have linearly independent solution For $n>2$ $$c_3=-\frac{c_2}{3}$$ $$c_4=-\frac{c_3}{8}$$ $$c_5=-\frac{c_4}{15}$$ $$c_6=-\frac{c_5}{24}$$ I notice that the two are the same. How shall I find the linearly independent $y_2$ with log in it. Any help would be appreciated.",,['ordinary-differential-equations']
40,"Suppose $x:[0,\infty) \to [0,\infty)$ is continuous and $x(0)=0 $",Suppose  is continuous and,"x:[0,\infty) \to [0,\infty) x(0)=0 ","Suppose $x:[0,\infty) \to [0,\infty)$ is continuous and $x(0)=0 $ If $$(x(t))^2 \leq  2+\int_{0}^{t}x\left(s\right){\rm d}s . $$  for all $t \geq 0$, Then which of the following is true? $a$) $x(\sqrt2)\in[0,2]$ $b$) $x(\sqrt2)\in [0,\frac3{\sqrt2}]$ $c$) $x(\sqrt2) \in [\frac5{\sqrt2},\frac7{\sqrt2}]$ $d$) $x(\sqrt2)\in [10,\infty)$ I tried function $x=0$ and $x=\frac{3t}2$. And I want to discard $a$ or $b$ ...(as I think $a$ should be wrong and $b$ is answer) not arguments like $a$ is true then $b$ should be true and only one answer can be correct so $b$ is correct.","Suppose $x:[0,\infty) \to [0,\infty)$ is continuous and $x(0)=0 $ If $$(x(t))^2 \leq  2+\int_{0}^{t}x\left(s\right){\rm d}s . $$  for all $t \geq 0$, Then which of the following is true? $a$) $x(\sqrt2)\in[0,2]$ $b$) $x(\sqrt2)\in [0,\frac3{\sqrt2}]$ $c$) $x(\sqrt2) \in [\frac5{\sqrt2},\frac7{\sqrt2}]$ $d$) $x(\sqrt2)\in [10,\infty)$ I tried function $x=0$ and $x=\frac{3t}2$. And I want to discard $a$ or $b$ ...(as I think $a$ should be wrong and $b$ is answer) not arguments like $a$ is true then $b$ should be true and only one answer can be correct so $b$ is correct.",,['ordinary-differential-equations']
41,Express the differential equation as power series,Express the differential equation as power series,,"My attempt: $y=\sum^{\infty}_{n=0}a_n x^n \rightarrow y'=\sum^{\infty}_{n=0}na_n x^{n-1}\rightarrow y''=\sum^{\infty}_{n=0}n(n-1)a_n x^{n-2}\\ (2+x^2)y""+x^2y'+3y=(2+x^2)\sum^{\infty}_{n=0}n(n-1)a_n x^{n-2}+x^2\sum^{\infty}_{n=0}na_n x^{n-1}+3\sum^{\infty}_{n=0}a_n x^n\\ 2\sum^{\infty}_{n=0}n(n-1)a_n x^{n-2}+\sum^{\infty}_{n=0}n(n-1)a_n x^{n}+\sum^{\infty}_{n=0}na_n x^{n+1}+3\sum^{\infty}_{n=0}a_n x^{n}$ from here can any help me","My attempt: $y=\sum^{\infty}_{n=0}a_n x^n \rightarrow y'=\sum^{\infty}_{n=0}na_n x^{n-1}\rightarrow y''=\sum^{\infty}_{n=0}n(n-1)a_n x^{n-2}\\ (2+x^2)y""+x^2y'+3y=(2+x^2)\sum^{\infty}_{n=0}n(n-1)a_n x^{n-2}+x^2\sum^{\infty}_{n=0}na_n x^{n-1}+3\sum^{\infty}_{n=0}a_n x^n\\ 2\sum^{\infty}_{n=0}n(n-1)a_n x^{n-2}+\sum^{\infty}_{n=0}n(n-1)a_n x^{n}+\sum^{\infty}_{n=0}na_n x^{n+1}+3\sum^{\infty}_{n=0}a_n x^{n}$ from here can any help me",,['ordinary-differential-equations']
42,A question about Finite Element Method.,A question about Finite Element Method.,,"Can we solve a matrix differential equation $\textbf{X}'(t)=\textbf{A}\textbf{X}(t)+\textbf{B}(t)$ by Finite Element Method? I will be very happy if you give some special example or suggest some books about the method for solving matrix differential eauations. In here,  \begin{equation} \begin{aligned} &\textbf{X}(t)=\left( \begin{array}{cccc} x_{1}(t) \\ x_{2}(t) \\ x_{3}(t)\\ x_{4}(t) \end{array} \right), \textbf{A} = \left( \begin{array}{cccc} a_{11} & a_{12} &a_{13} & a_{14}\\ a_{21} & a_{22} & a_{23} & a_{24} \\ a_{31} & a_{32}& a_{33} & a_{34} \\ a_{41} & a_{42} & a_{43} & a_{44} \end{array} \right),\\ &\textbf{B}(t)=\left( \begin{array}{cccc} b_{1}(t)\\ b_{2}(t)\\ b_{3}(t)\\ b_{4}(t) \end{array} \right). \end{aligned} \end{equation} initial conditions: $$x_1(0)=m_1$$ $$x_2(0)=m_2$$ $$x_3(0)=m_3$$ $$x_4(0)=m_4.$$","Can we solve a matrix differential equation $\textbf{X}'(t)=\textbf{A}\textbf{X}(t)+\textbf{B}(t)$ by Finite Element Method? I will be very happy if you give some special example or suggest some books about the method for solving matrix differential eauations. In here,  \begin{equation} \begin{aligned} &\textbf{X}(t)=\left( \begin{array}{cccc} x_{1}(t) \\ x_{2}(t) \\ x_{3}(t)\\ x_{4}(t) \end{array} \right), \textbf{A} = \left( \begin{array}{cccc} a_{11} & a_{12} &a_{13} & a_{14}\\ a_{21} & a_{22} & a_{23} & a_{24} \\ a_{31} & a_{32}& a_{33} & a_{34} \\ a_{41} & a_{42} & a_{43} & a_{44} \end{array} \right),\\ &\textbf{B}(t)=\left( \begin{array}{cccc} b_{1}(t)\\ b_{2}(t)\\ b_{3}(t)\\ b_{4}(t) \end{array} \right). \end{aligned} \end{equation} initial conditions: $$x_1(0)=m_1$$ $$x_2(0)=m_2$$ $$x_3(0)=m_3$$ $$x_4(0)=m_4.$$",,"['ordinary-differential-equations', 'numerical-methods', 'finite-element-method']"
43,Convert vector differential equation's order,Convert vector differential equation's order,,"Here is a 2nd order vector differential equation: $$\overrightarrow{Y}''= \begin{pmatrix}a & b \\c & d \end{pmatrix} \overrightarrow{Y}$$ Don't work it out, but write it as a vector differential eqn in $1$st order in higher dimensions. I am not sure where to begin. How does one convert from 2nd to 1st order? Hints are appreciated.","Here is a 2nd order vector differential equation: $$\overrightarrow{Y}''= \begin{pmatrix}a & b \\c & d \end{pmatrix} \overrightarrow{Y}$$ Don't work it out, but write it as a vector differential eqn in $1$st order in higher dimensions. I am not sure where to begin. How does one convert from 2nd to 1st order? Hints are appreciated.",,"['calculus', 'linear-algebra', 'analysis', 'ordinary-differential-equations', 'laplace-transform']"
44,Weird differential equation,Weird differential equation,,"How to solve the given differential equation(given $ f(1)=e$ and  $f(0)=0$, $f'(0)=1$,$f''(0)=0.$) $$f''(x)=2xf'(x)+4f(x).$$ Where $f'(x)$ is first derivative and $f''(x)$ is second derivative. I was trying to guess that the function is exponential(of form of $e^{x^2}$) but couldn't get the final function.","How to solve the given differential equation(given $ f(1)=e$ and  $f(0)=0$, $f'(0)=1$,$f''(0)=0.$) $$f''(x)=2xf'(x)+4f(x).$$ Where $f'(x)$ is first derivative and $f''(x)$ is second derivative. I was trying to guess that the function is exponential(of form of $e^{x^2}$) but couldn't get the final function.",,['ordinary-differential-equations']
45,Solution of nonlinear ODE: $x= yy'-(y')^2$,Solution of nonlinear ODE:,x= yy'-(y')^2,How to solve  $x= yy'-(y')^2.$  Can somebody please hint at some substitution or refer any text related to these type of ode.,How to solve  $x= yy'-(y')^2.$  Can somebody please hint at some substitution or refer any text related to these type of ode.,,['ordinary-differential-equations']
46,Help on solving equation via Laplace Transform,Help on solving equation via Laplace Transform,,"We see that $$f(t) = e^{-3t} + e^{-t} \int_0^t e^{-\tau}f(\tau) d\tau$$ I am having issues solving for $f(t)$. I recognize that if we set $g(\tau) = e^{-\tau} f(\tau)$ and if we set $h(t) = \int_0^t g(\tau) d\tau,$ Then $$f(t) = e^{-3t} + e^{-t}h(t)$$ $$\implies \mathscr{L}\{f(t)\} = \mathscr{L}\{e^{-3t} + e^{-t}h(t)\} = \mathscr{L}\{e^{-3t}\} + \mathscr{L}\{e^{-t}h(t)\}$$ $$= \frac{1}{s+3} + H(s + 1) = \frac{1}{s+3} + \mathscr{L}\{h(t)\}\mid_{s \rightarrow s+1}$$ $$= \frac{1}{s+3} +\mathscr{L}\{\int_0^t g(\tau) d\tau\}\mid_{s \rightarrow s+1} = \frac{1}{s+3} + \frac{G(s + 1)}{s+1}.$$ However, I feel like I'm stuck here, because when I take the inverse Laplace Transform I end up this what I started with. Did I miss a step? I feel like I should be doing something with the convolution theorem.","We see that $$f(t) = e^{-3t} + e^{-t} \int_0^t e^{-\tau}f(\tau) d\tau$$ I am having issues solving for $f(t)$. I recognize that if we set $g(\tau) = e^{-\tau} f(\tau)$ and if we set $h(t) = \int_0^t g(\tau) d\tau,$ Then $$f(t) = e^{-3t} + e^{-t}h(t)$$ $$\implies \mathscr{L}\{f(t)\} = \mathscr{L}\{e^{-3t} + e^{-t}h(t)\} = \mathscr{L}\{e^{-3t}\} + \mathscr{L}\{e^{-t}h(t)\}$$ $$= \frac{1}{s+3} + H(s + 1) = \frac{1}{s+3} + \mathscr{L}\{h(t)\}\mid_{s \rightarrow s+1}$$ $$= \frac{1}{s+3} +\mathscr{L}\{\int_0^t g(\tau) d\tau\}\mid_{s \rightarrow s+1} = \frac{1}{s+3} + \frac{G(s + 1)}{s+1}.$$ However, I feel like I'm stuck here, because when I take the inverse Laplace Transform I end up this what I started with. Did I miss a step? I feel like I should be doing something with the convolution theorem.",,"['ordinary-differential-equations', 'laplace-transform']"
47,Frobenius Method general solution not providing an expected solution,Frobenius Method general solution not providing an expected solution,,"I'm trying to solve the following ODE: $$3xy''+(3x+1)y'+y=0$$ So I started by assuming a solution(and its derivates) with the form: $$ \tag{1}y = \sum_{n=0}^{\infty } a_{n} x^{n + r}$$ $$y' = \sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r-1}$$ $$y'' = \sum_{n=0}^{\infty } {(n+r-1)(n+r)}a_{n} x^{n+r-2}$$ Replacing in original equation: $$3x\sum_{n=0}^{\infty } {(n+r-1)(n+r)}a_{n} x^{n+r-2} + 3x\sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r-1}+\sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r-1} + \sum_{n=0}^{\infty } a_{n} x^{n + r} = 0$$ $$ 3\sum_{n=0}^{\infty } {(n+r-1)(n+r)}a_{n} x^{n+r-1} + \sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r-1} + 3\sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r} + \sum_{n=0}^{\infty } a_{n} x^{n + r} = 0   $$ To transform all $x^{n+r}$ into $x^{n+r-1}$ I've rewritten the equation as: $$ 3\sum_{n=0}^{\infty } {(n+r-1)(n+r)}a_{n} x^{n+r-1} +\sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r-1} +  3\sum_{n=1}^{\infty } {(n+r-1)}a_{n-1} x^{n + r -1} + \sum_{n=1}^{\infty } a_{n-1} x^{n + r - 1} = 0 $$ In order to make all sums start from the $n=1$: $$ \tag{2}  3r(r-1)a_{0}x^{r-1} + ra_{0}x^{r-1} + 3\sum_{n=1}^{\infty } {(n+r-1)(n+r)}a_{n} x^{n+r-1} + \sum_{n=1}^{\infty } {(n+r)}a_{n} x^{n + r-1} + 3\sum_{n=1}^{\infty } {(n+r-1)}a_{n-1} x^{n + r -1} + \sum_{n=1}^{\infty } a_{n-1} x^{n + r - 1} = 0    $$ If I understood correctly, the indicial equation should then be: $$ 3r(r-1)a_{0}x^{r-1} + ra_{0}x^{r-1} = 0 $$ and since $x^{r-1} \neq 0 $ and $a_{0} \neq 0 $: $$ 3r(r-1) + r = 0 $$ $$ r_{1} = 0 $$ $$ r_{2} = \frac{2}{3} $$ From $(2)$: $$ [3(n+r)(n+r-1)+(n+r)]a_{n} + [3(n+r-1)+1]a_{n-1} = 0 $$ which seems to give for both solutions of $r$: $$ a_{n} = \frac{-a_{n-1}}{n}  $$ It seems to me that this is in terms of $a_{0}$: $$ a_{n} = \frac{(-1)^{n}}{n!}a_{0} $$ So rewriting $(1)$ now would give me the solution and $y$ would be an infinite sum since all $a_{n} \neq 0$. And I know this can't be true because by deduction one can see that $y = e^{-x}$ is a solution to the ODE, which makes me think I made a mistake in some of the steps shown above, I'd be glad if anyone could show me where I made the mistake and give me some help in finding the correct general solution for this ODE.","I'm trying to solve the following ODE: $$3xy''+(3x+1)y'+y=0$$ So I started by assuming a solution(and its derivates) with the form: $$ \tag{1}y = \sum_{n=0}^{\infty } a_{n} x^{n + r}$$ $$y' = \sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r-1}$$ $$y'' = \sum_{n=0}^{\infty } {(n+r-1)(n+r)}a_{n} x^{n+r-2}$$ Replacing in original equation: $$3x\sum_{n=0}^{\infty } {(n+r-1)(n+r)}a_{n} x^{n+r-2} + 3x\sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r-1}+\sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r-1} + \sum_{n=0}^{\infty } a_{n} x^{n + r} = 0$$ $$ 3\sum_{n=0}^{\infty } {(n+r-1)(n+r)}a_{n} x^{n+r-1} + \sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r-1} + 3\sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r} + \sum_{n=0}^{\infty } a_{n} x^{n + r} = 0   $$ To transform all $x^{n+r}$ into $x^{n+r-1}$ I've rewritten the equation as: $$ 3\sum_{n=0}^{\infty } {(n+r-1)(n+r)}a_{n} x^{n+r-1} +\sum_{n=0}^{\infty } {(n+r)}a_{n} x^{n + r-1} +  3\sum_{n=1}^{\infty } {(n+r-1)}a_{n-1} x^{n + r -1} + \sum_{n=1}^{\infty } a_{n-1} x^{n + r - 1} = 0 $$ In order to make all sums start from the $n=1$: $$ \tag{2}  3r(r-1)a_{0}x^{r-1} + ra_{0}x^{r-1} + 3\sum_{n=1}^{\infty } {(n+r-1)(n+r)}a_{n} x^{n+r-1} + \sum_{n=1}^{\infty } {(n+r)}a_{n} x^{n + r-1} + 3\sum_{n=1}^{\infty } {(n+r-1)}a_{n-1} x^{n + r -1} + \sum_{n=1}^{\infty } a_{n-1} x^{n + r - 1} = 0    $$ If I understood correctly, the indicial equation should then be: $$ 3r(r-1)a_{0}x^{r-1} + ra_{0}x^{r-1} = 0 $$ and since $x^{r-1} \neq 0 $ and $a_{0} \neq 0 $: $$ 3r(r-1) + r = 0 $$ $$ r_{1} = 0 $$ $$ r_{2} = \frac{2}{3} $$ From $(2)$: $$ [3(n+r)(n+r-1)+(n+r)]a_{n} + [3(n+r-1)+1]a_{n-1} = 0 $$ which seems to give for both solutions of $r$: $$ a_{n} = \frac{-a_{n-1}}{n}  $$ It seems to me that this is in terms of $a_{0}$: $$ a_{n} = \frac{(-1)^{n}}{n!}a_{0} $$ So rewriting $(1)$ now would give me the solution and $y$ would be an infinite sum since all $a_{n} \neq 0$. And I know this can't be true because by deduction one can see that $y = e^{-x}$ is a solution to the ODE, which makes me think I made a mistake in some of the steps shown above, I'd be glad if anyone could show me where I made the mistake and give me some help in finding the correct general solution for this ODE.",,"['ordinary-differential-equations', 'power-series', 'frobenius-method']"
48,Discrepancy between closed form solution and numerical solution,Discrepancy between closed form solution and numerical solution,,"I have the following differential equation $$ \frac{dx}{dt} = -\frac{1}{\alpha t}x(1-x) \,,\,x\in [1,20) \tag 1$$ The solution on this interval  by taking $x(1) = 0.9999$ is given by $$ x(t) = \frac{1}{1+0.0001\exp\left( \frac{\ln(t) - \ln t(1)}{ \alpha}\right)}  ~. \tag 2$$ Now if $\alpha = 0.2$, then $x(20) = \lim_{x\to 20^-} x(t)$, so that $$ x(20) =\frac{1}{1+0.0001\exp\left( \frac{\ln(20)}{ 0.2}\right)}\approx 0.0031 ~. \tag 3$$ On the other hand discretizing (1) using Euler approximation yields  $$ x(t_i)= x(t_{i-1}) - \frac{1}{\alpha t_{i-1}}x(t_{i-1}) (1-x(t_{i-1}) )\Delta t_i~. \tag 4$$ Then $$ x(20) = 0.9999 - \frac{1}{0.2 \cdot 1} 0.9999\cdot (1-0.9999)\cdot (20-1) \approx  0.9904  \tag 5$$ I know the Euler approximation is very conservative but this result is puzzling. I'm sure I'm missing something but I don't know what. Why is there such a ""big"" discrepancy between (3) and (5)?","I have the following differential equation $$ \frac{dx}{dt} = -\frac{1}{\alpha t}x(1-x) \,,\,x\in [1,20) \tag 1$$ The solution on this interval  by taking $x(1) = 0.9999$ is given by $$ x(t) = \frac{1}{1+0.0001\exp\left( \frac{\ln(t) - \ln t(1)}{ \alpha}\right)}  ~. \tag 2$$ Now if $\alpha = 0.2$, then $x(20) = \lim_{x\to 20^-} x(t)$, so that $$ x(20) =\frac{1}{1+0.0001\exp\left( \frac{\ln(20)}{ 0.2}\right)}\approx 0.0031 ~. \tag 3$$ On the other hand discretizing (1) using Euler approximation yields  $$ x(t_i)= x(t_{i-1}) - \frac{1}{\alpha t_{i-1}}x(t_{i-1}) (1-x(t_{i-1}) )\Delta t_i~. \tag 4$$ Then $$ x(20) = 0.9999 - \frac{1}{0.2 \cdot 1} 0.9999\cdot (1-0.9999)\cdot (20-1) \approx  0.9904  \tag 5$$ I know the Euler approximation is very conservative but this result is puzzling. I'm sure I'm missing something but I don't know what. Why is there such a ""big"" discrepancy between (3) and (5)?",,"['real-analysis', 'ordinary-differential-equations', 'numerical-methods']"
49,Find The Wronskian Of The Following ODE,Find The Wronskian Of The Following ODE,,"Let $y_1$ and $y_2$ be two solutions of the problem, $$\begin{align} y''(t)+ay'(t)+by(t)=0,t\in \Bbb R\\ y(0)=0\;\;\;\;\;\;\;\;\; \end{align}$$ where $a$ and $b$ are real constants. Let $W$ be the Wronskian of $y_1$ and $y_2$ . Then ( A ) $W(t)=0\;, \forall t\in \Bbb R$ ( B ) $W(t)=c\;, \forall t\in \Bbb R$ , for some  positive constant $c$ ( C ) $W$ is a non-constant positive function. ( D ) $\exists \;\;t_1,t_2\in \Bbb R $ such that $ W(t_1)<0<W(t_2)\;$ ============================================ My Attempt: Option ( D ) Can be eliminated straight away as the Wronskian will always be strictly positive,strictly negative or zero. I'm pretty confused with the first three options. Since $y_1$ and $y_2$ need not be $2$ independent solutions we cannot directly say the Wronskian vanishes. Hints Please!","Let and be two solutions of the problem, where and are real constants. Let be the Wronskian of and . Then ( A ) ( B ) , for some  positive constant ( C ) is a non-constant positive function. ( D ) such that ============================================ My Attempt: Option ( D ) Can be eliminated straight away as the Wronskian will always be strictly positive,strictly negative or zero. I'm pretty confused with the first three options. Since and need not be independent solutions we cannot directly say the Wronskian vanishes. Hints Please!","y_1 y_2 \begin{align}
y''(t)+ay'(t)+by(t)=0,t\in \Bbb R\\
y(0)=0\;\;\;\;\;\;\;\;\;
\end{align} a b W y_1 y_2 W(t)=0\;, \forall t\in \Bbb R W(t)=c\;, \forall t\in \Bbb R c W \exists \;\;t_1,t_2\in \Bbb R   W(t_1)<0<W(t_2)\; y_1 y_2 2","['ordinary-differential-equations', 'wronskian']"
50,Solutions to $y''(x)-y(x)=0$,Solutions to,y''(x)-y(x)=0,"When I solve the differential equation $$y''(x)-y(x)=0$$ using the auxiliary equation, I get $$y(x)=Ae^{x}+Be^{-x}.$$ Is $$y(x)=A\sinh(x)+B\cosh(x)$$ just as good of a solution? How about $$y(x)=A\cosh(x)+B\cosh(x),$$ etc.? Does it have infinitely many general solutions? Is this normal? Thanks. :)","When I solve the differential equation $$y''(x)-y(x)=0$$ using the auxiliary equation, I get $$y(x)=Ae^{x}+Be^{-x}.$$ Is $$y(x)=A\sinh(x)+B\cosh(x)$$ just as good of a solution? How about $$y(x)=A\cosh(x)+B\cosh(x),$$ etc.? Does it have infinitely many general solutions? Is this normal? Thanks. :)",,"['calculus', 'ordinary-differential-equations', 'hyperbolic-functions']"
51,Wronskian of a self-adjoint form (sturm-liouville),Wronskian of a self-adjoint form (sturm-liouville),,"I'm studying Sturm-Liouville theory and an exercise asks me: Show the following when the linear second-order differential equation   is expressed in self-adjoint form: a) The Wronskian is equal to a constant divided by the initial   coefficient $p$: $$W(x) = \frac{C}{p(x)}$$ b) A second solution is given by $$y_2(x) = Cy_1(x)\int^x \frac{dt}{p(t)[y_1(t)]^2}$$ As far as I know, a self-adjoitn form is something in the form: $$\frac{d}{dx}\left[p(x)\frac{du(x)}{dx}\right] + q(x)u(x)$$ And the Wronskian of the two solutions $y_1, y_2$ of a general second order linear ODE, is $W(x) = y_1(x)y_2'(x)-y_1'(x)y_2(x)$. We also have that the wronskian can be expressed as $$W(x) = W(a) \exp\left[-\int_a^xP(x_1)dx_1\right]$$ for some $a$ that I don't remember. Is there some connection between these formulas and what I need to prove? Also, for b), I know that there is a method of finding a second solution based on the first solution, the wronskian, and the coefficients of the ODE, but how exactly should I do? I presume we have to use the wronskian above, but how?","I'm studying Sturm-Liouville theory and an exercise asks me: Show the following when the linear second-order differential equation   is expressed in self-adjoint form: a) The Wronskian is equal to a constant divided by the initial   coefficient $p$: $$W(x) = \frac{C}{p(x)}$$ b) A second solution is given by $$y_2(x) = Cy_1(x)\int^x \frac{dt}{p(t)[y_1(t)]^2}$$ As far as I know, a self-adjoitn form is something in the form: $$\frac{d}{dx}\left[p(x)\frac{du(x)}{dx}\right] + q(x)u(x)$$ And the Wronskian of the two solutions $y_1, y_2$ of a general second order linear ODE, is $W(x) = y_1(x)y_2'(x)-y_1'(x)y_2(x)$. We also have that the wronskian can be expressed as $$W(x) = W(a) \exp\left[-\int_a^xP(x_1)dx_1\right]$$ for some $a$ that I don't remember. Is there some connection between these formulas and what I need to prove? Also, for b), I know that there is a method of finding a second solution based on the first solution, the wronskian, and the coefficients of the ODE, but how exactly should I do? I presume we have to use the wronskian above, but how?",,"['calculus', 'linear-algebra', 'ordinary-differential-equations', 'derivatives', 'sturm-liouville']"
52,Solution to ODE $y'y^2+yy''-(y')^2=0$,Solution to ODE,y'y^2+yy''-(y')^2=0,I am asked to solve $y'y^2+yy''-(y')^2=0$ My first thought was to substitute $y''=p$ yet I would not know what to do with $y$. I am out of ideas how to approach this problem.,I am asked to solve $y'y^2+yy''-(y')^2=0$ My first thought was to substitute $y''=p$ yet I would not know what to do with $y$. I am out of ideas how to approach this problem.,,['ordinary-differential-equations']
53,Solve the differential equation IVP,Solve the differential equation IVP,,"Solve $\displaystyle x \frac{dy}{dx} = 2y + x^2, y(1) = 2$ We can use the homogeneous form, $y = vx$, so $dy/dx = xdv/dx + v$ $x^2dv/dx + vx = 2x(v + x) \implies x^2 dv/dx = vx + x \implies x dv/dx = v + 1 \implies (v+1) dv = xdx \implies v^2/2 + v = x^2 + C$ Then we would just substitute $v = y/x$ But is there a simpler method?","Solve $\displaystyle x \frac{dy}{dx} = 2y + x^2, y(1) = 2$ We can use the homogeneous form, $y = vx$, so $dy/dx = xdv/dx + v$ $x^2dv/dx + vx = 2x(v + x) \implies x^2 dv/dx = vx + x \implies x dv/dx = v + 1 \implies (v+1) dv = xdx \implies v^2/2 + v = x^2 + C$ Then we would just substitute $v = y/x$ But is there a simpler method?",,"['calculus', 'ordinary-differential-equations']"
54,derivative of a quotient is not the quotient of the derivatives,derivative of a quotient is not the quotient of the derivatives,,"Ive seen multiple sources say something like ""A derivative of a quotient is not the quotient (in general) of the derivatives"" and noticed they all said ""In general"" but never gave such example. So Ive been searching the internet for any possible examples in which a derivative of a quotient is the quotient of the derivatives but without any luck. Could someone give me an example? e.g.  f(x) = ... g(x) = ... so that: f'(x)/g'(x) = (f(x)/g(x))'","Ive seen multiple sources say something like ""A derivative of a quotient is not the quotient (in general) of the derivatives"" and noticed they all said ""In general"" but never gave such example. So Ive been searching the internet for any possible examples in which a derivative of a quotient is the quotient of the derivatives but without any luck. Could someone give me an example? e.g.  f(x) = ... g(x) = ... so that: f'(x)/g'(x) = (f(x)/g(x))'",,['ordinary-differential-equations']
55,Fredholm Alternative - what went wrong?,Fredholm Alternative - what went wrong?,,"So I found the general solution for this DE with these BCs: $$\frac{d^2u}{dx^2}+u=1$$ $$u(0)=u(L)=0$$ by doing: $$u(x)=Acos(x)+Bsin(x)+1$$ $$u(0)=0=A+1 \therefore A=-1$$ $$u(L)=0=-cos(L)+Bsin(L)+1 \therefore B=\frac{cos(L)-1}{sin(L)}$$ $$\implies u(x)=-cos(x)+1+sin(x)\frac{cos(L)-1}{sin(L)}=1-cos(x)-sin(x)tan(\frac{L}{2})$$ So then I wanted to check the Fredholm Alternative. So the homogeneous solution would be: $$u_h(x)=Acos(x)+Bsin(x)$$ $$u_h(0)=0=A \therefore A=0$$ $$u_h(L)=0=Bsin(L) \implies L=n\pi \implies B=1$$ $$\implies u_h(x)=sin(x)$$ so checking Fredholm... $$\int_{0}^{L} (1)sin(x) dx=-cos(x)\Big|_0^L=1-cos(L)\ne0$$ Could anyone give me insight what's going on here? Doesn't this mean that the Fredholm Alternative is telling me there's no solution? What's going on here? Any help in understanding this would be greatly appreciated. edit: The Fredholm alternative summarizes the results for nonhomogeneous problems: $$L(u)=f(x)$$ subject to homogeneous BCs of the self-adjoint type. Either: $u=0$ is the only homogeneous solution (i.e. $\lambda \ne 0$) in which case the nonhomogeneous problem has a unqiue solutions, or there are nontrivial homogeneous solutions $\phi_h(x)$ (i.e. $\lambda = 0$ is an eigenvalue) in which case the nonhomogeneous problem has no solutions or an infinite number of solutions For the nonhomogeneous problem with homogeneous BCs, then, solutions exist only if the forcing function is orthogonal to all homogeneous solutions. The forcing function being orthogonal to the homogeneous solution (with weight 1) is represented by: $$\int_{a}^{b}f(x) \phi_h(x) dx=0$$ The number of solutions of $L(u)=f(x)$ subject to homogeneous BCs is given by: if $\phi_h=0$ ($\lambda\ne0$) and $\int_{a}^{b}f(x) \phi_h(x) dx=0$ then the number of solutions is $=1$ if $\phi_h\ne0$ ($\lambda=0$) and $\int_{a}^{b}f(x) \phi_h(x) dx=0$ then the number of solutions is $=\infty$ if $\phi_h\ne0$ ($\lambda=0$) and $\int_{a}^{b}f(x) \phi_h(x) dx\ne0$ then the number of solutions is $=0$","So I found the general solution for this DE with these BCs: $$\frac{d^2u}{dx^2}+u=1$$ $$u(0)=u(L)=0$$ by doing: $$u(x)=Acos(x)+Bsin(x)+1$$ $$u(0)=0=A+1 \therefore A=-1$$ $$u(L)=0=-cos(L)+Bsin(L)+1 \therefore B=\frac{cos(L)-1}{sin(L)}$$ $$\implies u(x)=-cos(x)+1+sin(x)\frac{cos(L)-1}{sin(L)}=1-cos(x)-sin(x)tan(\frac{L}{2})$$ So then I wanted to check the Fredholm Alternative. So the homogeneous solution would be: $$u_h(x)=Acos(x)+Bsin(x)$$ $$u_h(0)=0=A \therefore A=0$$ $$u_h(L)=0=Bsin(L) \implies L=n\pi \implies B=1$$ $$\implies u_h(x)=sin(x)$$ so checking Fredholm... $$\int_{0}^{L} (1)sin(x) dx=-cos(x)\Big|_0^L=1-cos(L)\ne0$$ Could anyone give me insight what's going on here? Doesn't this mean that the Fredholm Alternative is telling me there's no solution? What's going on here? Any help in understanding this would be greatly appreciated. edit: The Fredholm alternative summarizes the results for nonhomogeneous problems: $$L(u)=f(x)$$ subject to homogeneous BCs of the self-adjoint type. Either: $u=0$ is the only homogeneous solution (i.e. $\lambda \ne 0$) in which case the nonhomogeneous problem has a unqiue solutions, or there are nontrivial homogeneous solutions $\phi_h(x)$ (i.e. $\lambda = 0$ is an eigenvalue) in which case the nonhomogeneous problem has no solutions or an infinite number of solutions For the nonhomogeneous problem with homogeneous BCs, then, solutions exist only if the forcing function is orthogonal to all homogeneous solutions. The forcing function being orthogonal to the homogeneous solution (with weight 1) is represented by: $$\int_{a}^{b}f(x) \phi_h(x) dx=0$$ The number of solutions of $L(u)=f(x)$ subject to homogeneous BCs is given by: if $\phi_h=0$ ($\lambda\ne0$) and $\int_{a}^{b}f(x) \phi_h(x) dx=0$ then the number of solutions is $=1$ if $\phi_h\ne0$ ($\lambda=0$) and $\int_{a}^{b}f(x) \phi_h(x) dx=0$ then the number of solutions is $=\infty$ if $\phi_h\ne0$ ($\lambda=0$) and $\int_{a}^{b}f(x) \phi_h(x) dx\ne0$ then the number of solutions is $=0$",,"['integration', 'ordinary-differential-equations', 'partial-differential-equations', 'orthogonality', 'homogeneous-equation']"
56,Different solutions for PDE $u_x+u_y=1$,Different solutions for PDE,u_x+u_y=1,"I try to solve a very basic PDE problem ""$u_x+u_y=1$"" Here is my approach. I try to use change coordinates method. notice $(u_x, u_y)$ $\cdot$ (1, 1)=1 Let $x_1=x+y, y_1=x-y$, so $x = (x_1+y_2)/2, y = (x_1 - y_2)/2$ Then $""u_x = u_{x_1} + u_{y_1}, u_y= u_{x_1} - u_{y_1}$ Then we get $U_x+u_y=2u_{x_1}=1, i.e. u_{x_1}=1/2.$  Then general solution for such ODE is $u=(x_1)/2 + f(y_1)$ So we get solution to the ODE =(x+y)/2 + f(x-y) However, I search online found there are solutions y+ f(x - y) =1 or = x+ f(x - y) which still work. Are all of them(including mine) the correct solutions? Why there are such huge differences?","I try to solve a very basic PDE problem ""$u_x+u_y=1$"" Here is my approach. I try to use change coordinates method. notice $(u_x, u_y)$ $\cdot$ (1, 1)=1 Let $x_1=x+y, y_1=x-y$, so $x = (x_1+y_2)/2, y = (x_1 - y_2)/2$ Then $""u_x = u_{x_1} + u_{y_1}, u_y= u_{x_1} - u_{y_1}$ Then we get $U_x+u_y=2u_{x_1}=1, i.e. u_{x_1}=1/2.$  Then general solution for such ODE is $u=(x_1)/2 + f(y_1)$ So we get solution to the ODE =(x+y)/2 + f(x-y) However, I search online found there are solutions y+ f(x - y) =1 or = x+ f(x - y) which still work. Are all of them(including mine) the correct solutions? Why there are such huge differences?",,"['ordinary-differential-equations', 'partial-differential-equations']"
57,How to solve first order non-linear ODE $y'=\frac{2x+y+4}{4x-2y}$,How to solve first order non-linear ODE,y'=\frac{2x+y+4}{4x-2y},"I can solve first order non-linear ODE like below: $$y'=\frac{2x+y+4}{4x+2y}$$ This is quite easy by substituting $2x+y$ with $u$. However, I have tried to solve the differential equation as title: $$y'=\frac{2x+y+4}{4x-2y}$$ Note that the denominator has the negative sign $4x-2y%$. It seems that neither substituting nor setting $v=\frac{y}{x}$ is working. Please help me with the detailed answer. Thanks a lot.","I can solve first order non-linear ODE like below: $$y'=\frac{2x+y+4}{4x+2y}$$ This is quite easy by substituting $2x+y$ with $u$. However, I have tried to solve the differential equation as title: $$y'=\frac{2x+y+4}{4x-2y}$$ Note that the denominator has the negative sign $4x-2y%$. It seems that neither substituting nor setting $v=\frac{y}{x}$ is working. Please help me with the detailed answer. Thanks a lot.",,"['ordinary-differential-equations', 'nonlinear-system']"
58,What is the easiest way to solve the following differential equation?,What is the easiest way to solve the following differential equation?,,"I am only just starting online courses on the subject of differential equations, and I passed the following differential equation $$\frac{dy}{dx}= -2x + 3y - 5$$ I'm a bit confused by the use of two variables in the equation. What is the easiest way to solve this? In other words, in a way that a complete differential equation beginner like me can understand it.","I am only just starting online courses on the subject of differential equations, and I passed the following differential equation $$\frac{dy}{dx}= -2x + 3y - 5$$ I'm a bit confused by the use of two variables in the equation. What is the easiest way to solve this? In other words, in a way that a complete differential equation beginner like me can understand it.",,['ordinary-differential-equations']
59,Can we use numerical methods to get a symbolic/analytical solution of a PDE?,Can we use numerical methods to get a symbolic/analytical solution of a PDE?,,"I know the basic differences of numerical and analytical (symbolic) solutions to differential and complicated algebraic equations. Everyone knows that numerical solutions can be obtained even when an analytical solution can't be obtained. However, sometimes analytical solutions even if cannot be found would be preferred in many engineering and scientific applications, as they often give a physical insight to the mathematical description of a system that is not easy to get with a numerical solution. An example where it could be useful would be to see what inputs/parameters are influence the output the most in a model of a multi-input multi-output (MIMO) system. (See When are analytical solutions preferred over numerical solutions in practical problems? ). If indeed it was required, could it be possible to use numerical techniques to help in obtaining an analytical solution? Is it even remotely possible, or is what I am asking meaningless? Keep in mind that my knowledge of math is not advanced by any means, so am I missing something important here?","I know the basic differences of numerical and analytical (symbolic) solutions to differential and complicated algebraic equations. Everyone knows that numerical solutions can be obtained even when an analytical solution can't be obtained. However, sometimes analytical solutions even if cannot be found would be preferred in many engineering and scientific applications, as they often give a physical insight to the mathematical description of a system that is not easy to get with a numerical solution. An example where it could be useful would be to see what inputs/parameters are influence the output the most in a model of a multi-input multi-output (MIMO) system. (See When are analytical solutions preferred over numerical solutions in practical problems? ). If indeed it was required, could it be possible to use numerical techniques to help in obtaining an analytical solution? Is it even remotely possible, or is what I am asking meaningless? Keep in mind that my knowledge of math is not advanced by any means, so am I missing something important here?",,"['ordinary-differential-equations', 'numerical-methods', 'symbolic-computation']"
60,Solving the diffusion equation in a ball with Neumann BC,Solving the diffusion equation in a ball with Neumann BC,,"I was hoping to ask for finding $\lambda_n$ in the following problem, I need to jog my memory as to how to solve these using fourier series. I am only interested in $\lambda_n$ as these terms will represent my decay rates. Suppose we have a ball, radius $r_0$, with the diffusion equation $$\frac{\partial c(r,t)}{\partial t} =  \frac{D}{r^2}  \frac{\partial }{\partial r} \left( r^2 \frac{\partial }{\partial r} c(r,t) \right), \quad r\in[0,r_0],t\geq 0$$ with neumann boundary conditions $$ D\frac{\partial c}{\partial r} = -pc ,\quad \text{on}\ \ r=r_0 $$ with initial conditions $$c(r,0)= \begin{cases} c_0 & r\in[0,R] \\ 0 & r\in(R,r_0] \end{cases}$$ where $D,p>0$. If we let $$c = R(r)T(t)$$ then we find  $$ \frac{1}{D}\frac{T'}{T} =  \frac{\partial^2 R}{\partial r^2} + \frac{2}{r}\frac{\partial^2 R}{\partial r^2} = -\lambda^2 $$ Therefore $T(t)=\exp(-D \lambda^2 t)$. I'm interested in describing the half-life of this system by examining the decay rates of the series solution to it.","I was hoping to ask for finding $\lambda_n$ in the following problem, I need to jog my memory as to how to solve these using fourier series. I am only interested in $\lambda_n$ as these terms will represent my decay rates. Suppose we have a ball, radius $r_0$, with the diffusion equation $$\frac{\partial c(r,t)}{\partial t} =  \frac{D}{r^2}  \frac{\partial }{\partial r} \left( r^2 \frac{\partial }{\partial r} c(r,t) \right), \quad r\in[0,r_0],t\geq 0$$ with neumann boundary conditions $$ D\frac{\partial c}{\partial r} = -pc ,\quad \text{on}\ \ r=r_0 $$ with initial conditions $$c(r,0)= \begin{cases} c_0 & r\in[0,R] \\ 0 & r\in(R,r_0] \end{cases}$$ where $D,p>0$. If we let $$c = R(r)T(t)$$ then we find  $$ \frac{1}{D}\frac{T'}{T} =  \frac{\partial^2 R}{\partial r^2} + \frac{2}{r}\frac{\partial^2 R}{\partial r^2} = -\lambda^2 $$ Therefore $T(t)=\exp(-D \lambda^2 t)$. I'm interested in describing the half-life of this system by examining the decay rates of the series solution to it.",,"['ordinary-differential-equations', 'partial-differential-equations']"
61,About a particular solution of the equation $y''-y=e^x$,About a particular solution of the equation,y''-y=e^x,"ODE:  $y''-y=e^x$. I found  that complemantary solution of the ode is $y_c=c_1e^x+c_2e^{-x}$. No problem. But, I don' t understand why do we suppose that trial solution is $y_p=Axe^x$ instead of $y_p=Ae^x$ ? What is the key point for selection of a particular solution of an ODE?","ODE:  $y''-y=e^x$. I found  that complemantary solution of the ode is $y_c=c_1e^x+c_2e^{-x}$. No problem. But, I don' t understand why do we suppose that trial solution is $y_p=Axe^x$ instead of $y_p=Ae^x$ ? What is the key point for selection of a particular solution of an ODE?",,[]
62,Problem solving a differential equation $y'=y+y^2$,Problem solving a differential equation,y'=y+y^2,I tried solving this differential equation: $$y'=y+y^2$$ I tried the substitution $$z=y^{-1}\\z'=-y^{-2}y'$$ and the differential equation becomes $z'+z=-1$ and now I have as solution $$z=ce^{-x}-1$$ and $$y=\frac{1}{ce^{-x}-1}$$  But the solution is not correct. Does  someone can find my error?,I tried solving this differential equation: $$y'=y+y^2$$ I tried the substitution $$z=y^{-1}\\z'=-y^{-2}y'$$ and the differential equation becomes $z'+z=-1$ and now I have as solution $$z=ce^{-x}-1$$ and $$y=\frac{1}{ce^{-x}-1}$$  But the solution is not correct. Does  someone can find my error?,,['ordinary-differential-equations']
63,Differential Equation of orbit of a planet,Differential Equation of orbit of a planet,,"Every where I found the solution of the differential equation orbit ( they make the differential equation in the 1st place )  by letting r=1/u  , using reciprocal coordinates. I was trying  to do away with this substitution but I couldn't form the equation. I was having a problem in whether (r) is a function of theta which in turn is a function of t or is r directly a function of t. Or is theta the function of r and then r is a function of t or theta directly a function of t . I tried this but produced a mess I need help in forming the equation without using u , just r and in understanding what is function of what both implicitly and explicitly ! Edit: After searching on the net I found out that without taking the substitution , I had actually formed the correct equation but that equation is non linear ! I got that correct non linear equation. To solve the equation we use the substitution r=1/u which will linearize it. So it's just linearization ! What I want is that please someone check  This statement ""whether (r) is a function of theta which in turn is a function of t or is r directly a function of t. Or is theta the function of r and then r is a function of t or theta directly a function of t"" What is correct in the above statement ? I am getting confused because if theta is function of r then the differential equation would be different . Would it be so that we just replace r and theta in final answer and get the same answer back ?","Every where I found the solution of the differential equation orbit ( they make the differential equation in the 1st place )  by letting r=1/u  , using reciprocal coordinates. I was trying  to do away with this substitution but I couldn't form the equation. I was having a problem in whether (r) is a function of theta which in turn is a function of t or is r directly a function of t. Or is theta the function of r and then r is a function of t or theta directly a function of t . I tried this but produced a mess I need help in forming the equation without using u , just r and in understanding what is function of what both implicitly and explicitly ! Edit: After searching on the net I found out that without taking the substitution , I had actually formed the correct equation but that equation is non linear ! I got that correct non linear equation. To solve the equation we use the substitution r=1/u which will linearize it. So it's just linearization ! What I want is that please someone check  This statement ""whether (r) is a function of theta which in turn is a function of t or is r directly a function of t. Or is theta the function of r and then r is a function of t or theta directly a function of t"" What is correct in the above statement ? I am getting confused because if theta is function of r then the differential equation would be different . Would it be so that we just replace r and theta in final answer and get the same answer back ?",,"['ordinary-differential-equations', 'functions', 'polar-coordinates', 'implicit-differentiation']"
64,How do I solve these trivial partial differential equations?,How do I solve these trivial partial differential equations?,,"In every book I have, these are solved like “we guessed this solution”, but I’m sure there is analytic way to solve them: $y\frac{\partial u}{\partial x} - x\frac{\partial u}{\partial y} = 0$ $(x + 2y)\frac{\partial z}{\partial x} - y \frac{\partial z}{\partial y} = 0$ There are more similar equations but I guess I just need to see at least one analytically solved.","In every book I have, these are solved like “we guessed this solution”, but I’m sure there is analytic way to solve them: $y\frac{\partial u}{\partial x} - x\frac{\partial u}{\partial y} = 0$ $(x + 2y)\frac{\partial z}{\partial x} - y \frac{\partial z}{\partial y} = 0$ There are more similar equations but I guess I just need to see at least one analytically solved.",,"['ordinary-differential-equations', 'partial-differential-equations']"
65,What is the coefficient in the differential coefficient?,What is the coefficient in the differential coefficient?,,"A term that apparently not used so much now is that of Differential Coefficient But I can not understand what we mean by the term coefficient in this context. So basically if $y=6x-x^2$ then the differencial coefficient is $dy/dx=6-2x$ but what part is the ""coefficient""?","A term that apparently not used so much now is that of Differential Coefficient But I can not understand what we mean by the term coefficient in this context. So basically if $y=6x-x^2$ then the differencial coefficient is $dy/dx=6-2x$ but what part is the ""coefficient""?",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
66,How to calculate an integral with a Heaviside function: $\int_{0}^{t} {sinh(2(t - \tau))*u_{T}(\tau)d\tau}$?,How to calculate an integral with a Heaviside function: ?,\int_{0}^{t} {sinh(2(t - \tau))*u_{T}(\tau)d\tau},"I am trying to calculate this integral, resulting from convolution method: $\int_{0}^{t} {\sinh(2(t - \tau))*u_{T}(\tau)d\tau}$, where $u_{T}(\tau)$ is a Heaviside function. I have tried integrating it by parts, but it does not work. I was not bale to apply substitution method either. EDIT: Using equality $\sinh{t} = \frac{e^{t} - e^{-t}}{2}$ gives: $\frac{1}{2}\int_{0}^{t}{e^{2(t - \tau)}u_{T}(\tau)d\tau} - \frac{1}{2}\int_{0}^{t}{e^{-2(t - \tau)}u_{T}(\tau)d\tau}$. But the biggest problem is that I do not understand how to integrate Heaviside function, so I do not know how to proceed.","I am trying to calculate this integral, resulting from convolution method: $\int_{0}^{t} {\sinh(2(t - \tau))*u_{T}(\tau)d\tau}$, where $u_{T}(\tau)$ is a Heaviside function. I have tried integrating it by parts, but it does not work. I was not bale to apply substitution method either. EDIT: Using equality $\sinh{t} = \frac{e^{t} - e^{-t}}{2}$ gives: $\frac{1}{2}\int_{0}^{t}{e^{2(t - \tau)}u_{T}(\tau)d\tau} - \frac{1}{2}\int_{0}^{t}{e^{-2(t - \tau)}u_{T}(\tau)d\tau}$. But the biggest problem is that I do not understand how to integrate Heaviside function, so I do not know how to proceed.",,"['integration', 'ordinary-differential-equations']"
67,How to solve $y'' + y = y^{-3}$,How to solve,y'' + y = y^{-3},I am trying to solve a central force problem and came to an equation like this: $$\frac{d^2 y}{dx^2} + y = \frac{1}{y^3}$$ I can't find a decent method to solve it.,I am trying to solve a central force problem and came to an equation like this: $$\frac{d^2 y}{dx^2} + y = \frac{1}{y^3}$$ I can't find a decent method to solve it.,,"['ordinary-differential-equations', 'physics']"
68,How to come from $\frac{d}{dt} f(t) = \frac{2t}{1-t} f(t) + \frac{1}{(1-t)^2}$ to $f(t) =\frac{1-e^{-2t}}{2(1-t)^2}$,How to come from  to,\frac{d}{dt} f(t) = \frac{2t}{1-t} f(t) + \frac{1}{(1-t)^2} f(t) =\frac{1-e^{-2t}}{2(1-t)^2},"From an answer on this question , I got the differential equation $$\frac{d}{dt} f(t) = \frac{2t}{1-t} f(t) + \frac{1}{(1-t)^2}$$ and I was even given the solution $$f(t) = \frac{1-e^{-2t}}{2(1-t)^2}.$$ It seems to be correct, however, I have trouble understanding how the solution was achieved. I tried building a power series $f(t) = \sum_{n=0}^\infty a_n t^n$ out of $f$ and finding the coefficients for it like in this tutorial , but I ended up in a huge ugly sum term $$ a_n = \frac{(2t)^n}{n!(1-t)^n} a_0 + \sum_{k=1}^n \frac{(2t)^{n-k}}{n!/k! (1-t)^{n-k}}. $$ Not fully shure if this is right. Somehow, the taylor series of the $e$-function sticks in there, but I don't think that this will lead to the solution above. So I am stuck here and any help would be appreciated.","From an answer on this question , I got the differential equation $$\frac{d}{dt} f(t) = \frac{2t}{1-t} f(t) + \frac{1}{(1-t)^2}$$ and I was even given the solution $$f(t) = \frac{1-e^{-2t}}{2(1-t)^2}.$$ It seems to be correct, however, I have trouble understanding how the solution was achieved. I tried building a power series $f(t) = \sum_{n=0}^\infty a_n t^n$ out of $f$ and finding the coefficients for it like in this tutorial , but I ended up in a huge ugly sum term $$ a_n = \frac{(2t)^n}{n!(1-t)^n} a_0 + \sum_{k=1}^n \frac{(2t)^{n-k}}{n!/k! (1-t)^{n-k}}. $$ Not fully shure if this is right. Somehow, the taylor series of the $e$-function sticks in there, but I don't think that this will lead to the solution above. So I am stuck here and any help would be appreciated.",,"['ordinary-differential-equations', 'power-series']"
69,Proof of Peano's existence theorem for ODEs,Proof of Peano's existence theorem for ODEs,,"Theorem : Given a continuous function $f:(a,b)\times (c,d)\to \mathbb R$, the problem $$\begin{cases} y'(t)=f(t,y(t))\\ y(t_0)=y_0\end{cases}$$ has a local solution in some neighbourhood of a given point $t_0$. A ""standard"" proof of Peano existence theorem makes use of the Ascoli-Arzelà theorem. I suspect that the following proof, which doesn't, is therefore wrong. But where? Consider, instead of the open rectangle $(a,b)\times (c,d)$, a closed rectangle contained in it (since the thesis is local, this makes no difference), say $[a',b']\times[c',d']$. Consider polynomials $P_n(t,y)$ which converge uniformly to $f$ on the closed rectangle. These exist because of the Weiertrass theorem. Now the problems $$\begin{cases} y_n'(t)=P_n(t,y_n(t))\\ y_n(t_0)=y_0\end{cases}$$ have a unique global solution in $[a,b]$, since $P_n$ is a polynomial, hence globally Lipschitz on the rectangle (recall that globally Lipschitz implies global existence by gluing local solutions). Now we say that the function $P_n$ has an uniform limit $f$, and therefore the functions $t\to y_n'(t)$ have an uniform limit, which we shall denote by $g(t)$. (This is the point which I am less sure of.) Now we have a sequence of functions $y_n$ which take the same value in $t_0$, which are $C^1([a',b;])$ and whose derivatives converge uniformly to a function $g$. Therefore a limit for $y_n$ exists, is differentiable in a neighbourhood of $t_0$ and its derivative is $g$. (This is a theorem of elementary analysis: $f_n(a)\to l$ for a given $a$, $f'_n\to g$ uniformly, then $f_n$ has a limit $f$ which satisfies $f(a)=l$, $f'=g$.) Thank you in advance for helping me finding the (possible) error.","Theorem : Given a continuous function $f:(a,b)\times (c,d)\to \mathbb R$, the problem $$\begin{cases} y'(t)=f(t,y(t))\\ y(t_0)=y_0\end{cases}$$ has a local solution in some neighbourhood of a given point $t_0$. A ""standard"" proof of Peano existence theorem makes use of the Ascoli-Arzelà theorem. I suspect that the following proof, which doesn't, is therefore wrong. But where? Consider, instead of the open rectangle $(a,b)\times (c,d)$, a closed rectangle contained in it (since the thesis is local, this makes no difference), say $[a',b']\times[c',d']$. Consider polynomials $P_n(t,y)$ which converge uniformly to $f$ on the closed rectangle. These exist because of the Weiertrass theorem. Now the problems $$\begin{cases} y_n'(t)=P_n(t,y_n(t))\\ y_n(t_0)=y_0\end{cases}$$ have a unique global solution in $[a,b]$, since $P_n$ is a polynomial, hence globally Lipschitz on the rectangle (recall that globally Lipschitz implies global existence by gluing local solutions). Now we say that the function $P_n$ has an uniform limit $f$, and therefore the functions $t\to y_n'(t)$ have an uniform limit, which we shall denote by $g(t)$. (This is the point which I am less sure of.) Now we have a sequence of functions $y_n$ which take the same value in $t_0$, which are $C^1([a',b;])$ and whose derivatives converge uniformly to a function $g$. Therefore a limit for $y_n$ exists, is differentiable in a neighbourhood of $t_0$ and its derivative is $g$. (This is a theorem of elementary analysis: $f_n(a)\to l$ for a given $a$, $f'_n\to g$ uniformly, then $f_n$ has a limit $f$ which satisfies $f(a)=l$, $f'=g$.) Thank you in advance for helping me finding the (possible) error.",,"['ordinary-differential-equations', 'proof-verification', 'uniform-convergence', 'weierstrass-approximation']"
70,"Explicit solutions for differential system $x'=y^2,y'=x^2$ [closed]",Explicit solutions for differential system  [closed],"x'=y^2,y'=x^2","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Could the system $$ \begin{cases} x'=y^2 \\ y'=x^2  \end{cases} $$ be explicitly solved? I should determine if for every initial condition the system has a solution defined in all $\mathbb{R}$.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Could the system $$ \begin{cases} x'=y^2 \\ y'=x^2  \end{cases} $$ be explicitly solved? I should determine if for every initial condition the system has a solution defined in all $\mathbb{R}$.",,"['real-analysis', 'ordinary-differential-equations']"
71,Prove periodic solution of an ODE,Prove periodic solution of an ODE,,Consider the differential equation $x'=x+cos(t) $ a) find the general solution of this equation b) prove there is a unique periodic solution for this equation c) compute the Poincaré map $p:{t=0} \to {t=2\pi}$ for this equation and use this to verify again that there is a unique periodic solution. I solved part a) and found $x(t)=\frac{1}{2}sin(t)-\frac{1}{2}cos(t)+ce^t$ but I'm unsure how to prove the solution is periodic or how to do the map.,Consider the differential equation $x'=x+cos(t) $ a) find the general solution of this equation b) prove there is a unique periodic solution for this equation c) compute the Poincaré map $p:{t=0} \to {t=2\pi}$ for this equation and use this to verify again that there is a unique periodic solution. I solved part a) and found $x(t)=\frac{1}{2}sin(t)-\frac{1}{2}cos(t)+ce^t$ but I'm unsure how to prove the solution is periodic or how to do the map.,,"['ordinary-differential-equations', 'periodic-functions']"
72,Another way to solve $y=-\frac1{x^2y'}-xy'$,Another way to solve,y=-\frac1{x^2y'}-xy',"It is seen that this is a differential equation which can be written as $y=f(x,y')$, and so I solve the equation as follows: My solution: $1- $ Using the substitution $z=y'$ which leads to $y=-\frac1{x^2z}-xz\ \ \ \ (*)$. $2-$ deriving in terms of $x$ and then solving the new equation which is related to $x$ and $z$ with solutions $$(I)\ z=\pm \frac1{x^{3/2}}\ \text{and }\ (II)\ z=\frac c{x^2}. $$ $3-$ Replacing the obtained answers $(I)$ and $(II)$ to $(*)$ leading to $$ (I)\ y^2=\frac4x\ \ \text{and}\ \ \ (II)\ y=-\frac1c-\frac cx.$$ My question: Is there another technique to solve such a differential equation?","It is seen that this is a differential equation which can be written as $y=f(x,y')$, and so I solve the equation as follows: My solution: $1- $ Using the substitution $z=y'$ which leads to $y=-\frac1{x^2z}-xz\ \ \ \ (*)$. $2-$ deriving in terms of $x$ and then solving the new equation which is related to $x$ and $z$ with solutions $$(I)\ z=\pm \frac1{x^{3/2}}\ \text{and }\ (II)\ z=\frac c{x^2}. $$ $3-$ Replacing the obtained answers $(I)$ and $(II)$ to $(*)$ leading to $$ (I)\ y^2=\frac4x\ \ \text{and}\ \ \ (II)\ y=-\frac1c-\frac cx.$$ My question: Is there another technique to solve such a differential equation?",,['ordinary-differential-equations']
73,"Inner Product Examples, what is the points?","Inner Product Examples, what is the points?",,"Example: For $ -\pi<x<\pi$, $$x =-2  \sum_{n=1}^{\infty} \frac{(-1)^n}{n} \sin(nx)$$ and $$x^3 =-2  \sum_{n=1}^{\infty} \left( \frac{\pi^2}{n}-\frac{6}{n^3} \right)(-1)^n \sin(nx)$$ by using inner products of these two functions, the value of $$\sum_{n=1}^{\infty} \frac{1}{n^4}$$ is equal to $\frac {\pi^4}{90} $ . My question is what is the point in this example that the author gives   the solution without any detail? any users can tips me how this value   is reached?","Example: For $ -\pi<x<\pi$, $$x =-2  \sum_{n=1}^{\infty} \frac{(-1)^n}{n} \sin(nx)$$ and $$x^3 =-2  \sum_{n=1}^{\infty} \left( \frac{\pi^2}{n}-\frac{6}{n^3} \right)(-1)^n \sin(nx)$$ by using inner products of these two functions, the value of $$\sum_{n=1}^{\infty} \frac{1}{n^4}$$ is equal to $\frac {\pi^4}{90} $ . My question is what is the point in this example that the author gives   the solution without any detail? any users can tips me how this value   is reached?",,"['calculus', 'linear-algebra', 'ordinary-differential-equations', 'trigonometry', 'inner-products']"
74,Find the first $4$ Hermite polynomials using a recursion relation,Find the first  Hermite polynomials using a recursion relation,4,"Given the Probabilists' Hermite differential equation: $$U''-xU'+\lambda U=0\tag{1}$$ A book question asks me to: Find the first $4$ polynomial solutions (for $\lambda = 0,1,2,3$), each normalised such that the highest power of $x$ has a coefficient of unity. So I substituted $$U=\sum_{n=0}^{\lambda}C_nx^n\tag{2}$$ into $(1)$ and equated coefficients of $x^n$ to get the recursion relation: $$C_{n+2}=\frac{(n-\lambda)}{(n+1)(n+2)}C_n$$ So I am looking to find the values of the coefficients $C_0,C_1,C_2$ for $$U=\sum_{n=0}^{3}C_nx^n=C_0+C_1x+C_2x^2+x^3$$ where we are given that $\color{blue}{C_3}=1$. Using the recursion relation I find that for $(n=0, \lambda=2)$: $$C_2=-C_0$$  and for $(n=1, \lambda=3)$: $$\color{blue}{C_3}=-\frac13C_1\implies C_1=-3$$ This is as far as I can get to in this question. The book answer simply states: Using the recurrence relations, and setting the coefficient of the   highest power to $1$, $U_0=1, \,U_1=x,\, U_2=x^2-1\,$ and $U_3=x^3-3x$ I know from this page on Wikipedia that the book answer is right, but the answer is not very helpful to me as I have no idea why $U_0=1$. Does this mean that $C_0=1$, if so how did the author deduce this from the recursion relation? I showed that $C_1$ is equal to $-3$. So why is $U_1\ne-3x$? The same misunderstanding follows for $U_2$ and $U_3$. In fact I don't understand why $U_3$ doesn't contain an $x^2$ term and a constant term. Also, where is the $x$ term in $U_2$ and the constant term in $U_1$? I have only just started reading about Hermite polynomials so my understanding is very weak (apologies). Is there any chance someone could explain as simply as possible how to obtain $U_0,U_1,U_2,U_3$? Thank you.","Given the Probabilists' Hermite differential equation: $$U''-xU'+\lambda U=0\tag{1}$$ A book question asks me to: Find the first $4$ polynomial solutions (for $\lambda = 0,1,2,3$), each normalised such that the highest power of $x$ has a coefficient of unity. So I substituted $$U=\sum_{n=0}^{\lambda}C_nx^n\tag{2}$$ into $(1)$ and equated coefficients of $x^n$ to get the recursion relation: $$C_{n+2}=\frac{(n-\lambda)}{(n+1)(n+2)}C_n$$ So I am looking to find the values of the coefficients $C_0,C_1,C_2$ for $$U=\sum_{n=0}^{3}C_nx^n=C_0+C_1x+C_2x^2+x^3$$ where we are given that $\color{blue}{C_3}=1$. Using the recursion relation I find that for $(n=0, \lambda=2)$: $$C_2=-C_0$$  and for $(n=1, \lambda=3)$: $$\color{blue}{C_3}=-\frac13C_1\implies C_1=-3$$ This is as far as I can get to in this question. The book answer simply states: Using the recurrence relations, and setting the coefficient of the   highest power to $1$, $U_0=1, \,U_1=x,\, U_2=x^2-1\,$ and $U_3=x^3-3x$ I know from this page on Wikipedia that the book answer is right, but the answer is not very helpful to me as I have no idea why $U_0=1$. Does this mean that $C_0=1$, if so how did the author deduce this from the recursion relation? I showed that $C_1$ is equal to $-3$. So why is $U_1\ne-3x$? The same misunderstanding follows for $U_2$ and $U_3$. In fact I don't understand why $U_3$ doesn't contain an $x^2$ term and a constant term. Also, where is the $x$ term in $U_2$ and the constant term in $U_1$? I have only just started reading about Hermite polynomials so my understanding is very weak (apologies). Is there any chance someone could explain as simply as possible how to obtain $U_0,U_1,U_2,U_3$? Thank you.",,"['ordinary-differential-equations', 'polynomials', 'intuition']"
75,Method of Annihilators Tedium...,Method of Annihilators Tedium...,,"One of the exam preparation questions for MIT's online Honors Differential Equations course asks for a general solution of  \begin{align} (D^2 - 1)^4(D^3 + 1)^5y = 3e^t \end{align} The fact that the equation is written in terms of differential operators makes me think that I'm to use the method of annihilators; indeed, it's easy enough to see that $D-1$ annihilates $3e^t$.  So we have the homogeneous ODE \begin{align} (D^2 - 1)^4(D^3 + 1)^5(D - 1) & = 0\Rightarrow\\ (D + 1)^9(D-1)^5\left(D - \frac{1}{2} + i\frac{\sqrt{3}}{2}\right)^5\left(D + \frac{1}{2} - i\frac{\sqrt{3}}{2}\right)^5 & = 0 \end{align} which means we have (the absurdly unwieldy, at least to me) basis of solutions \begin{align} 	\lbrace e^{-t},te^{-t},\ldots, t^8e^{-t}, e^t, te^t, \ldots, t^4e^t, e^{\frac{t}{2}}\cos\left(\frac{x\sqrt{3}}{2}\right), te^{\frac{t}{2}}\cos\left(\frac{t\sqrt{3}}{2}\right), \ldots, t^4e^{\frac{t}{2}}\cos\left(\frac{t\sqrt{3}}{2}\right),\\ e^{\frac{t}{2}}\sin\left(\frac{t\sqrt{3}}{2}\right), te^{\frac{t}{2}}\sin\left(\frac{t\sqrt{3}}{2}\right), t^4e^{\frac{t}{2}}\sin\left(\frac{t\sqrt{3}}{2}\right)\rbrace \end{align} My question is this:  do I now really need to go through what appears to be an insane amount of differentiation/algebra to get the particular solution?  Or is there some trick I'm not seeing/fundamental fact I'm missing?","One of the exam preparation questions for MIT's online Honors Differential Equations course asks for a general solution of  \begin{align} (D^2 - 1)^4(D^3 + 1)^5y = 3e^t \end{align} The fact that the equation is written in terms of differential operators makes me think that I'm to use the method of annihilators; indeed, it's easy enough to see that $D-1$ annihilates $3e^t$.  So we have the homogeneous ODE \begin{align} (D^2 - 1)^4(D^3 + 1)^5(D - 1) & = 0\Rightarrow\\ (D + 1)^9(D-1)^5\left(D - \frac{1}{2} + i\frac{\sqrt{3}}{2}\right)^5\left(D + \frac{1}{2} - i\frac{\sqrt{3}}{2}\right)^5 & = 0 \end{align} which means we have (the absurdly unwieldy, at least to me) basis of solutions \begin{align} 	\lbrace e^{-t},te^{-t},\ldots, t^8e^{-t}, e^t, te^t, \ldots, t^4e^t, e^{\frac{t}{2}}\cos\left(\frac{x\sqrt{3}}{2}\right), te^{\frac{t}{2}}\cos\left(\frac{t\sqrt{3}}{2}\right), \ldots, t^4e^{\frac{t}{2}}\cos\left(\frac{t\sqrt{3}}{2}\right),\\ e^{\frac{t}{2}}\sin\left(\frac{t\sqrt{3}}{2}\right), te^{\frac{t}{2}}\sin\left(\frac{t\sqrt{3}}{2}\right), t^4e^{\frac{t}{2}}\sin\left(\frac{t\sqrt{3}}{2}\right)\rbrace \end{align} My question is this:  do I now really need to go through what appears to be an insane amount of differentiation/algebra to get the particular solution?  Or is there some trick I'm not seeing/fundamental fact I'm missing?",,['ordinary-differential-equations']
76,Solving the given differential equation.,Solving the given differential equation.,,"We need to solve : $$ ( \sqrt{x+y} + \sqrt{x-y}) \,dx + ( \sqrt{x-y} - \sqrt{x+y})\,dy=0$$ I tried as follows : $$ \frac{dy}{dx} = \frac{ \sqrt{x+y} + \sqrt{x-y}}{\sqrt{x-y} - \sqrt{x+y}}$$ And hence letting $ y = vx$ yields : $$ v + x \: \frac{dv}{dx} = \frac{\sqrt{1+v} + \sqrt{1-v}}{\sqrt{1-v} - \sqrt{1+v}}$$ Continuing from here yields a very complicated integral , is there a simpler way to proceed ?","We need to solve : $$ ( \sqrt{x+y} + \sqrt{x-y}) \,dx + ( \sqrt{x-y} - \sqrt{x+y})\,dy=0$$ I tried as follows : $$ \frac{dy}{dx} = \frac{ \sqrt{x+y} + \sqrt{x-y}}{\sqrt{x-y} - \sqrt{x+y}}$$ And hence letting $ y = vx$ yields : $$ v + x \: \frac{dv}{dx} = \frac{\sqrt{1+v} + \sqrt{1-v}}{\sqrt{1-v} - \sqrt{1+v}}$$ Continuing from here yields a very complicated integral , is there a simpler way to proceed ?",,['ordinary-differential-equations']
77,Can there be a limit cycle without a fixed point in 3D space?,Can there be a limit cycle without a fixed point in 3D space?,,"I am working with a population dynamics model.  Basically, I have a nonlinear ODE in $R^3$ space, (X,Y,Z), and I know that if I start in the an open region ($0<X<1,0<Y<1,0<Z<1$, basically each species with positive density), then it will stay in that region.  I can prove that (0,0,0) is a stable fixed point, and that (X*,Y*,0) is an unstable fixed point.  I can also prove that there are no fixed points (stable nor unstable) when all species are present. Does the lack of fixed points in the bounded area mean that there can't be a limit cycle?  I know it would in 2 dimensions, but I don't know if that rule generalizes to 3 dimensions.  And I know that I cannot rule out chaos, though that doesn't worry me. Thank you.","I am working with a population dynamics model.  Basically, I have a nonlinear ODE in $R^3$ space, (X,Y,Z), and I know that if I start in the an open region ($0<X<1,0<Y<1,0<Z<1$, basically each species with positive density), then it will stay in that region.  I can prove that (0,0,0) is a stable fixed point, and that (X*,Y*,0) is an unstable fixed point.  I can also prove that there are no fixed points (stable nor unstable) when all species are present. Does the lack of fixed points in the bounded area mean that there can't be a limit cycle?  I know it would in 2 dimensions, but I don't know if that rule generalizes to 3 dimensions.  And I know that I cannot rule out chaos, though that doesn't worry me. Thank you.",,"['ordinary-differential-equations', '3d', 'fixed-point-theorems']"
78,Difference between 'stable node' and 'stable spiral',Difference between 'stable node' and 'stable spiral',,"Suppose I have a pair of 2 non-linear differential equations of the form: $$\begin{matrix} \frac{dy}{dt}=f(x,y)\\  \frac{dx}{dt}=g(x,y) \end{matrix}$$ Equilibrium points are where the trajectory ends up on, when plotted on the $x-y$ plane. What are the qualitative differences between a 'stable node' and a 'stable spiral'? Are they both stable?","Suppose I have a pair of 2 non-linear differential equations of the form: $$\begin{matrix} \frac{dy}{dt}=f(x,y)\\  \frac{dx}{dt}=g(x,y) \end{matrix}$$ Equilibrium points are where the trajectory ends up on, when plotted on the $x-y$ plane. What are the qualitative differences between a 'stable node' and a 'stable spiral'? Are they both stable?",,"['ordinary-differential-equations', 'stability-in-odes']"
79,what is the ODE that gives rise to the Laplace Transform.,what is the ODE that gives rise to the Laplace Transform.,,"It can be shown that the transform pairs can be obtained from a differential equation with some boundary conditions. See, for example, Keener's book chapter 7. For example. The Fourier transform can be obtained by starting with the ODE: \begin{eqnarray}   -u''(x) - \lambda u(x) = 0  \quad , \quad \lim_{x \to \pm \infty} u(x)=0. \end{eqnarray} with $u(x) \in L^2(-\infty, \infty)$ .This ODE has a Green function $G(x, \xi, \lambda)$ with the property that \begin{eqnarray}   \delta(x-\xi) = - \frac{1}{2 \pi \mathrm{i} } \int_{C_{\infty}} G(x,\xi, \lambda) d \lambda  \end{eqnarray} where the contour $C_{\infty}$ should enclose all the spectrum of the operator $L=-d^2/dx^2 - \lambda$. Then the $\delta(x-\xi)$ comes as the function composition of the forward and inverse transforms. What is the differential equation (ODE) and its boundary conditions such that the integral of its Green function having all its spectrum (a Dirac delta) is a composition of a forward and inverse Laplace transforms? Thanks.","It can be shown that the transform pairs can be obtained from a differential equation with some boundary conditions. See, for example, Keener's book chapter 7. For example. The Fourier transform can be obtained by starting with the ODE: \begin{eqnarray}   -u''(x) - \lambda u(x) = 0  \quad , \quad \lim_{x \to \pm \infty} u(x)=0. \end{eqnarray} with $u(x) \in L^2(-\infty, \infty)$ .This ODE has a Green function $G(x, \xi, \lambda)$ with the property that \begin{eqnarray}   \delta(x-\xi) = - \frac{1}{2 \pi \mathrm{i} } \int_{C_{\infty}} G(x,\xi, \lambda) d \lambda  \end{eqnarray} where the contour $C_{\infty}$ should enclose all the spectrum of the operator $L=-d^2/dx^2 - \lambda$. Then the $\delta(x-\xi)$ comes as the function composition of the forward and inverse transforms. What is the differential equation (ODE) and its boundary conditions such that the integral of its Green function having all its spectrum (a Dirac delta) is a composition of a forward and inverse Laplace transforms? Thanks.",,"['complex-analysis', 'functional-analysis', 'ordinary-differential-equations']"
80,Iterative trapezoidal method for differential equations,Iterative trapezoidal method for differential equations,,"I am studying numerical methods for differential equations. I came accros the trapezoidal method in two forms, an explicit and an iterative one. I would like to know the advantages and disadvantages of each of those methods. Furthermore, how can I study the stability for the iterative method? Which stability definition is the better one and why?. I explain both methods below. Consider an initial value problem given by $y' = f(t, y)$ and $y(a) = t_0$, where $f$ is defined in $[a, b]\times[\alpha, \beta]$ and satisfies the Lipschitz property with Lipschitz's constant $L$. Given a natural number $n$ and $h = \frac{b-a}{n}$, we are trying to approximate the unique solution of the problem at $t_i = a + ih \ \forall i = 0, 1 \ldots n$. If $y$ is the solution, we call $y_i = y(t_i)$ and we denote $w_i$ to the approximations obtained by the applied method. One of the methods studied is the explicit trapezoidal method. It follows the following rule: $w_{i+1} = w_i + \frac{h}{2} \left[f(t_i,w_i) + f(t_{i}+h, w_i + h f(t_i,w_i))\right]$ We have proved that it has a local error of order 3 and, hence, a global error of order 2. Then, reading some books I came accros the iterative trapezoidal method, which solves the following implicit equation: $ w_{i}= w_{i-1} + \frac{h}{2} \left[f(t_{i-1}, w_{i-1}) + f(t_i, w_{i})\right] $ The idea is taking an initial approximation $w_i^{(0)}$ and defining the following sequence: $w_{i} ^{(j+1)} = w_{i-1} + \frac{h}{2} \left[f(t_{i-1}, w_{i-1}) + f(t_i, w_{i}^{(j)})\right]$ The limit of that sequence is taken as $w_i$. I have proved that the sequence converges if $hL/2 < 1$ and that if we use $w_i$, then the local error is $O(h^3)$. However, why is this method useful and how can I study its stability?","I am studying numerical methods for differential equations. I came accros the trapezoidal method in two forms, an explicit and an iterative one. I would like to know the advantages and disadvantages of each of those methods. Furthermore, how can I study the stability for the iterative method? Which stability definition is the better one and why?. I explain both methods below. Consider an initial value problem given by $y' = f(t, y)$ and $y(a) = t_0$, where $f$ is defined in $[a, b]\times[\alpha, \beta]$ and satisfies the Lipschitz property with Lipschitz's constant $L$. Given a natural number $n$ and $h = \frac{b-a}{n}$, we are trying to approximate the unique solution of the problem at $t_i = a + ih \ \forall i = 0, 1 \ldots n$. If $y$ is the solution, we call $y_i = y(t_i)$ and we denote $w_i$ to the approximations obtained by the applied method. One of the methods studied is the explicit trapezoidal method. It follows the following rule: $w_{i+1} = w_i + \frac{h}{2} \left[f(t_i,w_i) + f(t_{i}+h, w_i + h f(t_i,w_i))\right]$ We have proved that it has a local error of order 3 and, hence, a global error of order 2. Then, reading some books I came accros the iterative trapezoidal method, which solves the following implicit equation: $ w_{i}= w_{i-1} + \frac{h}{2} \left[f(t_{i-1}, w_{i-1}) + f(t_i, w_{i})\right] $ The idea is taking an initial approximation $w_i^{(0)}$ and defining the following sequence: $w_{i} ^{(j+1)} = w_{i-1} + \frac{h}{2} \left[f(t_{i-1}, w_{i-1}) + f(t_i, w_{i}^{(j)})\right]$ The limit of that sequence is taken as $w_i$. I have proved that the sequence converges if $hL/2 < 1$ and that if we use $w_i$, then the local error is $O(h^3)$. However, why is this method useful and how can I study its stability?",,"['ordinary-differential-equations', 'numerical-methods']"
81,"If $\varphi$ is bounded above, increasing, and concave down, does $x\varphi'(x)$ go to zero? How fast?","If  is bounded above, increasing, and concave down, does  go to zero? How fast?",\varphi x\varphi'(x),"Suppose $\varphi: [0,\infty)\rightarrow [0,1)$ is an increasing differentiable function ($C^\infty$ if you want) with $\varphi \rightarrow 1$ and $\varphi'>0, \varphi''<0$. My question is: Is it true that $$\limsup_{x\rightarrow \infty}\frac{{\color{blue}{x\varphi'(x)}}}{\color{green}{1-\varphi(x)}}\leq 1~~~, \text{or even} < \infty~~?$$ If not, and you see some additional hypotheses on $\varphi$ that make it true, that would also be welcome. In fact, I'm even having trouble proving $x\phi'(x)\rightarrow 0$, even though it seems obvious from the picture.","Suppose $\varphi: [0,\infty)\rightarrow [0,1)$ is an increasing differentiable function ($C^\infty$ if you want) with $\varphi \rightarrow 1$ and $\varphi'>0, \varphi''<0$. My question is: Is it true that $$\limsup_{x\rightarrow \infty}\frac{{\color{blue}{x\varphi'(x)}}}{\color{green}{1-\varphi(x)}}\leq 1~~~, \text{or even} < \infty~~?$$ If not, and you see some additional hypotheses on $\varphi$ that make it true, that would also be welcome. In fact, I'm even having trouble proving $x\phi'(x)\rightarrow 0$, even though it seems obvious from the picture.",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
82,$f '' - (f ')^2 + f=0$; what is known about solutions?,; what is known about solutions?,f '' - (f ')^2 + f=0,"I'm curious about solutions to the equation  $$f''-(f')^2+f=0$$ on the whole real line, as well as solutions which are periodic. Any info about the obvious multivariable generalization would interest me as well. I'm not necessarily looking for explicit solutions, though if there are nontrivial explicit solutions that would be interesting. I'm curious about techniques used to analyze such solutions and the general features of the solutions. I'm not well versed in ODE's, but this equation came up in something I was looking at, so don't be afraid to talk down to me and start with the basics if need be.","I'm curious about solutions to the equation  $$f''-(f')^2+f=0$$ on the whole real line, as well as solutions which are periodic. Any info about the obvious multivariable generalization would interest me as well. I'm not necessarily looking for explicit solutions, though if there are nontrivial explicit solutions that would be interesting. I'm curious about techniques used to analyze such solutions and the general features of the solutions. I'm not well versed in ODE's, but this equation came up in something I was looking at, so don't be afraid to talk down to me and start with the basics if need be.",,"['ordinary-differential-equations', 'partial-differential-equations']"
83,Is $(2x+ y) dx - xdy = 0$ a separable differential equation?,Is  a separable differential equation?,(2x+ y) dx - xdy = 0,"I was given the following differential equation in an assignment the other day: $(2x+ y) dx - xdy = 0$ The problem specified to solve the equation using the method of separation of variables. My problem was setting the integral, I tried multiple manipulations with but nothing seemed to work. So, I have to ask can this equation be solved using separation of variables?","I was given the following differential equation in an assignment the other day: $(2x+ y) dx - xdy = 0$ The problem specified to solve the equation using the method of separation of variables. My problem was setting the integral, I tried multiple manipulations with but nothing seemed to work. So, I have to ask can this equation be solved using separation of variables?",,"['calculus', 'ordinary-differential-equations']"
84,Is there a way to solve an arbitrary ODE of first order?,Is there a way to solve an arbitrary ODE of first order?,,"$$\frac{\text{d}y}{\text{d}x}=f(x,y)$$ I know there are some tricks to solve like separations of variables and change of variables. Especially, change of variables, how can I see what I shall substitute. But they seem to be ""coincident""! Is there any algorithms to apply?","$$\frac{\text{d}y}{\text{d}x}=f(x,y)$$ I know there are some tricks to solve like separations of variables and change of variables. Especially, change of variables, how can I see what I shall substitute. But they seem to be ""coincident""! Is there any algorithms to apply?",,['ordinary-differential-equations']
85,Interesting power series for $y'+y=\frac1x$,Interesting power series for,y'+y=\frac1x,"I had the differential equation $y'+y=\frac1x$, which I solved for $y$ as a power series: $$y=\frac1x\sum_{n=0}^{\infty}\frac{n!}{x^n}$$ Which was a power series at $\infty$, so it doesn't really help me much. So my first question is whether or not $y$ is solvable here (as a power series if needed) where it actually converges. My second question is if it pure coincidence that the summation is very similar to the power series of $e^x$. $$e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!}$$ Is there some reason for their very similar forms, or just my stumbling upon these two unrelated power series?","I had the differential equation $y'+y=\frac1x$, which I solved for $y$ as a power series: $$y=\frac1x\sum_{n=0}^{\infty}\frac{n!}{x^n}$$ Which was a power series at $\infty$, so it doesn't really help me much. So my first question is whether or not $y$ is solvable here (as a power series if needed) where it actually converges. My second question is if it pure coincidence that the summation is very similar to the power series of $e^x$. $$e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!}$$ Is there some reason for their very similar forms, or just my stumbling upon these two unrelated power series?",,"['ordinary-differential-equations', 'power-series']"
86,Awkward terms in Differential equation,Awkward terms in Differential equation,,"Find the solution of the equation: $(x^2+y^2+x)d x-(2x^2+2y^2-y)dy=0$ As a standard approach, I brought $dy/dx$ to the RHS of the equation. Then, I added and subtracted $x^2+y^2-y$ to the numerator of the LHS to find some symmetric relation between the numerator and denominator. Perfecting squares of the numerator or the denominator does not help either. Please advice.","Find the solution of the equation: $(x^2+y^2+x)d x-(2x^2+2y^2-y)dy=0$ As a standard approach, I brought $dy/dx$ to the RHS of the equation. Then, I added and subtracted $x^2+y^2-y$ to the numerator of the LHS to find some symmetric relation between the numerator and denominator. Perfecting squares of the numerator or the denominator does not help either. Please advice.",,"['calculus', 'ordinary-differential-equations']"
87,Strange guess for a particular solution,Strange guess for a particular solution,,"The problem is $x'' + w^2 x = \cos w \, t$, where $x$ is a function of $t$. For my particular solution I guessed $x_p = A \cos w \, t + B \sin w \, t$. Then $x_p' = - w A \sin w \, t + w B \cos w \, t$ and $x_p'' = - w^2 A \cos w \, t - w^2 B \sin w \, t$. But then $ - w^2 A \cos w \, t - w^2 B \sin w \, t + w^2 (A \cos w \, t + B \sin w \, t) = 0 \neq \cos w \, t$. So no particular solution. Later I found out that I was supposed to guess $x_p = A t \sin w \, t$, which I have verified does work. So how was I supposed to know to guess that? Of the guess lists I've seen they only list $A \cos t + B \sin t$ as guesses. None have an extra $t$.","The problem is $x'' + w^2 x = \cos w \, t$, where $x$ is a function of $t$. For my particular solution I guessed $x_p = A \cos w \, t + B \sin w \, t$. Then $x_p' = - w A \sin w \, t + w B \cos w \, t$ and $x_p'' = - w^2 A \cos w \, t - w^2 B \sin w \, t$. But then $ - w^2 A \cos w \, t - w^2 B \sin w \, t + w^2 (A \cos w \, t + B \sin w \, t) = 0 \neq \cos w \, t$. So no particular solution. Later I found out that I was supposed to guess $x_p = A t \sin w \, t$, which I have verified does work. So how was I supposed to know to guess that? Of the guess lists I've seen they only list $A \cos t + B \sin t$ as guesses. None have an extra $t$.",,['ordinary-differential-equations']
88,Solution of the first order differential equation $y'=\frac{x(x^2+y^2)^2}{4y}$.,Solution of the first order differential equation .,y'=\frac{x(x^2+y^2)^2}{4y},I m stuck in finding the solution of the following differential equation $$y'=\frac{x(x^2+y^2)^2}{4y}.$$ Please give me some hint.,I m stuck in finding the solution of the following differential equation $$y'=\frac{x(x^2+y^2)^2}{4y}.$$ Please give me some hint.,,['ordinary-differential-equations']
89,differential equation of family of circles passing through origin,differential equation of family of circles passing through origin,,"How do I find the DE of all circles passing through origin? I tried something like this The family of circles passing through the origin is given by      $$ 	 (x- r \cos \theta )^2 + (y- r\sin \theta)^2 = r^2  	 $$Differentiating once, we get      $$  	  2 ( (x- r \cos \theta) + (y - r \sin \theta) y') = 0 	 $$      Differentiating again, we get      $$ 	  1 + y'^2  + (y - r \sin \theta) y'' = 0  	 $$ How to get rid of $r$ and $\theta$ algebraically? Is there any other approach?","How do I find the DE of all circles passing through origin? I tried something like this The family of circles passing through the origin is given by      $$ 	 (x- r \cos \theta )^2 + (y- r\sin \theta)^2 = r^2  	 $$Differentiating once, we get      $$  	  2 ( (x- r \cos \theta) + (y - r \sin \theta) y') = 0 	 $$      Differentiating again, we get      $$ 	  1 + y'^2  + (y - r \sin \theta) y'' = 0  	 $$ How to get rid of $r$ and $\theta$ algebraically? Is there any other approach?",,['ordinary-differential-equations']
90,Solution of $y'' - (k+\pi ^2)y=0$,Solution of,y'' - (k+\pi ^2)y=0,Give this D-E: $$y'' - (k+\pi ^2)y=0$$ $$k>0$$ $$y(0)=0$$ $$y(1)=1$$ How can I get to this solution:  $$y=\frac{ \sinh \sqrt{k+\pi ^2}x}{\sinh \sqrt{k+\pi ^2}} $$ What I did: ***$$r^2-(k+\pi ^2)=0 $$ $$r^2=(k+\pi ^2) $$ $$r=+-\sqrt{(k+\pi ^2}) $$ The solution then is: $$y=Ae^{\sqrt{(k+\pi ^2})x} +Be^{-\sqrt{(k+\pi ^2})x} $$ Managed also to get : $$A=-B$$ $$B=\frac{1}{-e^{\sqrt{(k+\pi ^2}}+e^{-\sqrt{(k+\pi ^2}}}$$ And what next?***,Give this D-E: $$y'' - (k+\pi ^2)y=0$$ $$k>0$$ $$y(0)=0$$ $$y(1)=1$$ How can I get to this solution:  $$y=\frac{ \sinh \sqrt{k+\pi ^2}x}{\sinh \sqrt{k+\pi ^2}} $$ What I did: ***$$r^2-(k+\pi ^2)=0 $$ $$r^2=(k+\pi ^2) $$ $$r=+-\sqrt{(k+\pi ^2}) $$ The solution then is: $$y=Ae^{\sqrt{(k+\pi ^2})x} +Be^{-\sqrt{(k+\pi ^2})x} $$ Managed also to get : $$A=-B$$ $$B=\frac{1}{-e^{\sqrt{(k+\pi ^2}}+e^{-\sqrt{(k+\pi ^2}}}$$ And what next?***,,['ordinary-differential-equations']
91,Differential equation with reciprocal and constant,Differential equation with reciprocal and constant,,"I have the following differential equation with a reciprocal and a constant: $$ dx/dt =-\frac{a}{x}-b, $$ where $a$ is a positive constant and $b$ a nonnegative constant. I can solve this for $b=0$. Then (if this is allowed...) $$x dx = -adt.$$ Integrating both sides and summing up the constants of integration gives $$\frac{1}{2} x^2 = -at+C,$$ hence $$x = \sqrt{2(-at+C)}. $$ Perhaps this is a rather elementary question, but how can this be solved for $b>0$? Since $1/x$ is not a well-behaved function, does this differential equation have an analytical solution? Thanks in advance.","I have the following differential equation with a reciprocal and a constant: $$ dx/dt =-\frac{a}{x}-b, $$ where $a$ is a positive constant and $b$ a nonnegative constant. I can solve this for $b=0$. Then (if this is allowed...) $$x dx = -adt.$$ Integrating both sides and summing up the constants of integration gives $$\frac{1}{2} x^2 = -at+C,$$ hence $$x = \sqrt{2(-at+C)}. $$ Perhaps this is a rather elementary question, but how can this be solved for $b>0$? Since $1/x$ is not a well-behaved function, does this differential equation have an analytical solution? Thanks in advance.",,['ordinary-differential-equations']
92,Finding the general solution of this differential equation: $y'''' - 2y''' + 3y'' - 4y' + 2y = 0$?,Finding the general solution of this differential equation: ?,y'''' - 2y''' + 3y'' - 4y' + 2y = 0,I am trying to find the general solution of the following differential equation: $$y'''' - 2y''' + 3y'' - 4y' + 2y = 0?$$ I don't know how to factor the characteristic equation of this. What should I do?,I am trying to find the general solution of the following differential equation: $$y'''' - 2y''' + 3y'' - 4y' + 2y = 0?$$ I don't know how to factor the characteristic equation of this. What should I do?,,"['calculus', 'ordinary-differential-equations']"
93,Solve $y-x\frac{dy}{dx}=y^2\cos x(1-\sin x)$ [closed],Solve  [closed],y-x\frac{dy}{dx}=y^2\cos x(1-\sin x),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Solve the given differential equation. $y-x\frac{dy}{dx}=y^2\cos x(1-\sin x)$ Can anyone give some hint as how to initiate the solution?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Solve the given differential equation. $y-x\frac{dy}{dx}=y^2\cos x(1-\sin x)$ Can anyone give some hint as how to initiate the solution?",,"['calculus', 'ordinary-differential-equations']"
94,Solving for the function $x(t)$ in the differential equation $\frac{dx}{x}=\frac{3}{2}C\left[\frac{\sqrt{A}}{x}+\sqrt{B}\right]dt$,Solving for the function  in the differential equation,x(t) \frac{dx}{x}=\frac{3}{2}C\left[\frac{\sqrt{A}}{x}+\sqrt{B}\right]dt,I'm a little stumped on something I'm working on. I have an expression in this following form: $$\frac{dx}{x}=\frac{3}{2}C\left[\frac{\sqrt{A}}{x}+\sqrt{B}\right]dt$$ I was essentially wondering how I go about solving this for $x$ explicitly. I'm perhaps thinking I might need to make a substitution which may involve a $\sinh$ function? Help!,I'm a little stumped on something I'm working on. I have an expression in this following form: $$\frac{dx}{x}=\frac{3}{2}C\left[\frac{\sqrt{A}}{x}+\sqrt{B}\right]dt$$ I was essentially wondering how I go about solving this for $x$ explicitly. I'm perhaps thinking I might need to make a substitution which may involve a $\sinh$ function? Help!,,"['calculus', 'integration', 'ordinary-differential-equations']"
95,Approximation of $2$nd Derivative Up to $O(h^4)$,Approximation of nd Derivative Up to,2 O(h^4),"Investigate if it is possible to obtain 4th order accuracy using 5 points for a 2nd derivative approximation, i.e. is it possible to determine a, b, c, d, e in $$y''(0) = \frac{ay(2h)+by(h)+cy(0)+dy(-h)+ey(-2h)}{h^2}+O(h^4)$$ The solution starts by saying that due to symmetry properties $a=e, b=d$ but i don't understand why.","Investigate if it is possible to obtain 4th order accuracy using 5 points for a 2nd derivative approximation, i.e. is it possible to determine a, b, c, d, e in $$y''(0) = \frac{ay(2h)+by(h)+cy(0)+dy(-h)+ey(-2h)}{h^2}+O(h^4)$$ The solution starts by saying that due to symmetry properties $a=e, b=d$ but i don't understand why.",,"['ordinary-differential-equations', 'derivatives', 'approximation']"
96,Stability of sampled-data systems using Lyapunov functions,Stability of sampled-data systems using Lyapunov functions,,"For continuous systems, Lyapunov functions provide a general technique to establish stability. For example, the simple system $x' = -x$, a Lyapunov function is $V(x) = \frac{1}{2}x^2$. It is easy to see that $V(0) = 0$ $V(x) > 0$ for $x \neq 0$ $V(x)' = \frac{dV(x)}{dx}\frac{dx}{dt} = -x^2 < 0$ for $x\neq 0$ Is there an analogous technique for sampled-data systems? For example, suppose I take the simple continuous system $x' = -x$ and naively turn it into a sampled-data system by introducing a zero-order hold. In other words, every $\delta$ time units, $x'$ is set to $-x$ and held at that value for the next $\delta$ time units. More precisely, $$\forall k \in N, ~~~~ \forall t \in [k*\delta, (k+1)*\delta], ~~~~ x'(t) = -x(k*\delta)$$ Looking at the solutions to this system shows that it is asymptotically stable for $\delta < 2$. However, looking at the solutions of a system is only possible for simple systems. Is there instead a way of establishing asymptotic stability of this system using some sort of Lyapunov function?","For continuous systems, Lyapunov functions provide a general technique to establish stability. For example, the simple system $x' = -x$, a Lyapunov function is $V(x) = \frac{1}{2}x^2$. It is easy to see that $V(0) = 0$ $V(x) > 0$ for $x \neq 0$ $V(x)' = \frac{dV(x)}{dx}\frac{dx}{dt} = -x^2 < 0$ for $x\neq 0$ Is there an analogous technique for sampled-data systems? For example, suppose I take the simple continuous system $x' = -x$ and naively turn it into a sampled-data system by introducing a zero-order hold. In other words, every $\delta$ time units, $x'$ is set to $-x$ and held at that value for the next $\delta$ time units. More precisely, $$\forall k \in N, ~~~~ \forall t \in [k*\delta, (k+1)*\delta], ~~~~ x'(t) = -x(k*\delta)$$ Looking at the solutions to this system shows that it is asymptotically stable for $\delta < 2$. However, looking at the solutions of a system is only possible for simple systems. Is there instead a way of establishing asymptotic stability of this system using some sort of Lyapunov function?",,"['ordinary-differential-equations', 'control-theory', 'linear-control']"
97,Solutions of this system of differential equations stay in first quadrant,Solutions of this system of differential equations stay in first quadrant,,"How do I show that all solutions $x(t)$ and $y(t)$ of $$\frac{dx}{dt}=x^2+y\sin x,$$ $$\frac{dy}{dt}=-1+xy+\cos y$$ which start in the first quadrant remain there for all time? I thought that looking at $$\frac{dy}{dx}=\frac{-1+xy+\cos y}{x^2+y\sin x}$$ could help, but I don't know what do with this.","How do I show that all solutions $x(t)$ and $y(t)$ of $$\frac{dx}{dt}=x^2+y\sin x,$$ $$\frac{dy}{dt}=-1+xy+\cos y$$ which start in the first quadrant remain there for all time? I thought that looking at $$\frac{dy}{dx}=\frac{-1+xy+\cos y}{x^2+y\sin x}$$ could help, but I don't know what do with this.",,[]
98,Generalization of integrating factor?,Generalization of integrating factor?,,"For example, if we have $y'+p(x)y=q(x)$, we can obtain $\mu(x)=e^{\int p(x)dx}$ as integrating factor. My question is: exist a generalization of this? ie, if I have $y^{(n)}+p_{n-1}(x)y^{(n-1)}+...+p_0(x)y=q(x)$, exist a ""integrating factor"" for this?","For example, if we have $y'+p(x)y=q(x)$, we can obtain $\mu(x)=e^{\int p(x)dx}$ as integrating factor. My question is: exist a generalization of this? ie, if I have $y^{(n)}+p_{n-1}(x)y^{(n-1)}+...+p_0(x)y=q(x)$, exist a ""integrating factor"" for this?",,['ordinary-differential-equations']
99,How to solve the ODE: $ \frac{dy}{dx}=\frac{\sqrt{1-x^2}}{x} $,How to solve the ODE:, \frac{dy}{dx}=\frac{\sqrt{1-x^2}}{x} ,"I am learning differential equations and have been stuck on : $$ \dfrac{dy}{dx}=\frac{\sqrt{1-x^2}}{x} $$ I tried using $\cos{t}, \sin{t}$ but failed.. how to solve this? please help.","I am learning differential equations and have been stuck on : $$ \dfrac{dy}{dx}=\frac{\sqrt{1-x^2}}{x} $$ I tried using $\cos{t}, \sin{t}$ but failed.. how to solve this? please help.",,"['ordinary-differential-equations', 'substitution']"
