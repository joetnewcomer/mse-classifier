,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Show $\left| \frac{d^m}{dx^m} e^{-|x|^k} \right| \le C^{m+1} |x|^{2(k-1)}e^{-|x|^k}, \forall |x|>a$",Show,"\left| \frac{d^m}{dx^m} e^{-|x|^k} \right| \le C^{m+1} |x|^{2(k-1)}e^{-|x|^k}, \forall |x|>a","How to show the following bound: Let $k \in (0,\infty)$ and let $  |x|>a>0$, then there exists $C>0$ sucht that for all $m\ge 1$ \begin{align} \left| \frac{d^m}{dx^m}  e^{-|x|^k} \right| \le  C^{m+1} |x|^{2(k-1)}e^{-|x|^k}, \forall |x|>a \end{align} This question was raised here where it was also pointed out that the bound is almost there but not correct.  The proof by induction was also suggested, but I am not sure how to do that.","How to show the following bound: Let $k \in (0,\infty)$ and let $  |x|>a>0$, then there exists $C>0$ sucht that for all $m\ge 1$ \begin{align} \left| \frac{d^m}{dx^m}  e^{-|x|^k} \right| \le  C^{m+1} |x|^{2(k-1)}e^{-|x|^k}, \forall |x|>a \end{align} This question was raised here where it was also pointed out that the bound is almost there but not correct.  The proof by induction was also suggested, but I am not sure how to do that.",,"['real-analysis', 'derivatives', 'analyticity']"
1,Find the constant $C$ for two integration results of the same expression,Find the constant  for two integration results of the same expression,C,"Integrating the following expression: $$ \frac{2 M x}{R^3 \left(3 \sqrt{1-\frac{2 M}{R}} \sqrt{1-\frac{2 M x^2}{R^3}}-1\right)+2 M x^2} \quad \quad  x \geq 0, \quad  R>0, \quad M >0  \tag 1$$ with respect to $x$ using Mathematica yields $$ -\frac{1}{2} \left(\log  \left(\frac{\sqrt{R^3-2 M x^2}}{3 R \sqrt{R-2 M}}+1\right)\right)+\frac{1}{2} \log  \left(1-\frac{\sqrt{R^3-2 M x^2}}{3 R \sqrt{R-2 M}}\right)+\frac{1}{2} \log  \left(-9 M R^2+M x^2+4 R^3\right) \tag 2 $$ but the result is also known to be from other sources as $$ \log  \left(3 \sqrt{1-\frac{2 M}{R}}-\sqrt{1-\frac{2 M x^2}{R^3}}\right)   \tag 3$$ I don't see the resemblance between them.  Both $(2)$ and $(3)$ have the same derivative with respect to $x$, but I cannot find the constant $C$ such that $(3) + C = (2)$. If I try to calculate $C$ I end up having $x$ in the expression which indicates it is not a constant.  I need to find $C$.","Integrating the following expression: $$ \frac{2 M x}{R^3 \left(3 \sqrt{1-\frac{2 M}{R}} \sqrt{1-\frac{2 M x^2}{R^3}}-1\right)+2 M x^2} \quad \quad  x \geq 0, \quad  R>0, \quad M >0  \tag 1$$ with respect to $x$ using Mathematica yields $$ -\frac{1}{2} \left(\log  \left(\frac{\sqrt{R^3-2 M x^2}}{3 R \sqrt{R-2 M}}+1\right)\right)+\frac{1}{2} \log  \left(1-\frac{\sqrt{R^3-2 M x^2}}{3 R \sqrt{R-2 M}}\right)+\frac{1}{2} \log  \left(-9 M R^2+M x^2+4 R^3\right) \tag 2 $$ but the result is also known to be from other sources as $$ \log  \left(3 \sqrt{1-\frac{2 M}{R}}-\sqrt{1-\frac{2 M x^2}{R^3}}\right)   \tag 3$$ I don't see the resemblance between them.  Both $(2)$ and $(3)$ have the same derivative with respect to $x$, but I cannot find the constant $C$ such that $(3) + C = (2)$. If I try to calculate $C$ I end up having $x$ in the expression which indicates it is not a constant.  I need to find $C$.",,"['integration', 'derivatives']"
2,Can you have a vertical tangent where a function is undefined?,Can you have a vertical tangent where a function is undefined?,,"Can you have a vertical tangent where a function is undefined? For example, the function $y = 1/x$ is undefined at $x=0$, but that's where the denominator of its derivative is equal to $0$.","Can you have a vertical tangent where a function is undefined? For example, the function $y = 1/x$ is undefined at $x=0$, but that's where the denominator of its derivative is equal to $0$.",,[]
3,"Proof verification - $f'(x)$ is increasing on $(0,1)$ $\implies$ $\frac{f(x)}{x}$ is increasing on $(0,1)$ under specified conditions",Proof verification -  is increasing on    is increasing on  under specified conditions,"f'(x) (0,1) \implies \frac{f(x)}{x} (0,1)","I'm stating the problem statement and my solution to it. I'd appreciate if someone checks if there's any gap in my arguments and whether the proof could be made shorter. Any other suggestions (regarding proof-writing style or other details) are also welcome. Thank you. The Problem : Let $f$ be continuous on $[0,1],$ differentiable on $(0,1)$ with $f(0)=0$. To show that if $f'$ is increasing on $(0,1),$ then $\frac{f(x)}{x}$ is increasing in $(0,1)$. My Solution : Let $x,y$ be arbitrary such that $0<x<y<1$. Since $f$ is continuous in $[0,1]$ and is differentiable in $(0,1),$ it is continuous in $[a,b]$ and differentiable in $(a,b)$ for any $a,b$ such that $0<a<b<1,$ enabling us to apply MVT on $f|_{(a,b)}$ Applying MVT on $f|_{[0,x]}$ we see that $\exists c \in (0,x)$ such that $f(x)-f(0)=f'(c)(x-0)$. Since $f(0)=0,$ we have $\frac{f(x)}{x}=f'(c)$ Applying MVT on $f|_{[x,y]}$ we see that $\exists d \in (x,y)$ such that $f(y)-f(x)=f'(d)(y-x)$ Note that $0<c<x<d<y<1$. Now, $\frac{f(y)}{y}-\frac{f(x)}{x}=\frac{f(x)+f'(d)(y-x)}{y}-\frac{f(x)}{x}=f(x)\Big(\frac{1}{y}-\frac{1}{x}\Big)+f'(d)\Big(\frac{y-x}{y}\Big)$ $=f(x)\Big(\frac{x-y}{xy}\Big)+f'(d)\Big(\frac{y-x}{y}\Big)=\Big(\frac{y-x}{y}\Big)\Big(f'(d)-\frac{f(x)}{x}\Big)=\Big(\frac{y-x}{y}\Big)\Big(f'(d)-f'(c)\Big)$ Since $f'$ is increasing in $(0,1),$ and $0<c<d<1,$ we have $f'(c)<f'(d)$ Thus $\frac{f(y)}{y}>\frac{f(x)}{x}$. Since $x,y$ are arbitrarily chosen, we have $\frac{f(x)}{x}<\frac{f(y)}{y},$ for all $x,y$ such that $0<x<y<1$. Hence $\frac{f(x)}{x}$ is increasing in $(0,1)$. $\blacksquare$","I'm stating the problem statement and my solution to it. I'd appreciate if someone checks if there's any gap in my arguments and whether the proof could be made shorter. Any other suggestions (regarding proof-writing style or other details) are also welcome. Thank you. The Problem : Let $f$ be continuous on $[0,1],$ differentiable on $(0,1)$ with $f(0)=0$. To show that if $f'$ is increasing on $(0,1),$ then $\frac{f(x)}{x}$ is increasing in $(0,1)$. My Solution : Let $x,y$ be arbitrary such that $0<x<y<1$. Since $f$ is continuous in $[0,1]$ and is differentiable in $(0,1),$ it is continuous in $[a,b]$ and differentiable in $(a,b)$ for any $a,b$ such that $0<a<b<1,$ enabling us to apply MVT on $f|_{(a,b)}$ Applying MVT on $f|_{[0,x]}$ we see that $\exists c \in (0,x)$ such that $f(x)-f(0)=f'(c)(x-0)$. Since $f(0)=0,$ we have $\frac{f(x)}{x}=f'(c)$ Applying MVT on $f|_{[x,y]}$ we see that $\exists d \in (x,y)$ such that $f(y)-f(x)=f'(d)(y-x)$ Note that $0<c<x<d<y<1$. Now, $\frac{f(y)}{y}-\frac{f(x)}{x}=\frac{f(x)+f'(d)(y-x)}{y}-\frac{f(x)}{x}=f(x)\Big(\frac{1}{y}-\frac{1}{x}\Big)+f'(d)\Big(\frac{y-x}{y}\Big)$ $=f(x)\Big(\frac{x-y}{xy}\Big)+f'(d)\Big(\frac{y-x}{y}\Big)=\Big(\frac{y-x}{y}\Big)\Big(f'(d)-\frac{f(x)}{x}\Big)=\Big(\frac{y-x}{y}\Big)\Big(f'(d)-f'(c)\Big)$ Since $f'$ is increasing in $(0,1),$ and $0<c<d<1,$ we have $f'(c)<f'(d)$ Thus $\frac{f(y)}{y}>\frac{f(x)}{x}$. Since $x,y$ are arbitrarily chosen, we have $\frac{f(x)}{x}<\frac{f(y)}{y},$ for all $x,y$ such that $0<x<y<1$. Hence $\frac{f(x)}{x}$ is increasing in $(0,1)$. $\blacksquare$",,['real-analysis']
4,Why do derivatives behave like fractions when they are not fractions but an operator as a whole?,Why do derivatives behave like fractions when they are not fractions but an operator as a whole?,,The operator  $\frac{d}{dx}$ sometimes behaves like a fraction.For eg. when we use chain rule to write $\frac {d}{dy}=\frac {d}{dx}\frac {dx}{dy}$ it apparently seems the $dx$ cancel out to give the previous thing.Also when differentiating parametric functions we use $\frac {dy}{dx}=\frac {dy/dt}{dx/dt}$ it seems that the $dt$ cancel out as if they were fractions. I referred a few books but could not find any explanations (rather how the thing works).I will be great if someone explains this. Thanks for any help!!,The operator  $\frac{d}{dx}$ sometimes behaves like a fraction.For eg. when we use chain rule to write $\frac {d}{dy}=\frac {d}{dx}\frac {dx}{dy}$ it apparently seems the $dx$ cancel out to give the previous thing.Also when differentiating parametric functions we use $\frac {dy}{dx}=\frac {dy/dt}{dx/dt}$ it seems that the $dt$ cancel out as if they were fractions. I referred a few books but could not find any explanations (rather how the thing works).I will be great if someone explains this. Thanks for any help!!,,['derivatives']
5,Logarithmic differentiation and negative valued functions,Logarithmic differentiation and negative valued functions,,"I have seen the following proof for the power rule: $$ \begin{align*} y &= x^n\\ \ln(y) &= n\ln(x)\\ \frac{d}{dx}\ln(y) &= \frac{d}{dx}(n\ln(x))\\ \frac{y'}{y} &= \frac{n}{x}\\ y' &= nx^{n-1} \end{align*} $$ However, what I don't understand is how is this proof valid if $y$ may be negative and then $\ln(y)$ would not be defined. Even if $y'$ is indeed $nx^{n-1}$, isn't this proof a wrong way to achieve the right answer?","I have seen the following proof for the power rule: $$ \begin{align*} y &= x^n\\ \ln(y) &= n\ln(x)\\ \frac{d}{dx}\ln(y) &= \frac{d}{dx}(n\ln(x))\\ \frac{y'}{y} &= \frac{n}{x}\\ y' &= nx^{n-1} \end{align*} $$ However, what I don't understand is how is this proof valid if $y$ may be negative and then $\ln(y)$ would not be defined. Even if $y'$ is indeed $nx^{n-1}$, isn't this proof a wrong way to achieve the right answer?",,"['real-analysis', 'derivatives', 'proof-explanation']"
6,Directional derivative of a vector field in the direction of another vector field,Directional derivative of a vector field in the direction of another vector field,,"So, I was reading some introductory things about differential geometry, and just discovered the big gap I have in analysis. For instance, I do understand what is means to ask for the directional derivative of a smooth real valued function $f: \mathbb{R}^n \rightarrow \mathbb{R}$, in $u \in \mathbb{R}^n$ direction, and how does this generalizes to the case of vector fields of the form $F: \mathbb{R}^n \rightarrow \mathbb{R}^n$, where we have $\nabla_u F(p) = \lim_{t \to 0} \frac{F(p+tu)-F(p)}{t},$ as the directional derivative. Now, I'm looking for the directional derivative of a vector field say $X$, in the direction of another vector field $Y$, but I can't find any reference or online handout with this sort of things. I presume that the definition should be look like  $$\nabla_Y X(p) = \lim_{t \to 0} \frac{X(p+tY)-X(p)}{t},$$ but I need to see it written anyway, so any comments, references or online notes concerning those things are welcome.","So, I was reading some introductory things about differential geometry, and just discovered the big gap I have in analysis. For instance, I do understand what is means to ask for the directional derivative of a smooth real valued function $f: \mathbb{R}^n \rightarrow \mathbb{R}$, in $u \in \mathbb{R}^n$ direction, and how does this generalizes to the case of vector fields of the form $F: \mathbb{R}^n \rightarrow \mathbb{R}^n$, where we have $\nabla_u F(p) = \lim_{t \to 0} \frac{F(p+tu)-F(p)}{t},$ as the directional derivative. Now, I'm looking for the directional derivative of a vector field say $X$, in the direction of another vector field $Y$, but I can't find any reference or online handout with this sort of things. I presume that the definition should be look like  $$\nabla_Y X(p) = \lim_{t \to 0} \frac{X(p+tY)-X(p)}{t},$$ but I need to see it written anyway, so any comments, references or online notes concerning those things are welcome.",,"['calculus', 'derivatives', 'reference-request', 'vector-analysis']"
7,Hot air balloon Related Rates,Hot air balloon Related Rates,,"A man in a hot air balloon is ascending at a rate of $10\frac{ft}{sec}$. How fast is the distance from the balloon to the horizon ( the distance the man can see ) increasing when the balloon is $1,000$ feet high. (Hint: assume the earth is a ball of radius $4000$ miles). My Attempt: Please tell me where it is wrong. First we construct a triangle such that, $x=$ radius of the earth in feet $=4000*5280$ $y=$ height of ballon in feet $=1000$ $z=$ distance the man can see in feet $=\sqrt{1000^2+(4000\times5280)^2}$ So, $$\frac{dx}{dt}=0, \frac{dy}{dt}=10\frac{ft}{sec}$$ We need to solve for $\frac{dz}{dt}$. $$x^2+y^2=z^2 \implies 2x\frac{dx}{dt}+2y\frac{dy}{dt}=2z\frac{dz}{dt} \implies \frac{dz}{dt}=\frac{x\frac{dx}{dt}+y\frac{dy}{dt}}{z}$$ $$\frac{dz}{dt}=\frac{(4000\times5280)(0)+(1000)(10)}{\sqrt{1000^2+(4000\times5280)^2}} $$ Right here is where everything feels wrong. Is $\frac{dx}{dt}=0$? Can anyone please help me close this guy off, and did I let $x$ equal the correct value?","A man in a hot air balloon is ascending at a rate of $10\frac{ft}{sec}$. How fast is the distance from the balloon to the horizon ( the distance the man can see ) increasing when the balloon is $1,000$ feet high. (Hint: assume the earth is a ball of radius $4000$ miles). My Attempt: Please tell me where it is wrong. First we construct a triangle such that, $x=$ radius of the earth in feet $=4000*5280$ $y=$ height of ballon in feet $=1000$ $z=$ distance the man can see in feet $=\sqrt{1000^2+(4000\times5280)^2}$ So, $$\frac{dx}{dt}=0, \frac{dy}{dt}=10\frac{ft}{sec}$$ We need to solve for $\frac{dz}{dt}$. $$x^2+y^2=z^2 \implies 2x\frac{dx}{dt}+2y\frac{dy}{dt}=2z\frac{dz}{dt} \implies \frac{dz}{dt}=\frac{x\frac{dx}{dt}+y\frac{dy}{dt}}{z}$$ $$\frac{dz}{dt}=\frac{(4000\times5280)(0)+(1000)(10)}{\sqrt{1000^2+(4000\times5280)^2}} $$ Right here is where everything feels wrong. Is $\frac{dx}{dt}=0$? Can anyone please help me close this guy off, and did I let $x$ equal the correct value?",,"['calculus', 'trigonometry', 'derivatives']"
8,"If $f$ is differentiable everywhere and $|f(y)-f(x)| \leq |x-y|^n$ for all $n >1$, then $f^{\prime}(x)= 0 $ for all $x$.","If  is differentiable everywhere and  for all , then  for all .",f |f(y)-f(x)| \leq |x-y|^n n >1 f^{\prime}(x)= 0  x,"Suppose $f:\mathbb{R} \rightarrow \mathbb{R}$ is a differentiable function and satisfies $|f(x) - f(y)| \leq |x-y|^n$ for all $n > 1.$ Show that $f^{\prime}(x)=0$ for all $x\in \mathbb{R}.$ My first attempt: Choose $x < y$ such that $y - x < 1$. By the Mean Value Theorem on $[x,y]$, we have $\frac{f(y) - f(x)}{y-x} = f^{\prime}(c)$ for some $c \in (x,y).$ Note that we have $| f^{\prime}(c) | = \frac{|f(y) - f(x)|}{y-x} \leq |x-y|^{n-1}$. Since it holds for any $n > 1$, we have $\displaystyle\lim_{n \rightarrow \infty}|f^{\prime}(c)| \leq \displaystyle\lim_{n \rightarrow \infty}|x-y|^{n-1}.$ Since $y - x < 1,$ we have $\displaystyle\lim_{ \rightarrow \infty}|x-y|^{n-1} = 0$. Hence, $|f^{\prime}(c)| = \displaystyle\lim_{n \rightarrow \infty}|f^{\prime}(c)| = 0 .$ Therefore, $f^{\prime}(c) = 0$. Since it holds for any interval $[x,y]$ that shrinks to a point in it, we can have $f^{\prime}(x)=0$ for all $x \in \mathbb{R}.$ My second attempt: Since $f$ is differentiable everywhere, we have for any $x \in \mathbb{R}, f^{\prime}(x) = \displaystyle\lim_{y \rightarrow x }\frac{f(x)-f(y)}{x-y}.$ Therefore, $|f^{\prime}(x)| = \displaystyle\lim_{y \rightarrow x }\frac{|f(x)-f(y)|}{|x-y|} \leq \displaystyle\lim_{y \rightarrow x} |x-y|^{n-1} = 0.$ Hence, $|f^{\prime}(x)| = 0$, which implies that $f^{\prime}(x) = 0.$ Since it holds for any $x \in \mathbb{R}$, we have $f^{\prime}(x) = 0$ for any $x \in \mathbb{R}.$ Are my two attempts correct? I am quite doubtful about my first attempt on the choosing $x$ and $y$ part.","Suppose $f:\mathbb{R} \rightarrow \mathbb{R}$ is a differentiable function and satisfies $|f(x) - f(y)| \leq |x-y|^n$ for all $n > 1.$ Show that $f^{\prime}(x)=0$ for all $x\in \mathbb{R}.$ My first attempt: Choose $x < y$ such that $y - x < 1$. By the Mean Value Theorem on $[x,y]$, we have $\frac{f(y) - f(x)}{y-x} = f^{\prime}(c)$ for some $c \in (x,y).$ Note that we have $| f^{\prime}(c) | = \frac{|f(y) - f(x)|}{y-x} \leq |x-y|^{n-1}$. Since it holds for any $n > 1$, we have $\displaystyle\lim_{n \rightarrow \infty}|f^{\prime}(c)| \leq \displaystyle\lim_{n \rightarrow \infty}|x-y|^{n-1}.$ Since $y - x < 1,$ we have $\displaystyle\lim_{ \rightarrow \infty}|x-y|^{n-1} = 0$. Hence, $|f^{\prime}(c)| = \displaystyle\lim_{n \rightarrow \infty}|f^{\prime}(c)| = 0 .$ Therefore, $f^{\prime}(c) = 0$. Since it holds for any interval $[x,y]$ that shrinks to a point in it, we can have $f^{\prime}(x)=0$ for all $x \in \mathbb{R}.$ My second attempt: Since $f$ is differentiable everywhere, we have for any $x \in \mathbb{R}, f^{\prime}(x) = \displaystyle\lim_{y \rightarrow x }\frac{f(x)-f(y)}{x-y}.$ Therefore, $|f^{\prime}(x)| = \displaystyle\lim_{y \rightarrow x }\frac{|f(x)-f(y)|}{|x-y|} \leq \displaystyle\lim_{y \rightarrow x} |x-y|^{n-1} = 0.$ Hence, $|f^{\prime}(x)| = 0$, which implies that $f^{\prime}(x) = 0.$ Since it holds for any $x \in \mathbb{R}$, we have $f^{\prime}(x) = 0$ for any $x \in \mathbb{R}.$ Are my two attempts correct? I am quite doubtful about my first attempt on the choosing $x$ and $y$ part.",,"['real-analysis', 'derivatives', 'proof-verification']"
9,Thermodynamic/Partial Differentiation Quesiton,Thermodynamic/Partial Differentiation Quesiton,,"I have a question which is somewhat physics-y but it's still a maths question at heart, I hope someone here can help. Basically, here is the problem, I get to this intermediate stage in a thermo proof: $$ dH = T \bigg( \frac{\partial S}{\partial T} \bigg)_P dT $$ The next stage just skips to  $$ \bigg( \frac{\partial H}{\partial T} \bigg)_P = T \bigg( \frac{\partial S}{\partial T} \bigg) _P $$ Did they just divide by dT? I thought that, that was not something that is even possible (in terms of proper maths). I've tried applying the product rule but then I end up in the precarious position of trying to to figure out what the derivative of dT is wrt to T, can someone please guide me through how you get from the first equation to the second in formal math terms please! Thank you.","I have a question which is somewhat physics-y but it's still a maths question at heart, I hope someone here can help. Basically, here is the problem, I get to this intermediate stage in a thermo proof: $$ dH = T \bigg( \frac{\partial S}{\partial T} \bigg)_P dT $$ The next stage just skips to  $$ \bigg( \frac{\partial H}{\partial T} \bigg)_P = T \bigg( \frac{\partial S}{\partial T} \bigg) _P $$ Did they just divide by dT? I thought that, that was not something that is even possible (in terms of proper maths). I've tried applying the product rule but then I end up in the precarious position of trying to to figure out what the derivative of dT is wrt to T, can someone please guide me through how you get from the first equation to the second in formal math terms please! Thank you.",,"['calculus', 'derivatives', 'partial-derivative', 'physics']"
10,Geometric intuition behind the derivative of $\sin^2(x)$ being $\sin(2x)$.,Geometric intuition behind the derivative of  being .,\sin^2(x) \sin(2x),"If you take the derivative of $\sin^2(x)$ and remember your double-angle formulas, you see that $$   \frac{\operatorname{d}}{\operatorname{d}\!x}\; \sin^2(x)   = 2\sin(x)\cos(x)    = \sin(2x)\,. $$ This looks surprisingly clean. You can say the rate of change of $\sin^2$ at a value is given by the $\sin$ of twice that value. Is this just a happy accident? Or is there some nice geometric/trigonometric intuition behind this that I'm not seeing?","If you take the derivative of $\sin^2(x)$ and remember your double-angle formulas, you see that $$   \frac{\operatorname{d}}{\operatorname{d}\!x}\; \sin^2(x)   = 2\sin(x)\cos(x)    = \sin(2x)\,. $$ This looks surprisingly clean. You can say the rate of change of $\sin^2$ at a value is given by the $\sin$ of twice that value. Is this just a happy accident? Or is there some nice geometric/trigonometric intuition behind this that I'm not seeing?",,"['calculus', 'trigonometry', 'derivatives', 'intuition']"
11,Derivatives on quadratic programming,Derivatives on quadratic programming,,"I'm trying to minimize a function using quadratic programming. The loss function to minimize is: $$L=\sum_{i}x_{i}M(a_{i}, a_{i})-\sum_{i,j}x_{i}x{j}M(a_{i},a_{j})$$ where M is a $NxN$ matrix and I have to calculate values of x. For quadratic programming y need to calculate the $H$ matrix (hessian matrix) and the $c$ coefficients vector. I get that $H$ matrix is $-2M$ and $c$ is the diagonal of $M$ => $c=diag(M)$ calculating derivatives. I have seen the solution and $H$ matrix is equal to $2M$ and $c=-diag(M)$. My question is: why the solution changes the sign of $H$ and $c$? and why it works and my solution doesn't? Thanks","I'm trying to minimize a function using quadratic programming. The loss function to minimize is: $$L=\sum_{i}x_{i}M(a_{i}, a_{i})-\sum_{i,j}x_{i}x{j}M(a_{i},a_{j})$$ where M is a $NxN$ matrix and I have to calculate values of x. For quadratic programming y need to calculate the $H$ matrix (hessian matrix) and the $c$ coefficients vector. I get that $H$ matrix is $-2M$ and $c$ is the diagonal of $M$ => $c=diag(M)$ calculating derivatives. I have seen the solution and $H$ matrix is equal to $2M$ and $c=-diag(M)$. My question is: why the solution changes the sign of $H$ and $c$? and why it works and my solution doesn't? Thanks",,"['derivatives', 'lagrange-multiplier', 'quadratic-programming']"
12,Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be differenciable and $[\nabla f(y)-\nabla f(x)]\cdot (y-x)\ge 0$. Is $f$ a convex fuucntion?,Let  be differenciable and . Is  a convex fuucntion?,f:\mathbb{R}^n\rightarrow\mathbb{R} [\nabla f(y)-\nabla f(x)]\cdot (y-x)\ge 0 f,"Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be differenciable and $[\nabla f(y)-\nabla f(x)]\cdot (y-x)\ge 0\ \forall x,y\in \mathbb{R}^n$. Is $f$ a convex function? Since $f$ is differentiable then convexity is equivalent to the condition $$f(y)\ge f(x)+\nabla f(x)\cdot (y-x)\ \forall x,y\in \mathbb{R}^n\ \ \ \ \ \ (1)$$ So If there is a function whose derivative satisfy $$[\nabla f(y)-\nabla f(x)]\cdot (y-x)\ge 0\ \forall x,y\in \mathbb{R}^n\ \ \ \ \ \ (2)$$ but $(1)$ is not true then the statement is false. I know that $$f(y)\ge f(x)+\nabla f(x)\cdot (y-x)\ \forall x,y\in \mathbb{R}^n\Rightarrow [\nabla f(y)-\nabla f(x)]\cdot (y-x)\ge 0\ \forall x,y\in \mathbb{R}^n,$$ but I can't prove the conversely or find a counterexample.","Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be differenciable and $[\nabla f(y)-\nabla f(x)]\cdot (y-x)\ge 0\ \forall x,y\in \mathbb{R}^n$. Is $f$ a convex function? Since $f$ is differentiable then convexity is equivalent to the condition $$f(y)\ge f(x)+\nabla f(x)\cdot (y-x)\ \forall x,y\in \mathbb{R}^n\ \ \ \ \ \ (1)$$ So If there is a function whose derivative satisfy $$[\nabla f(y)-\nabla f(x)]\cdot (y-x)\ge 0\ \forall x,y\in \mathbb{R}^n\ \ \ \ \ \ (2)$$ but $(1)$ is not true then the statement is false. I know that $$f(y)\ge f(x)+\nabla f(x)\cdot (y-x)\ \forall x,y\in \mathbb{R}^n\Rightarrow [\nabla f(y)-\nabla f(x)]\cdot (y-x)\ge 0\ \forall x,y\in \mathbb{R}^n,$$ but I can't prove the conversely or find a counterexample.",,"['derivatives', 'convex-analysis']"
13,"Differentiable and Continuous functions on [0,1] with 'weird' conditions.","Differentiable and Continuous functions on [0,1] with 'weird' conditions.",,"I've been stuck on this one for a while. Comes from an analysis qual question. Let f be a function that is continuous on $\left[0,1\right]$ and differentiable on $(0,1)$. Show that if $f(0)=0$ and $|f'(x)| \leq |f(x)|$ for all $x \in (0,1)$, then $f(x)=0$ for all $x \in \left[0,1\right]$. What I've tried doing so far is see if there was anything I could do with MVT. I didn't really see anything to do with definitions either..to which I have a feeling I'll be playing around with them. Drawing a picture was a little difficult with these conditions as well Any hints/suggestions?","I've been stuck on this one for a while. Comes from an analysis qual question. Let f be a function that is continuous on $\left[0,1\right]$ and differentiable on $(0,1)$. Show that if $f(0)=0$ and $|f'(x)| \leq |f(x)|$ for all $x \in (0,1)$, then $f(x)=0$ for all $x \in \left[0,1\right]$. What I've tried doing so far is see if there was anything I could do with MVT. I didn't really see anything to do with definitions either..to which I have a feeling I'll be playing around with them. Drawing a picture was a little difficult with these conditions as well Any hints/suggestions?",,['real-analysis']
14,Differentiating an improper integral with a variable limit,Differentiating an improper integral with a variable limit,,"Question is to find the optimal value(s) (i.e., values of $x$ and corresponding $f$ values) of $\displaystyle f(x)=\int_{x}^{\infty}\frac{xs^{2}}{-1+e^{\alpha s}}~ds$ where $x>0$ and $\alpha>0$. If any one knows how to solve this analytically please explain. If analytical solution is not possible what sort of numerical method I can use? I think Taylor series approximation cannot work as $s$ approaches infinity any polynomial approaches $\pm\infty$ while the integrand approaches zero. Am I correct here?  Further, how safe it is to go inside the integral sign and differentiate when differentiate $f$ with respect to $x$ as the upper limit is infinite, if I am to do that?","Question is to find the optimal value(s) (i.e., values of $x$ and corresponding $f$ values) of $\displaystyle f(x)=\int_{x}^{\infty}\frac{xs^{2}}{-1+e^{\alpha s}}~ds$ where $x>0$ and $\alpha>0$. If any one knows how to solve this analytically please explain. If analytical solution is not possible what sort of numerical method I can use? I think Taylor series approximation cannot work as $s$ approaches infinity any polynomial approaches $\pm\infty$ while the integrand approaches zero. Am I correct here?  Further, how safe it is to go inside the integral sign and differentiate when differentiate $f$ with respect to $x$ as the upper limit is infinite, if I am to do that?",,"['calculus', 'integration', 'derivatives', 'optimization']"
15,Derivatives-Piecewise Function problem,Derivatives-Piecewise Function problem,,"I was doing exercises from a book when I tried this particular one: Find $f ^\prime (0)$ knowing that $g(0)=g^\prime (0)=0$ and $g^{\prime\prime}(0)=2$. $$f(x)= \begin{cases} g(x)/x  & \text{if $x$≠0} \\ 0 & \text{if $x$=0} \end{cases} $$ I used the definition of derivative: $f^\prime(0)=\lim\limits_{x \to 0} \frac{f(x)-f(0)}{x-0}$ $f^\prime(0)=\lim\limits_{x \to 0} \frac{\frac{g(x)}x-0}{x-0}$ $f^\prime(0)=\lim\limits_{x \to 0} \frac{g(x)}{x^2}$ Since $g(0)=0$, I have and $0/0$ indetermination, So I applied L'Hopital: $f ^\prime(0)=\lim\limits_{x \to 0} \frac{g^\prime(x)}{2x}$ Again, $0/0$ indetermination because $g^\prime(0)=0$, I applied L'Hopital one more time: $f ^\prime(0)=\lim\limits_{x \to 0} \frac{g^{\prime\prime}(x)}{2}$ Now I got somewhere, $g^{\prime\prime}(0)=2$ so my limit equals $1$: $f^\prime(0)=\lim\limits_{x \to 0} \frac{g^{\prime\prime}(0)}{2}=1$ However, answer says $f^\prime (0)= -1$ so I dont know what to think. Did I miss something? Is this the correct way to solve this exercise?. Thanks in advance for helping.","I was doing exercises from a book when I tried this particular one: Find $f ^\prime (0)$ knowing that $g(0)=g^\prime (0)=0$ and $g^{\prime\prime}(0)=2$. $$f(x)= \begin{cases} g(x)/x  & \text{if $x$≠0} \\ 0 & \text{if $x$=0} \end{cases} $$ I used the definition of derivative: $f^\prime(0)=\lim\limits_{x \to 0} \frac{f(x)-f(0)}{x-0}$ $f^\prime(0)=\lim\limits_{x \to 0} \frac{\frac{g(x)}x-0}{x-0}$ $f^\prime(0)=\lim\limits_{x \to 0} \frac{g(x)}{x^2}$ Since $g(0)=0$, I have and $0/0$ indetermination, So I applied L'Hopital: $f ^\prime(0)=\lim\limits_{x \to 0} \frac{g^\prime(x)}{2x}$ Again, $0/0$ indetermination because $g^\prime(0)=0$, I applied L'Hopital one more time: $f ^\prime(0)=\lim\limits_{x \to 0} \frac{g^{\prime\prime}(x)}{2}$ Now I got somewhere, $g^{\prime\prime}(0)=2$ so my limit equals $1$: $f^\prime(0)=\lim\limits_{x \to 0} \frac{g^{\prime\prime}(0)}{2}=1$ However, answer says $f^\prime (0)= -1$ so I dont know what to think. Did I miss something? Is this the correct way to solve this exercise?. Thanks in advance for helping.",,"['derivatives', 'problem-solving']"
16,Determine $z = x+iy \in \mathbb{C}$ for which the derivative of $f(z) = x^{2}-y+ixy^{2}$ exists.,Determine  for which the derivative of  exists.,z = x+iy \in \mathbb{C} f(z) = x^{2}-y+ixy^{2},"May i know if my approach is correct? We need to solve the Cauchy Riemann equations to deduce which points $f(z)$ is differentiable at. Let $u(x,y) = x^{2}-y, v(x,y)= xy^{2}$ and thus $$u_x = 2x ~,~ u_y = -1~,~v_x =y^{2}~,~v_y = 2xy$$ we clearly see that $u_x,u_y,v_x,v_y$ are continuous on $\mathbb{C}$ since they are polynomials, thus to find where $f$ is differentiable, we proceed to solve the Cauchy - Riemann equations. $$u_x = v_y, u_y = -v_x$$ \begin{equation} u_x = v_y, u_y = -v_x \Rightarrow \begin{cases} 2x=2xy \\ y^{2} = 1 \end{cases} \end{equation} By solving the above equation, we have two sets of solutions where $(x,1),(0,-1)$ and hence $f$ is differentiable at $z = -i$ and $z= x+i$ for all $x \in \mathbb{R}$","May i know if my approach is correct? We need to solve the Cauchy Riemann equations to deduce which points $f(z)$ is differentiable at. Let $u(x,y) = x^{2}-y, v(x,y)= xy^{2}$ and thus $$u_x = 2x ~,~ u_y = -1~,~v_x =y^{2}~,~v_y = 2xy$$ we clearly see that $u_x,u_y,v_x,v_y$ are continuous on $\mathbb{C}$ since they are polynomials, thus to find where $f$ is differentiable, we proceed to solve the Cauchy - Riemann equations. $$u_x = v_y, u_y = -v_x$$ \begin{equation} u_x = v_y, u_y = -v_x \Rightarrow \begin{cases} 2x=2xy \\ y^{2} = 1 \end{cases} \end{equation} By solving the above equation, we have two sets of solutions where $(x,1),(0,-1)$ and hence $f$ is differentiable at $z = -i$ and $z= x+i$ for all $x \in \mathbb{R}$",,"['complex-analysis', 'derivatives', 'complex-numbers', 'partial-derivative']"
17,Find Maximal Edge of rectangle trapped in a triangle with a given surface using derivative,Find Maximal Edge of rectangle trapped in a triangle with a given surface using derivative,,"Inside $\triangle EFG$  with surface of $9 \ cm^2$ there is a rectangle $ABCD$ shuch as $AB=2CD$. I need to find the maximal length of BC. I've tried putting $BC=y$ and $x$ to several other stuff in order to find $y(x)$ and then solving $y'(x)=0$ in order to maximize $x$, but I'm getting equations that makes is really hard to solve for $y$. Any ideas maybe? Thanks","Inside $\triangle EFG$  with surface of $9 \ cm^2$ there is a rectangle $ABCD$ shuch as $AB=2CD$. I need to find the maximal length of BC. I've tried putting $BC=y$ and $x$ to several other stuff in order to find $y(x)$ and then solving $y'(x)=0$ in order to maximize $x$, but I'm getting equations that makes is really hard to solve for $y$. Any ideas maybe? Thanks",,"['geometry', 'derivatives', 'maxima-minima']"
18,"""Division"" of an inexact differential form by an exact differential form","""Division"" of an inexact differential form by an exact differential form",,"In thermodynamics I have come across a peculiar expression whose mathematical nature eludes me. Maybe one of you can help me finding out. First of all let us assume that in this scenario all premises of the Poincaré lemma are met. We discussed the first theorem of thermodynamics and some applications: $$dU=\delta Q +\delta W.$$ Where '$d$' denotes an exact differential form and $'\delta'$ denotes an inexact one. Using the state equation $pV = nRT$ of an ideal gas and the definition of the work differential for ideal gases $\delta W = -pdV$ we derived this thing : $$\frac{\delta W}{dp}=-dV.$$ Physically speaking I understand what this equation means. However, mathematically I do not understand what the left-hand side actually means. Also, given a differential form like $\delta Q=c_V\cdot dT$ one can calculate $\frac{\delta Q}{dp}=c_V\cdot\left(\frac{\partial T}{\partial p}\right)_V$. Our professor said that this quotient is not a derivative but a difference quotient. But then I don't understand how in the second case this difference quotient comes out to be a partial derivative anyway. I suppose it has to do with the fact that path integrals of inexact differential forms are path-dependent. However, as opposed to difference quotients of exact differentials I cannot imagine how to define a difference quotient of an inexact differential since those don't have an anti-derivative.","In thermodynamics I have come across a peculiar expression whose mathematical nature eludes me. Maybe one of you can help me finding out. First of all let us assume that in this scenario all premises of the Poincaré lemma are met. We discussed the first theorem of thermodynamics and some applications: $$dU=\delta Q +\delta W.$$ Where '$d$' denotes an exact differential form and $'\delta'$ denotes an inexact one. Using the state equation $pV = nRT$ of an ideal gas and the definition of the work differential for ideal gases $\delta W = -pdV$ we derived this thing : $$\frac{\delta W}{dp}=-dV.$$ Physically speaking I understand what this equation means. However, mathematically I do not understand what the left-hand side actually means. Also, given a differential form like $\delta Q=c_V\cdot dT$ one can calculate $\frac{\delta Q}{dp}=c_V\cdot\left(\frac{\partial T}{\partial p}\right)_V$. Our professor said that this quotient is not a derivative but a difference quotient. But then I don't understand how in the second case this difference quotient comes out to be a partial derivative anyway. I suppose it has to do with the fact that path integrals of inexact differential forms are path-dependent. However, as opposed to difference quotients of exact differentials I cannot imagine how to define a difference quotient of an inexact differential since those don't have an anti-derivative.",,"['derivatives', 'partial-derivative', 'differential-forms', 'differential']"
19,"Prove that if $f'(x) > g'(x)$, $f(a) \ge g(a)$ then $f(x) > g(x)$ in $(a,b]$","Prove that if ,  then  in","f'(x) > g'(x) f(a) \ge g(a) f(x) > g(x) (a,b]","There's a similar question here: If $f(a) = g(a)$ and $f'(x) < g'(x)$ for all $x \in (a,b)$, then $f(b) < g(b)$ however in the original answer the conditions are a bit different as well as the solution uses Mean value theorem which we didn't learn. $f(x)$ and $g(x)$ are continuous in $[a,b]$ and differentiable in $(a,b)$. $\quad f(a) \ge g(a)$ and $f'(x) > g'(x)$ in $(a,b)$. Prove that $f(x) > g(x)$ in $(a, b]$. Let $E(x) = f(x) - g(x)$. If $E(a) = E(b)$ then by Rolle theorem there must be a $c$ such that $E'(c) = 0$. But $E'(x) = f'(x) - g'(x) > 0$ for $a<x<b$ therefore $E'(x) \neq 0$. Therefore $E(a) \neq E(b)$. Because $E(x)$ is continuous in $[a,b]$ there must be an absolute minimum and maximum for $E(x)$ in $[a,b]$. According to max/min theorem possible max/min can be on the edges of an interval, where the derivative is $0$ or where derivative doesn't exist. Because $E(x)>0$ and differentiable in $(a,b)$ the max/min has to be on the edges. $E'(x) > 0 \Rightarrow$ $E(x)$ is ascending for $a<x<b$ therefore $E(a)$ = min, $E(b)$ = max. Because $f(a) \ge g(a)$ then $E(a) \ge 0$. Because $E(a)$ is the absolute min from there $E(b) >E(x)>E(a)$ for $a<x<b$. Of course there can't be any $x_1, x_2$ such that $E(x_1)=E(x_2)$ because then again by Rolle $E'(x)$  will have a zero but this contradicts the given conditions. Therefore if $E(b) >E(x)>E(a)$ for $a<x<b$ then $f(x)>g(x)$ for $a<x<b$. Q.E.D. Please let me know if there were parts I could've skipped in the proof.","There's a similar question here: If $f(a) = g(a)$ and $f'(x) < g'(x)$ for all $x \in (a,b)$, then $f(b) < g(b)$ however in the original answer the conditions are a bit different as well as the solution uses Mean value theorem which we didn't learn. $f(x)$ and $g(x)$ are continuous in $[a,b]$ and differentiable in $(a,b)$. $\quad f(a) \ge g(a)$ and $f'(x) > g'(x)$ in $(a,b)$. Prove that $f(x) > g(x)$ in $(a, b]$. Let $E(x) = f(x) - g(x)$. If $E(a) = E(b)$ then by Rolle theorem there must be a $c$ such that $E'(c) = 0$. But $E'(x) = f'(x) - g'(x) > 0$ for $a<x<b$ therefore $E'(x) \neq 0$. Therefore $E(a) \neq E(b)$. Because $E(x)$ is continuous in $[a,b]$ there must be an absolute minimum and maximum for $E(x)$ in $[a,b]$. According to max/min theorem possible max/min can be on the edges of an interval, where the derivative is $0$ or where derivative doesn't exist. Because $E(x)>0$ and differentiable in $(a,b)$ the max/min has to be on the edges. $E'(x) > 0 \Rightarrow$ $E(x)$ is ascending for $a<x<b$ therefore $E(a)$ = min, $E(b)$ = max. Because $f(a) \ge g(a)$ then $E(a) \ge 0$. Because $E(a)$ is the absolute min from there $E(b) >E(x)>E(a)$ for $a<x<b$. Of course there can't be any $x_1, x_2$ such that $E(x_1)=E(x_2)$ because then again by Rolle $E'(x)$  will have a zero but this contradicts the given conditions. Therefore if $E(b) >E(x)>E(a)$ for $a<x<b$ then $f(x)>g(x)$ for $a<x<b$. Q.E.D. Please let me know if there were parts I could've skipped in the proof.",,"['calculus', 'derivatives', 'proof-verification']"
20,Uniform Convergence of convex functions implies convergence of derivatives in $L^1$?,Uniform Convergence of convex functions implies convergence of derivatives in ?,L^1,"It is known that if a sequence of continously differentiable, convex functions $f_n: [a,b] \longrightarrow [c,d]$ converges uniformly to a continously differentiable function $f: [a,b] \longrightarrow [c,d]$, that the derivatives $f_n^\prime$ converges pointwise to $f^\prime$. But the convergence doesn't need to be uniformly, see Convex functions and uniform convergence of derivatives Does the convergence of $f_n^\prime$ to $f^\prime$ hold in the $L^1$-sense?","It is known that if a sequence of continously differentiable, convex functions $f_n: [a,b] \longrightarrow [c,d]$ converges uniformly to a continously differentiable function $f: [a,b] \longrightarrow [c,d]$, that the derivatives $f_n^\prime$ converges pointwise to $f^\prime$. But the convergence doesn't need to be uniformly, see Convex functions and uniform convergence of derivatives Does the convergence of $f_n^\prime$ to $f^\prime$ hold in the $L^1$-sense?",,"['derivatives', 'convex-analysis', 'uniform-convergence']"
21,Lipschitz Implicit Function,Lipschitz Implicit Function,,"I am reading a paper entitles ""Sphere Tracing: a Robust Antialiased Rendering of Distance Based Implicit Surfaces"" , written by John C. Hart. Here are some definitions: Surfaces/shapes can be defined implicitly. An implicit function is a continuous mapping $f$ : $R^n \rightarrow R$ that describes the set $A \subset R^n$ as the locus of points: $A \equiv \{ x : f(x) \le 0\}.$ The boundary of the surface may be denoted $f^{-1}(0)$ . Distance to a set. The distance $d(x,A)$ from a point $x \in R^3$ to a closed set $A \subset R^3$ is given by $d(x,A) = \min_{y \in A}||x - y||.$ Then Hart writes that if you have $|f(x)| = |d(x, f^{-1}(0))|$ (eq. 1) then you have what he calls a distance implicit function or DIF. And if you have $|f(x)| \le |d(x, f^{-1}(0))|$ then you have distance underestimate implicit function or DUF. I don't want to get into the details of why he is interested in the DUF type of function but in short within the context of ray-tracing (in computer graphics) this helps the process of finding if a ray (defined by an origin and a direction) intersects surfaces (which can be defined implicitly in this case). The property of DUF functions is that they guarantee to return a distance to the surface from a given point $x$ within which the ray ""cannot interest the surface or can intersect the surface (where the intersection point is exactly on the surface)"" but in no circumstance can overshoot the surface. That's just to provide some context to my questions. Then he goes on defining Lipschitz implicit functions or LIF. It is an implicit function such that $|f(x) - f(y)| \le \lambda ||x-y||$ (eq. 2) for some positive constant $\lambda$ . This constant is denoted the Lipschitz constant and is written as $Lip \; f$ . Any LIF can be made into a DUF. Let $f$ be a LIF with Lipschitz constant $\lambda$ . Let $y \in f^{-1}(0)$ be one of the points such that: $||x -y|| = d(x, f^{-1}(0))$ (eq. 3). Then by eq. 2 $|f(x)| \le Lip \; f \; d(x, f^{-1}(0)).$ Hence, $f(x)/ Lip \; f$ is a DUF for any LIF $f$ . QUESTIONS: I believe that Hart's point is to show that if an implicit function is a Lipschitz function, then it has a DUF, and thus this DUF can be used later on to compute the ""underestimate"" distance of a ray's origin to a given surface. Okay I get that. The proof from his paper is something I hope I understand as well: $\lambda ||x-y|| \ge |f(x) - f(y)|\\$ then replace $||x-y||$ by $d(x,f^{-1}(0))$ (eq. 3) $\lambda \; d(x,f^{-1}(0)) \ge |f(x) - f(y)|\\$ $f(y)=0$ in this particular case since $y \in A$ thus we can re-write: $\lambda \; d(x,f^{-1}(0)) \ge |f(x)|\\$ and finally: $ \dfrac {f(x)}{\lambda} \le d(x,f^{-1}(0))$ If this correct? Finally, this is the part where I am the most interested in an explanation. Hart writes: The Lipschitz constant of a continuous function is its maximum slope. The maximum slope can be found by setting the function's second derivative equal to zero and solving for x. Since x is a vector, these derivatives can be the squared magnitude of the gradient. Could someone explain me this part with possibly an example? (using a real function) PS: as a helper later on in the paper Hart uses the function: $C(r) = 2 r^3/R^3 - 3r^2/R^2 + 1$ then he computes the first and second derivative: $C'(r) = 6r^2/R^3 - 6r/R^2$ and $C''(r) = 12r/R^3 - 6/R^2.$ Setting C''(r) = 0 yield the max slope, which occurs at the midpoint r=R/2, which is C'(R/2) = -3/2R = Lip C(r). I really don't understand this at all. Could someone please explain? EDIT I believe I understand most of it myself now. The paper says the Lipschitz constant is equal to the maxima of the function which you can find if you know the second derivative of that function and solve for f''(x) = 0. So it would be great to learn/know how I can explain this property. So I understand how Hart finds the value of $r=R/2$ for C''(r) but why do we re-inject this value in C'(r) ( $C'(R/2)$ ) in order to find about the value of Lip C(r)? Is that because: $\lambda = \dfrac{|f(x) - f(y)|}{||x - y||}.$ Can be assumed as the first order derivative of the function f(x)?","I am reading a paper entitles ""Sphere Tracing: a Robust Antialiased Rendering of Distance Based Implicit Surfaces"" , written by John C. Hart. Here are some definitions: Surfaces/shapes can be defined implicitly. An implicit function is a continuous mapping : that describes the set as the locus of points: The boundary of the surface may be denoted . Distance to a set. The distance from a point to a closed set is given by Then Hart writes that if you have (eq. 1) then you have what he calls a distance implicit function or DIF. And if you have then you have distance underestimate implicit function or DUF. I don't want to get into the details of why he is interested in the DUF type of function but in short within the context of ray-tracing (in computer graphics) this helps the process of finding if a ray (defined by an origin and a direction) intersects surfaces (which can be defined implicitly in this case). The property of DUF functions is that they guarantee to return a distance to the surface from a given point within which the ray ""cannot interest the surface or can intersect the surface (where the intersection point is exactly on the surface)"" but in no circumstance can overshoot the surface. That's just to provide some context to my questions. Then he goes on defining Lipschitz implicit functions or LIF. It is an implicit function such that (eq. 2) for some positive constant . This constant is denoted the Lipschitz constant and is written as . Any LIF can be made into a DUF. Let be a LIF with Lipschitz constant . Let be one of the points such that: (eq. 3). Then by eq. 2 Hence, is a DUF for any LIF . QUESTIONS: I believe that Hart's point is to show that if an implicit function is a Lipschitz function, then it has a DUF, and thus this DUF can be used later on to compute the ""underestimate"" distance of a ray's origin to a given surface. Okay I get that. The proof from his paper is something I hope I understand as well: then replace by (eq. 3) in this particular case since thus we can re-write: and finally: If this correct? Finally, this is the part where I am the most interested in an explanation. Hart writes: The Lipschitz constant of a continuous function is its maximum slope. The maximum slope can be found by setting the function's second derivative equal to zero and solving for x. Since x is a vector, these derivatives can be the squared magnitude of the gradient. Could someone explain me this part with possibly an example? (using a real function) PS: as a helper later on in the paper Hart uses the function: then he computes the first and second derivative: and Setting C''(r) = 0 yield the max slope, which occurs at the midpoint r=R/2, which is C'(R/2) = -3/2R = Lip C(r). I really don't understand this at all. Could someone please explain? EDIT I believe I understand most of it myself now. The paper says the Lipschitz constant is equal to the maxima of the function which you can find if you know the second derivative of that function and solve for f''(x) = 0. So it would be great to learn/know how I can explain this property. So I understand how Hart finds the value of for C''(r) but why do we re-inject this value in C'(r) ( ) in order to find about the value of Lip C(r)? Is that because: Can be assumed as the first order derivative of the function f(x)?","f R^n \rightarrow R A \subset R^n A \equiv \{ x : f(x) \le 0\}. f^{-1}(0) d(x,A) x \in R^3 A \subset R^3 d(x,A) = \min_{y \in A}||x - y||. |f(x)| = |d(x, f^{-1}(0))| |f(x)| \le |d(x, f^{-1}(0))| x |f(x) - f(y)| \le \lambda ||x-y|| \lambda Lip \; f f \lambda y \in f^{-1}(0) ||x -y|| = d(x, f^{-1}(0)) |f(x)| \le Lip \; f \; d(x, f^{-1}(0)). f(x)/ Lip \; f f \lambda ||x-y|| \ge |f(x) - f(y)|\\ ||x-y|| d(x,f^{-1}(0)) \lambda \; d(x,f^{-1}(0)) \ge |f(x) - f(y)|\\ f(y)=0 y \in A \lambda \; d(x,f^{-1}(0)) \ge |f(x)|\\  \dfrac {f(x)}{\lambda} \le d(x,f^{-1}(0)) C(r) = 2 r^3/R^3 - 3r^2/R^2 + 1 C'(r) = 6r^2/R^3 - 6r/R^2 C''(r) = 12r/R^3 - 6/R^2. r=R/2 C'(R/2) \lambda = \dfrac{|f(x) - f(y)|}{||x - y||}.","['real-analysis', 'derivatives', 'implicit-differentiation']"
22,I am having trouble finding the first derivative of $R(P) = (Pe^r)(1-\frac{P}{K})$,I am having trouble finding the first derivative of,R(P) = (Pe^r)(1-\frac{P}{K}),I am having trouble finding the first derivative of $R(P) = (P\textrm{e}^r)(1-\frac{P}{K})$ I am told to use the product rule for this. The first part of it $(P\textrm{e}^r)$ I believe will remain the same for the derivative of it. I am struggling on the $(1-P/K)$.,I am having trouble finding the first derivative of $R(P) = (P\textrm{e}^r)(1-\frac{P}{K})$ I am told to use the product rule for this. The first part of it $(P\textrm{e}^r)$ I believe will remain the same for the derivative of it. I am struggling on the $(1-P/K)$.,,['derivatives']
23,taylor expansions for composition with a non differentiable function,taylor expansions for composition with a non differentiable function,,"Suppose we have a taylor series expansion for f(x), and g(x) is continuous in the interval of convergence of f but it is not differentiable at some points. Can we use the taylor expansion for f(g(x)) by replacing x by g(x) in the taylor expansion of f(x) at those points where g is not differentiable? But then considering f(g(x)) as a new function we won't be able to obtain the derivatives...eg. f(g(x))=cos(|x|) . (In this case the error function also tends to zero). Thomas calculus early transcendentals book just states that g(x) needs to be continuous over the interval, but i wanted to get some clarity over this","Suppose we have a taylor series expansion for f(x), and g(x) is continuous in the interval of convergence of f but it is not differentiable at some points. Can we use the taylor expansion for f(g(x)) by replacing x by g(x) in the taylor expansion of f(x) at those points where g is not differentiable? But then considering f(g(x)) as a new function we won't be able to obtain the derivatives...eg. f(g(x))=cos(|x|) . (In this case the error function also tends to zero). Thomas calculus early transcendentals book just states that g(x) needs to be continuous over the interval, but i wanted to get some clarity over this",,"['derivatives', 'taylor-expansion', 'function-and-relation-composition']"
24,Energy-term Derivation in the Level Set Formulation of Mumford-Shah,Energy-term Derivation in the Level Set Formulation of Mumford-Shah,,"In the level set formulation of the piece-wise constant Mumford-Shah energy for two regions (see e.g. • this presentation on slide 41 • or this video • or page 4 of Chan & Vese, 2001 ), the energy function is $$ E = \underbrace{\int_\Omega \left(\left(I-\mu_1\right)^2 - \left(I-\mu_2\right)^2\right) H\phi + \left(I-\mu_2\right)^2 \operatorname dx}_\text{data term} + \underbrace{\int_\Omega \left\|\nabla H\phi \right\|_1 \operatorname dx}_\text{perimeter penalty} \text, \tag{1} \label{eq:energy} $$ where $I$ is the image intensity at location $x$, the constants $\mu_1, \mu_2$ are the expected mean intensitites in the two regions that are to be separated, $\phi\left(x\right)$ is the surface and $H\phi$ is the Heaviside function of $\phi$, i.e. it is $H\phi = 1$ if $H\phi>0$ and $0$ else. In all sources that I've looked into (e.g. Eq. (9) of Chan & Vese, 2001 ), the derivative of the data term of $E$ from Eq. \eqref{eq:energy} w.r.t. $\phi$ is given as \begin{align} \delta\left(\phi\right)\cdot\left(\left(I - \mu_1\right)^2 - \left(I - \mu_2\right)^2\right) \text, \tag{2} \label{eq:data_term_deriv} \end{align} where $\delta$ is the Dirac delta function, i.e. $\frac{\operatorname d}{\operatorname d\phi} H\phi = \delta$. Why is the above the derivative of the data term w.r.t. $\phi$? Shouldn't it rather be \begin{align} \frac{\operatorname d}{\operatorname d\phi} \int_\Omega \left(\left(I-\mu_1\right)^2 - \left(I-\mu_2\right)^2\right) \cdot H\phi + \left(I-\mu_2\right)^2 \operatorname dx &\\ = \int_\Omega \left(\left(I-\mu_1\right)^2 - \left(I-\mu_2\right)^2\right) \cdot \frac{\operatorname d}{\operatorname d\phi} H\phi\operatorname dx &\\ = \int_\Omega \left(\left(I-\mu_1\right)^2 - \left(I-\mu_2\right)^2\right) \cdot \delta\left(\phi\right) \operatorname dx &\text? \end{align} So where's the integral gone in Eq. \eqref{eq:data_term_deriv}? I'm aware of the shifting property of the $\delta$ function, which would explain the vanishing of the integral, but then the $\delta$ itself would vanish as well; yet, it's present in Eq. \eqref{eq:data_term_deriv}.","In the level set formulation of the piece-wise constant Mumford-Shah energy for two regions (see e.g. • this presentation on slide 41 • or this video • or page 4 of Chan & Vese, 2001 ), the energy function is $$ E = \underbrace{\int_\Omega \left(\left(I-\mu_1\right)^2 - \left(I-\mu_2\right)^2\right) H\phi + \left(I-\mu_2\right)^2 \operatorname dx}_\text{data term} + \underbrace{\int_\Omega \left\|\nabla H\phi \right\|_1 \operatorname dx}_\text{perimeter penalty} \text, \tag{1} \label{eq:energy} $$ where $I$ is the image intensity at location $x$, the constants $\mu_1, \mu_2$ are the expected mean intensitites in the two regions that are to be separated, $\phi\left(x\right)$ is the surface and $H\phi$ is the Heaviside function of $\phi$, i.e. it is $H\phi = 1$ if $H\phi>0$ and $0$ else. In all sources that I've looked into (e.g. Eq. (9) of Chan & Vese, 2001 ), the derivative of the data term of $E$ from Eq. \eqref{eq:energy} w.r.t. $\phi$ is given as \begin{align} \delta\left(\phi\right)\cdot\left(\left(I - \mu_1\right)^2 - \left(I - \mu_2\right)^2\right) \text, \tag{2} \label{eq:data_term_deriv} \end{align} where $\delta$ is the Dirac delta function, i.e. $\frac{\operatorname d}{\operatorname d\phi} H\phi = \delta$. Why is the above the derivative of the data term w.r.t. $\phi$? Shouldn't it rather be \begin{align} \frac{\operatorname d}{\operatorname d\phi} \int_\Omega \left(\left(I-\mu_1\right)^2 - \left(I-\mu_2\right)^2\right) \cdot H\phi + \left(I-\mu_2\right)^2 \operatorname dx &\\ = \int_\Omega \left(\left(I-\mu_1\right)^2 - \left(I-\mu_2\right)^2\right) \cdot \frac{\operatorname d}{\operatorname d\phi} H\phi\operatorname dx &\\ = \int_\Omega \left(\left(I-\mu_1\right)^2 - \left(I-\mu_2\right)^2\right) \cdot \delta\left(\phi\right) \operatorname dx &\text? \end{align} So where's the integral gone in Eq. \eqref{eq:data_term_deriv}? I'm aware of the shifting property of the $\delta$ function, which would explain the vanishing of the integral, but then the $\delta$ itself would vanish as well; yet, it's present in Eq. \eqref{eq:data_term_deriv}.",,"['calculus', 'integration', 'derivatives']"
25,Principles of integrating/differentiating infinite sums,Principles of integrating/differentiating infinite sums,,"In which cases is it allowed to distribute integration/differentiation sign over an infinite sum? Per se, to perform the following operation(analogically for the differentiation as well). $$\int\sum_{i=1}^\infty f_i(x) dx\ =\sum_{i=1}^\infty \int f_i(x) dx\ $$ Particularly it is quite popular to apply this rule to the expansion of geometric series: e.i. $ \frac{1}{1-x}$ Note: I know that it can happen that all member functions of the series to be differentiable, yet the sum be nowhere differentiable(Weierstrass function) or even vice versa. So I am wandering if there is a theorem or a heuristic according to which one can decide whether that operation is allowed to perform or not.","In which cases is it allowed to distribute integration/differentiation sign over an infinite sum? Per se, to perform the following operation(analogically for the differentiation as well). $$\int\sum_{i=1}^\infty f_i(x) dx\ =\sum_{i=1}^\infty \int f_i(x) dx\ $$ Particularly it is quite popular to apply this rule to the expansion of geometric series: e.i. $ \frac{1}{1-x}$ Note: I know that it can happen that all member functions of the series to be differentiable, yet the sum be nowhere differentiable(Weierstrass function) or even vice versa. So I am wandering if there is a theorem or a heuristic according to which one can decide whether that operation is allowed to perform or not.",,"['real-analysis', 'integration', 'sequences-and-series', 'derivatives']"
26,Find $f^{(100)}(0) $ and $f^{(101)}(0) $ if $f(x)=xe^{\arctan{x}}$,Find  and  if,f^{(100)}(0)  f^{(101)}(0)  f(x)=xe^{\arctan{x}},$$f(x)=xe^{\arctan{x}}$$ Part of my solution $$f^{(n)}(x)=\sum_{k=0}^{n}\binom{n}{k}x^{(k)}(e^{\arctan{x}})^{(n-k)}=x(e^{\arctan{x}})^{(n)}+(e^{\arctan{x}})^{(n-1)}$$ First term probably disappears because $x=0$ but i don't know what to do with second term.,$$f(x)=xe^{\arctan{x}}$$ Part of my solution $$f^{(n)}(x)=\sum_{k=0}^{n}\binom{n}{k}x^{(k)}(e^{\arctan{x}})^{(n-k)}=x(e^{\arctan{x}})^{(n)}+(e^{\arctan{x}})^{(n-1)}$$ First term probably disappears because $x=0$ but i don't know what to do with second term.,,"['real-analysis', 'derivatives']"
27,"Find the linear approximation and the derivative of $m$ at $(A,B)\in \mbox{GL}(V)\times \mbox{GL}(V)$ where $m$ is ""composition"" map.","Find the linear approximation and the derivative of  at  where  is ""composition"" map.","m (A,B)\in \mbox{GL}(V)\times \mbox{GL}(V) m","Let $V$ be a finite dimensional vector space over $\mathbb{R}$. Let  $\mbox{GL}(V)\subset\mbox{End}(V)$ denote the subset of invertible maps. Let  $$m:\mbox{GL}(V)\times \mbox{GL}(V)\longrightarrow \mbox{GL}(V)$$ denote the ""composition"" map defined by $$(A,B)\mapsto m(A,B)=A\circ B.$$ Find the linear approximation to $m$ at $(A,B)\in \mbox{GL}(V)\times \mbox{GL}(V)$. Give a formula for the derivative of $m$ at $(A,B)\in \mbox{GL}(V)\times \mbox{GL}(V)$. My attempt and questions: We know that $\mbox{GL}(V)$ is an open subset of $\mbox{End}(V)$, therefore $\mbox{GL}(V)\times \mbox{GL}(V)$ is an open subset of $\mbox{End}(V)\times \mbox{End}(V)$, also we know that $\mbox{End}(V)\times \mbox{End}(V)$ is a vector space. Therefore, for $(\tilde{A},\tilde{B})\in \mbox{GL}(V)\times \mbox{GL}(V)$ we have $$m(\tilde{A},\tilde{B})=A\circ B + A\circ (\tilde{A}-A)+(\tilde{B}-B)\circ B+(\tilde{A}-A)\circ (\tilde{B}-B).$$ Then, for $(\tilde{A},\tilde{B})$ sufficiently close to  $(A,B)$ we have $$m(\tilde{A},\tilde{B})\thickapprox A\circ B + A\circ (\tilde{A}-A)+(\tilde{B}-B)\circ B.$$ Is this the linear approximation to $m$ at $(A,B)\in \mbox{GL}(V)\times \mbox{GL}(V)$? In this item, we know that $T_{(A,B)}\left(\mbox{GL}(V)\times \mbox{GL}(V)\right)=\mbox{End}(V)\times \mbox{End}(V)$ and $T_{m(A,B)}\mbox{GL}(V)=\mbox{End}(V)$. Therefore, given $(K,S)\in T_{(A,B)}\left(\mbox{GL}(V)\times \mbox{GL}(V)\right)$, i. e, $K, S \in \mbox{GL}(V)$, let  $c(t):=(A+tK,B+tS)$ be a curve, note that $c(0)=(A,B)$ and $c'(0)=(K,S)$. Therefore, $$\begin{array}{rcl}\left.Dm\right|_{(A,B)}(K,S)&=&\left.\frac{d}{dt}\right|_{t=0}m(c(t))=\left.\frac{d}{dt}\right|_{t=0}m(A+tK,B+tS) \\ &=& \left.\frac{d}{dt}\right|_{t=0}\left(A\circ B+t K\circ B+t A\circ S+t^{2} K\circ S\right) \\ &=&K\circ B + A \circ S.\end{array}$$   Therefore, the formula for the derivative of $m$ at $(A,B)$ is $$\begin{array}{rcl} \left.Dm\right|_{(A,B)}:T_{(A,B)}\left(\mbox{GL}(V)\times \mbox{GL}(V)\right) &\rightarrow & T_{m(A,B)}\mbox{GL}(V) \\ (K,S) &\mapsto & K\circ B + A \circ S. \end{array}$$ Is my answer correct?","Let $V$ be a finite dimensional vector space over $\mathbb{R}$. Let  $\mbox{GL}(V)\subset\mbox{End}(V)$ denote the subset of invertible maps. Let  $$m:\mbox{GL}(V)\times \mbox{GL}(V)\longrightarrow \mbox{GL}(V)$$ denote the ""composition"" map defined by $$(A,B)\mapsto m(A,B)=A\circ B.$$ Find the linear approximation to $m$ at $(A,B)\in \mbox{GL}(V)\times \mbox{GL}(V)$. Give a formula for the derivative of $m$ at $(A,B)\in \mbox{GL}(V)\times \mbox{GL}(V)$. My attempt and questions: We know that $\mbox{GL}(V)$ is an open subset of $\mbox{End}(V)$, therefore $\mbox{GL}(V)\times \mbox{GL}(V)$ is an open subset of $\mbox{End}(V)\times \mbox{End}(V)$, also we know that $\mbox{End}(V)\times \mbox{End}(V)$ is a vector space. Therefore, for $(\tilde{A},\tilde{B})\in \mbox{GL}(V)\times \mbox{GL}(V)$ we have $$m(\tilde{A},\tilde{B})=A\circ B + A\circ (\tilde{A}-A)+(\tilde{B}-B)\circ B+(\tilde{A}-A)\circ (\tilde{B}-B).$$ Then, for $(\tilde{A},\tilde{B})$ sufficiently close to  $(A,B)$ we have $$m(\tilde{A},\tilde{B})\thickapprox A\circ B + A\circ (\tilde{A}-A)+(\tilde{B}-B)\circ B.$$ Is this the linear approximation to $m$ at $(A,B)\in \mbox{GL}(V)\times \mbox{GL}(V)$? In this item, we know that $T_{(A,B)}\left(\mbox{GL}(V)\times \mbox{GL}(V)\right)=\mbox{End}(V)\times \mbox{End}(V)$ and $T_{m(A,B)}\mbox{GL}(V)=\mbox{End}(V)$. Therefore, given $(K,S)\in T_{(A,B)}\left(\mbox{GL}(V)\times \mbox{GL}(V)\right)$, i. e, $K, S \in \mbox{GL}(V)$, let  $c(t):=(A+tK,B+tS)$ be a curve, note that $c(0)=(A,B)$ and $c'(0)=(K,S)$. Therefore, $$\begin{array}{rcl}\left.Dm\right|_{(A,B)}(K,S)&=&\left.\frac{d}{dt}\right|_{t=0}m(c(t))=\left.\frac{d}{dt}\right|_{t=0}m(A+tK,B+tS) \\ &=& \left.\frac{d}{dt}\right|_{t=0}\left(A\circ B+t K\circ B+t A\circ S+t^{2} K\circ S\right) \\ &=&K\circ B + A \circ S.\end{array}$$   Therefore, the formula for the derivative of $m$ at $(A,B)$ is $$\begin{array}{rcl} \left.Dm\right|_{(A,B)}:T_{(A,B)}\left(\mbox{GL}(V)\times \mbox{GL}(V)\right) &\rightarrow & T_{m(A,B)}\mbox{GL}(V) \\ (K,S) &\mapsto & K\circ B + A \circ S. \end{array}$$ Is my answer correct?",,"['differential-geometry', 'derivatives', 'proof-verification', 'lie-groups']"
28,The second differential as a differential on the double tangent bundle,The second differential as a differential on the double tangent bundle,,"I know what the second differential of $f : \Bbb R^n \to \Bbb R$ means. Nevertheless, when working with abstract manifolds and in the absence of a connection, one cannot come up with a 2-covariant tensor that could reasonably be called $\Bbb d ^2 f$. This is why one gets around this by viewing $\Bbb d ^2 f$ as the differential $\Bbb d (\Bbb d f) : T(TM) \to \Bbb R$ of $\Bbb d f : TM \to \Bbb R$. In a bit of spare time I decided to try this on $\Bbb R^n$ and see what it gives. Unfortunately, the result lost me, because I do not know how to interpret it. Here it is. If $(\Bbb d _x f) (v) = \sum \limits _i (\partial _{x_i} f) (x) v_i$, then $$\begin{align} \Bbb d _{(x,v)} (\Bbb d f) &= \sum _j \partial _{x_j} \left( \sum \limits _i (\partial _{x_i} f) (x) v_i \right) \Bbb d x_j + \sum _j \partial _{v_j} \left( \sum \limits _i (\partial _{x_i} f) (x) v_i \right) \Bbb d v_j \\ &= \sum _{j,i} (\partial _{x_j} \partial _{x_i} f) (x) v_i \ \Bbb d x_j + \sum _{j,i} (\partial _{x_i} f) (x) \delta _{ij} \ \Bbb d v_j \\ &= \sum _{j,i} (\partial _{x_j} \partial _{x_i} f) (x) v_i \ \Bbb d x_j + \sum _i (\partial _{x_i} f) (x) \ \Bbb d v_i . \end{align}$$ There are two things bothering me here: the presence of those parasitic $v_i$ in the first sum; they should somehow play the role of $\Bbb d x_i$; the second term, containing derivatives of order 1. I guess that the first question is answered by noting that $\Bbb d (\Bbb d f)$ and $\Bbb d ^2 f$ are not really identical, that in fact they just correspond to each other by the isomorpshim that takes the map $(X, V) \to \Bbb d _{(x,v)} (\Bbb d f) (X, V)$ to... to what? I already have 4 arguments $x,v,X,V$, while $\Bbb d^2 f$ should have only 3. Equally important, how to eliminate the second sum?","I know what the second differential of $f : \Bbb R^n \to \Bbb R$ means. Nevertheless, when working with abstract manifolds and in the absence of a connection, one cannot come up with a 2-covariant tensor that could reasonably be called $\Bbb d ^2 f$. This is why one gets around this by viewing $\Bbb d ^2 f$ as the differential $\Bbb d (\Bbb d f) : T(TM) \to \Bbb R$ of $\Bbb d f : TM \to \Bbb R$. In a bit of spare time I decided to try this on $\Bbb R^n$ and see what it gives. Unfortunately, the result lost me, because I do not know how to interpret it. Here it is. If $(\Bbb d _x f) (v) = \sum \limits _i (\partial _{x_i} f) (x) v_i$, then $$\begin{align} \Bbb d _{(x,v)} (\Bbb d f) &= \sum _j \partial _{x_j} \left( \sum \limits _i (\partial _{x_i} f) (x) v_i \right) \Bbb d x_j + \sum _j \partial _{v_j} \left( \sum \limits _i (\partial _{x_i} f) (x) v_i \right) \Bbb d v_j \\ &= \sum _{j,i} (\partial _{x_j} \partial _{x_i} f) (x) v_i \ \Bbb d x_j + \sum _{j,i} (\partial _{x_i} f) (x) \delta _{ij} \ \Bbb d v_j \\ &= \sum _{j,i} (\partial _{x_j} \partial _{x_i} f) (x) v_i \ \Bbb d x_j + \sum _i (\partial _{x_i} f) (x) \ \Bbb d v_i . \end{align}$$ There are two things bothering me here: the presence of those parasitic $v_i$ in the first sum; they should somehow play the role of $\Bbb d x_i$; the second term, containing derivatives of order 1. I guess that the first question is answered by noting that $\Bbb d (\Bbb d f)$ and $\Bbb d ^2 f$ are not really identical, that in fact they just correspond to each other by the isomorpshim that takes the map $(X, V) \to \Bbb d _{(x,v)} (\Bbb d f) (X, V)$ to... to what? I already have 4 arguments $x,v,X,V$, while $\Bbb d^2 f$ should have only 3. Equally important, how to eliminate the second sum?",,"['differential-geometry', 'derivatives', 'smooth-manifolds', 'bilinear-form', 'vector-space-isomorphism']"
29,Any reason not to define a derivative as the average of the derivatives on all sides?,Any reason not to define a derivative as the average of the derivatives on all sides?,,"We all know $\operatorname{abs}$ is not differentiable in a classical sense, but one question that's always bothered me is, why not define the derivative as the average derivative in each direction? i.e., why not say: $$f'(z) = \frac{f'(z^+) + f'(z^-)}{2}$$ Of course, for complex $z$, we can generalize this to $$f'(z) = \int_{-\pi}^{+\pi}\frac{1}{\theta}\lim_{\Delta z \to 0}\frac{f(z + e^{i\theta}\Delta z) - f(z)}{\Delta z}\,d\theta$$ It coincides with the classical definition wherever the original exists, and it has the same useful properties (e.g. linearity) while being more general.    It also seems intuitive. So why not define it this way? Is it not useful? I don't imagine no one's tried it before!","We all know $\operatorname{abs}$ is not differentiable in a classical sense, but one question that's always bothered me is, why not define the derivative as the average derivative in each direction? i.e., why not say: $$f'(z) = \frac{f'(z^+) + f'(z^-)}{2}$$ Of course, for complex $z$, we can generalize this to $$f'(z) = \int_{-\pi}^{+\pi}\frac{1}{\theta}\lim_{\Delta z \to 0}\frac{f(z + e^{i\theta}\Delta z) - f(z)}{\Delta z}\,d\theta$$ It coincides with the classical definition wherever the original exists, and it has the same useful properties (e.g. linearity) while being more general.    It also seems intuitive. So why not define it this way? Is it not useful? I don't imagine no one's tried it before!",,"['calculus', 'derivatives']"
30,"How to show for a distribution $T$ and a test function $\varphi,~~T'[\varphi]\equiv -T[\varphi']\;?$",How to show for a distribution  and a test function,"T \varphi,~~T'[\varphi]\equiv -T[\varphi']\;?","For a generalized   function $T,$ we define   $$T'[\varphi] ~≡~ −T[φ']~~~~~~\forall φ ∈ \mathcal D(Ω).$$ where $\mathcal D(\Omega)$ denotes the test function space. I'm not getting how they deduced this relation. Can anyone tell me how to prove the relation?","For a generalized   function $T,$ we define   $$T'[\varphi] ~≡~ −T[φ']~~~~~~\forall φ ∈ \mathcal D(Ω).$$ where $\mathcal D(\Omega)$ denotes the test function space. I'm not getting how they deduced this relation. Can anyone tell me how to prove the relation?",,['functional-analysis']
31,Is the $x$-axis a differentiable function? [closed],Is the -axis a differentiable function? [closed],x,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Is the $x$-axis a differentiable function?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Is the $x$-axis a differentiable function?,,['derivatives']
32,Derivative of this function,Derivative of this function,,"Let $f : S^{n-1} \subset \mathbb{R}^n \to \mathbb{R}^n \setminus \{0\}$ be a differentiable mapping, $n \geq 2$, and consider the function $F = \frac{f}{\|f\|} : S^{n-1} \to S^{n-1}$. I calculated the derivative of $F$ and arrived at $$DF(p) \cdot v = \frac{1}{\|f(p)\|} \left( Df(p) \cdot v - \frac{\langle Df(p) \cdot v, f(p) \rangle}{\|f(p)\|^2} f(p) \right), \quad p \in S^{n-1}, \, v \in T_p S^{n-1}$$ Is it correct?","Let $f : S^{n-1} \subset \mathbb{R}^n \to \mathbb{R}^n \setminus \{0\}$ be a differentiable mapping, $n \geq 2$, and consider the function $F = \frac{f}{\|f\|} : S^{n-1} \to S^{n-1}$. I calculated the derivative of $F$ and arrived at $$DF(p) \cdot v = \frac{1}{\|f(p)\|} \left( Df(p) \cdot v - \frac{\langle Df(p) \cdot v, f(p) \rangle}{\|f(p)\|^2} f(p) \right), \quad p \in S^{n-1}, \, v \in T_p S^{n-1}$$ Is it correct?",,"['calculus', 'derivatives', 'chain-rule']"
33,O'Neill's differential geometry: typo in formula for partial derivative?,O'Neill's differential geometry: typo in formula for partial derivative?,,"I am working through Barrett O'Neill's Elementary Differential Geometry and I'm mildly confused. Exercise 3 in section 4.3 ask you to verify that $$\mathbf{y}_{u}=\mathbf{x}_{u}\frac{\partial \bar{u}}{\partial u}+\mathbf{x}_{v}\frac{\partial \bar{v}}{\partial u} $$ with a similar equation for $\mathbf{y}_{v}$. I have that $\mathbf{y}(u,v)=\mathbf{x}(\bar{u},\bar{v})$. I thought this was a typo, however this same notation appears in the next section as is. Any and all help will be appreciated.","I am working through Barrett O'Neill's Elementary Differential Geometry and I'm mildly confused. Exercise 3 in section 4.3 ask you to verify that $$\mathbf{y}_{u}=\mathbf{x}_{u}\frac{\partial \bar{u}}{\partial u}+\mathbf{x}_{v}\frac{\partial \bar{v}}{\partial u} $$ with a similar equation for $\mathbf{y}_{v}$. I have that $\mathbf{y}(u,v)=\mathbf{x}(\bar{u},\bar{v})$. I thought this was a typo, however this same notation appears in the next section as is. Any and all help will be appreciated.",,"['calculus', 'differential-geometry', 'derivatives']"
34,Derivative of $(\lambda I - A)^{-1}$ with respect to $\lambda$,Derivative of  with respect to,(\lambda I - A)^{-1} \lambda,Is need to work with $\frac{d}{d\lambda} (1 - v^{T}(\lambda I - A)^{-1}u)$. Is it true that:  $$\frac{d}{d\lambda} (1 - v^{T}(\lambda I - A)^{-1}u) = -v^{T}\frac{d}{d\lambda}(\lambda I - A)^{-1}u$$ Then from $(\lambda I - A)^{-1}(\lambda I - A) = I$ differentiating both sides with respect to $\lambda$ one obtains: $$ \frac{d}{d\lambda} (\lambda I - A)^{-1} (\lambda I - A) + (\lambda I -A)^{-1} I = O$$ hence $$ \frac{d}{d\lambda}(\lambda I - A)^{-1} = - ((\lambda I - A)^{-1})^{2}$$ In general  $$ \frac{d^{k}}{d\lambda^{k}} (\lambda I - A)^{-1} = (-1)^{k} k!((\lambda I - A)^{-1})^{k+1}$$ similar to the scalar case $\frac{1}{\lambda - a}$ ?,Is need to work with $\frac{d}{d\lambda} (1 - v^{T}(\lambda I - A)^{-1}u)$. Is it true that:  $$\frac{d}{d\lambda} (1 - v^{T}(\lambda I - A)^{-1}u) = -v^{T}\frac{d}{d\lambda}(\lambda I - A)^{-1}u$$ Then from $(\lambda I - A)^{-1}(\lambda I - A) = I$ differentiating both sides with respect to $\lambda$ one obtains: $$ \frac{d}{d\lambda} (\lambda I - A)^{-1} (\lambda I - A) + (\lambda I -A)^{-1} I = O$$ hence $$ \frac{d}{d\lambda}(\lambda I - A)^{-1} = - ((\lambda I - A)^{-1})^{2}$$ In general  $$ \frac{d^{k}}{d\lambda^{k}} (\lambda I - A)^{-1} = (-1)^{k} k!((\lambda I - A)^{-1})^{k+1}$$ similar to the scalar case $\frac{1}{\lambda - a}$ ?,,"['matrices', 'derivatives', 'inverse']"
35,Derivation of tetration by iteration,Derivation of tetration by iteration,,"I was screwing around a bit differentiating tetrations and was trying to write some rules for them. I came up with this recursive definition: $$ \frac{d\ ^nt}{dt} =\ ^nt \cdot log(t)\frac{d\ ^{n-1}t}{dt}+\ ^nt\ ^{n-1}tt^{-1}$$ Now I was trying to convert it to an iteration and read up on some programming note which suggested to do something like this: $$ d(a, i, n) = \begin{cases}     d(^it \cdot log(t) \cdot a+\ ^it\ ^{i-1}tt^{-1}, i+1,n),& \text{if } i\leq n\\     a,              & \text{if }i = n+1 \end{cases} \\ \frac{d\ ^nt}{dt} = d(0,1,n)$$ Which is tail-recursive and therefore can be turned into an iteration by a few minor changes. I was wondering whether that function could be written to something like a combination of a sum and a product function and how. Is this even possible? Thanks EDIT: I've found these guys having fun with this as well, but the posts are kind of old, the approaches are different from mine and they both seem to have hit a dead end. Because of that I find that this is not a duplicate. (I just wanted to get that out before this is marked as one.): What is the derivative of ${}^xx$ $n^{th}$ derivative of a tetration function (these might even come in handy.) EDIT: I came up with some things, so, this is the iteration (Java, pseudocode, assuming a and b are some sort of object Java has implemented a computer algebra system for and n[4]t is our implementation for $ \ ^nt $, $log(t)$ is an object, blah blah blah): d(int n){     CASObject a = 0;     for (int i = 1; i <= n; ++i)     {         a = i[4]t * log(t) * a + i[4]t * (i - 1)[4]t / t;     } return a; Now if I'm correct, I could move the addend out of there like this (I think): d2(int n){     CASObject a = 0;     for (int i = 1; i <= n; ++i)     {         b = i[4]t * (i - 1)[4]t / t;         for (int j = i; j <= n; ++j)         {             b *= j[4]t * log(t);         }         a += b;     }     return a; } Which in turn can be rewritten to: $$ \frac{d\ ^nt}{dt} = \sum_{i = 1}^n\ ^it\ ^{i - 1}tt^{-1} \prod_{j = i + 1}^n\ ^jt \cdot log(t)$$ Is this even still correct or would someone have any comments or suggestions on my method? It's a shame WolframAlpha doesn't support actual tetration, so I'll have to check it with pen and paper.","I was screwing around a bit differentiating tetrations and was trying to write some rules for them. I came up with this recursive definition: $$ \frac{d\ ^nt}{dt} =\ ^nt \cdot log(t)\frac{d\ ^{n-1}t}{dt}+\ ^nt\ ^{n-1}tt^{-1}$$ Now I was trying to convert it to an iteration and read up on some programming note which suggested to do something like this: $$ d(a, i, n) = \begin{cases}     d(^it \cdot log(t) \cdot a+\ ^it\ ^{i-1}tt^{-1}, i+1,n),& \text{if } i\leq n\\     a,              & \text{if }i = n+1 \end{cases} \\ \frac{d\ ^nt}{dt} = d(0,1,n)$$ Which is tail-recursive and therefore can be turned into an iteration by a few minor changes. I was wondering whether that function could be written to something like a combination of a sum and a product function and how. Is this even possible? Thanks EDIT: I've found these guys having fun with this as well, but the posts are kind of old, the approaches are different from mine and they both seem to have hit a dead end. Because of that I find that this is not a duplicate. (I just wanted to get that out before this is marked as one.): What is the derivative of ${}^xx$ $n^{th}$ derivative of a tetration function (these might even come in handy.) EDIT: I came up with some things, so, this is the iteration (Java, pseudocode, assuming a and b are some sort of object Java has implemented a computer algebra system for and n[4]t is our implementation for $ \ ^nt $, $log(t)$ is an object, blah blah blah): d(int n){     CASObject a = 0;     for (int i = 1; i <= n; ++i)     {         a = i[4]t * log(t) * a + i[4]t * (i - 1)[4]t / t;     } return a; Now if I'm correct, I could move the addend out of there like this (I think): d2(int n){     CASObject a = 0;     for (int i = 1; i <= n; ++i)     {         b = i[4]t * (i - 1)[4]t / t;         for (int j = i; j <= n; ++j)         {             b *= j[4]t * log(t);         }         a += b;     }     return a; } Which in turn can be rewritten to: $$ \frac{d\ ^nt}{dt} = \sum_{i = 1}^n\ ^it\ ^{i - 1}tt^{-1} \prod_{j = i + 1}^n\ ^jt \cdot log(t)$$ Is this even still correct or would someone have any comments or suggestions on my method? It's a shame WolframAlpha doesn't support actual tetration, so I'll have to check it with pen and paper.",,"['derivatives', 'recursion', 'tetration']"
36,find $\frac{\partial z }{\partial x}$ of $\tan(xy) + \tan(xz) + \tan(yz)$,find  of,\frac{\partial z }{\partial x} \tan(xy) + \tan(xz) + \tan(yz),Find $\frac{\partial z }{\partial x}$ of $\tan(xy)+\tan(xz)+\tan(yz)=0$ $$\frac{\partial z }{\partial x}\tan(xy)+\frac{\partial z }{\partial x}\tan(xz)+\frac{\partial z }{\partial x}\tan(yz)=0\rightarrow y\sec^2(xy)+\sec^2(xz)(z+\frac{\partial z }{\partial x}x)+\sec^2(yz)y\frac{\partial z }{\partial x}=0$$ $$y\sec^2(xy)+z\sec^2(xz)+x\sec^2(xz)\frac{\partial z }{\partial x}+y\sec^2(yz)\frac{\partial z }{\partial x}=0$$ $$\frac{\partial z }{\partial x}(x\sec^2(xz)+y\sec^2(yz))=-z\sec^2(xz)-y\sec^2(xy)$$ $$\frac{\partial z }{\partial x}=\frac{-z\sec^2(xz)-y\sec^2(xy)}{x\sec^2(xz)+y\sec^2(yz)}$$ Is the following correct?,Find of Is the following correct?,\frac{\partial z }{\partial x} \tan(xy)+\tan(xz)+\tan(yz)=0 \frac{\partial z }{\partial x}\tan(xy)+\frac{\partial z }{\partial x}\tan(xz)+\frac{\partial z }{\partial x}\tan(yz)=0\rightarrow y\sec^2(xy)+\sec^2(xz)(z+\frac{\partial z }{\partial x}x)+\sec^2(yz)y\frac{\partial z }{\partial x}=0 y\sec^2(xy)+z\sec^2(xz)+x\sec^2(xz)\frac{\partial z }{\partial x}+y\sec^2(yz)\frac{\partial z }{\partial x}=0 \frac{\partial z }{\partial x}(x\sec^2(xz)+y\sec^2(yz))=-z\sec^2(xz)-y\sec^2(xy) \frac{\partial z }{\partial x}=\frac{-z\sec^2(xz)-y\sec^2(xy)}{x\sec^2(xz)+y\sec^2(yz)},"['derivatives', 'partial-derivative']"
37,Second derivative test for $f(0)=f'(0)=0$ and $f''(0)=2$,Second derivative test for  and,f(0)=f'(0)=0 f''(0)=2,"Let $f\in C(\mathbb{R}^1)$ with $f(0)=f'(0)=0$ and $f''(0)=2$. Prove that $0$ is strict local minima of $f$. Proof: Since $f''(0)=2$ then $\exists\delta>0$ such that for any $t\in \mathbb{R}^1$ with $0<|t|<\delta$ we have $\left|\frac{f'(t)}{t}-2\right|<1 \Leftrightarrow$ $1<\frac{f'(t)}{t}<3$. For $t\in(0,\delta)$ we have $f'(t)>t>0$ then $f'(t)>0$ and for $t\in(-\delta,0)$ we have $f'(t)<t<0$ then $f'(t)<0$. Hence $f$ strictly increases on $(0,\delta)$ and strictly decreases on $(-\delta,0)$ with $f(0)=0$. Hence $0$ is strict local minima of $f$. I am sorry if this topic is repeated since this is famous result but I have never seen it's proof. Can anyone check my proof please?","Let $f\in C(\mathbb{R}^1)$ with $f(0)=f'(0)=0$ and $f''(0)=2$. Prove that $0$ is strict local minima of $f$. Proof: Since $f''(0)=2$ then $\exists\delta>0$ such that for any $t\in \mathbb{R}^1$ with $0<|t|<\delta$ we have $\left|\frac{f'(t)}{t}-2\right|<1 \Leftrightarrow$ $1<\frac{f'(t)}{t}<3$. For $t\in(0,\delta)$ we have $f'(t)>t>0$ then $f'(t)>0$ and for $t\in(-\delta,0)$ we have $f'(t)<t<0$ then $f'(t)<0$. Hence $f$ strictly increases on $(0,\delta)$ and strictly decreases on $(-\delta,0)$ with $f(0)=0$. Hence $0$ is strict local minima of $f$. I am sorry if this topic is repeated since this is famous result but I have never seen it's proof. Can anyone check my proof please?",,"['real-analysis', 'derivatives']"
38,"Let $\displaystyle f$ be differentiable, $\displaystyle f(x)=0$ for $|x| \geq 10 $ and $g(x)=\sum_{k \in \mathbb Z}f(x+k).$","Let  be differentiable,  for  and",\displaystyle f \displaystyle f(x)=0 |x| \geq 10  g(x)=\sum_{k \in \mathbb Z}f(x+k).,I came across the following problem that says: Let $\displaystyle f \colon \mathbb R \rightarrow \mathbb R$ be differentiable function and $\displaystyle f(x)=0$ for $|x| \geq 10.$ Let  $g(x)=\sum_{k \in \mathbb Z}f(x+k).$ Then   which of the following is true? $1.g$ is differentiable and $g'$ has infinitely many zeros $2.g$ is continuous and $g'$ has no zeros $3.g$ is differentiable and $g'$ has no zeros $4.g$ is differentiable and $g'$ has only finitely many zeros. I am not sure about how to progress with it.Can someone point me in the right direction? Thanks in advance for your time.,I came across the following problem that says: Let $\displaystyle f \colon \mathbb R \rightarrow \mathbb R$ be differentiable function and $\displaystyle f(x)=0$ for $|x| \geq 10.$ Let  $g(x)=\sum_{k \in \mathbb Z}f(x+k).$ Then   which of the following is true? $1.g$ is differentiable and $g'$ has infinitely many zeros $2.g$ is continuous and $g'$ has no zeros $3.g$ is differentiable and $g'$ has no zeros $4.g$ is differentiable and $g'$ has only finitely many zeros. I am not sure about how to progress with it.Can someone point me in the right direction? Thanks in advance for your time.,,[]
39,The function integrated over the two-sphere is smooth in the radial parameter,The function integrated over the two-sphere is smooth in the radial parameter,,"Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be a smooth function. Define the function $h: \mathbb{R} \rightarrow \mathbb{R}$ $$ h(w) = \int_{S^2} d\Omega(\hat{n})\,f(\sqrt{|w|}\, \hat{n}) $$ where $d\Omega(\hat{n})$ is the standard measure on two-dimensional sphere. That is, in $\theta$, $\phi$ coordinates $\sin\theta d\theta d\phi$, $\hat{n}(\theta,\phi)=(\sin\theta\cos\phi,\sin\theta\sin\phi,\cos\theta)$) and $$ h(w) = \int_{0}^\pi d \theta\int_0^{2\pi} d \phi~~ f(\sqrt{|w|}(\sin\theta\cos\phi,\sin\theta\sin\phi,\cos\theta)). $$ Is it true that $h$ is smooth everywhere?  This statement is certainly true if $f$ is polynomial (terms containing odd powers of $\sqrt{|w|}$ vanish after integration).","Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be a smooth function. Define the function $h: \mathbb{R} \rightarrow \mathbb{R}$ $$ h(w) = \int_{S^2} d\Omega(\hat{n})\,f(\sqrt{|w|}\, \hat{n}) $$ where $d\Omega(\hat{n})$ is the standard measure on two-dimensional sphere. That is, in $\theta$, $\phi$ coordinates $\sin\theta d\theta d\phi$, $\hat{n}(\theta,\phi)=(\sin\theta\cos\phi,\sin\theta\sin\phi,\cos\theta)$) and $$ h(w) = \int_{0}^\pi d \theta\int_0^{2\pi} d \phi~~ f(\sqrt{|w|}(\sin\theta\cos\phi,\sin\theta\sin\phi,\cos\theta)). $$ Is it true that $h$ is smooth everywhere?  This statement is certainly true if $f$ is polynomial (terms containing odd powers of $\sqrt{|w|}$ vanish after integration).",,"['real-analysis', 'integration', 'derivatives']"
40,"If number of points of discontinuity of the function $f(x)=\lfloor{2+10 \sin x\rfloor}$,in $[0,\frac{\pi}{2}]$ is same as","If number of points of discontinuity of the function ,in  is same as","f(x)=\lfloor{2+10 \sin x\rfloor} [0,\frac{\pi}{2}]","If number of points of discontinuity of the function $f(x)=\lfloor{2+10 \sin x\rfloor}$,in $[0,\frac{\pi}{2}]$ is same as number of points of non-differentiability of the function $g(x)=(x-1)(x-2)|(x-1)(x-2)....(x-2m)|,m\in N$ in $x \in (-\infty,\infty)$,then find the value of $m$ I drew the graph of $f(x)=\lfloor2+10 \sin x\rfloor$,in $[0,\frac{\pi}{2}]$ and counted points of discontinuity are $9$ And i counted the number of points of non-differentiability of the function $g(x)=(x-1)(x-2)|(x-1)(x-2)....(x-2m)|$ are $2m-2$ because the function will be differentiable at $x=1,2$. So $2m-2=9$ $m=5.5$ but the question says $m$ should be natural number.I dont know where i am wrong. Please help me.Thanks.","If number of points of discontinuity of the function $f(x)=\lfloor{2+10 \sin x\rfloor}$,in $[0,\frac{\pi}{2}]$ is same as number of points of non-differentiability of the function $g(x)=(x-1)(x-2)|(x-1)(x-2)....(x-2m)|,m\in N$ in $x \in (-\infty,\infty)$,then find the value of $m$ I drew the graph of $f(x)=\lfloor2+10 \sin x\rfloor$,in $[0,\frac{\pi}{2}]$ and counted points of discontinuity are $9$ And i counted the number of points of non-differentiability of the function $g(x)=(x-1)(x-2)|(x-1)(x-2)....(x-2m)|$ are $2m-2$ because the function will be differentiable at $x=1,2$. So $2m-2=9$ $m=5.5$ but the question says $m$ should be natural number.I dont know where i am wrong. Please help me.Thanks.",,"['real-analysis', 'derivatives', 'continuity']"
41,Find the number of points where the function $f(x)=(x^2-1)|(x+1)(x-2)|+\sin|x|$ is not differentiable.,Find the number of points where the function  is not differentiable.,f(x)=(x^2-1)|(x+1)(x-2)|+\sin|x|,"Find the number of points where the function $f(x)=(x^2-1)|(x+1)(x-2)|+\sin|x|$ is not differentiable. My Attempt: Since the $\sin|x|$ has a non differentiability point at $x=0$ and $(x^2-1)|(x+1)(x-2)|$ has two non differentiability points at $x=-1,x=2$,so i write the number of points where the function $f(x)=(x^2-1)|(x+1)(x-2)|+\sin|x|$ is not differentiable are $1+2=3$ but the answer given in my book is $2$.So i was wrong somewhere. My Second Attempt: I redefined $f(x)$ as $(x^2-1)(x^2-x-2)-\sin x$ when $x<-1$ $-(x^2-1)(x^2-x-2)-\sin x$ when $-1<x<0$ $-(x^2-1)(x^2-x-2)+\sin x$ when $0<x<2$ $(x^2-1)(x^2-x-2)+\sin x$ when $x>2$ I found its derivative $f'(x)$ as $(x+1)(4x^2-7x+1)-\cos x$ when $x<-1$ $-(x+1)(4x^2-7x+1)-\cos x$ when $-1<x<0$ $-(x+1)(4x^2-7x+1)+\cos x$ when $0<x<2$ $(x+1)(4x^2-7x+1)+\cos x$ when $x>2$ I found $LHD$ at $x=-1$ is not equal to $RHD$ at $x=-1$. I found $LHD$ at $x=0$ is not equal to $RHD$ at $x=0$. I found $LHD$ at $x=2$ is not equal to $RHD$ at $x=2$. So again i am getting three points of non differentiability.I dont know where i have gone wrong.Please help me.Thanks.","Find the number of points where the function $f(x)=(x^2-1)|(x+1)(x-2)|+\sin|x|$ is not differentiable. My Attempt: Since the $\sin|x|$ has a non differentiability point at $x=0$ and $(x^2-1)|(x+1)(x-2)|$ has two non differentiability points at $x=-1,x=2$,so i write the number of points where the function $f(x)=(x^2-1)|(x+1)(x-2)|+\sin|x|$ is not differentiable are $1+2=3$ but the answer given in my book is $2$.So i was wrong somewhere. My Second Attempt: I redefined $f(x)$ as $(x^2-1)(x^2-x-2)-\sin x$ when $x<-1$ $-(x^2-1)(x^2-x-2)-\sin x$ when $-1<x<0$ $-(x^2-1)(x^2-x-2)+\sin x$ when $0<x<2$ $(x^2-1)(x^2-x-2)+\sin x$ when $x>2$ I found its derivative $f'(x)$ as $(x+1)(4x^2-7x+1)-\cos x$ when $x<-1$ $-(x+1)(4x^2-7x+1)-\cos x$ when $-1<x<0$ $-(x+1)(4x^2-7x+1)+\cos x$ when $0<x<2$ $(x+1)(4x^2-7x+1)+\cos x$ when $x>2$ I found $LHD$ at $x=-1$ is not equal to $RHD$ at $x=-1$. I found $LHD$ at $x=0$ is not equal to $RHD$ at $x=0$. I found $LHD$ at $x=2$ is not equal to $RHD$ at $x=2$. So again i am getting three points of non differentiability.I dont know where i have gone wrong.Please help me.Thanks.",,"['calculus', 'real-analysis', 'derivatives']"
42,Is there a general algebraic notion of the chain rule?,Is there a general algebraic notion of the chain rule?,,"To motivate this, I should explain that I have been studying differential fields, i.e. fields endowed with a differentiation operator such that $(a+b)'=a'+b'$ and $(ab)'=a'b+ab'$. Using these rules, a lot of the standard differentiation rules that you would expect can be easily proven: $$ (a^n)'=na^{n-1}a'$$ $$ \left( \frac{a}{b} \right)'=\frac{a'b-b'a}{b^2}$$ The field is not required to be a field of functions; however, I have found that every instance of a differential field that I have seen (other than any field being endowed with the trivial derivation $\forall x,x'=0$ has been some sort of function field (usually a $F(x)$ for some field $F$ or something like the field of meromorphic functions). I was then curious if the chain rule could somehow carry over, and this got me thinking about what a functions field really was. I guess my question can then be formulated: Let $K$ be a field. Now let $F$ be a set of functions which map from $K$ into $K$ such that $F$ forms a field under point-wise addition and multiplication and $F$ is closed under composition and differentiation. My question is, what conditions must we place on $K$ or $F$ for it to be true that whenever $f,g\in F$, the following holds: $$ (f \circ g)'=(f' \circ g)\cdot g'$$ The only proof I have ever seen of the chain rule is done by analytical means; however, I suspect that it can be proven for fields like $Q(x)$ using purely algebraic means. Does anyone know anything about this?","To motivate this, I should explain that I have been studying differential fields, i.e. fields endowed with a differentiation operator such that $(a+b)'=a'+b'$ and $(ab)'=a'b+ab'$. Using these rules, a lot of the standard differentiation rules that you would expect can be easily proven: $$ (a^n)'=na^{n-1}a'$$ $$ \left( \frac{a}{b} \right)'=\frac{a'b-b'a}{b^2}$$ The field is not required to be a field of functions; however, I have found that every instance of a differential field that I have seen (other than any field being endowed with the trivial derivation $\forall x,x'=0$ has been some sort of function field (usually a $F(x)$ for some field $F$ or something like the field of meromorphic functions). I was then curious if the chain rule could somehow carry over, and this got me thinking about what a functions field really was. I guess my question can then be formulated: Let $K$ be a field. Now let $F$ be a set of functions which map from $K$ into $K$ such that $F$ forms a field under point-wise addition and multiplication and $F$ is closed under composition and differentiation. My question is, what conditions must we place on $K$ or $F$ for it to be true that whenever $f,g\in F$, the following holds: $$ (f \circ g)'=(f' \circ g)\cdot g'$$ The only proof I have ever seen of the chain rule is done by analytical means; however, I suspect that it can be proven for fields like $Q(x)$ using purely algebraic means. Does anyone know anything about this?",,"['derivatives', 'field-theory']"
43,Study $ h(x)= \sqrt{x^2-1}-x-3$,Study, h(x)= \sqrt{x^2-1}-x-3,"Let $g$ be the function defined by     $$\begin{array}{lrcl}  h : & [1;+\infty[  & \longrightarrow & \mathbb{R} \\      & x & \longmapsto & h(x)= \sqrt{x^2-1}-x-3 \end{array}$$ Study the Variations on $[1;+\infty [$, of function $g$ Deduce the sign of $g$ according to $x$ My proof: we have $h'(x)=\dfrac{x}{\sqrt{x^{2}-1} }-1$, always positive on the interval$[1; +\infty[$ , then $h$ is strictly increasing ($ h'$ isn't vanishes for any number $\forall x \in[1;+\infty[,\quad  h'(x)\neq 0$) on the same interval, since it is continuous, it has $h( [1;+\infty[)=[h(1) ;\lim_{x\to +\infty}h(x)[=[-4;-3[$ and then  $h$ is negative on the interval $[1 ;+\infty[$ Is My proof correct ? is there any other way to prove it. Thanks. Update: here how i calculate the $\lim_{x\to +\infty}h(x)=-3$ \begin{align} \lim_{x\to+\infty}h(x)&=\lim_{x\to+\infty}\sqrt{x^2-1}-x-3\\ &=\lim_{x\to+\infty}\dfrac{(\sqrt{x^2-1}-(x+3))(\sqrt{x^2-1}+x+3)}{\sqrt{x^2-1}+(x+3)}\\ &=\lim_{x\to+\infty}\dfrac{((x^2-1)-(x+3)^2)}{x\sqrt{1-\frac{1}{x^2}}+(x+3)}\\ &=\lim_{x\to+\infty}\dfrac{-6-\frac{10}{x}}{\sqrt{1-\frac{1}{x^2}}+1+\frac{3}{x}}=\frac{-6}{2}=-3 \end{align}","Let $g$ be the function defined by     $$\begin{array}{lrcl}  h : & [1;+\infty[  & \longrightarrow & \mathbb{R} \\      & x & \longmapsto & h(x)= \sqrt{x^2-1}-x-3 \end{array}$$ Study the Variations on $[1;+\infty [$, of function $g$ Deduce the sign of $g$ according to $x$ My proof: we have $h'(x)=\dfrac{x}{\sqrt{x^{2}-1} }-1$, always positive on the interval$[1; +\infty[$ , then $h$ is strictly increasing ($ h'$ isn't vanishes for any number $\forall x \in[1;+\infty[,\quad  h'(x)\neq 0$) on the same interval, since it is continuous, it has $h( [1;+\infty[)=[h(1) ;\lim_{x\to +\infty}h(x)[=[-4;-3[$ and then  $h$ is negative on the interval $[1 ;+\infty[$ Is My proof correct ? is there any other way to prove it. Thanks. Update: here how i calculate the $\lim_{x\to +\infty}h(x)=-3$ \begin{align} \lim_{x\to+\infty}h(x)&=\lim_{x\to+\infty}\sqrt{x^2-1}-x-3\\ &=\lim_{x\to+\infty}\dfrac{(\sqrt{x^2-1}-(x+3))(\sqrt{x^2-1}+x+3)}{\sqrt{x^2-1}+(x+3)}\\ &=\lim_{x\to+\infty}\dfrac{((x^2-1)-(x+3)^2)}{x\sqrt{1-\frac{1}{x^2}}+(x+3)}\\ &=\lim_{x\to+\infty}\dfrac{-6-\frac{10}{x}}{\sqrt{1-\frac{1}{x^2}}+1+\frac{3}{x}}=\frac{-6}{2}=-3 \end{align}",,"['calculus', 'real-analysis', 'derivatives', 'continuity']"
44,"Proving a Function has $f'(0) >0$ but for every $h >0$, $f(x)$ is not strictly increasing on $[-h,h]$.","Proving a Function has  but for every ,  is not strictly increasing on .","f'(0) >0 h >0 f(x) [-h,h]","From the title, consider the function $f(x) = x + x^2 S(1/x^2)$ if $x\ne 0$ and $f(x) = 0$ if $x=0$. $S(x) = 1-|x-1|$ if $-1\le x \le 3$ and $S(x) = S(x+4)$ for all real numbers $x$. I can prove the derivative at 0 is greater than 0 by the Squeeze Theorem but am unsure about the interval piece of the question.","From the title, consider the function $f(x) = x + x^2 S(1/x^2)$ if $x\ne 0$ and $f(x) = 0$ if $x=0$. $S(x) = 1-|x-1|$ if $-1\le x \le 3$ and $S(x) = S(x+4)$ for all real numbers $x$. I can prove the derivative at 0 is greater than 0 by the Squeeze Theorem but am unsure about the interval piece of the question.",,"['real-analysis', 'derivatives', 'monotone-functions']"
45,Why Does $f(x) = x\sqrt{x+3}$ Only Have One Critical Point?,Why Does  Only Have One Critical Point?,f(x) = x\sqrt{x+3},"I am trying to find the critical points of the function $f(x) = x\sqrt{x+3}$, then by using the First Derivative Test, determine which ones are a local maximum, local minimum, or neither. Using the product rule, we get $\frac{3(x+2)}{2\sqrt{x+3}}$. So $x=-2$ is a critical point. But isn't $x=-3$ also a critical point because if $x=-3$ then the derivative of $f(x)$ wouldn't exist. To give an example of why I am thinking this, look at $g(x) = x^{2/3}(6-x)^{1/3}$. The derivative of $g(x)$ is $\frac{4-x}{x^{1/3}(6-x)^{2/3}}$, where $x$ cannot equal $0$ or $6$. The critical points for $g(x)$ are $0$, $6$, and $4$. So why aren't $-2$ and $-3$ the critical points for $f(x)$?","I am trying to find the critical points of the function $f(x) = x\sqrt{x+3}$, then by using the First Derivative Test, determine which ones are a local maximum, local minimum, or neither. Using the product rule, we get $\frac{3(x+2)}{2\sqrt{x+3}}$. So $x=-2$ is a critical point. But isn't $x=-3$ also a critical point because if $x=-3$ then the derivative of $f(x)$ wouldn't exist. To give an example of why I am thinking this, look at $g(x) = x^{2/3}(6-x)^{1/3}$. The derivative of $g(x)$ is $\frac{4-x}{x^{1/3}(6-x)^{2/3}}$, where $x$ cannot equal $0$ or $6$. The critical points for $g(x)$ are $0$, $6$, and $4$. So why aren't $-2$ and $-3$ the critical points for $f(x)$?",,"['calculus', 'derivatives', 'fractions']"
46,"Is there a rectangle with a maximum area which has two corners at the x axis, one corner at $y_1=e^x$ and one at $y_2=2e^{-x}$ .","Is there a rectangle with a maximum area which has two corners at the x axis, one corner at  and one at  .",y_1=e^x y_2=2e^{-x},The inverses of $y_1$ and $y_2$ are : $x_1=ln y$ and $x_2=-ln\frac{y}{2}$ we need them to to calculate the side $a$ of a rectangle. The area of a rectangle is defined as its one side multiplied by its other one. $P=a*b$ $a=x_2-x_1$ $b=y$ $P=y*(-ln\frac{y}{2}-ln y)$ We derive $P$. $P'> 0$ Is this method of solving that problem correct?,The inverses of $y_1$ and $y_2$ are : $x_1=ln y$ and $x_2=-ln\frac{y}{2}$ we need them to to calculate the side $a$ of a rectangle. The area of a rectangle is defined as its one side multiplied by its other one. $P=a*b$ $a=x_2-x_1$ $b=y$ $P=y*(-ln\frac{y}{2}-ln y)$ We derive $P$. $P'> 0$ Is this method of solving that problem correct?,,['calculus']
47,Derivation of the Leibniz (product) rule for differentiating Grassman numbers,Derivation of the Leibniz (product) rule for differentiating Grassman numbers,,"In Chapter 1 of Nakahara's Geometry, Topology, Physics , Grassman numbers are defined as linear combinations of objects $\theta_i$ which satisfy anti-commutation relations $\{ \theta_i, \theta_j\} = 0$.  Then differentiation with respect to these $\theta_i$ variables is introduced, and is stated to follow these two rules: $\frac{d}{d\theta_i}\theta_j = \delta_{ij}$ ""It is also assumed that the differential operator anti-commutes with $\theta_k$.""  My reading of this is that for any given element $f$ of the Grassman algebra, the following holds: $\frac{d}{d\theta_i}(\theta_k\times f) + \theta_k \times \frac{d}{d\theta_i} f = 0$ Nakahara then writes ""The Leibnitz rule then takes the form $\frac{d}{d\theta_i}(\theta_j \theta_k) = \frac{d\theta_j}{d\theta_i}\theta_k - \theta_j\frac{d\theta_k}{d\theta_i}$,"" but I don't see how this follows from the previous two rules.  Naively, I would think that I could evaluate $\frac{d}{d\theta_i}(\theta_j \theta_k)$ by letting $\theta_k$ play the role of the function $f$ in rule (2) above, and letting $\theta_j$ play the role of $\theta_k$.  Explicitly, from rule (2) we know that $\frac{d}{d\theta_i}(\theta_k\times f) + \theta_k \times \frac{d}{d\theta_i} f = 0$ Then making the substitution $j\rightarrow k$ gives $\frac{d}{d\theta_i}(\theta_j\times f) + \theta_j \times \frac{d}{d\theta_i} f = 0$ Then since $f$ is any function, let $f = \theta_k$, so we have $\frac{d}{d\theta_i}(\theta_j\times \theta_k) + \theta_j \times \frac{d}{d\theta_i} \theta_k = 0$ So $\frac{d}{d\theta_i}(\theta_j\times \theta_k) = - \theta_j \times \frac{d}{d\theta_i} \theta_k$ Please help me understand where that derivation goes wrong.  Thanks!","In Chapter 1 of Nakahara's Geometry, Topology, Physics , Grassman numbers are defined as linear combinations of objects $\theta_i$ which satisfy anti-commutation relations $\{ \theta_i, \theta_j\} = 0$.  Then differentiation with respect to these $\theta_i$ variables is introduced, and is stated to follow these two rules: $\frac{d}{d\theta_i}\theta_j = \delta_{ij}$ ""It is also assumed that the differential operator anti-commutes with $\theta_k$.""  My reading of this is that for any given element $f$ of the Grassman algebra, the following holds: $\frac{d}{d\theta_i}(\theta_k\times f) + \theta_k \times \frac{d}{d\theta_i} f = 0$ Nakahara then writes ""The Leibnitz rule then takes the form $\frac{d}{d\theta_i}(\theta_j \theta_k) = \frac{d\theta_j}{d\theta_i}\theta_k - \theta_j\frac{d\theta_k}{d\theta_i}$,"" but I don't see how this follows from the previous two rules.  Naively, I would think that I could evaluate $\frac{d}{d\theta_i}(\theta_j \theta_k)$ by letting $\theta_k$ play the role of the function $f$ in rule (2) above, and letting $\theta_j$ play the role of $\theta_k$.  Explicitly, from rule (2) we know that $\frac{d}{d\theta_i}(\theta_k\times f) + \theta_k \times \frac{d}{d\theta_i} f = 0$ Then making the substitution $j\rightarrow k$ gives $\frac{d}{d\theta_i}(\theta_j\times f) + \theta_j \times \frac{d}{d\theta_i} f = 0$ Then since $f$ is any function, let $f = \theta_k$, so we have $\frac{d}{d\theta_i}(\theta_j\times \theta_k) + \theta_j \times \frac{d}{d\theta_i} \theta_k = 0$ So $\frac{d}{d\theta_i}(\theta_j\times \theta_k) = - \theta_j \times \frac{d}{d\theta_i} \theta_k$ Please help me understand where that derivation goes wrong.  Thanks!",,['derivatives']
48,Gradient of inner product in Hilbert space,Gradient of inner product in Hilbert space,,"Let $\mathcal{H}$ be a Hilbert space and \begin{align} f&\colon \mathcal{H} \to \mathbb{R}\\ f(x) &= ||x-c||_\mathcal{H} ^2 \end{align} from some constant $c \in \mathbb{H}$ Is the derivative of $f$ at $x$ equal $2x-2c$? I used  \begin{align} f(x) &= \langle x,x \rangle - 2\langle x,c \rangle + \langle c,c \rangle \end{align} and the standard rules for derivation \begin{align} \frac{\delta}{\delta x} f &= \frac{\delta}{\delta x} \langle x,x \rangle - \frac{\delta}{\delta x} 2\langle x,c \rangle + \frac{\delta}{\delta x} \langle c,c \rangle \\ &= 2x - 2 c \end{align} Are there any special ""obstacles"" I have to consider or can I just use the these rules as in the standard euclidean space?","Let $\mathcal{H}$ be a Hilbert space and \begin{align} f&\colon \mathcal{H} \to \mathbb{R}\\ f(x) &= ||x-c||_\mathcal{H} ^2 \end{align} from some constant $c \in \mathbb{H}$ Is the derivative of $f$ at $x$ equal $2x-2c$? I used  \begin{align} f(x) &= \langle x,x \rangle - 2\langle x,c \rangle + \langle c,c \rangle \end{align} and the standard rules for derivation \begin{align} \frac{\delta}{\delta x} f &= \frac{\delta}{\delta x} \langle x,x \rangle - \frac{\delta}{\delta x} 2\langle x,c \rangle + \frac{\delta}{\delta x} \langle c,c \rangle \\ &= 2x - 2 c \end{align} Are there any special ""obstacles"" I have to consider or can I just use the these rules as in the standard euclidean space?",,"['derivatives', 'hilbert-spaces', 'inner-products']"
49,How to differentiate a harmonic function presented by Poisson integral formula,How to differentiate a harmonic function presented by Poisson integral formula,,"Let $h(x+iy)$ be a harmonic function in the open neighbourhood of the closed unit disc $\overline\Delta(0;1)$ of $\mathbb{C}.$ Then it can be presented by Poisson integral formula in the following way: $$h(x+iy)=\frac{1}{2\pi}\int_{\partial\Delta(0;1)}\frac{1-x^2-y^2}{||x-iy-\zeta||^2}h(\zeta)d\sigma(\zeta),$$ where $\sigma$ is a uniformly distributed measure on the boundary of the disc. I need partial derivatives of $h,$ so I will have to differentiate under the integral sign. How can I justify this operation?","Let $h(x+iy)$ be a harmonic function in the open neighbourhood of the closed unit disc $\overline\Delta(0;1)$ of $\mathbb{C}.$ Then it can be presented by Poisson integral formula in the following way: $$h(x+iy)=\frac{1}{2\pi}\int_{\partial\Delta(0;1)}\frac{1-x^2-y^2}{||x-iy-\zeta||^2}h(\zeta)d\sigma(\zeta),$$ where $\sigma$ is a uniformly distributed measure on the boundary of the disc. I need partial derivatives of $h,$ so I will have to differentiate under the integral sign. How can I justify this operation?",,"['derivatives', 'harmonic-functions']"
50,Continuity of $f^{(n-1)}$ in Taylor's Theorem with Mean-value remainder,Continuity of  in Taylor's Theorem with Mean-value remainder,f^{(n-1)},"I refer to Rudin's proof of Taylor's Theorem with the Mean-value form of the remainder. I'm not sure if I'm understanding the proof correctly. Why must $f^{(n-1)}$ be continuous on $[a,b]$ ? I believe continuity at the endpoints is only used in the application of the mean value theorem for $g(\beta)$ and $g(\alpha), g'(\alpha), \cdots, g^{(n-1)}(\alpha)$ . Also, I don't get the point of $a$ and $b$ . Thus, is it ok to restate Taylor's theorem as follows: Suppose $f : D \to \mathbb{R}$ is continuous on $[\alpha, \beta] \subseteq D$ , $n$ is a positive integer, $f^{(n-1)}$ is continuous on $[\alpha, \beta)$ , $f^{(n)}(t)$ exists for every $t \in (\alpha, \beta)$ . Then there exists a point $x \in (\alpha, \beta)$ such that $$f(\beta) = \sum_{k=0}^{n-1}\frac{f^{(k)}(\alpha)}{k!}(\beta-\alpha)^k + \frac{f^{(n)}(x)}{n!}(\beta-\alpha)^k.$$ I checked the statement of the theorem on Wikipedia and it also has the condition of continuity of $f^{(n-1)}$ on the closed interval between $\alpha$ and $\beta$ . So I thought I might be missing something. Thanks!","I refer to Rudin's proof of Taylor's Theorem with the Mean-value form of the remainder. I'm not sure if I'm understanding the proof correctly. Why must be continuous on ? I believe continuity at the endpoints is only used in the application of the mean value theorem for and . Also, I don't get the point of and . Thus, is it ok to restate Taylor's theorem as follows: Suppose is continuous on , is a positive integer, is continuous on , exists for every . Then there exists a point such that I checked the statement of the theorem on Wikipedia and it also has the condition of continuity of on the closed interval between and . So I thought I might be missing something. Thanks!","f^{(n-1)} [a,b] g(\beta) g(\alpha), g'(\alpha), \cdots, g^{(n-1)}(\alpha) a b f : D \to \mathbb{R} [\alpha, \beta] \subseteq D n f^{(n-1)} [\alpha, \beta) f^{(n)}(t) t \in (\alpha, \beta) x \in (\alpha, \beta) f(\beta) = \sum_{k=0}^{n-1}\frac{f^{(k)}(\alpha)}{k!}(\beta-\alpha)^k + \frac{f^{(n)}(x)}{n!}(\beta-\alpha)^k. f^{(n-1)} \alpha \beta","['calculus', 'real-analysis', 'derivatives', 'continuity', 'taylor-expansion']"
51,$e^{-x}f(x)=2+ \int_0^x\sqrt{t^4+1}dt$ function for finding the inverse,function for finding the inverse,e^{-x}f(x)=2+ \int_0^x\sqrt{t^4+1}dt,"Let f be the real-valued function defined on the interval $(-1,1)$ such that $e^{-x}f(x)=2+ \int_0^x\sqrt{t^4+1}dt$ for all $x\in(-1,1)$ and let $f^{-1}$ be the inverse function of $f$. Then find the value of $(f^{-1})'(2)$. This is my step 1-$f(x)=y$ 2-$f^{-1}(y)=x$ 3-$f^{-1}(x)=y$ 4-$f^{-1}(x)'=\frac{dy}{dx}$ 5-  $y=f(x)=2e^{x}+e^{x} \int_0^x\sqrt{t^4+1}dt$ 6-  $y'=2e^{x}+e^{x} \int_0^x\sqrt(t^4+1)dt+e^x\sqrt{x^4+1}$ I am lost after this step, please help me as I have jolted down the step.","Let f be the real-valued function defined on the interval $(-1,1)$ such that $e^{-x}f(x)=2+ \int_0^x\sqrt{t^4+1}dt$ for all $x\in(-1,1)$ and let $f^{-1}$ be the inverse function of $f$. Then find the value of $(f^{-1})'(2)$. This is my step 1-$f(x)=y$ 2-$f^{-1}(y)=x$ 3-$f^{-1}(x)=y$ 4-$f^{-1}(x)'=\frac{dy}{dx}$ 5-  $y=f(x)=2e^{x}+e^{x} \int_0^x\sqrt{t^4+1}dt$ 6-  $y'=2e^{x}+e^{x} \int_0^x\sqrt(t^4+1)dt+e^x\sqrt{x^4+1}$ I am lost after this step, please help me as I have jolted down the step.",,"['calculus', 'integration', 'derivatives']"
52,Continuity of left derivative implies differentiability?,Continuity of left derivative implies differentiability?,,"Suppose $f:\mathbb{R}\rightarrow \mathbb{R}$ is continuous and has a left derivative, $f^-$, everywhere in a neighborhood of $x.$ Suppose $f^-$ is continuous at $x.$ Does this imply that $f$ is differentiable at $x$?","Suppose $f:\mathbb{R}\rightarrow \mathbb{R}$ is continuous and has a left derivative, $f^-$, everywhere in a neighborhood of $x.$ Suppose $f^-$ is continuous at $x.$ Does this imply that $f$ is differentiable at $x$?",,"['calculus', 'derivatives']"
53,Proof of Lipschitz continuous,Proof of Lipschitz continuous,,Wikipedia says that an everywhere differentiable function g : R → R is Lipschitz continuous (with K = sup |g′(x)|) if it has bounded first derivative. How to prove that?,Wikipedia says that an everywhere differentiable function g : R → R is Lipschitz continuous (with K = sup |g′(x)|) if it has bounded first derivative. How to prove that?,,"['derivatives', 'continuity']"
54,How to solve this related rates question?,How to solve this related rates question?,,"Wheat falls from an overhead bin and accumulates in a conical pile, so that the radius of the base of the cone is always twice the height of the cone. If the wheat falls at a rate of $3$ m$^3$/min, how fast is the height of the wheat pile changing when the diameter of the pile is $20$ meters? Here is what I tried. Let $r =$ radius, $h =$ height, $v =$ volume, and $t =$ time.  $$r = 2h.$$  $$v = \frac13\pi r^2h =\pi (2h)^2h/3 = 4\pi h^3/3.$$ $$\frac{\mathrm d v}{\mathrm dt} = \frac{\mathrm d}{\mathrm dt}(4\pi h^3/3).$$ $$\frac{\mathrm d v}{\mathrm dt} = 4\pi h^2 \frac{\mathrm d h}{\mathrm dt}.$$ When diameter $= 10$, height $= 2.5$.  $$3 = 4\pi(2.5)^2 \frac{\mathrm d h}{\mathrm dt}$$ $$3 = 25\pi\frac{\mathrm d h}{\mathrm dt}.$$ $$\frac{\mathrm d h}{\mathrm dt} =\frac3{25\pi} = 0.038\text{ m/min.}$$","Wheat falls from an overhead bin and accumulates in a conical pile, so that the radius of the base of the cone is always twice the height of the cone. If the wheat falls at a rate of $3$ m$^3$/min, how fast is the height of the wheat pile changing when the diameter of the pile is $20$ meters? Here is what I tried. Let $r =$ radius, $h =$ height, $v =$ volume, and $t =$ time.  $$r = 2h.$$  $$v = \frac13\pi r^2h =\pi (2h)^2h/3 = 4\pi h^3/3.$$ $$\frac{\mathrm d v}{\mathrm dt} = \frac{\mathrm d}{\mathrm dt}(4\pi h^3/3).$$ $$\frac{\mathrm d v}{\mathrm dt} = 4\pi h^2 \frac{\mathrm d h}{\mathrm dt}.$$ When diameter $= 10$, height $= 2.5$.  $$3 = 4\pi(2.5)^2 \frac{\mathrm d h}{\mathrm dt}$$ $$3 = 25\pi\frac{\mathrm d h}{\mathrm dt}.$$ $$\frac{\mathrm d h}{\mathrm dt} =\frac3{25\pi} = 0.038\text{ m/min.}$$",,['derivatives']
55,"Finding the largest constant $C$ such that $|\ln x−\ln y| \geq C|x−y|$ for all $x, y \in (0, 1]$",Finding the largest constant  such that  for all,"C |\ln x−\ln y| \geq C|x−y| x, y \in (0, 1]","Find the greatest value of C such that $|\ln x−\ln y|≥C|x−y|$ for any $x,y∈(0,1]$. What should my approach be? I can't think of much options.","Find the greatest value of C such that $|\ln x−\ln y|≥C|x−y|$ for any $x,y∈(0,1]$. What should my approach be? I can't think of much options.",,['calculus']
56,Jacobian Matrix of 6DOF Body (with IMU),Jacobian Matrix of 6DOF Body (with IMU),,"I am trying to derive the analytical Jacobian for a system that is essentially the equations of motion of a body (6 degrees of freedom) with gyro and accelerometer measurements. This is part of an Extended Kalman Filter. The system state is given by: $ \mathbf{x} = \left( \begin{array}{c} \mathbf{q}\\ \mathbf{b_\omega}\\ \mathbf{v}\\ \mathbf{b_a}\\ \mathbf{p}\\ \end{array} \right) $ where $q$ is the quaternion orientation of the body expressed in the global frame, $b_\omega$ and $b_a$ are the biases in the gyro and accelerometer respectively (expressed in the body frame) and $v$ and $p$ are the velocity and position of the body expressed in the global frame. All vectors are [3x1] except $q$ which is [4x1] in $[w,x,y,z]^\top$ format, and $R$ (below) which is [3x3]. The equations of motion $\frac{dx}{dt}=\dot{x}$ (t is time) are: $$ \dot{\mathbf{q}} = \frac{1}{2}\mathbf{q} \otimes  \left( \begin{array}{c} 0\\ \hat{\omega}\\ \end{array} \right) \\ \dot{\mathbf{b_\omega}} = 0 \\ \dot{\mathbf{v}} = R^\top (\hat{\mathbf{a}} + [\hat{\mathbf{\omega}}\times]R \mathbf{v})+ g \\ \dot{\mathbf{b_a}} = 0 \\ \dot{\mathbf{p}} = \mathbf{v} $$ Second-order terms are ignored. $\hat{a} = a - b_a$ and $\hat{\omega} = \omega - b_\omega$ are the corrected accelerometer and gyro biases ($a$ and $\omega$ are known) and are expressed in the body frame. $R$ is the rotation matrix (DCM) formed from $q$ and $g$ is the gravity vector $[0,0,9.81]^\top$. These equations have been validated against an aerospace engineering software library. I need the jacobian $F = \frac{d\dot{x}}{dx}$ but I cannot find this jacobian in any texts (I do find the error-state jacobian eg this paper ). I am struggling with doing this myself because I don't know how to handle the quaternion norm constraints. I also am concerned about the validity of a solution given through numerical differentiation. Any help or explanation would be greatly appreciated. This is going towards an open-source robot localisation project.","I am trying to derive the analytical Jacobian for a system that is essentially the equations of motion of a body (6 degrees of freedom) with gyro and accelerometer measurements. This is part of an Extended Kalman Filter. The system state is given by: $ \mathbf{x} = \left( \begin{array}{c} \mathbf{q}\\ \mathbf{b_\omega}\\ \mathbf{v}\\ \mathbf{b_a}\\ \mathbf{p}\\ \end{array} \right) $ where $q$ is the quaternion orientation of the body expressed in the global frame, $b_\omega$ and $b_a$ are the biases in the gyro and accelerometer respectively (expressed in the body frame) and $v$ and $p$ are the velocity and position of the body expressed in the global frame. All vectors are [3x1] except $q$ which is [4x1] in $[w,x,y,z]^\top$ format, and $R$ (below) which is [3x3]. The equations of motion $\frac{dx}{dt}=\dot{x}$ (t is time) are: $$ \dot{\mathbf{q}} = \frac{1}{2}\mathbf{q} \otimes  \left( \begin{array}{c} 0\\ \hat{\omega}\\ \end{array} \right) \\ \dot{\mathbf{b_\omega}} = 0 \\ \dot{\mathbf{v}} = R^\top (\hat{\mathbf{a}} + [\hat{\mathbf{\omega}}\times]R \mathbf{v})+ g \\ \dot{\mathbf{b_a}} = 0 \\ \dot{\mathbf{p}} = \mathbf{v} $$ Second-order terms are ignored. $\hat{a} = a - b_a$ and $\hat{\omega} = \omega - b_\omega$ are the corrected accelerometer and gyro biases ($a$ and $\omega$ are known) and are expressed in the body frame. $R$ is the rotation matrix (DCM) formed from $q$ and $g$ is the gravity vector $[0,0,9.81]^\top$. These equations have been validated against an aerospace engineering software library. I need the jacobian $F = \frac{d\dot{x}}{dx}$ but I cannot find this jacobian in any texts (I do find the error-state jacobian eg this paper ). I am struggling with doing this myself because I don't know how to handle the quaternion norm constraints. I also am concerned about the validity of a solution given through numerical differentiation. Any help or explanation would be greatly appreciated. This is going towards an open-source robot localisation project.",,"['derivatives', 'partial-derivative', 'control-theory', 'quaternions', 'nonlinear-system']"
57,Effect of differentiation on function growth rate,Effect of differentiation on function growth rate,,"For sufficiently ""nice"" functions, the differentiation operator appears to make slow growing functions grow slower and fast growing functions grow faster, with $e^x$ as a fixed point in the middle. My question is, how can we make this statement precise while still retaining as much generality as possible? Clearly, there are all sorts of oscillatory and fractal-like functions for which the statement does not hold, which we must exclude in some manner. This feels similar to the Gronwall inequality , but I'm having trouble making a precise connection.","For sufficiently ""nice"" functions, the differentiation operator appears to make slow growing functions grow slower and fast growing functions grow faster, with $e^x$ as a fixed point in the middle. My question is, how can we make this statement precise while still retaining as much generality as possible? Clearly, there are all sorts of oscillatory and fractal-like functions for which the statement does not hold, which we must exclude in some manner. This feels similar to the Gronwall inequality , but I'm having trouble making a precise connection.",,"['calculus', 'real-analysis', 'derivatives', 'asymptotics']"
58,Differentiable function only at $x=n$ where$ $n is an integer,Differentiable function only at  wheren is an integer,x=n  ,"Suppose $f:\mathbb R \to \mathbb R$ is only differentiable at integer points. Is this possible? If does, what kind of function is $f$?","Suppose $f:\mathbb R \to \mathbb R$ is only differentiable at integer points. Is this possible? If does, what kind of function is $f$?",,['derivatives']
59,"For every $x \in [a,b] , \exists n_x\in \mathbb Z^+$ such that $f^{(n_x) }(x)=0$ ; then to prove $f$ is a polynomial in $[a,b]$",For every  such that  ; then to prove  is a polynomial in,"x \in [a,b] , \exists n_x\in \mathbb Z^+ f^{(n_x) }(x)=0 f [a,b]","Let $f:[a,b] \to \mathbb R$ be a continuous  function having derivatives of all order such that for every $x \in [a,b] , \exists n_x\in \mathbb Z^+$ such that $f^{(n_x) }(x)=0$ ; then how do I show that $f$ is a polynomial in $[a,b]$ ? My try : For every $n \in \mathbb Z^+$ define $E_n:=\{x\in[a,b]:f^{(n)}(x)=0\}$ , so each $E_n$ is closed and by given condition , $[a,b]=\cup_{n=1}^\infty E_n$ , since $[a,b]$ is compact it is complete and also it is not hollow ( has non-empty interior) , so by Baire's category theorem , at least one of $E_n$ has non-empty interior , so for some $k\in \mathbb Z^+$ , $E_k$ contains an open ball $B(y,r)$ where $y \in [a,b]$ and then $f^{(k)}(x)=0 , \forall x \in B(y,r)  $ and then I am stuck , please help . I would also like to ask ; Can the proposition be proved without Baire's theorem ?","Let $f:[a,b] \to \mathbb R$ be a continuous  function having derivatives of all order such that for every $x \in [a,b] , \exists n_x\in \mathbb Z^+$ such that $f^{(n_x) }(x)=0$ ; then how do I show that $f$ is a polynomial in $[a,b]$ ? My try : For every $n \in \mathbb Z^+$ define $E_n:=\{x\in[a,b]:f^{(n)}(x)=0\}$ , so each $E_n$ is closed and by given condition , $[a,b]=\cup_{n=1}^\infty E_n$ , since $[a,b]$ is compact it is complete and also it is not hollow ( has non-empty interior) , so by Baire's category theorem , at least one of $E_n$ has non-empty interior , so for some $k\in \mathbb Z^+$ , $E_k$ contains an open ball $B(y,r)$ where $y \in [a,b]$ and then $f^{(k)}(x)=0 , \forall x \in B(y,r)  $ and then I am stuck , please help . I would also like to ask ; Can the proposition be proved without Baire's theorem ?",,"['real-analysis', 'derivatives']"
60,derivative of t distribution cdf wrt degrees of freedom [closed],derivative of t distribution cdf wrt degrees of freedom [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Given the cdf of a t distribution as follows: $T_\nu(x)=\frac{1}{2} + x\Gamma(\frac{\nu+1}{2}) + \frac{_2F_1 (\frac{1}{2},\frac{\nu+1}{2};\frac{3}{2};-\frac{x^2}{\nu})}{(\pi\nu)^{1/2}\Gamma(\frac{\nu}{2})} $, where $_2F_1$ is the hypergeometric function. We want to compute the derivative $\frac{\partial T_\nu(x)}{\partial\nu}$. Thanks!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Given the cdf of a t distribution as follows: $T_\nu(x)=\frac{1}{2} + x\Gamma(\frac{\nu+1}{2}) + \frac{_2F_1 (\frac{1}{2},\frac{\nu+1}{2};\frac{3}{2};-\frac{x^2}{\nu})}{(\pi\nu)^{1/2}\Gamma(\frac{\nu}{2})} $, where $_2F_1$ is the hypergeometric function. We want to compute the derivative $\frac{\partial T_\nu(x)}{\partial\nu}$. Thanks!",,"['calculus', 'derivatives', 'probability-distributions', 'power-series', 'partial-derivative']"
61,Nice proof for $\lim_{h\to 0}\frac{f(x+nh)-f(x)}{h}=nf'(x)$ besides LHR,Nice proof for  besides LHR,\lim_{h\to 0}\frac{f(x+nh)-f(x)}{h}=nf'(x),"Why is $$\lim_{h\to 0}\frac{f(x+nh)-f(x)}{h}=nf'(x)?$$ A cheap answer would be L'Hospital's rule, but I think there should be a more direct way to prove it, appealing to the first principles of the derivative. Intuitively, it makes sense that near $x$, the function $h\mapsto f(x+h) - f(x)$ is behaving approximately linearly, and so $n(f(x+h)-f(x)) \approx f(x+nh)-f(x)$. Anyone know of a slick way to show this?","Why is $$\lim_{h\to 0}\frac{f(x+nh)-f(x)}{h}=nf'(x)?$$ A cheap answer would be L'Hospital's rule, but I think there should be a more direct way to prove it, appealing to the first principles of the derivative. Intuitively, it makes sense that near $x$, the function $h\mapsto f(x+h) - f(x)$ is behaving approximately linearly, and so $n(f(x+h)-f(x)) \approx f(x+nh)-f(x)$. Anyone know of a slick way to show this?",,"['calculus', 'derivatives', 'arithmetic']"
62,"Project on slope, rates of change, and instantaneous rates of change","Project on slope, rates of change, and instantaneous rates of change",,"I was wondering if someone could look over my work and tell me if I am doing this correctly. Also, I need help on section D. Not understanding what values to substitute into the difference quotient. I know the formula, i just don't understand what vaules to input.","I was wondering if someone could look over my work and tell me if I am doing this correctly. Also, I need help on section D. Not understanding what values to substitute into the difference quotient. I know the formula, i just don't understand what vaules to input.",,"['calculus', 'algebra-precalculus', 'derivatives']"
63,Derivation of the integral,Derivation of the integral,,"Evaluate   $$\large\frac{d}{dx}\int_{0}^{\large\int_0^{e^x}{\cos (s)\,\mathrm  ds}}\sec(t^2)\,\mathrm dt$$ I got the answer to be $$e^x\cdot\sec(\sin^2(e^x))\cdot \cos(e^x)$$ but do not know if this is correct and if not some suggestions?","Evaluate   $$\large\frac{d}{dx}\int_{0}^{\large\int_0^{e^x}{\cos (s)\,\mathrm  ds}}\sec(t^2)\,\mathrm dt$$ I got the answer to be $$e^x\cdot\sec(\sin^2(e^x))\cdot \cos(e^x)$$ but do not know if this is correct and if not some suggestions?",,"['calculus', 'integration', 'derivatives']"
64,Find all numbers $c$ that satisfy Mean Value Theorem,Find all numbers  that satisfy Mean Value Theorem,c,"Verify that the function satisfies the hypotheses of the Mean Value Theorem on the given interval. Then find all numbers $c$ that satisfy the conclusion of the Mean Value Theorem. My function is a simple one: $x^{1/3}$ on the interval $[0,1]$. Where I'm screwing up (I believe) is setting the derivative $1/3(x^{-2/3})$ equal to $1$. Sorry if that too many parentheses or too silly of a question. It's my first on here. I know (via the back of the book) that the answer should be $\frac{\sqrt{3}}{9}$ Thanks.","Verify that the function satisfies the hypotheses of the Mean Value Theorem on the given interval. Then find all numbers $c$ that satisfy the conclusion of the Mean Value Theorem. My function is a simple one: $x^{1/3}$ on the interval $[0,1]$. Where I'm screwing up (I believe) is setting the derivative $1/3(x^{-2/3})$ equal to $1$. Sorry if that too many parentheses or too silly of a question. It's my first on here. I know (via the back of the book) that the answer should be $\frac{\sqrt{3}}{9}$ Thanks.",,"['calculus', 'derivatives']"
65,A property of homogeneous of degree p functions:,A property of homogeneous of degree p functions:,,"Prove that if $f(x_1,...,x_n)$ is homogeneous of degree $p$, i.e; $f(tx)=t^pf(x)$. Then: $$(x_1 \frac { \partial}{\partial x_1} +...+x_n \frac { \partial}{\partial x_n})^mf(x_1,...,x_n)=p(p-1)(p-2)...(p-m+1)f(x_1,...,x_n)$$ I've tried using the multinomial theorem and doing some induction over $m$, however I haven't been able to prove it even for $m=2$. Is this approach right, or what would you do?","Prove that if $f(x_1,...,x_n)$ is homogeneous of degree $p$, i.e; $f(tx)=t^pf(x)$. Then: $$(x_1 \frac { \partial}{\partial x_1} +...+x_n \frac { \partial}{\partial x_n})^mf(x_1,...,x_n)=p(p-1)(p-2)...(p-m+1)f(x_1,...,x_n)$$ I've tried using the multinomial theorem and doing some induction over $m$, however I haven't been able to prove it even for $m=2$. Is this approach right, or what would you do?",,"['real-analysis', 'derivatives', 'partial-derivative', 'homogeneous-equation']"
66,Total Time Taken Question,Total Time Taken Question,,"Distance of chord = Time taken to ""swim"" to the desalination plant = I'm stuck here! The textbook working out is as such: I don't understand how they have the 'k' or 1/2 the runs river at 2 km per hour.... isn't this DL/Dx=2","Distance of chord = Time taken to ""swim"" to the desalination plant = I'm stuck here! The textbook working out is as such: I don't understand how they have the 'k' or 1/2 the runs river at 2 km per hour.... isn't this DL/Dx=2",,"['calculus', 'integration', 'algebra-precalculus', 'derivatives']"
67,"Differentiation of the Law of Cosines, where a, b, c, A, B, and C are functions of time t","Differentiation of the Law of Cosines, where a, b, c, A, B, and C are functions of time t",,"Is the differentiation of the law of cosines ($c^2= a^2 + b^2 - 2ab\cos C$) this? a, b, c, A, B, and C are functions of time t. $$2c \frac{dc}{dt} = 2a \frac{da}{dt} + 2b\frac{db}{dt} - 2b \cos C \frac{da}{dt} - 2 a \cos C \frac{db}{dt} + 2ab \sin C \frac{dC}{dt}$$","Is the differentiation of the law of cosines ($c^2= a^2 + b^2 - 2ab\cos C$) this? a, b, c, A, B, and C are functions of time t. $$2c \frac{dc}{dt} = 2a \frac{da}{dt} + 2b\frac{db}{dt} - 2b \cos C \frac{da}{dt} - 2 a \cos C \frac{db}{dt} + 2ab \sin C \frac{dC}{dt}$$",,['derivatives']
68,A Tricky Weak Derivatives question,A Tricky Weak Derivatives question,,"I recently came across the following statement and am having trouble proving it correct. I wonder if you could help. Given a weak derivative, $x'$, there exists an absolutely continuous representative of $x$ (which we shall still call $x$, for convenience), such that $x(b)-x(a)= \int_{a}^{b}x'(v)\mathrm{d}v$, $\forall a,b \in\Bbb R^{+}$. Please, this has me somewhat stumped.","I recently came across the following statement and am having trouble proving it correct. I wonder if you could help. Given a weak derivative, $x'$, there exists an absolutely continuous representative of $x$ (which we shall still call $x$, for convenience), such that $x(b)-x(a)= \int_{a}^{b}x'(v)\mathrm{d}v$, $\forall a,b \in\Bbb R^{+}$. Please, this has me somewhat stumped.",,"['calculus', 'real-analysis']"
69,Differentiation with respect to the index of the summation notion?,Differentiation with respect to the index of the summation notion?,,"$$\sum_{t=1}^k \binom{N-1}{t-1} \int[1-F(s)]^{N-1}[F(s)]^{t-1}g(s)\,ds$$ $k\in\mathbb Z ^+$ If I want to find out the effects of changing $k$ (comparative statics), what can I do? Differentiation will not work in this case, will it?","$$\sum_{t=1}^k \binom{N-1}{t-1} \int[1-F(s)]^{N-1}[F(s)]^{t-1}g(s)\,ds$$ $k\in\mathbb Z ^+$ If I want to find out the effects of changing $k$ (comparative statics), what can I do? Differentiation will not work in this case, will it?",,"['integration', 'derivatives', 'summation']"
70,Derivative of Binomial Coefficient $\binom{2N}{N-x}$ with respect to $x$,Derivative of Binomial Coefficient  with respect to,\binom{2N}{N-x} x,"I've got $\binom{2N}{N-x}$ and I'd like to take the derivative with respect to $x$. I know that I can take the derivative of $\binom{n}{k}$ w.r.t. n using logarithmic differentiation, but that's not going to work for this. I plug it into Mathematica and get $\binom{2N}{N-x}[\psi(1+N-x)-\psi(1+N+x)]$, where $\psi$ is the digamma function. Can anybody help me figure out how to get from point A to point B without blindly trusting Mathematica?","I've got $\binom{2N}{N-x}$ and I'd like to take the derivative with respect to $x$. I know that I can take the derivative of $\binom{n}{k}$ w.r.t. n using logarithmic differentiation, but that's not going to work for this. I plug it into Mathematica and get $\binom{2N}{N-x}[\psi(1+N-x)-\psi(1+N+x)]$, where $\psi$ is the digamma function. Can anybody help me figure out how to get from point A to point B without blindly trusting Mathematica?",,"['derivatives', 'binomial-coefficients', 'polygamma']"
71,"Are ""most"" continuous functions also differentiable?","Are ""most"" continuous functions also differentiable?",,"Let $A$ be a nonempty open subset of $\mathbb{R}$. Consider a function $f : A \rightarrow \mathbb{R}$. Given that $f$ is continuous, what is the probability that it is differentiable? I suspect it is $0$.","Let $A$ be a nonempty open subset of $\mathbb{R}$. Consider a function $f : A \rightarrow \mathbb{R}$. Given that $f$ is continuous, what is the probability that it is differentiable? I suspect it is $0$.",,"['probability', 'measure-theory', 'derivatives', 'continuity']"
72,Rendering the derivative of composite functions from a graph,Rendering the derivative of composite functions from a graph,,"I'm on a workbook problem and I want to make sure I'm doing it properly. The problem asks me to find the derivatives of composite functions when given only the graphs of the original functions, here are my steps: I am supposed to find the derivative of $A'(1)$ where $A$ is a composite function in the form of $F(G(X))$. I start off by rendering the derivative of the composite, which renders as: $$f'(g(1)) * g'(1)$$ I look at the graph and find that $g(1)$ equals $3$. $$f'(3) * g'(1)$$ Next I look at the slope of the tangent at $f(3)$ and estimate it to be at $-\frac{1}{4}$. $$-\frac{1}{4}*g'(1)$$ Next I look at the slope of the tangent at $g(1)$ and estimate it to be $-3$. $$-\frac{1}{4}*-3 = \frac{3}{4}$$ So have I reached the right conclusion?","I'm on a workbook problem and I want to make sure I'm doing it properly. The problem asks me to find the derivatives of composite functions when given only the graphs of the original functions, here are my steps: I am supposed to find the derivative of $A'(1)$ where $A$ is a composite function in the form of $F(G(X))$. I start off by rendering the derivative of the composite, which renders as: $$f'(g(1)) * g'(1)$$ I look at the graph and find that $g(1)$ equals $3$. $$f'(3) * g'(1)$$ Next I look at the slope of the tangent at $f(3)$ and estimate it to be at $-\frac{1}{4}$. $$-\frac{1}{4}*g'(1)$$ Next I look at the slope of the tangent at $g(1)$ and estimate it to be $-3$. $$-\frac{1}{4}*-3 = \frac{3}{4}$$ So have I reached the right conclusion?",,"['calculus', 'algebra-precalculus', 'derivatives', 'function-and-relation-composition']"
73,Proving $f'(x)<0$ using sequential criterion of limit.,Proving  using sequential criterion of limit.,f'(x)<0,"I'm trying to prove the following: Let $f:\mathbb R\rightarrow\mathbb R$ be a function twice differentiable such that $\forall x\in \mathbb R , f(x)>0$ $\forall x\in \mathbb R , f''(x)>0$ $\lim_{x\to +\infty}f(x)=0$ Prove that for all $M\in \mathbb R$ there exists $x>M$ such that $f'(x)<0$ The only idea I can come up with is by contradiction. Suppose that there exists $M\in \mathbb R$ such that for all $x>M$, $f'(x)\geq 0$. This implies that $f(x)$ is increasing. Let $x_n$ be a sequence such that $(x_n)\to\infty$. Then $f(x_n)$  is increasing. There are two possibilities: $f(x_n)$ is bounded or is not bounded. If $f(x_n)$ is bounded, then $f(x_n)$ converges to some real number greater than $0$ (it can't converge to $0$ because $f(x_n)$ is increasing and $f(x_n)>0$ by 1.)  wich is a contradiction with 3. If $f(x_n)$ is not bounded, then $f(x_n)\rightarrow +\infty$ which is a contradiction with 3. I would like to know if the ideas are correct and if I could approach this other ways. Thank you in advance!","I'm trying to prove the following: Let $f:\mathbb R\rightarrow\mathbb R$ be a function twice differentiable such that $\forall x\in \mathbb R , f(x)>0$ $\forall x\in \mathbb R , f''(x)>0$ $\lim_{x\to +\infty}f(x)=0$ Prove that for all $M\in \mathbb R$ there exists $x>M$ such that $f'(x)<0$ The only idea I can come up with is by contradiction. Suppose that there exists $M\in \mathbb R$ such that for all $x>M$, $f'(x)\geq 0$. This implies that $f(x)$ is increasing. Let $x_n$ be a sequence such that $(x_n)\to\infty$. Then $f(x_n)$  is increasing. There are two possibilities: $f(x_n)$ is bounded or is not bounded. If $f(x_n)$ is bounded, then $f(x_n)$ converges to some real number greater than $0$ (it can't converge to $0$ because $f(x_n)$ is increasing and $f(x_n)>0$ by 1.)  wich is a contradiction with 3. If $f(x_n)$ is not bounded, then $f(x_n)\rightarrow +\infty$ which is a contradiction with 3. I would like to know if the ideas are correct and if I could approach this other ways. Thank you in advance!",,"['real-analysis', 'derivatives']"
74,"Strange things on WolframAlpha: derivation, modulo and doubling result","Strange things on WolframAlpha: derivation, modulo and doubling result",,"I asked WA what is the derivative of $\frac1{\cos((x \bmod \pi/2)-\pi/4))}$ equal to for $x=0$. A very strange result came out. The exact result is $-\sqrt2 \mathsf{Mod}^{(1,0)}(0,\frac\pi2)$, clicking on Approximate form shows cca. 79. This number was not what I expected so I clicked on More digits and the number doubled! Now it was about 316! ( Note: that's not a factorial ) The result doubles every time I click on More digits , up to cca. 1240. Link to WA: http://goo.gl/ntJ774 What's the meaning of $\mathsf{Mod}^{(1,0)}$ and is this behaviour correct?","I asked WA what is the derivative of $\frac1{\cos((x \bmod \pi/2)-\pi/4))}$ equal to for $x=0$. A very strange result came out. The exact result is $-\sqrt2 \mathsf{Mod}^{(1,0)}(0,\frac\pi2)$, clicking on Approximate form shows cca. 79. This number was not what I expected so I clicked on More digits and the number doubled! Now it was about 316! ( Note: that's not a factorial ) The result doubles every time I click on More digits , up to cca. 1240. Link to WA: http://goo.gl/ntJ774 What's the meaning of $\mathsf{Mod}^{(1,0)}$ and is this behaviour correct?",,"['derivatives', 'modular-arithmetic', 'wolfram-alpha']"
75,Zero points of derivatives,Zero points of derivatives,,"It's obvious that if $f(x)$ is a polynomial then it's derivatives $f^{(n)}$ are equal to zero  for $n>\deg f$. I'm trying to prove the ""inverse"" statement: if for each $x\in\mathbb{R}$ there exist $n\in\mathbb{R}$ such that $f^{(n)}(x)=0$ then $f$ is a polynomial. I've already proved using Baire theorem that there exist $[a,b]\subset\mathbb{R}$ and $i\in\mathbb{N}$ such that $f^{(i)}(x)=0$ for every $x\in[a,b]$, but I have no idea how to go further.","It's obvious that if $f(x)$ is a polynomial then it's derivatives $f^{(n)}$ are equal to zero  for $n>\deg f$. I'm trying to prove the ""inverse"" statement: if for each $x\in\mathbb{R}$ there exist $n\in\mathbb{R}$ such that $f^{(n)}(x)=0$ then $f$ is a polynomial. I've already proved using Baire theorem that there exist $[a,b]\subset\mathbb{R}$ and $i\in\mathbb{N}$ such that $f^{(i)}(x)=0$ for every $x\in[a,b]$, but I have no idea how to go further.",,"['real-analysis', 'derivatives']"
76,Proof regarding derivatives and Mean Value Theorem.,Proof regarding derivatives and Mean Value Theorem.,,"Original question: $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Show that for any $c \in (a,b)$ that is not a point of maximum or minimum for $f'$, there exist $x_1, x_2 \in (a,b)$ such that $f'(c)=\frac{f(x_2)-f(x_1)}{x_2 - x_1}$ What I have done: We argue by contradiction. Suppose there doesn't exist such $x_1$ and $x_2$, then $\forall x_1,x_2 \in (a,b)$, we have $f'(c)=\frac{f(x_2)-f(x_1)}{x_2 - x_1}$. By assumption, since $f$ is differentiable on $(a,b)$, then according to Mean Value Theorem, $\exists k \in (a,b)$ such that $f'(k)=\frac{f(x_2)-f(x_1)}{x_2 - x_1}$. Since $f'(c) \ne f'(k)$, we must have either $f'(c) < f'(k)$ or $f'(c) > f'(x)$. But we know that $c$ is not a point for maximum or minimum for $f'$, so this is a contradiction. My concern: Is it possible that we are missing some of the $k$ in $(a,b)$? Because if that's the case, then we wouldn't end up with a contradiction any more. I'm thinking since the choise of $x_1, x_2$ are arbitrary and since $k$ must lie between them, so we should be able to capture all the $k$'s, but how should I formalize that?","Original question: $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Show that for any $c \in (a,b)$ that is not a point of maximum or minimum for $f'$, there exist $x_1, x_2 \in (a,b)$ such that $f'(c)=\frac{f(x_2)-f(x_1)}{x_2 - x_1}$ What I have done: We argue by contradiction. Suppose there doesn't exist such $x_1$ and $x_2$, then $\forall x_1,x_2 \in (a,b)$, we have $f'(c)=\frac{f(x_2)-f(x_1)}{x_2 - x_1}$. By assumption, since $f$ is differentiable on $(a,b)$, then according to Mean Value Theorem, $\exists k \in (a,b)$ such that $f'(k)=\frac{f(x_2)-f(x_1)}{x_2 - x_1}$. Since $f'(c) \ne f'(k)$, we must have either $f'(c) < f'(k)$ or $f'(c) > f'(x)$. But we know that $c$ is not a point for maximum or minimum for $f'$, so this is a contradiction. My concern: Is it possible that we are missing some of the $k$ in $(a,b)$? Because if that's the case, then we wouldn't end up with a contradiction any more. I'm thinking since the choise of $x_1, x_2$ are arbitrary and since $k$ must lie between them, so we should be able to capture all the $k$'s, but how should I formalize that?",,"['calculus', 'real-analysis', 'derivatives', 'proof-writing', 'proof-verification']"
77,Is little-o preserved under integration and derivation of another variable?,Is little-o preserved under integration and derivation of another variable?,,"Given an integrable function $g:\mathbb{R}\longrightarrow\mathbb{R}$, and a function $f:\mathbb{R}^2\longrightarrow\mathbb{R}$ such that $$f(x,y)=o(x^{-1})\;\;\;\;\;\;\text{when }x\rightarrow\infty,$$ i.e. $\lim_{x\rightarrow\infty} xf(x,y)=0$ for all $y\in\mathbb{R}$. Is it true that then $$\int f(x,y)g(y)dy=o(x^{-1})\;\;\;\;\;\;\text{ when   }x\rightarrow\infty?$$ We know that $f(x,y)g(y)=o(x^{-1})$ and that $o(x^{-1})+o(x^{-1})=o(x^{-1})$, so intuitively it should go through. But when I try to prove it, I need to interchange the limits and the integral and I am not sure I can do it: $$\lim_{x\rightarrow\infty} x\int f(x,y)g(y)dy=\lim_{x\rightarrow\infty}\int xf(x,y)g(y)dy=\int \lim_{x\rightarrow\infty}f(x,y)g(y)dy=0$$ and I am not sure about the second equality here. Same question applies to derivatives. If $f:\mathbb{R}^2\longrightarrow\mathbb{R}$ such that $f(x,y)=o(x^{-1})$, can we say that $$\frac{\partial}{\partial y}f(x,y)=o(x^{-1})$$?","Given an integrable function $g:\mathbb{R}\longrightarrow\mathbb{R}$, and a function $f:\mathbb{R}^2\longrightarrow\mathbb{R}$ such that $$f(x,y)=o(x^{-1})\;\;\;\;\;\;\text{when }x\rightarrow\infty,$$ i.e. $\lim_{x\rightarrow\infty} xf(x,y)=0$ for all $y\in\mathbb{R}$. Is it true that then $$\int f(x,y)g(y)dy=o(x^{-1})\;\;\;\;\;\;\text{ when   }x\rightarrow\infty?$$ We know that $f(x,y)g(y)=o(x^{-1})$ and that $o(x^{-1})+o(x^{-1})=o(x^{-1})$, so intuitively it should go through. But when I try to prove it, I need to interchange the limits and the integral and I am not sure I can do it: $$\lim_{x\rightarrow\infty} x\int f(x,y)g(y)dy=\lim_{x\rightarrow\infty}\int xf(x,y)g(y)dy=\int \lim_{x\rightarrow\infty}f(x,y)g(y)dy=0$$ and I am not sure about the second equality here. Same question applies to derivatives. If $f:\mathbb{R}^2\longrightarrow\mathbb{R}$ such that $f(x,y)=o(x^{-1})$, can we say that $$\frac{\partial}{\partial y}f(x,y)=o(x^{-1})$$?",,"['integration', 'limits', 'derivatives']"
78,Integration over time by having derivation,Integration over time by having derivation,,Assume we want to find the following integration: \begin{equation}\int_{t=0}^{\infty} p(t)dt\end{equation} where $p(0)=p$ and also $$\frac{dp(t)}{dt}=-p(t)(1-p(t))\mu$$. Is there any easy way to exploit the second equation and calculate the first integration? Any comment is highly appreciated.,Assume we want to find the following integration: \begin{equation}\int_{t=0}^{\infty} p(t)dt\end{equation} where $p(0)=p$ and also $$\frac{dp(t)}{dt}=-p(t)(1-p(t))\mu$$. Is there any easy way to exploit the second equation and calculate the first integration? Any comment is highly appreciated.,,"['calculus', 'integration', 'derivatives']"
79,Interchange of limiting operations (question from an engineer),Interchange of limiting operations (question from an engineer),,"I need to clarify when are the below operations valid. If possible, please link me to the related theorems, where I can find details. 1- Given a double integral  \begin{equation} \int_{X}\int_Y f(x,y) \:\mathrm{d}y\:\mathrm{d}x \end{equation} Under what conditions on $f$ the interchange of the order of integration is valid so that \begin{equation} \int_{X}\int_Y f(x,y)\:\mathrm{d}y\:\mathrm{d}x = \int_{Y}\int_X f(x,y)\:\mathrm{d}x\:\mathrm{d}y \end{equation} A related question here, is when we have, instead of a double integral, one integral and infinite sum. For example  \begin{equation} \sum_{n=-\infty}^{+\infty} h(n) \int f(x)g(n) \:\mathrm{d}x \end{equation} where $n$ is integer, and $x\in \mathbb{R}$. When is the following valid? \begin{equation} \sum_{n=-\infty}^{+\infty} h(n) \int_X f(x)g(n) \:\mathrm{d}x = \int_X  f(x) \left(\sum_{n=-\infty}^{+\infty} h(n)g(n)\right) \:\mathrm{d}x \end{equation} 2- Under what conditions on the integrand, the interchange of the derivative and integral signs is vaild. For example,  when can we say that: \begin{equation} \frac{\partial}{\partial t} \left(\iint_S u(x,y,t) \:\mathrm{d}x \:\mathrm{d}y\right)= \iint_S \frac{\partial}{\partial t}u(x,y,t) \:\mathrm{d}x \:\mathrm{d}y \end{equation}","I need to clarify when are the below operations valid. If possible, please link me to the related theorems, where I can find details. 1- Given a double integral  \begin{equation} \int_{X}\int_Y f(x,y) \:\mathrm{d}y\:\mathrm{d}x \end{equation} Under what conditions on $f$ the interchange of the order of integration is valid so that \begin{equation} \int_{X}\int_Y f(x,y)\:\mathrm{d}y\:\mathrm{d}x = \int_{Y}\int_X f(x,y)\:\mathrm{d}x\:\mathrm{d}y \end{equation} A related question here, is when we have, instead of a double integral, one integral and infinite sum. For example  \begin{equation} \sum_{n=-\infty}^{+\infty} h(n) \int f(x)g(n) \:\mathrm{d}x \end{equation} where $n$ is integer, and $x\in \mathbb{R}$. When is the following valid? \begin{equation} \sum_{n=-\infty}^{+\infty} h(n) \int_X f(x)g(n) \:\mathrm{d}x = \int_X  f(x) \left(\sum_{n=-\infty}^{+\infty} h(n)g(n)\right) \:\mathrm{d}x \end{equation} 2- Under what conditions on the integrand, the interchange of the derivative and integral signs is vaild. For example,  when can we say that: \begin{equation} \frac{\partial}{\partial t} \left(\iint_S u(x,y,t) \:\mathrm{d}x \:\mathrm{d}y\right)= \iint_S \frac{\partial}{\partial t}u(x,y,t) \:\mathrm{d}x \:\mathrm{d}y \end{equation}",,"['limits', 'derivatives', 'summation']"
80,Problem with differentiable function: is it concave up when the derivative is increasing?,Problem with differentiable function: is it concave up when the derivative is increasing?,,"This makes sense to me, and I feel like it would be an easy argument IF I could use the second derivative. I'm only given that f is differentiable, NOT twice differentiable. Any help?","This makes sense to me, and I feel like it would be an easy argument IF I could use the second derivative. I'm only given that f is differentiable, NOT twice differentiable. Any help?",,"['calculus', 'real-analysis', 'derivatives']"
81,Formula for nth derivative of $\arcsin^k(x/2)$,Formula for nth derivative of,\arcsin^k(x/2),"I need to find formula for $n$-th derivative of $\arcsin^k(\frac{x}{2})$. I have found formula for $$\left(\arcsin\frac{x}{2}\right)^{(n)}=\frac{(-i)^{n-1}(n-1)!}{\left(4-z^2\right)^{n/2}}P_{n-1}\left(\frac{i z}{\sqrt{4-z^2}}\right)$$ Where $P_n$ - Legendre polynomial. I have found on the net formula for $n$-th derivative of composite function, however it didn't help. How can I find required formula?","I need to find formula for $n$-th derivative of $\arcsin^k(\frac{x}{2})$. I have found formula for $$\left(\arcsin\frac{x}{2}\right)^{(n)}=\frac{(-i)^{n-1}(n-1)!}{\left(4-z^2\right)^{n/2}}P_{n-1}\left(\frac{i z}{\sqrt{4-z^2}}\right)$$ Where $P_n$ - Legendre polynomial. I have found on the net formula for $n$-th derivative of composite function, however it didn't help. How can I find required formula?",,"['trigonometry', 'derivatives']"
82,"If a positive decreasing function satisfies $f(0)=1$ and $f''\le f$ on $[0,1)$, then $f'(0)\ge-\sqrt{2}$","If a positive decreasing function satisfies  and  on , then","f(0)=1 f''\le f [0,1) f'(0)\ge-\sqrt{2}","Let, $f(x)$ be a twice differentiable function defined on $(-1, 1)$ and $f(0) = 1$. Let, $f(x) ≥ 0$, $f'(x) ≤ 0$ and $f''(x) ≤ f(x)$ for all $x ≥ 0$. Show that, $f'(0) ≥ -√2$. I am telling you what I did. First, I noted that $-√2$ is somehow a bit odd to suddenly appear in such a situation, but then realized that $-√2$ is the minimum value of a function of the form $(sinx + cosx)$ ! This realization caused me to think of a function $f(x)$ such that it is decreasing (as $f'(x) ≤ 0$), and trigonometric. And, the words ""twice differentiable"" excited me: we know that a trigonometric sine or cosine function gives back somewhat the negative of the original function on twice differentiation. So, by careful choice, I selected the function $f(x) = \cos p√2x - \sin p√2x$, where $p$ is a real number belonging to $[0, 1]$. Then, $f'(x) = -p√2(\sin p√2x + \cos p√2x)$, which is clearly $≤ 0$ (and hence satisfies the problem.) We also find that $f'(0) = -p√2$ and as $1 ≥ p ≥ 0$, $-√2 ≤ p√2 ≤ 0$ (hence shown!) You may note that for this function, $f''(x) = -2p²f(x) ≤ f(x)$, equality holding if $p = 0$ or $f(x) = 0$. Now, is my method correct? I, actually, do not know how to work with these inequalities in derivatives so I just used my intuition to get this function and hopefully it should work!","Let, $f(x)$ be a twice differentiable function defined on $(-1, 1)$ and $f(0) = 1$. Let, $f(x) ≥ 0$, $f'(x) ≤ 0$ and $f''(x) ≤ f(x)$ for all $x ≥ 0$. Show that, $f'(0) ≥ -√2$. I am telling you what I did. First, I noted that $-√2$ is somehow a bit odd to suddenly appear in such a situation, but then realized that $-√2$ is the minimum value of a function of the form $(sinx + cosx)$ ! This realization caused me to think of a function $f(x)$ such that it is decreasing (as $f'(x) ≤ 0$), and trigonometric. And, the words ""twice differentiable"" excited me: we know that a trigonometric sine or cosine function gives back somewhat the negative of the original function on twice differentiation. So, by careful choice, I selected the function $f(x) = \cos p√2x - \sin p√2x$, where $p$ is a real number belonging to $[0, 1]$. Then, $f'(x) = -p√2(\sin p√2x + \cos p√2x)$, which is clearly $≤ 0$ (and hence satisfies the problem.) We also find that $f'(0) = -p√2$ and as $1 ≥ p ≥ 0$, $-√2 ≤ p√2 ≤ 0$ (hence shown!) You may note that for this function, $f''(x) = -2p²f(x) ≤ f(x)$, equality holding if $p = 0$ or $f(x) = 0$. Now, is my method correct? I, actually, do not know how to work with these inequalities in derivatives so I just used my intuition to get this function and hopefully it should work!",,"['calculus', 'real-analysis', 'inequality', 'derivatives']"
83,What is the meaning of the third derivative of a function at a point,What is the meaning of the third derivative of a function at a point,,"(Originally asked on MO by AJAY.) What is the geometric, physical, or other meaning of the third derivative of a function at a point? If you have interesting things to say about the meaning of the first and second derivatives, please do so.","(Originally asked on MO by AJAY.) What is the geometric, physical, or other meaning of the third derivative of a function at a point? If you have interesting things to say about the meaning of the first and second derivatives, please do so.",,"['calculus', 'geometry', 'physics', 'education']"
84,"Gram-Charlier expansion, option price, higher derivative and integration by parts","Gram-Charlier expansion, option price, higher derivative and integration by parts",,"I am currently reading a finance paper of Backus et al. (2004), called 'Accounting for biases in Black-Scholes'. To explain an abnormality called 'volatility smirk' that can be found in option prices, the authors use a Gram-charlier expansion of normal density whose function is defined as $$ f(w)=\left(1-\frac{\gamma_{1}}{3!}D^{3}+\frac{\gamma_{2}}{4!}D^{4}\right)\phi(w), $$ where $\gamma_{1}$ is skewness of the density, $\gamma_{2}$ is kurtosis of the density, $\phi(w)=(2\pi)^{1/2}\exp(-w^{2}/2)$ and $w$ stands for the one-period log-return $\log S_{t+1}-\log S_{t}$. As you may already know, a price of European call option $C(S(t),K,t,\tau)$ with underlying price $S(t)$, stirke price $K$, current time $t$ and time to maturity $\tau$ can be defined as $$ C(S,K,\tau)=e^{-r\tau}E(S(t+\tau)-K)^{+} \\ = e^{-r\tau}\int_{\log(K/S(t))}^{\infty}(S(t)e^{w}-K)p(w)dw, $$ where $p$ is the probability density function for the underlying asset price which is random. This paper says that given the distribution $f$ above, the integral part of call price equation becomes $$ \int_{\log(K/S(t))}^{\infty}(S(t)e^{w}-K)f(w)dw=\int_{w^{*}}^{\infty}(S(t)e^{\mu+\sigma w}-K)\phi(w)dw \\ -\frac{\gamma_{1}}{3!}\int_{w^{*}}^{\infty}(S(t)e^{\mu+\sigma w}-K)\phi^{'''}(w)dw \\ +\frac{\gamma_{2}}{4!}\int_{w^{*}}^{\infty}(S(t)e^{\mu+\sigma w}-K)\phi^{(4)}(w)dw \\ \equiv I_{1}-\frac{\gamma_{1}}{3!}I_{2}+\frac{\gamma_{1}}{4!}I_{3}, $$ where $w^{*}=\log(K/S_{t}-\mu)/\sigma$, $\mu$ is the expected rate of return, and $\sigma$ is the volatility of underlying asset price. My question is about the derivation of $I_{3}$. In this paper, authors say that $$ I_{3}=\sigma K\phi(w^{*})[(w^{*})^{2}-1+\sigma w^{*}+\sigma^{2}]+\sigma^{4}I_{1}+\sigma^{4}K\Phi(-w^{*}), $$ where $\Phi$ is the cumulative density function for $\phi$. It is mentioned that repeated application of integration by parts and a property of derivatives of the normal density $\lim_{x\rightarrow\infty}e^{x}\phi^{(n)}(x)=0$ are used for this derivation. Now I can derive all the other terms, but I still cannot find where the minus one in the middle comes from. I would be really appreciated if someone could tell me where it comes from. The original paper can be found at http://faculty.baruch.cuny.edu/lwu/papers/bias.pdf . This question is related to the proof of proposition 1, which can be found on page 23 and 24.","I am currently reading a finance paper of Backus et al. (2004), called 'Accounting for biases in Black-Scholes'. To explain an abnormality called 'volatility smirk' that can be found in option prices, the authors use a Gram-charlier expansion of normal density whose function is defined as $$ f(w)=\left(1-\frac{\gamma_{1}}{3!}D^{3}+\frac{\gamma_{2}}{4!}D^{4}\right)\phi(w), $$ where $\gamma_{1}$ is skewness of the density, $\gamma_{2}$ is kurtosis of the density, $\phi(w)=(2\pi)^{1/2}\exp(-w^{2}/2)$ and $w$ stands for the one-period log-return $\log S_{t+1}-\log S_{t}$. As you may already know, a price of European call option $C(S(t),K,t,\tau)$ with underlying price $S(t)$, stirke price $K$, current time $t$ and time to maturity $\tau$ can be defined as $$ C(S,K,\tau)=e^{-r\tau}E(S(t+\tau)-K)^{+} \\ = e^{-r\tau}\int_{\log(K/S(t))}^{\infty}(S(t)e^{w}-K)p(w)dw, $$ where $p$ is the probability density function for the underlying asset price which is random. This paper says that given the distribution $f$ above, the integral part of call price equation becomes $$ \int_{\log(K/S(t))}^{\infty}(S(t)e^{w}-K)f(w)dw=\int_{w^{*}}^{\infty}(S(t)e^{\mu+\sigma w}-K)\phi(w)dw \\ -\frac{\gamma_{1}}{3!}\int_{w^{*}}^{\infty}(S(t)e^{\mu+\sigma w}-K)\phi^{'''}(w)dw \\ +\frac{\gamma_{2}}{4!}\int_{w^{*}}^{\infty}(S(t)e^{\mu+\sigma w}-K)\phi^{(4)}(w)dw \\ \equiv I_{1}-\frac{\gamma_{1}}{3!}I_{2}+\frac{\gamma_{1}}{4!}I_{3}, $$ where $w^{*}=\log(K/S_{t}-\mu)/\sigma$, $\mu$ is the expected rate of return, and $\sigma$ is the volatility of underlying asset price. My question is about the derivation of $I_{3}$. In this paper, authors say that $$ I_{3}=\sigma K\phi(w^{*})[(w^{*})^{2}-1+\sigma w^{*}+\sigma^{2}]+\sigma^{4}I_{1}+\sigma^{4}K\Phi(-w^{*}), $$ where $\Phi$ is the cumulative density function for $\phi$. It is mentioned that repeated application of integration by parts and a property of derivatives of the normal density $\lim_{x\rightarrow\infty}e^{x}\phi^{(n)}(x)=0$ are used for this derivation. Now I can derive all the other terms, but I still cannot find where the minus one in the middle comes from. I would be really appreciated if someone could tell me where it comes from. The original paper can be found at http://faculty.baruch.cuny.edu/lwu/papers/bias.pdf . This question is related to the proof of proposition 1, which can be found on page 23 and 24.",,"['integration', 'probability-distributions', 'derivatives']"
85,Calculate Dq(x),Calculate Dq(x),,"Let A be a symmetric $m \times m$ matrix, and $q(x)=x\cdot Ax$ a quadratic form on $\mathbb{R}^m$. Question: Calculate $Dq(x)$; write your answer in vector notation. Does anyone knows the answer on this question? I don't know what they mean with a quadratic form on $\mathbb{R}^m$. I know that if $A$ is the unit matrix, $q(x)$ would look like $x_{1}^{2}+\dots +x_{m}^{2}$ but i don't know if $A$ is an unit matrix.","Let A be a symmetric $m \times m$ matrix, and $q(x)=x\cdot Ax$ a quadratic form on $\mathbb{R}^m$. Question: Calculate $Dq(x)$; write your answer in vector notation. Does anyone knows the answer on this question? I don't know what they mean with a quadratic form on $\mathbb{R}^m$. I know that if $A$ is the unit matrix, $q(x)$ would look like $x_{1}^{2}+\dots +x_{m}^{2}$ but i don't know if $A$ is an unit matrix.",,"['derivatives', 'convex-analysis']"
86,How to prove that a complex power series is differentiable,How to prove that a complex power series is differentiable,,I am always using the following result but I do not know why it is true. So: How to prove the following statement: Suppose the complex power series $\sum_{n = 0}^\infty a_n(z-z_0)^n$ has radius of convergence $R > 0$. Then the function $f: B_R(z_0) \to \mathbb C$ defined by \begin{align*} f(z) := \sum_{n = 0}^\infty a_n (z-z_0)^n \end{align*} is differentiable in $B_R(z_0)$ and for any $z \in B_R(z_0)$ the derivative is given by the formula \begin{align*} f'(z) = \sum_{n = 1}^\infty na_n(z-z_0)^{n-1}. \end{align*} Thanks in advance for explanations.,I am always using the following result but I do not know why it is true. So: How to prove the following statement: Suppose the complex power series $\sum_{n = 0}^\infty a_n(z-z_0)^n$ has radius of convergence $R > 0$. Then the function $f: B_R(z_0) \to \mathbb C$ defined by \begin{align*} f(z) := \sum_{n = 0}^\infty a_n (z-z_0)^n \end{align*} is differentiable in $B_R(z_0)$ and for any $z \in B_R(z_0)$ the derivative is given by the formula \begin{align*} f'(z) = \sum_{n = 1}^\infty na_n(z-z_0)^{n-1}. \end{align*} Thanks in advance for explanations.,,"['complex-analysis', 'derivatives', 'power-series', 'taylor-expansion']"
87,Strictly monotonic increasing function,Strictly monotonic increasing function,,"Suppose that $f$ is continuously differentiable on $[a,b]$ and $f'(x) > 0$ for all $x$. Prove that $f$ is strictly monotonic increasing on $[a,b]$; that is, if $x<y$, then $f(x) < f(y)$. My Approach. Since $f$ is continuously differentiable on $[a,b]$, if $a \leq x < y \leq b$, then according to Mean Value Theorem in the interval $[x,y]$ there is a point $c$ satisfying $x<c<y$ such that $$ \frac{f(y) - f(x)}{y - x} = f'(c) > 0 $$ $$ \Rightarrow f(x) < f(y)$$ But I was wondering what is the necessity of continuously differentiability. Seems that only differentiability would work too. Where wrong am I?","Suppose that $f$ is continuously differentiable on $[a,b]$ and $f'(x) > 0$ for all $x$. Prove that $f$ is strictly monotonic increasing on $[a,b]$; that is, if $x<y$, then $f(x) < f(y)$. My Approach. Since $f$ is continuously differentiable on $[a,b]$, if $a \leq x < y \leq b$, then according to Mean Value Theorem in the interval $[x,y]$ there is a point $c$ satisfying $x<c<y$ such that $$ \frac{f(y) - f(x)}{y - x} = f'(c) > 0 $$ $$ \Rightarrow f(x) < f(y)$$ But I was wondering what is the necessity of continuously differentiability. Seems that only differentiability would work too. Where wrong am I?",,"['calculus', 'derivatives']"
88,derivative of limit function vs limit of derivatives,derivative of limit function vs limit of derivatives,,"Suppose that we have a sequence of differentiable functions $f_n:\mathbb{R} \rightarrow \mathbb{R}$ such that $f_n$ converges to some function $f$. Then it is not necessary that the sequence of derivatives $f_n'$ converges. For example, if $$f_n(x):=\frac{\sin(nx)}{n}$$ then $f_n$ converges (uniformly) to constant function $0$ but $f_n'$ diverges almost everywhere. But what if we assume that the sequence $f_n'$ does converge for all $x$ and that $f$ is differentiable. Is it then the case that $f_n'$ converges to $f'$ for (almost) all $x$? If not, are there any other reasonable conditions (like uniform convergence) that we can add to make this work?","Suppose that we have a sequence of differentiable functions $f_n:\mathbb{R} \rightarrow \mathbb{R}$ such that $f_n$ converges to some function $f$. Then it is not necessary that the sequence of derivatives $f_n'$ converges. For example, if $$f_n(x):=\frac{\sin(nx)}{n}$$ then $f_n$ converges (uniformly) to constant function $0$ but $f_n'$ diverges almost everywhere. But what if we assume that the sequence $f_n'$ does converge for all $x$ and that $f$ is differentiable. Is it then the case that $f_n'$ converges to $f'$ for (almost) all $x$? If not, are there any other reasonable conditions (like uniform convergence) that we can add to make this work?",,"['real-analysis', 'derivatives', 'convergence-divergence']"
89,Solving optimization problems using derivatives and critical points,Solving optimization problems using derivatives and critical points,,"I have a homework question which I have completed 2/3 of; however I am stuck on the last part of the question. The question is: A drug used to treat cancer is effective at low doses with an efficacy that increases with the quantity of the drug. However, at sufficiently high doses, the drug becomes lethal. For positive values of the constants k_1 and k_2, the fraction of patients surviving cancer with this drug treatment is given by   $$S(q)=\frac{q^2}{k_1^2+q^2}-\frac{q^2}{k_2^2+q^2}.$$ There are three parts to the question. I have answered the first two parts, but I don't know how to approach the last part, which in this case is question 3. 1) Find $S(0)$ and $\lim_{q\to\infty}S(q)$ and in each case explain what your findings mean in medical terms. For part 1 I made $s(q)$ equal to $0$, which gave me $$\frac 0 {k_1^2} - \frac 0 {k_2^2} = 0-0 = 0.$$ This indicates that when $q$ is equal to $0$, no patients have survived. I also used the limits for when $q$ is equal to $+\infty$; this gave me $(1-1)$, which is also equal to $0$. Therefore the conclusion here is that when $q$ is very high, hence infinity, the patients will not survive either. 2) What is the optimal daily drug quantity to administer in terms of $k_1$ and $k_2$? For part two, I found the derivative of $s(q)$ and made is equal to $0$. So I found $s'(q)=0$. This was a very long process which will take me forever to type here, however my final answer here was $q=\sqrt{k_2k_1}$, where $k_2$ and $k_1$ are constants. I am not sure if this is correct, but it looks pretty right to me. 3) Suppose Health Canada has only approved the use of the drug of up to 45 mg/day and suppose $k_1=25 \mathrm{mg}/\mathrm{day}$ is the same for all patients but $k_2$ varies from patient to patient. To determine a personalized treatment strategy it would be useful for physicians to have a plot of the optimal daily drug quantity as a function of $k_2$, call it $q∗(k_2)$. Sketch a plot of $q∗(k_2)$ and explain why you drew it that way. Hint: don't forget that $k_1<k_2$! Tthis is the question that I need help approaching, any one care to help please? Am I meant to find the critical points..? Isn't that what I found in Part two? The graph it? If $k_2$ is meant to be greater than $k_1$ and $k_1$ is 25mg and overall it is 45mg, should $k_2$ be between 0 and 20mg? Is this how my graph should be?","I have a homework question which I have completed 2/3 of; however I am stuck on the last part of the question. The question is: A drug used to treat cancer is effective at low doses with an efficacy that increases with the quantity of the drug. However, at sufficiently high doses, the drug becomes lethal. For positive values of the constants k_1 and k_2, the fraction of patients surviving cancer with this drug treatment is given by   $$S(q)=\frac{q^2}{k_1^2+q^2}-\frac{q^2}{k_2^2+q^2}.$$ There are three parts to the question. I have answered the first two parts, but I don't know how to approach the last part, which in this case is question 3. 1) Find $S(0)$ and $\lim_{q\to\infty}S(q)$ and in each case explain what your findings mean in medical terms. For part 1 I made $s(q)$ equal to $0$, which gave me $$\frac 0 {k_1^2} - \frac 0 {k_2^2} = 0-0 = 0.$$ This indicates that when $q$ is equal to $0$, no patients have survived. I also used the limits for when $q$ is equal to $+\infty$; this gave me $(1-1)$, which is also equal to $0$. Therefore the conclusion here is that when $q$ is very high, hence infinity, the patients will not survive either. 2) What is the optimal daily drug quantity to administer in terms of $k_1$ and $k_2$? For part two, I found the derivative of $s(q)$ and made is equal to $0$. So I found $s'(q)=0$. This was a very long process which will take me forever to type here, however my final answer here was $q=\sqrt{k_2k_1}$, where $k_2$ and $k_1$ are constants. I am not sure if this is correct, but it looks pretty right to me. 3) Suppose Health Canada has only approved the use of the drug of up to 45 mg/day and suppose $k_1=25 \mathrm{mg}/\mathrm{day}$ is the same for all patients but $k_2$ varies from patient to patient. To determine a personalized treatment strategy it would be useful for physicians to have a plot of the optimal daily drug quantity as a function of $k_2$, call it $q∗(k_2)$. Sketch a plot of $q∗(k_2)$ and explain why you drew it that way. Hint: don't forget that $k_1<k_2$! Tthis is the question that I need help approaching, any one care to help please? Am I meant to find the critical points..? Isn't that what I found in Part two? The graph it? If $k_2$ is meant to be greater than $k_1$ and $k_1$ is 25mg and overall it is 45mg, should $k_2$ be between 0 and 20mg? Is this how my graph should be?",,"['calculus', 'optimization', 'derivatives', 'convex-optimization']"
90,Find all points where the tangent line has slope 1.,Find all points where the tangent line has slope 1.,,Let $f(x)=x-\cos(x)$. Find all points on the graph of $y=f(x)$ where the tangent line has slope 1.  (In each answer $n$ varies among all integers). So far I've used the Sum derivative rule for which I have $1+\sin(x)$. So do I put in 1 in for $x$ for sin$(x)$. Please Help!!,Let $f(x)=x-\cos(x)$. Find all points on the graph of $y=f(x)$ where the tangent line has slope 1.  (In each answer $n$ varies among all integers). So far I've used the Sum derivative rule for which I have $1+\sin(x)$. So do I put in 1 in for $x$ for sin$(x)$. Please Help!!,,"['calculus', 'derivatives']"
91,"Text with alternative definition of ""derivative""?","Text with alternative definition of ""derivative""?",,"Instantaneous rates of change are conventionally defined as limits of difference quotients. Rates of things moving at constant speed are definable without delicate issues.  If I pass someone moving at constant speed at a particular point in time, it doesn't mean I'm going faster at that instant than he is; it might only mean that for a short time before and after that instant I'm going faster than he is.  But it does mean that that instant I'm not going more slowly than he is.  A similar criterion tells me I'm not going faster than some other, slower things moving at constant speed.  The boundary between those whose speed I do not exceed and those whose speed does not exceed mine is my speed at that instant. More abstractly, if a line crosses a curve going from below it to above it as the independent variable increases, then at that point the curve is not steeper than the line; and similarly one can say that at that point the curve is not less steep than certain other lines.  The unique point on the boundary between those two sets of slopes is the slope of the curve at a point. I recall that somebody mentioned this characterization of slope of a curve at a point on stackexchange within the past year. Has a textbook been written that defines derivatives that way and gives proofs of the usual rule based on that definition?  If not, do those proofs appear in the literature or on the web or otherwise in public?","Instantaneous rates of change are conventionally defined as limits of difference quotients. Rates of things moving at constant speed are definable without delicate issues.  If I pass someone moving at constant speed at a particular point in time, it doesn't mean I'm going faster at that instant than he is; it might only mean that for a short time before and after that instant I'm going faster than he is.  But it does mean that that instant I'm not going more slowly than he is.  A similar criterion tells me I'm not going faster than some other, slower things moving at constant speed.  The boundary between those whose speed I do not exceed and those whose speed does not exceed mine is my speed at that instant. More abstractly, if a line crosses a curve going from below it to above it as the independent variable increases, then at that point the curve is not steeper than the line; and similarly one can say that at that point the curve is not less steep than certain other lines.  The unique point on the boundary between those two sets of slopes is the slope of the curve at a point. I recall that somebody mentioned this characterization of slope of a curve at a point on stackexchange within the past year. Has a textbook been written that defines derivatives that way and gives proofs of the usual rule based on that definition?  If not, do those proofs appear in the literature or on the web or otherwise in public?",,"['calculus', 'reference-request', 'derivatives']"
92,Proof of a method to find the points of maximum slope,Proof of a method to find the points of maximum slope,,"According to method described in a paper [1] if we want to find points of maximum slope in a signal $f(t)$, then one has to do following Convolve $f(t)$ with $g(t)$ where $g(t)=-cos(\omega t).(1/\sigma\sqrt{2.\pi}).e^{(-0.5.t^2/\sigma^2))}$ a modulated gaussian curve. Local maximas in $f(t)*g(t)$ will correspond to points with maximal slope in $f(t)$ Here is the simulation result done to check the veracity. Blue curve is sine wave ($f(t)$). Green curve is $g(t)$. Red curve is $f*g$. As one can see, points of maximal slope in $f(t)$ indeed do occur at maximas in $f*g$. Question How to mathematically show that local maximas in $f*g$ correspond to points of maximum slopes in $f$? I did this much analysis - Points with maximum slope will be points at which $df/dt$ will be maximum i.e. $d^2f/dt^2 = 0$ On the other side, $(f*g)(t) = \int_{-\inf}^{+\inf}g(\tau).f(t-\tau)d\tau$ I don't know how to write it in math notation but we have to prove that $t$ at which $d^2f/dt^2 = 0$ is same $t$ at which $d(f*g)(t)/dt = 0$ occurs. How to proceed? [1] Prasanna, SR Mahadeva, and B. Yegnanarayana. ""Detection of vowel onset point events using excitation information."" INTERSPEECH. 2005. Page 2, Right Column http://speech.iiit.ac.in/svlpubs/conference/MahadevaYegna2005.pdf 2 Matlab code for plotting >> n=1:160; >> g=(1/16*sqrt(2*pi))*exp(-0.5*((n.^2)/16^2)); >> plot(sin(n/5));hold on;plot(conv(sin(n/5),-g.*cos(n/10)),'r');plot(-g.*cos(n/10),'g');","According to method described in a paper [1] if we want to find points of maximum slope in a signal $f(t)$, then one has to do following Convolve $f(t)$ with $g(t)$ where $g(t)=-cos(\omega t).(1/\sigma\sqrt{2.\pi}).e^{(-0.5.t^2/\sigma^2))}$ a modulated gaussian curve. Local maximas in $f(t)*g(t)$ will correspond to points with maximal slope in $f(t)$ Here is the simulation result done to check the veracity. Blue curve is sine wave ($f(t)$). Green curve is $g(t)$. Red curve is $f*g$. As one can see, points of maximal slope in $f(t)$ indeed do occur at maximas in $f*g$. Question How to mathematically show that local maximas in $f*g$ correspond to points of maximum slopes in $f$? I did this much analysis - Points with maximum slope will be points at which $df/dt$ will be maximum i.e. $d^2f/dt^2 = 0$ On the other side, $(f*g)(t) = \int_{-\inf}^{+\inf}g(\tau).f(t-\tau)d\tau$ I don't know how to write it in math notation but we have to prove that $t$ at which $d^2f/dt^2 = 0$ is same $t$ at which $d(f*g)(t)/dt = 0$ occurs. How to proceed? [1] Prasanna, SR Mahadeva, and B. Yegnanarayana. ""Detection of vowel onset point events using excitation information."" INTERSPEECH. 2005. Page 2, Right Column http://speech.iiit.ac.in/svlpubs/conference/MahadevaYegna2005.pdf 2 Matlab code for plotting >> n=1:160; >> g=(1/16*sqrt(2*pi))*exp(-0.5*((n.^2)/16^2)); >> plot(sin(n/5));hold on;plot(conv(sin(n/5),-g.*cos(n/10)),'r');plot(-g.*cos(n/10),'g');",,"['integration', 'optimization', 'derivatives', 'convolution']"
93,Is this answer sufficient to prove? The question is related to second partial derivatives.,Is this answer sufficient to prove? The question is related to second partial derivatives.,,"Is this answer sufficient to prove ? Does there exist a notation mistake or else? Problem Suppose that the functions $\varphi: \mathbb R \rightarrow \mathbb R$ and $\psi: \mathbb R \rightarrow \mathbb R$ have continuous second partial derivatives. Define $f(x,y) = \varphi(x-y) + \psi (x + y), \quad \forall (x,y) \in \mathbb R^2.$ Prove that    $$ \frac {\partial^2f}{\partial x^2} - \frac {\partial^2f}{\partial y^2} = 0, \quad \forall (x,y) \in \mathbb R^2. $$ Solution \begin{align} \frac {\partial f}{\partial x} &= \varphi'(x-y) + \psi'(x+y) \\ \frac {\partial^2 f}{\partial x^2} &= \frac \partial{\partial x} \left( \frac {\partial f}{\partial x} \right ) = \varphi''(x-y) + \psi''(x+y)\\ \frac {\partial f}{\partial y} &= -\varphi'(x-y) + \psi'(x+y) \\ \frac {\partial^2 f}{\partial y^2} &= \frac \partial{\partial y} \left( \frac {\partial f}{\partial y} \right ) = \varphi''(x-y) + \psi''(x+y) \end{align} Then $$ \frac {\partial^2 f}{\partial x^2} - \frac {\partial^2 f}{\partial y^2} = \varphi''(x-y) + \psi''(x+y) - \left [ \varphi''(x-y) + \psi''(x+y)\right ] = 0\quad \blacksquare $$","Is this answer sufficient to prove ? Does there exist a notation mistake or else? Problem Suppose that the functions $\varphi: \mathbb R \rightarrow \mathbb R$ and $\psi: \mathbb R \rightarrow \mathbb R$ have continuous second partial derivatives. Define $f(x,y) = \varphi(x-y) + \psi (x + y), \quad \forall (x,y) \in \mathbb R^2.$ Prove that    $$ \frac {\partial^2f}{\partial x^2} - \frac {\partial^2f}{\partial y^2} = 0, \quad \forall (x,y) \in \mathbb R^2. $$ Solution \begin{align} \frac {\partial f}{\partial x} &= \varphi'(x-y) + \psi'(x+y) \\ \frac {\partial^2 f}{\partial x^2} &= \frac \partial{\partial x} \left( \frac {\partial f}{\partial x} \right ) = \varphi''(x-y) + \psi''(x+y)\\ \frac {\partial f}{\partial y} &= -\varphi'(x-y) + \psi'(x+y) \\ \frac {\partial^2 f}{\partial y^2} &= \frac \partial{\partial y} \left( \frac {\partial f}{\partial y} \right ) = \varphi''(x-y) + \psi''(x+y) \end{align} Then $$ \frac {\partial^2 f}{\partial x^2} - \frac {\partial^2 f}{\partial y^2} = \varphi''(x-y) + \psi''(x+y) - \left [ \varphi''(x-y) + \psi''(x+y)\right ] = 0\quad \blacksquare $$",,"['calculus', 'real-analysis', 'derivatives', 'solution-verification']"
94,Trying to find the lyapunov function,Trying to find the lyapunov function,,"I have the system that I want to show the global asymptotic stability of the origin $$\dot{x_1} = x_2 \\ \dot{x_2} = -g(k_1 x_1 + k_2 x _2) $$ where k1 and k2 are positive numbers. Also, $$g(y)y > 0 $$ for all $y\neq 0$ and $$\lim_{|y|\rightarrow \infty } \int_{0}^{y} g(\xi)d\xi = +\infty $$ With this information, I can see that I want to bring the derivative of my Lyapunov function to take form of $$\dot{V}=-(k_1 x_1+k_2 x_2)g(k_1 x_1 + k_2 x_2) \\$$ so that I can say the derivative is  negative definite, and that my Lyapunov function should take the combination of integral and quadratic form to have radially unboundedness.  I tried to back track from V_dot, just to realize it's not so feasible to get the original function from its partial derivatives. Second approach I tried is with $$V = \int_{0}^{x_1}g(\xi)d\xi + 0.5x_2^2$$  but it seems that its derivatives wouldn't tell me much info either. What would be a good next step to do to the candidate function?","I have the system that I want to show the global asymptotic stability of the origin $$\dot{x_1} = x_2 \\ \dot{x_2} = -g(k_1 x_1 + k_2 x _2) $$ where k1 and k2 are positive numbers. Also, $$g(y)y > 0 $$ for all $y\neq 0$ and $$\lim_{|y|\rightarrow \infty } \int_{0}^{y} g(\xi)d\xi = +\infty $$ With this information, I can see that I want to bring the derivative of my Lyapunov function to take form of $$\dot{V}=-(k_1 x_1+k_2 x_2)g(k_1 x_1 + k_2 x_2) \\$$ so that I can say the derivative is  negative definite, and that my Lyapunov function should take the combination of integral and quadratic form to have radially unboundedness.  I tried to back track from V_dot, just to realize it's not so feasible to get the original function from its partial derivatives. Second approach I tried is with $$V = \int_{0}^{x_1}g(\xi)d\xi + 0.5x_2^2$$  but it seems that its derivatives wouldn't tell me much info either. What would be a good next step to do to the candidate function?",,['derivatives']
95,Why is the subdifferential of norm of a matrix ||A|| defined like this?,Why is the subdifferential of norm of a matrix ||A|| defined like this?,,"I read in a paper called ""Characterization of the subdifferential of some matrix norms"" that it defines the subdifferential of the matrix norm like this: $$\partial ||A||=\{G \in R^{m\times n} : \forall B \in R^{m\times n}, ||B|| \geq ||A|| + \operatorname{Tr}[(B-A)^TG] \}$$ Can someone tell me why they define it like this? I really don't know the intuition behind it! Thanks!","I read in a paper called ""Characterization of the subdifferential of some matrix norms"" that it defines the subdifferential of the matrix norm like this: $$\partial ||A||=\{G \in R^{m\times n} : \forall B \in R^{m\times n}, ||B|| \geq ||A|| + \operatorname{Tr}[(B-A)^TG] \}$$ Can someone tell me why they define it like this? I really don't know the intuition behind it! Thanks!",,"['matrices', 'derivatives', 'normed-spaces']"
96,"Consider the correlation of two functions, what is the derivative of the result with respect to one of those functions?","Consider the correlation of two functions, what is the derivative of the result with respect to one of those functions?",,"I have a problem that comes up from time to time in signal processing applications. Let $f(x)\geq0\, \forall x$ and $g(x)$ be real functions with finite range and support. Let $I(f(x),g(x)) = \int_{-\infty}^{\infty} \text{d}x\, f(x+y)g(x)$. The variables $x$ and $y$ are just dummies on the same line so I think it is ok to abuse the notation a bit here. My question is, what is $\frac{\partial}{\partial f(x)}I$? From first principles (limit definition of a derivative and Riemann sum), the best I can determine is $\frac{\partial}{\partial f(x)}I = \int_{-\infty}^{\infty} \text{d}x\, w(x+y) g(x)$, where $w(x)$ is a window function with the same support as $f(x)$. This result jars with me, because from matrix calculus I know that $\frac{\partial}{\partial \textbf{b}} \textbf{A}\textbf{b} = \textbf{A}^T$, and since the integral is effectively a sum then by analogy I would expect that $\frac{\partial}{\partial f(x)}I = g(x+y)$. The first principles approach is shaky because it involves the interchange of two limits, of which which I'm not sure how to prove the validity. Unfortunately I'm not very well versed in the Lebesgue integral, would that make the problem clearer?","I have a problem that comes up from time to time in signal processing applications. Let $f(x)\geq0\, \forall x$ and $g(x)$ be real functions with finite range and support. Let $I(f(x),g(x)) = \int_{-\infty}^{\infty} \text{d}x\, f(x+y)g(x)$. The variables $x$ and $y$ are just dummies on the same line so I think it is ok to abuse the notation a bit here. My question is, what is $\frac{\partial}{\partial f(x)}I$? From first principles (limit definition of a derivative and Riemann sum), the best I can determine is $\frac{\partial}{\partial f(x)}I = \int_{-\infty}^{\infty} \text{d}x\, w(x+y) g(x)$, where $w(x)$ is a window function with the same support as $f(x)$. This result jars with me, because from matrix calculus I know that $\frac{\partial}{\partial \textbf{b}} \textbf{A}\textbf{b} = \textbf{A}^T$, and since the integral is effectively a sum then by analogy I would expect that $\frac{\partial}{\partial f(x)}I = g(x+y)$. The first principles approach is shaky because it involves the interchange of two limits, of which which I'm not sure how to prove the validity. Unfortunately I'm not very well versed in the Lebesgue integral, would that make the problem clearer?",,"['derivatives', 'convolution']"
97,Taking derivative below an integral,Taking derivative below an integral,,"I am trying to solve the following question : If $t>0$, then \begin{align*}     \int_{0}^{+\infty} e^{-tx} \; dx = \frac{1}{t} \end{align*} Moreover, if $t \geq a > 0$, then $e^{-tx} \leq e^{ax}$. Use this and Exercise 4.M. to justify differentiating under the integral sign and to obtain the formula \begin{align*}     \int_{0}^{+\infty} x^n e^{-x} \; dx = n! \end{align*} $\bf{Note:}$ Exercise 4.M. states that if $X = [0, \infty)$ and $\lambda$ is the Lebesgue measure and $f$ is a non-negative function on X, then $$\int f \; d\lambda = \lim_{b\to \infty} \int_0^b f \; d\lambda$$ I would like to use a Corollary of the Dominated Convergence Theorem,  $$\frac{d}{dt}\int f(x,t) \; d\mu(x) = \int \frac{\partial f}{\partial t}(x,t) \; d\mu(x)$$ However, for in order to use this I need to find an integrable function $g$ such that  $$\left|\frac{\partial f}{\partial t}(x,t)\right| \leq g(x) $$ I guess I am having difficulty finding this $g(x)$. If I start with $f(x,t) = e^{-tx}$ then, $$\left|\frac{\partial f}{\partial t}(x,t)\right| \leq xe^{-tx} \leq xe^{ax}$$ How would I show that this last term is in $L_1$? Moreover, I assume that I would have to iterate this process several times so what I am trying to bound should actually be $x^n e^{-tx}$. If so, how should I go about constructing my $g$? Also if anyone could redirect me to problems of this similar structure I would appreciate it, as I find them rather interesting. I am currently using Bartle's ""The Elements of Integration and Lebesgue Measure"".","I am trying to solve the following question : If $t>0$, then \begin{align*}     \int_{0}^{+\infty} e^{-tx} \; dx = \frac{1}{t} \end{align*} Moreover, if $t \geq a > 0$, then $e^{-tx} \leq e^{ax}$. Use this and Exercise 4.M. to justify differentiating under the integral sign and to obtain the formula \begin{align*}     \int_{0}^{+\infty} x^n e^{-x} \; dx = n! \end{align*} $\bf{Note:}$ Exercise 4.M. states that if $X = [0, \infty)$ and $\lambda$ is the Lebesgue measure and $f$ is a non-negative function on X, then $$\int f \; d\lambda = \lim_{b\to \infty} \int_0^b f \; d\lambda$$ I would like to use a Corollary of the Dominated Convergence Theorem,  $$\frac{d}{dt}\int f(x,t) \; d\mu(x) = \int \frac{\partial f}{\partial t}(x,t) \; d\mu(x)$$ However, for in order to use this I need to find an integrable function $g$ such that  $$\left|\frac{\partial f}{\partial t}(x,t)\right| \leq g(x) $$ I guess I am having difficulty finding this $g(x)$. If I start with $f(x,t) = e^{-tx}$ then, $$\left|\frac{\partial f}{\partial t}(x,t)\right| \leq xe^{-tx} \leq xe^{ax}$$ How would I show that this last term is in $L_1$? Moreover, I assume that I would have to iterate this process several times so what I am trying to bound should actually be $x^n e^{-tx}$. If so, how should I go about constructing my $g$? Also if anyone could redirect me to problems of this similar structure I would appreciate it, as I find them rather interesting. I am currently using Bartle's ""The Elements of Integration and Lebesgue Measure"".",,"['real-analysis', 'measure-theory', 'integration', 'derivatives']"
98,Question about smooth functions and their signs with given initial conditions,Question about smooth functions and their signs with given initial conditions,,"Problem Statement Suppose $f:\mathbb{R}\to\mathbb{R}$ is a smooth function (infinitely differentiable) where $f(x)\geq 0$ for all $x\in\mathbb{R}$ , $f(0)=0$ , and $f(1)=1$ . Show that there is some $n\in\mathbb{N}$ and an $x_0\in\mathbb{R}$ such that $f^{(n)}(x_0)<0$ . Proof Attempt We can consider two cases: Case (1): There exists a $z<0$ such that $f(z)>0$ . By the MVT, there exists a point $z_0\in(z,0)$ such that $$f'(z_0)=\frac{f(z)-f(0)}{z-0}=\frac{f(z)}{z}<0 $$ so we are done. Case (2): For all $x<0$ , $f(x)=0$ . In this case, $f'(x)=0$ for all $x<0$ , which implies that, by left sided derivatives and the fact that $f'$ is smooth, $f'(0)=0$ . It then follows that,  since $f'(x)=0$ on $(-\infty,0]$ , we have $f^{(n)}(x)=0$ for all $x\in(-\infty,0]$ . For $n=1$ , define $z_1$ as the real number in $(0,1)$ such that $$f'(z_1)=\frac{f(1)-f(0)}{1-0}=1$$ For $n\in\mathbb{N}$ , define $z_{n+1}$ by the real number in $(0,z_n)$ such that $$f^{(n+1)}(z_{n+1})=\frac{f^{(n)}(z_n)-f^{(n)}(0)}{z_n-0}=\frac{f^{(n)}(z_{n})}{z_n}>0$$ whose existence is given by the mean value theorem. This defines the sequence $(z_n)$ , which converges to $0$ and is strictly decreasing. Edit 2: Continuing, we see that by definition, we have $$f^{(n+1)}(z_{n+1})=\frac{f^{(n)}(z_n)}{z_n}>f^{(n)}(z_n)$$ for any $n\in\mathbb{N}$ with $n\geq 2$ , since $0<z_n<1$ , thus the sequence $f^{(n)}(z_n)$ is strictly increasing and always greater than or equal to $1$ . Now, $f^{(n)}(0)=0$ , but since $(z_n)$ can get arbitrarily close to $0$ and $f^{(n)}(z_n)\geq 1$ for all $n\in\mathbb{N}$ , we have $f^{(n)}(0)=0<1\leq f^{(n)}(z_n)$ for large enough $n$ . This is has almost shown that $f$ is not differentiable for some value of $n$ , but I am not quite sure that it has exactly shown that. If it has, then we are done, as Case (2) cannot happen. I would like help either formalizing this or fixing this if it is not true. This is where my argument, to me, seems to break down. Intuitively, the slope of the secant lines between $f^{(n)}(z_n)$ and $f^{(n)}(0)$ is increasing without bound as $n$ increases. My idea was to create a sequence of points which converged to $0$ , but broke the differentiability at $0$ . I would greatly appreciate any tips in proceeding, critiques of my proof outline, or alternative proof methods. Thank you in advance. Edit 1: In Case (2) , I am not explicitly making any assumptions that would lead to a contradiction, but why is this proof going in that direction? Either Case (2) cannot happen, which I find intuitively true, but my intuition often fails me, or one of my assumptions in the beginning of Case (2) is false. Edit: Just to close this off, one of my faulty assumptions was that I assumed the sequence I constructed converged to $0$ , which it doesn't have to.","Problem Statement Suppose is a smooth function (infinitely differentiable) where for all , , and . Show that there is some and an such that . Proof Attempt We can consider two cases: Case (1): There exists a such that . By the MVT, there exists a point such that so we are done. Case (2): For all , . In this case, for all , which implies that, by left sided derivatives and the fact that is smooth, . It then follows that,  since on , we have for all . For , define as the real number in such that For , define by the real number in such that whose existence is given by the mean value theorem. This defines the sequence , which converges to and is strictly decreasing. Edit 2: Continuing, we see that by definition, we have for any with , since , thus the sequence is strictly increasing and always greater than or equal to . Now, , but since can get arbitrarily close to and for all , we have for large enough . This is has almost shown that is not differentiable for some value of , but I am not quite sure that it has exactly shown that. If it has, then we are done, as Case (2) cannot happen. I would like help either formalizing this or fixing this if it is not true. This is where my argument, to me, seems to break down. Intuitively, the slope of the secant lines between and is increasing without bound as increases. My idea was to create a sequence of points which converged to , but broke the differentiability at . I would greatly appreciate any tips in proceeding, critiques of my proof outline, or alternative proof methods. Thank you in advance. Edit 1: In Case (2) , I am not explicitly making any assumptions that would lead to a contradiction, but why is this proof going in that direction? Either Case (2) cannot happen, which I find intuitively true, but my intuition often fails me, or one of my assumptions in the beginning of Case (2) is false. Edit: Just to close this off, one of my faulty assumptions was that I assumed the sequence I constructed converged to , which it doesn't have to.","f:\mathbb{R}\to\mathbb{R} f(x)\geq 0 x\in\mathbb{R} f(0)=0 f(1)=1 n\in\mathbb{N} x_0\in\mathbb{R} f^{(n)}(x_0)<0 z<0 f(z)>0 z_0\in(z,0) f'(z_0)=\frac{f(z)-f(0)}{z-0}=\frac{f(z)}{z}<0  x<0 f(x)=0 f'(x)=0 x<0 f' f'(0)=0 f'(x)=0 (-\infty,0] f^{(n)}(x)=0 x\in(-\infty,0] n=1 z_1 (0,1) f'(z_1)=\frac{f(1)-f(0)}{1-0}=1 n\in\mathbb{N} z_{n+1} (0,z_n) f^{(n+1)}(z_{n+1})=\frac{f^{(n)}(z_n)-f^{(n)}(0)}{z_n-0}=\frac{f^{(n)}(z_{n})}{z_n}>0 (z_n) 0 f^{(n+1)}(z_{n+1})=\frac{f^{(n)}(z_n)}{z_n}>f^{(n)}(z_n) n\in\mathbb{N} n\geq 2 0<z_n<1 f^{(n)}(z_n) 1 f^{(n)}(0)=0 (z_n) 0 f^{(n)}(z_n)\geq 1 n\in\mathbb{N} f^{(n)}(0)=0<1\leq f^{(n)}(z_n) n f n f^{(n)}(z_n) f^{(n)}(0) n 0 0 0","['real-analysis', 'derivatives', 'solution-verification', 'alternative-proof', 'smooth-functions']"
99,$n$-th derivative change of variable,-th derivative change of variable,n,"I have a change of variables from $\alpha$ to $\beta$ such such that $$\frac{d}{d\beta} = \frac{1}{f(\alpha)}\frac{d}{d\alpha}$$ therfore, the second derivative will read $$\frac{d^2}{d\beta^2} =\frac{1}{f(\alpha)}\frac{d}{d\alpha}\times \frac{1}{f(\alpha)}\frac{d}{d\alpha}= \frac{1}{f^2(\alpha)}\frac{d^2}{d\alpha^2}-\frac{f'(\alpha)}{f^3(\alpha)}\frac{d}{d\alpha}$$ and the third order reads $$\frac{d^3}{d\beta^3} =-\frac{f''(\alpha)}{f^4(\alpha)}\frac{d}{d\alpha}+\frac{3 f'^2(\alpha)}{f^5(\alpha)}\frac{d}{d\alpha}-\frac{3 f'(\alpha)}{f^4(\alpha)}\frac{d^2}{d\alpha^2}+\frac{1}{f^3(\alpha)} \frac{d^3}{d\alpha^3}$$ I need to find the $n$ -th derivative in $\beta$ : $$\frac{d^n}{d\beta^n} =\, ?$$ is there a general formula for this case?","I have a change of variables from to such such that therfore, the second derivative will read and the third order reads I need to find the -th derivative in : is there a general formula for this case?","\alpha \beta \frac{d}{d\beta} = \frac{1}{f(\alpha)}\frac{d}{d\alpha} \frac{d^2}{d\beta^2} =\frac{1}{f(\alpha)}\frac{d}{d\alpha}\times \frac{1}{f(\alpha)}\frac{d}{d\alpha}= \frac{1}{f^2(\alpha)}\frac{d^2}{d\alpha^2}-\frac{f'(\alpha)}{f^3(\alpha)}\frac{d}{d\alpha} \frac{d^3}{d\beta^3} =-\frac{f''(\alpha)}{f^4(\alpha)}\frac{d}{d\alpha}+\frac{3 f'^2(\alpha)}{f^5(\alpha)}\frac{d}{d\alpha}-\frac{3 f'(\alpha)}{f^4(\alpha)}\frac{d^2}{d\alpha^2}+\frac{1}{f^3(\alpha)} \frac{d^3}{d\alpha^3} n \beta \frac{d^n}{d\beta^n} =\, ?","['real-analysis', 'combinatorics', 'derivatives']"
