,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,need help with a Markov chain for weather conditions,need help with a Markov chain for weather conditions,,"This is my second time posting the question as I failed to do so the first time, because I did not know the proper way. My apologies. Question: Suppose that whether or not it rains today depends on weather conditions through the previous teo days. If it has rained for the past two days, then it will rain today with probability 0.7. If it did not rain for any of the past two days, then it will rain today with probability 0.4. In any other case the weather today will, with probability 0.5, be the same as the weather yesterday. Describe the weather condition over time with a Markov chain. Suppose that it rained today and yesterday. Calculate the expected number of days until it rains three consecutive days for the first time in the future. I have found 4 different states that I named RR(0), RN(2), NR(1), and NN(3). R stands for when it rains and N is for when it does not. As the question asks, I have tried finding the possible ways of three consecutive days being rainy. At time n, we are given it was rainy today and yesterday, meaning we are in State 0. 1-) First possibility is when it rains tomorrow, which gives us RRR (we got the three consecutive days) 2-) Second possibility is when we go from 0 to 2, from 2 to 1, from 1 to 0, and stay in 0 one day. That follows as : RRNRRR (In 4 days we can get rain for 3 consecutive days) 3-) Third is when we go from 0 to 2, from 2 to 3, from 3 to 1, from 1 to 0, and stay in 0 one day. That follows as: RRNNRRR. To conclude what I have in mind about the question, wherever we go, when we get to State 0, we need to stay there one more day to get three consecutive rainy days. That means the minimum # of days to get rain for three consecutive days is one day. However, after this point, I am not able to proceed with the question. Any help would be appreciated, thank you!! Edit: I think the maximum # of days is just a random number, which leads me to the expectation of the sum of a random number of geometric random variable, but still I can't go any further beyond that. Thank you!","This is my second time posting the question as I failed to do so the first time, because I did not know the proper way. My apologies. Question: Suppose that whether or not it rains today depends on weather conditions through the previous teo days. If it has rained for the past two days, then it will rain today with probability 0.7. If it did not rain for any of the past two days, then it will rain today with probability 0.4. In any other case the weather today will, with probability 0.5, be the same as the weather yesterday. Describe the weather condition over time with a Markov chain. Suppose that it rained today and yesterday. Calculate the expected number of days until it rains three consecutive days for the first time in the future. I have found 4 different states that I named RR(0), RN(2), NR(1), and NN(3). R stands for when it rains and N is for when it does not. As the question asks, I have tried finding the possible ways of three consecutive days being rainy. At time n, we are given it was rainy today and yesterday, meaning we are in State 0. 1-) First possibility is when it rains tomorrow, which gives us RRR (we got the three consecutive days) 2-) Second possibility is when we go from 0 to 2, from 2 to 1, from 1 to 0, and stay in 0 one day. That follows as : RRNRRR (In 4 days we can get rain for 3 consecutive days) 3-) Third is when we go from 0 to 2, from 2 to 3, from 3 to 1, from 1 to 0, and stay in 0 one day. That follows as: RRNNRRR. To conclude what I have in mind about the question, wherever we go, when we get to State 0, we need to stay there one more day to get three consecutive rainy days. That means the minimum # of days to get rain for three consecutive days is one day. However, after this point, I am not able to proceed with the question. Any help would be appreciated, thank you!! Edit: I think the maximum # of days is just a random number, which leads me to the expectation of the sum of a random number of geometric random variable, but still I can't go any further beyond that. Thank you!",,"['probability', 'statistics', 'stochastic-processes', 'markov-chains', 'stochastic-analysis']"
1,The expectation of sum of squares of the arrival times in a Poisson process,The expectation of sum of squares of the arrival times in a Poisson process,,"Question: Let $\{N(t)\}$ be rate $\lambda$ Poisson process, with arrival times $\{S_n,n=0,1,...\}$ . Evaluate the expected sum of squares of the arrival times occuring before $t$ , $$E(t)=\mathbb{E}\left[\sum_{n=1}^{N(t)}S_n^2\right]$$ where we define $\sum_{n=1}^{0} S_n=0$ . There's a similar question here . However, in this question, we are looking for the expected sum of squares . I tried to understand the answer to the similar question in order to solve the problem on my own. However, I cannot even understand that. I am confused. I do not even know where to start. Is there a formula for $S_i$ 's?","Question: Let be rate Poisson process, with arrival times . Evaluate the expected sum of squares of the arrival times occuring before , where we define . There's a similar question here . However, in this question, we are looking for the expected sum of squares . I tried to understand the answer to the similar question in order to solve the problem on my own. However, I cannot even understand that. I am confused. I do not even know where to start. Is there a formula for 's?","\{N(t)\} \lambda \{S_n,n=0,1,...\} t E(t)=\mathbb{E}\left[\sum_{n=1}^{N(t)}S_n^2\right] \sum_{n=1}^{0} S_n=0 S_i","['statistics', 'stochastic-processes', 'expected-value', 'poisson-process']"
2,"For valid probability densities $p(x)$ and $q(x), \int p(x)\ln(q(x))~dx = 0$. Can you catch my mistake?",For valid probability densities  and . Can you catch my mistake?,"p(x) q(x), \int p(x)\ln(q(x))~dx = 0","I was trying to solve the following integral by parts, $$ \int p(x)\ln(q(x))dx$$ for valid probability density functions $p(x)$ and $q(x)$ , $$ \int p(x)\ln(q(x))dx = \ln(q(x)) - \int\frac{q'(x)}{q(x)}dx,$$ since $p(x)$ is assumed second function in the product and $\int p(x)dx = 1$ . $$ \int p(x)\ln(q(x))dx = \ln(q(x)) - \int\frac{q'(x)}{q(x)}dx = \ln(q(x)) - \ln(q(x)) = 0.$$ Is this wrong even if the limits are assumed to be entire $\mathbb{R}$ . I'm sure this is very elementary and I'm missing something very fundamental here.","I was trying to solve the following integral by parts, for valid probability density functions and , since is assumed second function in the product and . Is this wrong even if the limits are assumed to be entire . I'm sure this is very elementary and I'm missing something very fundamental here."," \int p(x)\ln(q(x))dx p(x) q(x)  \int p(x)\ln(q(x))dx = \ln(q(x)) - \int\frac{q'(x)}{q(x)}dx, p(x) \int p(x)dx = 1  \int p(x)\ln(q(x))dx = \ln(q(x)) - \int\frac{q'(x)}{q(x)}dx = \ln(q(x)) - \ln(q(x)) = 0. \mathbb{R}","['calculus', 'integration', 'statistics', 'probability-distributions']"
3,Mid-range via minimax,Mid-range via minimax,,"Warning: crossposted at Statistics SE. Given vector ${\rm a} \in \Bbb R^n$ , $$\begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_2^2\end{array} = \frac1n {\Bbb 1}_n^\top {\rm a} \tag{mean}$$ is the (arithmetic) mean of the entries of vector ${\rm a}  \in \Bbb R^n$ , whereas $$\begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_1\end{array} \tag{median}$$ is a median of the entries of vector ${\rm a}  \in \Bbb R^n$ . Using the $\infty$ -norm instead, what is the following? $$\color{blue}{\boxed{\,\\\begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_{\infty}\end{array}}}$$ It appears to be the mid-range . I append a proof based on linear programming. Assuming that I have made no mistakes and my proof is indeed correct, I am interested in other proofs and in references . My proof $$\begin{array}{ll} \underset{x \in {\Bbb R}}{\text{minimize}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_{\infty}\end{array} $$ Introducing optimization variable $y \in {\Bbb R}$ , $$\begin{array}{ll} \underset{x, y \in {\Bbb R}}{\text{minimize}} & \qquad\qquad y\\ \text{subject to} & -y {\Bbb 1}_n \leq x {\Bbb 1}_n - {\rm a} \leq y {\Bbb 1}_n\end{array} $$ or, alternatively, $$\begin{array}{lrl} \underset{x, y \in {\Bbb R}}{\text{minimize}} & y & \\ \text{subject to} & {\rm a} & \leq (x + y) {\Bbb 1}_n \\ & (x - y) {\Bbb 1}_n & \leq {\rm a}\end{array}$$ Let the entries of vector ${\rm a} \in \Bbb R^n$ be denoted by $a_1, a_2, \dots, a_n$ . Note that there are many redundant inequalities: the set of $n$ inequalities ${\rm a} \leq (x + y) {\Bbb 1}_n$ can be replaced by $$x + y \geq \max \{ a_1, a_2, \dots, a_n \}$$ the set of $n$ inequalities $(x - y) {\Bbb 1}_n \leq {\rm a}$ can be replaced by $$x - y \leq \min \{ a_1, a_2, \dots, a_n \}$$ Subtracting the latter inequality from the former inequality, $$y \geq \frac{ \max \{ a_1, a_2, \dots, a_n \} - \min \{ a_1, a_2, \dots, a_n \} }{2} =: y_{\min}$$ and, thus, $$ \begin{aligned} x &\geq \max \{ a_1, a_2, \dots, a_n \} - y_{\min} = \frac{ \min \{ a_1, a_2, \dots, a_n \} + \max \{ a_1, a_2, \dots, a_n \} }{2} \\\\ x &\leq \min \{ a_1, a_2, \dots, a_n \} + y_{\min} = \frac{ \min \{ a_1, a_2, \dots, a_n \} + \max \{ a_1, a_2, \dots, a_n \} }{2} \end{aligned} $$ Hence, $$\begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_{\infty}\end{array} = \color{blue}{\frac{ \min \{ a_1, a_2, \dots, a_n \} + \max \{ a_1, a_2, \dots, a_n \} }{2}}$$ Some call this value the mid-range of $\{ a_1, a_2, \dots, a_n \}$ . Related Centroid under the Chebyshev distance The median minimizes the sum of absolute deviations (the $ {\ell}_{1} $ norm) Average of minimum and maximum in set What minimizes the Chebyshev Distance? Term for center of dataset","Warning: crossposted at Statistics SE. Given vector , is the (arithmetic) mean of the entries of vector , whereas is a median of the entries of vector . Using the -norm instead, what is the following? It appears to be the mid-range . I append a proof based on linear programming. Assuming that I have made no mistakes and my proof is indeed correct, I am interested in other proofs and in references . My proof Introducing optimization variable , or, alternatively, Let the entries of vector be denoted by . Note that there are many redundant inequalities: the set of inequalities can be replaced by the set of inequalities can be replaced by Subtracting the latter inequality from the former inequality, and, thus, Hence, Some call this value the mid-range of . Related Centroid under the Chebyshev distance The median minimizes the sum of absolute deviations (the $ {\ell}_{1} $ norm) Average of minimum and maximum in set What minimizes the Chebyshev Distance? Term for center of dataset","{\rm a} \in \Bbb R^n \begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_2^2\end{array} = \frac1n {\Bbb 1}_n^\top {\rm a} \tag{mean} {\rm a}  \in \Bbb R^n \begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_1\end{array} \tag{median} {\rm a}  \in \Bbb R^n \infty \color{blue}{\boxed{\,\\\begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_{\infty}\end{array}}} \begin{array}{ll} \underset{x \in {\Bbb R}}{\text{minimize}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_{\infty}\end{array}  y \in {\Bbb R} \begin{array}{ll} \underset{x, y \in {\Bbb R}}{\text{minimize}} & \qquad\qquad y\\ \text{subject to} & -y {\Bbb 1}_n \leq x {\Bbb 1}_n - {\rm a} \leq y {\Bbb 1}_n\end{array}  \begin{array}{lrl} \underset{x, y \in {\Bbb R}}{\text{minimize}} & y & \\ \text{subject to} & {\rm a} & \leq (x + y) {\Bbb 1}_n \\ & (x - y) {\Bbb 1}_n & \leq {\rm a}\end{array} {\rm a} \in \Bbb R^n a_1, a_2, \dots, a_n n {\rm a} \leq (x + y) {\Bbb 1}_n x + y \geq \max \{ a_1, a_2, \dots, a_n \} n (x - y) {\Bbb 1}_n \leq {\rm a} x - y \leq \min \{ a_1, a_2, \dots, a_n \} y \geq \frac{ \max \{ a_1, a_2, \dots, a_n \} - \min \{ a_1, a_2, \dots, a_n \} }{2} =: y_{\min}  \begin{aligned} x &\geq \max \{ a_1, a_2, \dots, a_n \} - y_{\min} = \frac{ \min \{ a_1, a_2, \dots, a_n \} + \max \{ a_1, a_2, \dots, a_n \} }{2} \\\\ x &\leq \min \{ a_1, a_2, \dots, a_n \} + y_{\min} = \frac{ \min \{ a_1, a_2, \dots, a_n \} + \max \{ a_1, a_2, \dots, a_n \} }{2} \end{aligned}  \begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_{\infty}\end{array} = \color{blue}{\frac{ \min \{ a_1, a_2, \dots, a_n \} + \max \{ a_1, a_2, \dots, a_n \} }{2}} \{ a_1, a_2, \dots, a_n \}","['statistics', 'optimization', 'reference-request', 'linear-programming', 'central-tendency']"
4,Are the null hypothesis and alternative hypothesis mutually exclusive?,Are the null hypothesis and alternative hypothesis mutually exclusive?,,"I am having a little bit of trouble understanding the concept of p-values. On the one hand, we are saying that the null hypothesis is a hypothesis we set, and the alternative hypothesis is something we would like to prove. In that case, how does it make sense to say that the p-value is the probability of the alternative hypothesis being true given that the null hypothesis is true if from the above definition it seems that both are mutually exclusive?","I am having a little bit of trouble understanding the concept of p-values. On the one hand, we are saying that the null hypothesis is a hypothesis we set, and the alternative hypothesis is something we would like to prove. In that case, how does it make sense to say that the p-value is the probability of the alternative hypothesis being true given that the null hypothesis is true if from the above definition it seems that both are mutually exclusive?",,"['statistics', 'statistical-inference']"
5,KL-Divergence of Uniform distributions,KL-Divergence of Uniform distributions,,"Having $P=Unif[0,\theta_1]$ and $Q=Unif[0,\theta_2]$ where $0<\theta_1<\theta_2$ I would like to calculate the KL divergence $KL(P,Q)=?$ I know the uniform pdf: $\frac{1}{b-a}$ and that the distribution is continous, therefore I use the general KL divergence formula: $$KL(P,Q)=\int f_{\theta}(x)*ln(\frac{f_{\theta}(x)}{f_{\theta^*}(x)})$$ $$=\int\frac{1}{\theta_1}*ln(\frac{\frac{1}{\theta_1}}{\frac{1}{\theta_2}})$$ $$=\int\frac{1}{\theta_1}*ln(\frac{\theta_2}{\theta_1})$$ From here on I am not sure how to use the integral to get to the solution.","Having and where I would like to calculate the KL divergence I know the uniform pdf: and that the distribution is continous, therefore I use the general KL divergence formula: From here on I am not sure how to use the integral to get to the solution.","P=Unif[0,\theta_1] Q=Unif[0,\theta_2] 0<\theta_1<\theta_2 KL(P,Q)=? \frac{1}{b-a} KL(P,Q)=\int f_{\theta}(x)*ln(\frac{f_{\theta}(x)}{f_{\theta^*}(x)}) =\int\frac{1}{\theta_1}*ln(\frac{\frac{1}{\theta_1}}{\frac{1}{\theta_2}}) =\int\frac{1}{\theta_1}*ln(\frac{\theta_2}{\theta_1})","['statistics', 'uniform-distribution']"
6,"Distribution of $\frac{2X_1 - X_2-X_3}{\sqrt{(X_1+X_2+X_3)^2 +\frac{3}{2} (X_2-X_3)^2}}$ when $X_1,X_2,X_3\sim N(0,\sigma^2)$",Distribution of  when,"\frac{2X_1 - X_2-X_3}{\sqrt{(X_1+X_2+X_3)^2 +\frac{3}{2} (X_2-X_3)^2}} X_1,X_2,X_3\sim N(0,\sigma^2)","Given that $X_1, X_2, X_3 $ are independent random variables form $N(0, \sigma^2 )$ , I have to indicate that the statistic given below has a $t$ distribution or not. \begin{equation} \frac{2X_1 - X_2-X_3}{\sqrt{(X_1+X_2+X_3)^2 +\frac{3}{2} (X_2-X_3)^2}} \end{equation} In my attempt of solving this problem: I start by showing that we can write the numerator as $a^TX$ , where $a^T = (2 -1 -1)$ and $X^T= (X_1 X_2 X_3)$ . Thus we have that $a^TX \sim N(0, a^T(\sigma^2 I)a)= N(0, 6\sigma^2)$ . And so $\frac{1}{\sqrt{6\sigma^2}} a^TX \sim N(0,1)$ or $\frac{1}{\sqrt{6\sigma^2}}(2X_1-X_2-X_3)\sim N(0,1)$ . Next, we know that $(X_1+X_2+X_3) \sim N(0, 3\sigma^2)$ . This implies that $\frac{1}{\sqrt{3\sigma^2}}(X_1+X_2+X_3) \sim N(0,1)$ and thus, $\frac{1}{{3\sigma^2}}(X_1+X_2+X_3)^2 \sim \chi^2(1)$ . Similarly, $\frac{1}{2\sigma^2}(X_2-X_3)^2 \sim \chi^2(1)$ . Therefore, $\frac{1}{{3\sigma^2}}(X_1+X_2+X_3)^2 + \frac{1}{2\sigma^2}(X_2-X_3)^2 \sim \chi^2(2)$ or $\frac{1}{{3\sigma^2}}\left((X_1+X_2+X_3)^2 + \frac{3}{2}(X_2-X_3)^2 \right) \sim \chi^2(2)$ . As a third step I have to show that $\frac{1}{\sqrt{6\sigma^2}}(2X_1-X_2-X_3)$ and $ \frac{1}{{3\sigma^2}}\left((X_1+X_2+X_3)^2 + \frac{3}{2}(X_2-X_3)^2 \right)$ are independent and I am not sure how to show that. Any help would be appreciated.","Given that are independent random variables form , I have to indicate that the statistic given below has a distribution or not. In my attempt of solving this problem: I start by showing that we can write the numerator as , where and . Thus we have that . And so or . Next, we know that . This implies that and thus, . Similarly, . Therefore, or . As a third step I have to show that and are independent and I am not sure how to show that. Any help would be appreciated.","X_1, X_2, X_3  N(0, \sigma^2 ) t \begin{equation}
\frac{2X_1 - X_2-X_3}{\sqrt{(X_1+X_2+X_3)^2 +\frac{3}{2} (X_2-X_3)^2}}
\end{equation} a^TX a^T = (2 -1 -1) X^T= (X_1 X_2 X_3) a^TX \sim N(0, a^T(\sigma^2 I)a)= N(0, 6\sigma^2) \frac{1}{\sqrt{6\sigma^2}} a^TX \sim N(0,1) \frac{1}{\sqrt{6\sigma^2}}(2X_1-X_2-X_3)\sim N(0,1) (X_1+X_2+X_3) \sim N(0, 3\sigma^2) \frac{1}{\sqrt{3\sigma^2}}(X_1+X_2+X_3) \sim N(0,1) \frac{1}{{3\sigma^2}}(X_1+X_2+X_3)^2 \sim \chi^2(1) \frac{1}{2\sigma^2}(X_2-X_3)^2 \sim \chi^2(1) \frac{1}{{3\sigma^2}}(X_1+X_2+X_3)^2 + \frac{1}{2\sigma^2}(X_2-X_3)^2 \sim \chi^2(2) \frac{1}{{3\sigma^2}}\left((X_1+X_2+X_3)^2 + \frac{3}{2}(X_2-X_3)^2 \right) \sim \chi^2(2) \frac{1}{\sqrt{6\sigma^2}}(2X_1-X_2-X_3)  \frac{1}{{3\sigma^2}}\left((X_1+X_2+X_3)^2 + \frac{3}{2}(X_2-X_3)^2 \right)","['statistics', 'probability-distributions', 'normal-distribution']"
7,Why is variance defined as $\sum\limits_{n} |\mu -x_i|^2$ and not $\sum\limits_{n} |\mu -x_i|$?,Why is variance defined as  and not ?,\sum\limits_{n} |\mu -x_i|^2 \sum\limits_{n} |\mu -x_i|,"If we wanted to measure how much the values $x_1, \ldots ,x_n$ of a sample differ from the mean $\mu$ , it seems more intuitive to me to use the formula $$\frac{\sum\limits_{n}  |\mu -x_i|}{n}$$ instead of the formula for variance. I've read about some geometric interpretations of variance as well as standard deviation, yet this just seems to push the questions further back, as we could ask what reason do we have to care more about the distance between the vectors $(x_1,\ldots x_n)$ and $(\mu ,\ldots ,\mu)$ as opposed to just the average distance between a possible value $x_0$ and $\mu$ . Some explanations of the variance formula point to the fact that variance pays more attention to values further apart from the mean, but two immediate questions come to mind: Why should we give more importance to values farther apart from the mean? And why should we do so by squaring the respective distances instead of, say, cubing them?","If we wanted to measure how much the values of a sample differ from the mean , it seems more intuitive to me to use the formula instead of the formula for variance. I've read about some geometric interpretations of variance as well as standard deviation, yet this just seems to push the questions further back, as we could ask what reason do we have to care more about the distance between the vectors and as opposed to just the average distance between a possible value and . Some explanations of the variance formula point to the fact that variance pays more attention to values further apart from the mean, but two immediate questions come to mind: Why should we give more importance to values farther apart from the mean? And why should we do so by squaring the respective distances instead of, say, cubing them?","x_1, \ldots ,x_n \mu \frac{\sum\limits_{n}  |\mu -x_i|}{n} (x_1,\ldots x_n) (\mu ,\ldots ,\mu) x_0 \mu","['probability', 'statistics', 'soft-question', 'definition', 'variance']"
8,Could finding the central tendency between mean and median be useful?,Could finding the central tendency between mean and median be useful?,,"I'm working with unpredictable data, it could have strong outliers in some cases, and not in others. Considering it's completely situational and random, would it make sense to just average the mean and median of the dataset to get the best of both worlds? Finding the central tendency of both common central tendencies. Or would that skewing be undesirable. Thanks.","I'm working with unpredictable data, it could have strong outliers in some cases, and not in others. Considering it's completely situational and random, would it make sense to just average the mean and median of the dataset to get the best of both worlds? Finding the central tendency of both common central tendencies. Or would that skewing be undesirable. Thanks.",,['statistics']
9,"Hoeffding for bounded random variables, extension of Rademacher case","Hoeffding for bounded random variables, extension of Rademacher case",,"In Vershynin's High-Dimensional Probability , he first proves the Hoeffding bound on page 17 $$\mathbb{P}\left\{\sum_{i=1}^N a_iX_i \geq t\right\} \leq \exp \left(  -\frac{1}{2} \frac{t^2}{\|a\|^2_2} \right)$$ for $X_i$ being Rademacher random variables and $(a_1, \dotsc, a_N) \in \mathbb{R}^N$ . Then he gives an exercise to extend it to bounded random variables. Exercise 2.2.7 : Prove that for $X_i$ independent and bounded, where $m_i \leq X_i \leq M_i$ almost surely, we have for any $t \geq  0$ , $$\mathbb{P}\left\{ \sum_{i=1}^N (X_i - \mathbb{E}X_i) \geq t \right\}  \leq \exp\left( -\frac{2t^2}{\sum_{i=1}^N (M_i - m_i)^2} \right)$$ perhaps with some absolute constant other than 2 in the tail. I'm not seeing what he wants here. Is he perhaps saying to compare it to a sum of linearly translated sum of Rademacher variables? He doesn't explicitly say to use the Rademacher case, but I assume that's what he means. It looks like he wants $a$ to be a vector in the $M_i - m_i$ , translating the Rademacher random variables. What I'm missing is how to link the general bounded case to the translated-Rademacher case. To be clear, I am aware of other proofs of Hoeffding for the bounded case; I am interested in a simple-ish way to leverage the linear-combination-of-Rademacher case to get this result.","In Vershynin's High-Dimensional Probability , he first proves the Hoeffding bound on page 17 for being Rademacher random variables and . Then he gives an exercise to extend it to bounded random variables. Exercise 2.2.7 : Prove that for independent and bounded, where almost surely, we have for any , perhaps with some absolute constant other than 2 in the tail. I'm not seeing what he wants here. Is he perhaps saying to compare it to a sum of linearly translated sum of Rademacher variables? He doesn't explicitly say to use the Rademacher case, but I assume that's what he means. It looks like he wants to be a vector in the , translating the Rademacher random variables. What I'm missing is how to link the general bounded case to the translated-Rademacher case. To be clear, I am aware of other proofs of Hoeffding for the bounded case; I am interested in a simple-ish way to leverage the linear-combination-of-Rademacher case to get this result.","\mathbb{P}\left\{\sum_{i=1}^N a_iX_i \geq t\right\} \leq \exp \left(
 -\frac{1}{2} \frac{t^2}{\|a\|^2_2} \right) X_i (a_1, \dotsc, a_N) \in \mathbb{R}^N X_i m_i \leq X_i \leq M_i t \geq
 0 \mathbb{P}\left\{ \sum_{i=1}^N (X_i - \mathbb{E}X_i) \geq t \right\}
 \leq \exp\left( -\frac{2t^2}{\sum_{i=1}^N (M_i - m_i)^2} \right) a M_i - m_i","['statistics', 'asymptotics', 'large-deviation-theory']"
10,Find global median from medians of subgroups,Find global median from medians of subgroups,,"Supposte I have a list of numbers $\{x_1, \ldots, x_N\}$ , not necessarily ordered, and I divide it in subsets $\{ \{x_1,\ldots,x_{d_1}\}, \{x_{d_1+1},\ldots,x_{d_2}\}, \ldots \}$ , where $d_n$ is the number of elements in each subset, then the mean over the entire set $\langle x\rangle$ is the weighted mean of the mean in each subset: $$ \langle x \rangle = \sum_j \langle x_j\rangle \frac{d_j}{N} $$ where $d_j$ is the number of elements in each subset and the sum is over the subsets. If I want to compute the median of the entire set and I know the median in each subset, does an analogous formula exists?","Supposte I have a list of numbers , not necessarily ordered, and I divide it in subsets , where is the number of elements in each subset, then the mean over the entire set is the weighted mean of the mean in each subset: where is the number of elements in each subset and the sum is over the subsets. If I want to compute the median of the entire set and I know the median in each subset, does an analogous formula exists?","\{x_1, \ldots, x_N\} \{ \{x_1,\ldots,x_{d_1}\}, \{x_{d_1+1},\ldots,x_{d_2}\}, \ldots \} d_n \langle x\rangle 
\langle x \rangle = \sum_j \langle x_j\rangle \frac{d_j}{N}
 d_j","['statistics', 'descriptive-statistics', 'median']"
11,Independence of functions of order statistics when the random variables are uniformly distributed,Independence of functions of order statistics when the random variables are uniformly distributed,,"Let $X_1$ , $X_2$ ,…, $X_n$ be $n$ i.i.d. random variables with $f(x)$ as the pdf and $F(x)$ as the cdf in interval $[0,1]$ . Let $F$ be uniformly distributed. Let $X_{i:n}$ be the $i^{th}$ order statistic such that $X_{1:n} \leq X_{2:n} \leq ... \leq X_{n:n}$ . I wish to compute the expected value $\mathbb{E} [\frac{X_{(k-1):n} X_{i:n}}{X_{k:n}} ]$ for any $ k < i \leq n$ . So the question is are $\frac{X_{(k-1):n}}{X_{k:n}}$ and $X_{i:n}$ independent? Because if they are not, then the problem is non-trivial. Due to a standard result in theory of order statistics, we already know that for any $i \leq n$ , $\frac{X_{(i-1):n}}{X_{i:n}}$ and $X_{i:n}$ are independent.","Let , ,…, be i.i.d. random variables with as the pdf and as the cdf in interval . Let be uniformly distributed. Let be the order statistic such that . I wish to compute the expected value for any . So the question is are and independent? Because if they are not, then the problem is non-trivial. Due to a standard result in theory of order statistics, we already know that for any , and are independent.","X_1 X_2 X_n n f(x) F(x) [0,1] F X_{i:n} i^{th} X_{1:n} \leq X_{2:n} \leq ... \leq X_{n:n} \mathbb{E} [\frac{X_{(k-1):n} X_{i:n}}{X_{k:n}} ]  k < i \leq n \frac{X_{(k-1):n}}{X_{k:n}} X_{i:n} i \leq n \frac{X_{(i-1):n}}{X_{i:n}} X_{i:n}","['probability', 'statistics', 'independence', 'uniform-distribution', 'order-statistics']"
12,How to find mode when modal class is first or last class?,How to find mode when modal class is first or last class?,,"We know that formula of finding mode of grouped data is Mode = $l+\frac{(f_1-f_0)}{(2f_1-f_0-f_2)}\cdot h$ Where, $f_0$ is frequency of the class preceding the modal class and $f_2$ is frequency of the class succeeding the modal class. But how to calculate mode when there is no class preceding or succeeding the modal class.","We know that formula of finding mode of grouped data is Mode = Where, is frequency of the class preceding the modal class and is frequency of the class succeeding the modal class. But how to calculate mode when there is no class preceding or succeeding the modal class.",l+\frac{(f_1-f_0)}{(2f_1-f_0-f_2)}\cdot h f_0 f_2,['statistics']
13,UMVUE of $\theta$ when $X_i$'s are i.i.d with pdf $f(x;\theta)=\theta x^{-(1+\theta)}I_{x>1}$,UMVUE of  when 's are i.i.d with pdf,\theta X_i f(x;\theta)=\theta x^{-(1+\theta)}I_{x>1},"I am trying to find the Unique minimum variance unbiased estimator for $\theta$ where $\{X_i\}_{i=i}^{n}\sim f(x;\theta)=\theta x^{-(1+\theta)}$ where $x>1$ and $\theta\in(1,\infty)$ . I start by showing that $f(x;\theta)=\theta x^{-(1+\theta)}$ is in the exponential: $$f(x;\theta)=e^{\ln(\theta)-(1+\theta)\ln(x)}I_{x>1}$$ Since $f(x;\theta)$ is a member of the exponential family of full rank since the parameter space contains an open interval. then $\sum_{i=1}^{n}\ln(x_i)$ is a complete and minimally sufficient statistic. Since, $g(x)=e^x$ is a one to one transformation then, $\prod_{i=1}^{n}x_i$ is also a minimally sufficient statistic. By a similar argument one can conclude that $S(X)=\sum_{i=1}^{n}x_i$ is also minimally sufficient and complete. Note: $$\int_{1}^{\infty}x\theta x^{-(1+\theta)}dx=\theta\int_{1}^{\infty}x^{-\theta}dx=\frac{\theta}{1-\theta}$$ Since, $\theta>1$ and $x>1$ . Then $E[\sum_{i=1}^{n}X_i]=\frac{n\theta}{1-\theta}$ . Note that can only achieve CR Lower bound if in exponential family and estimating a linear function of the minimum sufficient statistic. Note: $E[a+bS(X)]=a+\frac{bn\theta}{1-\theta}$ so no linear combination of $S(X)$ can achieve an unbiased estimator of $\theta$ so there does not exist a UMVUE for $\theta$ . IS my logic correct?","I am trying to find the Unique minimum variance unbiased estimator for where where and . I start by showing that is in the exponential: Since is a member of the exponential family of full rank since the parameter space contains an open interval. then is a complete and minimally sufficient statistic. Since, is a one to one transformation then, is also a minimally sufficient statistic. By a similar argument one can conclude that is also minimally sufficient and complete. Note: Since, and . Then . Note that can only achieve CR Lower bound if in exponential family and estimating a linear function of the minimum sufficient statistic. Note: so no linear combination of can achieve an unbiased estimator of so there does not exist a UMVUE for . IS my logic correct?","\theta \{X_i\}_{i=i}^{n}\sim f(x;\theta)=\theta x^{-(1+\theta)} x>1 \theta\in(1,\infty) f(x;\theta)=\theta x^{-(1+\theta)} f(x;\theta)=e^{\ln(\theta)-(1+\theta)\ln(x)}I_{x>1} f(x;\theta) \sum_{i=1}^{n}\ln(x_i) g(x)=e^x \prod_{i=1}^{n}x_i S(X)=\sum_{i=1}^{n}x_i \int_{1}^{\infty}x\theta x^{-(1+\theta)}dx=\theta\int_{1}^{\infty}x^{-\theta}dx=\frac{\theta}{1-\theta} \theta>1 x>1 E[\sum_{i=1}^{n}X_i]=\frac{n\theta}{1-\theta} E[a+bS(X)]=a+\frac{bn\theta}{1-\theta} S(X) \theta \theta","['statistics', 'probability-distributions', 'statistical-inference', 'expected-value', 'parameter-estimation']"
14,"Show that $(U+X)Y$ is uniformly distributed over $[-n,n]$",Show that  is uniformly distributed over,"(U+X)Y [-n,n]","$X,U$ and $Y$ are random variables for which: $$ \forall i\in\{0,1,\dots,n-1\}: P(X=i)=\frac 1n,$$ $$ P(Y=1)=P(Y=-1)=\frac 12,$$ $$ U =_d U[0,1].$$ Show that $(U+X)Y = _d U[-n,n]$ if $U,X$ and $Y$ are mutually independent. My attempt: We note that $P\left( (U+X)Y\le z\right) = \frac 12\left(P(U+X\le z)+P(U+X\ge -z \right)$ . We determine the distribution of $T=U+X$ : $f_{T,X}(t,x)=f_U(t-x)f_X(x)$ (using $U\perp\!\!\!\perp X$ ). Since $X$ is discrete, we get the marginal distribution of $T$ by summing over all possible values that $X$ can take: $$ f_{U+X}(t)=\sum_{x=0}^{n-1} f_U(t-x)P(X=x)=\frac 1n\sum_{x=0}^{n-1}f_U(t-x).$$ Inspired by the claim, we can note that $f_{U}(t-x)=1$ for $-n\le t \le n$ . Therefore $$ f_{U+X}(t)=1\cdot I(-n\le t\le n).$$ Then, $$ P(U+X\le z)=\int_{-n}^zdt=z+n =P(U+X\ge -z).$$ We get $P((U+X)Y\le z)=F_{(U+X)Y}(z)=z+n$ , $-n\le z\le n$ . This does not correspond to the CDF of a uniformly distributed RV on $[-n,n]$ . Where is my mistake?","and are random variables for which: Show that if and are mutually independent. My attempt: We note that . We determine the distribution of : (using ). Since is discrete, we get the marginal distribution of by summing over all possible values that can take: Inspired by the claim, we can note that for . Therefore Then, We get , . This does not correspond to the CDF of a uniformly distributed RV on . Where is my mistake?","X,U Y  \forall i\in\{0,1,\dots,n-1\}: P(X=i)=\frac 1n,  P(Y=1)=P(Y=-1)=\frac 12,  U =_d U[0,1]. (U+X)Y = _d U[-n,n] U,X Y P\left( (U+X)Y\le z\right) = \frac 12\left(P(U+X\le z)+P(U+X\ge -z \right) T=U+X f_{T,X}(t,x)=f_U(t-x)f_X(x) U\perp\!\!\!\perp X X T X  f_{U+X}(t)=\sum_{x=0}^{n-1} f_U(t-x)P(X=x)=\frac 1n\sum_{x=0}^{n-1}f_U(t-x). f_{U}(t-x)=1 -n\le t \le n  f_{U+X}(t)=1\cdot I(-n\le t\le n).  P(U+X\le z)=\int_{-n}^zdt=z+n =P(U+X\ge -z). P((U+X)Y\le z)=F_{(U+X)Y}(z)=z+n -n\le z\le n [-n,n]","['statistics', 'probability-distributions', 'solution-verification']"
15,Fitting a plane to points using SVD,Fitting a plane to points using SVD,,"I am trying to find a plane in 3D space that best fits a number of points.  I want to do this using SVD.  To calculate the SVD: Subtract the centroid of the points from each point. Put the points in an mx3 matrix. Calculate the SVD (e.g. [U, S, V] = SVD(A)). The last column of V, (e.g. V(:,3)), is supposed to be a normal vector to the plane.  While the other two columns of V (e.g. V(:,1) and V(:,2)) are vectors parallel to the plane (and orthogonal to each other).  I want to find the equation of the plane in ax+by+cz+d=0 form.  The last column of V (e.g. V(:,3)) gives ""a"", ""b"", and ""c"", however, in order to find ""d"", I need a point on the plane to plug in and solve for d.  The problem is that I don't know what are valid points to use to plug in. My question is: does the centroid of the points necessarily lie on the fitted plane? If so, then it's easy to just plug in the centroid values in the equation (along with the  from the norm) and solve for ""d"".  Otherwise, how can I calculate ""d"" in the above equation? The matrix U apparently gives the point values but I don't understand which values to take.","I am trying to find a plane in 3D space that best fits a number of points.  I want to do this using SVD.  To calculate the SVD: Subtract the centroid of the points from each point. Put the points in an mx3 matrix. Calculate the SVD (e.g. [U, S, V] = SVD(A)). The last column of V, (e.g. V(:,3)), is supposed to be a normal vector to the plane.  While the other two columns of V (e.g. V(:,1) and V(:,2)) are vectors parallel to the plane (and orthogonal to each other).  I want to find the equation of the plane in ax+by+cz+d=0 form.  The last column of V (e.g. V(:,3)) gives ""a"", ""b"", and ""c"", however, in order to find ""d"", I need a point on the plane to plug in and solve for d.  The problem is that I don't know what are valid points to use to plug in. My question is: does the centroid of the points necessarily lie on the fitted plane? If so, then it's easy to just plug in the centroid values in the equation (along with the  from the norm) and solve for ""d"".  Otherwise, how can I calculate ""d"" in the above equation? The matrix U apparently gives the point values but I don't understand which values to take.",,"['statistics', 'svd', 'principal-component-analysis']"
16,Unfair Gambler's Ruin problem,Unfair Gambler's Ruin problem,,"Here is the problem I am currently working on: In (a), why do we care about $E[2^{W_t}]$ and $E[2^{W_{t+1}}]$ ? Why not just $E[W_{t+1}], E[W_t]$ ?","Here is the problem I am currently working on: In (a), why do we care about and ? Why not just ?","E[2^{W_t}] E[2^{W_{t+1}}] E[W_{t+1}], E[W_t]","['probability', 'statistics']"
17,"Let $X$ and $Y$ be random variables. Suppose $f_{X,Y}(x,y)=g(x)h(y)$. Prove that $X$ and $Y$ are independent.",Let  and  be random variables. Suppose . Prove that  and  are independent.,"X Y f_{X,Y}(x,y)=g(x)h(y) X Y","Let $X$ and $Y$ be jointly absolutely continuous random variables. Suppose $f_{X,Y}(x,y)=g(x)h(y)$ . Prove that $X$ and $Y$ are independent. So I want to show that $f_X(x)=g(x)$ and that $f_Y(y)=h(y)$ And I have that $$\int_{-\infty}^\infty f_{X,Y}(x,y)dy=f_X(x)=\int_{-\infty}^\infty g(x)h(y)dy=g(x)\int_{-\infty}^\infty h(y)dy$$ and similarly $$f_Y(y)=\int_{-\infty}^\infty f_{X,Y}(x,y)dx=h(y)\int_{-\infty}^\infty g(x)dx$$ But to say $$\int_{-\infty}^\infty g(x)dx=1$$ I would have to know that $g(x)$ is the density function already.",Let and be jointly absolutely continuous random variables. Suppose . Prove that and are independent. So I want to show that and that And I have that and similarly But to say I would have to know that is the density function already.,"X Y f_{X,Y}(x,y)=g(x)h(y) X Y f_X(x)=g(x) f_Y(y)=h(y) \int_{-\infty}^\infty f_{X,Y}(x,y)dy=f_X(x)=\int_{-\infty}^\infty g(x)h(y)dy=g(x)\int_{-\infty}^\infty h(y)dy f_Y(y)=\int_{-\infty}^\infty f_{X,Y}(x,y)dx=h(y)\int_{-\infty}^\infty g(x)dx \int_{-\infty}^\infty g(x)dx=1 g(x)",['statistics']
18,Turán’s Graph Theorem: Motivation behind the Weight Function,Turán’s Graph Theorem: Motivation behind the Weight Function,,"If a graph $G = (V, E)$ on $n$ vertices has no $p$ -clique, $p \geq 2$ , then $$|E| \leq \Big(1- \frac{1}{p-1}\Big) \frac{n^2}{2} \;\;\; \;\;\; \;\;\;(1)$$ We get a proof from the book ""Proofs from THE BOOK"" as given below Now I would like to understand the motivation of the function that was being maximized - $$f(w)=\sum_{v_i v_j \in E} w_i w_j$$ The function is used to write $\frac{|E|}{n^2}$ , but for uniform distribution it is more intuitive to write, $\frac{|E|\times 2}{n}$ , since each edge has two vertices, with each node having $1/n$ weight, thus each edge would have weight $1/n+1/n=2/n$ , it implies $\frac{|E|\times 2}{n}$ . to follow this line of thinking we have to change the function for the above part of the proof, I don't know whether it yields the same result, but I wonder why it is not used? why $f(w)=\sum_{v_i v_j \in E} w_i w_j$ is used instead of $$f(w)=\sum_{v_i v_j \in E} w_i + w_j$$ ? What is the motivation ?","If a graph on vertices has no -clique, , then We get a proof from the book ""Proofs from THE BOOK"" as given below Now I would like to understand the motivation of the function that was being maximized - The function is used to write , but for uniform distribution it is more intuitive to write, , since each edge has two vertices, with each node having weight, thus each edge would have weight , it implies . to follow this line of thinking we have to change the function for the above part of the proof, I don't know whether it yields the same result, but I wonder why it is not used? why is used instead of ? What is the motivation ?","G = (V, E) n p p \geq 2 |E| \leq \Big(1- \frac{1}{p-1}\Big) \frac{n^2}{2} \;\;\; \;\;\; \;\;\;(1) f(w)=\sum_{v_i v_j \in E} w_i w_j \frac{|E|}{n^2} \frac{|E|\times 2}{n} 1/n 1/n+1/n=2/n \frac{|E|\times 2}{n} f(w)=\sum_{v_i v_j \in E} w_i w_j f(w)=\sum_{v_i v_j \in E} w_i + w_j","['combinatorics', 'statistics', 'graph-theory', 'proof-explanation', 'motivation']"
19,How to sample points uniformly from a finite volume region?,How to sample points uniformly from a finite volume region?,,"Lets say we have a finite volume, but not necessarily bounded, region in $\mathbb{R}^d$ like $R = \{(x, y) \in \mathbb{R}^2 \mid x \in (0, 1)\, y \in (1, 1/\sqrt{x})\}$ . This has finite area since $\int_0^1 \frac{1}{\sqrt{x}} - 1\,dx = 1$ . How can one sample points uniformly from that region?","Lets say we have a finite volume, but not necessarily bounded, region in like . This has finite area since . How can one sample points uniformly from that region?","\mathbb{R}^d R = \{(x, y) \in \mathbb{R}^2 \mid x \in (0, 1)\, y \in (1, 1/\sqrt{x})\} \int_0^1 \frac{1}{\sqrt{x}} - 1\,dx = 1",['probability']
20,"If $X_1,\ldots,X_n$ are i.i.d Exponential$(\theta)$, then $X_1/\bar{X}$ is an ancillary statistic","If  are i.i.d Exponential, then  is an ancillary statistic","X_1,\ldots,X_n (\theta) X_1/\bar{X}","Here is what I have done so far: Consider the random vector $X=(X_1,\ldots,X_n)$ which has pdf $$f(x_1,\ldots,x_n; \theta)=\theta^n e^{-\theta(x_1+\cdots+x_n)}.$$ Let $Y=T(X)$ where $T$ be the transformation that sends $(x_1,...,x_n)$ to $(nx_1/(x_1+\cdots+x_n),x_2,x_3,\cdots,x_n)$ . The the pdf of $Y$ is $$\theta^n\frac{n(y_2+\cdots+y_n)}{(n-y_1)^2}\exp\left\{ \frac{n\theta(y_2+\cdots+y_n)}{n-y_1}\right\}.$$ Thus, to get the pdf of $X_1/\bar{X}$ I just need to integrate out the $y_2,...,y_n$ to show that the pdf is independent of $\theta$ . And this is where I got stuck. I have tried it with $n=2$ and it worked, but integrating out this $y_2,...,y_n$ , I got....lazy. :D Do you know a better way for this problem?","Here is what I have done so far: Consider the random vector which has pdf Let where be the transformation that sends to . The the pdf of is Thus, to get the pdf of I just need to integrate out the to show that the pdf is independent of . And this is where I got stuck. I have tried it with and it worked, but integrating out this , I got....lazy. :D Do you know a better way for this problem?","X=(X_1,\ldots,X_n) f(x_1,\ldots,x_n; \theta)=\theta^n e^{-\theta(x_1+\cdots+x_n)}. Y=T(X) T (x_1,...,x_n) (nx_1/(x_1+\cdots+x_n),x_2,x_3,\cdots,x_n) Y \theta^n\frac{n(y_2+\cdots+y_n)}{(n-y_1)^2}\exp\left\{ \frac{n\theta(y_2+\cdots+y_n)}{n-y_1}\right\}. X_1/\bar{X} y_2,...,y_n \theta n=2 y_2,...,y_n","['statistics', 'probability-distributions', 'exponential-distribution']"
21,Minimize $\sum_{i=1}^{n}a_i^2\sigma_i^2$ subject to $\sum_{i=1}^{n}a_i=1$,Minimize  subject to,\sum_{i=1}^{n}a_i^2\sigma_i^2 \sum_{i=1}^{n}a_i=1,"I am having a little issue with this problem. I know that if we were to only have $$\sum_{i=1}^{n}a_i^2$$ subject to $$\sum_{i=1}^{n}a_i=1$$ then we can use Lagrange multipliers, and we get $a_i=1/n$ for all $i$ . In this case we have that the $\sigma_{i}$ 's are known constants, but they are not necessarily equal to each other, so I cannot really do anything else with those. I know the answer is $$a_i=\frac{(1/\sigma_i^2)}{\sum_{i=1}^{n}(1/\sigma_i^2)}$$ but I have tried to use this and have not had any luck thus far. Any help given is much appreciated. Thank you.","I am having a little issue with this problem. I know that if we were to only have subject to then we can use Lagrange multipliers, and we get for all . In this case we have that the 's are known constants, but they are not necessarily equal to each other, so I cannot really do anything else with those. I know the answer is but I have tried to use this and have not had any luck thus far. Any help given is much appreciated. Thank you.",\sum_{i=1}^{n}a_i^2 \sum_{i=1}^{n}a_i=1 a_i=1/n i \sigma_{i} a_i=\frac{(1/\sigma_i^2)}{\sum_{i=1}^{n}(1/\sigma_i^2)},"['calculus', 'statistics', 'multivariable-calculus', 'statistical-inference']"
22,Use proof by contradiction to prove markov's inequality,Use proof by contradiction to prove markov's inequality,,"Markov's inequality says: $P(X\geq \alpha ) \leq \frac{E[X] }{\alpha} $ I know the actual proof of this theorem, but i was talking to someone who used a proof by contradiction to prove this. Suppose $P(X\geq \alpha ) > \frac{E[X] }{\alpha} $ and assume that X can only take on values larger than alpha, then $\alpha>E[X]$ is the contradiction. I think this is incorrect because it does not take into account the possibility that there is no relationship between the left and right hand side at all. Is this right? Under what circumstances can you use proof by contradiction?","Markov's inequality says: I know the actual proof of this theorem, but i was talking to someone who used a proof by contradiction to prove this. Suppose and assume that X can only take on values larger than alpha, then is the contradiction. I think this is incorrect because it does not take into account the possibility that there is no relationship between the left and right hand side at all. Is this right? Under what circumstances can you use proof by contradiction?",P(X\geq \alpha ) \leq \frac{E[X] }{\alpha}  P(X\geq \alpha ) > \frac{E[X] }{\alpha}  \alpha>E[X],"['probability', 'statistics']"
23,What does P@1 mean in this scientific article?,What does P@1 mean in this scientific article?,,"In this scientific article: Label Filters for Large Scale Multilabel Classification by Alexandru Niculescu-Mizil and  Ehsan Abbasnejad, they use the notations P@1, P@5 and P@10 as the following table shows I was thinking that they were $p$ -values at the beginning, but it somehow doesn't make sense. Could someone explain me what this notations mean?","In this scientific article: Label Filters for Large Scale Multilabel Classification by Alexandru Niculescu-Mizil and  Ehsan Abbasnejad, they use the notations P@1, P@5 and P@10 as the following table shows I was thinking that they were -values at the beginning, but it somehow doesn't make sense. Could someone explain me what this notations mean?",p,"['statistics', 'notation']"
24,"Deriving MLE of $\theta$ in $\text{Exp}(\theta,\theta)$ distribution [duplicate]",Deriving MLE of  in  distribution [duplicate],"\theta \text{Exp}(\theta,\theta)","This question already has an answer here : Maximum likelihood estimator is sample mean or minimum ordered statistic? (1 answer) Closed 4 years ago . Suppose $X_1,X_2,\ldots,X_n$ are i.i.d variables having a two-parameter exponential distribution with common location and scale parameter $\theta$ : $$f_{\theta}(x)=\frac{1}{\theta}e^{-(x-\theta)/\theta}\mathbf1_{x>\theta}\quad,\,\theta>0$$ I am wondering if it is possible to derive a maximum likelihood estimator (MLE) of $\theta$ . The likelihood function given the sample $x_1,\ldots,x_n$ is $$L(\theta)=\frac{1}{\theta^n}e^{-n(\bar x-\theta)/\theta}\mathbf1_{x_{(1)}>\theta}\quad,\,\theta>0$$ , where $\bar x=\frac{1}{n}\sum\limits_{i=1}^n x_i$ and $x_{(1)}=\min\limits_{1\le i\le n} x_i$ . Since $L(\theta)$ is not differentiable at $\theta=x_{(1)}$ , I cannot apply the second-derivative test here. Even if I could say that $L(\theta)$ is increasing and/or decreasing in $\theta$ under the constraint $\theta<x_{(1)}$ , I am not sure what choice of $\theta$ maximises $L(\theta)$ . Differentiation is not valid as I understand. I think it is safe to assume $x_{(1)}<\bar x$ , so the constraint is actually $\theta<x_{(1)}<\bar x$ . If MLE is unique, then it is likely to be a function of the sufficient statistic $(\overline X,X_{(1)})$ . However I don't see how to derive it in this particular model. Any suggestion would be great.","This question already has an answer here : Maximum likelihood estimator is sample mean or minimum ordered statistic? (1 answer) Closed 4 years ago . Suppose are i.i.d variables having a two-parameter exponential distribution with common location and scale parameter : I am wondering if it is possible to derive a maximum likelihood estimator (MLE) of . The likelihood function given the sample is , where and . Since is not differentiable at , I cannot apply the second-derivative test here. Even if I could say that is increasing and/or decreasing in under the constraint , I am not sure what choice of maximises . Differentiation is not valid as I understand. I think it is safe to assume , so the constraint is actually . If MLE is unique, then it is likely to be a function of the sufficient statistic . However I don't see how to derive it in this particular model. Any suggestion would be great.","X_1,X_2,\ldots,X_n \theta f_{\theta}(x)=\frac{1}{\theta}e^{-(x-\theta)/\theta}\mathbf1_{x>\theta}\quad,\,\theta>0 \theta x_1,\ldots,x_n L(\theta)=\frac{1}{\theta^n}e^{-n(\bar x-\theta)/\theta}\mathbf1_{x_{(1)}>\theta}\quad,\,\theta>0 \bar x=\frac{1}{n}\sum\limits_{i=1}^n x_i x_{(1)}=\min\limits_{1\le i\le n} x_i L(\theta) \theta=x_{(1)} L(\theta) \theta \theta<x_{(1)} \theta L(\theta) x_{(1)}<\bar x \theta<x_{(1)}<\bar x (\overline X,X_{(1)})","['statistics', 'probability-distributions', 'optimization', 'maximum-likelihood', 'exponential-distribution']"
25,prove problem 12.1 in a probabilistic theory of pattern recognition,prove problem 12.1 in a probabilistic theory of pattern recognition,,"I try to prove Problem 12.1 in A Probabilistic Theory of Pattern Recognition by Luc Devroye. I follow the hint provided in the book. If $Z$ is non-negative r.v. satisfying $P\{Z > t \} \leq c e^{-2 nt^2}$ , then $E\{Z^2\}\leq \frac{ \log(ce)}{2n}$ Hint: $E[Z^2] = \int_0^{\infty} P\{Z^2>t\}dt$ $\int_0^\infty=\int_0^u+\int_u^\infty$ bound the first integral by $u$ , and bound the second integral by exponential inequality Fine u minimizing the upper bound When I follow the second step, I try to break $P\{Z^2>t\}$ into $P\{Z>\sqrt{t}\}+P\{Z<-\sqrt{t}\}$ . Then, $P\{Z<-\sqrt{t}\}$ should be zero.Thus, $P\{Z \leq \sqrt{t} \} \leq ce^{-2nt}$ . Finally, I get $E[Z^2] \leq \frac{c}{2n}$ . It doesn't match the right result. Where should I correct in order to achieve the right way?","I try to prove Problem 12.1 in A Probabilistic Theory of Pattern Recognition by Luc Devroye. I follow the hint provided in the book. If is non-negative r.v. satisfying , then Hint: bound the first integral by , and bound the second integral by exponential inequality Fine u minimizing the upper bound When I follow the second step, I try to break into . Then, should be zero.Thus, . Finally, I get . It doesn't match the right result. Where should I correct in order to achieve the right way?",Z P\{Z > t \} \leq c e^{-2 nt^2} E\{Z^2\}\leq \frac{ \log(ce)}{2n} E[Z^2] = \int_0^{\infty} P\{Z^2>t\}dt \int_0^\infty=\int_0^u+\int_u^\infty u P\{Z^2>t\} P\{Z>\sqrt{t}\}+P\{Z<-\sqrt{t}\} P\{Z<-\sqrt{t}\} P\{Z \leq \sqrt{t} \} \leq ce^{-2nt} E[Z^2] \leq \frac{c}{2n},"['probability', 'statistics', 'inequality']"
26,Correlation between two sequences of irrational numbers,Correlation between two sequences of irrational numbers,,"Let us consider the sequence $x(n+1) = \{b+x(n)\}$ with $x(0) = 0$ . Here the brackets represent the fractional part function. Thus $x(n)= \{nb\}$ is related to Beatty sequences. If $b$ is irrational, it is known that the numbers $x(n)$ are uniformly distributed, with a lag- $k$ auto-correlation equal to $6(b+k)^2 + 6(1-2\lfloor b+k+1\rfloor)(b+k)+6\lfloor b+k+1\rfloor^2 -6\lfloor b+k+1\rfloor+1$ . This result follows from section 5.4 in this article as well as results published here . Also, if $b_1$ and $b_2$ are two irrational numbers that are linearly independent over the set of rational numbers, then the correlation between the two sequences (one generated with $b_1$ and the other one with $b_2$ ) is zero. But what if $b_1 = -1 + \sqrt{5}/2$ and $b_2 = 2/\sqrt{5}$ ? In that case, I know with absolute certainty (yet with no proof so far) that the correlation between the two sequences is 1/20 = 0.05. How do you prove this result?","Let us consider the sequence with . Here the brackets represent the fractional part function. Thus is related to Beatty sequences. If is irrational, it is known that the numbers are uniformly distributed, with a lag- auto-correlation equal to . This result follows from section 5.4 in this article as well as results published here . Also, if and are two irrational numbers that are linearly independent over the set of rational numbers, then the correlation between the two sequences (one generated with and the other one with ) is zero. But what if and ? In that case, I know with absolute certainty (yet with no proof so far) that the correlation between the two sequences is 1/20 = 0.05. How do you prove this result?","x(n+1) = \{b+x(n)\} x(0) = 0 x(n)= \{nb\} b x(n) k 6(b+k)^2 + 6(1-2\lfloor b+k+1\rfloor)(b+k)+6\lfloor b+k+1\rfloor^2
-6\lfloor b+k+1\rfloor+1 b_1 b_2 b_1 b_2 b_1 = -1 + \sqrt{5}/2 b_2 = 2/\sqrt{5}","['probability', 'sequences-and-series', 'number-theory', 'statistics', 'irrational-numbers']"
27,Lower bounds on sum of squared sub-gaussians,Lower bounds on sum of squared sub-gaussians,,"Letting $\left\{X_{i}\right\}_{i=1}^{n}$ be an i.i.d. sequence of zero-mean sub-Gaussian variables with parameter $\sigma,$ define $Z_{n} :=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} .$ Prove that $$ \mathbb{P}\left[Z_{n} \leq \mathbb{E}\left[Z_{n}\right]-\sigma^{2} \delta\right] \leq e^{-n \delta^{2} / 16} \quad \text { for all } \delta \geq 0 $$ I know some results which says that the square of sub-gaussian variable are sub-exponential and maybe apply Chernoff bounds to the sum. But I am still not be able to prove this formally. Any hints will be very helpful.",Letting be an i.i.d. sequence of zero-mean sub-Gaussian variables with parameter define Prove that I know some results which says that the square of sub-gaussian variable are sub-exponential and maybe apply Chernoff bounds to the sum. But I am still not be able to prove this formally. Any hints will be very helpful.,"\left\{X_{i}\right\}_{i=1}^{n} \sigma, Z_{n} :=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} . 
\mathbb{P}\left[Z_{n} \leq \mathbb{E}\left[Z_{n}\right]-\sigma^{2} \delta\right] \leq e^{-n \delta^{2} / 16} \quad \text { for all } \delta \geq 0
","['statistics', 'inequality', 'upper-lower-bounds']"
28,MLE of simultaneous exponential distributions,MLE of simultaneous exponential distributions,,"Given the $X_i\sim \text{exp}({\theta})$ and $Y_i\sim \text{exp}(\frac{1}{\theta})$ , where $X_i$ and $Y_i$ are indpendent, with the same $\theta>0$ . I have to find the MLE and its distribution. I find the simultaneous distribution to be: $$f_{X_i,Y_i}(x,y)=e^{-\left(\frac{x_i}{\theta }+\theta  y_i\right)}1_{(0,\infty )}(x_i)1_{(0,\infty )}(y_i),$$ and the Fisherinformation: $$i_n(\theta)=\frac{2 n}{\theta ^2}$$ As it's convex, the maximum of the loglikelihood must be a minima, so I find the MLE: $$\hat{\theta}=\left(\frac{X_{\bullet }}{Y_{\bullet }}\right)^{0.5},$$ where $X_{\bullet }=\sum _{i=1}^n x_i$ and $Y_{\bullet }=\sum _{i=1}^n y_i$ . So the asymptotic distribution is: $$\hat{\theta}\sim\mathcal{N}\left(\theta ,\frac{\theta^2}{2n^2}\right),$$ as $\hat{\theta}\approx\mathcal{N}\left(\theta,\frac{1}{ni_n(\theta)}\right),$ where $i_\theta$ is the Fisherinformation. I am in doubt whether this is the right procedure. EDIT It is the right procedure - my professor provided the correct solution.","Given the and , where and are indpendent, with the same . I have to find the MLE and its distribution. I find the simultaneous distribution to be: and the Fisherinformation: As it's convex, the maximum of the loglikelihood must be a minima, so I find the MLE: where and . So the asymptotic distribution is: as where is the Fisherinformation. I am in doubt whether this is the right procedure. EDIT It is the right procedure - my professor provided the correct solution.","X_i\sim \text{exp}({\theta}) Y_i\sim \text{exp}(\frac{1}{\theta}) X_i Y_i \theta>0 f_{X_i,Y_i}(x,y)=e^{-\left(\frac{x_i}{\theta }+\theta  y_i\right)}1_{(0,\infty )}(x_i)1_{(0,\infty )}(y_i), i_n(\theta)=\frac{2 n}{\theta ^2} \hat{\theta}=\left(\frac{X_{\bullet }}{Y_{\bullet }}\right)^{0.5}, X_{\bullet }=\sum _{i=1}^n x_i Y_{\bullet }=\sum _{i=1}^n y_i \hat{\theta}\sim\mathcal{N}\left(\theta ,\frac{\theta^2}{2n^2}\right), \hat{\theta}\approx\mathcal{N}\left(\theta,\frac{1}{ni_n(\theta)}\right), i_\theta","['statistics', 'probability-distributions', 'maximum-likelihood', 'exponential-distribution', 'fisher-information']"
29,Inference regarding the mean lifetime of a bulb using a new technique,Inference regarding the mean lifetime of a bulb using a new technique,,"The lifetime in hours of each bulb manufactured by a particular company follows an independent exponential distribution with mean $\lambda$ . We need to test the null hypothesis $H_0: \lambda=1000$ against $H_1:\lambda=500$ . A statistician sets up an experiment with $50$ bulbs, with $5$ bulbs in each of $10$ different locations, to examine their lifetimes. To get quick preliminary results,the statistician decides to stop the experiment as soon as one bulb fails at each location.Let $Y_i$ denote the lifetime of the first bulb to fail at location $i$ .Obtain the most powerful test of $H_0$ against $H_1$ based on $Y_1,Y_2,...Y_{10}$ and compute its power. Now, I have problems regarding how to formulate the condition that a particular bulb has failed. Otherwise, how can I write the likelihood function in either of the hypotheses?","The lifetime in hours of each bulb manufactured by a particular company follows an independent exponential distribution with mean . We need to test the null hypothesis against . A statistician sets up an experiment with bulbs, with bulbs in each of different locations, to examine their lifetimes. To get quick preliminary results,the statistician decides to stop the experiment as soon as one bulb fails at each location.Let denote the lifetime of the first bulb to fail at location .Obtain the most powerful test of against based on and compute its power. Now, I have problems regarding how to formulate the condition that a particular bulb has failed. Otherwise, how can I write the likelihood function in either of the hypotheses?","\lambda H_0: \lambda=1000 H_1:\lambda=500 50 5 10 Y_i i H_0 H_1 Y_1,Y_2,...Y_{10}","['statistics', 'probability-distributions']"
30,Do 50% of people die on either side of an average life expectancy?,Do 50% of people die on either side of an average life expectancy?,,"I am no statistician so this may come across as very naive, but I've been trying to get my head around how to interpret life expectancy data. Say you have a population of $n$ and a range of possible ages from 0– $m$ . Am I right in thinking that given an average life expectancy of $x$ years, roughly 50% of the population must fall on either side of that line (ignoring those who fall directly on it)? As such, given an average life expectancy of 75, a maximum age of 120 and a population of 10,000, 5,000 (ish) would be expected to die between the ages of 0 and 74 and 5,000 (ish) between 76 and 110? And that that second half will be more densely packed than the first half (as it covers 34 years rather than 74 with the same number of people)? Finally, if the above is broadly correct (and if we assume for simplicity's sake that each age has an equal likelihood of fatality), does that mean you are more likely to die on the left side of the average until you make it halfway between 0 and $x$ , whereupon you are equally as likely to die on either side of the average, and each subsequent year survived increases your likelihood of outliving the average? My thinking being that if you have a $\frac{1}{120}$ chance of dying each year (0.83%), each year of the left side is $\frac{1}{74}$ (1.35%) whereas each year on the right side is $\frac{1}{34}$ , or ~ $\frac{2}{74}$ (2.94%).","I am no statistician so this may come across as very naive, but I've been trying to get my head around how to interpret life expectancy data. Say you have a population of and a range of possible ages from 0– . Am I right in thinking that given an average life expectancy of years, roughly 50% of the population must fall on either side of that line (ignoring those who fall directly on it)? As such, given an average life expectancy of 75, a maximum age of 120 and a population of 10,000, 5,000 (ish) would be expected to die between the ages of 0 and 74 and 5,000 (ish) between 76 and 110? And that that second half will be more densely packed than the first half (as it covers 34 years rather than 74 with the same number of people)? Finally, if the above is broadly correct (and if we assume for simplicity's sake that each age has an equal likelihood of fatality), does that mean you are more likely to die on the left side of the average until you make it halfway between 0 and , whereupon you are equally as likely to die on either side of the average, and each subsequent year survived increases your likelihood of outliving the average? My thinking being that if you have a chance of dying each year (0.83%), each year of the left side is (1.35%) whereas each year on the right side is , or ~ (2.94%).",n m x x \frac{1}{120} \frac{1}{74} \frac{1}{34} \frac{2}{74},"['statistics', 'actuarial-science']"
31,Proof of $O_P(1) o_P(1) = o_P(1)$,Proof of,O_P(1) o_P(1) = o_P(1),"Consider $U_n=O_P(1)$ and $X_n=o_P(1)$ . Show that $U_nX_n=o_P(1)$ . Since $o_P(1)$ is equivalent to convergence in probability, it suffices to show that $P(|U_nX_n|>\varepsilon) \to 0$ . However, I am not quite sure if my proof is right: $$P(|U_nX_n|>\varepsilon) = P(|U_nX_n|>\varepsilon, |U_n| \leq M) + P(|U_nX_n|>\varepsilon, |U_n|>M).$$ For the first term on the right-hand side it holds: $$P(|U_nX_n|>\varepsilon, |U_n| \leq M) \leq  P(|U_n| \leq M) < \epsilon.$$ From the definition of $O_P(1)$ we know that for each $\epsilon$ such an $M$ exists. For the second term, it holds that: $$P(|U_nX_n|>\varepsilon, |U_n| > M) \leq P(|MX_n|> \varepsilon) = P\left(|X_n|> \frac \varepsilon  M\right) \to 0.$$ Hence, $P(|U_nX_n|>\varepsilon) < \epsilon$ . Is this correct?","Consider and . Show that . Since is equivalent to convergence in probability, it suffices to show that . However, I am not quite sure if my proof is right: For the first term on the right-hand side it holds: From the definition of we know that for each such an exists. For the second term, it holds that: Hence, . Is this correct?","U_n=O_P(1) X_n=o_P(1) U_nX_n=o_P(1) o_P(1) P(|U_nX_n|>\varepsilon) \to 0 P(|U_nX_n|>\varepsilon) = P(|U_nX_n|>\varepsilon, |U_n| \leq M) + P(|U_nX_n|>\varepsilon, |U_n|>M). P(|U_nX_n|>\varepsilon, |U_n| \leq M) \leq  P(|U_n| \leq M) < \epsilon. O_P(1) \epsilon M P(|U_nX_n|>\varepsilon, |U_n| > M) \leq P(|MX_n|> \varepsilon) = P\left(|X_n|> \frac \varepsilon  M\right) \to 0. P(|U_nX_n|>\varepsilon) < \epsilon","['probability', 'statistics', 'proof-verification', 'convergence-divergence', 'weak-convergence']"
32,What is approximately the distribution of your total earning?,What is approximately the distribution of your total earning?,,"You play a game in a casino: you roll two dice and if the sum of the spots equals seven, you win $5$ €. In every other case, you lose $1$ €. You decide to play this game $120$ times. What is approximately the distribution of your total earning? So the probability that the sum  of two dice $X$ and $Y$ is $7$ can be calculate with: $P(X+Y=7)=\frac{6}{36}=\frac{1}{6}$ considering all the cases: ((1,6), (2,5), (3,4), (4,3), (5,2), (6,1)). Then the probability that the sum of the two dice is not $7$ is $1-\frac{1}{6}=\frac{5}{6}$ . So I think that the distribution should be $Bin(120,\frac{1}{6})$ but from the teacher answer it is $N(0,600)$ , where I'm wrong? Why is not Binomial but Normal?","You play a game in a casino: you roll two dice and if the sum of the spots equals seven, you win €. In every other case, you lose €. You decide to play this game times. What is approximately the distribution of your total earning? So the probability that the sum  of two dice and is can be calculate with: considering all the cases: ((1,6), (2,5), (3,4), (4,3), (5,2), (6,1)). Then the probability that the sum of the two dice is not is . So I think that the distribution should be but from the teacher answer it is , where I'm wrong? Why is not Binomial but Normal?","5 1 120 X Y 7 P(X+Y=7)=\frac{6}{36}=\frac{1}{6} 7 1-\frac{1}{6}=\frac{5}{6} Bin(120,\frac{1}{6}) N(0,600)","['probability', 'statistics', 'dice']"
33,"Is there any meaning of this ""Median-mean""","Is there any meaning of this ""Median-mean""",,"Given a data set $\{a_1,\cdots,a_n\}$ with median M Define the medimean to be the value of $x$ s.t. $$\left(\frac{1}{n}\sum_{n}{}{a_n}^x \right)^{1/x}=M$$ Is this $x$ value useful / used at all in statistics? Some examples $\{\frac{1}{5},\frac{1}{4},\frac{1}{3},\frac{1}{2},\frac{1}{1}\} : x=-1$ $\{2,4,8,16,32\} : x=0$ $\{\sqrt{1},\sqrt{2},\sqrt{3},\sqrt{4},\sqrt{5}\} : x=2$ $\{1,2,3,4,5\} : x=1$ $\{1,4,9,16,25\} : x=\frac{1}{2}$ $\frac{1}{x}$ seems to say something about how the numbers are distributed, apart from when $x=0$ Just an idea I was playing around with yesterday. $\{1,1,1,2,2\} : x=-\infty$ $\{1,1,2,2,2\} : x=\infty$","Given a data set with median M Define the medimean to be the value of s.t. Is this value useful / used at all in statistics? Some examples seems to say something about how the numbers are distributed, apart from when Just an idea I was playing around with yesterday.","\{a_1,\cdots,a_n\} x \left(\frac{1}{n}\sum_{n}{}{a_n}^x \right)^{1/x}=M x \{\frac{1}{5},\frac{1}{4},\frac{1}{3},\frac{1}{2},\frac{1}{1}\} : x=-1 \{2,4,8,16,32\} : x=0 \{\sqrt{1},\sqrt{2},\sqrt{3},\sqrt{4},\sqrt{5}\} : x=2 \{1,2,3,4,5\} : x=1 \{1,4,9,16,25\} : x=\frac{1}{2} \frac{1}{x} x=0 \{1,1,1,2,2\} : x=-\infty \{1,1,2,2,2\} : x=\infty","['statistics', 'statistical-inference', 'descriptive-statistics']"
34,Kendall's Tau of bivariate normal,Kendall's Tau of bivariate normal,,"Prove Kendall's tau of a bivariate normal is given by $$\rho_\tau (X_1,X_2)=\frac{2}{\pi}\arcsin\rho$$ I can derive the bivariate normal as $$F(x_1,x_2)=\frac{1}{2\pi\sqrt{1-\rho^2}}e^{-\frac{1}{2}x_1\Sigma^{-1}x_2}$$ where, for simplicity, I am assuming $X_1$ and $X_2$ have mean $0$ .  I also am aware that the formula for Kendall's tau involves an integral over some domain involving $X_1$ , $X_2$ , and $\rho$ , multiplied by $4$ . So it seems like all the pieces are there; the integral should result in the answer, if only the integral over the exponential term w.r.t. $X_1$ and $X_2$ were $1$ .  But this is not the case, so I am stuck.","Prove Kendall's tau of a bivariate normal is given by I can derive the bivariate normal as where, for simplicity, I am assuming and have mean .  I also am aware that the formula for Kendall's tau involves an integral over some domain involving , , and , multiplied by . So it seems like all the pieces are there; the integral should result in the answer, if only the integral over the exponential term w.r.t. and were .  But this is not the case, so I am stuck.","\rho_\tau (X_1,X_2)=\frac{2}{\pi}\arcsin\rho F(x_1,x_2)=\frac{1}{2\pi\sqrt{1-\rho^2}}e^{-\frac{1}{2}x_1\Sigma^{-1}x_2} X_1 X_2 0 X_1 X_2 \rho 4 X_1 X_2 1","['probability', 'integration', 'statistics', 'correlation']"
35,Is a sample of size $N$ uniquely described by $N$ sample moments?,Is a sample of size  uniquely described by  sample moments?,N N,"I have the intuition that a sample of size $n$ should be able to be described uniquely by just N moments. But I don't know if that's true. The idea is that, first, it is obvious that a sample of $n = 1$ can be uniquely describe by just the sample mean. For $n = 2$ , knowing the first two (uncentered) moments is also enough because (if I did my maths correctly) $$ X_1 = 2\overline{X} - X_2 $$ and $$ M_2 = 1/2X_2^2 - 2M_1X_2+2M_1^2 $$ From which I can solve for $X_2$ and then $X_1$ by knowing $M_1$ and $M_2$ . It seems intuitive that this must hold on for $n > 2 $ . The generalization for a sample of size $n$ and $m$ moments would be \begin{matrix} x_1 + x_2 + ... + x_n = nM_1\\  x_1^2 + x_2^2 +  ... + x_n^2 = nM_2 \\         \vdots \\  x_1^m + x_2^m + ... + x_n^m = nM_m \end{matrix} It these were linear equations, they would only have an unique solution for $n = m$ if the system described a full rank matrix. But they aren't, so... no dice. From an information theory standpoint it seems to me (again, intuitively) that you shouldn't need more than $N$ numbers to represent any other $N$ numbers. Note that I'm not trying to estimate the population distribution by the sample moments. I'm talking about describing a specific vector of $n$ values. Also note that I assume that the order of samples does not matter so that $(x_1, x_2)$ and $(x_2, x_2)$ would be treated as the same solution. Any hints on this problem?","I have the intuition that a sample of size should be able to be described uniquely by just N moments. But I don't know if that's true. The idea is that, first, it is obvious that a sample of can be uniquely describe by just the sample mean. For , knowing the first two (uncentered) moments is also enough because (if I did my maths correctly) and From which I can solve for and then by knowing and . It seems intuitive that this must hold on for . The generalization for a sample of size and moments would be It these were linear equations, they would only have an unique solution for if the system described a full rank matrix. But they aren't, so... no dice. From an information theory standpoint it seems to me (again, intuitively) that you shouldn't need more than numbers to represent any other numbers. Note that I'm not trying to estimate the population distribution by the sample moments. I'm talking about describing a specific vector of values. Also note that I assume that the order of samples does not matter so that and would be treated as the same solution. Any hints on this problem?","n n = 1 n = 2 
X_1 = 2\overline{X} - X_2
 
M_2 = 1/2X_2^2 - 2M_1X_2+2M_1^2
 X_2 X_1 M_1 M_2 n > 2  n m \begin{matrix}
x_1 + x_2 + ... + x_n = nM_1\\ 
x_1^2 + x_2^2 +  ... + x_n^2 = nM_2 \\ 
       \vdots \\
 x_1^m + x_2^m + ... + x_n^m = nM_m
\end{matrix} n = m N N n (x_1, x_2) (x_2, x_2)",['statistics']
36,"You throw a dice 3 times,probabilty whats the probability of the number k to be the highest?","You throw a dice 3 times,probabilty whats the probability of the number k to be the highest?",,"You throw a fair dice 3 times, let X be the highest number you will get. What is the PDF of $P(X=K), K=1,...6$ ? The answer is : $$\frac{k^3-(k-1)^3}{216}$$ However, I can't understand it, could help me to understand why is this the right answer? Thank you","You throw a fair dice 3 times, let X be the highest number you will get. What is the PDF of ? The answer is : However, I can't understand it, could help me to understand why is this the right answer? Thank you","P(X=K), K=1,...6 \frac{k^3-(k-1)^3}{216}","['probability', 'statistics', 'dice']"
37,Birthday problem: why not: combinations without/with replacement??,Birthday problem: why not: combinations without/with replacement??,,My first intuition on the birthday problem was: The number of ways $k$ people have different birthdays is the combinations $\binom{365}{k}$ The number of ways $k$ people can have birthdays is the combinations with replacement . So the probability that all $k$ people have birthdays on different days is $$ P = \frac{\binom{365}{k}}{\binom{365+k-1}{k}}$$ What is wrong with this logic?,My first intuition on the birthday problem was: The number of ways people have different birthdays is the combinations The number of ways people can have birthdays is the combinations with replacement . So the probability that all people have birthdays on different days is What is wrong with this logic?,k \binom{365}{k} k k  P = \frac{\binom{365}{k}}{\binom{365+k-1}{k}},"['combinatorics', 'statistics']"
38,Most powerful hypothesis test for given discrete distribution,Most powerful hypothesis test for given discrete distribution,,"I have two discrete distributions. The second one is a simple distribution where all values from 1 to 6 (think rolling dice) have equal probability (so, that must be $\frac{1}{6}$ ). The first one is defined like this: $f(6) = 0.18$ , $f(5) = 0.14$ , $\ f(4) = f(3) = f(2) = f(1) = 0.17$ . There are two observations. I need to find the most powerful test for a given significance level (0.0196, so this clearly hints at something like getting two 5s in a row, since $f(5)^2 = 0.0196$ ). I only know how to apply the Neyman-Pearson Lemma. First I need to get the likelihood ratio. I can't even do this because I don't understand how to construct the joint probability mass function for the first distribution since it's not a closed-form expression. Maybe it's supposed to be a sort of a multinomial distribution? But the number of observations (2) is less than the number of possible values (6), so how would this work... Then, I need to construct a test based on this likelihood ratio: $$ \phi(x_1, x_2) =  \begin{cases} 1, \lambda(x_1, x_2) > c \\ 0, \lambda(x_1, x_2) < c \\ \alpha, \lambda(x_1, x_2) = c \end{cases} $$ And I do not have a good grasp on how to derive $\alpha$ and $c$ . For $c$ I need to construct an inequality based on the likelihood ratio and use the significance level somehow, but I don't really understand how it's supposed to be done.","I have two discrete distributions. The second one is a simple distribution where all values from 1 to 6 (think rolling dice) have equal probability (so, that must be ). The first one is defined like this: , , . There are two observations. I need to find the most powerful test for a given significance level (0.0196, so this clearly hints at something like getting two 5s in a row, since ). I only know how to apply the Neyman-Pearson Lemma. First I need to get the likelihood ratio. I can't even do this because I don't understand how to construct the joint probability mass function for the first distribution since it's not a closed-form expression. Maybe it's supposed to be a sort of a multinomial distribution? But the number of observations (2) is less than the number of possible values (6), so how would this work... Then, I need to construct a test based on this likelihood ratio: And I do not have a good grasp on how to derive and . For I need to construct an inequality based on the likelihood ratio and use the significance level somehow, but I don't really understand how it's supposed to be done.","\frac{1}{6} f(6) = 0.18 f(5) = 0.14 \ f(4) = f(3) = f(2) = f(1) = 0.17 f(5)^2 = 0.0196 
\phi(x_1, x_2) = 
\begin{cases}
1, \lambda(x_1, x_2) > c \\ 0, \lambda(x_1, x_2) < c \\ \alpha, \lambda(x_1, x_2) = c
\end{cases}
 \alpha c c","['statistics', 'probability-distributions', 'hypothesis-testing']"
39,Words Counting and Ranking,Words Counting and Ranking,,"All words which contain $2, 3, 4$ or $5$ English letters (A to Z) are listed alphabetically. In each word, the letters can be repeated, but any two adjacent letters must be distinct. So, the first word is ""AB"", and the last one is ""ZYZYZ"". What is the total number of words in the list? What is the rank of the word ""PAPER""? What is the $13554$ th word? I have answered the first part as follows; Total number of words $=26(25^1+25^2+25^3+25^4)$ $=26(25+625+15625+390625)=26(406900)=10579400$ words. Am I right for this part? How to solve the second and the third part?","All words which contain or English letters (A to Z) are listed alphabetically. In each word, the letters can be repeated, but any two adjacent letters must be distinct. So, the first word is ""AB"", and the last one is ""ZYZYZ"". What is the total number of words in the list? What is the rank of the word ""PAPER""? What is the th word? I have answered the first part as follows; Total number of words words. Am I right for this part? How to solve the second and the third part?","2, 3, 4 5 13554 =26(25^1+25^2+25^3+25^4) =26(25+625+15625+390625)=26(406900)=10579400","['combinatorics', 'statistics', 'permutations']"
40,"Uniform Distribution: Broken Stick, Ratio of Parts [closed]","Uniform Distribution: Broken Stick, Ratio of Parts [closed]",,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question A stick of length 1 is broken at a uniformly random point, yielding two pieces. Let X and Y be the lengths of the shorter and longer pieces, respectively, and let R = X/Y be the ratio of the lengths X and Y . (a) Find the CDF and PDF of R. (b) Find the expected value of R (if it exists). (c) Find the expected value of 1/R (if it exists) I'm stuck on finding a PMF and CDF of R - how would I set this up?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question A stick of length 1 is broken at a uniformly random point, yielding two pieces. Let X and Y be the lengths of the shorter and longer pieces, respectively, and let R = X/Y be the ratio of the lengths X and Y . (a) Find the CDF and PDF of R. (b) Find the expected value of R (if it exists). (c) Find the expected value of 1/R (if it exists) I'm stuck on finding a PMF and CDF of R - how would I set this up?",,"['probability', 'statistics']"
41,Why is the standard deviation preferred over the mean deviation?,Why is the standard deviation preferred over the mean deviation?,,"To me, the mean deviation, which is the average distance that a data point in a sample lies from the sample's mean, seems a more natural measure of dispersion than the standard deviation; Yet the standard deviation seems to dominate in the field of statistics. I rarely see the mean deviation reported in studies; generally only the sample mean or median and the standard deviation are provided. Question: Why is the standard deviation preferred over the mean deviation as a measure of dispersion?","To me, the mean deviation, which is the average distance that a data point in a sample lies from the sample's mean, seems a more natural measure of dispersion than the standard deviation; Yet the standard deviation seems to dominate in the field of statistics. I rarely see the mean deviation reported in studies; generally only the sample mean or median and the standard deviation are provided. Question: Why is the standard deviation preferred over the mean deviation as a measure of dispersion?",,['statistics']
42,Sufficient statistics for $\lambda$ poisson distribution.,Sufficient statistics for  poisson distribution.,\lambda,"I have the following question out of a book. I even have the solution from solutions manual that I cannot  really follow either, so I thought I would ask here to see if someone could dumb it down for me. Let $Y_!, Y_2, ..., Y_n$ denote a random sample from a Poisson distribution with parameter $\lambda$ . Show by conditioning that $\sum_{i=1}^{n}Y_i$ is sufficient for $\lambda$ So that is the question. From the solution I can see that they are using the conditional probability, but from the chapter that this question is from you are taught that you show sufficiency by using the likelihood. So what does it mean by ""show by conditioning""? What does the act of conditioning do in this instance? Why would we use conditioning instead of likelihood? Also, just how would you do this question?","I have the following question out of a book. I even have the solution from solutions manual that I cannot  really follow either, so I thought I would ask here to see if someone could dumb it down for me. Let denote a random sample from a Poisson distribution with parameter . Show by conditioning that is sufficient for So that is the question. From the solution I can see that they are using the conditional probability, but from the chapter that this question is from you are taught that you show sufficiency by using the likelihood. So what does it mean by ""show by conditioning""? What does the act of conditioning do in this instance? Why would we use conditioning instead of likelihood? Also, just how would you do this question?","Y_!, Y_2, ..., Y_n \lambda \sum_{i=1}^{n}Y_i \lambda",['statistics']
43,"How many integers from 1 through 1000 are divisible by 3 and by at least one of 2,5,7, and 11?","How many integers from 1 through 1000 are divisible by 3 and by at least one of 2,5,7, and 11?",,"So I got 294 for this question, though I'm not a hundred percent sure but let me explain my process: I did to find all the multiples: (floor of each) 1000/6 = 166  1000/15 = 66 1000/21 = 47 1000/33 = 30 166+66+47+30 = 309 to eliminate the repeats: (floor of each) 66/6 = 11  47/6 = 7 30/6 = 5 47/15 = 3 30/15 = 2 30/21 = 1 11+7+5+3+2+1 = 29 to add in the repeats that were eliminated twice over: (floor of each) 5/2 = 2 7/2 = 3  11/2 = 5 7/5 = 1 11/5 = 2 11/7 = 1 2+3+5+1+2+1 = 14 309 - 29 + 14 = 294","So I got 294 for this question, though I'm not a hundred percent sure but let me explain my process: I did to find all the multiples: (floor of each) 1000/6 = 166  1000/15 = 66 1000/21 = 47 1000/33 = 30 166+66+47+30 = 309 to eliminate the repeats: (floor of each) 66/6 = 11  47/6 = 7 30/6 = 5 47/15 = 3 30/15 = 2 30/21 = 1 11+7+5+3+2+1 = 29 to add in the repeats that were eliminated twice over: (floor of each) 5/2 = 2 7/2 = 3  11/2 = 5 7/5 = 1 11/5 = 2 11/7 = 1 2+3+5+1+2+1 = 14 309 - 29 + 14 = 294",,"['probability', 'combinatorics', 'statistics']"
44,Expected value of a simple birth process,Expected value of a simple birth process,,Let $X_t$ be a simple birth process (with birth rate $\lambda_n = n\lambda$) such that $X_0 = 1$. Set $\mu_t = E[X_t]$ and $\nu_t = E[X_t^2]$. Find diﬀerential equations for $\mu_t$ and $\nu_t$ and hence calculate the variance of $X_t$. I've tried using the Kolmogorov forward differential equation to get $\mu_t$ which gave me $e^{-\lambda t}$ but I'm not sure how to calculate $\nu_t$.,Let $X_t$ be a simple birth process (with birth rate $\lambda_n = n\lambda$) such that $X_0 = 1$. Set $\mu_t = E[X_t]$ and $\nu_t = E[X_t^2]$. Find diﬀerential equations for $\mu_t$ and $\nu_t$ and hence calculate the variance of $X_t$. I've tried using the Kolmogorov forward differential equation to get $\mu_t$ which gave me $e^{-\lambda t}$ but I'm not sure how to calculate $\nu_t$.,,"['statistics', 'stochastic-processes', 'poisson-process']"
45,How can I calculate the odds of winning successive games of paper/scissors/rock?,How can I calculate the odds of winning successive games of paper/scissors/rock?,,"A friend and I have a game we play, and when we draw or call at the same time we use one round of paper/scissors/rock to determine the winner. I am currently 9-0 up on him and was wondering what the mathematical odds are of that? Is it as simple as $\frac{1}{3\times9}$? Or is there more to it? :edit: they were all individual times, we have only played 9 times, separated by weeks, each time there were no draws, I beat him first time every time, I'm not concious of any ""tactics"" that help win games of p/s/r, to my knowledge we are both playing completely randomly.","A friend and I have a game we play, and when we draw or call at the same time we use one round of paper/scissors/rock to determine the winner. I am currently 9-0 up on him and was wondering what the mathematical odds are of that? Is it as simple as $\frac{1}{3\times9}$? Or is there more to it? :edit: they were all individual times, we have only played 9 times, separated by weeks, each time there were no draws, I beat him first time every time, I'm not concious of any ""tactics"" that help win games of p/s/r, to my knowledge we are both playing completely randomly.",,"['probability', 'statistics']"
46,Find an estimate for $p=P(X\geqq 20)$,Find an estimate for,p=P(X\geqq 20),"I have this problem I'm stuck with. I would really appreciate some help. ""The following is a sample from a normal distribution. $7.6 , 9.6, 10.4, 10.7, 11.9, 14.1, 14.6, 18.5$ a) Let $X$ have this normal distribution and let $p=P(X\geqq 20)$. If   we estimate $p$ by the relative frequency, we just get 0. Suggest   another estimate. b)Let $x$ be the 95th percentil, i.e. a value such that $P(X \leqq x) > = 0.95$. Suggest an estimate for $x$."" This seems fairly straight forward. I dont get the same answer as my text book however. I'm thinking since $X\sim N(\mu,\sigma ^2)$ we can estimate $\mu = \bar{X} = 12.175$, and $\sigma ^2 = s^2 =\frac{1}{n-1}\sum_{k=1}^{n}(X_k-\bar{X})^2 \approx 11.79 \implies s \approx 3.43$. So far so good. Now $p = P(X\geqq 20) = 1- P(\frac{X-\bar{X}}{s}\leqq\frac{20-\bar{X}}{s})$ where $\frac{X-\bar{X}}{s} \sim F_{t_6}(0,1)$, i.e. a T-distribution of $n-2 = 8-2 = 6$ degrees of freedom. (This is where I'm uncertain whether this is correct). This leads to $p = 1-F_{t_6}(2.28)$ which gives the wrong answer according to the book. In the same way does the (b)-part go wrong as well. The main question is how to use the T-distribution correctly. Is the T-distribution even the right choice in this problem? I would very much appreciate some help because my text book dont explain this types of problems very well. Cheers!","I have this problem I'm stuck with. I would really appreciate some help. ""The following is a sample from a normal distribution. $7.6 , 9.6, 10.4, 10.7, 11.9, 14.1, 14.6, 18.5$ a) Let $X$ have this normal distribution and let $p=P(X\geqq 20)$. If   we estimate $p$ by the relative frequency, we just get 0. Suggest   another estimate. b)Let $x$ be the 95th percentil, i.e. a value such that $P(X \leqq x) > = 0.95$. Suggest an estimate for $x$."" This seems fairly straight forward. I dont get the same answer as my text book however. I'm thinking since $X\sim N(\mu,\sigma ^2)$ we can estimate $\mu = \bar{X} = 12.175$, and $\sigma ^2 = s^2 =\frac{1}{n-1}\sum_{k=1}^{n}(X_k-\bar{X})^2 \approx 11.79 \implies s \approx 3.43$. So far so good. Now $p = P(X\geqq 20) = 1- P(\frac{X-\bar{X}}{s}\leqq\frac{20-\bar{X}}{s})$ where $\frac{X-\bar{X}}{s} \sim F_{t_6}(0,1)$, i.e. a T-distribution of $n-2 = 8-2 = 6$ degrees of freedom. (This is where I'm uncertain whether this is correct). This leads to $p = 1-F_{t_6}(2.28)$ which gives the wrong answer according to the book. In the same way does the (b)-part go wrong as well. The main question is how to use the T-distribution correctly. Is the T-distribution even the right choice in this problem? I would very much appreciate some help because my text book dont explain this types of problems very well. Cheers!",,"['probability', 'statistics', 'estimation', 'parameter-estimation']"
47,Convergence in distribution: Show $ \lim_{n \rightarrow \infty} \sup \mathbb{P} (x^n \in C) \leq \ \mathbb{P}(x \in C) $,Convergence in distribution: Show, \lim_{n \rightarrow \infty} \sup \mathbb{P} (x^n \in C) \leq \ \mathbb{P}(x \in C) ,"Consider random vectors $(x^n), x \in \mathbb{R}^q$ defined on a probability space $(\Omega,A,\mathbb{P})$. Show if $x^n   \overset{d}{\rightarrow} x  $, it holds for any closed set $C$ that $$ \lim_{n \rightarrow \infty} \sup \mathbb{P} (x^n \in C) \leq \mathbb{P}(x \in C) $$ My thoughts: So obviously we have to use $x^n   \overset{d}{\rightarrow} x $. So here is the definition of convergence in distribution: We say that $(x^n) \in \mathbb{R}^q$ converges in distribution to $x \in \mathbb{R}^q$ if for the corresponding distribution functions $F_n$ and $F$, respectively, and for every continuity point $a \in \mathbb{R}^q$ of $F$, it holds that $\lim_{n \rightarrow \infty} F_n(a) = F(a)$ . And  $C$ closed means that $C^c$ ( complement of $C$ ) is open. Maybe this could be helpful? Maybe we could show $ \lim_{n \rightarrow \infty} \inf \mathbb{P} (x^n \in O) \geq \mathbb{P}(x \in O) $ ( $O$ open set ) to verify the claim above. But how can I show it? And how can I use it to show $ \lim_{n \rightarrow \infty} \sup \mathbb{P} (x^n \in C) \leq \mathbb{P}(x \in C) $ ? I know that $C$ is closed if and only if $C^c$ is open. Moreover I know that $\mathbb{P(A)} =  1-\mathbb{P(A^c)} $.  Or do you have another simple idea to solve this exercise? Update : I've almost showed that $ \lim_{n \rightarrow \infty} \inf \mathbb{P} (x^n \in O) \geq \mathbb{P}(x \in O) $, can someone explain me why $\lim_{n \rightarrow \infty} \inf  \mathbb{1}_O(x^n) \geq 1_O(x) $ is true for an open set $O$?  My assumption that $ \lim_{n \rightarrow \infty} \inf \mathbb{P} (x^n \in O) \geq \mathbb{P}(x \in O) $ ( $O$ open ) implies $ \lim_{n \rightarrow \infty} \sup \mathbb{P} (x^n \in C) \leq \mathbb{P}(x \in C) $.  According to a lecture I found : This follows upon noting that $C$ is closed if and only if $C^c$ is open, and that $\mathbb{P}(C) = 1 − \mathbb{P}(C^c)$ Can someone explain me this in detail?","Consider random vectors $(x^n), x \in \mathbb{R}^q$ defined on a probability space $(\Omega,A,\mathbb{P})$. Show if $x^n   \overset{d}{\rightarrow} x  $, it holds for any closed set $C$ that $$ \lim_{n \rightarrow \infty} \sup \mathbb{P} (x^n \in C) \leq \mathbb{P}(x \in C) $$ My thoughts: So obviously we have to use $x^n   \overset{d}{\rightarrow} x $. So here is the definition of convergence in distribution: We say that $(x^n) \in \mathbb{R}^q$ converges in distribution to $x \in \mathbb{R}^q$ if for the corresponding distribution functions $F_n$ and $F$, respectively, and for every continuity point $a \in \mathbb{R}^q$ of $F$, it holds that $\lim_{n \rightarrow \infty} F_n(a) = F(a)$ . And  $C$ closed means that $C^c$ ( complement of $C$ ) is open. Maybe this could be helpful? Maybe we could show $ \lim_{n \rightarrow \infty} \inf \mathbb{P} (x^n \in O) \geq \mathbb{P}(x \in O) $ ( $O$ open set ) to verify the claim above. But how can I show it? And how can I use it to show $ \lim_{n \rightarrow \infty} \sup \mathbb{P} (x^n \in C) \leq \mathbb{P}(x \in C) $ ? I know that $C$ is closed if and only if $C^c$ is open. Moreover I know that $\mathbb{P(A)} =  1-\mathbb{P(A^c)} $.  Or do you have another simple idea to solve this exercise? Update : I've almost showed that $ \lim_{n \rightarrow \infty} \inf \mathbb{P} (x^n \in O) \geq \mathbb{P}(x \in O) $, can someone explain me why $\lim_{n \rightarrow \infty} \inf  \mathbb{1}_O(x^n) \geq 1_O(x) $ is true for an open set $O$?  My assumption that $ \lim_{n \rightarrow \infty} \inf \mathbb{P} (x^n \in O) \geq \mathbb{P}(x \in O) $ ( $O$ open ) implies $ \lim_{n \rightarrow \infty} \sup \mathbb{P} (x^n \in C) \leq \mathbb{P}(x \in C) $.  According to a lecture I found : This follows upon noting that $C$ is closed if and only if $C^c$ is open, and that $\mathbb{P}(C) = 1 − \mathbb{P}(C^c)$ Can someone explain me this in detail?",,"['statistics', 'convergence-divergence', 'limsup-and-liminf']"
48,"How to see wether numbers are distributed ""evenly"" ([1,2,18,35,36]) or ""cluttered to one side"" ([1,2,3,30,31], [7,9,17,16,36])?","How to see wether numbers are distributed ""evenly"" ([1,2,18,35,36]) or ""cluttered to one side"" ([1,2,3,30,31], [7,9,17,16,36])?",,"I have a set of 5 integer numbers {1,23, 17, 33, 35}. Elements can take values only from [1..36], and happen only once within the set. What math can I use to understand, wether the numbers are distributed ""evenly"" (means very symmetric with respect to 18 - like ([1,2,18,35,36]) or ""cluttered to one side"" ([1,2,3,30,31], [7,9,17,16,36]) within single given set of 5 numbers? ""cluttered to the left"" - means there are more small numbers - below 18 (say 3, 4, 5 numbers are below 18). I need to analyze many such sets (assigning ""evenly""/""cluttered"" value to each) and then understand what happens more often. Besides, such indicator must show Numbers tend to be cluttered on the left or on the right ([1,2,3,30,31], [7,9,17,16,36]). Numbers tend to be close to 18 [16,15,18,19,20] I think of variance and standard deviation, but I am not sure - maybe there are better applicable or more advanced indicators/analysis methods. P.S. Seems standard deviation is not helping, or I cannot understand how to use it: std([1,2,18,35,36]) = 15.21315220458929  (""evenly"" distributed) std([1,2,3,30,31]) = 13.97998569384104   (""cluttered/skewed"" to the left) std([7,9,16,17,36]) = 10.25670512396647  (""cluttered/skewed"" to the left) std([1,30,31,32,33]) = 12.24091499847948 (""cluttered/skewed"" to the right) Besides, non-parametric skew can be used - it is within [-1..1] and is zero if values are symmetric with respect to the ""middle"".","I have a set of 5 integer numbers {1,23, 17, 33, 35}. Elements can take values only from [1..36], and happen only once within the set. What math can I use to understand, wether the numbers are distributed ""evenly"" (means very symmetric with respect to 18 - like ([1,2,18,35,36]) or ""cluttered to one side"" ([1,2,3,30,31], [7,9,17,16,36]) within single given set of 5 numbers? ""cluttered to the left"" - means there are more small numbers - below 18 (say 3, 4, 5 numbers are below 18). I need to analyze many such sets (assigning ""evenly""/""cluttered"" value to each) and then understand what happens more often. Besides, such indicator must show Numbers tend to be cluttered on the left or on the right ([1,2,3,30,31], [7,9,17,16,36]). Numbers tend to be close to 18 [16,15,18,19,20] I think of variance and standard deviation, but I am not sure - maybe there are better applicable or more advanced indicators/analysis methods. P.S. Seems standard deviation is not helping, or I cannot understand how to use it: std([1,2,18,35,36]) = 15.21315220458929  (""evenly"" distributed) std([1,2,3,30,31]) = 13.97998569384104   (""cluttered/skewed"" to the left) std([7,9,16,17,36]) = 10.25670512396647  (""cluttered/skewed"" to the left) std([1,30,31,32,33]) = 12.24091499847948 (""cluttered/skewed"" to the right) Besides, non-parametric skew can be used - it is within [-1..1] and is zero if values are symmetric with respect to the ""middle"".",,['statistics']
49,"Find the ""average"" discrete distribution for some summary statistics?","Find the ""average"" discrete distribution for some summary statistics?",,"The new law requires companies to make summary statistics of salaries publicly available: Mean Standard deviation First quartile Median Third quartile For $n$ people working at a company the true values of wages is a list of $n$ elements that has exactly this summary statistics. However, the number of possible lists is obviously finite! Let's take this finite number of lists and order their elements. It is now possible to calculate the average of each $k$ -th ( $1 \le k \le n$ ) element. I think the list of average elements would be a very reasonable reconstruction of possible wages (I called it an ""average"" discrete distribution in the title). How should I approach this problem? Could you suggest some references? Also, maybe there are more ways to reconstruct the sensible values easily? Edit: after more than a year, I'm still thinking about this problem.","The new law requires companies to make summary statistics of salaries publicly available: Mean Standard deviation First quartile Median Third quartile For people working at a company the true values of wages is a list of elements that has exactly this summary statistics. However, the number of possible lists is obviously finite! Let's take this finite number of lists and order their elements. It is now possible to calculate the average of each -th ( ) element. I think the list of average elements would be a very reasonable reconstruction of possible wages (I called it an ""average"" discrete distribution in the title). How should I approach this problem? Could you suggest some references? Also, maybe there are more ways to reconstruct the sensible values easily? Edit: after more than a year, I'm still thinking about this problem.",n n k 1 \le k \le n,"['statistics', 'probability-distributions', 'average', 'descriptive-statistics']"
50,Variance of variance of MLE,Variance of variance of MLE,,"I am trying to find the variance of maximum likelihod estimate $Var(s^2)$ where $ s^2 = \frac{1}{n} \sum^n_{i=1} (x_i-\bar{x})^2$ (model $X_i \sim N(\mu,\sigma^2)$). I try getting $Var(s^2) = Var(1/n \sum^n_{i=1} x_i) - Var(\bar{x}^2)$ $ = 1/n^2 \sum E(x^4_i)-E(x^2_i)^2- Var(\bar{x}^2)$ $= \frac{2\sigma^4+4\mu^2\sigma^2$}{n} -  Var(\bar{x}^2) $ and from here I can't getting further. How can I derive the variance of $\bar{x}$, and is it correct to to type $\mu$ or should I type $\bar{x}$ here?","I am trying to find the variance of maximum likelihod estimate $Var(s^2)$ where $ s^2 = \frac{1}{n} \sum^n_{i=1} (x_i-\bar{x})^2$ (model $X_i \sim N(\mu,\sigma^2)$). I try getting $Var(s^2) = Var(1/n \sum^n_{i=1} x_i) - Var(\bar{x}^2)$ $ = 1/n^2 \sum E(x^4_i)-E(x^2_i)^2- Var(\bar{x}^2)$ $= \frac{2\sigma^4+4\mu^2\sigma^2$}{n} -  Var(\bar{x}^2) $ and from here I can't getting further. How can I derive the variance of $\bar{x}$, and is it correct to to type $\mu$ or should I type $\bar{x}$ here?",,"['statistics', 'variance']"
51,"MLE of $\mu$ when $X_1,\ldots,X_n$ are i.i.d $N(\mu,\mu^2)$",MLE of  when  are i.i.d,"\mu X_1,\ldots,X_n N(\mu,\mu^2)","The Question: Given independent random variables $X_1,\dots,X_n \sim N(\mu,\mu^2)$ where $\mu>0$ is unknown, find the MLE $\hat \mu$ of $\mu$. My Attempt: Yes, I know this is a really standard question, but I got a really strange answer so I am not sure. We go through the usual drill: $$L(\mu)=(2\pi \mu^2)^{-n/2}\exp\Bigl(\frac{-1}{2\mu^2} \sum_{i=1}^n(x_i-\mu)^2 \Bigr)$$ $$\ell (\mu)=-\frac n2 \ln(2\pi\mu^2)-\frac{1}{2\mu^2}\sum_{i=1}^n(x_i-\mu)^2 $$ \begin{align} \ \frac{d\ell}{d\mu} & = \Bigl(-\frac n\mu\Bigl)+\Bigl(\frac{1}{\mu^2}\sum_{i=1}^n(x_i-\mu)\Bigl)+\Bigl(\frac{1}{\mu^3}\sum_{i=1}^n(x_i-\mu)^2\Bigl) \\ \ & = \Bigl(-\frac n\mu \Bigl) -\Bigl(\frac n\mu +\frac{1}{\mu^2}\sum_{i=1}^nx_i \Bigl)+ \Bigl(\frac{1}{\mu^3}\sum_{i=1}^nx_i^2-\frac{2}{\mu^2}\sum_{i=1}^nx_i+\frac n\mu \Bigl) \\ \ & = -\frac n\mu - \frac{1}{\mu^2}\sum_{i=1}^nx_i+\frac{1}{\mu^3}\sum_{i=1}^nx_i^2 \end{align} Thus \begin{align} \ & \frac{d\ell}{d\mu}=0 \\ \ \implies & n\mu^2+\mu \sum_{i=1}^nx_i-\sum_{i=1}^nx_i^2=0 \\ \ \implies & \mu = \frac{-\sum_{i=1}^nx_i + \sqrt{\bigl(\sum_{i=1}^nx_i \bigr)^2+4n\sum_{i=1}^nx_i^2}}{2n} \end{align} ... and I feel that I have done something wrong, since there is no way I can use this disgusting thing to do the next part of the question. Any help would be much appreciated!","The Question: Given independent random variables $X_1,\dots,X_n \sim N(\mu,\mu^2)$ where $\mu>0$ is unknown, find the MLE $\hat \mu$ of $\mu$. My Attempt: Yes, I know this is a really standard question, but I got a really strange answer so I am not sure. We go through the usual drill: $$L(\mu)=(2\pi \mu^2)^{-n/2}\exp\Bigl(\frac{-1}{2\mu^2} \sum_{i=1}^n(x_i-\mu)^2 \Bigr)$$ $$\ell (\mu)=-\frac n2 \ln(2\pi\mu^2)-\frac{1}{2\mu^2}\sum_{i=1}^n(x_i-\mu)^2 $$ \begin{align} \ \frac{d\ell}{d\mu} & = \Bigl(-\frac n\mu\Bigl)+\Bigl(\frac{1}{\mu^2}\sum_{i=1}^n(x_i-\mu)\Bigl)+\Bigl(\frac{1}{\mu^3}\sum_{i=1}^n(x_i-\mu)^2\Bigl) \\ \ & = \Bigl(-\frac n\mu \Bigl) -\Bigl(\frac n\mu +\frac{1}{\mu^2}\sum_{i=1}^nx_i \Bigl)+ \Bigl(\frac{1}{\mu^3}\sum_{i=1}^nx_i^2-\frac{2}{\mu^2}\sum_{i=1}^nx_i+\frac n\mu \Bigl) \\ \ & = -\frac n\mu - \frac{1}{\mu^2}\sum_{i=1}^nx_i+\frac{1}{\mu^3}\sum_{i=1}^nx_i^2 \end{align} Thus \begin{align} \ & \frac{d\ell}{d\mu}=0 \\ \ \implies & n\mu^2+\mu \sum_{i=1}^nx_i-\sum_{i=1}^nx_i^2=0 \\ \ \implies & \mu = \frac{-\sum_{i=1}^nx_i + \sqrt{\bigl(\sum_{i=1}^nx_i \bigr)^2+4n\sum_{i=1}^nx_i^2}}{2n} \end{align} ... and I feel that I have done something wrong, since there is no way I can use this disgusting thing to do the next part of the question. Any help would be much appreciated!",,"['statistics', 'normal-distribution', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation']"
52,Second-order stochastic dominance mean and variance,Second-order stochastic dominance mean and variance,,"Consider two random variables $X$ and $Y$. If $X$ dominates $Y$ by the second order, does that mean: (i) $\operatorname{E}(X)\geq \operatorname{E}(Y)$m and (ii) their variance $\operatorname{var}(X)\leq \operatorname{var}(Y)$? If yes, can anyone prove it?","Consider two random variables $X$ and $Y$. If $X$ dominates $Y$ by the second order, does that mean: (i) $\operatorname{E}(X)\geq \operatorname{E}(Y)$m and (ii) their variance $\operatorname{var}(X)\leq \operatorname{var}(Y)$? If yes, can anyone prove it?",,"['statistics', 'economics']"
53,Is the expectation of the minimum of a function equal to the minimum of the expectation?,Is the expectation of the minimum of a function equal to the minimum of the expectation?,,"Given two random variables $X$ and $Y$ and a differentiable function let $f(Y|X=x)$ be a function of Y for fixed values of $X$. Furthermore let $$\tilde{Y} = \text{argmin}_Y \ [f(Y|X=x)]$$ so that $f(\tilde{Y}|X=x) = \text{min} \ f(Y|X=x)$. Denote the expectation of $f(Y|X)$ over $X$ as $$E_X[f(Y|X)]=\int_X f(Y|X)p(X)dX.$$ Is $$\text{min} \ E_X[f(Y|X)] =\int_X f(\tilde{Y}|X)p(X)dX, $$ that is, is the minimum of the expectation of $f$ across $X$ equal to the expectation of the pointwise minimized function?","Given two random variables $X$ and $Y$ and a differentiable function let $f(Y|X=x)$ be a function of Y for fixed values of $X$. Furthermore let $$\tilde{Y} = \text{argmin}_Y \ [f(Y|X=x)]$$ so that $f(\tilde{Y}|X=x) = \text{min} \ f(Y|X=x)$. Denote the expectation of $f(Y|X)$ over $X$ as $$E_X[f(Y|X)]=\int_X f(Y|X)p(X)dX.$$ Is $$\text{min} \ E_X[f(Y|X)] =\int_X f(\tilde{Y}|X)p(X)dX, $$ that is, is the minimum of the expectation of $f$ across $X$ equal to the expectation of the pointwise minimized function?",,"['statistics', 'optimization', 'expectation']"
54,Show that $X_{(1)}=\min(X)$ is independent of ancillary $Y_i = X_{(n)}-X_{(i)}$ for an exponential location family,Show that  is independent of ancillary  for an exponential location family,X_{(1)}=\min(X) Y_i = X_{(n)}-X_{(i)},"Given a random sample $X_1,\ldots,X_n$ distributed according to: $$f(x_i\mid\theta) = e^{-(x_i-\theta)}$$ Show that the minimal sufficient statistic $X_{(1)}$ is independent of the set of ancillary statistics: $$(Y_1,\ldots,Y_{n-1})=(X_{(n)}-X_{(1)},\ldots,X_{(n)}-X_{(n-1)})$$ where $X_{(i)}$ denotes the $i^\text{th}$ order statistic of a random sample of size $n$. My first thought is to perform a simple transformation using the joint order statistic distribution of $X$:  $$f_{X_{(1)},\ldots,X_{(n)}}(x_1,\ldots,x_n)=n!f_X(x_1)\cdots f_X(x_n)$$  If the order statistic inequalities condition is met $x_1<\cdots<x_n$. Then confirm independence using factorization of the joint distribution. Using the definitions of $Y_i$ we perform the multivariate transformation, I defined $Y_n=X_{(1)}$ for simplicity. However I quickly run into a difficult jacobian matrix computation for the determinant. Each inverse mapping from $Y\to X$ is of form: $$X_i=Y_n+Y_1-Y_i$$ The jacobian of this takes on a form I'm not familiar with computing. Not sure how to proceed here or if I should take a look at another approach!","Given a random sample $X_1,\ldots,X_n$ distributed according to: $$f(x_i\mid\theta) = e^{-(x_i-\theta)}$$ Show that the minimal sufficient statistic $X_{(1)}$ is independent of the set of ancillary statistics: $$(Y_1,\ldots,Y_{n-1})=(X_{(n)}-X_{(1)},\ldots,X_{(n)}-X_{(n-1)})$$ where $X_{(i)}$ denotes the $i^\text{th}$ order statistic of a random sample of size $n$. My first thought is to perform a simple transformation using the joint order statistic distribution of $X$:  $$f_{X_{(1)},\ldots,X_{(n)}}(x_1,\ldots,x_n)=n!f_X(x_1)\cdots f_X(x_n)$$  If the order statistic inequalities condition is met $x_1<\cdots<x_n$. Then confirm independence using factorization of the joint distribution. Using the definitions of $Y_i$ we perform the multivariate transformation, I defined $Y_n=X_{(1)}$ for simplicity. However I quickly run into a difficult jacobian matrix computation for the determinant. Each inverse mapping from $Y\to X$ is of form: $$X_i=Y_n+Y_1-Y_i$$ The jacobian of this takes on a form I'm not familiar with computing. Not sure how to proceed here or if I should take a look at another approach!",,"['statistics', 'probability-distributions', 'statistical-inference', 'independence']"
55,Showing that $f(x)$ is a probability density function,Showing that  is a probability density function,f(x),"I'm having trouble showing the following $f(x)$ is a probability density function. The following information is given: $$f(x) = \frac{1}{\sum_{k=x}^\infty {k-1\choose x-1}(1-p)^{k-x-1}}; x = 0, 1, 2, \ldots$$ I'm having trouble in particular getting the denominator to something that I can take the integral of. I've attempted to get it to be equal to a binomial series but I eventually end up with an integral of $p^x$ which is zero. Thanks for any help or tips.","I'm having trouble showing the following $f(x)$ is a probability density function. The following information is given: $$f(x) = \frac{1}{\sum_{k=x}^\infty {k-1\choose x-1}(1-p)^{k-x-1}}; x = 0, 1, 2, \ldots$$ I'm having trouble in particular getting the denominator to something that I can take the integral of. I've attempted to get it to be equal to a binomial series but I eventually end up with an integral of $p^x$ which is zero. Thanks for any help or tips.",,"['probability', 'statistics', 'probability-distributions']"
56,What is the difference between a log of a price and a log of two prices?,What is the difference between a log of a price and a log of two prices?,,"So I  understand that to calculate the continuously compounded return between 2 prices, all you have to do is log the fraction of the 2 prices.  For example: log(price1/price2) What I don't get is how come if you take a log of just one price, it gives you a return or percent change as well?  I don't know how the interpret the 2. I am confused because let's say I am working with the CAPM model, which is: (Excess Stock Return) = Beta0 + Beta1 * (Excess Market Return) and am given a series of 100 stock prices and want to regress it against the market returns so I can get the Betas.  To get the stock returns, I would take the log(price1/price2) and end up with 99 observations.  Cool everything makes sense so far. Now let's say I had two vectors of data, which are price and sales of a product.  If I want the elasticity of sales with respect to price, that is, if I want to calculate how much a 1% change in price would affect a ?% change in sales, I would do the log-log regression, i.e. take the log of both price and sales and do a regression.  My Beta1 would be the elasticity. Let's say I have 100 observations of price and 100 observations of sales and I take the log of vectors.  I still end up with 100 observations of each. I don't get how log(price1) and log(price1/price2) both give me a return or % change?  What's the difference between these 2?  I get that that log(price1/price2) tells me the continuously compounded return going from price 2 to price 1.  So log(3/2) = .405 means that the continuously compounded return is 40.5%.  So what does log(price1) tell me aka just log(3) ?","So I  understand that to calculate the continuously compounded return between 2 prices, all you have to do is log the fraction of the 2 prices.  For example: log(price1/price2) What I don't get is how come if you take a log of just one price, it gives you a return or percent change as well?  I don't know how the interpret the 2. I am confused because let's say I am working with the CAPM model, which is: (Excess Stock Return) = Beta0 + Beta1 * (Excess Market Return) and am given a series of 100 stock prices and want to regress it against the market returns so I can get the Betas.  To get the stock returns, I would take the log(price1/price2) and end up with 99 observations.  Cool everything makes sense so far. Now let's say I had two vectors of data, which are price and sales of a product.  If I want the elasticity of sales with respect to price, that is, if I want to calculate how much a 1% change in price would affect a ?% change in sales, I would do the log-log regression, i.e. take the log of both price and sales and do a regression.  My Beta1 would be the elasticity. Let's say I have 100 observations of price and 100 observations of sales and I take the log of vectors.  I still end up with 100 observations of each. I don't get how log(price1) and log(price1/price2) both give me a return or % change?  What's the difference between these 2?  I get that that log(price1/price2) tells me the continuously compounded return going from price 2 to price 1.  So log(3/2) = .405 means that the continuously compounded return is 40.5%.  So what does log(price1) tell me aka just log(3) ?",,"['calculus', 'statistics', 'logarithms', 'finance', 'regression']"
57,Using the weak law of large numbers to find the limit of $\sum\limits_{r = an}^{bn} {n \choose r } p^r (1-p)^{n-r}$,Using the weak law of large numbers to find the limit of,\sum\limits_{r = an}^{bn} {n \choose r } p^r (1-p)^{n-r},"I need to find the limit of $\sum\limits_{r}^{} {n \choose r } p^r (1-p)^{n-r}$ such that $ an < r < bn $ in the cases $p < a$, $ a < p < b$, and $b < p$. I know I need to consider the sum of $n$ identically distributed independent Bernoulli random variables, but I'm not sure how to apply the weak law to show what is required. Any help you could offer would be very much appreciated.","I need to find the limit of $\sum\limits_{r}^{} {n \choose r } p^r (1-p)^{n-r}$ such that $ an < r < bn $ in the cases $p < a$, $ a < p < b$, and $b < p$. I know I need to consider the sum of $n$ identically distributed independent Bernoulli random variables, but I'm not sure how to apply the weak law to show what is required. Any help you could offer would be very much appreciated.",,"['probability', 'statistics', 'probability-distributions', 'probability-limit-theorems', 'law-of-large-numbers']"
58,Proof for sum of squares formula ( statistics - related ),Proof for sum of squares formula ( statistics - related ),,"I'm new to the domain of statistics and i'm trying to accumulate as much info as i can right now. I've considered that this question should be asked here as it is related to mathematics. The problem is that from the get go most statistics books use the sum of squares formula : $SS= \sum{X^2} - \frac{(\sum(X))^2}{n} .$ Where can i find a proof for this formula? I've tried to prove it myself from $SS= \sum{(X-m_x)} $ , where $m_x$ is the mean of the population $X$ but to no avail.","I'm new to the domain of statistics and i'm trying to accumulate as much info as i can right now. I've considered that this question should be asked here as it is related to mathematics. The problem is that from the get go most statistics books use the sum of squares formula : $SS= \sum{X^2} - \frac{(\sum(X))^2}{n} .$ Where can i find a proof for this formula? I've tried to prove it myself from $SS= \sum{(X-m_x)} $ , where $m_x$ is the mean of the population $X$ but to no avail.",,"['statistics', 'proof-explanation']"
59,Basic math explanation (related to estimating linear regression with no intercept),Basic math explanation (related to estimating linear regression with no intercept),,"I have a question on the Cross Validated Stack Exchange site where I ask how to update the exponential regression coefficient of a vertically translated depreciation curve . A Cross Validated community member has been kind enough to provide an answer to my question. The solution has been explained as follows: ...the equation you need to estimate is $$y=21-e^{ax},$$ which is equivalent to $$21-y=e^{ax}.$$ If you take logarithms both sides (you can do it because $y<21$ ), then $$log(21-y)=ax.$$ Renaming $log(21-y)=z$ , this is of the form $$z=ax,$$ which is a linear regression with no intercept that can be estimated with many standard software packages. I think I understand everything up to, and including this part: If you take logarithms both sides (you can do it because $y<21$ ), then $$log(21-y)=ax.$$ However, as someone who has limited math skills, I'm having difficulty understanding a couple of things: Why would I want to rename $log(21-y)=z$ --as-- $z=ax$ ? How do I estimate the ""linear regression with no intercept""? Could #1 and #2 be explained, in layman's terms?","I have a question on the Cross Validated Stack Exchange site where I ask how to update the exponential regression coefficient of a vertically translated depreciation curve . A Cross Validated community member has been kind enough to provide an answer to my question. The solution has been explained as follows: ...the equation you need to estimate is which is equivalent to If you take logarithms both sides (you can do it because ), then Renaming , this is of the form which is a linear regression with no intercept that can be estimated with many standard software packages. I think I understand everything up to, and including this part: If you take logarithms both sides (you can do it because ), then However, as someone who has limited math skills, I'm having difficulty understanding a couple of things: Why would I want to rename --as-- ? How do I estimate the ""linear regression with no intercept""? Could #1 and #2 be explained, in layman's terms?","y=21-e^{ax}, 21-y=e^{ax}. y<21 log(21-y)=ax. log(21-y)=z z=ax, y<21 log(21-y)=ax. log(21-y)=z z=ax","['statistics', 'linear-regression']"
60,What is the expected no. of local maxima in a circular permutation of $n$ numbers?,What is the expected no. of local maxima in a circular permutation of  numbers?,n,Given $n$ numbers. We permute these numbers uniformly at random and arrange them on circle. What is the expected number of local maxima? Note : We call a local maximum a number that is greater than both of its neighbors. This problem is similar to this one except that here we are dealing with numbers arranged on the circle. Does $(n + 1)/3$ hold for this problem?,Given $n$ numbers. We permute these numbers uniformly at random and arrange them on circle. What is the expected number of local maxima? Note : We call a local maximum a number that is greater than both of its neighbors. This problem is similar to this one except that here we are dealing with numbers arranged on the circle. Does $(n + 1)/3$ hold for this problem?,,"['statistics', 'permutations', 'uniform-distribution']"
61,Show that $\Sigma_{i=1}^{n}(x_i - \bar x_n )^2$ can be calculated with a recursion,Show that  can be calculated with a recursion,\Sigma_{i=1}^{n}(x_i - \bar x_n )^2,Specifically with the recursion $$\Sigma_{i=1}^{k+1}(x_i - \bar x_{k+1} )^2 = \Sigma_{i=1}^{k}(x_i - \bar x_k )^2 + \frac{k}{k+1}(x_{k+1} -\bar x_k)^2 $$ for $k = 1...n-1$. I know that $$\bar x_{k+1} = \frac{k}{k+1}\bar x_k + \frac{k}{k+1}\bar x_{k+1}$$ but after that I'm not sure. I showed that it's true for $k = 2$ but after that I'm stuck. I tried opening up the right hand side but just ended up in a morass of equations.,Specifically with the recursion $$\Sigma_{i=1}^{k+1}(x_i - \bar x_{k+1} )^2 = \Sigma_{i=1}^{k}(x_i - \bar x_k )^2 + \frac{k}{k+1}(x_{k+1} -\bar x_k)^2 $$ for $k = 1...n-1$. I know that $$\bar x_{k+1} = \frac{k}{k+1}\bar x_k + \frac{k}{k+1}\bar x_{k+1}$$ but after that I'm not sure. I showed that it's true for $k = 2$ but after that I'm stuck. I tried opening up the right hand side but just ended up in a morass of equations.,,"['real-analysis', 'probability', 'statistics', 'summation']"
62,How to interpret root-mean-square,How to interpret root-mean-square,,Could someone please describe the physical meaning of the RMS value of something? How does this interpretation relate to the average value of something?,Could someone please describe the physical meaning of the RMS value of something? How does this interpretation relate to the average value of something?,,"['statistics', 'average', 'means']"
63,Can an event be independent to the Sample Space. Can a Null Event be independent to any other event?,Can an event be independent to the Sample Space. Can a Null Event be independent to any other event?,,"How do we show mathematically that event $A$ cannot be independent of $S$ the sample space? How do we show event $A,$ if $P(A)=0$ cannot be independent of event $B$ if $0 < P(B) < 1?$ Seems like a dumb a question to me. A thing that exists cannot be independent of the universe that it is in, and a thing that does not exist is defined by the universe that it is not. Having tough time with the formulas.","How do we show mathematically that event $A$ cannot be independent of $S$ the sample space? How do we show event $A,$ if $P(A)=0$ cannot be independent of event $B$ if $0 < P(B) < 1?$ Seems like a dumb a question to me. A thing that exists cannot be independent of the universe that it is in, and a thing that does not exist is defined by the universe that it is not. Having tough time with the formulas.",,"['probability', 'statistics']"
64,Pearson's Chi Square Formula,Pearson's Chi Square Formula,,"The Pearson's Chi Square standard notation is a summation of $(O_i-E_i)^2/E_i$. However, I cannot really get why it is divided by $E_i$. For example, if I want to measure the distance between $f(x)$ and $g(x)$, then I would simply do the following: $d(x)=|f(x)-g(x)|$ or $d^2(x)=(f(x)-g(x))^2$. So, if I haven't decided upon my critical value yet and I just keep the results of the formula, what is the purpose of the division?","The Pearson's Chi Square standard notation is a summation of $(O_i-E_i)^2/E_i$. However, I cannot really get why it is divided by $E_i$. For example, if I want to measure the distance between $f(x)$ and $g(x)$, then I would simply do the following: $d(x)=|f(x)-g(x)|$ or $d^2(x)=(f(x)-g(x))^2$. So, if I haven't decided upon my critical value yet and I just keep the results of the formula, what is the purpose of the division?",,['statistics']
65,"Estimating $ p $ for a sequence of random variables $ X_1 \sim B(N_1, p), \ldots, X_k \sim B(N_k, p) $ when I have one realization for each",Estimating  for a sequence of random variables  when I have one realization for each," p   X_1 \sim B(N_1, p), \ldots, X_k \sim B(N_k, p) ","Let $ X_1, \ldots, X_k $ be independent random variables such that $ X_1 \sim B(N_1, p), \ldots, X_k \sim B(N_k, p) $. I have exactly one realization for each, i. e. a sequence $ x_1, \ldots, x_k $ where $ x_i $ is a realization of $ X_i $. (We could generalize, but let's keep things simple, since I don't need anything more complicated.) I know $ N_1, \ldots, N_k $, but I'm trying to estimate $ p $. I'm thinking that using $ \frac{1}{k}\sum_{i=1}^{k} x_i/N_i $ would not be ideal, because the $ X_i $'s don't necessarily have the same variance. My intuition is that this estimator is unbiased but has a larger variance than necessary. Thus, I'm thinking that I should use a weighted average to estimate $ p $, where $ x_i/N_i $ is given more weight than $ x_j/N_j $ if $ N_i < N_j $ because then $ \sigma_i^2 = N_ip(1 - p) < \sigma_j = N_jp(1 - p) $. However, I'm not sure exactly what's the estimator I should use, so I would appreciate some help.","Let $ X_1, \ldots, X_k $ be independent random variables such that $ X_1 \sim B(N_1, p), \ldots, X_k \sim B(N_k, p) $. I have exactly one realization for each, i. e. a sequence $ x_1, \ldots, x_k $ where $ x_i $ is a realization of $ X_i $. (We could generalize, but let's keep things simple, since I don't need anything more complicated.) I know $ N_1, \ldots, N_k $, but I'm trying to estimate $ p $. I'm thinking that using $ \frac{1}{k}\sum_{i=1}^{k} x_i/N_i $ would not be ideal, because the $ X_i $'s don't necessarily have the same variance. My intuition is that this estimator is unbiased but has a larger variance than necessary. Thus, I'm thinking that I should use a weighted average to estimate $ p $, where $ x_i/N_i $ is given more weight than $ x_j/N_j $ if $ N_i < N_j $ because then $ \sigma_i^2 = N_ip(1 - p) < \sigma_j = N_jp(1 - p) $. However, I'm not sure exactly what's the estimator I should use, so I would appreciate some help.",,"['probability', 'statistics', 'binomial-distribution']"
66,Defective washers in a sample of 20,Defective washers in a sample of 20,,"The diameter of washers produced by a machine has a mean of 0.502mm and a standard deviation of 0.005mm. Suppose that the maximum tolerance allows for diameters between 0.496mm and 0.508mm inclusive, otherwise a washer is classified as defective. Determine the percentage of defective washers produced by the machine, assuming that the diameters are normally distributed, and hence state how many defective washers you expect to see in a sample of 20 washers produced by this machine. 0,496 < x < 0,502 P[0,496- 0,502 /0,005 < x-m/sigma < 0,508 - 0,502 / 0,005] P[-1.2 < z < 1.2] 1.2 = 0,3849 2x 0,3849 = 0,7698= 76,98% 100- 76,98= 23,02% 20 x 23,02 /100= 4,604 defective washers in my sample of 20 Is my reasoning correct? `","The diameter of washers produced by a machine has a mean of 0.502mm and a standard deviation of 0.005mm. Suppose that the maximum tolerance allows for diameters between 0.496mm and 0.508mm inclusive, otherwise a washer is classified as defective. Determine the percentage of defective washers produced by the machine, assuming that the diameters are normally distributed, and hence state how many defective washers you expect to see in a sample of 20 washers produced by this machine. 0,496 < x < 0,502 P[0,496- 0,502 /0,005 < x-m/sigma < 0,508 - 0,502 / 0,005] P[-1.2 < z < 1.2] 1.2 = 0,3849 2x 0,3849 = 0,7698= 76,98% 100- 76,98= 23,02% 20 x 23,02 /100= 4,604 defective washers in my sample of 20 Is my reasoning correct? `",,"['probability', 'statistics', 'probability-distributions']"
67,if $X$ is a random variable what is the distribution of $W(x)=1/x$?,if  is a random variable what is the distribution of ?,X W(x)=1/x,"If $X$ is a random variable, what is $W(x) = 1/x$? if $X$ was say normal distribution $\frac{1}{\sqrt{2\pi\sigma}} e^{-(x-\mu)^2/(2\sigma^2)}$ Would that mean $W(x)$ is distributed as $\dfrac{1}{\frac{1}{\sqrt{2\pi\sigma}} e^{-(x-\mu)^2/(2\sigma^2)}} \text{?}$ or Would that mean W(x) is distributed as ${\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(\frac{1}{x}-\mu)^2}{2\sigma^2}}}$ Or is $W(x)=1/x$ to be interpreted as a conditional probability of $W$ given $X=x$? Or $f_{W\mid X}(W\mid X=x)$. I feel like this is right because X is used to denote distributions while x is used to denote realised variables.","If $X$ is a random variable, what is $W(x) = 1/x$? if $X$ was say normal distribution $\frac{1}{\sqrt{2\pi\sigma}} e^{-(x-\mu)^2/(2\sigma^2)}$ Would that mean $W(x)$ is distributed as $\dfrac{1}{\frac{1}{\sqrt{2\pi\sigma}} e^{-(x-\mu)^2/(2\sigma^2)}} \text{?}$ or Would that mean W(x) is distributed as ${\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(\frac{1}{x}-\mu)^2}{2\sigma^2}}}$ Or is $W(x)=1/x$ to be interpreted as a conditional probability of $W$ given $X=x$? Or $f_{W\mid X}(W\mid X=x)$. I feel like this is right because X is used to denote distributions while x is used to denote realised variables.",,"['statistics', 'normal-distribution']"
68,Given the cumulative distribution function find a random variable that has this distribution.,Given the cumulative distribution function find a random variable that has this distribution.,,"We are given a Cumulative distribution function $(CDF)$ how do we find a random variable that has the given distribution? Since there could be a lot of such variables we are to find anyone given that the function is differentiable and increasing. I read somewhere that we could simply take inverse of the function(if it exists) and that would be the Random variable you are looking for, but I don't understand it, even if it's true(?). If that's true, why? Else, how do we find a random variable for the distribution?","We are given a Cumulative distribution function $(CDF)$ how do we find a random variable that has the given distribution? Since there could be a lot of such variables we are to find anyone given that the function is differentiable and increasing. I read somewhere that we could simply take inverse of the function(if it exists) and that would be the Random variable you are looking for, but I don't understand it, even if it's true(?). If that's true, why? Else, how do we find a random variable for the distribution?",,"['probability', 'statistics', 'probability-distributions', 'random-variables']"
69,Finding $E[|X-\mu|]$ dealing with absolute integration.,Finding  dealing with absolute integration.,E[|X-\mu|],"I would like to find $E[|X-\mu|]$ when $X$ is $N(\mu, \sigma^2)$. I think I need to evaluate the integral: $$E[|X-\mu|]=\int_{\mathbb{R}} |x-\mu| \frac{1}{\sigma\sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2} dx$$ This is the same as the following since $e^u>0$ $$=\int_{\mathbb{R}} \left|(x-\mu) \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2}\right| \, dx$$ So, I need to find the solution to: $$(x-\mu) \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2}=0$$ To determine where to break the integral. The above is only zero where $x=\mu$. So I break it at $\mu$ $$E[|X-\mu|]=\int_{-\infty}^\mu \left|(x-\mu) \frac{1}{\sigma\sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2}\right| \, dx + \int_\mu^\infty \left|(x-\mu) \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2}\right| \, dx$$ If $x \in (-\infty, \mu)$ then $x-\mu<0$ so we can multiply the first integral by $-1$ and remove the absolute value bars. $$E[|X-\mu|]=-\int_{-\infty}^\mu (x-\mu) \frac{1}{\sigma\sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2} \, dx + \int_\mu^\infty (x-\mu) \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2}\left( \frac{x-\mu} \sigma \right)^2} \, dx$$ now we have: $$=-\int_{-\infty}^{\mu}  \frac{1}{\sigma\sqrt{2 \pi}}xe^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2}dx +\mu\int_{-\infty}^{\mu}  \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2}dx +{} $$ $$\int_\mu^\infty \frac{1}{\sigma\sqrt{2 \pi}}xe^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2} \, dx  -\mu\int_\mu^\infty \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2} \, dx$$ I'm not certain how to deal with these integrals? Any hints?","I would like to find $E[|X-\mu|]$ when $X$ is $N(\mu, \sigma^2)$. I think I need to evaluate the integral: $$E[|X-\mu|]=\int_{\mathbb{R}} |x-\mu| \frac{1}{\sigma\sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2} dx$$ This is the same as the following since $e^u>0$ $$=\int_{\mathbb{R}} \left|(x-\mu) \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2}\right| \, dx$$ So, I need to find the solution to: $$(x-\mu) \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2}=0$$ To determine where to break the integral. The above is only zero where $x=\mu$. So I break it at $\mu$ $$E[|X-\mu|]=\int_{-\infty}^\mu \left|(x-\mu) \frac{1}{\sigma\sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2}\right| \, dx + \int_\mu^\infty \left|(x-\mu) \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2}\right| \, dx$$ If $x \in (-\infty, \mu)$ then $x-\mu<0$ so we can multiply the first integral by $-1$ and remove the absolute value bars. $$E[|X-\mu|]=-\int_{-\infty}^\mu (x-\mu) \frac{1}{\sigma\sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2} \, dx + \int_\mu^\infty (x-\mu) \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2}\left( \frac{x-\mu} \sigma \right)^2} \, dx$$ now we have: $$=-\int_{-\infty}^{\mu}  \frac{1}{\sigma\sqrt{2 \pi}}xe^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2}dx +\mu\int_{-\infty}^{\mu}  \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2}dx +{} $$ $$\int_\mu^\infty \frac{1}{\sigma\sqrt{2 \pi}}xe^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2} \, dx  -\mu\int_\mu^\infty \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2} \, dx$$ I'm not certain how to deal with these integrals? Any hints?",,"['statistics', 'normal-distribution', 'expectation']"
70,"MLE for $a$ in a distribution on $[a, +\infty)$",MLE for  in a distribution on,"a [a, +\infty)","I want to ensure my reasoning is correct. The sample is distributed with the pdf:   $$f_a(x) = e^{-(x-a)}\cdot \chi_{[a, +\infty)}(x)$$   where $a>0$. Find the maximum likelihood estimator for $a$. The likelihood function would be: $$L(a, x_1, ..., x_n) = e^{-x_1+a}\cdot \chi_{[a, +\infty)}(x_1) \cdot \dots \cdot e^{-x_n+a}\cdot \chi_{[a, +\infty)}(x_n)$$ $$=e^{na-\sum\limits_{i=1}^n x_i} \prod\limits_{i=1}^n \chi_{[a, +\infty)}(x_i)$$ $$= \begin{cases} e^{na-\sum\limits_{i=1}^n x_i} & x_1, \dots, x_n \in [a, +\infty) \\ 0, & \text{otherwise} \end{cases}$$ $$= \begin{cases} e^{na-\sum\limits_{i=1}^n x_i} & \forall i \in \{1, \dots, n\}: x_i \geq a \\ 0, & \exists j \in \{1, \dots, n\}: x_j < a \end{cases}$$ which, if plotted, would probably look like this: Since all $x_i$ have to be greater than $a$ and the maximum value is reached at $a$, my guess is that the MLE would be: $$\hat{a}_n = \min{(x_1, \dots, x_n)}$$ Is this correct?","I want to ensure my reasoning is correct. The sample is distributed with the pdf:   $$f_a(x) = e^{-(x-a)}\cdot \chi_{[a, +\infty)}(x)$$   where $a>0$. Find the maximum likelihood estimator for $a$. The likelihood function would be: $$L(a, x_1, ..., x_n) = e^{-x_1+a}\cdot \chi_{[a, +\infty)}(x_1) \cdot \dots \cdot e^{-x_n+a}\cdot \chi_{[a, +\infty)}(x_n)$$ $$=e^{na-\sum\limits_{i=1}^n x_i} \prod\limits_{i=1}^n \chi_{[a, +\infty)}(x_i)$$ $$= \begin{cases} e^{na-\sum\limits_{i=1}^n x_i} & x_1, \dots, x_n \in [a, +\infty) \\ 0, & \text{otherwise} \end{cases}$$ $$= \begin{cases} e^{na-\sum\limits_{i=1}^n x_i} & \forall i \in \{1, \dots, n\}: x_i \geq a \\ 0, & \exists j \in \{1, \dots, n\}: x_j < a \end{cases}$$ which, if plotted, would probably look like this: Since all $x_i$ have to be greater than $a$ and the maximum value is reached at $a$, my guess is that the MLE would be: $$\hat{a}_n = \min{(x_1, \dots, x_n)}$$ Is this correct?",,"['statistics', 'statistical-inference', 'parameter-estimation']"
71,How do I solve this statistics question on approximation of distribution? [closed],How do I solve this statistics question on approximation of distribution? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Suppose that $U_i\sim \operatorname{iid} \operatorname{Unif} [-1,1]$ and consider $$\ X = \frac{U_1 + U_2 + \cdots + U_n} {\sqrt{U_1^2 + U_2^2 + \cdots + U_n^2}}$$ for large $n.$ What is the probability that $X > 1.$ Justify any approximation you use. How large $n$ should be?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Suppose that $U_i\sim \operatorname{iid} \operatorname{Unif} [-1,1]$ and consider $$\ X = \frac{U_1 + U_2 + \cdots + U_n} {\sqrt{U_1^2 + U_2^2 + \cdots + U_n^2}}$$ for large $n.$ What is the probability that $X > 1.$ Justify any approximation you use. How large $n$ should be?",,"['statistics', 'statistical-inference', 'probability-limit-theorems']"
72,Linear Least Square Equation is Strictly Convex,Linear Least Square Equation is Strictly Convex,,I know linear least square equation $\|y−X\beta\|^2$ can have multiple solutions since it is NOT a strictly convex equation. How to prove it is only a convex function and NOT a strictly convex function?,I know linear least square equation $\|y−X\beta\|^2$ can have multiple solutions since it is NOT a strictly convex equation. How to prove it is only a convex function and NOT a strictly convex function?,,"['calculus', 'statistics', 'optimization', 'convex-optimization', 'linear-regression']"
73,Is there a branch of mathematics that focuses on finding relationship between pairs?,Is there a branch of mathematics that focuses on finding relationship between pairs?,,"Lets say you are given a bunch of pairs like (3, 9), (4, 16), (5, 25) The relation between them is that n^2 = m This one is pretty easy but is there a branch of math that talks about this subject with algorithms to find the simplest relations and theory about the complexity of these relations how much examples you would need to guarantee you found the right one. I don't know anything except basic algebra and calculus but I am interested in this subject so if you can point me in the right direction I would be eternal grateful.","Lets say you are given a bunch of pairs like (3, 9), (4, 16), (5, 25) The relation between them is that n^2 = m This one is pretty easy but is there a branch of math that talks about this subject with algorithms to find the simplest relations and theory about the complexity of these relations how much examples you would need to guarantee you found the right one. I don't know anything except basic algebra and calculus but I am interested in this subject so if you can point me in the right direction I would be eternal grateful.",,"['statistics', 'relations']"
74,Why use averages vs medians in statistics?,Why use averages vs medians in statistics?,,"Why in statistics is the average used instead of the median, why are P values for example always calculated with averages and not with median? I have always been taught that stats and P values always have to be used with the average and I have no idea why, is it just a systemic choice of the person who created the data or can they be used interchangeably for the majority of statistical work?","Why in statistics is the average used instead of the median, why are P values for example always calculated with averages and not with median? I have always been taught that stats and P values always have to be used with the average and I have no idea why, is it just a systemic choice of the person who created the data or can they be used interchangeably for the majority of statistical work?",,"['statistics', 'average', 'median']"
75,Rewriting Sample Variance Formula,Rewriting Sample Variance Formula,,Why can $s^2 = \frac{\sum_{i=1}^{n}{(X_i - \bar{X})^2}}{n-1}$ be written as $ s^2 = \frac{\sum_{i=1}^{n}{X_i^2 - n(\bar{X})^2}}{n-1} $?,Why can $s^2 = \frac{\sum_{i=1}^{n}{(X_i - \bar{X})^2}}{n-1}$ be written as $ s^2 = \frac{\sum_{i=1}^{n}{X_i^2 - n(\bar{X})^2}}{n-1} $?,,"['statistics', 'variance']"
76,PDF of the Euclidean norm of $M$ normal distributions,PDF of the Euclidean norm of  normal distributions,M,"If $X_1, X_2, \dots, X_M$ are independent identically distributed (i.i.d.) normal random variables, how to calculate the PDF and mean of the Euclidean norm of $X_1,X_2,\ldots,X_M$? $$X_i\sim N(0,\sigma^2)$$ $$Y=\sqrt{\sum_{i=1}^M X_i^2}$$ There is a similarity between $Y$ and the square root of the $\chi$-squared distribution , i.e., the sum of the squares of $k$ independent standard normal random variables; however, in the case of $Y$, $\sigma$ may not necessarily be $1$. $$f_Y(y) = \text{?}$$ $$E[Y] = \text{?}$$","If $X_1, X_2, \dots, X_M$ are independent identically distributed (i.i.d.) normal random variables, how to calculate the PDF and mean of the Euclidean norm of $X_1,X_2,\ldots,X_M$? $$X_i\sim N(0,\sigma^2)$$ $$Y=\sqrt{\sum_{i=1}^M X_i^2}$$ There is a similarity between $Y$ and the square root of the $\chi$-squared distribution , i.e., the sum of the squares of $k$ independent standard normal random variables; however, in the case of $Y$, $\sigma$ may not necessarily be $1$. $$f_Y(y) = \text{?}$$ $$E[Y] = \text{?}$$",,"['statistics', 'probability-distributions', 'expectation', 'normal-distribution', 'chi-squared']"
77,How would I find the variance of a random variable with this distribution?,How would I find the variance of a random variable with this distribution?,,"Random variable $X$ has the following distribution: $P(X=0) = 1/2$ For all non-zero integers, $P(X=n) = (1/2)^{|n|+2}$ How would I find the variance of this random variable? I know that the variance can be found by calculating $\operatorname{E}(X^2) - \operatorname{E}(X)^2$ but I am unsure of how to do this since the PMF has two parts. Thanks!","Random variable $X$ has the following distribution: $P(X=0) = 1/2$ For all non-zero integers, $P(X=n) = (1/2)^{|n|+2}$ How would I find the variance of this random variable? I know that the variance can be found by calculating $\operatorname{E}(X^2) - \operatorname{E}(X)^2$ but I am unsure of how to do this since the PMF has two parts. Thanks!",,"['probability', 'statistics', 'random-variables']"
78,Binomial Distribution and the Moment Generating Function,Binomial Distribution and the Moment Generating Function,,"Please consider the problem and the answer below. The problem is from a text book on statistics. I believe that my answer is correct but it seems to easy so I am concerned that it is wrong. Is it wrong? Thanks, Bob Problem: If $Y$ has a binomial distribution with $n$ trials and the probability of success $p$, show that the moment generating function for $Y$ is: \begin{eqnarray} m(t) &=& (pe^t+q)^n \end{eqnarray} where $q = 1 - p$. Answer: The moment generating function is defined to be $m(t) = E(e^{ty})$. This gives us the following: \begin{eqnarray*} m(t) &=& E(e^{ty}) \\ m(t) &=& \sum_{i = 0}^{n} { {n \choose i} p^i q^{n-i} e^{ti} } \\ \end{eqnarray*} Now, I apply the binomial theorem to equation 1 and get: \begin{eqnarray*} m(t) &=& \sum_{i = 0}^{n} { {n \choose i} p^i q^{n-i} e^{ti} } \\ \end{eqnarray*} Q.E.D.","Please consider the problem and the answer below. The problem is from a text book on statistics. I believe that my answer is correct but it seems to easy so I am concerned that it is wrong. Is it wrong? Thanks, Bob Problem: If $Y$ has a binomial distribution with $n$ trials and the probability of success $p$, show that the moment generating function for $Y$ is: \begin{eqnarray} m(t) &=& (pe^t+q)^n \end{eqnarray} where $q = 1 - p$. Answer: The moment generating function is defined to be $m(t) = E(e^{ty})$. This gives us the following: \begin{eqnarray*} m(t) &=& E(e^{ty}) \\ m(t) &=& \sum_{i = 0}^{n} { {n \choose i} p^i q^{n-i} e^{ti} } \\ \end{eqnarray*} Now, I apply the binomial theorem to equation 1 and get: \begin{eqnarray*} m(t) &=& \sum_{i = 0}^{n} { {n \choose i} p^i q^{n-i} e^{ti} } \\ \end{eqnarray*} Q.E.D.",,"['probability', 'statistics']"
79,Finding the p-value in a proportion statistics with binomial distribution,Finding the p-value in a proportion statistics with binomial distribution,,"Given that the sample is 40, we observed that 80% of the sample (32 participants) have a successful trial. We want to test the hypothesis $H_{0} = p_{0} = 0.7$ versus $H_{a} \neq 0.7$ at 5% significance level. I would to calculate the $p$-value. My attempt Let X be the distribution under the nul hypothesis, where X ~ Bin(n,$p_{0}$), i.e. $P(X=x)= \binom{n}{x}(p_{0})^{x}(1-p_{0})^{40-x}$. For values less than the observed test statistic, i.e. for $x \lt 32$: $p$-value $= 2 * P(X \leq x)$. For $x =32$: p-value = 1. For values greater than the observed test statistic i.e. for $x \gt 32$:$p$-value $= 2 * P(X \geq x)$. However, i am getting p-values greater than 1, wish is wrong. i am wondering if there is any logical error in my testing.","Given that the sample is 40, we observed that 80% of the sample (32 participants) have a successful trial. We want to test the hypothesis $H_{0} = p_{0} = 0.7$ versus $H_{a} \neq 0.7$ at 5% significance level. I would to calculate the $p$-value. My attempt Let X be the distribution under the nul hypothesis, where X ~ Bin(n,$p_{0}$), i.e. $P(X=x)= \binom{n}{x}(p_{0})^{x}(1-p_{0})^{40-x}$. For values less than the observed test statistic, i.e. for $x \lt 32$: $p$-value $= 2 * P(X \leq x)$. For $x =32$: p-value = 1. For values greater than the observed test statistic i.e. for $x \gt 32$:$p$-value $= 2 * P(X \geq x)$. However, i am getting p-values greater than 1, wish is wrong. i am wondering if there is any logical error in my testing.",,"['statistics', 'hypothesis-testing']"
80,Joint Distribution of n Poisson Random Variables,Joint Distribution of n Poisson Random Variables,,"Let $x_1,x_2,\dots,x_n$ be a set of independent and identically distributed random variables with distribution Poisson with parameter λ. Write the joint distribution of all those random variables. Simplify as much as possible your final answer and show work. So I think that the joint probability of independent random variables is the product of all individual probability distribution function, but I don't actually understand how to implement that in this case, since it's for $n$ variables.","Let $x_1,x_2,\dots,x_n$ be a set of independent and identically distributed random variables with distribution Poisson with parameter λ. Write the joint distribution of all those random variables. Simplify as much as possible your final answer and show work. So I think that the joint probability of independent random variables is the product of all individual probability distribution function, but I don't actually understand how to implement that in this case, since it's for $n$ variables.",,"['probability', 'statistics', 'poisson-distribution']"
81,Expected number of consecutive numbers from a uniform draw,Expected number of consecutive numbers from a uniform draw,,"Say we pick $k$ numbers uniformly from $n$ without replacement and no order. What is the expected number of consecutive numbers we will get? For example, for $n=4, k=2$, only a draw of $(1,2),(2,3),(3,4)$ will result in $1$ consecutive numbers, so the expected number is $0.5$ I can also use a good upper bound instead of the expected value.","Say we pick $k$ numbers uniformly from $n$ without replacement and no order. What is the expected number of consecutive numbers we will get? For example, for $n=4, k=2$, only a draw of $(1,2),(2,3),(3,4)$ will result in $1$ consecutive numbers, so the expected number is $0.5$ I can also use a good upper bound instead of the expected value.",,"['combinatorics', 'statistics', 'expectation']"
82,"Probabilities of blown-away package ownership, based on frequency and distance","Probabilities of blown-away package ownership, based on frequency and distance",,"Yesterday, because of high wind, a package got blown into our backyard. We weren't sure whether the package belonged to us or to one of our five neighbors. It occurred to me that if we knew the frequency with which each neighbor receives packages, we might be able to calculate the probability that the package was blown from each house's doorstep. But because all of our doorsteps are different distances away from where the package was found, we would have to factor in not only the frequency, but also the distance from each doorstep to the location where the package was found. I know how to compute the probabilities based only on frequency, but I'm not quite sure how to also factor in the distance.  Any tips? P.S. In case you're curious, although our household probably receives more packages than any of our neighbors, the package that we found in our backyard belonged to the neighbor furthest away from us!","Yesterday, because of high wind, a package got blown into our backyard. We weren't sure whether the package belonged to us or to one of our five neighbors. It occurred to me that if we knew the frequency with which each neighbor receives packages, we might be able to calculate the probability that the package was blown from each house's doorstep. But because all of our doorsteps are different distances away from where the package was found, we would have to factor in not only the frequency, but also the distance from each doorstep to the location where the package was found. I know how to compute the probabilities based only on frequency, but I'm not quite sure how to also factor in the distance.  Any tips? P.S. In case you're curious, although our household probably receives more packages than any of our neighbors, the package that we found in our backyard belonged to the neighbor furthest away from us!",,"['probability', 'statistics']"
83,statistics finding the confidence level of the interval estimate medical expenses,statistics finding the confidence level of the interval estimate medical expenses,,"A confidence interval for the true mean of the annual medical expenses of a middle-class American family is given as ($ 738, $ 777). If this interval is based on interviews with 110 families and a standard deviation of $ 120 is assumed. Suppose all annual medical expenses of middle-class American families follow an approximately normal distribution. (a) What is the sample mean of annual medical expenses? sample mean = 777 + 738 / 2             = 757.5 (b) What is the confidence level of the interval estimate (as a decimal) CI = sample mean + z alpha/2 * (standard deviation / square root n) Isolate for z alpha/2 777 = 757.5 + z alpha/2 * (120/square root 110) Rearrange (777 * square root 110) / (757.5 * 120) = z alpha/2 0.089650657 = z alpha/2 What are the correct solutions and answers?","A confidence interval for the true mean of the annual medical expenses of a middle-class American family is given as ($ 738, $ 777). If this interval is based on interviews with 110 families and a standard deviation of $ 120 is assumed. Suppose all annual medical expenses of middle-class American families follow an approximately normal distribution. (a) What is the sample mean of annual medical expenses? sample mean = 777 + 738 / 2             = 757.5 (b) What is the confidence level of the interval estimate (as a decimal) CI = sample mean + z alpha/2 * (standard deviation / square root n) Isolate for z alpha/2 777 = 757.5 + z alpha/2 * (120/square root 110) Rearrange (777 * square root 110) / (757.5 * 120) = z alpha/2 0.089650657 = z alpha/2 What are the correct solutions and answers?",,"['probability', 'statistics', 'confidence-interval']"
84,Term for center of dataset,Term for center of dataset,,"There are a number of different ways of averaging a dataset, each with a specific definition. For example, mean: Sum of $N$ values divided by $N$ median: The $(N+1)/2$ term after $N$ values are sorted mode: The most common value in a dataset I am looking for a term for the $(\text{min}+\text{max})/2$, the 'middle' of a set of data. Is there a specific term for this definition already?","There are a number of different ways of averaging a dataset, each with a specific definition. For example, mean: Sum of $N$ values divided by $N$ median: The $(N+1)/2$ term after $N$ values are sorted mode: The most common value in a dataset I am looking for a term for the $(\text{min}+\text{max})/2$, the 'middle' of a set of data. Is there a specific term for this definition already?",,"['statistics', 'terminology', 'average']"
85,Biased estimator of the Variance of a Gaussian Distribution,Biased estimator of the Variance of a Gaussian Distribution,,I'm reading the Deep Learning book and I've encountered a section that is difficult for me to understand. Specifically transformation from equation (5.38) to (5.39) $$ \mathbb{E}\left[\hat{\sigma}^2_m\right]=\mathbb{E}\left[\frac{1}{m}\sum_{i=1}^{m}\left( x^{(i)} - \hat{\mu}_m \right)^2 \right] \\ = \frac{m - 1}{m}\sigma^2 $$ Can anyone explain me how we got to the $\frac{m - 1}{m}\sigma^2$?,I'm reading the Deep Learning book and I've encountered a section that is difficult for me to understand. Specifically transformation from equation (5.38) to (5.39) $$ \mathbb{E}\left[\hat{\sigma}^2_m\right]=\mathbb{E}\left[\frac{1}{m}\sum_{i=1}^{m}\left( x^{(i)} - \hat{\mu}_m \right)^2 \right] \\ = \frac{m - 1}{m}\sigma^2 $$ Can anyone explain me how we got to the $\frac{m - 1}{m}\sigma^2$?,,"['probability', 'statistics', 'self-learning', 'variance']"
86,Why do we transform data using natural logs?,Why do we transform data using natural logs?,,"I was reading an intro to stats textbook and I came across this: 'There are some standard transformations that are often applied when   much of the data cluster near zero (relative to the larger values in   the data set) and all observations are positive. A transformation is a   rescaling of the data using a function. For instance, a plot of the   natural logarithm...Transformed data are sometimes easier to work with   when applying statistical models because the transformed data are much   less skewed and outliers are usually less extreme. ' but doesn't applying a log function to every datapoint potentially change their relative distance to each other? For example, say I took the log base 10 of two numbers: 10 and 100. The respective logs would be 1 and 10. Oh I see... the two numbers are still in the same ratio. Is this why taking the logs of a data set is acceptable to do? The relative distance of each number is still the same? Does the skew change when you do this? Is that a problem? Do the numbers still have the same middle 50% of data?","I was reading an intro to stats textbook and I came across this: 'There are some standard transformations that are often applied when   much of the data cluster near zero (relative to the larger values in   the data set) and all observations are positive. A transformation is a   rescaling of the data using a function. For instance, a plot of the   natural logarithm...Transformed data are sometimes easier to work with   when applying statistical models because the transformed data are much   less skewed and outliers are usually less extreme. ' but doesn't applying a log function to every datapoint potentially change their relative distance to each other? For example, say I took the log base 10 of two numbers: 10 and 100. The respective logs would be 1 and 10. Oh I see... the two numbers are still in the same ratio. Is this why taking the logs of a data set is acceptable to do? The relative distance of each number is still the same? Does the skew change when you do this? Is that a problem? Do the numbers still have the same middle 50% of data?",,['statistics']
87,Why do a before and after in a control group on a t-test,Why do a before and after in a control group on a t-test,,"I need to split my data by dates. They want to split it in the middle of the year, before and after june. These are then made into 6 groups, before and after in the years 1999, 2000, 2001. They want to compare how much money they got before and after the interventions for all these years. However, they are also thinking of doing t-tests on control groups on these dates. For example, the control group did not have any interventions. They are then thinking of comparing the difference in the control group compared to the difference in the experimental group. This seems like a lot of t-tests which can increase type 1 error. Would a one way anova be the best way to go about doing a test like this. Is there a better way to do this? Also, is there a point of having a control group in this situation.Would the before count as a control group. This is very confusing, but i am hoping a stats guru can help.","I need to split my data by dates. They want to split it in the middle of the year, before and after june. These are then made into 6 groups, before and after in the years 1999, 2000, 2001. They want to compare how much money they got before and after the interventions for all these years. However, they are also thinking of doing t-tests on control groups on these dates. For example, the control group did not have any interventions. They are then thinking of comparing the difference in the control group compared to the difference in the experimental group. This seems like a lot of t-tests which can increase type 1 error. Would a one way anova be the best way to go about doing a test like this. Is there a better way to do this? Also, is there a point of having a control group in this situation.Would the before count as a control group. This is very confusing, but i am hoping a stats guru can help.",,['statistics']
88,Find UMVU estimator for $\frac{\mu }{\sigma}$,Find UMVU estimator for,\frac{\mu }{\sigma},"Let $X_{1},\cdots,X_{n}$ be random samples which has normal   distribution $N(\mu,\sigma^{2})$. When $\mu$ and $\sigma^{2}$ are unknown, I want to find UMVU estimator   for $\frac{\mu}{\sigma}$. I know that $(\sum_{i=1}^{n}X_{i},\sum_{i=1}^{n}X_{i}^{2})$ is complete sufficient statistic for $\left(\mu,\sigma^{2}\right)$. Let $\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$, $S^{2}=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}$. Then $E(\bar{X})=\mu$ and $E(S^{2})=\sigma^{2}+\mu^{2}$. So for $\frac{\mu}{\sigma}$, I guess as a estimator $$ Y^{2}=\frac{\bar{X}^{2}}{\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}-\bar{X}}. $$ But how can I compute $E(Y^{2})?$","Let $X_{1},\cdots,X_{n}$ be random samples which has normal   distribution $N(\mu,\sigma^{2})$. When $\mu$ and $\sigma^{2}$ are unknown, I want to find UMVU estimator   for $\frac{\mu}{\sigma}$. I know that $(\sum_{i=1}^{n}X_{i},\sum_{i=1}^{n}X_{i}^{2})$ is complete sufficient statistic for $\left(\mu,\sigma^{2}\right)$. Let $\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$, $S^{2}=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}$. Then $E(\bar{X})=\mu$ and $E(S^{2})=\sigma^{2}+\mu^{2}$. So for $\frac{\mu}{\sigma}$, I guess as a estimator $$ Y^{2}=\frac{\bar{X}^{2}}{\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}-\bar{X}}. $$ But how can I compute $E(Y^{2})?$",,"['statistics', 'normal-distribution', 'statistical-inference', 'parameter-estimation']"
89,Theoretical Median and Mean question [closed],Theoretical Median and Mean question [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How likely is this scenario? In a class of 100 people who take an exam, the median of the exam is 82 and the mean is 77.   What if the sample size is cut in half and the median/mean stay the same? So how likely is the initial scenario and how does it change when the sample size is reduced or increased. Thanks for any help. Edit: I would think that a difference between the median and mean for a sample of that size would be unlikely in the first place?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How likely is this scenario? In a class of 100 people who take an exam, the median of the exam is 82 and the mean is 77.   What if the sample size is cut in half and the median/mean stay the same? So how likely is the initial scenario and how does it change when the sample size is reduced or increased. Thanks for any help. Edit: I would think that a difference between the median and mean for a sample of that size would be unlikely in the first place?",,"['probability', 'statistics']"
90,derivation of normal distribution pdf question?,derivation of normal distribution pdf question?,,"I went through a derivation of the normal distribution probability density function here but was surprised how few assumptions are made in the derivation. In particular it seemed that any probability density function that was differentiable and integrable, symmetric about the mean, always greater than zero, and with finite variance (and possibly monotonically decreasing with increasing $|x|$) must always have this form: \begin{equation} f(x;\mu,\sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} e^{ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2 } \end{equation} What am I missing here? Does the mere existence of a pdf with the properties I mentioned above imply that it describes a normal distribution?","I went through a derivation of the normal distribution probability density function here but was surprised how few assumptions are made in the derivation. In particular it seemed that any probability density function that was differentiable and integrable, symmetric about the mean, always greater than zero, and with finite variance (and possibly monotonically decreasing with increasing $|x|$) must always have this form: \begin{equation} f(x;\mu,\sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} e^{ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2 } \end{equation} What am I missing here? Does the mere existence of a pdf with the properties I mentioned above imply that it describes a normal distribution?",,"['statistics', 'normal-distribution']"
91,How does one solve a bivariate normal density function?,How does one solve a bivariate normal density function?,,"If the exponent of $e$ of a bivariate normal density is $$\frac{-1}{54} *(x^2+4y^2+2xy+2x+8y+4) \\\text{find } \sigma_{1},\sigma_{2} \text{ and } p \text{ given that } \mu_{1} =0 \text{ and } \mu_{2}=-1.  $$ One must use this definition to solve. A pair of random variables $X$ and $Y$ have a bivariate normal distribution and they are referred to as jointly normally distribed random variables if and if their joint probability density is given by If $X$ and $Y$ have a bivariate normal distribution normal distribution the conditional density of $Y$ given $X =x$ is a normal distribution with the mean $\mu_{Y|x} = \mu_{2} + \rho \cdot \frac{\sigma_2}{\sigma_1}\cdot (x-\mu_{1})$ and the variance $\sigma^{2}_{Y|x} = \sigma^{2}_{2}(1-\rho^2)$ and the conditonal density of $X$ given $Y=y$ is a normal distribution with the mean $\mu_{X|y} = \mu_{1} + \rho\cdot \frac{\sigma_{1}}{\sigma_{2}}\cdot (y-\mu_{2})$ and the variance $\sigma^{2}_{X|y} = \sigma^{2}_{1}(1-\rho^2)$ Does one re arrange the formula so that one can just plug in what $\mu_{1} \text{ and } \mu_{2}$ are? Does one substitute like this? $$f(x,y) = \frac{-1}{54} *(x^2+4y^2+2xy+2x+8y+4) \\ = \frac{1}{-54} \left[ (x-\mu_{1})^2 +2(x-\mu_{1})(y-\mu_{2})+4(y-\mu_{2})^2 \right]$$ EDIT : Thanks to the valiant work of Anatoly I understand one needs to compare coefficients in order to derive $\rho,\sigma_{1},\sigma_{2}$ $$\color{lime}{(1)}: \frac{1}{2(1-p^2)\sigma^2_{1}} = \frac{1}{54}$$ However after this one I do not know what to do does anyone know how to do the rest?","If the exponent of $e$ of a bivariate normal density is $$\frac{-1}{54} *(x^2+4y^2+2xy+2x+8y+4) \\\text{find } \sigma_{1},\sigma_{2} \text{ and } p \text{ given that } \mu_{1} =0 \text{ and } \mu_{2}=-1.  $$ One must use this definition to solve. A pair of random variables $X$ and $Y$ have a bivariate normal distribution and they are referred to as jointly normally distribed random variables if and if their joint probability density is given by If $X$ and $Y$ have a bivariate normal distribution normal distribution the conditional density of $Y$ given $X =x$ is a normal distribution with the mean $\mu_{Y|x} = \mu_{2} + \rho \cdot \frac{\sigma_2}{\sigma_1}\cdot (x-\mu_{1})$ and the variance $\sigma^{2}_{Y|x} = \sigma^{2}_{2}(1-\rho^2)$ and the conditonal density of $X$ given $Y=y$ is a normal distribution with the mean $\mu_{X|y} = \mu_{1} + \rho\cdot \frac{\sigma_{1}}{\sigma_{2}}\cdot (y-\mu_{2})$ and the variance $\sigma^{2}_{X|y} = \sigma^{2}_{1}(1-\rho^2)$ Does one re arrange the formula so that one can just plug in what $\mu_{1} \text{ and } \mu_{2}$ are? Does one substitute like this? $$f(x,y) = \frac{-1}{54} *(x^2+4y^2+2xy+2x+8y+4) \\ = \frac{1}{-54} \left[ (x-\mu_{1})^2 +2(x-\mu_{1})(y-\mu_{2})+4(y-\mu_{2})^2 \right]$$ EDIT : Thanks to the valiant work of Anatoly I understand one needs to compare coefficients in order to derive $\rho,\sigma_{1},\sigma_{2}$ $$\color{lime}{(1)}: \frac{1}{2(1-p^2)\sigma^2_{1}} = \frac{1}{54}$$ However after this one I do not know what to do does anyone know how to do the rest?",,"['probability', 'statistics', 'normal-distribution', 'bivariate-distributions']"
92,Mean square error using linear regression gives a convex function,Mean square error using linear regression gives a convex function,,"In my machine learning course I'm asked to proof that $MSE(w) = \frac{1}{N} \sum_{n=1}^{N}[y_n - f(x_n)]^2$ when $f$ is of the form $w_0 + x_{n1} w_1 + \cdots + x_{nD} w_D$ is a convex function on the parameters $w_1,\cdots, w_D$. I'm a bit confused on how to do it though. My approach goes as follows: I first proof that the sum of two convex functions is convex. Then I try to develop the value of $(y_n - f(x_n))^2$ however I arrive to the point where I have to proof that a function of type $w_iw_j$ is convex which is not true as shown in this post. How can I proceed?","In my machine learning course I'm asked to proof that $MSE(w) = \frac{1}{N} \sum_{n=1}^{N}[y_n - f(x_n)]^2$ when $f$ is of the form $w_0 + x_{n1} w_1 + \cdots + x_{nD} w_D$ is a convex function on the parameters $w_1,\cdots, w_D$. I'm a bit confused on how to do it though. My approach goes as follows: I first proof that the sum of two convex functions is convex. Then I try to develop the value of $(y_n - f(x_n))^2$ however I arrive to the point where I have to proof that a function of type $w_iw_j$ is convex which is not true as shown in this post. How can I proceed?",,"['probability', 'statistics', 'convex-analysis', 'mean-square-error']"
93,Prove standard deviation $=\sqrt{npq}$,Prove standard deviation,=\sqrt{npq},"In class we learnt that for a binomial experiment, the standard deviation is equal to $\sqrt{npq}$. Is there a way to derive/prove this? I have a basic understanding of standard deviation and binomial but have no idea where to start in this proof.","In class we learnt that for a binomial experiment, the standard deviation is equal to $\sqrt{npq}$. Is there a way to derive/prove this? I have a basic understanding of standard deviation and binomial but have no idea where to start in this proof.",,"['statistics', 'standard-deviation', 'binomial-distribution']"
94,Outlier-resistant average of a set?,Outlier-resistant average of a set?,,"I have a web application in which users can provide valuations for items.  These valuations are currently unrestricted, and there is no non-arbitrary criteria by which I could restrict them, since valuations are purely subjective.  For each item, I have an ""average valuation"", which is currently just a simple mean of all valuations for the item.  This was a naive choice that has inevitably lead to abuse, whereby a small subset of users intentionally provide extreme valuations to manipulate this ""average valuation"" to their benefit. I'd like to solve this problem mathematically, without imposing arbitrary restrictions on submitted valuations.  As far as I can tell, it's very commonplace for item valuations to follow a normal distribution, but I admittedly don't remember much of my statistics from decades ago.  I have been attempting to brush up on standard deviations, but it seems like I can't exclude valuations over 2σ because these extreme outliers raise the population standard deviation (I could be misunderstanding this concept, though).  So... What statistical approach can I take to determine the ""average valuation"", while remaining resistant to this kind of abuse? For reference, here is an example set of valuations for a particular item.  The outliers here are clearly the [317, 318, 630, 630, 640, 6511] subset: 60, 63, 63, 63, 63, 63.5, 63.8, 63.8, 63.9, 63.9, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64.2, 64.5, 64.5, 64.5, 64.5, 64.5, 64.9, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66.5, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 72, 74, 74, 75, 75, 75, 75, 78, 80, 85, 317, 318, 630, 630, 640, 6511 For this set of valuations, I would expect an ""average valuation"" somewhere around 65 or 66.","I have a web application in which users can provide valuations for items.  These valuations are currently unrestricted, and there is no non-arbitrary criteria by which I could restrict them, since valuations are purely subjective.  For each item, I have an ""average valuation"", which is currently just a simple mean of all valuations for the item.  This was a naive choice that has inevitably lead to abuse, whereby a small subset of users intentionally provide extreme valuations to manipulate this ""average valuation"" to their benefit. I'd like to solve this problem mathematically, without imposing arbitrary restrictions on submitted valuations.  As far as I can tell, it's very commonplace for item valuations to follow a normal distribution, but I admittedly don't remember much of my statistics from decades ago.  I have been attempting to brush up on standard deviations, but it seems like I can't exclude valuations over 2σ because these extreme outliers raise the population standard deviation (I could be misunderstanding this concept, though).  So... What statistical approach can I take to determine the ""average valuation"", while remaining resistant to this kind of abuse? For reference, here is an example set of valuations for a particular item.  The outliers here are clearly the [317, 318, 630, 630, 640, 6511] subset: 60, 63, 63, 63, 63, 63.5, 63.8, 63.8, 63.9, 63.9, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64.2, 64.5, 64.5, 64.5, 64.5, 64.5, 64.9, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66.5, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 72, 74, 74, 75, 75, 75, 75, 78, 80, 85, 317, 318, 630, 630, 640, 6511 For this set of valuations, I would expect an ""average valuation"" somewhere around 65 or 66.",,"['statistics', 'standard-deviation']"
95,Probability to win one prize in 5 draws from 100 tickets with one prize for each draw,Probability to win one prize in 5 draws from 100 tickets with one prize for each draw,,"Suppose that you hold one of the 100 tickets in a lottery.  There are 5 prizes altogether, 1st prize for the first draw, 2nd prize for the second draw, etc.  Assume the draws are without replacement.  Find the probability that you win one prize. My attempt:  1/100+1/99+1/98+1/97+1/96.  Not sure if this is correct. Please help Edit: or would it be $\dfrac{5 \choose 1}{100 \choose 5}$?","Suppose that you hold one of the 100 tickets in a lottery.  There are 5 prizes altogether, 1st prize for the first draw, 2nd prize for the second draw, etc.  Assume the draws are without replacement.  Find the probability that you win one prize. My attempt:  1/100+1/99+1/98+1/97+1/96.  Not sure if this is correct. Please help Edit: or would it be $\dfrac{5 \choose 1}{100 \choose 5}$?",,"['probability', 'statistics']"
96,a loaded die? Practice,a loaded die? Practice,,"Three dice were rolled.  The first two were rolled 25 times, and the third was rolled 50 times.  For each die find a number that ranks it relative to the others in terms of how fair it is (or how much its distribution of scores differs from chance).  Circle which die you think is Most Fair, Somewhat fair, and least fair.  Make sure your numbers support your ranking.  There are no ties (i.e., only one die is Most Fair, etc.). Die 1: Rolling a 5-sided die.  The # of times each result appears. ""1""  6 times ""2""  5 times ""3""  4 times ""4""  4 times ""5""  6 times Die #2 ""1""  3 times ""2""  5 times ""3""  10 times ""4""  4 times ""5""  3 times Die #3 ""1""  11 times ""2""   10 times ""3""  9 times ""4""  9 times ""5""  11 times What is your formula????","Three dice were rolled.  The first two were rolled 25 times, and the third was rolled 50 times.  For each die find a number that ranks it relative to the others in terms of how fair it is (or how much its distribution of scores differs from chance).  Circle which die you think is Most Fair, Somewhat fair, and least fair.  Make sure your numbers support your ranking.  There are no ties (i.e., only one die is Most Fair, etc.). Die 1: Rolling a 5-sided die.  The # of times each result appears. ""1""  6 times ""2""  5 times ""3""  4 times ""4""  4 times ""5""  6 times Die #2 ""1""  3 times ""2""  5 times ""3""  10 times ""4""  4 times ""5""  3 times Die #3 ""1""  11 times ""2""   10 times ""3""  9 times ""4""  9 times ""5""  11 times What is your formula????",,['statistics']
97,Binomial Distribution Solving for N,Binomial Distribution Solving for N,,"I am having trouble trying to help a sibling do their statistics homework, so I thought someone could help me. The problem asks to solve for N here: $$\sum_{i=6}^N\binom Ni(0.25)^i(0.75)^{N-i}<0.005$$ Could someone please explain how to solve this by hand, as well as how to input this into a TI-83 or 84 calculator? If you could, please explain thoroughly, as it has been quite a while since I have taken statistics. Thank you!","I am having trouble trying to help a sibling do their statistics homework, so I thought someone could help me. The problem asks to solve for N here: $$\sum_{i=6}^N\binom Ni(0.25)^i(0.75)^{N-i}<0.005$$ Could someone please explain how to solve this by hand, as well as how to input this into a TI-83 or 84 calculator? If you could, please explain thoroughly, as it has been quite a while since I have taken statistics. Thank you!",,"['statistics', 'binomial-distribution']"
98,"A near ""geometric mean"" that handles zeros and negative numbers","A near ""geometric mean"" that handles zeros and negative numbers",,"By definition of a geomean, a geometric mean of a set of numbers containing zero is 0. (See Geometric mean of a dataset containing zeros )  However in financial data that is commonplace and sometimes finance data has negative numbers. The inverse hyperbolic sine mean doesn't suffer from these shortcomings. Let's define that as: sinh(mean(asinh(money))) It handles negatives, positives and zeros with ease. Why won't people use the inverse hyperbolic sine mean (IHS) mean? Why don't they teach the IHS mean? Is there a better term for this? Surely someone else has thought of it. This isn't a question about R as the code is only given to be informative. set.seed(12) (money=sort(round(rlnorm(100,6,4)))+1) sinh(mean(asinh(money))) #gives 393.7934 exp(mean(log(money))) #gives 391.2577 prod(money)^(1/length(money)) #gives 391.2577 but can overflow median(money) #261.5 Notice with positive numbers, the IHS mean is nearly identical. set.seed(12) (money=sort(round(rlnorm(100,6,4)))) sinh(mean(asinh(money))) #gives 369.4342 exp(mean(log(money))) #gives 0 prod(money)^(1/length(money)) #gives 0  median(money) #260.5 Notice with positive numbers and zeros, the IHS mean is more useful. set.seed(12) (money=sort(round(rlnorm(100,6,4)))-10000) sinh(mean(asinh(money))) #gives -201.5435 exp(mean(log(money))) #gives NaN prod(money)^(1/length(money)) #gives Inf so WRONG due to overflow median(money) #-9739.5 Notice with mixed negative/positive numbers and zeros, the IHS mean is more useful. set.seed(12) (money=sort(round(rlnorm(100,6,4)))*-1) sinh(mean(asinh(money))) #gives -369.4342 exp(mean(log(money))) #gives NaN prod(money)^(1/length(money)) #gives 0 median(money) #-260.5 Notice with purely negative, the IHS mean is still useful.","By definition of a geomean, a geometric mean of a set of numbers containing zero is 0. (See Geometric mean of a dataset containing zeros )  However in financial data that is commonplace and sometimes finance data has negative numbers. The inverse hyperbolic sine mean doesn't suffer from these shortcomings. Let's define that as: sinh(mean(asinh(money))) It handles negatives, positives and zeros with ease. Why won't people use the inverse hyperbolic sine mean (IHS) mean? Why don't they teach the IHS mean? Is there a better term for this? Surely someone else has thought of it. This isn't a question about R as the code is only given to be informative. set.seed(12) (money=sort(round(rlnorm(100,6,4)))+1) sinh(mean(asinh(money))) #gives 393.7934 exp(mean(log(money))) #gives 391.2577 prod(money)^(1/length(money)) #gives 391.2577 but can overflow median(money) #261.5 Notice with positive numbers, the IHS mean is nearly identical. set.seed(12) (money=sort(round(rlnorm(100,6,4)))) sinh(mean(asinh(money))) #gives 369.4342 exp(mean(log(money))) #gives 0 prod(money)^(1/length(money)) #gives 0  median(money) #260.5 Notice with positive numbers and zeros, the IHS mean is more useful. set.seed(12) (money=sort(round(rlnorm(100,6,4)))-10000) sinh(mean(asinh(money))) #gives -201.5435 exp(mean(log(money))) #gives NaN prod(money)^(1/length(money)) #gives Inf so WRONG due to overflow median(money) #-9739.5 Notice with mixed negative/positive numbers and zeros, the IHS mean is more useful. set.seed(12) (money=sort(round(rlnorm(100,6,4)))*-1) sinh(mean(asinh(money))) #gives -369.4342 exp(mean(log(money))) #gives NaN prod(money)^(1/length(money)) #gives 0 median(money) #-260.5 Notice with purely negative, the IHS mean is still useful.",,"['statistics', 'finance', 'economics', 'hyperbolic-functions', 'means']"
99,The covariance of two squared normal variables,The covariance of two squared normal variables,,"Let $X\sim\mathcal N(μ_1,σ_1^2)$ and $Y\sim\mathcal N(μ_2,σ_2^2)$ and $\mathsf{Cov}(X,Y)=c$ How can we compute $\mathsf {Cov}(X^2,Y^2)$? I think the answer should be something like $c^2$ and I think the joint PDF is really not necessary here. I think the approach is using some independent standard normal random variables by variable changing. But I can't make it out. Thanks.","Let $X\sim\mathcal N(μ_1,σ_1^2)$ and $Y\sim\mathcal N(μ_2,σ_2^2)$ and $\mathsf{Cov}(X,Y)=c$ How can we compute $\mathsf {Cov}(X^2,Y^2)$? I think the answer should be something like $c^2$ and I think the joint PDF is really not necessary here. I think the approach is using some independent standard normal random variables by variable changing. But I can't make it out. Thanks.",,"['probability', 'statistics', 'random-variables', 'normal-distribution', 'covariance']"
