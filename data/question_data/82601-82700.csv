,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Hunting for a basis: Rational Canonical Form,Hunting for a basis: Rational Canonical Form,,"I am trying to understand how to find the matrix of transition for the rational canonical form purely using linear algebra (so no $F[x]$ modules). I do realize that Dummit & Foote provide a method for finding the matrix of transition for both RCF and JCF that involves keeping track of the row and column operations done to put our matrix in smith normal form. However, I am trying to do this without that trouble. Here is the particular matrix I'm going to work with: $$ A = \begin{bmatrix} 2 & 0 & 0 \\ 9 & 7 & 5 \\ -9 & -5 & -3 \end{bmatrix}$$ With little to no effort, we get that the minimal polynomial of $A$ is $m(x) = (x-2)^2$ so that the largest invariant factor, and hence the largest block has size 2. All in all we know that the corresponding matrix is $$ R = \begin{bmatrix} 0 & -4 & 0 \\ 1 & 4 & 0 \\ 0 & 0 & 2 \end{bmatrix} $$ A basis vector corresponding to the block of size 1 is just an eigenvector, and should be easy enough to find once we have found the other two vectors for the block of size 2. The block of size 2 should have as a basis $\{v, Av\}$. Now I'm not sure where to hunt for the vector $v$. I realize that we need for $Av = Av$ and $A^2 v = 4Av - 4v$. So the question is, where do we hunt for these basis vectors when finding the matrix of transition to the rational canonical form?","I am trying to understand how to find the matrix of transition for the rational canonical form purely using linear algebra (so no $F[x]$ modules). I do realize that Dummit & Foote provide a method for finding the matrix of transition for both RCF and JCF that involves keeping track of the row and column operations done to put our matrix in smith normal form. However, I am trying to do this without that trouble. Here is the particular matrix I'm going to work with: $$ A = \begin{bmatrix} 2 & 0 & 0 \\ 9 & 7 & 5 \\ -9 & -5 & -3 \end{bmatrix}$$ With little to no effort, we get that the minimal polynomial of $A$ is $m(x) = (x-2)^2$ so that the largest invariant factor, and hence the largest block has size 2. All in all we know that the corresponding matrix is $$ R = \begin{bmatrix} 0 & -4 & 0 \\ 1 & 4 & 0 \\ 0 & 0 & 2 \end{bmatrix} $$ A basis vector corresponding to the block of size 1 is just an eigenvector, and should be easy enough to find once we have found the other two vectors for the block of size 2. The block of size 2 should have as a basis $\{v, Av\}$. Now I'm not sure where to hunt for the vector $v$. I realize that we need for $Av = Av$ and $A^2 v = 4Av - 4v$. So the question is, where do we hunt for these basis vectors when finding the matrix of transition to the rational canonical form?",,"['linear-algebra', 'matrices']"
1,What kind of radical makes sense?,What kind of radical makes sense?,,"I have the following exercise: Find the radical of the subring $S\subset M_n(\mathbb{C})$, where $S$ consists of upper triangular matrices. Unfortunately it is not specified what kind of radical is ment. What makes the most sense here? The Jacobson radical?","I have the following exercise: Find the radical of the subring $S\subset M_n(\mathbb{C})$, where $S$ consists of upper triangular matrices. Unfortunately it is not specified what kind of radical is ment. What makes the most sense here? The Jacobson radical?",,"['abstract-algebra', 'matrices', 'ideals']"
2,Is there a name for a matrix whose n-th power is the identity matrix,Is there a name for a matrix whose n-th power is the identity matrix,,"I am new to this community so my apologies it this is a duplicate, feel free to flag it. I am currently working on cyclically symmetric structural mechanics and we exploit the finite group linear representation. In this theory, properties of rotation matrices are used, specifically the fact that for a rotation matrix $\mathbf{R}$ of angle $2\pi/N$, $\mathbf{R}^{N} = \mathbf{I}$. Matrices such that $\mathbf{R}^{N} = \mathbf{0}$ are called nilpotent matrices, but is there a name for the mentioned rotation matrices ?","I am new to this community so my apologies it this is a duplicate, feel free to flag it. I am currently working on cyclically symmetric structural mechanics and we exploit the finite group linear representation. In this theory, properties of rotation matrices are used, specifically the fact that for a rotation matrix $\mathbf{R}$ of angle $2\pi/N$, $\mathbf{R}^{N} = \mathbf{I}$. Matrices such that $\mathbf{R}^{N} = \mathbf{0}$ are called nilpotent matrices, but is there a name for the mentioned rotation matrices ?",,"['linear-algebra', 'matrices', 'terminology']"
3,Jordan block size,Jordan block size,,I was wondering about the size of the Jordan blocks of the following matrix. $$\begin{bmatrix} 0 & 1 & 0\\ 0 & 0 & 1\\ 0 & 0 & 0  \end{bmatrix}$$ I know that Jordan blocks have $1$'s on the superdiagonal. So are these $3$ blocks of size $1 \times 1$ or is this one block of size $3 \times 3$? I'm not sure how to tell the difference. Thanks in advance.,I was wondering about the size of the Jordan blocks of the following matrix. $$\begin{bmatrix} 0 & 1 & 0\\ 0 & 0 & 1\\ 0 & 0 & 0  \end{bmatrix}$$ I know that Jordan blocks have $1$'s on the superdiagonal. So are these $3$ blocks of size $1 \times 1$ or is this one block of size $3 \times 3$? I'm not sure how to tell the difference. Thanks in advance.,,"['matrices', 'jordan-normal-form']"
4,Prove a rank equality for PSD matrices,Prove a rank equality for PSD matrices,,"(To make things a bit easier perhaps, we assume that $A,B$ are symmetric at least. If you find out that this assumption can be dropped, please also give your stronger version of course.) Prove that: $B$ is PSD (positive semi-definite) iff for any $A$ PSD we have $$r(A+B)=r([A |B])$$ where $r$ denotes the matrix rank and $|$ denotes matrix concatenation. This might be one of the few times that I haven't had any clue to the problem I'm asking on this site. Any help?","(To make things a bit easier perhaps, we assume that $A,B$ are symmetric at least. If you find out that this assumption can be dropped, please also give your stronger version of course.) Prove that: $B$ is PSD (positive semi-definite) iff for any $A$ PSD we have $$r(A+B)=r([A |B])$$ where $r$ denotes the matrix rank and $|$ denotes matrix concatenation. This might be one of the few times that I haven't had any clue to the problem I'm asking on this site. Any help?",,"['linear-algebra', 'matrices', 'positive-definite']"
5,How do you compute the derivative of a matrix algebra expression?,How do you compute the derivative of a matrix algebra expression?,,"I was working along with http://jimherold.com/2012/04/20/least-squares-bezier-fit/ to see if I could understand each step of the way for fitting a Bezier curve to a set of coordinates, and I understand every step except one: computing the derivative of a particular matrix multiplication. The article gives the least squares error function between the real coordinates and the coordinates yielded by the to-be-derived Bezier function as: $ E(C_y) = \sum^n_{i=1} \left ( y_i - B(t_i) \right )^2$ Which makes a lot of sense, and then restates this as a matrix expression: $ E(C_y) = \left ( y - \mathbb{T}MC_y \right )^T \left ( y - \mathbb{T}MC_y \right ) $ In this, $y$ is the y-coordinate vector, $M$ is the Bernstein polynomial coefficients for the Bezier curve in matrix form, $C_y$ is the ""y coordinates we're hoping to find, to use in constructing a well-fitting curve"", and $\mathbb{T}$ is a matrix of ""powers of $t$ specific to fitting a curve to the polygon described by the original coordinates. Going through the article, all of that makes sense. However, the article then describes that to find the smallest error, we want to find the roots for the derivative of the error function, and states this as solving: $ \frac{\partial E}{\partial C} = 0 = -2ð•‹^T \left ( y - ð•‹MC_y \right ) $ While I can verify that this works, I don't know why it works. I have no idea which rules to use to go from the original error function to this derivative. How can I get from the former to the latter?","I was working along with http://jimherold.com/2012/04/20/least-squares-bezier-fit/ to see if I could understand each step of the way for fitting a Bezier curve to a set of coordinates, and I understand every step except one: computing the derivative of a particular matrix multiplication. The article gives the least squares error function between the real coordinates and the coordinates yielded by the to-be-derived Bezier function as: $ E(C_y) = \sum^n_{i=1} \left ( y_i - B(t_i) \right )^2$ Which makes a lot of sense, and then restates this as a matrix expression: $ E(C_y) = \left ( y - \mathbb{T}MC_y \right )^T \left ( y - \mathbb{T}MC_y \right ) $ In this, $y$ is the y-coordinate vector, $M$ is the Bernstein polynomial coefficients for the Bezier curve in matrix form, $C_y$ is the ""y coordinates we're hoping to find, to use in constructing a well-fitting curve"", and $\mathbb{T}$ is a matrix of ""powers of $t$ specific to fitting a curve to the polygon described by the original coordinates. Going through the article, all of that makes sense. However, the article then describes that to find the smallest error, we want to find the roots for the derivative of the error function, and states this as solving: $ \frac{\partial E}{\partial C} = 0 = -2ð•‹^T \left ( y - ð•‹MC_y \right ) $ While I can verify that this works, I don't know why it works. I have no idea which rules to use to go from the original error function to this derivative. How can I get from the former to the latter?",,"['matrices', 'derivatives', 'bezier-curve']"
6,the connection between matrix and convex cone,the connection between matrix and convex cone,,I'm trying to understand the connection between convex cone and matarix. according to Boyd  as you can see in the pic: X is a p.s.d matrix but how this matrix represent a convex cone? and why the matrix is looking like this the the value 'y' appears twice? what am I missing from here? Another thins is what should I do when the matrix is 3x3 or 4x4? how do I need to build the matrix then? thnx in advanced,I'm trying to understand the connection between convex cone and matarix. according to Boyd  as you can see in the pic: X is a p.s.d matrix but how this matrix represent a convex cone? and why the matrix is looking like this the the value 'y' appears twice? what am I missing from here? Another thins is what should I do when the matrix is 3x3 or 4x4? how do I need to build the matrix then? thnx in advanced,,"['matrices', 'optimization', 'convex-optimization', 'convex-cone']"
7,"If there are $6$ matrices in $M_{6,6}(\Bbb C)$ such that they all satisfy $A^2=0$, does this imply that at least two of them are similar?","If there are  matrices in  such that they all satisfy , does this imply that at least two of them are similar?","6 M_{6,6}(\Bbb C) A^2=0","If there are $6$ matrices in the vector space $M_{6,6}(\Bbb C)$ such that they all satisfy $A^2=0$, does this imply that at least two of them are similar? My approach: Observation: $A^2=0$ implies that all the eigenvalues of these matrices must equal to zero. Which naturally led me to think about the Jordan forms of these, to try and see if there are less than $6$ types of Jordan matrices corresponding to this property as that would be mean that at least two of these matrices should have the same Jordan matrix, and hence must be similar.  I found that there are more than $6$ types of corresponding Jordan matrices, so my current answer is ""not necessarily true"", but I need a definite true or false answer here.  Could someone give me a thought process?","If there are $6$ matrices in the vector space $M_{6,6}(\Bbb C)$ such that they all satisfy $A^2=0$, does this imply that at least two of them are similar? My approach: Observation: $A^2=0$ implies that all the eigenvalues of these matrices must equal to zero. Which naturally led me to think about the Jordan forms of these, to try and see if there are less than $6$ types of Jordan matrices corresponding to this property as that would be mean that at least two of these matrices should have the same Jordan matrix, and hence must be similar.  I found that there are more than $6$ types of corresponding Jordan matrices, so my current answer is ""not necessarily true"", but I need a definite true or false answer here.  Could someone give me a thought process?",,['linear-algebra']
8,Trace of positive semidefinite matrix,Trace of positive semidefinite matrix,,Let $A =X +B$ with $X= (X_1+X_2) $all three semidefinite positive and B definite positive.  How can i proove that $$ trace A^{-1}X \leq trace( (X_1+ B)^{-1}X_1  +(X_2 +B)^{-1}X_2)$$,Let $A =X +B$ with $X= (X_1+X_2) $all three semidefinite positive and B definite positive.  How can i proove that $$ trace A^{-1}X \leq trace( (X_1+ B)^{-1}X_1  +(X_2 +B)^{-1}X_2)$$,,"['linear-algebra', 'matrices', 'inequality', 'trace', 'positive-semidefinite']"
9,Eigenvalues of a complex $4 \times 4$ Matrix,Eigenvalues of a complex  Matrix,4 \times 4,"Let $A\in \mathbb{C}^{4Ã—4}$. We are given the equation $$A^{4}+12A^{2}=6A^{3}+8A$$ and $$\textrm{rank}(A)=2\cdot\textrm{rank}(A-2I_{4})=4$$ I already found out that $2$ is an Eigenvalue, but how do I determine the other ones ? I'm mainly looking for hints.","Let $A\in \mathbb{C}^{4Ã—4}$. We are given the equation $$A^{4}+12A^{2}=6A^{3}+8A$$ and $$\textrm{rank}(A)=2\cdot\textrm{rank}(A-2I_{4})=4$$ I already found out that $2$ is an Eigenvalue, but how do I determine the other ones ? I'm mainly looking for hints.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
10,norm of a vector and linear transformation,norm of a vector and linear transformation,,"Let $A \in M_{n}(\mathbb{C})$ be nonsingular matrix. Then I m trying to find the dual norm of $\|x\|_{A} \triangleq\|Ax\|_{2}$. My try: \begin{align} \|y\|_{A}^{D} &= \max_{\|x\|_{A}=1} |y^*x| \\               &= \max _{\|Ax\|_{2}=1} |y^*x| \\               &= \max _{\|Ax\|=1} |y^*A^{-1}Ax| \\               &\leq \max _{\|Ax\|=1} \|y^*A^{-1}\|_{2} \|Ax\|_{2} \\               &= \|y^*A^{-1}\|_{2} \end{align} but now I don't know how to continue! Any idea, comment? Thanks","Let $A \in M_{n}(\mathbb{C})$ be nonsingular matrix. Then I m trying to find the dual norm of $\|x\|_{A} \triangleq\|Ax\|_{2}$. My try: \begin{align} \|y\|_{A}^{D} &= \max_{\|x\|_{A}=1} |y^*x| \\               &= \max _{\|Ax\|_{2}=1} |y^*x| \\               &= \max _{\|Ax\|=1} |y^*A^{-1}Ax| \\               &\leq \max _{\|Ax\|=1} \|y^*A^{-1}\|_{2} \|Ax\|_{2} \\               &= \|y^*A^{-1}\|_{2} \end{align} but now I don't know how to continue! Any idea, comment? Thanks",,"['linear-algebra', 'matrices']"
11,Show that: If $A \circ B =0$ but $A \neq 0$ and $B\neq 0$ then you have that $\det(A)=0=\det(B)$,Show that: If  but  and  then you have that,A \circ B =0 A \neq 0 B\neq 0 \det(A)=0=\det(B),"Show that: If $A$ and $B$ are real $n \times n$ matrices with $A \circ B=0$ but $A \neq 0$ and $B \neq 0$, then you have that   $\det(A)=0=\det(B)$ ($0$ is the matrix which only has zeroes) I'm not sure how to do this correctly but something tells me that a proof by contradiction would be very useful here. Let $\det(A) \neq 0 \Rightarrow B = Id \circ B = A^{-1} AB = 0 \Rightarrow B=0  \,\,\,\unicode{x21af}$ Let $\det(B) \neq 0 \Rightarrow A = Id \circ A = B^{-1} BA = 0 \Rightarrow A=0 \,\,\,\unicode{x21af}$ And that's why we must have that $\det(A)=0=\det(B)$ I think this should work because we just showed a contradiction for every possibility and this means that the negation of our assumption must be correct. But will this also cover that both determinants must necessarily be zero? I think it only shows that at least one determinant must be zero... : / Maybe there is a better way to prove this?","Show that: If $A$ and $B$ are real $n \times n$ matrices with $A \circ B=0$ but $A \neq 0$ and $B \neq 0$, then you have that   $\det(A)=0=\det(B)$ ($0$ is the matrix which only has zeroes) I'm not sure how to do this correctly but something tells me that a proof by contradiction would be very useful here. Let $\det(A) \neq 0 \Rightarrow B = Id \circ B = A^{-1} AB = 0 \Rightarrow B=0  \,\,\,\unicode{x21af}$ Let $\det(B) \neq 0 \Rightarrow A = Id \circ A = B^{-1} BA = 0 \Rightarrow A=0 \,\,\,\unicode{x21af}$ And that's why we must have that $\det(A)=0=\det(B)$ I think this should work because we just showed a contradiction for every possibility and this means that the negation of our assumption must be correct. But will this also cover that both determinants must necessarily be zero? I think it only shows that at least one determinant must be zero... : / Maybe there is a better way to prove this?",,"['linear-algebra', 'matrices', 'proof-verification', 'proof-writing', 'determinant']"
12,"$A,B$ are orthogonal projections and $\|Ax\|^2+\|Bx\|^2=\|x\|^2$ show $A+B=I$",are orthogonal projections and  show,"A,B \|Ax\|^2+\|Bx\|^2=\|x\|^2 A+B=I","Here is the problem : $A,B:\mathbb{C}^n\to\mathbb{C}^n$ are two orthogonal projections satisfying for any $x\in\mathbb{C}^n$, $$\|Ax\|^2+\|Bx\|^2=\|x\|^2$$ Show that $A+B=I$. I know that $\|Ax\|^2+\|Bx\|^2=\|x\|^2$ tells that $(Ax,Ax)+(Bx,Bx)=(x,x)$. Since $$\|(A+B)x\|^2=((A+B)x,(A+B)x)$$$$=(Ax,Ax)+(Bx,Bx)+(Ax,Bx)+(Bx,Ax)$$$$=(x,x)+(Ax,Bx)+(Bx,Ax)$$ It remian to show that $(Ax,Bx)+(Bx,Ax)=0$, but I am not sure how to show it. Please help, thanks a lot!","Here is the problem : $A,B:\mathbb{C}^n\to\mathbb{C}^n$ are two orthogonal projections satisfying for any $x\in\mathbb{C}^n$, $$\|Ax\|^2+\|Bx\|^2=\|x\|^2$$ Show that $A+B=I$. I know that $\|Ax\|^2+\|Bx\|^2=\|x\|^2$ tells that $(Ax,Ax)+(Bx,Bx)=(x,x)$. Since $$\|(A+B)x\|^2=((A+B)x,(A+B)x)$$$$=(Ax,Ax)+(Bx,Bx)+(Ax,Bx)+(Bx,Ax)$$$$=(x,x)+(Ax,Bx)+(Bx,Ax)$$ It remian to show that $(Ax,Bx)+(Bx,Ax)=0$, but I am not sure how to show it. Please help, thanks a lot!",,"['linear-algebra', 'matrices', 'inner-products', 'projection-matrices']"
13,Counting matrices over a finite field.,Counting matrices over a finite field.,,"Let $ \mathcal{M}^{k \times n} = \{ A \in \mathbb{F}_q^{k \times n} \mid A \text{ is full rank and every } \; k \times k \; \text{submatrix of }\; A \;\text{is invertible}\}$. I want to know $|\mathcal{M}^{k \times n}| $. I know that since $\mathcal{M}^{k \times n}$ is subset of the full rank matrices then the number I'm looking for is less than $(q^n-1)(q^n-q)\cdots (q^n-q^{k-1})$ (the number of full rank matrices) but if you make the count for $ k=1$ this seems to be false: When $ k=1$ every sub matrix having determinant distinct to zero is equivalent to being full rank, so one has to count only the number of vectors with non zero entries: $ (q^n-1)^n > q^n-1$. I know that I might be wrong but I can't see how Thank you all in advance Edit I forgot to add that $n>k $, this will implie that in $k=1 $ being full rank only means not to be the zero vector i.e we are only looking for the vectors with non zero entries (there are $ (q^n-1)^n > q^n-1$ many of them ) but hte equivalence I state does not holds.","Let $ \mathcal{M}^{k \times n} = \{ A \in \mathbb{F}_q^{k \times n} \mid A \text{ is full rank and every } \; k \times k \; \text{submatrix of }\; A \;\text{is invertible}\}$. I want to know $|\mathcal{M}^{k \times n}| $. I know that since $\mathcal{M}^{k \times n}$ is subset of the full rank matrices then the number I'm looking for is less than $(q^n-1)(q^n-q)\cdots (q^n-q^{k-1})$ (the number of full rank matrices) but if you make the count for $ k=1$ this seems to be false: When $ k=1$ every sub matrix having determinant distinct to zero is equivalent to being full rank, so one has to count only the number of vectors with non zero entries: $ (q^n-1)^n > q^n-1$. I know that I might be wrong but I can't see how Thank you all in advance Edit I forgot to add that $n>k $, this will implie that in $k=1 $ being full rank only means not to be the zero vector i.e we are only looking for the vectors with non zero entries (there are $ (q^n-1)^n > q^n-1$ many of them ) but hte equivalence I state does not holds.",,"['combinatorics', 'matrices', 'finite-fields']"
14,Show that the range of a linear transformation is a subspace,Show that the range of a linear transformation is a subspace,,"Let $T : \mathbb V \to \mathbb W$ be a linear transformation from a vector space $\mathbb V$ into a vector space $\mathbb W$. Prove that the range of $T$ is a subspace of $\mathbb W$. OK here is my attempt... If we let $x$ and $y$ be vectors in $\mathbb V$, then the transformation of these vectors will look like this... $T(x)$ and $T(y)$. If we let $\mathbb V$ be a vector space in $\mathbb R^3$ and $\mathbb W$ be a vector space in $\mathbb R^2$, then $$ T \begin{pmatrix} x_1\\ x_2 \\ x_3  \end{pmatrix} = T\begin{pmatrix} x_1 + 2x_2 \\ 3x_3 + 4 \end{pmatrix}. $$ Now if we tried to row reduce the matrix $\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$ we would get $\begin{pmatrix} 1 & 2 \\ 0 & 1 \end{pmatrix} .$ SO the range of $T$ are the linear combinations of the pivot colums of the matrix above. This is as much as I can do by myself. But now that i think about it, I believe this is wrong because the linear combinations of the pivot columns will give out any vector in $\mathbb R^2$, and not in the subspace of $\mathbb W$. Any help will be appreciated.","Let $T : \mathbb V \to \mathbb W$ be a linear transformation from a vector space $\mathbb V$ into a vector space $\mathbb W$. Prove that the range of $T$ is a subspace of $\mathbb W$. OK here is my attempt... If we let $x$ and $y$ be vectors in $\mathbb V$, then the transformation of these vectors will look like this... $T(x)$ and $T(y)$. If we let $\mathbb V$ be a vector space in $\mathbb R^3$ and $\mathbb W$ be a vector space in $\mathbb R^2$, then $$ T \begin{pmatrix} x_1\\ x_2 \\ x_3  \end{pmatrix} = T\begin{pmatrix} x_1 + 2x_2 \\ 3x_3 + 4 \end{pmatrix}. $$ Now if we tried to row reduce the matrix $\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$ we would get $\begin{pmatrix} 1 & 2 \\ 0 & 1 \end{pmatrix} .$ SO the range of $T$ are the linear combinations of the pivot colums of the matrix above. This is as much as I can do by myself. But now that i think about it, I believe this is wrong because the linear combinations of the pivot columns will give out any vector in $\mathbb R^2$, and not in the subspace of $\mathbb W$. Any help will be appreciated.",,"['linear-algebra', 'matrices', 'vector-spaces', 'vectors', 'linear-transformations']"
15,What can we say about the minimal polynomial over a field $\mathbb{F}$,What can we say about the minimal polynomial over a field,\mathbb{F},"Let $\mathbb{F}$ be a field. Suppose $A \in GL(m,\mathbb{F})$, i.e. $A$ is an $m \times m$ invertible matrix with coefficients in $\mathbb{F}$. Now let $A$ have order $n$. What can we say about the minimal polynomial of $A$?","Let $\mathbb{F}$ be a field. Suppose $A \in GL(m,\mathbb{F})$, i.e. $A$ is an $m \times m$ invertible matrix with coefficients in $\mathbb{F}$. Now let $A$ have order $n$. What can we say about the minimal polynomial of $A$?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'minimal-polynomials']"
16,Recovering a matrix after multiplication by its transpose,Recovering a matrix after multiplication by its transpose,,"Let $A$ be a $p \times k$ matrix (with $p > k$) that has full column rank, i.e., $\mbox{rank}(A) = k$. Suppose we have $$B = A A^t$$ $A$ and $B$ have the same degree of freedom. How can the original values of $A$ be calculated from $B$?","Let $A$ be a $p \times k$ matrix (with $p > k$) that has full column rank, i.e., $\mbox{rank}(A) = k$. Suppose we have $$B = A A^t$$ $A$ and $B$ have the same degree of freedom. How can the original values of $A$ be calculated from $B$?",,"['matrices', 'matrix-equations', 'matrix-decomposition', 'transpose']"
17,Given $RHR^{-1}=D$ where $H$ is Hermitian and $D$ is Diagonal. Show that $R$ is Unitary.,Given  where  is Hermitian and  is Diagonal. Show that  is Unitary.,RHR^{-1}=D H D R,"Given $RHR^{-1}=D$ where $H$ is Hermitian and $D$ is Diagonal. Show that $R$ is Unitary. where Unitary Matrix: $U^\dagger U=UU^\dagger =I$ and Hermitian Matrix: $H=H^\dagger$ My try to this question $RHR^{-1}=D \iff RH=DR$ and then applying dagger on both sides I get $HR^\dagger=R^\dagger D^\dagger$ I know $D$ consists of eigenvalues of $H$ and since $H$ is Hermitian implies eigen values are real. Thus, I get $HR^\dagger=R^\dagger D$. And the above equation implies $R^\dagger R H=H R^\dagger R $","Given $RHR^{-1}=D$ where $H$ is Hermitian and $D$ is Diagonal. Show that $R$ is Unitary. where Unitary Matrix: $U^\dagger U=UU^\dagger =I$ and Hermitian Matrix: $H=H^\dagger$ My try to this question $RHR^{-1}=D \iff RH=DR$ and then applying dagger on both sides I get $HR^\dagger=R^\dagger D^\dagger$ I know $D$ consists of eigenvalues of $H$ and since $H$ is Hermitian implies eigen values are real. Thus, I get $HR^\dagger=R^\dagger D$. And the above equation implies $R^\dagger R H=H R^\dagger R $",,"['matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
18,Are all adjacency matrices of connected graph diagonalizable?,Are all adjacency matrices of connected graph diagonalizable?,,"The related question has been asked in the following link: Are all adjacency matrices (graph theory) diagonalizables? However, the answer does not give a formal proof or explicit answer. My question is: For any connected graph (undirected graph), is its adjacency matrix diagonalizable? Note: it is obviously that it does not have to be full rank, for example a tree graph.","The related question has been asked in the following link: Are all adjacency matrices (graph theory) diagonalizables? However, the answer does not give a formal proof or explicit answer. My question is: For any connected graph (undirected graph), is its adjacency matrix diagonalizable? Note: it is obviously that it does not have to be full rank, for example a tree graph.",,"['matrices', 'graph-theory']"
19,$A$ is complex matrix and $A^3=A$. Show that $rk(A)=tr(A^2)$,is complex matrix and . Show that,A A^3=A rk(A)=tr(A^2),"$A$ is complex matrix and $A^3=A$. Show that $rk(A)=tr(A^2)$ I'm more concerned with how I can derive the prove of this question. before I ask this question, I fail to prove that whit jordan canonical form","$A$ is complex matrix and $A^3=A$. Show that $rk(A)=tr(A^2)$ I'm more concerned with how I can derive the prove of this question. before I ask this question, I fail to prove that whit jordan canonical form",,"['linear-algebra', 'matrices']"
20,Can $A^3=0$ imply $|I+A|=0$?,Can  imply ?,A^3=0 |I+A|=0,"Suppose $A$ is a non-zero matrix such that $A^3=0$ . Prove the following assertions or provide counter examples:- $(1) A^2$ is a zero matrix $(2) A+A^2$ can have zero trace $(3) A-A^2$ can have zero trace $(4) I+A$ is  singular. My Attempt :- I know if $A^3=0$ then $A^2=0$ can be true (though not always). I have no idea whether $tr(A+A^2)=0$ or $tr(A-A^2)=0$ is possible or not if $A^3=0$ . But when I looked closely at $|I+A|$ then I found that $$|I+A|=0$$ For $2\times2$ matrix, we have $$\Rightarrow |A|+tr(A)+1=0 $$ $$\Rightarrow \lambda_1\lambda_2+\lambda_1+\lambda_2+1=0 $$ where $\lambda_1$ and $\lambda_2$ are the two eigenvalues of $A$ $$\Rightarrow \lambda_1(\lambda_2+1)+1.(\lambda_2+1)=0 $$ $$\Rightarrow (\lambda_2+1)(\lambda_1+1)=0 $$ $$\Rightarrow \lambda_2=-1, \lambda_1=-1  \tag1$$ But we have $$A^3=0$$ $$\Rightarrow |A^3|=0$$ $$\Rightarrow |A|^3=0$$ $$\Rightarrow |A|=0$$ So, either $\lambda_1=0$ or $\lambda_2=0$ (or both may be zero) which contradicts with equation $(1)$ . So, $I+A$ is non singular. Am I Correct ?","Suppose is a non-zero matrix such that . Prove the following assertions or provide counter examples:- is a zero matrix can have zero trace can have zero trace is  singular. My Attempt :- I know if then can be true (though not always). I have no idea whether or is possible or not if . But when I looked closely at then I found that For matrix, we have where and are the two eigenvalues of But we have So, either or (or both may be zero) which contradicts with equation . So, is non singular. Am I Correct ?","A A^3=0 (1) A^2 (2) A+A^2 (3) A-A^2 (4) I+A A^3=0 A^2=0 tr(A+A^2)=0 tr(A-A^2)=0 A^3=0 |I+A| |I+A|=0 2\times2 \Rightarrow |A|+tr(A)+1=0  \Rightarrow \lambda_1\lambda_2+\lambda_1+\lambda_2+1=0  \lambda_1 \lambda_2 A \Rightarrow \lambda_1(\lambda_2+1)+1.(\lambda_2+1)=0  \Rightarrow (\lambda_2+1)(\lambda_1+1)=0  \Rightarrow \lambda_2=-1, \lambda_1=-1  \tag1 A^3=0 \Rightarrow |A^3|=0 \Rightarrow |A|^3=0 \Rightarrow |A|=0 \lambda_1=0 \lambda_2=0 (1) I+A",['linear-algebra']
21,Block Matrix Inversion in Wikipedia,Block Matrix Inversion in Wikipedia,,"Wikipedia provides two formulas for block-matrix inversion: $$ {\begin{bmatrix}\mathbf {A} &\mathbf {B} \\\mathbf {C} &\mathbf {D} \end{bmatrix}}^{-1}={\begin{bmatrix}\mathbf {A} ^{-1}+\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}&-\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\\-(\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}&(\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\end{bmatrix}},$$ and $${\begin{bmatrix}\mathbf {A} &\mathbf {B} \\\mathbf {C} &\mathbf {D} \end{bmatrix}}^{-1}={\begin{bmatrix}(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}&-(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1}\\-\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}&\quad \mathbf {D} ^{-1}+\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1}\end{bmatrix}}.$$ Is it true then that all of the following equalities are true? \begin{align}\mathbf {A} ^{-1}+\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}=&\;(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\\ -\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}=&\; -(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1} \\ -(\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}=&\;-\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1} \\ (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}=&\; \quad \mathbf {D} ^{-1}+\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1}\end{align}","Wikipedia provides two formulas for block-matrix inversion: $$ {\begin{bmatrix}\mathbf {A} &\mathbf {B} \\\mathbf {C} &\mathbf {D} \end{bmatrix}}^{-1}={\begin{bmatrix}\mathbf {A} ^{-1}+\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}&-\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\\-(\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}&(\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\end{bmatrix}},$$ and $${\begin{bmatrix}\mathbf {A} &\mathbf {B} \\\mathbf {C} &\mathbf {D} \end{bmatrix}}^{-1}={\begin{bmatrix}(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}&-(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1}\\-\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}&\quad \mathbf {D} ^{-1}+\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1}\end{bmatrix}}.$$ Is it true then that all of the following equalities are true? \begin{align}\mathbf {A} ^{-1}+\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}=&\;(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\\ -\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}=&\; -(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1} \\ -(\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}=&\;-\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1} \\ (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}=&\; \quad \mathbf {D} ^{-1}+\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1}\end{align}",,"['linear-algebra', 'matrices', 'inverse', 'matrix-equations']"
22,Understanding why linear independent columns relate to positive definite matrix and invertibility,Understanding why linear independent columns relate to positive definite matrix and invertibility,,"OK folks. So, lets say we have a matrix A. The inverse of $A^TA$ exists iff the columns of A are linearly independent. Then, we want to prove this by considering that the product $A^TA$ is positive definite, since, apparently, if $A^TA$ is positive definite and invertible, the columns are linearly independent. I hope I explained this well. Otherwise, would appreciate somebody can provide some insight and intuition of what is going on. Thank you all.","OK folks. So, lets say we have a matrix A. The inverse of $A^TA$ exists iff the columns of A are linearly independent. Then, we want to prove this by considering that the product $A^TA$ is positive definite, since, apparently, if $A^TA$ is positive definite and invertible, the columns are linearly independent. I hope I explained this well. Otherwise, would appreciate somebody can provide some insight and intuition of what is going on. Thank you all.",,"['linear-algebra', 'matrices', 'positive-definite']"
23,Find the values of the parameters for which the matrices commute,Find the values of the parameters for which the matrices commute,,"I have two matrices:   $A = \left( \begin{array}{cc} 1 & a \\ b & 6 \end{array} \right) % \ \ \ \ B = \left( \begin{array}{cc} 4 & c \\ d & 2 \end{array} \right)$.   How can I find the values of the parameters $a, b, c, d$ for which the matrices will commute? I tried calculate $AB$ and $BA$ and compare the matrices elements, but that is not enough to get the result. I think this have simple solution, but I have difficulties with it.","I have two matrices:   $A = \left( \begin{array}{cc} 1 & a \\ b & 6 \end{array} \right) % \ \ \ \ B = \left( \begin{array}{cc} 4 & c \\ d & 2 \end{array} \right)$.   How can I find the values of the parameters $a, b, c, d$ for which the matrices will commute? I tried calculate $AB$ and $BA$ and compare the matrices elements, but that is not enough to get the result. I think this have simple solution, but I have difficulties with it.",,"['linear-algebra', 'matrices', 'parametric']"
24,If $\text{tr}(M^3)=\text{tr}(M^2)=\text{tr}(M)=1$ then $M=\mathbf{uu}^\dagger$ for some unit vector $\mathbf{u}$,If  then  for some unit vector,\text{tr}(M^3)=\text{tr}(M^2)=\text{tr}(M)=1 M=\mathbf{uu}^\dagger \mathbf{u},"Consider an $n \times n$ complex Hermitian matrix $M$ which has trace one. I'm trying to show that if $$\text{tr}(M^3)=\text{tr}(M^2)=\text{tr}(M)=1$$ then $M=\mathbf{uu}^\dagger$ for some column vector $\mathbf {u} \in \mathbb C^n$ of unit length. I think it is enough to show that one of the eigenvalues of $M$ is $1$ and the rest are $0$, as $\mathbf {uu}^\dagger$ is a rank one projection matrix. From the trace relations we have $$\sum_i \lambda_i^3=\sum_i \lambda_i^2=\sum_i \lambda_i=1$$ where $\{\lambda_i\}$ are the (real) eigenvalues of $M$, but I'm struggling to show that this implies a single nonzero eigenvalue.","Consider an $n \times n$ complex Hermitian matrix $M$ which has trace one. I'm trying to show that if $$\text{tr}(M^3)=\text{tr}(M^2)=\text{tr}(M)=1$$ then $M=\mathbf{uu}^\dagger$ for some column vector $\mathbf {u} \in \mathbb C^n$ of unit length. I think it is enough to show that one of the eigenvalues of $M$ is $1$ and the rest are $0$, as $\mathbf {uu}^\dagger$ is a rank one projection matrix. From the trace relations we have $$\sum_i \lambda_i^3=\sum_i \lambda_i^2=\sum_i \lambda_i=1$$ where $\{\lambda_i\}$ are the (real) eigenvalues of $M$, but I'm struggling to show that this implies a single nonzero eigenvalue.",,"['linear-algebra', 'matrices', 'vector-spaces']"
25,"Let $A,B$ be $m \times n$ and $n \times m$ matrices, respectively. Prove that if $m > n$, $AB$ is not invertible","Let  be  and  matrices, respectively. Prove that if ,  is not invertible","A,B m \times n n \times m m > n AB","We haven't done anything about rank or dimensions or linear dependence / basis or determinants.  Possible related facts : A matrix is invertible iff it is bijective as a linear transformation. An invertible matrix is row-equivalent to the identity matrix. A matrix has a right inverse iff it has a left inverse. Also, invertability is only defined for square matrices.","We haven't done anything about rank or dimensions or linear dependence / basis or determinants.  Possible related facts : A matrix is invertible iff it is bijective as a linear transformation. An invertible matrix is row-equivalent to the identity matrix. A matrix has a right inverse iff it has a left inverse. Also, invertability is only defined for square matrices.",,"['linear-algebra', 'matrices', 'linear-transformations', 'inverse']"
26,How to check if the columns of a given vector spans Rn,How to check if the columns of a given vector spans Rn,,"Q1: In this question, find out if the given vectors $\{v_1,v_2,v_3\}$ span $R^4$. Vectors $v_1,v_2,v_3$ Q2: Given this matrix Matrix $B$ . Find out if the columns of this matrix span $R^4$. I came across several solutions for these two questions. In all of those augmented matrix was made and checked for pivot columns. My question is why are we creating augmented matrix to check the span ?  We should rather be making an equation like $[A]X = b$, where $A$ is the given matrix in the question, and then check for consistency.","Q1: In this question, find out if the given vectors $\{v_1,v_2,v_3\}$ span $R^4$. Vectors $v_1,v_2,v_3$ Q2: Given this matrix Matrix $B$ . Find out if the columns of this matrix span $R^4$. I came across several solutions for these two questions. In all of those augmented matrix was made and checked for pivot columns. My question is why are we creating augmented matrix to check the span ?  We should rather be making an equation like $[A]X = b$, where $A$ is the given matrix in the question, and then check for consistency.",,"['linear-algebra', 'matrices']"
27,Use Gershgorin's theorem to show that a matrix is nonsingular,Use Gershgorin's theorem to show that a matrix is nonsingular,,"Given two matrices $$A = \begin{pmatrix}     3 & -\frac{1}{2} & 0 \\     -\frac{1}{2} & 1 & -\frac{1}{2} \\     0 & -\frac{1}{2} & 1 \\     \end{pmatrix}, \qquad D = \begin{pmatrix}     \alpha & 0 & 0 \\     0 & 1 & 0 \\     0 & 0 & 1 \\     \end{pmatrix}, \quad\alpha \neq 0$$ I'm first asked to bound the eigenvalues of $\mathrm{A}$. And then using the fact that $\mathrm{D^{-1}AD}$ has the same eigenvalues as A finding a better bound for the eigenvalues of $\mathrm{A}$. I did this also with applying Gershgorin's theorem on $\mathrm{D^{-1}AD}$. Finally I'm asked to conclude that $\mathrm{A}$ is nonsingular. I don't know why I can conclude that $\mathrm{A}$ is nonsingular after applying Gershgorin's theorem on $\mathrm{D^{-1}AD}$. I know that a strictly diagonally dominant matrix is nonsingular and positive definite, but this is not the case for $\mathrm{A}$.","Given two matrices $$A = \begin{pmatrix}     3 & -\frac{1}{2} & 0 \\     -\frac{1}{2} & 1 & -\frac{1}{2} \\     0 & -\frac{1}{2} & 1 \\     \end{pmatrix}, \qquad D = \begin{pmatrix}     \alpha & 0 & 0 \\     0 & 1 & 0 \\     0 & 0 & 1 \\     \end{pmatrix}, \quad\alpha \neq 0$$ I'm first asked to bound the eigenvalues of $\mathrm{A}$. And then using the fact that $\mathrm{D^{-1}AD}$ has the same eigenvalues as A finding a better bound for the eigenvalues of $\mathrm{A}$. I did this also with applying Gershgorin's theorem on $\mathrm{D^{-1}AD}$. Finally I'm asked to conclude that $\mathrm{A}$ is nonsingular. I don't know why I can conclude that $\mathrm{A}$ is nonsingular after applying Gershgorin's theorem on $\mathrm{D^{-1}AD}$. I know that a strictly diagonally dominant matrix is nonsingular and positive definite, but this is not the case for $\mathrm{A}$.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
28,"A simple proof of $\{D \in M_n(\mathbb C),\ D $ diagonalizable with distinct eigenvalues$\}$ $\subset$ Int$\{ D $ diagonalizable$\}$",A simple proof of  diagonalizable with distinct eigenvalues  Int diagonalizable,"\{D \in M_n(\mathbb C),\ D  \} \subset \{ D  \}","What is the interior of $\{D \in M_n(\mathbb C),\ D $  diagonalizable$\}$ ? Actually, it is $I = \{D \in M_n(\mathbb C),\ D $  diagonalizable with distinct eigenvalues$\}$. Int$(D) \subset I $ is quite natural but I know a proof of the reciprocal which is astute : $M \in I \iff \gcd (P_M, P'_M) = 1 \iff \exists \ A,B \neq 0$ s.t. $A P_M = BP_M'\ $ with conditions on degrees and then create a continuous function using $\det$ characterizing this. Would you have a more natural way to proof $I \subset$ Int$(D)$ ?","What is the interior of $\{D \in M_n(\mathbb C),\ D $  diagonalizable$\}$ ? Actually, it is $I = \{D \in M_n(\mathbb C),\ D $  diagonalizable with distinct eigenvalues$\}$. Int$(D) \subset I $ is quite natural but I know a proof of the reciprocal which is astute : $M \in I \iff \gcd (P_M, P'_M) = 1 \iff \exists \ A,B \neq 0$ s.t. $A P_M = BP_M'\ $ with conditions on degrees and then create a continuous function using $\det$ characterizing this. Would you have a more natural way to proof $I \subset$ Int$(D)$ ?",,"['linear-algebra', 'general-topology']"
29,Determinant of triangular matrix,Determinant of triangular matrix,,"I understand that you can find the determinant of a matrix along its diagonal if it is in triangular form. For a matrix such as the following $$\begin{pmatrix} 1 & 5 & 0\\  2 & 4 & -1\\ 0 &-2 & 0  \end{pmatrix}$$ When put into triangular form, I get $$\begin{pmatrix} 1 & 5 & 0\\ 0 & 1 & 1/6\\ 0 & 0 & 1/3 \end{pmatrix}$$ Since I multiplied row two by $-\frac16$ during the row reduction, I would expect the determinant to be $$ 1 \cdot 1 \cdot \frac13 \cdot \left(-\frac16\right),$$ but the answer for the determinant of the original matrix is $-2$ . Where exactly am I going wrong?","I understand that you can find the determinant of a matrix along its diagonal if it is in triangular form. For a matrix such as the following When put into triangular form, I get Since I multiplied row two by during the row reduction, I would expect the determinant to be but the answer for the determinant of the original matrix is . Where exactly am I going wrong?","\begin{pmatrix}
1 & 5 & 0\\ 
2 & 4 & -1\\
0 &-2 & 0 
\end{pmatrix} \begin{pmatrix}
1 & 5 & 0\\
0 & 1 & 1/6\\
0 & 0 & 1/3
\end{pmatrix} -\frac16  1 \cdot 1 \cdot \frac13 \cdot \left(-\frac16\right), -2","['linear-algebra', 'matrices', 'determinant']"
30,Prove that the zero square matrices are the only matrices that are both symmetric and skew-symmetric.,Prove that the zero square matrices are the only matrices that are both symmetric and skew-symmetric.,,"Prove that the zero square matrices are the only matrices that are both symmetric and skew-symmetric. My Proof I will restate the proposition in a way that makes the proof easier to formulate: There exists a unique matrix $A = 0_{n \times n}$, such that it is both symmetric and skew-symmetric. We first prove that the object exists. $A = 0_{n \times n} = [a_{ij}]_{n \times n} = 0 \ \forall i,j$ $\implies -A = -[a_{ij}]_{n \times n}$ By the properties of scalar multiplication of matrices. $\implies (-A)^T = -(A)^T = -[a_{ji}]_{n \times n}$ By the properties and definition of matrix transposition. $= -0 \ \forall j,i$ By the definition of the zero matrix. $= 0 = A$ and $A^T = [a_{ji}]_{n \times n} = 0 \ \forall j,i$ By the definition of the zero matrix. $= A$ $Q.E.D.$ We must now prove that $A$ is unique. $A = A^T = (-A)^T$ Let $B = B^T = (-B)^T$ We want to prove that $A = B$. We now have $A = (-A)^T, A = A^T, B = (-B)^T, B = B^T$ Adding $A = (-A)^T, A = A^T$ and $B = (-B)^T, B = B^T$, we get $2A = A^T + (-A)^T$ and $2B = B^T + (-B)^T$ $\implies A = \dfrac{1}{2}(A^T) + \dfrac{1}{2}(-A)^T$ and $B = \dfrac{1}{2}(B^T) + \dfrac{1}{2}(-A)^T$ $A - B = \dfrac{1}{2}(A^T) + \dfrac{1}{2}(-A^T) - \dfrac{1}{2}(B^T) - \dfrac{1}{2}(-B^T)$ $= \dfrac{1}{2}(A^T) - \dfrac{1}{2}(A^T) - \dfrac{1}{2}(B^T) + \dfrac{1}{2}(B^T)$ Using the hypothesis. $= 0$ $\implies A = B$ $Q.E.D.$ I would greatly appreciate it if people could please take the time to review my proof for correctness and provide feedback.","Prove that the zero square matrices are the only matrices that are both symmetric and skew-symmetric. My Proof I will restate the proposition in a way that makes the proof easier to formulate: There exists a unique matrix $A = 0_{n \times n}$, such that it is both symmetric and skew-symmetric. We first prove that the object exists. $A = 0_{n \times n} = [a_{ij}]_{n \times n} = 0 \ \forall i,j$ $\implies -A = -[a_{ij}]_{n \times n}$ By the properties of scalar multiplication of matrices. $\implies (-A)^T = -(A)^T = -[a_{ji}]_{n \times n}$ By the properties and definition of matrix transposition. $= -0 \ \forall j,i$ By the definition of the zero matrix. $= 0 = A$ and $A^T = [a_{ji}]_{n \times n} = 0 \ \forall j,i$ By the definition of the zero matrix. $= A$ $Q.E.D.$ We must now prove that $A$ is unique. $A = A^T = (-A)^T$ Let $B = B^T = (-B)^T$ We want to prove that $A = B$. We now have $A = (-A)^T, A = A^T, B = (-B)^T, B = B^T$ Adding $A = (-A)^T, A = A^T$ and $B = (-B)^T, B = B^T$, we get $2A = A^T + (-A)^T$ and $2B = B^T + (-B)^T$ $\implies A = \dfrac{1}{2}(A^T) + \dfrac{1}{2}(-A)^T$ and $B = \dfrac{1}{2}(B^T) + \dfrac{1}{2}(-A)^T$ $A - B = \dfrac{1}{2}(A^T) + \dfrac{1}{2}(-A^T) - \dfrac{1}{2}(B^T) - \dfrac{1}{2}(-B^T)$ $= \dfrac{1}{2}(A^T) - \dfrac{1}{2}(A^T) - \dfrac{1}{2}(B^T) + \dfrac{1}{2}(B^T)$ Using the hypothesis. $= 0$ $\implies A = B$ $Q.E.D.$ I would greatly appreciate it if people could please take the time to review my proof for correctness and provide feedback.",,"['linear-algebra', 'matrices', 'proof-verification', 'symmetric-matrices']"
31,"The positive-definiteness of the special matrix created by shifting the vector $[1\, \cdots \, n\, \cdots\,1]$",The positive-definiteness of the special matrix created by shifting the vector,"[1\, \cdots \, n\, \cdots\,1]","I am wondering if there is a good way to prove if matrices with the following structure are positive definite: \begin{equation} \left[ \begin{array}{ccccccc} 10 &9 &8 &\cdots &3 &2 &1\\ 9  &10 &9 &\cdots &4 &3 &2\\ 8 &9 &10 &\cdots &5 &4 &3\\ \vdots &\vdots &\vdots &\ddots &\vdots &\vdots &\vdots \\ 3 &4 &5 &\cdots &10 &9 &8\\ 2 &3 &4 &\cdots &9 &10 &9\\ 1 &2 &3 &\cdots &8 &9 &10 \end{array} \right] \end{equation} Basically, each row is created by shifting the vector $[1\, \cdots \, n\, \cdots\,1]$. Any help is greatly appreciated, thanks so much!","I am wondering if there is a good way to prove if matrices with the following structure are positive definite: \begin{equation} \left[ \begin{array}{ccccccc} 10 &9 &8 &\cdots &3 &2 &1\\ 9  &10 &9 &\cdots &4 &3 &2\\ 8 &9 &10 &\cdots &5 &4 &3\\ \vdots &\vdots &\vdots &\ddots &\vdots &\vdots &\vdots \\ 3 &4 &5 &\cdots &10 &9 &8\\ 2 &3 &4 &\cdots &9 &10 &9\\ 1 &2 &3 &\cdots &8 &9 &10 \end{array} \right] \end{equation} Basically, each row is created by shifting the vector $[1\, \cdots \, n\, \cdots\,1]$. Any help is greatly appreciated, thanks so much!",,"['linear-algebra', 'matrices', 'positive-definite']"
32,"finding a matrix $A_2$ such that $A_2(\mbox{vect}(e_2,e_3)) \subseteq \mbox{vect}(e_2,e_3)$",finding a matrix  such that,"A_2 A_2(\mbox{vect}(e_2,e_3)) \subseteq \mbox{vect}(e_2,e_3)","Let us consider $$A_1 = \left[\begin{array}{ccc} 2&-1&0 \\  0&1&0\\  0&0&0\end{array}\right].$$ I want to find a matrix $A_2$ such that $A_1A_2=A_2A_1$ and $A_2F\subseteq F$ with $F=\mbox{vect}(e_2,e_3)$ Thank you.","Let us consider $$A_1 = \left[\begin{array}{ccc} 2&-1&0 \\  0&1&0\\  0&0&0\end{array}\right].$$ I want to find a matrix $A_2$ such that $A_1A_2=A_2A_1$ and $A_2F\subseteq F$ with $F=\mbox{vect}(e_2,e_3)$ Thank you.",,"['linear-algebra', 'matrices', 'functional-analysis']"
33,system of equations mod 7,system of equations mod 7,,"I am to solve the system of equations below for the field mod 7.  I am also to solve another system of equations that's the same system as below but with the right hand sides replaced with 2, 4, and 1 respectively.  I was told that these two problems could be solved simultaneously using an agumented matrix but wouldn't this completely change the general solutions for $x,y,z$? Thanks in advance! $$x + 4y +5z=1$$   $$6y +4z=2$$   $$4x +3z =3$$","I am to solve the system of equations below for the field mod 7.  I am also to solve another system of equations that's the same system as below but with the right hand sides replaced with 2, 4, and 1 respectively.  I was told that these two problems could be solved simultaneously using an agumented matrix but wouldn't this completely change the general solutions for $x,y,z$? Thanks in advance! $$x + 4y +5z=1$$   $$6y +4z=2$$   $$4x +3z =3$$",,"['linear-algebra', 'matrices', 'modular-arithmetic', 'matrix-equations']"
34,"Show that $M^m + A = M^{m+1}$ with $M,A$ matrices?",Show that  with  matrices?,"M^m + A = M^{m+1} M,A","Let $M,A$ denote matrices with $$ M = \left(\begin{array}{cc} 1 & 1 \\ 0 & 1 \end{array}\right), ~~~~ A = \left(\begin{array}{cc} 0 & 1 \\ 0 & 0 \end{array}\right). $$ Show that $$M^m + A = M^{m+1}.$$ $~$ Context: I encountered this problem when trying to prove that $M^m = E_2 + mA$ (1) by induction (where $E_2$ is the $2\times 2$ identity matrix. Adding $A$ on both sides yields $M^m + A = E_2 + mA +A =E_2 + (m+1)A$. Equation (1) would follow if $M^m + A = M^{m+1}$.","Let $M,A$ denote matrices with $$ M = \left(\begin{array}{cc} 1 & 1 \\ 0 & 1 \end{array}\right), ~~~~ A = \left(\begin{array}{cc} 0 & 1 \\ 0 & 0 \end{array}\right). $$ Show that $$M^m + A = M^{m+1}.$$ $~$ Context: I encountered this problem when trying to prove that $M^m = E_2 + mA$ (1) by induction (where $E_2$ is the $2\times 2$ identity matrix. Adding $A$ on both sides yields $M^m + A = E_2 + mA +A =E_2 + (m+1)A$. Equation (1) would follow if $M^m + A = M^{m+1}$.",,"['linear-algebra', 'matrices', 'induction', 'mathematical-physics', 'matrix-equations']"
35,What does a small circle operation between two matrices mean?,What does a small circle operation between two matrices mean?,,"I'm working on an assignment for my Discreet Mathematics and Logic class, and we're learning about Binary Relations. One question I'm failing to understand involves determining the result of operations with relationship matrices. Here's an example: The following matrices represent the following relations on a set $S = \{1, 2, 3\}$: $$ R_1 = \{(1,1), (1,3), (2,1), (3,1), (3,2), (3,3)\} \\ M_1 = \begin{bmatrix}1 & 0 & 1\\1 & 0 & 0\\1 & 1 & 1\end{bmatrix} $$ $$ R_2 = \{(1,2),(2,1),(3,1),(3,2)\} \\ M_2 = \begin{bmatrix}0 & 1 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0\end{bmatrix} $$ For example, $M_1 \cup M_2$ is $\begin{bmatrix}1 & 1 & 1\\1 & 0 & 0\\ 1 & 1 & 1\end{bmatrix}$ and $M_1 \cap M_2$ is $\begin{bmatrix}0 & 0 & 0\\1 & 0 & 0\\1 & 1 & 0\end{bmatrix}$ This is the bit I'm having difficulty understanding: What is the operation in $M_1\circ M_2$? I haven't seen this notation before?","I'm working on an assignment for my Discreet Mathematics and Logic class, and we're learning about Binary Relations. One question I'm failing to understand involves determining the result of operations with relationship matrices. Here's an example: The following matrices represent the following relations on a set $S = \{1, 2, 3\}$: $$ R_1 = \{(1,1), (1,3), (2,1), (3,1), (3,2), (3,3)\} \\ M_1 = \begin{bmatrix}1 & 0 & 1\\1 & 0 & 0\\1 & 1 & 1\end{bmatrix} $$ $$ R_2 = \{(1,2),(2,1),(3,1),(3,2)\} \\ M_2 = \begin{bmatrix}0 & 1 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0\end{bmatrix} $$ For example, $M_1 \cup M_2$ is $\begin{bmatrix}1 & 1 & 1\\1 & 0 & 0\\ 1 & 1 & 1\end{bmatrix}$ and $M_1 \cap M_2$ is $\begin{bmatrix}0 & 0 & 0\\1 & 0 & 0\\1 & 1 & 0\end{bmatrix}$ This is the bit I'm having difficulty understanding: What is the operation in $M_1\circ M_2$? I haven't seen this notation before?",,"['matrices', 'discrete-mathematics', 'computer-science', 'boolean-algebra', 'binary-operations']"
36,Solving continuous algebraic Riccati equation using generalized eigenproblem algorithm,Solving continuous algebraic Riccati equation using generalized eigenproblem algorithm,,"I want to solve the continuous algebraic riccati equation: $$A^TX + XA - XBR^{-1}B^TX + Q = 0$$ To solve this, I have been using Schur's method to solve algebratic riccati equations. Schur's method to solve algebratic riccati equations The problem is the solution $X$ is not the same solution when I use MATLAB's command X = CARE(A, B, Q, R). MATLAB is using Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations. Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations,'' Proc. IEEE, 72(1984), 1746--1754. So my question is how I can solve the algebraic riccati equation using the generalized eigenproblem algorithm? Assume that we have those matrices. A =      0     1    -2    -3  >> B  B =       0      1  >> Q  Q =       1     0      0     2  >> R  R =       1 Using Schur's method. We will found the solution $X$ by using this: >> H = [A -B*inv(R)*B'; -Q -A']; % Hamiltonian matrix >> [U, S] = schur(H); % Schur decomposition >> [m,n] = size(U); >> U11 = U(1:(m/2), 1:(n/2)); >> U21 = U((m/2+1):m, 1:(n/2)); >> X = U21*inv(U11) % The solution  X =     -6.0000   -2.8074    -8.1926   -3.0000  >> A'*X + X*A - X*B*inv(R)*B'*X + Q % Check if this equation is near 0  ans =     1.0e-13 *      0.0711    0.0888     0.1776    0.1155 Yes it is! Now we use MATLAB's command CARE which use the generalized eigenproblem algorithm. >> X = care(A, B, Q, R)  X =      1.5737    0.2361     0.2361    0.3871  >> A'*X + X*A - X*B*inv(R)*B'*X + Q  % Check if this equation is near 0  ans =     1.0e-14 *     -0.1332   -0.0208    -0.0208    0.0888 Let's try Octave/MATLAB's command fsolve. >> fun = @(P, A, B, Q, R) P*A+A'*P-P*B*inv(R)*B'*P + Q; >> p0 = 1*ones(2); >> X = fsolve(@(P) fun(P, A, B, Q, R), p0) X =     1.57368   0.23607    0.23607   0.38705  >> A'*X + X*A - X*B*inv(R)*B'*X + Q ans =    -1.7419e-08  -1.1153e-08   -1.1153e-08  -6.9926e-09 So both give good solutions. The only difference was that Schur's method give the solution $X$ as matrix only, but with the generalized eigenproblem algorithm, we got a positive semi-definite solution $X$. The fsolve command works very good and give a equal solution as MATLAB's command CARE do. Let's change matrix A. >> A = [0 1; -3 -5]; % The other matrices are the same >> X = care(A, B, Q,R)  X =      0.9118    0.0215     0.0215    0.1994 Let's use fsolve with the new matrix A. X = fsolve(@(P) fun(P, A, B, Q, R), p0) X =     1.53014   0.16228    0.16228   0.22729  >> A'*X + X*A - X*B*inv(R)*B'*X + Q ans =    -9.9554e-08  -5.1336e-08   -5.1336e-08  -2.1732e-08 So here we got a change of the solution X, but the solution X has still positive values.","I want to solve the continuous algebraic riccati equation: $$A^TX + XA - XBR^{-1}B^TX + Q = 0$$ To solve this, I have been using Schur's method to solve algebratic riccati equations. Schur's method to solve algebratic riccati equations The problem is the solution $X$ is not the same solution when I use MATLAB's command X = CARE(A, B, Q, R). MATLAB is using Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations. Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations,'' Proc. IEEE, 72(1984), 1746--1754. So my question is how I can solve the algebraic riccati equation using the generalized eigenproblem algorithm? Assume that we have those matrices. A =      0     1    -2    -3  >> B  B =       0      1  >> Q  Q =       1     0      0     2  >> R  R =       1 Using Schur's method. We will found the solution $X$ by using this: >> H = [A -B*inv(R)*B'; -Q -A']; % Hamiltonian matrix >> [U, S] = schur(H); % Schur decomposition >> [m,n] = size(U); >> U11 = U(1:(m/2), 1:(n/2)); >> U21 = U((m/2+1):m, 1:(n/2)); >> X = U21*inv(U11) % The solution  X =     -6.0000   -2.8074    -8.1926   -3.0000  >> A'*X + X*A - X*B*inv(R)*B'*X + Q % Check if this equation is near 0  ans =     1.0e-13 *      0.0711    0.0888     0.1776    0.1155 Yes it is! Now we use MATLAB's command CARE which use the generalized eigenproblem algorithm. >> X = care(A, B, Q, R)  X =      1.5737    0.2361     0.2361    0.3871  >> A'*X + X*A - X*B*inv(R)*B'*X + Q  % Check if this equation is near 0  ans =     1.0e-14 *     -0.1332   -0.0208    -0.0208    0.0888 Let's try Octave/MATLAB's command fsolve. >> fun = @(P, A, B, Q, R) P*A+A'*P-P*B*inv(R)*B'*P + Q; >> p0 = 1*ones(2); >> X = fsolve(@(P) fun(P, A, B, Q, R), p0) X =     1.57368   0.23607    0.23607   0.38705  >> A'*X + X*A - X*B*inv(R)*B'*X + Q ans =    -1.7419e-08  -1.1153e-08   -1.1153e-08  -6.9926e-09 So both give good solutions. The only difference was that Schur's method give the solution $X$ as matrix only, but with the generalized eigenproblem algorithm, we got a positive semi-definite solution $X$. The fsolve command works very good and give a equal solution as MATLAB's command CARE do. Let's change matrix A. >> A = [0 1; -3 -5]; % The other matrices are the same >> X = care(A, B, Q,R)  X =      0.9118    0.0215     0.0215    0.1994 Let's use fsolve with the new matrix A. X = fsolve(@(P) fun(P, A, B, Q, R), p0) X =     1.53014   0.16228    0.16228   0.22729  >> A'*X + X*A - X*B*inv(R)*B'*X + Q ans =    -9.9554e-08  -5.1336e-08   -5.1336e-08  -2.1732e-08 So here we got a change of the solution X, but the solution X has still positive values.",,"['matrices', 'control-theory', 'optimal-control', 'linear-control']"
37,How to prove positive semidefiniteness of square root and square of two matrices,How to prove positive semidefiniteness of square root and square of two matrices,,[1]  The similar question I post is here. Let matrices $X$ and $Y$ be positive semideï¬nite (PSD). (1) $X \succeq Y$ implies  $X^{1/2} \succeq Y^{1/2}$. (2) $X \succeq Y$ does not imply $X^{2} \succeq Y^{2}$. I try to prove these two inequalities through Schur Complement but could not succeed. And I utilize  reductio ad absurdum which does not work. Could anyone give some hints (maybe these two questions are in the same form so one hint is enough)? Thanks in advance! [1]: How to prove positive semidefiniteness of two matrices through Schur Complement? .....,[1]  The similar question I post is here. Let matrices $X$ and $Y$ be positive semideï¬nite (PSD). (1) $X \succeq Y$ implies  $X^{1/2} \succeq Y^{1/2}$. (2) $X \succeq Y$ does not imply $X^{2} \succeq Y^{2}$. I try to prove these two inequalities through Schur Complement but could not succeed. And I utilize  reductio ad absurdum which does not work. Could anyone give some hints (maybe these two questions are in the same form so one hint is enough)? Thanks in advance! [1]: How to prove positive semidefiniteness of two matrices through Schur Complement? .....,,"['linear-algebra', 'matrices', 'logic', 'proof-writing', 'positive-semidefinite']"
38,Matrix with eigenvalue less than $1$,Matrix with eigenvalue less than,1,"Let $A$ be an $n\times n$ real matrix such that: $a_{ii}=0$ for all $i$, $1>a_{ij}\geq0$ for all $i,j$. $\sum_{j=1}^n a_{ij}\in (0,1]$ for all $i$. There is at least a row $i_0$ such that $\sum_{j=1}^n a_{i_0j}<1$. Can we conclude that the maximum eigenvalue of $A$ is less than 1? By Perron-Frobenius Theorem the maximum eigenvalue is greater or equal than $0$. Note also that, without part 3, one could take a stochastic matrix satisfying 1 and 2, which has always $1$ as an eigenvalue. EDIT : I added the hypothesis $a_{ij}<1$ for all $i,j$. In this case, the example below does not work.","Let $A$ be an $n\times n$ real matrix such that: $a_{ii}=0$ for all $i$, $1>a_{ij}\geq0$ for all $i,j$. $\sum_{j=1}^n a_{ij}\in (0,1]$ for all $i$. There is at least a row $i_0$ such that $\sum_{j=1}^n a_{i_0j}<1$. Can we conclude that the maximum eigenvalue of $A$ is less than 1? By Perron-Frobenius Theorem the maximum eigenvalue is greater or equal than $0$. Note also that, without part 3, one could take a stochastic matrix satisfying 1 and 2, which has always $1$ as an eigenvalue. EDIT : I added the hypothesis $a_{ij}<1$ for all $i,j$. In this case, the example below does not work.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
39,Lower bound nuclear norm of $A$ by $\mathrm{tr}(|A|)$,Lower bound nuclear norm of  by,A \mathrm{tr}(|A|),Let $A = (a_{ij}) \in \mathbb R^{n \times n}$ be a square matrix. Let $\|A\|_\ast$ be the nuclear norm of $A$. Is the following true? $$\sum_{i=1}^n |a_{ii}| \le \|A\|_\ast$$,Let $A = (a_{ij}) \in \mathbb R^{n \times n}$ be a square matrix. Let $\|A\|_\ast$ be the nuclear norm of $A$. Is the following true? $$\sum_{i=1}^n |a_{ii}| \le \|A\|_\ast$$,,"['linear-algebra', 'matrices', 'inequality', 'nuclear-norm']"
40,Proving symmetric matrices are diagonalizable using fact eigenvectors must be orthogonal,Proving symmetric matrices are diagonalizable using fact eigenvectors must be orthogonal,,"I'm treating the fact that we know eigenvectors of symmetric matrices corresponding to distinct eigenvalues must be orthogonal as a given and trying to show (real) symmetric matrices are diagonalizable. Knowing the aforementioned fact, we can conclude that there exists an orthogonal basis of eigenvectors for any symmetric matrix. We know a matrix A is diagonalizable iff there exists a basis of eigenvectors. So therefore, symmetric matrices are diagonalizable. âˆŽ I'm not sure if I'm making a bit of a logical leap when I can conclude that there exists an orthogonal basis of eigenvectors, would appreciate any criticism/advice.","I'm treating the fact that we know eigenvectors of symmetric matrices corresponding to distinct eigenvalues must be orthogonal as a given and trying to show (real) symmetric matrices are diagonalizable. Knowing the aforementioned fact, we can conclude that there exists an orthogonal basis of eigenvectors for any symmetric matrix. We know a matrix A is diagonalizable iff there exists a basis of eigenvectors. So therefore, symmetric matrices are diagonalizable. âˆŽ I'm not sure if I'm making a bit of a logical leap when I can conclude that there exists an orthogonal basis of eigenvectors, would appreciate any criticism/advice.",,"['linear-algebra', 'matrices', 'diagonalization', 'symmetric-matrices']"
41,"Why is the dot product of two vectors $\mathbf{x},\mathbf{y}$ the same as $x^T y$?",Why is the dot product of two vectors  the same as ?,"\mathbf{x},\mathbf{y} x^T y","I've always thought that a $1\times 1$ matrix is not the same as a scalar. However, in $\textbf{many}$ times throughout my first year in undergrad, I see people interchange $x\cdot y$ with $x^T y$ (a scalar in the former, and a $1\times 1$ matrix in the latter). Is this just sloppy/lazy notation? Surely there will be instances where using one or the other ""breaks"" the working of a question? e.g. Let $\mathbf{x,y}$ be vectors in $\mathbb{R}^3$, and let $X$ be a $2\times 2$ matrix. Then $(x\cdot y)X$ is defined, yet $x^T y X$ is not defined. Is there a situation where treating them as equivalent is beneficial?","I've always thought that a $1\times 1$ matrix is not the same as a scalar. However, in $\textbf{many}$ times throughout my first year in undergrad, I see people interchange $x\cdot y$ with $x^T y$ (a scalar in the former, and a $1\times 1$ matrix in the latter). Is this just sloppy/lazy notation? Surely there will be instances where using one or the other ""breaks"" the working of a question? e.g. Let $\mathbf{x,y}$ be vectors in $\mathbb{R}^3$, and let $X$ be a $2\times 2$ matrix. Then $(x\cdot y)X$ is defined, yet $x^T y X$ is not defined. Is there a situation where treating them as equivalent is beneficial?",,"['matrices', 'inner-products']"
42,Angle from rotation matrix,Angle from rotation matrix,,I have an exam later today and I have come across something which is not covered in my lecture notes (that I know of) that I need to know the answer for! Could someone please explain to me how you get the answer for the following question? I have been looking all over to try and solve this but not having much luck with my google searches :-) As you can tell I am a bit of a novice when it comes to Maths! Which angle has the following rotation matrix: ( 0 1 )      (-1 0 )   [A] 0  [B] 1/2 Ï€  [C] Ï€  [D] 3/2 Ï€ Thank you in advance for any help that you may be able to give me,I have an exam later today and I have come across something which is not covered in my lecture notes (that I know of) that I need to know the answer for! Could someone please explain to me how you get the answer for the following question? I have been looking all over to try and solve this but not having much luck with my google searches :-) As you can tell I am a bit of a novice when it comes to Maths! Which angle has the following rotation matrix: ( 0 1 )      (-1 0 )   [A] 0  [B] 1/2 Ï€  [C] Ï€  [D] 3/2 Ï€ Thank you in advance for any help that you may be able to give me,,"['matrices', 'angle']"
43,Showing Two Operators Share an Eigenvector,Showing Two Operators Share an Eigenvector,,"Question 1 - Let $A$ and $B$ be complex $n \times n$ matrices such that $AB = BA^2$, and assume $A$ has no eigenvalues of absolute value 1. Prove that $A$ and $B$ have a common (nonzero) eigenvector. Question 2 - Let $A$ and $B$ denote real $n \times n$ symmetric matrices such that $AB = BA$. Prove that $A$ and $B$ have a common eigenvector in $\mathbb{R}^n$. For the second question. Suppose $v$ is an eigenvector of $A$ with eigenvalue $\lambda$ so $Av=\lambda v$. Then $$A(Bv) = B(Av) = B(\lambda v) = \lambda Bv$$ which shows that the eigenspace corresponding to $\lambda$ is invariant under B. Now what....","Question 1 - Let $A$ and $B$ be complex $n \times n$ matrices such that $AB = BA^2$, and assume $A$ has no eigenvalues of absolute value 1. Prove that $A$ and $B$ have a common (nonzero) eigenvector. Question 2 - Let $A$ and $B$ denote real $n \times n$ symmetric matrices such that $AB = BA$. Prove that $A$ and $B$ have a common eigenvector in $\mathbb{R}^n$. For the second question. Suppose $v$ is an eigenvector of $A$ with eigenvalue $\lambda$ so $Av=\lambda v$. Then $$A(Bv) = B(Av) = B(\lambda v) = \lambda Bv$$ which shows that the eigenspace corresponding to $\lambda$ is invariant under B. Now what....",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
44,Differentiation of $a^\top x x^\top a$,Differentiation of,a^\top x x^\top a,"I know that for matrix $A$ and vector $x$, the derivative of the quadratic form with respect to $x$ is $$\frac{\partial x^TAx}{\partial x} = (A+A^T)x$$ But how do we differentiate $a^T x x^T a$ with respect to $x$?","I know that for matrix $A$ and vector $x$, the derivative of the quadratic form with respect to $x$ is $$\frac{\partial x^TAx}{\partial x} = (A+A^T)x$$ But how do we differentiate $a^T x x^T a$ with respect to $x$?",,"['matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus', 'scalar-fields']"
45,Numerical range of $2\times 2$ matrices,Numerical range of  matrices,2\times 2,"Let $(A_1,\cdots, A_d)\in \mathcal{L}(\mathbb{C}^2)^d$. Consider $$W(A_1,\cdots, A_d)=\{(\langle A_1 x,x\rangle,\cdots,\langle A_d x,x\rangle):x \in E,\;\;\|x\|=1\}.$$ If $A_k$ are commuting, why $W(A_1,\cdots, A_d)$ is convex??","Let $(A_1,\cdots, A_d)\in \mathcal{L}(\mathbb{C}^2)^d$. Consider $$W(A_1,\cdots, A_d)=\{(\langle A_1 x,x\rangle,\cdots,\langle A_d x,x\rangle):x \in E,\;\;\|x\|=1\}.$$ If $A_k$ are commuting, why $W(A_1,\cdots, A_d)$ is convex??",,"['linear-algebra', 'matrices']"
46,"If $A + I$, $A^2 + I$ and $A^3 + I$ are all unitary, show that $A$ is the zero matrix","If ,  and  are all unitary, show that  is the zero matrix",A + I A^2 + I A^3 + I A,"Let $A$ be an $n \times n$ complex matrix such that the three matrices $A + I$, $A^2 + I$, $A^3 + I$ are all unitary. Prove that $A$ is the zero matrix. I know eigenvalues of a unitary matrix has modulus 1, so if $\lambda$ is an eigenvalue of A, then the eigenvalues of $A + I$, $A^2 + I$, $A^3 + I$ are $ \lambda + 1, \lambda^2 + 1, \lambda^3 + 1$ all on the circle $|z+1|<1$. Where to go from here though?","Let $A$ be an $n \times n$ complex matrix such that the three matrices $A + I$, $A^2 + I$, $A^3 + I$ are all unitary. Prove that $A$ is the zero matrix. I know eigenvalues of a unitary matrix has modulus 1, so if $\lambda$ is an eigenvalue of A, then the eigenvalues of $A + I$, $A^2 + I$, $A^3 + I$ are $ \lambda + 1, \lambda^2 + 1, \lambda^3 + 1$ all on the circle $|z+1|<1$. Where to go from here though?",,"['linear-algebra', 'matrices']"
47,Simplifying Inverse Matrices Expression,Simplifying Inverse Matrices Expression,,"On pg. 95 of Howard Anton's Elementary Linear Algebra, it asks to simplify $$(AB)^{-1}(AC)^{-1}(D^{-1}C^{-1})^{-1}D^{-1}$$ The answer is supposed to be $B^{-1}$ but I end up with $B^{-1}A^{-1}A^{-1}$ First, I undid all the brackets which led to $B^{-1}A^{-1}C^{-1}A^{-1}CDD^{-1}$ I ""cancelled out"" $CC^{-1}$ and $DD^{-1}$ using the associative property of matrix multiplication but not sure how to simplify it any further. I guess $A^{-1}A^{-1} =(AA)^{-1} = (A^2)^{-1}$ (not 100% here). However, I still have $B^{-1}(A^2)^{-1}.$","On pg. 95 of Howard Anton's Elementary Linear Algebra, it asks to simplify $$(AB)^{-1}(AC)^{-1}(D^{-1}C^{-1})^{-1}D^{-1}$$ The answer is supposed to be $B^{-1}$ but I end up with $B^{-1}A^{-1}A^{-1}$ First, I undid all the brackets which led to $B^{-1}A^{-1}C^{-1}A^{-1}CDD^{-1}$ I ""cancelled out"" $CC^{-1}$ and $DD^{-1}$ using the associative property of matrix multiplication but not sure how to simplify it any further. I guess $A^{-1}A^{-1} =(AA)^{-1} = (A^2)^{-1}$ (not 100% here). However, I still have $B^{-1}(A^2)^{-1}.$",,"['linear-algebra', 'matrices', 'inverse', 'products', 'associativity']"
48,Characteristic polynomial and minimal polynomial of a matrix,Characteristic polynomial and minimal polynomial of a matrix,,"let $A$ be a $n \times n$ matrix : $$A=\begin{bmatrix}1 & 2 &... & n\\n+1 & n+2 & ... & 2n\\&...\\&&...\\n^2-n+1 & n^2-n+2 & .... & n^2\end{bmatrix}$$ How can I find the characteristic polynomial  and minimal polynomial of A? I know that the row space and column space of A is $$\bigl\langle(1,1,...,1), (1,2,...,n)\bigr\rangle$$ but could not proceed.","let $A$ be a $n \times n$ matrix : $$A=\begin{bmatrix}1 & 2 &... & n\\n+1 & n+2 & ... & 2n\\&...\\&&...\\n^2-n+1 & n^2-n+2 & .... & n^2\end{bmatrix}$$ How can I find the characteristic polynomial  and minimal polynomial of A? I know that the row space and column space of A is $$\bigl\langle(1,1,...,1), (1,2,...,n)\bigr\rangle$$ but could not proceed.",,"['linear-algebra', 'matrices', 'determinant']"
49,"If $\sum_j \overline{E_j AE_j^\dagger}=\sum_j E_j AE_j^\dagger$ for all $A$, does $ \overline{E_j AE_j^\dagger}= E_j AE_j^\dagger$","If  for all , does",\sum_j \overline{E_j AE_j^\dagger}=\sum_j E_j AE_j^\dagger A  \overline{E_j AE_j^\dagger}= E_j AE_j^\dagger,"I'm currently optimizing a statistical algorithm and want to make sure that an assumption I'm making is correct. Let $E_j$ be a finite sequence of $n \times n$ complex matrices that satisfies: $\sum_j E_j E_j^\dagger=I.$ If $$\sum_j \overline{E_j AE_j^\dagger}=\sum_j E_j AE_j^\dagger$$ for all symmetric real matrices $A$, can we conclude that $ \overline{E_j AE_j^\dagger}= E_j AE_j^\dagger$ for all $j$? Here the bar denotes the complex conjugate and the dagger is the adjoint. I believe this is true as we can rewrite the equality as $$\sum_j \overline{E_j} AE_j^T-E_j AE_j^\dagger=0$$ for all $A$ where $E_j^T$ denotes the transpose of $E_j$ and I'm not sure how else we could construct a sequence of matrices that would hold for any given $A$.","I'm currently optimizing a statistical algorithm and want to make sure that an assumption I'm making is correct. Let $E_j$ be a finite sequence of $n \times n$ complex matrices that satisfies: $\sum_j E_j E_j^\dagger=I.$ If $$\sum_j \overline{E_j AE_j^\dagger}=\sum_j E_j AE_j^\dagger$$ for all symmetric real matrices $A$, can we conclude that $ \overline{E_j AE_j^\dagger}= E_j AE_j^\dagger$ for all $j$? Here the bar denotes the complex conjugate and the dagger is the adjoint. I believe this is true as we can rewrite the equality as $$\sum_j \overline{E_j} AE_j^T-E_j AE_j^\dagger=0$$ for all $A$ where $E_j^T$ denotes the transpose of $E_j$ and I'm not sure how else we could construct a sequence of matrices that would hold for any given $A$.",,"['linear-algebra', 'matrices', 'matrix-equations']"
50,Dimension of $U(n)$,Dimension of,U(n),"I've found the dimension of $O(n)$ by the following argument and tried to apply it to $U(n)$, but it didn't go well. my attempt on $O(n)$: define the map $f:M(n,\mathbb{R})\to Sym(n,\mathbb{R})$ by $A\mapsto A^TA$. then since $O(n)$ is the kernel of $f$, by the rank-nullity theorem, $\dim O(n) = \dim M(n,\mathbb{R})-\dim Sym(n,\mathbb{R})=n^2-\frac{n(n+1)}{2}=\frac{n(n-1)}{2}$. for $U(n)$, define the map $g:M(n,\mathbb{C})\to Sym(n,\mathbb{C})$ by $A\mapsto A^*A$. then since $U(n)$ is the kernel of $g$, by the rank-nullity theorem, $\dim U(n) = \dim M(n,\mathbb{C})-\dim Sym(n,\mathbb{C})=2n^2-n(n+1)=n(n-1)$. but in fact $\dim U(n) = n^2$ according to the wiki. where did I miss $n$?","I've found the dimension of $O(n)$ by the following argument and tried to apply it to $U(n)$, but it didn't go well. my attempt on $O(n)$: define the map $f:M(n,\mathbb{R})\to Sym(n,\mathbb{R})$ by $A\mapsto A^TA$. then since $O(n)$ is the kernel of $f$, by the rank-nullity theorem, $\dim O(n) = \dim M(n,\mathbb{R})-\dim Sym(n,\mathbb{R})=n^2-\frac{n(n+1)}{2}=\frac{n(n-1)}{2}$. for $U(n)$, define the map $g:M(n,\mathbb{C})\to Sym(n,\mathbb{C})$ by $A\mapsto A^*A$. then since $U(n)$ is the kernel of $g$, by the rank-nullity theorem, $\dim U(n) = \dim M(n,\mathbb{C})-\dim Sym(n,\mathbb{C})=2n^2-n(n+1)=n(n-1)$. but in fact $\dim U(n) = n^2$ according to the wiki. where did I miss $n$?",,"['matrices', 'lie-groups']"
51,problem : conditions of similar matrices,problem : conditions of similar matrices,,"Could you please check my solution? Q. State all conditions that make A and B similar. $$A=         \begin{pmatrix}         0 & 4 \\         a & 4 \\       \end{pmatrix} $$ $$B=         \begin{pmatrix}         2 & b \\         0 & c \\       \end{pmatrix} $$ My solution :  Since similar matrices have the same eigenvalues, trA=4=trB. so C=2. DetB=4=DetA so a=-1 Since both the sum and the product of the two eigenvalues are 4, the eigenvalues of A and B are 2,2 (multiplicity 2) $$B-2I=         \begin{pmatrix}         0 & b \\         0 & 0 \\       \end{pmatrix} $$ $$(B-2I)v=         \begin{pmatrix}         0 & b \\         0 & 0 \\       \end{pmatrix}\begin{pmatrix}         x  \\         y \\       \end{pmatrix}=\begin{pmatrix}         by  \\         0\\       \end{pmatrix}=\begin{pmatrix}         0  \\         0 \\       \end{pmatrix} $$, where $$v=\begin{pmatrix}         x  \\         y \\       \end{pmatrix}$$ is an eigenvector of B The value of $by$ must always be 0. So b is 0. So the answer is a=-1 and b=0 and c=2","Could you please check my solution? Q. State all conditions that make A and B similar. $$A=         \begin{pmatrix}         0 & 4 \\         a & 4 \\       \end{pmatrix} $$ $$B=         \begin{pmatrix}         2 & b \\         0 & c \\       \end{pmatrix} $$ My solution :  Since similar matrices have the same eigenvalues, trA=4=trB. so C=2. DetB=4=DetA so a=-1 Since both the sum and the product of the two eigenvalues are 4, the eigenvalues of A and B are 2,2 (multiplicity 2) $$B-2I=         \begin{pmatrix}         0 & b \\         0 & 0 \\       \end{pmatrix} $$ $$(B-2I)v=         \begin{pmatrix}         0 & b \\         0 & 0 \\       \end{pmatrix}\begin{pmatrix}         x  \\         y \\       \end{pmatrix}=\begin{pmatrix}         by  \\         0\\       \end{pmatrix}=\begin{pmatrix}         0  \\         0 \\       \end{pmatrix} $$, where $$v=\begin{pmatrix}         x  \\         y \\       \end{pmatrix}$$ is an eigenvector of B The value of $by$ must always be 0. So b is 0. So the answer is a=-1 and b=0 and c=2",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
52,"Minimize $\|AXBd -c \|^2$, enforcing $X$ to be a diagonal block matrix","Minimize , enforcing  to be a diagonal block matrix",\|AXBd -c \|^2 X,"Currently, I am minimizing the quadratic objective $\|\mathbf{A}\mathbf{X}\mathbf{B}\mathbf{d} -\mathbf{c} \|^2$ using CVX , as follows echo on cvx_begin     variable xx(num_triangles*3,num_triangles*3)     minimize( norm( A * xx * B * d - c ) ) cvx_end echo off However, $\mathbf{X}$ is a very large matrix (about $50,000 \times 50,000$, which is too big). Good news is that $\mathbf{X}$ is a block diagonal matrix (of $3 \times 3$ transformation matrices) and, therefore, it is highly sparse. To give an example, for $\mathbf{X} \in \mathbb{R}^{9 \times 9}$ I would be looking for such matrix. |  X1  X4  X7   0   0   0   0   0   0 | |  X2  X5  X8   0   0   0   0   0   0 | |  X3  X6  X9   0   0   0   0   0   0 | |   0   0   0 X10 x13 x16   0   0   0 | |   0   0   0 X11 x14 x17   0   0   0 | |   0   0   0 X12 x15 x18   0   0   0 | |   0   0   0   0   0   0 X19 x22 x25 | |   0   0   0   0   0   0 X20 x23 x26 | |   0   0   0   0   0   0 X21 x24 x27 | so in fact I am only solving for $27$ variables, not $9 \times 9 = 81$. And this scales pretty badly. But I don't know how I can enforce such structure when formulating the minimization problem. If I declare a normal square matrix of the size I need, the memory just explodes. Any ideas on how to reformulate the problem in a way that I actually solve for the number of unknowns? I currently look for solutions in MATLAB.","Currently, I am minimizing the quadratic objective $\|\mathbf{A}\mathbf{X}\mathbf{B}\mathbf{d} -\mathbf{c} \|^2$ using CVX , as follows echo on cvx_begin     variable xx(num_triangles*3,num_triangles*3)     minimize( norm( A * xx * B * d - c ) ) cvx_end echo off However, $\mathbf{X}$ is a very large matrix (about $50,000 \times 50,000$, which is too big). Good news is that $\mathbf{X}$ is a block diagonal matrix (of $3 \times 3$ transformation matrices) and, therefore, it is highly sparse. To give an example, for $\mathbf{X} \in \mathbb{R}^{9 \times 9}$ I would be looking for such matrix. |  X1  X4  X7   0   0   0   0   0   0 | |  X2  X5  X8   0   0   0   0   0   0 | |  X3  X6  X9   0   0   0   0   0   0 | |   0   0   0 X10 x13 x16   0   0   0 | |   0   0   0 X11 x14 x17   0   0   0 | |   0   0   0 X12 x15 x18   0   0   0 | |   0   0   0   0   0   0 X19 x22 x25 | |   0   0   0   0   0   0 X20 x23 x26 | |   0   0   0   0   0   0 X21 x24 x27 | so in fact I am only solving for $27$ variables, not $9 \times 9 = 81$. And this scales pretty badly. But I don't know how I can enforce such structure when formulating the minimization problem. If I declare a normal square matrix of the size I need, the memory just explodes. Any ideas on how to reformulate the problem in a way that I actually solve for the number of unknowns? I currently look for solutions in MATLAB.",,"['matrices', 'convex-optimization', 'matlab', 'least-squares', 'cvx']"
53,Is this inequality on Schatten p-norm and diagonal elements true?,Is this inequality on Schatten p-norm and diagonal elements true?,,"Let $A=[a_{ij}]\in\mathbb R^{m\times n}$ be a matrix with $m\ge n$, and $\Vert A\Vert_p$ be the Schatten p-norm of $A$. It is known that $\Vert A\Vert_1\ge \sum_i \vert a_{ii}\vert$ and $\Vert A\Vert_2^2=\Vert A\Vert_F^2\ge \sum_i \vert a_{ii}\vert^2$. Can we show that $\Vert A\Vert_p^p\ge \sum_i \vert a_{ii}\vert^p$ for general $p\ge 1$? Thanks!","Let $A=[a_{ij}]\in\mathbb R^{m\times n}$ be a matrix with $m\ge n$, and $\Vert A\Vert_p$ be the Schatten p-norm of $A$. It is known that $\Vert A\Vert_1\ge \sum_i \vert a_{ii}\vert$ and $\Vert A\Vert_2^2=\Vert A\Vert_F^2\ge \sum_i \vert a_{ii}\vert^2$. Can we show that $\Vert A\Vert_p^p\ge \sum_i \vert a_{ii}\vert^p$ for general $p\ge 1$? Thanks!",,"['linear-algebra', 'matrices', 'normed-spaces']"
54,Are the principal components in the SVD of a symmetric matrix also symmetric?,Are the principal components in the SVD of a symmetric matrix also symmetric?,,"If we have a symmetric matrix $A$ and compute its singular value decomposition $A=U\Sigma V^T$, is it the case that $U$ and $V$ are also symmetric? I know that $U$ and $V$ must be equal since they are the singular vectors of $A$.","If we have a symmetric matrix $A$ and compute its singular value decomposition $A=U\Sigma V^T$, is it the case that $U$ and $V$ are also symmetric? I know that $U$ and $V$ must be equal since they are the singular vectors of $A$.",,"['linear-algebra', 'matrices', 'svd', 'symmetric-matrices', 'singular-values']"
55,What is the number of non-conjugate $6 \times 6$ complex matrices having the characteristic polynomial $(x-5)^6=0$?,What is the number of non-conjugate  complex matrices having the characteristic polynomial ?,6 \times 6 (x-5)^6=0,Let $D$ be the $6 \times 6$ diagonal matrix with diagonal entries $5$. Then all the $6 \times 6$ complex matrices which are diagonalizable to $D$ are conjugate to $D$ and hence to each other. So I should find those matrices which aren't diagonalizable to $D$ but have same characteristic equation. I think of those matrices whose all diagonal elements are $5$ but Geometric multiplicity $\neq$ Algebraic multiplicity for $5$. But still not getting any concrete idea. What is the general way to approach?,Let $D$ be the $6 \times 6$ diagonal matrix with diagonal entries $5$. Then all the $6 \times 6$ complex matrices which are diagonalizable to $D$ are conjugate to $D$ and hence to each other. So I should find those matrices which aren't diagonalizable to $D$ but have same characteristic equation. I think of those matrices whose all diagonal elements are $5$ but Geometric multiplicity $\neq$ Algebraic multiplicity for $5$. But still not getting any concrete idea. What is the general way to approach?,,"['linear-algebra', 'matrices', 'diagonalization']"
56,Inverse of special type of symmetric block matrix,Inverse of special type of symmetric block matrix,,"$W = \begin{pmatrix}A & B &B &\cdots&B\\ B& A & B &\cdots &B\\ \vdots & \vdots & \vdots & \ddots &\vdots &\\ B& B & B &\cdots &A \end{pmatrix} $ where $A$ and $B$ are symmetric matrices of appropriate order. In fact $B = aJ$, where $J$ is the matrix of all ones. Is there any compact form of $W^{-1}$ ?. Thanks","$W = \begin{pmatrix}A & B &B &\cdots&B\\ B& A & B &\cdots &B\\ \vdots & \vdots & \vdots & \ddots &\vdots &\\ B& B & B &\cdots &A \end{pmatrix} $ where $A$ and $B$ are symmetric matrices of appropriate order. In fact $B = aJ$, where $J$ is the matrix of all ones. Is there any compact form of $W^{-1}$ ?. Thanks",,['matrices']
57,Can I modify the singular values of a matrix in order to get a negative eigenvalue?,Can I modify the singular values of a matrix in order to get a negative eigenvalue?,,"Let $A \in \mathbb{R}^{n \times n}$ be a real nonsymmetric matrix with eigenvalues $\left\{\lambda_i : i=1..n\right\}$ with positive real part $\Re(\lambda_i) > 0$ $\forall i=1..n$ Let $A=U\Sigma V^T$ be the singular value decomposition of $A$ with singular values $\left\{\sigma_i : i=1..n\right\}$, $\sigma_i>0$. Let $\Sigma'$ be a diagonal matrix with a different set of singular values $\left\{\sigma_i' : i=1..n\right\}$, $\sigma_i'>0$ and let $\left\{\lambda_i' : i=1..n\right\}$ be the eigenvalues of $A'= U\Sigma' V^T$. Is there a set $\left\{\sigma_i' : i=1..n\right\}$ such that there is at least one $\lambda_i'$ with negative real part $\Re(\lambda_i) < 0$?","Let $A \in \mathbb{R}^{n \times n}$ be a real nonsymmetric matrix with eigenvalues $\left\{\lambda_i : i=1..n\right\}$ with positive real part $\Re(\lambda_i) > 0$ $\forall i=1..n$ Let $A=U\Sigma V^T$ be the singular value decomposition of $A$ with singular values $\left\{\sigma_i : i=1..n\right\}$, $\sigma_i>0$. Let $\Sigma'$ be a diagonal matrix with a different set of singular values $\left\{\sigma_i' : i=1..n\right\}$, $\sigma_i'>0$ and let $\left\{\lambda_i' : i=1..n\right\}$ be the eigenvalues of $A'= U\Sigma' V^T$. Is there a set $\left\{\sigma_i' : i=1..n\right\}$ such that there is at least one $\lambda_i'$ with negative real part $\Re(\lambda_i) < 0$?",,"['linear-algebra', 'matrices', 'linear-transformations', 'matrix-decomposition', 'svd']"
58,What do you call a matrix that has at most one value in each row and each column?,What do you call a matrix that has at most one value in each row and each column?,,Is there a linear algebra terminology for matrices that are limited to at most one value in each row and column? For example: This counts: $\pmatrix{0& 0 & 1 & 0\\ 0& 1& 0& 0 \\ 0& 0& 0& 1 \\ 1&0&0&0}$ While this is not: $\pmatrix{1& 0 & 0 & 0\\ 0& 1& 1& 1 \\ 1& 0& 0& 1 \\ 1&0&0&0}$ Thanks very much!,Is there a linear algebra terminology for matrices that are limited to at most one value in each row and column? For example: This counts: $\pmatrix{0& 0 & 1 & 0\\ 0& 1& 0& 0 \\ 0& 0& 0& 1 \\ 1&0&0&0}$ While this is not: $\pmatrix{1& 0 & 0 & 0\\ 0& 1& 1& 1 \\ 1& 0& 0& 1 \\ 1&0&0&0}$ Thanks very much!,,"['linear-algebra', 'matrices', 'terminology']"
59,Finding Smith normal form of matrix over $ \mathbb{R} [ X ]$,Finding Smith normal form of matrix over, \mathbb{R} [ X ],"I am trying to find the Smith normal form of matrix over $ \mathbb{R} [ X ]$ of the 4x4 matrix $$M =\begin{pmatrix}  2X-1 & X & X-1 & 1\\  X & 0 & 1 & 0 \\  0 & 1 & X & X\\ 1 & X^2 & 0 & 2X-2 \end{pmatrix}$$ I know of two methods for computing the smith normal form of a matrix: 1) Using the algorithm 2) Computing the Fitting Ideals The first process looks horrible and cumbersome for such a matrix- maybe it all cancels nicely but I have tried and it doesn't appear clean. The second looks like it might work if the matix behaves 'nicely'- for example it isn't hard to see that $\mathrm{Fit}_1 (M)= \mathrm{Fit}_2 (M)= (1)  $ so we have the first two diagonal elements as 1. However computing the 3x3 sub determinants seems really lengthy and can't see any nice terms like 1 coming out. I could compute the whole 4x4, but again, this seems lengthy and then I'd hope it to not have many factors so I can 'deduce' the 3rd diagonal element, but this seems too hopeful. Is there some property of the ring/ overall technique I'm missing? Any help would be appreciated","I am trying to find the Smith normal form of matrix over $ \mathbb{R} [ X ]$ of the 4x4 matrix $$M =\begin{pmatrix}  2X-1 & X & X-1 & 1\\  X & 0 & 1 & 0 \\  0 & 1 & X & X\\ 1 & X^2 & 0 & 2X-2 \end{pmatrix}$$ I know of two methods for computing the smith normal form of a matrix: 1) Using the algorithm 2) Computing the Fitting Ideals The first process looks horrible and cumbersome for such a matrix- maybe it all cancels nicely but I have tried and it doesn't appear clean. The second looks like it might work if the matix behaves 'nicely'- for example it isn't hard to see that $\mathrm{Fit}_1 (M)= \mathrm{Fit}_2 (M)= (1)  $ so we have the first two diagonal elements as 1. However computing the 3x3 sub determinants seems really lengthy and can't see any nice terms like 1 coming out. I could compute the whole 4x4, but again, this seems lengthy and then I'd hope it to not have many factors so I can 'deduce' the 3rd diagonal element, but this seems too hopeful. Is there some property of the ring/ overall technique I'm missing? Any help would be appreciated",,"['abstract-algebra', 'matrices', 'ring-theory', 'smith-normal-form']"
60,Let $ 0 \preceq A \preceq I$ can we relate $Tr( (I-A) B+A C)$ to $Tr(I-A)Tr(B)+Tr(A) Tr(C)$,Let  can we relate  to, 0 \preceq A \preceq I Tr( (I-A) B+A C) Tr(I-A)Tr(B)+Tr(A) Tr(C),"Let $ 0 \preceq A \preceq I$  and let $B$ and $C$ be two symmetric positive definite matrices. Can we related \begin{align} {\rm Tr}( (I-A) B+A C) \end{align} to \begin{align} Tr(I-A)Tr(B)+Tr(A) Tr(C) \end{align} via some inequality? Note, that in the case $A=aI$  for any $a\in [0,1]$ there is an equality. Edit: Note, from a very nice answer below we have: \begin{align} \lambda_{\min}(I-A){\rm Tr}(B)+\lambda_{\min}(A){\rm Tr}(C) \le {\rm Tr}( (I-A) B+A C) \le \lambda_{\max}(I-A){\rm Tr}(B)+\lambda_{\max}(A){\rm Tr}(C). \end{align}","Let $ 0 \preceq A \preceq I$  and let $B$ and $C$ be two symmetric positive definite matrices. Can we related \begin{align} {\rm Tr}( (I-A) B+A C) \end{align} to \begin{align} Tr(I-A)Tr(B)+Tr(A) Tr(C) \end{align} via some inequality? Note, that in the case $A=aI$  for any $a\in [0,1]$ there is an equality. Edit: Note, from a very nice answer below we have: \begin{align} \lambda_{\min}(I-A){\rm Tr}(B)+\lambda_{\min}(A){\rm Tr}(C) \le {\rm Tr}( (I-A) B+A C) \le \lambda_{\max}(I-A){\rm Tr}(B)+\lambda_{\max}(A){\rm Tr}(C). \end{align}",,"['linear-algebra', 'matrices']"
61,"Tangent Space of SymSL(n,$\mathbb{R}$) at arbitrary point","Tangent Space of SymSL(n,) at arbitrary point",\mathbb{R},"I am looking for the tangent space of $SymSL(n,\mathbb{R}) = \{A\in\mathbb{R}^{n\times n} | \,A^{\rm T} = A,\; \det{A}=1 \}$ (actually $n=3$) at an arbitrary $M\in SymSL(n,\mathbb{R})$. (Actually I am interested in $SymSL^+(n,\mathbb{R}) = \{A\in\mathbb{R}^{n\times n} | \,A^{\rm T} = A,\; \det{A}=1,\; A \text{ is positive definite} \}$, but I have the feeling it boils down to the title of this question.) Related questions: Tangent Space of SL(n,â„) at arbitrary point, e.g. not at $\mathbb{1}$ Tangent space of Sym(n,â„) The first question's answer doesn't work here because $Sym(n)$ is not a group, the second question's answer is not applicable because $SL(n)$ is not a vector space. Hence $SymSL(n,\mathbb{R})$ is neither a group nor a vector space. I would very much appreciate any comments. edit #2: $SL(n,\mathbb{R})$ is connected and smooth. So $SymSL(n,\mathbb{R})$ is the intersection of a connected smooth manifold with a vector space. This makes me believe we can talk about a tangent space in this case. Some more possibly usefull facts: the set $SymSL^+(n,\mathbb{R})$ I am actually interested in is a connected, simply connected and complete Riemannian manifold.","I am looking for the tangent space of $SymSL(n,\mathbb{R}) = \{A\in\mathbb{R}^{n\times n} | \,A^{\rm T} = A,\; \det{A}=1 \}$ (actually $n=3$) at an arbitrary $M\in SymSL(n,\mathbb{R})$. (Actually I am interested in $SymSL^+(n,\mathbb{R}) = \{A\in\mathbb{R}^{n\times n} | \,A^{\rm T} = A,\; \det{A}=1,\; A \text{ is positive definite} \}$, but I have the feeling it boils down to the title of this question.) Related questions: Tangent Space of SL(n,â„) at arbitrary point, e.g. not at $\mathbb{1}$ Tangent space of Sym(n,â„) The first question's answer doesn't work here because $Sym(n)$ is not a group, the second question's answer is not applicable because $SL(n)$ is not a vector space. Hence $SymSL(n,\mathbb{R})$ is neither a group nor a vector space. I would very much appreciate any comments. edit #2: $SL(n,\mathbb{R})$ is connected and smooth. So $SymSL(n,\mathbb{R})$ is the intersection of a connected smooth manifold with a vector space. This makes me believe we can talk about a tangent space in this case. Some more possibly usefull facts: the set $SymSL^+(n,\mathbb{R})$ I am actually interested in is a connected, simply connected and complete Riemannian manifold.",,"['matrices', 'differential-geometry', 'lie-groups', 'matrix-equations', 'matrix-calculus']"
62,Help with showing equality of quadratic matrix equations,Help with showing equality of quadratic matrix equations,,"For two symetric matrices $A$ and $B$ of same dimension with compatible column vectors $a$ and $b$ I verified numerically that $$(A^{-1}a + B^{-1}b) ^T (A - A(A+B)^{-1}A) (A^{-1}a + B^{-1}b) - a^TA^{-1}a -b^TB^{-1}b $$ simplifies to $$(a-b)^T(A+B)^{-1}(a-b)$$ I would welcome pointers what is a good approach to derive this. I factored out the first term, but I am stuck after some steps, specifically at $$ 2 b^TB^{-1}a + b^TB^{-1}AB^{-1}b - b^TB^{-1}b - a^T(A+B)^{-1}a  - 2 b^T B^{-1} A (A+B)^{-1}a -b^TB^{-1}A(A+B)^{-1}AB^{-1}b$$ The latter three terms are clearly related through $$(A B^{-1} b  + a) ^T (A+B)^{-1} (A B^{-1} b  + a)$$ But this does not really help.","For two symetric matrices $A$ and $B$ of same dimension with compatible column vectors $a$ and $b$ I verified numerically that $$(A^{-1}a + B^{-1}b) ^T (A - A(A+B)^{-1}A) (A^{-1}a + B^{-1}b) - a^TA^{-1}a -b^TB^{-1}b $$ simplifies to $$(a-b)^T(A+B)^{-1}(a-b)$$ I would welcome pointers what is a good approach to derive this. I factored out the first term, but I am stuck after some steps, specifically at $$ 2 b^TB^{-1}a + b^TB^{-1}AB^{-1}b - b^TB^{-1}b - a^T(A+B)^{-1}a  - 2 b^T B^{-1} A (A+B)^{-1}a -b^TB^{-1}A(A+B)^{-1}AB^{-1}b$$ The latter three terms are clearly related through $$(A B^{-1} b  + a) ^T (A+B)^{-1} (A B^{-1} b  + a)$$ But this does not really help.",,"['linear-algebra', 'matrices']"
63,Prove $A^{T}$ is diagonalizable,Prove  is diagonalizable,A^{T},"Let $A$ be an $n \times n$ matrix. Prove that if $A$ is diagonalizable, then $A^T$ is also diagonalizable. By definition, since $A$ is diagonalizable, $A = P^{-1} B P$, where $B$ is a diagonal matrix and $P$ is an invertible matrix. $A^T = P^T B^T (P^{-1})^T$ $B^T$ is a diagonal matrix by definition, we have $(P^{-1})^T = (P^T)^{-1}$, let $D = P^{-1}$ then, $A^T = DB^T D^{-1}$ where $B^T$ is diagonal, thus $A^T$ is diagonalizable by definition Does this work?","Let $A$ be an $n \times n$ matrix. Prove that if $A$ is diagonalizable, then $A^T$ is also diagonalizable. By definition, since $A$ is diagonalizable, $A = P^{-1} B P$, where $B$ is a diagonal matrix and $P$ is an invertible matrix. $A^T = P^T B^T (P^{-1})^T$ $B^T$ is a diagonal matrix by definition, we have $(P^{-1})^T = (P^T)^{-1}$, let $D = P^{-1}$ then, $A^T = DB^T D^{-1}$ where $B^T$ is diagonal, thus $A^T$ is diagonalizable by definition Does this work?",,"['linear-algebra', 'matrices', 'vector-spaces', 'diagonalization']"
64,Shifting eigenvalues of a matrix,Shifting eigenvalues of a matrix,,"Is there a way to shift a specific eigenvalue of a matrix $A$ without changing any of its eigenvectors? In other words, can I find some $\Delta A$ such that $B = A + \Delta A$ that has the almost the same eigensystem except at 1 eigenvalue? I understand that I can shift or scale all the eigenvalues at once, but not a particular one. Brauer's theorem (below) is the closest thing I find to what I need. However, although the eigenvector corresponding with $\lambda_0$ is not changed, it is not guaranteed that the remaining eigenvectors stay the same. Any reference/suggestions are much appreciated!","Is there a way to shift a specific eigenvalue of a matrix $A$ without changing any of its eigenvectors? In other words, can I find some $\Delta A$ such that $B = A + \Delta A$ that has the almost the same eigensystem except at 1 eigenvalue? I understand that I can shift or scale all the eigenvalues at once, but not a particular one. Brauer's theorem (below) is the closest thing I find to what I need. However, although the eigenvector corresponding with $\lambda_0$ is not changed, it is not guaranteed that the remaining eigenvectors stay the same. Any reference/suggestions are much appreciated!",,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
65,Finding Hessian of tr ((AB)' (AB)),Finding Hessian of tr ((AB)' (AB)),,"I'm trying to find Hessian of $\text{tr}((AB)' (AB))$ where $A,B$ are matrices. There are nice expressions for $H_{AA}$ and $H_{BB}$ using standard approach from Magnus 1 , can anyone suggest how to do same for $H_{AB}$ and  $H_{BA}$ ? More specifically if $A$ is 2x3 and $B$ is 3x4, then we can vectorize A,B and stack them on top of each other so that we have a function from vectors, and Hessian is a block partitioned matrix with blocks 6x6, 6x12, 12x6 and 12x12 corresponding to $H_{AA}$, $H_{AB}$, $H_{BA}$ and $H_{BB}$ Edit (after following techniques in answer, I get following) $$H_{AA}=2(BB'\otimes I_2)$$ $$H_{BB}=2(I_4 \otimes A'A)$$ $$H_{AB}=2(B\otimes A)+2 (I_3 \otimes AB)K_{3,4}$$ $$H_{BA}=2(B'A'\otimes I_3)K_{2,3}+2(B'\otimes A')$$ BTW, the Hessian looks as follows when evaluated with all values being 1. Four colors represent values 0,2,4,8 so that $H_{AB}$ consists of just 2's and 8's. Mathematica code used to generate. 1 (Theorem 1 in 10.6 of Magnus/Nuedecker Matrix Differential Calculus with Applications in Statistics ebook )","I'm trying to find Hessian of $\text{tr}((AB)' (AB))$ where $A,B$ are matrices. There are nice expressions for $H_{AA}$ and $H_{BB}$ using standard approach from Magnus 1 , can anyone suggest how to do same for $H_{AB}$ and  $H_{BA}$ ? More specifically if $A$ is 2x3 and $B$ is 3x4, then we can vectorize A,B and stack them on top of each other so that we have a function from vectors, and Hessian is a block partitioned matrix with blocks 6x6, 6x12, 12x6 and 12x12 corresponding to $H_{AA}$, $H_{AB}$, $H_{BA}$ and $H_{BB}$ Edit (after following techniques in answer, I get following) $$H_{AA}=2(BB'\otimes I_2)$$ $$H_{BB}=2(I_4 \otimes A'A)$$ $$H_{AB}=2(B\otimes A)+2 (I_3 \otimes AB)K_{3,4}$$ $$H_{BA}=2(B'A'\otimes I_3)K_{2,3}+2(B'\otimes A')$$ BTW, the Hessian looks as follows when evaluated with all values being 1. Four colors represent values 0,2,4,8 so that $H_{AB}$ consists of just 2's and 8's. Mathematica code used to generate. 1 (Theorem 1 in 10.6 of Magnus/Nuedecker Matrix Differential Calculus with Applications in Statistics ebook )",,"['linear-algebra', 'matrices', 'matrix-calculus', 'kronecker-product', 'vectorization']"
66,"Find orbits of the group action of $GL(n, F)$ on set of $n \times n$ matrices",Find orbits of the group action of  on set of  matrices,"GL(n, F) n \times n","Let $M_{n}(F)$, $n\times n$ matrices over $F$, and $G=GL(n,F)$, $n\times n$ invertible matrices. $G$ acts on $M_{n}(F)$ via left multiplication. Find the orbits of this group action. Firstly, all the $G$ in $M$ is an orbit from the linear algebra fact. It's also clear that left multiplication can be seen as row operation. But when $G$ acts on the non-invertible matrices, things will become very complicated since there are so many possibilities of reduced row echelon forms. My question is how can we find a neater way to organize all those orbits and express them clearly ?","Let $M_{n}(F)$, $n\times n$ matrices over $F$, and $G=GL(n,F)$, $n\times n$ invertible matrices. $G$ acts on $M_{n}(F)$ via left multiplication. Find the orbits of this group action. Firstly, all the $G$ in $M$ is an orbit from the linear algebra fact. It's also clear that left multiplication can be seen as row operation. But when $G$ acts on the non-invertible matrices, things will become very complicated since there are so many possibilities of reduced row echelon forms. My question is how can we find a neater way to organize all those orbits and express them clearly ?",,"['abstract-algebra', 'matrices', 'linear-groups']"
67,What is $\lim_{t\to 0}\frac{\| I+At\|-1}{t}$? Is it the spectral abscissa?,What is ? Is it the spectral abscissa?,\lim_{t\to 0}\frac{\| I+At\|-1}{t},"I vaguely remember this, and after a lot of googling, I couldn't find anything about it. So I appreciate if someone provides a reference or explains what is this and where it is used. I saw in a book that for a square matrix $A$, there is some kind of measure defined as: $$\mu(A)=\lim_{t\to 0}\frac{\| I+At\|-1}{t}$$ where $I$ is the identity matrix and $\|.\|$ (possibly) denotes the euclidean norm. I didn't find any name/reference/usefulness for this kind of measure. Edit. I changed the title for future references. The wikipedia article for spectral abscissa looks too short and incomplete. And there is no mention of that on wolfram mathworld either. Although it looks like a useful thing, especially in the stability analysis of system matrices.","I vaguely remember this, and after a lot of googling, I couldn't find anything about it. So I appreciate if someone provides a reference or explains what is this and where it is used. I saw in a book that for a square matrix $A$, there is some kind of measure defined as: $$\mu(A)=\lim_{t\to 0}\frac{\| I+At\|-1}{t}$$ where $I$ is the identity matrix and $\|.\|$ (possibly) denotes the euclidean norm. I didn't find any name/reference/usefulness for this kind of measure. Edit. I changed the title for future references. The wikipedia article for spectral abscissa looks too short and incomplete. And there is no mention of that on wolfram mathworld either. Although it looks like a useful thing, especially in the stability analysis of system matrices.",,"['matrices', 'reference-request', 'normed-spaces']"
68,Determinant of a matrix that is almost lower triangular,Determinant of a matrix that is almost lower triangular,,"Calculate the determinant of $$ \left[ \begin{array}{cccc} 1 & 0 & 0 & 0  & \cdots & 1\\ 1 & a_1 & 0 & 0  &  \cdots & 0 \\ 1 & 1 & a_2 & 0  &  \cdots & 0 \\ 1 & 0 & 1 & a_3  &  \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots  \\  1 & 0 & 0  & \cdots& 1 & a_{n-1} \\ \end{array} \right]$$ I tried to develop at the first line, but got stuck. Any helps or hints appreciated.","Calculate the determinant of $$ \left[ \begin{array}{cccc} 1 & 0 & 0 & 0  & \cdots & 1\\ 1 & a_1 & 0 & 0  &  \cdots & 0 \\ 1 & 1 & a_2 & 0  &  \cdots & 0 \\ 1 & 0 & 1 & a_3  &  \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots  \\  1 & 0 & 0  & \cdots& 1 & a_{n-1} \\ \end{array} \right]$$ I tried to develop at the first line, but got stuck. Any helps or hints appreciated.",,"['matrices', 'determinant']"
69,Let $D$ be a diagonal matrix; find the inverse of $D+ab^T$,Let  be a diagonal matrix; find the inverse of,D D+ab^T,"It's an exercise from A Primer on Linear Models written by John F. Monahan. Is there any special property of $ab^T$,where $a$ and $b$ are both vectors?","It's an exercise from A Primer on Linear Models written by John F. Monahan. Is there any special property of $ab^T$,where $a$ and $b$ are both vectors?",,"['linear-algebra', 'matrices', 'inverse']"
70,Proving that if $A$ is an $m\times n$ matrix and has rank equal to $m$ then $A$ has a right inverse.,Proving that if  is an  matrix and has rank equal to  then  has a right inverse.,A m\times n m A,"By a ""right inverse"" I mean an $n\times m$ matrix $B$ such that $AB = I_m$, where $I_m$ is the $m\times m$ identity matrix. So, obviously $m \leq n$ for $A$ to be of rank $m$. I already know that if $m=n$ then $A$ is an $m$ x $m$ matrix with rank $m$ so it must be invertible. Any hints on how to prove for when $m < n$? Am I going about it wrongfully in separating the problem into two different cases?","By a ""right inverse"" I mean an $n\times m$ matrix $B$ such that $AB = I_m$, where $I_m$ is the $m\times m$ identity matrix. So, obviously $m \leq n$ for $A$ to be of rank $m$. I already know that if $m=n$ then $A$ is an $m$ x $m$ matrix with rank $m$ so it must be invertible. Any hints on how to prove for when $m < n$? Am I going about it wrongfully in separating the problem into two different cases?",,"['linear-algebra', 'matrices', 'matrix-rank']"
71,How many linearly independent eigenvectors are there?,How many linearly independent eigenvectors are there?,,"The number of linearly independent eigenvectors for eigenvalue $1$ for the given matrix ? $\begin{bmatrix} 1 & 3 & 2 \\ 0 & 4 & 2\\ 0 &-3 & -1 \end{bmatrix}$ Eigenvalues are $1,1,2$ for the above matrix .So, $1$ has multiplicity $2$ and $2$ has multiplicity $1$. After putting the value $1$, I am getting $ i= $any value, and the relation $3j = -2k$. Now, How to proceed further ?","The number of linearly independent eigenvectors for eigenvalue $1$ for the given matrix ? $\begin{bmatrix} 1 & 3 & 2 \\ 0 & 4 & 2\\ 0 &-3 & -1 \end{bmatrix}$ Eigenvalues are $1,1,2$ for the above matrix .So, $1$ has multiplicity $2$ and $2$ has multiplicity $1$. After putting the value $1$, I am getting $ i= $any value, and the relation $3j = -2k$. Now, How to proceed further ?",,"['matrices', 'eigenvalues-eigenvectors']"
72,"Find a finite generating set for $Gl(n,\mathbb{Z})$",Find a finite generating set for,"Gl(n,\mathbb{Z})","I need to find a finite generating set for $Gl(n,\mathbb{Z})$. I heard somewhere once that this group is generated by the elementary matrices - of course, if I'm going to prove that $GL(n,\mathbb{Z})$ has a finite generating set, I would need to prove that any matrix $M\in  GL(n,\mathbb{Z})$ can be generated by only finitely many of them. At first, I didn't have a clue as to how to do this, so I did a bit of scouring the internet for any information that might be useful. There were a few proofs or hints at proofs, including here on MSE and also on MathOverflow, but they were either too advanced, didn't give enough details, assumed theory I can't assume at this point (for example about rings or principle ideal domains), or were extremely complicated (as in 4 pages with 4 lemmas that needed to be proven first - and this example didn't even prove exactly what the finite generator of $GL(n,\mathbb{Z})$ is). This looks promising. In their notation, essentially, if $n$ is even, then $GL(n,\mathbb{Z})$ is generated by $s_{1}$ and $s_{3}$ And when $n$ is odd, $-s_{1}$ and $s_{3}$ generate $GL(n,\mathbb{Z})$, where $s_{1}=\begin{pmatrix} 0&0&0&\cdots &0&1\\ 1&0&0&\cdots & 0&0\\0&1&0&\cdots & 0 &0 \\ \vdots & \vdots & \vdots & & \vdots &\vdots \\ 0&0&0&\cdots & 0&0\\ 0&0&0&\cdots &1 & 0\end{pmatrix}$ and $s_{3}=\begin{pmatrix} 1&1&0&\cdots &0&0\\ 0&1&0&\cdots & 0&0\\0&0&1&\cdots & 0 &0 \\ \vdots & \vdots & \vdots & & \vdots &\vdots \\ 0&0&0&\cdots & 1&0\\ 0&0&0&\cdots &0& 1\end{pmatrix}$ How is a relatively simple way to prove this, that does not invoke rings or ideals at all (only group theory is permissible), and does not amake reference so to papers or $Hom(G,C_{p})$ (whatever that is)? I'm guessing since the group operation in $GL(n,\mathbb{Z})$ is matrix multiplication, I'm guessing I'd have to show that any matrix $A$ can be generated by multiplying various combinations of $s_{1}$ and $s_{3}$ in the case when $n$ is even and various combinations of $-s_{1}$ and $s_{3}$ in the case when $n$ is odd. But what do those combinations look like when we're dealing with matrix multiplication? Do they include scalar multiples like integer linear combinations when the operation is addition? And how do we know what order to put them in, since matrix multiplication is not commutative? Thank you.","I need to find a finite generating set for $Gl(n,\mathbb{Z})$. I heard somewhere once that this group is generated by the elementary matrices - of course, if I'm going to prove that $GL(n,\mathbb{Z})$ has a finite generating set, I would need to prove that any matrix $M\in  GL(n,\mathbb{Z})$ can be generated by only finitely many of them. At first, I didn't have a clue as to how to do this, so I did a bit of scouring the internet for any information that might be useful. There were a few proofs or hints at proofs, including here on MSE and also on MathOverflow, but they were either too advanced, didn't give enough details, assumed theory I can't assume at this point (for example about rings or principle ideal domains), or were extremely complicated (as in 4 pages with 4 lemmas that needed to be proven first - and this example didn't even prove exactly what the finite generator of $GL(n,\mathbb{Z})$ is). This looks promising. In their notation, essentially, if $n$ is even, then $GL(n,\mathbb{Z})$ is generated by $s_{1}$ and $s_{3}$ And when $n$ is odd, $-s_{1}$ and $s_{3}$ generate $GL(n,\mathbb{Z})$, where $s_{1}=\begin{pmatrix} 0&0&0&\cdots &0&1\\ 1&0&0&\cdots & 0&0\\0&1&0&\cdots & 0 &0 \\ \vdots & \vdots & \vdots & & \vdots &\vdots \\ 0&0&0&\cdots & 0&0\\ 0&0&0&\cdots &1 & 0\end{pmatrix}$ and $s_{3}=\begin{pmatrix} 1&1&0&\cdots &0&0\\ 0&1&0&\cdots & 0&0\\0&0&1&\cdots & 0 &0 \\ \vdots & \vdots & \vdots & & \vdots &\vdots \\ 0&0&0&\cdots & 1&0\\ 0&0&0&\cdots &0& 1\end{pmatrix}$ How is a relatively simple way to prove this, that does not invoke rings or ideals at all (only group theory is permissible), and does not amake reference so to papers or $Hom(G,C_{p})$ (whatever that is)? I'm guessing since the group operation in $GL(n,\mathbb{Z})$ is matrix multiplication, I'm guessing I'd have to show that any matrix $A$ can be generated by multiplying various combinations of $s_{1}$ and $s_{3}$ in the case when $n$ is even and various combinations of $-s_{1}$ and $s_{3}$ in the case when $n$ is odd. But what do those combinations look like when we're dealing with matrix multiplication? Do they include scalar multiples like integer linear combinations when the operation is addition? And how do we know what order to put them in, since matrix multiplication is not commutative? Thank you.",,"['abstract-algebra', 'matrices']"
73,Derivative of a function involving diagonal matrix,Derivative of a function involving diagonal matrix,,"Let $A$ be a $n\times n$ matrix, $\text{diag}(x)$ is the diagonal matrix with $x$ on the diagonal. How can I find $dF(x)$ for $F(x) = \text{diag}(x)Ax$? Thank you very much in advance!","Let $A$ be a $n\times n$ matrix, $\text{diag}(x)$ is the diagonal matrix with $x$ on the diagonal. How can I find $dF(x)$ for $F(x) = \text{diag}(x)Ax$? Thank you very much in advance!",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
74,Second derivative of $\det\sqrt{F^TF}$ with respect to $F$,Second derivative of  with respect to,\det\sqrt{F^TF} F,"I need to evaluate $$ \frac{\partial^2W}{\partial F^2}(I)(F,F)=\sum_{i,j,k,l=1}^3\frac{\partial^2W}{\partial F_{ij}F_{kl}}(I)F_{ij}F_{kl} $$ for $W=\det\sqrt{F^TF}$. Here $F\in\mathbb{R}^{3\times3}$. From First derivative we know that $$ \frac{\partial W}{\partial F}=WF^{-T}. $$ To evaluate the sum above, I write $$ \frac{\partial^2W}{\partial F^2}=W\left[\left(F^{-T}\right)^2+\frac{\partial F^{-T}}{\partial F}\right]â€¤ $$ Here I got stack.","I need to evaluate $$ \frac{\partial^2W}{\partial F^2}(I)(F,F)=\sum_{i,j,k,l=1}^3\frac{\partial^2W}{\partial F_{ij}F_{kl}}(I)F_{ij}F_{kl} $$ for $W=\det\sqrt{F^TF}$. Here $F\in\mathbb{R}^{3\times3}$. From First derivative we know that $$ \frac{\partial W}{\partial F}=WF^{-T}. $$ To evaluate the sum above, I write $$ \frac{\partial^2W}{\partial F^2}=W\left[\left(F^{-T}\right)^2+\frac{\partial F^{-T}}{\partial F}\right]â€¤ $$ Here I got stack.",,"['linear-algebra', 'matrices', 'derivatives', 'determinant']"
75,A problem with a determinant,A problem with a determinant,,Let $A$ and $B$ be $n\times n$ matrices. Let $x$ be a scalar variable and define $D(x):=det(A+Bx)$. It can be easily shown by induction on $n$ that $D(x)$ is a polynomial of degree at most $n$. The problem is to find an explicit polynomial if the matrices are as of the following form (with $a\neq b$): $$A= \begin{pmatrix}     \lambda_1       & a & a & \dots & a \\     b       & \lambda_2 & a & \dots & a \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     b      & b & b & \dots & \lambda_n \end{pmatrix}$$ $$B= \begin{pmatrix}     1     & 1 & 1 & \dots & 1 \\     1       & 1 & 1 & \dots & 1 \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     1      & 1 & 1 & \dots & 1 \end{pmatrix}$$ I already spotted an inductive pattern not really that useful: I found some regularity by calculating the determinant on the first column: the first term is $(\lambda_1+x)*D_{n-1}$ and the other terms are summations of $(b+x)(a+x)*D_{n-2}$ where every $D$ is called on submatrices with different $\lambda$ but it doesnt't really help for finding an explicit formulation. It seems to me at most useful for proving an already found polynomial is the right one.,Let $A$ and $B$ be $n\times n$ matrices. Let $x$ be a scalar variable and define $D(x):=det(A+Bx)$. It can be easily shown by induction on $n$ that $D(x)$ is a polynomial of degree at most $n$. The problem is to find an explicit polynomial if the matrices are as of the following form (with $a\neq b$): $$A= \begin{pmatrix}     \lambda_1       & a & a & \dots & a \\     b       & \lambda_2 & a & \dots & a \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     b      & b & b & \dots & \lambda_n \end{pmatrix}$$ $$B= \begin{pmatrix}     1     & 1 & 1 & \dots & 1 \\     1       & 1 & 1 & \dots & 1 \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     1      & 1 & 1 & \dots & 1 \end{pmatrix}$$ I already spotted an inductive pattern not really that useful: I found some regularity by calculating the determinant on the first column: the first term is $(\lambda_1+x)*D_{n-1}$ and the other terms are summations of $(b+x)(a+x)*D_{n-2}$ where every $D$ is called on submatrices with different $\lambda$ but it doesnt't really help for finding an explicit formulation. It seems to me at most useful for proving an already found polynomial is the right one.,,"['linear-algebra', 'matrices', 'polynomials', 'induction', 'determinant']"
76,Frobenius norm of Hadamard product,Frobenius norm of Hadamard product,,"I have an $(n\times n)$ real matrix obtained through the Hadamard product, $H=A\circ B$, of two real $(n\times n)$ symmetric matrices $A,B$. All elements of $A$ are positive, while diagonal elements of $B$ are zero and the rest are non-negative. My question is if there is any way I can relate (possibly through some inequalities) the Frobenius norm of $H$ to the ones of $A, B$. The Frobenius norm of a real $(n\times n)$ matrix $M$ is defined as $$ \| M \|_F = \sqrt{Tr\left(M^T M\right)} =\sqrt{\sum_{i,j=1}^n M_{i,j}^2} \,. $$ Thanks!","I have an $(n\times n)$ real matrix obtained through the Hadamard product, $H=A\circ B$, of two real $(n\times n)$ symmetric matrices $A,B$. All elements of $A$ are positive, while diagonal elements of $B$ are zero and the rest are non-negative. My question is if there is any way I can relate (possibly through some inequalities) the Frobenius norm of $H$ to the ones of $A, B$. The Frobenius norm of a real $(n\times n)$ matrix $M$ is defined as $$ \| M \|_F = \sqrt{Tr\left(M^T M\right)} =\sqrt{\sum_{i,j=1}^n M_{i,j}^2} \,. $$ Thanks!",,"['matrices', 'normed-spaces']"
77,"If a determinant of a matrix is 0, is the graph formed from it acyclic?","If a determinant of a matrix is 0, is the graph formed from it acyclic?",,"I read an answer on the Mathematica subforum that explained that a graph's adjacency matrix can be triangular (except the diagonal) if and only if the graph is a DAG. It is easy to see that the determinant of said matrix will always be 0. Does this mean that every binary matrix (n x n) with a determinant of 0 will be an adjacency matrix to a DAG? If not, then can you provide a counterexample?","I read an answer on the Mathematica subforum that explained that a graph's adjacency matrix can be triangular (except the diagonal) if and only if the graph is a DAG. It is easy to see that the determinant of said matrix will always be 0. Does this mean that every binary matrix (n x n) with a determinant of 0 will be an adjacency matrix to a DAG? If not, then can you provide a counterexample?",,"['matrices', 'graph-theory', 'determinant', 'adjacency-matrix']"
78,Calculating rotation axis from rotation matrix,Calculating rotation axis from rotation matrix,,"Suppose we are given a rotation matrix, i.e. an orthogonal $3 \times 3$ matrix $\mathbf{A} = (a_{ij})$ with $\det \mathbf{A} = 1$. Then the mapping $\mathbf{x} \mapsto \mathbf{A}\mathbf{x}$ will rotate points of $\mathbb{R}^3$ around some axis through the origin. I have some old notes (which I wrote decades ago) that say that the axis of rotation can be obtained in several ways: As an eigenvector of $\mathbf{A}$ corresponding to the eigenvalue $1$. As the vector $(a_{32} - a_{23}, \; a_{13} - a_{31}, \; a_{21} - a_{12})$ As any row of the matrix $\mathbf{A} + \mathbf{A}^T + [1 - \text{trace}(\mathbf{A})]\mathbf{I}$. As any row of the matrix $\text{adj}\,(\mathbf{A} - \mathbf{I})$. The rows of this matrix are all scalar multiples of one another, so it doesn't matter which row we use. As the cross product of the first two columns of the matrix $\mathbf{A} - \mathbf{I}$. This will actually be the third row of $\text{adj}\,(\mathbf{A} - \mathbf{I})$. I want to confirm all of these statements. Clearly #1 is obvious, and #2 is well-known, but I'm having trouble with the other three. It's possible that they're not even true. Can someone elucidate, please. In your answer, it's OK to assume that #1 and #2 are already known. Also, any thoughts on which calculations are most stable, numerically?","Suppose we are given a rotation matrix, i.e. an orthogonal $3 \times 3$ matrix $\mathbf{A} = (a_{ij})$ with $\det \mathbf{A} = 1$. Then the mapping $\mathbf{x} \mapsto \mathbf{A}\mathbf{x}$ will rotate points of $\mathbb{R}^3$ around some axis through the origin. I have some old notes (which I wrote decades ago) that say that the axis of rotation can be obtained in several ways: As an eigenvector of $\mathbf{A}$ corresponding to the eigenvalue $1$. As the vector $(a_{32} - a_{23}, \; a_{13} - a_{31}, \; a_{21} - a_{12})$ As any row of the matrix $\mathbf{A} + \mathbf{A}^T + [1 - \text{trace}(\mathbf{A})]\mathbf{I}$. As any row of the matrix $\text{adj}\,(\mathbf{A} - \mathbf{I})$. The rows of this matrix are all scalar multiples of one another, so it doesn't matter which row we use. As the cross product of the first two columns of the matrix $\mathbf{A} - \mathbf{I}$. This will actually be the third row of $\text{adj}\,(\mathbf{A} - \mathbf{I})$. I want to confirm all of these statements. Clearly #1 is obvious, and #2 is well-known, but I'm having trouble with the other three. It's possible that they're not even true. Can someone elucidate, please. In your answer, it's OK to assume that #1 and #2 are already known. Also, any thoughts on which calculations are most stable, numerically?",,"['linear-algebra', 'matrices', 'linear-transformations', 'rotations']"
79,Power of a Nonsymmetric Matrix,Power of a Nonsymmetric Matrix,,"I have a matrix $M$ and a vector $x$. I would like to compute the vector $y= M^n x$. I found the eigenvalues $Î»_k$ of $M$ and the left and right eigenvectors, properly normalized. If M was symmetric, I would now project $x$ on each eigenvector, multiply by $Î»_k^n$, and then recombine them. That would give me $y$. But $M$ is not symmetric, Is it any useful that I have the left and right eigenvectors? Would you know how to compute $y$? EDIT (in answer to Florian below): the matrix whose rows are the right eigenvectors of $M$ is too hard to invert, so my question is whether one can to bypass the inversion by using the fact that one knows both sets of eigenvectors?","I have a matrix $M$ and a vector $x$. I would like to compute the vector $y= M^n x$. I found the eigenvalues $Î»_k$ of $M$ and the left and right eigenvectors, properly normalized. If M was symmetric, I would now project $x$ on each eigenvector, multiply by $Î»_k^n$, and then recombine them. That would give me $y$. But $M$ is not symmetric, Is it any useful that I have the left and right eigenvectors? Would you know how to compute $y$? EDIT (in answer to Florian below): the matrix whose rows are the right eigenvectors of $M$ is too hard to invert, so my question is whether one can to bypass the inversion by using the fact that one knows both sets of eigenvectors?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant', 'inverse']"
80,"Why is that the matrix $1$-norm and $\infty$-norm are equal to the operator norm, while the $2$-norm is not?","Why is that the matrix -norm and -norm are equal to the operator norm, while the -norm is not?",1 \infty 2,"I had always assumed that the operator norm of a matrix $A$ : $$\|A\|_p = \sup\limits_{\|v\|_p \neq 0}\dfrac{\|Av\|_p}{\|v\|_p}$$ was the same as the norm of a matrix: $${\displaystyle \|A\|_{p}= \left(\sum _{i=1}^{m}\sum _{j=1}^{n}|a_{ij}|^{p}\right)^{1/p}}$$ However, this post seems to shatter my assumption: 2-norm vs operator norm . Upon further examination , it seems that the operator norm and matrix norm only coincide (=) for the matrix $1$ -norm or the matrix $\infty$ -norm (and extremely limited cases for matrix $2$ -norm). Why is this so? Is there a theorem that relates the operator norm with matrix $1$ or $\infty$ norm and show their equality? Note:","I had always assumed that the operator norm of a matrix : was the same as the norm of a matrix: However, this post seems to shatter my assumption: 2-norm vs operator norm . Upon further examination , it seems that the operator norm and matrix norm only coincide (=) for the matrix -norm or the matrix -norm (and extremely limited cases for matrix -norm). Why is this so? Is there a theorem that relates the operator norm with matrix or norm and show their equality? Note:",A \|A\|_p = \sup\limits_{\|v\|_p \neq 0}\dfrac{\|Av\|_p}{\|v\|_p} {\displaystyle \|A\|_{p}= \left(\sum _{i=1}^{m}\sum _{j=1}^{n}|a_{ij}|^{p}\right)^{1/p}} 1 \infty 2 1 \infty,"['linear-algebra', 'matrices', 'normed-spaces', 'definition', 'matrix-norms']"
81,Determinant of Skew-Symmetric Matrices,Determinant of Skew-Symmetric Matrices,,"Let $M \subset M_n(\mathbb C)$ be the set of $n \times n$ skew-symmetric matrices. 1) If $n$ is odd their determinant is equal to $0$. 2) If $n$ is even their determinant is a polynomial (in several variables) which is NOT irreducible as an element of $\mathbb C[x_1, \dots,x_r]$. Partial Solution: 1) $A=-^TA \implies \det(A)=(-1)^n\det(^TA) \implies \det(A)=0$. 2) When n=2 $$ \left( \begin{array}{ccc} 0 & x_1  \\ -x_1 & 0  \end{array} \right) $$ we get $p(x_1)=x_1^2$. If $n=4$ $$ \left( \begin{array}{ccc} 0 & a & b & c\\ -a & 0 & d & e\\ -b & -d & 0 & f\\ -c & -e & -f & 0\end{array} \right) $$ we get $p(a,b,c,d,e,f)=(be)^2 - (af+cd)^2=(be+af+cd)(be-af-cd)$. I tried to use $2\times 2$ block matrices or to compute $\det$ along a raw with no success in the general case.","Let $M \subset M_n(\mathbb C)$ be the set of $n \times n$ skew-symmetric matrices. 1) If $n$ is odd their determinant is equal to $0$. 2) If $n$ is even their determinant is a polynomial (in several variables) which is NOT irreducible as an element of $\mathbb C[x_1, \dots,x_r]$. Partial Solution: 1) $A=-^TA \implies \det(A)=(-1)^n\det(^TA) \implies \det(A)=0$. 2) When n=2 $$ \left( \begin{array}{ccc} 0 & x_1  \\ -x_1 & 0  \end{array} \right) $$ we get $p(x_1)=x_1^2$. If $n=4$ $$ \left( \begin{array}{ccc} 0 & a & b & c\\ -a & 0 & d & e\\ -b & -d & 0 & f\\ -c & -e & -f & 0\end{array} \right) $$ we get $p(a,b,c,d,e,f)=(be)^2 - (af+cd)^2=(be+af+cd)(be-af-cd)$. I tried to use $2\times 2$ block matrices or to compute $\det$ along a raw with no success in the general case.",,"['linear-algebra', 'matrices', 'algebraic-geometry']"
82,Find all matrices in $\{\pm1\}^{n \times n}$ such that the sums of its rows and columns are zero,Find all matrices in  such that the sums of its rows and columns are zero,\{\pm1\}^{n \times n},"Find all matrices in $\{\pm1\}^{n \times n}$ such that the sums of its rows and columns are zero. The question was for $n = 4$, which was easy to do with counting. For any $n$ how would we do it?","Find all matrices in $\{\pm1\}^{n \times n}$ such that the sums of its rows and columns are zero. The question was for $n = 4$, which was easy to do with counting. For any $n$ how would we do it?",,"['combinatorics', 'matrices']"
83,"Prove that $\{T(x),T^2(x),\dots,T^{n-1}(x)\}$ is linearly independent",Prove that  is linearly independent,"\{T(x),T^2(x),\dots,T^{n-1}(x)\}","Question : Assume that $V$ is a vector space with dimension $n-1$ and $T:V\to V$ is a linear transformation. If for one $x \in V$ :  $T^{n-1}(x) \neq 0$ but $T^n(x)=0$ , Prove that $\{T(x),T^2(x),\dots,T^{n-1}(x)\}$ is linearly independent. ( So it's a basis of $V$ ) .  Find Matrix respresntation of $T$ with respect to this basis. My work so far : Assume that there exists a linear combination like $\alpha_1T(x)+\alpha_2T^2(x)+\dots+a_{n-1}T^{n-1}(x)=0$. I want to prove that $\forall i\space\space\alpha_i=0$.  I must prove that none of $T^i(x) : i \le{n-1}$ are zero.  I have no idea how to show that ...","Question : Assume that $V$ is a vector space with dimension $n-1$ and $T:V\to V$ is a linear transformation. If for one $x \in V$ :  $T^{n-1}(x) \neq 0$ but $T^n(x)=0$ , Prove that $\{T(x),T^2(x),\dots,T^{n-1}(x)\}$ is linearly independent. ( So it's a basis of $V$ ) .  Find Matrix respresntation of $T$ with respect to this basis. My work so far : Assume that there exists a linear combination like $\alpha_1T(x)+\alpha_2T^2(x)+\dots+a_{n-1}T^{n-1}(x)=0$. I want to prove that $\forall i\space\space\alpha_i=0$.  I must prove that none of $T^i(x) : i \le{n-1}$ are zero.  I have no idea how to show that ...",,"['linear-algebra', 'matrices', 'linear-transformations']"
84,Is the complex symmetric part of a $2\times2$ normal matrix always normal?,Is the complex symmetric part of a  normal matrix always normal?,2\times2,"I want to prove (or disprove) that, for a normal matrix $A\in\mathbb{C}^{2x2}$ (so that $AA^\ast=A^\ast A$), the sum $A+A^T$ is (or isn't) normal. I know that for $3\times3$ normal matrices, $A+A^T$ isn't normal, but for the $2\times2$ case, when I try to come with a counterexample I do not manage to make it.","I want to prove (or disprove) that, for a normal matrix $A\in\mathbb{C}^{2x2}$ (so that $AA^\ast=A^\ast A$), the sum $A+A^T$ is (or isn't) normal. I know that for $3\times3$ normal matrices, $A+A^T$ isn't normal, but for the $2\times2$ case, when I try to come with a counterexample I do not manage to make it.",,"['linear-algebra', 'matrices']"
85,"Given the following linear transformation, find the matrix associated to $\varphi$ through a given base.","Given the following linear transformation, find the matrix associated to  through a given base.",\varphi,"Text of the exercise : Consider the linear transformation $\varphi:\mathbb{R}^3 \rightarrow \mathbb{R}^3$ defined by: $\varphi(1,0,0)  = (0,1,0)\\ \varphi(0,1,1) = (1,1,0) \\\varphi(0,0,1) =(1,0,0)$ Is $B={(0,1,1),(0,1,0),(1,0,0)}$ a basis of $\mathbb{R}^3$? If the answer is yes, find the matrix $M_{\varphi}^B$ associated to $\varphi$ through $B$. Reasoning : $B$ is a basis of $\mathbb{R}^3$ since it is a set of three linearly independent vectors (proof is given by the fact that the matrix constituted by their coordinates is not singular) in a three-dimensional space. I know from the definition of $\varphi$ the result of the transformation of two of the vectors of $B$; precisely $\varphi(1,0,0)=(0,1,0)$ and $\varphi(0,1,1)=(1,1,0)$. In order to infer the last one, I could use the definind property of a linear mapping, i.e. that a function $f:V\rightarrow W$ is linear if $\forall \textbf{v}_{1},\textbf{v}_{2}\in V$, $\forall\lambda_{1},\lambda_{2}\in \mathbb{K}$ then $f(\lambda_{1}\textbf{v}_{1}+\lambda_{2}\textbf{v}_{2}=\lambda_{1}f(\textbf{v}_{1})+\lambda_{1}f(\textbf{v}_{1})$, so that: \begin{equation} \varphi(0,1,0)=\varphi(0,1,1)-\varphi(0,0,1)=(1,1,0)-(1,0,0)=(0,1,0) \end{equation} The asked matrix is thus: \begin{equation} M_{\varphi}^B=\begin{bmatrix} 1&0&0\\1&1&1\\0&0&0 \end{bmatrix} \end{equation} I'd greatly appreciate a feedback. Thank you all!","Text of the exercise : Consider the linear transformation $\varphi:\mathbb{R}^3 \rightarrow \mathbb{R}^3$ defined by: $\varphi(1,0,0)  = (0,1,0)\\ \varphi(0,1,1) = (1,1,0) \\\varphi(0,0,1) =(1,0,0)$ Is $B={(0,1,1),(0,1,0),(1,0,0)}$ a basis of $\mathbb{R}^3$? If the answer is yes, find the matrix $M_{\varphi}^B$ associated to $\varphi$ through $B$. Reasoning : $B$ is a basis of $\mathbb{R}^3$ since it is a set of three linearly independent vectors (proof is given by the fact that the matrix constituted by their coordinates is not singular) in a three-dimensional space. I know from the definition of $\varphi$ the result of the transformation of two of the vectors of $B$; precisely $\varphi(1,0,0)=(0,1,0)$ and $\varphi(0,1,1)=(1,1,0)$. In order to infer the last one, I could use the definind property of a linear mapping, i.e. that a function $f:V\rightarrow W$ is linear if $\forall \textbf{v}_{1},\textbf{v}_{2}\in V$, $\forall\lambda_{1},\lambda_{2}\in \mathbb{K}$ then $f(\lambda_{1}\textbf{v}_{1}+\lambda_{2}\textbf{v}_{2}=\lambda_{1}f(\textbf{v}_{1})+\lambda_{1}f(\textbf{v}_{1})$, so that: \begin{equation} \varphi(0,1,0)=\varphi(0,1,1)-\varphi(0,0,1)=(1,1,0)-(1,0,0)=(0,1,0) \end{equation} The asked matrix is thus: \begin{equation} M_{\varphi}^B=\begin{bmatrix} 1&0&0\\1&1&1\\0&0&0 \end{bmatrix} \end{equation} I'd greatly appreciate a feedback. Thank you all!",,"['linear-algebra', 'matrices', 'linear-transformations']"
86,Proof regarding the positive definite matrix,Proof regarding the positive definite matrix,,"Assume $y,s\in \mathbb{R}^n$ are such that $y^Ts > 0$. Can we prove that there exists a symmetric and positive definite matrix $M \in \mathbb{R}^{n\times n}$ such that  $$Ms= y$$ In particular, there exists a non-singular matrix $W \in \mathbb{R}^{n\times n}$ such that $WW^Ts = y$.","Assume $y,s\in \mathbb{R}^n$ are such that $y^Ts > 0$. Can we prove that there exists a symmetric and positive definite matrix $M \in \mathbb{R}^{n\times n}$ such that  $$Ms= y$$ In particular, there exists a non-singular matrix $W \in \mathbb{R}^{n\times n}$ such that $WW^Ts = y$.",,"['linear-algebra', 'matrices', 'positive-definite']"
87,Inverse of a symmetric tridiagonal almost-Toeplitz matrix,Inverse of a symmetric tridiagonal almost-Toeplitz matrix,,"I'm trying to analytically find the inverse of the following $N \times N$ tridiagonal matrix: $$T = \begin{bmatrix} 1 & -c \\ -c & 2 & -c \\ & -c & 2 & \ddots \\ & & \ddots & \ddots & \ddots \\ & & & \ddots  & 2 & -c \\ & & & & -c & 2 & -c \\ & & & & & -c & 1 \end{bmatrix}$$ That is, all elements just off the main diagonal are $-c$, where $c \in (0, 1)$; the main diagonal has a one in the top-left and in the bottom-right, and twos in between; and everything else is a zero. This comes from a statistical model I'm using (a CAR spatial model for a path graph) where, by construction, $T^{-1}$ is a covariance matrix.  Thus I know $T^{-1}$ is symmetric and the elements of its main diagonal are all positive.  I'd happily settle for only getting formulas for those main diagonal elements. Copying from the Wikipedia page on tridiagonal matrices (and doing a little bit of work), the $i$th diagonal element of $T^{-1}$ is $(T^{-1})_{ii} = \theta_{i-1} \theta_{N-i} / \theta_N$, where $$\theta_i = a_i \theta_{i-1} - c^2 \theta_{i-2} \text{ for } i = 2,3,\dots,N$$ with initial conditions $\theta_0 = \theta_1 = 1$.  The $a_i$ are the main diagonal elements of $T$, so e.g. if $N = 5$ then $\{a_i\} = \{1,2,2,2,1\}$.  One way to answer my question would be to solve the above recurrence relation, i.e. find a closed-form for $\theta_i$ for any $N \ge 2$.  (I tried this using a generating function, but got stuck due to the pesky non-constant diagonal of $T$, i.e. the $a_i$.) In the following related question, the person who answered it used a very different, very involved approach: Inverse of a symmetric tridiagonal matrix. .  There, the matrix to be inverted had a constant main diagonal, so it was easier.","I'm trying to analytically find the inverse of the following $N \times N$ tridiagonal matrix: $$T = \begin{bmatrix} 1 & -c \\ -c & 2 & -c \\ & -c & 2 & \ddots \\ & & \ddots & \ddots & \ddots \\ & & & \ddots  & 2 & -c \\ & & & & -c & 2 & -c \\ & & & & & -c & 1 \end{bmatrix}$$ That is, all elements just off the main diagonal are $-c$, where $c \in (0, 1)$; the main diagonal has a one in the top-left and in the bottom-right, and twos in between; and everything else is a zero. This comes from a statistical model I'm using (a CAR spatial model for a path graph) where, by construction, $T^{-1}$ is a covariance matrix.  Thus I know $T^{-1}$ is symmetric and the elements of its main diagonal are all positive.  I'd happily settle for only getting formulas for those main diagonal elements. Copying from the Wikipedia page on tridiagonal matrices (and doing a little bit of work), the $i$th diagonal element of $T^{-1}$ is $(T^{-1})_{ii} = \theta_{i-1} \theta_{N-i} / \theta_N$, where $$\theta_i = a_i \theta_{i-1} - c^2 \theta_{i-2} \text{ for } i = 2,3,\dots,N$$ with initial conditions $\theta_0 = \theta_1 = 1$.  The $a_i$ are the main diagonal elements of $T$, so e.g. if $N = 5$ then $\{a_i\} = \{1,2,2,2,1\}$.  One way to answer my question would be to solve the above recurrence relation, i.e. find a closed-form for $\theta_i$ for any $N \ge 2$.  (I tried this using a generating function, but got stuck due to the pesky non-constant diagonal of $T$, i.e. the $a_i$.) In the following related question, the person who answered it used a very different, very involved approach: Inverse of a symmetric tridiagonal matrix. .  There, the matrix to be inverted had a constant main diagonal, so it was easier.",,"['linear-algebra', 'matrices', 'recurrence-relations', 'generating-functions', 'tridiagonal-matrices']"
88,What is the relationship between the characteristic polynomial of a product vs. the product of characteristic polynomials,What is the relationship between the characteristic polynomial of a product vs. the product of characteristic polynomials,,"What is the relationship between the characteristic polynomial of two square matrices and the characteristic polynomial of the product of these two square matrices? If I know the characteristic polynomials of each one of these matrices, what can I say about the characteristic polynomial of their product? I can't seem to find this information anywhere. Thank you!","What is the relationship between the characteristic polynomial of two square matrices and the characteristic polynomial of the product of these two square matrices? If I know the characteristic polynomials of each one of these matrices, what can I say about the characteristic polynomial of their product? I can't seem to find this information anywhere. Thank you!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
89,Cholesky decomposition vs. $\mathbf L\mathbf D\mathbf L^\top$ decomposition,Cholesky decomposition vs.  decomposition,\mathbf L\mathbf D\mathbf L^\top,"In different books and on Wikipedia, you can see frequent mentions of Cholesky decomposition and $\mathbf L\mathbf D\mathbf L^\top$ decomposition is seldom mentioned. Why so? As far as I understand, $\mathbf L\mathbf D\mathbf L^\top$ decomposition can be applied to a broader range of matrices (we don't need a matrix to be positive-definite) and for the same computational price.","In different books and on Wikipedia, you can see frequent mentions of Cholesky decomposition and decomposition is seldom mentioned. Why so? As far as I understand, decomposition can be applied to a broader range of matrices (we don't need a matrix to be positive-definite) and for the same computational price.",\mathbf L\mathbf D\mathbf L^\top \mathbf L\mathbf D\mathbf L^\top,"['matrices', 'numerical-linear-algebra']"
90,"How do I logically/conceptually approach this question in regards to Matrices, their Identities, and Partitions","How do I logically/conceptually approach this question in regards to Matrices, their Identities, and Partitions",,"I have linked to the question I am not looking for a direct answer, rather I need help getting started. We are focusing on Matrices and their uses/properties. I think I have a good idea of Matrix Identities, but the wording kind of throws me off with this question. I have dyscalculia and that may be throwing me off with the wording. I kind of want a conceptual step by step explanation of what I need to do with this problem so I can grasp its meaning/importance. I want to be able to figure it out on my own. Thanks so much in advance!","I have linked to the question I am not looking for a direct answer, rather I need help getting started. We are focusing on Matrices and their uses/properties. I think I have a good idea of Matrix Identities, but the wording kind of throws me off with this question. I have dyscalculia and that may be throwing me off with the wording. I kind of want a conceptual step by step explanation of what I need to do with this problem so I can grasp its meaning/importance. I want to be able to figure it out on my own. Thanks so much in advance!",,"['linear-algebra', 'matrices', 'matrix-equations']"
91,Degree of determinant of a matrix,Degree of determinant of a matrix,,"I want to show that the determinant of the following matrix, $f(t)$, has a degree which is less or equal than $1$. $$\left[ \begin{matrix}  c_1-t &   a-t &   a-t & ... &   a-t \\    b-t & c_2-t &   a-t & ... &   a-t \\    b-t &   b-t & c_3-t & ... &   a-t \\  \vdots & \vdots & \vdots &\ddots &\vdots \\         b-t &   b-t &   b-t & ... & c_n-t \\ \end{matrix} \right]$$ where $a,b,c_1,c_2, ... , c_n \in \mathbb{C} $ I've tried using induction on $n$ but this has led me nowhere. I'd appreciate some help.","I want to show that the determinant of the following matrix, $f(t)$, has a degree which is less or equal than $1$. $$\left[ \begin{matrix}  c_1-t &   a-t &   a-t & ... &   a-t \\    b-t & c_2-t &   a-t & ... &   a-t \\    b-t &   b-t & c_3-t & ... &   a-t \\  \vdots & \vdots & \vdots &\ddots &\vdots \\         b-t &   b-t &   b-t & ... & c_n-t \\ \end{matrix} \right]$$ where $a,b,c_1,c_2, ... , c_n \in \mathbb{C} $ I've tried using induction on $n$ but this has led me nowhere. I'd appreciate some help.",,['matrices']
92,Rate of decline of rank of matrix powers,Rate of decline of rank of matrix powers,,"Questions such as this one ( What is the limit of the rank of the power of a matrix? ) address the limit of the rank of matrix powers, but do not mention at what rate this limit is approached.  Is there a closed form expression for this rate?  If not in general, then for certain classes of matrices (e.g. Toeplitz)?","Questions such as this one ( What is the limit of the rank of the power of a matrix? ) address the limit of the rank of matrix powers, but do not mention at what rate this limit is approached.  Is there a closed form expression for this rate?  If not in general, then for certain classes of matrices (e.g. Toeplitz)?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
93,Minimal polynomial of a matrix does not check Hamilton-Cayley theorem,Minimal polynomial of a matrix does not check Hamilton-Cayley theorem,,"Hi there I have to calculate the minimal polynomial for the matrix  $$ \begin{pmatrix} 2 & 0 & 0 &0 \\  1 &  3& 2 & 1\\  0 &  -1& 0 &-1 \\   -1& 0 & 0 & 2 \end{pmatrix}$$ In oreder to do that I tried to put its characteristic matrix in the canonical diagonal form, and I got the matrix $$\begin{pmatrix} 1 & 0 & 0 &0 \\  0 &  1& 0 & 0\\  0 &  0& -x^2+3x-2 &0\\   0& 0 & 0 & (x-2)(2-x) \end{pmatrix}$$. From here I see that the minimal polynomial is $ \mu_A(x) = (x-2)(2-x)$, but the problem is that this minimal polynomial dosen't check the Hamilton-Cayley theorem(i.e. $\mu_A(A) = 0$).","Hi there I have to calculate the minimal polynomial for the matrix  $$ \begin{pmatrix} 2 & 0 & 0 &0 \\  1 &  3& 2 & 1\\  0 &  -1& 0 &-1 \\   -1& 0 & 0 & 2 \end{pmatrix}$$ In oreder to do that I tried to put its characteristic matrix in the canonical diagonal form, and I got the matrix $$\begin{pmatrix} 1 & 0 & 0 &0 \\  0 &  1& 0 & 0\\  0 &  0& -x^2+3x-2 &0\\   0& 0 & 0 & (x-2)(2-x) \end{pmatrix}$$. From here I see that the minimal polynomial is $ \mu_A(x) = (x-2)(2-x)$, but the problem is that this minimal polynomial dosen't check the Hamilton-Cayley theorem(i.e. $\mu_A(A) = 0$).",,"['linear-algebra', 'matrices', 'minimal-polynomials', 'cayley-hamilton']"
94,Nullspace of a Matrix and Linear Transformation Matrix,Nullspace of a Matrix and Linear Transformation Matrix,,"I'm having some issues to answer the following question: Consider the subspace of S of $\mathbb{R}$ generated by columns of the matrix A $$         \begin{pmatrix}         1 & 1 & 1 \\         1 & 2 & 3 \\         2 & 3 & 4 \\         \end{pmatrix} $$ Find the real numbers a, b, c for wich  S = {(x, y, z) âˆˆ $\mathbb{R^3}$ : ax + by + cz = 0. My tentative to solve the question was put the matrix in the echelon form and then solving the equation A.x = 0 Is this the right approach?","I'm having some issues to answer the following question: Consider the subspace of S of $\mathbb{R}$ generated by columns of the matrix A $$         \begin{pmatrix}         1 & 1 & 1 \\         1 & 2 & 3 \\         2 & 3 & 4 \\         \end{pmatrix} $$ Find the real numbers a, b, c for wich  S = {(x, y, z) âˆˆ $\mathbb{R^3}$ : ax + by + cz = 0. My tentative to solve the question was put the matrix in the echelon form and then solving the equation A.x = 0 Is this the right approach?",,"['linear-algebra', 'matrices', 'linear-transformations']"
95,Product of banded matrices,Product of banded matrices,,How can one show that the product of two banded matrices is a banded matrix with upper and lower bandwidths equal to the sum of the upper and lower bandwidths (respectively) of the multiplicands ?  Thank you,How can one show that the product of two banded matrices is a banded matrix with upper and lower bandwidths equal to the sum of the upper and lower bandwidths (respectively) of the multiplicands ?  Thank you,,"['linear-algebra', 'matrices', 'numerical-methods']"
96,Which one is the correct way to compute the change of basis matrix?,Which one is the correct way to compute the change of basis matrix?,,"I was looking for a way to compute the change of basis matrix (given the old basis and the new basis), but I found two methods that lead to different results, and I can't understand which one is correct. Method 1: $$  B = \left\{ \begin{pmatrix} 1 \\ 2  \end{pmatrix} ,  \begin{pmatrix} 3 \\ 4  \end{pmatrix}  \right\} ~ , ~  D= \left\{ \begin{pmatrix} 1 \\ 4  \end{pmatrix} ,  \begin{pmatrix} 2 \\ 3  \end{pmatrix} \right\}  $$ The vectors in $D$ are expressed as a linear combination of the ones in $B$, and then the coefficients are used to construct the change of basis matrix $S$, i.e.: $$ \begin{pmatrix}1 \\ 4 \end{pmatrix} = s_{11}\cdot \begin{pmatrix}1 \\ 2 \end{pmatrix} + s_{12} \cdot \begin{pmatrix}3 \\ 4 \end{pmatrix} \ \ \Rightarrow s_{11}=4 \ \ \text{and}  \ \ s_{12}=-1  \\ \begin{pmatrix}2 \\ 3 \end{pmatrix} = s_{21}\cdot \begin{pmatrix}1 \\ 2 \end{pmatrix} + s_{22} \cdot \begin{pmatrix}3 \\ 4 \end{pmatrix} \Rightarrow s_{21}=\frac{1}{2} \ \ \text{and}  \ \ s_{22}=\frac{1}{2}  \\ \Rightarrow S =\begin{pmatrix}4 & -1 \\ \frac{1}{2} & \frac{1}{2} \end{pmatrix} $$ Method 2: $$\begin{pmatrix}s_{11} & s_{12} \\ s_{21} & s_{22} \end{pmatrix} \cdot \begin{pmatrix}1 & 3 \\ 2 & 4 \end{pmatrix} = \begin{pmatrix}1 & 2 \\ 4 & 3 \end{pmatrix} \\ \Rightarrow s_{11}=0 \ \ ;\ \ s_{12}= \frac{1}{2} \ \ ; \ \ s_{21}= -5 \ \ ; \ \ s_{22}=\frac{9}{2}\\ \Rightarrow S =\begin{pmatrix}0 & \frac{1}{2} \\ -5 & \frac{9}{2} \end{pmatrix} $$","I was looking for a way to compute the change of basis matrix (given the old basis and the new basis), but I found two methods that lead to different results, and I can't understand which one is correct. Method 1: $$  B = \left\{ \begin{pmatrix} 1 \\ 2  \end{pmatrix} ,  \begin{pmatrix} 3 \\ 4  \end{pmatrix}  \right\} ~ , ~  D= \left\{ \begin{pmatrix} 1 \\ 4  \end{pmatrix} ,  \begin{pmatrix} 2 \\ 3  \end{pmatrix} \right\}  $$ The vectors in $D$ are expressed as a linear combination of the ones in $B$, and then the coefficients are used to construct the change of basis matrix $S$, i.e.: $$ \begin{pmatrix}1 \\ 4 \end{pmatrix} = s_{11}\cdot \begin{pmatrix}1 \\ 2 \end{pmatrix} + s_{12} \cdot \begin{pmatrix}3 \\ 4 \end{pmatrix} \ \ \Rightarrow s_{11}=4 \ \ \text{and}  \ \ s_{12}=-1  \\ \begin{pmatrix}2 \\ 3 \end{pmatrix} = s_{21}\cdot \begin{pmatrix}1 \\ 2 \end{pmatrix} + s_{22} \cdot \begin{pmatrix}3 \\ 4 \end{pmatrix} \Rightarrow s_{21}=\frac{1}{2} \ \ \text{and}  \ \ s_{22}=\frac{1}{2}  \\ \Rightarrow S =\begin{pmatrix}4 & -1 \\ \frac{1}{2} & \frac{1}{2} \end{pmatrix} $$ Method 2: $$\begin{pmatrix}s_{11} & s_{12} \\ s_{21} & s_{22} \end{pmatrix} \cdot \begin{pmatrix}1 & 3 \\ 2 & 4 \end{pmatrix} = \begin{pmatrix}1 & 2 \\ 4 & 3 \end{pmatrix} \\ \Rightarrow s_{11}=0 \ \ ;\ \ s_{12}= \frac{1}{2} \ \ ; \ \ s_{21}= -5 \ \ ; \ \ s_{22}=\frac{9}{2}\\ \Rightarrow S =\begin{pmatrix}0 & \frac{1}{2} \\ -5 & \frac{9}{2} \end{pmatrix} $$",,"['linear-algebra', 'matrices', 'change-of-basis']"
97,Invertible = nonzero det?,Invertible = nonzero det?,,"Let $A$, $B$ be $n\times n$ complex matrices and $I$ be the $n\times n$ identity matrix. Is $\left(\begin{array}{cc}A&I\\I&B\end{array}\right)$ being invertible the same as $\det(AB-I)\ne 0$? If not, what is the right condition for this $2n\times 2n$ matrix to be invertible? Thank you.","Let $A$, $B$ be $n\times n$ complex matrices and $I$ be the $n\times n$ identity matrix. Is $\left(\begin{array}{cc}A&I\\I&B\end{array}\right)$ being invertible the same as $\det(AB-I)\ne 0$? If not, what is the right condition for this $2n\times 2n$ matrix to be invertible? Thank you.",,"['linear-algebra', 'matrices', 'determinant', 'inverse']"
98,Given a matrix of the form $A=a\otimes b$ then $|A|_{nm}=|a|_{n} |b|_{m}$,Given a matrix of the form  then,A=a\otimes b |A|_{nm}=|a|_{n} |b|_{m},"I consider this matrix in $\mathbb{M}^{m\times n}$ $$A=a\otimes b,$$ with $b\in \mathbb{R}^m$ and $a\in\mathbb{R}^n$. I would like to prove that $|A|_{nm}=|a|_{n} |b|_{m}$, where $|.|_{p}$ is the module on $\mathbb{R}^p$. Is there a simple way to prove this? Should I use some other matrix norms to prove that? Thanks for the help!","I consider this matrix in $\mathbb{M}^{m\times n}$ $$A=a\otimes b,$$ with $b\in \mathbb{R}^m$ and $a\in\mathbb{R}^n$. I would like to prove that $|A|_{nm}=|a|_{n} |b|_{m}$, where $|.|_{p}$ is the module on $\mathbb{R}^p$. Is there a simple way to prove this? Should I use some other matrix norms to prove that? Thanks for the help!",,"['matrices', 'normed-spaces', 'tensor-products']"
99,Proving a matrix rank equality,Proving a matrix rank equality,,In my studies of the Schur complement within matrix theory I have come across this problem which seems tough: Let us consider the block matrix $ A = \left( \begin{array}{ccc} B & C \\ D & E \end{array} \right)  $ where $ E $ is an invertible principal square submatrix of $ A $ and we are asked to prove the following equality of ranks   $\operatorname{rank}\left( \begin{array}{ccc} CE^{-1}D & C \\ D & E \end{array} \right) = \operatorname{rank}(E) $ using the Schur complement I have thought about using the identity $\operatorname{rank}(A)=\operatorname{rank}(E)+\operatorname{rank}(A/E) $ and then using the definition of the Schur complement in this case $ A/E = B-CE^{-1}D $ but I have no idea how to show equality of ranks in this case. I appreciate all help.,In my studies of the Schur complement within matrix theory I have come across this problem which seems tough: Let us consider the block matrix $ A = \left( \begin{array}{ccc} B & C \\ D & E \end{array} \right)  $ where $ E $ is an invertible principal square submatrix of $ A $ and we are asked to prove the following equality of ranks   $\operatorname{rank}\left( \begin{array}{ccc} CE^{-1}D & C \\ D & E \end{array} \right) = \operatorname{rank}(E) $ using the Schur complement I have thought about using the identity $\operatorname{rank}(A)=\operatorname{rank}(E)+\operatorname{rank}(A/E) $ and then using the definition of the Schur complement in this case $ A/E = B-CE^{-1}D $ but I have no idea how to show equality of ranks in this case. I appreciate all help.,,"['linear-algebra', 'matrices', 'matrix-rank', 'block-matrices', 'schur-complement']"
