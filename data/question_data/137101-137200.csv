,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"System of differential equations $x'=f(y-x),\, y'=f(x-y)$",System of differential equations,"x'=f(y-x),\, y'=f(x-y)","I have a problem regarding differential equations, $f$ is increasing. $x,y$ are dependent on $t$  $$ f(0)=0 $$ $$x'=f(y-x)$$ $$y'=f(x-y)$$ $$x(0)=1$$ $$y(0)=0$$ prove that when $x,y$ are the solutions of this equation then $$ \lim_{t \to \infty} x = \lim_{t \to\infty}y $$ So far I have managed to see that $x'(0)=f(-1)$ and $y'(0)=f(1)$ which implies that $x'(0)<0$ and $y'(0)>0$ and basically $x'(t)<0$, $y'(t)>0$ as $x>y$ so as $f$ is increasing and $x$ is declining with $t$ and $y$ is increasing with $t$, the derivatives of $x$ and $y$ will be declining and increasing respectivly and they will have the boundary. Can anyone help from this on?","I have a problem regarding differential equations, $f$ is increasing. $x,y$ are dependent on $t$  $$ f(0)=0 $$ $$x'=f(y-x)$$ $$y'=f(x-y)$$ $$x(0)=1$$ $$y(0)=0$$ prove that when $x,y$ are the solutions of this equation then $$ \lim_{t \to \infty} x = \lim_{t \to\infty}y $$ So far I have managed to see that $x'(0)=f(-1)$ and $y'(0)=f(1)$ which implies that $x'(0)<0$ and $y'(0)>0$ and basically $x'(t)<0$, $y'(t)>0$ as $x>y$ so as $f$ is increasing and $x$ is declining with $t$ and $y$ is increasing with $t$, the derivatives of $x$ and $y$ will be declining and increasing respectivly and they will have the boundary. Can anyone help from this on?",,"['ordinary-differential-equations', 'differential']"
1,Using Homogeneous Coordinates in Differential Equations,Using Homogeneous Coordinates in Differential Equations,,"Recently, I asked a question on the Mathematica Stack Exchange website regarding the use of homogeneous coordinates in differential equations. The question is about extending the interval of existence in differential equations using homogeneous coordinates. A prototypical example is the initial value problem for the tangent function. $$\begin{cases} y'=1+y^2, \\ y(0)=0,  \end{cases}$$ with the corresponding interval of existence being $(-\pi/2,\pi/2)$. Identifying $y$ with a coordinate on the projective line, via $y=y_1/y_2$, results in the underdetermined system $$\begin{cases} y_1'y_2-y_1y_2'=y_2^2 \left[1+ \left(\dfrac{y_1}{y_2} \right)^2 \right] ,\\ \dfrac{y_1(0)}{y_2(0)}=0. \end{cases} $$ Adding the normalization condition $y_1y_1'+y_2y_2'=0$ (which is a consequence of fixing $y_1^2+y_2^2$), and using the initial conditions $y_1(0)=0,y_2(0)=1$ results in a solvable system, with global solutions  $y_1(t)=\sin(t),y_2(t)=\cos(t)$. Thus it can be said that the interval of existence now is the entire real line! I was trying to see how far I can extend this method. In the link above, for instance, I showed that it works (at least numerically) for the $\sec$-$\tan$ system $$\begin{cases} u'=uv \\ v'=u^2, \\ u(0)=1, \\ v(0)=0. \end{cases} $$ However, with other systems such as the first Painlevé equation $$\begin{cases} u'=v, \\ v'=6u^2+t, \end{cases} $$ the numerical integrator appears to get stuck at the first pole. My questions: Is there a way to employ homogeneous coordinates in order to extend the interval of existence in the case of the first Painlevé equation? More generally, what about systems $ \dot{\mathbf{x}}=\mathbf{f}(t,\mathbf{x})$ with the Painlevé property (where all spontaneous singularities are poles)? If so, I'd greatly appreciate some details. I'm not asking for the code here, just the appropriate mathematical formulation. Thank you!","Recently, I asked a question on the Mathematica Stack Exchange website regarding the use of homogeneous coordinates in differential equations. The question is about extending the interval of existence in differential equations using homogeneous coordinates. A prototypical example is the initial value problem for the tangent function. $$\begin{cases} y'=1+y^2, \\ y(0)=0,  \end{cases}$$ with the corresponding interval of existence being $(-\pi/2,\pi/2)$. Identifying $y$ with a coordinate on the projective line, via $y=y_1/y_2$, results in the underdetermined system $$\begin{cases} y_1'y_2-y_1y_2'=y_2^2 \left[1+ \left(\dfrac{y_1}{y_2} \right)^2 \right] ,\\ \dfrac{y_1(0)}{y_2(0)}=0. \end{cases} $$ Adding the normalization condition $y_1y_1'+y_2y_2'=0$ (which is a consequence of fixing $y_1^2+y_2^2$), and using the initial conditions $y_1(0)=0,y_2(0)=1$ results in a solvable system, with global solutions  $y_1(t)=\sin(t),y_2(t)=\cos(t)$. Thus it can be said that the interval of existence now is the entire real line! I was trying to see how far I can extend this method. In the link above, for instance, I showed that it works (at least numerically) for the $\sec$-$\tan$ system $$\begin{cases} u'=uv \\ v'=u^2, \\ u(0)=1, \\ v(0)=0. \end{cases} $$ However, with other systems such as the first Painlevé equation $$\begin{cases} u'=v, \\ v'=6u^2+t, \end{cases} $$ the numerical integrator appears to get stuck at the first pole. My questions: Is there a way to employ homogeneous coordinates in order to extend the interval of existence in the case of the first Painlevé equation? More generally, what about systems $ \dot{\mathbf{x}}=\mathbf{f}(t,\mathbf{x})$ with the Painlevé property (where all spontaneous singularities are poles)? If so, I'd greatly appreciate some details. I'm not asking for the code here, just the appropriate mathematical formulation. Thank you!",,"['ordinary-differential-equations', 'numerical-methods', 'singularity']"
2,periodic solutions with constant period,periodic solutions with constant period,,"Recently I am facing a problem which leads to the following type of ODE $$x'' + x + f(x') = 0.$$ It is obvious that when $f$ is a constant valued function, the solutions are all periodic and have a common period $2\pi$. I want to know if there is a nonconstant smooth function $f$, such that (1) the solutions are all periodic, and (2) they share a common period $T$. I have obtained many examples which satisfy the condition (1), for example $f(x)=(\sin x)^n$ for even integer $n>0$.  But none of them satisfy (2).","Recently I am facing a problem which leads to the following type of ODE $$x'' + x + f(x') = 0.$$ It is obvious that when $f$ is a constant valued function, the solutions are all periodic and have a common period $2\pi$. I want to know if there is a nonconstant smooth function $f$, such that (1) the solutions are all periodic, and (2) they share a common period $T$. I have obtained many examples which satisfy the condition (1), for example $f(x)=(\sin x)^n$ for even integer $n>0$.  But none of them satisfy (2).",,"['ordinary-differential-equations', 'dynamical-systems']"
3,Variational Equation in Perturbation Theory Chapter of Arnold *Geometric Methods in ODE*,Variational Equation in Perturbation Theory Chapter of Arnold *Geometric Methods in ODE*,,"In the preface to Chapter 4 ``Perturbation Theory"" in Arnold's book Geometric Methods in the Theory of Ordinary Differential Equations available for preview here https://www.springer.com/gp/book/9780387966496 he writes: ``If the size of the perturbation is characterized by a small parameter $\varepsilon$, then the effect of perturbations over time of order 1 leads to a change of order $\varepsilon$ of the solution.  This change can be calculated approximately by solving a variational equation along the unperturbed solution."" He then goes on to discuss the asymptotic methods necessary to discuss the validity of approximation at long times, but I am left wondering: Question: What is the ``variational equation along the unperturbed solution'' referred to here? I would guess that we are finding for finite time the next-order correction term in $\varepsilon$ to the leading order behavior (given by the solution of the unforced equation) but I am not sure about this and also not sure how this leads to a variational problem.  Any help would be greatly appreciated!!","In the preface to Chapter 4 ``Perturbation Theory"" in Arnold's book Geometric Methods in the Theory of Ordinary Differential Equations available for preview here https://www.springer.com/gp/book/9780387966496 he writes: ``If the size of the perturbation is characterized by a small parameter $\varepsilon$, then the effect of perturbations over time of order 1 leads to a change of order $\varepsilon$ of the solution.  This change can be calculated approximately by solving a variational equation along the unperturbed solution."" He then goes on to discuss the asymptotic methods necessary to discuss the validity of approximation at long times, but I am left wondering: Question: What is the ``variational equation along the unperturbed solution'' referred to here? I would guess that we are finding for finite time the next-order correction term in $\varepsilon$ to the leading order behavior (given by the solution of the unforced equation) but I am not sure about this and also not sure how this leads to a variational problem.  Any help would be greatly appreciated!!",,"['ordinary-differential-equations', 'perturbation-theory', 'variational-analysis']"
4,Solving the First-Order Homogeneous ODE $y'-\frac{1}{3x}y=\frac{x}{3y}$,Solving the First-Order Homogeneous ODE,y'-\frac{1}{3x}y=\frac{x}{3y},"I wish to solve  $$y'-\frac{1}{3x}y=\frac{x}{3y}$$ At first, rearrange and simplify to  $$y'=\frac{x^2+y^2}{3xy}$$ which clearly indicates that it is a homogeneous differential equation. Apply the substitution $y=ux\rightarrow y'=u'x+u$ which gives $$u'x=\frac{1-2u^2}{3u}$$ and by integrating we get that $$-\frac{3}{4}\ln\left| 2u^2-1\right| = \ln \left| x\right|+c$$ $$e^{-3/4\ln\left| 2u^2-1\right|} = e^{\ln \left| x\right|+c}$$  $$\left| 2u^2-1\right|^{-3/4}=C \left |x \right|$$ $$\left| 2u^2-1\right|=(C \left |x \right|)^{-4/3}$$ $$\left| 2u^2-1\right|=C' \left |x ^{-4/3}\right|$$ $$\left| 2u^2-1\right|=C' x ^{-4/3}$$ Now, if I choose $ 2u^2-1=C' x ^{-4/3}$ and solve for $u$ and then for $y$, I am getting the expected $y(x) = \pm\sqrt{x^2/2+C' x^{2/3}}$. But, if I choose $ 1-2u^2=C' x ^{-4/3}$, I am getting $y(x) = \pm\sqrt{x^2/2-C' x^{2/3}}$ which are not listed in the answers at least at Alpha. How to choose between the two? Note that $C'$ is clearly positive.","I wish to solve  $$y'-\frac{1}{3x}y=\frac{x}{3y}$$ At first, rearrange and simplify to  $$y'=\frac{x^2+y^2}{3xy}$$ which clearly indicates that it is a homogeneous differential equation. Apply the substitution $y=ux\rightarrow y'=u'x+u$ which gives $$u'x=\frac{1-2u^2}{3u}$$ and by integrating we get that $$-\frac{3}{4}\ln\left| 2u^2-1\right| = \ln \left| x\right|+c$$ $$e^{-3/4\ln\left| 2u^2-1\right|} = e^{\ln \left| x\right|+c}$$  $$\left| 2u^2-1\right|^{-3/4}=C \left |x \right|$$ $$\left| 2u^2-1\right|=(C \left |x \right|)^{-4/3}$$ $$\left| 2u^2-1\right|=C' \left |x ^{-4/3}\right|$$ $$\left| 2u^2-1\right|=C' x ^{-4/3}$$ Now, if I choose $ 2u^2-1=C' x ^{-4/3}$ and solve for $u$ and then for $y$, I am getting the expected $y(x) = \pm\sqrt{x^2/2+C' x^{2/3}}$. But, if I choose $ 1-2u^2=C' x ^{-4/3}$, I am getting $y(x) = \pm\sqrt{x^2/2-C' x^{2/3}}$ which are not listed in the answers at least at Alpha. How to choose between the two? Note that $C'$ is clearly positive.",,"['ordinary-differential-equations', 'absolute-value', 'homogeneous-equation']"
5,Computing the discrete-time difference equation when the system matrix is non-invertible,Computing the discrete-time difference equation when the system matrix is non-invertible,,"If we have a continuous time equation, $$ \dot{x}(t) = A x(t) + B u(t)$$ where $A \in \mathbb{R}^{n \times n}, x \in \mathbb{R}^{n\times1}, B \in \mathbb{R}^{n \times m}, u \in \mathbb{R}^{m \times 1}$, the analytical solution is obtained as $$x(t) = e^{A t} x(0) + \int_0^t e^{A(t-\tau)}B u(\tau)\, d\tau$$ Converting this to discrete-time, with sample-time $T_s$, and assuming that all eigen-values of $A$ are real & distinct and $u$ remains constant within each sample interval, $k$ to $k+1$, we get the following difference equation, $$x[k+1] = e^{A T_s} x[k] + \left[\int_0^t e^{A\tau}B \, d\tau\right] u[k]$$ Which can be written as $$x[k+1] = A_d  x[k] + B_d u[k]$$ When the original matrix $A$ is invertible, the integral term corresponding to $B_d$ can be written as $$B_d = A^{-1}(A_d - I)$$ The question is, What about the case when A is non-invertible ?  In particular, if my $A=\begin{bmatrix}a_1 & 0 & 0 \\ 0 & a_2 & 0 \\ 0 & 0 & 0  \end{bmatrix}$ and $B=\begin{bmatrix}b_1 \\ b_2 \\ b_3 \end{bmatrix}$. How do I obtain the corresponding $B_d$?","If we have a continuous time equation, $$ \dot{x}(t) = A x(t) + B u(t)$$ where $A \in \mathbb{R}^{n \times n}, x \in \mathbb{R}^{n\times1}, B \in \mathbb{R}^{n \times m}, u \in \mathbb{R}^{m \times 1}$, the analytical solution is obtained as $$x(t) = e^{A t} x(0) + \int_0^t e^{A(t-\tau)}B u(\tau)\, d\tau$$ Converting this to discrete-time, with sample-time $T_s$, and assuming that all eigen-values of $A$ are real & distinct and $u$ remains constant within each sample interval, $k$ to $k+1$, we get the following difference equation, $$x[k+1] = e^{A T_s} x[k] + \left[\int_0^t e^{A\tau}B \, d\tau\right] u[k]$$ Which can be written as $$x[k+1] = A_d  x[k] + B_d u[k]$$ When the original matrix $A$ is invertible, the integral term corresponding to $B_d$ can be written as $$B_d = A^{-1}(A_d - I)$$ The question is, What about the case when A is non-invertible ?  In particular, if my $A=\begin{bmatrix}a_1 & 0 & 0 \\ 0 & a_2 & 0 \\ 0 & 0 & 0  \end{bmatrix}$ and $B=\begin{bmatrix}b_1 \\ b_2 \\ b_3 \end{bmatrix}$. How do I obtain the corresponding $B_d$?",,"['ordinary-differential-equations', 'dynamical-systems', 'matrix-equations']"
6,"Apostol's Calculus Vol 2, Chapter 10.20, Exercise 7","Apostol's Calculus Vol 2, Chapter 10.20, Exercise 7",,"Let $\mu(x,y)$ be an integrating factor of the differential equation $P(x,y)\,dx+Q(x,y)\,dy=0$. I already showed that we have $$\frac{\partial P}{\partial y}-\frac{\partial Q}{\partial x}=Q\frac{\partial}{\partial x}  \log|\mu| - P\frac{\partial}{\partial y} \log|\mu|.$$ Now I have to deduce that if $(\partial P/\partial y-\partial Q/\partial x)/Q$ is a function of $x$ alone, say $f(x)$, then the function $e^{\int f(x) \, dx}$ is an integrating factor of the equation. I couldn't find the way to see this. The previous exercise shows that the equation $y'+P(x)y=Q(x)$ has the integrating factor $e^{\int P(x)\,dx}$, but I don't know if I can use it here.","Let $\mu(x,y)$ be an integrating factor of the differential equation $P(x,y)\,dx+Q(x,y)\,dy=0$. I already showed that we have $$\frac{\partial P}{\partial y}-\frac{\partial Q}{\partial x}=Q\frac{\partial}{\partial x}  \log|\mu| - P\frac{\partial}{\partial y} \log|\mu|.$$ Now I have to deduce that if $(\partial P/\partial y-\partial Q/\partial x)/Q$ is a function of $x$ alone, say $f(x)$, then the function $e^{\int f(x) \, dx}$ is an integrating factor of the equation. I couldn't find the way to see this. The previous exercise shows that the equation $y'+P(x)y=Q(x)$ has the integrating factor $e^{\int P(x)\,dx}$, but I don't know if I can use it here.",,"['ordinary-differential-equations', 'multivariable-calculus', 'integrating-factor']"
7,Well posedness of PDEs of the form $u_{tt} + \left[ P ( \partial_x)\right]^2u=0$,Well posedness of PDEs of the form,u_{tt} + \left[ P ( \partial_x)\right]^2u=0,"Let $P$ be a polynomial with real coefficients. I want to find the necessary&sufficient conditions on $P$ in order for the problem  $$u_{tt} + \left[ P ( \partial_x)\right]^2u=0$$ with $u (0,x) = f(x)$ $ u_t(0,x) = 0$ to be well-posed for every positive period of the data $f$. I am tempted to decompose the PDE to the form  $$P_1(\partial_t,\partial_x) \cdot \overline{P_1(\partial_t,\partial_x)}u=0$$ where $P_1(\partial_t,\partial_x) = \partial_t+iP(\partial_x)$ and $\overline{P_1(\partial_t,\partial_x)} =\partial_t - iP(\partial_x) $ is it's complex conjugate. But I don't know how to take it from there...","Let $P$ be a polynomial with real coefficients. I want to find the necessary&sufficient conditions on $P$ in order for the problem  $$u_{tt} + \left[ P ( \partial_x)\right]^2u=0$$ with $u (0,x) = f(x)$ $ u_t(0,x) = 0$ to be well-posed for every positive period of the data $f$. I am tempted to decompose the PDE to the form  $$P_1(\partial_t,\partial_x) \cdot \overline{P_1(\partial_t,\partial_x)}u=0$$ where $P_1(\partial_t,\partial_x) = \partial_t+iP(\partial_x)$ and $\overline{P_1(\partial_t,\partial_x)} =\partial_t - iP(\partial_x) $ is it's complex conjugate. But I don't know how to take it from there...",,"['real-analysis', 'ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'fourier-series']"
8,General solutions to first-order differential equations with disitrubutional coefficients,General solutions to first-order differential equations with disitrubutional coefficients,,"Consider the first-order differential equation $$\dot{x}+p(t)x=q(t).$$ This can be generally solved using an integrating factor $$a(t)=\exp\left(\int p(t)dt\right)$$ and the solution is $$x(t)=\frac{1}{a(t)}\left(\int a(t)q(t)dt+C\right)$$ (I'm going to skip over why this works - I can more information if people don't recognize this). The point is that this is a general method - it should work for any smooth functions $p(t)$ and $q(t)$. What I'm interested in is if a similar thing is true for the same differential equation, $$\dot{x}+P(t)x=Q(t),$$ but where $P(t)$ and $Q(t)$ are distributions, that is, linear functions on test functions. One wouldn't normally think techniques on smooth functions would extend to distributions, but since the final solution basically just depends on integrals of the distributions, I would be optimistic that it might work for all distributions. Are there any results in this area? I do have a specific case in mind, in which $P$ and $Q$ are made up of delta functions and it's derivatives, but the question is more general, and I would love a specific reference (which is hopefully readable :-) )","Consider the first-order differential equation $$\dot{x}+p(t)x=q(t).$$ This can be generally solved using an integrating factor $$a(t)=\exp\left(\int p(t)dt\right)$$ and the solution is $$x(t)=\frac{1}{a(t)}\left(\int a(t)q(t)dt+C\right)$$ (I'm going to skip over why this works - I can more information if people don't recognize this). The point is that this is a general method - it should work for any smooth functions $p(t)$ and $q(t)$. What I'm interested in is if a similar thing is true for the same differential equation, $$\dot{x}+P(t)x=Q(t),$$ but where $P(t)$ and $Q(t)$ are distributions, that is, linear functions on test functions. One wouldn't normally think techniques on smooth functions would extend to distributions, but since the final solution basically just depends on integrals of the distributions, I would be optimistic that it might work for all distributions. Are there any results in this area? I do have a specific case in mind, in which $P$ and $Q$ are made up of delta functions and it's derivatives, but the question is more general, and I would love a specific reference (which is hopefully readable :-) )",,"['ordinary-differential-equations', 'distribution-theory', 'dirac-delta']"
9,Euler method and bisection method,Euler method and bisection method,,"I'd like to solve the equation  $$ \phi''(x) = \lambda \sin (\phi(x)) $$ where $x \in (0,L)$, $\phi'(0) = 0$, $\phi'(L) = 0$. Let $ \psi = \phi'$ and $$ \phi'(x) - \psi(x) = 0$$ $$ \psi'(x) - \lambda \sin (\phi(x)) = 0$$ for $x \in (0,L)$ and $\psi(0) = 0, \phi(0) = \phi_0.$ Can anybody help me to find $\phi_0 $ numerically such that $\phi'(L) = 0$ holds? I received the advice to compute the solution of $(\phi, \psi)$ with the explicit Euler method for $\phi_0 = 1.5$ and $\phi_0 = 3$ and to use the method of bisection to compute $\phi_0$. In addition, I received the following values: Number of steps (bisection): $2^6$ Length of steps (Euler): L/100 L = 5 $\lambda$ = 2 Thanks for any help! EDIT: In the meantime, I coded a bit. I added your functions as well as an implementation of Euler and bisection. See what I did so far. Now my problem is to connect your functions with my functions. Can you please help a bit? (For example, it's not clear to me where to define the function, and it's not clear to me when calling your functions ""model"" and ""omegaL""...) funtion x = eubisect()     u = bisection(f, a, b, N, eps_step, eps_abs)  function dotu = model(t,u)     lambda = 2;     dotu = [ u(2); lambda*sin(u(1)) ] end  function omegaL= f(phi0)     L = 5;     N = 100;     t,u = Euler(model, 0, L, N, [phi0,0])     omegaL = u(end,2) end  function [t, y] = Euler(f, a, b, N, y0)     clear t % Clears old time steps and     clear y % y values from previous runs     %a=0; % Initial time     %b=1; % Final time     %N=10; % Number of time steps     %y0=0; % Initial value y(a)     h=(b-a)/N; % Time step     t(1)=a;     y(1)=y0;     for n=1:N % For loop, sets next t,y values         t(n+1)=t(n)+h;         y(n+1)=y(n)+h*f(t(n),y(n)); % Calls the function f(t,y)=dy/dt     end     %plot(t,y)     %title(['Euler Method using N=',num2str(N),' steps']) end   function [ r ] = bisection( f, a, b, N, eps_step, eps_abs )     % Check that that neither end-point is a root     % and if f(a) and f(b) have the same sign, throw an exception.      if ( abs(f(a)) < eps_abs )     r = a;     return;     elseif ( abs(f(b)) < eps_abs )     r = b;     return;     elseif ( f(a) * f(b) > 0 )         error( 'f(a) and f(b) do not have opposite signs' );     end      % We will iterate N times and if a root was not     % found after N iterations, an exception will be thrown.      for k = 1:N         % Find the mid-point         c = (a + b)/2;          % Check if we found a root or whether or not         % we should continue with:         %          [a, c] if f(a) and f(c) have opposite signs, or         %          [c, b] if f(c) and f(b) have opposite signs.          if ( abs(f(c)) < eps_abs )             r = c;             return;         elseif ( f(c)*f(a) < 0 )             b = c;         else             a = c;         end          % If |b - a| < eps_step, check whether or not         %       |f(a)| < |f(b)| and |f(a)| < eps_abs and return 'a', or         %       |f(b)| < eps_abs and return 'b'.          if ( b - a < eps_step )             if ( abs( f(a) ) < abs( f(b) ) && abs( f(a) ) < eps_abs )                 r = a;                 return;             elseif ( abs( f(b) ) < eps_abs )                 r = b;                 return;             end         end     end      error( 'the method did not converge' ); end","I'd like to solve the equation  $$ \phi''(x) = \lambda \sin (\phi(x)) $$ where $x \in (0,L)$, $\phi'(0) = 0$, $\phi'(L) = 0$. Let $ \psi = \phi'$ and $$ \phi'(x) - \psi(x) = 0$$ $$ \psi'(x) - \lambda \sin (\phi(x)) = 0$$ for $x \in (0,L)$ and $\psi(0) = 0, \phi(0) = \phi_0.$ Can anybody help me to find $\phi_0 $ numerically such that $\phi'(L) = 0$ holds? I received the advice to compute the solution of $(\phi, \psi)$ with the explicit Euler method for $\phi_0 = 1.5$ and $\phi_0 = 3$ and to use the method of bisection to compute $\phi_0$. In addition, I received the following values: Number of steps (bisection): $2^6$ Length of steps (Euler): L/100 L = 5 $\lambda$ = 2 Thanks for any help! EDIT: In the meantime, I coded a bit. I added your functions as well as an implementation of Euler and bisection. See what I did so far. Now my problem is to connect your functions with my functions. Can you please help a bit? (For example, it's not clear to me where to define the function, and it's not clear to me when calling your functions ""model"" and ""omegaL""...) funtion x = eubisect()     u = bisection(f, a, b, N, eps_step, eps_abs)  function dotu = model(t,u)     lambda = 2;     dotu = [ u(2); lambda*sin(u(1)) ] end  function omegaL= f(phi0)     L = 5;     N = 100;     t,u = Euler(model, 0, L, N, [phi0,0])     omegaL = u(end,2) end  function [t, y] = Euler(f, a, b, N, y0)     clear t % Clears old time steps and     clear y % y values from previous runs     %a=0; % Initial time     %b=1; % Final time     %N=10; % Number of time steps     %y0=0; % Initial value y(a)     h=(b-a)/N; % Time step     t(1)=a;     y(1)=y0;     for n=1:N % For loop, sets next t,y values         t(n+1)=t(n)+h;         y(n+1)=y(n)+h*f(t(n),y(n)); % Calls the function f(t,y)=dy/dt     end     %plot(t,y)     %title(['Euler Method using N=',num2str(N),' steps']) end   function [ r ] = bisection( f, a, b, N, eps_step, eps_abs )     % Check that that neither end-point is a root     % and if f(a) and f(b) have the same sign, throw an exception.      if ( abs(f(a)) < eps_abs )     r = a;     return;     elseif ( abs(f(b)) < eps_abs )     r = b;     return;     elseif ( f(a) * f(b) > 0 )         error( 'f(a) and f(b) do not have opposite signs' );     end      % We will iterate N times and if a root was not     % found after N iterations, an exception will be thrown.      for k = 1:N         % Find the mid-point         c = (a + b)/2;          % Check if we found a root or whether or not         % we should continue with:         %          [a, c] if f(a) and f(c) have opposite signs, or         %          [c, b] if f(c) and f(b) have opposite signs.          if ( abs(f(c)) < eps_abs )             r = c;             return;         elseif ( f(c)*f(a) < 0 )             b = c;         else             a = c;         end          % If |b - a| < eps_step, check whether or not         %       |f(a)| < |f(b)| and |f(a)| < eps_abs and return 'a', or         %       |f(b)| < eps_abs and return 'b'.          if ( b - a < eps_step )             if ( abs( f(a) ) < abs( f(b) ) && abs( f(a) ) < eps_abs )                 r = a;                 return;             elseif ( abs( f(b) ) < eps_abs )                 r = b;                 return;             end         end     end      error( 'the method did not converge' ); end",,"['real-analysis', 'ordinary-differential-equations', 'matlab', 'bisection', 'eulers-method']"
10,Perturbation evolution of a differential equation,Perturbation evolution of a differential equation,,"Let $a$, $b$ be two real positive parameters with $a>b$, and consider the following nonlinear differential equation: \begin{align} \dot{x}_{\varepsilon}(t) = a - b\sin(x_{\varepsilon}(t))+\varepsilon, \quad x_\varepsilon(0)\in\mathbb{R}, \end{align} where $\varepsilon$ is a real constant. Let us define  $$ \Delta(t,\varepsilon):= |\cos({x}_{\varepsilon}(t))-\cos({x}_{0}(t))| $$ Clearly, note that $\Delta(t,0)\equiv 0$. Is it true that $\Delta(t,\varepsilon)$ is linearly bounded in $t\ge 0$ and $|\varepsilon|$, that is   $$  \Delta(t,\varepsilon)\le K |\varepsilon| t, $$   where $K$ being a suitable constant? [In case the answer is no, is it possible to find upper bound such that $\Delta(t,\varepsilon)\le K |\varepsilon| p(t)$ where $p(t)$ is a polynomial function of $t$?] Numerical simulations seem to confirm this claim. Any help towards a theoretical confirmation (or rejection) of this claim is more than welcome! Many thanks! An initial (incomplete) attempt. We can rewrite $\Delta(t,\varepsilon)$ as $$ \Delta(t,\varepsilon)= 2\left|\sin\left(\frac{{x}_{\varepsilon}(t)-{x}_{0}(t)}{2}\right)\sin\left(\frac{{x}_{\varepsilon}(t)+{x}_{0}(t)}{2}\right)\right|, $$ so that  $$ \Delta(t,\varepsilon)\le 2\left|\sin\left(\frac{{x}_{\varepsilon}(t)-{x}_{0}(t)}{2}\right)\right|\le |{x}_{\varepsilon}(t)-{x}_{0}(t)|. $$ So now the problem is perhaps a bit simplified (?) and boils down to find an upper bound to $|{x}_{\varepsilon}(t)-{x}_{0}(t)|$. However, I don't know how to proceed from here.","Let $a$, $b$ be two real positive parameters with $a>b$, and consider the following nonlinear differential equation: \begin{align} \dot{x}_{\varepsilon}(t) = a - b\sin(x_{\varepsilon}(t))+\varepsilon, \quad x_\varepsilon(0)\in\mathbb{R}, \end{align} where $\varepsilon$ is a real constant. Let us define  $$ \Delta(t,\varepsilon):= |\cos({x}_{\varepsilon}(t))-\cos({x}_{0}(t))| $$ Clearly, note that $\Delta(t,0)\equiv 0$. Is it true that $\Delta(t,\varepsilon)$ is linearly bounded in $t\ge 0$ and $|\varepsilon|$, that is   $$  \Delta(t,\varepsilon)\le K |\varepsilon| t, $$   where $K$ being a suitable constant? [In case the answer is no, is it possible to find upper bound such that $\Delta(t,\varepsilon)\le K |\varepsilon| p(t)$ where $p(t)$ is a polynomial function of $t$?] Numerical simulations seem to confirm this claim. Any help towards a theoretical confirmation (or rejection) of this claim is more than welcome! Many thanks! An initial (incomplete) attempt. We can rewrite $\Delta(t,\varepsilon)$ as $$ \Delta(t,\varepsilon)= 2\left|\sin\left(\frac{{x}_{\varepsilon}(t)-{x}_{0}(t)}{2}\right)\sin\left(\frac{{x}_{\varepsilon}(t)+{x}_{0}(t)}{2}\right)\right|, $$ so that  $$ \Delta(t,\varepsilon)\le 2\left|\sin\left(\frac{{x}_{\varepsilon}(t)-{x}_{0}(t)}{2}\right)\right|\le |{x}_{\varepsilon}(t)-{x}_{0}(t)|. $$ So now the problem is perhaps a bit simplified (?) and boils down to find an upper bound to $|{x}_{\varepsilon}(t)-{x}_{0}(t)|$. However, I don't know how to proceed from here.",,"['ordinary-differential-equations', 'analysis', 'inequality', 'perturbation-theory', 'stability-in-odes']"
11,"Is there a clear, universal test for a separable diferential equation?","Is there a clear, universal test for a separable diferential equation?",,"I understand that an equation of the form: $$\frac{dy}{dx} = f(x, y)$$ is separable, if $f(x, y)$ can be rewritten as $g(x)\cdot h(y)$. But is there a way to test if the equation can be separated without having to guess until you find a valid separation? If there exists no universal method, is there a way to prove that a certain equation of a specific form cannot be separated, for example,$$ y' = \frac{x+y}{x}? $$","I understand that an equation of the form: $$\frac{dy}{dx} = f(x, y)$$ is separable, if $f(x, y)$ can be rewritten as $g(x)\cdot h(y)$. But is there a way to test if the equation can be separated without having to guess until you find a valid separation? If there exists no universal method, is there a way to prove that a certain equation of a specific form cannot be separated, for example,$$ y' = \frac{x+y}{x}? $$",,['ordinary-differential-equations']
12,How to deduce general solution of DE using Mobius transform,How to deduce general solution of DE using Mobius transform,,"We are given a differential equation, say of the form $$y''(z)+\frac Azy'(z)+\frac B{z^2}y(z)=0$$ This is a differential equation with regular singular points at $0$ and $\infty$. Then we find the general solution to this equation is something like $y(z)=a_1 z^{k_1}+a_2 z^{k_2}$ (given $A$ and $B$ do not satisfy particular conditions which would give repeated roots of the indicial equation rather than distinct $k_i$'s). How can we now use a Mobius transform to find the general solution of a similar DE with regular singular points at generic point $x_1,x_2$ now? What is the general method? I can't find any similar questions on this site. An idea I had was, of course we want a Mobius transform which maps $0,\infty$ to $x_1,x_2$. This would be $$f(z)=\frac{x_2z+x_1t}{z+t}$$ where $t$ is a free parameter. The inverse Mobius transform of this is $$f^{-1}(\tilde z)=\frac{\tilde z-x_1}{x_2-\tilde z}t$$Now we could let $z=f^{-1}(\tilde z)$ in the differential equation above, and this the solution above. In this case, whenever $(x_1,x_2)=(0,\infty)$, (by choosing $t=x_2$), we get that $f^{-1}$ is the identity map, and so we have recovered the original solution. If $x_i$ are different, then the general solution is $$y(z)=a_1 \left(f^{-1}(z)\right)^{k_1}+a_2\left(f^{-1}(z)\right)^{k_2}$$ Is that correct? In particular I am unsure about the step where we ""let $t=x_2$"", and how valid that is - I'd expect that we wouldn't have needed to make a choice, and that this should work for any $t$. Did my derivation depend on the choice of $t$, and should it? These are my main concerns with my method.","We are given a differential equation, say of the form $$y''(z)+\frac Azy'(z)+\frac B{z^2}y(z)=0$$ This is a differential equation with regular singular points at $0$ and $\infty$. Then we find the general solution to this equation is something like $y(z)=a_1 z^{k_1}+a_2 z^{k_2}$ (given $A$ and $B$ do not satisfy particular conditions which would give repeated roots of the indicial equation rather than distinct $k_i$'s). How can we now use a Mobius transform to find the general solution of a similar DE with regular singular points at generic point $x_1,x_2$ now? What is the general method? I can't find any similar questions on this site. An idea I had was, of course we want a Mobius transform which maps $0,\infty$ to $x_1,x_2$. This would be $$f(z)=\frac{x_2z+x_1t}{z+t}$$ where $t$ is a free parameter. The inverse Mobius transform of this is $$f^{-1}(\tilde z)=\frac{\tilde z-x_1}{x_2-\tilde z}t$$Now we could let $z=f^{-1}(\tilde z)$ in the differential equation above, and this the solution above. In this case, whenever $(x_1,x_2)=(0,\infty)$, (by choosing $t=x_2$), we get that $f^{-1}$ is the identity map, and so we have recovered the original solution. If $x_i$ are different, then the general solution is $$y(z)=a_1 \left(f^{-1}(z)\right)^{k_1}+a_2\left(f^{-1}(z)\right)^{k_2}$$ Is that correct? In particular I am unsure about the step where we ""let $t=x_2$"", and how valid that is - I'd expect that we wouldn't have needed to make a choice, and that this should work for any $t$. Did my derivation depend on the choice of $t$, and should it? These are my main concerns with my method.",,"['complex-analysis', 'ordinary-differential-equations', 'mobius-transformation']"
13,Non-linear Integral equation,Non-linear Integral equation,,Any ideas how to solve this integral equation? $$f\left(x\right)\int_{-\infty}^{x}f\left(y\right)dy=\int_{-\infty}^{x}\int_{-\infty}^{x}\left(x-y\right)\left(x-z\right)f\left(y\right)f\left(z\right)dydz$$,Any ideas how to solve this integral equation? $$f\left(x\right)\int_{-\infty}^{x}f\left(y\right)dy=\int_{-\infty}^{x}\int_{-\infty}^{x}\left(x-y\right)\left(x-z\right)f\left(y\right)f\left(z\right)dydz$$,,"['integration', 'ordinary-differential-equations']"
14,Converting Differential Operator to Integral Equation,Converting Differential Operator to Integral Equation,,"The Green's function of following differential operator $$(\mathcal{L}y)(x)=\frac{d}{dx}\left(x\,\frac{dy}{dx}\right)-\frac{n^2}{x}\,y(x), \:0<x<1$$ with boundary conditions $$y(0)=y(1)=0$$  can be the form of  $$G(x,t)=  \begin{cases}       \dfrac{1}{2n} \left(\dfrac{x}{t}\right)^n(1-t^{2n})& x\leq t\\       \dfrac{1}{2n} \left(\dfrac{t}{x}\right)^{\!n}(1-x^{2n}) &  x>t    \end{cases}. $$ Now using this, I would like to convert the following ODE to an integral equation for nonzero number $n$ (which I got stuck in) $$x^2y''+xy'+(\lambda x^2-n^2)y=0$$ $$y(0)=y(1)=0. $$ We also can write the above equation in $\mathcal{L}y(x)+\lambda xy=0$ relavant to $y(0)=y(1)=0$, so it may be correct to say $$\int_0^1G(x,s)(\mathcal{L}y(s)+\lambda sy(s))\,ds=0$$ So now it equals $\displaystyle y(x)+\lambda \int_0^xG(x,s)sy\,ds+\lambda \int_x^1G(x,s)sy\,ds=0$. I'm confused here, since I assumed $\displaystyle y(x)=\int_0^1G(x,s)Ly(s)\,ds $ from this http://www.nada.kth.se/~annak/greens1d_odes.pdf However from Zeyman, I think we have $\displaystyle Ly(x)=\int_0^1G(x,s)Ly(s)\,ds$, since $G(x,y)$ is a kernel. Anyways, I'm not sure how to solve this question; I appreciate any help.","The Green's function of following differential operator $$(\mathcal{L}y)(x)=\frac{d}{dx}\left(x\,\frac{dy}{dx}\right)-\frac{n^2}{x}\,y(x), \:0<x<1$$ with boundary conditions $$y(0)=y(1)=0$$  can be the form of  $$G(x,t)=  \begin{cases}       \dfrac{1}{2n} \left(\dfrac{x}{t}\right)^n(1-t^{2n})& x\leq t\\       \dfrac{1}{2n} \left(\dfrac{t}{x}\right)^{\!n}(1-x^{2n}) &  x>t    \end{cases}. $$ Now using this, I would like to convert the following ODE to an integral equation for nonzero number $n$ (which I got stuck in) $$x^2y''+xy'+(\lambda x^2-n^2)y=0$$ $$y(0)=y(1)=0. $$ We also can write the above equation in $\mathcal{L}y(x)+\lambda xy=0$ relavant to $y(0)=y(1)=0$, so it may be correct to say $$\int_0^1G(x,s)(\mathcal{L}y(s)+\lambda sy(s))\,ds=0$$ So now it equals $\displaystyle y(x)+\lambda \int_0^xG(x,s)sy\,ds+\lambda \int_x^1G(x,s)sy\,ds=0$. I'm confused here, since I assumed $\displaystyle y(x)=\int_0^1G(x,s)Ly(s)\,ds $ from this http://www.nada.kth.se/~annak/greens1d_odes.pdf However from Zeyman, I think we have $\displaystyle Ly(x)=\int_0^1G(x,s)Ly(s)\,ds$, since $G(x,y)$ is a kernel. Anyways, I'm not sure how to solve this question; I appreciate any help.",,"['functional-analysis', 'ordinary-differential-equations', 'integral-equations']"
15,Theory of Autonomous Differential Equations in $\mathbb{R}^n$,Theory of Autonomous Differential Equations in,\mathbb{R}^n,"Finals are coming up and my (graduate-level) Ordinary Differential Equations professor has gone off script for about half the class. Normally, we use the text ""Nonlinear Ordinary Differential Equations"" by Roger Grimshaw. However, when beginning the section of the course on autonomous systems, the professor has been going off of his own notes rather than the textbook, and I can't find any resources for the material. It appears to be a somewhat theoretical approach to autonomous systems in $\mathbb{R}^n$ rather than plane autonomous systems (in $\mathbb{R}^2$) as is covered in Grimshaw and most other texts. The general autonomous ODE material I know of is from my Dynamics classes, covering mostly bifurcations and types of attractors in a more computational way using perturbative methods. To give some broad strokes of the kinds of things the professor covered in this class: 1) If $\phi(t)$ is a solution, then $\phi(t-\tau)$ is a solution for all $\tau \in \mathbb{R}$. 2) If $\Gamma_1$ and $\Gamma_2$ are phase curves, then either they don't intersect or they conincide. 3) Any phase curve is a point, a closed curve, or a non-self-intersecting curve 4) Rectification Theorem: There is a smooth change of coordinates from $x$ to $y(x)$ such that $\frac{dy_i}{dt} = 0$ for $i$ from $1$ to $n-1$ and $\frac{dy_n}{dt} = 1$ in a neighborhood of a given (non-fixed) point. 5) If $x(t;y)$ solves $x' = f(x)$ with $x(0)=y$, then $\log\det\frac{\partial x}{\partial y}(t;y) = \text{div}f(x(t;y))$ 6) Liouville's Theorem: If $D_t$ is the time evolution of a domain in phase space with volume (area/length/measure) $V_t$, then $\frac{dV_t}{dt} = \int_{D_t} \text{div}f(x)dx$. Corollary: Hamiltonian systems conserve phase volume since divergence is $0$. 7) Lie derivative of function with respect to vector field (derivative along trajectories) is invariant w.r.t. smooth change of variables 8) First integrals are functions which are constant along trajectories (Lie derivative is 0), and every non-fixed point has a neighborhood with $n-1$ independent first integrals. 9) The actions $g^{t_0}(x_0)= \phi(t_0;x_0)$, where $\phi(t_0;x_0)$ is a solution to $x'=f(x)$ with $x(0) = x_0$ at time $t_0$, form a group under composition. These are the main things that were covered. I can locate some (but very few) of these ideas in other resources, but mostly I'm coming up with nothing. It seems like this should all be covered in a single text, and I would be interested in hearing about which texts could give me some more insight on this view of autonomous systems.","Finals are coming up and my (graduate-level) Ordinary Differential Equations professor has gone off script for about half the class. Normally, we use the text ""Nonlinear Ordinary Differential Equations"" by Roger Grimshaw. However, when beginning the section of the course on autonomous systems, the professor has been going off of his own notes rather than the textbook, and I can't find any resources for the material. It appears to be a somewhat theoretical approach to autonomous systems in $\mathbb{R}^n$ rather than plane autonomous systems (in $\mathbb{R}^2$) as is covered in Grimshaw and most other texts. The general autonomous ODE material I know of is from my Dynamics classes, covering mostly bifurcations and types of attractors in a more computational way using perturbative methods. To give some broad strokes of the kinds of things the professor covered in this class: 1) If $\phi(t)$ is a solution, then $\phi(t-\tau)$ is a solution for all $\tau \in \mathbb{R}$. 2) If $\Gamma_1$ and $\Gamma_2$ are phase curves, then either they don't intersect or they conincide. 3) Any phase curve is a point, a closed curve, or a non-self-intersecting curve 4) Rectification Theorem: There is a smooth change of coordinates from $x$ to $y(x)$ such that $\frac{dy_i}{dt} = 0$ for $i$ from $1$ to $n-1$ and $\frac{dy_n}{dt} = 1$ in a neighborhood of a given (non-fixed) point. 5) If $x(t;y)$ solves $x' = f(x)$ with $x(0)=y$, then $\log\det\frac{\partial x}{\partial y}(t;y) = \text{div}f(x(t;y))$ 6) Liouville's Theorem: If $D_t$ is the time evolution of a domain in phase space with volume (area/length/measure) $V_t$, then $\frac{dV_t}{dt} = \int_{D_t} \text{div}f(x)dx$. Corollary: Hamiltonian systems conserve phase volume since divergence is $0$. 7) Lie derivative of function with respect to vector field (derivative along trajectories) is invariant w.r.t. smooth change of variables 8) First integrals are functions which are constant along trajectories (Lie derivative is 0), and every non-fixed point has a neighborhood with $n-1$ independent first integrals. 9) The actions $g^{t_0}(x_0)= \phi(t_0;x_0)$, where $\phi(t_0;x_0)$ is a solution to $x'=f(x)$ with $x(0) = x_0$ at time $t_0$, form a group under composition. These are the main things that were covered. I can locate some (but very few) of these ideas in other resources, but mostly I'm coming up with nothing. It seems like this should all be covered in a single text, and I would be interested in hearing about which texts could give me some more insight on this view of autonomous systems.",,"['ordinary-differential-equations', 'reference-request']"
16,"On the flow of $v(x,y)=(x,y)$ and the Lefschetz number",On the flow of  and the Lefschetz number,"v(x,y)=(x,y)","Let $v$ be the vector field on $\mathbb R^2$ defined by $v(x,y)=(x,y)$. Show that the family of diffeomorphisms $h_t:\mathbb R^2\to \mathbb R^2$ defined by $h_t(z)=e^tz$ is the flow corresponding to $v$. That is, if we fix any $z$, then the curve $t\mapsto h_t(z)$ is always tangent to $v$; its tangent vector at any time $t$ equals $v(h_t(z))$. Draw a picture of $v$ and its flow curves. Compare $\operatorname{ind}_0(v)$ with $L_0(h_t)$. (Note that the original text defines $h_t(z)=tz$, but I used this errata list .) The first part is ""obvious"" from the picture. Formally, if $z$ is fixed, then the tangent vector of $t\mapsto e^tz$ at $t$ is the derivative of this map at $t$, which is again $h_t(z)$. But $v$ is the ""identity"" -- $v(z)=z$, so the tangent vector, which is $\frac{d}{dt}h_t(z)=h_t(z)$, is also $v(h_t(z))$.  But I have a problem with showing that the curve $t\mapsto h_t(z)$ is always tangent to $v$. This means that $\frac{d}{dt}h_t(z)=v(z)$. But this isn't true. For the second part, $\operatorname{ind}_0(v)=1$ since the corresponding map is the identity. To compute $L_0(h_t)$, consider the map $z\mapsto \frac{h_t(z)}{|h_t(z)|}$ and compute its degree, which is $L_0(h_t)$. It seems this map is closely related to the map in the definition of the index of a (local) vector field (is it the same map?) and it seems the degree must be also 1. But how to show this rigorously?","Let $v$ be the vector field on $\mathbb R^2$ defined by $v(x,y)=(x,y)$. Show that the family of diffeomorphisms $h_t:\mathbb R^2\to \mathbb R^2$ defined by $h_t(z)=e^tz$ is the flow corresponding to $v$. That is, if we fix any $z$, then the curve $t\mapsto h_t(z)$ is always tangent to $v$; its tangent vector at any time $t$ equals $v(h_t(z))$. Draw a picture of $v$ and its flow curves. Compare $\operatorname{ind}_0(v)$ with $L_0(h_t)$. (Note that the original text defines $h_t(z)=tz$, but I used this errata list .) The first part is ""obvious"" from the picture. Formally, if $z$ is fixed, then the tangent vector of $t\mapsto e^tz$ at $t$ is the derivative of this map at $t$, which is again $h_t(z)$. But $v$ is the ""identity"" -- $v(z)=z$, so the tangent vector, which is $\frac{d}{dt}h_t(z)=h_t(z)$, is also $v(h_t(z))$.  But I have a problem with showing that the curve $t\mapsto h_t(z)$ is always tangent to $v$. This means that $\frac{d}{dt}h_t(z)=v(z)$. But this isn't true. For the second part, $\operatorname{ind}_0(v)=1$ since the corresponding map is the identity. To compute $L_0(h_t)$, consider the map $z\mapsto \frac{h_t(z)}{|h_t(z)|}$ and compute its degree, which is $L_0(h_t)$. It seems this map is closely related to the map in the definition of the index of a (local) vector field (is it the same map?) and it seems the degree must be also 1. But how to show this rigorously?",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'manifolds', 'differential-topology']"
17,Showing that an operator between Hölder spaces is a contraction,Showing that an operator between Hölder spaces is a contraction,,"I'm having trouble with the following problem: Consider $\beta\in(\frac{1}{2},1]$, $\xi\in\mathbb{R}$, $f:\mathbb{R}\longrightarrow \mathbb{R}$ bounded with first and second derivatives bounded, $X\in \mathcal{C}^{\beta}[0,T]$ (Space of Hölder continuous functions of order $\beta$). Let $\alpha\in (\frac{1}{2},\beta)$ and define    $$\mathcal{M}_T:\mathcal{C}^{\alpha}[0,T]\longrightarrow \mathcal{C}^{\alpha}[0,T]$$   $$Y\mapsto \xi + \int_{0}^{t}f(Y(r))dX(r), \quad t\in[0,T]$$   (Usual Riemann-Stieltjes integral).   Show that there exist $T_0\in[0,T]$ such that for every $Y,\tilde{Y}\in\mathcal{C}^{\alpha}[0,T_0]$   $$\|\mathcal{M}_{T_0}(Y)-\mathcal{M}_{T_0}(\tilde{Y})\|_{\alpha, [0,T_0]}\leq \frac{1}{2}\|Y-\tilde{Y}\|_{\alpha,[0,T_0]},$$   where $$\|X\|_{\alpha,[0,T]}:=\sup_{s\neq t \in [0,T]}\dfrac{|X(t)-X(s)|}{|t-s|^{\alpha}}.$$ I don't see how to use the boundedness of the second derivative and to obtain the inequality. What is more, I don't even know if the function is well-defined. (I just know that the integral exists, and here I use the boundedness of the first derivative).","I'm having trouble with the following problem: Consider $\beta\in(\frac{1}{2},1]$, $\xi\in\mathbb{R}$, $f:\mathbb{R}\longrightarrow \mathbb{R}$ bounded with first and second derivatives bounded, $X\in \mathcal{C}^{\beta}[0,T]$ (Space of Hölder continuous functions of order $\beta$). Let $\alpha\in (\frac{1}{2},\beta)$ and define    $$\mathcal{M}_T:\mathcal{C}^{\alpha}[0,T]\longrightarrow \mathcal{C}^{\alpha}[0,T]$$   $$Y\mapsto \xi + \int_{0}^{t}f(Y(r))dX(r), \quad t\in[0,T]$$   (Usual Riemann-Stieltjes integral).   Show that there exist $T_0\in[0,T]$ such that for every $Y,\tilde{Y}\in\mathcal{C}^{\alpha}[0,T_0]$   $$\|\mathcal{M}_{T_0}(Y)-\mathcal{M}_{T_0}(\tilde{Y})\|_{\alpha, [0,T_0]}\leq \frac{1}{2}\|Y-\tilde{Y}\|_{\alpha,[0,T_0]},$$   where $$\|X\|_{\alpha,[0,T]}:=\sup_{s\neq t \in [0,T]}\dfrac{|X(t)-X(s)|}{|t-s|^{\alpha}}.$$ I don't see how to use the boundedness of the second derivative and to obtain the inequality. What is more, I don't even know if the function is well-defined. (I just know that the integral exists, and here I use the boundedness of the first derivative).",,"['real-analysis', 'ordinary-differential-equations', 'holder-spaces', 'stieltjes-integral']"
18,Simplifying an equation involving trigonometric matrices,Simplifying an equation involving trigonometric matrices,,"I have two $n$-dimensional ODEs: $$\begin{cases} \ddot x + K_1 x = 0 & \text{for $t$ in $[0,t_1)$}\\ \ddot x + K_2 x = v & \text{for $t$ in $[t_1,t_1+t_2)$} \end{cases}$$ where $K_1, K_2$ are symmetric positive definite matrices with $K_2=K_1+e_ne_n^\top$ ($e_n=[0 \ \dots \ 0 \ 1]^\top$). I am looking for $(x_0, \dot x_0)$ and $t_1,t_2>0$ that lead to periodic solutions. I managed to find some numerical solutions, but I am wondering if the following system of equations can be further simplified. Since $K_1$, $K_2$ are spd, there have spd square roots $L_1$, $L_2$. On each of the two intervals, the solutions can be expressed as functions of $L_1$, $L_2$: $$\begin{bmatrix} x(t) \\ \dot x(t)\end{bmatrix} = S_1(t) \begin{bmatrix} x_0 \\ \dot x_0\end{bmatrix} \quad \text{for $t$ in $[0,t_1)$}$$ $$\begin{bmatrix} x(t) \\ \dot x(t)\end{bmatrix} = S_2(t) \begin{bmatrix} x_0 \\ \dot x_0\end{bmatrix} +\begin{bmatrix} -L_2^{-2}v \\ 0 \end{bmatrix}\quad \text{for $t$ in $[t_1,t_1+t_2)$}$$ with $$S_i(t)=\begin{bmatrix} \cos(tL_i) & t\operatorname{sinc}(tL_i) \\ -L_i\sin(tL_i) & \cos(tL_i)\end{bmatrix}\in\mathbb{R}^{2n\times 2n}.$$ The problem of finding periodic solutions is reduced to finding $X_0=(x_0,\dot x_0)\in\mathbb{R}^{2n}$ and $(t_1,t_2)$ such that: $$\boxed{(S_2(t_2)S_1(t_1)-I)X_0 = (S_2(t_2) -I)V}$$ with $V^\top = [-(L_2^{-2}v)^\top,\ 0]$. I add the following two equations: $X_0 e_n = 1$ and $S_1(t_1)X_0e_n=1$, so there are $2n$ unknowns and $2n$ equations. The question is, can this problem be simplified by the choice of a more appropriate basis? Remarks: The solutions I found numerically all satisfy $\det(S_2(t_2)S_1(t_1)-I)=0$, the reason is not clear to me. I don't use the fact that $K_2=K_1+e_ne_n^\top$; could it be taken advantage of?","I have two $n$-dimensional ODEs: $$\begin{cases} \ddot x + K_1 x = 0 & \text{for $t$ in $[0,t_1)$}\\ \ddot x + K_2 x = v & \text{for $t$ in $[t_1,t_1+t_2)$} \end{cases}$$ where $K_1, K_2$ are symmetric positive definite matrices with $K_2=K_1+e_ne_n^\top$ ($e_n=[0 \ \dots \ 0 \ 1]^\top$). I am looking for $(x_0, \dot x_0)$ and $t_1,t_2>0$ that lead to periodic solutions. I managed to find some numerical solutions, but I am wondering if the following system of equations can be further simplified. Since $K_1$, $K_2$ are spd, there have spd square roots $L_1$, $L_2$. On each of the two intervals, the solutions can be expressed as functions of $L_1$, $L_2$: $$\begin{bmatrix} x(t) \\ \dot x(t)\end{bmatrix} = S_1(t) \begin{bmatrix} x_0 \\ \dot x_0\end{bmatrix} \quad \text{for $t$ in $[0,t_1)$}$$ $$\begin{bmatrix} x(t) \\ \dot x(t)\end{bmatrix} = S_2(t) \begin{bmatrix} x_0 \\ \dot x_0\end{bmatrix} +\begin{bmatrix} -L_2^{-2}v \\ 0 \end{bmatrix}\quad \text{for $t$ in $[t_1,t_1+t_2)$}$$ with $$S_i(t)=\begin{bmatrix} \cos(tL_i) & t\operatorname{sinc}(tL_i) \\ -L_i\sin(tL_i) & \cos(tL_i)\end{bmatrix}\in\mathbb{R}^{2n\times 2n}.$$ The problem of finding periodic solutions is reduced to finding $X_0=(x_0,\dot x_0)\in\mathbb{R}^{2n}$ and $(t_1,t_2)$ such that: $$\boxed{(S_2(t_2)S_1(t_1)-I)X_0 = (S_2(t_2) -I)V}$$ with $V^\top = [-(L_2^{-2}v)^\top,\ 0]$. I add the following two equations: $X_0 e_n = 1$ and $S_1(t_1)X_0e_n=1$, so there are $2n$ unknowns and $2n$ equations. The question is, can this problem be simplified by the choice of a more appropriate basis? Remarks: The solutions I found numerically all satisfy $\det(S_2(t_2)S_1(t_1)-I)=0$, the reason is not clear to me. I don't use the fact that $K_2=K_1+e_ne_n^\top$; could it be taken advantage of?",,"['linear-algebra', 'ordinary-differential-equations', 'trigonometry', 'change-of-basis']"
19,basic reproduction number using next generation matrix,basic reproduction number using next generation matrix,,"I have a system of 5 ODEs and I am trying to calculate the basic reproduction number($R_0$) using the method described in the article on next generation matrix . First I tried it on the simple virus dynamic model with immune response. I know I don't need to use this method to calculate R0 to this simple system, but I wanted to try it to get an idea as to how I can use this next generation matrix. The system is, \begin{alignat}{1} \dot{x} &= λ - dx - βxv,\\ \dot{y} &= βxv - ay-pyz, \\ \dot{v} &= ky - uv,\\ \dot{z}&=cyz-bz \end{alignat} where $x$ is the uninfectedd cells, $y$ is the infected cells $v$ is the virus and $z$ describes the immune response. When the infected subsystem is linearized around the infection free steady state of $({\lambda\over d},0 ,0,0)$ the system is \begin{alignat}{4} \dot{y} &=~& - ay &~+~& \frac{λβ}{d} v, \\ \dot{v} &=& ky &~-~& uv, \\ \dot {z}&=&-bz \end{alignat} I tried this with and without $\dot z$ for the infected subsystem (those equations of the ODE system that describe the production of new infections and changes in state among infected individuals), but in both cases as Ro I get   $R_0={\beta \lambda k\over adu}$ . 1)However, Ro for this system should be $R_0={\beta \lambda k\over (a+p \hat z)du}$ where $\hat z$ is the equilbrium state immune response when CTL response is activated. $\hat z={1\over p}({\lambda \beta c k\over cdu+\beta bk}-a)$  . How can I obtain this $R_0$ value using next generation matrix? 2) In this viral model because the virus load doesn't get cleared it reaches the steady state with non zero $\hat z$. But, what happens in a model where the viral load will peak and declines to zero and at the end the model reaches the infection free state with $\hat z$=0. In this case how to include the effect from immune response (z) into $R_0$ as if I let $\hat z=0$, then it will not be included in $R_0$. I don't understand what this $\hat z$ should represent? I am trying to find $R_0$ in a model similar to the model in this article where $B_s,B_r$ are two types of bacteria and $I,P$ are innate and adaptive immune response and $R$ is resource. 3) In a model where the  viral load increase to reach a peak and then declines, does R0 have to change over the increasing and declining phase? What I mean is in the viral growth phase should $R_0>1$ and in the viral decline phase $R_0<1$? So, is the growth and decline are governed by two different $R_0$ values?","I have a system of 5 ODEs and I am trying to calculate the basic reproduction number($R_0$) using the method described in the article on next generation matrix . First I tried it on the simple virus dynamic model with immune response. I know I don't need to use this method to calculate R0 to this simple system, but I wanted to try it to get an idea as to how I can use this next generation matrix. The system is, \begin{alignat}{1} \dot{x} &= λ - dx - βxv,\\ \dot{y} &= βxv - ay-pyz, \\ \dot{v} &= ky - uv,\\ \dot{z}&=cyz-bz \end{alignat} where $x$ is the uninfectedd cells, $y$ is the infected cells $v$ is the virus and $z$ describes the immune response. When the infected subsystem is linearized around the infection free steady state of $({\lambda\over d},0 ,0,0)$ the system is \begin{alignat}{4} \dot{y} &=~& - ay &~+~& \frac{λβ}{d} v, \\ \dot{v} &=& ky &~-~& uv, \\ \dot {z}&=&-bz \end{alignat} I tried this with and without $\dot z$ for the infected subsystem (those equations of the ODE system that describe the production of new infections and changes in state among infected individuals), but in both cases as Ro I get   $R_0={\beta \lambda k\over adu}$ . 1)However, Ro for this system should be $R_0={\beta \lambda k\over (a+p \hat z)du}$ where $\hat z$ is the equilbrium state immune response when CTL response is activated. $\hat z={1\over p}({\lambda \beta c k\over cdu+\beta bk}-a)$  . How can I obtain this $R_0$ value using next generation matrix? 2) In this viral model because the virus load doesn't get cleared it reaches the steady state with non zero $\hat z$. But, what happens in a model where the viral load will peak and declines to zero and at the end the model reaches the infection free state with $\hat z$=0. In this case how to include the effect from immune response (z) into $R_0$ as if I let $\hat z=0$, then it will not be included in $R_0$. I don't understand what this $\hat z$ should represent? I am trying to find $R_0$ in a model similar to the model in this article where $B_s,B_r$ are two types of bacteria and $I,P$ are innate and adaptive immune response and $R$ is resource. 3) In a model where the  viral load increase to reach a peak and then declines, does R0 have to change over the increasing and declining phase? What I mean is in the viral growth phase should $R_0>1$ and in the viral decline phase $R_0<1$? So, is the growth and decline are governed by two different $R_0$ values?",,"['ordinary-differential-equations', 'dynamical-systems', 'mathematical-modeling', 'biology']"
20,Expected value within system of coupled differential equations,Expected value within system of coupled differential equations,,"My apologies if this is a silly question. I'm developing a model for an applied system and I'm stuck. This is a (simplified) version of my model. $x'=m(y-x)-qx$ $y'=m(x-y)-qy+m(z-y)$ where $z=x{(t_0)=y(t_0)} $ and $m$ and $q$ are my parameters I want to estimate. I was able to calculate the analytical solution of the form $\begin{bmatrix}x\\y\end{bmatrix}=c_1v_1e^{\lambda t}+c_2v_2e^{\lambda t}$. Now I want to figure out how I'm actually going to input my data/likelihood in the model so that I can estimate m and q. My data are $qx$ and $qy$ at times $t=0, 1, ..., 5$, and the likelihood for them can be modeled via a Poisson distribution. I feel like I'm misunderstanding something, but I don't know how I would put in my expected values into this model. What am I missing?","My apologies if this is a silly question. I'm developing a model for an applied system and I'm stuck. This is a (simplified) version of my model. $x'=m(y-x)-qx$ $y'=m(x-y)-qy+m(z-y)$ where $z=x{(t_0)=y(t_0)} $ and $m$ and $q$ are my parameters I want to estimate. I was able to calculate the analytical solution of the form $\begin{bmatrix}x\\y\end{bmatrix}=c_1v_1e^{\lambda t}+c_2v_2e^{\lambda t}$. Now I want to figure out how I'm actually going to input my data/likelihood in the model so that I can estimate m and q. My data are $qx$ and $qy$ at times $t=0, 1, ..., 5$, and the likelihood for them can be modeled via a Poisson distribution. I feel like I'm misunderstanding something, but I don't know how I would put in my expected values into this model. What am I missing?",,"['ordinary-differential-equations', 'expectation', 'mathematical-modeling']"
21,Second Order D.E with non-constant (trig-functions) coefficients,Second Order D.E with non-constant (trig-functions) coefficients,,"This is not a homework exercise. I am just making it clear. I have the following second-order differential equation. $$\frac{4 q'(z)^3 \sin ^3(q(z))}{z^2 \left(z^2 q'(z)^2+1\right)^{3/2}}+\frac{3 \sin ^2(q(z)) \cos (q(z))}{z^5    \left(z^2 q'(z)^2+1\right)^{3/2}}+\frac{3 q'(z) \sin ^3(q(z))}{z^4 \left(z^2 q'(z)^2+1\right)^{3/2}}+\frac{3    q'(z)^2 \sin ^2(q(z)) \cos (q(z))}{z^3 \left(z^2 q'(z)^2+1\right)^{3/2}}-\frac{q''(z) \sin ^3(q(z))}{z^3    \left(z^2 q'(z)^2+1\right)^{3/2}}=0$$ I know that the solution to the above is $ArcCos(m z)$, and this is something easily verifiable using a simple Mathematica code. My question is the following: How would I go about solving this by hand if I didn't have the solution? From my undergraduate studies, I remember that $2^{nd}$ order D.E's with non-constant coefficients are used by applying the Frobenius Method; notes on the Frobenius method . However I seem to be stuck with this, so any suggestions would be more than helpful. Thanks in advance. P.S: I am not asking anyone to solve it for me, just some recommendations would be nice. In particular, if I apply the Frobenius method, what do I do with the trig functions, as this is something I've never done before. P.S: The simplified version is $$-q''(z)+4 z q'(z)^3+\frac{3 q'(z)}{z}+3 q'(z)^2 \cot (q(z))+\frac{3 \cot (q(z))}{z^2} = 0$$","This is not a homework exercise. I am just making it clear. I have the following second-order differential equation. $$\frac{4 q'(z)^3 \sin ^3(q(z))}{z^2 \left(z^2 q'(z)^2+1\right)^{3/2}}+\frac{3 \sin ^2(q(z)) \cos (q(z))}{z^5    \left(z^2 q'(z)^2+1\right)^{3/2}}+\frac{3 q'(z) \sin ^3(q(z))}{z^4 \left(z^2 q'(z)^2+1\right)^{3/2}}+\frac{3    q'(z)^2 \sin ^2(q(z)) \cos (q(z))}{z^3 \left(z^2 q'(z)^2+1\right)^{3/2}}-\frac{q''(z) \sin ^3(q(z))}{z^3    \left(z^2 q'(z)^2+1\right)^{3/2}}=0$$ I know that the solution to the above is $ArcCos(m z)$, and this is something easily verifiable using a simple Mathematica code. My question is the following: How would I go about solving this by hand if I didn't have the solution? From my undergraduate studies, I remember that $2^{nd}$ order D.E's with non-constant coefficients are used by applying the Frobenius Method; notes on the Frobenius method . However I seem to be stuck with this, so any suggestions would be more than helpful. Thanks in advance. P.S: I am not asking anyone to solve it for me, just some recommendations would be nice. In particular, if I apply the Frobenius method, what do I do with the trig functions, as this is something I've never done before. P.S: The simplified version is $$-q''(z)+4 z q'(z)^3+\frac{3 q'(z)}{z}+3 q'(z)^2 \cot (q(z))+\frac{3 \cot (q(z))}{z^2} = 0$$",,['ordinary-differential-equations']
22,Solving Laplace Equation with two dielectrics in cylindrical coordinates,Solving Laplace Equation with two dielectrics in cylindrical coordinates,,"Suppose a concentric cylinder of height $c$ and radius $b$ with it's top cap held at $V=V_0$ and all other surfaces held at $V=0$. For calculating the potential inside the cylinder I use the Laplace equation because there is no free charge. For solving the Laplace equation in cylindrical coordinates I use a product approach: \begin{align*} \varphi(r,z,\phi)=\mathcal{R}(r) \cdot \mathcal{Z}(z) \cdot \Psi(\phi) \end{align*} With the followings solutions: \begin{align*} \mathcal{Z} &\propto e^{\pm \, k z} \\ \Psi &\propto e^{\pm \, i \nu \phi} \\ \mathcal{R} &\propto J_{\nu}(k r) \ \mathrm{or} \ N_{\nu}(k r) \end{align*} Where $J_{\nu}$ are the Bessel functions and $N_{\nu}$ are the Neumann functions. Our boundary conditions are: \begin{align*} \varphi(r,z=c)&=V_0 \\ \varphi(r,z=0)&=0 \\ \varphi(r=b,z)&=0 \end{align*} Because of rotational symmetry we can find $\nu=0$ and therefore we can neglect the $\Psi$ component. Now following the calculation from the following lecture on page 4 we end up with the solution: \begin{align*} \varphi(r,z) = \sum_{n=0}^{\infty} A_n \, J_0(k_n r) \, \sinh(k_n z) \\ A_n = \frac{V_0}{\frac{b^2}{2} [J_1(k_n b)]^2 \, \sinh(k_n c)} \int_0^b r \, J_0(k_n r) \, dr \end{align*} ($A_n$ is obtained by making use of the orthogonality of the Bessel functions, see page 3.) I have already checked this solution numerically and it's correct. So far so good. But now suppose the same cylinder, filled with a dielectric material of $\epsilon=\epsilon_1$ from $r=0$ to $r=a$ and another dielectric material with $\epsilon=\epsilon_2$ from $r=a$ to $r=b$. Now the problem becomes more complicated and we get an additional boundary condition: \begin{align*} \varphi(r,z=c)&=V_0 \quad \quad (1) \\ \varphi(r,z=0)&=0 \quad \quad (2)  \\ \varphi(r=b,z)&=0 \quad \quad (3)  \\ \varphi(r=a,z)&=f(z) \quad \quad (4)  \\ \end{align*} To solve this problem I have tried two different approaches with yet no success. In both approaches I divided the solution into two solution. $\varphi_1$ in the area of $\epsilon_1$ and $\varphi_2$ in the area of $\epsilon_2$. First approach goes as follows: Approach 1 $\Psi_{1,2}$: Rotational symmetry $\rightarrow$ $\nu=0$ and $\Psi=1$ $\mathcal{Z}_{1,2}$: Due to boundary condition (1) and (2) $\mathcal{Z}=\sinh(k_{1n} z)$ $\mathcal{R}_{1 \ }$: Because $N_0(0) = \infty$ , $\mathcal{R}_{1}=J_0(k_{1n} r)$ $\mathcal{R}_{2 \ }$: We combine Bessel and Neumann functions so that that they hold boundary condition (3) $\rightarrow$  $\mathcal{R}_{2} = G_0 = \frac{J_0(k_{1n} r)}{J_0(k_{1n} b)} - \frac{N_0(k_{1n} r)}{N_0(k_{1n} b)}$ The resulting solutions are: \begin{align*} \varphi_1  &= \sum_{n=0}^{\infty} A_{1n} \, \sinh(k_{1n} z) \, J_0(k_{1n} r) \\ \varphi_2  &= \sum_{n=0}^{\infty} A_{2n} \, \sinh(k_{2n} z) \, G_0(k_{2n} r) \\ \end{align*} with \begin{align*} A_{1n} &= \frac{V_0}{\sinh(k_{1n} c) \frac{a^2}{2} \left( [J_0(k_{1n} c)]^2 + [J_1(k_{1n} c)]^2 \right)} \int_0^a r \, J_0(k_n r) \, dr \\ A_{2n} &= \frac{1}{\sinh(k_{2n} c) \int_a^b r \, G_0(k_{2n} r) \, dr} \int_a^b r \, G_0(k_{2n} r) \, V_0 \, dr = \frac{V_0}{\sinh(k_{2n} c)} \end{align*} Now the only two parameters undetermined are $k_{1n}$ and $k_{2n}$. To solve for the two unknowns we have the two continuity conditions: \begin{align*} \varphi_1(r=a,z) &= \varphi_2(r=a,z) = f(z) \\ \epsilon_1 \frac{\partial \varphi_1(r,z)}{\partial r} \biggr\rvert_{r=a} &= \epsilon_2 \frac{\partial \varphi_2(r,z)}{\partial r} \biggr\rvert_{r=a} \end{align*} But it turns out, that the equations are just too complicated to solve for $k_{1n}$ and $k_{2n}$. So I got stuck at this point here. Approach 2 So for me the more promising approach is to use the superposition principle and devide the problem in one problem A with the boundary conditions: \begin{align*} \varphi(r,z=c)&=V_0 \quad \ (1A) \\ \varphi(r,z=0)&=0 \quad \quad (2A)  \\ \varphi(r=b,z)&=0 \quad \quad (3A)  \\ \varphi(r=a,z)&=0 \quad \quad (4A)  \\ \end{align*} and another problem B with the boundary conditions: \begin{align*} \varphi(r,z=c)&=0 \quad \quad (1B) \\ \varphi(r,z=0)&=0 \quad \quad (2B)  \\ \varphi(r=b,z)&=0 \quad \quad (3B)  \\ \varphi(r=a,z)&=f(z) \ \  (4B)  \\ \end{align*} If I solve both of these problems and add up their solutions, due to the superposition principle I get the solution I desire: \begin{align*} \varphi = \varphi_A + \varphi_B \end{align*} The solution to $\varphi_A$ is already given in the lecture I have posted above, see link on page 6-7. For $\varphi_B$ it goes as follows: $\Psi^B_{1,2}$: Rotational symmetry $\rightarrow$ $\nu=0$ and $\Psi=1$ $\mathcal{Z}^B_{1,2}$: Because neither $\sinh$ nor $\cosh$ can satisfy $\varphi(r,z=c)=0$ and $\varphi(r,z=0)=0$, $k$ has to become complex $k \rightarrow ik$ so that the $\sinh$ will become a $\sin$ with $k_n= i \frac{n \pi}{c}$: $\mathcal{Z}^B_{1,2}= \sin(\frac{n \pi}{c} z)$ $\mathcal{R}^B_{1 \ }$: Similar to the first approch we get:   $\mathcal{R}^B_{1} = J_0(i \frac{n \pi}{c} r)$ $\mathcal{R}^B_{2 \ }$: And $\mathcal{R}^B_{1} = G_0 = \frac{J_0(i \frac{n \pi}{c} r)}{J_0(i \frac{n \pi}{c} b)} - \frac{N_0(i \frac{n \pi}{c} r)}{N_0(i \frac{n \pi}{c} b)}$ The resulting solutions are: \begin{align*} \varphi_A = \sum_{n=0}^{\infty} A_{n} \, \sinh ( k_n z) \, g_0 (k_n r) \end{align*} with \begin{align*} A_n = \frac{V_0}{\sinh(k_n c)} \quad \quad \mathrm{and} \quad \quad g_0 = \frac{J_0(k_n r)}{J_0(k_n a)} - \frac{N_0(k_n r)}{N_0(k_n a)} \end{align*} and \begin{align*} \varphi_{B,1} &= \sum_{n=0}^{\infty} B_{1n} \, \sin \left( \frac{n \pi}{b} z \right) \, J_0 \left(i \frac{n \pi}{c} r \right) \\ \varphi_{B,2}  &= \sum_{n=0}^{\infty} B_{2n} \, \sin \left( \frac{n \pi}{b} z \right) \, G_0 \left(i \frac{n \pi}{c} r \right) \end{align*} with \begin{align*} B_{1n} &= \frac{1}{\frac{c}{2} J_0 \left(i \frac{n \pi}{c} a \right)} \int_0^c f(z) \sin \left(\frac{n \pi}{b} z \right)  \, dz \\ B_{2n} &= \frac{1}{\frac{c}{2} G_0 \left(i \frac{n \pi}{c} a \right)} \int_0^c f(z) \sin \left(\frac{n \pi}{b} z \right)  \, dz \end{align*} The big advantage to this approach is that $k_n$ is easily determined by finding the $k_n$ for which $g_0(b)=0$. But the big disadvantage is that there is no way, to get the $f(z)$ out of the definite integral in $B_{1n}$ and $B_{2n}$. Here $f(z)$ is the only unknown left. But by using the continuity condition I see no way to find out $f(x)$. From my numerical solution and also in approach 1, one can see, that $f(x)$ has to be a $sinh(...)$. Maybe this is a hint for you. I think there is no way to solve the equation by using the definite integral of $f(x)$ to express $B_{1n}$ and $B_{2n}$. In Germany we have a saying ""...den Wald vor lauter Bäumen nicht sehen."" -- ""...overlooking the forest due to too many trees."". And I think, and I hope that it is something obvious that I'm overlooking and I hope you can help me to find it. Also I am open for new proposals to solve this equation. Big thanks in advance! Pearli","Suppose a concentric cylinder of height $c$ and radius $b$ with it's top cap held at $V=V_0$ and all other surfaces held at $V=0$. For calculating the potential inside the cylinder I use the Laplace equation because there is no free charge. For solving the Laplace equation in cylindrical coordinates I use a product approach: \begin{align*} \varphi(r,z,\phi)=\mathcal{R}(r) \cdot \mathcal{Z}(z) \cdot \Psi(\phi) \end{align*} With the followings solutions: \begin{align*} \mathcal{Z} &\propto e^{\pm \, k z} \\ \Psi &\propto e^{\pm \, i \nu \phi} \\ \mathcal{R} &\propto J_{\nu}(k r) \ \mathrm{or} \ N_{\nu}(k r) \end{align*} Where $J_{\nu}$ are the Bessel functions and $N_{\nu}$ are the Neumann functions. Our boundary conditions are: \begin{align*} \varphi(r,z=c)&=V_0 \\ \varphi(r,z=0)&=0 \\ \varphi(r=b,z)&=0 \end{align*} Because of rotational symmetry we can find $\nu=0$ and therefore we can neglect the $\Psi$ component. Now following the calculation from the following lecture on page 4 we end up with the solution: \begin{align*} \varphi(r,z) = \sum_{n=0}^{\infty} A_n \, J_0(k_n r) \, \sinh(k_n z) \\ A_n = \frac{V_0}{\frac{b^2}{2} [J_1(k_n b)]^2 \, \sinh(k_n c)} \int_0^b r \, J_0(k_n r) \, dr \end{align*} ($A_n$ is obtained by making use of the orthogonality of the Bessel functions, see page 3.) I have already checked this solution numerically and it's correct. So far so good. But now suppose the same cylinder, filled with a dielectric material of $\epsilon=\epsilon_1$ from $r=0$ to $r=a$ and another dielectric material with $\epsilon=\epsilon_2$ from $r=a$ to $r=b$. Now the problem becomes more complicated and we get an additional boundary condition: \begin{align*} \varphi(r,z=c)&=V_0 \quad \quad (1) \\ \varphi(r,z=0)&=0 \quad \quad (2)  \\ \varphi(r=b,z)&=0 \quad \quad (3)  \\ \varphi(r=a,z)&=f(z) \quad \quad (4)  \\ \end{align*} To solve this problem I have tried two different approaches with yet no success. In both approaches I divided the solution into two solution. $\varphi_1$ in the area of $\epsilon_1$ and $\varphi_2$ in the area of $\epsilon_2$. First approach goes as follows: Approach 1 $\Psi_{1,2}$: Rotational symmetry $\rightarrow$ $\nu=0$ and $\Psi=1$ $\mathcal{Z}_{1,2}$: Due to boundary condition (1) and (2) $\mathcal{Z}=\sinh(k_{1n} z)$ $\mathcal{R}_{1 \ }$: Because $N_0(0) = \infty$ , $\mathcal{R}_{1}=J_0(k_{1n} r)$ $\mathcal{R}_{2 \ }$: We combine Bessel and Neumann functions so that that they hold boundary condition (3) $\rightarrow$  $\mathcal{R}_{2} = G_0 = \frac{J_0(k_{1n} r)}{J_0(k_{1n} b)} - \frac{N_0(k_{1n} r)}{N_0(k_{1n} b)}$ The resulting solutions are: \begin{align*} \varphi_1  &= \sum_{n=0}^{\infty} A_{1n} \, \sinh(k_{1n} z) \, J_0(k_{1n} r) \\ \varphi_2  &= \sum_{n=0}^{\infty} A_{2n} \, \sinh(k_{2n} z) \, G_0(k_{2n} r) \\ \end{align*} with \begin{align*} A_{1n} &= \frac{V_0}{\sinh(k_{1n} c) \frac{a^2}{2} \left( [J_0(k_{1n} c)]^2 + [J_1(k_{1n} c)]^2 \right)} \int_0^a r \, J_0(k_n r) \, dr \\ A_{2n} &= \frac{1}{\sinh(k_{2n} c) \int_a^b r \, G_0(k_{2n} r) \, dr} \int_a^b r \, G_0(k_{2n} r) \, V_0 \, dr = \frac{V_0}{\sinh(k_{2n} c)} \end{align*} Now the only two parameters undetermined are $k_{1n}$ and $k_{2n}$. To solve for the two unknowns we have the two continuity conditions: \begin{align*} \varphi_1(r=a,z) &= \varphi_2(r=a,z) = f(z) \\ \epsilon_1 \frac{\partial \varphi_1(r,z)}{\partial r} \biggr\rvert_{r=a} &= \epsilon_2 \frac{\partial \varphi_2(r,z)}{\partial r} \biggr\rvert_{r=a} \end{align*} But it turns out, that the equations are just too complicated to solve for $k_{1n}$ and $k_{2n}$. So I got stuck at this point here. Approach 2 So for me the more promising approach is to use the superposition principle and devide the problem in one problem A with the boundary conditions: \begin{align*} \varphi(r,z=c)&=V_0 \quad \ (1A) \\ \varphi(r,z=0)&=0 \quad \quad (2A)  \\ \varphi(r=b,z)&=0 \quad \quad (3A)  \\ \varphi(r=a,z)&=0 \quad \quad (4A)  \\ \end{align*} and another problem B with the boundary conditions: \begin{align*} \varphi(r,z=c)&=0 \quad \quad (1B) \\ \varphi(r,z=0)&=0 \quad \quad (2B)  \\ \varphi(r=b,z)&=0 \quad \quad (3B)  \\ \varphi(r=a,z)&=f(z) \ \  (4B)  \\ \end{align*} If I solve both of these problems and add up their solutions, due to the superposition principle I get the solution I desire: \begin{align*} \varphi = \varphi_A + \varphi_B \end{align*} The solution to $\varphi_A$ is already given in the lecture I have posted above, see link on page 6-7. For $\varphi_B$ it goes as follows: $\Psi^B_{1,2}$: Rotational symmetry $\rightarrow$ $\nu=0$ and $\Psi=1$ $\mathcal{Z}^B_{1,2}$: Because neither $\sinh$ nor $\cosh$ can satisfy $\varphi(r,z=c)=0$ and $\varphi(r,z=0)=0$, $k$ has to become complex $k \rightarrow ik$ so that the $\sinh$ will become a $\sin$ with $k_n= i \frac{n \pi}{c}$: $\mathcal{Z}^B_{1,2}= \sin(\frac{n \pi}{c} z)$ $\mathcal{R}^B_{1 \ }$: Similar to the first approch we get:   $\mathcal{R}^B_{1} = J_0(i \frac{n \pi}{c} r)$ $\mathcal{R}^B_{2 \ }$: And $\mathcal{R}^B_{1} = G_0 = \frac{J_0(i \frac{n \pi}{c} r)}{J_0(i \frac{n \pi}{c} b)} - \frac{N_0(i \frac{n \pi}{c} r)}{N_0(i \frac{n \pi}{c} b)}$ The resulting solutions are: \begin{align*} \varphi_A = \sum_{n=0}^{\infty} A_{n} \, \sinh ( k_n z) \, g_0 (k_n r) \end{align*} with \begin{align*} A_n = \frac{V_0}{\sinh(k_n c)} \quad \quad \mathrm{and} \quad \quad g_0 = \frac{J_0(k_n r)}{J_0(k_n a)} - \frac{N_0(k_n r)}{N_0(k_n a)} \end{align*} and \begin{align*} \varphi_{B,1} &= \sum_{n=0}^{\infty} B_{1n} \, \sin \left( \frac{n \pi}{b} z \right) \, J_0 \left(i \frac{n \pi}{c} r \right) \\ \varphi_{B,2}  &= \sum_{n=0}^{\infty} B_{2n} \, \sin \left( \frac{n \pi}{b} z \right) \, G_0 \left(i \frac{n \pi}{c} r \right) \end{align*} with \begin{align*} B_{1n} &= \frac{1}{\frac{c}{2} J_0 \left(i \frac{n \pi}{c} a \right)} \int_0^c f(z) \sin \left(\frac{n \pi}{b} z \right)  \, dz \\ B_{2n} &= \frac{1}{\frac{c}{2} G_0 \left(i \frac{n \pi}{c} a \right)} \int_0^c f(z) \sin \left(\frac{n \pi}{b} z \right)  \, dz \end{align*} The big advantage to this approach is that $k_n$ is easily determined by finding the $k_n$ for which $g_0(b)=0$. But the big disadvantage is that there is no way, to get the $f(z)$ out of the definite integral in $B_{1n}$ and $B_{2n}$. Here $f(z)$ is the only unknown left. But by using the continuity condition I see no way to find out $f(x)$. From my numerical solution and also in approach 1, one can see, that $f(x)$ has to be a $sinh(...)$. Maybe this is a hint for you. I think there is no way to solve the equation by using the definite integral of $f(x)$ to express $B_{1n}$ and $B_{2n}$. In Germany we have a saying ""...den Wald vor lauter Bäumen nicht sehen."" -- ""...overlooking the forest due to too many trees."". And I think, and I hope that it is something obvious that I'm overlooking and I hope you can help me to find it. Also I am open for new proposals to solve this equation. Big thanks in advance! Pearli",,"['ordinary-differential-equations', 'harmonic-functions', 'boundary-value-problem', 'poissons-equation', 'cylindrical-coordinates']"
23,Fun PDEs and exact solutions,Fun PDEs and exact solutions,,I am building an PDE solver and would like some PDEs to test it on. However to test it works I want to compare it to exact solutions but I'm not having much luck finding these online. I currently have diffusion equation with time-varying boundaries wave equation with constant boundaries however I would like to also test diffusion equation with no-flux boundaries advection-diffusion equation with mixed boundaries advection-diffusion equation with constant boundaries kvd equation with any boundaries benjamin-bona-manhony equation with any boundaries however I am having trouble finding suitable initial conditions and an exact solutions to these. Any suggestions of initial conditions/boundary conditions/exact solutions would be very much appreciated. If you have any exciting equations you like I would also love to hear!,I am building an PDE solver and would like some PDEs to test it on. However to test it works I want to compare it to exact solutions but I'm not having much luck finding these online. I currently have diffusion equation with time-varying boundaries wave equation with constant boundaries however I would like to also test diffusion equation with no-flux boundaries advection-diffusion equation with mixed boundaries advection-diffusion equation with constant boundaries kvd equation with any boundaries benjamin-bona-manhony equation with any boundaries however I am having trouble finding suitable initial conditions and an exact solutions to these. Any suggestions of initial conditions/boundary conditions/exact solutions would be very much appreciated. If you have any exciting equations you like I would also love to hear!,,"['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods', 'matlab']"
24,If $L^*$ is the formal self-adjoint of and $n$th order differential operator $L$ show that the formal adjoint of $L^*$ is $L$.,If  is the formal self-adjoint of and th order differential operator  show that the formal adjoint of  is .,L^* n L L^* L,If $L^*$ is the formal self-adjoint of and $n$th order differential operator $L$ show that the formal adjoint of $L^*$ is $L$. Proof By definition and hypothesis $L^* = L$. Therefore $\left(L^* \right)^* = \left(L\right)^* = L.$ Is it that easy or do I have to do some integration?,If $L^*$ is the formal self-adjoint of and $n$th order differential operator $L$ show that the formal adjoint of $L^*$ is $L$. Proof By definition and hypothesis $L^* = L$. Therefore $\left(L^* \right)^* = \left(L\right)^* = L.$ Is it that easy or do I have to do some integration?,,"['functional-analysis', 'ordinary-differential-equations']"
25,Is this demonstration of Tubular Flow Theorem for Manifolds wrong?,Is this demonstration of Tubular Flow Theorem for Manifolds wrong?,,"I'm studying Dynamical Systems using the book ""Geometric Theory of Dynamic Systems - J. Palis and W. de Melo"". On page 40, the author proposes to demonstrate the following theorem. Theorem (Tubular Flow) Let $M \subset \mathbb{R}^n$ be a $\mathcal{C}^{\infty}$-manifold of dimention $m$,   $X$ $\in$ $\mathcal{X}^r(M) = \{F: M \rightarrow \mathbb{R}^n$; $F(x)$ $\in$ $T_x M$ , $\forall x$ $\in$ $M$ and $F$ is a $\mathcal{C}^r$ function  $\}$, and $p$ $\in$ $M$ be a regular point of $X$ ( i.e. $X(p) \neq 0$). Let $C =  \{(x^1 , ..., x^m) \in \mathbb{R}^m; |x^i|<1 $ $\}$ and let $X_C$ be the vector field on $C$ definied by $X_c (x) = (1,0,...,0)$. Then there exists a $\mathcal{C}^r$ diffeomorphism $h: V_p \rightarrow C$, for some neighbourhood $V_p$ of $p$ in $M$, taking trajectories of $X$ to trajectories of $Xc$. Proof: My Doubt: Can someone explain to me why the function $h$ defined on the line underlined in red, take trajectories of $X$ to   trajectories of $Xc$? I think that this is not true because $h^{-1} (t+x_1,x_2,...,x_m) = x^{-1} \circ \tilde{\psi} \circ f  (t +x_1, ... , x_m)$ is not a trajectory of $X$. In fact \begin{align*} \frac{d}{dt} h^{-1} (t + x_1 ,x_2 , ... ,x_m) &= \frac{d}{dt} \left(x^{-1} (\tilde{\psi} (f (t + x_1, x_2 ,...,x_m) ) \right)\\ &= D x^{-1}_{\tilde{\psi} (\varepsilon (t + x_1, x_2 ,...,x_m))} \cdot D \tilde{\psi}_{\varepsilon (t+ x_1, x_2 , ...,x_m)} \cdot \frac{d}{dt} f(t+x_1,x_2,...,x_m) \\ &=  D x^{-1}_{\tilde{\psi} (\varepsilon (t + x_1, x_2 ,...,x_m))} \cdot D \tilde{\psi}_{\varepsilon (t+ x_1, x_2 , ...,x_m)} \cdot (\varepsilon e_1) \\ &= \varepsilon D x^{-1}_{\tilde{\psi} (\varepsilon (t + x_1, x_2 ,...,x_m))}  \cdot \frac{d\tilde \psi}{dt} ( \varepsilon (t + x_1,x_2,...,x_m)\\ & = \varepsilon  D x^{-1}_{\tilde{\psi} (\varepsilon (t + x_1, x_2 ,...,x_m))}  \cdot x_* X(\tilde{\psi} ( \varepsilon(t+x_1,x_2,...,x_m)))\\ &= \varepsilon X(x^{-1} \circ \tilde{\psi} \circ f (t + x_1,x_2,...,x_m))\\ &= \varepsilon X( h^{-1} (t+x_1,x_2,...,x_m)) \neq X( h^{-1} (t+x_1,x_2,...,x_m)),\\ \end{align*} where $x_* X(p) = Dx_{x^{-1} (p)} X(x^{-1}(p)).$ Keeping in mind the above calculation, why $h$ satisfies the conditions in the theorem?","I'm studying Dynamical Systems using the book ""Geometric Theory of Dynamic Systems - J. Palis and W. de Melo"". On page 40, the author proposes to demonstrate the following theorem. Theorem (Tubular Flow) Let $M \subset \mathbb{R}^n$ be a $\mathcal{C}^{\infty}$-manifold of dimention $m$,   $X$ $\in$ $\mathcal{X}^r(M) = \{F: M \rightarrow \mathbb{R}^n$; $F(x)$ $\in$ $T_x M$ , $\forall x$ $\in$ $M$ and $F$ is a $\mathcal{C}^r$ function  $\}$, and $p$ $\in$ $M$ be a regular point of $X$ ( i.e. $X(p) \neq 0$). Let $C =  \{(x^1 , ..., x^m) \in \mathbb{R}^m; |x^i|<1 $ $\}$ and let $X_C$ be the vector field on $C$ definied by $X_c (x) = (1,0,...,0)$. Then there exists a $\mathcal{C}^r$ diffeomorphism $h: V_p \rightarrow C$, for some neighbourhood $V_p$ of $p$ in $M$, taking trajectories of $X$ to trajectories of $Xc$. Proof: My Doubt: Can someone explain to me why the function $h$ defined on the line underlined in red, take trajectories of $X$ to   trajectories of $Xc$? I think that this is not true because $h^{-1} (t+x_1,x_2,...,x_m) = x^{-1} \circ \tilde{\psi} \circ f  (t +x_1, ... , x_m)$ is not a trajectory of $X$. In fact \begin{align*} \frac{d}{dt} h^{-1} (t + x_1 ,x_2 , ... ,x_m) &= \frac{d}{dt} \left(x^{-1} (\tilde{\psi} (f (t + x_1, x_2 ,...,x_m) ) \right)\\ &= D x^{-1}_{\tilde{\psi} (\varepsilon (t + x_1, x_2 ,...,x_m))} \cdot D \tilde{\psi}_{\varepsilon (t+ x_1, x_2 , ...,x_m)} \cdot \frac{d}{dt} f(t+x_1,x_2,...,x_m) \\ &=  D x^{-1}_{\tilde{\psi} (\varepsilon (t + x_1, x_2 ,...,x_m))} \cdot D \tilde{\psi}_{\varepsilon (t+ x_1, x_2 , ...,x_m)} \cdot (\varepsilon e_1) \\ &= \varepsilon D x^{-1}_{\tilde{\psi} (\varepsilon (t + x_1, x_2 ,...,x_m))}  \cdot \frac{d\tilde \psi}{dt} ( \varepsilon (t + x_1,x_2,...,x_m)\\ & = \varepsilon  D x^{-1}_{\tilde{\psi} (\varepsilon (t + x_1, x_2 ,...,x_m))}  \cdot x_* X(\tilde{\psi} ( \varepsilon(t+x_1,x_2,...,x_m)))\\ &= \varepsilon X(x^{-1} \circ \tilde{\psi} \circ f (t + x_1,x_2,...,x_m))\\ &= \varepsilon X( h^{-1} (t+x_1,x_2,...,x_m)) \neq X( h^{-1} (t+x_1,x_2,...,x_m)),\\ \end{align*} where $x_* X(p) = Dx_{x^{-1} (p)} X(x^{-1}(p)).$ Keeping in mind the above calculation, why $h$ satisfies the conditions in the theorem?",,"['ordinary-differential-equations', 'dynamical-systems', 'smooth-manifolds']"
26,Finding action-angle variables for integrable Hamiltonian,Finding action-angle variables for integrable Hamiltonian,,"How to introduce action-angle variables in the following integrable 2 d.o.f. Hamiltonian system? $$H(q,p,x,y) = \frac{y^{2}}{2} - \frac{x^{2}}{2}\left(p^{2} + \omega^{2}q^{2}\right) + \frac{x^{4}}{4}$$ So $(q,p)$, $(x,y)$ are conjugated variables, and the first integrals are $F_{0}=H, F_{1} = p^{2} + \omega^{2}q^{2}$. Observe, for fixed $(q,p)$, the Hamiltonian has a saddle point in the $(x,y)$ plane, so one has to change $(q,p,x,y) \mapsto (\varphi, I, X, Y)$ where action-angle variables are $(\varphi, I)$ and $(X,Y)$ are a new canonically conjugate pair, i.e. we have $H(q,p,x,y)  = H(I, X, Y)$. So I need to find a generating function $S$. The question is, how to find this $S$? Note that if I take $F_{1}(q,p)$ as a ""Hamiltonian"" on its own, action-angle variables $(q,p) \mapsto (\varphi, I)$ are introduced through a generating function of the form $\tilde{S}(q, \varphi) = \frac{\omega q^{2} \cot 2\pi\varphi}{2}$. Can I use this function $\tilde{S}$ as an ""inspiration"" and modify it to take $S(q, \varphi, x, Y) = \tilde{S} + xY$ ? Or in such a setup, it is not as trivial?","How to introduce action-angle variables in the following integrable 2 d.o.f. Hamiltonian system? $$H(q,p,x,y) = \frac{y^{2}}{2} - \frac{x^{2}}{2}\left(p^{2} + \omega^{2}q^{2}\right) + \frac{x^{4}}{4}$$ So $(q,p)$, $(x,y)$ are conjugated variables, and the first integrals are $F_{0}=H, F_{1} = p^{2} + \omega^{2}q^{2}$. Observe, for fixed $(q,p)$, the Hamiltonian has a saddle point in the $(x,y)$ plane, so one has to change $(q,p,x,y) \mapsto (\varphi, I, X, Y)$ where action-angle variables are $(\varphi, I)$ and $(X,Y)$ are a new canonically conjugate pair, i.e. we have $H(q,p,x,y)  = H(I, X, Y)$. So I need to find a generating function $S$. The question is, how to find this $S$? Note that if I take $F_{1}(q,p)$ as a ""Hamiltonian"" on its own, action-angle variables $(q,p) \mapsto (\varphi, I)$ are introduced through a generating function of the form $\tilde{S}(q, \varphi) = \frac{\omega q^{2} \cot 2\pi\varphi}{2}$. Can I use this function $\tilde{S}$ as an ""inspiration"" and modify it to take $S(q, \varphi, x, Y) = \tilde{S} + xY$ ? Or in such a setup, it is not as trivial?",,"['ordinary-differential-equations', 'dynamical-systems', 'classical-mechanics', 'symplectic-geometry', 'hamilton-equations']"
27,On differential polynomials,On differential polynomials,,"Definition: A differential polynomial is a polynomial with indeterminates $y$, $y'$, $y''$, $\ldots$ with coefficients in $K[x]$, algebra of polynomials with coefficients in a field $K$. An example of a differential polynomial is $$(1+x)y^2y'+(y'')^2(y''')^5-x^4 yy'y''y'''y^{(4)}-2x. $$ Such polynomials are studied in ""Differential Algebraic Geometry"" and solution of this polynomials are called ""Differential Varieties"". My question: Is it possible to express a differential polynomial as an infinite series with terms of linear differential equations? For example, is it possible to have an equality of the form $$(y')^2=\sum_{n=1}^{\infty}a_n(x)y^{(n)}? $$ Of course, we need a norm to speak about convergence. Note that the Taylor series of $(y')^2 $ does not satisfies the condition.","Definition: A differential polynomial is a polynomial with indeterminates $y$, $y'$, $y''$, $\ldots$ with coefficients in $K[x]$, algebra of polynomials with coefficients in a field $K$. An example of a differential polynomial is $$(1+x)y^2y'+(y'')^2(y''')^5-x^4 yy'y''y'''y^{(4)}-2x. $$ Such polynomials are studied in ""Differential Algebraic Geometry"" and solution of this polynomials are called ""Differential Varieties"". My question: Is it possible to express a differential polynomial as an infinite series with terms of linear differential equations? For example, is it possible to have an equality of the form $$(y')^2=\sum_{n=1}^{\infty}a_n(x)y^{(n)}? $$ Of course, we need a norm to speak about convergence. Note that the Taylor series of $(y')^2 $ does not satisfies the condition.",,['ordinary-differential-equations']
28,Numerical methods for 2nd order non-linear ODE $\ddot y=f(x)$ where $f$ is unknown,Numerical methods for 2nd order non-linear ODE  where  is unknown,\ddot y=f(x) f,"Say we have a simple 2nd order non-linear ODE $\ddot y=f(x)$. We don't know what $f$ is but have several known data points $(x_1,f(x_1)),...,(x_n,f(x_n))$. Could you help suggest numerical methods (esp. iterative algorithms ) to numerically estimate $y$? Any suggestion is helpful, including classic ones and new methods in recent researches. Thanks!","Say we have a simple 2nd order non-linear ODE $\ddot y=f(x)$. We don't know what $f$ is but have several known data points $(x_1,f(x_1)),...,(x_n,f(x_n))$. Could you help suggest numerical methods (esp. iterative algorithms ) to numerically estimate $y$? Any suggestion is helpful, including classic ones and new methods in recent researches. Thanks!",,"['ordinary-differential-equations', 'numerical-methods', 'numerical-linear-algebra']"
29,Meaning of complex valued equilibria of ODE's,Meaning of complex valued equilibria of ODE's,,"In the study of differential equations, we often have to determine the equilibrium points of an ordinary differential equation (ODE). Let us assume we have the following differential equation $$\dot{x} = x(x^2+1).$$ For an equilibrium point, we have to determine all the values $x_\text{eq}$ such that $\dot{x}_\text{eq}=0.$ The algebraic solutions for the given example are $$x_{\text{eq};1} = 0$$ $$x_{\text{eq};2,3} = \pm i,$$ in which $i$ is the imaginary unit. From standard stability theory, we know that the first equilibrium is unstable. The other two equilibria are not investigated because they are not real. From a practical point of view, this makes sense. But in the spirit of a quote from Einstein 'God does not play dice with the universe' I am not fully satisfied with the practical point of view. I am interested in the theoretical meaning of these complex valued   equilibria. I do not (want to) believe that there is absolutely no real relevance of these equilibria to the behaviour of the real system . I   am looking forward to inspiring and enlightening answers from our   clever MathStackExchange users :).","In the study of differential equations, we often have to determine the equilibrium points of an ordinary differential equation (ODE). Let us assume we have the following differential equation $$\dot{x} = x(x^2+1).$$ For an equilibrium point, we have to determine all the values $x_\text{eq}$ such that $\dot{x}_\text{eq}=0.$ The algebraic solutions for the given example are $$x_{\text{eq};1} = 0$$ $$x_{\text{eq};2,3} = \pm i,$$ in which $i$ is the imaginary unit. From standard stability theory, we know that the first equilibrium is unstable. The other two equilibria are not investigated because they are not real. From a practical point of view, this makes sense. But in the spirit of a quote from Einstein 'God does not play dice with the universe' I am not fully satisfied with the practical point of view. I am interested in the theoretical meaning of these complex valued   equilibria. I do not (want to) believe that there is absolutely no real relevance of these equilibria to the behaviour of the real system . I   am looking forward to inspiring and enlightening answers from our   clever MathStackExchange users :).",,"['complex-analysis', 'ordinary-differential-equations', 'stability-in-odes', 'stability-theory']"
30,deriving the parametric position vector of a ball flight,deriving the parametric position vector of a ball flight,,"I am investigating the flight of a golf ball using mathematics, and I'm kind of stuck. For this investigation, im looking at the flight of a golf ball when it is experienceing all 3 forces (gravity, linear drag due to air resistance and the lift caused by the magnus effect). http://ww2.odu.edu/~agodunov/teaching/phys420/files/Erlichson.pdf So I'm using this essay as the base of my research, and on page 360, it gives the newton equation for when the ball is experiencing all 3 forces. equation I'm trying to derive a parametric position equation from this equation but Im stuck because it includes both vertical and horizontal velocities. I'm planning on using integrating factor, but im not getting anywhere Could someone help?","I am investigating the flight of a golf ball using mathematics, and I'm kind of stuck. For this investigation, im looking at the flight of a golf ball when it is experienceing all 3 forces (gravity, linear drag due to air resistance and the lift caused by the magnus effect). http://ww2.odu.edu/~agodunov/teaching/phys420/files/Erlichson.pdf So I'm using this essay as the base of my research, and on page 360, it gives the newton equation for when the ball is experiencing all 3 forces. equation I'm trying to derive a parametric position equation from this equation but Im stuck because it includes both vertical and horizontal velocities. I'm planning on using integrating factor, but im not getting anywhere Could someone help?",,"['calculus', 'integration', 'ordinary-differential-equations']"
31,Using Green's function to solve 2d laplace equation,Using Green's function to solve 2d laplace equation,,"Consider a domain $D : {(x,y) : x>0 , y>0}$ Let $\mathbf{x}= (x,y)$ and $\mathbf{\xi}= (\xi_x, \xi_y)$. Then the Green's function satisfying $$\nabla^2G = \delta(\mathbf{x} - \mathbf{\xi} )$$ subject to $$\frac{\partial G}{\partial x} (0, y, \mathbf{\xi})= 0 \, \, \,\,\, \mathrm{for} \, \, \,\,\, y>0$$ $$G(x, 0, \mathbf{\xi}) = 0 \, \, \,\,\, \mathrm{for} \, \, \,\,\, x>0$$ is given by: $$\frac{1}{4 \pi}\ln \huge| \small\frac{((x - \xi_x)^2 + (y- \xi_y)^2)((x + \xi_x)^2 + (y- \xi_y)^2)}{((x - \xi_x)^2 + (y+ \xi_y)^2)((x + \xi_x)^2 + (y+ \xi_y)^2)} \huge|$$ through the method of images. Now using this I have to solve Laplace's equation: $$\nabla^2u=0$$ subject to $u \rightarrow 0$ as $|\mathbf{x}| \rightarrow \infty$, $$\frac{\partial u(0,y)}{\partial x} = h(y)  \, \, \, \, \mathrm{for} \, \, \, \, y > 0$$ and $$u(x, 0) = g(x) \, \, \, \, \mathrm{for} \, \, \, \, x > 0$$ So far I've got that $$G(0, y, \mathbf{\xi}) = \frac{1}{2\pi} \ln \huge| \small \frac{\xi_x^2+(y- \xi_y)^2}{\xi_x^2 +(y+ \xi_y)^2} \huge|$$ which gets me $$u(\xi_x, \xi_y) = \frac{1}{2\pi} \int^{\infty}_{0}h(y) \ln \huge| \small \frac{\xi_x^2 +(y- \xi_y)^2}{\xi_x^2 +(y+ \xi_y)^2} \huge| \small dy$$ I'm not sure if I'm going about the rest of the question correctly, if I rewrite the above to $$u(x, y) = \frac{1}{2\pi} \int^{\infty}_{0}h(\lambda) \ln \huge| \small \frac{x^2 +(y- \lambda)^2}{x^2 +(y+ \lambda)^2} \huge|  \small d\lambda \, \, \, \,$$ would that be correct?","Consider a domain $D : {(x,y) : x>0 , y>0}$ Let $\mathbf{x}= (x,y)$ and $\mathbf{\xi}= (\xi_x, \xi_y)$. Then the Green's function satisfying $$\nabla^2G = \delta(\mathbf{x} - \mathbf{\xi} )$$ subject to $$\frac{\partial G}{\partial x} (0, y, \mathbf{\xi})= 0 \, \, \,\,\, \mathrm{for} \, \, \,\,\, y>0$$ $$G(x, 0, \mathbf{\xi}) = 0 \, \, \,\,\, \mathrm{for} \, \, \,\,\, x>0$$ is given by: $$\frac{1}{4 \pi}\ln \huge| \small\frac{((x - \xi_x)^2 + (y- \xi_y)^2)((x + \xi_x)^2 + (y- \xi_y)^2)}{((x - \xi_x)^2 + (y+ \xi_y)^2)((x + \xi_x)^2 + (y+ \xi_y)^2)} \huge|$$ through the method of images. Now using this I have to solve Laplace's equation: $$\nabla^2u=0$$ subject to $u \rightarrow 0$ as $|\mathbf{x}| \rightarrow \infty$, $$\frac{\partial u(0,y)}{\partial x} = h(y)  \, \, \, \, \mathrm{for} \, \, \, \, y > 0$$ and $$u(x, 0) = g(x) \, \, \, \, \mathrm{for} \, \, \, \, x > 0$$ So far I've got that $$G(0, y, \mathbf{\xi}) = \frac{1}{2\pi} \ln \huge| \small \frac{\xi_x^2+(y- \xi_y)^2}{\xi_x^2 +(y+ \xi_y)^2} \huge|$$ which gets me $$u(\xi_x, \xi_y) = \frac{1}{2\pi} \int^{\infty}_{0}h(y) \ln \huge| \small \frac{\xi_x^2 +(y- \xi_y)^2}{\xi_x^2 +(y+ \xi_y)^2} \huge| \small dy$$ I'm not sure if I'm going about the rest of the question correctly, if I rewrite the above to $$u(x, y) = \frac{1}{2\pi} \int^{\infty}_{0}h(\lambda) \ln \huge| \small \frac{x^2 +(y- \lambda)^2}{x^2 +(y+ \lambda)^2} \huge|  \small d\lambda \, \, \, \,$$ would that be correct?",,"['ordinary-differential-equations', 'partial-differential-equations', 'harmonic-functions', 'dirac-delta', 'greens-function']"
32,Differentiable dependence on initial conditions and parameters of an ODE,Differentiable dependence on initial conditions and parameters of an ODE,,"I don't know what equations they want me to found here: Let $\phi(t,t_0,x_0,(a,b))$ be the solution to the IVP $$\begin{cases}\dot{x}=2t(ax-bx^2) \\x(t_0)=x_0\end{cases}$$ Prove that the derivatives $\partial\phi_{x_0}(t,0,1,(1,0))$ , $\ \partial\phi_{a}(t,0,1,(1,0))$ , $\ \partial\phi_{b}(t,0,1,(1,0))$ are the same that the ones we obtain using the differentiable dependence theorem on initial conditions and parameters. I know how to solve this system and how to find $\partial\phi_{x_0}(t,0,1,(1,0))$ , $\ \partial\phi_{a}(t,0,1,(1,0))$ and $\ \partial\phi_{b}(t,0,1,(1,0))$ , but I don't know what they me to obtain using the differentiable dependence theorem on initial conditions and parameters. I'd appreciate some hint about it. Thanks.","I don't know what equations they want me to found here: Let be the solution to the IVP Prove that the derivatives , , are the same that the ones we obtain using the differentiable dependence theorem on initial conditions and parameters. I know how to solve this system and how to find , and , but I don't know what they me to obtain using the differentiable dependence theorem on initial conditions and parameters. I'd appreciate some hint about it. Thanks.","\phi(t,t_0,x_0,(a,b)) \begin{cases}\dot{x}=2t(ax-bx^2) \\x(t_0)=x_0\end{cases} \partial\phi_{x_0}(t,0,1,(1,0)) \ \partial\phi_{a}(t,0,1,(1,0)) \ \partial\phi_{b}(t,0,1,(1,0)) \partial\phi_{x_0}(t,0,1,(1,0)) \ \partial\phi_{a}(t,0,1,(1,0)) \ \partial\phi_{b}(t,0,1,(1,0))","['calculus', 'real-analysis', 'ordinary-differential-equations', 'derivatives', 'initial-value-problems']"
33,Differential Equation with Euler multiplier,Differential Equation with Euler multiplier,,"I want to solve the following D.E. $$xy'+(lnx)y =lnx$$ First I need to bring it to the form $y'+a(x)y=b(x)$, so it becomes: $$y'+\frac{lnx}{x}y=\frac{lnx}{x} $$ After this, I need to calculate the $\int a(x)dx$ which will give me: $$\int{\frac{lnx}{x}dx} = \frac{(lnx)^2}{2} +c$$ So the Euler multiplier is $e^\frac{(lnx)^2}{2}$. Now I multiply my equation with this, at each side and get on the left side the product rule: $$(ye^\frac{(lnx)^2}{2})'=e^\frac{(lnx)^2}{2}\frac{lnx}{x}$$ So the question that I have is this: Am I allowed to do the following? $$ye^\frac{(lnx)^2}{2}=\int{e^\frac{(lnx)^2}{2}\frac{lnx}{x}}dx$$ If no, why? Edit: If this is correct, then if I simplify I get: $$ yx=\int{lnx}dx $$ and by computing the integral I would end up with this: $$yx= xlnx -x +c \Rightarrow y=lnx -1+c$$ But if it is done otherwise, like this: $$(ye^\frac{(lnx)^2}{2})'=(e^\frac{(lnx)^2}{2})'  \Rightarrow ye^\frac{(lnx)^2}{2}=e^\frac{(lnx)^2}{2}$$ I get this result: $$ y= 1 +c *e^{-\frac{(lnx)^2}{2}} $$ which is different from the other result.","I want to solve the following D.E. $$xy'+(lnx)y =lnx$$ First I need to bring it to the form $y'+a(x)y=b(x)$, so it becomes: $$y'+\frac{lnx}{x}y=\frac{lnx}{x} $$ After this, I need to calculate the $\int a(x)dx$ which will give me: $$\int{\frac{lnx}{x}dx} = \frac{(lnx)^2}{2} +c$$ So the Euler multiplier is $e^\frac{(lnx)^2}{2}$. Now I multiply my equation with this, at each side and get on the left side the product rule: $$(ye^\frac{(lnx)^2}{2})'=e^\frac{(lnx)^2}{2}\frac{lnx}{x}$$ So the question that I have is this: Am I allowed to do the following? $$ye^\frac{(lnx)^2}{2}=\int{e^\frac{(lnx)^2}{2}\frac{lnx}{x}}dx$$ If no, why? Edit: If this is correct, then if I simplify I get: $$ yx=\int{lnx}dx $$ and by computing the integral I would end up with this: $$yx= xlnx -x +c \Rightarrow y=lnx -1+c$$ But if it is done otherwise, like this: $$(ye^\frac{(lnx)^2}{2})'=(e^\frac{(lnx)^2}{2})'  \Rightarrow ye^\frac{(lnx)^2}{2}=e^\frac{(lnx)^2}{2}$$ I get this result: $$ y= 1 +c *e^{-\frac{(lnx)^2}{2}} $$ which is different from the other result.",,['ordinary-differential-equations']
34,"Existence and uniqueness of limit cycle for $r' = μr(1-g(r^2)), \text{and} θ' = p(r^2)$",Existence and uniqueness of limit cycle for,"r' = μr(1-g(r^2)), \text{and} θ' = p(r^2)","Exercise : Consider the dynamical system :   $$r' = μr(1-g(r^2))$$   $$θ' = p(r^2)$$   where $μ>0$ is a constant and $g,p: \mathbb R_+ \to \mathbb R \space$ are smooth functions with $p(1) > 0$ and $g(1) = 1$. Show the existence of a limit cycle. Is the limit cycle unique ? Attempt : So, first of all, we have to check the sign of $r'$. Since $μ>0$, we observe that the points $r=0$ and $r=1$ are stationary points, since $g(1) = 1$. In previous cases, one would have to study the changes of the  sign of $r'$ depending on $r$ and would then fix an one-dimension phase portrait with arrows denoting the monotonic domains. If the arrows would ""converge"" $(\rightarrow \space \leftarrow$) around an equilibria, then this is enough to show that a unique cycle exists. In this particular case though, I am unable to conduct such a result, since we are not sure of the sign of the expression $(1-g(r^2))$ but we only know that it has a zero point at $r=1$. I'm pretty sure there's another way around, especially since we're given that $p(1) >0$ which I am not sure where to use yet (I guess it shows a clockwise ""movement"" of the phase portrait). I would appreciate any hints or help !","Exercise : Consider the dynamical system :   $$r' = μr(1-g(r^2))$$   $$θ' = p(r^2)$$   where $μ>0$ is a constant and $g,p: \mathbb R_+ \to \mathbb R \space$ are smooth functions with $p(1) > 0$ and $g(1) = 1$. Show the existence of a limit cycle. Is the limit cycle unique ? Attempt : So, first of all, we have to check the sign of $r'$. Since $μ>0$, we observe that the points $r=0$ and $r=1$ are stationary points, since $g(1) = 1$. In previous cases, one would have to study the changes of the  sign of $r'$ depending on $r$ and would then fix an one-dimension phase portrait with arrows denoting the monotonic domains. If the arrows would ""converge"" $(\rightarrow \space \leftarrow$) around an equilibria, then this is enough to show that a unique cycle exists. In this particular case though, I am unable to conduct such a result, since we are not sure of the sign of the expression $(1-g(r^2))$ but we only know that it has a zero point at $r=1$. I'm pretty sure there's another way around, especially since we're given that $p(1) >0$ which I am not sure where to use yet (I guess it shows a clockwise ""movement"" of the phase portrait). I would appreciate any hints or help !",,"['ordinary-differential-equations', 'dynamical-systems', 'polar-coordinates', 'stability-in-odes', 'stability-theory']"
35,"Existence of unique limit cycle for $r'=r(μ-r^2), \space θ' = ρ(r^2)$",Existence of unique limit cycle for,"r'=r(μ-r^2), \space θ' = ρ(r^2)","Exercise : Translate the dynamical system given in polar coordinates to cartesian coordinates :   $$r' = r(μ-r^2)$$   $$θ' = ρ(r^2)$$   where $μ>0$ is a constant and $p: \mathbb R_+ \to \mathbb R$ smooth function with $ρ(μ) > 0$. Show that there exists a unique limit cycle. Would we have the same result if it was $ρ(μ) = 0$ ? Attempt : Using the relations : $$x = r\cos(θ) \Rightarrow x' = r'\cos(θ) - r\sin(θ)θ'$$ $$y = r\sin(θ) \Rightarrow y' = r'\sin(θ) + r\cos(θ)θ'$$ I derived the equations for the given system in cartesian coordinates, as : $$x' =  μx - yρ(x^2 + y^2) - x(x^2+y^2)$$ $$y' =  μy - xρ(x^2 + y^2) - y(x^2+y^2)$$ Now, regarding the limit cycle, if : $$f(x,y) = \begin{bmatrix} μx - yρ(x^2 + y^2) - x(x^2+y^2) \\ μy - xρ(x^2+y^2) - y(x^2+y^2)\end{bmatrix}$$ How would I proceed about the limit cycle part of the problem though ? I know that one approach could be showing that $\text{div}(f) \neq 0$ for a simply connected set, but I'm not sure if this is correct and it seems that it wouldn't be leading anywhere since there is an unknown $ρ$ function there. This is the first problem I'm trying to solve regarding that part of dynamical systems so I'd appreciate any help. What would I need to do to show the existence of a unique limit cycle ?","Exercise : Translate the dynamical system given in polar coordinates to cartesian coordinates :   $$r' = r(μ-r^2)$$   $$θ' = ρ(r^2)$$   where $μ>0$ is a constant and $p: \mathbb R_+ \to \mathbb R$ smooth function with $ρ(μ) > 0$. Show that there exists a unique limit cycle. Would we have the same result if it was $ρ(μ) = 0$ ? Attempt : Using the relations : $$x = r\cos(θ) \Rightarrow x' = r'\cos(θ) - r\sin(θ)θ'$$ $$y = r\sin(θ) \Rightarrow y' = r'\sin(θ) + r\cos(θ)θ'$$ I derived the equations for the given system in cartesian coordinates, as : $$x' =  μx - yρ(x^2 + y^2) - x(x^2+y^2)$$ $$y' =  μy - xρ(x^2 + y^2) - y(x^2+y^2)$$ Now, regarding the limit cycle, if : $$f(x,y) = \begin{bmatrix} μx - yρ(x^2 + y^2) - x(x^2+y^2) \\ μy - xρ(x^2+y^2) - y(x^2+y^2)\end{bmatrix}$$ How would I proceed about the limit cycle part of the problem though ? I know that one approach could be showing that $\text{div}(f) \neq 0$ for a simply connected set, but I'm not sure if this is correct and it seems that it wouldn't be leading anywhere since there is an unknown $ρ$ function there. This is the first problem I'm trying to solve regarding that part of dynamical systems so I'd appreciate any help. What would I need to do to show the existence of a unique limit cycle ?",,"['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'stability-in-odes']"
36,Flow on a Torus is Transitive iff it is Incommensurate,Flow on a Torus is Transitive iff it is Incommensurate,,"Consider the system $$ \dot{\mathbf{x}}(t)=\mathbf{a} $$ where $t\in\mathbb{R},\mathbf{x}\in\mathbb{R}^n,$ and $\mathbf{a}\in\mathbb{R}^n.$ It is well known that the flow on the n-torus $\mathbb{T}^n$ generated by  $$\varphi_t:\mathbb{T}^n\to\mathbb{T}^n $$ given by  $$\varphi_t\left(\mathbf{x}_0\right):=t\mathbf{a}+\mathbf{x}_0 $$ is transitive if and only if it is incommensurate. This paper http://fe.math.kobe-u.ac.jp/FE/FE_pdf_with_bookmark/FE01-10-en_KML/fe07-091-102/fe07-091-102.pdf references several papers early on as a proof of this. However, I'm looking for a direct proof that doesn't rely on any measure theoretic notions or ergodic theory for pedagogical purposes. I think I have a proof, yet I haven't been able find one anywhere else despite this being an important example in integrable dynamical systems. Has anyone come across a direct proof of this well-known result?","Consider the system $$ \dot{\mathbf{x}}(t)=\mathbf{a} $$ where $t\in\mathbb{R},\mathbf{x}\in\mathbb{R}^n,$ and $\mathbf{a}\in\mathbb{R}^n.$ It is well known that the flow on the n-torus $\mathbb{T}^n$ generated by  $$\varphi_t:\mathbb{T}^n\to\mathbb{T}^n $$ given by  $$\varphi_t\left(\mathbf{x}_0\right):=t\mathbf{a}+\mathbf{x}_0 $$ is transitive if and only if it is incommensurate. This paper http://fe.math.kobe-u.ac.jp/FE/FE_pdf_with_bookmark/FE01-10-en_KML/fe07-091-102/fe07-091-102.pdf references several papers early on as a proof of this. However, I'm looking for a direct proof that doesn't rely on any measure theoretic notions or ergodic theory for pedagogical purposes. I think I have a proof, yet I haven't been able find one anywhere else despite this being an important example in integrable dynamical systems. Has anyone come across a direct proof of this well-known result?",,"['ordinary-differential-equations', 'reference-request', 'dynamical-systems', 'ergodic-theory', 'integrable-systems']"
37,Existence and Uniqueness of Equilibrium Points in Non-Linear Dynamical Systems,Existence and Uniqueness of Equilibrium Points in Non-Linear Dynamical Systems,,"Let $\dot{X} = F(X)$ be a non-linear dynamical system. I'm interested in knowing if there are any existence theorems for an equilibrium point, that is, an $X^*$ that satisfies: $$ F(X^*) = 0 $$ I know that in the linear case $(F(X) = AX)$, it's trivial to establish that $X^* = 0$ is the equilibrium point; but I haven't found any theorem that guarantees the existence of equilibrium points in the non-linear case. In case there is a theorem that guarantees existence, I'm also interested in a theorem that could tell us something about the uniqueness (or lack thereof) of equilibrium points. So far, I've only come up with the idea that one should check whether $F(X) + X = G(X)$ satisfies Brouwer's fixed point theorem to check if the system has an equilibrium point, but I don't know if this is the right approach. Thanks!","Let $\dot{X} = F(X)$ be a non-linear dynamical system. I'm interested in knowing if there are any existence theorems for an equilibrium point, that is, an $X^*$ that satisfies: $$ F(X^*) = 0 $$ I know that in the linear case $(F(X) = AX)$, it's trivial to establish that $X^* = 0$ is the equilibrium point; but I haven't found any theorem that guarantees the existence of equilibrium points in the non-linear case. In case there is a theorem that guarantees existence, I'm also interested in a theorem that could tell us something about the uniqueness (or lack thereof) of equilibrium points. So far, I've only come up with the idea that one should check whether $F(X) + X = G(X)$ satisfies Brouwer's fixed point theorem to check if the system has an equilibrium point, but I don't know if this is the right approach. Thanks!",,"['ordinary-differential-equations', 'dynamical-systems', 'fixed-point-theorems', 'nonlinear-dynamics']"
38,Functional Derivative on Manifold?,Functional Derivative on Manifold?,,"In M-theories, there are often Action functionals (in the physics sense), defined on manifold involving p-forms and such. Letting $\mathcal{M}$ denote the manifold with dimension $d$, one might encounter something of the form: $$S = a \int_\mathcal{M} A \wedge \mathrm{d}A + b\int_\mathcal{M}  A \wedge A \wedge A $$ Where these are some sort of non-abelian differential forms. In the usual case, one defines the integrand as the Lagrangian $\mathcal{L}$ and can determine the equations obeyed by the dynamical variables by solving the Euler-Lagrange equation, for example if $\mathcal{L}= \mathcal{L}(t, x, dx/dt)$, then we solve: $$ \frac{\partial \mathcal{L}}{\partial x } - \frac{\mathrm{d}}{\mathrm{d} t }\left(\frac{\partial \mathcal{L}}{\partial (\frac{\mathrm{d}x}{\mathrm{d}t})}  \right) = 0 $$  If there a mathematically rigorous way to generalize this to the case of differential forms, for example, in the action above, if I abuse notation assume that: $$ \frac{\partial \mathcal{L}}{\partial A}+ \mathrm{d}\left(\frac{\partial \mathcal{L}}{\partial (\mathrm{d} A)} \right)= 0 $$ Then I get something of the form: $$ a\mathrm{d}A + 3 b A\wedge A + a \mathrm{d}A = 2a\mathrm{d}A + 3 b A\wedge A = 0    $$ (Where $\mathrm{d}^2 =0$). Where this is in fact the correct equation of motion describing the system and is what one would compute by considering the variation $A \to A+ \delta A$. Is this a fluke? or can this method be trusted in general?","In M-theories, there are often Action functionals (in the physics sense), defined on manifold involving p-forms and such. Letting $\mathcal{M}$ denote the manifold with dimension $d$, one might encounter something of the form: $$S = a \int_\mathcal{M} A \wedge \mathrm{d}A + b\int_\mathcal{M}  A \wedge A \wedge A $$ Where these are some sort of non-abelian differential forms. In the usual case, one defines the integrand as the Lagrangian $\mathcal{L}$ and can determine the equations obeyed by the dynamical variables by solving the Euler-Lagrange equation, for example if $\mathcal{L}= \mathcal{L}(t, x, dx/dt)$, then we solve: $$ \frac{\partial \mathcal{L}}{\partial x } - \frac{\mathrm{d}}{\mathrm{d} t }\left(\frac{\partial \mathcal{L}}{\partial (\frac{\mathrm{d}x}{\mathrm{d}t})}  \right) = 0 $$  If there a mathematically rigorous way to generalize this to the case of differential forms, for example, in the action above, if I abuse notation assume that: $$ \frac{\partial \mathcal{L}}{\partial A}+ \mathrm{d}\left(\frac{\partial \mathcal{L}}{\partial (\mathrm{d} A)} \right)= 0 $$ Then I get something of the form: $$ a\mathrm{d}A + 3 b A\wedge A + a \mathrm{d}A = 2a\mathrm{d}A + 3 b A\wedge A = 0    $$ (Where $\mathrm{d}^2 =0$). Where this is in fact the correct equation of motion describing the system and is what one would compute by considering the variation $A \to A+ \delta A$. Is this a fluke? or can this method be trusted in general?",,"['ordinary-differential-equations', 'differential-geometry', 'differential-topology', 'differential-forms', 'calculus-of-variations']"
39,When is an ordinary differential equation truly inexact?,When is an ordinary differential equation truly inexact?,,"An exact ordinary differential equation is one of the form: $$M(x,y)dx+N(x,y)dy=0\tag{1}$$ where we must have that $$\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}\tag{2}$$ This is equivalent to saying that there is an underlying potential function whose total derivative gives the differential equation (1). When we have an ordinary differential equation of the form (1) that does not satisfy (2), we say that it is inexact. However what we normally do is realize that if we cleverly multiply the entire equation (1) by a function $\mu(x,y)$, called an ""integrating factor"", we can actually make the equation exact. In a sense, this seems to indicate to me that it actually was exact all along, just not manifestly exact. Is there a way to tell whether or not a differential equation of the form (1) is truly inexact, i.e. there exists no such integrating factor $\mu$ that will make it exact? Of course, I'm restricting my attention to well-behaved functions $M$ and $N$.","An exact ordinary differential equation is one of the form: $$M(x,y)dx+N(x,y)dy=0\tag{1}$$ where we must have that $$\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}\tag{2}$$ This is equivalent to saying that there is an underlying potential function whose total derivative gives the differential equation (1). When we have an ordinary differential equation of the form (1) that does not satisfy (2), we say that it is inexact. However what we normally do is realize that if we cleverly multiply the entire equation (1) by a function $\mu(x,y)$, called an ""integrating factor"", we can actually make the equation exact. In a sense, this seems to indicate to me that it actually was exact all along, just not manifestly exact. Is there a way to tell whether or not a differential equation of the form (1) is truly inexact, i.e. there exists no such integrating factor $\mu$ that will make it exact? Of course, I'm restricting my attention to well-behaved functions $M$ and $N$.",,['ordinary-differential-equations']
40,What are the solutions to the following equation (which may or may not be a differential equation)?,What are the solutions to the following equation (which may or may not be a differential equation)?,,"Let $X = \mathbb{R}^n$ and let $T\colon X\to X$. Suppose for every $x\in X$, $$x - T(x) = \nabla\tfrac{1}{2}\|x-T(x)\|^2.$$ What are all the solutions to this equation? I am aware the (generally nonsmooth ) projections onto nonempty closed convex subsets of $X$ will satisfy this but are there other solutions out there?","Let $X = \mathbb{R}^n$ and let $T\colon X\to X$. Suppose for every $x\in X$, $$x - T(x) = \nabla\tfrac{1}{2}\|x-T(x)\|^2.$$ What are all the solutions to this equation? I am aware the (generally nonsmooth ) projections onto nonempty closed convex subsets of $X$ will satisfy this but are there other solutions out there?",,"['real-analysis', 'ordinary-differential-equations', 'derivatives', 'partial-differential-equations', 'convex-analysis']"
41,Even and odd functions with Fourier series,Even and odd functions with Fourier series,,"I have a general question in partial differential equations. Can we say that when an even function is expressed as a Fourier series, the Fourier cosine series is also the Fourier series? My thinking is that a Fourier series has the form, $$f(x) = \frac{a_0}{2}+\sum^{\infty}_{n=1} a_n cos(nx) + \sum^\infty_{n=1}b_nsin(nx)$$ where $$a_0 = \frac{1}{\pi}\int ^\infty _{-\infty}f(x)dx$$, $$a_n = \frac{1}{\pi}\int ^\infty _{\infty}f(x)cos(nx)dx$$, $$b_n = \frac{1}{\pi}f(x)sin(nx)dx$$ where $cos$ is a even function and $sin$ is a odd function. Then if $f(x)$ is even, multiplying a even and odd function together gives a odd function which is $0$ which would eliminate the $b_n$ term, leaving just $a_0$ and $a_n$ which is the fourier cosine series. Therefore yes this is true","I have a general question in partial differential equations. Can we say that when an even function is expressed as a Fourier series, the Fourier cosine series is also the Fourier series? My thinking is that a Fourier series has the form, $$f(x) = \frac{a_0}{2}+\sum^{\infty}_{n=1} a_n cos(nx) + \sum^\infty_{n=1}b_nsin(nx)$$ where $$a_0 = \frac{1}{\pi}\int ^\infty _{-\infty}f(x)dx$$, $$a_n = \frac{1}{\pi}\int ^\infty _{\infty}f(x)cos(nx)dx$$, $$b_n = \frac{1}{\pi}f(x)sin(nx)dx$$ where $cos$ is a even function and $sin$ is a odd function. Then if $f(x)$ is even, multiplying a even and odd function together gives a odd function which is $0$ which would eliminate the $b_n$ term, leaving just $a_0$ and $a_n$ which is the fourier cosine series. Therefore yes this is true",,"['ordinary-differential-equations', 'fourier-analysis', 'fourier-series']"
42,Why does the closing lemma follow from the local closing lemma?,Why does the closing lemma follow from the local closing lemma?,,"Let $M$ be a closed smooth manifold. The $C^r$-closing lemma (open in the case $r >1$) says that if $x$ is a non-wandering point of $f \in \mathcal{Diff}^r(M)$ then there is a $g$ arbitrarily $C^r$ close to $f$ for which $x$ is periodic. The local $C^r$ closing lemma states that under the same hypothesis as above, for any neighborhood $U$ of $f$ in $\mathcal{Diff}^r(M)$ and neighborhood $V$ of $x$, there is a $g \in U$ and $x_g \in V$ such that $x_g$ is periodic for $g$. It's claimed in the 2012 survey article of D. V. Anosov and E. V. Zhuzhoma that the local closing lemma implies the closing lemma, by an argument from Pugh. The proof given in [1], for vector fields, but I'm having trouble understanding it. Does anyone know of a proof for diffeomorphisms? Thanks in advance. [1] Pugh, Charles C. ""The Closing Lemma."" American Journal of Mathematics 89, no. 4 (1967)","Let $M$ be a closed smooth manifold. The $C^r$-closing lemma (open in the case $r >1$) says that if $x$ is a non-wandering point of $f \in \mathcal{Diff}^r(M)$ then there is a $g$ arbitrarily $C^r$ close to $f$ for which $x$ is periodic. The local $C^r$ closing lemma states that under the same hypothesis as above, for any neighborhood $U$ of $f$ in $\mathcal{Diff}^r(M)$ and neighborhood $V$ of $x$, there is a $g \in U$ and $x_g \in V$ such that $x_g$ is periodic for $g$. It's claimed in the 2012 survey article of D. V. Anosov and E. V. Zhuzhoma that the local closing lemma implies the closing lemma, by an argument from Pugh. The proof given in [1], for vector fields, but I'm having trouble understanding it. Does anyone know of a proof for diffeomorphisms? Thanks in advance. [1] Pugh, Charles C. ""The Closing Lemma."" American Journal of Mathematics 89, no. 4 (1967)",,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
43,"Why are second order linear PDEs classified as either elliptic, hyperbolic or parabolic?","Why are second order linear PDEs classified as either elliptic, hyperbolic or parabolic?",,"Is there a geometric interpretation of second order linear partial differential equations which explains why they are classified as either elliptic, hyperbolic or parabolic, or is this just a naming convention? That is, do they have any relation with actual ellipses, hyperbolas and parabolas?","Is there a geometric interpretation of second order linear partial differential equations which explains why they are classified as either elliptic, hyperbolic or parabolic, or is this just a naming convention? That is, do they have any relation with actual ellipses, hyperbolas and parabolas?",,"['geometry', 'ordinary-differential-equations', 'partial-differential-equations', 'terminology', 'intuition']"
44,Blow up criterion in PDE theory,Blow up criterion in PDE theory,,I need a clear idea about how one can establish a blow up criterion on a solution of partial differential equation or ordinary one. I don't need lot of details. I just need a general idea supported by an example so everyone can understand it.,I need a clear idea about how one can establish a blow up criterion on a solution of partial differential equation or ordinary one. I don't need lot of details. I just need a general idea supported by an example so everyone can understand it.,,"['ordinary-differential-equations', 'partial-differential-equations']"
45,Power Series Solution for an ODE which has trigonometric coefficient functions,Power Series Solution for an ODE which has trigonometric coefficient functions,,"The ODE for which we seek a power series solution is: $$y''+ \cos(x)y' + x\sin(x)y = 0,\hspace{0.4cm} y(0) = 1,\hspace{.1cm} y'(0) = 0$$ I need to find the partial sum up to five, from the initial conditions I know $a_0=1$ and $a_1=0$, I also know I need to find the recurrence relation to get the coefficients but therein lies my issue. I'm as far as: $$\sum_{n=2}^{\infty}n(n-1)a_n x^{n-2} + \cos (x)\sum_{n=1}^{\infty} n a_n x^{n-1} + x\sin(x)\sum_{n=0}^{\infty}a_nx^n = 0$$ I believe I need to use the power series representation of cosine and sine then take the Cauchy product to simplify. Is this the right track? If so could someone help me understand how exactly the algebra of the Cauchy product in this case works out, I've only ever used it in the simple cases before. So I figured I'd update this post so anyone who may have a similar question somewhere down the line isn't left hanging. Thanks again to Ian for helping with this. First of all rather than using the full series representation of sine and cosine you really only need: $$\cos (x) = 1 - \frac{x^2}{2} + \frac {x^4}{24} + ..., \hspace{0.4cm} x\sin (x) = x^2 - \frac {x^4}{6}+ ...$$ You only need these terms because you're looking for the partial sum up to five. Now bear with me the next part is long. Substitute this along with the power series for $y =\sum_{n=0}^{5}a_nx^n$ again just use the first five terms. Then: \begin{align} y & = (2a_2 +6a_3 +12 a_$x^2 + 20a_3x^3)\\ &\hspace{.4cm} +(1 - \frac{x^2}{2} + \frac {x^4}{24})(2a_2x+3a_3x^2 +4a_4x^3+5a_5x^4)\\ &\hspace{.4cm} +(x^2-\frac{x^4}{6})(1+ a_2x^2+a_3x^3+a_4x^4+a_5x^5) \end{align} Now just collect the constants on like-ordered terms, for example: $$x^0 \to 2a_2 = 0 \Rightarrow a_2 = 0 $$ or $$x^2 \to 12a_4 +3a_3 +1 + a_2 = 0 \Rightarrow a_4 = \frac {-1}{12} $$ Just remember to do them in order since the constants ""build"". That is $a_0, a_1, \text{and } a_2 \text{ are needed to get } a_3 $. Finally just stick the constants into $y = \sum_{n=0}^{5}a_nx^n$ and then you can use it to approximate some value for $y(x)$. Sorry about the sort of messy formatting, I'm not overly familiar with MathJax formatting.","The ODE for which we seek a power series solution is: $$y''+ \cos(x)y' + x\sin(x)y = 0,\hspace{0.4cm} y(0) = 1,\hspace{.1cm} y'(0) = 0$$ I need to find the partial sum up to five, from the initial conditions I know $a_0=1$ and $a_1=0$, I also know I need to find the recurrence relation to get the coefficients but therein lies my issue. I'm as far as: $$\sum_{n=2}^{\infty}n(n-1)a_n x^{n-2} + \cos (x)\sum_{n=1}^{\infty} n a_n x^{n-1} + x\sin(x)\sum_{n=0}^{\infty}a_nx^n = 0$$ I believe I need to use the power series representation of cosine and sine then take the Cauchy product to simplify. Is this the right track? If so could someone help me understand how exactly the algebra of the Cauchy product in this case works out, I've only ever used it in the simple cases before. So I figured I'd update this post so anyone who may have a similar question somewhere down the line isn't left hanging. Thanks again to Ian for helping with this. First of all rather than using the full series representation of sine and cosine you really only need: $$\cos (x) = 1 - \frac{x^2}{2} + \frac {x^4}{24} + ..., \hspace{0.4cm} x\sin (x) = x^2 - \frac {x^4}{6}+ ...$$ You only need these terms because you're looking for the partial sum up to five. Now bear with me the next part is long. Substitute this along with the power series for $y =\sum_{n=0}^{5}a_nx^n$ again just use the first five terms. Then: \begin{align} y & = (2a_2 +6a_3 +12 a_$x^2 + 20a_3x^3)\\ &\hspace{.4cm} +(1 - \frac{x^2}{2} + \frac {x^4}{24})(2a_2x+3a_3x^2 +4a_4x^3+5a_5x^4)\\ &\hspace{.4cm} +(x^2-\frac{x^4}{6})(1+ a_2x^2+a_3x^3+a_4x^4+a_5x^5) \end{align} Now just collect the constants on like-ordered terms, for example: $$x^0 \to 2a_2 = 0 \Rightarrow a_2 = 0 $$ or $$x^2 \to 12a_4 +3a_3 +1 + a_2 = 0 \Rightarrow a_4 = \frac {-1}{12} $$ Just remember to do them in order since the constants ""build"". That is $a_0, a_1, \text{and } a_2 \text{ are needed to get } a_3 $. Finally just stick the constants into $y = \sum_{n=0}^{5}a_nx^n$ and then you can use it to approximate some value for $y(x)$. Sorry about the sort of messy formatting, I'm not overly familiar with MathJax formatting.",,"['ordinary-differential-equations', 'power-series', 'cauchy-product']"
46,Video feedback mathematical model,Video feedback mathematical model,,"There are several video demonstrations for video feedback leading chaos such as [1] , [2] , [3] , [3] . My question is how to model it? If I have such a color matrix of struct Color {     unsigned char r,g,b; };  Color screen[1280][720]; at time $t$, then how can I calculate screen at time $t+\Delta t$? I am looking for a model with zooming the camera enough so chaos happens.","There are several video demonstrations for video feedback leading chaos such as [1] , [2] , [3] , [3] . My question is how to model it? If I have such a color matrix of struct Color {     unsigned char r,g,b; };  Color screen[1280][720]; at time $t$, then how can I calculate screen at time $t+\Delta t$? I am looking for a model with zooming the camera enough so chaos happens.",,"['ordinary-differential-equations', 'analysis', 'mathematical-modeling', 'control-theory']"
47,Annihilator polynomial and the Laplace Transform,Annihilator polynomial and the Laplace Transform,,"Let $$L(f(t)) = F(s)=N(s)/M(s)$$ be the Laplace Transform of a certain (exponential-bounded) function, $N(s)$ and $M(s)$ being minimal polynomials. Is it true that the operator $M(D)$ always annihilates $f(t)$ (i.e., $M(D)f(t) = 0$, where $D=d/dt$)? Does this fact generalize to some more general function types? I'll illustrate this with several examples: 1) $f(t) = \sum_{k=0}^n c_k t^k, c_k$ are real parameters, $n$ is a natural number. In this case, $F(s)= pol_n(s)/s^{n+1}$, where $pol_n(s)$ is a polynomial in $s$ of degree $n$. $M(s)=s^{n+1}$, so $M(D)=D^{n+1}$, which clearly annihilates $f(t)$. 2) $f(t) = e^{at}, a$ is a real parameter. In this case, $F(s)=1/(s-a)$. $M(s)=s-a$, so $M(D)=D-a$, which clearly annihilates $f(t)$. 3) $f(t) = c_1\sin(wt) + c_2\cos(wt)$, where $w, c_1$ and $c_2$ are real parameters. In this case, $F(s)= (c_1w + c_2s)/(s^2+w^2)$. $M(s)=s^2+w^2$, so $M(D)=D^2+w^2$, which clearly annihilates $f(t)$. You could actually take advantage of several theorems about the Laplace Transform (e.g. Shift on the $s$-plane) to calculate the annihilator of more complex functions: 4) $f(t) = c_1e^{at}\sin(wt) + c_2e^{at}\cos(wt)$, where $a, w, c_1$ and $c_2$ are real parameters. In this case, $F(s)= (c_1w + c_2(s-a))/((s-a)^2+w^2)$. $M(s)=(s-a)^2+w^2$, so $M(D)=(D-a)^2+w^2$, which annihilates $f(t)$. 5) $f(t) = c_1t\sin(wt) + c_2t\cos(wt)$, where $w, c_1$ and $c_2$ are real parameters. In this case, $F(s)= (2c_1ws + c_2(s^2-w^2))/(s^2+w^2)^2$. $M(s)=(s^2+w^2)^2$, so $M(D)=(D^2+w^2)^2$, which annihilates $f(t)$. 6) $f(t) = c_1t^ne^{at}$, where $a, c_1$ are real parameters and $n$ is a natural number. In this case, $F(s)= c_1n!/(s-a)^{n+1}$. $M(s)=(s-a)^{n+1}$, so $M(D)=(D-a)^{n+1}$, which annihilates $f(t)$. Finally, you could sum up some of the previous results to get the following: 7) $f(t) = (\sum_{k=0}^n c_k t^k)e^{at}\sin(wt) + (\sum_{k=0}^n d_k t^k)e^{at}\cos(wt)$, where $a, w, c_k$ and $d_k$ are real parameters. In this case, $M(D)=((D-a)^2+w^2)^{n+1}$, which annihilates $f(t)$. What about $f(t)= \sqrt{t}$, for which $F(s)=\sqrt{\pi}/(2s^{3/2})$?","Let $$L(f(t)) = F(s)=N(s)/M(s)$$ be the Laplace Transform of a certain (exponential-bounded) function, $N(s)$ and $M(s)$ being minimal polynomials. Is it true that the operator $M(D)$ always annihilates $f(t)$ (i.e., $M(D)f(t) = 0$, where $D=d/dt$)? Does this fact generalize to some more general function types? I'll illustrate this with several examples: 1) $f(t) = \sum_{k=0}^n c_k t^k, c_k$ are real parameters, $n$ is a natural number. In this case, $F(s)= pol_n(s)/s^{n+1}$, where $pol_n(s)$ is a polynomial in $s$ of degree $n$. $M(s)=s^{n+1}$, so $M(D)=D^{n+1}$, which clearly annihilates $f(t)$. 2) $f(t) = e^{at}, a$ is a real parameter. In this case, $F(s)=1/(s-a)$. $M(s)=s-a$, so $M(D)=D-a$, which clearly annihilates $f(t)$. 3) $f(t) = c_1\sin(wt) + c_2\cos(wt)$, where $w, c_1$ and $c_2$ are real parameters. In this case, $F(s)= (c_1w + c_2s)/(s^2+w^2)$. $M(s)=s^2+w^2$, so $M(D)=D^2+w^2$, which clearly annihilates $f(t)$. You could actually take advantage of several theorems about the Laplace Transform (e.g. Shift on the $s$-plane) to calculate the annihilator of more complex functions: 4) $f(t) = c_1e^{at}\sin(wt) + c_2e^{at}\cos(wt)$, where $a, w, c_1$ and $c_2$ are real parameters. In this case, $F(s)= (c_1w + c_2(s-a))/((s-a)^2+w^2)$. $M(s)=(s-a)^2+w^2$, so $M(D)=(D-a)^2+w^2$, which annihilates $f(t)$. 5) $f(t) = c_1t\sin(wt) + c_2t\cos(wt)$, where $w, c_1$ and $c_2$ are real parameters. In this case, $F(s)= (2c_1ws + c_2(s^2-w^2))/(s^2+w^2)^2$. $M(s)=(s^2+w^2)^2$, so $M(D)=(D^2+w^2)^2$, which annihilates $f(t)$. 6) $f(t) = c_1t^ne^{at}$, where $a, c_1$ are real parameters and $n$ is a natural number. In this case, $F(s)= c_1n!/(s-a)^{n+1}$. $M(s)=(s-a)^{n+1}$, so $M(D)=(D-a)^{n+1}$, which annihilates $f(t)$. Finally, you could sum up some of the previous results to get the following: 7) $f(t) = (\sum_{k=0}^n c_k t^k)e^{at}\sin(wt) + (\sum_{k=0}^n d_k t^k)e^{at}\cos(wt)$, where $a, w, c_k$ and $d_k$ are real parameters. In this case, $M(D)=((D-a)^2+w^2)^{n+1}$, which annihilates $f(t)$. What about $f(t)= \sqrt{t}$, for which $F(s)=\sqrt{\pi}/(2s^{3/2})$?",,"['ordinary-differential-equations', 'laplace-transform']"
48,How to find a Lax pair,How to find a Lax pair,,I've been getting into Hamiltonian PDE's lately and they give the KdV equation a lax pair to proof that it is integrable in some sense. My question is the following how does on find a lax-pair if we just start from an hamiltonian PDE or more specific how did they find it for the KdV-equation. Or was it just guessing?,I've been getting into Hamiltonian PDE's lately and they give the KdV equation a lax pair to proof that it is integrable in some sense. My question is the following how does on find a lax-pair if we just start from an hamiltonian PDE or more specific how did they find it for the KdV-equation. Or was it just guessing?,,"['ordinary-differential-equations', 'hamilton-equations', 'lax-pair']"
49,Turning A Partial Differential Equation into an Algebraic Equation using Fourier Transforms,Turning A Partial Differential Equation into an Algebraic Equation using Fourier Transforms,,"It is commonly known that the Fourier transform, as an operator, can turn an Ordinary Differential Equation (ODE) into an Algebraic Equation, and a Partial Differential Equation (PDE) into an ODE. I was wondering if one can turn a PDE (say of two variables) into an Algebraic Equation by applying the Fourier Transform twice, once applied to one variable, and the second, applied to the second variable. For instance, Let: $\frac{du}{dx}=\frac{du}{dt}$ . we can apply the Fourier transform once with respect to $x$ and thus we have: $(is)F(y)(s,t)=\frac{dF(y)(s,t)}{dt}$ where $F(y)(s)$ is the Fourier of $y$ taking it from the spacial $x$ domain to the frequency $s$ domain. We can now repeat this process but taking the transform from the temporal domain to some new domain $k$ . Thus we have: $(is)F[F(y)](s,k)=(ik)F[F(y)](s,k)$ . as can be seen, this is now a Algebraic Equation, and by applying the inverse Fourier twice, we could arrive at our original function. Thanks.","It is commonly known that the Fourier transform, as an operator, can turn an Ordinary Differential Equation (ODE) into an Algebraic Equation, and a Partial Differential Equation (PDE) into an ODE. I was wondering if one can turn a PDE (say of two variables) into an Algebraic Equation by applying the Fourier Transform twice, once applied to one variable, and the second, applied to the second variable. For instance, Let: . we can apply the Fourier transform once with respect to and thus we have: where is the Fourier of taking it from the spacial domain to the frequency domain. We can now repeat this process but taking the transform from the temporal domain to some new domain . Thus we have: . as can be seen, this is now a Algebraic Equation, and by applying the inverse Fourier twice, we could arrive at our original function. Thanks.","\frac{du}{dx}=\frac{du}{dt} x (is)F(y)(s,t)=\frac{dF(y)(s,t)}{dt} F(y)(s) y x s k (is)F[F(y)](s,k)=(ik)F[F(y)](s,k)","['functional-analysis', 'ordinary-differential-equations', 'fourier-analysis']"
50,Geodesic differential equation with arc legth parametrization,Geodesic differential equation with arc legth parametrization,,"As known, if $S\subset \mathbb{R}^3$ is a surface, $\sigma: U\subset \mathbb{R}^2 \rightarrow V\cap S$ a parametrization, and  \begin{align}\alpha:I& \rightarrow S\\ t& \mapsto \sigma(u(t),v(t)) \end{align}  a curve on $S$, then $\alpha$ is a geodesic if, and only if, resolve que defferential equation \begin{align}u'' + \Gamma_{11}^1 (u')^2 + 2 \Gamma_{12}^{1}u' v' + \Gamma_{22}^{1} (v')&=0, \qquad(1) \\   v'' + \Gamma_{11}^2 (u')^2 + 2 \Gamma_{12}^{2}u' v' + \Gamma_{22}^{2} (v')&=0, \qquad(2)  \end{align} where $\Gamma_{ij}^{k}$ are the Christoffel symbols. I need to show that: QUESTION: When the differential equation of the geodesics are referred to the arc length then the second equation (2) is, except for the coordinate curves, a consequence of the first equation (1). The book suggests write each $\Gamma_{ij}^{k}$ in function of $E$, $F$, $G$ (where $E(u,v)= <\sigma_u(u,v),\sigma_u(u,v)>$, $F(u,v)= <\sigma_u(u,v),\sigma_v(u,v)>$), $ G(u,v)= <\sigma_v(u,v),\sigma_v (u,v)>$) i. e. \begin{align} \Gamma_{11}^{1} &= \frac{G E_u - 2 F F_u + F E_v}{2(E G - F^2)}, \quad \Gamma_{11}^{2} =\frac{2EF_u - E E_v - F E_u}{2(EG - F^2)},\\ \Gamma_{12}^{1} &= \frac{G E_v - F G_u}{2(E G - F^2)}, \hspace{2cm} \Gamma_{12}^{2} =\frac{EG_u - F E_v}{2(EG - F^2)},\\ \Gamma_{22}^{1} &= \frac{2GF_v - G G_u - F G_u}{2(EG - F^2)}, \quad \Gamma_{22}^{2} =\frac{E G_v - 2F F_v + F G_ u}{2(EG - F^2)}, \end{align} and use that $\sigma(u(t),v(t))$ is parametrized by arc length, i.e. $$1= E (u')^2 + 2F (u'v') + G (v')^2  $$ $$0= E_u (u')^3 + E_v (v')(u')^2 + 2 E u'' u' + 2F_u (u')^2 v' + 2F_v u' (v')^2 + 2F u'' v' + 2F u' v'' + G_u u' (v')^2 + G_v (u')(v')^2 + 2 G v'' v',   $$ to conclude the result, but I wasn't able to manipulate this equations in order to prove the required. Any help?","As known, if $S\subset \mathbb{R}^3$ is a surface, $\sigma: U\subset \mathbb{R}^2 \rightarrow V\cap S$ a parametrization, and  \begin{align}\alpha:I& \rightarrow S\\ t& \mapsto \sigma(u(t),v(t)) \end{align}  a curve on $S$, then $\alpha$ is a geodesic if, and only if, resolve que defferential equation \begin{align}u'' + \Gamma_{11}^1 (u')^2 + 2 \Gamma_{12}^{1}u' v' + \Gamma_{22}^{1} (v')&=0, \qquad(1) \\   v'' + \Gamma_{11}^2 (u')^2 + 2 \Gamma_{12}^{2}u' v' + \Gamma_{22}^{2} (v')&=0, \qquad(2)  \end{align} where $\Gamma_{ij}^{k}$ are the Christoffel symbols. I need to show that: QUESTION: When the differential equation of the geodesics are referred to the arc length then the second equation (2) is, except for the coordinate curves, a consequence of the first equation (1). The book suggests write each $\Gamma_{ij}^{k}$ in function of $E$, $F$, $G$ (where $E(u,v)= <\sigma_u(u,v),\sigma_u(u,v)>$, $F(u,v)= <\sigma_u(u,v),\sigma_v(u,v)>$), $ G(u,v)= <\sigma_v(u,v),\sigma_v (u,v)>$) i. e. \begin{align} \Gamma_{11}^{1} &= \frac{G E_u - 2 F F_u + F E_v}{2(E G - F^2)}, \quad \Gamma_{11}^{2} =\frac{2EF_u - E E_v - F E_u}{2(EG - F^2)},\\ \Gamma_{12}^{1} &= \frac{G E_v - F G_u}{2(E G - F^2)}, \hspace{2cm} \Gamma_{12}^{2} =\frac{EG_u - F E_v}{2(EG - F^2)},\\ \Gamma_{22}^{1} &= \frac{2GF_v - G G_u - F G_u}{2(EG - F^2)}, \quad \Gamma_{22}^{2} =\frac{E G_v - 2F F_v + F G_ u}{2(EG - F^2)}, \end{align} and use that $\sigma(u(t),v(t))$ is parametrized by arc length, i.e. $$1= E (u')^2 + 2F (u'v') + G (v')^2  $$ $$0= E_u (u')^3 + E_v (v')(u')^2 + 2 E u'' u' + 2F_u (u')^2 v' + 2F_v u' (v')^2 + 2F u'' v' + 2F u' v'' + G_u u' (v')^2 + G_v (u')(v')^2 + 2 G v'' v',   $$ to conclude the result, but I wasn't able to manipulate this equations in order to prove the required. Any help?",,"['ordinary-differential-equations', 'differential-geometry', 'arc-length']"
51,"Continuous dependence on the RHS in ODEs, and related exercises","Continuous dependence on the RHS in ODEs, and related exercises",,"In Birkhoff and Rota's ""Ordinary Differential Equations"", p. 177, the following theorem is formulated (I've modified it slightly): Let $\mathbf{x}(t)$ and $\mathbf{y}(t)$ satisfy the DEs   $$\mathrm{d} \mathbf{x}/ \mathrm{d}t= \mathbf{X}(\mathbf{x},t) \quad \text{ and } \quad \mathrm{d}\mathbf{y}/\mathrm{d} t= \mathbf{Y}(\mathbf{y},t) $$   respectively, on $a \leq t \leq b$. Further, let the functions $\mathbf{X}$ and $\mathbf{Y}$ be defined and continuous in a common domain $D \times [a,b]$, and let   $$|\mathbf{X}(\mathbf{z},t)-\mathbf{Y}(\mathbf{z},t)|\leq \epsilon, \quad a\leq t \leq b, \quad \mathbf{z} \in D $$   Finally let $\mathbf{X}(\mathbf{x},t)$ satisfy the Lipschitz condition   $$|\mathbf{X}(\mathbf{x},t)-\mathbf{X}(\mathbf{y},t)| \leq L |\mathbf x-\mathbf y|, \quad \text{if} \quad (\mathbf{x},t),(\mathbf{y},t) \in D \times [a,b]. $$   Then   $$|\mathbf{x}(t)-\mathbf{y}(t)| \leq |\mathbf{x}(a)-\mathbf{y}(a)| \mathrm{e}^{L|t-a|}+ \frac{\epsilon}{L} \left[ \mathrm{e}^{L|t-a|}-1 \right].$$   The function $\mathbf{Y}$ is not required to satisfy a Lipschitz condition. The proof of the above then makes use of a differential inequality, with the ""equals case"" being a Bernoulli differential equation. I'm upset at the fact that the authors haven't mentioned on what $t$-interval the result of the theorem holds. I suspect that the interval could be shorter than $[a,b]$, if the solutions leave the domain $D$. Later, in the exercises they ask to ""bound the difference on $[0,1]$ between solutions having the same initial value $y(0)=c$ for the DEs $$y'=\mathrm{e}^y \; \text{and} \; y'=1+y+\cdots+\frac{y^n}{n!}. ""$$ When trying to solve this exercise, I've identified the first RHS, $\mathrm{e}^y$ with $\mathbf{X}$. The first equation can be solved explicitly as $$y(x)=-\log(\mathrm{e}^{-c}-x), $$ with the interval of existence being $$(-\infty, \mathrm{e}^{-c}). $$ Hence, the exercise doesn't even make sense for $c \geq 0$. Even if I assume that $c<0$, I'm having difficulties finding a domain $D \ni y$, such that the solution will remain in it for $x \in [0,1]$ (which should be done without solving the equation, I suppose). Overall my questions are: In the statement of the theorem, what is the interval on which the conclusion holds? Can a claim of its size be made at all? How can one solve the aforementioned exercise? Thank you!","In Birkhoff and Rota's ""Ordinary Differential Equations"", p. 177, the following theorem is formulated (I've modified it slightly): Let $\mathbf{x}(t)$ and $\mathbf{y}(t)$ satisfy the DEs   $$\mathrm{d} \mathbf{x}/ \mathrm{d}t= \mathbf{X}(\mathbf{x},t) \quad \text{ and } \quad \mathrm{d}\mathbf{y}/\mathrm{d} t= \mathbf{Y}(\mathbf{y},t) $$   respectively, on $a \leq t \leq b$. Further, let the functions $\mathbf{X}$ and $\mathbf{Y}$ be defined and continuous in a common domain $D \times [a,b]$, and let   $$|\mathbf{X}(\mathbf{z},t)-\mathbf{Y}(\mathbf{z},t)|\leq \epsilon, \quad a\leq t \leq b, \quad \mathbf{z} \in D $$   Finally let $\mathbf{X}(\mathbf{x},t)$ satisfy the Lipschitz condition   $$|\mathbf{X}(\mathbf{x},t)-\mathbf{X}(\mathbf{y},t)| \leq L |\mathbf x-\mathbf y|, \quad \text{if} \quad (\mathbf{x},t),(\mathbf{y},t) \in D \times [a,b]. $$   Then   $$|\mathbf{x}(t)-\mathbf{y}(t)| \leq |\mathbf{x}(a)-\mathbf{y}(a)| \mathrm{e}^{L|t-a|}+ \frac{\epsilon}{L} \left[ \mathrm{e}^{L|t-a|}-1 \right].$$   The function $\mathbf{Y}$ is not required to satisfy a Lipschitz condition. The proof of the above then makes use of a differential inequality, with the ""equals case"" being a Bernoulli differential equation. I'm upset at the fact that the authors haven't mentioned on what $t$-interval the result of the theorem holds. I suspect that the interval could be shorter than $[a,b]$, if the solutions leave the domain $D$. Later, in the exercises they ask to ""bound the difference on $[0,1]$ between solutions having the same initial value $y(0)=c$ for the DEs $$y'=\mathrm{e}^y \; \text{and} \; y'=1+y+\cdots+\frac{y^n}{n!}. ""$$ When trying to solve this exercise, I've identified the first RHS, $\mathrm{e}^y$ with $\mathbf{X}$. The first equation can be solved explicitly as $$y(x)=-\log(\mathrm{e}^{-c}-x), $$ with the interval of existence being $$(-\infty, \mathrm{e}^{-c}). $$ Hence, the exercise doesn't even make sense for $c \geq 0$. Even if I assume that $c<0$, I'm having difficulties finding a domain $D \ni y$, such that the solution will remain in it for $x \in [0,1]$ (which should be done without solving the equation, I suppose). Overall my questions are: In the statement of the theorem, what is the interval on which the conclusion holds? Can a claim of its size be made at all? How can one solve the aforementioned exercise? Thank you!",,"['ordinary-differential-equations', 'continuity']"
52,Solving the ODE: $\frac{1}{r} \frac{d}{d r} \left( r \frac{d f}{d r} \right) + \left( a - b e^{r^2} \right) f = c+ d e^{- r^2} $,Solving the ODE:,\frac{1}{r} \frac{d}{d r} \left( r \frac{d f}{d r} \right) + \left( a - b e^{r^2} \right) f = c+ d e^{- r^2} ,"I'm trying hard to solve this: $$\frac{1}{r} \frac{d}{d r} \left( r \frac{d f}{d r} \right) + \left( a - b e^{r^2} \right) f = c+ d e^{- r^2} $$ where $r$ ranges between $0$ and $\infty$, $a$ and $b$ are positive constants, $c$ and $d$ may have either sign. Any of you is able to handle this?","I'm trying hard to solve this: $$\frac{1}{r} \frac{d}{d r} \left( r \frac{d f}{d r} \right) + \left( a - b e^{r^2} \right) f = c+ d e^{- r^2} $$ where $r$ ranges between $0$ and $\infty$, $a$ and $b$ are positive constants, $c$ and $d$ may have either sign. Any of you is able to handle this?",,"['ordinary-differential-equations', 'derivatives', 'laplace-transform', 'laplacian', 'stability-in-odes']"
53,What are the solutions of the equation $ (\partial_1 \cdots \partial_n)f=gf $?,What are the solutions of the equation ?, (\partial_1 \cdots \partial_n)f=gf ,"Let $n$ be a positive integer and $g:\textbf{R}^n\longrightarrow \textbf{R}$ be a smooth function. Let $\partial_i$ denotes the partial derivative with respect to the the $i$th coordinate ($i\in\{1,...,n\}$). What are the functions $f:\textbf{R}^n\longrightarrow \textbf{R}$  solutions to the following partial differential equation $$ (\partial_1 \cdots \partial_n)f=gf \quad ?$$ Or, at least, what is the dimension of the $\textbf{R}$-vector space of solutions? Many thanks!","Let $n$ be a positive integer and $g:\textbf{R}^n\longrightarrow \textbf{R}$ be a smooth function. Let $\partial_i$ denotes the partial derivative with respect to the the $i$th coordinate ($i\in\{1,...,n\}$). What are the functions $f:\textbf{R}^n\longrightarrow \textbf{R}$  solutions to the following partial differential equation $$ (\partial_1 \cdots \partial_n)f=gf \quad ?$$ Or, at least, what is the dimension of the $\textbf{R}$-vector space of solutions? Many thanks!",,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative', 'linear-pde']"
54,"When solving PDEs by separation of variables, why are we allowed to divide by the dependent variable?","When solving PDEs by separation of variables, why are we allowed to divide by the dependent variable?",,"When solving PDEs in physics, one common tool in use is separation of variables. However, to me, there exists a big problem when one divides both sides of a equation by some function that might be $0$. Here in this link ( Laplace's equation 1: Separation of variables , Rudolf Winter, 2008), when we divided by $XY$, should not we assume $XY$ is not $0$? But it seems like $XY$ could be 0 in the general solution. Why is that?","When solving PDEs in physics, one common tool in use is separation of variables. However, to me, there exists a big problem when one divides both sides of a equation by some function that might be $0$. Here in this link ( Laplace's equation 1: Separation of variables , Rudolf Winter, 2008), when we divided by $XY$, should not we assume $XY$ is not $0$? But it seems like $XY$ could be 0 in the general solution. Why is that?",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
55,Geometric proof of uniqueness and existence of ordinary differential equations,Geometric proof of uniqueness and existence of ordinary differential equations,,"In Arnold's book on ordinary differential equations, he proves the existence and uniqueness of one dimensional ordinary differential equations of the form $$ \frac{dx}{dt} = v(x) $$ by first showing that at poins with $v(x_0) \neq 0$, we have unique solutions described by the equation $$ t - t_0 = \int_{x_0}^x \frac{dy}{v(y)} $$ and for $v(x_0) = 0$, assuming $v$ is Lipschitz here, the function approaches $x_0$ too slowly, and as such integral curves approaching $x_0$ take infinite time to reach $x_0$ (the integral formula above diverges if we let $x_0$ converge to a point where the vector field vanishes). Arnold later indicates in an exercise that this is an idea behind a general method for proving the uniqueness of differential equations. However, I can't seem to prove the method in general myself, and it seems every reference I find relies on Picard's method for iteratively constructing a differential equation. Would anyone be able to tell me if I'm going in the wrong direction, or point me to a source with this result if I do?","In Arnold's book on ordinary differential equations, he proves the existence and uniqueness of one dimensional ordinary differential equations of the form $$ \frac{dx}{dt} = v(x) $$ by first showing that at poins with $v(x_0) \neq 0$, we have unique solutions described by the equation $$ t - t_0 = \int_{x_0}^x \frac{dy}{v(y)} $$ and for $v(x_0) = 0$, assuming $v$ is Lipschitz here, the function approaches $x_0$ too slowly, and as such integral curves approaching $x_0$ take infinite time to reach $x_0$ (the integral formula above diverges if we let $x_0$ converge to a point where the vector field vanishes). Arnold later indicates in an exercise that this is an idea behind a general method for proving the uniqueness of differential equations. However, I can't seem to prove the method in general myself, and it seems every reference I find relies on Picard's method for iteratively constructing a differential equation. Would anyone be able to tell me if I'm going in the wrong direction, or point me to a source with this result if I do?",,['ordinary-differential-equations']
56,Transformation of a differential equation,Transformation of a differential equation,,"Say I have a differential equation, $$ \dddot x = 4 \omega^2 (1 + \epsilon \cos (\Omega t)) \dot x-2 \epsilon \Omega \omega^2 \sin (\Omega t) x $$ Here $\epsilon \ll 1$ and $\Omega \gg \omega$. I want to tranform the above to this form, $$ \dddot x = -\Omega_{eff}^2 \dot x $$ Is there a way to transform such an equation. If yes can someone cite me some reference?","Say I have a differential equation, $$ \dddot x = 4 \omega^2 (1 + \epsilon \cos (\Omega t)) \dot x-2 \epsilon \Omega \omega^2 \sin (\Omega t) x $$ Here $\epsilon \ll 1$ and $\Omega \gg \omega$. I want to tranform the above to this form, $$ \dddot x = -\Omega_{eff}^2 \dot x $$ Is there a way to transform such an equation. If yes can someone cite me some reference?",,['ordinary-differential-equations']
57,Higher degree First order differential equation,Higher degree First order differential equation,,I'm having trouble solving this type of differential equation: $$(x^2y'-xy)\left(y'+\frac{x}{y}\right)=2y'$$ I tried using clairaut's equation or using equations solvable for P but I haven't got anything to work. I tried factoring and isolating variables but it seems impossible any ideas on how to solve it?,I'm having trouble solving this type of differential equation: $$(x^2y'-xy)\left(y'+\frac{x}{y}\right)=2y'$$ I tried using clairaut's equation or using equations solvable for P but I haven't got anything to work. I tried factoring and isolating variables but it seems impossible any ideas on how to solve it?,,"['integration', 'ordinary-differential-equations', 'derivatives', 'factoring']"
58,Poincaré-Bendixson Theorem and diffusion,Poincaré-Bendixson Theorem and diffusion,,"In this paper , the author proved the existence of a global compact attractor of the following system And then the author deduced the existence of a periodic solution: I feel like he somehow used a variant of the Poincaré-Bendixson Theorem for systems with diffusion despite the infinite dimensional aspect of such systems. I know that the the Poincaré-Bendixson Theorem fails even for dimension 3 ODE systems. Maybe here the diffusion does not affect the behavior. Do such Poincaré-Bendixson type theorems exist?","In this paper , the author proved the existence of a global compact attractor of the following system And then the author deduced the existence of a periodic solution: I feel like he somehow used a variant of the Poincaré-Bendixson Theorem for systems with diffusion despite the infinite dimensional aspect of such systems. I know that the the Poincaré-Bendixson Theorem fails even for dimension 3 ODE systems. Maybe here the diffusion does not affect the behavior. Do such Poincaré-Bendixson type theorems exist?",,"['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'dynamical-systems', 'periodic-functions']"
59,differential equation with random variable - approach with Ito?,differential equation with random variable - approach with Ito?,,"Assuming i have a random variable $F(t-t_1)$ for each $t-t_1$ that is distributed like $\mathcal{N}(0,f(t-t_1))$,  ($f(t-t_1)$ beeing a normal function)and i have the following differential equation: $\dot{c}(t)=\int_0^tF(t-t_1)c(t_1)\mathrm{d}t_1$ How do I approach something like that? Thanks already!","Assuming i have a random variable $F(t-t_1)$ for each $t-t_1$ that is distributed like $\mathcal{N}(0,f(t-t_1))$,  ($f(t-t_1)$ beeing a normal function)and i have the following differential equation: $\dot{c}(t)=\int_0^tF(t-t_1)c(t_1)\mathrm{d}t_1$ How do I approach something like that? Thanks already!",,"['ordinary-differential-equations', 'random-variables']"
60,Multivariable Piecewise continuous function on a compact set in $\mathbb{R}^{n}$,Multivariable Piecewise continuous function on a compact set in,\mathbb{R}^{n},"In optimal control and differential equations we often deal with piecewise continuous functions. Here I want to extend the piecewise continuous function from an interval in $\mathbb{R}$ to a compact set in $\mathbb{R}^{n}$ for which I think there should be a standard definition. But I searched online for a long time, yet still could not find one. Recall that a one-variable piecewise continuous function is defined as: A real-valued function $u(t)$, $t_{0}\le t\le t_{f}$ , is said to be piecewise continuous, denoted $u\in\hat{C}[t_{0},t_{f}]$, if there is a finite (irreducible) partition $t_{0}=\theta_{0}<\theta_{1}<\cdot\cdot\cdot<\theta_{N}<\theta_{N+1}=t_{f}$ such that $u$ may be regarded as a continuous function in $[\theta_{k},\theta_{k+1}]$ for each $k=0,1,...,N$. So my question is if I can define a multi-variable piecewise continuous function on a compact set $E\subset\mathbb{R}^{n}$ as follows: A real-valued function $u(x)$, $x\in E$ is said to be piecewise continuous if there is a finite partition of $E$ on each coordinate such that $u$ may be regarded as a continuous function on each polytope. The source of the question is from the book of Chachuat: Nonlinear and Dynamic Optimization, in theorem A.55 on page xv of appendix A.5: ""Suppose that $f(t,x,p)$ is piecewise continuous in $(t,x,p)$..."", so $f(t,x,p)$ is multivariate piecewise continuous function but Chachuat does not give the explicit definition.","In optimal control and differential equations we often deal with piecewise continuous functions. Here I want to extend the piecewise continuous function from an interval in $\mathbb{R}$ to a compact set in $\mathbb{R}^{n}$ for which I think there should be a standard definition. But I searched online for a long time, yet still could not find one. Recall that a one-variable piecewise continuous function is defined as: A real-valued function $u(t)$, $t_{0}\le t\le t_{f}$ , is said to be piecewise continuous, denoted $u\in\hat{C}[t_{0},t_{f}]$, if there is a finite (irreducible) partition $t_{0}=\theta_{0}<\theta_{1}<\cdot\cdot\cdot<\theta_{N}<\theta_{N+1}=t_{f}$ such that $u$ may be regarded as a continuous function in $[\theta_{k},\theta_{k+1}]$ for each $k=0,1,...,N$. So my question is if I can define a multi-variable piecewise continuous function on a compact set $E\subset\mathbb{R}^{n}$ as follows: A real-valued function $u(x)$, $x\in E$ is said to be piecewise continuous if there is a finite partition of $E$ on each coordinate such that $u$ may be regarded as a continuous function on each polytope. The source of the question is from the book of Chachuat: Nonlinear and Dynamic Optimization, in theorem A.55 on page xv of appendix A.5: ""Suppose that $f(t,x,p)$ is piecewise continuous in $(t,x,p)$..."", so $f(t,x,p)$ is multivariate piecewise continuous function but Chachuat does not give the explicit definition.",,"['real-analysis', 'ordinary-differential-equations', 'optimal-control']"
61,Sum of Homogeneous and Particular Solution Is the Full Solution Set?,Sum of Homogeneous and Particular Solution Is the Full Solution Set?,,"I was hoping if someone could point me to a (hopefully) not too technical proof of why the sum of the homogeneous and particular solution to a (linear) differential equation yields the full solution set that to that differential equation. Intuitively, I understand this as follows (from the answer to this question): ""Suppose I find one particular solution $x_{(p,1)}(t)$ while my friend finds another one $x_{(p,2)}(t)$. Then the difference $x_c(t)=x_{(p,2)}(t)−x_{(p,1)}(t)$ will satisfy the homogeneous equation, since you get $…=f(t)−f(t)=0…$ in the right-hand side when substituting. So two different particular solutions to my ODE can't be arbitrarily different; they can only differ by a solution to the homogeneous ODE."" I am trying to find a way this can be formalized. I am not looking for a proof on how the sum of the homogeneous and particular solutions satisfy the original differential equation - I am trying to see why this is the full solution set. Thanks!","I was hoping if someone could point me to a (hopefully) not too technical proof of why the sum of the homogeneous and particular solution to a (linear) differential equation yields the full solution set that to that differential equation. Intuitively, I understand this as follows (from the answer to this question): ""Suppose I find one particular solution $x_{(p,1)}(t)$ while my friend finds another one $x_{(p,2)}(t)$. Then the difference $x_c(t)=x_{(p,2)}(t)−x_{(p,1)}(t)$ will satisfy the homogeneous equation, since you get $…=f(t)−f(t)=0…$ in the right-hand side when substituting. So two different particular solutions to my ODE can't be arbitrarily different; they can only differ by a solution to the homogeneous ODE."" I am trying to find a way this can be formalized. I am not looking for a proof on how the sum of the homogeneous and particular solutions satisfy the original differential equation - I am trying to see why this is the full solution set. Thanks!",,['ordinary-differential-equations']
62,Continuity of Green's function and its derivatives,Continuity of Green's function and its derivatives,,"We have a differential equation: $$a_n(x)\frac{d^n y}{dx^n} + a_{n-1}(x)\frac{d^{n-1} y}{dx^{n-1}} + \dots + a_1(x)\frac{dy}{dx} + a_0(x)y = \delta(x-z) $$ And this is satisfied by the Green's function $G(x,z)$. I've understood this far. However, I don't understand the following paragraph (Riley Hobson and Bence, Mathematical methods for Physics and Engineering): My questions are as follows: Why is $\frac{d^n G(x,z)}{dx^n}$ infinity at $x = z$? Why are all the derivatives of order less than $n-1$ continuous? Thank you.","We have a differential equation: $$a_n(x)\frac{d^n y}{dx^n} + a_{n-1}(x)\frac{d^{n-1} y}{dx^{n-1}} + \dots + a_1(x)\frac{dy}{dx} + a_0(x)y = \delta(x-z) $$ And this is satisfied by the Green's function $G(x,z)$. I've understood this far. However, I don't understand the following paragraph (Riley Hobson and Bence, Mathematical methods for Physics and Engineering): My questions are as follows: Why is $\frac{d^n G(x,z)}{dx^n}$ infinity at $x = z$? Why are all the derivatives of order less than $n-1$ continuous? Thank you.",,"['ordinary-differential-equations', 'greens-function']"
63,Why solution is nonzero in the outer region,Why solution is nonzero in the outer region,,"Consider this equation in the limit of $A\rightarrow\infty$, $A$ is a positive real number. $$ y^{(4)} + i A (y''- y) = e^{-\sqrt{-i A} x}. $$  Boundary conditions are homogeneous: $y(0) = y(1) = y'(0) = y'(1) = 0$. $i$ is the imaginary unit. Edit Start A similar problem, but without the complex number is ($A$ is still positive and real and approaches the positive infinity) $$ y^{(4)} - A (y''- y) = e^{-\sqrt{A} x}. $$  Boundary conditions are homogeneous: $y(0) = y(1) = y'(0) = y'(1) = 0$. I suggest to start from this equation instead--and I will consider the question as answered :-) Edit End Note that $-\sqrt{-i} = -1/\sqrt{2}+i/\sqrt{2}$. Apparently the RHS decreases faster and faster as $A$ increases, and is nonzero only within a thin layer next to $x=0$. For this reason I expect a boundary layer behavior of the RHS. Because RHS is the source term, I expect boundary layer behavior of the solution $y(x)$ as well. Define $\epsilon = \frac{1}{\sqrt{A}}$ to convert the ODE into a singular perturbation problem. $$ \epsilon^2 y^{(4)} + (i ) (y''- y) = \epsilon^2 e^{-\sqrt{-i} x/\epsilon} $$ Outer solution The RHS is exponentially small thus varnishes as $\epsilon\rightarrow0$, so the ODE in the outer region is simplified: $$ \epsilon^2 y^{(4)}_o + (i ) (y_o''- y_o) = 0 $$ Because the (outer) boundary conditions at $x = 1$ are homogeneous, the outer solution is identically zero. As an example, I calculated the solution numerically using bvp4c for $A = 4000$, the figure for the real and imaginary parts of $y(x)$, and of $rhs(x)$ are shown below. The boundary layer behavior is there for $rhs(x)$, but both $real(y)$ and $imag(y)$ appear to vary linearly in the outer region , you need to scale $imag(y)$ in $y$ to see its linear behavior. It also appears $y(x)$ has a boundary layer at $x = 1$ as they change pretty quickly there, but I am not sure. This puzzles me a lot and any help is appreciated. Below is the MATLAB code used to obtain the above figure. A = 4000; x = linspace(0, 1, 401);  fsol = testSEode(A); tmp  = deval(fsol, x); ysol = tmp(1, :);  rhs = exp(-sqrt(-1j*A)*x); %% clf subplot(121) plot(x, real(ysol), 'r-', x, imag(ysol), 'b-') xlabel('x') ylabel('y(x)') legend('real(y)', 'imag(y)') title(sprintf('A = %g, y', A))  subplot(122) plot(x, real(rhs), 'r', x, imag(rhs), 'b') xlabel('x') ylabel('rhs(x)') legend('real(rhs)', 'imag(rhs)') title(sprintf('A = %g, rhs', A)) The function testSEode is below function sol = testSEode(A_in) global A A = A_in; solinit = bvpinit(linspace(0, 1, 5), @initialGuess); options = bvpset('Stats','off','RelTol',1e-5); sol = bvp4c(@RHS, @boundaryCondition, solinit, options); end  function dydx = RHS(x, y) global A rhs = exp(-sqrt(-1j*A)*x);  dydx =  [y(2)          y(3)          y(4)          rhs-1j*A*(y(3) - y(1))]; end  function res = boundaryCondition(ya, yb) % B.C., homogeneous res = [ ya(1)         yb(1)         ya(2)         yb(2)]; end  function v = initialGuess(x) % initial guess, not important here v = [x.*(1-x) 0 0 0]; end","Consider this equation in the limit of $A\rightarrow\infty$, $A$ is a positive real number. $$ y^{(4)} + i A (y''- y) = e^{-\sqrt{-i A} x}. $$  Boundary conditions are homogeneous: $y(0) = y(1) = y'(0) = y'(1) = 0$. $i$ is the imaginary unit. Edit Start A similar problem, but without the complex number is ($A$ is still positive and real and approaches the positive infinity) $$ y^{(4)} - A (y''- y) = e^{-\sqrt{A} x}. $$  Boundary conditions are homogeneous: $y(0) = y(1) = y'(0) = y'(1) = 0$. I suggest to start from this equation instead--and I will consider the question as answered :-) Edit End Note that $-\sqrt{-i} = -1/\sqrt{2}+i/\sqrt{2}$. Apparently the RHS decreases faster and faster as $A$ increases, and is nonzero only within a thin layer next to $x=0$. For this reason I expect a boundary layer behavior of the RHS. Because RHS is the source term, I expect boundary layer behavior of the solution $y(x)$ as well. Define $\epsilon = \frac{1}{\sqrt{A}}$ to convert the ODE into a singular perturbation problem. $$ \epsilon^2 y^{(4)} + (i ) (y''- y) = \epsilon^2 e^{-\sqrt{-i} x/\epsilon} $$ Outer solution The RHS is exponentially small thus varnishes as $\epsilon\rightarrow0$, so the ODE in the outer region is simplified: $$ \epsilon^2 y^{(4)}_o + (i ) (y_o''- y_o) = 0 $$ Because the (outer) boundary conditions at $x = 1$ are homogeneous, the outer solution is identically zero. As an example, I calculated the solution numerically using bvp4c for $A = 4000$, the figure for the real and imaginary parts of $y(x)$, and of $rhs(x)$ are shown below. The boundary layer behavior is there for $rhs(x)$, but both $real(y)$ and $imag(y)$ appear to vary linearly in the outer region , you need to scale $imag(y)$ in $y$ to see its linear behavior. It also appears $y(x)$ has a boundary layer at $x = 1$ as they change pretty quickly there, but I am not sure. This puzzles me a lot and any help is appreciated. Below is the MATLAB code used to obtain the above figure. A = 4000; x = linspace(0, 1, 401);  fsol = testSEode(A); tmp  = deval(fsol, x); ysol = tmp(1, :);  rhs = exp(-sqrt(-1j*A)*x); %% clf subplot(121) plot(x, real(ysol), 'r-', x, imag(ysol), 'b-') xlabel('x') ylabel('y(x)') legend('real(y)', 'imag(y)') title(sprintf('A = %g, y', A))  subplot(122) plot(x, real(rhs), 'r', x, imag(rhs), 'b') xlabel('x') ylabel('rhs(x)') legend('real(rhs)', 'imag(rhs)') title(sprintf('A = %g, rhs', A)) The function testSEode is below function sol = testSEode(A_in) global A A = A_in; solinit = bvpinit(linspace(0, 1, 5), @initialGuess); options = bvpset('Stats','off','RelTol',1e-5); sol = bvp4c(@RHS, @boundaryCondition, solinit, options); end  function dydx = RHS(x, y) global A rhs = exp(-sqrt(-1j*A)*x);  dydx =  [y(2)          y(3)          y(4)          rhs-1j*A*(y(3) - y(1))]; end  function res = boundaryCondition(ya, yb) % B.C., homogeneous res = [ ya(1)         yb(1)         ya(2)         yb(2)]; end  function v = initialGuess(x) % initial guess, not important here v = [x.*(1-x) 0 0 0]; end",,"['ordinary-differential-equations', 'perturbation-theory']"
64,"Is there a sensible way of composing ""non-instantaneous"" functions?","Is there a sensible way of composing ""non-instantaneous"" functions?",,"If we try to implement a function $\mathbb{R}^n \rightarrow \mathbb{R}$ using real-world devices gadgets, there'll inevitably be a delay between changing the input values and seeing the desired change in the output value. We could try to model this using a first-order differential equation. Think of $\mathbb{R}^n$ as a smooth manifold $A$ and a $\mathbb{R}$ as a smooth manifold $B$. we want to define that a ""non-instantaneous map"" $f : A \rightarrow B$ is a way of assigning to each $a \in A$ a corresponding autonomous first-order differential equation $f(a)$ that moves us non-instantaneously toward the desired output value in $B$. We can formalize this as follows: Definition. Given smooth manifolds $A$ and $B$, a non-instantaneous function $A \rightarrow B$ is a smooth function $f:A \times B \rightarrow TB$, where $TB$ is the tangent bundle of $B$, such that for all $a \in A$, the function $f(a,-) : B \rightarrow TB$ is a section of the bundle projection $TB \rightarrow B$. By currying $f$ into the form $A \rightarrow (B \rightarrow TB),$ we see that this really just a clever way of assigning a vector field $f(a)$ to each $a \in A$, and since vector fields are basically first-order autonomous differential equations in disguise, this is a reasonable definition. Now, I guess there's a reasonable way to compose such things, yielding a category. But honestly, I cannot see it. (In fact, I can't even see how to implement identity morphisms.) Question. Is there a sensible way of composing non-instantaneous functions as defined above? If not, what is the correct definition of non-instantaneous functions, such that composing them becomes possible and we get a category?","If we try to implement a function $\mathbb{R}^n \rightarrow \mathbb{R}$ using real-world devices gadgets, there'll inevitably be a delay between changing the input values and seeing the desired change in the output value. We could try to model this using a first-order differential equation. Think of $\mathbb{R}^n$ as a smooth manifold $A$ and a $\mathbb{R}$ as a smooth manifold $B$. we want to define that a ""non-instantaneous map"" $f : A \rightarrow B$ is a way of assigning to each $a \in A$ a corresponding autonomous first-order differential equation $f(a)$ that moves us non-instantaneously toward the desired output value in $B$. We can formalize this as follows: Definition. Given smooth manifolds $A$ and $B$, a non-instantaneous function $A \rightarrow B$ is a smooth function $f:A \times B \rightarrow TB$, where $TB$ is the tangent bundle of $B$, such that for all $a \in A$, the function $f(a,-) : B \rightarrow TB$ is a section of the bundle projection $TB \rightarrow B$. By currying $f$ into the form $A \rightarrow (B \rightarrow TB),$ we see that this really just a clever way of assigning a vector field $f(a)$ to each $a \in A$, and since vector fields are basically first-order autonomous differential equations in disguise, this is a reasonable definition. Now, I guess there's a reasonable way to compose such things, yielding a category. But honestly, I cannot see it. (In fact, I can't even see how to implement identity morphisms.) Question. Is there a sensible way of composing non-instantaneous functions as defined above? If not, what is the correct definition of non-instantaneous functions, such that composing them becomes possible and we get a category?",,"['calculus', 'ordinary-differential-equations', 'category-theory', 'differential-topology', 'vector-fields']"
65,Adapting Solution of Second Order Differential Equations,Adapting Solution of Second Order Differential Equations,,"I am interested in solving the following system of differential equations \begin{equation} 	\ddot{z}_j = \sum_{k\neq j}^n\frac{2\dot{z}_j\dot{z}_k}{z_j-z_k},\qquad \forall j\in\{1,\dots,n\}, \end{equation} for $n$ even and initial conditions satisfying \begin{equation} 	{z}_{j+n/2}(0) = {z}_{j}^{-1}(0),\qquad \forall j\in\{1,\dots,n/2\}. \end{equation} I have found the following solution that works except for the fact that it does not satisfy the initial conditions (for $n>2$): \begin{equation} 	{z}_{j}(t) = a + b e^{2\pi i \frac{j}{n}}(t-c)^{\frac{1}{n}}. \end{equation} I am looking for a way to modify this solution in order to be able to satisfy the initial condition, but haven't been successful. Any hints or references would be very welcome.","I am interested in solving the following system of differential equations \begin{equation} 	\ddot{z}_j = \sum_{k\neq j}^n\frac{2\dot{z}_j\dot{z}_k}{z_j-z_k},\qquad \forall j\in\{1,\dots,n\}, \end{equation} for $n$ even and initial conditions satisfying \begin{equation} 	{z}_{j+n/2}(0) = {z}_{j}^{-1}(0),\qquad \forall j\in\{1,\dots,n/2\}. \end{equation} I have found the following solution that works except for the fact that it does not satisfy the initial conditions (for $n>2$): \begin{equation} 	{z}_{j}(t) = a + b e^{2\pi i \frac{j}{n}}(t-c)^{\frac{1}{n}}. \end{equation} I am looking for a way to modify this solution in order to be able to satisfy the initial condition, but haven't been successful. Any hints or references would be very welcome.",,"['ordinary-differential-equations', 'initial-value-problems']"
66,Questions about stability in the sense of Lyapunov,Questions about stability in the sense of Lyapunov,,"I have two questions that are related to stability in the sense of Lyapunov. Is a system with multiple poles on the imaginary axis (e.g. double pole at $z=0$ or double pole at $z=i$) unstable in the sense of Lyapunov? From the example of $y''=0$ I would think that this linear system is unstable. When using the Linearization method from Lyapunov for investigating the stability of a nonlinear system, I know that if the linear system is asymptotically stable in the equilibrium point, then the equilibrium point of the nonlinear system is also asymptotically stable. If the linear system is unstable at the equilibrium point, then the equilibrium point of the nonlinear system is also unstable. It is said that for the case in which the linear system is marginally stable at the equilibrium point, then the linearization method is indecisive. Does the case from question 1 belong to the indecisive case, or would it imply that the nonlinear system is unstable at the equilibrium point?","I have two questions that are related to stability in the sense of Lyapunov. Is a system with multiple poles on the imaginary axis (e.g. double pole at $z=0$ or double pole at $z=i$) unstable in the sense of Lyapunov? From the example of $y''=0$ I would think that this linear system is unstable. When using the Linearization method from Lyapunov for investigating the stability of a nonlinear system, I know that if the linear system is asymptotically stable in the equilibrium point, then the equilibrium point of the nonlinear system is also asymptotically stable. If the linear system is unstable at the equilibrium point, then the equilibrium point of the nonlinear system is also unstable. It is said that for the case in which the linear system is marginally stable at the equilibrium point, then the linearization method is indecisive. Does the case from question 1 belong to the indecisive case, or would it imply that the nonlinear system is unstable at the equilibrium point?",,"['ordinary-differential-equations', 'nonlinear-system', 'stability-in-odes', 'stability-theory']"
67,Types of Differential Equation Questions,Types of Differential Equation Questions,,"I am self-studying linear algebra right now, and most of the ""solve the differential equation"" questions I see are somewhat in the form: $y'' + y' - 6y = 8e^{5x}$ (in terms of y, y'', etc) I understand these problems and I am usually able to solve them. However, I came to a question on a practice exam that is not in the same form and I am not exactly sure how to go about solving it.  I am unsure if this is the same type of question, or something entirely different. Find a particular solution to: $(\frac{d}{dx} + 1)^3(\frac{d}{dx} - 1) y(x) = -240x^2e^{-x} + 120e^{-x}$ The given solution is: $2x^5e^{-x} + 5x^4e^{-x}$ If someone could provide an explanation/walkthrough for this differential equation that would be incredibly helpful, so I can learn how to tackle similar problems of the same form in the future. Any help would be appreciated, thank you! Attempt at solution: We have $(r+1)^3(r-1)$ Which means we have the values -1 with a multiplicity of 3 and also the value 1. This is the general solution I came up with, unsure of how to reach a particular solution. $c_1e^t + c_2e^{-t} + c_3te^{-t} + c_4t^2e^{-t}$","I am self-studying linear algebra right now, and most of the ""solve the differential equation"" questions I see are somewhat in the form: $y'' + y' - 6y = 8e^{5x}$ (in terms of y, y'', etc) I understand these problems and I am usually able to solve them. However, I came to a question on a practice exam that is not in the same form and I am not exactly sure how to go about solving it.  I am unsure if this is the same type of question, or something entirely different. Find a particular solution to: $(\frac{d}{dx} + 1)^3(\frac{d}{dx} - 1) y(x) = -240x^2e^{-x} + 120e^{-x}$ The given solution is: $2x^5e^{-x} + 5x^4e^{-x}$ If someone could provide an explanation/walkthrough for this differential equation that would be incredibly helpful, so I can learn how to tackle similar problems of the same form in the future. Any help would be appreciated, thank you! Attempt at solution: We have $(r+1)^3(r-1)$ Which means we have the values -1 with a multiplicity of 3 and also the value 1. This is the general solution I came up with, unsure of how to reach a particular solution. $c_1e^t + c_2e^{-t} + c_3te^{-t} + c_4t^2e^{-t}$",,"['linear-algebra', 'integration', 'ordinary-differential-equations', 'derivatives']"
68,How to find stability of fixed point of a PDE?,How to find stability of fixed point of a PDE?,,"I have a system of coupled PDEs which have both time and spatial dependence with the form: \begin{align*} a_t &= D_aa_{xx}+f(a, b) \\ b_t &= D_bb_{xx}+g(a, b) \end{align*} where $f(a, b)$ and $g(a, b)$ are linear functions in terms of $a$ and $b$ and $D_a$ and $D_b$ are diffusion coefficients. When there is no spatial dependence I can easily calculate the fixed points, find the Jacobian and determine the stability of the fixed points. My question is then, how can I generalise this method when I have spatial dependence too? I was told I could Fourier Transform the system and turn it into a set of ODEs so that I have \begin{align*} \tilde{a}_t &= -k^2D_a\tilde{a}+f(\tilde{a}, \tilde{b}) \\ \tilde{b}_t &= -k^2D_b\tilde{b}+g(\tilde{a}, \tilde{b}) \end{align*} and find the Jacobian of this. However, I struggle to see how finding the Jacobian of the Fourier transformed equations could be related to the stability of my system? I was also told that the Jacobian of my FT equations is equal to the Jacobian of my non transformed system though I can't figure out how. Also: $$\tilde{a}(k, t) = \int a(x, t)e^{ikx}\,\mathrm{d}x$$","I have a system of coupled PDEs which have both time and spatial dependence with the form: \begin{align*} a_t &= D_aa_{xx}+f(a, b) \\ b_t &= D_bb_{xx}+g(a, b) \end{align*} where $f(a, b)$ and $g(a, b)$ are linear functions in terms of $a$ and $b$ and $D_a$ and $D_b$ are diffusion coefficients. When there is no spatial dependence I can easily calculate the fixed points, find the Jacobian and determine the stability of the fixed points. My question is then, how can I generalise this method when I have spatial dependence too? I was told I could Fourier Transform the system and turn it into a set of ODEs so that I have \begin{align*} \tilde{a}_t &= -k^2D_a\tilde{a}+f(\tilde{a}, \tilde{b}) \\ \tilde{b}_t &= -k^2D_b\tilde{b}+g(\tilde{a}, \tilde{b}) \end{align*} and find the Jacobian of this. However, I struggle to see how finding the Jacobian of the Fourier transformed equations could be related to the stability of my system? I was also told that the Jacobian of my FT equations is equal to the Jacobian of my non transformed system though I can't figure out how. Also: $$\tilde{a}(k, t) = \int a(x, t)e^{ikx}\,\mathrm{d}x$$",,"['ordinary-differential-equations', 'partial-differential-equations', 'nonlinear-system', 'fixed-points']"
69,Comparison of Runge-Kutta and Predictor-Corrector-methods,Comparison of Runge-Kutta and Predictor-Corrector-methods,,"I just have a short question about something: Which method calculates the more exact result for a differential equation? The Runge-Kutta-method with a constant step-size h or the predictor-corrector-method with the Adams-Bashford- and Adams-Moulton-method? I use the $P(EC)^mE$ method. Does this calculate a better result than the Runge-Kutta-method, even if $m=0$? I need no proof for that, just the information for a better understanding. I didn't find a comparison, yet.","I just have a short question about something: Which method calculates the more exact result for a differential equation? The Runge-Kutta-method with a constant step-size h or the predictor-corrector-method with the Adams-Bashford- and Adams-Moulton-method? I use the $P(EC)^mE$ method. Does this calculate a better result than the Runge-Kutta-method, even if $m=0$? I need no proof for that, just the information for a better understanding. I didn't find a comparison, yet.",,"['ordinary-differential-equations', 'numerical-methods']"
70,Find the differential equation of $\sqrt{1-x^2}+\sqrt{1-y^2}=c(x-y)$. [closed],Find the differential equation of . [closed],\sqrt{1-x^2}+\sqrt{1-y^2}=c(x-y),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I tried to find the differential equation of the following  $$\sqrt{1-x^2}+\sqrt{1-y^2}=c(x-y)$$ where $c$ is arbitrary constant. I would like to show what progress I have made so far. Let $x=\cos \theta$ and $y=\sin \theta$. Then the given equation becomes  $\sin \theta +\cos \theta = c(\sin \theta -\cos \theta)$ $\Rightarrow c=\tan(\frac{\pi}{4}+\theta)$. Differentiating both sides we get  $$0=\sec^2 (\frac{\pi}{4}+\theta)d\theta \,.$$ Now what ? Please show me the path. I don't know if this problem has been solved earlier or not. If solved before, kindly provide me the link. P.S. Many many thanks to you all. Finally the solution I have found. This question is now solved completely.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I tried to find the differential equation of the following  $$\sqrt{1-x^2}+\sqrt{1-y^2}=c(x-y)$$ where $c$ is arbitrary constant. I would like to show what progress I have made so far. Let $x=\cos \theta$ and $y=\sin \theta$. Then the given equation becomes  $\sin \theta +\cos \theta = c(\sin \theta -\cos \theta)$ $\Rightarrow c=\tan(\frac{\pi}{4}+\theta)$. Differentiating both sides we get  $$0=\sec^2 (\frac{\pi}{4}+\theta)d\theta \,.$$ Now what ? Please show me the path. I don't know if this problem has been solved earlier or not. If solved before, kindly provide me the link. P.S. Many many thanks to you all. Finally the solution I have found. This question is now solved completely.",,['ordinary-differential-equations']
71,How to classify the the critical points of this nonlinear system?,How to classify the the critical points of this nonlinear system?,,"Consider the nonlinear system: \begin{align}  \dot x &= -x - \frac{y}{\log \sqrt{x^2 + y^2}} \\  \dot y &= -y + \frac{x}{\log \sqrt{x^2 + y^2}} \end{align} a) Show that the origin is a stable node for linearized system: Attempt at the solution: The Jacobian is  $$ J(x, y) = \begin{bmatrix} \frac{x y}{(x^2 + y^2) \log^2{\sqrt{x^2 + y^2}}} - 1 & \frac{y^2}{(x^2 + y^2) \log^2{\sqrt{x^2 + y^2}}}- \frac{1}{\log\sqrt{x^2 + y^2}} \\ \frac{1}{\log\sqrt{x^2 + y^2}} - \frac{x^2}{(x^2 + y^2) \log^2\sqrt{x^2 + y^2}} &  -\frac{x y}{(x^2 + y^2) \log^2\sqrt{x^2 + y^2}} - 1 \\ \end{bmatrix}. $$ As we approach $(0, 0)$ it appears that $J$ approaches  $$ \begin{bmatrix} -1 & 0 \\  0  & -1 \\ \end{bmatrix}, $$ if this is true then it shows $(0, 0)$ is a stable node. However, I'm unsure how to show this rigorously. b) Show that the origin of the nonlinear system is not a stable node but rather a stable spiral. Attempt at the solution: I am unsure how to proceed for this part. Thanks for your help.","Consider the nonlinear system: \begin{align}  \dot x &= -x - \frac{y}{\log \sqrt{x^2 + y^2}} \\  \dot y &= -y + \frac{x}{\log \sqrt{x^2 + y^2}} \end{align} a) Show that the origin is a stable node for linearized system: Attempt at the solution: The Jacobian is  $$ J(x, y) = \begin{bmatrix} \frac{x y}{(x^2 + y^2) \log^2{\sqrt{x^2 + y^2}}} - 1 & \frac{y^2}{(x^2 + y^2) \log^2{\sqrt{x^2 + y^2}}}- \frac{1}{\log\sqrt{x^2 + y^2}} \\ \frac{1}{\log\sqrt{x^2 + y^2}} - \frac{x^2}{(x^2 + y^2) \log^2\sqrt{x^2 + y^2}} &  -\frac{x y}{(x^2 + y^2) \log^2\sqrt{x^2 + y^2}} - 1 \\ \end{bmatrix}. $$ As we approach $(0, 0)$ it appears that $J$ approaches  $$ \begin{bmatrix} -1 & 0 \\  0  & -1 \\ \end{bmatrix}, $$ if this is true then it shows $(0, 0)$ is a stable node. However, I'm unsure how to show this rigorously. b) Show that the origin of the nonlinear system is not a stable node but rather a stable spiral. Attempt at the solution: I am unsure how to proceed for this part. Thanks for your help.",,"['ordinary-differential-equations', 'nonlinear-system', 'stability-in-odes', 'stability-theory']"
72,General method for solving recursive linear ordinary differential equations for analytic functions,General method for solving recursive linear ordinary differential equations for analytic functions,,"Are there general methods to solve recursive ordinary differential equations? Consider the following two kinds of equations. Explicit recursion: Solve for $f_n(x)$ in $$\frac{\mathrm{d}\left(f_n(x)p(x)\right)}{\mathrm{d}x} = f_{n-1}(x)q(x) + r(x)\\ f_0(x)=h(x)$$ where $p(x), q(x), r(x), h(x)$ and $f_i(x), \forall i \in \mathbb{N}$ are real analytic functions. Assume the equation have solution(s). Implicit recursion: Consider the same set of equations, with a more general implicit acyclic recursive relation $ m(n): \mathbb{N} \rightarrow \mathbb{N}$ , instead of the explicit recursion $n \rightarrow n-1$ as in the first equation. Namely: $$\frac{\mathrm{d}\left(f_n(x)p(x)\right)}{\mathrm{d}x} = f_{m(n)}(x)q(x) + r(x)$$ Is there a general method of solving these? If not, is there a proof (apart from the ""obvious"" intuition that you would need to know the structure of the recursive map to actually solve it, so a general method of solving is a no-no) that such a general method does not exist?","Are there general methods to solve recursive ordinary differential equations? Consider the following two kinds of equations. Explicit recursion: Solve for in where and are real analytic functions. Assume the equation have solution(s). Implicit recursion: Consider the same set of equations, with a more general implicit acyclic recursive relation , instead of the explicit recursion as in the first equation. Namely: Is there a general method of solving these? If not, is there a proof (apart from the ""obvious"" intuition that you would need to know the structure of the recursive map to actually solve it, so a general method of solving is a no-no) that such a general method does not exist?","f_n(x) \frac{\mathrm{d}\left(f_n(x)p(x)\right)}{\mathrm{d}x} = f_{n-1}(x)q(x) + r(x)\\
f_0(x)=h(x) p(x), q(x), r(x), h(x) f_i(x), \forall i \in \mathbb{N}  m(n): \mathbb{N} \rightarrow \mathbb{N} n \rightarrow n-1 \frac{\mathrm{d}\left(f_n(x)p(x)\right)}{\mathrm{d}x} = f_{m(n)}(x)q(x) + r(x)","['functional-analysis', 'analysis', 'ordinary-differential-equations', 'recurrence-relations', 'computability']"
73,Frobenius’ Method to Solve an ODE (Hydrogen Atom - Radial Equation),Frobenius’ Method to Solve an ODE (Hydrogen Atom - Radial Equation),,"My goal is to find two linearly independent solutions to the ODE $$ r^2\frac{d^2R}{dr^2}+2r\frac{dR}{dr}+[r^2+\lambda r-l(l+1)]R=0 $$ in the interval $[0,\infty)\ni r$, where $R=R(r)$, and $\lambda\in\mathbb{R}_+$ and $l\in\mathbb{Z}_0$ are fixed constants. Just to put in context my problem, this ODE determined the radial component of the wavefunction of an electron in an atom. The standard approach (e.g. here ) is to investigate the asymptotic behaviour of the equation for large and small $r$, factor out this behaviour and then use a series to try to find the solution of what's left. Nonetheless, I'm looking for an answer of the form: ''The general solution to this ODE is given by $R(r)=c_1y_1(r)+c_2y_2$, but since $R(0)$ must be well-defined and $y_2(r)\to-\infty$ as $r\to0$, then $c_2=0$ for the answer to be physically acceptable''. I know that the ''physically acceptable'' solution is given by $$ y_1\propto\left(\frac{2r}{n}\right)^le^{-r/n}L^{2l+1}_{n+l}\left(\frac{2r}{n}\right) $$ for all $n\in\mathbb{Z}_+$, and where $L^{2l+1}_{n+l}$ are the associated Laguerre polynomials. My attempt to find $y_1$ and $y_2$ is to use Frobenius’ method, but i keep stuck at some point: since $r=0$ is a regular singular point of the differential equation, there exists at least one solution of the form $$ R(r)=\sum_{k=0}^\infty c_kr^{k+s} $$ where $s$ are indicial roots to be found. Then $$ \frac{dR}{dr}(r)=\sum_{k=0}^\infty (k+s)c_kr^{k+s-1}\quad\text{and}\quad\frac{d^2R}{dr^2}(r)=\sum_{k=0}^\infty (k+s)(k+s-1)c_kr^{k+s-2} $$ and so, substituting in the equation, it yields \begin{align}  0&=r^s\Biggl[c_0[s(s+1)-l(l+1)]+(c_1[(s+1)(s+2)-l(l+1)]+\lambda c_0)r \\  &\quad+\sum_{k=2}^\infty\Bigl(c_k[(k+s)(k+s-1)+2(k+s)-l(l+1)]+c_{k-2}+\lambda c_{k-1}\Bigr)r^k\Biggr]  \end{align} The first term gives me the indicial roots $$ c_0[s(s+1)-l(l+1)]=0\quad\Rightarrow\quad s_1=l\quad\text{and}\quad s_2=-l-1 $$ and because nothing is gained by taking $c_0=0$. Now, since $s_1$ and $s_2$ are distinct and the difference $s_1-s_2=2l+1$ is a positive integer, then there exist two linearly independent solutions of the form $$ R^1(r)=\sum_{k=0}^\infty c_kr^{k+l},\quad c_0\neq0 $$ and $$ R^2(r)=CR^1(r)\ln(r)+\sum_{k=0}^\infty b_kr^{k-l-1} $$ where $C$ is a constant that could be zero (I'm following the book ''Differential Equations with Boundary-Value Problems; DENNIS G. ZILL and MICHAEL R. CULLEN''.) For the second term $$ c_1[(s+1)(s+2)-l(l+1)]+\lambda c_0=0 $$ from where $$ c_1=\frac{l+l^2-\lambda c_0}{2+3l+l^2}\quad\text{for $s_1=l$},\quad\text{and}\quad c_1=\frac{l+l^2-\lambda c_0}{l(l-1)}\quad\text{for $s_2=-l-1$} $$ For $k\geq2$, I have $$ c_k[(k+s)(k+s-1)+2(k+s)-l(l+1)]+c_{k-2}+\lambda c_{k-1}=0 $$ How can I continue from here to find the solution/series $R^1(r)$ and $R^2(r)$ ? How are they ? Is there a better way to proceed in order to find $R^1(r)$ and $R^2(r)$ ? Thanks in advance.","My goal is to find two linearly independent solutions to the ODE $$ r^2\frac{d^2R}{dr^2}+2r\frac{dR}{dr}+[r^2+\lambda r-l(l+1)]R=0 $$ in the interval $[0,\infty)\ni r$, where $R=R(r)$, and $\lambda\in\mathbb{R}_+$ and $l\in\mathbb{Z}_0$ are fixed constants. Just to put in context my problem, this ODE determined the radial component of the wavefunction of an electron in an atom. The standard approach (e.g. here ) is to investigate the asymptotic behaviour of the equation for large and small $r$, factor out this behaviour and then use a series to try to find the solution of what's left. Nonetheless, I'm looking for an answer of the form: ''The general solution to this ODE is given by $R(r)=c_1y_1(r)+c_2y_2$, but since $R(0)$ must be well-defined and $y_2(r)\to-\infty$ as $r\to0$, then $c_2=0$ for the answer to be physically acceptable''. I know that the ''physically acceptable'' solution is given by $$ y_1\propto\left(\frac{2r}{n}\right)^le^{-r/n}L^{2l+1}_{n+l}\left(\frac{2r}{n}\right) $$ for all $n\in\mathbb{Z}_+$, and where $L^{2l+1}_{n+l}$ are the associated Laguerre polynomials. My attempt to find $y_1$ and $y_2$ is to use Frobenius’ method, but i keep stuck at some point: since $r=0$ is a regular singular point of the differential equation, there exists at least one solution of the form $$ R(r)=\sum_{k=0}^\infty c_kr^{k+s} $$ where $s$ are indicial roots to be found. Then $$ \frac{dR}{dr}(r)=\sum_{k=0}^\infty (k+s)c_kr^{k+s-1}\quad\text{and}\quad\frac{d^2R}{dr^2}(r)=\sum_{k=0}^\infty (k+s)(k+s-1)c_kr^{k+s-2} $$ and so, substituting in the equation, it yields \begin{align}  0&=r^s\Biggl[c_0[s(s+1)-l(l+1)]+(c_1[(s+1)(s+2)-l(l+1)]+\lambda c_0)r \\  &\quad+\sum_{k=2}^\infty\Bigl(c_k[(k+s)(k+s-1)+2(k+s)-l(l+1)]+c_{k-2}+\lambda c_{k-1}\Bigr)r^k\Biggr]  \end{align} The first term gives me the indicial roots $$ c_0[s(s+1)-l(l+1)]=0\quad\Rightarrow\quad s_1=l\quad\text{and}\quad s_2=-l-1 $$ and because nothing is gained by taking $c_0=0$. Now, since $s_1$ and $s_2$ are distinct and the difference $s_1-s_2=2l+1$ is a positive integer, then there exist two linearly independent solutions of the form $$ R^1(r)=\sum_{k=0}^\infty c_kr^{k+l},\quad c_0\neq0 $$ and $$ R^2(r)=CR^1(r)\ln(r)+\sum_{k=0}^\infty b_kr^{k-l-1} $$ where $C$ is a constant that could be zero (I'm following the book ''Differential Equations with Boundary-Value Problems; DENNIS G. ZILL and MICHAEL R. CULLEN''.) For the second term $$ c_1[(s+1)(s+2)-l(l+1)]+\lambda c_0=0 $$ from where $$ c_1=\frac{l+l^2-\lambda c_0}{2+3l+l^2}\quad\text{for $s_1=l$},\quad\text{and}\quad c_1=\frac{l+l^2-\lambda c_0}{l(l-1)}\quad\text{for $s_2=-l-1$} $$ For $k\geq2$, I have $$ c_k[(k+s)(k+s-1)+2(k+s)-l(l+1)]+c_{k-2}+\lambda c_{k-1}=0 $$ How can I continue from here to find the solution/series $R^1(r)$ and $R^2(r)$ ? How are they ? Is there a better way to proceed in order to find $R^1(r)$ and $R^2(r)$ ? Thanks in advance.",,"['ordinary-differential-equations', 'partial-differential-equations', 'special-functions', 'quantum-groups']"
74,Chicken population growth,Chicken population growth,,"A chicken starts laying eggs at age 1 year old. Given enough food, a chicken lays 1 egg per day until 8 years old. How often can a population of chickens double in population? Just Curious, Boston.","A chicken starts laying eggs at age 1 year old. Given enough food, a chicken lays 1 egg per day until 8 years old. How often can a population of chickens double in population? Just Curious, Boston.",,['ordinary-differential-equations']
75,How to analyse stability of a limit cycle for this example?,How to analyse stability of a limit cycle for this example?,,"Consider the following system in polar coordinates $$\frac{dr}{dt}=(r^2-1)(2 r \cos(\phi)-1), \\ \frac{d \phi}{dt}=1.$$ The question is if the limit cycle $r=1$ is stable, and what is the region of attraction. Simulations show that this limit cycle is attractive for $0<r<1$, and for $1<r<r+\epsilon$ with sufficiently small $\epsilon>0$. However, I cannot establish a strict proof and analysis.","Consider the following system in polar coordinates $$\frac{dr}{dt}=(r^2-1)(2 r \cos(\phi)-1), \\ \frac{d \phi}{dt}=1.$$ The question is if the limit cycle $r=1$ is stable, and what is the region of attraction. Simulations show that this limit cycle is attractive for $0<r<1$, and for $1<r<r+\epsilon$ with sufficiently small $\epsilon>0$. However, I cannot establish a strict proof and analysis.",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
76,Can we denote a function without specifying its input variable(s)?,Can we denote a function without specifying its input variable(s)?,,"I am in the process of writing my M.Sc. thesis. Coming from an ecology background, I have limited knowledge of mathematical notation. For the purpose of my project I define a differential equation system that I use as a base model upon which I build over the course of the paper, progressively adding complexity. For simplicity, let us say the base model would be: $\frac{d_y}{d_t}=fy+\alpha$ In the simpler instance of the model, the function $f$ can be defined as a constant, e.g. $f=\beta$. However, in more complex instances it is implemented as a function, e.g., $f(y)=\beta y$, but the rest of the model structure remains identical. I do not want to re-expose the whole model every time complexity is built-in, but I want to make it clear that complexity will be built into $f$. Can $f$ be exposed as a function from the get-go? If so, what notation can I use that does not require to specify the input variables? What I mean is if the base model is defined as $\frac{d_y}{d_t}=f(y)y+\alpha$ it induces confusion as to the structure of the simpler instances of the model, when $f=\beta$. I have tried presenting $f$, in the textual description of the equations system, as a function that represents, for instance, the growth rate, and that in the simplest instance of the model it is a constant, but readers have not picked it up and still wonder whether it is a function and, if so, why I do not use the $f()$ in the presentation of the base model. Would anyone have suggestions as to improve the clarity of the model presentation with respect to the introduction of $f$?","I am in the process of writing my M.Sc. thesis. Coming from an ecology background, I have limited knowledge of mathematical notation. For the purpose of my project I define a differential equation system that I use as a base model upon which I build over the course of the paper, progressively adding complexity. For simplicity, let us say the base model would be: $\frac{d_y}{d_t}=fy+\alpha$ In the simpler instance of the model, the function $f$ can be defined as a constant, e.g. $f=\beta$. However, in more complex instances it is implemented as a function, e.g., $f(y)=\beta y$, but the rest of the model structure remains identical. I do not want to re-expose the whole model every time complexity is built-in, but I want to make it clear that complexity will be built into $f$. Can $f$ be exposed as a function from the get-go? If so, what notation can I use that does not require to specify the input variables? What I mean is if the base model is defined as $\frac{d_y}{d_t}=f(y)y+\alpha$ it induces confusion as to the structure of the simpler instances of the model, when $f=\beta$. I have tried presenting $f$, in the textual description of the equations system, as a function that represents, for instance, the growth rate, and that in the simplest instance of the model it is a constant, but readers have not picked it up and still wonder whether it is a function and, if so, why I do not use the $f()$ in the presentation of the base model. Would anyone have suggestions as to improve the clarity of the model presentation with respect to the introduction of $f$?",,"['ordinary-differential-equations', 'functions', 'notation']"
77,"Value at $0$ of solution to ODE, given asymptotic behaviour","Value at  of solution to ODE, given asymptotic behaviour",0,"Consider the differential equation $$\forall x \in \mathbb R\qquad f'(x) =  f(x)^2 -x^2. $$ Every solution that crosses the line $y=x$ where $x>0$ is asymptotic to $y=-x$ as $x$ approaches $+\infty$. The other solutions with $f(0)>0$ are above the former and their limit at $+\infty$ is $+\infty$. I then expect there to be a solution $g$ which is asymptotic to $y=x$ as $x$ approaches $+\infty$. What I'd like to know is $g(0)$ and how to get the result. Well, an explicit expression for $g$ would be better, but I doubt there is one. The only thing I was I able to do so far was making GeoGebra plot some (approximated) solutions, as you can see from the picture.","Consider the differential equation $$\forall x \in \mathbb R\qquad f'(x) =  f(x)^2 -x^2. $$ Every solution that crosses the line $y=x$ where $x>0$ is asymptotic to $y=-x$ as $x$ approaches $+\infty$. The other solutions with $f(0)>0$ are above the former and their limit at $+\infty$ is $+\infty$. I then expect there to be a solution $g$ which is asymptotic to $y=x$ as $x$ approaches $+\infty$. What I'd like to know is $g(0)$ and how to get the result. Well, an explicit expression for $g$ would be better, but I doubt there is one. The only thing I was I able to do so far was making GeoGebra plot some (approximated) solutions, as you can see from the picture.",,"['real-analysis', 'ordinary-differential-equations']"
78,How can I solve -y''-y/|x|=y?,How can I solve -y''-y/|x|=y?,,"How to solve $-y''-\frac{y}{|x|}=y$ . Now $y$ is a function of $x$. And $|x|$ means the absolute value of $x$, that is, $|x|=x$ for $x>0$, and $-x$ for $x<0$. Can somebody please hint at some substitution or refer any text related to these type of ode. Motivation : This ODE comes from quantum mechanics and I need the solution to understand one-dimensional hydrogen atom. Actually, I have to consider $-\frac{\hbar2}{2m}\psi(x)''-\frac{e^2}{4\pi\epsilon |x|}\psi(x)=E\psi(x)$. $m,e,\epsilon,\hbar$ are constants. The boundary condition  is $\psi(\infty)=\psi(ー\infty)=0$","How to solve $-y''-\frac{y}{|x|}=y$ . Now $y$ is a function of $x$. And $|x|$ means the absolute value of $x$, that is, $|x|=x$ for $x>0$, and $-x$ for $x<0$. Can somebody please hint at some substitution or refer any text related to these type of ode. Motivation : This ODE comes from quantum mechanics and I need the solution to understand one-dimensional hydrogen atom. Actually, I have to consider $-\frac{\hbar2}{2m}\psi(x)''-\frac{e^2}{4\pi\epsilon |x|}\psi(x)=E\psi(x)$. $m,e,\epsilon,\hbar$ are constants. The boundary condition  is $\psi(\infty)=\psi(ー\infty)=0$",,['ordinary-differential-equations']
79,Bounded Laplacian and function implies bounded gradient,Bounded Laplacian and function implies bounded gradient,,"Let $f:\mathbb R^n\to \mathbb R$ be a smooth function. Suppose that in a neighbourhood $U$ of $0$, we have two bounds: $$ |f(x)|\leq A $$ and $$ |\Delta f(x)|\leq B $$ Is it true that we have some bound of the form $$ |\nabla f(x)|\lesssim A^\theta B^{1-\theta}, $$ where $x\in U$? The one-dimensional case is easier to verify. But I wonder if there is some high dimensional analogue.","Let $f:\mathbb R^n\to \mathbb R$ be a smooth function. Suppose that in a neighbourhood $U$ of $0$, we have two bounds: $$ |f(x)|\leq A $$ and $$ |\Delta f(x)|\leq B $$ Is it true that we have some bound of the form $$ |\nabla f(x)|\lesssim A^\theta B^{1-\theta}, $$ where $x\in U$? The one-dimensional case is easier to verify. But I wonder if there is some high dimensional analogue.",,"['calculus', 'real-analysis', 'analysis', 'ordinary-differential-equations', 'interpolation']"
80,Why was the Butterfly Effect so surprising?,Why was the Butterfly Effect so surprising?,,"I was reading a popular science book about the history of  Chaos theory and was curious about the part when Edward Lorenz discovered the so-called butterfly effect . Long story short, he omitted to key in an extra decimal place into his computer simulation of weather, thinking that it wouldn't make much a difference, but instead got a completely different result in a relatively short period of time. This led him to his theory about chaotic system. Sure, it might be counter-intuitive to think that a small error can magnify that fast, but even with hindsight this shouldn't be THAT surprising at all. I mean, for example, Hadamard had coined the term well-posedness and sensitivity to initial condition decades before that. Edward Lorenz, being a fine mathematician as he was, should be familiar with these things shouldn't he? I'm not an expert in this so there could be many points that I overlooked. I hope that the question is not too broad.","I was reading a popular science book about the history of  Chaos theory and was curious about the part when Edward Lorenz discovered the so-called butterfly effect . Long story short, he omitted to key in an extra decimal place into his computer simulation of weather, thinking that it wouldn't make much a difference, but instead got a completely different result in a relatively short period of time. This led him to his theory about chaotic system. Sure, it might be counter-intuitive to think that a small error can magnify that fast, but even with hindsight this shouldn't be THAT surprising at all. I mean, for example, Hadamard had coined the term well-posedness and sensitivity to initial condition decades before that. Edward Lorenz, being a fine mathematician as he was, should be familiar with these things shouldn't he? I'm not an expert in this so there could be many points that I overlooked. I hope that the question is not too broad.",,"['ordinary-differential-equations', 'soft-question', 'dynamical-systems', 'math-history']"
81,How to express Mathieu function's first derivative in terms of Mathieu function?,How to express Mathieu function's first derivative in terms of Mathieu function?,,"Mathieu function $\operatorname{ce}_m(q,x)$ and $\operatorname{se}_m(q,x)$ are periodic solution of Mathieu equation, which is analogous to $\cos(x)$ and $\sin(x)$. However, unlike $\cos(x)$ and $\sin(x)$, the derivative of Mathieu function $\operatorname{ce}^\prime_m(q,x)$ and $\operatorname{se}^\prime_m(q,x)$ are not equal to $-\operatorname{se}_m(q,x)$ and $\operatorname{ce}_m(q,x)$. My question is, how to express Mathieu function's first derivative in terms of Mathieu function? Explicitly, in the following expansion, what is the analytical expression of $A_m^k$(not in the integral form)? $$ \operatorname{ce}^\prime_m(q,x) = \sum_{k}a_m^n \operatorname{se}_n(q,x) $$ If no analytical expression is available, does anyone known the existing numerical library which can generate these coefficients conveniently?","Mathieu function $\operatorname{ce}_m(q,x)$ and $\operatorname{se}_m(q,x)$ are periodic solution of Mathieu equation, which is analogous to $\cos(x)$ and $\sin(x)$. However, unlike $\cos(x)$ and $\sin(x)$, the derivative of Mathieu function $\operatorname{ce}^\prime_m(q,x)$ and $\operatorname{se}^\prime_m(q,x)$ are not equal to $-\operatorname{se}_m(q,x)$ and $\operatorname{ce}_m(q,x)$. My question is, how to express Mathieu function's first derivative in terms of Mathieu function? Explicitly, in the following expansion, what is the analytical expression of $A_m^k$(not in the integral form)? $$ \operatorname{ce}^\prime_m(q,x) = \sum_{k}a_m^n \operatorname{se}_n(q,x) $$ If no analytical expression is available, does anyone known the existing numerical library which can generate these coefficients conveniently?",,"['sequences-and-series', 'ordinary-differential-equations', 'special-functions']"
82,Lax-Milgram on Parabolic Equation 2,Lax-Milgram on Parabolic Equation 2,,"Let $\Omega=(0,\pi)\times(0,\pi)$and the problem $$ \left\{ \begin{array}{lc}       \frac{\partial u}{\partial t}-\Delta u+u_{x}-u=t\sin(x+y) & (0,1)\times \Omega \\ \frac{\partial u}{\partial n}+u=1  &  (0,1)\times \partial \Omega \\ u(0,x)=1& \Omega \end{array} \right. $$ I was asked to find the variational form and to prove that this problem has an unique solution. I found the variational form: $$\frac{d}{dt}(u(t),v)+\int_{\Omega}\nabla u \nabla v-\int_{\partial{\Omega}}v+\int_{\partial{\Omega}}uv+\int_{\Omega}u_{x}v-\int_{\Omega}uv=\int_{\Omega}t\sin(x+y)v,$$where $v\in H^{1}_{0}$. How can I prove the coercitivity? Can you give me some suggestions? Thank you!","Let $\Omega=(0,\pi)\times(0,\pi)$and the problem $$ \left\{ \begin{array}{lc}       \frac{\partial u}{\partial t}-\Delta u+u_{x}-u=t\sin(x+y) & (0,1)\times \Omega \\ \frac{\partial u}{\partial n}+u=1  &  (0,1)\times \partial \Omega \\ u(0,x)=1& \Omega \end{array} \right. $$ I was asked to find the variational form and to prove that this problem has an unique solution. I found the variational form: $$\frac{d}{dt}(u(t),v)+\int_{\Omega}\nabla u \nabla v-\int_{\partial{\Omega}}v+\int_{\partial{\Omega}}uv+\int_{\Omega}u_{x}v-\int_{\Omega}uv=\int_{\Omega}t\sin(x+y)v,$$where $v\in H^{1}_{0}$. How can I prove the coercitivity? Can you give me some suggestions? Thank you!",,"['ordinary-differential-equations', 'partial-differential-equations']"
83,Help Solving a Differential Equation with Trigonometric Functions,Help Solving a Differential Equation with Trigonometric Functions,,"I am having some trouble solving/understanding: $$ b\cos(bx)\sin(2y) = -2\cos(2y)\sin(bx)\frac{dy}{dx} $$ for $y(x)$, where $b\in\mathbb{R}$ is constant. My attempt: \begin{align} b\tan(2y) &= -2\tan(bx)\frac{dy}{dx} \\ b\cot(bx) &= -2\cot(2y)\frac{dy}{dx} \\ \int b\cot(bx)dx &= \int -2\cot(2y) dy \\ -\log(\sin(bx)) &= \log(\sin(2y))-C_1 \\ y(x) &= \frac{1}{2}\arcsin\left( \exp(C_1 - \log(\sin(bx))) \right) \\ y(x) &= \frac{1}{2}\arcsin\left( e^{C_1}\csc(bx) \right) \\ \end{align} Now, this solution  lines up with Mathematica, but it is sort of bizarre. Problem 1: the original DE is well-defined everywhere, but this solution has singularities. Edit: it is not actually well-defined when the $dy/dx$ has been zeroed out. Problem 2: most importantly, notice that $y(x)=\frac{n\pi}{2}$ for $n\geq 0,n\in\mathbb{Z}$ is a solution to the DE. But the solution does not really reflect this. What am I missing?","I am having some trouble solving/understanding: $$ b\cos(bx)\sin(2y) = -2\cos(2y)\sin(bx)\frac{dy}{dx} $$ for $y(x)$, where $b\in\mathbb{R}$ is constant. My attempt: \begin{align} b\tan(2y) &= -2\tan(bx)\frac{dy}{dx} \\ b\cot(bx) &= -2\cot(2y)\frac{dy}{dx} \\ \int b\cot(bx)dx &= \int -2\cot(2y) dy \\ -\log(\sin(bx)) &= \log(\sin(2y))-C_1 \\ y(x) &= \frac{1}{2}\arcsin\left( \exp(C_1 - \log(\sin(bx))) \right) \\ y(x) &= \frac{1}{2}\arcsin\left( e^{C_1}\csc(bx) \right) \\ \end{align} Now, this solution  lines up with Mathematica, but it is sort of bizarre. Problem 1: the original DE is well-defined everywhere, but this solution has singularities. Edit: it is not actually well-defined when the $dy/dx$ has been zeroed out. Problem 2: most importantly, notice that $y(x)=\frac{n\pi}{2}$ for $n\geq 0,n\in\mathbb{Z}$ is a solution to the DE. But the solution does not really reflect this. What am I missing?",,"['integration', 'ordinary-differential-equations', 'trigonometry', 'trigonometric-integrals']"
84,How do find out what eigenvalue represent a state in a state space vector?,How do find out what eigenvalue represent a state in a state space vector?,,"Let's say that I have a system matrix A and to find out the eigenvalues $\lambda$ ,I do this: $$ \hbox{det}(\lambda I - A) = 0 $$ Then to find out if the system are controllable, I uses the Hautus Lemma test . This thest is mutch better that the regular $\hbox{rank}(\hbox{ctrb}(A, B)) = n\ $ test. Anyway! Here it is: $$ \hbox{rank}([\lambda_i I - A, B]) = n$$ Let's say that I got 3 eigenvalues of A . They are $\lambda_1 = -2$ , $\lambda_2 = -10$ and $\lambda_3 = -0.5$.  Now I test if the system is controllable: \begin{align}  \hbox{rank}([\lambda_1 I - A, B]) &= 3\, ,\\ \hbox{rank}([\lambda_2 I - A, B]) &= 1\, ,\\ \hbox{rank}([\lambda_3 I - A, B]) &= 3 \end{align} So something went wrong here! I got 3 eigenvalues, which mean that my state vector is the length 3. That means that my rank of the system should be number 3. But this: $$ \hbox{rank}([\lambda_2 I - A, B]) = 1$$ gives number 1 buy using $\lambda_2 = 10$. Question: Does this mean that something is wrong with my state vector at row number 2 beacuse the eigenvalue $\lambda_2 = 10$ must reprecent the state vector $x_2$ ?","Let's say that I have a system matrix A and to find out the eigenvalues $\lambda$ ,I do this: $$ \hbox{det}(\lambda I - A) = 0 $$ Then to find out if the system are controllable, I uses the Hautus Lemma test . This thest is mutch better that the regular $\hbox{rank}(\hbox{ctrb}(A, B)) = n\ $ test. Anyway! Here it is: $$ \hbox{rank}([\lambda_i I - A, B]) = n$$ Let's say that I got 3 eigenvalues of A . They are $\lambda_1 = -2$ , $\lambda_2 = -10$ and $\lambda_3 = -0.5$.  Now I test if the system is controllable: \begin{align}  \hbox{rank}([\lambda_1 I - A, B]) &= 3\, ,\\ \hbox{rank}([\lambda_2 I - A, B]) &= 1\, ,\\ \hbox{rank}([\lambda_3 I - A, B]) &= 3 \end{align} So something went wrong here! I got 3 eigenvalues, which mean that my state vector is the length 3. That means that my rank of the system should be number 3. But this: $$ \hbox{rank}([\lambda_2 I - A, B]) = 1$$ gives number 1 buy using $\lambda_2 = 10$. Question: Does this mean that something is wrong with my state vector at row number 2 beacuse the eigenvalue $\lambda_2 = 10$ must reprecent the state vector $x_2$ ?",,"['matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'determinant', 'optimal-control']"
85,Stability: orbits spiralling monotonically,Stability: orbits spiralling monotonically,,"I'm working my way through Dynamics and Bifurcations [Jack K. Hale, Huseyin Kocak] and I can't understand this probably trivial implication in Example 11.1 (page 334). We have a following system of ODEs (perturbed harmonic oscillator) \begin{aligned} \dot{x}_1 & = x_2 + a x_1 (x_1^2 + x_2^2), \\ \dot{x}_2 & = -x_1 + a x_2 (x_1^2 + x_2^2), \end{aligned} $a\in\mathbb{R}$, which in polar coordinates \begin{equation} x_1 = r \cos \theta, \quad x_2 = -r \sin \theta, \end{equation} reads \begin{aligned} \dot{r} & = a r^3, \\ \dot{\theta} & = 1.   \end{aligned} The book says: ""Since $\dot{\theta}>0$, the orbits spiral monotonically in $\theta$ around the origin. Therefore, the stability type of the origin of [the original planar system] is the same as that of the equilibrium point of the radial equation $\dot{r} = a r$."" I fail to see why it matters whether $\theta$ is a monotonic function of $t$ or not. Can't we draw the same conclusion that the radial equation determines the stability of the origin when this condition does not hold? (Let's say $\dot{\theta} = \sin\theta$?)","I'm working my way through Dynamics and Bifurcations [Jack K. Hale, Huseyin Kocak] and I can't understand this probably trivial implication in Example 11.1 (page 334). We have a following system of ODEs (perturbed harmonic oscillator) \begin{aligned} \dot{x}_1 & = x_2 + a x_1 (x_1^2 + x_2^2), \\ \dot{x}_2 & = -x_1 + a x_2 (x_1^2 + x_2^2), \end{aligned} $a\in\mathbb{R}$, which in polar coordinates \begin{equation} x_1 = r \cos \theta, \quad x_2 = -r \sin \theta, \end{equation} reads \begin{aligned} \dot{r} & = a r^3, \\ \dot{\theta} & = 1.   \end{aligned} The book says: ""Since $\dot{\theta}>0$, the orbits spiral monotonically in $\theta$ around the origin. Therefore, the stability type of the origin of [the original planar system] is the same as that of the equilibrium point of the radial equation $\dot{r} = a r$."" I fail to see why it matters whether $\theta$ is a monotonic function of $t$ or not. Can't we draw the same conclusion that the radial equation determines the stability of the origin when this condition does not hold? (Let's say $\dot{\theta} = \sin\theta$?)",,"['ordinary-differential-equations', 'dynamical-systems']"
86,Oscillator that only responds if two frequencies are present,Oscillator that only responds if two frequencies are present,,"I'm looking for a differential equation that responds to an driving force and which only responds if the driving force is a superposition of two frequencies? As a counterpoint, a harmonic oscillator will respond most when it is driven by a force which has the same frequency as the oscillator. That is, when the force is in resonance with the oscillator. I'd like to know if there is a differential equation which involves a driving force which for which the solution has the largest response when the driving force is the sum of two frequencies, such as $sin{\omega_1t} + sin{\omega_2t}$ but not when any one of the frequencies is present without the other. So for instance the solution would not respond much to $sin{\omega_1t}$ or $sin{\omega_2t}$ by themselves. Just to be precise, let me define the response of a solution to a driving force as the long term mean of the square of the solution, $\int_T^\infty x(t)^2 dt$ where $x(t)$ is the solution of the differential equation. Also, for this definition to make sense, the driving force has to have a bounded amplitude so that if $f(t)$ is the driving force, it has to be above and below two constants for all time.","I'm looking for a differential equation that responds to an driving force and which only responds if the driving force is a superposition of two frequencies? As a counterpoint, a harmonic oscillator will respond most when it is driven by a force which has the same frequency as the oscillator. That is, when the force is in resonance with the oscillator. I'd like to know if there is a differential equation which involves a driving force which for which the solution has the largest response when the driving force is the sum of two frequencies, such as $sin{\omega_1t} + sin{\omega_2t}$ but not when any one of the frequencies is present without the other. So for instance the solution would not respond much to $sin{\omega_1t}$ or $sin{\omega_2t}$ by themselves. Just to be precise, let me define the response of a solution to a driving force as the long term mean of the square of the solution, $\int_T^\infty x(t)^2 dt$ where $x(t)$ is the solution of the differential equation. Also, for this definition to make sense, the driving force has to have a bounded amplitude so that if $f(t)$ is the driving force, it has to be above and below two constants for all time.",,"['ordinary-differential-equations', 'harmonic-analysis', 'nonlinear-system']"
87,Solving an ODE for the Green function using Fourier transform,Solving an ODE for the Green function using Fourier transform,,"I'm trying to reproduce the results of a paper, I'll insert here what is important and then explain what I cannot understand. We have this equation for the Green function $\mathcal{G}$:   $$\left( D_y \frac{\partial^2}{\partial y^2}-v_y\frac{\partial}{\partial y} -\delta(y)D_xk_x^2-ik_xv_x-s \right) \mathcal{G} (k_x,y,\overline{y};s)=\delta (y- \overline{y}),$$   subjected to the boundary conditions $\mathcal{G}(k_x, \pm\infty, \overline{y};s)=0$. This equation can be solved by a Fourier transform with respect to $y$ (in the ""physicist form"": $\mathcal{F} [f(y)](k_y)=\int_{-\infty}^{\infty} dye^{-ik_y y}f(y)$ and $\mathcal{F}^{-1}[f(k_y)](y)=\frac{1}{2\pi} \int_{-\infty}^{\infty}dk_ye^{ik_yy}$ ):   $$\mathcal{G} (k_x,k_y,\overline{y};s)=- \frac{e^{-ik_y\overline{y}}}{D_yk_y^2 +ik_yv_y+ik_xv_x+s}-\frac{D_xk_x^2}{D_yk_y^2 +ik_yv_y+ik_xv_x+s} \mathcal{G}(k_x,0,\overline{y};s).$$ The last term has the presence of the Green function at $y=0$, which after some calculation it is possible to show that is given by   $$\mathcal{G}(k_x,0,\overline{y};s)=-\frac{e^{-\frac{v_y}{2 D_y}\overline{y}} e^{-\sqrt{\frac{\beta}{D_y}}\vert\overline{y}\vert }}{2\sqrt{D_y \beta} +D_xk_x^2},$$   where $\beta=s+ik_xv_x+v_y^2/(4D_y)$. I can't reproduce these ""some calculations"" that give the expression for $\mathcal{G}(k_x,0,\overline{y};s)$. Maybe I have to solve the first equation and then apply $y=0$, because when he says ""$y=0$"" implies that it should be done before the Fourier transform; but I don't know how to do it. PS: $D_y$, $D_x$, $v_y$ and $v_x$ are real constants.","I'm trying to reproduce the results of a paper, I'll insert here what is important and then explain what I cannot understand. We have this equation for the Green function $\mathcal{G}$:   $$\left( D_y \frac{\partial^2}{\partial y^2}-v_y\frac{\partial}{\partial y} -\delta(y)D_xk_x^2-ik_xv_x-s \right) \mathcal{G} (k_x,y,\overline{y};s)=\delta (y- \overline{y}),$$   subjected to the boundary conditions $\mathcal{G}(k_x, \pm\infty, \overline{y};s)=0$. This equation can be solved by a Fourier transform with respect to $y$ (in the ""physicist form"": $\mathcal{F} [f(y)](k_y)=\int_{-\infty}^{\infty} dye^{-ik_y y}f(y)$ and $\mathcal{F}^{-1}[f(k_y)](y)=\frac{1}{2\pi} \int_{-\infty}^{\infty}dk_ye^{ik_yy}$ ):   $$\mathcal{G} (k_x,k_y,\overline{y};s)=- \frac{e^{-ik_y\overline{y}}}{D_yk_y^2 +ik_yv_y+ik_xv_x+s}-\frac{D_xk_x^2}{D_yk_y^2 +ik_yv_y+ik_xv_x+s} \mathcal{G}(k_x,0,\overline{y};s).$$ The last term has the presence of the Green function at $y=0$, which after some calculation it is possible to show that is given by   $$\mathcal{G}(k_x,0,\overline{y};s)=-\frac{e^{-\frac{v_y}{2 D_y}\overline{y}} e^{-\sqrt{\frac{\beta}{D_y}}\vert\overline{y}\vert }}{2\sqrt{D_y \beta} +D_xk_x^2},$$   where $\beta=s+ik_xv_x+v_y^2/(4D_y)$. I can't reproduce these ""some calculations"" that give the expression for $\mathcal{G}(k_x,0,\overline{y};s)$. Maybe I have to solve the first equation and then apply $y=0$, because when he says ""$y=0$"" implies that it should be done before the Fourier transform; but I don't know how to do it. PS: $D_y$, $D_x$, $v_y$ and $v_x$ are real constants.",,"['ordinary-differential-equations', 'fourier-transform', 'greens-function']"
88,Show there are an infinite amount of roots to the Airy equation,Show there are an infinite amount of roots to the Airy equation,,"I tried the tip I received from the teacher but I can't solve this exercise, even that it looks simple. As $y(x)$ is the non-zero solution to the Airy equation \begin{equation} y''+q(x)y=0,\end{equation} where $q(x)>0,\forall x$ and \begin{equation} \int\limits_0^\infty q(x)dx=\infty, \end{equation} show that $y(x)$ has an infinite amount of positive roots. Tip I received: Supose, by contradiction, there is a $x_0$ such that $y(x_0)=0$ and $y(x)\neq 0$ for $x>x_0$. Show that there is   $x_1>0$ such that $y'(x_0)$ and $y'(x_1)$ has opposite signs so   there's a root larger than $x_0$. To prove the existence of $x_1$ one   can integrate by parts the Airy equation $y''/y=-q(x)$.","I tried the tip I received from the teacher but I can't solve this exercise, even that it looks simple. As $y(x)$ is the non-zero solution to the Airy equation \begin{equation} y''+q(x)y=0,\end{equation} where $q(x)>0,\forall x$ and \begin{equation} \int\limits_0^\infty q(x)dx=\infty, \end{equation} show that $y(x)$ has an infinite amount of positive roots. Tip I received: Supose, by contradiction, there is a $x_0$ such that $y(x_0)=0$ and $y(x)\neq 0$ for $x>x_0$. Show that there is   $x_1>0$ such that $y'(x_0)$ and $y'(x_1)$ has opposite signs so   there's a root larger than $x_0$. To prove the existence of $x_1$ one   can integrate by parts the Airy equation $y''/y=-q(x)$.",,"['ordinary-differential-equations', 'sturm-liouville', 'airy-functions']"
89,A question on the interval of existence of IVP,A question on the interval of existence of IVP,,"$\mathbf {(1)} $ The solution of the differential equation,  $$y'=1+y^2,\;\;\;y(0)=1$$ exists on the interval- (a) $ \displaystyle|x|<  \frac {\pi}{2}$ (b) $-\displaystyle\frac {\pi}{2}<x< \pi$ (c) $-\displaystyle\frac{3\pi}{4} <x< \frac{\pi}{4}$ (d) $ |x|<   \pi$ My Attempt: The given DE is in variable separable form, on solving we have, $$\tan^{-1}y=x+c\implies y=\tan(x+c)$$ using the initial condition we obtain, $c=\displaystyle \frac{\pi}{4}$ Thus, $ y=\tan\left(x+\displaystyle \frac{\pi}{4}\right)$ For the solution to exist, $$-\frac {\pi}{2}<x+ \frac{\pi}{4}<  \frac {\pi}{2}$$  (because only then will $y$ be defined) $$\implies  -\frac{3\pi}{4} <x<\frac{\pi}{4}$$ So, (c) is true. =================================================== (2) If $y(x)$ is the solution of the DE, $$y'=2(1+y)\sqrt y \;\; ,y\left( \frac{\pi}{2}\right)=1$$ then the largest interval of(to the right of origin) on which the solution exists is- (a) $\;\;\;\left[0, \displaystyle \frac {3\pi}{4}\right)$ (b) $\;\;\;[0, \pi)$ (c)$\;\;\;[0, 2\pi)$ (d)$\;\;\;\left[0, \displaystyle \frac {2\pi}{3}\right)$ My Attempt: The DE is again in variable separable from, on using the substitution $\sqrt y =t$ and solving we have, $$y=(\tan(x+c))^2$$ (I hope I have not made any mistakes while solving) Since we are dealing with only values from $\Bbb R$,I feel $\tan(x+c)$ must take non-negative values. Now from the initial conditions we get, $c=\displaystyle - \frac {\pi}{4}$ Thus, $y=\left(\tan \left (x\displaystyle - \frac {\pi}{4}\right) \right)^2$ Beyond this point I'm finding it difficult to proceed. ========================================================== I want to know if I've got the first problem right and whether or not all my arguments are correct in the first problem. For the second problem, Hints Please!","$\mathbf {(1)} $ The solution of the differential equation,  $$y'=1+y^2,\;\;\;y(0)=1$$ exists on the interval- (a) $ \displaystyle|x|<  \frac {\pi}{2}$ (b) $-\displaystyle\frac {\pi}{2}<x< \pi$ (c) $-\displaystyle\frac{3\pi}{4} <x< \frac{\pi}{4}$ (d) $ |x|<   \pi$ My Attempt: The given DE is in variable separable form, on solving we have, $$\tan^{-1}y=x+c\implies y=\tan(x+c)$$ using the initial condition we obtain, $c=\displaystyle \frac{\pi}{4}$ Thus, $ y=\tan\left(x+\displaystyle \frac{\pi}{4}\right)$ For the solution to exist, $$-\frac {\pi}{2}<x+ \frac{\pi}{4}<  \frac {\pi}{2}$$  (because only then will $y$ be defined) $$\implies  -\frac{3\pi}{4} <x<\frac{\pi}{4}$$ So, (c) is true. =================================================== (2) If $y(x)$ is the solution of the DE, $$y'=2(1+y)\sqrt y \;\; ,y\left( \frac{\pi}{2}\right)=1$$ then the largest interval of(to the right of origin) on which the solution exists is- (a) $\;\;\;\left[0, \displaystyle \frac {3\pi}{4}\right)$ (b) $\;\;\;[0, \pi)$ (c)$\;\;\;[0, 2\pi)$ (d)$\;\;\;\left[0, \displaystyle \frac {2\pi}{3}\right)$ My Attempt: The DE is again in variable separable from, on using the substitution $\sqrt y =t$ and solving we have, $$y=(\tan(x+c))^2$$ (I hope I have not made any mistakes while solving) Since we are dealing with only values from $\Bbb R$,I feel $\tan(x+c)$ must take non-negative values. Now from the initial conditions we get, $c=\displaystyle - \frac {\pi}{4}$ Thus, $y=\left(\tan \left (x\displaystyle - \frac {\pi}{4}\right) \right)^2$ Beyond this point I'm finding it difficult to proceed. ========================================================== I want to know if I've got the first problem right and whether or not all my arguments are correct in the first problem. For the second problem, Hints Please!",,"['ordinary-differential-equations', 'initial-value-problems']"
90,Checking the Solution of a First Order ODE,Checking the Solution of a First Order ODE,,"Please consider the following differential equation: \begin{eqnarray*} 	(x + 2y - 4) dx + (-2x + 4y) dy &=& 0 \\ \end{eqnarray*} I have solved the equation and my solution is: \begin{eqnarray*} \ln{( 4(y-1)^2) + (x-2)^2)} - 2 \arctan{ \Big( \frac{2y - 2}{x - 2} \Big) } &=& C \\ \end{eqnarray*} Since this solution matches the answer in the back of the book, I believe it to be correct but I always like to check my answer so I take the answer and try to derive the original equation which should work if my answer is wrong. \begin{eqnarray*} 	D\Big( \ln{( 4(y-1)^2) + (x-2)^2)} - 2 \arctan{ \Big( \frac{2y - 2}{x - 2} \Big) } \Big) &=& 0 \\ \end{eqnarray*} \begin{eqnarray*} 	D\Big( \ln{( 4(y-1)^2) + (x-2)^2)} \Big) &=& 	\frac{8(y-1) \frac{dy}{dx} +2(x-2)}{4(y-1)^2 + (x-2)^2} \\ 	D\Big(  \arctan{\Big(\frac{2y-2}{x-2}\Big)} \Big) &=& 	\frac{(x-2)(2 \frac{dy}{dx}) - (2y-2)} { (\frac{2y-2}{x-2})^2 + 1} \\ 	D\Big(  \arctan{\Big(\frac{2y-2}{x-2}\Big)} \Big) &=& 	\frac{(x-2)(2 \frac{dy}{dx}) - (2y-2)} { 4(\frac{y-1}{x-2})^2 + 1} \\ 	D\Big(  \arctan{\Big(\frac{2y-2}{x-2}\Big)} \Big) &=& 	\frac{(x-2)^3(2 \frac{dy}{dx}) - (2y-2)(x-2)^2} { 4(y-1)^2 + (x-2)^2} \\ \end{eqnarray*} % \begin{eqnarray*} 	\frac{ 8(y-1) \frac{dy}{dx} +2(x-2) - 2(x-2)^3 \frac{dy}{dx} + (2y-2)(x-2)^2}{4(y-1)^2 + (x-2)^2} &=& 0 \\ 	8(y-1) \frac{dy}{dx} +2(x-2) - 2(x-2)^3 \frac{dy}{dx} + (2y-2)(x-2)^2 &=& 0 \\ 	4(y-1)\frac{dy}{dx} + (x-2) - (x-2)^3 \frac{dy}{dx} + (y-1)(x-2)^2 &=& 0 \\ \end{eqnarray*} At this point, I feel my solution is not going to check. Please tell me what I am missing? Thanks Bob","Please consider the following differential equation: \begin{eqnarray*} 	(x + 2y - 4) dx + (-2x + 4y) dy &=& 0 \\ \end{eqnarray*} I have solved the equation and my solution is: \begin{eqnarray*} \ln{( 4(y-1)^2) + (x-2)^2)} - 2 \arctan{ \Big( \frac{2y - 2}{x - 2} \Big) } &=& C \\ \end{eqnarray*} Since this solution matches the answer in the back of the book, I believe it to be correct but I always like to check my answer so I take the answer and try to derive the original equation which should work if my answer is wrong. \begin{eqnarray*} 	D\Big( \ln{( 4(y-1)^2) + (x-2)^2)} - 2 \arctan{ \Big( \frac{2y - 2}{x - 2} \Big) } \Big) &=& 0 \\ \end{eqnarray*} \begin{eqnarray*} 	D\Big( \ln{( 4(y-1)^2) + (x-2)^2)} \Big) &=& 	\frac{8(y-1) \frac{dy}{dx} +2(x-2)}{4(y-1)^2 + (x-2)^2} \\ 	D\Big(  \arctan{\Big(\frac{2y-2}{x-2}\Big)} \Big) &=& 	\frac{(x-2)(2 \frac{dy}{dx}) - (2y-2)} { (\frac{2y-2}{x-2})^2 + 1} \\ 	D\Big(  \arctan{\Big(\frac{2y-2}{x-2}\Big)} \Big) &=& 	\frac{(x-2)(2 \frac{dy}{dx}) - (2y-2)} { 4(\frac{y-1}{x-2})^2 + 1} \\ 	D\Big(  \arctan{\Big(\frac{2y-2}{x-2}\Big)} \Big) &=& 	\frac{(x-2)^3(2 \frac{dy}{dx}) - (2y-2)(x-2)^2} { 4(y-1)^2 + (x-2)^2} \\ \end{eqnarray*} % \begin{eqnarray*} 	\frac{ 8(y-1) \frac{dy}{dx} +2(x-2) - 2(x-2)^3 \frac{dy}{dx} + (2y-2)(x-2)^2}{4(y-1)^2 + (x-2)^2} &=& 0 \\ 	8(y-1) \frac{dy}{dx} +2(x-2) - 2(x-2)^3 \frac{dy}{dx} + (2y-2)(x-2)^2 &=& 0 \\ 	4(y-1)\frac{dy}{dx} + (x-2) - (x-2)^3 \frac{dy}{dx} + (y-1)(x-2)^2 &=& 0 \\ \end{eqnarray*} At this point, I feel my solution is not going to check. Please tell me what I am missing? Thanks Bob",,['ordinary-differential-equations']
91,A partial differential equation- transport equation,A partial differential equation- transport equation,,"I am a begining learner of pde. I am trying to solve the following the transport equation $$u_t + [1- (t^2 - x^2)(2b + a x^2)]\frac t x u_x= -f u,$$ where $a$ and $b$ are the differentiable functions of $t$. My solution: $$\frac{dt}{1} = \frac{dx}{ [1- (t^2 - x^2)(2b + a x^2)]\frac t x } = \frac{du}{-fu}.$$ From $\frac{dt}{1} = \frac{dx}{ [1- (t^2 - x^2)(2b + a x^2)]\frac t x}$, we know that  $$\frac{d}{dt}(\frac{1}{x^2 - t^2}) + \frac{2t(a t^2 + 2b)}{x^2 - t^2} = -2at.$$ This is a Bernoulli equation. One can easily get its solution $$\frac{1}{x^2 - t^2} = \frac{c - \int 2at e^{\int 2t(a t^2 + 2b) dt} dt}{e^{\int 2t(a t^2 + 2b)dt}}.$$ Then  $$c= \frac{(x^2 - t^2)\int 2at e^{\int 2t(a t^2 + 2b) dt} dt + e^{\int 2t(a t^2 + 2b) dt}}{x^2 - t^2}.$$ But i do not know how should continue it??? Thanks for any help.","I am a begining learner of pde. I am trying to solve the following the transport equation $$u_t + [1- (t^2 - x^2)(2b + a x^2)]\frac t x u_x= -f u,$$ where $a$ and $b$ are the differentiable functions of $t$. My solution: $$\frac{dt}{1} = \frac{dx}{ [1- (t^2 - x^2)(2b + a x^2)]\frac t x } = \frac{du}{-fu}.$$ From $\frac{dt}{1} = \frac{dx}{ [1- (t^2 - x^2)(2b + a x^2)]\frac t x}$, we know that  $$\frac{d}{dt}(\frac{1}{x^2 - t^2}) + \frac{2t(a t^2 + 2b)}{x^2 - t^2} = -2at.$$ This is a Bernoulli equation. One can easily get its solution $$\frac{1}{x^2 - t^2} = \frac{c - \int 2at e^{\int 2t(a t^2 + 2b) dt} dt}{e^{\int 2t(a t^2 + 2b)dt}}.$$ Then  $$c= \frac{(x^2 - t^2)\int 2at e^{\int 2t(a t^2 + 2b) dt} dt + e^{\int 2t(a t^2 + 2b) dt}}{x^2 - t^2}.$$ But i do not know how should continue it??? Thanks for any help.",,"['ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations']"
92,Find $f_x$ in terms of $f$,Find  in terms of,f_x f,"Consider $f(x, y) =  g(x^2, y^2)$ Find $f_x(x, y)$ in terms of $f(x, y)$ or $g(x^2, y^2)$ or $g_x$ of $g_y$ if any of them is possible. I am not sure how to apply the rules here, but say it was a one variable function like $f(x) = g(x^2)$ then I can easily use the chain rule on the RHS and get $f'(x) = g'(x^2) 2x$ , but how do I apply it with 2 dimensional variables?","Consider Find in terms of or or of if any of them is possible. I am not sure how to apply the rules here, but say it was a one variable function like then I can easily use the chain rule on the RHS and get , but how do I apply it with 2 dimensional variables?","f(x, y) =  g(x^2, y^2) f_x(x, y) f(x, y) g(x^2, y^2) g_x g_y f(x) = g(x^2) f'(x) = g'(x^2) 2x","['calculus', 'ordinary-differential-equations']"
93,Why do we need isogonal trajectories?,Why do we need isogonal trajectories?,,"Could you, please, give me some examples, where we really need isogonal, but not orthogonal, trajectories? I know, how to find them using differential equations. I would like to see some examples in physics or engineering or somewhere else. Thanks in advance!","Could you, please, give me some examples, where we really need isogonal, but not orthogonal, trajectories? I know, how to find them using differential equations. I would like to see some examples in physics or engineering or somewhere else. Thanks in advance!",,"['ordinary-differential-equations', 'differential-geometry']"
94,How to show that a differential equation has an integrating factor of a specific form,How to show that a differential equation has an integrating factor of a specific form,,"I am given the differential equation $$4x^2y + 3xy^2 + 2y^3 + (2x^3 + 3x^2y + 4xy^2)\frac{dy}{dx} = 0$$ and I am asked to show that this equation has an integrating factor $\mu \equiv \mu(xy)$. Setting $M(x,y) = 4x^2y + 3xy^2 + 2y^3$ and $N(x,y) = 2x^3 + 3x^2y + 4xy^2$ I know that I want to show that, for some $\mu$ that is a function of $xy$, I have that $$\mu(xy) M(x,y) + \mu(xy) N(x,y)\frac{dy}{dx} = 0$$ is an exact equation, i.e. $$\frac{\partial (\mu M)}{\partial y} = \frac{\partial (\mu N)}{\partial x}$$ setting $v = xy$ and taking the derivatives, knowing that $$\frac{\partial \mu}{\partial x} = y\frac{\partial \mu}{\partial v}, \frac{\partial \mu}{\partial y} = x\frac{\partial \mu}{\partial v}$$ I will eventually get to $$\frac{\mu '}{\mu} = \frac1{xy}$$ where I wrote $\mu '$ instead of $\frac{\partial \mu}{\partial v}$. My question is, is that enough to show that the equation indeed has an integrating factor $\mu(xy)$? Or do I need to explicitly solve for $\mu$ and show that it depends only on $(xy)$?","I am given the differential equation $$4x^2y + 3xy^2 + 2y^3 + (2x^3 + 3x^2y + 4xy^2)\frac{dy}{dx} = 0$$ and I am asked to show that this equation has an integrating factor $\mu \equiv \mu(xy)$. Setting $M(x,y) = 4x^2y + 3xy^2 + 2y^3$ and $N(x,y) = 2x^3 + 3x^2y + 4xy^2$ I know that I want to show that, for some $\mu$ that is a function of $xy$, I have that $$\mu(xy) M(x,y) + \mu(xy) N(x,y)\frac{dy}{dx} = 0$$ is an exact equation, i.e. $$\frac{\partial (\mu M)}{\partial y} = \frac{\partial (\mu N)}{\partial x}$$ setting $v = xy$ and taking the derivatives, knowing that $$\frac{\partial \mu}{\partial x} = y\frac{\partial \mu}{\partial v}, \frac{\partial \mu}{\partial y} = x\frac{\partial \mu}{\partial v}$$ I will eventually get to $$\frac{\mu '}{\mu} = \frac1{xy}$$ where I wrote $\mu '$ instead of $\frac{\partial \mu}{\partial v}$. My question is, is that enough to show that the equation indeed has an integrating factor $\mu(xy)$? Or do I need to explicitly solve for $\mu$ and show that it depends only on $(xy)$?",,"['ordinary-differential-equations', 'integrating-factor']"
95,A space curve with constant curvature-torsion product,A space curve with constant curvature-torsion product,,"How do we solve for  curve coordinates in terms of parameter $t$  in $\mathbb R^3$ having scalar property $$ \tau \cdot \kappa = const.?$$ The curve would be with infinite twist at straight portions and confined to plane at cusp points. EDITS (1-3): I.e., how to find a vector $r(t)$ so that $$  \tau\, \kappa =\dfrac{r^{'} \times r^{''}\cdot r^{'''}  }{||r^{'} \times r^{''}||^2 }  \dfrac{|r^{'} \times r^{''}|  }{|r^{'} |^3 } =1? $$ The above formulae are of Frenet-Serret vector triad $T,N,B$ and derivatives, textbook derived with respect to parameter $t$. BTW, is there a word for infinite torsion of a line?( By analogy of infinite normal curvature being a cusp) The following surface (rotation symmetry assumed) and arc are obtained numerically but not without an extra assumed arbitrary relation of arc angle to meridian and axis of symmetry . Central parts are asymptotic/infinite torsion and the ends have infinite curvature cusp edge/zero torsion just as expected. This singularity terminates numerical computation.","How do we solve for  curve coordinates in terms of parameter $t$  in $\mathbb R^3$ having scalar property $$ \tau \cdot \kappa = const.?$$ The curve would be with infinite twist at straight portions and confined to plane at cusp points. EDITS (1-3): I.e., how to find a vector $r(t)$ so that $$  \tau\, \kappa =\dfrac{r^{'} \times r^{''}\cdot r^{'''}  }{||r^{'} \times r^{''}||^2 }  \dfrac{|r^{'} \times r^{''}|  }{|r^{'} |^3 } =1? $$ The above formulae are of Frenet-Serret vector triad $T,N,B$ and derivatives, textbook derived with respect to parameter $t$. BTW, is there a word for infinite torsion of a line?( By analogy of infinite normal curvature being a cusp) The following surface (rotation symmetry assumed) and arc are obtained numerically but not without an extra assumed arbitrary relation of arc angle to meridian and axis of symmetry . Central parts are asymptotic/infinite torsion and the ends have infinite curvature cusp edge/zero torsion just as expected. This singularity terminates numerical computation.",,"['ordinary-differential-equations', 'differential-geometry']"
96,"What is the Fourier Transform for $\mathscr{F} \left\{\frac{\partial^2 (x^2p(x,t))}{\partial x^2} \right\}$ w.r.t $x$?",What is the Fourier Transform for  w.r.t ?,"\mathscr{F} \left\{\frac{\partial^2 (x^2p(x,t))}{\partial x^2} \right\} x","What is the Fourier Transform for the following: $$ \mathscr{F} \left\{\frac{\partial^2 (x^2p(x,t))}{\partial x^2} \right\} = ? $$ Here is my problem: Suppose we are given the diffusion $$ dX(t)= \mu dt +  \sigma dW(t) \hspace{10mm}  (1) \\ X(0)=x_0 $$ And a function  $$ k(x)= \theta x^2 \hspace{10mm}  (2) $$ Then by Ito Lemma  $$ dk[X(t)]=(2 \mu  \theta x +  \theta  \sigma ^2)dt + (2 \sigma  \theta x)dW(t) \hspace{10mm}  (3) \\ k[X(0)]=0 $$ And the Forward Kolmogorov to find the transition density $$ \frac{\partial p(x,t)}{\partial t}=- \frac{\partial ((2 \mu  \theta x+ \theta  \sigma^2 )p(x,t))}{\partial x} +  \frac{1}{2} \frac{\partial^2 ((2 \sigma  \theta x)^2p(x,t))}{\partial x^2} \hspace{10mm}  (4)    \\ p(x,0)= \delta (x-x_0) $$ So far so good Now, I need to convert the 2nd order pde to ordinary so I can solve it. This is why I need the Fourier Transform. List of properties: $ (a) \ \mathscr{F} \left\{ p(x,t) \right\}= \overline{p}(\xi,t)= \intop\nolimits_{-\infty}^{\infty} p(x,t) e^{-i2 \pi x \xi} dx $ $ (b) \ \mathscr{F}^{-1} \{  \overline{p} (\xi,t) \}= p(x,t)= \intop\nolimits_{-\infty}^{\infty}  \overline{p} (\xi,t) e^{i2 \pi x \xi} d\xi $ $ (c) \ \mathscr{F} \{ xp(x,t) \}= \frac{i}{-2 \pi} \frac{\partial  \overline{p} (\xi,t)}{\partial \xi}  $ $ (d) \ \mathscr{F} \{ \frac{\partial^n p(x,t)}{\partial x^n} \}=(i2 \pi \xi)^n  \overline{p} (\xi,t) $ $ (e) \ \mathscr{F} \{  \delta (x-x_0) \}= \intop\nolimits_{-\infty}^{\infty}  \delta (x-x_0) e^{-i2 \pi x \xi}dx = e^{-i2 \pi x_0 \xi }  $ So now I can start to re-write (4) term by term: $$ \ \mathscr{F} \{ \frac{\partial p(x,t)}{\partial t} \}=   \frac{\partial  \overline{p} (\xi,t)}{\partial t} \hspace{10mm} (4-1) $$ By $(f+g)'=f'+g'$ $$ \frac{\partial ((2 \mu  \theta x + \theta \sigma^2) p(x,t) )}{\partial x}= 2 \mu  \theta  \frac{\partial (xp(x,t))}{\partial x} +   \theta  \sigma^2 \frac{\partial p(x,t)}{\partial x} $$ $$  \ \mathscr{F} \left\{ 2 \mu  \theta  \frac{\partial (xp(x,t))}{\partial x} \right\} = (2 \mu  \theta) (i2 \pi \xi) \frac{i}{-2 \pi } \frac{\partial p(\xi,t)}{\partial \xi} \hspace{10mm} (4-2) $$ $$ \ \mathscr{F} \left\{ \theta  \sigma^2 \frac{\partial p(x,t)}{\partial x} \right\} = (\theta  \sigma^2) (i2 \pi \xi)  \overline{p} (\xi,t) \hspace{10mm} (4-3) $$ And I get stuck on the 2nd term of (4) $$ \ \mathscr{F} \left\{\frac{\partial^2 ((x)^2p(x,t))}{\partial x^2} \right\} =?  \hspace{10mm} (4-4) $$ Any help/hint if I can re-write (4-4) in terms of the properties would be tremendous. Thank you! Additional stuff - 2nd derivative for $xp(x,t)$ $$  \ \mathscr{F} \left\{ \frac{\partial^2 (xp(x,t))}{\partial x^2} \right\} = (i2 \pi \xi)^2 \frac{i}{-2 \pi } \frac{\partial p(\xi,t)}{\partial \xi} $$","What is the Fourier Transform for the following: $$ \mathscr{F} \left\{\frac{\partial^2 (x^2p(x,t))}{\partial x^2} \right\} = ? $$ Here is my problem: Suppose we are given the diffusion $$ dX(t)= \mu dt +  \sigma dW(t) \hspace{10mm}  (1) \\ X(0)=x_0 $$ And a function  $$ k(x)= \theta x^2 \hspace{10mm}  (2) $$ Then by Ito Lemma  $$ dk[X(t)]=(2 \mu  \theta x +  \theta  \sigma ^2)dt + (2 \sigma  \theta x)dW(t) \hspace{10mm}  (3) \\ k[X(0)]=0 $$ And the Forward Kolmogorov to find the transition density $$ \frac{\partial p(x,t)}{\partial t}=- \frac{\partial ((2 \mu  \theta x+ \theta  \sigma^2 )p(x,t))}{\partial x} +  \frac{1}{2} \frac{\partial^2 ((2 \sigma  \theta x)^2p(x,t))}{\partial x^2} \hspace{10mm}  (4)    \\ p(x,0)= \delta (x-x_0) $$ So far so good Now, I need to convert the 2nd order pde to ordinary so I can solve it. This is why I need the Fourier Transform. List of properties: $ (a) \ \mathscr{F} \left\{ p(x,t) \right\}= \overline{p}(\xi,t)= \intop\nolimits_{-\infty}^{\infty} p(x,t) e^{-i2 \pi x \xi} dx $ $ (b) \ \mathscr{F}^{-1} \{  \overline{p} (\xi,t) \}= p(x,t)= \intop\nolimits_{-\infty}^{\infty}  \overline{p} (\xi,t) e^{i2 \pi x \xi} d\xi $ $ (c) \ \mathscr{F} \{ xp(x,t) \}= \frac{i}{-2 \pi} \frac{\partial  \overline{p} (\xi,t)}{\partial \xi}  $ $ (d) \ \mathscr{F} \{ \frac{\partial^n p(x,t)}{\partial x^n} \}=(i2 \pi \xi)^n  \overline{p} (\xi,t) $ $ (e) \ \mathscr{F} \{  \delta (x-x_0) \}= \intop\nolimits_{-\infty}^{\infty}  \delta (x-x_0) e^{-i2 \pi x \xi}dx = e^{-i2 \pi x_0 \xi }  $ So now I can start to re-write (4) term by term: $$ \ \mathscr{F} \{ \frac{\partial p(x,t)}{\partial t} \}=   \frac{\partial  \overline{p} (\xi,t)}{\partial t} \hspace{10mm} (4-1) $$ By $(f+g)'=f'+g'$ $$ \frac{\partial ((2 \mu  \theta x + \theta \sigma^2) p(x,t) )}{\partial x}= 2 \mu  \theta  \frac{\partial (xp(x,t))}{\partial x} +   \theta  \sigma^2 \frac{\partial p(x,t)}{\partial x} $$ $$  \ \mathscr{F} \left\{ 2 \mu  \theta  \frac{\partial (xp(x,t))}{\partial x} \right\} = (2 \mu  \theta) (i2 \pi \xi) \frac{i}{-2 \pi } \frac{\partial p(\xi,t)}{\partial \xi} \hspace{10mm} (4-2) $$ $$ \ \mathscr{F} \left\{ \theta  \sigma^2 \frac{\partial p(x,t)}{\partial x} \right\} = (\theta  \sigma^2) (i2 \pi \xi)  \overline{p} (\xi,t) \hspace{10mm} (4-3) $$ And I get stuck on the 2nd term of (4) $$ \ \mathscr{F} \left\{\frac{\partial^2 ((x)^2p(x,t))}{\partial x^2} \right\} =?  \hspace{10mm} (4-4) $$ Any help/hint if I can re-write (4-4) in terms of the properties would be tremendous. Thank you! Additional stuff - 2nd derivative for $xp(x,t)$ $$  \ \mathscr{F} \left\{ \frac{\partial^2 (xp(x,t))}{\partial x^2} \right\} = (i2 \pi \xi)^2 \frac{i}{-2 \pi } \frac{\partial p(\xi,t)}{\partial \xi} $$",,"['calculus', 'ordinary-differential-equations', 'derivatives', 'stochastic-processes', 'fourier-transform']"
97,A Jacobi field along a geodesic segment is the variation field of a family of geodesics,A Jacobi field along a geodesic segment is the variation field of a family of geodesics,,"I am reading an introductory textbook in Riemannian geometry (Lee), and met this problem: Any Jacobi field $V$ along a geodesic segment $\gamma$ is the variation field of some variation of $\gamma$ through geodesics. A hint is given: try to write the family $\Gamma (s,t)$ as $\exp_{\sigma(s)}tW(s)$ for some proper curve $\sigma(s)$ and vector field $W(s)$ along $\sigma$. This is reasonable since for each $s$, $\Gamma (s,t)$ gives a geodesic. The problem is of course how to find such $\sigma$ and $W$. Clearly we should require $\dot \sigma (0) = V(0)$, and $W(0)$ be determined by the original geodesic $\gamma$. But what's next? I plan to find $\partial / \partial t \ \Gamma $ and $\partial / \partial s\  \Gamma $, and see whether I can get something from the Jacobi equation. But I do not know how to differentiate  $\exp$ with respect to $\sigma(s)$! Anyone has more hint(s)? Thank a lot!","I am reading an introductory textbook in Riemannian geometry (Lee), and met this problem: Any Jacobi field $V$ along a geodesic segment $\gamma$ is the variation field of some variation of $\gamma$ through geodesics. A hint is given: try to write the family $\Gamma (s,t)$ as $\exp_{\sigma(s)}tW(s)$ for some proper curve $\sigma(s)$ and vector field $W(s)$ along $\sigma$. This is reasonable since for each $s$, $\Gamma (s,t)$ gives a geodesic. The problem is of course how to find such $\sigma$ and $W$. Clearly we should require $\dot \sigma (0) = V(0)$, and $W(0)$ be determined by the original geodesic $\gamma$. But what's next? I plan to find $\partial / \partial t \ \Gamma $ and $\partial / \partial s\  \Gamma $, and see whether I can get something from the Jacobi equation. But I do not know how to differentiate  $\exp$ with respect to $\sigma(s)$! Anyone has more hint(s)? Thank a lot!",,"['ordinary-differential-equations', 'differential-geometry', 'riemannian-geometry']"
98,Differential Equation Integrating Factor Problem,Differential Equation Integrating Factor Problem,,"I'm doing some practice to solve any exact(not) differential equations and i found this one. I'm having difficulty to determine the integrating factor which is really troublesome. I can't figure out the integrating at all, since it's not depends on one of the form $x$, $y$, $xy$, $x^2+y^2$. $$ \left(2x + \ln y\right) dx + \left( xy \right)dy = 0$$ My attempt to determine the integrating factor: $M_y=\frac{1}{y}\neq N_x =y$, $\frac{M_y-N_x}{N}={\frac{1-y^2}{xy^2}}\\$ $\frac{N_x-M_y}{M}=\frac{y^2-1}{y}.\frac{1}{2x+\ln y}$ $\mu(x,y)=\frac{M_y-N_x}{yN-xM}=\frac{\frac{1}{y}-y}{xy^2-x(2x+\ln y)}$ $\mu(x^2+y^2)=\frac{M_y-N_x}{2(xN-yM)}=\frac{\frac{1}{y}-y}{2(x^2y-y(2x+\ln y))}$ I have used both wolframalpha and maple, but i found no helps. Anybody could help me out of here? Thanks in advance.","I'm doing some practice to solve any exact(not) differential equations and i found this one. I'm having difficulty to determine the integrating factor which is really troublesome. I can't figure out the integrating at all, since it's not depends on one of the form $x$, $y$, $xy$, $x^2+y^2$. $$ \left(2x + \ln y\right) dx + \left( xy \right)dy = 0$$ My attempt to determine the integrating factor: $M_y=\frac{1}{y}\neq N_x =y$, $\frac{M_y-N_x}{N}={\frac{1-y^2}{xy^2}}\\$ $\frac{N_x-M_y}{M}=\frac{y^2-1}{y}.\frac{1}{2x+\ln y}$ $\mu(x,y)=\frac{M_y-N_x}{yN-xM}=\frac{\frac{1}{y}-y}{xy^2-x(2x+\ln y)}$ $\mu(x^2+y^2)=\frac{M_y-N_x}{2(xN-yM)}=\frac{\frac{1}{y}-y}{2(x^2y-y(2x+\ln y))}$ I have used both wolframalpha and maple, but i found no helps. Anybody could help me out of here? Thanks in advance.",,['ordinary-differential-equations']
99,A derivation of the Normal Distribution,A derivation of the Normal Distribution,,"I was reading the following article of a derivation of the Normal Distribution. In the beginning there is the following definition: Data are said to be normally distributed if the rate at which the frequencies fall off is proportional to the distance of the score from the mean, and to the frequencies themselves From this definition, the author gets the following differential equation: $$\frac{df}{dx}=-k(x-\mu)f(x)$$ What I don't get is that he later says that we can separate the variables in the following form: $$\frac{df}{f}=-k(x-\mu)~dx$$ I don't understand that part.","I was reading the following article of a derivation of the Normal Distribution. In the beginning there is the following definition: Data are said to be normally distributed if the rate at which the frequencies fall off is proportional to the distance of the score from the mean, and to the frequencies themselves From this definition, the author gets the following differential equation: $$\frac{df}{dx}=-k(x-\mu)f(x)$$ What I don't get is that he later says that we can separate the variables in the following form: $$\frac{df}{f}=-k(x-\mu)~dx$$ I don't understand that part.",,"['calculus', 'probability', 'ordinary-differential-equations']"
