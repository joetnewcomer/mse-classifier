,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Prove that $\omega(v,u)=0$ $\forall u \in \mathbb R^m$.",Prove that  .,"\omega(v,u)=0 \forall u \in \mathbb R^m","if $\omega:\mathbb R^m\times \mathbb R^m\to \mathbb R$ one alternant 2-tensor. Prove that if $m$ is odd then exist $v\in \mathbb R^m$ such that $\omega(v,u)=0$ $\forall u\in \mathbb R^m$ . I dont have idea how to start just use the definition,hence $\omega$ is alternant 2-tensor then $\omega(x,y)=-\omega(y,x)$ . So, for example $\omega(x,x)=0\quad\forall x\in \mathbb R^m$ , if i suppose that exist $\tilde{v}$ then $\omega(\tilde{v},u)=\omega(\tilde{v},u)+\omega(u,u)$ imply $\omega(\tilde{v}+u,u)=\omega(\tilde{v},u)=-\omega(u,\tilde{v})$ . But what can i do with all equalitys, how to use that $\mathbb R^m$ where $m$ is odd, somebody can help me please, thank you.","if one alternant 2-tensor. Prove that if is odd then exist such that . I dont have idea how to start just use the definition,hence is alternant 2-tensor then . So, for example , if i suppose that exist then imply . But what can i do with all equalitys, how to use that where is odd, somebody can help me please, thank you.","\omega:\mathbb R^m\times \mathbb R^m\to \mathbb R m v\in \mathbb R^m \omega(v,u)=0 \forall u\in \mathbb R^m \omega \omega(x,y)=-\omega(y,x) \omega(x,x)=0\quad\forall x\in \mathbb R^m \tilde{v} \omega(\tilde{v},u)=\omega(\tilde{v},u)+\omega(u,u) \omega(\tilde{v}+u,u)=\omega(\tilde{v},u)=-\omega(u,\tilde{v}) \mathbb R^m m","['multivariable-calculus', 'tensors']"
1,Intuition of the Surface Integral of a Real-Valued Function,Intuition of the Surface Integral of a Real-Valued Function,,"I'm having trouble understanding the idea of a surface integral of a real valued function $f$ . I've read some of the other answers here on Stack Exchange, but they seem to be focused on the surface integral of a vector field. If I have a surface $S$ , and I'm taking the surface integral $\int_S f dA$ , my understanding is that I'm finding the ""volume"" underneath the the surface S, analogous to how a line integral of a real valued function is finding the area underneath the graph of $z=f(x,y)$ when we traverse through a curve $C$ on the $x,y$ plane. But I am having trouble visualizing this. It seems like I can't visualize in a 3-d plot, since $f(x,y)$ would take 3 dimensions to plot, but the surface also lives in 3-d space, so I'm not sure what it means geometrically to take the surface integral of a real valued function. Does anyone have any analogies to help withe intuition here? Analogies: Line Integral over scalar function: Walking along a path in the x-y plane, graphing f(x,y) on the z-axis. The total area underneath the ""fence"" carved out by the the path, and f(x,y). Line Integral over vector field: Walking along a path in the x-y plane, and being pushed around by a mysterious force at each point. The total amount of ""work"" exerted on me as I walk along the curve. Surface Integral over vector field: Placing a parachute (surface) in a region with lots of turbulence, such that the force acting on the parachute at each point is different. Questions: What's the relevant one for the surface integrals over a real valued function, similar to the ones I've provided above? What's the geometric intuition? (i.e. what volume am I calculating)?","I'm having trouble understanding the idea of a surface integral of a real valued function . I've read some of the other answers here on Stack Exchange, but they seem to be focused on the surface integral of a vector field. If I have a surface , and I'm taking the surface integral , my understanding is that I'm finding the ""volume"" underneath the the surface S, analogous to how a line integral of a real valued function is finding the area underneath the graph of when we traverse through a curve on the plane. But I am having trouble visualizing this. It seems like I can't visualize in a 3-d plot, since would take 3 dimensions to plot, but the surface also lives in 3-d space, so I'm not sure what it means geometrically to take the surface integral of a real valued function. Does anyone have any analogies to help withe intuition here? Analogies: Line Integral over scalar function: Walking along a path in the x-y plane, graphing f(x,y) on the z-axis. The total area underneath the ""fence"" carved out by the the path, and f(x,y). Line Integral over vector field: Walking along a path in the x-y plane, and being pushed around by a mysterious force at each point. The total amount of ""work"" exerted on me as I walk along the curve. Surface Integral over vector field: Placing a parachute (surface) in a region with lots of turbulence, such that the force acting on the parachute at each point is different. Questions: What's the relevant one for the surface integrals over a real valued function, similar to the ones I've provided above? What's the geometric intuition? (i.e. what volume am I calculating)?","f S \int_S f dA z=f(x,y) C x,y f(x,y)","['multivariable-calculus', 'intuition', 'surface-integrals']"
2,Let $f: \mathbb{R}^n\rightarrow \mathbb{R}^n$ an application $f(x)=|x|^2x$,Let  an application,f: \mathbb{R}^n\rightarrow \mathbb{R}^n f(x)=|x|^2x,"Let $f: \mathbb{R}^n\rightarrow \mathbb{R}^n$ an application defined by the rule $f(x)=|x|^2x$ , where $|\cdot|$ and the euclidean norm, $| x | = \langle x,x \rangle.~$ Show that $f$ is of class $C^1$ (i) and carry a unit ball $B(0,1)$ over herself injectively. (ii) Show, ask, that your inverse is not differentiable at the origin (iii). (i) To prove that $f$ $\in$ $C^\infty$ , I did it like this: I know that the standard function is class $C^\infty$ and also the identity function is class $C^\infty$ . Logo $f$ is a product of functions that are class $C^\infty$ So $f$ is class $C^\infty$ . (ii) To show that $f$ takes an open ball of radius 1 centered injecting itself on the origin ... I don't know how to prove it ... looking at the function of the impression of actually being a normalized ball. Any help in that part? How do I show that you have a ball in a ball. Did you understand the reasoning of this. (iii) Finally, prove that $f^{-1}$ is not differentiable in origin. I thought of assuming that $f^{-1}$ is differentiable in origin, I thought of using the trick $f^{-1} \circ f = x$ , so if we derive, $Df^{-1}(f(x)) Df(x)=1$ . Then I don't know what I can do.","Let an application defined by the rule , where and the euclidean norm, Show that is of class (i) and carry a unit ball over herself injectively. (ii) Show, ask, that your inverse is not differentiable at the origin (iii). (i) To prove that , I did it like this: I know that the standard function is class and also the identity function is class . Logo is a product of functions that are class So is class . (ii) To show that takes an open ball of radius 1 centered injecting itself on the origin ... I don't know how to prove it ... looking at the function of the impression of actually being a normalized ball. Any help in that part? How do I show that you have a ball in a ball. Did you understand the reasoning of this. (iii) Finally, prove that is not differentiable in origin. I thought of assuming that is differentiable in origin, I thought of using the trick , so if we derive, . Then I don't know what I can do.","f: \mathbb{R}^n\rightarrow \mathbb{R}^n f(x)=|x|^2x |\cdot| | x | = \langle x,x \rangle.~ f C^1 B(0,1) f \in C^\infty C^\infty C^\infty f C^\infty f C^\infty f f^{-1} f^{-1} f^{-1} \circ f = x Df^{-1}(f(x)) Df(x)=1",['real-analysis']
3,Derivation of PCA using Multivariable Calculus,Derivation of PCA using Multivariable Calculus,,"I've been unable to solve $(12.11)$ to $(12.13)$ for the derivation of $z_{n i}$ and $b_j$ I've tried solving this for the last 3 hours, and I always end up with a completely different expression which includes the terms $z_{n i}$ and $b_j$ . This is taken from Bishop's Pattern Recognition and machine learning book. Specifically I am trying to obtain the term below, i.e step 12.13. $$  b_{j}=\overline{\mathbf{x}}^{\mathrm{T}} \mathbf{u}_{j} \tag{12.13}$$ I know I need to use the chain rule, but I am not getting the correct result! $$ J=\frac{1}{N}  \sum_{n=1}^{N}\left\|\mathbf{x}_{n}-\tilde{\mathbf{x}}_{n}\right\|^{2}  \tag{12.11} $$ Consider first of all the minimization with respect to the quantities $\left\{z_{n i}\right\} .$ Substituting for $\tilde{\mathbf{x}}_{n},$ setting the derivative with respect to $z_{n  j}$ to zero, and making use of the orthonormality conditions, we obtain $$ z_{n j}=\mathbf{x}_{n}^{\mathrm{T}} \mathbf{u}_{j}  \tag{12.12}$$ where $j=1, \ldots, M .$ Similarly, setting the derivative of $J$ with respect to $b_{i}$ to zero, and again making use of the orthonormality relations, gives $$  b_{j}=\overline{\mathbf{x}}^{\mathrm{T}} \mathbf{u}_{j} \tag{12.13}$$","I've been unable to solve to for the derivation of and I've tried solving this for the last 3 hours, and I always end up with a completely different expression which includes the terms and . This is taken from Bishop's Pattern Recognition and machine learning book. Specifically I am trying to obtain the term below, i.e step 12.13. I know I need to use the chain rule, but I am not getting the correct result! Consider first of all the minimization with respect to the quantities Substituting for setting the derivative with respect to to zero, and making use of the orthonormality conditions, we obtain where Similarly, setting the derivative of with respect to to zero, and again making use of the orthonormality relations, gives","(12.11) (12.13) z_{n i} b_j z_{n i} b_j 
 b_{j}=\overline{\mathbf{x}}^{\mathrm{T}} \mathbf{u}_{j} \tag{12.13}  J=\frac{1}{N}
 \sum_{n=1}^{N}\left\|\mathbf{x}_{n}-\tilde{\mathbf{x}}_{n}\right\|^{2}
 \tag{12.11}  \left\{z_{n i}\right\} . \tilde{\mathbf{x}}_{n}, z_{n
 j}  z_{n j}=\mathbf{x}_{n}^{\mathrm{T}} \mathbf{u}_{j}
 \tag{12.12} j=1, \ldots, M . J b_{i} 
 b_{j}=\overline{\mathbf{x}}^{\mathrm{T}} \mathbf{u}_{j} \tag{12.13}","['linear-algebra', 'multivariable-calculus', 'machine-learning', 'principal-component-analysis']"
4,"Evaluation of integral $\int_{S^2} \frac{dS}{((x-a)^2 +y^2+z^2)^{1/2}}$, where $a>1$ and $S$ is the unit sphere.","Evaluation of integral , where  and  is the unit sphere.",\int_{S^2} \frac{dS}{((x-a)^2 +y^2+z^2)^{1/2}} a>1 S,I want to evaluate $$\int_{S^2} \frac{dS}{((x-a)^2 +y^2+z^2)^{1/2}}$$ where $a >1$ and $S$ is the unit sphere. I'm not sure how to do this using only multivariable calculus techniques. My only idea was to use the fact that the function $(x^2+y^2+z^2)^{-1/2}$ is harmonic away from the origin and use the mean value formula.,I want to evaluate where and is the unit sphere. I'm not sure how to do this using only multivariable calculus techniques. My only idea was to use the fact that the function is harmonic away from the origin and use the mean value formula.,\int_{S^2} \frac{dS}{((x-a)^2 +y^2+z^2)^{1/2}} a >1 S (x^2+y^2+z^2)^{-1/2},"['integration', 'multivariable-calculus', 'spherical-coordinates']"
5,Integration identity.,Integration identity.,,"could you help me with the following problem please: Verify the following identity $\displaystyle \iint_{\partial{W}} f \frac{\partial{f}}{\partial{\hat{n}}} \,dS=\iiint_{W} \left \| \nabla f \right \| ^2\,dx\,dy\,dz$ if $f$ is harmonic My main question is, what do you mean $\displaystyle \frac{\partial{f}}{\partial{\hat{n}}}$ or if the problem is badly posed, thank you very much in advance","could you help me with the following problem please: Verify the following identity if is harmonic My main question is, what do you mean or if the problem is badly posed, thank you very much in advance","\displaystyle \iint_{\partial{W}} f \frac{\partial{f}}{\partial{\hat{n}}} \,dS=\iiint_{W} \left \| \nabla f \right \| ^2\,dx\,dy\,dz f \displaystyle \frac{\partial{f}}{\partial{\hat{n}}}","['integration', 'multivariable-calculus', 'contour-integration']"
6,Calculate $\iiint_V dx dy dz$,Calculate,\iiint_V dx dy dz,"Problem : Calculate $$\iiint_V dx dy dz$$ where $V$ is the domain bounded by the surface $(x^2+y^2+z^2)^2=a^2xy$ . My Solution : Make the following substitution: $$\begin{cases} x = r\sin\varphi\cos\theta,\\ y = r\sin\varphi\sin\theta,\\ z = r\cos\varphi \end{cases}$$ The limit of $V$ is equal to $r^2=\frac{a^2}{2}\sin^2 \varphi \sin 2\theta$ . So the integration is equal to $$\frac{2}{3}\int_0^{\frac{\pi}{2}}d \theta\int_{0}^{\pi}r^3\sin\varphi \,d\varphi =\frac{\sqrt{2}a^3}{6}\int_0^{\frac{\pi}{2}}d \theta\int_{0}^{\pi}\sin^4\varphi (\sin 2\theta)^{\frac{3}{2}}\,d\varphi $$ . But I can't figure out how to calculate $\int (\sin 2\theta)^{\frac{3}{2}}$ . I'm wondering if there's any convenient way to solve this question. I'll be grateful if there's any help. :)",Problem : Calculate where is the domain bounded by the surface . My Solution : Make the following substitution: The limit of is equal to . So the integration is equal to . But I can't figure out how to calculate . I'm wondering if there's any convenient way to solve this question. I'll be grateful if there's any help. :),"\iiint_V dx dy dz V (x^2+y^2+z^2)^2=a^2xy \begin{cases}
x = r\sin\varphi\cos\theta,\\
y = r\sin\varphi\sin\theta,\\
z = r\cos\varphi
\end{cases} V r^2=\frac{a^2}{2}\sin^2 \varphi \sin 2\theta \frac{2}{3}\int_0^{\frac{\pi}{2}}d \theta\int_{0}^{\pi}r^3\sin\varphi \,d\varphi =\frac{\sqrt{2}a^3}{6}\int_0^{\frac{\pi}{2}}d \theta\int_{0}^{\pi}\sin^4\varphi (\sin 2\theta)^{\frac{3}{2}}\,d\varphi  \int (\sin 2\theta)^{\frac{3}{2}}","['multivariable-calculus', 'definite-integrals', 'multiple-integral']"
7,Pulling a partial derivative out,Pulling a partial derivative out,,"Though the following seems intuitive, is it correct precisely? Suppose $T:\mathbb{R}^2\to\mathbb{R}$ is a smooth function. Then $$\Big({\partial\over\partial y}{\partial T\over\partial x}\Big)(0, y_0) = {d\over dy}\Big({\partial T\over\partial x}(0, y)\Big)(y_0)$$ for all $y_0\in\mathbb{R}$ . What I expect is to ""see"" the mathematical machinery that gets me from the LHS to the RHS. Thanks! Note: I'm a physics undergraduate, who is trying to understand the math (as precisely as possible) that the physics textbooks often sweep under the rug. Also, to prevent abuse of notation, I think that RHS is better written as $\mathcal{T}'(y_0)$ , where $\mathcal{T}:\mathbb{R\to R}$ is a function defined by $\mathcal{T}(y):={\partial T\over\partial x}(0, y)$","Though the following seems intuitive, is it correct precisely? Suppose is a smooth function. Then for all . What I expect is to ""see"" the mathematical machinery that gets me from the LHS to the RHS. Thanks! Note: I'm a physics undergraduate, who is trying to understand the math (as precisely as possible) that the physics textbooks often sweep under the rug. Also, to prevent abuse of notation, I think that RHS is better written as , where is a function defined by","T:\mathbb{R}^2\to\mathbb{R} \Big({\partial\over\partial y}{\partial T\over\partial x}\Big)(0, y_0) = {d\over dy}\Big({\partial T\over\partial x}(0, y)\Big)(y_0) y_0\in\mathbb{R} \mathcal{T}'(y_0) \mathcal{T}:\mathbb{R\to R} \mathcal{T}(y):={\partial T\over\partial x}(0, y)",['multivariable-calculus']
8,Evaluate the integral $\iint_S(x)dy\wedge dz+(x+y)dz\wedge dx+(x^2+2z)dx\wedge dy$,Evaluate the integral,\iint_S(x)dy\wedge dz+(x+y)dz\wedge dx+(x^2+2z)dx\wedge dy,"In a problem from my multivariable integration class, i've reached this problem. I will thank any comment with advice or answer. The problem asks me to calculate the integral $$\iint_S(x)dy\wedge dz+(x+y)dz\wedge dx+(x^2+2z)dx\wedge dy$$ Being $S$ the surface of the solid $V$ limited by: $$S_1=\{(x,y,z)\in\mathbb{R}:2x^2+y^2=4z\},$$ $$S_2=\{(x,y,z)\in\mathbb{R}:x^2+2z=2\}.$$ I'm told to solve twice it using two different methods: direct integration and Gauss' Theorem (divergence) . I've started trying with Gauss' Theorem, but i don't really get if i'm getting it correctly. The theorem tells that (under certaing region and surface conditions that this problem verifies) given $V$ a solid limited by a closed surface $S$ , $N$ the normal vector, and $F=(P,Q,R)$ a vectorial field of class $C^1$ , $$\iint_{\partial V}F=\iint_S(F\cdot N)d\sigma = \iiint_V\text{div}(F)dxdydz.$$ Being $\text{div}(F)=\frac{\partial P}{\partial x}+\frac{\partial Q}{\partial y}+\frac{\partial R}{\partial z}$ . I've started trying to find $P,Q,R$ in my example, but I'm not sure if that's possible, at least with my actual knowledge. What can I do to find my $\text{div}(F)$ ?. For the direct integration part, I have no idea about how to do it. I specially struggle finding the integration limits. Bounty edit: I need a step-by-step solution for both methods: direct integration and Gauss' Theroem (Divergence Theorem).","In a problem from my multivariable integration class, i've reached this problem. I will thank any comment with advice or answer. The problem asks me to calculate the integral Being the surface of the solid limited by: I'm told to solve twice it using two different methods: direct integration and Gauss' Theorem (divergence) . I've started trying with Gauss' Theorem, but i don't really get if i'm getting it correctly. The theorem tells that (under certaing region and surface conditions that this problem verifies) given a solid limited by a closed surface , the normal vector, and a vectorial field of class , Being . I've started trying to find in my example, but I'm not sure if that's possible, at least with my actual knowledge. What can I do to find my ?. For the direct integration part, I have no idea about how to do it. I specially struggle finding the integration limits. Bounty edit: I need a step-by-step solution for both methods: direct integration and Gauss' Theroem (Divergence Theorem).","\iint_S(x)dy\wedge dz+(x+y)dz\wedge dx+(x^2+2z)dx\wedge dy S V S_1=\{(x,y,z)\in\mathbb{R}:2x^2+y^2=4z\}, S_2=\{(x,y,z)\in\mathbb{R}:x^2+2z=2\}. V S N F=(P,Q,R) C^1 \iint_{\partial V}F=\iint_S(F\cdot N)d\sigma = \iiint_V\text{div}(F)dxdydz. \text{div}(F)=\frac{\partial P}{\partial x}+\frac{\partial Q}{\partial y}+\frac{\partial R}{\partial z} P,Q,R \text{div}(F)","['integration', 'multivariable-calculus', 'lebesgue-integral', 'differential-forms', 'surface-integrals']"
9,Verification of Integrals,Verification of Integrals,,"I am very new to integrals. If someone would kindly take a look at them and confirm they are set up correctly that would be great! D is the triangle with vertices $(0,0) (4,-2) (4,8)$ Evaluate $\int \int_D e^{x^2 +1}dA$ My attempt : $\int_0^4 \int_0^{2x}  e^{x^2 +1}dydx$ + $\int_0^4 \int_0^{\frac{1}{2}x}  e^{x^2 +1}dydx$ $\approx 18116212.53$ D is the region where $x \geq 0$ bounded by $z = 4-x^2-y^2$ and xy plane. Evaluate $\int \int \int_D z+2dA$ My attempt : $\int_\frac{-\pi}{2}^{\frac{\pi}{2}} \int_0^2 \int_0^{4-r^2} (z+2)r dzdrd\theta$ $= \frac{40\pi}{3}$",I am very new to integrals. If someone would kindly take a look at them and confirm they are set up correctly that would be great! D is the triangle with vertices Evaluate My attempt : + D is the region where bounded by and xy plane. Evaluate My attempt :,"(0,0) (4,-2) (4,8) \int \int_D e^{x^2 +1}dA \int_0^4 \int_0^{2x}  e^{x^2 +1}dydx \int_0^4 \int_0^{\frac{1}{2}x}  e^{x^2 +1}dydx \approx 18116212.53 x \geq 0 z = 4-x^2-y^2 \int \int \int_D z+2dA \int_\frac{-\pi}{2}^{\frac{\pi}{2}} \int_0^2 \int_0^{4-r^2} (z+2)r dzdrd\theta = \frac{40\pi}{3}","['calculus', 'integration', 'multivariable-calculus', 'solution-verification']"
10,"The function $R(x)=rank(Df(x))$ is locally constant on $\Omega$, i.e. it is constant in a neighbourhood of every point $x \in \Omega$.","The function  is locally constant on , i.e. it is constant in a neighbourhood of every point .",R(x)=rank(Df(x)) \Omega x \in \Omega,"Let $f:\Bbb R^n \to \Bbb R^n$ be a mapping of class $C^1$ . Prove that there is an open and dense set $\Omega \subseteq \Bbb R^n$ such that the function $R(x)=rank(Df(x))$ is locally constant on $\Omega$ , i.e. it is constant in a neighbourhood of every point $x \in \Omega$ . I was first thinking to use rank theorem which states that if $Df(x_0)=m$ then $\exists $ a diffeomorphism $\Phi$ in the neighbourhood of $x_0$ and $\Psi$ in the neighbourhood of $f(x_0)$ such that $\Psi \circ f \circ \Phi^{-1}(x_1, \cdots , x_n)=(x_1, , \cdots , x_m)$ . Then I was thinking to take differentiation and then chain rule but it won't work as we will get $D(f( \Phi^{-1}))$ . The second way I was thinking was to use the dense property e.g: Can we say anything about $\{x \in \Bbb R^n| D(f(x))\geq k\}$ and then work with it? I deleted my earlier post as that was incorrect. Please help..","Let be a mapping of class . Prove that there is an open and dense set such that the function is locally constant on , i.e. it is constant in a neighbourhood of every point . I was first thinking to use rank theorem which states that if then a diffeomorphism in the neighbourhood of and in the neighbourhood of such that . Then I was thinking to take differentiation and then chain rule but it won't work as we will get . The second way I was thinking was to use the dense property e.g: Can we say anything about and then work with it? I deleted my earlier post as that was incorrect. Please help..","f:\Bbb R^n \to \Bbb R^n C^1 \Omega \subseteq \Bbb R^n R(x)=rank(Df(x)) \Omega x \in \Omega Df(x_0)=m \exists  \Phi x_0 \Psi f(x_0) \Psi \circ f \circ \Phi^{-1}(x_1, \cdots , x_n)=(x_1, , \cdots , x_m) D(f( \Phi^{-1})) \{x \in \Bbb R^n| D(f(x))\geq k\}","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'manifolds', 'differential-topology']"
11,Is Leibniz rule applied correctly?,Is Leibniz rule applied correctly?,,"I wish to know if the Leibniz rule is correctly applied in the following equation or I am missing something: $$\int \int \frac{\partial f(x,y,z)}{\partial x} dy dz =\frac{\partial  \left(\int \int f(x,y,z)dy dz \right)}{\partial x}  $$ $x, y$ and $z$ are independent of each other. The integration limits are constant. Thanks in advance.",I wish to know if the Leibniz rule is correctly applied in the following equation or I am missing something: and are independent of each other. The integration limits are constant. Thanks in advance.,"\int \int \frac{\partial f(x,y,z)}{\partial x} dy dz =\frac{\partial  \left(\int \int f(x,y,z)dy dz \right)}{\partial x}   x, y z","['calculus', 'multivariable-calculus', 'leibniz-integral-rule']"
12,Help with the definition of the Gradient in Multi-variable Calculus,Help with the definition of the Gradient in Multi-variable Calculus,,"I was studying Multi-variable Calculus, and I got confused with the definition of the Gradient. The definition that I learned was this: But, doing some examples, and searching in Google I saw that the Gradient vector can be normal to every tangent plane to the surface given. The picture below gives the tangent plane and the gradient vector normal to that tangent plane. So, I got confused because I do not understand is the Gradient vector normal only to the level curves or also to other tangent planes? And if they are tangent to other planes, why there are a lot definition that define it differently? Update: Based on the comments what I understood was that a function of two variables has a level CURVE, and gradient is normal to that level curve. But, functions of three variables don’t have level curves but level SURFACES, that is why the gradient is normal to every point in that level surface. So, the second picture is the gradient vector normal at the level surface of some function with three variables. This is what I understood. If you believe that is not correct, than please provide with some explanation. The sources for the 1st and 2nd pictures: http://tutorial.math.lamar.edu/Classes/CalcIII/GradientVectorTangentPlane.aspx https://bvmtc.math.tamu.edu/~glahodny/Math251/Section%2012.6.pdf","I was studying Multi-variable Calculus, and I got confused with the definition of the Gradient. The definition that I learned was this: But, doing some examples, and searching in Google I saw that the Gradient vector can be normal to every tangent plane to the surface given. The picture below gives the tangent plane and the gradient vector normal to that tangent plane. So, I got confused because I do not understand is the Gradient vector normal only to the level curves or also to other tangent planes? And if they are tangent to other planes, why there are a lot definition that define it differently? Update: Based on the comments what I understood was that a function of two variables has a level CURVE, and gradient is normal to that level curve. But, functions of three variables don’t have level curves but level SURFACES, that is why the gradient is normal to every point in that level surface. So, the second picture is the gradient vector normal at the level surface of some function with three variables. This is what I understood. If you believe that is not correct, than please provide with some explanation. The sources for the 1st and 2nd pictures: http://tutorial.math.lamar.edu/Classes/CalcIII/GradientVectorTangentPlane.aspx https://bvmtc.math.tamu.edu/~glahodny/Math251/Section%2012.6.pdf",,"['multivariable-calculus', 'vector-analysis']"
13,Fourier transform difficulty 2D,Fourier transform difficulty 2D,,"I have solved easily the (1) but I am really struggling with (2) and (3) . Do i need to use change of order of integration, because of so many parameters... i am confused a little bit. My work: $$\hat{Xf}(\xi,\theta)=\int\limits_{-\infty}^{\infty}\left(\int\limits_{-\infty}^{\infty}f(t\cos\theta+s\sin\theta,t\sin\theta - s\cos\theta) ds\right)e^{-2\pi i \xi t} dt = ??$$ How do I achieve $d\theta$ , any hints please? I got Jacobian as $t$ So, $$dxdy=t dt d\theta$$ I guess I am very close, I have also obtained $$\hat{f}(\xi\cos\theta,\xi\sin\theta)=\int\limits_{-\infty}^{\infty}\int\limits_{-\pi}^{\pi}f(t\cos\theta+s\sin\theta,t\sin\theta - s\cos\theta)e^{-2\pi i \xi t}  t d\theta dt$$","I have solved easily the (1) but I am really struggling with (2) and (3) . Do i need to use change of order of integration, because of so many parameters... i am confused a little bit. My work: How do I achieve , any hints please? I got Jacobian as So, I guess I am very close, I have also obtained","\hat{Xf}(\xi,\theta)=\int\limits_{-\infty}^{\infty}\left(\int\limits_{-\infty}^{\infty}f(t\cos\theta+s\sin\theta,t\sin\theta - s\cos\theta) ds\right)e^{-2\pi i \xi t} dt = ?? d\theta t dxdy=t dt d\theta \hat{f}(\xi\cos\theta,\xi\sin\theta)=\int\limits_{-\infty}^{\infty}\int\limits_{-\pi}^{\pi}f(t\cos\theta+s\sin\theta,t\sin\theta - s\cos\theta)e^{-2\pi i \xi t}  t d\theta dt","['calculus', 'integration', 'multivariable-calculus', 'fourier-analysis', 'fourier-transform']"
14,Derivative of a map $ f:\mathbb{R^n\times R^n}\rightarrow\mathbb{R}$,Derivative of a map, f:\mathbb{R^n\times R^n}\rightarrow\mathbb{R},"I want to calculate the derivative of a function $$f:\mathbb{R^n×R^n}\rightarrow\mathbb{R}$$ defined by $$f(x,y)=\langle Ax,y \rangle,$$ where $A$ is any $n\times n$ matrix over set of reals $\mathbb{R}$ . I have never seen such types questions to calculate derivative in which domain is $\mathbb{R^n\times R^n}$ . Basically my question is that, is the derivative of $f$ is same  as that of    function $$g:\mathbb{R^{2n}}\rightarrow\mathbb{R}$$ defined by $$g(x_1,x_2,.......x_n,y_1,y_2,.....y_n)=\langle Ax,y \rangle,$$ where $x=(x_1,x_2......x_n)$ , $y=(y_1,y_2.....y_n)$ . Your help would be precious to me, thanks in advance!","I want to calculate the derivative of a function defined by where is any matrix over set of reals . I have never seen such types questions to calculate derivative in which domain is . Basically my question is that, is the derivative of is same  as that of    function defined by where , . Your help would be precious to me, thanks in advance!","f:\mathbb{R^n×R^n}\rightarrow\mathbb{R} f(x,y)=\langle Ax,y \rangle, A n\times n \mathbb{R} \mathbb{R^n\times R^n} f g:\mathbb{R^{2n}}\rightarrow\mathbb{R} g(x_1,x_2,.......x_n,y_1,y_2,.....y_n)=\langle Ax,y \rangle, x=(x_1,x_2......x_n) y=(y_1,y_2.....y_n)","['real-analysis', 'calculus', 'functional-analysis', 'multivariable-calculus']"
15,Energy method in the Wave equation,Energy method in the Wave equation,,"Let $U\in\mathbb{R}^n$ be open, bounded, and connected, with a smooth boundary $\partial U$ . Suppose that $u=u(x,t)$ is a smooth solution of the initial-boundary-value problem. $$ \left\{  \begin{aligned} u_{tt} - \Delta u + u^3 &=0\quad~~~~~~ \text{in}~ U\times (0,T]\\ u&=0\quad~~~~~~\text{on}~\partial U \times [0,T]\\ u(x,0)=0,\quad u_t(x,0)&=h(x)\quad~\text{in}~U\times\{t=0\} \end{aligned} \right. $$ Show that for each $t>0$ , $$ \int\limits_{U} \frac{1}{2}\left[u_{t}(x, t)\right]^{2}+\frac{1}{2}|D u(x, t)|^{2}\, d x \leq \int\limits_{U} \frac{1}{2}[h(x)]^{2}\, d x. $$ My Attempt: Suppose $E(t)=\int\limits_{U} \frac{1}{2}\left[u_{t}(x, t)\right]^{2}+\frac{1}{2}|D u(x, t)|^{2}\, d x$ . Then by using the Greens Identity we can get it to $\frac{d}{dt}E(t)=\int\limits_{U}u_t(u_{tt}-\Delta u)dx=-\int\limits_{U}u_tu^3$ . I was trying to show that this derivative is negative. so that $E(t)$ is decreasing. Hence $E(t)\leq E(0)$ that gives the answer. But I don't see how should I prove that the derivative is negative. Am I doing something wrong?","Let be open, bounded, and connected, with a smooth boundary . Suppose that is a smooth solution of the initial-boundary-value problem. Show that for each , My Attempt: Suppose . Then by using the Greens Identity we can get it to . I was trying to show that this derivative is negative. so that is decreasing. Hence that gives the answer. But I don't see how should I prove that the derivative is negative. Am I doing something wrong?","U\in\mathbb{R}^n \partial U u=u(x,t) 
\left\{ 
\begin{aligned}
u_{tt} - \Delta u + u^3 &=0\quad~~~~~~ \text{in}~ U\times (0,T]\\
u&=0\quad~~~~~~\text{on}~\partial U \times [0,T]\\
u(x,0)=0,\quad u_t(x,0)&=h(x)\quad~\text{in}~U\times\{t=0\}
\end{aligned}
\right.
 t>0 
\int\limits_{U} \frac{1}{2}\left[u_{t}(x, t)\right]^{2}+\frac{1}{2}|D u(x, t)|^{2}\, d x \leq \int\limits_{U} \frac{1}{2}[h(x)]^{2}\, d x.
 E(t)=\int\limits_{U} \frac{1}{2}\left[u_{t}(x, t)\right]^{2}+\frac{1}{2}|D u(x, t)|^{2}\, d x \frac{d}{dt}E(t)=\int\limits_{U}u_t(u_{tt}-\Delta u)dx=-\int\limits_{U}u_tu^3 E(t) E(t)\leq E(0)","['multivariable-calculus', 'partial-differential-equations']"
16,Proving sum of integrals is integral of sums.,Proving sum of integrals is integral of sums.,,"This comes from Spivak ""Calculus on Manifolds"" problem 3-3. I think I have proof but it seems too simple to be true so I would like others to look through it and maybe point me to some subtle issues. I want to prove that if $f$ and $g$ are integrable functions $A \to \mathbb{R}$ where $A$ is a closed rectangle in $\mathbb{R}^n$ , then $f+g$ is also integrable. Also, I would like to prove that the values are related in the following way. $$ \int_A f+g = \int_A f + \int_A g$$ I assume that the following has already been proven. Here, $L(f,P)$ is lower sum of $f$ for partition $P$ , $U(f,P)$ is upper sum of $f$ for partition $P$ . $$ L(f,P) + L(g,P) \leq L(f+g,P) $$ $$ U(f,P) + U(g,P) \geq U(f+g,P) $$ Then, because $f$ and $g$ are integrable, then for every $\varepsilon/2 > 0$ we can find partitions $P$ and $P'$ such that the following is true. $$ U(f,P) - \frac{\varepsilon}{2} \leq \int_A f \leq L(f,P) + \frac{\varepsilon}{2}$$ $$ U(g,P') - \frac{\varepsilon}{2} \leq \int_A g \leq L(g,P') + \frac{\varepsilon}{2} $$ Combining results, we get the following. $$ U(f,P) + U(g,P') - \varepsilon \leq \int_A f+\int_A g \leq L(f,P)+L(g,P')  +\varepsilon $$ Now, take refinement of $P$ , $P'$ and call it $P''$ . Then we have the following results for $f$ and similarly for $g$ . $$ U(f,P'') \leq U(f,P) $$ $$ L(f,P'') \geq L(f,P) $$ So, combining multiple results, we have the following inequalities. $$ U(f+g, P'') - \varepsilon \leq \int_A f+ \int_A g \leq L(f+g,P'') + \varepsilon$$ From here, first of all, we see that $f+g$ is integrable. $$ U(f+g, P'') - L(f+g, P'') \leq 2 \varepsilon $$ Now, consider $\mathrm{sup}(L(f+g,P'')) = \alpha$ . Assume that $\alpha < \int_A f+ \int_A g $ .  But then take $\varepsilon = (1/2)(\int_A f+ \int_A g - \alpha) > 0$ . In that case, the following inequality shows that it contradicts that $\alpha$ is an upper bound. $$ \alpha < \frac{\alpha + \int_A f + \int_A g}{2} = -\frac{\int_A f + \int_A g - \alpha}{2} + \int_A f + \int_A g \leq L(f+g,P'') $$ Assume that $\alpha > \int_A f + \int_A g$ . But then take $\varepsilon = (1/2)(\alpha-\int_A f - \int_A g) > 0$ . In that case, the following inequality shows that it contradicts that $\alpha$ is the least upper bound. This is due to each upper sum is an upper bound for the set of lower sums. $$ \alpha > \frac{\alpha + \int_A f + \int_A g}{2} = \frac{\alpha - \int_A f - \int_A g}{2} +\int_A f + \int_A g \geq U(f+g,P'') $$ I would really appreciate your thoughts and comments.","This comes from Spivak ""Calculus on Manifolds"" problem 3-3. I think I have proof but it seems too simple to be true so I would like others to look through it and maybe point me to some subtle issues. I want to prove that if and are integrable functions where is a closed rectangle in , then is also integrable. Also, I would like to prove that the values are related in the following way. I assume that the following has already been proven. Here, is lower sum of for partition , is upper sum of for partition . Then, because and are integrable, then for every we can find partitions and such that the following is true. Combining results, we get the following. Now, take refinement of , and call it . Then we have the following results for and similarly for . So, combining multiple results, we have the following inequalities. From here, first of all, we see that is integrable. Now, consider . Assume that .  But then take . In that case, the following inequality shows that it contradicts that is an upper bound. Assume that . But then take . In that case, the following inequality shows that it contradicts that is the least upper bound. This is due to each upper sum is an upper bound for the set of lower sums. I would really appreciate your thoughts and comments.","f g A \to \mathbb{R} A \mathbb{R}^n f+g  \int_A f+g = \int_A f + \int_A g L(f,P) f P U(f,P) f P  L(f,P) + L(g,P) \leq L(f+g,P)   U(f,P) + U(g,P) \geq U(f+g,P)  f g \varepsilon/2 > 0 P P'  U(f,P) - \frac{\varepsilon}{2} \leq \int_A f \leq L(f,P) + \frac{\varepsilon}{2}  U(g,P') - \frac{\varepsilon}{2} \leq \int_A g \leq L(g,P') + \frac{\varepsilon}{2}   U(f,P) + U(g,P') - \varepsilon \leq \int_A f+\int_A g \leq L(f,P)+L(g,P')  +\varepsilon  P P' P'' f g  U(f,P'') \leq U(f,P)   L(f,P'') \geq L(f,P)   U(f+g, P'') - \varepsilon \leq \int_A f+ \int_A g \leq L(f+g,P'') + \varepsilon f+g  U(f+g, P'') - L(f+g, P'') \leq 2 \varepsilon  \mathrm{sup}(L(f+g,P'')) = \alpha \alpha < \int_A f+ \int_A g  \varepsilon = (1/2)(\int_A f+ \int_A g - \alpha) > 0 \alpha  \alpha < \frac{\alpha + \int_A f + \int_A g}{2} = -\frac{\int_A f + \int_A g - \alpha}{2} + \int_A f + \int_A g \leq L(f+g,P'')  \alpha > \int_A f + \int_A g \varepsilon = (1/2)(\alpha-\int_A f - \int_A g) > 0 \alpha  \alpha > \frac{\alpha + \int_A f + \int_A g}{2} = \frac{\alpha - \int_A f - \int_A g}{2} +\int_A f + \int_A g \geq U(f+g,P'') ","['integration', 'multivariable-calculus']"
17,Find κ and λ $\in R$ for which $\frac {x^2+κx+λ}{x^2+1} \leq 2$ for all $x \in R$.,Find κ and λ  for which  for all .,\in R \frac {x^2+κx+λ}{x^2+1} \leq 2 x \in R,"By doing the math, we get $x^2-κx+2-λ \geq 0$ . Also $D = κ^2 - 4(2-λ)$ ...but I don't know how to continue!","By doing the math, we get . Also ...but I don't know how to continue!",x^2-κx+2-λ \geq 0 D = κ^2 - 4(2-λ),"['calculus', 'multivariable-calculus', 'quadratics', 'discriminant']"
18,Prove $v$ is not surjective,Prove  is not surjective,v,Let $ v$ : $\mathbb R \rightarrow \mathbb R^2$ be a differentiable function such that the velocity vector $\cfrac{dv}{ dt}\neq 0$ at all $t\in \mathbb R$ . Prove that $v$ is not surjective. I tried to work with definition and tried to prove contrapositive statement but It get me nowhere. Could anyone give me a hint to start with (not solution).,Let : be a differentiable function such that the velocity vector at all . Prove that is not surjective. I tried to work with definition and tried to prove contrapositive statement but It get me nowhere. Could anyone give me a hint to start with (not solution).," v \mathbb R \rightarrow \mathbb R^2 \cfrac{dv}{
dt}\neq 0 t\in \mathbb R v","['multivariable-calculus', 'differential-topology']"
19,When is a matrix-valued function the Hessian of some function?,When is a matrix-valued function the Hessian of some function?,,"What are necessary and sufficient conditions for a matrix valued function $H: \mathbb{R}^d \rightarrow \mathbb{R}^{d\times{}d}$ to be the Hessian of a convex scalar function $F:\mathbb{R}^d \rightarrow \mathbb{R}$ , i.e. $H(x) = \nabla^2 F(x)$ . I have heard informally that the following are necessary and sufficient: 1) $H(x)$ is symmetric and positive semi-definite 2) $\frac{\partial}{\partial x_k} H_{ij}(x) = \frac{\partial}{\partial x_j} H_{ik}(x)$ for all $i,j,k$ however, I have not been able to find any rigorous statement.","What are necessary and sufficient conditions for a matrix valued function to be the Hessian of a convex scalar function , i.e. . I have heard informally that the following are necessary and sufficient: 1) is symmetric and positive semi-definite 2) for all however, I have not been able to find any rigorous statement.","H: \mathbb{R}^d \rightarrow \mathbb{R}^{d\times{}d} F:\mathbb{R}^d \rightarrow \mathbb{R} H(x) = \nabla^2 F(x) H(x) \frac{\partial}{\partial x_k} H_{ij}(x) = \frac{\partial}{\partial x_j} H_{ik}(x) i,j,k","['real-analysis', 'multivariable-calculus']"
20,How do I find the minimum value of this function?,How do I find the minimum value of this function?,,"The question is: Consider the domain $\{ (x,y) \in \mathbb{R}^2 ; x \le y \}$ The function $h$ is mapped from this domain to $R$ — the set of all real numbers: $$h(x,y) = (x-2)^4 + (y-1)^4.$$ Find the minimum value of this function. I found out the necessary partial derivatives — $h_x$ and $h_y$ and equated them to $0$ to get $x=2$ and $y=1$ . Further, $h_{xx}$ , $h_{yy}$ and $h_{xy}$ are all zero for these values of $x=2$ and $y=1$ .  So,H = $h_{xx}h_{yy} - h_{xy}^2$ is coming out to equal 0. How should I proceed now?","The question is: Consider the domain The function is mapped from this domain to — the set of all real numbers: Find the minimum value of this function. I found out the necessary partial derivatives — and and equated them to to get and . Further, , and are all zero for these values of and .  So,H = is coming out to equal 0. How should I proceed now?","\{ (x,y) \in \mathbb{R}^2 ; x \le y \} h R h(x,y) = (x-2)^4 + (y-1)^4. h_x h_y 0 x=2 y=1 h_{xx} h_{yy} h_{xy} x=2 y=1 h_{xx}h_{yy} - h_{xy}^2","['calculus', 'multivariable-calculus', 'functions', 'maxima-minima']"
21,"Does $\iint_D \frac{x^2}{x^2+y^2} dx dy $ converge on $D= \left\{ (x, y) : x^2+y^2\leq ax \right\} $ ? If yes, what value does it converge to?","Does  converge on  ? If yes, what value does it converge to?","\iint_D \frac{x^2}{x^2+y^2} dx dy  D= \left\{ (x, y) : x^2+y^2\leq ax \right\} ","Powers equal to $2$ entice the polar substitution. $D= \left\{ (x, y) : x^2+y^2\leq ax \right\}$ , so $0 \leq r \leq a \cos \phi.$ For the domain to make sense, we need either $\phi \in [0, \frac{\pi}{2}] \cup [\frac{3 \pi}{2}, 2\pi)$ and $a \geq 0$ , or $\phi \in [\frac{\pi}{2}, \frac{3 \pi}{2}]$ and $a \leq 0$ . Everything below was achieved with help from the comments. $$\iint_D \frac{x^2}{x^2+y^2} dx dy = \iint_{D'} r \cos^2 \phi \ d\phi dr.$$ Case $1$ : Since $(0, 0) \in D $ (equivalently, $r=0$ for any $\phi$ ) makes the integrand indefinite, $$ \lim_{\epsilon \rightarrow 0} \int_{0}^{\frac{\pi}{2}} \cos^2 \phi \ d \phi  \int_{\epsilon}^{a \cos \phi} r \ dr  +  \lim_{\epsilon \rightarrow 0} \int_{\frac{3 \pi}{2}}^{2\pi} \cos^2 \phi \ d \phi  \int_{\epsilon}^{a \cos \phi} r \ dr $$ $$ 2\lim_{\epsilon \rightarrow 0} \int_{0}^{\frac{\pi}{2}} \cos^2 \phi \ d \phi  \int_{\epsilon}^{a \cos \phi} r \ dr  $$ $$ 2\lim_{\epsilon \rightarrow 0} \int_{0}^{\frac{\pi}{2}} \left( \frac{a^2 \cos^2 \phi}{2}-\frac{\epsilon^2}{2} \right) \cos^2 \phi \ d \phi  $$ $$ \lim_{\epsilon \rightarrow 0} \int_{0}^{\frac{\pi}{2}} \left( {a^2 \cos^2 \phi}-{\epsilon^2} \right) \cos^2 \phi \ d \phi  $$ Put $0$ instead of $\epsilon$ , because it won't let me integrate: $$ a^2 \int_{0}^{\frac{\pi}{2}} { \cos^4 \phi} \ d \phi = \frac{a^2}{2} Β \left(\frac{1}{2}, \frac{5}{2} \right) = \frac{a^2}{2} \cdot \frac{\Gamma(\frac{1}{2}) \Gamma(\frac{5}{2})}{\Gamma(3)} = \frac{a^2}{2} \cdot \frac{\sqrt{\pi} \cdot 0.75 \sqrt{\pi}}{2} = \frac{3 a^2 \pi}{16} $$ For case $2$ , the result should be same. Was this correct? Formula $Β(m, n)=2\int_0^{\pi/2}\sin^{2m-1} \theta \cos^{2n-1} \theta \ d \theta$ was used, but I'm not sure I can extract it myself, and would be grateful for reference. The answer would be, that for fixed $a$ , it's convergent. (I am not sure about the definition of convergence here, and can't look it up, since the classes were missed, and google is blocked). For Oliver Jones: let $r_i=a\frac{i}{i+1}$ , $\phi_i=\frac{\delta}{i}.$ $$\iint_{D_\delta} r \cos^2 \phi \ d\phi dr= \lim_{i \rightarrow \infty} \sum_i \left(a\frac{i}{i+1}\right) \left(\cos^2 \frac{\delta}{i} \right)  \left(  a\frac{i+1}{i+2} - a\frac{i}{i+2} \right)  \left(  \frac{\delta}{i+1}-\frac{\delta}{i} \right) = \lim_{i \rightarrow \infty} \sum_i \left(a\frac{i}{i+1}\right) \left(1 \right)  \left(  \frac{a}{(i+1)(i+2)} \right)  \left(  \frac{-\delta}{i(i+1)} \right) =  \sum_{i=0}^{\infty} \left(a^2\frac{1}{i+1}\right) \left(1 \right)  \left(  \frac{1}{(i+1)(i+2)} \right)  \left(  \frac{-\delta}{(i+1)} \right). $$ It converges because of the $4$ th power in the denominator. Or maybe like this: $$ \lim_{\epsilon \rightarrow 0} \int_{\epsilon}^a x \ dx \int_{-\sqrt{x(a-x)}}^{\sqrt{x(a-x)}} \frac{1}{1+ (y/x)^2} d (y/x)$$ $$ \lim_{\epsilon \rightarrow 0} \int_{\epsilon}^a 2 x \arctan \sqrt{x(a-x)} \ dx  $$ How to proceed here, I am not sure. $$ \lim_{\delta \rightarrow 0}\iint_{\delta^2 \leq r^2+ar \cos \theta + a^2/4 \leq a^2/4} r \cos ^2 \theta \ dr \ d\theta $$ Let's choose $\theta \in [-\pi/2, \pi/2], a\geq 0$ . Then $ \delta^2 \leq r^2 - ar + a^2/4 \leq  r^2+ar \cos \theta + a^2/4 \leq r^2 + a^2/4 \leq a^2/4. $ $ 0 \leq r^2 - ar + a^2/4 -\delta^2 \Longrightarrow r\in (0, \frac{a-2\delta}{2}). $ $$ \lim_{\delta \to 0} \int_{-\pi/2}^{\pi/2} \cos ^2 \theta \ d \theta \int_0^{(a-2\delta)/2} r dr = \frac{a^2}{8} В(1/2, 3/2) = \frac{a^2 \pi}{16} $$","Powers equal to entice the polar substitution. , so For the domain to make sense, we need either and , or and . Everything below was achieved with help from the comments. Case : Since (equivalently, for any ) makes the integrand indefinite, Put instead of , because it won't let me integrate: For case , the result should be same. Was this correct? Formula was used, but I'm not sure I can extract it myself, and would be grateful for reference. The answer would be, that for fixed , it's convergent. (I am not sure about the definition of convergence here, and can't look it up, since the classes were missed, and google is blocked). For Oliver Jones: let , It converges because of the th power in the denominator. Or maybe like this: How to proceed here, I am not sure. Let's choose . Then","2 D= \left\{ (x, y) : x^2+y^2\leq ax \right\} 0 \leq r \leq a \cos \phi. \phi \in [0, \frac{\pi}{2}] \cup [\frac{3 \pi}{2}, 2\pi) a \geq 0 \phi \in [\frac{\pi}{2}, \frac{3 \pi}{2}] a \leq 0 \iint_D \frac{x^2}{x^2+y^2} dx dy = \iint_{D'} r \cos^2 \phi \ d\phi dr. 1 (0, 0) \in D  r=0 \phi 
\lim_{\epsilon \rightarrow 0}
\int_{0}^{\frac{\pi}{2}} \cos^2 \phi \ d \phi 
\int_{\epsilon}^{a \cos \phi} r \ dr 
+ 
\lim_{\epsilon \rightarrow 0}
\int_{\frac{3 \pi}{2}}^{2\pi} \cos^2 \phi \ d \phi 
\int_{\epsilon}^{a \cos \phi} r \ dr
 
2\lim_{\epsilon \rightarrow 0}
\int_{0}^{\frac{\pi}{2}} \cos^2 \phi \ d \phi 
\int_{\epsilon}^{a \cos \phi} r \ dr 
 
2\lim_{\epsilon \rightarrow 0}
\int_{0}^{\frac{\pi}{2}} \left( \frac{a^2 \cos^2 \phi}{2}-\frac{\epsilon^2}{2} \right) \cos^2 \phi \ d \phi 
 
\lim_{\epsilon \rightarrow 0}
\int_{0}^{\frac{\pi}{2}} \left( {a^2 \cos^2 \phi}-{\epsilon^2} \right) \cos^2 \phi \ d \phi 
 0 \epsilon 
a^2
\int_{0}^{\frac{\pi}{2}} { \cos^4 \phi} \ d \phi = \frac{a^2}{2} Β \left(\frac{1}{2}, \frac{5}{2} \right) = \frac{a^2}{2} \cdot \frac{\Gamma(\frac{1}{2}) \Gamma(\frac{5}{2})}{\Gamma(3)} = \frac{a^2}{2} \cdot \frac{\sqrt{\pi} \cdot 0.75 \sqrt{\pi}}{2} = \frac{3 a^2 \pi}{16}
 2 Β(m, n)=2\int_0^{\pi/2}\sin^{2m-1} \theta \cos^{2n-1} \theta \ d \theta a r_i=a\frac{i}{i+1} \phi_i=\frac{\delta}{i}. \iint_{D_\delta} r \cos^2 \phi \ d\phi dr= \lim_{i \rightarrow \infty} \sum_i \left(a\frac{i}{i+1}\right) \left(\cos^2 \frac{\delta}{i} \right) 
\left( 
a\frac{i+1}{i+2} - a\frac{i}{i+2}
\right) 
\left( 
\frac{\delta}{i+1}-\frac{\delta}{i}
\right) =
\lim_{i \rightarrow \infty} \sum_i \left(a\frac{i}{i+1}\right) \left(1 \right) 
\left( 
\frac{a}{(i+1)(i+2)}
\right) 
\left( 
\frac{-\delta}{i(i+1)}
\right)
=
 \sum_{i=0}^{\infty} \left(a^2\frac{1}{i+1}\right) \left(1 \right) 
\left( 
\frac{1}{(i+1)(i+2)}
\right) 
\left( 
\frac{-\delta}{(i+1)}
\right).
 4 
\lim_{\epsilon \rightarrow 0} \int_{\epsilon}^a x \ dx \int_{-\sqrt{x(a-x)}}^{\sqrt{x(a-x)}} \frac{1}{1+ (y/x)^2} d (y/x) 
\lim_{\epsilon \rightarrow 0} \int_{\epsilon}^a 2 x \arctan \sqrt{x(a-x)} \ dx   
\lim_{\delta \rightarrow 0}\iint_{\delta^2 \leq r^2+ar \cos \theta + a^2/4 \leq a^2/4} r \cos ^2 \theta \ dr \ d\theta
 \theta \in [-\pi/2, \pi/2], a\geq 0 
\delta^2 \leq r^2 - ar + a^2/4 \leq  r^2+ar \cos \theta + a^2/4 \leq r^2 + a^2/4 \leq a^2/4.
 
0 \leq r^2 - ar + a^2/4 -\delta^2 \Longrightarrow r\in (0, \frac{a-2\delta}{2}).
 
\lim_{\delta \to 0} \int_{-\pi/2}^{\pi/2} \cos ^2 \theta \ d \theta \int_0^{(a-2\delta)/2} r dr = \frac{a^2}{8} В(1/2, 3/2) = \frac{a^2 \pi}{16}
","['integration', 'multivariable-calculus', 'solution-verification', 'multiple-integral', 'conditional-convergence']"
22,Vector potential for $\nabla f \times \nabla g$,Vector potential for,\nabla f \times \nabla g,"Let $f$ and $g$ be two smooth real-valued functions on $\mathbb{R}^3$ . How can we find a vector potential for the vector field $F = \nabla f \times \nabla g$ ? In this question - Show that $\nabla\cdot (\nabla f\times \nabla h)=0$ - we have that $\text{div}(F) = 0$ , so $F$ has a vector potential, i.e. a vector field $H$ on $\mathbb{R}^3$ such that $\text{curl}(H) = F.$ But how do we specifically find it? If we let $H = (H_1, H_2, H_3)$ , then we should have that $$\displaystyle \begin{pmatrix} \frac{\partial H_3}{\partial y} - \frac{\partial H_2}{\partial z} \\ \frac{\partial H_1}{\partial z} - \frac{\partial H_3}{\partial x} \\ \frac{\partial H_2}{\partial x} - \frac{\partial H_1}{\partial y} \end{pmatrix} = \begin{pmatrix} \frac{\partial f}{\partial y} \cdot \frac{\partial g}{\partial z} - \frac{\partial f}{\partial z} \cdot \frac{\partial g}{\partial y} \\ \frac{\partial f}{\partial z} \cdot \frac{\partial g}{\partial x} - \frac{\partial f}{\partial x} \cdot \frac{\partial g}{\partial z} \\ \frac{\partial f}{\partial x} \cdot \frac{\partial g}{\partial y} - \frac{\partial f}{\partial y} \cdot \frac{\partial g}{\partial x} \end{pmatrix}, $$ but i don't know how to continue from here.","Let and be two smooth real-valued functions on . How can we find a vector potential for the vector field ? In this question - Show that $\nabla\cdot (\nabla f\times \nabla h)=0$ - we have that , so has a vector potential, i.e. a vector field on such that But how do we specifically find it? If we let , then we should have that but i don't know how to continue from here.","f g \mathbb{R}^3 F = \nabla f \times \nabla g \text{div}(F) = 0 F H \mathbb{R}^3 \text{curl}(H) = F. H = (H_1, H_2, H_3) \displaystyle \begin{pmatrix} \frac{\partial H_3}{\partial y} - \frac{\partial H_2}{\partial z} \\ \frac{\partial H_1}{\partial z} - \frac{\partial H_3}{\partial x} \\ \frac{\partial H_2}{\partial x} - \frac{\partial H_1}{\partial y} \end{pmatrix} = \begin{pmatrix} \frac{\partial f}{\partial y} \cdot \frac{\partial g}{\partial z} - \frac{\partial f}{\partial z} \cdot \frac{\partial g}{\partial y} \\ \frac{\partial f}{\partial z} \cdot \frac{\partial g}{\partial x} - \frac{\partial f}{\partial x} \cdot \frac{\partial g}{\partial z} \\ \frac{\partial f}{\partial x} \cdot \frac{\partial g}{\partial y} - \frac{\partial f}{\partial y} \cdot \frac{\partial g}{\partial x} \end{pmatrix}, ","['multivariable-calculus', 'vector-analysis', 'vector-fields']"
23,Finding the tangent planes to a torus parallel to $3x+4y-5z=20$.,Finding the tangent planes to a torus parallel to .,3x+4y-5z=20,"I've been tasked with finding all the points on the torus $S$ given by $$ \left(6-\sqrt{x^2+y^2}\right)^2+z^2=2 $$ at which the tangent plane is parallel to the plane $P$ given by $3x+4y-5z=20$ (which is, itself, tangential to $S$ at $(3,4,1)$ ). The method that I have employed gives me another point, but since $S$ is a torus, and $P$ is not parallel to any of $x=0$ , $y=0$ or $z=0$ , I know there must be at least two more. The method that I have employed is as follows. Let $f(x,y,z)=\left(6-\sqrt{x^2+y^2}\right)^2+z^2$ , then we know that a vector normal to $S$ at $(x_0,y_0,z_0)$ is given by $\nabla f(x_0,y_0,z_0)$ . We use this fact, in conjunction with the fact that for two planes to be parallel, their normal vectors must also be parallel, to arrive at the condition $$ \nabla f(x_0,y_0,z_0)=\lambda\begin{pmatrix}3\\4\\-5\end{pmatrix} $$ for some scalar, $\lambda\in\mathbb{R}$ . We compare the components of these vectors to obtain expressions for $z$ and $x$ in terms of $y$ , and then substitute these expressions into the equation for $S$ to find that $y=4$ or $y=28/5$ . Since we already have a point at which $y=4$ , we discard this value, and hence we have that another tangent to $S$ which is parallel to $P$ is given by $5x/5+8y/5-2z=16$ . Graphically, these are the two tangent planes in question However, as we can see, there is an entire other half of the torus, one which I'm confident has two more points at which the tangent planes are parallel to $P$ . How come I have missed these points? Any help is appreciated.","I've been tasked with finding all the points on the torus given by at which the tangent plane is parallel to the plane given by (which is, itself, tangential to at ). The method that I have employed gives me another point, but since is a torus, and is not parallel to any of , or , I know there must be at least two more. The method that I have employed is as follows. Let , then we know that a vector normal to at is given by . We use this fact, in conjunction with the fact that for two planes to be parallel, their normal vectors must also be parallel, to arrive at the condition for some scalar, . We compare the components of these vectors to obtain expressions for and in terms of , and then substitute these expressions into the equation for to find that or . Since we already have a point at which , we discard this value, and hence we have that another tangent to which is parallel to is given by . Graphically, these are the two tangent planes in question However, as we can see, there is an entire other half of the torus, one which I'm confident has two more points at which the tangent planes are parallel to . How come I have missed these points? Any help is appreciated.","S 
\left(6-\sqrt{x^2+y^2}\right)^2+z^2=2
 P 3x+4y-5z=20 S (3,4,1) S P x=0 y=0 z=0 f(x,y,z)=\left(6-\sqrt{x^2+y^2}\right)^2+z^2 S (x_0,y_0,z_0) \nabla f(x_0,y_0,z_0) 
\nabla f(x_0,y_0,z_0)=\lambda\begin{pmatrix}3\\4\\-5\end{pmatrix}
 \lambda\in\mathbb{R} z x y S y=4 y=28/5 y=4 S P 5x/5+8y/5-2z=16 P","['calculus', 'multivariable-calculus']"
24,Flux and Stokes Theorem,Flux and Stokes Theorem,,"I am trying to evaluate the flux of a cylinder $C$ which is closed at either the top or bottom using Stokes. The actual integral is $\iint_{\partial S} \mathrm{curl}\ F\ dS$ and using Stokes I can break the surface down into the outer part of the cylinder and either the bottom disk or the top disk and evaluate $\int F \cdot dr$ . However, since the bottom disk has normal down and the top disk has normal up, wouldn't that lead to two different orientations and answers depending on which I use? PS the line integral of the circular projection of the surface is given to be -2 so the answer is -4, but I am getting $0$ when I use the bottom disk.","I am trying to evaluate the flux of a cylinder which is closed at either the top or bottom using Stokes. The actual integral is and using Stokes I can break the surface down into the outer part of the cylinder and either the bottom disk or the top disk and evaluate . However, since the bottom disk has normal down and the top disk has normal up, wouldn't that lead to two different orientations and answers depending on which I use? PS the line integral of the circular projection of the surface is given to be -2 so the answer is -4, but I am getting when I use the bottom disk.",C \iint_{\partial S} \mathrm{curl}\ F\ dS \int F \cdot dr 0,"['multivariable-calculus', 'line-integrals', 'stokes-theorem']"
25,How to analyze $\{|f(z)|\le M\}$ is bounded?,How to analyze  is bounded?,\{|f(z)|\le M\},"Given a continuously differentiable function $f:\mathbb{R^2}\to\mathbb{R^2}$ . If $\{x : \det Jf(x)=0 \}$ is finite, and for each positive constant $M$ , $\{z\in \mathbb{R^2} :|f(z)|\le M\}$ is bounded. Prove that $f$ is surjective. My attempt I wanted to prove that $ f(\mathbb{R^2})$ is open and closed. In order to do this I proved that $f(\mathbb{R^2}\setminus\{x : \det Jf(x)=0 \})$ is open. I wanted to prove then that $f(\mathbb{R^2})$ is closed. But I got stuck on how to use the condition that $\{z\in \mathbb{R^2} :|f(z)|\le M\}$ is bounded. Any hints? Thanks in advance!","Given a continuously differentiable function . If is finite, and for each positive constant , is bounded. Prove that is surjective. My attempt I wanted to prove that is open and closed. In order to do this I proved that is open. I wanted to prove then that is closed. But I got stuck on how to use the condition that is bounded. Any hints? Thanks in advance!",f:\mathbb{R^2}\to\mathbb{R^2} \{x : \det Jf(x)=0 \} M \{z\in \mathbb{R^2} :|f(z)|\le M\} f  f(\mathbb{R^2}) f(\mathbb{R^2}\setminus\{x : \det Jf(x)=0 \}) f(\mathbb{R^2}) \{z\in \mathbb{R^2} :|f(z)|\le M\},"['real-analysis', 'multivariable-calculus', 'jacobian']"
26,Derivative on $\mathbb{R}^n$,Derivative on,\mathbb{R}^n,"Definition. Let $A \subset \mathbf{R}^{m} ;$ let $f : A \rightarrow \mathbf{R}^{n} .$ Suppose $A$ contains a neighborhood of a. Given $\mathbf{u} \in \mathbf{R}^{m}$ with $\mathbf{u} \neq 0$ , define $$f^{\prime}(\mathbf{a} ; \mathbf{u})=\lim _{t \rightarrow 0} \frac{f(\mathbf{a}+t \mathbf{u})-f(\mathbf{a})}{t}$$ provided the limit exists. This limit depends both on a and on u; it is called the directional derivative of $f$ at a with respect to the vector $\mathbf{u}$ . Definition. Let $A \subset \mathbf{R}^{m},$ let $f : A \rightarrow \mathbf{R}^{n}$ Suppose $A$ contains $a$ neighborhood of $a$ . We say that $f$ is differentiable at $a$ if there is an $n$ by $m$ matrix $B$ such that $$\frac{f(\mathbf{a}+\mathbf{h})-f(\mathbf{a})-B \cdot \mathbf{h}}{|\mathbf{h}|} \rightarrow \mathbf{0} \quad \text{as} \quad \mathbf{h} \rightarrow \mathbf{0}$$ The matrix $B,$ which is unique, is called the derivative of $f$ at a; it is denoted $D f(\mathbf{a}) .$ EXAMPLE. Define $f : \mathbf{R}^{2} \rightarrow \mathbf{R}$ by setting $f(\mathbf{0})=0$ and $$ f(x, y)=x^{2} y /\left(x^{4}+y^{2}\right) \text { if }(x, y) \neq \mathbf{0} $$ We show all directional derivatives of $f$ exist at $0,$ but that $f$ is not differen- tiable at $0 .$ Let $\mathbf{u} \neq \mathbf{0}$ . Then $$\begin{aligned} \frac{f(0+t u)-f(0)}{t} &=\frac{(t h)^{2}(t k)}{(t h)^{4}+(t k)^{2}} \frac{1}{t} \text { if } u=\left[\begin{array}{l}{h} \\ {k}\end{array}\right] \\ &=\frac{h^{2} k}{t^{2} h^{4}+k^{2}} \\ \text { so that } & \text { if } k \neq 0 \\ f^{\prime}(0 ; \mathbf{u}) &=\left\{\begin{array}{ll}{h^{2} / k} & {\text { if } k \neq 0} \\ {0} & {\text { if } k=0}\end{array}\right.\end{aligned}$$ Thus $f^{\prime}(\mathbf{0} ; \mathbf{u})$ exists for all $\mathbf{u} \neq \mathbf{0}.$ My Question: How can I show the function $f$ is not differentiable at $0$ ? How can I show $f^{\prime}(0 ; \mathbf{u})$ is not a linear function of $\mathbf{u}$ ? Can you help? Thanks...","Definition. Let let Suppose contains a neighborhood of a. Given with , define provided the limit exists. This limit depends both on a and on u; it is called the directional derivative of at a with respect to the vector . Definition. Let let Suppose contains neighborhood of . We say that is differentiable at if there is an by matrix such that The matrix which is unique, is called the derivative of at a; it is denoted EXAMPLE. Define by setting and We show all directional derivatives of exist at but that is not differen- tiable at Let . Then Thus exists for all My Question: How can I show the function is not differentiable at ? How can I show is not a linear function of ? Can you help? Thanks...","A \subset \mathbf{R}^{m} ; f : A \rightarrow \mathbf{R}^{n} . A \mathbf{u} \in \mathbf{R}^{m} \mathbf{u} \neq 0 f^{\prime}(\mathbf{a} ; \mathbf{u})=\lim _{t \rightarrow 0} \frac{f(\mathbf{a}+t \mathbf{u})-f(\mathbf{a})}{t} f \mathbf{u} A \subset \mathbf{R}^{m}, f : A \rightarrow \mathbf{R}^{n} A a a f a n m B \frac{f(\mathbf{a}+\mathbf{h})-f(\mathbf{a})-B \cdot \mathbf{h}}{|\mathbf{h}|} \rightarrow \mathbf{0} \quad \text{as} \quad \mathbf{h} \rightarrow \mathbf{0} B, f D f(\mathbf{a}) . f : \mathbf{R}^{2} \rightarrow \mathbf{R} f(\mathbf{0})=0 
f(x, y)=x^{2} y /\left(x^{4}+y^{2}\right) \text { if }(x, y) \neq \mathbf{0}
 f 0, f 0 . \mathbf{u} \neq \mathbf{0} \begin{aligned} \frac{f(0+t u)-f(0)}{t} &=\frac{(t h)^{2}(t k)}{(t h)^{4}+(t k)^{2}} \frac{1}{t} \text { if } u=\left[\begin{array}{l}{h} \\ {k}\end{array}\right] \\ &=\frac{h^{2} k}{t^{2} h^{4}+k^{2}} \\ \text { so that } & \text { if } k \neq 0 \\ f^{\prime}(0 ; \mathbf{u}) &=\left\{\begin{array}{ll}{h^{2} / k} & {\text { if } k \neq 0} \\ {0} & {\text { if } k=0}\end{array}\right.\end{aligned} f^{\prime}(\mathbf{0} ; \mathbf{u}) \mathbf{u} \neq \mathbf{0}. f 0 f^{\prime}(0 ; \mathbf{u}) \mathbf{u}",['real-analysis']
27,Prove that the loxodrome crosses all meridians at a constant angle,Prove that the loxodrome crosses all meridians at a constant angle,,"How to prove that the loxodrome (the rhumb line) crosses all meridians at a constant angle? $$\tan\left(\frac{\pi}{4} + \frac{\psi}{2}\right) = e^{k\phi}, \quad k = \text{constant}$$ where $\psi$ is the latitude of a point on a sphere, and $\phi$ is the longitude.","How to prove that the loxodrome (the rhumb line) crosses all meridians at a constant angle? where is the latitude of a point on a sphere, and is the longitude.","\tan\left(\frac{\pi}{4} + \frac{\psi}{2}\right) = e^{k\phi}, \quad k = \text{constant} \psi \phi","['multivariable-calculus', 'differential-geometry', 'curves', 'spherical-geometry', 'geodesy']"
28,On derivation of curvature formulas,On derivation of curvature formulas,,"I can't figure out the idea behind the following part of the derivation of curvature formulas; We let $M$ be a surface in $\mathbb{R}^3$ and $N$ be it's Gauss map. Moreover we consider a point $p\in M$ and local parametrisation $X$ such that $X(0)=p$ . Given this, there is a matrix $A$ of $-dN: T_{p}M \rightarrow T_{p}M$ with respect to the basis $X_{u},X_{v}$ . So far I understand things, now however, two matrix valued maps $DX=[X_{u},X_{v}]$ and $DN=[N_{u},N_{v}]$ are introduced from $U$ which is the domain of the parametrisation which satisfy , $-DN=DX A$ I dont understand this last equality. The complete outline can be found on pages 46-47 in, http://www.matematik.lu.se/matematiklu/personal/sigma/Gauss.pdf","I can't figure out the idea behind the following part of the derivation of curvature formulas; We let be a surface in and be it's Gauss map. Moreover we consider a point and local parametrisation such that . Given this, there is a matrix of with respect to the basis . So far I understand things, now however, two matrix valued maps and are introduced from which is the domain of the parametrisation which satisfy , I dont understand this last equality. The complete outline can be found on pages 46-47 in, http://www.matematik.lu.se/matematiklu/personal/sigma/Gauss.pdf","M \mathbb{R}^3 N p\in M X X(0)=p A -dN: T_{p}M \rightarrow T_{p}M X_{u},X_{v} DX=[X_{u},X_{v}] DN=[N_{u},N_{v}] U -DN=DX A","['linear-algebra', 'multivariable-calculus', 'differential-geometry']"
29,Limit of integral over iterated image tending to $0$,Limit of integral over iterated image tending to,0,"Let $f:\mathbb{R}^m\to\mathbb{R}^m$ be a difeomorphism such that $f(B)\subset B$ , where $B$ is the unit closed ball and $|\det f'(x) |<1,\,\forall x \in B$ . Then, if $g:B\to\mathbb{R}$ is any continuous function, show that: $$\lim_{n\to \infty}\int_{f^n(B)}g(x)dx=0$$ I'm attempting to use the change of variables formula, but I'm having trouble since we dont have that $f$ is $C^1$ and thus we can't guarantee $|\det f'(x) |$ attains maximum, say $\lambda<1$ ... That is, we only have that: $$|\det (f^{n})'(x) |=|\det f'(f^{n-1}(x)) |\dots|\det f'(x) |<1$$ So that $\int_{f^n(B)}g(x)dx = \int_B g\circ f^n(x)|\det (f^{n})'|dx<\int_B g\circ f^n(x) dx$ ...Where do I go from here?","Let be a difeomorphism such that , where is the unit closed ball and . Then, if is any continuous function, show that: I'm attempting to use the change of variables formula, but I'm having trouble since we dont have that is and thus we can't guarantee attains maximum, say ... That is, we only have that: So that ...Where do I go from here?","f:\mathbb{R}^m\to\mathbb{R}^m f(B)\subset B B |\det f'(x) |<1,\,\forall x \in B g:B\to\mathbb{R} \lim_{n\to \infty}\int_{f^n(B)}g(x)dx=0 f C^1 |\det f'(x) | \lambda<1 |\det (f^{n})'(x) |=|\det f'(f^{n-1}(x)) |\dots|\det f'(x) |<1 \int_{f^n(B)}g(x)dx = \int_B g\circ f^n(x)|\det (f^{n})'|dx<\int_B g\circ f^n(x) dx","['calculus', 'integration', 'multivariable-calculus', 'volume', 'riemann-integration']"
30,Compute the exterior product of $n$ copies of $\omega=dx_1 \wedge dx_2+\cdots+dx_{2n-1}\wedge dx_{2n}$,Compute the exterior product of  copies of,n \omega=dx_1 \wedge dx_2+\cdots+dx_{2n-1}\wedge dx_{2n},"Let $\omega$ be the $2$ -form in $\mathbb{R}^{2n}$ given by $$\omega=dx_1 \wedge dx_2+dx_3\wedge dx_4+\cdots dx_{2n-1}\wedge  dx_{2n}$$ Compute the exterior product of $n$ copies of $\omega$ . (Chapter 1, Exercise 7 in Differential Forms and Applications by Manfredo P. do Carmo) The wording of the problem is kind of difficult for me to understand, but I suppose that the exercise asks us to calculate $\omega \wedge \cdots \wedge \omega$ . So, I think the only way one can get a non-zero coefficient is that if we permute the parentheses $(dx_1 \wedge dx_2)$ up to $(dx_{2n-1}\wedge dx_{2n})$ . There are $n!$ such permutations. Since each permutation can be arranged to $dx_1\wedge dx_2 \wedge dx_3 \wedge dx_4 \wedge \cdots \wedge dx_{2n-1}\wedge dx_{2n}$ with an even number of transpositions, we always get $+1$ as the coefficient of the differential form. So, the answer should be $$\omega\wedge\cdots\wedge\omega=\color{green}{n!}\,dx_1\wedge dx_2 \wedge dx_3\wedge dx_4 \wedge \cdots \wedge dx_{2n-1}\wedge dx_{2n}$$ Is that right? If I write my argument exactly like here, would it be considered a complete calculation and receive full points in an exam of differential manifolds?","Let be the -form in given by Compute the exterior product of copies of . (Chapter 1, Exercise 7 in Differential Forms and Applications by Manfredo P. do Carmo) The wording of the problem is kind of difficult for me to understand, but I suppose that the exercise asks us to calculate . So, I think the only way one can get a non-zero coefficient is that if we permute the parentheses up to . There are such permutations. Since each permutation can be arranged to with an even number of transpositions, we always get as the coefficient of the differential form. So, the answer should be Is that right? If I write my argument exactly like here, would it be considered a complete calculation and receive full points in an exam of differential manifolds?","\omega 2 \mathbb{R}^{2n} \omega=dx_1 \wedge dx_2+dx_3\wedge dx_4+\cdots dx_{2n-1}\wedge
 dx_{2n} n \omega \omega \wedge \cdots \wedge \omega (dx_1 \wedge dx_2) (dx_{2n-1}\wedge dx_{2n}) n! dx_1\wedge dx_2 \wedge dx_3 \wedge dx_4 \wedge \cdots \wedge dx_{2n-1}\wedge dx_{2n} +1 \omega\wedge\cdots\wedge\omega=\color{green}{n!}\,dx_1\wedge dx_2 \wedge dx_3\wedge dx_4 \wedge \cdots \wedge dx_{2n-1}\wedge dx_{2n}","['multivariable-calculus', 'manifolds', 'differential-forms']"
31,How can a velocity vector be determined literally as a derivative in the calculus sense?,How can a velocity vector be determined literally as a derivative in the calculus sense?,,"I am following Tu's book on manifolds. On pages 178-179 he proves that the Lie algebra of $\mathrm{SL}(n,\mathbb{R})$ is the set of traceless real $n\times n$ matrices. Specifically he writes: Suppose $X \in T_I \mathrm{SL}(2,\mathbb{R})$ . There is a curve $\gamma$ and $\varepsilon > 0$ such that $\gamma : (-\varepsilon,\varepsilon) \to \mathrm{SL}(2,\mathbb{R})$ , $\gamma(0) = I$ and $\gamma'(0) = X$ . Being in $\mathrm{SL}(2, \mathbb{R})$ , this curve satisfies $\mathrm{det} (\gamma(t)) = 1$ . Then comes the part I cannot understand, he then says: Now differentiate both sides with respect to $t$ and evaluate at $t=0$ . On the right hand side we have $0 = \frac{d}{dt}\big|_{t=0} 1$ , and on the left hand side we have $$ \frac{d}{dt}\big|_{t=0} \mathrm{det} (\gamma(t)) = (\mathrm{det \circ \gamma})_* \left( \frac{d}{dt}\big|_{0} \right) \qquad (\dagger) $$ [...] As far as I can tell, in $(\dagger)$ he makes the literal interpretation that the tangent vector $(\mathrm{det} \circ \gamma)_{*0} \frac{d}{dt}\big|_{0}$ is the calculus derivative: $$D_{\gamma(t)} \mathrm{det} \cdot D_t \gamma $$ I cannot understand this because a calculus derivative is a real number, while the tangent vector is a derivation. Could you please explain in a pedagogical way what is going on?","I am following Tu's book on manifolds. On pages 178-179 he proves that the Lie algebra of is the set of traceless real matrices. Specifically he writes: Suppose . There is a curve and such that , and . Being in , this curve satisfies . Then comes the part I cannot understand, he then says: Now differentiate both sides with respect to and evaluate at . On the right hand side we have , and on the left hand side we have [...] As far as I can tell, in he makes the literal interpretation that the tangent vector is the calculus derivative: I cannot understand this because a calculus derivative is a real number, while the tangent vector is a derivation. Could you please explain in a pedagogical way what is going on?","\mathrm{SL}(n,\mathbb{R}) n\times n X \in T_I \mathrm{SL}(2,\mathbb{R}) \gamma \varepsilon > 0 \gamma : (-\varepsilon,\varepsilon) \to \mathrm{SL}(2,\mathbb{R}) \gamma(0) = I \gamma'(0) = X \mathrm{SL}(2, \mathbb{R}) \mathrm{det} (\gamma(t)) = 1 t t=0 0 = \frac{d}{dt}\big|_{t=0} 1  \frac{d}{dt}\big|_{t=0} \mathrm{det} (\gamma(t)) = (\mathrm{det \circ \gamma})_* \left( \frac{d}{dt}\big|_{0} \right) \qquad (\dagger)  (\dagger) (\mathrm{det} \circ \gamma)_{*0} \frac{d}{dt}\big|_{0} D_{\gamma(t)} \mathrm{det} \cdot D_t \gamma ","['multivariable-calculus', 'lie-algebras', 'smooth-manifolds']"
32,Showing a mapping is bijective if and only if a matrix is invertible,Showing a mapping is bijective if and only if a matrix is invertible,,"Let $\mathbf{A}$ be an $n\times n$ matrix and let $\mathbf{c}$ and $x_{\star}$ be point in $\mathbb{R}^{n}$ . Define the affine mapping $\mathbf{G} : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ by $$\mathbf{G(x)} = \mathbf{c + A(x - x_{\star})} $$ for $\mathbf{x}$ in $\mathbb{R}^{n}$ . Show that the mapping $\mathbf{G} : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ is one-to-one   and onto if and only if $\mathbf{A}$ is invertible. I am not too sure about how to approach this problem. I've also got the following theorem that I think might help: Let $\mathcal{O}$ be an open subset of $\mathbb{R}^{n}$ and suppose $\mathbf{F} : \mathcal{O} \rightarrow \mathbb{R}^{n}$ is continuously differentiable. Let $x_{\star}$ be a point in $\mathcal{O}$ at which the derivative matrix $\mathbf{DF(x_{\star})}$ is invertible. Then there is a neighborhood $U$ of $x_{\star}$ and a neighborhood $V$ of its image $\mathbf{F(x_{\star})}$ such that $\mathbf{F} : U \rightarrow V$ is one-to-one and onto. I've tried taking the derivative of both sides of the equation, etc, but didn't get anywhere. Any help is appreciated.","Let be an matrix and let and be point in . Define the affine mapping by for in . Show that the mapping is one-to-one   and onto if and only if is invertible. I am not too sure about how to approach this problem. I've also got the following theorem that I think might help: Let be an open subset of and suppose is continuously differentiable. Let be a point in at which the derivative matrix is invertible. Then there is a neighborhood of and a neighborhood of its image such that is one-to-one and onto. I've tried taking the derivative of both sides of the equation, etc, but didn't get anywhere. Any help is appreciated.",\mathbf{A} n\times n \mathbf{c} x_{\star} \mathbb{R}^{n} \mathbf{G} : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n} \mathbf{G(x)} = \mathbf{c + A(x - x_{\star})}  \mathbf{x} \mathbb{R}^{n} \mathbf{G} : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n} \mathbf{A} \mathcal{O} \mathbb{R}^{n} \mathbf{F} : \mathcal{O} \rightarrow \mathbb{R}^{n} x_{\star} \mathcal{O} \mathbf{DF(x_{\star})} U x_{\star} V \mathbf{F(x_{\star})} \mathbf{F} : U \rightarrow V,"['linear-algebra', 'matrices']"
33,Exponential decay of the gradient if the function itself and the Laplacian have exponential decay,Exponential decay of the gradient if the function itself and the Laplacian have exponential decay,,"Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be twice continously differentiable. For $n=1$ one can prove using Taylors formula that we have $$ \sup_{\vert x \vert \leq R} \vert f'(x) \vert \leq 2\left( \sup_{\vert x \vert \leq R+1} \vert f(x) \vert \right) +  \left( \sup_{\vert x \vert \leq R+1} \vert f''(x) \vert \right) $$ From this we can show that if $f$ and $f''$ have exponential decay, then so does $f'$ . My question is, whether this is still true in higher dimension in the following version Is the following statement true: If $f$ and $\Delta f$ have exponential decay, then $\nabla f$ has exponential decay as well (I guess that it holds true if we have the full Hessian instead of just the Laplacian using again Taylor's theorem). Anyway, I would already be happy to know the answer for $n=3$ (also if you want to assume another continuous derivative or two, be my guest. Even assuming $f$ to be smooth would be fine for me). Even the radial case would be fine. However, even there I don't see how to do it. Changing to spherical coordinates we get $g: [0; \infty) \rightarrow \mathbb{R}$ such that $$ \vert g (r) \vert , \vert g''(r) + \frac{2}{r} g'(r) \vert \leq D e^{-C r} $$ and we want to conclude that the same holds true for $g$ .","Let be twice continously differentiable. For one can prove using Taylors formula that we have From this we can show that if and have exponential decay, then so does . My question is, whether this is still true in higher dimension in the following version Is the following statement true: If and have exponential decay, then has exponential decay as well (I guess that it holds true if we have the full Hessian instead of just the Laplacian using again Taylor's theorem). Anyway, I would already be happy to know the answer for (also if you want to assume another continuous derivative or two, be my guest. Even assuming to be smooth would be fine for me). Even the radial case would be fine. However, even there I don't see how to do it. Changing to spherical coordinates we get such that and we want to conclude that the same holds true for .","f: \mathbb{R}^n \rightarrow \mathbb{R} n=1  \sup_{\vert x \vert \leq R} \vert f'(x) \vert \leq 2\left( \sup_{\vert x \vert \leq R+1} \vert f(x) \vert \right) +  \left( \sup_{\vert x \vert \leq R+1} \vert f''(x) \vert \right)  f f'' f' f \Delta f \nabla f n=3 f g: [0; \infty) \rightarrow \mathbb{R}  \vert g (r) \vert , \vert g''(r) + \frac{2}{r} g'(r) \vert \leq D e^{-C r}  g","['real-analysis', 'multivariable-calculus']"
34,"Mass and center of mass of lamina: $B={(x,y);x^2+y^2\le1,0\le y}$? I'm close to the answer but I'm missing something.",Mass and center of mass of lamina: ? I'm close to the answer but I'm missing something.,"B={(x,y);x^2+y^2\le1,0\le y}","I'd like some help here because I can't get the right answer. The lamina we are working with is defined by: $B={(x,y);x^2+y^2\le1,0\le y}$ . Also the density function is ""proportional to the distance of the point (x,y) to the x-axis"" I think here is where my mistake lies, for me density function is: $p(x,y)=y$ Acording to the exercise center of mass should be: $(Xc=0,Yc=\frac{3pi}{32})$ So I did this: To find mass M I converted to polar coordinates. $x=rcos(\theta),y=rsin(\theta)$ $M=\int\int_Bp(x,y)dA => M=\int_0^{\pi}\int_0^1rsin(\theta)rdrd\theta=2/3$ . Now with the mass we can find $Xc$ and $Yc$ . It's easy to note that $Xc=0$ with symmetry $Yc = \frac{1}{M}\int\int_Byp(x,y)da = > Yc=\frac{3}{2}\int_0^\pi\int_0^1rsin(\theta)rsin(\theta)rdrd\theta=\frac{3\pi}{16}$ But Yc should be $\frac{3\pi}{32}$ , so I'm close but I'm doing something wrong.","I'd like some help here because I can't get the right answer. The lamina we are working with is defined by: . Also the density function is ""proportional to the distance of the point (x,y) to the x-axis"" I think here is where my mistake lies, for me density function is: Acording to the exercise center of mass should be: So I did this: To find mass M I converted to polar coordinates. . Now with the mass we can find and . It's easy to note that with symmetry But Yc should be , so I'm close but I'm doing something wrong.","B={(x,y);x^2+y^2\le1,0\le y} p(x,y)=y (Xc=0,Yc=\frac{3pi}{32}) x=rcos(\theta),y=rsin(\theta) M=\int\int_Bp(x,y)dA => M=\int_0^{\pi}\int_0^1rsin(\theta)rdrd\theta=2/3 Xc Yc Xc=0 Yc = \frac{1}{M}\int\int_Byp(x,y)da = > Yc=\frac{3}{2}\int_0^\pi\int_0^1rsin(\theta)rsin(\theta)rdrd\theta=\frac{3\pi}{16} \frac{3\pi}{32}",['multivariable-calculus']
35,Direct proof for differentiability of $\sin(x+y)$,Direct proof for differentiability of,\sin(x+y),"I've been triying expand the functions like this: $$\lim_{(x,y)\rightarrow(x_0,y_0)}\frac{|\sin(x+y)-\sin(x_0+y_0)-(\cos(x_0+y_0),\cos(x_0+y_0))\cdot(x-x_0,y-y_0)|}{||(x-x_0,y-y_0)||}\\ \lim_{(x,y)\rightarrow(x_0,y_0)}\frac{|\sin(x)\cos(y)+\sin(y)\cos(x)-\sin(x_0)\cos(y_0)-\sin(y_0)\cos(x_0)-\left[\cos(x_0)\cos(y_0)-\sin(x_0)\sin(y_0)\right]\left[(x-x_0)+(y-y_0)\right]|}{||(x-x_0,y-y_0)||}$$ But I don't know how to continue, \? Any hint to prove that this limit is actually zero?.","I've been triying expand the functions like this: But I don't know how to continue, \? Any hint to prove that this limit is actually zero?.","\lim_{(x,y)\rightarrow(x_0,y_0)}\frac{|\sin(x+y)-\sin(x_0+y_0)-(\cos(x_0+y_0),\cos(x_0+y_0))\cdot(x-x_0,y-y_0)|}{||(x-x_0,y-y_0)||}\\
\lim_{(x,y)\rightarrow(x_0,y_0)}\frac{|\sin(x)\cos(y)+\sin(y)\cos(x)-\sin(x_0)\cos(y_0)-\sin(y_0)\cos(x_0)-\left[\cos(x_0)\cos(y_0)-\sin(x_0)\sin(y_0)\right]\left[(x-x_0)+(y-y_0)\right]|}{||(x-x_0,y-y_0)||}","['calculus', 'multivariable-calculus', 'derivatives']"
36,"$y=f(x) \in C^1$ is defined implicitly by $ax + by = f(x^2+y^2), f'(x) = ?$",is defined implicitly by,"y=f(x) \in C^1 ax + by = f(x^2+y^2), f'(x) = ?","Problem $y=f(x) \in C^1$ is defined implicitly by $ax + by = f(x^2+y^2)$ . $a$ and $b$ are constants . $f'(x) = ?$ Analysis The answer for this exercise given by my teacher is $$f'(x) = \frac{2xf'(x^2+y^2)-a}{b-2yf'(x^2+y^2)}$$ I can't agree with that. Because if you express a derivative function with itself, in fact you don't find it. Is it possible to write the expression of $f'(x)$ with only $x, y, a $ and $b$ ? Thanks!","Problem is defined implicitly by . and are constants . Analysis The answer for this exercise given by my teacher is I can't agree with that. Because if you express a derivative function with itself, in fact you don't find it. Is it possible to write the expression of with only and ? Thanks!","y=f(x) \in C^1 ax + by = f(x^2+y^2) a b f'(x) = ? f'(x) = \frac{2xf'(x^2+y^2)-a}{b-2yf'(x^2+y^2)} f'(x) x, y, a  b","['calculus', 'multivariable-calculus', 'implicit-differentiation', 'implicit-function-theorem', 'implicit-function']"
37,Derivative of a multivariate quadratic function,Derivative of a multivariate quadratic function,,Let's define function $f : \mathbb{R}^n \to \mathbb{R}$ as $$ f(x) = {1\over2}x'Ax + b'x $$ where matrix $A \in \mathbb{R}^{n\times n}$ and vector $b\in \mathbb{R}^n$ are given. Function $f$ is twice differentiable. How can one prove the following? $$\nabla f(x) = {1\over 2} (A+A')x + b$$,Let's define function as where matrix and vector are given. Function is twice differentiable. How can one prove the following?,"f : \mathbb{R}^n \to \mathbb{R} 
f(x) = {1\over2}x'Ax + b'x
 A \in \mathbb{R}^{n\times n} b\in \mathbb{R}^n f \nabla f(x) = {1\over 2} (A+A')x + b","['multivariable-calculus', 'derivatives', 'quadratics', 'matrix-calculus']"
38,Proving that level sets of a surface near local max/min are closed curves,Proving that level sets of a surface near local max/min are closed curves,,"Let $S$ be a surface in $\mathbb{R}^3$ defined by the graph $z=f(x,y)$ of a smooth function $f: \mathbb{R}^2 \to \mathbb{R}$ . Suppose furthermore that $S$ satisfies the second-derivative test for a local max or min at a critical point $p^* = (x^*,y^*)$ ; i.e. the Hessian matrix $H(x,y)$ of $f$ satisfies det $(H)>0$ , and either $H_{xx}>0$ or $H_{xx}<0$ when evaluated at $p^*$ . I'm looking for a proof (or counterexample) of the following statement: Suppose $f(x^*,y^*)=c^*$ . Then horizontal slices $f(x,y) = c$ include   closed curves for values of $c$ sufficiently close to $c^*$ on either the right or the left. This is obvious for elliptic paraboloids $f(x,y) = x^2/a^2+y^2/b^2$ . Otherwise we can Taylor expand around the critical point so that the nontrivial leading-order terms are quadratic in $x$ and $y$ , but it's not clear how to show that the intersections contain 'just slightly perturbed ellipses near $p^*$ ' in that case.","Let be a surface in defined by the graph of a smooth function . Suppose furthermore that satisfies the second-derivative test for a local max or min at a critical point ; i.e. the Hessian matrix of satisfies det , and either or when evaluated at . I'm looking for a proof (or counterexample) of the following statement: Suppose . Then horizontal slices include   closed curves for values of sufficiently close to on either the right or the left. This is obvious for elliptic paraboloids . Otherwise we can Taylor expand around the critical point so that the nontrivial leading-order terms are quadratic in and , but it's not clear how to show that the intersections contain 'just slightly perturbed ellipses near ' in that case.","S \mathbb{R}^3 z=f(x,y) f: \mathbb{R}^2 \to \mathbb{R} S p^* = (x^*,y^*) H(x,y) f (H)>0 H_{xx}>0 H_{xx}<0 p^* f(x^*,y^*)=c^* f(x,y) = c c c^* f(x,y) = x^2/a^2+y^2/b^2 x y p^*",['multivariable-calculus']
39,How to compute the Jacobian matrix of a multivariate function in a nonstandard matrix?,How to compute the Jacobian matrix of a multivariate function in a nonstandard matrix?,,"Given a function $f:R^2\rightarrow R^2$ such that $f(x,y)=(xy, \cos xy)$ , I need to compute the Jacobian matrix Df with respect to the basis $\{(1,0), (1,1)\}$ . Not confident in my answer though. Please help me verify it. If it is not correct, please give me a hint. Here is my try: With respect to the standard basis $\{1,0), (0,1)\}$ , we could derive a Jacobian matrix $$Df= \begin{bmatrix} \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} \\ \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial x} \end{bmatrix} = \begin{bmatrix} y & x\\ -y\sin xy  & -x\sin xy \end{bmatrix} .$$ To compute the Jacobian matrix with respect to the nonstandard basis $\{(1,0), (1,1)\}$ , I multiply $Df(x,y)$ by this basis and get $$\overline{Df}(x,y)= \begin{bmatrix} y & x\\ -y\sin xy  & -x\sin xy \end{bmatrix} \times \begin{bmatrix} 1 & 1\\ 0 & 1 \end{bmatrix} = \begin{bmatrix} y & y+x\\ -y\sin xy & -y\sin xy-x\sin xy \end{bmatrix} .$$ I did it in this way, because my textbook [Hoffman] says, ""the columns of the matrix relative tot he new basis will be the derivative $Df(x,y)$ applied to the new basis in $R^2$ with this image vector expressed in the new basis in $R^2$ ."" Anyone disagrees or agrees with my answer?","Given a function such that , I need to compute the Jacobian matrix Df with respect to the basis . Not confident in my answer though. Please help me verify it. If it is not correct, please give me a hint. Here is my try: With respect to the standard basis , we could derive a Jacobian matrix To compute the Jacobian matrix with respect to the nonstandard basis , I multiply by this basis and get I did it in this way, because my textbook [Hoffman] says, ""the columns of the matrix relative tot he new basis will be the derivative applied to the new basis in with this image vector expressed in the new basis in ."" Anyone disagrees or agrees with my answer?","f:R^2\rightarrow R^2 f(x,y)=(xy, \cos xy) \{(1,0), (1,1)\} \{1,0), (0,1)\} Df=
\begin{bmatrix}
\frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} \\
\frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial x}
\end{bmatrix}
=
\begin{bmatrix}
y & x\\
-y\sin xy  & -x\sin xy
\end{bmatrix}
. \{(1,0), (1,1)\} Df(x,y) \overline{Df}(x,y)=
\begin{bmatrix}
y & x\\
-y\sin xy  & -x\sin xy
\end{bmatrix}
\times
\begin{bmatrix}
1 & 1\\
0 & 1
\end{bmatrix}
=
\begin{bmatrix}
y & y+x\\
-y\sin xy & -y\sin xy-x\sin xy
\end{bmatrix}
. Df(x,y) R^2 R^2","['multivariable-calculus', 'derivatives', 'partial-derivative', 'change-of-basis', 'jacobian']"
40,How to Find the integral $\int_{S}−(xy^2)~dydz + (2x ^2 y)~dzdx − (zy^2)~dxdy$ where is the portion of the sphere $x^2 + y^2 + z^2 = 1$,How to Find the integral  where is the portion of the sphere,\int_{S}−(xy^2)~dydz + (2x ^2 y)~dzdx − (zy^2)~dxdy x^2 + y^2 + z^2 = 1,"Find the integral $$ \int_{S}−(xy^2)~dydz + (2x^2y)~dzdx − (zy^2)~dxdy $$ where $S$ is the portion of the sphere $x^2 + y^2 + z^2 = 1$ above the plane $z=\frac{1}{2}$ . Choose the direction of the normal to be outside the sphere. Can i get some help? I got that the flux is $$ \huge \phi = \huge 0. $$ and I am not sure if thats right. \begin{split} \vec F &= (-xy^2,2x^2y,-zy^2)\\ \vec n_\mathrm{sphere} &= (2x,2y,2z)\\ \vec{F}\cdot{}\vec{n} = 2y^2(x^2 - z^2) &=^{{~(z^2 = 1-x^2-y^2)}}~4x^2y +2y^3 -2y\\ \int\int 4x^2y +2y^3 - 2ydydx &= \end{split} $$\int_{r=0}^{r=\sqrt{\frac{3}{4}}}\int_{\theta=0}^{\theta=2\pi}4r^4\cos^2\theta \sin\theta + 2r^4\sin^3\theta - 2r^2\sin\theta d\theta dr = \boxed{0}\\ $$",Find the integral where is the portion of the sphere above the plane . Choose the direction of the normal to be outside the sphere. Can i get some help? I got that the flux is and I am not sure if thats right.,"
\int_{S}−(xy^2)~dydz + (2x^2y)~dzdx − (zy^2)~dxdy
 S x^2 + y^2 + z^2 = 1 z=\frac{1}{2} 
\huge \phi = \huge 0.
 \begin{split}
\vec F &= (-xy^2,2x^2y,-zy^2)\\
\vec n_\mathrm{sphere} &= (2x,2y,2z)\\
\vec{F}\cdot{}\vec{n} = 2y^2(x^2 - z^2) &=^{{~(z^2 = 1-x^2-y^2)}}~4x^2y +2y^3 -2y\\
\int\int 4x^2y +2y^3 - 2ydydx &=
\end{split} \int_{r=0}^{r=\sqrt{\frac{3}{4}}}\int_{\theta=0}^{\theta=2\pi}4r^4\cos^2\theta \sin\theta + 2r^4\sin^3\theta - 2r^2\sin\theta d\theta dr = \boxed{0}\\
","['multivariable-calculus', 'vector-analysis', 'divergence-operator']"
41,Existence of continuous derivatives of partial functions and total differentiability,Existence of continuous derivatives of partial functions and total differentiability,,"We know that for a function $\mathbb{R}^m\to \mathbb{R}^n$ the existence and continuity of partial derivatives implies the differentiability of that function. Will this hold true for functions on product spaces of arbitrary Banach spaces? I.e., given $f\colon E_1\times E_2\to F$ ( $E_1,E_2,F$ being Banach spaces) and $(x,y)\in E_1\times E_2$ , does the existence and continuity of $(x,y)\mapsto Df(\cdot,y)(x)$ and $(x,y)\mapsto Df(x,\cdot)(y)$ locally at $(x,y)$ imply $f$ to be differentiable at $(x,y)$ ? I guess it won't because the proof in the euclidean case goes from $f(p)$ to $f(p+h)$ in finitely many steps along the coordinate axes... So, maybe it will hold in product spaces of Hilbert spaces? What would a counter-example be?","We know that for a function the existence and continuity of partial derivatives implies the differentiability of that function. Will this hold true for functions on product spaces of arbitrary Banach spaces? I.e., given ( being Banach spaces) and , does the existence and continuity of and locally at imply to be differentiable at ? I guess it won't because the proof in the euclidean case goes from to in finitely many steps along the coordinate axes... So, maybe it will hold in product spaces of Hilbert spaces? What would a counter-example be?","\mathbb{R}^m\to \mathbb{R}^n f\colon E_1\times E_2\to F E_1,E_2,F (x,y)\in E_1\times E_2 (x,y)\mapsto Df(\cdot,y)(x) (x,y)\mapsto Df(x,\cdot)(y) (x,y) f (x,y) f(p) f(p+h)","['real-analysis', 'multivariable-calculus', 'derivatives', 'frechet-derivative']"
42,Volume of intersection of a sphere and a paraboloid,Volume of intersection of a sphere and a paraboloid,,"Could I calculate the volume of the intersection of $x^2+y^2+z^2=8$ and $4z=x^2+y^2+4$ using spherical coordinates and treating the paraboloid as a function, as in the following example? $f(x,y)=\frac{1}{4}(x^2+y^2)+1=\frac{1}{4}(r^2\sin^2(\theta)\cos^2(\varphi))+r^2\sin^2(\theta)\sin^2(\varphi))+1=\frac{1}{4}(r^4\sin^2(\theta)) + 1$ Volume = $\int_0^{2\pi} \int_0^\pi \int_0^\sqrt{8} (\frac{1}{4}(r^4\sin^2(\theta))+1)\cdot r^2\sin(\theta) drd\theta d\varphi$ Should that work? I tried it and didn't get the correct result but then I'm sick and might have seen my error. Edit: I think I am mixing something up here. Basically I just add up the values of $f$ but still integrate over a spherical volume. So the idea to restrict the integrated volume with a function doesn't work - at least not like that. Right?","Could I calculate the volume of the intersection of and using spherical coordinates and treating the paraboloid as a function, as in the following example? Volume = Should that work? I tried it and didn't get the correct result but then I'm sick and might have seen my error. Edit: I think I am mixing something up here. Basically I just add up the values of but still integrate over a spherical volume. So the idea to restrict the integrated volume with a function doesn't work - at least not like that. Right?","x^2+y^2+z^2=8 4z=x^2+y^2+4 f(x,y)=\frac{1}{4}(x^2+y^2)+1=\frac{1}{4}(r^2\sin^2(\theta)\cos^2(\varphi))+r^2\sin^2(\theta)\sin^2(\varphi))+1=\frac{1}{4}(r^4\sin^2(\theta)) + 1 \int_0^{2\pi} \int_0^\pi \int_0^\sqrt{8} (\frac{1}{4}(r^4\sin^2(\theta))+1)\cdot r^2\sin(\theta) drd\theta d\varphi f","['calculus', 'integration', 'multivariable-calculus', 'volume']"
43,General Solution for Partial Differential Equation,General Solution for Partial Differential Equation,,"To be honest I'm a bit lost on this, and I would like to get a hint or something that can help me, thanks. I need to find the general solution $U(x,y,z)$ of the next equation: $$U_{xx}+U_{yy}+4U_{zz}-2U_{xy}+4U_{xz}-4U_{yz}=xyz$$ I know that most sure exists a change of variable that would help to solve the equation, but I don't know how to find it, our teacher of Multivariable Calculus asked to solve this. Thanks!","To be honest I'm a bit lost on this, and I would like to get a hint or something that can help me, thanks. I need to find the general solution of the next equation: I know that most sure exists a change of variable that would help to solve the equation, but I don't know how to find it, our teacher of Multivariable Calculus asked to solve this. Thanks!","U(x,y,z) U_{xx}+U_{yy}+4U_{zz}-2U_{xy}+4U_{xz}-4U_{yz}=xyz","['multivariable-calculus', 'partial-derivative']"
44,Intutition behind triple integrals between two surfaces,Intutition behind triple integrals between two surfaces,,"I am attempting to learn multivariable calculus on my own. A problem in my book asks me to evaluate $$ \iiint_\mathcal{W} xz \, \mathrm{d}V $$ where $\mathcal{W}$ is the domain bounded by the cylinder $\displaystyle \frac{x^2}{4} + \frac{y^2}{9} = 1$ and the sphere $x^2 + y^2 + z^2 = 16$ in the first octant ( $x, y, z \geq 0$ ). The answer is $\displaystyle \frac{126}{5}$ . However, I cannot obtain this number nor can I understand the logic behind evaluating such an integral. If suppose, I take the $z$ variable, I can see that it can vary from $z = 0$ to $z = \displaystyle \sqrt{16 - x^2 - y^2}$ . That reduces the integral to $$ \frac{1}{2}\int_{lowX}^{highX} \int_{lowY}^{highY} x(16 - x^2 - y^2) \,\mathrm{d}y\,\mathrm{d}x $$ My first question is what is this quantity, physically? Next, as I look down from the $z$ axis --which I have eliminated, I see two curves on the $xy$ plane --an ellipse $\displaystyle \frac{x^2}{4} + \frac{y^2}{9} = 1$ and a circle $x^2 + y^2 = 16$ . So, if I vary $y$ from $0$ to $4$ , $x$ goes from $\displaystyle 2\sqrt{1-y^2/9}$ to $\sqrt{16-y^2}$ . Hence, my next question is what is so special about $z=0$ ? Why would I look at the curves on that plane? Why not the plane $z=1$ ? Is it because this gives me the maximal domain on $x$ and $y$ --a way of saying telling me the extent to which the variables vary? Or am I wrong to reason along these lines? P.S.:- I better be wrong, because along these lines I do not get $126/5$ . I get $3592/81$ .","I am attempting to learn multivariable calculus on my own. A problem in my book asks me to evaluate where is the domain bounded by the cylinder and the sphere in the first octant ( ). The answer is . However, I cannot obtain this number nor can I understand the logic behind evaluating such an integral. If suppose, I take the variable, I can see that it can vary from to . That reduces the integral to My first question is what is this quantity, physically? Next, as I look down from the axis --which I have eliminated, I see two curves on the plane --an ellipse and a circle . So, if I vary from to , goes from to . Hence, my next question is what is so special about ? Why would I look at the curves on that plane? Why not the plane ? Is it because this gives me the maximal domain on and --a way of saying telling me the extent to which the variables vary? Or am I wrong to reason along these lines? P.S.:- I better be wrong, because along these lines I do not get . I get .","
\iiint_\mathcal{W} xz \, \mathrm{d}V
 \mathcal{W} \displaystyle \frac{x^2}{4} + \frac{y^2}{9} = 1 x^2 + y^2 + z^2 = 16 x, y, z \geq 0 \displaystyle \frac{126}{5} z z = 0 z = \displaystyle \sqrt{16 - x^2 - y^2} 
\frac{1}{2}\int_{lowX}^{highX} \int_{lowY}^{highY} x(16 - x^2 - y^2) \,\mathrm{d}y\,\mathrm{d}x
 z xy \displaystyle \frac{x^2}{4} + \frac{y^2}{9} = 1 x^2 + y^2 = 16 y 0 4 x \displaystyle 2\sqrt{1-y^2/9} \sqrt{16-y^2} z=0 z=1 x y 126/5 3592/81","['integration', 'multivariable-calculus', 'multiple-integral']"
45,Curvature vector and osculating circle radius,Curvature vector and osculating circle radius,,"I have found an incongruity into the evaluation of the osculating circle radius of the curve $\gamma(t) = R(cos(t),sin(t))$ using the formula: $$\vec r_c(t) = \vec \gamma(t) + \vec k(t)$$ Where: $\vec r_c(t)$ is the vector that identifies the osculating circle centre; $\vec \gamma(t)$ represents the point $P$ in the picture below; $\vec k(t)$ is the vector curvature. Now the problem comes: Rewriting the formula as: $$\vec r_c(t) - \vec \gamma(t) = \vec k(t)$$ and looking the vectors' norm... $$|\vec r_c(t) - \vec \gamma(t)| = |\vec k(t)|$$ I obtain that $R = \frac{1}{R}$ and that's absurd! Can somebody help me to find the mistake?",I have found an incongruity into the evaluation of the osculating circle radius of the curve using the formula: Where: is the vector that identifies the osculating circle centre; represents the point in the picture below; is the vector curvature. Now the problem comes: Rewriting the formula as: and looking the vectors' norm... I obtain that and that's absurd! Can somebody help me to find the mistake?,"\gamma(t) = R(cos(t),sin(t)) \vec r_c(t) = \vec \gamma(t) + \vec k(t) \vec r_c(t) \vec \gamma(t) P \vec k(t) \vec r_c(t) - \vec \gamma(t) = \vec k(t) |\vec r_c(t) - \vec \gamma(t)| = |\vec k(t)| R = \frac{1}{R}","['calculus', 'multivariable-calculus', 'osculating-circle']"
46,Given that derivative of a function is bounded. Prove surjectivity,Given that derivative of a function is bounded. Prove surjectivity,,"Given a differentiable function $f:\mathbf{R} \to \mathbf{R},$ such that $|f'(x)| < c < 1$ . Consider a function $g:\mathbf{R}^2 \to \mathbf{R}^2$ , such that $g(x,y) = (x+f(y),y+f(x))$ . Prove that g is surjective. My attempt: determinant of derivative of g is always non-zero. Consider a point $(a,b)$ which we want to show will have a preimage, by implicit function theorem, there's a neighbourhood around the point and also a neighbourhood around the point $(a+f(b),b+f(a))$ , such that function is a bijection. Now I wanted to make sure that the distance between $(a,b)$ and $(a+f(b),b+f(a))$ is small so that $(a,b)$ is attainable.","Given a differentiable function such that . Consider a function , such that . Prove that g is surjective. My attempt: determinant of derivative of g is always non-zero. Consider a point which we want to show will have a preimage, by implicit function theorem, there's a neighbourhood around the point and also a neighbourhood around the point , such that function is a bijection. Now I wanted to make sure that the distance between and is small so that is attainable.","f:\mathbf{R} \to \mathbf{R}, |f'(x)| < c < 1 g:\mathbf{R}^2 \to \mathbf{R}^2 g(x,y) = (x+f(y),y+f(x)) (a,b) (a+f(b),b+f(a)) (a,b) (a+f(b),b+f(a)) (a,b)","['multivariable-calculus', 'inverse-function-theorem', 'diffeomorphism']"
47,"$F(x)=\int_{-2}^{2} dy f(x,y)$ is an even function, is $G(x)=\int_{-2}^{2} dy [f(x,y)]^2$ even?","is an even function, is  even?","F(x)=\int_{-2}^{2} dy f(x,y) G(x)=\int_{-2}^{2} dy [f(x,y)]^2","I have a real valued function in two real variables $f(x,y)$ which is essentially a black box. The only thing I really know is that $$ F(x)=\int_{-2}^{2} f(x,y) dy $$ is even and that $$ \int_{-\infty}^{\infty} F(x) dx=1 $$ Can I conclude that $$ G(x)=\int_{-2}^{2} \left[f(x,y)\right]^2 dy $$ is also even? I looked for a counterexample, but couldn't find one. I intuitively feel like this should be true, but am struggling to make that more rigorous. Any ideas? Thanks!","I have a real valued function in two real variables which is essentially a black box. The only thing I really know is that is even and that Can I conclude that is also even? I looked for a counterexample, but couldn't find one. I intuitively feel like this should be true, but am struggling to make that more rigorous. Any ideas? Thanks!","f(x,y) 
F(x)=\int_{-2}^{2} f(x,y) dy
 
\int_{-\infty}^{\infty} F(x) dx=1
 
G(x)=\int_{-2}^{2} \left[f(x,y)\right]^2 dy
","['calculus', 'real-analysis', 'multivariable-calculus', 'even-and-odd-functions']"
48,Exercise about multiple integration,Exercise about multiple integration,,"Exercise text $\ \ \ $ Calculate $$\ \ \iiint_R y \ dxdydz\ \ $$ where $R$ is the cube portion $\ 0 \le x,y,z\le1 \ $ which is under the plane $\ x+y+z=2\ $ and above the plane $\ y+z=1\ $ . My solution Let $D=\{x+y\le2 \ ,\ y\ge1\}$ we have \begin{equation} \begin{split} \iiint_R y\;dxdydz=&\iint_D y\;dxdy\int_{1-y}^{2-x-y}\;dz\\ =&\iint_D\ {y(1-x)}\;dxdy\\ =&\int_0^2(1-x)\;dx\int_1^{2-x}y\;dy\\ =&\frac{1}{2}\int_0^2(1-x)(4+x^2-4x-1)\;dx\\ =&\frac{2}{3} \end{split} \end{equation} I'm not sure of the validity of my solution, could someone help me? Thanks in advance!","Exercise text Calculate where is the cube portion which is under the plane and above the plane . My solution Let we have I'm not sure of the validity of my solution, could someone help me? Thanks in advance!","\ \ \  \ \ \iiint_R y \ dxdydz\ \  R \ 0 \le x,y,z\le1 \  \ x+y+z=2\  \ y+z=1\  D=\{x+y\le2 \ ,\ y\ge1\} \begin{equation}
\begin{split}
\iiint_R y\;dxdydz=&\iint_D y\;dxdy\int_{1-y}^{2-x-y}\;dz\\
=&\iint_D\ {y(1-x)}\;dxdy\\
=&\int_0^2(1-x)\;dx\int_1^{2-x}y\;dy\\
=&\frac{1}{2}\int_0^2(1-x)(4+x^2-4x-1)\;dx\\
=&\frac{2}{3}
\end{split}
\end{equation}",['integration']
49,Verify divergence theorem (Integral Boundaries),Verify divergence theorem (Integral Boundaries),,"I am not sure how to get the boundaries for this problem. So far, I have worked out $\mathbf{n}=\nabla(1-r^2-z)=-(2r\mathbf{e}_r-\mathbf{e}_z)$ , normalize it so $$\hat{\mathbf{n}}=-\frac{1}{\sqrt{5}}(2r\mathbf{e}_r-\mathbf{e}_z).$$ I think the area element is $dA=rdrd\theta$ and volume element $dV=rdrd\theta dz$ and $$\nabla\cdot\mathbf{v}=\frac{z}{r}.$$ So, I have $$\int_V 1-r^2 drd\theta dz=\int_S -\frac{2}{\sqrt{5}}drd\theta.$$ But I don't know how to find the boundaries for this problem. I'd appreciate any help.","I am not sure how to get the boundaries for this problem. So far, I have worked out , normalize it so I think the area element is and volume element and So, I have But I don't know how to find the boundaries for this problem. I'd appreciate any help.",\mathbf{n}=\nabla(1-r^2-z)=-(2r\mathbf{e}_r-\mathbf{e}_z) \hat{\mathbf{n}}=-\frac{1}{\sqrt{5}}(2r\mathbf{e}_r-\mathbf{e}_z). dA=rdrd\theta dV=rdrd\theta dz \nabla\cdot\mathbf{v}=\frac{z}{r}. \int_V 1-r^2 drd\theta dz=\int_S -\frac{2}{\sqrt{5}}drd\theta.,"['calculus', 'multivariable-calculus', 'vector-analysis']"
50,Moment of Inertia of a Tetrahedron about the X-axis and its Centroid.,Moment of Inertia of a Tetrahedron about the X-axis and its Centroid.,,"My task is to compute the moment of inertia and the radius of gyration of a constant-density tetrahedron defined by $x,y,z\ge0$ and $\frac xa + \frac yb + \frac zc \leq 1$ about the x-axis. I know that the moment of inertia, $I$ , of an object is defined by: $I=\int_RdI$ over some region of integration R. Here, I'd let R be the volume of the tetrahedron. Then, $dI = r^2dM$ for mass element $dM=ρ(x,y,z)dV$ for some density constant density $ρ(x,y,z) = k$ . So, we should have: $I = \int_0^c \int_0^{b- \frac bcz} \int_0^{a- \frac aby - \frac acz} ρ(x,y,z)r^2dxdydz$ $r^2$ should represent the distance from any point in the tetrahedron to the x-axis, so we can (I think) let $r^2=y^2+z^2$ . Hence: $I = k\int_0^c \int_0^{b- \frac bcz} \int_0^{a- \frac aby - \frac acz}(y^2+z^2)dxdydz$ . From here, I would compute $I$ and then I would be able to calculate the radius of gyration, $r_g$ , where $r_g=\sqrt{\frac IM}$ . This integral looks unnecessarily complicated and I'm not sure if I set it up correctly. Any idea where I might have gone wrong?","My task is to compute the moment of inertia and the radius of gyration of a constant-density tetrahedron defined by and about the x-axis. I know that the moment of inertia, , of an object is defined by: over some region of integration R. Here, I'd let R be the volume of the tetrahedron. Then, for mass element for some density constant density . So, we should have: should represent the distance from any point in the tetrahedron to the x-axis, so we can (I think) let . Hence: . From here, I would compute and then I would be able to calculate the radius of gyration, , where . This integral looks unnecessarily complicated and I'm not sure if I set it up correctly. Any idea where I might have gone wrong?","x,y,z\ge0 \frac xa + \frac yb + \frac zc \leq 1 I I=\int_RdI dI = r^2dM dM=ρ(x,y,z)dV ρ(x,y,z) = k I = \int_0^c \int_0^{b- \frac bcz} \int_0^{a- \frac aby - \frac acz} ρ(x,y,z)r^2dxdydz r^2 r^2=y^2+z^2 I = k\int_0^c \int_0^{b- \frac bcz} \int_0^{a- \frac aby - \frac acz}(y^2+z^2)dxdydz I r_g r_g=\sqrt{\frac IM}","['calculus', 'integration', 'multivariable-calculus', 'physics', 'classical-mechanics']"
51,"Partial Derivatives of $F(x,y,z)$ where $z = f(x,y)$",Partial Derivatives of  where,"F(x,y,z) z = f(x,y)","When finding the tangent plane of a surface, given by $z = f(x,y)$ , one method for doing so is writing $z$ implicitly: $F(x,y,z) = z - f(x,y) = 0.$ It is well established that the tangent plane of $F(x,y,z) = 0$ at $P(a,b,c)$ , in general, is (1) $$F_x(a,b,c)(x-a) + F_y(a,b,c)(y-b) + F_z(a,b,c)(z-c) = 0.$$ However, the corollary of this theorem is that if $z = f(x,y)$ , then the tangent plane is given by (2) $$-f_x(a,b)(x-a) + -f_y(a,b)(y-b) + (z-c) = 0.$$ What I do not understand is why this follows from the (1). In particular, let's examine $F_x(a,b,c). F_x(a,b,c) = F_x(z - f(x,y))$ . I am confused why $F_x(z) = 0$ . I thought $z = f(x,y)$ , so shouldn't $F_x(z) = F_x(f) = f_x(a,b)$ ? Why can we treat $z$ as a constant here if it is a function of $x$ (and $y$ ).","When finding the tangent plane of a surface, given by , one method for doing so is writing implicitly: It is well established that the tangent plane of at , in general, is (1) However, the corollary of this theorem is that if , then the tangent plane is given by (2) What I do not understand is why this follows from the (1). In particular, let's examine . I am confused why . I thought , so shouldn't ? Why can we treat as a constant here if it is a function of (and ).","z = f(x,y) z F(x,y,z) = z - f(x,y) = 0. F(x,y,z) = 0 P(a,b,c) F_x(a,b,c)(x-a) + F_y(a,b,c)(y-b) + F_z(a,b,c)(z-c) = 0. z = f(x,y) -f_x(a,b)(x-a) + -f_y(a,b)(y-b) + (z-c) = 0. F_x(a,b,c). F_x(a,b,c) = F_x(z - f(x,y)) F_x(z) = 0 z = f(x,y) F_x(z) = F_x(f) = f_x(a,b) z x y","['multivariable-calculus', 'partial-derivative']"
52,"How to take the partial derivative of $f(x,y) = x\ln(x) + y\ln(y), x + y = 1$?",How to take the partial derivative of ?,"f(x,y) = x\ln(x) + y\ln(y), x + y = 1","Let $f(x,y) = x\ln(x) + y\ln(y)$ be defined on space $S = \{(x,y) \in \mathbb{R}^2| x> 0, y > 0, x + y = 1\}$. My question is, how do I take the partial derivative for this function, given that the parameters are coupled through $x+y = 1$. A first idea would be to do it ignoring the coupling constraint. For this, we will get, $\dfrac{\partial f(x,y)}{\partial x} =  \dfrac{\partial x\ln(x) + y\ln(y)}{\partial x}   = \ln(x) + x/x = \ln(x) + 1$ If we do not ignore the coupling constraint, and instead substitute $y = 1-x$, we will get, $\dfrac{\partial f(x,y)}{\partial x} =  \dfrac{\partial x\ln(x) + (1-x)\ln(1-x)}{\partial x}  = \ln(x) + 1 + \dfrac{1}{1-x} - \ln(1-x) - \dfrac{x}{1-x}$ Am I doing this correctly? Why do I get two different expressions of the gradient?","Let $f(x,y) = x\ln(x) + y\ln(y)$ be defined on space $S = \{(x,y) \in \mathbb{R}^2| x> 0, y > 0, x + y = 1\}$. My question is, how do I take the partial derivative for this function, given that the parameters are coupled through $x+y = 1$. A first idea would be to do it ignoring the coupling constraint. For this, we will get, $\dfrac{\partial f(x,y)}{\partial x} =  \dfrac{\partial x\ln(x) + y\ln(y)}{\partial x}   = \ln(x) + x/x = \ln(x) + 1$ If we do not ignore the coupling constraint, and instead substitute $y = 1-x$, we will get, $\dfrac{\partial f(x,y)}{\partial x} =  \dfrac{\partial x\ln(x) + (1-x)\ln(1-x)}{\partial x}  = \ln(x) + 1 + \dfrac{1}{1-x} - \ln(1-x) - \dfrac{x}{1-x}$ Am I doing this correctly? Why do I get two different expressions of the gradient?",,"['multivariable-calculus', 'derivatives', 'partial-derivative', 'constraints', 'constraint-programming']"
53,"$f(x) = (x_1-x_2^2)(x_1-\frac{1}{2}x_2^2)$, verify $\overline{x} = (0,0)^t$ is local min of $\phi(\lambda) = f(\overline{x}+\lambda d)$ but not of $f$",", verify  is local min of  but not of","f(x) = (x_1-x_2^2)(x_1-\frac{1}{2}x_2^2) \overline{x} = (0,0)^t \phi(\lambda) = f(\overline{x}+\lambda d) f","Let $f(x) = (x_1-x_2^2)(x_1-\frac{1}{2}x_2^2)$. Verify that   $\overline{x} = (0,0)^t$ is a local minimizer of $\phi(\lambda) = f(\overline{x}+\lambda d)$ for all $d\in\mathbb{R}^2$ but    $\overline{x}$ is not a local minimizer of $f$ $$\frac{\partial f}{\partial x_1} = 1\left(x_1-\frac{1}{2}x_2^2\right)+(x_1-x_2^2)$$ $$\frac{\partial f}{\partial x_2} = -2x_2\left(x_1-\frac{1}{2}x_2^2\right)-x_2^2(x_1-x_2^2)$$ $$\frac{\partial^2 f}{\partial x_1^2} = 2$$ $$\frac{\partial^2 f}{\partial x_2^2} = 6x^2$$ $$\frac{\partial^2 f}{\partial x_2\partial x_1} = -3x_2$$ $$\nabla^2 f =   \begin{bmatrix}     2 & -3x_2  \\     -3x_2 & 6x_2^2    \end{bmatrix}$$ $$\nabla^2 f(0,0) =   \begin{bmatrix}     2 & 0  \\     0 & 0    \end{bmatrix}$$ $$\begin{bmatrix}     a & b  \\   \end{bmatrix}\begin{bmatrix}     2 & 0  \\     0 & 0    \end{bmatrix}\begin{bmatrix}     a   \\     b     \end{bmatrix} = a^2$$ for $(0,0)$ to be a local minimizer we should have $\nabla f = 0$ (we have) and $\nabla^2\ge 0$. But $\nabla^2$ is not always semipositive definite so $(0,0)$ is not a local minimizer of $f$. Now let's analyze $f((0,0)+\lambda d)$. $$\nabla(\lambda d_1,\lambda d_2) = \begin{bmatrix}     \lambda d_1-\frac{1}{2}\lambda^2d_2^2 + \lambda d_1 -\lambda^2d_2^2   \\     -2\lambda d_2(\lambda d_1-\frac{1}{2}\lambda^2 d_2^2)-\lambda d_ 2^2(\lambda d_1-\lambda^2d_2^2)     \end{bmatrix}\neq 0$$ and the hessian gives an even worse thing. So I think instead I should analyze $$\phi(\lambda) = f(\lambda d_1,\lambda d_2) = (\lambda d_1-\lambda^2 d_2^2)(\lambda d_1-\frac{1}{2}\lambda^2 d_2^2)$$ but what does it mean for $\overline{x}$ to be a local minimizer of something that depends on $\lambda$? UPDATE: I think I have to minimize $$\phi(\lambda) = f(\lambda d_1,\lambda d_2) = (\lambda d_1-\lambda^2 d_2^2)(\lambda d_1-\frac{1}{2}\lambda^2 d_2^2) = \\\lambda^2 d_1^2 -\frac{1}{2}\lambda^3d_1d_2^2-\lambda^3d_2^2 d_1 + \frac{1}{2}\lambda^4d_2^4\implies \\ \phi'(\lambda) =  2d_1^2\lambda  -\frac{3}{2}\lambda^2d_1d_2^2 -3\lambda^2d_2^2d_1 + 2\lambda^3d_2^4 = 0\implies \\2d_2^4\lambda^2+\lambda\left(-\frac{9}{2}d_2^2d_1\right) + 2d_1^2 = 0\implies\\ \lambda = \frac{\frac{9}{2}d_2^2d_1\pm\sqrt{\ (\frac{9}{2}d_2^2d_1)^2 -4\cdot 2d_2^4\cdot2d_1^2}}{4d_2^4}, \lambda = 0$$ It's unpratical to test these values on the function to see which of them is smaller. And even with this I don't know how to proceed. Maybe I should test the second derivative too, I don't know what it means to prove $\overline{x} = (0,0)$ is a minimizer of $\phi(\lambda) = f(\overline{x}+\lambda d)$. The only possible thing I can image is $\lambda$ being $0$ giving us the minimum of $\phi(\lambda)$. So since we know $\lambda=0$ gives us derivative $0$, let's see how the second derivative is at this point (if it's positive, then $\lambda=0$ is a minimizer): $$\phi''(\lambda) = 6\lambda^2 d_2^4 -6\lambda d_2^2 d_1 -\frac{6}{2}\lambda d_1d_2^2 + 2d_1^2 \implies\\\phi''(0) = 2d_1^2$$ So for any direction with $d_1\neq 0$, we'll have $\lambda=0$ as minimizer. What about when the direction is $(0,d_2)$ for any $d_2$? In this case we have $$\phi(\lambda) = (0-(\lambda d_2)^2)(0-\frac{1}{2}(\lambda d_2)^2 ) = \frac{1}{2}\lambda^4d_2^4$$ for which $\lambda= 0$ is also a minimizer. So in all cases $\lambda=0$ is a minimizer of $f((0,0) + \lambda (d_1,d_2))$ which means $(0,0)$ is a minimizer of $\phi(\lambda)$ I guess?","Let $f(x) = (x_1-x_2^2)(x_1-\frac{1}{2}x_2^2)$. Verify that   $\overline{x} = (0,0)^t$ is a local minimizer of $\phi(\lambda) = f(\overline{x}+\lambda d)$ for all $d\in\mathbb{R}^2$ but    $\overline{x}$ is not a local minimizer of $f$ $$\frac{\partial f}{\partial x_1} = 1\left(x_1-\frac{1}{2}x_2^2\right)+(x_1-x_2^2)$$ $$\frac{\partial f}{\partial x_2} = -2x_2\left(x_1-\frac{1}{2}x_2^2\right)-x_2^2(x_1-x_2^2)$$ $$\frac{\partial^2 f}{\partial x_1^2} = 2$$ $$\frac{\partial^2 f}{\partial x_2^2} = 6x^2$$ $$\frac{\partial^2 f}{\partial x_2\partial x_1} = -3x_2$$ $$\nabla^2 f =   \begin{bmatrix}     2 & -3x_2  \\     -3x_2 & 6x_2^2    \end{bmatrix}$$ $$\nabla^2 f(0,0) =   \begin{bmatrix}     2 & 0  \\     0 & 0    \end{bmatrix}$$ $$\begin{bmatrix}     a & b  \\   \end{bmatrix}\begin{bmatrix}     2 & 0  \\     0 & 0    \end{bmatrix}\begin{bmatrix}     a   \\     b     \end{bmatrix} = a^2$$ for $(0,0)$ to be a local minimizer we should have $\nabla f = 0$ (we have) and $\nabla^2\ge 0$. But $\nabla^2$ is not always semipositive definite so $(0,0)$ is not a local minimizer of $f$. Now let's analyze $f((0,0)+\lambda d)$. $$\nabla(\lambda d_1,\lambda d_2) = \begin{bmatrix}     \lambda d_1-\frac{1}{2}\lambda^2d_2^2 + \lambda d_1 -\lambda^2d_2^2   \\     -2\lambda d_2(\lambda d_1-\frac{1}{2}\lambda^2 d_2^2)-\lambda d_ 2^2(\lambda d_1-\lambda^2d_2^2)     \end{bmatrix}\neq 0$$ and the hessian gives an even worse thing. So I think instead I should analyze $$\phi(\lambda) = f(\lambda d_1,\lambda d_2) = (\lambda d_1-\lambda^2 d_2^2)(\lambda d_1-\frac{1}{2}\lambda^2 d_2^2)$$ but what does it mean for $\overline{x}$ to be a local minimizer of something that depends on $\lambda$? UPDATE: I think I have to minimize $$\phi(\lambda) = f(\lambda d_1,\lambda d_2) = (\lambda d_1-\lambda^2 d_2^2)(\lambda d_1-\frac{1}{2}\lambda^2 d_2^2) = \\\lambda^2 d_1^2 -\frac{1}{2}\lambda^3d_1d_2^2-\lambda^3d_2^2 d_1 + \frac{1}{2}\lambda^4d_2^4\implies \\ \phi'(\lambda) =  2d_1^2\lambda  -\frac{3}{2}\lambda^2d_1d_2^2 -3\lambda^2d_2^2d_1 + 2\lambda^3d_2^4 = 0\implies \\2d_2^4\lambda^2+\lambda\left(-\frac{9}{2}d_2^2d_1\right) + 2d_1^2 = 0\implies\\ \lambda = \frac{\frac{9}{2}d_2^2d_1\pm\sqrt{\ (\frac{9}{2}d_2^2d_1)^2 -4\cdot 2d_2^4\cdot2d_1^2}}{4d_2^4}, \lambda = 0$$ It's unpratical to test these values on the function to see which of them is smaller. And even with this I don't know how to proceed. Maybe I should test the second derivative too, I don't know what it means to prove $\overline{x} = (0,0)$ is a minimizer of $\phi(\lambda) = f(\overline{x}+\lambda d)$. The only possible thing I can image is $\lambda$ being $0$ giving us the minimum of $\phi(\lambda)$. So since we know $\lambda=0$ gives us derivative $0$, let's see how the second derivative is at this point (if it's positive, then $\lambda=0$ is a minimizer): $$\phi''(\lambda) = 6\lambda^2 d_2^4 -6\lambda d_2^2 d_1 -\frac{6}{2}\lambda d_1d_2^2 + 2d_1^2 \implies\\\phi''(0) = 2d_1^2$$ So for any direction with $d_1\neq 0$, we'll have $\lambda=0$ as minimizer. What about when the direction is $(0,d_2)$ for any $d_2$? In this case we have $$\phi(\lambda) = (0-(\lambda d_2)^2)(0-\frac{1}{2}(\lambda d_2)^2 ) = \frac{1}{2}\lambda^4d_2^4$$ for which $\lambda= 0$ is also a minimizer. So in all cases $\lambda=0$ is a minimizer of $f((0,0) + \lambda (d_1,d_2))$ which means $(0,0)$ is a minimizer of $\phi(\lambda)$ I guess?",,"['multivariable-calculus', 'derivatives', 'optimization', 'maxima-minima']"
54,"Proof verification: Let $f:O\subset \Bbb{R}^n\to\Bbb{R}$ be a $C^3$ function. If $A$ is positive, then $x_0$ is a local minimum.","Proof verification: Let  be a  function. If  is positive, then  is a local minimum.",f:O\subset \Bbb{R}^n\to\Bbb{R} C^3 A x_0,"Let $f:O\subset \Bbb{R}^n\to\Bbb{R}$ be a $C^3$ function. Let $x_0\in O$ be a critical point of $f$. Let \begin{align}A=\left(\frac{\partial ^2f}{\partial x_i\partial x_j}\right)_{1\leq i,j\leq n}\end{align} If $A$ is positive, then $x_0$ is a local minimum. Proof By Taylor's formula \begin{align}f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+\Vert x-x_0 \Vert^2\epsilon(x-x_0)\end{align} But \begin{align}f'(x_0)=0\end{align}This implies \begin{align}f(x)-f(x_0)=\frac{f''(x_0)}{2!}(x-x_0)^2+\Vert x-x_0 \Vert^2\epsilon(x-x_0)\end{align} Now, \begin{align} f''(x_0)(x-x_0)^2=\langle A(x-x_0),(x-x_0)\rangle\end{align} \begin{align} =\sum^{n}_{i=1}\sum^{n}_{j=1}\frac{\partial ^2f(x_0)}{\partial x_i\partial x_j}(x_j-x_{0_j})^2\end{align} \begin{align} =\sum^{n}_{i=1}\sum^{n}_{j=1}(x_j-x_{0_j})\frac{\partial ^2f(x_0)}{\partial x_i\partial x_j}(x_j-x_{0_j})\end{align} \begin{align} =\langle (x-x_0),A(x-x_0)\rangle\end{align} Thus,  \begin{align}f(x)-f(x_0)=\frac{\langle (x-x_0),A(x-x_0)\rangle}{2!}(x-x_0)^2+\Vert x-x_0 \Vert^2\epsilon(x-x_0)\end{align} \begin{align}f(x)-f(x_0)\geq\Vert x-x_0\Vert^2\left[\alpha+\epsilon(x-x_0)\right],\;\;\text{for some}\;\alpha>0\end{align} Hence, $\exists\;\delta>0$ such that $\Vert x-x_0\Vert<\delta$ implies $\left[\alpha+\epsilon(x-x_0)\right]>0$. So, for $x\in(x_0-\delta,x_0+\delta)$ \begin{align}f(x)\geq f(x_0)\end{align} Therefore, $x_0$ is a local minimum. Can someone check if this proof is correct? Corrections will be highly welcome! Thanks","Let $f:O\subset \Bbb{R}^n\to\Bbb{R}$ be a $C^3$ function. Let $x_0\in O$ be a critical point of $f$. Let \begin{align}A=\left(\frac{\partial ^2f}{\partial x_i\partial x_j}\right)_{1\leq i,j\leq n}\end{align} If $A$ is positive, then $x_0$ is a local minimum. Proof By Taylor's formula \begin{align}f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+\Vert x-x_0 \Vert^2\epsilon(x-x_0)\end{align} But \begin{align}f'(x_0)=0\end{align}This implies \begin{align}f(x)-f(x_0)=\frac{f''(x_0)}{2!}(x-x_0)^2+\Vert x-x_0 \Vert^2\epsilon(x-x_0)\end{align} Now, \begin{align} f''(x_0)(x-x_0)^2=\langle A(x-x_0),(x-x_0)\rangle\end{align} \begin{align} =\sum^{n}_{i=1}\sum^{n}_{j=1}\frac{\partial ^2f(x_0)}{\partial x_i\partial x_j}(x_j-x_{0_j})^2\end{align} \begin{align} =\sum^{n}_{i=1}\sum^{n}_{j=1}(x_j-x_{0_j})\frac{\partial ^2f(x_0)}{\partial x_i\partial x_j}(x_j-x_{0_j})\end{align} \begin{align} =\langle (x-x_0),A(x-x_0)\rangle\end{align} Thus,  \begin{align}f(x)-f(x_0)=\frac{\langle (x-x_0),A(x-x_0)\rangle}{2!}(x-x_0)^2+\Vert x-x_0 \Vert^2\epsilon(x-x_0)\end{align} \begin{align}f(x)-f(x_0)\geq\Vert x-x_0\Vert^2\left[\alpha+\epsilon(x-x_0)\right],\;\;\text{for some}\;\alpha>0\end{align} Hence, $\exists\;\delta>0$ such that $\Vert x-x_0\Vert<\delta$ implies $\left[\alpha+\epsilon(x-x_0)\right]>0$. So, for $x\in(x_0-\delta,x_0+\delta)$ \begin{align}f(x)\geq f(x_0)\end{align} Therefore, $x_0$ is a local minimum. Can someone check if this proof is correct? Corrections will be highly welcome! Thanks",,"['calculus', 'multivariable-calculus', 'derivatives', 'proof-verification']"
55,Gradient of a function that involves a matrix square root [closed],Gradient of a function that involves a matrix square root [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let $S$ be a positive definite matrix of size $n$. Consider the function $f: \mathbb{R}_+^n \longrightarrow \mathbb{R}$ defined by $$\forall u \in \mathbb{R}^n, \; f(u) = \text{tr}((\text{diag}(u)S)^{1/2}).$$ What would be its gradient?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let $S$ be a positive definite matrix of size $n$. Consider the function $f: \mathbb{R}_+^n \longrightarrow \mathbb{R}$ defined by $$\forall u \in \mathbb{R}^n, \; f(u) = \text{tr}((\text{diag}(u)S)^{1/2}).$$ What would be its gradient?",,"['matrices', 'multivariable-calculus', 'differential', 'symmetric-matrices']"
56,"Prove that if $\langle df_x(v),v\rangle>0$, then $f$ is injective","Prove that if , then  is injective","\langle df_x(v),v\rangle>0 f","Suppose $f:\mathbb R^n\to \mathbb R^n$ is a $\mathcal C^1$ map such that $\langle df_x(v),v\rangle>0$ for all $x\in \mathbb R^n$ and $v\in R^n\setminus \{0\}$. Prove that $f$ is injective. Hint: For $x\ne 0, $ consider $g:\mathbb R\to \mathbb R^n$ given by $g(t)=f(tx)$. Find $g'(t)$ and show that $f(x)\ne f(0)$. Let $x=(x_1,\dots,x_n)$. To compute $g'(t)=dg_t$, note that $g$ is the composition of $h:\mathbb R\to \mathbb R^n$ given by $t\mapsto tx$ and $f$. By the chain rule, $dg_t=df_{tx}\cdot dh_t$ as matrices. Now $dh_t=[x_1,\dots,x_n]^t,df_{tx}=[D_if_j]$, where $f=(f_1,\dots,f_n)$, so $$dg_t=[D_1f_1(tx)x_1+\dots+D_1f_n(tx)x_n,\dots, D_nf_1(tx)x_1+\dots +D_nf_n(tx)x_n]^t.$$ Injectivity means that $g(0)=f(x)\ne 0$ if $x\ne 0$.  What does it have to do with injectivity? How to use the given inequality?","Suppose $f:\mathbb R^n\to \mathbb R^n$ is a $\mathcal C^1$ map such that $\langle df_x(v),v\rangle>0$ for all $x\in \mathbb R^n$ and $v\in R^n\setminus \{0\}$. Prove that $f$ is injective. Hint: For $x\ne 0, $ consider $g:\mathbb R\to \mathbb R^n$ given by $g(t)=f(tx)$. Find $g'(t)$ and show that $f(x)\ne f(0)$. Let $x=(x_1,\dots,x_n)$. To compute $g'(t)=dg_t$, note that $g$ is the composition of $h:\mathbb R\to \mathbb R^n$ given by $t\mapsto tx$ and $f$. By the chain rule, $dg_t=df_{tx}\cdot dh_t$ as matrices. Now $dh_t=[x_1,\dots,x_n]^t,df_{tx}=[D_if_j]$, where $f=(f_1,\dots,f_n)$, so $$dg_t=[D_1f_1(tx)x_1+\dots+D_1f_n(tx)x_n,\dots, D_nf_1(tx)x_1+\dots +D_nf_n(tx)x_n]^t.$$ Injectivity means that $g(0)=f(x)\ne 0$ if $x\ne 0$.  What does it have to do with injectivity? How to use the given inequality?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives']"
57,"$f(x) = x^TMx$, using Lagrange multipliers to prove SVD decomposition",", using Lagrange multipliers to prove SVD decomposition",f(x) = x^TMx,"I'm reading the existence proof of singular value decomposition . It considers $f:\mathbb{R}^n\to \mathbb{R}, f(x) = x^TMx$. It talks about the gradient of $f$ and make it equal to a multiple of the gradient of $x^tx$. I suppose that it's because the constraint is the unit sphere, so that's why it made $x^tx = x_1^2 + \cdots x_n^2$, right? I'm trying to understand this so I took $f$ with a generic matrix $M$ $$f(x) =\begin{bmatrix}      x_1  & \cdots & x_n      \end{bmatrix}\begin{bmatrix}      a_{11} & a_{12} & \dots \\     \vdots & \ddots & \\     a_{n1} &        & a_{nn}      \end{bmatrix}\begin{bmatrix}      x_1  \\     \vdots  \\     x_n       \end{bmatrix} = \\ x_1(a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n) + \\x_2 (a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n) + \\ \cdots  + \\x_n(a_{n1}x_1+a_{n2}x_2 + \cdots + a_{nn}x_n)$$ Taking the partials to construct the gradient vector, I can see that I'll end up with: $$\begin{bmatrix}      2a_{11}x_1 + a_{21} + \cdots a_{n1}  \\     a_{12} + 2a_2x_2 + \cdots + a_{n2} \\     \vdots \\     a_{1n} + a_{2n}\cdots + 2a_{nn}x_n\\       \end{bmatrix} $$ Now, I need to equal this with $\lambda$ gradient of $x^tx$: $$\begin{bmatrix}      2x_1  \\     2x_2 \\     \vdots \\     2x_n\\       \end{bmatrix}$$ so: $$\begin{bmatrix}      2a_{11}x_1 + a_{21} + \cdots a_{n1}  \\     a_{12} + 2a_2x_2 + \cdots + a_{n2} \\     \vdots \\     a_{1n} + a_{2n}\cdots + 2a_{nn}x_n\\       \end{bmatrix} = \lambda \begin{bmatrix}      2x_1  \\     2x_2 \\     \vdots \\     2x_n\\       \end{bmatrix} $$ As an example, the first line becomes: $2a_{11}x_1 + a_{21} + \cdots a_{n1} = \lambda 2x_1 \implies \lambda 2x_1 -2a_{11}x_1 =  a_{21} + \cdots a_{n1}\implies x_1(2\lambda - 2a_{11}) =  a_{21} + \cdots a_{n1}$ What should I do now? It says that I should end up with $Mu = \lambda u$ Also, is there a more elegant way of calculating the gradients or it's just all this mess?","I'm reading the existence proof of singular value decomposition . It considers $f:\mathbb{R}^n\to \mathbb{R}, f(x) = x^TMx$. It talks about the gradient of $f$ and make it equal to a multiple of the gradient of $x^tx$. I suppose that it's because the constraint is the unit sphere, so that's why it made $x^tx = x_1^2 + \cdots x_n^2$, right? I'm trying to understand this so I took $f$ with a generic matrix $M$ $$f(x) =\begin{bmatrix}      x_1  & \cdots & x_n      \end{bmatrix}\begin{bmatrix}      a_{11} & a_{12} & \dots \\     \vdots & \ddots & \\     a_{n1} &        & a_{nn}      \end{bmatrix}\begin{bmatrix}      x_1  \\     \vdots  \\     x_n       \end{bmatrix} = \\ x_1(a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n) + \\x_2 (a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n) + \\ \cdots  + \\x_n(a_{n1}x_1+a_{n2}x_2 + \cdots + a_{nn}x_n)$$ Taking the partials to construct the gradient vector, I can see that I'll end up with: $$\begin{bmatrix}      2a_{11}x_1 + a_{21} + \cdots a_{n1}  \\     a_{12} + 2a_2x_2 + \cdots + a_{n2} \\     \vdots \\     a_{1n} + a_{2n}\cdots + 2a_{nn}x_n\\       \end{bmatrix} $$ Now, I need to equal this with $\lambda$ gradient of $x^tx$: $$\begin{bmatrix}      2x_1  \\     2x_2 \\     \vdots \\     2x_n\\       \end{bmatrix}$$ so: $$\begin{bmatrix}      2a_{11}x_1 + a_{21} + \cdots a_{n1}  \\     a_{12} + 2a_2x_2 + \cdots + a_{n2} \\     \vdots \\     a_{1n} + a_{2n}\cdots + 2a_{nn}x_n\\       \end{bmatrix} = \lambda \begin{bmatrix}      2x_1  \\     2x_2 \\     \vdots \\     2x_n\\       \end{bmatrix} $$ As an example, the first line becomes: $2a_{11}x_1 + a_{21} + \cdots a_{n1} = \lambda 2x_1 \implies \lambda 2x_1 -2a_{11}x_1 =  a_{21} + \cdots a_{n1}\implies x_1(2\lambda - 2a_{11}) =  a_{21} + \cdots a_{n1}$ What should I do now? It says that I should end up with $Mu = \lambda u$ Also, is there a more elegant way of calculating the gradients or it's just all this mess?",,"['linear-algebra', 'multivariable-calculus', 'vector-analysis', 'lagrange-multiplier']"
58,"Definition of integration of forms over manifolds, Spivak.","Definition of integration of forms over manifolds, Spivak.",,"Let $c$ be an orientation preserving k-cube in $M$( k dimensional manifold with boundary with orientation $\mu$) such that $c_{(k,0)} $ lies in $\partial M$ and is the only face that has any interior points in $\partial M$. $c_{(i,\alpha)}=c\circ (I^n_{(i,\alpha)})$ and $I^n_{(i,\alpha)}=(x^1,\cdots , x^{i-1},\alpha,x^i,\cdots,x^{n-1})$ $\omega$ is a k-1 form on $M$ which is $0$ outside of $c([0,1]^k)$. 1) In $\int_{c_{(k,0)}} \omega =(-1)^k\int _{\partial M}\omega$ How did we get $(-1)^k$ ? 2) In $ \int _{\partial c}\omega=\int_{(-1)^kc_{(k,0)}}\omega=(-1)^k\int_{c_{(k,0)}} \omega =\int _{\partial M}\omega $ I am concerned with only the first equality and included the others just to give sense of what he is trying to show. Shouldn't it be $\sum_{i,\alpha} (-1)^k\int_{c_{(i,\alpha)}}\omega$ ? Why other faces not appear in the integration as a sum?","Let $c$ be an orientation preserving k-cube in $M$( k dimensional manifold with boundary with orientation $\mu$) such that $c_{(k,0)} $ lies in $\partial M$ and is the only face that has any interior points in $\partial M$. $c_{(i,\alpha)}=c\circ (I^n_{(i,\alpha)})$ and $I^n_{(i,\alpha)}=(x^1,\cdots , x^{i-1},\alpha,x^i,\cdots,x^{n-1})$ $\omega$ is a k-1 form on $M$ which is $0$ outside of $c([0,1]^k)$. 1) In $\int_{c_{(k,0)}} \omega =(-1)^k\int _{\partial M}\omega$ How did we get $(-1)^k$ ? 2) In $ \int _{\partial c}\omega=\int_{(-1)^kc_{(k,0)}}\omega=(-1)^k\int_{c_{(k,0)}} \omega =\int _{\partial M}\omega $ I am concerned with only the first equality and included the others just to give sense of what he is trying to show. Shouldn't it be $\sum_{i,\alpha} (-1)^k\int_{c_{(i,\alpha)}}\omega$ ? Why other faces not appear in the integration as a sum?",,"['calculus', 'real-analysis', 'general-topology', 'multivariable-calculus', 'manifolds']"
59,"Evaluating $\int_{\frac{-1}{2}}^{\frac{1}{2}} \int_{\frac{-1}{2}}^{\frac{1}{2}} \frac{x^2}{(x^2+y^2)^2 \log^2(\frac{2}{\sqrt{x^2+y^2}})} \,dx\,dy.$",Evaluating,"\int_{\frac{-1}{2}}^{\frac{1}{2}} \int_{\frac{-1}{2}}^{\frac{1}{2}} \frac{x^2}{(x^2+y^2)^2 \log^2(\frac{2}{\sqrt{x^2+y^2}})} \,dx\,dy.","$$\int_{\frac{-1}{2}}^{\frac{1}{2}} \int_{\frac{-1}{2}}^{\frac{1}{2}} \frac{x^2}{(x^2+y^2)^2 \log^2(\frac{2}{\sqrt{x^2+y^2}})} \,dx\,dy.$$ I encountered this integral while evaluating norm of a function. The first attempt was to change it into polar coordinates, which didn't work well, since the region of integration is rectangular. I found a similar integral here , but even WolframAlpha couldn't calculate this. Does anybody know how to evaluate this?","$$\int_{\frac{-1}{2}}^{\frac{1}{2}} \int_{\frac{-1}{2}}^{\frac{1}{2}} \frac{x^2}{(x^2+y^2)^2 \log^2(\frac{2}{\sqrt{x^2+y^2}})} \,dx\,dy.$$ I encountered this integral while evaluating norm of a function. The first attempt was to change it into polar coordinates, which didn't work well, since the region of integration is rectangular. I found a similar integral here , but even WolframAlpha couldn't calculate this. Does anybody know how to evaluate this?",,"['calculus', 'integration', 'multivariable-calculus', 'multiple-integral']"
60,Spherical coordinates when the ball is not centered in the origin,Spherical coordinates when the ball is not centered in the origin,,"I need to calculate $$\iiint _V\sqrt{x^2+y^2+z^2} \,dx \,dy\, dz$$ where $V$ is the ball $$x^2+y^2+z^2 \leq 4z \Leftrightarrow x^2 + y^2+(z-2)^2 \leq 4$$ The hint is to use origin centered spherical coordinates. So, after substitution I get: $$ r \leq 2 \cos \phi. $$ This obviously implies that $0\leq r \leq 2\cos \phi$. But as far as I can understand, it also implies that $\phi$, which is always bounded in $\left[ 0,\pi \right] $, now satisfies $0\leq \phi \leq \frac{\pi}{2}$. Is it true that indeed $\theta$, which has no constraints on it, will satisfy $0\leq \theta \leq 2\pi$? Is it true that the final integral is $$ \int_0^{2\pi}d\theta \int_0^{\frac{\pi}{2}} \int_0^{2\cos \phi} r^3\sin\phi \,dr\,d\phi ? $$ Thanks a lot!","I need to calculate $$\iiint _V\sqrt{x^2+y^2+z^2} \,dx \,dy\, dz$$ where $V$ is the ball $$x^2+y^2+z^2 \leq 4z \Leftrightarrow x^2 + y^2+(z-2)^2 \leq 4$$ The hint is to use origin centered spherical coordinates. So, after substitution I get: $$ r \leq 2 \cos \phi. $$ This obviously implies that $0\leq r \leq 2\cos \phi$. But as far as I can understand, it also implies that $\phi$, which is always bounded in $\left[ 0,\pi \right] $, now satisfies $0\leq \phi \leq \frac{\pi}{2}$. Is it true that indeed $\theta$, which has no constraints on it, will satisfy $0\leq \theta \leq 2\pi$? Is it true that the final integral is $$ \int_0^{2\pi}d\theta \int_0^{\frac{\pi}{2}} \int_0^{2\cos \phi} r^3\sin\phi \,dr\,d\phi ? $$ Thanks a lot!",,"['integration', 'multivariable-calculus']"
61,Find the surface area of the part of the sphere $x^2 + y^2 + z^2 = 16$ inside the cylinder $x^2 - 4x + y^2 = 0$,Find the surface area of the part of the sphere  inside the cylinder,x^2 + y^2 + z^2 = 16 x^2 - 4x + y^2 = 0,"Find the surface area of the part of the sphere $x^2 + y^2 + z^2 = 16$   inside the cylinder $x^2 - 4x + y^2 = 0$ First of all, I need to find the equation of the plane along which these two solids intersect. Solving for $y^2$ from the equation of the cylinder and substituting to the sphere, we get $$z^2+4x=16$$ And so we have the integration are with respect to $z$ and $x$. The transform we want are considering is $$T(x,z) = (x, z, 16-x^2-z^2)$$ We are integrating on the $z^2+4x=16$ surface. and so we need to set bounds for $z$ and $x$. $$z_0 = 0 \le z \le z_1 =4 $$ $$x_0 = -\sqrt{16-z^2} \le x  \le x_1 = \sqrt{16-z^2}$$ Because of symmetry, we can find the are above the $xy$ plane and then multiply by $2$. And so, using the formula for the surface integral, we get that the surface are $$S = \int_{z_0}^{z_1}\int_{x_o}^{x_1}\sqrt{1 + (\frac{\partial y}{\partial z})^2+ (\frac{\partial y}{\partial x})^2}$$ Is my attempt to solve this correct? I have never calculated a surface integral before and so I am not sure about the bounds and the method.","Find the surface area of the part of the sphere $x^2 + y^2 + z^2 = 16$   inside the cylinder $x^2 - 4x + y^2 = 0$ First of all, I need to find the equation of the plane along which these two solids intersect. Solving for $y^2$ from the equation of the cylinder and substituting to the sphere, we get $$z^2+4x=16$$ And so we have the integration are with respect to $z$ and $x$. The transform we want are considering is $$T(x,z) = (x, z, 16-x^2-z^2)$$ We are integrating on the $z^2+4x=16$ surface. and so we need to set bounds for $z$ and $x$. $$z_0 = 0 \le z \le z_1 =4 $$ $$x_0 = -\sqrt{16-z^2} \le x  \le x_1 = \sqrt{16-z^2}$$ Because of symmetry, we can find the are above the $xy$ plane and then multiply by $2$. And so, using the formula for the surface integral, we get that the surface are $$S = \int_{z_0}^{z_1}\int_{x_o}^{x_1}\sqrt{1 + (\frac{\partial y}{\partial z})^2+ (\frac{\partial y}{\partial x})^2}$$ Is my attempt to solve this correct? I have never calculated a surface integral before and so I am not sure about the bounds and the method.",,"['calculus', 'integration', 'multivariable-calculus', 'surface-integrals']"
62,Tangent Plane for a Level Set,Tangent Plane for a Level Set,,"I am currently looking at the following theorem which I know how to prove from one side but not from the another side: Theorem : If $a$ is a regular value of a smooth function $F:U\subset\mathbb{R}^3 \rightarrow \mathbb{R}$ and $p \in F^{-1}(a)=S$, then the tangent plane $T_pS=(\nabla F(p))^{\perp}$. The definition of surfaces and tangent planes follow the textbook ""Differential Geometry of Curves and Surfaces"" by M. do Carmo. Now, I know how to prove $T_pS\subset(\nabla F(p))^{\perp}$ simply by chain rule. But I have no idea how to prove the another side of the subset relation, because what I need to do is that given $u \in (\nabla F(p))^{\perp}$, I need a curve $\alpha:(-\epsilon,\epsilon)\rightarrow S$ such that $\alpha(0)=p$ and $\alpha'(0)=u$, but I have no idea how to find such a curve.","I am currently looking at the following theorem which I know how to prove from one side but not from the another side: Theorem : If $a$ is a regular value of a smooth function $F:U\subset\mathbb{R}^3 \rightarrow \mathbb{R}$ and $p \in F^{-1}(a)=S$, then the tangent plane $T_pS=(\nabla F(p))^{\perp}$. The definition of surfaces and tangent planes follow the textbook ""Differential Geometry of Curves and Surfaces"" by M. do Carmo. Now, I know how to prove $T_pS\subset(\nabla F(p))^{\perp}$ simply by chain rule. But I have no idea how to prove the another side of the subset relation, because what I need to do is that given $u \in (\nabla F(p))^{\perp}$, I need a curve $\alpha:(-\epsilon,\epsilon)\rightarrow S$ such that $\alpha(0)=p$ and $\alpha'(0)=u$, but I have no idea how to find such a curve.",,"['multivariable-calculus', 'differential-geometry']"
63,Finding surface of the set in $\mathbb{R}^3$,Finding surface of the set in,\mathbb{R}^3,"Given set $$M \equiv x^2 + y^2 \leq 2z,~z \in [0, 1]$$ find it's surface. Using cylindrical coordinates I'm finding that $$r^2 = 2z \implies z = \frac{r^2}{2}.$$ Now my transformation is of the form $x = r\cos\phi$, $y = r\sin\phi$ and $z = r^2/2$. Taking my transformation as a vector $\Psi := (x, y, z) = (r\cos\phi, r\sin\phi, r^2/2)$ norm of the normal vector is then $||\partial_r \Psi \times \partial_\phi \Psi|| = ||\mathbf{n}|| = r\sqrt{1 + r^2}$. Hence the surface of the set is $$S = \int_M 1~dS  =  \int_{0}^{2\pi}d\phi\int_{0}^{1}dr~r\sqrt{1 + r^2} = \frac{2\pi}{3}\left(2\sqrt{2} - 1\right).$$ Is my approach correct or did I made any mistake?","Given set $$M \equiv x^2 + y^2 \leq 2z,~z \in [0, 1]$$ find it's surface. Using cylindrical coordinates I'm finding that $$r^2 = 2z \implies z = \frac{r^2}{2}.$$ Now my transformation is of the form $x = r\cos\phi$, $y = r\sin\phi$ and $z = r^2/2$. Taking my transformation as a vector $\Psi := (x, y, z) = (r\cos\phi, r\sin\phi, r^2/2)$ norm of the normal vector is then $||\partial_r \Psi \times \partial_\phi \Psi|| = ||\mathbf{n}|| = r\sqrt{1 + r^2}$. Hence the surface of the set is $$S = \int_M 1~dS  =  \int_{0}^{2\pi}d\phi\int_{0}^{1}dr~r\sqrt{1 + r^2} = \frac{2\pi}{3}\left(2\sqrt{2} - 1\right).$$ Is my approach correct or did I made any mistake?",,"['integration', 'multivariable-calculus', 'surfaces', 'surface-integrals', 'multiple-integral']"
64,Questions about the potential of a conservative vector field,Questions about the potential of a conservative vector field,,"I recently started learning multivariable calculus and I came across some questions regarding conservative vector fields and it's a bit confusing. If I took the derivative of my potential function (as in $\nabla f(x,y,z)$), it will give me a vector field. But if I integrated my vector field, would it give me the potential function as well, or would it give me a completely different function? If I encounter a problem where I have to find the potential function of a conservative vector field, should I approach the problem as an initial value condition problem except with multiple independent variables? First of all, I can't imagine the problem graphically. What does it mean for a vector field to be conservative? I don't get why my professor said it has to be a closed path in order to find the potential. What's the meaning of this symbol $\oint$? How does this relate to other real-world applications? I don't really understand much of this at all. Note: I don't have a good background in physics, so it doesn't really help to say ""know your physics"" as others have said to me. And I just started learning line integrals, so I would really love some insight that allows to me view the problem more easily.","I recently started learning multivariable calculus and I came across some questions regarding conservative vector fields and it's a bit confusing. If I took the derivative of my potential function (as in $\nabla f(x,y,z)$), it will give me a vector field. But if I integrated my vector field, would it give me the potential function as well, or would it give me a completely different function? If I encounter a problem where I have to find the potential function of a conservative vector field, should I approach the problem as an initial value condition problem except with multiple independent variables? First of all, I can't imagine the problem graphically. What does it mean for a vector field to be conservative? I don't get why my professor said it has to be a closed path in order to find the potential. What's the meaning of this symbol $\oint$? How does this relate to other real-world applications? I don't really understand much of this at all. Note: I don't have a good background in physics, so it doesn't really help to say ""know your physics"" as others have said to me. And I just started learning line integrals, so I would really love some insight that allows to me view the problem more easily.",,"['integration', 'multivariable-calculus', 'vector-analysis']"
65,First variation of volume in $\mathbb{R}^3$,First variation of volume in,\mathbb{R}^3,"Let $U \subset \mathbb{R}^3$ be a bounded subset with smooth boundary. Let $Y \colon \mathbb{R}^3 \to \mathbb{R}^3 $ be a smooth vector field. I know that the first variation of the volume of $U$ w.r.t. $Y$ is given by $$ \delta_Y|U| = \int_U \text{div}Y \, dx_1 dx_2dx_3. $$ I know that it follows from some standard computation, but I can't remember how to derive it.","Let $U \subset \mathbb{R}^3$ be a bounded subset with smooth boundary. Let $Y \colon \mathbb{R}^3 \to \mathbb{R}^3 $ be a smooth vector field. I know that the first variation of the volume of $U$ w.r.t. $Y$ is given by $$ \delta_Y|U| = \int_U \text{div}Y \, dx_1 dx_2dx_3. $$ I know that it follows from some standard computation, but I can't remember how to derive it.",,"['multivariable-calculus', 'differential-geometry']"
66,Surface Integrals - Parametric Representation,Surface Integrals - Parametric Representation,,Find the parametric representation for the parts of the plane $$2x+3y+z=4$$ where $$1\leq x+y+z\leq 7$$ and $$2\leq x-y\leq4$$. My attempt: I thought to let $u=x+y+z$ and $v=x-y$ such that $1\leq u \leq 7$ and $2\leq v\leq4$. But I'm unable to find a suitable parametric representation.,Find the parametric representation for the parts of the plane $$2x+3y+z=4$$ where $$1\leq x+y+z\leq 7$$ and $$2\leq x-y\leq4$$. My attempt: I thought to let $u=x+y+z$ and $v=x-y$ such that $1\leq u \leq 7$ and $2\leq v\leq4$. But I'm unable to find a suitable parametric representation.,,['multivariable-calculus']
67,Laplacian of the inverse of a diffeomorphism,Laplacian of the inverse of a diffeomorphism,,Let $\Phi: \mathbb{R}^n \to \mathbb{R}^n$ be a diffeomorhism. Let $\Delta$ be the componentwise Laplacian.  Is it possible to wirite $$ \Delta (\Phi^{-1})\circ \Phi $$ in such a form that involves only $\Phi$ and not $\Phi^{-1}$? To clarify my question: For the Jacobian matrix (which I will denote by $D$) it is clearly possible. Indeed $$ D(\Phi^{-1})\circ \Phi = (D\Phi)^{-1} $$,Let $\Phi: \mathbb{R}^n \to \mathbb{R}^n$ be a diffeomorhism. Let $\Delta$ be the componentwise Laplacian.  Is it possible to wirite $$ \Delta (\Phi^{-1})\circ \Phi $$ in such a form that involves only $\Phi$ and not $\Phi^{-1}$? To clarify my question: For the Jacobian matrix (which I will denote by $D$) it is clearly possible. Indeed $$ D(\Phi^{-1})\circ \Phi = (D\Phi)^{-1} $$,,"['real-analysis', 'multivariable-calculus', 'laplacian', 'inverse-function-theorem']"
68,Compare two functions in neighborhood of zero,Compare two functions in neighborhood of zero,,"Basically, I have two functions: $$ f(x, y) = \sqrt{(x-y^2)^2 + x^4} $$ and  $$ g(x, y) = | (x-y^2)^3 | $$ I need to compare them in a punctured neighborhood of zero. I am a bit stuck here. Edit: compare in sense that there exists a punctured neighborhood of zero where for all $x$ and $y$ from that neighborhood one function is greater than the other. I.e. I want to prove that $$ \exists O(0,0):\quad \forall x, y \in O(0,0) \quad |f(x, y)| > |g(x,y)| $$","Basically, I have two functions: $$ f(x, y) = \sqrt{(x-y^2)^2 + x^4} $$ and  $$ g(x, y) = | (x-y^2)^3 | $$ I need to compare them in a punctured neighborhood of zero. I am a bit stuck here. Edit: compare in sense that there exists a punctured neighborhood of zero where for all $x$ and $y$ from that neighborhood one function is greater than the other. I.e. I want to prove that $$ \exists O(0,0):\quad \forall x, y \in O(0,0) \quad |f(x, y)| > |g(x,y)| $$",,[]
69,"Given $z+x\ln(z)+xe^{xy}-1=0$ find the directional derivative at $P=(0,1)$ in the direction of $v= \langle 4 \sqrt{3} , 3 \sqrt{3} \rangle$",Given  find the directional derivative at  in the direction of,"z+x\ln(z)+xe^{xy}-1=0 P=(0,1) v= \langle 4 \sqrt{3} , 3 \sqrt{3} \rangle","I am given the following exercise: Given $$z+x\ln(z)+xe^{xy}-1=0$$ find the directional derivative at $P=(0,1)$ in the direction of $$v= \langle 4 \sqrt{3} , 3 \sqrt{3} \rangle$$ There's no solution on the textbook so I would like to check my reasoning. Firstly I expressed the direction of the vector as being $v= \langle \frac{4}{5} , \frac{3}{5} \rangle$ so it is 1 unit long (I divided by its magnitude). After that, I proceeded evaluating the partial derivatives (that's the tricky part). Using the implicit differentiation theorem and substituting the point $(0,1)$ (when $x=0$ and $y=1$ then $z = 1$) \begin{align*} F(x,y,z) &= z+x\ln(z)+xe^{xy}-1\\ \\ F_x(x,y,z) &= \ln(z) + e^{xy}+xye^{xy} = 1\\ F_y(x,y,z) &= x^2 \cdot e^{xy} = 0\\ F_z(x,y,z) &= 1+\frac{x}{z} = 1\\ \\ \frac{\partial z}{\partial x} &= - \frac{F_x}{F_z} = -1\\ \frac{\partial z}{\partial y} &= - \frac{F_y}{F_z} =  0 \end{align*} So the directional derivative is given by $$D_{\vec{v}} = \langle -1 , 0 \rangle \left\langle \frac{4}{5} , \frac{3}{5} \right\rangle = - \frac{4}{5}$$ Is my solution correct? Thank you.","I am given the following exercise: Given $$z+x\ln(z)+xe^{xy}-1=0$$ find the directional derivative at $P=(0,1)$ in the direction of $$v= \langle 4 \sqrt{3} , 3 \sqrt{3} \rangle$$ There's no solution on the textbook so I would like to check my reasoning. Firstly I expressed the direction of the vector as being $v= \langle \frac{4}{5} , \frac{3}{5} \rangle$ so it is 1 unit long (I divided by its magnitude). After that, I proceeded evaluating the partial derivatives (that's the tricky part). Using the implicit differentiation theorem and substituting the point $(0,1)$ (when $x=0$ and $y=1$ then $z = 1$) \begin{align*} F(x,y,z) &= z+x\ln(z)+xe^{xy}-1\\ \\ F_x(x,y,z) &= \ln(z) + e^{xy}+xye^{xy} = 1\\ F_y(x,y,z) &= x^2 \cdot e^{xy} = 0\\ F_z(x,y,z) &= 1+\frac{x}{z} = 1\\ \\ \frac{\partial z}{\partial x} &= - \frac{F_x}{F_z} = -1\\ \frac{\partial z}{\partial y} &= - \frac{F_y}{F_z} =  0 \end{align*} So the directional derivative is given by $$D_{\vec{v}} = \langle -1 , 0 \rangle \left\langle \frac{4}{5} , \frac{3}{5} \right\rangle = - \frac{4}{5}$$ Is my solution correct? Thank you.",,"['multivariable-calculus', 'proof-verification', 'partial-derivative']"
70,How does $\iint_{\mathbf{ℝ^2}}\frac{x}{1+x^2+y^2}dxdy$ diverge?,How does  diverge?,\iint_{\mathbf{ℝ^2}}\frac{x}{1+x^2+y^2}dxdy,"I have a question regarding a improper double integral which will diverge but I cannot seem to understand how to reach that conclusion. The integral is the following: $$\iint_{\mathbf{ℝ^2}}\frac{x}{1+x^2+y^2}dxdy$$ I can see that $f(x,y) \geq 0 \space \forall x\geq0$ and $f(x,y)\leq 0 \space \forall x\leq 0$ and therefore I split $\mathbf{ℝ}$ into to parts such that: $ℝ=\Omega_1\cap \Omega_2 = \{(x,y)\in ℝ^2 \mid x\geq0\} \cap \{(x,y)\inℝ^2 \mid x\leq0\}$ and now we integrate over both $\Omega_1$ and $\Omega_2$. However, when I attempt this using polar coordinates I find that both the integrals have the final form of $ \space""0 \cdot\infty ""$. How can I conclude that the integral diverges from this? Or am I doing something completely wrong?","I have a question regarding a improper double integral which will diverge but I cannot seem to understand how to reach that conclusion. The integral is the following: $$\iint_{\mathbf{ℝ^2}}\frac{x}{1+x^2+y^2}dxdy$$ I can see that $f(x,y) \geq 0 \space \forall x\geq0$ and $f(x,y)\leq 0 \space \forall x\leq 0$ and therefore I split $\mathbf{ℝ}$ into to parts such that: $ℝ=\Omega_1\cap \Omega_2 = \{(x,y)\in ℝ^2 \mid x\geq0\} \cap \{(x,y)\inℝ^2 \mid x\leq0\}$ and now we integrate over both $\Omega_1$ and $\Omega_2$. However, when I attempt this using polar coordinates I find that both the integrals have the final form of $ \space""0 \cdot\infty ""$. How can I conclude that the integral diverges from this? Or am I doing something completely wrong?",,"['calculus', 'multivariable-calculus', 'improper-integrals']"
71,"Finding the partial derivatives of an integral given by $f(x,y) = \int_{y}^{x} \cos (t^2) \ dt$",Finding the partial derivatives of an integral given by,"f(x,y) = \int_{y}^{x} \cos (t^2) \ dt","I am asked to find the partial derivatives of an integral given by $f(x,y) = \int_{y}^{x} \cos (t^2) \ dt$. What I did was \begin{align} f(x,y) &= \int_{y}^{x} \cos (t^2) \ dt\\ \\ \frac{\partial f}{\partial x} &= \frac{\partial}{\partial x} \int_{y}^{x} \cos (t^2) \ dt = \cos (x^2)\\ \frac{\partial f}{\partial y} &= - \frac{\partial}{\partial y} \int_{x}^{y} \cos (t^2) \ dt = - \cos (y^2)\\ \end{align} It seems very similar to what we do on single variable calculus (Fundamental Theorem of Calculus part 1), that is, substituting the variable and aplying the chain rule (which in this case will lead to a multiplication by $1$ in both cases). Can someone confirm my answer?","I am asked to find the partial derivatives of an integral given by $f(x,y) = \int_{y}^{x} \cos (t^2) \ dt$. What I did was \begin{align} f(x,y) &= \int_{y}^{x} \cos (t^2) \ dt\\ \\ \frac{\partial f}{\partial x} &= \frac{\partial}{\partial x} \int_{y}^{x} \cos (t^2) \ dt = \cos (x^2)\\ \frac{\partial f}{\partial y} &= - \frac{\partial}{\partial y} \int_{x}^{y} \cos (t^2) \ dt = - \cos (y^2)\\ \end{align} It seems very similar to what we do on single variable calculus (Fundamental Theorem of Calculus part 1), that is, substituting the variable and aplying the chain rule (which in this case will lead to a multiplication by $1$ in both cases). Can someone confirm my answer?",,"['integration', 'multivariable-calculus']"
72,"$x^2+y^2+z^2 = 4, z \geq 4.$",,"x^2+y^2+z^2 = 4, z \geq 4.","Let $\mathbf{F}=3y \ \mathbf{i} -3xz \ \mathbf{j} + (x^2-y^2) \  \mathbf{k}.$ Compute the flux of the vectorfield    $\text{curl}(\mathbf{F})$ through the semi-sphere $x^2+y^2+z^2=4, \  z\geq 0$, by using direct parameterization of the surface and computation of $\text{curl}(\mathbf{F}).$ Denote the semisphere by $S$. In the book they start off by noting that $$\iint_S xy \ dS = \iint_S xz \ dS = \iint_S yz \ dS = 0 \quad (1)\\  \iint_S x^2 \ dS = \iint_S y^2 \ dS = \iint_S z^2 \ dS  \quad \quad \quad (2)\\ $$ They then proceed computing the curl of the field, which is $(2x-2y,-2x,-2z-3)$ and the normal of the surface, out from the surface is $\mathbf{N}=\frac{1}{2}(x,y,z).$ So $$\text{curl}(\mathbf{F})\cdot \mathbf{N} = \frac{1}{2}(2x^2-4xy-2z^2-3z).$$ Using the symmetries (1) and (2) we get the integral $$\frac{1}{2}\iint_S-3z \ dS.$$ They then proceed to compute $dS$ by arguing that: $S$ is a part of the implicitly defined surface $F(x,y,z)=4$, where $F(x,y,z)=x^2+y^2+z^2,$ so $$dS=\left|\frac{\nabla F}{F_z}\right|dxdy = \left|\frac{(2x,2y,2z)}{2z}\right| \ dxdy= \left|\frac{(x,y,z)}{z}\right| \ dxdy=\frac{2}{z} \ dxdy \quad (3).$$ Question: Can someone intuitively, and with other examples, explain the symmetry properties (1), (2) and why equation (3) holds and what it says?","Let $\mathbf{F}=3y \ \mathbf{i} -3xz \ \mathbf{j} + (x^2-y^2) \  \mathbf{k}.$ Compute the flux of the vectorfield    $\text{curl}(\mathbf{F})$ through the semi-sphere $x^2+y^2+z^2=4, \  z\geq 0$, by using direct parameterization of the surface and computation of $\text{curl}(\mathbf{F}).$ Denote the semisphere by $S$. In the book they start off by noting that $$\iint_S xy \ dS = \iint_S xz \ dS = \iint_S yz \ dS = 0 \quad (1)\\  \iint_S x^2 \ dS = \iint_S y^2 \ dS = \iint_S z^2 \ dS  \quad \quad \quad (2)\\ $$ They then proceed computing the curl of the field, which is $(2x-2y,-2x,-2z-3)$ and the normal of the surface, out from the surface is $\mathbf{N}=\frac{1}{2}(x,y,z).$ So $$\text{curl}(\mathbf{F})\cdot \mathbf{N} = \frac{1}{2}(2x^2-4xy-2z^2-3z).$$ Using the symmetries (1) and (2) we get the integral $$\frac{1}{2}\iint_S-3z \ dS.$$ They then proceed to compute $dS$ by arguing that: $S$ is a part of the implicitly defined surface $F(x,y,z)=4$, where $F(x,y,z)=x^2+y^2+z^2,$ so $$dS=\left|\frac{\nabla F}{F_z}\right|dxdy = \left|\frac{(2x,2y,2z)}{2z}\right| \ dxdy= \left|\frac{(x,y,z)}{z}\right| \ dxdy=\frac{2}{z} \ dxdy \quad (3).$$ Question: Can someone intuitively, and with other examples, explain the symmetry properties (1), (2) and why equation (3) holds and what it says?",,['multivariable-calculus']
73,Why are these two ways of evaluating the curl of a vector field not actually equivalent?,Why are these two ways of evaluating the curl of a vector field not actually equivalent?,,"I was asked to calculate the curl of the vector field $\theta^2\bf e_\theta$ in spherical coordinates . If we use the typical formula for curl in curvilinear coordinates: $$ \nabla\times \bf F = \left|\begin{matrix}h_1\hat{\bf e_1} & h_2\hat{\bf e_2} & h_3\hat{\bf e_3} \\ \displaystyle\frac{\partial}{\partial q_1} & \displaystyle\frac{\partial}{\partial q_2} & \displaystyle\frac{\partial}{\partial q_3} \\ h_1F_1 & h_2F_2 & h_3F_3\end{matrix}\right| $$ where, in spherical coordinates, $(q_1,q_2,q_3) = (r,\theta,\phi)$, and $h_1 = 1$, $h_2 = r$, $h_3 = r\sin\theta$, then we get $$ \nabla\times(\theta^2{\bf e}_\theta) = r\theta^2\sin\theta\bf e_\phi $$ I wanted to check this, so I thought I'd try converting the original vector field into Cartesian coordinates, taking the curl in Cartesian coordinates, and converting back. Using  $$ \theta = \arctan\left({\frac{\sqrt{x^2+y^2}}{z}}\right) \\ {\bf e_\theta} = \frac{(x\hat{\bf i} + y\hat{\bf j})z - (x^2+y^2)\hat{\bf k}}{\sqrt{x^2+y^2+z^2}\sqrt{x^2+y^2}} $$ we get $$ \theta^2{\bf e}_\theta = \frac{\arctan^2\left(\frac{\sqrt{x^2+y^2}}{z}\right)}{\sqrt{x^2+y^2}\sqrt{x^2+y^2+z^2}}\left(xz,yz,-(x^2+y^2)\right) $$ After some tedious calculation, this (somewhat remarkably) comes out to  $$ \nabla\times{\bf F} = \frac{\arctan^2\left(\frac{\sqrt{x^2+y^2}}{z}\right)}{\sqrt{x^2+y^2}\sqrt{x^2+y^2+z^2}}(-y{\bf\hat{i}}+x{\bf\hat{j}}) $$ This takes some work to convert back to polar coordinates, but without converting the unit vectors back immediately we have $$ \nabla\times{\bf F} = \frac{\theta^2}{r}(-\sin\phi{\bf\hat{i}} + \cos\phi{\bf\hat{j}}) $$ After substituting $$ {\bf\hat{i}} = \sin\theta\cos\phi{\bf\hat{r}}+\cos\theta\cos\phi\hat{\theta}-\sin\phi\hat{\phi} \\ {\bf\hat{j}} = \sin\theta\sin\phi{\bf\hat{r}} + \cos\theta\sin\phi\hat{\theta} + \cos\phi\hat{\phi} $$ into the above formula, we get $$ \frac{\theta^2}r{\bf e}_\theta $$ which is... not the same. So clearly, my intuition is wrong, or my assumption that these two ""curls"" in different coordinate systems are the same (since, assuming I didn't make any mistakes in my derivation, this would imply that ""spherical curl"" is somehow fundamentally different from ""Cartesian curl""), or I made a mistake somewhere. Or something else. Which is it? Can someone guide me in the right direction please?","I was asked to calculate the curl of the vector field $\theta^2\bf e_\theta$ in spherical coordinates . If we use the typical formula for curl in curvilinear coordinates: $$ \nabla\times \bf F = \left|\begin{matrix}h_1\hat{\bf e_1} & h_2\hat{\bf e_2} & h_3\hat{\bf e_3} \\ \displaystyle\frac{\partial}{\partial q_1} & \displaystyle\frac{\partial}{\partial q_2} & \displaystyle\frac{\partial}{\partial q_3} \\ h_1F_1 & h_2F_2 & h_3F_3\end{matrix}\right| $$ where, in spherical coordinates, $(q_1,q_2,q_3) = (r,\theta,\phi)$, and $h_1 = 1$, $h_2 = r$, $h_3 = r\sin\theta$, then we get $$ \nabla\times(\theta^2{\bf e}_\theta) = r\theta^2\sin\theta\bf e_\phi $$ I wanted to check this, so I thought I'd try converting the original vector field into Cartesian coordinates, taking the curl in Cartesian coordinates, and converting back. Using  $$ \theta = \arctan\left({\frac{\sqrt{x^2+y^2}}{z}}\right) \\ {\bf e_\theta} = \frac{(x\hat{\bf i} + y\hat{\bf j})z - (x^2+y^2)\hat{\bf k}}{\sqrt{x^2+y^2+z^2}\sqrt{x^2+y^2}} $$ we get $$ \theta^2{\bf e}_\theta = \frac{\arctan^2\left(\frac{\sqrt{x^2+y^2}}{z}\right)}{\sqrt{x^2+y^2}\sqrt{x^2+y^2+z^2}}\left(xz,yz,-(x^2+y^2)\right) $$ After some tedious calculation, this (somewhat remarkably) comes out to  $$ \nabla\times{\bf F} = \frac{\arctan^2\left(\frac{\sqrt{x^2+y^2}}{z}\right)}{\sqrt{x^2+y^2}\sqrt{x^2+y^2+z^2}}(-y{\bf\hat{i}}+x{\bf\hat{j}}) $$ This takes some work to convert back to polar coordinates, but without converting the unit vectors back immediately we have $$ \nabla\times{\bf F} = \frac{\theta^2}{r}(-\sin\phi{\bf\hat{i}} + \cos\phi{\bf\hat{j}}) $$ After substituting $$ {\bf\hat{i}} = \sin\theta\cos\phi{\bf\hat{r}}+\cos\theta\cos\phi\hat{\theta}-\sin\phi\hat{\phi} \\ {\bf\hat{j}} = \sin\theta\sin\phi{\bf\hat{r}} + \cos\theta\sin\phi\hat{\theta} + \cos\phi\hat{\phi} $$ into the above formula, we get $$ \frac{\theta^2}r{\bf e}_\theta $$ which is... not the same. So clearly, my intuition is wrong, or my assumption that these two ""curls"" in different coordinate systems are the same (since, assuming I didn't make any mistakes in my derivation, this would imply that ""spherical curl"" is somehow fundamentally different from ""Cartesian curl""), or I made a mistake somewhere. Or something else. Which is it? Can someone guide me in the right direction please?",,"['multivariable-calculus', 'curvilinear-coordinates']"
74,How do I prove this identity involving polar coordinates and $\nabla$? [duplicate],How do I prove this identity involving polar coordinates and ? [duplicate],\nabla,"This question already has answers here : How to obtain the gradient in polar coordinates (2 answers) Closed 6 years ago . Given that $$\begin{aligned} &x=r\cos(\theta),\\ &y=r\sin(\theta),\\ & \qquad \text{and}\\ &x^2+y^2=r^2 \end{aligned}$$ Use the chain rule to show that $\nabla=\mathbf{\hat{r}}\frac\partial{\partial r}+\mathbf{\hat{\theta}}\frac1r\frac\partial{\partial\theta}.$ I derived that $\,\mathbf{\hat{r}}=\cos(\theta)\mathbf{\hat{\text{i}}}+\sin(\theta)\mathbf{\hat{\text{j}}}\,$ and that  $\,\mathbf{\hat{\theta}}=-\sin(\theta)\mathbf{\hat{\text{i}}}+\cos(\theta)\mathbf{\hat{\text{j}}}.$ But I can't seem to gro from here.","This question already has answers here : How to obtain the gradient in polar coordinates (2 answers) Closed 6 years ago . Given that $$\begin{aligned} &x=r\cos(\theta),\\ &y=r\sin(\theta),\\ & \qquad \text{and}\\ &x^2+y^2=r^2 \end{aligned}$$ Use the chain rule to show that $\nabla=\mathbf{\hat{r}}\frac\partial{\partial r}+\mathbf{\hat{\theta}}\frac1r\frac\partial{\partial\theta}.$ I derived that $\,\mathbf{\hat{r}}=\cos(\theta)\mathbf{\hat{\text{i}}}+\sin(\theta)\mathbf{\hat{\text{j}}}\,$ and that  $\,\mathbf{\hat{\theta}}=-\sin(\theta)\mathbf{\hat{\text{i}}}+\cos(\theta)\mathbf{\hat{\text{j}}}.$ But I can't seem to gro from here.",,"['multivariable-calculus', 'derivatives', 'polar-coordinates', 'chain-rule']"
75,Multivariable chain rule and the behaviour of derivatives,Multivariable chain rule and the behaviour of derivatives,,"Suppose we have a function f (x , y) where x and y are functions of u and v i.e. x (u,v) and y (u,v). Then, from the chain rule, the following relationship holds $$\frac{\partial f}{\partial u} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial u} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial u}$$ However, on the RHS of the equation, can the two $\partial x$ and $\partial y$ not cancel like a fraction, which would then give $\frac{\partial f}{\partial u} = 2\frac{\partial f}{\partial u}$, which is a contradiction? In fact , when is it that the derivatives can be cancelled out like in a fraction, and when can they not? Thank you for your help.","Suppose we have a function f (x , y) where x and y are functions of u and v i.e. x (u,v) and y (u,v). Then, from the chain rule, the following relationship holds $$\frac{\partial f}{\partial u} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial u} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial u}$$ However, on the RHS of the equation, can the two $\partial x$ and $\partial y$ not cancel like a fraction, which would then give $\frac{\partial f}{\partial u} = 2\frac{\partial f}{\partial u}$, which is a contradiction? In fact , when is it that the derivatives can be cancelled out like in a fraction, and when can they not? Thank you for your help.",,"['multivariable-calculus', 'partial-derivative', 'chain-rule']"
76,Is the gradiant a column or a row? [duplicate],Is the gradiant a column or a row? [duplicate],,This question already has answers here : The gradient as a row versus column vector (2 answers) Closed 6 years ago . Suppose we have $f:\mathbb{R^2}\rightarrow \mathbb{R}$. Vectors which $f$ act on are column vectors i.e a $2 \times 1$ matrix. Is the gradiant $\nabla f$ then a row vector? And why is this logical?,This question already has answers here : The gradient as a row versus column vector (2 answers) Closed 6 years ago . Suppose we have $f:\mathbb{R^2}\rightarrow \mathbb{R}$. Vectors which $f$ act on are column vectors i.e a $2 \times 1$ matrix. Is the gradiant $\nabla f$ then a row vector? And why is this logical?,,[]
77,"Multivariate Taylor theorem with small o notation (Not continuous, only differentiable)","Multivariate Taylor theorem with small o notation (Not continuous, only differentiable)",,"I want to know whether the following is true. Let $n\in \mathbb{Z}$, $n\geq 2$, and $f:\mathbb{R}^n \rightarrow \mathbb{R}$ be $k$ times differentiable at a point $p\in\mathbb{R}^n$. Then $f(p+v)=f(p)+Df(p)(v)+\frac{1}{2!}D^2f(p)v^{(2)}+\cdots+\frac{1}{k!}D^kf(p)v^{(k)}+o(||v||^k)$ When $n=1$, I know the above statement is true, and it can be proved using L'Hospital's theorem or other methods( Taylor's Theorem with Peano's Form of Remainder ). However, when $n\geq 2$, I have tried to prove but I cannot prove the above statement. Note that the above condition of the function $f$ is NOT $k$ times continuously differentiable, ONLY $k$ times differentiable. (It can be easily proved when $f$ is $\mathcal{C}^k$, then $f(p+v)=f(p)+Df(p)(v)+\frac{1}{2!}D^2f(p)v^{(2)}+\cdots+\frac{1}{k!}D^kf(p^*)v^{(k)}$ for some $p^*$, where $p^*$ is on the line segment between $p$ and $p+v$, therefore $f(p+v)-f(p)-Df(p)(v)-\frac{1}{2!}D^2f(p)v^{(2)}-\cdots-\frac{1}{k!}D^kf(p)v^{(k)}=\frac{1}{k!}\{D^kf(p^*)-D^kf(p)\}v^{(k)}$ , and the continuity of $D^kf$ now implies $o(||v||^k)$) Without the continuity of $D^kf$, I guess the above statement is false, but I cannot find any counterexamples. Can someone have some idea and give me any hints? (c.f. $D^mf:\mathbb{R}^n \rightarrow \mathcal{L}^m(\mathbb{R}^n,\mathbb{R})$ and $\mathcal{L}^m(\mathbb{R}^n,\mathbb{R})$ is the set of $m$-multilinear maps of $\mathbb{R}^n$ to $\mathbb{R}$, so $D^mf(p)v^{(m)}:=D^mf(p)(v,v,\cdots,v).$)","I want to know whether the following is true. Let $n\in \mathbb{Z}$, $n\geq 2$, and $f:\mathbb{R}^n \rightarrow \mathbb{R}$ be $k$ times differentiable at a point $p\in\mathbb{R}^n$. Then $f(p+v)=f(p)+Df(p)(v)+\frac{1}{2!}D^2f(p)v^{(2)}+\cdots+\frac{1}{k!}D^kf(p)v^{(k)}+o(||v||^k)$ When $n=1$, I know the above statement is true, and it can be proved using L'Hospital's theorem or other methods( Taylor's Theorem with Peano's Form of Remainder ). However, when $n\geq 2$, I have tried to prove but I cannot prove the above statement. Note that the above condition of the function $f$ is NOT $k$ times continuously differentiable, ONLY $k$ times differentiable. (It can be easily proved when $f$ is $\mathcal{C}^k$, then $f(p+v)=f(p)+Df(p)(v)+\frac{1}{2!}D^2f(p)v^{(2)}+\cdots+\frac{1}{k!}D^kf(p^*)v^{(k)}$ for some $p^*$, where $p^*$ is on the line segment between $p$ and $p+v$, therefore $f(p+v)-f(p)-Df(p)(v)-\frac{1}{2!}D^2f(p)v^{(2)}-\cdots-\frac{1}{k!}D^kf(p)v^{(k)}=\frac{1}{k!}\{D^kf(p^*)-D^kf(p)\}v^{(k)}$ , and the continuity of $D^kf$ now implies $o(||v||^k)$) Without the continuity of $D^kf$, I guess the above statement is false, but I cannot find any counterexamples. Can someone have some idea and give me any hints? (c.f. $D^mf:\mathbb{R}^n \rightarrow \mathcal{L}^m(\mathbb{R}^n,\mathbb{R})$ and $\mathcal{L}^m(\mathbb{R}^n,\mathbb{R})$ is the set of $m$-multilinear maps of $\mathbb{R}^n$ to $\mathbb{R}$, so $D^mf(p)v^{(m)}:=D^mf(p)(v,v,\cdots,v).$)",,"['multivariable-calculus', 'taylor-expansion']"
78,Generalization of chain rule to tensors,Generalization of chain rule to tensors,,"Are there any generalization of chain rule of differentiation to tensors? For example, how can I differentiate f(g(X)) where: $g: Matrix(d_1, d_2) \to Matrix(d_3, d_4)$ and $f: Matrix(d_3, d_4) \to Matrix (d_5, d_6)$. P.S. I understand how to compute derivative, I want a rule which takes a tensor derivative of f and a tensor derivative of g and combines them in a short step.","Are there any generalization of chain rule of differentiation to tensors? For example, how can I differentiate f(g(X)) where: $g: Matrix(d_1, d_2) \to Matrix(d_3, d_4)$ and $f: Matrix(d_3, d_4) \to Matrix (d_5, d_6)$. P.S. I understand how to compute derivative, I want a rule which takes a tensor derivative of f and a tensor derivative of g and combines them in a short step.",,"['matrices', 'multivariable-calculus', 'tensors']"
79,"Why does a surface F(x,y,z) that never fold back on itself have $\nabla F$ $\cdot \vec{p} \neq 0$?","Why does a surface F(x,y,z) that never fold back on itself have  ?",\nabla F \cdot \vec{p} \neq 0,"Why does an implicit surface F(x,y,z) that never fold back on itself have $\nabla F$ $\cdot \vec{p} \neq 0$?","Why does an implicit surface F(x,y,z) that never fold back on itself have $\nabla F$ $\cdot \vec{p} \neq 0$?",,"['multivariable-calculus', 'differential-geometry']"
80,Visualizing Multivariable Functions,Visualizing Multivariable Functions,,The 2D Circle eliminates option B. The next two 2D graphs seem to work for both option A and C... How can I be 100% sure about which one it is?,The 2D Circle eliminates option B. The next two 2D graphs seem to work for both option A and C... How can I be 100% sure about which one it is?,,['multivariable-calculus']
81,"Derivative of $f(x) = |x|^2 - 2\langle a, x \rangle$",Derivative of,"f(x) = |x|^2 - 2\langle a, x \rangle","Define the function $f\colon \mathbb{R}^n \to \mathbb{R}$ by $f(x) = |x|^2 - 2\langle a , x \rangle$ where $a \in \mathbb{R}^n$ is non-zero. We have: $$f(x + h) - f(x) = |x+h|^2 - 2\langle a, x + h\rangle - |x|^2 + 2\langle a, x\rangle \\ = |h|^2 - 2\langle x, h\rangle - 2\langle a, h\rangle$$ And thus we have $Df(x)(h) = -2\langle x, h\rangle -2\langle a, h\rangle$. The only critical point is $x = -a$, and given that $D^2f(x)(h, h) = -2|h|^2 < 0$, this critical point is a maxima. But now let's look at the particular case where $n = 2$ and $a = (1, 0)$. We have $f(x, y) = x^2 + y^2 - 2x$. It's derivative is given by: $$f'(x, y) = \begin{pmatrix}2x - 2 & 2y \end{pmatrix}$$ This has a critical point at $x = 1$ and $y = 0$. Also, we have: $$f''(x, y) = \begin{pmatrix}2 & 0 \\ 0 & 2 \end{pmatrix}$$ This matrix is positive definite, so the critical point is actually a minima. What is wrong with the general case?","Define the function $f\colon \mathbb{R}^n \to \mathbb{R}$ by $f(x) = |x|^2 - 2\langle a , x \rangle$ where $a \in \mathbb{R}^n$ is non-zero. We have: $$f(x + h) - f(x) = |x+h|^2 - 2\langle a, x + h\rangle - |x|^2 + 2\langle a, x\rangle \\ = |h|^2 - 2\langle x, h\rangle - 2\langle a, h\rangle$$ And thus we have $Df(x)(h) = -2\langle x, h\rangle -2\langle a, h\rangle$. The only critical point is $x = -a$, and given that $D^2f(x)(h, h) = -2|h|^2 < 0$, this critical point is a maxima. But now let's look at the particular case where $n = 2$ and $a = (1, 0)$. We have $f(x, y) = x^2 + y^2 - 2x$. It's derivative is given by: $$f'(x, y) = \begin{pmatrix}2x - 2 & 2y \end{pmatrix}$$ This has a critical point at $x = 1$ and $y = 0$. Also, we have: $$f''(x, y) = \begin{pmatrix}2 & 0 \\ 0 & 2 \end{pmatrix}$$ This matrix is positive definite, so the critical point is actually a minima. What is wrong with the general case?",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'inner-products', 'maxima-minima']"
82,Error in Edwards's arclength proof?,Error in Edwards's arclength proof?,,"This question applies to Edwards's Advanced Calculus of Several Variables, the proof of theorem V-1.1, page 288, etc. If $\vec{\gamma}:\left[t_0,t_{L}\right]\to\mathbb{R}^n$ is   a $\mathscr{C}^1$ path, then $\mathscr{s}\left[\vec{\gamma}\right]$   exists, and $\mathscr{s}\left[\vec{\gamma}\right]=\int_{t_0}^{t_L}\left|\vec{\gamma}' [t]\right| \, dt$. I did this for $\mathbb{R}^{3}$. But replacing $3$ with $n$ will generalize my demonstration. It was not my intent to ask if Edwards was correct when I started writing this up, so I did it very much my way. It was only when I happened upon his claim that $\left|\mathscr{P}\right|<\delta_1$ was a sufficient restriction to satisfy $$\left|\mathfrak{t}_a-\mathfrak{t}_b\right|<\delta_1\implies\left|{\overset{*}{\gamma}}' \left[\mathfrak{t}_a\right]-{\overset{*}{\gamma}}' \left[\mathfrak{t}_b\right]\right|<\frac{\varepsilon}{2(t_L-t_0)},$$ that I came to believe Edwards was wrong about that. If someone has a copy of Edwards's text, please have a look to see if I am reading it correctly; and he was indeed mistaken. I contended he needs $\left|\mathscr{P}\right|<\frac{\delta_1}{\sqrt{n}}$, instead. In either case, could someone please verify my proof? I acknowledge it is a bit terse, and done using my own inventions. Let $\vec{\gamma}:[t_0,t_L]\to\mathbb{R}^3$ be a smooth path in $\mathbb{R}^3$. To find the arclength: Introduce the partition $\mathscr{P}=\left\{ t_0,\dots,<t_i,\dots,<t_k =t_L \right\} $ of the interval $\left[t_{0},t_{L}\right]$. Write $\Delta t_i=t_i-t_{i-1}$. Define the mesh of $\mathscr{P}$ as $\left|\mathscr{P}\right| \equiv \max[\Delta t_i]$. Write $\Delta\vec{\gamma}_{i}=\vec{\gamma}\left[t_{i}\right]-\vec{\gamma}\left[t_{i-1}\right]$. Define $ \overset{*}{\mathfrak{t}}_i\in\left[t_{i-1},t_i\right]^{3}$ as $\overset{*}{\mathfrak{t}}_i=\left\{ \overset{*}{t_i^1},\overset{*}{t_i^2}, \overset{*}{t_i^3}\right\}$ such that $\Delta\vec{\gamma}_i=\left\{ {\gamma^1}' \left[\overset{*}{t_i^1}\right],{\gamma^2}'\left[\overset{*}{t_i^2}\right],{\gamma^3}' \left[\overset{*}{t_i^3}\right]\right\} \Delta t_i$. Define $\overset{*}{\gamma}':\left[t_0,t_L\right]^3\to\mathbb{R}^3$ as $\overset{*}{\gamma}'\left[\mathfrak{t}\right]=\left\{ {\gamma^1}' [t^1], {\gamma^2}' [t^2], {\gamma^3}' [t^3] \right\} $. Define $$\mathscr{s}\left[\vec{\gamma},\mathscr{P}\right]=\sum_{i=1}^k\left|\overset{*}{\gamma}'\left[\overset{*}{\mathfrak{t}_i}\right]\right|\Delta t_i=\sum_{i=1}^k\left|\Delta\vec{\gamma}_i\right|$$ which is not a proper Riemann sum. Define $$\mathscr{R}\left[\vec{\gamma},\mathscr{P}\right]=\sum_{i=1}^k\left|\vec{\gamma}'\left[t_i\right]\right|\Delta t_i \approx \sum_{i=1}^k \left|\Delta\vec{\gamma}_i\right|,$$ which is a proper Riemann sum. Define $\mathfrak{t}_i\in\left[t_{i-1},t_i\right]^3$ as $\mathfrak{t}_i=\left\{ t_i,t_i,t_i\right\} $. So ${\overset{*}{\gamma}}' \left[\mathfrak{t}_i\right] = \vec{\gamma}'[t_i]$. Because $\vec{\gamma}$ is $\mathscr{C}^1$, the first derivatives of the component functions ${\gamma^1}',{\gamma^2}',{\gamma^3}'$ are uniformly continuous on the closed and bounded interval $[t_0,t_L]^3$. This means that given an arbitrarily small $\varepsilon>0$ there is a $\delta_1>0$ such that for $\mathfrak{t}_a, \mathfrak{t}_b \in [t_0,t_L]^3$ $$\left|\mathfrak{t}_a-\mathfrak{t}_b\right|<\delta_1 \implies \left|\overset{*}{\gamma}' \left[\mathfrak{t}_a \right]-\overset{*}{\gamma}'\left[\mathfrak{t}_b\right]\right|<\frac{\varepsilon}{2(t_L-t_0)}.$$ Applying the triangle inequality: $$\left|\left|\overset{*}{\gamma}'\left[\mathfrak{t}_{a}\right]\right|-\left|\overset{*}{\gamma}' \left[\mathfrak{t}_b\right]\right|\right| \le \left|\overset{*}{\gamma}' \left[\mathfrak{t}_a\right]-\overset{*}{\gamma}' \left[\mathfrak{t}_b\right] \right|.$$ So $$\left|\mathfrak{t}_a-\mathfrak{t}_b\right|<\delta_1 \implies \left|\left|\overset{*}{\gamma}' \left[\mathfrak{t}_a\right]\right|-\left|\overset{*}{\gamma}' \left[\mathfrak{t}_b\right]\right|\right| < \frac{\varepsilon}{2(t_L-t_0)}.$$ Taking the difference of the polygonal approximation and the Riemann sum gives $$\left|\mathscr{s}\left[\vec{\gamma},\mathscr{P}\right]-\mathscr{R}\left[\vec{\gamma},\mathscr{P}\right]\right|=\left|\sum_{i=1}^k \left(\left|\overset{*}{\gamma}'\left[\overset{*}{\mathfrak{t}_i} \right]\right| -\left|\vec{\gamma}' [t_i]\right|\right) \Delta t_i\right|$$ $$\le\sum_{i=1}^k\left|\left|\overset{*}{\gamma}'\left[\overset{*}{\mathfrak{t}_i}\right]\right|-\left|\overset{*}{\gamma}'\left[\mathfrak{t}_i\right]\right|\right|\Delta t_i.$$ If the mesh of the partition is restricted by $\left|\mathscr{P}\right|<\delta$, since $\mathfrak{t}_i,\overset{*}{\mathfrak{t}_i}\in\left[t_{i-1},t_i\right]^3$ it follows that $$\left|\mathfrak{t}_i-\overset{*}{\mathfrak{t}_i} \right| \le \delta \left|\left\{ 1,1,1\right\} \right|=\delta\sqrt{3}.$$ So, for $\left|\mathfrak{t}_{i}-\overset{*}{\mathfrak{t}_{i}}\right|<\delta_1$ to hold, the mesh shall be restricted to $\left|\mathscr{P}\right|<\frac{\delta_1}{\sqrt{3}}$. This means that for each term $$\left|\left|\overset{*}{\gamma}'\left[\overset{*}{\mathfrak{t}_i} \right]\right| -\left|\overset{*}{\gamma}' \left[\mathfrak{t}_i\right] \right| \right| < \frac{\varepsilon}{2(t_L-t_0)}.$$ So $$\left|\mathscr{s}\left[\vec{\gamma},\mathscr{P}\right]-\mathscr{R}\left[\vec{\gamma},\mathscr{P}\right]\right|< \sum_{i=1}^k \frac \varepsilon {2(t_L-t_0)}\Delta t_i = \frac \varepsilon 2.$$ The function $\left|\vec{\gamma}' \left[t\right]\right|$ is continuous on $\left[t_{0},t_{L}\right]$ and is, therefore, bounded. Since it can be extended beyond $\left[t_{0},t_{L}\right]$ by specifying $t\notin\left[t_{0},t_{L}\right]\implies\left|\vec{\gamma}'\left[t\right]\right|=0$, it has bounded support. It is, therefore, integrable so that, given $\frac{\varepsilon}{2}$ there exists $\delta_2>0$ such that $$\delta_2>\left|\mathscr{P}\right|\implies\left|\mathscr{R}\left[\vec{\gamma},\mathscr{P}\right]-\int_{t_{0}}^{t_{1}}\left|\vec{\gamma}' [t] \right| \,dt\right| <\frac \varepsilon 2.$$ Taking $\delta=\min\left[\delta_1,\delta_2\right]$ implies $$\left|\mathscr{s}\left[\vec{\gamma},\mathscr{P}\right]-\mathscr{R} \left[\vec{\gamma},\mathscr{P}\right]\right| + \left|\mathscr{R} \left[\vec{\gamma},\mathscr{P}\right]-\int_{t_0}^{t_L} \left|\vec{\gamma}' [t] \, \right| \, dt\right|<\varepsilon.$$ A sanity check $\left|a-b\right|=\left|\left(a-c\right)+\left(c-b\right)\right|\le\left|\left(a-c\right)\right|+\left|\left(c-b\right)\right|$ leads to $$\delta>\left|\mathscr{P}\right|\implies\left|\mathscr{s}\left[\vec{\gamma},\mathscr{P}\right]-\int_{t_0}^{t_L}\left| \vec{\gamma}' [t]\right|\,dt\right|<\varepsilon.$$ That is $$\lim_{\left|\mathscr{P}\right|\to0}\mathscr{s}\left[\vec{\gamma},\mathscr{P} \right]=\lim_{\left|\mathscr{P}\right|\to0}\mathscr{R}\left[\vec{\gamma},\mathscr{P}\right]\equiv\int_{t_0}^{t_L}\left|\vec{\gamma}' [t]\right| \, dt$$ Also written $$\int_{t_0}^{t_L}\left|\frac{d\vec{\gamma}}{dt}[t] \right| \,dt = \int_{t_0}^{t_L}\left|\vec{\gamma}'[t]\right| \, dt.$$ Verification of the triangle inequality application: $$\left|a-b\right| = \left|a+c\right| \le \left|a\right| + \left|c\right| = \left|a\right| + \left|b\right|$$ $A=a+b$; $B=a-b$ $\left|A+B\right|\le\left|A\right|+\left|B\right|$ So $$2\left|a\right|\le\left|a+b\right|+\left|a-b\right|\le\left|a-b\right|+\left|a\right|+\left|b\right|$$ $$\left|a\right|\le\left|a-b\right|+\left|b\right|$$ $$\left|a\right|-\left|b\right|\le\left|a-b\right|$$ Let $C=-B$. $$\left|A-B\right|=\left|A+C\right|\le\left|A\right|+\left|C\right|$$ $$\left|A-B\right|\le\left|A\right|+\left|B\right|$$ $$2\left|b\right|\le\left|a-b\right|+\left|a\right|+\left|b\right|$$ $$\left|b\right|-\left|a\right|\le\left|a-b\right|$$ $$\left|\left|a\right|-\left|b\right|\right|\le\left|a-b\right|$$","This question applies to Edwards's Advanced Calculus of Several Variables, the proof of theorem V-1.1, page 288, etc. If $\vec{\gamma}:\left[t_0,t_{L}\right]\to\mathbb{R}^n$ is   a $\mathscr{C}^1$ path, then $\mathscr{s}\left[\vec{\gamma}\right]$   exists, and $\mathscr{s}\left[\vec{\gamma}\right]=\int_{t_0}^{t_L}\left|\vec{\gamma}' [t]\right| \, dt$. I did this for $\mathbb{R}^{3}$. But replacing $3$ with $n$ will generalize my demonstration. It was not my intent to ask if Edwards was correct when I started writing this up, so I did it very much my way. It was only when I happened upon his claim that $\left|\mathscr{P}\right|<\delta_1$ was a sufficient restriction to satisfy $$\left|\mathfrak{t}_a-\mathfrak{t}_b\right|<\delta_1\implies\left|{\overset{*}{\gamma}}' \left[\mathfrak{t}_a\right]-{\overset{*}{\gamma}}' \left[\mathfrak{t}_b\right]\right|<\frac{\varepsilon}{2(t_L-t_0)},$$ that I came to believe Edwards was wrong about that. If someone has a copy of Edwards's text, please have a look to see if I am reading it correctly; and he was indeed mistaken. I contended he needs $\left|\mathscr{P}\right|<\frac{\delta_1}{\sqrt{n}}$, instead. In either case, could someone please verify my proof? I acknowledge it is a bit terse, and done using my own inventions. Let $\vec{\gamma}:[t_0,t_L]\to\mathbb{R}^3$ be a smooth path in $\mathbb{R}^3$. To find the arclength: Introduce the partition $\mathscr{P}=\left\{ t_0,\dots,<t_i,\dots,<t_k =t_L \right\} $ of the interval $\left[t_{0},t_{L}\right]$. Write $\Delta t_i=t_i-t_{i-1}$. Define the mesh of $\mathscr{P}$ as $\left|\mathscr{P}\right| \equiv \max[\Delta t_i]$. Write $\Delta\vec{\gamma}_{i}=\vec{\gamma}\left[t_{i}\right]-\vec{\gamma}\left[t_{i-1}\right]$. Define $ \overset{*}{\mathfrak{t}}_i\in\left[t_{i-1},t_i\right]^{3}$ as $\overset{*}{\mathfrak{t}}_i=\left\{ \overset{*}{t_i^1},\overset{*}{t_i^2}, \overset{*}{t_i^3}\right\}$ such that $\Delta\vec{\gamma}_i=\left\{ {\gamma^1}' \left[\overset{*}{t_i^1}\right],{\gamma^2}'\left[\overset{*}{t_i^2}\right],{\gamma^3}' \left[\overset{*}{t_i^3}\right]\right\} \Delta t_i$. Define $\overset{*}{\gamma}':\left[t_0,t_L\right]^3\to\mathbb{R}^3$ as $\overset{*}{\gamma}'\left[\mathfrak{t}\right]=\left\{ {\gamma^1}' [t^1], {\gamma^2}' [t^2], {\gamma^3}' [t^3] \right\} $. Define $$\mathscr{s}\left[\vec{\gamma},\mathscr{P}\right]=\sum_{i=1}^k\left|\overset{*}{\gamma}'\left[\overset{*}{\mathfrak{t}_i}\right]\right|\Delta t_i=\sum_{i=1}^k\left|\Delta\vec{\gamma}_i\right|$$ which is not a proper Riemann sum. Define $$\mathscr{R}\left[\vec{\gamma},\mathscr{P}\right]=\sum_{i=1}^k\left|\vec{\gamma}'\left[t_i\right]\right|\Delta t_i \approx \sum_{i=1}^k \left|\Delta\vec{\gamma}_i\right|,$$ which is a proper Riemann sum. Define $\mathfrak{t}_i\in\left[t_{i-1},t_i\right]^3$ as $\mathfrak{t}_i=\left\{ t_i,t_i,t_i\right\} $. So ${\overset{*}{\gamma}}' \left[\mathfrak{t}_i\right] = \vec{\gamma}'[t_i]$. Because $\vec{\gamma}$ is $\mathscr{C}^1$, the first derivatives of the component functions ${\gamma^1}',{\gamma^2}',{\gamma^3}'$ are uniformly continuous on the closed and bounded interval $[t_0,t_L]^3$. This means that given an arbitrarily small $\varepsilon>0$ there is a $\delta_1>0$ such that for $\mathfrak{t}_a, \mathfrak{t}_b \in [t_0,t_L]^3$ $$\left|\mathfrak{t}_a-\mathfrak{t}_b\right|<\delta_1 \implies \left|\overset{*}{\gamma}' \left[\mathfrak{t}_a \right]-\overset{*}{\gamma}'\left[\mathfrak{t}_b\right]\right|<\frac{\varepsilon}{2(t_L-t_0)}.$$ Applying the triangle inequality: $$\left|\left|\overset{*}{\gamma}'\left[\mathfrak{t}_{a}\right]\right|-\left|\overset{*}{\gamma}' \left[\mathfrak{t}_b\right]\right|\right| \le \left|\overset{*}{\gamma}' \left[\mathfrak{t}_a\right]-\overset{*}{\gamma}' \left[\mathfrak{t}_b\right] \right|.$$ So $$\left|\mathfrak{t}_a-\mathfrak{t}_b\right|<\delta_1 \implies \left|\left|\overset{*}{\gamma}' \left[\mathfrak{t}_a\right]\right|-\left|\overset{*}{\gamma}' \left[\mathfrak{t}_b\right]\right|\right| < \frac{\varepsilon}{2(t_L-t_0)}.$$ Taking the difference of the polygonal approximation and the Riemann sum gives $$\left|\mathscr{s}\left[\vec{\gamma},\mathscr{P}\right]-\mathscr{R}\left[\vec{\gamma},\mathscr{P}\right]\right|=\left|\sum_{i=1}^k \left(\left|\overset{*}{\gamma}'\left[\overset{*}{\mathfrak{t}_i} \right]\right| -\left|\vec{\gamma}' [t_i]\right|\right) \Delta t_i\right|$$ $$\le\sum_{i=1}^k\left|\left|\overset{*}{\gamma}'\left[\overset{*}{\mathfrak{t}_i}\right]\right|-\left|\overset{*}{\gamma}'\left[\mathfrak{t}_i\right]\right|\right|\Delta t_i.$$ If the mesh of the partition is restricted by $\left|\mathscr{P}\right|<\delta$, since $\mathfrak{t}_i,\overset{*}{\mathfrak{t}_i}\in\left[t_{i-1},t_i\right]^3$ it follows that $$\left|\mathfrak{t}_i-\overset{*}{\mathfrak{t}_i} \right| \le \delta \left|\left\{ 1,1,1\right\} \right|=\delta\sqrt{3}.$$ So, for $\left|\mathfrak{t}_{i}-\overset{*}{\mathfrak{t}_{i}}\right|<\delta_1$ to hold, the mesh shall be restricted to $\left|\mathscr{P}\right|<\frac{\delta_1}{\sqrt{3}}$. This means that for each term $$\left|\left|\overset{*}{\gamma}'\left[\overset{*}{\mathfrak{t}_i} \right]\right| -\left|\overset{*}{\gamma}' \left[\mathfrak{t}_i\right] \right| \right| < \frac{\varepsilon}{2(t_L-t_0)}.$$ So $$\left|\mathscr{s}\left[\vec{\gamma},\mathscr{P}\right]-\mathscr{R}\left[\vec{\gamma},\mathscr{P}\right]\right|< \sum_{i=1}^k \frac \varepsilon {2(t_L-t_0)}\Delta t_i = \frac \varepsilon 2.$$ The function $\left|\vec{\gamma}' \left[t\right]\right|$ is continuous on $\left[t_{0},t_{L}\right]$ and is, therefore, bounded. Since it can be extended beyond $\left[t_{0},t_{L}\right]$ by specifying $t\notin\left[t_{0},t_{L}\right]\implies\left|\vec{\gamma}'\left[t\right]\right|=0$, it has bounded support. It is, therefore, integrable so that, given $\frac{\varepsilon}{2}$ there exists $\delta_2>0$ such that $$\delta_2>\left|\mathscr{P}\right|\implies\left|\mathscr{R}\left[\vec{\gamma},\mathscr{P}\right]-\int_{t_{0}}^{t_{1}}\left|\vec{\gamma}' [t] \right| \,dt\right| <\frac \varepsilon 2.$$ Taking $\delta=\min\left[\delta_1,\delta_2\right]$ implies $$\left|\mathscr{s}\left[\vec{\gamma},\mathscr{P}\right]-\mathscr{R} \left[\vec{\gamma},\mathscr{P}\right]\right| + \left|\mathscr{R} \left[\vec{\gamma},\mathscr{P}\right]-\int_{t_0}^{t_L} \left|\vec{\gamma}' [t] \, \right| \, dt\right|<\varepsilon.$$ A sanity check $\left|a-b\right|=\left|\left(a-c\right)+\left(c-b\right)\right|\le\left|\left(a-c\right)\right|+\left|\left(c-b\right)\right|$ leads to $$\delta>\left|\mathscr{P}\right|\implies\left|\mathscr{s}\left[\vec{\gamma},\mathscr{P}\right]-\int_{t_0}^{t_L}\left| \vec{\gamma}' [t]\right|\,dt\right|<\varepsilon.$$ That is $$\lim_{\left|\mathscr{P}\right|\to0}\mathscr{s}\left[\vec{\gamma},\mathscr{P} \right]=\lim_{\left|\mathscr{P}\right|\to0}\mathscr{R}\left[\vec{\gamma},\mathscr{P}\right]\equiv\int_{t_0}^{t_L}\left|\vec{\gamma}' [t]\right| \, dt$$ Also written $$\int_{t_0}^{t_L}\left|\frac{d\vec{\gamma}}{dt}[t] \right| \,dt = \int_{t_0}^{t_L}\left|\vec{\gamma}'[t]\right| \, dt.$$ Verification of the triangle inequality application: $$\left|a-b\right| = \left|a+c\right| \le \left|a\right| + \left|c\right| = \left|a\right| + \left|b\right|$$ $A=a+b$; $B=a-b$ $\left|A+B\right|\le\left|A\right|+\left|B\right|$ So $$2\left|a\right|\le\left|a+b\right|+\left|a-b\right|\le\left|a-b\right|+\left|a\right|+\left|b\right|$$ $$\left|a\right|\le\left|a-b\right|+\left|b\right|$$ $$\left|a\right|-\left|b\right|\le\left|a-b\right|$$ Let $C=-B$. $$\left|A-B\right|=\left|A+C\right|\le\left|A\right|+\left|C\right|$$ $$\left|A-B\right|\le\left|A\right|+\left|B\right|$$ $$2\left|b\right|\le\left|a-b\right|+\left|a\right|+\left|b\right|$$ $$\left|b\right|-\left|a\right|\le\left|a-b\right|$$ $$\left|\left|a\right|-\left|b\right|\right|\le\left|a-b\right|$$",,"['multivariable-calculus', 'proof-verification']"
83,Contradiction of gradient between direction of steepest increase and local minimum,Contradiction of gradient between direction of steepest increase and local minimum,,"I can understand the proof that at a local minimum, the gradient of that function must be zero. But I can't understand it together with the fact that gradient points the steepest increase, since if it is at local minimum, then moving at any direction will give the function some increment. So what does gradient being zero at this point means? If any direction increases this function, then why don't we just choose the direction with the greatest increment but choose zero which means not to move at all?","I can understand the proof that at a local minimum, the gradient of that function must be zero. But I can't understand it together with the fact that gradient points the steepest increase, since if it is at local minimum, then moving at any direction will give the function some increment. So what does gradient being zero at this point means? If any direction increases this function, then why don't we just choose the direction with the greatest increment but choose zero which means not to move at all?",,"['multivariable-calculus', 'vector-analysis', 'gradient-descent']"
84,Do we need differentiability in problem 2-35 in Spivak?,Do we need differentiability in problem 2-35 in Spivak?,,"In Calculus on Manifolds by Spivak, Problem 2-35 goes as follows: If $f: \mathbb{R}^n \to \mathbb{R}$ is differentiable and $f(0) =0$,   prove that there exist $g_i: \mathbb{R}^n \to \mathbb{R}$ such that   $f(x) = \sum_{i=1}^n x^ig_i(x)$. My question is: do we really need differentiability? Can we just let $$g_i(x)=\begin{cases} {f(x)}/\left({x^i\cdot\left|\{j|x^j\neq 0\}\right|}\right) & x^i\neq 0\\ 0 & x^i=0 \end{cases}$$ so that $$x^ig_i(x)=\begin{cases} {f(x)}/\left({\left|\{j|x^j\neq 0\}\right|}\right) & x^i\neq 0\\ 0 & x^i=0 \end{cases}$$ so $\sum_{i=1}^n x^ig_i(x)={f(x)}$?","In Calculus on Manifolds by Spivak, Problem 2-35 goes as follows: If $f: \mathbb{R}^n \to \mathbb{R}$ is differentiable and $f(0) =0$,   prove that there exist $g_i: \mathbb{R}^n \to \mathbb{R}$ such that   $f(x) = \sum_{i=1}^n x^ig_i(x)$. My question is: do we really need differentiability? Can we just let $$g_i(x)=\begin{cases} {f(x)}/\left({x^i\cdot\left|\{j|x^j\neq 0\}\right|}\right) & x^i\neq 0\\ 0 & x^i=0 \end{cases}$$ so that $$x^ig_i(x)=\begin{cases} {f(x)}/\left({\left|\{j|x^j\neq 0\}\right|}\right) & x^i\neq 0\\ 0 & x^i=0 \end{cases}$$ so $\sum_{i=1}^n x^ig_i(x)={f(x)}$?",,"['multivariable-calculus', 'differential-geometry', 'proof-verification']"
85,$C^1$ function $f$ with bounded gradient such that $\frac{f(x)}{1+|x|}$ is unbounded,function  with bounded gradient such that  is unbounded,C^1 f \frac{f(x)}{1+|x|},"Consider the following proposition: Proposition 1: Let $\Omega$ be an open, convex subset of $\mathbb R^2$ and let $f:\Omega \to \mathbb R$ be a $C^1$ function with bounded gradient. Then  $ \frac{f(x)}{1+|x|} $ is bounded. Proof of proposition 1: A $C^1$ function with bounded gradient over a convex domain is Lipschitz (by the Mean Value Theorem). Hence, $\forall x,y\in \Omega,$ $$|f(x)-f(y)|\leq M |x-y|,$$ where $M>0$ is the Lipschitz constant. If we fix an $y_0\in \Omega$ we have, $\forall x \in\Omega$ $$|f(x)|\leq |f(y_0)|+|f(x)-f(y_0)|\leq |f(y_0)|+ M|x-y_0|\leq |f(y_0)|+ M|x|+M|y_0|\leq C(1+|x|),$$ where $C=max\{|f(y_0)|+M|y_0|;M\}.$ Now consider another similar proposition: Proposition 2: Let $\Omega$ be an open, convex subset of $\mathbb R^2$ and let $f:\Omega \to \mathbb R$ be an uniformly continuous function. Then  $ \frac{f(x)}{1+|x|} $ is bounded. Proof of proposition 2: $f$ is an uniformly continuous function over a convex domain. Hence, $\forall \epsilon>0\; \exists K>0 $ such that $$|f(x)-f(y)|\leq K|x-y|+\epsilon\quad\quad\quad \forall x,y\in \Omega.$$ (See http://orfe.princeton.edu/~rvdb/tex/unif_cont/uc3.pdf for a proof of this fact.) Reasoning as in the proof of proposition 1, we can conclude that $|f(x)|\leq C(1+|x|).$ Note also that the two hypotesis ""uniformly continuous"" and ""$C^1$ with bounded gradient"" are, in general, independent. If the domain is convex the bounded gradient implies that the function is Lipschitz and hence also uniformly continuous. In general, however, it is possible to find uniformly continuous functions with unbounded derivative ($\sqrt x$ over $(0, +\infty)$) or functions with bounded gradient but not uniformly continuous ($f(x,y)$= ""argument of $(x,y)$"" over the annulus with a radius removed). My questions are: Is it possible to find a non-convex domain $\Omega$ and a function $f:\Omega\to \mathbb R$ $C^1$ with bounded gradient such that $ \frac{f(x)}{1+|x|} $ is unbounded? Is it possible to find a non-convex domain $\Omega$ and a function $f:\Omega\to \mathbb R$ uniformly continuous such that $ \frac{f(x)}{1+|x|} $ is unbounded? Is it possible to find a non-convex domain $\Omega$ and a function $f:\Omega\to \mathbb R$ uniformly continuous, $C^1$ with bounded gradient such that $ \frac{f(x)}{1+|x|} $ is unbounded?","Consider the following proposition: Proposition 1: Let $\Omega$ be an open, convex subset of $\mathbb R^2$ and let $f:\Omega \to \mathbb R$ be a $C^1$ function with bounded gradient. Then  $ \frac{f(x)}{1+|x|} $ is bounded. Proof of proposition 1: A $C^1$ function with bounded gradient over a convex domain is Lipschitz (by the Mean Value Theorem). Hence, $\forall x,y\in \Omega,$ $$|f(x)-f(y)|\leq M |x-y|,$$ where $M>0$ is the Lipschitz constant. If we fix an $y_0\in \Omega$ we have, $\forall x \in\Omega$ $$|f(x)|\leq |f(y_0)|+|f(x)-f(y_0)|\leq |f(y_0)|+ M|x-y_0|\leq |f(y_0)|+ M|x|+M|y_0|\leq C(1+|x|),$$ where $C=max\{|f(y_0)|+M|y_0|;M\}.$ Now consider another similar proposition: Proposition 2: Let $\Omega$ be an open, convex subset of $\mathbb R^2$ and let $f:\Omega \to \mathbb R$ be an uniformly continuous function. Then  $ \frac{f(x)}{1+|x|} $ is bounded. Proof of proposition 2: $f$ is an uniformly continuous function over a convex domain. Hence, $\forall \epsilon>0\; \exists K>0 $ such that $$|f(x)-f(y)|\leq K|x-y|+\epsilon\quad\quad\quad \forall x,y\in \Omega.$$ (See http://orfe.princeton.edu/~rvdb/tex/unif_cont/uc3.pdf for a proof of this fact.) Reasoning as in the proof of proposition 1, we can conclude that $|f(x)|\leq C(1+|x|).$ Note also that the two hypotesis ""uniformly continuous"" and ""$C^1$ with bounded gradient"" are, in general, independent. If the domain is convex the bounded gradient implies that the function is Lipschitz and hence also uniformly continuous. In general, however, it is possible to find uniformly continuous functions with unbounded derivative ($\sqrt x$ over $(0, +\infty)$) or functions with bounded gradient but not uniformly continuous ($f(x,y)$= ""argument of $(x,y)$"" over the annulus with a radius removed). My questions are: Is it possible to find a non-convex domain $\Omega$ and a function $f:\Omega\to \mathbb R$ $C^1$ with bounded gradient such that $ \frac{f(x)}{1+|x|} $ is unbounded? Is it possible to find a non-convex domain $\Omega$ and a function $f:\Omega\to \mathbb R$ uniformly continuous such that $ \frac{f(x)}{1+|x|} $ is unbounded? Is it possible to find a non-convex domain $\Omega$ and a function $f:\Omega\to \mathbb R$ uniformly continuous, $C^1$ with bounded gradient such that $ \frac{f(x)}{1+|x|} $ is unbounded?",,"['real-analysis', 'multivariable-calculus', 'convex-analysis', 'uniform-continuity', 'lipschitz-functions']"
86,Volume of the region lying inside circular and parabolic cylinders,Volume of the region lying inside circular and parabolic cylinders,,"I have the following problem: Find the volume of the region lying inside the circular cylinder $x^2+y^2=2y$ and inside the parabolic cylinder $z^2 = y$. To solve the problem, I did the following: $x^2+y^2=2y \implies x^2 +(y-1)^2=1$. So we have a circle centered at $(0,1)$. I think writing volume $V=\displaystyle\int_{-1}^{1}\int_{-2}^2(?)dA$ is not useful at this point. We have $z^2 =y$ too so the lower bound for $y$ must be $0$. So, how can I calculate the volume, i.e., write down a double integral? Thanks.","I have the following problem: Find the volume of the region lying inside the circular cylinder $x^2+y^2=2y$ and inside the parabolic cylinder $z^2 = y$. To solve the problem, I did the following: $x^2+y^2=2y \implies x^2 +(y-1)^2=1$. So we have a circle centered at $(0,1)$. I think writing volume $V=\displaystyle\int_{-1}^{1}\int_{-2}^2(?)dA$ is not useful at this point. We have $z^2 =y$ too so the lower bound for $y$ must be $0$. So, how can I calculate the volume, i.e., write down a double integral? Thanks.",,"['calculus', 'integration', 'multivariable-calculus']"
87,"maximum and minimum of $\frac{a^4+b^4+c^4}{(a+b+c)^4},$",maximum and minimum of,"\frac{a^4+b^4+c^4}{(a+b+c)^4},","Finding maximum and minimum of $\displaystyle \frac{a^4+b^4+c^4}{(a+b+c)^4},$ Where $a,b,c>0$ and $(a+b+c)^3=32abc$ $\bf{My Attempt}$ with AM-GM inequality $\displaystyle \frac{a+b+c}{3}\geq (abc)^{\frac{1}{3}}\Rightarrow (a+b+c)^3\geq 27abc$ Could some help me to solve it , thanks","Finding maximum and minimum of $\displaystyle \frac{a^4+b^4+c^4}{(a+b+c)^4},$ Where $a,b,c>0$ and $(a+b+c)^3=32abc$ $\bf{My Attempt}$ with AM-GM inequality $\displaystyle \frac{a+b+c}{3}\geq (abc)^{\frac{1}{3}}\Rightarrow (a+b+c)^3\geq 27abc$ Could some help me to solve it , thanks",,"['multivariable-calculus', 'inequality', 'polynomials', 'optimization', 'uvw']"
88,Writing the general term of a sequence without double factorials,Writing the general term of a sequence without double factorials,,Here's a question I was assigned: Find the general term without using double factorials: $$1+\frac{1\cdot2}{1\cdot3}+\frac{1\cdot2\cdot3}{1\cdot3\cdot5}+\frac{1\cdot2\cdot3\cdot4}{1\cdot3\cdot5\cdot7}+...$$ I have no clue how to even approach this without using a double factorial?,Here's a question I was assigned: Find the general term without using double factorials: $$1+\frac{1\cdot2}{1\cdot3}+\frac{1\cdot2\cdot3}{1\cdot3\cdot5}+\frac{1\cdot2\cdot3\cdot4}{1\cdot3\cdot5\cdot7}+...$$ I have no clue how to even approach this without using a double factorial?,,"['calculus', 'sequences-and-series', 'multivariable-calculus']"
89,Does this equation admit a solution $f:\mathbb{R}^2 \to \mathbb{R}^2$ with non-constant Jacobian?,Does this equation admit a solution  with non-constant Jacobian?,f:\mathbb{R}^2 \to \mathbb{R}^2,"Does there exist a smooth map $f:\mathbb{R}^2 \to \mathbb{R}^2$ such that: $\det df$ is not constant. $d(\det df)(e_1)df(e_2)=d(\det df)(e_2)df(e_1)$. Such a map cannot be an immersion, since if $df$ is invertible, then $df(e_1),df(e_2)$ are linearly independent, hence $d(\det df)=0$. Of course, that does not rule out $\det df \neq 0$ at some points. It only means that $\det df \neq 0 \Rightarrow d(\det df) = 0$.","Does there exist a smooth map $f:\mathbb{R}^2 \to \mathbb{R}^2$ such that: $\det df$ is not constant. $d(\det df)(e_1)df(e_2)=d(\det df)(e_2)df(e_1)$. Such a map cannot be an immersion, since if $df$ is invertible, then $df(e_1),df(e_2)$ are linearly independent, hence $d(\det df)=0$. Of course, that does not rule out $\det df \neq 0$ at some points. It only means that $\det df \neq 0 \Rightarrow d(\det df) = 0$.",,"['real-analysis', 'multivariable-calculus', 'jacobian']"
90,Directional Derivative: why there is no cos($\alpha$) in formula,Directional Derivative: why there is no cos() in formula,\alpha,"I'm learning Directional Derivative on Khan Academy . Here is the definition of Directional Derivative: So the formula for calculating directional derivative is: But as I knew, the dot product should be: I don't understand this point. Please explain for me why the formula for calculating directional derivative doesn't have ""cos($\alpha$)"".","I'm learning Directional Derivative on Khan Academy . Here is the definition of Directional Derivative: So the formula for calculating directional derivative is: But as I knew, the dot product should be: I don't understand this point. Please explain for me why the formula for calculating directional derivative doesn't have ""cos($\alpha$)"".",,"['multivariable-calculus', 'derivatives']"
91,"If $\lim\limits_{x\to\infty}\frac{n}{u_n}=1$, what can we say about $\sum\limits_{n=1}^{\infty}(-1)^n \frac{1}{u_n}$?","If , what can we say about ?",\lim\limits_{x\to\infty}\frac{n}{u_n}=1 \sum\limits_{n=1}^{\infty}(-1)^n \frac{1}{u_n},"If $\lim\limits_{x\to \infty}\frac{n}{u_n}=1$, can we prove that series $\sum\limits_{n=1}^{\infty}(-1)^n \frac{1}{u_n}$ converges,  doesn't converge or it's indefinite? I know that $\sum\limits_{n=1}^{\infty}\frac{1}{u_n}$ diverges by the comparing theorem. But I didn't find an explicit theorem about the alternate series. Any help will be appreciated, thank you!","If $\lim\limits_{x\to \infty}\frac{n}{u_n}=1$, can we prove that series $\sum\limits_{n=1}^{\infty}(-1)^n \frac{1}{u_n}$ converges,  doesn't converge or it's indefinite? I know that $\sum\limits_{n=1}^{\infty}\frac{1}{u_n}$ diverges by the comparing theorem. But I didn't find an explicit theorem about the alternate series. Any help will be appreciated, thank you!",,"['calculus', 'sequences-and-series', 'multivariable-calculus']"
92,In a two dimensional curl what does the number you get represent?,In a two dimensional curl what does the number you get represent?,,"As for example let the field be $\mathbf{F} = \langle -y,x\rangle$.  When I do the math I get $2$!  But $2$ what ?   One source says it measure ""twice the angular speed"" because we are measuring unit angular speed"" .  I assume $2$ is a number that applies to only this particular example... then what happens with other examples? Is it possible to have $4$,$5$, $6$ and is that then $4$,$5$, $6$ times the angular speed because we are measuring unit angular speed?  Any help is appreciated. I have also another example $G= <x,x>$and the Curl = $1$. Now if the curl represents a direction , let's assume this and we place it in the context of the $3$ dimensional curl then since the direction is perpendicular to the plane of the forces causing the spin, how can the spin be in the plane when the force is pointing up?","As for example let the field be $\mathbf{F} = \langle -y,x\rangle$.  When I do the math I get $2$!  But $2$ what ?   One source says it measure ""twice the angular speed"" because we are measuring unit angular speed"" .  I assume $2$ is a number that applies to only this particular example... then what happens with other examples? Is it possible to have $4$,$5$, $6$ and is that then $4$,$5$, $6$ times the angular speed because we are measuring unit angular speed?  Any help is appreciated. I have also another example $G= <x,x>$and the Curl = $1$. Now if the curl represents a direction , let's assume this and we place it in the context of the $3$ dimensional curl then since the direction is perpendicular to the plane of the forces causing the spin, how can the spin be in the plane when the force is pointing up?",,['multivariable-calculus']
93,On Inverse function theorem,On Inverse function theorem,,"The inverse function theorem is applied for function $f: \Bbb R^n \to \Bbb R^n$. Question : What is the natural generalization of inverse mapping theorem to the functions $f: \Bbb R^n \to \Bbb R^m$  ? And how one can prove this using traditional inverse mapping theorem? Note that in latter case the Jacobian of $f$ is a $m \times n$ matrix, so we can not think about inevitability of Jacobian, but instead still we may impose the condition of being Jacobian full column rank.","The inverse function theorem is applied for function $f: \Bbb R^n \to \Bbb R^n$. Question : What is the natural generalization of inverse mapping theorem to the functions $f: \Bbb R^n \to \Bbb R^m$  ? And how one can prove this using traditional inverse mapping theorem? Note that in latter case the Jacobian of $f$ is a $m \times n$ matrix, so we can not think about inevitability of Jacobian, but instead still we may impose the condition of being Jacobian full column rank.",,"['real-analysis', 'multivariable-calculus', 'implicit-function-theorem']"
94,why the curl of the gradient of a scalar field is zero? geometric interpretation,why the curl of the gradient of a scalar field is zero? geometric interpretation,,"This is probably a very silly question, but am I correct in saying that a vector field has non zero curl at some point when the direction of transformation changes? If so I can think of plenty of two variable scalar fields whose gradient vector field changes direction. Isn't the curl non zero at these points? I'm not asking for purely an algebraic computation. There is a visualisation on page 2 that I don't understand. https://ccom.ucsd.edu/~ctiee/notes/grad_n_curl.pdf Why the need to have the vectors going around in a circle, can't they just wave around? Even as they go around in a circle, you can complete the loop by having a sort of maximum where the vectors have zero magnitude.","This is probably a very silly question, but am I correct in saying that a vector field has non zero curl at some point when the direction of transformation changes? If so I can think of plenty of two variable scalar fields whose gradient vector field changes direction. Isn't the curl non zero at these points? I'm not asking for purely an algebraic computation. There is a visualisation on page 2 that I don't understand. https://ccom.ucsd.edu/~ctiee/notes/grad_n_curl.pdf Why the need to have the vectors going around in a circle, can't they just wave around? Even as they go around in a circle, you can complete the loop by having a sort of maximum where the vectors have zero magnitude.",,"['multivariable-calculus', 'vector-fields', 'scalar-fields']"
95,"Calculating surface area of $x^2 + y^2 + z^2 = 4$, $z \geq 1$","Calculating surface area of ,",x^2 + y^2 + z^2 = 4 z \geq 1,"I'm asked to calculate the surface area of  $$D : x^2 + y^2 + z^2 = 4 , \quad z \geq 1.$$ My attempt Let  $$x=2\sin{\theta}\cos{\phi}$$ $$y=2\sin{\theta}\sin{\phi}$$ $$z=2\cos{\theta}$$ where  $$ 0 \leq \theta \leq \frac{\pi}{3}$$  $$0\leq\phi\leq2\pi.$$ I realize the normal vector to the surface is $(x,y,z)$ which has length $\sqrt{x^2 + y^2 + z^2} = 2$. Surface area is calculated by integrating over the area D, with the length of the normal vector as the integrand. But since I changed the surface D to the surface given by the spherical coordinate system (call this surface E), I need to add a factor to compensate (Jacobi Determinant). But since I've got a variable substitution with 3 functions and 2 variables, this won't be a square matrix so I won't be able to take the determinant of it. What am I doing wrong?","I'm asked to calculate the surface area of  $$D : x^2 + y^2 + z^2 = 4 , \quad z \geq 1.$$ My attempt Let  $$x=2\sin{\theta}\cos{\phi}$$ $$y=2\sin{\theta}\sin{\phi}$$ $$z=2\cos{\theta}$$ where  $$ 0 \leq \theta \leq \frac{\pi}{3}$$  $$0\leq\phi\leq2\pi.$$ I realize the normal vector to the surface is $(x,y,z)$ which has length $\sqrt{x^2 + y^2 + z^2} = 2$. Surface area is calculated by integrating over the area D, with the length of the normal vector as the integrand. But since I changed the surface D to the surface given by the spherical coordinate system (call this surface E), I need to add a factor to compensate (Jacobi Determinant). But since I've got a variable substitution with 3 functions and 2 variables, this won't be a square matrix so I won't be able to take the determinant of it. What am I doing wrong?",,"['multivariable-calculus', 'surface-integrals', 'jacobian']"
96,Derivative of trace,Derivative of trace,,"I had never even heard of matrix calculus before yesterday but need it for a simple calculation and am stumped. If I can solve the following problem, I think I'll be fine for my larger calculuation. Let $F: \text{GL}(2,\mathbb{C}) \to \mathbb{R}$ via  $$ g \mapsto \text{tr}((gAg^{-1})^*gAg^{-1}) $$ For some fixed matrix $A$. So this is like an inner product, but conjugated. My question is, what is the derivative of this? I want to take the derivative at the identity, so I should get a map: $$ dF\vert_{id}: T_0\text{GL}(2,\mathbb{C}) \cong \text{End}(2,\mathbb{C}) \to T_o\mathbb{R} \cong \mathbb{R}. $$ In particular, given some generic matrix $M$ (some endomorphism), What is $dF\vert_0 (M)$?","I had never even heard of matrix calculus before yesterday but need it for a simple calculation and am stumped. If I can solve the following problem, I think I'll be fine for my larger calculuation. Let $F: \text{GL}(2,\mathbb{C}) \to \mathbb{R}$ via  $$ g \mapsto \text{tr}((gAg^{-1})^*gAg^{-1}) $$ For some fixed matrix $A$. So this is like an inner product, but conjugated. My question is, what is the derivative of this? I want to take the derivative at the identity, so I should get a map: $$ dF\vert_{id}: T_0\text{GL}(2,\mathbb{C}) \cong \text{End}(2,\mathbb{C}) \to T_o\mathbb{R} \cong \mathbb{R}. $$ In particular, given some generic matrix $M$ (some endomorphism), What is $dF\vert_0 (M)$?",,"['linear-algebra', 'multivariable-calculus', 'matrix-equations', 'matrix-calculus']"
97,Area of plane inside cylinder; problem with parametrization of plane,Area of plane inside cylinder; problem with parametrization of plane,,"I'm being asked to find the surface area of a plane defined by $x+y+z=a$ inside a cylinder defined by $x^2+y^2=a^2$ and I thought, ""simple enough, I'll just use the normal vector and integrate its norm over a polar domain with $0\leq r\leq a$ and $0\leq \theta \leq 2\pi$"". Sure enough, that seems to give the correct solution, according to my solution sheet: $$\int_0^{2\pi}\int_0^a\sqrt{3}r\,dr\,d\theta = \int_0^{2\pi}\frac{\sqrt{3}}{2}a^2\,d\theta = \sqrt{3}\pi a^2$$ But let's say I want to use a parametrization $r(s,t)$ for the plane such as $r(s,t)=(a+ta+sa,-ta,-sa)$. That seems to be a correct parametrization, and it gives a normal vector $r_s \times r_t = (-a^2,-a^2,-a^2)$. Now the norm of this is $\sqrt{3}a^2$, which is gonna mess up the result obtained above. Is there a necessity for a change of variables due to the parametrization of the plane? I'm honestly out of ideas... It seems like the surface area of the plane would depend on the chosen normal vector, which is ridiculous. (edit: $a$ is some scalar.)","I'm being asked to find the surface area of a plane defined by $x+y+z=a$ inside a cylinder defined by $x^2+y^2=a^2$ and I thought, ""simple enough, I'll just use the normal vector and integrate its norm over a polar domain with $0\leq r\leq a$ and $0\leq \theta \leq 2\pi$"". Sure enough, that seems to give the correct solution, according to my solution sheet: $$\int_0^{2\pi}\int_0^a\sqrt{3}r\,dr\,d\theta = \int_0^{2\pi}\frac{\sqrt{3}}{2}a^2\,d\theta = \sqrt{3}\pi a^2$$ But let's say I want to use a parametrization $r(s,t)$ for the plane such as $r(s,t)=(a+ta+sa,-ta,-sa)$. That seems to be a correct parametrization, and it gives a normal vector $r_s \times r_t = (-a^2,-a^2,-a^2)$. Now the norm of this is $\sqrt{3}a^2$, which is gonna mess up the result obtained above. Is there a necessity for a change of variables due to the parametrization of the plane? I'm honestly out of ideas... It seems like the surface area of the plane would depend on the chosen normal vector, which is ridiculous. (edit: $a$ is some scalar.)",,['multivariable-calculus']
98,Simplification on stokes theorem problem,Simplification on stokes theorem problem,,"we have a problem , but is more conceptual , than getting the answer, is from larson calculus 9, 15.8.11 and says: $\ F(x,y,z) = 2y\mathbf{i} + 3z\mathbf{j} + x\mathbf{k}$ $\ C: triangle\ with\  vertex\ (2,0,0)\ (0,2,0)\ (0,0,2)$ the answer is not the problem, cause it is -12, the problem is here given that: $\ \int_c \mathbf{F}\cdot d\mathbf{r} = \int_s\int (curl\ \mathbf{F})\cdot \mathbf{N}\ dS $ so $\ curl\ F =\ <-3,-1,-2> $ given the points, we can find two vector, in order to find  $\ \mathbf{N} $ $\ \vec{A} = (2,0,0)-(0,2,0) $ $\ \vec{A} =\ <2,-2, 0 > $ $\ \vec{B} = (2,0,0)-(0,0,2) $ $\ \vec{B} =\ <2,0, -2 > $ $\ \textbf{N} = \vec{A} \times \vec{B}  = <4,4,4>$ how ever if we divide it by 4, we also have $\   \textbf{N}_{1}  = <1,1,1>$ since $\ \textbf{N},  \textbf{N}_{1} $ has to be an unit vector we divide by his magnitude $\  || \textbf{N} || \ = 4 \sqrt 3  $ $\ \frac{\textbf{N}}{|| \textbf{N} ||} = \  \frac{1}{4\sqrt 3} \cdot <4,4,4> $ $\  || \textbf{N}_{1} || \ = \sqrt 3  $ $\ \frac{\textbf{N}_{1}}{|| \textbf{N}_{1} ||} = \  \frac{1}{\sqrt 3} \cdot <1,1,1> $ here is the problem $\ dS = || r_{u} \times r_{v}  || dA $ $\  \textbf{N} =  r_{u} \times r_{v}  $ $\  dS = || \textbf{N} ||dA  $ $\ \int_{D}\int <-3,-1,-2> \cdot\  \frac{1}{4 \sqrt 3} \cdot <4,4,4> \cdot\ 4 \sqrt 3\   dA  = -24 $ $\ \int_{D}\int <-3,-1,-2> \cdot\  \frac{1}{\sqrt 3} \cdot <1,1,1> \cdot\  \sqrt 3\  dA = -12 $ I dont know why the answer is different if mathematically both are the same D = projection in the xy plane, but is not necessary because is the shape that is reflexed on the xy plane is a triangle","we have a problem , but is more conceptual , than getting the answer, is from larson calculus 9, 15.8.11 and says: $\ F(x,y,z) = 2y\mathbf{i} + 3z\mathbf{j} + x\mathbf{k}$ $\ C: triangle\ with\  vertex\ (2,0,0)\ (0,2,0)\ (0,0,2)$ the answer is not the problem, cause it is -12, the problem is here given that: $\ \int_c \mathbf{F}\cdot d\mathbf{r} = \int_s\int (curl\ \mathbf{F})\cdot \mathbf{N}\ dS $ so $\ curl\ F =\ <-3,-1,-2> $ given the points, we can find two vector, in order to find  $\ \mathbf{N} $ $\ \vec{A} = (2,0,0)-(0,2,0) $ $\ \vec{A} =\ <2,-2, 0 > $ $\ \vec{B} = (2,0,0)-(0,0,2) $ $\ \vec{B} =\ <2,0, -2 > $ $\ \textbf{N} = \vec{A} \times \vec{B}  = <4,4,4>$ how ever if we divide it by 4, we also have $\   \textbf{N}_{1}  = <1,1,1>$ since $\ \textbf{N},  \textbf{N}_{1} $ has to be an unit vector we divide by his magnitude $\  || \textbf{N} || \ = 4 \sqrt 3  $ $\ \frac{\textbf{N}}{|| \textbf{N} ||} = \  \frac{1}{4\sqrt 3} \cdot <4,4,4> $ $\  || \textbf{N}_{1} || \ = \sqrt 3  $ $\ \frac{\textbf{N}_{1}}{|| \textbf{N}_{1} ||} = \  \frac{1}{\sqrt 3} \cdot <1,1,1> $ here is the problem $\ dS = || r_{u} \times r_{v}  || dA $ $\  \textbf{N} =  r_{u} \times r_{v}  $ $\  dS = || \textbf{N} ||dA  $ $\ \int_{D}\int <-3,-1,-2> \cdot\  \frac{1}{4 \sqrt 3} \cdot <4,4,4> \cdot\ 4 \sqrt 3\   dA  = -24 $ $\ \int_{D}\int <-3,-1,-2> \cdot\  \frac{1}{\sqrt 3} \cdot <1,1,1> \cdot\  \sqrt 3\  dA = -12 $ I dont know why the answer is different if mathematically both are the same D = projection in the xy plane, but is not necessary because is the shape that is reflexed on the xy plane is a triangle",,"['multivariable-calculus', 'vector-fields', 'stokes-theorem']"
99,"There exist $3$ positive numbers $x,y,z$ such that $x\cdot y\cdot z=100$ but $x^2+y^2+z^2<65$.",There exist  positive numbers  such that  but .,"3 x,y,z x\cdot y\cdot z=100 x^2+y^2+z^2<65",I have to show this using a form of calculus. I know the answer is the cube root of $100$. I did this using intuition and upper and lower bounds but I have no idea how to show it from a calculus standpoint. Maybe showing the intersection of the surfaces?,I have to show this using a form of calculus. I know the answer is the cube root of $100$. I did this using intuition and upper and lower bounds but I have no idea how to show it from a calculus standpoint. Maybe showing the intersection of the surfaces?,,"['calculus', 'multivariable-calculus']"
