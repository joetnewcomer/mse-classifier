,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Non-homogeneous Differential Equation,Non-homogeneous Differential Equation,,"Trying to solve a non-homogeneous differential equation, whether it is linear, Bernoulli, Euler, you solve the related homogeneous equation and then you look for a particular solution depending on the ""class"" of the non-homogeneous term. Just to make an example, let's say we have this equation $$y'' - y' - 2y = 2x e^x$$ we solve $$y'' -2y' +y =0$$ and we get solutions of the form $y_{\text{hom}}(x) = C_1 e^{2x} + C_2 e^{-x}$. Then, since the known term is of the form $p(x)(x) e^{\mu x}$, and $\mu$ is not a solution to the characteristic polynomial, we should find the solutions through those of the form $q(x)e^{\mu x}$. If, on the other hand, $\mu$ were a solution of the characteristic polynomial, we should have looked for solutions through those of the form $q(x) x^q e^{\mu x}$ where $q$ is the multiplicity of the root $\mu$. (where $q(x)$ is the general polynomial of the same degree as $p(x)$. I have a whole list of these ""classes"" of known terms, like $p(x)$, $p(x)e^{\mu x}$, $e^{\alpha x}(a \sin(\beta x) + a' \cos(\beta x))$, and the corresponding general solutions (like I said before, also with the multiplicity involved). My question is, where do these come from? I have no idea how such classes were found or anything; my book and my professors just said these were given and we should just learn them. Edit: from the comment I see that something's going on, unfortunately I don't have enough background to know how to work with Laurent series. I would appreciate any explanation. Thanks.","Trying to solve a non-homogeneous differential equation, whether it is linear, Bernoulli, Euler, you solve the related homogeneous equation and then you look for a particular solution depending on the ""class"" of the non-homogeneous term. Just to make an example, let's say we have this equation $$y'' - y' - 2y = 2x e^x$$ we solve $$y'' -2y' +y =0$$ and we get solutions of the form $y_{\text{hom}}(x) = C_1 e^{2x} + C_2 e^{-x}$. Then, since the known term is of the form $p(x)(x) e^{\mu x}$, and $\mu$ is not a solution to the characteristic polynomial, we should find the solutions through those of the form $q(x)e^{\mu x}$. If, on the other hand, $\mu$ were a solution of the characteristic polynomial, we should have looked for solutions through those of the form $q(x) x^q e^{\mu x}$ where $q$ is the multiplicity of the root $\mu$. (where $q(x)$ is the general polynomial of the same degree as $p(x)$. I have a whole list of these ""classes"" of known terms, like $p(x)$, $p(x)e^{\mu x}$, $e^{\alpha x}(a \sin(\beta x) + a' \cos(\beta x))$, and the corresponding general solutions (like I said before, also with the multiplicity involved). My question is, where do these come from? I have no idea how such classes were found or anything; my book and my professors just said these were given and we should just learn them. Edit: from the comment I see that something's going on, unfortunately I don't have enough background to know how to work with Laurent series. I would appreciate any explanation. Thanks.",,['ordinary-differential-equations']
1,How to solve implicit solution of IVP,How to solve implicit solution of IVP,,"The given question is: $$  dy/dt + 2y = 1\ ;\qquad    y(0)= 5/2 $$ when i solve this i get $\ln(-4)=c$ now the problem is how to solve $\ln(-4)$?","The given question is: $$  dy/dt + 2y = 1\ ;\qquad    y(0)= 5/2 $$ when i solve this i get $\ln(-4)=c$ now the problem is how to solve $\ln(-4)$?",,['ordinary-differential-equations']
2,First Order DE  mixing problem,First Order DE  mixing problem,,"So for my homework I've gotten an incorrect answer on this problem 3 times in a row. Here's an overview of my work A large tank holds 250 liters of water with a salt concentration of 7 grams per liter. A brine solution containing 3 grams per liter is added to the tank at a rate of 9 liters per minute. The well-mixed solution is pumped out of the tank at a rate of 5 liters per minute. How much salt is in the tank after 15 minutes?    Enter your answer to the nearest 0.0001 grams. $$\begin{align*} S(t)&= \text{concentration of salt as a function of time.}\\ S'&=27-\left(\frac{5S}{250+4t}\right)\\ I&=(250+4t)^5\\ [S(250+4t)^5]'&=27(250+4t)^5\\ S(250+4t)^5&=\frac{9}{8}(250+4t)^6+C\\ S&=\frac{\frac{9}{8}(250+4t)+C}{(250+4t)^5}\\ S(0)&=1750=\frac{\frac{9}{8}(250)+C}{(250)^5}\\ C&=1468.75(250)^5\\ S(15)&=\frac{\frac{9}{8}(250+4(15))+1468.75(250)^5}{(250+4(15)^5}\\ S(15)&=849.7520 \end{align*}$$","So for my homework I've gotten an incorrect answer on this problem 3 times in a row. Here's an overview of my work A large tank holds 250 liters of water with a salt concentration of 7 grams per liter. A brine solution containing 3 grams per liter is added to the tank at a rate of 9 liters per minute. The well-mixed solution is pumped out of the tank at a rate of 5 liters per minute. How much salt is in the tank after 15 minutes?    Enter your answer to the nearest 0.0001 grams. $$\begin{align*} S(t)&= \text{concentration of salt as a function of time.}\\ S'&=27-\left(\frac{5S}{250+4t}\right)\\ I&=(250+4t)^5\\ [S(250+4t)^5]'&=27(250+4t)^5\\ S(250+4t)^5&=\frac{9}{8}(250+4t)^6+C\\ S&=\frac{\frac{9}{8}(250+4t)+C}{(250+4t)^5}\\ S(0)&=1750=\frac{\frac{9}{8}(250)+C}{(250)^5}\\ C&=1468.75(250)^5\\ S(15)&=\frac{\frac{9}{8}(250+4(15))+1468.75(250)^5}{(250+4(15)^5}\\ S(15)&=849.7520 \end{align*}$$",,['ordinary-differential-equations']
3,A differential-functional equation: $f'(f^{-1}(x)) = 1/g(x)$,A differential-functional equation:,f'(f^{-1}(x)) = 1/g(x),"Problem : Given $g(x)$, solve the equation $f'(f^{-1}(x)) = \frac{1}{g(x)}$ for an invertible and differentiable function $f(x)$. So far I have tried setting $y = f^{-1}(x) \Leftrightarrow x = f(y)$, obtaining the differential equation $$ f'(y) = \frac{dx}{dy} = \frac{1}{g(x)} $$ which we then solve to obtain $y$ in terms of $x$, i.e. to obtain the inverse function $f^{-1}(x)$. If this is easily inverted then we can find $f(x)$. What I am interested in is if anyone knows a better way to solve this, desirably one which allows us to determine $f(x)$ directly. I'm new to this kind of equation so please correct me if there's a better term for it than ""differential-functional"" :) Edit: I probably should say that in the particular context I am considering, $g(x)$ is the norm of a non-zero vector $\vec{r}(x)$ and is hence always positive.","Problem : Given $g(x)$, solve the equation $f'(f^{-1}(x)) = \frac{1}{g(x)}$ for an invertible and differentiable function $f(x)$. So far I have tried setting $y = f^{-1}(x) \Leftrightarrow x = f(y)$, obtaining the differential equation $$ f'(y) = \frac{dx}{dy} = \frac{1}{g(x)} $$ which we then solve to obtain $y$ in terms of $x$, i.e. to obtain the inverse function $f^{-1}(x)$. If this is easily inverted then we can find $f(x)$. What I am interested in is if anyone knows a better way to solve this, desirably one which allows us to determine $f(x)$ directly. I'm new to this kind of equation so please correct me if there's a better term for it than ""differential-functional"" :) Edit: I probably should say that in the particular context I am considering, $g(x)$ is the norm of a non-zero vector $\vec{r}(x)$ and is hence always positive.",,"['ordinary-differential-equations', 'functional-equations']"
4,Delta operator magic,Delta operator magic,,"What does it mean when, in (derivation of) (partial) differential equations, a term containing an operator like: $\delta t$ gets replaced by: $d t$ and what are the rules pertaining to this? What is the difference between the two? It seems black magic to me :). Thanks!","What does it mean when, in (derivation of) (partial) differential equations, a term containing an operator like: $\delta t$ gets replaced by: $d t$ and what are the rules pertaining to this? What is the difference between the two? It seems black magic to me :). Thanks!",,"['calculus', 'ordinary-differential-equations']"
5,Blow-up. Graph of canonical map: is a complex manifold no closed or a topological space no closed,Blow-up. Graph of canonical map: is a complex manifold no closed or a topological space no closed,,"I am reading by myself this book http://tinyurl.com/37z4bbt . But to be honest, I have several problems to fully understand some part of the text. Maybe because I have not yet solid knowledge or I still need to learn more about the subject, hence asked a little help To be exact, is this part of the construction concerning to Algebraic Blow-Up that I do not understand like trying... I quote part of the building in which I do not understand how to deal with some concepts Consider the canonical map from $\mathbb{C}^2$ to the projective line $\mathbb{CP}^1$ that associates with each point $(x,y)$ different from the origin, the line {$(tx, ty) : t \in \mathbb{C}$} passing through this point. The graph of this map is a complex 2-dimensional surface in the complex 3-dimensional manifold (the Cartesian product) $\mathbb{C}^2$ Ã— $\mathbb{CP}^1$, which is not closed . To obtain the closure, one has to add the exceptional curve $E={0} \times \mathbb{CP^{1}} \subset \mathbb{C^{2}} \times \mathbb{CP^{1}}$. then, I do not understand is how to handle this graph. If, as a topological space or as complex manifold . Well, in the sense of complex manifold, say it is not closed, it means that the manifold is not compact without boundary. I am really a little confused","I am reading by myself this book http://tinyurl.com/37z4bbt . But to be honest, I have several problems to fully understand some part of the text. Maybe because I have not yet solid knowledge or I still need to learn more about the subject, hence asked a little help To be exact, is this part of the construction concerning to Algebraic Blow-Up that I do not understand like trying... I quote part of the building in which I do not understand how to deal with some concepts Consider the canonical map from $\mathbb{C}^2$ to the projective line $\mathbb{CP}^1$ that associates with each point $(x,y)$ different from the origin, the line {$(tx, ty) : t \in \mathbb{C}$} passing through this point. The graph of this map is a complex 2-dimensional surface in the complex 3-dimensional manifold (the Cartesian product) $\mathbb{C}^2$ Ã— $\mathbb{CP}^1$, which is not closed . To obtain the closure, one has to add the exceptional curve $E={0} \times \mathbb{CP^{1}} \subset \mathbb{C^{2}} \times \mathbb{CP^{1}}$. then, I do not understand is how to handle this graph. If, as a topological space or as complex manifold . Well, in the sense of complex manifold, say it is not closed, it means that the manifold is not compact without boundary. I am really a little confused",,"['general-topology', 'algebraic-topology', 'ordinary-differential-equations']"
6,FitzHugh- Nagumo caricature of the Hodgkin-Huxley equations,FitzHugh- Nagumo caricature of the Hodgkin-Huxley equations,,"I've been trying to solve this problem from Elements of Applied Bifurcation Theory, but even with the hints given I haven't found out how to proceed yet. I would appreciate any further hints or insights you could give me in order to make some progress. It's exercise 2.6.(2): The following system of partial differential equations is the FitzHugh- Nagumo caricature of the Hodgkin-Huxley equations modeling the nerve impulse propagation along an axon: $ \frac{\partial u}{\partial t} = \frac{\partial^2 u}{\partial x^2}-f_a(u)-v$, $ \frac{\partial v}{\partial t} = bu $, where $u(x,t)$ represents the membrane potential, $v=v(x,t)$ is a ""recovery"" variable, $f_a(u)=u(u-a)(u-1)$, $1>a>0,b>0,-\infty < x < \infty$, and $t>0$. Traveling waves are solutions to these equations of the form $u(x,t)=U(\xi)$, $v(x,t)=V(\xi)$, $\xi = x+ct$, where $c$ is an a priori unknown wave propagation speed. The functions $U(\xi)$, $V(\xi)$ are the wave profiles. (a) Derive a system of three ordinary differential equations for the profiles with ""time"" $\xi$. (Hint: introduce an extra variable: $W=\dot{U}$. (b) Check that for all $c>0$ the system for the profiles (the wave system) has a unique equilibrium with one positive eigenvalue and two eigenvalues with negative real parts. (Hint: First, verify this assuming that eigenvalues are real. The, show that the characteristic equation cannot have roots on the imaginary axis, and finally, use the continuous dependence of the eigenvalues on the parameters. (c) Conclude that the equilibrium can be either a saddle or a saddle- focus with a one-dimensional unstable and a two-dimensional stable in- variant manifold, and find a condition on the system parameters that de- fines a boundary between these two cases. Plot several boundaries in the $(a,c)$-plane for different values of $b$ and specify the region corresponding to saddle-foci. (Hint: at the boundary the characteristic polynomial $h(\lambda)$ has a double root $\lambda_0:h(\lambda_0)=h'(\lambda_0)=0.$).","I've been trying to solve this problem from Elements of Applied Bifurcation Theory, but even with the hints given I haven't found out how to proceed yet. I would appreciate any further hints or insights you could give me in order to make some progress. It's exercise 2.6.(2): The following system of partial differential equations is the FitzHugh- Nagumo caricature of the Hodgkin-Huxley equations modeling the nerve impulse propagation along an axon: $ \frac{\partial u}{\partial t} = \frac{\partial^2 u}{\partial x^2}-f_a(u)-v$, $ \frac{\partial v}{\partial t} = bu $, where $u(x,t)$ represents the membrane potential, $v=v(x,t)$ is a ""recovery"" variable, $f_a(u)=u(u-a)(u-1)$, $1>a>0,b>0,-\infty < x < \infty$, and $t>0$. Traveling waves are solutions to these equations of the form $u(x,t)=U(\xi)$, $v(x,t)=V(\xi)$, $\xi = x+ct$, where $c$ is an a priori unknown wave propagation speed. The functions $U(\xi)$, $V(\xi)$ are the wave profiles. (a) Derive a system of three ordinary differential equations for the profiles with ""time"" $\xi$. (Hint: introduce an extra variable: $W=\dot{U}$. (b) Check that for all $c>0$ the system for the profiles (the wave system) has a unique equilibrium with one positive eigenvalue and two eigenvalues with negative real parts. (Hint: First, verify this assuming that eigenvalues are real. The, show that the characteristic equation cannot have roots on the imaginary axis, and finally, use the continuous dependence of the eigenvalues on the parameters. (c) Conclude that the equilibrium can be either a saddle or a saddle- focus with a one-dimensional unstable and a two-dimensional stable in- variant manifold, and find a condition on the system parameters that de- fines a boundary between these two cases. Plot several boundaries in the $(a,c)$-plane for different values of $b$ and specify the region corresponding to saddle-foci. (Hint: at the boundary the characteristic polynomial $h(\lambda)$ has a double root $\lambda_0:h(\lambda_0)=h'(\lambda_0)=0.$).",,"['ordinary-differential-equations', 'dynamical-systems']"
7,Characteristics for 2nd order differential equations,Characteristics for 2nd order differential equations,,"If I have an equation $p(x)\frac{\partial^2u}{\partial x^2} + r(x)\frac{\partial^2u}{\partial x\partial y} + q(x)\frac{\partial^2 u}{\partial y^2}=f(x,y,u)$ Where $f$ maybe contains first partial derivatives for $u$. Can anyone give me a worked example of how to solve this using the method of characteristics. Thanks in advance.","If I have an equation $p(x)\frac{\partial^2u}{\partial x^2} + r(x)\frac{\partial^2u}{\partial x\partial y} + q(x)\frac{\partial^2 u}{\partial y^2}=f(x,y,u)$ Where $f$ maybe contains first partial derivatives for $u$. Can anyone give me a worked example of how to solve this using the method of characteristics. Thanks in advance.",,"['ordinary-differential-equations', 'partial-differential-equations']"
8,Find self-similar solution for the heat equation,Find self-similar solution for the heat equation,,"I would like to find solutions to the equation $$\partial_t \phi = \partial_{rr}\phi + \frac{1}{r}\partial_r \phi - \frac{1}{r^2}\phi$$ of the form $\phi(t, r) = t^{-\gamma} g(r/\sqrt{t})$ . This equation comes from the heat equation in polar coordinates for function $\psi(r, \theta) = \phi(r)e^{i\theta}$ . Plugging this form of $\phi$ in the equation leads to the ODE in $\xi$ $$\frac{1}{t^{\gamma + 1}} \left( g'' + \frac{1}{\xi} g' - \frac{1}{\xi^2} g + \gamma g + \frac{1}{2}\xi g\right) = 0 \quad \text{for } \xi = \frac{r}{\sqrt t}.$$ Multiplying by $t^{\gamma + 1}$ on both sides, we get the ODE depending only on $\xi$ and $\gamma$ . I would like to find solution of this equation for specific value of $\gamma$ . I managed to find a straightforward solution $g(\xi) = \xi$ for $\gamma = -1/2$ . Moreover, for $\gamma = 3/2$ , I managed to find the solution $$g(\xi) = \xi e^{-\xi^2/4}.$$ Now I would like to find solution of this ODE for $\gamma$ positive and lower than $3/2$ , i.e. $\gamma \in (0,3/2)$ , but to be honest I don't see anything obvious. Any idea ?","I would like to find solutions to the equation of the form . This equation comes from the heat equation in polar coordinates for function . Plugging this form of in the equation leads to the ODE in Multiplying by on both sides, we get the ODE depending only on and . I would like to find solution of this equation for specific value of . I managed to find a straightforward solution for . Moreover, for , I managed to find the solution Now I would like to find solution of this ODE for positive and lower than , i.e. , but to be honest I don't see anything obvious. Any idea ?","\partial_t \phi = \partial_{rr}\phi + \frac{1}{r}\partial_r \phi - \frac{1}{r^2}\phi \phi(t, r) = t^{-\gamma} g(r/\sqrt{t}) \psi(r, \theta) = \phi(r)e^{i\theta} \phi \xi \frac{1}{t^{\gamma + 1}} \left( g'' + \frac{1}{\xi} g' - \frac{1}{\xi^2} g + \gamma g + \frac{1}{2}\xi g\right) = 0 \quad \text{for } \xi = \frac{r}{\sqrt t}. t^{\gamma + 1} \xi \gamma \gamma g(\xi) = \xi \gamma = -1/2 \gamma = 3/2 g(\xi) = \xi e^{-\xi^2/4}. \gamma 3/2 \gamma \in (0,3/2)","['calculus', 'functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'heat-equation']"
9,How do I check if set of solutions to differential equation form vector subspace,How do I check if set of solutions to differential equation form vector subspace,,"Let $\mathbb{R}^\mathbb{R}$ be the vector space of all functions $f:\mathbb{R}\to\mathbb{R}$ , with addition and scalar multiplication defned pointwise. Which of the following sets of functions form a vector subspace of $\mathbb{R}^\mathbb{R}?$ The set of solutions of the differential equation $\ddot{x}(t)+(t^2-3)\dot{x}(t)+t^4x(t)=0.$ The set of solutions of $\ddot{x}(t)+(t^2-3)\dot{x}(t)+t^4x(t)=\sin t.$ The set of solutions of $(\dot{x}(t))^2-x(t)=0.$ The set of solutions of $( \ddot{x} ( t) ) ^4+ ( x( t) ) ^2= 0.$ I already know that there's a theorem homogenous linear diff. equations being vector spaces, but without it how exactly do I verify each case manually? I suppose I can solve each one and write down the general solution, but I read that there are infinite number of solutions. So how do I verify if axioms hold if there's possibly infinite solutions to check?","Let be the vector space of all functions , with addition and scalar multiplication defned pointwise. Which of the following sets of functions form a vector subspace of The set of solutions of the differential equation The set of solutions of The set of solutions of The set of solutions of I already know that there's a theorem homogenous linear diff. equations being vector spaces, but without it how exactly do I verify each case manually? I suppose I can solve each one and write down the general solution, but I read that there are infinite number of solutions. So how do I verify if axioms hold if there's possibly infinite solutions to check?","\mathbb{R}^\mathbb{R} f:\mathbb{R}\to\mathbb{R} \mathbb{R}^\mathbb{R}? \ddot{x}(t)+(t^2-3)\dot{x}(t)+t^4x(t)=0. \ddot{x}(t)+(t^2-3)\dot{x}(t)+t^4x(t)=\sin
t. (\dot{x}(t))^2-x(t)=0. ( \ddot{x} ( t) ) ^4+ ( x( t) ) ^2= 0.","['linear-algebra', 'ordinary-differential-equations', 'vector-spaces']"
10,Pushforward of vector field and its divergence,Pushforward of vector field and its divergence,,"Let $X:U_1 \rightarrow \mathbb{R}^2$ be a smooth vector field defined in an open subset of $\Bbb{R}^2$ and $\phi:U_1\rightarrow U_2$ a diffeomorphism between open subsets of $\Bbb{R}^2$ . Let $Y = \phi_*X$ , ie, $$Y(p) = d\phi_{\phi^{-1}(p)}(X(\phi^{-1}(p))),\qquad  p \in U_2.$$ Here, we are considering that if $X(x,y) = (P(x,y), Q(x,y))$ , where $P:U_1 \to \Bbb{R}$ and $Q:U_1\to \Bbb{R}$ are smooth functions, then $$ \operatorname{div}(X) := \frac{\partial P}{\partial x}  + \frac{\partial Q}{\partial y}. $$ Is there any relation between $\operatorname{div}(X)$ and $\operatorname{div}(Y)$ ? Can I get $\operatorname{div}(Y)$ in terms of $\phi$ and $\operatorname{div}$ ?","Let be a smooth vector field defined in an open subset of and a diffeomorphism between open subsets of . Let , ie, Here, we are considering that if , where and are smooth functions, then Is there any relation between and ? Can I get in terms of and ?","X:U_1 \rightarrow \mathbb{R}^2 \Bbb{R}^2 \phi:U_1\rightarrow U_2 \Bbb{R}^2 Y = \phi_*X Y(p) = d\phi_{\phi^{-1}(p)}(X(\phi^{-1}(p))),\qquad  p \in U_2. X(x,y) = (P(x,y), Q(x,y)) P:U_1 \to \Bbb{R} Q:U_1\to \Bbb{R} 
\operatorname{div}(X) := \frac{\partial P}{\partial x} 
+ \frac{\partial Q}{\partial y}.
 \operatorname{div}(X) \operatorname{div}(Y) \operatorname{div}(Y) \phi \operatorname{div}","['ordinary-differential-equations', 'differential-geometry', 'vector-fields', 'divergence-operator', 'pushforward']"
11,Definition of orbit equivalence,Definition of orbit equivalence,,"I have a doubt regarding the definition of orbit equivalence as given by Fisher & Hasselblatt in their book Hyperbolic Flows . We say that two flows $\phi, \psi$ on $X, Y$ respectively are orbit equivalent if there exists a homeomorphism $h : X \to Y$ takes orbits of $\phi$ to orbits of $\psi$ . Let $x \in X$ and $O^{\phi}(x)$ its orbit under $\phi$ . Is the definition of orbit equivalence $h(O^{\phi}(x)) \subseteq O^{\psi}(h(x))$ or $h(O^{\phi}(x)) = O^{\psi}(h(x))$ ? It seems to me that in order for orbit equivalence to be an equivalence relation we need $h$ to take the orbits of $\phi$ onto the orbits of $\psi$ . Or does this come for free given just the inclusion $\subseteq$ ?",I have a doubt regarding the definition of orbit equivalence as given by Fisher & Hasselblatt in their book Hyperbolic Flows . We say that two flows on respectively are orbit equivalent if there exists a homeomorphism takes orbits of to orbits of . Let and its orbit under . Is the definition of orbit equivalence or ? It seems to me that in order for orbit equivalence to be an equivalence relation we need to take the orbits of onto the orbits of . Or does this come for free given just the inclusion ?,"\phi, \psi X, Y h : X \to Y \phi \psi x \in X O^{\phi}(x) \phi h(O^{\phi}(x)) \subseteq O^{\psi}(h(x)) h(O^{\phi}(x)) = O^{\psi}(h(x)) h \phi \psi \subseteq","['ordinary-differential-equations', 'differential-geometry', 'dynamical-systems', 'group-actions', 'topological-dynamics']"
12,What are the properties of the differential operator that justify this?,What are the properties of the differential operator that justify this?,,"Say I am solving the following differential equation: $$y'' - y = x^2$$ It is perfectly permissible for me to proceed as follows: $$D^2y- y = x^2$$ $$y(D^2-1) = x^2$$ $$y = (D^2 - 1)^{-1} \cdot x^2$$ $$y=\sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} D^n\cdot x^2$$ $$y = (-1 - D^2 - D^4...) x^2$$ $$y_p = -x^2 - 2$$ Which specific properties have been used (specifically the second and third step) which justify this? Similarly, why am I allowed to do this? $$\frac{dy}{dx}=2x$$ $$dy=2xdx$$ In general, which properties hold for the differential operator?","Say I am solving the following differential equation: It is perfectly permissible for me to proceed as follows: Which specific properties have been used (specifically the second and third step) which justify this? Similarly, why am I allowed to do this? In general, which properties hold for the differential operator?",y'' - y = x^2 D^2y- y = x^2 y(D^2-1) = x^2 y = (D^2 - 1)^{-1} \cdot x^2 y=\sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} D^n\cdot x^2 y = (-1 - D^2 - D^4...) x^2 y_p = -x^2 - 2 \frac{dy}{dx}=2x dy=2xdx,['ordinary-differential-equations']
13,Stochastic average of a differential equation is not the same as average of its solutions,Stochastic average of a differential equation is not the same as average of its solutions,,"Assume a static (time-independent) random variable $r$ for which we know its probability distribution $P(r)$ . Consider this to be a Gaussian distribution, such that $\langle r \rangle=0$ and $\langle r^2 \rangle=r_0$ , etc. Consider now the following differential equation: $\frac{d}{dt}X(t)=r A X(t)$ , where $X=(x_1,x_2)^T$ and $A$ is a time-independent $2\times 2$ matrix. I am interested in taking the average of the differential equation over the random variable. If I naively just take the expected value of the previous differential equation, the problem is that I would just get $\frac{d}{dt}X=0$ because $\langle r \rangle=0$ . This is not true, as $X$ is a functional of r, $X(r,t)$ , so $\langle r X(r,t) \rangle\neq0$ . One possible solution would be to formally solve the differential equation as $X(t)=e^{rAt}X(0)$ and then take the average. I would like to avoid this method, as it is not always aplicable. What would be the correct way to proceed if I would like to get an averaged differential equation that properly reproduces the results of averaging the solution? Note: I know about Novikov's theorem which is sometimes used to solve such stochastic differential equations using two-time correlation functions. However, as far as I am aware, it only works if the stochastic variable is dynamical, i.e. r(t) is sampled by the solution of a stochastic differential equation. In my case, my stochastic variable is static, so only the probability distribution is known.","Assume a static (time-independent) random variable for which we know its probability distribution . Consider this to be a Gaussian distribution, such that and , etc. Consider now the following differential equation: , where and is a time-independent matrix. I am interested in taking the average of the differential equation over the random variable. If I naively just take the expected value of the previous differential equation, the problem is that I would just get because . This is not true, as is a functional of r, , so . One possible solution would be to formally solve the differential equation as and then take the average. I would like to avoid this method, as it is not always aplicable. What would be the correct way to proceed if I would like to get an averaged differential equation that properly reproduces the results of averaging the solution? Note: I know about Novikov's theorem which is sometimes used to solve such stochastic differential equations using two-time correlation functions. However, as far as I am aware, it only works if the stochastic variable is dynamical, i.e. r(t) is sampled by the solution of a stochastic differential equation. In my case, my stochastic variable is static, so only the probability distribution is known.","r P(r) \langle r \rangle=0 \langle r^2 \rangle=r_0 \frac{d}{dt}X(t)=r A X(t) X=(x_1,x_2)^T A 2\times 2 \frac{d}{dt}X=0 \langle r \rangle=0 X X(r,t) \langle r X(r,t) \rangle\neq0 X(t)=e^{rAt}X(0)","['ordinary-differential-equations', 'stochastic-processes', 'average', 'stochastic-differential-equations']"
14,Solving non-homogenous differential equation $\ddot{y}+\frac km\dot{y}=-g\hat{j}$,Solving non-homogenous differential equation,\ddot{y}+\frac km\dot{y}=-g\hat{j},"How to solve this non-homogeneous second-order linear ordinary differential equation $$\ddot{y}+\frac km\dot{y}=-g\hat{j}$$ $\hat{j}$ is just unit vector in $y$ direction. I found the solution to homogenous part $y_h(t)=c_1+c_2e^{-\frac{k}{m}t}$ . By variation of parameters assume $y_p(t)=c_1(t)+c_2(t)e^{-\frac{k}{m}t}$ . I calculated $\dot{y}_p=c_1'(t)+c_2'(t)e^{-\frac{k}{m}t}-\frac{k}{m}c_2(t)e^{-\frac{k}{m}t}$ and $\ddot{y}_p(t)=c_1''(t)+c_2''(t)e^{-\frac{k}{m}t}-c_2'(t)\frac{k}{m}e^{-\frac{k}{m}t}-c_2''(t)\frac{k}{m}e^{-\frac{k}{m}t}+(\frac{k}{m})^2e^{-\frac{k}{m}t}$ , if I didn't make any errors.","How to solve this non-homogeneous second-order linear ordinary differential equation is just unit vector in direction. I found the solution to homogenous part . By variation of parameters assume . I calculated and , if I didn't make any errors.",\ddot{y}+\frac km\dot{y}=-g\hat{j} \hat{j} y y_h(t)=c_1+c_2e^{-\frac{k}{m}t} y_p(t)=c_1(t)+c_2(t)e^{-\frac{k}{m}t} \dot{y}_p=c_1'(t)+c_2'(t)e^{-\frac{k}{m}t}-\frac{k}{m}c_2(t)e^{-\frac{k}{m}t} \ddot{y}_p(t)=c_1''(t)+c_2''(t)e^{-\frac{k}{m}t}-c_2'(t)\frac{k}{m}e^{-\frac{k}{m}t}-c_2''(t)\frac{k}{m}e^{-\frac{k}{m}t}+(\frac{k}{m})^2e^{-\frac{k}{m}t},['ordinary-differential-equations']
15,How to compute $\int\frac1x\Bigl(\sum\limits_{n\ge0}\frac{x^{2n}}{2^{2n}(n!)^2}\Bigr)^{-2}\mathrm dx$?,How to compute ?,\int\frac1x\Bigl(\sum\limits_{n\ge0}\frac{x^{2n}}{2^{2n}(n!)^2}\Bigr)^{-2}\mathrm dx,"The background of $$\int\frac{1}{x}\left(\displaystyle\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^{-2}\text dx$$ is that it yields a second solution to $xy''+y-xy=0$ if you multiply it by the series in the integrand (first solution). My professor found an aproximate answer by multiplying the two series up to $5$ th term and then he did long division with $1$ as numerator and the series squared as denominator. Nonetheless, what I was looking for is an exact solution. I thought about this: $$\frac{1}{\left(\displaystyle\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^2}=\sum_{k\geq 0}b_n x^k\implies 1=\left(\displaystyle\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^2\left(\sum_{k\geq 0}b_n x^k\right),$$ where $$ \begin{aligned} \left(\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^2 &=\left(\sum_{i\geq 0}\frac{x^{2i}}{2^{2i}(i!)^2}\right)\left(\sum_{j\geq 0}\frac{x^{2j}}{2^{2j}(j!)^2}\right)\\ &=\sum_{i\geq 0}\sum_{j\geq 0}\frac{x^{2(i+j)}}{2^{2(i+j)}(i!j!)^2}\\ &\overset{n=i+j}{=}\sum_{n\geq 0}\sum_{i= 0}^n\frac{x^{2n}}{2^{2n}(i!(n-i)!)^2}\\ &=\sum_{n\geq 0}a_nx^{2n}, \end{aligned} $$ and thus, $$ \left(\sum_{n\geq 0}a_n x^{2n}\right)\left(\sum_{k\geq 0}b_k x^k\right)=\sum_{n\geq 0}\sum_{k\geq 0}a_nb_kx^{2n+k}=\sum_{m\geq 0}\sum_{n=0}^{\lfloor m/2\rfloor}a_nb_{m-2n}x^m $$ Finally, equating cofficients we find all $b_k$ and hence the antiderivative is simply $$\int\frac{1}{x}\sum_{k\geq 0}b_kx^k\text dx=\int\left(\frac{b_0}{x}+\sum_{k\geq 0}b_{k+1}x^k\right)\text dx=b_0\ln x +\sum_{k\geq 0}\frac{b_{k+1}}{k+1}x^{k+1}+C$$","The background of is that it yields a second solution to if you multiply it by the series in the integrand (first solution). My professor found an aproximate answer by multiplying the two series up to th term and then he did long division with as numerator and the series squared as denominator. Nonetheless, what I was looking for is an exact solution. I thought about this: where and thus, Finally, equating cofficients we find all and hence the antiderivative is simply","\int\frac{1}{x}\left(\displaystyle\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^{-2}\text dx xy''+y-xy=0 5 1 \frac{1}{\left(\displaystyle\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^2}=\sum_{k\geq 0}b_n x^k\implies 1=\left(\displaystyle\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^2\left(\sum_{k\geq 0}b_n x^k\right), 
\begin{aligned}
\left(\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^2 &=\left(\sum_{i\geq 0}\frac{x^{2i}}{2^{2i}(i!)^2}\right)\left(\sum_{j\geq 0}\frac{x^{2j}}{2^{2j}(j!)^2}\right)\\
&=\sum_{i\geq 0}\sum_{j\geq 0}\frac{x^{2(i+j)}}{2^{2(i+j)}(i!j!)^2}\\
&\overset{n=i+j}{=}\sum_{n\geq 0}\sum_{i= 0}^n\frac{x^{2n}}{2^{2n}(i!(n-i)!)^2}\\
&=\sum_{n\geq 0}a_nx^{2n},
\end{aligned}
 
\left(\sum_{n\geq 0}a_n x^{2n}\right)\left(\sum_{k\geq 0}b_k x^k\right)=\sum_{n\geq 0}\sum_{k\geq 0}a_nb_kx^{2n+k}=\sum_{m\geq 0}\sum_{n=0}^{\lfloor m/2\rfloor}a_nb_{m-2n}x^m
 b_k \int\frac{1}{x}\sum_{k\geq 0}b_kx^k\text dx=\int\left(\frac{b_0}{x}+\sum_{k\geq 0}b_{k+1}x^k\right)\text dx=b_0\ln x +\sum_{k\geq 0}\frac{b_{k+1}}{k+1}x^{k+1}+C","['integration', 'ordinary-differential-equations', 'power-series']"
16,Technique for generating Lie point symmetries,Technique for generating Lie point symmetries,,"Consider I believe that there is something wrong with this text. In particular, how is $$\Delta=0 \quad \Longrightarrow \quad V(\Delta)=0$$ completely non trivial by linearity of operators? Moreover, I do not understand how this is used to find lie point symmetries. More context For context, the text also mentions so I assume that we plug such $V$ in $$ V(\Delta)=0$$ and then try to solve for the coefficients. But again, I do not understand why all coefficient dont simply work. Question: Why is $$\Delta=0 \quad \Longrightarrow \quad V(\Delta)=0$$ not true for any $V$ ? Moreover, can someone give me a simple example of how this technique is used to generate lie point symmetries?","Consider I believe that there is something wrong with this text. In particular, how is completely non trivial by linearity of operators? Moreover, I do not understand how this is used to find lie point symmetries. More context For context, the text also mentions so I assume that we plug such in and then try to solve for the coefficients. But again, I do not understand why all coefficient dont simply work. Question: Why is not true for any ? Moreover, can someone give me a simple example of how this technique is used to generate lie point symmetries?",\Delta=0 \quad \Longrightarrow \quad V(\Delta)=0 V  V(\Delta)=0 \Delta=0 \quad \Longrightarrow \quad V(\Delta)=0 V,"['ordinary-differential-equations', 'partial-differential-equations', 'lie-algebras', 'integrable-systems']"
17,"Why is this differential equation's interval not (-$\infty$, $\infty$)","Why is this differential equation's interval not (-, )",\infty \infty,"I am currently using A First Course in Differential Equations with Modeling Applications, 10th Edition, by Dennis G. Zill. Section 2.3 Question #9. Find the general solution of the given differential equation. Give the largest interval $I$ over which the general solution is defined. Determine whether there are any transient terms in the general solution. The differential equation given was: $$ x\frac{dy}{dx} - y = x^2 \sin(x) $$ Here is my work: Divide by $x$ to get the standard form: $$ \frac{dy}{dx} - \frac{1}{x} y = x \sin(x) $$ Implement integrating factor: $$ \mu = \exp\biggl(\int{-\frac{1}{x}}\,dx\biggr)  = e^{-\ln(x)} = e^{\ln(x^{-1})} = \frac{1}{x} $$ Multiply both sides by the integrating factor: $$ \frac{dy}{dx}\frac{1}{x} - \frac{1}{x^2}y = \sin(x) $$ Notice the derivative of a product: $$ \frac{dy}{dx} \biggl[ y \, \frac{1}{x} \biggr] = \sin(x) $$ Take the integral of both sides: $$ \int \frac{dy}{dx} \biggl[ \frac{y}{x} \biggr] \, dx  = \int \sin(x) \, dx  \quad\implies\quad  \frac{y}{x} = -\cos(x) + C $$ Multiply both sides by $x$ : $$ y = -x\cos(x) + Cx  \quad\text{or}\quad  y = Cx - x\cos(x) $$ My answer and the book's answer matched but the back of the book had $(0, \infty)$ as the interval. My question is why the interval is not $(-\infty, \infty)$ when $y$ is defined and continuous for all $x$ .","I am currently using A First Course in Differential Equations with Modeling Applications, 10th Edition, by Dennis G. Zill. Section 2.3 Question #9. Find the general solution of the given differential equation. Give the largest interval over which the general solution is defined. Determine whether there are any transient terms in the general solution. The differential equation given was: Here is my work: Divide by to get the standard form: Implement integrating factor: Multiply both sides by the integrating factor: Notice the derivative of a product: Take the integral of both sides: Multiply both sides by : My answer and the book's answer matched but the back of the book had as the interval. My question is why the interval is not when is defined and continuous for all .","I 
x\frac{dy}{dx} - y = x^2 \sin(x)
 x 
\frac{dy}{dx} - \frac{1}{x} y = x \sin(x)
 
\mu = \exp\biggl(\int{-\frac{1}{x}}\,dx\biggr) 
= e^{-\ln(x)} = e^{\ln(x^{-1})} = \frac{1}{x}
 
\frac{dy}{dx}\frac{1}{x} - \frac{1}{x^2}y = \sin(x)
 
\frac{dy}{dx} \biggl[ y \, \frac{1}{x} \biggr] = \sin(x)
 
\int \frac{dy}{dx} \biggl[ \frac{y}{x} \biggr] \, dx 
= \int \sin(x) \, dx 
\quad\implies\quad 
\frac{y}{x} = -\cos(x) + C
 x 
y = -x\cos(x) + Cx 
\quad\text{or}\quad 
y = Cx - x\cos(x)
 (0, \infty) (-\infty, \infty) y x","['real-analysis', 'ordinary-differential-equations', 'analysis', 'continuity']"
18,Find all $a$ for a unique solution of $y'=[\cos(x+2y)+ay]^{\frac{4}{5}}$ with $y(0)=2\pi$,Find all  for a unique solution of  with,a y'=[\cos(x+2y)+ay]^{\frac{4}{5}} y(0)=2\pi,"Find all values of $a\in \mathbb{R}$ for which the IVP $y'=[\cos(x+2y)+ay]^{\frac{4}{5}}$ with $y(0)=2\pi$ has solution in some neighbourhood of $(x_0,y_0)=(0,2\pi)$ . For which values of $a$ is this solution unique? Attempt Let $D=\{(x,y):|x|\leqslant A,~|y-2\pi|\leqslant B\}$ and $f(x,y)=[\cos(x+2y)+ay]^{\frac{4}{5}}$ is continuous on $D$ as long as $\cos(x+2y)+ay\geqslant 0$ for all $(x,y)\in D$ . In that case,  theorem of Peano guarantees the existence of a solution, as wanted. To find all $a$ though, we have $a\geqslant -\min_{(x,y)\in D}\frac{\cos(x+2y)}{y}$ , which implies two issues: $\bullet$ how could we evaluate the above minimum? $\bullet$ even if we evaluate the min above, it would depend on the values $A,B$ of the rectangle $D$ . I would expect an answer independent of $A,\,B$ , meaning that we could work for all regions of $(x_0,y_0)=(0,2\pi)$ . For the next part of course, we could apply the Picard - Lindelof theorem, as long as $f_y(x,y)=\frac45\,[\cos(x+2y)+ay]^{-\frac{1}{5}} [a-2\sin(x+2y)]$ is bounded on $D$ , that is $\cos(x+2y)+ay>0$ for all $(x,y)\in D$ . Thanks for the help.","Find all values of for which the IVP with has solution in some neighbourhood of . For which values of is this solution unique? Attempt Let and is continuous on as long as for all . In that case,  theorem of Peano guarantees the existence of a solution, as wanted. To find all though, we have , which implies two issues: how could we evaluate the above minimum? even if we evaluate the min above, it would depend on the values of the rectangle . I would expect an answer independent of , meaning that we could work for all regions of . For the next part of course, we could apply the Picard - Lindelof theorem, as long as is bounded on , that is for all . Thanks for the help.","a\in \mathbb{R} y'=[\cos(x+2y)+ay]^{\frac{4}{5}} y(0)=2\pi (x_0,y_0)=(0,2\pi) a D=\{(x,y):|x|\leqslant A,~|y-2\pi|\leqslant B\} f(x,y)=[\cos(x+2y)+ay]^{\frac{4}{5}} D \cos(x+2y)+ay\geqslant 0 (x,y)\in D a a\geqslant -\min_{(x,y)\in D}\frac{\cos(x+2y)}{y} \bullet \bullet A,B D A,\,B (x_0,y_0)=(0,2\pi) f_y(x,y)=\frac45\,[\cos(x+2y)+ay]^{-\frac{1}{5}} [a-2\sin(x+2y)] D \cos(x+2y)+ay>0 (x,y)\in D",['ordinary-differential-equations']
19,One problem about a barrier,One problem about a barrier,,"Suppose you have two points $A(0,0)$ and $B(L,0)$ on a plane $\mathbb{R}^2$ and $AB$ is a perpendicilar bisector of a straight barrier $\Omega=\{\ (x,y)\ |\ x=L/2,y\in [-\frac{a}{2},\frac{a}{2}] \}$ , also, $A$ and $B$ are standing on equal distances from $\Omega$ . So, let us call the middle point of $\Omega$ $O(L/2,0)$ , then $AO=BO=L/2$ . You want to move from $A$ to $B$ by the shortest line with constant velocity , but your moving point only has information about the barrier in an $\varepsilon$ -disc around itself. Thus, the trajectory of a moving point can be determined only in some neighbourhood of a moving point. For example, if $\varepsilon=\infty$ (it means that you can see the straight segment fully), you can move (without loss of generality due to the symmetry within the problem) from $A$ to the 'highest' point of $\Omega$ by a straight segment and then to $B$ in the same manner. In the case when $\varepsilon=L/2$ (it means that in the beginning of the motion we can see only the middle point of $\Omega$ ) the problem gets much more interesting! Our point will not be able to move by a straight segment to the peak of $\Omega$ as it will not have the full information about where the peak is. So there appears some curve from $A$ heading to the peak. The algorithm of finding the desirable path is as following: Move forfard, towards $\Omega$ , a bit, by a step $ds$ ; Find two points (one of which we can omit by the symmetry) of the intersection of the $\varepsilon$ -circle (to remind, $\varepsilon=L/2$ ); From one of the found points build a segment to the point $(ds,0)$ ; Move a bit, by $ds$ , in the direction of the segment built in (3); Repeat (1) to (4). This way seems to be extra-intuitive and informal. My question is: how to describe the path in terms of differential equations? I am attaching my considerations below, so feel free to check and fix them. Let us be searching the path in a parametric form $(x(t),y(t))$ . Firstly, $ds=\sqrt{\dot{x}^2+\dot{y}^2}dt$ . Secondly, by stepping by $ds$ towards $\Omega$ for the first time (at the zero iteration) our $\varepsilon$ -circle intersects the barrier in the points $$y=\sqrt{L^2-(L-ds)^2}.$$ Having these facts in the arsenal, we can conclude that at the first iteration we will be moving by the segment $(L-(L-(ds+dx_1))),y-dy_1)$ , where $dx_1$ and $dy_1$ stand for the translations in $x$ and $y$ after the zero iteration. After some algebraic transformations and saying that $dx_1=\dot{x},dy_1=\dot{y}$ I get the following equation: $$\dot{y}-\ddot{y}=\sqrt{2\ddot{x}\sqrt{\dot{x}^2+\dot{y}^2}(L-1)-2\dot{x}\ddot{x}}.$$ But one needs some other equation to make up a system of two equations determine the trajectory. By the way, at the moment $x_0=L-\sqrt{L^2-a^2/4}$ we will be moving to the peak by a straight segment as at this moment we will be able to see it.","Suppose you have two points and on a plane and is a perpendicilar bisector of a straight barrier , also, and are standing on equal distances from . So, let us call the middle point of , then . You want to move from to by the shortest line with constant velocity , but your moving point only has information about the barrier in an -disc around itself. Thus, the trajectory of a moving point can be determined only in some neighbourhood of a moving point. For example, if (it means that you can see the straight segment fully), you can move (without loss of generality due to the symmetry within the problem) from to the 'highest' point of by a straight segment and then to in the same manner. In the case when (it means that in the beginning of the motion we can see only the middle point of ) the problem gets much more interesting! Our point will not be able to move by a straight segment to the peak of as it will not have the full information about where the peak is. So there appears some curve from heading to the peak. The algorithm of finding the desirable path is as following: Move forfard, towards , a bit, by a step ; Find two points (one of which we can omit by the symmetry) of the intersection of the -circle (to remind, ); From one of the found points build a segment to the point ; Move a bit, by , in the direction of the segment built in (3); Repeat (1) to (4). This way seems to be extra-intuitive and informal. My question is: how to describe the path in terms of differential equations? I am attaching my considerations below, so feel free to check and fix them. Let us be searching the path in a parametric form . Firstly, . Secondly, by stepping by towards for the first time (at the zero iteration) our -circle intersects the barrier in the points Having these facts in the arsenal, we can conclude that at the first iteration we will be moving by the segment , where and stand for the translations in and after the zero iteration. After some algebraic transformations and saying that I get the following equation: But one needs some other equation to make up a system of two equations determine the trajectory. By the way, at the moment we will be moving to the peak by a straight segment as at this moment we will be able to see it.","A(0,0) B(L,0) \mathbb{R}^2 AB \Omega=\{\ (x,y)\ |\ x=L/2,y\in [-\frac{a}{2},\frac{a}{2}] \} A B \Omega \Omega O(L/2,0) AO=BO=L/2 A B \varepsilon \varepsilon=\infty A \Omega B \varepsilon=L/2 \Omega \Omega A \Omega ds \varepsilon \varepsilon=L/2 (ds,0) ds (x(t),y(t)) ds=\sqrt{\dot{x}^2+\dot{y}^2}dt ds \Omega \varepsilon y=\sqrt{L^2-(L-ds)^2}. (L-(L-(ds+dx_1))),y-dy_1) dx_1 dy_1 x y dx_1=\dot{x},dy_1=\dot{y} \dot{y}-\ddot{y}=\sqrt{2\ddot{x}\sqrt{\dot{x}^2+\dot{y}^2}(L-1)-2\dot{x}\ddot{x}}. x_0=L-\sqrt{L^2-a^2/4}","['calculus', 'geometry', 'ordinary-differential-equations']"
20,Find distribution solution of differential equation,Find distribution solution of differential equation,,I want to find solution of the equation $T' +aT =0$ where $T \in D'(\mathbb{R})$ and $a \in \mathbb{R}$ . What I have done so far: $$\frac{d}{dx}(e^{ax}T) = a e^{ax}T + e^{ax} \frac{dT}{dx} = e^{ax} \left(aT +  \frac{dT}{dx} \right) = 0$$ This means that the distribution $e^{ax}T$ is induced by some constant $c$ .,I want to find solution of the equation where and . What I have done so far: This means that the distribution is induced by some constant .,T' +aT =0 T \in D'(\mathbb{R}) a \in \mathbb{R} \frac{d}{dx}(e^{ax}T) = a e^{ax}T + e^{ax} \frac{dT}{dx} = e^{ax} \left(aT +  \frac{dT}{dx} \right) = 0 e^{ax}T c,"['functional-analysis', 'ordinary-differential-equations', 'distribution-theory']"
21,How to show this perturbed matrix also has negative real parts for $\delta$ small enough,How to show this perturbed matrix also has negative real parts for  small enough,\delta,"Based on this question: Show that the $x_\delta$ is asymptotically stable of the following system ODE . Consider the following nonlinear system of ODE $$ x'(t)=f(x(t))+\delta g(x(t)), x\in R^N $$ where $f$ and $g$ are two smooth vector fields and $\delta$ is a parameter. Suppose that as $\delta=0$ , the system has a hyperbolic and asymptotically stable equilibrium $x_0$ . That means eigenvalues of its linearization have negative real parts. By implicit function theorem with $\phi(\delta,x) = f(x)+ \delta g(x)$ shows that for small $\delta$ the perturbed system has an equilibrium point $x_\delta$ , and $\lim_{\delta \to 0} x_\delta = x_0$ . Show that for small(er) $\delta$ , the eigenvalues of $A_\delta:={\partial \phi(\delta, x_\delta) \over \partial x}$ also have negative real parts. I am confused about how to show this perturbed matrix also has negative real parts for $\delta$ small enough. We know that as $\delta=0$ , the matrix $$A=\lim_{\delta\to 0}A_\delta=\frac{\partial f(x_0)}{\partial x}$$ has negative real parts. It seems one problem from Linear algebra...","Based on this question: Show that the $x_\delta$ is asymptotically stable of the following system ODE . Consider the following nonlinear system of ODE where and are two smooth vector fields and is a parameter. Suppose that as , the system has a hyperbolic and asymptotically stable equilibrium . That means eigenvalues of its linearization have negative real parts. By implicit function theorem with shows that for small the perturbed system has an equilibrium point , and . Show that for small(er) , the eigenvalues of also have negative real parts. I am confused about how to show this perturbed matrix also has negative real parts for small enough. We know that as , the matrix has negative real parts. It seems one problem from Linear algebra...","
x'(t)=f(x(t))+\delta g(x(t)), x\in R^N
 f g \delta \delta=0 x_0 \phi(\delta,x) = f(x)+ \delta g(x) \delta x_\delta \lim_{\delta \to 0} x_\delta = x_0 \delta A_\delta:={\partial \phi(\delta, x_\delta) \over \partial x} \delta \delta=0 A=\lim_{\delta\to 0}A_\delta=\frac{\partial f(x_0)}{\partial x}","['linear-algebra', 'ordinary-differential-equations']"
22,Proving that the zero solution of a linear periodic ODE system is unstable,Proving that the zero solution of a linear periodic ODE system is unstable,,"Prove that the zero solution is unstable for the system $x' = A(t)x$ with $A = \begin{pmatrix} \frac{1}{2} - \cos(t) & 12 \\ 147 & \frac{3}{2} + \sin(t)\end{pmatrix}$ . I've tried the following: The system is equivalent to $x' = Bx + D(t)x$ , with $B= \begin{pmatrix} \frac{1}{2} & 12 \\ 147 & \frac{3}{2} \end{pmatrix}$ and $D(t) = \text{diag}(-\cos(t), \sin(t))$ , then find the eigenvalues of $B$ and notice that one of them is positive. Therefore the linear system $x' = Bx$ has unstable zero solution. I'm unsure however on how to translate that into instability of the original system, since the system is not autonomous. The question is in the Floquet theory section of the book so presumably there is a way to find/involve the characteristic multipliers/exponents. Any help is appreciated.","Prove that the zero solution is unstable for the system with . I've tried the following: The system is equivalent to , with and , then find the eigenvalues of and notice that one of them is positive. Therefore the linear system has unstable zero solution. I'm unsure however on how to translate that into instability of the original system, since the system is not autonomous. The question is in the Floquet theory section of the book so presumably there is a way to find/involve the characteristic multipliers/exponents. Any help is appreciated.","x' = A(t)x A = \begin{pmatrix} \frac{1}{2} - \cos(t) & 12 \\ 147 & \frac{3}{2} + \sin(t)\end{pmatrix} x' = Bx + D(t)x B= \begin{pmatrix} \frac{1}{2} & 12 \\ 147 & \frac{3}{2} \end{pmatrix} D(t) = \text{diag}(-\cos(t), \sin(t)) B x' = Bx","['ordinary-differential-equations', 'periodic-functions', 'stability-theory']"
23,"Reduction of Order to solve, but can't get right answer","Reduction of Order to solve, but can't get right answer",,"Given the equation y'' + 2y' + y = 0  and the solution  y1 = ${xe}^{-x}$ . Solve for a second solution $y_2$ . I've solved it twice and get $y_2 = {xe}^{-x}(-{x}^{-1}+c)$ , but that's not being counted as correct. I distributed the ${xe}^{-x}$ in my actual answer. Can someone please walk me through how to get the solution? I solved once using $y_2 = y_1(x)\int\frac{e^{-\int P(x) dx}}{y_1^2}$ and again using $y_2 = y_1(x)u(x)$ .","Given the equation y'' + 2y' + y = 0  and the solution  y1 = . Solve for a second solution . I've solved it twice and get , but that's not being counted as correct. I distributed the in my actual answer. Can someone please walk me through how to get the solution? I solved once using and again using .",{xe}^{-x} y_2 y_2 = {xe}^{-x}(-{x}^{-1}+c) {xe}^{-x} y_2 = y_1(x)\int\frac{e^{-\int P(x) dx}}{y_1^2} y_2 = y_1(x)u(x),"['ordinary-differential-equations', 'reduction-of-order-ode']"
24,Existence a uniqueness theorems for ODEs: two proofs compared,Existence a uniqueness theorems for ODEs: two proofs compared,,"Lately I've been studying the existence [and uniqueness] theorem for ODEs. As is often the case with beautiful theorems, there are several proofs of this fact. Today I want to focus on two elementary proofs, and compare them somewhat. First of all, let's write down exactly what the theorem says in the case of a non-autonomous vector field. Theorem . Let $ E $ be a normed space ( $ \mathbb R^n $ will suffice). Let $ U\subset E $ be open. Let $ I\subset \mathbb R $ be an open interval. Let $ X\colon U\times I\to E $ be continuous and locally uniformly Lipschitz in the second variable. Let $ x_0\in U $ and $ t_0\in E $ . Then there exists an open ball $ B\subset U $ around $ x_0 $ , an open interval $ J = \left]t_0-\alpha,t_0+\alpha\right[\subset I $ around $ t_0 $ , and a unique integral curve $ x\colon J\to U $ of $ X $ such that $ x(t_0) = x_0 $ and such that $ x(t)\in B $ for every $ t\in J $ . A common ground Both the proofs I have in mind start by noticing that there exists an open ball $ B(x_0,\epsilon)\subset U $ and an open interval $ \left]t_0 - \delta,t_0 + \delta\right[\subset I $ where $ \lVert X(x,t) \rVert\leqq M $ for some $ M > 0 $ . This is kinda easy to prove. Moreover, let's call $ K_0 $ some local Lipschitz constant of $ X $ around $ (x_0,t_0) $ . The first proof Let $ \alpha $ be such that $ \alpha < \epsilon/M $ and $ \alpha < \delta $ . One way to prove the Theorem is to define inductively $ x_n\colon J = \left]t_0 - \alpha,t_0 + \alpha\right[\to U $ for every $ n\in \mathbb N $ by $$ x_0(t) = x_0\text{,}\qquad x_{n + 1}(t) = x_0 + \int_{t_0}^t X(x_n(\zeta),\zeta)\,\mathrm d\zeta\text{.} $$ By induction then $ x_n(t)\in B(x_0,\epsilon) $ and $$ \lVert x_n(t) - x_{n + 1}(t)\rVert\leqq \frac{MK^n}{(n + 1)!}\lvert t - t_0\rvert\text{,} $$ thus the $ x_n $ s form a uniformly Cauchy sequence that converges to an integral curve of $ X $ . The second proof The second proof I had in mind uses the celebrated Banach fixed point theorem, or rather one of its corollaries. We basically define the operator $$ T\colon H\to K\text{,}\qquad (Tx)(t) = x_0 + \int_{t_0}^t X(x(\zeta),\zeta)\,\mathrm d\zeta $$ where $ K $ is the space of bounded continuous functions $ x\colon J\to U $ equipped with the sup-norm $ \lVert\phantom{x}\rVert_\infty $ , and $ H $ is the open ball of radius $ \epsilon $ in $ K $ around the constant function $ \bar x_0 $ at $ x_0 $ . It is a general easy fact that if $ \lVert Tx - \bar x_0\rVert_\infty < (1 - C)\epsilon $ , where $ C $ is the contraction constant of $ T $ , then $ T $ has a fixed point that lies in $ H $ . A couple of questions The first one . The estimate to be made on $ \alpha $ in the second proof seems stronger. For the fixed point argument to work we need, in order: $ \alpha < \delta $ , $ \alpha < \epsilon/M $ (this ensures that $ Tx $ is again a bounded continuous mapping and thus lies in $ K $ ), and some other condition related to the fact that $ \lVert Tx - \bar x_0\rVert_\infty < (1 - C)\epsilon $ must hold. For example, Loomis&Sternberg's Advanced Calculus claims that $ \alpha $ should be smaller than $ \epsilon/(M + C\epsilon) $ . On the other hand, the first proof seems to work for all $ \alpha < \delta $ and $ \alpha < \epsilon/M $ . This surprises me. Did I do something wrong? The second one . This is more philosophic. I don't know anything about computational mathematics, but I was wondering: If one of these two iterative proofs were to be implemented on a computer, which one would it be? What's the differences between the two approaches. Or, on the contrary: How are they similar? I will follow a Dynamical Systems course next term and I hope it will clarify me some doubts, but for now I'm just leaving it there.","Lately I've been studying the existence [and uniqueness] theorem for ODEs. As is often the case with beautiful theorems, there are several proofs of this fact. Today I want to focus on two elementary proofs, and compare them somewhat. First of all, let's write down exactly what the theorem says in the case of a non-autonomous vector field. Theorem . Let be a normed space ( will suffice). Let be open. Let be an open interval. Let be continuous and locally uniformly Lipschitz in the second variable. Let and . Then there exists an open ball around , an open interval around , and a unique integral curve of such that and such that for every . A common ground Both the proofs I have in mind start by noticing that there exists an open ball and an open interval where for some . This is kinda easy to prove. Moreover, let's call some local Lipschitz constant of around . The first proof Let be such that and . One way to prove the Theorem is to define inductively for every by By induction then and thus the s form a uniformly Cauchy sequence that converges to an integral curve of . The second proof The second proof I had in mind uses the celebrated Banach fixed point theorem, or rather one of its corollaries. We basically define the operator where is the space of bounded continuous functions equipped with the sup-norm , and is the open ball of radius in around the constant function at . It is a general easy fact that if , where is the contraction constant of , then has a fixed point that lies in . A couple of questions The first one . The estimate to be made on in the second proof seems stronger. For the fixed point argument to work we need, in order: , (this ensures that is again a bounded continuous mapping and thus lies in ), and some other condition related to the fact that must hold. For example, Loomis&Sternberg's Advanced Calculus claims that should be smaller than . On the other hand, the first proof seems to work for all and . This surprises me. Did I do something wrong? The second one . This is more philosophic. I don't know anything about computational mathematics, but I was wondering: If one of these two iterative proofs were to be implemented on a computer, which one would it be? What's the differences between the two approaches. Or, on the contrary: How are they similar? I will follow a Dynamical Systems course next term and I hope it will clarify me some doubts, but for now I'm just leaving it there."," E   \mathbb R^n   U\subset E   I\subset \mathbb R   X\colon U\times I\to E   x_0\in U   t_0\in E   B\subset U   x_0   J = \left]t_0-\alpha,t_0+\alpha\right[\subset I   t_0   x\colon J\to U   X   x(t_0) = x_0   x(t)\in B   t\in J   B(x_0,\epsilon)\subset U   \left]t_0 - \delta,t_0 + \delta\right[\subset I   \lVert X(x,t) \rVert\leqq M   M > 0   K_0   X   (x_0,t_0)   \alpha   \alpha < \epsilon/M   \alpha < \delta   x_n\colon J = \left]t_0 - \alpha,t_0 + \alpha\right[\to U   n\in \mathbb N  
x_0(t) = x_0\text{,}\qquad x_{n + 1}(t) = x_0 + \int_{t_0}^t X(x_n(\zeta),\zeta)\,\mathrm d\zeta\text{.}
  x_n(t)\in B(x_0,\epsilon)  
\lVert x_n(t) - x_{n + 1}(t)\rVert\leqq \frac{MK^n}{(n + 1)!}\lvert t - t_0\rvert\text{,}
  x_n   X  
T\colon H\to K\text{,}\qquad (Tx)(t) = x_0 + \int_{t_0}^t X(x(\zeta),\zeta)\,\mathrm d\zeta
  K   x\colon J\to U   \lVert\phantom{x}\rVert_\infty   H   \epsilon   K   \bar x_0   x_0   \lVert Tx - \bar x_0\rVert_\infty < (1 - C)\epsilon   C   T   T   H   \alpha   \alpha < \delta   \alpha < \epsilon/M   Tx   K   \lVert Tx - \bar x_0\rVert_\infty < (1 - C)\epsilon   \alpha   \epsilon/(M + C\epsilon)   \alpha < \delta   \alpha < \epsilon/M ","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
25,Approximating ODE solution with polynomial function,Approximating ODE solution with polynomial function,,"I am trying to decipher the method for generating non-linear frequency-modulated signals described in this paper by A. W. Doerry .  On page 5, there is an algorithm described that boils down to approximating a solution to the following differential equation $$\omega'(t)=\frac{\omega'(0)}{W(\omega(t)-\omega(0))}$$ with the constraint of $$\int_{-T/2}^{+T/2}\omega'(t)\,dt=\Omega.$$ A few easy to work with forms for $\omega(t)$ are given.  The one I'm focusing on is the polynomial form. $$\tilde\omega(t)=\sum_{n=1}^N c_n\, t^{n-1} $$ The suggested iterative procedure for finding the polynomial (or whichever form) function fit is Start with some initial $\tilde\omega'(t)$ Integrate $\tilde\omega'(t)$ to get $\tilde\omega(t)$ Adjust $\tilde\omega'(t)$ and $\tilde\omega(t)$ to meet $\Omega$ constraint Calculate $W(\tilde\omega(t)-\tilde\omega(0))$ and the new $\tilde\omega'(t)$ Repeat steps 2-4 until the solution converges I am paraphrasing some of this and simplifying the notation a bit.  Either way, I interpreted the above to mean start with some polynomial coefficients, fit the polynomial coefficients in $\tilde\omega'(t)$ to $\tilde\omega'(0)/W(\tilde\omega(t)-\tilde\omega(0))$ based on the coefficients you've got, adjust for constraints, and repeat.  This does not appear to work and I don't really see how it would. Am I interpreting this wrong?  Is the algorithm in this paper referring to some well-known numerical differential equation solution approximation method? Edit $W(x)$ is a weighting function, specifically a sidelobe taper window.  Assume real and symmetric.  For example, a raised cosine window, $$W(x) = 1+\frac{1-\alpha}{\alpha}\cos(Cx)$$ with $C$ being some constant to make the span work out right.  With $\alpha=25/46$ this is the ever-popular Hamming window.","I am trying to decipher the method for generating non-linear frequency-modulated signals described in this paper by A. W. Doerry .  On page 5, there is an algorithm described that boils down to approximating a solution to the following differential equation with the constraint of A few easy to work with forms for are given.  The one I'm focusing on is the polynomial form. The suggested iterative procedure for finding the polynomial (or whichever form) function fit is Start with some initial Integrate to get Adjust and to meet constraint Calculate and the new Repeat steps 2-4 until the solution converges I am paraphrasing some of this and simplifying the notation a bit.  Either way, I interpreted the above to mean start with some polynomial coefficients, fit the polynomial coefficients in to based on the coefficients you've got, adjust for constraints, and repeat.  This does not appear to work and I don't really see how it would. Am I interpreting this wrong?  Is the algorithm in this paper referring to some well-known numerical differential equation solution approximation method? Edit is a weighting function, specifically a sidelobe taper window.  Assume real and symmetric.  For example, a raised cosine window, with being some constant to make the span work out right.  With this is the ever-popular Hamming window.","\omega'(t)=\frac{\omega'(0)}{W(\omega(t)-\omega(0))} \int_{-T/2}^{+T/2}\omega'(t)\,dt=\Omega. \omega(t) \tilde\omega(t)=\sum_{n=1}^N c_n\, t^{n-1}  \tilde\omega'(t) \tilde\omega'(t) \tilde\omega(t) \tilde\omega'(t) \tilde\omega(t) \Omega W(\tilde\omega(t)-\tilde\omega(0)) \tilde\omega'(t) \tilde\omega'(t) \tilde\omega'(0)/W(\tilde\omega(t)-\tilde\omega(0)) W(x) W(x) = 1+\frac{1-\alpha}{\alpha}\cos(Cx) C \alpha=25/46","['ordinary-differential-equations', 'numerical-methods', 'numerical-calculus']"
26,Solving quadratic ODE with a constraint,Solving quadratic ODE with a constraint,,"I am trying to force a constraint on a ODE. I have the following equation: $$ \begin{aligned} \dot x &= -0.04 x + 10 y z \\ \dot y &= 0.04 x - 10 y z - 3000 y^2 \\ \dot z &= 3000 y^2 \end{aligned} $$ This equation was derived assuming a continuity equation where $x+y+z=1$ Instead of solving for $\dot z$ I would like to solve the first two terms and apply the continuity constraint. I think this should be done with a Lagrangian multiplier technique. However, I do not know how to derive the expression. Can anyone  give me a hand with the derivation? I know that for a function $f\left(x,y\right)$ subjected to a constraint $g\left(x,y \right)$ , the lagrangian multiplier expression will be: $$ \left\{\begin{matrix}\nabla f\left(x,y\right)=-\lambda\nabla g\left(x,y\right)\\g\left(x,y\right)=0\end{matrix}\right. $$ Will this still hold for constraints in ODE's?","I am trying to force a constraint on a ODE. I have the following equation: This equation was derived assuming a continuity equation where Instead of solving for I would like to solve the first two terms and apply the continuity constraint. I think this should be done with a Lagrangian multiplier technique. However, I do not know how to derive the expression. Can anyone  give me a hand with the derivation? I know that for a function subjected to a constraint , the lagrangian multiplier expression will be: Will this still hold for constraints in ODE's?"," \begin{aligned} \dot x &= -0.04 x + 10 y z \\ \dot y &= 0.04 x - 10 y z - 3000 y^2 \\ \dot z &= 3000 y^2 \end{aligned}  x+y+z=1 \dot z f\left(x,y\right) g\left(x,y \right) 
\left\{\begin{matrix}\nabla f\left(x,y\right)=-\lambda\nabla g\left(x,y\right)\\g\left(x,y\right)=0\end{matrix}\right.
",['ordinary-differential-equations']
27,"Doubt in solving the initail value problem $2x\frac{dy}{dx}=3(2y-1),y(0)=\frac{1}{2}$?",Doubt in solving the initail value problem ?,"2x\frac{dy}{dx}=3(2y-1),y(0)=\frac{1}{2}","The initial value problem $2x\frac{dy}{dx}=3(2y-1),y(0)=\frac{1}{2}$ has (a) unique solution (b) more than one but finitely many solutions (c) Infinitely many solutions (d) no solutions If I solve $2x\frac{dy}{dx}=3(2y-1),y(0)=\frac{1}{2}$ the equations using variable seperable, I get $\log \frac{|2y-1|}{|x^3|}=c$ implies $e^c=\frac{2y-1}{x^3}$ but how should I move further. Cannot use $y(0)=\frac{1}{2}$ to obtain $c$ , it will require to put zero in the denominator. How should I move further? Thanks in advance!!","The initial value problem has (a) unique solution (b) more than one but finitely many solutions (c) Infinitely many solutions (d) no solutions If I solve the equations using variable seperable, I get implies but how should I move further. Cannot use to obtain , it will require to put zero in the denominator. How should I move further? Thanks in advance!!","2x\frac{dy}{dx}=3(2y-1),y(0)=\frac{1}{2} 2x\frac{dy}{dx}=3(2y-1),y(0)=\frac{1}{2} \log \frac{|2y-1|}{|x^3|}=c e^c=\frac{2y-1}{x^3} y(0)=\frac{1}{2} c",['ordinary-differential-equations']
28,When is a solution of a differential matrix Riccati equation positive definite?,When is a solution of a differential matrix Riccati equation positive definite?,,"The question is in the context of linear control systems. Let $A(t)$ , $R(t)$ , and $Q(t)$ be time-varing square $n\times n$ matrices of reals. For all $t$ , $R(t)\ge 0$ and $Q(t)\ge 0$ are semi positive-definite. Consider the differential matrix Riccati equation $$\dot{X}(t) = A(t) X(t)  + X(t) A^\top(t) - X(t) R(t) X(t) + Q(t)$$ with a positive-definite initial condition $X(0)>0$ . My question are When $X(t)$ is bounded for all $t$ ? Is it enough to say that $R(t)> 0$ or it should be $R(t) \ge R_0 > 0$ ? When $X(t)$ is non-singular for all $t$ ? Is it enough to say that $Q(t)> 0$ or it should be $Q(t) \ge Q_0 > 0$ ? When (necessary, sufficient, iff) does there exist $X_0>0$ such that $X(t) \ge X_0$ for all $t$ ? Usually, control books consider the algebraic equation, i.e. for $\dot{X}(t)=0$ , or start with assumptions on uniform controllability and observability of $R$ and $Q$ implying they are positive definite. Are you aware of books/papers that coniser semi definite cases?","The question is in the context of linear control systems. Let , , and be time-varing square matrices of reals. For all , and are semi positive-definite. Consider the differential matrix Riccati equation with a positive-definite initial condition . My question are When is bounded for all ? Is it enough to say that or it should be ? When is non-singular for all ? Is it enough to say that or it should be ? When (necessary, sufficient, iff) does there exist such that for all ? Usually, control books consider the algebraic equation, i.e. for , or start with assumptions on uniform controllability and observability of and implying they are positive definite. Are you aware of books/papers that coniser semi definite cases?","A(t) R(t) Q(t) n\times n t R(t)\ge 0 Q(t)\ge 0 \dot{X}(t) = A(t) X(t) 
+ X(t) A^\top(t) - X(t) R(t) X(t) + Q(t) X(0)>0 X(t) t R(t)> 0 R(t) \ge R_0 > 0 X(t) t Q(t)> 0 Q(t) \ge Q_0 > 0 X_0>0 X(t) \ge X_0 t \dot{X}(t)=0 R Q","['ordinary-differential-equations', 'matrix-equations', 'matrix-calculus', 'control-theory', 'linear-control']"
29,How to find the u that minimize this integral,How to find the u that minimize this integral,,"I am trying to find u that minimizes: $$\int_0^\infty x^2(t)+u^2(t)dt$$ where the provided information is $x(0)=1$ and: $$\dot x=x^2+u$$ Here is what I have done. I tried to use Euler-Lagurange Equations, and then try to solve this with things for second order nonlinear equation: Because I can't find a way to go through it, I then tried to use another variable to replace $dx/dt$ : but then I stuck as well. Could you tell me how to solve this?","I am trying to find u that minimizes: where the provided information is and: Here is what I have done. I tried to use Euler-Lagurange Equations, and then try to solve this with things for second order nonlinear equation: Because I can't find a way to go through it, I then tried to use another variable to replace : but then I stuck as well. Could you tell me how to solve this?",\int_0^\infty x^2(t)+u^2(t)dt x(0)=1 \dot x=x^2+u dx/dt,"['integration', 'ordinary-differential-equations', 'optimization', 'euler-lagrange-equation']"
30,How to solve the differential equation $y'=y/(y+a)+b$,How to solve the differential equation,y'=y/(y+a)+b,"I am trying to determine the proportion of a quantity $y$ in a mixture of total quantity $y + a$ . I know that the quantity $y$ evolves according to two components: one which is constant $b$ and another one which is proportional to the concentration of $y$ in $y + a$ . I get a weird differential equation where $y'$ depends both on $y$ but also on $1/y$ , so it's not a differential equation per se. Does anyone know how to solve this differential equation or have a trick to make it linear? Thank you so much","I am trying to determine the proportion of a quantity in a mixture of total quantity . I know that the quantity evolves according to two components: one which is constant and another one which is proportional to the concentration of in . I get a weird differential equation where depends both on but also on , so it's not a differential equation per se. Does anyone know how to solve this differential equation or have a trick to make it linear? Thank you so much",y y + a y b y y + a y' y 1/y,['ordinary-differential-equations']
31,Third Order Differential Equation with Variable Coefficient,Third Order Differential Equation with Variable Coefficient,,"I have the following third-order differential equation:- \begin{align*} &(x+1)\frac{d^3y}{dx^3} + 3\frac{d^2y}{dx^2}=0 \\ \\ & \text{such that, $y(1) = \frac{1}{2}$}  \end{align*} I have to show that the solution to the differential equation as series of ascending powers of $(x-1)$ , up to and including the term in $(x-1)^3$ is, \begin{align*} y \approx \frac{1}{2} + \frac{3}{4}(x-1) + \frac{1}{8}(x-1)^2 - \frac{1}{16}(x-1)^3 \end{align*} Can anyone help me solve this?","I have the following third-order differential equation:- I have to show that the solution to the differential equation as series of ascending powers of , up to and including the term in is, Can anyone help me solve this?","\begin{align*}
&(x+1)\frac{d^3y}{dx^3} + 3\frac{d^2y}{dx^2}=0
\\ \\
& \text{such that, y(1) = \frac{1}{2}} 
\end{align*} (x-1) (x-1)^3 \begin{align*}
y \approx \frac{1}{2} + \frac{3}{4}(x-1) + \frac{1}{8}(x-1)^2 - \frac{1}{16}(x-1)^3
\end{align*}","['calculus', 'ordinary-differential-equations', 'boundary-value-problem']"
32,Transformation of a vector field on $\mathbb{R}^2$ into one on the complex plane,Transformation of a vector field on  into one on the complex plane,\mathbb{R}^2,"To gain a better understanding of vector fields and their flow computation, I wanted to experiment with a coordinate transformation: real coordinate in $\mathbb{R}^2$ <-> polar coordinates <-> complex plane. I have no problem with polar coordinates, but I got really confused, about how to deal with the transformation into the complex plane. I have chosen a simple vector field that describes a counterclockwise circular movement: $$ V = -y \partial_x + x \partial_y. $$ Given a starting point $(x_0,y_0) \in \mathbb{R}^2$ , we know that there exist an integral curve $\phi \colon (-\epsilon, \epsilon) \to \mathbb{R}^2$ for some $\epsilon >0$ . It also holds that $$ \phi'(t) = \phi_1'(t) \partial_x + \phi_2'(t) \partial_y = V_{\phi(t)}. $$ So, to find $\phi$ , we need to solve the following system of ODEs: $$\phi_1'(t) = - \phi_2(t),$$ $$\phi_2'(t) = \phi_1(t).$$ A solution for a given initial problem is $$ \phi(t) = (x_0 \cos t - y_0 \sin t, x_0 \sin t + y_0 \cos t) \in \mathbb{R}^2.$$ If we transform this solution into complex numbers, we get that $\phi(t) = (x_0 + i y_0) e^{it} =: z_0  e^{it}$ . I find this form much easier to deal with, so I asked myself, how could I skip all this sine-cosine-computations, in such a way that I just need to solve something like $$\frac{\partial}{\partial t} \phi(t) = i \phi(t).$$ As in the case of the polar coordinate transformation, I started with the transformation of my vector field. Somehow I ended up on the Wiki page reading about the Wirtinger derivative (never heard about it before) and managed to transform my vector field to $$ V = i (z \frac{\partial}{\partial z} - \bar{z} \frac{\partial}{\partial \bar{z}}).$$ After thinking a lot about this new form of the vector field and the result that I wanted to achieve with the complex exponential function, I understood that I have no idea how these two thoughts connect. I have a feeling that I try to combine apples with chairs. I do not know how to get to the point, where I can compare the components of the vector field and the velocity of the curve $\phi$ as I did it in the example above. Or in general, I don't really understand, how to find a flow for this complex vector field. I also don't understand, why I have two ""components"" ( $\partial_z$ and $\partial_\bar{z}$ ). I have thought that I will get a vector field in the form of $V = i z \partial_z$ , in my head, this would somehow connect to the solution that I have. I would gladly appreciate any help in getting a (better) understanding of this problem.","To gain a better understanding of vector fields and their flow computation, I wanted to experiment with a coordinate transformation: real coordinate in <-> polar coordinates <-> complex plane. I have no problem with polar coordinates, but I got really confused, about how to deal with the transformation into the complex plane. I have chosen a simple vector field that describes a counterclockwise circular movement: Given a starting point , we know that there exist an integral curve for some . It also holds that So, to find , we need to solve the following system of ODEs: A solution for a given initial problem is If we transform this solution into complex numbers, we get that . I find this form much easier to deal with, so I asked myself, how could I skip all this sine-cosine-computations, in such a way that I just need to solve something like As in the case of the polar coordinate transformation, I started with the transformation of my vector field. Somehow I ended up on the Wiki page reading about the Wirtinger derivative (never heard about it before) and managed to transform my vector field to After thinking a lot about this new form of the vector field and the result that I wanted to achieve with the complex exponential function, I understood that I have no idea how these two thoughts connect. I have a feeling that I try to combine apples with chairs. I do not know how to get to the point, where I can compare the components of the vector field and the velocity of the curve as I did it in the example above. Or in general, I don't really understand, how to find a flow for this complex vector field. I also don't understand, why I have two ""components"" ( and ). I have thought that I will get a vector field in the form of , in my head, this would somehow connect to the solution that I have. I would gladly appreciate any help in getting a (better) understanding of this problem.","\mathbb{R}^2  V = -y \partial_x + x \partial_y.  (x_0,y_0) \in \mathbb{R}^2 \phi \colon (-\epsilon, \epsilon) \to \mathbb{R}^2 \epsilon >0  \phi'(t) = \phi_1'(t) \partial_x + \phi_2'(t) \partial_y = V_{\phi(t)}.  \phi \phi_1'(t) = - \phi_2(t), \phi_2'(t) = \phi_1(t).  \phi(t) = (x_0 \cos t - y_0 \sin t, x_0 \sin t + y_0 \cos t) \in \mathbb{R}^2. \phi(t) = (x_0 + i y_0) e^{it} =: z_0  e^{it} \frac{\partial}{\partial t} \phi(t) = i \phi(t).  V = i (z \frac{\partial}{\partial z} - \bar{z} \frac{\partial}{\partial \bar{z}}). \phi \partial_z \partial_\bar{z} V = i z \partial_z","['ordinary-differential-equations', 'differential-geometry']"
33,"$(\beta_n)_{n \geq 0}$ converges uniformly to the solution of $x' = F(t,x)$, variation of Picard iteration?","converges uniformly to the solution of , variation of Picard iteration?","(\beta_n)_{n \geq 0} x' = F(t,x)","This is exercise 2.7. from Differential Equations: A Dynamical Systems Approach to Theory and Practice by Marcelo Viana and JosÃ© Espinar. Let $F \colon \mathcal{U} \to \mathbb{R}^n$ be continuous and locally Lipschitz in $x$ in an open subset $\mathcal{U}$ of $\mathbb{R}\times > \mathbb{R}^n$ . Let $K$ be a compact subset of $\mathcal{U}$ which is also convex in the second variable, and let $(t_0, x_0)$ be a point in $K$ . Let $I \subseteq \mathbb{R}$ be any open interval containing $t_0$ and suppose that there exist $x_n \in \mathbb{R}^n, n \geq 0,$ converging to $x_0$ and curves $\beta_n \colon I \to \mathbb{R}^n, n > \geq 0$ such that $$ (t, \beta_n(t)) \in K \quad \text{and}\quad \beta_{n+1}(t) = x_n + \int_{t_0}^t  F(s, \beta_n(s) \,ds  $$ for every $t \in I$ and all $n \geq 0$ . (1) Show that there exists $\varepsilon > 0$ such that, restricted to $(t_0 - \varepsilon, t_0 + \varepsilon)$ , the sequence $(\beta_n)_n$ converges uniformly to the solution of $x' = F(t,x)$ with initial condition $x(t_0) = x_0.$ (2) Deduce that $(\beta_n)_n$ converges uniformly to the solution of $x' = F(t, x)$ with initial condition $x(t_0) = x_0$ in any compact subinterval. I am already struggling with the first item. Note that this is Homework, so I am not looking for a full solution (at this time), but only hints. Picard's Theorem, or the proof of it, should be useful for this. I tried to proceed as in the proof but there are several points where I got stuck. What I have: If I define the space $$ Y = \{\gamma\colon (t_0 - \varepsilon, t_0 + \varepsilon) \to K' \text{ continuous} \mid \sup_{t \in (t_0 - \varepsilon, t_0 + \varepsilon)} |\gamma(t) - x_0| \leq \alpha\}, $$ where $K'$ is the projection of $K$ to $\mathbb{R}^n$ , I can show that the operator defined as $$ \mathcal{L}(\gamma)(t) = x_0 + \int_{t_0}^t F(s, \gamma(s))\, ds $$ is well defined and a contraction for suitably chosen $\varepsilon$ and $\alpha$ . (This is basically just the proof of Picard's Theorem.) The unique fixed point of $\mathcal{L}$ will be the solution of $x' = F(t,x)$ . I was also able to show that for $n$ large enough, $\beta_n \in Y$ . This means in particular that taking such $\beta_n$ and applying the Picard operator, I get something that converges to the solution uniformly, i.e., $$ \mathcal{L}^k(\beta_n) \to \beta $$ as $k \to \infty$ , where I denote by $\beta$ the unique solution of the ODE. However, this is not quite the convergence I want since I would like to have $\beta_n \to \beta$ . I don't see a way how to salvage this. I also tried to define a slightly different operator, namely $$ \mathcal{T}(\gamma)(t) = \gamma(t_0) + \int_{t_0}^t F(s, \gamma(s))\, ds, $$ which would seem to work nicer with the $\beta_n$ since $$ \mathcal{T}(\beta_n)(t) = \beta_n(t_0) + \int_{t_0}^t F(s, \beta_n(s))\, ds = x_{n-1} + \int_{t_0}^t F(s, \beta_n(s))\, ds, $$ however, this last expression is not quite $\beta_{n+1}(t)$ . Further, I can't show (and don't know if it is true) that this is a contraction. I am looking for either a hint how to continue with one of my approaches, or a hint where else to start. Thanks!","This is exercise 2.7. from Differential Equations: A Dynamical Systems Approach to Theory and Practice by Marcelo Viana and JosÃ© Espinar. Let be continuous and locally Lipschitz in in an open subset of . Let be a compact subset of which is also convex in the second variable, and let be a point in . Let be any open interval containing and suppose that there exist converging to and curves such that for every and all . (1) Show that there exists such that, restricted to , the sequence converges uniformly to the solution of with initial condition (2) Deduce that converges uniformly to the solution of with initial condition in any compact subinterval. I am already struggling with the first item. Note that this is Homework, so I am not looking for a full solution (at this time), but only hints. Picard's Theorem, or the proof of it, should be useful for this. I tried to proceed as in the proof but there are several points where I got stuck. What I have: If I define the space where is the projection of to , I can show that the operator defined as is well defined and a contraction for suitably chosen and . (This is basically just the proof of Picard's Theorem.) The unique fixed point of will be the solution of . I was also able to show that for large enough, . This means in particular that taking such and applying the Picard operator, I get something that converges to the solution uniformly, i.e., as , where I denote by the unique solution of the ODE. However, this is not quite the convergence I want since I would like to have . I don't see a way how to salvage this. I also tried to define a slightly different operator, namely which would seem to work nicer with the since however, this last expression is not quite . Further, I can't show (and don't know if it is true) that this is a contraction. I am looking for either a hint how to continue with one of my approaches, or a hint where else to start. Thanks!","F \colon \mathcal{U} \to \mathbb{R}^n x \mathcal{U} \mathbb{R}\times
> \mathbb{R}^n K \mathcal{U} (t_0, x_0) K I \subseteq \mathbb{R} t_0 x_n \in \mathbb{R}^n, n \geq 0, x_0 \beta_n \colon I \to \mathbb{R}^n, n
> \geq 0  (t, \beta_n(t)) \in K \quad \text{and}\quad
\beta_{n+1}(t) = x_n + \int_{t_0}^t  F(s, \beta_n(s) \,ds 
 t \in I n \geq 0 \varepsilon > 0 (t_0 - \varepsilon, t_0 + \varepsilon) (\beta_n)_n x' = F(t,x) x(t_0) = x_0. (\beta_n)_n x' = F(t, x) x(t_0) = x_0 
Y = \{\gamma\colon (t_0 - \varepsilon, t_0 + \varepsilon) \to K' \text{ continuous} \mid \sup_{t \in (t_0 - \varepsilon, t_0 + \varepsilon)} |\gamma(t) - x_0| \leq \alpha\},
 K' K \mathbb{R}^n 
\mathcal{L}(\gamma)(t) = x_0 + \int_{t_0}^t F(s, \gamma(s))\, ds
 \varepsilon \alpha \mathcal{L} x' = F(t,x) n \beta_n \in Y \beta_n 
\mathcal{L}^k(\beta_n) \to \beta
 k \to \infty \beta \beta_n \to \beta 
\mathcal{T}(\gamma)(t) = \gamma(t_0) + \int_{t_0}^t F(s, \gamma(s))\, ds,
 \beta_n 
\mathcal{T}(\beta_n)(t) = \beta_n(t_0) + \int_{t_0}^t F(s, \beta_n(s))\, ds = x_{n-1} + \int_{t_0}^t F(s, \beta_n(s))\, ds,
 \beta_{n+1}(t)","['ordinary-differential-equations', 'analysis', 'uniform-convergence', 'fixed-point-theorems']"
34,Solutions of $dx/dt = \lambda x$ with initial condition,Solutions of  with initial condition,dx/dt = \lambda x,"I'm working through Robinson's Introduction to Ordinary Differential Equations (Section 8.2) and have a few questions. The section is on finding the solution of the simplest possible linear differential equation, $$\frac{dx}{dt} = \lambda x$$ with the initial condition $$x(t_0) = x_0.$$ Here's the first issue I encountered. If $x_0 = 0$ , the author claims $x(t) = 0$ for all time. Could anyone provide a formal proof of this? I can see that $x(t_0) = x_0 = 0$ leads to $\dot{x}(t_0) = 0$ , but why would this imply $x(t) \equiv 0$ ? I can sort of see why this is visually by drawing a phase diagram like the one in figure 8.1: in the case $x_0 = 0$ , the line has zero slope and goes through 0 , so $x$ must be the constant zero solution. However, I'd really appreciate a more detailed explanation. If $x_0 \neq 0$ , he goes on to integrate the equation: $$\int_{x = x_0}^{x(t)} \frac{1}{x}dx = \int_{t=t_0}^t \tau d\tau$$ and arrives at $$ \frac{|x|}{|x_0| } = e^{\lambda(t-t_0)} \tag{$*$}$$ Next he writes: To work out what to do about the modulus signs, the easiest thing is to draw the phase diagram. For the case $\lambda > 0$ this is shown in Figure 8.1, from which we can see that $x(t)$ and $x_0$ have the same sign. It follows that we can remove the modulus signs and multiply up to give $$x = x_0 e^{\lambda(t-t_0)}$$ The figure is reproduced below: The way I understand this is: if $\lambda >0$ , then from the differential equation, $x$ and $\dot{x}$ always have the same sign. So if $x(t_0) = x_0 > 0$ , then $\forall t$ , $x(t) > 0 $ . However, if $x(t_0) = x_0 < 0$ , then $\dot{x}<0$ and hence we will have $x(t) < 0 $ for all $t$ . So $x(t)$ and $x_0$ have the same sign and we can simply remove the modulus signs. The case $\lambda < 0$ was left to the reader. Here is my reasoning: If $x_0 < 0$ , then $\dot{x}(t_0) > 0$ , so $x(t)$ is increasing (at least until $x(t) = 0 \iff \dot{x}(t) = 0 $ ). But from ( $*$ ), $x=0 \iff e^{\lambda(t-t_0)} = 0$ , which is not satisfied by any $t\in \mathbb{R}$ . So $x$ never reaches $0$ . Hence the solution must start negative, be increasing and tend to $0$ : $x = -|x_0| e^{\lambda(t-t_0)} = x_0 e^{\lambda(t-t_0)}$ If $x_0 > 0$ , then $\dot{x}(t_0) < 0$ , so $x(t)$ is decreasing (at least until $x(t) = 0 \iff \dot{x}(t) = 0 $ ). But once again we can't have $x=0$ for any $t$ , so the solutions starts positive, is decreasing and tends to $0$ : $x =  x_0 e^{\lambda(t-t_0)}$ So in every single case ( $\lambda > 0$ or $\lambda <0$ ) we obtain $$x = x_0 e^{\lambda(t-t_0)}$$ Is my reasoning correct? Is there any simpler way to arrive to the same conclusions?","I'm working through Robinson's Introduction to Ordinary Differential Equations (Section 8.2) and have a few questions. The section is on finding the solution of the simplest possible linear differential equation, with the initial condition Here's the first issue I encountered. If , the author claims for all time. Could anyone provide a formal proof of this? I can see that leads to , but why would this imply ? I can sort of see why this is visually by drawing a phase diagram like the one in figure 8.1: in the case , the line has zero slope and goes through 0 , so must be the constant zero solution. However, I'd really appreciate a more detailed explanation. If , he goes on to integrate the equation: and arrives at Next he writes: To work out what to do about the modulus signs, the easiest thing is to draw the phase diagram. For the case this is shown in Figure 8.1, from which we can see that and have the same sign. It follows that we can remove the modulus signs and multiply up to give The figure is reproduced below: The way I understand this is: if , then from the differential equation, and always have the same sign. So if , then , . However, if , then and hence we will have for all . So and have the same sign and we can simply remove the modulus signs. The case was left to the reader. Here is my reasoning: If , then , so is increasing (at least until ). But from ( ), , which is not satisfied by any . So never reaches . Hence the solution must start negative, be increasing and tend to : If , then , so is decreasing (at least until ). But once again we can't have for any , so the solutions starts positive, is decreasing and tends to : So in every single case ( or ) we obtain Is my reasoning correct? Is there any simpler way to arrive to the same conclusions?",\frac{dx}{dt} = \lambda x x(t_0) = x_0. x_0 = 0 x(t) = 0 x(t_0) = x_0 = 0 \dot{x}(t_0) = 0 x(t) \equiv 0 x_0 = 0 x x_0 \neq 0 \int_{x = x_0}^{x(t)} \frac{1}{x}dx = \int_{t=t_0}^t \tau d\tau  \frac{|x|}{|x_0| } = e^{\lambda(t-t_0)} \tag{*} \lambda > 0 x(t) x_0 x = x_0 e^{\lambda(t-t_0)} \lambda >0 x \dot{x} x(t_0) = x_0 > 0 \forall t x(t) > 0  x(t_0) = x_0 < 0 \dot{x}<0 x(t) < 0  t x(t) x_0 \lambda < 0 x_0 < 0 \dot{x}(t_0) > 0 x(t) x(t) = 0 \iff \dot{x}(t) = 0  * x=0 \iff e^{\lambda(t-t_0)} = 0 t\in \mathbb{R} x 0 0 x = -|x_0| e^{\lambda(t-t_0)} = x_0 e^{\lambda(t-t_0)} x_0 > 0 \dot{x}(t_0) < 0 x(t) x(t) = 0 \iff \dot{x}(t) = 0  x=0 t 0 x =  x_0 e^{\lambda(t-t_0)} \lambda > 0 \lambda <0 x = x_0 e^{\lambda(t-t_0)},['ordinary-differential-equations']
35,How to show that the ratio of two linear second-order ODEs that solve $y'' + P(x)y' + Q(x)y = 0$ with a common zero at a point $x$ is constant.,How to show that the ratio of two linear second-order ODEs that solve  with a common zero at a point  is constant.,y'' + P(x)y' + Q(x)y = 0 x,"So far I've figured out $$y_1''(x_0) + p(x_0)y_1'(x_0) = y_2''(x_0) + p(x_0)y_2'(x_0) = 0,$$ and I've thought about integrating or doing something with this to help show that $y_2/y_1$ is constant, but don't think I can since we only know the equation holds at the point $x_0$ . You can get $$\frac{y_1}{y_2} = \frac{y_1''+P(x)y_1}{y_2'' + P(x)y_2}$$ just from plugging in the values, but I don't see how we can use these equations to show it's a constant. If anyone could help me out I would appreciate it - thank you! PS the problem is from Simmons' ""Differential equations with Application and Historical Notes"" (Section 14 exercise 10 - page 113) Edit: I've thought about this a little more, and I think it might have something to do with the fact that a second ODE is uniquely determined by its equation, $y(x_0)$ , and $y'(x_0)$ . Since the equation and $y(x_0)$ are already picked the difference in the two solutions correlates between the difference in $y'(x_0)$ , but I'm still not quite sure how this can help prove the ratio is constant.","So far I've figured out and I've thought about integrating or doing something with this to help show that is constant, but don't think I can since we only know the equation holds at the point . You can get just from plugging in the values, but I don't see how we can use these equations to show it's a constant. If anyone could help me out I would appreciate it - thank you! PS the problem is from Simmons' ""Differential equations with Application and Historical Notes"" (Section 14 exercise 10 - page 113) Edit: I've thought about this a little more, and I think it might have something to do with the fact that a second ODE is uniquely determined by its equation, , and . Since the equation and are already picked the difference in the two solutions correlates between the difference in , but I'm still not quite sure how this can help prove the ratio is constant.","y_1''(x_0) + p(x_0)y_1'(x_0) = y_2''(x_0) + p(x_0)y_2'(x_0) = 0, y_2/y_1 x_0 \frac{y_1}{y_2} = \frac{y_1''+P(x)y_1}{y_2'' + P(x)y_2} y(x_0) y'(x_0) y(x_0) y'(x_0)",['ordinary-differential-equations']
36,"The differential equation $\dot{\mathbf{x}}(t)=M(t)\mathbf{x}(t)$, conditions for a specific type of matrix $M$ to commute at different times","The differential equation , conditions for a specific type of matrix  to commute at different times",\dot{\mathbf{x}}(t)=M(t)\mathbf{x}(t) M,"I have some $N$ x $N$ matrices $M_N(t)$ of the general form $$\tag{1} M_4(t)=\left[\begin{matrix}-f_0 & f_1 & 0 & 0  \\  0 & -f_1 & f_2 & 0  \\ 0 & 0 & -f_2 & f_3  \\ 0 & 0 & 0 & -f_3  \end{matrix}\right] \qquad M_5(t)=\left[\begin{matrix}-f_0 & f_1 & 0 & 0  & 0\\  0 & -f_1 & f_2 & 0 & 0 \\ 0 & 0 & -f_2 & f_3 & 0 \\ 0 & 0 & 0 & -f_3 & f_4 \\ 0 & 0 & 0 &  0 &-f_4 \end{matrix}\right] $$ where each $f_n=f_n(t)$ . From the physics of the problem, none of the $f_n$ may vanish for all $t$ , and they are all non-negative. These matrices arise in studying the differential equation $$\tag{2} \dot{\mathbf{x}}(t)=M(t)\mathbf{x}(t) $$ where $\mathbf{x}(t)$ is an $N$ -tuple $(x_0(t), \ x_1(t), \ \dots, x_{N-1}(t))^\top$ . I would like to study the conditions on $f_n$ under which the matrices $M$ commute at unequal times , that is $$\tag{3} \left[ \int\limits_0^t dt' M(t'), \ M(t)  \right]\stackrel{?}{=}0 $$ where $[ \cdot , \cdot]$ is the commutator. This is important to me because under these conditions the solution to (2) is simply $\mathbf{x}(t)=\exp\left( \int\limits_0^t dt' M(t')\right)\mathbf{x}(0)$ while without condition (3) the solution is a time ordered exponential. By playing around with small $N$ cases in a CAS, I think that condition (3) may only be fulfilled only if $$\tag{4} M(t)=g(t) M_0 $$ where $g(t)$ is a function and $M_0$ is a constant matrix. Obviously (4) is a sufficient condition for (3), but I am uncertain about its necessity. To demonstrate this is a nontrivial claim, note that if we allow any of the $f_n$ to vanish for all $t$ , then $M$ may be partly block diagonalized and (4) is not a necessary condition. Question: Is (4) a necessary condition for (3) to hold with $f_n(t)\neq 0$ , or is there another condition? Related question : given the specific form of $M$ , can the general time ordered solution $\mathbf{x}(t)=\mathcal{T}\exp\left( \int\limits_0^t dt' M(t')\right)\mathbf{x}(0)$ be simplified at all? Here $\mathcal{T}$ denotes the time ordering symbol. Background (not necessary for the question) Equation (2) arises from a master equation governing the number of electrons trapped in a quantum dot. The functions $f_n(t)$ are complicated, but the controlling factor of each is of the form $f_n(t) \approx \exp(e^{\alpha_n t})$","I have some x matrices of the general form where each . From the physics of the problem, none of the may vanish for all , and they are all non-negative. These matrices arise in studying the differential equation where is an -tuple . I would like to study the conditions on under which the matrices commute at unequal times , that is where is the commutator. This is important to me because under these conditions the solution to (2) is simply while without condition (3) the solution is a time ordered exponential. By playing around with small cases in a CAS, I think that condition (3) may only be fulfilled only if where is a function and is a constant matrix. Obviously (4) is a sufficient condition for (3), but I am uncertain about its necessity. To demonstrate this is a nontrivial claim, note that if we allow any of the to vanish for all , then may be partly block diagonalized and (4) is not a necessary condition. Question: Is (4) a necessary condition for (3) to hold with , or is there another condition? Related question : given the specific form of , can the general time ordered solution be simplified at all? Here denotes the time ordering symbol. Background (not necessary for the question) Equation (2) arises from a master equation governing the number of electrons trapped in a quantum dot. The functions are complicated, but the controlling factor of each is of the form","N N M_N(t) \tag{1}
M_4(t)=\left[\begin{matrix}-f_0 & f_1 & 0 & 0  \\
 0 & -f_1 & f_2 & 0  \\
0 & 0 & -f_2 & f_3  \\
0 & 0 & 0 & -f_3 
\end{matrix}\right] \qquad M_5(t)=\left[\begin{matrix}-f_0 & f_1 & 0 & 0  & 0\\
 0 & -f_1 & f_2 & 0 & 0 \\
0 & 0 & -f_2 & f_3 & 0 \\
0 & 0 & 0 & -f_3 & f_4 \\
0 & 0 & 0 &  0 &-f_4
\end{matrix}\right]
 f_n=f_n(t) f_n t \tag{2}
\dot{\mathbf{x}}(t)=M(t)\mathbf{x}(t)
 \mathbf{x}(t) N (x_0(t), \ x_1(t), \ \dots, x_{N-1}(t))^\top f_n M \tag{3}
\left[ \int\limits_0^t dt' M(t'), \ M(t)  \right]\stackrel{?}{=}0
 [ \cdot , \cdot] \mathbf{x}(t)=\exp\left( \int\limits_0^t dt' M(t')\right)\mathbf{x}(0) N \tag{4}
M(t)=g(t) M_0
 g(t) M_0 f_n t M f_n(t)\neq 0 M \mathbf{x}(t)=\mathcal{T}\exp\left( \int\limits_0^t dt' M(t')\right)\mathbf{x}(0) \mathcal{T} f_n(t) f_n(t) \approx \exp(e^{\alpha_n t})","['matrices', 'ordinary-differential-equations', 'matrix-equations', 'mathematical-physics', 'matrix-calculus']"
37,Stochastic model of a RL circuit,Stochastic model of a RL circuit,,"I would like to solve stochastic electric circuits numerically. I already know that I should use Milstein method to solve those systems. However, I'm not sure if the stochastic differential equations are correct, if the modeling is correct. Thus, could you give me a feedback whether or not my reasoning is correct? Consider a series RL circuit . Mathematically, it can be modeled as: \begin{equation} \begin{cases} \dot{I}(t) = \frac{V_L(t)}{L} \\ \dot{V_L}(t) = \dot{V}(t) - R \dot{I}(t) \end{cases} \end{equation} As previously stated, I want it to be a stochastic electric circuit. Thus, all parameters should be random variables. For example, I want them to satisfy: \begin{equation} \begin{cases} L = L_0 + e^{B_t} \\ R = R_0 + e^{B_t} \\ V(t) = V_0 \sin(120 \pi t) + \sigma B_t \end{cases} \end{equation} where $B_t$ stands for brownian motion and the other constants are all real positive ones. With that in mind, Rewrite both equations in differential form: \begin{equation} \begin{cases} dI = \frac{V_L(t)}{L}dt \\ dV_L = dV - R dI \end{cases} \end{equation} Substitute the parameters: \begin{equation} \begin{cases} dI = \frac{V_L(t)}{L_0 + e^{B_t}}dt \\ dV_L = 120 \pi V_0 \cos( 120 \pi t) dt + \sigma dB_t - \underbrace{(R_0 + e^{B_t}) dI}_{?} \end{cases} \end{equation} The part ""?"" feels weird to me. Because I'm used to solve such kind of systems numerically: \begin{equation} d \vec{X} = \vec{F}(t, \vec{X}) dt + \vec{G}(t, \vec{X}) d\vec{W} \end{equation} Surely, I could use substitution. Thus I would arrive at the last kind of equation for one dimension. But I would like to solve systems of SDEs numerically. Is this reasoning correct? Personally, I do think it is. But this field is quite new to me and sometimes tricky. Thanks P.S.: I have just read this book on stochastic calculus . I'm an electrical engineering student. EDIT The original problem is: I would like to solve stochastic electrical circuits numerically. That means I have the following things to do: model the electrical circuit using stochastic differential equations. Which I'm yet not sure if the reasoning is correct; find the equations that can entirely describe the dynamics of the circuit; set up the following equation: \begin{equation} d \vec{X} = \vec{F}(t, \vec{X}) dt + \vec{G}(t, \vec{X}) d\vec{W} \end{equation} use Milstein's method to solve it numerically. An attempt to answer question 1 has been done right above. There I try to model a series RL circuit. The answers to question 2 and 3 depends whether or not my attempt to answer question 1 is correct or not. That's why this question focuses on the validity of the stochastic model and its reasoning. Finally, question 4 is already solved. I know how to implement Milstein's method numerically.","I would like to solve stochastic electric circuits numerically. I already know that I should use Milstein method to solve those systems. However, I'm not sure if the stochastic differential equations are correct, if the modeling is correct. Thus, could you give me a feedback whether or not my reasoning is correct? Consider a series RL circuit . Mathematically, it can be modeled as: As previously stated, I want it to be a stochastic electric circuit. Thus, all parameters should be random variables. For example, I want them to satisfy: where stands for brownian motion and the other constants are all real positive ones. With that in mind, Rewrite both equations in differential form: Substitute the parameters: The part ""?"" feels weird to me. Because I'm used to solve such kind of systems numerically: Surely, I could use substitution. Thus I would arrive at the last kind of equation for one dimension. But I would like to solve systems of SDEs numerically. Is this reasoning correct? Personally, I do think it is. But this field is quite new to me and sometimes tricky. Thanks P.S.: I have just read this book on stochastic calculus . I'm an electrical engineering student. EDIT The original problem is: I would like to solve stochastic electrical circuits numerically. That means I have the following things to do: model the electrical circuit using stochastic differential equations. Which I'm yet not sure if the reasoning is correct; find the equations that can entirely describe the dynamics of the circuit; set up the following equation: use Milstein's method to solve it numerically. An attempt to answer question 1 has been done right above. There I try to model a series RL circuit. The answers to question 2 and 3 depends whether or not my attempt to answer question 1 is correct or not. That's why this question focuses on the validity of the stochastic model and its reasoning. Finally, question 4 is already solved. I know how to implement Milstein's method numerically.","\begin{equation}
\begin{cases}
\dot{I}(t) = \frac{V_L(t)}{L} \\
\dot{V_L}(t) = \dot{V}(t) - R \dot{I}(t)
\end{cases}
\end{equation} \begin{equation}
\begin{cases}
L = L_0 + e^{B_t} \\
R = R_0 + e^{B_t} \\
V(t) = V_0 \sin(120 \pi t) + \sigma B_t
\end{cases}
\end{equation} B_t \begin{equation}
\begin{cases}
dI = \frac{V_L(t)}{L}dt \\
dV_L = dV - R dI
\end{cases}
\end{equation} \begin{equation}
\begin{cases}
dI = \frac{V_L(t)}{L_0 + e^{B_t}}dt \\
dV_L = 120 \pi V_0 \cos( 120 \pi t) dt + \sigma dB_t - \underbrace{(R_0 + e^{B_t}) dI}_{?}
\end{cases}
\end{equation} \begin{equation}
d \vec{X} = \vec{F}(t, \vec{X}) dt + \vec{G}(t, \vec{X}) d\vec{W}
\end{equation} \begin{equation}
d \vec{X} = \vec{F}(t, \vec{X}) dt + \vec{G}(t, \vec{X}) d\vec{W}
\end{equation}","['ordinary-differential-equations', 'stochastic-processes', 'numerical-methods', 'mathematical-modeling', 'stochastic-differential-equations']"
38,Asymptotic stability implies the existence of a strong Lyapunov function,Asymptotic stability implies the existence of a strong Lyapunov function,,"I am having trouble understanding the proof that asymptotic stability implies the existence of a strong Lyapunov function. Taken from the book ""Differential Dynamical Systems"", chapter 4, by James Meiss. The definitions are: and the theorem is: is a strong Lyapunov function. This is the start of the proof: I don't understand how asymptotic stability implies there exists such a ""uniform"" time $T(\rho)$ . I do understand how for all $x\in U$ there exists some time $T(\rho, x)$ but don't understand how there exists a uniform $T(\rho)$ which does not depend on $x$ .","I am having trouble understanding the proof that asymptotic stability implies the existence of a strong Lyapunov function. Taken from the book ""Differential Dynamical Systems"", chapter 4, by James Meiss. The definitions are: and the theorem is: is a strong Lyapunov function. This is the start of the proof: I don't understand how asymptotic stability implies there exists such a ""uniform"" time . I do understand how for all there exists some time but don't understand how there exists a uniform which does not depend on .","T(\rho) x\in U T(\rho, x) T(\rho) x","['ordinary-differential-equations', 'limits', 'uniform-convergence', 'stability-theory', 'lyapunov-functions']"
39,"Show that $x_i\leq \max{x_0,x_n,0}$, discrete maximum principle","Show that , discrete maximum principle","x_i\leq \max{x_0,x_n,0}","Let $x=(x_0,...,x_n)\in \mathbb R^{n+1}$ that satisfies $a_ix_{i-1}+b_ix_i+c_ix_{i+1}<0$ with coefficients $a_i,b_i,c_i\in\mathbb R$ with $a_i,c_i<0, b_i>0, a_i+b_i+c_i\geq 0$ for $1\leq i< n$ . Show that $x_i\leq \max\{x_0,x_n,0\}$ for all $i\in\{0,...,n\}$ . I tried to prove it by contradiction. This means there exists $i\in\{1,...,n-1\}$ so that $x_i\geq \max\{x_{i-1},x_{i+1},0\}$ and $x_i\geq \min\{x_{i-1},x_{i+1}\}$ . Can someone give me a hint?",Let that satisfies with coefficients with for . Show that for all . I tried to prove it by contradiction. This means there exists so that and . Can someone give me a hint?,"x=(x_0,...,x_n)\in \mathbb R^{n+1} a_ix_{i-1}+b_ix_i+c_ix_{i+1}<0 a_i,b_i,c_i\in\mathbb R a_i,c_i<0, b_i>0, a_i+b_i+c_i\geq 0 1\leq i< n x_i\leq \max\{x_0,x_n,0\} i\in\{0,...,n\} i\in\{1,...,n-1\} x_i\geq \max\{x_{i-1},x_{i+1},0\} x_i\geq \min\{x_{i-1},x_{i+1}\}","['ordinary-differential-equations', 'discrete-mathematics', 'partial-differential-equations', 'maxima-minima', 'maximum-principle']"
40,Equilibrium point of a function and its basin of attraction,Equilibrium point of a function and its basin of attraction,,"I'm very lost with the following problem: Consider $f=(f_1,f_2,f_3)\in\mathcal{C}(\mathbb{R}^3,\mathbb{R}^3)$ such that $f(0,0,0)=(0,0,0)$ and \begin{equation} x_1f_1+x_2f_2+x_3f_3<0 \tag{*} \end{equation} for all $(x_1,x_2,x_2)\in\mathbb{R}^3\backslash\{ (0,0,0)\}$ . Prove that the $(0,0,0)$ is the unique equilibrium point of $f$ , and its basin of attraction is all $\mathbb{R}^3$ . I have started my course of dynamical systems recently and I find this problem in my practice list of exercises. My problem is that I don't understand very well what does it means that and equilibrium point of a function. I find in my notes the definition of equilibrium point, but for a differential equation $x'=f(x)$ . It means to say that I need to find the equilibria of the system $$ \begin{bmatrix} x_1 \\ x_2 \\ x_3  \end{bmatrix}'=\begin{bmatrix} f_1 \\ f_2 \\ f_3  \end{bmatrix} \ ? $$ If so, how does the condition $(*)$ help me? And finally, how it is obtained his basin of attraction from this? I appreciate your help a lot.","I'm very lost with the following problem: Consider such that and for all . Prove that the is the unique equilibrium point of , and its basin of attraction is all . I have started my course of dynamical systems recently and I find this problem in my practice list of exercises. My problem is that I don't understand very well what does it means that and equilibrium point of a function. I find in my notes the definition of equilibrium point, but for a differential equation . It means to say that I need to find the equilibria of the system If so, how does the condition help me? And finally, how it is obtained his basin of attraction from this? I appreciate your help a lot.","f=(f_1,f_2,f_3)\in\mathcal{C}(\mathbb{R}^3,\mathbb{R}^3) f(0,0,0)=(0,0,0) \begin{equation} x_1f_1+x_2f_2+x_3f_3<0 \tag{*}
\end{equation} (x_1,x_2,x_2)\in\mathbb{R}^3\backslash\{ (0,0,0)\} (0,0,0) f \mathbb{R}^3 x'=f(x) 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 
\end{bmatrix}'=\begin{bmatrix}
f_1 \\
f_2 \\
f_3 
\end{bmatrix} \ ?
 (*)","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'basins-of-attraction']"
41,Help solving differential equation in closed form - Damped Harmonic Oscillator,Help solving differential equation in closed form - Damped Harmonic Oscillator,,"I am attempting to solve analytically, a differential equation of the form $$ -\alpha \frac{d^2y}{dx^2} + \beta y \frac{dy}{dx} + \gamma x^2 y = (\epsilon )y$$ Where, $\alpha $ , $ \beta$ , $\gamma$ and $ \epsilon$ are constants. The inclusion of the second term has thrown a spanner in the works. I attempted using Frobenius method but am unable to formulate a recursion relation. I have the solution of the equation with the second term excluded, and it yields a solution that depends on a Hermite polynomial. I am hoping the solution will incorporate that too. Let me know if you have any resources you can","I am attempting to solve analytically, a differential equation of the form Where, , , and are constants. The inclusion of the second term has thrown a spanner in the works. I attempted using Frobenius method but am unable to formulate a recursion relation. I have the solution of the equation with the second term excluded, and it yields a solution that depends on a Hermite polynomial. I am hoping the solution will incorporate that too. Let me know if you have any resources you can", -\alpha \frac{d^2y}{dx^2} + \beta y \frac{dy}{dx} + \gamma x^2 y = (\epsilon )y \alpha   \beta \gamma  \epsilon,['ordinary-differential-equations']
42,Literature containing the process of solving $ f''(x)+ae^{bx}f(x)=0 $,Literature containing the process of solving, f''(x)+ae^{bx}f(x)=0 ,"I tried to solve the following ODE using infinite summs, but failed: $$ f''(x)+ae^{bx}f(x)=0 \tag1$$ From a comment in this post, I found out that the solution of the above equation contains a linear combination of the Bessel functions. That means that there is a good probability that this equation is solved in detail in some literature. My question is, does equation $(1)$ have a specific name? I want to google it so I can go through the process of solving it. If not, is there any other way I can find some literature that goes through this problem?","I tried to solve the following ODE using infinite summs, but failed: From a comment in this post, I found out that the solution of the above equation contains a linear combination of the Bessel functions. That means that there is a good probability that this equation is solved in detail in some literature. My question is, does equation have a specific name? I want to google it so I can go through the process of solving it. If not, is there any other way I can find some literature that goes through this problem?", f''(x)+ae^{bx}f(x)=0 \tag1 (1),"['ordinary-differential-equations', 'bessel-functions']"
43,Solving and plotting the 2-D Lorenz Equation,Solving and plotting the 2-D Lorenz Equation,,"We are asked to first solve and then plot the phase plane of \begin{align} \begin{cases} \dot{x}=\sigma x - \sigma y\\ \dot{y} = \rho x-y \end{cases}, \ \sigma, \ \rho >0. \end{align} Now the textbook way of going at this is to derive the first line, replace in second line and as such remove one variable. Doing this we get \begin{align} \ddot{x} = \sigma \dot{x}-\sigma \dot{y}  = \sigma \dot{x}-\sigma \rho x+\sigma y = \sigma \dot{x} - \sigma \rho x + \sigma x-\dot{x}\\ \implies \ddot{x}+(1-\sigma)\dot{x}+\sigma (\rho-1)x=0\\ \implies P(\lambda)=\lambda^2+(1-\sigma)\lambda+\sigma(\rho-1)=0\\ \implies \Delta=\sigma^2+\sigma(2-4\rho)+1. \end{align} At this point we have two variables $\sigma, \rho$ . How exactly are we supposed to continue? Take a painstakingly $16$ (!!!) numbers of cases? I am sure there is a faster way for this, please someone enlighten me. Thank you.","We are asked to first solve and then plot the phase plane of Now the textbook way of going at this is to derive the first line, replace in second line and as such remove one variable. Doing this we get At this point we have two variables . How exactly are we supposed to continue? Take a painstakingly (!!!) numbers of cases? I am sure there is a faster way for this, please someone enlighten me. Thank you.","\begin{align}
\begin{cases}
\dot{x}=\sigma x - \sigma y\\
\dot{y} = \rho x-y
\end{cases}, \ \sigma, \ \rho >0.
\end{align} \begin{align}
\ddot{x} = \sigma \dot{x}-\sigma \dot{y}  = \sigma \dot{x}-\sigma \rho x+\sigma y = \sigma \dot{x} - \sigma \rho x + \sigma x-\dot{x}\\
\implies \ddot{x}+(1-\sigma)\dot{x}+\sigma (\rho-1)x=0\\
\implies P(\lambda)=\lambda^2+(1-\sigma)\lambda+\sigma(\rho-1)=0\\
\implies \Delta=\sigma^2+\sigma(2-4\rho)+1.
\end{align} \sigma, \rho 16","['ordinary-differential-equations', 'analysis', 'dynamical-systems', 'nonlinear-dynamics']"
44,Asymptotic solution of matrix differential equation expanded in powers of $1/x$,Asymptotic solution of matrix differential equation expanded in powers of,1/x,"Consider the differential equation $$ \frac{\text{d} y}{\text{d} t} = \left(c_0 + c_1 t^{-1} + c_2 t^{-2} + \dots c_n t^{-n} \right) y ;\quad y(1) = y_0 > 0; t \geq 1. $$ where $c_i > 0$ for each $i$ . This can be solved directly and we find, for some $\alpha \neq 0$ , that $$ y(t) \sim  \alpha t^{c_1} e^{c_0 t}. $$ where $f(t) \sim g(t)$ if $\lim_{t \to \infty} y(t)/g(t) = 1$ . I'm interested in the generalization of this result to the case of a system of differential equations. Basically, I think something like the following is probably true or almost true: Prop. 1 . Let $A_0$ be a $d \times d$ non-negative irreducible matrix with Perron-Frobenius eigenvalue $\lambda$ , and let $A_1, A_2 \dots A_n$ be nonnegative $d \times d$ matrices. Then, the system $$ \frac{\text{d} \mathbf{y}}{\text{d} t} = \left(A_0 + A_1 t^{-1} + A_2 t^{-2} + \dots A_n t^{-n} \right) \mathbf{y} ;\quad \mathbf{y}(1) = \mathbf{y}_0 > \mathbf{0}; t \geq 1. $$ has, for some $\alpha_i \neq 0$ and some $\beta_i$ , $$ |\mathbf{y}_i(t)| \sim \alpha_i t^{\beta_i} e^{\lambda t} \quad 1 \leq i \leq d. $$ Question : I'm looking for a reference which shows that Prop. 1 is true, or true provided with some other technical condition. It seems similar to Theorem 19.1 of Asymptotic Expansions for Ordinary Differential Equations by Wasow but I don't fully see the connection.","Consider the differential equation where for each . This can be solved directly and we find, for some , that where if . I'm interested in the generalization of this result to the case of a system of differential equations. Basically, I think something like the following is probably true or almost true: Prop. 1 . Let be a non-negative irreducible matrix with Perron-Frobenius eigenvalue , and let be nonnegative matrices. Then, the system has, for some and some , Question : I'm looking for a reference which shows that Prop. 1 is true, or true provided with some other technical condition. It seems similar to Theorem 19.1 of Asymptotic Expansions for Ordinary Differential Equations by Wasow but I don't fully see the connection.","
\frac{\text{d} y}{\text{d} t} = \left(c_0 + c_1 t^{-1} + c_2 t^{-2} + \dots c_n t^{-n} \right) y ;\quad y(1) = y_0 > 0; t \geq 1.
 c_i > 0 i \alpha \neq 0 
y(t) \sim  \alpha t^{c_1} e^{c_0 t}.
 f(t) \sim g(t) \lim_{t \to \infty} y(t)/g(t) = 1 A_0 d \times d \lambda A_1, A_2 \dots A_n d \times d  \frac{\text{d} \mathbf{y}}{\text{d} t} = \left(A_0 + A_1 t^{-1} + A_2 t^{-2} + \dots A_n t^{-n} \right) \mathbf{y} ;\quad \mathbf{y}(1) = \mathbf{y}_0 > \mathbf{0}; t \geq 1.  \alpha_i \neq 0 \beta_i  |\mathbf{y}_i(t)| \sim \alpha_i t^{\beta_i} e^{\lambda t} \quad 1 \leq i \leq d. ","['ordinary-differential-equations', 'reference-request', 'asymptotics', 'lyapunov-exponents']"
45,Prove solutions for $y'' = -y^3$ are periodic,Prove solutions for  are periodic,y'' = -y^3,"In physics, the equation $y''=-y^3$ represents the motion of a non-harmonic oscillator (for example, a mass between two walls with two springs that oscillates parallel to the walls). The solution is not given by elementary functions. I am trying to prove that the two solutions for this equation are periodic, without actually solving the equation. I tried to mimic the steps used to show that solutions to $y''=-y$ are periodic, but these heavily rely on the linearity of the equation (to solve by power series), as shown e.g. here . Proving  the solutions are bounded is pretty straightforward - Suppose we have initial conditions $y(0)=a, y'(0) = b$ . Multiplying our ODE by $y'$ yields $$ y'y'' = -y'y^3 \Longrightarrow \frac{1}{2}\frac{d}{dx}\left({y'}^2\right) = -\frac{1}{4}\frac{d}{dx}\left(y^4\right)\Longrightarrow{y'}^2+\frac{1}{2}y^4 = C $$ where $C$ is some constant. Then by the intial conditions, $C = b^2 + \frac{1}{2}a^4$ , and we can see that $y$ is bounded by $\pm\left(2C\right)^{1/4}$ . But this still doesn't mean the solution oscillates periodically between $\pm(2C)^{1/4}$ .","In physics, the equation represents the motion of a non-harmonic oscillator (for example, a mass between two walls with two springs that oscillates parallel to the walls). The solution is not given by elementary functions. I am trying to prove that the two solutions for this equation are periodic, without actually solving the equation. I tried to mimic the steps used to show that solutions to are periodic, but these heavily rely on the linearity of the equation (to solve by power series), as shown e.g. here . Proving  the solutions are bounded is pretty straightforward - Suppose we have initial conditions . Multiplying our ODE by yields where is some constant. Then by the intial conditions, , and we can see that is bounded by . But this still doesn't mean the solution oscillates periodically between .","y''=-y^3 y''=-y y(0)=a, y'(0) = b y' 
y'y'' = -y'y^3 \Longrightarrow \frac{1}{2}\frac{d}{dx}\left({y'}^2\right) = -\frac{1}{4}\frac{d}{dx}\left(y^4\right)\Longrightarrow{y'}^2+\frac{1}{2}y^4 = C
 C C = b^2 + \frac{1}{2}a^4 y \pm\left(2C\right)^{1/4} \pm(2C)^{1/4}","['ordinary-differential-equations', 'periodic-functions']"
46,On a solution of a fractional integral equation,On a solution of a fractional integral equation,,"I am looking for the solution of $$\frac{d^\alpha}{d x^\alpha}f(x)=g(x)f(x),$$ where $\alpha \in (0,1)$ and $\frac{d^\alpha}{d x^\alpha}$ is the Caputo derivative. A series of Jumarie's papers, ""2005On the solution of the stochastic differential equation of exponential growth driven by fractional Brownian motion"", ""2005On the representation of fractional Brownian motion as an integral with respect to and $(dt)^\alpha$ "" and ""2006Modified Riemann-Liouville derivative and fractional Taylor series of nondifferentiable functions further results"", and many other later papers, the author claim the solution is $$ f(x)=f(0)E_\alpha\left(\alpha \int_0^x (x-y)^{\alpha-1}g(y)dy\right), $$ where $E_\alpha$ is the Mittag-Leffler function. Apart from the idea provided in these papers, do we have other ways to prove it? By the way, the author also apply one result of the Mittag-Leffler function, $$ E_\alpha\left((x+y)^\alpha\right)=E_\alpha(x^\alpha)E_\alpha(y^\alpha). $$ May I ask what is the condition for this to be true? At least, Wolfram Mathematica does not support this equation. Many thanks in advance.","I am looking for the solution of where and is the Caputo derivative. A series of Jumarie's papers, ""2005On the solution of the stochastic differential equation of exponential growth driven by fractional Brownian motion"", ""2005On the representation of fractional Brownian motion as an integral with respect to and "" and ""2006Modified Riemann-Liouville derivative and fractional Taylor series of nondifferentiable functions further results"", and many other later papers, the author claim the solution is where is the Mittag-Leffler function. Apart from the idea provided in these papers, do we have other ways to prove it? By the way, the author also apply one result of the Mittag-Leffler function, May I ask what is the condition for this to be true? At least, Wolfram Mathematica does not support this equation. Many thanks in advance.","\frac{d^\alpha}{d x^\alpha}f(x)=g(x)f(x), \alpha \in (0,1) \frac{d^\alpha}{d x^\alpha} (dt)^\alpha 
f(x)=f(0)E_\alpha\left(\alpha \int_0^x (x-y)^{\alpha-1}g(y)dy\right),
 E_\alpha 
E_\alpha\left((x+y)^\alpha\right)=E_\alpha(x^\alpha)E_\alpha(y^\alpha).
","['ordinary-differential-equations', 'special-functions', 'fractional-calculus', 'mittag-leffler-function', 'fractional-differential-equations']"
47,Proving there is no solution to an ODE,Proving there is no solution to an ODE,,"Question: Show that, for any a > 0, there exists no solution u : $[0, \frac{1}{a}] \rightarrow \mathbb{R}$ to the problem $$ \begin{cases} \frac{du}{dt} = u^{2}+e^{-u}\\ u(0) = a \end{cases} $$ Hint: Check that $u(t) > 0$ for all $t$ and that $\frac{du}{dt} \geq u^{2}$ ; deduce that $\frac{1}{u}$ decreases at least linearly My work so far: Given that the first derivative of $u$ is strictly greater than $0$ , then $u$ is strictly increasing from $a>0$ . Thus, $u(t) > 0$ for all $t$ . Secondly, since $e^{-u}$ is non-zero and non-negative, $\frac{du}{dt}>u^{2}$ Thirdly, since it was established that $u$ is an increasing function, then $\frac{1}{u}$ is a decreasing function. To show that it decreases at least linearly, consider the following $$ \begin{aligned} \frac{du}{dt} &\geq u^{2}\\ \int \frac{du}{u^{2}} &\geq \int dt\\ \frac{-1}{u} &\geq t + c\\ -t - c &\geq \frac{1}{u}  \end{aligned} $$ Thus, $\frac{1}{u}$ decreases at least linearly. My obstacle: How can I deduce from that that there are no solutions to the ODE? Any help or guidance is appreciated.","Question: Show that, for any a > 0, there exists no solution u : to the problem Hint: Check that for all and that ; deduce that decreases at least linearly My work so far: Given that the first derivative of is strictly greater than , then is strictly increasing from . Thus, for all . Secondly, since is non-zero and non-negative, Thirdly, since it was established that is an increasing function, then is a decreasing function. To show that it decreases at least linearly, consider the following Thus, decreases at least linearly. My obstacle: How can I deduce from that that there are no solutions to the ODE? Any help or guidance is appreciated.","[0, \frac{1}{a}] \rightarrow \mathbb{R} 
\begin{cases}
\frac{du}{dt} = u^{2}+e^{-u}\\
u(0) = a
\end{cases}
 u(t) > 0 t \frac{du}{dt} \geq u^{2} \frac{1}{u} u 0 u a>0 u(t) > 0 t e^{-u} \frac{du}{dt}>u^{2} u \frac{1}{u} 
\begin{aligned}
\frac{du}{dt} &\geq u^{2}\\
\int \frac{du}{u^{2}} &\geq \int dt\\
\frac{-1}{u} &\geq t + c\\
-t - c &\geq \frac{1}{u} 
\end{aligned}
 \frac{1}{u}",['ordinary-differential-equations']
48,When is asymptotic stability preserved in this sequence of dynamical systems?,When is asymptotic stability preserved in this sequence of dynamical systems?,,"Setup: Suppose that for each $a>0$ , $(a,0)$ is an asymptotically stable steady state of the autonomous system $$ \begin{cases} \frac{d}{dt}{z}_1(t) = f_1\big(z_1(t),z_2(t);a\big)\\ \frac{d}{dt}{z}_2(t) = f_2\big(z_1(t),z_2(t);a\big)\\ \end{cases} \qquad (*) $$ where $f_i:\mathbb{R}^2\times$ $\mathbb{R}_{>0}$ $\to\mathbb{R}$ is continuously differentiable $(i=1,2)$ . Remark: note that this implies that -- given system $(*)$ -- the basin of attraction of $(a,0)$ $$ \left\{\vec{z}_0\in\mathbb{R}^2: \big(z_1(0),z_2(0)\big)\,\texttt{=}\,\vec{z}_0 \Rightarrow \lim_{t\to\infty}z_1(t)=a \text{ and } \lim_{t\to\infty}z_2(t)=0 \right\} $$ has strictly positive Lebesgue measure $\forall a>0$ ( details ). Assume that $\lim\limits_{a\to\infty}f_{i}(\cdot;a)=\hat{f}_{i}(\cdot)\in\mathcal{C}$ pointwise ( $i=1,2$ ), and define the autonomous system $$ \begin{cases} \frac{d}{dt}{z}_1(t) = \hat{f}_{1}\big(z_1(t),z_2(t)\big)\\ \frac{d}{dt}{z}_2(t) = \hat{f}_{2}\big(z_1(t),z_2(t)\big)\\ \end{cases} \qquad (**) $$ Question: Are any additional assumptions needed for the following statement to be true? $``$ Assuming $(z_1(t),z_2(t))$ obey system $(**)$ , the set $$ \left\{\vec{z}_0\in\mathbb{R}^2: \big(z_1(0),z_2(0)\big)\,\texttt{=}\,\vec{z}_0 \Rightarrow \lim_{t\to\infty}z_1(t)=\infty \text{ and } \lim_{t\to\infty}z_2(t)=0 \right\} $$ has strictly positive Lebesgue measure. $""$ If so, which assumptions are sufficient (and necessary) for this to be true?""","Setup: Suppose that for each , is an asymptotically stable steady state of the autonomous system where is continuously differentiable . Remark: note that this implies that -- given system -- the basin of attraction of has strictly positive Lebesgue measure ( details ). Assume that pointwise ( ), and define the autonomous system Question: Are any additional assumptions needed for the following statement to be true? Assuming obey system , the set has strictly positive Lebesgue measure. If so, which assumptions are sufficient (and necessary) for this to be true?""","a>0 (a,0) 
\begin{cases}
\frac{d}{dt}{z}_1(t) = f_1\big(z_1(t),z_2(t);a\big)\\
\frac{d}{dt}{z}_2(t) = f_2\big(z_1(t),z_2(t);a\big)\\
\end{cases} \qquad (*)
 f_i:\mathbb{R}^2\times \mathbb{R}_{>0} \to\mathbb{R} (i=1,2) (*) (a,0)  \left\{\vec{z}_0\in\mathbb{R}^2: \big(z_1(0),z_2(0)\big)\,\texttt{=}\,\vec{z}_0 \Rightarrow \lim_{t\to\infty}z_1(t)=a \text{ and } \lim_{t\to\infty}z_2(t)=0 \right\}  \forall a>0 \lim\limits_{a\to\infty}f_{i}(\cdot;a)=\hat{f}_{i}(\cdot)\in\mathcal{C} i=1,2 
\begin{cases}
\frac{d}{dt}{z}_1(t) = \hat{f}_{1}\big(z_1(t),z_2(t)\big)\\
\frac{d}{dt}{z}_2(t) = \hat{f}_{2}\big(z_1(t),z_2(t)\big)\\
\end{cases} \qquad (**)
 `` (z_1(t),z_2(t)) (**)  \left\{\vec{z}_0\in\mathbb{R}^2: \big(z_1(0),z_2(0)\big)\,\texttt{=}\,\vec{z}_0 \Rightarrow \lim_{t\to\infty}z_1(t)=\infty \text{ and } \lim_{t\to\infty}z_2(t)=0 \right\}  ""","['ordinary-differential-equations', 'limits', 'measure-theory', 'dynamical-systems', 'stability-in-odes']"
49,How to derive PDE for a given surface?,How to derive PDE for a given surface?,,"The title may be ambiguous, therefore I will illustrate it using an example. Let's say we have a curve $y=\sin \left( x\right)$ . We can say that corresponding ODE is $\dfrac{dy}{dx}=\cos \left( x\right)$ . i.e. $y=\sin \left( x\right)$ is solution of $\dfrac{dy}{dx}=\cos \left( x\right)$ . Now, lets say I have a surface in 3d i.e $z=\sin \left( x\right) +\cos \left( y\right)$ . Is it possible to come up with PDE whose solution is $z=\sin \left( x\right) +\cos \left( y\right)$ .","The title may be ambiguous, therefore I will illustrate it using an example. Let's say we have a curve . We can say that corresponding ODE is . i.e. is solution of . Now, lets say I have a surface in 3d i.e . Is it possible to come up with PDE whose solution is .",y=\sin \left( x\right) \dfrac{dy}{dx}=\cos \left( x\right) y=\sin \left( x\right) \dfrac{dy}{dx}=\cos \left( x\right) z=\sin \left( x\right) +\cos \left( y\right) z=\sin \left( x\right) +\cos \left( y\right),"['ordinary-differential-equations', 'functions', 'partial-differential-equations', 'curves', 'surfaces']"
50,Prove solution of a ODE is bounded,Prove solution of a ODE is bounded,,"I am trying to prove the following statement: ""Let $f:\mathbb{R} \rightarrow \mathbb{R}$ of class $C^1(\mathbb{R})$ verifying that $f(m)=0 \, \, \forall m \in \mathbb{Z}$ . Prove that all solutions of the ODE $y'=f(y)$ are bounded and defined in all $\mathbb{R}$ ."" I know I need to show that $|y|\leq M$ with $M$ a constant but I don't know how to work with the ODE to achieve this. I also looked for inspiration and found this question Proving all solutions of $y'+y=f(x)$ are bounded . Thanks for your attention.","I am trying to prove the following statement: ""Let of class verifying that . Prove that all solutions of the ODE are bounded and defined in all ."" I know I need to show that with a constant but I don't know how to work with the ODE to achieve this. I also looked for inspiration and found this question Proving all solutions of $y'+y=f(x)$ are bounded . Thanks for your attention.","f:\mathbb{R} \rightarrow \mathbb{R} C^1(\mathbb{R}) f(m)=0 \, \, \forall m \in \mathbb{Z} y'=f(y) \mathbb{R} |y|\leq M M",['ordinary-differential-equations']
51,When is $A+A^T$ positive definite?,When is  positive definite?,A+A^T,"Suppose $A$ is a real valued $d\times d$ matrix. Is there a way to characterize set of $A$ such that that it's ""symmetrization"" $B$ is positive definite? $$B=\frac{1}{2}(A+A^T)$$ Largest eigenvalue of $B$ is known as the numerical abscissa of $A$ and determines the starting slope of $\text{exp}(t A)$ Is there meaning attached to the ""smallest eigenvalue"" of $B$ ? (prompted by related question where this condition on $A=CD$ with p.s.d $C,D$ turns out necessary for random update $x=x-\alpha C x$ on $x$ with covariance matrix $D$ to reduce variance of $x$ in all directions)","Suppose is a real valued matrix. Is there a way to characterize set of such that that it's ""symmetrization"" is positive definite? Largest eigenvalue of is known as the numerical abscissa of and determines the starting slope of Is there meaning attached to the ""smallest eigenvalue"" of ? (prompted by related question where this condition on with p.s.d turns out necessary for random update on with covariance matrix to reduce variance of in all directions)","A d\times d A B B=\frac{1}{2}(A+A^T) B A \text{exp}(t A) B A=CD C,D x=x-\alpha C x x D x","['linear-algebra', 'ordinary-differential-equations']"
52,Solve the differential equation $L^2 y=0$ where $L=x^2\frac{d^2}{dx^2}+x\frac{d}{dx}-(x^2+1)\text{id}$,Solve the differential equation  where,L^2 y=0 L=x^2\frac{d^2}{dx^2}+x\frac{d}{dx}-(x^2+1)\text{id},"Consider the linear differential equation defined by $$ L^2y=0,\quad \text{where}\ L:=x^2\frac{d}{dx^2}+x\frac{d}{dx}-(x^2+1)\text{id}. \tag{1}$$ Here $\text{id}$ denotes the identity operator, and $L^2$ denotes the operator $L$ applied twice. Written out explicitly, (1) is of the form $$x^4\frac{d^4 y}{dx^4}+6 x^3\frac{d^3y}{dx^3}+(5x^2-2x^4)\frac{d^2y}{dx^2}-(x+6x^3)\frac{dy}{dx}+(x^2-1)^2y=0. \tag{2}$$ I am interested in the general solution to (1) which is bounded at $x=0$ . Since the solution that we are looking for is bounded at $x=0$ , we may write it as $$y(x)=\sum_{n=0}^\infty a_n x^n,$$ near the origin. Substituting the above solution into (2) and equating the coefficient of $x^n$ to be zero, we obtain the following recurrence relation $$ a_2=\frac{2}{9}a_0,\quad a_3=\frac{1}{8}a_1,\quad a_4=\frac{1}{75}a_0,\quad a_5=\frac{1}{192}{a_1},\quad a_6=\frac{4}{11025}a_0,\quad\dots.$$ Setting $(a_0,a_1)=(1,0)$ and $(a_0,a_1)=(0,1)$ , we obtain two linearly independent solutions whose linear combination gives the general solution to (1) bounded at $x=0$ . The solution to with $(a_0,a_1)=(0,1)$ can be written as $$ y_1(x)=x+\frac{1}{8}x^3+\frac{1}{192}x^5+\cdots=2I_1(x),$$ where $I_1(x)$ is the modified Bessel function of the first kind, while the solution with $(a_0,a_1)=(1,0)$ can be represented as $$ y_2(x)=1+\frac{2}{9}x^2+\frac{1}{75}x^4+\frac{4}{11025}x^6+\cdots$$ My question is that: can $y_2(x)$ be represented in terms of any special function (like $y_1(x))$ ? Update: As pointed out in the comments, the $x^0$ coefficient relation gives $a_0=0$ , so the solution $y_2(x)$ doest not exist.","Consider the linear differential equation defined by Here denotes the identity operator, and denotes the operator applied twice. Written out explicitly, (1) is of the form I am interested in the general solution to (1) which is bounded at . Since the solution that we are looking for is bounded at , we may write it as near the origin. Substituting the above solution into (2) and equating the coefficient of to be zero, we obtain the following recurrence relation Setting and , we obtain two linearly independent solutions whose linear combination gives the general solution to (1) bounded at . The solution to with can be written as where is the modified Bessel function of the first kind, while the solution with can be represented as My question is that: can be represented in terms of any special function (like ? Update: As pointed out in the comments, the coefficient relation gives , so the solution doest not exist.","
L^2y=0,\quad \text{where}\ L:=x^2\frac{d}{dx^2}+x\frac{d}{dx}-(x^2+1)\text{id}. \tag{1} \text{id} L^2 L x^4\frac{d^4 y}{dx^4}+6 x^3\frac{d^3y}{dx^3}+(5x^2-2x^4)\frac{d^2y}{dx^2}-(x+6x^3)\frac{dy}{dx}+(x^2-1)^2y=0. \tag{2} x=0 x=0 y(x)=\sum_{n=0}^\infty a_n x^n, x^n  a_2=\frac{2}{9}a_0,\quad a_3=\frac{1}{8}a_1,\quad a_4=\frac{1}{75}a_0,\quad a_5=\frac{1}{192}{a_1},\quad a_6=\frac{4}{11025}a_0,\quad\dots. (a_0,a_1)=(1,0) (a_0,a_1)=(0,1) x=0 (a_0,a_1)=(0,1) 
y_1(x)=x+\frac{1}{8}x^3+\frac{1}{192}x^5+\cdots=2I_1(x), I_1(x) (a_0,a_1)=(1,0) 
y_2(x)=1+\frac{2}{9}x^2+\frac{1}{75}x^4+\frac{4}{11025}x^6+\cdots y_2(x) y_1(x)) x^0 a_0=0 y_2(x)","['ordinary-differential-equations', 'bessel-functions']"
53,"simple pendulum equation, why it cannot be solved with laplace transform (the general solution)","simple pendulum equation, why it cannot be solved with laplace transform (the general solution)",,"Usually to solve the simple pendulum equation: $\qquad \ell {\ddot  \theta }+g\sin \theta =0\,$ Using the first term of Taylor series is used as approximation, but although $\sin \theta$ can be transformed to Laplace ""space"" and use it to find a general solution, I can't find it. Why there is no general solution in Laplace?","Usually to solve the simple pendulum equation: Using the first term of Taylor series is used as approximation, but although can be transformed to Laplace ""space"" and use it to find a general solution, I can't find it. Why there is no general solution in Laplace?","\qquad \ell {\ddot  \theta }+g\sin \theta =0\, \sin \theta","['ordinary-differential-equations', 'laplace-transform']"
54,"Floquet Theory $\phi(t)=P(t)e^{tR}$, can be $R$ Hermitian?","Floquet Theory , can be  Hermitian?",\phi(t)=P(t)e^{tR} R,"Let ${\displaystyle {\dot {x}}=A(t)x}$ be a linear first order differential equation, where ${\displaystyle x(t)}$ is a column vector of length ${\displaystyle n}$ and ${\displaystyle A(t)}$ an ${\displaystyle n\times n}$ periodic matrix with period ${\displaystyle T}$ (that is ${\displaystyle A(t+T)=A(t)}$ for all real values of ${\displaystyle t}$ . Let ${\displaystyle \phi(t)}$ be a fundamental matrix solution of this differential equation. The Floquet Theory tells us that there is a constant matrix $R$ (possibly complex) and a T-periodic matrix valued function ${\displaystyle t\mapsto P(t)}$ such that $$\phi(t)=P(t)e^{tR}.$$ This gives rise to a time-dependent change of coordinates ${\displaystyle y=P^{-1}(t)x}$ , under which our original system becomes a linear system with constant coefficients ${\displaystyle {\dot {y}}=Ry}$ . My question: Does $R$ follow the algebraic properties of $A(t)$ . For example if $A(t)$ is Hermitian, can we chose $R$ to be also Hermitian? The same question when $A(t)$ is skew-Hermitian, unitary,...","Let be a linear first order differential equation, where is a column vector of length and an periodic matrix with period (that is for all real values of . Let be a fundamental matrix solution of this differential equation. The Floquet Theory tells us that there is a constant matrix (possibly complex) and a T-periodic matrix valued function such that This gives rise to a time-dependent change of coordinates , under which our original system becomes a linear system with constant coefficients . My question: Does follow the algebraic properties of . For example if is Hermitian, can we chose to be also Hermitian? The same question when is skew-Hermitian, unitary,...",{\displaystyle {\dot {x}}=A(t)x} {\displaystyle x(t)} {\displaystyle n} {\displaystyle A(t)} {\displaystyle n\times n} {\displaystyle T} {\displaystyle A(t+T)=A(t)} {\displaystyle t} {\displaystyle \phi(t)} R {\displaystyle t\mapsto P(t)} \phi(t)=P(t)e^{tR}. {\displaystyle y=P^{-1}(t)x} {\displaystyle {\dot {y}}=Ry} R A(t) A(t) R A(t),"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'periodic-functions', 'hermitian-matrices']"
55,"""Spanning"" of solutions of ordinary differential equations","""Spanning"" of solutions of ordinary differential equations",,"Suppose we have a switched ODE $$\dot{x} = A_{\sigma(t)}x,$$ where $A_{\sigma(t)}$ is a constant matrix given $\sigma(t)\in\mathcal{M}=\{1,2,\cdots,m\}$ . If we fix the initial condition and can arbitrarily switch the signal $\sigma(t)$ , what would the reachable set of $x(t)$ look like? Intuitively, I think it is something like the spanning of the solutions of $\dot{x} = A_1x, \dot{x} = A_2x, \cdots,\dot{x} = A_mx$ . I wonder whether there are tools available for analyzing such a space.","Suppose we have a switched ODE where is a constant matrix given . If we fix the initial condition and can arbitrarily switch the signal , what would the reachable set of look like? Intuitively, I think it is something like the spanning of the solutions of . I wonder whether there are tools available for analyzing such a space.","\dot{x} = A_{\sigma(t)}x, A_{\sigma(t)} \sigma(t)\in\mathcal{M}=\{1,2,\cdots,m\} \sigma(t) x(t) \dot{x} = A_1x, \dot{x} = A_2x, \cdots,\dot{x} = A_mx","['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'nonlinear-dynamics']"
56,Examples of dynamical systems that have structural stability,Examples of dynamical systems that have structural stability,,"I am looking for simple examples of structural stability, I read the definition of structural stability but couldn't figure out a concrete example of a system, its perturbated version and its conjugacy. The definition that I am using is from Wikipedia. ""Structural stability means that the qualitative behavior of the trajectories is unaffected by small perturbations ( $C^1$ -small)."" Moreover, what does it mean a $C^1$ -small perturbation?","I am looking for simple examples of structural stability, I read the definition of structural stability but couldn't figure out a concrete example of a system, its perturbated version and its conjugacy. The definition that I am using is from Wikipedia. ""Structural stability means that the qualitative behavior of the trajectories is unaffected by small perturbations ( -small)."" Moreover, what does it mean a -small perturbation?",C^1 C^1,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'perturbation-theory', 'stability-theory']"
57,Globally asymptotic stable gradient system has unstable point,Globally asymptotic stable gradient system has unstable point,,"Given a gradient system $$\frac{d\theta_1}{dt}=-\sin(\theta_1-\theta_2)$$ $$\frac{d\theta_2}{dt}=-\sin(\theta_2-\theta_1)$$ The system is a gradient system since $$\frac{d\vec \theta}{dt}=-\nabla V(\vec\theta) = -\nabla(1-\cos(\theta_1-\theta_2))$$ Since the system is invariant by replacing all $\theta_i$ to $\theta_i+\alpha$ , this will leads to a continuum of equilibria. To remove this freedom, we fix $\theta_1=0$ , then the system becomes $$\frac{d\theta_2}{dt}=-\sin(\theta_2)$$ It has two equilibrium points $\theta_2=0$ and $\theta_2=\pi$ . The energy function is $V(\theta_2)=-\cos(\theta_2)+1.$ The second derivative of $V(\theta_2)$ is $\cos\theta_2$ . Since the second derivative is negative on $\theta_2=\pi$ , it is an unstable equilibrium point. Since the second derivative is positive $\theta_2=0$ , it is a locally asymptotically stable (LAS) equilibrium point. Since this is a gradient system, only $\theta_2=0$ is locally asymptotically stable, then it is globally asymptotically stable (GAS). But it is a little counterintuitive in the sense of convexity of energy function: how comes that the energy function is concave on $\theta_2=\pi$ , and $\theta_2=0$ is globally asymptotically stable? For example, let say the figure of an energy function looks like the following and it is convex on the local minimum, and concave on the local maximum. How come all trajectories will converge to the local minimum? As illustrated in the figure, trajectories starting from the part behind the local maximum cannot converge to the local minimum.","Given a gradient system The system is a gradient system since Since the system is invariant by replacing all to , this will leads to a continuum of equilibria. To remove this freedom, we fix , then the system becomes It has two equilibrium points and . The energy function is The second derivative of is . Since the second derivative is negative on , it is an unstable equilibrium point. Since the second derivative is positive , it is a locally asymptotically stable (LAS) equilibrium point. Since this is a gradient system, only is locally asymptotically stable, then it is globally asymptotically stable (GAS). But it is a little counterintuitive in the sense of convexity of energy function: how comes that the energy function is concave on , and is globally asymptotically stable? For example, let say the figure of an energy function looks like the following and it is convex on the local minimum, and concave on the local maximum. How come all trajectories will converge to the local minimum? As illustrated in the figure, trajectories starting from the part behind the local maximum cannot converge to the local minimum.",\frac{d\theta_1}{dt}=-\sin(\theta_1-\theta_2) \frac{d\theta_2}{dt}=-\sin(\theta_2-\theta_1) \frac{d\vec \theta}{dt}=-\nabla V(\vec\theta) = -\nabla(1-\cos(\theta_1-\theta_2)) \theta_i \theta_i+\alpha \theta_1=0 \frac{d\theta_2}{dt}=-\sin(\theta_2) \theta_2=0 \theta_2=\pi V(\theta_2)=-\cos(\theta_2)+1. V(\theta_2) \cos\theta_2 \theta_2=\pi \theta_2=0 \theta_2=0 \theta_2=\pi \theta_2=0,"['ordinary-differential-equations', 'dynamical-systems', 'vector-analysis', 'stability-in-odes', 'gradient-flows']"
58,Find the equation for the angle in which the particle abandons the hemicircle. No friction.,Find the equation for the angle in which the particle abandons the hemicircle. No friction.,,"I think I missed something in this mechanics problem. We're given a polish and homogeneous hemicircle which has mass $M$ and a particle of mass $m$ laying on the top of it. There is also no friction between the hemicircle and the ground. Find the equation for the angle $\theta$ in which the particle abandons the hemicircle surface. I can't find my mistake. As we have no external horizontal forces acting on the system particle + hemicircle we must have conservation of the horizontal position of the center of mass. I made this horrible drawing to try to understand the movement: we would have $x = \frac{mR \sin(\theta)}{M+m}$ , making $M = k \cdot m \iff k = \frac M m$ , we would have: $$x = \frac{R \sin(\theta)}{1+k} \implies \dot x = \frac{R \dot \theta \cos(\theta)}{1+k}$$ from this, I found that the velocity of the particle with respect to the ground was: $$R \dot \theta (\cos (\theta)(\frac k{k+1})u_x - \sin (\theta) u_y) = \vec v_P $$ And conservation of energy $$mgR = mgR\cos(\theta) + \frac{mv_P^2+ M \dot x^2}2$$ gave me: $$2\frac gR(1-\cos(\theta))(1+k) = \dot \theta^2 [\sin^2(\theta) +k]$$ Finally, I used the abandonment equation: $\frac gR \cos(\theta) = \dot \theta^2$ Which led me to: $$3 \cos(\theta) - \frac{\cos^3(\theta)}{1+k} = 2$$ but I feel something is wrong because $k=0$ should give me $\cos(\theta) = \frac23$ any insights on my mistakes? EDIT: I think on the line of the energy conservation the term $\sin^2(\theta)$ is the one causing trouble. I'm very confident about the expression for the velocity of the particle.","I think I missed something in this mechanics problem. We're given a polish and homogeneous hemicircle which has mass and a particle of mass laying on the top of it. There is also no friction between the hemicircle and the ground. Find the equation for the angle in which the particle abandons the hemicircle surface. I can't find my mistake. As we have no external horizontal forces acting on the system particle + hemicircle we must have conservation of the horizontal position of the center of mass. I made this horrible drawing to try to understand the movement: we would have , making , we would have: from this, I found that the velocity of the particle with respect to the ground was: And conservation of energy gave me: Finally, I used the abandonment equation: Which led me to: but I feel something is wrong because should give me any insights on my mistakes? EDIT: I think on the line of the energy conservation the term is the one causing trouble. I'm very confident about the expression for the velocity of the particle.",M m \theta x = \frac{mR \sin(\theta)}{M+m} M = k \cdot m \iff k = \frac M m x = \frac{R \sin(\theta)}{1+k} \implies \dot x = \frac{R \dot \theta \cos(\theta)}{1+k} R \dot \theta (\cos (\theta)(\frac k{k+1})u_x - \sin (\theta) u_y) = \vec v_P  mgR = mgR\cos(\theta) + \frac{mv_P^2+ M \dot x^2}2 2\frac gR(1-\cos(\theta))(1+k) = \dot \theta^2 [\sin^2(\theta) +k] \frac gR \cos(\theta) = \dot \theta^2 3 \cos(\theta) - \frac{\cos^3(\theta)}{1+k} = 2 k=0 \cos(\theta) = \frac23 \sin^2(\theta),"['ordinary-differential-equations', 'physics', 'classical-mechanics']"
59,Controllability for second order coupled system,Controllability for second order coupled system,,"I have the following system: $\ddot{y_1}=-y_1+\alpha y_2+u_1$ $\ddot{y_2}=-y_2+\alpha y_1-2u_2$ I am trying to answer 4 questions: For what values of $\alpha$ is the system controllable For what values of $\alpha$ is the system controllable from $u_1$ alone For what values of $\alpha$ is the system controllable from $u_2$ alone For what values of $\alpha$ is the system controllable if $u_1=u_2$ My workings: First, let's convert it into state space form by setting $x_1=y_1, x_2=y_2,x_3=\dot{y_1},x_4=\dot{y_2}$ We get $\begin{bmatrix}\dot{x_1}\\\dot{x_2}\\\dot{x_3}\\\dot{x_4}\\\end{bmatrix}=\begin{bmatrix}0&0&1&0\\0&0&0&1\\-1&\alpha&0&0\\\alpha&-1&0&0 \end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\\x_4\\\end{bmatrix}+\begin{bmatrix}0&0\\0&0\\1&0\\0&-2\end{bmatrix}\begin{bmatrix}u_1\\u_2\\\end{bmatrix}$ . To answer 1) We check controllability by checking the rank of $[B,AB,A^2B,A^3B]$ with $B=\begin{bmatrix}0&0\\0&0\\1&0\\0&-2\end{bmatrix}$ . This matrix appears to always be full rank. To answer 2) We check controllability by checking the rank of $[B,AB,A^2B,A^3B]$ with $B=\begin{bmatrix}0\\0\\1\\0\end{bmatrix}$ . This matrix appears to be full rank only when $\alpha\ne0$ . To answer 3) We check controllability by checking the rank of $[B,AB,A^2B,A^3B]$ with $B=\begin{bmatrix}0\\0\\0\\-2\end{bmatrix}$ This matrix appears to be full rank only when $\alpha\ne0$ . To answer 4) We check controllability by checking the rank of $[B,AB,A^2B,A^3B]$ with $B=\begin{bmatrix}0\\0\\1\\-2\end{bmatrix}$ This matrix appears to be full rank only when $\alpha\ne0$ . Are the answers really this trivial or am I making a mistake somewhere?","I have the following system: I am trying to answer 4 questions: For what values of is the system controllable For what values of is the system controllable from alone For what values of is the system controllable from alone For what values of is the system controllable if My workings: First, let's convert it into state space form by setting We get . To answer 1) We check controllability by checking the rank of with . This matrix appears to always be full rank. To answer 2) We check controllability by checking the rank of with . This matrix appears to be full rank only when . To answer 3) We check controllability by checking the rank of with This matrix appears to be full rank only when . To answer 4) We check controllability by checking the rank of with This matrix appears to be full rank only when . Are the answers really this trivial or am I making a mistake somewhere?","\ddot{y_1}=-y_1+\alpha y_2+u_1 \ddot{y_2}=-y_2+\alpha y_1-2u_2 \alpha \alpha u_1 \alpha u_2 \alpha u_1=u_2 x_1=y_1, x_2=y_2,x_3=\dot{y_1},x_4=\dot{y_2} \begin{bmatrix}\dot{x_1}\\\dot{x_2}\\\dot{x_3}\\\dot{x_4}\\\end{bmatrix}=\begin{bmatrix}0&0&1&0\\0&0&0&1\\-1&\alpha&0&0\\\alpha&-1&0&0 \end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\\x_4\\\end{bmatrix}+\begin{bmatrix}0&0\\0&0\\1&0\\0&-2\end{bmatrix}\begin{bmatrix}u_1\\u_2\\\end{bmatrix} [B,AB,A^2B,A^3B] B=\begin{bmatrix}0&0\\0&0\\1&0\\0&-2\end{bmatrix} [B,AB,A^2B,A^3B] B=\begin{bmatrix}0\\0\\1\\0\end{bmatrix} \alpha\ne0 [B,AB,A^2B,A^3B] B=\begin{bmatrix}0\\0\\0\\-2\end{bmatrix} \alpha\ne0 [B,AB,A^2B,A^3B] B=\begin{bmatrix}0\\0\\1\\-2\end{bmatrix} \alpha\ne0","['ordinary-differential-equations', 'optimization', 'dynamical-systems', 'control-theory', 'linear-control']"
60,"Example for a continuous function $x \geq 0$ on $[0,\infty)$ so that $x(0)=0$ and $\left(x(t) \right)^2\leq 2+\int_{0}^{t}x(u)du,~~~\forall ~t\geq 0$",Example for a continuous function  on  so that  and,"x \geq 0 [0,\infty) x(0)=0 \left(x(t) \right)^2\leq 2+\int_{0}^{t}x(u)du,~~~\forall ~t\geq 0","Q. Suppose $x:[0,\infty)\to [0,\infty)$ is continuous and $x(0)=0.$ If $$\left(x(t) \right)^2\leq 2+\int_{0}^{t}x(u)du,~~~\forall ~t\geq 0,$$ then which of the following is TRUE? $x(\sqrt{2})\in [0,2]$ $x(\sqrt{2})\in [0,\frac{3}{\sqrt{2}}]$ $x(\sqrt{2})\in [\frac{5}{\sqrt{2}},\frac{7}{\sqrt{2}}]$ $x(\sqrt{2})\in [10,\infty)$ By letting $y(t)=2+\int_0^tx(u)du$ , we have $\sqrt{x(t)^2}=x(t)=y'(t) \leq \sqrt{y(t)},$ and thereby considering the monotonicity of the function $g(t)=2\sqrt{y(t)}-t$ , we get $x(\sqrt{2})\leq 3/\sqrt 2$ . Now, can you point out a function $x$ with $2<x(\sqrt 2) \leq 3/\sqrt 2$ to ignore option 1?","Q. Suppose is continuous and If then which of the following is TRUE? By letting , we have and thereby considering the monotonicity of the function , we get . Now, can you point out a function with to ignore option 1?","x:[0,\infty)\to [0,\infty) x(0)=0. \left(x(t) \right)^2\leq 2+\int_{0}^{t}x(u)du,~~~\forall ~t\geq 0, x(\sqrt{2})\in [0,2] x(\sqrt{2})\in [0,\frac{3}{\sqrt{2}}] x(\sqrt{2})\in [\frac{5}{\sqrt{2}},\frac{7}{\sqrt{2}}] x(\sqrt{2})\in [10,\infty) y(t)=2+\int_0^tx(u)du \sqrt{x(t)^2}=x(t)=y'(t) \leq \sqrt{y(t)}, g(t)=2\sqrt{y(t)}-t x(\sqrt{2})\leq 3/\sqrt 2 x 2<x(\sqrt 2) \leq 3/\sqrt 2","['real-analysis', 'ordinary-differential-equations', 'integral-inequality']"
61,Domain of solution of differential equation $yâ€™=\frac{1}{(y+7)(t-3)}$,Domain of solution of differential equation,yâ€™=\frac{1}{(y+7)(t-3)},"If we are given the initial-value problem $$\frac{dy}{dt}=\frac{1}{(y+7)(t-3)}, y(0)=0$$ I want to solve said initial value problem & state the domain of the solution. I also want to observe what happens when $t$ approaches the limits of the solutionâ€™s domain. Via separation of variables, one obtains: $$\implies(y+7) \space dy = \frac{dt}{t-3}\implies\frac{1}{2}y^2+7y=\ln|t-3|+c_1$$ $$\iff y^2+14y+49=2\ln|t-3|+(2c_1+49)$$ Now call $C=2c_1+49$ , then $$(y+7)^2=2\ln|t-3|+C\implies y(t) = \pm\sqrt{2\ln|t-3|+C}-7$$ Substituting the initial condition: $$0=\pm\sqrt{2\ln|0-3|+C}-7\iff 7 = \pm\sqrt{2\ln(3)+C}$$ This shows we must choose the positive square root in order for our solution $y(t)$ to pass through the initial condition & solve the IVP. Then $$C=49-2\ln(3)$$ so that $$y(t)=\sqrt{2\ln\left|\frac{t-3}{3}\right|+49}-7$$ We know from the original differential equation that $y(t)\neq-7$ & $t\ne3$ . Thus: $$-7\neq \sqrt{2\ln\left|\frac{t-3}{3}\right|+49}-7 \iff\ln\left|\frac{t-3}{3}\right|\neq-\frac{49}{2}\iff t\neq\pm 3\exp\left(-\frac{49}{2}\right)+3$$ Does this tell us that the domain of the solution has to be $(-\infty,-3\exp\left(-\frac{49}{2}\right)+3)$ in order for $t=0$ to be on the domain of this solution (so the solution passes through the initial condition)? Would we just say $y(t)\to0$ as $t\to-3\exp\left(-\frac{49}{2}\right)+3$ ? Then finally $y(t)=\sqrt{2\ln\left(1-\frac{t}{3}\right)+49}-7$ , for $t\in(-\infty, -3\exp\left(-\frac{49}{2}\right)+3)$ .","If we are given the initial-value problem I want to solve said initial value problem & state the domain of the solution. I also want to observe what happens when approaches the limits of the solutionâ€™s domain. Via separation of variables, one obtains: Now call , then Substituting the initial condition: This shows we must choose the positive square root in order for our solution to pass through the initial condition & solve the IVP. Then so that We know from the original differential equation that & . Thus: Does this tell us that the domain of the solution has to be in order for to be on the domain of this solution (so the solution passes through the initial condition)? Would we just say as ? Then finally , for .","\frac{dy}{dt}=\frac{1}{(y+7)(t-3)}, y(0)=0 t \implies(y+7) \space dy = \frac{dt}{t-3}\implies\frac{1}{2}y^2+7y=\ln|t-3|+c_1 \iff y^2+14y+49=2\ln|t-3|+(2c_1+49) C=2c_1+49 (y+7)^2=2\ln|t-3|+C\implies y(t) = \pm\sqrt{2\ln|t-3|+C}-7 0=\pm\sqrt{2\ln|0-3|+C}-7\iff 7 = \pm\sqrt{2\ln(3)+C} y(t) C=49-2\ln(3) y(t)=\sqrt{2\ln\left|\frac{t-3}{3}\right|+49}-7 y(t)\neq-7 t\ne3 -7\neq \sqrt{2\ln\left|\frac{t-3}{3}\right|+49}-7 \iff\ln\left|\frac{t-3}{3}\right|\neq-\frac{49}{2}\iff t\neq\pm 3\exp\left(-\frac{49}{2}\right)+3 (-\infty,-3\exp\left(-\frac{49}{2}\right)+3) t=0 y(t)\to0 t\to-3\exp\left(-\frac{49}{2}\right)+3 y(t)=\sqrt{2\ln\left(1-\frac{t}{3}\right)+49}-7 t\in(-\infty, -3\exp\left(-\frac{49}{2}\right)+3)","['ordinary-differential-equations', 'solution-verification']"
62,Limit for $t\to+\infty$ of a solution to an ODE,Limit for  of a solution to an ODE,t\to+\infty,"Consider the Cauchy problem: \begin{cases} y'(t)=\frac{t^2}{2+\mathrm{sin}(t^2)}cos(y(t)^2) \\ y(0)=0 \end{cases} I have to prove that $\mathrm{lim}_{t\to+\infty} y(t)=\sqrt{\pi/2}$ . I have tried to solve it by separating the variables $t$ and $y$ :  dividing by $\cos(y(t)^2)$ both sides of the equation and integrating, one obtains: $$ \int^{y(t)} _0 \frac{1}{\cos(u^2)}\mathrm{d}u=\int^{t} _0 \frac{s^2}{2+\sin(s^2)}\mathrm{d}s,  $$ but it does not look very useful. Does anyone know how to solve it?","Consider the Cauchy problem: I have to prove that . I have tried to solve it by separating the variables and :  dividing by both sides of the equation and integrating, one obtains: but it does not look very useful. Does anyone know how to solve it?","\begin{cases}
y'(t)=\frac{t^2}{2+\mathrm{sin}(t^2)}cos(y(t)^2) \\
y(0)=0
\end{cases} \mathrm{lim}_{t\to+\infty} y(t)=\sqrt{\pi/2} t y \cos(y(t)^2) 
\int^{y(t)} _0 \frac{1}{\cos(u^2)}\mathrm{d}u=\int^{t} _0 \frac{s^2}{2+\sin(s^2)}\mathrm{d}s, 
","['real-analysis', 'ordinary-differential-equations', 'indefinite-integrals']"
63,How do I solve this stationary Kolmogorov Forward Equation with a Dirac Delta?,How do I solve this stationary Kolmogorov Forward Equation with a Dirac Delta?,,"I have the following stationary Kolmogorov Forward Equation $$ 0 = -\dfrac{\partial}{\partial_x}[axg(x)] - \lambda g(x) + \lambda \delta(x-\underline{x}),$$ where $a>0,\underline{x}>0$ and $\lambda>0$ . Basically we have an impulse at a point $\underline{x}$ and then the density flows to the right at a rate $ax$ . At each instant mass is pulled back into $\underline{x}$ at a rate $\lambda$ . If I ignore the Dirac delta I can easily solve for the density $g(x)$ by integrating the Kolomogorov Forward equation. $$g(x) = g(\underline{x})\left(\dfrac{\underline{x}}{x}\right)^{1+\frac{\lambda}{a}} $$ I then impose that all the probability mass must sum to 1 and that pins down what $g(\underline{x})$ is. $$g(\underline{x}) = \dfrac{\lambda}{a}\underline{x}^{-1}  $$ Solving this problem numerically gives me the same result as above. I have three questions. The way I solved for the stationary density completely ignores the Dirac delta term. How is that possible? A colleague mentioned that as I have an impulse at $\underline{x}$ , my density should sum to one with the mass split between $g(x)$ and a mass $m$ , where $m$ captures the Dirac impulse: $1=\int_{\underline{x}}^{\infty} g(x)dx + m$ . Is this correct, and how could the solution be obtained for the stationary density? Is it possible to have a closed form expression linking an initial condition $g(\underline{x},t=0)=\delta$ to the stationary solution $g(x,t=\infty)=\dfrac{\lambda}{\underline{x}a}\left(\dfrac{\underline{x}}{x}\right)^{1+\frac{\lambda}{a}}$ ? How the density looks:","I have the following stationary Kolmogorov Forward Equation where and . Basically we have an impulse at a point and then the density flows to the right at a rate . At each instant mass is pulled back into at a rate . If I ignore the Dirac delta I can easily solve for the density by integrating the Kolomogorov Forward equation. I then impose that all the probability mass must sum to 1 and that pins down what is. Solving this problem numerically gives me the same result as above. I have three questions. The way I solved for the stationary density completely ignores the Dirac delta term. How is that possible? A colleague mentioned that as I have an impulse at , my density should sum to one with the mass split between and a mass , where captures the Dirac impulse: . Is this correct, and how could the solution be obtained for the stationary density? Is it possible to have a closed form expression linking an initial condition to the stationary solution ? How the density looks:"," 0 = -\dfrac{\partial}{\partial_x}[axg(x)] - \lambda g(x) + \lambda \delta(x-\underline{x}), a>0,\underline{x}>0 \lambda>0 \underline{x} ax \underline{x} \lambda g(x) g(x) = g(\underline{x})\left(\dfrac{\underline{x}}{x}\right)^{1+\frac{\lambda}{a}}  g(\underline{x}) g(\underline{x}) = \dfrac{\lambda}{a}\underline{x}^{-1}   \underline{x} g(x) m m 1=\int_{\underline{x}}^{\infty} g(x)dx + m g(\underline{x},t=0)=\delta g(x,t=\infty)=\dfrac{\lambda}{\underline{x}a}\left(\dfrac{\underline{x}}{x}\right)^{1+\frac{\lambda}{a}}","['ordinary-differential-equations', 'probability-distributions', 'partial-differential-equations', 'dirac-delta', 'stochastic-differential-equations']"
64,Solution to $\frac{1}{6}g''=v g^3+k g^3-k g$,Solution to,\frac{1}{6}g''=v g^3+k g^3-k g,"I have a nice problem with a not-so simple solution. Consider the following differential equation: $$\frac{1}{6}g''=v g^3+kg^3 - k g$$ with boundary conditions $g(0)=0$ and $g(\infty)=1$ , where $k>0$ and $v \geq 0$ . What i've tried so far: Multiply by $g'$ and integrate once: $$(g')^2=3\left[(k+v)g^4-2k g^2+(k-v)\right]$$ where I've added the integration constant $(v+k)$ to ensure that $g(\infty)=1$ and thus $g'(\infty)=0$ . which has a nice solution for $v=0$ , $g'=3k(1-g^2)$ , thus $g=\tanh{(3k x)}$ . However,such a nice factorization of the polynomial does not hold for $v \neq 0$ . This is where your help may come in, if anyone has any suggestions, it would be greatly appreciated.","I have a nice problem with a not-so simple solution. Consider the following differential equation: with boundary conditions and , where and . What i've tried so far: Multiply by and integrate once: where I've added the integration constant to ensure that and thus . which has a nice solution for , , thus . However,such a nice factorization of the polynomial does not hold for . This is where your help may come in, if anyone has any suggestions, it would be greatly appreciated.",\frac{1}{6}g''=v g^3+kg^3 - k g g(0)=0 g(\infty)=1 k>0 v \geq 0 g' (g')^2=3\left[(k+v)g^4-2k g^2+(k-v)\right] (v+k) g(\infty)=1 g'(\infty)=0 v=0 g'=3k(1-g^2) g=\tanh{(3k x)} v \neq 0,"['real-analysis', 'calculus', 'ordinary-differential-equations']"
65,Pendulum with Dirac Comb excitation,Pendulum with Dirac Comb excitation,,"There is a pendulum that is excited by a Dirac Comb. $l \ddot\theta+b\dot \theta+g\theta=G\,\sum_{-\infty}^\infty\delta(t-nT)$ where $l, b, g, G$ are constants and $T=\dfrac{2\pi}{\omega}$ . Show that the resulting motion is given by $\theta(t)=\dfrac{G}{Tl\omega^2}+\dfrac{2G\cos(\omega t-\frac{\pi}{2})}{Tb\omega}$ +[terms with frequencies $\ge$ 2 $\omega$ ] and explain why the higher frequency terms are supressed. My first take was to rearrange to $ \ddot\theta+\dfrac{b}{l}\dot \theta+\omega^2\theta=\frac{G}{l}\,\sum_{-\infty}^\infty\delta(t-nT)$ where $\omega=\sqrt{\dfrac{g}{l}}$ Then, taking the Laplace transform of both sides I got $\Theta(s)=\dfrac{G}{l\,\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}}\, \dfrac{\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}}{\left(s+\frac{b}{2l} \right)^2-\left(\left(\frac{b}{2l}\right)^2-\omega^2 \right)}\, \dfrac{1}{1-e^{-sT}}$ which, as far as I'm concerned transforms to $\theta(t)=\dfrac{G}{l\,\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}}\sum_{n=0}^\infty\,H(t-nT)\,e^{-\frac{b}{2l}(t-nT)}\sin\left(\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}(t-nT) \right)$ And, assuming that this is a correct form of the solution, I can't see how that is equivalent with the function given in the question. I reckon it has something to do with using Fourier series/transform instead? If so, I'm not sure how to do that. Or, is there a way to convert my solution into the given one? I've been struggling with this for a good few days now, so any help would be much appreciated.","There is a pendulum that is excited by a Dirac Comb. where are constants and . Show that the resulting motion is given by +[terms with frequencies 2 ] and explain why the higher frequency terms are supressed. My first take was to rearrange to where Then, taking the Laplace transform of both sides I got which, as far as I'm concerned transforms to And, assuming that this is a correct form of the solution, I can't see how that is equivalent with the function given in the question. I reckon it has something to do with using Fourier series/transform instead? If so, I'm not sure how to do that. Or, is there a way to convert my solution into the given one? I've been struggling with this for a good few days now, so any help would be much appreciated.","l \ddot\theta+b\dot \theta+g\theta=G\,\sum_{-\infty}^\infty\delta(t-nT) l, b, g, G T=\dfrac{2\pi}{\omega} \theta(t)=\dfrac{G}{Tl\omega^2}+\dfrac{2G\cos(\omega t-\frac{\pi}{2})}{Tb\omega} \ge \omega  \ddot\theta+\dfrac{b}{l}\dot \theta+\omega^2\theta=\frac{G}{l}\,\sum_{-\infty}^\infty\delta(t-nT) \omega=\sqrt{\dfrac{g}{l}} \Theta(s)=\dfrac{G}{l\,\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}}\, \dfrac{\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}}{\left(s+\frac{b}{2l} \right)^2-\left(\left(\frac{b}{2l}\right)^2-\omega^2 \right)}\, \dfrac{1}{1-e^{-sT}} \theta(t)=\dfrac{G}{l\,\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}}\sum_{n=0}^\infty\,H(t-nT)\,e^{-\frac{b}{2l}(t-nT)}\sin\left(\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}(t-nT) \right)","['ordinary-differential-equations', 'fourier-series', 'laplace-transform', 'dirac-delta', 'periodic-functions']"
66,$\left\{\begin{matrix} x^2 x'=\sin^2(x^3-3t) \\ x(0)=1 \end{matrix}\right.$,,\left\{\begin{matrix} x^2 x'=\sin^2(x^3-3t) \\ x(0)=1 \end{matrix}\right.,"I have to solve the following Cauchy's problem: $$ \left\{   \begin{align}     & x^2 x'=\sin^2(x^3-3t) \\     & x(0)=1   \end{align} \right. $$ First of all I've tried to identify if it is homogeneous, linear, exact, euler's ... but I can't recognise it. So I don't know which procedure to follow to solve it... I think that my problem is that inside the $\sin$ there are both $x$ and $t$ . Could anyone help me?","I have to solve the following Cauchy's problem: First of all I've tried to identify if it is homogeneous, linear, exact, euler's ... but I can't recognise it. So I don't know which procedure to follow to solve it... I think that my problem is that inside the there are both and . Could anyone help me?","
\left\{
  \begin{align}
    & x^2 x'=\sin^2(x^3-3t) \\
    & x(0)=1
  \end{align}
\right.
 \sin x t","['ordinary-differential-equations', 'partial-differential-equations', 'cauchy-problem']"
67,Determine the stability of equilibrium point with Lyapunov function,Determine the stability of equilibrium point with Lyapunov function,,"I want to determine the stability of $(0,0)$ (stable, asymptotically stable or unstable) in the nonlinear system: $$ \begin{aligned}     \dot{x} &=  y +       xy \\    \dot{y} &= -y + \sin^2(x) \end{aligned} $$ My attempt I tried using the eigenvalues of Jacobian evaluated at (0,0), but since $$\det\left(J_{(0,0)}\right) = \det \begin{pmatrix} 0 & 1 \\ 0 & -1 \end{pmatrix} = 0$$ that criterion for evaluating stability does not work, so I then proposed the following Lyapunov function: $$V(x,y) = \dfrac{1}{2}x^2 + \dfrac{1}{4}y^4$$ then $$\dot{V}(x,y) = xy + x^2y - y^4 + y^3\sin^2(x)$$ Doing a simple analysis of regions, it is easy to see that $$\dot{V}(x,y) < 0, \ \mbox{ for } x > 0, \ y < 0$$ I understand that the Lyapunov stability criterion allows us to conclude that $(0,0)$ is stable if $\dot{V}(x,y) \leq 0$ and asymptotically stable if $\dot{V}(x,y) < 0$ for $(x,y) \neq (0,0)$ . My first question is about the previous result, since it is not clear to me whether to conclude the type of stability it is necessary to prove the results for every point $(x,y) \neq (0,0)$ in $\mathbb{R}^2$ or only for any $(x,y) \neq (0,0)$ within a neighborhood $U$ . The second question has to do with my analysis of the behavior of the derivative in a region: in the mentioned region $\{x>0,\ y <0\}$ , I could conclude that the derivative is strictly negative, however, for the region $\{x < 0 ,\  y < 0\}$ , the derivative can take positive values, can I state, just for the first region, that the (0,0) is asymptotically stable or due to the behavior not necessarily less than or equal to zero of the derivative at $\{x < 0, y < 0\}$ , should it be stated that the point is unstable? The problem is that I have done other Lyapunov stability exercises where it is very simple to verify that, for any point other than the equilibrium point, the derivative $\dot{V} \leq 0$ or $\dot{V} < 0$ and therefore, to conclude the stability of the equilibrium point is simple, however, this exercise has made me realize that maybe I am not understanding well the stability criterion or, I have problem finding a Lyapunov function that is useful for the analysis. I would appreciate any help you can give me to better understand and conclude my result.","I want to determine the stability of (stable, asymptotically stable or unstable) in the nonlinear system: My attempt I tried using the eigenvalues of Jacobian evaluated at (0,0), but since that criterion for evaluating stability does not work, so I then proposed the following Lyapunov function: then Doing a simple analysis of regions, it is easy to see that I understand that the Lyapunov stability criterion allows us to conclude that is stable if and asymptotically stable if for . My first question is about the previous result, since it is not clear to me whether to conclude the type of stability it is necessary to prove the results for every point in or only for any within a neighborhood . The second question has to do with my analysis of the behavior of the derivative in a region: in the mentioned region , I could conclude that the derivative is strictly negative, however, for the region , the derivative can take positive values, can I state, just for the first region, that the (0,0) is asymptotically stable or due to the behavior not necessarily less than or equal to zero of the derivative at , should it be stated that the point is unstable? The problem is that I have done other Lyapunov stability exercises where it is very simple to verify that, for any point other than the equilibrium point, the derivative or and therefore, to conclude the stability of the equilibrium point is simple, however, this exercise has made me realize that maybe I am not understanding well the stability criterion or, I have problem finding a Lyapunov function that is useful for the analysis. I would appreciate any help you can give me to better understand and conclude my result.","(0,0)  \begin{aligned} 
   \dot{x} &=  y +       xy \\
   \dot{y} &= -y + \sin^2(x) \end{aligned}  \det\left(J_{(0,0)}\right) = \det \begin{pmatrix} 0 & 1 \\ 0 & -1 \end{pmatrix} = 0 V(x,y) = \dfrac{1}{2}x^2 + \dfrac{1}{4}y^4 \dot{V}(x,y) = xy + x^2y - y^4 + y^3\sin^2(x) \dot{V}(x,y) < 0, \ \mbox{ for } x > 0, \ y < 0 (0,0) \dot{V}(x,y) \leq 0 \dot{V}(x,y) < 0 (x,y) \neq (0,0) (x,y) \neq (0,0) \mathbb{R}^2 (x,y) \neq (0,0) U \{x>0,\ y <0\} \{x < 0 ,\  y < 0\} \{x < 0, y < 0\} \dot{V} \leq 0 \dot{V} < 0","['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system', 'stability-in-odes', 'lyapunov-functions']"
68,"Differential equation $y'(t) = e^{-t \, y(t)}$ [closed]",Differential equation  [closed],"y'(t) = e^{-t \, y(t)}","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I was confronted to this equation $y^{'}(t) = e^{-t \, y(t)}$ ; I don't feel it's possible to explicitly solve it, but I don't see how to explain that it's not possible... Thanks for any suggestion.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I was confronted to this equation ; I don't feel it's possible to explicitly solve it, but I don't see how to explain that it's not possible... Thanks for any suggestion.","y^{'}(t) = e^{-t \, y(t)}",['ordinary-differential-equations']
69,Duhamel's Principle Intuition,Duhamel's Principle Intuition,,"I am trying to understand Duhamel's Principle by applying it to some simple problems.  I am thinking of $P(t)$ as expressing a bank account balance at time $t$ , to try to gain an intuition for Duhamel's Principle that doesn't depend on understanding physics. First I considered $P'(t) - \frac{1}{10}P(t) = \pi,\ P(0) = 0$ .  I started by solving $Q'(\tau) - \frac{1}{10}Q(\tau) = 0,\ Q(0) = \pi$ (swapping the forcing term and initial condition) and got the answer $Q(\tau) = \pi e^{\frac{1}{10}\tau}$ .  It appears that I can simply integrate this and produce the correct solution as $P(t) = \int_0^t \pi e^{\frac{1}{10}\tau} d\tau = 10\pi e^{\frac{1}{10}t} - 10\pi$ .  In English, this seems to say, ""To calculate the balance of a bank account starting with $\$0$ and continuously receiving deposits of $\$\pi$ per time unit, integrate between $0$ and $t$ the balance of a bank account starting with $\$\pi$ and continuously receiving deposits of $\$0$ per time unit.""  Of note is that both the real and hypothetical accounts have $10\%$ continuous compounding, but their time units are not necessarily the same ( $t$ and $\tau$ are presumably two forms of time). Since the above statement is not obvious, I decided to try a discrete version of the problem so I could then track the account balances at each time step.  To consider $P[n + 1] - \frac{1}{10}P[n] = \pi,\ P[0] = 0$ , I solved $Q[m + 1] - \frac{1}{10}Q[m] = 0,\ Q[0] = \pi$ .  The answer I got is $Q[m] = \frac{\pi}{10^m}$ , and inspired by the above integration, I took a summation from $0$ to $n$ to produce the solution $\displaystyle P[n] = \sum_{m = 0}^n \frac{\pi}{10^m} = \frac{\pi (10)^{n + 1} - \pi}{9(10)^n}$ .  However, I believe this is slightly incorrect and the solution should be $P[n] = \frac{\pi (10)^n - \pi}{9(10)^{n - 1}}$ . Can I use Duhamel's Principle to solve difference equations?  If so, what mistake have I made in the computations?  Is there a nice description of Duhamel's Principle in the context of this sort of bank account model (continuous or discrete)?","I am trying to understand Duhamel's Principle by applying it to some simple problems.  I am thinking of as expressing a bank account balance at time , to try to gain an intuition for Duhamel's Principle that doesn't depend on understanding physics. First I considered .  I started by solving (swapping the forcing term and initial condition) and got the answer .  It appears that I can simply integrate this and produce the correct solution as .  In English, this seems to say, ""To calculate the balance of a bank account starting with and continuously receiving deposits of per time unit, integrate between and the balance of a bank account starting with and continuously receiving deposits of per time unit.""  Of note is that both the real and hypothetical accounts have continuous compounding, but their time units are not necessarily the same ( and are presumably two forms of time). Since the above statement is not obvious, I decided to try a discrete version of the problem so I could then track the account balances at each time step.  To consider , I solved .  The answer I got is , and inspired by the above integration, I took a summation from to to produce the solution .  However, I believe this is slightly incorrect and the solution should be . Can I use Duhamel's Principle to solve difference equations?  If so, what mistake have I made in the computations?  Is there a nice description of Duhamel's Principle in the context of this sort of bank account model (continuous or discrete)?","P(t) t P'(t) - \frac{1}{10}P(t) = \pi,\ P(0) = 0 Q'(\tau) - \frac{1}{10}Q(\tau) = 0,\ Q(0) = \pi Q(\tau) = \pi e^{\frac{1}{10}\tau} P(t) = \int_0^t \pi e^{\frac{1}{10}\tau} d\tau = 10\pi e^{\frac{1}{10}t} - 10\pi \0 \\pi 0 t \\pi \0 10\% t \tau P[n + 1] - \frac{1}{10}P[n] = \pi,\ P[0] = 0 Q[m + 1] - \frac{1}{10}Q[m] = 0,\ Q[0] = \pi Q[m] = \frac{\pi}{10^m} 0 n \displaystyle P[n] = \sum_{m = 0}^n \frac{\pi}{10^m} = \frac{\pi (10)^{n + 1} - \pi}{9(10)^n} P[n] = \frac{\pi (10)^n - \pi}{9(10)^{n - 1}}","['ordinary-differential-equations', 'solution-verification', 'recurrence-relations', 'intuition', 'finance']"
70,Laplace Transform piecewise function with domain from 1 to inf,Laplace Transform piecewise function with domain from 1 to inf,,"I have been asked to compute the Laplace Transform of the following piecewise function \begin{equation} f(t) = \begin{cases} t - 1 \quad 1 \leq t < 2 \\ 3 - t \quad 2 \leq t < 3 \\ 0 \quad t \geq 3 \end{cases} \end{equation} I could not find an example where the domain of the piecewise function to transform does not begin at $0$ , I was wondering if it could be possible to shift the function to the left $f(t + 1)$ , so that the domain begins at $0$ , rewrite in terms of the unit step function and transform it. Is that something valid to do? or how should these cases be handled?","I have been asked to compute the Laplace Transform of the following piecewise function I could not find an example where the domain of the piecewise function to transform does not begin at , I was wondering if it could be possible to shift the function to the left , so that the domain begins at , rewrite in terms of the unit step function and transform it. Is that something valid to do? or how should these cases be handled?","\begin{equation}
f(t) = \begin{cases}
t - 1 \quad 1 \leq t < 2 \\
3 - t \quad 2 \leq t < 3 \\
0 \quad t \geq 3
\end{cases}
\end{equation} 0 f(t + 1) 0","['ordinary-differential-equations', 'laplace-transform']"
71,It is possible for a scalar finite-duration continuous system to achieve an infinite speed (in finite-time)? How if true? Why not if false?,It is possible for a scalar finite-duration continuous system to achieve an infinite speed (in finite-time)? How if true? Why not if false?,,"It is possible for a scalar finite-duration continuous system to achieve an infinite speed (in finite-time)? How if it true? Why not if it false? (Please read first the restrictions of the system I am interested in). Since new information has rise, now I am actually more interested in the ""Added Later"" part I have change the term ""time-limited"" for ""finite-duration"" since is more accurate and widely used, but it means that the scalar one-variable function $f(t)$ has an starting time $t_0$ for which $f(t) = 0\,\forall t<t_0$ , and also an ending time $t_F$ from which $f(t) = 0\,\forall t>t_F$ , with $t_0 < t_F$ To avoid some ""bad-behaved"" functions, I am thinking in functions $f(t)$ as a position of a classical object that change with time, with the function describing its position restricted as: $f(t)$ is continuous, including it been so in the following way: $\forall\,t_d,\,f(t_d^-) = f(t_d^+)$ so there is no ""jump-like discontinuities"" in the function $f(t)$ , so no ""teleportation"" is allowed. With this, I am avoiding the existence of delta functions components $\delta(t)$ in the derivative $f'(t)$ because of jump discontinuities (I donÂ´t know if could have them because of other reasons). Also, I am thinking in well-behaved ""classic"" continuous functions, not things like a Brownian motion which is continuous and nowhere differentiable (I donÂ´t know how to ""formally"" described this restriction - I hope you get the idea). The function $f(t)$ is of finite-duration, so compact-supported since is a one variable function (scalar). Since is continuous and compact-supported is also bounded, so $\sup\limits_t |f(t)| = \|f(t)\|_\infty < \infty$ . The function $f(t)$ has a well defined Fourier transform $F(w)$ (analytic, and follows the Riemann-Lebesgue Lemma). Since for a compacted-supported function the Fourier transform could have some ""issues"" at the boundaries of it domain $\partial t = \{t_0,\,t_F\}$ , you could assume that $f(t_0)=f(t_F)=0$ if that makes it more easy to work with ( here I found a way to overcome this issues for arbitrary finite $\{f(t_0),\,f(t_F)\}$ so generality is sustained). The function $f(t)$ have finite energy $\int_{t_0}^{t_F} |f(t)|^2 dt < \infty$ , and is also Lebesgue integrable $\int_{t_0}^{t_F} |f(t)| dt < \infty$ (later I learned that if continuous and compacted supported, then is bounded, and this imply that the Fourier Transform is analytic, and then adding is Lebesgue Integrable implies then is of finite energy since is bounded, and I believe all this implies then the Riemann-Lebesgue follows, but not quite sure about the last affirmation) . I believe that if the function $f(t)$ have sharp-edges/tips/spikes like the absolute-value function at the origin, the first derivative (speed) $f'(t)$ will be discontinuous, but since $f(t)$ is bounded the ""jump"" in the derivative will be also bounded: as example, let $f(t) = \sin(|t|\pi),\,|t|\leq 1$ (plot here ), it have a sharp edge at $t=0$ so its derivative have a jump-discontinuity, but $\sup\limits_t |f'(t)| = \pi$ . At first glance, I thought that to have a spike with $\sup\limits_t |f'(t)| \to \infty$ at some time $t_d$ it has to have some $|f(t_d^-)| = |\lim\limits_{t \to t_d^-} f(t)| \to \infty$ and/or $|f(t_d^+)| = |\lim\limits_{t \to t_d^+} f(t)| \to \infty$ making the function $f(t)$ discontinuous, which is not allowed (as counterexample, $f(t) = 1/|t|$ has an ""infinite jump"" in $f'(t)$ ), but then I found the function $f(t) = \left|t \cdot \displaystyle{\frac{\log(t^2)}{2}}\right|,\,|t|\leq 1$ which have a sharp edge where an infinite speed is achieved with an infinite-size jump-discontinuity on $f'(t)$ without being $f(t)$ a discontinuous function (it also has bounded variation equal to $4/e$ even given it has a discontinuity on its derivative). Nevertheless, note that this functions can achieve these infinite speeds only at isolated/disjointed single points (of measure zero), or it will have a sudden position change with zero time, creating a jump-discontinuity in $f(t)$ which is not allowed from the assumptions (continuity part). With this in mind, the second derivative (acceleration) $f''(t)$ , must be allowed to have infinite jump discontinuities, or no abrupt changes of direction could be allowed, like crashing and bouncing from a wall, so the jump-discontinuity in $f'(t)$ will become a delta function $\delta(t) \equiv \infty$ in the acceleration profile (as example for $f(t) = \sin(|t|\pi),\,|t|\leq 1$ , its second derivative is $f''(t) = 2\pi\delta(t)-\pi^2 \sin(|t|\pi),\,|t|\leq 1$ ). So acceleration is necessarily unbounded. Also, speed must be allow to have discontinuities or no sudden changes in direction would be achieved - meaning this that smoothness is a too restrictive for this function, discarding Analytic solutions! . So, If I am right, an unbounded acceleration makes possible to achieve an infinite speed on a finite time, but it doesnÂ´t mean that there are other laws making impossible to it to be unbounded - law which I am looking for. This behavior can be seen in the following examples: $f(t) = \sqrt{1-t^2},\,|t|\leq 1$ , which starts/ends with $|f'(t)| \to \infty$ as can be seen on its plot here . $f(t) = t \cdot \displaystyle{\frac{\log(t^2)}{2}},\,|t|\leq 1$ , which ""softly"" achieve $|f'(t)| \to \infty$ at $t=0$ as can be seen on its plot here . $f(t) = \left|t \cdot \displaystyle{\frac{\log(t^2)}{2}}\right|,\,|t|\leq 1$ , which achieve $|f'(t)| \to \infty$ at $t=0$ with an infinite-size jump-discontinuity in $f'(t)$ , as can be seen on its plot here . So the main question of the beginning can be divided as: It is possible for a real-life classic mechanical system to behave as the last examples? So achieving and infinite speed in a finite time? Here, I think that a model that can achieve an infinite speed, even if is only in one point in time, will violate every possible physics model with a finite speed for causality, but I havenÂ´t found yet how causality conditions will restrict the derivative of these finite-duration functions. Can you think of examples of real life classic mechanic systems that behaves as the last examples? (I canÂ´t made yet by myself an idea of an example). There are other ways of achieving an unbounded maximum rate of change for a classic mechanic system that are not included in the scenarios I mention?? (keeping the restrictions over $f(t)$ ). If is not possible, Which physics laws are avoiding it to happen? (here meaning that the maximum rate of change must be bounded because of these laws) . Here is where the problem of the bounds of the domain can make struggles in the Fourier Transform, since $\sup\limits_t |f'(t)| \leq \frac{1}{2\pi} \int_{-\infty}^\infty |w F(w)|dw$ which can fictitiously diverge because of the effect of the discontinuities at the ""edges"" of the time-limited compact-support, but they can be avoided as is explained here by using instead $\sup\limits_t |f'(t)| \leq \frac{1}{2\pi} \int_{-\infty}^\infty |iw F(w)+f(t_F)e^{-iwt_F}-f(t_0)e^{-iwt_0}|dw$ . If there are mistakes in my argumentation, please let me know what assumptions/lines-of-thought are wrong. Added Later I have found recently two papers from the author V. T. Haimo (1985), that analyze finite-duration differential equations [ 1 ] and [ 2 ]. From them, I realize I could be mixing two things into the question: first, math actually could stand any function cropped at some compact-domain to be treated as a finite-duration function, and second, that it is not really what I want to know, actually what I am looking for are for solutions to differential equations which are of finite-duration . On the papers the author explain that: ""One notices immediately that finite time differential equations cannot be Lipschitz at the origin. As all solutions reach zero in finite time, there is non-uniqueness of solutions through zero in backwards time. This, of course, violates the uniqueness condition for solutions of Lipschitz differential equations."" Since, linear differential equations have solutions that are unique, and finite-duration solutions arenÂ´t, finite-duration phenomena models must be non-linear differential eqs. to show the required behavior (non meaning this, that every non-linear dynamic system support finite-duration solutions). Also, since the system ""dies"" at the end of the domain, the solutions will have the same issues than compacted-supported functions in this ending point, which will leads that finite-duration solutions cannot be Analytic in the whole time domain (maybe using functions defined piece-wise could work, like common bump-functions $\in C_c^\infty$ are defined, but no restricting the starting point to be also zero - which is a requirement for bump-functions for keeping smoothness). Note: discarding ""whole-domain analytic functions"" like Power Series, and also Linear ODEs, actually ""discards"" almost-all the maths knowledge I acquire on engineering, so this is totally new for me. The papers also show which conditions must fulfill the non-linear differential equation to support finite-duration solutions, at least for first and second order scalar ODEs. And in [1] on Theorem 2 point (i), it is said that, without losing generality by considering that the ending time of the finite-duration solution happens at $t_F = 0$ , for a second order dynamical system described by $\ddot{x}(t) = g(x(t),\dot{x}(t))$ such $g(0,0)=0$ (the system dynamics ""die"" at $t_F = 0$ ), with $g \in C^1(\mathbb{R}\setminus \{0\})$ , then for the system to support finite-duration solutions, the following another differential equation must have solutions: $$q(z)\frac{dq(z)}{dz} = g(z,q(z)),\,q(0)=0$$ Honestly the papers are bit advanced to my mathematical skills, but if I didnÂ´t made any mistakes, what the author is doing is splitting the second derivative of the scalar one-variable function $x(t)$ as: $$\ddot{x} = \frac{d}{dt}\frac{dx(t)}{dt} = \frac{d}{dt}\frac{dx}{dx}\frac{d\,x(t)}{dt} = \frac{dx}{dt}\frac{d}{dx}\frac{dx}{dt} = q(z)\frac{dq(z)}{dz}$$ by using the change of variable $z=x(t)$ and $q(z)=\dot{x}$ . I donÂ´t really understand why this transformation leads to another differential equation that ""tells"" how the original equation will behave, so the following analysis is probably wrong, but I want to share it with you so you can correct me: Since I am looking from the maximum speed $\sup_t |\dot{x}|$ , and from the papers looks like I can figure out the behavior of $\dot{x}$ from $q(z)$ , I believe whatever it achieve a maximum, the values obtained should be the same, so finding $\sup_t |\dot{x}| \equiv \sup_z |q(z)|$ . With this, since $q(z)=0$ is not really a value I ""care"", I could use first order conditions to look for the maximum value of $q(z)$ , so I need to find $z$ such $$\frac{dq(z)}{dz}=0 \rightarrow z^* \rightarrow q(z^*)$$ So, since I am interested in $q(z) \neq 0$ , looking for the first order conditions is equivalent to looking for $q(z)\frac{dq(z)}{dz}=0$ , which is indeed the same that looking for $\ddot{x} = 0$ (if my assumption of interchangeability of equations is right - which I believe is not), so it would be meaning that finite-duration solutions of differential equations only can achieve their maximum speeds at inflection points of the acceleration profile where it is equal to zero $\ddot{x}=0$ , which instantly discard situations as the example $f(t) = t \cdot \displaystyle{\frac{\log(t^2)}{2}},\,|t|\leq 1$ , which ""softly"" achieve $|f'(t)| \to \infty$ at $t=0$ but at this points it second derivative is non-zero (actually diverges to infinity). This is quite an aggressive affirmation (so, probably wrong), since it can be reformulated as: finite-duration solutions to scalar-second-order differential equations, which are of unlimited bandwidth because of having finite-duration so they could achieve infinite speeds in principle, are actually restricted by being solutions of finite-time differential equations  (with $g(x, \dot{x}) \in C^1(\mathbb{R}\setminus \{0\})$ ) so they can achieve their maximum speeds only when it acceleration is zero discarding it of happening at discontinuities, so their maximum speeds are actually bounded .... this is actually too good to be true (maybe it happens because of the restriction on $g(x,\dot{x})$ ), but it is quite interesting to see which restrictions could rise for mechanical systems described by these finite-time differential equations, and I did not find too much information related to them so I think are quite unknown. So far I have only found nunerical representatuons of this finite-duration solutions, and maybe the mentioned paper is only reviewing the behavior near the ending point, but anyway, any example of a function that is a finite-duration solution of a differential equation will be preciated (I don't even know if a close-form is possible). Hope you can comment about.","It is possible for a scalar finite-duration continuous system to achieve an infinite speed (in finite-time)? How if it true? Why not if it false? (Please read first the restrictions of the system I am interested in). Since new information has rise, now I am actually more interested in the ""Added Later"" part I have change the term ""time-limited"" for ""finite-duration"" since is more accurate and widely used, but it means that the scalar one-variable function has an starting time for which , and also an ending time from which , with To avoid some ""bad-behaved"" functions, I am thinking in functions as a position of a classical object that change with time, with the function describing its position restricted as: is continuous, including it been so in the following way: so there is no ""jump-like discontinuities"" in the function , so no ""teleportation"" is allowed. With this, I am avoiding the existence of delta functions components in the derivative because of jump discontinuities (I donÂ´t know if could have them because of other reasons). Also, I am thinking in well-behaved ""classic"" continuous functions, not things like a Brownian motion which is continuous and nowhere differentiable (I donÂ´t know how to ""formally"" described this restriction - I hope you get the idea). The function is of finite-duration, so compact-supported since is a one variable function (scalar). Since is continuous and compact-supported is also bounded, so . The function has a well defined Fourier transform (analytic, and follows the Riemann-Lebesgue Lemma). Since for a compacted-supported function the Fourier transform could have some ""issues"" at the boundaries of it domain , you could assume that if that makes it more easy to work with ( here I found a way to overcome this issues for arbitrary finite so generality is sustained). The function have finite energy , and is also Lebesgue integrable (later I learned that if continuous and compacted supported, then is bounded, and this imply that the Fourier Transform is analytic, and then adding is Lebesgue Integrable implies then is of finite energy since is bounded, and I believe all this implies then the Riemann-Lebesgue follows, but not quite sure about the last affirmation) . I believe that if the function have sharp-edges/tips/spikes like the absolute-value function at the origin, the first derivative (speed) will be discontinuous, but since is bounded the ""jump"" in the derivative will be also bounded: as example, let (plot here ), it have a sharp edge at so its derivative have a jump-discontinuity, but . At first glance, I thought that to have a spike with at some time it has to have some and/or making the function discontinuous, which is not allowed (as counterexample, has an ""infinite jump"" in ), but then I found the function which have a sharp edge where an infinite speed is achieved with an infinite-size jump-discontinuity on without being a discontinuous function (it also has bounded variation equal to even given it has a discontinuity on its derivative). Nevertheless, note that this functions can achieve these infinite speeds only at isolated/disjointed single points (of measure zero), or it will have a sudden position change with zero time, creating a jump-discontinuity in which is not allowed from the assumptions (continuity part). With this in mind, the second derivative (acceleration) , must be allowed to have infinite jump discontinuities, or no abrupt changes of direction could be allowed, like crashing and bouncing from a wall, so the jump-discontinuity in will become a delta function in the acceleration profile (as example for , its second derivative is ). So acceleration is necessarily unbounded. Also, speed must be allow to have discontinuities or no sudden changes in direction would be achieved - meaning this that smoothness is a too restrictive for this function, discarding Analytic solutions! . So, If I am right, an unbounded acceleration makes possible to achieve an infinite speed on a finite time, but it doesnÂ´t mean that there are other laws making impossible to it to be unbounded - law which I am looking for. This behavior can be seen in the following examples: , which starts/ends with as can be seen on its plot here . , which ""softly"" achieve at as can be seen on its plot here . , which achieve at with an infinite-size jump-discontinuity in , as can be seen on its plot here . So the main question of the beginning can be divided as: It is possible for a real-life classic mechanical system to behave as the last examples? So achieving and infinite speed in a finite time? Here, I think that a model that can achieve an infinite speed, even if is only in one point in time, will violate every possible physics model with a finite speed for causality, but I havenÂ´t found yet how causality conditions will restrict the derivative of these finite-duration functions. Can you think of examples of real life classic mechanic systems that behaves as the last examples? (I canÂ´t made yet by myself an idea of an example). There are other ways of achieving an unbounded maximum rate of change for a classic mechanic system that are not included in the scenarios I mention?? (keeping the restrictions over ). If is not possible, Which physics laws are avoiding it to happen? (here meaning that the maximum rate of change must be bounded because of these laws) . Here is where the problem of the bounds of the domain can make struggles in the Fourier Transform, since which can fictitiously diverge because of the effect of the discontinuities at the ""edges"" of the time-limited compact-support, but they can be avoided as is explained here by using instead . If there are mistakes in my argumentation, please let me know what assumptions/lines-of-thought are wrong. Added Later I have found recently two papers from the author V. T. Haimo (1985), that analyze finite-duration differential equations [ 1 ] and [ 2 ]. From them, I realize I could be mixing two things into the question: first, math actually could stand any function cropped at some compact-domain to be treated as a finite-duration function, and second, that it is not really what I want to know, actually what I am looking for are for solutions to differential equations which are of finite-duration . On the papers the author explain that: ""One notices immediately that finite time differential equations cannot be Lipschitz at the origin. As all solutions reach zero in finite time, there is non-uniqueness of solutions through zero in backwards time. This, of course, violates the uniqueness condition for solutions of Lipschitz differential equations."" Since, linear differential equations have solutions that are unique, and finite-duration solutions arenÂ´t, finite-duration phenomena models must be non-linear differential eqs. to show the required behavior (non meaning this, that every non-linear dynamic system support finite-duration solutions). Also, since the system ""dies"" at the end of the domain, the solutions will have the same issues than compacted-supported functions in this ending point, which will leads that finite-duration solutions cannot be Analytic in the whole time domain (maybe using functions defined piece-wise could work, like common bump-functions are defined, but no restricting the starting point to be also zero - which is a requirement for bump-functions for keeping smoothness). Note: discarding ""whole-domain analytic functions"" like Power Series, and also Linear ODEs, actually ""discards"" almost-all the maths knowledge I acquire on engineering, so this is totally new for me. The papers also show which conditions must fulfill the non-linear differential equation to support finite-duration solutions, at least for first and second order scalar ODEs. And in [1] on Theorem 2 point (i), it is said that, without losing generality by considering that the ending time of the finite-duration solution happens at , for a second order dynamical system described by such (the system dynamics ""die"" at ), with , then for the system to support finite-duration solutions, the following another differential equation must have solutions: Honestly the papers are bit advanced to my mathematical skills, but if I didnÂ´t made any mistakes, what the author is doing is splitting the second derivative of the scalar one-variable function as: by using the change of variable and . I donÂ´t really understand why this transformation leads to another differential equation that ""tells"" how the original equation will behave, so the following analysis is probably wrong, but I want to share it with you so you can correct me: Since I am looking from the maximum speed , and from the papers looks like I can figure out the behavior of from , I believe whatever it achieve a maximum, the values obtained should be the same, so finding . With this, since is not really a value I ""care"", I could use first order conditions to look for the maximum value of , so I need to find such So, since I am interested in , looking for the first order conditions is equivalent to looking for , which is indeed the same that looking for (if my assumption of interchangeability of equations is right - which I believe is not), so it would be meaning that finite-duration solutions of differential equations only can achieve their maximum speeds at inflection points of the acceleration profile where it is equal to zero , which instantly discard situations as the example , which ""softly"" achieve at but at this points it second derivative is non-zero (actually diverges to infinity). This is quite an aggressive affirmation (so, probably wrong), since it can be reformulated as: finite-duration solutions to scalar-second-order differential equations, which are of unlimited bandwidth because of having finite-duration so they could achieve infinite speeds in principle, are actually restricted by being solutions of finite-time differential equations  (with ) so they can achieve their maximum speeds only when it acceleration is zero discarding it of happening at discontinuities, so their maximum speeds are actually bounded .... this is actually too good to be true (maybe it happens because of the restriction on ), but it is quite interesting to see which restrictions could rise for mechanical systems described by these finite-time differential equations, and I did not find too much information related to them so I think are quite unknown. So far I have only found nunerical representatuons of this finite-duration solutions, and maybe the mentioned paper is only reviewing the behavior near the ending point, but anyway, any example of a function that is a finite-duration solution of a differential equation will be preciated (I don't even know if a close-form is possible). Hope you can comment about.","f(t) t_0 f(t) = 0\,\forall t<t_0 t_F f(t) = 0\,\forall t>t_F t_0 < t_F f(t) f(t) \forall\,t_d,\,f(t_d^-) = f(t_d^+) f(t) \delta(t) f'(t) f(t) \sup\limits_t |f(t)| = \|f(t)\|_\infty < \infty f(t) F(w) \partial t = \{t_0,\,t_F\} f(t_0)=f(t_F)=0 \{f(t_0),\,f(t_F)\} f(t) \int_{t_0}^{t_F} |f(t)|^2 dt < \infty \int_{t_0}^{t_F} |f(t)| dt < \infty f(t) f'(t) f(t) f(t) = \sin(|t|\pi),\,|t|\leq 1 t=0 \sup\limits_t |f'(t)| = \pi \sup\limits_t |f'(t)| \to \infty t_d |f(t_d^-)| = |\lim\limits_{t \to t_d^-} f(t)| \to \infty |f(t_d^+)| = |\lim\limits_{t \to t_d^+} f(t)| \to \infty f(t) f(t) = 1/|t| f'(t) f(t) = \left|t \cdot \displaystyle{\frac{\log(t^2)}{2}}\right|,\,|t|\leq 1 f'(t) f(t) 4/e f(t) f''(t) f'(t) \delta(t) \equiv \infty f(t) = \sin(|t|\pi),\,|t|\leq 1 f''(t) = 2\pi\delta(t)-\pi^2 \sin(|t|\pi),\,|t|\leq 1 f(t) = \sqrt{1-t^2},\,|t|\leq 1 |f'(t)| \to \infty f(t) = t \cdot \displaystyle{\frac{\log(t^2)}{2}},\,|t|\leq 1 |f'(t)| \to \infty t=0 f(t) = \left|t \cdot \displaystyle{\frac{\log(t^2)}{2}}\right|,\,|t|\leq 1 |f'(t)| \to \infty t=0 f'(t) f(t) \sup\limits_t |f'(t)| \leq \frac{1}{2\pi} \int_{-\infty}^\infty |w F(w)|dw \sup\limits_t |f'(t)| \leq \frac{1}{2\pi} \int_{-\infty}^\infty |iw F(w)+f(t_F)e^{-iwt_F}-f(t_0)e^{-iwt_0}|dw \in C_c^\infty t_F = 0 \ddot{x}(t) = g(x(t),\dot{x}(t)) g(0,0)=0 t_F = 0 g \in C^1(\mathbb{R}\setminus \{0\}) q(z)\frac{dq(z)}{dz} = g(z,q(z)),\,q(0)=0 x(t) \ddot{x} = \frac{d}{dt}\frac{dx(t)}{dt} = \frac{d}{dt}\frac{dx}{dx}\frac{d\,x(t)}{dt} = \frac{dx}{dt}\frac{d}{dx}\frac{dx}{dt} = q(z)\frac{dq(z)}{dz} z=x(t) q(z)=\dot{x} \sup_t |\dot{x}| \dot{x} q(z) \sup_t |\dot{x}| \equiv \sup_z |q(z)| q(z)=0 q(z) z \frac{dq(z)}{dz}=0 \rightarrow z^* \rightarrow q(z^*) q(z) \neq 0 q(z)\frac{dq(z)}{dz}=0 \ddot{x} = 0 \ddot{x}=0 f(t) = t \cdot \displaystyle{\frac{\log(t^2)}{2}},\,|t|\leq 1 |f'(t)| \to \infty t=0 g(x, \dot{x}) \in C^1(\mathbb{R}\setminus \{0\}) g(x,\dot{x})","['ordinary-differential-equations', 'solution-verification', 'physics', 'nonlinear-dynamics', 'finite-duration']"
72,Help showing the classical Legendre equation has limit circle boundary points.,Help showing the classical Legendre equation has limit circle boundary points.,,"I am following this paper by Krall and Zettl . I am trying to use the results of Sturm-Louiville (SL) theory to study eigen functions of the classical Legendre equation: $$ \tag1 \frac{d}{dx}\left((1-x^2)\frac{du}{dx}\right)=\lambda u $$ This SL problem is singular at the points $x=\pm1$ as around these points $(1-x^2)^{-1}$ is not locally integrable. I am trying to show that the end points are limit-circle. i.e. that for all solutions $u$ , of $(1)$ that $$\tag 2u\in L^2(-1,\beta) \quad \forall \beta \in (-1,1) \quad\text{ ( so $x=-1$ is limit circle)}$$ $$ \tag 3u\in L^2(\alpha,1) \quad \forall \alpha \in (-1,1) \quad\text{ ( so $x=1$ is limit circle)}$$ I know the solutions of $(1)$ are given by the Legendre functions $P_\lambda(x),Q_\lambda(x)$ and so I think showing $(2)$ and $(3)$ is equivalent to showing that the $L^2$ norms for both (??) $P_\lambda(x)$ and $Q_\lambda(x)$ are finite, i.e. showing $$\int^\beta_{-1} P_\lambda(x)^2 dx, \int^\beta_{-1} Q_\lambda(x)^2 dx \quad\text{and}\quad\int^1_{\alpha} P_\lambda(x)^2 dx,\int^1_{\alpha} Q_\lambda(x)^2 dx$$ are finite. Here is where I get confused, it is stated in the above linked paper that the SL problem is indeed limit circle at both endpoints. But, If $\lambda$ is taken to be an integer then $P_\lambda(x)$ is a finite degree polynomial and $||P_\lambda||<\infty$ is obvious. However $Q_\lambda(x)$ can still be singular in this case...  Is it the case that $Q_\lambda(x)$ diverges slow enough as $x \to \pm1$ to have a finite $L^2$ norm? In the case $\lambda$ is not an integer (this case I am most interested in), the behaviour of $P_\lambda$ and $Q_\lambda$ only get more singular around $x=\pm1$ . In the linked article by Krall et.al. it is mentioned that the classification of boundary points into limit circle / limit point is independent of $\lambda$ so I am assuming there must be some argument in the non-integer $\lambda$ case which shows the poles of $P_\lambda$ and $Q_\lambda$ diverge sufficiently slowly to be in $L^2$ . My first question is how can I show this? I suspect it might be possible by considering the representations of $P_\lambda$ and $Q_\lambda$ in terms of the hypergeometric series, however I have little experience with this and am worried about $_2F_1$ diverging when $x=-1$ since I think this is outside of $_2F_1$ 's radius of convergence... Maybe there is an asymptotic argument? My second question concerns the study of the associated Legendre equation, where the solutions are given by the associated Legendre functions $P^\mu_\lambda(x)$ , $Q^\mu_\lambda(x)$ . In this case are the boundary points still limit circle? To show it I think there might be a different weight function $w(x)$ but I think the procedure should be more or less then same. Would the spectrum still be discrete for the non-integer associated Legendre equation? (for example when the boundary conditions do not require bounded solution at the end points but only $L^2$ ) Thank-you for any insight or help you are able to provide!","I am following this paper by Krall and Zettl . I am trying to use the results of Sturm-Louiville (SL) theory to study eigen functions of the classical Legendre equation: This SL problem is singular at the points as around these points is not locally integrable. I am trying to show that the end points are limit-circle. i.e. that for all solutions , of that I know the solutions of are given by the Legendre functions and so I think showing and is equivalent to showing that the norms for both (??) and are finite, i.e. showing are finite. Here is where I get confused, it is stated in the above linked paper that the SL problem is indeed limit circle at both endpoints. But, If is taken to be an integer then is a finite degree polynomial and is obvious. However can still be singular in this case...  Is it the case that diverges slow enough as to have a finite norm? In the case is not an integer (this case I am most interested in), the behaviour of and only get more singular around . In the linked article by Krall et.al. it is mentioned that the classification of boundary points into limit circle / limit point is independent of so I am assuming there must be some argument in the non-integer case which shows the poles of and diverge sufficiently slowly to be in . My first question is how can I show this? I suspect it might be possible by considering the representations of and in terms of the hypergeometric series, however I have little experience with this and am worried about diverging when since I think this is outside of 's radius of convergence... Maybe there is an asymptotic argument? My second question concerns the study of the associated Legendre equation, where the solutions are given by the associated Legendre functions , . In this case are the boundary points still limit circle? To show it I think there might be a different weight function but I think the procedure should be more or less then same. Would the spectrum still be discrete for the non-integer associated Legendre equation? (for example when the boundary conditions do not require bounded solution at the end points but only ) Thank-you for any insight or help you are able to provide!","
\tag1 \frac{d}{dx}\left((1-x^2)\frac{du}{dx}\right)=\lambda u
 x=\pm1 (1-x^2)^{-1} u (1) \tag 2u\in L^2(-1,\beta) \quad \forall \beta \in (-1,1) \quad\text{ ( so x=-1 is limit circle)}  \tag 3u\in L^2(\alpha,1) \quad \forall \alpha \in (-1,1) \quad\text{ ( so x=1 is limit circle)} (1) P_\lambda(x),Q_\lambda(x) (2) (3) L^2 P_\lambda(x) Q_\lambda(x) \int^\beta_{-1} P_\lambda(x)^2 dx, \int^\beta_{-1} Q_\lambda(x)^2 dx \quad\text{and}\quad\int^1_{\alpha} P_\lambda(x)^2 dx,\int^1_{\alpha} Q_\lambda(x)^2 dx \lambda P_\lambda(x) ||P_\lambda||<\infty Q_\lambda(x) Q_\lambda(x) x \to \pm1 L^2 \lambda P_\lambda Q_\lambda x=\pm1 \lambda \lambda P_\lambda Q_\lambda L^2 P_\lambda Q_\lambda _2F_1 x=-1 _2F_1 P^\mu_\lambda(x) Q^\mu_\lambda(x) w(x) L^2","['ordinary-differential-equations', 'operator-theory', 'special-functions', 'sturm-liouville', 'legendre-functions']"
73,resolve differential equation without using hypergeometric function?,resolve differential equation without using hypergeometric function?,,"I can't resolve this differential equation $$dx=\left(\frac{a +be^{\frac{y}{c}}}{a+b}\right)^{cd} dy,$$ where $a, b, c, d \in \mathbb{R_{\geq 0}}.$ I tried with separable variables, i.e $$x=\int_{0}^{y}\left(\frac{a +be^{\frac{y}{c}}}{a+b}\right)^{cd} dy \\= \frac{1}{(a+b)^{cd}}\int_{0}^{y}\left(a +be^{\frac{y}{c}}\right)^{cd} dy,$$ but i can't find a solution without calling on Hypergeometric function. Appreciate your help!!!","I can't resolve this differential equation where I tried with separable variables, i.e but i can't find a solution without calling on Hypergeometric function. Appreciate your help!!!","dx=\left(\frac{a +be^{\frac{y}{c}}}{a+b}\right)^{cd} dy, a, b, c, d \in \mathbb{R_{\geq 0}}. x=\int_{0}^{y}\left(\frac{a +be^{\frac{y}{c}}}{a+b}\right)^{cd} dy \\= \frac{1}{(a+b)^{cd}}\int_{0}^{y}\left(a +be^{\frac{y}{c}}\right)^{cd} dy,","['integration', 'ordinary-differential-equations']"
74,Flow of an ODE and continuity with respect to initial conditions and parameter - what is the relation between them?,Flow of an ODE and continuity with respect to initial conditions and parameter - what is the relation between them?,,"Let $$\frac{dx}{dt}=f(t, x, \lambda)$$ be a parametric differential equation, where $f:D\subset \mathbb{R}\times \mathbb{R}^n\times \mathbb{R}^k \to \mathbb{R}^n$ ( $\lambda$ is the parameter). Suppose that for every $\lambda$ the ODE admits uniqueness of solutions. My lecturer defined the parameterized flow of this equation as follows ( $(t_0, x_0, \lambda_0)\in D$ is fixed) : it is the function $\alpha:I_1 \times I_0 \times G_0 \times \Lambda_0\in \mathcal{N}(t_0, t_0, x_0, \lambda_0)\to \mathbb{R}^n$ defined as follows: for every $(\tau, \xi, \lambda)\in I_0\times G_0\times \Lambda_0$ the function $\alpha(., \tau, \xi, \lambda):I_1\to \mathbb{R}^n$ is the unique solution of the Cauchy problem with the initial condition $f(\tau)=\xi$ . He then went on to show that if $f$ is continuous and Lipschitz in the second variable then this $\alpha$ is a continuous function and he told us that this shows the continuous dependence of the ODE with respect to the initial conditions and the parameter. But here I am a bit puzzled. I knew that if there is no parameter then the continuous dependence with respect to the initial conditions for an equation of the form $\frac{dx}{dt}=f(t, x)$ is something like this: for every $\epsilon>0$ , there is some $\delta>0$ (which depends on $\epsilon$ and the point where I am given the initial condition) such that if $\phi$ and $\psi$ are two solutions of the ODE and $||\phi(t_0)-\psi(t_0)||\le \delta$ , then $||\phi(t)-\psi(t)||\le \epsilon$ for every $t$ for which these solutions are defined $(*)$ . Basically, if the initial values are sufficiently close, then the solutions will be arbitrarily close. However, even though intuitively I can see that these notions are the same, I can't see exactly how the fact that my flow $\alpha$ is continuous implies something like $(*)$ . I think that I somethow need to write the definition of continuity, but I get confused with all those arguments of $\alpha$ . Could you please explain this to me?","Let be a parametric differential equation, where ( is the parameter). Suppose that for every the ODE admits uniqueness of solutions. My lecturer defined the parameterized flow of this equation as follows ( is fixed) : it is the function defined as follows: for every the function is the unique solution of the Cauchy problem with the initial condition . He then went on to show that if is continuous and Lipschitz in the second variable then this is a continuous function and he told us that this shows the continuous dependence of the ODE with respect to the initial conditions and the parameter. But here I am a bit puzzled. I knew that if there is no parameter then the continuous dependence with respect to the initial conditions for an equation of the form is something like this: for every , there is some (which depends on and the point where I am given the initial condition) such that if and are two solutions of the ODE and , then for every for which these solutions are defined . Basically, if the initial values are sufficiently close, then the solutions will be arbitrarily close. However, even though intuitively I can see that these notions are the same, I can't see exactly how the fact that my flow is continuous implies something like . I think that I somethow need to write the definition of continuity, but I get confused with all those arguments of . Could you please explain this to me?","\frac{dx}{dt}=f(t, x, \lambda) f:D\subset \mathbb{R}\times \mathbb{R}^n\times \mathbb{R}^k \to \mathbb{R}^n \lambda \lambda (t_0, x_0, \lambda_0)\in D \alpha:I_1 \times I_0 \times G_0 \times \Lambda_0\in \mathcal{N}(t_0, t_0, x_0, \lambda_0)\to \mathbb{R}^n (\tau, \xi, \lambda)\in I_0\times G_0\times \Lambda_0 \alpha(., \tau, \xi, \lambda):I_1\to \mathbb{R}^n f(\tau)=\xi f \alpha \frac{dx}{dt}=f(t, x) \epsilon>0 \delta>0 \epsilon \phi \psi ||\phi(t_0)-\psi(t_0)||\le \delta ||\phi(t)-\psi(t)||\le \epsilon t (*) \alpha (*) \alpha","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
75,The ODE modeling of gradient descent with diminishing stepsize,The ODE modeling of gradient descent with diminishing stepsize,,"The gradient descent (GD) with constant stepsize $\alpha^{k}=\alpha$ takes the form $$x^{k+1} = x^{k} -\alpha\nabla f(x^{k}).$$ Then, by constructing a continuous-time version of GD iterates satisfying $X(k\alpha)=x^{k}$ and taking $\alpha\to 0$ , we could obtain a limiting ode for constant-stepsize GD of the form $$\lim_{\alpha\to 0}\frac{X(t+\alpha)-X(t)}{\alpha} = \nabla f(X(t))\Rightarrow\frac{dX(t)}{dt} = -\nabla f(X(t)).$$ My question is that if we use the diminishing stepsize with the form $\alpha^k = \alpha/(k+1)$ , could we derive the corresponding ODE as $\alpha\to 0$ . I guess, the limiting ode for diminishing-stepsize GD might take the form of $$\frac{dX(t)}{dt} = -\frac{1}{t+1}\nabla f(X(t)).$$ Thanks!","The gradient descent (GD) with constant stepsize takes the form Then, by constructing a continuous-time version of GD iterates satisfying and taking , we could obtain a limiting ode for constant-stepsize GD of the form My question is that if we use the diminishing stepsize with the form , could we derive the corresponding ODE as . I guess, the limiting ode for diminishing-stepsize GD might take the form of Thanks!",\alpha^{k}=\alpha x^{k+1} = x^{k} -\alpha\nabla f(x^{k}). X(k\alpha)=x^{k} \alpha\to 0 \lim_{\alpha\to 0}\frac{X(t+\alpha)-X(t)}{\alpha} = \nabla f(X(t))\Rightarrow\frac{dX(t)}{dt} = -\nabla f(X(t)). \alpha^k = \alpha/(k+1) \alpha\to 0 \frac{dX(t)}{dt} = -\frac{1}{t+1}\nabla f(X(t)).,"['ordinary-differential-equations', 'optimization', 'discrete-optimization', 'gradient-descent', 'gradient-flows']"
76,"Proving that $y''(x)=\cos(x)+y^3(x)+3x\,y^2(x)\,y'(x)>0.$",Proving that,"y''(x)=\cos(x)+y^3(x)+3x\,y^2(x)\,y'(x)>0.","Let $y'(x)=\sin(x)+x\,y^3(x)$ and $y(0)=0$ . How can I prove that $y''(x)>0$ for small $x$ ? Got the information $v'=\sin(x), v(0)=0$ and $w'=x\,y^3(x), w\left(\dfrac{\pi}{2}\right)=\dfrac{\pi}{2}.$ My way was: $$y''(x)=\cos(x)+y^3(x)+3x\,y^2(x)\,y'(x).$$ It's $\mathrm{cos}(x)>0$ for $x \in \left[0,\dfrac{\pi}{2}\right]$ . Now I don't know how to show that it's still positive for $y''(x)$ since I don't know what $y(x)$ is. How can it be shown?",Let and . How can I prove that for small ? Got the information and My way was: It's for . Now I don't know how to show that it's still positive for since I don't know what is. How can it be shown?,"y'(x)=\sin(x)+x\,y^3(x) y(0)=0 y''(x)>0 x v'=\sin(x), v(0)=0 w'=x\,y^3(x), w\left(\dfrac{\pi}{2}\right)=\dfrac{\pi}{2}. y''(x)=\cos(x)+y^3(x)+3x\,y^2(x)\,y'(x). \mathrm{cos}(x)>0 x \in \left[0,\dfrac{\pi}{2}\right] y''(x) y(x)",['ordinary-differential-equations']
77,Solution of the ordinary differential equation $y''+\sin(x)y'+2y=1$,Solution of the ordinary differential equation,y''+\sin(x)y'+2y=1,I have the following ordinary differential equation before me: $y''+\sin(x)y'+2y=1$ . I have to find its PI(particular integral).My strategy is to come up with the solution of corresponding homogeneous equation i.e. $y''+\sin(x)y'+2y=0$ and then use the method of variation of parameters to determine PI. But I am stuck at finding the solution of homogeneous equation. Could you please suggest a way out here?,I have the following ordinary differential equation before me: . I have to find its PI(particular integral).My strategy is to come up with the solution of corresponding homogeneous equation i.e. and then use the method of variation of parameters to determine PI. But I am stuck at finding the solution of homogeneous equation. Could you please suggest a way out here?,y''+\sin(x)y'+2y=1 y''+\sin(x)y'+2y=0,"['ordinary-differential-equations', 'homogeneous-equation']"
78,Power Series Solution of Non-homogeneous Differential Equation: $(1-x^2)y'' + y' + y = xe^x$,Power Series Solution of Non-homogeneous Differential Equation:,(1-x^2)y'' + y' + y = xe^x,"I am attempting to solve the non-homogeneous differential equation $(1-x^2)y'' + y' + y = xe^x$ via power series solution, but am running into an issue once I have expanded all the necessary terms and combined all series with like powers of $x$ into a common series. Please allow me to elaborate further. The differential equation $(1-x^2)y'' + y' + y = xe^x$ can be represented in the following power series form: $$\sum_{n=2}^{\infty}c_nn(n-1)x^{n-2} - \sum_{n=2}^{\infty}c_nn(n-1)x^{n}+ \sum_{n=1}^{\infty}c_nnx^{n-1}+\sum_{n=0}^{\infty}c_nx^{n}  = x\sum_{n=0}^{\infty}\frac{x^n}{n!}$$ These terms will hereafter be referred to as terms 1, 2, 3, 4, and sum, respectively. My aim was to convert all powers of $x$ to the like power of $x^n$ , however I feel this may be the decision that resulted in issues further down the road. Note that the highest index of a $x^n$ term is found in term 2, therefore all other terms will be expanded to put their summations in a form with an index of 2. To convert term 1 to a like power of $x$ , the substitution $n = n+2$ was made. The series was then expanded to an index of 2.  The resulting series is as follows: $$\sum_{n=2}^{\infty}c_nn(n-1)x^{n-2} = \sum_{n=0}^{\infty}c_{n+2}(n+2)(n+1)x^{n} = 2c_2 + 6c_3x + \sum_{n=2}^{\infty}c_{n+2}(n+2)(n+1)x^{n}$$ Term 2 is already in the form of a like power of $x$ with a common index of 2. To convert term 3 to a like power of $x$ , the substitution $n = n+1$ was made. The series was then expanded to an index of 2. The resulting series is as follows: $$\sum_{n=1}^{\infty}c_nnx^{n-1} = \sum_{n=0}^{\infty}c_{n+1}(n+1)x^{n} =  c_1 + 2c_2x + \sum_{n=2}^{\infty}c_{n+1}(n+1)x^{n}$$ Term 4 is already in the form of a like power of $x$ , however does not share a common index. The series was expanded to an index of 2. The resulting series is as follows: $$\sum_{n=0}^{\infty}c_nx^{n} = c_0 + c_1x + \sum_{n=2}^{\infty}c_nx^{n}$$ The sum term is already in the form of a like power of $x$ , however it does not share the common index of 2. The series was expanded to an index of 2. The resulting series is as follows: $$x\sum_{n=0}^{\infty}\frac{x^n}{n!} = x + x^2 + x\sum_{n=2}^{\infty}\frac{x^n}{n!}$$ Combining all of these terms and like power series, the result is as follows: $$c_0 + c_1 + 2c_2 + x(c_1 + 2c_2 + 6c_3) + \sum_{n=2}^{\infty}(c_{n+2}(n+2)(n+1) + c_{n+1}(n+1) + c_n(2-n))x^{n} = x + x^2 + x\sum_{n=2}^{\infty}\frac{x^n}{n!}$$ This is where the issue is found. The $0$ th order constant terms can be equated to $0$ , and the $1$ st order constant terms can be equated to $1$ . However, there are no terms on the left hand side of the equation that will equal the $x^2$ term on the right hand side, unless the series is expanded, resulting in a loss of common indices between the two series. The series is also problematic, as the internal constant expression on the left hand side can be evaluated to $\frac{1}{n!}$ , but the series will never be equal due to the $x$ multiplier on the right hand side. In summary, I appear to be missing a power of $x$ on the left hand side of my equation. As stated earlier, I have an inclination that my choice to set the common power of $x$ to $x^n$ may have been a mistake. If the $x$ is factored into the series expression for $e^x$ , the power of $x$ would be $x^{n+1}$ , which could potentially resolve my issue if I restate my terms in the form of that power. However, I figured I would pass it by Stack Exchange and hopefully gain some insights as to what I am doing wrong here. Thank you for your time! It is much appreciated, many thanks!","I am attempting to solve the non-homogeneous differential equation via power series solution, but am running into an issue once I have expanded all the necessary terms and combined all series with like powers of into a common series. Please allow me to elaborate further. The differential equation can be represented in the following power series form: These terms will hereafter be referred to as terms 1, 2, 3, 4, and sum, respectively. My aim was to convert all powers of to the like power of , however I feel this may be the decision that resulted in issues further down the road. Note that the highest index of a term is found in term 2, therefore all other terms will be expanded to put their summations in a form with an index of 2. To convert term 1 to a like power of , the substitution was made. The series was then expanded to an index of 2.  The resulting series is as follows: Term 2 is already in the form of a like power of with a common index of 2. To convert term 3 to a like power of , the substitution was made. The series was then expanded to an index of 2. The resulting series is as follows: Term 4 is already in the form of a like power of , however does not share a common index. The series was expanded to an index of 2. The resulting series is as follows: The sum term is already in the form of a like power of , however it does not share the common index of 2. The series was expanded to an index of 2. The resulting series is as follows: Combining all of these terms and like power series, the result is as follows: This is where the issue is found. The th order constant terms can be equated to , and the st order constant terms can be equated to . However, there are no terms on the left hand side of the equation that will equal the term on the right hand side, unless the series is expanded, resulting in a loss of common indices between the two series. The series is also problematic, as the internal constant expression on the left hand side can be evaluated to , but the series will never be equal due to the multiplier on the right hand side. In summary, I appear to be missing a power of on the left hand side of my equation. As stated earlier, I have an inclination that my choice to set the common power of to may have been a mistake. If the is factored into the series expression for , the power of would be , which could potentially resolve my issue if I restate my terms in the form of that power. However, I figured I would pass it by Stack Exchange and hopefully gain some insights as to what I am doing wrong here. Thank you for your time! It is much appreciated, many thanks!",(1-x^2)y'' + y' + y = xe^x x (1-x^2)y'' + y' + y = xe^x \sum_{n=2}^{\infty}c_nn(n-1)x^{n-2} - \sum_{n=2}^{\infty}c_nn(n-1)x^{n}+ \sum_{n=1}^{\infty}c_nnx^{n-1}+\sum_{n=0}^{\infty}c_nx^{n}  = x\sum_{n=0}^{\infty}\frac{x^n}{n!} x x^n x^n x n = n+2 \sum_{n=2}^{\infty}c_nn(n-1)x^{n-2} = \sum_{n=0}^{\infty}c_{n+2}(n+2)(n+1)x^{n} = 2c_2 + 6c_3x + \sum_{n=2}^{\infty}c_{n+2}(n+2)(n+1)x^{n} x x n = n+1 \sum_{n=1}^{\infty}c_nnx^{n-1} = \sum_{n=0}^{\infty}c_{n+1}(n+1)x^{n} =  c_1 + 2c_2x + \sum_{n=2}^{\infty}c_{n+1}(n+1)x^{n} x \sum_{n=0}^{\infty}c_nx^{n} = c_0 + c_1x + \sum_{n=2}^{\infty}c_nx^{n} x x\sum_{n=0}^{\infty}\frac{x^n}{n!} = x + x^2 + x\sum_{n=2}^{\infty}\frac{x^n}{n!} c_0 + c_1 + 2c_2 + x(c_1 + 2c_2 + 6c_3) + \sum_{n=2}^{\infty}(c_{n+2}(n+2)(n+1) + c_{n+1}(n+1) + c_n(2-n))x^{n} = x + x^2 + x\sum_{n=2}^{\infty}\frac{x^n}{n!} 0 0 1 1 x^2 \frac{1}{n!} x x x x^n x e^x x x^{n+1},"['sequences-and-series', 'ordinary-differential-equations', 'power-series', 'homogeneous-equation', 'eulers-number-e']"
79,Interchanging the variable while integrating - Allowed?,Interchanging the variable while integrating - Allowed?,,"Suppose we have this equation: $$\frac{dy}{dx} = \frac{y}{2x}$$ The next step usually is: $$\frac{dy}{y} = \frac{dx}{2x}$$ And then you integrate : $$\int\frac{dy}{y} = \int\frac{dx}{2x}$$ $$\ln(y) = \ln(\sqrt{x}) + c$$ But can we integrate like this? (i feel you can't, but can't find the reasoning): $$\int2x \ dy = \int y \ dx$$ $$2xy = xy + c$$ Can you tell if this is also plausible? If so, why? and If not, why not? Thanks!","Suppose we have this equation: The next step usually is: And then you integrate : But can we integrate like this? (i feel you can't, but can't find the reasoning): Can you tell if this is also plausible? If so, why? and If not, why not? Thanks!",\frac{dy}{dx} = \frac{y}{2x} \frac{dy}{y} = \frac{dx}{2x} \int\frac{dy}{y} = \int\frac{dx}{2x} \ln(y) = \ln(\sqrt{x}) + c \int2x \ dy = \int y \ dx 2xy = xy + c,"['calculus', 'algebra-precalculus', 'ordinary-differential-equations']"
80,Each solution of ODE approaches constant given inequality,Each solution of ODE approaches constant given inequality,,"given $x'(t)=f(x,t)$ , if $|f(x,t)|\leq A(t)|x|$ and $\int_a^\infty A(s)ds=C\neq\infty$ each solution of IC approaches a constant value. Approach: Let $x(T)=x_0$ integrating $x'(t)=f(x,t)$ over $t$ in [T; $\infty)\implies \lim\limits_{t\to\infty}x(t)-x_0=\int_{T}^{\infty}f(x,s)ds$ $\lim\limits_{t\to\infty}x(t)=x_0+\int_{T}^{\infty}f(x,s)ds$ $\lim\limits_{t\to\infty}x(t)\leq |x_0|+|\int_{T}^{\infty}f(x,s)ds|\\ \leq |x_0|+\int_{T}^{\infty}|f(x,s)|ds\\ \leq|x_0|+\int_{T}^{\infty}A(s)|x|ds$ from condition on $A(s)$ can conclude that $\lim\limits_{t\to\infty}x(t)<\infty$ but does it exist? function x may have no limit (like $\sin x$ ) Correct me if im wrong P.S. was also thinking about more general condition what will happen if $|f(x,t)-f(y,t)|\leq A(t)|x-y|$ if, suppose two different x,y solutions of ODE with different IC's, must these solutions converge to different values? Using the same approach led to: $x-y\leq|x-y|\leq|x_0-y_0|+\int_{T}^\infty A(s)|x-y|ds<\infty$ but again If I suppose the contrary that two different solutions converge to the same constant their difference at infinity must be $=0$ but it I'm stuck with applying limit to make use of condition on $A(t)$ to inequality above (just as in the special case checking one solution converging to constant). Proof after hint of Lutz Lehmann $x'(t)=f(x,t)\leq |f(x,t)|\leq A(t)|x|\\ |x'(t)|\leq A(t)|x|\\$ Applying Gronwalls inequality $x'\leq f(x,t)x\implies x\leq x_0 e^{\int_\tau^{\infty}f(s)ds}$ $$|x(t)| \leq |x_0| e^{\int_{\tau}^{\infty} A(s)ds}$$ Due to $\int_{\tau}^{\infty} A(s)ds=C<\infty$ , means function $x(t)$ is bounded from both sides by the constant factor $e^{\int_{\tau}^{\infty} A(s)ds}$ mentioned above. However question still remains: what happens when $t\to \infty$ with $x(t)$ as it is bounded from both sides due to derivation, but may oscillate at infinity.","given , if and each solution of IC approaches a constant value. Approach: Let integrating over in [T; from condition on can conclude that but does it exist? function x may have no limit (like ) Correct me if im wrong P.S. was also thinking about more general condition what will happen if if, suppose two different x,y solutions of ODE with different IC's, must these solutions converge to different values? Using the same approach led to: but again If I suppose the contrary that two different solutions converge to the same constant their difference at infinity must be but it I'm stuck with applying limit to make use of condition on to inequality above (just as in the special case checking one solution converging to constant). Proof after hint of Lutz Lehmann Applying Gronwalls inequality Due to , means function is bounded from both sides by the constant factor mentioned above. However question still remains: what happens when with as it is bounded from both sides due to derivation, but may oscillate at infinity.","x'(t)=f(x,t) |f(x,t)|\leq A(t)|x| \int_a^\infty A(s)ds=C\neq\infty x(T)=x_0 x'(t)=f(x,t) t \infty)\implies \lim\limits_{t\to\infty}x(t)-x_0=\int_{T}^{\infty}f(x,s)ds \lim\limits_{t\to\infty}x(t)=x_0+\int_{T}^{\infty}f(x,s)ds \lim\limits_{t\to\infty}x(t)\leq |x_0|+|\int_{T}^{\infty}f(x,s)ds|\\ \leq |x_0|+\int_{T}^{\infty}|f(x,s)|ds\\ \leq|x_0|+\int_{T}^{\infty}A(s)|x|ds A(s) \lim\limits_{t\to\infty}x(t)<\infty \sin x |f(x,t)-f(y,t)|\leq A(t)|x-y| x-y\leq|x-y|\leq|x_0-y_0|+\int_{T}^\infty A(s)|x-y|ds<\infty =0 A(t) x'(t)=f(x,t)\leq |f(x,t)|\leq A(t)|x|\\ |x'(t)|\leq A(t)|x|\\ x'\leq f(x,t)x\implies x\leq x_0 e^{\int_\tau^{\infty}f(s)ds} |x(t)| \leq |x_0| e^{\int_{\tau}^{\infty} A(s)ds} \int_{\tau}^{\infty} A(s)ds=C<\infty x(t) e^{\int_{\tau}^{\infty} A(s)ds} t\to \infty x(t)","['ordinary-differential-equations', 'solution-verification']"
81,Natural-Forced and Transient-SteadyState pairs of solutions,Natural-Forced and Transient-SteadyState pairs of solutions,,"We have the following circuit, where, $u(0)=V_{0}$ . The ode that describes this circuit that has $V_{s}$ as input and the voltage $u(t)$ of the capacitor as output is the following: $\dot{u} + \tau u = \tau V_{s}$ , where $\tau=\frac{1}{RC}$ . If I solve the ode, this way : for the homogeneous part, we have $u_{h}(t)=ce^{\lambda t}$ . After replacing it to the initial equation, we find that $\lambda=-\tau$ , and we get $u_{h}(t)=ce^{-\tau t}.$ To find the particular solution, we consider $u_{p}(t)=AV_{s}$ and after the replacement we get $A=1$ , which makes $u_{p}(t)=V_{s}$ Their sum is: $u(t)=ce^{-\tau t}+V_{s}$ , and by applying the initial condition, we get that $c=V_{0}-V_{s}$ Therefore, the total solution is: $u(t)=(V_{0}-V_{s})e^{-\tau t}+V_{s}$ the homogeneous solution coincides with the transient solution, while the particular one coincides with the steady-state solution If I solve it, that way : The homogeneous part it's the same as before, so: $u_{h}(t)=ce^{-\tau t}$ . We find the particular solution by using directly the following equation: $u_{p}(t)=e^{-\int_{0}^{t}\tau  dt }\int_{0}^{t} (\tau V_{s} e^{\int_{0}^{t}\tau  dt })dt=e^{-\tau t}V_{s}(1-e^{\tau t})=V_{s}(1-e^{\tau t})$ Their sum is: $u(t)=ce^{-\tau t}+V_{s}(1-e^{\tau t})$ , and by applying the initial condition, we get that $c=V_{0}$ Therefore, the total solution is: $u(t)=V_{0}e^{-\tau t}+V_{s}(1-e^{\tau t})$ the homogeneous solution coincides with the natural solution, while the particular one coincides with the forced solution Of course the total solutions are identical but I would be interested in knowing why the one way yields directly the $u_{natural}$ - $u_{forced}$ pair, while the other way yields directly the $u_{transient}$ - $u_{steady-state}$ pair.","We have the following circuit, where, . The ode that describes this circuit that has as input and the voltage of the capacitor as output is the following: , where . If I solve the ode, this way : for the homogeneous part, we have . After replacing it to the initial equation, we find that , and we get To find the particular solution, we consider and after the replacement we get , which makes Their sum is: , and by applying the initial condition, we get that Therefore, the total solution is: the homogeneous solution coincides with the transient solution, while the particular one coincides with the steady-state solution If I solve it, that way : The homogeneous part it's the same as before, so: . We find the particular solution by using directly the following equation: Their sum is: , and by applying the initial condition, we get that Therefore, the total solution is: the homogeneous solution coincides with the natural solution, while the particular one coincides with the forced solution Of course the total solutions are identical but I would be interested in knowing why the one way yields directly the - pair, while the other way yields directly the - pair.",u(0)=V_{0} V_{s} u(t) \dot{u} + \tau u = \tau V_{s} \tau=\frac{1}{RC} u_{h}(t)=ce^{\lambda t} \lambda=-\tau u_{h}(t)=ce^{-\tau t}. u_{p}(t)=AV_{s} A=1 u_{p}(t)=V_{s} u(t)=ce^{-\tau t}+V_{s} c=V_{0}-V_{s} u(t)=(V_{0}-V_{s})e^{-\tau t}+V_{s} u_{h}(t)=ce^{-\tau t} u_{p}(t)=e^{-\int_{0}^{t}\tau  dt }\int_{0}^{t} (\tau V_{s} e^{\int_{0}^{t}\tau  dt })dt=e^{-\tau t}V_{s}(1-e^{\tau t})=V_{s}(1-e^{\tau t}) u(t)=ce^{-\tau t}+V_{s}(1-e^{\tau t}) c=V_{0} u(t)=V_{0}e^{-\tau t}+V_{s}(1-e^{\tau t}) u_{natural} u_{forced} u_{transient} u_{steady-state},"['ordinary-differential-equations', 'steady-state']"
82,Find a IVP associated with $y' = y - y^{2}$,Find a IVP associated with,y' = y - y^{2},"The function $g(x, c) = \frac{1}{1-c e^{-x}} $ is a family of solutions (of one parameter) of the first order DE $y'(x) = y(x) - y^{2}(x)$ . Find a IVP associated with this differential equation and find the solution corresponding to the initial condition $y(0) = \frac{-1}{3}$ I did the next: The IVP associated with $y' = y- y^{2}$ is: $y' - y + y^{2} = 0$ and $y(0) = \frac{-1}{3}$ this is defined in $(- \infty,1) $ I'm not sure if that's right. It's seems incomplete to me.",The function is a family of solutions (of one parameter) of the first order DE . Find a IVP associated with this differential equation and find the solution corresponding to the initial condition I did the next: The IVP associated with is: and this is defined in I'm not sure if that's right. It's seems incomplete to me.,"g(x, c) = \frac{1}{1-c e^{-x}}  y'(x) = y(x) - y^{2}(x) y(0) = \frac{-1}{3} y' = y- y^{2} y' - y + y^{2} = 0 y(0) = \frac{-1}{3} (- \infty,1) ","['ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'initial-value-problems']"
83,Is it necessary to consider absolute values when solving the differential equation $\frac{dy}{dx}-\frac{1}{x}y=1$?,Is it necessary to consider absolute values when solving the differential equation ?,\frac{dy}{dx}-\frac{1}{x}y=1,"Given the differential equation $$\frac{dy}{dx}-\frac{1}{x}y=1$$ I'd like to solve it and understand where I should use absolute value functions and why or why not. This is a problem from MIT OCW's 18.03SC Differential Equations, and in their solutions they don't make any considerations for absolute values when integrating $x^{-1}$ in solving this differential equation. Edit: I failed to specify that the MIT OCW problem also specifies an initial value y(1)=7. I now think that perhaps they disregard negative x because the solution with this initial value will be the one with non-negative x, ie the solution to the right of the singularity at x=0. First, I find an integrating factor $$u(x)=e^{-\int x^{-1} dx}=e^{-\ln |x|}=|x|^{-1}$$ Then I multiply the differential equation by the integrating factor $$|x|^{-1}\frac{dy}{dx}-|x|^{-1}x^{-1}y=|x|^{-1}$$ This is where the absolute value starts to become important. $$\frac{d}{dx}(|x|^{-1}y)=|x|^{-1}$$ We can take the latter step because if we have $f(x)=|x|^{-1}$ then $$f'(x)=\begin{cases} -x^{-2} \ \ x\geq 0\\ x^{-2}\ \ x\leq 0\\ \end{cases}=-\frac{1}{x|x|}$$ $$|x|^{-1}y=\int |x|^{-1}dx+C$$ For the next step I will use that $$\int |x|^{-1}dx=\begin{cases} \int x^{-1}dx = \ln(x)\ \ x \geq 0 \\ - \int x^{-1} = -\ln(|x|)=-\ln(-x)\ \ x \leq 0 \\ \end{cases}$$ So $$y(x)=\begin{cases} x \ln(x) +cx\ \ x \geq 0 \\ x \ln(-x)-cx\ \ x \leq 0 \\ \end{cases}$$ In the case of the official solution to this problem the integrating factor is simply $x^{-1}$ and so the solution is just $y(x)=x \ln(x) +cx$ , the same solution I have but in my case that solution is only valid for non-negative x. Which answer is the inaccurate one?","Given the differential equation I'd like to solve it and understand where I should use absolute value functions and why or why not. This is a problem from MIT OCW's 18.03SC Differential Equations, and in their solutions they don't make any considerations for absolute values when integrating in solving this differential equation. Edit: I failed to specify that the MIT OCW problem also specifies an initial value y(1)=7. I now think that perhaps they disregard negative x because the solution with this initial value will be the one with non-negative x, ie the solution to the right of the singularity at x=0. First, I find an integrating factor Then I multiply the differential equation by the integrating factor This is where the absolute value starts to become important. We can take the latter step because if we have then For the next step I will use that So In the case of the official solution to this problem the integrating factor is simply and so the solution is just , the same solution I have but in my case that solution is only valid for non-negative x. Which answer is the inaccurate one?","\frac{dy}{dx}-\frac{1}{x}y=1 x^{-1} u(x)=e^{-\int x^{-1} dx}=e^{-\ln |x|}=|x|^{-1} |x|^{-1}\frac{dy}{dx}-|x|^{-1}x^{-1}y=|x|^{-1} \frac{d}{dx}(|x|^{-1}y)=|x|^{-1} f(x)=|x|^{-1} f'(x)=\begin{cases}
-x^{-2} \ \ x\geq 0\\
x^{-2}\ \ x\leq 0\\
\end{cases}=-\frac{1}{x|x|} |x|^{-1}y=\int |x|^{-1}dx+C \int |x|^{-1}dx=\begin{cases}
\int x^{-1}dx = \ln(x)\ \ x \geq 0 \\
- \int x^{-1} = -\ln(|x|)=-\ln(-x)\ \ x \leq 0 \\
\end{cases} y(x)=\begin{cases}
x \ln(x) +cx\ \ x \geq 0 \\
x \ln(-x)-cx\ \ x \leq 0 \\
\end{cases} x^{-1} y(x)=x \ln(x) +cx",['ordinary-differential-equations']
84,Tranformations of Curves,Tranformations of Curves,,"A while ago I asked a question similar to this, but looking back, I think I would have to further clarify. Please excuse how I ask this question, as I am very new to the Math Stack Exchange. Suppose I have a curve, with one endpoint on the origin of the x-y axis, and the other at some other point on the x-axis. In other words, the two endpoints of the curve are on (0,0) and some (x,0) If I were to change the position of the latter endpoint (x,0), how do other points on curve f(x) change with respect to the change of (x,0)? Intuitively, if I have a string, and I stretch an endpoint to some other position, how does the original string change? Moreover, if I were to move that endpoint in a specific path (represented as a function), what would the paths of the other points on the curve be? Looking at the problem, I thought about representing the function as a series of points, to which I would draw respective lines through each point, and would then calculate the motion of the other points with respect to the change of the latter endpoint. I did this for a 'one-point system', which is merely a point connected with 2 lines, with these lines connecting towards the two endpoints listed above (I would draw a diagram but I do not know how to on this software). Doing this, I found an equation, although it was huge. I then did this for two-point systems, and I soon realized that the solutions would be much bigger than I imagined. I am sure that if I continued to work on this problem, I would arrive at somewhat of a verdict. However, the people on this Stack Exchange are much more well-trained in mathematics than I am. And so my question is this: Does a solution currently exist to this problem? If so, what is it? If not, how would I further proceed with the method I used above? How would you all solve the problem? Please note that the new, transformed curve is subject to the previous curve's arc length (the original and new curve have the same length). The question I seek to find is how the original curve changes into its new form. Again, please excuse any difficulties with how I have written this post. Hopefully, you all do not mind.","A while ago I asked a question similar to this, but looking back, I think I would have to further clarify. Please excuse how I ask this question, as I am very new to the Math Stack Exchange. Suppose I have a curve, with one endpoint on the origin of the x-y axis, and the other at some other point on the x-axis. In other words, the two endpoints of the curve are on (0,0) and some (x,0) If I were to change the position of the latter endpoint (x,0), how do other points on curve f(x) change with respect to the change of (x,0)? Intuitively, if I have a string, and I stretch an endpoint to some other position, how does the original string change? Moreover, if I were to move that endpoint in a specific path (represented as a function), what would the paths of the other points on the curve be? Looking at the problem, I thought about representing the function as a series of points, to which I would draw respective lines through each point, and would then calculate the motion of the other points with respect to the change of the latter endpoint. I did this for a 'one-point system', which is merely a point connected with 2 lines, with these lines connecting towards the two endpoints listed above (I would draw a diagram but I do not know how to on this software). Doing this, I found an equation, although it was huge. I then did this for two-point systems, and I soon realized that the solutions would be much bigger than I imagined. I am sure that if I continued to work on this problem, I would arrive at somewhat of a verdict. However, the people on this Stack Exchange are much more well-trained in mathematics than I am. And so my question is this: Does a solution currently exist to this problem? If so, what is it? If not, how would I further proceed with the method I used above? How would you all solve the problem? Please note that the new, transformed curve is subject to the previous curve's arc length (the original and new curve have the same length). The question I seek to find is how the original curve changes into its new form. Again, please excuse any difficulties with how I have written this post. Hopefully, you all do not mind.",,['geometry']
85,Solve $y''-2y = 4x^2e^{x^2}$,Solve,y''-2y = 4x^2e^{x^2},"I want to use variation of parameters: $y_h = C_1e^{\sqrt{2}x} + C_2e^{-\sqrt{2}x}$ . The wronskian I get to be $w = -2\sqrt{2}$ , $w_1 = -4x^2e^{x^2}e^{-\sqrt{2}x}$ and $w_2 = 4x^2e^{x^2}e^{\sqrt{2}x}$ . Solving for $$u_1' = \frac{w_1}{w} \rightarrow \frac{\sqrt{2}}{2}x^2e^{x^2-2\sqrt{x}}$$ $$u_1 = \frac{\sqrt{2}}{2}\int x^2e^{x^2-\sqrt{2}x}dx$$ This integral seems like a headache. Is this right approach? Is there a trick to solving this with VOP?","I want to use variation of parameters: . The wronskian I get to be , and . Solving for This integral seems like a headache. Is this right approach? Is there a trick to solving this with VOP?",y_h = C_1e^{\sqrt{2}x} + C_2e^{-\sqrt{2}x} w = -2\sqrt{2} w_1 = -4x^2e^{x^2}e^{-\sqrt{2}x} w_2 = 4x^2e^{x^2}e^{\sqrt{2}x} u_1' = \frac{w_1}{w} \rightarrow \frac{\sqrt{2}}{2}x^2e^{x^2-2\sqrt{x}} u_1 = \frac{\sqrt{2}}{2}\int x^2e^{x^2-\sqrt{2}x}dx,['ordinary-differential-equations']
86,Example of an ODE with an asymptotically stable equilibrium which is unstable in the corresponding linearization,Example of an ODE with an asymptotically stable equilibrium which is unstable in the corresponding linearization,,"I am looking for an example of an ODE $x'=f(x)$ with an asymptotically stable equilibrium (WLOG $x=0$ ) such that $0$ is an unstable equilibrium of the linearized equation $x'=Ax$ where $A=Df(0)$ . The strategy is to choose $f$ so that zero is an eigenvalue of $A$ whose algebraic multiplicity exceeds its geometric multiplicity (leading to linearly growing solutions), but whose nonlinear parts promote stability. The example I came up with is $$f:\mathbb R^2\to\mathbb R^2, \quad f(x_1,x_2)=(x_2-x_1^3,-x_2^3),\quad A=Df(0)=\begin{pmatrix} 0 & 1\\ 0 & 0\end{pmatrix}.$$ In the above example, solutions to the linearized problem $x'=Ax$ have the form $x(t)=(c_2 t+c_1,c_2)$ for constants $c_1$ and $c_2$ , so $0$ is an unstable equilibrium. The nonlinear problem, however, has the following phase diagram: This phase diagram seems to strongly suggest that the origin is asymptotically stable in the nonlinear system, but I cannot prove it with, e.g., a Lyapunov function. How can I show the origin is asymptotically stable in this or a similar example?","I am looking for an example of an ODE with an asymptotically stable equilibrium (WLOG ) such that is an unstable equilibrium of the linearized equation where . The strategy is to choose so that zero is an eigenvalue of whose algebraic multiplicity exceeds its geometric multiplicity (leading to linearly growing solutions), but whose nonlinear parts promote stability. The example I came up with is In the above example, solutions to the linearized problem have the form for constants and , so is an unstable equilibrium. The nonlinear problem, however, has the following phase diagram: This phase diagram seems to strongly suggest that the origin is asymptotically stable in the nonlinear system, but I cannot prove it with, e.g., a Lyapunov function. How can I show the origin is asymptotically stable in this or a similar example?","x'=f(x) x=0 0 x'=Ax A=Df(0) f A f:\mathbb R^2\to\mathbb R^2, \quad f(x_1,x_2)=(x_2-x_1^3,-x_2^3),\quad A=Df(0)=\begin{pmatrix} 0 & 1\\ 0 & 0\end{pmatrix}. x'=Ax x(t)=(c_2 t+c_1,c_2) c_1 c_2 0","['ordinary-differential-equations', 'stability-in-odes']"
87,Cauchy problem for an ordinary equation not in normal form,Cauchy problem for an ordinary equation not in normal form,,"Let's consider a one-dimensional physical system ( $x$ is the position and $t$ is time) described by a first order differential equation. I'm aware of the fact that if the equation can be put in normal form: \begin{equation} \dot{x}=f(x,t) \end{equation} the Cauchy problem, provided some mathematical hypotheses are verified, tells us that the equation admits the existence of an unique solution. Therefore the system is deterministic, i.e., given the initial conditions there is only one admissible motion. I was wondering what happens when the system is described by an equation that cannot be put in normal form: \begin{equation} g(\dot{x},x,t)=0 \end{equation} Is there anything that proves that the Cauchy problem still provides the existence of an unique solution for an equation not in normal form or are there specific conditions?","Let's consider a one-dimensional physical system ( is the position and is time) described by a first order differential equation. I'm aware of the fact that if the equation can be put in normal form: the Cauchy problem, provided some mathematical hypotheses are verified, tells us that the equation admits the existence of an unique solution. Therefore the system is deterministic, i.e., given the initial conditions there is only one admissible motion. I was wondering what happens when the system is described by an equation that cannot be put in normal form: Is there anything that proves that the Cauchy problem still provides the existence of an unique solution for an equation not in normal form or are there specific conditions?","x t \begin{equation}
\dot{x}=f(x,t)
\end{equation} \begin{equation}
g(\dot{x},x,t)=0
\end{equation}","['ordinary-differential-equations', 'analysis', 'mathematical-physics', 'classical-mechanics']"
88,Help in solving the following Sturm-Liouville problem $-x^2y''-2xy'-\lambda y=0$,Help in solving the following Sturm-Liouville problem,-x^2y''-2xy'-\lambda y=0,"I have problems finding the eigenvalues and eigenfunctions of the equation: $$-x^2y''-2xy'-\lambda y=0$$ The domain is $[1,\pi]$ , with conditions $y(1)=y(\pi)=0$ . I have proved the values of $\lambda$ have to be nonnegative. By making the subsitution $y=x^m$ , I get to the condition: $$m^2+m+\lambda=0$$ Which leads to the solutions: $$y=ax^{-\frac{1+\sqrt{1-4\lambda}}{2}}+bx^{-\frac{1-\sqrt{1-4\lambda}}{2}}$$ I don't know if I'm going the right way about this or if I'm missing information about how to solve this type of problems.","I have problems finding the eigenvalues and eigenfunctions of the equation: The domain is , with conditions . I have proved the values of have to be nonnegative. By making the subsitution , I get to the condition: Which leads to the solutions: I don't know if I'm going the right way about this or if I'm missing information about how to solve this type of problems.","-x^2y''-2xy'-\lambda y=0 [1,\pi] y(1)=y(\pi)=0 \lambda y=x^m m^2+m+\lambda=0 y=ax^{-\frac{1+\sqrt{1-4\lambda}}{2}}+bx^{-\frac{1-\sqrt{1-4\lambda}}{2}}","['calculus', 'ordinary-differential-equations', 'sturm-liouville']"
89,"Explicit formula of the solution of $u_{tt}=au_{xx}$ and for which values of $a$ is this a ""wave equation""?","Explicit formula of the solution of  and for which values of  is this a ""wave equation""?",u_{tt}=au_{xx} a,"Let $a\in\mathbb R\setminus{0}$ and $u\in C^2((0,\infty)\times\mathbb R)$ be a solution of $$u_{tt}=au_{xx}\tag1.$$ I'm trying to find an explicit formula of $u$ using the ansatz $$u(t,x)=v(t)w(x);\tag2$$ and understand in which sense $(1)$ is a ""wave equation"" for appropriate values of $a$ . For 1.: By $(1)$ and $(2)$ , $$v''(t)w(x)=av(t)w''(x)\tag3$$ and hence $$\frac{v''(t)}{v(t)}=a\frac{w''(x)}{w(x)}=\lambda\tag4$$ for some constant $\lambda\in\mathbb R\setminus\{0\}$ and all $(t,x)\in(0,\infty)\times\mathbb R$ with $u(t,x)\ne0$ . $^1$ The first system, $v''=\lambda v$ , can be solved using the ansatz $v(t)=e^{\alpha t}$ . We easily see that if $\lambda>0$ , then $$v(t)=c_1e^{\alpha_1t}+c_2e^{-\alpha_1t}\tag5;$$ if $\lambda=0$ , then $$v(t)=c_1+c_2t\tag6;$$ if $\lambda<0$ , then $$v(t)=c_1e^{{\rm i}\alpha_1}+c_2e^{-{\rm i}\alpha_1}=\tilde c_1\cos(\alpha_1t)+\tilde c_2{\rm i}\sin(\alpha_1x)\tag7.$$ For the second system, $w''=\frac\lambda a$ , we obtain solutions of precisely the same form by considering the cases $\frac\lambda a>0$ , $\lambda=0$ and $\frac\lambda a<0$ . So, the solution is finally a product of the terms in $(5)$ - $(7)$ in $t$ and the corresponding terms in $x$ . For example, if $a,\lambda>0$ , then $$u(t,x)=c_1e^{\alpha_1t+\alpha_2x}+c_2e^{-\alpha_1t+\alpha_2x}+c_3e^{\alpha_1-\alpha_2x}+c_4e^{-\alpha_1t-\alpha_2x}\tag8.$$ Is there anything more we can do? And how do we need to approach question 2? $^1$ I'm not sure how we subsequently need to argue for $(t,x)\in(0,\infty)\times\mathbb R$ with $u(t,x)=0$ .","Let and be a solution of I'm trying to find an explicit formula of using the ansatz and understand in which sense is a ""wave equation"" for appropriate values of . For 1.: By and , and hence for some constant and all with . The first system, , can be solved using the ansatz . We easily see that if , then if , then if , then For the second system, , we obtain solutions of precisely the same form by considering the cases , and . So, the solution is finally a product of the terms in - in and the corresponding terms in . For example, if , then Is there anything more we can do? And how do we need to approach question 2? I'm not sure how we subsequently need to argue for with .","a\in\mathbb R\setminus{0} u\in C^2((0,\infty)\times\mathbb R) u_{tt}=au_{xx}\tag1. u u(t,x)=v(t)w(x);\tag2 (1) a (1) (2) v''(t)w(x)=av(t)w''(x)\tag3 \frac{v''(t)}{v(t)}=a\frac{w''(x)}{w(x)}=\lambda\tag4 \lambda\in\mathbb R\setminus\{0\} (t,x)\in(0,\infty)\times\mathbb R u(t,x)\ne0 ^1 v''=\lambda v v(t)=e^{\alpha t} \lambda>0 v(t)=c_1e^{\alpha_1t}+c_2e^{-\alpha_1t}\tag5; \lambda=0 v(t)=c_1+c_2t\tag6; \lambda<0 v(t)=c_1e^{{\rm i}\alpha_1}+c_2e^{-{\rm i}\alpha_1}=\tilde c_1\cos(\alpha_1t)+\tilde c_2{\rm i}\sin(\alpha_1x)\tag7. w''=\frac\lambda a \frac\lambda a>0 \lambda=0 \frac\lambda a<0 (5) (7) t x a,\lambda>0 u(t,x)=c_1e^{\alpha_1t+\alpha_2x}+c_2e^{-\alpha_1t+\alpha_2x}+c_3e^{\alpha_1-\alpha_2x}+c_4e^{-\alpha_1t-\alpha_2x}\tag8. ^1 (t,x)\in(0,\infty)\times\mathbb R u(t,x)=0","['ordinary-differential-equations', 'partial-differential-equations', 'wave-equation']"
90,Asymptotic location of the turning points of $\ddot x = -x-{\dot x}^3$,Asymptotic location of the turning points of,\ddot x = -x-{\dot x}^3,"Fix a solution $x(t)$ of the following ODE: $$\ddot x=-x-\dot {x}^3.$$ Does the energy $E(t) := x^2 + \dot x^2 $ decrease to zero as $t \to \infty$ ? Denote by $x_1,x_2,\ldots $ the positive values of $x(t)$ for which $\dot x(t)=0$ , ordered by time. Prove that the following limit exists: $$\lim_{n \to \infty} \frac{x_n-x_{n+1}}{x_n^3}.$$ Some Remarks To prove 1 it is enough to prove that $\dot x \to 0$ , since then $x\to c$ and the ODE forces $c=0$ . If we had $\dot x \not \to 0$ then $\dot x$ would have been bounded away from $0$ ""a lot"", so $\dot E=-\dot x ^4$ would decrease at a rate bounded from below, hence tend to zero. This is not yet a proof because $\dot x$ might only have spikes away from zero. Perhaps this will make its second derivative large and lead to a contradiction with the ODE? In the linearized equation the sequence $(x_n)$ is constant, and for $t\gg 0$ we expect the system to be very close to its linearization. The limit quantifies this. I cross-posted from the physics site.","Fix a solution of the following ODE: Does the energy decrease to zero as ? Denote by the positive values of for which , ordered by time. Prove that the following limit exists: Some Remarks To prove 1 it is enough to prove that , since then and the ODE forces . If we had then would have been bounded away from ""a lot"", so would decrease at a rate bounded from below, hence tend to zero. This is not yet a proof because might only have spikes away from zero. Perhaps this will make its second derivative large and lead to a contradiction with the ODE? In the linearized equation the sequence is constant, and for we expect the system to be very close to its linearization. The limit quantifies this. I cross-posted from the physics site.","x(t) \ddot x=-x-\dot {x}^3. E(t) := x^2 + \dot x^2  t \to \infty x_1,x_2,\ldots  x(t) \dot x(t)=0 \lim_{n \to \infty} \frac{x_n-x_{n+1}}{x_n^3}. \dot x \to 0 x\to c c=0 \dot x \not \to 0 \dot x 0 \dot E=-\dot x ^4 \dot x (x_n) t\gg 0","['ordinary-differential-equations', 'perturbation-theory']"
91,Pollution of 3 lakes with differential equations,Pollution of 3 lakes with differential equations,,"Consider three lakes of equal volumes connected to each other with a flow. An accident results in the spillage of 300 000 kg of chemicals in Lake 1. Calculate the quantity of this chemical present, after the accident, in each of the lakes, assuming that it remains in one of the three lakes. Then the problem gives me this suggestion: Suggestion: construct the illustrated compartmental model in which $l_i(t)$ indicates the quantity of pollutant in lake $i$ at time $t$ , $V$ is the volume of each of the lakes and $r$ is the flow rate between 2 lakes and deduce the equations \begin{align}     \frac{dl_1(t)}{dt}&= -r\frac{l_1}{V}+r\frac{l_3}{V}\nonumber\\     \frac{dl_2(t)}{dt}&=-r\frac{l_2}{V}+r\frac{l_1}{V}\nonumber\\     \frac{dl_3(t)}{dt}&=-r\frac{l_3}{V}+r\frac{l_2}{V}\nonumber \end{align} with the initial condition $l_1 (0) = 300000$ kg, $l_2 (0) = 0$ and $l_3 (0) = 0$ . Indicate how to bring these three equations to the system of two equations that need to be resolved: \begin{align}     \frac{dl_1(t)}{dt}&= -r\frac{l_1}{V}+r\frac{300 000-l_1-l_2}{V}\nonumber\\     \frac{dl_2(t)}{dt}&=-r\frac{l_2}{V}+r\frac{l_1}{V}\nonumber \end{align} I was wondering how can I transform the 3 differential equations into the 2 differential equations above? Thank you!","Consider three lakes of equal volumes connected to each other with a flow. An accident results in the spillage of 300 000 kg of chemicals in Lake 1. Calculate the quantity of this chemical present, after the accident, in each of the lakes, assuming that it remains in one of the three lakes. Then the problem gives me this suggestion: Suggestion: construct the illustrated compartmental model in which indicates the quantity of pollutant in lake at time , is the volume of each of the lakes and is the flow rate between 2 lakes and deduce the equations with the initial condition kg, and . Indicate how to bring these three equations to the system of two equations that need to be resolved: I was wondering how can I transform the 3 differential equations into the 2 differential equations above? Thank you!","l_i(t) i t V r \begin{align}
    \frac{dl_1(t)}{dt}&= -r\frac{l_1}{V}+r\frac{l_3}{V}\nonumber\\
    \frac{dl_2(t)}{dt}&=-r\frac{l_2}{V}+r\frac{l_1}{V}\nonumber\\
    \frac{dl_3(t)}{dt}&=-r\frac{l_3}{V}+r\frac{l_2}{V}\nonumber
\end{align} l_1 (0) = 300000 l_2 (0) = 0 l_3 (0) = 0 \begin{align}
    \frac{dl_1(t)}{dt}&= -r\frac{l_1}{V}+r\frac{300 000-l_1-l_2}{V}\nonumber\\
    \frac{dl_2(t)}{dt}&=-r\frac{l_2}{V}+r\frac{l_1}{V}\nonumber
\end{align}","['calculus', 'linear-algebra', 'ordinary-differential-equations']"
92,Loewy decomposition of differential operators,Loewy decomposition of differential operators,,"The paper by Fritz Schwarz, ""Loewy decomposition of linear differential equations"" , contains the following lemma, which I try to prove in order to understand the algorithm which Schwarz describes and which goes back to Loewy: Lemma: Determining the right irreducible factors of an ordinary operator up to order three with rational function coeffictents amounts to finding rational solutions of Riccati equations. A second order operator (with $D=\frac{d}{dx}$ ) $D^2+AD+B, \; A,B \in \mathbb{Q}(x), \qquad \qquad (1)$ has a right factor $D+a, \; a \in \mathbb{Q}(x)$ , if $a$ is a rational solution of $a'-a^2+Aa-B=0. \qquad \qquad (2)$ A third order operator $D^3+AD^2+BD+C, \; A,B,C \in \mathbb{Q}(x), \qquad \qquad (3)$ has a right factor $D+a, \; a \in \mathbb{Q}(x)$ , if $a$ is a rational solution of $a''-3aa'+a^3+A(a'-a^2)+Ba-C =0. \qquad \qquad$ (4) It has a right factor $D^2+bD+c, \, b,c \in \mathbb{Q}(x)$ , if b is a rational solution of $ b''-3bb'+b^3+2A(b'-b^2)+(A'+A^2+B)b-B'-AB+C=0. \qquad \qquad $ (6) Then $c=-(b'-b^2+Ab-B). \qquad (7) \qquad$ End of the lemma. The author of the paper takes the proof to be obvious when dividing the given operator by the right factor and requiring that the division be exact, but I don't really see that for the second and the third case. What we want to do with the help of this lemma is decompose a differential operator $L$ in order to solve the corresponding differential equation $Ly=0$ , with y being the unknown function of x and A,B,C,a,b,c being rational functions of x. I understand that the operator product for differential operators is noncommutative and obeys the rule $Da=aD+a'. \qquad \qquad (5)$ So in order to proof the lemma's first statement, I do the following: Polynomial division (in D) of (1) by $D+a$ gives me $D+A-a$ , plus some rest $a^2-Aa+B$ . Ignoring the rest for the moment, I build and simplify the operator product, paying attention to the above rule (5): $(D+A-a)(D+a)= D^2+AD-aD+Da+Aa-a^2 = D^2+AD+a'+Aa-a^2 $ , and what follows is (2), if the division is to be exact. Now the same approach for proving the second statement. Polynomial division of (3) by $D+a$ gives $D^2+D(A-a)-Aa+a^2+B$ plus some rest. Again we build the product, multiply it out and try to simplify acording to (5): \begin{align} &(D^2+DA-Da-Aa+a^2+B)(D+a)  \\ &= D^3+DAD-DaD-AaD+a^2D+BD +D^2a+DAa-Da^2-Aa^2+a^3+Ba  \\ &= D^3+((AD+A')D)-(D(Da-a'))-(DAa-(Aa)')+(Da^2-(a^2)')+BD+ \cdots \\ &= D^3+ (AD^2+A'D)-(D^2a-Da')-(DAa-A'a-Aa')+(Da^2-2aa')+BD+ \cdots  \\ &= D^3+AD^2+BD+A'D+Da'+A'a+Aa'-2aa'-Aa^2+a^3+Ba. \end{align} But this does not bring me to (4). In particular, I don't know how to get rid of the terms $A'D$ and $Da'$ , where D still appears. What am I doing wrong? Or is my whole approach not valid?","The paper by Fritz Schwarz, ""Loewy decomposition of linear differential equations"" , contains the following lemma, which I try to prove in order to understand the algorithm which Schwarz describes and which goes back to Loewy: Lemma: Determining the right irreducible factors of an ordinary operator up to order three with rational function coeffictents amounts to finding rational solutions of Riccati equations. A second order operator (with ) has a right factor , if is a rational solution of A third order operator has a right factor , if is a rational solution of (4) It has a right factor , if b is a rational solution of (6) Then End of the lemma. The author of the paper takes the proof to be obvious when dividing the given operator by the right factor and requiring that the division be exact, but I don't really see that for the second and the third case. What we want to do with the help of this lemma is decompose a differential operator in order to solve the corresponding differential equation , with y being the unknown function of x and A,B,C,a,b,c being rational functions of x. I understand that the operator product for differential operators is noncommutative and obeys the rule So in order to proof the lemma's first statement, I do the following: Polynomial division (in D) of (1) by gives me , plus some rest . Ignoring the rest for the moment, I build and simplify the operator product, paying attention to the above rule (5): , and what follows is (2), if the division is to be exact. Now the same approach for proving the second statement. Polynomial division of (3) by gives plus some rest. Again we build the product, multiply it out and try to simplify acording to (5): But this does not bring me to (4). In particular, I don't know how to get rid of the terms and , where D still appears. What am I doing wrong? Or is my whole approach not valid?","D=\frac{d}{dx} D^2+AD+B, \; A,B \in \mathbb{Q}(x), \qquad \qquad (1) D+a, \; a \in \mathbb{Q}(x) a a'-a^2+Aa-B=0. \qquad \qquad (2) D^3+AD^2+BD+C, \; A,B,C \in \mathbb{Q}(x), \qquad \qquad (3) D+a, \; a \in \mathbb{Q}(x) a a''-3aa'+a^3+A(a'-a^2)+Ba-C =0. \qquad \qquad D^2+bD+c, \, b,c \in \mathbb{Q}(x)  b''-3bb'+b^3+2A(b'-b^2)+(A'+A^2+B)b-B'-AB+C=0. \qquad \qquad  c=-(b'-b^2+Ab-B). \qquad (7) \qquad L Ly=0 Da=aD+a'. \qquad \qquad (5) D+a D+A-a a^2-Aa+B (D+A-a)(D+a)= D^2+AD-aD+Da+Aa-a^2 = D^2+AD+a'+Aa-a^2  D+a D^2+D(A-a)-Aa+a^2+B \begin{align}
&(D^2+DA-Da-Aa+a^2+B)(D+a) 
\\ &= D^3+DAD-DaD-AaD+a^2D+BD +D^2a+DAa-Da^2-Aa^2+a^3+Ba 
\\ &= D^3+((AD+A')D)-(D(Da-a'))-(DAa-(Aa)')+(Da^2-(a^2)')+BD+ \cdots
\\ &= D^3+ (AD^2+A'D)-(D^2a-Da')-(DAa-A'a-Aa')+(Da^2-2aa')+BD+ \cdots 
\\ &= D^3+AD^2+BD+A'D+Da'+A'a+Aa'-2aa'-Aa^2+a^3+Ba.
\end{align} A'D Da'","['ordinary-differential-equations', 'noncommutative-algebra', 'differential-operators', 'differential-algebra']"
93,Existence and uniqueness of maximal solution for first-order non-linear ODE,Existence and uniqueness of maximal solution for first-order non-linear ODE,,"Let $\alpha \in \mathbb{R}$ and $f$ be a real function defined by $f(u)=-ue^{\alpha u}\ln(\lvert u \rvert)$ if $u \neq 0$ and $f(0)=0$ . Let $u_0 \in \mathbb{R}^+$ for the following problem: \begin{equation} \begin{cases} u'(t)=f(u(t)), t\in \mathbb{R}\\  u(0)=0 \end{cases} \end{equation} Has the Cauchy problem a unique maximal solution? I tried: Let $g$ be a function such that: \begin{equation} \begin{cases} g:\mathbb{R}\times\mathbb{R} \rightarrow \mathbb{R}\\  g:(t,u) \rightarrow f(u(t)) \end{cases} \end{equation} I try to verify the Cauchy-Lipschitz theorem. $f$ is continuous so $g$ is also continuous. Now I need to prove that $g$ is locally Lipschitz with respect to its second variable. I can't manage to prove it. I tried to use the mean value theorem, but without success... Maybe it does not verify Cauchy-Lipschitz conditions.","Let and be a real function defined by if and . Let for the following problem: Has the Cauchy problem a unique maximal solution? I tried: Let be a function such that: I try to verify the Cauchy-Lipschitz theorem. is continuous so is also continuous. Now I need to prove that is locally Lipschitz with respect to its second variable. I can't manage to prove it. I tried to use the mean value theorem, but without success... Maybe it does not verify Cauchy-Lipschitz conditions.","\alpha \in \mathbb{R} f f(u)=-ue^{\alpha u}\ln(\lvert u \rvert) u \neq 0 f(0)=0 u_0 \in \mathbb{R}^+ \begin{equation}
\begin{cases}
u'(t)=f(u(t)), t\in \mathbb{R}\\ 
u(0)=0
\end{cases}
\end{equation} g \begin{equation}
\begin{cases}
g:\mathbb{R}\times\mathbb{R} \rightarrow \mathbb{R}\\ 
g:(t,u) \rightarrow f(u(t))
\end{cases}
\end{equation} f g g","['ordinary-differential-equations', 'lipschitz-functions']"
94,Shifrin Differential Geometry Exercise $1.2.27$ -- A Differential Equation For Bikes,Shifrin Differential Geometry Exercise  -- A Differential Equation For Bikes,1.2.27,"The Question Suppose the front wheel of a bicycle follows the arclength-parametrized plane curve $\vec{\alpha}$ . Determine the path $\vec{\beta}$ of the rear wheel, $1$ unit away. As the hint explains, the goal is a differential equation involving $\theta$ , the angle of the front wheel with the axle of the bike, and $\kappa$ , the curvature of $\vec{\alpha}$ . This question is very interesting to me, and I haven't seen a solution written up anywhere. This is sort of shocking to me as it seems like it should be a very relevant problem for e.g. autonomous driving. What I've Tried I've only been able to make minimal progress. The hint tells us to write $\vec{\alpha} - \vec{\beta}$ in terms of $\theta$ , $\vec{T}$ (i.e. $\vec{\alpha}'$ ), and $\vec{N}$ (i.e. $\frac{\vec{\alpha}''}{\kappa}$ ). We obviously have $\| \vec{\alpha} - \vec{\beta}\|^2 = 1$ . Differentiating, we obtain $$ (\vec{\alpha}' - \vec{\beta}') \cdot (\vec{\alpha} - \vec{\beta}) = 0 $$ That is: $$ (\vec{T} - \vec{\beta}') \cdot (\vec{\alpha} - \vec{\beta}) = 0 $$ Differentiating again, we obtain $$ (\kappa \vec{N} - \vec{\beta}'') \cdot (\vec{\alpha} - \vec{\beta}) + (\vec{T} - \vec{\beta}') \cdot (\vec{T} - \vec{\beta}') = 0 $$ Now it seems to me we should have $$ \vec{T} \cdot \vec{\beta}' = \|\vec{\beta}'\| \cos \theta $$ So we can expand $$ (\vec{T} - \vec{\beta}') \cdot (\vec{T} - \vec{\beta}') = 1 - 2\|\beta'\| \cos\theta + \|\vec{\beta}'\|^2 $$ And that's as far as I've gotten. It seems like I'm going about this all wrong. In particular, I have no idea what to do with the derivatives of $\vec{\beta}$ . If only $\vec{\beta}$ were arclength parametrized I feel like I could make some progress, but I don't think there is any reason it should be. The only thing I can think is that we should have $$ \vec{\alpha} - \vec{\beta} = \lambda \vec{\beta}' $$ for some $\lambda$ that could depend on the arclength of $\vec{\alpha}$ . I didn't push too far in this direction, though, since it required introducing yet another unknown. I thought of yet another line of attack. Since $\|\vec{\alpha} - \vec{\beta}\| = 1$ , we can say that $\|\vec{\alpha} - \vec{\beta}\|$ is just $\vec{T}$ rotated by $\theta$ , i.e. $$ \vec{\alpha} - \vec{\beta} = \cos (\theta) \vec{T} + \sin(\theta) \vec{N} $$ With this expression I've gone as far as the hint suggests, but I don't see what to do next. What am I missing? If this post summons Ted Shifrin, and he'd rather answers to his textbook questions not be given out, I'd be happy to delete this question and post it as a reference request instead. I really am shocked I haven't been able to find this problem written about anywhere. I'm guessing it's because I'm bad at searching the literature, not because it actually hasn't been written about.","The Question Suppose the front wheel of a bicycle follows the arclength-parametrized plane curve . Determine the path of the rear wheel, unit away. As the hint explains, the goal is a differential equation involving , the angle of the front wheel with the axle of the bike, and , the curvature of . This question is very interesting to me, and I haven't seen a solution written up anywhere. This is sort of shocking to me as it seems like it should be a very relevant problem for e.g. autonomous driving. What I've Tried I've only been able to make minimal progress. The hint tells us to write in terms of , (i.e. ), and (i.e. ). We obviously have . Differentiating, we obtain That is: Differentiating again, we obtain Now it seems to me we should have So we can expand And that's as far as I've gotten. It seems like I'm going about this all wrong. In particular, I have no idea what to do with the derivatives of . If only were arclength parametrized I feel like I could make some progress, but I don't think there is any reason it should be. The only thing I can think is that we should have for some that could depend on the arclength of . I didn't push too far in this direction, though, since it required introducing yet another unknown. I thought of yet another line of attack. Since , we can say that is just rotated by , i.e. With this expression I've gone as far as the hint suggests, but I don't see what to do next. What am I missing? If this post summons Ted Shifrin, and he'd rather answers to his textbook questions not be given out, I'd be happy to delete this question and post it as a reference request instead. I really am shocked I haven't been able to find this problem written about anywhere. I'm guessing it's because I'm bad at searching the literature, not because it actually hasn't been written about.","\vec{\alpha} \vec{\beta} 1 \theta \kappa \vec{\alpha} \vec{\alpha} - \vec{\beta} \theta \vec{T} \vec{\alpha}' \vec{N} \frac{\vec{\alpha}''}{\kappa} \| \vec{\alpha} - \vec{\beta}\|^2 = 1 
(\vec{\alpha}' - \vec{\beta}') \cdot (\vec{\alpha} - \vec{\beta}) = 0
 
(\vec{T} - \vec{\beta}') \cdot (\vec{\alpha} - \vec{\beta}) = 0
 
(\kappa \vec{N} - \vec{\beta}'') \cdot (\vec{\alpha} - \vec{\beta}) + (\vec{T} - \vec{\beta}') \cdot (\vec{T} - \vec{\beta}') = 0
 
\vec{T} \cdot \vec{\beta}' = \|\vec{\beta}'\| \cos \theta
 
(\vec{T} - \vec{\beta}') \cdot (\vec{T} - \vec{\beta}') = 1 - 2\|\beta'\| \cos\theta + \|\vec{\beta}'\|^2
 \vec{\beta} \vec{\beta} 
\vec{\alpha} - \vec{\beta} = \lambda \vec{\beta}'
 \lambda \vec{\alpha} \|\vec{\alpha} - \vec{\beta}\| = 1 \|\vec{\alpha} - \vec{\beta}\| \vec{T} \theta 
\vec{\alpha} - \vec{\beta} = \cos (\theta) \vec{T} + \sin(\theta) \vec{N}
","['ordinary-differential-equations', 'differential-geometry', 'curvature']"
95,Asymptotic solution to the differential equation $y''+(\pi^2+\varepsilon)y+y^2=0$ with $y'(0)=y'(1)=0$,Asymptotic solution to the differential equation  with,y''+(\pi^2+\varepsilon)y+y^2=0 y'(0)=y'(1)=0,"In studying perturbation methods, I am stuck with the following problem. Let $\varepsilon>0$ be a small parameter. Find the asymptotic approximation to the non-constant solution of the differential equation $$ y''+(\pi^2+\varepsilon)y+y^2=0,\\ y'(0)=y'(1)=0. $$ I tried to solving it by perturbation method. Assume that we have the expansion $$ y(x)=\varepsilon^\alpha y_1(x)+\varepsilon^\beta y_2(x)+\cdots, \quad (*)$$ where $0<\alpha<\beta$ . Substituting it into the original differential equation, at leading order $O(\varepsilon^\alpha)$ , we obtain $$ y''_1+\pi^2 y_1=0,\\ y'_1(0)=y'_1(1)=0, $$ which yields $y_1(x)=A\cos(\pi x)$ , where $A$ is a constant. To find the value of $A$ , we need to look at the next order, which is $$ \varepsilon^\beta y''_2+\varepsilon^\beta y_2+\varepsilon^{1+\alpha }y_1+\varepsilon^{2\alpha} y_1^2=0,\\ y'_2(0)=y'_2(0)=0$$ I tried to balance the terms by choosing $\beta=1+\alpha$ or $\beta=2\alpha$ or $1+\alpha=2\alpha$ or $\beta=1+\alpha=2\alpha$ . However, in each of the cases,  I ended up with $A=0$ when substituting the solution of $y_2$ into the boundary conditions. On the other hand, integrating the original differential equation once, we have $$ y'^2+(\pi^2+\varepsilon)y^2+\frac{2}{3}y^3=C,$$ where $C$ is a constant. Using the boundary conditions $y'(0)=y'(1)=0$ , we can find the values of $y(0)$ and $y(1)$ once $C$ is specified. By separating variables, we see that $$ \int_{y(0}^{y(1)}\frac{dy}{\sqrt{C-(\pi+\varepsilon^2)y^2-\frac{2}{3}y^3}}=\int_{0}^1 dx=1,$$ which can be used to find the constant $C$ . Numerical calculations show that for small $\varepsilon>0$ , $C$ exists, so the differential equation has non-constant solutions for small $\varepsilon>0$ . For instance, when $\varepsilon=0.01$ , $C\approx 1.1699$ and the graph of a non-constant solution plotted by Mathematica is I think that problem is that the asymptotic expansion of the non-constant solution may not be of form (*), but I don't know how what the correct form is.","In studying perturbation methods, I am stuck with the following problem. Let be a small parameter. Find the asymptotic approximation to the non-constant solution of the differential equation I tried to solving it by perturbation method. Assume that we have the expansion where . Substituting it into the original differential equation, at leading order , we obtain which yields , where is a constant. To find the value of , we need to look at the next order, which is I tried to balance the terms by choosing or or or . However, in each of the cases,  I ended up with when substituting the solution of into the boundary conditions. On the other hand, integrating the original differential equation once, we have where is a constant. Using the boundary conditions , we can find the values of and once is specified. By separating variables, we see that which can be used to find the constant . Numerical calculations show that for small , exists, so the differential equation has non-constant solutions for small . For instance, when , and the graph of a non-constant solution plotted by Mathematica is I think that problem is that the asymptotic expansion of the non-constant solution may not be of form (*), but I don't know how what the correct form is.","\varepsilon>0 
y''+(\pi^2+\varepsilon)y+y^2=0,\\
y'(0)=y'(1)=0.
 
y(x)=\varepsilon^\alpha y_1(x)+\varepsilon^\beta y_2(x)+\cdots, \quad (*) 0<\alpha<\beta O(\varepsilon^\alpha) 
y''_1+\pi^2 y_1=0,\\
y'_1(0)=y'_1(1)=0,
 y_1(x)=A\cos(\pi x) A A 
\varepsilon^\beta y''_2+\varepsilon^\beta y_2+\varepsilon^{1+\alpha }y_1+\varepsilon^{2\alpha} y_1^2=0,\\
y'_2(0)=y'_2(0)=0 \beta=1+\alpha \beta=2\alpha 1+\alpha=2\alpha \beta=1+\alpha=2\alpha A=0 y_2 
y'^2+(\pi^2+\varepsilon)y^2+\frac{2}{3}y^3=C, C y'(0)=y'(1)=0 y(0) y(1) C 
\int_{y(0}^{y(1)}\frac{dy}{\sqrt{C-(\pi+\varepsilon^2)y^2-\frac{2}{3}y^3}}=\int_{0}^1 dx=1, C \varepsilon>0 C \varepsilon>0 \varepsilon=0.01 C\approx 1.1699","['ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
96,Method of Undetermined Coefficients when ODE does not have constant coefficients,Method of Undetermined Coefficients when ODE does not have constant coefficients,,"Even though the Method of Undetermined Coefficients is usually taught as a method for solving nonhomogeneous linear ODEs with constant coefficients, it seems to also work if the coefficients are not constant. For example, let $$ t^2y''-2y=t. $$ The solution to the associated homogeneous equation, $t^2y''-2y=0$ , is spanned by the functions $t^2$ and $t^{-1}$ . Using the Method of Undetermined Coefficients, one can guess a particular solution of the form $y_p(t)=At+B$ , where $A$ and $B$ are constants. Substituting $y_p(t)$ into the differential equation and solving gives $A=-\frac{1}{2}$ and $B=0$ , so that the general solution to $t^2y''-2y=t$ is $$y(t)=C_1t^2+C_2t^{-1}-\frac{1}{2}t.$$ However, if we consider the equation $$t^2y''-2y=t^2$$ instead, then our initial guess for a particular solution, $y_p(t)=At^2+Bt+C$ ( $A$ , $B$ , $C$ constants) ""overlaps"" with part of our solution to $t^2y''-2y=0$ . This suggests that we should look for a particular solution of the form $y_p(t)=At^5+Bt^4+Ct^3$ instead. Substituting this into the ODE to try to solve for $A$ , $B$ , and $C$ doesn't give anything useful; in fact, through Variation of Parameters, we get a particular solution $y_p(t)=\frac{1}{3}\ln(t)t^2$ , which the Method of Undetermined Coefficients has no hope of finding. I'm not really sure what's going on here. Why does the Method of Undetermined Coefficients fail to give me the correct particular solution in the second case? Is there anything general that can be said here about applying this method in the nonconstant coefficient case?","Even though the Method of Undetermined Coefficients is usually taught as a method for solving nonhomogeneous linear ODEs with constant coefficients, it seems to also work if the coefficients are not constant. For example, let The solution to the associated homogeneous equation, , is spanned by the functions and . Using the Method of Undetermined Coefficients, one can guess a particular solution of the form , where and are constants. Substituting into the differential equation and solving gives and , so that the general solution to is However, if we consider the equation instead, then our initial guess for a particular solution, ( , , constants) ""overlaps"" with part of our solution to . This suggests that we should look for a particular solution of the form instead. Substituting this into the ODE to try to solve for , , and doesn't give anything useful; in fact, through Variation of Parameters, we get a particular solution , which the Method of Undetermined Coefficients has no hope of finding. I'm not really sure what's going on here. Why does the Method of Undetermined Coefficients fail to give me the correct particular solution in the second case? Is there anything general that can be said here about applying this method in the nonconstant coefficient case?","
t^2y''-2y=t.
 t^2y''-2y=0 t^2 t^{-1} y_p(t)=At+B A B y_p(t) A=-\frac{1}{2} B=0 t^2y''-2y=t y(t)=C_1t^2+C_2t^{-1}-\frac{1}{2}t. t^2y''-2y=t^2 y_p(t)=At^2+Bt+C A B C t^2y''-2y=0 y_p(t)=At^5+Bt^4+Ct^3 A B C y_p(t)=\frac{1}{3}\ln(t)t^2","['calculus', 'ordinary-differential-equations']"
97,Differential Equation Solution By Power Series,Differential Equation Solution By Power Series,,"Solve $(1 + x)y' = py;\ \ \ y(0) = 1$ , where $p$ is an arbitrary constant. First I plugged in the guess $y = \sum_{n = 0}^\infty a_n x^n$ : $(1 + x)(\sum_{n = 0}^\infty a_n x^n)' = p\sum_{n = 0}^\infty a_n x^n$ Then I expanded the derivative and multiplication: $\sum_{n = 0}^\infty n a_n x^{n - 1} + \sum_{n = 0}^\infty n a_n x^n = p\sum_{n = 0}^\infty a_n x^n$ Then I shifted the left index (the first term yielding $0$ allows the lower bound to remain $0$ ) and algebraically combined the summations: $\sum_{n = 0}^\infty (n + 1)a_{n + 1} x^n + (n - p)a_n x^n = 0$ This leads to the following recurrence relation: $a_{n + 1} = \frac{p - n}{n + 1}a_n$ Thus for various values of $n$ : $a_1 = p a_0$ , $a_2 = \frac{p(p - 1)}{2}a_0$ , $a_3 = \frac{p(p - 1)(p - 2)}{6} a_0$ , etc. So applying definitions for the exponential taylor series and falling factorial, the guessed solution would be: $y = \sum_{n = 0}^\infty \frac{p! a_0 x^n}{n! (p - n)!} = \sum_{n = 0}^\infty a_0 e^x p^{\underline n}$ Solving the initial value problem: $1 = \sum_{n = 0}^\infty a_0 e^0 p^{\underline n} \implies a_0 = \frac{1}{\sum_{n = 0}^\infty p^{\underline n}}$ My final solution is: $y = \frac{\sum_{n = 0}^\infty e^x p^{\underline n}}{\sum_{n = 0}^\infty p^{\underline n}}$ However, the answer is supposed to be $y = (1 + x)^p$ .  Are these identical, or did I make an error somewhere?","Solve , where is an arbitrary constant. First I plugged in the guess : Then I expanded the derivative and multiplication: Then I shifted the left index (the first term yielding allows the lower bound to remain ) and algebraically combined the summations: This leads to the following recurrence relation: Thus for various values of : , , , etc. So applying definitions for the exponential taylor series and falling factorial, the guessed solution would be: Solving the initial value problem: My final solution is: However, the answer is supposed to be .  Are these identical, or did I make an error somewhere?",(1 + x)y' = py;\ \ \ y(0) = 1 p y = \sum_{n = 0}^\infty a_n x^n (1 + x)(\sum_{n = 0}^\infty a_n x^n)' = p\sum_{n = 0}^\infty a_n x^n \sum_{n = 0}^\infty n a_n x^{n - 1} + \sum_{n = 0}^\infty n a_n x^n = p\sum_{n = 0}^\infty a_n x^n 0 0 \sum_{n = 0}^\infty (n + 1)a_{n + 1} x^n + (n - p)a_n x^n = 0 a_{n + 1} = \frac{p - n}{n + 1}a_n n a_1 = p a_0 a_2 = \frac{p(p - 1)}{2}a_0 a_3 = \frac{p(p - 1)(p - 2)}{6} a_0 y = \sum_{n = 0}^\infty \frac{p! a_0 x^n}{n! (p - n)!} = \sum_{n = 0}^\infty a_0 e^x p^{\underline n} 1 = \sum_{n = 0}^\infty a_0 e^0 p^{\underline n} \implies a_0 = \frac{1}{\sum_{n = 0}^\infty p^{\underline n}} y = \frac{\sum_{n = 0}^\infty e^x p^{\underline n}}{\sum_{n = 0}^\infty p^{\underline n}} y = (1 + x)^p,"['sequences-and-series', 'ordinary-differential-equations', 'solution-verification', 'power-series', 'factorial']"
98,How can I know if an equilibrium is stable in a difference equation?,How can I know if an equilibrium is stable in a difference equation?,,"Suppose that I have the following equation: $$ y_{t+1} = \frac{2+y_t-y^3_t}{2y_t} $$ Then $$ \bar{y} = \frac{2+\bar{y} -\bar{y}^3}{2\bar{y}} $$ And so I obtain these solutions: $$y_1 = -2, \quad y_2 = -1, \quad y_3 = 1 $$ Now how can I know if these equilibria are stable? I believe that it is useful to compute the derivative with respect to the solutions found. Therefore I obtain the derivative: $$ \frac{d}{dy} = -\frac{(y^3+1)}{y^2} $$ For $y_1 = -2$ the derivative becomes: $\frac{7}{4}$ For $y_2 = -1$ the derivative becomes: $0$ For $y_3 = 1$ the derivative becomes: $-2$ So how can I know if the equilibria found are stable?",Suppose that I have the following equation: Then And so I obtain these solutions: Now how can I know if these equilibria are stable? I believe that it is useful to compute the derivative with respect to the solutions found. Therefore I obtain the derivative: For the derivative becomes: For the derivative becomes: For the derivative becomes: So how can I know if the equilibria found are stable?,"
y_{t+1} = \frac{2+y_t-y^3_t}{2y_t}
 
\bar{y} = \frac{2+\bar{y} -\bar{y}^3}{2\bar{y}}
 y_1 = -2, \quad y_2 = -1, \quad y_3 = 1  
\frac{d}{dy}
=
-\frac{(y^3+1)}{y^2}
 y_1 = -2 \frac{7}{4} y_2 = -1 0 y_3 = 1 -2",['ordinary-differential-equations']
99,"Name of ""divided difference"" transform $\frac{f(x)-f(x_0)}{x-x_0}$ and special case $\frac{e^x - 1}{x}$?","Name of ""divided difference"" transform  and special case ?",\frac{f(x)-f(x_0)}{x-x_0} \frac{e^x - 1}{x},"Given an analytic function / formal power series $$\displaystyle f(x)=\sum _{n=0}^{\infty }\frac{f^{(n)}(x_0)}{n!}\left(x-x_{0}\right)^{n}=f(x_0)+f'(x_0)(x-x_{0})+ \tfrac{1}{2}f''(x_0)(x-x_{0})^{2}+\ldots$$ we can construct another analytical function via the ""divided difference"" transformation $$ R(f)(x) := \frac{f(x)-f(x_0)}{x-x_0} := f'(x_0) + \tfrac{1}{2}f''(x_0)(x-x_0) +  \tfrac{1}{6}f'''(x_0)(x-x_0)^2 + \ldots $$ Note that this is similar but not equal to the derivative operator $$ D(f)(x) = f'(x_0) + f''(x_0)(x-x_0) +  \tfrac{1}{2}f'''(x_0)(x-x_0)^2 + \ldots $$ Does this transformation have a name in the literature? What are its properties? In the discrete case it is known as divided differences, however here I am explicitly interested in this transform as a map $$T\colon A(D)\to A(D),\, f\mapsto R(f)$$ between the space of analytic functions $A(D)$ on an open interval $D\subset \mathbb R$ . Does the function we get by applying this transformation to the exponential function, i.e. $\frac{e^x-1}{x}$ have a name in the literature? The latter appears in the integrand of the exponential integral; is apparently related to the derivative of the exponential map from Lie groups and occurs in the solution of inhomogeneous linear ODEs, e.g. $\dot x = a x+b, x(t_0)=x_0$ has the solution $  x^*(t) = e^{a(t-t_0)}x_0 + \frac{e^{a(t-t_0)} -1}{a} b$ ; and in the multivariate case $\dot x(t) = A \cdot x(t) + b ,  x(t_0) = x_0$ we get the analogous $  x^*(t) = e^{A(t-t_0)}x_0 + \frac{e^{A(t-t_0)}-I}{A}b$ . Note that $\frac{e^{A(t-t_0)}-I}{A}$ exists even when $A$ is singular, hence it would be useful to drop this notation and give the function a name instead.","Given an analytic function / formal power series we can construct another analytical function via the ""divided difference"" transformation Note that this is similar but not equal to the derivative operator Does this transformation have a name in the literature? What are its properties? In the discrete case it is known as divided differences, however here I am explicitly interested in this transform as a map between the space of analytic functions on an open interval . Does the function we get by applying this transformation to the exponential function, i.e. have a name in the literature? The latter appears in the integrand of the exponential integral; is apparently related to the derivative of the exponential map from Lie groups and occurs in the solution of inhomogeneous linear ODEs, e.g. has the solution ; and in the multivariate case we get the analogous . Note that exists even when is singular, hence it would be useful to drop this notation and give the function a name instead.","\displaystyle f(x)=\sum _{n=0}^{\infty }\frac{f^{(n)}(x_0)}{n!}\left(x-x_{0}\right)^{n}=f(x_0)+f'(x_0)(x-x_{0})+ \tfrac{1}{2}f''(x_0)(x-x_{0})^{2}+\ldots  R(f)(x) := \frac{f(x)-f(x_0)}{x-x_0} := f'(x_0) + \tfrac{1}{2}f''(x_0)(x-x_0) +  \tfrac{1}{6}f'''(x_0)(x-x_0)^2 + \ldots   D(f)(x) = f'(x_0) + f''(x_0)(x-x_0) +  \tfrac{1}{2}f'''(x_0)(x-x_0)^2 + \ldots  T\colon A(D)\to A(D),\, f\mapsto R(f) A(D) D\subset \mathbb R \frac{e^x-1}{x} \dot x = a x+b, x(t_0)=x_0   x^*(t) = e^{a(t-t_0)}x_0 + \frac{e^{a(t-t_0)} -1}{a} b \dot x(t) = A \cdot x(t) + b ,  x(t_0) = x_0   x^*(t) = e^{A(t-t_0)}x_0 + \frac{e^{A(t-t_0)}-I}{A}b \frac{e^{A(t-t_0)}-I}{A} A","['ordinary-differential-equations', 'terminology', 'exponential-function', 'special-functions', 'analytic-functions']"
