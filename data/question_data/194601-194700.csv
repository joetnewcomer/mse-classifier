,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Derivatives of inverse functions,Derivatives of inverse functions,,"f(x)=x-tanx I am supposed to find the derivative to the inverse function in $(\pi$ /4) $-1$ . So my plan was to use the formula for the derivative of the inverse function. I have defined the domain to f(x) as $(-\pi/2,\pi/2)$ , to make it a one-to-one function. But I'm not able to solve this equation x-tanx= $(\pi/4)-1$ Could someone help me?","f(x)=x-tanx I am supposed to find the derivative to the inverse function in /4) . So my plan was to use the formula for the derivative of the inverse function. I have defined the domain to f(x) as , to make it a one-to-one function. But I'm not able to solve this equation x-tanx= Could someone help me?","(\pi -1 (-\pi/2,\pi/2) (\pi/4)-1","['derivatives', 'inverse', 'inverse-function']"
1,Partial Differential Equations - Characteristic Curves,Partial Differential Equations - Characteristic Curves,,"I am going through a book on partial differential equations, and am slightly confused by this paragraph: Consider the equation $\frac{dy}{dx}=p(x, y)$ . a) Think of solutions to this equations as curves in the $xy$ plane.    The equation is telling us that the slop of the tangent line to a   solution curve at the point $(x_0, y_0)$ is equal to $p(x_0, y_0)$ . b) We can also say that the solution curve at the point $(x_0, y_0)$ is changing in the direction of the vector $(1, p(x_0, y_0))$ . Where does the vector $(1, p(x_0, y_0))$ come from? Is this simply the gradient?","I am going through a book on partial differential equations, and am slightly confused by this paragraph: Consider the equation . a) Think of solutions to this equations as curves in the plane.    The equation is telling us that the slop of the tangent line to a   solution curve at the point is equal to . b) We can also say that the solution curve at the point is changing in the direction of the vector . Where does the vector come from? Is this simply the gradient?","\frac{dy}{dx}=p(x, y) xy (x_0, y_0) p(x_0, y_0) (x_0, y_0) (1, p(x_0, y_0)) (1, p(x_0, y_0))","['derivatives', 'partial-derivative', 'characteristic-functions', 'slope']"
2,Units of the rate of change of the area with respect to a linear measure,Units of the rate of change of the area with respect to a linear measure,,"Suppose we have: Find the instantaneous rate of change of the area of a square with respect to its side when the side is $2\,\textrm{cm}$ . Solution .  Let $a$ be the side of the square. Then $A=a^2$ is the respective area and $$ \dfrac{\textrm{d}A}{\textrm{d}a} = \dfrac{\textrm{d}}{\textrm{d}a}(a^2) = 2a. $$ Finally, $$ \left.\dfrac{\textrm{d}A}{\textrm{d}a}\right|_{a=2} = 4. $$ Now, the question: Is it $4\,\textrm{cm}$ ? I mean the units :) Or it would be better to say $\textrm{cm}^2/\textrm{cm}$ ? The later is the same as $\textrm{cm}$ , but since we are asking about the rate of change in the area, maybe we should keep it as $\textrm{cm}^2/\textrm{cm}$ ?","Suppose we have: Find the instantaneous rate of change of the area of a square with respect to its side when the side is . Solution .  Let be the side of the square. Then is the respective area and Finally, Now, the question: Is it ? I mean the units :) Or it would be better to say ? The later is the same as , but since we are asking about the rate of change in the area, maybe we should keep it as ?","2\,\textrm{cm} a A=a^2 
\dfrac{\textrm{d}A}{\textrm{d}a} = \dfrac{\textrm{d}}{\textrm{d}a}(a^2) = 2a.
 
\left.\dfrac{\textrm{d}A}{\textrm{d}a}\right|_{a=2} = 4.
 4\,\textrm{cm} \textrm{cm}^2/\textrm{cm} \textrm{cm} \textrm{cm}^2/\textrm{cm}","['derivatives', 'area', 'unit-of-measure']"
3,Derivative of polynomial all those roots are not simple(multiplicity > 1),Derivative of polynomial all those roots are not simple(multiplicity > 1),,"Let $\operatorname{char}\mathbb K = 0$ . Consider the polynomial $$q(x) := (x - a_1)^{n_1}...(x - a_m)^{n_m} \in \mathbb K[x]$$ and the polynomial $$Q(x) := q(x)(x - a_1)...(x - a_m) \in \mathbb K[x],$$ where $n_i \in \mathbb N \setminus\{0\}$ and $a_i \neq a_j, i \neq j$ . Now consider $$f(x) := \frac{Q^{'}}{q} \in \mathbb K[x].$$ Is it true that all roots of $f$ are simple(of multiplicity 1)? I can show that this is true for $m \leq 3$ . I'm doing this by directly writing down $f(x)$ and calculating it's discriminant. Any help is welcome",Let . Consider the polynomial and the polynomial where and . Now consider Is it true that all roots of are simple(of multiplicity 1)? I can show that this is true for . I'm doing this by directly writing down and calculating it's discriminant. Any help is welcome,"\operatorname{char}\mathbb K = 0 q(x) := (x - a_1)^{n_1}...(x - a_m)^{n_m} \in \mathbb K[x] Q(x) := q(x)(x - a_1)...(x - a_m) \in \mathbb K[x], n_i \in \mathbb N \setminus\{0\} a_i \neq a_j, i \neq j f(x) := \frac{Q^{'}}{q} \in \mathbb K[x]. f m \leq 3 f(x)","['derivatives', 'polynomials']"
4,To find derivative of $x!$ W.R.T $x$ [duplicate],To find derivative of  W.R.T  [duplicate],x! x,This question already has answers here : Did I derive a new form of the gamma function? (3 answers) Closed 4 years ago . I was curious to find the derivative of $x!$ and tried it myself on a basic level. This is what I tried :- (Please forgive me for not writing it in text I am bit slow in that and lazy too!) My question is whether this answer is correct or not since i find answers which are really beyond my knowledge:- Please help me figuring out whether my answer is correct or not and if correct then why wolfram shows such answer. (I am student and have no idea about those functions) Thanks!,This question already has answers here : Did I derive a new form of the gamma function? (3 answers) Closed 4 years ago . I was curious to find the derivative of and tried it myself on a basic level. This is what I tried :- (Please forgive me for not writing it in text I am bit slow in that and lazy too!) My question is whether this answer is correct or not since i find answers which are really beyond my knowledge:- Please help me figuring out whether my answer is correct or not and if correct then why wolfram shows such answer. (I am student and have no idea about those functions) Thanks!,x!,"['calculus', 'derivatives']"
5,"Find the $n$th derivative for the function $y=\sin(kx)$, $n\in\mathbb{R}$.","Find the th derivative for the function , .",n y=\sin(kx) n\in\mathbb{R},"I created a question for a high school calculus exam: Find the $n$ th derivative, $\dfrac{d^ny}{dx^n}$ , for the function $y=\sin(kx)$ , $n\in\mathbb{N}$ . The solution: \begin{align} \dfrac{d}{dx}\sin(kx)&=k\cos(kx) =k\sin k(x+\frac{\pi}{2})\\ \dfrac{d^2}{dx^2}\sin(kx)&=\dfrac{d}{dx}k\sin k(x+\frac{\pi}{2})=k^2\cos k(x+\frac{\pi}{2})=k^2\sin k(x+\pi)\\ \vdots \\ \dfrac{d^n}{dx^n}\sin(kx)&=k^n\sin k(x+\frac{n\pi}{2}) \end{align} One of my students had asked me ""Why did you include $n\in\mathbb{N}$ in the question?  Does that mean it's possible to be asked for $n\in\mathbb{R}$ later on?"" I'm aware of the existence of the fractional derivative, but I'm unable to give it any meaning or explain it.  So the problem I want to find a solution for is (and I'm totally aware that this is well beyond any high school calculus content): Find the $n$ th derivative for the function $y=\sin(kx)$ , $n\in\mathbb{R}$ . BONUS: Find the $n$ th derivative for the function $y=x^2e^x$ , $n\in\mathbb{R}$ .","I created a question for a high school calculus exam: Find the th derivative, , for the function , . The solution: One of my students had asked me ""Why did you include in the question?  Does that mean it's possible to be asked for later on?"" I'm aware of the existence of the fractional derivative, but I'm unable to give it any meaning or explain it.  So the problem I want to find a solution for is (and I'm totally aware that this is well beyond any high school calculus content): Find the th derivative for the function , . BONUS: Find the th derivative for the function , .","n \dfrac{d^ny}{dx^n} y=\sin(kx) n\in\mathbb{N} \begin{align}
\dfrac{d}{dx}\sin(kx)&=k\cos(kx) =k\sin k(x+\frac{\pi}{2})\\
\dfrac{d^2}{dx^2}\sin(kx)&=\dfrac{d}{dx}k\sin k(x+\frac{\pi}{2})=k^2\cos k(x+\frac{\pi}{2})=k^2\sin k(x+\pi)\\
\vdots \\
\dfrac{d^n}{dx^n}\sin(kx)&=k^n\sin k(x+\frac{n\pi}{2})
\end{align} n\in\mathbb{N} n\in\mathbb{R} n y=\sin(kx) n\in\mathbb{R} n y=x^2e^x n\in\mathbb{R}","['calculus', 'derivatives', 'fractional-calculus']"
6,Derivative of generalized hypergeometric function,Derivative of generalized hypergeometric function,,"Good evening Say we are working with a hypergeometric $3F3(a,b,c; d,e,f; z)$ function. I know that $\frac{d}{dz}3F3(a,b,c; d,e,f; z)=\frac{abc}{def}3F3(a+1,b+1,c+1; d+1,e+1,f+1; z)$ . There are numerous references for why this is so. For my specific parameters, $3F3(1,1,1; 2,2,2;z)$ Mathematica gives a different solution. The result I get is: $$ \frac{d}{dz}3F3(1,1,1; 2,2,2; z)=2F2(1,1; 2,2; z)-3F3(1,1,1; 2,2,2; z)$$ Can anyone explain why this would be the case, or provide a reference to this statement? I have been through Gradstein and Ryzhik and a few other books, but none of them give an identity that will result in a decrease in the number of hypergeometric parameters!","Good evening Say we are working with a hypergeometric function. I know that . There are numerous references for why this is so. For my specific parameters, Mathematica gives a different solution. The result I get is: Can anyone explain why this would be the case, or provide a reference to this statement? I have been through Gradstein and Ryzhik and a few other books, but none of them give an identity that will result in a decrease in the number of hypergeometric parameters!","3F3(a,b,c; d,e,f; z) \frac{d}{dz}3F3(a,b,c; d,e,f; z)=\frac{abc}{def}3F3(a+1,b+1,c+1; d+1,e+1,f+1; z) 3F3(1,1,1; 2,2,2;z)  \frac{d}{dz}3F3(1,1,1; 2,2,2; z)=2F2(1,1; 2,2; z)-3F3(1,1,1; 2,2,2; z)","['derivatives', 'hypergeometric-function']"
7,"Calculus for the Practical Man: Chapter 4, Problem 17 (Answer's differs from that of the book)","Calculus for the Practical Man: Chapter 4, Problem 17 (Answer's differs from that of the book)",,"A tank is in the form of a cone with the point downward, and the height and diameter are each 10 feet. How fast is the water pouring in at the moment when it is 5 feet deep and the surface is rising at the rate of 4 feet per minute? The volume of a cone is $$V_c = \frac{1}{12} \pi D^2 H.$$ Given that the diameter of the cone is equal to its height, the volume of the cone may be written $$V_c = \frac{1}{12} \pi H^3.$$ So, $$\frac{dV}{dt} = \frac{1}{4} \pi H^2 \frac{dH}{dt} = \frac{1}{4} \pi (5)^2\times 4 = 25 \pi \textrm{ ft^3/min}$$ So, my answer: $25 \pi \textrm{ ft^3/min}.$ The book's answer: $\frac{25 \pi}{12} \textrm{ ft^3/min}.$","A tank is in the form of a cone with the point downward, and the height and diameter are each 10 feet. How fast is the water pouring in at the moment when it is 5 feet deep and the surface is rising at the rate of 4 feet per minute? The volume of a cone is Given that the diameter of the cone is equal to its height, the volume of the cone may be written So, So, my answer: The book's answer:",V_c = \frac{1}{12} \pi D^2 H. V_c = \frac{1}{12} \pi H^3. \frac{dV}{dt} = \frac{1}{4} \pi H^2 \frac{dH}{dt} = \frac{1}{4} \pi (5)^2\times 4 = 25 \pi \textrm{ ft^3/min} 25 \pi \textrm{ ft^3/min}. \frac{25 \pi}{12} \textrm{ ft^3/min}.,"['calculus', 'derivatives', 'related-rates']"
8,Variations Calculus. How to compute some variations?,Variations Calculus. How to compute some variations?,,"I have the following situation: I'm trying to prove last point: the metric is invariant to re-parameterization of $f$ . So, the left member can be rewritten in the following way: $$a^2 \int^{1}_{0}{\delta((r_{1}\circ \gamma)\dot{\gamma})\delta((r_{2}\circ \gamma)\dot{\gamma})\frac{1}{r(t)}\text{d}t}+b^{2}\int^{1}_{0}{\delta(\Theta_{1} \circ \gamma)\delta(\Theta_{2} \circ \gamma)r(t) \text{d}t}$$ and i tried $\delta((r_1 \circ \gamma)\dot{\gamma})=\delta(r_1 \circ \gamma)\dot{\gamma}+(r_1 \circ \gamma)\delta(\dot{\gamma})$ ? I am thinking is not OK. Any help it will be appreciated.","I have the following situation: I'm trying to prove last point: the metric is invariant to re-parameterization of . So, the left member can be rewritten in the following way: and i tried ? I am thinking is not OK. Any help it will be appreciated.","f a^2 \int^{1}_{0}{\delta((r_{1}\circ \gamma)\dot{\gamma})\delta((r_{2}\circ \gamma)\dot{\gamma})\frac{1}{r(t)}\text{d}t}+b^{2}\int^{1}_{0}{\delta(\Theta_{1} \circ \gamma)\delta(\Theta_{2} \circ \gamma)r(t)
\text{d}t} \delta((r_1 \circ \gamma)\dot{\gamma})=\delta(r_1 \circ \gamma)\dot{\gamma}+(r_1 \circ \gamma)\delta(\dot{\gamma})","['integration', 'analysis', 'derivatives', 'metric-spaces', 'calculus-of-variations']"
9,From binomial theorem to differential calculus,From binomial theorem to differential calculus,,"I first posted this over at HSM, without much uptake. I'm trying to understand the development of the calculus. Does this sound plausible as one of the stages? Newton knows the binomial theorem, which gives $$(x+y)^n={n\choose0}x^ny^0+...\;\;\;\;\;\;\;\;\;\;(1)$$ Letting $y=δ_x$ (I realise this is Leizbniz's notation but I find it easier to follow than Newton's $o$ ), we get $$(x+δ_x)^n={n\choose0}x^nδ_x^0+...\;\;\;\;\;\;\;\;\;\;(2)$$ Considering $δ_x$ as the base of a differential triangle under a curve, the vertical of the triangle is given by $(x+δ_x)^n-x^n$ , which gives us $$(x+δ_x)^n-x^n={n\choose0}x^nδ_x^0+...-x^n\;\;\;\;\;\;\;\;\;\;(3)$$ But ${n\choose0}x^nδ_x^0=x^n$ , so the first part of the expansion disappears and everything else moves up one place to the left and we get $$(x+δ_x)^n-x^n={n\choose1}x^{n-1}δ_x^1+...\;\;\;\;\;\;\;\;\;\;(4)$$ Now, the vertical $(x+δ_x)^n-x^n$ can be called $δ_y$ , so now we can write $$\frac{δ_y}{δ_x}=\frac{(x+δ_x)^n-x^n}{δ_x}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\\ = \frac{1}{δ_x}{n\choose1}x^{n-1}δ_x^1+...\;\;\;\;\;\;\;\;\;\;(5)$$ As $δ_x$ gets small, the gradient approaches instantaneity, which is good, but $\frac{1}{δ_x}$ gets large, which seems problematic. Fortunately, it cancels with the $δ_x$ terms in the expansion to give $$\frac{δ_y}{δ_x}={n\choose1}x^{n-1}+{n\choose2}x^{n-2}δ_x^1+...\;\;\;\;\;\;\;\;\;\;(6)$$ And now we can use the biniomial theorem to make a differential triangle under any polynomial. Sound right?","I first posted this over at HSM, without much uptake. I'm trying to understand the development of the calculus. Does this sound plausible as one of the stages? Newton knows the binomial theorem, which gives Letting (I realise this is Leizbniz's notation but I find it easier to follow than Newton's ), we get Considering as the base of a differential triangle under a curve, the vertical of the triangle is given by , which gives us But , so the first part of the expansion disappears and everything else moves up one place to the left and we get Now, the vertical can be called , so now we can write As gets small, the gradient approaches instantaneity, which is good, but gets large, which seems problematic. Fortunately, it cancels with the terms in the expansion to give And now we can use the biniomial theorem to make a differential triangle under any polynomial. Sound right?",(x+y)^n={n\choose0}x^ny^0+...\;\;\;\;\;\;\;\;\;\;(1) y=δ_x o (x+δ_x)^n={n\choose0}x^nδ_x^0+...\;\;\;\;\;\;\;\;\;\;(2) δ_x (x+δ_x)^n-x^n (x+δ_x)^n-x^n={n\choose0}x^nδ_x^0+...-x^n\;\;\;\;\;\;\;\;\;\;(3) {n\choose0}x^nδ_x^0=x^n (x+δ_x)^n-x^n={n\choose1}x^{n-1}δ_x^1+...\;\;\;\;\;\;\;\;\;\;(4) (x+δ_x)^n-x^n δ_y \frac{δ_y}{δ_x}=\frac{(x+δ_x)^n-x^n}{δ_x}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\\ = \frac{1}{δ_x}{n\choose1}x^{n-1}δ_x^1+...\;\;\;\;\;\;\;\;\;\;(5) δ_x \frac{1}{δ_x} δ_x \frac{δ_y}{δ_x}={n\choose1}x^{n-1}+{n\choose2}x^{n-2}δ_x^1+...\;\;\;\;\;\;\;\;\;\;(6),"['calculus', 'derivatives', 'math-history', 'binomial-theorem']"
10,What's the meaning of a $d$ with a stroke when using Leibniz notation?,What's the meaning of a  with a stroke when using Leibniz notation?,d,"I'm studying engineering and there's a physics teacher that strikethroughs the derivative $d$ when writing an expression for power (which is work over time). I know physics teachers are known to abuse mathematical notation, but this intrigued me as I had never seen it used and couldn't find anything online. So the expression she writes is: $đW/dt = ...$ What does it mean for the $d$ to be struck like this?","I'm studying engineering and there's a physics teacher that strikethroughs the derivative when writing an expression for power (which is work over time). I know physics teachers are known to abuse mathematical notation, but this intrigued me as I had never seen it used and couldn't find anything online. So the expression she writes is: What does it mean for the to be struck like this?",d đW/dt = ... d,"['calculus', 'derivatives', 'notation', 'physics']"
11,Finding the Gradient Matrix for the given expression,Finding the Gradient Matrix for the given expression,,"Let $\rho$ be a matrix, and let $\rho_A$ be the partial trace of the matrix $\rho$ . For simplicity, let us assume $\rho$ is a $4 \times 4$ matrix. The partial trace is defined as follows: If $$\rho = \begin{bmatrix}a & b & c & d \\ e & f & g & h \\ i & j & k & l \\ m & n & o & p \end{bmatrix}$$ Then $$\rho_A = \begin{bmatrix} a+f & c+h \\i+n & k + p \end{bmatrix}$$ I would like to calculate the following: $$\nabla_\rho (Tr(\rho log \rho) - Tr(\rho_A log \rho_A))$$ which is the derivative of a continuous scalar quantity with respect to a matrix, so I should be able to calculate it right? Note that each of the entries are complex. I tried a few things, but I'm not sure whether chain rule works, because you end up getting gradients of matrices with respect to matrices, which is not defined? Any help on how to go about such a thing would be very appreciated.","Let be a matrix, and let be the partial trace of the matrix . For simplicity, let us assume is a matrix. The partial trace is defined as follows: If Then I would like to calculate the following: which is the derivative of a continuous scalar quantity with respect to a matrix, so I should be able to calculate it right? Note that each of the entries are complex. I tried a few things, but I'm not sure whether chain rule works, because you end up getting gradients of matrices with respect to matrices, which is not defined? Any help on how to go about such a thing would be very appreciated.","\rho \rho_A \rho \rho 4 \times 4 \rho = \begin{bmatrix}a & b & c & d \\
e & f & g & h \\
i & j & k & l \\
m & n & o & p \end{bmatrix} \rho_A = \begin{bmatrix} a+f & c+h \\i+n & k + p \end{bmatrix} \nabla_\rho (Tr(\rho log \rho) - Tr(\rho_A log \rho_A))","['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'quantum-information']"
12,"$f$ has $n$-order derivative for any $n$ and for each $x$, $ \mid f^{(n)}(x) - f^{(n-1)}(x) \mid < \frac{1}{n^2} $ to show its limit.","has -order derivative for any  and for each ,  to show its limit.",f n n x  \mid f^{(n)}(x) - f^{(n-1)}(x) \mid < \frac{1}{n^2} ,"$f$ is defined in $\mathbb{R}$ , has $n$ -order derivative for any $n$ , and for each $x$ , $$ \mid f^{(n)}(x) - f^{(n-1)}(x) \mid < \frac{1}{n^2} $$ show that $\lim_{n \to \infty} f^{(n)}(x) = c \exp(x)$ with constant $c$ . as $n \to \infty$ , the $f^{(n)}(x) - f^{(n-1)}(x)$ trends to $0$ , it is quite natural to say one of the limits of $f(x)$ is $ c \exp(x)$ intuitively, but I don't know how to exclude the rest and put it mathematically, can someone provide a clue.","is defined in , has -order derivative for any , and for each , show that with constant . as , the trends to , it is quite natural to say one of the limits of is intuitively, but I don't know how to exclude the rest and put it mathematically, can someone provide a clue.",f \mathbb{R} n n x  \mid f^{(n)}(x) - f^{(n-1)}(x) \mid < \frac{1}{n^2}  \lim_{n \to \infty} f^{(n)}(x) = c \exp(x) c n \to \infty f^{(n)}(x) - f^{(n-1)}(x) 0 f(x)  c \exp(x),"['real-analysis', 'derivatives']"
13,Why is the derivative of scalar with respect to vector a vector and not a scalar?,Why is the derivative of scalar with respect to vector a vector and not a scalar?,,"I'm really confused about matrix calculus and especially partial derivatives. When do we need to sum up partial derivatives to get a total derivative and when do we get a vector of partial derivatives as our derivative? I struggle with differentiating between the two. I'll make an example to make it clear: L is a scalar, $\mathbf{o}$ is a vector of size $K$ and $\mathbf{y}$ is a vector of size $K$ . $$L = -\sum_{k} \log(y_k)$$ $$\mathbf{y} = \text{softmax}(\mathbf{o})$$ So if we want to have the derivative of L with respect to $\mathbf{o}$ , we would need to sum over all the partial derivatives with respect to the terms $\mathbf{y}$ so that we get the total derivative, that is as much as I understood from reading about multivariate calculus: $$\frac{\partial L}{\partial \mathbf{o}} = \frac{\partial L}{\partial \mathbf{y}}\frac{\partial \mathbf{y}}{\partial \mathbf{o}} = \sum_{k}\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial \mathbf{o}} =  -\sum_{k} \frac{1}{y_k} \frac{\partial y_k}{\partial \mathbf{o}}$$ However, then $\frac{\partial L}{\partial \mathbf{o}}$ seems to be a vector of the partial derivatives of L with respect to every term of $\mathbf{o}$ , i.e.: $$ \frac{\partial L}{\partial \mathbf{o}} = \left< \frac{\partial L}{\partial o_1}, \frac{\partial L}{\partial o_2}, ..., \frac{\partial L}{\partial o_K} \right> $$ But shouldn't the derivative be the sum of all the partial derivatives of $\mathbf{o}$ to get the total derivative? i.e. shouldn't the solution be: $$\frac{\partial L}{\partial \mathbf{o}} = \frac{\partial L}{\partial \mathbf{y}}\frac{\partial \mathbf{y}}{\partial \mathbf{o}} = -\sum_{k} \frac{1}{y_k} \sum_{i} \frac{\partial y_k}{\partial o_i}$$ and then its just a scalar?","I'm really confused about matrix calculus and especially partial derivatives. When do we need to sum up partial derivatives to get a total derivative and when do we get a vector of partial derivatives as our derivative? I struggle with differentiating between the two. I'll make an example to make it clear: L is a scalar, is a vector of size and is a vector of size . So if we want to have the derivative of L with respect to , we would need to sum over all the partial derivatives with respect to the terms so that we get the total derivative, that is as much as I understood from reading about multivariate calculus: However, then seems to be a vector of the partial derivatives of L with respect to every term of , i.e.: But shouldn't the derivative be the sum of all the partial derivatives of to get the total derivative? i.e. shouldn't the solution be: and then its just a scalar?","\mathbf{o} K \mathbf{y} K L = -\sum_{k} \log(y_k) \mathbf{y} = \text{softmax}(\mathbf{o}) \mathbf{o} \mathbf{y} \frac{\partial L}{\partial \mathbf{o}} = \frac{\partial L}{\partial \mathbf{y}}\frac{\partial \mathbf{y}}{\partial \mathbf{o}} = \sum_{k}\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial \mathbf{o}} = 
-\sum_{k} \frac{1}{y_k} \frac{\partial y_k}{\partial \mathbf{o}} \frac{\partial L}{\partial \mathbf{o}} \mathbf{o}  \frac{\partial L}{\partial \mathbf{o}} = \left< \frac{\partial L}{\partial o_1}, \frac{\partial L}{\partial o_2}, ..., \frac{\partial L}{\partial o_K} \right>  \mathbf{o} \frac{\partial L}{\partial \mathbf{o}} = \frac{\partial L}{\partial \mathbf{y}}\frac{\partial \mathbf{y}}{\partial \mathbf{o}} = -\sum_{k} \frac{1}{y_k} \sum_{i} \frac{\partial y_k}{\partial o_i}","['calculus', 'derivatives', 'partial-derivative']"
14,.derivative of ceil(x) proof and explanation,.derivative of ceil(x) proof and explanation,,I know that the derivative is 0 for all x but can somebody explain this to me. I understand derivatives but I have been having a lot of trouble here. I know that $ceil(x)=(x+1/2) - (arctan(tan(pi*(x+1/2))))/(pi)$ for all non integer x so I differentiated that on wolfram alpha https://www.wolframalpha.com/input/?i=derivative+of+(x%2B1%2F2)+-+(arctan(tan(pi(x%2B1%2F2))))%2Fpi and got $1-(csc^2(pi*x)/(cot^2(pi*x)+1)$ and using the desmos graphing calculator to graph this I got https://www.desmos.com/calculator/mmzdqg6nba which is 0 for all non-integer x. can anybody show me an easier way to solve this problem.,I know that the derivative is 0 for all x but can somebody explain this to me. I understand derivatives but I have been having a lot of trouble here. I know that for all non integer x so I differentiated that on wolfram alpha https://www.wolframalpha.com/input/?i=derivative+of+(x%2B1%2F2)+-+(arctan(tan(pi(x%2B1%2F2))))%2Fpi and got and using the desmos graphing calculator to graph this I got https://www.desmos.com/calculator/mmzdqg6nba which is 0 for all non-integer x. can anybody show me an easier way to solve this problem.,ceil(x)=(x+1/2) - (arctan(tan(pi*(x+1/2))))/(pi) 1-(csc^2(pi*x)/(cot^2(pi*x)+1),['derivatives']
15,Sufficient condition(s) for $f'$ to be Riemann integrable,Sufficient condition(s) for  to be Riemann integrable,f',"Let $f:[0,1]\to \mathbb{R}$ be differentiable on $[0,1]$ . We know that the derivative of $f$ denoted by $f'$ may not be Riemann integrable on $[0,1]$ . We know that a necessary condition for $f'$ to be Riemann integrable is boundedness of $f'$ on $[0,1]$ . My question: Is boundedness of $f'$ on $[0,1]$ also a sufficient condition? What are other sufficient conditions of $f'$ to be Riemann integratble on $[0,1]$ ?",Let be differentiable on . We know that the derivative of denoted by may not be Riemann integrable on . We know that a necessary condition for to be Riemann integrable is boundedness of on . My question: Is boundedness of on also a sufficient condition? What are other sufficient conditions of to be Riemann integratble on ?,"f:[0,1]\to \mathbb{R} [0,1] f f' [0,1] f' f' [0,1] f' [0,1] f' [0,1]","['real-analysis', 'derivatives', 'riemann-integration']"
16,Proving Differentiability of a Function (Spivak),Proving Differentiability of a Function (Spivak),,"The following is an exercise from Calculus on Manifolds by Spivak. Let $g$ be a continuous, real-valued function on the unit circle $\{x\in\mathbb{R}^2:\|x\|=1\}$ , such that $g(-x)=-g(x)$ .   Now define $f:\mathbb{R}^2\to\mathbb{R}$ by $$f(x) =\begin{cases} \|x\|\cdot g\Big(\frac{x}{\|x\|}\Big) & x\neq 0 \\ 0 & x=0 \end{cases}$$ Fix $x\in\mathbb{R}^2$ , and further define a function $h:\mathbb{R}\to\mathbb{R}$ by $h(t)=f(tx)$ .    Show that $h$ is differentiable. My attempt: Observe that for $t\neq 0$ , we have \begin{align*} h(t)=f(tx) & = \|tx\|\cdot g\Bigg(\frac{tx}{\|tx\|}\Bigg) \\ & = t\|x\|\cdot g\Bigg(\frac{tx}{t\|x\|}\Bigg) \\ & = t\|x\|\cdot g\Bigg(\frac{x}{\|x\|}\Bigg) \\ & = tf(x) \end{align*} In the case $t=0$ , we have $$h(0) = f(0) = 0$$ Thus, $h(t)=tf(x)$ for all $t\in\mathbb{R}$ . A function $f:\mathbb{R}\to\mathbb{R}$ is differentiable at a point $a\in\mathbb{R}$ if there exists a linear transformation $\lambda(h):\mathbb{R}\to\mathbb{R}$ such that $$\lim_{h\to 0}\frac{f(a+h)-f(a)-\lambda(h)}{h}=0$$ Thus $h$ is differentiable if for all $t\in\mathbb{R}$ , there exists a transformation $Dh(k):\mathbb{R}\to\mathbb{R}$ such that $$\lim_{k\to 0}\frac{h(t+k)-h(t)-Dh(k)}{k}=0$$ We then have \begin{align*} \lim_{k\to 0}\frac{h(t+k)-h(t)-Dh(k)}{k} & =\lim_{k\to 0}\frac{(t+k)f(x)-tf(x)-Dh(k)}{k} \\ & = \lim_{k\to 0}\frac{kf(x)-Dh(k)}{k} \end{align*} Thus the natural choice for $Dh(k)$ is $kf(x)$ .  This completes the proof. Questions: Does this proof work? How do I know that $Dh(k)=kf(x)$ is a linear transformation?","The following is an exercise from Calculus on Manifolds by Spivak. Let be a continuous, real-valued function on the unit circle , such that .   Now define by Fix , and further define a function by .    Show that is differentiable. My attempt: Observe that for , we have In the case , we have Thus, for all . A function is differentiable at a point if there exists a linear transformation such that Thus is differentiable if for all , there exists a transformation such that We then have Thus the natural choice for is .  This completes the proof. Questions: Does this proof work? How do I know that is a linear transformation?","g \{x\in\mathbb{R}^2:\|x\|=1\} g(-x)=-g(x) f:\mathbb{R}^2\to\mathbb{R} f(x) =\begin{cases}
\|x\|\cdot g\Big(\frac{x}{\|x\|}\Big) & x\neq 0
\\ 0 & x=0
\end{cases} x\in\mathbb{R}^2 h:\mathbb{R}\to\mathbb{R} h(t)=f(tx) h t\neq 0 \begin{align*}
h(t)=f(tx) & = \|tx\|\cdot g\Bigg(\frac{tx}{\|tx\|}\Bigg)
\\ & = t\|x\|\cdot g\Bigg(\frac{tx}{t\|x\|}\Bigg)
\\ & = t\|x\|\cdot g\Bigg(\frac{x}{\|x\|}\Bigg)
\\ & = tf(x)
\end{align*} t=0 h(0) = f(0) = 0 h(t)=tf(x) t\in\mathbb{R} f:\mathbb{R}\to\mathbb{R} a\in\mathbb{R} \lambda(h):\mathbb{R}\to\mathbb{R} \lim_{h\to 0}\frac{f(a+h)-f(a)-\lambda(h)}{h}=0 h t\in\mathbb{R} Dh(k):\mathbb{R}\to\mathbb{R} \lim_{k\to 0}\frac{h(t+k)-h(t)-Dh(k)}{k}=0 \begin{align*}
\lim_{k\to 0}\frac{h(t+k)-h(t)-Dh(k)}{k} & =\lim_{k\to 0}\frac{(t+k)f(x)-tf(x)-Dh(k)}{k}
\\ & = \lim_{k\to 0}\frac{kf(x)-Dh(k)}{k}
\end{align*} Dh(k) kf(x) Dh(k)=kf(x)","['real-analysis', 'calculus', 'proof-verification', 'derivatives', 'proof-writing']"
17,Derivative of log of multivariate Normal respect to Matrix?,Derivative of log of multivariate Normal respect to Matrix?,,I am wondering how to calculate the gradient of a multivariate Normal respect to L where $\Sigma=LL^{T}$ . I know what would be the derivative respect to $\Sigma$ and I can use chain rule to get the derivative respect to L but since $\Sigma$ is a matrix and L is matrix then derivative of a matrix respect to matrix is 4D tensor. Therefore I am wondering can anyone help to find the derivative of log|L| $-(x-z)L^{-1}L^{-T}(x-z)^{T}$ respect to L. The first term involving the determinant is a standard calculation but I am stuck  in the second term.,I am wondering how to calculate the gradient of a multivariate Normal respect to L where . I know what would be the derivative respect to and I can use chain rule to get the derivative respect to L but since is a matrix and L is matrix then derivative of a matrix respect to matrix is 4D tensor. Therefore I am wondering can anyone help to find the derivative of log|L| respect to L. The first term involving the determinant is a standard calculation but I am stuck  in the second term.,\Sigma=LL^{T} \Sigma \Sigma -(x-z)L^{-1}L^{-T}(x-z)^{T},"['calculus', 'matrices', 'derivatives', 'normal-distribution', 'vector-analysis']"
18,Prove that there is no $a\in \mathbb R$ so that $f(x)= e^x(x^2+a) $ has only one extremum,Prove that there is no  so that  has only one extremum,a\in \mathbb R f(x)= e^x(x^2+a) ,"As to my understanding in order to calculate the behavior of the extrema of a function, a good approach is to examine the behavior of the first derivative of the given function. So by calculating the derivative I got: $$ f'(x)=(e^x(x^2+a))'=e^x(x^2+2x+a)$$ now in order to examine the behavior of $f(x)$ I examine where $f'(x)$ is positive or negative. And at the points where $f'(x)=0$ there will be an extremum. So because $$ e^x >0 \space \forall x\in \mathbb R $$ whether $f'(x)$ is negative or positive is defined by the polynom $x^2 +2x+a$ Then by using the quadratic formula I found that for the discriminant $\Delta$ the following are true. For $a<1$ it's $\Delta > 0 $ that means that for $a \in(-\infty,1)$ , $f(x)$ has two extrema because the polynom has two roots and therefore the monotony of $f(x)$ changes twice. And for $ a > 1$ it's $\Delta<0 $ , the polynom has no real roots and therefore for $a\in(1,+\infty)$ , $f(x)$ shows no change in its monotony and as a result it has $0$ extrema. For the special case of $\Delta = 0$ the polynom is as well everywhere positive and therefore once again $f(x)$ has not extrema. So there is no value for $a$ so that $f(x)$ will have only one extremum. The thing is that just examining that way doesn't seem that ""mathematically"" correct to me and I wonder if there is any better way to examine and actually prove that there is no $a$ so that $f(x)$ has only one extremum.","As to my understanding in order to calculate the behavior of the extrema of a function, a good approach is to examine the behavior of the first derivative of the given function. So by calculating the derivative I got: now in order to examine the behavior of I examine where is positive or negative. And at the points where there will be an extremum. So because whether is negative or positive is defined by the polynom Then by using the quadratic formula I found that for the discriminant the following are true. For it's that means that for , has two extrema because the polynom has two roots and therefore the monotony of changes twice. And for it's , the polynom has no real roots and therefore for , shows no change in its monotony and as a result it has extrema. For the special case of the polynom is as well everywhere positive and therefore once again has not extrema. So there is no value for so that will have only one extremum. The thing is that just examining that way doesn't seem that ""mathematically"" correct to me and I wonder if there is any better way to examine and actually prove that there is no so that has only one extremum."," f'(x)=(e^x(x^2+a))'=e^x(x^2+2x+a) f(x) f'(x) f'(x)=0  e^x >0 \space \forall x\in \mathbb R  f'(x) x^2 +2x+a \Delta a<1 \Delta > 0  a \in(-\infty,1) f(x) f(x)  a > 1 \Delta<0  a\in(1,+\infty) f(x) 0 \Delta = 0 f(x) a f(x) a f(x)","['real-analysis', 'derivatives', 'polynomials', 'maxima-minima']"
19,How is the derivative of $x\cos x\sin x$ $\frac12\sin2x + x\cos2x$?,How is the derivative of  ?,x\cos x\sin x \frac12\sin2x + x\cos2x,"Find $f'(x)$ when $f(x) = x\cos(x) \sin(x)$ I tried using the product rule on $x\cos x$ and got: $$(x\cos x)' = -x\sin x + \cos x$$ then I used the product rule to differentiate $x\cos x\sin x$ and got: $$(x\cos x \sin x)' = (-x \sin x + \cos x)\sin x+ (x\cos x \cos x)= -x\sin^2x+ \sin x \cos x + x\cos^2x$$ My testbook says the answer is $\frac12\sin2x+ x\cos2x$ Can someone please explain how to achieve the answer shown in my textbook? I am new to calc and trig, and I'm very confused.  Any help is appreciated.","Find when I tried using the product rule on and got: then I used the product rule to differentiate and got: My testbook says the answer is Can someone please explain how to achieve the answer shown in my textbook? I am new to calc and trig, and I'm very confused.  Any help is appreciated.",f'(x) f(x) = x\cos(x) \sin(x) x\cos x (x\cos x)' = -x\sin x + \cos x x\cos x\sin x (x\cos x \sin x)' = (-x \sin x + \cos x)\sin x+ (x\cos x \cos x)= -x\sin^2x+ \sin x \cos x + x\cos^2x \frac12\sin2x+ x\cos2x,"['calculus', 'derivatives', 'trigonometry']"
20,Proof that $f$ is differentiable,Proof that  is differentiable,f,"Let $$f(x) = \sum_{n=1}^{\infty} \frac{x^n}{2^n} \cos{nx}$$ Proof that $f$ is differentiable on $(-2,2)$ my approach let $ m := \frac{x}{2} $ so $m<1$ $$ \left| \frac{x^n}{2^n} \cos{nx}   \right| \le m^n \rightarrow 0 $$ So by comparison we have point convergence. But I don't know how to deal with diffentiable..",Let Proof that is differentiable on my approach let so So by comparison we have point convergence. But I don't know how to deal with diffentiable..,"f(x) = \sum_{n=1}^{\infty} \frac{x^n}{2^n} \cos{nx} f (-2,2)  m := \frac{x}{2}  m<1  \left| \frac{x^n}{2^n} \cos{nx}   \right| \le m^n \rightarrow 0 ","['real-analysis', 'derivatives', 'convergence-divergence', 'summation']"
21,"""Easy"" way to derive Black-Scholes delta","""Easy"" way to derive Black-Scholes delta",,"I was always amazed that the Black-Scholes delta i.e. the following expression: $$\frac{\partial}{\partial S}\left[ S\cdot \Phi\left(\frac{log \left(\frac{S}{K}\right)+(r+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}} \right)-Ke^{-rT} \cdot \Phi\left(\frac{log \left(\frac{S}{K}\right)-(r+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}} \right)\right]$$ Is just equal to the value of the CDF on the left: $$\Phi\left(\frac{log \left(\frac{S}{K}\right)+(r+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}} \right)$$ And similarly for derivative with respect to $K$ despite the fact that $S$ and $K$ appear in the middle of both of them. Recently I've stumbled upon a paper which states that this fact follows from Euler's theorem as the expression is homogeneous of degree $1$ as a function of $S$ and $K$ and therefore can be written as a linear combination of its partial derivatives: $$f(S,K)=S \cdot \frac{\partial f}{\partial S}+K \cdot \frac{\partial f}{\partial K}$$ I see how this makes the above result much more reasonable but I don't see how it actually proves this fact. Imagine for example the function $f(S,K)=S+K$ . We have: $$f(S,K)=S\cdot \frac{K}{S}+K\cdot \frac{S}{K}$$ And yet $\frac{\partial f}{\partial S}=1 \neq \frac{K}{S}$ as well as $\frac{\partial f}{\partial K}=1 \neq \frac{S}{K}$ . Can this reasoning be fixed somehow, so that it is possible to compute this derivative without angaging in messy calculations?","I was always amazed that the Black-Scholes delta i.e. the following expression: Is just equal to the value of the CDF on the left: And similarly for derivative with respect to despite the fact that and appear in the middle of both of them. Recently I've stumbled upon a paper which states that this fact follows from Euler's theorem as the expression is homogeneous of degree as a function of and and therefore can be written as a linear combination of its partial derivatives: I see how this makes the above result much more reasonable but I don't see how it actually proves this fact. Imagine for example the function . We have: And yet as well as . Can this reasoning be fixed somehow, so that it is possible to compute this derivative without angaging in messy calculations?","\frac{\partial}{\partial S}\left[ S\cdot \Phi\left(\frac{log \left(\frac{S}{K}\right)+(r+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}} \right)-Ke^{-rT} \cdot \Phi\left(\frac{log \left(\frac{S}{K}\right)-(r+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}} \right)\right] \Phi\left(\frac{log \left(\frac{S}{K}\right)+(r+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}} \right) K S K 1 S K f(S,K)=S \cdot \frac{\partial f}{\partial S}+K \cdot \frac{\partial f}{\partial K} f(S,K)=S+K f(S,K)=S\cdot \frac{K}{S}+K\cdot \frac{S}{K} \frac{\partial f}{\partial S}=1 \neq \frac{K}{S} \frac{\partial f}{\partial K}=1 \neq \frac{S}{K}","['calculus', 'analysis', 'derivatives', 'partial-derivative']"
22,Derivative at point but point itself converges,Derivative at point but point itself converges,,"Say, that $\lim_{x\to 0} f(x)=1$ anď $\lim_{x\to 0} \frac{f(x)-1}{x} = 1$ then we get $\lim_{x\to 0} f'(x)=1$ Is this an obvious conclusion? I think there is one omitted thing for this approach. $$\lim_{x\to 0, \tau\to 0} \frac{f(x)-f(\tau)}{x-\tau} = \lim_{x\to 0} f'(x)$$ Am I right? I don't know how to prove this since I haven't learned higher math but naive definition of derivative with graph (slope of tangent line)","Say, that anď then we get Is this an obvious conclusion? I think there is one omitted thing for this approach. Am I right? I don't know how to prove this since I haven't learned higher math but naive definition of derivative with graph (slope of tangent line)","\lim_{x\to 0} f(x)=1 \lim_{x\to 0} \frac{f(x)-1}{x} = 1 \lim_{x\to 0} f'(x)=1 \lim_{x\to 0, \tau\to 0} \frac{f(x)-f(\tau)}{x-\tau} = \lim_{x\to 0} f'(x)","['calculus', 'limits', 'derivatives']"
23,How to calculate $\frac{\partial^2}{\partial \theta^2} \bigg(\frac{1}{n} \sum_{i=1}^{n} \exp(-Y_i (\theta^T X_i))\bigg)$?,How to calculate ?,\frac{\partial^2}{\partial \theta^2} \bigg(\frac{1}{n} \sum_{i=1}^{n} \exp(-Y_i (\theta^T X_i))\bigg),"I have a learning sample $D_n = f(X_i, Y_i)_{i=1}^n$ where the $X_i$ ’s are $\mathbb{R}^d$ -valued and the $Y_i$ ’s are $\{-1, 1\}$ -valued. $$ f(\theta) = \frac{1}{n} \sum_{i=1}^{n} \exp(-Y_i (\theta^T X_i)) $$ Where $\theta \in [-B, +B]$ . I want to calculate the Hessian matrix: $\nabla^2 f(\theta)$ .",I have a learning sample where the ’s are -valued and the ’s are -valued. Where . I want to calculate the Hessian matrix: .,"D_n = f(X_i, Y_i)_{i=1}^n X_i \mathbb{R}^d Y_i \{-1, 1\} 
f(\theta) = \frac{1}{n} \sum_{i=1}^{n} \exp(-Y_i (\theta^T X_i))
 \theta \in [-B, +B] \nabla^2 f(\theta)","['derivatives', 'matrix-calculus', 'hessian-matrix']"
24,"Proving $C_{0} + C_1x +\cdots+ C_{n}x^{n} = 0$ has at least one real root in $(0,1)$ [duplicate]",Proving  has at least one real root in  [duplicate],"C_{0} + C_1x +\cdots+ C_{n}x^{n} = 0 (0,1)","This question already has answers here : Prove existence of a real root. (1 answer) Prove that the equation: $c_0+c_1x+\ldots+c_nx^n=0$ has a real solution between 0 and 1. [duplicate] (2 answers) Closed 5 years ago . If $$C_{0} + \frac{C_1}{2} + \cdots+ \frac{C_{n-1}}{n} + \frac{C_{n}}{n+1} = 0\,,$$ prove that $$C_{0} + C_{1}x + \cdots+ C_{n-1}x^{n-1} + C_{n}x^{n} = 0$$ has at least one real root between $0$ and $1$ . I know how to prove the result by using the mean value theorem, but I am not understanding how the result from the mean value theorem allows us to conclude the final result. What I am asking is how does the existence of an $x \in (0,1)$ such that $f'(x) = 0$ mean that there exists a real root between $(0,1)$ ? All I can conclude from that is that there is an $x \in (0,1)$ such that the slope is the same as a secant line from the endpoints.","This question already has answers here : Prove existence of a real root. (1 answer) Prove that the equation: $c_0+c_1x+\ldots+c_nx^n=0$ has a real solution between 0 and 1. [duplicate] (2 answers) Closed 5 years ago . If prove that has at least one real root between and . I know how to prove the result by using the mean value theorem, but I am not understanding how the result from the mean value theorem allows us to conclude the final result. What I am asking is how does the existence of an such that mean that there exists a real root between ? All I can conclude from that is that there is an such that the slope is the same as a secant line from the endpoints.","C_{0} + \frac{C_1}{2} + \cdots+ \frac{C_{n-1}}{n} + \frac{C_{n}}{n+1} = 0\,, C_{0} + C_{1}x + \cdots+ C_{n-1}x^{n-1} + C_{n}x^{n} = 0 0 1 x \in (0,1) f'(x) = 0 (0,1) x \in (0,1)","['real-analysis', 'calculus', 'derivatives']"
25,Derivative of matrix as a function of a vector w.r.t a vector,Derivative of matrix as a function of a vector w.r.t a vector,,"I want to compute the derivative of the matrix $ diag(x)M $ with respect to $ x $ , where $ x \in \mathbb{C}^{n \times 1} $ and $ M \in \mathbb{C}^{n \times m} $ . This is how I have approached it, but I have not been successful. First, $$ Y = diag(x) $$ Then, $$ Z = Y M  $$ The differential of $ Z $ is $$    dZ = dY M $$ If I am not mistaken $ dY = (I_{n \times n} \otimes 1_{n \times 1}) dx $ . So $$    dZ = (I_{n \times n} \otimes 1_{n \times 1}) (dx) M $$ But the dimensions do not make much sense in this last expression. Would you please help me to find the right way?","I want to compute the derivative of the matrix with respect to , where and . This is how I have approached it, but I have not been successful. First, Then, The differential of is If I am not mistaken . So But the dimensions do not make much sense in this last expression. Would you please help me to find the right way?"," diag(x)M   x   x \in \mathbb{C}^{n \times 1}   M \in \mathbb{C}^{n \times m}   Y = diag(x)   Z = Y M    Z  
   dZ = dY M
  dY = (I_{n \times n} \otimes 1_{n \times 1}) dx  
   dZ = (I_{n \times n} \otimes 1_{n \times 1}) (dx) M
","['calculus', 'linear-algebra', 'matrices', 'derivatives']"
26,Derivatives without limits,Derivatives without limits,,"Update: H/t David K for pointing out that my assumption that I can force $a^2+b^2=r^2$ is wrong. This led to analyzing a cubic equation which is now moot, but I think the bulk of the question remains sensible. I think I'm making some mistakes in the below, but I'm not sure where. Even if there are mistakes, I'm thinking there might not be any practical point to it, but I'm curious. I think it's possible to find the slope of a tangent line without using limits in some cases. Further it's possible to generalize the procedure to many common curves. I am having trouble with a proof of this and I'm not sure where I am going wrong. Proof we can find the slope of a tangent line to a parabola without taking a limit: First we consider a circle. If we know the center of a circle and we know a point on that circle, we know the equation of a line connecting the center to that point. The slope of the line perpendicular to this line is the slope of the tangent line to the circle at that point. So we have already proven the thesis in the case of a circle. A circle is tangent to some other curve if it only intersects that curve at a single point. A circle can be constructed to be tangent to certain points on certain curves. Tangent circles share tangent lines with the curves to which they are tangent. So the procedure above can find tangents to those curves by constructing the correct tangent circle. Parabola case: Consider $y=x^2$ . We want a circle such that $(x-a)^2+(x^2-b)^2=r^2$ So: $$x^4+(1-2b)x^2-2ax+(a^2+b^2-r^2)=0$$ We either have 2 complex roots or no real solutions. Assuming we have any real solutions, having a unique one requires that we have a repeated root. So to find the tangent to the curve at $x_0$ we want to divide the quartic by $(x-x_0)^2$ . We'd want to have the remainder terms be zero and the discriminant of the resulting quotient to be negative. EDIT: Turns out Descartes developed this method centuries ago bearing some fruit. It's algebraically more intense than Newton's and Leibniz's approach to finding slopes of tangent lines, so fell by the wayside. It's called Descartes' Method of Normals","Update: H/t David K for pointing out that my assumption that I can force is wrong. This led to analyzing a cubic equation which is now moot, but I think the bulk of the question remains sensible. I think I'm making some mistakes in the below, but I'm not sure where. Even if there are mistakes, I'm thinking there might not be any practical point to it, but I'm curious. I think it's possible to find the slope of a tangent line without using limits in some cases. Further it's possible to generalize the procedure to many common curves. I am having trouble with a proof of this and I'm not sure where I am going wrong. Proof we can find the slope of a tangent line to a parabola without taking a limit: First we consider a circle. If we know the center of a circle and we know a point on that circle, we know the equation of a line connecting the center to that point. The slope of the line perpendicular to this line is the slope of the tangent line to the circle at that point. So we have already proven the thesis in the case of a circle. A circle is tangent to some other curve if it only intersects that curve at a single point. A circle can be constructed to be tangent to certain points on certain curves. Tangent circles share tangent lines with the curves to which they are tangent. So the procedure above can find tangents to those curves by constructing the correct tangent circle. Parabola case: Consider . We want a circle such that So: We either have 2 complex roots or no real solutions. Assuming we have any real solutions, having a unique one requires that we have a repeated root. So to find the tangent to the curve at we want to divide the quartic by . We'd want to have the remainder terms be zero and the discriminant of the resulting quotient to be negative. EDIT: Turns out Descartes developed this method centuries ago bearing some fruit. It's algebraically more intense than Newton's and Leibniz's approach to finding slopes of tangent lines, so fell by the wayside. It's called Descartes' Method of Normals",a^2+b^2=r^2 y=x^2 (x-a)^2+(x^2-b)^2=r^2 x^4+(1-2b)x^2-2ax+(a^2+b^2-r^2)=0 x_0 (x-x_0)^2,"['calculus', 'derivatives']"
27,Proof of derivative of $x^TBx$ using the product rule [duplicate],Proof of derivative of  using the product rule [duplicate],x^TBx,"This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I'm trying to prove that when $f(x) =x^TBx$ , then $f'(x) = (B + B^T)x$ . I haven't found this formula online but going through the calculations using index notation this is what I came up with. This would simplify to $2Bx$ when $B$ is symmetric. The accepted response to this discussion says that the solution is actually $f'(x) = x^T(B + B^T)$ , going through the proof there, I see how he got there but I can't see where the mistake is in mine then. The setup $x \in \mathbb{R^n}$ , it is always a column vector $B \in \mathbb{R^{n \times n}}$ , $B$ may not be symmetric My approach Let $g(x)=x^TB$ and $h(x)=x$ , then I can write $f(x)=g(x)h(x)$ . Then $f(x) \in \mathbb{R}$ $g(x) \in \mathbb{R^{1 \times n}}$ $h(x) \in \mathbb{R^n}$ $f'(x) \in \mathbb{R^n}$ $g'(x) \in \mathbb{R^{n \times n}}$ $h'(x) \in \mathbb{R^{n \times n}}$ I've gone through myself why $g'(x) = B$ and $h'(x) = I_n$ , so I won't go through those here. Then, using the product rule I get: $$f'(x) = g'(x)h(x) + g(x)h'(x)$$ The problem is that the dimensions don't add up. I get $g'(x)h(x) = Bx \in \mathbb{R^{n}}$ , which is good. However, I also have $g(x)h'(x) = x^TBI_n = x^TB \in \mathbb{R^{1 \times n}}$ and as far as I know I can't add up two vectors of different sizes. I know that the solution is going to be the transpose of second term, I just can't seem to find where that transpose would come from. Why do I need to take the transpose of the second term? [Edit]: Please don't reply with a different proof. What I'm looking for is to understand where I made the mistake in my calculation because obviously I made a step which was incorrect and without understanding where that is I'm likely to make that mistake again.","This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I'm trying to prove that when , then . I haven't found this formula online but going through the calculations using index notation this is what I came up with. This would simplify to when is symmetric. The accepted response to this discussion says that the solution is actually , going through the proof there, I see how he got there but I can't see where the mistake is in mine then. The setup , it is always a column vector , may not be symmetric My approach Let and , then I can write . Then I've gone through myself why and , so I won't go through those here. Then, using the product rule I get: The problem is that the dimensions don't add up. I get , which is good. However, I also have and as far as I know I can't add up two vectors of different sizes. I know that the solution is going to be the transpose of second term, I just can't seem to find where that transpose would come from. Why do I need to take the transpose of the second term? [Edit]: Please don't reply with a different proof. What I'm looking for is to understand where I made the mistake in my calculation because obviously I made a step which was incorrect and without understanding where that is I'm likely to make that mistake again.",f(x) =x^TBx f'(x) = (B + B^T)x 2Bx B f'(x) = x^T(B + B^T) x \in \mathbb{R^n} B \in \mathbb{R^{n \times n}} B g(x)=x^TB h(x)=x f(x)=g(x)h(x) f(x) \in \mathbb{R} g(x) \in \mathbb{R^{1 \times n}} h(x) \in \mathbb{R^n} f'(x) \in \mathbb{R^n} g'(x) \in \mathbb{R^{n \times n}} h'(x) \in \mathbb{R^{n \times n}} g'(x) = B h'(x) = I_n f'(x) = g'(x)h(x) + g(x)h'(x) g'(x)h(x) = Bx \in \mathbb{R^{n}} g(x)h'(x) = x^TBI_n = x^TB \in \mathbb{R^{1 \times n}},['derivatives']
28,"Interpolation, identity for the derivative","Interpolation, identity for the derivative",,"Let $f\in C^{n+1}([a,b])$ , $x_0,\dotso, x_n\in[a,b]$ pairwise different and $p\in \mathbb{P}_n$ the interpolation polynomial (of degree n) with $f(x_j)=p(x_j)$ for all $0\leq j\leq n$ Show that for every $j$ there exists $\xi_j\in [a,b]$ with $f'(x_j)-p'(x_j)=\prod_{i\neq j} (x_i-x_j)\cdot\frac{f^{(n+1)}(\xi_j)}{(n+1)!}$ I struggle to find the idea for a proof. The only thing I got so far is, that I want to define a 'help-function', which might look like this: $h(x)=f(x)-p(x)-\frac{f'(x_j)-p'(x_j)}{\prod_{i\neq j}(x_j-x_i)}\cdot\prod_{i=0}^n (x-x_i)$ But this does not do it yet. And I doubt, that it is a correct approach anyways. I wanted to give a function $h$ with at least $n+2$ roots. The function above has at least $n+1$ roots $x_0,\dotso, x_n$ . I also thought about using some mean-value-theorem. Of the differential calculus or even for integrals, but never thought it through. Can you give me a hint for this problem? I would like to solve it alone, but I am stuck here. Is the idea with a 'help-function' correct? Thanks in advance.","Let , pairwise different and the interpolation polynomial (of degree n) with for all Show that for every there exists with I struggle to find the idea for a proof. The only thing I got so far is, that I want to define a 'help-function', which might look like this: But this does not do it yet. And I doubt, that it is a correct approach anyways. I wanted to give a function with at least roots. The function above has at least roots . I also thought about using some mean-value-theorem. Of the differential calculus or even for integrals, but never thought it through. Can you give me a hint for this problem? I would like to solve it alone, but I am stuck here. Is the idea with a 'help-function' correct? Thanks in advance.","f\in C^{n+1}([a,b]) x_0,\dotso, x_n\in[a,b] p\in \mathbb{P}_n f(x_j)=p(x_j) 0\leq j\leq n j \xi_j\in [a,b] f'(x_j)-p'(x_j)=\prod_{i\neq j} (x_i-x_j)\cdot\frac{f^{(n+1)}(\xi_j)}{(n+1)!} h(x)=f(x)-p(x)-\frac{f'(x_j)-p'(x_j)}{\prod_{i\neq j}(x_j-x_i)}\cdot\prod_{i=0}^n (x-x_i) h n+2 n+1 x_0,\dotso, x_n","['derivatives', 'interpolation']"
29,Not continuous on endpoints and differentiability,Not continuous on endpoints and differentiability,,"I have a general question. Say $f$ is a real function from $(a,b)$ to $\mathbb R$ . We usually prove continuity on $[a,b]$ , but if f were continuous on the open interval $(a,b)$ , would there be any issues with differentiability? Do we need continuity on $[a,b]$ in order to have differentiability on (a,b)?","I have a general question. Say is a real function from to . We usually prove continuity on , but if f were continuous on the open interval , would there be any issues with differentiability? Do we need continuity on in order to have differentiability on (a,b)?","f (a,b) \mathbb R [a,b] (a,b) [a,b]","['real-analysis', 'derivatives', 'continuity']"
30,Functional equation in single-variable calculus,Functional equation in single-variable calculus,,"Suppose we know that $f(x) \in C^2$ and $f(x)$ defined for all real numbers. Furthermore, $f(x)$ has following property: $$ \forall x,y \in \Bbb R \quad f(x+y) - f(x) =  yf'(x + \frac{y}{2})$$ How to proof that $f(x) = ax^2+bx +c \quad (a,b,c - const)$ ? Note: according to the textbook from which I get this task (B.P. Demidovich's Problems in mathematical analysis ), it can be proved without using multivariable calculus and integration.","Suppose we know that and defined for all real numbers. Furthermore, has following property: How to proof that ? Note: according to the textbook from which I get this task (B.P. Demidovich's Problems in mathematical analysis ), it can be proved without using multivariable calculus and integration.","f(x) \in C^2 f(x) f(x)  \forall x,y \in \Bbb R \quad f(x+y) - f(x) =  yf'(x + \frac{y}{2}) f(x) = ax^2+bx +c \quad (a,b,c - const)","['calculus', 'derivatives', 'functional-equations']"
31,The proof of general Leibniz rule: didn't quite understand the combinatorics of it.,The proof of general Leibniz rule: didn't quite understand the combinatorics of it.,,"Few questions regarding this proof: 1) As I understand the reason for the third line is to make the order of derivative for each function equal. The bit I don't understand is why k=1 under the sum symbol?  Why after the sum symbol $k$ changes to $k-1$ ? 2) On the fourth line highlighted n has changed from $n+1$ because we need to make the sums equal?  And if it had stayed the same how the last member highlighted in yellow would look like ( I know that it wouldn't be convenient and useful, just want to understand the reasoning). 3) Why does derivatives of k=0,n disappear on the last line?","Few questions regarding this proof: 1) As I understand the reason for the third line is to make the order of derivative for each function equal. The bit I don't understand is why k=1 under the sum symbol?  Why after the sum symbol changes to ? 2) On the fourth line highlighted n has changed from because we need to make the sums equal?  And if it had stayed the same how the last member highlighted in yellow would look like ( I know that it wouldn't be convenient and useful, just want to understand the reasoning). 3) Why does derivatives of k=0,n disappear on the last line?",k k-1 n+1,"['combinatorics', 'derivatives', 'proof-explanation']"
32,How do I find the derivative of $x$ with respect to $y$ when $y = e^{-x^2}$?,How do I find the derivative of  with respect to  when ?,x y y = e^{-x^2},"When I have $y = e^{-x^2}$ with $-1 \leq x \leq 1$ , I want to find the derivative of $x$ with respect to $y$ . Can I take the log of both sides? $$\ln y = -x^2$$ From here, can I say that $-x^2$ is always = $x^2$ ? If so, I get: $$\ln y = x^2 \implies \sqrt{\ln y} = x$$ What's the derivative? $$\frac{dx}{dy} = \frac{1}{2} \left(\ln y^{\frac{-1}{2}} \right) \cdot \frac{1}{y} = \frac{1}{2y \sqrt{\ln y}}$$ Is that right?","When I have with , I want to find the derivative of with respect to . Can I take the log of both sides? From here, can I say that is always = ? If so, I get: What's the derivative? Is that right?",y = e^{-x^2} -1 \leq x \leq 1 x y \ln y = -x^2 -x^2 x^2 \ln y = x^2 \implies \sqrt{\ln y} = x \frac{dx}{dy} = \frac{1}{2} \left(\ln y^{\frac{-1}{2}} \right) \cdot \frac{1}{y} = \frac{1}{2y \sqrt{\ln y}},"['calculus', 'derivatives']"
33,Find the ratio between the radius and the height,Find the ratio between the radius and the height,,"A coffee filter has the shape of an inverted cone. Water drains out of the filter at a rate of 10 cm $^3$ /min. When the depth of water in the cone is 8 cm, the depth is decreasing at 2 cm/min. What is the ratio of the height of the cone to the radius? So, ${dV \over dt} = -10$ $h=8$ ${dh \over dt}= -2$ Now, $${V}= {1 \over 3} \pi r^2h$$ However, it seems like I am not given enough information. What do I do? I tried: $${dV \over dh} = {2\over 3} \pi r {dr \over dh}$$ However, it feels like I am not given enough information since I don't have ${dr \over dh}$","A coffee filter has the shape of an inverted cone. Water drains out of the filter at a rate of 10 cm /min. When the depth of water in the cone is 8 cm, the depth is decreasing at 2 cm/min. What is the ratio of the height of the cone to the radius? So, Now, However, it seems like I am not given enough information. What do I do? I tried: However, it feels like I am not given enough information since I don't have",^3 {dV \over dt} = -10 h=8 {dh \over dt}= -2 {V}= {1 \over 3} \pi r^2h {dV \over dh} = {2\over 3} \pi r {dr \over dh} {dr \over dh},"['calculus', 'derivatives', 'related-rates']"
34,Please check my work finding related rates,Please check my work finding related rates,,"I have completed  a word problem involving related rates, and gone over it myself. However, this is the first relative rates problem I've ever done, and I would appreciate it if people would check my work. Problem Description: A boat is pulled into a dock by a rope attached to the bow of the boat and passing through a pulley on the dock that is 1 meter higher than the bow of the boat. If the rope is pulled at a rate of 1 meter per second, how fast is the boat approaching the dock when it is 8 meters from the dock? Since the bow of the boat, the pulley, and the dock can all form the corners of a triangle, I can solve the problem by thinking of it in terms of triangles. pulley height (adjacent) $= h = 1$ dock distance (opposite) $= b = 8$ rope length (hypotenuse) $= r = ???$ The rope's rate is $-1$ meters per second. According to the Pythagorean Theoream, $b^2 + h^2 = r^2$ . So, $2b \frac{db}{dt} + 0 = 2r\frac{dr}{dt}$ $$\frac{dr}{dt} = \frac{2(r)(-1)}{2(8)} = \frac{r(-1)}{8}$$ Using the Pythagorean Theorem again, I know that $1 + 8^2=r^2 \to \sqrt{1 + 64} = r \to \sqrt{65} = r$ . Therefore, since $r = \sqrt{65}$ , I know that $$\frac{dr}{dt}= \frac{\sqrt{65}(-1)}{8}=\frac{-\sqrt{65}}{8}$$ $\frac{-\sqrt{65}}{8}$ refers to meters per second of course.","I have completed  a word problem involving related rates, and gone over it myself. However, this is the first relative rates problem I've ever done, and I would appreciate it if people would check my work. Problem Description: A boat is pulled into a dock by a rope attached to the bow of the boat and passing through a pulley on the dock that is 1 meter higher than the bow of the boat. If the rope is pulled at a rate of 1 meter per second, how fast is the boat approaching the dock when it is 8 meters from the dock? Since the bow of the boat, the pulley, and the dock can all form the corners of a triangle, I can solve the problem by thinking of it in terms of triangles. pulley height (adjacent) dock distance (opposite) rope length (hypotenuse) The rope's rate is meters per second. According to the Pythagorean Theoream, . So, Using the Pythagorean Theorem again, I know that . Therefore, since , I know that refers to meters per second of course.",= h = 1 = b = 8 = r = ??? -1 b^2 + h^2 = r^2 2b \frac{db}{dt} + 0 = 2r\frac{dr}{dt} \frac{dr}{dt} = \frac{2(r)(-1)}{2(8)} = \frac{r(-1)}{8} 1 + 8^2=r^2 \to \sqrt{1 + 64} = r \to \sqrt{65} = r r = \sqrt{65} \frac{dr}{dt}= \frac{\sqrt{65}(-1)}{8}=\frac{-\sqrt{65}}{8} \frac{-\sqrt{65}}{8},"['calculus', 'derivatives', 'related-rates']"
35,"Why is $e^{it}$ a submersion, and what is the relationship between the derivative $\dot h(t)$ and the differential $h_{*,t}$?","Why is  a submersion, and what is the relationship between the derivative  and the differential ?","e^{it} \dot h(t) h_{*,t}","My book is An Introduction to Manifolds by Loring W. Tu. In the paragraph below, why exactly is $h: \mathbb R \to S^1, h(t)=(\cos t, \sin t) \cong e^{it}$ a submersion, and what is the relationship between the derivative $\dot h(t) = (-\sin t, \cos t) \cong ie^{it}$ and the differential $h_{*,t}$ ? My argument is that the image of the differential at any $t$ , $h_{*,t}: T_t(\mathbb R) \to T_{e^{it}}(S^1)$ , is a vector subspace of $S^1$ , which has dimension 1, so the image has dimension either $0$ or $1$ , so the differential is either trivial or surjective. I think I can show each $h_{*,t}$ is not trivial directly by computing the image: $$im(h_{*,t}) = h_{*,t}(T_t(\mathbb R)) = \{h_{*,t}(a \frac{d}{dt})\}_{a \in \mathbb R} = \{ah_{*,t}(\frac{d}{dt})\}_{a \in \mathbb R}$$ $$\ne \{\text{the zero element of} \ T_{e^{it}}(S^1) \ \text{which I think corresponds to the point} \ 1+0i \in S^1 \}$$ I cannot seem to identify the precise relationship between the derivative $\dot h(t)$ and the differential $h_{*,t}$ , which I think I would assume in proving the preceding inequality. For a function $f: M \to \mathbb R$ , there is a relationship between $f$ 's a submersion at $p$ and $f$ 's partial derivatives, but now we have $f: \mathbb R \to M$ . Update : My answer considers the inclusion $g = \iota \circ h$ of $h$ into $\mathbb R^2$ . After all, we have in the first place that $h$ is smooth by Theorem 11.15 because $g$ is smooth. For such inclusion $g = \iota \circ h: \mathbb R \to \mathbb R^2$ , $g(t_0) = (\iota \circ h)(t_0) = \iota(h(t_0))=h(t_0), \iota: S^1 \to \mathbb R^2$ , we have by chain rule that for each $t_0 \in \mathbb R$ , $g_{*,t_0}: T_{t_0}\mathbb R \to T_{h(t_0)}\mathbb R^2$ is given by $g_{*,t_0} = \iota_{*,h(t_0)} \circ h_{*,t_0}$ . For each $t_0 \in \mathbb R$ , $\iota_{*,h(t_0)}: T_{h(t_0)}S^1 \to T_{h(t_0)}\mathbb R^2$ is still inclusion so $\iota_{*,h(t_0)} \circ h_{*,t_0} = h_{*,t_0}$ Let $t_0 \in \mathbb R$ . If $h_{*,t_0}$ is not the zero map, then $h_{*,t_0}$ is surjective because the image of $h_{*,t_0}: T_{t_0}\mathbb R \to T_{h(t_0)}S^1$ is a subspace of $T_{h(t_0)}S^1 \cong \mathbb R$ and thus is (isomorphic to) either $\mathbb R$ or $\mathbb R^0 = \{0\}$ . Now, I'll argue that $h_{*,t_0}$ is not the zero map if I can show that $g_{*,t_0}$ is not the zero map since $T_{h(t_0)}S^1$ and $T_{h(t_0)}\mathbb R^2$ share the same zero element since $T_{h(t_0)}S^1$ is a vector subspace of $T_{h(t_0)}\mathbb R^2$ . $g_{*,t_0}$ is not the zero map if the geometric derivative (or velocity vector) $g'(t_0) := g_{*,t_0}[\frac{d}{dt}|_{t_0}]$ is not the zero vector i.e. if $g_{*,t_0}$ maps the basis element of $T_{t_0}\mathbb R$ to a nonzero element of $T_{h(t_0)}\mathbb R^2$ . Finally, $g'(t_0)$ , by Proposition 8.15 , is represented by calculus derivative $\dot g(t) = (-\sin(t),\cos(t))$ through $g'(t_0) = -\sin(t)\frac{\partial}{\partial r^1} + \cos(t) \frac{\partial}{\partial r^2}$ , where $r^1$ and $r^2$ are the standard coordinates on $\mathbb R^2$ such that $\{\frac{\partial}{\partial r^1}, \frac{\partial}{\partial r^2} \}$ forms a basis for $T_{h(t_0)}\mathbb R^2$ . By (6), $g'(t_0)$ is not the zero vector of $T_{h(t_0)}\mathbb R^2$ . Therefore, by (7),(5),(4) $g_{*,t_0}$ is the not the zero map. Therefore, by (3) and (8), neither is $h_{*,t_0}$ i.e. $h'(t_0)$ is the not the zero vector of $T_{h(t_0)}S^1$ .","My book is An Introduction to Manifolds by Loring W. Tu. In the paragraph below, why exactly is a submersion, and what is the relationship between the derivative and the differential ? My argument is that the image of the differential at any , , is a vector subspace of , which has dimension 1, so the image has dimension either or , so the differential is either trivial or surjective. I think I can show each is not trivial directly by computing the image: I cannot seem to identify the precise relationship between the derivative and the differential , which I think I would assume in proving the preceding inequality. For a function , there is a relationship between 's a submersion at and 's partial derivatives, but now we have . Update : My answer considers the inclusion of into . After all, we have in the first place that is smooth by Theorem 11.15 because is smooth. For such inclusion , , we have by chain rule that for each , is given by . For each , is still inclusion so Let . If is not the zero map, then is surjective because the image of is a subspace of and thus is (isomorphic to) either or . Now, I'll argue that is not the zero map if I can show that is not the zero map since and share the same zero element since is a vector subspace of . is not the zero map if the geometric derivative (or velocity vector) is not the zero vector i.e. if maps the basis element of to a nonzero element of . Finally, , by Proposition 8.15 , is represented by calculus derivative through , where and are the standard coordinates on such that forms a basis for . By (6), is not the zero vector of . Therefore, by (7),(5),(4) is the not the zero map. Therefore, by (3) and (8), neither is i.e. is the not the zero vector of .","h: \mathbb R \to S^1, h(t)=(\cos t, \sin t) \cong e^{it} \dot h(t) = (-\sin t, \cos t) \cong ie^{it} h_{*,t} t h_{*,t}: T_t(\mathbb R) \to T_{e^{it}}(S^1) S^1 0 1 h_{*,t} im(h_{*,t}) = h_{*,t}(T_t(\mathbb R)) = \{h_{*,t}(a \frac{d}{dt})\}_{a \in \mathbb R} = \{ah_{*,t}(\frac{d}{dt})\}_{a \in \mathbb R} \ne \{\text{the zero element of} \ T_{e^{it}}(S^1) \ \text{which I think corresponds to the point} \ 1+0i \in S^1 \} \dot h(t) h_{*,t} f: M \to \mathbb R f p f f: \mathbb R \to M g = \iota \circ h h \mathbb R^2 h g g = \iota \circ h: \mathbb R \to \mathbb R^2 g(t_0) = (\iota \circ h)(t_0) = \iota(h(t_0))=h(t_0), \iota: S^1 \to \mathbb R^2 t_0 \in \mathbb R g_{*,t_0}: T_{t_0}\mathbb R \to T_{h(t_0)}\mathbb R^2 g_{*,t_0} = \iota_{*,h(t_0)} \circ h_{*,t_0} t_0 \in \mathbb R \iota_{*,h(t_0)}: T_{h(t_0)}S^1 \to T_{h(t_0)}\mathbb R^2 \iota_{*,h(t_0)} \circ h_{*,t_0} = h_{*,t_0} t_0 \in \mathbb R h_{*,t_0} h_{*,t_0} h_{*,t_0}: T_{t_0}\mathbb R \to T_{h(t_0)}S^1 T_{h(t_0)}S^1 \cong \mathbb R \mathbb R \mathbb R^0 = \{0\} h_{*,t_0} g_{*,t_0} T_{h(t_0)}S^1 T_{h(t_0)}\mathbb R^2 T_{h(t_0)}S^1 T_{h(t_0)}\mathbb R^2 g_{*,t_0} g'(t_0) := g_{*,t_0}[\frac{d}{dt}|_{t_0}] g_{*,t_0} T_{t_0}\mathbb R T_{h(t_0)}\mathbb R^2 g'(t_0) \dot g(t) = (-\sin(t),\cos(t)) g'(t_0) = -\sin(t)\frac{\partial}{\partial r^1} + \cos(t) \frac{\partial}{\partial r^2} r^1 r^2 \mathbb R^2 \{\frac{\partial}{\partial r^1}, \frac{\partial}{\partial r^2} \} T_{h(t_0)}\mathbb R^2 g'(t_0) T_{h(t_0)}\mathbb R^2 g_{*,t_0} h_{*,t_0} h'(t_0) T_{h(t_0)}S^1","['general-topology', 'proof-verification']"
36,Subgradient of $\|AX\|_1$,Subgradient of,\|AX\|_1,"If $f(x) = \|AX\|_1$ , where $\|.\|$ denotes the entrywise $\ell_1$ -norm, what is the subgradient of $f(x)$ ? Is there an expression similar to $A'A \ \partial \|X\|_1$ ? For example, $A = [1 \ 1;  1 \ 0; 0 \ 1]$ .","If , where denotes the entrywise -norm, what is the subgradient of ? Is there an expression similar to ? For example, .",f(x) = \|AX\|_1 \|.\| \ell_1 f(x) A'A \ \partial \|X\|_1 A = [1 \ 1;  1 \ 0; 0 \ 1],"['derivatives', 'convex-analysis', 'convex-optimization', 'normed-spaces', 'subgradient']"
37,Let $f : \mathbb{R} \to \mathbb{R}$ be twice differentiable,Let  be twice differentiable,f : \mathbb{R} \to \mathbb{R},"Let $f : \mathbb{R} \to \mathbb{R}$ be twice differentiable. Let $$g(x) =f(x)−f(0)-f'(0)x-[f(1)-f(0)-f'(0)]x^2.$$ Show that there is a number $t \in (0, 1)$ where $g''(t) = 0.$ Deduce that $$ f(1) = f(0) + f ' (0) + (1 /2) f ''(t). $$ Could someone explain how to proceed? What I could use to solve this.",Let be twice differentiable. Let Show that there is a number where Deduce that Could someone explain how to proceed? What I could use to solve this.,"f : \mathbb{R} \to \mathbb{R} g(x) =f(x)−f(0)-f'(0)x-[f(1)-f(0)-f'(0)]x^2. t \in (0, 1) g''(t) = 0. 
f(1) = f(0) + f
'
(0) + (1
/2)
f
''(t).
",['analysis']
38,Is the moment generating function smooth by definition?,Is the moment generating function smooth by definition?,,"So I'm going through Casella's Statistical Inference , and in Definition 2.3.6 he defines the moment generating function of a random variable $X$ with cdf $F_X$ , denoted by $M_X(t)$ , as $$M_X(t) = Ee^{tx},$$ provided that the expectation exists for $t$ in some neighborhood of $0$ . In Theorem 2.3.7, he then states that if $X$ has a mgf $M_X(t)$ , then the nth moment of $X$ is equal to the nth derivative of $M_X(t)$ evaluated at $0$ . But why do all of these derivatives have to existed? Is it implicitly assumed without it being stated, or does it somehow just follow from the definition above? If so, how?","So I'm going through Casella's Statistical Inference , and in Definition 2.3.6 he defines the moment generating function of a random variable with cdf , denoted by , as provided that the expectation exists for in some neighborhood of . In Theorem 2.3.7, he then states that if has a mgf , then the nth moment of is equal to the nth derivative of evaluated at . But why do all of these derivatives have to existed? Is it implicitly assumed without it being stated, or does it somehow just follow from the definition above? If so, how?","X F_X M_X(t) M_X(t) = Ee^{tx}, t 0 X M_X(t) X M_X(t) 0","['derivatives', 'expected-value', 'moment-generating-functions']"
39,Existence of a strictly increasing transformation between two functions [closed],Existence of a strictly increasing transformation between two functions [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Assume $f$ and $g$ are two differentiable functions defined on a compact interval $X \subseteq \mathbb{R}$ mapping into $\mathbb{R}$ . I want to proof or disproof the following statement $ \forall x \in X: \operatorname{sign}(f'(x))=\operatorname{sign}(g'(x))\;\;  \implies \exists \;\; m: \mathbb{R} \to \mathbb{R}$ , strictly increasing s.t. $f=m \circ g$ My attempts raised the elementary question which conditions on arbitrary $f,g$ are in general sufficient for the existence of an $m$ such that $f=m \circ g$ . Any suggestions?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Assume and are two differentiable functions defined on a compact interval mapping into . I want to proof or disproof the following statement , strictly increasing s.t. My attempts raised the elementary question which conditions on arbitrary are in general sufficient for the existence of an such that . Any suggestions?","f g X \subseteq \mathbb{R} \mathbb{R}  \forall x \in X: \operatorname{sign}(f'(x))=\operatorname{sign}(g'(x))\;\;  \implies \exists \;\; m: \mathbb{R} \to \mathbb{R} f=m \circ g f,g m f=m \circ g","['derivatives', 'transformation', 'monotone-functions']"
40,Compute the gradient of $f(x)=\|\text{diag}(x)\|$ with the chain rule,Compute the gradient of  with the chain rule,f(x)=\|\text{diag}(x)\|,"Consider the function $f:\mathbb{R}^n\to\mathbb{R}$ given by $f(x)=\|\text{diag}(x)\|$ , where $\text{diag}(x)\in\mathbb{R}^{n\times{n}}$ is the diagonal matrix with diagonal entries $x_1,x_2,\dots,x_n$ , and $\|\cdot\|$ is the spectral norm (matrix 2-norm). Since the spectral norm of a matrix is its largest singular value, and the singular values of a (square) diagonal matrix are the absolute values of the diagonal entries, we see that $f(x)=\|x\|_\infty$ , where $\|\cdot\|_\infty$ is the (vector) sup-norm. In this form, it is easier to deduce the properties of $f$ --in particular, it is differentiable at any point $x\in\mathbb{R}^n$ where the largest element of $x$ (in absolute value) is unique. At such a point, the gradient of $f$ is given by $$ \nabla{f(x)}=\text{sgn}(x_k)e_k $$ where $k$ is the index of the (unique) largest entry of $x$ (in absolute value), $e_k$ is the $k^\text{th}$ standard basis vector in $\mathbb{R}^n$ , and $\text{sgn}(\cdot)$ is the sign function. I want to deduce the above expression for the gradient using the chain rule applied to $f(x)=(g\circ{h})(x)$ , where $g:\mathbb{R}^{n\times{n}}\to\mathbb{R}$ is given by $g(A)=\|A\|$ , and $h:\mathbb{R}^n\to\mathbb{R}^{n\times{n}}$ is given by $h(x)=\text{diag}(x)$ . The ""Jacobian"" of $h$ is a three-dimensional object, where $$ \frac{\partial[h(x)]_{ij}}{\partial{x_k}}=\begin{cases}1,&i=j=k,\\0,&\text{else.}\end{cases} $$ The function $g$ is differentiable at any point $A$ where $A$ has a unique largest singular value, in which case the gradient(?) is given by $$ \nabla{g(A)}=uv^\text{T}, $$ where $u$ and $v$ are the left and right singular vectors (respectively) corresponding to the (unique) largest singular value of $A$ . So I essentially have a three-dimensional object and a 2-dimensional object, and I want to apply the chain rule to get the gradient, a 1-dimensional object ( i.e. a vector). A straightforward application suggests ""multiplying them together"" (not sure that concept is even defined), which seems like it would produce a matrix. What simple thing am I missing here?","Consider the function given by , where is the diagonal matrix with diagonal entries , and is the spectral norm (matrix 2-norm). Since the spectral norm of a matrix is its largest singular value, and the singular values of a (square) diagonal matrix are the absolute values of the diagonal entries, we see that , where is the (vector) sup-norm. In this form, it is easier to deduce the properties of --in particular, it is differentiable at any point where the largest element of (in absolute value) is unique. At such a point, the gradient of is given by where is the index of the (unique) largest entry of (in absolute value), is the standard basis vector in , and is the sign function. I want to deduce the above expression for the gradient using the chain rule applied to , where is given by , and is given by . The ""Jacobian"" of is a three-dimensional object, where The function is differentiable at any point where has a unique largest singular value, in which case the gradient(?) is given by where and are the left and right singular vectors (respectively) corresponding to the (unique) largest singular value of . So I essentially have a three-dimensional object and a 2-dimensional object, and I want to apply the chain rule to get the gradient, a 1-dimensional object ( i.e. a vector). A straightforward application suggests ""multiplying them together"" (not sure that concept is even defined), which seems like it would produce a matrix. What simple thing am I missing here?","f:\mathbb{R}^n\to\mathbb{R} f(x)=\|\text{diag}(x)\| \text{diag}(x)\in\mathbb{R}^{n\times{n}} x_1,x_2,\dots,x_n \|\cdot\| f(x)=\|x\|_\infty \|\cdot\|_\infty f x\in\mathbb{R}^n x f 
\nabla{f(x)}=\text{sgn}(x_k)e_k
 k x e_k k^\text{th} \mathbb{R}^n \text{sgn}(\cdot) f(x)=(g\circ{h})(x) g:\mathbb{R}^{n\times{n}}\to\mathbb{R} g(A)=\|A\| h:\mathbb{R}^n\to\mathbb{R}^{n\times{n}} h(x)=\text{diag}(x) h 
\frac{\partial[h(x)]_{ij}}{\partial{x_k}}=\begin{cases}1,&i=j=k,\\0,&\text{else.}\end{cases}
 g A A 
\nabla{g(A)}=uv^\text{T},
 u v A","['derivatives', 'matrix-calculus', 'chain-rule']"
41,Derivatives of inverse matrix,Derivatives of inverse matrix,,"In the matrix cookbook , Eq. 59, the formula: $$ \partial_x Y^{-1}=-Y^{-1}(\partial_x Y) Y^{-1} $$ is provided without proof. How to prove this formula?","In the matrix cookbook , Eq. 59, the formula: is provided without proof. How to prove this formula?","
\partial_x Y^{-1}=-Y^{-1}(\partial_x Y) Y^{-1}
","['linear-algebra', 'derivatives']"
42,First and second differentiability of the piecewise function $x^4\sin(\frac{1}{x})$ if $x \neq 0$ and $0$ if $x=0$,First and second differentiability of the piecewise function  if  and  if,x^4\sin(\frac{1}{x}) x \neq 0 0 x=0,"I have the following function $f(x)$ defined as $x^4\sin(\frac{1}{x})$ if $x \neq 0$ and $0$ if $x=0$ . And I'm asked if the function is: a) differentiable b) two times differentiable c) two times continuously differentiable This function is of course quite similar to the function $x^2\sin(\frac{1}{x})$ , which can be found on the internet for the same type of exercises. But I still want to be sure that I proceeded in the correct way, because I have no solution to this exercise. Thanks for your feedback. a) At $x=0$ , we have $$\lim_{h\to 0}\frac{f(0+h)-f(0)}{h}=\lim_{h \to 0}\frac{h^4\sin(\frac{1}{h})}{h}=\lim_{h \to 0}h^3\sin(\frac{1}{h})=0$$ Everywhere else, we have simply $4x^3 \sin(\frac{1}{x})-x^2\cos(\frac{1}{x})$ . Thus the function is differentiable everywhere, even at $0$ where its value is $0$ . b) Again, if $x\neq 0$ , we have simply $f^{\prime \prime}(x)=12x^2\sin(\frac{1}{x})-\sin(\frac{1}{x})-6x\cos(\frac{1}{x})$ . At $x=0$ , we have $$\lim_{h \to 0}\frac{f'(0+h)-f'(0)}{h}=\lim_{h \to 0}\frac{4h^3\sin(\frac{1}{h})-h^2\cos(\frac{1}{h})}{h}=\lim_{h \to 0}4h^2\sin(\frac{1}{h})-h\cos(\frac{1}{h})=0$$ So the  function is also two times differentiable at $0$ , where the derivative is $0$ . c) The function is however not two times continuously differentiable, because as said, the second derivative at $0$ is $0$ , but if we take $\lim_{x \to 0_{\pm}} 12x^2\sin(\frac{1}{x})-\sin(\frac{1}{x})-6x\cos(\frac{1}{x})$ , we don't always reach $0$ because $\sin(\frac{1}{x})$ oscillates between $-1$ and $1$ . So the function is not two times continuously differentiable, even if the second derivative is defined everywhere, even at $0$ Are my results correct ? Thanks for your help !","I have the following function defined as if and if . And I'm asked if the function is: a) differentiable b) two times differentiable c) two times continuously differentiable This function is of course quite similar to the function , which can be found on the internet for the same type of exercises. But I still want to be sure that I proceeded in the correct way, because I have no solution to this exercise. Thanks for your feedback. a) At , we have Everywhere else, we have simply . Thus the function is differentiable everywhere, even at where its value is . b) Again, if , we have simply . At , we have So the  function is also two times differentiable at , where the derivative is . c) The function is however not two times continuously differentiable, because as said, the second derivative at is , but if we take , we don't always reach because oscillates between and . So the function is not two times continuously differentiable, even if the second derivative is defined everywhere, even at Are my results correct ? Thanks for your help !",f(x) x^4\sin(\frac{1}{x}) x \neq 0 0 x=0 x^2\sin(\frac{1}{x}) x=0 \lim_{h\to 0}\frac{f(0+h)-f(0)}{h}=\lim_{h \to 0}\frac{h^4\sin(\frac{1}{h})}{h}=\lim_{h \to 0}h^3\sin(\frac{1}{h})=0 4x^3 \sin(\frac{1}{x})-x^2\cos(\frac{1}{x}) 0 0 x\neq 0 f^{\prime \prime}(x)=12x^2\sin(\frac{1}{x})-\sin(\frac{1}{x})-6x\cos(\frac{1}{x}) x=0 \lim_{h \to 0}\frac{f'(0+h)-f'(0)}{h}=\lim_{h \to 0}\frac{4h^3\sin(\frac{1}{h})-h^2\cos(\frac{1}{h})}{h}=\lim_{h \to 0}4h^2\sin(\frac{1}{h})-h\cos(\frac{1}{h})=0 0 0 0 0 \lim_{x \to 0_{\pm}} 12x^2\sin(\frac{1}{x})-\sin(\frac{1}{x})-6x\cos(\frac{1}{x}) 0 \sin(\frac{1}{x}) -1 1 0,"['real-analysis', 'calculus']"
43,"Tangent line, normal line - confusing.","Tangent line, normal line - confusing.",,"I am practicing finding tangent and normal line. The tangent/normal line is usually to some graph, and parallel/perpendicular to some other line at the same time. Not that complicated. Can someone, please , verify my solutions? The more I study the less I know, and I don't even know how much I don't know. Thank you! But I'm confused, mostly about the $a$ ""directional coefficient"" of $y=ax+b$ part. Let's say the task is: Find tangent and normal lines to the function $f(x) = \ln(x+1)$ , parallel&perpendicular to $y=\frac{1}{2}x$ . (so 4 variations in total.) What I know: I do know that if a line has to be parallel to other line, then the $a$ coefficients have to be the same. And if perpendicular, then I have to inverse it and change the sign. What I am not sure of: how to properly write the equations. How to properly substitute into the equations. I am doing mistakes here related to $a$ coefficient. I am looking for an algorithmic way so I can easily understand and remember how to solve it. $f(x) = \ln(x+1)$ $f'(x) = \frac{1}{x+1}$ 1. Finding tangent parallel to $y=\frac{1}{2}x$ . $\frac{1}{x+1} = \frac{1}{2}$ $x = 1 = x_{0}$ $f(x_{0}) = f(1) = \ln2$ $\Rightarrow y = \frac{1}{2}x - \frac{1}{2} + \ln2$ 2. Finding tangent perpendicular to $y=\frac{1}{2}x$ $\frac{1}{x+1} = -2$ $x = 1 = x_{0}$ $f(x_{0}) = f(1) = \ln2$ $\Rightarrow y - \ln2 = -2(x-x_{0})$ $\Rightarrow y = -2x + 2 + \ln2$ 3. Finding normal line parallel to $y = \frac{1}{2}x$ $\frac{1}{x+1} = -2$ $x = \frac{-3}{2} = x_{0}$ $f(x_{0}) = f(-\frac{3}{2})= \ln(\frac{-1}{2})$ - does not exist $y = \frac{1}{2}(x-x_{0})$ $y = \frac{1}{2}x + \frac{3}{4}$ 4. Finding normal line perpendicular to $y = \frac{1}{2}x$ $\frac{1}{x+1} = -2$ $x = \frac{-3}{2} = x_{0}$ $f(x_{0}) = f(\frac{-3}{2}) = \ln(\frac{-1}{2})$ - does not exist $y - 0 = -2(x-x_{0})$ $y = -2x - 3$ Thanks for your time!","I am practicing finding tangent and normal line. The tangent/normal line is usually to some graph, and parallel/perpendicular to some other line at the same time. Not that complicated. Can someone, please , verify my solutions? The more I study the less I know, and I don't even know how much I don't know. Thank you! But I'm confused, mostly about the ""directional coefficient"" of part. Let's say the task is: Find tangent and normal lines to the function , parallel&perpendicular to . (so 4 variations in total.) What I know: I do know that if a line has to be parallel to other line, then the coefficients have to be the same. And if perpendicular, then I have to inverse it and change the sign. What I am not sure of: how to properly write the equations. How to properly substitute into the equations. I am doing mistakes here related to coefficient. I am looking for an algorithmic way so I can easily understand and remember how to solve it. 1. Finding tangent parallel to . 2. Finding tangent perpendicular to 3. Finding normal line parallel to - does not exist 4. Finding normal line perpendicular to - does not exist Thanks for your time!",a y=ax+b f(x) = \ln(x+1) y=\frac{1}{2}x a a f(x) = \ln(x+1) f'(x) = \frac{1}{x+1} y=\frac{1}{2}x \frac{1}{x+1} = \frac{1}{2} x = 1 = x_{0} f(x_{0}) = f(1) = \ln2 \Rightarrow y = \frac{1}{2}x - \frac{1}{2} + \ln2 y=\frac{1}{2}x \frac{1}{x+1} = -2 x = 1 = x_{0} f(x_{0}) = f(1) = \ln2 \Rightarrow y - \ln2 = -2(x-x_{0}) \Rightarrow y = -2x + 2 + \ln2 y = \frac{1}{2}x \frac{1}{x+1} = -2 x = \frac{-3}{2} = x_{0} f(x_{0}) = f(-\frac{3}{2})= \ln(\frac{-1}{2}) y = \frac{1}{2}(x-x_{0}) y = \frac{1}{2}x + \frac{3}{4} y = \frac{1}{2}x \frac{1}{x+1} = -2 x = \frac{-3}{2} = x_{0} f(x_{0}) = f(\frac{-3}{2}) = \ln(\frac{-1}{2}) y - 0 = -2(x-x_{0}) y = -2x - 3,"['real-analysis', 'calculus', 'linear-algebra', 'derivatives', 'tangent-line']"
44,Does an endpoint of a function have a slope?,Does an endpoint of a function have a slope?,,"Say I have the following function which is defined for $0 \leq x \leq 1$ : If you would derive this function and substitute $y = 0$ , you would get $x = 0.5$ and $x = 0.75$ for the respective as the maximum and minimum point respectively. Now, what if I ask for which range of $x$ does this function rise? Well, I know that it's: $$0 < x < 0.5$$ $$0.75 < x < 1$$ What confuses me is: why don't we include the endpoints and make the inequality inclusive? The explanation I have been given is that a slope is determined between two points, but then why is it when you substitute the $x$ coordinate of the endpoints in the derivative, you get a value?","Say I have the following function which is defined for : If you would derive this function and substitute , you would get and for the respective as the maximum and minimum point respectively. Now, what if I ask for which range of does this function rise? Well, I know that it's: What confuses me is: why don't we include the endpoints and make the inequality inclusive? The explanation I have been given is that a slope is determined between two points, but then why is it when you substitute the coordinate of the endpoints in the derivative, you get a value?",0 \leq x \leq 1 y = 0 x = 0.5 x = 0.75 x 0 < x < 0.5 0.75 < x < 1 x,"['real-analysis', 'calculus', 'derivatives', 'slope']"
45,Gradient of Square of Quadratic Inner-Product,Gradient of Square of Quadratic Inner-Product,,"\begin{equation} \begin{aligned} f(x) :=&   {\langle}x,Ax{\rangle}^2\\ =& x^{T}Axx^{T} Ax\\ & x \in \mathbb{R}^n,\\ & A \in \mathbb{R}^{n \times n}, \;A = A^T. \end{aligned} \end{equation} The 2nd answer is the generalization + example code to validate.",The 2nd answer is the generalization + example code to validate.,"\begin{equation}
\begin{aligned}
f(x) :=&   {\langle}x,Ax{\rangle}^2\\
=& x^{T}Axx^{T} Ax\\
& x \in \mathbb{R}^n,\\
& A \in \mathbb{R}^{n \times n}, \;A = A^T.
\end{aligned}
\end{equation}","['linear-algebra', 'derivatives']"
46,Apply Rolle's theorem to find real roots,Apply Rolle's theorem to find real roots,,"Suppose the function $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$ such that $f(a)=f(b)=0$ . Prove that there exist a point $c\in(a,b)$ such that $$f(c)-f'(c)=0$$ From the question above, or otherwise, show that the equation $$1+x+\frac{x^2}{2!}+\cdots+\frac{x^{2n+1}}{(2n+1)!}=0$$ has real roots on $\mathbb{R}$ but not more than two.   Additionally, show that the equation $$e^x-x^n=0$$ has at most three real roots on $\mathbb{R}$ , where $n \in \mathbb{N}$ . My attempt: Suppose the function $$h(x)=e^{-x}f(x)$$ Then $$h'(x)=e^{-x}[f(x)-f'(x)]$$ Since $h(a)=h(b)=0$ and $e^{-x}>0$ for all real $x$ , then there exist a point $c\in(a,b)$ such that $h'(c)=0$ i.e. $f(c)-f'(c)=0$ . Then I get stuck on the following question. Should I start it with construct a function $$h(x)=e^{-x}[1+x+\frac{x^2}{2!}+\cdots+\frac{x^{2n+1}}{(2n+1)!}]$$ and follow my previous procedure?","Suppose the function is continuous on and differentiable on such that . Prove that there exist a point such that From the question above, or otherwise, show that the equation has real roots on but not more than two.   Additionally, show that the equation has at most three real roots on , where . My attempt: Suppose the function Then Since and for all real , then there exist a point such that i.e. . Then I get stuck on the following question. Should I start it with construct a function and follow my previous procedure?","f [a,b] (a,b) f(a)=f(b)=0 c\in(a,b) f(c)-f'(c)=0 1+x+\frac{x^2}{2!}+\cdots+\frac{x^{2n+1}}{(2n+1)!}=0 \mathbb{R} e^x-x^n=0 \mathbb{R} n
\in \mathbb{N} h(x)=e^{-x}f(x) h'(x)=e^{-x}[f(x)-f'(x)] h(a)=h(b)=0 e^{-x}>0 x c\in(a,b) h'(c)=0 f(c)-f'(c)=0 h(x)=e^{-x}[1+x+\frac{x^2}{2!}+\cdots+\frac{x^{2n+1}}{(2n+1)!}]","['analysis', 'derivatives', 'proof-verification', 'rolles-theorem']"
47,"Twice diffrentiable function $f: \mathbb R \to \mathbb R$ such that $f''(x)+e^xf(x)=0, \forall x \in \mathbb R$ [closed]",Twice diffrentiable function  such that  [closed],"f: \mathbb R \to \mathbb R f''(x)+e^xf(x)=0, \forall x \in \mathbb R","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $f: \mathbb R \to \mathbb R$ be a twice differentiable function such that $f''(x)+e^xf(x)=0, \forall x \in \mathbb R$ . Then is it true that $f(x)$ is bounded in $[0,\infty)$ i.e. does there exist $M >0$ such that $|f(x)| <M, \forall x >0$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let be a twice differentiable function such that . Then is it true that is bounded in i.e. does there exist such that ?","f: \mathbb R \to \mathbb R f''(x)+e^xf(x)=0, \forall x \in \mathbb R f(x) [0,\infty) M >0 |f(x)| <M, \forall x >0","['real-analysis', 'derivatives', 'continuity']"
48,Not sure what derivative to take,Not sure what derivative to take,,"I have a relation $$\frac{(x\cos(\tan^{-1}(x_1))-y\sin(\tan^{-1}(x_1)))^2}{a^2}+\frac{(y\cos(\tan^{-1}(x_1))+x\sin(\tan^{-1}(x_1)))^2}{b^2}=1$$ $a^2$ and $b^2$ are both functions solely in terms of $x_1$ . For various real number values of $x_1$ , the function on the Cartesian coordinate plane is that of a rotated ellipse. I want to find what the equation of the derivative of that ellipse trends towards as $x_1$ approaches toward infinity. I believe to get that I need the limit as $x_1$ approaches infinity of some sort of function in both $x_1$ and $x$ in which I can substitute $x_1$ and get the equation of the derivative of the  ellipse that is formed by that value of $x_1$ .  I presume to get that I need to take some type of derivative, however, I do not know what type that might be, or whether it is even possible to get the derivatives of the ellipses simply by substituting a variable.","I have a relation and are both functions solely in terms of . For various real number values of , the function on the Cartesian coordinate plane is that of a rotated ellipse. I want to find what the equation of the derivative of that ellipse trends towards as approaches toward infinity. I believe to get that I need the limit as approaches infinity of some sort of function in both and in which I can substitute and get the equation of the derivative of the  ellipse that is formed by that value of .  I presume to get that I need to take some type of derivative, however, I do not know what type that might be, or whether it is even possible to get the derivatives of the ellipses simply by substituting a variable.",\frac{(x\cos(\tan^{-1}(x_1))-y\sin(\tan^{-1}(x_1)))^2}{a^2}+\frac{(y\cos(\tan^{-1}(x_1))+x\sin(\tan^{-1}(x_1)))^2}{b^2}=1 a^2 b^2 x_1 x_1 x_1 x_1 x_1 x x_1 x_1,"['calculus', 'derivatives']"
49,"$G'(0) = \int_{[0,1] \backslash Z(g)} h(x) \cdot \text{sign}(g(x)) \mathrm{d}x$",,"G'(0) = \int_{[0,1] \backslash Z(g)} h(x) \cdot \text{sign}(g(x)) \mathrm{d}x","Today during an exam I got the following exercise : Let $h, g \in C^0([0,1], \| \cdot \|_1)$ such that the set : $Z(g) = \{x \in [0,1] \mid g(x) = 0\}$ is a finite union of intervals. Then let's defined : $$ G : t \mapsto \| g + th \|_1$$ If the function $G$ has a derivative at $0$ prove that : $$G'(0) = \int_{[0,1] \backslash Z(g)} h(x) \cdot sign(g(x)) \mathrm{d}x$$ First of all I don't understand how $G$ is defined because do we consider that : $$G(t) = \int_{[0,1]} \mid g(x) + th(x) \mid \mathrm{d}x$$ Or we consider that : $$G(t) = \int_{[0,1]} \mid g(t) + th(t) \mid \mathrm{d}t$$ ? Then I tried considering : $$\frac{G(h) - G(0)}{h}$$ In order to find the derivative. Yet the $\mid \cdot \mid$ make the task not so easy. So I tried the to split the integrand and study the part : $\int_{Z(g)}$ I get (using the second interpretation of $G$ ) : $$\frac{\int_{Z(g)} \mid th(t) \mid \mathrm{d}t}{t} $$ But it doesn't seem to help...",Today during an exam I got the following exercise : Let such that the set : is a finite union of intervals. Then let's defined : If the function has a derivative at prove that : First of all I don't understand how is defined because do we consider that : Or we consider that : ? Then I tried considering : In order to find the derivative. Yet the make the task not so easy. So I tried the to split the integrand and study the part : I get (using the second interpretation of ) : But it doesn't seem to help...,"h, g \in C^0([0,1], \| \cdot \|_1) Z(g) = \{x \in [0,1] \mid g(x) = 0\}  G : t \mapsto \| g + th \|_1 G 0 G'(0) = \int_{[0,1] \backslash Z(g)} h(x) \cdot sign(g(x)) \mathrm{d}x G G(t) = \int_{[0,1]} \mid g(x) + th(x) \mid \mathrm{d}x G(t) = \int_{[0,1]} \mid g(t) + th(t) \mid \mathrm{d}t \frac{G(h) - G(0)}{h} \mid \cdot \mid \int_{Z(g)} G \frac{\int_{Z(g)} \mid th(t) \mid \mathrm{d}t}{t} ","['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'derivatives']"
50,How to determine local extrema for $f(x) = x\cdot \sin(x) ^ {\sin(x)}$,How to determine local extrema for,f(x) = x\cdot \sin(x) ^ {\sin(x)},I need to find the local extrema points of the following function: $f(x)  = x\cdot\sin(x) ^ {\sin(x)}$ I was already able to derive to this function: $f'(x)  = x (\ln(\sin(x))+1)\cos(x)\sin(x)^{\sin(x)}+\sin(x)^ {\sin(x)}$,I need to find the local extrema points of the following function: I was already able to derive to this function:,f(x)  = x\cdot\sin(x) ^ {\sin(x)} f'(x)  = x (\ln(\sin(x))+1)\cos(x)\sin(x)^{\sin(x)}+\sin(x)^ {\sin(x)},"['calculus', 'derivatives', 'trigonometry', 'maxima-minima']"
51,"Is there anything that non-newtonian calculus can do, which newtonian calculus cannot?","Is there anything that non-newtonian calculus can do, which newtonian calculus cannot?",,"As well, are there problems where non-newtonian calculus leads to a more elegant or simple solution than regular calculus?","As well, are there problems where non-newtonian calculus leads to a more elegant or simple solution than regular calculus?",,"['calculus', 'derivatives', 'problem-solving']"
52,Differentiation under an integral sign,Differentiation under an integral sign,,"I found this integral in a solution of heat equation: $$I(b)=\int_{0}^{\infty}e^{-z^2}\cos(bz)dz$$ Differentiating in b, $$I'(b)=\int_{0}^{\infty}(-ze^{-z^2})\sin(bz)dz$$ How is the differentiation done in the integral sign ? This looks too simple. But is it correct?","I found this integral in a solution of heat equation: Differentiating in b, How is the differentiation done in the integral sign ? This looks too simple. But is it correct?",I(b)=\int_{0}^{\infty}e^{-z^2}\cos(bz)dz I'(b)=\int_{0}^{\infty}(-ze^{-z^2})\sin(bz)dz,"['integration', 'derivatives']"
53,How to use derivatives to prove that $f(x)=2\cos^2\left(\frac{\pi }{4}-\frac{x}{2}\right)-\sin \left(x\right)=1$?,How to use derivatives to prove that ?,f(x)=2\cos^2\left(\frac{\pi }{4}-\frac{x}{2}\right)-\sin \left(x\right)=1,"I'm supposed to use the derivative to prove that $$f(x)=2\cos ^2\left(\frac{\pi }{4}-\frac{x}{2}\right)-\sin \left(x\right)=1$$ What I have so far is: $$f'\left(x\right)=D\left(2\left(\cos\left(\frac{\pi}{4}-\frac{x}{2}\right)\right)^2\right)-D\left(\sin\left(x\right)\right)=1$$ $$f\:'\left(x\right)=4\cos\left(\frac{\pi}{4}-\frac{x}{2}\right)\left(-\frac{1}{2}\sin\left(\frac{\pi}{4}-\frac{x}{2}\right)\right)-\cos\left(x\right)=1$$ $$f\:'\left(x\right)=-2\sin\left(\frac{\pi}{4}-\frac{x}{2}\right)\cos\left(\frac{\pi}{4}-\frac{x}{2}\right)-\cos\left(x\right)=1$$ And by continuing this I can get all the way to zero, which would mean that $0=1$ and from there by taking the derivative of $1$ we'd get $0=0$ , which is obviously true. But what I'm confused about is that even if the constant was any number, let's say two, the equation would still come out as true. And obviously, the original function doesn't equal two. I'm assuming that I'm somehow supposed to get $1$ to the left side as well? Honestly, I've gone through the formulas so many times, and yet I can't figure out how to do it.","I'm supposed to use the derivative to prove that What I have so far is: And by continuing this I can get all the way to zero, which would mean that and from there by taking the derivative of we'd get , which is obviously true. But what I'm confused about is that even if the constant was any number, let's say two, the equation would still come out as true. And obviously, the original function doesn't equal two. I'm assuming that I'm somehow supposed to get to the left side as well? Honestly, I've gone through the formulas so many times, and yet I can't figure out how to do it.",f(x)=2\cos ^2\left(\frac{\pi }{4}-\frac{x}{2}\right)-\sin \left(x\right)=1 f'\left(x\right)=D\left(2\left(\cos\left(\frac{\pi}{4}-\frac{x}{2}\right)\right)^2\right)-D\left(\sin\left(x\right)\right)=1 f\:'\left(x\right)=4\cos\left(\frac{\pi}{4}-\frac{x}{2}\right)\left(-\frac{1}{2}\sin\left(\frac{\pi}{4}-\frac{x}{2}\right)\right)-\cos\left(x\right)=1 f\:'\left(x\right)=-2\sin\left(\frac{\pi}{4}-\frac{x}{2}\right)\cos\left(\frac{\pi}{4}-\frac{x}{2}\right)-\cos\left(x\right)=1 0=1 1 0=0 1,"['calculus', 'derivatives']"
54,if neither f nor g is differentiable at x=a. is $f+g$ differentiable at $x=a$?(2),if neither f nor g is differentiable at x=a. is  differentiable at ?(2),f+g x=a,"if neither f nor g is differentiable at x=a. is $f+g$ differentiable at $x=a$ ? My answer: yes it is differentiable , because according to this question here: if neither f nor g is differentiable at x=a. is $f+g$ differentiable at $x=a$ ? If $f$ is not differentiable then $-f$ is not differentiable (but I do not understand exactly why ? may be because f is not differentiable meaning that the limit of differentiability does not exist for f and hence it trivially does not exist also for -f ..... am I correct? ). now f+(-f) = 0 which is a differentiable function .... am I correct?","if neither f nor g is differentiable at x=a. is differentiable at ? My answer: yes it is differentiable , because according to this question here: if neither f nor g is differentiable at x=a. is differentiable at ? If is not differentiable then is not differentiable (but I do not understand exactly why ? may be because f is not differentiable meaning that the limit of differentiability does not exist for f and hence it trivially does not exist also for -f ..... am I correct? ). now f+(-f) = 0 which is a differentiable function .... am I correct?",f+g x=a f+g x=a f -f,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
55,Backpropagation with two hidden layers - matrix dimension doesn't add up,Backpropagation with two hidden layers - matrix dimension doesn't add up,,"I'm currently trying to create a neural network with 2 hidden layers from scratch. The input layer has 784 dimensions (MNIST dataset) The first hidden layer has 100 neurons using sigmoid activator The second hidden layer has 10 neurons using sigmoid activator The output layer has 10 possible outcomes (digit 0-9) using softmax activator I easily computed the output (third) layer derivative with respect to the final (third) weight matrix: $$\frac{\partial L}{\partial W_3} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z_3}  \frac{\partial z_3}{\partial W_3} = [1\ H_2]^T(\hat{y} - y)$$ $\frac{\partial L}{\partial z_3} = (\hat{y} - y)$ is the partial derivative of the loss function with respect to the softmax input. $\frac{\partial z3}{\partial z_3} = \frac{\partial}{\partial z_3} [1\ H_2]W_3 = [1\ H_2]^T$ is the partial derivative of the softmax input with respect to the weight matrix. I checked the dimension, and since $[1\ H_2]^T \in \mathbb{R}^{11\times1}$ and $(\hat{y} - y) \in \mathbb{R}^{1\times10}$ , $\frac{\partial L}{\partial W_3}\in \mathbb{R}^{11\times10}$ , which has the same dimension as $W_3$ . Thus, it would be possible to perform gradient descent. Next up, I want to compute the derivative of loss function with respect to the second layer's weight matrix: $$\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z_3}  \frac{\partial z_3}{\partial H_2} \frac{\partial H_2}{\partial z_2} \frac{\partial z_2}{\partial W_2}$$ $\frac{\partial L}{\partial z_3} = (\hat{y} - y) \in \mathbb{R}^{1\times10}$ $\frac{\partial z_3}{\partial H_2} = \frac{\partial}{\partial H_2} [1\ H_2]W_3=W_3^T \in \mathbb{R}^{10\times10}$ $\frac{\partial H_2}{\partial z_2} = \frac{\partial}{\partial z_2} \sigma(z_2)=\sigma(z_2)(1-\sigma(z_2))=H_2(1-H_2) \in \mathbb{R}^{10\times1}$ $\frac{\partial z_2}{\partial W_2} = \frac{\partial}{\partial W_2} [1\ H_1]W_2=[1\ H_1] \in \mathbb{R}^{1\times101}$ I can't merge these derivatives using chain rule into $Dim(W_2) \in \mathbb{R}^{101\times10}$ no matter what, which is needed for gradient descent. I feel like I'm missing something in my $\frac{\partial L}{\partial W_2}$ . Could someone please give me some insights?","I'm currently trying to create a neural network with 2 hidden layers from scratch. The input layer has 784 dimensions (MNIST dataset) The first hidden layer has 100 neurons using sigmoid activator The second hidden layer has 10 neurons using sigmoid activator The output layer has 10 possible outcomes (digit 0-9) using softmax activator I easily computed the output (third) layer derivative with respect to the final (third) weight matrix: is the partial derivative of the loss function with respect to the softmax input. is the partial derivative of the softmax input with respect to the weight matrix. I checked the dimension, and since and , , which has the same dimension as . Thus, it would be possible to perform gradient descent. Next up, I want to compute the derivative of loss function with respect to the second layer's weight matrix: I can't merge these derivatives using chain rule into no matter what, which is needed for gradient descent. I feel like I'm missing something in my . Could someone please give me some insights?",\frac{\partial L}{\partial W_3} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z_3}  \frac{\partial z_3}{\partial W_3} = [1\ H_2]^T(\hat{y} - y) \frac{\partial L}{\partial z_3} = (\hat{y} - y) \frac{\partial z3}{\partial z_3} = \frac{\partial}{\partial z_3} [1\ H_2]W_3 = [1\ H_2]^T [1\ H_2]^T \in \mathbb{R}^{11\times1} (\hat{y} - y) \in \mathbb{R}^{1\times10} \frac{\partial L}{\partial W_3}\in \mathbb{R}^{11\times10} W_3 \frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z_3}  \frac{\partial z_3}{\partial H_2} \frac{\partial H_2}{\partial z_2} \frac{\partial z_2}{\partial W_2} \frac{\partial L}{\partial z_3} = (\hat{y} - y) \in \mathbb{R}^{1\times10} \frac{\partial z_3}{\partial H_2} = \frac{\partial}{\partial H_2} [1\ H_2]W_3=W_3^T \in \mathbb{R}^{10\times10} \frac{\partial H_2}{\partial z_2} = \frac{\partial}{\partial z_2} \sigma(z_2)=\sigma(z_2)(1-\sigma(z_2))=H_2(1-H_2) \in \mathbb{R}^{10\times1} \frac{\partial z_2}{\partial W_2} = \frac{\partial}{\partial W_2} [1\ H_1]W_2=[1\ H_1] \in \mathbb{R}^{1\times101} Dim(W_2) \in \mathbb{R}^{101\times10} \frac{\partial L}{\partial W_2},"['calculus', 'linear-algebra', 'derivatives', 'neural-networks']"
56,Understanding definite integrals with functions as bounds [closed],Understanding definite integrals with functions as bounds [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I can't quite grasp the meaning behind definite integrals defined on two bounds, which appear as functions.  For instance, $$\int_{x^2}^{\cos x}t^2dt$$ What is this notation telling me? What does it mean that the lower bound is $x^2$ , and the upper bound is $\cos x$ ? Where does the definite integral ""stop"" and ""end"", if $x^2$ and $\cos x$ are not single values, but a collection of values? Wouldn't these $x$ values then overlap...? Moreover, when I wish to take the derivative of such an integral, how do I know that $0$ (or any specified constant of integration for that matter) exists ""between"" $x^2$ and $\cos x$ ? Very confused, my apologies. Thanks for any clarification you can provide.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I can't quite grasp the meaning behind definite integrals defined on two bounds, which appear as functions.  For instance, What is this notation telling me? What does it mean that the lower bound is , and the upper bound is ? Where does the definite integral ""stop"" and ""end"", if and are not single values, but a collection of values? Wouldn't these values then overlap...? Moreover, when I wish to take the derivative of such an integral, how do I know that (or any specified constant of integration for that matter) exists ""between"" and ? Very confused, my apologies. Thanks for any clarification you can provide.",\int_{x^2}^{\cos x}t^2dt x^2 \cos x x^2 \cos x x 0 x^2 \cos x,['derivatives']
57,How to use limit definition to find derivative with a radical,How to use limit definition to find derivative with a radical,,I'm trying to find the derivative of $f(x)=3\sqrt x$ at $25$ . How would you go about this using the limit definition of a derivative? I'm currently stuck at $(3\sqrt{25+h}+15)/h$ .,I'm trying to find the derivative of at . How would you go about this using the limit definition of a derivative? I'm currently stuck at .,f(x)=3\sqrt x 25 (3\sqrt{25+h}+15)/h,"['real-analysis', 'limits', 'derivatives']"
58,"Second ""Total Derivative"" of a Vector-Valued Function","Second ""Total Derivative"" of a Vector-Valued Function",,"I am working with a function $F : \mathbb{R}^3 \to \mathbb{R}^3$ and need to compute the vector of quadratic forms $Q$ given by $$Q(\textbf{u}) = \frac{1}{2} \textbf{u}^T\frac{d^2 F}{d\textbf{u}^2}(x,y,z)\textbf{u},$$ where $(x,y,z)$ is a point in $\mathbb{R}^3$ that should be consider to be fixed in the computation of $Q$ . I vaguely remember learning about higher order total derivatives in my graduate level introductory analysis course, but cannot seem to find my notes.  I guess what would be best is if someone could point me to a nice reference.","I am working with a function and need to compute the vector of quadratic forms given by where is a point in that should be consider to be fixed in the computation of . I vaguely remember learning about higher order total derivatives in my graduate level introductory analysis course, but cannot seem to find my notes.  I guess what would be best is if someone could point me to a nice reference.","F : \mathbb{R}^3 \to \mathbb{R}^3 Q Q(\textbf{u}) = \frac{1}{2} \textbf{u}^T\frac{d^2 F}{d\textbf{u}^2}(x,y,z)\textbf{u}, (x,y,z) \mathbb{R}^3 Q","['real-analysis', 'derivatives']"
59,Gradient of $X \mapsto \mbox{trace} \left( X^T A X \right)$,Gradient of,X \mapsto \mbox{trace} \left( X^T A X \right),"I think the question is close to Derivative of trace of inverse of a matrix function I have a function: $$f(X) = \mbox{trace} \left( X^T A X \right)$$ and I am trying to derive $df/dX$ Update: I know that, for function $g(X) = X^TAX$ $\frac{dg}{dX} = (A+A^T)X$ Then now I need to calculate $f(g(X))$ . I tried to apply chain-rule: $\frac{df}{dX} = \frac{df}{dg} * \frac{dg}{dX}$ And we already know $\frac{dg}{dX}$ . But what is $\frac{df}{dg}$ ?","I think the question is close to Derivative of trace of inverse of a matrix function I have a function: and I am trying to derive Update: I know that, for function Then now I need to calculate . I tried to apply chain-rule: And we already know . But what is ?",f(X) = \mbox{trace} \left( X^T A X \right) df/dX g(X) = X^TAX \frac{dg}{dX} = (A+A^T)X f(g(X)) \frac{df}{dX} = \frac{df}{dg} * \frac{dg}{dX} \frac{dg}{dX} \frac{df}{dg},"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
60,Are all derivatives vectors?,Are all derivatives vectors?,,"Before reading this question, let me just state that prior to this year, I had allways thought vectors were arrows in space or lists of numbers, and I am still getting accustomed to their more formal definition of meeting a list of rules in their operations, having a magnitude, and some ""direction"" (although that is not necessarilly a direction in physical space). My question is this: Are all derivatives vectors? Even of things that aren't? Allow me to explain: I was reading in a physics book that although angular displacement in itself is not a vector, angular velocity and acceleration is. This makes sense: Consider 2 rotations on an object, X and Y. $X$ spins the object around the X-axis and $Y$ spins the object around the Y-axis. Saying that we are going to produce $X+Y$ means we are going to spin the object around the X-axis and then the Y-axis while saying we are going to do $Y+X$ means we are going to rotate the object around the Y-axis and then around the X-axis. As you can see from the picture below, $X+Y\neq Y+X$ However, as the change becomes infinitely small, the prospect of applying $Y$ after $X$ or $X$ after $Y$ becomes meaningless, since there is an infinitely small amount of time between the application of each tiny rotation, meaning that $dX/dt + dY/dt    =     dX/dt + dX/dt$ . It doesn't matter whether we apply an infinitely small rotation in the X direction first or in the Y direction first, the rotations are infinitely small, so applying one after the other kind of loses all meaning. They are being applied at the same time. We can also justify other vector properties via the same reasoning, such as $C(w_{X} + w_{y}) = Cw_{X} + Cw_{y}$ ( $w$ is the symbol for rotational velocity) . While with overall rotations, multiplying by the constant made a difference in the final position, since the velocities are over infinitely small time intervals and are therefore being applied at the same time, scaling each of the velocities by a constant is the same thing as applying each of the scaled velocities one after the other (which has lost all meaning when we are referring to derivatives) and scaling them separately: it doesn't make a difference their ""magnitude"" relative to the other angular velocity, since at the end of the day, since we are considering infinitely small time intervals, they will still be happenning at the same time, unlike overall rotations which will be happenning one after the other and making one of the rotations greater may affect the total effect after applying the other. We can even think of more examples where this applies, such as (and this is a completely random example just to get my point across) the effects of temperature change, let's say. While raising the temperature to 1000 degrees and then lowering it to -500 will leave you as frozen ashes, but lowering to -500 and then raising it to 1000 will leave you with ashes surrounded by steam, doing each of them in infinitely small intervals, it doesn't matter if you start by raising the temperature or start by lowering it, at the end of the day you will have the same result (if the temperature is being raised faster than it is being lowered, the room will get hotter, and if it's being lowered faster than its being raised, the room will get colder, and if they're happening at the same time you won't end up as ashes OR frozen since the temperature won't change at all). Anyways, are there other examples such as this, where the derivative is a vector but the change in itself is not? Are all derivatives vectors? Thanks!","Before reading this question, let me just state that prior to this year, I had allways thought vectors were arrows in space or lists of numbers, and I am still getting accustomed to their more formal definition of meeting a list of rules in their operations, having a magnitude, and some ""direction"" (although that is not necessarilly a direction in physical space). My question is this: Are all derivatives vectors? Even of things that aren't? Allow me to explain: I was reading in a physics book that although angular displacement in itself is not a vector, angular velocity and acceleration is. This makes sense: Consider 2 rotations on an object, X and Y. spins the object around the X-axis and spins the object around the Y-axis. Saying that we are going to produce means we are going to spin the object around the X-axis and then the Y-axis while saying we are going to do means we are going to rotate the object around the Y-axis and then around the X-axis. As you can see from the picture below, However, as the change becomes infinitely small, the prospect of applying after or after becomes meaningless, since there is an infinitely small amount of time between the application of each tiny rotation, meaning that . It doesn't matter whether we apply an infinitely small rotation in the X direction first or in the Y direction first, the rotations are infinitely small, so applying one after the other kind of loses all meaning. They are being applied at the same time. We can also justify other vector properties via the same reasoning, such as ( is the symbol for rotational velocity) . While with overall rotations, multiplying by the constant made a difference in the final position, since the velocities are over infinitely small time intervals and are therefore being applied at the same time, scaling each of the velocities by a constant is the same thing as applying each of the scaled velocities one after the other (which has lost all meaning when we are referring to derivatives) and scaling them separately: it doesn't make a difference their ""magnitude"" relative to the other angular velocity, since at the end of the day, since we are considering infinitely small time intervals, they will still be happenning at the same time, unlike overall rotations which will be happenning one after the other and making one of the rotations greater may affect the total effect after applying the other. We can even think of more examples where this applies, such as (and this is a completely random example just to get my point across) the effects of temperature change, let's say. While raising the temperature to 1000 degrees and then lowering it to -500 will leave you as frozen ashes, but lowering to -500 and then raising it to 1000 will leave you with ashes surrounded by steam, doing each of them in infinitely small intervals, it doesn't matter if you start by raising the temperature or start by lowering it, at the end of the day you will have the same result (if the temperature is being raised faster than it is being lowered, the room will get hotter, and if it's being lowered faster than its being raised, the room will get colder, and if they're happening at the same time you won't end up as ashes OR frozen since the temperature won't change at all). Anyways, are there other examples such as this, where the derivative is a vector but the change in itself is not? Are all derivatives vectors? Thanks!",X Y X+Y Y+X X+Y\neq Y+X Y X X Y dX/dt + dY/dt    =     dX/dt + dX/dt C(w_{X} + w_{y}) = Cw_{X} + Cw_{y} w,"['derivatives', 'vectors', 'rotations']"
61,Differentiate $ e^{-[y/b]^a} $ respect to y?,Differentiate  respect to y?, e^{-[y/b]^a} ,"I would like to ask that, when I do the differentiation $ \frac{\partial }{ \partial y} e^{-[y/b]^a}$ , my answer is $a \times e^{-[y/b]^a} \times (-1/b) $ . Is this correct?  I am not sure whether $(-1/b)$ should be added. Thank you very much for reading!! Any suggestions are appreciated!","I would like to ask that, when I do the differentiation , my answer is . Is this correct?  I am not sure whether should be added. Thank you very much for reading!! Any suggestions are appreciated!", \frac{\partial }{ \partial y} e^{-[y/b]^a} a \times e^{-[y/b]^a} \times (-1/b)  (-1/b),['derivatives']
62,Proof of $\frac{\partial {det(\mathbf X)}}{\partial \mathbf X}=det(\mathbf X)(\mathbf X^{-1})^*$,Proof of,\frac{\partial {det(\mathbf X)}}{\partial \mathbf X}=det(\mathbf X)(\mathbf X^{-1})^*,"let X be a square, complex, and Hermitian matrix. Show that $\frac{\partial {det(\mathbf X)}}{\partial \mathbf X}=det(\mathbf X)(\mathbf X^{-1})^*$ and $\frac{\partial {det(\mathbf X)}}{\partial \mathbf X^*}=det(\mathbf X)(\mathbf X^{-1})$ Can anyone teach me how to prove this?","let X be a square, complex, and Hermitian matrix. Show that and Can anyone teach me how to prove this?",\frac{\partial {det(\mathbf X)}}{\partial \mathbf X}=det(\mathbf X)(\mathbf X^{-1})^* \frac{\partial {det(\mathbf X)}}{\partial \mathbf X^*}=det(\mathbf X)(\mathbf X^{-1}),"['matrices', 'derivatives']"
63,Derivative of a vector times its transpose,Derivative of a vector times its transpose,,"I am trying to work out how to solve a derivative of the form: $${d \over dx}(M(x)M(x)^T)$$ where M is a vector. In my case specifically, M is the (1x3) vector $$M(x)={df(x) \over dx}$$ where f(x) is a scalar and x is a (3x1) vector. Could I just use the product rule like this: $${d(uv) \over dx}=u{dv \over dx}+v{du \over dx}$$ if $M(x)=u$ and $M(x)^T=v$ then $${d \over dx}(M(x)M(x)^T)=M(x){d(M(x)^T) \over dx}+M(x)^T{d(M(x)) \over dx}$$ $${d \over dx}(M(x)M(x)^T)=M(x){d(M(x)) \over dx}^T+M(x)^T{d(M(x)) \over dx}$$ $${d \over dx}(M(x)M(x)^T)={df(x) \over dx}({{d^2 f(x) \over dx^2}})^T+({df(x) \over dx})^T{{d^2 f(x) \over dx^2}}$$ I assume that I made a few mistakes above. Whether you help with the general solution or my problem specifically it would be much appreciated!","I am trying to work out how to solve a derivative of the form: where M is a vector. In my case specifically, M is the (1x3) vector where f(x) is a scalar and x is a (3x1) vector. Could I just use the product rule like this: if and then I assume that I made a few mistakes above. Whether you help with the general solution or my problem specifically it would be much appreciated!",{d \over dx}(M(x)M(x)^T) M(x)={df(x) \over dx} {d(uv) \over dx}=u{dv \over dx}+v{du \over dx} M(x)=u M(x)^T=v {d \over dx}(M(x)M(x)^T)=M(x){d(M(x)^T) \over dx}+M(x)^T{d(M(x)) \over dx} {d \over dx}(M(x)M(x)^T)=M(x){d(M(x)) \over dx}^T+M(x)^T{d(M(x)) \over dx} {d \over dx}(M(x)M(x)^T)={df(x) \over dx}({{d^2 f(x) \over dx^2}})^T+({df(x) \over dx})^T{{d^2 f(x) \over dx^2}},"['calculus', 'derivatives', 'vector-analysis', 'matrix-calculus']"
64,Matrix differentiation: $\frac{\partial{w}}{\alpha}$ for $w=(X^\top X + \alpha \textbf{I})^{-1}X^\top y$,Matrix differentiation:  for,\frac{\partial{w}}{\alpha} w=(X^\top X + \alpha \textbf{I})^{-1}X^\top y,"What is $\frac{\partial{w}}{\partial{\alpha}}$ for $w=(X^\top X + \alpha \boldsymbol{I})^{-1} X^\top y$ where X is an $N \times D$ matrix, y is an N dimensional vector, $\boldsymbol{I}$ is an identity matrix of size $D \times D$ and $\alpha$ is a scalar? edit: Actually I was trying to differentiate $\mathcal{F} (\hat{w}(\alpha)) = (\boldsymbol{y}-X\hat{\boldsymbol{w}})^\top (\boldsymbol{y}-X\hat{\boldsymbol{w}})+ \alpha (||{\hat{\boldsymbol{w}}||}^2 - c^2)$ w.r.t $\alpha$ . Can I do $\dfrac{\partial{\mathcal{F} (\hat{w}(\alpha))}}{\partial{\alpha}}= \dfrac{\partial{\mathcal{F} (\hat{w}(\alpha))}}{\partial{\hat{w}}}\times  \dfrac{\partial{\hat{w}}}{\partial{\alpha}}$ ? if I do so, then $\dfrac{\partial{\mathcal{F} (\hat{w}(\alpha))}}{\partial{\alpha}}= (-2X^\top y + 2X^\top X \hat{w} )\dfrac{\partial{\hat{w}}}{\partial{\alpha}} + (||{\hat{\boldsymbol{w}}||}^2 - c^2)+ (2\alpha\hat{w}) \dfrac{\partial{\hat{w}}}{\partial{\alpha}} $ But the dimension of $\dfrac{\partial{\hat{w}}}{\partial{\alpha}}$ is $D \times 1$ since its expression is $-(X^\top X +\alpha \boldsymbol{I})^{-1}\boldsymbol{\hat{w}}$ and that of $\boldsymbol{\hat{w}}$ is also $D \times 1$ so they can't be multiplied in the order seen in the third term. Is there a problem with the chain rule?","What is for where X is an matrix, y is an N dimensional vector, is an identity matrix of size and is a scalar? edit: Actually I was trying to differentiate w.r.t . Can I do ? if I do so, then But the dimension of is since its expression is and that of is also so they can't be multiplied in the order seen in the third term. Is there a problem with the chain rule?",\frac{\partial{w}}{\partial{\alpha}} w=(X^\top X + \alpha \boldsymbol{I})^{-1} X^\top y N \times D \boldsymbol{I} D \times D \alpha \mathcal{F} (\hat{w}(\alpha)) = (\boldsymbol{y}-X\hat{\boldsymbol{w}})^\top (\boldsymbol{y}-X\hat{\boldsymbol{w}})+ \alpha (||{\hat{\boldsymbol{w}}||}^2 - c^2) \alpha \dfrac{\partial{\mathcal{F} (\hat{w}(\alpha))}}{\partial{\alpha}}= \dfrac{\partial{\mathcal{F} (\hat{w}(\alpha))}}{\partial{\hat{w}}}\times  \dfrac{\partial{\hat{w}}}{\partial{\alpha}} \dfrac{\partial{\mathcal{F} (\hat{w}(\alpha))}}{\partial{\alpha}}= (-2X^\top y + 2X^\top X \hat{w} )\dfrac{\partial{\hat{w}}}{\partial{\alpha}} + (||{\hat{\boldsymbol{w}}||}^2 - c^2)+ (2\alpha\hat{w}) \dfrac{\partial{\hat{w}}}{\partial{\alpha}}  \dfrac{\partial{\hat{w}}}{\partial{\alpha}} D \times 1 -(X^\top X +\alpha \boldsymbol{I})^{-1}\boldsymbol{\hat{w}} \boldsymbol{\hat{w}} D \times 1,"['matrices', 'derivatives', 'matrix-calculus']"
65,"Let $f$ be continuous on $[a,b]$, be differentiable on $(a,b)$.","Let  be continuous on , be differentiable on .","f [a,b] (a,b)","Let $f$ be continuous on $[a,b]$ , be differentiable on $(a,b)$ . If $f$ is strictly increasing in a neighborhood of $a$ .  Can we show that $f'(x)$ is bounded in  a neighborhood of $a$ ? If $f$ is not strictly increasing in a neighborhood of $a$ . Then $f(x)=x \sin 1/x$ shows that it is wrong. However, if we add '' If $f$ is strictly increasing in a neighborhood of $a$ '', is it right then?","Let be continuous on , be differentiable on . If is strictly increasing in a neighborhood of .  Can we show that is bounded in  a neighborhood of ? If is not strictly increasing in a neighborhood of . Then shows that it is wrong. However, if we add '' If is strictly increasing in a neighborhood of '', is it right then?","f [a,b] (a,b) f a f'(x) a f a f(x)=x \sin 1/x f a","['calculus', 'derivatives', 'continuity']"
66,Spivak Physics for Mathematicians - Help understanding the derivative of a one-parameter family of linear transformations.,Spivak Physics for Mathematicians - Help understanding the derivative of a one-parameter family of linear transformations.,,"Most physics textbooks I've consulted only confused me more with regards to how to understand rotating coordinate systems so I consulted Spivak's book but I have 2 questions; in particular regarding the derivative near the bottom of the page. $r' = B(\rho') + B'(\rho)$ Even if we write it out in uncondensed notation, the formula reads $r'(t) = B(t)(\rho'(t)) + B'(t)(\rho(t))$. Firstly I am not sure how he gets this formula, because it seems like he used a product rule of sorts, which I am not sure how/why it even applies. And secondly, I'm not sure how to interpret the term $B'(t)$, because from my understanding, $B$ is a function from $\mathbb{R}_{\geq0}$ into Hom$(\mathbb{R}^3)$, so how can we differentiate such an object? I know that the derivative of a function from say from $\mathbb{R}^n$ into $\mathbb{R}^m$ is a linear transformation, but what about the case where the target space is a general vector space? So if someone could explain this step in more detail, that would be much appreciated :)","Most physics textbooks I've consulted only confused me more with regards to how to understand rotating coordinate systems so I consulted Spivak's book but I have 2 questions; in particular regarding the derivative near the bottom of the page. $r' = B(\rho') + B'(\rho)$ Even if we write it out in uncondensed notation, the formula reads $r'(t) = B(t)(\rho'(t)) + B'(t)(\rho(t))$. Firstly I am not sure how he gets this formula, because it seems like he used a product rule of sorts, which I am not sure how/why it even applies. And secondly, I'm not sure how to interpret the term $B'(t)$, because from my understanding, $B$ is a function from $\mathbb{R}_{\geq0}$ into Hom$(\mathbb{R}^3)$, so how can we differentiate such an object? I know that the derivative of a function from say from $\mathbb{R}^n$ into $\mathbb{R}^m$ is a linear transformation, but what about the case where the target space is a general vector space? So if someone could explain this step in more detail, that would be much appreciated :)",,"['derivatives', 'linear-transformations', 'physics', 'coordinate-systems']"
67,How to deduce that the rotation matrix is constant from the following?,How to deduce that the rotation matrix is constant from the following?,,"As it is clear for everyone the deformation gradient $\mathbf{F}$ can be decomposed into, $\mathbf{F}=\mathbf{Q} \cdot \mathbf{U}$ where $\mathbf{Q}$ is the rotation matrix. Now if I consider $\mathbf{U}=\mathbf{1}$, I can write $\displaystyle Q_{ij}=\frac{\partial y_i}{\partial x_j}$. According to the Schwarz integrability condition we have, $\displaystyle \frac{\partial Q_{ij}}{\partial x_k}=\frac{\partial Q_{ik}}{\partial x_j}$. Is it possible to show that, since $\mathbf{Q}$ is a rotation matrix, the only possibility consistent with the Schwatz integrability condition is $\mathbf{Q}=$constant? The above has been taken from a book. But I don't know how it has deduced $\mathbf{Q}$ is constant.","As it is clear for everyone the deformation gradient $\mathbf{F}$ can be decomposed into, $\mathbf{F}=\mathbf{Q} \cdot \mathbf{U}$ where $\mathbf{Q}$ is the rotation matrix. Now if I consider $\mathbf{U}=\mathbf{1}$, I can write $\displaystyle Q_{ij}=\frac{\partial y_i}{\partial x_j}$. According to the Schwarz integrability condition we have, $\displaystyle \frac{\partial Q_{ij}}{\partial x_k}=\frac{\partial Q_{ik}}{\partial x_j}$. Is it possible to show that, since $\mathbf{Q}$ is a rotation matrix, the only possibility consistent with the Schwatz integrability condition is $\mathbf{Q}=$constant? The above has been taken from a book. But I don't know how it has deduced $\mathbf{Q}$ is constant.",,"['derivatives', 'optimization', 'rotations', 'schwartz-space']"
68,second order derivative of log det of matrix,second order derivative of log det of matrix,,I need to find the second order derivative of a matrix. $f(\pmb{\Delta})=\log \det(\pmb{I}+k\pmb{V^T \Delta V})$ Where $ \pmb{\Delta}$ is a triangular matrix. I did the first order derivative but not sure if is correct. $$\frac{\partial f(\pmb{\Delta})}{\partial  [\pmb{\Delta}]_{ii}} =   \left[ k\pmb{V} \left( \pmb{I}+k\pmb{V^H}\pmb{\Delta}\pmb{V} \right)^{-1}\pmb{V^H} \right]_{ii}  $$ This is correct? How about the second order derivative?,I need to find the second order derivative of a matrix. $f(\pmb{\Delta})=\log \det(\pmb{I}+k\pmb{V^T \Delta V})$ Where $ \pmb{\Delta}$ is a triangular matrix. I did the first order derivative but not sure if is correct. $$\frac{\partial f(\pmb{\Delta})}{\partial  [\pmb{\Delta}]_{ii}} =   \left[ k\pmb{V} \left( \pmb{I}+k\pmb{V^H}\pmb{\Delta}\pmb{V} \right)^{-1}\pmb{V^H} \right]_{ii}  $$ This is correct? How about the second order derivative?,,"['matrices', 'derivatives', 'determinant', 'vector-analysis', 'matrix-calculus']"
69,Clarifying definition of maximum/minimum point,Clarifying definition of maximum/minimum point,,"Can a point be considered maximum/minimum if the graph ends at that point? Consider the following image. Point A is a typical maximum point. At that point, $\frac{dy}{dx}= 0$ and $\frac{d^2y}{dx^2} < 0$. Now consider this image. What about Point B? At Point B, both the conditions $\frac{dy}{dx}= 0$ and $\frac{d^2y}{dx^2} < 0$ are also fulfilled. Yet, we don't usually think of it as a ""maximum"" point. Is it actually one? Am I missing something?","Can a point be considered maximum/minimum if the graph ends at that point? Consider the following image. Point A is a typical maximum point. At that point, $\frac{dy}{dx}= 0$ and $\frac{d^2y}{dx^2} < 0$. Now consider this image. What about Point B? At Point B, both the conditions $\frac{dy}{dx}= 0$ and $\frac{d^2y}{dx^2} < 0$ are also fulfilled. Yet, we don't usually think of it as a ""maximum"" point. Is it actually one? Am I missing something?",,"['calculus', 'derivatives']"
70,Proving $V(x) = \frac{|y|^2-|x|^2}{|y-x|^N}$ is harmonic in $\mathbb{R}^N-\{y\}$,Proving  is harmonic in,V(x) = \frac{|y|^2-|x|^2}{|y-x|^N} \mathbb{R}^N-\{y\},"$$V(x) = \frac{|y|^2-|x|^2}{|y-x|^N}$$ for fixed $y\in\mathbb{R}^n$, and $x\in\mathbb{R}^n-\{y\}$ I need to prove that this this is harmonic. That is, the sum of its second partial derivatives are $0$. Let's calculate them. $$\left(\frac{f}{g}\right)' = \frac{fg'-gf'}{g^2}$$ so: $$\frac{\partial V(x)}{\partial x_j} = \frac{(|y|^2-|x|^2)N|y-x|^{N-1}\frac{(-x_j)}{|y-x|} - |y-x|^N(-2|x|\frac{x_j}{|x|})}{|y-x|^{2N}} = \\\frac{-Nx_j(|y|^2-|x|^2)|y-x|^{N-2}+2x_j|y-x|^N}{|y-x|^{2N}}$$ so $$\frac{\partial^2 V(x)}{\partial x_j^2} = \frac{\left(-Nx_j(|y|^2-|x|^2)|y-x|^{N-2}+2x_j|y-x|^N\right)\left(2N|y-x|^{2N-1}\frac{(-x_j)}{|y-x|}\right)-|y-x|^{2N}\left(\cdots\right)}{|y-x|^{4N^2}}$$ where $\left(\cdots\right)$ is the derivative of $\left(-Nx_j(|y|^2-|x|^2)|y-x|^{N-2}+2x_j|y-x|^N\right)$. It has a product of three things, and another term. It's huge. Multiplied by the other things, it's even bigger. I think this is not the right way to solve this exercise. Is there a better way? If not, how can I do these giant calculations by hand? It's too already in the computer using LaTeX.","$$V(x) = \frac{|y|^2-|x|^2}{|y-x|^N}$$ for fixed $y\in\mathbb{R}^n$, and $x\in\mathbb{R}^n-\{y\}$ I need to prove that this this is harmonic. That is, the sum of its second partial derivatives are $0$. Let's calculate them. $$\left(\frac{f}{g}\right)' = \frac{fg'-gf'}{g^2}$$ so: $$\frac{\partial V(x)}{\partial x_j} = \frac{(|y|^2-|x|^2)N|y-x|^{N-1}\frac{(-x_j)}{|y-x|} - |y-x|^N(-2|x|\frac{x_j}{|x|})}{|y-x|^{2N}} = \\\frac{-Nx_j(|y|^2-|x|^2)|y-x|^{N-2}+2x_j|y-x|^N}{|y-x|^{2N}}$$ so $$\frac{\partial^2 V(x)}{\partial x_j^2} = \frac{\left(-Nx_j(|y|^2-|x|^2)|y-x|^{N-2}+2x_j|y-x|^N\right)\left(2N|y-x|^{2N-1}\frac{(-x_j)}{|y-x|}\right)-|y-x|^{2N}\left(\cdots\right)}{|y-x|^{4N^2}}$$ where $\left(\cdots\right)$ is the derivative of $\left(-Nx_j(|y|^2-|x|^2)|y-x|^{N-2}+2x_j|y-x|^N\right)$. It has a product of three things, and another term. It's huge. Multiplied by the other things, it's even bigger. I think this is not the right way to solve this exercise. Is there a better way? If not, how can I do these giant calculations by hand? It's too already in the computer using LaTeX.",,"['calculus', 'derivatives', 'partial-differential-equations', 'partial-derivative']"
71,Find the partial derivative of a weighted sum function,Find the partial derivative of a weighted sum function,,"I am a high school student who is having problems understanding a resource which I am using for a research project. I basically am trying to get a derivative of a weighted sum function in relation to a particular value of weight (particular index of the wjk matrix). $W_{jk}$ is a specific index in a matrix so can be thought of as a numerical value. $$\frac{\partial}{\partial W_{jk}}\left(\sum_{j}W_{jk}\cdot O_j\right)$$ My resource states that $O_j$ is the derivative. I think because we are looking at the derivative of the function output in relation to a specific index ($W_{jk}$), the summation can be ignored/dropped as it takes in values which as I understand, don't influence how the particular index ($W_{jk}$) changes the function output. Or this is at least how I understand it (the book explains the removal of another summation in a similar way). My resource gives and answer but doesn't explain the process so I am left scratching my head. I would appreciate any help!","I am a high school student who is having problems understanding a resource which I am using for a research project. I basically am trying to get a derivative of a weighted sum function in relation to a particular value of weight (particular index of the wjk matrix). $W_{jk}$ is a specific index in a matrix so can be thought of as a numerical value. $$\frac{\partial}{\partial W_{jk}}\left(\sum_{j}W_{jk}\cdot O_j\right)$$ My resource states that $O_j$ is the derivative. I think because we are looking at the derivative of the function output in relation to a specific index ($W_{jk}$), the summation can be ignored/dropped as it takes in values which as I understand, don't influence how the particular index ($W_{jk}$) changes the function output. Or this is at least how I understand it (the book explains the removal of another summation in a similar way). My resource gives and answer but doesn't explain the process so I am left scratching my head. I would appreciate any help!",,"['derivatives', 'summation', 'partial-derivative']"
72,Determine this limit using L'Hopitals rule [duplicate],Determine this limit using L'Hopitals rule [duplicate],,"This question already has answers here : Evaluating: $\lim\limits_{x\to0}\left(\frac{\sin x}{x}\right)^{{6}/{x^{2}}}$ (5 answers) Closed 2 years ago . I couldn't find a way to get the answer for $$\lim\limits_{x \to 0} \left(\frac{\sin x}{x}\right)^{\frac{1}{x^{2}}}$$ From my knowledge of L'Hopital's Rule, I see that this is some kind of $1^{\infty}$ indeterminate form since I know from previous results that $\lim\limits_{x \to 0} \left(\frac{\sin x}{x}\right)=1$. Proceeded to find the limit of its natural $\log$ which is $$\lim\limits_{x \to 0} \left( \frac{\ln(\frac{\sin x}{x})}{x^{2}}\right)$$ then got stuck when I got to $$\lim\limits_{x \to 0} \left( \frac{\cot x-\frac{1}{x}}{2x}\right)$$ as I now get $\infty/0$ and this hasn't happened to me before since I just started not long ago on this topic. Can someone give me a further hint as to which direction I should head to or recommend me another more suitable approach to solve this problem? If it helps, the given answer is $e^{-1/6}$.","This question already has answers here : Evaluating: $\lim\limits_{x\to0}\left(\frac{\sin x}{x}\right)^{{6}/{x^{2}}}$ (5 answers) Closed 2 years ago . I couldn't find a way to get the answer for $$\lim\limits_{x \to 0} \left(\frac{\sin x}{x}\right)^{\frac{1}{x^{2}}}$$ From my knowledge of L'Hopital's Rule, I see that this is some kind of $1^{\infty}$ indeterminate form since I know from previous results that $\lim\limits_{x \to 0} \left(\frac{\sin x}{x}\right)=1$. Proceeded to find the limit of its natural $\log$ which is $$\lim\limits_{x \to 0} \left( \frac{\ln(\frac{\sin x}{x})}{x^{2}}\right)$$ then got stuck when I got to $$\lim\limits_{x \to 0} \left( \frac{\cot x-\frac{1}{x}}{2x}\right)$$ as I now get $\infty/0$ and this hasn't happened to me before since I just started not long ago on this topic. Can someone give me a further hint as to which direction I should head to or recommend me another more suitable approach to solve this problem? If it helps, the given answer is $e^{-1/6}$.",,"['calculus', 'limits', 'derivatives']"
73,Approximation to the second derivative of a function through second differences,Approximation to the second derivative of a function through second differences,,"Professor Strang introduces approximations to second derivatives through second differences after clearly explaining the idea of first derivatives approximated by either of three first differences: $$\begin{align} \text{forward:           }&\Delta_F U=\frac{U(x+h)-U(x)}{h}\tag1\\ \text{backward:           }&\Delta_B U=\frac{U(x)-U(x-h)}{h}\tag 2\\ \text{centered:           }&\Delta_C U=\frac{U(x+h)-U(x-h)}{2h}\\ \end{align}$$ This is clear, and the advantage of centering is explained well. However, there is a bit of a leap in the explanation of second derivatives: $$\frac{d^2U}{dx^2}\approx\frac{-U_{i+1}+2U_i-U_{i-1}}{(\Delta x)^2}\\$$ The numerator seems to arise from the substraction of the numerators of Eq. 2 from Eq.1 with a change of sign. But it is not clear why this works. It does look like taking two adjacent approximations to the first derivative side-by-side, as opposed to the difference of a difference, or a multiplication of $\Delta_F\Delta_B.$ The squared delta in the denominator will presumably follow from the answer to the problem with the numerator. What is the explanation as to why this approximation of the second derivative through second differences works?","Professor Strang introduces approximations to second derivatives through second differences after clearly explaining the idea of first derivatives approximated by either of three first differences: $$\begin{align} \text{forward:           }&\Delta_F U=\frac{U(x+h)-U(x)}{h}\tag1\\ \text{backward:           }&\Delta_B U=\frac{U(x)-U(x-h)}{h}\tag 2\\ \text{centered:           }&\Delta_C U=\frac{U(x+h)-U(x-h)}{2h}\\ \end{align}$$ This is clear, and the advantage of centering is explained well. However, there is a bit of a leap in the explanation of second derivatives: $$\frac{d^2U}{dx^2}\approx\frac{-U_{i+1}+2U_i-U_{i-1}}{(\Delta x)^2}\\$$ The numerator seems to arise from the substraction of the numerators of Eq. 2 from Eq.1 with a change of sign. But it is not clear why this works. It does look like taking two adjacent approximations to the first derivative side-by-side, as opposed to the difference of a difference, or a multiplication of $\Delta_F\Delta_B.$ The squared delta in the denominator will presumably follow from the answer to the problem with the numerator. What is the explanation as to why this approximation of the second derivative through second differences works?",,"['calculus', 'linear-algebra', 'derivatives']"
74,transpose appearing for the chain rule in the matrix form,transpose appearing for the chain rule in the matrix form,,"I am trying to understand deriving the derivative of a matrix equation of the form: $$a = \tanh(WX + b)$$ in which $W$ is a $M*N$ matrix, $X$ is $N*1$, and $b$ is $M*1$. I'm trying to take the derivative of $a$ with respect to $W$, $X$, and $b$. I already have the final answers as: $$\partial a/ \partial X= W^T(1 - \tanh(WX+b)^2)$$ I don't understand how $W$ moves to the left hand side of $(1 - \tanh(WX+b)^2$ and gets transposed!? I understand that the chain rule is: $$\partial f(u)/ \partial x= f'(u)\partial u/ \partial x$$ so in my example $\partial u/ \partial x$ is on the right hand side of the equation. $$\partial a/ \partial W=(1 - \tanh(WX+b)^2)X^T$$ in which I don't understand how $X$ gets transposed and moves to the left hand side.","I am trying to understand deriving the derivative of a matrix equation of the form: $$a = \tanh(WX + b)$$ in which $W$ is a $M*N$ matrix, $X$ is $N*1$, and $b$ is $M*1$. I'm trying to take the derivative of $a$ with respect to $W$, $X$, and $b$. I already have the final answers as: $$\partial a/ \partial X= W^T(1 - \tanh(WX+b)^2)$$ I don't understand how $W$ moves to the left hand side of $(1 - \tanh(WX+b)^2$ and gets transposed!? I understand that the chain rule is: $$\partial f(u)/ \partial x= f'(u)\partial u/ \partial x$$ so in my example $\partial u/ \partial x$ is on the right hand side of the equation. $$\partial a/ \partial W=(1 - \tanh(WX+b)^2)X^T$$ in which I don't understand how $X$ gets transposed and moves to the left hand side.",,"['calculus', 'derivatives', 'matrix-calculus']"
75,Implicit Differentiation Misunderstanding.,Implicit Differentiation Misunderstanding.,,"Say we are given the unit circle, defined by all real numbers $x,y$ such that $x^2+y^2=1$. Now, if one tried to solve explicitly for $y$, one would get that $y=\pm{\sqrt{1-x^2}}$ and this is obviously not a function, so traditional calculus techniques cannot be used. As $x^2+y^2=1$ defines our curve, we can take the derivative of all terms with respect to $x$ to get that $2x+2y\frac{dy}{dx}=0$, as by the chain rule $\frac{d(y^2)}{dx}=\frac{d(y^2)}{dy}\frac{dy}{dx}$, and one can therefore do some algebraic manipulation to get that $\frac{dy}{dx}=-\frac{x}{y}$. However, I have a few questions: 1) I thought the whole point was that $y$ is not a function, and so $\frac{dy}{dx}$ does not make sense as this notation is defined only when $y$ is a function (or am I wrong in this regard?) 2) Why are we allowed to take the derivative with respect to $x$ on both sides of the equation, but not for example to solve $x^2=1$ to get $2x=0$ which would produce the wrong answer that $x=0$ (my inkling tells me that this is because $x^2=1$ is not an identity but an equation, but I just want to check up on this...)","Say we are given the unit circle, defined by all real numbers $x,y$ such that $x^2+y^2=1$. Now, if one tried to solve explicitly for $y$, one would get that $y=\pm{\sqrt{1-x^2}}$ and this is obviously not a function, so traditional calculus techniques cannot be used. As $x^2+y^2=1$ defines our curve, we can take the derivative of all terms with respect to $x$ to get that $2x+2y\frac{dy}{dx}=0$, as by the chain rule $\frac{d(y^2)}{dx}=\frac{d(y^2)}{dy}\frac{dy}{dx}$, and one can therefore do some algebraic manipulation to get that $\frac{dy}{dx}=-\frac{x}{y}$. However, I have a few questions: 1) I thought the whole point was that $y$ is not a function, and so $\frac{dy}{dx}$ does not make sense as this notation is defined only when $y$ is a function (or am I wrong in this regard?) 2) Why are we allowed to take the derivative with respect to $x$ on both sides of the equation, but not for example to solve $x^2=1$ to get $2x=0$ which would produce the wrong answer that $x=0$ (my inkling tells me that this is because $x^2=1$ is not an identity but an equation, but I just want to check up on this...)",,"['calculus', 'derivatives', 'implicit-differentiation']"
76,Doubt in quotient rule for derivatives.,Doubt in quotient rule for derivatives.,,"I'm reading Rudin, and in theorem 5.3 he states that: Suppose $f,g$ are real functions on $[a,b]$ that are differentiable at $x \in [a,b]$. Then $f/g$ is differentiable at $x$, provided that $g(x) \neq 0$ and $$\left(\frac{f}{g}\right)'(x) = \frac{g(x)f'(x) - g'(x)f(x)}{g^2(x)}$$ Following Rudin's convention, the domain of $f/g$ are those point $x$ of $[a,b]$ where $g(x) \neq 0$. However, Rudin's definition of differentiability only mentions the cases where $f$ is differentiable on an open or closed interval. How do I interpret/solve this? For completeness, here is the relevant definition: Let $f$ be defined (and real-valued) on $[a,b]$. For any $x \in [a,b]$, form the quotient $$\phi(t) = \frac{f(t) - f(x)}{t-x}\quad (a < t <b, t \neq x)$$ and define $$f'(x) = \lim_{t \to x} \phi(t)$$ Maybe I'm just worrying too much..","I'm reading Rudin, and in theorem 5.3 he states that: Suppose $f,g$ are real functions on $[a,b]$ that are differentiable at $x \in [a,b]$. Then $f/g$ is differentiable at $x$, provided that $g(x) \neq 0$ and $$\left(\frac{f}{g}\right)'(x) = \frac{g(x)f'(x) - g'(x)f(x)}{g^2(x)}$$ Following Rudin's convention, the domain of $f/g$ are those point $x$ of $[a,b]$ where $g(x) \neq 0$. However, Rudin's definition of differentiability only mentions the cases where $f$ is differentiable on an open or closed interval. How do I interpret/solve this? For completeness, here is the relevant definition: Let $f$ be defined (and real-valued) on $[a,b]$. For any $x \in [a,b]$, form the quotient $$\phi(t) = \frac{f(t) - f(x)}{t-x}\quad (a < t <b, t \neq x)$$ and define $$f'(x) = \lim_{t \to x} \phi(t)$$ Maybe I'm just worrying too much..",,['real-analysis']
77,Complicated differential operator and application of the Zassenhaus formula,Complicated differential operator and application of the Zassenhaus formula,,"I have a differential operator $$D_x =  \frac{1}{x} \left[ (x^2 - a^2) \frac{d}{dx} \left( \frac{1}{x} \frac{d}{dx} \right) + 2 \frac{d }{dx} \right], $$ and I would like to compute $$ e^{i \lambda D_x} ~j_0(x) = \sum_{n=0}^\infty \frac{(i\lambda)^n}{n!} [D_x]^n ~j_0(x), $$ where $~j_0(x) = \sin(x)/x~$ is the spherical Bessel function.  Is there some nice way I could do this? I tried using the Zassenhaus formula , using the splitting $$ D_x = A_x + B_x, ~~{\rm where}~~  A_x=\frac{(x^2 - a^2)}{x}\frac{d}{dx} \left( \frac{1}{x} \frac{d}{dx} \right), ~~B_x = \frac{2}{x} \frac{d }{dx},$$ but it does not seem like a promising path, given that higher commutators $[A_x,B_x],~[A_x,[A_x,B_x]],\ldots$ get more and more complicated. I also tried just brute force derivation that would give me a  series representation as  $$\sum_{n=0}^\infty \frac{(i\lambda)^n}{n!} [D_x]^n ~j_0(x) = \sum_{n=0}^\infty \frac{(i\lambda)^n}{n!} \left[ f_n(x) \sin(x) + g_n(x) \cos(x) \right],$$ but I couldn't find close formulas for $f_n(x)$ and $g_n(x)$ functions. What else could I do?","I have a differential operator $$D_x =  \frac{1}{x} \left[ (x^2 - a^2) \frac{d}{dx} \left( \frac{1}{x} \frac{d}{dx} \right) + 2 \frac{d }{dx} \right], $$ and I would like to compute $$ e^{i \lambda D_x} ~j_0(x) = \sum_{n=0}^\infty \frac{(i\lambda)^n}{n!} [D_x]^n ~j_0(x), $$ where $~j_0(x) = \sin(x)/x~$ is the spherical Bessel function.  Is there some nice way I could do this? I tried using the Zassenhaus formula , using the splitting $$ D_x = A_x + B_x, ~~{\rm where}~~  A_x=\frac{(x^2 - a^2)}{x}\frac{d}{dx} \left( \frac{1}{x} \frac{d}{dx} \right), ~~B_x = \frac{2}{x} \frac{d }{dx},$$ but it does not seem like a promising path, given that higher commutators $[A_x,B_x],~[A_x,[A_x,B_x]],\ldots$ get more and more complicated. I also tried just brute force derivation that would give me a  series representation as  $$\sum_{n=0}^\infty \frac{(i\lambda)^n}{n!} [D_x]^n ~j_0(x) = \sum_{n=0}^\infty \frac{(i\lambda)^n}{n!} \left[ f_n(x) \sin(x) + g_n(x) \cos(x) \right],$$ but I couldn't find close formulas for $f_n(x)$ and $g_n(x)$ functions. What else could I do?",,"['derivatives', 'noncommutative-algebra']"
78,What is the meaning of $ \mathbb{R} \bmod T$?,What is the meaning of ?, \mathbb{R} \bmod T,What is the meaning of $ \mathbb{R}\bmod\ T$ for a fixed $T>0$. The set of all  equivalence classes? And how can I derive a function of the form $f:\mathbb{R}^d \to  \mathbb{R}\bmod T$?,What is the meaning of $ \mathbb{R}\bmod\ T$ for a fixed $T>0$. The set of all  equivalence classes? And how can I derive a function of the form $f:\mathbb{R}^d \to  \mathbb{R}\bmod T$?,,"['derivatives', 'differential-geometry']"
79,"Given the function $y=\frac{m}{n}\sqrt{n^2-x^2}$, prove that $y′=\frac{-m^2x}{n^2y}$","Given the function , prove that",y=\frac{m}{n}\sqrt{n^2-x^2} y′=\frac{-m^2x}{n^2y},"I am trying to solve an exercise with derivatives. As the tittle says, I have the following function: $$y=\frac{m}{n}\sqrt{n^2-x^2}$$ $m$ and $n$ are constants. After finding the first derivative, I need to prove that: $$y′=\frac{-m^2x}{n^2y}$$ You must have noticed that there is a $y$ in the answer, but you do not need to find the implicit derivative to get the answer. It is supposed that finding the first function somewhere in the first derivative and replacing it with $y$ you get that answer. I already solved another exercise of this kind, so you can better understand how I am supposed to solve this one. Example: Given the function $y=\sqrt{2mx}$, prove that $y′=\frac{m}{y}$ $$y=\sqrt{2mx}$$ $$y′=\frac{1}{2\sqrt{2mx}}\left(2m\right)$$ $$y′=\frac{m}{\sqrt{2mx}}$$ Since $y=\sqrt{2mx}$: $$y′=\frac{m}{y}$$","I am trying to solve an exercise with derivatives. As the tittle says, I have the following function: $$y=\frac{m}{n}\sqrt{n^2-x^2}$$ $m$ and $n$ are constants. After finding the first derivative, I need to prove that: $$y′=\frac{-m^2x}{n^2y}$$ You must have noticed that there is a $y$ in the answer, but you do not need to find the implicit derivative to get the answer. It is supposed that finding the first function somewhere in the first derivative and replacing it with $y$ you get that answer. I already solved another exercise of this kind, so you can better understand how I am supposed to solve this one. Example: Given the function $y=\sqrt{2mx}$, prove that $y′=\frac{m}{y}$ $$y=\sqrt{2mx}$$ $$y′=\frac{1}{2\sqrt{2mx}}\left(2m\right)$$ $$y′=\frac{m}{\sqrt{2mx}}$$ Since $y=\sqrt{2mx}$: $$y′=\frac{m}{y}$$",,['derivatives']
80,Existence of elementary integral because of simple product rule?,Existence of elementary integral because of simple product rule?,,"Let's say I wish to evaluate $$\int\frac{\sin(x)}{x}+\log(x)\cos(x)\ dx$$and$$\int\frac{xe^{2x}}{(1+2x)^2}\ dx$$ I can recognize at once the integrals as antiderivatives, or results of the product rule, quotient rule, power rule, etc. For example in the first integral, $$\frac{d(\sin(x)\log(x)}{\ dx}=\frac{\sin(x)}{x}+\log(x)\cos(x)$$ thus $$\int\frac{\sin(x)}{x}+\log(x)\cos(x)\ dx= \sin(x)\log(x) +C$$ The same is true for the second integral by a multiplicative constant, $$\frac{\ d\big(\frac{e^{2x}}{4(1+2x)}\big)}{dx}=\frac{xe^{2x}}{(1+2x)^2}$$ thus $$\int\frac{xe^{2x}}{(1+2x)^2}\ dx=\frac{e^{2x}}{4(1+2x)}+C$$ My question is, does there exist an elementary path for these integrals, by that I mean can these integrals be  explicitly evaluated in an easy way, if there exists a simple product rule, quotient rule, power rule, etc  derivation such as the ones above? I tried the first one for a long time but couldn't find a way to  combine the fractions and make a substitution. Thanks in advance!","Let's say I wish to evaluate $$\int\frac{\sin(x)}{x}+\log(x)\cos(x)\ dx$$and$$\int\frac{xe^{2x}}{(1+2x)^2}\ dx$$ I can recognize at once the integrals as antiderivatives, or results of the product rule, quotient rule, power rule, etc. For example in the first integral, $$\frac{d(\sin(x)\log(x)}{\ dx}=\frac{\sin(x)}{x}+\log(x)\cos(x)$$ thus $$\int\frac{\sin(x)}{x}+\log(x)\cos(x)\ dx= \sin(x)\log(x) +C$$ The same is true for the second integral by a multiplicative constant, $$\frac{\ d\big(\frac{e^{2x}}{4(1+2x)}\big)}{dx}=\frac{xe^{2x}}{(1+2x)^2}$$ thus $$\int\frac{xe^{2x}}{(1+2x)^2}\ dx=\frac{e^{2x}}{4(1+2x)}+C$$ My question is, does there exist an elementary path for these integrals, by that I mean can these integrals be  explicitly evaluated in an easy way, if there exists a simple product rule, quotient rule, power rule, etc  derivation such as the ones above? I tried the first one for a long time but couldn't find a way to  combine the fractions and make a substitution. Thanks in advance!",,"['calculus', 'derivatives', 'indefinite-integrals']"
81,Second Derivative with respect to a Matrix,Second Derivative with respect to a Matrix,,"I have a question regarding (second order) derivative with respect to a matrix. I encounter this question because I am calculating Fisher Information, but I guess the context is not very relevant in this question. Here is the derivative: $$ \frac{\partial}{\partial \Sigma} \Sigma^{-1}A\Sigma^{-1} $$ where $\Sigma$ is a covariance matrix (positive semi-definite, symmetric), and $A = (x_i - \mu_0)(x_i -\mu_o)^T$, but we may simply use $A$ instead while knowing $A$ is symmetric. Before posting this question, I have searched on google, and found several sources useful and relevant, but do not answer my question straightaway: https://www.ics.uci.edu/~welling/teaching/KernelsICS273B/MatrixCookBook.pdf Second order derivative of the inverse matrix operator Consequently, I have made a coarse attempt to derive it, but I am not confident whether it is correct. ==================================================================== Consider a very small $\delta\Sigma$ \begin{align*} (\Sigma + \delta\Sigma)^{-1}A(\Sigma+\delta\Sigma)^{-1} &= [\Sigma(I+\Sigma^{-1}(\delta\Sigma))]^{-1}A[(I+(\delta\Sigma)\Sigma^{-1})\Sigma]^{-1}\\ &=(I+\Sigma^{-1}(\delta\Sigma))^{-1}\Sigma^{-1}A\Sigma^{-1}(I+(\delta\Sigma)\Sigma^{-1})^{-1}\\ &=(\sum_{n=0}^\infty(-1)^n[\Sigma^{-1}(\delta\Sigma)]^n)\Sigma^{-1}A\Sigma^{-1}(\sum_{n=0}^\infty(-1)^n[(\delta\Sigma)\Sigma^{-1}]^n)\\ &\approx (I-\Sigma^{-1}(\delta\Sigma))\Sigma^{-1}A\Sigma^{-1}(I-(\delta\Sigma)\Sigma^{-1})\\ &=\Sigma^{-1}A\Sigma^{-1} - \Sigma^{-1}(\delta\Sigma)\Sigma^{-1}A\Sigma^{-1}-\Sigma^{-1}A\Sigma^{-1}(\delta\Sigma)\Sigma^{-1}\\ +\Sigma^{-1}(\delta\Sigma)\Sigma^{-1}A\Sigma^{-1}(\delta\Sigma)\Sigma^{-1} \end{align*} Then, we may have \begin{align*} (\frac{\partial}{\partial \Sigma} \Sigma^{-1}A\Sigma^{-1})\delta\Sigma &= \lim_{||\delta\Sigma||\rightarrow0}(\Sigma + \delta\Sigma)^{-1}A(\Sigma+\delta\Sigma)^{-1} - \Sigma^{-1}A\Sigma^{-1}\\ &\approx \lim_{||\delta\Sigma||\rightarrow0}- \Sigma^{-1}(\delta\Sigma)\Sigma^{-1}A\Sigma^{-1}-\Sigma^{-1}A\Sigma^{-1}(\delta\Sigma)\Sigma^{-1} \end{align*} (somehow by magic or by speculating, I guess) $$ \frac{\partial}{\partial \Sigma} \Sigma^{-1}A\Sigma^{-1} = - \Sigma^{-2}A\Sigma^{-1}-\Sigma^{-1}A\Sigma^{-2} $$ ==================================================================== I have a feeling that I may be around there, but not quite yet. I am really hoping to get from this question the output of the derivative. Thank you so much for all of your time! p.s.: You do not have to follow my trail of thoughts (which could be wrong per se), and you may just show the correct way of doing this. I call this second order derivative because $\Sigma^{-1}A\Sigma^{-1}$ is what I have obtained by taking first derivative of $(x_i-\mu_0)^T\Sigma^{-1}(x_i-\mu_0)$, and yes, all you smart people may have realized this is multivariate normal. ==================================================================== In a month after I posted this question, I managed to find a great reference I would like to share. For those people who are having similar questions, here is a book that will give you a great insight (which closely resembles the method presented by @greg). ""Matrix Differential Calculus with applications in statistics"" by Magnus and Neudecker. Take a look at its Chapter 2, which offers great explanations (and examples) about kronecker product and vector operation, two important concepts when dealing with matrix differential.","I have a question regarding (second order) derivative with respect to a matrix. I encounter this question because I am calculating Fisher Information, but I guess the context is not very relevant in this question. Here is the derivative: $$ \frac{\partial}{\partial \Sigma} \Sigma^{-1}A\Sigma^{-1} $$ where $\Sigma$ is a covariance matrix (positive semi-definite, symmetric), and $A = (x_i - \mu_0)(x_i -\mu_o)^T$, but we may simply use $A$ instead while knowing $A$ is symmetric. Before posting this question, I have searched on google, and found several sources useful and relevant, but do not answer my question straightaway: https://www.ics.uci.edu/~welling/teaching/KernelsICS273B/MatrixCookBook.pdf Second order derivative of the inverse matrix operator Consequently, I have made a coarse attempt to derive it, but I am not confident whether it is correct. ==================================================================== Consider a very small $\delta\Sigma$ \begin{align*} (\Sigma + \delta\Sigma)^{-1}A(\Sigma+\delta\Sigma)^{-1} &= [\Sigma(I+\Sigma^{-1}(\delta\Sigma))]^{-1}A[(I+(\delta\Sigma)\Sigma^{-1})\Sigma]^{-1}\\ &=(I+\Sigma^{-1}(\delta\Sigma))^{-1}\Sigma^{-1}A\Sigma^{-1}(I+(\delta\Sigma)\Sigma^{-1})^{-1}\\ &=(\sum_{n=0}^\infty(-1)^n[\Sigma^{-1}(\delta\Sigma)]^n)\Sigma^{-1}A\Sigma^{-1}(\sum_{n=0}^\infty(-1)^n[(\delta\Sigma)\Sigma^{-1}]^n)\\ &\approx (I-\Sigma^{-1}(\delta\Sigma))\Sigma^{-1}A\Sigma^{-1}(I-(\delta\Sigma)\Sigma^{-1})\\ &=\Sigma^{-1}A\Sigma^{-1} - \Sigma^{-1}(\delta\Sigma)\Sigma^{-1}A\Sigma^{-1}-\Sigma^{-1}A\Sigma^{-1}(\delta\Sigma)\Sigma^{-1}\\ +\Sigma^{-1}(\delta\Sigma)\Sigma^{-1}A\Sigma^{-1}(\delta\Sigma)\Sigma^{-1} \end{align*} Then, we may have \begin{align*} (\frac{\partial}{\partial \Sigma} \Sigma^{-1}A\Sigma^{-1})\delta\Sigma &= \lim_{||\delta\Sigma||\rightarrow0}(\Sigma + \delta\Sigma)^{-1}A(\Sigma+\delta\Sigma)^{-1} - \Sigma^{-1}A\Sigma^{-1}\\ &\approx \lim_{||\delta\Sigma||\rightarrow0}- \Sigma^{-1}(\delta\Sigma)\Sigma^{-1}A\Sigma^{-1}-\Sigma^{-1}A\Sigma^{-1}(\delta\Sigma)\Sigma^{-1} \end{align*} (somehow by magic or by speculating, I guess) $$ \frac{\partial}{\partial \Sigma} \Sigma^{-1}A\Sigma^{-1} = - \Sigma^{-2}A\Sigma^{-1}-\Sigma^{-1}A\Sigma^{-2} $$ ==================================================================== I have a feeling that I may be around there, but not quite yet. I am really hoping to get from this question the output of the derivative. Thank you so much for all of your time! p.s.: You do not have to follow my trail of thoughts (which could be wrong per se), and you may just show the correct way of doing this. I call this second order derivative because $\Sigma^{-1}A\Sigma^{-1}$ is what I have obtained by taking first derivative of $(x_i-\mu_0)^T\Sigma^{-1}(x_i-\mu_0)$, and yes, all you smart people may have realized this is multivariate normal. ==================================================================== In a month after I posted this question, I managed to find a great reference I would like to share. For those people who are having similar questions, here is a book that will give you a great insight (which closely resembles the method presented by @greg). ""Matrix Differential Calculus with applications in statistics"" by Magnus and Neudecker. Take a look at its Chapter 2, which offers great explanations (and examples) about kronecker product and vector operation, two important concepts when dealing with matrix differential.",,"['matrices', 'derivatives', 'matrix-calculus', 'fisher-information']"
82,Statements on calculus and linear algebra,Statements on calculus and linear algebra,,"So i have this really important assignment, were i have made a couple of   statements and I want to check with you if they are correct or not : A function is continuous at a point xo, but is not differentiable at xo. In an interval around xo, it is twice differentiable and f'' does not change the sign. Then this implies that x0 is a corner point. By corner point i mean , for example, the point where x=0 in a function like f=absolute value of x. My second statement that i wanted to check is (reguarding linear algebra): There are five linearly independent vectors. These five linearly independent vectors must have at least 5 components each . Thanks for the help :) really appreciate it","So i have this really important assignment, were i have made a couple of   statements and I want to check with you if they are correct or not : A function is continuous at a point xo, but is not differentiable at xo. In an interval around xo, it is twice differentiable and f'' does not change the sign. Then this implies that x0 is a corner point. By corner point i mean , for example, the point where x=0 in a function like f=absolute value of x. My second statement that i wanted to check is (reguarding linear algebra): There are five linearly independent vectors. These five linearly independent vectors must have at least 5 components each . Thanks for the help :) really appreciate it",,"['calculus', 'linear-algebra', 'derivatives', 'vector-spaces']"
83,Derivative of exponential function wrt a vector,Derivative of exponential function wrt a vector,,"Let $\mathbf{A}$ be a $k\times k$ invertible matrix, let $\mathbf{x}$ be a $k\times 1$ vector and let $\mathbf{1}$ be a $k\times 1$ vector of ones. For a generic $k\times 1$ vector $\mathbf{z}$, let the function $\exp\left(\cdot\right)$  be defined as follows: $\exp\left(\mathbf{z}\right)=\exp\left(\left[\begin{array}{c} z_{1}\\ z_{2}\\ \vdots\\ z_{k} \end{array}\right]\right)=\left[\begin{array}{c} e^{z_{1}}\\ e^{z_{2}}\\ \vdots\\ e^{z_{k}} \end{array}\right]$ Is the following equality true? If so, under what conditions? $\frac{d}{d\mathbf{x}}\;\mathbf{1}'\mathbf{A}^{-1}\exp\left(\mathbf{Ax}\right)=\exp\left(\mathbf{Ax}\right)$ More in general, I am looking for a scalar function whose derivative with respect to vector $\mathbf{x}$ is $\exp\left(\mathbf{Ax}\right)$ (or its transpose). Thanks a lot in advance!","Let $\mathbf{A}$ be a $k\times k$ invertible matrix, let $\mathbf{x}$ be a $k\times 1$ vector and let $\mathbf{1}$ be a $k\times 1$ vector of ones. For a generic $k\times 1$ vector $\mathbf{z}$, let the function $\exp\left(\cdot\right)$  be defined as follows: $\exp\left(\mathbf{z}\right)=\exp\left(\left[\begin{array}{c} z_{1}\\ z_{2}\\ \vdots\\ z_{k} \end{array}\right]\right)=\left[\begin{array}{c} e^{z_{1}}\\ e^{z_{2}}\\ \vdots\\ e^{z_{k}} \end{array}\right]$ Is the following equality true? If so, under what conditions? $\frac{d}{d\mathbf{x}}\;\mathbf{1}'\mathbf{A}^{-1}\exp\left(\mathbf{Ax}\right)=\exp\left(\mathbf{Ax}\right)$ More in general, I am looking for a scalar function whose derivative with respect to vector $\mathbf{x}$ is $\exp\left(\mathbf{Ax}\right)$ (or its transpose). Thanks a lot in advance!",,"['derivatives', 'vectors', 'exponential-function', 'matrix-equations', 'matrix-calculus']"
84,Prove that $g$ is tangent to the graph of $f$,Prove that  is tangent to the graph of,g f,"So I had these two functions and the following exercise: $f(x)=x^3-2x^2$ $g_p(x)=px$ Prove that $g_{-1}$ is a tangent line to the graph of $f$ . How many points do $g_{-1}$ and $f(x)$ have in common? So yeah, the points in common are fairly simple to figure out: Points in common: $x=1$ and $x=0$ So to see if it's actually tangent to the graph, I took the derivative of both functions at the points where they meet. The result: $f'(x)=3x^2-4x$ $g'_{-1}(x)=-1$ So $f'(1)=-1$ and $f'(0)=0$ So I'm not sure why I get this answer? By the question, we are supposed to prove that its tangent to the graph of $f$ but by my calculation, it seems like only $f(1)$ is tangent with $g_{-1}(1)$ . Help is appreciated! (Please avoid hint answers).","So I had these two functions and the following exercise: Prove that is a tangent line to the graph of . How many points do and have in common? So yeah, the points in common are fairly simple to figure out: Points in common: and So to see if it's actually tangent to the graph, I took the derivative of both functions at the points where they meet. The result: So and So I'm not sure why I get this answer? By the question, we are supposed to prove that its tangent to the graph of but by my calculation, it seems like only is tangent with . Help is appreciated! (Please avoid hint answers).",f(x)=x^3-2x^2 g_p(x)=px g_{-1} f g_{-1} f(x) x=1 x=0 f'(x)=3x^2-4x g'_{-1}(x)=-1 f'(1)=-1 f'(0)=0 f f(1) g_{-1}(1),"['calculus', 'derivatives', 'tangent-line']"
85,How to calculate the replicating portfolio of a derivative,How to calculate the replicating portfolio of a derivative,,"Suppose I have a derivative that will pay $\Phi(S_T)=S_T^2$, where $S_t$ evolves as $$dS_t=\mu S_tdt+\sigma S_tdWt$$ where $(W_t)_{t\geq0}$ is a Brownian motion I have shown that its price is $V(S_t,t)=S_t^2e^{(r+\sigma^2)(T-t)}$ Now the question asks from me to calculate the replication portfolio of this derivative and I am not really sure what to do. My idea was the following: Consider a self-financing portfolio with $\phi_t$ shares at time $t$ and $\psi_t$ bonds at time $t$: $$\Pi_t=\phi_tS_t+\psi_tB_t$$ Since it is self financing then we have that $$(S_t+dS_t)d\phi_t+(B_t+dB_t)d\psi_t=0$$ Since it replicates the payoff at time $T$ we have  $$S_T^2=\phi_TS_T+\psi_TB_T$$ and these $2$ conditions (probably?) should be enough to find $\phi_t, \psi_t$. However, I am struggling to solve this system which makes me think I have a mistake somewhere. Thanks in advance","Suppose I have a derivative that will pay $\Phi(S_T)=S_T^2$, where $S_t$ evolves as $$dS_t=\mu S_tdt+\sigma S_tdWt$$ where $(W_t)_{t\geq0}$ is a Brownian motion I have shown that its price is $V(S_t,t)=S_t^2e^{(r+\sigma^2)(T-t)}$ Now the question asks from me to calculate the replication portfolio of this derivative and I am not really sure what to do. My idea was the following: Consider a self-financing portfolio with $\phi_t$ shares at time $t$ and $\psi_t$ bonds at time $t$: $$\Pi_t=\phi_tS_t+\psi_tB_t$$ Since it is self financing then we have that $$(S_t+dS_t)d\phi_t+(B_t+dB_t)d\psi_t=0$$ Since it replicates the payoff at time $T$ we have  $$S_T^2=\phi_TS_T+\psi_TB_T$$ and these $2$ conditions (probably?) should be enough to find $\phi_t, \psi_t$. However, I am struggling to solve this system which makes me think I have a mistake somewhere. Thanks in advance",,"['derivatives', 'finance']"
86,Implicit Differentiation for calculating area of a circle,Implicit Differentiation for calculating area of a circle,,I am working on a implicit differentiation equation of a circle where the radius grows by 1/per sec. Can someone advise if I have differentiate the equation correctly? The radius of the circle grows by 1/s given: $$\frac{dr}{dt} = 1$$ The area of the circle given: $$A = \pi r^2$$ Differentiate both sides of the equation with respect to the third variable: time $$\frac{dA}{dt} = 2\pi r \frac{dr}{dt}$$ The differentiation of the left hand side of the equation: $$\frac{d}{dt}[A] = \frac{dA}{dt}$$ The differentiation of the right hand side of the equation:  $$\frac{d}{dt} [\pi r^2] = [f(x)g'(x)+g(x)f'(x)]\frac{dr}{dt}$$ $$= [\pi\frac{d}{dr}[r^2] + r^2 \frac{d}{dr}[\pi]]\frac{dr}{dt}$$ $$= 2\pi r \frac{dr}{dt}$$ $$= 2\pi r$$ (since dr/dt = 1),I am working on a implicit differentiation equation of a circle where the radius grows by 1/per sec. Can someone advise if I have differentiate the equation correctly? The radius of the circle grows by 1/s given: $$\frac{dr}{dt} = 1$$ The area of the circle given: $$A = \pi r^2$$ Differentiate both sides of the equation with respect to the third variable: time $$\frac{dA}{dt} = 2\pi r \frac{dr}{dt}$$ The differentiation of the left hand side of the equation: $$\frac{d}{dt}[A] = \frac{dA}{dt}$$ The differentiation of the right hand side of the equation:  $$\frac{d}{dt} [\pi r^2] = [f(x)g'(x)+g(x)f'(x)]\frac{dr}{dt}$$ $$= [\pi\frac{d}{dr}[r^2] + r^2 \frac{d}{dr}[\pi]]\frac{dr}{dt}$$ $$= 2\pi r \frac{dr}{dt}$$ $$= 2\pi r$$ (since dr/dt = 1),,"['calculus', 'derivatives']"
87,"If $\exists f''(0)$, show $\lim _{h \rightarrow 0} \frac{f(h)-2f(0)+f(-h)}{h^2}=f''(0)$","If , show",\exists f''(0) \lim _{h \rightarrow 0} \frac{f(h)-2f(0)+f(-h)}{h^2}=f''(0),"Suppose $f$ is differentiable and $\exists f''(0)$ . I want to show $$\lim _{h \rightarrow 0} \frac{f(h)-2f(0)+f(-h)}{h^2}=f''(0)$$ not using the L'hospital rule. I know how to calculate with L'hospital rule, but I cannot see how otherwise.","Suppose is differentiable and . I want to show not using the L'hospital rule. I know how to calculate with L'hospital rule, but I cannot see how otherwise.",f \exists f''(0) \lim _{h \rightarrow 0} \frac{f(h)-2f(0)+f(-h)}{h^2}=f''(0),"['calculus', 'limits', 'analysis', 'derivatives']"
88,Problem related to Fundamental Theorem of Integral Calculus [duplicate],Problem related to Fundamental Theorem of Integral Calculus [duplicate],,"This question already has an answer here : Differentiability of antiderivative and continuity of derivative (1 answer) Closed 6 years ago . This is a problem from the book of Terence Tao, Analysis I and it looks like- Let $a<b$ be two real numbers, and let $f:[a,b]\to\Bbb{R}$ be a monotone increasing function . Let $F:[a,b]\to\Bbb{R}$ be the function   $F(x)=\int_{[a,x]} f$. Let $x_0\in[a,b]$. Show that $F$ is   differentiable at $x_0$ if and only if $f$ is continuous at $x_0$. Observe that, the ""if"" part can be easily proved by using Fundamental Theorem of Calculus. Just to recall ... Statement of Fundamental Theorem of Calculus: Let $a<b$ be two real numbers, and let $f:[a,b]\to\Bbb{R}$ be a   Riemann Integrable function. Let $F:[a,b]\to\Bbb{R}$ be the function   $F(x)=\int_{[a,x]} f$. Then $F$ is continuous on $[a,b]$. Furthermore,   if $x_0\in[a,b]$ and $f$ is continuous at $x_0$, then $F$ is   differentiable at $x_0$, and $F'(x_0)=f(x_0)$. Here, $f$ is monotone increasing on $[a,b]$, hence $f$ is Riemann Integrable on $[a,b]$ and $f$ is continuous at $x_0\implies F$ is differentiable at $x_0$ I can't understand how to tackle the ""only if"" part. There is a hint given in the book: Consider left and right limits of $f$ and argue by contradiction . Can anybody solve the part in a rigorous and analytical way? Thanks for your help in advance!","This question already has an answer here : Differentiability of antiderivative and continuity of derivative (1 answer) Closed 6 years ago . This is a problem from the book of Terence Tao, Analysis I and it looks like- Let $a<b$ be two real numbers, and let $f:[a,b]\to\Bbb{R}$ be a monotone increasing function . Let $F:[a,b]\to\Bbb{R}$ be the function   $F(x)=\int_{[a,x]} f$. Let $x_0\in[a,b]$. Show that $F$ is   differentiable at $x_0$ if and only if $f$ is continuous at $x_0$. Observe that, the ""if"" part can be easily proved by using Fundamental Theorem of Calculus. Just to recall ... Statement of Fundamental Theorem of Calculus: Let $a<b$ be two real numbers, and let $f:[a,b]\to\Bbb{R}$ be a   Riemann Integrable function. Let $F:[a,b]\to\Bbb{R}$ be the function   $F(x)=\int_{[a,x]} f$. Then $F$ is continuous on $[a,b]$. Furthermore,   if $x_0\in[a,b]$ and $f$ is continuous at $x_0$, then $F$ is   differentiable at $x_0$, and $F'(x_0)=f(x_0)$. Here, $f$ is monotone increasing on $[a,b]$, hence $f$ is Riemann Integrable on $[a,b]$ and $f$ is continuous at $x_0\implies F$ is differentiable at $x_0$ I can't understand how to tackle the ""only if"" part. There is a hint given in the book: Consider left and right limits of $f$ and argue by contradiction . Can anybody solve the part in a rigorous and analytical way? Thanks for your help in advance!",,"['calculus', 'real-analysis', 'derivatives', 'continuity', 'riemann-integration']"
89,Proper use of various derivative and partial derivative notations,Proper use of various derivative and partial derivative notations,,"I think I have seen all of these notations and more used for derivatives: $\dot f(x)$, $f'(x)$, $f_x(x)$, $df(x)/dx$, $D f(x)$ and for partial derivatives: $\partial f(x,y)/\partial x$, $\partial_x f(x,y)$, $f_x(x,y)$, $D_x f(x,y)$ and the corresponding notations used for higher order derivatives for derivatives: $f''(x)$, $f_{xx}(x)$, $d^2f(x)/dx^2$, $D^2_x f(x)$ and for partial derivatives: $\partial^2 f(x,y)/\partial x^2$, $\partial_{xx} f(x,y)$, $f_{xx}(x,y)$, $D_{xx} f(x,y)$ I also think I have seen many of these used without the function argument, e.g $f'$ or $f_x$. My question is: Is there an accepted proper usage for these derivative notations. Any links or references would be appreciated. See also Why are there so many notations for differentiation?","I think I have seen all of these notations and more used for derivatives: $\dot f(x)$, $f'(x)$, $f_x(x)$, $df(x)/dx$, $D f(x)$ and for partial derivatives: $\partial f(x,y)/\partial x$, $\partial_x f(x,y)$, $f_x(x,y)$, $D_x f(x,y)$ and the corresponding notations used for higher order derivatives for derivatives: $f''(x)$, $f_{xx}(x)$, $d^2f(x)/dx^2$, $D^2_x f(x)$ and for partial derivatives: $\partial^2 f(x,y)/\partial x^2$, $\partial_{xx} f(x,y)$, $f_{xx}(x,y)$, $D_{xx} f(x,y)$ I also think I have seen many of these used without the function argument, e.g $f'$ or $f_x$. My question is: Is there an accepted proper usage for these derivative notations. Any links or references would be appreciated. See also Why are there so many notations for differentiation?",,"['derivatives', 'notation', 'partial-derivative']"
90,Differentiate $y=x$ with respect to $x^2$?,Differentiate  with respect to ?,y=x x^2,"Let $y=x$. Differentiate $y$ with respect to $x^2$. My intuition tells me to go down a substitution route here. So, let $u=x^2~~ \Leftrightarrow~~x=\sqrt{u}$, then what we are looking to find is simply $$\frac{d}{du}\sqrt{u}$$ which is $$\frac{1}{2\sqrt{u}}$$ as $\sqrt{u}=x$, $$\frac{d}{dx^2}~x=\frac{1}{2x}$$ is this approach/solution correct, or is there something fundamentally wrong here? Any help is appreciated.","Let $y=x$. Differentiate $y$ with respect to $x^2$. My intuition tells me to go down a substitution route here. So, let $u=x^2~~ \Leftrightarrow~~x=\sqrt{u}$, then what we are looking to find is simply $$\frac{d}{du}\sqrt{u}$$ which is $$\frac{1}{2\sqrt{u}}$$ as $\sqrt{u}=x$, $$\frac{d}{dx^2}~x=\frac{1}{2x}$$ is this approach/solution correct, or is there something fundamentally wrong here? Any help is appreciated.",,"['calculus', 'derivatives']"
91,Two cars start at the same time from the junction of two roads one on each road with the uniform speed of $v m/sec$.,Two cars start at the same time from the junction of two roads one on each road with the uniform speed of .,v m/sec,"Two cars start at the same time from the junction of two roads one on each road with the uniform speed of $v m/sec$. If the roads are inclined at $120°$, show that the distance between them increases at the rate of $\sqrt {3}v m/s$ My Approach: Given: $\dfrac {dx}{dt}=v m/s$ and $\dfrac {dy}{dt}=v m/s$ Using cosine law, $$s^2=x^2+y^2-2xy\cos (120°)$$ $$s^2=x^2+y^2+xy$$ differentiating both sides w.r.t $t$ $$2s.\dfrac {ds}{dt}=2x.\dfrac {dx}{dt}+2y.\dfrac {dy}{dt}+x.\dfrac {dy}{dt}+y.\dfrac {dx}{dt}$$ $$2s\dfrac {ds}{dt}=3xv+3yv$$ $$2s.\dfrac {ds}{dt}=3v(x+y)$$ How do I proceed further?","Two cars start at the same time from the junction of two roads one on each road with the uniform speed of $v m/sec$. If the roads are inclined at $120°$, show that the distance between them increases at the rate of $\sqrt {3}v m/s$ My Approach: Given: $\dfrac {dx}{dt}=v m/s$ and $\dfrac {dy}{dt}=v m/s$ Using cosine law, $$s^2=x^2+y^2-2xy\cos (120°)$$ $$s^2=x^2+y^2+xy$$ differentiating both sides w.r.t $t$ $$2s.\dfrac {ds}{dt}=2x.\dfrac {dx}{dt}+2y.\dfrac {dy}{dt}+x.\dfrac {dy}{dt}+y.\dfrac {dx}{dt}$$ $$2s\dfrac {ds}{dt}=3xv+3yv$$ $$2s.\dfrac {ds}{dt}=3v(x+y)$$ How do I proceed further?",,"['calculus', 'derivatives']"
92,"Given $0\leq f(x) \leq xh(x)$ and the existence of $\frac{\mathrm d}{\mathrm dx}\big(xh(x)\big)(0)$, can we conclude that $f'(0)$ exists?","Given  and the existence of , can we conclude that  exists?",0\leq f(x) \leq xh(x) \frac{\mathrm d}{\mathrm dx}\big(xh(x)\big)(0) f'(0),"I have some troubles with the following exercise. I think that it's trivial but I can't see the solution. Let $f,h:\mathbb{R}\to\mathbb{R}$ be continuous functions over $\mathbb{R}$. Given that for all $x\in\mathbb{R}$ we have $$0\leq f(x)\leq xh(x)$$ and that the derivative of $xh(x)$ exists at $0$ (in fact, $\big(xh(x)\big)'(0)=h(0)$). Can we conclude that $f'(0)$ exists? Firts, because $0\leq f(0)\leq 0h(0)=0$, we have that $f(0)=0$. Next  $$f'(0)=\lim\limits_{x\to 0}\dfrac{f(x)-f(0)}{x-0}=\lim\limits_{x\to 0}\dfrac{f(x)-0}{x}=\lim\limits_{x\to 0}\dfrac{f(x)}{x}.$$ By the inequality of the hypothesis, we have $0\leq \dfrac{f(x)}{x}\leq h(x)$. Thus  $$0\leq \lim\limits_{x\to 0}\dfrac{f(x)}{x}\leq \lim\limits_{x\to 0} h(x)=h(0).$$ But then what can I do? If $h(0)=0$ the result follows. But if $h(0)>0$ I don't know how to proceed.","I have some troubles with the following exercise. I think that it's trivial but I can't see the solution. Let $f,h:\mathbb{R}\to\mathbb{R}$ be continuous functions over $\mathbb{R}$. Given that for all $x\in\mathbb{R}$ we have $$0\leq f(x)\leq xh(x)$$ and that the derivative of $xh(x)$ exists at $0$ (in fact, $\big(xh(x)\big)'(0)=h(0)$). Can we conclude that $f'(0)$ exists? Firts, because $0\leq f(0)\leq 0h(0)=0$, we have that $f(0)=0$. Next  $$f'(0)=\lim\limits_{x\to 0}\dfrac{f(x)-f(0)}{x-0}=\lim\limits_{x\to 0}\dfrac{f(x)-0}{x}=\lim\limits_{x\to 0}\dfrac{f(x)}{x}.$$ By the inequality of the hypothesis, we have $0\leq \dfrac{f(x)}{x}\leq h(x)$. Thus  $$0\leq \lim\limits_{x\to 0}\dfrac{f(x)}{x}\leq \lim\limits_{x\to 0} h(x)=h(0).$$ But then what can I do? If $h(0)=0$ the result follows. But if $h(0)>0$ I don't know how to proceed.",,"['real-analysis', 'limits', 'derivatives']"
93,Positive integer powers of a real valued function on real line that has an anti-derivative,Positive integer powers of a real valued function on real line that has an anti-derivative,,"Let $f : \mathbb R \to \mathbb R$ be a differentiable function. For every integer $n \ge 2$, does there exist a differentiable function $h_n : \mathbb R \to \mathbb R$ such that $h_n'(x)= (f'(x))^n, \forall x \in \mathbb R$ ?","Let $f : \mathbb R \to \mathbb R$ be a differentiable function. For every integer $n \ge 2$, does there exist a differentiable function $h_n : \mathbb R \to \mathbb R$ such that $h_n'(x)= (f'(x))^n, \forall x \in \mathbb R$ ?",,"['real-analysis', 'derivatives', 'riemann-integration']"
94,Double limit in the proof of differentiability,Double limit in the proof of differentiability,,"A sufficient condition for differentiability of a function $f$ about a point $(x_0,y_0)$ is that the partial derivatives exist in a neighborhood of the point and they are continuous at the point. To prove this, Thomas calculus uses the linearization definition of differentiable function  namely $f(x_0+h,y_0+k)=f(x_0,y_0)+hf_x(x_0,y_0)+kf_y(x_0,y_0)+h\epsilon_1+k\epsilon_2$ where $\epsilon_1\to0$ and $\epsilon_2\to0$ as $(h,k)\to(0,0)$. (See appendix 7 in 11th edition for the proof) My question is on one step used in this proof . The proof they use is given below: $f(x_0+h,y_0+k)-f(x_0,y_0)=f(x_0+h,y_0+k)-f(x_0+h,y_0)\ \ +f(x_0+h,y_0)-f(x_0,y_0)$. Now the function $G$ defined on the interval with end points $y_0,y_0+k$ as $G(y)=f(x_0+h,y)$ is differentiable and hence continuous with derivative $G'(y)=f_y(x_0+h,y)$. By applying the mean value theorem on $G$, we obtain a value $d$ between $y_0$ and $y_0+k$ at which $G(y_0+k)-G(y_0)=k G'(d)$ or $f(x_0+h,y_0+k)-f(x_0+h,y_0)=k f_y(x_0+h,d)$. Similarly the function $F(x)=f(x,y_0)$ is differentiable and hence continuous in an interval with end points $x_0,x_0+h$ and $F'(x)=f_x(x,y_0)$. Thus by appling MVT on $F$, we obtain a point $c$ between $x_0$ and $x_0+h$ at which $F(x_0+h)-F(x_0)=h F'(c)$ or $f(x_0+h,y_0)-f(x_0,y_0)=h f_x(c,y_0)$ The next is the part that is not clear to me. Now, as both $h$ and $k\to0$, we know that $c\to x_0$ and $d\to y_0$. Since $f_x$ and $f_y$ are continuous at $(x_0,y_0)$, the quantities $\epsilon_1=f_x(c,y_0)-f_x(x_0,y_0)$ and $\epsilon_2=f_y(x_0+h,d)-f_y(x_0,y_0)$ both approach zero as $h$ and $k\to0$. Once we get $\epsilon_1\to0$ and $\epsilon_2\to0$ as $(h,k)\to(0,0)$, we are through. It is obvious that $\epsilon_1$ doesn't depend $k$ so that $\epsilon_1\to0$ as $(h,k)\to(0,0)$. But $\epsilon_2$ depends on both $h$ and $k$. What we have directly is the iterated limit $\lim_{h\to0}\lim_{k\to0}\epsilon_2$. I am not quite convinced that the double limit $\boldsymbol{\lim_{(h,k)\to(0,0)}\epsilon_2}$ exists . Don't we need to show that it exists and the limit is indeed 0? Isn't the proof in the text a little sloppy there? Any kind of help is appreciated. Thank you.","A sufficient condition for differentiability of a function $f$ about a point $(x_0,y_0)$ is that the partial derivatives exist in a neighborhood of the point and they are continuous at the point. To prove this, Thomas calculus uses the linearization definition of differentiable function  namely $f(x_0+h,y_0+k)=f(x_0,y_0)+hf_x(x_0,y_0)+kf_y(x_0,y_0)+h\epsilon_1+k\epsilon_2$ where $\epsilon_1\to0$ and $\epsilon_2\to0$ as $(h,k)\to(0,0)$. (See appendix 7 in 11th edition for the proof) My question is on one step used in this proof . The proof they use is given below: $f(x_0+h,y_0+k)-f(x_0,y_0)=f(x_0+h,y_0+k)-f(x_0+h,y_0)\ \ +f(x_0+h,y_0)-f(x_0,y_0)$. Now the function $G$ defined on the interval with end points $y_0,y_0+k$ as $G(y)=f(x_0+h,y)$ is differentiable and hence continuous with derivative $G'(y)=f_y(x_0+h,y)$. By applying the mean value theorem on $G$, we obtain a value $d$ between $y_0$ and $y_0+k$ at which $G(y_0+k)-G(y_0)=k G'(d)$ or $f(x_0+h,y_0+k)-f(x_0+h,y_0)=k f_y(x_0+h,d)$. Similarly the function $F(x)=f(x,y_0)$ is differentiable and hence continuous in an interval with end points $x_0,x_0+h$ and $F'(x)=f_x(x,y_0)$. Thus by appling MVT on $F$, we obtain a point $c$ between $x_0$ and $x_0+h$ at which $F(x_0+h)-F(x_0)=h F'(c)$ or $f(x_0+h,y_0)-f(x_0,y_0)=h f_x(c,y_0)$ The next is the part that is not clear to me. Now, as both $h$ and $k\to0$, we know that $c\to x_0$ and $d\to y_0$. Since $f_x$ and $f_y$ are continuous at $(x_0,y_0)$, the quantities $\epsilon_1=f_x(c,y_0)-f_x(x_0,y_0)$ and $\epsilon_2=f_y(x_0+h,d)-f_y(x_0,y_0)$ both approach zero as $h$ and $k\to0$. Once we get $\epsilon_1\to0$ and $\epsilon_2\to0$ as $(h,k)\to(0,0)$, we are through. It is obvious that $\epsilon_1$ doesn't depend $k$ so that $\epsilon_1\to0$ as $(h,k)\to(0,0)$. But $\epsilon_2$ depends on both $h$ and $k$. What we have directly is the iterated limit $\lim_{h\to0}\lim_{k\to0}\epsilon_2$. I am not quite convinced that the double limit $\boldsymbol{\lim_{(h,k)\to(0,0)}\epsilon_2}$ exists . Don't we need to show that it exists and the limit is indeed 0? Isn't the proof in the text a little sloppy there? Any kind of help is appreciated. Thank you.",,"['limits', 'derivatives']"
95,"Application of the Inverse function theorem for $f:M_n(\mathbb{R})\rightarrow M_n(\mathbb{R})$, $f(A)=A^2$.","Application of the Inverse function theorem for , .",f:M_n(\mathbb{R})\rightarrow M_n(\mathbb{R}) f(A)=A^2,"Let $f: M_n(\mathbb{R}) \rightarrow M_n(\mathbb{R})$ defined as $f(A) = A^2$. 1) Show that $f$ is $\mathcal{C}^1$ and calculate its differential. 2) Show that there exists $g$ a $\mathcal{C}^1$ function defined on a neighbourhood of the identity matrix such that $\forall A \in V$, we have $g(A)^2 = A$ I managed to show that beginning. For a matrix $A \in M_n(\mathbb{R})$ we have $f(A) = A^2$ thus if we defined $A = (a_{i,j})_{1\leq i,j \leq n}$ we then have $f(A) = \sum^{n}_{k1}a_{i,k}a_{k,j}$. Thus it's a polynomial, thus it's $\mathcal{C}^1$ (and even $\mathcal{C}^{\infty}$). Now, we have $f(A+H) = A^2 + AH + HA + H^2$, thus I deduce that $df(A)H = AH + HA$. Now, for the second question: let $H \in M_n(\mathbb{R})$ then $df(I)= 2I$ thus the differential is an isomorphism, we can use the inverse function theorem. We thus have two neighbourhoods $V \subset M_n(\mathbb{R})$ and $W\subset M_n(\mathbb{R})$ such that $f: V \rightarrow W$ is a diffeomorphism of class $\mathcal{C}^1$. Now, we can put $g = f^{-1}$ that is $\mathcal{C}^1$, and $g(A)^2 = (f^{-1})^2(A)$, and here I struggle to show that $(f^{-1})^2(A) = A$. Can someone help me out?","Let $f: M_n(\mathbb{R}) \rightarrow M_n(\mathbb{R})$ defined as $f(A) = A^2$. 1) Show that $f$ is $\mathcal{C}^1$ and calculate its differential. 2) Show that there exists $g$ a $\mathcal{C}^1$ function defined on a neighbourhood of the identity matrix such that $\forall A \in V$, we have $g(A)^2 = A$ I managed to show that beginning. For a matrix $A \in M_n(\mathbb{R})$ we have $f(A) = A^2$ thus if we defined $A = (a_{i,j})_{1\leq i,j \leq n}$ we then have $f(A) = \sum^{n}_{k1}a_{i,k}a_{k,j}$. Thus it's a polynomial, thus it's $\mathcal{C}^1$ (and even $\mathcal{C}^{\infty}$). Now, we have $f(A+H) = A^2 + AH + HA + H^2$, thus I deduce that $df(A)H = AH + HA$. Now, for the second question: let $H \in M_n(\mathbb{R})$ then $df(I)= 2I$ thus the differential is an isomorphism, we can use the inverse function theorem. We thus have two neighbourhoods $V \subset M_n(\mathbb{R})$ and $W\subset M_n(\mathbb{R})$ such that $f: V \rightarrow W$ is a diffeomorphism of class $\mathcal{C}^1$. Now, we can put $g = f^{-1}$ that is $\mathcal{C}^1$, and $g(A)^2 = (f^{-1})^2(A)$, and here I struggle to show that $(f^{-1})^2(A) = A$. Can someone help me out?",,"['derivatives', 'differential', 'inverse-function', 'inverse-function-theorem']"
96,Differentiability of the maximum,Differentiability of the maximum,,"For $i=1,\ldots,n$, let $f_i\colon(0,\infty)\to(0,1)$ and $g_i\colon(0,\infty)\to(0,1)$ be $C^{\infty}$ functions. Define $h\colon(0,\infty)\to(0,\infty)$ as follows: $$h(t)=\max_{i=1,\ldots,n} \frac{f_i(t)}{g_i(t)}\qquad \forall t\in(0,\infty).$$ I am wondering if the following is true: The function $h$ is differentiable almost everywhere, and there exists $f,g\colon(0,\infty)\to(0,1)$ differentiable almost everywhere such that $h(t)=\frac{f(t)}{g(t)}$ for every $t\in (0,\infty)$. I am pretty sure it is true, but I would like to have confirmation (and, if possible, a reference). This question is quite related however it is not exactly the same. Still the answer of @Alex R. seems to confirm my belief that the affirmation is indeed true. Note: I don't know if it helps, but the case I am interested in have the additional properties that $h$ is strictly decreasing, $\lim_{t\to 0}h(t)=\infty$ and $\lim_{t\to \infty}h(t)=1$.","For $i=1,\ldots,n$, let $f_i\colon(0,\infty)\to(0,1)$ and $g_i\colon(0,\infty)\to(0,1)$ be $C^{\infty}$ functions. Define $h\colon(0,\infty)\to(0,\infty)$ as follows: $$h(t)=\max_{i=1,\ldots,n} \frac{f_i(t)}{g_i(t)}\qquad \forall t\in(0,\infty).$$ I am wondering if the following is true: The function $h$ is differentiable almost everywhere, and there exists $f,g\colon(0,\infty)\to(0,1)$ differentiable almost everywhere such that $h(t)=\frac{f(t)}{g(t)}$ for every $t\in (0,\infty)$. I am pretty sure it is true, but I would like to have confirmation (and, if possible, a reference). This question is quite related however it is not exactly the same. Still the answer of @Alex R. seems to confirm my belief that the affirmation is indeed true. Note: I don't know if it helps, but the case I am interested in have the additional properties that $h$ is strictly decreasing, $\lim_{t\to 0}h(t)=\infty$ and $\lim_{t\to \infty}h(t)=1$.",,"['calculus', 'real-analysis', 'derivatives']"
97,Confusion with Matrix calculus derivative computation.,Confusion with Matrix calculus derivative computation.,,"The text I am reading has the following $$E=(I+\nabla I\frac{\partial W}{\partial p}\Delta p -T)^2$$ where $\nabla I$ us a row vector $1$ by $2$, $\frac{\partial W}{\partial p}$ is a $2$ by $3$ matrix and $\Delta p$ is a $3$ by $1$ column vector. $I$ and $T$ are scalars.  We seek the gradient $\frac{\partial E}{\partial \Delta p_k}$, but I do not understand their solution. I will present my attempt, and than their solution. In index notation, we have $(I+\nabla I_i\frac{\partial W^i}{\partial \Delta p_k}\Delta p^k -T)^2$, where summation if implied by repeated indicies, Than the derivative is $$\frac{\partial E}{\partial \Delta p_m}=2(I+\nabla I_i\frac{\partial W^i}{\partial p_k}\Delta p^k -T)\nabla I_n\frac{\partial W^n}{\partial p_m}$$ or returning to the matrix notation $$\frac{\partial E}{\partial \Delta p}=2(I+\nabla I\frac{\partial W}{\partial p}\Delta p -T)\nabla I\frac{\partial W}{\partial p}\tag{1}\label{1}$$ But their solution is $$\frac{\partial E}{\partial \Delta p}=2(\nabla I\frac{\partial W}{\partial p})^T(I+\nabla I\frac{\partial W}{\partial p}\Delta p -T)\tag{2}\label{2}$$ Why is their solution correct and where did I make the mistake? My solution is equation \eqref{1}, but theirs is \eqref{2}. I am confused, why in theirs,  $$(\nabla I\frac{\partial W}{\partial p})$$ is transposed?","The text I am reading has the following $$E=(I+\nabla I\frac{\partial W}{\partial p}\Delta p -T)^2$$ where $\nabla I$ us a row vector $1$ by $2$, $\frac{\partial W}{\partial p}$ is a $2$ by $3$ matrix and $\Delta p$ is a $3$ by $1$ column vector. $I$ and $T$ are scalars.  We seek the gradient $\frac{\partial E}{\partial \Delta p_k}$, but I do not understand their solution. I will present my attempt, and than their solution. In index notation, we have $(I+\nabla I_i\frac{\partial W^i}{\partial \Delta p_k}\Delta p^k -T)^2$, where summation if implied by repeated indicies, Than the derivative is $$\frac{\partial E}{\partial \Delta p_m}=2(I+\nabla I_i\frac{\partial W^i}{\partial p_k}\Delta p^k -T)\nabla I_n\frac{\partial W^n}{\partial p_m}$$ or returning to the matrix notation $$\frac{\partial E}{\partial \Delta p}=2(I+\nabla I\frac{\partial W}{\partial p}\Delta p -T)\nabla I\frac{\partial W}{\partial p}\tag{1}\label{1}$$ But their solution is $$\frac{\partial E}{\partial \Delta p}=2(\nabla I\frac{\partial W}{\partial p})^T(I+\nabla I\frac{\partial W}{\partial p}\Delta p -T)\tag{2}\label{2}$$ Why is their solution correct and where did I make the mistake? My solution is equation \eqref{1}, but theirs is \eqref{2}. I am confused, why in theirs,  $$(\nabla I\frac{\partial W}{\partial p})$$ is transposed?",,"['derivatives', 'matrix-calculus']"
98,"Can the total derivative, as a linear map, have a different domain from the domain of the function?","Can the total derivative, as a linear map, have a different domain from the domain of the function?",,"I'm reading the total derivative as a linear map definition on Wikipedia . The function in the definition is $f:U \mapsto \mathbb{R}^m$, where $U$ is an open set in $\mathbb{R}^n$. However, the derivative is defined as a linear map that maps from $\mathbb{R}^n$ to $\mathbb{R}^m$. Does that mean the linear map has a different domain (i.e. $\mathbb{R}^n$) than the function itself (the function has domain $U$)? How is that possible? In my understanding, the total derivative, when given a point from the domain of the function itself (i.e. $U$), tells us the speed and direction of the function at that point. If so, it doesn't make sense to put a point that's in $\mathbb{R}^n$ but not in $U$ into the total derivative, does it?","I'm reading the total derivative as a linear map definition on Wikipedia . The function in the definition is $f:U \mapsto \mathbb{R}^m$, where $U$ is an open set in $\mathbb{R}^n$. However, the derivative is defined as a linear map that maps from $\mathbb{R}^n$ to $\mathbb{R}^m$. Does that mean the linear map has a different domain (i.e. $\mathbb{R}^n$) than the function itself (the function has domain $U$)? How is that possible? In my understanding, the total derivative, when given a point from the domain of the function itself (i.e. $U$), tells us the speed and direction of the function at that point. If so, it doesn't make sense to put a point that's in $\mathbb{R}^n$ but not in $U$ into the total derivative, does it?",,"['calculus', 'real-analysis', 'derivatives', 'linear-transformations']"
99,Differentiation under the Integration Sign,Differentiation under the Integration Sign,,"I have difficulty with the following question: My understanding is that I can partially differentiate the integral with respect to any parameter (in this case, a or b) to obtain a derivate of the initial function. My problem is that when I partially differentiate the integral with respect to a or b, I obtain the following: I am unsure how I could obtain anything useful by evaluating either integrals (and then integrating that result to obtain $g(x)$). Any help would be much appreciated.","I have difficulty with the following question: My understanding is that I can partially differentiate the integral with respect to any parameter (in this case, a or b) to obtain a derivate of the initial function. My problem is that when I partially differentiate the integral with respect to a or b, I obtain the following: I am unsure how I could obtain anything useful by evaluating either integrals (and then integrating that result to obtain $g(x)$). Any help would be much appreciated.",,"['integration', 'derivatives']"
