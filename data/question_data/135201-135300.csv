,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Must a solution defined for $(t_0,+\infty)$ with bounded limit in $+\infty$ tend to a constant solution?",Must a solution defined for  with bounded limit in  tend to a constant solution?,"(t_0,+\infty) +\infty","Let $x'=f(t,x)$ be a differential equation with $f$ in the hypothesis of Picard's theorem. Let $\varphi$ be a solution such that its interval of definition contains $(t_0,+\infty)$ for some fixed $t_0\in \mathbb{R}$. Suppose $\lim_{t\to+\infty} \varphi(t)=a \in \mathbb{R}$. Must the constant function $t\mapsto a$ be a solution of the equation? (This is a question I've asked myself while studying the logistic model $x'=ax(1-x)$ and wondering how to justify the solutions look the way they do.) EDIT: By ""the hypothesis of Picard's theorem"", I mean $f$ continuous and locally Lipschitz with respect to $x$. I'm interested also in particular cases (e.g. autonomous system) where this holds.","Let $x'=f(t,x)$ be a differential equation with $f$ in the hypothesis of Picard's theorem. Let $\varphi$ be a solution such that its interval of definition contains $(t_0,+\infty)$ for some fixed $t_0\in \mathbb{R}$. Suppose $\lim_{t\to+\infty} \varphi(t)=a \in \mathbb{R}$. Must the constant function $t\mapsto a$ be a solution of the equation? (This is a question I've asked myself while studying the logistic model $x'=ax(1-x)$ and wondering how to justify the solutions look the way they do.) EDIT: By ""the hypothesis of Picard's theorem"", I mean $f$ continuous and locally Lipschitz with respect to $x$. I'm interested also in particular cases (e.g. autonomous system) where this holds.",,['ordinary-differential-equations']
1,The solution of $\frac{\mathrm d\mathbf u}{\mathrm dt} = \mathbf A\mathbf u$ using eigenvectors,The solution of  using eigenvectors,\frac{\mathrm d\mathbf u}{\mathrm dt} = \mathbf A\mathbf u,"We are given a problem $$\frac{du}{dt} = Au$$ where $$ u(t) = \left[\begin{array}{r} y(t) \\ z(t) \end{array}\right] $$ and $$\frac{dy}{dt} = 2y - z$$ $$\frac{dz}{dt} = -y + 2z$$ hence $$ \frac{d}{dt} \left[\begin{array}{r} y \\ z \end{array}\right]= \left[\begin{array}{rr} 2 & -1 \\ -1 & 2 \end{array}\right] \left[\begin{array}{r} y \\ z \end{array}\right] $$ The teacher uses this example for a demonstration on the use of eigenvalues. If one represents $u(0)$ as a combination of eigenvalues of $A$, then the solution can be represented as a combination of $e^t$ and $e^{3t}$ which correspond to eigenvalues $\lambda_1 =1$ and $\lambda_2=3$. Hence $$u(t) = Ce^t x_1+ De^{3t}x_2$$ $x_1$ and $x_2$ are the eigenvectors of $A$, and the constants $C$ and $D$ are determined by two initial values $y(0)$ and $z(0)$. I am curious on how to derive the result such as $$z(t) = Ce^{t} - De^{3t}$$ I am not able to replicate this derivation, that is, how to combine eigenvectors. exponential solutions in a way that will result in $u(t)$. The eigenvalues of $A$ are 1 and 3, and eigenvectors are $$\begin{bmatrix}1\\1\end{bmatrix}\text{ and }\begin{bmatrix}1\\-1\end{bmatrix}.$$","We are given a problem $$\frac{du}{dt} = Au$$ where $$ u(t) = \left[\begin{array}{r} y(t) \\ z(t) \end{array}\right] $$ and $$\frac{dy}{dt} = 2y - z$$ $$\frac{dz}{dt} = -y + 2z$$ hence $$ \frac{d}{dt} \left[\begin{array}{r} y \\ z \end{array}\right]= \left[\begin{array}{rr} 2 & -1 \\ -1 & 2 \end{array}\right] \left[\begin{array}{r} y \\ z \end{array}\right] $$ The teacher uses this example for a demonstration on the use of eigenvalues. If one represents $u(0)$ as a combination of eigenvalues of $A$, then the solution can be represented as a combination of $e^t$ and $e^{3t}$ which correspond to eigenvalues $\lambda_1 =1$ and $\lambda_2=3$. Hence $$u(t) = Ce^t x_1+ De^{3t}x_2$$ $x_1$ and $x_2$ are the eigenvectors of $A$, and the constants $C$ and $D$ are determined by two initial values $y(0)$ and $z(0)$. I am curious on how to derive the result such as $$z(t) = Ce^{t} - De^{3t}$$ I am not able to replicate this derivation, that is, how to combine eigenvectors. exponential solutions in a way that will result in $u(t)$. The eigenvalues of $A$ are 1 and 3, and eigenvectors are $$\begin{bmatrix}1\\1\end{bmatrix}\text{ and }\begin{bmatrix}1\\-1\end{bmatrix}.$$",,"['linear-algebra', 'ordinary-differential-equations']"
2,Stability of a generalized form of the Mathieu equation,Stability of a generalized form of the Mathieu equation,,"I am using the Mathieu equation $$x''(t) + a x(t) + 2 q \cos(2 t) x(t) = 0,$$ as a model of a physical system. The conditions on the parameters $a$ and $q$ for stable solutions to exist are well known and are even implemented as built-in functions in systems such as Mathematica. Now, I would like to extend my model to the form    $$x''(t) + c x^2(t)+ a x(t) + 2 q \cos(2 t) x(t) = 0,$$ and establish the stability criteria for this modified model. Is this a known model, or can it be transformed to one? Does it even make sense to talk about stability conditions for this case -- as far as I can see Floquet's theorem does not apply? Although the most general form would be most interesting, it would also be interesting to know the stability criteria for the less general case of $a=0$.","I am using the Mathieu equation $$x''(t) + a x(t) + 2 q \cos(2 t) x(t) = 0,$$ as a model of a physical system. The conditions on the parameters $a$ and $q$ for stable solutions to exist are well known and are even implemented as built-in functions in systems such as Mathematica. Now, I would like to extend my model to the form    $$x''(t) + c x^2(t)+ a x(t) + 2 q \cos(2 t) x(t) = 0,$$ and establish the stability criteria for this modified model. Is this a known model, or can it be transformed to one? Does it even make sense to talk about stability conditions for this case -- as far as I can see Floquet's theorem does not apply? Although the most general form would be most interesting, it would also be interesting to know the stability criteria for the less general case of $a=0$.",,['ordinary-differential-equations']
3,What does the little d and d^2 mean in equations?,What does the little d and d^2 mean in equations?,,I'm reading a text on ray tracing. There is this section about radiometric quantities where radiance is defined as $L = \frac{d^2\Phi}{dA cos\Theta d\omega}$ $\Phi$ is the radiant flux $\Theta$ is the solid angle (sr) subtended by the observation or measurement $\omega$ is the incidence angle measured from the surface normal This is just one of many equations using $d$ and $d^2$. I'm pretty sure that $d$ has something to do with differential equations. I already read some texts on differential equations but I still don't understand the meaning of $d$ and $d^2$ in this context. Can someone explain this to me or point me to some reference/resource/book whatever? Especially the $d^2$ puzzles me.,I'm reading a text on ray tracing. There is this section about radiometric quantities where radiance is defined as $L = \frac{d^2\Phi}{dA cos\Theta d\omega}$ $\Phi$ is the radiant flux $\Theta$ is the solid angle (sr) subtended by the observation or measurement $\omega$ is the incidence angle measured from the surface normal This is just one of many equations using $d$ and $d^2$. I'm pretty sure that $d$ has something to do with differential equations. I already read some texts on differential equations but I still don't understand the meaning of $d$ and $d^2$ in this context. Can someone explain this to me or point me to some reference/resource/book whatever? Especially the $d^2$ puzzles me.,,"['trigonometry', 'ordinary-differential-equations', 'physics']"
4,Solve the IVP $y'+2y = \frac{1}{1+x^2}$,Solve the IVP,y'+2y = \frac{1}{1+x^2},"Find the solution of the DE $$y'+2y = \frac{1}{1+x^2}\,\,\,\,\,\,\forall x \in \mathbb R$$ satisfying $y(0) = a$ where $a \in \mathbb R$ is a constant. My attempt: Since it's a linear ODE, therefore the Integration factor (I.F.) is $ e^{\int 2\,dx} = e^{2x}$ .And the solution is $$ ye^{2x} = \int \frac{e^{2x}}{1+x^2} dx$$ I'm facing trouble in solving the integral. I tried using some online integral calculator but the solutions over there tends to include imaginary expressions. I'm not sure if my approach was incorrect or if I'm missing something while solving the ODE. Edit: The question furter required us to find the value $$\lim_{x \to \infty} y_a(x)$$ . Can we find the limit for $$y(x) = e^{-2x}\int \frac{e^{2x}}{1+x^2} dx + e^{-2x}C$$ where C is the constant of integration. I'm not sure how to use the initial value in this case. The solution is: $$\lim_{x \to \infty} y_a(x) = 0\,\,\,\,,\forall\,a\in\mathbb R$$ Note: The question was asked in a maths competition where the syllabus doesn't include ODE with complex functions as a solution or complex analysis.","Find the solution of the DE satisfying where is a constant. My attempt: Since it's a linear ODE, therefore the Integration factor (I.F.) is .And the solution is I'm facing trouble in solving the integral. I tried using some online integral calculator but the solutions over there tends to include imaginary expressions. I'm not sure if my approach was incorrect or if I'm missing something while solving the ODE. Edit: The question furter required us to find the value . Can we find the limit for where C is the constant of integration. I'm not sure how to use the initial value in this case. The solution is: Note: The question was asked in a maths competition where the syllabus doesn't include ODE with complex functions as a solution or complex analysis.","y'+2y = \frac{1}{1+x^2}\,\,\,\,\,\,\forall x \in \mathbb R y(0) = a a \in \mathbb R  e^{\int 2\,dx} = e^{2x}  ye^{2x} = \int \frac{e^{2x}}{1+x^2} dx \lim_{x \to \infty} y_a(x) y(x) = e^{-2x}\int \frac{e^{2x}}{1+x^2} dx + e^{-2x}C \lim_{x \to \infty} y_a(x) = 0\,\,\,\,,\forall\,a\in\mathbb R","['calculus', 'integration', 'ordinary-differential-equations', 'indefinite-integrals']"
5,Find an asymptotic solution of a ODE system,Find an asymptotic solution of a ODE system,,"Consider the following ODE system: $\frac{dx}{dt}=x^2+y^2-y\\ \frac{dy}{dt}=-2xy-x$ I want to prove that there exists a solution $(x(t),y(t))$ such that $\lim_{t\to +\infty}(x(t),y(t))=\lim_{t\to -\infty}(x(t),y(t))=(0,0)$ and $(x(t),y(t))\neq (0,0)$ for any $t$ . I can prove that when $F(x,y)=6x^2y+3x^2+2y^3-3y^2$ , we have $\frac{d}{dt}F(x(t),y(t))=0$ . So if there exists a solution satisfying the condition,  it should be on the curve of $6x^2y+3x^2+2y^3-3y^2=0$ . Then I have no idea about what I can do next. Should I try to find a special solution?","Consider the following ODE system: I want to prove that there exists a solution such that and for any . I can prove that when , we have . So if there exists a solution satisfying the condition,  it should be on the curve of . Then I have no idea about what I can do next. Should I try to find a special solution?","\frac{dx}{dt}=x^2+y^2-y\\ \frac{dy}{dt}=-2xy-x (x(t),y(t)) \lim_{t\to +\infty}(x(t),y(t))=\lim_{t\to -\infty}(x(t),y(t))=(0,0) (x(t),y(t))\neq (0,0) t F(x,y)=6x^2y+3x^2+2y^3-3y^2 \frac{d}{dt}F(x(t),y(t))=0 6x^2y+3x^2+2y^3-3y^2=0","['calculus', 'ordinary-differential-equations', 'dynamical-systems']"
6,How to fit and ODE to data?,How to fit and ODE to data?,,"Consider the following ODE $$ y'(t)=\alpha x(t)-\beta y(t) $$ and the following datasets $$ X=\{(t_0,x_0),...,(t_n,x_n)\}\\ Y=\{(t_0,y_0),...,(t_n,y_n)\} $$ How can I find $\alpha$ and $\beta$ that best fits this data? In particular, is it possible to use the following solution $$ y(t) = e^{-\beta t} \left( \alpha \int_0^t e^{\beta \tau} x(\tau) \, d\tau + C \right), $$ directly, where $C$ is a constant? Thoughts: This a fairly new field for me and I have briefly heard about inverse problems in ODEs , but rather than setting up a global minimizer that numerically finds it, I am wondering whether there is an analytical way of solving this problem, to the best possible fit in some sense . Example: As an example, one can consider the following data $$ \begin{align} X&=\{91, 110, 125, 105, 88, 84\}\\ Y&=\{1.0, 0.97, 1.0, 0.95, 0.92, 0.89\} \end{align} $$ at time points $$ T=\{0, 5, 9, 18, 28, 38\} $$ Using a numerical approach (see Python script here ), I get the following fit with $\alpha\simeq 0.000486$ , $\beta\simeq 0.057717$ , and a chi-square error of $0.0017296$ . Is it possible to improve this? I have tried global optimizers, but to no avail, hence thinking of a potential analytical approach.","Consider the following ODE and the following datasets How can I find and that best fits this data? In particular, is it possible to use the following solution directly, where is a constant? Thoughts: This a fairly new field for me and I have briefly heard about inverse problems in ODEs , but rather than setting up a global minimizer that numerically finds it, I am wondering whether there is an analytical way of solving this problem, to the best possible fit in some sense . Example: As an example, one can consider the following data at time points Using a numerical approach (see Python script here ), I get the following fit with , , and a chi-square error of . Is it possible to improve this? I have tried global optimizers, but to no avail, hence thinking of a potential analytical approach.","
y'(t)=\alpha x(t)-\beta y(t)
 
X=\{(t_0,x_0),...,(t_n,x_n)\}\\
Y=\{(t_0,y_0),...,(t_n,y_n)\}
 \alpha \beta 
y(t) = e^{-\beta t} \left( \alpha \int_0^t e^{\beta \tau} x(\tau) \, d\tau + C \right),
 C 
\begin{align}
X&=\{91, 110, 125, 105, 88, 84\}\\
Y&=\{1.0, 0.97, 1.0, 0.95, 0.92, 0.89\}
\end{align}
 
T=\{0, 5, 9, 18, 28, 38\}
 \alpha\simeq 0.000486 \beta\simeq 0.057717 0.0017296","['ordinary-differential-equations', 'optimization', 'regression', 'linear-regression']"
7,Solving system of differential equations using elimination,Solving system of differential equations using elimination,,"I am trying to solve this system of differential equations using elimination method, but I am stuck. $$\left.\left\{\begin{aligned}&y_1^{\prime}=y_2,\\&y_2^{\prime}=-y_1+\frac1{\cos x}.\end{aligned}\right.\right.$$ My attempt: I tried differentiating the $y_1'= y_2$ again to get $y_1''= y_2'= -y_1+ \frac{1}{\cos(x)}$ . But I am not sure how to proceed further. I've been suggested to use Lagrange method later, but I am not sure. I would appreciate if someone could help me get to the final solution.","I am trying to solve this system of differential equations using elimination method, but I am stuck. My attempt: I tried differentiating the again to get . But I am not sure how to proceed further. I've been suggested to use Lagrange method later, but I am not sure. I would appreciate if someone could help me get to the final solution.","\left.\left\{\begin{aligned}&y_1^{\prime}=y_2,\\&y_2^{\prime}=-y_1+\frac1{\cos x}.\end{aligned}\right.\right. y_1'= y_2 y_1''= y_2'= -y_1+ \frac{1}{\cos(x)}","['ordinary-differential-equations', 'derivatives', 'systems-of-equations']"
8,Confusion regarding the definition of the state of a physical system,Confusion regarding the definition of the state of a physical system,,"I'm currently covering Jan de Vries' Elements of Topological Dynamics and in it he gives a brief introduction to the field through the lens of classical mechanics. He defines the state of a mechanical system in the following way: In order to generalize his definition of state, as used in this paragraph, say to a physical system consisting of $n$ objects $p_1, \ldots, p_n$ , is it right to say that a system's state, at a particular time $t$ , can be represented by a tuple $$(x_1(t), \ldots, x_n(t), \dot x_1(t), \ldots, \dot x_n(t))$$ where $x_i(t)$ and $\dot x_i(t)$ are the position and velocity vectors, respectively, of object $p_i$ at time $t$ ? Here, if I've understood things correctly, the first $n$ components give information on the systems configuration while the latter $n$ components give information on the system's velocity (by giving the velocity of each of its constituents). If I've understood the above notion of state correctly (and please tell me if I haven't), I'm a little confused at the way he introduces the notion of a law of motion. He briefly introduced it in the former picture, but he finishes here in this image: Since he uses the notation $x(t) = (x_1(t), \ldots, x_n(t))$ to denote the state of a system, I'm assuming some of the components of this tuple are velocities and not everything is simply configuration. So, if $n = 2k$ (for $k$ the number of objects in our system), the family of autonomous ordinary differential equations he gives in $(5)$ can be written as $$\dot x_i(t) = F(x_1(t), \ldots, x_k(t), \dot x_1(t), \ldots, \dot x_i(t), \ldots, \dot x_k(t))$$ So, $\dot x_i$ is the output of a function with $\dot x_i$ itself as one of the input variables. Is this correct or have I mixed up the notation?","I'm currently covering Jan de Vries' Elements of Topological Dynamics and in it he gives a brief introduction to the field through the lens of classical mechanics. He defines the state of a mechanical system in the following way: In order to generalize his definition of state, as used in this paragraph, say to a physical system consisting of objects , is it right to say that a system's state, at a particular time , can be represented by a tuple where and are the position and velocity vectors, respectively, of object at time ? Here, if I've understood things correctly, the first components give information on the systems configuration while the latter components give information on the system's velocity (by giving the velocity of each of its constituents). If I've understood the above notion of state correctly (and please tell me if I haven't), I'm a little confused at the way he introduces the notion of a law of motion. He briefly introduced it in the former picture, but he finishes here in this image: Since he uses the notation to denote the state of a system, I'm assuming some of the components of this tuple are velocities and not everything is simply configuration. So, if (for the number of objects in our system), the family of autonomous ordinary differential equations he gives in can be written as So, is the output of a function with itself as one of the input variables. Is this correct or have I mixed up the notation?","n p_1, \ldots, p_n t (x_1(t), \ldots, x_n(t), \dot x_1(t), \ldots, \dot x_n(t)) x_i(t) \dot x_i(t) p_i t n n x(t) = (x_1(t), \ldots, x_n(t)) n = 2k k (5) \dot x_i(t) = F(x_1(t), \ldots, x_k(t), \dot x_1(t), \ldots, \dot x_i(t), \ldots, \dot x_k(t)) \dot x_i \dot x_i","['ordinary-differential-equations', 'dynamical-systems', 'physics', 'classical-mechanics']"
9,"If $f''(x) +f(x) \geq 0$, show that $f(x) + f(x+\pi) \geq 0$ for all $x$ [duplicate]","If , show that  for all  [duplicate]",f''(x) +f(x) \geq 0 f(x) + f(x+\pi) \geq 0 x,"This question already has an answer here : $f''+f \ge 0$ implies $f(x)+f(x+\pi) \ge 0$ (1 answer) Closed 7 months ago . I've been trying to solve the following exercise and simply have no idea how to solve it whatsoever. Let $f:\mathbb{R}\to\mathbb{R}$ be a function of class $\mathcal{C}^2$ , such that for all $x\in\mathbb{R}$ one has: $$f''(x)+f(x)\geq0$$ Show that for all $x\in\mathbb{R}$ one has: $$f(x)+f(x+\pi)\geq0$$ How can I even begin to tackle a question like this? I'm used to differential equations and solve them without a problem most of the time, but I can't find any information on differential inequalities , which is why I'm desperately resorting to this forum (which I know normally doesn't receive problems without much of an attempt behind them very well). What's the general approach one must adopt in order to tackle a problem like this?","This question already has an answer here : $f''+f \ge 0$ implies $f(x)+f(x+\pi) \ge 0$ (1 answer) Closed 7 months ago . I've been trying to solve the following exercise and simply have no idea how to solve it whatsoever. Let be a function of class , such that for all one has: Show that for all one has: How can I even begin to tackle a question like this? I'm used to differential equations and solve them without a problem most of the time, but I can't find any information on differential inequalities , which is why I'm desperately resorting to this forum (which I know normally doesn't receive problems without much of an attempt behind them very well). What's the general approach one must adopt in order to tackle a problem like this?",f:\mathbb{R}\to\mathbb{R} \mathcal{C}^2 x\in\mathbb{R} f''(x)+f(x)\geq0 x\in\mathbb{R} f(x)+f(x+\pi)\geq0,"['calculus', 'ordinary-differential-equations']"
10,Searching for Functions Exhibiting Semigroup Property for Energy-Efficient Neuronal Modeling,Searching for Functions Exhibiting Semigroup Property for Energy-Efficient Neuronal Modeling,,"I am working on designing energy-efficient neurons for neuromorphic computing. One of the critical aspects of the dynamics I'm exploring is that they should adhere to the semigroup property. Specifically, given a function $f$ with state $x$ and time $t$ , it should satisfy: $ f(x, t_1 + t_2) = f(f(x, t_1), t_2) $ This property is vital as, during training, I am updating the dynamics in equally-sized small timesteps, while during inference, the evolution of the state $x$ over many timesteps should be condensable into a single, cheap function evaluation. The semigroup property ensures consistent behavior between these stages. It's worth noting that my primary concern is not biological plausibility. Instead, I aim to discover and design energy-efficient neural models. To this end: I am interested in classes of functions (potentially vector-valued) that inherently satisfy the semigroup property, which I can explore through grid search or genetic algorithms to find well-performing candidates. I am aware of matrix exponentiation, which offers a class of functions/differential equations that inherently possess the semigroup property. However, I am curious if there are additional classes of functions or mathematical models that exhibit this property. Are there methods or techniques to construct functions (or systems of functions for multi-dimensional states) that adhere to this property? Given the context of energy-efficient neural dynamics, are there any known mathematical constructs or approximations that might be suitable? Any insights, references, or pointers on this topic would be highly appreciated.","I am working on designing energy-efficient neurons for neuromorphic computing. One of the critical aspects of the dynamics I'm exploring is that they should adhere to the semigroup property. Specifically, given a function with state and time , it should satisfy: This property is vital as, during training, I am updating the dynamics in equally-sized small timesteps, while during inference, the evolution of the state over many timesteps should be condensable into a single, cheap function evaluation. The semigroup property ensures consistent behavior between these stages. It's worth noting that my primary concern is not biological plausibility. Instead, I aim to discover and design energy-efficient neural models. To this end: I am interested in classes of functions (potentially vector-valued) that inherently satisfy the semigroup property, which I can explore through grid search or genetic algorithms to find well-performing candidates. I am aware of matrix exponentiation, which offers a class of functions/differential equations that inherently possess the semigroup property. However, I am curious if there are additional classes of functions or mathematical models that exhibit this property. Are there methods or techniques to construct functions (or systems of functions for multi-dimensional states) that adhere to this property? Given the context of energy-efficient neural dynamics, are there any known mathematical constructs or approximations that might be suitable? Any insights, references, or pointers on this topic would be highly appreciated.","f x t  f(x, t_1 + t_2) = f(f(x, t_1), t_2)  x","['ordinary-differential-equations', 'neural-networks', 'semigroup-of-operators']"
11,On sub and super solutions; Teschl and others,On sub and super solutions; Teschl and others,,"I'm reading Ordinary Differential Equations by Andersson and Böiers. There is a comparison theorem I have some questions about. I have also checked Teschl's Ordinary Differential Equations and Dynamical Systems, but there I have problems with his definition of a sub solution. I'll elaborate below. What follows is the comparison theorem in the book I first stated: Theorem . Assume that $f(t,x)$ is a continuous function in the strip $\{(t,x); t_0\leq t\leq t_1\}$ and satisfies a Lipschitz condition in a neighborhood of every point there. Furthermore, assume that $x(t)$ and $y(t)$ satisfy $$x'(t)=f(t,x)\quad\text{and}\quad y'(t)\geq f(t,y)$$ respectively, when $t_0\leq t\leq t_1$ . Then $$x(t_0)=y(t_0)\implies x(t)\leq y(t)\quad\text{when }t_0\leq t\leq t_1.$$ This definition is not made in the book, but I guess $y(t)$ is called a super solution. What confuses me in this theorem are the inequalities and how the theorem is modified when we change some of the inequalities to strict inequalities. First, I assume a corresponding result holds for a function $w(t)$ that satisfies $w'(t)\leq f(t,w)$ , so that $x(t_0)=w(t_0)\implies x(t)\geq w(t)$ when $t_0\leq t\leq t_1$ , right? Second, I'm working a problem where a function $y(t)$ satisfies $y'(t)> f(t,y)$ on a half-open strip, i.e. $t_0\leq t<t_1$ (because it is undefined at $t_1$ ). So how is the conclusion of the theorem modified if we change the assumptions to $y'(t)> f(t,y)$ and a half-open strip? Finally, in Teschl's book, he defines a sub solution $w(t)$ to be a function that satisfies $w'(t)< f(t,w)$ for $t_0\leq t<t_1$ . However, in my problem, I have a function $w(t)$ that satisfies $w'(t)\leq  f(t,w)$ for $t_0\leq t<t_1$ (in particular, $w'(t_0)=f(t_0,w(t_0))$ . Is this not a sub solution then? For completion, I post the proof of the theorem here. You can skip this of course. It uses the following lemma, stated without proof for the sake of brevity; Lemma . Let $x(t)$ be a differentiable function such that $$x'(t)\leq > Mx(t)+a,$$ where $M\neq 0$ and $a$ are fixed constants. Then $$x(t)\leq e^{M(t-t_0)}x(t_0)+\frac{a}{M}(e^{M(t-t_0)}-1),\quad t\geq > t_0.$$ Proof (of theorem). Assume that there is some point $\tau$ in the interval $[t_0,t_1]$ where $x(\tau)>y(\tau)$ . Then let $\bar t$ be the largest $t$ in $[t_0,\tau]$ with $x(t)\leq y(t)$ . Put $z(t)=x(t)-y(t)$ . Then $z(t)>0$ in $(\bar t,\tau]$ and $z(\bar t)=0$ . Furthermore, for $t$ near $\bar t$ , $$z'(t)=x'(t)-y'(t)\leq f(t,x(t))-f(t,y(t))\leq L(x(t)-y(t))=Lz(t).$$ The first inequality comes from the assumptions on $x(t)$ and $y(t)$ , the second one makes use of the Lipschitz condition. [The] lemma (with $a=0$ ) now implies, for $t$ in a right neighborhood of $\bar t$ , $$z(t)\leq e^{L(t-\bar t)}z(\bar t)=0.$$ We have arrived at a contradiction.","I'm reading Ordinary Differential Equations by Andersson and Böiers. There is a comparison theorem I have some questions about. I have also checked Teschl's Ordinary Differential Equations and Dynamical Systems, but there I have problems with his definition of a sub solution. I'll elaborate below. What follows is the comparison theorem in the book I first stated: Theorem . Assume that is a continuous function in the strip and satisfies a Lipschitz condition in a neighborhood of every point there. Furthermore, assume that and satisfy respectively, when . Then This definition is not made in the book, but I guess is called a super solution. What confuses me in this theorem are the inequalities and how the theorem is modified when we change some of the inequalities to strict inequalities. First, I assume a corresponding result holds for a function that satisfies , so that when , right? Second, I'm working a problem where a function satisfies on a half-open strip, i.e. (because it is undefined at ). So how is the conclusion of the theorem modified if we change the assumptions to and a half-open strip? Finally, in Teschl's book, he defines a sub solution to be a function that satisfies for . However, in my problem, I have a function that satisfies for (in particular, . Is this not a sub solution then? For completion, I post the proof of the theorem here. You can skip this of course. It uses the following lemma, stated without proof for the sake of brevity; Lemma . Let be a differentiable function such that where and are fixed constants. Then Proof (of theorem). Assume that there is some point in the interval where . Then let be the largest in with . Put . Then in and . Furthermore, for near , The first inequality comes from the assumptions on and , the second one makes use of the Lipschitz condition. [The] lemma (with ) now implies, for in a right neighborhood of , We have arrived at a contradiction.","f(t,x) \{(t,x); t_0\leq t\leq t_1\} x(t) y(t) x'(t)=f(t,x)\quad\text{and}\quad y'(t)\geq f(t,y) t_0\leq t\leq t_1 x(t_0)=y(t_0)\implies x(t)\leq y(t)\quad\text{when }t_0\leq t\leq t_1. y(t) w(t) w'(t)\leq f(t,w) x(t_0)=w(t_0)\implies x(t)\geq w(t) t_0\leq t\leq t_1 y(t) y'(t)> f(t,y) t_0\leq t<t_1 t_1 y'(t)> f(t,y) w(t) w'(t)< f(t,w) t_0\leq t<t_1 w(t) w'(t)\leq  f(t,w) t_0\leq t<t_1 w'(t_0)=f(t_0,w(t_0)) x(t) x'(t)\leq
> Mx(t)+a, M\neq 0 a x(t)\leq e^{M(t-t_0)}x(t_0)+\frac{a}{M}(e^{M(t-t_0)}-1),\quad t\geq
> t_0. \tau [t_0,t_1] x(\tau)>y(\tau) \bar t t [t_0,\tau] x(t)\leq y(t) z(t)=x(t)-y(t) z(t)>0 (\bar t,\tau] z(\bar t)=0 t \bar t z'(t)=x'(t)-y'(t)\leq f(t,x(t))-f(t,y(t))\leq L(x(t)-y(t))=Lz(t). x(t) y(t) a=0 t \bar t z(t)\leq e^{L(t-\bar t)}z(\bar t)=0.",['ordinary-differential-equations']
12,Solution of Bernoulli's differential equation,Solution of Bernoulli's differential equation,,"I am trying to solve the following Bernoulli's equation: $\frac{dx}{dt}=a(t)x-b(t)x^2$ . with $x(0)=x_0$ and $a(t)>0$ , $b(t)>0$ . On the way to solution, I make the substitution $u=1/x$ implying $u'=-x^{-2}\frac{dx}{dt}$ to transform it to the following equation in $u$ . $u'+a(t)u-b(t)=0$ , which is a linear first order ODE, can be solved via Integrating factor: $e^{\int a(t)dt}$ . Leading to the following: $\frac{d}{dt}\left[e^{\int a(t)dt)}*u\right]=b(t)e^{\int a(t)dt}$ $\implies u=\frac{1}{e^{\int a(t)dt}}\int b(t)e^{\int a(\tau)d\tau}dt$ Lets call $\int a(t)dt=\alpha(t)$ . $\implies u=\frac{1}{e^\alpha}\int b(t)e^{\alpha}dt$ . Integrate by parts: $\int b(t)e^{\alpha(t)}dt=b\int e^{\alpha(t)}dt-\int b(t)\left[\int e^{\alpha(\tau)}d\tau\right]dt+c$ which means: $u=\frac{1}{e^{\alpha(t)}}\left[b\int e^{\alpha(t)}dt-\int b(t)\left[\int e^{\alpha(\tau)}d\tau\right]dt \right]$ , this leads us to the solution $x(t)$ by substituting back $u=x^{-1}$ and $\alpha=\int a(t) dt$ i.e., $x(t)=\frac{e^{\int a(\tau)d\tau}}{\frac{1}{x_0}+b\int e^{\alpha(t)}dt-\int b(t)\left[\int e^{\alpha(\tau)}d\tau\right]dt}$ .  where $c=1/x_0$ . However, the solution that I needed to come to is the following: $x(t)=\frac{e^{\int a(\tau)d\tau}}{\frac{1}{x_0}+\int b(t)e^{\int a(\tau)d\tau}dt}$ . So I have one extra term in my solution $b(t)\int e^{\int a dt} $ , that emerges from the 'uv' term in integration by parts. I would appreciate if someone can point out where I am doing a mistake?","I am trying to solve the following Bernoulli's equation: . with and , . On the way to solution, I make the substitution implying to transform it to the following equation in . , which is a linear first order ODE, can be solved via Integrating factor: . Leading to the following: Lets call . . Integrate by parts: which means: , this leads us to the solution by substituting back and i.e., .  where . However, the solution that I needed to come to is the following: . So I have one extra term in my solution , that emerges from the 'uv' term in integration by parts. I would appreciate if someone can point out where I am doing a mistake?",\frac{dx}{dt}=a(t)x-b(t)x^2 x(0)=x_0 a(t)>0 b(t)>0 u=1/x u'=-x^{-2}\frac{dx}{dt} u u'+a(t)u-b(t)=0 e^{\int a(t)dt} \frac{d}{dt}\left[e^{\int a(t)dt)}*u\right]=b(t)e^{\int a(t)dt} \implies u=\frac{1}{e^{\int a(t)dt}}\int b(t)e^{\int a(\tau)d\tau}dt \int a(t)dt=\alpha(t) \implies u=\frac{1}{e^\alpha}\int b(t)e^{\alpha}dt \int b(t)e^{\alpha(t)}dt=b\int e^{\alpha(t)}dt-\int b(t)\left[\int e^{\alpha(\tau)}d\tau\right]dt+c u=\frac{1}{e^{\alpha(t)}}\left[b\int e^{\alpha(t)}dt-\int b(t)\left[\int e^{\alpha(\tau)}d\tau\right]dt \right] x(t) u=x^{-1} \alpha=\int a(t) dt x(t)=\frac{e^{\int a(\tau)d\tau}}{\frac{1}{x_0}+b\int e^{\alpha(t)}dt-\int b(t)\left[\int e^{\alpha(\tau)}d\tau\right]dt} c=1/x_0 x(t)=\frac{e^{\int a(\tau)d\tau}}{\frac{1}{x_0}+\int b(t)e^{\int a(\tau)d\tau}dt} b(t)\int e^{\int a dt} ,"['calculus', 'ordinary-differential-equations']"
13,What is the difference between a general solution of a differential equation and a family of solutions to a differential equation?,What is the difference between a general solution of a differential equation and a family of solutions to a differential equation?,,"I'm having trouble learning differential equations. I'm a bit confused about the difference between a general solution of a differential equation and a family of solutions to a differential equation. I've repeatedly read obscure textbooks and concluded the following: The general solution of a differential equation refers to the set of all solutions that satisfy the differential equation, and it usually contains some arbitrary constants, which can be determined according to the initial conditions or boundary conditions. For example, the general solution to the differential equation $y′=y$ is $y=Ce^{x}$ , where $C$ is an arbitrary constant. a family of solutions to a differential equation refers to a class of special solutions that satisfy differential equations, and there is a certain relationship between them, usually in the form of parameterization. For example, the family of differential equations $y′=y$ is $y=ae^{x+b}$ , where a and b are parameters (1)The general solution of differential equations contains all the solutions satisfying the differential equations, while the family of solutions to a differential equation to a differential equation contains only a part of the solutions satisfying the differential equations. (2)Any constant in the general solution of differential equations can determine the unique solution through initial conditions or boundary conditions, but the parameters in the  family of solutions to a differential equation cannot determine the unique solution through initial conditions or boundary conditions, but need additional parameter values. (3)The family of solutions to a differential equation can be obtained by some transformation of the general solution of differential equations, but not necessarily vice versa May I ask if there are any mistakes in the summary of the above three points?","I'm having trouble learning differential equations. I'm a bit confused about the difference between a general solution of a differential equation and a family of solutions to a differential equation. I've repeatedly read obscure textbooks and concluded the following: The general solution of a differential equation refers to the set of all solutions that satisfy the differential equation, and it usually contains some arbitrary constants, which can be determined according to the initial conditions or boundary conditions. For example, the general solution to the differential equation is , where is an arbitrary constant. a family of solutions to a differential equation refers to a class of special solutions that satisfy differential equations, and there is a certain relationship between them, usually in the form of parameterization. For example, the family of differential equations is , where a and b are parameters (1)The general solution of differential equations contains all the solutions satisfying the differential equations, while the family of solutions to a differential equation to a differential equation contains only a part of the solutions satisfying the differential equations. (2)Any constant in the general solution of differential equations can determine the unique solution through initial conditions or boundary conditions, but the parameters in the  family of solutions to a differential equation cannot determine the unique solution through initial conditions or boundary conditions, but need additional parameter values. (3)The family of solutions to a differential equation can be obtained by some transformation of the general solution of differential equations, but not necessarily vice versa May I ask if there are any mistakes in the summary of the above three points?",y′=y y=Ce^{x} C y′=y y=ae^{x+b},"['ordinary-differential-equations', 'terminology']"
14,How is the damping equation obtained?,How is the damping equation obtained?,,"I modified some online notes in the internet and prepared this illustration for later use. Why does the formula fail? If $B=0$ (it is a sinusoidal system with no exponential components, undamped oscillation), it gets the form $x'' = -Cx$ . This is a sinusoidal movement with $\omega_0^2$ as C. Everything is okay here. If $C=0$ (it is an exponential system with no sinusoidal components, damped without oscillation for positive B), it gets the form $x' = -Bx + k$ after an integral is applied. This has a decay rate $B$ (not $\frac{B}{2}$ ! Here the formula fails) with $\lambda = -\lambda_0 = B$ . $$x'' + 2\zeta\omega_0 x' + \omega_0^2 x = 0$$ Is not this a general formula applicable to any possible situation? 2. $$\frac{\sqrt{B^2 - 4\omega_0^2}}{2} = \pm\omega_0i$$ $\omega_0^2 = C$ also produces mistaken results when $B \ne 0$ and I put it into the complex component of the quadratic formula. With some manipulation, it will take the form (multiplied by 2i): $$\sqrt{-B^2 + 4\omega_0^2} = \pm2\omega_0$$ $$-B^2 + 4\omega_0^2 = 4\omega_0^2$$ $$B=0$$ But this contradicts our very assumption that $B \ne 0$ . What am I confusing here? Why define $\zeta$ (the Damping Ratio) as $\frac{\lambda}{\omega_0}$ ? I think this comes from Euler's equation for a complex number as an exponent of $e$ : $$e^{(\lambda_0 \pm \omega_0i)t} = e^{\lambda_0t}e^{(\pm\omega_0t)i}$$ Here the exponential with the imaginary power produces an oscillating, sinusoidal factor, while the other is about decay. So decay divided by oscillation should overall say us how neatly the system is stabilizing. A high $\zeta$ means more damping and less oscillation ( $\zeta = 1$ being the best and more than it is overdamped). But now is not the formula Wikipedia gives ( $\frac{\lambda}{\sqrt{\lambda^2 + \omega_0^2}}$ ) more formal and logical? For that, it compares the decay rate (the real component of $r$ ) to the magnitude of $r$ . I am quite a bit confused. Hopefully not too many questions. Thanks! I have this unfortunate habit to not progress in my course until every detail makes sense to me.","I modified some online notes in the internet and prepared this illustration for later use. Why does the formula fail? If (it is a sinusoidal system with no exponential components, undamped oscillation), it gets the form . This is a sinusoidal movement with as C. Everything is okay here. If (it is an exponential system with no sinusoidal components, damped without oscillation for positive B), it gets the form after an integral is applied. This has a decay rate (not ! Here the formula fails) with . Is not this a general formula applicable to any possible situation? 2. also produces mistaken results when and I put it into the complex component of the quadratic formula. With some manipulation, it will take the form (multiplied by 2i): But this contradicts our very assumption that . What am I confusing here? Why define (the Damping Ratio) as ? I think this comes from Euler's equation for a complex number as an exponent of : Here the exponential with the imaginary power produces an oscillating, sinusoidal factor, while the other is about decay. So decay divided by oscillation should overall say us how neatly the system is stabilizing. A high means more damping and less oscillation ( being the best and more than it is overdamped). But now is not the formula Wikipedia gives ( ) more formal and logical? For that, it compares the decay rate (the real component of ) to the magnitude of . I am quite a bit confused. Hopefully not too many questions. Thanks! I have this unfortunate habit to not progress in my course until every detail makes sense to me.",B=0 x'' = -Cx \omega_0^2 C=0 x' = -Bx + k B \frac{B}{2} \lambda = -\lambda_0 = B x'' + 2\zeta\omega_0 x' + \omega_0^2 x = 0 \frac{\sqrt{B^2 - 4\omega_0^2}}{2} = \pm\omega_0i \omega_0^2 = C B \ne 0 \sqrt{-B^2 + 4\omega_0^2} = \pm2\omega_0 -B^2 + 4\omega_0^2 = 4\omega_0^2 B=0 B \ne 0 \zeta \frac{\lambda}{\omega_0} e e^{(\lambda_0 \pm \omega_0i)t} = e^{\lambda_0t}e^{(\pm\omega_0t)i} \zeta \zeta = 1 \frac{\lambda}{\sqrt{\lambda^2 + \omega_0^2}} r r,"['ordinary-differential-equations', 'control-theory']"
15,About differentiable dependence in a Cauchy problem,About differentiable dependence in a Cauchy problem,,"Let $\epsilon >0$ , consider the Cauchy problem: $$\epsilon x' = x^2 + (1-\epsilon)t \quad , x(0)=1$$ If $x(t;\epsilon)$ denotes the solution (defined on the maximum interval) of the problem, I'm asked to verify that: $$\frac{\partial x}{\partial \epsilon} (t,1) = \frac{t}{(1-t)^2} \left ( \frac{t^2}{3} - \frac{t}{2} -1 \right )$$ I'm not able to get the previous expression and I'm getting kinda crazy xd. I know for a fact that the function $u = \partial_\epsilon (\cdot,1)$ solves the linear problem: $$u' = f_x (t,x(t;1),1) u + f_\epsilon (t,x(t;1),1) \quad , u(0)=0$$ In this case $f(t,x,\epsilon) = \frac{x^2}{\epsilon} + \frac{1-\epsilon}{\epsilon} t$ so: $$f_x (t,x,\epsilon) = \frac{2x}{\epsilon} \qquad f_\epsilon (t,x,\epsilon) = - \frac{x^2}{\epsilon^2} - \frac{t}{\epsilon^2}$$ , and the Cauchy problem I should work on is: $$u' = \frac{2}{1-t}u -t - \frac{1}{(1-t)^2} \quad , u(0)=0$$ , since $x(t;1) = (1-t)^{-1}$ . The solution to this IVP is: $$u(t) = - \frac{t (3t^3 - 8t^2 +6t+12)}{12 (1-t)^2}$$ (by Wolfram-Alpha), and it doesn't match with the solution given. Is there any mistake in my work?. ANY suggestion will be appreciated :)","Let , consider the Cauchy problem: If denotes the solution (defined on the maximum interval) of the problem, I'm asked to verify that: I'm not able to get the previous expression and I'm getting kinda crazy xd. I know for a fact that the function solves the linear problem: In this case so: , and the Cauchy problem I should work on is: , since . The solution to this IVP is: (by Wolfram-Alpha), and it doesn't match with the solution given. Is there any mistake in my work?. ANY suggestion will be appreciated :)","\epsilon >0 \epsilon x' = x^2 + (1-\epsilon)t \quad , x(0)=1 x(t;\epsilon) \frac{\partial x}{\partial \epsilon} (t,1) = \frac{t}{(1-t)^2} \left ( \frac{t^2}{3} - \frac{t}{2} -1 \right ) u = \partial_\epsilon (\cdot,1) u' = f_x (t,x(t;1),1) u + f_\epsilon (t,x(t;1),1) \quad , u(0)=0 f(t,x,\epsilon) = \frac{x^2}{\epsilon} + \frac{1-\epsilon}{\epsilon} t f_x (t,x,\epsilon) = \frac{2x}{\epsilon} \qquad f_\epsilon (t,x,\epsilon) = - \frac{x^2}{\epsilon^2} - \frac{t}{\epsilon^2} u' = \frac{2}{1-t}u -t - \frac{1}{(1-t)^2} \quad , u(0)=0 x(t;1) = (1-t)^{-1} u(t) = - \frac{t (3t^3 - 8t^2 +6t+12)}{12 (1-t)^2}","['ordinary-differential-equations', 'analysis', 'cauchy-problem']"
16,How can I solve this second order differential equation?,How can I solve this second order differential equation?,,"So, I have to solve the following system $$\frac{d}{dt}x(t) = x(t)y(t)$$ $$\frac{d}{dt}y(t) = x(t)$$ I can just substitute $x(t)$ in the first equation and get $$\frac{d^2}{dt^2}y(t) = y(t)\frac{dy(t)}{dt}$$ but I can't go further. I also tried to use python but gave me a weird function. How can I go on and solve this equation? Edit: I tried integrating both sides and got $$\frac{dy}{dt} = \frac{y(t)^2}{2} + c_1$$ and then integrated again but got the wrong answer.","So, I have to solve the following system I can just substitute in the first equation and get but I can't go further. I also tried to use python but gave me a weird function. How can I go on and solve this equation? Edit: I tried integrating both sides and got and then integrated again but got the wrong answer.",\frac{d}{dt}x(t) = x(t)y(t) \frac{d}{dt}y(t) = x(t) x(t) \frac{d^2}{dt^2}y(t) = y(t)\frac{dy(t)}{dt} \frac{dy}{dt} = \frac{y(t)^2}{2} + c_1,"['calculus', 'ordinary-differential-equations']"
17,Derivation of an asymptotic error formula for the Trapezoidal method for IVPs,Derivation of an asymptotic error formula for the Trapezoidal method for IVPs,,"I am trying to prove the following theorem, which is a paraphrased version of Exercise 6.18 in Kendall Atkinson's An Introduction to Numerical Analysis : Theorem. Let $[x_0,b]$ be a finite interval, let $h > 0$ , and let $N \equiv N(h) := \lfloor \frac{b-x_0}{h} \rfloor$ . Let $f \in C^3([x_0,b] \times \mathbb{R},\mathbb{R})$ be uniformly Lipschitz in its second argument with constant $K \geq 0$ . Assume $f_{yy}$ is bounded and that $hK \leq 1$ . Let $Y:[x_0,b] \to \mathbb{R}$ be the unique solution to the IVP \begin{equation}        \hspace{3.7cm} Y'(x) = f(x,Y(x)), \quad Y(x_0) = Y_0.  \hspace{3.8cm} (1) \end{equation} Let $x_n := x_0 + nh$ for $n=0,1,\ldots,N(h)$ , let $y_0 \in \mathbb{R}$ , and let $(y_h(x_n))_{n \geq 1} = (y_n)_{n \geq 1}$ be defined according to the implicit Trapezoidal method applied to (1): \begin{equation}    \hspace{2cm} y_{n+1} = y_n + \frac{h}{2}\left[f(x_n,y_n) + f(x_{n+1},y_{n+1}) \right], \qquad n \geq 0.  \hspace{2.1cm} (2) \end{equation} Let $e_n := Y(x_n) - y_n$ be the error at the $n$ -th iterate, let $\delta_0 \in \mathbb{R}$ , and assume that the initial error satisfies $e_0 = \delta_0 h^2 + O(h^3)$ . Then \begin{align*}     \hspace{5cm} e_n = D(x_n)h^2 + O(h^3) \hspace{5cm} (3) \end{align*} for all $n \geq 0$ , where $D:[x_0,b] \to \mathbb{R}$ is the solution of the IVP \begin{equation}    \hspace{1.95cm} D'(x) = f_y(x,Y(x))D(x) - \frac{1}{12}Y^{(3)}(x), \quad D(x_0) = \delta_0. \hspace{2.1cm} (4)  \end{equation} Some remarks: The Lipschitz condition on $f$ is not really relevant to this proof; it's just there to guarantee the convergence of the Trapezoidal method. Atkinson proves a similar asymptotic error formula for the forward Euler method (p.352-354), which I have included at the end of this post for reference. Assuming the proof of the above theorem is analogous, I think it should follow these steps: Step 1 . Derive a recurrence relation for the error $e_n$ in terms of $h$ , $f$ and $Y$ , using equation (2) and the truncation error of the trapezoidal method. Step 2 . Identify the so-called principal part of the error, call it $g_n$ , in the recurrence formula for $e_n$ . (The principal part of the error consists of the terms that make the dominant contribution to the error.) Set up a new recurrence formula of the form $g_{n+1} := F(g_n)$ for the accumulation of the principal part of the error. Step 3 . By making an appropriate change of variables from $g_n$ to $\delta_n := h^{-2} g_n$ (or something like that), obtain a formula that is the Trapezoidal method applied to the IVP in (4), with $\delta_{n+1}$ playing the role of $y_{n+1}$ : \begin{align*}    \hspace{5mm} \delta_{n+1} = \delta_n + \tfrac{h}{2} \big[ f_y(x_n,Y(x_n)) \delta_n - \tfrac{1}{12}Y^{(3)}(x_n) + f_y(x_{n+1},Y(x_{n+1})) \delta_{n+1} - \tfrac{1}{12}Y^{(3)}(x_{n+1}) \big]  \hspace{5mm} (5) \end{align*} Step 4 . Prove that $g_n = D(x_n)h^2 + O(h^3)$ and $e_n - g_n = O(h^m)$ for some $m \geq 3$ . The first equality follows easily from Step 3: Since the Trapezoidal method is convergent as $h \to 0$ , we have \begin{align*}    \delta_n - D(x_n) = O(h). \end{align*} Multiplying through by $h^2$ then gives $g_n - D(x_n)h^2 = O(h^3).$ I think I should be able to prove the second equality. We'll then have \begin{align*}     e_n &= g_n + [e_n - g_n] \\[3pt]         &= D(x_n)h^2 + O(h^3) + O(h^m) \\[3pt]         &= D(x_n)h^2 + O(h^3)  \end{align*} as desired. My main difficulty is in Step 3. Here is what I have come up with so far. Step 1 : Firstly, since $f \in C^3([x_0,b] \times \mathbb{R},\mathbb{R})$ , we have $Y \in C^4([x_0,b],\mathbb{R})$ (by this post ). Then from the local truncation error formula of the Trapezoidal rule (equation (6.5.1) in Atkinson), \begin{align*}     Y(x_{n+1}) = Y(x_n) + \frac{h}{2}\big[f(x_n,Y(x_n)) + f(x_{n+1},Y(x_{n+1})) \big] - \frac{h^3}{12} Y^{(3)}(\xi_n) \end{align*} for some $\xi_n \in [x_n,x_{n+1}]$ . By Taylor's Theorem applied to the last term, $$ Y^{(3)}(x_n) = Y^{(3)}(x_n) + Y^{(4)}(\xi_n)(x_n - \xi_n)$$ and so we can write \begin{equation}     Y(x_{n+1}) = Y(x_n) + \frac{h}{2}\big[f(x_n,Y(x_n)) + f(x_{n+1},Y(x_{n+1})) \big] - \frac{h^3}{12} Y^{(3)}(x_n) + O(h^4). \qquad (6)  \end{equation} To ease notation, let $Y_n := Y(x_n)$ . Then subtracting (6) from (2) gives \begin{align*}    e_{n+1} &= e_n + \frac{h}{2}\left[f(x_n,Y_n) - f(x_n,y_n) + f(x_{n+1},Y_{n+1}) - f(x_{n+1},y_{n+1})  \right] \\[4pt]            &\quad - \frac{h^3}{12} Y^{(3)}(x_n) + O(h^4).  \qquad (7) \end{align*} Now applying Taylor's theorem to the functions $y \mapsto f(x_n,y)$ , we get \begin{align*}     f(x_n,y_n) = f(x_n,Y_n) + f_y(x_n,Y_n)(y_n - Y_n) + f_{yy}(x_n,\zeta_n) \frac{(y_n - Y_n)^2}{2} \end{align*} for some $\zeta_n$ between $y_n$ and $Y_n$ . Rearranging, we get \begin{align*}     f(x_n,Y_n) - f(x_n,y_n) = e_n f_y(x_n,Y_n) - \tfrac{1}{2}f_{yy}(x_n,\zeta_n)e_n^2.  \end{align*} Similarly, \begin{align*}     f(x_{n+1},Y_{n+1}) - f(x_n,y_n) = e_n f_y(x_{n+1},Y_{n+1}) - \tfrac{1}{2}f_{yy}(x_{n+1},\zeta_{n+1})e_n^2  \end{align*} for some $\zeta_{n+1}$ between $y_{n+1}$ and $Y_{n+1}$ . Substituting the above into (7) gives \begin{align*}     e_{n+1} &= e_n + \frac{h}{2} \left[ e_n f_y(x_n,Y_n) - \tfrac{1}{2}f_{yy}(x_n,\zeta_n)e_n^2 +  e_n f_y(x_{n+1},Y_{n+1}) - \tfrac{1}{2}f_{yy}(x_{n+1},\zeta_{n+1})e_n^2  \right] \\[5pt]     & \quad -\frac{h^3}{12} Y^{(3)}(x_n) + O(h^4) \\[5pt]     & \hspace{-5mm} = \left[1 + \frac{h}{2}f_y(x_n,Y_n) + \frac{h}{2}f_y(x_{n+1},Y_{n+1}) \right] e_n - \frac{h^3}{12} Y^{(3)}(x_n) - \frac{h}{4}e_n^2 \left[f_{yy}(x_n,\zeta_n) + f_{yy}(x_{n+1},\zeta_{n+1}) \right] + O(h^4) \end{align*} Step 2. We should have $e_n = O(h^2)$ for the Trapezoidal method (by (6.5.18) in Atkinson), so we have $\frac{h}{4}e_n^2 \left[f_{yy}(x_n,\zeta_n) + f_{yy}(x_{n+1},\zeta_{n+1}) \right] = O(h^5)$ . Dropping this term and the $O(h^4)$ terms, we define \begin{align*}     \hspace{2cm} g_{n+1} =  \left[1 + \frac{h}{2}f_y(x_n,Y_n) + \frac{h}{2}f_y(x_{n+1},Y_{n+1}) \right] g_n - \frac{h^3}{12} Y^{(3)}(x_n), \qquad n \geq 0  \hspace{2cm} (8) \end{align*} with $g_0 := \delta_0 h^2$ , to represent the principal part of the error. Step 3. Now define a new sequence $\delta_n := g_n/h^2$ . Then write (8) in terms of $\delta_n$ : \begin{align*}     h^2 \delta_{n+1} =  \left[1 + \frac{h}{2}f_y(x_n,Y_n) + \frac{h}{2}f_y(x_{n+1},Y_{n+1}) \right] h^2 \delta_n - \frac{h^3}{12} Y^{(3)}(x_n)  \end{align*} Cancelling out $h^2$ gives and rearranging leads to the equation \begin{align*}     \delta_{n+1} = \delta_n + \frac{h}{2}\left[f_y(x_n,Y_n) \delta_n + f_y(x_{n+1},Y_{n+1}) \delta_n - \frac{1}{6} Y^{(3)}(x_n)   \right] \end{align*} This is the point where I got stuck. How can arrive at equation (5)? Or did I go wrong in a previous step? Any help would be greatly appreciated! For reference, here is Atkinson's proof of a similar theorem for Euler's method:","I am trying to prove the following theorem, which is a paraphrased version of Exercise 6.18 in Kendall Atkinson's An Introduction to Numerical Analysis : Theorem. Let be a finite interval, let , and let . Let be uniformly Lipschitz in its second argument with constant . Assume is bounded and that . Let be the unique solution to the IVP Let for , let , and let be defined according to the implicit Trapezoidal method applied to (1): Let be the error at the -th iterate, let , and assume that the initial error satisfies . Then for all , where is the solution of the IVP Some remarks: The Lipschitz condition on is not really relevant to this proof; it's just there to guarantee the convergence of the Trapezoidal method. Atkinson proves a similar asymptotic error formula for the forward Euler method (p.352-354), which I have included at the end of this post for reference. Assuming the proof of the above theorem is analogous, I think it should follow these steps: Step 1 . Derive a recurrence relation for the error in terms of , and , using equation (2) and the truncation error of the trapezoidal method. Step 2 . Identify the so-called principal part of the error, call it , in the recurrence formula for . (The principal part of the error consists of the terms that make the dominant contribution to the error.) Set up a new recurrence formula of the form for the accumulation of the principal part of the error. Step 3 . By making an appropriate change of variables from to (or something like that), obtain a formula that is the Trapezoidal method applied to the IVP in (4), with playing the role of : Step 4 . Prove that and for some . The first equality follows easily from Step 3: Since the Trapezoidal method is convergent as , we have Multiplying through by then gives I think I should be able to prove the second equality. We'll then have as desired. My main difficulty is in Step 3. Here is what I have come up with so far. Step 1 : Firstly, since , we have (by this post ). Then from the local truncation error formula of the Trapezoidal rule (equation (6.5.1) in Atkinson), for some . By Taylor's Theorem applied to the last term, and so we can write To ease notation, let . Then subtracting (6) from (2) gives Now applying Taylor's theorem to the functions , we get for some between and . Rearranging, we get Similarly, for some between and . Substituting the above into (7) gives Step 2. We should have for the Trapezoidal method (by (6.5.18) in Atkinson), so we have . Dropping this term and the terms, we define with , to represent the principal part of the error. Step 3. Now define a new sequence . Then write (8) in terms of : Cancelling out gives and rearranging leads to the equation This is the point where I got stuck. How can arrive at equation (5)? Or did I go wrong in a previous step? Any help would be greatly appreciated! For reference, here is Atkinson's proof of a similar theorem for Euler's method:","[x_0,b] h > 0 N \equiv N(h) := \lfloor \frac{b-x_0}{h} \rfloor f \in C^3([x_0,b] \times \mathbb{R},\mathbb{R}) K \geq 0 f_{yy} hK \leq 1 Y:[x_0,b] \to \mathbb{R} \begin{equation}
       \hspace{3.7cm} Y'(x) = f(x,Y(x)), \quad Y(x_0) = Y_0.  \hspace{3.8cm} (1)
\end{equation} x_n := x_0 + nh n=0,1,\ldots,N(h) y_0 \in \mathbb{R} (y_h(x_n))_{n \geq 1} = (y_n)_{n \geq 1} \begin{equation}
   \hspace{2cm} y_{n+1} = y_n + \frac{h}{2}\left[f(x_n,y_n) + f(x_{n+1},y_{n+1}) \right], \qquad n \geq 0.  \hspace{2.1cm} (2)
\end{equation} e_n := Y(x_n) - y_n n \delta_0 \in \mathbb{R} e_0 = \delta_0 h^2 + O(h^3) \begin{align*}
    \hspace{5cm} e_n = D(x_n)h^2 + O(h^3) \hspace{5cm} (3)
\end{align*} n \geq 0 D:[x_0,b] \to \mathbb{R} \begin{equation}
   \hspace{1.95cm} D'(x) = f_y(x,Y(x))D(x) - \frac{1}{12}Y^{(3)}(x), \quad D(x_0) = \delta_0. \hspace{2.1cm} (4) 
\end{equation} f e_n h f Y g_n e_n g_{n+1} := F(g_n) g_n \delta_n := h^{-2} g_n \delta_{n+1} y_{n+1} \begin{align*}
   \hspace{5mm} \delta_{n+1} = \delta_n + \tfrac{h}{2} \big[ f_y(x_n,Y(x_n)) \delta_n - \tfrac{1}{12}Y^{(3)}(x_n) + f_y(x_{n+1},Y(x_{n+1})) \delta_{n+1} - \tfrac{1}{12}Y^{(3)}(x_{n+1}) \big]  \hspace{5mm} (5)
\end{align*} g_n = D(x_n)h^2 + O(h^3) e_n - g_n = O(h^m) m \geq 3 h \to 0 \begin{align*}
   \delta_n - D(x_n) = O(h).
\end{align*} h^2 g_n - D(x_n)h^2 = O(h^3). \begin{align*}
    e_n &= g_n + [e_n - g_n] \\[3pt]
        &= D(x_n)h^2 + O(h^3) + O(h^m) \\[3pt]
        &= D(x_n)h^2 + O(h^3) 
\end{align*} f \in C^3([x_0,b] \times \mathbb{R},\mathbb{R}) Y \in C^4([x_0,b],\mathbb{R}) \begin{align*}
    Y(x_{n+1}) = Y(x_n) + \frac{h}{2}\big[f(x_n,Y(x_n)) + f(x_{n+1},Y(x_{n+1})) \big] - \frac{h^3}{12} Y^{(3)}(\xi_n)
\end{align*} \xi_n \in [x_n,x_{n+1}]  Y^{(3)}(x_n) = Y^{(3)}(x_n) + Y^{(4)}(\xi_n)(x_n - \xi_n) \begin{equation}
    Y(x_{n+1}) = Y(x_n) + \frac{h}{2}\big[f(x_n,Y(x_n)) + f(x_{n+1},Y(x_{n+1})) \big] - \frac{h^3}{12} Y^{(3)}(x_n) + O(h^4). \qquad (6) 
\end{equation} Y_n := Y(x_n) \begin{align*}
   e_{n+1} &= e_n + \frac{h}{2}\left[f(x_n,Y_n) - f(x_n,y_n) + f(x_{n+1},Y_{n+1}) - f(x_{n+1},y_{n+1})  \right] \\[4pt]
           &\quad - \frac{h^3}{12} Y^{(3)}(x_n) + O(h^4).  \qquad (7)
\end{align*} y \mapsto f(x_n,y) \begin{align*}
    f(x_n,y_n) = f(x_n,Y_n) + f_y(x_n,Y_n)(y_n - Y_n) + f_{yy}(x_n,\zeta_n) \frac{(y_n - Y_n)^2}{2}
\end{align*} \zeta_n y_n Y_n \begin{align*}
    f(x_n,Y_n) - f(x_n,y_n) = e_n f_y(x_n,Y_n) - \tfrac{1}{2}f_{yy}(x_n,\zeta_n)e_n^2. 
\end{align*} \begin{align*}
    f(x_{n+1},Y_{n+1}) - f(x_n,y_n) = e_n f_y(x_{n+1},Y_{n+1}) - \tfrac{1}{2}f_{yy}(x_{n+1},\zeta_{n+1})e_n^2 
\end{align*} \zeta_{n+1} y_{n+1} Y_{n+1} \begin{align*}
    e_{n+1} &= e_n + \frac{h}{2} \left[ e_n f_y(x_n,Y_n) - \tfrac{1}{2}f_{yy}(x_n,\zeta_n)e_n^2 +  e_n f_y(x_{n+1},Y_{n+1}) - \tfrac{1}{2}f_{yy}(x_{n+1},\zeta_{n+1})e_n^2  \right] \\[5pt]
    & \quad -\frac{h^3}{12} Y^{(3)}(x_n) + O(h^4) \\[5pt]
    & \hspace{-5mm} = \left[1 + \frac{h}{2}f_y(x_n,Y_n) + \frac{h}{2}f_y(x_{n+1},Y_{n+1}) \right] e_n - \frac{h^3}{12} Y^{(3)}(x_n) - \frac{h}{4}e_n^2 \left[f_{yy}(x_n,\zeta_n) + f_{yy}(x_{n+1},\zeta_{n+1}) \right] + O(h^4)
\end{align*} e_n = O(h^2) \frac{h}{4}e_n^2 \left[f_{yy}(x_n,\zeta_n) + f_{yy}(x_{n+1},\zeta_{n+1}) \right] = O(h^5) O(h^4) \begin{align*}
    \hspace{2cm} g_{n+1} =  \left[1 + \frac{h}{2}f_y(x_n,Y_n) + \frac{h}{2}f_y(x_{n+1},Y_{n+1}) \right] g_n - \frac{h^3}{12} Y^{(3)}(x_n), \qquad n \geq 0  \hspace{2cm} (8)
\end{align*} g_0 := \delta_0 h^2 \delta_n := g_n/h^2 \delta_n \begin{align*}
    h^2 \delta_{n+1} =  \left[1 + \frac{h}{2}f_y(x_n,Y_n) + \frac{h}{2}f_y(x_{n+1},Y_{n+1}) \right] h^2 \delta_n - \frac{h^3}{12} Y^{(3)}(x_n) 
\end{align*} h^2 \begin{align*}
    \delta_{n+1} = \delta_n + \frac{h}{2}\left[f_y(x_n,Y_n) \delta_n + f_y(x_{n+1},Y_{n+1}) \delta_n - \frac{1}{6} Y^{(3)}(x_n)   \right]
\end{align*}","['ordinary-differential-equations', 'numerical-methods', 'recurrence-relations', 'initial-value-problems']"
18,Minimizer over set of $C^1$ functions,Minimizer over set of  functions,C^1,"Exercise (1.4) of Renardy and Rogers: ""An introduction to partial differential equations"" asks to Show that there is an infinite family of minimizers of $$  J(u) = \int_0^1 (1-u'(t)^2)^2\,dt  $$ over the set of all piecewise $C^1$ functions satisfying $u(0)=u(1)=0$ . Now the corresponding Euler-Lagrange equation is $$ \frac{d}{dt} \left[4 (1-u'(t)^2) u'(t) \right] = 0. $$ I would argue that the equation above has no solutions satisfying the given boundary conditions. Integrating once leaves us with $$ (1-u'(t)^2) u'(t) = C $$ and, depending on $C$ , there might be zero to three solutions. However, any solution, if it exists, requires $u'(t)=\mathrm{const}$ and, consequently, $u(t)$ is linear and hence cannot satisfy the boundary conditions (except for the trivial solution). I suspect that the key to the solution lies in the fact that we allow piecewise $C^1$ functions. However, I cannot see how we could employ that. Doesn't any discontinuity in the first derivative show up as a ( $\delta$ ) source term in the Euler-Lagrange equation? I'd be grateful for any pointers. Thanks heaps!","Exercise (1.4) of Renardy and Rogers: ""An introduction to partial differential equations"" asks to Show that there is an infinite family of minimizers of over the set of all piecewise functions satisfying . Now the corresponding Euler-Lagrange equation is I would argue that the equation above has no solutions satisfying the given boundary conditions. Integrating once leaves us with and, depending on , there might be zero to three solutions. However, any solution, if it exists, requires and, consequently, is linear and hence cannot satisfy the boundary conditions (except for the trivial solution). I suspect that the key to the solution lies in the fact that we allow piecewise functions. However, I cannot see how we could employ that. Doesn't any discontinuity in the first derivative show up as a ( ) source term in the Euler-Lagrange equation? I'd be grateful for any pointers. Thanks heaps!","
 J(u) = \int_0^1 (1-u'(t)^2)^2\,dt
  C^1 u(0)=u(1)=0 
\frac{d}{dt} \left[4 (1-u'(t)^2) u'(t) \right] = 0.
 
(1-u'(t)^2) u'(t) = C
 C u'(t)=\mathrm{const} u(t) C^1 \delta","['ordinary-differential-equations', 'optimization', 'boundary-value-problem', 'euler-lagrange-equation']"
19,"Find $\int_0^\infty f(x)dx$ given that $f(0)=1$ and for all $a>0$, the arc length from $0$ to $a$ equals the $x$-intercept of the tangent at $a$.","Find  given that  and for all , the arc length from  to  equals the -intercept of the tangent at .",\int_0^\infty f(x)dx f(0)=1 a>0 0 a x a,"I made up the following problem. A curve $y=f(x)$ passes through $(0,1)$ , and for all $a>0$ , the arc length of $y=f(x)$ from $x=0$ to $x=a$ equals the $x$ -intercept of the tangent to $y=f(x)$ at $x=a$ . That is, in the graph below, the red arc and the red line segment have the same length. What is the area under the curve, $A=\int_0^\infty f(x)dx$ ? My attempt We have $$\int_0^a \sqrt{1+(f'(x))^2}dx=a-\frac{f(a)}{f'(a)}$$ Differentiating both sides with respect to $a$ gives $$\sqrt{1+(f'(a))^2}=1-\frac{(f'(a))^2-f(a)f''(a)}{(f'(a))^2}$$ $$(y')^6+(y')^4-(yy'')^2=0$$ But I don't know how to solve this differential equation. Then I approximated the curve with a sequence of triangles as shown below. Each of the colored line segments has length $\epsilon$ , which approaches $0$ . Let $h_n$ be the height, and let $\theta_n$ be the lower-right angle, of the $n$ th triangle from the left. Let $\theta_0=\pi/2$ . $h_n=1-\epsilon\sum\limits_{k=0}^{n-1} \sin{\theta_k}$ $\theta_n = \arctan{\left(\dfrac{h_n}{n\epsilon -\epsilon\sum\limits_{k=1}^{n-1}\cos{\theta_k}}\right)}$ I made an Excel simulation using these two equations, and it suggests that $A=1/3$ . (The length of the longest side of the triangles seems to approach $1/2$ .) I tried to rearrange the triangles in some clever way to get a total area of $1/3$ , but I haven't found a way.","I made up the following problem. A curve passes through , and for all , the arc length of from to equals the -intercept of the tangent to at . That is, in the graph below, the red arc and the red line segment have the same length. What is the area under the curve, ? My attempt We have Differentiating both sides with respect to gives But I don't know how to solve this differential equation. Then I approximated the curve with a sequence of triangles as shown below. Each of the colored line segments has length , which approaches . Let be the height, and let be the lower-right angle, of the th triangle from the left. Let . I made an Excel simulation using these two equations, and it suggests that . (The length of the longest side of the triangles seems to approach .) I tried to rearrange the triangles in some clever way to get a total area of , but I haven't found a way.","y=f(x) (0,1) a>0 y=f(x) x=0 x=a x y=f(x) x=a A=\int_0^\infty f(x)dx \int_0^a \sqrt{1+(f'(x))^2}dx=a-\frac{f(a)}{f'(a)} a \sqrt{1+(f'(a))^2}=1-\frac{(f'(a))^2-f(a)f''(a)}{(f'(a))^2} (y')^6+(y')^4-(yy'')^2=0 \epsilon 0 h_n \theta_n n \theta_0=\pi/2 h_n=1-\epsilon\sum\limits_{k=0}^{n-1} \sin{\theta_k} \theta_n = \arctan{\left(\dfrac{h_n}{n\epsilon -\epsilon\sum\limits_{k=1}^{n-1}\cos{\theta_k}}\right)} A=1/3 1/2 1/3","['calculus', 'integration', 'ordinary-differential-equations', 'definite-integrals', 'improper-integrals']"
20,Fundamental questions about differential forms:,Fundamental questions about differential forms:,,"I have read through multiple books and watched Dr. Theodore Shifrin's video series . However, the questions that I had when going through much  of the material have remained largely unanswered. They are: (1) Notational meaning: From my understanding, a differential form is a covector that takes as input a vector and returns some number. For instance take the 1-form $\omega = dx + 4dy$ . Is the ""mapping"" to some vector $v$ implicit in the definition? For instance, would we more rigorously be able to write out $\omega$ as $ \omega(v) = dx(v) + 4\cdot dy(v)$ ? Let $v$ = ( $\mathbf{i} + 2 \cdot \mathbf{j}$ ). Can we intuitively put the mapping as: $$  \omega(\mathbf{i} + 2 \cdot \mathbf{j}) = dx(\mathbf{i} + 2 \cdot \mathbf{j}) + 4\cdot dy(\mathbf{i} + 2 \cdot \mathbf{j}) = 1 + 8 \cdot 1 = 9$$ (2) Differential forms in fractions: In calculus we often run into certain terms of  form $\left({dy \over dx}\right)$ . Given the same vector $v$ , can this be written rigorously as: $$ {dy(v) \over dx(v)} = {dy(v) \over dx(v)} = {dy(\mathbf{i} + 2 \cdot \mathbf{j}) \over dx(\mathbf{i} + 2 \cdot \mathbf{j})} = {2 \over 1} = 2$$ (3) Ordinary and partial derivatives: For one dimension, we commonly write $d(A) = {\left( \partial A \over \partial x \right)} dx$ , where $ {\left( \partial A \over \partial x \right)}$ denotes the partial derivative of $A$ with respect to variable $x$ . Let $A$ = $x^2$ . In beginner calculus, we see that ${d A \over dx} = 2x$ . If we take my assumptions from (1) and (2) to be gospel and treat $dx(v)$ as a scalar then we can put: $$ {d A \over dx} = {(dA)(v) \over dx(v)} = {{\left( \partial A \over \partial x \right)} \cdot dx(v)\over dx(v)} = {\left( \partial A \over \partial x \right)} = {\left( \partial (x^2) \over \partial x \right)} = 2x$$ I am confused however. Some answers on pages like this one and this one seem to suggest that $dx$ and $\partial x$ are cosmetic and interchangeable. However, the $dx$ here seems to behave very differently from $\partial x$ , and it almost seems like $\partial x$ acts as some kind of fundamental underpinning operation. Are they completely divorced? Is there some underpinning structure I could know of? It is hard for me to understand this, especially since I struggle to unwrap my head from the way these things are manipulated in beginner calculus. I greatly appreciate any help with this.","I have read through multiple books and watched Dr. Theodore Shifrin's video series . However, the questions that I had when going through much  of the material have remained largely unanswered. They are: (1) Notational meaning: From my understanding, a differential form is a covector that takes as input a vector and returns some number. For instance take the 1-form . Is the ""mapping"" to some vector implicit in the definition? For instance, would we more rigorously be able to write out as ? Let = ( ). Can we intuitively put the mapping as: (2) Differential forms in fractions: In calculus we often run into certain terms of  form . Given the same vector , can this be written rigorously as: (3) Ordinary and partial derivatives: For one dimension, we commonly write , where denotes the partial derivative of with respect to variable . Let = . In beginner calculus, we see that . If we take my assumptions from (1) and (2) to be gospel and treat as a scalar then we can put: I am confused however. Some answers on pages like this one and this one seem to suggest that and are cosmetic and interchangeable. However, the here seems to behave very differently from , and it almost seems like acts as some kind of fundamental underpinning operation. Are they completely divorced? Is there some underpinning structure I could know of? It is hard for me to understand this, especially since I struggle to unwrap my head from the way these things are manipulated in beginner calculus. I greatly appreciate any help with this.",\omega = dx + 4dy v \omega  \omega(v) = dx(v) + 4\cdot dy(v) v \mathbf{i} + 2 \cdot \mathbf{j}   \omega(\mathbf{i} + 2 \cdot \mathbf{j}) = dx(\mathbf{i} + 2 \cdot \mathbf{j}) + 4\cdot dy(\mathbf{i} + 2 \cdot \mathbf{j}) = 1 + 8 \cdot 1 = 9 \left({dy \over dx}\right) v  {dy(v) \over dx(v)} = {dy(v) \over dx(v)} = {dy(\mathbf{i} + 2 \cdot \mathbf{j}) \over dx(\mathbf{i} + 2 \cdot \mathbf{j})} = {2 \over 1} = 2 d(A) = {\left( \partial A \over \partial x \right)} dx  {\left( \partial A \over \partial x \right)} A x A x^2 {d A \over dx} = 2x dx(v)  {d A \over dx} = {(dA)(v) \over dx(v)} = {{\left( \partial A \over \partial x \right)} \cdot dx(v)\over dx(v)} = {\left( \partial A \over \partial x \right)} = {\left( \partial (x^2) \over \partial x \right)} = 2x dx \partial x dx \partial x \partial x,"['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'vector-analysis', 'differential-forms']"
21,Boundedness of a simple differential equation's solution,Boundedness of a simple differential equation's solution,,"I have a simple problem that I encountered when I was trying to prove the boundedness of a function created from a smooth manifold. The problem is simplified to the next general statement: Let $f:(-\infty,\infty)\to (-\infty,\infty)$ be a bounded, $C^{\infty}$ -class function. Suppose $\frac{d^nf}{dt^n}<\infty$ for any $n=1,2,...$ . Then, is the function below bounded from above? $$ g(x)=\int_0^x\left(\frac{df}{dt}\right)^3dt,\quad x\in[0,\infty). $$ If the function is monotonically increasing, the answer is ""yes"" since if we let $|f|<N_0$ and $\left|\frac{df}{dt}\right|<N_1$ , then $$ g(x)\le N_1^2\int_0^x\left|\frac{df}{dt}\right|dt=N_1^2\int_0^x\frac{df}{dt}dt=N_1^2(f(x)-f(0))\le 2N_1^2N_0. $$ I can't figure out if the statement is true when $f$ is not monotonical. I am a student in the field of differential geometry so any advice would be great.","I have a simple problem that I encountered when I was trying to prove the boundedness of a function created from a smooth manifold. The problem is simplified to the next general statement: Let be a bounded, -class function. Suppose for any . Then, is the function below bounded from above? If the function is monotonically increasing, the answer is ""yes"" since if we let and , then I can't figure out if the statement is true when is not monotonical. I am a student in the field of differential geometry so any advice would be great.","f:(-\infty,\infty)\to (-\infty,\infty) C^{\infty} \frac{d^nf}{dt^n}<\infty n=1,2,... 
g(x)=\int_0^x\left(\frac{df}{dt}\right)^3dt,\quad x\in[0,\infty).
 |f|<N_0 \left|\frac{df}{dt}\right|<N_1 
g(x)\le N_1^2\int_0^x\left|\frac{df}{dt}\right|dt=N_1^2\int_0^x\frac{df}{dt}dt=N_1^2(f(x)-f(0))\le 2N_1^2N_0.
 f","['integration', 'ordinary-differential-equations', 'functions', 'derivatives']"
22,Solve the differential equation: $(x^2-y^2)dx+2xydy=0$.,Solve the differential equation: .,(x^2-y^2)dx+2xydy=0,Given $(x^2-y^2)dx+2xydy=0$ My solution- Divide the differential equation by $dx$ $\Rightarrow x^2-y^2+2xy\frac{dy}{dx}=0$ $\Rightarrow 2xy\frac{dy}{dx}=y^2-x^2$ Divide both sides by $2xy$ $\Rightarrow \frac{dy}{dx}=\frac{1}{2}[\frac{y}{x}-\frac{x}{y}]$ This is a homogenous differential equation . Substitute $y=vx$ $\Rightarrow \frac{dy}{dx}=v+x\frac{dv}{dx}$ $\Rightarrow v+x\frac{dv}{dx}=\frac{1}{2}[v-\frac{1}{v}]$ $\Rightarrow x\frac{dv}{dx}=-\frac{v^2+1}{2v}$ $\Rightarrow -\frac{2v}{v^2+1}dv=\frac{dx}{x}$ Integrating both sides $\Rightarrow -\log|v^2+1|=\log x+\log c$ $\Rightarrow -\log|\frac{y^2}{x^2}+1|=\log xc$ $\Rightarrow -\log|\frac{x^2+y^2}{x^2}|=\log xc$ $\Rightarrow \frac{x^2}{x^2+y^2}= xc$ $\Rightarrow x= c(x^2+y^2)$ $\Rightarrow y=\pm \sqrt{xc-x^2}$ Kindly review my solution and let me know if there are other methods of solving such problems.,Given My solution- Divide the differential equation by Divide both sides by This is a homogenous differential equation . Substitute Integrating both sides Kindly review my solution and let me know if there are other methods of solving such problems.,(x^2-y^2)dx+2xydy=0 dx \Rightarrow x^2-y^2+2xy\frac{dy}{dx}=0 \Rightarrow 2xy\frac{dy}{dx}=y^2-x^2 2xy \Rightarrow \frac{dy}{dx}=\frac{1}{2}[\frac{y}{x}-\frac{x}{y}] y=vx \Rightarrow \frac{dy}{dx}=v+x\frac{dv}{dx} \Rightarrow v+x\frac{dv}{dx}=\frac{1}{2}[v-\frac{1}{v}] \Rightarrow x\frac{dv}{dx}=-\frac{v^2+1}{2v} \Rightarrow -\frac{2v}{v^2+1}dv=\frac{dx}{x} \Rightarrow -\log|v^2+1|=\log x+\log c \Rightarrow -\log|\frac{y^2}{x^2}+1|=\log xc \Rightarrow -\log|\frac{x^2+y^2}{x^2}|=\log xc \Rightarrow \frac{x^2}{x^2+y^2}= xc \Rightarrow x= c(x^2+y^2) \Rightarrow y=\pm \sqrt{xc-x^2},"['ordinary-differential-equations', 'solution-verification']"
23,Show that the Maclaurin expansion of exact ODE general solution has the same form as the power series solution,Show that the Maclaurin expansion of exact ODE general solution has the same form as the power series solution,,"So, given the ODE $$ y''+2y'+y=0 $$ I have found a power series solution, coefficient recurrence relation and the general solution in terms of elementary functions: $$ \begin{aligned}  a_{n+2} &= -\frac{2a_{n+1}}{n+2} - \frac{a_n}{(n+1)(n+2)}, \: \: n=0,1,2,... \\ \\ y(x) &= \sum_{n=0}^{\infty}a_nx^n\\ &= a_0(1 - \frac{1}{2}x^2 + \frac{1}{3}x^3 - \frac{1}{8}x^4 +...)+ a_1(x - x^2 + \frac{1}{2}x^3 - \frac{1}{6}x^4 + ...)\\ \\ &= C_1e^{-x} + C_2xe^{-x}\\ \\ &= C_1\sum_{n=0}^{\infty}\frac{(-x)^n}{n!} + C_2x\sum_{n=0}^{\infty}\frac{(-x)^n}{n!} \end{aligned} $$ Now, I want to confirm my power series solution by showing that the Maclaurin expansion of the exact general solution has the same form as the power series, and find relations for $C_1$ and $C_2$ in terms of $a_0$ and $a_1$ . I.e. find A, B, C, D such that $$ \begin{align} C_1 &= Aa_0 + Ba_1\\ C_2 &= Ca_0 + Da_1 \end{align} $$ I'm really struggling with this.. Equating coefficients feels like it's leading me nowhere and I don't know what else to try. Edit: my original ODE power series solution $$ \begin{align} y = \sum_{n = 0}^{\infty}a_nx^n, \:\: y' = \sum_{n = 0}^{\infty}na_nx^{n-1}, \:\: y'' = \sum_{n = 0}^{\infty}(n-1)na_nx^{n-2} \end{align} $$ So the ODE becomes $$ \begin{align} &\sum_{n = 0}^{\infty}(n-1)na_nx^{n-2} + 2\sum_{n = 0}^{\infty}na_nx^{n-1} + \sum_{n = 0}^{\infty}a_nx^n = 0 \\ \implies  & \sum_{n = 2}^{\infty}(n-1)na_nx^{n-2} + 2\sum_{n = 1}^{\infty}na_nx^{n-1} + \sum_{n = 0}^{\infty}a_nx^n = 0\\ \implies & \sum_{i=0}^{\infty}(i+1)(i+2)a_{i+2}x^{i} + 2\sum_{k=0}^{\infty}(k+1)a_{k+1}x^{k} + \sum_{n = 0}^{\infty}a_nx^n = 0\\ \implies & \sum_{n=0}^{\infty}[(n+1)(n+2)a_{n+2}+(n+1)2a_{n+1} + a_n]x^n = 0\\ \end{align} $$ Comparing Coefficients leads to the recurrence relation: $$ \begin{align} (n+1)(n+2)a_{n+2}+(n+1)2a_{n+1} + a_n &= 0\\ \implies a_{n+2} &= -\frac{2a_{n+1}}{n+2} - \frac{a_n}{(n+1)(n+2)}, \: \: n=0,1,2,... \end{align} $$ Which gives the solution: $$ y = a_0(1 - \frac{1}{2}x^2 + \frac{1}{3}x^3 - \frac{1}{8}x^4 +...)+ a_1(x - x^2 + \frac{1}{2}x^3 - \frac{1}{6}x^4 + ...) $$ Apparently this is incorrect but I am really struggling to see a mistake after re-doing this","So, given the ODE I have found a power series solution, coefficient recurrence relation and the general solution in terms of elementary functions: Now, I want to confirm my power series solution by showing that the Maclaurin expansion of the exact general solution has the same form as the power series, and find relations for and in terms of and . I.e. find A, B, C, D such that I'm really struggling with this.. Equating coefficients feels like it's leading me nowhere and I don't know what else to try. Edit: my original ODE power series solution So the ODE becomes Comparing Coefficients leads to the recurrence relation: Which gives the solution: Apparently this is incorrect but I am really struggling to see a mistake after re-doing this","
y''+2y'+y=0
 
\begin{aligned} 
a_{n+2} &= -\frac{2a_{n+1}}{n+2} - \frac{a_n}{(n+1)(n+2)}, \: \: n=0,1,2,... \\
\\
y(x) &= \sum_{n=0}^{\infty}a_nx^n\\
&= a_0(1 - \frac{1}{2}x^2 + \frac{1}{3}x^3 - \frac{1}{8}x^4 +...)+ a_1(x - x^2 + \frac{1}{2}x^3 - \frac{1}{6}x^4 + ...)\\ \\
&= C_1e^{-x} + C_2xe^{-x}\\ \\
&= C_1\sum_{n=0}^{\infty}\frac{(-x)^n}{n!} + C_2x\sum_{n=0}^{\infty}\frac{(-x)^n}{n!}
\end{aligned}
 C_1 C_2 a_0 a_1 
\begin{align}
C_1 &= Aa_0 + Ba_1\\
C_2 &= Ca_0 + Da_1
\end{align}
 
\begin{align}
y = \sum_{n = 0}^{\infty}a_nx^n, \:\: y' = \sum_{n = 0}^{\infty}na_nx^{n-1}, \:\: y'' = \sum_{n = 0}^{\infty}(n-1)na_nx^{n-2}
\end{align}
 
\begin{align}
&\sum_{n = 0}^{\infty}(n-1)na_nx^{n-2} + 2\sum_{n = 0}^{\infty}na_nx^{n-1} + \sum_{n = 0}^{\infty}a_nx^n = 0 \\
\implies 
& \sum_{n = 2}^{\infty}(n-1)na_nx^{n-2} + 2\sum_{n = 1}^{\infty}na_nx^{n-1} + \sum_{n = 0}^{\infty}a_nx^n = 0\\
\implies
& \sum_{i=0}^{\infty}(i+1)(i+2)a_{i+2}x^{i} + 2\sum_{k=0}^{\infty}(k+1)a_{k+1}x^{k} + \sum_{n = 0}^{\infty}a_nx^n = 0\\
\implies
& \sum_{n=0}^{\infty}[(n+1)(n+2)a_{n+2}+(n+1)2a_{n+1} + a_n]x^n = 0\\
\end{align}
 
\begin{align}
(n+1)(n+2)a_{n+2}+(n+1)2a_{n+1} + a_n &= 0\\
\implies
a_{n+2} &= -\frac{2a_{n+1}}{n+2} - \frac{a_n}{(n+1)(n+2)}, \: \: n=0,1,2,...
\end{align}
 
y = a_0(1 - \frac{1}{2}x^2 + \frac{1}{3}x^3 - \frac{1}{8}x^4 +...)+ a_1(x - x^2 + \frac{1}{2}x^3 - \frac{1}{6}x^4 + ...)
","['ordinary-differential-equations', 'recurrence-relations', 'taylor-expansion', 'power-series']"
24,Approximating the solution to a system of 3 oscillatory ODEs?,Approximating the solution to a system of 3 oscillatory ODEs?,,"ODE System I have the following system of ODEs: $x'(t)=x(t)\frac{z(t)}{Z}-x(t)\frac{x(t)+y(t)}{J}$ $y'(t)=y(t)\left(1-\frac{z(t)}{Z}\right)(1-q)-y(t)\frac{x(t)+y(t)}{J}$ $z'(t)=y(t)\left(1-\frac{z(t)}{Z}\right)(1-q)-mz(t)$ , where all variables and parameters are positive and $0<q<1$ . Background The system exhibits an equilibrium, $\left(\overline{x},\overline{y},\overline{z}\right)$ , where $\overline{x}$ , $\overline{y}$ , and $\overline{z}$ are positive when $J$ is greater than a critical value $J_{Crit}$ . The real parts of the eigenvalues $\left(\lambda_1,\lambda_2,\lambda_3\right)$ corresponding to $\left(\overline{x},\overline{y},\overline{z}\right)$ are all negative when $J>J_{Crit}$ . Upon further analysis of the eigenvalues, one finds that $\left(\overline{x},\overline{y},\overline{z}\right)$ always exhibits oscillations (i.e., two of the eigenvalues are complex). The answer to this post showed that the solution to such a 3-dimensional system is well-approximated by $\overline{V} + Ae^{\sigma t}\cos{\left(\omega t+ \phi\right)} + be^{\lambda_3t} \ \forall \ \overline{V} \in \left(\overline{x},\overline{y},\overline{z}\right)$ , where $A$ , $\sigma$ , $\omega$ , and $\phi$ give the oscillations' amplitude, decay rate, frequency, and initial phase, respectively. Both $A$ and $b$ are functions of initial conditions, which are $x(0)$ , $y(0)$ , $z(0)$ . Question In general, how does one calculate $b$ for a 3-dimensional ODE system like the one presented here?","ODE System I have the following system of ODEs: , where all variables and parameters are positive and . Background The system exhibits an equilibrium, , where , , and are positive when is greater than a critical value . The real parts of the eigenvalues corresponding to are all negative when . Upon further analysis of the eigenvalues, one finds that always exhibits oscillations (i.e., two of the eigenvalues are complex). The answer to this post showed that the solution to such a 3-dimensional system is well-approximated by , where , , , and give the oscillations' amplitude, decay rate, frequency, and initial phase, respectively. Both and are functions of initial conditions, which are , , . Question In general, how does one calculate for a 3-dimensional ODE system like the one presented here?","x'(t)=x(t)\frac{z(t)}{Z}-x(t)\frac{x(t)+y(t)}{J} y'(t)=y(t)\left(1-\frac{z(t)}{Z}\right)(1-q)-y(t)\frac{x(t)+y(t)}{J} z'(t)=y(t)\left(1-\frac{z(t)}{Z}\right)(1-q)-mz(t) 0<q<1 \left(\overline{x},\overline{y},\overline{z}\right) \overline{x} \overline{y} \overline{z} J J_{Crit} \left(\lambda_1,\lambda_2,\lambda_3\right) \left(\overline{x},\overline{y},\overline{z}\right) J>J_{Crit} \left(\overline{x},\overline{y},\overline{z}\right) \overline{V} + Ae^{\sigma t}\cos{\left(\omega t+ \phi\right)} + be^{\lambda_3t} \ \forall \ \overline{V} \in \left(\overline{x},\overline{y},\overline{z}\right) A \sigma \omega \phi A b x(0) y(0) z(0) b","['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
25,"Prove that if $y'=f(x,y),y(x_0)=y_0$ is invariant under the transformation $(x,y) \mapsto (-x,-y)$, then $y(-x)=-y(x)$ provided the existence","Prove that if  is invariant under the transformation , then  provided the existence","y'=f(x,y),y(x_0)=y_0 (x,y) \mapsto (-x,-y) y(-x)=-y(x)","How to make a clear justification for the proposition: If $y'=f(x,y),y(x_0)=y_0$ is invariant under the transformation $(x,y) \mapsto  (-x,-y)$ , then $y(-x)=-y(x)$ provided the existence of solution. For instance, $u(x)=-y(-x)$ can also satisfy the ODE $z'(x)=z(x)^2,~z(x_0)=z_0$ , but how it is necessary whenever the ODE is invariant under $(x,y) \mapsto  (-x,-y)$ ?","How to make a clear justification for the proposition: If is invariant under the transformation , then provided the existence of solution. For instance, can also satisfy the ODE , but how it is necessary whenever the ODE is invariant under ?","y'=f(x,y),y(x_0)=y_0 (x,y) \mapsto  (-x,-y) y(-x)=-y(x) u(x)=-y(-x) z'(x)=z(x)^2,~z(x_0)=z_0 (x,y) \mapsto  (-x,-y)",['ordinary-differential-equations']
26,"Behavior for small and large times of $\ddot{y}(t)=y^2(t), y(0)=y_0>0, \dot{y}(0)=0$.",Behavior for small and large times of .,"\ddot{y}(t)=y^2(t), y(0)=y_0>0, \dot{y}(0)=0","I am trying to understand the behavior of solutions for small and large times to the differential equation $$\ddot{y}(t)=3y(t)^2, y(0)=y_0>0, \dot{y}(0)=0.$$ For small times we have that $$\ddot{y}(t)=3y_0^2+6y_0(y-y_0)+O(y-y_0)^2$$ so we expect the solution to behave like a quadratic plus a positive  exponential. I'm not sure if the solution is defined for all times $t\ge 0$ or whether it blows up at some finite time $t$ . We can integrate the equation of motion to get $$\frac{1}{2}\dot{y}(t)^2=y(t)^3-y_0^3,$$ hence, assuming that $\dot{y}(t)\ge 0$ for all $t\ge 0$ , $$\int_{y_0}^{y(t)}\frac{dy}{\sqrt{y^3-y_0^3}}=\sqrt{2}t$$ though I'm not sure how to extract information out of this integral, which is not easily integrated. I thought about using the substitution $y(t)=y_0 e^{S(t)}$ , $S(0)=0$ to obtain $$\ddot{S}(t)+\dot{S}(t)^2=3y_0 e^{S(t)}$$ If we assume that the $\dot{S}^2$ dominates over $\ddot{S}$ , we end up with $$\dot{S}=\sqrt{3y_0}e^{S(t)},$$ i.e. $$1-e^{-S(t)}=\sqrt{3y_0}t. $$ Thus, $$S(t)=\log\left(\frac{1}{1-\sqrt{3y_0}t}\right)$$ giving us blow up at finite time, however $\ddot{S}\sim \dot{S}^2%$ , violating our assumption so this approximation doesn't work. I would appreciate any ideas. If we have that $y_0>1$ , then $\ddot{y}(t)\ge 3y(t)$ and thus we should have $y(t)\ge y_0\cosh(\sqrt{3} t)$ and so we have super-exponential growth.","I am trying to understand the behavior of solutions for small and large times to the differential equation For small times we have that so we expect the solution to behave like a quadratic plus a positive  exponential. I'm not sure if the solution is defined for all times or whether it blows up at some finite time . We can integrate the equation of motion to get hence, assuming that for all , though I'm not sure how to extract information out of this integral, which is not easily integrated. I thought about using the substitution , to obtain If we assume that the dominates over , we end up with i.e. Thus, giving us blow up at finite time, however , violating our assumption so this approximation doesn't work. I would appreciate any ideas. If we have that , then and thus we should have and so we have super-exponential growth.","\ddot{y}(t)=3y(t)^2, y(0)=y_0>0, \dot{y}(0)=0. \ddot{y}(t)=3y_0^2+6y_0(y-y_0)+O(y-y_0)^2 t\ge 0 t \frac{1}{2}\dot{y}(t)^2=y(t)^3-y_0^3, \dot{y}(t)\ge 0 t\ge 0 \int_{y_0}^{y(t)}\frac{dy}{\sqrt{y^3-y_0^3}}=\sqrt{2}t y(t)=y_0 e^{S(t)} S(0)=0 \ddot{S}(t)+\dot{S}(t)^2=3y_0 e^{S(t)} \dot{S}^2 \ddot{S} \dot{S}=\sqrt{3y_0}e^{S(t)}, 1-e^{-S(t)}=\sqrt{3y_0}t.  S(t)=\log\left(\frac{1}{1-\sqrt{3y_0}t}\right) \ddot{S}\sim \dot{S}^2% y_0>1 \ddot{y}(t)\ge 3y(t) y(t)\ge y_0\cosh(\sqrt{3} t)","['ordinary-differential-equations', 'asymptotics']"
27,Euler equation of a function in a function.,Euler equation of a function in a function.,,"Let's assume you have some function $F = e^ab(c(t))$ , where $a \in \mathbb{R}$ and $b(c(t))$ is a function, also $c(t)$ is a function. To give the Euler equation of $c(t)$ we have to use: $$\frac{\partial F}{\partial c} - \frac{dF}{dt}(\frac{\partial F}{\partial \dot{c}}) = 0$$ The first element is equal to: $$ \frac{\partial F}{\partial c} = e^a\dot{b}(c(t))\dot{c}(t)$$ Now the question is what is the derivative of F w.r.t. $\dot{c}$ , I think it is $0$ can anyone confirm this?","Let's assume you have some function , where and is a function, also is a function. To give the Euler equation of we have to use: The first element is equal to: Now the question is what is the derivative of F w.r.t. , I think it is can anyone confirm this?",F = e^ab(c(t)) a \in \mathbb{R} b(c(t)) c(t) c(t) \frac{\partial F}{\partial c} - \frac{dF}{dt}(\frac{\partial F}{\partial \dot{c}}) = 0  \frac{\partial F}{\partial c} = e^a\dot{b}(c(t))\dot{c}(t) \dot{c} 0,"['ordinary-differential-equations', 'derivatives', 'solution-verification', 'calculus-of-variations']"
28,How to solve $x^2y''+xy'+x^2ky = 0$,How to solve,x^2y''+xy'+x^2ky = 0,"I am working on a $2D$ steady state heat equation (Laplacian). I did Separation of Variables and am evaluating $3$ cases ( $k>0, k<0, k=0$ ). For the last scenario, I am not sure how to solve this ODE that developed after Separation of Variables. It is almost a Bessel equation of order zero, but not quite because of that pesky $k$ . Could anyone give me a hint about how to solve this ODE? $$r^2R''+rR'+r^2kR = 0$$ Edit to add some additional information: I started with: $${∂^2T\over∂r^2} +{1\over r}{∂T\over∂r} + {∂^2T\over∂z^2}=0$$ Tried Separation of Variables as follows: $T(r,z) = R(r)Z(z)$ Which resulted in these two ODEs: $${1\over R}{∂^2T\over∂r^2} +{1\over r}{∂T\over∂r} = k$$ $$-{1\over Z}{∂^2Z\over∂z^2} = k$$ The problem is a cylinder with height 1 and radius 1, where the temperature is 1 on the top, and 0 on the curved wall and the bottom. Thus $T(r,0)=0$ , $ T(r,1)=1$ , $T(1,z)=0$ . I am struggling to understand which case is appropriate for this problem. I believe $k=0$ results in a singularity at $r=0$ due to a natural logarithm term, so that case does not seem correct.","I am working on a steady state heat equation (Laplacian). I did Separation of Variables and am evaluating cases ( ). For the last scenario, I am not sure how to solve this ODE that developed after Separation of Variables. It is almost a Bessel equation of order zero, but not quite because of that pesky . Could anyone give me a hint about how to solve this ODE? Edit to add some additional information: I started with: Tried Separation of Variables as follows: Which resulted in these two ODEs: The problem is a cylinder with height 1 and radius 1, where the temperature is 1 on the top, and 0 on the curved wall and the bottom. Thus , , . I am struggling to understand which case is appropriate for this problem. I believe results in a singularity at due to a natural logarithm term, so that case does not seem correct.","2D 3 k>0, k<0, k=0 k r^2R''+rR'+r^2kR = 0 {∂^2T\over∂r^2} +{1\over r}{∂T\over∂r} + {∂^2T\over∂z^2}=0 T(r,z) = R(r)Z(z) {1\over R}{∂^2T\over∂r^2} +{1\over r}{∂T\over∂r} = k -{1\over Z}{∂^2Z\over∂z^2} = k T(r,0)=0  T(r,1)=1 T(1,z)=0 k=0 r=0",['ordinary-differential-equations']
29,Proof of Bonnet's Recursion Formula for Legendre Functions of the Second Kind?,Proof of Bonnet's Recursion Formula for Legendre Functions of the Second Kind?,,"I'm doing some self-study on Legendre's Equation.  I have seen and understand the proof of Bonnet's Recursion Formula for the Legendre Polynomials, $P_n(x)$ . $$(n+1)P_{n+1}(x) = (1+2n)xP_n(x) - nP_{n-1}(x)$$ Additionally, I have read that this formula also applies to Legendre functions of the second kind $Q_n(x)$ , however I have not found any source that does more than simply state this fact.  My question is: how does one prove that Bonnet's Recursion Formula extends to Legendre functions of the second kind? For context, the proof I am familiar with for Bonnet's Recursion Formula relies on the generating function for the Legendre Polynomials $$ \frac{1}{\sqrt{1-2xt+t^2}} = \sum_{k=0}^{\infty} P_n(x){t^n} $$ where you differentiate with respect to $t$ and then collect coefficients of ${t^n}$ .","I'm doing some self-study on Legendre's Equation.  I have seen and understand the proof of Bonnet's Recursion Formula for the Legendre Polynomials, . Additionally, I have read that this formula also applies to Legendre functions of the second kind , however I have not found any source that does more than simply state this fact.  My question is: how does one prove that Bonnet's Recursion Formula extends to Legendre functions of the second kind? For context, the proof I am familiar with for Bonnet's Recursion Formula relies on the generating function for the Legendre Polynomials where you differentiate with respect to and then collect coefficients of .",P_n(x) (n+1)P_{n+1}(x) = (1+2n)xP_n(x) - nP_{n-1}(x) Q_n(x)  \frac{1}{\sqrt{1-2xt+t^2}} = \sum_{k=0}^{\infty} P_n(x){t^n}  t {t^n},"['ordinary-differential-equations', 'special-functions', 'legendre-polynomials', 'legendre-functions']"
30,"Variable-step Runge-Kutta methods, Fehlberg vs Dormand-Prince: why is the order reversed?","Variable-step Runge-Kutta methods, Fehlberg vs Dormand-Prince: why is the order reversed?",,"Dormand-Prince and Fehlberg are two popular Runge-Kutta embedded methods for ODE integration with adaptive stepsize. The former one estimates the error with the lower-order method and steps forward with the higher-order method; in the latter one the two roles are reversed. I do not understand the reason for this. Why does Fehlberg method step forward with the lower-order method, instead of stepping forward with the higher-order method which is more accurate and whose result is readily available? Whatever is the problem that I am not seeing: I expect that Dormand-Prince method is not affected by that, since it does indeed step forward with the higher-order method","Dormand-Prince and Fehlberg are two popular Runge-Kutta embedded methods for ODE integration with adaptive stepsize. The former one estimates the error with the lower-order method and steps forward with the higher-order method; in the latter one the two roles are reversed. I do not understand the reason for this. Why does Fehlberg method step forward with the lower-order method, instead of stepping forward with the higher-order method which is more accurate and whose result is readily available? Whatever is the problem that I am not seeing: I expect that Dormand-Prince method is not affected by that, since it does indeed step forward with the higher-order method",,"['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
31,"Finding a counterexample of approximating the solution to $x'=f(x,t)$ by $x_{n+1}'=f(x_n,t)$",Finding a counterexample of approximating the solution to  by,"x'=f(x,t) x_{n+1}'=f(x_n,t)","Suppose we have a Cauchy problem $$x'=f(x(t),t),\quad x(t_0)=C,$$ where $f$ is Lipschitz continuous in its first argument and continuous in its second argument. By Picard theorem, there must exist a unique solution within some interval $t\in[t_0-\varepsilon,t_0+\varepsilon]$ . Now, on this interval, construct a sequence of functions $(x_n)$ by recursive formula $$x_0(t):=C,\quad x_{n+1}(t):=\int_{t_0}^tf(x_n(s),s)\,\mathrm ds+C.$$ It is clear that $x_{n+1}'(t)=f(x_n(t),t)$ , and if we take the limit $n\to\infty$ , we can see $\lim_{n\to\infty}x_n$ is the solution to the original ODE if the sequence $(x_n')$ converges uniformly on the interval we consider. Because of the uniform convergence condition, we can potentially get a wrong solution by this procedure. Is it guaranteed that $\lim x_n$ exists? Can we construct $f$ and $C$ such that $(x_n')$ fails to converge uniformly, and $\lim x_n$ exists but is a wrong solution? If we cannot, then is it guaranteed that $\lim x_n$ is a solution as long as it exists? Can we find the sufficient and necessary condition for $\lim x_n$ to exist and be the solution?","Suppose we have a Cauchy problem where is Lipschitz continuous in its first argument and continuous in its second argument. By Picard theorem, there must exist a unique solution within some interval . Now, on this interval, construct a sequence of functions by recursive formula It is clear that , and if we take the limit , we can see is the solution to the original ODE if the sequence converges uniformly on the interval we consider. Because of the uniform convergence condition, we can potentially get a wrong solution by this procedure. Is it guaranteed that exists? Can we construct and such that fails to converge uniformly, and exists but is a wrong solution? If we cannot, then is it guaranteed that is a solution as long as it exists? Can we find the sufficient and necessary condition for to exist and be the solution?","x'=f(x(t),t),\quad x(t_0)=C, f t\in[t_0-\varepsilon,t_0+\varepsilon] (x_n) x_0(t):=C,\quad x_{n+1}(t):=\int_{t_0}^tf(x_n(s),s)\,\mathrm ds+C. x_{n+1}'(t)=f(x_n(t),t) n\to\infty \lim_{n\to\infty}x_n (x_n') \lim x_n f C (x_n') \lim x_n \lim x_n \lim x_n","['real-analysis', 'ordinary-differential-equations', 'perturbation-theory']"
32,How do I solve this comparing of coefficient in this differential equation?,How do I solve this comparing of coefficient in this differential equation?,,"Solve $y'' + 2y' = \cos \pi x$ Homogeneous equation: $y= C_1 + C_2e^{-2x}$ Particular solution: $y_p = A \cos \pi x + B\sin \pi x$ $y'=-A \pi \sin \pi x + B\pi \cos \pi x$ $y'' = -A \pi^2 \cos \pi x - B\pi^2 \sin \pi x$ Substituting back into the equation, $-A \pi^2 \cos \pi x - B\pi^2\sin \pi x + 2(-A\pi\sin \pi x + B\pi \cos \pi x) = \cos \pi x$ $\cos \pi x (-A\pi^2 + 2B\pi) - \sin\pi x(B\pi^2 + 2A\pi)= \cos \pi x$ So, $ (-A\pi^2 + 2B\pi) =1 $ $(-B\pi^2 - 2A\pi) =0$ How do I solve this to get $y_p = \frac{1}{(4+\pi^2)\pi} (-\pi \cos \pi x + 2 \sin \pi x)$","Solve Homogeneous equation: Particular solution: Substituting back into the equation, So, How do I solve this to get",y'' + 2y' = \cos \pi x y= C_1 + C_2e^{-2x} y_p = A \cos \pi x + B\sin \pi x y'=-A \pi \sin \pi x + B\pi \cos \pi x y'' = -A \pi^2 \cos \pi x - B\pi^2 \sin \pi x -A \pi^2 \cos \pi x - B\pi^2\sin \pi x + 2(-A\pi\sin \pi x + B\pi \cos \pi x) = \cos \pi x \cos \pi x (-A\pi^2 + 2B\pi) - \sin\pi x(B\pi^2 + 2A\pi)= \cos \pi x  (-A\pi^2 + 2B\pi) =1  (-B\pi^2 - 2A\pi) =0 y_p = \frac{1}{(4+\pi^2)\pi} (-\pi \cos \pi x + 2 \sin \pi x),['ordinary-differential-equations']
33,Salt concentration differential equation,Salt concentration differential equation,,"There is a 100 liter container full with a 10 kg salt solution. There is a 10% salt solution going into the container with 5liter/min speed, which dissolves instantly with the solution in the container. The solution exits the container at the bottom with the same speed. How much salt will there be in the container after 2 hours? My solution goes like: Let $x(t)$ be the salts amount in the tank after t time $x(0) = 10 (kg)$ $t \Rightarrow t+Δt$ $x(t+Δt)-x(t) =$ ""The amount that flows in - The amount that flows out"" Flows in: 5l/min of 10% salt solution $\Rightarrow$ 0.5 kg of salt/min Flows out: $5l/min -> \frac{5}{100}$ x( t ) = $\frac{x(t)}{20}$ in $Δt$ minutes $Δt$ $\times$ $5l$ $\Rightarrow  \frac{Δt \times x(t)}{20}$ So the whole equation looks like this: ${x(t+Δt)-x(t)} = 0.5-\frac{Δt \times x(t)}{20}$ My question really is if this thought process for solving this is good, or if not where have I made a mistake? Also sorry for the terrible translation of the exercise.","There is a 100 liter container full with a 10 kg salt solution. There is a 10% salt solution going into the container with 5liter/min speed, which dissolves instantly with the solution in the container. The solution exits the container at the bottom with the same speed. How much salt will there be in the container after 2 hours? My solution goes like: Let be the salts amount in the tank after t time ""The amount that flows in - The amount that flows out"" Flows in: 5l/min of 10% salt solution 0.5 kg of salt/min Flows out: x( t ) = in minutes So the whole equation looks like this: My question really is if this thought process for solving this is good, or if not where have I made a mistake? Also sorry for the terrible translation of the exercise.",x(t) x(0) = 10 (kg) t \Rightarrow t+Δt x(t+Δt)-x(t) = \Rightarrow 5l/min -> \frac{5}{100} \frac{x(t)}{20} Δt Δt \times 5l \Rightarrow  \frac{Δt \times x(t)}{20} {x(t+Δt)-x(t)} = 0.5-\frac{Δt \times x(t)}{20},['ordinary-differential-equations']
34,What is the defininition of phase Space in simple terms?,What is the defininition of phase Space in simple terms?,,"I’m currently studying a Differential Equations Course, and I’m focusing on a chapter called “Systems of ODE”, the text gives a definition of phase space, but I can’t seem to visualise or understand what it is graphically. Could someone break it down? Here is the definition of phase space I’ve been given: For an autonomous $N$ -th order ODE system $dx/dt = f(x)$ , $x$ , $f \in \mathbb R^N$ , the phase space is $\text{dom }(f) \subset \mathbb R^N$ , that is, the sub-set of the $N$ -dimensional space, where the right-hand sides of the system are defined. Thank you","I’m currently studying a Differential Equations Course, and I’m focusing on a chapter called “Systems of ODE”, the text gives a definition of phase space, but I can’t seem to visualise or understand what it is graphically. Could someone break it down? Here is the definition of phase space I’ve been given: For an autonomous -th order ODE system , , , the phase space is , that is, the sub-set of the -dimensional space, where the right-hand sides of the system are defined. Thank you",N dx/dt = f(x) x f \in \mathbb R^N \text{dom }(f) \subset \mathbb R^N N,"['ordinary-differential-equations', 'differential-geometry', 'systems-of-equations']"
35,Solve $y''-y'-2y=0$ using the power series,Solve  using the power series,y''-y'-2y=0,"First of all, by solving this the normal way, the answer should be: $y=Ae^{2x}+Be^{-x}$ What I have so far: $y=\sum_{n=0}^{\infty }a_nx^n$ $y'=\sum_{n=1}^{\infty }na_nx^{n-1}$ $y''=\sum_{n=2}^{\infty }(n-1)(n)a_nx^{n-2}$ Shifting the counter to make it all $\sum_{n=0}^{\infty }$ $y'=\sum_{n=0}^{\infty }(n+1)a_{n+1}x^{n}$ $y''=\sum_{n=0}^{\infty }(n+1)(n+2)a_{n+2}x^{n}$ Therefore, I get: $(n+1)(n+2)a_{n+2}-(n+1)a_{n+1}-2a_n=0$ From this, my recursion formula is: $a_{n+2}=\frac{2a_n+(n+1)a_{n+1}}{(n+1)(n+2)}$ I then evaluated this from $n=0$ to $n=6$ but my $a_2$ to $a_8$ values seem to be all over the place. $a_2 = \frac{2a_0+a_{1}}{2!}$ $a_3 = \frac{2a_0+3a_{1}}{3!}$ $a_4 = \frac{6a_0+5a_{1}}{4!}$ $a_5 = \frac{10a_0+11a_{1}}{5!}$ $a_6 = \frac{22a_0+21a_{1}}{6!}$ $a_7 = \frac{42a_0+43a_{1}}{7!}$ $a_8 = \frac{86a_0+85a_{1}}{8!}$ I researched a bit and found out that the coefficients of $a_1$ follow the Jacobsthal sequence while the coefficients of $a_0$ are twice the Jacobsthal sequence but I think this is wrong as it is nowhere near the power series expansion of $y=Ae^{2x}+Be^{-x}$","First of all, by solving this the normal way, the answer should be: What I have so far: Shifting the counter to make it all Therefore, I get: From this, my recursion formula is: I then evaluated this from to but my to values seem to be all over the place. I researched a bit and found out that the coefficients of follow the Jacobsthal sequence while the coefficients of are twice the Jacobsthal sequence but I think this is wrong as it is nowhere near the power series expansion of",y=Ae^{2x}+Be^{-x} y=\sum_{n=0}^{\infty }a_nx^n y'=\sum_{n=1}^{\infty }na_nx^{n-1} y''=\sum_{n=2}^{\infty }(n-1)(n)a_nx^{n-2} \sum_{n=0}^{\infty } y'=\sum_{n=0}^{\infty }(n+1)a_{n+1}x^{n} y''=\sum_{n=0}^{\infty }(n+1)(n+2)a_{n+2}x^{n} (n+1)(n+2)a_{n+2}-(n+1)a_{n+1}-2a_n=0 a_{n+2}=\frac{2a_n+(n+1)a_{n+1}}{(n+1)(n+2)} n=0 n=6 a_2 a_8 a_2 = \frac{2a_0+a_{1}}{2!} a_3 = \frac{2a_0+3a_{1}}{3!} a_4 = \frac{6a_0+5a_{1}}{4!} a_5 = \frac{10a_0+11a_{1}}{5!} a_6 = \frac{22a_0+21a_{1}}{6!} a_7 = \frac{42a_0+43a_{1}}{7!} a_8 = \frac{86a_0+85a_{1}}{8!} a_1 a_0 y=Ae^{2x}+Be^{-x},"['ordinary-differential-equations', 'power-series']"
36,If $u(x)-\sin(x)=2\int_{0}^x \cos(x-t)u(t)dt$ then $u(x)$ equals,If  then  equals,u(x)-\sin(x)=2\int_{0}^x \cos(x-t)u(t)dt u(x),"If $u(x)-\sin(x)=2\int_{0}^x \cos(x-t)u(t)dt$ then $u(x)$ equals $(1)\frac{e^x}{x}$ $(2)\frac{x}{e^x}$ $(3)xe^x$ $(4)\frac{1}{xe^x}$ we have, $$u(x)-\sin(x)=2\int_{0}^x \cos(x-t)u(t)dt\tag{A}$$ Using Leibnitz's rule of differentiation under integral sign,we get $$u'(x)-\cos(x)=2[-\int_{0}^x \sin(x-t)u(t)dt+u(x)]$$ $$\implies u'(x)-u(x)-\cos(x)=-2\int_{0}^x \sin(x-t)u(t)dt$$ Using Leibnitz's rule of differentiation under integral sign,we get $$u''(x)-u'(x)+\sin(x)=-2\int_{0}^x \cos(x-t)u(t)dt\tag{B}$$ Using equations $(A)$ and $(B)$ ,we get $$u''(x)-u'(x)+\sin(x)=\sin(x)-u(x)$$ $$u''(x)-u'(x)+u(x)=0\tag{C}$$ Auxiliary equation for $(C)$ is $m^2-m+1=0\implies m=\frac{1\pm \iota\sqrt 3}{2}$ So, $u(x)=c_1\cos(\frac{\sqrt 3}{2})x+c_2\sin(\frac{\sqrt 3}{2})$ ,where $c_1,c_2$ are arbitrary constants. which is not in any of the option,what is wrong with my solution?","If then equals we have, Using Leibnitz's rule of differentiation under integral sign,we get Using Leibnitz's rule of differentiation under integral sign,we get Using equations and ,we get Auxiliary equation for is So, ,where are arbitrary constants. which is not in any of the option,what is wrong with my solution?","u(x)-\sin(x)=2\int_{0}^x \cos(x-t)u(t)dt u(x) (1)\frac{e^x}{x} (2)\frac{x}{e^x} (3)xe^x (4)\frac{1}{xe^x} u(x)-\sin(x)=2\int_{0}^x \cos(x-t)u(t)dt\tag{A} u'(x)-\cos(x)=2[-\int_{0}^x \sin(x-t)u(t)dt+u(x)] \implies u'(x)-u(x)-\cos(x)=-2\int_{0}^x \sin(x-t)u(t)dt u''(x)-u'(x)+\sin(x)=-2\int_{0}^x \cos(x-t)u(t)dt\tag{B} (A) (B) u''(x)-u'(x)+\sin(x)=\sin(x)-u(x) u''(x)-u'(x)+u(x)=0\tag{C} (C) m^2-m+1=0\implies m=\frac{1\pm \iota\sqrt 3}{2} u(x)=c_1\cos(\frac{\sqrt 3}{2})x+c_2\sin(\frac{\sqrt 3}{2}) c_1,c_2","['ordinary-differential-equations', 'trigonometry', 'integral-equations']"
37,How to analytically solve a Second Order ODE? [closed],How to analytically solve a Second Order ODE? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I got the following ODE in my work. I haven't seen ODEs carrying trigonometric functions this way. See if anyone could help on solving it analytically. Here is the ODE $y''(x)+\sqrt{2} c \tanh \left(\dfrac{c x}{3 \sqrt{2}}\right) y'(x)=0$ where $c>0.$ The boundary conditions could be $y(\infty)=0, y(0)=1.$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I got the following ODE in my work. I haven't seen ODEs carrying trigonometric functions this way. See if anyone could help on solving it analytically. Here is the ODE where The boundary conditions could be","y''(x)+\sqrt{2} c \tanh \left(\dfrac{c x}{3 \sqrt{2}}\right) y'(x)=0 c>0. y(\infty)=0, y(0)=1.",['ordinary-differential-equations']
38,Does the Wronskian of three or more linearly independent functions change its sign?,Does the Wronskian of three or more linearly independent functions change its sign?,,"If $y_1, y_2$ are two Linearly independent solutions of a differential equation of order $2$ then we know that if the Wronskian is not zero then it never changes its sign (using the Abel's identity ). How far is this true for a third order linear differential equation having $y_1, y_2, y_3$ as linearly independent solutions? I tried finding a solution but i dont think i have an answer. How far is this true in a general case that if $W(y_1,\dots,y_n)$ if non zero then it never changes its sign?",If are two Linearly independent solutions of a differential equation of order then we know that if the Wronskian is not zero then it never changes its sign (using the Abel's identity ). How far is this true for a third order linear differential equation having as linearly independent solutions? I tried finding a solution but i dont think i have an answer. How far is this true in a general case that if if non zero then it never changes its sign?,"y_1, y_2 2 y_1, y_2, y_3 W(y_1,\dots,y_n)","['real-analysis', 'ordinary-differential-equations', 'analysis', 'determinant', 'wronskian']"
39,Proof for positivity of solutions of an ODE system,Proof for positivity of solutions of an ODE system,,"I have an ODE system  that takes a mathematical model describing the dynamics between HCV and the immune system. My question is about the proof that the solution of the ODE system is positive if the initial conditions are all positive. I tried this proof: Consider the DE $\dot{x}=f(x).$ The vector function $f$ is said to be essentially nonnegative if for all $i=1,\ldots,n, f_i(X)\geq 0,$ where $X\in\mathbb{R}^n\geq 0$ such that $X_i=0,$ where $X_i$ denotes the $i$ -th element of $X.$ So for my system: \begin{align} \dot{x}&= \lambda − dx − \beta vx \\ \dot{y}&= \beta vx − ay − pyz \\ \dot{v}&= ky − uv − qvw \\ \dot{w}&= gvw − hw \\ \dot{z}&= cyz − bz \end{align} I have an issue with proving that, the r.h.s. of $\dot{y}$ for $y=0$ is $\beta vx,$ is non-negative for all $v.$","I have an ODE system  that takes a mathematical model describing the dynamics between HCV and the immune system. My question is about the proof that the solution of the ODE system is positive if the initial conditions are all positive. I tried this proof: Consider the DE The vector function is said to be essentially nonnegative if for all where such that where denotes the -th element of So for my system: I have an issue with proving that, the r.h.s. of for is is non-negative for all","\dot{x}=f(x). f i=1,\ldots,n, f_i(X)\geq 0, X\in\mathbb{R}^n\geq 0 X_i=0, X_i i X. \begin{align}
\dot{x}&= \lambda − dx − \beta vx \\
\dot{y}&= \beta vx − ay − pyz \\
\dot{v}&= ky − uv − qvw \\
\dot{w}&= gvw − hw \\
\dot{z}&= cyz − bz
\end{align} \dot{y} y=0 \beta vx, v.","['ordinary-differential-equations', 'mathematical-modeling', 'population-dynamics', 'epidemiology']"
40,Is the Comparison Lemma valid for Asymptotic solutions of the differential inequality?,Is the Comparison Lemma valid for Asymptotic solutions of the differential inequality?,,"The comparison lemma (from Khalil's Nonlinear Systems book) is stated as follows: Lemma 3.4 Consider the scalar differential equation $$\dot{u} = f(t,u), \quad u(t_0) = u_0 $$ where $f(t,u)$ is continuous in $t$ and locally Lipschitz in $u$ for all $t \ge0$ . Let $[t_0,T)$ be the maximimal interval of existence for the solution $u(t)$ . Let $v(t)$ be a continuous function that satisfies the differential inequality $$\dot{v}(t) \le f(t,v(t)), \quad v(t_0)\le u_0, \quad \forall t\in [t_0,T) $$ Then, $$ v(t) \le u(t), \quad \forall t\in [t_0,T).$$ Question I have an inequality such as this, but $f(t,u)$ is nontrivial to solve analytically, but I believe I can obtain an asymptotic solution, say $$ u(t) \sim \tilde{u}(t),\quad  t \rightarrow \infty.$$ For, say, $T = \infty$ , are there any existing theorems such that it possible to conclude that $$ v(t) \le \tilde{u}(t), \quad \forall t\in [T_0,\infty),$$ for some $T_0$ sufficiently large?","The comparison lemma (from Khalil's Nonlinear Systems book) is stated as follows: Lemma 3.4 Consider the scalar differential equation where is continuous in and locally Lipschitz in for all . Let be the maximimal interval of existence for the solution . Let be a continuous function that satisfies the differential inequality Then, Question I have an inequality such as this, but is nontrivial to solve analytically, but I believe I can obtain an asymptotic solution, say For, say, , are there any existing theorems such that it possible to conclude that for some sufficiently large?","\dot{u} = f(t,u), \quad u(t_0) = u_0  f(t,u) t u t \ge0 [t_0,T) u(t) v(t) \dot{v}(t) \le f(t,v(t)), \quad v(t_0)\le u_0, \quad \forall t\in [t_0,T)   v(t) \le u(t), \quad \forall t\in [t_0,T). f(t,u)  u(t) \sim \tilde{u}(t),\quad  t \rightarrow \infty. T = \infty  v(t) \le \tilde{u}(t), \quad \forall t\in [T_0,\infty), T_0","['ordinary-differential-equations', 'asymptotics', 'stability-in-odes']"
41,Solving $ y' = x+y $ with Euler's method,Solving  with Euler's method, y' = x+y ,"I was going over Euler's method for solving DE and I had an idea: Could we use it to get an exact solution to a DE by considering an infinitesimal step size? This is the main idea:  if the approximations get better the smaller the step size, what if we took the limit as the step size $ h $ go to zero (consider an infinitesimal step size), in theory that should make the approximations exact. This is how I implemented that idea: Given a first order linear ODE $ dy/dx=F(x,y) $ with initial conditions $ (x_0, y_0) $ and step size $ h $ . Euler's Method: $ x_{n+1} = x_n + h , \ \ \ \    y_{n+1} = y_n +F(x_n, y_n)h  $ Consider the DE $ dy/dx=x+y, \ \ \ y(0)=1 $ and consider an arbitrary step size $ h $ . To approximate the value of the function at an arbitrary $ x $ value we would need the following iterations $ x_1=x_o+h $ $ x_2 = x_1+h  = (x_o+h)+h = x_o+2h $ $ x_3 = x_2+h = (x_o+2h)+h = x_o+3h $ $ \vdots $ $ x_n = x $ (Iterate until we reach our desired $ x $ value). If we take a look at the pattern that emerges for each successive iteration, we can see that $ x_n = x_0+nh $ . Solving the equation above for $ n $ we get.. $ x_n=x $ $ x_0+nh=x \implies n = \cfrac{x-x_0}{h} $ Here, $ n $ is the number of iterations needed to reach a desired $ x $ value with an initial condition $ x_0 $ and step size $ h $ . Here we can see why having a step size that is infinitesimally small would result in an infinite number of iterations (as $ h \to 0 $ , $ n \to \infty $ ). Now let us consider the iterations for $ y $ . $ y_{n+1}=y_n+F(x_n,y_n)h $ $ y_{n+1}=y_n+(x_n+y_n)h $ (Plugging in our $ x_n $ from above, we get...) $ y_{n+1}=y_n+((x_0+nh)+y_n)h $ We can now solve this recurrence relation with initial conditions $ y(x_0)=y_0 $ $ y(n+1)=y(n)+(x_0+nh+y(n))h, \ \ \ \ y(x_0)=y_0 $ $ y(n)=(x_0h + x_0 + y_0 + 1)(h+1)^{n-x_0}-x_0 -hn -1 $ (Plugging in our initial conditions $ (x_0, y_0) = (0, 1) $ , we get...) $ y(n)=2(h+1)^n -hn-1 $ This is a function in terms of $ n $ and our solution has to be in terms of $ x $ . At this step we plug in our $ n $ from above and rewrite this in terms of $ x $ . $ n = \cfrac{x-x_0}{h} = \cfrac{x}{h} $ $ y(x) \approx 2(h+1)^{x/h}-h \left( \cfrac{x}{h}\right)-1 $ $ y(x) \approx 2(h+1)^{x/h}-x-1 $ And finally we take the limit as the step size $ h $ go to zero. $ \displaystyle y(x) = \lim_{h\to 0} \left[ 2(h+1)^{x/h}-x-1 \right] $ $ \displaystyle y(x)=\lim_{h\to 0}\left[ 2(h+1)^{x/h} \right]-x-1 $ $ \displaystyle y(x)=2\lim_{h\to 0}\left[(h+1)^{x/h} \right]-x-1 $ $ y(x) = 2e^x-x-1 $ It can be verified that $ \displaystyle \lim_{h\to 0}(h+1)^{x/h}=e^x $ And there we have it, we have just solved $ dy/dx = x+y, \ \ y(0)=1 $ exactly using Euler's Method by considering an infinitesimally small step size $ h $ . Has this been done before? Or does this method have a special name? I tried researching 'Euler's method with an infinitesimal step size' but I found nothing. (I've tried it on a few other ODE and came to an exact solution.)","I was going over Euler's method for solving DE and I had an idea: Could we use it to get an exact solution to a DE by considering an infinitesimal step size? This is the main idea:  if the approximations get better the smaller the step size, what if we took the limit as the step size go to zero (consider an infinitesimal step size), in theory that should make the approximations exact. This is how I implemented that idea: Given a first order linear ODE with initial conditions and step size . Euler's Method: Consider the DE and consider an arbitrary step size . To approximate the value of the function at an arbitrary value we would need the following iterations (Iterate until we reach our desired value). If we take a look at the pattern that emerges for each successive iteration, we can see that . Solving the equation above for we get.. Here, is the number of iterations needed to reach a desired value with an initial condition and step size . Here we can see why having a step size that is infinitesimally small would result in an infinite number of iterations (as , ). Now let us consider the iterations for . (Plugging in our from above, we get...) We can now solve this recurrence relation with initial conditions (Plugging in our initial conditions , we get...) This is a function in terms of and our solution has to be in terms of . At this step we plug in our from above and rewrite this in terms of . And finally we take the limit as the step size go to zero. It can be verified that And there we have it, we have just solved exactly using Euler's Method by considering an infinitesimally small step size . Has this been done before? Or does this method have a special name? I tried researching 'Euler's method with an infinitesimal step size' but I found nothing. (I've tried it on a few other ODE and came to an exact solution.)"," h   dy/dx=F(x,y)   (x_0, y_0)   h   x_{n+1} = x_n + h , \ \ \ \    y_{n+1} = y_n +F(x_n, y_n)h    dy/dx=x+y, \ \ \ y(0)=1   h   x   x_1=x_o+h   x_2 = x_1+h  = (x_o+h)+h = x_o+2h   x_3 = x_2+h = (x_o+2h)+h = x_o+3h   \vdots   x_n = x   x   x_n = x_0+nh   n   x_n=x   x_0+nh=x \implies n = \cfrac{x-x_0}{h}   n   x   x_0   h   h \to 0   n \to \infty   y   y_{n+1}=y_n+F(x_n,y_n)h   y_{n+1}=y_n+(x_n+y_n)h   x_n   y_{n+1}=y_n+((x_0+nh)+y_n)h   y(x_0)=y_0   y(n+1)=y(n)+(x_0+nh+y(n))h, \ \ \ \ y(x_0)=y_0   y(n)=(x_0h + x_0 + y_0 + 1)(h+1)^{n-x_0}-x_0 -hn -1   (x_0, y_0) = (0, 1)   y(n)=2(h+1)^n -hn-1   n   x   n   x   n = \cfrac{x-x_0}{h} = \cfrac{x}{h}   y(x) \approx 2(h+1)^{x/h}-h \left( \cfrac{x}{h}\right)-1   y(x) \approx 2(h+1)^{x/h}-x-1   h   \displaystyle y(x) = \lim_{h\to 0} \left[ 2(h+1)^{x/h}-x-1 \right]   \displaystyle y(x)=\lim_{h\to 0}\left[ 2(h+1)^{x/h} \right]-x-1   \displaystyle y(x)=2\lim_{h\to 0}\left[(h+1)^{x/h} \right]-x-1   y(x) = 2e^x-x-1   \displaystyle \lim_{h\to 0}(h+1)^{x/h}=e^x   dy/dx = x+y, \ \ y(0)=1   h ","['calculus', 'ordinary-differential-equations', 'numerical-methods', 'eulers-method']"
42,Solving the differential equation $y' = x - y^2$,Solving the differential equation,y' = x - y^2,"I know this differential equation is ""unsolvable"", this is my attempt at trying to solve it. I will start by finding a formula for a general DE of the form $ y' = f(x, y) $ , where $ f(x,y) $ can be written as $ g(x)+h(y) $ Take the differential of both sides $ d(y') = d(f(x, y)) $ $ y''(x)dx = \cfrac{\partial f}{\partial x}dx + \cfrac{\partial f}{\partial y}dy $ Divide both sides by $ dx $ $ y'' = \cfrac{\partial f}{\partial x} + \cfrac{\partial f}{\partial y}\cfrac{dy}{dx} $ $ y'' - \cfrac{\partial f}{\partial y}y' = \cfrac{\partial f}{\partial x} $ Using the definition of $ f $ we have $ \cfrac{\partial f}{\partial y} = \cfrac{\partial}{\partial y}(h(y)) = \cfrac{dh}{dy} $ and $ \cfrac{\partial f}{\partial x} = \cfrac{\partial}{\partial x}(g(x)) = \cfrac{dg}{dx} $ $ y'' - \cfrac{dh}{dy}\cfrac{dy}{dx} = \cfrac{dg}{dx} $ $ y'' - \cfrac{dh}{dx} = \cfrac{dg}{dx} $ $  y'' = \cfrac{dg}{dx} + \cfrac{dh}{dx} $ Integrate both sides $ \displaystyle \int y''dx = \int \cfrac{dg}{dx}dx + \int \cfrac{dh}{dx}dx $ $ y' = g(x)+h(x) + c_1 $ $ \displaystyle \int y'dx = \int (g(x)+h(x)+c_1)dx $ $ \displaystyle y(x)= \int (g(x)+h(x))dx + c_1x + c_2 $ Now we can use this formula to solve $ y'= x - y^2 \implies g(x) = x $ and $ h(y)=y^2 $ $ \displaystyle y(x) = \int (x - x^2)dx + c_1x+c_2  = \cfrac{x^2}{2} - \cfrac{x^3}{3} + c_1x + c_2 $ Apply the initial conditions $ y(0) = 1 \implies (c_2 = 1) \wedge (y'(0) = 0 - y(0)^2 = -1) $ $ y'(0) = -1 \implies c_1 = -1 $ $ y(x) = \cfrac{x^2}{2} - \cfrac{x^3}{3} -x + 1 $ This function satisfies the initial conditions but doesn't solve the DE and I can't see where I went wrong. Which step above was invalid? I'm thinking it has something to do with $ \cfrac{\partial f}{\partial y} = \cfrac{\partial}{\partial y}(h(y)) = \cfrac{dh}{dy} $ and $ \cfrac{\partial f}{\partial x} = \cfrac{\partial}{\partial x}(g(x)) = \cfrac{dg}{dx} $ but I'm pretty sure that is valid.","I know this differential equation is ""unsolvable"", this is my attempt at trying to solve it. I will start by finding a formula for a general DE of the form , where can be written as Take the differential of both sides Divide both sides by Using the definition of we have and Integrate both sides Now we can use this formula to solve and Apply the initial conditions This function satisfies the initial conditions but doesn't solve the DE and I can't see where I went wrong. Which step above was invalid? I'm thinking it has something to do with and but I'm pretty sure that is valid."," y' = f(x, y)   f(x,y)   g(x)+h(y)   d(y') = d(f(x, y))   y''(x)dx = \cfrac{\partial f}{\partial x}dx + \cfrac{\partial f}{\partial y}dy   dx   y'' = \cfrac{\partial f}{\partial x} + \cfrac{\partial f}{\partial y}\cfrac{dy}{dx}   y'' - \cfrac{\partial f}{\partial y}y' = \cfrac{\partial f}{\partial x}   f   \cfrac{\partial f}{\partial y} = \cfrac{\partial}{\partial y}(h(y)) = \cfrac{dh}{dy}   \cfrac{\partial f}{\partial x} = \cfrac{\partial}{\partial x}(g(x)) = \cfrac{dg}{dx}   y'' - \cfrac{dh}{dy}\cfrac{dy}{dx} = \cfrac{dg}{dx}   y'' - \cfrac{dh}{dx} = \cfrac{dg}{dx}    y'' = \cfrac{dg}{dx} + \cfrac{dh}{dx}   \displaystyle \int y''dx = \int \cfrac{dg}{dx}dx + \int \cfrac{dh}{dx}dx   y' = g(x)+h(x) + c_1   \displaystyle \int y'dx = \int (g(x)+h(x)+c_1)dx   \displaystyle y(x)= \int (g(x)+h(x))dx + c_1x + c_2   y'= x - y^2 \implies g(x) = x   h(y)=y^2   \displaystyle y(x) = \int (x - x^2)dx + c_1x+c_2  = \cfrac{x^2}{2} - \cfrac{x^3}{3} + c_1x + c_2   y(0) = 1 \implies (c_2 = 1) \wedge (y'(0) = 0 - y(0)^2 = -1)   y'(0) = -1 \implies c_1 = -1   y(x) = \cfrac{x^2}{2} - \cfrac{x^3}{3} -x + 1   \cfrac{\partial f}{\partial y} = \cfrac{\partial}{\partial y}(h(y)) = \cfrac{dh}{dy}   \cfrac{\partial f}{\partial x} = \cfrac{\partial}{\partial x}(g(x)) = \cfrac{dg}{dx} ","['calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
43,Inverse Laplace transform of $\dfrac{e^s}{s(e^s+1)}$ [duplicate],Inverse Laplace transform of  [duplicate],\dfrac{e^s}{s(e^s+1)},This question already has answers here : inverse laplace transform of $\frac{1}{s(e^s+1)}$ (2 answers) Closed 1 year ago . The original problem is to solve $$\mathcal{L}^{-1}\left\lbrace\frac{e^s}{s(e^s+1)}\right\rbrace.$$ Doing partial fractions $$\frac{e^s}{s(e^s+1)}=\frac{1}{s}-\frac{1}{s(e^s+1)}$$ the problem reduces to solve the inverse Laplace transform of the last member. Using Heaviside function the problem is the same. Any ideas?,This question already has answers here : inverse laplace transform of $\frac{1}{s(e^s+1)}$ (2 answers) Closed 1 year ago . The original problem is to solve Doing partial fractions the problem reduces to solve the inverse Laplace transform of the last member. Using Heaviside function the problem is the same. Any ideas?,\mathcal{L}^{-1}\left\lbrace\frac{e^s}{s(e^s+1)}\right\rbrace. \frac{e^s}{s(e^s+1)}=\frac{1}{s}-\frac{1}{s(e^s+1)},"['ordinary-differential-equations', 'laplace-transform', 'inverse-laplace']"
44,Solution to 2nd-order homogeneous linear ODE with variable coefficients.,Solution to 2nd-order homogeneous linear ODE with variable coefficients.,,"Consider the following second-order linear and homogeneous ODE: $$f''(x)+ \frac{\lambda}{x} \cdot f'(x) - \mu \cdot f(x) \enspace = \enspace 0$$ where $\lambda, \mu \in \mathbb{R}$ . I am looking for solutions to this ODE. Unfortunately, I am not able to find them myself. I have already tried several ansatzes, but none of them succeeded. Any ideas or hints on how to find a solution to this equation?","Consider the following second-order linear and homogeneous ODE: where . I am looking for solutions to this ODE. Unfortunately, I am not able to find them myself. I have already tried several ansatzes, but none of them succeeded. Any ideas or hints on how to find a solution to this equation?","f''(x)+ \frac{\lambda}{x} \cdot f'(x) - \mu \cdot f(x) \enspace = \enspace 0 \lambda, \mu \in \mathbb{R}",['ordinary-differential-equations']
45,Help Proving that Set Invariant for a Dynamical System.,Help Proving that Set Invariant for a Dynamical System.,,"I'm practicing for an upcoming test, and this one has been giving me some problems. Suppose we have $$ \begin{cases} \dot x = x^2 + 2y - 4 \\ \dot y = -2xy \end{cases}. $$ Let $R = \{(x,y) : |x| \leq 2,\, 0 \leq y \leq 4 - x^2\}$ . I'm to prove this is invariant, i.e., if $x_0 \in R$ , then $$ \Phi(t, x_0) \in R, \hspace{1em} \forall t\in \mathbb{R}. $$ So far, I have $y = 0$ stays on $\{(x,0) : |x| \leq 2\}$ , so the bottom line boundary is invariant. But, I'm having trouble with the boundary of the parabola. If it's invariant, then obviously we are done; but it may just be a trapping region. Any hints on where to go would be most appreciated.","I'm practicing for an upcoming test, and this one has been giving me some problems. Suppose we have Let . I'm to prove this is invariant, i.e., if , then So far, I have stays on , so the bottom line boundary is invariant. But, I'm having trouble with the boundary of the parabola. If it's invariant, then obviously we are done; but it may just be a trapping region. Any hints on where to go would be most appreciated.","
\begin{cases}
\dot x = x^2 + 2y - 4 \\
\dot y = -2xy
\end{cases}.
 R = \{(x,y) : |x| \leq 2,\, 0 \leq y \leq 4 - x^2\} x_0 \in R 
\Phi(t, x_0) \in R, \hspace{1em} \forall t\in \mathbb{R}.
 y = 0 \{(x,0) : |x| \leq 2\}",['ordinary-differential-equations']
46,Find solution of ordinary differential equation,Find solution of ordinary differential equation,,"$ \frac{dy}{dx} + 2xy=y $ which satisfies the boundary conditon $y(3)=1$ $$\frac{dy}{dx} + (2x-1)y = 0,$$ then using integrating factor $e^{\int2x-1 dx}$ I then get: $e^{x^2-x}\frac{dy}{dx} + (2x-1)ye^{x^2-x} = 0e^{x^2-x}$ giving: $\frac{d}{dx}{ye^{x^2-x}}=0e^{x^2-x}$ integrate both sides: ${ye^{x^2-x}}=C$ and thus: $y=Ce^{-x^2+x}$ but I'm not sure this is correct as I have been given a different answer. Any help would be appreciated.",which satisfies the boundary conditon then using integrating factor I then get: giving: integrate both sides: and thus: but I'm not sure this is correct as I have been given a different answer. Any help would be appreciated.," \frac{dy}{dx} + 2xy=y  y(3)=1 \frac{dy}{dx} + (2x-1)y = 0, e^{\int2x-1 dx} e^{x^2-x}\frac{dy}{dx} + (2x-1)ye^{x^2-x} = 0e^{x^2-x} \frac{d}{dx}{ye^{x^2-x}}=0e^{x^2-x} {ye^{x^2-x}}=C y=Ce^{-x^2+x}",['ordinary-differential-equations']
47,The derivative of flow for an autonomous system,The derivative of flow for an autonomous system,,"We are considering an autonomous equation $$\dot x=f(x),$$ where $f: \mathbb{R}^n\rightarrow\mathbb{R}^n$ is a $C^1$ vectorfield. Let $\phi^t$ be the corresponding flow. Is it true that $$D\phi^t(x)f(x)=f(\phi^t(x)),$$ why? $D$ stands for the Jacobian matrix of $\phi^t(x)$ . This is a question I encountered while I was reading researching articles: Coomes, B. A., Koçak, H., Palmer, K. J. (1995). A shadowing theorem for ordinary differential equations. Zeitschrift für angewandte Mathematik und Physik ZAMP , 46(1), 85-106. K. J. Palmer (1996) Shadowing and Silnikov Chaos. Nonlinear Analysis, Theory, Methods & Applications , 27(9), 1075-1093. Palmer, K. J. (2008). Transversal periodic-to-periodic homoclinic orbits. Handbook of Differential Equations: Ordinary Differential Equations , 4, 365-439. The authors simply state this is a fact on Page 96, 1077, 382 of the articles above, but without any derivation process. So I think this might come from some fundamental theory in dynamical systems. So I calculate like this: $$D\phi^t(x(s))f(x(s))=D\phi^t(x(s))\dot x(s)=\frac{d}{ds}[\phi^t(x(s))]=f(\phi^t(x)).$$ I'm not sure if this is correct or in a standard way. Any help will be deeply appreciated.","We are considering an autonomous equation where is a vectorfield. Let be the corresponding flow. Is it true that why? stands for the Jacobian matrix of . This is a question I encountered while I was reading researching articles: Coomes, B. A., Koçak, H., Palmer, K. J. (1995). A shadowing theorem for ordinary differential equations. Zeitschrift für angewandte Mathematik und Physik ZAMP , 46(1), 85-106. K. J. Palmer (1996) Shadowing and Silnikov Chaos. Nonlinear Analysis, Theory, Methods & Applications , 27(9), 1075-1093. Palmer, K. J. (2008). Transversal periodic-to-periodic homoclinic orbits. Handbook of Differential Equations: Ordinary Differential Equations , 4, 365-439. The authors simply state this is a fact on Page 96, 1077, 382 of the articles above, but without any derivation process. So I think this might come from some fundamental theory in dynamical systems. So I calculate like this: I'm not sure if this is correct or in a standard way. Any help will be deeply appreciated.","\dot x=f(x), f: \mathbb{R}^n\rightarrow\mathbb{R}^n C^1 \phi^t D\phi^t(x)f(x)=f(\phi^t(x)), D \phi^t(x) D\phi^t(x(s))f(x(s))=D\phi^t(x(s))\dot x(s)=\frac{d}{ds}[\phi^t(x(s))]=f(\phi^t(x)).","['ordinary-differential-equations', 'dynamical-systems']"
48,Properties of periodic Sturm-Liouville Problem,Properties of periodic Sturm-Liouville Problem,,"Consider the Regular Sturm Liouville(RSLP) problem $$(p(x)y’)’+(q(x)+\lambda r(x))y=0$$ where $p,q$ and $r$ are functions such that $p$ has continuous derivative, $q$ and $r$ are continuous, and $p(x)>0$ and $r(x)>0$ for all $x$ on a real interval $a\leq x\leq b$ and $\lambda $ is a parameter independent of $x$ and with boundary conditions $$A_1y(a)+A_2y’(a)=0$$ $$B_1y(b)+B_2y(b)=0$$ I know properties of this regular Sturm Liouville problems as follows( Given in Shepley L. Ross Differential Equations) $1$ . Eigen values of RSLP are reals and can be arrange in an increasing order. $2$ . Eigen values of RSLP are simple. $3$ . Eigen function $\phi_n(x)$ corresponding to $n$ -th eigen value $\lambda_n$ has exactly $n-1$ -zeros in the open interval $(a,b)$ . $4$ . Eigen functions corresponding to different eigen values are orthogonal with respect to weight function $r$ on $[a,b]$ . Now I only know that for Periodic Strum Liouville problem eigen values many not be simple I.e. there many be two linearly independent eigen functions corresponding to an eigen value . I want to know whether all others $3$ properties written above are true for periodic Sturm Liouville problem . By a periodic Sturm Liouville problem  I mean the above differential equation with boundary conditions $y(a)=y(b), y’(a)=y’(b)$ and one more condition as $p(a)=p(b)$ .Thank you .","Consider the Regular Sturm Liouville(RSLP) problem where and are functions such that has continuous derivative, and are continuous, and and for all on a real interval and is a parameter independent of and with boundary conditions I know properties of this regular Sturm Liouville problems as follows( Given in Shepley L. Ross Differential Equations) . Eigen values of RSLP are reals and can be arrange in an increasing order. . Eigen values of RSLP are simple. . Eigen function corresponding to -th eigen value has exactly -zeros in the open interval . . Eigen functions corresponding to different eigen values are orthogonal with respect to weight function on . Now I only know that for Periodic Strum Liouville problem eigen values many not be simple I.e. there many be two linearly independent eigen functions corresponding to an eigen value . I want to know whether all others properties written above are true for periodic Sturm Liouville problem . By a periodic Sturm Liouville problem  I mean the above differential equation with boundary conditions and one more condition as .Thank you .","(p(x)y’)’+(q(x)+\lambda r(x))y=0 p,q r p q r p(x)>0 r(x)>0 x a\leq x\leq b \lambda  x A_1y(a)+A_2y’(a)=0 B_1y(b)+B_2y(b)=0 1 2 3 \phi_n(x) n \lambda_n n-1 (a,b) 4 r [a,b] 3 y(a)=y(b), y’(a)=y’(b) p(a)=p(b)","['ordinary-differential-equations', 'boundary-value-problem']"
49,Solution to this differential equation that does not diverge at $x=0$,Solution to this differential equation that does not diverge at,x=0,"I have the differential equation $$\tan x \; \frac{\text{d} y}{\text{d} x} + y = {\rm e}^x \tan x$$ By using the integrating factor $\mu(x)=\cos{x}$ , I solved it as an equation in full differentials and got the solution (verified with WolframAlpha). $$y(x) = A\csc{x} + \frac{1}{2}{\rm e}^x(1-\cot{x})$$ However, the question I am solving asks for a solution that does not diverge at $x=0$ , which this solution clearly does because of the $\cot{x}$ . How can I get a solution that converges?","I have the differential equation By using the integrating factor , I solved it as an equation in full differentials and got the solution (verified with WolframAlpha). However, the question I am solving asks for a solution that does not diverge at , which this solution clearly does because of the . How can I get a solution that converges?",\tan x \; \frac{\text{d} y}{\text{d} x} + y = {\rm e}^x \tan x \mu(x)=\cos{x} y(x) = A\csc{x} + \frac{1}{2}{\rm e}^x(1-\cot{x}) x=0 \cot{x},"['calculus', 'ordinary-differential-equations']"
50,Stuck solving this differential equation,Stuck solving this differential equation,,"I'm trying to solve this differential equation: $y' = 9x^2+y+4+3x(2y+1)$ My approach to the problem: First, I multiplied $3x$ by $(2y+1)$ , which brought me to this equation: \begin{align*}     y' &= 9x^2+y^2+y+4+6xy+3x \\     y' &= (3x+y)^2+(3x+y)+4 \end{align*} Then I tried replacing $(3x+y)$ with $u$ , so $y = u - 3x$ and $y' = u' - 3$ With this kind of replacement the equation then looked like this: \begin{align*}     u' - 3 &= u^2 + u + 4 \\     u' &= u^2 + u + 7 \\     du &/ (u^2+u+7)=dx  \end{align*} Problem: This led to some confusion because $u^2 + u + 7$ does not seem to have any rational solutions and there is no mention of complex numbers in the answer. What exactly I might be doing wrong here? Maybe I've picked the wrong path from the start? Would really appreciate any help. Answer to the differential equation:","I'm trying to solve this differential equation: My approach to the problem: First, I multiplied by , which brought me to this equation: Then I tried replacing with , so and With this kind of replacement the equation then looked like this: Problem: This led to some confusion because does not seem to have any rational solutions and there is no mention of complex numbers in the answer. What exactly I might be doing wrong here? Maybe I've picked the wrong path from the start? Would really appreciate any help. Answer to the differential equation:","y' = 9x^2+y+4+3x(2y+1) 3x (2y+1) \begin{align*}
    y' &= 9x^2+y^2+y+4+6xy+3x \\
    y' &= (3x+y)^2+(3x+y)+4
\end{align*} (3x+y) u y = u - 3x y' = u' - 3 \begin{align*}
    u' - 3 &= u^2 + u + 4 \\
    u' &= u^2 + u + 7 \\
    du &/ (u^2+u+7)=dx 
\end{align*} u^2 + u + 7","['ordinary-differential-equations', 'derivatives']"
51,Solve $ x^{3}{y}'''+x{y}'-y = x\ln(x) \\ $,Solve, x^{3}{y}'''+x{y}'-y = x\ln(x) \\ ,"Solve $$ x^{3}{y}'''+x{y}'-y = x\ln(x) \\    $$ using shift $x=e^{z}$ and differential operator $Dz=\frac{d}{dz}$ What does $Dz = d / dz$ mean? I did this but I don't know how to continue. Please help. $$ (e^{z})^{3}{y}'''+e^{z}{y}'-y = e^{z}\ln(e^{z}) \\\\(e^{3z}){y}'''+e^{z}{y}'-y = e^{z}{z}$$ And I tried $\,\,y=z^r$ $$e^{3}r(r^{2}-r-2)z^{r-3}+e^{z}rz^{r-1}-z^{r}=0$$",Solve using shift and differential operator What does mean? I did this but I don't know how to continue. Please help. And I tried," x^{3}{y}'''+x{y}'-y = x\ln(x) \\     x=e^{z} Dz=\frac{d}{dz} Dz = d / dz  (e^{z})^{3}{y}'''+e^{z}{y}'-y = e^{z}\ln(e^{z}) \\\\(e^{3z}){y}'''+e^{z}{y}'-y = e^{z}{z} \,\,y=z^r e^{3}r(r^{2}-r-2)z^{r-3}+e^{z}rz^{r-1}-z^{r}=0",['ordinary-differential-equations']
52,Solve a first-order nonlinear ordinary differential equation (boundary value problem),Solve a first-order nonlinear ordinary differential equation (boundary value problem),,"I have been trying to solve the following boundary value problem: $$(x^2-36)(y')^2-2xyy'+y^2-36=0$$ Given the conditions $y(0)=6$ and $y(10)=0$ , where $y(x)$ is continuously twice differentiable on $[0,10]$ . The requirement is not really to solve the problem itself, but to find $$\int_{0}^{10}y(x)dx$$ I have almost found the answer, but got stuck. My idea was to solve for $y$ and not for $y'$ : $$y=xy'\pm6\sqrt{(y')^2+1}$$ And then to directly integrate this expression. Finally, I ended up with: $$\int_{0}^{10}y(x)dx=\pm3\int_{0}^{10}\left(\sqrt{(y')^2+1}\right)dx$$ But I do not understand how to solve the integral on the right, I have tried many substitutions, but they led nowhere. This leads me to believe that my overall idea to solve for $y$ was a mistake, but I am not sure. I have tried solving for $y'$ and for $x$ , but this did not help either. I would appreciate any suggestions on how to solve the problem.","I have been trying to solve the following boundary value problem: Given the conditions and , where is continuously twice differentiable on . The requirement is not really to solve the problem itself, but to find I have almost found the answer, but got stuck. My idea was to solve for and not for : And then to directly integrate this expression. Finally, I ended up with: But I do not understand how to solve the integral on the right, I have tried many substitutions, but they led nowhere. This leads me to believe that my overall idea to solve for was a mistake, but I am not sure. I have tried solving for and for , but this did not help either. I would appreciate any suggestions on how to solve the problem.","(x^2-36)(y')^2-2xyy'+y^2-36=0 y(0)=6 y(10)=0 y(x) [0,10] \int_{0}^{10}y(x)dx y y' y=xy'\pm6\sqrt{(y')^2+1} \int_{0}^{10}y(x)dx=\pm3\int_{0}^{10}\left(\sqrt{(y')^2+1}\right)dx y y' x","['ordinary-differential-equations', 'definite-integrals', 'boundary-value-problem']"
53,Solve $(y+1)dx+(x+1)dy=0$,Solve,(y+1)dx+(x+1)dy=0,Solve $(y+1)dx+(x+1)dy=0$ $$\frac{dy}{y+1}=-\frac{dx}{x+1}$$ then we get $\ln|y+1|=-\ln|x+1|+c$ $$\ln(|(y+1)(x+1)|)=c$$ $$|(y+1)(x+1)|=e^c=c_1$$ but answer is $y+1=\frac{c}{x+1}$ can you help to find where is my mistake?,Solve then we get but answer is can you help to find where is my mistake?,(y+1)dx+(x+1)dy=0 \frac{dy}{y+1}=-\frac{dx}{x+1} \ln|y+1|=-\ln|x+1|+c \ln(|(y+1)(x+1)|)=c |(y+1)(x+1)|=e^c=c_1 y+1=\frac{c}{x+1},"['calculus', 'ordinary-differential-equations']"
54,$AB=Y = y + y'(x) (X-x)$ $PG= Y = y - \frac{1}{y'(x)} (X-x)$ Find the curves such that $PM=MG$.,Find the curves such that .,AB=Y = y + y'(x) (X-x) PG= Y = y - \frac{1}{y'(x)} (X-x) PM=MG,"$C$ is a curve of $y(x)$ function on $XY$ , there is point $P:=(x,y)$ . A parallel line and perpendicular line of C pass through the point $P$ . $AB=Y = y + y'(x) (X-x)$ $PG= Y = y - \frac{1}{y'(x)} (X-x)$ Find the curves such that $PM=MG$ . I need to find the formula of the curves family and don't know how to do it. My solution : I will find $M$ and $G$ : $0=y - \frac{1}{y'(x)} (X-x) \implies X_G=yy'(x)+x \implies G=(yy'(x)+x,0).$ $Y_M = y - \frac{1}{y'(x)} (0-x) \implies Y_M=y+\frac{x}{y'(x)} \implies M=(0,y+\frac{x}{y'(x)}).$ Find $PM : \sqrt{(x-0)^2+(y-(y+\frac{x}{y'(x)})^2} = \sqrt{x^2+(\frac{x}{y'(x)})^2}$ Find $MG$ : $\sqrt{(0-(yy'(x)+x))^2+(y+\frac{x}{y'(x)}-0)^2}$ $PM=MG : \sqrt{x^2+(\frac{x}{y'(x)})^2}= \sqrt{(0-(yy'(x)+x))^2+(y+\frac{x}{y'(x)}-0)^2}$ $ \implies x^2+(\frac{x}{y'(x)})^2=(y\cdot y'(x)+x)^2+(y+\frac{x}{y'(x))}^2$ $ \implies x^2 + \frac{x^2}{(y'(x))^2}=y^2\cdot (y'(x))^2+2y\cdot y'(x)\cdot x+x^2+y^2+\frac{2xy}{y'(x)}+\frac{x^2}{(y'(x))^2} $ $\implies 0=y^2\cdot y'(x)+2y\cdot y'(x)\cdot x+y^2+\frac{2xy}{y'(x)} \implies y'(x)=\sqrt{\frac{x}{-y-x}}.$ Is this part of my solution correct?","is a curve of function on , there is point . A parallel line and perpendicular line of C pass through the point . Find the curves such that . I need to find the formula of the curves family and don't know how to do it. My solution : I will find and : Find Find : Is this part of my solution correct?","C y(x) XY P:=(x,y) P AB=Y = y + y'(x) (X-x) PG= Y = y - \frac{1}{y'(x)} (X-x) PM=MG M G 0=y - \frac{1}{y'(x)} (X-x) \implies X_G=yy'(x)+x \implies G=(yy'(x)+x,0). Y_M = y - \frac{1}{y'(x)} (0-x) \implies Y_M=y+\frac{x}{y'(x)} \implies M=(0,y+\frac{x}{y'(x)}). PM : \sqrt{(x-0)^2+(y-(y+\frac{x}{y'(x)})^2} = \sqrt{x^2+(\frac{x}{y'(x)})^2} MG \sqrt{(0-(yy'(x)+x))^2+(y+\frac{x}{y'(x)}-0)^2} PM=MG : \sqrt{x^2+(\frac{x}{y'(x)})^2}= \sqrt{(0-(yy'(x)+x))^2+(y+\frac{x}{y'(x)}-0)^2}  \implies x^2+(\frac{x}{y'(x)})^2=(y\cdot y'(x)+x)^2+(y+\frac{x}{y'(x))}^2  \implies x^2 + \frac{x^2}{(y'(x))^2}=y^2\cdot (y'(x))^2+2y\cdot y'(x)\cdot x+x^2+y^2+\frac{2xy}{y'(x)}+\frac{x^2}{(y'(x))^2}  \implies 0=y^2\cdot y'(x)+2y\cdot y'(x)\cdot x+y^2+\frac{2xy}{y'(x)} \implies y'(x)=\sqrt{\frac{x}{-y-x}}.",['ordinary-differential-equations']
55,Solve the ODE $y'' = (y')^2$,Solve the ODE,y'' = (y')^2,"I am asked in a past question paper to solve the following ODE: $$ y'' = (y')^{2}$$ To solve this, I began by equating $$y' = u$$ Differentiating both sides $w.r.t.x$ $$ \frac{d^{2}y}{dx^{2}} = \frac{du}{dx}$$ And therefore substituting back to out original equation I got: $$  \frac{du}{dx} = u^{2}$$ Rearranging $$\frac{du}{u^{2}} = dx$$ Integrating $w.r.t.x$ I got $$ \frac{-1}{u} = x+C$$ and substituting back $u$ $$ \frac{-dx}{dy} = x+C$$ $$ =x + \frac{dx}{dy} = C$$ $$=\frac{-dx}{x+c} = dy$$ $$ \log(\frac{1}{x+c}) = y$$ $$ -(x+c) = e^{y}$$ Is my answer and method correct? EDIT As pointed out by @MtGlasser We can re-write the D.E as $$ \frac{y''}{y'} = y'$$ Which is of the form : $$(ln(y'))' = y'$$ Integrating on both sides we get $$ ln (y') = y + c$$ And taking exponent on both sides and re-arranging we get $$ e^{-1}e^{-y} dy = dx$$ Integrating we get the asnwer $$ e^{-(y+1)} - C = x$$","I am asked in a past question paper to solve the following ODE: To solve this, I began by equating Differentiating both sides And therefore substituting back to out original equation I got: Rearranging Integrating I got and substituting back Is my answer and method correct? EDIT As pointed out by @MtGlasser We can re-write the D.E as Which is of the form : Integrating on both sides we get And taking exponent on both sides and re-arranging we get Integrating we get the asnwer", y'' = (y')^{2} y' = u w.r.t.x  \frac{d^{2}y}{dx^{2}} = \frac{du}{dx}   \frac{du}{dx} = u^{2} \frac{du}{u^{2}} = dx w.r.t.x  \frac{-1}{u} = x+C u  \frac{-dx}{dy} = x+C  =x + \frac{dx}{dy} = C =\frac{-dx}{x+c} = dy  \log(\frac{1}{x+c}) = y  -(x+c) = e^{y}  \frac{y''}{y'} = y' (ln(y'))' = y'  ln (y') = y + c  e^{-1}e^{-y} dy = dx  e^{-(y+1)} - C = x,"['ordinary-differential-equations', 'solution-verification']"
56,Is my second order ODE solution correct?,Is my second order ODE solution correct?,,"$$xy'' + 2y' - xy = e^x$$ Now, I solved the homogenous equation correctly using reduction of order, I even verified my solution on wolframalpha. $$y_h = \frac{e^xC_1}{x} + \frac{e^{-x}C_1}{x}$$ However, next I tried to find the particular solution using variation of parameters. I calculated that the wronskian is $$W = \frac{-2}{x^2}$$ and that $$W_1 = -\frac{1}{x^2}$$ and $$W_2 = \frac{e^{2x}}{x^2}$$ This means that $C_1' = \frac{1}{2}$ and $C_2' = -\frac{e^{2x}}{2}.$ So that $$C_1 = \frac{x}{2} + K_1$$ $$C_2 = -\frac{e^{2x}}{4} + K_2$$ Plugging this into my solution I get that $$\frac{K_1 e^x}{x} + \frac{K_2 e^{-x}}{x} + \frac{e^x}{2} - \frac{e^x}{4 x}$$ However, the WolframAlpha solution is $$\frac{K_1 e^x}{x} + \frac{K_2 e^{-x}}{x} + \frac{e^x}{2}$$ Where did I go wrong? I tried to find my mistake so I can't rule out an error made due to lack of concentration but I seriously can't find it.","Now, I solved the homogenous equation correctly using reduction of order, I even verified my solution on wolframalpha. However, next I tried to find the particular solution using variation of parameters. I calculated that the wronskian is and that and This means that and So that Plugging this into my solution I get that However, the WolframAlpha solution is Where did I go wrong? I tried to find my mistake so I can't rule out an error made due to lack of concentration but I seriously can't find it.",xy'' + 2y' - xy = e^x y_h = \frac{e^xC_1}{x} + \frac{e^{-x}C_1}{x} W = \frac{-2}{x^2} W_1 = -\frac{1}{x^2} W_2 = \frac{e^{2x}}{x^2} C_1' = \frac{1}{2} C_2' = -\frac{e^{2x}}{2}. C_1 = \frac{x}{2} + K_1 C_2 = -\frac{e^{2x}}{4} + K_2 \frac{K_1 e^x}{x} + \frac{K_2 e^{-x}}{x} + \frac{e^x}{2} - \frac{e^x}{4 x} \frac{K_1 e^x}{x} + \frac{K_2 e^{-x}}{x} + \frac{e^x}{2},"['calculus', 'ordinary-differential-equations']"
57,Solve differential equation $y'=a \sin(bt-y)$,Solve differential equation,y'=a \sin(bt-y),"I'm looking for a solution to $y'=a \sin(bt-y)$ , where a and b are constants. I've looked at previous posts were b=1, which leads to nice solutions. I'm looking for a solution when b is an arbitrary constant. To kick off: Substitute $u=bt-y$ $u'=b-y'$ -> $y'=b-u'$ . $u'=b-a\sin(u)$ $\int\frac{du}{b-a\sin(u)}=\int dt$ And this is where I'm stuck. Differential/integral equation solvers give me some solutions including tans and arctans. Plotting those solutions does not match my numerical approach. Maybe it has something to do with the domains of the tan and arctan? I look forward to your help!","I'm looking for a solution to , where a and b are constants. I've looked at previous posts were b=1, which leads to nice solutions. I'm looking for a solution when b is an arbitrary constant. To kick off: Substitute -> . And this is where I'm stuck. Differential/integral equation solvers give me some solutions including tans and arctans. Plotting those solutions does not match my numerical approach. Maybe it has something to do with the domains of the tan and arctan? I look forward to your help!",y'=a \sin(bt-y) u=bt-y u'=b-y' y'=b-u' u'=b-a\sin(u) \int\frac{du}{b-a\sin(u)}=\int dt,"['integration', 'ordinary-differential-equations']"
58,A simple looking yet tricky Second Order Nonlinear Differential Equation.,A simple looking yet tricky Second Order Nonlinear Differential Equation.,,"I have been struggling with the following second order differential equation for quite a while, $$y''(t)=\frac{(y'(t))^2}{y(t)}-\frac{(y'(0))^2}{y'(0)\times t+c}$$ Where $y'(0)$ is a constant of units $\frac{1}{[T]}$ and c is dimensionless. It looks fairly simple and I can bet there is a simple trick to be used to solve it but I just cannot see it. Any help would be greatly appreciated. If necessary, the initial conditions known are, $y(0)=1$ and $y'(0)$ is approximately $8.7e43$ . My first reflex was to write $y''(t)$ as $\frac{dy'(t)}{dt}$ since I could potentially just integrate $\int\frac{(y'(0))^2}{y'(0)\times t+c}dt$ , which is easy. However it didn't work. Simiralry, I have tried separating $y''(t)$ into $\frac{dy'(t)}{dy(t)}\times y'(t)$ . This however didn't work since isolating $y'(t)dy'$ was out of my range of skills. Finally, I just tried to integrate both sides with respects to $t$ , however, I lack information to compute $\int\frac{(y'(t)^2)}{y(t)}dt$ when using integration by parts.","I have been struggling with the following second order differential equation for quite a while, Where is a constant of units and c is dimensionless. It looks fairly simple and I can bet there is a simple trick to be used to solve it but I just cannot see it. Any help would be greatly appreciated. If necessary, the initial conditions known are, and is approximately . My first reflex was to write as since I could potentially just integrate , which is easy. However it didn't work. Simiralry, I have tried separating into . This however didn't work since isolating was out of my range of skills. Finally, I just tried to integrate both sides with respects to , however, I lack information to compute when using integration by parts.",y''(t)=\frac{(y'(t))^2}{y(t)}-\frac{(y'(0))^2}{y'(0)\times t+c} y'(0) \frac{1}{[T]} y(0)=1 y'(0) 8.7e43 y''(t) \frac{dy'(t)}{dt} \int\frac{(y'(0))^2}{y'(0)\times t+c}dt y''(t) \frac{dy'(t)}{dy(t)}\times y'(t) y'(t)dy' t \int\frac{(y'(t)^2)}{y(t)}dt,"['calculus', 'ordinary-differential-equations', 'derivatives']"
59,What method was used to solve this non-linear differential equation?,What method was used to solve this non-linear differential equation?,,"Consider the following first-order non-linear differential equation: $$\left(\frac{dx}{dt}\right)^2+1=\frac{a}{x}\,,$$ where $a\in \mathbb{R}_{>0}$ . I have been reading a book where they provide the following solution, assuming that $x(t)$ has a local maximum at $t=0$ , in the parametric form: $$ \left\{ \begin{aligned}t\left(\eta\right) & =\frac{1}{2}x_{max}\left(\eta+\sin\eta\right)\,,\\ x\left(\eta\right) & =\frac{1}{2}x_{max}\left(1+\cos\eta\right)\,. \end{aligned} \right. $$ My question is how was this solution found? I've been playing around with the equation, but I cannot find a way to solve it.","Consider the following first-order non-linear differential equation: where . I have been reading a book where they provide the following solution, assuming that has a local maximum at , in the parametric form: My question is how was this solution found? I've been playing around with the equation, but I cannot find a way to solve it.","\left(\frac{dx}{dt}\right)^2+1=\frac{a}{x}\,, a\in \mathbb{R}_{>0} x(t) t=0 
\left\{ \begin{aligned}t\left(\eta\right) & =\frac{1}{2}x_{max}\left(\eta+\sin\eta\right)\,,\\
x\left(\eta\right) & =\frac{1}{2}x_{max}\left(1+\cos\eta\right)\,.
\end{aligned}
\right.
",['ordinary-differential-equations']
60,a nonstandard differential equation - product of a function and her consecutive derivatives equal 1,a nonstandard differential equation - product of a function and her consecutive derivatives equal 1,,"For which $n\in\mathbb{N}$ there exists an open interval $I$ and a function $f\colon I\to\mathbb{R}$ , that is $n$ times differentiable and $f(x)\cdot f'(x)\cdot\ldots\cdot f^{(n)}(x)=1$ for all $x\in I$ ? Own problem. For $n=1$ it's easy to see that $I=(0,\infty)$ and $f(x)=2\sqrt{x}$ work fine. It seems to complicate a lot already for $n=2$ : https://www.wolframalpha.com/input/?i=f%28x%29*f%27%28x%29*f%27%27%28x%29%3D1 (I hardly understand this reply). What tools could crack a problem like this?","For which there exists an open interval and a function , that is times differentiable and for all ? Own problem. For it's easy to see that and work fine. It seems to complicate a lot already for : https://www.wolframalpha.com/input/?i=f%28x%29*f%27%28x%29*f%27%27%28x%29%3D1 (I hardly understand this reply). What tools could crack a problem like this?","n\in\mathbb{N} I f\colon I\to\mathbb{R} n f(x)\cdot f'(x)\cdot\ldots\cdot f^{(n)}(x)=1 x\in I n=1 I=(0,\infty) f(x)=2\sqrt{x} n=2","['ordinary-differential-equations', 'derivatives']"
61,"In general, for a graph of solutions to a differential equation is there a 3D graph s.t. these curves are its level curves? Then, what's this graph?","In general, for a graph of solutions to a differential equation is there a 3D graph s.t. these curves are its level curves? Then, what's this graph?",,"I've noticed that the solution curves to a differential equation look like the level curves to some 3D graph. E.g., for the system of equations: $$\begin{cases}x_1' = \dfrac{1}{10}x_1 \\[1mm] x_2' = -\dfrac{1}{2}x_2\end{cases}$$ with e.g. a solution given by a parametrized curve: $(k_1e^{\frac{1}{10}t}, k_2e^{-\frac{1}{2}t})$ , with some of the solution curves plotted: It seems to me like there should be some three dimensional graph whose level curves would be precisely these solution curves. Is there any way to prove this in general, or to find a general way to find what is this 3D graph? Any reading recommendation is also appreciated!","I've noticed that the solution curves to a differential equation look like the level curves to some 3D graph. E.g., for the system of equations: with e.g. a solution given by a parametrized curve: , with some of the solution curves plotted: It seems to me like there should be some three dimensional graph whose level curves would be precisely these solution curves. Is there any way to prove this in general, or to find a general way to find what is this 3D graph? Any reading recommendation is also appreciated!","\begin{cases}x_1' = \dfrac{1}{10}x_1 \\[1mm]
x_2' = -\dfrac{1}{2}x_2\end{cases} (k_1e^{\frac{1}{10}t}, k_2e^{-\frac{1}{2}t})","['ordinary-differential-equations', 'reference-request', '3d', 'parametrization']"
62,Methods for solving nonhomogeneous system of ODEs,Methods for solving nonhomogeneous system of ODEs,,"I have the system $$ \mathbf{x}'=\begin{pmatrix}2&1\\1&2\end{pmatrix}\mathbf{x}+\begin{pmatrix}e^t\\t\end{pmatrix}. $$ The eigenvalues of the matrix are 1 and 3 with eigenvectors $(1,-1)^T$ and $(1,1)^T$ respectively. Thus, we have the fundamental matrix $$ \Psi(t)=\begin{pmatrix}e^t & e^{3t}\\-e^t & e^{3t}\end{pmatrix}. $$ From here I used the method of variation of parameters to assume there's a solution $\mathbf{x}=\Psi(t)\mathbf{u}(t)$ , where $\mathbf{u}(t)$ satisfies $\Psi(t)\mathbf{u}'(t)=\mathbf{g}(t)$ . I solved this matrix equation and multiplied to find $$ \mathbf{x}=c_1\begin{pmatrix}1\\-1\end{pmatrix}e^t+c_2\begin{pmatrix}1\\1\end{pmatrix}e^{3t}+\frac{1}{2}\begin{pmatrix}1\\-1\end{pmatrix}te^t-\frac{5}{12}\begin{pmatrix}1\\1\end{pmatrix}e^t-\frac{1}{2}\begin{pmatrix}1\\-1\end{pmatrix}t+\frac{1}{9}\begin{pmatrix}4\\-5\end{pmatrix}. $$ I wanted to do this same problem using the Laplace transform given $\mathbf{x}(0)=\mathbf{0}$ . When I take the Laplace transform of the system and solve for $\mathbf{X}(s)$ I get $$ \mathbf{X}(s)= \begin{pmatrix} s-2&-1\\ -1&s-2 \end{pmatrix}^{-1} \begin{pmatrix} (s-1)^{-1}\\ s^{-2}. \end{pmatrix}. $$ Performing this computation and taking the inverse Laplace transform, I get $$ \mathbf{x}=\begin{pmatrix} \frac{1}{8}e^t+\frac{1}{8}e^{5t}-\frac{1}{4}e^t+t\\ e^t+\frac{2}{5}t+\frac{1}{2}e^{-t}+\frac{1}{50}e^{5t}-\frac{13}{25}. \end{pmatrix} $$ At this point I realized that there's no way this is the same answer. Does someone see where I'm going wrong or what I misunderstood about one of the methods?","I have the system The eigenvalues of the matrix are 1 and 3 with eigenvectors and respectively. Thus, we have the fundamental matrix From here I used the method of variation of parameters to assume there's a solution , where satisfies . I solved this matrix equation and multiplied to find I wanted to do this same problem using the Laplace transform given . When I take the Laplace transform of the system and solve for I get Performing this computation and taking the inverse Laplace transform, I get At this point I realized that there's no way this is the same answer. Does someone see where I'm going wrong or what I misunderstood about one of the methods?"," \mathbf{x}'=\begin{pmatrix}2&1\\1&2\end{pmatrix}\mathbf{x}+\begin{pmatrix}e^t\\t\end{pmatrix}.  (1,-1)^T (1,1)^T 
\Psi(t)=\begin{pmatrix}e^t & e^{3t}\\-e^t & e^{3t}\end{pmatrix}.
 \mathbf{x}=\Psi(t)\mathbf{u}(t) \mathbf{u}(t) \Psi(t)\mathbf{u}'(t)=\mathbf{g}(t) 
\mathbf{x}=c_1\begin{pmatrix}1\\-1\end{pmatrix}e^t+c_2\begin{pmatrix}1\\1\end{pmatrix}e^{3t}+\frac{1}{2}\begin{pmatrix}1\\-1\end{pmatrix}te^t-\frac{5}{12}\begin{pmatrix}1\\1\end{pmatrix}e^t-\frac{1}{2}\begin{pmatrix}1\\-1\end{pmatrix}t+\frac{1}{9}\begin{pmatrix}4\\-5\end{pmatrix}.
 \mathbf{x}(0)=\mathbf{0} \mathbf{X}(s) 
\mathbf{X}(s)=
\begin{pmatrix}
s-2&-1\\
-1&s-2
\end{pmatrix}^{-1}
\begin{pmatrix}
(s-1)^{-1}\\
s^{-2}.
\end{pmatrix}.
 
\mathbf{x}=\begin{pmatrix}
\frac{1}{8}e^t+\frac{1}{8}e^{5t}-\frac{1}{4}e^t+t\\
e^t+\frac{2}{5}t+\frac{1}{2}e^{-t}+\frac{1}{50}e^{5t}-\frac{13}{25}.
\end{pmatrix}
","['ordinary-differential-equations', 'laplace-transform']"
63,Differential equation: $2x^2y'=y^2(2xy'-y)$,Differential equation:,2x^2y'=y^2(2xy'-y),"Solve the differential equation: $$2x^2y'=y^2(2xy'-y)$$ I tried to convert it to the form of a total differential equation. $$\begin{array}{lrl} &2x^2y'&=y^2(2xy'-y)\\ \Leftrightarrow&y'(2x^2-2xy^2)+y^3&=0\\ \Leftrightarrow&y^3dx+(2x^2-2xy^2)dy&=0 \end{array}$$ Here, I tried my best to find the integrating factor, but can't. Of course, it has no form $\mu (x)$ or $\mu (y)$ . I also tried to set $\dfrac{y}{x}=u^\alpha$ . I wanted a perfect "" $\alpha$ "" in order to have a ""beautiful"" form. But, it isn't successful.","Solve the differential equation: I tried to convert it to the form of a total differential equation. Here, I tried my best to find the integrating factor, but can't. Of course, it has no form or . I also tried to set . I wanted a perfect "" "" in order to have a ""beautiful"" form. But, it isn't successful.","2x^2y'=y^2(2xy'-y) \begin{array}{lrl}
&2x^2y'&=y^2(2xy'-y)\\
\Leftrightarrow&y'(2x^2-2xy^2)+y^3&=0\\
\Leftrightarrow&y^3dx+(2x^2-2xy^2)dy&=0
\end{array} \mu (x) \mu (y) \dfrac{y}{x}=u^\alpha \alpha","['real-analysis', 'ordinary-differential-equations', 'systems-of-equations']"
64,Is there a (nice) closed form solution to $\frac{dP(t)}{dt} = AP(t)+P(t)A^T+R$,Is there a (nice) closed form solution to,\frac{dP(t)}{dt} = AP(t)+P(t)A^T+R,"The problem : Consider constant matrices $A,R\in\mathbb{R}^{n\times n}$ . I want to solve: $$ \frac{dP(t)}{dt} = AP(t)+P(t)A^T+R $$ for $P(t)\in\mathbb{R}^{n\times n}$ . My attempt : note that the differential equation is linear, so up to know I have been able to write $P(t)$ as vector $p(t)\in\mathbb{R}^{n^2}$ using the vectorization operator $p(t):=\text{vec}(P(t))$ from wikipedia which comply: $$ \text{vec}(AB) = (I\otimes A)\text{vec}(B) = (B^T\otimes I)\text{vec}(A) $$ hence, $$ \begin{aligned} \text{vec}\left(\frac{dP(t)}{dt}\right) &= \frac{dp(t)}{dt}= \text{vec}(AP(t)) + \text{vec}(P(t)A^T) + \text{vec}(R)\\ &=(I\otimes A)p(t) + (A\otimes I)p(t)+r\\ &=\tilde{A}p(t) + r \end{aligned} $$ where $\tilde{A} = (I\otimes A) + (A\otimes I)$ and $r=\text{vec}(R)$ . Then, $$ p(t) = \exp(\tilde{A}t)p(0) + \int_0^t\exp(\tilde{A}(t-\tau))rd\tau $$ However, I still haven't managed to convert $p(t)$ to $P(t)$ without making a total mess with the components of the matrices of the form $ \exp(\tilde{A}t)$ . I have only been able to ""revert"" the vectorization operation ""by hand"", so that I don't see the pattern in the form of $P(t)$ I obtain. Moreover, I have only been able to do this for a particular value of $n$ ( $n=2, n=3$ ) and I don't see the patter for general $n$ . Is there a way to obtain a more compact closed form solution for $P(t)$ (converting $p(t)$ to $P(t)$ for the general case)? Moreover, my procedure may be totally wrong. In that case, can you suggest the correct path? EDIT : I found this paper here where they obtain: $$ P(t) = \exp(At)P(0)\exp(A^Tt) + \int_0^t\exp(A\tau)R\exp(A^T\tau)d\tau $$ which is easily verified that it complies the differential equation. However, I still have no clue on how one can derive that solution.","The problem : Consider constant matrices . I want to solve: for . My attempt : note that the differential equation is linear, so up to know I have been able to write as vector using the vectorization operator from wikipedia which comply: hence, where and . Then, However, I still haven't managed to convert to without making a total mess with the components of the matrices of the form . I have only been able to ""revert"" the vectorization operation ""by hand"", so that I don't see the pattern in the form of I obtain. Moreover, I have only been able to do this for a particular value of ( ) and I don't see the patter for general . Is there a way to obtain a more compact closed form solution for (converting to for the general case)? Moreover, my procedure may be totally wrong. In that case, can you suggest the correct path? EDIT : I found this paper here where they obtain: which is easily verified that it complies the differential equation. However, I still have no clue on how one can derive that solution.","A,R\in\mathbb{R}^{n\times n} 
\frac{dP(t)}{dt} = AP(t)+P(t)A^T+R
 P(t)\in\mathbb{R}^{n\times n} P(t) p(t)\in\mathbb{R}^{n^2} p(t):=\text{vec}(P(t)) 
\text{vec}(AB) = (I\otimes A)\text{vec}(B) = (B^T\otimes I)\text{vec}(A)
 
\begin{aligned}
\text{vec}\left(\frac{dP(t)}{dt}\right) &= \frac{dp(t)}{dt}= \text{vec}(AP(t)) + \text{vec}(P(t)A^T) + \text{vec}(R)\\
&=(I\otimes A)p(t) + (A\otimes I)p(t)+r\\
&=\tilde{A}p(t) + r
\end{aligned}
 \tilde{A} = (I\otimes A) + (A\otimes I) r=\text{vec}(R) 
p(t) = \exp(\tilde{A}t)p(0) + \int_0^t\exp(\tilde{A}(t-\tau))rd\tau
 p(t) P(t)  \exp(\tilde{A}t) P(t) n n=2, n=3 n P(t) p(t) P(t) 
P(t) = \exp(At)P(0)\exp(A^Tt) + \int_0^t\exp(A\tau)R\exp(A^T\tau)d\tau
","['linear-algebra', 'ordinary-differential-equations', 'vectorization']"
65,differential equation $y' = \frac{a}{(b+xy)^2}$,differential equation,y' = \frac{a}{(b+xy)^2},Solve the differential equation $$y' = \frac{a}{(b+xy)^2}$$ My attempt: Substitution $$y = \frac{z}{x} \implies y' =  \frac{xz' - z}{x^2} \\  \frac{xz'-z}{x^2} = \frac{a}{(b+z)^2}$$ But this doesn't seem to help much. Wolfram Alpha says $$-\frac{\sqrt a \tanh^{-1}\left(\frac{\sqrt b y}{\sqrt a}\right)}{b^{3/2}} + \frac{\sqrt a \tanh^{-1}\left(\frac{ b y(b+y)-ax}{\sqrt a b^{3/2}}\right)}{b^{3/2}} + \frac{y}{b} = c_1$$,Solve the differential equation My attempt: Substitution But this doesn't seem to help much. Wolfram Alpha says,"y' = \frac{a}{(b+xy)^2} y = \frac{z}{x} \implies y' =  \frac{xz' - z}{x^2} \\ 
\frac{xz'-z}{x^2} = \frac{a}{(b+z)^2} -\frac{\sqrt a \tanh^{-1}\left(\frac{\sqrt b y}{\sqrt a}\right)}{b^{3/2}} + \frac{\sqrt a \tanh^{-1}\left(\frac{ b y(b+y)-ax}{\sqrt a b^{3/2}}\right)}{b^{3/2}} + \frac{y}{b} = c_1",['ordinary-differential-equations']
66,Integral equation with kernel $x-y$,Integral equation with kernel,x-y,"I am trying to solve the integral equation $$ f(x)=\int_0^x (x-y)f(y)dy, \ 0\leq x\leq 1  $$ in the space $C([0,1])$ of continuous functions on $[0,1]$ . My reasoning is this: Since $u(y)=(x-y)f(y)$ is continuous, the integral $\int_0^x (x-y)f(y)dy$ is a differentiable function, which means that the solution $f$ will also be differentiable. Differentiating gives $f'(x)=\int_0^xf(y)dy$ . By a similar argument as before $f'$ is differentiable as well, therefore $f''=f$ . This equation has the general solution $$ f(x)=a\exp(x)+b\exp(-x) $$ for some constants a,b. Now I can use $f(0)=\int_0^0\ldots=0$ to obtain $a=-b$ . My question is whether these are correct and if they are, and how to proceed in determining the constants...","I am trying to solve the integral equation in the space of continuous functions on . My reasoning is this: Since is continuous, the integral is a differentiable function, which means that the solution will also be differentiable. Differentiating gives . By a similar argument as before is differentiable as well, therefore . This equation has the general solution for some constants a,b. Now I can use to obtain . My question is whether these are correct and if they are, and how to proceed in determining the constants...","
f(x)=\int_0^x (x-y)f(y)dy, \ 0\leq x\leq 1 
 C([0,1]) [0,1] u(y)=(x-y)f(y) \int_0^x (x-y)f(y)dy f f'(x)=\int_0^xf(y)dy f' f''=f 
f(x)=a\exp(x)+b\exp(-x)
 f(0)=\int_0^0\ldots=0 a=-b","['integration', 'ordinary-differential-equations', 'integral-equations']"
67,How to relate non-harmonic oscillation to harmonic oscillation,How to relate non-harmonic oscillation to harmonic oscillation,,"Consider the follwoing equation $$\partial_t^2\phi(t)+\lambda\phi^3(t)=0. \tag{1}$$ This equation describes an oscillator in a potential $V(\phi)=\phi^4/4$ . And we know that the solution is the elliptic consine function: $$\phi(t)=\phi_0{\rm cn}[\sqrt{\lambda}\phi_0t,1/\sqrt{2}],$$ where $\phi_0$ is an integration constant. Since the elliptic cosine function is also a periodic function, I would assume that it is possible to  relate it to the cosine function by a rescalling of the time. So is it possible to transform Eq. (1) to the equation of motion for a harmonic oscillator with a rescalling of time $t\rightarrow \tau=f(t)$ , $$(\partial_\tau^2+\omega^2)\phi=0$$ with some frequency $\omega$ ?","Consider the follwoing equation This equation describes an oscillator in a potential . And we know that the solution is the elliptic consine function: where is an integration constant. Since the elliptic cosine function is also a periodic function, I would assume that it is possible to  relate it to the cosine function by a rescalling of the time. So is it possible to transform Eq. (1) to the equation of motion for a harmonic oscillator with a rescalling of time , with some frequency ?","\partial_t^2\phi(t)+\lambda\phi^3(t)=0. \tag{1} V(\phi)=\phi^4/4 \phi(t)=\phi_0{\rm cn}[\sqrt{\lambda}\phi_0t,1/\sqrt{2}], \phi_0 t\rightarrow \tau=f(t) (\partial_\tau^2+\omega^2)\phi=0 \omega",['ordinary-differential-equations']
68,Sketching phase portraits using Hamiltonians,Sketching phase portraits using Hamiltonians,,"Consider the system \begin{align*}     x' &= 2y^{3} - y \\     y' &= x^{3} - x \\ \end{align*} The problem I am working on asks to find the Hamiltonian for the system, find the equilibria, and then sketch the phase portrait. Finding equilibria is easy, and I believe the Hamiltonian is $$ H(x,y) = \frac{1}{2}(x^{2} + y^{4} - y^{2}) - \frac{1}{4}x^{4}. $$ I understand Hamiltonians give conserved quantities and therefore level curves in the phase plane. I understand how to sketch phase portraits from a system in polar coordinates. However, I do not at all understand how the Hamiltonian is supposed to help me visualize the phase portrait. I am studying for a test with questions like this so I can not use any mathematical software. How do you use Hamiltonians to visualize level curves and sketch phase portraits? Thanks.","Consider the system The problem I am working on asks to find the Hamiltonian for the system, find the equilibria, and then sketch the phase portrait. Finding equilibria is easy, and I believe the Hamiltonian is I understand Hamiltonians give conserved quantities and therefore level curves in the phase plane. I understand how to sketch phase portraits from a system in polar coordinates. However, I do not at all understand how the Hamiltonian is supposed to help me visualize the phase portrait. I am studying for a test with questions like this so I can not use any mathematical software. How do you use Hamiltonians to visualize level curves and sketch phase portraits? Thanks.","\begin{align*}
    x' &= 2y^{3} - y \\
    y' &= x^{3} - x \\
\end{align*} 
H(x,y) = \frac{1}{2}(x^{2} + y^{4} - y^{2}) - \frac{1}{4}x^{4}.
","['ordinary-differential-equations', 'dynamical-systems']"
69,$u(x)$ satisfies the integral equation $u(x) = \int_0^x \sin(u(t))u(t)^p dt$ on $0 \leq x \leq 1$. show that $u(x) = 0$ on this interval,satisfies the integral equation  on . show that  on this interval,u(x) u(x) = \int_0^x \sin(u(t))u(t)^p dt 0 \leq x \leq 1 u(x) = 0,"Suppose that $u(x)$ is continuous and satisfies the integral equation \begin{equation}\label{1.4.1}         u(x) = \int_0^x \sin(u(t))u(t)^p dt     \end{equation} on the interval $0 \leq x \leq 1$ . show that $u(x) = 0$ on this interval if $p \geq 0$ . This is what I have: Since $\sin(u(t))u(t)^p$ is continuous, it follows from the integral definition that $u(x)$ is differentiable. Let us differentiate both sides of equation above with respect to $x$ . This yields: \begin{equation}     u'(x) = \sin(u(x))u(x)^p     \end{equation} This ODE is separabale and becomes: \begin{equation}         \frac{1}{\sin(u(x))u(x)^p}du(x) = dx     \end{equation} However, this doesn't seem easily solvable so I'm not sure how to show that $u(x) = 0$ from this.","Suppose that is continuous and satisfies the integral equation on the interval . show that on this interval if . This is what I have: Since is continuous, it follows from the integral definition that is differentiable. Let us differentiate both sides of equation above with respect to . This yields: This ODE is separabale and becomes: However, this doesn't seem easily solvable so I'm not sure how to show that from this.","u(x) \begin{equation}\label{1.4.1}
        u(x) = \int_0^x \sin(u(t))u(t)^p dt
    \end{equation} 0 \leq x \leq 1 u(x) = 0 p \geq 0 \sin(u(t))u(t)^p u(x) x \begin{equation}
    u'(x) = \sin(u(x))u(x)^p
    \end{equation} \begin{equation}
        \frac{1}{\sin(u(x))u(x)^p}du(x) = dx
    \end{equation} u(x) = 0",['ordinary-differential-equations']
70,Relationship Between Power Series ODE Solution Techniques?,Relationship Between Power Series ODE Solution Techniques?,,"When solving an ODE via a power series at an ordinary (nonsingular) point, the initial guess is $y = \sum_{n = 0}^\infty a_n x^n$ . When solving an Euler ODE, the second order equation $x^2 y'' + pxy' + qy = 0$ where $p$ and $q$ are constant coefficients for example, the initial guess is $y = x^s$ . When solving an ODE via The Method Of Frobenius at a regular singular point, the initial guess is said to be the product of the two prior guesses, $y = x^s \sum_{n = 0}^\infty a_n x^n$ . My question is whether The Method Of Frobenius is a generalization of each of the two prior solution techniques, and if not, what its relationship is to those techniques.  To break this down: Does applying The Method Of Frobenius to an ODE at an ordinary point always produce $s = 0$ with algebraic multiplicity equal to the order of the ODE? Does applying The Method Of Frobenius to an ODE and finding that $s = 0$ with algebraic multiplicity equal to the order of the ODE entail that the point approximated around was ordinary? Does applying The Method Of Frobenius to an Euler ODE always produce a power series factor $\sum_{n = 0}^\infty a_n x^n$ in the guess identically equal to $1$ ? Does applying The Method Of Frobenius to an ODE and finding that the power series factor $\sum_{n = 0}^\infty a_n x^n$ in the guess is identically equal to $1$ entail that the ODE was an Euler ODE? If the answers are not all ""yes,"" then The Method Of Frobenius is not a generalization of each of the other solution techniques.  In this case, what is the correct, high-level way to think about how these solution techniques relate, aside from symbolic similarity?","When solving an ODE via a power series at an ordinary (nonsingular) point, the initial guess is . When solving an Euler ODE, the second order equation where and are constant coefficients for example, the initial guess is . When solving an ODE via The Method Of Frobenius at a regular singular point, the initial guess is said to be the product of the two prior guesses, . My question is whether The Method Of Frobenius is a generalization of each of the two prior solution techniques, and if not, what its relationship is to those techniques.  To break this down: Does applying The Method Of Frobenius to an ODE at an ordinary point always produce with algebraic multiplicity equal to the order of the ODE? Does applying The Method Of Frobenius to an ODE and finding that with algebraic multiplicity equal to the order of the ODE entail that the point approximated around was ordinary? Does applying The Method Of Frobenius to an Euler ODE always produce a power series factor in the guess identically equal to ? Does applying The Method Of Frobenius to an ODE and finding that the power series factor in the guess is identically equal to entail that the ODE was an Euler ODE? If the answers are not all ""yes,"" then The Method Of Frobenius is not a generalization of each of the other solution techniques.  In this case, what is the correct, high-level way to think about how these solution techniques relate, aside from symbolic similarity?",y = \sum_{n = 0}^\infty a_n x^n x^2 y'' + pxy' + qy = 0 p q y = x^s y = x^s \sum_{n = 0}^\infty a_n x^n s = 0 s = 0 \sum_{n = 0}^\infty a_n x^n 1 \sum_{n = 0}^\infty a_n x^n 1,"['ordinary-differential-equations', 'power-series', 'approximation', 'localization', 'frobenius-method']"
71,"If an ODE has only periodic solutions and one equilibrium point, is that equilibrium point Lyapunov stable?","If an ODE has only periodic solutions and one equilibrium point, is that equilibrium point Lyapunov stable?",,"Consider the non-linear IVP $$\dot x=f(x),$$ $$x(0)=x_0.$$ where $f$ is locally Lipschitz and $$f(0)=0$$ and $$f(x)\ne 0\ \forall x\ne 0.$$ If all solutions to this IVP for various initial conditions are periodic does that mean that the origin is Lyapunov stable, i.e., $$\forall \varepsilon>0, \ \exists\delta>0, \ |x(0)|<\delta\implies|x(t)|<\varepsilon \ \forall t\ge 0.$$ My professor said yes in class and he ""justified"" this by drawing periodic orbits that are circular. I know that periodic solutions are bounded, but what if a solution close starts very close to the origin, moves far away from the origin, then returns to the same point? Is there an example of such a system or is my professor right? I was thinking of a solution like $$x(t)=\begin{cases}\frac{x(0)}{|x(0)|}\sin(t)+x(0)(1-\sin(t))&x\ne 0\\0&x=0\end{cases}.$$ $x(t)$ oscillates between $x(0)$ and a point with norm 1, so no matter how small $\delta$ is, the solution will never satisfy the definition of Lyapunov stability with $\varepsilon=\frac{1}{2}$ . The origin is an equilibrium point and the only equilibrium point and $x(t)$ satisfies the initial condition. $x(t)$ is differentiable, $2\pi$ periodic and satisfies the time-invariance property. I know that $x(t)$ does not work because it is not continuous with respect to the initial condition and $f$ is locally Lipschitz but I'm wondering if there is a way to alter $x(t)$ to make it a solution to an autonomous ODE and still have the same properties.","Consider the non-linear IVP where is locally Lipschitz and and If all solutions to this IVP for various initial conditions are periodic does that mean that the origin is Lyapunov stable, i.e., My professor said yes in class and he ""justified"" this by drawing periodic orbits that are circular. I know that periodic solutions are bounded, but what if a solution close starts very close to the origin, moves far away from the origin, then returns to the same point? Is there an example of such a system or is my professor right? I was thinking of a solution like oscillates between and a point with norm 1, so no matter how small is, the solution will never satisfy the definition of Lyapunov stability with . The origin is an equilibrium point and the only equilibrium point and satisfies the initial condition. is differentiable, periodic and satisfies the time-invariance property. I know that does not work because it is not continuous with respect to the initial condition and is locally Lipschitz but I'm wondering if there is a way to alter to make it a solution to an autonomous ODE and still have the same properties.","\dot x=f(x), x(0)=x_0. f f(0)=0 f(x)\ne 0\ \forall x\ne 0. \forall \varepsilon>0, \ \exists\delta>0, \ |x(0)|<\delta\implies|x(t)|<\varepsilon \ \forall t\ge 0. x(t)=\begin{cases}\frac{x(0)}{|x(0)|}\sin(t)+x(0)(1-\sin(t))&x\ne 0\\0&x=0\end{cases}. x(t) x(0) \delta \varepsilon=\frac{1}{2} x(t) x(t) 2\pi x(t) f x(t)","['ordinary-differential-equations', 'periodic-functions', 'stability-in-odes']"
72,Derivation of $\frac{du}{dt}$ from The Chemical Basis of Morphogenesis by A. Turing,Derivation of  from The Chemical Basis of Morphogenesis by A. Turing,\frac{du}{dt},"I am currently reading The Chemical Basis of Morphogenesis by A. Turing. On page $56$ of his article, Turing uses the substitution \begin{align} \xi&=b(u+v), \\ \eta&=(p-a')u+(p'-a')v, \end{align} to transform \begin{align} \frac{d\xi}{dt}&=a'\xi+b\eta+R_1(t),\\ \frac{d\eta}{dt}&=c\xi+d'\eta+R_2(t), \end{align} into $$\frac{du}{dt}=pu+\frac{p'-a'}{(p'-p)b}R_1(t)-\frac{R_2(t)}{p'-p}+\xi\frac{d}{dt}\left(\frac{p'-a'}{(p'-p)b}\right)-\eta\frac{d}{dt}\left(\frac{1}{p'-p}\right).$$ Here, $p$ and $p'$ are the (real) roots of $(p-a')(p-d')=bc$ . I am seeking advice/hints on how I can derive this final expression.","I am currently reading The Chemical Basis of Morphogenesis by A. Turing. On page of his article, Turing uses the substitution to transform into Here, and are the (real) roots of . I am seeking advice/hints on how I can derive this final expression.","56 \begin{align}
\xi&=b(u+v), \\
\eta&=(p-a')u+(p'-a')v,
\end{align} \begin{align}
\frac{d\xi}{dt}&=a'\xi+b\eta+R_1(t),\\
\frac{d\eta}{dt}&=c\xi+d'\eta+R_2(t),
\end{align} \frac{du}{dt}=pu+\frac{p'-a'}{(p'-p)b}R_1(t)-\frac{R_2(t)}{p'-p}+\xi\frac{d}{dt}\left(\frac{p'-a'}{(p'-p)b}\right)-\eta\frac{d}{dt}\left(\frac{1}{p'-p}\right). p p' (p-a')(p-d')=bc","['ordinary-differential-equations', 'proof-writing']"
73,Neural ODE definition of derivative $\frac{d L}{dz(t)}$ (adjoint),Neural ODE definition of derivative  (adjoint),\frac{d L}{dz(t)},"The authors of Neural Ordinary Differential Equations (NeuRIPS 2018 best paper award), propose to model machine learning problems with an ODE $$ \dot z(t) = f(t, z(t), \theta) \quad{\text{s.t.}}\quad z(t_0)=z_0$$ where $f$ is e.g. a neural network with parameters $\theta$ . The Gradient with respect to a loss function $L$ is computed via the adjoint method: $$\begin{aligned} \frac{d L}{d\theta} = -\int_{t_1}^{t_0}a(t)^T\frac{\partial f}{\partial \theta} d t \quad\text{where}\quad \dot a(t) = -a(t)^T\frac{\partial f}{\partial z},\, a(t_1)=\frac{dL}{dz(t_1)} \end{aligned}$$ I have a problem with interpreting the adjoint state $a(t)$ . The paper claims that the adjoint state is given by $a(t)=\frac{dL}{dz(t)}$ . However, let's consider the simple example $\,\dot z = w z,\, z(t_0)=z_0$ with solution $\hat z(t) := z^*(t;t_0, z_0, w) = e^{w (t-t_0)}z_0$ and loss function $L(w)=\frac{1}{2}|\hat z(t_1) - z_1|^2$ . Then, solving the adjoint system yields: $$\left.\begin{aligned} \dot a(t) &= - a(t)^T \tfrac{\partial f}{\partial z} = -w \cdot  a(t)  \\ a(t_1) &= \frac{d L }{d z(t_1)} = \hat z(t_1) - z_1 \end{aligned} \right\rbrace\implies a(t) = e^{-w(t-t_1)}(\hat z(t_1) - z_1) $$ I veried that this is indeed correct, integrating $-\int_{t_1}^{t_0} a(t)\frac{\partial f}{\partial\theta}dt$ gives the correct derivative $\frac{dL}{dw}$ . Plugging back the solution $\hat z$ of the ODE yields $a(t)=e^{-w(t-t_1)}(e^{w (t_1-t_0)}z_0 - z_1)$ (I think I got all the signs correct here.) However, I do not see how to mechanistically form the derivative $\frac{d L}{dz(t)}$ directly and arrive at the same result. If we interpret $\frac{dL}{dz(t)}$ as $\frac{d}{dz}(\frac{1}{2}|z-z_1|^2)\big|_{z=\hat z(t)}$ we get $\hat z(t)-z_1$ , which is unequal to $a(t)$ . So how is $\frac{dL}{dz(t)}$ formally defined, such that it ends up equal ? The paper never explains this. Secondly, I would like to know if there is a more direct way to derive the adjoint equation. I am aware of the author's derivation in the appendix as well as the derivation via Pontryagin's maximum principle . The first one appears unmotived whilst the second one for some reason requires the consideration of an optimization problem, when all we want to do is compute a derivative.","The authors of Neural Ordinary Differential Equations (NeuRIPS 2018 best paper award), propose to model machine learning problems with an ODE where is e.g. a neural network with parameters . The Gradient with respect to a loss function is computed via the adjoint method: I have a problem with interpreting the adjoint state . The paper claims that the adjoint state is given by . However, let's consider the simple example with solution and loss function . Then, solving the adjoint system yields: I veried that this is indeed correct, integrating gives the correct derivative . Plugging back the solution of the ODE yields (I think I got all the signs correct here.) However, I do not see how to mechanistically form the derivative directly and arrive at the same result. If we interpret as we get , which is unequal to . So how is formally defined, such that it ends up equal ? The paper never explains this. Secondly, I would like to know if there is a more direct way to derive the adjoint equation. I am aware of the author's derivation in the appendix as well as the derivation via Pontryagin's maximum principle . The first one appears unmotived whilst the second one for some reason requires the consideration of an optimization problem, when all we want to do is compute a derivative."," \dot z(t) = f(t, z(t), \theta) \quad{\text{s.t.}}\quad z(t_0)=z_0 f \theta L \begin{aligned}
\frac{d L}{d\theta} = -\int_{t_1}^{t_0}a(t)^T\frac{\partial f}{\partial \theta} d t
\quad\text{where}\quad
\dot a(t) = -a(t)^T\frac{\partial f}{\partial z},\,
a(t_1)=\frac{dL}{dz(t_1)}
\end{aligned} a(t) a(t)=\frac{dL}{dz(t)} \,\dot z = w z,\, z(t_0)=z_0 \hat z(t) := z^*(t;t_0, z_0, w) = e^{w (t-t_0)}z_0 L(w)=\frac{1}{2}|\hat z(t_1) - z_1|^2 \left.\begin{aligned}
\dot a(t) &= - a(t)^T \tfrac{\partial f}{\partial z} = -w \cdot  a(t) 
\\ a(t_1) &= \frac{d L }{d z(t_1)} = \hat z(t_1) - z_1
\end{aligned}
\right\rbrace\implies a(t) = e^{-w(t-t_1)}(\hat z(t_1) - z_1)
 -\int_{t_1}^{t_0} a(t)\frac{\partial f}{\partial\theta}dt \frac{dL}{dw} \hat z a(t)=e^{-w(t-t_1)}(e^{w (t_1-t_0)}z_0 - z_1) \frac{d L}{dz(t)} \frac{dL}{dz(t)} \frac{d}{dz}(\frac{1}{2}|z-z_1|^2)\big|_{z=\hat z(t)} \hat z(t)-z_1 a(t) \frac{dL}{dz(t)}","['ordinary-differential-equations', 'chain-rule', 'neural-networks']"
74,Solutions of $y'-2y=1$,Solutions of,y'-2y=1,"I claim the solution of $y'-2y=1$ , is $$\phi(x)=e^{2x}\int_{x_0}^xe^{-2t}dt +ce^{2x}=e^{2x}\left(-\frac{1}{2}e^{-2x}\right)+ce^{2x}=-\frac{1}{2}+ce^{2x}$$ where $x_0$ is a fixed point on the interval where the function $b(x)=1$ is continuous. Apparently the correct solution is $-\frac{1}{2}+ce^{-2x}$ . A second question is, why can I evaluate the integral above and ignore the term which involves $\frac{1}{2}e^{-2x_0}$ after applying FTC? Somehow it vanishes, and this integral is basically just the primitive evaluated at $x$ .","I claim the solution of , is where is a fixed point on the interval where the function is continuous. Apparently the correct solution is . A second question is, why can I evaluate the integral above and ignore the term which involves after applying FTC? Somehow it vanishes, and this integral is basically just the primitive evaluated at .",y'-2y=1 \phi(x)=e^{2x}\int_{x_0}^xe^{-2t}dt +ce^{2x}=e^{2x}\left(-\frac{1}{2}e^{-2x}\right)+ce^{2x}=-\frac{1}{2}+ce^{2x} x_0 b(x)=1 -\frac{1}{2}+ce^{-2x} \frac{1}{2}e^{-2x_0} x,"['calculus', 'ordinary-differential-equations']"
75,Showing a solution to this differential equation is never zero,Showing a solution to this differential equation is never zero,,"Say I want to find a differentiable function $f: \mathbb{R} \rightarrow \mathbb{R}$ such that for some $k \neq 0$ , $f' = kf$ . Every time I have seen a book solve this, the expression is rewritten  as $$\frac{1}{f}f' = k$$ and then solved using u-substitution. To be technical though, must one first know that $f$ is never zero? If so, how could I go about showing that $f$ is never zero? I know that $f$ must be infinitely differentiable and if for some $a \in \mathbb{R}$ , $f(a) = 0$ , then for each $n \in \mathbb{N}_0$ , $f^{(n)}(a) = 0$ . I think I remember from complex analysis, if $f$ is holomorphic with this property, then $f$ is zero everywhere, but I'm not sure about the usual case of $\mathbb{R} \rightarrow \mathbb{R}$ .","Say I want to find a differentiable function such that for some , . Every time I have seen a book solve this, the expression is rewritten  as and then solved using u-substitution. To be technical though, must one first know that is never zero? If so, how could I go about showing that is never zero? I know that must be infinitely differentiable and if for some , , then for each , . I think I remember from complex analysis, if is holomorphic with this property, then is zero everywhere, but I'm not sure about the usual case of .",f: \mathbb{R} \rightarrow \mathbb{R} k \neq 0 f' = kf \frac{1}{f}f' = k f f f a \in \mathbb{R} f(a) = 0 n \in \mathbb{N}_0 f^{(n)}(a) = 0 f f \mathbb{R} \rightarrow \mathbb{R},"['real-analysis', 'calculus', 'ordinary-differential-equations']"
76,Existence of a global solution to a matrix Riccati differential equation - indefinite & constant terms?,Existence of a global solution to a matrix Riccati differential equation - indefinite & constant terms?,,"Consider the symmetric matrix Riccati differential equation : $$ P'(t) = Q + X^\intercal P(t) +  P(t) X +  P(t)UP(t) $$ Where everything is symmetric and the coefficients are constant (independent of time). The quadratic term U and the term Q are not definite, instead have a mix of positive and negative eigenvalues (say half half), in different bases. If it can help, U can be written as : $$ U = -U_1+\gamma U_2 $$ where $U_1$ is symmetric singular and $U_2$ symmetric semidefinite positive. Is it possible to tackle the issue of finding conditions for boundedness of P ?","Consider the symmetric matrix Riccati differential equation : Where everything is symmetric and the coefficients are constant (independent of time). The quadratic term U and the term Q are not definite, instead have a mix of positive and negative eigenvalues (say half half), in different bases. If it can help, U can be written as : where is symmetric singular and symmetric semidefinite positive. Is it possible to tackle the issue of finding conditions for boundedness of P ?","
P'(t) = Q + X^\intercal P(t) +  P(t) X +  P(t)UP(t)
 
U = -U_1+\gamma U_2
 U_1 U_2","['ordinary-differential-equations', 'matrix-equations', 'control-theory', 'optimal-control']"
77,Solution differential equation $\frac{d^2x}{dt^2}=ax+bx^3$,Solution differential equation,\frac{d^2x}{dt^2}=ax+bx^3,"I'm trying to solve the following differential equation: $$\frac{d^2x}{dt^2}=ax+bx^3$$ I tried the following: $$\frac{d^2x}{ax+bx^3}=dt^2$$ But I'm not sure how to continue. Can I use $d^2x$ the same as $dx^2$ and use partial fraction decomposition to work out the left side of the equation? Like this: $$ \begin{split} \iint{\frac{1}{ax+bx^3}d^2x}&=\iint{dt^2}\\ \frac{1}{b}\iint{\frac{A}{x}+\frac{B}{x+\sqrt{\frac{a}{b}}}+\frac{C}{x-\sqrt{\frac{a}{b}}}d^2x} &= \frac{t^2}{2}+c_1t+c_2 \end{split} $$ But am I using $d^2x$ correctly here? If not, how can I solve this equation?","I'm trying to solve the following differential equation: I tried the following: But I'm not sure how to continue. Can I use the same as and use partial fraction decomposition to work out the left side of the equation? Like this: But am I using correctly here? If not, how can I solve this equation?","\frac{d^2x}{dt^2}=ax+bx^3 \frac{d^2x}{ax+bx^3}=dt^2 d^2x dx^2 
\begin{split}
\iint{\frac{1}{ax+bx^3}d^2x}&=\iint{dt^2}\\
\frac{1}{b}\iint{\frac{A}{x}+\frac{B}{x+\sqrt{\frac{a}{b}}}+\frac{C}{x-\sqrt{\frac{a}{b}}}d^2x} &= \frac{t^2}{2}+c_1t+c_2
\end{split}
 d^2x",['ordinary-differential-equations']
78,Wronskian type of equation,Wronskian type of equation,,"I am reviewing some old notes on dynamical system and came across a result that reminds me to the Wronskian equation except that here we are dealing with a nonlinear equation: Let $\phi(t;{\bf x})$ be a solution to the equation $\dot{{\bf x}}(t)= f(t,{\bf x}(t))$ , with $\phi(0;{\bf x})={\bf x}$ . Define the function $W$ by $$ \begin{align} W(t,{\bf x})&=\det\left[\frac{\partial \phi}{\partial {\bf x}}(t;{\bf x})\right]. \end{align} $$ Then, $W$ satisfies the differential equation $$ \dot{W}(t)=W(t)\, (\nabla_{\bf x}\cdot f)(t,\phi(t;\mathbf{x})); \qquad W(0)=1, $$ where $\left(\nabla_{\bf x}\cdot f\right)(t,\phi(t;{\bf x})) =\sum_{j=1}^n \frac{\partial f}{\partial x_j}(t,\phi(t;{\bf x}))$ I am trying to prove this result but I am completely at odds. Any hints or a sketch of a solution will be appreciated.","I am reviewing some old notes on dynamical system and came across a result that reminds me to the Wronskian equation except that here we are dealing with a nonlinear equation: Let be a solution to the equation , with . Define the function by Then, satisfies the differential equation where I am trying to prove this result but I am completely at odds. Any hints or a sketch of a solution will be appreciated.","\phi(t;{\bf x}) \dot{{\bf x}}(t)= f(t,{\bf x}(t)) \phi(0;{\bf x})={\bf x} W 
\begin{align}
W(t,{\bf x})&=\det\left[\frac{\partial \phi}{\partial {\bf x}}(t;{\bf x})\right].
\end{align}
 W 
\dot{W}(t)=W(t)\, (\nabla_{\bf x}\cdot f)(t,\phi(t;\mathbf{x})); \qquad W(0)=1,
 \left(\nabla_{\bf x}\cdot f\right)(t,\phi(t;{\bf x})) =\sum_{j=1}^n \frac{\partial f}{\partial x_j}(t,\phi(t;{\bf x}))","['ordinary-differential-equations', 'dynamical-systems']"
79,Multiplying linear differential operators,Multiplying linear differential operators,,"This seems like such a simple thing but I can't get it to work. I have two linear differential operators $$ L_1 = x\text D - 2 \text{ and } L_2 = (x+2)\text D + (x+1) $$ and a function $$ f(x) = a x^2 + b\exp(-x) $$ Now, I have that $$ L_1 f(x) = g(x) = -b(x+2)\exp(-x) $$ and that $$ L_2 g(x) = 0 $$ so reasonably, I should have $$ L_2 L_1 f(x) = Lf(x) = 0 $$ but when I multiply them, I get $$ L = (x^2+2x)\text D^2 + (x^2 - x - 4)\text D - (2x+2) $$ and I find that $$ Lf(x) = -(x+2)(2ax - b\exp(-x)) = -(x+2)f'(x) \neq 0 $$ I was under the impression that linear differential operators could be multiplied as polynomials to perform composition, but it doesn't seem to work here. However, for a different set of operators $$ M_1 = \text D+1 \text{ and } M_2 = (x^2+2x)\text D - (2x+2) $$ I get the product as $$ M = (x^2 + 2x)\text D^2 + (x^2 - 2)\text D - (2x+2) $$ and $$ Mf(x) = 0 $$ so it seems to work fine for this set. What's going on here? Why does it work for one but not the other?","This seems like such a simple thing but I can't get it to work. I have two linear differential operators and a function Now, I have that and that so reasonably, I should have but when I multiply them, I get and I find that I was under the impression that linear differential operators could be multiplied as polynomials to perform composition, but it doesn't seem to work here. However, for a different set of operators I get the product as and so it seems to work fine for this set. What's going on here? Why does it work for one but not the other?", L_1 = x\text D - 2 \text{ and } L_2 = (x+2)\text D + (x+1)   f(x) = a x^2 + b\exp(-x)   L_1 f(x) = g(x) = -b(x+2)\exp(-x)   L_2 g(x) = 0   L_2 L_1 f(x) = Lf(x) = 0   L = (x^2+2x)\text D^2 + (x^2 - x - 4)\text D - (2x+2)   Lf(x) = -(x+2)(2ax - b\exp(-x)) = -(x+2)f'(x) \neq 0   M_1 = \text D+1 \text{ and } M_2 = (x^2+2x)\text D - (2x+2)   M = (x^2 + 2x)\text D^2 + (x^2 - 2)\text D - (2x+2)   Mf(x) = 0 ,['ordinary-differential-equations']
80,Problem with lyapunov function,Problem with lyapunov function,,"I have a question about a differential equation I tried to analyse: $$ \begin{align} \dfrac {dx}{dt} &= v \\ \dfrac {dv}{dt} &= -x+x^3-v^3 \\ \end{align} $$ I plotted this differential equation, and it looks like it has a spiral going inward (see picture). Now I tried to prove this: Therfore I searched a liapunov function and I think I found a local one: $$  L = \dfrac 14(2v^2+2x^2-x^4)$$ for $v^2+x^2 > \dfrac { x^4}2 $ and $\dfrac {dL}{dt} = -v^4$ Now if I understand it right, the fixed point in the middle should attract all trajectories in the region where the lyapunov function is defined: $v^2+x^2 > x^4/2$ But if I compare the ‚region of attraction‘ of the lyapunov function with the computer simulation, it does not fit together. Can someone help me and tell me what I am doing wrong? I appreciate every help! Region for Lyapunov function Simulation with pplane","I have a question about a differential equation I tried to analyse: I plotted this differential equation, and it looks like it has a spiral going inward (see picture). Now I tried to prove this: Therfore I searched a liapunov function and I think I found a local one: for and Now if I understand it right, the fixed point in the middle should attract all trajectories in the region where the lyapunov function is defined: But if I compare the ‚region of attraction‘ of the lyapunov function with the computer simulation, it does not fit together. Can someone help me and tell me what I am doing wrong? I appreciate every help! Region for Lyapunov function Simulation with pplane","
\begin{align}
\dfrac {dx}{dt} &= v \\
\dfrac {dv}{dt} &= -x+x^3-v^3 \\
\end{align}
   L = \dfrac 14(2v^2+2x^2-x^4) v^2+x^2 > \dfrac { x^4}2  \dfrac {dL}{dt} = -v^4 v^2+x^2 > x^4/2","['ordinary-differential-equations', 'nonlinear-dynamics', 'lyapunov-functions']"
81,How to solve $\frac{dx}{dt} = ax^2 + bx + c$?,How to solve ?,\frac{dx}{dt} = ax^2 + bx + c,"It's been a while since I've solved ODEs analytically, but this looks like a first order nonlinear ODE that is separable. When I separate the terms, I see \begin{align}     \frac{1}{ax^2 + bx + c}dx = dt \\ \end{align} The LHS it'll be quite difficult to evaluate. Is there a simple solution to this problem that I am not seeing?","It's been a while since I've solved ODEs analytically, but this looks like a first order nonlinear ODE that is separable. When I separate the terms, I see The LHS it'll be quite difficult to evaluate. Is there a simple solution to this problem that I am not seeing?","\begin{align}
    \frac{1}{ax^2 + bx + c}dx = dt \\
\end{align}",['ordinary-differential-equations']
82,Why we can not define the solution of the differential equation beyond a particular $t$? [duplicate],Why we can not define the solution of the differential equation beyond a particular ? [duplicate],t,"This question already has an answer here : Why can't a union of two intervals be the maximum existence interval of a solution? (1 answer) Closed 3 years ago . I was studying Ordinary Differential Equations, and my book was trying to explain the maximal interval of solution of a differential equation. If we consider this differential equation $$ \frac{dx}{dt} = x^2 \\ \text{with initial condition}~x(0)= a,~~~a \gt 0 $$ Then, the solution to this equation is $$ x(t) = \frac{1}{ a^{-1} - t}$$ Now, if $t$ starts to increase from $0$ the denominator will decrease and consequently $x(t)$ will increase, when $t =a^{-1}$ the denominator is $0$ and $x(t) = + \infty$ . As we let $t$ to decrease from $0$ , our denominator will increase and finally when $t = -\infty$ , $x(t) = 0$ . So, the solution of the differential equation is defined on the interval $(-\infty, a^{-1})$ . But the problem comes when the book makes this statement but there is no way to define the solution that extends further into the future beyond $t=a^{-1}$ . Why the function is not defined for $t \gt a^{-1}$ ? There is a discontinuity at $t=a^{-1}$ but beyond that the function is nice , why the solution is not defined after $t = a^{-1}$ ? If compare $x(t)$ with some other simple functions like, for example, $f(x) = \frac{1}{2-x}$ , $f(x)$ is well defined after $x = 2$ , and here we have its graph: . Why the book says that beyond $t=a^{-1}$ the solution is not defined?","This question already has an answer here : Why can't a union of two intervals be the maximum existence interval of a solution? (1 answer) Closed 3 years ago . I was studying Ordinary Differential Equations, and my book was trying to explain the maximal interval of solution of a differential equation. If we consider this differential equation Then, the solution to this equation is Now, if starts to increase from the denominator will decrease and consequently will increase, when the denominator is and . As we let to decrease from , our denominator will increase and finally when , . So, the solution of the differential equation is defined on the interval . But the problem comes when the book makes this statement but there is no way to define the solution that extends further into the future beyond . Why the function is not defined for ? There is a discontinuity at but beyond that the function is nice , why the solution is not defined after ? If compare with some other simple functions like, for example, , is well defined after , and here we have its graph: . Why the book says that beyond the solution is not defined?","
\frac{dx}{dt} = x^2 \\
\text{with initial condition}~x(0)= a,~~~a \gt 0
 
x(t) = \frac{1}{ a^{-1} - t} t 0 x(t) t =a^{-1} 0 x(t) = + \infty t 0 t = -\infty x(t) = 0 (-\infty, a^{-1}) t=a^{-1} t \gt a^{-1} t=a^{-1} t = a^{-1} x(t) f(x) = \frac{1}{2-x} f(x) x = 2 t=a^{-1}","['ordinary-differential-equations', 'functions']"
83,Galerkin method for nonlinear ode,Galerkin method for nonlinear ode,,"I'm trying to solve the following differential equation: $$\frac{d^2u}{dx^2}=\frac{du}{dx}u+u^2+x$$ $$x \in \Omega=[0,1]$$ $$BCS:u|_{x=0}=1;\frac{du}{dx}|_{x=1}=1$$ You can see that the right side contains $u^2$ . So when I paste it in the weighted residual form, I get nonlinear term. For example, if I have approximation: $$ u=1+\sum_{i=1}^n\alpha_i x^i$$ There will be nonlinear integral in weighted residuals $$\int (1+\sum_{i=1}^n\alpha_i x^i)^2dx$$ That's why the system will be nonlinear. What am I missing? I tried to switch from $u$ to $u^2$ in equation because $u\frac{du}{dx}=\frac{1}{2}\frac{du^2}{dx}$ , but can't make it for $\frac{d^2u}{dx^2}$ Edit, according to the answer: I won't write BCS integrals, because they don't make real sense in the question. I'll write only the integral in the main domain. So I have $$\int_0^1w(\frac{d^2u}{dx^2}-\frac{du}{dx}u-u^2-x)dx=0$$ $w-$ weight function. Paste approximation of $u$ . Let's take $n = 2$ $$\int_0^1w(2\alpha_2-(\alpha_1 + 2\alpha_2 x)(1+\alpha_1 x +\alpha_2 x^2)-(1+\alpha_1 x +\alpha_2 x^2)^2-x)dx=0$$ Take in account Bubnov-Galerkin approximation of weight function: $$ w=\beta_1x+\beta_2x^2$$ $$\int_0^1\beta_1x(2\alpha_2-(\alpha_1 + 2\alpha_2 x)(1+\alpha_1 x +\alpha_2 x^2)-(1+\alpha_1 x +\alpha_2 x^2)^2-x)dx +\int_0^1\beta_2x^2(2\alpha_2-(\alpha_1 + 2\alpha_2 x)(1+\alpha_1 x +\alpha_2 x^2)-(1+\alpha_1 x +\alpha_2 x^2)^2-x)dx=0$$ From here since $\beta_i $ arbitrary we have system $$\begin{cases}  \int_0^1x(2\alpha_2-(\alpha_1 + 2\alpha_2 x)(1+\alpha_1 x +\alpha_2 x^2)-(1+\alpha_1 x +\alpha_2 x^2)^2-x)dx =0\\ \int_0^1x^2(2\alpha_2-(\alpha_1 + 2\alpha_2 x)(1+\alpha_1 x +\alpha_2 x^2)-(1+\alpha_1 x +\alpha_2 x^2)^2-x)dx=0 \end{cases} $$ Here we exactly have unknowns only $\alpha_i;i=1,2$ .But if we extend polynomial to $2n=4$ we will have new $\alpha_i;i=1..4$ with 2 equations only Edit 2: Actually I need two terms approximation, so I don't think that switching to 2n terms and then solving 2n equations is the key point. I suppose we should simplify ode, or choose another interpolation functions rather then $x^i$","I'm trying to solve the following differential equation: You can see that the right side contains . So when I paste it in the weighted residual form, I get nonlinear term. For example, if I have approximation: There will be nonlinear integral in weighted residuals That's why the system will be nonlinear. What am I missing? I tried to switch from to in equation because , but can't make it for Edit, according to the answer: I won't write BCS integrals, because they don't make real sense in the question. I'll write only the integral in the main domain. So I have weight function. Paste approximation of . Let's take Take in account Bubnov-Galerkin approximation of weight function: From here since arbitrary we have system Here we exactly have unknowns only .But if we extend polynomial to we will have new with 2 equations only Edit 2: Actually I need two terms approximation, so I don't think that switching to 2n terms and then solving 2n equations is the key point. I suppose we should simplify ode, or choose another interpolation functions rather then","\frac{d^2u}{dx^2}=\frac{du}{dx}u+u^2+x x \in \Omega=[0,1] BCS:u|_{x=0}=1;\frac{du}{dx}|_{x=1}=1 u^2  u=1+\sum_{i=1}^n\alpha_i x^i \int (1+\sum_{i=1}^n\alpha_i x^i)^2dx u u^2 u\frac{du}{dx}=\frac{1}{2}\frac{du^2}{dx} \frac{d^2u}{dx^2} \int_0^1w(\frac{d^2u}{dx^2}-\frac{du}{dx}u-u^2-x)dx=0 w- u n = 2 \int_0^1w(2\alpha_2-(\alpha_1 + 2\alpha_2 x)(1+\alpha_1 x +\alpha_2 x^2)-(1+\alpha_1 x +\alpha_2 x^2)^2-x)dx=0  w=\beta_1x+\beta_2x^2 \int_0^1\beta_1x(2\alpha_2-(\alpha_1 + 2\alpha_2 x)(1+\alpha_1 x +\alpha_2 x^2)-(1+\alpha_1 x +\alpha_2 x^2)^2-x)dx +\int_0^1\beta_2x^2(2\alpha_2-(\alpha_1 + 2\alpha_2 x)(1+\alpha_1 x +\alpha_2 x^2)-(1+\alpha_1 x +\alpha_2 x^2)^2-x)dx=0 \beta_i  \begin{cases}
 \int_0^1x(2\alpha_2-(\alpha_1 + 2\alpha_2 x)(1+\alpha_1 x +\alpha_2 x^2)-(1+\alpha_1 x +\alpha_2 x^2)^2-x)dx =0\\
\int_0^1x^2(2\alpha_2-(\alpha_1 + 2\alpha_2 x)(1+\alpha_1 x +\alpha_2 x^2)-(1+\alpha_1 x +\alpha_2 x^2)^2-x)dx=0
\end{cases}
 \alpha_i;i=1,2 2n=4 \alpha_i;i=1..4 x^i","['ordinary-differential-equations', 'numerical-methods', 'finite-element-method', 'galerkin-methods']"
84,Help solving $y^{\prime\prime}(1+2\ln(y^\prime)) = 1$,Help solving,y^{\prime\prime}(1+2\ln(y^\prime)) = 1,"Let $p = y^\prime$ , then we get $p^\prime(1+2\ln(p))=1$ , so $x + c_1 = \int(1+2\ln(p))dp = p(2\ln(p)-1)$ . But then I'm stuck because I don't know what to do next.","Let , then we get , so . But then I'm stuck because I don't know what to do next.",p = y^\prime p^\prime(1+2\ln(p))=1 x + c_1 = \int(1+2\ln(p))dp = p(2\ln(p)-1),['ordinary-differential-equations']
85,Question about Euler's Method and the SIR epidemic model using a spreadsheet,Question about Euler's Method and the SIR epidemic model using a spreadsheet,,"I am creating a spreadsheet to figure out how to correctly use Euler's Method with the SIR Model for Spread of Disease. I took a random question from the text Calculus in Context by Callahan and Hoffman. Given that $S(t)$ is the number of susceptible individuals, $I(t)$ is the number of infected individuals, and $R(t)$ is the number of recovered individuals, I am using the following differential equations: $\frac{dS}{dt} = -bSI$ $\frac{dI}{dt} = bSI-kI$ $\frac{dR}{dt} = kI$ So, with Euler's method, I used the following idea to get each successive iteration: $S_{new}=S-(bSI)\Delta t$ $I_{new}=I+(bSI-kI)\Delta t$ $R_{new}=R+(kI)\Delta t$ The particular problem I am working with has the following initial condition: $S(0)=35,400$ $I(0)=13,500$ $R(0)=22,100$ Also, the text set $b = 0.00001$ and $k=0.08$ . I made $\Delta t = 1$ to calculate $S$ , $I$ , and $R$ each day. Here is a screenshot of the spreadsheet showing my results for the first 10 days: Screenshot of the first 10 days Here is what I have entered in the cells Note that $b$ is stored as 0.00001, $k$ is stored as 0.08, and $t$ is the $\Delta t$ , which is stored as 1. In cell B3 I have entered: =B2-b*B2*C2*t In cell C3, I have entered: =C2+(b*B2*C2-k*C2)*t In cell D3, I have entered: D2+k*C2*t For the rest of the cells up to 40 days, I just dragged down the same formulas. So, for B4, C4, and D4, for example, it would be the identical formulas I listed above except with B3, C3 and D3 inside. I am first making sure I have the right idea and am not totally missing something crucial. I am also very interested in relating this to the COVID-19. Correct me if I'm wrong, but I believe $k$ is 1/(average period of infectiousness). I am still confused about how to estimate a $b$ value. I'd like to relate this to the US population, but, if that is too big, maybe another specific state or city. If someone could help me a bit with this piece, that would be great!","I am creating a spreadsheet to figure out how to correctly use Euler's Method with the SIR Model for Spread of Disease. I took a random question from the text Calculus in Context by Callahan and Hoffman. Given that is the number of susceptible individuals, is the number of infected individuals, and is the number of recovered individuals, I am using the following differential equations: So, with Euler's method, I used the following idea to get each successive iteration: The particular problem I am working with has the following initial condition: Also, the text set and . I made to calculate , , and each day. Here is a screenshot of the spreadsheet showing my results for the first 10 days: Screenshot of the first 10 days Here is what I have entered in the cells Note that is stored as 0.00001, is stored as 0.08, and is the , which is stored as 1. In cell B3 I have entered: =B2-b*B2*C2*t In cell C3, I have entered: =C2+(b*B2*C2-k*C2)*t In cell D3, I have entered: D2+k*C2*t For the rest of the cells up to 40 days, I just dragged down the same formulas. So, for B4, C4, and D4, for example, it would be the identical formulas I listed above except with B3, C3 and D3 inside. I am first making sure I have the right idea and am not totally missing something crucial. I am also very interested in relating this to the COVID-19. Correct me if I'm wrong, but I believe is 1/(average period of infectiousness). I am still confused about how to estimate a value. I'd like to relate this to the US population, but, if that is too big, maybe another specific state or city. If someone could help me a bit with this piece, that would be great!","S(t) I(t) R(t) \frac{dS}{dt} = -bSI \frac{dI}{dt} = bSI-kI \frac{dR}{dt} = kI S_{new}=S-(bSI)\Delta t I_{new}=I+(bSI-kI)\Delta t R_{new}=R+(kI)\Delta t S(0)=35,400 I(0)=13,500 R(0)=22,100 b = 0.00001 k=0.08 \Delta t = 1 S I R b k t \Delta t k b","['calculus', 'ordinary-differential-equations']"
86,Solving $y''=\lambda y$,Solving,y''=\lambda y,"""Show that all solutions of $y''(x)=\lambda y(x)$ on $0\leqslant x \leqslant L$ with $y(0)=y(L)=0$ are of the form $c\sin\left(\frac{k\pi}{L}\right)x.$ (Hint: write down all solutions of the o.d.e and impose boundary conditions.)"" Here is an image from the textbook I'm using describing this problem. I'm not sure what the hint means; what entails ""writing down all solutions""? I'm not sure what method I should use to begin approaching this problem.","""Show that all solutions of on with are of the form (Hint: write down all solutions of the o.d.e and impose boundary conditions.)"" Here is an image from the textbook I'm using describing this problem. I'm not sure what the hint means; what entails ""writing down all solutions""? I'm not sure what method I should use to begin approaching this problem.",y''(x)=\lambda y(x) 0\leqslant x \leqslant L y(0)=y(L)=0 c\sin\left(\frac{k\pi}{L}\right)x.,['real-analysis']
87,"Solving Differential Equation $(1+xy) y + (1-xy) x \frac{\,dy}{\,dx} = 0$",Solving Differential Equation,"(1+xy) y + (1-xy) x \frac{\,dy}{\,dx} = 0","I was solving this question on differential equations: Solve the differential equation: $$(1+xy) y + (1-xy) x \frac{\,dy}{\,dx} = 0$$ I tried the problem by the following: $$(1+xy)y\,dx + (1-xy)\,dy=0$$ $$y\,dx+x\,dy = x^2y\,dy - y^2x\,dx$$ $$2\,d(xy) = x^2{\,d \left(\frac{y^2}{x^2}\right)}$$ $$2\frac {\,d(xy)}{xy} = \frac{\,d \left(\frac{y^2}{x^2}\right)}{\sqrt{\frac{y^2}{x^2}}}$$ $$2ln(xy) = 2\frac{y}{x} +C$$ $$$$ But the answer given was : $ln \left( \frac{x}{y} \right) - \frac{1}{xy} = C $ I don't see anything I did wrong...... Could anyone share how to do this problem or where I went wrong? Thanks EDIT: I found out what I did wrong thanks to an answer by @J.G. NOw I just wish to know the various methods to this question.",I was solving this question on differential equations: Solve the differential equation: I tried the problem by the following: But the answer given was : I don't see anything I did wrong...... Could anyone share how to do this problem or where I went wrong? Thanks EDIT: I found out what I did wrong thanks to an answer by @J.G. NOw I just wish to know the various methods to this question.,"(1+xy) y + (1-xy) x \frac{\,dy}{\,dx} = 0 (1+xy)y\,dx + (1-xy)\,dy=0 y\,dx+x\,dy = x^2y\,dy - y^2x\,dx 2\,d(xy) = x^2{\,d \left(\frac{y^2}{x^2}\right)} 2\frac {\,d(xy)}{xy} = \frac{\,d \left(\frac{y^2}{x^2}\right)}{\sqrt{\frac{y^2}{x^2}}} 2ln(xy) = 2\frac{y}{x} +C  ln \left( \frac{x}{y} \right) - \frac{1}{xy} = C ",['ordinary-differential-equations']
88,Finding Radius and period time of a limit cycle using Melnikov Integration,Finding Radius and period time of a limit cycle using Melnikov Integration,,"The system of equations I am working on is the following: $$ \begin{align} \dot{x}& = y \\ \dot{y}& = -\mu(x^2 + ax^4 - 1)y - x \end{align} $$ The question asks first to find the Hamiltonian of the system at $\mu = 0$ which I evaluate to be: $$ \begin{equation} H(x,y) = \frac{y^2 - x^2}{2} \end{equation} $$ The proceeding parts are the ones I am having trouble with. This is the body of the question: ""Find an explicit expression for the trajectories solving this Hamiltonian system. Choose suitable initial conditions having in mind that in the next step you will integrate over a closed orbit."" Is this done by solving the system of equations when $ \mu = 0$ ? Doing so with initial conditions x(0) = 1 and y(0) = 1 results in: $$ \begin{align} x(t)& = cos(t) + sin(t)\\ y(t)& = cos(t) - sin(t) \end{align} $$ After plugging this into the Hamiltonian and simplifying I arrive at: $$ \begin{equation} H(x,y) = -sin(2t) \end{equation} $$ The next part which involves using the Melnikov method is as follows: ""Now consider a small positive $\mu$ . To determine the radius R and period time T to lowest (zeroth) order in $\mu$ , evaluate the change $\Delta H$ in the Hamiltonian H(x; y) as you follow a trajectory governed by the dynamics in the system of equations. Show that H must vanish after following a limit-cycle trajectory one lap. Use this fact to find the radius R as a function of a."" When H completes one lap I assume that it goes from 0 to $\pi$ and $H = 0$ at $t = \pi$ . I am not sure how to find the radius and the period after this step. For example, I don't know how to handle $\mu$ being non-zero. Could someone give me some pointers as to how this should be done or if what I have done so far is correct?","The system of equations I am working on is the following: The question asks first to find the Hamiltonian of the system at which I evaluate to be: The proceeding parts are the ones I am having trouble with. This is the body of the question: ""Find an explicit expression for the trajectories solving this Hamiltonian system. Choose suitable initial conditions having in mind that in the next step you will integrate over a closed orbit."" Is this done by solving the system of equations when ? Doing so with initial conditions x(0) = 1 and y(0) = 1 results in: After plugging this into the Hamiltonian and simplifying I arrive at: The next part which involves using the Melnikov method is as follows: ""Now consider a small positive . To determine the radius R and period time T to lowest (zeroth) order in , evaluate the change in the Hamiltonian H(x; y) as you follow a trajectory governed by the dynamics in the system of equations. Show that H must vanish after following a limit-cycle trajectory one lap. Use this fact to find the radius R as a function of a."" When H completes one lap I assume that it goes from 0 to and at . I am not sure how to find the radius and the period after this step. For example, I don't know how to handle being non-zero. Could someone give me some pointers as to how this should be done or if what I have done so far is correct?","
\begin{align}
\dot{x}& = y \\
\dot{y}& = -\mu(x^2 + ax^4 - 1)y - x
\end{align}
 \mu = 0 
\begin{equation}
H(x,y) = \frac{y^2 - x^2}{2}
\end{equation}
  \mu = 0 
\begin{align}
x(t)& = cos(t) + sin(t)\\
y(t)& = cos(t) - sin(t)
\end{align}
 
\begin{equation}
H(x,y) = -sin(2t)
\end{equation}
 \mu \mu \Delta H \pi H = 0 t = \pi \mu","['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system']"
89,Radial eigenfunctions of the Laplace operator in spherical coordinates,Radial eigenfunctions of the Laplace operator in spherical coordinates,,"I am trying to find solutions for the following ODE (which was derived trying to find the fundamental solution of the PDE $\Delta u+cu=0$ where $c > 0$ see below for the approach) The ODE is $v''(r)+\frac{2}{r} v'(r)+cv(r)=0$ I am not really sure how to solve this. I haven't done a computational ODE course, and I did look around but couldn't find any resource which tells a way to approach such problems clearly. Any help or at least a hint or a resource would be appreciated. As for solving the PDE, I was looking for radial solutions and thats how I arrived at the ODE. (Mainly following the Laplace Equation method from Evans). Please let me know if there is anything wrong with this approach too Thank You","I am trying to find solutions for the following ODE (which was derived trying to find the fundamental solution of the PDE where see below for the approach) The ODE is I am not really sure how to solve this. I haven't done a computational ODE course, and I did look around but couldn't find any resource which tells a way to approach such problems clearly. Any help or at least a hint or a resource would be appreciated. As for solving the PDE, I was looking for radial solutions and thats how I arrived at the ODE. (Mainly following the Laplace Equation method from Evans). Please let me know if there is anything wrong with this approach too Thank You",\Delta u+cu=0 c > 0 v''(r)+\frac{2}{r} v'(r)+cv(r)=0,"['ordinary-differential-equations', 'partial-differential-equations', 'spherical-coordinates']"
90,Fourier transform (The 1-D neutron diffusion equation),Fourier transform (The 1-D neutron diffusion equation),,"The $1-D$ neutron diffusion equation with a (plane) source is $-D\frac{\mathrm{d^{2}}\varphi (x) }{\mathrm{d} x^{2}}+K^{2}D\varphi (x)=Q\delta (x)$ where $\varphi (x)$ is the neutron flux, $Q\delta (x)$ is the (plane) source at $x = 0$ and $D$ and $K^2$ are constants. Apply a Fourier transform. Solve the equation in transform space. Transform your solution back into x-space. ANS : $\varphi (x)=\frac{Q}{2KD}e^{-|Kx|}$","The neutron diffusion equation with a (plane) source is where is the neutron flux, is the (plane) source at and and are constants. Apply a Fourier transform. Solve the equation in transform space. Transform your solution back into x-space. ANS :",1-D -D\frac{\mathrm{d^{2}}\varphi (x) }{\mathrm{d} x^{2}}+K^{2}D\varphi (x)=Q\delta (x) \varphi (x) Q\delta (x) x = 0 D K^2 \varphi (x)=\frac{Q}{2KD}e^{-|Kx|},"['ordinary-differential-equations', 'fourier-transform']"
91,Solve the equation $y=2xy'+y^2y'^3$,Solve the equation,y=2xy'+y^2y'^3,Solve the equation $y=2xy'+y^2y'^3$ . I have tried to solve it but I can't recognize what type of equation I have learnt.,Solve the equation . I have tried to solve it but I can't recognize what type of equation I have learnt.,y=2xy'+y^2y'^3,['ordinary-differential-equations']
92,General solution of $x^{\prime}(t)=A x(t)$ [repeated real/complex eigenvalues case],General solution of  [repeated real/complex eigenvalues case],x^{\prime}(t)=A x(t),"Could anybody give me the general solution of autonomous linear systems $X^{\prime}(t)=A X(t)$ in the case where there are repeated real eigenvalues and some complex repeated complex eigenvalues? The case where they are all distinct is done by the following theorem but in the case where the eigenvalues are repeated, I didn't find something that could make me satisfied. Theorem: Consider the system $X^{\prime}=A X$ where $A$ has distinct eigenvalues $\lambda_{1}, \ldots, \lambda_{k_{1}} \in \mathbb{R}$ and $\alpha_{1}+i \beta_{1}, \ldots, \alpha_{k_{2}}+i \beta_{k_{2}} \in \mathbb{C} .$ Let $T$ be the matrix that puts A in the canonical form $T^{-1} A T=\left(\begin{array}{cccccc}{\lambda_{1}} \\ {} & {\ddots} & {} \\ {} & {} & {\lambda_{k_{1}}} & {} \\ {} & {} & {} & {B_{1}} \\ {} & {} & {} & {} & {\ddots} \\ {} & {} & {} & {} & {} & {B_{k_{2}}}\end{array}\right)$ where $$ B_{j}=\left(\begin{array}{cc}{\alpha_{j}} & {\beta_{j}} \\ {-\beta_{j}} & {\alpha_{j}}\end{array}\right) $$ Then the general solution of $X^{\prime}=A X$ is TY (t) where $Y(t)=\left(\begin{array}{c}{c_{1} e^{\lambda_{1} t}} \\ {\vdots} \\ {\vdots} \\ {a_{1} e^{\alpha_{1} t} \cos \beta_{1} t+b_{1} e^{\alpha_{1} t} \sin \beta_{1} t} \\ {-a_{1} e^{\alpha_{1} t} \sin \beta_{1} t+b_{1} e^{\alpha_{1} t} \cos \beta_{1} t} \\ {\vdots} \\ {a_{k_{2}} e^{\alpha_{k_{2}} t} \cos \beta_{k_{2}} t+b_{k_{2}} e^{\alpha_{k_{2}} t} \sin \beta_{k_{2}} t} \\ {-a_{k_{2}} e^{\alpha_{k_{2}} t} \sin \beta_{k_{2}} t+b_{k_{2}} e^{\alpha_{k_{2}} t} \cos \beta_{k_{2}} t}\end{array}\right)$ Thanks for your help","Could anybody give me the general solution of autonomous linear systems in the case where there are repeated real eigenvalues and some complex repeated complex eigenvalues? The case where they are all distinct is done by the following theorem but in the case where the eigenvalues are repeated, I didn't find something that could make me satisfied. Theorem: Consider the system where has distinct eigenvalues and Let be the matrix that puts A in the canonical form where Then the general solution of is TY (t) where Thanks for your help","X^{\prime}(t)=A X(t) X^{\prime}=A X A \lambda_{1}, \ldots, \lambda_{k_{1}} \in \mathbb{R} \alpha_{1}+i \beta_{1}, \ldots, \alpha_{k_{2}}+i \beta_{k_{2}} \in \mathbb{C} . T T^{-1} A T=\left(\begin{array}{cccccc}{\lambda_{1}} \\ {} & {\ddots} & {} \\ {} & {} & {\lambda_{k_{1}}} & {} \\ {} & {} & {} & {B_{1}} \\ {} & {} & {} & {} & {\ddots} \\ {} & {} & {} & {} & {} & {B_{k_{2}}}\end{array}\right) 
B_{j}=\left(\begin{array}{cc}{\alpha_{j}} & {\beta_{j}} \\ {-\beta_{j}} & {\alpha_{j}}\end{array}\right)
 X^{\prime}=A X Y(t)=\left(\begin{array}{c}{c_{1} e^{\lambda_{1} t}} \\ {\vdots} \\ {\vdots} \\ {a_{1} e^{\alpha_{1} t} \cos \beta_{1} t+b_{1} e^{\alpha_{1} t} \sin \beta_{1} t} \\ {-a_{1} e^{\alpha_{1} t} \sin \beta_{1} t+b_{1} e^{\alpha_{1} t} \cos \beta_{1} t} \\ {\vdots} \\ {a_{k_{2}} e^{\alpha_{k_{2}} t} \cos \beta_{k_{2}} t+b_{k_{2}} e^{\alpha_{k_{2}} t} \sin \beta_{k_{2}} t} \\ {-a_{k_{2}} e^{\alpha_{k_{2}} t} \sin \beta_{k_{2}} t+b_{k_{2}} e^{\alpha_{k_{2}} t} \cos \beta_{k_{2}} t}\end{array}\right)","['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'matrix-exponential']"
93,Find power series solution of $y^\prime=x^2y$,Find power series solution of,y^\prime=x^2y,"Find power series solution of $y^\prime=x^2y$ , determine the radius of convergence and identify the series solutions in terms of elementary functions. So I started with $y=\sum_{n=0}^\infty c_nx^n$ then $y^\prime=\sum_{n=1}^\infty n c_nx^{n-1}$ So rearranging and substituting the series I get $$\sum_{n=1}^\infty n c_nx^{n-1}-x^2 \sum_{n=0}^\infty c_nx^n=0$$ then I changed the index on the first sum $$\sum_{n=0}^\infty (n+1) c_{n+1}x^n-x^2 \sum_{n=0}^\infty c_nx^n=0$$ $$\sum_{n=0}^\infty (n+1) c_{n+1}x^n- c_nx^{n+2}=0$$ $$\sum_{n=0}^\infty x^n\big((n+1)c_{n+1}-c_nx^2\big)=0$$ I'm not sure how to simplify this further.","Find power series solution of , determine the radius of convergence and identify the series solutions in terms of elementary functions. So I started with then So rearranging and substituting the series I get then I changed the index on the first sum I'm not sure how to simplify this further.",y^\prime=x^2y y=\sum_{n=0}^\infty c_nx^n y^\prime=\sum_{n=1}^\infty n c_nx^{n-1} \sum_{n=1}^\infty n c_nx^{n-1}-x^2 \sum_{n=0}^\infty c_nx^n=0 \sum_{n=0}^\infty (n+1) c_{n+1}x^n-x^2 \sum_{n=0}^\infty c_nx^n=0 \sum_{n=0}^\infty (n+1) c_{n+1}x^n- c_nx^{n+2}=0 \sum_{n=0}^\infty x^n\big((n+1)c_{n+1}-c_nx^2\big)=0,['ordinary-differential-equations']
94,Laplace transform of integral constant,Laplace transform of integral constant,,"From the book First course on DE, there's the problem $2y''+ty'-2y=10, y(0)=0, y'(0)=0$ . Implying laplace transform to both sides give a linear homogenuous DE $Y'(s)-(2s-\frac{3}{s})Y(s)=-\frac{10}{s^2}$ and it gives $Y=\frac{5}{s^3}+C\frac{e^{s^2}}{s^3}$ , but I'm stuck here. The answer is $\frac{5}{2}t^2$ , which means that $c=0$ . Now I can't find the reason why $c=0$ . Can anyone help???","From the book First course on DE, there's the problem . Implying laplace transform to both sides give a linear homogenuous DE and it gives , but I'm stuck here. The answer is , which means that . Now I can't find the reason why . Can anyone help???","2y''+ty'-2y=10, y(0)=0, y'(0)=0 Y'(s)-(2s-\frac{3}{s})Y(s)=-\frac{10}{s^2} Y=\frac{5}{s^3}+C\frac{e^{s^2}}{s^3} \frac{5}{2}t^2 c=0 c=0","['integration', 'ordinary-differential-equations', 'functions', 'laplace-transform']"
95,Find the general solution of $e^y (\cos xy - y \sin xy)dx + e^y (\cos xy - x \sin xy)dy = 0$,Find the general solution of,e^y (\cos xy - y \sin xy)dx + e^y (\cos xy - x \sin xy)dy = 0,"I'm trying to find out whether I screwed up. I used $\mu(x,y)=e^{-y}\cos^{-1}xy$ as an integrating factor: $$(1-y\tan xy)dx + (1-x\tan xy)dy = 0$$ But since this is an exact differential equation, we can just look for a differentiable field $F$ such that $$\begin{cases} F_x & = 1-y\tan xy  \\ F_y & = 1-x\tan xy \end{cases}$$ Integrating $F_x$ w.r.t. $x$ we get $$\int (1-y\tan xy) dx = x + \ln|\cos xy| + \phi(y)$$ When we compute the derivative of this expression w.r.t. $y$ we can infer that $\phi'(y)=1$ , so $\phi(y)=y+C$ . Hence $$F(x,y)= x + y + \ln|\cos xy| + C$$ So the general solution is implicitly defined by the one-parameter family of equations $F(x,y)=0$ . Is this correct? How can I know I did not gain nor lose solutions when multiplying by $\mu(x,y)$ ?","I'm trying to find out whether I screwed up. I used as an integrating factor: But since this is an exact differential equation, we can just look for a differentiable field such that Integrating w.r.t. we get When we compute the derivative of this expression w.r.t. we can infer that , so . Hence So the general solution is implicitly defined by the one-parameter family of equations . Is this correct? How can I know I did not gain nor lose solutions when multiplying by ?","\mu(x,y)=e^{-y}\cos^{-1}xy (1-y\tan xy)dx + (1-x\tan xy)dy = 0 F \begin{cases} F_x & = 1-y\tan xy  \\ F_y & = 1-x\tan xy \end{cases} F_x x \int (1-y\tan xy) dx = x + \ln|\cos xy| + \phi(y) y \phi'(y)=1 \phi(y)=y+C F(x,y)= x + y + \ln|\cos xy| + C F(x,y)=0 \mu(x,y)",['ordinary-differential-equations']
96,Definition of $C^k$ boundaries,Definition of  boundaries,C^k,"I am reading the book ""Partial Differential Equations"" of Lawrance c. Evans by myself and started with Appendix part. At the very beginning of Appendix C, there exists a definition ""We say $\partial U$ is $C^k$ if for each point $x^0\in\partial U$ , there exists $r>0$ and a $C^k$ function $\gamma:\mathbb{R}^{n-1}\longrightarrow\mathbb{R}$ such that we have $ U\cap B(x^0,r)=\{x\in B(x^0,r)\lvert x_n>\gamma(x_1,...,x_{n-1})\}$ I do not understand the intuition behind this definition.To my understanding it does not correspond to derivatives. Can anyone help me with that? Why we call such boundary sets as $C^k$ ?","I am reading the book ""Partial Differential Equations"" of Lawrance c. Evans by myself and started with Appendix part. At the very beginning of Appendix C, there exists a definition ""We say is if for each point , there exists and a function such that we have I do not understand the intuition behind this definition.To my understanding it does not correspond to derivatives. Can anyone help me with that? Why we call such boundary sets as ?","\partial U C^k x^0\in\partial U r>0 C^k \gamma:\mathbb{R}^{n-1}\longrightarrow\mathbb{R}  U\cap B(x^0,r)=\{x\in B(x^0,r)\lvert x_n>\gamma(x_1,...,x_{n-1})\} C^k","['real-analysis', 'ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
97,Stability without Lyapunov methods,Stability without Lyapunov methods,,"I've been having some problems trying to solve a problem which appears in the book im following, so any help would be really appreciated. Defn. A fixed point $x_{0}$ is asymptotically stable if it is stable and (1) if there is a neighborhood $U$ , s.t. $x_{0} \in U$ and $|\phi(t,x)-x_{0} | \rightarrow 0$ as $t \rightarrow \infty$ . Problem. let $\dot x = x - y - x(x^{2}+y^{2}) + \frac{xy}{\sqrt{x^{2}+y^{2}}}$ and $\dot y = x + y - y(x^{2}+y^{2}) - \frac{x^{2}}{\sqrt{x^{2}+y^{2}}}$ . I have to show that $(1,0)$ is not stable even though it satisfies (1) . My solution . First, I change the cartesian coord. to polar coordinates. Then our new system is $\dot r = r-r^{3}$ and $\dot \theta = 2sin^{2}(\theta/2)$ . The point we're studying stays the same after the transformation, $(r,\theta)=(1,0)$ . Now, my problems begin, I tried to apply the linearization theorem and we got that our Jacobian Matrix will be \begin{bmatrix}1-3r^{2}&0\\0&2sin(\theta/2)cos(\theta/2)\end{bmatrix} and evaluating $(1,0)$ \begin{bmatrix}-2&0\\0&0\end{bmatrix} then its eigenvalues are $\lambda_{1,2}=-2,0$ and this implies that this is no good por my analysis cause we can say nothing ( right? ). And if we could say that it is unstable by this analysis, I havent been able to prove that (1) holds. The other way I think we could prove this is by definition directly, but since I couldn't find the flow for the ODE I cant try the definitions. So if you guys could help me with it I'd more than glad. Thanks so much in advance, I really appreciate it. <3","I've been having some problems trying to solve a problem which appears in the book im following, so any help would be really appreciated. Defn. A fixed point is asymptotically stable if it is stable and (1) if there is a neighborhood , s.t. and as . Problem. let and . I have to show that is not stable even though it satisfies (1) . My solution . First, I change the cartesian coord. to polar coordinates. Then our new system is and . The point we're studying stays the same after the transformation, . Now, my problems begin, I tried to apply the linearization theorem and we got that our Jacobian Matrix will be and evaluating then its eigenvalues are and this implies that this is no good por my analysis cause we can say nothing ( right? ). And if we could say that it is unstable by this analysis, I havent been able to prove that (1) holds. The other way I think we could prove this is by definition directly, but since I couldn't find the flow for the ODE I cant try the definitions. So if you guys could help me with it I'd more than glad. Thanks so much in advance, I really appreciate it. <3","x_{0} U x_{0} \in U |\phi(t,x)-x_{0} | \rightarrow 0 t \rightarrow \infty \dot x = x - y - x(x^{2}+y^{2}) + \frac{xy}{\sqrt{x^{2}+y^{2}}} \dot y = x + y - y(x^{2}+y^{2}) - \frac{x^{2}}{\sqrt{x^{2}+y^{2}}} (1,0) \dot r = r-r^{3} \dot \theta = 2sin^{2}(\theta/2) (r,\theta)=(1,0) \begin{bmatrix}1-3r^{2}&0\\0&2sin(\theta/2)cos(\theta/2)\end{bmatrix} (1,0) \begin{bmatrix}-2&0\\0&0\end{bmatrix} \lambda_{1,2}=-2,0","['calculus', 'ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
98,Uniform stability of equilibrium,Uniform stability of equilibrium,,"I have been given the following definition for uniform stability: the equilibrium state $x_e$ is uniformly stable, if for any $\epsilon > 0$ there is a $\delta > 0$ such that $$\|x(0)-x_e\|<\delta~~\Rightarrow ~~\|x(t)-x_e\|<\epsilon, ~~\forall t\geq 0$$ In my opinion this definition does not have anything to do with stability. Imagine a system with $x(t)$ going to infinity and $x(t) \geq 10^{100} ~~\forall t \geq 0$ and $x_e = 0$ . Then the system would be uniformly stable following the above definition, as for any $\epsilon$ I can use $\delta = 10^{100}$ and that would fulfill the implication. The reason is that $x(0) - x_e$ is never smaller than $10^{100}$ so the left side of the implication is always false and therefore the implication always true. What am I missing here?","I have been given the following definition for uniform stability: the equilibrium state is uniformly stable, if for any there is a such that In my opinion this definition does not have anything to do with stability. Imagine a system with going to infinity and and . Then the system would be uniformly stable following the above definition, as for any I can use and that would fulfill the implication. The reason is that is never smaller than so the left side of the implication is always false and therefore the implication always true. What am I missing here?","x_e \epsilon > 0 \delta > 0 \|x(0)-x_e\|<\delta~~\Rightarrow ~~\|x(t)-x_e\|<\epsilon, ~~\forall t\geq 0 x(t) x(t) \geq 10^{100} ~~\forall t \geq 0 x_e = 0 \epsilon \delta = 10^{100} x(0) - x_e 10^{100}","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'neural-networks', 'lyapunov-functions']"
99,what is the particular solution of $y'' - y= -cos(x)$?,what is the particular solution of ?,y'' - y= -cos(x),"I solved it like this $r^2 - 1= 0 \Rightarrow r=1,-1$ , so the roots are real, this means the complementary function will be of the form $$y = A \exp(x)+ B \exp(-x).$$ Then, I tried to find the particular solution using variation of parameters method, I got particular solution $y=\frac{9}{4}\sin(x) -\frac{7}{4}\cos(x)$ . Is this correct?","I solved it like this , so the roots are real, this means the complementary function will be of the form Then, I tried to find the particular solution using variation of parameters method, I got particular solution . Is this correct?","r^2 - 1= 0 \Rightarrow r=1,-1 y = A \exp(x)+ B \exp(-x). y=\frac{9}{4}\sin(x) -\frac{7}{4}\cos(x)",['ordinary-differential-equations']
