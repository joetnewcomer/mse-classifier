,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Continuity of functions inside a open ball,Continuity of functions inside a open ball,,"Let $ f: X \subset \mathbb{R}^p \to \mathbb{R}^q $ and $ a \in X$. Supose that for all $ \epsilon > 0 $ exists $ g: X \to \mathbb{R}^q $ continuous at $a$ such as $ \| f(x) - g(x) \| < \epsilon $ for all $x \in X$, prove that $f$ is continuous in $a$. Demonstration: Since $g$ is continuous we have: $ \forall \epsilon > 0 $, $ \exists \delta > 0 $; $ \| x -a \ < \delta \implies \| g(x) - g(a) \| < \epsilon $. We know by hypothesis that $ \| f(x) - g(x) \| < \epsilon \; \forall \; x \in X $. We wanto to prove: $ \forall \epsilon > 0 $, $ \exists \delta > 0 $; $ \| x -a \| < \delta \implies \| f(x) - f(a) \| < \epsilon $. My intuition was that the norm/distance (in the metric space ) from $ \| f(a) - f(x) \| \le \| g(x) - g(a) \| - \frac{\epsilon}{2} = r $. But I could not follow up since the distance do no implies that $f(x)$ is inside the ball on $g(x)$","Let $ f: X \subset \mathbb{R}^p \to \mathbb{R}^q $ and $ a \in X$. Supose that for all $ \epsilon > 0 $ exists $ g: X \to \mathbb{R}^q $ continuous at $a$ such as $ \| f(x) - g(x) \| < \epsilon $ for all $x \in X$, prove that $f$ is continuous in $a$. Demonstration: Since $g$ is continuous we have: $ \forall \epsilon > 0 $, $ \exists \delta > 0 $; $ \| x -a \ < \delta \implies \| g(x) - g(a) \| < \epsilon $. We know by hypothesis that $ \| f(x) - g(x) \| < \epsilon \; \forall \; x \in X $. We wanto to prove: $ \forall \epsilon > 0 $, $ \exists \delta > 0 $; $ \| x -a \| < \delta \implies \| f(x) - f(a) \| < \epsilon $. My intuition was that the norm/distance (in the metric space ) from $ \| f(a) - f(x) \| \le \| g(x) - g(a) \| - \frac{\epsilon}{2} = r $. But I could not follow up since the distance do no implies that $f(x)$ is inside the ball on $g(x)$",,"['real-analysis', 'general-topology', 'metric-spaces', 'continuity']"
1,Example for a sequence of continuous functions which converge to a continuous function pointwise but not uniformly on a compact set.,Example for a sequence of continuous functions which converge to a continuous function pointwise but not uniformly on a compact set.,,"My actual aim is to verify that each of the conditions in Dini's theorem is essential or not.Theorem says that  A sequence {$f_{n}$} of continuous functions defined on a compact set $E$  converges to $f$ point wise  satisfying the following properties : $f_{n}$ is monotonic  and $f$ is continuous.  Then {$f_{n}$} converges to $f$ uniformly. So there are three hypothesis are used in theorem compactness of $E$ , continuity of $f$ and monotonicity of $f_{n}$ . First one is essential because $f_{n}= \frac{1}{1+nx}$ defined on open interval $(0,1)$ converge to $o$ point wise but not uniformly.Second condition cannot be omitted since $f_{n}=x^{n}$ defined on $[0,1]$ converges to a discontinuous function. Left out one is monotonicity.For that I need to construct a sequence of functions which are continuous, non-monotonic,and converges point wise to a continuous function.But convergence should not be uniform. I could not get such a sequence.","My actual aim is to verify that each of the conditions in Dini's theorem is essential or not.Theorem says that  A sequence {$f_{n}$} of continuous functions defined on a compact set $E$  converges to $f$ point wise  satisfying the following properties : $f_{n}$ is monotonic  and $f$ is continuous.  Then {$f_{n}$} converges to $f$ uniformly. So there are three hypothesis are used in theorem compactness of $E$ , continuity of $f$ and monotonicity of $f_{n}$ . First one is essential because $f_{n}= \frac{1}{1+nx}$ defined on open interval $(0,1)$ converge to $o$ point wise but not uniformly.Second condition cannot be omitted since $f_{n}=x^{n}$ defined on $[0,1]$ converges to a discontinuous function. Left out one is monotonicity.For that I need to construct a sequence of functions which are continuous, non-monotonic,and converges point wise to a continuous function.But convergence should not be uniform. I could not get such a sequence.",,"['real-analysis', 'examples-counterexamples', 'uniform-convergence']"
2,"Example: operator injective, then the adjoint is NOT surjective","Example: operator injective, then the adjoint is NOT surjective",,"Let $T: V \rightarrow W$ be a bounded operator on normed spaces $V,W$. Now, there is a unique adjoint operator $T': W' \rightarrow V'$ defined by $T'(\alpha) = \alpha \circ T$. In finite dimensional spaces, we have that if $T$ is injective, then $T'$ is surjective. From basic Hilbert space theory, I suspect that the Banach space adjoint must not be surjective in general. Probably, $T'$ has only dense range. Since we don't know that the range of $T'$ is closed( which is trivial in the finite-dimensional case), $T'$ does not have to be onto. Does anybody know an example of this situation, where $T$ is injective, but $T'$ not surjective? If anything is unclear, please let me know.","Let $T: V \rightarrow W$ be a bounded operator on normed spaces $V,W$. Now, there is a unique adjoint operator $T': W' \rightarrow V'$ defined by $T'(\alpha) = \alpha \circ T$. In finite dimensional spaces, we have that if $T$ is injective, then $T'$ is surjective. From basic Hilbert space theory, I suspect that the Banach space adjoint must not be surjective in general. Probably, $T'$ has only dense range. Since we don't know that the range of $T'$ is closed( which is trivial in the finite-dimensional case), $T'$ does not have to be onto. Does anybody know an example of this situation, where $T$ is injective, but $T'$ not surjective? If anything is unclear, please let me know.",,"['real-analysis', 'functional-analysis', 'hilbert-spaces', 'banach-spaces']"
3,How does adding $0$ to the set $\mathbf A = \bigl\{\frac{1}{n}: n \in \mathbb N \bigr\}$ make it a closed set?,How does adding  to the set  make it a closed set?,0 \mathbf A = \bigl\{\frac{1}{n}: n \in \mathbb N \bigr\},"By definition, a closed set is a set that contains its limit points. However, by the time the closed set contains its limit points, those points are no longer limit points and become isolated points. For example: $\mathbf A = \{\frac{1}{n}: n \in \mathbb N \}$. The limit of this set (set $\mathbf A$) is clearly equal to $0$. This is because the $\epsilon$ -neighborhood $\mathbf V_{\epsilon}(0) \cap \mathbf A = \{\frac{1}{n} \}$, and $\frac{1}{n} \neq 0$. However, when $0$ is included, the $\epsilon$ -neighborhood $\mathbf V_{\epsilon}(0) \cap \mathbf A = \{0 \}$ for $\mathbf A=[0,\frac{1}{n} ]$. This will contradicts the definition of limit point of set A and hence $0$ must be an isolated point. Another example: $\left(a,b\right)$ is an open interval with limit $a$ and $b$. Then its closure $\bar A$ will be $\left[a,b\right]$. By definition, the $\epsilon$ -neighborhood of any point in $\left[a,b\right]$ intersects the closure $\bar A$ at that same point, and hence, no points in that closure set is a limit point: A contradiction that closure sets are closed sets. Also, I am trying to prove the lemma: If x is a limit point of $A \subseteq A'$, then x is limit point of $A'$. Proof: Suppose x is a limit point of $A$, then there exists a sequence $(a_n)$ $\subset A \subseteq A'$: lim($a_n$)=x with $a_n$ $\neq x \forall n \in \mathbb N$. Then since $(a_n)$ $\subset A'$, it follows that x must be a limit point of $A'$. So my questions are: 1. What is wrong with my contradiction in the 2 examples? Please explain them to me. 2. Is my proof for the lemma correct? I am going to use it for the proof that closure set is closed. My background: I am studying elementary Real Analysis by starting with Abbot. I thank you very much for your help. Extra question : We have this theorem: x is a limit point of set $A$ if and only if there exists a sequence $(a_n) \subset A$ such that $\lim (a_n)=x$ $\forall a_n \neq x$. So, for some finite $n \in \mathbb N$ such that $a_n = x$, x is still a limit point of set A . Is this correct? I thought that x would be an isolated points since we need $a_n \neq x \forall n \in \mathbb N$ I thank you again for your answers.","By definition, a closed set is a set that contains its limit points. However, by the time the closed set contains its limit points, those points are no longer limit points and become isolated points. For example: $\mathbf A = \{\frac{1}{n}: n \in \mathbb N \}$. The limit of this set (set $\mathbf A$) is clearly equal to $0$. This is because the $\epsilon$ -neighborhood $\mathbf V_{\epsilon}(0) \cap \mathbf A = \{\frac{1}{n} \}$, and $\frac{1}{n} \neq 0$. However, when $0$ is included, the $\epsilon$ -neighborhood $\mathbf V_{\epsilon}(0) \cap \mathbf A = \{0 \}$ for $\mathbf A=[0,\frac{1}{n} ]$. This will contradicts the definition of limit point of set A and hence $0$ must be an isolated point. Another example: $\left(a,b\right)$ is an open interval with limit $a$ and $b$. Then its closure $\bar A$ will be $\left[a,b\right]$. By definition, the $\epsilon$ -neighborhood of any point in $\left[a,b\right]$ intersects the closure $\bar A$ at that same point, and hence, no points in that closure set is a limit point: A contradiction that closure sets are closed sets. Also, I am trying to prove the lemma: If x is a limit point of $A \subseteq A'$, then x is limit point of $A'$. Proof: Suppose x is a limit point of $A$, then there exists a sequence $(a_n)$ $\subset A \subseteq A'$: lim($a_n$)=x with $a_n$ $\neq x \forall n \in \mathbb N$. Then since $(a_n)$ $\subset A'$, it follows that x must be a limit point of $A'$. So my questions are: 1. What is wrong with my contradiction in the 2 examples? Please explain them to me. 2. Is my proof for the lemma correct? I am going to use it for the proof that closure set is closed. My background: I am studying elementary Real Analysis by starting with Abbot. I thank you very much for your help. Extra question : We have this theorem: x is a limit point of set $A$ if and only if there exists a sequence $(a_n) \subset A$ such that $\lim (a_n)=x$ $\forall a_n \neq x$. So, for some finite $n \in \mathbb N$ such that $a_n = x$, x is still a limit point of set A . Is this correct? I thought that x would be an isolated points since we need $a_n \neq x \forall n \in \mathbb N$ I thank you again for your answers.",,"['calculus', 'real-analysis', 'general-topology']"
4,Lagrange/Cauchy mean value theorem,Lagrange/Cauchy mean value theorem,,"Let $\,\,f,g\in C^2([0,1])$ such that $f'(0)g''(0)-f''(0)g'(0)\neq 0$ and $g'(x)\neq 0$ for all $x\in (0,1)$. Let $\theta(x)$ be a real number such that $$\frac{f(x)-f(0)}{g(x)-g(0)}=\frac{f'(\theta(x))}{g'(\theta (x))}.$$ What can I say about $\lim_{x\rightarrow 0^+} \frac{\theta(x)}{x}$?","Let $\,\,f,g\in C^2([0,1])$ such that $f'(0)g''(0)-f''(0)g'(0)\neq 0$ and $g'(x)\neq 0$ for all $x\in (0,1)$. Let $\theta(x)$ be a real number such that $$\frac{f(x)-f(0)}{g(x)-g(0)}=\frac{f'(\theta(x))}{g'(\theta (x))}.$$ What can I say about $\lim_{x\rightarrow 0^+} \frac{\theta(x)}{x}$?",,['real-analysis']
5,Sufficient conditions to have $f' = O(f(x)/x)$.,Sufficient conditions to have .,f' = O(f(x)/x),"Suppose $f$ a nonnegative real-valued function, non-decreasing, $O(x^m)$ for some $m \in \mathbb{Z}_{\geqslant 0}$ and $C^1$, with $f'$ being monotonic and nonnegative. Are this sufficient conditions to have $f'(x) = O(f(x)/x)$ as $x\to +\infty$? I've took these hypotheses from known cases, like $\sqrt[3]{x}$, $\sqrt{x}\log{x}$, $x/{\log{x}}$, etc., but I was unable to prove this in general, not even assuming $f = O(x)$ and $f'$ bounded.","Suppose $f$ a nonnegative real-valued function, non-decreasing, $O(x^m)$ for some $m \in \mathbb{Z}_{\geqslant 0}$ and $C^1$, with $f'$ being monotonic and nonnegative. Are this sufficient conditions to have $f'(x) = O(f(x)/x)$ as $x\to +\infty$? I've took these hypotheses from known cases, like $\sqrt[3]{x}$, $\sqrt{x}\log{x}$, $x/{\log{x}}$, etc., but I was unable to prove this in general, not even assuming $f = O(x)$ and $f'$ bounded.",,"['real-analysis', 'derivatives', 'asymptotics']"
6,Finishing a proof: $f$ is injective if and only if it has a left inverse,Finishing a proof:  is injective if and only if it has a left inverse,f,"I've already done a lot of searching (in particular: https://www.proofwiki.org/wiki/Injection_iff_Left_Inverse ) to try to prove this statement: $f: A \to B$ is injective if and only if it has a left inverse. [I'm going to also assume $A$ and $B$ are nonempty.] My attempt so far: $\Leftarrow$ Suppose $f$ has a left inverse and $f(a_1) = f(a_2)$ for    $a_1, a_2 \in A$. Then there is a function $g: B \to A$ such that   $g\left(f(a_1)\right) = a_1$ and $g\left(f(a_2)\right) = a_2$. Since   $g$ is well-defined, it follows that $g\left(f(a_1)\right) =  g\left(f(a_2)\right)$, and hence $a_1 = a_2$. $\Rightarrow$Now   suppose $f$ is injective. Then $f(a_1) = f(a_2) \implies a_1 = a_2$.   Furthermore, since $A$ is nonempty, there is an element $x \in A$.   Define $g: B \to A$ by  \begin{equation*} g(b) = \begin{cases}  f^{-1}(b), & b \in \text{Im}(f) \\ x, & b \in A \setminus  \text{Im}(f)\text{.} \end{cases} \end{equation*} This mapping is   well-defined, since if $f$ is injective, $f^{-1}(b)$ is unique. My main question: does this imply that if $f: A \to B$ is injective that any mapping $g: \text{Im}(f) \to A$ is bijective ? Otherwise, I don't understand why we can even use $f^{-1}$ on an element. [Please read the link above for more details - in proof 1.]","I've already done a lot of searching (in particular: https://www.proofwiki.org/wiki/Injection_iff_Left_Inverse ) to try to prove this statement: $f: A \to B$ is injective if and only if it has a left inverse. [I'm going to also assume $A$ and $B$ are nonempty.] My attempt so far: $\Leftarrow$ Suppose $f$ has a left inverse and $f(a_1) = f(a_2)$ for    $a_1, a_2 \in A$. Then there is a function $g: B \to A$ such that   $g\left(f(a_1)\right) = a_1$ and $g\left(f(a_2)\right) = a_2$. Since   $g$ is well-defined, it follows that $g\left(f(a_1)\right) =  g\left(f(a_2)\right)$, and hence $a_1 = a_2$. $\Rightarrow$Now   suppose $f$ is injective. Then $f(a_1) = f(a_2) \implies a_1 = a_2$.   Furthermore, since $A$ is nonempty, there is an element $x \in A$.   Define $g: B \to A$ by  \begin{equation*} g(b) = \begin{cases}  f^{-1}(b), & b \in \text{Im}(f) \\ x, & b \in A \setminus  \text{Im}(f)\text{.} \end{cases} \end{equation*} This mapping is   well-defined, since if $f$ is injective, $f^{-1}(b)$ is unique. My main question: does this imply that if $f: A \to B$ is injective that any mapping $g: \text{Im}(f) \to A$ is bijective ? Otherwise, I don't understand why we can even use $f^{-1}$ on an element. [Please read the link above for more details - in proof 1.]",,"['real-analysis', 'functions', 'inverse']"
7,An example of a function not in $L^2$ but such that $\int_{E} f dm\leq \sqrt{m(E)}$ for every set $E$,An example of a function not in  but such that  for every set,L^2 \int_{E} f dm\leq \sqrt{m(E)} E,"I am thinking about this problem: Let $f\in L^1 [0,1]$ to be a nonnegative function satisfied: $$\int_{E} f dm\leq \sqrt{m(E)}$$ for every measurable set $E\subset [0,1]$, Prove that $f\in L^{p}[0,1]$ for $1\leq p<2$. I have already done this. But I want a counterexample for $p=2$. How to obtain one?","I am thinking about this problem: Let $f\in L^1 [0,1]$ to be a nonnegative function satisfied: $$\int_{E} f dm\leq \sqrt{m(E)}$$ for every measurable set $E\subset [0,1]$, Prove that $f\in L^{p}[0,1]$ for $1\leq p<2$. I have already done this. But I want a counterexample for $p=2$. How to obtain one?",,"['real-analysis', 'examples-counterexamples', 'lp-spaces', 'lebesgue-measure']"
8,Compute $\lim_{x \to 0} \frac{\sin(x)+\cos(x)-e^x}{\log(1+x^2)}$.,Compute .,\lim_{x \to 0} \frac{\sin(x)+\cos(x)-e^x}{\log(1+x^2)},"Question: Compute $\lim_{x \to 0} \frac{\sin(x)+\cos(x)-e^x}{\log(1+x^2)}$. Attempt: Using L'Hopital's Rule, I have come to $$ \lim_{x \to 0} \frac{\cos(x)}{2x} - \lim_{x \to 0} \frac{\sin(x)}{2x} - \lim_{x \to 0} \frac{e^x}{2x}.$$ My thought was to use the power series representations of these functions. However, that' doesn't seem to get me anywhere. Am I on the wrong track using L'Hopital's?","Question: Compute $\lim_{x \to 0} \frac{\sin(x)+\cos(x)-e^x}{\log(1+x^2)}$. Attempt: Using L'Hopital's Rule, I have come to $$ \lim_{x \to 0} \frac{\cos(x)}{2x} - \lim_{x \to 0} \frac{\sin(x)}{2x} - \lim_{x \to 0} \frac{e^x}{2x}.$$ My thought was to use the power series representations of these functions. However, that' doesn't seem to get me anywhere. Am I on the wrong track using L'Hopital's?",,"['calculus', 'real-analysis']"
9,"$f:\mathbb R \to \mathbb R$ be twice differentiable , $f(x)+f''(x)=-xg(x)f'(x) , g(x) \ge 0 , \forall x \in \mathbb R$ , then $f$ is bounded?","be twice differentiable ,  , then  is bounded?","f:\mathbb R \to \mathbb R f(x)+f''(x)=-xg(x)f'(x) , g(x) \ge 0 , \forall x \in \mathbb R f","Let $g:\mathbb R \to [ 0,\infty)$ be a function and $f:\mathbb R \to \mathbb R$ be a twice differentiable function such that $f(x)+f''(x)=-xg(x)f'(x) , \forall x \in \mathbb R$ , then is it true that $f$ is bounded i.e. $\exists M \ge0 $ such that $|f(x)| \le M , \forall x \in \mathbb R$ ?","Let $g:\mathbb R \to [ 0,\infty)$ be a function and $f:\mathbb R \to \mathbb R$ be a twice differentiable function such that $f(x)+f''(x)=-xg(x)f'(x) , \forall x \in \mathbb R$ , then is it true that $f$ is bounded i.e. $\exists M \ge0 $ such that $|f(x)| \le M , \forall x \in \mathbb R$ ?",,['real-analysis']
10,Show that $f=0$ almost everywhere.,Show that  almost everywhere.,f=0,"If $f$ is integrable in $\mathbb{R}^d$ as for the Lebesgue measure and $\int_{R}f=0$ for each rectangle $R$, then $f=0$ almost everywhere. Could you give me some hints how to show it??","If $f$ is integrable in $\mathbb{R}^d$ as for the Lebesgue measure and $\int_{R}f=0$ for each rectangle $R$, then $f=0$ almost everywhere. Could you give me some hints how to show it??",,"['real-analysis', 'measure-theory']"
11,A smooth nonzero function $\mathbb R\to\mathbb R$ with uniformly bounded derivatives tending to zero at infinity?,A smooth nonzero function  with uniformly bounded derivatives tending to zero at infinity?,\mathbb R\to\mathbb R,"$\def\scrBp{\mathscr B\boldsymbol.}\def\rD{{\rm D}\kern.4mm}\def\ssp{\kern.4mm}\def\bbR{\mathbb R} $For a certain purpose I invented the Banach space $\scrBp^\infty(\ssp\bbR\ssp)$ defined as follows. The vectors are all smooth functions $x:\bbR\to\bbR$ such that there is $M\in\bbR^+$ with $|\,\rD^kx(t)\,|\le M$ and $\lim_{\,s\to\pm\infty\,}|\,\rD^kx(s)\,|=0$ for all $k\in\mathbb N_0$ and $t\in\bbR$ . The norm of $x$ is the infimum of the set of all these $M$ . Now I am wondering whether this space has any nonzero vectors. Hence the question.","$\def\scrBp{\mathscr B\boldsymbol.}\def\rD{{\rm D}\kern.4mm}\def\ssp{\kern.4mm}\def\bbR{\mathbb R} $For a certain purpose I invented the Banach space $\scrBp^\infty(\ssp\bbR\ssp)$ defined as follows. The vectors are all smooth functions $x:\bbR\to\bbR$ such that there is $M\in\bbR^+$ with $|\,\rD^kx(t)\,|\le M$ and $\lim_{\,s\to\pm\infty\,}|\,\rD^kx(s)\,|=0$ for all $k\in\mathbb N_0$ and $t\in\bbR$ . The norm of $x$ is the infimum of the set of all these $M$ . Now I am wondering whether this space has any nonzero vectors. Hence the question.",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'banach-spaces']"
12,"If $\sum\limits_{n=1}^\infty a_n$ diverges, then $\sum\limits_{n=1}^\infty\exp\left(-\sum\limits_{k=1}^n k a_k\right)$ converges?","If  diverges, then  converges?",\sum\limits_{n=1}^\infty a_n \sum\limits_{n=1}^\infty\exp\left(-\sum\limits_{k=1}^n k a_k\right),"Let $(a_n)$ be a decreasing sequence of positive numbers and let $$b_n = \exp\left(-\sum_{k=1}^n k\,a_k\right).$$ Is is generally true that $$ \sum_{n=1}^\infty a_n = +\infty \implies \sum_{n=1}^\infty b_n < +\infty? $$ On the first hand I am inclined to answer negatively because some series diverge very very slowly, but on the other hand I could not come up with a counterexample. For instance, $a_n = \dfrac{1}{n(\log n)}$ yields $b_n = \exp\left(-\dfrac{n}{\log n} + o(1)\right)$ so the series is convergent.","Let $(a_n)$ be a decreasing sequence of positive numbers and let $$b_n = \exp\left(-\sum_{k=1}^n k\,a_k\right).$$ Is is generally true that $$ \sum_{n=1}^\infty a_n = +\infty \implies \sum_{n=1}^\infty b_n < +\infty? $$ On the first hand I am inclined to answer negatively because some series diverge very very slowly, but on the other hand I could not come up with a counterexample. For instance, $a_n = \dfrac{1}{n(\log n)}$ yields $b_n = \exp\left(-\dfrac{n}{\log n} + o(1)\right)$ so the series is convergent.",,"['real-analysis', 'sequences-and-series', 'divergent-series']"
13,Prove this limit of $x^4$ using $\epsilon - \delta$,Prove this limit of  using,x^4 \epsilon - \delta,"$\lim_{x\to a} x^4 = a^4$ using epsilon and delta. We know $|x^4 - a^4| < \epsilon \space \text{such that} \space |x - a| < \delta$ So we have to find a $\delta$ for, which it works. $|x^4 - a^4| = |x^2 + a^2||x-a||x+a|$ $|x^2 + a^2||x-a||x+a| < \epsilon \space \text{such that} \space |x - a| < \delta$ Let's require, $|x - a| < 1 \implies |x + a| < 2|a| + 1$ $1 > |x - a|$ $|x - a| \ge |x| - |a|$ Therefore, $|x| - |a| < |x - a| < 1$ $|x| < 1 + |a|$ $x^2 < (1 + |a|)^2$ $\implies x^2 + a^2 < (1 + |a|)^2 + a^2 \implies |x^2 + a^2| < (1 + |a|)^2 + a^2$ $ \implies |x + a| < 2|a| + 1$ All together, this implies, finally: $ |x^2 + a^2| \cdot |x+a| < ((1 + |a|)^2 + a^2)\cdot (2|a| + 1)$ This requires, $|x - a| < \text{min}(1, \frac{1}{((1 + |a|)^2 + a^2)\cdot (2|a| + 1)})$ Thus, $\delta = \frac{1}{((1 + |a|)^2 + a^2)\cdot (2|a| + 1)})$ Thanks, please suggest if this is correct!","$\lim_{x\to a} x^4 = a^4$ using epsilon and delta. We know $|x^4 - a^4| < \epsilon \space \text{such that} \space |x - a| < \delta$ So we have to find a $\delta$ for, which it works. $|x^4 - a^4| = |x^2 + a^2||x-a||x+a|$ $|x^2 + a^2||x-a||x+a| < \epsilon \space \text{such that} \space |x - a| < \delta$ Let's require, $|x - a| < 1 \implies |x + a| < 2|a| + 1$ $1 > |x - a|$ $|x - a| \ge |x| - |a|$ Therefore, $|x| - |a| < |x - a| < 1$ $|x| < 1 + |a|$ $x^2 < (1 + |a|)^2$ $\implies x^2 + a^2 < (1 + |a|)^2 + a^2 \implies |x^2 + a^2| < (1 + |a|)^2 + a^2$ $ \implies |x + a| < 2|a| + 1$ All together, this implies, finally: $ |x^2 + a^2| \cdot |x+a| < ((1 + |a|)^2 + a^2)\cdot (2|a| + 1)$ This requires, $|x - a| < \text{min}(1, \frac{1}{((1 + |a|)^2 + a^2)\cdot (2|a| + 1)})$ Thus, $\delta = \frac{1}{((1 + |a|)^2 + a^2)\cdot (2|a| + 1)})$ Thanks, please suggest if this is correct!",,"['calculus', 'real-analysis', 'limits', 'proof-verification', 'epsilon-delta']"
14,Limit of Lebesgue integrable function,Limit of Lebesgue integrable function,,"Let $f$ be a real valued, Lebesgue integrable function on $\mathbb{R}$. Prove that $$\lim_{t \to 0} \int_{\mathbb R} |f(x+t)-f(x)|\, dx=0.$$","Let $f$ be a real valued, Lebesgue integrable function on $\mathbb{R}$. Prove that $$\lim_{t \to 0} \int_{\mathbb R} |f(x+t)-f(x)|\, dx=0.$$",,"['real-analysis', 'lebesgue-integral']"
15,Cardinality of a line and a half plane,Cardinality of a line and a half plane,,"intuitively it seems like the cardinality of the set of points that make up a line should be different than the cardinality of the set of points that make up a half plane but I couldn't come up with a proof, does a simple proof exist?","intuitively it seems like the cardinality of the set of points that make up a line should be different than the cardinality of the set of points that make up a half plane but I couldn't come up with a proof, does a simple proof exist?",,"['real-analysis', 'elementary-set-theory']"
16,Continuous iff Oscillation is zero,Continuous iff Oscillation is zero,,"For  a bounded function $f:D\subset \Bbb R^n \rightarrow  \Bbb R$ , $b$ in $\Bbb R^n$ ,   and a real number $\delta>0$ . Define the following: $M(f,b,\delta)$ =sup{f(x) $: x$ in $D$ and $||x-b||<\delta$ } $m(f,b,\delta)$ =inf{f(x) $: x$ in $D$ and $||x-b||<\delta$ } Define the oscillation of $f$ in $b$ as: $o(f,b)= \lim\limits_{\delta \to 0} \ (M(f,b,\delta)-m(f,b,\delta))$ Prove that $f$ is continuous in $b$ if and only if $o(f,b)=0$ The first direction would be easier if we had a closed interval. As this one is closed, I haven't figured out how to prove the first implication. Also, what would you suggest for the second one? For the converse, it it enough to say that if $||x-b||< \hat{\delta}$ then $||f(x)-f(b)||\leq|| M(f,b,\hat{\delta})-m(f,b,\hat{\delta})||\leq \epsilon$ ?","For  a bounded function , in ,   and a real number . Define the following: =sup{f(x) in and } =inf{f(x) in and } Define the oscillation of in as: Prove that is continuous in if and only if The first direction would be easier if we had a closed interval. As this one is closed, I haven't figured out how to prove the first implication. Also, what would you suggest for the second one? For the converse, it it enough to say that if then ?","f:D\subset \Bbb R^n \rightarrow  \Bbb R b \Bbb R^n \delta>0 M(f,b,\delta) : x D ||x-b||<\delta m(f,b,\delta) : x D ||x-b||<\delta f b o(f,b)= \lim\limits_{\delta \to 0} \ (M(f,b,\delta)-m(f,b,\delta)) f b o(f,b)=0 ||x-b||< \hat{\delta} ||f(x)-f(b)||\leq|| M(f,b,\hat{\delta})-m(f,b,\hat{\delta})||\leq \epsilon","['real-analysis', 'analysis']"
17,"Proof of FTC, continuity part, for Lebesgue integrable functions","Proof of FTC, continuity part, for Lebesgue integrable functions",,"The part of the FTC I am interested in says: If $f$ is a Lebesgue-integrable function on $[a,b]$, then $F(x)=\int_a^xf(t)\,dt$ is continuous. This is usually considered a lemma or something for the first part of the FTC, which adds the condition that $f$ is continuous and gets the extra information that $F$ is differentiable with $F'=f$. But this part of the theorem, without the continuity assumption, is giving me trouble. Combined with the fact that this is a Lebesgue integral and not a Riemann integral, it means that $f$ is not necessarily bounded, so I can't just prove that $F$ is Lipschitz-continuous as in the usual proof with Riemann-integrable functions. A simple example of a function that is not bounded but is Lebesgue-integrable on compact sets (and such that $F$ is continuous) is $f(x)=|x|^{-1/2}$. The weakening to Lebesgue-integrable functions is my own addition, so I don't have independent confirmation that the theorem is true, but it seems clear to me that any counterexample will essentially reproduce the characteristics of the delta function, which is not a function in the usual sense. Any ideas? Edit : It suffices to prove this for nonnegative functions $f$, because $$|F(y)-F(x)|=\left|\int_x^yf(t)\,dt\right|\le\int_x^y|f(t)|\,dt=\int_x^yg(t)\,dt=G(y)-G(x)$$ where $g(t)=|f(t)|$ and $G(x)=\int_a^xg(t)\,dt$, so if $G$ is continuous then $F$ is also continuous.","The part of the FTC I am interested in says: If $f$ is a Lebesgue-integrable function on $[a,b]$, then $F(x)=\int_a^xf(t)\,dt$ is continuous. This is usually considered a lemma or something for the first part of the FTC, which adds the condition that $f$ is continuous and gets the extra information that $F$ is differentiable with $F'=f$. But this part of the theorem, without the continuity assumption, is giving me trouble. Combined with the fact that this is a Lebesgue integral and not a Riemann integral, it means that $f$ is not necessarily bounded, so I can't just prove that $F$ is Lipschitz-continuous as in the usual proof with Riemann-integrable functions. A simple example of a function that is not bounded but is Lebesgue-integrable on compact sets (and such that $F$ is continuous) is $f(x)=|x|^{-1/2}$. The weakening to Lebesgue-integrable functions is my own addition, so I don't have independent confirmation that the theorem is true, but it seems clear to me that any counterexample will essentially reproduce the characteristics of the delta function, which is not a function in the usual sense. Any ideas? Edit : It suffices to prove this for nonnegative functions $f$, because $$|F(y)-F(x)|=\left|\int_x^yf(t)\,dt\right|\le\int_x^y|f(t)|\,dt=\int_x^yg(t)\,dt=G(y)-G(x)$$ where $g(t)=|f(t)|$ and $G(x)=\int_a^xg(t)\,dt$, so if $G$ is continuous then $F$ is also continuous.",,"['calculus', 'real-analysis', 'lebesgue-integral']"
18,Measure of Elementary Sets Proof,Measure of Elementary Sets Proof,,"I am struggling with what seems like a very simple problem from Terrence Tao's Introduction to Measure Theory book (which is available for free online by the way). What I am trying to prove is the following: Give an alternate proof of Lemma 1.1.2(ii) by showing that any two partitions of $E$ into boxes admit a mutual refinement into boxes that arise from taking Cartesian products of elements from finite collections of disjoint intervals. The referenced Lemma is provided below: Lemma 1.1.2 (Measure of an elementary set). Let $E \subset \mathbb{R}^d$ be an elementary set. $E$ can be expressed as the finite union of disjoint boxes. If $E$ is partitioned as the finite union $B_1 \cup \ldots \cup B_k$ of disjoint boxes, then the quantity $m(E):=|B_1|+ \ldots + |B_k|$ is independent of the partition. In other words, given any other partition $B'_1 \cup \ldots \cup B'_{k'}$ of $E$, one has  $|B_1|+ \ldots + |B_k| = |B'_1|+ \ldots + |B'_{k'}|$. The proof that is provided in the text uses a discretization argument that I do not understand, but the problem at hand is to show the same result holds regardless of the partition used. My approach was to let $X=B_1 \cup \ldots \cup B_k$ and $Y=B'_1 \cup \ldots \cup B'_{k'}$ and then show that $X=Y$. I can rewrite both of these sets as  $X=\bigcup\limits _{i=1}^kB_i$ and $Y=\bigcup\limits_{j=1}^{k'}B'_j$, but then I am confused on how to proceed to show their measures are equivalent. The problem states to use Cartesian products, but I notice that my attempt does not seem to use it which is why I am starting to think that I am on the wrong path. Any assistance, suggestions, and/or advice on this would be greatly appreciated. Many thanks in advance.","I am struggling with what seems like a very simple problem from Terrence Tao's Introduction to Measure Theory book (which is available for free online by the way). What I am trying to prove is the following: Give an alternate proof of Lemma 1.1.2(ii) by showing that any two partitions of $E$ into boxes admit a mutual refinement into boxes that arise from taking Cartesian products of elements from finite collections of disjoint intervals. The referenced Lemma is provided below: Lemma 1.1.2 (Measure of an elementary set). Let $E \subset \mathbb{R}^d$ be an elementary set. $E$ can be expressed as the finite union of disjoint boxes. If $E$ is partitioned as the finite union $B_1 \cup \ldots \cup B_k$ of disjoint boxes, then the quantity $m(E):=|B_1|+ \ldots + |B_k|$ is independent of the partition. In other words, given any other partition $B'_1 \cup \ldots \cup B'_{k'}$ of $E$, one has  $|B_1|+ \ldots + |B_k| = |B'_1|+ \ldots + |B'_{k'}|$. The proof that is provided in the text uses a discretization argument that I do not understand, but the problem at hand is to show the same result holds regardless of the partition used. My approach was to let $X=B_1 \cup \ldots \cup B_k$ and $Y=B'_1 \cup \ldots \cup B'_{k'}$ and then show that $X=Y$. I can rewrite both of these sets as  $X=\bigcup\limits _{i=1}^kB_i$ and $Y=\bigcup\limits_{j=1}^{k'}B'_j$, but then I am confused on how to proceed to show their measures are equivalent. The problem states to use Cartesian products, but I notice that my attempt does not seem to use it which is why I am starting to think that I am on the wrong path. Any assistance, suggestions, and/or advice on this would be greatly appreciated. Many thanks in advance.",,"['real-analysis', 'measure-theory', 'alternative-proof']"
19,Derivative of $\frac{x}{f(x)}\frac{df}{dx}$,Derivative of,\frac{x}{f(x)}\frac{df}{dx},"Suppose we have a function $f(x):\mathbb R^+\to\mathbb R^+$ that satisfies: 1) $0\leq\frac{df}{dx} \leq 1$ 2) $f(0) = 0$, then do we have $$\frac{d}{dx}\left(\frac{x}{f(x)}\frac{df}{dx} \right)\leq 0?$$","Suppose we have a function $f(x):\mathbb R^+\to\mathbb R^+$ that satisfies: 1) $0\leq\frac{df}{dx} \leq 1$ 2) $f(0) = 0$, then do we have $$\frac{d}{dx}\left(\frac{x}{f(x)}\frac{df}{dx} \right)\leq 0?$$",,"['real-analysis', 'functional-analysis', 'derivatives']"
20,Is the sequence $u_n=n\sum_{i=1}^\infty2^{-i}(1-2^{-i})^n$ convergent when $n\to \infty$?,Is the sequence  convergent when ?,u_n=n\sum_{i=1}^\infty2^{-i}(1-2^{-i})^n n\to \infty,"Problem Is the sequence $u_n=n\sum_{i=1}^\infty2^{-i}(1-2^{-i})^n$ convergent when $n\to \infty$ ? Progress If $i\leq \log n$ then $(1-2^{-i})^n<e^{-\frac{n}{2^i}}$ . Unfortunately, I do not know what to do next","Problem Is the sequence convergent when ? Progress If then . Unfortunately, I do not know what to do next",u_n=n\sum_{i=1}^\infty2^{-i}(1-2^{-i})^n n\to \infty i\leq \log n (1-2^{-i})^n<e^{-\frac{n}{2^i}},"['real-analysis', 'sequences-and-series']"
21,An algebraic topology proof of a result from analysis,An algebraic topology proof of a result from analysis,,"A colleague of mine recently brought up the following result from real analysis: Theorem: If $f:\mathbb{R}^2\to\mathbb{R}^2$ is continuous and $|f(x)-f(y)|\geq |x-y|$ for all $x,y$, then $f$ is onto. It feels like there should be a slick proof of this result using the fundamental or singular homology groups but I couldn't quite find a way to make it work. Does anyone know of a nice proof using algebraic topology?","A colleague of mine recently brought up the following result from real analysis: Theorem: If $f:\mathbb{R}^2\to\mathbb{R}^2$ is continuous and $|f(x)-f(y)|\geq |x-y|$ for all $x,y$, then $f$ is onto. It feels like there should be a slick proof of this result using the fundamental or singular homology groups but I couldn't quite find a way to make it work. Does anyone know of a nice proof using algebraic topology?",,"['real-analysis', 'algebraic-topology', 'fundamental-groups']"
22,Does every inner product space has an orthogonal basis?,Does every inner product space has an orthogonal basis?,,"It is proved that every inner product space has a basis $W$, but I am not sure if every inner product space has an orthogonal basis? It is known that every inner space has a maximal orthogonal set $S$, so if I want to prove the conclusion, we need to show that if $w\in W$ is linearly independent to $S$, then we can construct a vector that is orthogonal to $\mbox{span}\{S\}$, which leads to contradiction, but it seems needs the condition that the space is complete. If not, can anyone give me a counterexample?","It is proved that every inner product space has a basis $W$, but I am not sure if every inner product space has an orthogonal basis? It is known that every inner space has a maximal orthogonal set $S$, so if I want to prove the conclusion, we need to show that if $w\in W$ is linearly independent to $S$, then we can construct a vector that is orthogonal to $\mbox{span}\{S\}$, which leads to contradiction, but it seems needs the condition that the space is complete. If not, can anyone give me a counterexample?",,['real-analysis']
23,Series $\sum \frac{1}{n^2\sin^3n}$,Series,\sum \frac{1}{n^2\sin^3n},Question : Show that series $\sum \cfrac{1}{n^{2}\sin^{3}n}$ is divergent. Hint: Show that $$\sum \frac{1}{n|\sin(n)|}$$ is divergent. I am interested in other possible proofs for this question.,Question : Show that series $\sum \cfrac{1}{n^{2}\sin^{3}n}$ is divergent. Hint: Show that $$\sum \frac{1}{n|\sin(n)|}$$ is divergent. I am interested in other possible proofs for this question.,,"['real-analysis', 'sequences-and-series', 'summation']"
24,Can monsters of real analysis be tamed in this way?,Can monsters of real analysis be tamed in this way?,,"Consider the Weierstrass Function (somewhat generalized for arbitrary wavelengths $\,\lambda > 0$ ): $$ W(x) = \sum_{n=1}^\infty \frac{\sin\left(n^2\,2\pi/\lambda\,x\right)}{n^2} $$ $W(x)$ is an example of a function that is everywhere continuous but nowhere differentiable (Or, for the nitpickers among us: differentiable only on a set of points of measure zero). We seek to remove the pathological character of the function $W(x)$ in  very much the same way as has been tried with removing singularities in Could this be called Renormalization? , namely by convolution with a ""broadened"" Dirac-Delta function, in our case a Gaussian: $$ \int_{-\infty}^{^\infty} W(\xi) \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}(x-\xi)^2/\sigma^2}\,d\xi =\sum_{n=1}^\infty \frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{^\infty} \frac{e^{-\frac{1}{2}(x-\xi)^2/\sigma^2}\left[e^{i\,n^2\,2\pi/\lambda\,\xi}-e^{-i\,n^2\,2\pi/\lambda\,\xi}\right]}{2\,i\,n^2} $$ Exponents are rewritten as follows: $$ -\frac{1}{2}(x-\xi)^2/\sigma^2 \pm i\,n^2\frac{2\pi}{\lambda}\xi = - \frac{1}{2}\left(\frac{\xi}{\sigma}\right)^2 + \frac{x}{\sigma}\frac{\xi}{\sigma} \pm i\,n^2\frac{2\pi}{\lambda}\sigma\frac{\xi}{\sigma} - \frac{1}{2}\left(\frac{x}{\sigma}\right)^2 = -\frac{1}{2}\left[\frac{\xi}{\sigma}-\left(\frac{x}{\sigma} \pm i\,n^2\frac{2\pi}{\lambda}\sigma\right)\right]^2 -\frac{1}{2}\left(n^2\frac{2\pi}{\lambda}\sigma\right)^2 \pm i\,n^2\frac{2\pi}{\lambda}x $$ Giving for the convolution integral: $$ \sum_{n=1}^\infty \int_{-\infty}^{^\infty} e^{-\frac{1}{2}\left[\xi/\sigma-(x/\sigma \pm i\,n^2\,2\pi/\lambda\,\sigma)\right]^2}\,d(\xi/\sigma)\times\sigma\times\frac{1}{\sigma\sqrt{2\pi}}\;(=1)\\ \times e^{-\frac{1}{2}\left(n^2\,2\pi/\lambda\,\sigma\right)^2}\frac{e^{i\,n^2\,2\pi/\lambda\,x} - e^{-i\,n^2\,2\pi/\lambda\,x}}{2i\,n^2}\qquad \Longrightarrow \\ \overline{W}(x) = \sum_{n=1}^\infty e^{-\frac{1}{2}\left(n^2\,2\pi/\lambda\,\sigma\right)^2}\frac{\sin\left(n^2\,2\pi/\lambda\,x\right)}{n^2} $$ The idea is that a monster function which is ""smoothed"" in this way might be differentiable as well as continuous. A possible argument is that the derivative of the convolution integral can also be calculated in the following way: $$ \frac{d}{dx} \int_{-\infty}^{^\infty} W(\xi) \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}(x-\xi)^2/\sigma^2}\,d\xi = \int_{-\infty}^{^\infty} W(\xi) \frac{1}{\sigma\sqrt{2\pi}} \left(-\frac{x-\xi}{\sigma^2}\right) e^{-\frac{1}{2}(x-\xi)^2/\sigma^2}\,d\xi $$ But I'm not sure if this counts as a proof. Hence the question: is the Weierstrass monster function ""tamed"" by this ""renormalization"" procedure and has it become differentiable everywhere indeed?","Consider the Weierstrass Function (somewhat generalized for arbitrary wavelengths $\,\lambda > 0$ ): $$ W(x) = \sum_{n=1}^\infty \frac{\sin\left(n^2\,2\pi/\lambda\,x\right)}{n^2} $$ $W(x)$ is an example of a function that is everywhere continuous but nowhere differentiable (Or, for the nitpickers among us: differentiable only on a set of points of measure zero). We seek to remove the pathological character of the function $W(x)$ in  very much the same way as has been tried with removing singularities in Could this be called Renormalization? , namely by convolution with a ""broadened"" Dirac-Delta function, in our case a Gaussian: $$ \int_{-\infty}^{^\infty} W(\xi) \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}(x-\xi)^2/\sigma^2}\,d\xi =\sum_{n=1}^\infty \frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{^\infty} \frac{e^{-\frac{1}{2}(x-\xi)^2/\sigma^2}\left[e^{i\,n^2\,2\pi/\lambda\,\xi}-e^{-i\,n^2\,2\pi/\lambda\,\xi}\right]}{2\,i\,n^2} $$ Exponents are rewritten as follows: $$ -\frac{1}{2}(x-\xi)^2/\sigma^2 \pm i\,n^2\frac{2\pi}{\lambda}\xi = - \frac{1}{2}\left(\frac{\xi}{\sigma}\right)^2 + \frac{x}{\sigma}\frac{\xi}{\sigma} \pm i\,n^2\frac{2\pi}{\lambda}\sigma\frac{\xi}{\sigma} - \frac{1}{2}\left(\frac{x}{\sigma}\right)^2 = -\frac{1}{2}\left[\frac{\xi}{\sigma}-\left(\frac{x}{\sigma} \pm i\,n^2\frac{2\pi}{\lambda}\sigma\right)\right]^2 -\frac{1}{2}\left(n^2\frac{2\pi}{\lambda}\sigma\right)^2 \pm i\,n^2\frac{2\pi}{\lambda}x $$ Giving for the convolution integral: $$ \sum_{n=1}^\infty \int_{-\infty}^{^\infty} e^{-\frac{1}{2}\left[\xi/\sigma-(x/\sigma \pm i\,n^2\,2\pi/\lambda\,\sigma)\right]^2}\,d(\xi/\sigma)\times\sigma\times\frac{1}{\sigma\sqrt{2\pi}}\;(=1)\\ \times e^{-\frac{1}{2}\left(n^2\,2\pi/\lambda\,\sigma\right)^2}\frac{e^{i\,n^2\,2\pi/\lambda\,x} - e^{-i\,n^2\,2\pi/\lambda\,x}}{2i\,n^2}\qquad \Longrightarrow \\ \overline{W}(x) = \sum_{n=1}^\infty e^{-\frac{1}{2}\left(n^2\,2\pi/\lambda\,\sigma\right)^2}\frac{\sin\left(n^2\,2\pi/\lambda\,x\right)}{n^2} $$ The idea is that a monster function which is ""smoothed"" in this way might be differentiable as well as continuous. A possible argument is that the derivative of the convolution integral can also be calculated in the following way: $$ \frac{d}{dx} \int_{-\infty}^{^\infty} W(\xi) \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}(x-\xi)^2/\sigma^2}\,d\xi = \int_{-\infty}^{^\infty} W(\xi) \frac{1}{\sigma\sqrt{2\pi}} \left(-\frac{x-\xi}{\sigma^2}\right) e^{-\frac{1}{2}(x-\xi)^2/\sigma^2}\,d\xi $$ But I'm not sure if this counts as a proof. Hence the question: is the Weierstrass monster function ""tamed"" by this ""renormalization"" procedure and has it become differentiable everywhere indeed?",,"['real-analysis', 'functions', 'derivatives', 'continuity', 'fractals']"
25,Dirichlet series,Dirichlet series,,"Suppose that the series $\sum \limits_{n=1}^{\infty}\dfrac{a_n}{n^{\sigma}} \quad(\sigma>0)$ converges. Prove that $\lim \limits_{n\to \infty}\dfrac{a_1+a_2+\dots+a_n}{n^{\sigma}}=0$. I applied the Abel transformation, but unsuccessfully.","Suppose that the series $\sum \limits_{n=1}^{\infty}\dfrac{a_n}{n^{\sigma}} \quad(\sigma>0)$ converges. Prove that $\lim \limits_{n\to \infty}\dfrac{a_1+a_2+\dots+a_n}{n^{\sigma}}=0$. I applied the Abel transformation, but unsuccessfully.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
26,Computing the Fourier transform of the surface measure on $S^2$,Computing the Fourier transform of the surface measure on,S^2,"I am almost a beginner in the topics of Fourier transform. So, I am asking this question here. Let $n=3$ and let $\mu_t$ denote surface measure on the sphere $|x|=t$ . Then how do we show that $$ \frac{\text{sin} 2\pi t|\xi|}{2\pi|\xi|}=\frac{1}{4\pi t}\hat{\mu_{t}}(\xi) $$ where $\hat{\mu_{t}}$ is the Fourier transform of $\mu_t$ .","I am almost a beginner in the topics of Fourier transform. So, I am asking this question here. Let and let denote surface measure on the sphere . Then how do we show that where is the Fourier transform of .","n=3 \mu_t |x|=t 
\frac{\text{sin} 2\pi t|\xi|}{2\pi|\xi|}=\frac{1}{4\pi t}\hat{\mu_{t}}(\xi)
 \hat{\mu_{t}} \mu_t","['real-analysis', 'measure-theory']"
27,submodular-like functions on $\mathbb{R}$,submodular-like functions on,\mathbb{R},"The definition of submodularity led me to define a type of function on numbers (I suppose ordered rings, but consider the reals for now) as $f(x) + f(y) \ge f(x+y) + f(xy)$. Such functions exist: for example, the constant functions. I can infer some basic properties of these functions: plugging in $x = 1$ and $y = c-1$ yields $f(1) \ge f(c)$ so 1 is a global maximum; similarly, $x = 2$ and $y = 2$ yields $f(2) \ge f(4)$. Another property of interest is that if $f$ is differentiable at $x$ and 0, $y = \epsilon$ gives $f(x+\epsilon) - f(x) \le f(\epsilon) - f(x\epsilon)$ and letting $\epsilon \rightarrow 0$ yields $f'(x) \le (1-x)f'(0)$. (edit: thanks kingW3) Can anyone characterize such functions more completely? Are there any such functions beside the constant ones? I am having difficulty coming up with one.","The definition of submodularity led me to define a type of function on numbers (I suppose ordered rings, but consider the reals for now) as $f(x) + f(y) \ge f(x+y) + f(xy)$. Such functions exist: for example, the constant functions. I can infer some basic properties of these functions: plugging in $x = 1$ and $y = c-1$ yields $f(1) \ge f(c)$ so 1 is a global maximum; similarly, $x = 2$ and $y = 2$ yields $f(2) \ge f(4)$. Another property of interest is that if $f$ is differentiable at $x$ and 0, $y = \epsilon$ gives $f(x+\epsilon) - f(x) \le f(\epsilon) - f(x\epsilon)$ and letting $\epsilon \rightarrow 0$ yields $f'(x) \le (1-x)f'(0)$. (edit: thanks kingW3) Can anyone characterize such functions more completely? Are there any such functions beside the constant ones? I am having difficulty coming up with one.",,"['real-analysis', 'functions', 'functional-equations', 'functional-inequalities']"
28,Proof of Darboux's theorem,Proof of Darboux's theorem,,"I tried to prove Darboux's theorem. It is the following theorem: Let $f: [a,b]\to \mathbb R$ be a differentiable function and let $f'(a) < \alpha < f'(b)$. Then there exists $c \in [a,b]$ with $f'(c) = \alpha$. Please could somebody check my proof? Define $g(x) = f(x) - \alpha x$. Then $g$ is continuous and because $[a,b]$ is compact $g$ attains its minimum on $[a,b]$. Let $x_m \in [a,b]$ be such that $g(x_m) \le g(x)$ for all $x\in [a,b]$. If $x_m \in (a,b)$ then $g'(x_m) = 0 = f'(x_m) - \alpha$ which shows the claim. If $x_m = a$ then $g'(x_m) = g'(a) < 0$. Because $g$ is continuous and $g'(a) < 0$ there exists $\delta>0$ such that if $x \in (a,a+\delta)$ then $g(x) < g(a) = g(x_m)$. But this is a contradiction because $x_m$ is the minimum. If $x_m = b$ then again there is $\delta>0$ such that if $x \in (b-\delta, b)$ then $g(x)<g(b)=g(x_m)$ because  $g'(b) > 0$. Again this is a contradiction. It follows that the minimum is attained in the interior $(a,b)$.","I tried to prove Darboux's theorem. It is the following theorem: Let $f: [a,b]\to \mathbb R$ be a differentiable function and let $f'(a) < \alpha < f'(b)$. Then there exists $c \in [a,b]$ with $f'(c) = \alpha$. Please could somebody check my proof? Define $g(x) = f(x) - \alpha x$. Then $g$ is continuous and because $[a,b]$ is compact $g$ attains its minimum on $[a,b]$. Let $x_m \in [a,b]$ be such that $g(x_m) \le g(x)$ for all $x\in [a,b]$. If $x_m \in (a,b)$ then $g'(x_m) = 0 = f'(x_m) - \alpha$ which shows the claim. If $x_m = a$ then $g'(x_m) = g'(a) < 0$. Because $g$ is continuous and $g'(a) < 0$ there exists $\delta>0$ such that if $x \in (a,a+\delta)$ then $g(x) < g(a) = g(x_m)$. But this is a contradiction because $x_m$ is the minimum. If $x_m = b$ then again there is $\delta>0$ such that if $x \in (b-\delta, b)$ then $g(x)<g(b)=g(x_m)$ because  $g'(b) > 0$. Again this is a contradiction. It follows that the minimum is attained in the interior $(a,b)$.",,"['real-analysis', 'proof-verification']"
29,Help with epsilon-delta proof that 1/(x^2) is continuous at a point.,Help with epsilon-delta proof that 1/(x^2) is continuous at a point.,,"I'm trying to prove that $\lim_{x \to x_0} \frac{1}{ x^2 } = \frac{1}{ {x_0}^2 }$. I know this means that for all $\epsilon > 0$, I must show that there exists a $\delta > 0$ such that $\left | x - x_0 \right | < \delta \Rightarrow  \left | \frac{1}{ x^2 } - \frac{1}{ {x_0}^2 } \right | < \epsilon$. Since $f(x) = \frac{1}{x^2}$ is an even function, I'll restrict the domain to $x>0$. After some manipulation of $\left | \frac{1}{ x^2 } - \frac{1}{ {x_0}^2 } \right | < \epsilon$ I have $\left | x - x_0 \right | < \epsilon\frac{x^2{x_0}^2}{x+{x_0}}$. I know that I can't just let $\delta = \epsilon\frac{x^2{x_0}^2}{x+{x_0}}$ since $\delta$ should not depend on $x$, so I need something else. This is where I'm stuck. I think I need to choose an upper bound for $\left|x-x_0\right|$ which depends on $x_0$ and find a sufficient $\delta$ from this restriction, but I'm not sure how to go about this.","I'm trying to prove that $\lim_{x \to x_0} \frac{1}{ x^2 } = \frac{1}{ {x_0}^2 }$. I know this means that for all $\epsilon > 0$, I must show that there exists a $\delta > 0$ such that $\left | x - x_0 \right | < \delta \Rightarrow  \left | \frac{1}{ x^2 } - \frac{1}{ {x_0}^2 } \right | < \epsilon$. Since $f(x) = \frac{1}{x^2}$ is an even function, I'll restrict the domain to $x>0$. After some manipulation of $\left | \frac{1}{ x^2 } - \frac{1}{ {x_0}^2 } \right | < \epsilon$ I have $\left | x - x_0 \right | < \epsilon\frac{x^2{x_0}^2}{x+{x_0}}$. I know that I can't just let $\delta = \epsilon\frac{x^2{x_0}^2}{x+{x_0}}$ since $\delta$ should not depend on $x$, so I need something else. This is where I'm stuck. I think I need to choose an upper bound for $\left|x-x_0\right|$ which depends on $x_0$ and find a sufficient $\delta$ from this restriction, but I'm not sure how to go about this.",,"['real-analysis', 'limits', 'epsilon-delta']"
30,The Limit: $\lim_{x \to \infty}\frac{e^{f(x+a)}}{e^{f(x)}}$,The Limit:,\lim_{x \to \infty}\frac{e^{f(x+a)}}{e^{f(x)}},"I'm doing some challenge review problems and I was wondering whether this proof looked correct: Suppose that $f: \mathbb{R} \rightarrow \mathbb{R}$ be a differentiable function with $\lim_{x \to \infty}f'(x)=1$ and $a \in \mathbb{R}$. Prove the limit exists and find it. $$ \lim_{x \to \infty} \frac{e^{f(x+a)}}{e^{f(x)}} $$ Here is what I did, $$ \begin{align} \lim_{x \to \infty} \frac{e^{f(x+a)}}{e^{f(x)}}&=\lim_{n \to \infty}e^{f(x+a)}e^{-f(x)} \\ &=\lim_{x \to \infty}e^{f(x+a)-f(x)} \\ &=\lim_{x \to \infty}\left(e^{f(x+a)-f(x)}\right)^\frac{a}{a} \\ &=\lim_{x \to \infty}\left(e^\frac{f(x+a)-f(x)}{a}\right)^a \\ &=\left(e^{\lim_{x \to \infty}\frac{f(x+a)-f(x)}{a}}\right)^a \\ &=\left(e^{\lim_{x \to \infty}f'(x)}\right)^a \\ &=(e^1)^a\\ &=e^a \end{align} $$ Though missing verbal explanation of the individual steps to be a 'good' proof, does this look like the right idea?","I'm doing some challenge review problems and I was wondering whether this proof looked correct: Suppose that $f: \mathbb{R} \rightarrow \mathbb{R}$ be a differentiable function with $\lim_{x \to \infty}f'(x)=1$ and $a \in \mathbb{R}$. Prove the limit exists and find it. $$ \lim_{x \to \infty} \frac{e^{f(x+a)}}{e^{f(x)}} $$ Here is what I did, $$ \begin{align} \lim_{x \to \infty} \frac{e^{f(x+a)}}{e^{f(x)}}&=\lim_{n \to \infty}e^{f(x+a)}e^{-f(x)} \\ &=\lim_{x \to \infty}e^{f(x+a)-f(x)} \\ &=\lim_{x \to \infty}\left(e^{f(x+a)-f(x)}\right)^\frac{a}{a} \\ &=\lim_{x \to \infty}\left(e^\frac{f(x+a)-f(x)}{a}\right)^a \\ &=\left(e^{\lim_{x \to \infty}\frac{f(x+a)-f(x)}{a}}\right)^a \\ &=\left(e^{\lim_{x \to \infty}f'(x)}\right)^a \\ &=(e^1)^a\\ &=e^a \end{align} $$ Though missing verbal explanation of the individual steps to be a 'good' proof, does this look like the right idea?",,"['real-analysis', 'limits', 'proof-verification']"
31,Find min of $\frac{\left( \sum kx_k \right) \left( \sum x^2_k \right)}{\left( \sum x_k \right)^3}$,Find min of,\frac{\left( \sum kx_k \right) \left( \sum x^2_k \right)}{\left( \sum x_k \right)^3},"With $n \ge 2$ and $x_1,\ x_2,\ \dots,\ x_n > 0$. Find the minimum of: $$ M = \frac{(x_1 + 2 x_2 + ...+ nx_n)( x^2_1 + x^2_2 +...+x^2_n)} {\left( x_1 + x_2 +...+ x_n \right)^3}$$ For specific $n$, for example, $n = 3$, I can get: $\min \{M\} = \frac{2}{27}(10 - \sqrt 2)$ But in above generalized problem, how can I solve that ? Can I get the result by using sage or mathematica ?","With $n \ge 2$ and $x_1,\ x_2,\ \dots,\ x_n > 0$. Find the minimum of: $$ M = \frac{(x_1 + 2 x_2 + ...+ nx_n)( x^2_1 + x^2_2 +...+x^2_n)} {\left( x_1 + x_2 +...+ x_n \right)^3}$$ For specific $n$, for example, $n = 3$, I can get: $\min \{M\} = \frac{2}{27}(10 - \sqrt 2)$ But in above generalized problem, how can I solve that ? Can I get the result by using sage or mathematica ?",,"['real-analysis', 'inequality', 'optimization', 'nonlinear-optimization', 'mathematica']"
32,Uniform convergence and differentiation proof of remark 7.17 in Rudin's mathematical analysis,Uniform convergence and differentiation proof of remark 7.17 in Rudin's mathematical analysis,,"Rudin page 152 Theorem 7.17 : Suppose $\{f_n\}$ a sequence of functions, differentiable on $[a,b]$ and such that $\{f_n(x_0)\}$ converges for some point $x_0$ on [a,b]. If $\{f_n'\}$ converges uniformly on [a,b], then $\{f_n\}$ converges uniformly on [a,b] to $f$ and $f'(x)=lim_{n \to \infty} f_n'(x)$ with $a\le x \le b$. Remark : if the continuity of the function $f′_n$ is assumed in addition to the above hypothesis, then a much shorter proof can be based on Theorem 7.16 and the fundamental theorem of calculus. Theorem 7.16 states: Let $\alpha$ be monotonically increasing on [a,b]. Suppose $f_n$ is in Reimann(alpha) on [a,b], for n=1,2,3,.... and suppose $f_n \to f$ uniformly on [a,b]. Then f is in Reimann(alpha) on [a,b] and $$\int_a^bfd\alpha = \lim_{n \to \infty} \int_a^bf_nd\alpha .$$ Prove the Remark.  I think I have a good enough understanding of theorem 7.17 but I am very interested in the proof of the remark to help gain a better understanding.I cannot seem to make the connection. Would anyone be able to point me in the right direction?","Rudin page 152 Theorem 7.17 : Suppose $\{f_n\}$ a sequence of functions, differentiable on $[a,b]$ and such that $\{f_n(x_0)\}$ converges for some point $x_0$ on [a,b]. If $\{f_n'\}$ converges uniformly on [a,b], then $\{f_n\}$ converges uniformly on [a,b] to $f$ and $f'(x)=lim_{n \to \infty} f_n'(x)$ with $a\le x \le b$. Remark : if the continuity of the function $f′_n$ is assumed in addition to the above hypothesis, then a much shorter proof can be based on Theorem 7.16 and the fundamental theorem of calculus. Theorem 7.16 states: Let $\alpha$ be monotonically increasing on [a,b]. Suppose $f_n$ is in Reimann(alpha) on [a,b], for n=1,2,3,.... and suppose $f_n \to f$ uniformly on [a,b]. Then f is in Reimann(alpha) on [a,b] and $$\int_a^bfd\alpha = \lim_{n \to \infty} \int_a^bf_nd\alpha .$$ Prove the Remark.  I think I have a good enough understanding of theorem 7.17 but I am very interested in the proof of the remark to help gain a better understanding.I cannot seem to make the connection. Would anyone be able to point me in the right direction?",,"['real-analysis', 'sequences-and-series', 'derivatives', 'convergence-divergence', 'uniform-convergence']"
33,Prove $\lim\limits_{n\to\infty}n^{-1/(p+1)}(1^{1^p}\cdot 2^{2^p}\cdots n^{n^p})^{n^{-p-1}}=e^{-1/(p+1)^2}$,Prove,\lim\limits_{n\to\infty}n^{-1/(p+1)}(1^{1^p}\cdot 2^{2^p}\cdots n^{n^p})^{n^{-p-1}}=e^{-1/(p+1)^2},Prove that for real number $p\geq  0$ we have $$\lim_{n\to\infty}\frac{(1^{1^p} \cdot2^{2^p}\cdots n^{n^p})^\frac{1}{n^{p+1}}}{n^\frac{1}{(p+1)}} = \exp\left(\dfrac{-1}{(p+1)^2}\right).$$,Prove that for real number $p\geq  0$ we have $$\lim_{n\to\infty}\frac{(1^{1^p} \cdot2^{2^p}\cdots n^{n^p})^\frac{1}{n^{p+1}}}{n^\frac{1}{(p+1)}} = \exp\left(\dfrac{-1}{(p+1)^2}\right).$$,,"['real-analysis', 'limits']"
34,Proving well definedness of addition in real numbers constrructed from cauchy sequences.,Proving well definedness of addition in real numbers constrructed from cauchy sequences.,,"While studying real analysis, I got confused on the following issue. Suppose we construct real numbers as equivalence classes of cauchy sequences. Let $x = (a_n)$ and $y= (b_n)$ be two cauchy sequences,  representing real numbers $x$ and $y$. Addition operation $x+y$ is defined as $x+y = (a_n + b_n)$. To check if this operation is well defined, we substitute $x = (a_n)$ with some real number $x' = (c_n)$ and verify that $x+y  = x'+y$. We also repeat it for $y$. i.e. we verify that  $x+y = x+y'$. Question: Instead of checking that $x+y = x+y'$ and $x'+y = x+y$ seperately, would it suffice to check that $x+y = x' + y'$ in a single operation in order to show that addition is well defined for real numbers. Would it hurt to checking well  definedness? Can any one explain me the logic behind ?","While studying real analysis, I got confused on the following issue. Suppose we construct real numbers as equivalence classes of cauchy sequences. Let $x = (a_n)$ and $y= (b_n)$ be two cauchy sequences,  representing real numbers $x$ and $y$. Addition operation $x+y$ is defined as $x+y = (a_n + b_n)$. To check if this operation is well defined, we substitute $x = (a_n)$ with some real number $x' = (c_n)$ and verify that $x+y  = x'+y$. We also repeat it for $y$. i.e. we verify that  $x+y = x+y'$. Question: Instead of checking that $x+y = x+y'$ and $x'+y = x+y$ seperately, would it suffice to check that $x+y = x' + y'$ in a single operation in order to show that addition is well defined for real numbers. Would it hurt to checking well  definedness? Can any one explain me the logic behind ?",,"['real-analysis', 'proof-verification', 'real-numbers']"
35,Prove that there exist $c_n>0$ such that $f^{(n)}(c_n)=0 \forall n$,Prove that there exist  such that,c_n>0 f^{(n)}(c_n)=0 \forall n,"Let $f \in C^\infty([0,+\infty),\mathbb{R})$, and $f(0)=\lim\limits_{x \to \infty}f(x)$ Prove that there exist $c_n>0$ such that $f^{(n)}(c_n)=0 $ for all $n$ integer, where $f^{(n)}$ is the n-th derivative of $f$ My attempt: If $f$ is constant then the result is obvious. If not there exist $x_0 \in (0,+\infty)$ such that $f(x_0) \neq f(0)$ Let $y=\frac{1}2 (f(x_0)+f(0))$, then by IVT theorem there exist $a  \in (0,x_0)$ such that $f(a)=y$ Despite $f(0)=\lim_{+\infty}f(x)$, $y$ is an intermediate value of   $f(x_0)$ and $f(x_1)$ with $x_1$ sufficiently large. Then, by IVT there exist $b \in (x_0,x_1]$ such that $f(b)=y$ Therefore by Rolle's theorem on $[a,b]$ we can conclude that   $f'$ vanishes. Unfortunately I don't see any direct proof to show that $f^{(n)}$ vanishes. I managed to find a HINT in a book : Prove that either $\lim\limits_{x \to \infty}f'(x)=0$ or $f'$ admits an infinite number of extrema.","Let $f \in C^\infty([0,+\infty),\mathbb{R})$, and $f(0)=\lim\limits_{x \to \infty}f(x)$ Prove that there exist $c_n>0$ such that $f^{(n)}(c_n)=0 $ for all $n$ integer, where $f^{(n)}$ is the n-th derivative of $f$ My attempt: If $f$ is constant then the result is obvious. If not there exist $x_0 \in (0,+\infty)$ such that $f(x_0) \neq f(0)$ Let $y=\frac{1}2 (f(x_0)+f(0))$, then by IVT theorem there exist $a  \in (0,x_0)$ such that $f(a)=y$ Despite $f(0)=\lim_{+\infty}f(x)$, $y$ is an intermediate value of   $f(x_0)$ and $f(x_1)$ with $x_1$ sufficiently large. Then, by IVT there exist $b \in (x_0,x_1]$ such that $f(b)=y$ Therefore by Rolle's theorem on $[a,b]$ we can conclude that   $f'$ vanishes. Unfortunately I don't see any direct proof to show that $f^{(n)}$ vanishes. I managed to find a HINT in a book : Prove that either $\lim\limits_{x \to \infty}f'(x)=0$ or $f'$ admits an infinite number of extrema.",,"['calculus', 'real-analysis', 'functions']"
36,Non trivial solutions of $g\circ f-f\circ g=g\circ f\circ g$,Non trivial solutions of,g\circ f-f\circ g=g\circ f\circ g,"While thinking of perfect numbers, I came across the functional equation $g\circ f-f\circ g=g\circ f\circ g$ where the unknowns $f$ and $g$ are functions from $\mathbb{R}$ to itself. I only know one non trivial solution: $(f,g)=(x\mapsto 2^x,x\mapsto x-1)$. Can someone tell me whether it is the only one or not? Thanks in advance.","While thinking of perfect numbers, I came across the functional equation $g\circ f-f\circ g=g\circ f\circ g$ where the unknowns $f$ and $g$ are functions from $\mathbb{R}$ to itself. I only know one non trivial solution: $(f,g)=(x\mapsto 2^x,x\mapsto x-1)$. Can someone tell me whether it is the only one or not? Thanks in advance.",,"['real-analysis', 'functional-equations']"
37,"Prove that: $(f(x))^{2} + (f'(x))^{2} \leqslant \max(a,b)$ where $(a,b) \in \mathbb{R}^2$",Prove that:  where,"(f(x))^{2} + (f'(x))^{2} \leqslant \max(a,b) (a,b) \in \mathbb{R}^2","Let $f\in C^2(\mathbb{R},\mathbb{R})$. Assume there exist  $(a,b) \in \mathbb{R}^2$ such that  $\forall x \in \mathbb{R}, (f(x))^{2} \leqslant a$ and $(f'(x))^{2} + (f''(x))^{2} \leqslant b$. Prove that: $$(f(x))^{2} + (f'(x))^{2} \leqslant \max(a,b)$$ for all $x \in \mathbb{R}$. I tried to use taylor lagrange with the integral form of the remainder but I did not succeed.  Ì really don't know how to tackle  the problem. Any suggestion, Hint (or Answer) will be very helpful, Thank you in advance,","Let $f\in C^2(\mathbb{R},\mathbb{R})$. Assume there exist  $(a,b) \in \mathbb{R}^2$ such that  $\forall x \in \mathbb{R}, (f(x))^{2} \leqslant a$ and $(f'(x))^{2} + (f''(x))^{2} \leqslant b$. Prove that: $$(f(x))^{2} + (f'(x))^{2} \leqslant \max(a,b)$$ for all $x \in \mathbb{R}$. I tried to use taylor lagrange with the integral form of the remainder but I did not succeed.  Ì really don't know how to tackle  the problem. Any suggestion, Hint (or Answer) will be very helpful, Thank you in advance,",,['real-analysis']
38,"If $f:[a,b]\to \mathbb R$ is continuous then so is $\max_{t\in [a,x]}f(t)$?",If  is continuous then so is ?,"f:[a,b]\to \mathbb R \max_{t\in [a,x]}f(t)","A problem that I have been thinking: For what kind of functions $f:[a,b]\to \mathbb R$ is  $x\mapsto\sup_{t\in [a,x]}f(t)$ continuous? This is not true for bounded functions, take for example $f(x)=0$ in $[0,1]$ and $f(x)=1$ in $(1,2]$. It is however trivially true for continuous monotone functions. Assuming mere continuity for $f$ I can show right continuity of $\max_{t\in[a,x]}f(t)$: For $\epsilon$ and some $\delta>0$,  $$x\in [x_0-\delta,x_0+\delta]\implies f(x_0)-\epsilon<f(x)<f(x_0)+\epsilon$$ If $x\in [x_0,x_0+\delta]$ then $$\max_{t\in [a,x]}f(t)=\max\left\{\max_{t\in [a,x_0]}f(t),\max_{t\in [x_0,x]}f(t)\right\}$$ If $max_{t\in [a,x_0]}f(t)\ge \max_{t\in [x_0,x]}f(t)$ then $$\left|\max_{t\in [a,x]}f(t)-\max_{t\in [a,x_0]}f(t)\right|=0< \epsilon$$ Otherwise, $$\left|\max_{t\in [a,x]}f(t)-\max_{t\in [a,x_0]}f(t)\right|\le \max_{t\in [x_0,x]}f(t)-\max_{t\in [a,x_0]}f(t)<f(x_0)+\epsilon-\max_{t\in [a,x_0]}f(t)\le \epsilon$$ which proves right continuity. When I tried to prove left continuity I was stuck on showing $$\max_{t\in [x,x_0]}f(t)-\max_{t\in [a,x]}f(t)<\epsilon$$ Can this be circumvented?","A problem that I have been thinking: For what kind of functions $f:[a,b]\to \mathbb R$ is  $x\mapsto\sup_{t\in [a,x]}f(t)$ continuous? This is not true for bounded functions, take for example $f(x)=0$ in $[0,1]$ and $f(x)=1$ in $(1,2]$. It is however trivially true for continuous monotone functions. Assuming mere continuity for $f$ I can show right continuity of $\max_{t\in[a,x]}f(t)$: For $\epsilon$ and some $\delta>0$,  $$x\in [x_0-\delta,x_0+\delta]\implies f(x_0)-\epsilon<f(x)<f(x_0)+\epsilon$$ If $x\in [x_0,x_0+\delta]$ then $$\max_{t\in [a,x]}f(t)=\max\left\{\max_{t\in [a,x_0]}f(t),\max_{t\in [x_0,x]}f(t)\right\}$$ If $max_{t\in [a,x_0]}f(t)\ge \max_{t\in [x_0,x]}f(t)$ then $$\left|\max_{t\in [a,x]}f(t)-\max_{t\in [a,x_0]}f(t)\right|=0< \epsilon$$ Otherwise, $$\left|\max_{t\in [a,x]}f(t)-\max_{t\in [a,x_0]}f(t)\right|\le \max_{t\in [x_0,x]}f(t)-\max_{t\in [a,x_0]}f(t)<f(x_0)+\epsilon-\max_{t\in [a,x_0]}f(t)\le \epsilon$$ which proves right continuity. When I tried to prove left continuity I was stuck on showing $$\max_{t\in [x,x_0]}f(t)-\max_{t\in [a,x]}f(t)<\epsilon$$ Can this be circumvented?",,"['real-analysis', 'general-topology', 'continuity']"
39,Can the following integral be computed?,Can the following integral be computed?,,Can the following integral be computed?,Can the following integral be computed?,,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
40,Real analysis derivate problem. How can I prove this?,Real analysis derivate problem. How can I prove this?,,"Let $f$ be differentiable in $[a,b]$ and $f(a)=0$. If $\exists M \in R$ such that $\vert f'(x) \vert \leq M \vert f(x)\vert \ $,  $\forall x \in [a,b]$, then $f(x)=0, \ \forall x \in [a,b]$. $\\$ I have an idea, but I can´t see it clear. I have arrived to the inequality $\vert f(x)\vert \leq  K\vert f(y)\vert $ for $y \in (a,x)$, supposing $k>0$ and repeating the process $\vert f(x)\vert \leq  K^n\vert f(z)\vert$ for $a<z<y<x$. If $K<1$, then if $n \rightarrow \infty \ $, $f(x)=0$. That´s a sort of intuitive idea.","Let $f$ be differentiable in $[a,b]$ and $f(a)=0$. If $\exists M \in R$ such that $\vert f'(x) \vert \leq M \vert f(x)\vert \ $,  $\forall x \in [a,b]$, then $f(x)=0, \ \forall x \in [a,b]$. $\\$ I have an idea, but I can´t see it clear. I have arrived to the inequality $\vert f(x)\vert \leq  K\vert f(y)\vert $ for $y \in (a,x)$, supposing $k>0$ and repeating the process $\vert f(x)\vert \leq  K^n\vert f(z)\vert$ for $a<z<y<x$. If $K<1$, then if $n \rightarrow \infty \ $, $f(x)=0$. That´s a sort of intuitive idea.",,['real-analysis']
41,weakly convergence of periodic functions,weakly convergence of periodic functions,,"Let $ \displaystyle I \subset \mathbb R$ be a bounded interval and    $\displaystyle 1 < p \leq \infty $. Let $ f \displaystyle \in L^\infty  (\mathbb R) $ periodic function with $\displaystyle f(x+T)=f(x), \quad  \forall x \in I$.  Consider now the sequence of functions   $\displaystyle f_n (x):= f(nx), \quad n \in \mathbb N$. Prove that: (i) $\displaystyle f_n \to \frac{1}{T} \int_0^T f(x) dx $ weakly in $\displaystyle L^p (I)$ forall $ \displaystyle 1<p < \infty $. (ii) $\displaystyle f_n \to \frac{1}{T} \int_0^T f(x) dx $ weakly* in $\displaystyle L^\infty (I)$. I thought the following about the first one: First of all we can assume that $\displaystyle \frac{1}{T} \int_0^T f(x) dx =0$, otherwise work with $\displaystyle f(x) -  \frac{1}{T} \int_0^T f(x) dx =0$. Consider now an arbitrary compact sub-interval of $I$, i.e., $\displaystyle [a,b] \subset I$ . Then we have that: $\displaystyle \int_a^b f_n(x) dx= \int_a^b f(nx) dx = \frac{1}{n} \int_{na}^{nb} f(x) dx = \frac{1}{n} \left( F(nb) -F(na) \right) \to 0 $, as $n \to \infty$ where $\displaystyle F(x) = \int_0^x f(t) dt , \quad x \in I$. Since the sub-interval $[a,b]$ was rbitrary, we can conclude that for every step function $\phi$ we have that: $\displaystyle \int_I f_n(x) \phi (x) dx \to 0$ , as $n \to \infty $. Because the step functions are dense in $L^q(I)$ (is the dual space of $L^p(I)$) we have that $\displaystyle \int_I f_n(x) g(x) dx \to 0$ as $n \to \infty $, for every $g \in L^q(I)$. From here how can I get the desired result? Also I would like if it is possible some hints for the (ii) Any help would be really appreciated. Thakning in advance.","Let $ \displaystyle I \subset \mathbb R$ be a bounded interval and    $\displaystyle 1 < p \leq \infty $. Let $ f \displaystyle \in L^\infty  (\mathbb R) $ periodic function with $\displaystyle f(x+T)=f(x), \quad  \forall x \in I$.  Consider now the sequence of functions   $\displaystyle f_n (x):= f(nx), \quad n \in \mathbb N$. Prove that: (i) $\displaystyle f_n \to \frac{1}{T} \int_0^T f(x) dx $ weakly in $\displaystyle L^p (I)$ forall $ \displaystyle 1<p < \infty $. (ii) $\displaystyle f_n \to \frac{1}{T} \int_0^T f(x) dx $ weakly* in $\displaystyle L^\infty (I)$. I thought the following about the first one: First of all we can assume that $\displaystyle \frac{1}{T} \int_0^T f(x) dx =0$, otherwise work with $\displaystyle f(x) -  \frac{1}{T} \int_0^T f(x) dx =0$. Consider now an arbitrary compact sub-interval of $I$, i.e., $\displaystyle [a,b] \subset I$ . Then we have that: $\displaystyle \int_a^b f_n(x) dx= \int_a^b f(nx) dx = \frac{1}{n} \int_{na}^{nb} f(x) dx = \frac{1}{n} \left( F(nb) -F(na) \right) \to 0 $, as $n \to \infty$ where $\displaystyle F(x) = \int_0^x f(t) dt , \quad x \in I$. Since the sub-interval $[a,b]$ was rbitrary, we can conclude that for every step function $\phi$ we have that: $\displaystyle \int_I f_n(x) \phi (x) dx \to 0$ , as $n \to \infty $. Because the step functions are dense in $L^q(I)$ (is the dual space of $L^p(I)$) we have that $\displaystyle \int_I f_n(x) g(x) dx \to 0$ as $n \to \infty $, for every $g \in L^q(I)$. From here how can I get the desired result? Also I would like if it is possible some hints for the (ii) Any help would be really appreciated. Thakning in advance.",,"['real-analysis', 'functional-analysis', 'weak-convergence']"
42,"If $0\leq f_n$ and $f_n\rightarrow f$ a.e and $\lim\int_Xf_n=\int_X f$, is it true that $\lim\int_Ef_n=\int_E f$ for all $E\in\mathcal{M}$.","If  and  a.e and , is it true that  for all .",0\leq f_n f_n\rightarrow f \lim\int_Xf_n=\int_X f \lim\int_Ef_n=\int_E f E\in\mathcal{M},"If $0\leq f_n$ and  $f_n\rightarrow f$ a.e and $\lim\int_Xf_n=\int_X f$, p,rove or disprove that $\lim\int_Ef_n=\int_E f$ for all $E\in\mathcal{M}$. I think it is true. It is easy to see $\lim\int_Ef_n\geq\int_E f$ using Fatou's Lemma, yet I could not show that $\lim\int_Ef_n\leq\int_E f$. To disprove, each time I am either frustrated by either a.e convergence or non-negativity. Thanks in advance,","If $0\leq f_n$ and  $f_n\rightarrow f$ a.e and $\lim\int_Xf_n=\int_X f$, p,rove or disprove that $\lim\int_Ef_n=\int_E f$ for all $E\in\mathcal{M}$. I think it is true. It is easy to see $\lim\int_Ef_n\geq\int_E f$ using Fatou's Lemma, yet I could not show that $\lim\int_Ef_n\leq\int_E f$. To disprove, each time I am either frustrated by either a.e convergence or non-negativity. Thanks in advance,",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
43,Cannot follow proof that between two reals there is a rational.,Cannot follow proof that between two reals there is a rational.,,"I am following a textbook and as is good practice, I am only skipping things I can do. this is self-learning I always struggle with chapter 1, the one that builds the ""axioms"", hate it. Anyway, I am struggling to follow some proofs, and it's supposed to not require the application of later things. There are two parts, firstly: a) if $x,y\in\mathbb{R}$ and $x>0$ then $\exists$ a positive integer $n$ such that $nx>y$ b) if $x,y\in\mathbb{R}$ and $x<y$ then there $\exists$ a $p\in\mathbb{Q}$ such that $x<p<y$ Both things I can do, but not the way it is laid out. Part A For part a) it goes as follows: (I follow this one) Let $A$ be the set of all $nx$ (I would have used a sequence.... but it's not a huge problem) if a) were false (negating yields: $\forall n,\ nx\le y$ ) then y is an upper bound. Then $A$ has a least upper bound in $\mathbb{R}$. I am happy with this. Let $\alpha = \sup(A)$, since $x>0,\ \alpha-x<\alpha$ and $\alpha-x$ is not an upper bound of $A$ - a little confused here with the since part,I don't see how since $x>0$ $\alpha-x$ is not an upper bound. I agree with the conclusion though. This is why I would have used a sequence, I'm not sure how to show $\alpha-x$ is not a lower bound with the equipment to hand. But I am happy with the process. Here's where the author looses me: Hence $\alpha-x<mx$ for some positive integer m Then I am happy again: but then $\alpha<mx+x=(m+1)x$ which is a contradiction that $\alpha$ is an upper bound. Part B Since $x<y$ we have $y-x>0$ and using a) we can furnish ourselves with a positive integer, $n$, such that $n(y-x)>1$ (I am happy with this, the 1 is the ""y"" from part a)) Apply a) again to obtain positive integers $m_1$ and $m_2$ such that $m_1>nx$ and $m_2>-nx$ okay,in this case ""x"" is 1 (from part a) and ""y"" is nx, so I am happy with this. I'm also happy with orders, so $-m_2<nx<m_1$ Here is where I get lost. Hence there is an integer $m$ (with $-m_2\le m\le m_1$) such that $m-1\le nx<m$ I'm not sure why it says ""hence"" nor what has been applied here. If we combine these inequalities we obtain $nx<m\le 1+nx<ny$ Since n > 0 it follows that $x<\frac{m}{n}<y$ this proves b) with $p=\frac{m}{n}$ I ""sort of"" see it, in that $-m_2<m_1$ and as they're integers.... but I wouldn't say I am confident in it. Nor do I see why. Hence usually means ""thus we can get to"" and I'm uncomfortable because I cannot see how one can think to do this given what's at hand, which is why I am doing the chapter, I should be able to think for myself, rather than recall proofs Addendum Thinking about it, $-m_2<nx<m_1$ is useful, as the set $\{-m_2,-m_2+1,-m_2+2,...,m_1-1,m_1\}$ is finite. not quite sure how to use this though.","I am following a textbook and as is good practice, I am only skipping things I can do. this is self-learning I always struggle with chapter 1, the one that builds the ""axioms"", hate it. Anyway, I am struggling to follow some proofs, and it's supposed to not require the application of later things. There are two parts, firstly: a) if $x,y\in\mathbb{R}$ and $x>0$ then $\exists$ a positive integer $n$ such that $nx>y$ b) if $x,y\in\mathbb{R}$ and $x<y$ then there $\exists$ a $p\in\mathbb{Q}$ such that $x<p<y$ Both things I can do, but not the way it is laid out. Part A For part a) it goes as follows: (I follow this one) Let $A$ be the set of all $nx$ (I would have used a sequence.... but it's not a huge problem) if a) were false (negating yields: $\forall n,\ nx\le y$ ) then y is an upper bound. Then $A$ has a least upper bound in $\mathbb{R}$. I am happy with this. Let $\alpha = \sup(A)$, since $x>0,\ \alpha-x<\alpha$ and $\alpha-x$ is not an upper bound of $A$ - a little confused here with the since part,I don't see how since $x>0$ $\alpha-x$ is not an upper bound. I agree with the conclusion though. This is why I would have used a sequence, I'm not sure how to show $\alpha-x$ is not a lower bound with the equipment to hand. But I am happy with the process. Here's where the author looses me: Hence $\alpha-x<mx$ for some positive integer m Then I am happy again: but then $\alpha<mx+x=(m+1)x$ which is a contradiction that $\alpha$ is an upper bound. Part B Since $x<y$ we have $y-x>0$ and using a) we can furnish ourselves with a positive integer, $n$, such that $n(y-x)>1$ (I am happy with this, the 1 is the ""y"" from part a)) Apply a) again to obtain positive integers $m_1$ and $m_2$ such that $m_1>nx$ and $m_2>-nx$ okay,in this case ""x"" is 1 (from part a) and ""y"" is nx, so I am happy with this. I'm also happy with orders, so $-m_2<nx<m_1$ Here is where I get lost. Hence there is an integer $m$ (with $-m_2\le m\le m_1$) such that $m-1\le nx<m$ I'm not sure why it says ""hence"" nor what has been applied here. If we combine these inequalities we obtain $nx<m\le 1+nx<ny$ Since n > 0 it follows that $x<\frac{m}{n}<y$ this proves b) with $p=\frac{m}{n}$ I ""sort of"" see it, in that $-m_2<m_1$ and as they're integers.... but I wouldn't say I am confident in it. Nor do I see why. Hence usually means ""thus we can get to"" and I'm uncomfortable because I cannot see how one can think to do this given what's at hand, which is why I am doing the chapter, I should be able to think for myself, rather than recall proofs Addendum Thinking about it, $-m_2<nx<m_1$ is useful, as the set $\{-m_2,-m_2+1,-m_2+2,...,m_1-1,m_1\}$ is finite. not quite sure how to use this though.",,"['real-analysis', 'real-numbers']"
44,An example of a sequence of Riemann integrable functions $(f_n)$ that converges pointwise to a function $f$ that is not Riemann integrable.,An example of a sequence of Riemann integrable functions  that converges pointwise to a function  that is not Riemann integrable.,(f_n) f,"I have been given the example $$ f_n(x)= \begin{cases} 1, & \text{if}\; x\in\{(x_k)_{k=1}^{n}\}, \\ 0, & \text{otherwise.} \end{cases} $$ Here the sequence $(x_k)_{k=1}^{\infty}$ is the sequence that ""enumerates"" elements of $\mathbb{Q}$ . The sequence of functions converges pointwise to $$ f(x)= \begin{cases} 1, & x\in\mathbb{Q}, \\ 0, & x\notin\mathbb{Q}, \end{cases} $$ which isn't Riemann integrable. However, I am not quite convinced that $f_n$ is integrable. I would like some help in seeing how $f_n$ is Riemann integrable. Also, I would like more examples of sequences of Riemann integrable functions $f_n$ that converge pointwise to a function $f$ that is not Riemann integrable.","I have been given the example Here the sequence is the sequence that ""enumerates"" elements of . The sequence of functions converges pointwise to which isn't Riemann integrable. However, I am not quite convinced that is integrable. I would like some help in seeing how is Riemann integrable. Also, I would like more examples of sequences of Riemann integrable functions that converge pointwise to a function that is not Riemann integrable.","
f_n(x)=
\begin{cases}
1, & \text{if}\; x\in\{(x_k)_{k=1}^{n}\}, \\
0, & \text{otherwise.}
\end{cases}
 (x_k)_{k=1}^{\infty} \mathbb{Q} 
f(x)=
\begin{cases}
1, & x\in\mathbb{Q}, \\
0, & x\notin\mathbb{Q},
\end{cases}
 f_n f_n f_n f","['real-analysis', 'integration']"
45,Fourier transform of $f(x)=\frac{1}{e^x+e^{-x}+2}$,Fourier transform of,f(x)=\frac{1}{e^x+e^{-x}+2},"Let $$f(x)=\large \frac{1}{e^x+e^{-x}+2}$$ Compute the Fourier transform of $f$. We can factor the denominator to get $$f(x)=\frac1{(\exp(x/2)+\exp(-x/2))^2}=\frac1{(2\cosh(x/2))^2}$$ I'm thinking of using residue from complex analysis. To find the singularity, we have $$\exp(x/2)=-\exp(-x/2)\iff\exp(x)=-1$$ We know $\exp(i\pi)=-1$. So the singularities are $i\pi+2\pi k i $.","Let $$f(x)=\large \frac{1}{e^x+e^{-x}+2}$$ Compute the Fourier transform of $f$. We can factor the denominator to get $$f(x)=\frac1{(\exp(x/2)+\exp(-x/2))^2}=\frac1{(2\cosh(x/2))^2}$$ I'm thinking of using residue from complex analysis. To find the singularity, we have $$\exp(x/2)=-\exp(-x/2)\iff\exp(x)=-1$$ We know $\exp(i\pi)=-1$. So the singularities are $i\pi+2\pi k i $.",,"['real-analysis', 'integration', 'complex-analysis', 'analysis', 'fourier-analysis']"
46,Show that the Kelvin-transform is harmonic,Show that the Kelvin-transform is harmonic,,"First, I have to give you our definitions: Consider $\Omega:=\mathbb{R}^n\setminus\overline{B}_R(0)$ with $R>0$ and $n>1$. The function $\phi\colon\Omega\to G:=B_R(0)\setminus\left\{0\right\}$ with $$ y=\phi(x):=\frac{R^2}{\| x\|^2}x\text{  for  }\| x\|>R $$ is called Kelvin-transformation (relating to the sphere $S_R(0)$) . Let $u\colon\Omega\to\mathbb{R}$. The function defined by $$ v(y):=\frac{\| x\|^{n-2}}{R^{n-2}}u(x)\text{  for  }\lVert x\rVert >R~~~(*) $$ is called Kelvin-transform of u . Alternatively, the Kelvin-transform of $u$ can be definied by $$ v(z):=\frac{R^{n-2}}{\lVert z\rVert^{n-2}}u\left(\frac{R^2}{\lVert z\rVert^2}z\right)\text{  for  }\lVert z\rVert <R. $$ Let $u$ be harmonic in $\Omega$. Then its Kelvin-transform is harmonic in $G$. Show that for the case $n=2$. To be honest, I do not know, what is to show here, because if I take the alternative definition of the Kelvin-transform, for $n=2$ it is $$ v(z)=u\left(\frac{R^2}{\lVert z\rVert^2}z\right), z\in G $$ and then $$ \Delta v(z)=\Delta u\left(\frac{R^2}{\lVert z\rVert^2}z\right)=0, $$ because $x:=\frac{R^2}{\lVert z\rVert^2}z$ is in $\Omega$ and $u$ is harmonic in $\Omega$. So I only had to use that the inverse of the Kelvin-transformation is given by $$ \phi^{-1}\colon G\to\Omega, y\longmapsto\frac{R^2}{\lVert y\rVert^2}y. $$ Is that the proof? It seems TOO easy...","First, I have to give you our definitions: Consider $\Omega:=\mathbb{R}^n\setminus\overline{B}_R(0)$ with $R>0$ and $n>1$. The function $\phi\colon\Omega\to G:=B_R(0)\setminus\left\{0\right\}$ with $$ y=\phi(x):=\frac{R^2}{\| x\|^2}x\text{  for  }\| x\|>R $$ is called Kelvin-transformation (relating to the sphere $S_R(0)$) . Let $u\colon\Omega\to\mathbb{R}$. The function defined by $$ v(y):=\frac{\| x\|^{n-2}}{R^{n-2}}u(x)\text{  for  }\lVert x\rVert >R~~~(*) $$ is called Kelvin-transform of u . Alternatively, the Kelvin-transform of $u$ can be definied by $$ v(z):=\frac{R^{n-2}}{\lVert z\rVert^{n-2}}u\left(\frac{R^2}{\lVert z\rVert^2}z\right)\text{  for  }\lVert z\rVert <R. $$ Let $u$ be harmonic in $\Omega$. Then its Kelvin-transform is harmonic in $G$. Show that for the case $n=2$. To be honest, I do not know, what is to show here, because if I take the alternative definition of the Kelvin-transform, for $n=2$ it is $$ v(z)=u\left(\frac{R^2}{\lVert z\rVert^2}z\right), z\in G $$ and then $$ \Delta v(z)=\Delta u\left(\frac{R^2}{\lVert z\rVert^2}z\right)=0, $$ because $x:=\frac{R^2}{\lVert z\rVert^2}z$ is in $\Omega$ and $u$ is harmonic in $\Omega$. So I only had to use that the inverse of the Kelvin-transformation is given by $$ \phi^{-1}\colon G\to\Omega, y\longmapsto\frac{R^2}{\lVert y\rVert^2}y. $$ Is that the proof? It seems TOO easy...",,['real-analysis']
47,Convergence of a certain series.,Convergence of a certain series.,,"If $(q_n)$ is an enumeration of rationals in $(-1,1)$ except $0$, is the following series convergent? $$ \sum_{n=1}^{\infty}\frac{1}{(nq_n)^2} $$ Are there enumerations for which is convergent and enumerations for which is divergent? I accidentally asked this question on mathoverflow. If anyone is on both sites, please delete it there. I did not sign up for an account, and I did not know how to erase it myself.","If $(q_n)$ is an enumeration of rationals in $(-1,1)$ except $0$, is the following series convergent? $$ \sum_{n=1}^{\infty}\frac{1}{(nq_n)^2} $$ Are there enumerations for which is convergent and enumerations for which is divergent? I accidentally asked this question on mathoverflow. If anyone is on both sites, please delete it there. I did not sign up for an account, and I did not know how to erase it myself.",,"['real-analysis', 'sequences-and-series']"
48,limit for applying of sin operation n times and multiplying the result by square root of n,limit for applying of sin operation n times and multiplying the result by square root of n,,"Numerically, I found that if one builds a sequence of $sin(sin(sin(....(sin(x)...))$ with $n$ being the number of times sin operation is performed, then with n going to infinity the product of this operation multiplied by the square root of n approaches the value, which is remarkably close to the square root of 3 for any $0<x<\pi$. The same if at every step instead of using sin function I use Taylor series. However, starting from scratch I could not find analytical approximation because for every power of x in Taylor series I stop at, the result diverges; it goes to negative infinity if $sin(x)$ is approximated as $x-x^3/3!$ and to positive infinity if $sin(x)$ is approximated as $x-x^3/3!+x^5/5!$ and so forth. Please, advise if anyone observed this yet and has analytical proof/disproof. Thanks, Anthony","Numerically, I found that if one builds a sequence of $sin(sin(sin(....(sin(x)...))$ with $n$ being the number of times sin operation is performed, then with n going to infinity the product of this operation multiplied by the square root of n approaches the value, which is remarkably close to the square root of 3 for any $0<x<\pi$. The same if at every step instead of using sin function I use Taylor series. However, starting from scratch I could not find analytical approximation because for every power of x in Taylor series I stop at, the result diverges; it goes to negative infinity if $sin(x)$ is approximated as $x-x^3/3!$ and to positive infinity if $sin(x)$ is approximated as $x-x^3/3!+x^5/5!$ and so forth. Please, advise if anyone observed this yet and has analytical proof/disproof. Thanks, Anthony",,['real-analysis']
49,Show that $T$ is or isn't a strict contraction but have fixed points [closed],Show that  is or isn't a strict contraction but have fixed points [closed],T,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Define $T: C[0,1] \to C[0,1]$ by $$(Tf)(x) = \int^x_0 f(t)dt.$$ Show that $T$ is not a strict contraction while $T^2$ is. What is the fixed point of $T$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Define $T: C[0,1] \to C[0,1]$ by $$(Tf)(x) = \int^x_0 f(t)dt.$$ Show that $T$ is not a strict contraction while $T^2$ is. What is the fixed point of $T$?",,['real-analysis']
50,Consequence of Cauchy Schwarz in $\mathscr{L}^2$?,Consequence of Cauchy Schwarz in ?,\mathscr{L}^2,"If $f,g\in \mathscr{L}^2$, then $\|fg\|_1\leq\|f\|_2\|g\|_2$. My textbook says that this is a consequence of Cauchy-Schwarz inequality. How so? Cauchy-Schwarz says that $|\langle v,w\rangle|\leq\|v\|\|w\|$. Putting in $f$ and $g$, we have $$\left|\int_X f\overline{g}d\mu\right|\leq \|f\|_2\|g\|_2$$ But $$\|fg\|_1=\int_X|fg|d\mu$$ So I don't see how $\|fg\|_1\leq\|f\|_2\|g\|_2$ is a consequence of Cauchy-Schwarz.","If $f,g\in \mathscr{L}^2$, then $\|fg\|_1\leq\|f\|_2\|g\|_2$. My textbook says that this is a consequence of Cauchy-Schwarz inequality. How so? Cauchy-Schwarz says that $|\langle v,w\rangle|\leq\|v\|\|w\|$. Putting in $f$ and $g$, we have $$\left|\int_X f\overline{g}d\mu\right|\leq \|f\|_2\|g\|_2$$ But $$\|fg\|_1=\int_X|fg|d\mu$$ So I don't see how $\|fg\|_1\leq\|f\|_2\|g\|_2$ is a consequence of Cauchy-Schwarz.",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
51,Proof verification that Q is dense in R,Proof verification that Q is dense in R,,"Hi everybody I'd like to know if the next argument is sound. Definitions : A real number x is said to be positive if can be written as a formal limit of a Cauchy sequence of rational numbers $(x_n)$ which is positively bounded away from zero, i.e., $\,x_n\ge c$ for each n, where $c$ is a positive rational number. Proposition : Given any two real numbers $x<y$, we can find a rational number $q$ such that $x<q<y$. Proof: It will suffice to find two rational numbers such that $x\le q_1<q_2 \le y$, because we can define $q= \frac{q_1+q_2}{2}$ and we're done. Let  $(a_n)_{n=1}^{\infty}$, $(b_n)_{n=1}^{\infty}$ be  Cauchy sequences such that their formal limits are the real number $y$ and $x$ respectively. Since $y-x$ is a positive real number there exists a positive rational number $c$ such that $y-x>c$. Let $\,\epsilon= c/6$. Then there exists sufficient large $N_{\epsilon}\in \mathbb{N}$ such that: $|a_n-a_m|\le \epsilon$, $|b_n-b_m|\le \epsilon$ occurs simultaneously for all $n,m\ge N_{\epsilon}$. Moreover the sequence $(a_n-b_n)$ is eventually greater than $c$, let $M$ be the natural number such that $ a_n-b_n \ge  c$ for all $n\ge M$. We set $N =max(N_{\epsilon}, M)$. Now let us fix some $n_0$ such that $n_0\ge N$. Then clearly $|y - a_{n_0}|\le c/6$ and  $|x-b_{n_0}|\le c/6$ at the same time. We claim that  $a_{n_0}-\frac{c}{6}$ and $b_{n_0}+\frac{c}{6}$ have the desired property.  It is not difficult to see that both are rational numbers, follows immediately by construction so, we will show that $a_{n_0}-\frac{c}{6}\le y$, $x\le b_{n_0}+\frac{c}{6}$ and $a_{n_0}-\frac{c}{6}>b_{n_0}+\frac{c}{6}$. We already know that $|y - a_{n_0}|\le c/6$, so  $a_{n_0}-c/6\le y \le a_{n_0}+c/6$ as desired. A similar argument shows that $x\le b_{n_0}+c/6$. To conclude the proof our task is to show $b_{n_0}+c/6<a_{n_0}-c/6$.  We argue by contradiction, suppose $b_{n_0}+c/6\ge a_{n_0}-c/6$ then $c/3\ge a_{n_0}-b_{n_0}$. But since $n_0\ge N$, it follows that  $a_{n_0}-b_{n_0}\ge c$ and then $c/3\ge c$ a contradiction. We define $q$ as the average of $a_{n_0}-\frac{c}{6}$ and $b_{n_0}+\frac{c}{6}$ and we're done. I would appreciate for any suggestion. Thanks in advance. :)","Hi everybody I'd like to know if the next argument is sound. Definitions : A real number x is said to be positive if can be written as a formal limit of a Cauchy sequence of rational numbers $(x_n)$ which is positively bounded away from zero, i.e., $\,x_n\ge c$ for each n, where $c$ is a positive rational number. Proposition : Given any two real numbers $x<y$, we can find a rational number $q$ such that $x<q<y$. Proof: It will suffice to find two rational numbers such that $x\le q_1<q_2 \le y$, because we can define $q= \frac{q_1+q_2}{2}$ and we're done. Let  $(a_n)_{n=1}^{\infty}$, $(b_n)_{n=1}^{\infty}$ be  Cauchy sequences such that their formal limits are the real number $y$ and $x$ respectively. Since $y-x$ is a positive real number there exists a positive rational number $c$ such that $y-x>c$. Let $\,\epsilon= c/6$. Then there exists sufficient large $N_{\epsilon}\in \mathbb{N}$ such that: $|a_n-a_m|\le \epsilon$, $|b_n-b_m|\le \epsilon$ occurs simultaneously for all $n,m\ge N_{\epsilon}$. Moreover the sequence $(a_n-b_n)$ is eventually greater than $c$, let $M$ be the natural number such that $ a_n-b_n \ge  c$ for all $n\ge M$. We set $N =max(N_{\epsilon}, M)$. Now let us fix some $n_0$ such that $n_0\ge N$. Then clearly $|y - a_{n_0}|\le c/6$ and  $|x-b_{n_0}|\le c/6$ at the same time. We claim that  $a_{n_0}-\frac{c}{6}$ and $b_{n_0}+\frac{c}{6}$ have the desired property.  It is not difficult to see that both are rational numbers, follows immediately by construction so, we will show that $a_{n_0}-\frac{c}{6}\le y$, $x\le b_{n_0}+\frac{c}{6}$ and $a_{n_0}-\frac{c}{6}>b_{n_0}+\frac{c}{6}$. We already know that $|y - a_{n_0}|\le c/6$, so  $a_{n_0}-c/6\le y \le a_{n_0}+c/6$ as desired. A similar argument shows that $x\le b_{n_0}+c/6$. To conclude the proof our task is to show $b_{n_0}+c/6<a_{n_0}-c/6$.  We argue by contradiction, suppose $b_{n_0}+c/6\ge a_{n_0}-c/6$ then $c/3\ge a_{n_0}-b_{n_0}$. But since $n_0\ge N$, it follows that  $a_{n_0}-b_{n_0}\ge c$ and then $c/3\ge c$ a contradiction. We define $q$ as the average of $a_{n_0}-\frac{c}{6}$ and $b_{n_0}+\frac{c}{6}$ and we're done. I would appreciate for any suggestion. Thanks in advance. :)",,"['real-analysis', 'self-learning', 'proof-verification']"
52,Continuous function that vanish in infinity,Continuous function that vanish in infinity,,Can we say that every bounded continuous function is the limit of linear combination of  continuous functions that vanish in infinity?In fact if X be a locally compact then can we say that ${\overline{lin(C_0(X))}}=C_b(X)$ or ${\overline{lin(C_0(X))}}=C(X)$ ?,Can we say that every bounded continuous function is the limit of linear combination of  continuous functions that vanish in infinity?In fact if X be a locally compact then can we say that ${\overline{lin(C_0(X))}}=C_b(X)$ or ${\overline{lin(C_0(X))}}=C(X)$ ?,,['real-analysis']
53,$P(X)$ is locally compact if $X$ is?,is locally compact if  is?,P(X) X,"Let us assume, as in here Measurable structure on the space of probability measures that $X$ is a locally compact Polish space.  Then can the same thing be said of $P(X)$, its probability measures with weak * convergence?  I am only missing local compactness and completeness relative to one metrization now, as I realized that appeal to the 1 point compactification of X helps. (This process gives a bigger space that is actually a metric space.)","Let us assume, as in here Measurable structure on the space of probability measures that $X$ is a locally compact Polish space.  Then can the same thing be said of $P(X)$, its probability measures with weak * convergence?  I am only missing local compactness and completeness relative to one metrization now, as I realized that appeal to the 1 point compactification of X helps. (This process gives a bigger space that is actually a metric space.)",,"['real-analysis', 'general-topology', 'analysis', 'functional-analysis', 'probability-theory']"
54,"Show that $ M$ is constant on $[a,b]$ (variational calculus)",Show that  is constant on  (variational calculus)," M [a,b]","Let $F:\mathbb{R}^n\times \mathbb{R}^n \to \mathbb{R}$ be $C^2$ on $[a,b]$  and $u$ be a solution for the Euler-lagrange equations for the functional given by $$J(u) = \int F(u(t),\dot{u}(t)).dt, $$ Show that the function $ M$ on $[a,b]$ given by $$M(t)= \dot{u}(t)F_p(u(t),\dot{u}(t))-F(u(t),\dot{u}(t)) $$ is constant on $[a,b]$. The Euler lagrange equations tell us that $$\frac{d}{dt}F_p = F_u $$ (where $F_u$ is the vector of partial derivatives w.r.t. the first argument, and $F_p$ w.r.t. the second). Some light on how tot tackle this problem is very much appreciated.","Let $F:\mathbb{R}^n\times \mathbb{R}^n \to \mathbb{R}$ be $C^2$ on $[a,b]$  and $u$ be a solution for the Euler-lagrange equations for the functional given by $$J(u) = \int F(u(t),\dot{u}(t)).dt, $$ Show that the function $ M$ on $[a,b]$ given by $$M(t)= \dot{u}(t)F_p(u(t),\dot{u}(t))-F(u(t),\dot{u}(t)) $$ is constant on $[a,b]$. The Euler lagrange equations tell us that $$\frac{d}{dt}F_p = F_u $$ (where $F_u$ is the vector of partial derivatives w.r.t. the first argument, and $F_p$ w.r.t. the second). Some light on how tot tackle this problem is very much appreciated.",,"['calculus', 'real-analysis', 'functional-analysis', 'multivariable-calculus', 'calculus-of-variations']"
55,"Given that $5a+2b+3c=10$, What is the minimum value of $a^2+b^2+c^2$?","Given that , What is the minimum value of ?",5a+2b+3c=10 a^2+b^2+c^2,"The question is , Given that $$5a+2b+3c=10$$ What is the minimum value of $$a^2+b^2+c^2$$? I know that I have to use AM-GM inequality somehow but I have no idea how to use it for this problem. Help would be much appreciated.","The question is , Given that $$5a+2b+3c=10$$ What is the minimum value of $$a^2+b^2+c^2$$? I know that I have to use AM-GM inequality somehow but I have no idea how to use it for this problem. Help would be much appreciated.",,['real-analysis']
56,Intersection of a closed set and compact set is compact [duplicate],Intersection of a closed set and compact set is compact [duplicate],,"This question already has answers here : Is the intersection of a closed set and a compact set always compact? (2 answers) Closed 10 years ago . I've been stuck on the following problem for several days: Let $(M,d)$ be an arbitrary metric space and $S, T$ be subsets of $M$. If $S$ is closed and $T$ is compact, then $S \cap T$ is compact. I know that if $T$ is compact, $T$ is  closed and bounded. That would imply that $S \cap T$ is also closed and bounded since $(S \cap T) \subseteq T$. Also since $S$ is closed, $S$ contains all its accumulation points. Other than writing down the definitions, I really don't know how to proceed. Could someone give me a hint?","This question already has answers here : Is the intersection of a closed set and a compact set always compact? (2 answers) Closed 10 years ago . I've been stuck on the following problem for several days: Let $(M,d)$ be an arbitrary metric space and $S, T$ be subsets of $M$. If $S$ is closed and $T$ is compact, then $S \cap T$ is compact. I know that if $T$ is compact, $T$ is  closed and bounded. That would imply that $S \cap T$ is also closed and bounded since $(S \cap T) \subseteq T$. Also since $S$ is closed, $S$ contains all its accumulation points. Other than writing down the definitions, I really don't know how to proceed. Could someone give me a hint?",,"['real-analysis', 'general-topology', 'metric-spaces', 'compactness']"
57,Expectation of composition of functions with density as R-N derivative,Expectation of composition of functions with density as R-N derivative,,"In prior probability courses, I've always seen and used the fact that, for a continuous random variable X and a function $\phi$, $E[\phi(X)]=\int_{ \mathbb{R}}\phi(x) f_X(x)dx,$ but I cannot find a rigorous statement of such a theorem listing all of the necessary assumptions on $X$ or $\phi$ in any of my probability references. I'm currently trying to reconcile my previous probability knowledge and what I've learned in my measure theory course, using Rudin's ""Real and Complex Analysis"" and would like to prove this result. My questions are: Is my understanding of the set up correct, and is the proof of my result below correct? Can the theorem below be strengthened? Theorem: Let $(X, \mathcal{M}, \mu)$ be a positive measure space with $\mu(X)=1$, and let $g \in L^1(\mu)$ be a real ""absolutely continuous"" function with density $f:\mathbb{R} \to \mathbb{R}$. If $\phi: \mathbb{R} \to \mathbb{R}$ is absolutely continuous on $[a,b]$ for every $a,b \in \mathbb{R}$ and $\lim_{t \to -\infty} \phi(t)=0$ with $\phi' \in L^1(\mathbb{R})$, then $$\int_X \phi \circ g d\mu = \int_{\mathbb{R}}\phi f dm.$$ SET UP: Define $F: \mathbb{R} \to \mathbb{R}$ by $F(x) = \mu(g \leq x)$, the distribution function of $g$. Keeping things general, let $F_-$ be the left continuous function agreeing with $F$ on all points of continuity (this function won't matter once absolute continuity is defined, but it is necessary in general). Define $\Lambda: C_c(\mathbb{R}) \to \mathbb{C}$ by $\Lambda(h)=\int_{\mathbb{R}} h dF_-$ (the Riemann-Stieltjes integral). Since $F$ is bounded and monotone, $F \in BV$, so $\Lambda$ is well defined and forms a positive linear functional. The Riesz representation theorem implies the existance of a measure $F_*$ on $\mathbb{R}$ so that $F_-(b)-F_-(a) = F_*([a,b))$ for all real $a < b$. (as in Rudin's exercise 7.12). We say $g$ is ""absolutely continuous"" if $F_* << m$, in which case the density of $g$ is $f:= dF_*/dm.$  Note that if $g$ is absolutely continuous, it follows that $F_-=F.$ Proof I first show that $\int_X \phi \circ f d\mu=\int_{\mathbb{R}}\mu(g > t) \phi'(t)dt$, and the proof closely follows that of Rudin's theorem 8.16 about distribution functions. Define $E=\{(x,t): g(x) > t\} \subset X \times \mathbb{R}$, which Rudin argues is measurable for positive $g$, so taking the positive and negative parts of $g$ implies that $E$ is measurable. Also, observe that $\int_X \int_{\mathbb{R}} |\chi_E(x,t) \phi'(t)|dt d\mu(x) \leq \int_X ||\phi'||_{L^1(\mathbb{R})} d\mu < \infty.$ Therefore, Fubini's theorem implies that  $$\int_{\mathbb{R}}\mu(g>t)\phi'(t)dt =\int_{\mathbb{R}}\int_X \chi_E(x,t)\phi'(t) d\mu(x) dt =\int_X \int_{\mathbb{R}} \chi_E(x,t) \phi'(t)dt d\mu(x) =$$ $$\int_X \int_{-\infty}^{g(x)} \phi'(t) dt d\mu(x) .$$ Since $\phi' \in L^1(\mathbb{R})$, the dominated convergence theorem and absolute continuity imply that this equals $\int_X \phi(g(x))-\phi(-\infty) d\mu(x)= \int_X \phi \circ g d\mu.$ Also, $$\int_{\mathbb{R}}\mu(g > t) \phi'(t)dt = \int_{\mathbb{R}}(1-F(t)) \phi'(t)dt = \int_{\mathbb{R}}\int_{t}^{\infty}f(x)\phi'(t)dx dt.$$ A similar argument as before implies that Fubini's theorem applies to this integral, and therefore $$\int_X\phi \circ g d\mu= \int_{\mathbb{R}}\int_{-\infty}^{x}f(x) \phi'(t) dt dx = \int_{\mathbb{R}} f(x)(\phi(x)-\phi(-\infty))dx=\int_{\mathbb{R}}\phi f dm.$$ Note that the second to last equality holds again by the dominated convergence theorem, since $\phi$ is absolutely continuous and $\phi' \in L^1(\mathbb{R})$.","In prior probability courses, I've always seen and used the fact that, for a continuous random variable X and a function $\phi$, $E[\phi(X)]=\int_{ \mathbb{R}}\phi(x) f_X(x)dx,$ but I cannot find a rigorous statement of such a theorem listing all of the necessary assumptions on $X$ or $\phi$ in any of my probability references. I'm currently trying to reconcile my previous probability knowledge and what I've learned in my measure theory course, using Rudin's ""Real and Complex Analysis"" and would like to prove this result. My questions are: Is my understanding of the set up correct, and is the proof of my result below correct? Can the theorem below be strengthened? Theorem: Let $(X, \mathcal{M}, \mu)$ be a positive measure space with $\mu(X)=1$, and let $g \in L^1(\mu)$ be a real ""absolutely continuous"" function with density $f:\mathbb{R} \to \mathbb{R}$. If $\phi: \mathbb{R} \to \mathbb{R}$ is absolutely continuous on $[a,b]$ for every $a,b \in \mathbb{R}$ and $\lim_{t \to -\infty} \phi(t)=0$ with $\phi' \in L^1(\mathbb{R})$, then $$\int_X \phi \circ g d\mu = \int_{\mathbb{R}}\phi f dm.$$ SET UP: Define $F: \mathbb{R} \to \mathbb{R}$ by $F(x) = \mu(g \leq x)$, the distribution function of $g$. Keeping things general, let $F_-$ be the left continuous function agreeing with $F$ on all points of continuity (this function won't matter once absolute continuity is defined, but it is necessary in general). Define $\Lambda: C_c(\mathbb{R}) \to \mathbb{C}$ by $\Lambda(h)=\int_{\mathbb{R}} h dF_-$ (the Riemann-Stieltjes integral). Since $F$ is bounded and monotone, $F \in BV$, so $\Lambda$ is well defined and forms a positive linear functional. The Riesz representation theorem implies the existance of a measure $F_*$ on $\mathbb{R}$ so that $F_-(b)-F_-(a) = F_*([a,b))$ for all real $a < b$. (as in Rudin's exercise 7.12). We say $g$ is ""absolutely continuous"" if $F_* << m$, in which case the density of $g$ is $f:= dF_*/dm.$  Note that if $g$ is absolutely continuous, it follows that $F_-=F.$ Proof I first show that $\int_X \phi \circ f d\mu=\int_{\mathbb{R}}\mu(g > t) \phi'(t)dt$, and the proof closely follows that of Rudin's theorem 8.16 about distribution functions. Define $E=\{(x,t): g(x) > t\} \subset X \times \mathbb{R}$, which Rudin argues is measurable for positive $g$, so taking the positive and negative parts of $g$ implies that $E$ is measurable. Also, observe that $\int_X \int_{\mathbb{R}} |\chi_E(x,t) \phi'(t)|dt d\mu(x) \leq \int_X ||\phi'||_{L^1(\mathbb{R})} d\mu < \infty.$ Therefore, Fubini's theorem implies that  $$\int_{\mathbb{R}}\mu(g>t)\phi'(t)dt =\int_{\mathbb{R}}\int_X \chi_E(x,t)\phi'(t) d\mu(x) dt =\int_X \int_{\mathbb{R}} \chi_E(x,t) \phi'(t)dt d\mu(x) =$$ $$\int_X \int_{-\infty}^{g(x)} \phi'(t) dt d\mu(x) .$$ Since $\phi' \in L^1(\mathbb{R})$, the dominated convergence theorem and absolute continuity imply that this equals $\int_X \phi(g(x))-\phi(-\infty) d\mu(x)= \int_X \phi \circ g d\mu.$ Also, $$\int_{\mathbb{R}}\mu(g > t) \phi'(t)dt = \int_{\mathbb{R}}(1-F(t)) \phi'(t)dt = \int_{\mathbb{R}}\int_{t}^{\infty}f(x)\phi'(t)dx dt.$$ A similar argument as before implies that Fubini's theorem applies to this integral, and therefore $$\int_X\phi \circ g d\mu= \int_{\mathbb{R}}\int_{-\infty}^{x}f(x) \phi'(t) dt dx = \int_{\mathbb{R}} f(x)(\phi(x)-\phi(-\infty))dx=\int_{\mathbb{R}}\phi f dm.$$ Note that the second to last equality holds again by the dominated convergence theorem, since $\phi$ is absolutely continuous and $\phi' \in L^1(\mathbb{R})$.",,"['real-analysis', 'measure-theory', 'probability-theory', 'probability-distributions', 'random-variables']"
58,Why does the boundary of a cube in $\mathbb{R}^n$ have measure zero?,Why does the boundary of a cube in  have measure zero?,\mathbb{R}^n,"I'm trying to develop the fact that the measure of a box is equal to the volume of a box for any measure $m$, i.e., a function satisfying the standard properties of monotonicity, positivity, countable sub-additivity, translation invariance, etc. I'm trying to show that the sets $(1,\frac{1}{q})^n$ and $[1,\frac{1}{q}]^n$ in $\mathbb{R}^n$ both have measure $\frac{1}{q^n}$, for $q>1$ an integer. First, by translating each coordinate by $k/q$ where $0\leq k\leq q-1$, one sees that $q^n$ disjoint translates of $(0,1/q)^n$ are contained in $[0,1]^n$. But normalization, monotonicity, and translation invariance, it follows that  $$ q^n m((0,1/q)^n)\leq 1$$ so $m((0,1/q)^n)\leq q^{-n}$. Similarly, the $q^n$ translates of $[0,1/q]^n$ cover $[0,1]^n$, so $q^nm([0,1/q]^n)\geq 1$, which implies $m([0,1/q]^n)\geq q^{-n}$. To finish, I'd like to show that $m([0,1/q]^n\setminus(0,1/q)^n)=0$. So given $\epsilon>0$, I'm trying to cover the boundary of the cube by a family of boxes whose measure is less than $\epsilon$. But without knowing at this point what the measure of a box is, since the measure is not necessarily the Lesbegue measure, I'm stuck. How can I show the measure of the boundary of the cube is $0$? Thanks. The axioms for $m$ are as follows: $\mathbb{R}^n$ is a Euclidean space. Every open and closed set is measurable. The complement of every measurable set is measurable. Finite/countable unions and intersections of measurable sets are measurable. $m(\emptyset)=0$. $0\leq m(A)\leq\infty$ for every measurable set. If $A\subseteq B$, then $m(A)\leq m(B)$. If $(A_j)_{j\in J}$ is a family of measurable sets, then $m(\bigcup A_j)\leq\sum m(A_j)$. If $(A_j)_{j\in J}$ is a family of disjoint measurable sets, then $m(\bigcup A_j)=\sum m(A_j)$. $m([0,1]^n)=1$. If $A$ is measurable, $m(x+A)=m(A)$ for all $x\in\mathbb{R}^n$.","I'm trying to develop the fact that the measure of a box is equal to the volume of a box for any measure $m$, i.e., a function satisfying the standard properties of monotonicity, positivity, countable sub-additivity, translation invariance, etc. I'm trying to show that the sets $(1,\frac{1}{q})^n$ and $[1,\frac{1}{q}]^n$ in $\mathbb{R}^n$ both have measure $\frac{1}{q^n}$, for $q>1$ an integer. First, by translating each coordinate by $k/q$ where $0\leq k\leq q-1$, one sees that $q^n$ disjoint translates of $(0,1/q)^n$ are contained in $[0,1]^n$. But normalization, monotonicity, and translation invariance, it follows that  $$ q^n m((0,1/q)^n)\leq 1$$ so $m((0,1/q)^n)\leq q^{-n}$. Similarly, the $q^n$ translates of $[0,1/q]^n$ cover $[0,1]^n$, so $q^nm([0,1/q]^n)\geq 1$, which implies $m([0,1/q]^n)\geq q^{-n}$. To finish, I'd like to show that $m([0,1/q]^n\setminus(0,1/q)^n)=0$. So given $\epsilon>0$, I'm trying to cover the boundary of the cube by a family of boxes whose measure is less than $\epsilon$. But without knowing at this point what the measure of a box is, since the measure is not necessarily the Lesbegue measure, I'm stuck. How can I show the measure of the boundary of the cube is $0$? Thanks. The axioms for $m$ are as follows: $\mathbb{R}^n$ is a Euclidean space. Every open and closed set is measurable. The complement of every measurable set is measurable. Finite/countable unions and intersections of measurable sets are measurable. $m(\emptyset)=0$. $0\leq m(A)\leq\infty$ for every measurable set. If $A\subseteq B$, then $m(A)\leq m(B)$. If $(A_j)_{j\in J}$ is a family of measurable sets, then $m(\bigcup A_j)\leq\sum m(A_j)$. If $(A_j)_{j\in J}$ is a family of disjoint measurable sets, then $m(\bigcup A_j)=\sum m(A_j)$. $m([0,1]^n)=1$. If $A$ is measurable, $m(x+A)=m(A)$ for all $x\in\mathbb{R}^n$.",,"['real-analysis', 'measure-theory']"
59,"Prove $g_n f_n \rightarrow gf$ in $L_p([0,1])$. Where $f_n$ converges and $(g_n)_{n=1}^\infty $ is a sequence of bounded measurable functions",Prove  in . Where  converges and  is a sequence of bounded measurable functions,"g_n f_n \rightarrow gf L_p([0,1]) f_n (g_n)_{n=1}^\infty ","Suppose that $ f_n \rightarrow f $ in $ L_p([0,1])$ with respect to the $|| . ||_p$ norm. $\{g_n\}_{n=1}^{\infty} $is a sequence of measurable functions such that $|g_n| \leq M$ for each $n$ and $g_n \rightarrow g$. Work done so far: $g_n \rightarrow g$ a.e. implies that $g_n^p \rightarrow g^p$ a.e. Next since $|g_n|^p \leq M^p \in \mathbb{R} $ by Lebesgue Dominated Convergence Theorem we get $\int_{[0,1]} g_n^p \rightarrow \int_{[0,1]} g^p. $ Then,  $\int_{[0,1]} |g_n f_n - gf|^p \leq \int_{[0,1]} |g_nf_n - g_nf|^p + \int_{[0,1]} |g_n f - gf|^p $ (by Minkowski's) $=  \int_{[0,1] }|g_n|^p|f_n-f|^p + \int_{[0,1]} |f|^p |g_n - g|^p$ So at this point I will put $ (\int_{[0,1]} |g_n f_n - gf|^p)^{1/p} = || g_n f_n - gf||$ And I can almost show that the last two terms go to 0, but am having trouble dealing with $|g_n|^p$ in the first term and $|f|^p$ in the second term.","Suppose that $ f_n \rightarrow f $ in $ L_p([0,1])$ with respect to the $|| . ||_p$ norm. $\{g_n\}_{n=1}^{\infty} $is a sequence of measurable functions such that $|g_n| \leq M$ for each $n$ and $g_n \rightarrow g$. Work done so far: $g_n \rightarrow g$ a.e. implies that $g_n^p \rightarrow g^p$ a.e. Next since $|g_n|^p \leq M^p \in \mathbb{R} $ by Lebesgue Dominated Convergence Theorem we get $\int_{[0,1]} g_n^p \rightarrow \int_{[0,1]} g^p. $ Then,  $\int_{[0,1]} |g_n f_n - gf|^p \leq \int_{[0,1]} |g_nf_n - g_nf|^p + \int_{[0,1]} |g_n f - gf|^p $ (by Minkowski's) $=  \int_{[0,1] }|g_n|^p|f_n-f|^p + \int_{[0,1]} |f|^p |g_n - g|^p$ So at this point I will put $ (\int_{[0,1]} |g_n f_n - gf|^p)^{1/p} = || g_n f_n - gf||$ And I can almost show that the last two terms go to 0, but am having trouble dealing with $|g_n|^p$ in the first term and $|f|^p$ in the second term.",,"['real-analysis', 'lebesgue-integral']"
60,A question about the definition of partition in Riemann Integral,A question about the definition of partition in Riemann Integral,,"I am reading Baby Rudin's definition on Riemann Integral, where partition $P$ is restricted to a finite set of points. When Rudin says ""where the inf and the sup are taken over all partitions $P$ of $[a,b]$"", I'm assuming he is meant to say over all finite partitions. This is somewhat different from what I was taught, where $P$ essentially consists of countably many ""anchor"" points. For a function which has countably many discontinuities (e.g., a monotone function), I know it is Riemann integrable because the measure is 0, but does it fail if only finite partitions are allowed? I've seen a somewhat related discussion: Proof that a function with a countable set of discontinuities is Riemann integrable without the notion of measure So the compactness of [a,b] actually implies that a finite partition is sufficient?","I am reading Baby Rudin's definition on Riemann Integral, where partition $P$ is restricted to a finite set of points. When Rudin says ""where the inf and the sup are taken over all partitions $P$ of $[a,b]$"", I'm assuming he is meant to say over all finite partitions. This is somewhat different from what I was taught, where $P$ essentially consists of countably many ""anchor"" points. For a function which has countably many discontinuities (e.g., a monotone function), I know it is Riemann integrable because the measure is 0, but does it fail if only finite partitions are allowed? I've seen a somewhat related discussion: Proof that a function with a countable set of discontinuities is Riemann integrable without the notion of measure So the compactness of [a,b] actually implies that a finite partition is sufficient?",,"['real-analysis', 'integration']"
61,Can conjugate real algebraic numbers always be distinguished by the sign of an iterated derivative of their minimal polynomial?,Can conjugate real algebraic numbers always be distinguished by the sign of an iterated derivative of their minimal polynomial?,,"Let $P$ be an irreducible polynomial with rational coefficients and degree $d$. Let $\alpha$ and $\beta$ be two real roots of $P$. Then the numbers $a_k=P^{(k)}(\alpha)$ and $b_k=P^{(k)}(\beta) \ (1\leq k\leq d-1)$ are real numbers. If $a_kb_k \gt 0$  for $1\leq k\leq d-1$, does it always follow that $\alpha=\beta$ ?","Let $P$ be an irreducible polynomial with rational coefficients and degree $d$. Let $\alpha$ and $\beta$ be two real roots of $P$. Then the numbers $a_k=P^{(k)}(\alpha)$ and $b_k=P^{(k)}(\beta) \ (1\leq k\leq d-1)$ are real numbers. If $a_kb_k \gt 0$  for $1\leq k\leq d-1$, does it always follow that $\alpha=\beta$ ?",,"['real-analysis', 'polynomials']"
62,$||f-f_n||_{L^1} \rightarrow 0$ but $f_n \rightarrow f$ for no $x$,but  for no,||f-f_n||_{L^1} \rightarrow 0 f_n \rightarrow f x,Show that there are $f\in L^1(\mathbb{R}^d)$ and a sequence $\{ {f_n}\}$ with ${f_n}\in L^1(\mathbb{R}^d)$ such that $||f-f_n||_{L^1} \rightarrow 0$ but $f_n \rightarrow f$ for no $x$ Thanks.,Show that there are $f\in L^1(\mathbb{R}^d)$ and a sequence $\{ {f_n}\}$ with ${f_n}\in L^1(\mathbb{R}^d)$ such that $||f-f_n||_{L^1} \rightarrow 0$ but $f_n \rightarrow f$ for no $x$ Thanks.,,['real-analysis']
63,Why do we use 'this' Gamma Function. [duplicate],Why do we use 'this' Gamma Function. [duplicate],,"This question already has answers here : Why is Euler's Gamma function the ""best"" extension of the factorial function to the reals? (7 answers) Closed 10 years ago . The Gamma function is a generalization of the factorial defined by Euler as: $$\Gamma(z)=\int\limits_{0}^{\infty}t^{z-1}e^{-t}\,dt$$ for $z\in\mathbb{C}$ with positive real part. It satisfies, $\Gamma(n)=(n-1)!$ for all $n\in\mathbb{N}$. My question is why we choose this particular function from all the functions that satisfy the previous property. And, how is the Gamma function deduced?","This question already has answers here : Why is Euler's Gamma function the ""best"" extension of the factorial function to the reals? (7 answers) Closed 10 years ago . The Gamma function is a generalization of the factorial defined by Euler as: $$\Gamma(z)=\int\limits_{0}^{\infty}t^{z-1}e^{-t}\,dt$$ for $z\in\mathbb{C}$ with positive real part. It satisfies, $\Gamma(n)=(n-1)!$ for all $n\in\mathbb{N}$. My question is why we choose this particular function from all the functions that satisfy the previous property. And, how is the Gamma function deduced?",,"['real-analysis', 'gamma-function']"
64,Hölder 1/2 condition for curve,Hölder 1/2 condition for curve,,"Does there exist a continuous  map $\gamma:[0,1] \to \mathbb{R}^n$ s.t. $$|\gamma(x)-\gamma(y)| \ge |x-y|^{\frac 1 2}  $$ for all $x,y \in [0,1]$?","Does there exist a continuous  map $\gamma:[0,1] \to \mathbb{R}^n$ s.t. $$|\gamma(x)-\gamma(y)| \ge |x-y|^{\frac 1 2}  $$ for all $x,y \in [0,1]$?",,['real-analysis']
65,Is $f(x)=o(x^\alpha)$ for every $\alpha\gt0$ enough to know that $\int_c^x dt/f(t) \sim x/f(x)$?,Is  for every  enough to know that ?,f(x)=o(x^\alpha) \alpha\gt0 \int_c^x dt/f(t) \sim x/f(x),"Let $f$ be a monotone increasing function $[c,\infty)\rightarrow \mathbb{R}^{\geq 0}$ satisfying $f(x)/x^\alpha\to 0$ for all $\alpha>0$. Is it true that $\int_c^x \frac{dt}{f(t)} \sim \frac{x}{f(x)}$ as $x\to \infty$? This question is inspired by the fact that $\operatorname{li}(x)\sim x/\log x$. It seems to me that on heuristic grounds it should be true: in the case $f(t)=\log t$, what's going on is that $\log$ grows slow enough that it spends ""most of the time"" on the interval $[c,x]$ close to its value at the endpoint.  Thus $t/\log t$ is well-approximated by $t/\log x$ on ""most of"" the interval of integration.  The length of the interval is asymptotically $x$, and this is the explanation, morally anyway. Thus it should be that other functions that grow ""slow enough"" work the same way. And it seems to me (this is extremely unrigorous) that if $f(x)/x^\alpha\to 0,\forall \alpha>0$, that should be ""slow enough"", because $$\int_c^x \frac{dt}{t^\alpha} = \frac{1}{1-\alpha}\cdot \frac{x}{x^\alpha} - \text{const.}$$ and $\frac{1}{1-\alpha}\to 1$ as $\alpha\to 0$. However, I have been unable to satisfy myself with a proper proof. The proof I know in the $f=\log$ case doesn't generalize because it involves the specifics of a particular integration by parts and involves facts like $\text{const.}/\log x\to 0$ that seem likely to be extrinsic to the result to me. Is there a proof that works in the generality in which I've posed the question? Or is it not actually true?","Let $f$ be a monotone increasing function $[c,\infty)\rightarrow \mathbb{R}^{\geq 0}$ satisfying $f(x)/x^\alpha\to 0$ for all $\alpha>0$. Is it true that $\int_c^x \frac{dt}{f(t)} \sim \frac{x}{f(x)}$ as $x\to \infty$? This question is inspired by the fact that $\operatorname{li}(x)\sim x/\log x$. It seems to me that on heuristic grounds it should be true: in the case $f(t)=\log t$, what's going on is that $\log$ grows slow enough that it spends ""most of the time"" on the interval $[c,x]$ close to its value at the endpoint.  Thus $t/\log t$ is well-approximated by $t/\log x$ on ""most of"" the interval of integration.  The length of the interval is asymptotically $x$, and this is the explanation, morally anyway. Thus it should be that other functions that grow ""slow enough"" work the same way. And it seems to me (this is extremely unrigorous) that if $f(x)/x^\alpha\to 0,\forall \alpha>0$, that should be ""slow enough"", because $$\int_c^x \frac{dt}{t^\alpha} = \frac{1}{1-\alpha}\cdot \frac{x}{x^\alpha} - \text{const.}$$ and $\frac{1}{1-\alpha}\to 1$ as $\alpha\to 0$. However, I have been unable to satisfy myself with a proper proof. The proof I know in the $f=\log$ case doesn't generalize because it involves the specifics of a particular integration by parts and involves facts like $\text{const.}/\log x\to 0$ that seem likely to be extrinsic to the result to me. Is there a proof that works in the generality in which I've posed the question? Or is it not actually true?",,"['real-analysis', 'analysis']"
66,Representation of elements in Cantor set.,Representation of elements in Cantor set.,,"Every number in $[0,1]$ has a ternary expansion $$x=\sum_{k=1}^{\infty}a_k3^{-k}$$ where $a_k=0,1,$ or $2$.Note that this decomposition is not unique since, for example, $1/3=\sum_{k=2}^{\infty}2/3^k$.Prove that $x\in \text{Cantor set}$ if and only if $x$ has a representation as above where every $a_k$ is either $0$ or $2$ I have tried some points in Cantor set. for example: $1/3=\sum_{k\geq 2} 2/3^k$ and $1/9=\sum_{k\geq 3} 2/3^k$,but I can't do it for all points in the Cantor set. I think I can't express a general point in that set. thanks very much","Every number in $[0,1]$ has a ternary expansion $$x=\sum_{k=1}^{\infty}a_k3^{-k}$$ where $a_k=0,1,$ or $2$.Note that this decomposition is not unique since, for example, $1/3=\sum_{k=2}^{\infty}2/3^k$.Prove that $x\in \text{Cantor set}$ if and only if $x$ has a representation as above where every $a_k$ is either $0$ or $2$ I have tried some points in Cantor set. for example: $1/3=\sum_{k\geq 2} 2/3^k$ and $1/9=\sum_{k\geq 3} 2/3^k$,but I can't do it for all points in the Cantor set. I think I can't express a general point in that set. thanks very much",,['real-analysis']
67,Can a sequence of a function with a single variable be thought about as a function with two variables?,Can a sequence of a function with a single variable be thought about as a function with two variables?,,"Long title, but first off is it logically ok to think of $\{f_n(x)\}$ as $f(n,x)$ where $n$ is restricted to a natural number? Second, would this at all be useful? Thus far in my study of sequences of functions you deal strictly with convergence (similar to sequences in general) - either pointwise or universal convergence. It seems interesting to me because then we can consider some function of two variables as converging to a function with one. It seems as though this would be useful... So it's as if we could have the definition of convergence be: $$ \forall \; \epsilon > 0 \\ \exists \; N \in \mathbb{N} : \lvert f(n,x) - f(x) \rvert < \epsilon $$ and then could this possibly be extrapolated out to an $n$ with any domain one wishes? EDIT: as Andre reminded me, sequences can be thought of as functions, i.e. $$ a : \mathbb{N} \to \mathbb{R} $$ however it seems to be then that this becomes the concatenation of functions, i.e. $$ g : \mathbb{R} \to \mathbb{R}\quad f : \{ \mathbb{R} , \mathbb{N} \} \to \mathbb{R} $$ and $f_n (x) = f(g(x),n)$ again, would this be an ok way of thinking about sequences of functions, is it useful, and could this be extrapolated so that we can view any $n$ variable function as the concatenation of $n$ functions, and the variables dont need to have any specific domain?","Long title, but first off is it logically ok to think of as where is restricted to a natural number? Second, would this at all be useful? Thus far in my study of sequences of functions you deal strictly with convergence (similar to sequences in general) - either pointwise or universal convergence. It seems interesting to me because then we can consider some function of two variables as converging to a function with one. It seems as though this would be useful... So it's as if we could have the definition of convergence be: and then could this possibly be extrapolated out to an with any domain one wishes? EDIT: as Andre reminded me, sequences can be thought of as functions, i.e. however it seems to be then that this becomes the concatenation of functions, i.e. and again, would this be an ok way of thinking about sequences of functions, is it useful, and could this be extrapolated so that we can view any variable function as the concatenation of functions, and the variables dont need to have any specific domain?","\{f_n(x)\} f(n,x) n 
\forall \; \epsilon > 0 \\
\exists \; N \in \mathbb{N} : \lvert f(n,x) - f(x) \rvert < \epsilon
 n 
a : \mathbb{N} \to \mathbb{R}
 
g : \mathbb{R} \to \mathbb{R}\quad
f : \{ \mathbb{R} , \mathbb{N} \} \to \mathbb{R}
 f_n (x) = f(g(x),n) n n","['real-analysis', 'sequences-and-series', 'analysis', 'functions']"
68,"Continuity of $d(x,A)$",Continuity of,"d(x,A)","I am doing a head-check here. I keep seeing this theorem quoted as requiring $A$ to be closed (as in Is the distance function in a metric space (uniformly) continuous? ), but I don't think that it is needed. Theorem. Let $(X,d)$ be a metric space and $\emptyset \neq A\subseteq X$. Then $x \longmapsto d(x,A)$ is Lipschitz continuous. Proof. Fix $x,y \in X$. We note that for any $a \in A$ we have $d(x,a) \leq d(x,y) + d(y,a)$. Taking the infimum over $a \in A$ then gives $d(x,A) \leq d(x,y) + d(y,A)$. It follows quickly that $$ |d(x,A)-d(y,A)| \leq d(x,y) $$ and the claim is proved. Have I accidentally used closedness of $A$ somewhere in my proof? I don't think it is necessary. Maybe the reason that it is usually quoted with $A$ closed is so that there is an $a \in A$ with $d(x,A) = d(x,a)$? (As @Martin points out, this also requires some compactness assumption on the unit ball, e.g. it holds if $X=\mathbb{R}^n$ with Euclidean metric)","I am doing a head-check here. I keep seeing this theorem quoted as requiring $A$ to be closed (as in Is the distance function in a metric space (uniformly) continuous? ), but I don't think that it is needed. Theorem. Let $(X,d)$ be a metric space and $\emptyset \neq A\subseteq X$. Then $x \longmapsto d(x,A)$ is Lipschitz continuous. Proof. Fix $x,y \in X$. We note that for any $a \in A$ we have $d(x,a) \leq d(x,y) + d(y,a)$. Taking the infimum over $a \in A$ then gives $d(x,A) \leq d(x,y) + d(y,A)$. It follows quickly that $$ |d(x,A)-d(y,A)| \leq d(x,y) $$ and the claim is proved. Have I accidentally used closedness of $A$ somewhere in my proof? I don't think it is necessary. Maybe the reason that it is usually quoted with $A$ closed is so that there is an $a \in A$ with $d(x,A) = d(x,a)$? (As @Martin points out, this also requires some compactness assumption on the unit ball, e.g. it holds if $X=\mathbb{R}^n$ with Euclidean metric)",,"['real-analysis', 'metric-spaces']"
69,Proving integrability in integration by parts in Rudin's text,Proving integrability in integration by parts in Rudin's text,,"Integration by parts, as stated in W. Rudin's Principles of Mathematical Analysis , Theorem 6.22, goes as follows: Suppose F and G are differentiable functions in $[a,b]$, $F'=f\in \mathcal{R}$, and $G'= g\in \mathcal{R}$. Then $\int_a^bF(x)g(x)dx = F(b)G(b) - F(a)G(a) - \int_a^bf(x)G(x)dx$. The proof is to put $H(x)=F(x)G(x)$ and apply the fundamental theorem of calculus to H and its derivative. The fundamental theorem of calculus is stated as: If $f \in \mathcal{R}$ on $[a,b]$ and if there is a differentiable function $F$ on $[a,b]$ s.t. $F'=f$, then $\int_a^bf(x)dx=F(b)-F(a)$. We also have, from an earlier theorem (6.13), that: If $f \in \mathcal{R}$ and $g \in \mathcal{R}$ on $[a,b]$, then $fg \in \mathcal{R}$. Rudin notes that ""$H' \in \mathcal{R}$, by Theorem 6.13"" (above). Is that theorem really enough to prove this? It doesn't seem to be. After all, if $H(x)=F(x)G(x)$ then $H'(x)=F(x)g(x)+f(x)G(x)$, and we only have $f(x)g(x) \in \mathcal{R}$, while $F(x)g(x) \neq f(x)g(x)$. Where do we get integrability of $F(x)g(x)$ and $f(x)G(x)$? I'd be grateful for any pointers. Thanks!","Integration by parts, as stated in W. Rudin's Principles of Mathematical Analysis , Theorem 6.22, goes as follows: Suppose F and G are differentiable functions in $[a,b]$, $F'=f\in \mathcal{R}$, and $G'= g\in \mathcal{R}$. Then $\int_a^bF(x)g(x)dx = F(b)G(b) - F(a)G(a) - \int_a^bf(x)G(x)dx$. The proof is to put $H(x)=F(x)G(x)$ and apply the fundamental theorem of calculus to H and its derivative. The fundamental theorem of calculus is stated as: If $f \in \mathcal{R}$ on $[a,b]$ and if there is a differentiable function $F$ on $[a,b]$ s.t. $F'=f$, then $\int_a^bf(x)dx=F(b)-F(a)$. We also have, from an earlier theorem (6.13), that: If $f \in \mathcal{R}$ and $g \in \mathcal{R}$ on $[a,b]$, then $fg \in \mathcal{R}$. Rudin notes that ""$H' \in \mathcal{R}$, by Theorem 6.13"" (above). Is that theorem really enough to prove this? It doesn't seem to be. After all, if $H(x)=F(x)G(x)$ then $H'(x)=F(x)g(x)+f(x)G(x)$, and we only have $f(x)g(x) \in \mathcal{R}$, while $F(x)g(x) \neq f(x)g(x)$. Where do we get integrability of $F(x)g(x)$ and $f(x)G(x)$? I'd be grateful for any pointers. Thanks!",,['real-analysis']
70,Showing ${\frak c}:=|\Bbb{R}|=2^{\aleph_0}$ using only the axioms of the reals,Showing  using only the axioms of the reals,{\frak c}:=|\Bbb{R}|=2^{\aleph_0},"It is a well-known fact that the cardinality of the continuum, ${\frak c}:=|\Bbb{R}|$, is the same as the cardinality of the powerset of the natural numbers, but I've tasked myself with proving the result from scratch, and naturally a whole bevy of little issues come up that I hadn't thought of to begin with. The kicker here is that I want to prove this using only the axioms of the real numbers, not any construction-dependent facts. It is sufficient by Schroeder-Bernstein to prove ${\frak c}\le2^{\aleph_0}$ and ${\frak c}\ge2^{\aleph_0}$, and one direction is clear: the function $f(A)=\sum_{i\in A}3^{-i}$ for $A\subseteq\Bbb{N}$ is an injective map from ${\cal P}(\Bbb{N})\approx2^{\aleph_0}$ to the Cantor set, which is a subset of the reals (note that by using $3^{-i}$ instead of $2^{-i}$, I avoid that silly technicality about non-unique decimal expansions), so ${\frak c}\ge2^{\aleph_0}$. To prove the converse, I don't know whether it can be done using only the axioms (stated here ). Given a construction of the reals by Dedekind cuts, we note that each real number is a set of rationals, so $\Bbb{R}\subseteq{\cal P}(\Bbb{Q})\Rightarrow{\frak c}\le2^\Bbb{Q}=2^{\aleph_0}$. Alternatively, if we use Cauchy sequences, we know that each real number is a an equivalence class of sequences of rational numbers, so ${\frak c}\le_*\Bbb{Q^N}=2^{\aleph_0}$ (where the $\le_*$ indicates the existence of a surjection), and after an application of AC, we get ${\frak c}\le2^{\aleph_0}$. But if I had no construction, can it even be done? I've heard some stuff about $\Bbb{R}$ being ""the unique order-complete field up to isomorphism"" or something like that, so it should be possible to use a pure axiomatic approach to get the result, but the principal counterexample in my mind is the surreal numbers $\sf{No}$, which (I believe) also satisfy all the listed axioms (except the existence one, since $\sf{No}\supseteq\sf{On}$), and obviously there are a lot more surreal numbers than real numbers. Are there missing axioms that I need to prove this?","It is a well-known fact that the cardinality of the continuum, ${\frak c}:=|\Bbb{R}|$, is the same as the cardinality of the powerset of the natural numbers, but I've tasked myself with proving the result from scratch, and naturally a whole bevy of little issues come up that I hadn't thought of to begin with. The kicker here is that I want to prove this using only the axioms of the real numbers, not any construction-dependent facts. It is sufficient by Schroeder-Bernstein to prove ${\frak c}\le2^{\aleph_0}$ and ${\frak c}\ge2^{\aleph_0}$, and one direction is clear: the function $f(A)=\sum_{i\in A}3^{-i}$ for $A\subseteq\Bbb{N}$ is an injective map from ${\cal P}(\Bbb{N})\approx2^{\aleph_0}$ to the Cantor set, which is a subset of the reals (note that by using $3^{-i}$ instead of $2^{-i}$, I avoid that silly technicality about non-unique decimal expansions), so ${\frak c}\ge2^{\aleph_0}$. To prove the converse, I don't know whether it can be done using only the axioms (stated here ). Given a construction of the reals by Dedekind cuts, we note that each real number is a set of rationals, so $\Bbb{R}\subseteq{\cal P}(\Bbb{Q})\Rightarrow{\frak c}\le2^\Bbb{Q}=2^{\aleph_0}$. Alternatively, if we use Cauchy sequences, we know that each real number is a an equivalence class of sequences of rational numbers, so ${\frak c}\le_*\Bbb{Q^N}=2^{\aleph_0}$ (where the $\le_*$ indicates the existence of a surjection), and after an application of AC, we get ${\frak c}\le2^{\aleph_0}$. But if I had no construction, can it even be done? I've heard some stuff about $\Bbb{R}$ being ""the unique order-complete field up to isomorphism"" or something like that, so it should be possible to use a pure axiomatic approach to get the result, but the principal counterexample in my mind is the surreal numbers $\sf{No}$, which (I believe) also satisfy all the listed axioms (except the existence one, since $\sf{No}\supseteq\sf{On}$), and obviously there are a lot more surreal numbers than real numbers. Are there missing axioms that I need to prove this?",,"['real-analysis', 'elementary-set-theory']"
71,Bounded variation implies Borel measurable,Bounded variation implies Borel measurable,,"Suppose that $f\colon[a, b] \to \mathbb{R}$ is a function of bounded variation. Show that $f$ is Borel measurable. I was wondering if I could get a hint.","Suppose that $f\colon[a, b] \to \mathbb{R}$ is a function of bounded variation. Show that $f$ is Borel measurable. I was wondering if I could get a hint.",,"['real-analysis', 'measure-theory', 'bounded-variation']"
72,Does anyone know of a proof ot the recursion for Legendre polynomials which follows directly from Gram Schmidt?,Does anyone know of a proof ot the recursion for Legendre polynomials which follows directly from Gram Schmidt?,,"If we start with the functions $\{1,x,x^2,...\}$ on $[-1,1]$ and apply the Gram-Schmidt process, we obtain the nonnormalized Legendre Polynomials. Does anyone know a proof of the recursive relation $$ (n+1)P_{n+1}(x) = (2n+1)xP_n(x)-nP_{n-1}(x) $$ which uses nothing more than this, and which does not depend on the differential equation, the generating function, Rodrigues' formula, etc?","If we start with the functions $\{1,x,x^2,...\}$ on $[-1,1]$ and apply the Gram-Schmidt process, we obtain the nonnormalized Legendre Polynomials. Does anyone know a proof of the recursive relation $$ (n+1)P_{n+1}(x) = (2n+1)xP_n(x)-nP_{n-1}(x) $$ which uses nothing more than this, and which does not depend on the differential equation, the generating function, Rodrigues' formula, etc?",,"['real-analysis', 'linear-algebra']"
73,Extension of a continuous map on $ {\mathbf{GL}_{n}}(\mathbb{R}) $ to $ {\mathbf{M}_{n}}(\mathbb{R}) $.,Extension of a continuous map on  to ., {\mathbf{GL}_{n}}(\mathbb{R})   {\mathbf{M}_{n}}(\mathbb{R}) ,I was reading in my analysis textbook that the map $ f: {\mathbf{GL}_{n}}(\mathbb{R}) \to {\mathbf{GL}_{n}}(\mathbb{R}) $ defined by $ f(A) := A^{-1} $ is a continuous map. I also saw that $ {\mathbf{GL}_{n}}(\mathbb{R}) $ is dense in $ {\mathbf{M}_{n}}(\mathbb{R}) $. My question is: What is the unique extension of $ f $ to $ {\mathbf{M}_{n}}(\mathbb{R}) $?,I was reading in my analysis textbook that the map $ f: {\mathbf{GL}_{n}}(\mathbb{R}) \to {\mathbf{GL}_{n}}(\mathbb{R}) $ defined by $ f(A) := A^{-1} $ is a continuous map. I also saw that $ {\mathbf{GL}_{n}}(\mathbb{R}) $ is dense in $ {\mathbf{M}_{n}}(\mathbb{R}) $. My question is: What is the unique extension of $ f $ to $ {\mathbf{M}_{n}}(\mathbb{R}) $?,,"['real-analysis', 'general-topology', 'matrices', 'continuity']"
74,Pointwise version of Fejer's Theorem,Pointwise version of Fejer's Theorem,,"In Principles of Mathematical Analysis Rudin asks that if $f$ is integrable in $[-\pi,\pi]$ of period $2\pi$ and $f(x+)$ and $f(x-)$ exist for some $x$, then $$\lim_{N\rightarrow \infty}\sigma_N(f;x)=\frac{1}{2}[f(x+)+f(x-)].$$ How is this done?","In Principles of Mathematical Analysis Rudin asks that if $f$ is integrable in $[-\pi,\pi]$ of period $2\pi$ and $f(x+)$ and $f(x-)$ exist for some $x$, then $$\lim_{N\rightarrow \infty}\sigma_N(f;x)=\frac{1}{2}[f(x+)+f(x-)].$$ How is this done?",,['real-analysis']
75,References on Constrained Least Squares Problems?,References on Constrained Least Squares Problems?,,"I have met some constrained least square problems, for example, my last post . I found that there are various methods for slightly different constraints, and still I often had little clue about how to solve such problems that I have met. I have learned some nonlinear optimization and had some good references. I wish to learn more about constrained least square problems, such as what cases admit analytical solutions, and when a case has no analyitical solution, whether the case belongs to convex optimization, and if yes or no, what numerical methods can apply to it the best? So could someone recommend some references on this topic? Thank you very much!","I have met some constrained least square problems, for example, my last post . I found that there are various methods for slightly different constraints, and still I often had little clue about how to solve such problems that I have met. I have learned some nonlinear optimization and had some good references. I wish to learn more about constrained least square problems, such as what cases admit analytical solutions, and when a case has no analyitical solution, whether the case belongs to convex optimization, and if yes or no, what numerical methods can apply to it the best? So could someone recommend some references on this topic? Thank you very much!",,"['real-analysis', 'linear-algebra', 'reference-request', 'optimization', 'numerical-methods']"
76,"With $xy+yz+zx=-1$, proving: $x^2+2y^2+2z^2 .....$","With , proving:",xy+yz+zx=-1 x^2+2y^2+2z^2 .....,"Assuming $xy+yz+zx=-1$, prove that : $$x^2+2y^2+2z^2 \geq \frac{1+\sqrt{17}}{2}$$","Assuming $xy+yz+zx=-1$, prove that : $$x^2+2y^2+2z^2 \geq \frac{1+\sqrt{17}}{2}$$",,"['calculus', 'real-analysis', 'inequality']"
77,Convergence of nested radicals,Convergence of nested radicals,,"In general if we have $$\sqrt{a_0+\sqrt{a_1+\sqrt{a_2....}}}$$  Are there easy ways of finding out if it converges, like from infinite products and series? Are there ways for converting the question of convergence of the radical to a one about series, like the logarithm does for products?","In general if we have $$\sqrt{a_0+\sqrt{a_1+\sqrt{a_2....}}}$$  Are there easy ways of finding out if it converges, like from infinite products and series? Are there ways for converting the question of convergence of the radical to a one about series, like the logarithm does for products?",,"['real-analysis', 'convergence-divergence', 'nested-radicals']"
78,Is this Riemann-Integrable?,Is this Riemann-Integrable?,,Let $\lfloor x\rfloor$ be the integer part function. Let $$f(x)= \sum_{n=0}^\infty \frac{(-1)^n}{2^n} \lfloor nx\rfloor$$ Is $f(x)$ Riemann-Integrable ? Thanks in advance,Let $\lfloor x\rfloor$ be the integer part function. Let $$f(x)= \sum_{n=0}^\infty \frac{(-1)^n}{2^n} \lfloor nx\rfloor$$ Is $f(x)$ Riemann-Integrable ? Thanks in advance,,['real-analysis']
79,Infinite series : $\sum_{n=1}^{\infty}\prod_{i=1}^n\frac{(3i-1)}{(4i-3)} $,Infinite series :,\sum_{n=1}^{\infty}\prod_{i=1}^n\frac{(3i-1)}{(4i-3)} ,How to evaluate this? $$\sum_{n=1}^{\infty}\prod_{i=1}^n\frac{(3i-1)}{(4i-3)} $$,How to evaluate this? $$\sum_{n=1}^{\infty}\prod_{i=1}^n\frac{(3i-1)}{(4i-3)} $$,,"['calculus', 'real-analysis', 'sequences-and-series']"
80,How do i prove that $C_c(X)$ is a vector space?,How do i prove that  is a vector space?,C_c(X),"Let $X$ be a topological space and $C_c(X)$ be the set of all continuous complex functions on $X$ whose support is compact. Let $f,g\in C_c(X)$. Trivially, $f+g$ are continuous, but how do i prove that supp$(f+g)$ is compact?","Let $X$ be a topological space and $C_c(X)$ be the set of all continuous complex functions on $X$ whose support is compact. Let $f,g\in C_c(X)$. Trivially, $f+g$ are continuous, but how do i prove that supp$(f+g)$ is compact?",,['real-analysis']
81,$\displaystyle\lim_{n\to \infty} \sqrt[n]{x_n+y_n}$ is the maximum of the sequences' nth root limits,is the maximum of the sequences' nth root limits,\displaystyle\lim_{n\to \infty} \sqrt[n]{x_n+y_n},"Let $x_n $ and $y_n$ be two sequences of positive real numbers then  $\displaystyle\lim_{n\to \infty} \sqrt[n]{x_n+y_n}=  \max\left\{\lim_{n\to \infty} \sqrt[n]{x_n} \hspace{1 mm} ; \lim_{n\to \infty} \sqrt[n]{y_n} \right\}  $ I showed the result if both $x_n $ and $y_n$ converge to different limits but I don't know what to do in case they are equal(the limits). EDIT: let $\displaystyle\lim_{n\to \infty} \sqrt[n]{x_n}=l$ and $\displaystyle\lim_{n\to \infty} \sqrt[n]{y_n}=l'$ and $l>l'$ we have $|\sqrt[n]{x_n}-l|<\epsilon$ and  $|\sqrt[n]{y_n}-l'|<\epsilon$ take $\epsilon=\displaystyle\frac{l-l'}{2}$ then $l-\displaystyle\frac{l-l'}{2}<\sqrt[n]{x_n}$ $\implies \left(\displaystyle\frac{l+l'}{2}\right)^n<x_n$ we also have $\sqrt[n]{y_n}<l'+\displaystyle\frac{l-l'}{2}$ then ${y_n}<\left(\displaystyle\frac{l+l'}{2}\right)^n$ hence $x_n>y_n$ then $\sqrt[n]x_n <\sqrt[n]{x_n+y_n}< \sqrt[n]{2x_n}$,hence by the squeeze theorem $\sqrt[n]{x_n+y_n}=l$ But what if $l=l'$","Let $x_n $ and $y_n$ be two sequences of positive real numbers then  $\displaystyle\lim_{n\to \infty} \sqrt[n]{x_n+y_n}=  \max\left\{\lim_{n\to \infty} \sqrt[n]{x_n} \hspace{1 mm} ; \lim_{n\to \infty} \sqrt[n]{y_n} \right\}  $ I showed the result if both $x_n $ and $y_n$ converge to different limits but I don't know what to do in case they are equal(the limits). EDIT: let $\displaystyle\lim_{n\to \infty} \sqrt[n]{x_n}=l$ and $\displaystyle\lim_{n\to \infty} \sqrt[n]{y_n}=l'$ and $l>l'$ we have $|\sqrt[n]{x_n}-l|<\epsilon$ and  $|\sqrt[n]{y_n}-l'|<\epsilon$ take $\epsilon=\displaystyle\frac{l-l'}{2}$ then $l-\displaystyle\frac{l-l'}{2}<\sqrt[n]{x_n}$ $\implies \left(\displaystyle\frac{l+l'}{2}\right)^n<x_n$ we also have $\sqrt[n]{y_n}<l'+\displaystyle\frac{l-l'}{2}$ then ${y_n}<\left(\displaystyle\frac{l+l'}{2}\right)^n$ hence $x_n>y_n$ then $\sqrt[n]x_n <\sqrt[n]{x_n+y_n}< \sqrt[n]{2x_n}$,hence by the squeeze theorem $\sqrt[n]{x_n+y_n}=l$ But what if $l=l'$",,"['real-analysis', 'sequences-and-series']"
82,Proving the integral of the Dirac delta function is 1,Proving the integral of the Dirac delta function is 1,,"Was wondering if my solution is mathematically accurate enough: The question in the book yields: Derive   $$ 1=\int_{-\infty}^{\infty} \delta(x-x_i)\ dx_i $$   From   $$ f(x)=\int_{-\infty}^{\infty} f(x_i)\delta(x-x_i)\ dx_i  $$   [Hint: let $f(x)=1$] My method is: $$ f(x)=\int_{-\infty}^\infty f(x_i)\delta(x-x_i)\ dx_i $$ $$ f(x)=1 $$ so   $$  1=\int_{-\infty}^{\infty} \delta(x-x_i)\ dx_i $$ I think this is not correct. Does someone know is this is correct, or how to do it better? Cheers","Was wondering if my solution is mathematically accurate enough: The question in the book yields: Derive   $$ 1=\int_{-\infty}^{\infty} \delta(x-x_i)\ dx_i $$   From   $$ f(x)=\int_{-\infty}^{\infty} f(x_i)\delta(x-x_i)\ dx_i  $$   [Hint: let $f(x)=1$] My method is: $$ f(x)=\int_{-\infty}^\infty f(x_i)\delta(x-x_i)\ dx_i $$ $$ f(x)=1 $$ so   $$  1=\int_{-\infty}^{\infty} \delta(x-x_i)\ dx_i $$ I think this is not correct. Does someone know is this is correct, or how to do it better? Cheers",,"['real-analysis', 'integration', 'distribution-theory']"
83,Commuting limit and integral in an improper Riemann intergral,Commuting limit and integral in an improper Riemann intergral,,"How does one prove the following limit? $$ \lim_{b\rightarrow 0}\int_0^\infty{\frac{\sin x}{x}e^{-bx}dx} = \lim_{N\rightarrow\infty}\int_0^N{\frac{\sin x}{x}dx} $$ I don't think I could apply the Dominated Convergence Theorem outright because any estimate I can come up with is not $L^1$, so I guess this becomes a question of commuting limits.","How does one prove the following limit? $$ \lim_{b\rightarrow 0}\int_0^\infty{\frac{\sin x}{x}e^{-bx}dx} = \lim_{N\rightarrow\infty}\int_0^N{\frac{\sin x}{x}dx} $$ I don't think I could apply the Dominated Convergence Theorem outright because any estimate I can come up with is not $L^1$, so I guess this becomes a question of commuting limits.",,"['calculus', 'real-analysis', 'analysis']"
84,$\lim_{n\rightarrow \infty } \int_{a}^{b}g_{n}(x)\sin (2n\pi x)dx=0$ where $g_{n}$ is uniformly Lipschitz,where  is uniformly Lipschitz,\lim_{n\rightarrow \infty } \int_{a}^{b}g_{n}(x)\sin (2n\pi x)dx=0 g_{n},"Let {$g_{n}$}be a bounded sequence of functions on $[0,1]$ which is uniformly Lipschitz. That is, there is a constant $M$ (independent of $n$) such that for all $n$, $|g_{n}(x)-g_n(y)|\leq M|x-y|$ for all $x,y\in [0,1]$ and $|g_{n}(x)|\leq M$ for all $x\in [0,1]$. Then I have the following two questions: (a) prove for all any $0\leq a\leq b\leq 1$, $$\lim_{n\rightarrow \infty } \int_{a}^{b}g_{n}(x)\sin (2n\pi x)\,dx=0. $$ (b) prove that for any $f\in L^{1}[0,1]$, $$\lim_{n\rightarrow \infty } \int_{0}^{1}f(x)g_{n}(x)\sin (2n\pi x)\,dx=0.$$","Let {$g_{n}$}be a bounded sequence of functions on $[0,1]$ which is uniformly Lipschitz. That is, there is a constant $M$ (independent of $n$) such that for all $n$, $|g_{n}(x)-g_n(y)|\leq M|x-y|$ for all $x,y\in [0,1]$ and $|g_{n}(x)|\leq M$ for all $x\in [0,1]$. Then I have the following two questions: (a) prove for all any $0\leq a\leq b\leq 1$, $$\lim_{n\rightarrow \infty } \int_{a}^{b}g_{n}(x)\sin (2n\pi x)\,dx=0. $$ (b) prove that for any $f\in L^{1}[0,1]$, $$\lim_{n\rightarrow \infty } \int_{0}^{1}f(x)g_{n}(x)\sin (2n\pi x)\,dx=0.$$",,"['real-analysis', 'measure-theory']"
85,Is this true for $3$ by $2$ matrices?,Is this true for  by  matrices?,3 2,Suppose that $M$ is a $3$ by $2$ matrix in which every $2$ by $2$ submatrix is invertible. Is it true that $M$ always has a $2$ by $2$ submatrix $M_{1}$ such that $\left\Vert M_{1}\right\Vert ^{2}\left\Vert M_{1}^{-1}\right\Vert ^{2}\lambda_{2}\left(MM^{T}\right)\ge\left\Vert M\right\Vert ^{2}$ where $\left\Vert \cdot\right\Vert $ is the norm of a matrix (refer to here for the definition) and $\lambda_{2}\left(\cdot\right)$ is the second largest eigenvalue of a $3$ by $3$ symmetric matrix? Thanks,Suppose that $M$ is a $3$ by $2$ matrix in which every $2$ by $2$ submatrix is invertible. Is it true that $M$ always has a $2$ by $2$ submatrix $M_{1}$ such that $\left\Vert M_{1}\right\Vert ^{2}\left\Vert M_{1}^{-1}\right\Vert ^{2}\lambda_{2}\left(MM^{T}\right)\ge\left\Vert M\right\Vert ^{2}$ where $\left\Vert \cdot\right\Vert $ is the norm of a matrix (refer to here for the definition) and $\lambda_{2}\left(\cdot\right)$ is the second largest eigenvalue of a $3$ by $3$ symmetric matrix? Thanks,,"['linear-algebra', 'real-analysis', 'analysis']"
86,Prove a set is at most countable,Prove a set is at most countable,,"Suppose that for each $\lambda$ in a set $\Lambda$ we have a positive real number $a_\lambda > 0$. Suppose also that for any natural number $n$ and any $\lambda_1, \cdots, \lambda_n \in \Lambda$ we have $$ \sum_{ i = 1 }^n a_{ \lambda_i } < 1 $$ Prove that the set $\Lambda$ is at most countable. I know that every $0 < a_{ \lambda_i } < 1$. I was wondering if I could get a hint. Thanks!","Suppose that for each $\lambda$ in a set $\Lambda$ we have a positive real number $a_\lambda > 0$. Suppose also that for any natural number $n$ and any $\lambda_1, \cdots, \lambda_n \in \Lambda$ we have $$ \sum_{ i = 1 }^n a_{ \lambda_i } < 1 $$ Prove that the set $\Lambda$ is at most countable. I know that every $0 < a_{ \lambda_i } < 1$. I was wondering if I could get a hint. Thanks!",,['real-analysis']
87,Compact subsets in $l_\infty$ (converse of my last question),Compact subsets in  (converse of my last question),l_\infty,"(Converse of my last question) If $A \subseteq \ell_\infty$, and $A=\{l\in \ell_\infty: |l_n| \le b_n \}$, where $b_n$ is a sequence of real, non-negative numbers, then if $\lim (b_n) = 0$  it must mean that $A$ is compact subset of $X$. Take any sequence of sequences $(x_n)$, our goal is to construct a subsequence, $(x_{n_k})$ which will converge. Suppose for $n \geq N$ we have $|b_n|<\epsilon$ (by convergence of $(b_n)$). For the first $N-1$ points, however, I thought we could use Bolzanno-Weirstrass on each of the $N-1$ first terms (since $|x_n| \le b_n \forall n$) in the following way: apply Bolzano Weirstrass on all the first terms, then for the second terms apply it on the subsequence we got from the first terms, and so on... and claiming this subsequence will converge to $\{x_1,x_2,...,x_{N},...\}$ - edited, where $x_i$ is the limit of the $i_{th}$ subsequence. However, the subsequence created could have a few cases where we end up with no terms at the end of this inductive argument. My teacher told me to use Cantor Diagonalization to avoid this, but I don't see how this would work.","(Converse of my last question) If $A \subseteq \ell_\infty$, and $A=\{l\in \ell_\infty: |l_n| \le b_n \}$, where $b_n$ is a sequence of real, non-negative numbers, then if $\lim (b_n) = 0$  it must mean that $A$ is compact subset of $X$. Take any sequence of sequences $(x_n)$, our goal is to construct a subsequence, $(x_{n_k})$ which will converge. Suppose for $n \geq N$ we have $|b_n|<\epsilon$ (by convergence of $(b_n)$). For the first $N-1$ points, however, I thought we could use Bolzanno-Weirstrass on each of the $N-1$ first terms (since $|x_n| \le b_n \forall n$) in the following way: apply Bolzano Weirstrass on all the first terms, then for the second terms apply it on the subsequence we got from the first terms, and so on... and claiming this subsequence will converge to $\{x_1,x_2,...,x_{N},...\}$ - edited, where $x_i$ is the limit of the $i_{th}$ subsequence. However, the subsequence created could have a few cases where we end up with no terms at the end of this inductive argument. My teacher told me to use Cantor Diagonalization to avoid this, but I don't see how this would work.",,"['real-analysis', 'sequences-and-series', 'metric-spaces', 'compactness']"
88,$\sin(1/x)$ not uniformly continuous,not uniformly continuous,\sin(1/x),"I looked at your answer to the question posted How to prove $\sin(1/x)$ is not uniformly continuous thank you for your helpful explanation on how to think about it. But I am failing to see why we will choose $x=\frac{1}{2πk−π/2}$ . I know that we want something $\sin(1/x)=1$ and $\sin(1/y)=−1$, but how did you get that answer. Would you mind elucidating on your motivation? Also, I am struggling to get an intuitive idea to work with continuous uniform problems. I understand the definition, but am failing to work with it. Thanks","I looked at your answer to the question posted How to prove $\sin(1/x)$ is not uniformly continuous thank you for your helpful explanation on how to think about it. But I am failing to see why we will choose $x=\frac{1}{2πk−π/2}$ . I know that we want something $\sin(1/x)=1$ and $\sin(1/y)=−1$, but how did you get that answer. Would you mind elucidating on your motivation? Also, I am struggling to get an intuitive idea to work with continuous uniform problems. I understand the definition, but am failing to work with it. Thanks",,['real-analysis']
89,Upper and lower limits of $f(x)=\cos x^2 - \cos (x+1)^2$ as $x\to\infty$,Upper and lower limits of  as,f(x)=\cos x^2 - \cos (x+1)^2 x\to\infty,"I believe that the upper limit is $+2$ and the lower limit is $-2$ . We have the trigonometric identity $$\cos x^2 -\cos (x+1)^2 =2\sin(x^2+x+\frac{1}{2})\sin(x+\frac{1}{2}).$$ We then make the substitution $x=m2\sqrt{\pi}$ , where $m$ is a positive integer, so that $$f(m2\sqrt{\pi})=2\sin^2(m2\sqrt{\pi}+ \frac{1}{2}).$$ From here we use the fact that for irrational $\gamma$ , the set $$S_\gamma =\{a+b\gamma\mid a\in\mathbb{Z}, b\in\mathbb{N}\}$$ is dense in the reals, together with the continuity of sine to prove that $f$ can get arbitrarily close to $+2$ on any domain of the form $(x,\infty)$ . We can show that the lower limit of $f$ is $-2$ using similar reasoning and the substitution $x=(2n+1)\sqrt{\pi}$ . Is this proof right? Is there another proof that doesn't use the density lemma or even better, that doesn't discretize the domain (ie. doesn't restrict the domain to a countable set)? (I can give a proof of the density lemma, but it hasn't been checked.)","I believe that the upper limit is and the lower limit is . We have the trigonometric identity We then make the substitution , where is a positive integer, so that From here we use the fact that for irrational , the set is dense in the reals, together with the continuity of sine to prove that can get arbitrarily close to on any domain of the form . We can show that the lower limit of is using similar reasoning and the substitution . Is this proof right? Is there another proof that doesn't use the density lemma or even better, that doesn't discretize the domain (ie. doesn't restrict the domain to a countable set)? (I can give a proof of the density lemma, but it hasn't been checked.)","+2 -2 \cos x^2 -\cos (x+1)^2 =2\sin(x^2+x+\frac{1}{2})\sin(x+\frac{1}{2}). x=m2\sqrt{\pi} m f(m2\sqrt{\pi})=2\sin^2(m2\sqrt{\pi}+ \frac{1}{2}). \gamma S_\gamma =\{a+b\gamma\mid a\in\mathbb{Z}, b\in\mathbb{N}\} f +2 (x,\infty) f -2 x=(2n+1)\sqrt{\pi}","['real-analysis', 'solution-verification', 'limsup-and-liminf']"
90,On cardinality of equivalence classes,On cardinality of equivalence classes,,"Say $R$ is the set of all Riemann integrable functions $f:[a,b]\rightarrow \mathbb{R}$. We define an equivalence relation in $R$ as follows: $f$ and $g$ are said to be integrally equivalent iff they differ only on a (Lebesgue) measure zero set. Prove that the collection of these equivalence classes has a cardinality of a continuum. Please let me remind that a function is Riemann integrable on $[a,b]$ iff it is bounded there and its discontinuity set there is a null set (has measure zero). It is known that $R$ has cardinality of $2^{|\mathbb{R}|}$, that is, greater that the continuum. Also, the collection of null sets has the same cardinality. Please give your opinions on how one could approach this problem. Complete proofs are also welcome. Thank you.","Say $R$ is the set of all Riemann integrable functions $f:[a,b]\rightarrow \mathbb{R}$. We define an equivalence relation in $R$ as follows: $f$ and $g$ are said to be integrally equivalent iff they differ only on a (Lebesgue) measure zero set. Prove that the collection of these equivalence classes has a cardinality of a continuum. Please let me remind that a function is Riemann integrable on $[a,b]$ iff it is bounded there and its discontinuity set there is a null set (has measure zero). It is known that $R$ has cardinality of $2^{|\mathbb{R}|}$, that is, greater that the continuum. Also, the collection of null sets has the same cardinality. Please give your opinions on how one could approach this problem. Complete proofs are also welcome. Thank you.",,"['real-analysis', 'elementary-set-theory', 'cardinals']"
91,Boundary Question in $\mathbb{R}^{2}$ (Manifolds),Boundary Question in  (Manifolds),\mathbb{R}^{2},"Given a subset $A$ of $\mathbb{R}^{n}$, a point $x \in \mathbb{R}^{n}$ is said to be in the boundary of A if and only if for every open rectangle $B\subseteq\mathbb{R}^{n}$ with $x\in B$, $B$ contains both a point of $A$ and a point of $\mathbb{R}^{n}\setminus A$. My question is from Spivak's Calculus on Manifolds: Construct a set $A \subseteq [0,1]\times [0,1]$ such that $A$ contains at most one point on each horizontal and vertical line but has boundary equal to $[0,1]\times[0,1]$.","Given a subset $A$ of $\mathbb{R}^{n}$, a point $x \in \mathbb{R}^{n}$ is said to be in the boundary of A if and only if for every open rectangle $B\subseteq\mathbb{R}^{n}$ with $x\in B$, $B$ contains both a point of $A$ and a point of $\mathbb{R}^{n}\setminus A$. My question is from Spivak's Calculus on Manifolds: Construct a set $A \subseteq [0,1]\times [0,1]$ such that $A$ contains at most one point on each horizontal and vertical line but has boundary equal to $[0,1]\times[0,1]$.",,"['real-analysis', 'manifolds']"
92,Range of bounded operator is of first category,Range of bounded operator is of first category,,"Let $T$ be a bounded operator from a Banach Space $X$ to a normed space $Y$ such that $T$ is not onto, but $R(T)\subset Y$ is dense. Prove that $R(T)$ is of first category and not no-where dense. Since $\mathring{\overline{R(T)}}=\mathring{Y}=Y\not=\emptyset$ the range is not nowhere dense but how to see that it is of first category?","Let $T$ be a bounded operator from a Banach Space $X$ to a normed space $Y$ such that $T$ is not onto, but $R(T)\subset Y$ is dense. Prove that $R(T)$ is of first category and not no-where dense. Since $\mathring{\overline{R(T)}}=\mathring{Y}=Y\not=\emptyset$ the range is not nowhere dense but how to see that it is of first category?",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
93,A set that is open in any metric space that contains it,A set that is open in any metric space that contains it,,Let $X$ be a set with the following property: For all metric space $Y$ such that $X\subset Y$ we have that $X$ is an open set on $Y$. $X$ should be the empty set?,Let $X$ be a set with the following property: For all metric space $Y$ such that $X\subset Y$ we have that $X$ is an open set on $Y$. $X$ should be the empty set?,,"['real-analysis', 'general-topology', 'metric-spaces']"
94,"Show that if $f^{-1}((\alpha, \infty))$ is open for any $\alpha \in \mathbb{R}$, then $f$ is lower-semicontinuous.","Show that if  is open for any , then  is lower-semicontinuous.","f^{-1}((\alpha, \infty)) \alpha \in \mathbb{R} f","I've tried looking for this question on this site, but I can't seem to find it. But if anyone can direct me to it, that would be great. But I'll pose my question in the mean time. As the title states, if $f^{-1}(\alpha, \infty)$ is open, then show that $f$ is lower-semicontinuous. Starting with the basics. The definition according to my professor gave on lower-semicontinuous is the following: Given a function $f: X \to \mathbb{R}$ on a topological space $X$ , $f$ is lower-semicontinuous if, for any $x \in X$ and for any $\epsilon > 0$ , there is a neighborhood $N$ of $x$ such that $f(x) - \epsilon < f(x')$ for all $x' \in N$ . I proved the conversed of this statement, but for this direction I seem to be stuck, been thinking for it for an hour or two. To my understanding a neighborhood of a point $x$ is a subset of $X$ such that it contains an open set which has $x$ . To me, it seems that I must consider if $x \in f^{-1}(\alpha, \infty)$ or $x \in f^{-1}(-\infty, \alpha)$ . I started with the consideration of $x \in f^{-1}(\alpha, \infty)$ . So by definition of pre-image, $f(x) \in (\alpha, \infty)$ . We have that $\alpha < f(x) < \infty$ .  So it seems to me that I want a neighborhood of $x$ such that it satisfies $f(x) - f(x') < \epsilon$ for all $x' \in$ nieghborhood of $x$ . Fix $x \in X$ . Then I went on the path of suppose that $x \in f^{-1}(\alpha, \infty)$ and using that using $f^{-1}(\alpha, b)$ where $b = f(x)$ . Since $f^{-1}(\alpha, \infty)$ can be written as the union (index starting at $k$ ) of $f^{-1}(\alpha, k)$ for $k > \alpha$ and $k \in \mathbb{R}$ , then $f^{-1}(\alpha, b)$ must be open. Then the problem is that I can't get a neighborhood of $x$ such that $f(x) - f(x') < \epsilon$ . I get a neighborhood of $x$ but it has elements such that $f(x) - f(x') \not<\epsilon$ . I'll currently will update the progress of my work. Hopefully I can progress somewhere. Thanks.","I've tried looking for this question on this site, but I can't seem to find it. But if anyone can direct me to it, that would be great. But I'll pose my question in the mean time. As the title states, if is open, then show that is lower-semicontinuous. Starting with the basics. The definition according to my professor gave on lower-semicontinuous is the following: Given a function on a topological space , is lower-semicontinuous if, for any and for any , there is a neighborhood of such that for all . I proved the conversed of this statement, but for this direction I seem to be stuck, been thinking for it for an hour or two. To my understanding a neighborhood of a point is a subset of such that it contains an open set which has . To me, it seems that I must consider if or . I started with the consideration of . So by definition of pre-image, . We have that .  So it seems to me that I want a neighborhood of such that it satisfies for all nieghborhood of . Fix . Then I went on the path of suppose that and using that using where . Since can be written as the union (index starting at ) of for and , then must be open. Then the problem is that I can't get a neighborhood of such that . I get a neighborhood of but it has elements such that . I'll currently will update the progress of my work. Hopefully I can progress somewhere. Thanks.","f^{-1}(\alpha, \infty) f f: X \to \mathbb{R} X f x \in X \epsilon > 0 N x f(x) - \epsilon < f(x') x' \in N x X x x \in f^{-1}(\alpha, \infty) x \in f^{-1}(-\infty, \alpha) x \in f^{-1}(\alpha, \infty) f(x) \in (\alpha, \infty) \alpha < f(x) < \infty x f(x) - f(x') < \epsilon x' \in x x \in X x \in f^{-1}(\alpha, \infty) f^{-1}(\alpha, b) b = f(x) f^{-1}(\alpha, \infty) k f^{-1}(\alpha, k) k > \alpha k \in \mathbb{R} f^{-1}(\alpha, b) x f(x) - f(x') < \epsilon x f(x) - f(x') \not<\epsilon","['real-analysis', 'general-topology', 'semicontinuous-functions']"
95,Existence of measure zero generating sets of additive real numbers.,Existence of measure zero generating sets of additive real numbers.,,"Just getting around to posting thoughts I had regarding this question about the additive structure of the real numbers. I was interested in which sets generate $(\mathbb{R},+)$. First, is the following argument correct? Given any set $A$ of positive Lebesgue measure, the Steinhaus theorem says that $A-A$ contains an open neighborhood of the origin. As per Arturo Magidin's answer to the original question, any such interval generates $\mathbb{R}$. Noting that $A-A$ is contained in the subgroup of $\mathbb{R}$ generated by $A$, we see that $A$ in fact generates $\mathbb{R}$. Second, are there sets of measure zero which generate $\mathbb{R}$? I looked around a bit, but am not really sure what tools to use to approach this question.","Just getting around to posting thoughts I had regarding this question about the additive structure of the real numbers. I was interested in which sets generate $(\mathbb{R},+)$. First, is the following argument correct? Given any set $A$ of positive Lebesgue measure, the Steinhaus theorem says that $A-A$ contains an open neighborhood of the origin. As per Arturo Magidin's answer to the original question, any such interval generates $\mathbb{R}$. Noting that $A-A$ is contained in the subgroup of $\mathbb{R}$ generated by $A$, we see that $A$ in fact generates $\mathbb{R}$. Second, are there sets of measure zero which generate $\mathbb{R}$? I looked around a bit, but am not really sure what tools to use to approach this question.",,"['real-analysis', 'group-theory', 'measure-theory']"
96,Convergence of $\prod\limits_{n=0}^\infty ( 1+u_{n}) $ when $\sum\limits_{n=0}^{\infty }u_{n}$ and $\sum \limits_{n=0}^\infty u_{n}^{2}$ diverge,Convergence of  when  and  diverge,\prod\limits_{n=0}^\infty ( 1+u_{n})  \sum\limits_{n=0}^{\infty }u_{n} \sum \limits_{n=0}^\infty u_{n}^{2},"I am trying to show that if $u_{0}=u_{1}=u_{2}=0$, and if, when $n>1$, $$u_{2n-1}=\dfrac {-1} {\sqrt {n}}, u_{2n}=\dfrac {1} {\sqrt {n}}+\dfrac {1} {n}+\dfrac {1} {n\sqrt {n}}$$ then $\prod \limits_{n=0}^{\infty }\left( 1+u_{n}\right) $ converges. We observe that $\sum \limits_{n=0}^{\infty }u_{n}$ and $\sum \limits_{n=0}^{\infty }u_{n}^{2}$ are divergent by ratio test. I am unsure how to proceed from here. Any help would be much appreciated. Could we argue possibly since $\lim _{n\rightarrow \infty }u_{n}=0$ that's why $\prod \limits_{n=0}^{\infty }\left( 1+u_{n}\right) $ converges ?","I am trying to show that if $u_{0}=u_{1}=u_{2}=0$, and if, when $n>1$, $$u_{2n-1}=\dfrac {-1} {\sqrt {n}}, u_{2n}=\dfrac {1} {\sqrt {n}}+\dfrac {1} {n}+\dfrac {1} {n\sqrt {n}}$$ then $\prod \limits_{n=0}^{\infty }\left( 1+u_{n}\right) $ converges. We observe that $\sum \limits_{n=0}^{\infty }u_{n}$ and $\sum \limits_{n=0}^{\infty }u_{n}^{2}$ are divergent by ratio test. I am unsure how to proceed from here. Any help would be much appreciated. Could we argue possibly since $\lim _{n\rightarrow \infty }u_{n}=0$ that's why $\prod \limits_{n=0}^{\infty }\left( 1+u_{n}\right) $ converges ?",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'infinite-product']"
97,Show a function is continuous if and only if it is both upper and lower semi-continuous,Show a function is continuous if and only if it is both upper and lower semi-continuous,,"Let $f$ be a real-valued function on $\mathbb{R}$. Show $f$ is continuous if and only if $f$ is both upper and lower semi-continuous, using the definition of continuity and semi-continuity based on open set. It's difficult for me to be rigourous here. For example for necessity. Let $O=\{f(x) : f(x) > a, x \in \mathbb{R}\}$ with $a$ real. By continuity, if $O$ is open then the pre-image of $O$ is open whence $f$ is lower semi-continuous. Is $O$ open? If so why? This point blocks me. I think I can do sufficiency correctly though. Suppose $f$ is both upper and lower semi-continuous. Let $O$ be an open set.  Since any open set in $\mathbb{R}$ can be expressed as a union of open sets of the form $(a,b)$, then $O=\cup (a_n,b_n)$ for some open sets $(a_n,b_n)$. Since each $(a_n,b_n)$ can be written has $(a_n,\infty) \cap (-\infty,b_n)$, by semi-continuity, the pre-image of each $(a_n,b_n)$ is open. Since $f^{-1}(O)=f^{-1}(\cup(a_n,b_n))=\cup f^{-1}(a_n,b_n)$, then the pre-image of $O$ is open anf $f$ is continuous. Definition: A function is continuous if and only if the pre-image of any open set is open. A function is lower semi-continuous if and only if the set $\{x \in \mathbb{R} : f(x) > a\}$ is open for any $a$ real. A function is upper semi-continuous if and only if the set $\{x \in \mathbb{R} : f(x) < a\}$ is open for any $a$ real.","Let $f$ be a real-valued function on $\mathbb{R}$. Show $f$ is continuous if and only if $f$ is both upper and lower semi-continuous, using the definition of continuity and semi-continuity based on open set. It's difficult for me to be rigourous here. For example for necessity. Let $O=\{f(x) : f(x) > a, x \in \mathbb{R}\}$ with $a$ real. By continuity, if $O$ is open then the pre-image of $O$ is open whence $f$ is lower semi-continuous. Is $O$ open? If so why? This point blocks me. I think I can do sufficiency correctly though. Suppose $f$ is both upper and lower semi-continuous. Let $O$ be an open set.  Since any open set in $\mathbb{R}$ can be expressed as a union of open sets of the form $(a,b)$, then $O=\cup (a_n,b_n)$ for some open sets $(a_n,b_n)$. Since each $(a_n,b_n)$ can be written has $(a_n,\infty) \cap (-\infty,b_n)$, by semi-continuity, the pre-image of each $(a_n,b_n)$ is open. Since $f^{-1}(O)=f^{-1}(\cup(a_n,b_n))=\cup f^{-1}(a_n,b_n)$, then the pre-image of $O$ is open anf $f$ is continuous. Definition: A function is continuous if and only if the pre-image of any open set is open. A function is lower semi-continuous if and only if the set $\{x \in \mathbb{R} : f(x) > a\}$ is open for any $a$ real. A function is upper semi-continuous if and only if the set $\{x \in \mathbb{R} : f(x) < a\}$ is open for any $a$ real.",,"['real-analysis', 'analysis']"
98,Monotone Convergence Theorem,Monotone Convergence Theorem,,"One simple case of the monotone convergence theorem for integration is: Let $E \subset \mathbb{R}^n$ and suppose that $f_k : E \rightarrow \mathbb{R}$ is a sequence of non-negative measurable functions which increases monotonically to a limit $f$. Then $f : E \rightarrow \mathbb{R}$ is measurable and \begin{equation*} \lim_k \int_E f_k = \int_E f \end{equation*} where here we mean the Lesbegue integral on $\mathbb{R}^n$. I know one proof of this theorem which is very measure-theoretic, and I was wondering is there a non-measure theoretic proof for the theorem if we only assume that the $f_k$ are Riemann integrable?","One simple case of the monotone convergence theorem for integration is: Let $E \subset \mathbb{R}^n$ and suppose that $f_k : E \rightarrow \mathbb{R}$ is a sequence of non-negative measurable functions which increases monotonically to a limit $f$. Then $f : E \rightarrow \mathbb{R}$ is measurable and \begin{equation*} \lim_k \int_E f_k = \int_E f \end{equation*} where here we mean the Lesbegue integral on $\mathbb{R}^n$. I know one proof of this theorem which is very measure-theoretic, and I was wondering is there a non-measure theoretic proof for the theorem if we only assume that the $f_k$ are Riemann integrable?",,"['real-analysis', 'measure-theory']"
99,Two $L^p$ space problems,Two  space problems,L^p,"I have two problems here. 1) I need to show $\|xf(x)\|_2 \le \|f(x)\|_3$ given $f \in L^3[0,1]$. My approach: I know $\|f(x)\|_2 \le \|f(x)\|_3$, and hope to show $\|xf(x)\|_2 \le \|f(x)\|_2$. That's true because $x \in [0,1]$, $xf(x) \le f(x)$. Am I right? 2) If $f \in L^{5}(E)$, $E=[0,1]$ and $\int_{E} f(x)dx=0$, then $\int_{E} |1-f|^5 dx \ge 1$. I think the key is to define $g=1-f$ and use $\|g\|_p \ge \|g\|_2$ for $p \ge 2$. Please give me any confirmation or tell me wrong.","I have two problems here. 1) I need to show $\|xf(x)\|_2 \le \|f(x)\|_3$ given $f \in L^3[0,1]$. My approach: I know $\|f(x)\|_2 \le \|f(x)\|_3$, and hope to show $\|xf(x)\|_2 \le \|f(x)\|_2$. That's true because $x \in [0,1]$, $xf(x) \le f(x)$. Am I right? 2) If $f \in L^{5}(E)$, $E=[0,1]$ and $\int_{E} f(x)dx=0$, then $\int_{E} |1-f|^5 dx \ge 1$. I think the key is to define $g=1-f$ and use $\|g\|_p \ge \|g\|_2$ for $p \ge 2$. Please give me any confirmation or tell me wrong.",,"['real-analysis', 'measure-theory']"
