,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Combinatorics and Expected Value,Combinatorics and Expected Value,,"There are 15 candidates running for a given Senate seat, comprised of 10 men and 5 women. There are 32 polls, in which any of the candidates are equally likely to be ranked first, independently of the other polls (meaning the position of each candidate on any of the polls is purely random). Find the expected value of the number of times that a woman will rank 1st in any of the polls. I understand I need to find the probability of that any given woman will rank 1st in at least 1 of the polls, which I need direction on how exactly to approach.","There are 15 candidates running for a given Senate seat, comprised of 10 men and 5 women. There are 32 polls, in which any of the candidates are equally likely to be ranked first, independently of the other polls (meaning the position of each candidate on any of the polls is purely random). Find the expected value of the number of times that a woman will rank 1st in any of the polls. I understand I need to find the probability of that any given woman will rank 1st in at least 1 of the polls, which I need direction on how exactly to approach.",,"['probability', 'combinatorics', 'statistics']"
1,When is the Markov Inequality Equivalent?,When is the Markov Inequality Equivalent?,,For the Markov Inequality: $$\Pr(X > a) \leq \frac{E(X)}{a}$$ What nonnegative random variable $X$ and constant $a > 0$ makes the Markov Inequality equal?,For the Markov Inequality: What nonnegative random variable and constant makes the Markov Inequality equal?,\Pr(X > a) \leq \frac{E(X)}{a} X a > 0,"['probability', 'statistics', 'inequality', 'proof-writing']"
2,How do I create a probability distribution for the number of games people play in this scenario?,How do I create a probability distribution for the number of games people play in this scenario?,,"From a population of individuals the following statistics are reported: 20% play league of legends (event L) 45% play dota (event D) 30% play halo (event H) 15% play league of legends and dota 20% play dota and halo 10% play league of legends and halo 5%  play all games mentioned. Consider a randomly selected gamer from this population. Let Y= the number of games they play. Find a probability distribution for Y. I don't see how I'd find any p values here. I'm not told how to find when Y=1, 2 or 3 since they're all combined? I was thinking I could do $P(Y\geq 1)$ , $P(Y\geq 2)$ , $P(Y=3)$ but I'm not sure if that would answer the question.","From a population of individuals the following statistics are reported: 20% play league of legends (event L) 45% play dota (event D) 30% play halo (event H) 15% play league of legends and dota 20% play dota and halo 10% play league of legends and halo 5%  play all games mentioned. Consider a randomly selected gamer from this population. Let Y= the number of games they play. Find a probability distribution for Y. I don't see how I'd find any p values here. I'm not told how to find when Y=1, 2 or 3 since they're all combined? I was thinking I could do , , but I'm not sure if that would answer the question.",P(Y\geq 1) P(Y\geq 2) P(Y=3),"['probability', 'statistics', 'probability-distributions']"
3,Theoretical limit on Random Forest performance,Theoretical limit on Random Forest performance,,"If I run a Random Forest on a set of data and get an accuracy of let's say 85% and I want to produce better results, I could just increase the amount of decision trees I use. Lets say I Increase the amount of trees I use and now I get an accuracy of 94%. If theoretically I used an infinite amount of trees would I get a perfect accuracy of 1 or as we increase the amount of trees is there a limit to how accurate we can become as we converge to a certain accuracy (like 98.76%). Of course there are many different factors to this but I think there should be a limit ""n amount of trees"" to where adding more wouldn't increase our accuracy or increase it by such a small amount that it doesn't matter anymore.","If I run a Random Forest on a set of data and get an accuracy of let's say 85% and I want to produce better results, I could just increase the amount of decision trees I use. Lets say I Increase the amount of trees I use and now I get an accuracy of 94%. If theoretically I used an infinite amount of trees would I get a perfect accuracy of 1 or as we increase the amount of trees is there a limit to how accurate we can become as we converge to a certain accuracy (like 98.76%). Of course there are many different factors to this but I think there should be a limit ""n amount of trees"" to where adding more wouldn't increase our accuracy or increase it by such a small amount that it doesn't matter anymore.",,['statistics']
4,A level conditional probability,A level conditional probability,,"In an A Level maths textbook, the answer to an exercise question involves the use of $$P(B|A')=1-P(B|A)$$ where A and B are two events. However, this equality has never been mentioned elsewhere in the curriculum and does not seem to yield the correct answers for conditional probability questions. So does such an identity actually hold true or could it be just a typo? If it is true then how can one make sense out of it?","In an A Level maths textbook, the answer to an exercise question involves the use of where A and B are two events. However, this equality has never been mentioned elsewhere in the curriculum and does not seem to yield the correct answers for conditional probability questions. So does such an identity actually hold true or could it be just a typo? If it is true then how can one make sense out of it?",P(B|A')=1-P(B|A),"['probability', 'statistics', 'intuition', 'conditional-probability']"
5,Showing that an estimator for covariance is consistent?,Showing that an estimator for covariance is consistent?,,"I'm having trouble proving that a certain estimator is consistent. I know that to show an estimator is consistent, I have to show that the variance of the estimator approaches zero as n grows large/goes to infinity. The estimator for $Cov(X_i,Y_i)$ from a random sample $(X_i,Y_i)$ for $i = (1, ... , n)$ is as follows: $$\frac{1}{n}\sum_{i=1}^{n} (X_{i}-\overline{X})(Y_i-\overline{Y})$$ I let $\hat{\theta}$ be the estimator and thought that doing this would work: $$V(\hat{\theta}) = V(\frac{1}{n}\sum_{i=1}^{n} (X_{i}Y_{i}-X_{i}\overline{Y}-Y_{i}\overline{X}+\overline{X} \overline{Y}))$$ I expanded the terms but I have no idea how to go from here. Any help is greatly appreciated!","I'm having trouble proving that a certain estimator is consistent. I know that to show an estimator is consistent, I have to show that the variance of the estimator approaches zero as n grows large/goes to infinity. The estimator for from a random sample for is as follows: I let be the estimator and thought that doing this would work: I expanded the terms but I have no idea how to go from here. Any help is greatly appreciated!","Cov(X_i,Y_i) (X_i,Y_i) i = (1, ... , n) \frac{1}{n}\sum_{i=1}^{n} (X_{i}-\overline{X})(Y_i-\overline{Y}) \hat{\theta} V(\hat{\theta}) = V(\frac{1}{n}\sum_{i=1}^{n} (X_{i}Y_{i}-X_{i}\overline{Y}-Y_{i}\overline{X}+\overline{X} \overline{Y}))","['statistics', 'covariance', 'variance']"
6,Is there a simple explanation to why the line of best fit passes through $\bar x$ and $\bar y$?,Is there a simple explanation to why the line of best fit passes through  and ?,\bar x \bar y,"Is there a clear explanation someone can give an undergrad as to why a line of best fit in a linear model must always pass through a point/coordinate indicating the mean of the $x$ and $y$ values ( $\bar{x}$ , $\bar{y}$ ) and why the sums of the least squares must equal $0$ ? It is a hard concept I have not been able to grasp… Thank you.","Is there a clear explanation someone can give an undergrad as to why a line of best fit in a linear model must always pass through a point/coordinate indicating the mean of the and values ( , ) and why the sums of the least squares must equal ? It is a hard concept I have not been able to grasp… Thank you.",x y \bar{x} \bar{y} 0,"['statistics', 'linear-regression']"
7,Domain of joint density function,Domain of joint density function,,"In this question we have that $G \sim \operatorname{Exp}(0.3)$ and $M \sim \operatorname{Exp}(0.6)$ where $G$ and $M$ are independent of each other. I need to find the joint density function and then find $P(G>2M)$. So far I have that the joint density function is \begin{align} & f_{GM}(g,m)=f_G(g)f_M(m) \\[10pt] = {} & (0.3e^{-0.3g})(0.6e^{-0.6m}) \\[10pt] = {} & 0.18e^{-0.3g-0.6m} \text{(and $0$ otherwise??)} \end{align} If this is correct then what is the domain of the density function? If it is $h,m>0$ then I'm not sure what the limits for each of my integrals are. I believe to find $P(G>2M)$ I need to solve a double integral like this: $$\iint 0.18e^{-0.3g-0.6m} \, dg \, dm$$ ...but I don't know how to find the limits of integration. Thanks for any input.","In this question we have that $G \sim \operatorname{Exp}(0.3)$ and $M \sim \operatorname{Exp}(0.6)$ where $G$ and $M$ are independent of each other. I need to find the joint density function and then find $P(G>2M)$. So far I have that the joint density function is \begin{align} & f_{GM}(g,m)=f_G(g)f_M(m) \\[10pt] = {} & (0.3e^{-0.3g})(0.6e^{-0.6m}) \\[10pt] = {} & 0.18e^{-0.3g-0.6m} \text{(and $0$ otherwise??)} \end{align} If this is correct then what is the domain of the density function? If it is $h,m>0$ then I'm not sure what the limits for each of my integrals are. I believe to find $P(G>2M)$ I need to solve a double integral like this: $$\iint 0.18e^{-0.3g-0.6m} \, dg \, dm$$ ...but I don't know how to find the limits of integration. Thanks for any input.",,"['probability', 'statistics', 'probability-distributions', 'density-function']"
8,"Simple probability question - how to approach systematically, not overthink, or misinterpret?","Simple probability question - how to approach systematically, not overthink, or misinterpret?",,"I have been studying some probability 'story problems' and surprisingly the ones with very simple solutions tend to confuse me. Particularly, the question below. The question: Losses covered by an insurance policy are modeled by a uniform distribution on the interval [0,1000]. An   insurance company reimburses losses in excess of a deductible of 250. Calculate the difference between the median and the 20th percentile of   the insurance company reimbursement, over all losses. (A) 225 (B) 250 (C) 300 (D) 375 (E) 500 If you are young like me and don't know what a deductible is, it means for a loss $L > 0$ the insurance company will pay you a reimbursement $R = \mathrm{max}\{ L -250, 0 \}$ . The solution provided: Before applying the deductible, the median is 500 and the 20th percentile is 200. After applying   the deductible, the median payment is 500 – 250 = 250 and the 20th percentile is max(0, 200 –   250) = 0. The difference is 250. I have a strong background in math but not a good foundation in probability. The solution seems so simple, but without seeing the solution I would not have been confident that method is correct and justified. Part of this may be the wording of the questions - does the part 'over all losses' make a difference? I could not figure out what that meant. Here is how I interpreted the question as I read it for the first time: Losses $L$ are a random variable with distribution $L$ ~ $U[0,1000]$ . (This makes the first part of the solution make sense since median and percentile of uniform distribution are trivial.) Reimbursements $R$ are a new random variable and to find the median and 20th percentile I should find how $R$ is distributed. For some particular loss $\ell > 0$ , the reimbursement is given by $r = \mathrm{max}\{\ell - 250, 0 \}$ . Then a PDF for $R$ should be $$p(r) = \cases{ .25, \hspace{5mm} \text{ if } r = 0 \\ 750^{-1}, \hspace{2mm} \text{ if } r \in (0,750]} \, $$ Using this, I would have found a median to be 281.25 by solving for a number $k$ such that $p(0 < r < k) = p(k < r < 750)$ which is inconsistent with the solution provided. My question for you: It seems I am overthinking or misunderstanding. The solution provided seems much simpler. Is there a concept I am missing that I can learn about that would allow me to confidently and quickly answer this question?  Am I misinterpreting the question? Am I misunderstanding the definition of something? It is very important to me to feel like I understand the definitions and justifications. I do not understand why the solution provided is justified, nor would I have solved it that way on my own - so I am unsatisfied with my understanding. Any guidance appreciated.","I have been studying some probability 'story problems' and surprisingly the ones with very simple solutions tend to confuse me. Particularly, the question below. The question: Losses covered by an insurance policy are modeled by a uniform distribution on the interval [0,1000]. An   insurance company reimburses losses in excess of a deductible of 250. Calculate the difference between the median and the 20th percentile of   the insurance company reimbursement, over all losses. (A) 225 (B) 250 (C) 300 (D) 375 (E) 500 If you are young like me and don't know what a deductible is, it means for a loss the insurance company will pay you a reimbursement . The solution provided: Before applying the deductible, the median is 500 and the 20th percentile is 200. After applying   the deductible, the median payment is 500 – 250 = 250 and the 20th percentile is max(0, 200 –   250) = 0. The difference is 250. I have a strong background in math but not a good foundation in probability. The solution seems so simple, but without seeing the solution I would not have been confident that method is correct and justified. Part of this may be the wording of the questions - does the part 'over all losses' make a difference? I could not figure out what that meant. Here is how I interpreted the question as I read it for the first time: Losses are a random variable with distribution ~ . (This makes the first part of the solution make sense since median and percentile of uniform distribution are trivial.) Reimbursements are a new random variable and to find the median and 20th percentile I should find how is distributed. For some particular loss , the reimbursement is given by . Then a PDF for should be Using this, I would have found a median to be 281.25 by solving for a number such that which is inconsistent with the solution provided. My question for you: It seems I am overthinking or misunderstanding. The solution provided seems much simpler. Is there a concept I am missing that I can learn about that would allow me to confidently and quickly answer this question?  Am I misinterpreting the question? Am I misunderstanding the definition of something? It is very important to me to feel like I understand the definitions and justifications. I do not understand why the solution provided is justified, nor would I have solved it that way on my own - so I am unsatisfied with my understanding. Any guidance appreciated.","L > 0 R = \mathrm{max}\{ L -250, 0 \} L L U[0,1000] R R \ell > 0 r = \mathrm{max}\{\ell - 250, 0 \} R p(r) = \cases{ .25, \hspace{5mm} \text{ if } r = 0 \\ 750^{-1}, \hspace{2mm} \text{ if } r \in (0,750]} \,  k p(0 < r < k) = p(k < r < 750)","['probability', 'statistics', 'probability-distributions', 'mathematical-modeling', 'descriptive-statistics']"
9,Cumulative distribution function for geometric random variable,Cumulative distribution function for geometric random variable,,"For geometric random variable $f(k)=(1-p)^{k-1}p$ . \begin{align} F(k) = P(X\leq x) &= 1-P(x>k) \\ &= 1 - \sum_{i=k+1}p(1-p)^{i-1} \\ &= 1 - (1-p)^k \sum_{i=1}^ \infty p(1-p)^{i-1} \\ &= 1 - (1-p)^k \end{align} I would appreciate a breakdown of these steps. Especially why do we take $1-P(x>k)$ and what operations are preformed on the summation sign?  Stating the obvious is very welcone, my mathematical background is quite limited.","For geometric random variable . I would appreciate a breakdown of these steps. Especially why do we take and what operations are preformed on the summation sign?  Stating the obvious is very welcone, my mathematical background is quite limited.","f(k)=(1-p)^{k-1}p \begin{align}
F(k) = P(X\leq x) &= 1-P(x>k) \\
&= 1 - \sum_{i=k+1}p(1-p)^{i-1} \\
&= 1 - (1-p)^k \sum_{i=1}^ \infty p(1-p)^{i-1} \\
&= 1 - (1-p)^k
\end{align} 1-P(x>k)","['statistics', 'probability-distributions']"
10,I roll three dice and keep only the largest two,I roll three dice and keep only the largest two,,"I know that rolling two 6-sided dice I will get 7 as medium result. But what if I roll three dice and get only the largest two? How much is that going to increase the medium result? And what about with 4 dice, 5 etc...? Thank you!","I know that rolling two 6-sided dice I will get 7 as medium result. But what if I roll three dice and get only the largest two? How much is that going to increase the medium result? And what about with 4 dice, 5 etc...? Thank you!",,"['probability', 'statistics', 'probability-distributions', 'dice']"
11,"If the probability density function is equal to $mx+b$ for $0<x<5$, and $0$ other wise, what is the probability of $x=1$?","If the probability density function is equal to  for , and  other wise, what is the probability of ?",mx+b 0<x<5 0 x=1,"The pdf is $$ f(x) = \left\{         \begin{array}{ll}             mx+b & \quad 0<x<5 \\             0 & \quad \text {otherwise}         \end{array}     \right. $$ The question is: What is P(X=1)? In my textbook, it says $P(X=1)=F(1)-F(1^-)$. F is the cumulative density function CDF. My answer:$$P(X=1)=\int^1_0 (mx+b) dx-\int^{1^-}_0 (mx+b) dx$$ Is this zero?","The pdf is $$ f(x) = \left\{         \begin{array}{ll}             mx+b & \quad 0<x<5 \\             0 & \quad \text {otherwise}         \end{array}     \right. $$ The question is: What is P(X=1)? In my textbook, it says $P(X=1)=F(1)-F(1^-)$. F is the cumulative density function CDF. My answer:$$P(X=1)=\int^1_0 (mx+b) dx-\int^{1^-}_0 (mx+b) dx$$ Is this zero?",,"['probability', 'combinatorics', 'statistics']"
12,Converting bounds of double integral to polar coordinates,Converting bounds of double integral to polar coordinates,,"I'm trying to convert the following to polar coordinates: $$\int_0^\infty \int_{-\infty}^{-x}\frac{1}{2\pi}e^{-(x^2+y^2)/2}\,dx\,dy$$ After converting to polar coordinates, it should be: $$\int_0^\infty \int_{(3/2)\pi}^{(7/4)\pi}\frac{1}{2\pi}e^{-r^2/2}\,dr\,d\theta$$ My question is how do we arrive at the bounds of $(3/2)\pi$ and $(7/4)\pi$.","I'm trying to convert the following to polar coordinates: $$\int_0^\infty \int_{-\infty}^{-x}\frac{1}{2\pi}e^{-(x^2+y^2)/2}\,dx\,dy$$ After converting to polar coordinates, it should be: $$\int_0^\infty \int_{(3/2)\pi}^{(7/4)\pi}\frac{1}{2\pi}e^{-r^2/2}\,dr\,d\theta$$ My question is how do we arrive at the bounds of $(3/2)\pi$ and $(7/4)\pi$.",,"['calculus', 'statistics', 'normal-distribution', 'polar-coordinates']"
13,Moore-Penrose equals original matrix,Moore-Penrose equals original matrix,,"I'm a bit stuck with my homework in a subject called ""Matrices in Statistics"". The task is as follows: Prove, that if $A$ is symmetric ($A=A^{T}$) and idempotent ($A=A^2$). Then $$ A^{+} = A $$ Where $A^{+}$ is called the Moore-Penrose generalized inverse matrix. Can you give me any ideas/tips, how to get started with this one? I would be very thankful.","I'm a bit stuck with my homework in a subject called ""Matrices in Statistics"". The task is as follows: Prove, that if $A$ is symmetric ($A=A^{T}$) and idempotent ($A=A^2$). Then $$ A^{+} = A $$ Where $A^{+}$ is called the Moore-Penrose generalized inverse matrix. Can you give me any ideas/tips, how to get started with this one? I would be very thankful.",,"['linear-algebra', 'matrices', 'statistics', 'symmetric-matrices']"
14,Plants and probabilities,Plants and probabilities,,"I am trying to work through some probability and statistics questions. We recently covered joint probability distributions but it is still quite fuzzy to me. I am not sure if this question (at least all of it) uses JPD's but here goes. There are three varieties of a particular type of plant: variety A, variety B and a hybrid variety. 50% of the plants are of variety A, 20% of variety B and the remainder are hybrids. (a) If a random sample of 10 plants is collected, nd the probability that the sample contains: (i) Exactly three plants of variety A; (ii) Exactly two plants of variety B; (iii) Exactly three plants of variety A and two plants of variety B. In the sample, are the numbers of plants of each variety independent of each other? Justify your answer clearly. (b) Suppose now that the sample of 10 plants contains exactly four plants of variety A. In this case, what is the distribution of the number of variety B plants in the sample? For part a), I think that for i and ii, it is enough to use Binomial but  I feel like there is a catch and that I will have to change the parameters for the 2 or 3 picks (namely, taking away one of the plants from the sample size and changing the probability accordingly). In iii I have to use Bayes from what I understand For part b), I am not quite sure where to begin.","I am trying to work through some probability and statistics questions. We recently covered joint probability distributions but it is still quite fuzzy to me. I am not sure if this question (at least all of it) uses JPD's but here goes. There are three varieties of a particular type of plant: variety A, variety B and a hybrid variety. 50% of the plants are of variety A, 20% of variety B and the remainder are hybrids. (a) If a random sample of 10 plants is collected, nd the probability that the sample contains: (i) Exactly three plants of variety A; (ii) Exactly two plants of variety B; (iii) Exactly three plants of variety A and two plants of variety B. In the sample, are the numbers of plants of each variety independent of each other? Justify your answer clearly. (b) Suppose now that the sample of 10 plants contains exactly four plants of variety A. In this case, what is the distribution of the number of variety B plants in the sample? For part a), I think that for i and ii, it is enough to use Binomial but  I feel like there is a catch and that I will have to change the parameters for the 2 or 3 picks (namely, taking away one of the plants from the sample size and changing the probability accordingly). In iii I have to use Bayes from what I understand For part b), I am not quite sure where to begin.",,"['probability', 'statistics']"
15,Method of Maximum Likelihood for Normal Distribution CDF,Method of Maximum Likelihood for Normal Distribution CDF,,"Based on a random sample of size n from a normal distribution, $X$ ~ $N(\mu, \sigma^2)$ find the MLE (maximum likelihood estimator) of the following: $P[X>c]$ for arbitrary $c$ . This seems to be strange question, and the provided solution is even more troubling: Would that not be the method of moments estimate?  Surely that solution isn't correct.  But if it's not... then how exactly do I calculate the MLE?  I'm just downright confused.","Based on a random sample of size n from a normal distribution, ~ find the MLE (maximum likelihood estimator) of the following: for arbitrary . This seems to be strange question, and the provided solution is even more troubling: Would that not be the method of moments estimate?  Surely that solution isn't correct.  But if it's not... then how exactly do I calculate the MLE?  I'm just downright confused.","X N(\mu, \sigma^2) P[X>c] c","['statistics', 'normal-distribution', 'maximum-likelihood', 'parameter-estimation']"
16,Piecewise integration: find the 75th percentile of this continuous random variable.,Piecewise integration: find the 75th percentile of this continuous random variable.,,"The problem from a quiz of mine is as follows: In general, I know how to find the $p$th percentile of a random variable $X$, given its probability density function $f(x)$. You simply solve for the value $x_p$ that satisfies the following equation: $\int^{x_p}_{-\infty}{f(x)dx} = \frac{p}{100}$ When given just a single function that isn't broken up piecewise like this, I know how to proceed. But here's my issue with this problem: How do I split up the integral into multiple pieces, considering the upper limit of integration is a variable?","The problem from a quiz of mine is as follows: In general, I know how to find the $p$th percentile of a random variable $X$, given its probability density function $f(x)$. You simply solve for the value $x_p$ that satisfies the following equation: $\int^{x_p}_{-\infty}{f(x)dx} = \frac{p}{100}$ When given just a single function that isn't broken up piecewise like this, I know how to proceed. But here's my issue with this problem: How do I split up the integral into multiple pieces, considering the upper limit of integration is a variable?",,"['integration', 'statistics', 'percentile']"
17,Probability of sum of product of uniform R.V.,Probability of sum of product of uniform R.V.,,"Suppose uniform[0,1] random variables $X_i$, $i=1,...,\infty$ are also independent. Whats the following probability? \begin{split} P\Big(\sum_{n=1}^\infty\prod_{i=1}^nX_i<\infty\Big) \end{split}","Suppose uniform[0,1] random variables $X_i$, $i=1,...,\infty$ are also independent. Whats the following probability? \begin{split} P\Big(\sum_{n=1}^\infty\prod_{i=1}^nX_i<\infty\Big) \end{split}",,"['probability', 'statistics', 'uniform-distribution']"
18,Joint probability density function and limits of integration,Joint probability density function and limits of integration,,"Suppose we have $f(x,y) = 2$ when $x>0$, $y>0$, $x+y <1$ and $0$ elsewhere.  I want to find the $\Pr(Y>X)$ and I'm struggling with finding the correct limits of integration generally but especially when we want to find $\Pr(Y>X)$ or similar. Can anyone explain the general method please, would be very grateful.","Suppose we have $f(x,y) = 2$ when $x>0$, $y>0$, $x+y <1$ and $0$ elsewhere.  I want to find the $\Pr(Y>X)$ and I'm struggling with finding the correct limits of integration generally but especially when we want to find $\Pr(Y>X)$ or similar. Can anyone explain the general method please, would be very grateful.",,['statistics']
19,Find pdf given transformation,Find pdf given transformation,,"I'm given that the random variable $X$ is distributed as $UNIF(0,1)$. Moreover, I'm given that: $U = X(1-X)$ Using the CDF method (i.e. finding the CDF, then taking the derivative), how would I find the PDF of $U$? I can't isolate X in the equation, so how would I go about getting the PDF of $U$ as a result? Thank you","I'm given that the random variable $X$ is distributed as $UNIF(0,1)$. Moreover, I'm given that: $U = X(1-X)$ Using the CDF method (i.e. finding the CDF, then taking the derivative), how would I find the PDF of $U$? I can't isolate X in the equation, so how would I go about getting the PDF of $U$ as a result? Thank you",,"['calculus', 'statistics', 'derivatives', 'transformation']"
20,Probability that the sum of N dice is at least X [closed],Probability that the sum of N dice is at least X [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question If N fair 6 sided dice are thrown, what is the probability that the sum of the thrown dice is at least X?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question If N fair 6 sided dice are thrown, what is the probability that the sum of the thrown dice is at least X?",,"['probability', 'statistics', 'dice']"
21,Question on foiling with vectors?,Question on foiling with vectors?,,"Show that $$\frac1{f}\sum_{i=1}^n(\mathbf{x}_i-\bar{\mathbf{x}})(\mathbf{x}_i-\bar{\mathbf{x}})'=\frac1{f}\sum_{i=1}^n(\mathbf{x}_i\mathbf{x}_i'-n\bar{\mathbf{x}}\bar {\mathbf{x}}').$$ I need this to understand an example in my textbook on Hotelling's $T^2$ but I can't figure out how they get this result. I imagine it's quite trivial but I can't seem to figure it out.. Edit, my textbook is using this result to simplify computation. Otherwise you need to sum a lot of vectors :/","Show that $$\frac1{f}\sum_{i=1}^n(\mathbf{x}_i-\bar{\mathbf{x}})(\mathbf{x}_i-\bar{\mathbf{x}})'=\frac1{f}\sum_{i=1}^n(\mathbf{x}_i\mathbf{x}_i'-n\bar{\mathbf{x}}\bar {\mathbf{x}}').$$ I need this to understand an example in my textbook on Hotelling's $T^2$ but I can't figure out how they get this result. I imagine it's quite trivial but I can't seem to figure it out.. Edit, my textbook is using this result to simplify computation. Otherwise you need to sum a lot of vectors :/",,"['linear-algebra', 'statistics']"
22,Mean and variance of a lognormal random variable? [duplicate],Mean and variance of a lognormal random variable? [duplicate],,"This question already has answers here : How to compute moments of log normal distribution? (2 answers) Closed 3 years ago . A random variable $X$ is lognormal if ln $X$, the natural logarithm of $X$, is normally distributed. Find the mean and variance of a lognormal random variable with $ln X ∼ N(µ, σ^2)$ How should I solve this using the moment generating function of normal distribution, without differentiation?","This question already has answers here : How to compute moments of log normal distribution? (2 answers) Closed 3 years ago . A random variable $X$ is lognormal if ln $X$, the natural logarithm of $X$, is normally distributed. Find the mean and variance of a lognormal random variable with $ln X ∼ N(µ, σ^2)$ How should I solve this using the moment generating function of normal distribution, without differentiation?",,"['probability', 'statistics', 'random-variables']"
23,Logistic Regression: When can the cost function be non-convex?,Logistic Regression: When can the cost function be non-convex?,,"I'm studying the lecture on the Machine Learning at Coursera. When introducing the cost function for Logistic Regression, they said that we shouldn't use the same cost function as the one for Linear Regression since that cost function can have multiple local minima that would cause gradient descent to fail. However, I'm having a hard time finding an example of an equation which would result in multiple local minima if one uses the Linear Regression cost function on Logistic Regression. Could someone please provide such an example and give me some intuition about why it occurs? Side note: I can see other good properties of using the log(h(x)) cost function for logistic regression, that it strongly penalizes being certain when you're actually wrong. But I'm trying to understand the claim that the linear regression's cost function will be non-convex for logistic regression.","I'm studying the lecture on the Machine Learning at Coursera. When introducing the cost function for Logistic Regression, they said that we shouldn't use the same cost function as the one for Linear Regression since that cost function can have multiple local minima that would cause gradient descent to fail. However, I'm having a hard time finding an example of an equation which would result in multiple local minima if one uses the Linear Regression cost function on Logistic Regression. Could someone please provide such an example and give me some intuition about why it occurs? Side note: I can see other good properties of using the log(h(x)) cost function for logistic regression, that it strongly penalizes being certain when you're actually wrong. But I'm trying to understand the claim that the linear regression's cost function will be non-convex for logistic regression.",,"['statistics', 'regression', 'machine-learning', 'linear-regression', 'logistic-regression']"
24,Large sample confidence interval,Large sample confidence interval,,Can someone please be so kind to check if my answer is correct,Can someone please be so kind to check if my answer is correct,,"['statistics', 'normal-distribution', 'confidence-interval']"
25,P(At Least 1 Boy Born on Tuesday out of 2 Children),P(At Least 1 Boy Born on Tuesday out of 2 Children),,"A family has two children. Given that at least one of the children is a boy who was born on a Tuesday, what is the probability that both children are boys? Assume that the probability of a child being born on a particular day of the week is 1/7. I'm wondering if there's a better way to calculate P(At least 1 boy born on a Tuesday) than the explanation. [In the solution, this is P(B) in Bayes' Theorem. Here is how they calculate: To calculate  we note that there are  14^2 = 196 possible ways to select the gender and the day of the week the child was born on. Of these, there are 13^2 = 169 ways which do not have a boy born on Tuesday, and 196 - 169 = 27 which do, so P(B) = 27/196. I understand this intuitively, but my statistics classes shy away from this ""naive"" definition of probability (although we do assume equally likely boy vs girl and day of the week for this problem). So is there a way I can calculate this P(B) in a more stepwise fashion (for example: {1/7 chance of born on Tuesday • 1/2 chance boy} + {1/2 chance boy • 1/2 chance other was not a boy • 1/7 chance of Tuesday • 6/7 chance the other was not}. I know the example I just wrote isn't correct but can someone intuit for me a way I could go about getting P(B) in this problem in a similar method? Perhaps I just need it illustrated. Note: I found another thread with this question but wasn't sure I understood the explanation. Perhaps someone has an enlightening comment to help.","A family has two children. Given that at least one of the children is a boy who was born on a Tuesday, what is the probability that both children are boys? Assume that the probability of a child being born on a particular day of the week is 1/7. I'm wondering if there's a better way to calculate P(At least 1 boy born on a Tuesday) than the explanation. [In the solution, this is P(B) in Bayes' Theorem. Here is how they calculate: To calculate  we note that there are  14^2 = 196 possible ways to select the gender and the day of the week the child was born on. Of these, there are 13^2 = 169 ways which do not have a boy born on Tuesday, and 196 - 169 = 27 which do, so P(B) = 27/196. I understand this intuitively, but my statistics classes shy away from this ""naive"" definition of probability (although we do assume equally likely boy vs girl and day of the week for this problem). So is there a way I can calculate this P(B) in a more stepwise fashion (for example: {1/7 chance of born on Tuesday • 1/2 chance boy} + {1/2 chance boy • 1/2 chance other was not a boy • 1/7 chance of Tuesday • 6/7 chance the other was not}. I know the example I just wrote isn't correct but can someone intuit for me a way I could go about getting P(B) in this problem in a similar method? Perhaps I just need it illustrated. Note: I found another thread with this question but wasn't sure I understood the explanation. Perhaps someone has an enlightening comment to help.",,"['calculus', 'probability', 'statistics']"
26,Is this ratio of normal PDFs and CDFs decreasing?,Is this ratio of normal PDFs and CDFs decreasing?,,"I am trying to show that the following function is decreasing in x: $$ \frac{\phi(x+a)-\phi(a)}{\Phi(x+a)-\Phi(a)}, $$ where $\Phi(x)$ and $\phi(x)$ are CDF and PDF of the standard normal distribution and $a \in \mathbb{R}$. Taking derivatives leads to an expression that I have problem signing: $$ \frac{-(x+a)\phi(x+a)[\Phi(x+a)-\Phi(a)] - \phi(x+a) [\phi(x+a)-\phi(a)] }{[\Phi(x+a)-\Phi(a)]^{2}}$$ This suggests that it is enough to show that $$-(x+a)[\Phi(x+a)-\Phi(a)] -  [\phi(x+a)-\phi(a)]<0$$ but was not able to establish this inequality. I have tried using simple results about Mill's ratio to sign the derivative (or to prove the above inequality) but was not able to. However, numerically, it seems that this function is decreasing. Any help would be much appreciated!","I am trying to show that the following function is decreasing in x: $$ \frac{\phi(x+a)-\phi(a)}{\Phi(x+a)-\Phi(a)}, $$ where $\Phi(x)$ and $\phi(x)$ are CDF and PDF of the standard normal distribution and $a \in \mathbb{R}$. Taking derivatives leads to an expression that I have problem signing: $$ \frac{-(x+a)\phi(x+a)[\Phi(x+a)-\Phi(a)] - \phi(x+a) [\phi(x+a)-\phi(a)] }{[\Phi(x+a)-\Phi(a)]^{2}}$$ This suggests that it is enough to show that $$-(x+a)[\Phi(x+a)-\Phi(a)] -  [\phi(x+a)-\phi(a)]<0$$ but was not able to establish this inequality. I have tried using simple results about Mill's ratio to sign the derivative (or to prove the above inequality) but was not able to. However, numerically, it seems that this function is decreasing. Any help would be much appreciated!",,"['calculus', 'statistics', 'derivatives', 'normal-distribution']"
27,Random simulations of the distribution of genders in a classroom,Random simulations of the distribution of genders in a classroom,,"For my AP statistic class, we had to design an experiment (usually a survey of what brand of soda people prefer), so me being an overachiever decided to study how boys and girls distribute themselves among other genders. For instance, girls will sit near each other normally. My way of doing this would be to find the average percentage of people guys sit around that are the same gender for both genders independently. I'd expect to see these numbers be $50$% if they sat randomly, but I'm willing to bet they're around $75$%-$85$%. How can I produce random simulations of this test? It's a requirement for the assignment.","For my AP statistic class, we had to design an experiment (usually a survey of what brand of soda people prefer), so me being an overachiever decided to study how boys and girls distribute themselves among other genders. For instance, girls will sit near each other normally. My way of doing this would be to find the average percentage of people guys sit around that are the same gender for both genders independently. I'd expect to see these numbers be $50$% if they sat randomly, but I'm willing to bet they're around $75$%-$85$%. How can I produce random simulations of this test? It's a requirement for the assignment.",,['statistics']
28,what does it mean to get the cdf of a constant variable,what does it mean to get the cdf of a constant variable,,"I saw this theorem If $X_n \ \xrightarrow{d}\ c$, where $c$ is a constant, then $X_n \ \xrightarrow{p}\ c$ In order to get $X_n \ \xrightarrow{d}\ c$, I need to prove $\begin{align}%\label{eq:union-bound}    \lim_{n \rightarrow \infty} F_{X_n}(x)=F_X(x), \end{align}$ which means I'm trying to get $F_c(x)$, my question is what does it mean to get the cdf of a constant variable and what would the density function for $c$ be? Is it just 0? If it is what is the intuition behind this?","I saw this theorem If $X_n \ \xrightarrow{d}\ c$, where $c$ is a constant, then $X_n \ \xrightarrow{p}\ c$ In order to get $X_n \ \xrightarrow{d}\ c$, I need to prove $\begin{align}%\label{eq:union-bound}    \lim_{n \rightarrow \infty} F_{X_n}(x)=F_X(x), \end{align}$ which means I'm trying to get $F_c(x)$, my question is what does it mean to get the cdf of a constant variable and what would the density function for $c$ be? Is it just 0? If it is what is the intuition behind this?",,"['probability', 'statistics', 'convergence-divergence']"
29,Maximum Likelihood Parameter Estimation: Assuming Mean of Observations,Maximum Likelihood Parameter Estimation: Assuming Mean of Observations,,"I'm currently in a probability class learning about parameter estimation using the maximum likelihood estimator. The problem is as follows: we have a list of independent observations Y y[1]...y[n], that came from some probability distribution $f_Y(y,\lambda) $ with an unknown parameter $\lambda $. (For example, exponential, Gaussian, Poisson, etc.) We want to estimate the parameter $\lambda$ by maximizing the likelihood that we see the observations we do. Since all observations are independent, we have probability $P(Y,\lambda)= \prod_{i=1}^{n} f_Y(y_i,\lambda)$. To maximize this, we take the derivative with resepect to $\lambda$ and set to 0. $$ \hat\lambda= \arg \max_{\lambda} \left[ P(Y,\lambda) \right]$$ Something I noticed: for every example of this I've seen so far (only about 2 or 3 now), the end result is always the same: the value of the parameter is whatever makes the mean of your observation vector equal $E[f_Y(y,\lambda)] $. For example, for an exponential distribution, we get $$\hat\lambda=\frac{1}{\frac{1}{n}\sum_{i}y_i} = \frac{1}{\mu_Y} $$ This makes intuitive sense, because for an exponential distribution, the expected value is $1/\lambda $. My question is this: can you always assume that the mean of your observations is the mean of your probability distribution and just solve for the unkown parameters using that assumption? Just because it works for the few cases I've seen, I don't know whether this can be generalized to any probability distribution. I'm completely new to these topics, so any additional info would be appreciated. Thanks in advance!","I'm currently in a probability class learning about parameter estimation using the maximum likelihood estimator. The problem is as follows: we have a list of independent observations Y y[1]...y[n], that came from some probability distribution $f_Y(y,\lambda) $ with an unknown parameter $\lambda $. (For example, exponential, Gaussian, Poisson, etc.) We want to estimate the parameter $\lambda$ by maximizing the likelihood that we see the observations we do. Since all observations are independent, we have probability $P(Y,\lambda)= \prod_{i=1}^{n} f_Y(y_i,\lambda)$. To maximize this, we take the derivative with resepect to $\lambda$ and set to 0. $$ \hat\lambda= \arg \max_{\lambda} \left[ P(Y,\lambda) \right]$$ Something I noticed: for every example of this I've seen so far (only about 2 or 3 now), the end result is always the same: the value of the parameter is whatever makes the mean of your observation vector equal $E[f_Y(y,\lambda)] $. For example, for an exponential distribution, we get $$\hat\lambda=\frac{1}{\frac{1}{n}\sum_{i}y_i} = \frac{1}{\mu_Y} $$ This makes intuitive sense, because for an exponential distribution, the expected value is $1/\lambda $. My question is this: can you always assume that the mean of your observations is the mean of your probability distribution and just solve for the unkown parameters using that assumption? Just because it works for the few cases I've seen, I don't know whether this can be generalized to any probability distribution. I'm completely new to these topics, so any additional info would be appreciated. Thanks in advance!",,"['probability', 'statistics', 'probability-distributions', 'parameter-estimation']"
30,why t-distribution and normal distribution?,why t-distribution and normal distribution?,,Why do we use the t-distribution when the population standard deviation is not known? And similarly why the normal distribution when standard deviation is known?,Why do we use the t-distribution when the population standard deviation is not known? And similarly why the normal distribution when standard deviation is known?,,"['probability', 'statistics', 'parameter-estimation']"
31,"Normal and Uniform Distribution, calculate $P(Y>X\mid X=x)$","Normal and Uniform Distribution, calculate",P(Y>X\mid X=x),"Let $X$ and $Y$ be independent random variables distributed as $X \sim N(0,1)$ and $Y \sim \operatorname{Unif}(0,1)$. (a) Find $P(Y > X\mid X = x)$. (b) Use your answer in part (a) to compute $P(Y > X)$ I'm not sure where to start... I think the joint density is $$ f(x,y) = \frac{1}{\sqrt{2π}}e^{-x^2/2},  \quad 0<x<1, 0<y<1 $$ And do I just integrate that? If so from where, 0 to 1 on both integrals? I'm pretty lost with this and any help would be so appreciated. Apologies if it's not formatted correctly or if my attempt at an answer is plain stupid. I don't even really understand what part (a) means...","Let $X$ and $Y$ be independent random variables distributed as $X \sim N(0,1)$ and $Y \sim \operatorname{Unif}(0,1)$. (a) Find $P(Y > X\mid X = x)$. (b) Use your answer in part (a) to compute $P(Y > X)$ I'm not sure where to start... I think the joint density is $$ f(x,y) = \frac{1}{\sqrt{2π}}e^{-x^2/2},  \quad 0<x<1, 0<y<1 $$ And do I just integrate that? If so from where, 0 to 1 on both integrals? I'm pretty lost with this and any help would be so appreciated. Apologies if it's not formatted correctly or if my attempt at an answer is plain stupid. I don't even really understand what part (a) means...",,"['probability', 'statistics', 'probability-distributions', 'normal-distribution', 'uniform-distribution']"
32,Is there an equation for the maximum of n random draws from a Gamma distribution,Is there an equation for the maximum of n random draws from a Gamma distribution,,"Suppose a random variable $X$ follows a Gamma distribution with parameters $\alpha$ and $\beta$ with the probability density function for $x>0$ as $$f(x;\alpha,\beta)= \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} \exp(-\beta x)$$ where $\Gamma(\alpha)$ represents the Gamma function. Suppose we take n samples from this distribution and only record the maximum value. For a given n, is there a general way to describe the expectation of this maximum value that we record? Below is a plot of the expectation (circles) and also the standard deviation (lines) for 1000 trials for each n draws, which produces a log-like curve with $\alpha=0.02$ and $\beta=0.0025$.","Suppose a random variable $X$ follows a Gamma distribution with parameters $\alpha$ and $\beta$ with the probability density function for $x>0$ as $$f(x;\alpha,\beta)= \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} \exp(-\beta x)$$ where $\Gamma(\alpha)$ represents the Gamma function. Suppose we take n samples from this distribution and only record the maximum value. For a given n, is there a general way to describe the expectation of this maximum value that we record? Below is a plot of the expectation (circles) and also the standard deviation (lines) for 1000 trials for each n draws, which produces a log-like curve with $\alpha=0.02$ and $\beta=0.0025$.",,"['probability', 'statistics', 'probability-distributions', 'stochastic-calculus']"
33,Random placement of rooks on a chessboard [duplicate],Random placement of rooks on a chessboard [duplicate],,"This question already has answers here : What is the probability that when you place 8 towers on a chess-board, none of them can beat the other. (3 answers) Closed 7 years ago . $8$ rooks are placed randomly on an $8\times 8$ chess board. What is the probability of having exactly one rook each row and each column? I guess there is no meaningful order here?","This question already has answers here : What is the probability that when you place 8 towers on a chess-board, none of them can beat the other. (3 answers) Closed 7 years ago . $8$ rooks are placed randomly on an $8\times 8$ chess board. What is the probability of having exactly one rook each row and each column? I guess there is no meaningful order here?",,['statistics']
34,Is there a probability distribution with a CDF shaped like the sinh or logit function?,Is there a probability distribution with a CDF shaped like the sinh or logit function?,,"I am looking for a probability distribution with the shape of the CDF similar to the sinh or logit function. I need it to be centered at 0 and symmetric, meet the usual CDF restrictions. I understand that most likely it will need to be defined on a specific interval, that's okay. The important property is that F'(x) < 1, F''(x) < 0 close to the left of the mean, and F'(x)<1 , F''(x)>0 close to the right. For more farther away values, however defined, I need F'(x) > 1, F''(x) > 0. Needless to say, the easier the formula, the better! I tried to play around with the sinh function but I couldn't get it to work. Any ideas? Here is an example of the cdf I am looking for:","I am looking for a probability distribution with the shape of the CDF similar to the sinh or logit function. I need it to be centered at 0 and symmetric, meet the usual CDF restrictions. I understand that most likely it will need to be defined on a specific interval, that's okay. The important property is that F'(x) < 1, F''(x) < 0 close to the left of the mean, and F'(x)<1 , F''(x)>0 close to the right. For more farther away values, however defined, I need F'(x) > 1, F''(x) > 0. Needless to say, the easier the formula, the better! I tried to play around with the sinh function but I couldn't get it to work. Any ideas? Here is an example of the cdf I am looking for:",,"['statistics', 'probability-distributions']"
35,Find the limiting distribution of $Z_n=n[1-F(Y_n)]$,Find the limiting distribution of,Z_n=n[1-F(Y_n)],Let $Y_n$ denote the maximum (the last order statistic) of a random sample of size $n$ from a distribution of the continuous type that has c.d.f $F(x)$ and pdf $f(x) = F'(x)$. Find the limiting distribution of $Z_n=n[1-F(Y_n)]$ I dont have the solution to this but the answer my teacher gave us is that the limiting distribution of $Z_n$ should be exponential with mean $1$. If you define $Z_n=n[1-Y_n]$ then that limiting distribution is exponential with mean $1$ but I dont know how to work with $F(Y_n)$,Let $Y_n$ denote the maximum (the last order statistic) of a random sample of size $n$ from a distribution of the continuous type that has c.d.f $F(x)$ and pdf $f(x) = F'(x)$. Find the limiting distribution of $Z_n=n[1-F(Y_n)]$ I dont have the solution to this but the answer my teacher gave us is that the limiting distribution of $Z_n$ should be exponential with mean $1$. If you define $Z_n=n[1-Y_n]$ then that limiting distribution is exponential with mean $1$ but I dont know how to work with $F(Y_n)$,,"['statistics', 'probability-distributions', 'order-statistics']"
36,Expected Value - Finding the Price of a Raffle Ticket,Expected Value - Finding the Price of a Raffle Ticket,,"This question has been bugging me for a while now and I want to know where I'm going wrong. There are $20$ tickets in a raffle with one prize. What should each ticket cost if the prize is \$80 and the expected gain to the organizer is \$30? Now I can get the right answer by adding \$80 and \$30 then dividing by the 20 tickets to get \$5.50 per ticket, but when I use the expected value equation such as $\frac{1}{20}(p-80) + \frac{19}{20}p = 30$ to find the price of a ticket I get a much larger value which is indeed incorrect. What am I doing wrong in my equation?","This question has been bugging me for a while now and I want to know where I'm going wrong. There are $20$ tickets in a raffle with one prize. What should each ticket cost if the prize is \$80 and the expected gain to the organizer is \$30? Now I can get the right answer by adding \$80 and \$30 then dividing by the 20 tickets to get \$5.50 per ticket, but when I use the expected value equation such as $\frac{1}{20}(p-80) + \frac{19}{20}p = 30$ to find the price of a ticket I get a much larger value which is indeed incorrect. What am I doing wrong in my equation?",,"['probability', 'statistics']"
37,Tail of probability distribution,Tail of probability distribution,,"I need to analyze the plot of a probability distribution for a group of random samples. The question asked: ""What does the tail of probability distribution of the sample values look like?"" I don't know how should I answer this question. Are there specific categories and definition for the tail of probability distribution? Can anyone provide an insight (or introduce a source) please? Thanks in advance.","I need to analyze the plot of a probability distribution for a group of random samples. The question asked: ""What does the tail of probability distribution of the sample values look like?"" I don't know how should I answer this question. Are there specific categories and definition for the tail of probability distribution? Can anyone provide an insight (or introduce a source) please? Thanks in advance.",,"['statistics', 'probability-distributions', 'sampling', 'distribution-tails']"
38,Queen’s random walk,Queen’s random walk,,"(Queen’s random walk). A queen can move any number of squares horizontally, vertically, or diagonally. Let Xn be the sequence of squares that results if we pick one of queen’s legal moves at random. $(a)$ Find the stationary distribution $(b)$ Find the expected number of moves to return to corner $(1,1)$ when we start there . So the answer is $\sum_{x∈S} deg(x)$$ = 1452$, and for the corner $deg(x) = 21$. expected number of moves to return to the corner $≈ 69.14.$ But there are no steps to the answer. I really appreciate if you could show me how to get to the answer, thanks!","(Queen’s random walk). A queen can move any number of squares horizontally, vertically, or diagonally. Let Xn be the sequence of squares that results if we pick one of queen’s legal moves at random. $(a)$ Find the stationary distribution $(b)$ Find the expected number of moves to return to corner $(1,1)$ when we start there . So the answer is $\sum_{x∈S} deg(x)$$ = 1452$, and for the corner $deg(x) = 21$. expected number of moves to return to the corner $≈ 69.14.$ But there are no steps to the answer. I really appreciate if you could show me how to get to the answer, thanks!",,"['probability', 'statistics', 'markov-chains', 'markov-process', 'random-walk']"
39,Solving Summation Equations for Method of Moments Proof,Solving Summation Equations for Method of Moments Proof,,"A part of the solution in a proof of a question related to the method of moments states that, $$\frac{1}n\sum_{i=1}^n{(y_i-\bar{y})^2}=\frac{1}n\sum_{i=1}^n{y_i^2-\bar{y}^2}$$ How does this follow?","A part of the solution in a proof of a question related to the method of moments states that, $$\frac{1}n\sum_{i=1}^n{(y_i-\bar{y})^2}=\frac{1}n\sum_{i=1}^n{y_i^2-\bar{y}^2}$$ How does this follow?",,"['statistics', 'summation']"
40,Expectation in kernel density estimate,Expectation in kernel density estimate,,"Let $X_1,\ldots,X_n$ be i.i.d. random variables with common density $f$.   Let $K(\cdot)$ be a probability density function defined on the real line.   Then for a nonstochastic $h$: $$E[\hat{f}]=\frac{1}{nh}\sum_{i=1}^n E\left[K\left(\frac{x-X_i}{h}\right)\right]$$   $$=\frac{1}{h}E\left[K\left(\frac{x-X_i}{h}\right)\right]=\frac{1}{h}\int  K\left(\frac{x-u}{h}\right)f(u)du$$ $$=\int K(y)f(x+hy)dy$$ I'm having trouble to understand how they get the two last equalities. Since $K$ is a probability density function $$\frac{1}{h}E[K\left(\frac{x-X_i}{h}\right)]=\frac{1}{h}\int K\left(\frac{x-X_i}{h}\right)d\left(\frac{x-X_i}{h}\right)$$ but I had a density that is function of another density. Anyone can help me understood that?","Let $X_1,\ldots,X_n$ be i.i.d. random variables with common density $f$.   Let $K(\cdot)$ be a probability density function defined on the real line.   Then for a nonstochastic $h$: $$E[\hat{f}]=\frac{1}{nh}\sum_{i=1}^n E\left[K\left(\frac{x-X_i}{h}\right)\right]$$   $$=\frac{1}{h}E\left[K\left(\frac{x-X_i}{h}\right)\right]=\frac{1}{h}\int  K\left(\frac{x-u}{h}\right)f(u)du$$ $$=\int K(y)f(x+hy)dy$$ I'm having trouble to understand how they get the two last equalities. Since $K$ is a probability density function $$\frac{1}{h}E[K\left(\frac{x-X_i}{h}\right)]=\frac{1}{h}\int K\left(\frac{x-X_i}{h}\right)d\left(\frac{x-X_i}{h}\right)$$ but I had a density that is function of another density. Anyone can help me understood that?",,"['probability', 'statistics', 'self-learning', 'expectation']"
41,Comparing a value with mean and standard deviation,Comparing a value with mean and standard deviation,,"See the following table, Mean    Standard Deviation   Marks of Tom  English     65            10                  55 Maths       51             4                  59 Science     65             4                  65 History     82             6                  64 By using the given details what can you tell about the level of performance of each subject and level of performance of Tom. Plz Help.","See the following table, Mean    Standard Deviation   Marks of Tom  English     65            10                  55 Maths       51             4                  59 Science     65             4                  65 History     82             6                  64 By using the given details what can you tell about the level of performance of each subject and level of performance of Tom. Plz Help.",,"['statistics', 'standard-deviation', 'descriptive-statistics']"
42,What is the expected number of suits in a hand of 5 cards?,What is the expected number of suits in a hand of 5 cards?,,"Suppose we draw 5 cards out of a deck of 52.What is the expected number of different suits in our hand? For example, if we draw K♠ 3♠ 10♥ 8♥ 6♣, there are three different suits in our hand. Answer: 3.114 Here’s what I’ve tried. N = number of suits E[X] = (1*P(N=1) + 2*P(N=2) + 3*P(N=3) + 4*P(N=4))/52C5 Now to calculate the probability of N suits is where I get a problem. P(N=1) = (4C1 *13C5)/52C5 My Reasoning: (Pick a suit * choose 5 cards from it) P(N=2) = (4C2 *13C1*13C1*24C3)/52C5 My Reasoning: (Pick two suits * pick 1 card from each * choose 3 from the remaining cards in those two suits) P(N=3) = (4C3 *13C1*13C1*13C1*36C2)/52C5 My Reasoning: (Pick three suits * pick 1 card from each * choose 2 from the remaining cards in those three suits) P(N=4) = (4C4 *13C1^4*48C1)/52C5 My Reasoning: (Pick four suits * pick 1 card from each * choose 1 from the remaining cards in those four suits) This leaves me with: 1(5148)+2(2052336)+3(5536440)+4(1370928)/2598960 Which equals 10.002 They’re aren’t even 10 suits in a deck so I’ve done something very wrong.","Suppose we draw 5 cards out of a deck of 52.What is the expected number of different suits in our hand? For example, if we draw K♠ 3♠ 10♥ 8♥ 6♣, there are three different suits in our hand. Answer: 3.114 Here’s what I’ve tried. N = number of suits E[X] = (1*P(N=1) + 2*P(N=2) + 3*P(N=3) + 4*P(N=4))/52C5 Now to calculate the probability of N suits is where I get a problem. P(N=1) = (4C1 *13C5)/52C5 My Reasoning: (Pick a suit * choose 5 cards from it) P(N=2) = (4C2 *13C1*13C1*24C3)/52C5 My Reasoning: (Pick two suits * pick 1 card from each * choose 3 from the remaining cards in those two suits) P(N=3) = (4C3 *13C1*13C1*13C1*36C2)/52C5 My Reasoning: (Pick three suits * pick 1 card from each * choose 2 from the remaining cards in those three suits) P(N=4) = (4C4 *13C1^4*48C1)/52C5 My Reasoning: (Pick four suits * pick 1 card from each * choose 1 from the remaining cards in those four suits) This leaves me with: 1(5148)+2(2052336)+3(5536440)+4(1370928)/2598960 Which equals 10.002 They’re aren’t even 10 suits in a deck so I’ve done something very wrong.",,"['probability', 'statistics']"
43,Tricky permutations question,Tricky permutations question,,"There are 8 buckets, each bucket is a different color (for simplicity, let's label the colors A, B, C, D, E, F, G and H; if you like: Aqua, Brown, Cyan, Diamond, Eggshell, Fuchsia, Green, Hot-pink). There are 8 balls, each a different colour (also A, B, C, D, E, F, G, H). Someone randomly throws all 8 balls into all 8 buckets, such that all buckets have exactly 1 ball in them after all 8 balls are thrown, and the process of throwing the balls into each bucket is completely random. Q: What's the probability of exactly 3 colour matches? That is, what's the probability that 3 (and only 3) buckets have a ball tossed in them that is the same color as the bucket? Here is what I've done so-far: The no. of sample points in the sample space is P(8,8) = 8!. The number of sample points in the event of exactly 3 color matches is C(8,3) (no. of ways of chooisng the 3 'matched' colors of 8), multiplied by the number of ways of arranging the 'unmatched colors' is (8 - 3 - 1 )! = 4! (For example, suppose the unmatched colors are D, E, F, G and H: The color of the ball in the D bucket can be E, F, G or H, i.e. 4 choices to prevent a color match. The number of choices to prevent a match in bucket E will be 3, then 2 in F, then 1 in G.) So, the answer I have is: C(8,3)*4! / 8!. One possible issue is with the calculation of 4! To follow from the example in the previous paragraph, suppose an Eggplant ball is placed in the Diamond bucket. Now, there are 4 choices on the second 'unmatched' throw rather than 3... And I don't know how to resolve this. I'm also not sure if the '3 from 8' color matches should be a combination as written, or permutation. If anyone has any thoughts on this problem -- suggestions for improvement or validation of what I've done -- I would really appreciate it. Incidentally, the problem is based on Example 2.22 from Wackerly et. al.'s 'Mathematical Statistics with Applications' 7ed, 2007, but an extension (they use 3 and 1 instead of 8 and 3), and using a different method from what was done in the example.","There are 8 buckets, each bucket is a different color (for simplicity, let's label the colors A, B, C, D, E, F, G and H; if you like: Aqua, Brown, Cyan, Diamond, Eggshell, Fuchsia, Green, Hot-pink). There are 8 balls, each a different colour (also A, B, C, D, E, F, G, H). Someone randomly throws all 8 balls into all 8 buckets, such that all buckets have exactly 1 ball in them after all 8 balls are thrown, and the process of throwing the balls into each bucket is completely random. Q: What's the probability of exactly 3 colour matches? That is, what's the probability that 3 (and only 3) buckets have a ball tossed in them that is the same color as the bucket? Here is what I've done so-far: The no. of sample points in the sample space is P(8,8) = 8!. The number of sample points in the event of exactly 3 color matches is C(8,3) (no. of ways of chooisng the 3 'matched' colors of 8), multiplied by the number of ways of arranging the 'unmatched colors' is (8 - 3 - 1 )! = 4! (For example, suppose the unmatched colors are D, E, F, G and H: The color of the ball in the D bucket can be E, F, G or H, i.e. 4 choices to prevent a color match. The number of choices to prevent a match in bucket E will be 3, then 2 in F, then 1 in G.) So, the answer I have is: C(8,3)*4! / 8!. One possible issue is with the calculation of 4! To follow from the example in the previous paragraph, suppose an Eggplant ball is placed in the Diamond bucket. Now, there are 4 choices on the second 'unmatched' throw rather than 3... And I don't know how to resolve this. I'm also not sure if the '3 from 8' color matches should be a combination as written, or permutation. If anyone has any thoughts on this problem -- suggestions for improvement or validation of what I've done -- I would really appreciate it. Incidentally, the problem is based on Example 2.22 from Wackerly et. al.'s 'Mathematical Statistics with Applications' 7ed, 2007, but an extension (they use 3 and 1 instead of 8 and 3), and using a different method from what was done in the example.",,"['probability', 'statistics']"
44,Distribution of residual term in regression.,Distribution of residual term in regression.,,In regression analysis for classical linear regression model the residual term is independent of x and y and normally distributed and it is a random variable but i found somewhere written u~N and u~NID.I cannot understand the difference so can someone explain the meaning of NID (normally and independently distributed)?(sorry for my bad English).,In regression analysis for classical linear regression model the residual term is independent of x and y and normally distributed and it is a random variable but i found somewhere written u~N and u~NID.I cannot understand the difference so can someone explain the meaning of NID (normally and independently distributed)?(sorry for my bad English).,,"['statistics', 'normal-distribution']"
45,Prove that the distance from the mean to the inflection points is one standard deviation.,Prove that the distance from the mean to the inflection points is one standard deviation.,,Prove that the distance from the mean to the inflection points is one standard deviation. So I know in order to find critical points one must find the second derivative and set it equal to 0. However I'm confused how to approach this proof. Step by step explanation please!,Prove that the distance from the mean to the inflection points is one standard deviation. So I know in order to find critical points one must find the second derivative and set it equal to 0. However I'm confused how to approach this proof. Step by step explanation please!,,['statistics']
46,Asymptotically unbiased estimator for 1/p in Bernouilli distribution?,Asymptotically unbiased estimator for 1/p in Bernouilli distribution?,,"Suppose I have a sample of $n$ independent stochastic variables, each Bernouilli distributed with parameter $p$ (you may assume $0 < p <1$). I was wondering if there exist (asymptotically) unbiased estimators for $1/p$ and, if there exist multiple, which are to be preferred. In this question , it is explained why no estimator can exist, which is unbiased for infinitely many $p$. This answers the question of unbiased estimation negatively, so now I'm still hoping for asymptotically unbiased estimation  (i.e. the bias tends to $0$ as $n$ tends to $+\infty$). $1/\bar{X}_n$, where $\bar{X}_n$ is the (binomially distributed) sample mean, seems to be an obvious choice. There is of course the problem that $\bar{X}_n$ might become zero (with non-zero probability), but since the probability that this happens tends to zero as $n$ grows, I presume one could take  $$ T = \begin{cases}1/\bar{X}_n \quad n \neq 0 \\ \omega_n \quad n = 0 \end{cases} $$  for some fixed value $\omega_n \geq n$. Even then, I wouldn't know if this estimator $T$ would have any desirable properties if $\omega_n$ is chosen adequately, or even whether we can choose $\omega_n$ such that $T$ is asymptotically unbiased. Any thoughts you have on this are welcome!","Suppose I have a sample of $n$ independent stochastic variables, each Bernouilli distributed with parameter $p$ (you may assume $0 < p <1$). I was wondering if there exist (asymptotically) unbiased estimators for $1/p$ and, if there exist multiple, which are to be preferred. In this question , it is explained why no estimator can exist, which is unbiased for infinitely many $p$. This answers the question of unbiased estimation negatively, so now I'm still hoping for asymptotically unbiased estimation  (i.e. the bias tends to $0$ as $n$ tends to $+\infty$). $1/\bar{X}_n$, where $\bar{X}_n$ is the (binomially distributed) sample mean, seems to be an obvious choice. There is of course the problem that $\bar{X}_n$ might become zero (with non-zero probability), but since the probability that this happens tends to zero as $n$ grows, I presume one could take  $$ T = \begin{cases}1/\bar{X}_n \quad n \neq 0 \\ \omega_n \quad n = 0 \end{cases} $$  for some fixed value $\omega_n \geq n$. Even then, I wouldn't know if this estimator $T$ would have any desirable properties if $\omega_n$ is chosen adequately, or even whether we can choose $\omega_n$ such that $T$ is asymptotically unbiased. Any thoughts you have on this are welcome!",,"['statistics', 'parameter-estimation']"
47,Poisson Distribution - Poor Understanding,Poisson Distribution - Poor Understanding,,"So I have this question about poisson distribution: ""The number of computers bought during one day from shop A is given a Poisson distribution mean of 3.5, while the same for another shop B is 5.0, calculate the probability that a total of fewer than 10 computers are sold from both shops in 4 out of 5 consecutive days"" I proceded to calculate the net probability which came to $0.653$, I then realised you'd need to use Binomial Distributiopn, so I put in the given and needed values giving me $0.315$, this however is where I get confused, I thought this was the answer but the markscheme says add on $(0.635^5)$ and I have no idea why. Could someone explain this to me? Many thanks.","So I have this question about poisson distribution: ""The number of computers bought during one day from shop A is given a Poisson distribution mean of 3.5, while the same for another shop B is 5.0, calculate the probability that a total of fewer than 10 computers are sold from both shops in 4 out of 5 consecutive days"" I proceded to calculate the net probability which came to $0.653$, I then realised you'd need to use Binomial Distributiopn, so I put in the given and needed values giving me $0.315$, this however is where I get confused, I thought this was the answer but the markscheme says add on $(0.635^5)$ and I have no idea why. Could someone explain this to me? Many thanks.",,"['statistics', 'poisson-distribution']"
48,"If $X\sim\operatorname{Poisson}(u)$ and $\theta = \mathbb{P}\{X=0\} = e^{-u}$, is $\hat{\theta}_1 = e^{-X}$ an unbiased estimator?","If  and , is  an unbiased estimator?",X\sim\operatorname{Poisson}(u) \theta = \mathbb{P}\{X=0\} = e^{-u} \hat{\theta}_1 = e^{-X},"If $X\sim\operatorname{Poisson}(u)$ and $\theta = \mathbb{P}\{X=0\} = e^{-u}$, is $\hat{\theta}_1  = e^{-X}$ an unbiased estimator? Here's what I tried, is this right? $$ \begin{align} \mathbb{E}[\hat{\theta}_1] &= \mathbb{E}[e^{-X}]  \\ &= e^{\mathbb{E}[-X]}\\ &= e^{-u} \\ &= \theta \end{align} $$ Show that $\hat{\theta}_2 = w(X)$ is an unbiased estimator of $\theta$, where $w(0)=1$ and $w(x)=0$ if $x> 0$. I honestly don't know how to do this part. Thanks for the help! Compare MSEs of $\hat{\theta}_1$ and $\hat{\theta}_2$ for estimating $\theta = e^{-u}$ when $u=1$ and $u=2$ So the MSE is equal to $Var() + Bias()^2$ Since $\hat{\theta}_2$ is unbiased, $MSE=Var(\hat{\theta}_2)$ My variance is $E(X^2)-E(X)^2$. I think $E(X)^2$ is $(e^{-u})^2=e^{-2u}$, and for $E(X^2)$ do I just use the $\sum_{k=0}^\infty(w(k)^2P[X=k])$?","If $X\sim\operatorname{Poisson}(u)$ and $\theta = \mathbb{P}\{X=0\} = e^{-u}$, is $\hat{\theta}_1  = e^{-X}$ an unbiased estimator? Here's what I tried, is this right? $$ \begin{align} \mathbb{E}[\hat{\theta}_1] &= \mathbb{E}[e^{-X}]  \\ &= e^{\mathbb{E}[-X]}\\ &= e^{-u} \\ &= \theta \end{align} $$ Show that $\hat{\theta}_2 = w(X)$ is an unbiased estimator of $\theta$, where $w(0)=1$ and $w(x)=0$ if $x> 0$. I honestly don't know how to do this part. Thanks for the help! Compare MSEs of $\hat{\theta}_1$ and $\hat{\theta}_2$ for estimating $\theta = e^{-u}$ when $u=1$ and $u=2$ So the MSE is equal to $Var() + Bias()^2$ Since $\hat{\theta}_2$ is unbiased, $MSE=Var(\hat{\theta}_2)$ My variance is $E(X^2)-E(X)^2$. I think $E(X)^2$ is $(e^{-u})^2=e^{-2u}$, and for $E(X^2)$ do I just use the $\sum_{k=0}^\infty(w(k)^2P[X=k])$?",,"['statistics', 'parameter-estimation']"
49,Why the standard deviation of the sample mean is calculated as $\frac{\sigma}{\sqrt{n}}$?,Why the standard deviation of the sample mean is calculated as ?,\frac{\sigma}{\sqrt{n}},"According to Wikipedia, the standard deviation of a sample mean is calculated as follows $$\frac{\sigma}{\sqrt{n}}$$ Why is that? Why do we need to divide the standard deviation of the population by the square root of $n$ (which should I think be the size of the sample)? Why should that make sense?","According to Wikipedia, the standard deviation of a sample mean is calculated as follows $$\frac{\sigma}{\sqrt{n}}$$ Why is that? Why do we need to divide the standard deviation of the population by the square root of $n$ (which should I think be the size of the sample)? Why should that make sense?",,['statistics']
50,What is the probability that for exactly three calls the lines are occupied?,What is the probability that for exactly three calls the lines are occupied?,,"I am unable to understand the following statement. The phone lines to an airline reservation system are occupied 40% of the time. Assume that the events that the lines are occupied on successive calls are independent. Assume that 10 calls are placed to the airline. $\textbf{What is the probability that for exactly three calls the lines are occupied?}$ I am not exactly confused with how I will find the answer. I am simply having difficulties understanding the question. Here is my current understanding of the statement: Each of the phone lines of an airline reservation system are occupied 40% of the time on a certain time interval. The probability that one phone line is occupied is independent of the vacancy of other phone lines, thus, the probability that any phone lines will be occupied at a given time interval will always be 40%. $\textbf{The airline has 10 phone lines.}$ The last sentence stating ""Assume that 10 calls are placed to the airline"" confuses me. Does this mean that the airline has 10 phone lines? Or are there 10 phone calls to the airline. The latter interpretation seems to be pointless however. I just want to make sure that I am not missing something.","I am unable to understand the following statement. The phone lines to an airline reservation system are occupied 40% of the time. Assume that the events that the lines are occupied on successive calls are independent. Assume that 10 calls are placed to the airline. $\textbf{What is the probability that for exactly three calls the lines are occupied?}$ I am not exactly confused with how I will find the answer. I am simply having difficulties understanding the question. Here is my current understanding of the statement: Each of the phone lines of an airline reservation system are occupied 40% of the time on a certain time interval. The probability that one phone line is occupied is independent of the vacancy of other phone lines, thus, the probability that any phone lines will be occupied at a given time interval will always be 40%. $\textbf{The airline has 10 phone lines.}$ The last sentence stating ""Assume that 10 calls are placed to the airline"" confuses me. Does this mean that the airline has 10 phone lines? Or are there 10 phone calls to the airline. The latter interpretation seems to be pointless however. I just want to make sure that I am not missing something.",,"['probability', 'statistics', 'probability-distributions', 'binomial-distribution']"
51,MLE of β in the gamma distribution?,MLE of β in the gamma distribution?,,"So I have the pdf for the gamma distribution, $$f(x) = \frac{1}{\Gamma(\alpha)} \beta^\alpha x^{\alpha - 1} e^{-\beta x} $$ and I'm having trouble getting to the MLE of $\beta$, which should be $\frac{\alpha}{\overline{x}}$.  I keep messing up when it comes to taking the log but I'm not sure where.  Thanks!","So I have the pdf for the gamma distribution, $$f(x) = \frac{1}{\Gamma(\alpha)} \beta^\alpha x^{\alpha - 1} e^{-\beta x} $$ and I'm having trouble getting to the MLE of $\beta$, which should be $\frac{\alpha}{\overline{x}}$.  I keep messing up when it comes to taking the log but I'm not sure where.  Thanks!",,"['probability', 'statistics', 'estimation']"
52,Interpreting OLS Regression Coefficients with High Multicolinearity,Interpreting OLS Regression Coefficients with High Multicolinearity,,"I am having trouble understanding the interpretation of OLS coefficients when predictors are highly correlated. My understanding of OLS coefficients is that they estimate a change in the expected outcome following a 1 unit increase in the predictor, holding all other predictors constant. However, I cannot understand why this doesn't lead to underestimates following an increase in both positively correlated predictors. To give a concrete example. Suppose income = constant + 0.9*innate intelligence + 0.05*skills acquired through education +  random variation but we can only observe performance in an iq test and years at school, where years of education = 0.1* innate intelligence + random variation and performance in iq test = innate intelligence + random variation Would OLS underestimate the expected salary of someone who has both a high score in an iq test and many years of education, since the predictive power of either coefficient holding the other constant would be small?","I am having trouble understanding the interpretation of OLS coefficients when predictors are highly correlated. My understanding of OLS coefficients is that they estimate a change in the expected outcome following a 1 unit increase in the predictor, holding all other predictors constant. However, I cannot understand why this doesn't lead to underestimates following an increase in both positively correlated predictors. To give a concrete example. Suppose income = constant + 0.9*innate intelligence + 0.05*skills acquired through education +  random variation but we can only observe performance in an iq test and years at school, where years of education = 0.1* innate intelligence + random variation and performance in iq test = innate intelligence + random variation Would OLS underestimate the expected salary of someone who has both a high score in an iq test and many years of education, since the predictive power of either coefficient holding the other constant would be small?",,"['statistics', 'statistical-inference', 'regression']"
53,Why is $R^2=\rho^2$,Why is,R^2=\rho^2,"Considering $y_i=\beta_1+\beta_2x_i+\epsilon_i$ $\bar y_i=\hat\beta_1+\hat\beta_2\bar x_i+\bar\epsilon_i$ a linear equation of least square used when it seems that there is a like between two data, $\epsilon_i$ is the noise, usually $\epsilon$~$N(0,\sigma)$ $\beta_i$ are constant we find by the least square regression method. $\hat\beta_i$ are estimated values of those $\beta_i$ Having $$||y-\bar y||^2=||\hat y-\bar y||^2+ ||\epsilon||^2$$ $$\underbrace{\sum\limits_{i=1}^{n}(y_i-\bar y)^2}_{TSS}=\underbrace{\sum\limits_{i=1}^{n}(\hat y_i-\bar y)^2}_{ESS} +\underbrace{\sum\limits_{i=1}^{n}(\hat\epsilon_i^2)}_{RSS}$$ I have to show that $R^2=\frac{ESS}{TSS}=\frac{(\hat y-\bar y)^2}{(y_-\bar y)^2}=1-\frac{||\hat \epsilon||^2}{(\hat y-\bar y)^2}=\underbrace{1-\frac{RSS}{TSS}=\rho_{xy}^2}_{\text{The step I don't get}}$ with $$\rho = \rho_{xy} =\frac{\sum ^n _{i=1}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum ^n _{i=1}(x_i - \bar{x})^2} \sqrt{\sum ^n _{i=1}(y_i - \bar{y})^2}}$$ I don't even know where to start... Any hint appreciate.","Considering $y_i=\beta_1+\beta_2x_i+\epsilon_i$ $\bar y_i=\hat\beta_1+\hat\beta_2\bar x_i+\bar\epsilon_i$ a linear equation of least square used when it seems that there is a like between two data, $\epsilon_i$ is the noise, usually $\epsilon$~$N(0,\sigma)$ $\beta_i$ are constant we find by the least square regression method. $\hat\beta_i$ are estimated values of those $\beta_i$ Having $$||y-\bar y||^2=||\hat y-\bar y||^2+ ||\epsilon||^2$$ $$\underbrace{\sum\limits_{i=1}^{n}(y_i-\bar y)^2}_{TSS}=\underbrace{\sum\limits_{i=1}^{n}(\hat y_i-\bar y)^2}_{ESS} +\underbrace{\sum\limits_{i=1}^{n}(\hat\epsilon_i^2)}_{RSS}$$ I have to show that $R^2=\frac{ESS}{TSS}=\frac{(\hat y-\bar y)^2}{(y_-\bar y)^2}=1-\frac{||\hat \epsilon||^2}{(\hat y-\bar y)^2}=\underbrace{1-\frac{RSS}{TSS}=\rho_{xy}^2}_{\text{The step I don't get}}$ with $$\rho = \rho_{xy} =\frac{\sum ^n _{i=1}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum ^n _{i=1}(x_i - \bar{x})^2} \sqrt{\sum ^n _{i=1}(y_i - \bar{y})^2}}$$ I don't even know where to start... Any hint appreciate.",,"['statistics', 'regression', 'correlation']"
54,Proof Chebyshev's Inequality,Proof Chebyshev's Inequality,,"I want to proof Chebyshev's Inequality using Markov's inequality . Cheb.Ineq: $$P(|X-\mu| \ge a) \le \frac{Var(X)}{a}$$ So I'm starting with Markov's Inequality: $$P(|X-\mu| \ge a) \le \frac{E(|X-\mu|)}{a}$$ I replace $|X-\mu|$ by $(X-\mu)$, and square both sides, which leads to: $$P((X-\mu)^2 \ge a^2) \le \frac{E((X-\mu)^2)}{a^2}$$ The numerator of the right hand side is the variance of $X$. So we get: $$P((X-\mu)^2 \ge a^2) \le \frac{Var(X)}{a^2}$$ So far so good. The proof that I read now just simply states that $P((X-\mu)^2 \ge a^2)$ is equal to $P(|X-\mu| \ge a)$ and this leads to $$P(|X-\mu| \ge a) \le \frac{Var(X)}{a^2}=\frac {\sigma^2} {a^2}$$ I really don't get the last step. Why are $P((X-\mu)^2 \ge a^2)$ and $P(|X-\mu| \ge a)$ are equal?","I want to proof Chebyshev's Inequality using Markov's inequality . Cheb.Ineq: $$P(|X-\mu| \ge a) \le \frac{Var(X)}{a}$$ So I'm starting with Markov's Inequality: $$P(|X-\mu| \ge a) \le \frac{E(|X-\mu|)}{a}$$ I replace $|X-\mu|$ by $(X-\mu)$, and square both sides, which leads to: $$P((X-\mu)^2 \ge a^2) \le \frac{E((X-\mu)^2)}{a^2}$$ The numerator of the right hand side is the variance of $X$. So we get: $$P((X-\mu)^2 \ge a^2) \le \frac{Var(X)}{a^2}$$ So far so good. The proof that I read now just simply states that $P((X-\mu)^2 \ge a^2)$ is equal to $P(|X-\mu| \ge a)$ and this leads to $$P(|X-\mu| \ge a) \le \frac{Var(X)}{a^2}=\frac {\sigma^2} {a^2}$$ I really don't get the last step. Why are $P((X-\mu)^2 \ge a^2)$ and $P(|X-\mu| \ge a)$ are equal?",,['statistics']
55,What's the difference between MCMC and particle MCMC?,What's the difference between MCMC and particle MCMC?,,Markov chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution as its equilibrium distribution. So how does the 'particle' bit augment the 'MCMC' bit?,Markov chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution as its equilibrium distribution. So how does the 'particle' bit augment the 'MCMC' bit?,,"['statistics', 'sampling']"
56,What is the variance of the volumes of particles?,What is the variance of the volumes of particles?,,"According to Zimmels (1983), the sizes of particles used in sedimentation experiments often have a uniform distribution. In sedimentation involving mixtures of particles of various sizes, the larger particles hinder the movements of the smaller ones. Thus, it is important to study both the mean and the variance of particle sizes. Suppose that spherical particles have diameters that are uniformly distributed between $0.01$ and $0.05$ centimeters. Find the mean and variance of the volumes of these particles (Volume of the sphere is $\frac 4 3 \pi R^3$). Book's solution: Mean $= 0.0000065\pi$ Variance $= 0.0003525\pi^2$ My solution: Volume $= \frac 4 3 \pi R^3 = \pi D^3$, where $D$ is diameter. Mean $\displaystyle = E\left[\frac \pi 6 D^3\right] = \int_{0.01}^{0.05} \frac \pi 6  D^3 \cdot25$. $= 0.0000065\pi$ (same answer with the book) \begin{align} \text{Variance} & = E\left[\frac{\pi^2}{36} D^6\right] - E\left[ \frac \pi 6  D^3\right]^2 \\[8pt] & = \int_{0.01}^{0.05} \left[ \frac{\pi^2}{36} D^6\right] \cdot 25 - 4.225\times(10^{-11})\times \pi^2 \\[8pt] & = 3.5254 \times (10^{-11})\times \pi^2 \text{ (about $10^7$ times smaller than the book solution)} \end{align} Is my solution wrong?","According to Zimmels (1983), the sizes of particles used in sedimentation experiments often have a uniform distribution. In sedimentation involving mixtures of particles of various sizes, the larger particles hinder the movements of the smaller ones. Thus, it is important to study both the mean and the variance of particle sizes. Suppose that spherical particles have diameters that are uniformly distributed between $0.01$ and $0.05$ centimeters. Find the mean and variance of the volumes of these particles (Volume of the sphere is $\frac 4 3 \pi R^3$). Book's solution: Mean $= 0.0000065\pi$ Variance $= 0.0003525\pi^2$ My solution: Volume $= \frac 4 3 \pi R^3 = \pi D^3$, where $D$ is diameter. Mean $\displaystyle = E\left[\frac \pi 6 D^3\right] = \int_{0.01}^{0.05} \frac \pi 6  D^3 \cdot25$. $= 0.0000065\pi$ (same answer with the book) \begin{align} \text{Variance} & = E\left[\frac{\pi^2}{36} D^6\right] - E\left[ \frac \pi 6  D^3\right]^2 \\[8pt] & = \int_{0.01}^{0.05} \left[ \frac{\pi^2}{36} D^6\right] \cdot 25 - 4.225\times(10^{-11})\times \pi^2 \\[8pt] & = 3.5254 \times (10^{-11})\times \pi^2 \text{ (about $10^7$ times smaller than the book solution)} \end{align} Is my solution wrong?",,"['probability', 'statistics']"
57,Geometric sum of geometric random variables,Geometric sum of geometric random variables,,"I am looking to find the probability mass function of $Y=\sum_{i=1}^NX_i$ where $X_i\sim\textrm{Geometric}(a)$ and $N\sim\textrm{Geometric}(b)$. I attempted to do this by finding the probability generating function of $Y$ and comparing it to known probability generating functions to take advantage of the uniqueness property. (In my searches online, it sounds like I should find that $Y\sim\textrm{Geometric}(ab)$.) (Note that I am using the geometric distribution with support $0,1,\dots$) I calculated the probability generating function of $X$, $g_X(s)=\frac{a}{1-(1-a)s}$ and correspondingly, $g_N(s)=\frac{b}{1-(1-b)s}$, so the law of total expectation implies that $g_Y(s)=g_N(g_X(s))=\frac{b}{1-(1-b)\frac{a}{1-(1-a)s}}$, however I cannot manipulate this to look like any probability generating function that I recognize, and certainly not $g_Z(s)=\frac{ab}{1-(1-ab)s}$ where $Z\sim\textrm{Geometric}(ab)$. Is there something wrong with my thinking here? Thanks for the help.","I am looking to find the probability mass function of $Y=\sum_{i=1}^NX_i$ where $X_i\sim\textrm{Geometric}(a)$ and $N\sim\textrm{Geometric}(b)$. I attempted to do this by finding the probability generating function of $Y$ and comparing it to known probability generating functions to take advantage of the uniqueness property. (In my searches online, it sounds like I should find that $Y\sim\textrm{Geometric}(ab)$.) (Note that I am using the geometric distribution with support $0,1,\dots$) I calculated the probability generating function of $X$, $g_X(s)=\frac{a}{1-(1-a)s}$ and correspondingly, $g_N(s)=\frac{b}{1-(1-b)s}$, so the law of total expectation implies that $g_Y(s)=g_N(g_X(s))=\frac{b}{1-(1-b)\frac{a}{1-(1-a)s}}$, however I cannot manipulate this to look like any probability generating function that I recognize, and certainly not $g_Z(s)=\frac{ab}{1-(1-ab)s}$ where $Z\sim\textrm{Geometric}(ab)$. Is there something wrong with my thinking here? Thanks for the help.",,"['probability', 'statistics']"
58,Link between exponential distribution and poisson probability mass function,Link between exponential distribution and poisson probability mass function,,"Customers arrive randomly and independently at a service window, and the time between arrivals has an exponential distribution with a mean of 12 minutes. Let X equal the number of arrivals per hour. What is P[X = 10]? Now the solution to this problem uses this logic: If the time between arrivals is exponential with mean 12 minutes and arrival times are independent then the number of arrivals in any single minute is Poisson with mean 1/12. Since the sum of independent Poisson variables is also Poisson, the number of arrivals in an hour will be Poisson with mean: 5 My question is how can an exponential distribution be related to a Poisson PMF? Thank you","Customers arrive randomly and independently at a service window, and the time between arrivals has an exponential distribution with a mean of 12 minutes. Let X equal the number of arrivals per hour. What is P[X = 10]? Now the solution to this problem uses this logic: If the time between arrivals is exponential with mean 12 minutes and arrival times are independent then the number of arrivals in any single minute is Poisson with mean 1/12. Since the sum of independent Poisson variables is also Poisson, the number of arrivals in an hour will be Poisson with mean: 5 My question is how can an exponential distribution be related to a Poisson PMF? Thank you",,"['probability', 'statistics', 'probability-distributions', 'exponential-function', 'poisson-distribution']"
59,Take the outcome of a draw in ELO formula,Take the outcome of a draw in ELO formula,,Is there any way to get the probability of a draw outcome using ELO formula as it only gives the Win probability ELO formula is given by $E = \frac{1}{1+10^\frac{d}{a}}$ where d is the difference in ELO Rating and a is a constant,Is there any way to get the probability of a draw outcome using ELO formula as it only gives the Win probability ELO formula is given by $E = \frac{1}{1+10^\frac{d}{a}}$ where d is the difference in ELO Rating and a is a constant,,"['probability', 'statistics']"
60,Prove that a symmetric distribution has zero skewness,Prove that a symmetric distribution has zero skewness,,"Prove that a symmetric distribution has zero skewness. Okay so the question states : First prove that a distribution symmetric about a point a, has mean a. I found an answer on how to prove this here: Proof of $E(X)=a$ when $a$ is a point of symmetry Of course I used method 2 But now for the rest of this proof I'm struggling. $\mu_{2X}$ and $\mu_{3X}$ are the $2^{nd}$ and $3^{rd}$ moments about the mean, respectively (w.r.t X). $$E[X] = \mu = a$$ $$\mu_{2X}= E[(X-a)^2] = E[((a+Y)-a)^2] = E[Y^2]$$ $$\mu_{3X} = E[Y^3]$$ Please just check my notation. I always use subscripts to show which distribution is in queston but especially with Skewness, can this be done? $$\text{Skewness} = \sqrt{\beta_{1X}} = \frac{\frac{1}{n}\sum_{x \in S}y^3}{(\sqrt{\frac{1}{n}\sum_{x \in S}y^2})^3}$$ So I thought it just needs to be shown that the numerator is always equal to 0. I don't know if I approached it correctly but it seemed to make sense to me and now I'm stuck","Prove that a symmetric distribution has zero skewness. Okay so the question states : First prove that a distribution symmetric about a point a, has mean a. I found an answer on how to prove this here: Proof of $E(X)=a$ when $a$ is a point of symmetry Of course I used method 2 But now for the rest of this proof I'm struggling. $\mu_{2X}$ and $\mu_{3X}$ are the $2^{nd}$ and $3^{rd}$ moments about the mean, respectively (w.r.t X). $$E[X] = \mu = a$$ $$\mu_{2X}= E[(X-a)^2] = E[((a+Y)-a)^2] = E[Y^2]$$ $$\mu_{3X} = E[Y^3]$$ Please just check my notation. I always use subscripts to show which distribution is in queston but especially with Skewness, can this be done? $$\text{Skewness} = \sqrt{\beta_{1X}} = \frac{\frac{1}{n}\sum_{x \in S}y^3}{(\sqrt{\frac{1}{n}\sum_{x \in S}y^2})^3}$$ So I thought it just needs to be shown that the numerator is always equal to 0. I don't know if I approached it correctly but it seemed to make sense to me and now I'm stuck",,"['probability', 'statistics', 'probability-distributions']"
61,"Probability - something with a small chance of occurring, but is repeated multiple times.","Probability - something with a small chance of occurring, but is repeated multiple times.",,"For example, if you have a $1.5\%$ chance of obtaining admission to any school and you apply to $15$ schools what is the chance that you'll get into a least $1$ school? Is this as simple as $1.5\%$ $×$ $15 $= $22.5\%$ chance of getting into one school? Phrased another way: if an event has a $1.5\%$ chance of occurring every time it is performed, and you perform it $15$ times what is the probability that the event occurs at least once?","For example, if you have a $1.5\%$ chance of obtaining admission to any school and you apply to $15$ schools what is the chance that you'll get into a least $1$ school? Is this as simple as $1.5\%$ $×$ $15 $= $22.5\%$ chance of getting into one school? Phrased another way: if an event has a $1.5\%$ chance of occurring every time it is performed, and you perform it $15$ times what is the probability that the event occurs at least once?",,"['probability', 'statistics']"
62,Box-Muller Independence Proof by Change of Variables (Help finding the Inverses),Box-Muller Independence Proof by Change of Variables (Help finding the Inverses),,"Let $X_1=\cos(2 \pi U_1)\sqrt{-2 \log(U_2)}$ and $X_2=\sin(2 \pi U_1)\sqrt{-2 \log(U_2)}$ wher $U_1$ and $U_2$ are iid uniform (0,1). Prove that $X_1$ and $X_2$ are independent N(0,1) random variables. So my approach (and one of the proofs I was shown) did this though change of variables where $g_1(u_1,u_2)=\cos(2 \pi U_1)\sqrt{-2 \log(U_2)}$ and $g_2(u_1,u_2)=\cos(2 \pi U_1)\sqrt{-2 \log(U_2)}$ and essentially found their inverses $g_1^{-1}(x_1,x_2)$ and $g_2^{-1}(x_1,x_2)$, and used those to find the Jacobian (essentially here Box-Muller Transform Normality ) I'm wondering how they found those inverses? The rest of the proof I get, but I am not as familiar with a multi-variable inverse.","Let $X_1=\cos(2 \pi U_1)\sqrt{-2 \log(U_2)}$ and $X_2=\sin(2 \pi U_1)\sqrt{-2 \log(U_2)}$ wher $U_1$ and $U_2$ are iid uniform (0,1). Prove that $X_1$ and $X_2$ are independent N(0,1) random variables. So my approach (and one of the proofs I was shown) did this though change of variables where $g_1(u_1,u_2)=\cos(2 \pi U_1)\sqrt{-2 \log(U_2)}$ and $g_2(u_1,u_2)=\cos(2 \pi U_1)\sqrt{-2 \log(U_2)}$ and essentially found their inverses $g_1^{-1}(x_1,x_2)$ and $g_2^{-1}(x_1,x_2)$, and used those to find the Jacobian (essentially here Box-Muller Transform Normality ) I'm wondering how they found those inverses? The rest of the proof I get, but I am not as familiar with a multi-variable inverse.",,"['statistics', 'random-variables', 'self-learning', 'inverse']"
63,"Confusion regarding limiting variances (Casella, Statistical Inference, 2nd edition, example 10.1.8)","Confusion regarding limiting variances (Casella, Statistical Inference, 2nd edition, example 10.1.8)",,"In the book Statistical Inference (George Casella 2nd ed.), page 470, there is an example: $\bar{X}_n$ is the mean of $n$ iid observations, and E$X=\mu$, $\operatorname{Var}X=\sigma^2$. ""If we take $T_n=1/\bar{X}_n$, we find that the variance is $\operatorname{Var}T_n=\infty$, so the limit of the variances is infinity."" Why is the limit of the variance infinity? I can go as far as $$\operatorname{Var}\frac 1{\bar{X}_n}=\operatorname{E}_{\bar{X}_n}\left[\left(\frac1{\bar{X}_n}-\operatorname{E} \frac 1{\bar X_n} \right)^2 \mid \mu\right] $$ what's next? I know that $\lim_{n\to\infty}\operatorname{Var}{\bar X_n}=0$. However, $\lim_{n\to\infty}1/\operatorname{Var}{\bar X_n}\not=\lim_{n\to\infty}\operatorname{Var}1/\bar X_n$. How can we show the variance approaches to infinity for sufficiently large $n$? Thanks!","In the book Statistical Inference (George Casella 2nd ed.), page 470, there is an example: $\bar{X}_n$ is the mean of $n$ iid observations, and E$X=\mu$, $\operatorname{Var}X=\sigma^2$. ""If we take $T_n=1/\bar{X}_n$, we find that the variance is $\operatorname{Var}T_n=\infty$, so the limit of the variances is infinity."" Why is the limit of the variance infinity? I can go as far as $$\operatorname{Var}\frac 1{\bar{X}_n}=\operatorname{E}_{\bar{X}_n}\left[\left(\frac1{\bar{X}_n}-\operatorname{E} \frac 1{\bar X_n} \right)^2 \mid \mu\right] $$ what's next? I know that $\lim_{n\to\infty}\operatorname{Var}{\bar X_n}=0$. However, $\lim_{n\to\infty}1/\operatorname{Var}{\bar X_n}\not=\lim_{n\to\infty}\operatorname{Var}1/\bar X_n$. How can we show the variance approaches to infinity for sufficiently large $n$? Thanks!",,"['limits', 'statistics', 'asymptotics', 'statistical-inference']"
64,Covariance of Two Dependent Variables,Covariance of Two Dependent Variables,,"So I'm looking at the following: $\operatorname{Cov}(X, X^2)$ where $X$ is the standard normal.  So I know that $E[X] = 0$ and we have: $$\operatorname{Cov}(X, X^2) = E(X \cdot X^2) - E[X] \cdot E[X^2] = E[X^3] - 0 \cdot E[X^2] = E[X^3]$$ From googling around, apparently this $= 0$, but I'm not sure why. Any clarification here? Thanks, Mariogs","So I'm looking at the following: $\operatorname{Cov}(X, X^2)$ where $X$ is the standard normal.  So I know that $E[X] = 0$ and we have: $$\operatorname{Cov}(X, X^2) = E(X \cdot X^2) - E[X] \cdot E[X^2] = E[X^3] - 0 \cdot E[X^2] = E[X^3]$$ From googling around, apparently this $= 0$, but I'm not sure why. Any clarification here? Thanks, Mariogs",,"['probability', 'statistics']"
65,Interpreting The Weak Law of Large Numbers,Interpreting The Weak Law of Large Numbers,,"Given the usual setup, the weak law of large numbers states that for any $\epsilon > 0$ $$ \lim_{n\rightarrow \infty}P(|M_n - \mu| > \epsilon) = 0 $$ According to this author, the interpretation is as follows: (1) For large values of $n$, (i.e. $n \ge N$ for some $N$) the   probability that the value of $M_n$ (the sample mean) differs from the   population mean $\mu$ by more than any given number $\epsilon > 0$   is 0. (2) Alternatively, all probability is concentrated in an $\epsilon$   interval around $\mu$. (3) Alternatively, almost surely, for large samples, the sample mean is   within an $\epsilon$ neighborhood of the population mean. I understand all of this, but am having trouble reconciling this interpretation with the following statement from Exploring Monte Carlo Methods by Dunn and Shultis, which states (notation adapted slightly) The weak form of the law of large numbers states that, for a specified   large $N$, the sample mean $M_N$ is likely to be near $\mu$. But   it leaves open the possibility that cases when $|M_N - \mu| > \epsilon$, i.e. when the deviation of $M_N$ from $\mu$ is   outside some small tolerance interval $\epsilon$, can occur an   arbitrary (even infinite) number of times as $N$ increases; however   such occurrences happen at infrequent intervals. Is there a contradiction between these two interpretations? The first author is saying that for all $n \ge N$, with probability 1, the sample mean is within $\epsilon$ of $\mu$, but the second author says that for $n > N$ the weak law leaves open the possibility we have an infinite number of deviations larger than $\epsilon$. But if this happens an infinite number of times, how can the event occur almost surely ?","Given the usual setup, the weak law of large numbers states that for any $\epsilon > 0$ $$ \lim_{n\rightarrow \infty}P(|M_n - \mu| > \epsilon) = 0 $$ According to this author, the interpretation is as follows: (1) For large values of $n$, (i.e. $n \ge N$ for some $N$) the   probability that the value of $M_n$ (the sample mean) differs from the   population mean $\mu$ by more than any given number $\epsilon > 0$   is 0. (2) Alternatively, all probability is concentrated in an $\epsilon$   interval around $\mu$. (3) Alternatively, almost surely, for large samples, the sample mean is   within an $\epsilon$ neighborhood of the population mean. I understand all of this, but am having trouble reconciling this interpretation with the following statement from Exploring Monte Carlo Methods by Dunn and Shultis, which states (notation adapted slightly) The weak form of the law of large numbers states that, for a specified   large $N$, the sample mean $M_N$ is likely to be near $\mu$. But   it leaves open the possibility that cases when $|M_N - \mu| > \epsilon$, i.e. when the deviation of $M_N$ from $\mu$ is   outside some small tolerance interval $\epsilon$, can occur an   arbitrary (even infinite) number of times as $N$ increases; however   such occurrences happen at infrequent intervals. Is there a contradiction between these two interpretations? The first author is saying that for all $n \ge N$, with probability 1, the sample mean is within $\epsilon$ of $\mu$, but the second author says that for $n > N$ the weak law leaves open the possibility we have an infinite number of deviations larger than $\epsilon$. But if this happens an infinite number of times, how can the event occur almost surely ?",,"['probability', 'statistics', 'random-variables']"
66,Probability of getting a parking ticket,Probability of getting a parking ticket,,"The city of Ithaca, New York, allows for two-hour parking in all downtown spaces. Methodical parking officials patrol the downtown area, passing the same point every two hours. When an official encounters a car, he marks it with chalk. If the car is still there two hours later, a ticket is written. Suppose that you park your car for a random amount of time that is uniformly distributed on (0, 4) hours. What is the probability you will get a ticket? (from Rick Durett) When I park my car somewhere, it takes $2h-t_1$, until the official marks my car. ($t_1$ is the time, that has passed, since the official was the last time at this place.) I suppose, that $t_1$ is uniformly distributed on the interval [0,2]. You get a ticket, if your parking time $t_2$ is longer than $4h-t_1$, so I have to compute the probability $\mathbb{P}(t_2 \geq 4h-t_1)$. This should be $\mathbb{P}(t_1+t_2 \geq 4h) = 1-F(4h)$. F is 0 if $x \notin [0,6]$, $x/8$, if $x \in [0,2]$, 1/4 if $x \in [2,4]$ and $6/8-x/8$, if $x \in [4,6]$. So I get $\mathbb{P}(t_1+t_2 \geq 4h) = 1-1/4 = 3/4$ and this probability seems too much for me, so something must be missing. Where is my mistake? Thank you! :)","The city of Ithaca, New York, allows for two-hour parking in all downtown spaces. Methodical parking officials patrol the downtown area, passing the same point every two hours. When an official encounters a car, he marks it with chalk. If the car is still there two hours later, a ticket is written. Suppose that you park your car for a random amount of time that is uniformly distributed on (0, 4) hours. What is the probability you will get a ticket? (from Rick Durett) When I park my car somewhere, it takes $2h-t_1$, until the official marks my car. ($t_1$ is the time, that has passed, since the official was the last time at this place.) I suppose, that $t_1$ is uniformly distributed on the interval [0,2]. You get a ticket, if your parking time $t_2$ is longer than $4h-t_1$, so I have to compute the probability $\mathbb{P}(t_2 \geq 4h-t_1)$. This should be $\mathbb{P}(t_1+t_2 \geq 4h) = 1-F(4h)$. F is 0 if $x \notin [0,6]$, $x/8$, if $x \in [0,2]$, 1/4 if $x \in [2,4]$ and $6/8-x/8$, if $x \in [4,6]$. So I get $\mathbb{P}(t_1+t_2 \geq 4h) = 1-1/4 = 3/4$ and this probability seems too much for me, so something must be missing. Where is my mistake? Thank you! :)",,['statistics']
67,Can you select random entry from unknown number of entries?,Can you select random entry from unknown number of entries?,,"Imagine you've got entries coming in, without knowing when they end (how many will follow from now on). You're supposed to pick random one and be fair. You can't save the entries that passed but you can remember how many passed. At this point, if you imagine it, it seems impossible. At the point of choosing, you don't know what should be item's chance to be chosen. But maybe there is some trick. And if not we'll have to prove that it's impossible - which I expect to be part of negative answer.","Imagine you've got entries coming in, without knowing when they end (how many will follow from now on). You're supposed to pick random one and be fair. You can't save the entries that passed but you can remember how many passed. At this point, if you imagine it, it seems impossible. At the point of choosing, you don't know what should be item's chance to be chosen. But maybe there is some trick. And if not we'll have to prove that it's impossible - which I expect to be part of negative answer.",,"['statistics', 'random']"
68,"Normal distribution problem - ""6 times the standard deviation""","Normal distribution problem - ""6 times the standard deviation""",,"An old textbook says the range of data can be estimated as 6 times the standard deviation. If the data is normally distributed what percentage of the data is within the range? By 'range of data', does the question mean biggest - smallest? In that case, how do I write an expression for it if it's normally distributed?","An old textbook says the range of data can be estimated as 6 times the standard deviation. If the data is normally distributed what percentage of the data is within the range? By 'range of data', does the question mean biggest - smallest? In that case, how do I write an expression for it if it's normally distributed?",,['statistics']
69,Maximum Likelihood Estimation Question,Maximum Likelihood Estimation Question,,"Consider the probability density function $$f(x)=\frac{1}{\theta^2}xe^{-x/\theta},\;\;\;\;\; 0\le x\lt\infty,\;\;0\lt\theta\lt\infty$$ Find the maximum likelihood estimator for $\theta$ . I'm really struggling with this question. From my understanding in order to find the maximum likelihood estimator for $\theta$ , the function needs to be partially differentiated with respect to $\theta$ , equated to zero, and solved for $\theta$ ; however for this question the differentiation is very messy and even more difficult, is solving the derivative for $\theta$ .","Consider the probability density function Find the maximum likelihood estimator for . I'm really struggling with this question. From my understanding in order to find the maximum likelihood estimator for , the function needs to be partially differentiated with respect to , equated to zero, and solved for ; however for this question the differentiation is very messy and even more difficult, is solving the derivative for .","f(x)=\frac{1}{\theta^2}xe^{-x/\theta},\;\;\;\;\; 0\le x\lt\infty,\;\;0\lt\theta\lt\infty \theta \theta \theta \theta \theta","['statistics', 'optimization', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation']"
70,Probability You Choose at least one chip of every color,Probability You Choose at least one chip of every color,,"There are 16 chips: 6 red, 7 white, and 3 blue. 4 chips are selected randomly and are not replaced once selected. What is the probability that at least one chip of every color is selected? I'm not every good at figuring out how to create an equation of probability for these ""at least one of each kind"" problems. Thanks for your help and kindness in advance. It is much appreciated.","There are 16 chips: 6 red, 7 white, and 3 blue. 4 chips are selected randomly and are not replaced once selected. What is the probability that at least one chip of every color is selected? I'm not every good at figuring out how to create an equation of probability for these ""at least one of each kind"" problems. Thanks for your help and kindness in advance. It is much appreciated.",,"['probability', 'combinatorics', 'statistics']"
71,"""Practical"" Claim about Hypothesis Testing of Bernoulli Distribution Parameter","""Practical"" Claim about Hypothesis Testing of Bernoulli Distribution Parameter",,"First, let me state the original problem (in my own wording): Describe the decision procedure for testing the hypothesis about the parameter $p$ (success rate) of a Bernoulli distribution. The hypotheses are \begin{gather} H_0: p = p_0 \\ H_1: p \ne p_0 \end{gather} where $p_0$ is a fixed number. If $Y = \sum_{i=1}^n X_i$, where $X_i \sim \text{Bernoulli}(p)$ are i.i.d., is available, describe the decision procedure based on $Y$ that will guarantee that the probability of type 1 error does not exceed $\alpha$. (Assume $n$ is large.) The first solution without the assumption that $n$ is large: Find two critical values $Y_{lower}$ and $Y_{upper}$ such that $P(Y_{lower} < U < Y_{upper}) = 1 - \alpha$, where $U \sim \text{Binomial}(p_0, n)$. (There are many possible choices.) We accept $H_0$ if $Y_{lower} < Y < Y_{upper}$. Then if $n$ is assumed large and we are allowed to approximate the distribution of $Y$ with a normal distribution, the method simplifies to Find two critical values $Z_{lower}$ and $Z_{upper}$ such that $P(Z_{lower} < U < Z_{upper}) = 1 - \alpha$ where $U \sim \mathcal N(0, 1)$. We accept $H_0$ if $Z_{lower} < Z < Z_{upper}$, where $Z = \sqrt{n}\frac{(Y/n) - p_0}{\sqrt{p_0(1-p_0)}}$. This simplifies the problem slightly if we take the symmetric interval around the mean, i.e., $Z_{lower} = -Z_{upper}$. Here comes my question: I was told by a teacher that in practice , some people use $$ \tilde Z = \sqrt n \frac{(Y/n) - p_0}{\sqrt{(Y/n)(1 - (Y/n))}} $$ instead of $Z$. In other words, $p_0$ in the denominator of $Z = \sqrt{n}\frac{(Y/n) - p_0}{\sqrt{p_0(1-p_0)}}$ is replaced by $\hat p = \frac{Y}{n}$. His explanation was that $\sqrt{p_0(1-p_0)}$ might not be representative of the true variance, and the sample variance $\sqrt{\hat p(1 - \hat p)}$ may be better. I don't think this reasoning is valid because we are testing the hypothesis! However, after some reflection, I am starting to think that using $\tilde Z$ might not be such a wrong thing because we did employ the central limit approximation, and using $\tilde Z$ might correct the approximation in a proper way. Of course this would somewhat contradict the assumption that the problem allows you to use normal approximation, but is this kind of correction (if it's really a correction) valid to some degree? Why I think it might be correction in the right direction: Suppose $\frac 12 < \hat p < p_0$. Then $\sqrt{\hat p(1 - \hat p)} > \sqrt{p_0(1 - p_0)}$, so $Z < \tilde Z < 0$, making it more probable to accept $H_0$ using $\tilde Z$ than $Z$. On the other hand, if $\frac 12 < p_0 < \hat p$, it becomes less probable to accept $H_0$ using $\tilde Z$.","First, let me state the original problem (in my own wording): Describe the decision procedure for testing the hypothesis about the parameter $p$ (success rate) of a Bernoulli distribution. The hypotheses are \begin{gather} H_0: p = p_0 \\ H_1: p \ne p_0 \end{gather} where $p_0$ is a fixed number. If $Y = \sum_{i=1}^n X_i$, where $X_i \sim \text{Bernoulli}(p)$ are i.i.d., is available, describe the decision procedure based on $Y$ that will guarantee that the probability of type 1 error does not exceed $\alpha$. (Assume $n$ is large.) The first solution without the assumption that $n$ is large: Find two critical values $Y_{lower}$ and $Y_{upper}$ such that $P(Y_{lower} < U < Y_{upper}) = 1 - \alpha$, where $U \sim \text{Binomial}(p_0, n)$. (There are many possible choices.) We accept $H_0$ if $Y_{lower} < Y < Y_{upper}$. Then if $n$ is assumed large and we are allowed to approximate the distribution of $Y$ with a normal distribution, the method simplifies to Find two critical values $Z_{lower}$ and $Z_{upper}$ such that $P(Z_{lower} < U < Z_{upper}) = 1 - \alpha$ where $U \sim \mathcal N(0, 1)$. We accept $H_0$ if $Z_{lower} < Z < Z_{upper}$, where $Z = \sqrt{n}\frac{(Y/n) - p_0}{\sqrt{p_0(1-p_0)}}$. This simplifies the problem slightly if we take the symmetric interval around the mean, i.e., $Z_{lower} = -Z_{upper}$. Here comes my question: I was told by a teacher that in practice , some people use $$ \tilde Z = \sqrt n \frac{(Y/n) - p_0}{\sqrt{(Y/n)(1 - (Y/n))}} $$ instead of $Z$. In other words, $p_0$ in the denominator of $Z = \sqrt{n}\frac{(Y/n) - p_0}{\sqrt{p_0(1-p_0)}}$ is replaced by $\hat p = \frac{Y}{n}$. His explanation was that $\sqrt{p_0(1-p_0)}$ might not be representative of the true variance, and the sample variance $\sqrt{\hat p(1 - \hat p)}$ may be better. I don't think this reasoning is valid because we are testing the hypothesis! However, after some reflection, I am starting to think that using $\tilde Z$ might not be such a wrong thing because we did employ the central limit approximation, and using $\tilde Z$ might correct the approximation in a proper way. Of course this would somewhat contradict the assumption that the problem allows you to use normal approximation, but is this kind of correction (if it's really a correction) valid to some degree? Why I think it might be correction in the right direction: Suppose $\frac 12 < \hat p < p_0$. Then $\sqrt{\hat p(1 - \hat p)} > \sqrt{p_0(1 - p_0)}$, so $Z < \tilde Z < 0$, making it more probable to accept $H_0$ using $\tilde Z$ than $Z$. On the other hand, if $\frac 12 < p_0 < \hat p$, it becomes less probable to accept $H_0$ using $\tilde Z$.",,"['probability', 'statistics', 'soft-question', 'hypothesis-testing', 'central-limit-theorem']"
72,unbiased estimator in a random sample,unbiased estimator in a random sample,,"I Have a statistic statement here which I need to decide if it's true or false Statement:  ""When the sample size is random, there is no way to get an unbiased estimator for the population average."" I think that the statement above is false since the Horvitz–Thompson estimator may be a counter example, but I'm not really sure. Thanks in advance,","I Have a statistic statement here which I need to decide if it's true or false Statement:  ""When the sample size is random, there is no way to get an unbiased estimator for the population average."" I think that the statement above is false since the Horvitz–Thompson estimator may be a counter example, but I'm not really sure. Thanks in advance,",,"['statistics', 'estimation', 'sampling']"
73,How to calculate historical data knowing only yearly percentage increase and data for year 2015?,How to calculate historical data knowing only yearly percentage increase and data for year 2015?,,"How do I calculate historical yearly market worth knowing only its yearly growth rate and its martket worth for year 2015? Example: According to MarketLine, the global home improvement market is   expected to grow at 2.5% yearly rate between 2010-2015 and is   predicted to be worth almost $678 billion by 2015 I need to find X for each year since 2010 until 2014. I have yearly 2.5% percentage increase and 678,000,000,000 for year 2015. How do I calculate historical worth for respective years?","How do I calculate historical yearly market worth knowing only its yearly growth rate and its martket worth for year 2015? Example: According to MarketLine, the global home improvement market is   expected to grow at 2.5% yearly rate between 2010-2015 and is   predicted to be worth almost $678 billion by 2015 I need to find X for each year since 2010 until 2014. I have yearly 2.5% percentage increase and 678,000,000,000 for year 2015. How do I calculate historical worth for respective years?",,"['statistics', 'percentages']"
74,combinatoric question - balls arrangement,combinatoric question - balls arrangement,,"In how many ways can you arrange 200 balls into 40 cells, such that the sum of balls in cells 1-20 is greater (not equal) the sum of balls in cells 21-40? So, the number of posibilities can be described by: $$\sum\limits_{k = 0}^{99} {\left( {\begin{array}{*{20}{c}}    {k + 20 - 1}  \\    {20 - 1}  \\ \end{array}} \right)}$$ I was aked not to use $\sum$. Is there another form to express it?","In how many ways can you arrange 200 balls into 40 cells, such that the sum of balls in cells 1-20 is greater (not equal) the sum of balls in cells 21-40? So, the number of posibilities can be described by: $$\sum\limits_{k = 0}^{99} {\left( {\begin{array}{*{20}{c}}    {k + 20 - 1}  \\    {20 - 1}  \\ \end{array}} \right)}$$ I was aked not to use $\sum$. Is there another form to express it?",,"['combinatorics', 'statistics', 'discrete-mathematics']"
75,Approximating Distribution of a Data Set,Approximating Distribution of a Data Set,,"If I have a set of data point and I want to approximate the distribution of that data set. What methods can be employed to fit the data set with the best most distribution. Whether it be gamma, normal, log normal, exponential, etc. I am trying to find the best distribution and the parameters that optimizes the best fit. What methods are out there to do so? Here is the data, I am trying to approximate. With a distribution. I generated the data by running a 3,466 binary simulations (1,0) and summing the number of 1's in each simulation. According to probability theory, the sum of the outcomes of a Bernoulli distribution is  a binomial. But for the sake of being ignorant, if I didn't know this was binomial, how could I build a function that approximates the data. My end goal is to build an excel function that draws on the inverse of the density function and spits out a random number from the distribution. x   #occurance  P(x) 1636    1   0% 1646    2   0% 1656    2   0% 1666    6   1% 1676    13  2% 1686    20  2% 1696    44  5% 1706    61  7% 1716    79  10% 1726    115 14% 1736    120 14% 1746    97  12% 1756    88  11% 1766    81  10% 1776    48  6% 1786    31  4% 1796    13  2% 1806    7   1% 1816    3   0% 1826    0   0%","If I have a set of data point and I want to approximate the distribution of that data set. What methods can be employed to fit the data set with the best most distribution. Whether it be gamma, normal, log normal, exponential, etc. I am trying to find the best distribution and the parameters that optimizes the best fit. What methods are out there to do so? Here is the data, I am trying to approximate. With a distribution. I generated the data by running a 3,466 binary simulations (1,0) and summing the number of 1's in each simulation. According to probability theory, the sum of the outcomes of a Bernoulli distribution is  a binomial. But for the sake of being ignorant, if I didn't know this was binomial, how could I build a function that approximates the data. My end goal is to build an excel function that draws on the inverse of the density function and spits out a random number from the distribution. x   #occurance  P(x) 1636    1   0% 1646    2   0% 1656    2   0% 1666    6   1% 1676    13  2% 1686    20  2% 1696    44  5% 1706    61  7% 1716    79  10% 1726    115 14% 1736    120 14% 1746    97  12% 1756    88  11% 1766    81  10% 1776    48  6% 1786    31  4% 1796    13  2% 1806    7   1% 1816    3   0% 1826    0   0%",,['statistics']
76,"Suggestions for the optimal estimator in ""one-shot"" prediction problems?","Suggestions for the optimal estimator in ""one-shot"" prediction problems?",,"Assume you have a prediction distribution for a quantity. What point on this distribution should you use if the process you are predicting will end after the next observation and you want to be within $\epsilon$ units of that value? I have seen the mean touted as an optimal predictor, but that only makes sense if you want to minimize your expected mean square error (and assuming stationarity of the process being predicted!). I've considered two other obvious options: the median and the mode. In principle, it seems that the mode is ideal since the mode  will maximize the probability that the observation will be within the desired range if the underlying distribution is unimodal (i.e., P($X\;\in\;(a-\epsilon,\;a+\epsilon))$ is maximized if a=mode). Specifically, I am trying to find the optimal predictor for the following problem: Let $Y =\{X_{1},X_{2}...X_{N}\}$ be a set of $N$ random quantities with assocated set of distributions $F=\{F_{1},F_{2}...F_{N}\}$ and acceptable absolute errors $\epsilon = \{\epsilon_{1},\epsilon_{2}...\epsilon_{N}\}$. You are to develop a single-point prediction set $\hat X =\{\hat X_{1},\hat X_{2}...\hat X_{N}\}$ which contains one point from the possible range of each random quantity such that you maximize  $P(\sum\limits_{i=1}^{N}\textbf{1}_{|X_{i}-\hat X_{i}|\leq \epsilon_{i}}= N)$. Anyone have experience with these types of predictive situations and can share insights or good heuristics? Thanks :)","Assume you have a prediction distribution for a quantity. What point on this distribution should you use if the process you are predicting will end after the next observation and you want to be within $\epsilon$ units of that value? I have seen the mean touted as an optimal predictor, but that only makes sense if you want to minimize your expected mean square error (and assuming stationarity of the process being predicted!). I've considered two other obvious options: the median and the mode. In principle, it seems that the mode is ideal since the mode  will maximize the probability that the observation will be within the desired range if the underlying distribution is unimodal (i.e., P($X\;\in\;(a-\epsilon,\;a+\epsilon))$ is maximized if a=mode). Specifically, I am trying to find the optimal predictor for the following problem: Let $Y =\{X_{1},X_{2}...X_{N}\}$ be a set of $N$ random quantities with assocated set of distributions $F=\{F_{1},F_{2}...F_{N}\}$ and acceptable absolute errors $\epsilon = \{\epsilon_{1},\epsilon_{2}...\epsilon_{N}\}$. You are to develop a single-point prediction set $\hat X =\{\hat X_{1},\hat X_{2}...\hat X_{N}\}$ which contains one point from the possible range of each random quantity such that you maximize  $P(\sum\limits_{i=1}^{N}\textbf{1}_{|X_{i}-\hat X_{i}|\leq \epsilon_{i}}= N)$. Anyone have experience with these types of predictive situations and can share insights or good heuristics? Thanks :)",,['probability']
77,Means and Variances,Means and Variances,,"For a laboratory assignment, if the equipment is working, the density function of the observed outcome $X$ is $$ f(x) = \begin{cases} 2(1-x), & 0 <x<1, \\ 0, & \text{otherwise.} \end{cases} $$ Find the variance and standard deviation of $X$. We know that the variance is related to the mean and the second moment. I am stuck on how to set up the integrals for the both of them.","For a laboratory assignment, if the equipment is working, the density function of the observed outcome $X$ is $$ f(x) = \begin{cases} 2(1-x), & 0 <x<1, \\ 0, & \text{otherwise.} \end{cases} $$ Find the variance and standard deviation of $X$. We know that the variance is related to the mean and the second moment. I am stuck on how to set up the integrals for the both of them.",,['statistics']
78,Poisson Estimators,Poisson Estimators,,Consider a simple random sample of size $n$ from a Poisson distribution with mean $\mu$. Let $\theta=P(X=0)$. Let $T=\sum X_{i}$. Show that $\tilde{\theta}=[(n-1)/n]^{T}$ is an unbiased estimator of $\theta$.,Consider a simple random sample of size $n$ from a Poisson distribution with mean $\mu$. Let $\theta=P(X=0)$. Let $T=\sum X_{i}$. Show that $\tilde{\theta}=[(n-1)/n]^{T}$ is an unbiased estimator of $\theta$.,,"['statistics', 'estimation']"
79,Fitting a polynomial + exponential curve of a given form to data,Fitting a polynomial + exponential curve of a given form to data,,"I have got a number of data sets of some parameter $m_x$ against an independent variable $x$. Through each of the data sets I need to best fit a curve of the form $A + Bc^x$ such that $A$, $B$ and $c$ are arbitrary parameters. How can I best do it, preferably with Excel or Mathematica?","I have got a number of data sets of some parameter $m_x$ against an independent variable $x$. Through each of the data sets I need to best fit a curve of the form $A + Bc^x$ such that $A$, $B$ and $c$ are arbitrary parameters. How can I best do it, preferably with Excel or Mathematica?",,"['statistics', 'regression']"
80,"You have 4 prizes, 3 tickets, n tickets- what is the probability of winning","You have 4 prizes, 3 tickets, n tickets- what is the probability of winning",,"You have bought 3 tickets in a lottery. There are n total tickets and 4 prizes. What are the odds of winning at least one prize? I thought of it like this: The total possible ways of extracting 4 prizes is: a= $${n\choose 4}$$ The possibilities of extracting at least 1 winning prize is: b=$${4\choose 1} + {4\choose 2} +{4\choose 3}$$ so the probability of winning is $$\frac{{4\choose 1} + {4\choose 2} +{4\choose 3}}{{n\choose 4}}$$ If I'm wrong, which might be the case, please tell me what the right solution is, as this is the only I could come up with. Thank you","You have bought 3 tickets in a lottery. There are n total tickets and 4 prizes. What are the odds of winning at least one prize? I thought of it like this: The total possible ways of extracting 4 prizes is: a= $${n\choose 4}$$ The possibilities of extracting at least 1 winning prize is: b=$${4\choose 1} + {4\choose 2} +{4\choose 3}$$ so the probability of winning is $$\frac{{4\choose 1} + {4\choose 2} +{4\choose 3}}{{n\choose 4}}$$ If I'm wrong, which might be the case, please tell me what the right solution is, as this is the only I could come up with. Thank you",,['statistics']
81,Is $E(xy) \geq E(x) E(y)$?,Is ?,E(xy) \geq E(x) E(y),"Is $E(xy) \geq E(x) E(y)$ always?  Where $E$ means expectation. Intuitively I feel like covariance should always be nonnegative, but I cannot prove why. Thanks for your help","Is $E(xy) \geq E(x) E(y)$ always?  Where $E$ means expectation. Intuitively I feel like covariance should always be nonnegative, but I cannot prove why. Thanks for your help",,['statistics']
82,how to compute unbiased estimator,how to compute unbiased estimator,,"Suppose $X_1,X_2,...,X_n$ is a random sample of size n drawn from a Poisson pdf where $\lambda$ is an unknown parameter. Show that $\hat \lambda = \bar X$ is unbiased for $\lambda$. For what type of parameter, in general, will the sample mean necessarily be an unbiased estimator? (Hint: The answer is implicit in the derivation showing that $\bar X$ is unbiased for the Poisson $\lambda$) $$E(\bar X) = E(\frac{1}{n} \sum_{i=1}^n X_i)= \frac{1}{n} \sum_{i=1}^nE(X_i)=\frac{1}{n}\sum_{i=1}^n\lambda =\lambda$$ im totally confused by this idea of unbiasedness? what is it? and how do you compute it in the general sense, whats the first step, second step ...","Suppose $X_1,X_2,...,X_n$ is a random sample of size n drawn from a Poisson pdf where $\lambda$ is an unknown parameter. Show that $\hat \lambda = \bar X$ is unbiased for $\lambda$. For what type of parameter, in general, will the sample mean necessarily be an unbiased estimator? (Hint: The answer is implicit in the derivation showing that $\bar X$ is unbiased for the Poisson $\lambda$) $$E(\bar X) = E(\frac{1}{n} \sum_{i=1}^n X_i)= \frac{1}{n} \sum_{i=1}^nE(X_i)=\frac{1}{n}\sum_{i=1}^n\lambda =\lambda$$ im totally confused by this idea of unbiasedness? what is it? and how do you compute it in the general sense, whats the first step, second step ...",,['statistics']
83,How do I prove Poisson appraches Normal distribution,How do I prove Poisson appraches Normal distribution,,"I want to prove why the mean and variance of a $\operatorname{Poisson}(\lambda)$, is different when the time index approaches infinite (it's approximated by the mean and variance of a Normal). For example:  $$ N_k = N_1 + (N_2 - N_1) + (N_3 - N_2) + ... + (N_k - N_{k-1}) $$ Using CLT: $\frac{N_k - k\lambda}{\sqrt{k\lambda}}$ is normally distributed (in the limit). I want to answer why is that a Poisson R.V. characterized by $\mathbb{E}[X] = \lambda$ and $\operatorname{Var}[X] = \lambda$. Because when it approaches a normal distribution, $\mathbb{E}[Z] = \mu$ and $\operatorname{Var}[Z] = \sigma^2$.","I want to prove why the mean and variance of a $\operatorname{Poisson}(\lambda)$, is different when the time index approaches infinite (it's approximated by the mean and variance of a Normal). For example:  $$ N_k = N_1 + (N_2 - N_1) + (N_3 - N_2) + ... + (N_k - N_{k-1}) $$ Using CLT: $\frac{N_k - k\lambda}{\sqrt{k\lambda}}$ is normally distributed (in the limit). I want to answer why is that a Poisson R.V. characterized by $\mathbb{E}[X] = \lambda$ and $\operatorname{Var}[X] = \lambda$. Because when it approaches a normal distribution, $\mathbb{E}[Z] = \mu$ and $\operatorname{Var}[Z] = \sigma^2$.",,"['probability', 'statistics', 'normal-distribution', 'probability-distributions']"
84,Coinflip statistics,Coinflip statistics,,"There is a betting strategy for coinflips, called Martingale, in which the player should pick heads or tails and keep betting on it (doubling their bet on every loss) to eventually make a win when it appears. In theory this is watertight, but with one caveat: there can be very long strings of losses, and the player runs out of money, and looses it all. Now, some people wait until they have seen a very long string of outcomes for one side, and start betting at the other side, with the idea that has a larger chance now. I know that mathematicly this is nonsense, both sides still have 50% chance no matter what happend before. Although I can understand a dice/coin/ball has no notion of past/future, so will always remain 50% chance, I never quite understanded the following. You can proof there is only a very slight possibility of me ever witnessing 50 heads in a row during my lifetime. So after 49 spins, I will either observe something extraordinary (50 heads) or something less special (first tail), how can that not effect chance? Maybe the answer is obvious, that for math/statistics it doesnt matter whether someone is observing the event or not, but please enlighten me.","There is a betting strategy for coinflips, called Martingale, in which the player should pick heads or tails and keep betting on it (doubling their bet on every loss) to eventually make a win when it appears. In theory this is watertight, but with one caveat: there can be very long strings of losses, and the player runs out of money, and looses it all. Now, some people wait until they have seen a very long string of outcomes for one side, and start betting at the other side, with the idea that has a larger chance now. I know that mathematicly this is nonsense, both sides still have 50% chance no matter what happend before. Although I can understand a dice/coin/ball has no notion of past/future, so will always remain 50% chance, I never quite understanded the following. You can proof there is only a very slight possibility of me ever witnessing 50 heads in a row during my lifetime. So after 49 spins, I will either observe something extraordinary (50 heads) or something less special (first tail), how can that not effect chance? Maybe the answer is obvious, that for math/statistics it doesnt matter whether someone is observing the event or not, but please enlighten me.",,"['probability', 'statistics', 'dice']"
85,How I can find the expected value of $G$?,How I can find the expected value of ?,G,"Suppose two teams play a series of games, each producing a winner and a loser, until one team   has won two more games than the other. Let $G$ be the total number of games played. Assuming   each team has chance $0.5$ to win each game, independent of results of the previous games. Find the expected value of $G$. I think $G$ takes even values and I need to use negative binomial.But I am unable to find probability distribution and expectation. Please help.","Suppose two teams play a series of games, each producing a winner and a loser, until one team   has won two more games than the other. Let $G$ be the total number of games played. Assuming   each team has chance $0.5$ to win each game, independent of results of the previous games. Find the expected value of $G$. I think $G$ takes even values and I need to use negative binomial.But I am unable to find probability distribution and expectation. Please help.",,"['statistics', 'probability-distributions']"
86,How do I calculate the cdf over this range? I'm very confused.,How do I calculate the cdf over this range? I'm very confused.,,"I don't want the answer, I just want an explanation. Say $F(x) = 0.25 x^2$ for $0 \leq x < 2$. If $F(x) = 1$ for $x \geq 2$ and $F(x) = 0$ everywhere else, how would I calculate $P(X \leq 1.5)$?","I don't want the answer, I just want an explanation. Say $F(x) = 0.25 x^2$ for $0 \leq x < 2$. If $F(x) = 1$ for $x \geq 2$ and $F(x) = 0$ everywhere else, how would I calculate $P(X \leq 1.5)$?",,"['probability', 'statistics']"
87,Statistics Probability Help,Statistics Probability Help,,"I just began to take this stats course in HS and I'm a little stuck on these 2 problems below. Can anybody please help me out with the solutions? Thank you. Anything is appreciated. Let $Y$ be a random variable. Define a function $Q$ by $Q(m) = E[(Y − m)^{2}]$ (a) Write $\mu E[Y]$. How do you show that $Q(m) = \text{var}[Y] + (m − \mu)^{2}$. I was thinking it had something to do with $Y −m=(Y −\mu)−(m−\mu)$. (b) Also, how do you show that $Q(m)$ is minimized at $m = E[Y]$. I was thinking that it had something to do with checking the sign of $Q(m) − Q(\mu)$. Let $Y$ be a random variable distributed with $N(3, 16)$. Find $Pr[Y \gt 9.2]$.","I just began to take this stats course in HS and I'm a little stuck on these 2 problems below. Can anybody please help me out with the solutions? Thank you. Anything is appreciated. Let $Y$ be a random variable. Define a function $Q$ by $Q(m) = E[(Y − m)^{2}]$ (a) Write $\mu E[Y]$. How do you show that $Q(m) = \text{var}[Y] + (m − \mu)^{2}$. I was thinking it had something to do with $Y −m=(Y −\mu)−(m−\mu)$. (b) Also, how do you show that $Q(m)$ is minimized at $m = E[Y]$. I was thinking that it had something to do with checking the sign of $Q(m) − Q(\mu)$. Let $Y$ be a random variable distributed with $N(3, 16)$. Find $Pr[Y \gt 9.2]$.",,"['probability', 'statistics', 'random']"
88,"How can I tell if dice are biased or unbiased, given a number of trials?","How can I tell if dice are biased or unbiased, given a number of trials?",,"If I'm given the outcome of a number of dice rolls (say, 5 twos, 8 threes, etc), is there a way to assign a probability that the dice are biased or unbiased?  If so, how? Or alternatively, how can I say with a given level of confidence that the dice are biased or unbiased?  I'm not even sure if these are the same question.","If I'm given the outcome of a number of dice rolls (say, 5 twos, 8 threes, etc), is there a way to assign a probability that the dice are biased or unbiased?  If so, how? Or alternatively, how can I say with a given level of confidence that the dice are biased or unbiased?  I'm not even sure if these are the same question.",,['statistics']
89,Finding expected value of $X^2$,Finding expected value of,X^2,"The question is: $X$ is a random variable, and $f(x) = (x-1)/2$ for $1 \le x \le 3$ Find $\Bbb E (X^2)$ Here's my solution: \begin{align} \Bbb P(1)&= 0/2= 0 \\ \Bbb P(2)&= 1/2  \\ \Bbb P(3)&= 2/2= 1 \\ \end{align} \begin{align} \Bbb E(X^2) = & 1^2 \Bbb P(1) + 2^2 \Bbb P(2) + 3^2 \Bbb P(3) \\  = & 1 \cdot 0 + 4 \cdot 1/2 + 9 \cdot 1  \\ = & 11 \end{align} This is my solution, but it is wrong. I need help in understanding where my mistake is. Thanks for your help!","The question is: $X$ is a random variable, and $f(x) = (x-1)/2$ for $1 \le x \le 3$ Find $\Bbb E (X^2)$ Here's my solution: \begin{align} \Bbb P(1)&= 0/2= 0 \\ \Bbb P(2)&= 1/2  \\ \Bbb P(3)&= 2/2= 1 \\ \end{align} \begin{align} \Bbb E(X^2) = & 1^2 \Bbb P(1) + 2^2 \Bbb P(2) + 3^2 \Bbb P(3) \\  = & 1 \cdot 0 + 4 \cdot 1/2 + 9 \cdot 1  \\ = & 11 \end{align} This is my solution, but it is wrong. I need help in understanding where my mistake is. Thanks for your help!",,"['probability', 'statistics']"
90,Show that estimates are unbiased,Show that estimates are unbiased,,"The following is a problem in my book that I don't really understand: We take a random sample: $x_1,x_2,\ldots,x_n$ from a population that is $N(μ,σ)$ where $\mu$ and $\sigma$ are unknown. We build two estimates: $$\mu^*_{\text{obs}} = \overline{x} = (x_1 + x_2 + \cdots + x_n)/n$$ and $$\hat{\mu}^*_{\text{obs}} = (x_1+x_2)/2$$ Show that both estimates are unbiased. I know that an estimate of a sample mean is unbiased when we divide by $n-1$ instead of $n$. How come those two estimates are unbiased? In my eyes they are biased.","The following is a problem in my book that I don't really understand: We take a random sample: $x_1,x_2,\ldots,x_n$ from a population that is $N(μ,σ)$ where $\mu$ and $\sigma$ are unknown. We build two estimates: $$\mu^*_{\text{obs}} = \overline{x} = (x_1 + x_2 + \cdots + x_n)/n$$ and $$\hat{\mu}^*_{\text{obs}} = (x_1+x_2)/2$$ Show that both estimates are unbiased. I know that an estimate of a sample mean is unbiased when we divide by $n-1$ instead of $n$. How come those two estimates are unbiased? In my eyes they are biased.",,"['statistics', 'parameter-estimation']"
91,Probability of character series in random word,Probability of character series in random word,,"I have a word with 13 characters. Each character is random and alphanumerical (a-z, 0-9). So for each character there are 36 possibilities: 10 numbers and 26 letters. I am trying to find out the probabilities for different amounts of segments. I define a segment to be a series of numbers or letters without an element from the other group. Example: abcdefg123456 = 2 Segments abcdefg123abc = 3 Segments abcdefg123ab1 = 4 Segments, etc. Now I am looking for the probabilities of having 1 segment, 2 segments, ..., 13 segments but I can't figure it out. Any help is greatly appreciated.","I have a word with 13 characters. Each character is random and alphanumerical (a-z, 0-9). So for each character there are 36 possibilities: 10 numbers and 26 letters. I am trying to find out the probabilities for different amounts of segments. I define a segment to be a series of numbers or letters without an element from the other group. Example: abcdefg123456 = 2 Segments abcdefg123abc = 3 Segments abcdefg123ab1 = 4 Segments, etc. Now I am looking for the probabilities of having 1 segment, 2 segments, ..., 13 segments but I can't figure it out. Any help is greatly appreciated.",,"['probability', 'statistics']"
92,"What is the difference between empirical distribution , classical probability and axiomatic definition","What is the difference between empirical distribution , classical probability and axiomatic definition",,"Can you tell me what is the difference between empirical distribution and classical probability ? My teacher has told me that when we take limit empirical distribution will get a constant value $$P(A)=\lim_{N\rightarrow\infty}f(A)=\lim_{N\rightarrow \infty}\frac{N(A)}{N}= \mathrm{constant}$$ where $F(A)$ is the frequency ratio, $N(A)$ is the number of  times Event $A$ is found to occur and $N$ is the number of times random experiment repeated But classical probability will give $$P(A)=\lim_{n\rightarrow \infty }\frac{m}{n}= 0$$ but what I know of limit is like this $$\lim_{x\rightarrow\infty}\frac{1}{x}=0$$ Then how come empirical distribution is giving a constant instead of zero? And last can you explain what and why we use axiomatic definition ? Advance thanks for your help... I am a newb to probability statistics","Can you tell me what is the difference between empirical distribution and classical probability ? My teacher has told me that when we take limit empirical distribution will get a constant value $$P(A)=\lim_{N\rightarrow\infty}f(A)=\lim_{N\rightarrow \infty}\frac{N(A)}{N}= \mathrm{constant}$$ where $F(A)$ is the frequency ratio, $N(A)$ is the number of  times Event $A$ is found to occur and $N$ is the number of times random experiment repeated But classical probability will give $$P(A)=\lim_{n\rightarrow \infty }\frac{m}{n}= 0$$ but what I know of limit is like this $$\lim_{x\rightarrow\infty}\frac{1}{x}=0$$ Then how come empirical distribution is giving a constant instead of zero? And last can you explain what and why we use axiomatic definition ? Advance thanks for your help... I am a newb to probability statistics",,"['probability', 'statistics', 'probability-distributions']"
93,"Finding arithmetic mean, standard deviation, mode and median","Finding arithmetic mean, standard deviation, mode and median",,On the market quality of the fruit was measured and following results came out: Quality of Fruit( in measuring units ) 65 70 75 80 85 90 95 100   Number                                  2  3  2  5  8 7  5  3 Define: a) arithmetic mean and standard deviation b) mode and median How to find what is wanted here?,On the market quality of the fruit was measured and following results came out: Quality of Fruit( in measuring units ) 65 70 75 80 85 90 95 100   Number                                  2  3  2  5  8 7  5  3 Define: a) arithmetic mean and standard deviation b) mode and median How to find what is wanted here?,,"['probability', 'statistics']"
94,Calculating statistic for multiple runs,Calculating statistic for multiple runs,,"I have s imple, general question regarding calculating statistic for N runs of the same experiment. Suppose I would like to calculate mean of values returned by some Test. Each run of the test generates $ \langle x_1 ... x_n \rangle$ , possibly of different length. Let's say the statistic is mean. Which approach would be better and why: Sum all values from M runs, and then divide by number of values for each run calculate average, and then average across all averages I believe one of the above might beunder/overestimating the mean slightly and I don't know which. Thanks for your answers.","I have s imple, general question regarding calculating statistic for N runs of the same experiment. Suppose I would like to calculate mean of values returned by some Test. Each run of the test generates $ \langle x_1 ... x_n \rangle$ , possibly of different length. Let's say the statistic is mean. Which approach would be better and why: Sum all values from M runs, and then divide by number of values for each run calculate average, and then average across all averages I believe one of the above might beunder/overestimating the mean slightly and I don't know which. Thanks for your answers.",,"['statistics', 'average']"
95,"""Flattening"" a 2D Normal Distribution","""Flattening"" a 2D Normal Distribution",,"I would like to model the probability of a point being at a certain place on a 2D grid. The X coordinate of the point varies according to a normal distribution with mean $0$ and standard deviation $\sigma$. The Y coordinate varies according to a normal distribution with the same mean and same standard deviation. I know that the probability of being a certain distance from $(0, 0)$ is the same in all directions. I would therefore like to ""flatten"" my 2 distributions into a single distribution where the random variable is the distance from $(0, 0)$. In other words, if I know the distributions of x and y, what is the distribution of $\sqrt{x^2+y^2}$? Am I right in my intuition that this will also be a normal distribution? (Or, half of a normal distibution, since the distance cannot be below zero). If so, how do I calculate the standard deviation of this distribution? If not, what type of distribution would it be, and what are the pdf and cdf functions of it?","I would like to model the probability of a point being at a certain place on a 2D grid. The X coordinate of the point varies according to a normal distribution with mean $0$ and standard deviation $\sigma$. The Y coordinate varies according to a normal distribution with the same mean and same standard deviation. I know that the probability of being a certain distance from $(0, 0)$ is the same in all directions. I would therefore like to ""flatten"" my 2 distributions into a single distribution where the random variable is the distance from $(0, 0)$. In other words, if I know the distributions of x and y, what is the distribution of $\sqrt{x^2+y^2}$? Am I right in my intuition that this will also be a normal distribution? (Or, half of a normal distibution, since the distance cannot be below zero). If so, how do I calculate the standard deviation of this distribution? If not, what type of distribution would it be, and what are the pdf and cdf functions of it?",,"['statistics', 'probability-distributions', 'normal-distribution']"
96,Constructing correlated random variables,Constructing correlated random variables,,How do I construct two correlated random variables with correlation $\rho$ given two i.i.d normal r.v.? Do I just multiply the correlation matrix by a vector generated with two i.i.d normal variables?,How do I construct two correlated random variables with correlation $\rho$ given two i.i.d normal r.v.? Do I just multiply the correlation matrix by a vector generated with two i.i.d normal variables?,,"['probability', 'statistics', 'normal-distribution']"
97,"Proving independence of two variables in a joint distribution, using cumulative distribution functions","Proving independence of two variables in a joint distribution, using cumulative distribution functions",,"I have two variables $X$ and $Y$ with the following joint probablity density function $$ f (x,y) = \begin{cases} \frac14 (1+xy) & \text{if } |x| < 1, |y| < 1\\ &\\ 0 & \text{otherwise} \end{cases} $$ The problem is to prove that $X$ and $Y$ are not independent, but that $X^2$ and $Y^2$ are. I calculated the marginal density functions of both $X$ and $Y$ and since their product doesn't equal the marginal density function, I proved they are not independent. However, I wasn't sure about the second part and reviewed the solution. In the given solution, independence was not proven by the following. $$ f_{x^2,y^2} (u,v) = f_{x^2} (u) \centerdot f_{y^2} (v) $$ Instead, it was proven that the cumulative distribution functions exhibit this property and that this implies independence. $$ P (X^2 \leq u \cap Y^2 \leq v) = P (X^2 \leq u) \centerdot P (Y^2 \leq v) $$ I couldn't find any reference that said this implied independence. I believed such implication only worked with the density functions. This is also not mentioned on the wikipedia page for cumulative distribution function. So, I'm wondering, is this also a way to prove independence? Can I use this technique when possible?","I have two variables $X$ and $Y$ with the following joint probablity density function $$ f (x,y) = \begin{cases} \frac14 (1+xy) & \text{if } |x| < 1, |y| < 1\\ &\\ 0 & \text{otherwise} \end{cases} $$ The problem is to prove that $X$ and $Y$ are not independent, but that $X^2$ and $Y^2$ are. I calculated the marginal density functions of both $X$ and $Y$ and since their product doesn't equal the marginal density function, I proved they are not independent. However, I wasn't sure about the second part and reviewed the solution. In the given solution, independence was not proven by the following. $$ f_{x^2,y^2} (u,v) = f_{x^2} (u) \centerdot f_{y^2} (v) $$ Instead, it was proven that the cumulative distribution functions exhibit this property and that this implies independence. $$ P (X^2 \leq u \cap Y^2 \leq v) = P (X^2 \leq u) \centerdot P (Y^2 \leq v) $$ I couldn't find any reference that said this implied independence. I believed such implication only worked with the density functions. This is also not mentioned on the wikipedia page for cumulative distribution function. So, I'm wondering, is this also a way to prove independence? Can I use this technique when possible?",,"['statistics', 'probability-distributions']"
98,You throw a fair coin one million times. what is the expected number of strings of 6 heads followed by 6 tails.,You throw a fair coin one million times. what is the expected number of strings of 6 heads followed by 6 tails.,,"I tried to solve this problem, and I used something similar to the probability of the  Bernoulli distribution to find how many tosses you need to obtain a probability of almost one for one occurrence. Then I just divide the number of tosses in the problem statement by the number of tosses I get. Is there a better way to do this? Edit: What I tried is m = $12$ N = $10^6$ Probability of having X = ""m heads followed by m tails"" is $\frac{1}{2^m}$ Probability of X at exactly at position i is $1/2^m*(1-1/2^m)^i$ Probability of X after p tries is $\sum_{i=0}^{p}1/2^m*(1-1/2^m)^i =1-(1-1/2^m)^{p+1}$ Now when p is big enough the probability is 1. And then I can just divide . But it depends what you define as big enough and lead to speculative results. There must be a solution based on statistics but It's been a while and I forgot quite a bit.","I tried to solve this problem, and I used something similar to the probability of the  Bernoulli distribution to find how many tosses you need to obtain a probability of almost one for one occurrence. Then I just divide the number of tosses in the problem statement by the number of tosses I get. Is there a better way to do this? Edit: What I tried is m = $12$ N = $10^6$ Probability of having X = ""m heads followed by m tails"" is $\frac{1}{2^m}$ Probability of X at exactly at position i is $1/2^m*(1-1/2^m)^i$ Probability of X after p tries is $\sum_{i=0}^{p}1/2^m*(1-1/2^m)^i =1-(1-1/2^m)^{p+1}$ Now when p is big enough the probability is 1. And then I can just divide . But it depends what you define as big enough and lead to speculative results. There must be a solution based on statistics but It's been a while and I forgot quite a bit.",,"['probability', 'statistics']"
99,Chi-square distribution,Chi-square distribution,,$X_1$ and $X_2$ are independent random variables. $X_1$ and $Y = X_1 + X_2$ have $\chi^2$ distributions with $r_1$ and $r$ degrees of freedom and $r_1 < r$. How would I show that $X_2$ has a $\chi^2$ distribution with $r - r_1$ degrees of freedom?,$X_1$ and $X_2$ are independent random variables. $X_1$ and $Y = X_1 + X_2$ have $\chi^2$ distributions with $r_1$ and $r$ degrees of freedom and $r_1 < r$. How would I show that $X_2$ has a $\chi^2$ distribution with $r - r_1$ degrees of freedom?,,['statistics']
