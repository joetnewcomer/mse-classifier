,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find volume of the cap of a sphere of radius R with thickness h,Find volume of the cap of a sphere of radius R with thickness h,,"I have to determine the volume and the formula for the volume for this spherical cap of height $h$, and the radius of the sphere is $R$: Two methods:  *I just need help setting up the triple integrals 1) Cylindrical For for this method I am thinking that $\theta$ goes from $0$ to $2 \pi$, $r$ from $0$ to $5$, and $z$ from $R-h$ to $R$ with the integral of $1\text{d}z\text{d}r\text{d}\theta$.  However, I'm not sure if this right?? 2) Spherical For this I know that $\theta$ ranges from $0$ to $2 \pi$, but I cannot figure out the range for $\phi$?? I know its from $0$ to some angle where the cap lies ($R-h$), however, I cannot figure it out.  Same goes for the range for $\rho$, for this I am assuming it would start at $(R-h)\sec\theta$ to what the outer boundary is? Sorry, I'm completely lost. I've been working on this problem and trying to set it up for quite some time and have had no luck, and as a last resort, I am asking on here. To whomever can help me, could you please keep it very detailed? Thank you.","I have to determine the volume and the formula for the volume for this spherical cap of height $h$, and the radius of the sphere is $R$: Two methods:  *I just need help setting up the triple integrals 1) Cylindrical For for this method I am thinking that $\theta$ goes from $0$ to $2 \pi$, $r$ from $0$ to $5$, and $z$ from $R-h$ to $R$ with the integral of $1\text{d}z\text{d}r\text{d}\theta$.  However, I'm not sure if this right?? 2) Spherical For this I know that $\theta$ ranges from $0$ to $2 \pi$, but I cannot figure out the range for $\phi$?? I know its from $0$ to some angle where the cap lies ($R-h$), however, I cannot figure it out.  Same goes for the range for $\rho$, for this I am assuming it would start at $(R-h)\sec\theta$ to what the outer boundary is? Sorry, I'm completely lost. I've been working on this problem and trying to set it up for quite some time and have had no luck, and as a last resort, I am asking on here. To whomever can help me, could you please keep it very detailed? Thank you.",,"['multivariable-calculus', 'volume', 'spherical-coordinates']"
1,"What exactly are the ""higher order terms"" (H.O.T.) in Taylor series expansion in multivariable case?","What exactly are the ""higher order terms"" (H.O.T.) in Taylor series expansion in multivariable case?",,"I know the Taylor series expansion in single variable case: $$ f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2 + \frac{1}{3!}f^{(3)}(x_0)(x-x_0)^3 + \frac{1}{4!}f^{(4)}(x_0)(x-x_0)^4 + \frac{1}{5!}f^{(5)}(x_0)(x-x_0)^5 + \dots $$ Multivariable case is stated as: $$ f(\bar{x}) = f(\bar{x_0}) + \bar\nabla f(\bar{x_0})^T(\bar{x} - \bar{x_0}) + \frac{1}{2}(\bar{x} - \bar{x_0})^T\bar\nabla^2 f(\bar{x_0})(\bar{x} - \bar{x_0}) + H.O.T. $$ I can find the expression above everywhere, but it is not possible to find the open expression for the H.O.T. (at least I wasn't able to find it). No source states the explicit expression for the higher order terms . Do people avoid them for some reason? For example, are those terms too complex or too long to write? So, what are these higher order terms in the multivariable case? Do they involve a $\bar\nabla^3 f(\bar{x_0})$, $\bar\nabla^4 f(\bar{x_0})$, $\bar\nabla^5 f(\bar{x_0})$, ... terms; if yes, how are they defined? Please write a few terms from H.O.T. to make the pattern clear.","I know the Taylor series expansion in single variable case: $$ f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2 + \frac{1}{3!}f^{(3)}(x_0)(x-x_0)^3 + \frac{1}{4!}f^{(4)}(x_0)(x-x_0)^4 + \frac{1}{5!}f^{(5)}(x_0)(x-x_0)^5 + \dots $$ Multivariable case is stated as: $$ f(\bar{x}) = f(\bar{x_0}) + \bar\nabla f(\bar{x_0})^T(\bar{x} - \bar{x_0}) + \frac{1}{2}(\bar{x} - \bar{x_0})^T\bar\nabla^2 f(\bar{x_0})(\bar{x} - \bar{x_0}) + H.O.T. $$ I can find the expression above everywhere, but it is not possible to find the open expression for the H.O.T. (at least I wasn't able to find it). No source states the explicit expression for the higher order terms . Do people avoid them for some reason? For example, are those terms too complex or too long to write? So, what are these higher order terms in the multivariable case? Do they involve a $\bar\nabla^3 f(\bar{x_0})$, $\bar\nabla^4 f(\bar{x_0})$, $\bar\nabla^5 f(\bar{x_0})$, ... terms; if yes, how are they defined? Please write a few terms from H.O.T. to make the pattern clear.",,"['multivariable-calculus', 'taylor-expansion']"
2,How to determine whether domain is an open or a closed region (or both open and closed)?,How to determine whether domain is an open or a closed region (or both open and closed)?,,"Right now I am studying Partial Derivative in my university, but I am confused in its basic topic; Multi-variable Functions More specifically, I am having difficulty in understanding open or closed domain regions in XY Plane and whether they are bounded or not? For Example: Question Given that $f(x, y)=x^2 - y^2$ (a) Find the function's domain (b) Find the function's range (c) Describe function's level curves (d) Find the boundary of the function’s domain (e) Determine if the domain is an open region, a closed region, or neither (f) Decide if the domain is bounded or unbounded Solution (a) Domain: Entire XY Plane (b) Range: $(-\infty, \infty)$ (c) Level Curves: $x^2-y^2=c$ (d) Since the domain is entire plane, there is no boundary (e) Following is my understanding of open and closed domain in XY Plane: A domain (denoted by region R) is said to be closed if the region R contains all boundary points If the region R does not contain any boundary points , then the Domain is said to be open If the region R contains some but not all of the boundary points , then the Domain is said to be both open and closed The answer in book is: both open and closed , but how can we even say that if there is no boundary as answered in part (d)? (f) Following is my understanding of bounded and unbounded domains: If the boundary points form a closed loop, then the domain is said to be bounded, otherwise they are considered unbounded. OR Books Definition : A region in the plane is bounded if it lies inside a disk of finite radius. A region is unbounded if it is not bounded. The answer of this part in book is: Unbounded , and this is because I think (please correct me if I am wrong), since there is no boundary, the region can not be bounded, so by default, the region is unbounded.","Right now I am studying Partial Derivative in my university, but I am confused in its basic topic; Multi-variable Functions More specifically, I am having difficulty in understanding open or closed domain regions in XY Plane and whether they are bounded or not? For Example: Question Given that $f(x, y)=x^2 - y^2$ (a) Find the function's domain (b) Find the function's range (c) Describe function's level curves (d) Find the boundary of the function’s domain (e) Determine if the domain is an open region, a closed region, or neither (f) Decide if the domain is bounded or unbounded Solution (a) Domain: Entire XY Plane (b) Range: $(-\infty, \infty)$ (c) Level Curves: $x^2-y^2=c$ (d) Since the domain is entire plane, there is no boundary (e) Following is my understanding of open and closed domain in XY Plane: A domain (denoted by region R) is said to be closed if the region R contains all boundary points If the region R does not contain any boundary points , then the Domain is said to be open If the region R contains some but not all of the boundary points , then the Domain is said to be both open and closed The answer in book is: both open and closed , but how can we even say that if there is no boundary as answered in part (d)? (f) Following is my understanding of bounded and unbounded domains: If the boundary points form a closed loop, then the domain is said to be bounded, otherwise they are considered unbounded. OR Books Definition : A region in the plane is bounded if it lies inside a disk of finite radius. A region is unbounded if it is not bounded. The answer of this part in book is: Unbounded , and this is because I think (please correct me if I am wrong), since there is no boundary, the region can not be bounded, so by default, the region is unbounded.",,"['functions', 'multivariable-calculus']"
3,Fritz John PDE book chapter 1 exercise: Prove that $u$ vanishes identically if $au_x+bu_y=-u$,Fritz John PDE book chapter 1 exercise: Prove that  vanishes identically if,u au_x+bu_y=-u,"I was trying out this question: Let $u$ be a solution in $C^1$ of the PDE $$ a(x,y)u_x + b(x,y)u_y = -u $$ on the closed unit disc $\Omega$ in the xy-plane. Let $a(x,y)x + b(x,y)y > 0$ on the boundary of $\Omega$. Prove that $u$ vanishes identically. According to the hint, we are supposed to show that $\max_\Omega u \leq 0 $ and $\min_\Omega u \geq 0$. If the maxima/minima occurs in the interior of $\Omega$, since $u_x = u_y = 0$ we get $\max u = \min u = 0 $, and so $u$ vanishes identically. Now, if, say the maxima occurs on the boundary, we take $f(\theta) = u(\cos\theta,\sin\theta)$ and differentiate to get a condition on maxima: $$ f'(\theta) = -u_xy + u_yx = 0 \implies u_x = \lambda x; \, u_y = \lambda y $$ This is where I am stuck. To specify that this is a maxima and not a minima, we need to differentiate $f$ further. But we aren't allowed to do that since $u \in C^1$. Even if I disregard that constraint and differentiate anyway (and use $u_x,\,u_y$ from above), I am stuck in terms containing $\lambda_x,\,\lambda_y$. What am I missing? Or am I doing something completely wrong?","I was trying out this question: Let $u$ be a solution in $C^1$ of the PDE $$ a(x,y)u_x + b(x,y)u_y = -u $$ on the closed unit disc $\Omega$ in the xy-plane. Let $a(x,y)x + b(x,y)y > 0$ on the boundary of $\Omega$. Prove that $u$ vanishes identically. According to the hint, we are supposed to show that $\max_\Omega u \leq 0 $ and $\min_\Omega u \geq 0$. If the maxima/minima occurs in the interior of $\Omega$, since $u_x = u_y = 0$ we get $\max u = \min u = 0 $, and so $u$ vanishes identically. Now, if, say the maxima occurs on the boundary, we take $f(\theta) = u(\cos\theta,\sin\theta)$ and differentiate to get a condition on maxima: $$ f'(\theta) = -u_xy + u_yx = 0 \implies u_x = \lambda x; \, u_y = \lambda y $$ This is where I am stuck. To specify that this is a maxima and not a minima, we need to differentiate $f$ further. But we aren't allowed to do that since $u \in C^1$. Even if I disregard that constraint and differentiate anyway (and use $u_x,\,u_y$ from above), I am stuck in terms containing $\lambda_x,\,\lambda_y$. What am I missing? Or am I doing something completely wrong?",,"['multivariable-calculus', 'derivatives', 'partial-differential-equations']"
4,Relation between exterior (second) derivative $d^2=0$ and second derivative in multi-variable calculus.,Relation between exterior (second) derivative  and second derivative in multi-variable calculus.,d^2=0,"What does an exterior (second) derivative such as in $d^2=0$ have to do with second derivatives as in single- or multi-variable calculus? Is this a correct start: Calculus derivatives are good for Taylor expansions (and thus optimization), and curvature. Exterior derivatives are needed for integration -- and are essential for generalizing the Fundamental Theorem of Calculus (i.e., Stokes theorem).","What does an exterior (second) derivative such as in $d^2=0$ have to do with second derivatives as in single- or multi-variable calculus? Is this a correct start: Calculus derivatives are good for Taylor expansions (and thus optimization), and curvature. Exterior derivatives are needed for integration -- and are essential for generalizing the Fundamental Theorem of Calculus (i.e., Stokes theorem).",,"['multivariable-calculus', 'differential-forms', 'exterior-algebra']"
5,Better Notation for Partial Derivatives,Better Notation for Partial Derivatives,,"I'm constantly seeing questions here where people are confused about the notation $\frac {\partial f}{\partial x}$ or $\frac {\partial f}{\partial x} (x,y)$ or $\frac {\partial f(x,y)}{\partial x}$.  Is there some better notation which exists for partial derivatives?  If not, can anyone suggest one? Problems with this notation: Is the derivative evaluated at the point $(x,y)$ or is that part of the definition of the function?  If it's evaluated at the point, then should we really use $x$ as the name of the first parameter of the function?  If it's just a part of the definition of the function, how do we specify where the function is evaluated?  And if we leave off the $(x,y)$, how do we even know that $x$ is supposed to be the first parameter as opposed to anything else? The same problem that $\frac {df}{dx}$ shares, it looks like division.  But of course $\frac {\partial f}{\partial x}$ is not as simple as division: it's a shorthand for $\lim_{h\to 0} \frac {f(x+h,y)-f(x,y)}{h}$.  This leads students to think that $\frac {\partial f}{\partial g}\frac {\partial g}{\partial x} = \frac {\partial f}{\partial x}$ and other such ""cancellations"" hold.  However, an operator which looks like division isn't entirely a bad thing, in that you get the correct units when doing a physical problem -- obviously this is because the definition does include a division, but only as part of a limit. Kind of a nitpick, but $\frac {\partial^n f}{\partial x^n}$ looks awfully weird to me.  And worse, it could be confusing to some who see the ""denominator"" as $\partial(x^2)$ instead of $(\partial x)^2$. Other notations include $f_x$, $\partial_x f$, and $D_x f$.  I don't like $f_x$ because that looks like an indexed function to me, not like $f$ is being operated on by a differential operator.  $\partial_x f$ and $D_x f$ both look like operators acting on $f$ but don't tell you the units and we get back to ""can we really use $x$ to refer to the first parameter of $f$ outside of the definition of $f$""? Does anyone know of a better notation?  Can anyone come up with one?","I'm constantly seeing questions here where people are confused about the notation $\frac {\partial f}{\partial x}$ or $\frac {\partial f}{\partial x} (x,y)$ or $\frac {\partial f(x,y)}{\partial x}$.  Is there some better notation which exists for partial derivatives?  If not, can anyone suggest one? Problems with this notation: Is the derivative evaluated at the point $(x,y)$ or is that part of the definition of the function?  If it's evaluated at the point, then should we really use $x$ as the name of the first parameter of the function?  If it's just a part of the definition of the function, how do we specify where the function is evaluated?  And if we leave off the $(x,y)$, how do we even know that $x$ is supposed to be the first parameter as opposed to anything else? The same problem that $\frac {df}{dx}$ shares, it looks like division.  But of course $\frac {\partial f}{\partial x}$ is not as simple as division: it's a shorthand for $\lim_{h\to 0} \frac {f(x+h,y)-f(x,y)}{h}$.  This leads students to think that $\frac {\partial f}{\partial g}\frac {\partial g}{\partial x} = \frac {\partial f}{\partial x}$ and other such ""cancellations"" hold.  However, an operator which looks like division isn't entirely a bad thing, in that you get the correct units when doing a physical problem -- obviously this is because the definition does include a division, but only as part of a limit. Kind of a nitpick, but $\frac {\partial^n f}{\partial x^n}$ looks awfully weird to me.  And worse, it could be confusing to some who see the ""denominator"" as $\partial(x^2)$ instead of $(\partial x)^2$. Other notations include $f_x$, $\partial_x f$, and $D_x f$.  I don't like $f_x$ because that looks like an indexed function to me, not like $f$ is being operated on by a differential operator.  $\partial_x f$ and $D_x f$ both look like operators acting on $f$ but don't tell you the units and we get back to ""can we really use $x$ to refer to the first parameter of $f$ outside of the definition of $f$""? Does anyone know of a better notation?  Can anyone come up with one?",,"['multivariable-calculus', 'soft-question', 'partial-derivative']"
6,Jacobian or No Jacobian - Surface Integrals,Jacobian or No Jacobian - Surface Integrals,,"I've read http://www.physicsforums.com/showthread.php?t=310220 and http://www.physicsforums.com/showthread.php?t=458840 , but I'm still confused whether we need the Jacobian or not in computing surface integrals. The following examples are from P1091 and 1092 in Section 16.7 from Calculus , 6th Edition, by James Stewart. Why is there a Jacobian for #23 $\color{red}{\text{in red}}$ but NOT for #15 and #47? $\Large{\text{15.}}$ Evaluate the surface integral: $\iint_Sz(x^2 + y^2) dS $ where $S$ is the hemisphere $x^2 + y^2 + z^2 = 4, z \geq 0$. $\Large{\text{23.}}$ Evaluate the surface integral $\iint_S \mathbf{F} \cdot d\mathbf{S},$ where $\mathbf{F} = (x,-z,y)$ and $S$ is the part of $x^2 + y^2 + z^2 = 4$ in the first octant and oriented towards the origin. Remember to use the positive (outward) orientation. $\Large{\text{47.}}$ Let $\mathbf{F(r)}$ = $\cfrac{c\mathbf{r}}{{\vert \mathbf{r} \vert}^3} $ for some constant $c$ and $\mathbf{r} = (x,y,z) $ and $S$ be a sphere with center the origin.  Show that the flux of $F$ across $S$ is independent of the radius of $S$. Given solutions: $\Large{\text{15.}}$ Parameterise with spherical coordinates : $x = \color{green}{2}\cos\theta\sin\phi,  y = \color{green}{2}\sin\theta\sin\phi, z = \color{green}{2}\cos\phi.$ Then    $\iint_Sz(x^2 + y^2) dS = \int_0^{2\pi}\int_0^{\pi/2}[4\sin^2\phi2cos\phi] \underbrace{(4\sin\phi)}_{\vert \partial_{\phi} \mathbf{r} \times \partial_{\theta} \mathbf{r} \vert} \require{enclose} \enclose{horizontalstrike}{(p \color{green}{= 2})^2\sin\phi}  dA $. $\Large{\text{23.}} $ $\iint_S \mathbf{F} \cdot d\mathbf{S} = \iint \mathbf{F} \cdot \mathbf{\hat{n}} dS = \iint_{x^2 + y^2 \leq 4} \dfrac{-x}{\sqrt{4 - x^2 - y^2}} dA = \int_0^{\pi/2}\int_0^{2}  \dfrac{-(r\cos\theta)^2}{\sqrt{4 - r^2}} \color{red}{(r dr d\theta)}. $ $\Large{\text{47.}}$ Let the sphere's radius $:= \vert \mathbf{r} \vert := k$. Parameterise with spherical coordinates : $x = \color{brown}{k}\cos\theta\sin\phi\,  y = \color{brown}{k}\sin\theta\sin\phi, z = \color{brown}{k}\cos\phi.$ Then $\iint_S \mathbf{F} \cdot d\mathbf{S} = \iint_D \mathbf{F[r}(\phi, \theta)] \cdot (\partial_{\phi} \mathbf{r} \times \partial_{\theta} \mathbf{r})  dA  $ $            = \int_0^{\pi}\int_0^{2\pi} \frac{c}{k^3}(k\cos\theta\sin\phi\,  k\sin\theta\sin\phi, k\cos\phi) \cdot (k^2\cos\theta\sin^2\phi, k^2\sin\theta\sin\phi, k\sin\phi\cos\phi) \require{enclose} \enclose{horizontalstrike}{(p\color{brown}{= k})^2\sin\phi}d\theta d\phi$ $\Large{\text{Supplementaries in response to Muphrid's answer:}}$ $\Large{\text{Q1.}}$ In your first paragraph, you wrote that if $\mathbf{r}(x,y)$ then $\partial_x \mathbf{r} = \mathbf{\hat{x}} $ and $\partial_y \mathbf{r} = \mathbf{\hat{y}} $. How is this true? For example, if $\mathbf{r}(x,y) = (x, y, z(x,y))$ then $\partial_x \mathbf{r} = (1, 0, \partial_xz) $ and $\partial_y \mathbf{r} = (1, 0, \partial_yz) $? $\Large{\text{Q2.}}$ In your sixth paragraph, you wrote: ""Here, we don't have the area element vector expressed in a coordinate system yet, so it doesn't make sense to use (say) Cartesian and then push it forward with the Jacobian."" In your last paragraph, you wrote: ""The solutions here skip the Jacobian..."" However, are you only referring to #15 and #47 here? With many thanks to you, I now understand that #15 and #47 start and remain working with spherical coordinates. But in #23, my understanding is that the solution starts with $(x, y, z(x,y))$. Then it uses the Jacobian ($r dr d\theta$) while converting to polar coordinates. #23 is the only question for which the Jacobian is used (and not in #15 or #47). $\Large{\text{Q3.}}$ In your third paragraph, you commented on ""why this integral is phrased in terms of $r$ at all."" I'm inferring that you disagree with the solution's choice of starting with $(x, y, z(x,y))$? You (and I) believe that only spherical coordinates should be used the whole time because of convenience? $\Large{\text{Supplementary in response to Muphrid's 2nd answer:}}$ $\Large{\text{Q2.1}}$ Many thanks for your second answer. You wrote in your second answer, for problem #23:  ""...But I must emphasize that that, in itself, is not using the Jacobian matrix, because the Jacobian matrix must act on some vector, converting it from one coordinate system to another.  Since they don't write down the cartesian components of $\hat{\mathbf n}$, it doesn't seem to me that they're actually using it."" How does solution for #23 NOT use the $\color{magenta}{\text{Jacobian}}$? The solution starts with $(x, y, z(x,y))$ and then convert to polar coordinates.  $\Longrightarrow \iint_R f(x,y, z(x,y)) dA = \iint_D f(r\cos\theta, r\sin\theta, z(r\cos\theta, r\sin\theta) \color{magenta}{\underbrace{\dfrac{\partial(x,y)}{\partial(r,\theta)}}_{\huge{= ... = r > 0}}} dr \, d\theta  $ (More info on Stewart P1017)","I've read http://www.physicsforums.com/showthread.php?t=310220 and http://www.physicsforums.com/showthread.php?t=458840 , but I'm still confused whether we need the Jacobian or not in computing surface integrals. The following examples are from P1091 and 1092 in Section 16.7 from Calculus , 6th Edition, by James Stewart. Why is there a Jacobian for #23 $\color{red}{\text{in red}}$ but NOT for #15 and #47? $\Large{\text{15.}}$ Evaluate the surface integral: $\iint_Sz(x^2 + y^2) dS $ where $S$ is the hemisphere $x^2 + y^2 + z^2 = 4, z \geq 0$. $\Large{\text{23.}}$ Evaluate the surface integral $\iint_S \mathbf{F} \cdot d\mathbf{S},$ where $\mathbf{F} = (x,-z,y)$ and $S$ is the part of $x^2 + y^2 + z^2 = 4$ in the first octant and oriented towards the origin. Remember to use the positive (outward) orientation. $\Large{\text{47.}}$ Let $\mathbf{F(r)}$ = $\cfrac{c\mathbf{r}}{{\vert \mathbf{r} \vert}^3} $ for some constant $c$ and $\mathbf{r} = (x,y,z) $ and $S$ be a sphere with center the origin.  Show that the flux of $F$ across $S$ is independent of the radius of $S$. Given solutions: $\Large{\text{15.}}$ Parameterise with spherical coordinates : $x = \color{green}{2}\cos\theta\sin\phi,  y = \color{green}{2}\sin\theta\sin\phi, z = \color{green}{2}\cos\phi.$ Then    $\iint_Sz(x^2 + y^2) dS = \int_0^{2\pi}\int_0^{\pi/2}[4\sin^2\phi2cos\phi] \underbrace{(4\sin\phi)}_{\vert \partial_{\phi} \mathbf{r} \times \partial_{\theta} \mathbf{r} \vert} \require{enclose} \enclose{horizontalstrike}{(p \color{green}{= 2})^2\sin\phi}  dA $. $\Large{\text{23.}} $ $\iint_S \mathbf{F} \cdot d\mathbf{S} = \iint \mathbf{F} \cdot \mathbf{\hat{n}} dS = \iint_{x^2 + y^2 \leq 4} \dfrac{-x}{\sqrt{4 - x^2 - y^2}} dA = \int_0^{\pi/2}\int_0^{2}  \dfrac{-(r\cos\theta)^2}{\sqrt{4 - r^2}} \color{red}{(r dr d\theta)}. $ $\Large{\text{47.}}$ Let the sphere's radius $:= \vert \mathbf{r} \vert := k$. Parameterise with spherical coordinates : $x = \color{brown}{k}\cos\theta\sin\phi\,  y = \color{brown}{k}\sin\theta\sin\phi, z = \color{brown}{k}\cos\phi.$ Then $\iint_S \mathbf{F} \cdot d\mathbf{S} = \iint_D \mathbf{F[r}(\phi, \theta)] \cdot (\partial_{\phi} \mathbf{r} \times \partial_{\theta} \mathbf{r})  dA  $ $            = \int_0^{\pi}\int_0^{2\pi} \frac{c}{k^3}(k\cos\theta\sin\phi\,  k\sin\theta\sin\phi, k\cos\phi) \cdot (k^2\cos\theta\sin^2\phi, k^2\sin\theta\sin\phi, k\sin\phi\cos\phi) \require{enclose} \enclose{horizontalstrike}{(p\color{brown}{= k})^2\sin\phi}d\theta d\phi$ $\Large{\text{Supplementaries in response to Muphrid's answer:}}$ $\Large{\text{Q1.}}$ In your first paragraph, you wrote that if $\mathbf{r}(x,y)$ then $\partial_x \mathbf{r} = \mathbf{\hat{x}} $ and $\partial_y \mathbf{r} = \mathbf{\hat{y}} $. How is this true? For example, if $\mathbf{r}(x,y) = (x, y, z(x,y))$ then $\partial_x \mathbf{r} = (1, 0, \partial_xz) $ and $\partial_y \mathbf{r} = (1, 0, \partial_yz) $? $\Large{\text{Q2.}}$ In your sixth paragraph, you wrote: ""Here, we don't have the area element vector expressed in a coordinate system yet, so it doesn't make sense to use (say) Cartesian and then push it forward with the Jacobian."" In your last paragraph, you wrote: ""The solutions here skip the Jacobian..."" However, are you only referring to #15 and #47 here? With many thanks to you, I now understand that #15 and #47 start and remain working with spherical coordinates. But in #23, my understanding is that the solution starts with $(x, y, z(x,y))$. Then it uses the Jacobian ($r dr d\theta$) while converting to polar coordinates. #23 is the only question for which the Jacobian is used (and not in #15 or #47). $\Large{\text{Q3.}}$ In your third paragraph, you commented on ""why this integral is phrased in terms of $r$ at all."" I'm inferring that you disagree with the solution's choice of starting with $(x, y, z(x,y))$? You (and I) believe that only spherical coordinates should be used the whole time because of convenience? $\Large{\text{Supplementary in response to Muphrid's 2nd answer:}}$ $\Large{\text{Q2.1}}$ Many thanks for your second answer. You wrote in your second answer, for problem #23:  ""...But I must emphasize that that, in itself, is not using the Jacobian matrix, because the Jacobian matrix must act on some vector, converting it from one coordinate system to another.  Since they don't write down the cartesian components of $\hat{\mathbf n}$, it doesn't seem to me that they're actually using it."" How does solution for #23 NOT use the $\color{magenta}{\text{Jacobian}}$? The solution starts with $(x, y, z(x,y))$ and then convert to polar coordinates.  $\Longrightarrow \iint_R f(x,y, z(x,y)) dA = \iint_D f(r\cos\theta, r\sin\theta, z(r\cos\theta, r\sin\theta) \color{magenta}{\underbrace{\dfrac{\partial(x,y)}{\partial(r,\theta)}}_{\huge{= ... = r > 0}}} dr \, d\theta  $ (More info on Stewart P1017)",,[]
7,Evaluating differential forms.,Evaluating differential forms.,,"Can someone please check my work? It's an exercise from Barret O'Neill's Elementary Differential Geometry. I want to be really sure that my understanding of this is right. I see that the forms $\mathrm{dx, dy, dz}$ work as projections of the vector part, and the functions multiplying these forms are evaluated in the point in question. I'm given the tangent vector $v_p = (1,2,-3)_p$, where $p = (0,-2,1)$  I must evaluate some forms on $v_p$.  My work: a) $y^2~ \mathrm{dx}$. $$\begin{align} (y^2 ~\mathrm{dx})(v_p) &= (-2)^2 \cdot \mathrm{dx}(1,2,-3) \\ &= 4 \cdot 1 \\ &= 4 \end{align}$$ b) $z~ \mathrm{dy} - y~ \mathrm{dz}$. $$\begin{align} (z~ \mathrm{dy} - y~ \mathrm{dz})(v_p) &= 1 \cdot \mathrm{dy}(1,2,-3) - (-2) \cdot \mathrm{dz}(1,2,-3) \\ &= 2 + 2 \cdot (-3) \\ &= -4 \end{align}$$ c) $ (z^2 - 1) ~\mathrm{dx} - \mathrm{dy} + x^2~ \mathrm{dz}$. $$\begin{align} \left((z^2 - 1)~ \mathrm{dx} - \mathrm{dy} + x^2 ~ \mathrm{dz} \right)(v_p) &= (1^2 - 1) \cdot \mathrm{dx}(1, 2,-3) - \mathrm{dy}(1,2,-3) + 0^2 \cdot \mathrm{dz}(1,2,-3) \\ &= - 2 \end{align}$$ Thank you.","Can someone please check my work? It's an exercise from Barret O'Neill's Elementary Differential Geometry. I want to be really sure that my understanding of this is right. I see that the forms $\mathrm{dx, dy, dz}$ work as projections of the vector part, and the functions multiplying these forms are evaluated in the point in question. I'm given the tangent vector $v_p = (1,2,-3)_p$, where $p = (0,-2,1)$  I must evaluate some forms on $v_p$.  My work: a) $y^2~ \mathrm{dx}$. $$\begin{align} (y^2 ~\mathrm{dx})(v_p) &= (-2)^2 \cdot \mathrm{dx}(1,2,-3) \\ &= 4 \cdot 1 \\ &= 4 \end{align}$$ b) $z~ \mathrm{dy} - y~ \mathrm{dz}$. $$\begin{align} (z~ \mathrm{dy} - y~ \mathrm{dz})(v_p) &= 1 \cdot \mathrm{dy}(1,2,-3) - (-2) \cdot \mathrm{dz}(1,2,-3) \\ &= 2 + 2 \cdot (-3) \\ &= -4 \end{align}$$ c) $ (z^2 - 1) ~\mathrm{dx} - \mathrm{dy} + x^2~ \mathrm{dz}$. $$\begin{align} \left((z^2 - 1)~ \mathrm{dx} - \mathrm{dy} + x^2 ~ \mathrm{dz} \right)(v_p) &= (1^2 - 1) \cdot \mathrm{dx}(1, 2,-3) - \mathrm{dy}(1,2,-3) + 0^2 \cdot \mathrm{dz}(1,2,-3) \\ &= - 2 \end{align}$$ Thank you.",,"['multivariable-calculus', 'differential-forms']"
8,Solving $\nabla \times \mathbf{b} = \mathbf{b} \times \mathbf{a}$,Solving,\nabla \times \mathbf{b} = \mathbf{b} \times \mathbf{a},"Suppose we are given a fixed vector field $\mathbf{a}$. I am interested in the problem of determining a vector field $\mathbf{b}$ such that  $$\nabla \times \mathbf{b} = \mathbf{b} \times \mathbf{a}.$$ This has another interpretation. Suppose $\alpha$ and $\beta$ are the 1-forms dual to $\mathbf{a}$ and $\mathbf{b}$. The above equation can be written as $$d \beta = \beta \wedge \alpha,$$  and so we can interpret this problem as finding, for a fixed 1-form $\alpha$, a foliation $\mathcal{F} = \text{ker}\ \beta$ such that $\alpha$ determines the Godbillon-Vey class of $\mathcal{F}$. It seems unlikely to me that a solution always exists, but I have been unable to prove anything beyond the obvious fact that we must have $\mathbf{b} \cdot \nabla \times \mathbf{b} = 0$, and that $\mathbf{b} \cdot \nabla \times \mathbf{a} = \mathbf{a} \cdot \nabla \times \mathbf{b}$ (take the divergence), which implies (Mark's comment) that $\mathbf{b}$ is orthogonal to $\nabla \times \mathbf{a}$. Are there any known results about such equations, or techniques one could use to construct a solution other than crunching through the PDEs for each component?","Suppose we are given a fixed vector field $\mathbf{a}$. I am interested in the problem of determining a vector field $\mathbf{b}$ such that  $$\nabla \times \mathbf{b} = \mathbf{b} \times \mathbf{a}.$$ This has another interpretation. Suppose $\alpha$ and $\beta$ are the 1-forms dual to $\mathbf{a}$ and $\mathbf{b}$. The above equation can be written as $$d \beta = \beta \wedge \alpha,$$  and so we can interpret this problem as finding, for a fixed 1-form $\alpha$, a foliation $\mathcal{F} = \text{ker}\ \beta$ such that $\alpha$ determines the Godbillon-Vey class of $\mathcal{F}$. It seems unlikely to me that a solution always exists, but I have been unable to prove anything beyond the obvious fact that we must have $\mathbf{b} \cdot \nabla \times \mathbf{b} = 0$, and that $\mathbf{b} \cdot \nabla \times \mathbf{a} = \mathbf{a} \cdot \nabla \times \mathbf{b}$ (take the divergence), which implies (Mark's comment) that $\mathbf{b}$ is orthogonal to $\nabla \times \mathbf{a}$. Are there any known results about such equations, or techniques one could use to construct a solution other than crunching through the PDEs for each component?",,"['multivariable-calculus', 'cross-product', 'foliations']"
9,Can a differential k-form be integrated on a manifold that is not k-dimensional?,Can a differential k-form be integrated on a manifold that is not k-dimensional?,,"For example, can you integrate a 2-form on some curve, a 1-dimensional manifold, or some 3-dimensional manifold? I know that Stokes's Theorem states that if you integrate $\omega \in \mathcal A^{k-1}(M)$ or a (k-1)-differential form when integrated over the (k-1)-dimensional boundary of the k-dimensional manifold $M$, it is equal to integrating $d\omega \in \mathcal A^{k}(M)$ over the k-dimensional manifold $M$. I.e. $\int_{\partial M} \omega = \int_{M} d\omega$. I'm not very confident in all this because I'm new to learning university math, I am a novice in it and I'm doing this for fun as I am in Grade 11 still so I am not really forcing myself to learn all the details which can be bad. If you can recommend an article or book that explains my question that is suitable for my level, I would appreciate it a lot. I'm studying from Professor Shifrin's lectures on Multivariable Calculus. I was recently on one of his lectures on Stoke's Theorem and it was interesting because it seemed that all the dimensions of the forms and the manifolds (or its boundaries) matched up. Therefore, I was curious if I can integrate k-forms on manifolds of dimension less than k or bigger than k, as I can integrate at least some k-forms on k-dimensional manifolds according to Stokes's Theorem. I have edited this question as I had no details whatsoever which can seem rude so I tried to put some context into it but I'm new to this site so please bear with me.","For example, can you integrate a 2-form on some curve, a 1-dimensional manifold, or some 3-dimensional manifold? I know that Stokes's Theorem states that if you integrate $\omega \in \mathcal A^{k-1}(M)$ or a (k-1)-differential form when integrated over the (k-1)-dimensional boundary of the k-dimensional manifold $M$, it is equal to integrating $d\omega \in \mathcal A^{k}(M)$ over the k-dimensional manifold $M$. I.e. $\int_{\partial M} \omega = \int_{M} d\omega$. I'm not very confident in all this because I'm new to learning university math, I am a novice in it and I'm doing this for fun as I am in Grade 11 still so I am not really forcing myself to learn all the details which can be bad. If you can recommend an article or book that explains my question that is suitable for my level, I would appreciate it a lot. I'm studying from Professor Shifrin's lectures on Multivariable Calculus. I was recently on one of his lectures on Stoke's Theorem and it was interesting because it seemed that all the dimensions of the forms and the manifolds (or its boundaries) matched up. Therefore, I was curious if I can integrate k-forms on manifolds of dimension less than k or bigger than k, as I can integrate at least some k-forms on k-dimensional manifolds according to Stokes's Theorem. I have edited this question as I had no details whatsoever which can seem rude so I tried to put some context into it but I'm new to this site so please bear with me.",,"['multivariable-calculus', 'manifolds', 'differential-forms']"
10,Prove that a non-zero acceleration is perpendicular to a constant speed,Prove that a non-zero acceleration is perpendicular to a constant speed,,"Take the differentiable vector function $\vec{v}(t)$ (a velocity vector).  If its speed, $|\vec{v}(t)|=constant$, then prove that at any point which $\frac{d\vec{v}}{dt}$ is non-zero, $\frac{d\vec{v}}{dt}$ is perpendicular to $\vec{v}(t)$. In other words, if a velocity vector has constant speed, show that whenever its acceleration vector is non-zero, it is perpendicular to that velocity vector. Intuitively this seems clear, since whenever the speed is remains constant, the acceleration is 0.  Why though are the velocity and acceleration vectors perpendicular though?  Because they share no intersection when the acceleration is non-zero since the speed is constant? I'm assuming we should somehow show that $\vec{v} \cdot \vec{a} = 0$.  The thing is, I can't figure out what knowing that the speed is constant tells us about the velocity.  It could still be anything (like some trig functions squaring up and becoming 1 through an identity)?","Take the differentiable vector function $\vec{v}(t)$ (a velocity vector).  If its speed, $|\vec{v}(t)|=constant$, then prove that at any point which $\frac{d\vec{v}}{dt}$ is non-zero, $\frac{d\vec{v}}{dt}$ is perpendicular to $\vec{v}(t)$. In other words, if a velocity vector has constant speed, show that whenever its acceleration vector is non-zero, it is perpendicular to that velocity vector. Intuitively this seems clear, since whenever the speed is remains constant, the acceleration is 0.  Why though are the velocity and acceleration vectors perpendicular though?  Because they share no intersection when the acceleration is non-zero since the speed is constant? I'm assuming we should somehow show that $\vec{v} \cdot \vec{a} = 0$.  The thing is, I can't figure out what knowing that the speed is constant tells us about the velocity.  It could still be anything (like some trig functions squaring up and becoming 1 through an identity)?",,['multivariable-calculus']
11,"Continuity of $f(x,y)=4x^3y^{11}(x^4+y^8)^{-2}$ at $(0,0)$",Continuity of  at,"f(x,y)=4x^3y^{11}(x^4+y^8)^{-2} (0,0)","Well, the function is $$\frac{4x^3y^{11}}{(x^4+y^{8})^2}$$ and we want to know if it's continuous at $(0,0)$. I've tried as many trajectories as I could think of, and they all give $0$ as the limit. So I tried proving, by definition that its limit is in fact $0$, but to no avail.","Well, the function is $$\frac{4x^3y^{11}}{(x^4+y^{8})^2}$$ and we want to know if it's continuous at $(0,0)$. I've tried as many trajectories as I could think of, and they all give $0$ as the limit. So I tried proving, by definition that its limit is in fact $0$, but to no avail.",,"['multivariable-calculus', 'continuity']"
12,"Can a function have partial derivatives, be continuous but not be differentiable?","Can a function have partial derivatives, be continuous but not be differentiable?",,"I have a function: $$     f(x,y)=     \begin{cases}       \dfrac{2x^2y+y^3}{x^2+y^2} & \text{if $(x,y) \neq (0,0)$}\\       0 & \text{if $(x,y) = (0,0)$}\\     \end{cases} $$ which I think I managed to show: a) continuity at $(0,0)$ by $\lim_{(x,y) \to (0,0)} f(x,y) = 0$ b) has partial derivatives at $(0,0)$ by the definition of derivatives and found $f'_x(0,0) = 0, f'_y(0,0) =1$ . Still not 100% sure if did this correctly. c) not differentiable at $(0,0)$ by definition of differentiable functions and that a limit didn't exist. However, I feel like because of this I can tell more about the function. I'd like it if someone can confirm this. I assumed, that because it wasn't differentiable, the partial derivatives might not be continuous around $(0,0)$ . $$\frac{\partial f}{\partial x} = \frac{2y^3x}{\left(x^2+y^2\right)^2}$$ $$\frac{\partial f}{\partial y} =   \frac{y^4+y^2x^2+2x^4}{\left(x^2+y^2\right)^2}$$ Is that the case? I checked the limits $$\lim_{(x,y) \to (0,0)} \frac{\partial f}{\partial x} \quad \text{and} \quad \lim_{(x,y) \to (0,0)} \frac{\partial f}{\partial y}$$ and they don't seem to exist. What would happen if one existed but not the other? Is this possible? What would happen if the limit was something else than $0$ and $1$ I calculated in b)? Just not being continuous? I am just worried if the function really has partial derivatives in $(0,0)$ . Thank you in advance!","I have a function: which I think I managed to show: a) continuity at by b) has partial derivatives at by the definition of derivatives and found . Still not 100% sure if did this correctly. c) not differentiable at by definition of differentiable functions and that a limit didn't exist. However, I feel like because of this I can tell more about the function. I'd like it if someone can confirm this. I assumed, that because it wasn't differentiable, the partial derivatives might not be continuous around . Is that the case? I checked the limits and they don't seem to exist. What would happen if one existed but not the other? Is this possible? What would happen if the limit was something else than and I calculated in b)? Just not being continuous? I am just worried if the function really has partial derivatives in . Thank you in advance!","    
f(x,y)=
    \begin{cases}
      \dfrac{2x^2y+y^3}{x^2+y^2} & \text{if (x,y) \neq (0,0)}\\
      0 & \text{if (x,y) = (0,0)}\\
    \end{cases}
 (0,0) \lim_{(x,y) \to (0,0)} f(x,y) = 0 (0,0) f'_x(0,0) = 0, f'_y(0,0) =1 (0,0) (0,0) \frac{\partial f}{\partial x} = \frac{2y^3x}{\left(x^2+y^2\right)^2} \frac{\partial f}{\partial y} =   \frac{y^4+y^2x^2+2x^4}{\left(x^2+y^2\right)^2} \lim_{(x,y) \to (0,0)} \frac{\partial f}{\partial x} \quad \text{and} \quad \lim_{(x,y) \to (0,0)} \frac{\partial f}{\partial y} 0 1 (0,0)","['multivariable-calculus', 'derivatives', 'continuity', 'partial-derivative']"
13,How can it be proved that the geometric mean function is concave?,How can it be proved that the geometric mean function is concave?,,"A function $f: \mathbb R^n \rightarrow \mathbb R$ is said to be concave if $$\left(\forall x,y \in \mathbb{R}^n \right) \left( \forall \lambda \in [0,1] \right) \left(\lambda f(x) + (1-\lambda)f(y) \le f(\lambda x + (1- \lambda)y)\right)$$ In the case of the geometric mean function (defined below), how would we prove concavity? $$f(x_1,\dots,x_n) := \left(\prod_{i=1}^n x_i \right)^\frac1n$$ I have been trying all day to find a proof, mostly by induction, but also considering the Hessian, which if always negative semidefinite implies concavity. Any tips, please?","A function is said to be concave if In the case of the geometric mean function (defined below), how would we prove concavity? I have been trying all day to find a proof, mostly by induction, but also considering the Hessian, which if always negative semidefinite implies concavity. Any tips, please?","f: \mathbb R^n \rightarrow \mathbb R \left(\forall x,y \in \mathbb{R}^n \right) \left( \forall \lambda \in [0,1] \right) \left(\lambda f(x) + (1-\lambda)f(y) \le f(\lambda x + (1- \lambda)y)\right) f(x_1,\dots,x_n) := \left(\prod_{i=1}^n x_i \right)^\frac1n","['multivariable-calculus', 'induction', 'convex-analysis', 'means']"
14,"Differentiability of a two variable function $f(x,y)=\dfrac{1}{1+x-y}$",Differentiability of a two variable function,"f(x,y)=\dfrac{1}{1+x-y}","We're given the following function : $$f(x,y)=\dfrac{1}{1+x-y}$$ Now , how to prove that the given function is differentiable at $(0,0)$ ? I found out the partial derivatives as $f_x(0,0)=(-1)$ and $f_y(0,0)=1$ , Clearly the partial derivatives are continuous , but that doesn't guarantee differentiability , does it ? Is there any other way to prove the same ?","We're given the following function : $$f(x,y)=\dfrac{1}{1+x-y}$$ Now , how to prove that the given function is differentiable at $(0,0)$ ? I found out the partial derivatives as $f_x(0,0)=(-1)$ and $f_y(0,0)=1$ , Clearly the partial derivatives are continuous , but that doesn't guarantee differentiability , does it ? Is there any other way to prove the same ?",,['multivariable-calculus']
15,About the definition of curvature,About the definition of curvature,,"In Do Carmo's differential geometry book, he says for a curve $\alpha: I=(a,b)\rightarrow\mathbb{R}^3$ parametrized by arc length, ""since the tangent vector $\alpha'$ (s) has unit length, the norm $|\alpha''(s)|$ of the second derivative measures the rate of change of the angle which neighboring tangents make with the tangent at $s$ . Why does the unit length of the tangent vector imply this geometric meaning of $|\alpha''(s)|$ ?","In Do Carmo's differential geometry book, he says for a curve parametrized by arc length, ""since the tangent vector (s) has unit length, the norm of the second derivative measures the rate of change of the angle which neighboring tangents make with the tangent at . Why does the unit length of the tangent vector imply this geometric meaning of ?","\alpha: I=(a,b)\rightarrow\mathbb{R}^3 \alpha' |\alpha''(s)| s |\alpha''(s)|","['multivariable-calculus', 'differential-geometry']"
16,Why is a gradient field a special case of a vector field?,Why is a gradient field a special case of a vector field?,,"My calculus manual suggests a gradient field is just a special case of a vector field. That implies that there are vector fields that there are not gradient fields. The gradient field is composted of a vector and each $\mathbf{i}$, $\mathbf{j}$, $\mathbf{k}$ component (using 3 dimensions) is multiplied by a scalar that is a partial derivative. Is this because the scalar may be of the form that there is no antiderivative of? Perhaps of a function that can not be differentiated? Maybe a vector field that has sharp turns but I can't come up with any that I can't find a derivative for. Does anybody have any so I can get some intuition of the problem?","My calculus manual suggests a gradient field is just a special case of a vector field. That implies that there are vector fields that there are not gradient fields. The gradient field is composted of a vector and each $\mathbf{i}$, $\mathbf{j}$, $\mathbf{k}$ component (using 3 dimensions) is multiplied by a scalar that is a partial derivative. Is this because the scalar may be of the form that there is no antiderivative of? Perhaps of a function that can not be differentiated? Maybe a vector field that has sharp turns but I can't come up with any that I can't find a derivative for. Does anybody have any so I can get some intuition of the problem?",,['multivariable-calculus']
17,Does the Divergence Theorem Work on a Surface?,Does the Divergence Theorem Work on a Surface?,,"The divergence theorem in $\mathbb{R}^3$ says that the integral of the divergence of a vector field over a solid $\Omega$ in $\mathbb{R}^3$ equals the flux through the surface of $\Omega$ denoted by $\partial \Omega$. My question is that can you use the divergence theorem in $\mathbb{R}^3$ where you're integrating the divergence of a vector field over a surface $S$ in $\mathbb{R}^3$ to get the flux through the edges ($\partial S$) of the surface? So if the surface $S$ is closed in $\mathbb{R}^3$ such as a sphere, then clearly $\partial S = 0$ so the integral of the divergence over this particular $S$ is $0$?","The divergence theorem in $\mathbb{R}^3$ says that the integral of the divergence of a vector field over a solid $\Omega$ in $\mathbb{R}^3$ equals the flux through the surface of $\Omega$ denoted by $\partial \Omega$. My question is that can you use the divergence theorem in $\mathbb{R}^3$ where you're integrating the divergence of a vector field over a surface $S$ in $\mathbb{R}^3$ to get the flux through the edges ($\partial S$) of the surface? So if the surface $S$ is closed in $\mathbb{R}^3$ such as a sphere, then clearly $\partial S = 0$ so the integral of the divergence over this particular $S$ is $0$?",,['multivariable-calculus']
18,How to evaluate this nonelementary integral?,How to evaluate this nonelementary integral?,,"Let $x>0$ . I have to prove that $$ \int_{0}^{\infty}\frac{\cos x}{x^p}dx=\frac{\pi}{2\Gamma(p)\cos(p\frac{\pi}{2})}\tag{1} $$ by converting the integral on the left side to a double integral using the expression below: $$ \frac{1}{x^p}=\frac{1}{\Gamma(p)}\int_{0}^{\infty}e^{-xt}t^{p-1}dt\tag{2} $$ By plugging $(2)$ into $(1)$ I get the following double integral: $$ \frac{1}{\Gamma(p)}\int_{0}^{\infty}\int_{0}^{\infty}e^{-xt}t^{p-1}\cos xdtdx\tag{3} $$ However, I unable to proceed any further as I am unclear as to what method should I use in order to compute this integral. I thought that an appropriate change of variables could transform it into a product of two gamma functions but I cannot see how that would work. Any help would be greatly appreciated.","Let . I have to prove that by converting the integral on the left side to a double integral using the expression below: By plugging into I get the following double integral: However, I unable to proceed any further as I am unclear as to what method should I use in order to compute this integral. I thought that an appropriate change of variables could transform it into a product of two gamma functions but I cannot see how that would work. Any help would be greatly appreciated.","x>0 
\int_{0}^{\infty}\frac{\cos x}{x^p}dx=\frac{\pi}{2\Gamma(p)\cos(p\frac{\pi}{2})}\tag{1}
 
\frac{1}{x^p}=\frac{1}{\Gamma(p)}\int_{0}^{\infty}e^{-xt}t^{p-1}dt\tag{2}
 (2) (1) 
\frac{1}{\Gamma(p)}\int_{0}^{\infty}\int_{0}^{\infty}e^{-xt}t^{p-1}\cos xdtdx\tag{3}
","['multivariable-calculus', 'gamma-function']"
19,"If a plane intersects a regular surface at exactly one point, then it is the tangent plane","If a plane intersects a regular surface at exactly one point, then it is the tangent plane",,"Question Let a regular surface, $S$, intersect a plane, $P$, at only one point, $p_0 = (x_0, y_0, z_0)$ in $\mathbb{R}^3$. Show that the plane coincides with the tangent plane to the surface at $p_0.$ Remarks The problem seems so easy and unassuming, however, I have been stuck on it for several days. I have come up with proofs but I am unconvinced by any so far. Please provide as elementary a proof as possible. Nothing greater than multivariable calculus and some linear algebra (if necessary). I have a feeling that it may be best solved by contradiction but my attempt does not follow this strategy. Attempted Proof Let $S$ be a regular surface and $P$ be a plane which passes through $p_0$. Locally, any regular surfaces may be represented by a function $z=f(x,y)$.  Hence the tangent plane, $L(x,y)$, to $S$ at $p_0$ is  \begin{align*} L(x,y) &= f(x_0, y_0) + f_x(x_0,y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0). \end{align*} Generally, a plane, $P$, is given by  $$ A(x-x_0) + B(y-y_0) + C(z-z_0) = 0. $$  $$ \hat{n}\cdot(\bar{p}-\bar{p_0}) $$ where $\hat{n} = (A,B,C)$ is the normal vector to the plane and $p=(x,y,z)$ is any point on the plane. Note that since $p$ is on the plane it cannot be on the surface by hypothesis. Here is where I start to get stuck. Help would be appreciated. I'm sure that there is a simple way of thinking about, and proving it. Thanks","Question Let a regular surface, $S$, intersect a plane, $P$, at only one point, $p_0 = (x_0, y_0, z_0)$ in $\mathbb{R}^3$. Show that the plane coincides with the tangent plane to the surface at $p_0.$ Remarks The problem seems so easy and unassuming, however, I have been stuck on it for several days. I have come up with proofs but I am unconvinced by any so far. Please provide as elementary a proof as possible. Nothing greater than multivariable calculus and some linear algebra (if necessary). I have a feeling that it may be best solved by contradiction but my attempt does not follow this strategy. Attempted Proof Let $S$ be a regular surface and $P$ be a plane which passes through $p_0$. Locally, any regular surfaces may be represented by a function $z=f(x,y)$.  Hence the tangent plane, $L(x,y)$, to $S$ at $p_0$ is  \begin{align*} L(x,y) &= f(x_0, y_0) + f_x(x_0,y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0). \end{align*} Generally, a plane, $P$, is given by  $$ A(x-x_0) + B(y-y_0) + C(z-z_0) = 0. $$  $$ \hat{n}\cdot(\bar{p}-\bar{p_0}) $$ where $\hat{n} = (A,B,C)$ is the normal vector to the plane and $p=(x,y,z)$ is any point on the plane. Note that since $p$ is on the plane it cannot be on the surface by hypothesis. Here is where I start to get stuck. Help would be appreciated. I'm sure that there is a simple way of thinking about, and proving it. Thanks",,"['multivariable-calculus', 'differential-geometry']"
20,Multivariable limit - perhaps a trickier problem I am stuck on. [duplicate],Multivariable limit - perhaps a trickier problem I am stuck on. [duplicate],,"This question already has answers here : Check if limit exists and its value (2 answers) Closed 5 years ago . I am trying to solve the following limit: $\lim_{(x,y) \to (0,0)} \frac{x^4y^4}{(x^2 + y^4)^3}$ (This is a more challenging problem from Folland Calculus, it seems). I am pretty sure this limit does not exist (however, this is just a guess, and I am not 100% sure.) I am trying to approach via the paths y = mx and x = my but... they don't seem to work. I have tried simpler cases such as x = 0 and y = 0. Any help appreciate.","This question already has answers here : Check if limit exists and its value (2 answers) Closed 5 years ago . I am trying to solve the following limit: $\lim_{(x,y) \to (0,0)} \frac{x^4y^4}{(x^2 + y^4)^3}$ (This is a more challenging problem from Folland Calculus, it seems). I am pretty sure this limit does not exist (however, this is just a guess, and I am not 100% sure.) I am trying to approach via the paths y = mx and x = my but... they don't seem to work. I have tried simpler cases such as x = 0 and y = 0. Any help appreciate.",,['multivariable-calculus']
21,How can I prove that these definitions of curl are equivalent?,How can I prove that these definitions of curl are equivalent?,,"I am reading the book ""Div, Grad, Curl, and All that"" and I got to the section about curl. In this section, the author defines the curl to be $$ (\nabla \times \mathbf{F})\cdot \mathbf{\hat{n}} \ \overset{\underset{\mathrm{def}}{}}{=} \lim_{S \to 0}\left( \frac{1}{|S|}\oint_C \mathbf{F} \cdot d\mathbf{r}\right) \tag{1}$$ The author gives a ""physicist's rough-and-ready proof"" of how this expression reduces to each of the $x,y$ and $z$ components of $$ \nabla \times \mathbf{F} = \left(\frac{\partial F_z}{\partial y} - \frac{\partial F_y}{\partial z}\right) \boldsymbol{\hat\imath} + \left(\frac{\partial F_x}{\partial z} - \frac{\partial F_z}{\partial x} \right) \boldsymbol{\hat\jmath} + \left(\frac{\partial F_y}{\partial x} - \frac{\partial F_x}{\partial y} \right) \boldsymbol{\hat k} \tag{2} $$ when analyzing a closed path $C$ which is parallel to the $yz, xz$ , and $xy$ planes respectively. After this, I started wondering if there's a more rigorous way to show that, in cartesian coordinates, equation $(2)$ satisfies the definition in equation $(1)$ . I'm specifically interested in a proof where you assume any arbitrary closed path $C$ . If someone could tell me how this is can be done, or could point me in a direction where this proof is already given I would greatly appreciate it. Thank you!","I am reading the book ""Div, Grad, Curl, and All that"" and I got to the section about curl. In this section, the author defines the curl to be The author gives a ""physicist's rough-and-ready proof"" of how this expression reduces to each of the and components of when analyzing a closed path which is parallel to the , and planes respectively. After this, I started wondering if there's a more rigorous way to show that, in cartesian coordinates, equation satisfies the definition in equation . I'm specifically interested in a proof where you assume any arbitrary closed path . If someone could tell me how this is can be done, or could point me in a direction where this proof is already given I would greatly appreciate it. Thank you!"," (\nabla \times \mathbf{F})\cdot \mathbf{\hat{n}} \ \overset{\underset{\mathrm{def}}{}}{=} \lim_{S \to 0}\left( \frac{1}{|S|}\oint_C \mathbf{F} \cdot d\mathbf{r}\right) \tag{1} x,y z 
\nabla \times \mathbf{F} =
\left(\frac{\partial F_z}{\partial y} - \frac{\partial F_y}{\partial z}\right) \boldsymbol{\hat\imath} + \left(\frac{\partial F_x}{\partial z} - \frac{\partial F_z}{\partial x} \right) \boldsymbol{\hat\jmath} + \left(\frac{\partial F_y}{\partial x} - \frac{\partial F_x}{\partial y} \right) \boldsymbol{\hat k} \tag{2}
 C yz, xz xy (2) (1) C","['multivariable-calculus', 'vector-analysis', 'vector-fields', 'line-integrals', 'curl']"
22,The integral of the mean curvature vector over a closed immersed surface,The integral of the mean curvature vector over a closed immersed surface,,"Suppose we have a closed, orientable, smooth surface $\Sigma$ immersed smoothly in $\mathbb R^n$ via $f:\Sigma \rightarrow \mathbb R^n$.  Impose a Riemannian structure on $\Sigma$ by taking $g_{ij} = \partial_if\cdot\partial_jf$, the metric induced on $\Sigma$ by the immersion $f$.  The inner product here is just the usual inner product from $\mathbb R^n$. The mean curvature vector is $$ \vec H = \Delta f, $$ where $\Delta$ is the Laplace-Beltrami operator on $(\Sigma,g)$. Consider the integral of the mean curvature vector over the surface $\Sigma$: $$ \int_\Sigma \vec H\ d\mu. $$ It seems rather plausible that this ought to be zero in the case where $\Sigma$ is closed, embedded, and has only one codimension.  Is this known?  Is it easy to prove? If it is not zero in the generality above, as a surface immersed in $\mathbb R^n$, is it equal to some expression involving topological information of $\Sigma$?","Suppose we have a closed, orientable, smooth surface $\Sigma$ immersed smoothly in $\mathbb R^n$ via $f:\Sigma \rightarrow \mathbb R^n$.  Impose a Riemannian structure on $\Sigma$ by taking $g_{ij} = \partial_if\cdot\partial_jf$, the metric induced on $\Sigma$ by the immersion $f$.  The inner product here is just the usual inner product from $\mathbb R^n$. The mean curvature vector is $$ \vec H = \Delta f, $$ where $\Delta$ is the Laplace-Beltrami operator on $(\Sigma,g)$. Consider the integral of the mean curvature vector over the surface $\Sigma$: $$ \int_\Sigma \vec H\ d\mu. $$ It seems rather plausible that this ought to be zero in the case where $\Sigma$ is closed, embedded, and has only one codimension.  Is this known?  Is it easy to prove? If it is not zero in the generality above, as a surface immersed in $\mathbb R^n$, is it equal to some expression involving topological information of $\Sigma$?",,"['differential-geometry', 'multivariable-calculus', 'riemannian-geometry']"
23,Parametric curve for a tennis ball seam,Parametric curve for a tennis ball seam,,"I hadn't noticed until now tennis balls were symmetric about more than one axis. Which lead me to think there could be an elegant way of expressing the seam as a parametric curve, but I haven't found any yet. I've tried using products of sines and cosines, but none of them even approximate the curve and it's getting rather ugly. This is not homework, by the way.","I hadn't noticed until now tennis balls were symmetric about more than one axis. Which lead me to think there could be an elegant way of expressing the seam as a parametric curve, but I haven't found any yet. I've tried using products of sines and cosines, but none of them even approximate the curve and it's getting rather ugly. This is not homework, by the way.",,['multivariable-calculus']
24,Normal Vector to a Sphere,Normal Vector to a Sphere,,"I'm having kind of a problem on calculating the normal vector to a sphere using a parameterization. Consider a unit-radius sphere centered at the origin. One can parameterize it using the following: $$P(\phi, \theta)=(\sin(\phi)\cos(\theta),\,\sin(\phi)\sin(\theta),\,\cos(\phi)) $$ My Vector Calculus book says that the Vector Product between the two partial derivates of the parameterized surface gives a Normal Vector to the surface. I found that $$\frac {\partial P(\phi, \theta)}{\partial \phi} \times \frac {\partial P(\phi, \theta)}{\partial \theta} = (\sin^2(\phi)\cos(\theta))\hat i + (\sin^2(\phi)\sin(\theta))\hat j + (\sin(\phi)\cos(\phi))\hat k$$ Which, as one can easily verify, yields $(0, 0,0)$ for $(\phi,\theta) = (0,0)$ which means that the sphere is not regular at the point $(0,0,1)$. That is obviously wrong, so I would like to know where is my mistake.","I'm having kind of a problem on calculating the normal vector to a sphere using a parameterization. Consider a unit-radius sphere centered at the origin. One can parameterize it using the following: $$P(\phi, \theta)=(\sin(\phi)\cos(\theta),\,\sin(\phi)\sin(\theta),\,\cos(\phi)) $$ My Vector Calculus book says that the Vector Product between the two partial derivates of the parameterized surface gives a Normal Vector to the surface. I found that $$\frac {\partial P(\phi, \theta)}{\partial \phi} \times \frac {\partial P(\phi, \theta)}{\partial \theta} = (\sin^2(\phi)\cos(\theta))\hat i + (\sin^2(\phi)\sin(\theta))\hat j + (\sin(\phi)\cos(\phi))\hat k$$ Which, as one can easily verify, yields $(0, 0,0)$ for $(\phi,\theta) = (0,0)$ which means that the sphere is not regular at the point $(0,0,1)$. That is obviously wrong, so I would like to know where is my mistake.",,['multivariable-calculus']
25,When is the line integral independent of parameterization?,When is the line integral independent of parameterization?,,"Let $\alpha: [a,b] \rightarrow \mathbb{R}^2$ be a smooth path (i.e. $\alpha'$ is continuous on $[a,b]$), and let $f$ be a continuous vector field. The line integral of $f$ along $\alpha$ is defined as $\int_a^b f[\alpha(t)]\cdot \alpha'(t) dt$. In a typical calculus textbook, it is written as $\int_C f_1 dx + f_2 dy$, suggesting that the line integral depends only on $C$ - the image of the path, not the actual parameterization $\alpha$. In general however, this is not true. For example, $\oint_C -ydx + x dy =2\pi$ when integrated along the path $\alpha: [0,2\pi]\rightarrow \mathbb{R}^2, \alpha(t) = (\cos(t), \sin(t))$. The value becomes $4\pi$ when integrated along $\beta: [0,4\pi]\rightarrow \mathbb{R}^2, \beta(t)= (\cos(t),\sin(t))$, even though $\alpha$ and $\beta$ have the same image. My question: what additional assumption is required to make the notation $\int_C f_1dx +f_2 dy$ unambiguous?","Let $\alpha: [a,b] \rightarrow \mathbb{R}^2$ be a smooth path (i.e. $\alpha'$ is continuous on $[a,b]$), and let $f$ be a continuous vector field. The line integral of $f$ along $\alpha$ is defined as $\int_a^b f[\alpha(t)]\cdot \alpha'(t) dt$. In a typical calculus textbook, it is written as $\int_C f_1 dx + f_2 dy$, suggesting that the line integral depends only on $C$ - the image of the path, not the actual parameterization $\alpha$. In general however, this is not true. For example, $\oint_C -ydx + x dy =2\pi$ when integrated along the path $\alpha: [0,2\pi]\rightarrow \mathbb{R}^2, \alpha(t) = (\cos(t), \sin(t))$. The value becomes $4\pi$ when integrated along $\beta: [0,4\pi]\rightarrow \mathbb{R}^2, \beta(t)= (\cos(t),\sin(t))$, even though $\alpha$ and $\beta$ have the same image. My question: what additional assumption is required to make the notation $\int_C f_1dx +f_2 dy$ unambiguous?",,"['multivariable-calculus', 'line-integrals']"
26,Is the curl of every non-conservative vector field nonzero at some point?,Is the curl of every non-conservative vector field nonzero at some point?,,"Counterexamples? Intuitively, why? Thanks for any answers. As a side note, in what math class are gradient, divergence and curl taught typically?","Counterexamples? Intuitively, why? Thanks for any answers. As a side note, in what math class are gradient, divergence and curl taught typically?",,['multivariable-calculus']
27,Are gradient flows the quickest way to minimize a function for a short time?,Are gradient flows the quickest way to minimize a function for a short time?,,"Let $F:\mathbb{R}^n \to \mathbb{R}$ be a smooth function, and let $p \in \mathbb{R}^n$ . Let $\alpha(t)$ be the solution to the negative gradient flow of $F$ , i.e. $$ \alpha(0)=p, \, \, \dot \alpha(t)=-\nabla F(\alpha(t)).$$ Let $\beta(t)$ be a smooth path starting at $p$ (i.e. $\beta(0)=p$ ) and suppose that $\|\dot \beta(t)\|=\|\dot \alpha(t)\|$ . Is it true that $F(\alpha(t)) \le F(\beta(t))$ for sufficiently small $t$ ? We can assume that $\nabla F(p) \neq 0$ , since otherwise $\alpha$ is constant, and then $\|\dot \beta \|=\|\dot  \alpha\|=0$ implies $\beta$ is also constant. It is easy to see that the answer is positive if $\dot \beta(0) \neq -\nabla F(p)$ (see details below). I am not sure what happens when $\dot \beta(0) = -\nabla F(p)$ . Details: Write $G(t)=F(\beta(t))-F(\alpha(t)) $ . Then, $G(0)=0$ , and $$G'(0)=\langle \nabla F(p),\dot \beta(0) \rangle-\langle \nabla F(p),\dot \alpha(0) \rangle=\langle \nabla F(p),\dot \beta(0) \rangle+\|\nabla F(p)\|^2 \ge 0,$$ Since by the C-S inequality, we have $\langle \nabla F(p),\dot \beta(0) \rangle \ge - \|\nabla F(p)\| \cdot \|\dot \beta(0)\|=-\|\nabla F(p)\| \cdot \|\dot \alpha(0)\|=-\|\nabla F(p)\|^2$ , with equality if and only if $\dot \beta(0)=-\lambda  \nabla F(p)$ for some positive scalar $\lambda$ . So, we showed that if $\dot \beta(0) \neq -\nabla F(p)$ , then $G(0)=0,G'(0) >0$ , hence $G(t)>G(0)$ . Analysis of the case $\dot \beta(0) = -\nabla F(p)$ : Writing $$G'(t)=\langle \nabla F(\beta(t)),\dot \beta(t) \rangle-\langle \nabla F(\alpha(t)),\dot \alpha(t) \rangle$$ we get $$G''(t)= d^2F_{\beta(t)}(\dot \beta(t),\dot \beta(t))+\langle \nabla F(\beta(t)),\ddot \beta(t) \rangle -d^2F_{\alpha(t)}(\dot \alpha(t),\dot \alpha(t))-\langle \nabla F(\alpha(t)),\ddot \alpha(t) \rangle,$$ so $$ G''(0)= d^2F_{p}(-\nabla F(p),-\nabla F(p))+\langle \nabla F(p),\ddot \beta(0) \rangle -d^2F_{p}(-\nabla F(p),-\nabla F(p))-\langle \nabla F(p),\ddot \alpha(0) \rangle=\langle \nabla F(p),\ddot \beta(0) -\ddot \alpha(0) \rangle. \tag{1}$$ Now, we use our assumption that $\langle \dot \beta(t),\dot \beta(t) \rangle=\langle \dot \alpha(t),\dot \alpha(t) \rangle$ . Differentiating this, we obtain $$ \langle \dot \beta(0),\ddot \beta(0) \rangle=\langle \dot \alpha(0),\ddot \alpha(0) \rangle \tag{2},$$ which really means $$ \langle \nabla F(p),\ddot \beta(0) \rangle=\langle \nabla F(p),\ddot \alpha(0) \rangle. \tag{3}$$ Combining $(1)$ and $(3)$ implies that $G''(0)=0$ , so this does not seem to help us. Do we need to proceed to third derivatives? It seems interesting to see if using $\|\dot \beta \|=\|\dot  \alpha\|$ we can express neatly $G'''(0)$ . Edit: As commented by Anthony Carapetis, this ""differential analysis"" approach is doomed to fail: Indeed, if we want to show $G(t)\ge 0$ by examining derivatives of $G$ at zero, we will have to show that the first non-zero derivative is strictly positive. However, $\alpha$ and $\beta$ may have arbitrarily many derivatives agreeing at zero. (they can even agree on the derivatives of all orders).","Let be a smooth function, and let . Let be the solution to the negative gradient flow of , i.e. Let be a smooth path starting at (i.e. ) and suppose that . Is it true that for sufficiently small ? We can assume that , since otherwise is constant, and then implies is also constant. It is easy to see that the answer is positive if (see details below). I am not sure what happens when . Details: Write . Then, , and Since by the C-S inequality, we have , with equality if and only if for some positive scalar . So, we showed that if , then , hence . Analysis of the case : Writing we get so Now, we use our assumption that . Differentiating this, we obtain which really means Combining and implies that , so this does not seem to help us. Do we need to proceed to third derivatives? It seems interesting to see if using we can express neatly . Edit: As commented by Anthony Carapetis, this ""differential analysis"" approach is doomed to fail: Indeed, if we want to show by examining derivatives of at zero, we will have to show that the first non-zero derivative is strictly positive. However, and may have arbitrarily many derivatives agreeing at zero. (they can even agree on the derivatives of all orders).","F:\mathbb{R}^n \to \mathbb{R} p \in \mathbb{R}^n \alpha(t) F  \alpha(0)=p, \, \, \dot \alpha(t)=-\nabla F(\alpha(t)). \beta(t) p \beta(0)=p \|\dot \beta(t)\|=\|\dot \alpha(t)\| F(\alpha(t)) \le F(\beta(t)) t \nabla F(p) \neq 0 \alpha \|\dot \beta \|=\|\dot  \alpha\|=0 \beta \dot \beta(0) \neq -\nabla F(p) \dot \beta(0) = -\nabla F(p) G(t)=F(\beta(t))-F(\alpha(t))  G(0)=0 G'(0)=\langle \nabla F(p),\dot \beta(0) \rangle-\langle \nabla F(p),\dot \alpha(0) \rangle=\langle \nabla F(p),\dot \beta(0) \rangle+\|\nabla F(p)\|^2 \ge 0, \langle \nabla F(p),\dot \beta(0) \rangle \ge - \|\nabla F(p)\| \cdot \|\dot \beta(0)\|=-\|\nabla F(p)\| \cdot \|\dot \alpha(0)\|=-\|\nabla F(p)\|^2 \dot \beta(0)=-\lambda  \nabla F(p) \lambda \dot \beta(0) \neq -\nabla F(p) G(0)=0,G'(0) >0 G(t)>G(0) \dot \beta(0) = -\nabla F(p) G'(t)=\langle \nabla F(\beta(t)),\dot \beta(t) \rangle-\langle \nabla F(\alpha(t)),\dot \alpha(t) \rangle G''(t)= d^2F_{\beta(t)}(\dot \beta(t),\dot \beta(t))+\langle \nabla F(\beta(t)),\ddot \beta(t) \rangle -d^2F_{\alpha(t)}(\dot \alpha(t),\dot \alpha(t))-\langle \nabla F(\alpha(t)),\ddot \alpha(t) \rangle,  G''(0)= d^2F_{p}(-\nabla F(p),-\nabla F(p))+\langle \nabla F(p),\ddot \beta(0) \rangle -d^2F_{p}(-\nabla F(p),-\nabla F(p))-\langle \nabla F(p),\ddot \alpha(0) \rangle=\langle \nabla F(p),\ddot \beta(0) -\ddot \alpha(0) \rangle. \tag{1} \langle \dot \beta(t),\dot \beta(t) \rangle=\langle \dot \alpha(t),\dot \alpha(t) \rangle  \langle \dot \beta(0),\ddot \beta(0) \rangle=\langle \dot \alpha(0),\ddot \alpha(0) \rangle \tag{2},  \langle \nabla F(p),\ddot \beta(0) \rangle=\langle \nabla F(p),\ddot \alpha(0) \rangle. \tag{3} (1) (3) G''(0)=0 \|\dot \beta \|=\|\dot  \alpha\| G'''(0) G(t)\ge 0 G \alpha \beta","['multivariable-calculus', 'differential-geometry', 'optimization', 'gradient-descent', 'gradient-flows']"
28,What is the Exterior Derivative Trying to Do?,What is the Exterior Derivative Trying to Do?,,"$\newcommand{\R}{\mathbf R}$ Consider a smooth function $f:\R^n\to\R$ and let $Df:\R^n\to \R^{n*}$ be the map which takes a point $\mathbf a\in R^n$ to the linear map $Df_{\mathbf a}:\R^n\to \R$. This itself is a smooth map whose derivative at a point can be thought of as a multilinear map from $\R^n\times \R^n\to \R$, the matrix representation of which (with respect to, say, the standard basis) will have second partial derivatives. On the other hand, the map $Df$ can be thought of as a $1$-form on $\R^n$, for at each point $\mathbf a\in \R^n$, we have a functional $Df_{\mathbf a}$. Explicitly, this $1$-form can be written as $$\sum_{i=1}^n\frac{\partial f}{\partial x^i}dx^i$$ The exterior derivative of this is $0$. So there is a big difference between the notion of the (usual?) derivative and that of the exterior derivative. If I were asked to say something about what the usual derivative is trying to capture, I would say that the derivative is the natural generalization of the derivative of a map from $\R$ to $\R$. A smooth function $f:\R^n\to \R^m$ at any given point in the domain behaves as closely as we please to a linear map, the derivative, in a sufficiently small neighborhood of the given point. But I do not have any intuition as to what is the idea behind the exterior derivative. Can somebody say something about this and about how does the idea of the exterior derivative help us formalize the truth of stokes theorem? Thanks.","$\newcommand{\R}{\mathbf R}$ Consider a smooth function $f:\R^n\to\R$ and let $Df:\R^n\to \R^{n*}$ be the map which takes a point $\mathbf a\in R^n$ to the linear map $Df_{\mathbf a}:\R^n\to \R$. This itself is a smooth map whose derivative at a point can be thought of as a multilinear map from $\R^n\times \R^n\to \R$, the matrix representation of which (with respect to, say, the standard basis) will have second partial derivatives. On the other hand, the map $Df$ can be thought of as a $1$-form on $\R^n$, for at each point $\mathbf a\in \R^n$, we have a functional $Df_{\mathbf a}$. Explicitly, this $1$-form can be written as $$\sum_{i=1}^n\frac{\partial f}{\partial x^i}dx^i$$ The exterior derivative of this is $0$. So there is a big difference between the notion of the (usual?) derivative and that of the exterior derivative. If I were asked to say something about what the usual derivative is trying to capture, I would say that the derivative is the natural generalization of the derivative of a map from $\R$ to $\R$. A smooth function $f:\R^n\to \R^m$ at any given point in the domain behaves as closely as we please to a linear map, the derivative, in a sufficiently small neighborhood of the given point. But I do not have any intuition as to what is the idea behind the exterior derivative. Can somebody say something about this and about how does the idea of the exterior derivative help us formalize the truth of stokes theorem? Thanks.",,"['multivariable-calculus', 'differential-geometry', 'derivatives']"
29,A better resource for vector calculus than Stewart?,A better resource for vector calculus than Stewart?,,"I took the Math GRE Subject Test last October and tried to relearn vector calculus via the current edition of Stewart's text. I thought, to put it lightly, that the exposition was atrocious and unmotivated, with too much of a focus on memorizing the equations needed to solve problems. I did do well on the calculus questions on the Subject Test [I think], but if I need to teach myself vector calculus again, I can't use Stewart to do it. What do you all recommend for a good rigorous, motivational text on vector calculus? I need the book to cover Green's Theorem [which I've noticed, for example, Kline's text does not cover]. [This is one of those times that I wished Spivak covered vector calculus.]","I took the Math GRE Subject Test last October and tried to relearn vector calculus via the current edition of Stewart's text. I thought, to put it lightly, that the exposition was atrocious and unmotivated, with too much of a focus on memorizing the equations needed to solve problems. I did do well on the calculus questions on the Subject Test [I think], but if I need to teach myself vector calculus again, I can't use Stewart to do it. What do you all recommend for a good rigorous, motivational text on vector calculus? I need the book to cover Green's Theorem [which I've noticed, for example, Kline's text does not cover]. [This is one of those times that I wished Spivak covered vector calculus.]",,"['multivariable-calculus', 'reference-request', 'vectors']"
30,Intuition Behind Generalized Stokes Theorem,Intuition Behind Generalized Stokes Theorem,,"Consider the Generalized Stokes Theorem: \begin{equation} \int_Md\omega = \int_{\partial{M}} \omega \end{equation} Here, $\omega$ is a k-form defined on $R^n$ , and $d\omega$ (a k+1 form defined on $R^n$ ) is the exterior derivative of $\omega$ . Let M be a smooth k+1-manifold in $R^n$ and $\partial{M}$ (the boundary of M) be a smooth k manifold. I know that the above theorem is simply a generalization of well-known vector calculus theorems. However, I am looking for the intuition behind the Generalized Stokes Theorem itself. I started off by defining the exterior derivative at a point p in $R^n$ as: \begin{equation} d\omega_p =\lim_{|vol|\to 0}\frac{\int_{\partial{vol}} \omega}{|vol|} \end{equation} In this case, "" $vol$ "" represents a k+1 ""parallelpiped"" in $R^n$ that contains point p (with $|vol|$ being its ""volume""). $\partial{vol}$ represents the boundary of this k+1 ""parallelpiped"", a k ""parallelpiped"" itself. With this definition (assuming it is correct), can we say that $\omega$ represents an infinitesimal ""flux"" element through $\partial{vol}$ which would imply that $d\omega_p$ is simply the ""flux density"" at a point p? If the above is true, can we take the idea that (when applying the Generalized Stokes Theorem) the interior ""fluxes"" through each $\partial{vol}$ within M cancel out leaving us with the total ""flux"" out of $\partial{M}$ as the intuition behind the Generalized Stokes Theorem? Any help is much appreciated.","Consider the Generalized Stokes Theorem: Here, is a k-form defined on , and (a k+1 form defined on ) is the exterior derivative of . Let M be a smooth k+1-manifold in and (the boundary of M) be a smooth k manifold. I know that the above theorem is simply a generalization of well-known vector calculus theorems. However, I am looking for the intuition behind the Generalized Stokes Theorem itself. I started off by defining the exterior derivative at a point p in as: In this case, "" "" represents a k+1 ""parallelpiped"" in that contains point p (with being its ""volume""). represents the boundary of this k+1 ""parallelpiped"", a k ""parallelpiped"" itself. With this definition (assuming it is correct), can we say that represents an infinitesimal ""flux"" element through which would imply that is simply the ""flux density"" at a point p? If the above is true, can we take the idea that (when applying the Generalized Stokes Theorem) the interior ""fluxes"" through each within M cancel out leaving us with the total ""flux"" out of as the intuition behind the Generalized Stokes Theorem? Any help is much appreciated.","\begin{equation}
\int_Md\omega = \int_{\partial{M}} \omega
\end{equation} \omega R^n d\omega R^n \omega R^n \partial{M} R^n \begin{equation}
d\omega_p =\lim_{|vol|\to 0}\frac{\int_{\partial{vol}} \omega}{|vol|}
\end{equation} vol R^n |vol| \partial{vol} \omega \partial{vol} d\omega_p \partial{vol} \partial{M}","['multivariable-calculus', 'differential-geometry', 'vector-analysis', 'differential-forms']"
31,Why does $\nabla$ behave like a member of $\mathbb{R}^3$ (Euclidean vector in 3d space) in many cases?,Why does  behave like a member of  (Euclidean vector in 3d space) in many cases?,\nabla \mathbb{R}^3,"Is there a reason why $\vec{\nabla} = \left[\; \dfrac{\partial}{\partial x}, \dfrac{\partial}{\partial y}, \dfrac{\partial}{\partial z}\; \right]$ , behaves like a member of $\mathbb{R}^3$ (Euclidean vector in 3d space) in so many cases: Dot products and cross products of $\vec{\nabla}$ with multivariable vector function like $\vec{\nabla} \cdot \mathbf{\vec{f}}$ and $\vec{\nabla} \times \mathbf{\vec{f}}$ or with scalar functions (gradient) are vectors (i.e. independent of particular choice of co-ordinates). Properties like the Lagrange's formula that are valid for vectors have direct analogues involving the $\vec{ \nabla}$ operator. We can write pseudo-determinants for curls just like we could with components of vectors. Most of its vector properties hold if $\vec{\nabla}$ is replaced by any other vector. There will be separate proofs for all of these properties. But is there any underlying reason that works for all such properties? Is there a fundamental reason why $\vec{\nabla}$ is $\mathbb{R}^3$ vector-ish ?","Is there a reason why , behaves like a member of (Euclidean vector in 3d space) in so many cases: Dot products and cross products of with multivariable vector function like and or with scalar functions (gradient) are vectors (i.e. independent of particular choice of co-ordinates). Properties like the Lagrange's formula that are valid for vectors have direct analogues involving the operator. We can write pseudo-determinants for curls just like we could with components of vectors. Most of its vector properties hold if is replaced by any other vector. There will be separate proofs for all of these properties. But is there any underlying reason that works for all such properties? Is there a fundamental reason why is vector-ish ?","\vec{\nabla} = \left[\; \dfrac{\partial}{\partial x}, \dfrac{\partial}{\partial y}, \dfrac{\partial}{\partial z}\; \right] \mathbb{R}^3 \vec{\nabla} \vec{\nabla} \cdot \mathbf{\vec{f}} \vec{\nabla} \times \mathbf{\vec{f}} \vec{
\nabla} \vec{\nabla} \vec{\nabla} \mathbb{R}^3","['multivariable-calculus', 'vector-analysis']"
32,Lipschitz continuous and Jacobian matrix,Lipschitz continuous and Jacobian matrix,,"Consider a function $f:\mathbb{R}^n\longrightarrow\mathbb{R}^m$ with partial derivatives everywhere so that the Jacobian matrix is well-defined. Let $L>0$ be a real number. Is it true that: $$|f(x)-f(y)|\leq L|x-y|,\forall x,y \Longleftrightarrow |J_f(x)|_2\leq L,\forall x$$ where $|\cdot|$ denotes the euclidean vector norm and $|\cdot|_2$ the spectral matrix norm.",Consider a function with partial derivatives everywhere so that the Jacobian matrix is well-defined. Let be a real number. Is it true that: where denotes the euclidean vector norm and the spectral matrix norm.,"f:\mathbb{R}^n\longrightarrow\mathbb{R}^m L>0 |f(x)-f(y)|\leq L|x-y|,\forall x,y \Longleftrightarrow |J_f(x)|_2\leq L,\forall x |\cdot| |\cdot|_2","['multivariable-calculus', 'lipschitz-functions', 'jacobian']"
33,a function with differentiable partial derivatives but unequal mixed derivatives,a function with differentiable partial derivatives but unequal mixed derivatives,,"I am looking for an example of a function $f:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ are both differentiable at some point, say the origin, but $\frac{\partial^2 f}{\partial x\partial y}\neq\frac{\partial^2 f}{\partial y\partial x}$ at that point.","I am looking for an example of a function $f:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ are both differentiable at some point, say the origin, but $\frac{\partial^2 f}{\partial x\partial y}\neq\frac{\partial^2 f}{\partial y\partial x}$ at that point.",,['multivariable-calculus']
34,"Find all functions $F(x,y)$ such as $\frac{\sqrt{3}}{2}\frac{\partial{f}}{\partial{x}}+\frac{1}{2}\frac{\partial{f}}{\partial{y}}=0$",Find all functions  such as,"F(x,y) \frac{\sqrt{3}}{2}\frac{\partial{f}}{\partial{x}}+\frac{1}{2}\frac{\partial{f}}{\partial{y}}=0","How to find all possible functions $f(x,y)$ such as: $$ \frac{\sqrt{3}}{2}f_x+\frac{1}{2}f_y=0$$ (with $f_x = \frac{\partial{f}}{\partial{x}}$ ) Here's everything I tried: 1) I can guess the most simple cases, $f_x=k$ and $f_y=-k\sqrt{3}$ so the following is a solution (but not all possible functions): $$ f(x,y)= k(x-\sqrt{3}y)+c$$ 2) The equation could be writen as: $$ cos (\frac{\pi}{6})f_x+sin(\frac{\pi}{6})f_y=0$$ But I culdn't see what can I do with that. 3) By the implicit function theorem I could find that: $$\frac{1}{\sqrt{3}} = - \frac{f_x}{f_y} = \frac{dy}{dx} $$ So $f(x,y)$ has level curves in the form: $$y=\frac{x}{\sqrt{3}}+c $$ 4) I could also find the level curves by noticing that the equation says that the partial derivative of $f(x,y)$ in the direction of $\langle\sqrt{3},1\rangle$ is zero. 5) Or by solving the differential equation: $$ dx-\sqrt{3}dy=0 $$ But 3), 4) and 5) couldn't help since I don't know how to find $f(x,y)$ based on its level curves. 6) After some trial and error I could find (and prove) that for any $g:R\rightarrow R$ the following function is ok: $$ f(x,y) = g(x-\sqrt{3}y) $$ I can also see that $\langle\sqrt{3},1\rangle$ is perpendicular to $\langle1,-\sqrt{3}\rangle$, so that's related to the fact that the partial derivative of $f(x,y)$ in the direction of $\langle\sqrt{3},1\rangle$ is zero. I can see that I'm very close, but I can't find out: How to prove that all possible functions are of that form? How could I find that form by calculations? If there are different ways to find the general form of $f(x,y)$ or to prove it, I would like to read about them all.","How to find all possible functions $f(x,y)$ such as: $$ \frac{\sqrt{3}}{2}f_x+\frac{1}{2}f_y=0$$ (with $f_x = \frac{\partial{f}}{\partial{x}}$ ) Here's everything I tried: 1) I can guess the most simple cases, $f_x=k$ and $f_y=-k\sqrt{3}$ so the following is a solution (but not all possible functions): $$ f(x,y)= k(x-\sqrt{3}y)+c$$ 2) The equation could be writen as: $$ cos (\frac{\pi}{6})f_x+sin(\frac{\pi}{6})f_y=0$$ But I culdn't see what can I do with that. 3) By the implicit function theorem I could find that: $$\frac{1}{\sqrt{3}} = - \frac{f_x}{f_y} = \frac{dy}{dx} $$ So $f(x,y)$ has level curves in the form: $$y=\frac{x}{\sqrt{3}}+c $$ 4) I could also find the level curves by noticing that the equation says that the partial derivative of $f(x,y)$ in the direction of $\langle\sqrt{3},1\rangle$ is zero. 5) Or by solving the differential equation: $$ dx-\sqrt{3}dy=0 $$ But 3), 4) and 5) couldn't help since I don't know how to find $f(x,y)$ based on its level curves. 6) After some trial and error I could find (and prove) that for any $g:R\rightarrow R$ the following function is ok: $$ f(x,y) = g(x-\sqrt{3}y) $$ I can also see that $\langle\sqrt{3},1\rangle$ is perpendicular to $\langle1,-\sqrt{3}\rangle$, so that's related to the fact that the partial derivative of $f(x,y)$ in the direction of $\langle\sqrt{3},1\rangle$ is zero. I can see that I'm very close, but I can't find out: How to prove that all possible functions are of that form? How could I find that form by calculations? If there are different ways to find the general form of $f(x,y)$ or to prove it, I would like to read about them all.",,"['multivariable-calculus', 'proof-writing', 'partial-derivative']"
35,Can we tell if a function has a max or min by looking along specific directions?,Can we tell if a function has a max or min by looking along specific directions?,,"Suppose we have a smooth function $f$ from $\mathbb R^n\to\mathbb R$ such that $\nabla f(0)=0$, and we want to check if $f$ has a local maximum at $0$ (as opposed to a local min or a saddle point). For any vector $v$ in $\mathbb R^n$, we can form a function $g_v:\mathbb R\to\mathbb R$ defined by $g_v(x)=f(vx)$. Intuitively, we are looking at the behavior of $f$ along the $v$ direction. If $f$ has a local max at zero, then of course $g_v$ also has a local max at zero. My question is, is the converse true? If the single-variable function $g_v$ has a local max at $0$ for every possible direction $v$, does $f$ also have a local max at $0$? If the Hessian of $f$ at zero has full rank, then this question is easily answered as yes. By taking $v$ to be in turn each of the eigenvectors of the Hessian, the fact that $g_v$ has a local max means that the corresponding eigenvalues are all negative, and so $f$ has a local max at $0$. But if some of the eigenvalues are $0$, then I'm not sure how to analyze it. As a first step in a proof, I might think: well, for any fixed direction, there is some radius $r$ such that as long as $v$ is within distance $r$ of the origin, then $f(v)\le f(0)$. But since the $r$ depends on the direction, we can't necessarily find a single $r$ which works for all directions, so that might allow for a counter-example. As a bonus, if it does hold for smooth functions, what about functions that are merely differentiable? Continuous? Or even all functions?","Suppose we have a smooth function $f$ from $\mathbb R^n\to\mathbb R$ such that $\nabla f(0)=0$, and we want to check if $f$ has a local maximum at $0$ (as opposed to a local min or a saddle point). For any vector $v$ in $\mathbb R^n$, we can form a function $g_v:\mathbb R\to\mathbb R$ defined by $g_v(x)=f(vx)$. Intuitively, we are looking at the behavior of $f$ along the $v$ direction. If $f$ has a local max at zero, then of course $g_v$ also has a local max at zero. My question is, is the converse true? If the single-variable function $g_v$ has a local max at $0$ for every possible direction $v$, does $f$ also have a local max at $0$? If the Hessian of $f$ at zero has full rank, then this question is easily answered as yes. By taking $v$ to be in turn each of the eigenvectors of the Hessian, the fact that $g_v$ has a local max means that the corresponding eigenvalues are all negative, and so $f$ has a local max at $0$. But if some of the eigenvalues are $0$, then I'm not sure how to analyze it. As a first step in a proof, I might think: well, for any fixed direction, there is some radius $r$ such that as long as $v$ is within distance $r$ of the origin, then $f(v)\le f(0)$. But since the $r$ depends on the direction, we can't necessarily find a single $r$ which works for all directions, so that might allow for a counter-example. As a bonus, if it does hold for smooth functions, what about functions that are merely differentiable? Continuous? Or even all functions?",,"['multivariable-calculus', 'maxima-minima']"
36,Multivariable calculus: hard problems with solutions,Multivariable calculus: hard problems with solutions,,"I'm practicing for my multivariable calculus exam and I'm having some trouble mostly because I have no way of knowing if my solutions are correct or not. For example, a typical problem goes like this: Let $f:\mathbb{R^2}\longrightarrow\mathbb{R}$ defined by: $$f(x,y)=\begin{cases} \sin(y-x) & \text{for} & y>|x| \\  \\ 0 & \text{for} & y=|x| \\  \\ \frac{x-y}{\sqrt{x^2 + y^2}} & \text{for} & y<|x|  \end{cases}$$ Study $f$ with respect to continuity on its domain. Study $f$ with respect to differentiability on its domain. I think I know how to solve this, but I have no way to verify my answer and I might be unaware of some subtleties. Moreover, I did some browsing, but I was unable to find examples containing functions defined with branches such as this one. As you probably know, branches are precisely what make this a non-trivial problem (at least for me!). So, I came here to ask for recommendations on books or online resources with solutions (don't need all the details, just the results) to problems like this one. Thanks!","I'm practicing for my multivariable calculus exam and I'm having some trouble mostly because I have no way of knowing if my solutions are correct or not. For example, a typical problem goes like this: Let $f:\mathbb{R^2}\longrightarrow\mathbb{R}$ defined by: $$f(x,y)=\begin{cases} \sin(y-x) & \text{for} & y>|x| \\  \\ 0 & \text{for} & y=|x| \\  \\ \frac{x-y}{\sqrt{x^2 + y^2}} & \text{for} & y<|x|  \end{cases}$$ Study $f$ with respect to continuity on its domain. Study $f$ with respect to differentiability on its domain. I think I know how to solve this, but I have no way to verify my answer and I might be unaware of some subtleties. Moreover, I did some browsing, but I was unable to find examples containing functions defined with branches such as this one. As you probably know, branches are precisely what make this a non-trivial problem (at least for me!). So, I came here to ask for recommendations on books or online resources with solutions (don't need all the details, just the results) to problems like this one. Thanks!",,['multivariable-calculus']
37,An expression for computing second order partial derivatives of an implicitely defined function,An expression for computing second order partial derivatives of an implicitely defined function,,"Let $\Phi(x,y)=0$ be an implicit function s.t. $\Phi:\mathbb{R}^n\times \mathbb{R}^k\rightarrow \mathbb{R}^n$ and $\det\left(\frac{\partial \Phi}{\partial   x}(x_0,y_0)\right)\neq 0$ . This means that locally at $(x_0,y_0)$ we can express $x_i$ as functions of $y$ . Next, we can compute partial derivatives of $x$ as \begin{equation}\tag{*}\frac{\partial x_i}{\partial y_j}=-\frac{\det\left(\left[\frac{\partial \Phi}{\partial   x_1},\dots,\frac{\partial \Phi}{\partial   x_{i-1}}, \frac{\partial \Phi}{\partial   y_j}, \frac{\partial \Phi}{\partial   x_{i+1}},\dots, \frac{\partial \Phi}{\partial   x_n}\right]\right)}{\det\left(\frac{\partial \Phi}{\partial   x}\right)}.\end{equation} This is known. What I wonder is: Q: is it possible to compute second order partial derivatives in a systematic way? I tried to differentiate determinants using the Jacobi formula, but this leads to very complicated expressions that I cannot handle. I also expanded the determinants in ( $*$ ) along the $i$ column (by which the respective matrices differ) and tried some other approaches, but they do not seem to bring me any further. On the other hand, if a go a straightforward way and differentiate $\Phi(x,y)$ twice, I get expressions involving tensors or rather multiindex notations, because neither second order partial derivatives, nor the derivatives of type $\frac{\partial x^i}{\partial y^j}$ are actually tensors. My hope is that maybe it is still possible to extract some nice tractable expression similar to how we got ( $*$ ) from $\frac{\partial x}{\partial y}=-\left[\frac{\partial \Phi}{\partial x}\right]^{-1}\frac{\partial \Phi}{\partial y}$ ? Here is a related question . UPDATE: It seems that the problem turned out to be more difficult than I expected (although many people told me that it must have been solved by somebody). Since the hope for getting a resolutive answer fades and the bounty will expire in a couple of days, I'd gladly grant it to anybody who could point out a way to approach (if not solve) this problem. UPDATE 2: Let me expand a bit on the above. To illustrate my problem let's differentiate $\left[\frac{\partial \Phi}{\partial x}\right]^{-1}$ w.r.t. $y_i$ : \begin{multline*}\frac{\partial}{\partial y_i}\left[\frac{\partial \Phi}{\partial x}\right]^{-1}=-\left[\frac{\partial \Phi}{\partial x}\right]^{-1}\frac{\partial}{\partial y_i}\left[\frac{\partial \Phi}{\partial x}\right]\left[\frac{\partial \Phi}{\partial x}\right]^{-1}\\ =-\left[\frac{\partial \Phi}{\partial x}\right]^{-1}\left[\frac{\partial^2 \Phi}{\partial x\partial x}\right]\frac{\partial x}{\partial y_i}\left[\frac{\partial \Phi}{\partial x}\right]^{-1}-\left[\frac{\partial \Phi}{\partial x}\right]^{-1}\left[\frac{\partial^2 \Phi}{\partial y_i\partial x}\right]\left[\frac{\partial \Phi}{\partial x}\right]^{-1}.\end{multline*} So, what is $\left[\frac{\partial^2 \Phi}{\partial x\partial x}\right]\frac{\partial x}{\partial y_i}$ ? A 3D matrix multiplied with a vector? How to treat these expressions? To make the things even more complicated we should now substitute $\frac{\partial x}{\partial y_i}$ with the respective expression for the first order partial derivatives. It becomes completely obscure and I cannot recognize any structure in it.","Let be an implicit function s.t. and . This means that locally at we can express as functions of . Next, we can compute partial derivatives of as This is known. What I wonder is: Q: is it possible to compute second order partial derivatives in a systematic way? I tried to differentiate determinants using the Jacobi formula, but this leads to very complicated expressions that I cannot handle. I also expanded the determinants in ( ) along the column (by which the respective matrices differ) and tried some other approaches, but they do not seem to bring me any further. On the other hand, if a go a straightforward way and differentiate twice, I get expressions involving tensors or rather multiindex notations, because neither second order partial derivatives, nor the derivatives of type are actually tensors. My hope is that maybe it is still possible to extract some nice tractable expression similar to how we got ( ) from ? Here is a related question . UPDATE: It seems that the problem turned out to be more difficult than I expected (although many people told me that it must have been solved by somebody). Since the hope for getting a resolutive answer fades and the bounty will expire in a couple of days, I'd gladly grant it to anybody who could point out a way to approach (if not solve) this problem. UPDATE 2: Let me expand a bit on the above. To illustrate my problem let's differentiate w.r.t. : So, what is ? A 3D matrix multiplied with a vector? How to treat these expressions? To make the things even more complicated we should now substitute with the respective expression for the first order partial derivatives. It becomes completely obscure and I cannot recognize any structure in it.","\Phi(x,y)=0 \Phi:\mathbb{R}^n\times \mathbb{R}^k\rightarrow \mathbb{R}^n \det\left(\frac{\partial \Phi}{\partial 
 x}(x_0,y_0)\right)\neq 0 (x_0,y_0) x_i y x \begin{equation}\tag{*}\frac{\partial x_i}{\partial y_j}=-\frac{\det\left(\left[\frac{\partial \Phi}{\partial 
 x_1},\dots,\frac{\partial \Phi}{\partial 
 x_{i-1}}, \frac{\partial \Phi}{\partial 
 y_j}, \frac{\partial \Phi}{\partial 
 x_{i+1}},\dots, \frac{\partial \Phi}{\partial 
 x_n}\right]\right)}{\det\left(\frac{\partial \Phi}{\partial 
 x}\right)}.\end{equation} * i \Phi(x,y) \frac{\partial x^i}{\partial y^j} * \frac{\partial x}{\partial y}=-\left[\frac{\partial \Phi}{\partial x}\right]^{-1}\frac{\partial \Phi}{\partial y} \left[\frac{\partial \Phi}{\partial x}\right]^{-1} y_i \begin{multline*}\frac{\partial}{\partial y_i}\left[\frac{\partial \Phi}{\partial x}\right]^{-1}=-\left[\frac{\partial \Phi}{\partial x}\right]^{-1}\frac{\partial}{\partial y_i}\left[\frac{\partial \Phi}{\partial x}\right]\left[\frac{\partial \Phi}{\partial x}\right]^{-1}\\
=-\left[\frac{\partial \Phi}{\partial x}\right]^{-1}\left[\frac{\partial^2 \Phi}{\partial x\partial x}\right]\frac{\partial x}{\partial y_i}\left[\frac{\partial \Phi}{\partial x}\right]^{-1}-\left[\frac{\partial \Phi}{\partial x}\right]^{-1}\left[\frac{\partial^2 \Phi}{\partial y_i\partial x}\right]\left[\frac{\partial \Phi}{\partial x}\right]^{-1}.\end{multline*} \left[\frac{\partial^2 \Phi}{\partial x\partial x}\right]\frac{\partial x}{\partial y_i} \frac{\partial x}{\partial y_i}","['multivariable-calculus', 'derivatives', 'reference-request', 'partial-derivative', 'implicit-differentiation']"
38,Evaluating the Surface Integral $\iint_{x^3+y^3+z^3=a^3} \frac{\bf{x}}{||\bf{x}||} \cdot d\bf{S}$,Evaluating the Surface Integral,\iint_{x^3+y^3+z^3=a^3} \frac{\bf{x}}{||\bf{x}||} \cdot d\bf{S},"Compute the surface integral: $$\int_S({x\over \sqrt{x^2+y^2+z^2}}, {y\over \sqrt{ x^2+y^2+z^2}}, {z\over \sqrt{x^2+y^2+z^2}}) \cdot \vec n \ dS$$ where $S: x^3+y^3+z^3=a^3$ The first parametrization that came to my mind was: $r(x,y)=(x,y,(a^3-x^3-y^3)^{1/3})$ but the integral becomes very hard to compute; I also gave $$r(u,v)=(a(\cos(u)\sin(v))^{2/3},a(\sin(u)\sin(v))^{2/3},a(\cos(v))^{2/3})$$ (I was thinking about some type of spherical transformation) but then again the integral becomes vey hard to compute. Can you please help me with this problem? I would really appreciate it :)","Compute the surface integral: $$\int_S({x\over \sqrt{x^2+y^2+z^2}}, {y\over \sqrt{ x^2+y^2+z^2}}, {z\over \sqrt{x^2+y^2+z^2}}) \cdot \vec n \ dS$$ where $S: x^3+y^3+z^3=a^3$ The first parametrization that came to my mind was: $r(x,y)=(x,y,(a^3-x^3-y^3)^{1/3})$ but the integral becomes very hard to compute; I also gave $$r(u,v)=(a(\cos(u)\sin(v))^{2/3},a(\sin(u)\sin(v))^{2/3},a(\cos(v))^{2/3})$$ (I was thinking about some type of spherical transformation) but then again the integral becomes vey hard to compute. Can you please help me with this problem? I would really appreciate it :)",,"['multivariable-calculus', 'vector-analysis', 'parametric', 'surface-integrals']"
39,Munkres' Question on Manifolds,Munkres' Question on Manifolds,,"In Munkres' 'Analysis on Manifolds' on pg. 208 there's a question which reads: QUESTION: Let $f:\mathbb R^{n+k}\to \mathbb R^n$ be of class $\mathscr C^r$. Let $M$ be the set of all the points $\mathbf x$ such that $f(\mathbf x)=\mathbf 0$ and $N$ be the set of all the points $\mathbf x$ such that $$f_1(\mathbf x)=\cdots=f_{n-1}(\mathbf x)=0\text{ and } f_n(\mathbf x)\geq 0$$ Assume $M$ is non-empty. 1) Assume $\text{rank} ~ Df(\mathbf x)=n$ for all $\mathbf x\in M$ and show that $M$ is a $k$-manifold without boundary in $\mathbb R^{n+k}$. 2) Assume that the matrix $\displaystyle\frac{\partial(f_1,\ldots,f_{n-1})}{\partial \mathbf x}$ has rank $n-1$ for all $\mathbf x\in N$ and show that $N$ is a $(k+1)$-manifold with boundary in $\mathbb R^{n+k}$. I am trying to show $(2)$ and I am not sure if the hypothesis of $(1)$ is required to do that. I have approached this question using the constant rank theorem which dictates: Constant Rank Theorem: Let $U$ be open in $\mathbb R^n$ and $\mathbf a$ be any point in $U$. Let $f:U\to \mathbb R^m$ be a function of class $\mathscr C^p$ such that $\text{rank } Df(\mathbf z) =r$ for all $\mathbf z\in U$. Then there exist open sets $U_1,U_2\subseteq U$ and $V\subseteq \mathbb R^m$ such that $\mathbf a\in U_1$ and $f(\mathbf a)\in V$, and $\mathscr C^p$-diffeomorphisms $\phi:U_1\to U_2$ and $\psi:V\to V$ such that $$(\psi\circ f\circ \phi^{-1})(\mathbf z)=(z_1,\ldots,z_r,0,\ldots,0)$$ for all $\mathbf z\in U_2$. My approach to solve $(2)$ shall be clear by my solution of $(1)$: Let $\mathbf a\in M$.    We know that there exists $U$ open in $\mathbb R^{n+k}$ such that $\mathbf a\in U$ and $\text{rank }Df(\mathbf x)=n$ for all $\mathbf x\in U$.    By the Constant Rank Theorem there exists open sets $U_1$ and $U_2$ in $\mathbb R^{n+k}$ and $V$ in $\mathbb R^n$ such that $\mathbf a\in U_1\subseteq U_1$ and $f(\mathbf a)=\mathbf \in V$, along with diffeomorphisms $\phi:U_1\to U_2$ and $\psi:V\to V$ satisfying    $$(\psi\circ f\circ \phi^{-1})(\mathbf x) =(x_1,\ldots,x_n)$$    for all $\mathbf x\in U_2$.    Say $\psi(\mathbf 0)=(t_1,\ldots,t_n)$ and define $S=\{(t_1,\ldots,t_n,z_1,\ldots,z_k):z_i\in \mathbb R\}\cap U_2$. Claim 1: $\phi^{-1}(S)=M\cap U_1$. Proof: Let $\mathbf q=(t_1,\ldots,t_n,z_1,\ldots,z_k)$ be in $S$.             Then $\phi^{-1}(\mathbf q)$ obviously lies in $U_1$.            We now show that $\phi^{-1}(\mathbf q)$ lies in $M$.             Note that $(\psi\circ f\circ \phi^{-1})(\mathbf q)=(t_1,\ldots,t_n)$.             This gives $(f\circ \phi^{-1})(\mathbf q)=\psi^{-1}(t_1,\ldots,t_n)=\mathbf 0$.            This means that $f(\phi^{-1}(\mathbf q))=\mathbf 0$ and hence $\phi^{-1}(\mathbf q)$ is in $M$.            For the reverse containment assume that $\mathbf q\in M\cap U_1$.            Then $\mathbf q=\phi^{-1}(\mathbf s)$ for some $\mathbf s\in U_2$.            Also, $f(\mathbf q)=0$ since $\mathbf q\in M$.            Thus $(f\circ\phi^{-1})(\mathbf s)=\mathbf 0$.             Operating $\psi$ on both the sides we get $(\psi\circ f\circ \phi^{-1})(\mathbf s)=\psi(\mathbf 0)$.             But the LHS of the last equation is $(s_1,\ldots,s_n)$ and the RHS is $(t_1,\ldots,t_n)$.             Thus $s_i=t_i$ for $1\leq i\leq n$.             Therefore $\mathbf s\in S$ and $\mathbf q\in\phi^{-1}(S)$.             This settles the claim. Now define $T=\{(z_1,\ldots,z_k)\in\mathbb R^k: (t_1,\ldots,t_n,z_1,\ldots,z_k)\in S\}$. Claim 2: $T$ is open in $\mathbb R^k$. Proof: Define $g:\mathbb R^k\to \mathbb R^{n+k}$ as $$g(z_1,\ldots, z_k)=(t_1,\ldots,t_n,z_1,\ldots,z_k)$$             Clearly $g$ is injective and continuous.             We now show that $g^{-1}(U_2)=T$.             Note that $g^{-1}(U_2)=g^{-1}(S)$.             Let $\mathbf q\in S$.             Say $\mathbf q=(t_1,\ldots,t_n,q_1,\ldots,q_k)$ and it is obvious that $g^{-1}(\mathbf q)\in T$.             Now let $g^{-1}(\mathbf q)\in T$ for some $q\in \mathbb R^{n+k}$.             We need to show that $\mathbf q\in U_2$.             Say $g^{-1}(\mathbf q)=(b_1,\ldots,b_k)$.             Then $\mathbf q=(t_1,\ldots,t_n,b_1,\ldots,b_k)\in S$ and thus $\mathbf q\in U_2$. So we have shown that $T=g^{-1}(U_2)$.             Now since $g$ is a continuous function and $U_2$ is open in $\mathbb R^{n+k}$, we infer that $T$ is open in $\mathbb R^k$ and the claim is settled.     Now define a function $\alpha:T\to M\cap U_1$ as $$\alpha(\mathbf z)=\phi^{-1}\circ g(\mathbf z)$$     It is a trivial matter to verify that $\alpha$ is a coordinate patch about the point $\mathbf a$ in $M$ and the proof is complete. To solve $(2)$ what I did was define a function $g:\mathbb R^{n+k}\to \mathbb R^{n-1}$ as $$g(\mathbf x)=(f_1(\mathbf x),\ldots,f_{n-1}(\mathbf x))$$ Then $\text{rank }Dg(\mathbf x)=n-1$ for all $\mathbf x\in N$. Let $\mathbf z_0\in N$. I can show that there exists an open set $U\subseteq \mathbb R^{n+k}$ such that $\mathbf z_0\in U$ and $\text{rank }Dg(\mathbf z)=n-1$ for all $\mathbf z\in U$. Thereby, using the conastant rank theorem I get $U_1, U_2,\psi$ and $\phi$ such that $(\psi\circ g\circ\phi^{-1})(\mathbf x)=(x_1,\ldots,x_{n-1})$ Can somebody guide me what to do from here?","In Munkres' 'Analysis on Manifolds' on pg. 208 there's a question which reads: QUESTION: Let $f:\mathbb R^{n+k}\to \mathbb R^n$ be of class $\mathscr C^r$. Let $M$ be the set of all the points $\mathbf x$ such that $f(\mathbf x)=\mathbf 0$ and $N$ be the set of all the points $\mathbf x$ such that $$f_1(\mathbf x)=\cdots=f_{n-1}(\mathbf x)=0\text{ and } f_n(\mathbf x)\geq 0$$ Assume $M$ is non-empty. 1) Assume $\text{rank} ~ Df(\mathbf x)=n$ for all $\mathbf x\in M$ and show that $M$ is a $k$-manifold without boundary in $\mathbb R^{n+k}$. 2) Assume that the matrix $\displaystyle\frac{\partial(f_1,\ldots,f_{n-1})}{\partial \mathbf x}$ has rank $n-1$ for all $\mathbf x\in N$ and show that $N$ is a $(k+1)$-manifold with boundary in $\mathbb R^{n+k}$. I am trying to show $(2)$ and I am not sure if the hypothesis of $(1)$ is required to do that. I have approached this question using the constant rank theorem which dictates: Constant Rank Theorem: Let $U$ be open in $\mathbb R^n$ and $\mathbf a$ be any point in $U$. Let $f:U\to \mathbb R^m$ be a function of class $\mathscr C^p$ such that $\text{rank } Df(\mathbf z) =r$ for all $\mathbf z\in U$. Then there exist open sets $U_1,U_2\subseteq U$ and $V\subseteq \mathbb R^m$ such that $\mathbf a\in U_1$ and $f(\mathbf a)\in V$, and $\mathscr C^p$-diffeomorphisms $\phi:U_1\to U_2$ and $\psi:V\to V$ such that $$(\psi\circ f\circ \phi^{-1})(\mathbf z)=(z_1,\ldots,z_r,0,\ldots,0)$$ for all $\mathbf z\in U_2$. My approach to solve $(2)$ shall be clear by my solution of $(1)$: Let $\mathbf a\in M$.    We know that there exists $U$ open in $\mathbb R^{n+k}$ such that $\mathbf a\in U$ and $\text{rank }Df(\mathbf x)=n$ for all $\mathbf x\in U$.    By the Constant Rank Theorem there exists open sets $U_1$ and $U_2$ in $\mathbb R^{n+k}$ and $V$ in $\mathbb R^n$ such that $\mathbf a\in U_1\subseteq U_1$ and $f(\mathbf a)=\mathbf \in V$, along with diffeomorphisms $\phi:U_1\to U_2$ and $\psi:V\to V$ satisfying    $$(\psi\circ f\circ \phi^{-1})(\mathbf x) =(x_1,\ldots,x_n)$$    for all $\mathbf x\in U_2$.    Say $\psi(\mathbf 0)=(t_1,\ldots,t_n)$ and define $S=\{(t_1,\ldots,t_n,z_1,\ldots,z_k):z_i\in \mathbb R\}\cap U_2$. Claim 1: $\phi^{-1}(S)=M\cap U_1$. Proof: Let $\mathbf q=(t_1,\ldots,t_n,z_1,\ldots,z_k)$ be in $S$.             Then $\phi^{-1}(\mathbf q)$ obviously lies in $U_1$.            We now show that $\phi^{-1}(\mathbf q)$ lies in $M$.             Note that $(\psi\circ f\circ \phi^{-1})(\mathbf q)=(t_1,\ldots,t_n)$.             This gives $(f\circ \phi^{-1})(\mathbf q)=\psi^{-1}(t_1,\ldots,t_n)=\mathbf 0$.            This means that $f(\phi^{-1}(\mathbf q))=\mathbf 0$ and hence $\phi^{-1}(\mathbf q)$ is in $M$.            For the reverse containment assume that $\mathbf q\in M\cap U_1$.            Then $\mathbf q=\phi^{-1}(\mathbf s)$ for some $\mathbf s\in U_2$.            Also, $f(\mathbf q)=0$ since $\mathbf q\in M$.            Thus $(f\circ\phi^{-1})(\mathbf s)=\mathbf 0$.             Operating $\psi$ on both the sides we get $(\psi\circ f\circ \phi^{-1})(\mathbf s)=\psi(\mathbf 0)$.             But the LHS of the last equation is $(s_1,\ldots,s_n)$ and the RHS is $(t_1,\ldots,t_n)$.             Thus $s_i=t_i$ for $1\leq i\leq n$.             Therefore $\mathbf s\in S$ and $\mathbf q\in\phi^{-1}(S)$.             This settles the claim. Now define $T=\{(z_1,\ldots,z_k)\in\mathbb R^k: (t_1,\ldots,t_n,z_1,\ldots,z_k)\in S\}$. Claim 2: $T$ is open in $\mathbb R^k$. Proof: Define $g:\mathbb R^k\to \mathbb R^{n+k}$ as $$g(z_1,\ldots, z_k)=(t_1,\ldots,t_n,z_1,\ldots,z_k)$$             Clearly $g$ is injective and continuous.             We now show that $g^{-1}(U_2)=T$.             Note that $g^{-1}(U_2)=g^{-1}(S)$.             Let $\mathbf q\in S$.             Say $\mathbf q=(t_1,\ldots,t_n,q_1,\ldots,q_k)$ and it is obvious that $g^{-1}(\mathbf q)\in T$.             Now let $g^{-1}(\mathbf q)\in T$ for some $q\in \mathbb R^{n+k}$.             We need to show that $\mathbf q\in U_2$.             Say $g^{-1}(\mathbf q)=(b_1,\ldots,b_k)$.             Then $\mathbf q=(t_1,\ldots,t_n,b_1,\ldots,b_k)\in S$ and thus $\mathbf q\in U_2$. So we have shown that $T=g^{-1}(U_2)$.             Now since $g$ is a continuous function and $U_2$ is open in $\mathbb R^{n+k}$, we infer that $T$ is open in $\mathbb R^k$ and the claim is settled.     Now define a function $\alpha:T\to M\cap U_1$ as $$\alpha(\mathbf z)=\phi^{-1}\circ g(\mathbf z)$$     It is a trivial matter to verify that $\alpha$ is a coordinate patch about the point $\mathbf a$ in $M$ and the proof is complete. To solve $(2)$ what I did was define a function $g:\mathbb R^{n+k}\to \mathbb R^{n-1}$ as $$g(\mathbf x)=(f_1(\mathbf x),\ldots,f_{n-1}(\mathbf x))$$ Then $\text{rank }Dg(\mathbf x)=n-1$ for all $\mathbf x\in N$. Let $\mathbf z_0\in N$. I can show that there exists an open set $U\subseteq \mathbb R^{n+k}$ such that $\mathbf z_0\in U$ and $\text{rank }Dg(\mathbf z)=n-1$ for all $\mathbf z\in U$. Thereby, using the conastant rank theorem I get $U_1, U_2,\psi$ and $\phi$ such that $(\psi\circ g\circ\phi^{-1})(\mathbf x)=(x_1,\ldots,x_{n-1})$ Can somebody guide me what to do from here?",,"['multivariable-calculus', 'manifolds']"
40,Show symmetry of the critical points of this function,Show symmetry of the critical points of this function,,"I am trying to prove that the global maxima of the following function  $$f_n(x_1,\ldots,x_n):=\exp(-\sum_{i=1}^n x_i ^2)\prod_{1\leq i<j\leq n}(x_i-x_j)^2\prod_{1\leq i,j\leq n}\frac{1}{\sqrt{1+(x_i+x_j)^2}}$$ (note the second product does not have the restriction $i\lt j$) satisfy the following symmetry property: If $x^*=(x_1^*,\ldots,x_n^*)$ is a global maximum of $f_n$, then the set $\{x_1^*,\ldots,x_n^*\}$ is symmetric around the origin in the sense that if $a\in \{x_1^*,\ldots,x_n^*\}$, then $-a\in \{x_1^*,\ldots,x_n^*\}$ too. I expect this to be true, but I have no proof. I have checked it numerically for $n=2,3,4$, and I am trying to come up with a nice proof of this fact without much success. Any suggestions would be appreciated!","I am trying to prove that the global maxima of the following function  $$f_n(x_1,\ldots,x_n):=\exp(-\sum_{i=1}^n x_i ^2)\prod_{1\leq i<j\leq n}(x_i-x_j)^2\prod_{1\leq i,j\leq n}\frac{1}{\sqrt{1+(x_i+x_j)^2}}$$ (note the second product does not have the restriction $i\lt j$) satisfy the following symmetry property: If $x^*=(x_1^*,\ldots,x_n^*)$ is a global maximum of $f_n$, then the set $\{x_1^*,\ldots,x_n^*\}$ is symmetric around the origin in the sense that if $a\in \{x_1^*,\ldots,x_n^*\}$, then $-a\in \{x_1^*,\ldots,x_n^*\}$ too. I expect this to be true, but I have no proof. I have checked it numerically for $n=2,3,4$, and I am trying to come up with a nice proof of this fact without much success. Any suggestions would be appreciated!",,['multivariable-calculus']
41,Can you approximate a vector field?,Can you approximate a vector field?,,"Say you have a physical simulation, there are ""wind current"" vectors stored in a 2d space. So you know that the vectors near each other will likely be similar in direction. Can we capitalize on the ""similarity"" across the vector field, and use it to write an approximation to the vector field? So is there an alternative way to represent a vector field (something like a Fourier Transform for vector fields?)","Say you have a physical simulation, there are ""wind current"" vectors stored in a 2d space. So you know that the vectors near each other will likely be similar in direction. Can we capitalize on the ""similarity"" across the vector field, and use it to write an approximation to the vector field? So is there an alternative way to represent a vector field (something like a Fourier Transform for vector fields?)",,['multivariable-calculus']
42,"Multivariable Integral, How to compute it?","Multivariable Integral, How to compute it?",,"Q How to evaluate a multivariate integral with a Gaussian weight function? $$ \mathcal{Z_{n}} \equiv\int_{-\infty}^{\infty} \exp\left(-a\sum_{j = 1}^{n}x_{j}^2\right)\, {\rm f}\left(x_{1},x_{2},\ldots,x_{n}\right)\, {\rm d}x_{1}\,{\rm d}x_{2}\ldots{\rm d}x_{n} $$ Where $\displaystyle{% {\rm f}(x_{1},x_{2},\ldots,x_{n}) =\prod_{j} {1 \over \sqrt{1 + {\rm i}\,b\left(x_{j}^2-x_{j+1}^2\right)^2}}}$ . I need a hint to solve this integral and this is how I proceeded: $$ \mathcal{Z_{n}}= \int_{-\infty}^{\infty}\prod_{j=1}^{n}{\rm d}x_{j}\, \exp\left(% -\,{a \over 2}\sum_{j = 1}^{n}x_{j}^{2} - {1 \over 2} \log\left(1 + {\rm i}\,b\left[x_{j}^{2} - x_{j + 1}^{2}\right]^{2}\right)\right) $$ Now the integral is of the form of the canonical partition function integrated over the configuration space. Hence the integral can be identified as an $n-$ particle partition of the canonical ensemble, which is given by $$ \mathcal{Z}_{n}= \int_{-\infty}^{\infty}\prod_{j = 1}^{n}{\rm d}x_{j}\,  {\rm e}^{-\beta{\cal H}}, $$ Where $$\mathcal{H}=\Bigg(-\frac{a}{2}\sum_{j=1}^{n}x_{j}^2-\frac{1}{2}\log{\Big(1+i b(x_{j}^2-x_{j+1}^2)^{2}\Big)\Bigg)}.$$ Then I got stuck. How to proceed? Thanks.","Q How to evaluate a multivariate integral with a Gaussian weight function? Where . I need a hint to solve this integral and this is how I proceeded: Now the integral is of the form of the canonical partition function integrated over the configuration space. Hence the integral can be identified as an particle partition of the canonical ensemble, which is given by Where Then I got stuck. How to proceed? Thanks.","
\mathcal{Z_{n}}
\equiv\int_{-\infty}^{\infty}
\exp\left(-a\sum_{j = 1}^{n}x_{j}^2\right)\,
{\rm f}\left(x_{1},x_{2},\ldots,x_{n}\right)\,
{\rm d}x_{1}\,{\rm d}x_{2}\ldots{\rm d}x_{n}
 \displaystyle{%
{\rm f}(x_{1},x_{2},\ldots,x_{n}) =\prod_{j}
{1 \over \sqrt{1 + {\rm i}\,b\left(x_{j}^2-x_{j+1}^2\right)^2}}} 
\mathcal{Z_{n}}=
\int_{-\infty}^{\infty}\prod_{j=1}^{n}{\rm d}x_{j}\,
\exp\left(%
-\,{a \over 2}\sum_{j = 1}^{n}x_{j}^{2} - {1 \over 2}
\log\left(1 + {\rm i}\,b\left[x_{j}^{2} - x_{j + 1}^{2}\right]^{2}\right)\right)
 n- 
\mathcal{Z}_{n}= \int_{-\infty}^{\infty}\prod_{j = 1}^{n}{\rm d}x_{j}\, 
{\rm e}^{-\beta{\cal H}},
 \mathcal{H}=\Bigg(-\frac{a}{2}\sum_{j=1}^{n}x_{j}^2-\frac{1}{2}\log{\Big(1+i b(x_{j}^2-x_{j+1}^2)^{2}\Big)\Bigg)}.","['multivariable-calculus', 'mathematical-physics', 'gaussian-integral', 'quantum-field-theory']"
43,Singular jacobian matrix?,Singular jacobian matrix?,,"I have a series of questions, in various degrees of befuddled muddledness (and they are related to my previous questions: this and this ) First question: how do I do a change of variable if the determinant of the jacobian is singular? The setting for this question is as follows: I have one $n$-dimensional standard gaussian random variable $u \sim N(0,I)$ and a fixed $v \in \mathbb{R}^n$. Then I define the random variable $$z = u - \frac{u^Tv}{v^Tv}v$$ and I'd like to derive a density for $z$. So the Jacobian: $$\frac{dz}{du} = I - \frac{vv^T}{v^Tv}$$ which turns out to be singular and so $|\frac{dz}{du}|=0$. Does this mean trying to do a change of variable is fundamentally wrong here? Or is there a way to do this? Second question: Aside from the Jacobian, I'm not sure how to change a standard normal distribution on $u$ to a distribution on $z$. So if the density on $u$ is $$\frac{1}{\sqrt{2\pi}}\exp(-u^Tu/2)$$ is there an inverse function $z^{-1}$ such that $z^{-1}(z) = u$? Then (I think) $$\frac{1}{\sqrt{2\pi}}\exp(-u^Tu/2) du = \frac{1}{\sqrt{2\pi}}\exp(-z^{-T}z^{-1}/2) \left|\frac{dz}{du}\right| du$$ So, is there such a $z^{-1}$? And if there is, what is it? Third question: ultimately, I'm trying to answer this question . Am I going about this the right way by asking the two questions above? (The person who replied to my question there says something about Haar measure which I'd never heard of before so it's not enlightening to me as a proof.)","I have a series of questions, in various degrees of befuddled muddledness (and they are related to my previous questions: this and this ) First question: how do I do a change of variable if the determinant of the jacobian is singular? The setting for this question is as follows: I have one $n$-dimensional standard gaussian random variable $u \sim N(0,I)$ and a fixed $v \in \mathbb{R}^n$. Then I define the random variable $$z = u - \frac{u^Tv}{v^Tv}v$$ and I'd like to derive a density for $z$. So the Jacobian: $$\frac{dz}{du} = I - \frac{vv^T}{v^Tv}$$ which turns out to be singular and so $|\frac{dz}{du}|=0$. Does this mean trying to do a change of variable is fundamentally wrong here? Or is there a way to do this? Second question: Aside from the Jacobian, I'm not sure how to change a standard normal distribution on $u$ to a distribution on $z$. So if the density on $u$ is $$\frac{1}{\sqrt{2\pi}}\exp(-u^Tu/2)$$ is there an inverse function $z^{-1}$ such that $z^{-1}(z) = u$? Then (I think) $$\frac{1}{\sqrt{2\pi}}\exp(-u^Tu/2) du = \frac{1}{\sqrt{2\pi}}\exp(-z^{-T}z^{-1}/2) \left|\frac{dz}{du}\right| du$$ So, is there such a $z^{-1}$? And if there is, what is it? Third question: ultimately, I'm trying to answer this question . Am I going about this the right way by asking the two questions above? (The person who replied to my question there says something about Haar measure which I'd never heard of before so it's not enlightening to me as a proof.)",,"['multivariable-calculus', 'probability-distributions']"
44,"Is $f(x,y) = f(\mathbf{x})$ abuse of notation?",Is  abuse of notation?,"f(x,y) = f(\mathbf{x})","A scalar function $f(x,y)$ is often written as $f(\mathbf{x})$, where $\mathbf{x} = (x,y)$, but as far as I know, there is a difference between the scalar function inputs $(x,y)$ and the vector input $(x,y) = x\imath+y\jmath$. As I see it $f(\mathbf{x}) = f((x,y)) = f(x\imath+y\jmath) \neq f(x,y)$. Am I wrong, or is there a simple bijection between the two concepts? Is it simply shorthand for $f': \mathbf{x} \mapsto f''(\imath\cdot\mathbf{x},\jmath\cdot\mathbf{x})$, s.t. $f'(\mathbf{x})= f''(x,y)$? If it's of an relevance, I'm reading about scalar fields, and this definition came up: $\displaystyle\dfrac{\partial f}{\partial x}(x,y) = \lim_{h\to 0} \dfrac{f(x+h, y)-f(x,y)}{h} \overset{\color{green}{?}}{=} \dfrac{f(\mathbf{x}+h\imath)-f(\mathbf{x})}{h} = \dfrac{\partial f}{\partial\imath}(\mathbf{x})$ While it looks nice I'm just curious if it's correct. However I don't see how $f$ can be differentiated with respect to both $\imath$ (a vector) and $x$ (a scalar), unless it's actually two different functions $f'$ and $f''$...","A scalar function $f(x,y)$ is often written as $f(\mathbf{x})$, where $\mathbf{x} = (x,y)$, but as far as I know, there is a difference between the scalar function inputs $(x,y)$ and the vector input $(x,y) = x\imath+y\jmath$. As I see it $f(\mathbf{x}) = f((x,y)) = f(x\imath+y\jmath) \neq f(x,y)$. Am I wrong, or is there a simple bijection between the two concepts? Is it simply shorthand for $f': \mathbf{x} \mapsto f''(\imath\cdot\mathbf{x},\jmath\cdot\mathbf{x})$, s.t. $f'(\mathbf{x})= f''(x,y)$? If it's of an relevance, I'm reading about scalar fields, and this definition came up: $\displaystyle\dfrac{\partial f}{\partial x}(x,y) = \lim_{h\to 0} \dfrac{f(x+h, y)-f(x,y)}{h} \overset{\color{green}{?}}{=} \dfrac{f(\mathbf{x}+h\imath)-f(\mathbf{x})}{h} = \dfrac{\partial f}{\partial\imath}(\mathbf{x})$ While it looks nice I'm just curious if it's correct. However I don't see how $f$ can be differentiated with respect to both $\imath$ (a vector) and $x$ (a scalar), unless it's actually two different functions $f'$ and $f''$...",,"['multivariable-calculus', 'notation', 'vectors']"
45,divergence of the cross product of two vectors proof [duplicate],divergence of the cross product of two vectors proof [duplicate],,This question already has answers here : Verify the following relationship: $\nabla \cdot (a \times b) = b \cdot \nabla \times a - a \cdot \nabla \times b $ (2 answers) Closed 2 years ago . Prove $\vec{\nabla} \cdot\left ( \vec{A}\times\vec{B} \right )=\vec{B}\cdot\left ( \nabla \times\vec{A} \right )-\vec{A}\cdot\left ( \nabla \times\vec{B} \right )$ I have expanded the LHS for this and obtain a horrible expression that would be even tedious to put them into latex form without a headache. I attempted to rearrange the LHS so as to obtain the RHS but to no avail. any help is appreciated.,This question already has answers here : Verify the following relationship: $\nabla \cdot (a \times b) = b \cdot \nabla \times a - a \cdot \nabla \times b $ (2 answers) Closed 2 years ago . Prove $\vec{\nabla} \cdot\left ( \vec{A}\times\vec{B} \right )=\vec{B}\cdot\left ( \nabla \times\vec{A} \right )-\vec{A}\cdot\left ( \nabla \times\vec{B} \right )$ I have expanded the LHS for this and obtain a horrible expression that would be even tedious to put them into latex form without a headache. I attempted to rearrange the LHS so as to obtain the RHS but to no avail. any help is appreciated.,,"['multivariable-calculus', 'vector-analysis']"
46,Why does the magnitude of the cross-product of a and b give the area of a parallelogram spanned by a and b?,Why does the magnitude of the cross-product of a and b give the area of a parallelogram spanned by a and b?,,"I tried looking it up but many websites just state it without proof and without intuition.  I'm hoping to learn it a little bit better so that I don't forget how to compute the Jacobian when working with surface integrals where the divergence theorem is not applicable. If you have a good online reference instead, please feel free to provide it :-) Thanks,","I tried looking it up but many websites just state it without proof and without intuition.  I'm hoping to learn it a little bit better so that I don't forget how to compute the Jacobian when working with surface integrals where the divergence theorem is not applicable. If you have a good online reference instead, please feel free to provide it :-) Thanks,",,"['multivariable-calculus', 'area', 'cross-product', 'surface-integrals']"
47,How to create a vector field whose Curl and Divergence are zero at any point?,How to create a vector field whose Curl and Divergence are zero at any point?,,What is the mathematical procedure to derive a vector field whose curl and divergence are zero at any point at any time? Edit: Please explain it by solving the differential equations of curl and divergence.,What is the mathematical procedure to derive a vector field whose curl and divergence are zero at any point at any time? Edit: Please explain it by solving the differential equations of curl and divergence.,,"['multivariable-calculus', 'partial-differential-equations']"
48,Why not use two vectors to define a plane instead of a point and a normal vector?,Why not use two vectors to define a plane instead of a point and a normal vector?,,"In Multivariable calculus, it seems planes are defined by a point and a vector normal to the plane. That makes sense, but whereas a single vector clearly isn't enough to define a plane, aren't two non-colinear vectors enough? What I'm thinking is that since we need the cross-product of two vectors to find our normal vector in the first place, why not just use those two vectors to define our plane. After all, don't two non-colinear vectors define a basis in R2?","In Multivariable calculus, it seems planes are defined by a point and a vector normal to the plane. That makes sense, but whereas a single vector clearly isn't enough to define a plane, aren't two non-colinear vectors enough? What I'm thinking is that since we need the cross-product of two vectors to find our normal vector in the first place, why not just use those two vectors to define our plane. After all, don't two non-colinear vectors define a basis in R2?",,"['multivariable-calculus', 'plane-curves']"
49,An inequality for polynomials with positives coefficients,An inequality for polynomials with positives coefficients,,"I have found in my old paper this theorem : Let $a_i>0$ be real numbers and $x,y>0$ then we have : $$(x+y)f\Big(\frac{x^2+y^2}{x+y}\Big)(f(x)+f(y))\geq 2(xf(x)+yf(y))f\Big(\frac{x+y}{2}\Big)$$ Where : $$f(x)=\sum_{i=0}^{n}a_ix^i$$ The problem is I can't find the proof I made before . Furthermore I don't know if it's true but I have checked this inequality a week with Pari\Gp and random polynomials defined as before . So first I just want a counter-examples if it exists . If it's true if think it's a little bit hard to prove . I have tried the power series but without success .  Finally  it's a refinement of Jensen's inequality for polynomials with positives coefficients . Thanks a lot if you have a hint or a counter-example . Ps:I continue to check this  and the equality case is to $x=y$",I have found in my old paper this theorem : Let be real numbers and then we have : Where : The problem is I can't find the proof I made before . Furthermore I don't know if it's true but I have checked this inequality a week with Pari\Gp and random polynomials defined as before . So first I just want a counter-examples if it exists . If it's true if think it's a little bit hard to prove . I have tried the power series but without success .  Finally  it's a refinement of Jensen's inequality for polynomials with positives coefficients . Thanks a lot if you have a hint or a counter-example . Ps:I continue to check this  and the equality case is to,"a_i>0 x,y>0 (x+y)f\Big(\frac{x^2+y^2}{x+y}\Big)(f(x)+f(y))\geq 2(xf(x)+yf(y))f\Big(\frac{x+y}{2}\Big) f(x)=\sum_{i=0}^{n}a_ix^i x=y","['multivariable-calculus', 'inequality', 'polynomials', 'examples-counterexamples', 'jensen-inequality']"
50,Firm Non Expansiveness in the Context of Proximal Mapping / Proximal Operators,Firm Non Expansiveness in the Context of Proximal Mapping / Proximal Operators,,"$\newcommand{\prox}{\operatorname{prox}}$ Probably the most remarkable property of the proximal operator is the fixed point property: The point $x^*$ minimizes $f$ if and only if $x^* = \prox_f(x^*) $ So, indeed, $f$ can be minimized by find a fixed point of its proximal operator.  See Proximal Algorithms by Neal Parikh and Stephen Boyd . Question 1 . In the paper given above, author is saying: If $\prox_f$ were a contraction, i.e., Lipschitz continuous with constant less than $1$ , repeatedly applying $\prox_f$ would find a (here, unique) fixed point Why the bound on the first-order derivative guarantees finding a fixed point by repeatedly applying proximal operator? Question 2 . Let me quote a paragraph from the same paper: It turns out that while $\prox_f$ need not be a contraction (unless $f$ is strongly convex), it does have a different property, firm nonexpansiveness, sufficient for fixed point iteration: $\|\prox_f(x) - \prox_f(y)\|^2_2 \le (x-y)^T (\prox_f(x) - \prox_f(y))$ $\forall x,y \in \mathbb{R}^n$ Firmly nonexpansive operators are special cases of nonexpansive operators (those that are Lipschitz continuous with constant 1). Iteration of a general nonexpansive operator need not converge to a fixed point: consider operators like $-I$ or rotations. However, it tunrs out that if $N$ is nonexpansive, then the operator $T = (1-\alpha)I + \alpha N$ , where $\alpha \in (0,1)$ has the same fixed points as $N$ and simple iteration of $T$ will converge to a fixed point of $T$ (and thus of $N$ ), i.e. the sequence: $x^{k+1} := (1-\alpha)x^k +\alpha N(x^k)$ will converge to a fixed point of $N$ . Put differently, damped iteration of a nonexpansive operator will converge to one of its fixed points. Operators in the form $(1-\alpha)I + \alpha N$ , where $N$ is non-expansive and $\alpha \in (0,1)$ , are called $\alpha$ -averaged operators. Firmly nonexpansive operators are averaged: indeed, they are precisely the (1/2)-averaged operators. Why ""unless $f$ is strongly convex""? What is the intuition behind the given expression for firm nonexpansiveness? How can you show that firm nonexpansive operators are $\alpha$ -averaged with $\alpha = \frac{1}{2}$ ? Is anyone aware of the proof of why proximal map is firm nonexpansive?","Probably the most remarkable property of the proximal operator is the fixed point property: The point minimizes if and only if So, indeed, can be minimized by find a fixed point of its proximal operator.  See Proximal Algorithms by Neal Parikh and Stephen Boyd . Question 1 . In the paper given above, author is saying: If were a contraction, i.e., Lipschitz continuous with constant less than , repeatedly applying would find a (here, unique) fixed point Why the bound on the first-order derivative guarantees finding a fixed point by repeatedly applying proximal operator? Question 2 . Let me quote a paragraph from the same paper: It turns out that while need not be a contraction (unless is strongly convex), it does have a different property, firm nonexpansiveness, sufficient for fixed point iteration: Firmly nonexpansive operators are special cases of nonexpansive operators (those that are Lipschitz continuous with constant 1). Iteration of a general nonexpansive operator need not converge to a fixed point: consider operators like or rotations. However, it tunrs out that if is nonexpansive, then the operator , where has the same fixed points as and simple iteration of will converge to a fixed point of (and thus of ), i.e. the sequence: will converge to a fixed point of . Put differently, damped iteration of a nonexpansive operator will converge to one of its fixed points. Operators in the form , where is non-expansive and , are called -averaged operators. Firmly nonexpansive operators are averaged: indeed, they are precisely the (1/2)-averaged operators. Why ""unless is strongly convex""? What is the intuition behind the given expression for firm nonexpansiveness? How can you show that firm nonexpansive operators are -averaged with ? Is anyone aware of the proof of why proximal map is firm nonexpansive?","\newcommand{\prox}{\operatorname{prox}} x^* f x^* = \prox_f(x^*)  f \prox_f 1 \prox_f \prox_f f \|\prox_f(x) - \prox_f(y)\|^2_2 \le (x-y)^T (\prox_f(x) - \prox_f(y)) \forall x,y \in \mathbb{R}^n -I N T = (1-\alpha)I + \alpha N \alpha \in (0,1) N T T N x^{k+1} := (1-\alpha)x^k +\alpha N(x^k) N (1-\alpha)I + \alpha N N \alpha \in (0,1) \alpha f \alpha \alpha = \frac{1}{2}","['multivariable-calculus', 'convex-analysis', 'convex-optimization', 'fixed-point-theorems', 'proximal-operators']"
51,If $\mathrm{Int}(\mathrm{Im}(f(\mathbb {R^n})))=\varnothing$ then the determinant of the Jacobian matrix is zero.,If  then the determinant of the Jacobian matrix is zero.,\mathrm{Int}(\mathrm{Im}(f(\mathbb {R^n})))=\varnothing,"I need some light in this exercise: ""If $f:\mathbb {R^n}\rightarrow \mathbb {R^n}$ is continuous differentiable and the image of $f(\mathbb {R^n})$ has $\text{int} =\varnothing$ then the determinant of the Jacobian matrix is zero."" I really don't know where to start... I appreciate any help that helps me to get started.","I need some light in this exercise: ""If $f:\mathbb {R^n}\rightarrow \mathbb {R^n}$ is continuous differentiable and the image of $f(\mathbb {R^n})$ has $\text{int} =\varnothing$ then the determinant of the Jacobian matrix is zero."" I really don't know where to start... I appreciate any help that helps me to get started.",,['multivariable-calculus']
52,"Piecewise smooth, non-orientable, closed-surface: a contradiction in terms, or am I going mad?","Piecewise smooth, non-orientable, closed-surface: a contradiction in terms, or am I going mad?",,"We had a lecture a few weeks back, looking at Gauss' divergence theorem, and in the definition of the theorem, it specified that the boundary of the volume under consideration, S, had to be a 'piecewise smooth, orientable, closed surface'. What bothers/intrigues me is that I cannot understand how a closed surface in 3D space CANNOT be orientable. Surely every closed surface is orientable! My highly non-rigorous, intuitive argument runs as follows: 1) As the surface is closed, we can define two regions, one inside the surface, and one outside 2) We can construct a normal to the surface at any point P that is pointing towards the inside region. Thus the direction of the normal is defined for every point. 3) As the surface is piecewise continuous, this normal will vary continuously. 4) Coupling (2) (defined direction of normal) with (3) (continuously changing normal) gives us an orientation for the closed surface. 5) Therefore every closed surface is orientable. But of course, the precise wording of the statement for Gauss' Law strongly suggests that people far smarter than me have discovered some exotic non-orientable, closed surface. Is this true? When I asked my lecturer about this, he just smiled and said he didn't know any examples, but that they do exist, and then said something even more tantalising about 'reflections of higher dimensional objects' I would love it if anyone could shed some light on my situation. Thanks","We had a lecture a few weeks back, looking at Gauss' divergence theorem, and in the definition of the theorem, it specified that the boundary of the volume under consideration, S, had to be a 'piecewise smooth, orientable, closed surface'. What bothers/intrigues me is that I cannot understand how a closed surface in 3D space CANNOT be orientable. Surely every closed surface is orientable! My highly non-rigorous, intuitive argument runs as follows: 1) As the surface is closed, we can define two regions, one inside the surface, and one outside 2) We can construct a normal to the surface at any point P that is pointing towards the inside region. Thus the direction of the normal is defined for every point. 3) As the surface is piecewise continuous, this normal will vary continuously. 4) Coupling (2) (defined direction of normal) with (3) (continuously changing normal) gives us an orientation for the closed surface. 5) Therefore every closed surface is orientable. But of course, the precise wording of the statement for Gauss' Law strongly suggests that people far smarter than me have discovered some exotic non-orientable, closed surface. Is this true? When I asked my lecturer about this, he just smiled and said he didn't know any examples, but that they do exist, and then said something even more tantalising about 'reflections of higher dimensional objects' I would love it if anyone could shed some light on my situation. Thanks",,"['differential-geometry', 'multivariable-calculus']"
53,Inverse of a multivariable function,Inverse of a multivariable function,,"I was trying to find the inverse of $f(x,y)=(u,v)=(\sqrt{x+y},\sqrt{x-y})$ and I found that $x=\frac{u^2+v^2}{2}, y=\frac{u^2-v^2}{2}$. This expression seems well defined to me, but how can I be certain that $f$ really has an inverse? Have I proven the existence of an inverse just by finding this expression, or have I missed some important difference between invertibility in one variable and invertibility in multiple variables? Also, can I run into trouble if I fail to specify the domain/image of these functions?","I was trying to find the inverse of $f(x,y)=(u,v)=(\sqrt{x+y},\sqrt{x-y})$ and I found that $x=\frac{u^2+v^2}{2}, y=\frac{u^2-v^2}{2}$. This expression seems well defined to me, but how can I be certain that $f$ really has an inverse? Have I proven the existence of an inverse just by finding this expression, or have I missed some important difference between invertibility in one variable and invertibility in multiple variables? Also, can I run into trouble if I fail to specify the domain/image of these functions?",,['multivariable-calculus']
54,"Minimize $P=\dfrac{\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}}{\sqrt{ab+bc+ca+6}},$ if $a+b+c+abc=4.$",Minimize  if,"P=\dfrac{\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}}{\sqrt{ab+bc+ca+6}}, a+b+c+abc=4.","Problem. Let $a,b,c\ge 0: a+b+c+abc=4.$ Find minimal value of $P$ $$P=\frac{\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}}{\sqrt{ab+bc+ca+6}}.$$ Source: Vo Quoc Ba Can. My attempt: Set $$P(a,b,c)=\frac{\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}}{\sqrt{ab+bc+ca+6}}.$$ After some calculating works, I've tried prove $P(1,1,1)$ is the minimal value. It means that we need to prove $$\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}\ge\sqrt{2}\cdot\sqrt{ab+bc+ca+6}.$$ By squaring both side, it remains to prove $$a+b+c+3+2\sum_{cyc}\sqrt{(a+1)(b+1)}\ge 2(ab+bc+ca+6),$$ or $$2\sum_{cyc}\sqrt{(a+1)(b+1)}\ge 5+abc+2(ab+bc+ca).$$ I was stucked here. Hope to see some ideas to continue the idea. Also, all answer and comment are welcome. Thank for your attention. Updated edit: Thank you @138 Aspen. I made a mistake in calculating. Minimum should be $$P(0,2,2)=\frac{2\sqrt{30}+\sqrt{10}}{10}.$$ It means that we need to prove $$\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}\ge\frac{2\sqrt{30}+\sqrt{10}}{10}\cdot\sqrt{ab+bc+ca+6},$$ which seems ugly. Hope MV (mixing variables) technique works.","Problem. Let Find minimal value of Source: Vo Quoc Ba Can. My attempt: Set After some calculating works, I've tried prove is the minimal value. It means that we need to prove By squaring both side, it remains to prove or I was stucked here. Hope to see some ideas to continue the idea. Also, all answer and comment are welcome. Thank for your attention. Updated edit: Thank you @138 Aspen. I made a mistake in calculating. Minimum should be It means that we need to prove which seems ugly. Hope MV (mixing variables) technique works.","a,b,c\ge 0: a+b+c+abc=4. P P=\frac{\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}}{\sqrt{ab+bc+ca+6}}. P(a,b,c)=\frac{\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}}{\sqrt{ab+bc+ca+6}}. P(1,1,1) \sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}\ge\sqrt{2}\cdot\sqrt{ab+bc+ca+6}. a+b+c+3+2\sum_{cyc}\sqrt{(a+1)(b+1)}\ge 2(ab+bc+ca+6), 2\sum_{cyc}\sqrt{(a+1)(b+1)}\ge 5+abc+2(ab+bc+ca). P(0,2,2)=\frac{2\sqrt{30}+\sqrt{10}}{10}. \sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}\ge\frac{2\sqrt{30}+\sqrt{10}}{10}\cdot\sqrt{ab+bc+ca+6},","['multivariable-calculus', 'inequality', 'maxima-minima']"
55,Why is arc length not a differential form?,Why is arc length not a differential form?,,I read that the arc length is not a differential form. But I don't understand why it isn't. I understand that differential forms are integrands and arc length is an expression which is integrable. What property of differential form does it not satisfy?,I read that the arc length is not a differential form. But I don't understand why it isn't. I understand that differential forms are integrands and arc length is an expression which is integrable. What property of differential form does it not satisfy?,,"['multivariable-calculus', 'differential-geometry', 'riemannian-geometry', 'differential-forms']"
56,Is there analytical solution to this heat equation?,Is there analytical solution to this heat equation?,,"I have a PDE of the following form: $$\frac{1}{\sin\theta}\frac{\partial}{\partial\theta}\left(\sin\theta\frac{\partial f}{\partial\theta}\right)+\frac{1}{\sin^2\theta}\frac{\partial^2f}{\partial\phi^2} = A\cos\theta\,\max(\cos\phi, 0) + B-Cf^4~.$$ Does anyone know if an analytical solution exists for this equation? We can assume periodic boundary condition such that $f(\theta,0)=f(\theta, 2\pi)$ .",I have a PDE of the following form: Does anyone know if an analytical solution exists for this equation? We can assume periodic boundary condition such that .,"\frac{1}{\sin\theta}\frac{\partial}{\partial\theta}\left(\sin\theta\frac{\partial f}{\partial\theta}\right)+\frac{1}{\sin^2\theta}\frac{\partial^2f}{\partial\phi^2} = A\cos\theta\,\max(\cos\phi, 0) + B-Cf^4~. f(\theta,0)=f(\theta, 2\pi)","['multivariable-calculus', 'partial-differential-equations', 'heat-equation', 'elliptic-equations']"
57,Minimizing $ \sum\limits_{cyc}\frac{\sqrt{5a+8bc}}{8a+5bc}$ with $ \sum\limits_{cyc}ab=1$,Minimizing  with, \sum\limits_{cyc}\frac{\sqrt{5a+8bc}}{8a+5bc}  \sum\limits_{cyc}ab=1,"Olympiad inequality. Let $a,b,c\ge 0: ab+bc+ca=1.$ Find the minimal value $P$ of $$f:=\frac{\sqrt{5a+8bc}}{8a+5bc}+\frac{\sqrt{5b+8ca}}{8b+5ca}+\frac{\sqrt{5c+8ab}}{8c+5ab}.$$ Note: Often Stack Exchange asks to show some work before answering the question. This inequality was used as a proposed problem for the National TST of an Asian country a few years back. However, upon receiving the official solution, the committee decided to drop this problem immediately. They didn't believe that any student can solve this problem in the $3$ hour timeframe. It seems that minimum is achieved at $(a,b,c)=(0,1,1).$ I've tried to prove $$f\ge \frac{\sqrt{5}}{4}+\frac{2\sqrt{2}}{5}. \tag{1}$$ A big problem around here is $a=b=c=\dfrac{\sqrt{3}}{3}$ since $LHS_{(1)}-RHS_{(1)}\approx 0.000151$ I hope to see some ideas. Thank you! Update 1: There are two answers and RiverLi's proof is good but not easy to full it by hand. Update 2: You can see also here.","Olympiad inequality. Let Find the minimal value of Note: Often Stack Exchange asks to show some work before answering the question. This inequality was used as a proposed problem for the National TST of an Asian country a few years back. However, upon receiving the official solution, the committee decided to drop this problem immediately. They didn't believe that any student can solve this problem in the hour timeframe. It seems that minimum is achieved at I've tried to prove A big problem around here is since I hope to see some ideas. Thank you! Update 1: There are two answers and RiverLi's proof is good but not easy to full it by hand. Update 2: You can see also here.","a,b,c\ge 0: ab+bc+ca=1. P f:=\frac{\sqrt{5a+8bc}}{8a+5bc}+\frac{\sqrt{5b+8ca}}{8b+5ca}+\frac{\sqrt{5c+8ab}}{8c+5ab}. 3 (a,b,c)=(0,1,1). f\ge \frac{\sqrt{5}}{4}+\frac{2\sqrt{2}}{5}. \tag{1} a=b=c=\dfrac{\sqrt{3}}{3} LHS_{(1)}-RHS_{(1)}\approx 0.000151","['multivariable-calculus', 'inequality', 'optimization', 'contest-math', 'symmetric-polynomials']"
58,"Identities with Div, Grad, Curl","Identities with Div, Grad, Curl",,In physics there are lots of identities like: $$\nabla \times (\nabla \times A) = \nabla (\nabla \cdot A) - (\nabla \cdot \nabla) A$$ I'm wondering if there is an algorithmic algebraic method to prove and/or derive these identities (something like using $e^{i\theta}$ to prove trigonometric identities)?,In physics there are lots of identities like: $$\nabla \times (\nabla \times A) = \nabla (\nabla \cdot A) - (\nabla \cdot \nabla) A$$ I'm wondering if there is an algorithmic algebraic method to prove and/or derive these identities (something like using $e^{i\theta}$ to prove trigonometric identities)?,,['multivariable-calculus']
59,How to estimate the size of the neighborhoods in the Inverse Function Theorem,How to estimate the size of the neighborhoods in the Inverse Function Theorem,,Given a function $f:U \subset V\to W$ such that $\textbf{D}f(x_0)\neq 0$ for some $x_0$. How to estimate the neighborhood for which it's invertible?  Assuming the second derivative exists and is continuous.,Given a function $f:U \subset V\to W$ such that $\textbf{D}f(x_0)\neq 0$ for some $x_0$. How to estimate the neighborhood for which it's invertible?  Assuming the second derivative exists and is continuous.,,['multivariable-calculus']
60,When does order of partial derivatives matter?,When does order of partial derivatives matter?,,"I've taken multivariate calculus and am wondering if I can see a specific function where the order of taking the partial derivative matters. I've been told that there are some exceptions where $ \dfrac{\partial ^2 f}{\partial x \partial y} \ne \dfrac{\partial ^2 f}{\partial y \partial x} $, so I'm curious to see what this looks like. EDIT: And why would this true?","I've taken multivariate calculus and am wondering if I can see a specific function where the order of taking the partial derivative matters. I've been told that there are some exceptions where $ \dfrac{\partial ^2 f}{\partial x \partial y} \ne \dfrac{\partial ^2 f}{\partial y \partial x} $, so I'm curious to see what this looks like. EDIT: And why would this true?",,"['multivariable-calculus', 'partial-derivative']"
61,Composition Rule for Laplacian,Composition Rule for Laplacian,,Is there a easy composition Rule for Laplacian? Assume that $$u : \mathbb{R}^2 \rightarrow \mathbb{R}$$ $$I : \mathbb{R}^2 \rightarrow \mathbb{R}^2$$ so what will be $$Δu(I(x))=...$$,Is there a easy composition Rule for Laplacian? Assume that $$u : \mathbb{R}^2 \rightarrow \mathbb{R}$$ $$I : \mathbb{R}^2 \rightarrow \mathbb{R}^2$$ so what will be $$Δu(I(x))=...$$,,['multivariable-calculus']
62,Proving that a strongly convex function is coercive,Proving that a strongly convex function is coercive,,"I am having trouble with this proof. I am given the following 2 definitions: 1) A function $f$ is coercive if $\lim_{||x|| \rightarrow \infty} f(x) = \infty$ 2) A $C^2$ function $f$ is strongly convex if there exists a constant $c_0 > 0$ such that:  $(x - y)^T (\nabla f(x) - \nabla f(y)) \geq c_0 ||x - y||^2 \hspace{5mm}$ $\forall x,y \in \mathbb{R}^n$ The question is to show that if $f$ is strongly convex then it is coercive. I am only allowed to use basic theorems such as Taylor expansion, triangle inequality etc. However, I can use the fact that a function is coercive $\iff$ all its level sets are compact. My instinct is that the answer can be obtained by a doing a Taylor expansion and manipulating the result, but I've been stuck for days using this approach. Any help would be greatly appreciated.","I am having trouble with this proof. I am given the following 2 definitions: 1) A function $f$ is coercive if $\lim_{||x|| \rightarrow \infty} f(x) = \infty$ 2) A $C^2$ function $f$ is strongly convex if there exists a constant $c_0 > 0$ such that:  $(x - y)^T (\nabla f(x) - \nabla f(y)) \geq c_0 ||x - y||^2 \hspace{5mm}$ $\forall x,y \in \mathbb{R}^n$ The question is to show that if $f$ is strongly convex then it is coercive. I am only allowed to use basic theorems such as Taylor expansion, triangle inequality etc. However, I can use the fact that a function is coercive $\iff$ all its level sets are compact. My instinct is that the answer can be obtained by a doing a Taylor expansion and manipulating the result, but I've been stuck for days using this approach. Any help would be greatly appreciated.",,"['multivariable-calculus', 'convex-optimization', 'nonlinear-optimization', 'coercive']"
63,"Show that the function $f(x,y) = |xy|$ is differentiable at 0, but is not of class $C^1$ in any neighborhood of 0.","Show that the function  is differentiable at 0, but is not of class  in any neighborhood of 0.","f(x,y) = |xy| C^1","The problem from Munkres' Analysis on Manifold is that Show that the function $f(x,y) = |xy|$ is differentiable at 0, but is not of class $C^1$ in any neighborhood of $0$ . My thought on the first part is that since $Df(0)= [0  0]$ , $f$ is differentiable at $0$ . However, I have no idea of how to approach to the second part. This was my initial thought : Let $x=(x_1, x_2)\in\Bbb R^2$ . Then, $D_1f(x) =\begin{cases}x_2&\text{ if }\quad x_1x_2 &> 0\\x_1&\text{ if }\quad x_1x_2&<0\end{cases}$ Therefore, $D_1f(x) =0\quad as \quad x \to 0$ . Since $D_1f(0) = 0$ , this implies that $D_1f$ is continuous at $0$ . Similar argument goes for $D_2f$ . Is this result wrong or compatible to what the problem requires? And how can I prove that $f$ is not continuously differentiable in neighborhood of $0$ ? Thanks!","The problem from Munkres' Analysis on Manifold is that Show that the function is differentiable at 0, but is not of class in any neighborhood of . My thought on the first part is that since , is differentiable at . However, I have no idea of how to approach to the second part. This was my initial thought : Let . Then, Therefore, . Since , this implies that is continuous at . Similar argument goes for . Is this result wrong or compatible to what the problem requires? And how can I prove that is not continuously differentiable in neighborhood of ? Thanks!","f(x,y) = |xy| C^1 0 Df(0)= [0  0] f 0 x=(x_1, x_2)\in\Bbb R^2 D_1f(x) =\begin{cases}x_2&\text{ if }\quad x_1x_2 &> 0\\x_1&\text{ if }\quad x_1x_2&<0\end{cases} D_1f(x) =0\quad as \quad x \to 0 D_1f(0) = 0 D_1f 0 D_2f f 0","['multivariable-calculus', 'derivatives', 'partial-derivative']"
64,Finding extreme values when the determinant of the Hessian at a critical point is zero.,Finding extreme values when the determinant of the Hessian at a critical point is zero.,,"We want to determine extreme values of $f(x,y)=x^3+xy^2-x^2y-y^3$. We first determine critical points by solving $\dfrac{\partial f(x,y)}{\partial x}=0$ and  $\dfrac{\partial f(x,y)}{\partial y}=0$ which gives that the only critical point is $(0,0)$. Now we compute the determinant of the Hessian $$D(x,y)=(6x-2y)(2x-6y)-(2y-2x)^2$$ Hence $D(0,0)=0$ and the determinant of the Hessian test is not conclusive, so what to do next to verify existence of local and global extreme values? thank you for your help.","We want to determine extreme values of $f(x,y)=x^3+xy^2-x^2y-y^3$. We first determine critical points by solving $\dfrac{\partial f(x,y)}{\partial x}=0$ and  $\dfrac{\partial f(x,y)}{\partial y}=0$ which gives that the only critical point is $(0,0)$. Now we compute the determinant of the Hessian $$D(x,y)=(6x-2y)(2x-6y)-(2y-2x)^2$$ Hence $D(0,0)=0$ and the determinant of the Hessian test is not conclusive, so what to do next to verify existence of local and global extreme values? thank you for your help.",,['multivariable-calculus']
65,Show $\nabla\cdot\left(\mathbf{F}\times\mathbf{G}\right)=\mathbf{G}\cdot(\nabla\times\mathbf{F})-\mathbf{F}\cdot(\nabla\times\mathbf{G})$,Show,\nabla\cdot\left(\mathbf{F}\times\mathbf{G}\right)=\mathbf{G}\cdot(\nabla\times\mathbf{F})-\mathbf{F}\cdot(\nabla\times\mathbf{G}),"Question as follows. Suppose that $\mathbf{F}$,$\mathbf{G}:\mathbb{R^3}\rightarrow\mathbb{R^3}$ and $\phi:\mathbb{R^3}\rightarrow\mathbb{R}$ are smooth. Show using the summation convention that $$\nabla\cdot\left(\mathbf{F}\times\mathbf{G}\right)=\mathbf{G}\cdot(\nabla\times\mathbf{F})-\mathbf{F}\cdot(\nabla\times\mathbf{G}).$$ So far I have $$\mathrm{LHS}=\partial_i\mathbf{e}_i\cdot\left(F_j\mathbf{e}_j\times G_k\mathbf{e}_k\right)=\partial_i\mathbf{e}_i\cdot(F_j G_k \epsilon_{jki}\mathbf{e}_i)=\partial_iF_jG_k\epsilon_{ijk}.$$ I'm under the impression that this should be $0$ as expanding the $\mathrm{RHS}$ gives $2\partial_iF_jG_k\epsilon_{ijk}$. Is this true and is it true because for $\partial_iF_jG_k\ne0$ iff $i=j$ or $i=k$ but if $i=j$ or $i=k$ then $\epsilon_{ijk}=0$?","Question as follows. Suppose that $\mathbf{F}$,$\mathbf{G}:\mathbb{R^3}\rightarrow\mathbb{R^3}$ and $\phi:\mathbb{R^3}\rightarrow\mathbb{R}$ are smooth. Show using the summation convention that $$\nabla\cdot\left(\mathbf{F}\times\mathbf{G}\right)=\mathbf{G}\cdot(\nabla\times\mathbf{F})-\mathbf{F}\cdot(\nabla\times\mathbf{G}).$$ So far I have $$\mathrm{LHS}=\partial_i\mathbf{e}_i\cdot\left(F_j\mathbf{e}_j\times G_k\mathbf{e}_k\right)=\partial_i\mathbf{e}_i\cdot(F_j G_k \epsilon_{jki}\mathbf{e}_i)=\partial_iF_jG_k\epsilon_{ijk}.$$ I'm under the impression that this should be $0$ as expanding the $\mathrm{RHS}$ gives $2\partial_iF_jG_k\epsilon_{ijk}$. Is this true and is it true because for $\partial_iF_jG_k\ne0$ iff $i=j$ or $i=k$ but if $i=j$ or $i=k$ then $\epsilon_{ijk}=0$?",,['multivariable-calculus']
66,Implicit Function Theorem: a counter-example,Implicit Function Theorem: a counter-example,,"The following theorem is stated in Spivak's ""Calculus on Manifolds"" as a follow-up on the Implicit Function Theorem: Theorem 2.13: Let $f: \mathbb{R}^n \to \mathbb{R}^p$ be continuously differentiable in an open set containing $a$, where $p \le n$. if $f(a) = 0$ and the $p \times n$ matrix $(D_jf_i(a))$ has rank $p$, then there is an open set $A \subset \mathbb{R}^n$ containing $a$ and a differentiable function $h: A \to \mathbb{R}^n$ with differentiable inverse such that $$f \circ h (x^1, \dots, x^n) = (x^{n-p+1}, \dots, x^n).$$ I don't see how this can be true. For a simple counter-example, let $f(x) = \sin(x)$ with $n=p=1$. Since $f'(2\pi)=1$, the theorem should hold at $a = 2\pi$, and since $a \in A$ we get for $x = a = 2\pi$: $$\sin(h(a)) = a = 2\pi,$$ which cannot be true for any $h$. Where is the mistake?","The following theorem is stated in Spivak's ""Calculus on Manifolds"" as a follow-up on the Implicit Function Theorem: Theorem 2.13: Let $f: \mathbb{R}^n \to \mathbb{R}^p$ be continuously differentiable in an open set containing $a$, where $p \le n$. if $f(a) = 0$ and the $p \times n$ matrix $(D_jf_i(a))$ has rank $p$, then there is an open set $A \subset \mathbb{R}^n$ containing $a$ and a differentiable function $h: A \to \mathbb{R}^n$ with differentiable inverse such that $$f \circ h (x^1, \dots, x^n) = (x^{n-p+1}, \dots, x^n).$$ I don't see how this can be true. For a simple counter-example, let $f(x) = \sin(x)$ with $n=p=1$. Since $f'(2\pi)=1$, the theorem should hold at $a = 2\pi$, and since $a \in A$ we get for $x = a = 2\pi$: $$\sin(h(a)) = a = 2\pi,$$ which cannot be true for any $h$. Where is the mistake?",,['multivariable-calculus']
67,Minimum and maximum sum of squares given constraints,Minimum and maximum sum of squares given constraints,,"Say that we know that $$\sum_{i=1}^n x_i = x_1+x_2+...+x_n = 1$$ for some positive integer $n$ , with $x_1 \le x_2 \le x_3 \le ... \le x_n$ . The values of $x_1$ and $x_n$ are also known. How can the minimum and maximum values of $$\sum_{i=1}^n x_i^2$$ be found? My attempt: I found the minimum value by setting all the $x_i$ other than $x_1$ and $x_n$ equal to each other. This means that $(n-2)x_i + x_1 + x_n = 1 \rightarrow x_i = \frac{1-x_1-x_n}{n-2}$ . Therefore, $$\sum_{i=1}^n x_i^2 = \frac{(1-x_1-x_n)^2}{n-2}+x_1^2+x_n^2$$ However, I do not know how to find the maximum. The hard part is that $x_1 \le x_i \le x_n$ must be satisfied.","Say that we know that for some positive integer , with . The values of and are also known. How can the minimum and maximum values of be found? My attempt: I found the minimum value by setting all the other than and equal to each other. This means that . Therefore, However, I do not know how to find the maximum. The hard part is that must be satisfied.",\sum_{i=1}^n x_i = x_1+x_2+...+x_n = 1 n x_1 \le x_2 \le x_3 \le ... \le x_n x_1 x_n \sum_{i=1}^n x_i^2 x_i x_1 x_n (n-2)x_i + x_1 + x_n = 1 \rightarrow x_i = \frac{1-x_1-x_n}{n-2} \sum_{i=1}^n x_i^2 = \frac{(1-x_1-x_n)^2}{n-2}+x_1^2+x_n^2 x_1 \le x_i \le x_n,"['multivariable-calculus', 'optimization', 'maxima-minima', 'cauchy-schwarz-inequality', 'karamata-inequality']"
68,"Why do we require differential forms to be smooth, rather than $C^2$?","Why do we require differential forms to be smooth, rather than ?",C^2,"I feel like this is probably a naive question with an obvious answer, but I haven't been able to think of one. So I'll ask away. In every treatment I've seen, differential forms are required to be smooth. $0$ -forms are smooth functions, $1$ -forms are smooth covector fields, etc. I'm puzzled by this requirement, because the theory of differential forms doesn't seem to require us taking derivatives of order $>2$ . Clearly the $C^2$ condition is necessary for the identity $d^2=0$ . But once we have this identity it seems to do away with all higher-order phenomena, and that's the end of the story. Despite this, every treatment I've ever seen requires smoothness. What am I missing?","I feel like this is probably a naive question with an obvious answer, but I haven't been able to think of one. So I'll ask away. In every treatment I've seen, differential forms are required to be smooth. -forms are smooth functions, -forms are smooth covector fields, etc. I'm puzzled by this requirement, because the theory of differential forms doesn't seem to require us taking derivatives of order . Clearly the condition is necessary for the identity . But once we have this identity it seems to do away with all higher-order phenomena, and that's the end of the story. Despite this, every treatment I've ever seen requires smoothness. What am I missing?",0 1 >2 C^2 d^2=0,"['multivariable-calculus', 'differential-geometry', 'vector-analysis', 'differential-forms']"
69,Does the inverse function theorem hold over $\mathbb{Q}$?,Does the inverse function theorem hold over ?,\mathbb{Q},"Let $f:\mathbb{Q}^n \longrightarrow  \mathbb{Q}^n$. We can define what it means for such $f$ to be differentiable: (The differential will be a linear transformation $\mathbb{Q}^n \longrightarrow  \mathbb{Q}^n$) The definition over $\mathbb{R}$ uses the fact it is an ordered field, and the norm structure on $\mathbb{R}^n$.  We do not have the standard euclidean norm on $\mathbb{Q}^n$ (we cannot take square roots in $\mathbb{Q}$,  and normed spaces are usually defined to be vector spaces over $\mathbb{R}$ or $\mathbb{C}$) but we can use the ""rational"" $1$-norm instead. (This is only one option which works ""intrinsically"", i.e does not require using numbers outside $\mathbb{Q}$, we can use other alternatives of course). We can also say what it means for $f$ to be continuously differentiable: $f=(f_1,\cdots,f_n)$ , $f_i:\mathbb{Q}^n \longrightarrow  \mathbb{Q}$, and the $n$ partial derivatives of $f_i$ are functions $\mathbb{Q}^n \longrightarrow  \mathbb{Q}$, and we have a notion of continuity for these creatures. My question: Assume $f:\mathbb{Q}^n \longrightarrow  \mathbb{Q}^n$ is continuously differentiable, and that $Df(x)$ is invertible for some  $x\in \mathbb{Q}^n$ . Does the conclusion of the inverse function theorem hold? This question can be clearly generalized to arbitrary ordered fields. Does this adapted version of the theorem holds anywhere besides $\mathbb{R}$? (I suspect not since the proofs uses compactness of a closed ball in $\mathbb{R}^n$ or the contraction mapping theorem which assume completeness).","Let $f:\mathbb{Q}^n \longrightarrow  \mathbb{Q}^n$. We can define what it means for such $f$ to be differentiable: (The differential will be a linear transformation $\mathbb{Q}^n \longrightarrow  \mathbb{Q}^n$) The definition over $\mathbb{R}$ uses the fact it is an ordered field, and the norm structure on $\mathbb{R}^n$.  We do not have the standard euclidean norm on $\mathbb{Q}^n$ (we cannot take square roots in $\mathbb{Q}$,  and normed spaces are usually defined to be vector spaces over $\mathbb{R}$ or $\mathbb{C}$) but we can use the ""rational"" $1$-norm instead. (This is only one option which works ""intrinsically"", i.e does not require using numbers outside $\mathbb{Q}$, we can use other alternatives of course). We can also say what it means for $f$ to be continuously differentiable: $f=(f_1,\cdots,f_n)$ , $f_i:\mathbb{Q}^n \longrightarrow  \mathbb{Q}$, and the $n$ partial derivatives of $f_i$ are functions $\mathbb{Q}^n \longrightarrow  \mathbb{Q}$, and we have a notion of continuity for these creatures. My question: Assume $f:\mathbb{Q}^n \longrightarrow  \mathbb{Q}^n$ is continuously differentiable, and that $Df(x)$ is invertible for some  $x\in \mathbb{Q}^n$ . Does the conclusion of the inverse function theorem hold? This question can be clearly generalized to arbitrary ordered fields. Does this adapted version of the theorem holds anywhere besides $\mathbb{R}$? (I suspect not since the proofs uses compactness of a closed ball in $\mathbb{R}^n$ or the contraction mapping theorem which assume completeness).",,"['multivariable-calculus', 'derivatives', 'ordered-fields']"
70,Which operators commute with curl?,Which operators commute with curl?,,"Let $X$ be the space of infinitely differentiable maps from $\mathbb{R}^3$ to $\mathbb{R}^3$ . Let $C:X\rightarrow X$ denote the curl map. What are all the linear maps from $X$ to $X$ that commute with $C$ ? For example, rotations and translations commute with $C$ . Also of course $C$ commutes with itself. Other than products of these three types of linear operators (translation, rotation, and curl) what other operators commute with $C$ ?","Let be the space of infinitely differentiable maps from to . Let denote the curl map. What are all the linear maps from to that commute with ? For example, rotations and translations commute with . Also of course commutes with itself. Other than products of these three types of linear operators (translation, rotation, and curl) what other operators commute with ?",X \mathbb{R}^3 \mathbb{R}^3 C:X\rightarrow X X X C C C C,"['multivariable-calculus', 'differential-geometry', 'vector-analysis', 'examples-counterexamples', 'curl']"
71,PDE : Mixture of Wave and Heat equations,PDE : Mixture of Wave and Heat equations,,"Today I was given the following equation : $$\frac{1}{c^2}u_{tt} + \frac{1}{D}u_t = u_{xx}$$ with initial conditions : $u(x,0) = 1$ if $|x|<L$ and $0$ otherwise, $u_t(x,0) = 0$. So fairly simple initial conditions. I can see that there is a bit of wave and heat equation so I first solved each case but I couldn't ""glue"" the answers together. If $c$ gets large, then the equation will behave like a heat equation and similarly, if $D$ is large then it will behave like a wave equation. Using dimensional analysis I deduced that if $\frac{c^2}{LD}$ is the criterion to say if $c$ and $D$ are ""large"". I know that I can solve the equation using separation of variable but what would be a way to be able see how the solution behaves without solving it? Like being able to sketch a solution for varying $t$ would be really nice. Cheers","Today I was given the following equation : $$\frac{1}{c^2}u_{tt} + \frac{1}{D}u_t = u_{xx}$$ with initial conditions : $u(x,0) = 1$ if $|x|<L$ and $0$ otherwise, $u_t(x,0) = 0$. So fairly simple initial conditions. I can see that there is a bit of wave and heat equation so I first solved each case but I couldn't ""glue"" the answers together. If $c$ gets large, then the equation will behave like a heat equation and similarly, if $D$ is large then it will behave like a wave equation. Using dimensional analysis I deduced that if $\frac{c^2}{LD}$ is the criterion to say if $c$ and $D$ are ""large"". I know that I can solve the equation using separation of variable but what would be a way to be able see how the solution behaves without solving it? Like being able to sketch a solution for varying $t$ would be really nice. Cheers",,"['multivariable-calculus', 'partial-differential-equations', 'asymptotics', 'heat-equation', 'wave-equation']"
72,Remainder Taylor series two+ variables,Remainder Taylor series two+ variables,,Hard to find an article /  tutorial specifically about this subject online.... Can anyone explain / send link to explanation or tutorial regarding how to calculate remainder for multi-variable taylor series? Thanks much!,Hard to find an article /  tutorial specifically about this subject online.... Can anyone explain / send link to explanation or tutorial regarding how to calculate remainder for multi-variable taylor series? Thanks much!,,"['multivariable-calculus', 'taylor-expansion', 'error-function']"
73,Independence of function and its derivative in calculus of variations,Independence of function and its derivative in calculus of variations,,"It's common to see in calculus of variation that the integrand $f$ of functional $F[y]=\int f(y,y',x)dx$ is a function of $y,y'$ and $x$. Why do we regard the derivative $y'$ as an independent variable to $y$? And why don't we involve $x$ explicitly in $\displaystyle \delta F=\int\delta f~dx=\int\left(\frac{\partial f}{\partial y}\delta y+\frac{\partial f}{\partial y'}\delta y'\right)~dx$ ?","It's common to see in calculus of variation that the integrand $f$ of functional $F[y]=\int f(y,y',x)dx$ is a function of $y,y'$ and $x$. Why do we regard the derivative $y'$ as an independent variable to $y$? And why don't we involve $x$ explicitly in $\displaystyle \delta F=\int\delta f~dx=\int\left(\frac{\partial f}{\partial y}\delta y+\frac{\partial f}{\partial y'}\delta y'\right)~dx$ ?",,"['multivariable-calculus', 'calculus-of-variations']"
74,Regarding functions from R² to R: continuity and differentiability,Regarding functions from R² to R: continuity and differentiability,,"Let $f : U \rightarrow \mathbb{R}$ where $U \subseteq \mathbb{R}^2$ is an open set and $P \in U$. I am almost sure the following statements are correct, but please confirm: The only requirement for $f$ to have a tangent plane at $P$ is: $\exists \  \nabla f(P)$ (in other words, both $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ exist at $P$. Edit: the partial derivatives must be continuous! If the tangent plane exists, still not necessarily the function is continuous at $P$. The statement 2 is reverse order: if the function is continuous at $P$, still not necessarily the tangent plane exists. If the function happen to be continuous at $P$, still not necessarily the function is differentiable at $P$. And now, assuming those are correct, my main question comes: If $f$ is continuous at $P$, and $f$ has a tangent plane at $P$, is it possible that $f$ still is not differentiable? I might have misunderstood what my teacher said, but it seems that the answer is yes! If you agree, can you provide at least one example? Thank you! EDIT (Six months later): This question awarded me the Tumbleweed badge and even after six months there hasn't been any answers, comments or even votes! Today something made me remember of this question. Fortunately, I already made some progress: I was able to confirm the statements 1, 3 and 4. I couldn't confirm statement 2 yet, and my main question also still stands (I put it in italic above). Thanks for any help. EDIT 2 (May 28): Thanks for the attention. I have felt the need to quote James Stewart in his definition of tangent plane: Suppose f has continuous partial derivatives. An equation of the tangent plane to the surface $z = f(x, y)$ at the point $P(x_0, y_0, z_0)$ is   $$z - z_0 = f_x(x_0, y_0)(x - x_0) + f_y(x_0, y_0)(y - y_0)$$ Therefore, Stewart needs only continuous partial derivatives to define the tangent plane. This shows that I was wrong about my statement 1 . I have edited it now, adding the ""must be continuous"" requirement. This doesn't change any of my other thoughts though (yet). I would like to emphasize that I am considering any weird function you could come up with, not just real life functions and such. In this question I am looking for properties that will formally apply in any case. If anyone disagrees that statements 1, 3 and 4 are true please comment for further discussion (maybe I did miss something!). Now I'll try to give an example of what makes me believe in statement 2: Please correct me if I'm wrong! Take this weird function: $$ f(x, y)=\begin{cases}     0, & \text{if $x = 0$ or $y = 0$}.\\ \\     1, & \text{otherwise}.   \end{cases} $$ Hopefully, if I'm not mistaken, this function is an example that shows that my second statement is true. (please let me know if I'm mistaken). Assuming everything is fine by now, don't forget of my main question: If $f$ is continuous at $P$, and $f$ has a tangent plane at $P$, is it possible that $f$ still is not differentiable?","Let $f : U \rightarrow \mathbb{R}$ where $U \subseteq \mathbb{R}^2$ is an open set and $P \in U$. I am almost sure the following statements are correct, but please confirm: The only requirement for $f$ to have a tangent plane at $P$ is: $\exists \  \nabla f(P)$ (in other words, both $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ exist at $P$. Edit: the partial derivatives must be continuous! If the tangent plane exists, still not necessarily the function is continuous at $P$. The statement 2 is reverse order: if the function is continuous at $P$, still not necessarily the tangent plane exists. If the function happen to be continuous at $P$, still not necessarily the function is differentiable at $P$. And now, assuming those are correct, my main question comes: If $f$ is continuous at $P$, and $f$ has a tangent plane at $P$, is it possible that $f$ still is not differentiable? I might have misunderstood what my teacher said, but it seems that the answer is yes! If you agree, can you provide at least one example? Thank you! EDIT (Six months later): This question awarded me the Tumbleweed badge and even after six months there hasn't been any answers, comments or even votes! Today something made me remember of this question. Fortunately, I already made some progress: I was able to confirm the statements 1, 3 and 4. I couldn't confirm statement 2 yet, and my main question also still stands (I put it in italic above). Thanks for any help. EDIT 2 (May 28): Thanks for the attention. I have felt the need to quote James Stewart in his definition of tangent plane: Suppose f has continuous partial derivatives. An equation of the tangent plane to the surface $z = f(x, y)$ at the point $P(x_0, y_0, z_0)$ is   $$z - z_0 = f_x(x_0, y_0)(x - x_0) + f_y(x_0, y_0)(y - y_0)$$ Therefore, Stewart needs only continuous partial derivatives to define the tangent plane. This shows that I was wrong about my statement 1 . I have edited it now, adding the ""must be continuous"" requirement. This doesn't change any of my other thoughts though (yet). I would like to emphasize that I am considering any weird function you could come up with, not just real life functions and such. In this question I am looking for properties that will formally apply in any case. If anyone disagrees that statements 1, 3 and 4 are true please comment for further discussion (maybe I did miss something!). Now I'll try to give an example of what makes me believe in statement 2: Please correct me if I'm wrong! Take this weird function: $$ f(x, y)=\begin{cases}     0, & \text{if $x = 0$ or $y = 0$}.\\ \\     1, & \text{otherwise}.   \end{cases} $$ Hopefully, if I'm not mistaken, this function is an example that shows that my second statement is true. (please let me know if I'm mistaken). Assuming everything is fine by now, don't forget of my main question: If $f$ is continuous at $P$, and $f$ has a tangent plane at $P$, is it possible that $f$ still is not differentiable?",,"['multivariable-calculus', 'continuity', 'partial-derivative', 'differential']"
75,Use the divergent theorem to verify the volume of a circular cone,Use the divergent theorem to verify the volume of a circular cone,,"Let $T$ be a region with boundary surface $S$ such that $T$ has volume $$V=\frac{1}{3} \iint_S (x dydz +ydzdx+zdxdy)$$ use this equation to verify the volume of a circular cone with height $h$ and radius of base $a$ is $V = \pi a^2 \frac{h}{3}$ Here is what I got so far Since $$V=\frac{1}{3} \iint_S (x dydz +ydzdx+zdxdy)$$ $$V= \iint_S x dydz =\iint_S y dxdz=\iint_S z dxdy$$ Since this is a cone so I can have $r(w,\theta)=[w\cos(\theta),w\sin(\theta),w]$ with $0\leq w\leq a$ and $0 \leq \theta \leq 2\pi$ as the parametrization. Now I'm stuck. I wonder if any one can give me a hint. Please.",Let be a region with boundary surface such that has volume use this equation to verify the volume of a circular cone with height and radius of base is Here is what I got so far Since Since this is a cone so I can have with and as the parametrization. Now I'm stuck. I wonder if any one can give me a hint. Please.,"T S T V=\frac{1}{3} \iint_S (x dydz +ydzdx+zdxdy) h a V = \pi a^2 \frac{h}{3} V=\frac{1}{3} \iint_S (x dydz +ydzdx+zdxdy) V= \iint_S x dydz =\iint_S y dxdz=\iint_S z dxdy r(w,\theta)=[w\cos(\theta),w\sin(\theta),w] 0\leq w\leq a 0 \leq \theta \leq 2\pi","['multivariable-calculus', 'volume', 'surface-integrals', 'divergence-theorem']"
76,A lower bound of $\sum_{i=1}^n a_i \sum_{i=1}^n \frac{1}{a_i}$,A lower bound of,\sum_{i=1}^n a_i \sum_{i=1}^n \frac{1}{a_i},"For fixed $n \ge 2$ , find the maximum of real number $t$ such that $$ \sum_{i=1}^n a_i \sum_{i=1}^n \frac{1}{a_i} \ge n^2 + t \cdot \frac{\displaystyle\sum_{1\le i<j \le n} (a_i - a_j)^2 }{\left( \displaystyle\sum_{i=1}^n a_i\right)^2} $$ holds for arbitrary positive numbers $a_i\quad(i=1,2,\dots,n)$ . This problem comes from a training math camp for CMO competitors in 2021. The standard solution seems to be a little unkind to high school students as Lagrange multiplier method is somehow involved. I wonder if here is any solution without it. Following is my attempt. By Lagrange identity, it's clear that $$ \sum_{i=1}^n a_i \sum_{i=1}^n \frac{1}{a_i}=n^2 + \sum_{1\le i<j \le n}\left(\sqrt{\frac{a_i}{a_j}}-\sqrt{\frac{a_j}{a_i}}\right)^2=n^2 + \sum_{1\le i<j \le n} \frac{(a_i - a_j)^2}{a_ia_j} $$ Then we only need to find the maximum of $t$ such that $$ \sum_{1\le i<j \le n} \frac{(a_i - a_j)^2}{a_ia_j} \ge t \cdot \frac{\displaystyle\sum_{1\le i<j \le n} (a_i - a_j)^2 }{\left( \displaystyle\sum_{i=1}^n a_i\right)^2} $$ However, by Cauchy-Schwarz we have $$ \sum_{1\le i<j \le n} \frac{(a_i - a_j)^2}{a_ia_j} \ge \frac{\left(\displaystyle\sum_{1\le i<j \le n} |a_i - a_j|\right)^2}{\displaystyle\sum_{1\le i<j \le n} a_ia_j} $$ which means nothing.","For fixed , find the maximum of real number such that holds for arbitrary positive numbers . This problem comes from a training math camp for CMO competitors in 2021. The standard solution seems to be a little unkind to high school students as Lagrange multiplier method is somehow involved. I wonder if here is any solution without it. Following is my attempt. By Lagrange identity, it's clear that Then we only need to find the maximum of such that However, by Cauchy-Schwarz we have which means nothing.","n \ge 2 t 
\sum_{i=1}^n a_i \sum_{i=1}^n \frac{1}{a_i} \ge n^2 + t \cdot \frac{\displaystyle\sum_{1\le i<j \le n} (a_i - a_j)^2 }{\left( \displaystyle\sum_{i=1}^n a_i\right)^2}
 a_i\quad(i=1,2,\dots,n) 
\sum_{i=1}^n a_i \sum_{i=1}^n \frac{1}{a_i}=n^2 + \sum_{1\le i<j \le n}\left(\sqrt{\frac{a_i}{a_j}}-\sqrt{\frac{a_j}{a_i}}\right)^2=n^2 + \sum_{1\le i<j \le n} \frac{(a_i - a_j)^2}{a_ia_j}
 t 
\sum_{1\le i<j \le n} \frac{(a_i - a_j)^2}{a_ia_j} \ge t \cdot \frac{\displaystyle\sum_{1\le i<j \le n} (a_i - a_j)^2 }{\left( \displaystyle\sum_{i=1}^n a_i\right)^2}
 
\sum_{1\le i<j \le n} \frac{(a_i - a_j)^2}{a_ia_j} \ge \frac{\left(\displaystyle\sum_{1\le i<j \le n} |a_i - a_j|\right)^2}{\displaystyle\sum_{1\le i<j \le n} a_ia_j}
","['multivariable-calculus', 'inequality', 'contest-math', 'lagrange-multiplier', 'cauchy-schwarz-inequality']"
77,"If the sum of cubes of $a,b,c,d$ is $1$, then $\frac{1}{1-bcd}+\frac{1}{1-cda}+\frac{1}{1-dab}+\frac{1}{1-abc}\le \frac{16}{3}$","If the sum of cubes of  is , then","a,b,c,d 1 \frac{1}{1-bcd}+\frac{1}{1-cda}+\frac{1}{1-dab}+\frac{1}{1-abc}\le \frac{16}{3}","$a,b,c,d>0$ satisfying $a^3+b^3+c^3+d^3=1$. Prove   $$\frac{1}{1-bcd}+\frac{1}{1-cda}+\frac{1}{1-dab}+\frac{1}{1-abc}\le \frac{16}{3}$$ I tried to go the normal way, by Cauchy-Schwarz, but that doesn't work. So I tried to incorporate this newly learned idea, since $a,b,c,d<1$ we can write the left as a power series: $$\sum_{n=0}^{\infty}(bcd)^n+(cda)^n+(dab)^n+(abc)^n$$ If we can show, $(bcd)^n+(cda)^n+(dab)^n+(abc)^n\ge (K(a^3+b^3+c^3+d^3))^n$ for some suitable constant $K$ we can finish. But I can't really do it. Can someone help me?","$a,b,c,d>0$ satisfying $a^3+b^3+c^3+d^3=1$. Prove   $$\frac{1}{1-bcd}+\frac{1}{1-cda}+\frac{1}{1-dab}+\frac{1}{1-abc}\le \frac{16}{3}$$ I tried to go the normal way, by Cauchy-Schwarz, but that doesn't work. So I tried to incorporate this newly learned idea, since $a,b,c,d<1$ we can write the left as a power series: $$\sum_{n=0}^{\infty}(bcd)^n+(cda)^n+(dab)^n+(abc)^n$$ If we can show, $(bcd)^n+(cda)^n+(dab)^n+(abc)^n\ge (K(a^3+b^3+c^3+d^3))^n$ for some suitable constant $K$ we can finish. But I can't really do it. Can someone help me?",,"['multivariable-calculus', 'inequality', 'substitution', 'symmetric-polynomials', 'uvw']"
78,History of line integral.,History of line integral.,,"I'm looking for some information about how the line integral was discovered, since I've been looking for a long time for this. I found that Riemann could integer discontinuity functions, then Poisson said that the definite integral could vary if the interval is real or imaginary, saying that the integral depends on the travel, which is the basis of the concept of the line integral. So it would be helpful to find something more related to it.","I'm looking for some information about how the line integral was discovered, since I've been looking for a long time for this. I found that Riemann could integer discontinuity functions, then Poisson said that the definite integral could vary if the interval is real or imaginary, saying that the integral depends on the travel, which is the basis of the concept of the line integral. So it would be helpful to find something more related to it.",,"['multivariable-calculus', 'math-history']"
79,"""Line integrals"" vs ""integration on manifolds"" in Lee's Introduction to Smooth Manifolds","""Line integrals"" vs ""integration on manifolds"" in Lee's Introduction to Smooth Manifolds",,"I'm getting very muddled by the notion(s) of integration used in Lee's book. At first I thought the 4th chapter on the integral of 1-forms was separated from the main treatment of integration in chapter 10 simply because the author wished to present an application early on in the text to stop things getting too terse, but after closely looking at the two definitions I'm starting to get paranoid that these integrals are subtly different. In chapter 4 our integral requires a curve $\gamma : [a,b] \rightarrow M$ and a $1$ -form to be integrated $\omega$ that eats vectors from $T_{\gamma(t)}[a,b]$ . Our ""abstract manifold"" is really just the interval $[a,b]$ . What confuses me is we have this freedom to choose the manifold $M$ which $\gamma$ maps into. We define $\displaystyle \int_\gamma \omega:=\int_a^b\gamma^*\omega$ Compare this to chapter 10 where we are integrating in an $n$ manifold $N$ , and we integrate the $n$ -form $\omega \in T^* N$ by taking the pullback of an inverse chart map $\varphi^{-1}$ . Here we have $\displaystyle \int_U \omega:=\int_{\varphi(U)}{(\varphi^{-1})}^*\omega$ At first glance it looks obvious, the curve is just like the inverse chart map for a 1-D manifold, job is done. However, if I want to do the ""chapter 10 integrals"" in 1-D then $N = [a,b]$ my chart maps $\varphi : U\subset N \rightarrow \varphi(U) \subset \mathbb{R}$ , in particular, note how since $N$ is a 1-d manifold, my chart can only map to subsets of the real line by definition of being a smooth manifold of dimension 1. There is no freedom like in chapter 4 to get exotic curves in $\mathbb{R}^2, \mathbb{R}^3$ or any $M$ . What's going on here? Is there some freedoms that you can get in 1-D that don't exist in higher dimensions? I certainly remember my vector calculus courses letting me do line integrals in any number of dimensions I wanted, Lee's example 4.18 certainly shows a nice curve in $\mathbb{R}^3$ , while the chapter 10.22 example has a 2-form integrated on sphere which is calculated by pulling back with a map $F:D \subset \mathbb{R}^2 \rightarrow \mathbb{S}^2$ , like we are trapped in the $\mathbb{R}^n$ of the same dimension as the manifold! Even the name line integral begins to confuse me, are integrals of differential forms meant to generalise integrals from calculus such as $\hspace{15mm}\displaystyle \int {\bf{v}} \cdot d{\bf{r}} \hspace{10mm}\text{or}\hspace{10mm} \int f({\bf{r}})ds \hspace{10mm}\text{or}\hspace{10mm} \int f(x)dV$ ? Chapter 4 makes it sound like differential forms give us $n$ -dimensional analogues on curved spaces to integrals like the left, while chapter 10 sounds more like an n-dimensional curved analogue of the right most integral, but I thought integrating functions was done by the Riemannian volume form? And that still leaves me wondering how to generalise the middle to get a ""curved scalar hypersurface integral"", of the kind you might see on the RHS of an $n$ -dimensional divergence theorem, say. I think this needs a Riemannian metric, but am unsure how it could be used, since problem 4.3 rules out the existence of any differential form $\omega$ such that $\omega ""="" ds$ generalises the concept of a line element. I would appreciate any assistance in clearing up this huge error in my understanding.","I'm getting very muddled by the notion(s) of integration used in Lee's book. At first I thought the 4th chapter on the integral of 1-forms was separated from the main treatment of integration in chapter 10 simply because the author wished to present an application early on in the text to stop things getting too terse, but after closely looking at the two definitions I'm starting to get paranoid that these integrals are subtly different. In chapter 4 our integral requires a curve and a -form to be integrated that eats vectors from . Our ""abstract manifold"" is really just the interval . What confuses me is we have this freedom to choose the manifold which maps into. We define Compare this to chapter 10 where we are integrating in an manifold , and we integrate the -form by taking the pullback of an inverse chart map . Here we have At first glance it looks obvious, the curve is just like the inverse chart map for a 1-D manifold, job is done. However, if I want to do the ""chapter 10 integrals"" in 1-D then my chart maps , in particular, note how since is a 1-d manifold, my chart can only map to subsets of the real line by definition of being a smooth manifold of dimension 1. There is no freedom like in chapter 4 to get exotic curves in or any . What's going on here? Is there some freedoms that you can get in 1-D that don't exist in higher dimensions? I certainly remember my vector calculus courses letting me do line integrals in any number of dimensions I wanted, Lee's example 4.18 certainly shows a nice curve in , while the chapter 10.22 example has a 2-form integrated on sphere which is calculated by pulling back with a map , like we are trapped in the of the same dimension as the manifold! Even the name line integral begins to confuse me, are integrals of differential forms meant to generalise integrals from calculus such as ? Chapter 4 makes it sound like differential forms give us -dimensional analogues on curved spaces to integrals like the left, while chapter 10 sounds more like an n-dimensional curved analogue of the right most integral, but I thought integrating functions was done by the Riemannian volume form? And that still leaves me wondering how to generalise the middle to get a ""curved scalar hypersurface integral"", of the kind you might see on the RHS of an -dimensional divergence theorem, say. I think this needs a Riemannian metric, but am unsure how it could be used, since problem 4.3 rules out the existence of any differential form such that generalises the concept of a line element. I would appreciate any assistance in clearing up this huge error in my understanding.","\gamma : [a,b] \rightarrow M 1 \omega T_{\gamma(t)}[a,b] [a,b] M \gamma \displaystyle \int_\gamma \omega:=\int_a^b\gamma^*\omega n N n \omega \in T^* N \varphi^{-1} \displaystyle \int_U \omega:=\int_{\varphi(U)}{(\varphi^{-1})}^*\omega N = [a,b] \varphi : U\subset N \rightarrow \varphi(U) \subset \mathbb{R} N \mathbb{R}^2, \mathbb{R}^3 M \mathbb{R}^3 F:D \subset \mathbb{R}^2 \rightarrow \mathbb{S}^2 \mathbb{R}^n \hspace{15mm}\displaystyle \int {\bf{v}} \cdot d{\bf{r}} \hspace{10mm}\text{or}\hspace{10mm} \int f({\bf{r}})ds \hspace{10mm}\text{or}\hspace{10mm} \int f(x)dV n n \omega \omega ""="" ds","['multivariable-calculus', 'differential-geometry', 'manifolds', 'vector-analysis', 'differential-forms']"
80,Regular Value Theorem Using Implicit Function Theorem in Calculus.,Regular Value Theorem Using Implicit Function Theorem in Calculus.,,"I want to prove the following: THEOREM. (Regular Value Theorem.) Let $f:\mathbf R^n\to\mathbf R^m$ be a smooth function and $\mathbf a\in\mathbf R^n$ be a regular point of $f$.     Let $f(\mathbf a)=\mathbf 0$ and $\text{rank }Df(\mathbf a)=r$.     Let $R$ be the set of all the regular points of $f$.     Then $R\cap f^{-1}(\mathbf 0)$ is an $(n-r)$-manifold (without boundary) in $\mathbf R^n$. I can prove the above using the Rank Theorem but I was wondering if this can be proved using the Implicit Function Theorem. In fact, the instructor of my Differential Equations course hinted that this can be done. THEOREM (Implicit Function Theorem.) Let $f:\mathbf R^{m+n}\to \mathbf R^n$ be a smooth function. Interpret $f$ in the form $f(\mathbf x,\mathbf y)$ for $\mathbf x\in\mathbf R^m$ and $\mathbf y\in \mathbf R^n$. Let $(\mathbf a,\mathbf b)\in\mathbf R^{m+n}$ be such that $f(\mathbf a,\mathbf b)=\mathbf 0$ and $\det\displaystyle\frac{\partial f}{\partial \mathbf y}\neq 0$. Then there exists a neighborhood $U$ of $\mathbf a$ in $\mathbf R^m$, and a unique continuous function $g:U\to \mathbf R^n$ such that $g(\mathbf a)=\mathbf b$ and $f(\mathbf x,g(\mathbf x))=\mathbf 0$ for all $\mathbf x\in U$. Further, the function $g$ is a smooth function. The definitions I am using are DEFINITION. Let $f:\mathbf R^n\to\mathbf R^m$ be a smooth function. A point $\mathbf a\in \mathbf R^n$ is said to be a regular point of $f$ if $$\text{rank }Df(\mathbf a)=\max\{\text{rank }Df(\mathbf x):\mathbf x\in\mathbf R^n\}$$ DEFINITION. A subset $M$ of $\mathbf R^n$ is said to be a $k$- manifold (without boundary) if for each point $\mathbf p\in M$, there is a set $V$ open in $M$ containing $\mathbf p$, an open set $U$ in $\mathbf R^k$, and a bijective map $\alpha:U\to V$ satisfying: $\alpha^{-1}:V\to U$ is continuous. $D\alpha(\mathbf x)$ has rank $k$ for each $\mathbf x\in U$. Attempt: Consider the statement of the Regular Value Theorem as given above and assume for a special case that $m=r$. Write $M=R\cap f^{-1}(\mathbf 0)$. Then using the Implicit Function Theorem, there exists a neighborhood $U$ of $\mathbf a$ in $\mathbf R^m$, and a continuous function $g:U\to\mathbf R^n$ such that $f(\mathbf x,g(\mathbf x))=\mathbf 0$ for all $\mathbf x\in U$. Now define the function $\alpha:U\to \mathbf R^n$ as $\alpha(\mathbf x)=(x,g(\mathbf x))$. The only trouble now is to show that $\alpha(U)$ is open in $M$. Can anybody see how to do that? Also, can anybody suggest an approach when $m>r$? Thanks.","I want to prove the following: THEOREM. (Regular Value Theorem.) Let $f:\mathbf R^n\to\mathbf R^m$ be a smooth function and $\mathbf a\in\mathbf R^n$ be a regular point of $f$.     Let $f(\mathbf a)=\mathbf 0$ and $\text{rank }Df(\mathbf a)=r$.     Let $R$ be the set of all the regular points of $f$.     Then $R\cap f^{-1}(\mathbf 0)$ is an $(n-r)$-manifold (without boundary) in $\mathbf R^n$. I can prove the above using the Rank Theorem but I was wondering if this can be proved using the Implicit Function Theorem. In fact, the instructor of my Differential Equations course hinted that this can be done. THEOREM (Implicit Function Theorem.) Let $f:\mathbf R^{m+n}\to \mathbf R^n$ be a smooth function. Interpret $f$ in the form $f(\mathbf x,\mathbf y)$ for $\mathbf x\in\mathbf R^m$ and $\mathbf y\in \mathbf R^n$. Let $(\mathbf a,\mathbf b)\in\mathbf R^{m+n}$ be such that $f(\mathbf a,\mathbf b)=\mathbf 0$ and $\det\displaystyle\frac{\partial f}{\partial \mathbf y}\neq 0$. Then there exists a neighborhood $U$ of $\mathbf a$ in $\mathbf R^m$, and a unique continuous function $g:U\to \mathbf R^n$ such that $g(\mathbf a)=\mathbf b$ and $f(\mathbf x,g(\mathbf x))=\mathbf 0$ for all $\mathbf x\in U$. Further, the function $g$ is a smooth function. The definitions I am using are DEFINITION. Let $f:\mathbf R^n\to\mathbf R^m$ be a smooth function. A point $\mathbf a\in \mathbf R^n$ is said to be a regular point of $f$ if $$\text{rank }Df(\mathbf a)=\max\{\text{rank }Df(\mathbf x):\mathbf x\in\mathbf R^n\}$$ DEFINITION. A subset $M$ of $\mathbf R^n$ is said to be a $k$- manifold (without boundary) if for each point $\mathbf p\in M$, there is a set $V$ open in $M$ containing $\mathbf p$, an open set $U$ in $\mathbf R^k$, and a bijective map $\alpha:U\to V$ satisfying: $\alpha^{-1}:V\to U$ is continuous. $D\alpha(\mathbf x)$ has rank $k$ for each $\mathbf x\in U$. Attempt: Consider the statement of the Regular Value Theorem as given above and assume for a special case that $m=r$. Write $M=R\cap f^{-1}(\mathbf 0)$. Then using the Implicit Function Theorem, there exists a neighborhood $U$ of $\mathbf a$ in $\mathbf R^m$, and a continuous function $g:U\to\mathbf R^n$ such that $f(\mathbf x,g(\mathbf x))=\mathbf 0$ for all $\mathbf x\in U$. Now define the function $\alpha:U\to \mathbf R^n$ as $\alpha(\mathbf x)=(x,g(\mathbf x))$. The only trouble now is to show that $\alpha(U)$ is open in $M$. Can anybody see how to do that? Also, can anybody suggest an approach when $m>r$? Thanks.",,['multivariable-calculus']
81,"How to compute the Lipschitz constant for the multivariate function $f(x,y)=1-xy$?",How to compute the Lipschitz constant for the multivariate function ?,"f(x,y)=1-xy","How to compute the Lipschitz constant for the multivariate function $f(x,y) = 1 - xy$ ? I know the definition for one variable, but what is its definition for multivariate functions?","How to compute the Lipschitz constant for the multivariate function ? I know the definition for one variable, but what is its definition for multivariate functions?","f(x,y) = 1 - xy","['multivariable-calculus', 'lipschitz-functions']"
82,Are derivatives linear maps?,Are derivatives linear maps?,,"I am reading Rudin and I am very confused what a derivative is now. I used to think a derivative was just the process of taking the limit like this $$\lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{(x+h)-x}$$ But between Apostol and Rudin, I am confused in what sense total derivatives are derivatives. Partial derivatives much more resemble the usual derivatives taught in high school $$f(x,y) = xy$$ $$\frac{\partial f}{\partial x} = y$$ But the Jacobian doesn't resemble this at all. And according to my books it is a linear map. If derivatives are linear maps, can someone help me see more clearly how my intuitions about simpler derivatives relate to the more complicated forms? I just don't understand where the limits have gone, why its more complex, and why the simpler forms aren't described as linear maps.","I am reading Rudin and I am very confused what a derivative is now. I used to think a derivative was just the process of taking the limit like this $$\lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{(x+h)-x}$$ But between Apostol and Rudin, I am confused in what sense total derivatives are derivatives. Partial derivatives much more resemble the usual derivatives taught in high school $$f(x,y) = xy$$ $$\frac{\partial f}{\partial x} = y$$ But the Jacobian doesn't resemble this at all. And according to my books it is a linear map. If derivatives are linear maps, can someone help me see more clearly how my intuitions about simpler derivatives relate to the more complicated forms? I just don't understand where the limits have gone, why its more complex, and why the simpler forms aren't described as linear maps.",,"['multivariable-calculus', 'derivatives']"
83,Volume of Ellipsoid using Triple Integrals,Volume of Ellipsoid using Triple Integrals,,"Given the general equation of the ellipsoid $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} =1$, I am supposed to use a 3D Jacobian to prove that the volume of the ellipsoid is $\frac{4}{3}\pi abc$ I decided to consider the first octant where $0\le x\le a, 0\le y \le b, 0 \le z \le c$ I then obtained $8\iiint _E dV$ where $E = \{(x, y, z): 0\le x \le a, 0\le y \le b\sqrt{1-\frac{x^2}{a^2}}, 0\le z \le c\sqrt{1-\frac{x^2}{a^2} - \frac{y^2}{b^2}} \}$ I understood that a 3D Jacobian requires 3 variables, $x$, $y$ and $z$, but in this case I noticed that I can simple reduce the triple integral into a double integral: $$8 \int_0^a \int_0^{b\sqrt{1-\frac{x^2}{a^2}}} c\sqrt{1-\frac{x^2}{a^2} - \frac{y^2}{b^2}} dydx$$ which I am not sure what substitution I should do in order to solve this, any advise on this matter is much appreciated!","Given the general equation of the ellipsoid $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} =1$, I am supposed to use a 3D Jacobian to prove that the volume of the ellipsoid is $\frac{4}{3}\pi abc$ I decided to consider the first octant where $0\le x\le a, 0\le y \le b, 0 \le z \le c$ I then obtained $8\iiint _E dV$ where $E = \{(x, y, z): 0\le x \le a, 0\le y \le b\sqrt{1-\frac{x^2}{a^2}}, 0\le z \le c\sqrt{1-\frac{x^2}{a^2} - \frac{y^2}{b^2}} \}$ I understood that a 3D Jacobian requires 3 variables, $x$, $y$ and $z$, but in this case I noticed that I can simple reduce the triple integral into a double integral: $$8 \int_0^a \int_0^{b\sqrt{1-\frac{x^2}{a^2}}} c\sqrt{1-\frac{x^2}{a^2} - \frac{y^2}{b^2}} dydx$$ which I am not sure what substitution I should do in order to solve this, any advise on this matter is much appreciated!",,"['multivariable-calculus', 'jacobian']"
84,Trouble with gradient intuition,Trouble with gradient intuition,,"I'm currently learning about gradients, and I thought khanacademy could help me acquiring some intuition. The actual computation is clear to me, however I'm having trouble understand the intuition. This is the video I'm talking about, and the part I have a part with is 6:11. He says the gradient vector shows the direction you have to travel in the x,y-plane in order to get a maximum slope in the z-direction. This sounds like gibberish to me. How do you have to 'travel' it? Why do you get a maximum slope? I just have no clue.","I'm currently learning about gradients, and I thought khanacademy could help me acquiring some intuition. The actual computation is clear to me, however I'm having trouble understand the intuition. This is the video I'm talking about, and the part I have a part with is 6:11. He says the gradient vector shows the direction you have to travel in the x,y-plane in order to get a maximum slope in the z-direction. This sounds like gibberish to me. How do you have to 'travel' it? Why do you get a maximum slope? I just have no clue.",,['multivariable-calculus']
85,Existence of an Infinite Length Path,Existence of an Infinite Length Path,,"I came across the following simple definition A path $\gamma$ in $\mathbb{R}^n$ that connects the point $a \in \mathbb{R}^n$ to the point $b \in \mathbb{R}^n$, is a continuous $\gamma : [0, 1] \to \mathbb{R}^n$ such that $\gamma(0) = a$ and $\gamma(1) = b$. We denote by $\ell(\gamma)$ the (Euclidean) length of $\gamma$. $\ell(\gamma)$ is always defined and is either a non-negative realnumber or $\infty$. However, I cannot seem to think of a path, defined in this manner (specifically, where the domain is compact), the length of which is infinite. Can anyone provide an example ?","I came across the following simple definition A path $\gamma$ in $\mathbb{R}^n$ that connects the point $a \in \mathbb{R}^n$ to the point $b \in \mathbb{R}^n$, is a continuous $\gamma : [0, 1] \to \mathbb{R}^n$ such that $\gamma(0) = a$ and $\gamma(1) = b$. We denote by $\ell(\gamma)$ the (Euclidean) length of $\gamma$. $\ell(\gamma)$ is always defined and is either a non-negative realnumber or $\infty$. However, I cannot seem to think of a path, defined in this manner (specifically, where the domain is compact), the length of which is infinite. Can anyone provide an example ?",,['multivariable-calculus']
86,solenoid and irrotational vector,solenoid and irrotational vector,,Let $V$ be a vector point function. $V$ is solenoid if $\operatorname{div} V =0$ and irrotational if $\operatorname{curl} V =0$. How can one visualize examples of solenoid or irrotational functions? Are there any functions such that they are both irrotational and solenoid? (Except constant functions),Let $V$ be a vector point function. $V$ is solenoid if $\operatorname{div} V =0$ and irrotational if $\operatorname{curl} V =0$. How can one visualize examples of solenoid or irrotational functions? Are there any functions such that they are both irrotational and solenoid? (Except constant functions),,"['multivariable-calculus', 'differential-geometry']"
87,Why do parametrizations to the normal of a sphere sometimes fail?,Why do parametrizations to the normal of a sphere sometimes fail?,,"If I take the upper hemisphere of a sphere, $x^2 + y^2 + z^2 = 1$, to be $\sqrt{1 - x^2 - y^2}$, then the normal is given by $\langle -f_x, - f_y, 1\rangle$ at any point. This leads to an odd result: on the plane $z = 0$, while one might expect all normals of the sphere to not have any $z$ component (i.e, to only point radially outwards), the $\vec{k}$ component is still 1. A similar parametrization in cylindrical coordinates is: $\langle \sin(\phi)^2 \cos(\theta), \sin(\phi)^2 \sin(\theta), \sin(\phi) \cos(\phi)\rangle$. At $\phi = 0$ and $\theta = 0$, which corresponds to the point $(0,0,1)$ in cartesian coordinates, the normal is $\langle 0,0,0 \rangle$ while one would expect $(0,0,1)$.","If I take the upper hemisphere of a sphere, $x^2 + y^2 + z^2 = 1$, to be $\sqrt{1 - x^2 - y^2}$, then the normal is given by $\langle -f_x, - f_y, 1\rangle$ at any point. This leads to an odd result: on the plane $z = 0$, while one might expect all normals of the sphere to not have any $z$ component (i.e, to only point radially outwards), the $\vec{k}$ component is still 1. A similar parametrization in cylindrical coordinates is: $\langle \sin(\phi)^2 \cos(\theta), \sin(\phi)^2 \sin(\theta), \sin(\phi) \cos(\phi)\rangle$. At $\phi = 0$ and $\theta = 0$, which corresponds to the point $(0,0,1)$ in cartesian coordinates, the normal is $\langle 0,0,0 \rangle$ while one would expect $(0,0,1)$.",,"['multivariable-calculus', 'parametrization']"
88,continuously differentiable multivariable functions,continuously differentiable multivariable functions,,"What does it really mean to say a function $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ is continuously differentiable? A function $f:\mathbb{R}\rightarrow \mathbb{R}$ is continuously differentiable if $f$ is differentiable and $f':\mathbb{R}\rightarrow \mathbb{R}$ is continuous.. But what is $f'$ in case of $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$.. Is it the jacobian $\begin{bmatrix}\frac{\partial f_1}{\partial x_1}& \frac{\partial f_1}{\partial x_2}& \cdots &\frac{\partial f_1}{\partial x_n} \\ \frac{\partial f_2}{\partial x_1}& \frac{\partial f_2}{\partial x_2}& \cdots &\frac{\partial f_2}{\partial x_n} \\ \vdots \\ \frac{\partial f_n}{\partial x_1}& \frac{\partial f_n}{\partial x_2}& \cdots &\frac{\partial f_n}{\partial x_n} \end{bmatrix}$ I could not understand how does this matrix act on $\mathbb{R}^n$.. As an example, for $f(x,y)=(xy,x+y)$ we see that jacobian is  $\begin{bmatrix}y&x\\1&1\end{bmatrix}$.. I do not understand how do i define  $Df: \mathbb{R}^2\rightarrow \mathbb{R}^2$.. In case of derivative at a point we have $Df((a,b))=\begin{bmatrix}b&a\\1&1\end{bmatrix}$ and we define $Df(a,b)(x,y)=\begin{bmatrix}b&a\\1&1\end{bmatrix} \begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}bx+ay\\x+y\end{bmatrix}$ I mean there are already variables in $Df$ and so i am getting confused how to act on $\mathbb{R}^n$.. But in case of $Df(a,b)$ there are no variables.. So, it seems to be natural.","What does it really mean to say a function $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ is continuously differentiable? A function $f:\mathbb{R}\rightarrow \mathbb{R}$ is continuously differentiable if $f$ is differentiable and $f':\mathbb{R}\rightarrow \mathbb{R}$ is continuous.. But what is $f'$ in case of $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$.. Is it the jacobian $\begin{bmatrix}\frac{\partial f_1}{\partial x_1}& \frac{\partial f_1}{\partial x_2}& \cdots &\frac{\partial f_1}{\partial x_n} \\ \frac{\partial f_2}{\partial x_1}& \frac{\partial f_2}{\partial x_2}& \cdots &\frac{\partial f_2}{\partial x_n} \\ \vdots \\ \frac{\partial f_n}{\partial x_1}& \frac{\partial f_n}{\partial x_2}& \cdots &\frac{\partial f_n}{\partial x_n} \end{bmatrix}$ I could not understand how does this matrix act on $\mathbb{R}^n$.. As an example, for $f(x,y)=(xy,x+y)$ we see that jacobian is  $\begin{bmatrix}y&x\\1&1\end{bmatrix}$.. I do not understand how do i define  $Df: \mathbb{R}^2\rightarrow \mathbb{R}^2$.. In case of derivative at a point we have $Df((a,b))=\begin{bmatrix}b&a\\1&1\end{bmatrix}$ and we define $Df(a,b)(x,y)=\begin{bmatrix}b&a\\1&1\end{bmatrix} \begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}bx+ay\\x+y\end{bmatrix}$ I mean there are already variables in $Df$ and so i am getting confused how to act on $\mathbb{R}^n$.. But in case of $Df(a,b)$ there are no variables.. So, it seems to be natural.",,['multivariable-calculus']
89,How can I setup a triple integral for ellipsoid volume?,How can I setup a triple integral for ellipsoid volume?,,"Find the volume of the ellipsoid $$\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} \leq 1$$ where $a,b,c \in \mathbb R$. Attempt: Ok well, I figured spherical coordinates would probably be the best approach. In spherical, $(x,y,z) \mapsto (sin(\phi)cos(\theta),sin(\phi)sin(\theta),cos(\phi))$ where $\phi \in [0,\pi]$ and $\theta \in [0,2\pi]$. So would my integral look something like: $$\int_0^{2\pi}\int_0^{\pi}\int_0^{1} \rho ^2sin(\phi) \space dp \space d\phi \space d\theta$$ Where $\rho = \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = \frac{sin^2(\phi)cos^2(\theta)}{a^2} + \frac{sin^2(\phi)sin^2(\theta)}{b^2} + \frac{cos^2(\phi)}{c^2}$ ... ? If not, could someone guide me through the process of setting up the integral?","Find the volume of the ellipsoid $$\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} \leq 1$$ where $a,b,c \in \mathbb R$. Attempt: Ok well, I figured spherical coordinates would probably be the best approach. In spherical, $(x,y,z) \mapsto (sin(\phi)cos(\theta),sin(\phi)sin(\theta),cos(\phi))$ where $\phi \in [0,\pi]$ and $\theta \in [0,2\pi]$. So would my integral look something like: $$\int_0^{2\pi}\int_0^{\pi}\int_0^{1} \rho ^2sin(\phi) \space dp \space d\phi \space d\theta$$ Where $\rho = \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = \frac{sin^2(\phi)cos^2(\theta)}{a^2} + \frac{sin^2(\phi)sin^2(\theta)}{b^2} + \frac{cos^2(\phi)}{c^2}$ ... ? If not, could someone guide me through the process of setting up the integral?",,['multivariable-calculus']
90,Reference for multivariable calculus,Reference for multivariable calculus,,"I'm looking for a book to learn multivariable calculus that is rigorous, but not overly technical, and also provides meaningful insight. Standard calculus texts like Stewart and Thomas are too sketchy. I've also skimmed through some texts in analysis, e.g. Rudin and Pugh, but they are not so readable due to unpleasant notation (which is probably inevitable) and lack of intuitive motivation. I came across Terence Tao's article on differential forms. I like his way of explaining the analogues and intuitions behind the definitions and theorems. This kind of writing is what I'm looking for. Please advice me some reference. Thanks in advance.","I'm looking for a book to learn multivariable calculus that is rigorous, but not overly technical, and also provides meaningful insight. Standard calculus texts like Stewart and Thomas are too sketchy. I've also skimmed through some texts in analysis, e.g. Rudin and Pugh, but they are not so readable due to unpleasant notation (which is probably inevitable) and lack of intuitive motivation. I came across Terence Tao's article on differential forms. I like his way of explaining the analogues and intuitions behind the definitions and theorems. This kind of writing is what I'm looking for. Please advice me some reference. Thanks in advance.",,"['reference-request', 'multivariable-calculus']"
91,Intuition of the connection between the graph Laplacian and the Laplace operator,Intuition of the connection between the graph Laplacian and the Laplace operator,,"In Physics SO the intuition of the Laplace operator (divergence of the gradient) is explained by resorting to the finite difference version: the Laplace equation is satisfied as long as the value at a vertex is the average of the surrounding values, akin to the explanation in Wikipedia of harmonic functions . Or the more enjoyable blog explanation , motivating them through minimal surfaces: soap bubbles, or the 3Blue1Brown video : Much like a minimum with a positive second derivative, a positive Laplacian in a 2D surface would indicate local minimum (concave up) in the way that the neighboring points on average are higher in value. Professor Strang gives a rather artistic impromptu intuition right here proposing a grid with unweighted edges, and noticing that for internal vertices, the degree would be $4,$ corresponding to second differences with the surrounding unit-value edges, but this is a bit shaky in what it is really happening in the example. Clearly the average of adjacent entries in a Laplacian matrix doesn't really work because of the embedded degree matrix in the diagonal: Others approach the graph Laplacian by noticing the connection to Newton's law of cooling . Is there a better ""visual"" to see the intuition of the graph Laplacian?","In Physics SO the intuition of the Laplace operator (divergence of the gradient) is explained by resorting to the finite difference version: the Laplace equation is satisfied as long as the value at a vertex is the average of the surrounding values, akin to the explanation in Wikipedia of harmonic functions . Or the more enjoyable blog explanation , motivating them through minimal surfaces: soap bubbles, or the 3Blue1Brown video : Much like a minimum with a positive second derivative, a positive Laplacian in a 2D surface would indicate local minimum (concave up) in the way that the neighboring points on average are higher in value. Professor Strang gives a rather artistic impromptu intuition right here proposing a grid with unweighted edges, and noticing that for internal vertices, the degree would be corresponding to second differences with the surrounding unit-value edges, but this is a bit shaky in what it is really happening in the example. Clearly the average of adjacent entries in a Laplacian matrix doesn't really work because of the embedded degree matrix in the diagonal: Others approach the graph Laplacian by noticing the connection to Newton's law of cooling . Is there a better ""visual"" to see the intuition of the graph Laplacian?","4,","['multivariable-calculus', 'graph-theory', 'intuition', 'laplacian']"
92,"evaluate $\iiint\limits_{B}^{} \cosh \left ( x+y+z \right ) dxdydz$, where $B=\left \{ \left ( x,y,z \right ) \in R^3|x^2+y^2+z^2\le 1 \right \} $","evaluate , where","\iiint\limits_{B}^{} \cosh \left ( x+y+z \right ) dxdydz B=\left \{ \left ( x,y,z \right ) \in R^3|x^2+y^2+z^2\le 1 \right \} ","Evaluate $\iiint\limits_{B}^{} \cosh \left ( x+y+z \right ) dxdydz$ , where $B=\left \{ \left ( x,y,z \right ) \in R^3|x^2+y^2+z^2\le 1 \right \} $ Since B is a unit ball which centers at the origin, I think it should be highly usefull to change coordinates to spherical coordinates. And of course, by definition, $$\cosh\left (  x+y+z\right )  = \frac{e^{x+y+z}+e^{-x-y-z}}{2}$$ But after changing coordinates, I still know little about how to integral such thing like...... $$\int_{0}^{2\pi} \int_{0}^{\pi} \int_{0}^{1} \cosh \left ( r\sin \phi \cos \theta+r\sin \phi \sin \theta+r\cos \phi \right )r^2\sin \phi drd\phi d\theta $$ or $$\int_{0}^{2\pi} \int_{0}^{\pi} \int_{0}^{1} \frac{e^{\left ( r\sin \phi \cos \theta+r\sin \phi \sin \theta+r\cos \phi \right )}+e^{\left ( -r\sin \phi \cos \theta-r\sin \phi \sin \theta-r\cos \phi \right )}}{2} r^2\sin \phi drd\phi d\theta $$ Could anyone give some hints how I can do it please?","Evaluate , where Since B is a unit ball which centers at the origin, I think it should be highly usefull to change coordinates to spherical coordinates. And of course, by definition, But after changing coordinates, I still know little about how to integral such thing like...... or Could anyone give some hints how I can do it please?","\iiint\limits_{B}^{} \cosh \left ( x+y+z \right ) dxdydz B=\left \{ \left ( x,y,z \right ) \in R^3|x^2+y^2+z^2\le 1 \right \}  \cosh\left (  x+y+z\right )  = \frac{e^{x+y+z}+e^{-x-y-z}}{2} \int_{0}^{2\pi} \int_{0}^{\pi} \int_{0}^{1} \cosh \left ( r\sin \phi \cos \theta+r\sin \phi \sin \theta+r\cos \phi \right )r^2\sin \phi drd\phi d\theta  \int_{0}^{2\pi} \int_{0}^{\pi} \int_{0}^{1} \frac{e^{\left ( r\sin \phi \cos \theta+r\sin \phi \sin \theta+r\cos \phi \right )}+e^{\left ( -r\sin \phi \cos \theta-r\sin \phi \sin \theta-r\cos \phi \right )}}{2} r^2\sin \phi drd\phi d\theta ","['multivariable-calculus', 'definite-integrals', 'multiple-integral', 'spherical-coordinates']"
93,"If $\vec{\nabla} \cdot \vec{V} \neq 0$ at only one point, will this prevent us from saying that $\vec{V}=\vec{\nabla} \times \vec{U}$?","If  at only one point, will this prevent us from saying that ?",\vec{\nabla} \cdot \vec{V} \neq 0 \vec{V}=\vec{\nabla} \times \vec{U},"This question has an answer in the language of high level mathematics. Can somebody explain this in the language of vector calculus. Part I: Let us consider Cartesian coordinate system with origin $O$ and axes $x,y,z$ . Let: $$r=\sqrt{x^2+y^2+z^2}$$ $$\text{and }\vec{V}=0\ (\hat{i}) +\dfrac{\partial}{\partial z} \left( \dfrac{1}{r}  \right) (\hat{j}) -\dfrac{\partial}{\partial y} \left( \dfrac{1}{r}  \right) (\hat{k})$$ It is obvious that $\dfrac{1}{r}$ is defined everywhere except at the origin. Now let us take the divergence of $\vec{V}$ : $$\vec{\nabla} \cdot \vec{V}=0$$ Since $\dfrac{1}{r}$ is not defined at the origin, $\vec{\nabla} \cdot \vec{V}=0$ is true everywhere except at the origin. Since $\vec{\nabla} \cdot \vec{V} \neq 0$ at a point $P (0,0,0)$ , will this prevent us from concluding $\vec{V}=\vec{\nabla} \times \vec{U}$ at points other than $P$ ? Why? Why not? Part II: If $\vec{\nabla} \cdot \vec{V} \neq 0$ at points on a one dimensional arbitrary curve in space, will this prevent us from concluding $\vec{V}=\vec{\nabla} \times \vec{U}$ at other points not on the curve? Why? Why not? NOTE - For both Part I and Part II: If (Why/Why not) is beyond the scope of vector (multivariable) calculus, just reply as yes/no . However please try your best to explain (Why/Why not) in the language of vector (multivariable) calculus. SEMI ANSWER: Please point out the limitations I have stumbled upon a derivation in the language of elementary vector calculus. Please point out if there are any limitations in my derivation. In the context of advanced mathematics (de Rham cohomology or Poincare lemma), it seems to me that there are limitations. Derivation: To prove: At all points where $\vec{B}$ is defined (whatever be the domain of $\vec{B}$ ), if $\vec{\nabla} \cdot \vec{B}=0$ , then $\vec{B}=\vec{\nabla} \times \vec{A}$ Proof: At all points where $\vec{B}$ is defined (whatever be the domain of $\vec{B}$ ): \begin{align} \vec{B} &= B_x\ (\hat{i}) + B_y\ (\hat{j}) + B_z\ (\hat{k})\\ &= B_x\ (\hat{i}) + B_y\ (\hat{j}) + \int^{(x,y,z)}_{(x,y,\infty)} \dfrac{\partial B_z}{\partial z}\ dz\ (\hat{k})\\ &= B_x\ (\hat{i}) + B_y\ (\hat{j}) - \int^{(x,y,z)}_{(x,y,\infty)} \left(  \vec{\nabla} \cdot \vec{B} - \dfrac{\partial B_z}{\partial z}\   \right) dz\ (\hat{k})\\ &\text{{Since $\vec{\nabla} \cdot \vec{B}=0$}}\\ &= B_x\ (\hat{i}) + B_y\ (\hat{j}) - \int^{(x,y,z)}_{(x,y,\infty)} \left(   \dfrac{\partial B_x}{\partial x} + \dfrac{\partial B_y}{\partial y}\   \right) dz\ (\hat{k})\\ &= B_x\ (\hat{i}) + B_y\ (\hat{j})\ + \left[  \dfrac{\partial}{\partial x} \left(- \int^{(x,y,z)}_{(x,y,\infty)}B_x\ dz \right)  -\dfrac{\partial}{\partial y} \left(  \int^{(x,y,z)}_{(x,y,\infty)}B_y\ dz \right) \right] (\hat{k})\\ &\text{{By changing the order of integration and differentiation}}\\ \end{align} At all points where $\vec{B}$ is defined (whatever be the domain of $\vec{B}$ ), let's define: $\displaystyle \vec{A}=\int^{(x,y,z)}_{(x,y,\infty)}B_y\ dz\ (\hat{i}) - \int^{(x,y,z)}_{(x,y,\infty)}B_x\ dz\ (\hat{j}) + 0\ (\hat{k}) + \vec{\nabla}f$ where $f$ is an arbitrary function of $(x,y,z)$ Therefore at all points where $\vec{B}$ is defined (whatever be the domain of $\vec{B}$ ): \begin{align} \vec{B} &= \left( \dfrac{\partial A_z}{\partial y}-\dfrac{\partial A_y}{\partial z} \right) (\hat{i}) +\left( \dfrac{\partial A_x}{\partial z}-\dfrac{\partial A_z}{\partial x} \right) (\hat{j}) +\left( \dfrac{\partial A_y}{\partial x}-\dfrac{\partial A_x}{\partial y} \right) (\hat{k})\\ &= \vec{\nabla} \times \vec{A} \end{align}","This question has an answer in the language of high level mathematics. Can somebody explain this in the language of vector calculus. Part I: Let us consider Cartesian coordinate system with origin and axes . Let: It is obvious that is defined everywhere except at the origin. Now let us take the divergence of : Since is not defined at the origin, is true everywhere except at the origin. Since at a point , will this prevent us from concluding at points other than ? Why? Why not? Part II: If at points on a one dimensional arbitrary curve in space, will this prevent us from concluding at other points not on the curve? Why? Why not? NOTE - For both Part I and Part II: If (Why/Why not) is beyond the scope of vector (multivariable) calculus, just reply as yes/no . However please try your best to explain (Why/Why not) in the language of vector (multivariable) calculus. SEMI ANSWER: Please point out the limitations I have stumbled upon a derivation in the language of elementary vector calculus. Please point out if there are any limitations in my derivation. In the context of advanced mathematics (de Rham cohomology or Poincare lemma), it seems to me that there are limitations. Derivation: To prove: At all points where is defined (whatever be the domain of ), if , then Proof: At all points where is defined (whatever be the domain of ): At all points where is defined (whatever be the domain of ), let's define: where is an arbitrary function of Therefore at all points where is defined (whatever be the domain of ):","O x,y,z r=\sqrt{x^2+y^2+z^2} \text{and }\vec{V}=0\ (\hat{i})
+\dfrac{\partial}{\partial z} \left( \dfrac{1}{r}  \right) (\hat{j})
-\dfrac{\partial}{\partial y} \left( \dfrac{1}{r}  \right) (\hat{k}) \dfrac{1}{r} \vec{V} \vec{\nabla} \cdot \vec{V}=0 \dfrac{1}{r} \vec{\nabla} \cdot \vec{V}=0 \vec{\nabla} \cdot \vec{V} \neq 0 P (0,0,0) \vec{V}=\vec{\nabla} \times \vec{U} P \vec{\nabla} \cdot \vec{V} \neq 0 \vec{V}=\vec{\nabla} \times \vec{U} \vec{B} \vec{B} \vec{\nabla} \cdot \vec{B}=0 \vec{B}=\vec{\nabla} \times \vec{A} \vec{B} \vec{B} \begin{align}
\vec{B} &= B_x\ (\hat{i}) + B_y\ (\hat{j}) + B_z\ (\hat{k})\\
&= B_x\ (\hat{i}) + B_y\ (\hat{j}) + \int^{(x,y,z)}_{(x,y,\infty)}
\dfrac{\partial B_z}{\partial z}\ dz\ (\hat{k})\\
&= B_x\ (\hat{i}) + B_y\ (\hat{j}) - \int^{(x,y,z)}_{(x,y,\infty)}
\left(  \vec{\nabla} \cdot \vec{B} - \dfrac{\partial B_z}{\partial z}\   \right) dz\ (\hat{k})\\
&\text{{Since \vec{\nabla} \cdot \vec{B}=0}}\\
&= B_x\ (\hat{i}) + B_y\ (\hat{j}) - \int^{(x,y,z)}_{(x,y,\infty)}
\left(   \dfrac{\partial B_x}{\partial x} + \dfrac{\partial B_y}{\partial y}\   \right) dz\ (\hat{k})\\
&= B_x\ (\hat{i}) + B_y\ (\hat{j})\
+ \left[
 \dfrac{\partial}{\partial x}
\left(- \int^{(x,y,z)}_{(x,y,\infty)}B_x\ dz \right) 
-\dfrac{\partial}{\partial y}
\left(  \int^{(x,y,z)}_{(x,y,\infty)}B_y\ dz \right)
\right]
(\hat{k})\\
&\text{{By changing the order of integration and differentiation}}\\
\end{align} \vec{B} \vec{B} \displaystyle \vec{A}=\int^{(x,y,z)}_{(x,y,\infty)}B_y\ dz\ (\hat{i}) - \int^{(x,y,z)}_{(x,y,\infty)}B_x\ dz\ (\hat{j}) + 0\ (\hat{k}) + \vec{\nabla}f f (x,y,z) \vec{B} \vec{B} \begin{align}
\vec{B} &= \left(
\dfrac{\partial A_z}{\partial y}-\dfrac{\partial A_y}{\partial z}
\right) (\hat{i})
+\left(
\dfrac{\partial A_x}{\partial z}-\dfrac{\partial A_z}{\partial x}
\right) (\hat{j})
+\left(
\dfrac{\partial A_y}{\partial x}-\dfrac{\partial A_x}{\partial y}
\right) (\hat{k})\\
&= \vec{\nabla} \times \vec{A}
\end{align}","['multivariable-calculus', 'vector-analysis', 'vector-fields', 'divergence-operator', 'curl']"
94,General solution for the system of PDEs from the curl of a vector field equaling another,General solution for the system of PDEs from the curl of a vector field equaling another,,"In my vector calculus class, when we were introduced to the curl operator the professor gave us this example: Is it possible to find a vector field $\mathbf{G}$ such that  $$\mathbf{F} = \nabla \times {\mathbf{G}}?$$ As a motivation for the identity that the divergence of a curl of a vector field is $0$. My question is: Given that the identity holds true, how do you solve for a general solution for the component functions of $\mathbf{G}$, given $\mathbf{F}$? The best I can do is find one or two solutions. For example, if $\mathbf{F}$ = $\langle-y,-z,-x\rangle$, a solution I find is $\mathbf{G}$ = $\langle xy,0,-\frac{1}{2}y^2+xz\rangle$. I had to go through a system of partial differential equations and made it work, but can the general solution be written explicitly? Thanks!","In my vector calculus class, when we were introduced to the curl operator the professor gave us this example: Is it possible to find a vector field $\mathbf{G}$ such that  $$\mathbf{F} = \nabla \times {\mathbf{G}}?$$ As a motivation for the identity that the divergence of a curl of a vector field is $0$. My question is: Given that the identity holds true, how do you solve for a general solution for the component functions of $\mathbf{G}$, given $\mathbf{F}$? The best I can do is find one or two solutions. For example, if $\mathbf{F}$ = $\langle-y,-z,-x\rangle$, a solution I find is $\mathbf{G}$ = $\langle xy,0,-\frac{1}{2}y^2+xz\rangle$. I had to go through a system of partial differential equations and made it work, but can the general solution be written explicitly? Thanks!",,"['multivariable-calculus', 'partial-differential-equations']"
95,Greens theorem: why does path orientation matter?,Greens theorem: why does path orientation matter?,,"$$\oint_{\partial D} P\;dx + Q\;dy = \iint_D \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right)\;dA$$ Is it correct to interpret this equation as relating the surface area of a three dimensional object (a subset of R3) to the closed path around the object (or the perimeter)? Assuming this is the correct interpretation, why does one need to concern themselves with the clockwise/counter-clockwise convention when: 1) The start and endpoint of the path are the same, regardless of how one travels the boundary of the object. 2) What meaning could a negative surface area have? update: I know wikipedia tells you this a two-dimensional formula, yet from the left side of the equation the integral would look like it resolves into some function F(x2,y2) - F(x1,y1) and furthermore I don't see the use of orientation or vector calculus in 2d when one can use non-vector calculus to find areas in 2d.","$$\oint_{\partial D} P\;dx + Q\;dy = \iint_D \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right)\;dA$$ Is it correct to interpret this equation as relating the surface area of a three dimensional object (a subset of R3) to the closed path around the object (or the perimeter)? Assuming this is the correct interpretation, why does one need to concern themselves with the clockwise/counter-clockwise convention when: 1) The start and endpoint of the path are the same, regardless of how one travels the boundary of the object. 2) What meaning could a negative surface area have? update: I know wikipedia tells you this a two-dimensional formula, yet from the left side of the equation the integral would look like it resolves into some function F(x2,y2) - F(x1,y1) and furthermore I don't see the use of orientation or vector calculus in 2d when one can use non-vector calculus to find areas in 2d.",,"['soft-question', 'multivariable-calculus']"
96,$\frac{\partial F}{\partial y}\neq0\implies$ continuous contour line? (Implicit Function Theorem),continuous contour line? (Implicit Function Theorem),\frac{\partial F}{\partial y}\neq0\implies,"I have a function $F(x,y)=z$ and two points $(x_1,y_1),(x_2,y_2)$ s.t. $F(x_1,y_1)=F(x_2,y_2)=c$ , $x_1<x_2$ . I know that $\frac{\partial F}{\partial y}<0$ in $ [x_1,x_2]\times\mathbb{R}$ . I'd like to prove that there is a continuous contour line between the two points. I know that there's a rectangle $V\times W $ that contains $(x_1,y_1)$ s.t. $F^{-1}(c)\cap V\times W $ is the graphic of a function, i.e., I have in there a continuous contour line. I'd like to know if the fact that I have $\frac{\partial F}{\partial y}<0$ in the entire interval $[x_1,x_2]$ allows that I consider $V=[x_1,x_2]$ and $W=\mathbb{R}$ , so I can prove the statement. Many thanks! ========== Edit for comment on 12/26 Edit for new comments","I have a function and two points s.t. , . I know that in . I'd like to prove that there is a continuous contour line between the two points. I know that there's a rectangle that contains s.t. is the graphic of a function, i.e., I have in there a continuous contour line. I'd like to know if the fact that I have in the entire interval allows that I consider and , so I can prove the statement. Many thanks! ========== Edit for comment on 12/26 Edit for new comments","F(x,y)=z (x_1,y_1),(x_2,y_2) F(x_1,y_1)=F(x_2,y_2)=c x_1<x_2 \frac{\partial F}{\partial y}<0  [x_1,x_2]\times\mathbb{R} V\times W  (x_1,y_1) F^{-1}(c)\cap V\times W  \frac{\partial F}{\partial y}<0 [x_1,x_2] V=[x_1,x_2] W=\mathbb{R}","['multivariable-calculus', 'continuity', 'partial-derivative', 'implicit-function-theorem']"
97,Lagrange interpolation of multivariate polynomials,Lagrange interpolation of multivariate polynomials,,"Given a univariate polynomial of degree $n$ with coefficients from a field, the polynomial is uniquely defined by $n+1$ evaluation points. That is, given $n+1$ points $(i, f(i))$ , one can uniquely determine $f$ . Is there a similar statement for multivariate polynomials? For example, given a polynomial $f(x,y)$ , where the degree in each variable is at most $n$ , how many evaluation points uniquely determine this polynomial? My current intuition: If I fix the first variable to say 1, then I get that if polynomials $f$ and $g$ of degree $n$ agree on $(1 ,y_1), ~\dots~, (1 ,y_{n+1})$ , then I have $f(1,x) = g(1,x)$ for all $x$ (since fixing the polynomials in one variable gives me univariate polynomials of degree $n$ ). Now I can define a polynomial $h(x,y) = f(x,y) - g(x,y)$ , which is $0$ at any point of the form $(1,x)$ . Assume $f$ and $g$ agree on the points above, but $f \neq g$ , then maybe I can say something about $h$ not being the constant $0$ polynomial and having too many roots? Is there a bound on how many roots a multivariate polynomial can have? Furthermore, for this intuition I assumed that the polynomials agree on some specific points. Preferably, I'd like to have an argument that starts with given a set of arbitrary points.","Given a univariate polynomial of degree with coefficients from a field, the polynomial is uniquely defined by evaluation points. That is, given points , one can uniquely determine . Is there a similar statement for multivariate polynomials? For example, given a polynomial , where the degree in each variable is at most , how many evaluation points uniquely determine this polynomial? My current intuition: If I fix the first variable to say 1, then I get that if polynomials and of degree agree on , then I have for all (since fixing the polynomials in one variable gives me univariate polynomials of degree ). Now I can define a polynomial , which is at any point of the form . Assume and agree on the points above, but , then maybe I can say something about not being the constant polynomial and having too many roots? Is there a bound on how many roots a multivariate polynomial can have? Furthermore, for this intuition I assumed that the polynomials agree on some specific points. Preferably, I'd like to have an argument that starts with given a set of arbitrary points.","n n+1 n+1 (i, f(i)) f f(x,y) n f g n (1 ,y_1), ~\dots~, (1 ,y_{n+1}) f(1,x) = g(1,x) x n h(x,y) = f(x,y) - g(x,y) 0 (1,x) f g f \neq g h 0","['multivariable-calculus', 'polynomials', 'lagrange-interpolation', 'multivariate-polynomial']"
98,Directional Derivatives - Geometric intuition,Directional Derivatives - Geometric intuition,,"I just have a question about directional derivatives. I know that for a unit vector $v$, it is $\nabla f \cdot v$ (where $v$ is the direction we want to find the derivative of). The proof of this using limits makes sense, but how would you look at this geometrically? The gradient vector is the direction in which the function increases the most, but then having a dot product of this with a unit vector apparently gives the derivative going in that direction. I'm not quite sure how to interpret the dot product geometrically. Thank you!","I just have a question about directional derivatives. I know that for a unit vector $v$, it is $\nabla f \cdot v$ (where $v$ is the direction we want to find the derivative of). The proof of this using limits makes sense, but how would you look at this geometrically? The gradient vector is the direction in which the function increases the most, but then having a dot product of this with a unit vector apparently gives the derivative going in that direction. I'm not quite sure how to interpret the dot product geometrically. Thank you!",,"['multivariable-calculus', 'vector-spaces', 'vectors', 'inner-products']"
99,"For non-negative reals $a$, $b$, $c$, show that $3(1-a+a^2)(1-b+b^2)(1-c+c^2)\ge(1+abc+a^2b^2c^2)$","For non-negative reals , , , show that",a b c 3(1-a+a^2)(1-b+b^2)(1-c+c^2)\ge(1+abc+a^2b^2c^2),"A 11th grade inequality problem: Let $a,b,c$ be non-negative real numbers. Prove that $$3(1-a+a^2)(1-b+b^2)(1-c+c^2)\ge(1+abc+a^2b^2c^2)$$ Do you have any hints to solve this inequality? Any hints would be fine. I tried this: $$3(1-a+a^2)(1-b+b^2)(1-c+c^2)\ge(1+abc+a^2b^2c^2)$$ By Cauchy's inequality, $$a^2+1\ge(2a)$$ and I did the same to $b$ and $c$ and applied it to the problem but the results are $$2abc\ge1+a^2b^2c^2$$ and this is wrong.","A 11th grade inequality problem: Let be non-negative real numbers. Prove that Do you have any hints to solve this inequality? Any hints would be fine. I tried this: By Cauchy's inequality, and I did the same to and and applied it to the problem but the results are and this is wrong.","a,b,c 3(1-a+a^2)(1-b+b^2)(1-c+c^2)\ge(1+abc+a^2b^2c^2) 3(1-a+a^2)(1-b+b^2)(1-c+c^2)\ge(1+abc+a^2b^2c^2) a^2+1\ge(2a) b c 2abc\ge1+a^2b^2c^2","['multivariable-calculus', 'inequality', 'symmetric-polynomials', 'holder-inequality']"
