,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How do I write power series for $\sum_{n=0}^{\infty}(n)(n-1)(z/5)^n$?,How do I write power series for ?,\sum_{n=0}^{\infty}(n)(n-1)(z/5)^n,What I know is that $n(n-1)$ is the derivative of $\frac{n^3}{3}-\frac{n^2}{2}$.I don't know what to do next? I have even compared it with $\sum(a_n z')$ and found that here $a_n$ is my $n(n-1)$  and my $z'$ is my $z/5$. What to do next??,What I know is that $n(n-1)$ is the derivative of $\frac{n^3}{3}-\frac{n^2}{2}$.I don't know what to do next? I have even compared it with $\sum(a_n z')$ and found that here $a_n$ is my $n(n-1)$  and my $z'$ is my $z/5$. What to do next??,,"['calculus', 'sequences-and-series', 'derivatives']"
1,Confusion about partial and total derivatives.,Confusion about partial and total derivatives.,,"I stumbled upon this: And there is something that specifically bothers me. Say $$L(q(t),\dot{q}(t),t)$$ Then I claim it makes no sense that $\frac{\partial L}{\partial t}$ only differentiates the explicit dependencies of $L$ on $t$ without also differentiating the contributions of $q(t)$ and $\dot{q}(t)$. Why do I claim this? Well suppose we have a function such that $$f(x(r,\theta),y(r,\theta))$$ Then the operator $\frac{\partial}{\partial r}$ does act on the functions and not only on ""explicit dependences"". What is going on here? It might look as if I'm an advanced student because of the image I've posted, but I really am not, so don't be misguided (if that were to be the case). Thanks.","I stumbled upon this: And there is something that specifically bothers me. Say $$L(q(t),\dot{q}(t),t)$$ Then I claim it makes no sense that $\frac{\partial L}{\partial t}$ only differentiates the explicit dependencies of $L$ on $t$ without also differentiating the contributions of $q(t)$ and $\dot{q}(t)$. Why do I claim this? Well suppose we have a function such that $$f(x(r,\theta),y(r,\theta))$$ Then the operator $\frac{\partial}{\partial r}$ does act on the functions and not only on ""explicit dependences"". What is going on here? It might look as if I'm an advanced student because of the image I've posted, but I really am not, so don't be misguided (if that were to be the case). Thanks.",,"['calculus', 'derivatives']"
2,Check this function is continuously differentiable on $\mathbb{R}^2$,Check this function is continuously differentiable on,\mathbb{R}^2,"by http://mathworld.wolfram.com/GreensTheorem.html (Green's theorem) I choose $Q=0$ and defind P(x,y) : $$ P(x,y) = \left\{ \begin{array}{ll} \frac{e^{-xy}\sin x}{x} & \text{if} ~~x\ne 0 \\ 1 & \textrm{if}~~ x=0 \\ \end{array} \right. $$ How to check this function is continuously differentiable on $\mathbb{R}^2$","by http://mathworld.wolfram.com/GreensTheorem.html (Green's theorem) I choose $Q=0$ and defind P(x,y) : $$ P(x,y) = \left\{ \begin{array}{ll} \frac{e^{-xy}\sin x}{x} & \text{if} ~~x\ne 0 \\ 1 & \textrm{if}~~ x=0 \\ \end{array} \right. $$ How to check this function is continuously differentiable on $\mathbb{R}^2$",,"['calculus', 'real-analysis', 'functional-analysis', 'derivatives']"
3,"Finding y prime, derivative trouble","Finding y prime, derivative trouble",,"I have $$y=\frac{4x}{x^{3/2}} - 8x - 2 \cos(\frac{\pi}{4}) $$ The best I could get it to was $$\frac{4}{2}x^{-3/4} - 6x - 2\cos(\frac{\pi}{4})  $$ but the answer doesn't even have a cos or even sin in it. I understood the example problem before this just fine, but it was a lot simpler and I'm completely lost on this...","I have $$y=\frac{4x}{x^{3/2}} - 8x - 2 \cos(\frac{\pi}{4}) $$ The best I could get it to was $$\frac{4}{2}x^{-3/4} - 6x - 2\cos(\frac{\pi}{4})  $$ but the answer doesn't even have a cos or even sin in it. I understood the example problem before this just fine, but it was a lot simpler and I'm completely lost on this...",,"['calculus', 'derivatives']"
4,On an equivalent definition of the derivative.,On an equivalent definition of the derivative.,,I am following Real Mathematical Analysis by Pugh Pugh says that the statement $$f(x+h) = f(x) +f'(x)h + R(h) \implies \lim_{h \rightarrow 0} \frac{R(h)}{|h|} = 0 $$ is equivalent to the definition of derivative  $$\lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h} = f'(x)$$ I am having troubles with proving that one implies the other and the backwards direction too (I am not sure how can I pull $h$ out of the limit).,I am following Real Mathematical Analysis by Pugh Pugh says that the statement $$f(x+h) = f(x) +f'(x)h + R(h) \implies \lim_{h \rightarrow 0} \frac{R(h)}{|h|} = 0 $$ is equivalent to the definition of derivative  $$\lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h} = f'(x)$$ I am having troubles with proving that one implies the other and the backwards direction too (I am not sure how can I pull $h$ out of the limit).,,"['real-analysis', 'derivatives']"
5,Prove That $f(x)>f(y)+f'(y)(x-y)$ if $f''(x)>0$ For All $x$,Prove That  if  For All,f(x)>f(y)+f'(y)(x-y) f''(x)>0 x,"Here's my question: Let $f$ be a function in a interval $I$ , where $f''(x)>0$ for all $x\in I$ . Prove that for every $x,y \in I$ $$f(x)>f(y)+f'(y)(x-y)$$ I'm sorry to say that but, I don't have an idea how to solve it. I tried to move all the arguments to one side and calculate the derivative of it. $$f'(x)-f'(y)-f''(y)(x-y)$$ I also know that since $f''(x)>0$ , $f'$ is monotonic increasing. But nothing more. Any ideas? Thanks, Alan","Here's my question: Let be a function in a interval , where for all . Prove that for every I'm sorry to say that but, I don't have an idea how to solve it. I tried to move all the arguments to one side and calculate the derivative of it. I also know that since , is monotonic increasing. But nothing more. Any ideas? Thanks, Alan","f I f''(x)>0 x\in I x,y \in I f(x)>f(y)+f'(y)(x-y) f'(x)-f'(y)-f''(y)(x-y) f''(x)>0 f'","['calculus', 'real-analysis', 'derivatives']"
6,Binomial expansion derivative limit definition,Binomial expansion derivative limit definition,,Can someone help me with this? I am supposed to use a binomial expansion to calculate $\sqrt x$ directly from the limit definition of a derivative.,Can someone help me with this? I am supposed to use a binomial expansion to calculate $\sqrt x$ directly from the limit definition of a derivative.,,"['calculus', 'limits', 'derivatives']"
7,"Let $f$ be double differentiable function such that $|f′′(x)|\le 1$ for all $x\in [0,1]$. If $f(0)=f(1)$, then,","Let  be double differentiable function such that  for all . If , then,","f |f′′(x)|\le 1 x\in [0,1] f(0)=f(1)","options: A) $|f(x)|>1 $ B) $|f(x)|<1  $ C) $|f′(x)|>1 $ D) $|f′(x)|<1$ attempt: I first tried using integration. $−1\le f′′(x)\le 1$ integrating from $0$ to $x$, $−x\le f′(x)−f′(0)\le x$ Again integrating from $0$ to $x$, $\frac{−x^2}{2}\le f(x)−f(0)−f′(0)x\le\frac{x^2}{2}$ at $x=1$, $−\frac{1}{2}\le f′(0)\le\frac{1}{2}$ from the equation, $−x\le f′(x)−f(0)\le x$ and by substituting the max value of $f'(0)$, $-x+0.5\le f′(x)\le x+0.5$ But this doesn't give me the correct answer.","options: A) $|f(x)|>1 $ B) $|f(x)|<1  $ C) $|f′(x)|>1 $ D) $|f′(x)|<1$ attempt: I first tried using integration. $−1\le f′′(x)\le 1$ integrating from $0$ to $x$, $−x\le f′(x)−f′(0)\le x$ Again integrating from $0$ to $x$, $\frac{−x^2}{2}\le f(x)−f(0)−f′(0)x\le\frac{x^2}{2}$ at $x=1$, $−\frac{1}{2}\le f′(0)\le\frac{1}{2}$ from the equation, $−x\le f′(x)−f(0)\le x$ and by substituting the max value of $f'(0)$, $-x+0.5\le f′(x)\le x+0.5$ But this doesn't give me the correct answer.",,"['calculus', 'derivatives']"
8,Is there enough information given to solve this related rates problem?,Is there enough information given to solve this related rates problem?,,"This is the question from a practice exam: Suppose a pyramid has 4 lateral faces that are all equilateral triangles. Find the rate at which the volume of the pyramid is changing if each side of each triangle increases at a rate of 2 inches per second. Hint: The volume of a square pyramid is given by the formula $V={1\over 3}Bh$, where $B$ is the area of the base of the pyramid and $h$ is the height of the pyramid. This is the only information I am given. The answer key has just been released, so I know the answer is $16\sqrt{2}in^3/sec$, but I don't know how to get there. And then a friend (who was helping me) thought that maybe there isn't enough information. Is there enough? EDIT: I used the answer given by the key and worked out the problem. I believe that the missing information is ""Find the rate at which the volume of the pyramid is changing when the side length is 4 inches."" When I put $h$ (height) in terms of $s$ (side length), then differentiated implicitly, and plugging in $4$ for $s$ at the end, I found the solution to be ${16\cdot2\over \sqrt{2}}$, which simplifies to $16\sqrt{2}$ by rationalizing the denominator.","This is the question from a practice exam: Suppose a pyramid has 4 lateral faces that are all equilateral triangles. Find the rate at which the volume of the pyramid is changing if each side of each triangle increases at a rate of 2 inches per second. Hint: The volume of a square pyramid is given by the formula $V={1\over 3}Bh$, where $B$ is the area of the base of the pyramid and $h$ is the height of the pyramid. This is the only information I am given. The answer key has just been released, so I know the answer is $16\sqrt{2}in^3/sec$, but I don't know how to get there. And then a friend (who was helping me) thought that maybe there isn't enough information. Is there enough? EDIT: I used the answer given by the key and worked out the problem. I believe that the missing information is ""Find the rate at which the volume of the pyramid is changing when the side length is 4 inches."" When I put $h$ (height) in terms of $s$ (side length), then differentiated implicitly, and plugging in $4$ for $s$ at the end, I found the solution to be ${16\cdot2\over \sqrt{2}}$, which simplifies to $16\sqrt{2}$ by rationalizing the denominator.",,"['calculus', 'derivatives']"
9,Solving for y' in a fraction,Solving for y' in a fraction,,Given the equation $x+xy^2 = \tan^{-1}(x^2y)$ find $y'$. I have tried doing this but solving for $y'$ I need some help and would like your advice. Work so far... $$1+y^2+2xy\left(\frac{dy}{dx}\right)= \frac{2xy+x^2\left(\frac{dy}{dx}\right)}{1+x^4y^2}$$ What can be done now to solve for $y'$?,Given the equation $x+xy^2 = \tan^{-1}(x^2y)$ find $y'$. I have tried doing this but solving for $y'$ I need some help and would like your advice. Work so far... $$1+y^2+2xy\left(\frac{dy}{dx}\right)= \frac{2xy+x^2\left(\frac{dy}{dx}\right)}{1+x^4y^2}$$ What can be done now to solve for $y'$?,,"['calculus', 'derivatives']"
10,Multiplicity of a root of a polynomial,Multiplicity of a root of a polynomial,,":) It's true that, if a polynomial has a root (let's say,  k, for example) with multiplicity n (n>1, for n integer), then it's true that the derivate polynomial have k as a root with multiplicity n-1. The reciprocal is always true? I.e., it's true that if a derivative polynomial have k as a root with multiplicity n-1, then the original polynomial have k as a root with multiplicity n? If this is not always true, in which conditions this is true? Thanks every one! :)",":) It's true that, if a polynomial has a root (let's say,  k, for example) with multiplicity n (n>1, for n integer), then it's true that the derivate polynomial have k as a root with multiplicity n-1. The reciprocal is always true? I.e., it's true that if a derivative polynomial have k as a root with multiplicity n-1, then the original polynomial have k as a root with multiplicity n? If this is not always true, in which conditions this is true? Thanks every one! :)",,"['algebra-precalculus', 'derivatives', 'polynomials']"
11,"Given that $f(1) = f'(1) = 1$, use Taylor polynomials to show that $\lvert f(x) - x \rvert \leq A(x - 1)^2$","Given that , use Taylor polynomials to show that",f(1) = f'(1) = 1 \lvert f(x) - x \rvert \leq A(x - 1)^2,"Given that $\ f$ has continuous second derivatives in$\ [0,2]$ and  $\ f(1)=f'(1)=1$, I'm trying to prove that for every $\ x \in [0,2]$ exists an A so that: $$ |f(x)-x| \le A(x-1)^2 $$ The second derivative made me try Taylor with little success, so far I manage to develop the data on both sides, but I just can't get them to connect. Any ideas?","Given that $\ f$ has continuous second derivatives in$\ [0,2]$ and  $\ f(1)=f'(1)=1$, I'm trying to prove that for every $\ x \in [0,2]$ exists an A so that: $$ |f(x)-x| \le A(x-1)^2 $$ The second derivative made me try Taylor with little success, so far I manage to develop the data on both sides, but I just can't get them to connect. Any ideas?",,"['calculus', 'derivatives', 'taylor-expansion']"
12,Show $f(x) = x^2\sin{\frac{1}{x}} + \frac{x}{2}$ is not increasing on any open interval containing $0$,Show  is not increasing on any open interval containing,f(x) = x^2\sin{\frac{1}{x}} + \frac{x}{2} 0,"$f(x) = x^2\sin{\frac{1}{x}} + \frac{x}{2}$ for $x\not= 0$ and $f(0) = 0$. Show $f$ is not increasing on any open interval containing $0$. At first glance, we notice $f'(x) \le 0$ for some $x \in I$ where $I$ is any open interval. We can also recognize $f'(0) > 0$ by directly plugging in $0$ into the definition of a limit: $f'(x) = \lim_{x\to x_0}{\frac{f(x)-f(x_0)}{x-x_0}}$ We know that if $f'(0)>0$, then $f'(x)<0$ either above or below $0$. However, at this point I get stuck on how to continue.","$f(x) = x^2\sin{\frac{1}{x}} + \frac{x}{2}$ for $x\not= 0$ and $f(0) = 0$. Show $f$ is not increasing on any open interval containing $0$. At first glance, we notice $f'(x) \le 0$ for some $x \in I$ where $I$ is any open interval. We can also recognize $f'(0) > 0$ by directly plugging in $0$ into the definition of a limit: $f'(x) = \lim_{x\to x_0}{\frac{f(x)-f(x_0)}{x-x_0}}$ We know that if $f'(0)>0$, then $f'(x)<0$ either above or below $0$. However, at this point I get stuck on how to continue.",,"['real-analysis', 'derivatives']"
13,Show that $\|Du_{\lambda}\|_{L^2(\mathbb{R}^n)} = \|Du\|_{L^2(\mathbb{R}^n)}$,Show that,\|Du_{\lambda}\|_{L^2(\mathbb{R}^n)} = \|Du\|_{L^2(\mathbb{R}^n)},"Let $x \in \mathbb{R}^n$. Given $$u(x) := \left(\frac 1{1+|x|^2} \right)^{\frac{n-2}2}, \quad u_\lambda(x):=\left(\frac \lambda{\lambda^2+|x|^2} \right)^{\frac{n-2}2},$$ I need to show that  $\|Du_{\lambda}\|_{L^2(\mathbb{R}^n)} = \|Du\|_{L^2(\mathbb{R}^n)}$. (Note that $Du=\nabla u = \sum_{i=1}^n u_{x_i}$.) Equivalently, I can show $\|Du_{\lambda}\|_{L^2(\mathbb{R}^n)}^2 = \|Du\|_{L^2(\mathbb{R}^n)}^2$. My following derivatives are $$\small {u_{x_i}=(2-n)\left(\frac 1{1+|x|^2} \right)^{\frac{n-2}2}\left(\frac 1{1+|x|^2} \right) |x| x_i  \text{ and } (u_\lambda)_{x_i}=(2-n)\left(\frac {\lambda}{\lambda^2+|x|^2} \right)^{\frac{n-2}2}\left(\frac 1{\lambda^2+|x|^2} \right) |x| x_i,}$$ for $i \in \{1,\ldots,n\}$. Thus, $$\small{Du =(2-n)\left(\frac 1{1+|x|^2} \right)^{\frac{n-2}2}\left(\frac {|x|^2}{1+|x|^2} \right) \text{ and }Du_{\lambda}=(2-n)\left(\frac {\lambda}{\lambda^2+|x|^2} \right)^{\frac{n-2}2}\left(\frac {|x|^2}{\lambda^2+|x|^2} \right).}$$ Therefore, $$\small{\|Du\|_{L^2(\mathbb{R}^n)}^2=\int_{\mathbb{R}^n} (2-n)^2\left( \frac 1{1+|x|^2}\right)^{n-2} \frac {|x|^4}{1+|x|^2} \, dx, \\\|Du_{\lambda}\|_{L^2(\mathbb{R}^n)}^2=\int_{\mathbb{R}^n} (2-n)^2\left( \frac {\lambda}{\lambda^2+|x|^2}\right)^{n-2} \frac {|x|^4}{\lambda^2+|x|^2} \, dx}.$$ For $\|Du_{\lambda}\|_{L^2(\mathbb{R}^n)}$, I perform a substitution of $u_i = \frac{x_i}{\lambda}$ and $du_i=\frac{dx_i}{\lambda}$. Then $|u|=\frac{|x|}{\lambda}$ and $du=\frac{dx}{\lambda^n}$. When I apply these substitutions, and simplify algebraically, I obtained $$\|Du\|_{L^2(\mathbb{R}^n)}^2=\int_{\mathbb{R}^n} (2-n)^2 \color{red}{\lambda^2} \frac {|u|^4}{(1+|u|^2)^n} \, du.$$ The problem is that I get an extra $\lambda^2$ that is not supposed to be there...","Let $x \in \mathbb{R}^n$. Given $$u(x) := \left(\frac 1{1+|x|^2} \right)^{\frac{n-2}2}, \quad u_\lambda(x):=\left(\frac \lambda{\lambda^2+|x|^2} \right)^{\frac{n-2}2},$$ I need to show that  $\|Du_{\lambda}\|_{L^2(\mathbb{R}^n)} = \|Du\|_{L^2(\mathbb{R}^n)}$. (Note that $Du=\nabla u = \sum_{i=1}^n u_{x_i}$.) Equivalently, I can show $\|Du_{\lambda}\|_{L^2(\mathbb{R}^n)}^2 = \|Du\|_{L^2(\mathbb{R}^n)}^2$. My following derivatives are $$\small {u_{x_i}=(2-n)\left(\frac 1{1+|x|^2} \right)^{\frac{n-2}2}\left(\frac 1{1+|x|^2} \right) |x| x_i  \text{ and } (u_\lambda)_{x_i}=(2-n)\left(\frac {\lambda}{\lambda^2+|x|^2} \right)^{\frac{n-2}2}\left(\frac 1{\lambda^2+|x|^2} \right) |x| x_i,}$$ for $i \in \{1,\ldots,n\}$. Thus, $$\small{Du =(2-n)\left(\frac 1{1+|x|^2} \right)^{\frac{n-2}2}\left(\frac {|x|^2}{1+|x|^2} \right) \text{ and }Du_{\lambda}=(2-n)\left(\frac {\lambda}{\lambda^2+|x|^2} \right)^{\frac{n-2}2}\left(\frac {|x|^2}{\lambda^2+|x|^2} \right).}$$ Therefore, $$\small{\|Du\|_{L^2(\mathbb{R}^n)}^2=\int_{\mathbb{R}^n} (2-n)^2\left( \frac 1{1+|x|^2}\right)^{n-2} \frac {|x|^4}{1+|x|^2} \, dx, \\\|Du_{\lambda}\|_{L^2(\mathbb{R}^n)}^2=\int_{\mathbb{R}^n} (2-n)^2\left( \frac {\lambda}{\lambda^2+|x|^2}\right)^{n-2} \frac {|x|^4}{\lambda^2+|x|^2} \, dx}.$$ For $\|Du_{\lambda}\|_{L^2(\mathbb{R}^n)}$, I perform a substitution of $u_i = \frac{x_i}{\lambda}$ and $du_i=\frac{dx_i}{\lambda}$. Then $|u|=\frac{|x|}{\lambda}$ and $du=\frac{dx}{\lambda^n}$. When I apply these substitutions, and simplify algebraically, I obtained $$\|Du\|_{L^2(\mathbb{R}^n)}^2=\int_{\mathbb{R}^n} (2-n)^2 \color{red}{\lambda^2} \frac {|u|^4}{(1+|u|^2)^n} \, du.$$ The problem is that I get an extra $\lambda^2$ that is not supposed to be there...",,"['integration', 'derivatives']"
14,Limit of a sum of two variables,Limit of a sum of two variables,,"Recently at my calculus course we are doing derivatives and integrals. I've stumbled upon a sum that seems to have nothing in common with our current objectives, though I'm sure it does have, but still I have no idea how to solve it. $$ \lim_{n\to\infty} \sum_{k=1}^{n} \frac{1}{4n - \frac{k^2}{n}} $$ I tried to represent it as a $$ f_{n}(x) = \sum_{k=1}^{n} \frac{x}{4n - \frac{k^2}{n}}$$ and to resolve it at a point $x = 1$, but I failed because of the two variables instead of one. EDIT: Of course, I would like to get some CLUES how to solve it, not the whole solution.","Recently at my calculus course we are doing derivatives and integrals. I've stumbled upon a sum that seems to have nothing in common with our current objectives, though I'm sure it does have, but still I have no idea how to solve it. $$ \lim_{n\to\infty} \sum_{k=1}^{n} \frac{1}{4n - \frac{k^2}{n}} $$ I tried to represent it as a $$ f_{n}(x) = \sum_{k=1}^{n} \frac{x}{4n - \frac{k^2}{n}}$$ and to resolve it at a point $x = 1$, but I failed because of the two variables instead of one. EDIT: Of course, I would like to get some CLUES how to solve it, not the whole solution.",,"['calculus', 'derivatives']"
15,Finding horizontal tangents to a function.,Finding horizontal tangents to a function.,,Find the points at which the line tangent to the following function is horizontal $$q(x)=(x+3)^4(2x-1)^7$$ Every time I've gotten to the point of finding $x$ the numbers are all irrationally too large. I've gotten $2(2x-1)^6(x+3)^3(11x+19).$ But I do not see a way to solve from these numbers. Thank you for any help.,Find the points at which the line tangent to the following function is horizontal $$q(x)=(x+3)^4(2x-1)^7$$ Every time I've gotten to the point of finding $x$ the numbers are all irrationally too large. I've gotten $2(2x-1)^6(x+3)^3(11x+19).$ But I do not see a way to solve from these numbers. Thank you for any help.,,"['calculus', 'algebra-precalculus', 'derivatives', 'polynomials', 'analytic-geometry']"
16,What derivative should be taken for relative maxima and absolute maxima (or minima)?,What derivative should be taken for relative maxima and absolute maxima (or minima)?,,I get confused on what derivative should be taken for defining relative maxima and absolute maxima because some sources said to use first derivative while the others said to use second derivative. Also is relative maxima same as local maxima? I really need detail with prove or example about them.,I get confused on what derivative should be taken for defining relative maxima and absolute maxima because some sources said to use first derivative while the others said to use second derivative. Also is relative maxima same as local maxima? I really need detail with prove or example about them.,,"['derivatives', 'graphing-functions']"
17,Existence of complicated convex functions,Existence of complicated convex functions,,"In Stochastic Finance: An Introduction In Discrete Time (by Follmer, Schied), page 400, I found the following proposition: Proposition A.4. Let $I\subseteq\mathbb R$ be an open interval and $f:I\to\mathbb R$ a convex function. Then (i) $f$ is continuous in $I$ (ii) for all $x\in I$ the function $f$ admits left- and right-hand derivatives $$f_-'(x) = \lim_{y\nearrow x} \frac{f(y)-f(x)}{y-x},\quad f_+'(x) = \lim_{y\searrow x} \frac{f(y)-f(x)}{y-x}$$ (iii) $f$ is Lebesgue almost everywhere differentiable in $I$ Let $D_f$ denote the set of points $z\in I$ such that $f_-'(z)<f_+'(z)$. By (iii) the set $D_f$ must always have Lebesgue measure zero. Now $f(x)=|x|$ and $g(x)=\max(1-x,-x-1,1)$ are convex functions for which $D_f=\{0\}$ and $D_g=\{-2,2\}$ respectively. But I am wondering whether there are more abstract convex functions such that $D$ is perhaps countable or uncountable (Cantor set?).","In Stochastic Finance: An Introduction In Discrete Time (by Follmer, Schied), page 400, I found the following proposition: Proposition A.4. Let $I\subseteq\mathbb R$ be an open interval and $f:I\to\mathbb R$ a convex function. Then (i) $f$ is continuous in $I$ (ii) for all $x\in I$ the function $f$ admits left- and right-hand derivatives $$f_-'(x) = \lim_{y\nearrow x} \frac{f(y)-f(x)}{y-x},\quad f_+'(x) = \lim_{y\searrow x} \frac{f(y)-f(x)}{y-x}$$ (iii) $f$ is Lebesgue almost everywhere differentiable in $I$ Let $D_f$ denote the set of points $z\in I$ such that $f_-'(z)<f_+'(z)$. By (iii) the set $D_f$ must always have Lebesgue measure zero. Now $f(x)=|x|$ and $g(x)=\max(1-x,-x-1,1)$ are convex functions for which $D_f=\{0\}$ and $D_g=\{-2,2\}$ respectively. But I am wondering whether there are more abstract convex functions such that $D$ is perhaps countable or uncountable (Cantor set?).",,"['real-analysis', 'derivatives', 'convex-analysis']"
18,"If $f+f'<\varepsilon$, then $f'<\varepsilon$","If , then",f+f'<\varepsilon f'<\varepsilon,"Let $f$ be continuous on $[a,b]$ and differentiable on $(a,b)$ and there exstis $\varepsilon>0$ such that $f(x)+f'(x)<\varepsilon$ for all $x\in (a,b)$. Prove that $f'(x)<\varepsilon$ for all $x\in (a,b)$. These are my efforts: $\bullet$ If $f(x)\ge 0$ then $f'(x)<\varepsilon$, so we just need to consider those $x$ such that $f(x)<0$. $\bullet$ Consider $g(x)=f(x)\cdot e^x-\varepsilon\cdot e^x$ then $g$ decreases on $(a,b)$. If $f(c)<\varepsilon$, then $f(x)<\varepsilon$ for all $x\in [c,b]$. $\bullet$ If $f(x_0)\ge \varepsilon$ for some $x_0$ then $f$ increases in some neighborhood of $x$. I cannot completely solve it. Thanks so much for any help.","Let $f$ be continuous on $[a,b]$ and differentiable on $(a,b)$ and there exstis $\varepsilon>0$ such that $f(x)+f'(x)<\varepsilon$ for all $x\in (a,b)$. Prove that $f'(x)<\varepsilon$ for all $x\in (a,b)$. These are my efforts: $\bullet$ If $f(x)\ge 0$ then $f'(x)<\varepsilon$, so we just need to consider those $x$ such that $f(x)<0$. $\bullet$ Consider $g(x)=f(x)\cdot e^x-\varepsilon\cdot e^x$ then $g$ decreases on $(a,b)$. If $f(c)<\varepsilon$, then $f(x)<\varepsilon$ for all $x\in [c,b]$. $\bullet$ If $f(x_0)\ge \varepsilon$ for some $x_0$ then $f$ increases in some neighborhood of $x$. I cannot completely solve it. Thanks so much for any help.",,"['real-analysis', 'derivatives']"
19,Statement about Rolle's Theorem (true or false?),Statement about Rolle's Theorem (true or false?),,"There's a statement, that I believe is false Between two distinct zeroes of a polynomial $p$, there is a number $c$ such that $p′(c) = 0$. Here is my reasoning: A polynomial of an even degree has a derivative of an odd degree, so it has no root, in this case the theorem fails. The statement doesn't say that there's at least a number $c$. Therefore, the statement fails. Is my thinking process correct?","There's a statement, that I believe is false Between two distinct zeroes of a polynomial $p$, there is a number $c$ such that $p′(c) = 0$. Here is my reasoning: A polynomial of an even degree has a derivative of an odd degree, so it has no root, in this case the theorem fails. The statement doesn't say that there's at least a number $c$. Therefore, the statement fails. Is my thinking process correct?",,"['calculus', 'derivatives']"
20,What is the derivative of the square root of a function f(x)?,What is the derivative of the square root of a function f(x)?,,I'm trying to figure out if the $\frac{d}{dx} \sqrt{f(x)} = \frac{f'(x)}{2\sqrt{f(x)}}$ If possible can you give me the proof for the function?,I'm trying to figure out if the $\frac{d}{dx} \sqrt{f(x)} = \frac{f'(x)}{2\sqrt{f(x)}}$ If possible can you give me the proof for the function?,,"['calculus', 'derivatives']"
21,Proving inequality,Proving inequality,,"Let $f$ be a twice differentiable function and  let M, N, and P be the least upper bounds of |$f$(x)| |$f'$(x)| and |$f''$(x)| respectively prove that the square of N can never exceed 4 MP. I thought about using Taylor's theorem, but I do not know how to manage it.","Let $f$ be a twice differentiable function and  let M, N, and P be the least upper bounds of |$f$(x)| |$f'$(x)| and |$f''$(x)| respectively prove that the square of N can never exceed 4 MP. I thought about using Taylor's theorem, but I do not know how to manage it.",,"['real-analysis', 'inequality', 'derivatives']"
22,to show that $\frac{df}{dz} = \bar{(\frac{d\bar{f}}{d\bar{z}})}$,to show that,\frac{df}{dz} = \bar{(\frac{d\bar{f}}{d\bar{z}})},I need to show to show that $\frac{df}{dz} = \bar{(\frac{d\bar{f}}{d\bar{z}})}$ given that $f : \omega$ ---> $ \mathbb{C} $ and all the partial derivatives are continuous. I tried using $f=u+iv$ and then using the standard formula for differentiation by $\bar{z} $ but was not getting it.,I need to show to show that $\frac{df}{dz} = \bar{(\frac{d\bar{f}}{d\bar{z}})}$ given that $f : \omega$ ---> $ \mathbb{C} $ and all the partial derivatives are continuous. I tried using $f=u+iv$ and then using the standard formula for differentiation by $\bar{z} $ but was not getting it.,,"['complex-analysis', 'derivatives']"
23,Similar problem to Taylor's theorem proof,Similar problem to Taylor's theorem proof,,"43. Let $a_1,\dots,a_{n+1}$ be arbitrary points in $[a,b]$ and let   $$Q(x)=\prod_{i=1}^{n+1}(x-x_i).$$   Suppose that $f$ is $(n+1)$-times differentiable and that $P$ is a polynomial function of degree $\le n$ such that $P(x_i)=f(x_i)$ for $i=1,\dots,n+1$. Show that for each $x$ in $[a,b]$  there is a number $c$ in $(a,b)$ such that   $$f(x)-P(x)= Q(x) \cdot \frac{f(n+1)(c)}{(n+1)!}.$$   Hint: consider the function   $$F(t)=Q(x)[f(t)-P(t)]-Q(t)[f(x)-P(x)].$$   Show that $F$ is zero at $n+2$ different points in $[a,b]$, and use Problem 42. Problem 42 was: ""Suppose that $f$ is $n$ times differentiable and that $f(x) = 0$ for $n+1$ different $x$. Prove that $f^{(n)}(x) = 0$ for some $x$. "" I will start: $$F(t) = Q(x)[f(t) - P(t)] - Q(t)[f(x) - P(x)]$$ $$F(a) = Q(x)[f(a) - P(a)] - Q(a)[f(x) - P(x)]$$ $$Q(a)  = \prod_{k=1}^{n+1} x- x_k = \prod_{k=1}^{n+1} a- a_k = (a- a_1)(a - a_2).... (a - a_{n+1})$$ But I don't see any options? Can someone guide me? Thanks! PLEASE DO NOT GIVE A FULL ANSWER","43. Let $a_1,\dots,a_{n+1}$ be arbitrary points in $[a,b]$ and let   $$Q(x)=\prod_{i=1}^{n+1}(x-x_i).$$   Suppose that $f$ is $(n+1)$-times differentiable and that $P$ is a polynomial function of degree $\le n$ such that $P(x_i)=f(x_i)$ for $i=1,\dots,n+1$. Show that for each $x$ in $[a,b]$  there is a number $c$ in $(a,b)$ such that   $$f(x)-P(x)= Q(x) \cdot \frac{f(n+1)(c)}{(n+1)!}.$$   Hint: consider the function   $$F(t)=Q(x)[f(t)-P(t)]-Q(t)[f(x)-P(x)].$$   Show that $F$ is zero at $n+2$ different points in $[a,b]$, and use Problem 42. Problem 42 was: ""Suppose that $f$ is $n$ times differentiable and that $f(x) = 0$ for $n+1$ different $x$. Prove that $f^{(n)}(x) = 0$ for some $x$. "" I will start: $$F(t) = Q(x)[f(t) - P(t)] - Q(t)[f(x) - P(x)]$$ $$F(a) = Q(x)[f(a) - P(a)] - Q(a)[f(x) - P(x)]$$ $$Q(a)  = \prod_{k=1}^{n+1} x- x_k = \prod_{k=1}^{n+1} a- a_k = (a- a_1)(a - a_2).... (a - a_{n+1})$$ But I don't see any options? Can someone guide me? Thanks! PLEASE DO NOT GIVE A FULL ANSWER",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
24,derivative of $\sec^2(x/12)$,derivative of,\sec^2(x/12),"Alright, so the derivative of $\sec^2(x/12)$ is $\frac{1}{6} \tan\left(\frac{x}{12}\right) \sec^2\left(\frac{x}{12}\right)$ But if you use chain rule, you get: $$2  \sec\left(\frac{x}{12}\right)  \left(\sec\left(\frac{x}{12}\right)\right)'$$ $$2  \sec\left(\frac{x}{12}\right)  \sec\left(\frac{x}{12}\right)  \tan\left(\frac{x}{12}\right)$$ So why do they multiply by the derivative of $\frac{x}{12}$ as well? I don't get it, what formula do they use then?","Alright, so the derivative of $\sec^2(x/12)$ is $\frac{1}{6} \tan\left(\frac{x}{12}\right) \sec^2\left(\frac{x}{12}\right)$ But if you use chain rule, you get: $$2  \sec\left(\frac{x}{12}\right)  \left(\sec\left(\frac{x}{12}\right)\right)'$$ $$2  \sec\left(\frac{x}{12}\right)  \sec\left(\frac{x}{12}\right)  \tan\left(\frac{x}{12}\right)$$ So why do they multiply by the derivative of $\frac{x}{12}$ as well? I don't get it, what formula do they use then?",,"['calculus', 'derivatives']"
25,Divergence (or second derivative) of circle,Divergence (or second derivative) of circle,,"The circle has the uniform shape because a second derivative is 1. That is an intuitive guess - the line turns around at constant rate (i.e. the first derivative changes at constant rate), which means that it is not dependent on x and y coordinates. If the rate of the turn would increase, one would get inward spiral, etc. The shape of the circle is uniform so the step of the turn in x variable and in y variable is the same. How to support this mathematically? Should a divergence be computed? I tried simply computing derivates however the circle is not a function, it is an equation and it is quickly very confusing.","The circle has the uniform shape because a second derivative is 1. That is an intuitive guess - the line turns around at constant rate (i.e. the first derivative changes at constant rate), which means that it is not dependent on x and y coordinates. If the rate of the turn would increase, one would get inward spiral, etc. The shape of the circle is uniform so the step of the turn in x variable and in y variable is the same. How to support this mathematically? Should a divergence be computed? I tried simply computing derivates however the circle is not a function, it is an equation and it is quickly very confusing.",,"['derivatives', 'circles', 'partial-derivative']"
26,"Proof: If $ f $ is differentiable on $ [a,b] $ then $ f' $ cannot have any simple discontinuities on $[a,b] $",Proof: If  is differentiable on  then  cannot have any simple discontinuities on," f   [a,b]   f'  [a,b] ","This is a corollary following Rudin's theorem 5.12, which states: Suppose $ f $ is a real differentiable function on $ [a,b]$ and suppose that $f'(a)<\lambda<f'(b).$ Then there is a point $x \in (a,b) $ such that $ f'(x)=\lambda$. The corollary says if $ f $ is differentiable on $ [a,b] $ then $ f' $ cannot have any simple discontinuities on $[a,b] $. Intuitively this makes sense but I can't come up with a neat proof. I was thinking to do it by contradiction; to suppose $ f' $ has a simple discontinuity. This means the lefthand and righthand limits at $ x\in (a,b) $ exist but there are two cases;either $ f(x+)$ does not equal $ f(x)$ or $f(x-) $ does not equal $f(x) $. Then I'm not sure where to go from here/how to apply the theorem above.","This is a corollary following Rudin's theorem 5.12, which states: Suppose $ f $ is a real differentiable function on $ [a,b]$ and suppose that $f'(a)<\lambda<f'(b).$ Then there is a point $x \in (a,b) $ such that $ f'(x)=\lambda$. The corollary says if $ f $ is differentiable on $ [a,b] $ then $ f' $ cannot have any simple discontinuities on $[a,b] $. Intuitively this makes sense but I can't come up with a neat proof. I was thinking to do it by contradiction; to suppose $ f' $ has a simple discontinuity. This means the lefthand and righthand limits at $ x\in (a,b) $ exist but there are two cases;either $ f(x+)$ does not equal $ f(x)$ or $f(x-) $ does not equal $f(x) $. Then I'm not sure where to go from here/how to apply the theorem above.",,"['analysis', 'derivatives', 'continuity']"
27,"How to calculate derivative of a multi-variable function, if variables are dependent of each other?","How to calculate derivative of a multi-variable function, if variables are dependent of each other?",,"For a multiple variable function, such as $f(x, y)$, if $y$ is actually dependent on $x$, then I think there are two ways to calculate $df$: replace $y$ by $x$ in $f(x,y)$ and then treat the result function as a single variable function $g(x)$, then calculate $dg$ calculate $df(x,y)$ using partial derivatives and then substitute. However, I have a problem with the second method, and cannot reconcile the two. As an example, suppose we have a function $$f(x, y) = x + 2y + 3xy$$ but $x$ and $y$ are related via $$y = 1 - x$$ Then the first method will give me the following $$g(x) = f(x, 1-x) = x + 2(1-x) + 3x(1-x) = -3x^2 + 2x + 2$$ so $$dg(x) = (-6x + 2)dx$$ Using the second method, I have the following. calculate $df(x,y)$ as partial derivatives of $x$ and $y$ $$df(x, y) = f_x(x, y)dx + f_y(x,y)dy + f_{x,y}(x,y) dx dy $$ $$df(x, y) = (1+3y)dx + (2+3x)dy + 3 dx dy $$ replace $y$ by $1-x$ and $dy$ by $(dy/dx)dx = -dx$ throughout $$df(x) = (1+3(1-x))dx + (2+3x)(-dx) + 3 dx(-dx) $$ $$df(x) = (-6x + 2)dx - 3 (dx)^2 $$ As you can see, there is an extra $-3(dx)^2$ term using the partial derivative method. What went wrong? I also have two questions is it legitimate to replace $dy$ by $(dy/dx)dx$ in step 2? Shouldn't term $f_ydy$ mean keeping $x$ unchanged? Then how can we replace $dy$ by $(dy/dx)dx$ as it means $x$ changes? when I substitute $y$ by $1-x$ and $dy$ by $(dy/dx)dx$, what does $df(x,y)$ become? do we write it as another function $dg(x)$ or $df(x, 1-x)$? I'm really confused. Thank you!","For a multiple variable function, such as $f(x, y)$, if $y$ is actually dependent on $x$, then I think there are two ways to calculate $df$: replace $y$ by $x$ in $f(x,y)$ and then treat the result function as a single variable function $g(x)$, then calculate $dg$ calculate $df(x,y)$ using partial derivatives and then substitute. However, I have a problem with the second method, and cannot reconcile the two. As an example, suppose we have a function $$f(x, y) = x + 2y + 3xy$$ but $x$ and $y$ are related via $$y = 1 - x$$ Then the first method will give me the following $$g(x) = f(x, 1-x) = x + 2(1-x) + 3x(1-x) = -3x^2 + 2x + 2$$ so $$dg(x) = (-6x + 2)dx$$ Using the second method, I have the following. calculate $df(x,y)$ as partial derivatives of $x$ and $y$ $$df(x, y) = f_x(x, y)dx + f_y(x,y)dy + f_{x,y}(x,y) dx dy $$ $$df(x, y) = (1+3y)dx + (2+3x)dy + 3 dx dy $$ replace $y$ by $1-x$ and $dy$ by $(dy/dx)dx = -dx$ throughout $$df(x) = (1+3(1-x))dx + (2+3x)(-dx) + 3 dx(-dx) $$ $$df(x) = (-6x + 2)dx - 3 (dx)^2 $$ As you can see, there is an extra $-3(dx)^2$ term using the partial derivative method. What went wrong? I also have two questions is it legitimate to replace $dy$ by $(dy/dx)dx$ in step 2? Shouldn't term $f_ydy$ mean keeping $x$ unchanged? Then how can we replace $dy$ by $(dy/dx)dx$ as it means $x$ changes? when I substitute $y$ by $1-x$ and $dy$ by $(dy/dx)dx$, what does $df(x,y)$ become? do we write it as another function $dg(x)$ or $df(x, 1-x)$? I'm really confused. Thank you!",,"['derivatives', 'partial-derivative']"
28,Chain rule proof doubt,Chain rule proof doubt,,"I was reading this pdf document that shows a proof of the chain rule. My doubt is in the second slide I dont understand why the $k$ value is equal to $g'(x)$ plus $v$ all of this plus $h$ . Sorry I dont write the equations, I dont know how to do it thats why I added the link to the pdf","I was reading this pdf document that shows a proof of the chain rule. My doubt is in the second slide I dont understand why the value is equal to plus all of this plus . Sorry I dont write the equations, I dont know how to do it thats why I added the link to the pdf",k g'(x) v h,"['calculus', 'derivatives']"
29,Is there a better way of writing differentiation and integration?,Is there a better way of writing differentiation and integration?,,"Differentiation is commonly written simply with a prime mark and an equation, as $(x^2)' = 2x$. Although I find this confusing and think it'd better be written $D(x\mapsto x^2) = x\mapsto 2x$, as $x^2$ itself isn't a function. But this notation doesn't allow for specific values, so I was wondering if there was a common way of writing it? Would this be acceptable: $D(x\mapsto x^2)|_{x=a} = 2a$? And how would one write integration in similar terms? $I(x\mapsto x^2) = x\mapsto \frac{1}{3}x^3+C$ ? And how would one write definite integration? $I(x\mapsto x^2)_{x_1=a}^{x_2=b} = \frac{1}{3}b^3-\frac{1}{3}a^3$? Edit: What I'd like to know is if there is a more rigorous notation for differentiation and integration that doesn't hide the fact that you're dealing with functions and not equations. Simple and shortened notation is useful when doing calculations, but not for getting an understanding of what you're doing. I want a better understanding. Thanks in advance","Differentiation is commonly written simply with a prime mark and an equation, as $(x^2)' = 2x$. Although I find this confusing and think it'd better be written $D(x\mapsto x^2) = x\mapsto 2x$, as $x^2$ itself isn't a function. But this notation doesn't allow for specific values, so I was wondering if there was a common way of writing it? Would this be acceptable: $D(x\mapsto x^2)|_{x=a} = 2a$? And how would one write integration in similar terms? $I(x\mapsto x^2) = x\mapsto \frac{1}{3}x^3+C$ ? And how would one write definite integration? $I(x\mapsto x^2)_{x_1=a}^{x_2=b} = \frac{1}{3}b^3-\frac{1}{3}a^3$? Edit: What I'd like to know is if there is a more rigorous notation for differentiation and integration that doesn't hide the fact that you're dealing with functions and not equations. Simple and shortened notation is useful when doing calculations, but not for getting an understanding of what you're doing. I want a better understanding. Thanks in advance",,"['integration', 'derivatives', 'notation']"
30,Proof of $f'(x)+\alpha f(x)=0.$,Proof of,f'(x)+\alpha f(x)=0.,"Let $f\in C[a,b]$ be differentiable in $(a,b)$. If $f(a)=f(b)=0$ then prove that for any real number $\alpha$ $\exists$ $x\in (a,b)$ such that, $$f'(x)+\alpha f(x)=0.$$ Clearly,this is a problem of Roll's theorem. First consider the function $\phi (t)$  such that $\phi$ is continuous on $[a,b]$  & differentiable on $(a,b)$ & $\phi (a)=\phi (b)$ & such that $\phi'(t)=f'(t)+\alpha f(t)$. But I can't construct such a function. Please help on construction such a function $\phi$.","Let $f\in C[a,b]$ be differentiable in $(a,b)$. If $f(a)=f(b)=0$ then prove that for any real number $\alpha$ $\exists$ $x\in (a,b)$ such that, $$f'(x)+\alpha f(x)=0.$$ Clearly,this is a problem of Roll's theorem. First consider the function $\phi (t)$  such that $\phi$ is continuous on $[a,b]$  & differentiable on $(a,b)$ & $\phi (a)=\phi (b)$ & such that $\phi'(t)=f'(t)+\alpha f(t)$. But I can't construct such a function. Please help on construction such a function $\phi$.",,"['real-analysis', 'derivatives', 'continuity']"
31,Derivative of integral with infinity as upper bound,Derivative of integral with infinity as upper bound,,What is the solution to the derivative of following integral? I know how to take derivatives of integrals but I never came across one with infinity in one of his bounds. $F(t) = \int^{\infty}_t \frac{x-4}{(x^2+4)(x+1)}$ $t >= 0$,What is the solution to the derivative of following integral? I know how to take derivatives of integrals but I never came across one with infinity in one of his bounds. $F(t) = \int^{\infty}_t \frac{x-4}{(x^2+4)(x+1)}$ $t >= 0$,,"['derivatives', 'improper-integrals']"
32,Finding Derivatives $f(x)={1\over x+1}$,Finding Derivatives,f(x)={1\over x+1},"I'm using the Limit Definition to find the derivative, $$f'(x)=\lim_{\Delta x \to 0} {f(x+\Delta x) - f(x) \over \Delta x}$$ $$$$ Now, I want to find the derivative for the function, $$f(x)={1 \over x+1}$$ So, here's what I did. $$\lim_{\Delta x \to 0} {{1 \over (x+\Delta x) +1} - {1 \over x+1}\over \Delta x}$$ Now, I think I can multiply the numerator and the denominator by the least common multiple to get rid of the denominator in the numerator?? I'm not sure what to do from here. Thanks","I'm using the Limit Definition to find the derivative, $$f'(x)=\lim_{\Delta x \to 0} {f(x+\Delta x) - f(x) \over \Delta x}$$ $$$$ Now, I want to find the derivative for the function, $$f(x)={1 \over x+1}$$ So, here's what I did. $$\lim_{\Delta x \to 0} {{1 \over (x+\Delta x) +1} - {1 \over x+1}\over \Delta x}$$ Now, I think I can multiply the numerator and the denominator by the least common multiple to get rid of the denominator in the numerator?? I'm not sure what to do from here. Thanks",,"['calculus', 'limits', 'derivatives']"
33,How to find the derivative of $F(x)=\int_{x^2}^{4x^2} \sin \sqrt t\;\;dt$?,How to find the derivative of ?,F(x)=\int_{x^2}^{4x^2} \sin \sqrt t\;\;dt,"For a real number $t>0$, let $\sqrt t$ denote the positive square root of t.  For a real number $x>0$, let $F(x)=\int_{x^2}^{4x^2} \sin \sqrt t\;\;dt$.  If $F'$ is the derivative of $F$, then (a). $F'(\frac \pi 2)=0$ (b). $F'(\frac \pi 2)=\pi$ (c). $F'(\frac \pi 2)=-\pi$ (d). $F'(\frac \pi 2)=2\pi$ I am guessing that I could use Fundamental theorem of integral calculus. But I couldn't construct a function whose derivative is $\sin \sqrt t$. If I through I could find the answer.","For a real number $t>0$, let $\sqrt t$ denote the positive square root of t.  For a real number $x>0$, let $F(x)=\int_{x^2}^{4x^2} \sin \sqrt t\;\;dt$.  If $F'$ is the derivative of $F$, then (a). $F'(\frac \pi 2)=0$ (b). $F'(\frac \pi 2)=\pi$ (c). $F'(\frac \pi 2)=-\pi$ (d). $F'(\frac \pi 2)=2\pi$ I am guessing that I could use Fundamental theorem of integral calculus. But I couldn't construct a function whose derivative is $\sin \sqrt t$. If I through I could find the answer.",,"['calculus', 'real-analysis', 'integration', 'derivatives']"
34,A limit question involving power of positive numbers,A limit question involving power of positive numbers,,"I'm trying compute the following limit: $$\lim_{t\to0}\left(\frac{1}{t+1}\cdot\frac{b^{t+1}-a^{t+1}}{b-a}\right)^{1/t},\quad b>a>0.$$ I know $\displaystyle\lim_{t\to0}(1+t)^{1/t}=e$ and here I've found $\displaystyle\lim_{t\to0}\left(\frac{b^{t+1}-a^{t+1}}{b-a}\right)^{1/t}=\frac{a^{\frac{a}{a-b}}}{b^{\frac{b}{a-b}}}$. Hence the answer is $\displaystyle\frac{1}{e}\cdot\frac{a^{\frac{a}{a-b}}}{b^{\frac{b}{a-b}}}$. How can I get that answer? A classmate of mine had a good idea, but we are stuck. The idea is take $f(x)=x^{t+1}$, by the mean value theorem exists $c\in(a,b)$ such that $$f'(c)=\frac{f(b)-f(a)}{b-a}.$$ But $$\frac{f(b)-f(a)}{b-a}=\frac{b^{t+1}-a^{t+1}}{b-a}\quad\mbox{and}\quad f'(c)=(t+1)c^t.$$ Hence, since $c>0$ $$\left(\frac{1}{t+1}\cdot\frac{b^{t+1}-a^{t+1}}{b-a}\right)^{1/t}=(c^t)^{1/t}=c.$$ Thus $$\lim_{t\to0}\left(\frac{1}{t+1}\cdot\frac{b^{t+1}-a^{t+1}}{b-a}\right)^{1/t}=\lim_{t\to0}c=c.$$ We are stuck here. Someone has some hint for this? Or some other idea for compute the limit above? Thanks!","I'm trying compute the following limit: $$\lim_{t\to0}\left(\frac{1}{t+1}\cdot\frac{b^{t+1}-a^{t+1}}{b-a}\right)^{1/t},\quad b>a>0.$$ I know $\displaystyle\lim_{t\to0}(1+t)^{1/t}=e$ and here I've found $\displaystyle\lim_{t\to0}\left(\frac{b^{t+1}-a^{t+1}}{b-a}\right)^{1/t}=\frac{a^{\frac{a}{a-b}}}{b^{\frac{b}{a-b}}}$. Hence the answer is $\displaystyle\frac{1}{e}\cdot\frac{a^{\frac{a}{a-b}}}{b^{\frac{b}{a-b}}}$. How can I get that answer? A classmate of mine had a good idea, but we are stuck. The idea is take $f(x)=x^{t+1}$, by the mean value theorem exists $c\in(a,b)$ such that $$f'(c)=\frac{f(b)-f(a)}{b-a}.$$ But $$\frac{f(b)-f(a)}{b-a}=\frac{b^{t+1}-a^{t+1}}{b-a}\quad\mbox{and}\quad f'(c)=(t+1)c^t.$$ Hence, since $c>0$ $$\left(\frac{1}{t+1}\cdot\frac{b^{t+1}-a^{t+1}}{b-a}\right)^{1/t}=(c^t)^{1/t}=c.$$ Thus $$\lim_{t\to0}\left(\frac{1}{t+1}\cdot\frac{b^{t+1}-a^{t+1}}{b-a}\right)^{1/t}=\lim_{t\to0}c=c.$$ We are stuck here. Someone has some hint for this? Or some other idea for compute the limit above? Thanks!",,"['limits', 'derivatives', 'exponentiation']"
35,"Prove that $ y''(0) = -1, x = \cos\left(\frac{t}{1+t}\right), y = \sin\left(\frac{t}{1+t}\right)$",Prove that," y''(0) = -1, x = \cos\left(\frac{t}{1+t}\right), y = \sin\left(\frac{t}{1+t}\right)","My attempt: $$x'= -\sin\left(\frac{t}{1+t}\right) * \frac{1}{(1+t)^2}$$ $$y'= \cos\left(\frac{t}{1+t}\right) * \frac{1}{(1+t)^2}$$ then I attempted to divide y' on x' which resulted in $$-\cot\left(\frac{t}{1+t}\right)$$ When I try to take the double dash of this it gives me $$\csc^2\left(\frac{t}{1+t}\right),$$ but this function is undefined at 0.","My attempt: $$x'= -\sin\left(\frac{t}{1+t}\right) * \frac{1}{(1+t)^2}$$ $$y'= \cos\left(\frac{t}{1+t}\right) * \frac{1}{(1+t)^2}$$ then I attempted to divide y' on x' which resulted in $$-\cot\left(\frac{t}{1+t}\right)$$ When I try to take the double dash of this it gives me $$\csc^2\left(\frac{t}{1+t}\right),$$ but this function is undefined at 0.",,"['calculus', 'derivatives']"
36,"If $||\mathbf{J}_f(x) - I_n|| < \frac{1}{2n}$ for all $x \in \mathbb{R^n}$, then $f$ is a diffeomorphism","If  for all , then  is a diffeomorphism",||\mathbf{J}_f(x) - I_n|| < \frac{1}{2n} x \in \mathbb{R^n} f,"Suppose that $f: \mathbb{R^n} \rightarrow \mathbb{R^n}$ is a differentiable function such that $||\mathbf{J}_f(x) - I_n|| < \frac{1}{2n}$ for all $x \in \mathbb{R^n}$. (Note that $\mathbf{J}_f$ is the Jacobian matrix, $I_n$ is the identity matrix and $||M|| = \big( \sum m_{ij}^2 \big)^{1/2}$). Then I want to prove that $f$ is a diffeomorphism. I guess I should apply Implicit function theorem but I do not know how to bring the condition $||\mathbf{J}_f(x) - I_n|| < \frac{1}{2n}$ into play.","Suppose that $f: \mathbb{R^n} \rightarrow \mathbb{R^n}$ is a differentiable function such that $||\mathbf{J}_f(x) - I_n|| < \frac{1}{2n}$ for all $x \in \mathbb{R^n}$. (Note that $\mathbf{J}_f$ is the Jacobian matrix, $I_n$ is the identity matrix and $||M|| = \big( \sum m_{ij}^2 \big)^{1/2}$). Then I want to prove that $f$ is a diffeomorphism. I guess I should apply Implicit function theorem but I do not know how to bring the condition $||\mathbf{J}_f(x) - I_n|| < \frac{1}{2n}$ into play.",,"['real-analysis', 'derivatives']"
37,Analysis-Baby Rudin's differentiability and continuity: theorem 5.2 and 5.6,Analysis-Baby Rudin's differentiability and continuity: theorem 5.2 and 5.6,,"I am very confused about differentiability and continuity. At the beginning of the differentiation chapter, we proved that differentiability contains continuity. (Theorem 5.2) But in example 5.6 and the next section of the chapter implies that there is a function that is differentiable at some points or interval but the function is not continuous.  I guess I am missing something or misunderstanding definition of continuity and differentiability. Please help how I can understand the basic definition and ideas of continuity and differentiability.  Thanks in advance","I am very confused about differentiability and continuity. At the beginning of the differentiation chapter, we proved that differentiability contains continuity. (Theorem 5.2) But in example 5.6 and the next section of the chapter implies that there is a function that is differentiable at some points or interval but the function is not continuous.  I guess I am missing something or misunderstanding definition of continuity and differentiability. Please help how I can understand the basic definition and ideas of continuity and differentiability.  Thanks in advance",,"['real-analysis', 'derivatives', 'continuity']"
38,Evaluating $(\frac{\cos x}{1-\sin x})^2$,Evaluating,(\frac{\cos x}{1-\sin x})^2,$(\dfrac{\cos x}{1-\sin x})^2$ $f\;'(x)= 2(\dfrac{\cos x}{1-\sin x}) \times (\dfrac{-\sin x+\sin^2x-\cos^2x}{(1-\sin x)^2})$ Does $\sin^2x-\cos^2=1$? or $-1$?  Then it could factor with the bottom and the answer would be $\dfrac{2\cos x}{(1-\sin x)^2}$. Is this right?,$(\dfrac{\cos x}{1-\sin x})^2$ $f\;'(x)= 2(\dfrac{\cos x}{1-\sin x}) \times (\dfrac{-\sin x+\sin^2x-\cos^2x}{(1-\sin x)^2})$ Does $\sin^2x-\cos^2=1$? or $-1$?  Then it could factor with the bottom and the answer would be $\dfrac{2\cos x}{(1-\sin x)^2}$. Is this right?,,"['calculus', 'derivatives']"
39,Is $x=0$ an inflection point?,Is  an inflection point?,x=0,"Consider $f(x)=x^{\frac {5}{7}}$, is it $x=0$ an inflection point? $$f'(x)=\frac {5}{7}x^{\frac {-2}{7}}$$ $$f''(x)=\frac {-10}{49}x^{\frac {-9}{7}}$$ As far as I know, the inflection point is the point in which $f''(x)=0$ or $f''(x)$ does not exist and $f''(x)$ also changes sign at that point. However, here $f'(0)=+\infty $ from both sides, so it should have verical tangent at point $x=0$. My calculation shows that $f_+''(0)=-\infty$ and $f_\_''(0)=-\infty$ so the $f''(x)$ changes sign around the point $x=0$. I'm not sure whether I'm correct, please tell me the safe steps to find the inflection point.","Consider $f(x)=x^{\frac {5}{7}}$, is it $x=0$ an inflection point? $$f'(x)=\frac {5}{7}x^{\frac {-2}{7}}$$ $$f''(x)=\frac {-10}{49}x^{\frac {-9}{7}}$$ As far as I know, the inflection point is the point in which $f''(x)=0$ or $f''(x)$ does not exist and $f''(x)$ also changes sign at that point. However, here $f'(0)=+\infty $ from both sides, so it should have verical tangent at point $x=0$. My calculation shows that $f_+''(0)=-\infty$ and $f_\_''(0)=-\infty$ so the $f''(x)$ changes sign around the point $x=0$. I'm not sure whether I'm correct, please tell me the safe steps to find the inflection point.",,"['calculus', 'derivatives']"
40,Calculus - Implicit Differentiation,Calculus - Implicit Differentiation,,I'm reading math notes online here . In the notes there is a problem that differentiates the following equation: $$    \sec(A) = \frac x {50};$$ ...where the angle $A$ is a function of time (ie. $A = A(t)$) The answer for which is: $$    \sec(A)\tan(A) A' = \frac{x'}{50};$$ I understand the differentiation of the left side of the equation but I don't understand why the derivative of the right side equates to x' / 50...why isn't the right side differentiated like so: $$\frac1{50}  \frac{\mathrm d}{\mathrm dx}(x) = \frac1{50}\cdot1 = \frac1{50}$$,I'm reading math notes online here . In the notes there is a problem that differentiates the following equation: $$    \sec(A) = \frac x {50};$$ ...where the angle $A$ is a function of time (ie. $A = A(t)$) The answer for which is: $$    \sec(A)\tan(A) A' = \frac{x'}{50};$$ I understand the differentiation of the left side of the equation but I don't understand why the derivative of the right side equates to x' / 50...why isn't the right side differentiated like so: $$\frac1{50}  \frac{\mathrm d}{\mathrm dx}(x) = \frac1{50}\cdot1 = \frac1{50}$$,,"['calculus', 'derivatives', 'implicit-differentiation']"
41,Can anyone explain how to differentiate the Lambert W function?,Can anyone explain how to differentiate the Lambert W function?,,I'm interested in the differentiation of the Lambert W function $y = xe^x$. I am unable to understand how to proceed for it.,I'm interested in the differentiation of the Lambert W function $y = xe^x$. I am unable to understand how to proceed for it.,,"['derivatives', 'lambert-w']"
42,Definite integral-dot product,Definite integral-dot product,,"I have an integral equation containing dot product $$\int_{0}^{L} \left(\frac{a}{L}.b(s)\right)\mathrm ds\tag 1$$ Data Given a is a constant vector of size 3 b(s) is a varying vector of size 3 "" . ""  means dot product Question Can we write $$\int_{0}^{L} \left(\frac{a}{L}.b(s)\right)\mathrm ds =  a . \int_{0}^{L} b(s)\mathrm ds\tag 2$$ Is it right to write like this? I am not familiar with calculus on dot product","I have an integral equation containing dot product $$\int_{0}^{L} \left(\frac{a}{L}.b(s)\right)\mathrm ds\tag 1$$ Data Given a is a constant vector of size 3 b(s) is a varying vector of size 3 "" . ""  means dot product Question Can we write $$\int_{0}^{L} \left(\frac{a}{L}.b(s)\right)\mathrm ds =  a . \int_{0}^{L} b(s)\mathrm ds\tag 2$$ Is it right to write like this? I am not familiar with calculus on dot product",,"['calculus', 'derivatives', 'vector-spaces', 'definite-integrals', 'vector-analysis']"
43,what does the second derivative of a linear function mean?,what does the second derivative of a linear function mean?,,"So if I have a function f(x) = 7x-2 the first derivative is 7 which I'm inclined to think that the second derivative exists because 7 = 0x+7 and the second derivative is 0 makes sense, I guess, because the slope never ever changes. But how am I supposed to put that into context with the rest of the info available? There is no concaving up or down in either direction (though I can imagine it being a really straight curve) and if all I knew was the first and second derivatives, I might just think I'm at an inflection point and that there's a horizon somewhere else.  This might be a moot point since I'm likely always gonna have the original function at hand in the real world but I was wondering if anyone else had any thoughts on the derivatives of a straight line.  Maybe the second derivative doesn't exist?","So if I have a function f(x) = 7x-2 the first derivative is 7 which I'm inclined to think that the second derivative exists because 7 = 0x+7 and the second derivative is 0 makes sense, I guess, because the slope never ever changes. But how am I supposed to put that into context with the rest of the info available? There is no concaving up or down in either direction (though I can imagine it being a really straight curve) and if all I knew was the first and second derivatives, I might just think I'm at an inflection point and that there's a horizon somewhere else.  This might be a moot point since I'm likely always gonna have the original function at hand in the real world but I was wondering if anyone else had any thoughts on the derivatives of a straight line.  Maybe the second derivative doesn't exist?",,"['calculus', 'linear-algebra', 'derivatives']"
44,"Strict local extremum without $f'$ ""changing signs""","Strict local extremum without  ""changing signs""",f',"Let $f:\mathbb{R}\to \mathbb{R}$. Is it possible that $f$ has the following properties: $f$ is differentiable in a neighborhood of $a\in \mathbb{R}$ $a$ is a strict local minimum There is no neighborhood $(a-\delta, a+\delta)$ of $a$ such that $f'<0$ in $(a-\delta, a)$ and $f'>0$ in $(a,a+\delta)$? It is definitely possible if we require only that $a$ be a (non-strict) local minimum; the function $x^2\sin^2(1/x)$ exhibits such behavior at zero. It is definitely impossible if $f$ is a polynomial, since we can write $f(x) = (x-a)^mg(x) + f(a)$, where $g(a)\neq 0$, and $f(x)-f(a)$ behaves locally like $(x-a)^m$. (This also shows that if $f$ is a polynomial, $f$ has a local extremum at $a$ iff $f'$ has a zero of odd order at $a$.) I believe it is impossible if $f'$ is continuous at $a$, since then I claim that $f$ has a local extremum iff $f'$ has a neighborhood as in property (3). If you're interested, the way I came up with this question is: I was showing that if $f=x^n + a_{n-1}x^{n-1} + \dotsb + a_1x + a_0$ has number of local maxima equal to $k_1$, and number of local minima equal to $k_2$, then $k_1=k_2$ if $n$ is odd, and $k_1 + 1=k_2$ if $n$ is even. I wondered if a similar property held for non-polynomial functions if we require that they be strict local maxima and minima.","Let $f:\mathbb{R}\to \mathbb{R}$. Is it possible that $f$ has the following properties: $f$ is differentiable in a neighborhood of $a\in \mathbb{R}$ $a$ is a strict local minimum There is no neighborhood $(a-\delta, a+\delta)$ of $a$ such that $f'<0$ in $(a-\delta, a)$ and $f'>0$ in $(a,a+\delta)$? It is definitely possible if we require only that $a$ be a (non-strict) local minimum; the function $x^2\sin^2(1/x)$ exhibits such behavior at zero. It is definitely impossible if $f$ is a polynomial, since we can write $f(x) = (x-a)^mg(x) + f(a)$, where $g(a)\neq 0$, and $f(x)-f(a)$ behaves locally like $(x-a)^m$. (This also shows that if $f$ is a polynomial, $f$ has a local extremum at $a$ iff $f'$ has a zero of odd order at $a$.) I believe it is impossible if $f'$ is continuous at $a$, since then I claim that $f$ has a local extremum iff $f'$ has a neighborhood as in property (3). If you're interested, the way I came up with this question is: I was showing that if $f=x^n + a_{n-1}x^{n-1} + \dotsb + a_1x + a_0$ has number of local maxima equal to $k_1$, and number of local minima equal to $k_2$, then $k_1=k_2$ if $n$ is odd, and $k_1 + 1=k_2$ if $n$ is even. I wondered if a similar property held for non-polynomial functions if we require that they be strict local maxima and minima.",,"['calculus', 'polynomials', 'derivatives']"
45,A function is real-differentiable iff it has a complex-differentiable extension,A function is real-differentiable iff it has a complex-differentiable extension,,"Is this conjecture true? A function $f:\Bbb R\to\Bbb R$ is real differentiable at $a$ if and only if there exists a complex-differentiable function $g:A\to\Bbb C$ for some neighborhood of $a\in A\subseteq \Bbb C$ such that $f\Big|_{A\cap \Bbb R}=g\Big|_{\Bbb R\cap A}$. The reverse implication is trivial; a complex differentiable function $g$ is real differentiable on $A$, and since differentiability is local and $f$ and $g$ agree on $A$, $f$ is real differentiable as well. But the forward implication is probably too strong to be true, although I can't think of any counterexamples. Is it true if $g$ is only required to be complex differentiable at $a$? If we only require $f-g\in o(x-a)$, then the statement can be satisfied by letting $g(z)=f(a)+f'(a)(z-a)$. I'm interested in finding a characterization of real differentiability in terms of complex differentiability; other suggestions for definitional theorems of this sort are also welcome.","Is this conjecture true? A function $f:\Bbb R\to\Bbb R$ is real differentiable at $a$ if and only if there exists a complex-differentiable function $g:A\to\Bbb C$ for some neighborhood of $a\in A\subseteq \Bbb C$ such that $f\Big|_{A\cap \Bbb R}=g\Big|_{\Bbb R\cap A}$. The reverse implication is trivial; a complex differentiable function $g$ is real differentiable on $A$, and since differentiability is local and $f$ and $g$ agree on $A$, $f$ is real differentiable as well. But the forward implication is probably too strong to be true, although I can't think of any counterexamples. Is it true if $g$ is only required to be complex differentiable at $a$? If we only require $f-g\in o(x-a)$, then the statement can be satisfied by letting $g(z)=f(a)+f'(a)(z-a)$. I'm interested in finding a characterization of real differentiability in terms of complex differentiability; other suggestions for definitional theorems of this sort are also welcome.",,"['real-analysis', 'complex-analysis', 'derivatives']"
46,How to evaluate $\lim_{x \to \infty}\left(1 + \frac{2}{x}\right)^{3x}$ using L'Hôpital's rule?,How to evaluate  using L'Hôpital's rule?,\lim_{x \to \infty}\left(1 + \frac{2}{x}\right)^{3x},I'm stuck on how to evaluate the following using L'Hôpital's rule: $$\lim_{x \to \infty}\left(1 + \frac{2}{x}\right)^{3x}$$ This is a problem that I encountered on Khan Academy and I attempted to understand it using the resources there.  Here are the tips given for the problem; the portion that I'm having trouble understanding is highlighted: I also attempted to use this video (screenshot following) to help; I understand the concepts in the video but it seems like there are some missing steps in the tips above. I also attempted to use WolframAlpha's step-by-step solution but it was indecipherable to me. Any help is greatly appreciated.,I'm stuck on how to evaluate the following using L'Hôpital's rule: $$\lim_{x \to \infty}\left(1 + \frac{2}{x}\right)^{3x}$$ This is a problem that I encountered on Khan Academy and I attempted to understand it using the resources there.  Here are the tips given for the problem; the portion that I'm having trouble understanding is highlighted: I also attempted to use this video (screenshot following) to help; I understand the concepts in the video but it seems like there are some missing steps in the tips above. I also attempted to use WolframAlpha's step-by-step solution but it was indecipherable to me. Any help is greatly appreciated.,,"['limits', 'derivatives']"
47,I need help on the process of solving this derivative.,I need help on the process of solving this derivative.,,"How do I go about solving this derivative. $$f(x)=\ln\left(\frac{7x}{x+4}\right)$$ I go from this to $$1. \quad f(x)=\ln(7)+\ln(x)-\ln(x+4)$$ and then $$2. \quad f'(x)=\frac{1}{x}-\frac{1}{x+4}$$ then $$3. \quad f'(x)=\frac{x+4-x}{x(x+4)}$$ and end up with $$4. \quad f'(x)=\frac{4}{x(x+4)}$$ which is the correct answer. This procedure is what we did in class, but I do not get how I went from step 2 to 3 which is what has me confused. A step by step explanation would be greatly appreciated. Thanks!","How do I go about solving this derivative. $$f(x)=\ln\left(\frac{7x}{x+4}\right)$$ I go from this to $$1. \quad f(x)=\ln(7)+\ln(x)-\ln(x+4)$$ and then $$2. \quad f'(x)=\frac{1}{x}-\frac{1}{x+4}$$ then $$3. \quad f'(x)=\frac{x+4-x}{x(x+4)}$$ and end up with $$4. \quad f'(x)=\frac{4}{x(x+4)}$$ which is the correct answer. This procedure is what we did in class, but I do not get how I went from step 2 to 3 which is what has me confused. A step by step explanation would be greatly appreciated. Thanks!",,"['calculus', 'derivatives', 'logarithms', 'natural-numbers']"
48,L'Hôpital's Rule and Infinite Limits,L'Hôpital's Rule and Infinite Limits,,"I was wondering if anyone could help me with computing a limit using L'Hôpital's Rule. Using L'Hôpital Rule for the following limit, I get the following result: \begin{equation} \lim_{x \to 0} \frac{e^x-1-x^2}{x^4+x^3+x^2} \therefore \lim_{x \to 0} \frac{e^x-1-x^2}{x^4+x^3+x^2} \stackrel{L'H}{=} \lim_{x \to 0} \frac{e^x-2x}{4x^3+3x^2+2x} \end{equation} (1/0) which means that the limit is either $+\infty$ , $-\infty$ or it does not exist. If I want to find out the answer, I should take the side limits - but here lies my question: the side mimits of what? The original function or the function after L'Hôpital rule? Or any of the two? Should I do this \begin{equation} \lim_{x \to 0^-} \frac{e^x-2x}{4x^3+3x^2+2x} = -\infty \end{equation} \begin{equation} \lim_{x \to 0^+} \frac{e^x-2x}{4x^3+3x^2+2x} = +\infty \end{equation} Or this? \begin{equation} \lim_{x \to 0^-} \frac{e^x-1-x^2}{x^4+x^3+x^2} = -\infty \end{equation} \begin{equation} \lim_{x \to 0^+} \frac{e^x-1-x^2}{x^4+x^3+x^2} = +\infty \end{equation} Coincidentally (or not) both limits (from the right of both equations and from the left of both equations) give the same answer. Therefore, the limit does not exist. For the other function below, the same thing happens: \begin{equation} \lim_{x \to 0} \frac{sin(x)}{x^4+x^3} \end{equation} So what I dind't find out is which function I should use, or if it really doens't matter. Thank you","I was wondering if anyone could help me with computing a limit using L'Hôpital's Rule. Using L'Hôpital Rule for the following limit, I get the following result: (1/0) which means that the limit is either , or it does not exist. If I want to find out the answer, I should take the side limits - but here lies my question: the side mimits of what? The original function or the function after L'Hôpital rule? Or any of the two? Should I do this Or this? Coincidentally (or not) both limits (from the right of both equations and from the left of both equations) give the same answer. Therefore, the limit does not exist. For the other function below, the same thing happens: So what I dind't find out is which function I should use, or if it really doens't matter. Thank you","\begin{equation}
\lim_{x \to 0} \frac{e^x-1-x^2}{x^4+x^3+x^2} \therefore \lim_{x \to 0} \frac{e^x-1-x^2}{x^4+x^3+x^2} \stackrel{L'H}{=} \lim_{x \to 0} \frac{e^x-2x}{4x^3+3x^2+2x}
\end{equation} +\infty -\infty \begin{equation}
\lim_{x \to 0^-} \frac{e^x-2x}{4x^3+3x^2+2x} = -\infty
\end{equation} \begin{equation}
\lim_{x \to 0^+} \frac{e^x-2x}{4x^3+3x^2+2x} = +\infty
\end{equation} \begin{equation}
\lim_{x \to 0^-} \frac{e^x-1-x^2}{x^4+x^3+x^2} = -\infty
\end{equation} \begin{equation}
\lim_{x \to 0^+} \frac{e^x-1-x^2}{x^4+x^3+x^2} = +\infty
\end{equation} \begin{equation}
\lim_{x \to 0} \frac{sin(x)}{x^4+x^3}
\end{equation}","['calculus', 'limits', 'derivatives']"
49,Simplest form of $h'(y)$ given $h(y)= (1-3y^2)^5 \cdot ( y^2 + 2)^6$,Simplest form of  given,h'(y) h(y)= (1-3y^2)^5 \cdot ( y^2 + 2)^6,Find $h'(y)$ in the simplest form if the $$h(y)= (1-3y^2)^5 \cdot ( y^2 + 2)^6$$ My answer was: $$-30y(1-3y^2)^4 \cdot (y^2+2)^6 + 12y(y^2+2)^5 \cdot (1-3y^2)^5$$ But according to wolfram alpha the answer was $$ -30y(1-3y^2)^4 \cdot (y^2+2)^6 $$ only. which makes it weird that they did not differentiate the second term my answer was different. I posted it again because I did not get an answer.,Find $h'(y)$ in the simplest form if the $$h(y)= (1-3y^2)^5 \cdot ( y^2 + 2)^6$$ My answer was: $$-30y(1-3y^2)^4 \cdot (y^2+2)^6 + 12y(y^2+2)^5 \cdot (1-3y^2)^5$$ But according to wolfram alpha the answer was $$ -30y(1-3y^2)^4 \cdot (y^2+2)^6 $$ only. which makes it weird that they did not differentiate the second term my answer was different. I posted it again because I did not get an answer.,,"['calculus', 'derivatives']"
50,Directional derivatives exist for function neither continuous nor differentiable at the point they exist,Directional derivatives exist for function neither continuous nor differentiable at the point they exist,,"It's an old past paper with no mark scheme, this always makes me a slightly afraid of exploring unaided. The function is: $f(x,y)=\frac{xy^2}{x^2+y^6}$ if $(x,y)\ne(0,0)$ else $0$. I am to show all the directional derivatives exist at $(0,0)$ Intuitively, $(\nabla f)(0,y)=0=(\nabla f)(x,0)$ because clearly in either case $f$ is constant, at 0. So using some linear algebra, and the linear map that it the derivative ($(Df(a))(v_x,v_y)$ is the derivative function at a in direction $v=(v_x,v_y)$, well as it is a linear map $(Df(a))(v)=(Df(a))(v_x(1,0)+v_y(0,1))=v_x(Df(a))(1,0)+v_y(Df(a))(0,1)$ By using definitions (namely: $[(Df(a))]_{i,j}=(\nabla f_i)_j$) we see $(Df(a))(v)=(\nabla f)(v_x,0)+(\nabla f)(v_y,0)=0$ The question asks for: 1) State the exact definition of differentiability at $(0,0)$ for $f:\mathbb{R}^2\rightarrow\mathbb{R}$ 2) Show all the directional derivatives of f exist at (0,0) and comput their values. 3) Show that f is not continuous at (0,0) I can (probably) do this - I'd show it isn't sequentially continuous $\implies$ not continuous 4) Show that $f$ is not differentiable at $(0,0)$ using the definition in part 1. (Hint: you can take a small deviation away from the origin param by $h_1=t^3,h_2=t,t\in\mathbb{R},t\ne0$) For part 4 I think I can use: $f$ differentiable $\iff$ $$lim_{h->0}(\frac{f(a+h)-f(a)-(Df(a))(h)} {||h||})=0$$ Then show the limit from $t<0$ and $t>0$ disagree (or something) I am confused because the curve $f(x,y)=xy$ also has derivatives of 0 along each axis, yet for the direction (1,1) the curve is certainly not doing nothing (staying constant), but the properties of a linear map.... I am also confused because $(t^3,t)$ is a direction!","It's an old past paper with no mark scheme, this always makes me a slightly afraid of exploring unaided. The function is: $f(x,y)=\frac{xy^2}{x^2+y^6}$ if $(x,y)\ne(0,0)$ else $0$. I am to show all the directional derivatives exist at $(0,0)$ Intuitively, $(\nabla f)(0,y)=0=(\nabla f)(x,0)$ because clearly in either case $f$ is constant, at 0. So using some linear algebra, and the linear map that it the derivative ($(Df(a))(v_x,v_y)$ is the derivative function at a in direction $v=(v_x,v_y)$, well as it is a linear map $(Df(a))(v)=(Df(a))(v_x(1,0)+v_y(0,1))=v_x(Df(a))(1,0)+v_y(Df(a))(0,1)$ By using definitions (namely: $[(Df(a))]_{i,j}=(\nabla f_i)_j$) we see $(Df(a))(v)=(\nabla f)(v_x,0)+(\nabla f)(v_y,0)=0$ The question asks for: 1) State the exact definition of differentiability at $(0,0)$ for $f:\mathbb{R}^2\rightarrow\mathbb{R}$ 2) Show all the directional derivatives of f exist at (0,0) and comput their values. 3) Show that f is not continuous at (0,0) I can (probably) do this - I'd show it isn't sequentially continuous $\implies$ not continuous 4) Show that $f$ is not differentiable at $(0,0)$ using the definition in part 1. (Hint: you can take a small deviation away from the origin param by $h_1=t^3,h_2=t,t\in\mathbb{R},t\ne0$) For part 4 I think I can use: $f$ differentiable $\iff$ $$lim_{h->0}(\frac{f(a+h)-f(a)-(Df(a))(h)} {||h||})=0$$ Then show the limit from $t<0$ and $t>0$ disagree (or something) I am confused because the curve $f(x,y)=xy$ also has derivatives of 0 along each axis, yet for the direction (1,1) the curve is certainly not doing nothing (staying constant), but the properties of a linear map.... I am also confused because $(t^3,t)$ is a direction!",,"['real-analysis', 'derivatives']"
51,"An object is travelling in a straight line. Its distance, s meters, from a fixed point at time t seconds is given by the expression","An object is travelling in a straight line. Its distance, s meters, from a fixed point at time t seconds is given by the expression",,$$s=t^3−t^2−6t$$ a) Find ds/dt when t=3 and interpret this result. b) Find d^2s/dt^2 when t=3 and interpret this result. c) Find the time in seconds when the velocity is 2m/s (d) Using the results from 5(c) find the distance travelled when the velocity is 2m/s A) $$S=t^3-t^2-6t$$ $$dy/dx = 3t^2-2t-6$$ $$When t = 3 = ds/df = 27-6-6 = 15ms^{-1}$$ B) $$d^2s/dt^2 = 6t -2$$ $$=6(3)-2$$ $$=16$$ If i'm honest i don't know how to progress across the other questions from here. Can some show me how to do it/answer it? Really don't understand it,$$s=t^3−t^2−6t$$ a) Find ds/dt when t=3 and interpret this result. b) Find d^2s/dt^2 when t=3 and interpret this result. c) Find the time in seconds when the velocity is 2m/s (d) Using the results from 5(c) find the distance travelled when the velocity is 2m/s A) $$S=t^3-t^2-6t$$ $$dy/dx = 3t^2-2t-6$$ $$When t = 3 = ds/df = 27-6-6 = 15ms^{-1}$$ B) $$d^2s/dt^2 = 6t -2$$ $$=6(3)-2$$ $$=16$$ If i'm honest i don't know how to progress across the other questions from here. Can some show me how to do it/answer it? Really don't understand it,,"['calculus', 'derivatives', 'partial-differential-equations', 'differential-forms']"
52,A particle moves along a path described by $y = 4 − x^2$ . At what point along the curve are $x$ and $y$ changing at the same rate?,A particle moves along a path described by  . At what point along the curve are  and  changing at the same rate?,y = 4 − x^2 x y,"Why is the answer $\left(-\frac 1 2 , \frac{15}{4} \right)$? I have no idea how to approach this problem. Can someone guide me/explain it to me step by step? I have a final on this in less than 2 hours and I'm freaking out!","Why is the answer $\left(-\frac 1 2 , \frac{15}{4} \right)$? I have no idea how to approach this problem. Can someone guide me/explain it to me step by step? I have a final on this in less than 2 hours and I'm freaking out!",,"['calculus', 'derivatives']"
53,"Prove that if the derivative $f'(x)$ of a function exists on the measurable set $E$, then $f'(x)$ is measurable on $E$.","Prove that if the derivative  of a function exists on the measurable set , then  is measurable on .",f'(x) E f'(x) E,"Prove that if the derivative $f'(x)$ of a function exists on the measurable set $E$, then $f'(x)$ is measurable on $E$. We are told to only consider 1 dimensional spaces,that f is  a  measurable function  in one variable.that is, f is  a  measurable function  in one variable. Following is my solution, I am not sure whether I am on the right track.Can someone have a look? Many thanks, I can explain further if I should.  Thanks in advance!","Prove that if the derivative $f'(x)$ of a function exists on the measurable set $E$, then $f'(x)$ is measurable on $E$. We are told to only consider 1 dimensional spaces,that f is  a  measurable function  in one variable.that is, f is  a  measurable function  in one variable. Following is my solution, I am not sure whether I am on the right track.Can someone have a look? Many thanks, I can explain further if I should.  Thanks in advance!",,"['measure-theory', 'derivatives', 'proof-verification', 'lebesgue-measure']"
54,The derivative of $x^TAx$ w.r.t $t$,The derivative of  w.r.t,x^TAx t,"Suppose $P = x^TAx$ How to find $\frac{dP}{dt}$? if $x' = Bx$   , where $B$ has the same dimension as $A$. How to find the final answer? my answer is: $$\frac{dP}{dt} = 2[(A+A^T)x]x' = 2[(A+A^T)x]Bx$$ However, it seems that $Bx$ is a $(n\times 1)$ vector and $x$ is also an vector so we cannot find the final answer. Is it true?","Suppose $P = x^TAx$ How to find $\frac{dP}{dt}$? if $x' = Bx$   , where $B$ has the same dimension as $A$. How to find the final answer? my answer is: $$\frac{dP}{dt} = 2[(A+A^T)x]x' = 2[(A+A^T)x]Bx$$ However, it seems that $Bx$ is a $(n\times 1)$ vector and $x$ is also an vector so we cannot find the final answer. Is it true?",,"['matrices', 'derivatives']"
55,"Least Squares Curve fitting, why make the error derivative 0?","Least Squares Curve fitting, why make the error derivative 0?",,"My notes define the error of least squares approximation as: $$ E=\sum_{i=1}^n(y_i-f(x_i))^2\tag1 $$ Which, for a straight line gives: $$ f(x_i)=a+bx\tag2$$ $$ E=\sum_{i=1}^n(y_i-(a+bx_i))^2\tag3 $$ Which makes sense to me.  The notes then claim that to minimise the error, make the derivatives of $E$ w.r.t $a$ and $b$ equal to zero.  Why is this? I think of the derivative of a function as the rate of change that function.  But why would a constant error necessarily be the smallest error? Thanks","My notes define the error of least squares approximation as: $$ E=\sum_{i=1}^n(y_i-f(x_i))^2\tag1 $$ Which, for a straight line gives: $$ f(x_i)=a+bx\tag2$$ $$ E=\sum_{i=1}^n(y_i-(a+bx_i))^2\tag3 $$ Which makes sense to me.  The notes then claim that to minimise the error, make the derivatives of $E$ w.r.t $a$ and $b$ equal to zero.  Why is this? I think of the derivative of a function as the rate of change that function.  But why would a constant error necessarily be the smallest error? Thanks",,['derivatives']
56,Lagrange Method Problem,Lagrange Method Problem,,"I am from engineering background and I am currently studying calculus. I had a question from assignment to be solved from a course on coursera but I could not do it. People have posted solution in the discussion but I cannot understand it. The question is as follows: Minimize the following function using the Lagrangean method: \begin{cases} f(x,y) = 6x+\frac{96}{x}+\frac{4y}{x}+\frac{x}{y}\\ x+y=6 \end{cases} Can anyone help me in understanding the approach of how to apply Lagrangean method here. Thank you.","I am from engineering background and I am currently studying calculus. I had a question from assignment to be solved from a course on coursera but I could not do it. People have posted solution in the discussion but I cannot understand it. The question is as follows: Minimize the following function using the Lagrangean method: \begin{cases} f(x,y) = 6x+\frac{96}{x}+\frac{4y}{x}+\frac{x}{y}\\ x+y=6 \end{cases} Can anyone help me in understanding the approach of how to apply Lagrangean method here. Thank you.",,"['calculus', 'real-analysis', 'derivatives', 'optimization', 'lagrange-multiplier']"
57,Integral of $e^{(a+ib)x}$,Integral of,e^{(a+ib)x},"Given the function $f:\mathbb{R}\rightarrow \mathbb{C}$, such that $f(x)=e^{(a+ib)x}$, how can I compute $f'(x)$ and $\int f(x)dx$ ? Certanly, one can use the identity $e^{ibx}=\cos(bx)+i\sin(bx)$ and then compute the derivative and integral using the well known rules from real analysis. But is it true that $f'(x)=(a+ib)e^{(a+ib)x}$ (I've shown this but I want to be sure) and is there a similar rule for computing the integral?","Given the function $f:\mathbb{R}\rightarrow \mathbb{C}$, such that $f(x)=e^{(a+ib)x}$, how can I compute $f'(x)$ and $\int f(x)dx$ ? Certanly, one can use the identity $e^{ibx}=\cos(bx)+i\sin(bx)$ and then compute the derivative and integral using the well known rules from real analysis. But is it true that $f'(x)=(a+ib)e^{(a+ib)x}$ (I've shown this but I want to be sure) and is there a similar rule for computing the integral?",,"['calculus', 'real-analysis', 'integration', 'derivatives']"
58,"Given a differentiable function for every $x \geq 0$, define a differentiable function for every $x$","Given a differentiable function for every , define a differentiable function for every",x \geq 0 x,"Given $f(x)$: $f(0)=1$ Positive for every $x \geq 0$ Differentiable for every $x \geq 0$ Let $g(x)=  \begin{cases}   f(x)    & \text{$x \geq 0$}\\   1/f(-x) & \text{$x \leq 0$}  \end{cases} $ Is $g(x)$ differentiable for every $x$? I believe that the answer is yes, because the only questionable point is at $x=0$. And since $\displaystyle{\lim_{x \to 0^-}{g(x)} = \lim_{x \to 0^+}{g(x)} = 1}$, function $g(x)$ is differentiable  at that point. For $x>0$, function $g(x)$ is differentiable because function $f(x)$ is differentiable. For $x<0$, function $g(x)$ is differentiable because function $1/f(-x)$ is differentiable. Is this correct, or am I missing something (a counterexample would be appreciated in this case)?","Given $f(x)$: $f(0)=1$ Positive for every $x \geq 0$ Differentiable for every $x \geq 0$ Let $g(x)=  \begin{cases}   f(x)    & \text{$x \geq 0$}\\   1/f(-x) & \text{$x \leq 0$}  \end{cases} $ Is $g(x)$ differentiable for every $x$? I believe that the answer is yes, because the only questionable point is at $x=0$. And since $\displaystyle{\lim_{x \to 0^-}{g(x)} = \lim_{x \to 0^+}{g(x)} = 1}$, function $g(x)$ is differentiable  at that point. For $x>0$, function $g(x)$ is differentiable because function $f(x)$ is differentiable. For $x<0$, function $g(x)$ is differentiable because function $1/f(-x)$ is differentiable. Is this correct, or am I missing something (a counterexample would be appreciated in this case)?",,"['calculus', 'derivatives']"
59,How to take derivative of sums of absolute values,How to take derivative of sums of absolute values,,Take the derivative of $f(m) = \sum_i | x_i - m |$. I've been told that derivative of each term is +1 or -1. How do you show that?,Take the derivative of $f(m) = \sum_i | x_i - m |$. I've been told that derivative of each term is +1 or -1. How do you show that?,,"['derivatives', 'absolute-value']"
60,Dini derivatives and fundamental theorem of calculus,Dini derivatives and fundamental theorem of calculus,,"I have been looking for some references concerning the fundamental theorem of calculus and Dini derivatives and I did not find it. I would like to know if given a locally Lipschitz function $f:\mathbb{R}\to\mathbb{R}$, then it is related to its Dini derivative by \begin{equation*}  f(t)=f(0)+\int_0^tD^+f(s)\,ds. \end{equation*} Does someone knows a reference on that?","I have been looking for some references concerning the fundamental theorem of calculus and Dini derivatives and I did not find it. I would like to know if given a locally Lipschitz function $f:\mathbb{R}\to\mathbb{R}$, then it is related to its Dini derivative by \begin{equation*}  f(t)=f(0)+\int_0^tD^+f(s)\,ds. \end{equation*} Does someone knows a reference on that?",,"['real-analysis', 'integration', 'analysis', 'derivatives', 'definite-integrals']"
61,"Prove that $DT = I_v$, $TD \neq I_v$, where $D$ = differentiation operator and $T$ is integration","Prove that , , where  = differentiation operator and  is integration",DT = I_v TD \neq I_v D T,"Let $V$ be the linear space of all real polys $p(x)$. Let $D$ denote the differentiation operator, and let $T$ the integration operator that maps each polynomial $p$ onto the polynomial $q$ given by $q(x) = \int_0^x p(t) dt$. Prove that $DT = I_v$ and $TD \neq I_v$. Describe the null space and range of $TD$. $My$ $Previous$ $work$: $D\int_0^x p(t) dt = D(P(x) -P(0)) = D(P(x)) - D(P(0)) = p(x)$. First, is this correct? It can't be because $p(x)$ can't be $I_v$ (which I take to mean the identity element in $V$). $\int_0^x Dp(t) dt  = \int_0^x p'(t) dt = p(x) - p(0)$. I'm not sure what to do with this? And what is the null space and range of this?","Let $V$ be the linear space of all real polys $p(x)$. Let $D$ denote the differentiation operator, and let $T$ the integration operator that maps each polynomial $p$ onto the polynomial $q$ given by $q(x) = \int_0^x p(t) dt$. Prove that $DT = I_v$ and $TD \neq I_v$. Describe the null space and range of $TD$. $My$ $Previous$ $work$: $D\int_0^x p(t) dt = D(P(x) -P(0)) = D(P(x)) - D(P(0)) = p(x)$. First, is this correct? It can't be because $p(x)$ can't be $I_v$ (which I take to mean the identity element in $V$). $\int_0^x Dp(t) dt  = \int_0^x p'(t) dt = p(x) - p(0)$. I'm not sure what to do with this? And what is the null space and range of this?",,"['linear-algebra', 'integration', 'derivatives', 'vector-spaces']"
62,Show that $f$ is not increasing on any interval containing $0$,Show that  is not increasing on any interval containing,f 0,"$f:R\to R$, $f(x)=x^2\sin(1/x)+x$ if $x\ne 0$ and $0$ if $x=0$ In the first part of this problem, I showed that $f'(0)>0$ The second part of the problem is this: Show that $f$ is not increasing on any interval containing $0$ I tried to find one positive number $h$ arbitrarily close to $0$, and one negative number $k$ arbitrarily close to $0$, such that $f(h)$ & $f(k)$ were both $\lt 0$ but I could not figure out how to do so. Here is my attempt: Let $k\to 0^-$. As long as $|k|\lt 1$, we have $|k^2|\lt |k|$ so $f(k)\lt 0$. Let $h\to 0^+$. Note $1/h\to \infty$, so $\sin(1/h)$ cycles between $-1$ and $1$. To find the minimum of $f(h)$, choose $h\gt 0$ s.t. $\sin(1/h)=-1$. $|h|\lt1$ so $h^2<h \implies 0\lt h-h^2=f(h)$. Clearly this does not prove what I want. How should I approach this problem?","$f:R\to R$, $f(x)=x^2\sin(1/x)+x$ if $x\ne 0$ and $0$ if $x=0$ In the first part of this problem, I showed that $f'(0)>0$ The second part of the problem is this: Show that $f$ is not increasing on any interval containing $0$ I tried to find one positive number $h$ arbitrarily close to $0$, and one negative number $k$ arbitrarily close to $0$, such that $f(h)$ & $f(k)$ were both $\lt 0$ but I could not figure out how to do so. Here is my attempt: Let $k\to 0^-$. As long as $|k|\lt 1$, we have $|k^2|\lt |k|$ so $f(k)\lt 0$. Let $h\to 0^+$. Note $1/h\to \infty$, so $\sin(1/h)$ cycles between $-1$ and $1$. To find the minimum of $f(h)$, choose $h\gt 0$ s.t. $\sin(1/h)=-1$. $|h|\lt1$ so $h^2<h \implies 0\lt h-h^2=f(h)$. Clearly this does not prove what I want. How should I approach this problem?",,"['calculus', 'real-analysis', 'derivatives']"
63,Why can't a non-zero polynomial satisfy some equations?,Why can't a non-zero polynomial satisfy some equations?,,I'm having a hard time visually picturing/understanding how to explain why a non-zero polynomial function cannot satisfy the equation: $f''(x)$ = $-f(x)$ So is it basically asking to explain why a polynomial function must contain a 0 coefficient? I understand that taking derivatives decreases the degree of the functions. I need some help please. Thank you!,I'm having a hard time visually picturing/understanding how to explain why a non-zero polynomial function cannot satisfy the equation: $f''(x)$ = $-f(x)$ So is it basically asking to explain why a polynomial function must contain a 0 coefficient? I understand that taking derivatives decreases the degree of the functions. I need some help please. Thank you!,,"['calculus', 'polynomials', 'derivatives']"
64,Does the derivative of $\;\sqrt{x}- \arctan \sqrt{x} \;$ exist at $x=0$?,Does the derivative of  exist at ?,\;\sqrt{x}- \arctan \sqrt{x} \; x=0,"This is an exercise in Apostol Calculus Vol.1 E6.22 Q.16. Let $\,f(x)=\sqrt{x} - \arctan \sqrt{x}.\;$ Then $$\begin{align} f'(x) &= \frac{1}{2 \sqrt{x}}- \frac{1}{1+(\sqrt{x})^2} \frac{1}  {2\sqrt{x}} \\&= \frac{1}{2\sqrt{x}} \frac {1+x-1}{1+x} \\&= \frac {1} {2\sqrt{x}} \frac{x}{1+x} \\&= \frac{\sqrt{x}}{2(1+x)} \end{align}$$ The answer from the textbook said it's valid if $x\geq0$, but we cancel out $\sqrt{x}$ when deriving $f'(x)$. I don't know why it's valid when $x=0$, so I hope someone could give me an explanation. Thanks.","This is an exercise in Apostol Calculus Vol.1 E6.22 Q.16. Let $\,f(x)=\sqrt{x} - \arctan \sqrt{x}.\;$ Then $$\begin{align} f'(x) &= \frac{1}{2 \sqrt{x}}- \frac{1}{1+(\sqrt{x})^2} \frac{1}  {2\sqrt{x}} \\&= \frac{1}{2\sqrt{x}} \frac {1+x-1}{1+x} \\&= \frac {1} {2\sqrt{x}} \frac{x}{1+x} \\&= \frac{\sqrt{x}}{2(1+x)} \end{align}$$ The answer from the textbook said it's valid if $x\geq0$, but we cancel out $\sqrt{x}$ when deriving $f'(x)$. I don't know why it's valid when $x=0$, so I hope someone could give me an explanation. Thanks.",,"['real-analysis', 'derivatives']"
65,How to determine quantity of concavity?,How to determine quantity of concavity?,,"Given a function $f(x)$ I can determine whether its concave up or concave down by using the second derivative as it says e. g. here . $$f''(x) > 0 \qquad \text{concave up}$$ $$f''(x) < 0 \qquad \text{concave down}$$ For instance $$f(x) = x^2 \qquad f'(x) = 2x \qquad f''(x) = 2$$ Does that mean that since $f''> 0$ is true for all x, the function $f(x)$ is always concave up? And how can I determine the quantity of concavity? I thought somehow that $f''$ gives me the quantity but $f''$ is constant and to me it seems that the concavity of $f$ changes, so I guess this is wrong. I mean $f(x)$ has a stronger concavity around $x = 0$, hasn't it?","Given a function $f(x)$ I can determine whether its concave up or concave down by using the second derivative as it says e. g. here . $$f''(x) > 0 \qquad \text{concave up}$$ $$f''(x) < 0 \qquad \text{concave down}$$ For instance $$f(x) = x^2 \qquad f'(x) = 2x \qquad f''(x) = 2$$ Does that mean that since $f''> 0$ is true for all x, the function $f(x)$ is always concave up? And how can I determine the quantity of concavity? I thought somehow that $f''$ gives me the quantity but $f''$ is constant and to me it seems that the concavity of $f$ changes, so I guess this is wrong. I mean $f(x)$ has a stronger concavity around $x = 0$, hasn't it?",,['analysis']
66,Weak derivative of one parameter group and the domain of its generator,Weak derivative of one parameter group and the domain of its generator,,"Let $U(t)=\exp(i t A)$ be a one parameter group generated by self-adjoint (unbounded) operator A. It is well-known that if $$  \lim_{t\rightarrow 0} \frac{U(t)\psi-\psi}{t} $$ exists then $\psi$ belongs to the domain of $A$ (see e.g. Reed and Simon 'Methods of Modern Mathematical Physics I', theorem VIII.7). I would like to replace the condition with the following one: $$  \forall_{\phi\in\mathcal{H}}\lim_{t\rightarrow 0} \frac{(\phi|U(t)\psi-\psi)}{t} ~~~~~~\textrm{and}~~~~~  \forall_{\phi\in\mathcal{H}} \lim_{t\rightarrow 0} \frac{(\phi|U(t)\psi+U(-t)\psi-2\psi)}{t^2} $$ exist. Does it imply that $\psi$ is in the domain of $A$? Intuitively the first condition guarantees the existence of the weak derivative, and the second one tells us that the norm is continuous. Am I right? Note that for $\psi \in \operatorname{Dom}(A)$ we have $$  \forall_{\phi\in\mathcal{H}}\lim_{t\rightarrow 0} \frac{(\phi|U(t)\psi-\psi)}{t}=(\phi|A\psi) ~~~~~~\textrm{and}~~~~~  \lim_{t\rightarrow 0} \frac{(\psi|U(t)\psi+U(-t)\psi-2\psi)}{t^2}=(A\psi,A\psi). $$","Let $U(t)=\exp(i t A)$ be a one parameter group generated by self-adjoint (unbounded) operator A. It is well-known that if $$  \lim_{t\rightarrow 0} \frac{U(t)\psi-\psi}{t} $$ exists then $\psi$ belongs to the domain of $A$ (see e.g. Reed and Simon 'Methods of Modern Mathematical Physics I', theorem VIII.7). I would like to replace the condition with the following one: $$  \forall_{\phi\in\mathcal{H}}\lim_{t\rightarrow 0} \frac{(\phi|U(t)\psi-\psi)}{t} ~~~~~~\textrm{and}~~~~~  \forall_{\phi\in\mathcal{H}} \lim_{t\rightarrow 0} \frac{(\phi|U(t)\psi+U(-t)\psi-2\psi)}{t^2} $$ exist. Does it imply that $\psi$ is in the domain of $A$? Intuitively the first condition guarantees the existence of the weak derivative, and the second one tells us that the norm is continuous. Am I right? Note that for $\psi \in \operatorname{Dom}(A)$ we have $$  \forall_{\phi\in\mathcal{H}}\lim_{t\rightarrow 0} \frac{(\phi|U(t)\psi-\psi)}{t}=(\phi|A\psi) ~~~~~~\textrm{and}~~~~~  \lim_{t\rightarrow 0} \frac{(\psi|U(t)\psi+U(-t)\psi-2\psi)}{t^2}=(A\psi,A\psi). $$",,"['functional-analysis', 'derivatives', 'operator-theory', 'hilbert-spaces', 'weak-convergence']"
67,The 7-th derivative of $ x^3 \cdot\tan(2x) $ is this right,The 7-th derivative of  is this right, x^3 \cdot\tan(2x) ,"I have to find $y^{(7)}\left(0\right)$ of $y(x)=x^3\cdot\tan{(2x)}$ So my idea was to use Taylor expansion for $\tan(2x)$ to the $7$-th element and then multiply the hole thing by $x^3 $ and then replacing $x$ with $ 0$, since $ x^3$ isn't 7-th times differentiable. My question is, is this allowed  and if yes when and how. If not any ideas for an easy way of doing it ?","I have to find $y^{(7)}\left(0\right)$ of $y(x)=x^3\cdot\tan{(2x)}$ So my idea was to use Taylor expansion for $\tan(2x)$ to the $7$-th element and then multiply the hole thing by $x^3 $ and then replacing $x$ with $ 0$, since $ x^3$ isn't 7-th times differentiable. My question is, is this allowed  and if yes when and how. If not any ideas for an easy way of doing it ?",,"['calculus', 'derivatives', 'taylor-expansion']"
68,L'Hôpital's rule for $\mathbb R ^n \to \mathbb R $ functions.,L'Hôpital's rule for  functions.,\mathbb R ^n \to \mathbb R ,"I want to know if there exists a generalization of  L'Hopital rule in $n$ dimensions? For example, let us consider this problem . There it is just said that we should take separate  path and see if they will end with same  number,but can't we generalize L'Hopital rule and partial derivatives in $n$ dimensions? Just a little hint will help me too much.","I want to know if there exists a generalization of  L'Hopital rule in $n$ dimensions? For example, let us consider this problem . There it is just said that we should take separate  path and see if they will end with same  number,but can't we generalize L'Hopital rule and partial derivatives in $n$ dimensions? Just a little hint will help me too much.",,"['calculus', 'limits', 'derivatives', 'partial-derivative']"
69,find the second derivative $(x^3+x-1)(x^3+1)$,find the second derivative,(x^3+x-1)(x^3+1),$$(x^3+x-1)(x^3+1)$$ $$=x^6+x^4-x^3+x^3+x-1$$ $$f'(x)= 6x^5+4x^3-3x^2+3x^2+1$$ Am I suppose to cancel out $-3x^2+3x^2$? $$f''(x)= 30x^4+12x^2-6x+6x$$ Can someone check my work please? Thank you so much!,$$(x^3+x-1)(x^3+1)$$ $$=x^6+x^4-x^3+x^3+x-1$$ $$f'(x)= 6x^5+4x^3-3x^2+3x^2+1$$ Am I suppose to cancel out $-3x^2+3x^2$? $$f''(x)= 30x^4+12x^2-6x+6x$$ Can someone check my work please? Thank you so much!,,"['calculus', 'derivatives']"
70,Find the 100th derivative of $x \sinh(2x)$,Find the 100th derivative of,x \sinh(2x),"If $f(x) = x \sinh(2x)$, find $f^{({100})}(x)$. My (Incorrect) working so far: Using Leibniz' Formula for derivatives: $$(fg)^{(n)}=\sum_{k=0}^n{n\choose k}f^{(k)}g^{(n-k)}$$ $$(x\sinh(2x))^{(100)}=\sum_{k=0}^{100}{100\choose k}f^{(k)}g^{(n-k)}=$$ Since the second derivative of $x$ is $0$... $$=\sum_{k=0}^1{2\choose k}f^{(k)}g^{(n-k)}$$ $$={2\choose 0}(x)(\sinh(2x))+{2\choose 1}(1)(2\cosh(2x))$$ $$=4x \sinh(2x)+4 \cosh(2x)$$ This is my answer and it is not correct. What do I need to change to make it correct? Thanks in advance.","If $f(x) = x \sinh(2x)$, find $f^{({100})}(x)$. My (Incorrect) working so far: Using Leibniz' Formula for derivatives: $$(fg)^{(n)}=\sum_{k=0}^n{n\choose k}f^{(k)}g^{(n-k)}$$ $$(x\sinh(2x))^{(100)}=\sum_{k=0}^{100}{100\choose k}f^{(k)}g^{(n-k)}=$$ Since the second derivative of $x$ is $0$... $$=\sum_{k=0}^1{2\choose k}f^{(k)}g^{(n-k)}$$ $$={2\choose 0}(x)(\sinh(2x))+{2\choose 1}(1)(2\cosh(2x))$$ $$=4x \sinh(2x)+4 \cosh(2x)$$ This is my answer and it is not correct. What do I need to change to make it correct? Thanks in advance.",,"['derivatives', 'summation', 'binomial-coefficients']"
71,Inequality involving derivative of a complex function,Inequality involving derivative of a complex function,,"Let $D = B(0,1) \subset \mathbb{C} $ a disc, $f$ holomorphic on $D$. Show that $$ 2|f^{'}(0)| \le \sup_{z, w \in D} |f(z)-f(w)|$$  Furthermore, there is equality if and only if $f$ is linear. Any hint ?","Let $D = B(0,1) \subset \mathbb{C} $ a disc, $f$ holomorphic on $D$. Show that $$ 2|f^{'}(0)| \le \sup_{z, w \in D} |f(z)-f(w)|$$  Furthermore, there is equality if and only if $f$ is linear. Any hint ?",,"['complex-analysis', 'analysis', 'derivatives']"
72,Calculating generalized derivatives for piecewise constant functions,Calculating generalized derivatives for piecewise constant functions,,Let $$f(x) = \begin{cases} 0 &\mbox{if } x< 0 \\ 2 & \mbox{if } x=0 \\ 1 &\mbox{if } x>0 \end{cases}$$ $$f_1(x) = \begin{cases} -2 &\mbox{if } x< 0 \\ 0 & \mbox{if } x=0 \\ 1 &\mbox{if } x>0 \end{cases}$$ $$f_2(x) = \begin{cases} -2 &\mbox{if } x< 0 \\ -2 & \mbox{if } x=0 \\ 0 &\mbox{if } x>0 \end{cases}$$ Then $f(x)=f_1(x)-f_2(x)$ for all $x\in\mathbb{R}$. My notes say that the generalized derivative of $f$ is $df_1-df_2$. (I'm not sure what that means). How can we calculate $df_1-df_2$?,Let $$f(x) = \begin{cases} 0 &\mbox{if } x< 0 \\ 2 & \mbox{if } x=0 \\ 1 &\mbox{if } x>0 \end{cases}$$ $$f_1(x) = \begin{cases} -2 &\mbox{if } x< 0 \\ 0 & \mbox{if } x=0 \\ 1 &\mbox{if } x>0 \end{cases}$$ $$f_2(x) = \begin{cases} -2 &\mbox{if } x< 0 \\ -2 & \mbox{if } x=0 \\ 0 &\mbox{if } x>0 \end{cases}$$ Then $f(x)=f_1(x)-f_2(x)$ for all $x\in\mathbb{R}$. My notes say that the generalized derivative of $f$ is $df_1-df_2$. (I'm not sure what that means). How can we calculate $df_1-df_2$?,,"['calculus', 'real-analysis', 'derivatives']"
73,Proving the derivative is $0$ at the extremum and all derivatives are $0$.,Proving the derivative is  at the extremum and all derivatives are .,0 0,"The pictures below show the proof that Apostol uses in his book. I can't understand why Apostol introduces the function $Q(x)$ and proves the theorem by contradiction using the sign preserving property. What we want to prove:$$\lim_{h\to 0}\frac{f(c+h)-f(c)}h=0$$ The way I proved it was: Since $f'(c)$ exists, this means that the function $f$ is continuous at point $c$. Using the definition of continuity at a point $c$ this means that $$\lim_{x\to c}f(x)=f(c)$$ Let $x=c+h$ so the above limit becomes $$\lim_{c+h \to c} f(c+h)=f(c)$$ which is equivalent to $$\lim_{h \to 0} f(c+h)-f(c)=0$$  This is all assuming $c+h$ lies in open interval $I$. So since the above limit appoahes $0$ as $h$ approaches $0$ the limit to be proved is proved. $\square$ Is that right ??","The pictures below show the proof that Apostol uses in his book. I can't understand why Apostol introduces the function $Q(x)$ and proves the theorem by contradiction using the sign preserving property. What we want to prove:$$\lim_{h\to 0}\frac{f(c+h)-f(c)}h=0$$ The way I proved it was: Since $f'(c)$ exists, this means that the function $f$ is continuous at point $c$. Using the definition of continuity at a point $c$ this means that $$\lim_{x\to c}f(x)=f(c)$$ Let $x=c+h$ so the above limit becomes $$\lim_{c+h \to c} f(c+h)=f(c)$$ which is equivalent to $$\lim_{h \to 0} f(c+h)-f(c)=0$$  This is all assuming $c+h$ lies in open interval $I$. So since the above limit appoahes $0$ as $h$ approaches $0$ the limit to be proved is proved. $\square$ Is that right ??",,"['calculus', 'limits', 'derivatives', 'proof-verification', 'alternative-proof']"
74,The Concept of Instantaneous Velocity,The Concept of Instantaneous Velocity,,"The concept of instantaneous velocity really becomes counter-intuitive to me when I really think deeply about it. Instantaneous velocity is the velocity of something at an instant of time; however, at the very next instant the velocity changes. In general, speed tells us how quickly something is changing its position with respect to some point in space. Then by instantaneous velocity we mean that the object at some instant is changing its position with respect to time, but may change to a velocity much faster. But if it is changing its position with that rate then the time passes by and that instant as well and at another instant the velocity is different. Did it even move with that velocity for a moment? I’m sorry that I’m not exactly able to tell you what my problem is; however, it is something like I've described.","The concept of instantaneous velocity really becomes counter-intuitive to me when I really think deeply about it. Instantaneous velocity is the velocity of something at an instant of time; however, at the very next instant the velocity changes. In general, speed tells us how quickly something is changing its position with respect to some point in space. Then by instantaneous velocity we mean that the object at some instant is changing its position with respect to time, but may change to a velocity much faster. But if it is changing its position with that rate then the time passes by and that instant as well and at another instant the velocity is different. Did it even move with that velocity for a moment? I’m sorry that I’m not exactly able to tell you what my problem is; however, it is something like I've described.",,"['derivatives', 'calculus']"
75,integrating to clear differential,integrating to clear differential,,"I'm unsure if that's the correct term. I have an equation $v^2 = \frac{2ILB}{m}x$ and I need to find distance with respect to time (yes, physics.. but the silly math is what trips me up so I'm posting here) Here's my attempt $v^2 = [\frac{dx}{dt}]^2 = ...$ $\frac{dx}{dt} = \sqrt{\frac{2ILB}{m}x}$ $\frac{dx}{\sqrt{x}} = dt \sqrt{\frac{2ILB}{m}}$ $2\sqrt{x} = t \sqrt{\frac{2ILB}{m}}$ $x(t) = t^2 \frac{2ILB}{4m}$ My rule of thumb when I don't know if I'm doing something is ""well, if I'm not lying at any point how could I be wrong?"". I've finished calc3 and honestly never recall dealing with these outside of physics, so I can never know for sure if I'm lying at some point ! Anyways, I'm just wondering if I approached this right and if not, what exactly am I supposed to do?","I'm unsure if that's the correct term. I have an equation $v^2 = \frac{2ILB}{m}x$ and I need to find distance with respect to time (yes, physics.. but the silly math is what trips me up so I'm posting here) Here's my attempt $v^2 = [\frac{dx}{dt}]^2 = ...$ $\frac{dx}{dt} = \sqrt{\frac{2ILB}{m}x}$ $\frac{dx}{\sqrt{x}} = dt \sqrt{\frac{2ILB}{m}}$ $2\sqrt{x} = t \sqrt{\frac{2ILB}{m}}$ $x(t) = t^2 \frac{2ILB}{4m}$ My rule of thumb when I don't know if I'm doing something is ""well, if I'm not lying at any point how could I be wrong?"". I've finished calc3 and honestly never recall dealing with these outside of physics, so I can never know for sure if I'm lying at some point ! Anyways, I'm just wondering if I approached this right and if not, what exactly am I supposed to do?",,"['calculus', 'integration', 'derivatives']"
76,Is this function twice differentiable at $0$?,Is this function twice differentiable at ?,0,"I have a function $f(x)$: $$f(x)=\frac{\exp(-|x|)}{1-0.5|\tanh(2x)|}$$ If I try differentiating it in Mathematica (taking $|x|=(2\theta(x)-1)x$ where $\theta(x)$ is Heaviside step function), I get answer in terms of Heaviside functions. But looking at the derivative I can't really see any jump there, which is why I start thinking that it has a second derivative too. But differentiating once more, I get answer with Dirac functions, which don't disappear when evaluating the result at $x=0$. If I instead represent $|x|=\sqrt{x^2}$, then for the first derivative I get an expression which has $0$ as its limit as $x\to0$, and for second derivative the limit appears to be 1. So from this I could make the conclusion that $f(x)$ is twice differentiable at $x=0$. So the question: is this function differentiable at $x=0$? Is it twice differentiable?","I have a function $f(x)$: $$f(x)=\frac{\exp(-|x|)}{1-0.5|\tanh(2x)|}$$ If I try differentiating it in Mathematica (taking $|x|=(2\theta(x)-1)x$ where $\theta(x)$ is Heaviside step function), I get answer in terms of Heaviside functions. But looking at the derivative I can't really see any jump there, which is why I start thinking that it has a second derivative too. But differentiating once more, I get answer with Dirac functions, which don't disappear when evaluating the result at $x=0$. If I instead represent $|x|=\sqrt{x^2}$, then for the first derivative I get an expression which has $0$ as its limit as $x\to0$, and for second derivative the limit appears to be 1. So from this I could make the conclusion that $f(x)$ is twice differentiable at $x=0$. So the question: is this function differentiable at $x=0$? Is it twice differentiable?",,"['real-analysis', 'derivatives']"
77,"If $f(x)$ is 2x differentiable in $(a,b)$ & $f'(a)=f'(b)=0$, prove that, $\exists\xi $ in $(a,b)$ S.T. $|f''(\xi )|\leq\frac{4(f(b)-f(a))}{(b-a)^{2}}$","If  is 2x differentiable in  & , prove that,  in  S.T.","f(x) (a,b) f'(a)=f'(b)=0 \exists\xi  (a,b) |f''(\xi )|\leq\frac{4(f(b)-f(a))}{(b-a)^{2}}","Here is my argument (it doesn't feel 100% correct for some reason): By the mean value theorem, there exists $\xi_{1}$ in $(a,b)$ such that, $$f'(\xi_{1}) =  \frac{f(b)-f(a)}{b-a}$$ Since, $f'(a)=f'(b)=0$, then by the mean value theorem again, there exists $\xi_{2}$ in $(\xi_{1},b)$ such that, $$f''(\xi_{2})=\frac{f'(b)-f'(\xi_{1})}{b-\xi_{1}}=\frac{-(f(b) -f(a))}{(b-a)(b-\xi_{1})}$$ Since $\xi_{1}$ can be no lower than $a$, $$f''(\xi_{2}) \geq \frac{-(f(b) -f(a))}{(b-a)^{2}} \, or\,  f''(\xi_{2}) \leq \frac{f(b) -f(a)}{(b-a)^{2}} \leq \frac{4(f(b) -f(a))}{(b-a)^{2}}$$ $$Assuming\, f(b)\geq f(a)$$ So it follows,  $|f''(\xi_{2} )| \leq  \frac{4(f(b)-f(a))}{(b-a)^{2}}$?","Here is my argument (it doesn't feel 100% correct for some reason): By the mean value theorem, there exists $\xi_{1}$ in $(a,b)$ such that, $$f'(\xi_{1}) =  \frac{f(b)-f(a)}{b-a}$$ Since, $f'(a)=f'(b)=0$, then by the mean value theorem again, there exists $\xi_{2}$ in $(\xi_{1},b)$ such that, $$f''(\xi_{2})=\frac{f'(b)-f'(\xi_{1})}{b-\xi_{1}}=\frac{-(f(b) -f(a))}{(b-a)(b-\xi_{1})}$$ Since $\xi_{1}$ can be no lower than $a$, $$f''(\xi_{2}) \geq \frac{-(f(b) -f(a))}{(b-a)^{2}} \, or\,  f''(\xi_{2}) \leq \frac{f(b) -f(a)}{(b-a)^{2}} \leq \frac{4(f(b) -f(a))}{(b-a)^{2}}$$ $$Assuming\, f(b)\geq f(a)$$ So it follows,  $|f''(\xi_{2} )| \leq  \frac{4(f(b)-f(a))}{(b-a)^{2}}$?",,"['calculus', 'derivatives', 'proof-verification']"
78,Optimization Homework,Optimization Homework,,I need help with this math question: A farmer with 720 ft of fencing wants to enclose a rectangular area and then divide it into four pens with fencing parallel to one side of the rectangle. What is the largest possible total area of the four pens. This is what I have attempted so far. Perimeter = 2*L + 5*W = 720 Solve for W  W = 144 - 2/5*L Area of each pen = L/4*W  Plug in W  Area = L/4*144 - L/4*2/5*L  or is it [Length/4*144 -Lenght/4*2/5lenght) I have a feeling I am attempting this question incorrectly..,I need help with this math question: A farmer with 720 ft of fencing wants to enclose a rectangular area and then divide it into four pens with fencing parallel to one side of the rectangle. What is the largest possible total area of the four pens. This is what I have attempted so far. Perimeter = 2*L + 5*W = 720 Solve for W  W = 144 - 2/5*L Area of each pen = L/4*W  Plug in W  Area = L/4*144 - L/4*2/5*L  or is it [Length/4*144 -Lenght/4*2/5lenght) I have a feeling I am attempting this question incorrectly..,,"['calculus', 'optimization', 'derivatives']"
79,Derivative of inner product (taking limit inside),Derivative of inner product (taking limit inside),,"For each $x \in [a,b]$ let $A_x: H \to H$ be an operator on a Hilbert space. The inner product $(A_xu,v)_{H}$ can be thought of as a function from $[a,b] \to \mathbb{R}.$ I want to say that $(A_xu,v)_{H}$ is differentiable in the classical sense $$\lim_{h \to 0}\frac{(A_{x+h}u,v)_{H}-(A_xu,v)_{H}}{h}=\lim_{h \to 0}\frac{(A_{x+h}u-A_xu,v)_{H}}{h}=(\lim_{h \to 0}\frac{A_{x+h}u-A_xu}{h},v)_{H}=0$$ Does it make sense to write the above? In what sense then is the limit on the RHS (the one inside the inner product) taken??","For each $x \in [a,b]$ let $A_x: H \to H$ be an operator on a Hilbert space. The inner product $(A_xu,v)_{H}$ can be thought of as a function from $[a,b] \to \mathbb{R}.$ I want to say that $(A_xu,v)_{H}$ is differentiable in the classical sense $$\lim_{h \to 0}\frac{(A_{x+h}u,v)_{H}-(A_xu,v)_{H}}{h}=\lim_{h \to 0}\frac{(A_{x+h}u-A_xu,v)_{H}}{h}=(\lim_{h \to 0}\frac{A_{x+h}u-A_xu}{h},v)_{H}=0$$ Does it make sense to write the above? In what sense then is the limit on the RHS (the one inside the inner product) taken??",,"['derivatives', 'hilbert-spaces', 'inner-products']"
80,Using both Leibniz' notation and prime-notation for a derivative,Using both Leibniz' notation and prime-notation for a derivative,,"I am presented with the following task: ""Assume that the function $f(x)$ has the derivative $f'(x) = \frac{1}{x}$ and that $f$ is one-to-one. If $y = f^{-1}(x)$, show that $\frac{dy}{dx} = 1$. The solution given in the same textbook starts out with the following statement: $$x = f(y) \rightarrow f'(y)*\frac{dy}{dx} = 1$$ From this point, the proof is trivial, but I am confused by the notation given. Would $f'(y)*\frac{dy}{dx}$ translate to ""the derivative of the function $f(y)$ with respect to $x$? If so, could you elaborate?","I am presented with the following task: ""Assume that the function $f(x)$ has the derivative $f'(x) = \frac{1}{x}$ and that $f$ is one-to-one. If $y = f^{-1}(x)$, show that $\frac{dy}{dx} = 1$. The solution given in the same textbook starts out with the following statement: $$x = f(y) \rightarrow f'(y)*\frac{dy}{dx} = 1$$ From this point, the proof is trivial, but I am confused by the notation given. Would $f'(y)*\frac{dy}{dx}$ translate to ""the derivative of the function $f(y)$ with respect to $x$? If so, could you elaborate?",,"['derivatives', 'notation', 'inverse']"
81,How to show that $f(x)-x \times df(x)/dx ≥ 0$ when f(x) is concave?,How to show that  when f(x) is concave?,f(x)-x \times df(x)/dx ≥ 0,"How do you intuitively (perhaps even graphically) show that $f(x)-x {{df(x)}\over{dx}} ≥ 0$ when the function $f(x)$ is concave, e.g. when ${{df(x)}\over{dx}}>0$ and ${{d^2f(x)}\over{dx^2}} < 0$? Under the conditions that $x ≥ 0$.","How do you intuitively (perhaps even graphically) show that $f(x)-x {{df(x)}\over{dx}} ≥ 0$ when the function $f(x)$ is concave, e.g. when ${{df(x)}\over{dx}}>0$ and ${{d^2f(x)}\over{dx^2}} < 0$? Under the conditions that $x ≥ 0$.",,"['inequality', 'derivatives', 'graphing-functions']"
82,Determine if a function is a total derivative,Determine if a function is a total derivative,,"Lagrangian is defined up to addition of a total derivative of function of positions and time. Now suppose we have a function $f(x,\dot x,t)$. How can one show (check) that $$\not\exists g(x,t):\; f(x,\dot x,t)=\frac{\text{d}}{\text{d}t}g(x,t)$$ ?","Lagrangian is defined up to addition of a total derivative of function of positions and time. Now suppose we have a function $f(x,\dot x,t)$. How can one show (check) that $$\not\exists g(x,t):\; f(x,\dot x,t)=\frac{\text{d}}{\text{d}t}g(x,t)$$ ?",,['derivatives']
83,Maximum and minimum values for a curve,Maximum and minimum values for a curve,,"Considering the curve: $$ f(x) = x² $$ Does the curve $f$ have a maximum in the open interval $-1 < x < 1$? A minimum? I had a hard time interpreting this question. My first thought was: Yes, it has a maximum. The maximum value in the open interval $-1 < x < 1$ is $$ lim_{x -> 1} x² = lim_{x -> -1} x² = 1 $$ The minimum value is naturally $$ f(0) = 0² = 0 $$ However, I started wondering if thinking this way about the problem is wrong. Would it be more accurate to consider the problem as the whole curve $f$ having a maximum and a minimum in $-1 < x < 1$? Thus, arguing that it does NOT have a maximum in the interval (or any maximums at all, for that matter) but a minimum in $x = 0$, seeing as $$ f'(x) = 2x, 2x = 0 \rightarrow x = 0 $$ Which way would be the most natural way to interpret the question?","Considering the curve: $$ f(x) = x² $$ Does the curve $f$ have a maximum in the open interval $-1 < x < 1$? A minimum? I had a hard time interpreting this question. My first thought was: Yes, it has a maximum. The maximum value in the open interval $-1 < x < 1$ is $$ lim_{x -> 1} x² = lim_{x -> -1} x² = 1 $$ The minimum value is naturally $$ f(0) = 0² = 0 $$ However, I started wondering if thinking this way about the problem is wrong. Would it be more accurate to consider the problem as the whole curve $f$ having a maximum and a minimum in $-1 < x < 1$? Thus, arguing that it does NOT have a maximum in the interval (or any maximums at all, for that matter) but a minimum in $x = 0$, seeing as $$ f'(x) = 2x, 2x = 0 \rightarrow x = 0 $$ Which way would be the most natural way to interpret the question?",,"['calculus', 'limits', 'derivatives']"
84,When can one conclude that a sequence of uniformly bounded equicontinuous functions converges uniformly?,When can one conclude that a sequence of uniformly bounded equicontinuous functions converges uniformly?,,"Let  $~f_{n}: [0,1] \rightarrow \mathbb{R}$ be a sequence of smooth functions  that are uniformly bounded and equicontinuous. By Arzela Ascoli theorem we  know that a subsequence $\{ f_{n_k} \} $ converges uniformly. Is there is any additional condition under which one can say that the sequence  $\{ f_{n} \} $ converges uniformly? In my case, I have  sequence of  functions that are uniformly bounded and the derivatives $f_{n}^{\prime}$  are also uniformly bounded. By fundamnetal theorem of calculus this  implies the sequence is equicontinuous. Under what additional hypothesis  can one conclude this sequence converges uniformly? For example is this a sufficient criteria:  $$ f_{n+1}(x) \geq f_{n}(x) \qquad \forall ~~x, ~~n $$ ?","Let  $~f_{n}: [0,1] \rightarrow \mathbb{R}$ be a sequence of smooth functions  that are uniformly bounded and equicontinuous. By Arzela Ascoli theorem we  know that a subsequence $\{ f_{n_k} \} $ converges uniformly. Is there is any additional condition under which one can say that the sequence  $\{ f_{n} \} $ converges uniformly? In my case, I have  sequence of  functions that are uniformly bounded and the derivatives $f_{n}^{\prime}$  are also uniformly bounded. By fundamnetal theorem of calculus this  implies the sequence is equicontinuous. Under what additional hypothesis  can one conclude this sequence converges uniformly? For example is this a sufficient criteria:  $$ f_{n+1}(x) \geq f_{n}(x) \qquad \forall ~~x, ~~n $$ ?",,"['real-analysis', 'sequences-and-series', 'derivatives', 'convergence-divergence', 'uniform-convergence']"
85,"prove that there is $c \in (a,b)$ (about calculus)",prove that there is  (about calculus),"c \in (a,b)","Statement $f$ is not a constant function. $f'$ is continuous, differentiable in $(a, b)$. $f(a)=f(b)=0$ $n\in\mathbb{R}$ Prove it There is $c$ in $(a, b)$ such that $f'(c)+nf(c)=0$. I used the Rolle's theorem to prove it, but I didn't prove. Because of $+nf(c)$. +Please do not use DE.","Statement $f$ is not a constant function. $f'$ is continuous, differentiable in $(a, b)$. $f(a)=f(b)=0$ $n\in\mathbb{R}$ Prove it There is $c$ in $(a, b)$ such that $f'(c)+nf(c)=0$. I used the Rolle's theorem to prove it, but I didn't prove. Because of $+nf(c)$. +Please do not use DE.",,"['calculus', 'derivatives']"
86,"Time, Velocity and Acceleration","Time, Velocity and Acceleration",,"I'm working through some exercises and have again come across one that is giving me some trouble. The topic is calculating velocity and acceleration when time varies. Here it is: A body moves in such a way that the space described in the time $t$ from starting is given by $s$ = $t^n$, where $n$ is a constant. Find the value of $n$ when the velocity is doubled from the 5th to the 10th second; find it also when the velocity is numerically equal to the acceleration at the end of the 10th second. I understand the velocity of this to be $v$ = $nt^{n-1}$ and the acceleration to be $a$ = $(n - 1)nt^{n - 2}$. The other exercises in this chapter barely compare to this one. This is the last exercise of the chapter and as expected is the most difficult. Any help on this is truly appreciated.","I'm working through some exercises and have again come across one that is giving me some trouble. The topic is calculating velocity and acceleration when time varies. Here it is: A body moves in such a way that the space described in the time $t$ from starting is given by $s$ = $t^n$, where $n$ is a constant. Find the value of $n$ when the velocity is doubled from the 5th to the 10th second; find it also when the velocity is numerically equal to the acceleration at the end of the 10th second. I understand the velocity of this to be $v$ = $nt^{n-1}$ and the acceleration to be $a$ = $(n - 1)nt^{n - 2}$. The other exercises in this chapter barely compare to this one. This is the last exercise of the chapter and as expected is the most difficult. Any help on this is truly appreciated.",,"['calculus', 'derivatives', 'physics']"
87,A problem related to mean value theorem and taylor's formula,A problem related to mean value theorem and taylor's formula,,"I guess I need to use Taylor's formula and the mean value theorem. I have no idea except for them.  Note: honestly, this is not homework. I am studying by myself. Suppose that $f\colon\mathbf{R}^2\to\mathbf{R}$ is $\mathcal{C}^p$ on $B_r(x_0,y_0)$ for some $r>0$. Prove that, given $(x,y)\in B_r(x_0,y_0)$, there is a point $(c,d)$ on the line segment between $(x_0,y_0)$ and $(x,y)$ such that $$f(x,y)=f(x_0,y_0)+\sum_{k=1}^{p-1}\frac{1}{k!}\left(\sum_{j=0}^k\binom{k}{j}(x-x_0)^j(y-y_0)^{k-j}\frac{\partial^k f}{\partial x^k\partial y^{k-j}}(x_0,y_0)\right)\\ +\frac{1}{p!}\sum_{j=0}^p\binom{p}{j}(x-x_0)^j(y-y_0)^{p-j}\frac{\partial^p f}{\partial x^k\partial y^{p-j}}(c,d)$$","I guess I need to use Taylor's formula and the mean value theorem. I have no idea except for them.  Note: honestly, this is not homework. I am studying by myself. Suppose that $f\colon\mathbf{R}^2\to\mathbf{R}$ is $\mathcal{C}^p$ on $B_r(x_0,y_0)$ for some $r>0$. Prove that, given $(x,y)\in B_r(x_0,y_0)$, there is a point $(c,d)$ on the line segment between $(x_0,y_0)$ and $(x,y)$ such that $$f(x,y)=f(x_0,y_0)+\sum_{k=1}^{p-1}\frac{1}{k!}\left(\sum_{j=0}^k\binom{k}{j}(x-x_0)^j(y-y_0)^{k-j}\frac{\partial^k f}{\partial x^k\partial y^{k-j}}(x_0,y_0)\right)\\ +\frac{1}{p!}\sum_{j=0}^p\binom{p}{j}(x-x_0)^j(y-y_0)^{p-j}\frac{\partial^p f}{\partial x^k\partial y^{p-j}}(c,d)$$",,"['calculus', 'real-analysis', 'derivatives', 'taylor-expansion']"
88,"Is there a ""more correct"" way to differentiate $\frac{f(x)}{g(x)}$?","Is there a ""more correct"" way to differentiate ?",\frac{f(x)}{g(x)},"In calculus class, as a shortcut for $\dfrac{\mathrm d}{\mathrm dx} \dfrac{f(x)}{g(x)}$ I'd often just do $\dfrac{\mathrm d}{\mathrm dx} f(x) g(x)^{-1}$ and then use the multiplication rule. I'd do the same with integration. Is there any harm with this approach? I don't know of ant special case where perhaps 0 might mess up the equation.","In calculus class, as a shortcut for $\dfrac{\mathrm d}{\mathrm dx} \dfrac{f(x)}{g(x)}$ I'd often just do $\dfrac{\mathrm d}{\mathrm dx} f(x) g(x)^{-1}$ and then use the multiplication rule. I'd do the same with integration. Is there any harm with this approach? I don't know of ant special case where perhaps 0 might mess up the equation.",,"['calculus', 'integration', 'derivatives']"
89,Notation for function being differentiable at a certain point,Notation for function being differentiable at a certain point,,"This question describes a notation for a function $f(x)$ being (continuously) differentiable on some domain $A$. Often, I see the requirement that some function $f(x)$ be differentiable only (or rather, at least) at a certain point $\tilde{x}$. This is usually written in plain text (e.g. ""let $f: X \rightarrow Y$ be differentiable at $\tilde{x}$""). I was wondering whether there is some more formal notation for such (local) differentiability. Thanks in advance!","This question describes a notation for a function $f(x)$ being (continuously) differentiable on some domain $A$. Often, I see the requirement that some function $f(x)$ be differentiable only (or rather, at least) at a certain point $\tilde{x}$. This is usually written in plain text (e.g. ""let $f: X \rightarrow Y$ be differentiable at $\tilde{x}$""). I was wondering whether there is some more formal notation for such (local) differentiability. Thanks in advance!",,"['calculus', 'notation', 'derivatives']"
90,Find maximum value of $f(x)=2\cos 2x + 4 \sin x$ where $0 < x <\pi$,Find maximum value of  where,f(x)=2\cos 2x + 4 \sin x 0 < x <\pi,Find the maximum value of $f(x)$ where  \begin{equation}  f(x)=2\cos 2x + 4 \sin x \ \  \text{for} \ \ 0<x<\pi \end{equation},Find the maximum value of $f(x)$ where  \begin{equation}  f(x)=2\cos 2x + 4 \sin x \ \  \text{for} \ \ 0<x<\pi \end{equation},,"['trigonometry', 'derivatives']"
91,Derivative of order 16 - is there a method to do so?,Derivative of order 16 - is there a method to do so?,,"I have the following exercise: Find the $16^{\text{th}}$ derivative of $y$, (i.e. $y^{(16)}$), for $y = \sin x$. Is there any method to do so, or I simply have to differentiate the function $16$ times?","I have the following exercise: Find the $16^{\text{th}}$ derivative of $y$, (i.e. $y^{(16)}$), for $y = \sin x$. Is there any method to do so, or I simply have to differentiate the function $16$ times?",,"['calculus', 'derivatives']"
92,Find the minimum of $|a+\frac 2 {a-1}|$ where $|a|\leq2$.,Find the minimum of  where .,|a+\frac 2 {a-1}| |a|\leq2,"Find the minimum of $|a+\frac 2 {a-1}|$ where $|a|\leq2$. I tried using differentiation, but the absolute makes things troublesome... Please help. Thank you.","Find the minimum of $|a+\frac 2 {a-1}|$ where $|a|\leq2$. I tried using differentiation, but the absolute makes things troublesome... Please help. Thank you.",,"['optimization', 'derivatives']"
93,Applying the Mean Value Theorem to conclude a function has a zero,Applying the Mean Value Theorem to conclude a function has a zero,,"Consider the function $f$ given by $f(x)=(x-2)^4\cos(x^2-4x+4)$. Use the Mean Value Theorem to show that $f'$ has a zero on the interval on $[1,3]$. I notice that to do this we must show $f'(c)=0$ where $c$ is real number in the interval $[1,3]$. Now by the Mean Value Theorem, $$\frac{f(3)-f(1)}{3-1} =f'(c)\,.$$ Notice that $f'(c)$ is indeed $0$ on the left hand side.","Consider the function $f$ given by $f(x)=(x-2)^4\cos(x^2-4x+4)$. Use the Mean Value Theorem to show that $f'$ has a zero on the interval on $[1,3]$. I notice that to do this we must show $f'(c)=0$ where $c$ is real number in the interval $[1,3]$. Now by the Mean Value Theorem, $$\frac{f(3)-f(1)}{3-1} =f'(c)\,.$$ Notice that $f'(c)$ is indeed $0$ on the left hand side.",,"['calculus', 'derivatives', 'rolles-theorem']"
94,Help with $\arcsin(x)$ derivative and differentials.,Help with  derivative and differentials.,\arcsin(x),"I'm watching this video lecture http://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/video-lectures/lecture-11-chain-rule/ and I'm stuck at around 3:40, I can't seem to figure out what he is doing. He is showing how to derive $f(x)=\sin^{-1}(x)$. At some point he goes from the expression $\frac{dy}{dx}=\frac{1}{\cos(y)}$ to $\frac{dy}{dx}= \frac{1}{\sqrt{1-x^2}}$. Ok, I know that's the result, that's how I always did it, but I never actually derived it myself. So yeah, I'd like to know what he did in those last two steps, to get from the first expression to the second. I know it may (will) be something completely stupid and I'll say 'oh... facepalm ', but for some reason I can't figure out how he did it. I'm guessing the next logical step is to replace $y=\sin^{-1}(x)$, but then? I've never been in good terms with trigonometric functions and identities really, so I'd appreciate some enlightening. Thank you.","I'm watching this video lecture http://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/video-lectures/lecture-11-chain-rule/ and I'm stuck at around 3:40, I can't seem to figure out what he is doing. He is showing how to derive $f(x)=\sin^{-1}(x)$. At some point he goes from the expression $\frac{dy}{dx}=\frac{1}{\cos(y)}$ to $\frac{dy}{dx}= \frac{1}{\sqrt{1-x^2}}$. Ok, I know that's the result, that's how I always did it, but I never actually derived it myself. So yeah, I'd like to know what he did in those last two steps, to get from the first expression to the second. I know it may (will) be something completely stupid and I'll say 'oh... facepalm ', but for some reason I can't figure out how he did it. I'm guessing the next logical step is to replace $y=\sin^{-1}(x)$, but then? I've never been in good terms with trigonometric functions and identities really, so I'd appreciate some enlightening. Thank you.",,"['trigonometry', 'derivatives']"
95,"Consider the map $ f \colon \Bbb R^2 \rightarrow \Bbb R^2$ defined by $f(x,y)=(3x-2y+x^2,4x+5y+y^2).$",Consider the map  defined by," f \colon \Bbb R^2 \rightarrow \Bbb R^2 f(x,y)=(3x-2y+x^2,4x+5y+y^2).","I came across the following problem that says: Consider the map $f \colon \Bbb R^2 \rightarrow \Bbb R^2$ defined by $$f(x,y)=(3x-2y+x^2,4x+5y+y^2)$$ Then I have to determine whether the following statements are true or not? 1. $f$ is continuous at $(0,0)$ and all directional derivatives exist at $(0,0).$ 2. $f$ is differentiable at $(0,0)$ and the derivative $Df(0,0)$ is invertible. The problem is that I can not compute  $Df(0,0)$.Can someone provide me the formula by means of which I can compute it. With regards and thanks in advance for your time.","I came across the following problem that says: Consider the map $f \colon \Bbb R^2 \rightarrow \Bbb R^2$ defined by $$f(x,y)=(3x-2y+x^2,4x+5y+y^2)$$ Then I have to determine whether the following statements are true or not? 1. $f$ is continuous at $(0,0)$ and all directional derivatives exist at $(0,0).$ 2. $f$ is differentiable at $(0,0)$ and the derivative $Df(0,0)$ is invertible. The problem is that I can not compute  $Df(0,0)$.Can someone provide me the formula by means of which I can compute it. With regards and thanks in advance for your time.",,"['real-analysis', 'derivatives']"
96,Critical Points,Critical Points,,"I'm slightly confused as to how to solve for critical points, for example: $$f(x) = x^{3} + 3x^{2}-24x$$ You take the derivative of the function, $$f'(x) = 3x^{2}+6x-24$$ And then solve for x, $$3x^{2}+6x-24 = 0$$ $$3x^{2}+6x = 24$$ $$3x(x+2) = 24$$ And this is the part where I am confused, my textbook says that the critical points are -4, and 2. But I'm not understanding how these are gotten, or if there's an error in the textbook (it's known to have some mistakes in it), can anyone help clarify this? Thanks.","I'm slightly confused as to how to solve for critical points, for example: $$f(x) = x^{3} + 3x^{2}-24x$$ You take the derivative of the function, $$f'(x) = 3x^{2}+6x-24$$ And then solve for x, $$3x^{2}+6x-24 = 0$$ $$3x^{2}+6x = 24$$ $$3x(x+2) = 24$$ And this is the part where I am confused, my textbook says that the critical points are -4, and 2. But I'm not understanding how these are gotten, or if there's an error in the textbook (it's known to have some mistakes in it), can anyone help clarify this? Thanks.",,"['calculus', 'derivatives']"
97,Derivative of constant divided by equation,Derivative of constant divided by equation,,I'm having trouble figuring out how to derive $\frac{5}{4 + 3\cos{2x}}$. Using the $D\frac{f}{g} = \frac{gDf - fDg}{g^2}$ rule doesn't seem to work: it results in zero.,I'm having trouble figuring out how to derive $\frac{5}{4 + 3\cos{2x}}$. Using the $D\frac{f}{g} = \frac{gDf - fDg}{g^2}$ rule doesn't seem to work: it results in zero.,,['derivatives']
98,Vertical cusp vertical tangent question,Vertical cusp vertical tangent question,,"Can anyone help me solve the following two problems: Does the following function have a vertical  tangent or vertical cusp at $c=0$ $$f(x)=(3+x^{\frac{2}{5}})$$ I got for the derivative $$f'(x)=\frac{2}{5\sqrt[5]{x}^3}$$ Now I think this would be a cusp because as x approaches zero from the left and right f(x) approches infinity. My second question is with the function $f(x)=|{(x+8)^{1/3}}|$ . For my derivative I got $$f'(x)=\frac{1}{3}(x+8)^{\frac{-2}{3}},\;x > -8$$ $$f'(x)=\frac{-1}{3}(x+8)^{\frac{-2}{3}},\;x < -8$$ Would this not be a tangent because of the square expoent.",Can anyone help me solve the following two problems: Does the following function have a vertical  tangent or vertical cusp at I got for the derivative Now I think this would be a cusp because as x approaches zero from the left and right f(x) approches infinity. My second question is with the function . For my derivative I got Would this not be a tangent because of the square expoent.,"c=0 f(x)=(3+x^{\frac{2}{5}}) f'(x)=\frac{2}{5\sqrt[5]{x}^3} f(x)=|{(x+8)^{1/3}}| f'(x)=\frac{1}{3}(x+8)^{\frac{-2}{3}},\;x > -8 f'(x)=\frac{-1}{3}(x+8)^{\frac{-2}{3}},\;x < -8","['calculus', 'derivatives']"
99,find derivative of $e^{3\sqrt{x}}$ using chain rule.,find derivative of  using chain rule.,e^{3\sqrt{x}},im asked to find the derivative of this function using the chain rule. $$e^{3\sqrt{x}}$$ here are my steps. step 1 -  identify the inner and outer functions. therefore I identified outer function as  $e^x$ inner function as $3\sqrt{x}$ step 2-  i used derivative of outer function with respect to  inner times the derivative of inner function.  so the answer as i see it should be  $$e(3\sqrt{x})\left(\frac{3}{2}x^{-1/2}\right) $$ however the answer is $$e^{3\sqrt{x}}\left(\frac{3}{2}x^{-1/2}\right)$$ what I'm missing? I know the derivative of $e^x = e^x$ is that were i'm going wrong? Thanks in advance for any explanation clarification you guys can offer. Miguel,im asked to find the derivative of this function using the chain rule. $$e^{3\sqrt{x}}$$ here are my steps. step 1 -  identify the inner and outer functions. therefore I identified outer function as  $e^x$ inner function as $3\sqrt{x}$ step 2-  i used derivative of outer function with respect to  inner times the derivative of inner function.  so the answer as i see it should be  $$e(3\sqrt{x})\left(\frac{3}{2}x^{-1/2}\right) $$ however the answer is $$e^{3\sqrt{x}}\left(\frac{3}{2}x^{-1/2}\right)$$ what I'm missing? I know the derivative of $e^x = e^x$ is that were i'm going wrong? Thanks in advance for any explanation clarification you guys can offer. Miguel,,"['calculus', 'derivatives']"
