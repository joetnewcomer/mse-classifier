,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Transforming real matrix to integer matrix while preserving row and column sums,Transforming real matrix to integer matrix while preserving row and column sums,,"Let $A$ be an $m \times n$ matrix with real-valued entries such that the rows and columns sum up to integers. I wish to show that there exists a matrix $B$ , with integer entries, such that $A$ and $B$ share the same row and column sums, and $|A_{i,j} - B_{i,j}| < 1$ for all $i,j$ . I tried various methods but none came to fruition. For instance, I tried to induct on $m + n$ . The case where $m = 1$ or $n = 1$ is clearly trivial. The $2 \times 2$ case is also simple: We must have $A = \begin{pmatrix} a & r_1 - a \\ c_1 - a & r_2 - c_1 + a \end{pmatrix}$ , where $r_1,c_1,r_2 \in \mathbb{Z}$ are integers. Then $B = \begin{pmatrix} \lfloor{a}\rfloor & r_1 - \lfloor{a}\rfloor \\ c_1 - \lfloor{a}\rfloor & r_2 - c_1 + \lfloor{a}\rfloor  \end{pmatrix}$ does the trick. Given an $(m + 1) \times n$ matrix $A$ , I considered: $$ A_{i,j}' :=  \begin{cases} A_{i,j}, &\text{if $i < m$} \\ A_{m,j} + A_{m+1,j}, &\text{if $i = m$} \end{cases} $$ Then $A'$ is an $m \times n$ matrix, so by induction hypothesis, we may approximate it with some integer-valued $m \times n$ matrix $B$ . However, I'm not sure how to proceed from there onwards.","Let be an matrix with real-valued entries such that the rows and columns sum up to integers. I wish to show that there exists a matrix , with integer entries, such that and share the same row and column sums, and for all . I tried various methods but none came to fruition. For instance, I tried to induct on . The case where or is clearly trivial. The case is also simple: We must have , where are integers. Then does the trick. Given an matrix , I considered: Then is an matrix, so by induction hypothesis, we may approximate it with some integer-valued matrix . However, I'm not sure how to proceed from there onwards.","A m \times n B A B |A_{i,j} - B_{i,j}| < 1 i,j m + n m = 1 n = 1 2 \times 2 A = \begin{pmatrix} a & r_1 - a \\ c_1 - a & r_2 - c_1 + a \end{pmatrix} r_1,c_1,r_2 \in \mathbb{Z} B = \begin{pmatrix} \lfloor{a}\rfloor & r_1 - \lfloor{a}\rfloor \\ c_1 - \lfloor{a}\rfloor & r_2 - c_1 + \lfloor{a}\rfloor  \end{pmatrix} (m + 1) \times n A 
A_{i,j}' := 
\begin{cases}
A_{i,j}, &\text{if i < m} \\
A_{m,j} + A_{m+1,j}, &\text{if i = m}
\end{cases}
 A' m \times n m \times n B","['linear-algebra', 'combinatorics', 'matrices']"
1,Why an LTI system with some zero eigenvalues still stable?,Why an LTI system with some zero eigenvalues still stable?,,"The textbook says an LTI system $\dot x=Ax$ is stable if and only if the eigenvalues of $A$ have the strictly negative real part. However, I found a counterexample. If $$A= \begin{bmatrix}-3 &  -1 &  -1\\      1 &  -0.5 &  -0.5\\      1 &  -0.5  & -0.5\end{bmatrix}$$ The state response of this system is convergent, and $x_2 = x_3$ . The system is stable even if an eigenvalue of $A$ is $0$ . Am I wrong?","The textbook says an LTI system is stable if and only if the eigenvalues of have the strictly negative real part. However, I found a counterexample. If The state response of this system is convergent, and . The system is stable even if an eigenvalue of is . Am I wrong?","\dot x=Ax A A= \begin{bmatrix}-3 &  -1 &  -1\\
     1 &  -0.5 &  -0.5\\
     1 &  -0.5  & -0.5\end{bmatrix} x_2 = x_3 A 0","['linear-algebra', 'dynamical-systems', 'control-theory', 'stability-theory']"
2,Proof of this Linear Algebra Identity,Proof of this Linear Algebra Identity,,"Let $x, w \in \mathbb{R}^n$ How to show that: $(w^tx)x = (xx^t)w$ $\hspace{1em}$ ( $t$ denotes transpose) Attempt at proof: $$ \begin{aligned} \alpha &= (w^tx)x\\ \alpha^t &= x^t(w^tx)^t = x^t(x^tw)\\ (\alpha^t)^t &= (x^t)^t(x^tw) \hspace{1em} (\text{treating $x^tw$ as a scalar here})\\ \alpha &= x(x^tw) = (xx^t)w \hspace{1em} (\text{associativity}) \end{aligned} $$ The above proof looks correct to me, but I'm still unsure as the last two steps feel like an ""hack"".","Let How to show that: ( denotes transpose) Attempt at proof: The above proof looks correct to me, but I'm still unsure as the last two steps feel like an ""hack"".","x, w \in \mathbb{R}^n (w^tx)x = (xx^t)w \hspace{1em} t 
\begin{aligned}
\alpha &= (w^tx)x\\
\alpha^t &= x^t(w^tx)^t = x^t(x^tw)\\
(\alpha^t)^t &= (x^t)^t(x^tw) \hspace{1em} (\text{treating x^tw as a scalar here})\\
\alpha &= x(x^tw) = (xx^t)w \hspace{1em} (\text{associativity})
\end{aligned}
","['linear-algebra', 'matrices', 'vectors']"
3,how many $2\times2$ matrices exist with this condition?,how many  matrices exist with this condition?,2\times2,"Let $p=10007$ which is prime. I want to find the number of matrices X of $2\times2$ dimension with elements from $\mathbb{Z}_p$ for which $X^2\equiv I$ (mod $p$ ). Where $I$ - identity matrix. How can I solve this problem? I tried to write down the square of matrix so that I get the system of congruences, but I can't figure out what to do after that, and I don't think this is the right way to solve this.","Let which is prime. I want to find the number of matrices X of dimension with elements from for which (mod ). Where - identity matrix. How can I solve this problem? I tried to write down the square of matrix so that I get the system of congruences, but I can't figure out what to do after that, and I don't think this is the right way to solve this.",p=10007 2\times2 \mathbb{Z}_p X^2\equiv I p I,"['linear-algebra', 'modular-arithmetic']"
4,"Is a real matrix that is both normal and diagonalizable symmetric? If so, is there a proof of this not using the spectral theorem?","Is a real matrix that is both normal and diagonalizable symmetric? If so, is there a proof of this not using the spectral theorem?",,"Given a quadratic real matrix $A$ for which we know it is diagonalizable ( $D = P^{-1}AP$ for a diagonal matrix $D$ ) and that it is normal ( $AA^T = A^TA$ ), is it true that $A$ is symmetric? By the spectral theorem over $\mathbb{C}$ , $A$ is orthogonally diagonalizable (say via some unitary matrix $Q$ ). It then seems to me that from this we should get that the eigenspaces are orthogonal and hence there must also exist a real orthonormal basis of eigenvectors. Is this last correct? From this it would then follow by the real version of the spectral theorem that $A$ is symmetric. If it is, is there a way to prove this statement directly, without appealing to the spectral theorem? This would give a nice justification/explanation for the difference in the two spectral theorems over $\mathbb{C}$ and $\mathbb{R}$ relating it directly to whether the matrix is diagonalizable in the first place.","Given a quadratic real matrix for which we know it is diagonalizable ( for a diagonal matrix ) and that it is normal ( ), is it true that is symmetric? By the spectral theorem over , is orthogonally diagonalizable (say via some unitary matrix ). It then seems to me that from this we should get that the eigenspaces are orthogonal and hence there must also exist a real orthonormal basis of eigenvectors. Is this last correct? From this it would then follow by the real version of the spectral theorem that is symmetric. If it is, is there a way to prove this statement directly, without appealing to the spectral theorem? This would give a nice justification/explanation for the difference in the two spectral theorems over and relating it directly to whether the matrix is diagonalizable in the first place.",A D = P^{-1}AP D AA^T = A^TA A \mathbb{C} A Q A \mathbb{C} \mathbb{R},"['linear-algebra', 'diagonalization', 'symmetric-matrices']"
5,Proof that strong convexity implies Polyak-Lojasiewicz inequality is satisfied,Proof that strong convexity implies Polyak-Lojasiewicz inequality is satisfied,,"I am attempting to find a proof of strong-convexity implying the Polyak- Lojasiewicz (PL) inequality is satisfied. As I understand it, strong convexity means that: $$ \textbf{H} f \succcurlyeq \mu I$$ for some $\mu>0$ , where $\textbf{H}f$ is the Hessian matrix of $f$ and $I$ is the identity matrix. The PL inequality states that: $$\frac{1}{2}\|\nabla f(x)\|^2 \geq \mu(f(x) - f(x^*))$$ where $x^*$ is the value of $x$ for which $f(x)$ is minimized. This paper http://www.optimization-online.org/DB_FILE/2016/08/5590.pdf states that if a function is strongly convex with constant $\mu$ , then it must satisfy the PL inequality with the same constant (page 4, section 2.3, first sentence). I found a source that says this can be done by minimizing the following inequality: $$f(y)\ge f(x)+\nabla f(x)^T(y-x)+\frac{\mu}{2}\lVert y-x \rVert^2 \tag{1}$$ in terms of $y$ to get $$f(x^*) \ge f(x)-\frac{1}{2\mu}\|\nabla f(x)\|^2.$$ I don't understand this. How exactly is the inequality being minimized in terms of $y$ here? How do I go from strong convexity to equation (1) to this result?","I am attempting to find a proof of strong-convexity implying the Polyak- Lojasiewicz (PL) inequality is satisfied. As I understand it, strong convexity means that: for some , where is the Hessian matrix of and is the identity matrix. The PL inequality states that: where is the value of for which is minimized. This paper http://www.optimization-online.org/DB_FILE/2016/08/5590.pdf states that if a function is strongly convex with constant , then it must satisfy the PL inequality with the same constant (page 4, section 2.3, first sentence). I found a source that says this can be done by minimizing the following inequality: in terms of to get I don't understand this. How exactly is the inequality being minimized in terms of here? How do I go from strong convexity to equation (1) to this result?", \textbf{H} f \succcurlyeq \mu I \mu>0 \textbf{H}f f I \frac{1}{2}\|\nabla f(x)\|^2 \geq \mu(f(x) - f(x^*)) x^* x f(x) \mu f(y)\ge f(x)+\nabla f(x)^T(y-x)+\frac{\mu}{2}\lVert y-x \rVert^2 \tag{1} y f(x^*) \ge f(x)-\frac{1}{2\mu}\|\nabla f(x)\|^2. y,"['linear-algebra', 'multivariable-calculus', 'optimization', 'convex-analysis', 'convex-optimization']"
6,An open set invariant under a linear map implies it is an isometry or of finite order?,An open set invariant under a linear map implies it is an isometry or of finite order?,,"Let $U \subseteq \mathbb R^2$ be an open, bounded, connected subset. Let $A \in \text{SL}_2$ ( $A$ is an invertible $2 \times 2$ matrix with determinant $1$ ) and suppose that $AU = U$ . Must $A$ be either orthogonal or of finite order? If we assume that $0 \in U$ , then I can prove that $A$ is diagonalizable (over $\mathbb C$ ), with all eigenvalues of modulus $1$ . I am not sure if this helps though. Indeed, we can assume that $B_r(0) \subseteq U \subseteq B_{R}(0)$ . Thus for any $x \in B_r(0)$ and for any $k \in \mathbb{Z}$ , since $A^k U \subseteq U$ , $|A^k x| \le R$ . This implies that all orbits of $A$ are bounded, i.e. $\sup_{k\in\mathbb{Z}}\|A^k x\|<+\infty$ for any $x\in \mathbb{R}^n$ , which implies the required assertion about diagonalizablilty .","Let be an open, bounded, connected subset. Let ( is an invertible matrix with determinant ) and suppose that . Must be either orthogonal or of finite order? If we assume that , then I can prove that is diagonalizable (over ), with all eigenvalues of modulus . I am not sure if this helps though. Indeed, we can assume that . Thus for any and for any , since , . This implies that all orbits of are bounded, i.e. for any , which implies the required assertion about diagonalizablilty .",U \subseteq \mathbb R^2 A \in \text{SL}_2 A 2 \times 2 1 AU = U A 0 \in U A \mathbb C 1 B_r(0) \subseteq U \subseteq B_{R}(0) x \in B_r(0) k \in \mathbb{Z} A^k U \subseteq U |A^k x| \le R A \sup_{k\in\mathbb{Z}}\|A^k x\|<+\infty x\in \mathbb{R}^n,"['linear-algebra', 'matrices', 'geometry', 'matrix-calculus', 'orthogonal-matrices']"
7,Infinite dimensional vector space has almost complex structure if and only if it is 'even-dimensional'?,Infinite dimensional vector space has almost complex structure if and only if it is 'even-dimensional'?,,"I started studying the book of Daniel Huybrechts, Complex Geometry An Introduction. I tried studying backwards as much as possible, but I have been stuck on the concepts of almost complex structures and complexification . I have studied several books and articles on the matter including ones by Keith Conrad , Jordan Bell , Gregory W. Moore , Steven Roman , Suetin, Kostrikin and Mainin , Gauthier I have several questions on the concepts of almost complex structures and complexification. Here is one: I understand for a finite dimensional $\mathbb R-$ vector space $V=(V,\text{Add}_V: V^2 \to V,s_V: \mathbb R \times V \to V)$ , the following are equivalent $\dim V$ even $V$ has an almost complex structure $J: V \to V$ $V$ has a complex structure $s_V^{\#}: \mathbb C \times V \to V$ that agrees with its real structure: $s_V^{\#} (r,v)=s_V(r,v)$ , for any $r \in \mathbb R$ and $v \in V$ if and only if $V \cong \mathbb R^{2n} \cong (\mathbb R^{n})^2$ for some positive integer $n$ (that turns out to be half of $\dim V$ ) if and only if $V \cong$ (maybe even $=$ ) $W^2=W \bigoplus W$ for some $\mathbb R-$ vector space $W$ . The last condition makes me think that the property 'even-dimensional' for finite-dimensional $V$ is generalised by the property ' $V \cong W^2$ for some $\mathbb R-$ vector space $W$ ' for finite or infinite dimensional $V$ . Question: For $V$ finite or infinite dimensional $\mathbb R-$ vector space, are the following equivalent? $V$ has an almost complex structure $J: V \to V$ Externally, $V \cong$ (maybe even $=$ ) $W^2=W \bigoplus W$ for some $\mathbb R-$ vector space $W$ Internally, $V=S \bigoplus U$ for some $\mathbb R-$ vector subspaces $S$ and $U$ of $V$ with $S \cong U$ (and $S \cap U = \{0_V\}$ )","I started studying the book of Daniel Huybrechts, Complex Geometry An Introduction. I tried studying backwards as much as possible, but I have been stuck on the concepts of almost complex structures and complexification . I have studied several books and articles on the matter including ones by Keith Conrad , Jordan Bell , Gregory W. Moore , Steven Roman , Suetin, Kostrikin and Mainin , Gauthier I have several questions on the concepts of almost complex structures and complexification. Here is one: I understand for a finite dimensional vector space , the following are equivalent even has an almost complex structure has a complex structure that agrees with its real structure: , for any and if and only if for some positive integer (that turns out to be half of ) if and only if (maybe even ) for some vector space . The last condition makes me think that the property 'even-dimensional' for finite-dimensional is generalised by the property ' for some vector space ' for finite or infinite dimensional . Question: For finite or infinite dimensional vector space, are the following equivalent? has an almost complex structure Externally, (maybe even ) for some vector space Internally, for some vector subspaces and of with (and )","\mathbb R- V=(V,\text{Add}_V: V^2 \to V,s_V: \mathbb R \times V \to V) \dim V V J: V \to V V s_V^{\#}: \mathbb C \times V \to V s_V^{\#} (r,v)=s_V(r,v) r \in \mathbb R v \in V V \cong \mathbb R^{2n} \cong (\mathbb R^{n})^2 n \dim V V \cong = W^2=W \bigoplus W \mathbb R- W V V \cong W^2 \mathbb R- W V V \mathbb R- V J: V \to V V \cong = W^2=W \bigoplus W \mathbb R- W V=S \bigoplus U \mathbb R- S U V S \cong U S \cap U = \{0_V\}","['linear-algebra', 'abstract-algebra', 'complex-analysis', 'complex-geometry', 'almost-complex']"
8,Does this condition imply isomorphism?,Does this condition imply isomorphism?,,"Let $k$ be a field and let $X$ and $Y$ be objects of a category $C$ enriched over $k$ -vector spaces. Composition gives us a linear map $\hom(X,Y) \otimes \hom(Y,X) \to \hom(X,X) \oplus \hom(Y,Y)$ , that sends an elementary tensor $f\otimes g$ to $(g\circ f, f \circ g)$ . Assume that $(id_X,id_Y)$ is in the image of this map. Is it then true that $X$ and $Y$ are isomorphic? I tried finding a counterexample when $C$ is the category of vector spaces, but couldn't find one. I also tried proving it by taking a sum of elementary tensors in the preimage of $(id_X,id_Y)$ and trying to construct an elementary tensor out of them. I didn't succeed in doing this either, even in the case of a sum of two elementary tensors. Counterexamples or proofs (maybe with more hypothesis) are much appreciated.","Let be a field and let and be objects of a category enriched over -vector spaces. Composition gives us a linear map , that sends an elementary tensor to . Assume that is in the image of this map. Is it then true that and are isomorphic? I tried finding a counterexample when is the category of vector spaces, but couldn't find one. I also tried proving it by taking a sum of elementary tensors in the preimage of and trying to construct an elementary tensor out of them. I didn't succeed in doing this either, even in the case of a sum of two elementary tensors. Counterexamples or proofs (maybe with more hypothesis) are much appreciated.","k X Y C k \hom(X,Y) \otimes \hom(Y,X) \to \hom(X,X) \oplus \hom(Y,Y) f\otimes g (g\circ f, f \circ g) (id_X,id_Y) X Y C (id_X,id_Y)","['linear-algebra', 'enriched-category-theory']"
9,"Over $\mathbb{F}_2$, find a $5\times 5$ matrix of order 31.","Over , find a  matrix of order 31.",\mathbb{F}_2 5\times 5,"So I found that $x^5+x+1$ is irreducible over $\mathbb{F}_2$ and produced the quotient, a field of order $32$ . Now, I need to find a $5\times 5$ matrix of order 31 over $\mathbb{F}_2$ . I don't see the direction to take here. Could someone point me in the right direction?","So I found that is irreducible over and produced the quotient, a field of order . Now, I need to find a matrix of order 31 over . I don't see the direction to take here. Could someone point me in the right direction?",x^5+x+1 \mathbb{F}_2 32 5\times 5 \mathbb{F}_2,"['linear-algebra', 'finite-fields']"
10,Does $\det(A+xB)$ have a nice form for $3 \times 3$ matrices?,Does  have a nice form for  matrices?,\det(A+xB) 3 \times 3,"I know that if $A,B \in M_2(\mathbb{C})$ , then $$\det(A+xB)=x^2 \det B +x(\operatorname{Tr}A\cdot \operatorname{Tr}B-\operatorname{Tr}(AB))+\det A.$$ What if $A,B \in M_3(\mathbb{C})$ ? Can we find a similar relation for $\det(A+xB)$ ?","I know that if , then What if ? Can we find a similar relation for ?","A,B \in M_2(\mathbb{C}) \det(A+xB)=x^2 \det B +x(\operatorname{Tr}A\cdot \operatorname{Tr}B-\operatorname{Tr}(AB))+\det A. A,B \in M_3(\mathbb{C}) \det(A+xB)","['linear-algebra', 'matrices', 'determinant']"
11,How many non-diagonalizable $2\times 2$ matrices are there with all entries single-digit strictly positive integers?,How many non-diagonalizable  matrices are there with all entries single-digit strictly positive integers?,2\times 2,"I'd like to know how many non-diagonalizable size 2 matrices there are with integer coefficient between 1 and 9 . I built a python program which counts the non-diagonalizable matrices with such coefficients: from sympy import *  count = 0 for a in range(1,10):     for b in range(1,10):         for c in range(1,10):             for d in range(1,10):                 M = Matrix([[a,b],[c,d]])                 if not M.is_diagonalizable():                     pprint(M)                     count+=1 print(""Number of non-diagonalizable matrices :"", count) The output was : Number of non-diagonalizable matrices : 0 I wonder if there is a problem with my program or if it's true, that all the size 2 matrices with integer coefficient between 1 and 9 are diagonalizable . Please help me.","I'd like to know how many non-diagonalizable size 2 matrices there are with integer coefficient between 1 and 9 . I built a python program which counts the non-diagonalizable matrices with such coefficients: from sympy import *  count = 0 for a in range(1,10):     for b in range(1,10):         for c in range(1,10):             for d in range(1,10):                 M = Matrix([[a,b],[c,d]])                 if not M.is_diagonalizable():                     pprint(M)                     count+=1 print(""Number of non-diagonalizable matrices :"", count) The output was : Number of non-diagonalizable matrices : 0 I wonder if there is a problem with my program or if it's true, that all the size 2 matrices with integer coefficient between 1 and 9 are diagonalizable . Please help me.",,"['linear-algebra', 'matrices', 'diagonalization', 'python']"
12,"Find matrices that commute with $\operatorname{Diag}(1,1,-1)$.",Find matrices that commute with .,"\operatorname{Diag}(1,1,-1)","Let, $A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1& 0 \\ 0 & 0 & -1 \end{bmatrix}$ . Then find $ S= \{B \in M_{3\times3}(\Bbb R) :AB=BA\}$ . My attempt: Bare computation yielded $$\left\{B\in M_{3\times3}(\Bbb R)\,\middle|\,B = \begin{bmatrix} c_1 & c_2 & 0 \\ c_3 & c_4& 0 \\ 0 & 0 & c_5 \end{bmatrix} , c_1,c_2,c_3,c_4,c_5 \in \Bbb R\right\} \subseteq S$$ but I am having trouble showing the reverse inclusion (if it is true!). I was also tempted to use eigenvalues and eigenspaces to reach some conclusion by looking at the diagonal form of $A$ (displaying its eigenvalues), but can't get my way through! Thanks in advance for help.","Let, . Then find . My attempt: Bare computation yielded but I am having trouble showing the reverse inclusion (if it is true!). I was also tempted to use eigenvalues and eigenspaces to reach some conclusion by looking at the diagonal form of (displaying its eigenvalues), but can't get my way through! Thanks in advance for help.","A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1& 0 \\ 0 & 0 & -1 \end{bmatrix}  S= \{B \in M_{3\times3}(\Bbb R) :AB=BA\} \left\{B\in M_{3\times3}(\Bbb R)\,\middle|\,B = \begin{bmatrix} c_1 & c_2 & 0 \\ c_3 & c_4& 0 \\ 0 & 0 & c_5 \end{bmatrix} , c_1,c_2,c_3,c_4,c_5 \in \Bbb R\right\} \subseteq S A","['linear-algebra', 'matrices']"
13,Why doesn't this example of basis change work?,Why doesn't this example of basis change work?,,"I'm learning about the change of basis in linear algebra, and trying to come up with an example to understand it. But somehow my example below doesn't make sense. Let $B_1 = ((1,0),(0,1))$ be the standard basis in $R^2$, and $v = (2,3)$ be a vector. Let $B_2$ be another basis of $R^2$, $B_2 = ((1,-1),(1,1))$. My understanding is that, in this system, the same $v$ above has the coordinate $v = (-1/2, 5/2)$. Let $T: R^2 \rightarrow R^2$, $T(x,y) = (x+2y, 3x - 2y)$. With $B_1$,  $M(T) =  \begin{bmatrix}     1 & 2 \\     3 & -2\\     \end{bmatrix}$,  and  $T(2,3) =  M(T).v =  \begin{bmatrix}     1 & 2 \\     3 & -2\\     \end{bmatrix} .\begin{bmatrix}     2 \\     3\\     \end{bmatrix}  =\begin{bmatrix}     8\\     0\\     \end{bmatrix}$ With $B_2$, $M(T) = \begin{bmatrix}     -1 & 3 \\     5 & 1\\     \end{bmatrix}$,  but $T(-1/2,5/2) = (9/2,-13/2)$, and  $M(T).v =  \begin{bmatrix}     -1 & 3 \\     5 & 1\\     \end{bmatrix} .\begin{bmatrix}     -1/2 \\     5/2\\     \end{bmatrix} =\begin{bmatrix}     8\\     0\\     \end{bmatrix}$ I don't understand why in the calculations using $B_2$, $T(-1/2,5/2)$ is not equal to $M(T).v$, and why $M(T).v=\begin{bmatrix}     8\\     0\\     \end{bmatrix}$. I was expecting that $M(T).v$ would give me something different, because we're using another basis. This ""something"" could then be ""converted"" back to $\begin{bmatrix}     8\\     0\\     \end{bmatrix}$ in the standard basis.","I'm learning about the change of basis in linear algebra, and trying to come up with an example to understand it. But somehow my example below doesn't make sense. Let $B_1 = ((1,0),(0,1))$ be the standard basis in $R^2$, and $v = (2,3)$ be a vector. Let $B_2$ be another basis of $R^2$, $B_2 = ((1,-1),(1,1))$. My understanding is that, in this system, the same $v$ above has the coordinate $v = (-1/2, 5/2)$. Let $T: R^2 \rightarrow R^2$, $T(x,y) = (x+2y, 3x - 2y)$. With $B_1$,  $M(T) =  \begin{bmatrix}     1 & 2 \\     3 & -2\\     \end{bmatrix}$,  and  $T(2,3) =  M(T).v =  \begin{bmatrix}     1 & 2 \\     3 & -2\\     \end{bmatrix} .\begin{bmatrix}     2 \\     3\\     \end{bmatrix}  =\begin{bmatrix}     8\\     0\\     \end{bmatrix}$ With $B_2$, $M(T) = \begin{bmatrix}     -1 & 3 \\     5 & 1\\     \end{bmatrix}$,  but $T(-1/2,5/2) = (9/2,-13/2)$, and  $M(T).v =  \begin{bmatrix}     -1 & 3 \\     5 & 1\\     \end{bmatrix} .\begin{bmatrix}     -1/2 \\     5/2\\     \end{bmatrix} =\begin{bmatrix}     8\\     0\\     \end{bmatrix}$ I don't understand why in the calculations using $B_2$, $T(-1/2,5/2)$ is not equal to $M(T).v$, and why $M(T).v=\begin{bmatrix}     8\\     0\\     \end{bmatrix}$. I was expecting that $M(T).v$ would give me something different, because we're using another basis. This ""something"" could then be ""converted"" back to $\begin{bmatrix}     8\\     0\\     \end{bmatrix}$ in the standard basis.",,[]
14,Skeptical about an Elementry point concerning polynomials of linear operators,Skeptical about an Elementry point concerning polynomials of linear operators,,"Consider the following scenario. We have a linear operator $p(T) = c_0I+c_1T+c_2T^2+\cdot\cdot\cdot+c_mT^m$ over the finite dimensional vector space  $V$ such that $p(T)v = 0$ for some non-zero $v\in V$. Now lets say that we are able to factorize the aforementioned polynomial to yield $p(T) = \beta(T-\lambda_1I)(T-\lambda_2I)\cdot\cdot\cdot(T-\lambda_mI)$ where $\beta\neq 0$ consequently $\beta(T-\lambda_1I)(T-\lambda_2I)\cdot\cdot\cdot(T-\lambda_mI)v = 0$ does this then imply that $(T- \lambda_jI)v = 0$ for at least one $j\in\{1,2,....,n\}$ and if not why not? I know this is an elementrary point but i seem to be having some trouble understanding it?","Consider the following scenario. We have a linear operator $p(T) = c_0I+c_1T+c_2T^2+\cdot\cdot\cdot+c_mT^m$ over the finite dimensional vector space  $V$ such that $p(T)v = 0$ for some non-zero $v\in V$. Now lets say that we are able to factorize the aforementioned polynomial to yield $p(T) = \beta(T-\lambda_1I)(T-\lambda_2I)\cdot\cdot\cdot(T-\lambda_mI)$ where $\beta\neq 0$ consequently $\beta(T-\lambda_1I)(T-\lambda_2I)\cdot\cdot\cdot(T-\lambda_mI)v = 0$ does this then imply that $(T- \lambda_jI)v = 0$ for at least one $j\in\{1,2,....,n\}$ and if not why not? I know this is an elementrary point but i seem to be having some trouble understanding it?",,"['linear-algebra', 'polynomials', 'eigenvalues-eigenvectors']"
15,Change of basis for linear transformation - Linear algebra,Change of basis for linear transformation - Linear algebra,,"So I'm having a lot of difficulties with change of basis. Watched tons of tutorials on youtube but they only seem to confuse me more. Let $T: \mathbb{R^2} \to \mathbb{R^2}$ be defined by $T(a,b) = (a +  2b, 3a - b)$ . Let $\mathcal{B} = \{(1,1),(1,0)\}$ and $\mathcal{C} = > \{(4,7),(4,8)\}$ . Find $[T]_\mathcal{B}$ and $[T]_\mathcal{C}$ and show that $[T]_\mathcal{C} = Q^{-1} \cdot [T]_\mathcal{B}\cdot Q$ for some invertible matrix $Q$ . So I've done some thinking, and the matrix $Q$ might be the matrix that goes from basis $c$ to $b$ ? and then if I invert that one I get matrix $Q^{-1}$ ? I just have 0 idea where to start to problem from, like what's the first matrix that I have to work with? Do I start with the basic basis in $R^2$ that is $\{(1,0),(0,1)\}$ and apply the transformation to it?","So I'm having a lot of difficulties with change of basis. Watched tons of tutorials on youtube but they only seem to confuse me more. Let be defined by . Let and . Find and and show that for some invertible matrix . So I've done some thinking, and the matrix might be the matrix that goes from basis to ? and then if I invert that one I get matrix ? I just have 0 idea where to start to problem from, like what's the first matrix that I have to work with? Do I start with the basic basis in that is and apply the transformation to it?","T: \mathbb{R^2} \to \mathbb{R^2} T(a,b) = (a +
 2b, 3a - b) \mathcal{B} = \{(1,1),(1,0)\} \mathcal{C} =
> \{(4,7),(4,8)\} [T]_\mathcal{B} [T]_\mathcal{C} [T]_\mathcal{C} = Q^{-1} \cdot [T]_\mathcal{B}\cdot Q Q Q c b Q^{-1} R^2 \{(1,0),(0,1)\}","['linear-algebra', 'linear-transformations', 'change-of-basis']"
16,Maximizing the trace in an elegant way,Maximizing the trace in an elegant way,,"Suppose $\Lambda = {\rm diag}(\lambda_1,\cdots,\lambda_n)$, $\Sigma = {\rm diag}(\sigma_1,\cdots,\sigma_n)$, and we have $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0$ and $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_n \geq 0$. I want to prove that $${\rm max}[ {\rm Tr}(O^{\rm T}\Lambda O \Sigma)] = \sum_{i=1}^n \lambda_i \sigma_i,$$ where $OO^{\rm T} = I$. One approach is to maximize the function $$f(o_{ij}) = \sum_{i,j} \lambda_i (o_{ij})^2 \sigma_j$$ under the constraint $\sum_{k} o_{ik}o_{jk} = \delta_{ij}$ by Lagrange multiplier method. I can prove the statement in this way but the whole proof is lengthy and lack of the flavor of linear algebra. Since the statement ""must be true"" intuitively, I want to know if there is an elegant way to prove it. A possible way may be mathematical induction but I failed to make it. Any help is appreciated.","Suppose $\Lambda = {\rm diag}(\lambda_1,\cdots,\lambda_n)$, $\Sigma = {\rm diag}(\sigma_1,\cdots,\sigma_n)$, and we have $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0$ and $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_n \geq 0$. I want to prove that $${\rm max}[ {\rm Tr}(O^{\rm T}\Lambda O \Sigma)] = \sum_{i=1}^n \lambda_i \sigma_i,$$ where $OO^{\rm T} = I$. One approach is to maximize the function $$f(o_{ij}) = \sum_{i,j} \lambda_i (o_{ij})^2 \sigma_j$$ under the constraint $\sum_{k} o_{ik}o_{jk} = \delta_{ij}$ by Lagrange multiplier method. I can prove the statement in this way but the whole proof is lengthy and lack of the flavor of linear algebra. Since the statement ""must be true"" intuitively, I want to know if there is an elegant way to prove it. A possible way may be mathematical induction but I failed to make it. Any help is appreciated.",,"['linear-algebra', 'matrices', 'optimization', 'trace']"
17,How to find the intersection of two hyperplanes in $n$ dimensions?,How to find the intersection of two hyperplanes in  dimensions?,n,"I want to find out the intersection of two surfaces that exist in $n$-dimensions. These surfaces are defined by a system of $2$ linear equations. $$A_1x+B_1y+C_1z+\cdots = D_1$$ $$A_2x+B_2y+C_2z+\cdots = D_2$$ But first, how would these surfaces look like? In $3$-D space, they would be planes, but in dimensions higher than $3$ are these not planes? Can we generalize a way to find the intersection of such linear expressions in some way? Also can this problem be solved using some tools, preferably Matlab?","I want to find out the intersection of two surfaces that exist in $n$-dimensions. These surfaces are defined by a system of $2$ linear equations. $$A_1x+B_1y+C_1z+\cdots = D_1$$ $$A_2x+B_2y+C_2z+\cdots = D_2$$ But first, how would these surfaces look like? In $3$-D space, they would be planes, but in dimensions higher than $3$ are these not planes? Can we generalize a way to find the intersection of such linear expressions in some way? Also can this problem be solved using some tools, preferably Matlab?",,"['linear-algebra', 'systems-of-equations', 'surfaces']"
18,Why is eigendecomposition $V \Lambda V^{-1}$ not $V^{-1} \Lambda V$,Why is eigendecomposition  not,V \Lambda V^{-1} V^{-1} \Lambda V,"Say you have a linear transformation matrix $A$. In the basis of eigenvectors, this transformation simply becomes a scaling, represented by the diagonal matrix of eigenvalues. Thus, intuitively the transformation A can be decomposed into the following: Transform into the basis of eigenvectors (using the transformation matrix $V$, where the eigenvectors form the columns) Apply the scaling. Transform back. This would seem to correspond to $V^{-1} \Lambda  V$, where the standard notation of matrices being applied on the left and vectors on the right holds. Yet everywhere I always see the formula as $V \Lambda  V^{-1}$. Why isn't my intuition correct?","Say you have a linear transformation matrix $A$. In the basis of eigenvectors, this transformation simply becomes a scaling, represented by the diagonal matrix of eigenvalues. Thus, intuitively the transformation A can be decomposed into the following: Transform into the basis of eigenvectors (using the transformation matrix $V$, where the eigenvectors form the columns) Apply the scaling. Transform back. This would seem to correspond to $V^{-1} \Lambda  V$, where the standard notation of matrices being applied on the left and vectors on the right holds. Yet everywhere I always see the formula as $V \Lambda  V^{-1}$. Why isn't my intuition correct?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations']"
19,"If $S$ is a subset of a vector space $V$, then $\text{span}(S)$ equals the intersection of all subspaces of $V$ that contain $S$.","If  is a subset of a vector space , then  equals the intersection of all subspaces of  that contain .",S V \text{span}(S) V S,"My introductory linear algebra textbook claims the following: If $S$ is a subset of a vector space $V$ , then $\text{span}(S)$ equals the intersection of all subspaces of $V$ that contain $S$ . I understand the aforementioned individual concepts, such as subsets, vector spaces, subspaces, and span, but I do not understand what is meant by, "" $\text{span}(S)$ equals the intersection of all subspaces of $V$ that contain $S$ ."" In addition, it seems to me that this statement is false: $\text{span}(S)$ does not necessarily have to equal the intersection of all subspaces of $V$ that contain $S$ . I would greatly appreciate it if someone would please take the time to elaborate on this concept and clarify what the textbook is saying. Please refrain from introducing more complex concepts from linear algebra in any explanation.","My introductory linear algebra textbook claims the following: If is a subset of a vector space , then equals the intersection of all subspaces of that contain . I understand the aforementioned individual concepts, such as subsets, vector spaces, subspaces, and span, but I do not understand what is meant by, "" equals the intersection of all subspaces of that contain ."" In addition, it seems to me that this statement is false: does not necessarily have to equal the intersection of all subspaces of that contain . I would greatly appreciate it if someone would please take the time to elaborate on this concept and clarify what the textbook is saying. Please refrain from introducing more complex concepts from linear algebra in any explanation.",S V \text{span}(S) V S \text{span}(S) V S \text{span}(S) V S,"['linear-algebra', 'vector-spaces']"
20,"adjacency matrix, maximal eigenvalue","adjacency matrix, maximal eigenvalue",,"I'm given a $d$ regular graph $G$. Dentote by $A(G)$ the adjacency matrix of $G$. I have to show that $\lambda_{\max}=d$, where $\lambda_{\max}$ is the biggest eigenvalue of $A(G)$. I know that the vector $(1,1,\ldots,1)$ is an eigenvector of $A(G)$ with eigenvalue $d$ but I cannot show that that this eigenvalue is indeed the maximal one. Any help on this would be great.","I'm given a $d$ regular graph $G$. Dentote by $A(G)$ the adjacency matrix of $G$. I have to show that $\lambda_{\max}=d$, where $\lambda_{\max}$ is the biggest eigenvalue of $A(G)$. I know that the vector $(1,1,\ldots,1)$ is an eigenvector of $A(G)$ with eigenvalue $d$ but I cannot show that that this eigenvalue is indeed the maximal one. Any help on this would be great.",,"['linear-algebra', 'combinatorics', 'graph-theory']"
21,"If $T$ and $T^2$ have equal rank then $V=\ker T\oplus {\rm im}\, T$ for $V$ finite dimensional.",If  and  have equal rank then  for  finite dimensional.,"T T^2 V=\ker T\oplus {\rm im}\, T V",I am trying to prove the following: Let $V$ be a finite-dimensional vector space. Consider an operator $T$ on $V$ such that $\text{dim range}(T)=\text{dim range}(T^2)$. Show that $V=\text{null}(T)\oplus \text{range}(T)$. I have been given the following hint: $\textit{Hint:}$ Show that there does not exist a $y\in V$ such that $y\notin \text{null}(T)$ and $y\notin \text{range}(T)$. I decided to try and prove the hint by contradiction... Suppose that there exists a $y\in V$ such that $y\notin \text{null}(T)$ and $y\notin \text{range}(T)$. Then  $$Ty\neq 0$$ and for every $v\in V$ we have $$Tv\neq y.$$ I have NO idea what to do with this information! I would greatly appreciate some hints/help! Thank you.,I am trying to prove the following: Let $V$ be a finite-dimensional vector space. Consider an operator $T$ on $V$ such that $\text{dim range}(T)=\text{dim range}(T^2)$. Show that $V=\text{null}(T)\oplus \text{range}(T)$. I have been given the following hint: $\textit{Hint:}$ Show that there does not exist a $y\in V$ such that $y\notin \text{null}(T)$ and $y\notin \text{range}(T)$. I decided to try and prove the hint by contradiction... Suppose that there exists a $y\in V$ such that $y\notin \text{null}(T)$ and $y\notin \text{range}(T)$. Then  $$Ty\neq 0$$ and for every $v\in V$ we have $$Tv\neq y.$$ I have NO idea what to do with this information! I would greatly appreciate some hints/help! Thank you.,,"['linear-algebra', 'linear-transformations', 'direct-sum']"
22,"Any ""interesting"" theorems for element-wise matrix product?","Any ""interesting"" theorems for element-wise matrix product?",,"From the point of view of linear algebra, the ""natural"" multiplication operation for matrices is the usual matrix product, and there are lots of theorems involving this product---e.g. the result $\det(AB) = \det(A)\det(B)$, or $\text{tr}(AB) = \text{tr}(BA)$, etc.  However, there are lots of matrices one encounters in practice whose structure allows them to be written in a convenient way as an element-wise (Hadamard) product of two other matrices.  This is one of the reasons why the default multiplication of arrays is element-wise in many programming languages (e.g. Python).  In situations where element-wise products appear, it could be very nice to have theorems (like the above determinant & trace relations) concerning the linear algebraic character of the element-wise product.  My question is: Do any ""interesting"" such theorems exist? [I don't expect to find any results as slick as the above $\det$ and $\text{tr}$ identities, but perhaps there are analogous inequalities, or maybe some non-trivial statements about diagonalizability, or eigenvalue relations, etc.]","From the point of view of linear algebra, the ""natural"" multiplication operation for matrices is the usual matrix product, and there are lots of theorems involving this product---e.g. the result $\det(AB) = \det(A)\det(B)$, or $\text{tr}(AB) = \text{tr}(BA)$, etc.  However, there are lots of matrices one encounters in practice whose structure allows them to be written in a convenient way as an element-wise (Hadamard) product of two other matrices.  This is one of the reasons why the default multiplication of arrays is element-wise in many programming languages (e.g. Python).  In situations where element-wise products appear, it could be very nice to have theorems (like the above determinant & trace relations) concerning the linear algebraic character of the element-wise product.  My question is: Do any ""interesting"" such theorems exist? [I don't expect to find any results as slick as the above $\det$ and $\text{tr}$ identities, but perhaps there are analogous inequalities, or maybe some non-trivial statements about diagonalizability, or eigenvalue relations, etc.]",,"['linear-algebra', 'matrices', 'soft-question']"
23,the physical significance of the Lie Algebra of SE(3),the physical significance of the Lie Algebra of SE(3),,"The Lie group of $SE(3)$ can be written in the form of $4\times4$ matrix, say $$ \begin{pmatrix} R & t\\ 0 & 1 \end{pmatrix},\tag{1} $$ and its Lie Algebra, denoted as $se(3)$ , can be composed by 6 generators $$ \begin{aligned} G_1&=\begin{pmatrix}0 & 0 & 0 & 1\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \end{pmatrix} & G_2&=\begin{pmatrix}0 & 0 & 0 & 0\\ 0 & 0 & 0 & 1\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \end{pmatrix} & G_3&=\begin{pmatrix}0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 1\\ 0 & 0 & 0 & 0 \end{pmatrix}\\ \\ G_4&=\begin{pmatrix}0 & 0 & 0 & 0\\ 0 & 0 & -1 & 0\\ 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 0 \end{pmatrix} & G_5&=\begin{pmatrix}0 & 0 & 1 & 0\\ 0 & 0 & 0 & 0\\ -1 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \end{pmatrix} & G_6&=\begin{pmatrix}0 & -1 & 0 & 0\\ 1 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \end{pmatrix}, \end{aligned}\tag{2} $$ so its Lie Algebra can be represented by multiples of the generators in the form of $4\times4$ matrix, $$ \begin{pmatrix} w_x & u\\ 0 & 0 \end{pmatrix}.\tag{3} $$ We can use the coordinates of rotation axis to build $w_x$ , skew symmetric matrix, and the coordinates of translation vector to build $u$ , so by exponential map, we can get homogeneous matrix in the form of (3) which we can use in the displacement of a frame. I am confused that why we can use the coordinates of rotation axis and translation vector to build $se(3)$ by multiples of the six generators, or what is the link among rotation axis ,translation vector and  the six generators? What is the physical significance of the $se(3)$ ? I hope that you can help me by explain the physical significance of $se(2)$ or $so(2)$ first to me. Thanks for anyone's help.","The Lie group of can be written in the form of matrix, say and its Lie Algebra, denoted as , can be composed by 6 generators so its Lie Algebra can be represented by multiples of the generators in the form of matrix, We can use the coordinates of rotation axis to build , skew symmetric matrix, and the coordinates of translation vector to build , so by exponential map, we can get homogeneous matrix in the form of (3) which we can use in the displacement of a frame. I am confused that why we can use the coordinates of rotation axis and translation vector to build by multiples of the six generators, or what is the link among rotation axis ,translation vector and  the six generators? What is the physical significance of the ? I hope that you can help me by explain the physical significance of or first to me. Thanks for anyone's help.","SE(3) 4\times4 
\begin{pmatrix}
R & t\\
0 & 1
\end{pmatrix},\tag{1}
 se(3) 
\begin{aligned}
G_1&=\begin{pmatrix}0 & 0 & 0 & 1\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{pmatrix} &
G_2&=\begin{pmatrix}0 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{pmatrix} &
G_3&=\begin{pmatrix}0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0
\end{pmatrix}\\
\\
G_4&=\begin{pmatrix}0 & 0 & 0 & 0\\
0 & 0 & -1 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0
\end{pmatrix} &
G_5&=\begin{pmatrix}0 & 0 & 1 & 0\\
0 & 0 & 0 & 0\\
-1 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{pmatrix} &
G_6&=\begin{pmatrix}0 & -1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{pmatrix},
\end{aligned}\tag{2}
 4\times4 
\begin{pmatrix}
w_x & u\\
0 & 0
\end{pmatrix}.\tag{3}
 w_x u se(3) se(3) se(2) so(2)","['linear-algebra', 'lie-groups']"
24,Existence of vector space complement and axiom of choice,Existence of vector space complement and axiom of choice,,"Let say we live in the category of vector spaces over $\mathbb{R}$ or $\mathbb{C}.$ Here are three sentences: Axiom of choice Every vector space has a base. For every vector space $V$ and its subspace $E\subset V$ there is a subspace $F\subset V$ such that $V=E\oplus F.$ I know how to prove that (1)->(2)->(3). How about the inverese? Do (2)->(1) and (3)->(2) hold? If this is not the case, then is there some weaker version of AC which imply (3)?","Let say we live in the category of vector spaces over $\mathbb{R}$ or $\mathbb{C}.$ Here are three sentences: Axiom of choice Every vector space has a base. For every vector space $V$ and its subspace $E\subset V$ there is a subspace $F\subset V$ such that $V=E\oplus F.$ I know how to prove that (1)->(2)->(3). How about the inverese? Do (2)->(1) and (3)->(2) hold? If this is not the case, then is there some weaker version of AC which imply (3)?",,"['linear-algebra', 'vector-spaces', 'axiom-of-choice']"
25,How to compute $\cos(\arctan(2)) = 1/\sqrt{5}$,How to compute,\cos(\arctan(2)) = 1/\sqrt{5},"I'm doing matrices and I rotated a line about an angle. The gradient of my line I'm rotating to the $x$-axis is $2$, from $y=2x$. So obviously the angle that the line $y=2x$ makes with the $x$-axis is $\arctan(2)$. My question is how do I arrive at  $\cos(\arctan(2)) = 1/\sqrt{5}$. <--- The $1/\sqrt{5}$ is what I'm confused with, how do I get this?","I'm doing matrices and I rotated a line about an angle. The gradient of my line I'm rotating to the $x$-axis is $2$, from $y=2x$. So obviously the angle that the line $y=2x$ makes with the $x$-axis is $\arctan(2)$. My question is how do I arrive at  $\cos(\arctan(2)) = 1/\sqrt{5}$. <--- The $1/\sqrt{5}$ is what I'm confused with, how do I get this?",,"['linear-algebra', 'trigonometry']"
26,$A \in M_3(\mathbb Z)$ be such that $\det(A)=1$ ; then what is the maximum possible number of entries of $A$ that are even ?,be such that  ; then what is the maximum possible number of entries of  that are even ?,A \in M_3(\mathbb Z) \det(A)=1 A,Let $A \in M_3(\mathbb Z)$ be such that $\det(A)=1$ ; then what is the maximum possible number of entries of $A$ that are even ?,Let $A \in M_3(\mathbb Z)$ be such that $\det(A)=1$ ; then what is the maximum possible number of entries of $A$ that are even ?,,"['linear-algebra', 'matrices']"
27,Cardinality of a basis of an infinite-dimensional vector space,Cardinality of a basis of an infinite-dimensional vector space,,"How would you find the cardinality of the basis of $\mathbb{R}$ over $\mathbb{Q}$? Is it countable or uncountable? In general, how do you find the cardinality of a basis of an infinite-dimensional vector space? Do you just search for a bijection between the basis and, say, $\mathbb{N}$ or $\mathbb{R}$? What are some instructive examples?","How would you find the cardinality of the basis of $\mathbb{R}$ over $\mathbb{Q}$? Is it countable or uncountable? In general, how do you find the cardinality of a basis of an infinite-dimensional vector space? Do you just search for a bijection between the basis and, say, $\mathbb{N}$ or $\mathbb{R}$? What are some instructive examples?",,['linear-algebra']
28,Why is the volume of a parallelepiped equal to the square root of $\sqrt{det(AA^T)}$,Why is the volume of a parallelepiped equal to the square root of,\sqrt{det(AA^T)},"Why is the $\sqrt{det(AA^T)}$ equal to the volume of a parallelepiped? Is is somehow related to the fact that $det(A) = det(A^T)$? EDIT: To clarify, the parallelepiped is spanned by the columns of A.","Why is the $\sqrt{det(AA^T)}$ equal to the volume of a parallelepiped? Is is somehow related to the fact that $det(A) = det(A^T)$? EDIT: To clarify, the parallelepiped is spanned by the columns of A.",,"['linear-algebra', 'matrices']"
29,Generate arbitrary numerically invertable matrix,Generate arbitrary numerically invertable matrix,,"I'm designing a unit-test for a matrix inversion function. Currently I make a random matrix as a test case by generating its elements with random numbers uniformly distributed in $[0,1)$.  If I stopped here, there would be a small, but non-zero, chance that the resulting matrix will be numerically ill-conditioned for inversion. I know that I can solve this problem by diagonal loading of the matrix, i.e. adding a value $c$ to the diagonal elements $M_{ii}$. Is there an easy way to compute that amount of diagonal loading $c$ that I can add to such a random matrix to ensure that it is not (numerically)  ill-conditioned for inversion? My hunch is that the size of $c$ depends (at least) on the size of the matrix $N$, but I don't know how to derive the relationship.  Also note, that I'm not trying to find a smallest $c$ for the specific matrix under consideration, but rather a general $c$ applicable to any of my random matrices.","I'm designing a unit-test for a matrix inversion function. Currently I make a random matrix as a test case by generating its elements with random numbers uniformly distributed in $[0,1)$.  If I stopped here, there would be a small, but non-zero, chance that the resulting matrix will be numerically ill-conditioned for inversion. I know that I can solve this problem by diagonal loading of the matrix, i.e. adding a value $c$ to the diagonal elements $M_{ii}$. Is there an easy way to compute that amount of diagonal loading $c$ that I can add to such a random matrix to ensure that it is not (numerically)  ill-conditioned for inversion? My hunch is that the size of $c$ depends (at least) on the size of the matrix $N$, but I don't know how to derive the relationship.  Also note, that I'm not trying to find a smallest $c$ for the specific matrix under consideration, but rather a general $c$ applicable to any of my random matrices.",,"['linear-algebra', 'numerical-linear-algebra']"
30,$\text{rank}(AB^2)=\text{rank}(AB)$,,\text{rank}(AB^2)=\text{rank}(AB),"Let $A$ and $B$ be real $n×n$ matrices such that $$AB=BA,~~\text{rank}(A^2)=\text{rank}(A),~~\text{rank} (B^2)=\text{rank}(B)$$ Show that: $$\text{rank}(AB^2)=\text{rank}(AB)$$","Let $A$ and $B$ be real $n×n$ matrices such that $$AB=BA,~~\text{rank}(A^2)=\text{rank}(A),~~\text{rank} (B^2)=\text{rank}(B)$$ Show that: $$\text{rank}(AB^2)=\text{rank}(AB)$$",,"['linear-algebra', 'matrix-rank']"
31,Positive semi-definite of a matrix composed of semi-definite blocks,Positive semi-definite of a matrix composed of semi-definite blocks,,"Say a matrix A is positive semi-definite. Let B be a square matrix composed of replicas of A as sub-blocks, s.t. $$B=\begin{pmatrix} A & A \\ A & A \\ \end{pmatrix},$$ or $$\begin{pmatrix} A & A & A \\  A & A & A \\ A & A & A \\ \end{pmatrix},$$ etc. Would $B$ be semi-definite as well?","Say a matrix A is positive semi-definite. Let B be a square matrix composed of replicas of A as sub-blocks, s.t. $$B=\begin{pmatrix} A & A \\ A & A \\ \end{pmatrix},$$ or $$\begin{pmatrix} A & A & A \\  A & A & A \\ A & A & A \\ \end{pmatrix},$$ etc. Would $B$ be semi-definite as well?",,"['linear-algebra', 'matrices', 'block-matrices']"
32,Prove that matrices of the form $\begin{pmatrix} x & x \\ x & x \end{pmatrix}$ are a group under matrix multiplication.,Prove that matrices of the form  are a group under matrix multiplication.,\begin{pmatrix} x & x \\ x & x \end{pmatrix},"G is the set of matrices of the form $G=$$\begin{pmatrix} x & x \\ x & x \end{pmatrix}$. So for this set to be a group I know it needs to be: Closed under matrix multiplication The Associative Property holds Contains an Identity Element Every element needs to have an inverse So the form of the matrices is such that all the elements are the same but not 0. How do I go about proving these? Working through this problem, I seem to have hit a contradiction. Since G is a subgroup of the bigger $2x2$ nonsingular matrices group why does G not have the same identity element as its parent group? Namely \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} Isn't the subgroup supposed to have the same identity element as its parent group?","G is the set of matrices of the form $G=$$\begin{pmatrix} x & x \\ x & x \end{pmatrix}$. So for this set to be a group I know it needs to be: Closed under matrix multiplication The Associative Property holds Contains an Identity Element Every element needs to have an inverse So the form of the matrices is such that all the elements are the same but not 0. How do I go about proving these? Working through this problem, I seem to have hit a contradiction. Since G is a subgroup of the bigger $2x2$ nonsingular matrices group why does G not have the same identity element as its parent group? Namely \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} Isn't the subgroup supposed to have the same identity element as its parent group?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory']"
33,"Given matrix A with eigenvalue $\lambda$ and corresponding eigenvector x, prove $A^k$ has eigenvalue $\lambda^k$","Given matrix A with eigenvalue  and corresponding eigenvector x, prove  has eigenvalue",\lambda A^k \lambda^k,"Given matrix A with eigenvalue $\lambda$ and corresponding eigenvector x, prove $A^k$ has eigenvalue $\lambda^k$ for the same eigenvector x for any positive integer k. Can I just use the eigenvalue definition Ax = $\lambda$x, divide x on both sides, then raising the power?","Given matrix A with eigenvalue $\lambda$ and corresponding eigenvector x, prove $A^k$ has eigenvalue $\lambda^k$ for the same eigenvector x for any positive integer k. Can I just use the eigenvalue definition Ax = $\lambda$x, divide x on both sides, then raising the power?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
34,Can non-normal matrices with double eigenvalues never be diagonalized?,Can non-normal matrices with double eigenvalues never be diagonalized?,,"Is there a matrix $A$ with $A^TA≠ AA^T$ (non-normality) and double eigenvalue that is still diagonalizable? If $A^TA \neq AA^T$ and $λ_1 =λ_2 = λ$ (double eigenvalue) $\stackrel{?}{⇒}$ not exists $V$, $Ω$ with $A = V Ω V^{-1}$  ? As one example consider $\begin{bmatrix}  1 & 1\\ 0 & a \end{bmatrix}$ which is obviously non-normal. When I change the matrix to $\begin{bmatrix}  1 & 1\\ 0 & 1 \end{bmatrix}$ in order to get a double eigenvalue, it turns out to become non-diagonalizable. Is this always the case?","Is there a matrix $A$ with $A^TA≠ AA^T$ (non-normality) and double eigenvalue that is still diagonalizable? If $A^TA \neq AA^T$ and $λ_1 =λ_2 = λ$ (double eigenvalue) $\stackrel{?}{⇒}$ not exists $V$, $Ω$ with $A = V Ω V^{-1}$  ? As one example consider $\begin{bmatrix}  1 & 1\\ 0 & a \end{bmatrix}$ which is obviously non-normal. When I change the matrix to $\begin{bmatrix}  1 & 1\\ 0 & 1 \end{bmatrix}$ in order to get a double eigenvalue, it turns out to become non-diagonalizable. Is this always the case?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
35,Understanding Dual Transformations and reasoning behind definition,Understanding Dual Transformations and reasoning behind definition,,"In Linear Algebra working with Dual space and dual transformations I've come along this very basic definition of the dual transformations: Suppose: $T^*$ is a dual transformations from $W^*\to V^*$ $T$ is a linear transformation from $V\to W$ $u$ is a linear functional which belongs to $W^*$ $v$ is a vector which belongs to $V$ Then the following applies:      $$ (T^*u)(v)  = u(Tv) $$ Why is the dual transformation defined this way? (I know this is a very problematic question, but please any intuition will be very helpful)","In Linear Algebra working with Dual space and dual transformations I've come along this very basic definition of the dual transformations: Suppose: $T^*$ is a dual transformations from $W^*\to V^*$ $T$ is a linear transformation from $V\to W$ $u$ is a linear functional which belongs to $W^*$ $v$ is a vector which belongs to $V$ Then the following applies:      $$ (T^*u)(v)  = u(Tv) $$ Why is the dual transformation defined this way? (I know this is a very problematic question, but please any intuition will be very helpful)",,['linear-algebra']
36,Every $v \in V - \{ 0 \}$ is cyclic iff the characteristic polynomial of $T : V \to V$ is irreducible over $F$,Every  is cyclic iff the characteristic polynomial of  is irreducible over,v \in V - \{ 0 \} T : V \to V F,"Let $V\neq \{0\}$ be a vector space over $F$ , and $T$ a linear operator on $V$ . Prove that every $0\neq v \in V$ is a cyclic vector if and only if the characteristic polynomial of $T$ is irreducible over $F$ . I can't seem to get anywhere on either side... Can someone please help?","Let be a vector space over , and a linear operator on . Prove that every is a cyclic vector if and only if the characteristic polynomial of is irreducible over . I can't seem to get anywhere on either side... Can someone please help?",V\neq \{0\} F T V 0\neq v \in V T F,['linear-algebra']
37,"Embeddings of $\mathrm{GL}(n-1,q)$ into $\mathrm{GL}(n,q)$",Embeddings of  into,"\mathrm{GL}(n-1,q) \mathrm{GL}(n,q)","Let $n>2$ and $q$ be a prime power. I want to find all the embeddings of $\mathrm{GL}(n-1,q)$ into $\mathrm{GL}(n,q)$ . An embedding I first thought of is as following. Let $V$ be an $n$ -dimensional vector space over $\mathrm{GF}(q)$ and $H$ is a subgroup of $\mathrm{GL}(V)$ fixing a nonzero vector in $V$ . Then $H$ is isomorphic to $\mathrm{GL}(n-1,q)$ .",Let and be a prime power. I want to find all the embeddings of into . An embedding I first thought of is as following. Let be an -dimensional vector space over and is a subgroup of fixing a nonzero vector in . Then is isomorphic to .,"n>2 q \mathrm{GL}(n-1,q) \mathrm{GL}(n,q) V n \mathrm{GF}(q) H \mathrm{GL}(V) V H \mathrm{GL}(n-1,q)","['linear-algebra', 'group-theory', 'finite-groups']"
38,Special orthogonal matrices have orthogonal square roots,Special orthogonal matrices have orthogonal square roots,,Let $A$ be an orthogonal matrix with $\det (A)=1$. Show that there exists an orthogonal matrix $B$ such that $B^2=A$. Thank you very much.,Let $A$ be an orthogonal matrix with $\det (A)=1$. Show that there exists an orthogonal matrix $B$ such that $B^2=A$. Thank you very much.,,"['linear-algebra', 'matrices', 'determinant', 'inner-products']"
39,Eigenvalues of block matrices with zero diagonal blocks,Eigenvalues of block matrices with zero diagonal blocks,,Is there any simple relation between the eigenvalues (or the characteristic polynomials) of two matrices $A$ and $B$ with that of matrix $C$ defined as  $$ C=\begin{bmatrix} 0 & A \\ B & 0 \end{bmatrix}. $$  $A$ and $B$ are invertible $n\times n$ matrices.,Is there any simple relation between the eigenvalues (or the characteristic polynomials) of two matrices $A$ and $B$ with that of matrix $C$ defined as  $$ C=\begin{bmatrix} 0 & A \\ B & 0 \end{bmatrix}. $$  $A$ and $B$ are invertible $n\times n$ matrices.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices', 'characteristic-polynomial']"
40,Why is it that a linear transformation can only preserve or reduce a vector space's dimension?,Why is it that a linear transformation can only preserve or reduce a vector space's dimension?,,I am able to prove the above-asked using the fact that a linear map preserves linear dependence. I also vaguely suspect a connection from group theory (homomorphisms?). But I'm having trouble getting an intuition for that fact that a linear transformation cannot increase the dimension of a vector space. Help?,I am able to prove the above-asked using the fact that a linear map preserves linear dependence. I also vaguely suspect a connection from group theory (homomorphisms?). But I'm having trouble getting an intuition for that fact that a linear transformation cannot increase the dimension of a vector space. Help?,,"['linear-algebra', 'matrices']"
41,How to calculate a linear transformation given its effect on some vectors,How to calculate a linear transformation given its effect on some vectors,,"Im not sure if my question is worded very well, but I'm having trouble understanding how to tackle this problem. Let $T\colon\mathbb{R}^3\to\mathbb{R}^2$ be the linear transformation such that $T(1,-1,2)=(-3,1)$ and $T(3,-1,1) = (-1,2)$. Find $T(9,-1,10)$. Thanks","Im not sure if my question is worded very well, but I'm having trouble understanding how to tackle this problem. Let $T\colon\mathbb{R}^3\to\mathbb{R}^2$ be the linear transformation such that $T(1,-1,2)=(-3,1)$ and $T(3,-1,1) = (-1,2)$. Find $T(9,-1,10)$. Thanks",,['linear-algebra']
42,How to evaluate this determinant?,How to evaluate this determinant?,,"Can someone give me a hint how to solve $$\left|\begin{array}{ccccc} 1 & 1 & \ldots &  & 1\\ 2x_{1} & 2x_{2} &  &  & 2x_{n}\\ \vdots\\ nx_{1}^{n-1} & nx_{2}^{n-1} & \ldots &  & nx_{n}^{n-1}\\ \\ \end{array}\right|$$ ? I know that I somehow have to use the Vandermonde determinant to do this, but I can't figure out how to get rid of the coefficients. Can someone give me a hint please ?","Can someone give me a hint how to solve $$\left|\begin{array}{ccccc} 1 & 1 & \ldots &  & 1\\ 2x_{1} & 2x_{2} &  &  & 2x_{n}\\ \vdots\\ nx_{1}^{n-1} & nx_{2}^{n-1} & \ldots &  & nx_{n}^{n-1}\\ \\ \end{array}\right|$$ ? I know that I somehow have to use the Vandermonde determinant to do this, but I can't figure out how to get rid of the coefficients. Can someone give me a hint please ?",,['linear-algebra']
43,Minimize $\mbox{tr}(AX)$ where $X$ is an orthogonal matrix,Minimize  where  is an orthogonal matrix,\mbox{tr}(AX) X,"Let $A, X\in\mathbb{R}^{n\times n}$. The scalar objective function is $$J=\mathrm{tr}(AX)$$ If no constraints, let the derivative of $J$ with respect to $X$ be zeros, then we have $$A=0$$ Suppose $A$ is also a complex function, from $A=0$ I can further calculate something. My question is : what if $X$ is an orthogonal matrix, i.e., $X^TX=I$? Then it becomes an constrained optimization problem. Can I still use matrix differential techniques to derive the minimizer? Thanks.","Let $A, X\in\mathbb{R}^{n\times n}$. The scalar objective function is $$J=\mathrm{tr}(AX)$$ If no constraints, let the derivative of $J$ with respect to $X$ be zeros, then we have $$A=0$$ Suppose $A$ is also a complex function, from $A=0$ I can further calculate something. My question is : what if $X$ is an orthogonal matrix, i.e., $X^TX=I$? Then it becomes an constrained optimization problem. Can I still use matrix differential techniques to derive the minimizer? Thanks.",,"['linear-algebra', 'matrices', 'optimization']"
44,Skew-symmetric matrix and quadratic form,Skew-symmetric matrix and quadratic form,,"Show that the quadratic form of a matrix is $0$ if and only if the matrix is skew– symmetric, i.e., show that $q_A(x) = 0$ for all $x$ iff $A^t = −A$. Thanks a lot!","Show that the quadratic form of a matrix is $0$ if and only if the matrix is skew– symmetric, i.e., show that $q_A(x) = 0$ for all $x$ iff $A^t = −A$. Thanks a lot!",,['linear-algebra']
45,Decomposition of product of exterior products,Decomposition of product of exterior products,,Suppose $V$ is a $n$-dimensional vector space. What is the kernel of $$\bigwedge^p V \otimes \bigwedge^q V\longrightarrow \bigwedge^{p+q} V$$ here $p+q \le n$.,Suppose $V$ is a $n$-dimensional vector space. What is the kernel of $$\bigwedge^p V \otimes \bigwedge^q V\longrightarrow \bigwedge^{p+q} V$$ here $p+q \le n$.,,"['linear-algebra', 'multilinear-algebra', 'exterior-algebra']"
46,"What's the difference between ""generate"" and ""linear span"" in linear algebra?","What's the difference between ""generate"" and ""linear span"" in linear algebra?",,"In this question , the word "" generate "" is used when the author of the book mentioned a vector space. And also, in a Gowers's article , it is used in ""vector space generated by the functions $[v,w]$"". There is another similar concept called "" linear span "" in linear algebra. In Gowers's article, he also uses the word ""span"" as ""the space spanned by the functions [v,w]"". Here is my question: Are these two concepts the same? Or what's the difference?","In this question , the word "" generate "" is used when the author of the book mentioned a vector space. And also, in a Gowers's article , it is used in ""vector space generated by the functions $[v,w]$"". There is another similar concept called "" linear span "" in linear algebra. In Gowers's article, he also uses the word ""span"" as ""the space spanned by the functions [v,w]"". Here is my question: Are these two concepts the same? Or what's the difference?",,['linear-algebra']
47,Does $\det(A) \neq 0$ (where A is the coefficient matrix) $\rightarrow$ a basis in vector spaces other than $R^{n}$?,Does  (where A is the coefficient matrix)  a basis in vector spaces other than ?,\det(A) \neq 0 \rightarrow R^{n},"I know that for a set of vectors $\{ v_{1}, v_{2}, \ldots , v_{n} \} \in \mathbb{R}^{n}$ we can show that the vectors form a basis in $\mathbb{R}^{n}$ if we show that the coefficient matrix $A$ has the property $\det(A) \neq 0$, because this shows the homogeneous system has only the trivial solution, and the non-homogeneous system is consistent for every vector $(b_{1}, b_{2}, \ldots , b_{n}) \in \mathbb{R}^{n}$. Intuitively, this concept seems applicable to all polynomials in $\mathbf{P}_{n}$ and all matrices in $M_{nn}$. Can someone validate this? edit: I think to make the intuition hold, $A$ must be defined as follows in $M_{nn}$: Let $M_{1}, M_{2}, ... , M_{k}$ be matrices in $M_{nn}$. To prove these form a basis for $M_{nn}$, we must show that $c_{1}M_{1} + c_{2}M_{2} + ... + c_{k}M_{k} = 0$ has the only trivial solution, and that every $n \times n$ matrix can be expressed as $c_{1}M_{1} + c_{2}M_{2} + ... + c_{k}M_{k} = B$. So I believe that for $M_{nn}$, $A$ must be defined as a $n^{2} \times n^{2}$ matrix where each row vector is formed from all the $(i, j)$ entries taken from $M_{1}, M_{2}, ... , M_{k}$ (in that order.) $\text{e.g. } A = \begin{pmatrix} M_{1_{1,1}} & M_{2_{1,1}} & ... & M_{k_{1,1}} \\ M_{1_{1,2}} & M_{2_{1,2}} & ... & M_{k_{1,2}} \\ ... & ... & ... & ... \\ M_{1_{n,n}} & M_{2_{n,n}} & ... & M_{k_{n,n}} &  \end{pmatrix}$ However, I am not sure about this.","I know that for a set of vectors $\{ v_{1}, v_{2}, \ldots , v_{n} \} \in \mathbb{R}^{n}$ we can show that the vectors form a basis in $\mathbb{R}^{n}$ if we show that the coefficient matrix $A$ has the property $\det(A) \neq 0$, because this shows the homogeneous system has only the trivial solution, and the non-homogeneous system is consistent for every vector $(b_{1}, b_{2}, \ldots , b_{n}) \in \mathbb{R}^{n}$. Intuitively, this concept seems applicable to all polynomials in $\mathbf{P}_{n}$ and all matrices in $M_{nn}$. Can someone validate this? edit: I think to make the intuition hold, $A$ must be defined as follows in $M_{nn}$: Let $M_{1}, M_{2}, ... , M_{k}$ be matrices in $M_{nn}$. To prove these form a basis for $M_{nn}$, we must show that $c_{1}M_{1} + c_{2}M_{2} + ... + c_{k}M_{k} = 0$ has the only trivial solution, and that every $n \times n$ matrix can be expressed as $c_{1}M_{1} + c_{2}M_{2} + ... + c_{k}M_{k} = B$. So I believe that for $M_{nn}$, $A$ must be defined as a $n^{2} \times n^{2}$ matrix where each row vector is formed from all the $(i, j)$ entries taken from $M_{1}, M_{2}, ... , M_{k}$ (in that order.) $\text{e.g. } A = \begin{pmatrix} M_{1_{1,1}} & M_{2_{1,1}} & ... & M_{k_{1,1}} \\ M_{1_{1,2}} & M_{2_{1,2}} & ... & M_{k_{1,2}} \\ ... & ... & ... & ... \\ M_{1_{n,n}} & M_{2_{n,n}} & ... & M_{k_{n,n}} &  \end{pmatrix}$ However, I am not sure about this.",,"['linear-algebra', 'vector-spaces', 'determinant']"
48,"Determinants: multiply a row by a scalar, all is good; multiply a row by a square matrix?","Determinants: multiply a row by a scalar, all is good; multiply a row by a square matrix?",,"When calculating determinants it can be nice to multiply a row by a number or to add one row to another (your basic row operations).  Each has an easy to understand effect on the determinant.  Today I ran across a type of row operation whose effect on the determinant is a lot less clear to me. Take an $n \times n$ matrix $M$ and an $n \times n$ matrix $A$.  While calculating the determinant of $M$, I might mess around with the rows of $M$.   Every time I mess with the first row of $M$, I would like to be able to change my mind and mess with the second row of $M$ instead. If I multiply the first row of $M$ by $2$ to get $M'$, or if I multiply the second row of $M$ by $2$ to get $M″$, I get the same determinants, $\det(M') = \det(M″)$.  This does not depend on the matrix $M$; I always have that $2$ times the first row is equivalent to $2$ times the second row. For instance, if $M = \left(\begin{smallmatrix}a& b \\ c& d \end{smallmatrix}\right)$ then $M' = \left(\begin{smallmatrix}2a& 2b \\ c& d \end{smallmatrix}\right)$ and $M'' = \left(\begin{smallmatrix}a& b \\ 2c& 2d \end{smallmatrix}\right)$ and $$\det(M') = \det(M″) = 2\det(M) = 2ad−2bc.$$ Can I do something similar with ""$A$"" instead of ""$2$""? If I multiply the first row of $M$ by $A$ to get $M'$, then what matrix $B$ do I need to multiply the second row of $M$ by to get $M''$ so that $\det(M')=\det(M'')$ have the same determinant? I would like the answer to be depend only on $A$, not on $M$.  I calculated an answer for $2×2$ matrices, but I'm not happy with it. For instance, if $A = \left(\begin{smallmatrix}2& 0 \\ 0& 1 \end{smallmatrix}\right)$, and $M = \left(\begin{smallmatrix}a& b \\ c& d \end{smallmatrix}\right)$, then $M' = \left(\begin{smallmatrix}2a& b \\ c& d \end{smallmatrix}\right)$ and $\det(M') = 2ad−bc$.  I can choose $B = \left(\begin{smallmatrix}1& 0 \\ 0& 2 \end{smallmatrix}\right)$ so that $M'' = \left(\begin{smallmatrix}a& b \\ c& 2d \end{smallmatrix}\right)$ and $\det(M'') = 2ad−bc = \det(M')$. However, I'd like to understand more clearly the relationship between $A$ and $B$.  Perhaps there is a fair amount of freedom in choosing $B$ and I have chosen a bad one in my work.  What is a natural choice of $B$ (for general $A$)?","When calculating determinants it can be nice to multiply a row by a number or to add one row to another (your basic row operations).  Each has an easy to understand effect on the determinant.  Today I ran across a type of row operation whose effect on the determinant is a lot less clear to me. Take an $n \times n$ matrix $M$ and an $n \times n$ matrix $A$.  While calculating the determinant of $M$, I might mess around with the rows of $M$.   Every time I mess with the first row of $M$, I would like to be able to change my mind and mess with the second row of $M$ instead. If I multiply the first row of $M$ by $2$ to get $M'$, or if I multiply the second row of $M$ by $2$ to get $M″$, I get the same determinants, $\det(M') = \det(M″)$.  This does not depend on the matrix $M$; I always have that $2$ times the first row is equivalent to $2$ times the second row. For instance, if $M = \left(\begin{smallmatrix}a& b \\ c& d \end{smallmatrix}\right)$ then $M' = \left(\begin{smallmatrix}2a& 2b \\ c& d \end{smallmatrix}\right)$ and $M'' = \left(\begin{smallmatrix}a& b \\ 2c& 2d \end{smallmatrix}\right)$ and $$\det(M') = \det(M″) = 2\det(M) = 2ad−2bc.$$ Can I do something similar with ""$A$"" instead of ""$2$""? If I multiply the first row of $M$ by $A$ to get $M'$, then what matrix $B$ do I need to multiply the second row of $M$ by to get $M''$ so that $\det(M')=\det(M'')$ have the same determinant? I would like the answer to be depend only on $A$, not on $M$.  I calculated an answer for $2×2$ matrices, but I'm not happy with it. For instance, if $A = \left(\begin{smallmatrix}2& 0 \\ 0& 1 \end{smallmatrix}\right)$, and $M = \left(\begin{smallmatrix}a& b \\ c& d \end{smallmatrix}\right)$, then $M' = \left(\begin{smallmatrix}2a& b \\ c& d \end{smallmatrix}\right)$ and $\det(M') = 2ad−bc$.  I can choose $B = \left(\begin{smallmatrix}1& 0 \\ 0& 2 \end{smallmatrix}\right)$ so that $M'' = \left(\begin{smallmatrix}a& b \\ c& 2d \end{smallmatrix}\right)$ and $\det(M'') = 2ad−bc = \det(M')$. However, I'd like to understand more clearly the relationship between $A$ and $B$.  Perhaps there is a fair amount of freedom in choosing $B$ and I have chosen a bad one in my work.  What is a natural choice of $B$ (for general $A$)?",,"['linear-algebra', 'matrices']"
49,Linear Algebra Done Right Exercise 6.B.16,Linear Algebra Done Right Exercise 6.B.16,,"I am trying to solve Exercise 16 from Section 6.B of the third edition of Linear Algebra Done Right by Axler. Suppose $ \mathbf{F} = \mathbf{C}, V $ is finite-dimensional, $ T \in \mathcal{L}(V) $ , all the eigenvalues of $ T $ have absolute value less than $1$ , and $ \epsilon > 0 $ . Prove that there exists a positive integer $ m $ such that $ \lVert T^m v \rVert \leq \epsilon \lVert v \rVert $ for every $ v \in V $ . ( $V$ is an inner product space.) I think this is a quick corollary of the implication $\rho(T) < 1 \implies \lim_{m \to \infty} \lVert T^m \rVert = 0 $ . However, at this point in the textbook we have discussed neither spectral radius nor operator norm; for this reason I think Mr. Axler had another approach in mind. The title of Section 6.B is 'Orthonormal Bases', so I tried to look for a solution which only uses basic properties of orthonormal bases. Since $V$ is a finite-dimensional complex vector space, Schur decomposition (Theorem 6.38 in the text) ensures there is an orthonormal basis of $V$ with respect to which $T$ has an upper-triangular matrix. I managed to prove the result in the special case where this matrix is diagonal and also in the special case where $\dim V = 2$ , but I haven't been able to prove the general case. Can anyone see an 'orthonormal basis approach'? I did find this post . Unfortunately, I think there is a mistake in that answer. At least, I can't see how the equality $$ \sum_{k=1}^n \langle T^j(v), e_k \rangle e_k = \sum_{k=1}^n a_k \lambda_k^j e_k $$ follows from $\langle T^j(e_k), e_k \rangle = \lambda_k^j$ .","I am trying to solve Exercise 16 from Section 6.B of the third edition of Linear Algebra Done Right by Axler. Suppose is finite-dimensional, , all the eigenvalues of have absolute value less than , and . Prove that there exists a positive integer such that for every . ( is an inner product space.) I think this is a quick corollary of the implication . However, at this point in the textbook we have discussed neither spectral radius nor operator norm; for this reason I think Mr. Axler had another approach in mind. The title of Section 6.B is 'Orthonormal Bases', so I tried to look for a solution which only uses basic properties of orthonormal bases. Since is a finite-dimensional complex vector space, Schur decomposition (Theorem 6.38 in the text) ensures there is an orthonormal basis of with respect to which has an upper-triangular matrix. I managed to prove the result in the special case where this matrix is diagonal and also in the special case where , but I haven't been able to prove the general case. Can anyone see an 'orthonormal basis approach'? I did find this post . Unfortunately, I think there is a mistake in that answer. At least, I can't see how the equality follows from ."," \mathbf{F} = \mathbf{C}, V   T \in \mathcal{L}(V)   T  1  \epsilon > 0   m   \lVert T^m v \rVert \leq \epsilon \lVert v \rVert   v \in V  V \rho(T) < 1 \implies \lim_{m \to \infty} \lVert T^m \rVert = 0  V V T \dim V = 2 
\sum_{k=1}^n \langle T^j(v), e_k \rangle e_k = \sum_{k=1}^n a_k \lambda_k^j e_k
 \langle T^j(e_k), e_k \rangle = \lambda_k^j","['linear-algebra', 'eigenvalues-eigenvectors']"
50,"Question about a proof: If $AB=BA$ then $A$ and $B$ share a common eigenvector, from Strang's Linear Algebra","Question about a proof: If  then  and  share a common eigenvector, from Strang's Linear Algebra",AB=BA A B,"In Strang's Linear algebra, he proves the following (which has been asked and answered on SE, but my question is about a particular part of his proof): Let $A$ and $B$ be complex $n\times n$ matricies.  Prove that if $AB=BA$ , then $A$ and $B$ share a common eigenvector. His proof is as follows: Let $\lambda$ be a eigenvalue of $A$ .  Starting from $Ax=\lambda x$ , we have $ABx=BAx=B\lambda x=\lambda Bx$ .  So, $x$ and $Bx$ are both eigenvectors of $A$ sharing the same $\lambda$ (or else $Bx=0$ ).  If we assume the eigenvalues of $A$ are distinct, so the eigenspaces are one dimensional, then $Bx$ must be a multiple of $x$ .  In other words, $x$ is an eigenvector of $B$ as well as $A$ . My question is, and I am probably overthinking something quite elementary, but when he says ""if we assume the eigenvalues of $A$ are distinct..."", I agree then with the rest of the argument.... but why can he do that?","In Strang's Linear algebra, he proves the following (which has been asked and answered on SE, but my question is about a particular part of his proof): Let and be complex matricies.  Prove that if , then and share a common eigenvector. His proof is as follows: Let be a eigenvalue of .  Starting from , we have .  So, and are both eigenvectors of sharing the same (or else ).  If we assume the eigenvalues of are distinct, so the eigenspaces are one dimensional, then must be a multiple of .  In other words, is an eigenvector of as well as . My question is, and I am probably overthinking something quite elementary, but when he says ""if we assume the eigenvalues of are distinct..."", I agree then with the rest of the argument.... but why can he do that?",A B n\times n AB=BA A B \lambda A Ax=\lambda x ABx=BAx=B\lambda x=\lambda Bx x Bx A \lambda Bx=0 A Bx x x B A A,"['linear-algebra', 'eigenvalues-eigenvectors']"
51,Generalized eigenvector for product of commuting matrices,Generalized eigenvector for product of commuting matrices,,"Suppose $A,B$ are commuting invertible matrices with a common generalized eigenvector $v$ with eigenvalues $a,b$ respectively. That is, suppose there exist positive integers $K,L$ such that $(A-aI)^K v= 0$ and $(B-bI)^Lv=0$ . Is it true that there exists a positive integer $M$ such that $(AB-abI)^Mv = 0$ ? A bit of context: I came across this while thinking about the analogous version of this for eigenvalues (when $K=1$ and $L=1$ ). This is easy to show. In fact, the above statement is also easy to show if only one of $K=1$ or $L=1$ is true (that is, $v$ is an eigenvector for one matrix and a generalized eigenvector for the other). Proof: WLOG suppose $L=1$ . Then we have \begin{align} (AB-abI)^Kv &= \sum_{j=0}^K {K \choose j} A^{K-j}B^{K-j}a^jb^jv\\ &= \sum_{j=0}^K {K \choose j} A^{K-j}a^jb^Kv\\ &= b^K(A-aI)^Kv\\ &= 0 \end{align} A natural question: is it true if $v$ is a generalized eigenvector for both matrices?","Suppose are commuting invertible matrices with a common generalized eigenvector with eigenvalues respectively. That is, suppose there exist positive integers such that and . Is it true that there exists a positive integer such that ? A bit of context: I came across this while thinking about the analogous version of this for eigenvalues (when and ). This is easy to show. In fact, the above statement is also easy to show if only one of or is true (that is, is an eigenvector for one matrix and a generalized eigenvector for the other). Proof: WLOG suppose . Then we have A natural question: is it true if is a generalized eigenvector for both matrices?","A,B v a,b K,L (A-aI)^K v= 0 (B-bI)^Lv=0 M (AB-abI)^Mv = 0 K=1 L=1 K=1 L=1 v L=1 \begin{align}
(AB-abI)^Kv &= \sum_{j=0}^K {K \choose j} A^{K-j}B^{K-j}a^jb^jv\\
&= \sum_{j=0}^K {K \choose j} A^{K-j}a^jb^Kv\\
&= b^K(A-aI)^Kv\\
&= 0
\end{align} v","['linear-algebra', 'eigenvalues-eigenvectors', 'generalized-eigenvector']"
52,What's the Lie algebra associated to positive-defined Hermitian matrices?,What's the Lie algebra associated to positive-defined Hermitian matrices?,,"I'm studying polar decomposition $GL(n,\Bbb C)\cong U(n)\times Herm^+(n)$ , where $Herm^+(n)$ are positive-defined Hermitian matrices. I was trying to understand what happens to the associated Lie algebras, and I was wondering what $Herm^+(n)$ Lie algebra would look like. What I tried: I know that considering $F:SL(n,\Bbb R)\to SL(n,\Bbb R):A\mapsto A^\top A$ , since $SO(n)=F^{-1}(I_n)$ , we can prove that $\mathfrak{so}(n)$ is the space of skew-symmetric matrices by calculating $$\mathfrak{so}(n)=\ker (dF_{I_n})=\left\{A\in\Bbb R^{n\times n} : \left. \frac d{dt}\right\vert_{t=0}F(I_n+tA)=0\right\}.$$ I wondered if I could do something similar for $Herm(n)$ , defining $G:\Bbb C^{n\times n}\to\Bbb C^{n\times n}:A\mapsto A^*-A$ and $Herm(n)=G^{-1}(O_n)$ , but I then realised that being $Herm(n)$ a linear subspace of $\Bbb C^{n\times n}$ I would just find $Herm(n)$ again. Is this a correct answer? Is $Herm(n)$ itself its own associated Lie algebra (with the commutator as Lie bracket)? And what about positive-defined ones? What useful characterization may I take advantage of? Thanks in advance!","I'm studying polar decomposition , where are positive-defined Hermitian matrices. I was trying to understand what happens to the associated Lie algebras, and I was wondering what Lie algebra would look like. What I tried: I know that considering , since , we can prove that is the space of skew-symmetric matrices by calculating I wondered if I could do something similar for , defining and , but I then realised that being a linear subspace of I would just find again. Is this a correct answer? Is itself its own associated Lie algebra (with the commutator as Lie bracket)? And what about positive-defined ones? What useful characterization may I take advantage of? Thanks in advance!","GL(n,\Bbb C)\cong U(n)\times Herm^+(n) Herm^+(n) Herm^+(n) F:SL(n,\Bbb R)\to SL(n,\Bbb R):A\mapsto A^\top A SO(n)=F^{-1}(I_n) \mathfrak{so}(n) \mathfrak{so}(n)=\ker (dF_{I_n})=\left\{A\in\Bbb R^{n\times n} : \left. \frac d{dt}\right\vert_{t=0}F(I_n+tA)=0\right\}. Herm(n) G:\Bbb C^{n\times n}\to\Bbb C^{n\times n}:A\mapsto A^*-A Herm(n)=G^{-1}(O_n) Herm(n) \Bbb C^{n\times n} Herm(n) Herm(n)","['linear-algebra', 'differential-geometry', 'lie-groups', 'lie-algebras']"
53,A question related to $S^{\perp}$ and closure of span of $S$,A question related to  and closure of span of,S^{\perp} S,"This question was asked in my linear algebra quiz previous year exam and I was unable to solve it. Let V be an inner ( in question it's written integer , but i think he means inner) product space and S be a subset of V. Let $\bar S$ denote the closure of S in V with respect to topology induced by the metric given by inner product. Which of the following statements are  true? A $ S $ = $ (S^{\bot})^{\bot}$ B $ \overline S$ = $(S^{\perp})^{\perp}$ C $\overline {\text{span}(S)}$ = $(S^{\bot})^{\bot}$ D $ S^{\bot} $ = $ ((S^{\bot})^{\bot})^{\bot}$ I was completely blank on how can I approach this problem although I have studied linear algebra carefully. Can you please tell on how I should approach the problem . Edit : I tried It again . I marked A ,D but answer is C,D. If A is false I don't see why D must be true. So, I think I am missing some concepts.","This question was asked in my linear algebra quiz previous year exam and I was unable to solve it. Let V be an inner ( in question it's written integer , but i think he means inner) product space and S be a subset of V. Let denote the closure of S in V with respect to topology induced by the metric given by inner product. Which of the following statements are  true? A = B = C = D = I was completely blank on how can I approach this problem although I have studied linear algebra carefully. Can you please tell on how I should approach the problem . Edit : I tried It again . I marked A ,D but answer is C,D. If A is false I don't see why D must be true. So, I think I am missing some concepts.",\bar S  S   (S^{\bot})^{\bot}  \overline S (S^{\perp})^{\perp} \overline {\text{span}(S)} (S^{\bot})^{\bot}  S^{\bot}   ((S^{\bot})^{\bot})^{\bot},"['linear-algebra', 'functional-analysis']"
54,Every complex matrix can be written as the sum of a Hermitian and a skew-Hermitian matrix. What is this called?,Every complex matrix can be written as the sum of a Hermitian and a skew-Hermitian matrix. What is this called?,,"Given $T\in\mathrm M_n(\mathbb C)$ , there exists $A$ and $B$ such that $A$ is Hermitian and $B$ is skew-Hermitian such that $T=A+B$ . Or, equivalently, $T=A+iB$ where $A$ and $B$ are both Hermitian matrices. I wonder what is this decomposition called. My friend recalls it is the Toeplitz decomposition, but I barely find any information on that. EDIT: The decomposition described above appears in many texts. To illustrate, here is an excerpt from page 145 of this article : In [52], Kaluznin and Havidi approach the problem of unitary similarity for $m$ -tuples, $(A_1,A_2,\ldots,A_n)$ and $(B_1,B_2,\ldots,B_n)$ , of square matrices from a more geometric point of view. First note that when we decompose each matrix into its Hermitian and skew Hermitian components , $A_j = H_j + iK$ , and $B_j = L_j + iM_j$ , where $H_j$ , $K_j$ , $L_j$ , and $M_j$ are all Hermitian, a unitary matrix $U$ satisfies $U^*A$ , $U = B_j$ if and only if $U^*H_jU = L_j$ and $U^*K_jU = M_j$ . Thus, we may replace each $A_j$ and $B_j$ with a pair of Hermitian matices and study the $2m$ -tuples in which every entry is Hermitian.","Given , there exists and such that is Hermitian and is skew-Hermitian such that . Or, equivalently, where and are both Hermitian matrices. I wonder what is this decomposition called. My friend recalls it is the Toeplitz decomposition, but I barely find any information on that. EDIT: The decomposition described above appears in many texts. To illustrate, here is an excerpt from page 145 of this article : In [52], Kaluznin and Havidi approach the problem of unitary similarity for -tuples, and , of square matrices from a more geometric point of view. First note that when we decompose each matrix into its Hermitian and skew Hermitian components , , and , where , , , and are all Hermitian, a unitary matrix satisfies , if and only if and . Thus, we may replace each and with a pair of Hermitian matices and study the -tuples in which every entry is Hermitian.","T\in\mathrm M_n(\mathbb C) A B A B T=A+B T=A+iB A B m (A_1,A_2,\ldots,A_n) (B_1,B_2,\ldots,B_n) A_j = H_j + iK B_j = L_j + iM_j H_j K_j L_j M_j U U^*A U = B_j U^*H_jU = L_j U^*K_jU = M_j A_j B_j 2m","['linear-algebra', 'matrices', 'terminology']"
55,"If a matrix and its transpose both have the same eigenvectors, is it necessarily symmetric?","If a matrix and its transpose both have the same eigenvectors, is it necessarily symmetric?",,"If a matrix and its transpose both have the same eigenvectors, is it necessarily symmetric? It's clear to see how if $A$ = $A^T$ , they would have the same eigenvectors, but is it the only way? And how would you show it?","If a matrix and its transpose both have the same eigenvectors, is it necessarily symmetric? It's clear to see how if = , they would have the same eigenvectors, but is it the only way? And how would you show it?",A A^T,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors', 'linear-transformations']"
56,"Skew-symmetric, Time Dependent, Linear Ordinary Differential Equations","Skew-symmetric, Time Dependent, Linear Ordinary Differential Equations",,"Let $I \subset \Bbb R \tag 1$ be an open interval, not necessarily bounded, and let $A(t)$ be a continuous matrix function of $t \in I$ , taking values in $M(n, \Bbb R)$ ; that is $A(t) \in C^0(I, M(n,, \Bbb R)); \tag 2$ suppose further that $A(t)$ is skew-symmetric for every $t$ : $A^T(t) = -A(t), \tag 3$ and consider the ordinary time-varying linear system $\dot{\vec x}(t) = A(t) \vec x(t), \tag 4$ where $\vec x(t) \in C^1(I, \Bbb R^n). \tag 5$ It is well-known, and easy to see, that when $A$ is a constant matrix the solutions to (4) are given by $\vec x(t) = e^{A(t - t_0)} \vec x(t_0), \tag 6$ where the matrix exponential is orthogonal; indeed we have $(e^{A(t - t_0)})^T e^{A(t - t_0)} = e^{A^T(t - t_0)} e^{A(t - t_0)} = e^{-A(t - t_0)}e^{A(t - t_0)} = I. \tag 7$ The fact that $e^{A(t - t_0)}$ is orthogonal leads to $\langle \vec x(t), \vec y(t) \rangle = \langle e^{A(t - t_0)} \vec x(t_0), e^{A(t - t_0)} \vec y(t_0) \rangle = \langle \vec x(t_0), \vec y(t_0) \rangle; \tag 8$ that is, the evolution of vectors according to (4) preserves inner products. We recall that $e^{A(t - t_0)}$ is a fundamental matrix solution of (4), and we observe that it takes the value $I$ at $t = t_0$ : $e^{A(t_0 - t_0)} = e^{A(0)} = I. \tag 9$ The purpose here is to investigate extending this observation to the case in which $A(t)$ is not a constant matrix. The Question is then:  given a system of the form (4), with $A(t)$ as in (3), show that a fundamental solution matrix $X(t, t_0)$ of the system (4) with $X(t_0, t_0) = I \tag{10}$ is orthogonal.  Conversely, show that a system (4) with orthogonal fundamental matrix satisfies (3).","Let be an open interval, not necessarily bounded, and let be a continuous matrix function of , taking values in ; that is suppose further that is skew-symmetric for every : and consider the ordinary time-varying linear system where It is well-known, and easy to see, that when is a constant matrix the solutions to (4) are given by where the matrix exponential is orthogonal; indeed we have The fact that is orthogonal leads to that is, the evolution of vectors according to (4) preserves inner products. We recall that is a fundamental matrix solution of (4), and we observe that it takes the value at : The purpose here is to investigate extending this observation to the case in which is not a constant matrix. The Question is then:  given a system of the form (4), with as in (3), show that a fundamental solution matrix of the system (4) with is orthogonal.  Conversely, show that a system (4) with orthogonal fundamental matrix satisfies (3).","I \subset \Bbb R \tag 1 A(t) t \in I M(n, \Bbb R) A(t) \in C^0(I, M(n,, \Bbb R)); \tag 2 A(t) t A^T(t) = -A(t), \tag 3 \dot{\vec x}(t) = A(t) \vec x(t), \tag 4 \vec x(t) \in C^1(I, \Bbb R^n). \tag 5 A \vec x(t) = e^{A(t - t_0)} \vec x(t_0), \tag 6 (e^{A(t - t_0)})^T e^{A(t - t_0)} = e^{A^T(t - t_0)} e^{A(t - t_0)} = e^{-A(t - t_0)}e^{A(t - t_0)} = I. \tag 7 e^{A(t - t_0)} \langle \vec x(t), \vec y(t) \rangle = \langle e^{A(t - t_0)} \vec x(t_0), e^{A(t - t_0)} \vec y(t_0) \rangle = \langle \vec x(t_0), \vec y(t_0) \rangle; \tag 8 e^{A(t - t_0)} I t = t_0 e^{A(t_0 - t_0)} = e^{A(0)} = I. \tag 9 A(t) A(t) X(t, t_0) X(t_0, t_0) = I \tag{10}","['linear-algebra', 'ordinary-differential-equations', 'lie-groups', 'lie-algebras']"
57,The squares of skew-symmetric matrices span all symmetric matrices,The squares of skew-symmetric matrices span all symmetric matrices,,"This is a self-answered question. I post this here, since it wasn't obvious for me at first, and I think it might be helpful for someone at some future time (maybe even future me...). Claim: Let $n \ge 3$ , and let $X$ be the set of all squares of real $n \times n$ skew-symmetric matrices. Then $\text{span}(X)$ is the space of all symmetric matrices. How to prove this claim?","This is a self-answered question. I post this here, since it wasn't obvious for me at first, and I think it might be helpful for someone at some future time (maybe even future me...). Claim: Let , and let be the set of all squares of real skew-symmetric matrices. Then is the space of all symmetric matrices. How to prove this claim?",n \ge 3 X n \times n \text{span}(X),"['linear-algebra', 'matrices', 'symmetric-matrices', 'skew-symmetric-matrices']"
58,Values of integer such that a matrix equality does hold,Values of integer such that a matrix equality does hold,,"The problem is to find all integer values $n\geq 2$ such that there exist two non-zero $n\times n$ real matrices $A,B$ satisfying $$A^2B-BA^2=A.$$ For $n=2$ such matrices do not exist. Therefore, I am a little bit puzzled on the path I should follow: that there are no such matrices for any $n$ , or to prove that such matrices exist, at least for some values of $n$ (maybe related to the parity of $n$ ...). I have managed to prove that if such matrices exist, then $\hbox{tr}(A)=0$ and $\det(A)=0$ .","The problem is to find all integer values such that there exist two non-zero real matrices satisfying For such matrices do not exist. Therefore, I am a little bit puzzled on the path I should follow: that there are no such matrices for any , or to prove that such matrices exist, at least for some values of (maybe related to the parity of ...). I have managed to prove that if such matrices exist, then and .","n\geq 2 n\times n A,B A^2B-BA^2=A. n=2 n n n \hbox{tr}(A)=0 \det(A)=0","['linear-algebra', 'matrices', 'matrix-equations', 'matrix-calculus']"
59,Proving there exist three different vectors that sum to zero,Proving there exist three different vectors that sum to zero,,"I'm trying to prove the following statement: Prove that for each $S \subset {\mathbb{Z}_3}^3$ satisfaying $|S|=9$ , there exist three different vectors $a, b, c \in S$ such that $a+b+c=\overrightarrow{0}$ . I know that a direct verification (which may be done by a computer) is an ""immediate"" proof. However, I'm interested in a simpler and more sophisticated proof. I've tried several approaches; The first one is linear algebra - I've placed the elements of $S$ in a $9 \times 3$ matrix and tried to show that there exist three rows which sum to zero using the fact that the rank of this matrix $\leq 3$ . It didn't work. The second approach I've tried is the pigeonhole principle, and it also didn't work. The third approach I've tried is the probabilistic method by randomly selecting three different vectors $a,b,c \in S$ and examining the three-dimensional random variable $a+b+c$ , using the fact that it equals the zero vector iff each coordinate is zero. I've also tried considering two cases: $\overrightarrow{0} \in S$ and $\overrightarrow{0} \not \in S$ . I've proved that there exists some coordinate $1 \leq i \leq 3$ such that there exist three vectors $a,b,c \in S$ such that their $i$ th coordinates are $-1, 0, 1$ . It might be useful. Thanks in advance.","I'm trying to prove the following statement: Prove that for each satisfaying , there exist three different vectors such that . I know that a direct verification (which may be done by a computer) is an ""immediate"" proof. However, I'm interested in a simpler and more sophisticated proof. I've tried several approaches; The first one is linear algebra - I've placed the elements of in a matrix and tried to show that there exist three rows which sum to zero using the fact that the rank of this matrix . It didn't work. The second approach I've tried is the pigeonhole principle, and it also didn't work. The third approach I've tried is the probabilistic method by randomly selecting three different vectors and examining the three-dimensional random variable , using the fact that it equals the zero vector iff each coordinate is zero. I've also tried considering two cases: and . I've proved that there exists some coordinate such that there exist three vectors such that their th coordinates are . It might be useful. Thanks in advance.","S \subset {\mathbb{Z}_3}^3 |S|=9 a, b, c \in S a+b+c=\overrightarrow{0} S 9 \times 3 \leq 3 a,b,c \in S a+b+c \overrightarrow{0} \in S \overrightarrow{0} \not \in S 1 \leq i \leq 3 a,b,c \in S i -1, 0, 1","['linear-algebra', 'combinatorics', 'pigeonhole-principle']"
60,Does the Fundamental Theorem of Algebra follow from the real Spectral Theorem?,Does the Fundamental Theorem of Algebra follow from the real Spectral Theorem?,,"I found a slick way to prove that every real normal matrix is orthogonally equivalent to a block diagonal form consisting of diagonal parts and $2 \times 2$ parts of the form $$\begin{pmatrix}a & b \\ -b & a\end{pmatrix}.$$ This seems awfully close to proving FTA - if every real polynomial is the characteristic polynomial of a normal matrix, then every real polynomial consequently splits over $\mathbb C$ . So my question is: Is there a  slick way to find a real normal matrix given a characteristic polynomial? Note that the companion matrix is not normal in general.","I found a slick way to prove that every real normal matrix is orthogonally equivalent to a block diagonal form consisting of diagonal parts and parts of the form This seems awfully close to proving FTA - if every real polynomial is the characteristic polynomial of a normal matrix, then every real polynomial consequently splits over . So my question is: Is there a  slick way to find a real normal matrix given a characteristic polynomial? Note that the companion matrix is not normal in general.",2 \times 2 \begin{pmatrix}a & b \\ -b & a\end{pmatrix}. \mathbb C,"['linear-algebra', 'abstract-algebra', 'general-topology']"
61,Bound on operator norm of block matrices,Bound on operator norm of block matrices,,"Let $A\in\mathbb{R}^{m\times n}$ be a block matrix which is partitioned into submatrices $A_{ij} \in \mathbb{R}^{m_i\times n_j}$ .  Let $\|\cdot\|_{p\times q}$ denote the spectral (operator) norm on $p\times q$ matrices. Is it true that $$\|A\|_{m\times n} \leq \sum_{i,j}\|A_{ij}\|_{m_i \times n_i}\:?$$",Let be a block matrix which is partitioned into submatrices .  Let denote the spectral (operator) norm on matrices. Is it true that,"A\in\mathbb{R}^{m\times n} A_{ij} \in \mathbb{R}^{m_i\times n_j} \|\cdot\|_{p\times q} p\times q \|A\|_{m\times n} \leq \sum_{i,j}\|A_{ij}\|_{m_i \times n_i}\:?","['linear-algebra', 'matrices', 'inequality', 'block-matrices', 'spectral-norm']"
62,Matrices with all non-zero entries.,Matrices with all non-zero entries.,,"I am reading a paper and it uses one of these facts, I would like to know if it has  a simple proof: Let $F$ be an infinite field and $n\ge2$ and integer. Then for any non-scalar matrices $A_1,A_2,...,A_k$ in $M_{n}(F)$ , there exist some invertible matrix $Q \in M_{n}(F)$ such that each matrix $QA_1Q^{-1}, QA_2Q^{-1},..., QA_{k}Q^{-1}$ have all non-zero entries. I just don't know where to start at the first place, could have used diagonalizability but not all non-scalar matrices are diagonalizable. Maybe its too simple, please help. Thanks in advance.","I am reading a paper and it uses one of these facts, I would like to know if it has  a simple proof: Let be an infinite field and and integer. Then for any non-scalar matrices in , there exist some invertible matrix such that each matrix have all non-zero entries. I just don't know where to start at the first place, could have used diagonalizability but not all non-scalar matrices are diagonalizable. Maybe its too simple, please help. Thanks in advance.","F n\ge2 A_1,A_2,...,A_k M_{n}(F) Q \in M_{n}(F) QA_1Q^{-1}, QA_2Q^{-1},..., QA_{k}Q^{-1}","['linear-algebra', 'matrices', 'ring-theory']"
63,"Prove that the set $\left\{ x, Ax, \dots, A^{k-1} x \right\}$ is linearly independent",Prove that the set  is linearly independent,"\left\{ x, Ax, \dots, A^{k-1} x \right\}","Problem: Let $A\in M_{n\times n}(\mathbb R)\,$ be a matrix and suppose that a positive number $k\,$ exists such that $A^k = 0\,$ and $A^{k-1} \neq 0$. Suppose that $x=\left[ \begin{matrix} x_1 \\ \vdots \\ x_n \end{matrix} \right]$ is a vector in $\mathbb{R^n}$ such that $A^{k-1} x \neq 0$. Prove that the $k\,$ vectors $\,x,Ax,\dots,A^{k-1}x\,$  are linearly independent. My attempt: Suppose $x + Ax + \dots + A^{k-1}x = 0$. Multiply both sides with $A^{k-1}$. Then we have $A^{k-1}x + A^k (x + Ax + \dots + A^{k-2}x) = 0 \Leftrightarrow A^{k-1}x = 0 \Leftrightarrow x = 0$ which implies $x + Ax + \dots + A^{k-1}x\,$ is linear independent. This problem looks quite easy but I want my proof to be checked. Is it correct?","Problem: Let $A\in M_{n\times n}(\mathbb R)\,$ be a matrix and suppose that a positive number $k\,$ exists such that $A^k = 0\,$ and $A^{k-1} \neq 0$. Suppose that $x=\left[ \begin{matrix} x_1 \\ \vdots \\ x_n \end{matrix} \right]$ is a vector in $\mathbb{R^n}$ such that $A^{k-1} x \neq 0$. Prove that the $k\,$ vectors $\,x,Ax,\dots,A^{k-1}x\,$  are linearly independent. My attempt: Suppose $x + Ax + \dots + A^{k-1}x = 0$. Multiply both sides with $A^{k-1}$. Then we have $A^{k-1}x + A^k (x + Ax + \dots + A^{k-2}x) = 0 \Leftrightarrow A^{k-1}x = 0 \Leftrightarrow x = 0$ which implies $x + Ax + \dots + A^{k-1}x\,$ is linear independent. This problem looks quite easy but I want my proof to be checked. Is it correct?",,"['linear-algebra', 'matrices', 'vector-spaces']"
64,Vector Space with unusual addition?,Vector Space with unusual addition?,,"I'm studying before my class starts in a few weeks and I encountered this question in one of the practice problems: The addition it has given me is defined as, $(a,b)+(c,d)= (ac,bd)$ It's asking me if this is a vector of space and I am stuck after proving this, There is an element $0$ in $V$ so that $v + 0 = v$ for all $v$ in $V$. I did this -> $(a,b)+(1,1) = (1a,1b) = (a,b)$ Stuck right here, For each $v$ in $V$ there is an element $-v$ in $V$ so that $v+(-v) = 0$. $(a,b)+(0,0) = (0a,0b) = (0,0)$ Is $(0,0)$ $a$ $-v$ when there's no such thing as '$-0$'? Do I stop proving right at the step? So this is not a vector of space? Thank you for your time. Edit: Thank you everyone! The question is stated exactly like so, Show that the set of ordered pairs of positive real numbers is a vector space under the addition and scalar multiplication.   $$(a,b)+(c,d) = (ac,bd),$$   $$c(a,b) = (a^c, b^c).$$ So the additive inverse is an element that, when added to $(a,b)$, will give me the additive identity, which in this case is $(1,1)$?","I'm studying before my class starts in a few weeks and I encountered this question in one of the practice problems: The addition it has given me is defined as, $(a,b)+(c,d)= (ac,bd)$ It's asking me if this is a vector of space and I am stuck after proving this, There is an element $0$ in $V$ so that $v + 0 = v$ for all $v$ in $V$. I did this -> $(a,b)+(1,1) = (1a,1b) = (a,b)$ Stuck right here, For each $v$ in $V$ there is an element $-v$ in $V$ so that $v+(-v) = 0$. $(a,b)+(0,0) = (0a,0b) = (0,0)$ Is $(0,0)$ $a$ $-v$ when there's no such thing as '$-0$'? Do I stop proving right at the step? So this is not a vector of space? Thank you for your time. Edit: Thank you everyone! The question is stated exactly like so, Show that the set of ordered pairs of positive real numbers is a vector space under the addition and scalar multiplication.   $$(a,b)+(c,d) = (ac,bd),$$   $$c(a,b) = (a^c, b^c).$$ So the additive inverse is an element that, when added to $(a,b)$, will give me the additive identity, which in this case is $(1,1)$?",,"['linear-algebra', 'vector-spaces']"
65,Why is the group of sphere rotations isomorphic to $SO(3)$?,Why is the group of sphere rotations isomorphic to ?,SO(3),"If we consider the sphere in $\mathbb{R}^3$, the set of all possible rotations of it forms a group isomorphic to $SO(3)$. That is, rotations of the sphere are exactly the orthogonal linear transformations from $\mathbb{R}^3$ to $\mathbb{R}^3$ with determinant $1$. I understand why rotations of the sphere are linear transformations, and I also understand that since they all preserve length they must be orthogonal. What I don't understand is why are they necessarily of determinant $1$? My intuition is that if we allow for negative determinant that would correspond to some reflection of the sphere, which would reverse its 'orientation' and that's impossible for rotation. I don't know how to make this intuition precise and follow up on it though. Is there a way to precisely define the 'orientation' of a space and 'orientation-preserving transformations' and then conclude such transformations have non-negative determinants? If not, is there some other geometric notion that would explain why sphere rotations can't have determinant $-1$?","If we consider the sphere in $\mathbb{R}^3$, the set of all possible rotations of it forms a group isomorphic to $SO(3)$. That is, rotations of the sphere are exactly the orthogonal linear transformations from $\mathbb{R}^3$ to $\mathbb{R}^3$ with determinant $1$. I understand why rotations of the sphere are linear transformations, and I also understand that since they all preserve length they must be orthogonal. What I don't understand is why are they necessarily of determinant $1$? My intuition is that if we allow for negative determinant that would correspond to some reflection of the sphere, which would reverse its 'orientation' and that's impossible for rotation. I don't know how to make this intuition precise and follow up on it though. Is there a way to precisely define the 'orientation' of a space and 'orientation-preserving transformations' and then conclude such transformations have non-negative determinants? If not, is there some other geometric notion that would explain why sphere rotations can't have determinant $-1$?",,"['linear-algebra', 'group-theory', 'geometry', 'linear-transformations']"
66,Why $\displaystyle\lim_{n\to+\infty}x_n\otimes y_n=x\otimes y\;?$,Why,\displaystyle\lim_{n\to+\infty}x_n\otimes y_n=x\otimes y\;?,"Let  $(E,\langle \cdot,\cdot\rangle_1)$, $(F,\langle \cdot,\cdot\rangle_2)$ be two complex Hilbert spaces. We recall $$E \otimes F:=\left\{\xi=\sum_{i=1}^dv_i\otimes w_i:\;d\in \mathbb{N},\;\;v_i\in E,\;\;w_i\in F \right\}.$$ We endow $E \otimes F$, with the following inner product $$ \langle \xi,\eta\rangle=\sum_{i=1}^n\sum_{j=1}^m \langle x_i,z_j\rangle_1\langle y_i ,t_j\rangle_2, $$ for $\xi=\displaystyle\sum_{i=1}^nx_i\otimes y_i\in E \otimes F$ and $\eta=\displaystyle\sum_{j=1}^mz_j\otimes w_j\in E \otimes F$. Let $(x_n)_n\subset E$ and $(y_n)_n\subset F$ such that $\displaystyle\lim_{n\to+\infty}x_n=x$ and $\displaystyle\lim_{n\to+\infty}y_n=y$. Why   $$\displaystyle\lim_{n\to+\infty}x_n\otimes y_n=x\otimes y\;?$$ Thank you.","Let  $(E,\langle \cdot,\cdot\rangle_1)$, $(F,\langle \cdot,\cdot\rangle_2)$ be two complex Hilbert spaces. We recall $$E \otimes F:=\left\{\xi=\sum_{i=1}^dv_i\otimes w_i:\;d\in \mathbb{N},\;\;v_i\in E,\;\;w_i\in F \right\}.$$ We endow $E \otimes F$, with the following inner product $$ \langle \xi,\eta\rangle=\sum_{i=1}^n\sum_{j=1}^m \langle x_i,z_j\rangle_1\langle y_i ,t_j\rangle_2, $$ for $\xi=\displaystyle\sum_{i=1}^nx_i\otimes y_i\in E \otimes F$ and $\eta=\displaystyle\sum_{j=1}^mz_j\otimes w_j\in E \otimes F$. Let $(x_n)_n\subset E$ and $(y_n)_n\subset F$ such that $\displaystyle\lim_{n\to+\infty}x_n=x$ and $\displaystyle\lim_{n\to+\infty}y_n=y$. Why   $$\displaystyle\lim_{n\to+\infty}x_n\otimes y_n=x\otimes y\;?$$ Thank you.",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces', 'tensor-products']"
67,Let $A$ be a real $4\times4$ matrix such that $A^2+A+I = 0$. Can $A$ be orthogonal?,Let  be a real  matrix such that . Can  be orthogonal?,A 4\times4 A^2+A+I = 0 A,I'm struggling on the third point of this exercise: Let $A$ be a $4 \times 4$ matrix such that $A^2 + A + I=0$. Are this statements true or false? $A$ is invertible $A$ can be skew-symmetric $A$ can be orthogonal This is what i have done so far: $A^2 + A = -I \Rightarrow det(A^2+A)=det(A) \cdot det(A+I) = 1$ hence $det(A) \neq 0$ so the first statement is true. For the second i noticed that $-1$ is an eigenvalue for $A^2+A$ and its geometric multiplicity is $4$ so also the algebraic multiplicity is $4$. Hence $-1$ is the only eigenvalue of $A^2+A$. Now let $\lambda$ be an eigenvalue of $A$ and $v \neq 0$ be a $\lambda$-eigenvector: $$(A^2+A)v = A^2v+Av=\lambda^2v+\lambda v = (\lambda^2 +\lambda)v$$  hence $\lambda^2+\lambda$ is an eigenvalue of $A^2+A$ so $\lambda^2+\lambda=-1 \Rightarrow \lambda = -\displaystyle\frac{1}{2} \pm \frac{\sqrt3}{2}i $. Since skew-symmetric matrices has only pure imaginary eigenvalues the second statement is false. For the third point i have seen that $det(A) = \left(-\displaystyle\frac{1}{2} + \frac{\sqrt3}{2}i\right)\left(-\displaystyle\frac{1}{2} - \frac{\sqrt3}{2}i\right) = 1$ and also that $\mid\lambda\mid = 1$ thus i believe that the third is true but i can't find an example of an orthogonal matrix. Am i missing something? Have i made any mistake in my reasoning? Thank you very much in advance!,I'm struggling on the third point of this exercise: Let $A$ be a $4 \times 4$ matrix such that $A^2 + A + I=0$. Are this statements true or false? $A$ is invertible $A$ can be skew-symmetric $A$ can be orthogonal This is what i have done so far: $A^2 + A = -I \Rightarrow det(A^2+A)=det(A) \cdot det(A+I) = 1$ hence $det(A) \neq 0$ so the first statement is true. For the second i noticed that $-1$ is an eigenvalue for $A^2+A$ and its geometric multiplicity is $4$ so also the algebraic multiplicity is $4$. Hence $-1$ is the only eigenvalue of $A^2+A$. Now let $\lambda$ be an eigenvalue of $A$ and $v \neq 0$ be a $\lambda$-eigenvector: $$(A^2+A)v = A^2v+Av=\lambda^2v+\lambda v = (\lambda^2 +\lambda)v$$  hence $\lambda^2+\lambda$ is an eigenvalue of $A^2+A$ so $\lambda^2+\lambda=-1 \Rightarrow \lambda = -\displaystyle\frac{1}{2} \pm \frac{\sqrt3}{2}i $. Since skew-symmetric matrices has only pure imaginary eigenvalues the second statement is false. For the third point i have seen that $det(A) = \left(-\displaystyle\frac{1}{2} + \frac{\sqrt3}{2}i\right)\left(-\displaystyle\frac{1}{2} - \frac{\sqrt3}{2}i\right) = 1$ and also that $\mid\lambda\mid = 1$ thus i believe that the third is true but i can't find an example of an orthogonal matrix. Am i missing something? Have i made any mistake in my reasoning? Thank you very much in advance!,,"['linear-algebra', 'matrices', 'proof-verification', 'orthogonality']"
68,Lower bound for a trace of a matrix product,Lower bound for a trace of a matrix product,,Let $A$ and $B$ be positive definite Hermitian matrices. They need not necessarily commute. Let $a_m$ be the minimum eigenvalue of $A$. Is it true that  $$ \text{Tr} (AB) \geq a_m \text{Tr} (B)  $$ My attempt: let $a_i$ and $b_i$ be the eigenvalues (which are all real and positive) of $A$ and $B$. If $A$ and $B$ are simultaneously diagonalizable by a similarity transformation then $ \text{Tr} (AB) = \sum_i a_i b_i \geq  a_m \sum_i b_i = a_m \text{Tr} (B)  $. But is this generalizable to arbitrary $A$ and $B$ as laid out above?,Let $A$ and $B$ be positive definite Hermitian matrices. They need not necessarily commute. Let $a_m$ be the minimum eigenvalue of $A$. Is it true that  $$ \text{Tr} (AB) \geq a_m \text{Tr} (B)  $$ My attempt: let $a_i$ and $b_i$ be the eigenvalues (which are all real and positive) of $A$ and $B$. If $A$ and $B$ are simultaneously diagonalizable by a similarity transformation then $ \text{Tr} (AB) = \sum_i a_i b_i \geq  a_m \sum_i b_i = a_m \text{Tr} (B)  $. But is this generalizable to arbitrary $A$ and $B$ as laid out above?,,"['linear-algebra', 'spectral-theory']"
69,Proof of Gelfand's formula without using $\rho(A) < 1$ iff $\lim A^n = 0$,Proof of Gelfand's formula without using  iff,\rho(A) < 1 \lim A^n = 0,"Gelfand's formula states that the spectral radius $\rho(A)$ of a square matrix $A$ satisfies $$\rho(A) = \lim_{n \to \infty} \|A^n\|^{\frac{1}{n}}$$ The standard proof relies on knowing that $\rho(A) < 1$ iff $\lim_{n \rightarrow \infty} A^n = 0$. Is there a proof of Gelfand's formula without using this result, whose proof happens to rely on a burdensome use of Jordan Normal form.","Gelfand's formula states that the spectral radius $\rho(A)$ of a square matrix $A$ satisfies $$\rho(A) = \lim_{n \to \infty} \|A^n\|^{\frac{1}{n}}$$ The standard proof relies on knowing that $\rho(A) < 1$ iff $\lim_{n \rightarrow \infty} A^n = 0$. Is there a proof of Gelfand's formula without using this result, whose proof happens to rely on a burdensome use of Jordan Normal form.",,"['linear-algebra', 'matrices', 'matrix-calculus', 'spectral-radius']"
70,Why do $A$ and $A^TA$ have the same row space?,Why do  and  have the same row space?,A A^TA,Theorem: Let $A$ be an $m \times n$ real matrix. Then $A$ and $A^T A$ have the same row space. I am trying to derive and intuitively understand why this theorem holds. Would appreciate any help. Thanks.,Theorem: Let $A$ be an $m \times n$ real matrix. Then $A$ and $A^T A$ have the same row space. I am trying to derive and intuitively understand why this theorem holds. Would appreciate any help. Thanks.,,"['linear-algebra', 'matrices']"
71,Easiest way to find characteristic polynomial for this 4x4 matrix,Easiest way to find characteristic polynomial for this 4x4 matrix,,"I have been given the matrix $$ \begin{bmatrix} 1 & 3 & 0 & 3 \\ 1 & 1 & 1 & 1 \\ 0 & 4 & 2 & 8 \\ 2 & 0 & 3 & 1 \\ \end{bmatrix} $$ and told I must find the characteristic polynomial. I began by applying cofactor expansion along the top row  of the matrix $$ \begin{bmatrix} 1-\lambda & 3 & 0 & 3 \\ 1 & 1-\lambda & 1 & 1 \\ 0 & 4 & 2-\lambda & 8 \\ 2 & 0 & 3 & 1-\lambda \\ \end{bmatrix} $$ and attempting to multiply out my results to get the correct answer of $\lambda^4 -5\lambda^3 - 28\lambda^2 + 58\lambda - 8$. However, this takes several pages of work and I keep making calculation errors and ending up with the wrong answer. My question is, is there an easier way to find the determinant of this specific matrix, or, once the determinant is found, to multiply out the result to find the polynomial? The only methods I have been taught have been to either try to find or create a row with several 0's to make the cofactor expansion easier, or to get an upper or lower triangular matrix, however, those seem equally as messy here.","I have been given the matrix $$ \begin{bmatrix} 1 & 3 & 0 & 3 \\ 1 & 1 & 1 & 1 \\ 0 & 4 & 2 & 8 \\ 2 & 0 & 3 & 1 \\ \end{bmatrix} $$ and told I must find the characteristic polynomial. I began by applying cofactor expansion along the top row  of the matrix $$ \begin{bmatrix} 1-\lambda & 3 & 0 & 3 \\ 1 & 1-\lambda & 1 & 1 \\ 0 & 4 & 2-\lambda & 8 \\ 2 & 0 & 3 & 1-\lambda \\ \end{bmatrix} $$ and attempting to multiply out my results to get the correct answer of $\lambda^4 -5\lambda^3 - 28\lambda^2 + 58\lambda - 8$. However, this takes several pages of work and I keep making calculation errors and ending up with the wrong answer. My question is, is there an easier way to find the determinant of this specific matrix, or, once the determinant is found, to multiply out the result to find the polynomial? The only methods I have been taught have been to either try to find or create a row with several 0's to make the cofactor expansion easier, or to get an upper or lower triangular matrix, however, those seem equally as messy here.",,"['linear-algebra', 'matrices', 'determinant']"
72,Connection Between the Traditional Equation of a Line ($y = mx + c$) and the Vector Equation of a Line,Connection Between the Traditional Equation of a Line () and the Vector Equation of a Line,y = mx + c,"My textbook says, ""... an equation of the line through $A$ and $B$ is $x = u + tw = u + t(v — u)$, where $t$ is a real number and $x$ denotes an arbitrary point on the line. An illustration of this is shown below: Since $x = u + tw$ is the equation of a line, I presume that it is equivalent to the equation $y = mx + c$, where $m$ is the slope, $x$ is the x-intercept (when $y = 0$), and $c$ is the $y-$intercept (when $x = 0$).  Therefore, in the equation $x = u + tw$, $w$ must represent the $x-$intercept $($when $x = 0), u$ must represent the y-intercept (when $w = 0$), and $t$ must represent the slope. It is clear that there is a connection between the traditional equation of a line, $y = mx + c$, and the vector equation of a line, $x = u + tw = u + t(v — u)$. However, I am struggling to clearly understand the connection between the vectors and the traditional equation: How can these vectors represent slope ($m$)? How can these vectors represent x- and y-intercepts? When I look at the diagram and the vector equations of a line, there does not seem to be a clear indication of how they relate to the cartesian illustration of the traditional equation of a line ($y = mx + c$). For instance, the vectors $w$ and $u$ in $x = u + tw$ would represent the $x$ and $y$ intercepts; But how can a vector represent the $x$ or $y$ intercepts? This makes no sense to me. To provide further context, take the vector equation of a line $x = (-2,0.1)-t(6,5,2)$. In this case, $(-2,0.1) = u$ and $(6,5,2) = w$; But in what way do the vectors $u$ and $w$ represent $y$ and $x$ intercepts? This is not clear to me. I would greatly appreciate it if the knowledgeable members of MSE could please take the time to clarify the connection between these two forms of a line. Thank you.","My textbook says, ""... an equation of the line through $A$ and $B$ is $x = u + tw = u + t(v — u)$, where $t$ is a real number and $x$ denotes an arbitrary point on the line. An illustration of this is shown below: Since $x = u + tw$ is the equation of a line, I presume that it is equivalent to the equation $y = mx + c$, where $m$ is the slope, $x$ is the x-intercept (when $y = 0$), and $c$ is the $y-$intercept (when $x = 0$).  Therefore, in the equation $x = u + tw$, $w$ must represent the $x-$intercept $($when $x = 0), u$ must represent the y-intercept (when $w = 0$), and $t$ must represent the slope. It is clear that there is a connection between the traditional equation of a line, $y = mx + c$, and the vector equation of a line, $x = u + tw = u + t(v — u)$. However, I am struggling to clearly understand the connection between the vectors and the traditional equation: How can these vectors represent slope ($m$)? How can these vectors represent x- and y-intercepts? When I look at the diagram and the vector equations of a line, there does not seem to be a clear indication of how they relate to the cartesian illustration of the traditional equation of a line ($y = mx + c$). For instance, the vectors $w$ and $u$ in $x = u + tw$ would represent the $x$ and $y$ intercepts; But how can a vector represent the $x$ or $y$ intercepts? This makes no sense to me. To provide further context, take the vector equation of a line $x = (-2,0.1)-t(6,5,2)$. In this case, $(-2,0.1) = u$ and $(6,5,2) = w$; But in what way do the vectors $u$ and $w$ represent $y$ and $x$ intercepts? This is not clear to me. I would greatly appreciate it if the knowledgeable members of MSE could please take the time to clarify the connection between these two forms of a line. Thank you.",,"['linear-algebra', 'geometry', 'vectors', 'analytic-geometry']"
73,Trace of adjugate,Trace of adjugate,,"Let $A \in M(\mathbb{R}, n)$ and $C_A(\lambda)$ its characteristic polynomial. Let $$ \Gamma_A(\lambda) := (-1)^{n+1}\frac{C_A(\lambda) - C_A(0)}{\lambda} $$ Then, $\text{adj}(A) = \Gamma_A(A)$ which is a form of the Cramer's rule $\text{adj}(A)A = \det(A) I$ using the Cayley-Hamilton theorem. There is an identitiy that $$ \text{tr} ( \text{adj} (A) ) = (-1)^{n+1} \Gamma_A(0)$$ Question: how is this identity derived? Background: I was studying Lemma 1.4 of this book on page 86, claim 6.","Let $A \in M(\mathbb{R}, n)$ and $C_A(\lambda)$ its characteristic polynomial. Let $$ \Gamma_A(\lambda) := (-1)^{n+1}\frac{C_A(\lambda) - C_A(0)}{\lambda} $$ Then, $\text{adj}(A) = \Gamma_A(A)$ which is a form of the Cramer's rule $\text{adj}(A)A = \det(A) I$ using the Cayley-Hamilton theorem. There is an identitiy that $$ \text{tr} ( \text{adj} (A) ) = (-1)^{n+1} \Gamma_A(0)$$ Question: how is this identity derived? Background: I was studying Lemma 1.4 of this book on page 86, claim 6.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
74,Eigenvalues of $AB$ and $BA$ are the same?,Eigenvalues of  and  are the same?,AB BA,"In the MIT linear algebra online lecture, when doing SVD, Gilbert Strang said that the eigenvalues of $AB$ and $BA$ are the same. I was trying to prove this as follows: Let $A$ be $m \times n$ matrix and $B$ be $n \times m$ matrix. Then $AB$ is $m \times m$ and $BA$ is $n \times n$. Let $$ABx=\lambda x$$ Then $$BA(Bx)=\lambda(Bx)$$ and $\lambda$ is an eigenvalue of $BA$ as well, and vice versa. Q.E.D. However, after a second thought I think the above proof has a pitfall. Namely, if $x$ is in the null-space of $B$ then $BA$ needs not have eigenvalue $\lambda$. So my questions is: Is the statement that $AB$ and $BA$ have the same eigenvalues true for general $m \times n$ matrix $A$ and $n \times m$ matrix $B$? If yes, how to prove it? If no, is it true for the special case when $B=A^\dagger$? And how to prove it?","In the MIT linear algebra online lecture, when doing SVD, Gilbert Strang said that the eigenvalues of $AB$ and $BA$ are the same. I was trying to prove this as follows: Let $A$ be $m \times n$ matrix and $B$ be $n \times m$ matrix. Then $AB$ is $m \times m$ and $BA$ is $n \times n$. Let $$ABx=\lambda x$$ Then $$BA(Bx)=\lambda(Bx)$$ and $\lambda$ is an eigenvalue of $BA$ as well, and vice versa. Q.E.D. However, after a second thought I think the above proof has a pitfall. Namely, if $x$ is in the null-space of $B$ then $BA$ needs not have eigenvalue $\lambda$. So my questions is: Is the statement that $AB$ and $BA$ have the same eigenvalues true for general $m \times n$ matrix $A$ and $n \times m$ matrix $B$? If yes, how to prove it? If no, is it true for the special case when $B=A^\dagger$? And how to prove it?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
75,To show: $\det\left[\begin{smallmatrix} -bc & b^2+bc & c^2+bc\\ a^2+ac & -ac & c^2+ac \\ a^2+ab & b^2+ab & -ab \end{smallmatrix}\right]=(ab+bc+ca)^3$,To show:,\det\left[\begin{smallmatrix} -bc & b^2+bc & c^2+bc\\ a^2+ac & -ac & c^2+ac \\ a^2+ab & b^2+ab & -ab \end{smallmatrix}\right]=(ab+bc+ca)^3,"I've been having quite some trouble with this question. I'm required to show that the below equation holds, by using properties of determinants (i.e. not allowed to directly expand the determinant before greatly simplifying it). $$\begin{vmatrix} -bc & b^2+bc & c^2+bc\\  a^2+ac & -ac & c^2+ac \\ a^2+ab & b^2+ab & -ab \end{vmatrix}= (ab+bc+ca)^3$$ I tried everything: $R_1\to R_1+R_2+R_3$ and similar transformations to extract that $ab+bc+ca$ term, but to no avail. $C_2\to C_1+C_2$ and $C_3\to C_3+C_1$ seemed to be a good lead, but I couldn't follow up. How can I solve this question?","I've been having quite some trouble with this question. I'm required to show that the below equation holds, by using properties of determinants (i.e. not allowed to directly expand the determinant before greatly simplifying it). I tried everything: and similar transformations to extract that term, but to no avail. and seemed to be a good lead, but I couldn't follow up. How can I solve this question?","\begin{vmatrix}
-bc & b^2+bc & c^2+bc\\ 
a^2+ac & -ac & c^2+ac \\
a^2+ab & b^2+ab & -ab
\end{vmatrix}= (ab+bc+ca)^3 R_1\to R_1+R_2+R_3 ab+bc+ca C_2\to C_1+C_2 C_3\to C_3+C_1","['linear-algebra', 'matrices', 'determinant']"
76,Finding matrix BA given AB,Finding matrix BA given AB,,"Given a matrix $$AB = \begin{bmatrix}-2&-14&14\\5&15&-10\\4&8&-3\end{bmatrix},$$ where $A$ is a $3\times 2$ matrix, and $B$ a $2\times 3$ matrix, how do I find the matrix $BA$ ? I was told to find the basis for the rowspace of $AB$ , and that $(AB)^2 = 5AB$ . However, I do not see how these 2 informations can help me find $BA$ at all. Any help would be appreciated.","Given a matrix where is a matrix, and a matrix, how do I find the matrix ? I was told to find the basis for the rowspace of , and that . However, I do not see how these 2 informations can help me find at all. Any help would be appreciated.","AB = \begin{bmatrix}-2&-14&14\\5&15&-10\\4&8&-3\end{bmatrix}, A 3\times 2 B 2\times 3 BA AB (AB)^2 = 5AB BA","['linear-algebra', 'matrices']"
77,Show that the dual space of the vector space of all polynomials is isomorphic to the infinite-dimensional Euclidean vector space over the reals,Show that the dual space of the vector space of all polynomials is isomorphic to the infinite-dimensional Euclidean vector space over the reals,,May I please ask how to show the dual space of the vector space of all polynomials is isomorphic to the infinite-dimensional Euclidean vector space over the reals? (i.e. Show that $(\Bbb{R}[X])^*$ and $\Bbb{R}^{∞}$ are isomorphic where $\Bbb{R}$ means the set of all reals),May I please ask how to show the dual space of the vector space of all polynomials is isomorphic to the infinite-dimensional Euclidean vector space over the reals? (i.e. Show that $(\Bbb{R}[X])^*$ and $\Bbb{R}^{∞}$ are isomorphic where $\Bbb{R}$ means the set of all reals),,"['linear-algebra', 'polynomials']"
78,"If $\operatorname{rank} (AB) = \operatorname{rank} (BA)$ for any $B$, then is $A$ invertible?","If  for any , then is  invertible?",\operatorname{rank} (AB) = \operatorname{rank} (BA) B A,Let $A$ and $B$ be two (nonzero) real square matrices and suppose that $\operatorname{rank} (AB) = \operatorname{rank} (BA)$ for any $B$ . Can one prove that $A$ is invertible? (The converse is true and a simple linear algebra question but I'm stuck on this one.),Let and be two (nonzero) real square matrices and suppose that for any . Can one prove that is invertible? (The converse is true and a simple linear algebra question but I'm stuck on this one.),A B \operatorname{rank} (AB) = \operatorname{rank} (BA) B A,"['linear-algebra', 'matrices', 'inverse', 'matrix-rank']"
79,"""Vanishing inner product implies orthogonality"" Is it a definition or theorem?","""Vanishing inner product implies orthogonality"" Is it a definition or theorem?",,"The definition of inner product is $\langle u,v\rangle=u_1\bar{v_1}+\cdots+u_n\bar{v_n}$ . Two vectors $u,v \in V$ are said to be orthogonal if $\langle u,v\rangle=0$ , where $V$ is complex vector space. But why is it true? A lot of books just present this fact without giving any proof. Why such an expression $u_1\bar{v_1}+\cdots+u_n\bar{v_n}=0$ will lead to the fact that $u$ and $v$ are orthogonal? Is it an intuition behind the expression $u_1\bar{v_1}+\cdots+u_n\bar{v_n}$ that can explain orthogonality? I read some proof about it but they don't seem correct. For example, some said that $\langle u,v\rangle=|u||v|\cos\theta$ and thus $\theta=90^\circ$ . But why are they equivalent? Some use the Pythagoras theorem, $$\begin{array}{rl} |u+v|^2 &=\langle u+v,u+v\rangle\\ &=|u|^2+2\langle u,v\rangle+|v|^2 \end{array}$$ So dot product $= 0$ implies orthogonality, but this only works for real vector space. So why exactly inner product $= 0$ implies orthogonality?","The definition of inner product is . Two vectors are said to be orthogonal if , where is complex vector space. But why is it true? A lot of books just present this fact without giving any proof. Why such an expression will lead to the fact that and are orthogonal? Is it an intuition behind the expression that can explain orthogonality? I read some proof about it but they don't seem correct. For example, some said that and thus . But why are they equivalent? Some use the Pythagoras theorem, So dot product implies orthogonality, but this only works for real vector space. So why exactly inner product implies orthogonality?","\langle u,v\rangle=u_1\bar{v_1}+\cdots+u_n\bar{v_n} u,v \in V \langle u,v\rangle=0 V u_1\bar{v_1}+\cdots+u_n\bar{v_n}=0 u v u_1\bar{v_1}+\cdots+u_n\bar{v_n} \langle u,v\rangle=|u||v|\cos\theta \theta=90^\circ \begin{array}{rl}
|u+v|^2 &=\langle u+v,u+v\rangle\\
&=|u|^2+2\langle u,v\rangle+|v|^2
\end{array} = 0 = 0","['linear-algebra', 'inner-products']"
80,Relation between linear independence and inner product,Relation between linear independence and inner product,,"I was given the following question: Let $V$ be an inner product space and let $u,v\in V$ be two nonzero vectors. Prove or disprove: If $\langle u,v\rangle=0$, then $u,v$ are linearly independent. If $u,v$ are independent, then $\langle u,v\rangle=0$. I know that $u,v$ are arthogonal if $\langle u,v\rangle = 0$. So, since $\langle u,v\rangle = 0$, and $u,v$ are non zero vectors can I claim linear independence between the vectors directly? And if so, how do I explain it? This just seems wrong... I don't see how linear independence leads to this vectors having inner product of zero, meaning they are orthogonal. Any help or direction would be very helpful.","I was given the following question: Let $V$ be an inner product space and let $u,v\in V$ be two nonzero vectors. Prove or disprove: If $\langle u,v\rangle=0$, then $u,v$ are linearly independent. If $u,v$ are independent, then $\langle u,v\rangle=0$. I know that $u,v$ are arthogonal if $\langle u,v\rangle = 0$. So, since $\langle u,v\rangle = 0$, and $u,v$ are non zero vectors can I claim linear independence between the vectors directly? And if so, how do I explain it? This just seems wrong... I don't see how linear independence leads to this vectors having inner product of zero, meaning they are orthogonal. Any help or direction would be very helpful.",,"['linear-algebra', 'inner-products']"
81,Equivalent characterizations of the dual norm on finite dimensional vector spaces,Equivalent characterizations of the dual norm on finite dimensional vector spaces,,"In their book on Convex Optimization, Boyd and Vandenberghe state that given a norm, $||\cdot||$, defined on $\mathbb{R}^n$, the dual norm is defined as $$||z||_*= \sup \{ z^Tx : ||x|| \leq 1 \}$$ In other places, I have encountered an equivalent characterization of the dual norm as $$||z||_*= \sup_{x \neq 0} \displaystyle\frac{z^Tx}{||x||}$$ I don't actually see how these two things are equivalent, even though this is said to be a simple one-liner. In particular, what's confusing to me is that I would have argued the following, even though this seems to not be correct:  By norm homogeneity, the set we're taking the supremum over is invariant to dilations.  That is, for any $\alpha >0$, we have $$\displaystyle\frac{z^T(\alpha x)}{||\alpha x||} = \displaystyle\frac{\alpha (z^Tx)}{ |\alpha|  ||x||} = \displaystyle\frac{z^Tx}{||x||}$$ and therefore we may find the supremum of the set by merely considering the $x$ values with some constant norm, for example, the unit norm:  $$||z||_*= \sup_{x \neq 0} \displaystyle\frac{z^Tx}{||x||} = \sup_{x : ||x||=1} \displaystyle\frac{z^Tx}{||x||} = \sup \{ z^T x : ||x|| = 1\}$$. Why is this logic incorrect?","In their book on Convex Optimization, Boyd and Vandenberghe state that given a norm, $||\cdot||$, defined on $\mathbb{R}^n$, the dual norm is defined as $$||z||_*= \sup \{ z^Tx : ||x|| \leq 1 \}$$ In other places, I have encountered an equivalent characterization of the dual norm as $$||z||_*= \sup_{x \neq 0} \displaystyle\frac{z^Tx}{||x||}$$ I don't actually see how these two things are equivalent, even though this is said to be a simple one-liner. In particular, what's confusing to me is that I would have argued the following, even though this seems to not be correct:  By norm homogeneity, the set we're taking the supremum over is invariant to dilations.  That is, for any $\alpha >0$, we have $$\displaystyle\frac{z^T(\alpha x)}{||\alpha x||} = \displaystyle\frac{\alpha (z^Tx)}{ |\alpha|  ||x||} = \displaystyle\frac{z^Tx}{||x||}$$ and therefore we may find the supremum of the set by merely considering the $x$ values with some constant norm, for example, the unit norm:  $$||z||_*= \sup_{x \neq 0} \displaystyle\frac{z^Tx}{||x||} = \sup_{x : ||x||=1} \displaystyle\frac{z^Tx}{||x||} = \sup \{ z^T x : ||x|| = 1\}$$. Why is this logic incorrect?",,"['linear-algebra', 'normed-spaces', 'convex-optimization']"
82,Intersection of 2D planes in 4D space,Intersection of 2D planes in 4D space,,"If I had a four dimensional space, in which I embedded two planes, what possible intersections could they have? Constructing a Plane To give this more context, consider the following. If I had a 4d tuple (w, x, y, z) with no restriction, it can assume any position in 4D space. By imposing one restriction, and then another, we can reduce the degrees of freedom by two, such that I end up with a 2D plane in 4D space, we can write this pair of restrictions in matrix form as: $$ \begin{bmatrix} p_1 & p_2 & p_3 & p_4 \\ q_1 & q_2 & q_3 & q_4  \end{bmatrix} \begin{bmatrix} w \\x \\ y\\ z  \end{bmatrix} = \begin{bmatrix} c_1 \\ c_2  \end{bmatrix}  $$ Now if this forms a plane, then the rank of the matrix should be two dimensional, as there will only be two linearly independent column vectors. I believe this is a condition for the plane to be two dimensional. Intersecting Planes Now I'd like to consider the result when I intersect that plane with the plane represented by: $$ \begin{bmatrix} r_1 & r_2 & r_3 & r_4 \\ s_1 & s_2 & s_3 & s_4  \end{bmatrix} \begin{bmatrix} w \\x \\ y\\ z  \end{bmatrix} = \begin{bmatrix} c_3 \\ c_4  \end{bmatrix}  $$ I've already established that this will display weird behaviour. For example, if we take the xy plane and the wz planes and intersect them, there is only one solution (the zero vector), indicating that they intersect at a plane. Getting Insights If we constructed this final matrix, what restrictions can we apply knowing that the original matrix had a rank of 2? $$ \begin{bmatrix} p_1 & p_2 & p_3 & p_4 \\ q_1 & q_2 & q_3 & q_4 \\r_1 & r_2 & r_3 & r_4 \\ s_1 & s_2 & s_3 & s_4  \end{bmatrix} \begin{bmatrix} w \\x \\ y\\ z  \end{bmatrix} = \begin{bmatrix}c_1 \\ c_2\\ c_3 \\ c_4  \end{bmatrix}  $$","If I had a four dimensional space, in which I embedded two planes, what possible intersections could they have? Constructing a Plane To give this more context, consider the following. If I had a 4d tuple (w, x, y, z) with no restriction, it can assume any position in 4D space. By imposing one restriction, and then another, we can reduce the degrees of freedom by two, such that I end up with a 2D plane in 4D space, we can write this pair of restrictions in matrix form as: $$ \begin{bmatrix} p_1 & p_2 & p_3 & p_4 \\ q_1 & q_2 & q_3 & q_4  \end{bmatrix} \begin{bmatrix} w \\x \\ y\\ z  \end{bmatrix} = \begin{bmatrix} c_1 \\ c_2  \end{bmatrix}  $$ Now if this forms a plane, then the rank of the matrix should be two dimensional, as there will only be two linearly independent column vectors. I believe this is a condition for the plane to be two dimensional. Intersecting Planes Now I'd like to consider the result when I intersect that plane with the plane represented by: $$ \begin{bmatrix} r_1 & r_2 & r_3 & r_4 \\ s_1 & s_2 & s_3 & s_4  \end{bmatrix} \begin{bmatrix} w \\x \\ y\\ z  \end{bmatrix} = \begin{bmatrix} c_3 \\ c_4  \end{bmatrix}  $$ I've already established that this will display weird behaviour. For example, if we take the xy plane and the wz planes and intersect them, there is only one solution (the zero vector), indicating that they intersect at a plane. Getting Insights If we constructed this final matrix, what restrictions can we apply knowing that the original matrix had a rank of 2? $$ \begin{bmatrix} p_1 & p_2 & p_3 & p_4 \\ q_1 & q_2 & q_3 & q_4 \\r_1 & r_2 & r_3 & r_4 \\ s_1 & s_2 & s_3 & s_4  \end{bmatrix} \begin{bmatrix} w \\x \\ y\\ z  \end{bmatrix} = \begin{bmatrix}c_1 \\ c_2\\ c_3 \\ c_4  \end{bmatrix}  $$",,"['linear-algebra', 'independence']"
83,When is the additive identity not the zero vector?,When is the additive identity not the zero vector?,,"My teacher cryptically mentioned today that the zero vector is not always the additive identity. When asked for clarification I was told ""we'll get there"". He did confirm it is always 0 in matrices filled with real numbers, but I can't think of or find any matrix, whether complex or variable or whatever where anything else would work, or where the zero vector wouldn't work. It might be half a joke to keep me interested, but I'll be a minkeys uncle if it didn't work! I don't know, any ideas?","My teacher cryptically mentioned today that the zero vector is not always the additive identity. When asked for clarification I was told ""we'll get there"". He did confirm it is always 0 in matrices filled with real numbers, but I can't think of or find any matrix, whether complex or variable or whatever where anything else would work, or where the zero vector wouldn't work. It might be half a joke to keep me interested, but I'll be a minkeys uncle if it didn't work! I don't know, any ideas?",,"['linear-algebra', 'vector-spaces']"
84,What should we understand from the definition of orthogonality in inner product spaces other than $\mathbb R^n$?,What should we understand from the definition of orthogonality in inner product spaces other than ?,\mathbb R^n,"In the beginning of linear algebra courses, there are vectors in $\mathbb R^n$ and the dot product is introduced. We learn that if the dot product of two vectors is zero, then these vectors are called orthogonal and there is a right angle between them. Therefore, we understand perpendicularity from the definition of orthogonality. In the last chapters, all previous notions are generalized as vector spaces and inner products are introduced. Then we learn vectors $\vec u$ and $\vec v$ are orthogonal if $\langle \vec u, \vec v \rangle=0$ in inner product spaces. Our previous knowledge guides us to seek a right angle between them. Let $P$ be the vector space of first degree polynomials. Let $p(x)=ax+b$ and $q(x)=cx+d$. Define $\langle p(x), q(x) \rangle$ as $$\langle p(x), q(x) \rangle=ac+bd$$ Then if we consider $p(x)=x-2$ and $q(x)=4x+2$, we find that $\langle p(x), q(x) \rangle=0$. But if we draw them, we see that there is $30.96$ degrees between them. So orthogonality in inner product spaces does not necessarily mean perpendicularity. If not perpendicularity, what should we understand from orthogonality in inner product spaces?","In the beginning of linear algebra courses, there are vectors in $\mathbb R^n$ and the dot product is introduced. We learn that if the dot product of two vectors is zero, then these vectors are called orthogonal and there is a right angle between them. Therefore, we understand perpendicularity from the definition of orthogonality. In the last chapters, all previous notions are generalized as vector spaces and inner products are introduced. Then we learn vectors $\vec u$ and $\vec v$ are orthogonal if $\langle \vec u, \vec v \rangle=0$ in inner product spaces. Our previous knowledge guides us to seek a right angle between them. Let $P$ be the vector space of first degree polynomials. Let $p(x)=ax+b$ and $q(x)=cx+d$. Define $\langle p(x), q(x) \rangle$ as $$\langle p(x), q(x) \rangle=ac+bd$$ Then if we consider $p(x)=x-2$ and $q(x)=4x+2$, we find that $\langle p(x), q(x) \rangle=0$. But if we draw them, we see that there is $30.96$ degrees between them. So orthogonality in inner product spaces does not necessarily mean perpendicularity. If not perpendicularity, what should we understand from orthogonality in inner product spaces?",,['linear-algebra']
85,Show the inner product spaces are isometric,Show the inner product spaces are isometric,,"Let $V$, $|| \bullet ||_V$ and let $W$, $|| \bullet ||_W$ be two $n$-dimensional normed linear spaces over $\mathbb R$. We know that $V,W$ are then isomorphic as vector spaces. We say that $V,W$ are isometric if there exists a linear map $T: V \to W$ such that $T$ is one to one and onto, and such that for all $v \in V$, $||T(v)||_w = ||v||_V$. a. Assume additionally that $V,W$ are $n$-dimensional inner product spaces. Show that $V,W$ are then isometric. We've done nothing in class regarding isometric stuff so I have no idea what this question is even asking of me.","Let $V$, $|| \bullet ||_V$ and let $W$, $|| \bullet ||_W$ be two $n$-dimensional normed linear spaces over $\mathbb R$. We know that $V,W$ are then isomorphic as vector spaces. We say that $V,W$ are isometric if there exists a linear map $T: V \to W$ such that $T$ is one to one and onto, and such that for all $v \in V$, $||T(v)||_w = ||v||_V$. a. Assume additionally that $V,W$ are $n$-dimensional inner product spaces. Show that $V,W$ are then isometric. We've done nothing in class regarding isometric stuff so I have no idea what this question is even asking of me.",,"['linear-algebra', 'linear-transformations', 'isometry']"
86,Formula to best fit a rectangle inside another by scaling,Formula to best fit a rectangle inside another by scaling,,"I am very weak in Math. I am a web programmer, and usually my work does not involve too much math - it's more of putting records into databases, pulling out reports, making those fancy web pages etc etc. So, I got a task that is ""mathy"" for me, but I am hoping it's child's play for some of you Math guys out there, and you will help me out... It might even be an interesting problem for you, its just too complex for me. So recently I got this task, where I am given an image with an area marked on top of it. This area would be marked as top left pixels (x1,y1) and bottom right pixels (x2,y2) So for the image I have the following information: Image Width (iw) Image Height (ih) Image Area Top Left (x1,y1) Image Area Bottom Right (x2,y2) Now there are a collection of frames, each frame will have Frame Width (fw) and Frame Height (fh) . My job is to find optimal way to fit Image Area inside frame without having any blank area inside frame, using image scaling if required. So I broke it down to following tasks: Image Area (not fill image) has to fit inside Frame. If Image Area > Frame , I should scale Image down (to a %). If Image Area < Frame I should scale Image Up. The aspect ratio of the image should not be disturbed when I scale. I should scale Image based on the longer side of Image Area to make sure that whole Image Area comes inside Frame. However when scaling Image down, sometimes this may cause the other side to have blank space inside Frame because of extreme aspect ratios. In such case, I should only scale down until one side hits Image Height or Image Width. This means that the Image Area is not fully shown inside the Frame, but that is fine because the first priority is to not have blank space inside Frame. Now that I have written all these rules, I am unable to wrap my head around formalizing this into a mathematical ""formula""/""procedure""/""rule-set"" that I can then recreate in my software. For scaling, I found that this formula works well (this is a programming pseudo-code): imageX = Image.width() imageY = Image.height(); imageRatio = imageX / imageY;  frameX = Frame.width(); frameY = Frame.height(); frameRatio = frameX / frameY;  if (imageRatio < wellRatio)  {     scale = imageX / wellX; }  else  {     scale = imageY / wellY; } resizeX = (imageX / scale); resizeY = (imageY / scale); However I do not know how to tie this into the whole picture. Can anyone help me please? I have been trying unsuccessfully for over a week now :( P.S- I do not know what tags to add for this question, so I would appreciate if anyone reading can update the tags to more correct/relevant ones.","I am very weak in Math. I am a web programmer, and usually my work does not involve too much math - it's more of putting records into databases, pulling out reports, making those fancy web pages etc etc. So, I got a task that is ""mathy"" for me, but I am hoping it's child's play for some of you Math guys out there, and you will help me out... It might even be an interesting problem for you, its just too complex for me. So recently I got this task, where I am given an image with an area marked on top of it. This area would be marked as top left pixels (x1,y1) and bottom right pixels (x2,y2) So for the image I have the following information: Image Width (iw) Image Height (ih) Image Area Top Left (x1,y1) Image Area Bottom Right (x2,y2) Now there are a collection of frames, each frame will have Frame Width (fw) and Frame Height (fh) . My job is to find optimal way to fit Image Area inside frame without having any blank area inside frame, using image scaling if required. So I broke it down to following tasks: Image Area (not fill image) has to fit inside Frame. If Image Area > Frame , I should scale Image down (to a %). If Image Area < Frame I should scale Image Up. The aspect ratio of the image should not be disturbed when I scale. I should scale Image based on the longer side of Image Area to make sure that whole Image Area comes inside Frame. However when scaling Image down, sometimes this may cause the other side to have blank space inside Frame because of extreme aspect ratios. In such case, I should only scale down until one side hits Image Height or Image Width. This means that the Image Area is not fully shown inside the Frame, but that is fine because the first priority is to not have blank space inside Frame. Now that I have written all these rules, I am unable to wrap my head around formalizing this into a mathematical ""formula""/""procedure""/""rule-set"" that I can then recreate in my software. For scaling, I found that this formula works well (this is a programming pseudo-code): imageX = Image.width() imageY = Image.height(); imageRatio = imageX / imageY;  frameX = Frame.width(); frameY = Frame.height(); frameRatio = frameX / frameY;  if (imageRatio < wellRatio)  {     scale = imageX / wellX; }  else  {     scale = imageY / wellY; } resizeX = (imageX / scale); resizeY = (imageY / scale); However I do not know how to tie this into the whole picture. Can anyone help me please? I have been trying unsuccessfully for over a week now :( P.S- I do not know what tags to add for this question, so I would appreciate if anyone reading can update the tags to more correct/relevant ones.",,"['linear-algebra', 'trigonometry', 'vector-spaces', 'metric-spaces', 'graphing-functions']"
87,Why is quadratic form defined via a symmetric bilinear form?,Why is quadratic form defined via a symmetric bilinear form?,,"A typical definition of quadratic form goes like this: Let $B:V\times V \to F$ be a symmetric bilinear form. A   function $Q : V → F$ defined by $Q(v) = B(v, v)$ is called a quadratic form. Why do we need $B$ to be symmetric if the way $Q$ is defined doesn't use it? Edit: And is that related to the reason why we ask for a symmetric bilinear form in the definition of a positive definite bilinear form even though the definition itself $(B(v,v)>0)$ again doesn't directly use it?","A typical definition of quadratic form goes like this: Let $B:V\times V \to F$ be a symmetric bilinear form. A   function $Q : V → F$ defined by $Q(v) = B(v, v)$ is called a quadratic form. Why do we need $B$ to be symmetric if the way $Q$ is defined doesn't use it? Edit: And is that related to the reason why we ask for a symmetric bilinear form in the definition of a positive definite bilinear form even though the definition itself $(B(v,v)>0)$ again doesn't directly use it?",,"['linear-algebra', 'vector-spaces', 'quadratic-forms', 'multilinear-algebra']"
88,Prove that $AB = 0 \iff \mathrm{im}(B) \subseteq \ker(A)$,Prove that,AB = 0 \iff \mathrm{im}(B) \subseteq \ker(A),"Problem 1. Prove that for any $\ell \times m$-matrix $A$ and any $m \times n$-matrix $B$, $$ AB = 0 \quad\text{ if and only if }\quad \mathrm{im}(B) \subseteq \ker(A) $$ I have no idea on how to start this...  I'm new to proofs and this is my first proof.","Problem 1. Prove that for any $\ell \times m$-matrix $A$ and any $m \times n$-matrix $B$, $$ AB = 0 \quad\text{ if and only if }\quad \mathrm{im}(B) \subseteq \ker(A) $$ I have no idea on how to start this...  I'm new to proofs and this is my first proof.",,"['linear-algebra', 'matrices']"
89,Inverse of diagonally dominant matrix with equal off-diagonal entries,Inverse of diagonally dominant matrix with equal off-diagonal entries,,"Is there an explicit expression for the inverse of strictly diagonally dominant matrix with identical off-diagonal elements? For example: $$ \begin{pmatrix} a & -b & -b \\                   -b &  c & -b \\                   -b & -b &  d \end{pmatrix} $$ where $|a|\gt 2|b|$, $|c|\gt 2|b|$, and $|d|\gt 2|b|$. I'm looking for an inverse for an arbitrary $n\times n$ matrix with the above property.","Is there an explicit expression for the inverse of strictly diagonally dominant matrix with identical off-diagonal elements? For example: $$ \begin{pmatrix} a & -b & -b \\                   -b &  c & -b \\                   -b & -b &  d \end{pmatrix} $$ where $|a|\gt 2|b|$, $|c|\gt 2|b|$, and $|d|\gt 2|b|$. I'm looking for an inverse for an arbitrary $n\times n$ matrix with the above property.",,"['linear-algebra', 'matrices', 'inverse', 'symmetry']"
90,What does solving $Ax=b$ mean?,What does solving  mean?,Ax=b,"We have been going through how to solve the system of equations known as $Ax=b$ , where $A$ is a matrix, $x$ is a vector and $b$ is a vector. I understand that if we have $A$ and $b$ we must find out what $x$ is, this happens via Gauss-Jordan elimination, back substitution, etc. What does solving the linear system of equations actually mean though? Is it the point in space where all of the vectors of $A$ intersect, and what will this be useful for? Real-life examples are appreciated!","We have been going through how to solve the system of equations known as , where is a matrix, is a vector and is a vector. I understand that if we have and we must find out what is, this happens via Gauss-Jordan elimination, back substitution, etc. What does solving the linear system of equations actually mean though? Is it the point in space where all of the vectors of intersect, and what will this be useful for? Real-life examples are appreciated!",Ax=b A x b A b x A,"['linear-algebra', 'systems-of-equations']"
91,The rotation group $SO(3)$ may be mapped to a $2$-sphere by sending a rotation matrix to its first column. How can I describe the fibres of the map?,The rotation group  may be mapped to a -sphere by sending a rotation matrix to its first column. How can I describe the fibres of the map?,SO(3) 2,The rotation group $SO(3)$ may be mapped to a $2$-sphere by sending a rotation matrix to its first column.  How can I describe the fibres of the map?,The rotation group $SO(3)$ may be mapped to a $2$-sphere by sending a rotation matrix to its first column.  How can I describe the fibres of the map?,,"['linear-algebra', 'abstract-algebra']"
92,Finite dimensional spaces,Finite dimensional spaces,,"What are the finite-dimensional spaces $W$ of differentiable functions with this property: If $f$ is in $W$, then $\frac{df}{dx}$ is in $W$.","What are the finite-dimensional spaces $W$ of differentiable functions with this property: If $f$ is in $W$, then $\frac{df}{dx}$ is in $W$.",,['linear-algebra']
93,Decompose a real symmetric matrix,Decompose a real symmetric matrix,,"Prove that, without using induction, A real symmetric matrix $A$ can be decomposed as $A = Q^T \Lambda Q$, where $Q$ is an orthogonal matrix and $\Lambda$ is a diagonal matrix with eigenvalues of $A$ as its diagonal elements. I can see that all eigenvalues of $A$ are real, and the corresponding eigenvectors are orthogonal, but I failed to see that when putting all (interesting) eigenvectors together, they form a basis of $\mathbb{R}^n$. Edit The reason I asked this question is to show that a real symmetric matrix is diagonalizable, so let's not use that fact for a while. Other than that, any undergraduate level linear algebra can be used. Edit 2 After reading Algebraic Pavel 's answer, I feel like ruling out Schur Decomposition as well, but I can't keep ruling out theorems, so...if a proof is too obvious, that's probably not what I am looking for, though, it maybe a technically correct answer. Thanks.","Prove that, without using induction, A real symmetric matrix $A$ can be decomposed as $A = Q^T \Lambda Q$, where $Q$ is an orthogonal matrix and $\Lambda$ is a diagonal matrix with eigenvalues of $A$ as its diagonal elements. I can see that all eigenvalues of $A$ are real, and the corresponding eigenvectors are orthogonal, but I failed to see that when putting all (interesting) eigenvectors together, they form a basis of $\mathbb{R}^n$. Edit The reason I asked this question is to show that a real symmetric matrix is diagonalizable, so let's not use that fact for a while. Other than that, any undergraduate level linear algebra can be used. Edit 2 After reading Algebraic Pavel 's answer, I feel like ruling out Schur Decomposition as well, but I can't keep ruling out theorems, so...if a proof is too obvious, that's probably not what I am looking for, though, it maybe a technically correct answer. Thanks.",,['linear-algebra']
94,Taking product of cofactor with different row,Taking product of cofactor with different row,,"Given a matrix $A=(a_{ij})_{n\times n}$, let $C_{i,j}$ be the cofactor in position $(i,j)$. By the determinant formula, we have $$\det A=\sum_{i=1}^n a_{i,1}C_{i,1}.$$ What about if we take a different column for the cofactors, that is $$\sum_{i=1}^n a_{i,1}C_{i,2}$$ Must this evaluate to zero?","Given a matrix $A=(a_{ij})_{n\times n}$, let $C_{i,j}$ be the cofactor in position $(i,j)$. By the determinant formula, we have $$\det A=\sum_{i=1}^n a_{i,1}C_{i,1}.$$ What about if we take a different column for the cofactors, that is $$\sum_{i=1}^n a_{i,1}C_{i,2}$$ Must this evaluate to zero?",,"['linear-algebra', 'determinant']"
95,Two proofs about invertible Matrix and row equivalent to the identity matrix.,Two proofs about invertible Matrix and row equivalent to the identity matrix.,,2 proofs:  1. A matrix n*n is invertible if and only if it's row equivalent to the identity matrix.  2. A matrix n*n is not invertible if and only if it is row equivalent to a matrix with zero row. (proofs without determinant of course) I know it's a simple question but I would like to see a simple and formalic proof for this issue because I could only write one with words and it looks terrible.. Thanks,2 proofs:  1. A matrix n*n is invertible if and only if it's row equivalent to the identity matrix.  2. A matrix n*n is not invertible if and only if it is row equivalent to a matrix with zero row. (proofs without determinant of course) I know it's a simple question but I would like to see a simple and formalic proof for this issue because I could only write one with words and it looks terrible.. Thanks,,['linear-algebra']
96,Classifying complex $2\times 2$ matrices up to similarity,Classifying complex  matrices up to similarity,2\times 2,"I would like to prove the following proposition, which is given as an exercise in Hoffman and Kunze: If $A$ is a $2\times 2$ matrix with coefficients in $\mathbb{C}$, then $A$ is similar either to a matrix of the form $\begin{pmatrix} a & 0 \\ 0 & b \end{pmatrix}$ or to a matrix of the form $\begin{pmatrix} a & 0 \\ 1 & a \end{pmatrix}$. A hint directs the reader to prove that if $N$ is an nilpotent matrix (also in $M_{2}(\mathbb{C})$), then either $N=0$ or $N$ is similar to $\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}$. I have proven this claim (by supposing that $N \neq 0$ and showing that the transformation induced by multiplication by $N$ has matrix representation $\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}$ under a particular basis). However, I'm not really sure how to use this knowledge to prove the proposition in question. Any steps in the right direction would be appreciated. Also, I'd love to see any other proofs of the proposition. Thanks in advance!","I would like to prove the following proposition, which is given as an exercise in Hoffman and Kunze: If $A$ is a $2\times 2$ matrix with coefficients in $\mathbb{C}$, then $A$ is similar either to a matrix of the form $\begin{pmatrix} a & 0 \\ 0 & b \end{pmatrix}$ or to a matrix of the form $\begin{pmatrix} a & 0 \\ 1 & a \end{pmatrix}$. A hint directs the reader to prove that if $N$ is an nilpotent matrix (also in $M_{2}(\mathbb{C})$), then either $N=0$ or $N$ is similar to $\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}$. I have proven this claim (by supposing that $N \neq 0$ and showing that the transformation induced by multiplication by $N$ has matrix representation $\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}$ under a particular basis). However, I'm not really sure how to use this knowledge to prove the proposition in question. Any steps in the right direction would be appreciated. Also, I'd love to see any other proofs of the proposition. Thanks in advance!",,"['linear-algebra', 'matrices']"
97,How many Jordan normal forms are there for this characteristic polynomial?,How many Jordan normal forms are there for this characteristic polynomial?,,"Given the characteristic polynomial of a matrix $A \in \mathbb{C}^{6x6}$ with $p(A)=(\lambda-2)^2(\lambda-1)^4$, we were supposed to determine all Jordan normal forms that have this characteristic polynomial. I determined 10 (is this correct?) and was wondering whether this is a general way to compute the number of them for an arbitrary characteristic polynomial.","Given the characteristic polynomial of a matrix $A \in \mathbb{C}^{6x6}$ with $p(A)=(\lambda-2)^2(\lambda-1)^4$, we were supposed to determine all Jordan normal forms that have this characteristic polynomial. I determined 10 (is this correct?) and was wondering whether this is a general way to compute the number of them for an arbitrary characteristic polynomial.",,['linear-algebra']
98,Dimensions of vector subspaces in a direct sum are additive,Dimensions of vector subspaces in a direct sum are additive,,"$V = U_1\oplus U_2~\oplus~...~ \oplus~ U_n~(\dim V < ∞)$ $\implies \dim V = \dim U_1 + \dim U_2 + ... + \dim U_n.$ [Using the result if $B_i$ is a basis of $U_i$ then $\cup_{i=1}^n B_i$ is a basis of $V$] Then it suffices to show $U_i\cap U_j-\{0\}=\emptyset$ for $i\ne j.$ If not, let $v\in U_i\cap U_j-\{0\}.$ Then \begin{align*} v=&0\,(\in U_1)+0\,(\in U_2)\,+\ldots+0\,(\in U_{i-1})+v\,(\in U_{i})+0\,(\in U_{i+1})+\ldots\\ & +\,0\,(\in U_j)+\ldots+0\,(\in U_{n})\\ =&0\,(\in U_1)+0\,(\in U_2)+\ldots+0\,(\in U_i)+\ldots+0\,(\in U_{j-1})+\,v(\in U_{j})\\ & +\,0\,(\in U_{j+1})+\ldots+0\,(\in U_{n}). \end{align*} Hence $v$ fails to have a unique linear sum of elements of $U_i's.$ Hence etc ... Am I right?","$V = U_1\oplus U_2~\oplus~...~ \oplus~ U_n~(\dim V < ∞)$ $\implies \dim V = \dim U_1 + \dim U_2 + ... + \dim U_n.$ [Using the result if $B_i$ is a basis of $U_i$ then $\cup_{i=1}^n B_i$ is a basis of $V$] Then it suffices to show $U_i\cap U_j-\{0\}=\emptyset$ for $i\ne j.$ If not, let $v\in U_i\cap U_j-\{0\}.$ Then \begin{align*} v=&0\,(\in U_1)+0\,(\in U_2)\,+\ldots+0\,(\in U_{i-1})+v\,(\in U_{i})+0\,(\in U_{i+1})+\ldots\\ & +\,0\,(\in U_j)+\ldots+0\,(\in U_{n})\\ =&0\,(\in U_1)+0\,(\in U_2)+\ldots+0\,(\in U_i)+\ldots+0\,(\in U_{j-1})+\,v(\in U_{j})\\ & +\,0\,(\in U_{j+1})+\ldots+0\,(\in U_{n}). \end{align*} Hence $v$ fails to have a unique linear sum of elements of $U_i's.$ Hence etc ... Am I right?",,['linear-algebra']
99,tensor product and wedge product for direct sum decomposition,tensor product and wedge product for direct sum decomposition,,"If we have a real vector space $V=W_1\oplus W_2$, is it true that $W_1 \otimes W_2 = W_1 \wedge W_2 $? My guess is that this is true. The definition of the $k$-exterior power is the quotient of $V^{\otimes k}/I$ where $I$ is the subspace generated by the elements of the form $v_1\otimes \cdots \otimes v_k$ where $v_i=v_{i+1}$ for some $i$. Then $W_1 \otimes W_2$ is a subspace of $V^{\otimes 2}$ in which the only element of the form $v\otimes v$ is $0 \otimes 0 = 0$. Is this right? Is there another way to see this? thanks","If we have a real vector space $V=W_1\oplus W_2$, is it true that $W_1 \otimes W_2 = W_1 \wedge W_2 $? My guess is that this is true. The definition of the $k$-exterior power is the quotient of $V^{\otimes k}/I$ where $I$ is the subspace generated by the elements of the form $v_1\otimes \cdots \otimes v_k$ where $v_i=v_{i+1}$ for some $i$. Then $W_1 \otimes W_2$ is a subspace of $V^{\otimes 2}$ in which the only element of the form $v\otimes v$ is $0 \otimes 0 = 0$. Is this right? Is there another way to see this? thanks",,"['linear-algebra', 'abstract-algebra', 'commutative-algebra']"
