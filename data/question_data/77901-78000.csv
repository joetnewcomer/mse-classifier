,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Alternative proof for continuity of matrix inversion,Alternative proof for continuity of matrix inversion,,"I am to show that $\mathrm{inv}: \mathrm{GL}_{n \times n}(\mathbb{R}) \to \mathrm{GL}_{n \times n}(\mathbb{R}); A \mapsto A^{-1}$ is a continuous function. I have shown this by showing that the deteriminant is continuous, and that forming the adjunct is continuous, in which case one can apply the formula $A^{-1} = \frac{1}{\mathrm{det}(A)}\mathrm{adj}(A)$. This is all fine, but I would love to see a more intuitive proof that doesn’t simply rely on showing that some formula is composed of continuous functions. I tried finding some sort of relation between the operator norms of $A$ and $A^{-1}$ but couldn’t find anything that holds in the general case. For example, if something along the lines of $\min_{x \in \mathbb{S}^{n-1}}{(Ax)} \times \max_{x \in \mathbb{S}^{n-1}}{(A^{-1}x)} = 1$ were true, one could construct an epsilon-delta proof via that. However, the above expression while inutive, appears to be false (it seems to hold only if the chosen x happen to be eigenvectors unless my attempts to verify it in mathematica were somehow wrong). Does anyone know of a more intuitive proof of the statement than the one I outlined above?","I am to show that $\mathrm{inv}: \mathrm{GL}_{n \times n}(\mathbb{R}) \to \mathrm{GL}_{n \times n}(\mathbb{R}); A \mapsto A^{-1}$ is a continuous function. I have shown this by showing that the deteriminant is continuous, and that forming the adjunct is continuous, in which case one can apply the formula $A^{-1} = \frac{1}{\mathrm{det}(A)}\mathrm{adj}(A)$. This is all fine, but I would love to see a more intuitive proof that doesn’t simply rely on showing that some formula is composed of continuous functions. I tried finding some sort of relation between the operator norms of $A$ and $A^{-1}$ but couldn’t find anything that holds in the general case. For example, if something along the lines of $\min_{x \in \mathbb{S}^{n-1}}{(Ax)} \times \max_{x \in \mathbb{S}^{n-1}}{(A^{-1}x)} = 1$ were true, one could construct an epsilon-delta proof via that. However, the above expression while inutive, appears to be false (it seems to hold only if the chosen x happen to be eigenvectors unless my attempts to verify it in mathematica were somehow wrong). Does anyone know of a more intuitive proof of the statement than the one I outlined above?",,"['matrices', 'continuity', 'alternative-proof']"
1,Show that any orthogonal matrix has determinant 1 or -1 [duplicate],Show that any orthogonal matrix has determinant 1 or -1 [duplicate],,"This question already has answers here : A better proof for $\det(P) = \pm1$ if $P$ is an orthogonal matrix (3 answers) Closed 9 years ago . Hello fellow users of this forum: Show that for any orthogonal matrix Q, either det(Q)=1 or -1. Thanks","This question already has answers here : A better proof for $\det(P) = \pm1$ if $P$ is an orthogonal matrix (3 answers) Closed 9 years ago . Hello fellow users of this forum: Show that for any orthogonal matrix Q, either det(Q)=1 or -1. Thanks",,"['matrices', 'determinant']"
2,How can I calculate a $4\times 4$ rotation matrix to match a 4d direction vector?,How can I calculate a  rotation matrix to match a 4d direction vector?,4\times 4,"I have two 4d vectors, and need to calculate a $4\times 4$ rotation matrix to point from one to the other. edit - I'm getting an idea of how to do it conceptually: find the plane in which the vectors lie, calculate the angle between the vectors using the dot product, then construct the rotation matrix based on the two. The trouble is I don't know how to mechanically do the first or last of those three steps. I'm trying to program objects in 4space, so an ideal solution would be computationally efficient too, but that is secondary.","I have two 4d vectors, and need to calculate a $4\times 4$ rotation matrix to point from one to the other. edit - I'm getting an idea of how to do it conceptually: find the plane in which the vectors lie, calculate the angle between the vectors using the dot product, then construct the rotation matrix based on the two. The trouble is I don't know how to mechanically do the first or last of those three steps. I'm trying to program objects in 4space, so an ideal solution would be computationally efficient too, but that is secondary.",,"['matrices', 'analytic-geometry', 'rotations']"
3,Prove $\det(P+Q+R)=\det(P+Q)+\det(Q+R)+\det(R+P)-\det(P)-\det(Q)-\det(R)$,Prove,\det(P+Q+R)=\det(P+Q)+\det(Q+R)+\det(R+P)-\det(P)-\det(Q)-\det(R),"If $P,Q,R$ be three $2\times2$ matrices, then prove that  $$\det(P+Q+R)=\det(P+Q)+\det(Q+R)+\det(R+P)-\det(P)-\det(Q)-\det(R).$$ My Attempt : I have no idea from where to start","If $P,Q,R$ be three $2\times2$ matrices, then prove that  $$\det(P+Q+R)=\det(P+Q)+\det(Q+R)+\det(R+P)-\det(P)-\det(Q)-\det(R).$$ My Attempt : I have no idea from where to start",,"['matrices', 'determinant']"
4,How to establish that an anti-idempotent matrix is singular?,How to establish that an anti-idempotent matrix is singular?,,"In linear algebra, idempotent matrices are defined by $$ A^2 = A \tag{1} $$ for a square matrix $A$ . Obviously, the identity matrix $I$ is an idempotent matrix. It can be also shown that if $M$ is idempotent, then $I - M$ is idempotent by a trivial calculation. $$ (I - M) (I - M) = I - M - M + M^2 = I - M - M + M = I - M $$ In a similar manner, we can define an anti-idempotent matrix $A$ by the condition $$ A^2 = - A \tag{2} $$ (A trivial example is the zero matrix.) To find non-trivial examples of an anti-idempotent matrix $A$ , I considered the case of $(2 \times 2)$ matrices: $$ A = \left[ \matrix{ a & b \cr                     c & d \cr} \right] $$ If $A$ is anti-idempotent, then it must satisfy: $A^2 = -A$ . This leads to a set of $4$ equations: $$ a^2 + b c = - a, \ a b + b d = - b, \ c a + c d = -c, \ \ b c + d^2 = -d  $$ A simple manipulation results in the equations $$ b (a + d + 1) = 0, c (a + d + 1) = 0, a^2 + b c = -a, b c + d^2 = -d. $$ Taking $a = 2$ , we see that $a + d + 1 = 0$ or $d = -3$ . We can choose $b$ and $c$ from $b c = -6$ . One choice is $b = 2, c = -3$ . Thus, an anti-idempotent matrix is: $$ A = \left[ \begin{array}{cc}             2 & 2 \\             -3 & -3 \\         \end{array} \right] $$ (It is easy to check that $A^2 = -A$ .) It can be easily shown : If $M$ is an anti-idempotent matrix, then $I + M$ is also anti-idempotent. Indeed, $$ (I + M) (I + M) = I + M + M + M^2 = I + M + M - M = I + M. $$ The examples I considered for anti-idempotent matrices yield singular matrices. I like to know if it is generally true that anti-idempotent matrices are singular matrices. How to establish this result? Your comments are welcome.","In linear algebra, idempotent matrices are defined by for a square matrix . Obviously, the identity matrix is an idempotent matrix. It can be also shown that if is idempotent, then is idempotent by a trivial calculation. In a similar manner, we can define an anti-idempotent matrix by the condition (A trivial example is the zero matrix.) To find non-trivial examples of an anti-idempotent matrix , I considered the case of matrices: If is anti-idempotent, then it must satisfy: . This leads to a set of equations: A simple manipulation results in the equations Taking , we see that or . We can choose and from . One choice is . Thus, an anti-idempotent matrix is: (It is easy to check that .) It can be easily shown : If is an anti-idempotent matrix, then is also anti-idempotent. Indeed, The examples I considered for anti-idempotent matrices yield singular matrices. I like to know if it is generally true that anti-idempotent matrices are singular matrices. How to establish this result? Your comments are welcome.","
A^2 = A \tag{1}
 A I M I - M 
(I - M) (I - M) = I - M - M + M^2 = I - M - M + M = I - M
 A 
A^2 = - A \tag{2}
 A (2 \times 2) 
A = \left[ \matrix{ a & b \cr
                    c & d \cr} \right]
 A A^2 = -A 4 
a^2 + b c = - a, \ a b + b d = - b, \ c a + c d = -c, \ \ b c + d^2 = -d 
 
b (a + d + 1) = 0, c (a + d + 1) = 0, a^2 + b c = -a, b c + d^2 = -d.
 a = 2 a + d + 1 = 0 d = -3 b c b c = -6 b = 2, c = -3 
A = \left[ \begin{array}{cc}
            2 & 2 \\
            -3 & -3 \\
        \end{array} \right]
 A^2 = -A M I + M 
(I + M) (I + M) = I + M + M + M^2 = I + M + M - M = I + M.
",['matrices']
5,Square root-related calculations with matrices,Square root-related calculations with matrices,,"If $\mathbf A$ is an $n \times n$ matrix such that $\mathbf A^6 = \mathbf I_n$ (the identity matrix), is it true that either $\mathbf A^3 = \mathbf I_n$ or $\mathbf A^3 = \mathbf -I_n$? I'm struggling to solve this question. I re-wrote $\mathbf A^3$ as $\mathbf B$, which gives $\mathbf B^2 = \mathbf I_n$. I do know that, if $\mathbf B^2 = \mathbf I_n$, then it is not necessarily true that $\mathbf B = \pm\mathbf I$ (I had a look at the following link, though I haven't fully understood the explanations given: If $A^2 = I$ (Identity Matrix) then $A = \pm I$ ). Can I (and if so, how do I) apply this fact to the case where $\mathbf B$ is the cube of another matrix $\mathbf A$?","If $\mathbf A$ is an $n \times n$ matrix such that $\mathbf A^6 = \mathbf I_n$ (the identity matrix), is it true that either $\mathbf A^3 = \mathbf I_n$ or $\mathbf A^3 = \mathbf -I_n$? I'm struggling to solve this question. I re-wrote $\mathbf A^3$ as $\mathbf B$, which gives $\mathbf B^2 = \mathbf I_n$. I do know that, if $\mathbf B^2 = \mathbf I_n$, then it is not necessarily true that $\mathbf B = \pm\mathbf I$ (I had a look at the following link, though I haven't fully understood the explanations given: If $A^2 = I$ (Identity Matrix) then $A = \pm I$ ). Can I (and if so, how do I) apply this fact to the case where $\mathbf B$ is the cube of another matrix $\mathbf A$?",,['matrices']
6,Embedding $GL_2(\mathbb{F}_l)$ into $GL_2(\mathbb{C})$,Embedding  into,GL_2(\mathbb{F}_l) GL_2(\mathbb{C}),"I recently learned that $GL_2(\mathbb{F}_3)$ can be embedded into $GL_2(\mathbb{C})$; specifically, $$ \left(\begin{array}{cc} -1 & -1 \\ -1 & 0 \end{array} \right) \mapsto \left(\begin{array}{cc} -1 & -1 \\ -1 & 0 \end{array} \right) $$ and $$ \left(\begin{array}{cc} 1 & -1 \\ 1 & 1 \end{array} \right) \mapsto \left(\begin{array}{cc} 1 & -1 \\ -\sqrt{-2} & -1 + \sqrt{-2} \end{array} \right). $$ I think same goes for $\mathbb{F}_5$. Can this be done for other larger primes? (Motivation: I am interested in making some mod-$l$ representations complex. )","I recently learned that $GL_2(\mathbb{F}_3)$ can be embedded into $GL_2(\mathbb{C})$; specifically, $$ \left(\begin{array}{cc} -1 & -1 \\ -1 & 0 \end{array} \right) \mapsto \left(\begin{array}{cc} -1 & -1 \\ -1 & 0 \end{array} \right) $$ and $$ \left(\begin{array}{cc} 1 & -1 \\ 1 & 1 \end{array} \right) \mapsto \left(\begin{array}{cc} 1 & -1 \\ -\sqrt{-2} & -1 + \sqrt{-2} \end{array} \right). $$ I think same goes for $\mathbb{F}_5$. Can this be done for other larger primes? (Motivation: I am interested in making some mod-$l$ representations complex. )",,"['group-theory', 'matrices', 'representation-theory']"
7,"If $A^2 = I$, $B^2=I$ and $(AB)^2=I$, then $AB = BA$","If ,  and , then",A^2 = I B^2=I (AB)^2=I AB = BA,"Matrix Question If $A^2 = I$, $B^2=I$ and $(AB)^2=I$, then $AB = BA$ Basically, got up to $A(BA-AB)B = 0$ (by cancelling and equating terms from $I^2 = I$ and to $A^2B^2 = A^2B^2$ and using distributive laws), but that doesn't work out too well! Thanks for help in advance!","Matrix Question If $A^2 = I$, $B^2=I$ and $(AB)^2=I$, then $AB = BA$ Basically, got up to $A(BA-AB)B = 0$ (by cancelling and equating terms from $I^2 = I$ and to $A^2B^2 = A^2B^2$ and using distributive laws), but that doesn't work out too well! Thanks for help in advance!",,['matrices']
8,Prove that $|AB - \lambda I| = |BA - \lambda I|$.,Prove that .,|AB - \lambda I| = |BA - \lambda I|,"Suppose that one has two matrices $A$, $B$. Then Prove that $$|AB - \lambda I| = |BA - \lambda I|,$$   where $|\cdot|$ denotes the determinant, $I$ - identity matrix and $\lambda \in \mathbb{C}$. Note that $A$ and $B$ are not necessary invertible. For invertible matrices I easily found $$|AB - \lambda I| = |B(AB - \lambda I)B^{-1}| = |BA - \lambda I|.$$","Suppose that one has two matrices $A$, $B$. Then Prove that $$|AB - \lambda I| = |BA - \lambda I|,$$   where $|\cdot|$ denotes the determinant, $I$ - identity matrix and $\lambda \in \mathbb{C}$. Note that $A$ and $B$ are not necessary invertible. For invertible matrices I easily found $$|AB - \lambda I| = |B(AB - \lambda I)B^{-1}| = |BA - \lambda I|.$$",,"['matrices', 'determinant']"
9,Differentiation of a double summation,Differentiation of a double summation,,How does one reach from from 45 to 46?,How does one reach from from 45 to 46?,,"['matrices', 'derivatives', 'summation']"
10,Prove that $e^{-A} = (e^{A})^{-1}$,Prove that,e^{-A} = (e^{A})^{-1},"Let $A, B \in R^{n \times n}$. Prove that $e^{-A} = (e^{A})^{-1}$. ($R$ is the real numbers) I've tried messing around with both sides, evaluated as sums. I just can't get the two to match up. Any ideas?","Let $A, B \in R^{n \times n}$. Prove that $e^{-A} = (e^{A})^{-1}$. ($R$ is the real numbers) I've tried messing around with both sides, evaluated as sums. I just can't get the two to match up. Any ideas?",,"['matrices', 'exponential-function']"
11,A be a $3\times 3$ matrix over $\mathbb {R}$ such that $AB =BA$ for all matrices $B$. what can we say about such matrix $A$ [duplicate],A be a  matrix over  such that  for all matrices . what can we say about such matrix  [duplicate],3\times 3 \mathbb {R} AB =BA B A,This question already has answers here : A linear operator commuting with all such operators is a scalar multiple of the identity. (10 answers) Closed 9 years ago . Let $A$ be a $3\times 3$ matrix over $\mathbb {R}$ such that $AB =BA$ for all matrices $B$ over $\mathbb {R}$ then what can we say about such matrix $A$.  or such matrix $A$ must be orthogonal matrix? Can we say anything about its eigen values?  I tried by taking random examples also tried to construct such $3 \times 3$ matrices. But i am not able to get any proper conclusion and proof.,This question already has answers here : A linear operator commuting with all such operators is a scalar multiple of the identity. (10 answers) Closed 9 years ago . Let $A$ be a $3\times 3$ matrix over $\mathbb {R}$ such that $AB =BA$ for all matrices $B$ over $\mathbb {R}$ then what can we say about such matrix $A$.  or such matrix $A$ must be orthogonal matrix? Can we say anything about its eigen values?  I tried by taking random examples also tried to construct such $3 \times 3$ matrices. But i am not able to get any proper conclusion and proof.,,['matrices']
12,Geometric intuition and visualization of matrix exponential,Geometric intuition and visualization of matrix exponential,,What is the geometric intuition and visualization of the matrix exponential $e^{\textbf{X}}$ ? Does that explain why matrix exponential is only valid with square matrices ?,What is the geometric intuition and visualization of the matrix exponential $e^{\textbf{X}}$ ? Does that explain why matrix exponential is only valid with square matrices ?,,"['matrices', 'matrix-exponential']"
13,Show that invertible matrices with an additional condition are diagonalizable.,Show that invertible matrices with an additional condition are diagonalizable.,,Let $A$ and $B$ be invertible $2 \times 2$ matrices such that $AB = -BA$ over the complex numbers. Show that $A$ and $B$ are diagonalizable.,Let $A$ and $B$ be invertible $2 \times 2$ matrices such that $AB = -BA$ over the complex numbers. Show that $A$ and $B$ are diagonalizable.,,['matrices']
14,Is $f(x) = x^T A x$ a convex function?,Is  a convex function?,f(x) = x^T A x,"Is $f(x) = x^T A x$ a convex function, where  $x \in \mathbb{R}^n$, and $A$ is a $ n\times n$ matrix? If not, my question can be reformed to: when is $f(x)$ convex, any restriction for $A$? For example, like positive definite and symmetric?","Is $f(x) = x^T A x$ a convex function, where  $x \in \mathbb{R}^n$, and $A$ is a $ n\times n$ matrix? If not, my question can be reformed to: when is $f(x)$ convex, any restriction for $A$? For example, like positive definite and symmetric?",,"['matrices', 'convex-analysis']"
15,Determine whether a $3\times3$ matrix has a positive eigenvalue?,Determine whether a  matrix has a positive eigenvalue?,3\times3,Given a $3\times3$ matrix is there a criterion capable of telling whether the matrix has a positive eigenvalue?,Given a $3\times3$ matrix is there a criterion capable of telling whether the matrix has a positive eigenvalue?,,"['matrices', 'eigenvalues-eigenvectors']"
16,Problem on Determinant.,Problem on Determinant.,,"Q. $$\text{If } \Delta = \left|\begin{array}{ccc} a & b & c \\ c & a & b \\ b & c & a \end{array}\right|,$$ $$\text{ then the value of }$$ $$ \left|\begin{array}{ccc} a^2 - bc & b^2 - ca & c^2 - ab \\ c^2 - ab & a^2 - bc & b^2 - ca \\ b^2 - ca & c^2 - ab & a^2 - bc \end{array}\right| \text{ is:}$$ Express the Answer in terms of $ \Delta $ . My Attempt - I tried Rearranging the Second determinant (by adding or subtracting particular rows or Columns) such that after this, It'll be easy for me write the second determinant as the product of two or more other Determinants that might be same as $\Delta$ . But I Found that rearranging the Second Det. Wasn't a good choice as it seems to never simplify itself!! So, I tried Expressing the Second Det. directly as the product of two or more Det. ! Still It was no use. I finally concluded that the Second Det. Can't be expressed as a product without Rearrangement ! But Rearranging it makes it more creepier! Could you please Guide me how to Rearrange it so that it could be easily expressed as a product? Or is their Another Way for this type of Problem? Please Let me know! Any help would be Appreciated.","Q. Express the Answer in terms of . My Attempt - I tried Rearranging the Second determinant (by adding or subtracting particular rows or Columns) such that after this, It'll be easy for me write the second determinant as the product of two or more other Determinants that might be same as . But I Found that rearranging the Second Det. Wasn't a good choice as it seems to never simplify itself!! So, I tried Expressing the Second Det. directly as the product of two or more Det. ! Still It was no use. I finally concluded that the Second Det. Can't be expressed as a product without Rearrangement ! But Rearranging it makes it more creepier! Could you please Guide me how to Rearrange it so that it could be easily expressed as a product? Or is their Another Way for this type of Problem? Please Let me know! Any help would be Appreciated.","\text{If } \Delta = \left|\begin{array}{ccc} a & b & c \\ c & a & b \\ b & c & a \end{array}\right|, \text{ then the value of }  \left|\begin{array}{ccc} a^2 - bc & b^2 - ca & c^2 - ab \\ c^2 - ab & a^2 - bc & b^2 - ca \\ b^2 - ca & c^2 - ab & a^2 - bc \end{array}\right| \text{ is:}  \Delta  \Delta","['matrices', 'determinant']"
17,Every primitive matrix is irreducible?,Every primitive matrix is irreducible?,,"$A$ is reducible if there is some permutation matrix $P$ such that $$ PAP^T =   \begin{bmatrix} B & C \\  O & D \\  \end{bmatrix} $$ And, if $A^k > O$ for some k, then $A$ is called primitive. Then, how can I show that every primitive matrix is irreducible?","$A$ is reducible if there is some permutation matrix $P$ such that $$ PAP^T =   \begin{bmatrix} B & C \\  O & D \\  \end{bmatrix} $$ And, if $A^k > O$ for some k, then $A$ is called primitive. Then, how can I show that every primitive matrix is irreducible?",,['matrices']
18,Freedoms of real orthogonal matrices,Freedoms of real orthogonal matrices,,"I was trying to figure out, how many degrees of freedoms a $n\times n$-orthogonal matrix posses.The easiest way to determine that seems to be the fact that the matrix exponential of an antisymmetric matrix yields an orthogonal matrix: $M^T=-M, c=\exp(M) \Rightarrow c^T=c^{-1}$ A antisymmetric matrix possesses $\frac{n(n-1)}{2}$ degrees of freedom. BUT: When I also thought about how to parametrize these freedoms explicitly (without the exponential) I remembered, that rotations in $\mathbb{R}^n$ can be parametrized using $n-1$ angles or cosines. I dont' understand, where the remaining parameters are hidden? My guess is, that a orthoganl transformation in $n>3$ can be more complicated than a rotation or that there are different types of rotation (containing reflectiong or such things) and that all the combination of these different types accounts for the rest of the parameters. Thanks you for your help!","I was trying to figure out, how many degrees of freedoms a $n\times n$-orthogonal matrix posses.The easiest way to determine that seems to be the fact that the matrix exponential of an antisymmetric matrix yields an orthogonal matrix: $M^T=-M, c=\exp(M) \Rightarrow c^T=c^{-1}$ A antisymmetric matrix possesses $\frac{n(n-1)}{2}$ degrees of freedom. BUT: When I also thought about how to parametrize these freedoms explicitly (without the exponential) I remembered, that rotations in $\mathbb{R}^n$ can be parametrized using $n-1$ angles or cosines. I dont' understand, where the remaining parameters are hidden? My guess is, that a orthoganl transformation in $n>3$ can be more complicated than a rotation or that there are different types of rotation (containing reflectiong or such things) and that all the combination of these different types accounts for the rest of the parameters. Thanks you for your help!",,"['matrices', 'orthonormal', 'rotations']"
19,Eigenvalues bounded $\implies$ Matrix entries bounded?,Eigenvalues bounded  Matrix entries bounded?,\implies,Suppose $H$ is a Hermitian matrix with all its eigenvalues be bounded (below and above) say between $-n_0$ and $n_0$. Is it true that all members of the matrix $H$ is bounded (below and above) by some constant which does not depend on the order of the matrix?,Suppose $H$ is a Hermitian matrix with all its eigenvalues be bounded (below and above) say between $-n_0$ and $n_0$. Is it true that all members of the matrix $H$ is bounded (below and above) by some constant which does not depend on the order of the matrix?,,"['matrices', 'eigenvalues-eigenvectors']"
20,Question on upper triangular matrix with complex eigenvalues with modulus less than 1,Question on upper triangular matrix with complex eigenvalues with modulus less than 1,,"This is problem 16, Section 6.B from Linear Algebra Done Right, 3rd Edition. Suppose the field is $\mathbb{C}$, $V$ is finite-dimensional, $T \in  \mathcal{L}(V)$, all the eigenvalues of $T$ have absolute value less than 1, and $\epsilon > 0$.  Show that there exists a positive integer $m$ such that $||T^m v|| < \epsilon ||v|| \; \forall \; v \in V$. At this point in the book Jordan form has not been proved, so all I can use is Schur's Theorem that guarantees upper triangular matrix with eigenvalues on the diagonal. Could anyone give me a hint on how to proceed ? I tried computing powers of $T$, but while I get $\lambda^m$ on diagonal and same upper triangular structure, I am unable to manage the off-diagonal entries and control their growth. Note that in this case if matrix is split as diagonal and strictly upper diagonal matrix as $T = D + U$, $D$ and $U$ do not commute because entries on diagoals of $D$ are unequal.","This is problem 16, Section 6.B from Linear Algebra Done Right, 3rd Edition. Suppose the field is $\mathbb{C}$, $V$ is finite-dimensional, $T \in  \mathcal{L}(V)$, all the eigenvalues of $T$ have absolute value less than 1, and $\epsilon > 0$.  Show that there exists a positive integer $m$ such that $||T^m v|| < \epsilon ||v|| \; \forall \; v \in V$. At this point in the book Jordan form has not been proved, so all I can use is Schur's Theorem that guarantees upper triangular matrix with eigenvalues on the diagonal. Could anyone give me a hint on how to proceed ? I tried computing powers of $T$, but while I get $\lambda^m$ on diagonal and same upper triangular structure, I am unable to manage the off-diagonal entries and control their growth. Note that in this case if matrix is split as diagonal and strictly upper diagonal matrix as $T = D + U$, $D$ and $U$ do not commute because entries on diagoals of $D$ are unequal.",,"['matrices', 'eigenvalues-eigenvectors', 'normed-spaces']"
21,"If $\det A=1$ and the matrices $A^{2015}$ and $A^{2017}$ are integer, is $A$ an integer matrix?","If  and the matrices  and  are integer, is  an integer matrix?",\det A=1 A^{2015} A^{2017} A,Assume $\det(A) = 1$ and all the numbers in the matrices $A^{2015}$ and $A^{2017}$ are integers. Can I say that all numbers in $A$ are integers too? How can I prove it?,Assume $\det(A) = 1$ and all the numbers in the matrices $A^{2015}$ and $A^{2017}$ are integers. Can I say that all numbers in $A$ are integers too? How can I prove it?,,['matrices']
22,How to calculate the covariance matrix,How to calculate the covariance matrix,,I tried searching a lot on the net and got the following sources: Source One Source Two The first source seems to be incorrect cause when I calculate it using matlab it comes to be different from what they have given as the answer. As for the second link I cant understand that cause its not completely explaining as to how to calculate. Could anyone please provide me with a sound link or explain how to calculate a co-variance matrix?,I tried searching a lot on the net and got the following sources: Source One Source Two The first source seems to be incorrect cause when I calculate it using matlab it comes to be different from what they have given as the answer. As for the second link I cant understand that cause its not completely explaining as to how to calculate. Could anyone please provide me with a sound link or explain how to calculate a co-variance matrix?,,"['matrices', 'statistics', 'statistical-inference']"
23,How to calculate what matrix will transform specified points to other specified points,How to calculate what matrix will transform specified points to other specified points,,"I want to transform an image. As far as I was able to find out, I can achieve this with a matrix, right? So here is my problem: how do I get this matrix if the only thing I know are the following starting and ending points? $$\begin{matrix} \text{Starting Points} & \text{Target Points} \\ \mathrm{TL}(3,5)     &    \mathrm{TL}'(0,3) \\ \mathrm{TR}(5,5)     &    \mathrm{TR}'(3,3) \\ \mathrm{LL}(2,2)     &    \mathrm{LL}'(0,0) \\ \mathrm{LR}(6,1)     &    \mathrm{LR}'(3,0) \end{matrix}$$","I want to transform an image. As far as I was able to find out, I can achieve this with a matrix, right? So here is my problem: how do I get this matrix if the only thing I know are the following starting and ending points? $$\begin{matrix} \text{Starting Points} & \text{Target Points} \\ \mathrm{TL}(3,5)     &    \mathrm{TL}'(0,3) \\ \mathrm{TR}(5,5)     &    \mathrm{TR}'(3,3) \\ \mathrm{LL}(2,2)     &    \mathrm{LL}'(0,0) \\ \mathrm{LR}(6,1)     &    \mathrm{LR}'(3,0) \end{matrix}$$",,['matrices']
24,Is it possible to swap only two elements on a 3x3 grid by swapping rows and columns?,Is it possible to swap only two elements on a 3x3 grid by swapping rows and columns?,,"Take a 3x3 grid with a different number in each spot, like one cell of a Sudoku puzzle: \begin{array}{|c|c|c|} \hline 1&2&3\\ \hline 4&5&6\\ \hline 7&8&9\\ \hline \end{array} Suppose you can rearrange the number by swapping two rows or two columns. $$\left.\begin{array}{|c|c|c|}\hline 1&2&3\\ \hline 4&5&6\\ \hline7 & 8 & 9\\ \hline \end{array} \longrightarrow \begin{array}{|c|c|c|}\hline 1&2&3\\ \hline7&8&9\\ \hline4&5&6\\ \hline \end{array} \longrightarrow \begin{array}{|c|c|c|}\hline2&1&3\\ \hline8&7&9\\ \hline5&4&6\\ \hline \end{array} \right.$$ Is it possible to manipulate the grid such that an arbitrary pair of numbers is swapped (with the rest returned to their original positions)? e.g. $$\left.\begin{array}{|c|c|c|}\hline 1&2&6\\ \hline 4&5&3\\ \hline7&8&9\\ \hline \end{array} \qquad\text{or}\qquad \begin{array}{|c|c|c|}\hline 8&2&3\\ \hline4&5&6\\ \hline7&1&9\\ \hline \end{array} \qquad\text{or}\qquad \begin{array}{|c|c|c|}\hline1&2&3\\ \hline4&7&6\\ \hline5&8&9\\ \hline \end{array} \right.$$","Take a 3x3 grid with a different number in each spot, like one cell of a Sudoku puzzle: Suppose you can rearrange the number by swapping two rows or two columns. Is it possible to manipulate the grid such that an arbitrary pair of numbers is swapped (with the rest returned to their original positions)? e.g.","\begin{array}{|c|c|c|}
\hline
1&2&3\\
\hline
4&5&6\\
\hline
7&8&9\\
\hline
\end{array} \left.\begin{array}{|c|c|c|}\hline 1&2&3\\ \hline 4&5&6\\ \hline7 & 8 & 9\\ \hline
\end{array} \longrightarrow \begin{array}{|c|c|c|}\hline 1&2&3\\ \hline7&8&9\\ \hline4&5&6\\ \hline
\end{array} \longrightarrow \begin{array}{|c|c|c|}\hline2&1&3\\ \hline8&7&9\\ \hline5&4&6\\ \hline
\end{array}
\right. \left.\begin{array}{|c|c|c|}\hline 1&2&6\\ \hline 4&5&3\\ \hline7&8&9\\ \hline
\end{array} \qquad\text{or}\qquad \begin{array}{|c|c|c|}\hline 8&2&3\\ \hline4&5&6\\ \hline7&1&9\\ \hline
\end{array} \qquad\text{or}\qquad \begin{array}{|c|c|c|}\hline1&2&3\\ \hline4&7&6\\ \hline5&8&9\\ \hline
\end{array}
\right.","['matrices', 'permutations', 'puzzle']"
25,Is the negative power of a matrix defined?,Is the negative power of a matrix defined?,,"I had a matrices exam last week and I wrote $A^{-2}$ to refer to $(A^{-1})^2$ (A being an invertible matrix). Initially, I was given the question wrong, but I told the professor that I saw in a book that $(A^{-1})^n = A^{-n} = (A^n)^{-1}$ and gave me the point. He said that $A^{-n}$ isn't defined for matrices, which I may believe since I haven't seen it in this forum yet, but I would find having to write $(A^{-1})^n$ or $(A^n)^{-1}$ very annoying just for the fact that it isn't defined. So, is it (defined)? P.D.: The book is Linear Algebra by Paul Dawkins 1 .","I had a matrices exam last week and I wrote $A^{-2}$ to refer to $(A^{-1})^2$ (A being an invertible matrix). Initially, I was given the question wrong, but I told the professor that I saw in a book that $(A^{-1})^n = A^{-n} = (A^n)^{-1}$ and gave me the point. He said that $A^{-n}$ isn't defined for matrices, which I may believe since I haven't seen it in this forum yet, but I would find having to write $(A^{-1})^n$ or $(A^n)^{-1}$ very annoying just for the fact that it isn't defined. So, is it (defined)? P.D.: The book is Linear Algebra by Paul Dawkins 1 .",,"['matrices', 'inverse']"
26,Who established the tradition of using $X^{\prime}$ instead of $X^{T}$ to denote the matrix transpose?,Who established the tradition of using  instead of  to denote the matrix transpose?,X^{\prime} X^{T},"From being away from mathematicians for a while and spending most of my time with econometricians and statisticians, one thing I've noticed is that econometricians and statisticians like to use $\prime$ to denote the matrix transpose, e.g., $X^{\prime}$. However, when you show this notation to a mathematician, they'd think that you mean a matrix distinct from $X$; i.e., $X$ and $X^{\prime}$ are just two distinct matrices with no explicit relation. Hence when I'm on MSE, I always try to use $X^{T}$ instead. My guess is that the $X^{T}$ notation has been around longer than $X^{\prime}$. Who was the first to use $X^{\prime}$ to denote the matrix transpose?","From being away from mathematicians for a while and spending most of my time with econometricians and statisticians, one thing I've noticed is that econometricians and statisticians like to use $\prime$ to denote the matrix transpose, e.g., $X^{\prime}$. However, when you show this notation to a mathematician, they'd think that you mean a matrix distinct from $X$; i.e., $X$ and $X^{\prime}$ are just two distinct matrices with no explicit relation. Hence when I'm on MSE, I always try to use $X^{T}$ instead. My guess is that the $X^{T}$ notation has been around longer than $X^{\prime}$. Who was the first to use $X^{\prime}$ to denote the matrix transpose?",,"['matrices', 'notation', 'math-history', 'transpose']"
27,Does a symmetric matrix necessarily have a symmetric square root?,Does a symmetric matrix necessarily have a symmetric square root?,,"Does a symmetric matrix necessarily have a symmetric square root, and why? If not, then does a symmetric matrix that is also semi-definite necessarily have a symmetric square root (that may or may not be semi-definite), and why?","Does a symmetric matrix necessarily have a symmetric square root, and why? If not, then does a symmetric matrix that is also semi-definite necessarily have a symmetric square root (that may or may not be semi-definite), and why?",,"['matrices', 'symmetric-matrices']"
28,Find the determinant when all entries are $1$ except for zeroes on the main diagonal,Find the determinant when all entries are  except for zeroes on the main diagonal,1,"Let $J_n$ be an $n\times n$ matrix all of whose entries are $1$, and $I_n$ be the identity matrix. Define $$K_n = J_n-I_n$$ For $n=1$ to $5$ my (usually unreliable!) hand calculations suggest that $$\det K_n = (-1)^{n-1}(n-1)$$ Question (a) are these values correct? (b) is the generalization to any positive integer $n$ valid? (c) if (b) is true, how can the result be demonstrated? My only idea so far is to use the product of eigenvalues: $$\det K_n = \prod_{j=1}^n \lambda_j$$ ($-1$ is an eigenvalue for all $n$) Any assistance much appreciated. (note: a similar question concerning skew-symmetric matrices Determinant of a special skew-symmetric matrix may contribute some relevant ideas. or perhaps Determinant of a matrix with $t$ in all off-diagonal entries. has greater relevance)","Let $J_n$ be an $n\times n$ matrix all of whose entries are $1$, and $I_n$ be the identity matrix. Define $$K_n = J_n-I_n$$ For $n=1$ to $5$ my (usually unreliable!) hand calculations suggest that $$\det K_n = (-1)^{n-1}(n-1)$$ Question (a) are these values correct? (b) is the generalization to any positive integer $n$ valid? (c) if (b) is true, how can the result be demonstrated? My only idea so far is to use the product of eigenvalues: $$\det K_n = \prod_{j=1}^n \lambda_j$$ ($-1$ is an eigenvalue for all $n$) Any assistance much appreciated. (note: a similar question concerning skew-symmetric matrices Determinant of a special skew-symmetric matrix may contribute some relevant ideas. or perhaps Determinant of a matrix with $t$ in all off-diagonal entries. has greater relevance)",,"['matrices', 'determinant']"
29,"Powers of matrices: if $A^3 = A$, how can we show that $A^{27}= A$?","Powers of matrices: if , how can we show that ?",A^3 = A A^{27}= A,Suppose that $A^3 = A$. How can we show that $A^{27}= A$? Any guidance would be much appreciated. Is it sufficient or correct to write that: $$A^9=(A^3)^3=(A)^3= A \implies  A^{27}= (A^9)^3 = (A)^3=A ?$$,Suppose that $A^3 = A$. How can we show that $A^{27}= A$? Any guidance would be much appreciated. Is it sufficient or correct to write that: $$A^9=(A^3)^3=(A)^3= A \implies  A^{27}= (A^9)^3 = (A)^3=A ?$$,,['matrices']
30,"If $A^kX=B^kY$ for all $k$, is $X=Y$?","If  for all , is ?",A^kX=B^kY k X=Y,"This one is more general than the one I asked before. Given invertible matrices $A,B$ and matrices $X,Y$ all with size $n$, such that $A^k X = B^k Y$ for $k=1,2,...,2n$. Does it follow that $X = Y$? I have no idea where to work. Thanks for any help.","This one is more general than the one I asked before. Given invertible matrices $A,B$ and matrices $X,Y$ all with size $n$, such that $A^k X = B^k Y$ for $k=1,2,...,2n$. Does it follow that $X = Y$? I have no idea where to work. Thanks for any help.",,['matrices']
31,Proof of Neumann Lemma,Proof of Neumann Lemma,,"Prove that if $\|A\| < 1$, then $I-A$ is invertible. Here, $\|\cdot\|$ is a matrix norm induced by a vector norm. This lemma is referred to as Neumann Lemma . Any ideas on how to go ahead with this? Thanks.","Prove that if $\|A\| < 1$, then $I-A$ is invertible. Here, $\|\cdot\|$ is a matrix norm induced by a vector norm. This lemma is referred to as Neumann Lemma . Any ideas on how to go ahead with this? Thanks.",,"['matrices', 'normed-spaces']"
32,"What does it mean for a matrix to be ""non-negative definite""?","What does it mean for a matrix to be ""non-negative definite""?",,"In some old course notes I'm reading to touch up on statistical forecasting methods, the book often makes reference to ""non-negative definite"" matrices. I know what a semi-positive definite, positive definite, and indefinite matrix are, but I've never heard this terminology before. Further, online resources don't really seem to mention it, so I'm unsure what exactly it is equivalent to as the ""non-negative definite"" property isn't directly applied anywhere. Any ideas?","In some old course notes I'm reading to touch up on statistical forecasting methods, the book often makes reference to ""non-negative definite"" matrices. I know what a semi-positive definite, positive definite, and indefinite matrix are, but I've never heard this terminology before. Further, online resources don't really seem to mention it, so I'm unsure what exactly it is equivalent to as the ""non-negative definite"" property isn't directly applied anywhere. Any ideas?",,"['matrices', 'terminology']"
33,Derivative of Matrix Exponential as Integral,Derivative of Matrix Exponential as Integral,,"I saw this ""standard"" identity in a physics paper and I was wondering how to prove it \begin{align*} \frac{d}{dx} e^{A+xB}\bigg|_{x = 0} = e^A\int_0^1 e^{A\tau}B e^{-A\tau}\,d\tau \end{align*} I tried using Baker-Campbell-Hausdorff but I don't really know how to continue and I'm especially confused where the integral comes from. 2023 edit: I made a typo in the formula above, the integrand of the RHS should instead be $e^{-A\tau}Be^{A\tau}$ , as in martini's answer.","I saw this ""standard"" identity in a physics paper and I was wondering how to prove it I tried using Baker-Campbell-Hausdorff but I don't really know how to continue and I'm especially confused where the integral comes from. 2023 edit: I made a typo in the formula above, the integrand of the RHS should instead be , as in martini's answer.","\begin{align*}
\frac{d}{dx} e^{A+xB}\bigg|_{x = 0} = e^A\int_0^1 e^{A\tau}B e^{-A\tau}\,d\tau
\end{align*} e^{-A\tau}Be^{A\tau}","['matrices', 'exponential-function']"
34,Inverse of a rigid transformation,Inverse of a rigid transformation,,"I would be grateful for any help with the steps required to complete this calculation. You may assume that I have some experience with matrices from before, but I am obviously no master! So we have a transformation between the frame $O$ and $O'$. It's a rigid transformation so it shouldn't deform the object.  Supposedly first you align the axes by rotating the frame $O$ around $x$, then translate the frame $O$ to $O'$ - and finally align the two coordinate frames. I've seen the problem in practice, but I struggle to understand the problem when I'm supposed to express it as a function of the rotation matrix. I've found it difficult to find documentation on this online, any good resources relevant to this question are gladly accepted!","I would be grateful for any help with the steps required to complete this calculation. You may assume that I have some experience with matrices from before, but I am obviously no master! So we have a transformation between the frame $O$ and $O'$. It's a rigid transformation so it shouldn't deform the object.  Supposedly first you align the axes by rotating the frame $O$ around $x$, then translate the frame $O$ to $O'$ - and finally align the two coordinate frames. I've seen the problem in practice, but I struggle to understand the problem when I'm supposed to express it as a function of the rotation matrix. I've found it difficult to find documentation on this online, any good resources relevant to this question are gladly accepted!",,"['matrices', 'rotations', 'rigid-transformation']"
35,Matrix Multiplication Understanding,Matrix Multiplication Understanding,,I have to multiply two 3x3 matrices. I don't understand the answer. $$A=\begin{pmatrix}1&0&1\\ 0&1&1\\ 0&0&1 \end{pmatrix}$$ $$B=\begin{pmatrix}1&0&-1\\ 0&1&-1\\ 0&0&1 \end{pmatrix}$$ The answer given online is $$AB=\begin{pmatrix}1&0&1\\ 0&1&1\\ 0&0&1 \end{pmatrix}$$ I don't understand how this is so. I have said that: $$AB=\begin{pmatrix}(1)(1)+(0)(0)+(1)(0) & (1)(0)+(0)(1)+(1)(0) & (1)(-1)+(0)(-1)+(1)(1)\\(0)(1)+(1)(0)+(0)(0) & (0)(0)+(1)(1)+(1)(0) & (0)(-1)+(1)(-1)+(1)(1) \\ (0)(1)+(0)(0)+(1)(0) & (0)(0)+(0)(1)+(1)(0) & (0)(-1)+(0)(-1)+(1)(1) \end{pmatrix}$$ $$AB=\begin{pmatrix}1 & 0 & (1)(-1)+(0)(-1)+(1)(1)\\0 & 1 & (0)(-1)+(1)(-1)+(1)(1) \\ 0 & 0 & 1 \end{pmatrix}$$ $$AB=\begin{pmatrix}1 & 0 & 0\\0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}$$ Can anyone tell me where I am going wrong?,I have to multiply two 3x3 matrices. I don't understand the answer. $$A=\begin{pmatrix}1&0&1\\ 0&1&1\\ 0&0&1 \end{pmatrix}$$ $$B=\begin{pmatrix}1&0&-1\\ 0&1&-1\\ 0&0&1 \end{pmatrix}$$ The answer given online is $$AB=\begin{pmatrix}1&0&1\\ 0&1&1\\ 0&0&1 \end{pmatrix}$$ I don't understand how this is so. I have said that: $$AB=\begin{pmatrix}(1)(1)+(0)(0)+(1)(0) & (1)(0)+(0)(1)+(1)(0) & (1)(-1)+(0)(-1)+(1)(1)\\(0)(1)+(1)(0)+(0)(0) & (0)(0)+(1)(1)+(1)(0) & (0)(-1)+(1)(-1)+(1)(1) \\ (0)(1)+(0)(0)+(1)(0) & (0)(0)+(0)(1)+(1)(0) & (0)(-1)+(0)(-1)+(1)(1) \end{pmatrix}$$ $$AB=\begin{pmatrix}1 & 0 & (1)(-1)+(0)(-1)+(1)(1)\\0 & 1 & (0)(-1)+(1)(-1)+(1)(1) \\ 0 & 0 & 1 \end{pmatrix}$$ $$AB=\begin{pmatrix}1 & 0 & 0\\0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}$$ Can anyone tell me where I am going wrong?,,['matrices']
36,Show non-symmetric matrix has non-orthogonal eigenvectors,Show non-symmetric matrix has non-orthogonal eigenvectors,,"I'm struggling with a problem from Boas's Mathematical Methods in the Physical Sciences. The question is, for a 2x2 matrix M s.t. M is real, not symmetric, with eigenvalues real and not equal, show that the eigenvectors of M are not orthogonal. I've tried to manipulate MC = CD in Einstein notation (C, a matrix that diagonalizes M and D, the matrix of eigenvalues) with little luck. I've also tried manipulation of arbitrary elements of M*C and C*D to get an expression for the inner product of the eigenvectors, but I'm not finding any relationships that force the dot product to be non-zero when the off-diagonal elements of M aren't equal. Any suggestions for a better approach would be appreciated. I've already turned in the assignment, but this question's still bugging me!","I'm struggling with a problem from Boas's Mathematical Methods in the Physical Sciences. The question is, for a 2x2 matrix M s.t. M is real, not symmetric, with eigenvalues real and not equal, show that the eigenvectors of M are not orthogonal. I've tried to manipulate MC = CD in Einstein notation (C, a matrix that diagonalizes M and D, the matrix of eigenvalues) with little luck. I've also tried manipulation of arbitrary elements of M*C and C*D to get an expression for the inner product of the eigenvectors, but I'm not finding any relationships that force the dot product to be non-zero when the off-diagonal elements of M aren't equal. Any suggestions for a better approach would be appreciated. I've already turned in the assignment, but this question's still bugging me!",,"['matrices', 'eigenvalues-eigenvectors']"
37,Trace of the matrix power,Trace of the matrix power,,Say I have matrix $A = \begin{bmatrix} a & 0 & -c\\ 0 & b & 0\\ -c & 0 & a  \end{bmatrix}$. What is matrix trace tr(A^200) Thanks much!,Say I have matrix $A = \begin{bmatrix} a & 0 & -c\\ 0 & b & 0\\ -c & 0 & a  \end{bmatrix}$. What is matrix trace tr(A^200) Thanks much!,,"['matrices', 'determinant', 'exponentiation', 'trace']"
38,Formal proof of $\det(I + tA) = \prod\limits_{i=1}^n (1 + t\lambda_i)$,Formal proof of,\det(I + tA) = \prod\limits_{i=1}^n (1 + t\lambda_i),"I'm looking for a formal proof for: $$\det(I + tA) = \prod\limits_{i=1}^n (1 + t\lambda_i).$$ I'm very new to matrix theory therefore please forgive me if you find this elementary. Your help in this matter is appreciated. Thanks, Edit: $A$ is a symmetric $n\times n$ matrix and $\lambda_i$ are the eigenvalues of $A$.","I'm looking for a formal proof for: $$\det(I + tA) = \prod\limits_{i=1}^n (1 + t\lambda_i).$$ I'm very new to matrix theory therefore please forgive me if you find this elementary. Your help in this matter is appreciated. Thanks, Edit: $A$ is a symmetric $n\times n$ matrix and $\lambda_i$ are the eigenvalues of $A$.",,"['matrices', 'eigenvalues-eigenvectors', 'determinant']"
39,How to convert any non-negative matrix into a doubly stochastic matrix?,How to convert any non-negative matrix into a doubly stochastic matrix?,,"Given a non-negative real matrix $A \in \Bbb R_+^{m \times n}$ , how do I convert it to a doubly stochastic matrix (each row and column sums to $1$ ) $$\sum_{j=1}^n A_{ij}= 1, \qquad \forall i = 1, \dots, m \tag{row sum}$$ $$\sum_{i=1}^m A_{ij}= 1, \qquad \forall j = 1, \dots, n \tag{column sum}$$ Is the conversion possible? If not, can we find a nearest matrix that is doubly stochastic matrix?","Given a non-negative real matrix , how do I convert it to a doubly stochastic matrix (each row and column sums to ) Is the conversion possible? If not, can we find a nearest matrix that is doubly stochastic matrix?","A \in \Bbb R_+^{m \times n} 1 \sum_{j=1}^n A_{ij}= 1, \qquad \forall i = 1, \dots, m \tag{row sum} \sum_{i=1}^m A_{ij}= 1, \qquad \forall j = 1, \dots, n \tag{column sum}","['matrices', 'stochastic-matrices', 'nonnegative-matrices']"
40,Maximize sum of $(x_1+\dots+x_k)^2$ on the unit $n$-sphere,Maximize sum of  on the unit -sphere,(x_1+\dots+x_k)^2 n,"Given any positive integer $n$ , let $t(n)$ be the smallest real number such that for any real numbers $x_1,\dots,x_n$ , the following inequality holds $$ \sum\limits_{k=1}^n (x_1+\dots+x_k)^2 \leqslant t(n) \sum\limits_{i=1}^n x_i^2.$$ Please give an expression for $t(n)$ . Attempt: The problem is equivalent to calculating the maximum of $\sum\limits_{k=1}^n (x_1+\dots+x_k)^2$ on the unit $n$ -sphere $\sum\limits_{i=1}^n x_i^2=1$ . Let $$ A := \begin{pmatrix} n & n-1 & n-2 & \cdots & 1\\ n-1 & n-1 & n-2 & \cdots & 1\\ n-2 & n-2 & n-2 & \cdots & 1\\ \vdots & \vdots & \vdots & \ddots & \vdots\\ 1 & 1 & 1 & \cdots & 1 \end{pmatrix}$$ Then $$\sum\limits_{k=1}^n (x_1+\cdots+x_k)^2=xAx^T$$ where $x=(x_1,\dots,x_n)$ . According to Lagrange multipliers, we know the problem is equivalent to calculating the maximum eigenvalue of $A$ . Then I'm stuck. The book says that the answer is $$\frac{1}{4\sin^2\left(\frac{\pi}{4n+2}\right)}.$$ It is really amazing. I would appreciate it if someone could please give me some hints.","Given any positive integer , let be the smallest real number such that for any real numbers , the following inequality holds Please give an expression for . Attempt: The problem is equivalent to calculating the maximum of on the unit -sphere . Let Then where . According to Lagrange multipliers, we know the problem is equivalent to calculating the maximum eigenvalue of . Then I'm stuck. The book says that the answer is It is really amazing. I would appreciate it if someone could please give me some hints.","n t(n) x_1,\dots,x_n  \sum\limits_{k=1}^n (x_1+\dots+x_k)^2 \leqslant t(n) \sum\limits_{i=1}^n x_i^2. t(n) \sum\limits_{k=1}^n (x_1+\dots+x_k)^2 n \sum\limits_{i=1}^n x_i^2=1  A := \begin{pmatrix}
n & n-1 & n-2 & \cdots & 1\\
n-1 & n-1 & n-2 & \cdots & 1\\
n-2 & n-2 & n-2 & \cdots & 1\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
1 & 1 & 1 & \cdots & 1
\end{pmatrix} \sum\limits_{k=1}^n (x_1+\cdots+x_k)^2=xAx^T x=(x_1,\dots,x_n) A \frac{1}{4\sin^2\left(\frac{\pi}{4n+2}\right)}.","['matrices', 'inequality', 'eigenvalues-eigenvectors', 'quadratic-forms']"
41,how to use matrix to prove this identity?,how to use matrix to prove this identity?,,"if $a_{n},b_{n}$ such $a_{0}=b_{0}=1$ $$\begin{cases}a_{n}=5a_{n-1}+7b_{n-1}\\ b_{n}=7a_{n-1}+10b_{n-1},\forall n=1,2,3,\cdots \end{cases}$$ show that $$a_{m+n}+b_{m+n}=a_{m}a_{n}+b_{m}b_{n}$$ It's an interesting identity, and I've proved it with mathematical induction, but I feel like it's more obvious with a matrix, but I don't, so can someone please prove it with a matrix? Thank you $$\begin{bmatrix} a_{n}\\ b_{n}\end{bmatrix}=\begin{bmatrix} 5&7\\ 7&10\end{bmatrix}\cdot \begin{bmatrix} a_{n-1}\\ b_{n-1}\end{bmatrix}$$","if such show that It's an interesting identity, and I've proved it with mathematical induction, but I feel like it's more obvious with a matrix, but I don't, so can someone please prove it with a matrix? Thank you","a_{n},b_{n} a_{0}=b_{0}=1 \begin{cases}a_{n}=5a_{n-1}+7b_{n-1}\\
b_{n}=7a_{n-1}+10b_{n-1},\forall n=1,2,3,\cdots
\end{cases} a_{m+n}+b_{m+n}=a_{m}a_{n}+b_{m}b_{n} \begin{bmatrix}
a_{n}\\
b_{n}\end{bmatrix}=\begin{bmatrix}
5&7\\
7&10\end{bmatrix}\cdot \begin{bmatrix}
a_{n-1}\\
b_{n-1}\end{bmatrix}",['matrices']
42,Determinant of Symmetric Matrix $\mathbf{G}=a\mathbf{I}+b\boldsymbol{ee}^T$,Determinant of Symmetric Matrix,\mathbf{G}=a\mathbf{I}+b\boldsymbol{ee}^T,"To give the close-form of $\det(\mathbf{G})$, where $\mathbf{G}$ is  \begin{align} \mathbf{G}=a\mathbf{I}+b\boldsymbol{ee}^T \end{align} in which $a$ and $b$ are constant, and $\boldsymbol{e}$ is a column vector with all elements being $1$. In addition, $(\cdot)^T$ is transposition operation. $\mathbf{G}$ is $u\times u$. We use $\mathbf{G}_u$ to underline the dimension of $\mathbf{G}$. The question is to determine $\det(\mathbf{G})$. As I know: We rewrite $\mathbf{G}_u$ as  \begin{align} \mathbf{G}_u=\left[\begin{array}{ccc} \mathbf{G}_{u-1} & b\\ b & a+b \end{array} \right] \end{align} Using the determinant of  block matrix lemma  \begin{align} \det\left[\begin{array}{ccc} \mathbf{A} & \mathbf{B}\\ \mathbf{C} & \mathbf{D} \end{array} \right]=\det(\mathbf{A})\det(\mathbf{D}-\mathbf{C}\mathbf{A}^{-1}\mathbf{B}) \end{align} We then have  \begin{align} \det(\mathbf{G}_u)=\det(\mathbf{G}_{u-1})\det(a+b-b^2\boldsymbol{e}^T\mathbf{G}_{u-1}^{-1}\boldsymbol{e}) \end{align} It still needs to get $\mathbf{G}_{u-1}^{-1}$ via matrix inverse lemma \begin{align} (\mathbf{A}+\mathbf{BC})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1}\mathbf{B}(\mathbf{I}+\mathbf{CA}^{-1}\mathbf{B})^{-1}\mathbf{CA}^{-1} \end{align} We then have  \begin{align} \mathbf{G}_{u} &=\frac{1}{a}\mathbf{I}-\frac{1}{a^2}b\boldsymbol{e}\left(1+\frac{b}{a}\boldsymbol{e}^T\boldsymbol{e}\right)^{-1}\boldsymbol{e}^T\\ &=\frac{1}{a}\mathbf{I}-\frac{b}{a(a+bu)}\boldsymbol{ee}^T \end{align} where $\boldsymbol{e}^T\boldsymbol{e}=u$ is used. Plugging it into  \begin{align} &\det(a+b-b^2\boldsymbol{e}^T\mathbf{G}_{u-1}^{-1}\boldsymbol{e})\\ =&a+b-b^2\left({\frac{u-1}{a}-\frac{b(u-1)^2}{a[a+b(u-1)]}}\right) \end{align} Then  \begin{align} \det(\mathbf{G}_u)=\det(\mathbf{G}_{u-1})\left[{a+b-b^2\left({\frac{u-1}{a}-\frac{b(u-1)^2}{a[a+b(u-1)]}}\right)}\right] \end{align} Although, the connection between $\mathbf{G}_u$ and $\mathbf{G}_{u-1}$ is found, I can't give the expression of $\mathbf{G}_u$. Please give me hand, thanks a lot!","To give the close-form of $\det(\mathbf{G})$, where $\mathbf{G}$ is  \begin{align} \mathbf{G}=a\mathbf{I}+b\boldsymbol{ee}^T \end{align} in which $a$ and $b$ are constant, and $\boldsymbol{e}$ is a column vector with all elements being $1$. In addition, $(\cdot)^T$ is transposition operation. $\mathbf{G}$ is $u\times u$. We use $\mathbf{G}_u$ to underline the dimension of $\mathbf{G}$. The question is to determine $\det(\mathbf{G})$. As I know: We rewrite $\mathbf{G}_u$ as  \begin{align} \mathbf{G}_u=\left[\begin{array}{ccc} \mathbf{G}_{u-1} & b\\ b & a+b \end{array} \right] \end{align} Using the determinant of  block matrix lemma  \begin{align} \det\left[\begin{array}{ccc} \mathbf{A} & \mathbf{B}\\ \mathbf{C} & \mathbf{D} \end{array} \right]=\det(\mathbf{A})\det(\mathbf{D}-\mathbf{C}\mathbf{A}^{-1}\mathbf{B}) \end{align} We then have  \begin{align} \det(\mathbf{G}_u)=\det(\mathbf{G}_{u-1})\det(a+b-b^2\boldsymbol{e}^T\mathbf{G}_{u-1}^{-1}\boldsymbol{e}) \end{align} It still needs to get $\mathbf{G}_{u-1}^{-1}$ via matrix inverse lemma \begin{align} (\mathbf{A}+\mathbf{BC})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1}\mathbf{B}(\mathbf{I}+\mathbf{CA}^{-1}\mathbf{B})^{-1}\mathbf{CA}^{-1} \end{align} We then have  \begin{align} \mathbf{G}_{u} &=\frac{1}{a}\mathbf{I}-\frac{1}{a^2}b\boldsymbol{e}\left(1+\frac{b}{a}\boldsymbol{e}^T\boldsymbol{e}\right)^{-1}\boldsymbol{e}^T\\ &=\frac{1}{a}\mathbf{I}-\frac{b}{a(a+bu)}\boldsymbol{ee}^T \end{align} where $\boldsymbol{e}^T\boldsymbol{e}=u$ is used. Plugging it into  \begin{align} &\det(a+b-b^2\boldsymbol{e}^T\mathbf{G}_{u-1}^{-1}\boldsymbol{e})\\ =&a+b-b^2\left({\frac{u-1}{a}-\frac{b(u-1)^2}{a[a+b(u-1)]}}\right) \end{align} Then  \begin{align} \det(\mathbf{G}_u)=\det(\mathbf{G}_{u-1})\left[{a+b-b^2\left({\frac{u-1}{a}-\frac{b(u-1)^2}{a[a+b(u-1)]}}\right)}\right] \end{align} Although, the connection between $\mathbf{G}_u$ and $\mathbf{G}_{u-1}$ is found, I can't give the expression of $\mathbf{G}_u$. Please give me hand, thanks a lot!",,"['matrices', 'determinant']"
43,Matrix satisfying $A-I = A^{-1}$,Matrix satisfying,A-I = A^{-1},"Recall the (infinitely) continued fraction definition of the golden ratio \begin{align} \phi = 1+\frac{1}{1+\frac{1}{1+\frac{1}{1+\frac{1}{1+\frac{1}{1+\cdots}}}}} \end{align} This is equivalent to the expression \begin{align} \phi = 1+\frac{1}{\phi} \end{align} Saying that the inverse of $\phi$ is equal to $\phi-1$. Inspired by this fact, consider a square matrix $A$ satisfying: \begin{align} A-I = A^{-1} \end{align} Does this relationship have any significance?","Recall the (infinitely) continued fraction definition of the golden ratio \begin{align} \phi = 1+\frac{1}{1+\frac{1}{1+\frac{1}{1+\frac{1}{1+\frac{1}{1+\cdots}}}}} \end{align} This is equivalent to the expression \begin{align} \phi = 1+\frac{1}{\phi} \end{align} Saying that the inverse of $\phi$ is equal to $\phi-1$. Inspired by this fact, consider a square matrix $A$ satisfying: \begin{align} A-I = A^{-1} \end{align} Does this relationship have any significance?",,"['matrices', 'golden-ratio']"
44,"Examples about that $\exp(X+Y)=\exp(X) \exp(Y)$ does not imply $[X,Y]=0$ where $X,Y$ are $n \times n $ matrix",Examples about that  does not imply  where  are  matrix,"\exp(X+Y)=\exp(X) \exp(Y) [X,Y]=0 X,Y n \times n ","I read the https://en.wikipedia.org/wiki/Matrix_exponential There is a saying that ""The converse is not true in general. The equation $\exp(X+Y)=\exp(X) \exp(Y)$ does not imply that X and Y commute."" I would like to know some concrete examples.","I read the https://en.wikipedia.org/wiki/Matrix_exponential There is a saying that ""The converse is not true in general. The equation $\exp(X+Y)=\exp(X) \exp(Y)$ does not imply that X and Y commute."" I would like to know some concrete examples.",,"['matrices', 'examples-counterexamples', 'matrix-exponential']"
45,Tensors and matrices multiplication,Tensors and matrices multiplication,,"I have to prove an equality between matrices $R=OTDO$ where $R$ is a $M\times M$ matrix $O$ is a $2\times M$ matrix $T$ is a $M\times M\times M$ tensor $D$ is a diagonal $2\times 2$ matrix The entries of the matrices and the tensor are probabilities so the result should somehow be the consequence of Bayes formula. The problem is that I have no idea how to compute that because I don't know how to use tensors. I had an algebra course about tensor products of vector spaces a long time ago but it was very abstract so I don't know how to multiply tensors in practice. I'm surprised because the first matrix of the product has $2$ rows, the last one has $M$ columns and yet the result is a $M\times M$ matrix. Could you explain how to do this? For example, what's the dimension of $OT$? I am familiar with the Kronecker product of matrices, is it useful here? EDIT Stupid me... I've spent hours trying to understand this product and... this was a typo. It was $O^TDO$ and the equality was straightforward... I've been confused by the fact that the tensor $T$ did exist and there could be and equality involving it. At least I've learnt a few things about tensors, thank you again for the answers!","I have to prove an equality between matrices $R=OTDO$ where $R$ is a $M\times M$ matrix $O$ is a $2\times M$ matrix $T$ is a $M\times M\times M$ tensor $D$ is a diagonal $2\times 2$ matrix The entries of the matrices and the tensor are probabilities so the result should somehow be the consequence of Bayes formula. The problem is that I have no idea how to compute that because I don't know how to use tensors. I had an algebra course about tensor products of vector spaces a long time ago but it was very abstract so I don't know how to multiply tensors in practice. I'm surprised because the first matrix of the product has $2$ rows, the last one has $M$ columns and yet the result is a $M\times M$ matrix. Could you explain how to do this? For example, what's the dimension of $OT$? I am familiar with the Kronecker product of matrices, is it useful here? EDIT Stupid me... I've spent hours trying to understand this product and... this was a typo. It was $O^TDO$ and the equality was straightforward... I've been confused by the fact that the tensor $T$ did exist and there could be and equality involving it. At least I've learnt a few things about tensors, thank you again for the answers!",,"['matrices', 'tensors']"
46,"What is the Haar measures on $SL(2, \mathbb R)$ And $SL(2,\mathbb R) / SL(2,\mathbb Z)$?",What is the Haar measures on  And ?,"SL(2, \mathbb R) SL(2,\mathbb R) / SL(2,\mathbb Z)",How does one parametrize those spaces in order to do integration over them? What's a good reference for doing integral a with Haar measures over matrix groups?,How does one parametrize those spaces in order to do integration over them? What's a good reference for doing integral a with Haar measures over matrix groups?,,"['matrices', 'analysis']"
47,Expression of rotation matrix from two vectors,Expression of rotation matrix from two vectors,,"What is the matrix expression of the rotation matrix in 3D which turns a vector $\vec{a}$ into a vector $\vec{b}$, with both vectors given by their coordinates? ($\vec{a} = (a_x, a_y, a_z)$ and $\vec{b} = (b_x, b_y, b_z)$, both already normalized). Other answers give a construction using an augmented 3D rotation matrix, where the angle and the base change matrices are given using the dot/cross products, but I couldn't find a direct expression of the 9 matrix fields using the 6 vector coordinates.","What is the matrix expression of the rotation matrix in 3D which turns a vector $\vec{a}$ into a vector $\vec{b}$, with both vectors given by their coordinates? ($\vec{a} = (a_x, a_y, a_z)$ and $\vec{b} = (b_x, b_y, b_z)$, both already normalized). Other answers give a construction using an augmented 3D rotation matrix, where the angle and the base change matrices are given using the dot/cross products, but I couldn't find a direct expression of the 9 matrix fields using the 6 vector coordinates.",,"['matrices', '3d', 'rotations', 'linear-transformations', 'rigid-transformation']"
48,An old test question proving $\|\mathbf{B} - \mathbf{A}\| \lt \frac{1}{\|\mathbf{A}^{-1}\|}$ implies invertiblity of $\mathbf{B}$,An old test question proving  implies invertiblity of,\|\mathbf{B} - \mathbf{A}\| \lt \frac{1}{\|\mathbf{A}^{-1}\|} \mathbf{B},"I have an old test question that I am not sure about and would like some idea. It is from a Numerical Analysis class. Suppose that $A$ is an invertible $n$-by-$n$ matrix. Prove that for every $n$-by-$n$ matrix $B$, the inequality $$ \|\mathbf{B} - \mathbf{A}\| \lt \frac{1}{\|\mathbf{A}^{-1}\|}$$ implies that $B$ is invertible. I answered the test question correctly, but am also interested in what this particular equation could be used for. EDIT: Here is what I did on my test (the test did give a hint to use the contrapositive): Suppose $A$ is invertible and $B$ is singular. Then for some $x \ne 0$, we have $Bx = 0$ Note that $$\begin{align} 0  \ne & \| x\| & = & \| A^{-1}Ax\| \\ &&  = & \|A^{-1}(Ax - Bx)\| \\ &&  = & \|A^{-1}(A-B)x\| \\ &&  \le & \|A^{-1}\|\cdot\|(A-B)x\| \\ \implies &  \|x\| & \le &  \|A^{-1}\|\cdot\|A-B\| \cdot \|x\|\\ \\ \implies & \frac{ \|x\|}{\|x\|} & \le &  \|A^{-1}\|\cdot\|A-B\| \cdot \frac{\|x\|}{\|x\|}  \,\,\,\,\text{since $\|x\| \ne 0$} \\ \implies & 1 & \le & \|A^{-1}\|\cdot\|A-B\| \\ \implies & \frac{ 1}{\|A^{-1}\|} & \le & \|A-B\| \\ \therefore & B \;\text{singular} \implies && \|A-B\| \ge \frac{1}{A^{-1}} \\ \text{By contrapositive argument,}&\text{ we have} \\ &\|A-B\|&  \lt &\frac{1}{\|A^{-1}\|} \implies \text{$B$ is invertible}\\ \end{align}  $$","I have an old test question that I am not sure about and would like some idea. It is from a Numerical Analysis class. Suppose that $A$ is an invertible $n$-by-$n$ matrix. Prove that for every $n$-by-$n$ matrix $B$, the inequality $$ \|\mathbf{B} - \mathbf{A}\| \lt \frac{1}{\|\mathbf{A}^{-1}\|}$$ implies that $B$ is invertible. I answered the test question correctly, but am also interested in what this particular equation could be used for. EDIT: Here is what I did on my test (the test did give a hint to use the contrapositive): Suppose $A$ is invertible and $B$ is singular. Then for some $x \ne 0$, we have $Bx = 0$ Note that $$\begin{align} 0  \ne & \| x\| & = & \| A^{-1}Ax\| \\ &&  = & \|A^{-1}(Ax - Bx)\| \\ &&  = & \|A^{-1}(A-B)x\| \\ &&  \le & \|A^{-1}\|\cdot\|(A-B)x\| \\ \implies &  \|x\| & \le &  \|A^{-1}\|\cdot\|A-B\| \cdot \|x\|\\ \\ \implies & \frac{ \|x\|}{\|x\|} & \le &  \|A^{-1}\|\cdot\|A-B\| \cdot \frac{\|x\|}{\|x\|}  \,\,\,\,\text{since $\|x\| \ne 0$} \\ \implies & 1 & \le & \|A^{-1}\|\cdot\|A-B\| \\ \implies & \frac{ 1}{\|A^{-1}\|} & \le & \|A-B\| \\ \therefore & B \;\text{singular} \implies && \|A-B\| \ge \frac{1}{A^{-1}} \\ \text{By contrapositive argument,}&\text{ we have} \\ &\|A-B\|&  \lt &\frac{1}{\|A^{-1}\|} \implies \text{$B$ is invertible}\\ \end{align}  $$",,['matrices']
49,Is there a general form for the derivative of a matrix to a power?,Is there a general form for the derivative of a matrix to a power?,,"Let $S:Mat(2,2) \rightarrow Mat(2,2)$ be the squaring map $S(A)=A^2$ then $[DS(A)]B=AB+BA$. I was wondering if there was a general form for this solution ($S(A)=A^n$, then $[DS(A)]B =$...). I have tried using the definition of derivative, where I remove all the linear terms of H, but it is becoming very messy to compute, very fast.","Let $S:Mat(2,2) \rightarrow Mat(2,2)$ be the squaring map $S(A)=A^2$ then $[DS(A)]B=AB+BA$. I was wondering if there was a general form for this solution ($S(A)=A^n$, then $[DS(A)]B =$...). I have tried using the definition of derivative, where I remove all the linear terms of H, but it is becoming very messy to compute, very fast.",,"['matrices', 'multivariable-calculus', 'matrix-equations']"
50,How can I compare two matrices?,How can I compare two matrices?,,"I have a matrice A. It is model probability matrice for some process (Markov chain). Then, I have estimated matrice B. I have to somehow compare these two matrices to tell whether process that gave matrice B in result matches model matrice A. How can I do this? I would like to have some parameter to change a tollerancy for difference. I need at least three different methods so I can compare their results. Those matrices are stochastic matrices. Their size is n x n. I don't know how to put this. I have two series of observations of state. One is a ""model"" and for other I need to decide if it matches the ""model"". From two different series of state observations I estimate ""model"" matrice A and matrice B. Then I need to check if process that resulted in matrix B is the same process that gave model matrice A. Please, see my new question How can I compare two Markov processes?","I have a matrice A. It is model probability matrice for some process (Markov chain). Then, I have estimated matrice B. I have to somehow compare these two matrices to tell whether process that gave matrice B in result matches model matrice A. How can I do this? I would like to have some parameter to change a tollerancy for difference. I need at least three different methods so I can compare their results. Those matrices are stochastic matrices. Their size is n x n. I don't know how to put this. I have two series of observations of state. One is a ""model"" and for other I need to decide if it matches the ""model"". From two different series of state observations I estimate ""model"" matrice A and matrice B. Then I need to check if process that resulted in matrix B is the same process that gave model matrice A. Please, see my new question How can I compare two Markov processes?",,"['matrices', 'markov-chains']"
51,Is $ SU_2(\Bbb Z[1/\sqrt{2}]) $ finite?,Is  finite?, SU_2(\Bbb Z[1/\sqrt{2}]) ,"Let $ G=SU_2(\mathbb{Z}[1/\sqrt{2}]) $ be the group of $ 2 \times 2 $ unitary determinant one matrices with entries whose real and imaginary parts are from $\mathbb{Z}[i,1/\sqrt{2}]$ . Is $ G $ finite? For $ SU_2(\mathbb{Z}[1/\sqrt{p}]) $ for $ p=3,5 $ etc I have already found some matrices of infinite order. But for $ p=2 $ I am having difficulty finding any matrices of infinite order or any other indication that $ G $ is infinite.",Let be the group of unitary determinant one matrices with entries whose real and imaginary parts are from . Is finite? For for etc I have already found some matrices of infinite order. But for I am having difficulty finding any matrices of infinite order or any other indication that is infinite.," G=SU_2(\mathbb{Z}[1/\sqrt{2}])   2 \times 2  \mathbb{Z}[i,1/\sqrt{2}]  G   SU_2(\mathbb{Z}[1/\sqrt{p}])   p=3,5   p=2   G ","['matrices', 'group-theory', 'finite-groups', 'algebraic-groups']"
52,"If $A=[ij(i+j)]_{n \times n}$, why $\det(A)=0$, when $n>2$","If , why , when",A=[ij(i+j)]_{n \times n} \det(A)=0 n>2,"My numerical experiments suggest that if $A=[ij(i+j)]_{n \times n}$ , then $\det(A)=0$ , $i,j \in[1,n] $ and $n>2.$ What could be an analytic proof for this observation? Can there be a general result? EDIT: Further, if $A=[(i+j)^k]_{n\times n}$ , then $\det(A)=0$ for $n\ge 2+k$ . The idea of degree of polynomial $k$ seems to work here according to the rank connection: $ rank(A+B+C+...) \le[ rank(A)+ rank(B)+rank(C)+...]$ or the matrices as pointed out by @orangeskid in his answer below. This is why for $A=[i^k+j^k]_{n\times n}, k=1,2,3,4,5,...$ we have $\det(A)=0$ for $n\ge 3$ (independent of $k$ ).","My numerical experiments suggest that if , then , and What could be an analytic proof for this observation? Can there be a general result? EDIT: Further, if , then for . The idea of degree of polynomial seems to work here according to the rank connection: or the matrices as pointed out by @orangeskid in his answer below. This is why for we have for (independent of ).","A=[ij(i+j)]_{n \times n} \det(A)=0 i,j \in[1,n]  n>2. A=[(i+j)^k]_{n\times n} \det(A)=0 n\ge 2+k k  rank(A+B+C+...) \le[ rank(A)+ rank(B)+rank(C)+...] A=[i^k+j^k]_{n\times n}, k=1,2,3,4,5,... \det(A)=0 n\ge 3 k","['matrices', 'determinant']"
53,Show that $A$ is diagonalisable when $P(A)$ is diagonalisable and $P'(A)$ is invertible,Show that  is diagonalisable when  is diagonalisable and  is invertible,A P(A) P'(A),"Let $n \geq 1$ and $A \in M_n(\mathbb{C})$ . Assume there exists $P \in \mathbb{C}[X]$ such that: $P(A)$ is diagonalisable $P'(A)$ is invertible I have to show that $A$ is diagonalisable. My try: I solved the problem when $\textrm{deg}(P) \leq 1$ . I also know that $A$ is trigonalisable, then I deduced that: $\forall \lambda \in \textrm{Sp}(A), P'(\lambda) \neq 0$ . Also, there exists a diagonal matrix $D$ and $S \in GL_n(\mathbb{C})$ such that $P(A) = SDS^{-1}$ . But I can't say anything more. Any help is welcome.","Let and . Assume there exists such that: is diagonalisable is invertible I have to show that is diagonalisable. My try: I solved the problem when . I also know that is trigonalisable, then I deduced that: . Also, there exists a diagonal matrix and such that . But I can't say anything more. Any help is welcome.","n \geq 1 A \in M_n(\mathbb{C}) P \in \mathbb{C}[X] P(A) P'(A) A \textrm{deg}(P) \leq 1 A \forall \lambda \in \textrm{Sp}(A), P'(\lambda) \neq 0 D S \in GL_n(\mathbb{C}) P(A) = SDS^{-1}","['matrices', 'polynomials', 'inverse', 'diagonalization']"
54,What is the solution to linear ODE $\dot x = Ax + b$?,What is the solution to linear ODE ?,\dot x = Ax + b,"Suppose I have a system of linear ODE $$\dot x = Ax + b$$ where $A \in \mathbb{R}^{n \times n}, b \in \mathbb{R}^n$ I know that when $b = 0$ , using Laplace transform we have $x(s) = (sI-A)^{-1} x(0)$ , which upon inverting (pg.13) we obtain $x(t) = e^{At}x(0)$ However, when $b$ is non-zero, the Laplace transform gives $$x(s)  = (sI-A)^{-1}x(0) + (sI-A)^{-1}\dfrac{b}{s}$$ I have no idea how to invert $(sI-A)^{-1}\dfrac{b}{s}$ Does anyone know how to solve for this? Some thoughts, $(sI-A)^{-1}\dfrac{b}{s} = (sI-A)^{-1}(sI)^{-1}b = ((sI)(sI-A))^{-1}b = (s^2I - sA)^{-1}b$","Suppose I have a system of linear ODE where I know that when , using Laplace transform we have , which upon inverting (pg.13) we obtain However, when is non-zero, the Laplace transform gives I have no idea how to invert Does anyone know how to solve for this? Some thoughts,","\dot x = Ax + b A \in \mathbb{R}^{n \times n}, b \in \mathbb{R}^n b = 0 x(s) = (sI-A)^{-1} x(0) x(t) = e^{At}x(0) b x(s)  = (sI-A)^{-1}x(0) + (sI-A)^{-1}\dfrac{b}{s} (sI-A)^{-1}\dfrac{b}{s} (sI-A)^{-1}\dfrac{b}{s} = (sI-A)^{-1}(sI)^{-1}b = ((sI)(sI-A))^{-1}b = (s^2I - sA)^{-1}b","['matrices', 'ordinary-differential-equations', 'laplace-transform', 'control-theory']"
55,When is the sum and difference of two projection matrices $P_1$ and $P_2$ a projection matrix?,When is the sum and difference of two projection matrices  and  a projection matrix?,P_1 P_2,"Let $P_1$ and $P_2$ be two projection matrices for orthogonal projections onto $S_1 \in \mathcal{R}^m$ and $S_2 \in \mathcal{R}^m$ , respectively. When does $P_1+P_2$ and $P_1-P_2$ result in a projection matrix? Prove it. I am confident that $P_1+P_2$ is a projection matrix iff $P_1P_2=P_2P_1=(0).$ Similarly, I feel like $P_1-P_2$ is a projection matrix iff $P_1P_2=P_2P_1=P_2.$ However, I do not know how to formally prove either of the above statements. How should I formalize the proof?","Let and be two projection matrices for orthogonal projections onto and , respectively. When does and result in a projection matrix? Prove it. I am confident that is a projection matrix iff Similarly, I feel like is a projection matrix iff However, I do not know how to formally prove either of the above statements. How should I formalize the proof?",P_1 P_2 S_1 \in \mathcal{R}^m S_2 \in \mathcal{R}^m P_1+P_2 P_1-P_2 P_1+P_2 P_1P_2=P_2P_1=(0). P_1-P_2 P_1P_2=P_2P_1=P_2.,"['matrices', 'projection-matrices']"
56,"What is exactly the relation between vectors, matrices, and tensors?","What is exactly the relation between vectors, matrices, and tensors?",,"I am trying to understand what Tensors are (I have no physics or pure math background, and am starting with machine learning). In an introduction to Tensors it is said that tensors are a generalization of scalars, vectors and matrices: Scalars are 0-order tensors, vectors are 1-order tensors, and matrices are 2-order tensors. n-order tensors are simply an n-dimensional array of numbers. However, this does not say anything about the algebraic or geometric properties of tensors, and it seems from reading around the internet that tensors are algebraically defined, rather than defined as an array of numbers. Similarly, vectors are defined as elements of a set that satisfy the vector space axioms. I understand this definition of a vector space. And I understand that matrices are representations of linear transformations of vectors. Question: I am trying to understand what a Tensor is more intuitively, and what the algebraic and intuitive/geometric relation is between tensors on the one hand, and vectors/matrices on the other. (taking into account that matrices are representations of linear transformations of vectors)","I am trying to understand what Tensors are (I have no physics or pure math background, and am starting with machine learning). In an introduction to Tensors it is said that tensors are a generalization of scalars, vectors and matrices: Scalars are 0-order tensors, vectors are 1-order tensors, and matrices are 2-order tensors. n-order tensors are simply an n-dimensional array of numbers. However, this does not say anything about the algebraic or geometric properties of tensors, and it seems from reading around the internet that tensors are algebraically defined, rather than defined as an array of numbers. Similarly, vectors are defined as elements of a set that satisfy the vector space axioms. I understand this definition of a vector space. And I understand that matrices are representations of linear transformations of vectors. Question: I am trying to understand what a Tensor is more intuitively, and what the algebraic and intuitive/geometric relation is between tensors on the one hand, and vectors/matrices on the other. (taking into account that matrices are representations of linear transformations of vectors)",,"['matrices', 'vectors', 'tensors']"
57,Show that 1 + $\lambda$ is an eigenvalue of $I + A$,Show that 1 +  is an eigenvalue of,\lambda I + A,"Show that if $\lambda$ is an eigenvalue of $A$, then 1+$\lambda$ is an eigenvalue of $I+A$. What is the corresponding eigenvector? What I have done so far (if it is correct at all...): $(I+A)x=Ix+Ax=1x+Ax=1x+{\lambda}x=(1+\lambda)x$, thus, $1+{\lambda}$ is an eigenvector of $I+A$ But how do I find the corresponding eigenvector just from the information above?","Show that if $\lambda$ is an eigenvalue of $A$, then 1+$\lambda$ is an eigenvalue of $I+A$. What is the corresponding eigenvector? What I have done so far (if it is correct at all...): $(I+A)x=Ix+Ax=1x+Ax=1x+{\lambda}x=(1+\lambda)x$, thus, $1+{\lambda}$ is an eigenvector of $I+A$ But how do I find the corresponding eigenvector just from the information above?",,"['matrices', 'eigenvalues-eigenvectors']"
58,Loewner ordering of symetric positive definite matrices and their inverse,Loewner ordering of symetric positive definite matrices and their inverse,,"Let $M_1$ and $M_2$ be symmetric positive definite matrices and $M_2 > M_1$ in the Loewner ordering , i.e., $M_2 - M_1$ is positive definite. Does this imply that $M_1^{-1} > M_2^{-1}$ ?","Let and be symmetric positive definite matrices and in the Loewner ordering , i.e., is positive definite. Does this imply that ?",M_1 M_2 M_2 > M_1 M_2 - M_1 M_1^{-1} > M_2^{-1},"['matrices', 'order-theory', 'symmetric-matrices', 'positive-definite']"
59,Is it possible to compute factorials by converting to matrix multiplications?,Is it possible to compute factorials by converting to matrix multiplications?,,"An $n$-th term of the Fibonacci sequence can be computed by a nice trick by converting the recurrence relation in a matrix form . Then we compute $M^n$ in $O(\log n)$ steps using exponentiation by squaring . Would it be possible to use such a trick to compute factorials? If not, can it be proved? I figured out how to compute any polynomial in $n$ using this approach, but for factorials I wasn't able to express the factorial's recurrence relation as a linear transformation.","An $n$-th term of the Fibonacci sequence can be computed by a nice trick by converting the recurrence relation in a matrix form . Then we compute $M^n$ in $O(\log n)$ steps using exponentiation by squaring . Would it be possible to use such a trick to compute factorials? If not, can it be proved? I figured out how to compute any polynomial in $n$ using this approach, but for factorials I wasn't able to express the factorial's recurrence relation as a linear transformation.",,"['matrices', 'recurrence-relations', 'factorial']"
60,Casorati matrix,Casorati matrix,,"Does anyone know what the Casorati matrix is? I read that the Casorati matrix is useful in the study of linear difference equations. However, I couldn't find what the Casorati matrix is. Any help?","Does anyone know what the Casorati matrix is? I read that the Casorati matrix is useful in the study of linear difference equations. However, I couldn't find what the Casorati matrix is. Any help?",,"['matrices', 'discrete-mathematics', 'recurrence-relations']"
61,Power method for finding all eigenvectors,Power method for finding all eigenvectors,,"This is my homework. I was asked to find all eigenvectors of a symmetric and positive definite matrix by inverse power method with shifted. I encountered three problems: The eigenvalues to the matrix may not be distinct. In this case, how to find all eigenvectors corresponding to one eigenvalue? By the inverse power method, I can find the smallest eigenvalue and eigenvector. However, it seems the inverse power method with deflation does not work for finding other eigenvalues. I think the reason is the deflation shift the largest eigenvalue to zero, and after I inverse the matrix, this eigenvalue goes to infinity and therefore it is impossible to find an infinite large eigenvalue. How to modified power method with deflation in the inverse case? How to use power method with deflation to find all eigenvector? I can find the largest (power method) and second largest (power method with deflation). My question is how to construct the ""modifed matrix"", i.e. $B = A$ minus some modifying term? $${}$$","This is my homework. I was asked to find all eigenvectors of a symmetric and positive definite matrix by inverse power method with shifted. I encountered three problems: The eigenvalues to the matrix may not be distinct. In this case, how to find all eigenvectors corresponding to one eigenvalue? By the inverse power method, I can find the smallest eigenvalue and eigenvector. However, it seems the inverse power method with deflation does not work for finding other eigenvalues. I think the reason is the deflation shift the largest eigenvalue to zero, and after I inverse the matrix, this eigenvalue goes to infinity and therefore it is impossible to find an infinite large eigenvalue. How to modified power method with deflation in the inverse case? How to use power method with deflation to find all eigenvector? I can find the largest (power method) and second largest (power method with deflation). My question is how to construct the ""modifed matrix"", i.e. $B = A$ minus some modifying term? $${}$$",,"['matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'positive-definite', 'symmetric-matrices']"
62,A property of positive definite matrices,A property of positive definite matrices,,Assume we have 2 positive definite matrices A and B . Show that there exists a non-singular matrix S such that - SAS' = I SBS' = L Here I is the Identity matrix and L is a diagonal matrix. S' is the transpose of S .,Assume we have 2 positive definite matrices A and B . Show that there exists a non-singular matrix S such that - SAS' = I SBS' = L Here I is the Identity matrix and L is a diagonal matrix. S' is the transpose of S .,,['matrices']
63,Finding all n×n permutation matrices,Finding all n×n permutation matrices,,"If I have a doubly stochastic matrix, how can I find the set of all basic feasible solutions? Here's Wikipedia on doubly stochastic matrices .","If I have a doubly stochastic matrix, how can I find the set of all basic feasible solutions? Here's Wikipedia on doubly stochastic matrices .",,"['algorithms', 'matrices', 'linear-programming', 'permutations']"
64,Derivative of the inverse of a symmetric matrix w.r.t itself,Derivative of the inverse of a symmetric matrix w.r.t itself,,"I'm trying take the derivative of a symmetrix matrix $\mathbf{C}$ with respect to itself. $$ \begin{equation} \frac{\partial \mathbf{C}^{-1}}{\partial \mathbf{C}} \end{equation} $$ Using the indicial notation, above equation can be rewritten as follows $$ \begin{equation} \frac{\partial C_{ij}^{-1}}{\partial C_{kl}} \end{equation} $$ At first I've used the following formula, $$ \begin{equation} \frac{\partial C_{ij}^{-1}}{\partial C_{kl}} = -C^{-1}_{ik}C^{-1}_{lj} \end{equation} $$ But I quickly realized that we've lost the symmetry of the problem now. I read The Matrix Cookbook and the other posts about the same problem but unfortunately, I couldn't understand the things they've done. For example in this article , at Eq.(100) authors have used the property below when taking the derivative of Eq.(99) $$ \begin{equation} \frac{\partial \mathbf{C}^{-1}}{\partial \mathbf{C}} = -\mathbf{C}^{-1} \boxtimes \mathbf{C}^{-T} \mathbf{I}_s \end{equation} $$ Where $\boxtimes$ is the square product, $\mathbf{I}_s$ is the symmetric fourth-order identity tensor and they are defined as follows $$ \begin{align} (\mathbf{A} \boxtimes \mathbf{B})_{ijkl} &= \mathbf{A}_{ik}\mathbf{B}_{jl} \\ (\mathbf{I}_s)_{ijkl} &= \frac{1}{2}(\delta_{ik}\delta_{jl}+\delta_{il}\delta_{jk}) \end{align} $$ I couldn't understand how did they achieve this result and how can I derive it myself.","I'm trying take the derivative of a symmetrix matrix with respect to itself. Using the indicial notation, above equation can be rewritten as follows At first I've used the following formula, But I quickly realized that we've lost the symmetry of the problem now. I read The Matrix Cookbook and the other posts about the same problem but unfortunately, I couldn't understand the things they've done. For example in this article , at Eq.(100) authors have used the property below when taking the derivative of Eq.(99) Where is the square product, is the symmetric fourth-order identity tensor and they are defined as follows I couldn't understand how did they achieve this result and how can I derive it myself.","\mathbf{C} 
\begin{equation}
\frac{\partial \mathbf{C}^{-1}}{\partial \mathbf{C}}
\end{equation}
 
\begin{equation}
\frac{\partial C_{ij}^{-1}}{\partial C_{kl}}
\end{equation}
 
\begin{equation}
\frac{\partial C_{ij}^{-1}}{\partial C_{kl}} = -C^{-1}_{ik}C^{-1}_{lj}
\end{equation}
 
\begin{equation}
\frac{\partial \mathbf{C}^{-1}}{\partial \mathbf{C}} = -\mathbf{C}^{-1} \boxtimes \mathbf{C}^{-T} \mathbf{I}_s
\end{equation}
 \boxtimes \mathbf{I}_s 
\begin{align}
(\mathbf{A} \boxtimes \mathbf{B})_{ijkl} &= \mathbf{A}_{ik}\mathbf{B}_{jl} \\
(\mathbf{I}_s)_{ijkl} &= \frac{1}{2}(\delta_{ik}\delta_{jl}+\delta_{il}\delta_{jk})
\end{align}
","['matrices', 'derivatives', 'matrix-calculus', 'tensors']"
65,"Cute problem: determinant of $I_n+(f_if_j)_{i,j}$ [duplicate]",Cute problem: determinant of  [duplicate],"I_n+(f_if_j)_{i,j}","This question already has an answer here : A determinant coming out from the computation of a volume form (1 answer) Closed 3 years ago . I thought of the following little problem. Given numbers $f_1,\dots f_n$ , what is the determinant of the symmetric matrix $I_n+(f_if_j)_{i,j}$ ? I have found a cute combinatorial-style proof that it is $1+\Sigma_i f_i^2$ . using the sum over permutations formula for the determinant. Here $F(\sigma)$ denotes the set of fixed points of $\sigma$ . Does anyone have a faster/more elegant method?","This question already has an answer here : A determinant coming out from the computation of a volume form (1 answer) Closed 3 years ago . I thought of the following little problem. Given numbers , what is the determinant of the symmetric matrix ? I have found a cute combinatorial-style proof that it is . using the sum over permutations formula for the determinant. Here denotes the set of fixed points of . Does anyone have a faster/more elegant method?","f_1,\dots f_n I_n+(f_if_j)_{i,j} 1+\Sigma_i f_i^2 F(\sigma) \sigma",['matrices']
66,Matrices and power series,Matrices and power series,,"For any square matrix $A$ , we can define $\sin A$ using the formal power series as follows: \begin{equation} \sin A=\sum_{n=0}^{\infty}\frac{(-1)^n}{(2n+1)!}A^{2n+1} \end{equation} Prove or disprove: there exists a $2\times2$ real matrix $A$ such that \begin{equation} \sin A=\begin{bmatrix}1 & 2020 \\0& 1\end{bmatrix} \end{equation}","For any square matrix , we can define using the formal power series as follows: Prove or disprove: there exists a real matrix such that","A \sin A \begin{equation}
\sin A=\sum_{n=0}^{\infty}\frac{(-1)^n}{(2n+1)!}A^{2n+1}
\end{equation} 2\times2 A \begin{equation}
\sin A=\begin{bmatrix}1 & 2020 \\0& 1\end{bmatrix}
\end{equation}","['matrices', 'trigonometry', 'power-series']"
67,Derivative of matrix exponential of linear combination,Derivative of matrix exponential of linear combination,,"Let's say $t$ is a real parameter and $\textbf{A}$ is $n \times n$ matrix. I know that $$\frac{d}{dt} \exp(t\textbf{A}) = \textbf{A} \exp(t\textbf{A})$$ But what if there are multiple parameters $t_1, ..., t_n$ and multiple matrices $\textbf{A}_1, \dots , \textbf{A}_n$ that don't commute , does the same relation, namely $$\frac{\partial}{\partial t_k}\exp \left(\sum_{i=1}^n t_i \textbf{A}_i \right) = \textbf{A}_k \exp \left(\sum_{i=1}^n t_i \textbf{A}_i\right)$$ still hold? If not, is there a closed form for the derivative?","Let's say is a real parameter and is matrix. I know that But what if there are multiple parameters and multiple matrices that don't commute , does the same relation, namely still hold? If not, is there a closed form for the derivative?","t \textbf{A} n \times n \frac{d}{dt} \exp(t\textbf{A}) = \textbf{A} \exp(t\textbf{A}) t_1, ..., t_n \textbf{A}_1, \dots , \textbf{A}_n \frac{\partial}{\partial t_k}\exp \left(\sum_{i=1}^n t_i \textbf{A}_i \right) = \textbf{A}_k \exp \left(\sum_{i=1}^n t_i \textbf{A}_i\right)","['matrices', 'derivatives', 'matrix-calculus', 'matrix-exponential']"
68,"What does this alphabet,$\mathbb C$, mean?","What does this alphabet,, mean?",\mathbb C,"I see this representation in the paper,but i don't know what does this alphabet mean,neither the paper said. The sentence about this says: The energy-carrying information signal is denoted as $\mathbf s_B \in \mathbb C^{N_B} $with covariance matrix $\mathbf W_B = E[\mathbf s_B\mathbf s_B^H ]\in \mathbb C^{N_B \times N_B}$. Does anyone know that what do $\mathbb C^{N_B \times N_B}$ and $\mathbb C^{N_B} $ mean?","I see this representation in the paper,but i don't know what does this alphabet mean,neither the paper said. The sentence about this says: The energy-carrying information signal is denoted as $\mathbf s_B \in \mathbb C^{N_B} $with covariance matrix $\mathbf W_B = E[\mathbf s_B\mathbf s_B^H ]\in \mathbb C^{N_B \times N_B}$. Does anyone know that what do $\mathbb C^{N_B \times N_B}$ and $\mathbb C^{N_B} $ mean?",,"['matrices', 'notation']"
69,Is $A=0$ the only solution to $\exp(A)=I$?,Is  the only solution to ?,A=0 \exp(A)=I,"So obviously one solution to $\exp(A)=I$ is $A=0$, however is it the only solution? And also If $\exp(A)$ is diagonalizable does this mean $A$ is diagonalizable?","So obviously one solution to $\exp(A)=I$ is $A=0$, however is it the only solution? And also If $\exp(A)$ is diagonalizable does this mean $A$ is diagonalizable?",,"['matrices', 'matrix-equations', 'matrix-calculus', 'matrix-exponential']"
70,"If $a_{ij}=\max(i,j)$, calculate the determinant of $A$","If , calculate the determinant of","a_{ij}=\max(i,j) A","If $A$ is an $n \times n$ real matrix and $$a_{ij}=\max(i,j)$$ for $i,j = 1,2,\dots,n$, calculate the determinant of $A$. So, we know that $$A=\begin{pmatrix} 1 & 2 & 3 & \dots & n\\  2 & 2 & 3 & \dots & n\\  3 & 3 & 3 & \dots & n\\  \vdots & \vdots & \vdots & \ddots  & \vdots\\   n& n & n & \dots & n \end{pmatrix}$$ but what do I do after?","If $A$ is an $n \times n$ real matrix and $$a_{ij}=\max(i,j)$$ for $i,j = 1,2,\dots,n$, calculate the determinant of $A$. So, we know that $$A=\begin{pmatrix} 1 & 2 & 3 & \dots & n\\  2 & 2 & 3 & \dots & n\\  3 & 3 & 3 & \dots & n\\  \vdots & \vdots & \vdots & \ddots  & \vdots\\   n& n & n & \dots & n \end{pmatrix}$$ but what do I do after?",,"['matrices', 'determinant']"
71,Is the matrix norm of a matrix equal to the maximum of the norms of its Jordan block?,Is the matrix norm of a matrix equal to the maximum of the norms of its Jordan block?,,"Let $J$ be a Jordan block matrix with blocks $J_1,\cdots,J_n$. I came up with some examples of $J$ and noticed that $\|J\|=\max_{i=1,\cdots,n}\|J_i\|$. Does this result always hold? The norm I use here is the induced 2-norm . (I used Matlab for my examples. The norm would be the one that Matlab uses when I type norm(A).)","Let $J$ be a Jordan block matrix with blocks $J_1,\cdots,J_n$. I came up with some examples of $J$ and noticed that $\|J\|=\max_{i=1,\cdots,n}\|J_i\|$. Does this result always hold? The norm I use here is the induced 2-norm . (I used Matlab for my examples. The norm would be the one that Matlab uses when I type norm(A).)",,"['matrices', 'normed-spaces', 'jordan-normal-form']"
72,Paradox on the derivative of the rank of a matrix,Paradox on the derivative of the rank of a matrix,,"It is clear that the function $$f : \mathbb R^{m \times n} \to \mathbb N, \qquad X \mapsto \mbox{rank}(X)$$ has no derivative at all $X$ because the image of $f(X)$ assume values in the natural set. On the other hand, we know that the rank of any matrix can be computed by $$\mbox{rank}(X) = \mbox{tr} \left( X^+ X \right) \tag{1}$$ where $\text{tr}$ is the trace operator and $X^+$ is the Moore-Penrose pseudoinverse of $X$ . Notice, however, that the RHS of $(1)$ is differentiable everywhere (the trace operator and the pseudo inverse have derivatives) and it can be computed as $$f'(X)  = (X^T\otimes I_n)\left(-(X^+)^T \otimes X^+ \right) + (I_n \otimes X^+).$$ I am confusing if the rank has or not derivative at all points $X$ . Probably I have made some mistake and I am missing something. I need some help. Thanks in advance! Similar behavior  happens with the nuclear norm (see Derivative of nuclear norm ).","It is clear that the function has no derivative at all because the image of assume values in the natural set. On the other hand, we know that the rank of any matrix can be computed by where is the trace operator and is the Moore-Penrose pseudoinverse of . Notice, however, that the RHS of is differentiable everywhere (the trace operator and the pseudo inverse have derivatives) and it can be computed as I am confusing if the rank has or not derivative at all points . Probably I have made some mistake and I am missing something. I need some help. Thanks in advance! Similar behavior  happens with the nuclear norm (see Derivative of nuclear norm ).","f : \mathbb R^{m \times n} \to \mathbb N, \qquad X \mapsto \mbox{rank}(X) X f(X) \mbox{rank}(X) = \mbox{tr} \left( X^+ X \right) \tag{1} \text{tr} X^+ X (1) f'(X)  = (X^T\otimes I_n)\left(-(X^+)^T \otimes X^+ \right) + (I_n \otimes X^+). X","['matrices', 'derivatives', 'matrix-calculus', 'matrix-rank', 'pseudoinverse']"
73,Properties of the matrix square root,Properties of the matrix square root,,"In a paper I am reading, it is claimed that if $A, B \in \mathbb{R}^{n \times n}$ are positive definite, then $$ A^{1/2} (A^{−1/2} B A^{−1/2})^{1/2} A^{1/2} = A (A^{-1}B)^{1/2} $$ because of the properties of the principal matrix square root, but I am not sure how this equality can be proved. Has anybody got a hint?","In a paper I am reading, it is claimed that if $A, B \in \mathbb{R}^{n \times n}$ are positive definite, then $$ A^{1/2} (A^{−1/2} B A^{−1/2})^{1/2} A^{1/2} = A (A^{-1}B)^{1/2} $$ because of the properties of the principal matrix square root, but I am not sure how this equality can be proved. Has anybody got a hint?",,"['matrices', 'matrix-equations']"
74,"List of connected Lie subgroups of $\mathrm{SL}(2,\mathbb{C})$.",List of connected Lie subgroups of .,"\mathrm{SL}(2,\mathbb{C})","I am not familiar with the theory of Lie groups, so I am having a hard time finding all of the connected closed real Lie subgroups of $\mathrm{SL}(2, \mathbb{C})$ , up to conjugation. One can find the real and complex parabolic, elliptic, hyperbolic, subgroups, $\mathrm{SU}(2)$ , $\mathrm{SU}(1,1)$ and $\mathrm{SL}(2,\mathbb{R})$ (the last two ones are isomorphic however), the subgroup of real upper triangular matrices, the subgroup of upper triangular matrices with unitary diagonal coefficients, and the subgroup of complex triangular matrices. Are there any others?","I am not familiar with the theory of Lie groups, so I am having a hard time finding all of the connected closed real Lie subgroups of , up to conjugation. One can find the real and complex parabolic, elliptic, hyperbolic, subgroups, , and (the last two ones are isomorphic however), the subgroup of real upper triangular matrices, the subgroup of upper triangular matrices with unitary diagonal coefficients, and the subgroup of complex triangular matrices. Are there any others?","\mathrm{SL}(2, \mathbb{C}) \mathrm{SU}(2) \mathrm{SU}(1,1) \mathrm{SL}(2,\mathbb{R})","['matrices', 'lie-groups', 'hyperbolic-geometry']"
75,Fast multiplication of orthogonal matrices,Fast multiplication of orthogonal matrices,,"Given $A,B\in SO(3)$, direct matrix multiplication computes $C=AB$ with 27 multiplies. The group $SO(3)$ is a $3$-dimensional manifold. This suggests that direct matrix multiplication, which thinks of elements of $SO(3)$ as 9-dimensional, is not optimal. What is the minimal number of multiplies necessary to compute $C$?","Given $A,B\in SO(3)$, direct matrix multiplication computes $C=AB$ with 27 multiplies. The group $SO(3)$ is a $3$-dimensional manifold. This suggests that direct matrix multiplication, which thinks of elements of $SO(3)$ as 9-dimensional, is not optimal. What is the minimal number of multiplies necessary to compute $C$?",,['matrices']
76,Calculate determinant of $n$-th order,Calculate determinant of -th order,n,I have problem with creating a upper triangular matrix (in order to calculate the determinant) of the following matrix: $$\begin{pmatrix}  1&  2&  3&  ...& n-2 &n-1  &1 \\   1&  2&  3&  ...&  n-2&  1&n \\   1&  2&  3&  ...&  1&n-1  &n \\   .&  .&  .&  .&  .&  .& \\  .&  .& .& .& .& .& \\  1&  2&  1&  ...& n-2 &n-1  &n \\   1&  1&  3&  ...&  n-2& n-1 &n \\   1&  2&  3&  ...&  n-2& n-1 &n  \end{pmatrix}$$ One of my attempts included subtracting the first row from the others which got me to this point: $$\begin{pmatrix}  1&  2&  3&  ...& n-2 &n-1  &1 \\   0&  0&  0&  ...&  0&  2-n&n-1 \\   0&  0&  0&  ...&  3-n&0 &n-1 \\   .&  .&  .&  .&  .&  .& \\  .&  .& .& .& .& .& \\  0&  0&  -2&  ...& 0 &0 &n-1 \\   0&  -1&  0&  ...&  0&0 &n-1 \\   0&  0&  0&  ...&  0&0 &n-1  \end{pmatrix}$$ Any ideas on how to move on from here ?,I have problem with creating a upper triangular matrix (in order to calculate the determinant) of the following matrix: One of my attempts included subtracting the first row from the others which got me to this point: Any ideas on how to move on from here ?,"\begin{pmatrix}
 1&  2&  3&  ...& n-2 &n-1  &1 \\ 
 1&  2&  3&  ...&  n-2&  1&n \\ 
 1&  2&  3&  ...&  1&n-1  &n \\ 
 .&  .&  .&  .&  .&  .& \\
 .&  .& .& .& .& .& \\
 1&  2&  1&  ...& n-2 &n-1  &n \\ 
 1&  1&  3&  ...&  n-2& n-1 &n \\ 
 1&  2&  3&  ...&  n-2& n-1 &n 
\end{pmatrix} \begin{pmatrix}
 1&  2&  3&  ...& n-2 &n-1  &1 \\ 
 0&  0&  0&  ...&  0&  2-n&n-1 \\ 
 0&  0&  0&  ...&  3-n&0 &n-1 \\ 
 .&  .&  .&  .&  .&  .& \\
 .&  .& .& .& .& .& \\
 0&  0&  -2&  ...& 0 &0 &n-1 \\ 
 0&  -1&  0&  ...&  0&0 &n-1 \\ 
 0&  0&  0&  ...&  0&0 &n-1 
\end{pmatrix}","['matrices', 'determinant']"
77,Who first invented/introduced the concept of the trace of a Matrix and Why?,Who first invented/introduced the concept of the trace of a Matrix and Why?,,"Could anyone give any information  about the invention of the concept of the trace of a Matrix, as this concept  is so important and useful in linear algebra. I searched on the internet, but found nothing on its origin.","Could anyone give any information  about the invention of the concept of the trace of a Matrix, as this concept  is so important and useful in linear algebra. I searched on the internet, but found nothing on its origin.",,"['matrices', 'math-history', 'trace']"
78,What is the best algorithm to find the inverse of matrix $A$,What is the best algorithm to find the inverse of matrix,A,"I'm going to build a C-library so I can do linear algebra at embedded systems. It's most for machine learning. https://github.com/DanielMartensson/EmbeddedAlgebra/ Anyway, I need to compute inverse of matrix $A$ . What is the absolute best algorithm to use for that? Remember it must be a numerical algorithm. Which one should I use? Analytic solution $\mathbf{A}^{-1}={1 \over \begin{vmatrix}\mathbf{A}\end{vmatrix}}\mathbf{C}^{\mathrm{T}}={1 \over \begin{vmatrix}\mathbf{A}\end{vmatrix}} \begin{pmatrix} \mathbf{C}_{11} & \mathbf{C}_{21} & \cdots & \mathbf{C}_{n1} \\ \mathbf{C}_{12} & \mathbf{C}_{22} & \cdots & \mathbf{C}_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ \mathbf{C}_{1n} & \mathbf{C}_{2n} & \cdots & \mathbf{C}_{nn} \\ \end{pmatrix}$ Cholesky decomposition $\mathbf{A}^{-1} = (\mathbf{L}^{*})^{-1} \mathbf{L}^{-1}$ Eigendecomposition $\mathbf{A}^{-1}=\mathbf{Q}\mathbf{\Lambda}^{-1}\mathbf{Q}^{-1}$ Cayley–Hamilton method $\mathbf{A}^{-1} = \frac{1}{\det (\mathbf{A})}\sum_{s=0}^{n-1}A^s$ Newton's method $X_{k+1} = 2X_k - X_k A X_k$ Neuman series $\lim_{n \to \infty} (\mathbf I - \mathbf A)^n = 0$ Gaussian elimination $A = \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}$ To find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 × 6 matrix: $  [ A | I ] =  \left[ \begin{array}{rrr|rrr} 2 & -1 & 0 & 1 & 0 & 0\\ -1 & 2 & -1 & 0 & 1 & 0\\ 0 & -1 & 2 & 0 & 0 & 1 \end{array} \right]$ By performing row operations, one can check that the reduced row echelon form of this augmented matrix is: $[ I | B ] =  \left[ \begin{array}{rrr|rrr} 1 & 0 & 0 & \frac34 & \frac12 & \frac14\\[3pt] 0 & 1 & 0 & \frac12 & 1 & \frac12\\[3pt] 0 & 0 & 1 & \frac14 & \frac12 & \frac34 \end{array} \right]$","I'm going to build a C-library so I can do linear algebra at embedded systems. It's most for machine learning. https://github.com/DanielMartensson/EmbeddedAlgebra/ Anyway, I need to compute inverse of matrix . What is the absolute best algorithm to use for that? Remember it must be a numerical algorithm. Which one should I use? Analytic solution Cholesky decomposition Eigendecomposition Cayley–Hamilton method Newton's method Neuman series Gaussian elimination To find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 × 6 matrix: By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:","A \mathbf{A}^{-1}={1 \over \begin{vmatrix}\mathbf{A}\end{vmatrix}}\mathbf{C}^{\mathrm{T}}={1 \over \begin{vmatrix}\mathbf{A}\end{vmatrix}}
\begin{pmatrix}
\mathbf{C}_{11} & \mathbf{C}_{21} & \cdots & \mathbf{C}_{n1} \\
\mathbf{C}_{12} & \mathbf{C}_{22} & \cdots & \mathbf{C}_{n2} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{C}_{1n} & \mathbf{C}_{2n} & \cdots & \mathbf{C}_{nn} \\
\end{pmatrix} \mathbf{A}^{-1} = (\mathbf{L}^{*})^{-1} \mathbf{L}^{-1} \mathbf{A}^{-1}=\mathbf{Q}\mathbf{\Lambda}^{-1}\mathbf{Q}^{-1} \mathbf{A}^{-1} = \frac{1}{\det (\mathbf{A})}\sum_{s=0}^{n-1}A^s X_{k+1} = 2X_k - X_k A X_k \lim_{n \to \infty} (\mathbf I - \mathbf A)^n = 0 A =
\begin{bmatrix}
2 & -1 & 0 \\
-1 & 2 & -1 \\
0 & -1 & 2
\end{bmatrix}  
[ A | I ] = 
\left[ \begin{array}{rrr|rrr}
2 & -1 & 0 & 1 & 0 & 0\\
-1 & 2 & -1 & 0 & 1 & 0\\
0 & -1 & 2 & 0 & 0 & 1
\end{array} \right] [ I | B ] = 
\left[ \begin{array}{rrr|rrr}
1 & 0 & 0 & \frac34 & \frac12 & \frac14\\[3pt]
0 & 1 & 0 & \frac12 & 1 & \frac12\\[3pt]
0 & 0 & 1 & \frac14 & \frac12 & \frac34
\end{array} \right]","['matrices', 'inverse', 'numerical-linear-algebra']"
79,Sum of a matrix with its transpose [duplicate],Sum of a matrix with its transpose [duplicate],,"This question already has answers here : How to prove $A+ A^T$ symmetric, $A-A^T$ skew-symmetric. [closed] (5 answers) Closed 6 years ago . I've a question that many of yours could consider stupid: if i sum a matrix with its transpose, I obtain a particular result? E.g. $A + A^T = B$, $B$ has some particular properties?","This question already has answers here : How to prove $A+ A^T$ symmetric, $A-A^T$ skew-symmetric. [closed] (5 answers) Closed 6 years ago . I've a question that many of yours could consider stupid: if i sum a matrix with its transpose, I obtain a particular result? E.g. $A + A^T = B$, $B$ has some particular properties?",,['matrices']
80,Matrix notation for element-wise raising to the power of $n$,Matrix notation for element-wise raising to the power of,n,"The Hadamard product $A \odot B$ gives the element-wise multiplication of matricies $A$ and $B$. How do I denote the raising a matrix to the power $n$, element-wise?","The Hadamard product $A \odot B$ gives the element-wise multiplication of matricies $A$ and $B$. How do I denote the raising a matrix to the power $n$, element-wise?",,"['matrices', 'notation', 'hadamard-product']"
81,Inverse of sum of nilpotent matrix and identity matrix,Inverse of sum of nilpotent matrix and identity matrix,,"Suppose $A$ is a $n\times n$ nilpotent matrix of index $m$. i.e. $A^m=0$ but $A^{m-1}\neq0$. Construct $M=A+\lambda I_n$. It is known that $M^{-1}=\sum_{i=1}^{m}\lambda^{-i}(-A)^{i-1}$ I want to prove $M^{-1}= B+\lambda^{-1}I_{n}$, where B is a nilpotent matrix of index $m$, i.e. $B^m=0$ but $B^{m-1}\neq0$. By the way, I want to get the characteristic polynomial and minimal polynomial of $M^{-1}$ It is easy to show $B^m=0$. However, I do not have a clue how to prove $B^{m-1}\neq0$.","Suppose $A$ is a $n\times n$ nilpotent matrix of index $m$. i.e. $A^m=0$ but $A^{m-1}\neq0$. Construct $M=A+\lambda I_n$. It is known that $M^{-1}=\sum_{i=1}^{m}\lambda^{-i}(-A)^{i-1}$ I want to prove $M^{-1}= B+\lambda^{-1}I_{n}$, where B is a nilpotent matrix of index $m$, i.e. $B^m=0$ but $B^{m-1}\neq0$. By the way, I want to get the characteristic polynomial and minimal polynomial of $M^{-1}$ It is easy to show $B^m=0$. However, I do not have a clue how to prove $B^{m-1}\neq0$.",,"['matrices', 'eigenvalues-eigenvectors']"
82,Can we use simultaneous row and column operations in solving same determinant?,Can we use simultaneous row and column operations in solving same determinant?,,Please help.. Can both(row and column) operations be used simultaneously in finding the value of same determinant means in solving same question at a single time?,Please help.. Can both(row and column) operations be used simultaneously in finding the value of same determinant means in solving same question at a single time?,,"['matrices', 'determinant']"
83,"How can I solve ""average"" best rank-$1$ approximation?","How can I solve ""average"" best rank- approximation?",1,"Assume I want to minimise this $$ \min_{x,y} \left\| A - x y^T \right\|_{\text{F}}^2$$ then I am finding best rank- $1$ approximation of $A$ in the squared-error sense and this can be done via the SVD, selecting $x$ and $y$ as left and right singular vectors corresponding to the largest singular value of $A$ . Now instead, is possible to solve the following for $b$ also fixed? $$ \min_{x} \left\| A - x b^T \right\|_{\text{F}}^2$$ If this is possible, is there also a way to solve $$ \min_{x} \left\| A - x b^T \right\|_{\text{F}}^2 + \left\| C - x d^T \right\|_{\text{F}}^2$$ where I think of $x$ as the best ""average"" solution between the two parts of the cost function? I am of course longing for a closed-form solution but a nice iterative optimisation approach to a solution could also be useful.","Assume I want to minimise this then I am finding best rank- approximation of in the squared-error sense and this can be done via the SVD, selecting and as left and right singular vectors corresponding to the largest singular value of . Now instead, is possible to solve the following for also fixed? If this is possible, is there also a way to solve where I think of as the best ""average"" solution between the two parts of the cost function? I am of course longing for a closed-form solution but a nice iterative optimisation approach to a solution could also be useful."," \min_{x,y} \left\| A - x y^T \right\|_{\text{F}}^2 1 A x y A b  \min_{x} \left\| A - x b^T \right\|_{\text{F}}^2  \min_{x} \left\| A - x b^T \right\|_{\text{F}}^2 + \left\| C - x d^T \right\|_{\text{F}}^2 x","['matrices', 'optimization', 'least-squares', 'quadratic-programming', 'rank-1-matrices']"
84,Proof of determinants for matrices of any order,Proof of determinants for matrices of any order,,"I was told that the determinant of a square matrix can be expanded along any row or column and given a proof by expanding in all possible ways, but only for square matrices of order 2 and 3. Is a general proof for any order even possible ? If so, how is this done ? On a similar note, how can we prove the various properties of determinants for square matrices for any order like the following: Swap two rows/columns and all we get is a minus sign as a result. $R_1 \to  R_1+ aR_2$ does not change the determinant. Determinant of the transpose is the same as the determinant of the original matrix.","I was told that the determinant of a square matrix can be expanded along any row or column and given a proof by expanding in all possible ways, but only for square matrices of order 2 and 3. Is a general proof for any order even possible ? If so, how is this done ? On a similar note, how can we prove the various properties of determinants for square matrices for any order like the following: Swap two rows/columns and all we get is a minus sign as a result. $R_1 \to  R_1+ aR_2$ does not change the determinant. Determinant of the transpose is the same as the determinant of the original matrix.",,"['matrices', 'proof-writing', 'determinant']"
85,Convexity of matrix trace functions $A\mapsto \operatorname{Tr} (Bf(A))$,Convexity of matrix trace functions,A\mapsto \operatorname{Tr} (Bf(A)),"Any function $f:\mathbb{R}\rightarrow\mathbb{R}$ is can be extended to a function on self-adjoint $n\times n$ matrices by $$ f(A) = \sum_{i=1}^n f(a_i)v_iv_i^* $$ where $A = \sum_{i=1}^n a_iv_iv_i^*$ is the spectral decomposition of $A$. If $f:\mathbb{R}\rightarrow\mathbb{R}$ is convex, then the function $A\mapsto  \operatorname{Tr}(f(A))$ is also convex as a function of matrices. (See e.g. Theorem 3.27 in Introduction to Matrix Analysis and Applications, or see proof below). My question : If $f:\mathbb{R}\rightarrow\mathbb{R}$ is convex, is the function  $$\tag{1} A\mapsto \operatorname{Tr}(Bf(A)) $$ convex for any self-adjoint positive matrix $B\geq 0$? I'm wondering if the following proofs might be adapted to show convexity of (1). Here I show that $A\mapsto\operatorname{Tr}f(A)$ is a convex function as long as $f$ is convex. I can't get a similar proof to work to show convexity of $A\mapsto\operatorname{Tr}(Bf(A))$, nor can I find a counterexample. Lemma. Suppose $f:\mathbb{R}\rightarrow\mathbb{R}$ is convex and let $A$ be a self-adjoint $n\times n$ matrix. For any orthonormal basis $\{u_1,\dots,u_n\}$ of $\mathbb{C}^n$, it holds that $$ \operatorname{Tr}f(A)\geq \sum_{j=1}^n f(u_j^*Au_j). $$ Proof. Let $A = \sum_{i=1}^n a_iv_iv_i^*$ be the spectral decomposition of $A$. Then $\sum_{j=1}^{n}|u_j^*v_i|^2=1$ for each $j$ and \begin{align*} \operatorname{Tr}f(A)  &= \sum_{j=1}^n\sum_{i=1}^n f(a_i) |u_j^*v_i|^2\\ & \geq \sum_{j=1} f \left(\sum_{i=1}^na_i |u_j^*v_i|^2\right)= \sum_{j=1}f (u_j^*Au_j) \end{align*} where the inequality is due to the convexity of $f$. $\Box$ Theorem. Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be convex. Then $A\mapsto \operatorname{Tr}f(A)$ is convex as a function of matrices. Proof. Let $A$ and $B$ be self-adjoint $n\times n$ matrices and let $t\in(0,1)$. We will show that $$t\operatorname{Tr}f(A)+(1-t)\operatorname{Tr}f(B)\geq \operatorname{Tr}(f(tA+(1-t)B)).$$ Let $\{u_1,\dots,u_n\}$ be the eigenbasis of $tA+(1-t)B$. By the Lemma, \begin{align*} t\operatorname{Tr}f(A)+(1-t)\operatorname{Tr}f(B) & \geq t\sum_{j=1}^n f(u_j^*Au_j) + (1-t)\sum_{j=1}^nf(u_j^*Bu_j)\\ & = \sum_{j=1}^n\bigl(tf(u_j^*Au_j) + (1-t)f(u_j^*Bu_j)\bigr)\\ & \geq \sum_{j=1}^nf\bigl(tu_j^*Au_j+ (1-t)u_j^*Bu_j)\\ & = \sum_{j=1}^nf\bigl(u_j^*(tA+(1-t))Bu_j)\\ & = \operatorname{Tr}(f(tA+(1-t)B)), \end{align*} as desired, where the second inequality is due to convexity of $f$. $\Box$","Any function $f:\mathbb{R}\rightarrow\mathbb{R}$ is can be extended to a function on self-adjoint $n\times n$ matrices by $$ f(A) = \sum_{i=1}^n f(a_i)v_iv_i^* $$ where $A = \sum_{i=1}^n a_iv_iv_i^*$ is the spectral decomposition of $A$. If $f:\mathbb{R}\rightarrow\mathbb{R}$ is convex, then the function $A\mapsto  \operatorname{Tr}(f(A))$ is also convex as a function of matrices. (See e.g. Theorem 3.27 in Introduction to Matrix Analysis and Applications, or see proof below). My question : If $f:\mathbb{R}\rightarrow\mathbb{R}$ is convex, is the function  $$\tag{1} A\mapsto \operatorname{Tr}(Bf(A)) $$ convex for any self-adjoint positive matrix $B\geq 0$? I'm wondering if the following proofs might be adapted to show convexity of (1). Here I show that $A\mapsto\operatorname{Tr}f(A)$ is a convex function as long as $f$ is convex. I can't get a similar proof to work to show convexity of $A\mapsto\operatorname{Tr}(Bf(A))$, nor can I find a counterexample. Lemma. Suppose $f:\mathbb{R}\rightarrow\mathbb{R}$ is convex and let $A$ be a self-adjoint $n\times n$ matrix. For any orthonormal basis $\{u_1,\dots,u_n\}$ of $\mathbb{C}^n$, it holds that $$ \operatorname{Tr}f(A)\geq \sum_{j=1}^n f(u_j^*Au_j). $$ Proof. Let $A = \sum_{i=1}^n a_iv_iv_i^*$ be the spectral decomposition of $A$. Then $\sum_{j=1}^{n}|u_j^*v_i|^2=1$ for each $j$ and \begin{align*} \operatorname{Tr}f(A)  &= \sum_{j=1}^n\sum_{i=1}^n f(a_i) |u_j^*v_i|^2\\ & \geq \sum_{j=1} f \left(\sum_{i=1}^na_i |u_j^*v_i|^2\right)= \sum_{j=1}f (u_j^*Au_j) \end{align*} where the inequality is due to the convexity of $f$. $\Box$ Theorem. Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be convex. Then $A\mapsto \operatorname{Tr}f(A)$ is convex as a function of matrices. Proof. Let $A$ and $B$ be self-adjoint $n\times n$ matrices and let $t\in(0,1)$. We will show that $$t\operatorname{Tr}f(A)+(1-t)\operatorname{Tr}f(B)\geq \operatorname{Tr}(f(tA+(1-t)B)).$$ Let $\{u_1,\dots,u_n\}$ be the eigenbasis of $tA+(1-t)B$. By the Lemma, \begin{align*} t\operatorname{Tr}f(A)+(1-t)\operatorname{Tr}f(B) & \geq t\sum_{j=1}^n f(u_j^*Au_j) + (1-t)\sum_{j=1}^nf(u_j^*Bu_j)\\ & = \sum_{j=1}^n\bigl(tf(u_j^*Au_j) + (1-t)f(u_j^*Bu_j)\bigr)\\ & \geq \sum_{j=1}^nf\bigl(tu_j^*Au_j+ (1-t)u_j^*Bu_j)\\ & = \sum_{j=1}^nf\bigl(u_j^*(tA+(1-t))Bu_j)\\ & = \operatorname{Tr}(f(tA+(1-t)B)), \end{align*} as desired, where the second inequality is due to convexity of $f$. $\Box$",,"['matrices', 'convex-analysis', 'matrix-equations', 'trace']"
86,Generalization to higher dimension of $e^r \not \in \mathbb Q$,Generalization to higher dimension of,e^r \not \in \mathbb Q,"The following is well known and not difficult to prove:  $$\forall r \in \mathbb Q^*,  e^r \not \in \mathbb Q.$$ See for instance https://proofwiki.org/wiki/Exponential_of_Rational_Number_is_Irrational Could this be generalized with the following result, for $n\geq 2$: $$\forall M \in \mathrm{GL}(n,\mathbb Q),  \exp(M) \in \mathrm{GL}(n,\mathbb R)\setminus \mathrm{GL}(n,\mathbb Q)?$$ If yes, do you have any proof or reference?","The following is well known and not difficult to prove:  $$\forall r \in \mathbb Q^*,  e^r \not \in \mathbb Q.$$ See for instance https://proofwiki.org/wiki/Exponential_of_Rational_Number_is_Irrational Could this be generalized with the following result, for $n\geq 2$: $$\forall M \in \mathrm{GL}(n,\mathbb Q),  \exp(M) \in \mathrm{GL}(n,\mathbb R)\setminus \mathrm{GL}(n,\mathbb Q)?$$ If yes, do you have any proof or reference?",,"['matrices', 'rational-numbers', 'matrix-exponential']"
87,"In mathematics, what is an $N \times N \times N$ matrix?","In mathematics, what is an  matrix?",N \times N \times N,"In mathematics, what is an $N \times N \times N$ matrix? I think this is a tensor but definitions of tensors that I have read are so overly complicated and verbose that I have trouble understanding them.","In mathematics, what is an $N \times N \times N$ matrix? I think this is a tensor but definitions of tensors that I have read are so overly complicated and verbose that I have trouble understanding them.",,"['matrices', 'tensors']"
88,Matrix transponse in tensor notation,Matrix transponse in tensor notation,,"In this paper , at the end of chapter 2, the author says that in index notation a matrix is written as $A^\mu_{\;\;\nu}$ and its transpose as $A_\nu^{\;\;\mu}$. $A^\mu_{\;\;\nu}$ looks like a (1,1)-tensor, but in a tensor the order of indices does not matter. How to explain this discrepancy? I am still trying to build intuition on tensors, so forgive me if the following questions are stupid. If $x$ is a vector and $A$ the matrix of a linear transformation, how to express $A^Tx$ in tensor notation? The linear algebra concept for a (1,1)-tensor is the matrix of a linear transformation, for a (0,2)-tensor is the matrix of a quadratic form. What it is for a (2,0)-tensor?","In this paper , at the end of chapter 2, the author says that in index notation a matrix is written as $A^\mu_{\;\;\nu}$ and its transpose as $A_\nu^{\;\;\mu}$. $A^\mu_{\;\;\nu}$ looks like a (1,1)-tensor, but in a tensor the order of indices does not matter. How to explain this discrepancy? I am still trying to build intuition on tensors, so forgive me if the following questions are stupid. If $x$ is a vector and $A$ the matrix of a linear transformation, how to express $A^Tx$ in tensor notation? The linear algebra concept for a (1,1)-tensor is the matrix of a linear transformation, for a (0,2)-tensor is the matrix of a quadratic form. What it is for a (2,0)-tensor?",,"['matrices', 'tensor-products', 'tensors', 'convention']"
89,Uniqueness of pseudoinverse?,Uniqueness of pseudoinverse?,,"Recently, I read a line of reasoning as Since $A$ (a $3\times 3$ matrix) is of rank $2$, its pseudoinverse is not unique. May I ask if there is a quickie to show this?","Recently, I read a line of reasoning as Since $A$ (a $3\times 3$ matrix) is of rank $2$, its pseudoinverse is not unique. May I ask if there is a quickie to show this?",,"['matrices', 'pseudoinverse']"
90,"In this situation, based on order of operations, would cross product happen first or dot product?","In this situation, based on order of operations, would cross product happen first or dot product?",,"I got from wikipedia that the dot product is also referred to as the ""scalar product"" and that the cross product is also referred to as the ""vector product"". Can anyone confirm my inference on the order of operations based off the previous statement? Say I have three vectors, A, B, and C. If I perform this operation D = A * B X C. I know in basic arithmetic, there is PEMDAS, where if you had something like d = 4 + 3 * 2, the 3 * 2 operation would be completed first. Is there also similar for vector operations. Based my inference, I assume here that the cross product gets calculated first, then the dot product. Is that a correct assumption?","I got from wikipedia that the dot product is also referred to as the ""scalar product"" and that the cross product is also referred to as the ""vector product"". Can anyone confirm my inference on the order of operations based off the previous statement? Say I have three vectors, A, B, and C. If I perform this operation D = A * B X C. I know in basic arithmetic, there is PEMDAS, where if you had something like d = 4 + 3 * 2, the 3 * 2 operation would be completed first. Is there also similar for vector operations. Based my inference, I assume here that the cross product gets calculated first, then the dot product. Is that a correct assumption?",,"['matrices', 'cross-product']"
91,Is there any graph property that is equivalent to the spectral radius of its adjacency matrix being less than $1$?,Is there any graph property that is equivalent to the spectral radius of its adjacency matrix being less than ?,1,"Let $G$ be a directed graph and $A$ be the corresponding adjacency matrix . Let $\rho$ denote the spectral radius , and let $I$ be the identity matrix . What can we say about $G$ when the spectral radius of $A$ is less than $1$? Is there any graph property that shows us that fact? More generally, which is equivalent to it? I know that this is spectral graph theory topic, but I'm not an expert in this field. I know that classic result, that $[A^k]_{ij}$ shows the $k$ length walks from $i$ to $j$, and when $\rho(A) < 1$, then exist $(I-A)^{-1}$, and the Neumann-series of $A$ converges to it. Furthermore because $A$ is nonnegative $\rho(A)=\lambda_{\max}$ without abolute value, and $(I-A)^{-1}$ is also nonnegative. That's all I know. Is there any result, that say some graph property which is equivalent to $\rho(A)< 1$?","Let $G$ be a directed graph and $A$ be the corresponding adjacency matrix . Let $\rho$ denote the spectral radius , and let $I$ be the identity matrix . What can we say about $G$ when the spectral radius of $A$ is less than $1$? Is there any graph property that shows us that fact? More generally, which is equivalent to it? I know that this is spectral graph theory topic, but I'm not an expert in this field. I know that classic result, that $[A^k]_{ij}$ shows the $k$ length walks from $i$ to $j$, and when $\rho(A) < 1$, then exist $(I-A)^{-1}$, and the Neumann-series of $A$ converges to it. Furthermore because $A$ is nonnegative $\rho(A)=\lambda_{\max}$ without abolute value, and $(I-A)^{-1}$ is also nonnegative. That's all I know. Is there any result, that say some graph property which is equivalent to $\rho(A)< 1$?",,"['matrices', 'graph-theory', 'spectral-graph-theory', 'adjacency-matrix', 'spectral-radius']"
92,Determinant of matrix with entries $a_{ij}=e^{a_ib_j}$,Determinant of matrix with entries,a_{ij}=e^{a_ib_j},"1) Let $a_1<\dots<a_n$ real numbers and $\lambda_1,\dots,\lambda_n\in\mathbb{R}\backslash\{0\}$ Let $f(x)=\lambda_1e^{a_1x}+\dots+\lambda_ne^{a_nx}$ Show that $f$ has at most $n-1$ zeroes 2) Furthermore, let $b_1<\dots<b_n$ real numbers. Show that $\det A>0$ where $A$ is the matrix with coefficients $a_{ij}=e^{a_ib_j}$ For 1) : I tried a recurrence but gave up quite fast as it didn't seem to lead me anywhere. Then, I remarked that $\sum_{k=1}^n\lambda_ke^{a_kx}=0$ looked like what we do when trying to show that a set is linearly independent, but as that didn't hold for every $\lambda_k\in\mathbb{R}$ I was stuck. For 2) : After searching, I didnt find anything good. I truly have no idea for this one.","1) Let $a_1<\dots<a_n$ real numbers and $\lambda_1,\dots,\lambda_n\in\mathbb{R}\backslash\{0\}$ Let $f(x)=\lambda_1e^{a_1x}+\dots+\lambda_ne^{a_nx}$ Show that $f$ has at most $n-1$ zeroes 2) Furthermore, let $b_1<\dots<b_n$ real numbers. Show that $\det A>0$ where $A$ is the matrix with coefficients $a_{ij}=e^{a_ib_j}$ For 1) : I tried a recurrence but gave up quite fast as it didn't seem to lead me anywhere. Then, I remarked that $\sum_{k=1}^n\lambda_ke^{a_kx}=0$ looked like what we do when trying to show that a set is linearly independent, but as that didn't hold for every $\lambda_k\in\mathbb{R}$ I was stuck. For 2) : After searching, I didnt find anything good. I truly have no idea for this one.",,"['matrices', 'roots', 'determinant']"
93,Lesser known books on matrix properties/identities/inequalities,Lesser known books on matrix properties/identities/inequalities,,"I am looking for books on matrices that are perhaps lesser known than the usual suspects (some of which I list below).. Non-english books are welcome (this question is partly motivated by a book on inequalities that I saw some time ago that was in German and now which I cannot find). Books dealing with inequalities on partitioned matrices and also with strong ties to statistics and covariance matrices would be appreciated. Generally looking for books that may contain facts etc that are not in every other matrix analysis book and perhaps some non-English books that contain a different perspective. (There are a couple of other questions similar to this but the list of recommendations were average/standard and many of them focused on linear algebra whereas I am mainly interested in matrix properties and more obscure references). Some usual suspects: -Matrix Mathematics: Theory, Facts, and Formulas by Dennis S. Bernstein I find this first one really comprehensive. This is really along the lines I am thinking also. The German book I cannot find was in the spirit of this first one. Matrix Analysis by Roger A. Horn and Charles R. Johnson Matrix Analysis (Graduate Texts in Mathematics) by Rajendra Bhatia Topics in Matrix Analysis by Roger A. Horn and Charles R. Johnson Matrix Computations by Gene H. Golub and Charles F. van Van Loan Matrix Analysis for Statistics by James R. Schott Matrix Algebra From a Statistician's Perspective by David A. Harville Matrix Differential Calculus with Applications in Statistics and Econometrics by Jan R. Magnus and Heinz Neudecker Introduction to Matrix Analysis by Richard Bellman Linear Matrix Inequalities in System and Control Theory by Stephen Boyd, Laurent El Ghaoui, Eric Feron and Venkataramanan Balakrishnan Inequalities: Theory of Majorization and Its Applications by Albert W. Marshall, Ingram Olkin and Barry C. Arnold A Survey of Matrix Theory and Matrix Inequalities by Marvin Marcus, Henryk Minc There is also the Matrix Cookbook but that has a lot of errors and surprisingly does not cover a lot of inequalities/identities.","I am looking for books on matrices that are perhaps lesser known than the usual suspects (some of which I list below).. Non-english books are welcome (this question is partly motivated by a book on inequalities that I saw some time ago that was in German and now which I cannot find). Books dealing with inequalities on partitioned matrices and also with strong ties to statistics and covariance matrices would be appreciated. Generally looking for books that may contain facts etc that are not in every other matrix analysis book and perhaps some non-English books that contain a different perspective. (There are a couple of other questions similar to this but the list of recommendations were average/standard and many of them focused on linear algebra whereas I am mainly interested in matrix properties and more obscure references). Some usual suspects: -Matrix Mathematics: Theory, Facts, and Formulas by Dennis S. Bernstein I find this first one really comprehensive. This is really along the lines I am thinking also. The German book I cannot find was in the spirit of this first one. Matrix Analysis by Roger A. Horn and Charles R. Johnson Matrix Analysis (Graduate Texts in Mathematics) by Rajendra Bhatia Topics in Matrix Analysis by Roger A. Horn and Charles R. Johnson Matrix Computations by Gene H. Golub and Charles F. van Van Loan Matrix Analysis for Statistics by James R. Schott Matrix Algebra From a Statistician's Perspective by David A. Harville Matrix Differential Calculus with Applications in Statistics and Econometrics by Jan R. Magnus and Heinz Neudecker Introduction to Matrix Analysis by Richard Bellman Linear Matrix Inequalities in System and Control Theory by Stephen Boyd, Laurent El Ghaoui, Eric Feron and Venkataramanan Balakrishnan Inequalities: Theory of Majorization and Its Applications by Albert W. Marshall, Ingram Olkin and Barry C. Arnold A Survey of Matrix Theory and Matrix Inequalities by Marvin Marcus, Henryk Minc There is also the Matrix Cookbook but that has a lot of errors and surprisingly does not cover a lot of inequalities/identities.",,"['matrices', 'matrix-equations']"
94,Convergence of exponential matrix sum,Convergence of exponential matrix sum,,Let $A$ be an $n\times n$ matrix. Consider the infinite sum $$B=\sum_{k=1}^\infty\frac{A^kt^k}{k!}$$ Each term $\dfrac{A^kt^k}{k!}$ is an $n\times n$ matrix. Does the sum $B$ always converge? (i.e. does the sum for each of the $n^2$ entries always converge?),Let $A$ be an $n\times n$ matrix. Consider the infinite sum $$B=\sum_{k=1}^\infty\frac{A^kt^k}{k!}$$ Each term $\dfrac{A^kt^k}{k!}$ is an $n\times n$ matrix. Does the sum $B$ always converge? (i.e. does the sum for each of the $n^2$ entries always converge?),,"['matrices', 'convergence-divergence', 'exponentiation']"
95,Hall's identity and beyond?,Hall's identity and beyond?,,"There is a identity, well-known among people that know this sort of things, that is called Hall's identity (or Wagner's identity): for all choices of $2\times 2$ matrices over a fixed field $A$, $B$, $C$, we have $$[[A,B]^2,C]=0.$$ Here $[X,Y]=XY-YX$ is the usual commutator. Does this generalize in some way to other contexts? N.B. It is a result of  Shoda in characteristic zero and of Albert and Muckenhoupt for any field, that every matrix of trace zero is a commutator, so Hall's identity tells us that the square of a traceless $2\times 2$ matrix is central. N.B. If $Q$ is a quaternion algebra over a field $k$, then there is a an extension $K/k$ such that the result of extending scalars $Q_K$ from $k$ to $K$ in $Q$ is isomorphic to the $K$-algebra $M_2(K)$ of $2\times 2$  matrices. It follows easily from this that Hall's identity holds also in all quaternion algebras over all fields.","There is a identity, well-known among people that know this sort of things, that is called Hall's identity (or Wagner's identity): for all choices of $2\times 2$ matrices over a fixed field $A$, $B$, $C$, we have $$[[A,B]^2,C]=0.$$ Here $[X,Y]=XY-YX$ is the usual commutator. Does this generalize in some way to other contexts? N.B. It is a result of  Shoda in characteristic zero and of Albert and Muckenhoupt for any field, that every matrix of trace zero is a commutator, so Hall's identity tells us that the square of a traceless $2\times 2$ matrix is central. N.B. If $Q$ is a quaternion algebra over a field $k$, then there is a an extension $K/k$ such that the result of extending scalars $Q_K$ from $k$ to $K$ in $Q$ is isomorphic to the $K$-algebra $M_2(K)$ of $2\times 2$  matrices. It follows easily from this that Hall's identity holds also in all quaternion algebras over all fields.",,"['matrices', 'algebras']"
96,Is $\mathrm{GL}_n(K)$ divisible for an algebraically closed field $K?$,Is  divisible for an algebraically closed field,\mathrm{GL}_n(K) K?,"This is a follow-up question to this one . To reiterate the definition, a group $G$ (possibly non-abelian) is divisible when for all $k\in \Bbb N$ and $g\in G$ there exists $h\in G$ such that $g=h^k.$ Let $K$ be an algebraically closed field. For which $n$ is $\mathrm{GL}_n(K)$ divisible? (It is clearly true for $n=0$ and $n=1$.) The linked question is about the case of $K=\Bbb C$ and the answer is ""for all $n$"" there.","This is a follow-up question to this one . To reiterate the definition, a group $G$ (possibly non-abelian) is divisible when for all $k\in \Bbb N$ and $g\in G$ there exists $h\in G$ such that $g=h^k.$ Let $K$ be an algebraically closed field. For which $n$ is $\mathrm{GL}_n(K)$ divisible? (It is clearly true for $n=0$ and $n=1$.) The linked question is about the case of $K=\Bbb C$ and the answer is ""for all $n$"" there.",,"['group-theory', 'matrices', 'field-theory']"
97,Each automorphism of the matrix algebra is inner.,Each automorphism of the matrix algebra is inner.,,"Exercise I.II.IV in the book Local Representation Theory by J.L.Alperin : Demonstrate that any automorphism of the algebra $M_n(k)$ is inner by using the fact that $M_n(k)$ has a unique simple module. I want only hints, as this seems elementary. Thanks for your attention.","Exercise I.II.IV in the book Local Representation Theory by J.L.Alperin : Demonstrate that any automorphism of the algebra $M_n(k)$ is inner by using the fact that $M_n(k)$ has a unique simple module. I want only hints, as this seems elementary. Thanks for your attention.",,"['matrices', 'noncommutative-algebra']"
98,Determining the Smith Normal Form,Determining the Smith Normal Form,,"Consider the integral matrix $$R = \left(\begin{matrix} 2 & 4 & 6 & -8 \\ 1 & 3 & 2 & -1 \\ 1 & 1 & 4 & -1 \\ 1 & 1 & 2 & 5 \end{matrix}\right).$$ Determine the structure of the abelian group given by generators and relations. $$A_r = \{a_1, a_2, a_3, a_4 | R \circ \vec{a} = 0\}$$ I know you have to row/column reduce the matrix however am unsure what to do next.",Consider the integral matrix Determine the structure of the abelian group given by generators and relations. I know you have to row/column reduce the matrix however am unsure what to do next.,"R = \left(\begin{matrix} 2 & 4 & 6 & -8 \\ 1 & 3 & 2 & -1 \\ 1 & 1 & 4 & -1 \\ 1 & 1 & 2 & 5 \end{matrix}\right). A_r = \{a_1, a_2, a_3, a_4 | R \circ \vec{a} = 0\}","['matrices', 'abelian-groups', 'smith-normal-form']"
99,Can every non-hermitian matrix be written as a sum of hermitan matrices?,Can every non-hermitian matrix be written as a sum of hermitan matrices?,,"If $X$ is non- hermitian i.e., $X^\dagger = (X^*)^T \ne X$ , can one express it as $X = \sum_i \alpha_i Y_i$ where $\alpha_i$ is a complex scalar and $Y_i$ is hermitian?","If is non- hermitian i.e., , can one express it as where is a complex scalar and is hermitian?",X X^\dagger = (X^*)^T \ne X X = \sum_i \alpha_i Y_i \alpha_i Y_i,"['matrices', 'hermitian-matrices']"
