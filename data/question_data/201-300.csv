,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"A game with $\delta$, $\epsilon$ and uniform continuity.","A game with ,  and uniform continuity.",\delta \epsilon,"UPDATE : Bounty awarded, but it is still shady about what f) is. In Makarov's Selected Problems in Real Analysis there's this challenging problem: Describe the set of functions $f: \mathbb R \rightarrow \mathbb R$ having the following properties ( $\epsilon, \delta,x_1,x_2 \in \mathbb R$ ) : a) $\forall \epsilon \qquad\qquad, \exists \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon$ b) $\forall \epsilon >0 \qquad, \exists \delta \qquad \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon$ c) $\forall \epsilon >0 \qquad, \exists \delta>0 \qquad, (x_1-x_2) < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon$ d) $\forall \epsilon >0 \qquad, \forall \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon$ e) $\forall \epsilon >0 \qquad, \exists \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|>\epsilon$ f) $\forall \epsilon >0 \qquad, \exists \delta>0 \qquad, |x_1-x_2| < \epsilon \Rightarrow |f(x_1)-f(x_2)|<\delta$ g) $\forall \epsilon >0 \qquad, \exists \delta>0 \qquad, |f(x_1)-f(x_2)| > \epsilon \Rightarrow |x_1-x_2|> \delta$ h) $\exists \epsilon >0 \qquad, \forall \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon$ i) $\forall \epsilon >0 \qquad, \exists \delta>0 \qquad, x_1-x_2 < \delta \Rightarrow f(x_1)-f(x_2)<\epsilon$ Here's what everybody got so far: a) $\{ \}$ b) every functions c) constant functions d) constant functions e) $\{ \}$ f) functions that are bounded on any closed interval (not sure) g) uniform continous functions h) bounded functions i) Non-decreasing and uniformly continuous. Thanks for your suggestions.","UPDATE : Bounty awarded, but it is still shady about what f) is. In Makarov's Selected Problems in Real Analysis there's this challenging problem: Describe the set of functions having the following properties ( ) : a) b) c) d) e) f) g) h) i) Here's what everybody got so far: a) b) every functions c) constant functions d) constant functions e) f) functions that are bounded on any closed interval (not sure) g) uniform continous functions h) bounded functions i) Non-decreasing and uniformly continuous. Thanks for your suggestions.","f: \mathbb R \rightarrow \mathbb R \epsilon, \delta,x_1,x_2 \in \mathbb R \forall \epsilon \qquad\qquad, \exists \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon \forall \epsilon >0 \qquad, \exists \delta \qquad \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon \forall \epsilon >0 \qquad, \exists \delta>0 \qquad, (x_1-x_2) < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon \forall \epsilon >0 \qquad, \forall \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon \forall \epsilon >0 \qquad, \exists \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|>\epsilon \forall \epsilon >0 \qquad, \exists \delta>0 \qquad, |x_1-x_2| < \epsilon \Rightarrow |f(x_1)-f(x_2)|<\delta \forall \epsilon >0 \qquad, \exists \delta>0 \qquad, |f(x_1)-f(x_2)| > \epsilon \Rightarrow |x_1-x_2|> \delta \exists \epsilon >0 \qquad, \forall \delta>0 \qquad, |x_1-x_2| < \delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon \forall \epsilon >0 \qquad, \exists \delta>0 \qquad, x_1-x_2 < \delta \Rightarrow f(x_1)-f(x_2)<\epsilon \{ \} \{ \}","['real-analysis', 'continuity', 'solution-verification', 'uniform-continuity']"
1,Mathematical symbol for 'slightly greater than'?,Mathematical symbol for 'slightly greater than'?,,"I am wondering if there is a mathematical symbol which indicates that a value is slightly greater than, or slightly less than, another value. I know there is a symbol which indicates that a value is much greater than, or much less than, another value: $$a\gg b \qquad\text{or}\qquad a\ll b$$ I am wondering if there is a counterpart to this, which indicates: $$a\,\text{ is slightly greater than }\, b \qquad\text{or}\qquad a \,\text{ is slightly less than }\, b$$","I am wondering if there is a mathematical symbol which indicates that a value is slightly greater than, or slightly less than, another value. I know there is a symbol which indicates that a value is much greater than, or much less than, another value: $$a\gg b \qquad\text{or}\qquad a\ll b$$ I am wondering if there is a counterpart to this, which indicates: $$a\,\text{ is slightly greater than }\, b \qquad\text{or}\qquad a \,\text{ is slightly less than }\, b$$",,"['calculus', 'real-analysis', 'algebra-precalculus', 'notation']"
2,Ramanujan's radical and how we define an infinite nested radical [duplicate],Ramanujan's radical and how we define an infinite nested radical [duplicate],,"This question already has answers here : Definition of convergence of a nested radical $\sqrt{a_1 + \sqrt{a_2 + \sqrt{a_3 + \sqrt{a_4+\cdots}}}}$? (2 answers) Closed 5 years ago . I know it is true that we have $$\sqrt{1+2\sqrt{1+3\sqrt{1+4\sqrt{1+\cdots}}}}=3$$ The argument is to break the nested radical into something  like $$3 = \sqrt{9}=\sqrt{1+2\sqrt{16}}=\sqrt{1+2\sqrt{1+3\sqrt{25}}}=...=\sqrt{1+2\sqrt{1+3\sqrt{1+4\sqrt{1+\cdots}}}}$$ However, I am not convinced. I can do something like $$4 = \sqrt{16}=\sqrt{1+2\sqrt{56.25}}=\sqrt{1+2\sqrt{1+3\sqrt{\frac{48841}{144}}}}=...=\sqrt{1+2\sqrt{1+3\sqrt{1+4\sqrt{1+\cdots}}}}$$ Something must be wrong and the reason behind should be a misunderstanding of how we define infinite nested radical in the form of $$ \sqrt{a_{0}+a_{1}\sqrt{a_{2}+a_{3}\sqrt{a_{4}+a_{5}\sqrt{a_{6}+\cdots}}}} $$ I researched for a while but all I could find was computation tricks but not a strict definition. Really need help here. Thanks.","This question already has answers here : Definition of convergence of a nested radical $\sqrt{a_1 + \sqrt{a_2 + \sqrt{a_3 + \sqrt{a_4+\cdots}}}}$? (2 answers) Closed 5 years ago . I know it is true that we have The argument is to break the nested radical into something  like However, I am not convinced. I can do something like Something must be wrong and the reason behind should be a misunderstanding of how we define infinite nested radical in the form of I researched for a while but all I could find was computation tricks but not a strict definition. Really need help here. Thanks.",\sqrt{1+2\sqrt{1+3\sqrt{1+4\sqrt{1+\cdots}}}}=3 3 = \sqrt{9}=\sqrt{1+2\sqrt{16}}=\sqrt{1+2\sqrt{1+3\sqrt{25}}}=...=\sqrt{1+2\sqrt{1+3\sqrt{1+4\sqrt{1+\cdots}}}} 4 = \sqrt{16}=\sqrt{1+2\sqrt{56.25}}=\sqrt{1+2\sqrt{1+3\sqrt{\frac{48841}{144}}}}=...=\sqrt{1+2\sqrt{1+3\sqrt{1+4\sqrt{1+\cdots}}}}  \sqrt{a_{0}+a_{1}\sqrt{a_{2}+a_{3}\sqrt{a_{4}+a_{5}\sqrt{a_{6}+\cdots}}}} ,"['real-analysis', 'sequences-and-series', 'elementary-number-theory', 'convergence-divergence', 'nested-radicals']"
3,"Sine function dense in $[-1,1]$",Sine function dense in,"[-1,1]","We know that the sine function takes it values between $[-1,1]$. So is the set $$A = \{ \sin{n} \ : \ n \in \mathbb{N}\}$$ dense in $[-1,1]$. Generally, for showing the set is dense, one proceeds, by finding out what is $\overline{A}$ of this given set. And if $\overline{A} = [-1,1]$, we are through with the proof, but i having trouble here! Similarly can one do this with cosine function also, that is proving $B= \{ \cos{n} \ : \ n \in \mathbb{N}\}$ being dense in $[-1,1]$","We know that the sine function takes it values between $[-1,1]$. So is the set $$A = \{ \sin{n} \ : \ n \in \mathbb{N}\}$$ dense in $[-1,1]$. Generally, for showing the set is dense, one proceeds, by finding out what is $\overline{A}$ of this given set. And if $\overline{A} = [-1,1]$, we are through with the proof, but i having trouble here! Similarly can one do this with cosine function also, that is proving $B= \{ \cos{n} \ : \ n \in \mathbb{N}\}$ being dense in $[-1,1]$",,['real-analysis']
4,"What does the antiderivative of a continuous-but-nowhere-differentiable function ""look like""?","What does the antiderivative of a continuous-but-nowhere-differentiable function ""look like""?",,"Weierstrass' function is an example of a function that is continuous, but nowhere differentiable, and can be visualized as being ""infinitely wrinkled"". I'm having trouble, however, imagining how the integral of such a function would appear. All the techniques that I know of for approximating functions (Taylor series, etc.) would fail on this one. How can this be visualized?","Weierstrass' function is an example of a function that is continuous, but nowhere differentiable, and can be visualized as being ""infinitely wrinkled"". I'm having trouble, however, imagining how the integral of such a function would appear. All the techniques that I know of for approximating functions (Taylor series, etc.) would fail on this one. How can this be visualized?",,"['calculus', 'real-analysis']"
5,Convergence in measure implies convergence almost everywhere of a subsequence,Convergence in measure implies convergence almost everywhere of a subsequence,,"How can I prove that if a sequence of functions $\{f_n\}$ that converges to $f$ in measure on a space of finite measure, then there exists a subsequence of $\{f_n\}$ that converges to $f$ almost everywhere?","How can I prove that if a sequence of functions $\{f_n\}$ that converges to $f$ in measure on a space of finite measure, then there exists a subsequence of $\{f_n\}$ that converges to $f$ almost everywhere?",,"['real-analysis', 'measure-theory', 'convergence-divergence']"
6,What's the relationship between a measure space and a metric space?,What's the relationship between a measure space and a metric space?,,"Definition of Measurable Space : An ordered pair $(\Omega, \mathcal{F})$ is a measurable space if $\mathcal{F}$ is a $\sigma$ -algebra on $\Omega$ . Definition of Measure : Let $(\Omega, \mathcal{F})$ be a measurable space, $Î¼$ is an non-negative function defined on $\mathcal{F}$ (that is $\mu: \mathcal{F} \to [0, +\infty]$ ). If $\mu(\emptyset) = 0$ and $\mu$ is countably additive (that is $A_n \in \mathcal{F}$ , $n \geqslant 1$ , $A_n \cap A_m = \emptyset$ , $n \neq m \Rightarrow \mu(\cup_{n=1}^{\infty} A_n) = \sum_{n=1}^{\infty} \mu(A_n)$ ) then $\mu$ is a measure on $(\Omega, \mathcal{F})$ . Definition of Measure Space : Let Î¼ is a measure on $(\Omega, \mathcal{F})$ then $(\Omega, \mathcal{F}, \mu)$ is a measure space . Definition of Metric Space : A metric space is an ordered pair $(M,d)$ where $M$ is a set and $d$ is a metric on $M$ , i.e., a function $$d \colon M \times M \rightarrow \mathbb{R}$$ such that for any $x, y, z \in M$ , the following holds: $d(x,y) \ge 0$ (non-negative), $d(x,y) = 0\, \iff x = y\ $ ,     (identity of indiscernibles), $d(x,y) = d(y,x)\ $ ,     (symmetry), $d(x,z) \le d(x,y) + d(y,z)$ (triangle inequality). Is a measure space $(\Omega, \mathcal{F}, \mu)$ necessarily a metric space? What's the relationship between them?","Definition of Measurable Space : An ordered pair is a measurable space if is a -algebra on . Definition of Measure : Let be a measurable space, is an non-negative function defined on (that is ). If and is countably additive (that is , , , ) then is a measure on . Definition of Measure Space : Let Î¼ is a measure on then is a measure space . Definition of Metric Space : A metric space is an ordered pair where is a set and is a metric on , i.e., a function such that for any , the following holds: (non-negative), ,     (identity of indiscernibles), ,     (symmetry), (triangle inequality). Is a measure space necessarily a metric space? What's the relationship between them?","(\Omega, \mathcal{F}) \mathcal{F} \sigma \Omega (\Omega, \mathcal{F}) Î¼ \mathcal{F} \mu: \mathcal{F} \to [0, +\infty] \mu(\emptyset) = 0 \mu A_n \in \mathcal{F} n \geqslant 1 A_n \cap A_m = \emptyset n \neq m \Rightarrow \mu(\cup_{n=1}^{\infty} A_n) = \sum_{n=1}^{\infty} \mu(A_n) \mu (\Omega, \mathcal{F}) (\Omega, \mathcal{F}) (\Omega, \mathcal{F}, \mu) (M,d) M d M d \colon M \times M \rightarrow \mathbb{R} x, y, z \in M d(x,y) \ge 0 d(x,y) = 0\, \iff x = y\  d(x,y) = d(y,x)\  d(x,z) \le d(x,y) + d(y,z) (\Omega, \mathcal{F}, \mu)","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
7,Example of a continuous function that is not Lebesgue measurable,Example of a continuous function that is not Lebesgue measurable,,"Let $\mathcal{L}$ denote the $\sigma$-algebra of Lebesgue measurable sets on $\mathbb{R}$.  Then, if memory serves, there is an example (and of course, if there is one, there are many) of a continuous function $f:\mathbb{R}\rightarrow \mathbb{R}$ that is not measurable in the sense that $f:(\mathbb{R},\mathcal{L})\rightarrow (\mathbb{R},\mathcal{L})$ is measurable, but unfortunately, I was not able to recall the example.  Could somebody please enlighten me? Note that this is not in contradiction with the usual ""Every continuous function is measurable."", because in this statement it is implicit that the co-domain is equipped with the Borel sets, not the Lebesgue measurable sets.","Let $\mathcal{L}$ denote the $\sigma$-algebra of Lebesgue measurable sets on $\mathbb{R}$.  Then, if memory serves, there is an example (and of course, if there is one, there are many) of a continuous function $f:\mathbb{R}\rightarrow \mathbb{R}$ that is not measurable in the sense that $f:(\mathbb{R},\mathcal{L})\rightarrow (\mathbb{R},\mathcal{L})$ is measurable, but unfortunately, I was not able to recall the example.  Could somebody please enlighten me? Note that this is not in contradiction with the usual ""Every continuous function is measurable."", because in this statement it is implicit that the co-domain is equipped with the Borel sets, not the Lebesgue measurable sets.",,"['real-analysis', 'measure-theory']"
8,"Show that there exist $[a,b]\subset [0,1]$, such that $\int_{a}^{b}f(x)dx$ = $\int_{a}^{b}g(x)dx$ = $\frac{1}{2}$","Show that there exist , such that  =  =","[a,b]\subset [0,1] \int_{a}^{b}f(x)dx \int_{a}^{b}g(x)dx \frac{1}{2}","Let $f(x)$ and $g(x)$ be two continuous functions on $[0,1]$ and $$\int_{0}^{1}f(x) dx= \int_{0}^{1}g(x)dx = 1$$ Show that there exist $[a,b]\subset [0,1]$ , such that $$\int_{a}^{b}f(x) dx= \int_{a}^{b}g(x)dx = \frac{1}{2} $$ The question can be solved by considering the fundamental group of $S^1$ , now I am wondering if we can solve it by real-analysis. Here is the topological solution: Assume that for any $[a,b]\subset[0,1]$ , we always have $$(\int_{a}^{b}f(x) dx\neq \frac{1}{2}) \quad \vee \quad(\int_{a}^{b}g(x)dx \neq \frac{1}{2}) $$ Consider mapping $$\phi:D\rightarrow\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\},\,\,(x,y)\mapsto(\int_{y}^{x}f(t) dt,\int_{y}^{x}g(t) dt)$$ where $D=\{(x,y)|0\leq x\leq y\leq 1\}$ . Let $a$ be path from $(0,0)$ to $(0,1) $ in $D$ and $b$ be path from $(0,1)$ to $(1,1) $ in $D$ , then $ab$ is a path from $(0,0)$ to $(1,1) $ in $D$ and $$\phi\circ (ab):[0,1]\rightarrow\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\} $$ Notice that $$(\phi\circ (ab))(0)=(\phi\circ (ab))(1)=(0,0)$$ so $\phi\circ (ab)$ is a loop based on $(0,0)$ in $\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\}$ . For any $t\in [0,\frac{1}{2}]$ , it's not hard to get that $$(\phi\circ (ab))(t+\frac{1}{2})=(1,1)-(\phi\circ (ab))(t)$$ is equivalent to $$(\phi\circ (ab))(t+\frac{1}{2})-(\frac{1}{2},\frac{1}{2})=-((\phi\circ (ab))(t)-(\frac{1}{2},\frac{1}{2}))$$ Define retraction $$r:\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\}\rightarrow S^1,\quad (x,y)\rightarrow\ \frac{(x,y)-(\frac{1}{2},\frac{1}{2})}{||(x,y)-(\frac{1}{2},\frac{1}{2})||}$$ Then $r\circ \phi\circ (ab)$ is a loop on $S^1$ , such that $$(r\circ \phi\circ (ab))(t+\frac{1}{2})=-(r\circ \phi\circ (ab))(t),\quad\forall t\in [0,\frac{1}{2}]$$ So $<r\circ \phi\circ (ab)>$ is not trivial in $\pi_1(S^1)$ . However, $ab$ is path homotopic to $c$ in $D$ , where $c$ is the path between $(0,0)$ and $(1,1)$ in $D$ . In this case, $r\circ \phi\circ (ab)$ is a point-path in $S^1$ by $$(\phi\circ c)(t)=\phi(t,t)\equiv (0,0)$$ which leads to contradiction.","Let and be two continuous functions on and Show that there exist , such that The question can be solved by considering the fundamental group of , now I am wondering if we can solve it by real-analysis. Here is the topological solution: Assume that for any , we always have Consider mapping where . Let be path from to in and be path from to in , then is a path from to in and Notice that so is a loop based on in . For any , it's not hard to get that is equivalent to Define retraction Then is a loop on , such that So is not trivial in . However, is path homotopic to in , where is the path between and in . In this case, is a point-path in by which leads to contradiction.","f(x) g(x) [0,1] \int_{0}^{1}f(x) dx= \int_{0}^{1}g(x)dx = 1 [a,b]\subset [0,1] \int_{a}^{b}f(x) dx= \int_{a}^{b}g(x)dx = \frac{1}{2}  S^1 [a,b]\subset[0,1] (\int_{a}^{b}f(x) dx\neq \frac{1}{2}) \quad \vee \quad(\int_{a}^{b}g(x)dx \neq \frac{1}{2})  \phi:D\rightarrow\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\},\,\,(x,y)\mapsto(\int_{y}^{x}f(t) dt,\int_{y}^{x}g(t) dt) D=\{(x,y)|0\leq x\leq y\leq 1\} a (0,0) (0,1)  D b (0,1) (1,1)  D ab (0,0) (1,1)  D \phi\circ (ab):[0,1]\rightarrow\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\}  (\phi\circ (ab))(0)=(\phi\circ (ab))(1)=(0,0) \phi\circ (ab) (0,0) \mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\} t\in [0,\frac{1}{2}] (\phi\circ (ab))(t+\frac{1}{2})=(1,1)-(\phi\circ (ab))(t) (\phi\circ (ab))(t+\frac{1}{2})-(\frac{1}{2},\frac{1}{2})=-((\phi\circ (ab))(t)-(\frac{1}{2},\frac{1}{2})) r:\mathbb{E}^2\backslash\{(\frac{1}{2},\frac{1}{2})\}\rightarrow S^1,\quad (x,y)\rightarrow\ \frac{(x,y)-(\frac{1}{2},\frac{1}{2})}{||(x,y)-(\frac{1}{2},\frac{1}{2})||} r\circ \phi\circ (ab) S^1 (r\circ \phi\circ (ab))(t+\frac{1}{2})=-(r\circ \phi\circ (ab))(t),\quad\forall t\in [0,\frac{1}{2}] <r\circ \phi\circ (ab)> \pi_1(S^1) ab c D c (0,0) (1,1) D r\circ \phi\circ (ab) S^1 (\phi\circ c)(t)=\phi(t,t)\equiv (0,0)",['real-analysis']
9,"How to evaluate double limit of multifactorial $\lim\limits_{k\to\infty}\lim\limits_{n\to 0} \sqrt[n]{n\underbrace{!!!!\cdots!}_{k\,\text{times}}}$",How to evaluate double limit of multifactorial,"\lim\limits_{k\to\infty}\lim\limits_{n\to 0} \sqrt[n]{n\underbrace{!!!!\cdots!}_{k\,\text{times}}}","Define the multifactorial function $$n!^{(k)}=n(n-k)(n-2k)\cdots$$ where the product extends to the least positive integer of $n$ modulo $k$ . In this answer , I derived one of several analytic continuations of this function to the real numbers, which is as follows $$x!^{(k)}=k^{x/k}\Gamma\left(1+\frac xk\right)\prod_{i=1}^{k-1}\left(\frac{ik^{-i/k}}{\Gamma(1+i/k)}\right)^{\sin(\pi(x-i))\cot(\pi(x-i)/k)/k}.$$ The limit of original interest $$F(k)=\lim_{x\to0}\,(x!^{(k)})^{1/x}=\left[\frac k{e^\gamma}\prod_{i=1}^{k-1}\left(\frac{\Gamma(1+i/k)}{ik^{-i/k}}\right)^{\pi(-1)^i\cot\frac{\pi i}k}\right]^{1/k}$$ is a straightforward consequence of the above result. Out of curiosity I plotted $F(k)$ and it appears that the double limit $$m=\lim_{k\to\infty}\lim_{x\to0}\,(x!^{(k)})^{1/x}$$ exists, converging rapidly to around $0.852$ when $k\in(s-1/2,s+1/2)$ for all positive odd integers $s$ , but converging much slower to the same value when $k\in(s+1/2,s+3/2)$ . We can rewrite the limit as \begin{align}m&=\lim_{k\to\infty}\exp\left(\frac{-\gamma+\log k+\pi\sum\limits_{i=1}^{k-1}(-1)^i\cot\frac{\pi i}k\log\left(\frac{\Gamma(1+i/k)}{ik^{-i/k}}\right)}k\right)\\&=\lim_{k\to\infty}\exp\left(\frac\pi k\sum\limits_{i=1}^{k-1}(-1)^i\cot\frac{\pi i}k\log\frac{\Gamma(1+i/k)}{ik^{-i/k}}\right)\\&=\exp\left(\lim_{k\to\infty}\frac\pi k\sum\limits_{i=1}^{k-1}(-1)^i\cot\frac{\pi i}k\log\frac{k^{i/k}\Gamma(i/k)}k\right)\end{align} using L'Hopital on $(-\gamma+\log k)/k$ . The term inside the exponential looks very much like a Riemann sum but I'm not sure where to go after that. It seems that none of the terms in the logarithm can be split additively to evaluate the limit as each component by itself is divergent. Is there a closed form for this double limit?","Define the multifactorial function where the product extends to the least positive integer of modulo . In this answer , I derived one of several analytic continuations of this function to the real numbers, which is as follows The limit of original interest is a straightforward consequence of the above result. Out of curiosity I plotted and it appears that the double limit exists, converging rapidly to around when for all positive odd integers , but converging much slower to the same value when . We can rewrite the limit as using L'Hopital on . The term inside the exponential looks very much like a Riemann sum but I'm not sure where to go after that. It seems that none of the terms in the logarithm can be split additively to evaluate the limit as each component by itself is divergent. Is there a closed form for this double limit?","n!^{(k)}=n(n-k)(n-2k)\cdots n k x!^{(k)}=k^{x/k}\Gamma\left(1+\frac xk\right)\prod_{i=1}^{k-1}\left(\frac{ik^{-i/k}}{\Gamma(1+i/k)}\right)^{\sin(\pi(x-i))\cot(\pi(x-i)/k)/k}. F(k)=\lim_{x\to0}\,(x!^{(k)})^{1/x}=\left[\frac k{e^\gamma}\prod_{i=1}^{k-1}\left(\frac{\Gamma(1+i/k)}{ik^{-i/k}}\right)^{\pi(-1)^i\cot\frac{\pi i}k}\right]^{1/k} F(k) m=\lim_{k\to\infty}\lim_{x\to0}\,(x!^{(k)})^{1/x} 0.852 k\in(s-1/2,s+1/2) s k\in(s+1/2,s+3/2) \begin{align}m&=\lim_{k\to\infty}\exp\left(\frac{-\gamma+\log k+\pi\sum\limits_{i=1}^{k-1}(-1)^i\cot\frac{\pi i}k\log\left(\frac{\Gamma(1+i/k)}{ik^{-i/k}}\right)}k\right)\\&=\lim_{k\to\infty}\exp\left(\frac\pi k\sum\limits_{i=1}^{k-1}(-1)^i\cot\frac{\pi i}k\log\frac{\Gamma(1+i/k)}{ik^{-i/k}}\right)\\&=\exp\left(\lim_{k\to\infty}\frac\pi k\sum\limits_{i=1}^{k-1}(-1)^i\cot\frac{\pi i}k\log\frac{k^{i/k}\Gamma(i/k)}k\right)\end{align} (-\gamma+\log k)/k","['real-analysis', 'limits', 'factorial', 'closed-form', 'gamma-function']"
10,Give a concrete sequence of rationals which converges to an irrational number and vice versa.,Give a concrete sequence of rationals which converges to an irrational number and vice versa.,,"Give a concrete sequence of rationals which converges to an irrational number and vice versa.... My work I could give a sequence of irrationals which converges to a rational number... Let $r\in \mathbb Q,$ $$a_n=\frac {\sqrt 2} n+r$$ But I couldn't give a sequence of rationals which converges to an irrational.. Help me to work out.....","Give a concrete sequence of rationals which converges to an irrational number and vice versa.... My work I could give a sequence of irrationals which converges to a rational number... Let $r\in \mathbb Q,$ $$a_n=\frac {\sqrt 2} n+r$$ But I couldn't give a sequence of rationals which converges to an irrational.. Help me to work out.....",,"['real-analysis', 'sequences-and-series']"
11,What does Big O actually tell you?,What does Big O actually tell you?,,"Two days ago I felt very uncomfortable with Big O notation. I've already asked two questions: Why to calculate ""Big O"" if we can just calculate number of steps? The main idea behind Big O notation And now almost everything has become clear. But there are few questions that I still can't understand: Suppose we have an algorithm that runs in $1000n$ steps. Why people say that $1000$ coefficient becomes insignificant when $n$ gets really large (and that's why we can throw it away)? This really confuses me because no matter how large $n$ is but $1000n$ is going to be $1000$ times bigger than $n$ . And this is very significant (in my head). Any examples why it is considered insignificant as $n$ tends to infinity would be appreciated. Why Big O is told to estimate running time in worst case? Given running time $O(n)$ , how is it considered to be worst case behavior? I mean in this case we think that our algorithm is not slower than $n$ , right? But in reality the actual running time could be $1000n$ and it is indeed slower than $n$ . According to the definition, Big O gives us a scaled upper bound of $f$ as $n \to +\infty$ , where $f$ is our function of time. But how do we interpret it? I mean, given algorithm running in $O(n)$ , we will never be able to calculate the actual number of steps this algorithm takes. We just know that if we double the size of the input, we double the computation time as well, right? But if that $O(n)$ algorithm really takes $1000n$ steps then we also need to multiply the size of the input by $1000$ to be able to visualise how it grows, because $1000n$ and $n$ have very different slopes. Thus in this case if you just double the computation time for the doubled size of the input, you're going to get wrong idea about how the running time grows, right? So how then do you visualise its growth rate? I want to add that I know the definition of Big O and how to calculate it, so there is no need in explaining it. Also I've already googled all these questions tons of times and no luck. I'm learning calculus at the moment, so I hope I asked this question in the right place. Thank you in advance!","Two days ago I felt very uncomfortable with Big O notation. I've already asked two questions: Why to calculate ""Big O"" if we can just calculate number of steps? The main idea behind Big O notation And now almost everything has become clear. But there are few questions that I still can't understand: Suppose we have an algorithm that runs in steps. Why people say that coefficient becomes insignificant when gets really large (and that's why we can throw it away)? This really confuses me because no matter how large is but is going to be times bigger than . And this is very significant (in my head). Any examples why it is considered insignificant as tends to infinity would be appreciated. Why Big O is told to estimate running time in worst case? Given running time , how is it considered to be worst case behavior? I mean in this case we think that our algorithm is not slower than , right? But in reality the actual running time could be and it is indeed slower than . According to the definition, Big O gives us a scaled upper bound of as , where is our function of time. But how do we interpret it? I mean, given algorithm running in , we will never be able to calculate the actual number of steps this algorithm takes. We just know that if we double the size of the input, we double the computation time as well, right? But if that algorithm really takes steps then we also need to multiply the size of the input by to be able to visualise how it grows, because and have very different slopes. Thus in this case if you just double the computation time for the doubled size of the input, you're going to get wrong idea about how the running time grows, right? So how then do you visualise its growth rate? I want to add that I know the definition of Big O and how to calculate it, so there is no need in explaining it. Also I've already googled all these questions tons of times and no luck. I'm learning calculus at the moment, so I hope I asked this question in the right place. Thank you in advance!",1000n 1000 n n 1000n 1000 n n O(n) n 1000n n f n \to +\infty f O(n) O(n) 1000n 1000 1000n n,"['real-analysis', 'calculus', 'algorithms', 'asymptotics', 'computational-complexity']"
12,Why is $l^\infty$ not separable?,Why is  not separable?,l^\infty,"My functional analysis textbook says ""The metric space $l^\infty$ is not separable."" The metric defined between two sequences $\{a_1,a_2,a_3\dots\}$ and $\{b_1,b_2,b_3,\dots\}$ is $\sup\limits_{i\in\Bbb{N}}|{a_i-b_i}|$. How can this be? Isn't the set of sequences containing complex numbers with rational coefficients the required countable dense subset of $l^\infty$? Thanks in advance!","My functional analysis textbook says ""The metric space $l^\infty$ is not separable."" The metric defined between two sequences $\{a_1,a_2,a_3\dots\}$ and $\{b_1,b_2,b_3,\dots\}$ is $\sup\limits_{i\in\Bbb{N}}|{a_i-b_i}|$. How can this be? Isn't the set of sequences containing complex numbers with rational coefficients the required countable dense subset of $l^\infty$? Thanks in advance!",,"['real-analysis', 'functional-analysis']"
13,Multiples of an irrational number forming a dense subset,Multiples of an irrational number forming a dense subset,,"Say you picked your favorite irrational number $q$ and looking at $S = \{nq: n\in \mathbb{Z} \}$ in $\mathbb{R}$, you chopped off everything but the decimal of $nq$, leaving you with a number in $[0,1]$.  Is this new set dense in $[0,1]$? If so, why? (Basically looking at the $\mathbb{Z}$-orbit of a fixed irrational number in $\mathbb{R}/\mathbb{Z}$ where we mean the quotient by the group action of $\mathbb{Z}$.) Thanks!","Say you picked your favorite irrational number $q$ and looking at $S = \{nq: n\in \mathbb{Z} \}$ in $\mathbb{R}$, you chopped off everything but the decimal of $nq$, leaving you with a number in $[0,1]$.  Is this new set dense in $[0,1]$? If so, why? (Basically looking at the $\mathbb{Z}$-orbit of a fixed irrational number in $\mathbb{R}/\mathbb{Z}$ where we mean the quotient by the group action of $\mathbb{Z}$.) Thanks!",,"['real-analysis', 'sequences-and-series', 'general-topology', 'irrational-numbers', 'faq']"
14,Proving that the set of limit points of a set is closed,Proving that the set of limit points of a set is closed,,"From Rudin's Principles of Mathematical Analysis (Chapter 2, Exercise 6) Let $E'$ be the set of all limit points of a set $E$. Prove that $E'$ is closed. I think I got it but my argument is a bit hand wavy: If $x$ is a limit point of $E'$, then every neighborhood of $x$ contains some $y\in E'$, and every neighborhood of $y$ contains some $z\in E$. Therefore every neighborhood of $x$ contains some $z\in E$, and so $x$ is a limit point of $E$. Then $x\in E'$, so $E'$ is closed. The thing that's bugging me is the leap from one neighborhood to another. Is this formally correct?","From Rudin's Principles of Mathematical Analysis (Chapter 2, Exercise 6) Let $E'$ be the set of all limit points of a set $E$. Prove that $E'$ is closed. I think I got it but my argument is a bit hand wavy: If $x$ is a limit point of $E'$, then every neighborhood of $x$ contains some $y\in E'$, and every neighborhood of $y$ contains some $z\in E$. Therefore every neighborhood of $x$ contains some $z\in E$, and so $x$ is a limit point of $E$. Then $x\in E'$, so $E'$ is closed. The thing that's bugging me is the leap from one neighborhood to another. Is this formally correct?",,"['real-analysis', 'general-topology', 'analysis']"
15,How do you prove that $\{ Ax \mid x \geq 0 \}$ is closed?,How do you prove that  is closed?,\{ Ax \mid x \geq 0 \},"Let $A$ be a real $m \times n$ matrix. How do you prove that $\{ Ax \mid x \geq 0, x \in \mathbb R^n \}$ is closed (as in, contains all its limit points)? The inequality $x \geq 0$ is interpreted component-wise. This fact is used in some proofs of Farkas's lemma.  It seems like it should be easy, but the proof I've seen seems to be unexpectedly complicated.  Is there a very clear / easy / obvious proof of this fact? (Note that linear transformations do not always map closed sets to closed sets, as discussed in this question .  For example, let $S = \{ (x,y) \in \mathbb R^2 \mid y \geq e^x \}$ and let $T:\mathbb R^2 \to \mathbb R^2$ such that $T(x,y) = (0,y)$.  Then $S$ is closed, but $T(S)$ is not closed.) Edit: Here is a simple proof in the case where $A$ has full column rank.  (A very similar proof is given in Nocedal and Wright, in the Notes and References at the end of chapter 12.) Let $y^*$ be a limit point of $\Omega = \{ Ax \mid x \geq 0, x \in \mathbb R^n \}$.  There exists a sequence $(x_i)_{i=1}^\infty$ of points in $\mathbb R^n$ such that $x_i \geq 0$ for all $i$ and $A x_i \to y^*$ as $i \to \infty$.  Let $B$ be a left inverse for $A$. Then $B A x_i \to B y^*$ as $i \to \infty$.  In other words, $x_i \to x^*$ as $i \to \infty$, where we have defined $x^* = B y^*$.  Clearly, $x^* \geq 0$ and $A x^* = y^*$. This shows that $y^* \in \Omega$. (Alternatively, you could just note that if $A$ has full column rank then the mapping $x \mapsto Ax$ is a homeomorphism between $\mathbb R^n$ and $R(A)$, so it maps closed sets to closed sets.  This shows that $\Omega$ is a closed subset of $R(A)$, and it follows that $\Omega$ is a closed subset of $\mathbb R^m$.)","Let $A$ be a real $m \times n$ matrix. How do you prove that $\{ Ax \mid x \geq 0, x \in \mathbb R^n \}$ is closed (as in, contains all its limit points)? The inequality $x \geq 0$ is interpreted component-wise. This fact is used in some proofs of Farkas's lemma.  It seems like it should be easy, but the proof I've seen seems to be unexpectedly complicated.  Is there a very clear / easy / obvious proof of this fact? (Note that linear transformations do not always map closed sets to closed sets, as discussed in this question .  For example, let $S = \{ (x,y) \in \mathbb R^2 \mid y \geq e^x \}$ and let $T:\mathbb R^2 \to \mathbb R^2$ such that $T(x,y) = (0,y)$.  Then $S$ is closed, but $T(S)$ is not closed.) Edit: Here is a simple proof in the case where $A$ has full column rank.  (A very similar proof is given in Nocedal and Wright, in the Notes and References at the end of chapter 12.) Let $y^*$ be a limit point of $\Omega = \{ Ax \mid x \geq 0, x \in \mathbb R^n \}$.  There exists a sequence $(x_i)_{i=1}^\infty$ of points in $\mathbb R^n$ such that $x_i \geq 0$ for all $i$ and $A x_i \to y^*$ as $i \to \infty$.  Let $B$ be a left inverse for $A$. Then $B A x_i \to B y^*$ as $i \to \infty$.  In other words, $x_i \to x^*$ as $i \to \infty$, where we have defined $x^* = B y^*$.  Clearly, $x^* \geq 0$ and $A x^* = y^*$. This shows that $y^* \in \Omega$. (Alternatively, you could just note that if $A$ has full column rank then the mapping $x \mapsto Ax$ is a homeomorphism between $\mathbb R^n$ and $R(A)$, so it maps closed sets to closed sets.  This shows that $\Omega$ is a closed subset of $R(A)$, and it follows that $\Omega$ is a closed subset of $\mathbb R^m$.)",,"['real-analysis', 'general-topology', 'convex-analysis', 'convex-optimization']"
16,"What function can be differentiated twice, but not 3 times?","What function can be differentiated twice, but not 3 times?",,"In complex analysis class professor said that in complex analysis if a function is differentiable once, it can be differentiated infinite number of times. In real analysis there are cases where a function can be differentiated twice, but not 3 times. Do anyone have idea what he had in mind? I mean specific example where function can be differentiated two times but not three? EDIT. Thank you for answers! but if we replace $x\to z$ and treat it as a complex function. Why are we not getting in the same problem? Why according to my professor it is still differentiable at $0$?","In complex analysis class professor said that in complex analysis if a function is differentiable once, it can be differentiated infinite number of times. In real analysis there are cases where a function can be differentiated twice, but not 3 times. Do anyone have idea what he had in mind? I mean specific example where function can be differentiated two times but not three? EDIT. Thank you for answers! but if we replace $x\to z$ and treat it as a complex function. Why are we not getting in the same problem? Why according to my professor it is still differentiable at $0$?",,"['calculus', 'real-analysis', 'complex-analysis', 'derivatives']"
17,"Evaluating $\int_0^{\frac{\pi}{2}}\ln\left(\frac{\ln^2\sin\theta}{\pi^2+\ln^2\sin\theta}\right)\,\frac{\ln\cos\theta}{\tan\theta}\,d\theta$",Evaluating,"\int_0^{\frac{\pi}{2}}\ln\left(\frac{\ln^2\sin\theta}{\pi^2+\ln^2\sin\theta}\right)\,\frac{\ln\cos\theta}{\tan\theta}\,d\theta","Prove $$\int_0^{\frac{\pi}{2}}\ln\left(\frac{\ln^2\sin\theta}{\pi^2+\ln^2\sin\theta}\right)\,\frac{\ln\cos\theta}{\tan\theta}\,d\theta = \frac{\pi^2}{4}$$",Prove,"\int_0^{\frac{\pi}{2}}\ln\left(\frac{\ln^2\sin\theta}{\pi^2+\ln^2\sin\theta}\right)\,\frac{\ln\cos\theta}{\tan\theta}\,d\theta = \frac{\pi^2}{4}","['calculus', 'real-analysis', 'integration', 'definite-integrals', 'contest-math']"
18,How can I prove $\pi=e^{3/2}\prod_{n=2}^{\infty}e\left(1-\frac{1}{n^2}\right)^{n^2}$?,How can I prove ?,\pi=e^{3/2}\prod_{n=2}^{\infty}e\left(1-\frac{1}{n^2}\right)^{n^2},I am interested about some infinite product representations of $\pi$ and $e$ like this. Last week I found this formula on internet $$\pi=e^{3/2}\prod_{n=2}^{\infty}e\left(1-\frac{1}{n^2}\right)^{n^2}$$ which  looks like unbelievable. (I forgot the link but I am sure that this is the formula.) How can I start to prove this formula? Thank You.,I am interested about some infinite product representations of $\pi$ and $e$ like this. Last week I found this formula on internet $$\pi=e^{3/2}\prod_{n=2}^{\infty}e\left(1-\frac{1}{n^2}\right)^{n^2}$$ which  looks like unbelievable. (I forgot the link but I am sure that this is the formula.) How can I start to prove this formula? Thank You.,,"['real-analysis', 'exponential-function', 'closed-form', 'pi', 'infinite-product']"
19,Completing the square to solve limit problems,Completing the square to solve limit problems,,"There's a trick I've been using to solve a common class of limit problems for a while now.  I've never seen it taught in a textbook, but I once wrote out a few lines of work to justify it to myself in one of my notebooks.  Here is a sample problem to illustrate my technique: $$\lim_{x\to\infty}\sqrt{x^2+x}-x=\lim_{x\to\infty}\sqrt{x^2+x+\frac14}-x=\lim_{x\to\infty}\left(x+\frac12\right)-x=\frac12$$ It's such a shortcut compared to rationalization or however you're ""supposed"" to solve that, and I'm quite certain that it's valid. But I'm starting to feel a little leery posting this as a solution to MSE problems since I don't quite remember the few lines of justification all those years ago.  Could someone please provide a proof that $$\lim_{x\to\infty}\sqrt{x^2+2\alpha x}-\sqrt{x^2+2\alpha x+\alpha^2}=0$$ or whatever equivalent formulation you would prefer?  I'm sure that delta-epsilon drudgery is not necessary at all.  (If nobody gets to this by the end of the day, I'll self-answer just to have something to link to.) Thanks!","There's a trick I've been using to solve a common class of limit problems for a while now.  I've never seen it taught in a textbook, but I once wrote out a few lines of work to justify it to myself in one of my notebooks.  Here is a sample problem to illustrate my technique: It's such a shortcut compared to rationalization or however you're ""supposed"" to solve that, and I'm quite certain that it's valid. But I'm starting to feel a little leery posting this as a solution to MSE problems since I don't quite remember the few lines of justification all those years ago.  Could someone please provide a proof that or whatever equivalent formulation you would prefer?  I'm sure that delta-epsilon drudgery is not necessary at all.  (If nobody gets to this by the end of the day, I'll self-answer just to have something to link to.) Thanks!",\lim_{x\to\infty}\sqrt{x^2+x}-x=\lim_{x\to\infty}\sqrt{x^2+x+\frac14}-x=\lim_{x\to\infty}\left(x+\frac12\right)-x=\frac12 \lim_{x\to\infty}\sqrt{x^2+2\alpha x}-\sqrt{x^2+2\alpha x+\alpha^2}=0,['real-analysis']
20,Enumerations of the rationals with summable gaps $(q_i-q_{i-1})^2$,Enumerations of the rationals with summable gaps,(q_i-q_{i-1})^2,"Here is a question from my undergraduate days which I never knew the answer to. I just want to know if anyone can offer me a hint. Consider the rationals in $[0,1]$. Does there exist a (bijective) enumeration of these rationals $q_1,q_2,\ldots$ such that the sum $\sum\limits^\infty_{i=1} (q_i-q_{i-1})^2$ is finite?","Here is a question from my undergraduate days which I never knew the answer to. I just want to know if anyone can offer me a hint. Consider the rationals in $[0,1]$. Does there exist a (bijective) enumeration of these rationals $q_1,q_2,\ldots$ such that the sum $\sum\limits^\infty_{i=1} (q_i-q_{i-1})^2$ is finite?",,"['real-analysis', 'sequences-and-series']"
21,Computing $\lim\limits_{n\to\infty} \Big(\sum\limits_{i = 1}^n \sum\limits_{j = 1}^n \frac1{i^2+j^2}-\frac{\pi}{2} \log(n)\Big)$.,Computing .,\lim\limits_{n\to\infty} \Big(\sum\limits_{i = 1}^n \sum\limits_{j = 1}^n \frac1{i^2+j^2}-\frac{\pi}{2} \log(n)\Big),"In the chatroom we discussed about the asymptotic of $\displaystyle \sum_{i = 1}^n \sum_{j = 1}^n \frac1{i^2+j^2}$, and if we think of the inverse tangent integral, it's easy to see that $\displaystyle \sum_{i = 1}^n \sum_{j = 1}^n \frac1{i^2+j^2}\approx \operatorname{Ti}_2(n)\approx \frac{\pi}{2} \log(n)$ where I used the well-known relation $\displaystyle \operatorname{Ti}_2(x)-\operatorname{Ti}_2(1/x)=\frac{\pi}{2}\operatorname{sgn}(x) \log|x|$. At this point, @robjohn posed the following  limit $$\lim_{n\to\infty} \left(\displaystyle \sum_{i = 1}^n \sum_{j = 1}^n \frac1{i^2+j^2}-\frac{\pi}{2} \log(n)\right)$$ that looks like a pretty tough limit. Using $\coth(z)$ one can see the limit is approximately $-\dfrac{\pi^2}{12}$, but how about finding a way to precisely compute the limit?","In the chatroom we discussed about the asymptotic of $\displaystyle \sum_{i = 1}^n \sum_{j = 1}^n \frac1{i^2+j^2}$, and if we think of the inverse tangent integral, it's easy to see that $\displaystyle \sum_{i = 1}^n \sum_{j = 1}^n \frac1{i^2+j^2}\approx \operatorname{Ti}_2(n)\approx \frac{\pi}{2} \log(n)$ where I used the well-known relation $\displaystyle \operatorname{Ti}_2(x)-\operatorname{Ti}_2(1/x)=\frac{\pi}{2}\operatorname{sgn}(x) \log|x|$. At this point, @robjohn posed the following  limit $$\lim_{n\to\infty} \left(\displaystyle \sum_{i = 1}^n \sum_{j = 1}^n \frac1{i^2+j^2}-\frac{\pi}{2} \log(n)\right)$$ that looks like a pretty tough limit. Using $\coth(z)$ one can see the limit is approximately $-\dfrac{\pi^2}{12}$, but how about finding a way to precisely compute the limit?",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'limits']"
22,"When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$?","When , what are the solutions for ?",f(x+1)-f(x)=f'(x) f(x),"The question is: When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? The most obvious solution is a linear function of the form $f(x)=ax+b$. Is this the only solution? Edit I should add that $f:\mathbb R\to\mathbb R$ to the question.","The question is: When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? The most obvious solution is a linear function of the form $f(x)=ax+b$. Is this the only solution? Edit I should add that $f:\mathbb R\to\mathbb R$ to the question.",,"['real-analysis', 'ordinary-differential-equations', 'delay-differential-equations']"
23,Examples of uncountable sets with zero Lebesgue measure,Examples of uncountable sets with zero Lebesgue measure,,I would like examples of uncountable subsets of $\mathbb{R}$ that have zero Lebesgue measure and are not obtained by the Cantor set. Thanks.,I would like examples of uncountable subsets of $\mathbb{R}$ that have zero Lebesgue measure and are not obtained by the Cantor set. Thanks.,,"['real-analysis', 'analysis', 'measure-theory', 'examples-counterexamples', 'descriptive-set-theory']"
24,"Evaluation of $\int_0^{\pi/3} \ln^2\left(\frac{\sin x }{\sin (x+\pi/3)}\right)\,\mathrm{d}x$",Evaluation of,"\int_0^{\pi/3} \ln^2\left(\frac{\sin x }{\sin (x+\pi/3)}\right)\,\mathrm{d}x","I plan to evaluate $$\int_0^{\pi/3} \ln^2\left(\frac{\sin x }{\sin (x+\pi/3)}\right)\, \mathrm{d}x$$ and I need a starting point for both real and complex methods. Thanks ! Sis.","I plan to evaluate $$\int_0^{\pi/3} \ln^2\left(\frac{\sin x }{\sin (x+\pi/3)}\right)\, \mathrm{d}x$$ and I need a starting point for both real and complex methods. Thanks ! Sis.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'residue-calculus']"
25,Dog bone-shaped curve: $|x|^x=|y|^y$,Dog bone-shaped curve:,|x|^x=|y|^y,"EDITED: Some of the questions are ansered, some aren't. EDITED: In order not to make this post too long, I posted another post which consists of more questions. Let $f$ be (almost) the implicit curve $$|x|^x=|y|^y$$ See the graph of the particularly interesting dog bone-shaped curve below Obviously, the graph should contains the line $x=y$ . However, what I want $f$ to be, is, the graph $|x|^x=|y|^y$ , without line $x=y$ , but nevertheless contains the two points which the curve intersect with $x=y$ . Note that the convention $0^0=1$ , maybe not as usaul. Clearly $f$ is not a function. I doesn't have any idea about continuity, derivative, integral, and many other important technique, apply on an implicit curve. However, I does want to ask: $1$ ) How to write $f$ in an rigorous way? (How to represents 'the intersection of the curve with $x=y$ , as the line $x=y$ is originally contains in the curve?) $2$ ) Is $f$ continuous?(What does it means to be a continuous implicit curve? Is it smooth?) $3$ ) How can we break it down into several functions so that we can use the properties of functions? $4$ )[ANSWERED] Can we find the area (maybe not 'integral', since in definition to Riemann-integral, integral under x-axis is negative, but clearly area can't be negative) surrounding by the curve $f$ ? Or does it make any sense to say the area of an implicit curve? Is there a strict definition? From wikipedia , maybe we need to show that it is Jordan curve? (Thanks, Peter Heinig, in the comment. Answer : By Barry Cipra, the integral is $3.527\dots$ . $5$ ) Given a straight line, at most how many points can the line intersects $f$ ? I believe the answer is 4, as an example, $y=-x-1$ . Moreover, what if the line is in the form $y=a$ ,while $a\in\mathbb R$ ? I believe is 2. See the image below, for some example $6$ ) Where are the extrema of $f$ , a) in the normal sense, b) when we take $x=y$ as the $x$ -axis, c) when we take $x=-y$ as the $x$ -axis? $7$ ) What is the intersections of the curve with, a) $x=y$ , (ANSWERED by Rahul) b) $x=-y$ ,  (ANSWERED) If $x>0$ and $x=ây$ , then we get $x^x=x^{âx}$ , $x^{2x}=1$ , $x=1$ , $y=â1$ , and similarly for $x<0$ we get $x=â1,y=1$ . This answers 7b. â Wojowu c) $x$ -axis? (ANSWERED) $8$ ) a) What is the circumscribed circle to the curve $f$ ? (I think that it may be $x^2+y^2=2$ ) b) What is the inscribed circle to the curve $f$ ? $\space\space\space$ (I observed by trial and error that it may be $x^2+y^2=a$ $\space\space\space$ while $a\approx 0.27067056\approx\frac{1352}{4995}$ , see the image below) 9)[ANSWERED] Perhaps we should restrict our attention to $x^x=y^y$ , consider the part of $f$ which is in the first quadrant (including positive x and y axes),see the image below, the red curve. How can we find the integral of it on [0,1](How can we write it in a more convenient form?) I observed that the integral should be $\gt 1-\frac{\pi}{4}$ (See the image below) and $\lt \frac{1}{2}$ , perhaps, $\frac{e}{10}$ ?. Answer : By Yuriy S, the integral is $0.317\dots$ . 10)Is the locus, when $f$ is rotated around the origin, the area between the two concentric circles $x^2+y^2=2$ and $x^2+y^2=\frac{2}{e^2}$ ? See the image and here . The equations of the graph: Using rotation matrix, as mentioned in Yuriy S's answer, Define a paremeter $a$ , as the rotation degree measures in radian (clockwise), $f$ , after rotation, is $$\left|x\cdot\cos a-y\sin a\right|^{x\cdot\cos a-y\sin a}=\left|x\sin a+y\cos a\right|^{x\sin a+y\cos a}$$ For $$\space x\cdot\cos a-y\sin a\neq x\sin a+y\cos a$$ Of course, there are two points more. Does it helps to prove that $f$ is continuous, i.e., question 2 above? NOTE: To interpret the graph with Desmos, we need to consider two cases, namely, $\space x\cdot\cos a-y\sin a<x\sin a+y\cos a$ and $\space x\cdot\cos a-y\sin a> x\sin a+y\cos a$ respectively, see the link above. I need really to apologize that I really doesn't have any idea to answer. I am not presently familar with implicit curve, but only a little knowledge about real analysis. However, I ask it because I think it is interesting. Moreover, it is good for an easy answer, but it is not neccesary that the answer should be very elementary, since knowing how the question can be solve by wonderful mathematical techniques is itself interesting. Welcome for any answer, even not as a complete answer to all 10 questions. Thanks for answering my naive questions! Sorry for a new question, but I really like to ask :)","EDITED: Some of the questions are ansered, some aren't. EDITED: In order not to make this post too long, I posted another post which consists of more questions. Let be (almost) the implicit curve See the graph of the particularly interesting dog bone-shaped curve below Obviously, the graph should contains the line . However, what I want to be, is, the graph , without line , but nevertheless contains the two points which the curve intersect with . Note that the convention , maybe not as usaul. Clearly is not a function. I doesn't have any idea about continuity, derivative, integral, and many other important technique, apply on an implicit curve. However, I does want to ask: ) How to write in an rigorous way? (How to represents 'the intersection of the curve with , as the line is originally contains in the curve?) ) Is continuous?(What does it means to be a continuous implicit curve? Is it smooth?) ) How can we break it down into several functions so that we can use the properties of functions? )[ANSWERED] Can we find the area (maybe not 'integral', since in definition to Riemann-integral, integral under x-axis is negative, but clearly area can't be negative) surrounding by the curve ? Or does it make any sense to say the area of an implicit curve? Is there a strict definition? From wikipedia , maybe we need to show that it is Jordan curve? (Thanks, Peter Heinig, in the comment. Answer : By Barry Cipra, the integral is . ) Given a straight line, at most how many points can the line intersects ? I believe the answer is 4, as an example, . Moreover, what if the line is in the form ,while ? I believe is 2. See the image below, for some example ) Where are the extrema of , a) in the normal sense, b) when we take as the -axis, c) when we take as the -axis? ) What is the intersections of the curve with, a) , (ANSWERED by Rahul) b) ,  (ANSWERED) If and , then we get , , , , and similarly for we get . This answers 7b. â Wojowu c) -axis? (ANSWERED) ) a) What is the circumscribed circle to the curve ? (I think that it may be ) b) What is the inscribed circle to the curve ? (I observed by trial and error that it may be while , see the image below) 9)[ANSWERED] Perhaps we should restrict our attention to , consider the part of which is in the first quadrant (including positive x and y axes),see the image below, the red curve. How can we find the integral of it on [0,1](How can we write it in a more convenient form?) I observed that the integral should be (See the image below) and , perhaps, ?. Answer : By Yuriy S, the integral is . 10)Is the locus, when is rotated around the origin, the area between the two concentric circles and ? See the image and here . The equations of the graph: Using rotation matrix, as mentioned in Yuriy S's answer, Define a paremeter , as the rotation degree measures in radian (clockwise), , after rotation, is For Of course, there are two points more. Does it helps to prove that is continuous, i.e., question 2 above? NOTE: To interpret the graph with Desmos, we need to consider two cases, namely, and respectively, see the link above. I need really to apologize that I really doesn't have any idea to answer. I am not presently familar with implicit curve, but only a little knowledge about real analysis. However, I ask it because I think it is interesting. Moreover, it is good for an easy answer, but it is not neccesary that the answer should be very elementary, since knowing how the question can be solve by wonderful mathematical techniques is itself interesting. Welcome for any answer, even not as a complete answer to all 10 questions. Thanks for answering my naive questions! Sorry for a new question, but I really like to ask :)","f |x|^x=|y|^y x=y f |x|^x=|y|^y x=y x=y 0^0=1 f 1 f x=y x=y 2 f 3 4 f 3.527\dots 5 f y=-x-1 y=a a\in\mathbb R 6 f x=y x x=-y x 7 x=y x=-y x>0 x=ây x^x=x^{âx} x^{2x}=1 x=1 y=â1 x<0 x=â1,y=1 x 8 f x^2+y^2=2 f \space\space\space x^2+y^2=a \space\space\space a\approx 0.27067056\approx\frac{1352}{4995} x^x=y^y f \gt 1-\frac{\pi}{4} \lt \frac{1}{2} \frac{e}{10} 0.317\dots f x^2+y^2=2 x^2+y^2=\frac{2}{e^2} a f \left|x\cdot\cos a-y\sin a\right|^{x\cdot\cos a-y\sin a}=\left|x\sin a+y\cos a\right|^{x\sin a+y\cos a} \space x\cdot\cos a-y\sin a\neq x\sin a+y\cos a f \space x\cdot\cos a-y\sin a<x\sin a+y\cos a \space x\cdot\cos a-y\sin a> x\sin a+y\cos a","['real-analysis', 'functions', 'analytic-geometry', 'graphing-functions', 'implicit-function']"
26,Infimum and supremum of the empty set,Infimum and supremum of the empty set,,"Let $E$ be an empty set. Then, $\sup(E) = -\infty$ and $\inf(E)=+\infty$. I thought it is only meaningful to talk about $\inf(E)$ and $\sup(E)$ if $E$ is non-empty and bounded? Thank you.","Let $E$ be an empty set. Then, $\sup(E) = -\infty$ and $\inf(E)=+\infty$. I thought it is only meaningful to talk about $\inf(E)$ and $\sup(E)$ if $E$ is non-empty and bounded? Thank you.",,"['real-analysis', 'order-theory']"
27,The monster continued fraction,The monster continued fraction,,"My title may have come off as informal or nonspecific. But, in fact, my title could not be more specific. Define a sequence with initial term: $$S_0=1+\frac{1}{1+\frac{1}{1+\frac1{\ddots}}}$$ It is well-known that $S_0=\frac{1+\sqrt{5}}{2}$ . I do not write it as such because, in order to describe the next term, we must look at its continued fraction representation. Take every $1$ not a numerator and replace it with $\frac1{1+\frac1{\ddots}}$ to get: $$S_1=\frac1{1+\frac1{1+\frac1{\ddots}}} +\frac{1}{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}}$$ Repeat to get: $$S_2=\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac{1}{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\ddots}}}$$ One more time, for good measure: $$S_3=\frac{1}{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\ddots}}}+\frac1{\frac{1}{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\ddots}}}+\frac1{\frac{1}{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\ddots}}}+\frac1{\ddots}}}$$ We buzz like fat bees to the question: $$\text{What is } \lim_{n \to \infty}S_n\text{?}$$ My almost-solution: Define a recursive sequence as follows: $$\eta_0 = 1$$ $$\eta_k = \frac1{\eta_{k-1}+\frac1{\eta_{k-1}+\frac1\ddots}}$$ Notice: $$\eta_k = \frac1{\eta_{k-1}+\eta_k} \implies \eta_k\eta_{k-1}+\eta_k^2-1=0 \implies \eta_k=\frac{-\eta_{k-1}+\sqrt{\eta_{k-1}^2+4}}2$$ We force the square root in the numerator to be positive for obvious reasons. Now, notice: $$S_0 = \eta_0+\eta_1 = \varphi$$ $$S_1 = S_0-\eta_0+\eta_2=\eta_1+\eta_2$$ $$S_2 = S_1-\eta_1+\eta_3 = \eta_2 + \eta_3$$ $$S_3 = S_2-\eta_2+\eta_4 = \eta_3 + \eta_4$$ $$\vdots$$ $$S_n = S_{n-1} - \eta_{n-1} + \eta_{n+1} = \eta_n + \eta_{n+1}$$ By how I defined $\eta_i$ , we may rewrite the RHS as: $$S_n = \eta_n + \frac{-\eta_n+\sqrt{\eta_n^2+4}}2 \implies S_n = \frac{\eta_n+\sqrt{\eta_n^2+4}}2$$ This may also be obtained by the equivalence (which I noticed visually): $$\eta_k = \frac1{S_{k-1}}$$ For $k \geq 1$ . I omit the proof of this, although it is simple. With this in mind, we see: $$S_k = \frac{\eta_k+\sqrt{\eta_k^2+4}}2 \implies 2S_k = \eta_k+\sqrt{\eta_k^2+4}$$ $$\implies 2S_k = \frac1{\eta_{k-1}+\eta_k} + \sqrt{\frac1{(\eta_{k-1}+\eta_k)^2 }+4}$$ $$\implies 2S_k = \frac1{\frac1{S_{k-2}} + \frac1{S_{k-1}}} + \sqrt{\frac1{(\frac1{S_{k-2}} + \frac1{S_{k-1}})^2 }+4}$$ Which, as $k \to \infty$ , becomes the sum of an infinitely nested fraction and the square root of the sum of the reciprocal of a square of an infinitely nested fraction and $4$ . In theory, this can be evaluated. But I draw my final breath and collapse, dead. More seriously, what I have done is defined $S_n$ recursively, with initial values $S_0 = \varphi$ and $S_1 = \varphi + \frac1{\varphi+\frac1{\ddots}}$ . But I don't know if this converges. An inverse symbolic calculator might find something. Edit (Solution): it converges to $\sqrt{2}$ .","My title may have come off as informal or nonspecific. But, in fact, my title could not be more specific. Define a sequence with initial term: It is well-known that . I do not write it as such because, in order to describe the next term, we must look at its continued fraction representation. Take every not a numerator and replace it with to get: Repeat to get: One more time, for good measure: We buzz like fat bees to the question: My almost-solution: Define a recursive sequence as follows: Notice: We force the square root in the numerator to be positive for obvious reasons. Now, notice: By how I defined , we may rewrite the RHS as: This may also be obtained by the equivalence (which I noticed visually): For . I omit the proof of this, although it is simple. With this in mind, we see: Which, as , becomes the sum of an infinitely nested fraction and the square root of the sum of the reciprocal of a square of an infinitely nested fraction and . In theory, this can be evaluated. But I draw my final breath and collapse, dead. More seriously, what I have done is defined recursively, with initial values and . But I don't know if this converges. An inverse symbolic calculator might find something. Edit (Solution): it converges to .",S_0=1+\frac{1}{1+\frac{1}{1+\frac1{\ddots}}} S_0=\frac{1+\sqrt{5}}{2} 1 \frac1{1+\frac1{\ddots}} S_1=\frac1{1+\frac1{1+\frac1{\ddots}}} +\frac{1}{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}} S_2=\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac{1}{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\ddots}}} S_3=\frac{1}{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\ddots}}}+\frac1{\frac{1}{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\ddots}}}+\frac1{\frac{1}{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\frac1{\frac1{1+\frac1{1+\frac1{\ddots}}}+\frac1\ddots}+\frac1{\ddots}}}+\frac1{\ddots}}} \text{What is } \lim_{n \to \infty}S_n\text{?} \eta_0 = 1 \eta_k = \frac1{\eta_{k-1}+\frac1{\eta_{k-1}+\frac1\ddots}} \eta_k = \frac1{\eta_{k-1}+\eta_k} \implies \eta_k\eta_{k-1}+\eta_k^2-1=0 \implies \eta_k=\frac{-\eta_{k-1}+\sqrt{\eta_{k-1}^2+4}}2 S_0 = \eta_0+\eta_1 = \varphi S_1 = S_0-\eta_0+\eta_2=\eta_1+\eta_2 S_2 = S_1-\eta_1+\eta_3 = \eta_2 + \eta_3 S_3 = S_2-\eta_2+\eta_4 = \eta_3 + \eta_4 \vdots S_n = S_{n-1} - \eta_{n-1} + \eta_{n+1} = \eta_n + \eta_{n+1} \eta_i S_n = \eta_n + \frac{-\eta_n+\sqrt{\eta_n^2+4}}2 \implies S_n = \frac{\eta_n+\sqrt{\eta_n^2+4}}2 \eta_k = \frac1{S_{k-1}} k \geq 1 S_k = \frac{\eta_k+\sqrt{\eta_k^2+4}}2 \implies 2S_k = \eta_k+\sqrt{\eta_k^2+4} \implies 2S_k = \frac1{\eta_{k-1}+\eta_k} + \sqrt{\frac1{(\eta_{k-1}+\eta_k)^2 }+4} \implies 2S_k = \frac1{\frac1{S_{k-2}} + \frac1{S_{k-1}}} + \sqrt{\frac1{(\frac1{S_{k-2}} + \frac1{S_{k-1}})^2 }+4} k \to \infty 4 S_n S_0 = \varphi S_1 = \varphi + \frac1{\varphi+\frac1{\ddots}} \sqrt{2},"['real-analysis', 'limits', 'convergence-divergence', 'recurrence-relations', 'continued-fractions']"
28,Prove that $\sum_{k=1}^{\infty} \large\frac{k}{\text{e}^{2\pi k}-1}=\frac{1}{24}-\frac{1}{8\pi}$,Prove that,\sum_{k=1}^{\infty} \large\frac{k}{\text{e}^{2\pi k}-1}=\frac{1}{24}-\frac{1}{8\pi},Prove that  $$\sum_{k=1}^{\infty} \frac{k}{\text{e}^{2\pi k}-1}=\frac{1}{24}-\frac{1}{8\pi}$$,Prove that  $$\sum_{k=1}^{\infty} \frac{k}{\text{e}^{2\pi k}-1}=\frac{1}{24}-\frac{1}{8\pi}$$,,"['calculus', 'real-analysis', 'sequences-and-series']"
29,"What are ways to compute polynomials that converge from above and below to a continuous and bounded function on $[0,1]$?",What are ways to compute polynomials that converge from above and below to a continuous and bounded function on ?,"[0,1]","Main Question Suppose $f:[0,1]\to [0,1]$ is continuous and belongs to a large class of functions (for example, the $k$ -th derivative, $k\ge 0$ , is continuous, Lipschitz continuous, concave, strictly increasing, bounded variation, and/or in the Zygmund class, or $f$ is real analytic). Then, compute the Bernstein coefficients of a sequence of polynomials ( $g_n$ ) of degree 2, 4, 8, ..., $2^i$ , ... that converge to $f$ from below and satisfy: $(g_{2n}-g_{n})$ is a polynomial with non-negative Bernstein coefficients once it's rewritten to a polynomial in Bernstein form of degree exactly $2n$ . Assume $0\lt f(\lambda)\lt 1$ or $f$ is polynomially bounded. The convergence rate must be $O(1/n^{r/2})$ if the class has only functions with Lipschitz-continuous $(r-1)$ -th derivative.  The method may not introduce transcendental or trigonometric functions (as with Chebyshev interpolants). See ""Strategies"", below, for different ways to answer this question. In this question, a polynomial $P(x)$ is written in Bernstein form of degree $n$ if it is written asâ $$P(x)=\sum_{k=0}^n a_k {n \choose k} x^k (1-x)^{n-k},$$ where $a_0, ..., a_n$ are the polynomial's Bernstein coefficients . The degree- $n$ Bernstein polynomial of an arbitrary function $f(x)$ has Bernstein coefficients $a_k = f(k/n)$ .  In general, this Bernstein polynomial differs from $f$ even if $f$ is a polynomial. Background I asked this question in order to solve the so-called Bernoulli factory problem , described next. We're given a coin that shows heads with an unknown probability, $\lambda$ . The goal is to use that coin (and possibly also a fair coin) to build a ""new"" coin that shows heads with a probability that depends on $\lambda$ , call it $f(\lambda)$ . This is the Bernoulli factory problem , and it can be solved only if $f$ is continuous (Keane and O'Brien 1994). However, since I asked this question I have found a Bernoulli factory algorithm that I believe is general enough to cover all the cases that this question would help solve. Since this question may be of broader interest, though, I leave this question open.  See also my other open questions about the Bernoulli factory problem. Polynomials that approach a factory function An algorithm simulates a factory function $f(\lambda)$ via two sequences of polynomials that converge from above and below to that function.  To use the algorithm, however, the polynomial sequences must meet certain requirements; among them, the sequences must be of Bernstein-form polynomials that converge from above and below to a factory function.  Specifically: For $f(\lambda)$ there must be a sequence of polynomials ( $g_n$ ) in Bernstein form of degree 1, 2, 3, ... that converge to $f$ from below and satisfy: $(g_{n+1}-g_{n})$ is a polynomial with non-negative Bernstein coefficients once it's rewritten to a polynomial in Bernstein form of degree exactly $n+1$ ( see end notes ; Nacu and Peres 2005; Holtz et al. 2011).  For $f(\lambda)=1-f(\lambda)$ there must likewise be a sequence of this kind. A Matter of Efficiency However, ordinary Bernstein polynomials converge to a function at the rate $\Omega(1/n)$ in general, a result known since Voronovskaya (1932) and a rate that will lead to an infinite expected number of coin flips in general .  (See also my supplemental notes .) But Lorentz (1966) showed that if the function is positive and has a continuous $k$ -th derivative, there are polynomials with nonnegative Bernstein coefficients that converge at the rate $O(1/n^{k/2})$ (and thus can enable a finite expected number of coin flips if the function is ""smooth"" enough). Thus, people have developed alternatives, including linear combinations and iterated Boolean sums of Bernstein polynomials, to improve the convergence rate. These include Micchelli (1973), Guan (2009), GÃ¼ntÃ¼rk and Li (2021a, 2021b), the ""Lorentz operator"" in Holtz et al. (2011), Draganov (2014), and Tachev (2022). These alternative polynomials usually include results where the error bound is the desired $O(1/n^{k/2})$ , but nearly all those results (e.g., Theorem 4.4 in Micchelli; Theorem 5 in GÃ¼ntÃ¼rk and Li) have hidden constants with no upper bounds given, making them unimplementable (that is, it can't be known beforehand whether a given polynomial will come close to the target function within a user-specified error tolerance). (See end notes.) A Conjecture on Polynomial Approximation The following is a conjecture that could help reduce this problem to the problem of finding explicit error bounds when approximating a function by polynomials. Let $f(\lambda):[0,1]\to(0,1)$ have $r\ge 1$ continuous derivatives, let $M$ be the maximum of the absolute value of $f$ and its derivatives up to the $r$ -th derivative, and denote the Bernstein polynomial of degree $n$ of a function $g$ as $B_n(g)$ . Let $W_{2^0}(\lambda), W_{2^1}(\lambda), ..., W_{2^i}(\lambda),...$ be a sequence of functions on [0, 1] that converge uniformly to $f$ . For each integer $n\ge 1$ that's a power of 2, suppose that there is $D>0$ such thatâ $$|f(\lambda)-B_n(W_n(\lambda))| \le DM/n^{r/2},$$ whenever $0\le \lambda\le 1$ .  Then there is $C_0\ge D$ such that for every $C\ge C_0$ , the polynomials $(g_n)$ in Bernstein form of degree 2, 4, 8, ..., $2^i$ , ..., defined as $g_n=B_n(W_n(\lambda) - CM/n^{r/2})$ , converge from below to $f$ and satisfy: $(g_{2n}-g_{n})$ is a polynomial with non-negative Bernstein coefficients once it's rewritten to a polynomial in Bernstein form of degree exactly $2n$ . (See end notes.) Equivalently (see also Nacu and Peres 2005), there is $C_1>0$ such that the inequality $(PB)$ (see below) holds true for each integer $n\ge 1$ that's a power of 2 (see ""Strategies"", below). My goal is to see not just whether this conjecture is true, but also which value of $C_0$ (or $C_1$ ) suffices for the conjecture, especially for any combination of the special cases mentioned at the end of "" Main Question "", above. Strategies The following are some strategies for answering these questions: For iterated Boolean sums of Bernstein polynomials ( $U_{n,k}$ in Micchelli 1973 ; see also GÃ¼ntÃ¼rk and Li ), find an explicit bound, with no hidden constants, on the approximation error for functions with continuous $r$ -th derivative, or verify my proof of those error bounds . For linear combinations of Bernstein polynomials (Butzer 1953, Tachev 2022 ), verify my proof of those error bounds in my Proposition B10 . For the "" Lorentz operator "", find an explicit bound, with no hidden constants, on the approximation error for the operator $Q_{n,r}$ and for the polynomials $(f_n)$ and $(g_n)$ formed with it, and find the hidden constants $\theta_\alpha$ , $s$ , and $D$ as well as those in Lemmas 15, 17 to 22, and 24 in the paper.  Or verify my proof of the order-2 operator's error bounds in my Proposition B10A . Find other polynomial operators meeting the requirements of the main question (see ""Main Question"", above) and having explicit error bounds, with no hidden constants, especially operators that preserve polynomials of a higher degree than linear functions. Find a sequence of functions $(W_n(f))$ and an explicit and tight upper bound on $C_1>0$ such that, for each integer $n\ge 1$ that's a power of 2â $$\left|\left(\sum_{i=0}^k W_n\left(\frac{i}{n}\right)\sigma_{n,k,i}\right)-W_{2n}\left(\frac{k}{2n}\right)\right|=|\mathbb{E}[W_n(X_k/n)] - W_{2n}(\mathbb{E}[X_k/n])|\le \frac{C_1 M}{n^{r/2}},\tag{PB}$$ whenever $0\le k\le 2n$ , where $M = \max(L, \max|f^{(0)}|, ...,\max|f^{(r-1)}|)$ , $L$ is $\max|f^{(r)}|$ or the Lipschitz constant of $f^{(r-1)}$ , $X_k$ is a hypergeometric( $2n$ , $k$ , $n$ ) random variable, and $\sigma_{n,k,i} = {n\choose i}{n\choose {k-i}}/{2n \choose k}=\mathbb{P}(X_k=i)$ is the probability that $X_k$ equals $i$ . ( See end notes as well as "" Proofs for Polynomial-Building Schemes . ) References ÅatuszyÅski, K., Kosmidis, I., Papaspiliopoulos, O., Roberts, G.O., ""Simulating events of unknown probabilities via reverse time martingales"", arXiv:0907.4018v2 [stat.CO], 2009/2011. Keane, M. S., and O'Brien, G. L., ""A Bernoulli factory"", ACM Transactions on Modeling and Computer Simulation 4(2), 1994. Holtz, O., Nazarov, F., Peres, Y., ""New Coins from Old, Smoothly"", Constructive Approximation 33 (2011). Nacu, Åerban, and Yuval Peres. ""Fast simulation of new coins from old"", The Annals of Applied Probability 15, no. 1A (2005): 93-115. Micchelli, C. (1973). The saturation class and iterates of the Bernstein polynomials. Journal of Approximation Theory, 8(1), 1-18. Guan, Zhong. "" Iterated Bernstein polynomial approximations ."" arXiv preprint arXiv:0909.0684 (2009). GÃ¼ntÃ¼rk, C. Sinan, and Weilin Li. "" Approximation with one-bit polynomials in Bernstein form "" arXiv preprint arXiv:2112.09183 (2021). C.S. GÃ¼ntÃ¼rk, W. Li, "" Approximation of functions with one-bit neural networks "", arXiv:2112.09181 [cs.LG], 2021. Draganov, Borislav R. ""On simultaneous approximation by iterated Boolean sums of Bernstein operators."" Results in Mathematics 66, no. 1 (2014): 21-41. Farouki, R.T., and Rajan, V.T., ""Algorithms for polynomials in Bernstein form"", Computer Aided Geometric Design 5(1), 1988. Tachev, Gancho. "" Linear combinations of two Bernstein polynomials "", Mathematical Foundations of Computing, 2022. Lee, Sang Kyu, Jae Ho Chang, and Hyoung-Moon Kim. ""Further sharpening of Jensen's inequality."" Statistics 55, no. 5 (2021): 1154-1168. Butzer, P.L., ""Linear combinations of Bernstein polynomials"", Canadian Journal of Mathematics 15 (1953). Note 5 : This condition is also known as a ""consistency requirement""; it ensures that not only the polynomials ""increase"" to $f(\lambda)$ , but also their Bernstein coefficients do as well.  This condition is equivalent in practice to the following statement (Nacu & Peres 2005). For every integer $n\ge 1$ that's a power of 2, $a(2n, k)\ge\mathbb{E}[a(n, X_{n,k})]= \left(\sum_{i=0}^k a(n,i) {n\choose i}{n\choose {k-i}}/{2n\choose k}\right)$ , where $a(n,k)$ is the degree- $n$ polynomial's $k$ -th Bernstein coefficient, where $0\le k\le 2n$ is an integer, and where $X_{n,k}$ is a hypergeometric( $2n$ , $k$ , $n$ ) random variable.  A hypergeometric( $2n$ , $k$ , $n$ ) random variable is the number of ""good"" balls out of $n$ balls taken uniformly at random, all at once, from a bag containing $2n$ balls, $k$ of which are ""good"".  See also my MathOverflow question on finding bounds for hypergeometric variables. Note 6 : If $W_n(0)=f(0)$ and $W_n(1)=f(1)$ for every $n$ , then the inequality $(PB)$ is automatically true when $k=0$ and $k=2n$ , so that the statement has to be checked only for $0\lt k\lt 2n$ .  If, in addition, $W_n$ is symmetric about 1/2, so that $W_n(\lambda)=W_n(1-\lambda)$ whenever $0\le \lambda\le 1$ , then the statement has to be checked only for $0\lt k\le n$ (since the values $\sigma_{n,k,i} = {n\choose i}{n\choose {k-i}}/{2n \choose k}$ are symmetric in that they satisfy $\sigma_{n,k,i}=\sigma_{n,k,k-i}$ ). This question is a problem of finding the Jensen gap of $W_n$ for certain kinds of hypergeometric random variables ( see Note 5 ).  Lee et al. (2021) deal with a problem very similar to this one and find results that take advantage of $f$ 's (here, $W_n$ 's) smoothness, but unfortunately assume the variable is supported on an open interval, rather than a closed one (namely $[0,1]$ ) as in this question.","Main Question Suppose is continuous and belongs to a large class of functions (for example, the -th derivative, , is continuous, Lipschitz continuous, concave, strictly increasing, bounded variation, and/or in the Zygmund class, or is real analytic). Then, compute the Bernstein coefficients of a sequence of polynomials ( ) of degree 2, 4, 8, ..., , ... that converge to from below and satisfy: is a polynomial with non-negative Bernstein coefficients once it's rewritten to a polynomial in Bernstein form of degree exactly . Assume or is polynomially bounded. The convergence rate must be if the class has only functions with Lipschitz-continuous -th derivative.  The method may not introduce transcendental or trigonometric functions (as with Chebyshev interpolants). See ""Strategies"", below, for different ways to answer this question. In this question, a polynomial is written in Bernstein form of degree if it is written asâ where are the polynomial's Bernstein coefficients . The degree- Bernstein polynomial of an arbitrary function has Bernstein coefficients .  In general, this Bernstein polynomial differs from even if is a polynomial. Background I asked this question in order to solve the so-called Bernoulli factory problem , described next. We're given a coin that shows heads with an unknown probability, . The goal is to use that coin (and possibly also a fair coin) to build a ""new"" coin that shows heads with a probability that depends on , call it . This is the Bernoulli factory problem , and it can be solved only if is continuous (Keane and O'Brien 1994). However, since I asked this question I have found a Bernoulli factory algorithm that I believe is general enough to cover all the cases that this question would help solve. Since this question may be of broader interest, though, I leave this question open.  See also my other open questions about the Bernoulli factory problem. Polynomials that approach a factory function An algorithm simulates a factory function via two sequences of polynomials that converge from above and below to that function.  To use the algorithm, however, the polynomial sequences must meet certain requirements; among them, the sequences must be of Bernstein-form polynomials that converge from above and below to a factory function.  Specifically: For there must be a sequence of polynomials ( ) in Bernstein form of degree 1, 2, 3, ... that converge to from below and satisfy: is a polynomial with non-negative Bernstein coefficients once it's rewritten to a polynomial in Bernstein form of degree exactly ( see end notes ; Nacu and Peres 2005; Holtz et al. 2011).  For there must likewise be a sequence of this kind. A Matter of Efficiency However, ordinary Bernstein polynomials converge to a function at the rate in general, a result known since Voronovskaya (1932) and a rate that will lead to an infinite expected number of coin flips in general .  (See also my supplemental notes .) But Lorentz (1966) showed that if the function is positive and has a continuous -th derivative, there are polynomials with nonnegative Bernstein coefficients that converge at the rate (and thus can enable a finite expected number of coin flips if the function is ""smooth"" enough). Thus, people have developed alternatives, including linear combinations and iterated Boolean sums of Bernstein polynomials, to improve the convergence rate. These include Micchelli (1973), Guan (2009), GÃ¼ntÃ¼rk and Li (2021a, 2021b), the ""Lorentz operator"" in Holtz et al. (2011), Draganov (2014), and Tachev (2022). These alternative polynomials usually include results where the error bound is the desired , but nearly all those results (e.g., Theorem 4.4 in Micchelli; Theorem 5 in GÃ¼ntÃ¼rk and Li) have hidden constants with no upper bounds given, making them unimplementable (that is, it can't be known beforehand whether a given polynomial will come close to the target function within a user-specified error tolerance). (See end notes.) A Conjecture on Polynomial Approximation The following is a conjecture that could help reduce this problem to the problem of finding explicit error bounds when approximating a function by polynomials. Let have continuous derivatives, let be the maximum of the absolute value of and its derivatives up to the -th derivative, and denote the Bernstein polynomial of degree of a function as . Let be a sequence of functions on [0, 1] that converge uniformly to . For each integer that's a power of 2, suppose that there is such thatâ whenever .  Then there is such that for every , the polynomials in Bernstein form of degree 2, 4, 8, ..., , ..., defined as , converge from below to and satisfy: is a polynomial with non-negative Bernstein coefficients once it's rewritten to a polynomial in Bernstein form of degree exactly . (See end notes.) Equivalently (see also Nacu and Peres 2005), there is such that the inequality (see below) holds true for each integer that's a power of 2 (see ""Strategies"", below). My goal is to see not just whether this conjecture is true, but also which value of (or ) suffices for the conjecture, especially for any combination of the special cases mentioned at the end of "" Main Question "", above. Strategies The following are some strategies for answering these questions: For iterated Boolean sums of Bernstein polynomials ( in Micchelli 1973 ; see also GÃ¼ntÃ¼rk and Li ), find an explicit bound, with no hidden constants, on the approximation error for functions with continuous -th derivative, or verify my proof of those error bounds . For linear combinations of Bernstein polynomials (Butzer 1953, Tachev 2022 ), verify my proof of those error bounds in my Proposition B10 . For the "" Lorentz operator "", find an explicit bound, with no hidden constants, on the approximation error for the operator and for the polynomials and formed with it, and find the hidden constants , , and as well as those in Lemmas 15, 17 to 22, and 24 in the paper.  Or verify my proof of the order-2 operator's error bounds in my Proposition B10A . Find other polynomial operators meeting the requirements of the main question (see ""Main Question"", above) and having explicit error bounds, with no hidden constants, especially operators that preserve polynomials of a higher degree than linear functions. Find a sequence of functions and an explicit and tight upper bound on such that, for each integer that's a power of 2â whenever , where , is or the Lipschitz constant of , is a hypergeometric( , , ) random variable, and is the probability that equals . ( See end notes as well as "" Proofs for Polynomial-Building Schemes . ) References ÅatuszyÅski, K., Kosmidis, I., Papaspiliopoulos, O., Roberts, G.O., ""Simulating events of unknown probabilities via reverse time martingales"", arXiv:0907.4018v2 [stat.CO], 2009/2011. Keane, M. S., and O'Brien, G. L., ""A Bernoulli factory"", ACM Transactions on Modeling and Computer Simulation 4(2), 1994. Holtz, O., Nazarov, F., Peres, Y., ""New Coins from Old, Smoothly"", Constructive Approximation 33 (2011). Nacu, Åerban, and Yuval Peres. ""Fast simulation of new coins from old"", The Annals of Applied Probability 15, no. 1A (2005): 93-115. Micchelli, C. (1973). The saturation class and iterates of the Bernstein polynomials. Journal of Approximation Theory, 8(1), 1-18. Guan, Zhong. "" Iterated Bernstein polynomial approximations ."" arXiv preprint arXiv:0909.0684 (2009). GÃ¼ntÃ¼rk, C. Sinan, and Weilin Li. "" Approximation with one-bit polynomials in Bernstein form "" arXiv preprint arXiv:2112.09183 (2021). C.S. GÃ¼ntÃ¼rk, W. Li, "" Approximation of functions with one-bit neural networks "", arXiv:2112.09181 [cs.LG], 2021. Draganov, Borislav R. ""On simultaneous approximation by iterated Boolean sums of Bernstein operators."" Results in Mathematics 66, no. 1 (2014): 21-41. Farouki, R.T., and Rajan, V.T., ""Algorithms for polynomials in Bernstein form"", Computer Aided Geometric Design 5(1), 1988. Tachev, Gancho. "" Linear combinations of two Bernstein polynomials "", Mathematical Foundations of Computing, 2022. Lee, Sang Kyu, Jae Ho Chang, and Hyoung-Moon Kim. ""Further sharpening of Jensen's inequality."" Statistics 55, no. 5 (2021): 1154-1168. Butzer, P.L., ""Linear combinations of Bernstein polynomials"", Canadian Journal of Mathematics 15 (1953). Note 5 : This condition is also known as a ""consistency requirement""; it ensures that not only the polynomials ""increase"" to , but also their Bernstein coefficients do as well.  This condition is equivalent in practice to the following statement (Nacu & Peres 2005). For every integer that's a power of 2, , where is the degree- polynomial's -th Bernstein coefficient, where is an integer, and where is a hypergeometric( , , ) random variable.  A hypergeometric( , , ) random variable is the number of ""good"" balls out of balls taken uniformly at random, all at once, from a bag containing balls, of which are ""good"".  See also my MathOverflow question on finding bounds for hypergeometric variables. Note 6 : If and for every , then the inequality is automatically true when and , so that the statement has to be checked only for .  If, in addition, is symmetric about 1/2, so that whenever , then the statement has to be checked only for (since the values are symmetric in that they satisfy ). This question is a problem of finding the Jensen gap of for certain kinds of hypergeometric random variables ( see Note 5 ).  Lee et al. (2021) deal with a problem very similar to this one and find results that take advantage of 's (here, 's) smoothness, but unfortunately assume the variable is supported on an open interval, rather than a closed one (namely ) as in this question.","f:[0,1]\to [0,1] k k\ge 0 f g_n 2^i f (g_{2n}-g_{n}) 2n 0\lt f(\lambda)\lt 1 f O(1/n^{r/2}) (r-1) P(x) n P(x)=\sum_{k=0}^n a_k {n \choose k} x^k (1-x)^{n-k}, a_0, ..., a_n n f(x) a_k = f(k/n) f f \lambda \lambda f(\lambda) f f(\lambda) f(\lambda) g_n f (g_{n+1}-g_{n}) n+1 f(\lambda)=1-f(\lambda) \Omega(1/n) k O(1/n^{k/2}) O(1/n^{k/2}) f(\lambda):[0,1]\to(0,1) r\ge 1 M f r n g B_n(g) W_{2^0}(\lambda), W_{2^1}(\lambda), ..., W_{2^i}(\lambda),... f n\ge 1 D>0 |f(\lambda)-B_n(W_n(\lambda))| \le DM/n^{r/2}, 0\le \lambda\le 1 C_0\ge D C\ge C_0 (g_n) 2^i g_n=B_n(W_n(\lambda) - CM/n^{r/2}) f (g_{2n}-g_{n}) 2n C_1>0 (PB) n\ge 1 C_0 C_1 U_{n,k} r Q_{n,r} (f_n) (g_n) \theta_\alpha s D (W_n(f)) C_1>0 n\ge 1 \left|\left(\sum_{i=0}^k W_n\left(\frac{i}{n}\right)\sigma_{n,k,i}\right)-W_{2n}\left(\frac{k}{2n}\right)\right|=|\mathbb{E}[W_n(X_k/n)] - W_{2n}(\mathbb{E}[X_k/n])|\le \frac{C_1 M}{n^{r/2}},\tag{PB} 0\le k\le 2n M = \max(L, \max|f^{(0)}|, ...,\max|f^{(r-1)}|) L \max|f^{(r)}| f^{(r-1)} X_k 2n k n \sigma_{n,k,i} = {n\choose i}{n\choose {k-i}}/{2n \choose k}=\mathbb{P}(X_k=i) X_k i f(\lambda) n\ge 1 a(2n, k)\ge\mathbb{E}[a(n, X_{n,k})]= \left(\sum_{i=0}^k a(n,i) {n\choose i}{n\choose {k-i}}/{2n\choose k}\right) a(n,k) n k 0\le k\le 2n X_{n,k} 2n k n 2n k n n 2n k W_n(0)=f(0) W_n(1)=f(1) n (PB) k=0 k=2n 0\lt k\lt 2n W_n W_n(\lambda)=W_n(1-\lambda) 0\le \lambda\le 1 0\lt k\le n \sigma_{n,k,i} = {n\choose i}{n\choose {k-i}}/{2n \choose k} \sigma_{n,k,i}=\sigma_{n,k,k-i} W_n f W_n [0,1]","['real-analysis', 'probability', 'polynomials', 'uniform-convergence', 'approximation-theory']"
30,What is the geometrical difference between continuity and uniform continuity?,What is the geometrical difference between continuity and uniform continuity?,,Can we explain  between ordinary continuity and Uniform Continuity difference geometrically ? What is the best way to describe the difference between these two concepts? Where the motivation of Uniform Continuity came from? Thank You.,Can we explain  between ordinary continuity and Uniform Continuity difference geometrically ? What is the best way to describe the difference between these two concepts? Where the motivation of Uniform Continuity came from? Thank You.,,"['real-analysis', 'continuity', 'uniform-continuity']"
31,Find $\lim_{n \to \infty}\frac{\sin 1+2\sin \frac{1}{2}+\cdots+n\sin \frac{1}{n}}{n}$ [closed],Find  [closed],\lim_{n \to \infty}\frac{\sin 1+2\sin \frac{1}{2}+\cdots+n\sin \frac{1}{n}}{n},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Find $$\lim_{n \to \infty}\frac{\sin 1+2\sin \frac{1}{2}+\cdots+n\sin \frac{1}{n}}{n}.$$ This is a recent exam question, which I couldn't figure out in the exam. My guess is it doesn't exist.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Find $$\lim_{n \to \infty}\frac{\sin 1+2\sin \frac{1}{2}+\cdots+n\sin \frac{1}{n}}{n}.$$ This is a recent exam question, which I couldn't figure out in the exam. My guess is it doesn't exist.",,['real-analysis']
32,How to prove $\sum_{n=0}^{\infty} \frac{n^2}{2^n} = 6$?,How to prove ?,\sum_{n=0}^{\infty} \frac{n^2}{2^n} = 6,I'd like to find out why \begin{align} \sum_{n=0}^{\infty} \frac{n^2}{2^n} = 6 \end{align} I tried to rewrite it into a geometric series \begin{align} \sum_{n=0}^{\infty} \frac{n^2}{2^n} = \sum_{n=0}^{\infty} \Big(\frac{1}{2}\Big)^nn^2 \end{align} But I don't know what to do with the $n^2$.,I'd like to find out why \begin{align} \sum_{n=0}^{\infty} \frac{n^2}{2^n} = 6 \end{align} I tried to rewrite it into a geometric series \begin{align} \sum_{n=0}^{\infty} \frac{n^2}{2^n} = \sum_{n=0}^{\infty} \Big(\frac{1}{2}\Big)^nn^2 \end{align} But I don't know what to do with the $n^2$.,,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis']"
33,Proof that the real numbers are countable: Help with why this is wrong,Proof that the real numbers are countable: Help with why this is wrong,,"I was just thinking about this recently, and I thought of a possible bijection between the natural numbers and the real numbers.  First, take the numbers between zero and one, exclusive.  The following sequence of real numbers is suggested so that we have bijection. 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.01, 0.02, ... , 0.09, 0.10, 0.11, ... , 0.99, 0.001, 0.002, ... , 0.999, 0.0001, etc. Obviously, this includes repeats, but this set is countable.  Therefore, the set of all numbers between zero and one is a subset of the above countable set, and is thus countable.  Then we simply extend this to all real numbers and all the whole numbers themselves, and since the real numbers, as demonstrated above, between any two whole numbers is countable, the real numbers are the union of countably many countable sets, and thus the real numbers are countable. Please help me with this.  I understand the diagonalization argument by Cantor, but I am curious specifically about this proof which I thought of and its strengths and flaws. Thanks.","I was just thinking about this recently, and I thought of a possible bijection between the natural numbers and the real numbers.  First, take the numbers between zero and one, exclusive.  The following sequence of real numbers is suggested so that we have bijection. 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.01, 0.02, ... , 0.09, 0.10, 0.11, ... , 0.99, 0.001, 0.002, ... , 0.999, 0.0001, etc. Obviously, this includes repeats, but this set is countable.  Therefore, the set of all numbers between zero and one is a subset of the above countable set, and is thus countable.  Then we simply extend this to all real numbers and all the whole numbers themselves, and since the real numbers, as demonstrated above, between any two whole numbers is countable, the real numbers are the union of countably many countable sets, and thus the real numbers are countable. Please help me with this.  I understand the diagonalization argument by Cantor, but I am curious specifically about this proof which I thought of and its strengths and flaws. Thanks.",,"['real-analysis', 'elementary-set-theory', 'decimal-expansion', 'fake-proofs']"
34,Prove $\int_0^{\infty}\! \frac{\mathbb{d}x}{1+x^n}=\frac{\pi}{n \sin\frac{\pi}{n}}$ using real analysis techniques only,Prove  using real analysis techniques only,\int_0^{\infty}\! \frac{\mathbb{d}x}{1+x^n}=\frac{\pi}{n \sin\frac{\pi}{n}},"I have found a proof using complex analysis techniques (contour integral, residue theorem, etc.) that shows $$\int_0^{\infty}\! \frac{\mathbb{d}x}{1+x^n}=\frac{\pi}{n \sin\frac{\pi}{n}}$$ for $n\in \mathbb{N}^+\setminus\{1\}$ I wonder if it is possible by using only real analysis to demonstrate this ""innocent"" result? Edit A more general result showing that $$\int\limits_{0}^{\infty} \frac{x^{a-1}}{1+x^{b}} \ \text{dx} = \frac{\pi}{b \sin(\pi{a}/b)}, \qquad 0 < a <b$$ can be found in another math.SE post","I have found a proof using complex analysis techniques (contour integral, residue theorem, etc.) that shows for I wonder if it is possible by using only real analysis to demonstrate this ""innocent"" result? Edit A more general result showing that can be found in another math.SE post","\int_0^{\infty}\! \frac{\mathbb{d}x}{1+x^n}=\frac{\pi}{n \sin\frac{\pi}{n}} n\in \mathbb{N}^+\setminus\{1\} \int\limits_{0}^{\infty} \frac{x^{a-1}}{1+x^{b}} \ \text{dx} = \frac{\pi}{b \sin(\pi{a}/b)}, \qquad 0 < a <b","['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
35,Proofs of the Cauchy-Schwarz Inequality?,Proofs of the Cauchy-Schwarz Inequality?,,How many proofs of the Cauchy-Schwarz inequality are there? Is there some kind of reference that lists all of these proofs?,How many proofs of the Cauchy-Schwarz inequality are there? Is there some kind of reference that lists all of these proofs?,,"['real-analysis', 'inequality', 'big-list', 'inner-products']"
36,Why does the Dedekind Cut work well enough to define the Reals?,Why does the Dedekind Cut work well enough to define the Reals?,,"I am a seventeen year old high school student and I was studying some Real Analysis on my own. In the process, I encountered the Dedekind Cut being used to construct the Reals.  I just can't get the hang of the definition that $\mathbb R =\{ \alpha \mid \alpha \text{ is a cut} \}$. Why does this work? Why is this a good definition for the reals? What got me really thinking was the fact that cuts are subsets of $\mathbb Q$. Why are they used to construct the reals? Also, to define the reals, we are to consider cuts at exactly the ""real"" points on the number line, right? If we don't know what they are (the Reals) how can we find a cut corresponding to a real (at the ""rational"" points there seems to be no problem) and how can I Prove that they are unique. Maybe my naive thoughts are hindering my progress but I just can't seem to understand the cuts being used here. So could you please elaborate your answers a bit more than necessary so that I can get the concept right. Any help is much appreciated! Thanks in advance.","I am a seventeen year old high school student and I was studying some Real Analysis on my own. In the process, I encountered the Dedekind Cut being used to construct the Reals.  I just can't get the hang of the definition that $\mathbb R =\{ \alpha \mid \alpha \text{ is a cut} \}$. Why does this work? Why is this a good definition for the reals? What got me really thinking was the fact that cuts are subsets of $\mathbb Q$. Why are they used to construct the reals? Also, to define the reals, we are to consider cuts at exactly the ""real"" points on the number line, right? If we don't know what they are (the Reals) how can we find a cut corresponding to a real (at the ""rational"" points there seems to be no problem) and how can I Prove that they are unique. Maybe my naive thoughts are hindering my progress but I just can't seem to understand the cuts being used here. So could you please elaborate your answers a bit more than necessary so that I can get the concept right. Any help is much appreciated! Thanks in advance.",,"['real-analysis', 'elementary-set-theory']"
37,Integral Inequality Absolute Value: $\left| \int_{a}^{b} f(x) g(x) \ dx \right| \leq \int_{a}^{b} |f(x)|\cdot |g(x)| \ dx$,Integral Inequality Absolute Value:,\left| \int_{a}^{b} f(x) g(x) \ dx \right| \leq \int_{a}^{b} |f(x)|\cdot |g(x)| \ dx,Suppose we are given the following: $$\left| \int_{a}^{b} f(x) g(x) \ dx \right| \leq \int_{a}^{b} |f(x)|\cdot |g(x)| \ dx$$ How would we prove this? Does this follow from Cauchy Schwarz? Intuitively this is how I see it: In the LHS we could have a negative area that reduces the positive area. In the RHS the area can only increase because we take the absolute values of the functions first.,Suppose we are given the following: $$\left| \int_{a}^{b} f(x) g(x) \ dx \right| \leq \int_{a}^{b} |f(x)|\cdot |g(x)| \ dx$$ How would we prove this? Does this follow from Cauchy Schwarz? Intuitively this is how I see it: In the LHS we could have a negative area that reduces the positive area. In the RHS the area can only increase because we take the absolute values of the functions first.,,"['real-analysis', 'integration', 'inequality', 'absolute-value', 'integral-inequality']"
38,Prove the convergence/divergence of $\sum \limits_{k=1}^{\infty} \frac{\tan(k)}{k}$,Prove the convergence/divergence of,\sum \limits_{k=1}^{\infty} \frac{\tan(k)}{k},Can be easily proved that the following series onverges/diverges? $$\sum_{k=1}^{\infty} \frac{\tan(k)}{k}$$ I'd really appreciate your support on this problem. I'm looking for some easy proof here. Thanks.,Can be easily proved that the following series onverges/diverges? $$\sum_{k=1}^{\infty} \frac{\tan(k)}{k}$$ I'd really appreciate your support on this problem. I'm looking for some easy proof here. Thanks.,,"['real-analysis', 'sequences-and-series']"
39,"Is the theory of dual numbers strong enough to develop real analysis, and does it resemble Newton's historical method for doing calculus?","Is the theory of dual numbers strong enough to develop real analysis, and does it resemble Newton's historical method for doing calculus?",,"I've been interested in non-standard analysis recently. I was reading up on it and noticed the following interesting comment on the Wikipedia page about hyperreal numbers , right after giving an example of a nonstandard differentiation: The use of the standard part in the definition of the derivative is a rigorous alternative to the traditional practice of neglecting the square of an infinitesimal quantity... the typical method from Newton through the 19th century would have been simply to discard the $dx^2$ term. I've never heard anything like this before, and really find it fascinating that Newton's method was to define the relation $dx^2 = 0$. If we actually formalize the above structure by taking $\mathbb{R}$ and adjoining an element $dx^2 = 0$ to it, we get the ""dual numbers,"" isomorphic to the quotient ring $\mathbb{R}[x]/x^2$. I'd seen some things about how this algebra plays into automated differentiation algorithms for some computer software systems, but I've never heard anything about Newton directly working in this algebra. So I have a few questions: Does anyone have more historical information on the way that Newton performed differentiation, and its relation to the dual numbers? Does anyone know how effectively real analysis can be formalized with the dual numbers? Does the resulting system play nice enough to develop all of the important modern results? If we start with $\mathbb{C}[x]/x^2$ instead, can we likewise develop complex analysis? Since this idea is so simple, I'm very curious how powerful it is. I'm also curious if it has any major drawbacks too, since I'm not sure why anyone would mess with the foundational baggage involved in defining the hyperreals if this simple 2-dimensional real algebra could really do the trick.","I've been interested in non-standard analysis recently. I was reading up on it and noticed the following interesting comment on the Wikipedia page about hyperreal numbers , right after giving an example of a nonstandard differentiation: The use of the standard part in the definition of the derivative is a rigorous alternative to the traditional practice of neglecting the square of an infinitesimal quantity... the typical method from Newton through the 19th century would have been simply to discard the $dx^2$ term. I've never heard anything like this before, and really find it fascinating that Newton's method was to define the relation $dx^2 = 0$. If we actually formalize the above structure by taking $\mathbb{R}$ and adjoining an element $dx^2 = 0$ to it, we get the ""dual numbers,"" isomorphic to the quotient ring $\mathbb{R}[x]/x^2$. I'd seen some things about how this algebra plays into automated differentiation algorithms for some computer software systems, but I've never heard anything about Newton directly working in this algebra. So I have a few questions: Does anyone have more historical information on the way that Newton performed differentiation, and its relation to the dual numbers? Does anyone know how effectively real analysis can be formalized with the dual numbers? Does the resulting system play nice enough to develop all of the important modern results? If we start with $\mathbb{C}[x]/x^2$ instead, can we likewise develop complex analysis? Since this idea is so simple, I'm very curious how powerful it is. I'm also curious if it has any major drawbacks too, since I'm not sure why anyone would mess with the foundational baggage involved in defining the hyperreals if this simple 2-dimensional real algebra could really do the trick.",,"['calculus', 'real-analysis', 'abstract-algebra', 'math-history', 'nonstandard-analysis']"
40,Prove that $\int_0^1 \left| \frac{f^{''}(x)}{f(x)} \right| dx \geq4$.,Prove that .,\int_0^1 \left| \frac{f^{''}(x)}{f(x)} \right| dx \geq4,"Let $f(x) \in C^2[0,1]$, with $f(0)=f(1)=0$, and $f(x)\neq 0$ when $x \in(0,1)$. Show that $$\int_0^1 \left| \frac{f''(x)}{f(x)} \right| dx \geq 4.$$","Let $f(x) \in C^2[0,1]$, with $f(0)=f(1)=0$, and $f(x)\neq 0$ when $x \in(0,1)$. Show that $$\int_0^1 \left| \frac{f''(x)}{f(x)} \right| dx \geq 4.$$",,"['calculus', 'real-analysis']"
41,"Prime powers, patterns similar to $\lbrace 0,1,0,2,0,1,0,3\ldots \rbrace$ and formulas for $\sigma_k(n)$","Prime powers, patterns similar to  and formulas for","\lbrace 0,1,0,2,0,1,0,3\ldots \rbrace \sigma_k(n)","Some time ago when decomponsing the natural numbers, $\mathbb{N}$, in prime powes I noticed a pattern in their powers.  Taking, for example, the numbers $\lbrace 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16 \rbrace$ and factorize them, we will get  $$\begin{align} 1&=2^0\times 3^0\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 2&=2^1\times 3^0\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\3&=2^0\times 3^1\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 4&=2^2\times 3^0\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 5&=2^0\times 3^0\times 5^1\times 7^0\times 11^0\times 13^0\times\ldots \\ 6&=2^1\times 3^1\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 7&=2^0\times 3^0\times 5^0\times 7^1\times 11^0\times 13^0\times\ldots \\ 8&=2^3\times 3^0\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 9&=2^0\times 3^2\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 10&=2^1\times 3^0\times 5^1\times 7^0\times 11^0\times 13^0\times\ldots \\ 11&=2^0\times 3^0\times 5^0\times 7^0\times 11^1\times 13^0\times\ldots \\ 12&=2^2\times 3^1\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 13&=2^0\times 3^0\times 5^0\times 7^0\times 11^0\times 13^1\times\ldots \\ 14&=2^1\times 3^0\times 5^0\times 7^1\times 11^0\times 13^0\times\ldots \\ 15&=2^0\times 3^1\times 5^1\times 7^0\times 11^0\times 13^0\times\ldots \\ 16&=2^4\times 3^0\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\\end{align}$$ Now if we look at the powers of $2$ we will notice that they are $$\lbrace f_2(n)\rbrace=\lbrace 0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4\rbrace$$ and for the powers of $3$ we have $$\lbrace f_3(n)\rbrace=\lbrace 0,0,1,0,0,1,0,0,2,0,0,1,0,0,1,0\rbrace$$ This, of course, is a well known fact. Since then I wondered if there was a formula for $f_2(n)$ or $f_3(n)$ or $f_p(n)$, with $p\in \mathbb{P}$. It seemed impossible but I was able to devise the suitable formulas. They are $$\displaystyle\begin{align} f_2(n)=\sum_{r=1}^{\infty}\frac{r}{{2^{r+1}}}\sum_{k=0}^{2^{r+1}-1}\cos\left( \frac{2k\pi(n+2^{r})}{2^{r+1}} \right)\end{align}$$ and for the general case we have $$\displaystyle f_p(n)=\sum_{r=1}^{\infty}\frac{r}{p^{r+1}}\sum_{j=1}^{p-1}\left(\sum_{k=0}^{p^{r+1}-1}\cos\left( \frac{2k\pi(n+(p-j)p^{r})}{p^{r+1}} \right)\right)$$ If one cares to analyse the formula for $f_p(n)$ it can be concluded that it needs not to be restricted to the prime numbers, so that we have $f_m(n), m \in \mathbb{N}$ and similar  patterns for $\lbrace f_m(n)\rbrace$ will result. Now, the wonderfull thing is that we can express the arithmetical divisor functions $\sigma_k(n)$ in terms of $f_m(n)$ as follows $$\displaystyle \sigma_a(n)=1+\sum_{m=2}^{\infty}\sum_{r=1}^{\infty}\frac{m^{a}}{m^{r+1}}\sum_{j=1}^{m-1}\left(\sum_{k=0}^{m^{r+1}-1}\cos\left( \frac{2k\pi(n+(m-j)m^{r})}{m^{r+1}} \right)\right)$$ And, if we consider the divisor summatory function , $D(n)$, as $$D(n)=\sum_{m \leq n}d(m)$$ with $$d(n)=\sigma_{0}(n)=\sum_{d|n}1$$ we can express $D(n)$ as  $$D(n)=\sum_{m=2}^{\infty}\sum_{r=1}^{\infty}\frac{r}{m^{r+1}}\sum_{j=1}^{m-1}\left(\sum_{k=0}^{m^{r+1}-1}\cos\left( \frac{2k\pi(2^{n}+(m-j)m^{r})}{m^{r+1}} \right)\right)$$ Now, we know that, $d(n)$ and $D(n)$ are related to the Riemann zeta-function by $$\zeta^{2}(z)=\sum_{n=1}^{\infty}\frac{d(n)}{n^{z}}$$ and  $$\zeta^{2}(z)=z\int_{1}^{\infty}\frac{D(x)}{x^{z+1}}dx$$ Now, my questions What can we say about the convergence of $f_m(z)$, $\sigma_a(z)$ and $D(z)$ with $z \in \mathbb{C}$? We can see that they converge for $z \in \mathbb{N}$. I think that $\sigma_a(z)$ and $D(z)$ are only curiosities and aren't interesting in the context of the Riemann zeta-function because they are hard to compute. What do you think? Are formulas $f_m(z)$, $\sigma_a(z)$ and $D(z)$ original. I think they are. I'd like to know if anyone has found something like this before. I've posted this as an answer to this post sometime ago. Finally, is this intereting enough to publish somewhere? I'm just an amateur... To conclude I'd like to apologise for presenting all this formulas without showing how I got them but you can consider this previous post of mine and the question Greatest power of two dividing an integer , Difficult Infinite Sum and On the 61-st, the 62-nd, and the 63-rd Smarandache's problem page 38. And now a challenge, can you present a formula for the characteristic function of the prime numbers ? EDIT: I'm answering my challenge and leaving another. Considering that the characteristic function of the primes , $u(n)$, is given by $$ \begin{equation} u(n)=\begin{cases} &1\;\;\;\text{ if } n \in \mathbb{P} \\ &\\ &\\ &0\;\;\;\text{ if } n \notin \mathbb{P} \end{cases} \end{equation} $$ I have found that $u(n)$ is given by the following formula $$ \begin{equation} u(n)=\prod_{m=2}^{\infty}\;\;\prod _{r=1}^{\infty} \left\{1-\frac{1}{m^{r+1}} \sum _{j=1}^{m-1}\;\;\;\sum _{k=0}^{m^{r+1}-1} \cos\left(2 k \pi \cdot\frac{n-m+(m-j) m^r }{m^{r+1}}\right)\right\} \end{equation} $$ Now, in the same spirit, what is formula for the prime counting function , $\pi(x)$?","Some time ago when decomponsing the natural numbers, $\mathbb{N}$, in prime powes I noticed a pattern in their powers.  Taking, for example, the numbers $\lbrace 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16 \rbrace$ and factorize them, we will get  $$\begin{align} 1&=2^0\times 3^0\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 2&=2^1\times 3^0\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\3&=2^0\times 3^1\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 4&=2^2\times 3^0\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 5&=2^0\times 3^0\times 5^1\times 7^0\times 11^0\times 13^0\times\ldots \\ 6&=2^1\times 3^1\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 7&=2^0\times 3^0\times 5^0\times 7^1\times 11^0\times 13^0\times\ldots \\ 8&=2^3\times 3^0\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 9&=2^0\times 3^2\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 10&=2^1\times 3^0\times 5^1\times 7^0\times 11^0\times 13^0\times\ldots \\ 11&=2^0\times 3^0\times 5^0\times 7^0\times 11^1\times 13^0\times\ldots \\ 12&=2^2\times 3^1\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\ 13&=2^0\times 3^0\times 5^0\times 7^0\times 11^0\times 13^1\times\ldots \\ 14&=2^1\times 3^0\times 5^0\times 7^1\times 11^0\times 13^0\times\ldots \\ 15&=2^0\times 3^1\times 5^1\times 7^0\times 11^0\times 13^0\times\ldots \\ 16&=2^4\times 3^0\times 5^0\times 7^0\times 11^0\times 13^0\times\ldots \\\end{align}$$ Now if we look at the powers of $2$ we will notice that they are $$\lbrace f_2(n)\rbrace=\lbrace 0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4\rbrace$$ and for the powers of $3$ we have $$\lbrace f_3(n)\rbrace=\lbrace 0,0,1,0,0,1,0,0,2,0,0,1,0,0,1,0\rbrace$$ This, of course, is a well known fact. Since then I wondered if there was a formula for $f_2(n)$ or $f_3(n)$ or $f_p(n)$, with $p\in \mathbb{P}$. It seemed impossible but I was able to devise the suitable formulas. They are $$\displaystyle\begin{align} f_2(n)=\sum_{r=1}^{\infty}\frac{r}{{2^{r+1}}}\sum_{k=0}^{2^{r+1}-1}\cos\left( \frac{2k\pi(n+2^{r})}{2^{r+1}} \right)\end{align}$$ and for the general case we have $$\displaystyle f_p(n)=\sum_{r=1}^{\infty}\frac{r}{p^{r+1}}\sum_{j=1}^{p-1}\left(\sum_{k=0}^{p^{r+1}-1}\cos\left( \frac{2k\pi(n+(p-j)p^{r})}{p^{r+1}} \right)\right)$$ If one cares to analyse the formula for $f_p(n)$ it can be concluded that it needs not to be restricted to the prime numbers, so that we have $f_m(n), m \in \mathbb{N}$ and similar  patterns for $\lbrace f_m(n)\rbrace$ will result. Now, the wonderfull thing is that we can express the arithmetical divisor functions $\sigma_k(n)$ in terms of $f_m(n)$ as follows $$\displaystyle \sigma_a(n)=1+\sum_{m=2}^{\infty}\sum_{r=1}^{\infty}\frac{m^{a}}{m^{r+1}}\sum_{j=1}^{m-1}\left(\sum_{k=0}^{m^{r+1}-1}\cos\left( \frac{2k\pi(n+(m-j)m^{r})}{m^{r+1}} \right)\right)$$ And, if we consider the divisor summatory function , $D(n)$, as $$D(n)=\sum_{m \leq n}d(m)$$ with $$d(n)=\sigma_{0}(n)=\sum_{d|n}1$$ we can express $D(n)$ as  $$D(n)=\sum_{m=2}^{\infty}\sum_{r=1}^{\infty}\frac{r}{m^{r+1}}\sum_{j=1}^{m-1}\left(\sum_{k=0}^{m^{r+1}-1}\cos\left( \frac{2k\pi(2^{n}+(m-j)m^{r})}{m^{r+1}} \right)\right)$$ Now, we know that, $d(n)$ and $D(n)$ are related to the Riemann zeta-function by $$\zeta^{2}(z)=\sum_{n=1}^{\infty}\frac{d(n)}{n^{z}}$$ and  $$\zeta^{2}(z)=z\int_{1}^{\infty}\frac{D(x)}{x^{z+1}}dx$$ Now, my questions What can we say about the convergence of $f_m(z)$, $\sigma_a(z)$ and $D(z)$ with $z \in \mathbb{C}$? We can see that they converge for $z \in \mathbb{N}$. I think that $\sigma_a(z)$ and $D(z)$ are only curiosities and aren't interesting in the context of the Riemann zeta-function because they are hard to compute. What do you think? Are formulas $f_m(z)$, $\sigma_a(z)$ and $D(z)$ original. I think they are. I'd like to know if anyone has found something like this before. I've posted this as an answer to this post sometime ago. Finally, is this intereting enough to publish somewhere? I'm just an amateur... To conclude I'd like to apologise for presenting all this formulas without showing how I got them but you can consider this previous post of mine and the question Greatest power of two dividing an integer , Difficult Infinite Sum and On the 61-st, the 62-nd, and the 63-rd Smarandache's problem page 38. And now a challenge, can you present a formula for the characteristic function of the prime numbers ? EDIT: I'm answering my challenge and leaving another. Considering that the characteristic function of the primes , $u(n)$, is given by $$ \begin{equation} u(n)=\begin{cases} &1\;\;\;\text{ if } n \in \mathbb{P} \\ &\\ &\\ &0\;\;\;\text{ if } n \notin \mathbb{P} \end{cases} \end{equation} $$ I have found that $u(n)$ is given by the following formula $$ \begin{equation} u(n)=\prod_{m=2}^{\infty}\;\;\prod _{r=1}^{\infty} \left\{1-\frac{1}{m^{r+1}} \sum _{j=1}^{m-1}\;\;\;\sum _{k=0}^{m^{r+1}-1} \cos\left(2 k \pi \cdot\frac{n-m+(m-j) m^r }{m^{r+1}}\right)\right\} \end{equation} $$ Now, in the same spirit, what is formula for the prime counting function , $\pi(x)$?",,"['real-analysis', 'number-theory', 'prime-numbers', 'riemann-zeta', 'arithmetic-functions']"
42,Is it possible to construct a strictly monotonic sequence of all rational numbers?,Is it possible to construct a strictly monotonic sequence of all rational numbers?,,"I know that the set of all rational numbers is countable, and can be enumerated by a sequence, say $\{a_n\}$.  But can we construct a monotonic $\{a_n\}_{n=1}^{\infty}$, e.g. with $a_k<a_{k+1}$?  It doesn't seem plausible to me, because then $a_1$ would be the smallest rational number, which clearly can't be any finite number.  Am I mistaken?","I know that the set of all rational numbers is countable, and can be enumerated by a sequence, say $\{a_n\}$.  But can we construct a monotonic $\{a_n\}_{n=1}^{\infty}$, e.g. with $a_k<a_{k+1}$?  It doesn't seem plausible to me, because then $a_1$ would be the smallest rational number, which clearly can't be any finite number.  Am I mistaken?",,"['real-analysis', 'sequences-and-series', 'rational-numbers']"
43,A strange integral having to do with the sophomore's dream:,A strange integral having to do with the sophomore's dream:,,"I recently noticed that this really weird equation actually carries a closed form! $$\int_0^1 \left(\frac{x^x}{(1-x)^{1-x}}-\frac{(1-x)^{1-x}}{x^x}\right)\text{d}x=0$$ I honestly do not know how to prove this amazing result! I do not know nearly enough about the sophomore's dream integral properties to answer this question, which I have been trying to apply here. (If possible, please stay with real methods, as I do not know contour integration yet)","I recently noticed that this really weird equation actually carries a closed form! $$\int_0^1 \left(\frac{x^x}{(1-x)^{1-x}}-\frac{(1-x)^{1-x}}{x^x}\right)\text{d}x=0$$ I honestly do not know how to prove this amazing result! I do not know nearly enough about the sophomore's dream integral properties to answer this question, which I have been trying to apply here. (If possible, please stay with real methods, as I do not know contour integration yet)",,"['calculus', 'real-analysis']"
44,Why doesn't a Taylor series converge always?,Why doesn't a Taylor series converge always?,,The Taylor expansion itself can be derived from mean value theorems which themselves are valid over the entire domain of the function. Then why doesn't the Taylor series converge over the entire domain? I understand the part about the convergence of infinite series and the various tests. But I seem to be missing something very fundamental here..,The Taylor expansion itself can be derived from mean value theorems which themselves are valid over the entire domain of the function. Then why doesn't the Taylor series converge over the entire domain? I understand the part about the convergence of infinite series and the various tests. But I seem to be missing something very fundamental here..,,"['real-analysis', 'soft-question', 'taylor-expansion']"
45,Prove: Convergent sequences are bounded,Prove: Convergent sequences are bounded,,"I don't understand this one part in the proof for convergent sequences are bounded. Proof: Let $s_n$ be a convergent sequence, and let $\lim s_n = s$ . Then taking $\epsilon = 1$ we have: $n > N \implies |s_n - s| < 1$ From the triangle inequality we see that: $ n > N \implies|s_n| - |s| < 1 \iff |s_n| < |s| + 1$ . Define $M= \max\{|s|+1, |s_1|, |s_2|, ..., |s_N|\}$ . Then we have $|s_n| \leq M$ for all $n \in N$ . I do not understand the defining $M$ part. Why not just take $|s| + 1$ as the bound, since for $n > N \implies |s_n| < |s| + 1$ ?","I don't understand this one part in the proof for convergent sequences are bounded. Proof: Let be a convergent sequence, and let . Then taking we have: From the triangle inequality we see that: . Define . Then we have for all . I do not understand the defining part. Why not just take as the bound, since for ?","s_n \lim s_n = s \epsilon = 1 n > N \implies |s_n - s| < 1  n > N \implies|s_n| - |s| < 1 \iff |s_n| < |s| + 1 M= \max\{|s|+1, |s_1|, |s_2|, ..., |s_N|\} |s_n| \leq M n \in N M |s| + 1 n > N \implies |s_n| < |s| + 1","['real-analysis', 'sequences-and-series']"
46,Can you take the derivative of a function at infinity?,Can you take the derivative of a function at infinity?,,"Exactly the title: can you take the derivative of a function at infinity? I asked my maths teacher, and while she thought it was an original question, she didn't know the answer, and I couldn't find anything online about this. Maybe this is just me completely misunderstanding derivatives and functions at infinity, but to me, a high schooler, it makes sense that you can. For example, I'd imagine that a function with a horizontal asymptote would have a derivative of zero at infinity.","Exactly the title: can you take the derivative of a function at infinity? I asked my maths teacher, and while she thought it was an original question, she didn't know the answer, and I couldn't find anything online about this. Maybe this is just me completely misunderstanding derivatives and functions at infinity, but to me, a high schooler, it makes sense that you can. For example, I'd imagine that a function with a horizontal asymptote would have a derivative of zero at infinity.",,"['calculus', 'real-analysis', 'derivatives']"
47,What is an example that a function is differentiable but derivative is not Riemann integrable,What is an example that a function is differentiable but derivative is not Riemann integrable,,"I have two questions that i'm curious about. If $f$ is differentiable real function on its domain, then $f'$ is Riemann integrable. If $g$ is a real function with intermediate value property, then $g$ is Riemann integrable. Thank you in advance.","I have two questions that i'm curious about. If $f$ is differentiable real function on its domain, then $f'$ is Riemann integrable. If $g$ is a real function with intermediate value property, then $g$ is Riemann integrable. Thank you in advance.",,"['real-analysis', 'examples-counterexamples']"
48,Proving that the sequence $F_{n}(x)=\sum\limits_{k=1}^{n} \frac{\sin{kx}}{k}$ is boundedly convergent on $\mathbb{R}$,Proving that the sequence  is boundedly convergent on,F_{n}(x)=\sum\limits_{k=1}^{n} \frac{\sin{kx}}{k} \mathbb{R},"Here is an exercise, on analysis which i am stuck. How do I prove that if $F_{n}(x)=\sum\limits_{k=1}^{n} \frac{\sin{kx}}{k}$, then the sequence $\{F_{n}(x)\}$ is boundedly convergent on $\mathbb{R}$?","Here is an exercise, on analysis which i am stuck. How do I prove that if $F_{n}(x)=\sum\limits_{k=1}^{n} \frac{\sin{kx}}{k}$, then the sequence $\{F_{n}(x)\}$ is boundedly convergent on $\mathbb{R}$?",,['real-analysis']
49,Why Do We Care About HÃ¶lder Continuity?,Why Do We Care About HÃ¶lder Continuity?,,"I have often encountered HÃ¶lder continuity in books on analysis, but the books I've read tend to pass over HÃ¶lder functions quickly, without developing applications. While the definition seems natural enough, it's not clear to me what we actually gain from knowing that a function is $\alpha$ -HÃ¶lder continuous, for some $\alpha<1$ . I have some guesses, but they are just guesses: do $\alpha$ -HÃ¶lder conditions give rise to useful weak solution concepts in PDEs? Are there important results that apply only to $\alpha$ -HÃ¶lder functions, for some fixed $\alpha$ ? For $\alpha=1$ (Lipschitz continuity) the answer to both of these questions seems to be yes, but I know nothing for lower values of $\alpha$ . I'd be interested in answers that describe specific applications, as well as answers that give more of a ''big picture''.","I have often encountered HÃ¶lder continuity in books on analysis, but the books I've read tend to pass over HÃ¶lder functions quickly, without developing applications. While the definition seems natural enough, it's not clear to me what we actually gain from knowing that a function is -HÃ¶lder continuous, for some . I have some guesses, but they are just guesses: do -HÃ¶lder conditions give rise to useful weak solution concepts in PDEs? Are there important results that apply only to -HÃ¶lder functions, for some fixed ? For (Lipschitz continuity) the answer to both of these questions seems to be yes, but I know nothing for lower values of . I'd be interested in answers that describe specific applications, as well as answers that give more of a ''big picture''.",\alpha \alpha<1 \alpha \alpha \alpha \alpha=1 \alpha,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'dynamical-systems', 'holder-spaces']"
50,The definition of continuously differentiable functions,The definition of continuously differentiable functions,,"When we say $f \in C^1$ , we mean that $f$ is continuously differentiable. Isn't the continuity a redundant word? I mean, we have a theorem that says if $f$ is differentiable then it is continuous. So why in most of the textbooks they always mention them two? So these are all equivalent: $f \in C^1$ $f$ is continuously differentiable $f'$ exists","When we say , we mean that is continuously differentiable. Isn't the continuity a redundant word? I mean, we have a theorem that says if is differentiable then it is continuous. So why in most of the textbooks they always mention them two? So these are all equivalent: is continuously differentiable exists",f \in C^1 f f f \in C^1 f f',"['real-analysis', 'derivatives', 'continuity', 'definition']"
51,What is the rigorous definition of $dy$ and $dx$?,What is the rigorous definition of  and ?,dy dx,"Some background: I am a third year undergrad. I have completed two courses on Real Analysis (I have studied $\epsilon$-$\delta$ definitions of limit, continuity, differentiability, Reimann integration and basic topology of $\mathbb{R}$). This semester I have enrolled in our first differential equation course. This is not the first time I am studying DE. In high school, we studied first-order ODE: separation of variables, homogenous DEs, linear first-order DEs and Bernoulli's differential equation. There was a common theme in solution of most of these DEs, the author would somehow manipulate the DE to arrive at an equation of the following form: $f(x)dx = g(y)dy$, then, they would integrate both sides to obtain the solution.  I never understood what the equation $f(x)dx = g(y)dy$ meant, since, the only place where we were introduced to the symbols $dx$ and $dy$ was the definition of derivative: $\dfrac{dy}{dx}$ and integraition $\int f(x)dx$. Note that in these definitions, neither $dx$ nor $dy$ occurs on its own separately, they either occur with each other (in the case of derivative) or with $\int f$ (in the case of integral). As a high school student, I was never satisfied with this way of solving DEs but I soon convinced myself that this was correct because it seemed impossible to solve certain DEs without abusing this notation. And, very often, the DEs would already be in the form containing $dx$ and $dy$ separately. Now, in the current course, I was expecting that we would be taught a mathematically rigorous way of solving DEs, but I was disappointed after reading first few sections in the recommended textbooks. The books are Simmons' Differential Equations with Applications and Historical Notes and Ross' Differential Equations . In section 7, Simmons writes: The simplest of the standard types is that in which the variables are separable:   $$\dfrac{dy}{dx} = g(x)h(y)$$As we know, to solve this we have only to write it in the separated form $dy/h(y) = g(x)dx$ and integrate ... Similarly, Ross also uses $dy$ and $dx$ freely without defining what they mean. In section 2.1, Ross writes: The first-order differential equations to be studied in this chapter may be expressed in either the derivative form $$\dfrac{dy}{dx} = f(x, y)$$ or the differential form $$M(x, y)dx + N(x, y)dy = 0$$ Thus, I would like to ask the following questions: What is the rigorous definition of $dx$ and $dy$? How do we conclude from $\dfrac{dy}{dx} = k$ that $dy = k dx ?$ Since, we integrate both sides after arriving at the form $f(x)dx = g(y)dy$, does it mean that $f(x)dx$ and $g(y)dy$ are integrable (possibly, Reimann) functions? If so, how do we show that? What does Ross mean by differential form ? Wikipedia gives me something related to differential geometry and multivariable calculus, what has that got to do with ODEs? Why do all authors don't care to define $dx$? I have seen undergrad texts on number theory which begin by defining divisibility, grad texts on Fourier analysis which define what Reimann integration means. And yet, we have introductory texts on ODEs which do not have enough content on something which to me appears to be one of the most common method to solve first-order ODEs. Why is this so? Do authors assume that the reader has already studied that in an earlier course? Should I have studied this in the Real Analysis course? Finally, is there any textbook which takes an axiomatic approach to solving DEs? I understand that some authors want to spend more time discussing motivation behind DEs: their origin in the physics, their applications in economics etc. But I believe I already have enough motivation to study DEs: in the first year I had to take classical mechanics, in which I studied many types DEs, including second-order DEs for harmonic oscillator and I also had to take quantum chemistry in which I studied PDEs like the classical wave equation and Schrodinger equation. In Terry Tao's language, I believe I am past the pre-rigorous phase of DEs, and what I expect from an undergrad course is an axiomatic treatment of the subject. What is a good textbook which serves this purpose? EDIT: I did go through a similar question . But I am not looking for a way to completely do away with the so called differentials $dx$ and $dy$, because, that would make solving certain DEs very difficult. Rather, I am looking for a rigorous theory, which formalizes operations like taking $dx$ to the other side and integrating both sides .","Some background: I am a third year undergrad. I have completed two courses on Real Analysis (I have studied $\epsilon$-$\delta$ definitions of limit, continuity, differentiability, Reimann integration and basic topology of $\mathbb{R}$). This semester I have enrolled in our first differential equation course. This is not the first time I am studying DE. In high school, we studied first-order ODE: separation of variables, homogenous DEs, linear first-order DEs and Bernoulli's differential equation. There was a common theme in solution of most of these DEs, the author would somehow manipulate the DE to arrive at an equation of the following form: $f(x)dx = g(y)dy$, then, they would integrate both sides to obtain the solution.  I never understood what the equation $f(x)dx = g(y)dy$ meant, since, the only place where we were introduced to the symbols $dx$ and $dy$ was the definition of derivative: $\dfrac{dy}{dx}$ and integraition $\int f(x)dx$. Note that in these definitions, neither $dx$ nor $dy$ occurs on its own separately, they either occur with each other (in the case of derivative) or with $\int f$ (in the case of integral). As a high school student, I was never satisfied with this way of solving DEs but I soon convinced myself that this was correct because it seemed impossible to solve certain DEs without abusing this notation. And, very often, the DEs would already be in the form containing $dx$ and $dy$ separately. Now, in the current course, I was expecting that we would be taught a mathematically rigorous way of solving DEs, but I was disappointed after reading first few sections in the recommended textbooks. The books are Simmons' Differential Equations with Applications and Historical Notes and Ross' Differential Equations . In section 7, Simmons writes: The simplest of the standard types is that in which the variables are separable:   $$\dfrac{dy}{dx} = g(x)h(y)$$As we know, to solve this we have only to write it in the separated form $dy/h(y) = g(x)dx$ and integrate ... Similarly, Ross also uses $dy$ and $dx$ freely without defining what they mean. In section 2.1, Ross writes: The first-order differential equations to be studied in this chapter may be expressed in either the derivative form $$\dfrac{dy}{dx} = f(x, y)$$ or the differential form $$M(x, y)dx + N(x, y)dy = 0$$ Thus, I would like to ask the following questions: What is the rigorous definition of $dx$ and $dy$? How do we conclude from $\dfrac{dy}{dx} = k$ that $dy = k dx ?$ Since, we integrate both sides after arriving at the form $f(x)dx = g(y)dy$, does it mean that $f(x)dx$ and $g(y)dy$ are integrable (possibly, Reimann) functions? If so, how do we show that? What does Ross mean by differential form ? Wikipedia gives me something related to differential geometry and multivariable calculus, what has that got to do with ODEs? Why do all authors don't care to define $dx$? I have seen undergrad texts on number theory which begin by defining divisibility, grad texts on Fourier analysis which define what Reimann integration means. And yet, we have introductory texts on ODEs which do not have enough content on something which to me appears to be one of the most common method to solve first-order ODEs. Why is this so? Do authors assume that the reader has already studied that in an earlier course? Should I have studied this in the Real Analysis course? Finally, is there any textbook which takes an axiomatic approach to solving DEs? I understand that some authors want to spend more time discussing motivation behind DEs: their origin in the physics, their applications in economics etc. But I believe I already have enough motivation to study DEs: in the first year I had to take classical mechanics, in which I studied many types DEs, including second-order DEs for harmonic oscillator and I also had to take quantum chemistry in which I studied PDEs like the classical wave equation and Schrodinger equation. In Terry Tao's language, I believe I am past the pre-rigorous phase of DEs, and what I expect from an undergrad course is an axiomatic treatment of the subject. What is a good textbook which serves this purpose? EDIT: I did go through a similar question . But I am not looking for a way to completely do away with the so called differentials $dx$ and $dy$, because, that would make solving certain DEs very difficult. Rather, I am looking for a rigorous theory, which formalizes operations like taking $dx$ to the other side and integrating both sides .",,"['real-analysis', 'ordinary-differential-equations', 'reference-request']"
52,"Infinite tetration, convergence radius","Infinite tetration, convergence radius",,"I got this problem from my teacher as a optional challenge. I am open about this being a given problem, however it is not homework. The problem is stated as follows. Assume we have an infinite tetration as follows $$x^{x^{x^{.^{.^.}}}} \, = \, a$$ With a given $a$ find $x$ . The next part of the problem was to discuss the convergence radius of a. If a is too big or too small the tetration does not converge. Below is my humble stab at the problem. My friend said you would have to treat the tetration as a infinite series, and therefore could not perform algebraic manipulations on it before it is know whether it converges or diverges. However my attempt is to first do some algebraic steps, then discuss the convergence radius. I) Initial discussion At the start it is obvious that the tetration converges when $a=1$ (just set $x=1$ ) Now after some computer hardwork it seems that the tetration fails to converge when a is roughly larger than 3. II) Algebraic manipulation $$ x^{x^{x^{.^{.^.}}}} \, = \, a$$ This is the same as $$ x^a \, = \, a$$ $$ \log_a(x^a) \, = \, \log_a(a)$$ $$ \log_a(x) \, = \, \frac{1}{a}$$ $$ x \, = \, a^{\frac{1}{a}}$$ Now, if we let $a=2$ then $x = \sqrt{2}$ . After some more computational work, this seems to be correct, which makes me believe this formula is correct. III) Discsussion about convergence By looking at the derivative of $ \displaystyle \large a^{\frac{1}{a}} $ we see that the maxima occurs when $a=e$ , which also seems to correspond to the inital computational work. Now I think, that the minima of $\displaystyle \large a^{1/a}$ is zero by looking at its graph, studying its derivative and the endpoints. So that my ""guess"" or work shows that it converges when $$ a \in [0 \, , \,  1/e] $$ VI) My questions Can my algebraic manipulations be justified? They seem rather sketchy taking the a`th logarithm and so on . (Although they seem to ""magically"" give out the right answer) By looking at Wikipedia it seems that the tetration converge when $$ a \in \left[ 1/e \, , \, e \right] $$ This is almost what I have, why is my lower bound wrong? How can  I find the correct lower bound?","I got this problem from my teacher as a optional challenge. I am open about this being a given problem, however it is not homework. The problem is stated as follows. Assume we have an infinite tetration as follows With a given find . The next part of the problem was to discuss the convergence radius of a. If a is too big or too small the tetration does not converge. Below is my humble stab at the problem. My friend said you would have to treat the tetration as a infinite series, and therefore could not perform algebraic manipulations on it before it is know whether it converges or diverges. However my attempt is to first do some algebraic steps, then discuss the convergence radius. I) Initial discussion At the start it is obvious that the tetration converges when (just set ) Now after some computer hardwork it seems that the tetration fails to converge when a is roughly larger than 3. II) Algebraic manipulation This is the same as Now, if we let then . After some more computational work, this seems to be correct, which makes me believe this formula is correct. III) Discsussion about convergence By looking at the derivative of we see that the maxima occurs when , which also seems to correspond to the inital computational work. Now I think, that the minima of is zero by looking at its graph, studying its derivative and the endpoints. So that my ""guess"" or work shows that it converges when VI) My questions Can my algebraic manipulations be justified? They seem rather sketchy taking the a`th logarithm and so on . (Although they seem to ""magically"" give out the right answer) By looking at Wikipedia it seems that the tetration converge when This is almost what I have, why is my lower bound wrong? How can  I find the correct lower bound?","x^{x^{x^{.^{.^.}}}} \, = \, a a x a=1 x=1  x^{x^{x^{.^{.^.}}}} \, = \, a  x^a \, = \, a  \log_a(x^a) \, = \, \log_a(a)  \log_a(x) \, = \, \frac{1}{a}  x \, = \, a^{\frac{1}{a}} a=2 x = \sqrt{2}  \displaystyle \large a^{\frac{1}{a}}  a=e \displaystyle \large a^{1/a}  a \in [0 \, , \,  1/e]   a \in \left[ 1/e \, , \, e \right] ","['real-analysis', 'convergence-divergence', 'exponentiation', 'tetration']"
53,"Differential ""Freshman's dream"" for Laplacian operator.","Differential ""Freshman's dream"" for Laplacian operator.",,"Today I encountered quite an interesting phenomenon. There is an exercise in multivariable calculus that asks students to prove the identity $$ \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} = e^{-2\xi} \left( \frac{\partial^2 f}{\partial \xi^2} + \frac{\partial^2 f}{\partial \theta^2} \right), $$ where the coordinates transformation is given by $(x,y) = F(\xi,\theta) = (e^\xi \cos(\theta), e^\xi \sin(\theta))$ , assuming $f \in C^2$ . I have seen a person misunderstood the question and proved $$ \left(\frac{\partial f}{\partial x} \right)^2 + \left(\frac{\partial f}{\partial y} \right)^2 = e^{-2\xi} \left( \left(\frac{\partial f}{\partial \xi} \right)^2 + \left(\frac{\partial f}{\partial \theta} \right)^2 \right) $$ instead. To my surprise, his proof contains no mistake and the misinterpreted equation is actually true! This got me into thinking about the generalization of the above ""Freshman's dream"" for Laplacian operator: Which coordinates transformation $(x,y) = F(\xi,\theta)$ (or, equivalently, $(\xi,\theta) = G(x,y)$ )has the property that for any $f\in C^2$ , we have $$\begin{align} \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} &= h(\xi,\theta) \left( \frac{\partial^2 f}{\partial \xi^2} + \frac{\partial^2 f}{\partial \theta^2} \right) \\ &\text{if and only if} \\ \left(\frac{\partial f}{\partial x} \right)^2 + \left(\frac{\partial f}{\partial y} \right)^2 &= h(\xi,\theta) \left( \left(\frac{\partial f}{\partial \xi} \right)^2 + \left(\frac{\partial f}{\partial \theta} \right)^2 \right) \end{align}$$ on an open domain $D\subset\Bbb R^2$ for some (sufficiently smooth) function $h>0$ ? Here's my thought so far: Suppose that our coordinates transformation is given by $(\xi,\theta) = G(x,y) = (G_1(x,y),G_2(x,y))$ . By some calculation (that I shall skip), we can compute that the Laplacian $\Delta = \partial_x^2 + \partial_y^2$ in the coordinate $(\xi,\theta)$ can be written as $$ \partial_x^2 + \partial_y^2 = (\Delta G_1)\partial_\xi + (\Delta G_2)\partial_\theta + |\nabla G_1|^2 \partial_\xi^2 + |\nabla G_2|^2 \partial_\theta^2 + 2(\nabla G_1 \cdot \nabla G_2)\partial_\xi \partial_\theta, $$ hence $\partial_x^2 + \partial_y^2 = h(\partial_\xi^2 + \partial_\theta^2)$ if and only if $$ |\nabla G_1|^2 = |\nabla G_2|^2 = h, \quad \Delta G_1 = \Delta G_2 = 0, \quad \text{and}\quad \nabla G_1 \cdot \nabla G_2 = 0. $$ On the other hand, we have $$ \begin{pmatrix}\partial_x f &\partial_y f \end{pmatrix} = \begin{pmatrix}\partial_\xi f &\partial_\theta f \end{pmatrix} \begin{pmatrix}\partial_x G_1 &\partial_y G_1 \\ \partial_x G_2 &\partial_y G_2 \end{pmatrix}, $$ hence in order that $(\partial_x f)^2 + (\partial_y f)^2 = h ((\partial_\xi f)^2 + (\partial_\theta f)^2 )$ , we need the Jacobian matrix to be of the form $$ \begin{pmatrix}\partial_x G_1 &\partial_y G_1 \\ \partial_x G_2 &\partial_y G_2 \end{pmatrix} = \sqrt{h}\ M, $$ where $M$ is an orthogonal matrix at each point on $D$ . In particular, this is equivalent to $$ |\nabla G_1|^2 = |\nabla G_2|^2 = h \quad \text{and}\quad \nabla G_1 \cdot \nabla G_2 = 0. $$ It seems like the ""miracle"" we have seen earlier is a little bit less surprising than expected! From my calculation above (unless I made some mistakes), it seems like $$\begin{align} \partial_x^2 f + \partial_y^2 f &= h(\partial_\xi^2 f + \partial_\theta^2 f) \\ &\text{if and only if} \\  (\partial_x f)^2 + (\partial_y f)^2 &= h ((\partial_\xi f)^2 + (\partial_\theta f)^2 ) \ \ \text{and} \ \ \Delta G_1 = \Delta G_2 = 0,  \end{align}$$ i.e. $G$ is a harmonic function coordinate-wise. In particular, this is true for our original coordinate transformation function since we can rewrite $(x,y) = (e^\xi \cos(\theta), e^\xi \sin(\theta))$ as $(\xi,\theta) = G(x,y)$ , where $$ G(x,y) = (G_1(x,y) , G_2(x,y)) = \left( \frac12 \ln(x^2+y^2), \arctan\left( \frac{y}{x} \right) \right). $$ Here $G_1$ and $G_2$ are indeed harmonic (on an appropriate domain). I want to know if we can find more interesting examples like this? I have a feeling that this should be related to holomorphic functions and harmonic conjugate but my knowledge of complex analysis is pretty limited. Is there a general theory that would allow us to construct a coordinate transformation function $G$ satisfying the above properties?","Today I encountered quite an interesting phenomenon. There is an exercise in multivariable calculus that asks students to prove the identity where the coordinates transformation is given by , assuming . I have seen a person misunderstood the question and proved instead. To my surprise, his proof contains no mistake and the misinterpreted equation is actually true! This got me into thinking about the generalization of the above ""Freshman's dream"" for Laplacian operator: Which coordinates transformation (or, equivalently, )has the property that for any , we have on an open domain for some (sufficiently smooth) function ? Here's my thought so far: Suppose that our coordinates transformation is given by . By some calculation (that I shall skip), we can compute that the Laplacian in the coordinate can be written as hence if and only if On the other hand, we have hence in order that , we need the Jacobian matrix to be of the form where is an orthogonal matrix at each point on . In particular, this is equivalent to It seems like the ""miracle"" we have seen earlier is a little bit less surprising than expected! From my calculation above (unless I made some mistakes), it seems like i.e. is a harmonic function coordinate-wise. In particular, this is true for our original coordinate transformation function since we can rewrite as , where Here and are indeed harmonic (on an appropriate domain). I want to know if we can find more interesting examples like this? I have a feeling that this should be related to holomorphic functions and harmonic conjugate but my knowledge of complex analysis is pretty limited. Is there a general theory that would allow us to construct a coordinate transformation function satisfying the above properties?","
\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} = e^{-2\xi} \left( \frac{\partial^2 f}{\partial \xi^2} + \frac{\partial^2 f}{\partial \theta^2} \right),
 (x,y) = F(\xi,\theta) = (e^\xi \cos(\theta), e^\xi \sin(\theta)) f \in C^2 
\left(\frac{\partial f}{\partial x} \right)^2 + \left(\frac{\partial f}{\partial y} \right)^2 = e^{-2\xi} \left( \left(\frac{\partial f}{\partial \xi} \right)^2 + \left(\frac{\partial f}{\partial \theta} \right)^2 \right)
 (x,y) = F(\xi,\theta) (\xi,\theta) = G(x,y) f\in C^2 \begin{align}
\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} &= h(\xi,\theta) \left( \frac{\partial^2 f}{\partial \xi^2} + \frac{\partial^2 f}{\partial \theta^2} \right) \\
&\text{if and only if} \\
\left(\frac{\partial f}{\partial x} \right)^2 + \left(\frac{\partial f}{\partial y} \right)^2 &= h(\xi,\theta) \left( \left(\frac{\partial f}{\partial \xi} \right)^2 + \left(\frac{\partial f}{\partial \theta} \right)^2 \right)
\end{align} D\subset\Bbb R^2 h>0 (\xi,\theta) = G(x,y) = (G_1(x,y),G_2(x,y)) \Delta = \partial_x^2 + \partial_y^2 (\xi,\theta) 
\partial_x^2 + \partial_y^2 = (\Delta G_1)\partial_\xi + (\Delta G_2)\partial_\theta + |\nabla G_1|^2 \partial_\xi^2 + |\nabla G_2|^2 \partial_\theta^2 + 2(\nabla G_1 \cdot \nabla G_2)\partial_\xi \partial_\theta,
 \partial_x^2 + \partial_y^2 = h(\partial_\xi^2 + \partial_\theta^2) 
|\nabla G_1|^2 = |\nabla G_2|^2 = h, \quad \Delta G_1 = \Delta G_2 = 0, \quad \text{and}\quad \nabla G_1 \cdot \nabla G_2 = 0.
 
\begin{pmatrix}\partial_x f &\partial_y f \end{pmatrix} = \begin{pmatrix}\partial_\xi f &\partial_\theta f \end{pmatrix} \begin{pmatrix}\partial_x G_1 &\partial_y G_1 \\
\partial_x G_2 &\partial_y G_2 \end{pmatrix},
 (\partial_x f)^2 + (\partial_y f)^2 = h ((\partial_\xi f)^2 + (\partial_\theta f)^2 ) 
\begin{pmatrix}\partial_x G_1 &\partial_y G_1 \\
\partial_x G_2 &\partial_y G_2 \end{pmatrix} = \sqrt{h}\ M,
 M D 
|\nabla G_1|^2 = |\nabla G_2|^2 = h \quad \text{and}\quad \nabla G_1 \cdot \nabla G_2 = 0.
 \begin{align}
\partial_x^2 f + \partial_y^2 f &= h(\partial_\xi^2 f + \partial_\theta^2 f) \\
&\text{if and only if} \\ 
(\partial_x f)^2 + (\partial_y f)^2 &= h ((\partial_\xi f)^2 + (\partial_\theta f)^2 ) \ \ \text{and} \ \ \Delta G_1 = \Delta G_2 = 0, 
\end{align} G (x,y) = (e^\xi \cos(\theta), e^\xi \sin(\theta)) (\xi,\theta) = G(x,y) 
G(x,y) = (G_1(x,y) , G_2(x,y)) = \left( \frac12 \ln(x^2+y^2), \arctan\left( \frac{y}{x} \right) \right).
 G_1 G_2 G","['real-analysis', 'complex-analysis', 'multivariable-calculus', 'differential-geometry', 'partial-differential-equations']"
54,"Evaluating the integral, $\int_{0}^{\infty} \ln\left(1 - e^{-x}\right) \,\mathrm dx $","Evaluating the integral,","\int_{0}^{\infty} \ln\left(1 - e^{-x}\right) \,\mathrm dx ","I recently got stuck on evaluating the following integral. I do not know an effective substitution to use. Could you please help me evaluate: $$\int_{0}^{\infty} \ln\left(1 - e^{-x}\right) \,\mathrm dx $$","I recently got stuck on evaluating the following integral. I do not know an effective substitution to use. Could you please help me evaluate: $$\int_{0}^{\infty} \ln\left(1 - e^{-x}\right) \,\mathrm dx $$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
55,Infinite Series $â\sum\limits_{n=2}^{\infty}\frac{\zeta(n)}{k^n}$,Infinite Series,â\sum\limits_{n=2}^{\infty}\frac{\zeta(n)}{k^n},"âIf $f\left(z \right)=\sum_{n=2}^{\infty}a_{n}z^n$ and $\sum_{n=2}^{\infty}|a_n|$ converges thenâ, $$\sum_{n=1}^{\infty}f\left(\frac{1}{n}\right)=\sum_{n=2}^{\infty}a_n\zeta\left(n\right)â.$$ âSince if we set $C:=\sum_{m=2}^{\infty}|a_m|<\infty$â, âthenâ $$\sum_{n=1}^{\infty}\sum_{m=2}^{\infty}|a_m\frac{1}{n^m}|\leq\sum_{n=1}^{\infty}\sum_{m=2}^{\infty}|a_m|\frac{1}{n^2}\leq C\sum_{n=1}^{\infty}\frac{1}{n^2}<\inftyâ$$ and âby Cauchy's double series theoremâ, âwe can switch the order of summationâ: $$\sum_{n=1}^{\infty}f\left(\frac{1}{n}\right)=\sum_{n=1}^{\infty}\sum_{m=2}^{\infty}a_m\frac{1}{n^m}=\sum_{m=2}^{\infty}a_m\sum_{n=1}^{\infty}\frac{1}{n^m}=\sum_{n=2}^{\infty}a_n\zeta\left(n\right)â.$$ This shows that $â\sum_{n=2}^{\infty}\frac{\zeta(n)}{k^n}=\sum_{n=1}^{\infty}\frac{1}{kn(kn-1)}$. My Questions: 1) It's obvious that $\sum_{n=1}^{\infty}\frac{1}{2n(2n-1)}=\log(2)$, but how can I evaluate $\sum_{n=1}^{\infty}\frac{1}{3n(3n-1)}$? 2) Is there another method to evaluate $\sum_{n=2}^{\infty}\frac{\zeta(n)}{k^n}$?","âIf $f\left(z \right)=\sum_{n=2}^{\infty}a_{n}z^n$ and $\sum_{n=2}^{\infty}|a_n|$ converges thenâ, $$\sum_{n=1}^{\infty}f\left(\frac{1}{n}\right)=\sum_{n=2}^{\infty}a_n\zeta\left(n\right)â.$$ âSince if we set $C:=\sum_{m=2}^{\infty}|a_m|<\infty$â, âthenâ $$\sum_{n=1}^{\infty}\sum_{m=2}^{\infty}|a_m\frac{1}{n^m}|\leq\sum_{n=1}^{\infty}\sum_{m=2}^{\infty}|a_m|\frac{1}{n^2}\leq C\sum_{n=1}^{\infty}\frac{1}{n^2}<\inftyâ$$ and âby Cauchy's double series theoremâ, âwe can switch the order of summationâ: $$\sum_{n=1}^{\infty}f\left(\frac{1}{n}\right)=\sum_{n=1}^{\infty}\sum_{m=2}^{\infty}a_m\frac{1}{n^m}=\sum_{m=2}^{\infty}a_m\sum_{n=1}^{\infty}\frac{1}{n^m}=\sum_{n=2}^{\infty}a_n\zeta\left(n\right)â.$$ This shows that $â\sum_{n=2}^{\infty}\frac{\zeta(n)}{k^n}=\sum_{n=1}^{\infty}\frac{1}{kn(kn-1)}$. My Questions: 1) It's obvious that $\sum_{n=1}^{\infty}\frac{1}{2n(2n-1)}=\log(2)$, but how can I evaluate $\sum_{n=1}^{\infty}\frac{1}{3n(3n-1)}$? 2) Is there another method to evaluate $\sum_{n=2}^{\infty}\frac{\zeta(n)}{k^n}$?",,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'closed-form', 'riemann-zeta']"
56,Why do we study real numbers?,Why do we study real numbers?,,"I apologize if this is a somewhat naive question, but is there any particular reason mathematicians disproportionately study the field $\mathbb{R}$ and its subsets (as opposed to any other algebraic structure)? Is this because $\mathbb{R}$ is ""objectively"" more interesting in that studying it allows one to gain deep insights into mathematics, or is it sort of ""arbitrary"" in the sense that we are inclined to study $\mathbb{R}$ due to historical reasons, real-world applications and because human beings have a strong natural intuition of real numbers? Edit: Note that I am not asking why $\mathbb{Q}$ is insufficient as a number system; this has been asked and answered on this site and elsewhere. Rather, why, in a more deep sense, are $\mathbb{N} \subset \mathbb{Z} \subset \mathbb{Q} \subset \mathbb{R}$ so crucial to mathematics? Would we be able to construct a meaningful study of mathematics with absolutely no reference to these sets, or are they fundamentally imperative?","I apologize if this is a somewhat naive question, but is there any particular reason mathematicians disproportionately study the field $\mathbb{R}$ and its subsets (as opposed to any other algebraic structure)? Is this because $\mathbb{R}$ is ""objectively"" more interesting in that studying it allows one to gain deep insights into mathematics, or is it sort of ""arbitrary"" in the sense that we are inclined to study $\mathbb{R}$ due to historical reasons, real-world applications and because human beings have a strong natural intuition of real numbers? Edit: Note that I am not asking why $\mathbb{Q}$ is insufficient as a number system; this has been asked and answered on this site and elsewhere. Rather, why, in a more deep sense, are $\mathbb{N} \subset \mathbb{Z} \subset \mathbb{Q} \subset \mathbb{R}$ so crucial to mathematics? Would we be able to construct a meaningful study of mathematics with absolutely no reference to these sets, or are they fundamentally imperative?",,"['real-analysis', 'soft-question']"
57,A Simple Calculus view on Fermat's Last Theorem,A Simple Calculus view on Fermat's Last Theorem,,"My question (more of a hypothesis really) is basically this: If a function $f(x)$ is defined such that $f'(x)$ is not constant and never the same for any 2 values of $x$. Then there do not exist positive integers $a,b,c$ and $a\le b\le c$ such that, $$\int_0^{a} f(x)dx = \int_b^{c} f(x)dx\tag1$$ Taking $f(x)=x^n$ then for $n \gt 1$, $(1)$ becomes Fermat's Last Theorem, while for $n=1$, $(1)$ becomes $a^2 + b^2 = c^2$ Maybe this has something to do with the ""curviness"" (I am 16 so please forgive me for non-technical language) of the graphs of $x^n$ since for $n=1$ the graph is linear and for $n>1$ the graph is curvy. My school teachers are not talking to me about this because ""it is out of syllabus"", so I thought maybe this community could help.","My question (more of a hypothesis really) is basically this: If a function $f(x)$ is defined such that $f'(x)$ is not constant and never the same for any 2 values of $x$. Then there do not exist positive integers $a,b,c$ and $a\le b\le c$ such that, $$\int_0^{a} f(x)dx = \int_b^{c} f(x)dx\tag1$$ Taking $f(x)=x^n$ then for $n \gt 1$, $(1)$ becomes Fermat's Last Theorem, while for $n=1$, $(1)$ becomes $a^2 + b^2 = c^2$ Maybe this has something to do with the ""curviness"" (I am 16 so please forgive me for non-technical language) of the graphs of $x^n$ since for $n=1$ the graph is linear and for $n>1$ the graph is curvy. My school teachers are not talking to me about this because ""it is out of syllabus"", so I thought maybe this community could help.",,"['real-analysis', 'calculus', 'number-theory', 'conjectures']"
58,Uniform convergence of $\sum_{n=1}^{\infty} \frac{\sin(n x) \sin(n^2 x)}{n+x^2}$,Uniform convergence of,\sum_{n=1}^{\infty} \frac{\sin(n x) \sin(n^2 x)}{n+x^2},I'm not sure wether or not the following sum uniformly converge on $\mathbb{R}$ : $$\sum_{n=1}^{\infty} \frac{\sin(n x) \sin(n^2 x)}{n+x^2}$$ Can someone help me with it? (I can't use Dirichlet' because of the areas where $x$ is close to $0$),I'm not sure wether or not the following sum uniformly converge on $\mathbb{R}$ : $$\sum_{n=1}^{\infty} \frac{\sin(n x) \sin(n^2 x)}{n+x^2}$$ Can someone help me with it? (I can't use Dirichlet' because of the areas where $x$ is close to $0$),,"['real-analysis', 'sequences-and-series', 'uniform-convergence']"
59,Is there a function with infinite integral on every interval?,Is there a function with infinite integral on every interval?,,"Could give some examples of nonnegative measurable function $f:\mathbb{R}\to[0,\infty)$, such that its integral over any bounded interval is infinite?","Could give some examples of nonnegative measurable function $f:\mathbb{R}\to[0,\infty)$, such that its integral over any bounded interval is infinite?",,"['real-analysis', 'measure-theory', 'examples-counterexamples']"
60,Show that $\sum{\frac{x^n}{(1+x^n)^n}}$ converges uniformly,Show that  converges uniformly,\sum{\frac{x^n}{(1+x^n)^n}},"Show that $$\sum{\frac{x^n}{(1+x^n)^n}}$$ converges uniformly on $[0,1].$ I am sorry but for this exercise I got exactly nothing. It seems to be difficult.","Show that $$\sum{\frac{x^n}{(1+x^n)^n}}$$ converges uniformly on $[0,1].$ I am sorry but for this exercise I got exactly nothing. It seems to be difficult.",,['real-analysis']
61,Stronger version of AMM problem 11145 (April 2005)?,Stronger version of AMM problem 11145 (April 2005)?,,"How to show that for $a_1,a_2,\cdots,a_n >0$ real numbers and for $n \ge 3$: $$\sum_{k=1}^{n}\dfrac{k}{a_{1}+a_{2}+\cdots+a_{k}}\le\left(2-\dfrac{7\ln{2}}{8\ln{n}}\right)\sum_{k=1}^{n}\dfrac{1}{a_{k}}$$ This version seems stronger than the inequality mentioned here . Addition: A sister problem : For $a_1,a_2,\cdots,a_n >0$ real numbers and for $n \ge 2$, we have the version: $$\displaystyle \dfrac{1}{1+a_{1}}+\dfrac{1}{1+a_{1}+a_{2}}+\cdots+\dfrac{1}{1+a_{1}+a_{2}+\cdots+a_{n}} \le \sqrt{\dfrac{1}{a_{1}}+\dfrac{1}{a_{2}}+\cdots+\dfrac{1}{a_{n}}}$$ The stronger vesrion claims that: For each $n$, $c_n = \left(1-\dfrac{\ln{n}}{2n}\right)$ we have: $$\displaystyle \dfrac{1}{1+a_{1}}+\dfrac{1}{1+a_{1}+a_{2}}+\cdots+\dfrac{1}{1+a_{1}+a_{2}+\cdots+a_{n}} \le c_n\sqrt{\dfrac{1}{a_{1}}+\dfrac{1}{a_{2}}+\cdots+\dfrac{1}{a_{n}}}$$","How to show that for $a_1,a_2,\cdots,a_n >0$ real numbers and for $n \ge 3$: $$\sum_{k=1}^{n}\dfrac{k}{a_{1}+a_{2}+\cdots+a_{k}}\le\left(2-\dfrac{7\ln{2}}{8\ln{n}}\right)\sum_{k=1}^{n}\dfrac{1}{a_{k}}$$ This version seems stronger than the inequality mentioned here . Addition: A sister problem : For $a_1,a_2,\cdots,a_n >0$ real numbers and for $n \ge 2$, we have the version: $$\displaystyle \dfrac{1}{1+a_{1}}+\dfrac{1}{1+a_{1}+a_{2}}+\cdots+\dfrac{1}{1+a_{1}+a_{2}+\cdots+a_{n}} \le \sqrt{\dfrac{1}{a_{1}}+\dfrac{1}{a_{2}}+\cdots+\dfrac{1}{a_{n}}}$$ The stronger vesrion claims that: For each $n$, $c_n = \left(1-\dfrac{\ln{n}}{2n}\right)$ we have: $$\displaystyle \dfrac{1}{1+a_{1}}+\dfrac{1}{1+a_{1}+a_{2}}+\cdots+\dfrac{1}{1+a_{1}+a_{2}+\cdots+a_{n}} \le c_n\sqrt{\dfrac{1}{a_{1}}+\dfrac{1}{a_{2}}+\cdots+\dfrac{1}{a_{n}}}$$",,"['real-analysis', 'inequality']"
62,"If $f(x)=\frac{1}{x^2+x+1}$, how to find $f^{(36)} (0)$?","If , how to find ?",f(x)=\frac{1}{x^2+x+1} f^{(36)} (0),"If $f(x)=\frac{1}{x^2+x+1}$, find $f^{(36)} (0)$. So far I have tried letting $a=x^2+x+1$ and then finding the first several derivatives to see if some terms would disappear because the third derivative of $a$ is $0$, but the derivatives keep getting longer and longer. Am I on the right track? Thanks!","If $f(x)=\frac{1}{x^2+x+1}$, find $f^{(36)} (0)$. So far I have tried letting $a=x^2+x+1$ and then finding the first several derivatives to see if some terms would disappear because the third derivative of $a$ is $0$, but the derivatives keep getting longer and longer. Am I on the right track? Thanks!",,"['calculus', 'real-analysis', 'derivatives', 'complex-numbers', 'summation']"
63,Why do differentiation rules work? What's the intuition behind them? (Not asking for proofs),Why do differentiation rules work? What's the intuition behind them? (Not asking for proofs),,"Differentiation rules have been bugging me ever since I took Basic Calculus. I thought I'd develop some intuitive understanding of them eventually, but so far all my other math courses (including Multivariable Calculus) take the rules for granted. I know how to prove some of the rules. The problem is that algebra manipulation alone isn't quite convincing to me. Is there any possibility of understanding why the algebra happens to work that way? For example, why do the slopes of the tangent line to the parabola x^2 happen to be determined by 2x? Looking at it graphically, there's no way I could've told that. Any sources covering this issue (books; internet sites; etc) would be very greatly appreciated. Thanks in advance.","Differentiation rules have been bugging me ever since I took Basic Calculus. I thought I'd develop some intuitive understanding of them eventually, but so far all my other math courses (including Multivariable Calculus) take the rules for granted. I know how to prove some of the rules. The problem is that algebra manipulation alone isn't quite convincing to me. Is there any possibility of understanding why the algebra happens to work that way? For example, why do the slopes of the tangent line to the parabola x^2 happen to be determined by 2x? Looking at it graphically, there's no way I could've told that. Any sources covering this issue (books; internet sites; etc) would be very greatly appreciated. Thanks in advance.",,"['calculus', 'real-analysis', 'derivatives', 'intuition']"
64,Showing that $\frac{\sqrt[n]{n!}}{n}$ $\rightarrow \frac{1}{e}$ [duplicate],Showing that   [duplicate],\frac{\sqrt[n]{n!}}{n} \rightarrow \frac{1}{e},This question already has answers here : Finding the limit of $\frac {n}{\sqrt[n]{n!}}$ (11 answers) Closed 4 years ago . Show:$$\lim_{n\to\infty}\frac{\sqrt[n]{n!}}{n}= \frac{1}{e}$$ So I can expand the numerator by geometric mean. Letting $C_{n}=\left(\ln(a_{1})+...+\ln(a_{n})\right)/n$. Let the numerator be called $a_{n}$ and the denominator be $b_{n}$ Is there a way to use this statement so that I could force the original sequence into the form of $1/\left(1+\frac{1}{n}\right)^n$,This question already has answers here : Finding the limit of $\frac {n}{\sqrt[n]{n!}}$ (11 answers) Closed 4 years ago . Show:$$\lim_{n\to\infty}\frac{\sqrt[n]{n!}}{n}= \frac{1}{e}$$ So I can expand the numerator by geometric mean. Letting $C_{n}=\left(\ln(a_{1})+...+\ln(a_{n})\right)/n$. Let the numerator be called $a_{n}$ and the denominator be $b_{n}$ Is there a way to use this statement so that I could force the original sequence into the form of $1/\left(1+\frac{1}{n}\right)^n$,,"['real-analysis', 'sequences-and-series', 'limits', 'radicals', 'factorial']"
65,Difference between $\mathbb C$ and $\mathbb R^2$ [duplicate],Difference between  and  [duplicate],\mathbb C \mathbb R^2,"This question already has answers here : What's the difference between $\mathbb{R}^2$ and the complex plane? (14 answers) Closed 8 years ago . What are the basic differences between $\mathbb C$ and $\mathbb R^2$? The points in these two sets are written as ordered pairs, I mean the structure looks similar to me. So what is the reason to denote these two sets differently?","This question already has answers here : What's the difference between $\mathbb{R}^2$ and the complex plane? (14 answers) Closed 8 years ago . What are the basic differences between $\mathbb C$ and $\mathbb R^2$? The points in these two sets are written as ordered pairs, I mean the structure looks similar to me. So what is the reason to denote these two sets differently?",,"['real-analysis', 'abstract-algebra', 'complex-analysis']"
66,Prove that a subset of a separable set is itself separable,Prove that a subset of a separable set is itself separable,,"The problem statement, all variables and given/known data: Show that if $X$ is a subset of $M$ and $(M,d)$ is separable, then $(X,d)$ is separable. [This may be a little bit trickier than it looks - $E$ may be a countable dense subset of $M$ with $X\cap E = \varnothing$.]  Definitions Per our book: A metric space $(M,d)$ is separable if there exists a countable dense $E$ contained in $M$. $E$ contained in $M$ is dense if $\forall m\in M$, $\forall Îµ>0\in\mathbb R$, $\exists e\in E$ s.t. $d(m,e) < Îµ$ The attempt at a solution My best attempt was doomed from the start, because I don't quite understand the hint. My thought process went as follows: Since $X$ is a subset of $M$, $\forall x\in X, x\in M$. Thus, since $E$ is dense in $M$, $\forall x\in X$, and Îµ > 0, $\exists e\in E$ st $d(x,e)<Îµ $. At this point, I was done, because the set of $e$'s satisfying the above, is a subset of $E$, a countable set. So a subset of a countable set is dense in $X$, and $X$ is separable. This is incorrect, but I cannot see why.  Any help clearing up the confusion would be greatly appreciated. Thanks! Edit: I wish I could upvote all of you for your help! I really appreciate the speedy replies and attempts to make this information clear to me.","The problem statement, all variables and given/known data: Show that if $X$ is a subset of $M$ and $(M,d)$ is separable, then $(X,d)$ is separable. [This may be a little bit trickier than it looks - $E$ may be a countable dense subset of $M$ with $X\cap E = \varnothing$.]  Definitions Per our book: A metric space $(M,d)$ is separable if there exists a countable dense $E$ contained in $M$. $E$ contained in $M$ is dense if $\forall m\in M$, $\forall Îµ>0\in\mathbb R$, $\exists e\in E$ s.t. $d(m,e) < Îµ$ The attempt at a solution My best attempt was doomed from the start, because I don't quite understand the hint. My thought process went as follows: Since $X$ is a subset of $M$, $\forall x\in X, x\in M$. Thus, since $E$ is dense in $M$, $\forall x\in X$, and Îµ > 0, $\exists e\in E$ st $d(x,e)<Îµ $. At this point, I was done, because the set of $e$'s satisfying the above, is a subset of $E$, a countable set. So a subset of a countable set is dense in $X$, and $X$ is separable. This is incorrect, but I cannot see why.  Any help clearing up the confusion would be greatly appreciated. Thanks! Edit: I wish I could upvote all of you for your help! I really appreciate the speedy replies and attempts to make this information clear to me.",,"['real-analysis', 'general-topology', 'metric-spaces']"
67,"Why does ""convex function"" mean ""concave *up*""?","Why does ""convex function"" mean ""concave *up*""?",,"A function $f : \mathbb{R} \to \mathbb{R}$ is convex (or ""concave up"") provided that for all $x,y \in \mathbb{R}$ and $t \in [0,1]$, $$f(tx + (1-t)y) \le tf(x) + (1-t)f(y).$$ Equivalently, a line segment between two points on the graph lies above the graph, the region above the graph is convex, etc.  I want to know why the word ""convex"" goes with the inequality in this direction, and how I can remember it.  Every reason I have heard makes just as much sense applied to the opposite inequality (""concave down"").","A function $f : \mathbb{R} \to \mathbb{R}$ is convex (or ""concave up"") provided that for all $x,y \in \mathbb{R}$ and $t \in [0,1]$, $$f(tx + (1-t)y) \le tf(x) + (1-t)f(y).$$ Equivalently, a line segment between two points on the graph lies above the graph, the region above the graph is convex, etc.  I want to know why the word ""convex"" goes with the inequality in this direction, and how I can remember it.  Every reason I have heard makes just as much sense applied to the opposite inequality (""concave down"").",,"['soft-question', 'real-analysis', 'terminology', 'functions']"
68,Proving that sum of two measurable functions is measurable.,Proving that sum of two measurable functions is measurable.,,"Suppose $f$ and $g$ are Lebesgue measurable, we want to show $f+g$ is measurable. So, the hint is to consider the continuous functions $F : \mathbb{R}^2 \to \mathbb{R} $ given by $h(x) = F(f ,g ) $. If we can show $F$ Is measurable, then Taking $F = f +g $ would solve our problem. In other words, I want to show that the set $R = \{ (f,g) : F(f,g) > a $ } is lebesgue measurable.. But this set is just a rectangle in the plane. And since $F$ is continuous, then $R$ must be open, and hence a union of open rectangles which are measurable and hence $R$ must be measurable. Is this a correct approach to the problem? Can someone help me to make this formal? thanks","Suppose $f$ and $g$ are Lebesgue measurable, we want to show $f+g$ is measurable. So, the hint is to consider the continuous functions $F : \mathbb{R}^2 \to \mathbb{R} $ given by $h(x) = F(f ,g ) $. If we can show $F$ Is measurable, then Taking $F = f +g $ would solve our problem. In other words, I want to show that the set $R = \{ (f,g) : F(f,g) > a $ } is lebesgue measurable.. But this set is just a rectangle in the plane. And since $F$ is continuous, then $R$ must be open, and hence a union of open rectangles which are measurable and hence $R$ must be measurable. Is this a correct approach to the problem? Can someone help me to make this formal? thanks",,"['real-analysis', 'analysis']"
69,Is there a $\sigma$-algebra on $\mathbb{R}$ strictly between the Borel and Lebesgue algebras?,Is there a -algebra on  strictly between the Borel and Lebesgue algebras?,\sigma \mathbb{R},"So, after proving that $\mathfrak{B}(\mathbb{R})\subset \mathfrak{L}(\mathbb{R})$, I asked myself, and now asking you, is there a set $\mathfrak{S}(\mathbb{R})$, which satisfies: $$\mathfrak{B}(\mathbb{R} )\subset \mathfrak{S}(\mathbb{R})\subset \mathfrak{L}(\mathbb{R})$$ ($\mathfrak{B}(\mathbb{R})$ is the Borel's set on $\mathbb{R}$ ; $\mathfrak{L}(\mathbb{R})$ is family of sets which are Lebesgue's measurable - which is a $\sigma$ algebra.)","So, after proving that $\mathfrak{B}(\mathbb{R})\subset \mathfrak{L}(\mathbb{R})$, I asked myself, and now asking you, is there a set $\mathfrak{S}(\mathbb{R})$, which satisfies: $$\mathfrak{B}(\mathbb{R} )\subset \mathfrak{S}(\mathbb{R})\subset \mathfrak{L}(\mathbb{R})$$ ($\mathfrak{B}(\mathbb{R})$ is the Borel's set on $\mathbb{R}$ ; $\mathfrak{L}(\mathbb{R})$ is family of sets which are Lebesgue's measurable - which is a $\sigma$ algebra.)",,"['real-analysis', 'measure-theory', 'descriptive-set-theory']"
70,Evaluate $\int\frac1{1+x^n}dx$ for $n\in\mathbb R$,Evaluate  for,\int\frac1{1+x^n}dx n\in\mathbb R,"I was wondering on how to evaluate the following indefinite integral for all $n\in\mathbb R$. $$\int\frac1{1+x^n}dx$$ It seems to be peculiar in that we have $$\begin{align} \int\frac1{1+x^{-1}}dx&=x-\ln(x+1)+c\\ \int\frac1{1+x^0}dx&=\frac12x+c\\ \int\frac1{1+x^{1/2}}dx&=2\sqrt x-2\ln(1+\sqrt x)+c\\ \int\frac1{1+x^1}dx&=\ln(x+1)+c\\ \int\frac1{1+x^2}dx&=\arctan(x)+c\\ \int\frac1{1+x^3}dx&=\frac13\ln(1+x)-\frac2{3\sqrt3}\arctan\left(\sqrt{\frac43}\left(x-\frac12\right)\right)+c \end{align}$$ Naturally, there appears to be some combination of $\ln$ and $\arctan$, though no simple formula arises to solve the general case. It is, however, easy to see that $$\int\frac1{1+x^{-n}}dx=\int1-\frac1{1+x^n}dx$$ So there is an easy enough connection between positive and negative $n$. Also, it is easy enough to find the series expansion, taking advantage of the above connection we just made to circumvent problems concerning convergence. $$\frac1{1+x^n}=1-x^n+x^{2n}-x^{3n}+\dots\forall\ |x|<1$$ $$\int\frac1{1+x^n}dx=c+x-\frac1{n+1}x^{n+1}+\frac1{2n+1}x^{2n+1}-\dots$$ $$=c+\sum_{k=0}^\infty\frac{(-1)^k}{kn+1}x^{kn+1}\ \forall\ |x|<1$$ Though this isn't very much along the lines of closed form. For $n=\frac ab$, where $a$ and $b$ are whole numbers, we can use the substitution $x=u^b$ to get $$\int\frac1{1+x^n}dx=\int\frac{bu^{b-1}}{1+u^a}du$$ though I'm unsure where that could lead. This reduces the integral down to $$\int\frac1{1+x^n}dx=b\int P(u)+\frac{u^{b-1-ak}}{1+u^a}du,\quad k\in\mathbb N$$ for some polynomial $P(u)$.  Though I'm still clueless as to how this can be advanced. How can I evaluate $\int\frac1{1+x^n}dx\ \forall\ n\in\mathbb R$ in closed form?  Can someone prove there at least exists some closed form solution for all $n\in\mathbb Q$ if the above is not possible?  If possible, use real numbers.","I was wondering on how to evaluate the following indefinite integral for all $n\in\mathbb R$. $$\int\frac1{1+x^n}dx$$ It seems to be peculiar in that we have $$\begin{align} \int\frac1{1+x^{-1}}dx&=x-\ln(x+1)+c\\ \int\frac1{1+x^0}dx&=\frac12x+c\\ \int\frac1{1+x^{1/2}}dx&=2\sqrt x-2\ln(1+\sqrt x)+c\\ \int\frac1{1+x^1}dx&=\ln(x+1)+c\\ \int\frac1{1+x^2}dx&=\arctan(x)+c\\ \int\frac1{1+x^3}dx&=\frac13\ln(1+x)-\frac2{3\sqrt3}\arctan\left(\sqrt{\frac43}\left(x-\frac12\right)\right)+c \end{align}$$ Naturally, there appears to be some combination of $\ln$ and $\arctan$, though no simple formula arises to solve the general case. It is, however, easy to see that $$\int\frac1{1+x^{-n}}dx=\int1-\frac1{1+x^n}dx$$ So there is an easy enough connection between positive and negative $n$. Also, it is easy enough to find the series expansion, taking advantage of the above connection we just made to circumvent problems concerning convergence. $$\frac1{1+x^n}=1-x^n+x^{2n}-x^{3n}+\dots\forall\ |x|<1$$ $$\int\frac1{1+x^n}dx=c+x-\frac1{n+1}x^{n+1}+\frac1{2n+1}x^{2n+1}-\dots$$ $$=c+\sum_{k=0}^\infty\frac{(-1)^k}{kn+1}x^{kn+1}\ \forall\ |x|<1$$ Though this isn't very much along the lines of closed form. For $n=\frac ab$, where $a$ and $b$ are whole numbers, we can use the substitution $x=u^b$ to get $$\int\frac1{1+x^n}dx=\int\frac{bu^{b-1}}{1+u^a}du$$ though I'm unsure where that could lead. This reduces the integral down to $$\int\frac1{1+x^n}dx=b\int P(u)+\frac{u^{b-1-ak}}{1+u^a}du,\quad k\in\mathbb N$$ for some polynomial $P(u)$.  Though I'm still clueless as to how this can be advanced. How can I evaluate $\int\frac1{1+x^n}dx\ \forall\ n\in\mathbb R$ in closed form?  Can someone prove there at least exists some closed form solution for all $n\in\mathbb Q$ if the above is not possible?  If possible, use real numbers.",,"['real-analysis', 'integration', 'sequences-and-series', 'indefinite-integrals']"
71,"Totally bounded, complete $\implies$ compact","Totally bounded, complete  compact",\implies,"Show that a totally bounded complete metric space $X$ is compact. I can use the fact that sequentially compact $\Leftrightarrow$ compact. Attempt: Complete $\implies$ every Cauchy sequence converges. Totally bounded $\implies$ $\forall\epsilon>0$, $X$ can be covered by a finite number of balls of radius $\epsilon$. I'm trying to show that all sequences in $X$ have a subsequence that converges to an element in $X$. I don't see how to go from convergent Cauchy sequences and totally bounded to subsequence convergent $in$ $X$.","Show that a totally bounded complete metric space $X$ is compact. I can use the fact that sequentially compact $\Leftrightarrow$ compact. Attempt: Complete $\implies$ every Cauchy sequence converges. Totally bounded $\implies$ $\forall\epsilon>0$, $X$ can be covered by a finite number of balls of radius $\epsilon$. I'm trying to show that all sequences in $X$ have a subsequence that converges to an element in $X$. I don't see how to go from convergent Cauchy sequences and totally bounded to subsequence convergent $in$ $X$.",,"['real-analysis', 'metric-spaces', 'compactness', 'complete-spaces']"
72,Lebesgue Measure of the Graph of a Function,Lebesgue Measure of the Graph of a Function,,"Let $f:R^n \rightarrow R^m$ be any function. Will the graph of $f$ always have Lebesgue measure zero? $(1)$ I could prove that this is true if $f$ is continuous. $(2)$ I suspect it is true if $f$ is measurable, but I'm not sure. (My idea was to use Fubini's Theorem to integrate the indicator function of the graph, but I don't know if I'm using the Theorem properly). If $(2)$ is incorrect, what would be a counterexample where the graph of $f$ has positive measure? If $(2)$ is correct, can we prove the existence of a non-measurable function whose graph has positive outer measure?","Let $f:R^n \rightarrow R^m$ be any function. Will the graph of $f$ always have Lebesgue measure zero? $(1)$ I could prove that this is true if $f$ is continuous. $(2)$ I suspect it is true if $f$ is measurable, but I'm not sure. (My idea was to use Fubini's Theorem to integrate the indicator function of the graph, but I don't know if I'm using the Theorem properly). If $(2)$ is incorrect, what would be a counterexample where the graph of $f$ has positive measure? If $(2)$ is correct, can we prove the existence of a non-measurable function whose graph has positive outer measure?",,"['real-analysis', 'measure-theory']"
73,Global invertibility of a map $\mathbb{R}^n\to \mathbb{R}^n$ from everywhere local invertibility,Global invertibility of a map  from everywhere local invertibility,\mathbb{R}^n\to \mathbb{R}^n,"I was told by a tutor that if $f: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ has an invertible Jacobian Matrix for all $x \in \mathbb{R}^n$ and $\lim_{|x_k| \rightarrow \infty}|f(x_k)|=\infty$ for all such sequences, then $f$ is already globally bijective. This was very surprising for me (as this seems to be a very strong statement), so I tried to search on the internet for this theorem but I only came up with theorems about local inverses. I hope someone here can give me a reference or a name of this theorem so I could read a detailed proof. I sanity checked it for $n=1$ where it works perfectly.","I was told by a tutor that if $f: \mathbb{R}^n \longrightarrow \mathbb{R}^n$ has an invertible Jacobian Matrix for all $x \in \mathbb{R}^n$ and $\lim_{|x_k| \rightarrow \infty}|f(x_k)|=\infty$ for all such sequences, then $f$ is already globally bijective. This was very surprising for me (as this seems to be a very strong statement), so I tried to search on the internet for this theorem but I only came up with theorems about local inverses. I hope someone here can give me a reference or a name of this theorem so I could read a detailed proof. I sanity checked it for $n=1$ where it works perfectly.",,"['calculus', 'real-analysis', 'analysis']"
74,About the inequality $x^{x^{x^{x^{x^x}}}} \ge \frac12 x^2 + \frac12$,About the inequality,x^{x^{x^{x^{x^x}}}} \ge \frac12 x^2 + \frac12,"Problem : Let $x > 0$ . Prove that $$x^{x^{x^{x^{x^x}}}} \ge \frac12 x^2 + \frac12.$$ Remark 1 : The problem was posted on MSE (now closed ). Remark 2 : I have a proof (see below). My proof is not nice. For example, we need to prove that $\frac{3x^2 - 3}{x^2 + 4x + 1} + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7} \le 0$ for all $0 < x < 1$ for which my proof is not nice. I want to know if there are some nice proofs. Also, I want my proof reviewed for its correctness. Any comments and solutions are welcome and appreciated. My proof (sketch) : We split into cases: i) $x \ge 1$ : Clearly, $x^{x^{x^{x^{x^x}}}}\ge x^x$ . By Bernoulli's inequality, we have $x^x = (1 + (x - 1))^x \ge 1 + (x - 1)x = x^2 - x + 1 \ge \frac12 x^2 + \frac12$ . The inequality is true. ii) $0 < x < 1$ : It suffices to prove that $$x^{x^{x^{x^x}}}\ln x \ge \ln \frac{x^2 + 1}{2}$$ or $$x^{x^{x^{x^x}}} \le \frac{\ln \frac{x^2 + 1}{2}}{\ln x}$$ or $$x^{x^{x^x}}\ln x \le \ln \frac{\ln \frac{x^2 + 1}{2}}{\ln x}$$ or $$x^{x^{x^x}}\ge \frac{1}{\ln x}\ln \frac{\ln \frac{x^2 + 1}{2}}{\ln x}.$$ It suffices to prove that $$x^{x^{x^x}}\ge \frac{7}{12} \ge \frac{1}{\ln x}\ln \frac{\ln \frac{x^2 + 1}{2}}{\ln x}. \tag{1}$$ First, it is easy to prove that $$x^x \ge \mathrm{e}^{-1/\mathrm{e}} \ge \frac{1}{\ln x}\ln\frac{\ln\frac{7}{12}}{\ln x}.$$ Thus, the left inequality in (1) is true. Second, let $f(x) = x^{7/12}\ln x - \ln \frac{x^2 + 1}{2}$ . We have \begin{align*} 	f'(x) &= \frac{7}{12x^{5/12}} 	\left(\ln x + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7}\right)\\ 	&\le \frac{7}{12x^{5/12}} 	\left(\frac{3x^2 - 3}{x^2 + 4x + 1} + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7}\right)\\ 	&\le 0 \tag{2} \end{align*} where we have used $\ln x \le \frac{3x^2 - 3}{x^2 + 4x + 1}$ for all $x$ in $(0, 1]$ . Also, $f(1) = 0$ . Thus, $f(x) \ge 0$ for all $x$ in $(0, 1)$ . Thus, the right inequality in (1) is true. Note : For the inequality $\frac{3x^2 - 3}{x^2 + 4x + 1} + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7} \le 0$ for all $0 < x < 1$ , we let $x = y^{12}$ and it suffices to prove that $11y^{47} + \cdots + 3 \ge 0$ (a polynomial of degree $47$ , a long expression) for all $0 < y < 1$ . We are done.","Problem : Let . Prove that Remark 1 : The problem was posted on MSE (now closed ). Remark 2 : I have a proof (see below). My proof is not nice. For example, we need to prove that for all for which my proof is not nice. I want to know if there are some nice proofs. Also, I want my proof reviewed for its correctness. Any comments and solutions are welcome and appreciated. My proof (sketch) : We split into cases: i) : Clearly, . By Bernoulli's inequality, we have . The inequality is true. ii) : It suffices to prove that or or or It suffices to prove that First, it is easy to prove that Thus, the left inequality in (1) is true. Second, let . We have where we have used for all in . Also, . Thus, for all in . Thus, the right inequality in (1) is true. Note : For the inequality for all , we let and it suffices to prove that (a polynomial of degree , a long expression) for all . We are done.","x > 0 x^{x^{x^{x^{x^x}}}} \ge \frac12 x^2 + \frac12. \frac{3x^2 - 3}{x^2 + 4x + 1} + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7} \le 0 0 < x < 1 x \ge 1 x^{x^{x^{x^{x^x}}}}\ge x^x x^x = (1 + (x - 1))^x \ge 1 + (x - 1)x = x^2 - x + 1 \ge \frac12 x^2 + \frac12 0 < x < 1 x^{x^{x^{x^x}}}\ln x \ge \ln \frac{x^2 + 1}{2} x^{x^{x^{x^x}}} \le \frac{\ln \frac{x^2 + 1}{2}}{\ln x} x^{x^{x^x}}\ln x \le \ln \frac{\ln \frac{x^2 + 1}{2}}{\ln x} x^{x^{x^x}}\ge \frac{1}{\ln x}\ln \frac{\ln \frac{x^2 + 1}{2}}{\ln x}. x^{x^{x^x}}\ge \frac{7}{12} \ge \frac{1}{\ln x}\ln \frac{\ln \frac{x^2 + 1}{2}}{\ln x}. \tag{1} x^x \ge \mathrm{e}^{-1/\mathrm{e}}
\ge \frac{1}{\ln x}\ln\frac{\ln\frac{7}{12}}{\ln x}. f(x) = x^{7/12}\ln x - \ln \frac{x^2 + 1}{2} \begin{align*}
	f'(x) &= \frac{7}{12x^{5/12}}
	\left(\ln x + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7}\right)\\
	&\le \frac{7}{12x^{5/12}}
	\left(\frac{3x^2 - 3}{x^2 + 4x + 1} + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7}\right)\\
	&\le 0 \tag{2}
\end{align*} \ln x \le \frac{3x^2 - 3}{x^2 + 4x + 1} x (0, 1] f(1) = 0 f(x) \ge 0 x (0, 1) \frac{3x^2 - 3}{x^2 + 4x + 1} + \frac{12}{7} - \frac{24x^{17/12}}{7x^2 + 7} \le 0 0 < x < 1 x = y^{12} 11y^{47} + \cdots + 3 \ge 0 47 0 < y < 1","['real-analysis', 'calculus', 'inequality']"
75,$\frac{1}{p}+\frac{1}{q}=1$ vs $\sum_{n=0}^\infty \frac{1}{p^n}=q$,vs,\frac{1}{p}+\frac{1}{q}=1 \sum_{n=0}^\infty \frac{1}{p^n}=q,"It just occurred to me that conjugate exponents, i.e. $p,q\in(1,+\infty)$ such that $$\frac{1}{p}+\frac{1}{q} =1$$ also satisfy the relations: $\sum_{n=0}^\infty \frac{1}{p^n}=q;$ $\sum_{n=0}^\infty \frac{1}{q^n}=p.$ I never saw this fact used in the study of $L^p$ spaces... does anyone know any application of these relations in that context?","It just occurred to me that conjugate exponents, i.e. $p,q\in(1,+\infty)$ such that $$\frac{1}{p}+\frac{1}{q} =1$$ also satisfy the relations: $\sum_{n=0}^\infty \frac{1}{p^n}=q;$ $\sum_{n=0}^\infty \frac{1}{q^n}=p.$ I never saw this fact used in the study of $L^p$ spaces... does anyone know any application of these relations in that context?",,"['real-analysis', 'soft-question', 'lp-spaces']"
76,How can I prove that $xy\leq x^2+y^2$? [closed],How can I prove that ? [closed],xy\leq x^2+y^2,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How can I prove that $xy\leq x^2+y^2$ for all $x,y\in\mathbb{R}$ ?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How can I prove that $xy\leq x^2+y^2$ for all $x,y\in\mathbb{R}$ ?",,"['real-analysis', 'inequality']"
77,Is it necessary that every function is a derivative of some function?,Is it necessary that every function is a derivative of some function?,,I thought about this a lot and consulted a lot of people but everyone had contradicting answers. I am a high school student. please help.,I thought about this a lot and consulted a lot of people but everyone had contradicting answers. I am a high school student. please help.,,"['real-analysis', 'calculus', 'derivatives', 'examples-counterexamples']"
78,Non-trivial open dense subset of $\mathbb{R}$.,Non-trivial open dense subset of .,\mathbb{R},"I recently found the following exercise real analysis: Let $A\subseteq\mathbb{R}$ be open and dense. Show that   $$\mathbb{R}=\{x+y:x,y\in A\}$$ I think it is not too hard to prove. But do we have a non-trivial example of such a set? So my question is: Can we find an example of a subset $A\subset\mathbb{R}$ that is open and dense, but $A\neq \mathbb{R}$? I don't know a lot of examples of dense subset of $\mathbb{R}$. The rational and irrational numbers are dense, but clearly not open.","I recently found the following exercise real analysis: Let $A\subseteq\mathbb{R}$ be open and dense. Show that   $$\mathbb{R}=\{x+y:x,y\in A\}$$ I think it is not too hard to prove. But do we have a non-trivial example of such a set? So my question is: Can we find an example of a subset $A\subset\mathbb{R}$ that is open and dense, but $A\neq \mathbb{R}$? I don't know a lot of examples of dense subset of $\mathbb{R}$. The rational and irrational numbers are dense, but clearly not open.",,"['real-analysis', 'general-topology']"
79,A function that is $L^p$ for all $p$ but is not $L^\infty$?,A function that is  for all  but is not ?,L^p p L^\infty,"Let $X$ be the interval $[0,1]$ with Lebesgue measure. Is there a function $f\in L^p(X)$ for all $p\in[1,\infty)$ that is not $\in L^\infty(X)$? If so, what is an example? Motivation: In a course on measure theory this fall, I've learned proofs that $L^p(X)\supset L^q(X)$ if $q>p$ and that if $f\in L^\infty(X)$, then $\|f\|_\infty = \lim \limits_{p\to\infty} \|f\|_p$.  This prompted me to wonder if $L^\infty(X) = \bigcap _p L^p(X)$. A classmate gave me a general theoretical reason to believe the contrary: $L^p(X)$ is a reflexive space for $1 \lt p \lt \infty$ but not for $p=1,\infty$; but intersections of reflexive spaces are reflexive. This logic seems sound to me; but it implies the containment $L^\infty(X) \subset \bigcap_p L^p(X)$ is strict.  If so, there must be a function that is $L^p$ for all $p$ but not a.e. bounded. What is it?","Let $X$ be the interval $[0,1]$ with Lebesgue measure. Is there a function $f\in L^p(X)$ for all $p\in[1,\infty)$ that is not $\in L^\infty(X)$? If so, what is an example? Motivation: In a course on measure theory this fall, I've learned proofs that $L^p(X)\supset L^q(X)$ if $q>p$ and that if $f\in L^\infty(X)$, then $\|f\|_\infty = \lim \limits_{p\to\infty} \|f\|_p$.  This prompted me to wonder if $L^\infty(X) = \bigcap _p L^p(X)$. A classmate gave me a general theoretical reason to believe the contrary: $L^p(X)$ is a reflexive space for $1 \lt p \lt \infty$ but not for $p=1,\infty$; but intersections of reflexive spaces are reflexive. This logic seems sound to me; but it implies the containment $L^\infty(X) \subset \bigcap_p L^p(X)$ is strict.  If so, there must be a function that is $L^p$ for all $p$ but not a.e. bounded. What is it?",,"['real-analysis', 'measure-theory', 'functional-analysis']"
80,Function $f: \mathbb{R} \to \mathbb{R}$ that takes each value in $\mathbb{R}$ three times,Function  that takes each value in  three times,f: \mathbb{R} \to \mathbb{R} \mathbb{R},"Does there exist a function $f: \mathbb{R} \to \mathbb{R}$ that takes each value in $\mathbb{R}$ three times? If not, how could I prove that such a function does not exist?","Does there exist a function $f: \mathbb{R} \to \mathbb{R}$ that takes each value in $\mathbb{R}$ three times? If not, how could I prove that such a function does not exist?",,"['real-analysis', 'functions', 'examples-counterexamples']"
81,Lipschitz Continuous $\Rightarrow$ Uniformly Continuous,Lipschitz Continuous  Uniformly Continuous,\Rightarrow,"The Question : Prove that if a function $f$ defined on $S \subseteq \mathbb R$ is Lipschitz continuous then $f$ is uniformly continuous on $S$. Definition . A function $f$ defined on a set $S \subseteq \mathbb R$ is said to be Lipschitz continuous on $S$ if there exists an $M$ so that $$\frac{|f(x) - f(c)|}{|x - c|} \le M$$ for all $x$ and $c$ in $S$ such that $x \ne c$. My Heuristic Interpretation: if $f$ is Lipschitz continuous then the ""absolute slope"" of $f$ is never unbounded i.e. no asymptotes. Definition . A continuous function $f$ defined on $\mathrm{Dom}\, (f)$ is said to be uniformly continuous if for each $\varepsilon > 0 \ \exists \ \delta > 0$ s.t. $\forall \ x, c \in \mathrm{Dom}\, (f)$ $$ |x - c| \le \delta \ \Rightarrow \ |f(x) - f(c)| \le \varepsilon$$ Proof : $f$ Lipschitz continuous $\Rightarrow$ $|f(x) - f(c)| \le M|x - c|$. Since we suppose $|x - c| \le \delta$ for uniform continuity, we have $x$ within $\delta$ of $c$, so $|x| \le |c| + \delta$. So taking $\delta = \varepsilon/M$ \begin{align*} |f(x) - f(c)| &\le M|x - c| \\ & \le M\delta \\ & = \varepsilon \end{align*} My Question : Is my proof valid with the assumptions taken?","The Question : Prove that if a function $f$ defined on $S \subseteq \mathbb R$ is Lipschitz continuous then $f$ is uniformly continuous on $S$. Definition . A function $f$ defined on a set $S \subseteq \mathbb R$ is said to be Lipschitz continuous on $S$ if there exists an $M$ so that $$\frac{|f(x) - f(c)|}{|x - c|} \le M$$ for all $x$ and $c$ in $S$ such that $x \ne c$. My Heuristic Interpretation: if $f$ is Lipschitz continuous then the ""absolute slope"" of $f$ is never unbounded i.e. no asymptotes. Definition . A continuous function $f$ defined on $\mathrm{Dom}\, (f)$ is said to be uniformly continuous if for each $\varepsilon > 0 \ \exists \ \delta > 0$ s.t. $\forall \ x, c \in \mathrm{Dom}\, (f)$ $$ |x - c| \le \delta \ \Rightarrow \ |f(x) - f(c)| \le \varepsilon$$ Proof : $f$ Lipschitz continuous $\Rightarrow$ $|f(x) - f(c)| \le M|x - c|$. Since we suppose $|x - c| \le \delta$ for uniform continuity, we have $x$ within $\delta$ of $c$, so $|x| \le |c| + \delta$. So taking $\delta = \varepsilon/M$ \begin{align*} |f(x) - f(c)| &\le M|x - c| \\ & \le M\delta \\ & = \varepsilon \end{align*} My Question : Is my proof valid with the assumptions taken?",,"['real-analysis', 'continuity', 'solution-verification']"
82,Is there an integral for $\pi^4-\frac{2143}{22}$?,Is there an integral for ?,\pi^4-\frac{2143}{22},"In Ramanujan's Notebooks, Vol 4 , p.48 (and a related one in Quarterly Journal of Mathematics , XLV, 1914) there are various approximations, including the close (by just $10^{-7}$), $$\pi^4 \approx 2^4+3^4+\frac{1}{2+\Big(\frac{2}{3}\Big)^2} = \frac{2143}{22}$$ We can also easily calculate $\pi^3 \approx 31.0062$ and know that , $$ \int_0^1 \frac{x^4(1-x)^4}{1+x^2} \mathrm{d}x = \frac{22}{7} - \pi $$ $$ \int_0^1 \frac{x^8(1-x)^8(25+816x^2)}{28\cdot113(1+x^2)} \mathrm{d}x = \frac{355}{113} - \pi $$ Q: Are there then analogous integrals for these early approximations of $\pi^k$, $$ \int_0^1 f(x)\, \mathrm{d}x = \pi^2 -\frac{1+2\times113}{23} $$ $$ \int_0^1 f(x)\, \mathrm{d}x = \pi^3 - 31 $$ $$ \int_0^1 f(x)\, \mathrm{d}x = \pi^4 -\frac{2143}{22} $$","In Ramanujan's Notebooks, Vol 4 , p.48 (and a related one in Quarterly Journal of Mathematics , XLV, 1914) there are various approximations, including the close (by just $10^{-7}$), $$\pi^4 \approx 2^4+3^4+\frac{1}{2+\Big(\frac{2}{3}\Big)^2} = \frac{2143}{22}$$ We can also easily calculate $\pi^3 \approx 31.0062$ and know that , $$ \int_0^1 \frac{x^4(1-x)^4}{1+x^2} \mathrm{d}x = \frac{22}{7} - \pi $$ $$ \int_0^1 \frac{x^8(1-x)^8(25+816x^2)}{28\cdot113(1+x^2)} \mathrm{d}x = \frac{355}{113} - \pi $$ Q: Are there then analogous integrals for these early approximations of $\pi^k$, $$ \int_0^1 f(x)\, \mathrm{d}x = \pi^2 -\frac{1+2\times113}{23} $$ $$ \int_0^1 f(x)\, \mathrm{d}x = \pi^3 - 31 $$ $$ \int_0^1 f(x)\, \mathrm{d}x = \pi^4 -\frac{2143}{22} $$",,"['calculus', 'real-analysis', 'integration', 'approximation', 'pi']"
83,A question on Taylor Series and polynomial,A question on Taylor Series and polynomial,,"Suppose $ f(x)$ that is infinitely differentiable in $[a,b]$. For every $c\in[a,b] $ the series $\sum\limits_{n=0}^\infty \cfrac{f^{(n)}(c)}{n!}(x-c)^n $ is a polynomial. Is true that $f(x)$ is a polynomial? I can show it is true if for every $c\in [a,b]$, there exists a neighborhood $U_c$ of $c$, such that $$f(x)=\sum\limits_{n=0}^\infty \cfrac{f^{(n)}(c)}{n!}(x-c)^n\quad\text{for every }x\in U_c,$$ but, this equality is not always true. What can I do when $f(x)\not=\sum\limits_{n=0}^\infty \cfrac{f^{(n)}(c)}{n!}(x-c)^n$?","Suppose $ f(x)$ that is infinitely differentiable in $[a,b]$. For every $c\in[a,b] $ the series $\sum\limits_{n=0}^\infty \cfrac{f^{(n)}(c)}{n!}(x-c)^n $ is a polynomial. Is true that $f(x)$ is a polynomial? I can show it is true if for every $c\in [a,b]$, there exists a neighborhood $U_c$ of $c$, such that $$f(x)=\sum\limits_{n=0}^\infty \cfrac{f^{(n)}(c)}{n!}(x-c)^n\quad\text{for every }x\in U_c,$$ but, this equality is not always true. What can I do when $f(x)\not=\sum\limits_{n=0}^\infty \cfrac{f^{(n)}(c)}{n!}(x-c)^n$?",,"['calculus', 'real-analysis', 'sequences-and-series']"
84,What's the value of $\sum\limits_{k=1}^{\infty}\frac{k^2}{k!}$?,What's the value of ?,\sum\limits_{k=1}^{\infty}\frac{k^2}{k!},"For some series, it is easy to say whether it is convergent or not by the ""convergence test"", e.g., ratio test. However, it is nontrivial to calculate the value of the sum when the series converges. The question is motivated from the simple exercise to determining whether the series $\sum\limits_{k=1}^{\infty}\frac{k^2}{k!}$ is convergent. One may immediately get that it is convergent by the ratio test. So here is my question: What's the value of $$\sum_{k=1}^{\infty}\frac{k^2}{k!}?$$","For some series, it is easy to say whether it is convergent or not by the ""convergence test"", e.g., ratio test. However, it is nontrivial to calculate the value of the sum when the series converges. The question is motivated from the simple exercise to determining whether the series $\sum\limits_{k=1}^{\infty}\frac{k^2}{k!}$ is convergent. One may immediately get that it is convergent by the ratio test. So here is my question: What's the value of $$\sum_{k=1}^{\infty}\frac{k^2}{k!}?$$",,"['calculus', 'real-analysis']"
85,Intuitive proofs that $\lim\limits_{n\to\infty}\left(1+\frac xn\right)^n=e^x$,Intuitive proofs that,\lim\limits_{n\to\infty}\left(1+\frac xn\right)^n=e^x,"At this link someone asked how to prove rigorously that $$ \lim_{n\to\infty}\left(1+\frac xn\right)^n = e^x. $$ What good intuitive arguments exist for this statement? Later edit: .Â .Â .Â where $e$ is defined as the base of an exponential function equal to its own derivative. I will post my own answer, but that shouldn't deter anyone else from posting one as well.","At this link someone asked how to prove rigorously that $$ \lim_{n\to\infty}\left(1+\frac xn\right)^n = e^x. $$ What good intuitive arguments exist for this statement? Later edit: .Â .Â .Â where $e$ is defined as the base of an exponential function equal to its own derivative. I will post my own answer, but that shouldn't deter anyone else from posting one as well.",,"['calculus', 'real-analysis', 'exponential-function']"
86,"Is there a function whose inverse is exactly the reciprocal of the function, that is $f^{-1} = \frac{1}{f}$?","Is there a function whose inverse is exactly the reciprocal of the function, that is ?",f^{-1} = \frac{1}{f},Is there a function whose inverse is exactly the reciprocal of the function? That is $f^{-1} = \frac{1}{f}$. We know that the inverse of a function is not necessarily equal to its reciprocal in general.,Is there a function whose inverse is exactly the reciprocal of the function? That is $f^{-1} = \frac{1}{f}$. We know that the inverse of a function is not necessarily equal to its reciprocal in general.,,"['real-analysis', 'functions']"
87,Understanding the definition of nowhere dense sets in Abbott's Understanding Analysis,Understanding the definition of nowhere dense sets in Abbott's Understanding Analysis,,"First of all, I am sorry for asking a question about understanding a definition in a book named Understanding Analysis. But it is my first time to encounter basic topology, so I hope you can excuse me.  I have searched previous questions like this and this . I have looked to the wiki page. Still I am having a hard time to understand the following definition: A set $E$ is nowhere dense if $\overline E$ (the closure of $E$ )   contains no nonempty open intervals. I am not familiar with other concepts of topology which are not available in the Abbott's Understanding Analysis like balls or interior. I know a set $A$ is dense in $B$ if and only if $\overline A = B$ . For example, $\mathbb Q$ is dense in $\mathbb R$ , because its limit points are all real numbers and its closure gives $\mathbb R$ . Similarly, $\mathbb Z$ is not dense in $\mathbb R$ because it doesn't have limit points and hence its closure is itself. According to my knowledge of denseness, could you help me to understand the above definition with an example?","First of all, I am sorry for asking a question about understanding a definition in a book named Understanding Analysis. But it is my first time to encounter basic topology, so I hope you can excuse me.  I have searched previous questions like this and this . I have looked to the wiki page. Still I am having a hard time to understand the following definition: A set is nowhere dense if (the closure of )   contains no nonempty open intervals. I am not familiar with other concepts of topology which are not available in the Abbott's Understanding Analysis like balls or interior. I know a set is dense in if and only if . For example, is dense in , because its limit points are all real numbers and its closure gives . Similarly, is not dense in because it doesn't have limit points and hence its closure is itself. According to my knowledge of denseness, could you help me to understand the above definition with an example?",E \overline E E A B \overline A = B \mathbb Q \mathbb R \mathbb R \mathbb Z \mathbb R,['real-analysis']
88,How to prove limit of measurable functions is measurable,How to prove limit of measurable functions is measurable,,"I need help to prove the following theorem Suppose $f$ is the pointwise limit of a sequence of $f_n$, $n = 1, 2, \cdots$, where $f_n$ is a Borel measurable function on $X$. Then $f$ is Borel measurable on $X$. My idea is to use the standard definition like for every $c$,$\{x:f(x)<c\}$ is Borel measurable. But got stuck as how to do it for sequence of $f_n$.","I need help to prove the following theorem Suppose $f$ is the pointwise limit of a sequence of $f_n$, $n = 1, 2, \cdots$, where $f_n$ is a Borel measurable function on $X$. Then $f$ is Borel measurable on $X$. My idea is to use the standard definition like for every $c$,$\{x:f(x)<c\}$ is Borel measurable. But got stuck as how to do it for sequence of $f_n$.",,['real-analysis']
89,"Closed form of $\int_{0}^{\infty} \frac{\tanh(x)\,\tanh(2x)}{x^2}\;dx$",Closed form of,"\int_{0}^{\infty} \frac{\tanh(x)\,\tanh(2x)}{x^2}\;dx","I have homework to evaluate this integral $$I=\int_{0}^{\infty} \frac{\tanh(x)\,\tanh(2x)}{x^2}\;dx$$ Here is what I have done so far. I tried integration by parts using $u=\tanh(x)\,\tanh(2x)$ and $dv=\frac{dx}{x^2}$, I got $$\begin{align}\int_{0}^{\infty} \frac{\tanh(x)\,\tanh(2x)}{x^2}\;dx&=-\left.\frac{\tanh(x)\,\tanh(2x)}{x}\right|_{0}^{\infty}+2\int_{0}^{\infty}\frac{\tanh(2x)\,\text{sech}(2x)}{x}\;dx\\&=2\int_{0}^{\infty}\frac{\tanh(2x)\,\text{sech}(2x)}{x}\;dx\end{align}$$ At this part I'm stuck. I'm thinking of using Frullani's integral but I'm having trouble to find a relation as such $\tanh(2x)\,\text{sech}(2x)=f(ax)-f(bx)$. I also tried using differentiation under integral sign by considering $$I(a,b)=\int_{0}^{\infty} \frac{\tanh(ax)\,\tanh(bx)}{x^2}\;dx$$ then $$\frac{dI}{da}=\int_{0}^{\infty} \frac{\text{sech}^2(ax)\,\tanh(bx)}{x}\;dx=\int_{0}^{\infty} \frac{\tanh(bx)-\tanh^2(ax)\tanh(bx)}{x}\;dx$$ Again I tried to use Frullani's integral but I'm having trouble to find the sufficient $f(x)$. Integrating again with respect to $b$, I got $$\frac{d^2I}{da\;db}=\int_{0}^{\infty} \text{sech}^2(ax)\text{sech}^2(bx)\;dx$$ It's obviously a dead end to me. At this rate my friends and I contacted my professor to confirm whether the integral can be evaluated in terms of elementary functions or not because W|A cannot find it (I know that W|A cannot do everything). He only said, "" Sure! The answer is only 3 characters "" and then he left us. Assuming he is right, so $I$ must have a nice closed form, but I'm unable to find it. Would you help me? Any help would be appreciated. Thanks in advance.","I have homework to evaluate this integral $$I=\int_{0}^{\infty} \frac{\tanh(x)\,\tanh(2x)}{x^2}\;dx$$ Here is what I have done so far. I tried integration by parts using $u=\tanh(x)\,\tanh(2x)$ and $dv=\frac{dx}{x^2}$, I got $$\begin{align}\int_{0}^{\infty} \frac{\tanh(x)\,\tanh(2x)}{x^2}\;dx&=-\left.\frac{\tanh(x)\,\tanh(2x)}{x}\right|_{0}^{\infty}+2\int_{0}^{\infty}\frac{\tanh(2x)\,\text{sech}(2x)}{x}\;dx\\&=2\int_{0}^{\infty}\frac{\tanh(2x)\,\text{sech}(2x)}{x}\;dx\end{align}$$ At this part I'm stuck. I'm thinking of using Frullani's integral but I'm having trouble to find a relation as such $\tanh(2x)\,\text{sech}(2x)=f(ax)-f(bx)$. I also tried using differentiation under integral sign by considering $$I(a,b)=\int_{0}^{\infty} \frac{\tanh(ax)\,\tanh(bx)}{x^2}\;dx$$ then $$\frac{dI}{da}=\int_{0}^{\infty} \frac{\text{sech}^2(ax)\,\tanh(bx)}{x}\;dx=\int_{0}^{\infty} \frac{\tanh(bx)-\tanh^2(ax)\tanh(bx)}{x}\;dx$$ Again I tried to use Frullani's integral but I'm having trouble to find the sufficient $f(x)$. Integrating again with respect to $b$, I got $$\frac{d^2I}{da\;db}=\int_{0}^{\infty} \text{sech}^2(ax)\text{sech}^2(bx)\;dx$$ It's obviously a dead end to me. At this rate my friends and I contacted my professor to confirm whether the integral can be evaluated in terms of elementary functions or not because W|A cannot find it (I know that W|A cannot do everything). He only said, "" Sure! The answer is only 3 characters "" and then he left us. Assuming he is right, so $I$ must have a nice closed form, but I'm unable to find it. Would you help me? Any help would be appreciated. Thanks in advance.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
90,Is Inner product continuous when one arg is fixed?,Is Inner product continuous when one arg is fixed?,,"In a inner product space with inner product $\langle\ ,\ \rangle$ and real or complex line as its base field, for each point $x$ in the space, is $\langle x,-\rangle$ continuous function on the second argument, and is $\langle - ,x\rangle$ continuous function on the first argument? ""Continuous"" is defined respect to the topology induced by the inner product. Thanks and regards!","In a inner product space with inner product $\langle\ ,\ \rangle$ and real or complex line as its base field, for each point $x$ in the space, is $\langle x,-\rangle$ continuous function on the second argument, and is $\langle - ,x\rangle$ continuous function on the first argument? ""Continuous"" is defined respect to the topology induced by the inner product. Thanks and regards!",,"['real-analysis', 'general-topology', 'inner-products']"
91,Is there a monotonic function discontinuous over some dense set?,Is there a monotonic function discontinuous over some dense set?,,"Can we construct a monotonic function $f : \mathbb{R} \to \mathbb{R}$ such that there is a dense set in some interval $(a,b)$ for which $f$ is discontinuous at all points in the dense set?  What about a strictly monotonic function? My intuition tells me that such a function is impossible. Here is a rough sketch of an attempt at proving that such a function does not exist: we could suppose a function satisfies these conditions.  Take an $\epsilon > 0$ and two points $x,y$ in this dense set such that $x<y$.  Then, $f(x)<f(y)$ because if they are equal, then the function is constant at all points in between, and there is another element of $X$ between $x$ and $y$, which would be a contradiction.  Take $f(y)-f(x)$.  By the Archimedean property of the reals, $f(y)-f(x)<n\epsilon$ for some $n$. However, after this point, I am stuck.  Could we somehow partition $(x,y)$ into $n$ subintervals and conclude that there must be some point on the dense set that is continuous?","Can we construct a monotonic function $f : \mathbb{R} \to \mathbb{R}$ such that there is a dense set in some interval $(a,b)$ for which $f$ is discontinuous at all points in the dense set?  What about a strictly monotonic function? My intuition tells me that such a function is impossible. Here is a rough sketch of an attempt at proving that such a function does not exist: we could suppose a function satisfies these conditions.  Take an $\epsilon > 0$ and two points $x,y$ in this dense set such that $x<y$.  Then, $f(x)<f(y)$ because if they are equal, then the function is constant at all points in between, and there is another element of $X$ between $x$ and $y$, which would be a contradiction.  Take $f(y)-f(x)$.  By the Archimedean property of the reals, $f(y)-f(x)<n\epsilon$ for some $n$. However, after this point, I am stuck.  Could we somehow partition $(x,y)$ into $n$ subintervals and conclude that there must be some point on the dense set that is continuous?",,"['real-analysis', 'continuity']"
92,"$f$ uniformly continuous and $\int_a^\infty f(x)\,dx$ converges imply $\lim_{x \to \infty} f(x) = 0$ [closed]",uniformly continuous and  converges imply  [closed],"f \int_a^\infty f(x)\,dx \lim_{x \to \infty} f(x) = 0","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Trying to solve $f(x)$ is uniformly continuous in the range of $[0, +\infty)$ and $\int_a^\infty f(x)dx  $ converges. I need to prove that: $$\lim \limits_{x \to \infty} f(x) = 0$$ Would appreciate your help!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Trying to solve $f(x)$ is uniformly continuous in the range of $[0, +\infty)$ and $\int_a^\infty f(x)dx  $ converges. I need to prove that: $$\lim \limits_{x \to \infty} f(x) = 0$$ Would appreciate your help!",,"['real-analysis', 'analysis', 'integration', 'limits']"
93,What is a limit point,What is a limit point,,"Wikipedia seems to describe the topic with extreme complexity for me. In mathematics, a limit point of a set $S$ in a topological space $X$ is a   point $x$ (which is in $X$, but not necessarily in $S$) that can be   ""approximated"" by points of $S$ in the sense that every neighbourhood of   $x$ with respect to the topology on $X$ also contains a point of $S$ other   than $x$ itself. Note that $x$ does not have to be an element of $S$. I don't understand the relationship between $S$ and $X$ and what it means for $x$ to be a limit point.","Wikipedia seems to describe the topic with extreme complexity for me. In mathematics, a limit point of a set $S$ in a topological space $X$ is a   point $x$ (which is in $X$, but not necessarily in $S$) that can be   ""approximated"" by points of $S$ in the sense that every neighbourhood of   $x$ with respect to the topology on $X$ also contains a point of $S$ other   than $x$ itself. Note that $x$ does not have to be an element of $S$. I don't understand the relationship between $S$ and $X$ and what it means for $x$ to be a limit point.",,"['real-analysis', 'general-topology', 'analysis']"
94,The composition of two convex functions is convex,The composition of two convex functions is convex,,"Let $f$ be a convex function on a convex domain $\Omega$ and $g$ a convex non-decreasing function on $\mathbb{R}$. prove that the composition of $g(f)$ is convex on $\Omega$. Under what conditions is $g(f)$ strictly convex. My attempt, since $f$ is convex, $$f([1-t]x_0 +ty_0)\le [1-t]f(x_0) + tf(y_0)\:,\quad  t \in [0,1]  \,\text{and} \: x_0,y_0\in \Omega$$  Since $g$ is convex $$g([1-s]x_1 +sy_1) \le [1-s]g(x_1) + sg(y_1)\:,\quad s \in [0,1]\:and \: x_1,y_1 \in \mathbb{R}$$  So $$g([1-s]f([1-t]x_2 +ty_2) +sf([1-t]x_2 +ty_2)) \\\le [1-s]g([1-t]f(x_2) + tf(y_2)) + sg([1-t]f(x_3) + tf(y_3))\: for\:x_2,y_2,x_3,y_3 \in \Omega.$$ Im not sure if this is always true. Any help would be appreciated.  Thanks","Let $f$ be a convex function on a convex domain $\Omega$ and $g$ a convex non-decreasing function on $\mathbb{R}$. prove that the composition of $g(f)$ is convex on $\Omega$. Under what conditions is $g(f)$ strictly convex. My attempt, since $f$ is convex, $$f([1-t]x_0 +ty_0)\le [1-t]f(x_0) + tf(y_0)\:,\quad  t \in [0,1]  \,\text{and} \: x_0,y_0\in \Omega$$  Since $g$ is convex $$g([1-s]x_1 +sy_1) \le [1-s]g(x_1) + sg(y_1)\:,\quad s \in [0,1]\:and \: x_1,y_1 \in \mathbb{R}$$  So $$g([1-s]f([1-t]x_2 +ty_2) +sf([1-t]x_2 +ty_2)) \\\le [1-s]g([1-t]f(x_2) + tf(y_2)) + sg([1-t]f(x_3) + tf(y_3))\: for\:x_2,y_2,x_3,y_3 \in \Omega.$$ Im not sure if this is always true. Any help would be appreciated.  Thanks",,"['real-analysis', 'multivariable-calculus', 'optimization', 'convex-analysis', 'nonlinear-optimization']"
95,"If a continuous function is positive on the rationals, is it positive almost everywhere?","If a continuous function is positive on the rationals, is it positive almost everywhere?",,"I made up this question, but unable to solve it: Let $f : \mathbb R \to \mathbb R$ be a continuous function such that $f(x) > 0$ for all $x \in \mathbb Q$. Is it necessary that $f(x) > 0$ almost everywhere? This is my attempt. It is easy to show that $f(x) \geq 0$ everywhere, so the real question is whether $f$ can be zero at an ""almost all"" irrational points. The function can become $0$ at isolated points, e.g., $f(x) = (x - \sqrt{2})^2$. In particular, the qualification ""almost"" is necessary for the question to be nontrivial. Every rational point has an open neighborhood where $f$ is positive. Hence at least know that the set $\{ x \,:\, f(x) > 0 \}$ is not a measure-zero set. I first mistakenly assumed that Thomae's function provides a counter-example to this. Indeed, it is positive at all rationals and zero at all irrationals, but the function is continuous at only the irrationals, not everywhere. Then I tried to prove that the question has an affirmative answer, but do not have much progress there. Please suggest some hints!","I made up this question, but unable to solve it: Let $f : \mathbb R \to \mathbb R$ be a continuous function such that $f(x) > 0$ for all $x \in \mathbb Q$. Is it necessary that $f(x) > 0$ almost everywhere? This is my attempt. It is easy to show that $f(x) \geq 0$ everywhere, so the real question is whether $f$ can be zero at an ""almost all"" irrational points. The function can become $0$ at isolated points, e.g., $f(x) = (x - \sqrt{2})^2$. In particular, the qualification ""almost"" is necessary for the question to be nontrivial. Every rational point has an open neighborhood where $f$ is positive. Hence at least know that the set $\{ x \,:\, f(x) > 0 \}$ is not a measure-zero set. I first mistakenly assumed that Thomae's function provides a counter-example to this. Indeed, it is positive at all rationals and zero at all irrationals, but the function is continuous at only the irrationals, not everywhere. Then I tried to prove that the question has an affirmative answer, but do not have much progress there. Please suggest some hints!",,"['real-analysis', 'measure-theory', 'examples-counterexamples']"
96,To show that the set point distant by 1 of a compact set has Lebesgue measure $0$,To show that the set point distant by 1 of a compact set has Lebesgue measure,0,"Could any one tell me how to solve this one? Let $K$ be a compact subset of $\mathbb{R}^n$, and $$A:=\{x\in\mathbb{R}^n:d(x,K)=1\}.$$ Show that $A$ has Lebesgue measure $0$. Thank you!","Could any one tell me how to solve this one? Let $K$ be a compact subset of $\mathbb{R}^n$, and $$A:=\{x\in\mathbb{R}^n:d(x,K)=1\}.$$ Show that $A$ has Lebesgue measure $0$. Thank you!",,"['real-analysis', 'measure-theory', 'compactness', 'geometric-measure-theory']"
97,A finite set always has a maximum and a minimum.,A finite set always has a maximum and a minimum.,,"I am pretty confident that this statement is true. However, I am not sure how to prove it. Any hints/ideas/answers would be appreciated.","I am pretty confident that this statement is true. However, I am not sure how to prove it. Any hints/ideas/answers would be appreciated.",,"['real-analysis', 'elementary-set-theory', 'order-theory']"
98,Prove that $(1+x)^\frac{1}{x}+(1+\frac{1}{x})^x \leq 4$,Prove that,(1+x)^\frac{1}{x}+(1+\frac{1}{x})^x \leq 4,"Prove that $f(x)=(1+x)^\frac{1}{x}+(1+\frac{1}{x})^x \leq 4$ for all $x>0.$ We have $f(x)=f(\frac{1}{x}), f'(x)=-\frac{1}{x^2}f'(\frac{1}{x}),$ so we only need to prove $f'(x)ï¼0$ for $0 < x < 1.$","Prove that $f(x)=(1+x)^\frac{1}{x}+(1+\frac{1}{x})^x \leq 4$ for all $x>0.$ We have $f(x)=f(\frac{1}{x}), f'(x)=-\frac{1}{x^2}f'(\frac{1}{x}),$ so we only need to prove $f'(x)ï¼0$ for $0 < x < 1.$",,"['real-analysis', 'inequality', 'exponential-function', 'maxima-minima', 'jensen-inequality']"
99,From $e^n$ to $e^x$,From  to,e^n e^x,"Solve for $f: \mathbb{R}\to\mathbb{R}\ \ \ $        s.t. $$f(n)=e^n \ \ \forall n\in\mathbb{N}$$ $$f^{(y)}(x)>0 \ \forall y\in\mathbb{N^*} \ \forall x\in\mathbb R$$ Could you please prove that there exists an unique solution: $f(x)=e^x$? (Anyway, this problem is not about fractional calculus) $\mathbb N^*=\{1,2,3...\}, \ \mathbb N=\{0,1,2....\}$ How about try to construct a few functional spaces that intersect at one point? Try Sard Theorem and Pre image Theorem.","Solve for $f: \mathbb{R}\to\mathbb{R}\ \ \ $        s.t. $$f(n)=e^n \ \ \forall n\in\mathbb{N}$$ $$f^{(y)}(x)>0 \ \forall y\in\mathbb{N^*} \ \forall x\in\mathbb R$$ Could you please prove that there exists an unique solution: $f(x)=e^x$? (Anyway, this problem is not about fractional calculus) $\mathbb N^*=\{1,2,3...\}, \ \mathbb N=\{0,1,2....\}$ How about try to construct a few functional spaces that intersect at one point? Try Sard Theorem and Pre image Theorem.",,"['real-analysis', 'algebra-precalculus', 'functional-analysis', 'exponential-function', 'differential-topology']"
