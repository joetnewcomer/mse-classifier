,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proving a determinant of a particular type is always null,Proving a determinant of a particular type is always null,,"Let $n\ge 1$ and $A,B\in\mathrm M_n(\mathbb R)$ . Let's assume that $$\forall Q\in\mathrm M_n(\mathbb R), \quad \det\begin{pmatrix} I_n & A \\ Q & B\end{pmatrix}=0$$ where $I_n$ is the identity matrix of $\mathrm M_n(\mathbb R)$ . Can we prove that $\mathrm{rank} \begin{pmatrix}A\\ B\end{pmatrix}<n$ ? This fact seems quite obvious, but I can't find any straightforward argument to prove it. Some ideas. With $Q=0$ , we deal with a block-triangular matrix, so we have $\det B=0$ . Moreover, with $Q=\lambda I_n$ , $\lambda\in\mathbb R$ , since it commutes with $B$ , we have $$\forall \lambda\in\mathbb R,\quad \det(B-\lambda A)=0,$$ so if $\det(A)\ne 0$ , we have $$\forall \lambda\in\mathbb R,\quad\det((BA-\lambda I_n)A^{-1})=\det(BA-\lambda I_n)\det(A)^{-1}=0,$$ which means that every $\lambda\in\mathbb R$ is an eigenvalue of $BA$ (since for all $\lambda\in\mathbb R$ , $\det(BA-\lambda I_n)=0$ ), which is absurd. So $\det(A)=0$ also.","Let and . Let's assume that where is the identity matrix of . Can we prove that ? This fact seems quite obvious, but I can't find any straightforward argument to prove it. Some ideas. With , we deal with a block-triangular matrix, so we have . Moreover, with , , since it commutes with , we have so if , we have which means that every is an eigenvalue of (since for all , ), which is absurd. So also.","n\ge 1 A,B\in\mathrm M_n(\mathbb R) \forall Q\in\mathrm M_n(\mathbb R), \quad \det\begin{pmatrix} I_n & A \\ Q & B\end{pmatrix}=0 I_n \mathrm M_n(\mathbb R) \mathrm{rank} \begin{pmatrix}A\\ B\end{pmatrix}<n Q=0 \det B=0 Q=\lambda I_n \lambda\in\mathbb R B \forall \lambda\in\mathbb R,\quad \det(B-\lambda A)=0, \det(A)\ne 0 \forall \lambda\in\mathbb R,\quad\det((BA-\lambda I_n)A^{-1})=\det(BA-\lambda I_n)\det(A)^{-1}=0, \lambda\in\mathbb R BA \lambda\in\mathbb R \det(BA-\lambda I_n)=0 \det(A)=0","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
1,How to factorize this determinant?,How to factorize this determinant?,,"The question is to factorize $$\det\begin{pmatrix}(x^2+1)^2 & (xy+1)^2 & (xz+1)^2  \\ (xy+1)^2 & (y^2+1)^2 & (yz+1)^2 \\ (xz+1)^2 & (yz+1)^2 & (z^2+1)^2  \end{pmatrix}.$$ I have a hint which is considering the factorization of $\det\begin{pmatrix}1 & a & a^2  \\ 1 & b & b^2 \\ 1 & c & c^2  \end{pmatrix}$ where all entries are real numbers. I don't have any idea how to use the hint. So, I factorize $\det\begin{pmatrix}(x^2+1)^2 & (xy+1)^2 & (xz+1)^2  \\ (xy+1)^2 & (y^2+1)^2 & (yz+1)^2 \\ (xz+1)^2 & (yz+1)^2 & (z^2+1)^2  \end{pmatrix}$ directly and I get the answer which is $2(z-y)^2(z-x)^2(y-x)^2$ . My question is how to use the hint to factorize the given determinant? It is because my method seems very tedious.","The question is to factorize I have a hint which is considering the factorization of where all entries are real numbers. I don't have any idea how to use the hint. So, I factorize directly and I get the answer which is . My question is how to use the hint to factorize the given determinant? It is because my method seems very tedious.",\det\begin{pmatrix}(x^2+1)^2 & (xy+1)^2 & (xz+1)^2  \\ (xy+1)^2 & (y^2+1)^2 & (yz+1)^2 \\ (xz+1)^2 & (yz+1)^2 & (z^2+1)^2  \end{pmatrix}. \det\begin{pmatrix}1 & a & a^2  \\ 1 & b & b^2 \\ 1 & c & c^2  \end{pmatrix} \det\begin{pmatrix}(x^2+1)^2 & (xy+1)^2 & (xz+1)^2  \\ (xy+1)^2 & (y^2+1)^2 & (yz+1)^2 \\ (xz+1)^2 & (yz+1)^2 & (z^2+1)^2  \end{pmatrix} 2(z-y)^2(z-x)^2(y-x)^2,"['linear-algebra', 'matrices', 'determinant', 'factoring']"
2,Inverse of any element in a group of matrices [duplicate],Inverse of any element in a group of matrices [duplicate],,"This question already has an answer here : Prove that matrices of the form $\begin{pmatrix} x & x \\ x & x \end{pmatrix}$ are a group under matrix multiplication. (1 answer) Closed 4 years ago . I came across the following question in an under graduate course examination: Let $G$ be a group of all matrices of the form $  \pmatrix{x&x\\x&x}$ , where $0\neq x\in \Bbb R$ under matrix multiplication. Find the inverse of any element in $G$ . It appears to me that the question is wrongly framed because any matrix of the given form doesn't have inverse due to the fact that its determinant is zero.","This question already has an answer here : Prove that matrices of the form $\begin{pmatrix} x & x \\ x & x \end{pmatrix}$ are a group under matrix multiplication. (1 answer) Closed 4 years ago . I came across the following question in an under graduate course examination: Let be a group of all matrices of the form , where under matrix multiplication. Find the inverse of any element in . It appears to me that the question is wrongly framed because any matrix of the given form doesn't have inverse due to the fact that its determinant is zero.",G   \pmatrix{x&x\\x&x} 0\neq x\in \Bbb R G,"['matrices', 'group-theory']"
3,Is every solution of this matrix equation diagonal?,Is every solution of this matrix equation diagonal?,,"Let $A$ be a real $n \times n$ matrix with positive determinant. Suppose that $A$ is diagonal , and that $ A=S-\lambda (S^T)^{-1}$ , where $S$ is another invertible real $n \times n$ matrix, and $\lambda$ is some real number. Must $S$ be diagonal? Edit: It suffices to prove that $S$ is symmetric, at least in the generic case where $A$ has distinct diagonal entries. Indeed, the assumption is equivalent to $S^T(S-A)=\lambda I$ , hence $$S^T(S-A)=\lambda I=(\lambda I)^T=(S^T(S-A))^T=(S^T-A^T)S,$$ i.e. $ S^TA=A^TS$ . Since $A$ is symmetric, we obtain $ S^TA=AS$ . Thus, if we assume $S^T=S$ , then $S$ commutes with $A$ , hence it must be diagonal.","Let be a real matrix with positive determinant. Suppose that is diagonal , and that , where is another invertible real matrix, and is some real number. Must be diagonal? Edit: It suffices to prove that is symmetric, at least in the generic case where has distinct diagonal entries. Indeed, the assumption is equivalent to , hence i.e. . Since is symmetric, we obtain . Thus, if we assume , then commutes with , hence it must be diagonal.","A n \times n A  A=S-\lambda (S^T)^{-1} S n \times n \lambda S S A S^T(S-A)=\lambda I S^T(S-A)=\lambda I=(\lambda I)^T=(S^T(S-A))^T=(S^T-A^T)S,  S^TA=A^TS A  S^TA=AS S^T=S S A","['linear-algebra', 'matrices', 'matrix-equations', 'block-matrices']"
4,if negative determinant -> not positive semidefinite,if negative determinant -> not positive semidefinite,,"If $\det(A)$ is $< 0$ , then $A$ is not a positive semidefinite matrix. How can I prove that statement?","If is , then is not a positive semidefinite matrix. How can I prove that statement?",\det(A) < 0 A,"['linear-algebra', 'matrices', 'determinant', 'positive-definite', 'positive-semidefinite']"
5,"Suppose $U_1,\dots,U_k$ and $V_1,\dots,V_k$ are $n\times n$ unitary matrices. Show that $\|U_1\cdots U_k-V_1\cdots V_k\|\leq\sum_{i=1}^k\|U_i-V_i\|$",Suppose  and  are  unitary matrices. Show that,"U_1,\dots,U_k V_1,\dots,V_k n\times n \|U_1\cdots U_k-V_1\cdots V_k\|\leq\sum_{i=1}^k\|U_i-V_i\|","Let $V,W$ be  complex inner product spaces. Suppose $T: V \to W$ is a linear map, then we define $$\|T\|:=\sup\{\|Tv\|_{W}:\|v\|_{V}=1\}$$ where $\|v\_{V}\|:=\sqrt{\langle v,v\rangle}$ and $\|Tv\|_{W}:=\sqrt{\langle Tv,Tv\rangle}$ . Question: Suppose $U_1,\ldots,U_k$ and $V_1,\ldots,V_k$ are $n {\times} n$ unitary matrices. Show that $$\|U_1\cdots U_k-V_1\cdots V_k\| \leq \sum_{i=1}^{k}\|U_i-V_i\|$$ I have tried to use triangle inequality for norms and induction but failed. Can anyone give some hints? Thank you!","Let be  complex inner product spaces. Suppose is a linear map, then we define where and . Question: Suppose and are unitary matrices. Show that I have tried to use triangle inequality for norms and induction but failed. Can anyone give some hints? Thank you!","V,W T: V \to W \|T\|:=\sup\{\|Tv\|_{W}:\|v\|_{V}=1\} \|v\_{V}\|:=\sqrt{\langle v,v\rangle} \|Tv\|_{W}:=\sqrt{\langle Tv,Tv\rangle} U_1,\ldots,U_k V_1,\ldots,V_k n {\times} n \|U_1\cdots U_k-V_1\cdots V_k\| \leq \sum_{i=1}^{k}\|U_i-V_i\|","['linear-algebra', 'matrices', 'functional-analysis', 'normed-spaces']"
6,Derivative with respect to diagonal of diagonal matrix,Derivative with respect to diagonal of diagonal matrix,,"Suppose I have a diagonal matrix $\pmb{D}$ and a symmetric matrix $\pmb{X}$ that is not a function of $\pmb{D}$ , and I wish to find the following derivative: $$ \frac{\partial}{\partial \mathrm{diag}(\pmb{D})} \mathrm{vec}\left(\pmb{D}\pmb{X}\pmb{D}\right), $$ in which $\mathrm{diag}(\pmb{D})$ represents the diagonal of $\pmb{D}$ . I know the following derivative: $$ \frac{\partial}{\partial \mathrm{vec}(\pmb{D})} \mathrm{vec}\left(\pmb{D}\pmb{X}\pmb{D}\right) = \left(\pmb{D}\pmb{X} \otimes \pmb{I} \right) + \left(\pmb{I} \otimes \pmb{X}\pmb{D}\right) $$ So I guess I can find the answer by multyplying this with $\frac{\partial \mathrm{vec}\left( \pmb{D} \right)}{\partial \mathrm{diag}(\pmb{D})}$ , which should be some straightforward matrix with zeroes and ones. So my question is, (a) does this matrix have a name and can it easily be determined? and (b) isn't there some simpler way to do this?","Suppose I have a diagonal matrix and a symmetric matrix that is not a function of , and I wish to find the following derivative: in which represents the diagonal of . I know the following derivative: So I guess I can find the answer by multyplying this with , which should be some straightforward matrix with zeroes and ones. So my question is, (a) does this matrix have a name and can it easily be determined? and (b) isn't there some simpler way to do this?","\pmb{D} \pmb{X} \pmb{D} 
\frac{\partial}{\partial \mathrm{diag}(\pmb{D})} \mathrm{vec}\left(\pmb{D}\pmb{X}\pmb{D}\right),
 \mathrm{diag}(\pmb{D}) \pmb{D} 
\frac{\partial}{\partial \mathrm{vec}(\pmb{D})} \mathrm{vec}\left(\pmb{D}\pmb{X}\pmb{D}\right) = \left(\pmb{D}\pmb{X} \otimes \pmb{I} \right) + \left(\pmb{I} \otimes \pmb{X}\pmb{D}\right)
 \frac{\partial \mathrm{vec}\left( \pmb{D} \right)}{\partial \mathrm{diag}(\pmb{D})}","['matrices', 'derivatives', 'vectors', 'matrix-calculus', 'jacobian']"
7,Prove Nice Determinant Equations [duplicate],Prove Nice Determinant Equations [duplicate],,"This question already has an answer here : Computing determinant without expansion (1 answer) Closed 4 years ago . I often come across these kinds of problems in A-level exam papers: Prove that $$ \begin{vmatrix}     (a+b)^2 & 1       & 1      \\     a^2     & (1+b)^2 & a^2    \\     b^2     & b^2     & (1+a)^2  \end{vmatrix}  = 2ab(1+a+b)^3 $$ or Prove that $$ \begin{vmatrix}     1  & 1  & 1 \\     x  & y  & z \\     yz & xz & xy  \end{vmatrix}  = (x-y)(y-z)(z-x) $$ Questions which involve nice determinant results, but which are quite a pain to prove by expanding. Are there any tricks that one can use to prove such results? I'm familiar with Vandermonde matrices for example, but I haven't come across anything that might help me with these, especially the first one.","This question already has an answer here : Computing determinant without expansion (1 answer) Closed 4 years ago . I often come across these kinds of problems in A-level exam papers: Prove that or Prove that Questions which involve nice determinant results, but which are quite a pain to prove by expanding. Are there any tricks that one can use to prove such results? I'm familiar with Vandermonde matrices for example, but I haven't come across anything that might help me with these, especially the first one.","
\begin{vmatrix}
    (a+b)^2 & 1       & 1      \\
    a^2     & (1+b)^2 & a^2    \\
    b^2     & b^2     & (1+a)^2 
\end{vmatrix} 
= 2ab(1+a+b)^3
 
\begin{vmatrix}
    1  & 1  & 1 \\
    x  & y  & z \\
    yz & xz & xy 
\end{vmatrix} 
= (x-y)(y-z)(z-x)
","['linear-algebra', 'matrices', 'determinant']"
8,All nilpotent $2 \times 2$ matrices satisfy $A^{2}=0$,All nilpotent  matrices satisfy,2 \times 2 A^{2}=0,I have problems to show that if $A$ is a $2 \times 2$ matrix and if there exists some positive integer such that $A^{n}=0$ then $A^{2}=0$ . I only showed that $A$ is a singular matrix but nothing else. Thanks any help will be appreciated.,I have problems to show that if is a matrix and if there exists some positive integer such that then . I only showed that is a singular matrix but nothing else. Thanks any help will be appreciated.,A 2 \times 2 A^{n}=0 A^{2}=0 A,"['linear-algebra', 'matrices', 'nilpotence']"
9,Frobenius Norm Inequality; Spectral Radius is smaller than Frobenius Norm,Frobenius Norm Inequality; Spectral Radius is smaller than Frobenius Norm,,"Let $A \in \mathbb{R}^{n \times m}$ and $x \in \mathbb{R}^n$ . Prove the following inequality. $\left\lVert \cdot \right\rVert_F$ denotes the Frobenius norm and $\left\lVert \cdot \right\rVert_2$ denotes the $p$ -norm with $p=2$ . $$\left\lVert Ax \right\rVert_2 \leq \left\lVert A \right\rVert_F \left\lVert x \right\rVert_2$$ tl;dr: I'm essentially stuck at this (or similar) inequality: $$ \lambda_{max} (A^T \cdot A) \leq \left\lVert A^T \cdot A \right\rVert_F$$ while $\lambda_{max}$ is the largest eigenvalue of $A^T \cdot A$ . I know that this inequality holds if the norm was a natural norm, but since frobenius norm isn't induced by a vector norm, I'm not sure how to proceed. How I got to this point: $$\left\lVert Ax \right\rVert_2 \leq \left\lVert A \right\rVert_2 \left\lVert x \right\rVert_2$$ So we have to show: $$\left\lVert A \right\rVert_2 \leq \left\lVert A \right\rVert_F$$ or $$\left\lVert A \right\rVert_2^2 \leq \left\lVert A \right\rVert_F^{2}$$ We have: $$\left\lVert A \right\rVert_2^2 = \lambda_{max}(A^TA) \leq \left\lVert A^T A \right\rVert_F$$ The last inequality is the part I can't prove. If I could show it, we have: $$\left\lVert A^T A \right\rVert_F \leq \left\lVert A^T \right\rVert_F \left\lVert A \right\rVert_F = \left\lVert A \right\rVert_F^2$$ Which was the thing we wanted to show above. These threads were helpful: Show that $ \lVert A \rVert_2^2 \leq \lVert A \rVert _1 \lVert A \rVert _ \infty $ The spectral radius of the matrix $A$ is less than or equal any natural norm Anyways, thank you for your help. It is greatly appreciated.","Let and . Prove the following inequality. denotes the Frobenius norm and denotes the -norm with . tl;dr: I'm essentially stuck at this (or similar) inequality: while is the largest eigenvalue of . I know that this inequality holds if the norm was a natural norm, but since frobenius norm isn't induced by a vector norm, I'm not sure how to proceed. How I got to this point: So we have to show: or We have: The last inequality is the part I can't prove. If I could show it, we have: Which was the thing we wanted to show above. These threads were helpful: Show that The spectral radius of the matrix is less than or equal any natural norm Anyways, thank you for your help. It is greatly appreciated.",A \in \mathbb{R}^{n \times m} x \in \mathbb{R}^n \left\lVert \cdot \right\rVert_F \left\lVert \cdot \right\rVert_2 p p=2 \left\lVert Ax \right\rVert_2 \leq \left\lVert A \right\rVert_F \left\lVert x \right\rVert_2  \lambda_{max} (A^T \cdot A) \leq \left\lVert A^T \cdot A \right\rVert_F \lambda_{max} A^T \cdot A \left\lVert Ax \right\rVert_2 \leq \left\lVert A \right\rVert_2 \left\lVert x \right\rVert_2 \left\lVert A \right\rVert_2 \leq \left\lVert A \right\rVert_F \left\lVert A \right\rVert_2^2 \leq \left\lVert A \right\rVert_F^{2} \left\lVert A \right\rVert_2^2 = \lambda_{max}(A^TA) \leq \left\lVert A^T A \right\rVert_F \left\lVert A^T A \right\rVert_F \leq \left\lVert A^T \right\rVert_F \left\lVert A \right\rVert_F = \left\lVert A \right\rVert_F^2  \lVert A \rVert_2^2 \leq \lVert A \rVert _1 \lVert A \rVert _ \infty  A,"['linear-algebra', 'matrices', 'inequality', 'matrix-norms', 'spectral-radius']"
10,Sparsest similar matrix,Sparsest similar matrix,,"Given a square matrix A (say with complex entries), which is the sparsest matrix which is similar to A? I guess it has to be its Jordan normal form but I am not sure. Remarks: A matrix is sparser than other if it has less nonzero entries. Two square $n \times n$ matrices $A,C$ are similar if there exists and invertible matrix $P$ such that $A = P^{-1}CP$","Given a square matrix A (say with complex entries), which is the sparsest matrix which is similar to A? I guess it has to be its Jordan normal form but I am not sure. Remarks: A matrix is sparser than other if it has less nonzero entries. Two square matrices are similar if there exists and invertible matrix such that","n \times n A,C P A = P^{-1}CP","['linear-algebra', 'matrices', 'sparse-matrices']"
11,What can be added to a non positive definite matrix to make it positive definite?,What can be added to a non positive definite matrix to make it positive definite?,,"I'm reading about the Newton Algorithm for optimization, and when the hessian is not positive definite, it says that I can add a matrix $E_k$ such that $B_k = \nabla^2 f(x_k) + E_k$ is sufficiently positive definite. What does that mean? How does $E_k$ looks like?","I'm reading about the Newton Algorithm for optimization, and when the hessian is not positive definite, it says that I can add a matrix such that is sufficiently positive definite. What does that mean? How does looks like?",E_k B_k = \nabla^2 f(x_k) + E_k E_k,"['linear-algebra', 'matrices', 'optimization', 'nonlinear-optimization']"
12,$A^TA$ eigenvalues,eigenvalues,A^TA,"Let $A\in M_{1,3}(\mathbb{R})$ be any $1\times 3$ matrix. Find the eigenvalues of $A^{T}A$ . I understand that this will obviously be a symmetric matrix, but I still can't see the solution. Also, is there any way to see the eigenvalues of symmetric matrices in general? EDIT: I tried some examples and it turns out that the first two eigenvalues are 0 and 0 and the third one is $$a_{11}^{2}+a_{12}^{2}+a_{13}^{2}.$$ Any way to show this is true in general?","Let be any matrix. Find the eigenvalues of . I understand that this will obviously be a symmetric matrix, but I still can't see the solution. Also, is there any way to see the eigenvalues of symmetric matrices in general? EDIT: I tried some examples and it turns out that the first two eigenvalues are 0 and 0 and the third one is Any way to show this is true in general?","A\in M_{1,3}(\mathbb{R}) 1\times 3 A^{T}A a_{11}^{2}+a_{12}^{2}+a_{13}^{2}.","['linear-algebra', 'matrices']"
13,How can I solve for an unknown vector given an otherwise known cross product?,How can I solve for an unknown vector given an otherwise known cross product?,,"Given known vectors $\vec a$ and $\vec b$, is it possible to solve for $\vec u$, given the following equation? $\vec a \times \vec u = \vec b$ So far I have found that the following must be true $\vec u = \vec c + \lambda \hat a$ where $\vec c = \frac{b}{a} (\hat b \times \hat a) = \frac{\vec b \times \vec a}{a^2}$ $\lambda \in \Bbb R $ but I'm unsure whether it is possible to further determine $\vec u$, since any vector $\vec u$ such that $(\vec u \cdot \hat c) \cdot \hat c = \vec c$ should do the trick, which means that $\lambda$ can take any value. However, when developing the cross product $\vec a \times \vec u = \vec b$ by hand, one can reach a matricial equation of the form $ A \cdot (\vec x)^t = (\vec b)^t $ where $A$ is a matrix with coefficients from $ \vec a $ and which is solvable with at least one method, one example thereof being $ (\vec x)^t = A^{-1} \cdot (\vec b)^t $ Am I doing something wrong? Or is this equation truly not solvable using vectorial math?","Given known vectors $\vec a$ and $\vec b$, is it possible to solve for $\vec u$, given the following equation? $\vec a \times \vec u = \vec b$ So far I have found that the following must be true $\vec u = \vec c + \lambda \hat a$ where $\vec c = \frac{b}{a} (\hat b \times \hat a) = \frac{\vec b \times \vec a}{a^2}$ $\lambda \in \Bbb R $ but I'm unsure whether it is possible to further determine $\vec u$, since any vector $\vec u$ such that $(\vec u \cdot \hat c) \cdot \hat c = \vec c$ should do the trick, which means that $\lambda$ can take any value. However, when developing the cross product $\vec a \times \vec u = \vec b$ by hand, one can reach a matricial equation of the form $ A \cdot (\vec x)^t = (\vec b)^t $ where $A$ is a matrix with coefficients from $ \vec a $ and which is solvable with at least one method, one example thereof being $ (\vec x)^t = A^{-1} \cdot (\vec b)^t $ Am I doing something wrong? Or is this equation truly not solvable using vectorial math?",,"['linear-algebra', 'matrices', 'cross-product']"
14,"Find the eigenvalues and eigenvectors of the matrix $A = uu^t$, where $u\in\mathbb{R}^n$","Find the eigenvalues and eigenvectors of the matrix , where",A = uu^t u\in\mathbb{R}^n,"Find the eigenvalues and eigenvectors of the matrix $A = uu^t$ , where $u\in\mathbb{R}^n$ The multiplication will give me a $n \times n$ matrix like this: $$\begin{bmatrix}    u_1^2   & u_1 u_2 & \dots  & u_1u_n \\    u_2 u_1 & u_2^2   & \dots  & u_2u_n \\    \vdots  & \vdots  & \ddots & \vdots \\    u_n u_1 & \dots   & \dots& u_n^2 \end{bmatrix}$$ I suppose there is some trick using the fact that this matrix is symmetric and square. This should help taking the determinant $$\det \begin{bmatrix}    u_1^2 - \lambda   & u_1 u_2 & \dots  & u_1u_n \\    u_2 u_1 & u_2^2 - \lambda  & \dots  & u_2u_n \\    \vdots  & \vdots  & \ddots & \vdots \\    u_n u_1 & \dots   & \dots& u_n^2 - \lambda \end{bmatrix}$$","Find the eigenvalues and eigenvectors of the matrix , where The multiplication will give me a matrix like this: I suppose there is some trick using the fact that this matrix is symmetric and square. This should help taking the determinant","A = uu^t u\in\mathbb{R}^n n \times n \begin{bmatrix}
   u_1^2   & u_1 u_2 & \dots  & u_1u_n \\
   u_2 u_1 & u_2^2   & \dots  & u_2u_n \\
   \vdots  & \vdots  & \ddots & \vdots \\
   u_n u_1 & \dots   & \dots& u_n^2 \end{bmatrix} \det \begin{bmatrix}
   u_1^2 - \lambda   & u_1 u_2 & \dots  & u_1u_n \\
   u_2 u_1 & u_2^2 - \lambda  & \dots  & u_2u_n \\
   \vdots  & \vdots  & \ddots & \vdots \\
   u_n u_1 & \dots   & \dots& u_n^2 - \lambda \end{bmatrix}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'rank-1-matrices']"
15,Alternative ways to represent the complex numbers as matrices,Alternative ways to represent the complex numbers as matrices,,"The canonical way to represent the complex number $a+bi$ as a $2\times2$ matrix is with $\pmatrix{a &b\\-b&a}$, but I have also found that $\pmatrix{a&bx\\\frac{-b}{x}&a}$ will do for any non-zero $x$. Is there some error here or is this also a perfect representation? Furthermore, is this the only way to represent the complex numbers as matrices?","The canonical way to represent the complex number $a+bi$ as a $2\times2$ matrix is with $\pmatrix{a &b\\-b&a}$, but I have also found that $\pmatrix{a&bx\\\frac{-b}{x}&a}$ will do for any non-zero $x$. Is there some error here or is this also a perfect representation? Furthermore, is this the only way to represent the complex numbers as matrices?",,"['matrices', 'complex-numbers']"
16,Are there always matrices $X$ and $Y$ such that $XY=A$ and $YX=B$?,Are there always matrices  and  such that  and ?,X Y XY=A YX=B,"Fix a natural number $n$ and let $A,B\in\mathbb{C}^{n\times n}$. Are there matrices $X,Y\in\mathbb{C}^{n\times n}$ such that $XY=A$ and that $YX=B$? A necessary condition for that to happen is that $A$ and $B$ have the same characteristic polynomials . But this condition is not sufficient: if $A=\operatorname{Id}_n$, then $XY=A\implies YX=A$. So, my question is: are there necessary and sufficient conditions about $A$ and $B$ so that the problem has a solution?","Fix a natural number $n$ and let $A,B\in\mathbb{C}^{n\times n}$. Are there matrices $X,Y\in\mathbb{C}^{n\times n}$ such that $XY=A$ and that $YX=B$? A necessary condition for that to happen is that $A$ and $B$ have the same characteristic polynomials . But this condition is not sufficient: if $A=\operatorname{Id}_n$, then $XY=A\implies YX=A$. So, my question is: are there necessary and sufficient conditions about $A$ and $B$ so that the problem has a solution?",,"['linear-algebra', 'matrices', 'matrix-equations']"
17,"What's the opposite of ""main diagonal""?","What's the opposite of ""main diagonal""?",,"This matrix has only '1's on its main diagonal: $A = \begin{pmatrix}   1 & 0 & 0 & 0\\   0 & 1 & 0 & 0\\   0 & 0 & 1 & 0\\   0 & 0 & 0 & 1 \end{pmatrix}$ But whats the opposite of main diagonal? I want to say something like: ""There are only '0's on the NOT-main-diagonal"" Can someone help me with a word?","This matrix has only '1's on its main diagonal: $A = \begin{pmatrix}   1 & 0 & 0 & 0\\   0 & 1 & 0 & 0\\   0 & 0 & 1 & 0\\   0 & 0 & 0 & 1 \end{pmatrix}$ But whats the opposite of main diagonal? I want to say something like: ""There are only '0's on the NOT-main-diagonal"" Can someone help me with a word?",,['matrices']
18,Positive-semidefiniteness of certain matrix,Positive-semidefiniteness of certain matrix,,"I have the following (real) matrix which I need to be positive-semidefinite, $P = \begin{bmatrix} P_1 & -\frac{1}{2}(P_1+P_2)\\-\frac{1}{2}(P_1+P_2) & P_2\end{bmatrix} \succeq 0$, where $P_1, P_2 \in \mathbb{R}^{n\times n}$ and $P_1, P_2 \succeq 0$. I think this is only the case when, $P_1 = P_2$, but I couldn't find a way to prove this (only for the case where $n=1$, through the eigenvalues). I was therefore wondering if this is even the case and if so how to prove this. (I already asked this question before but forgot to mention that $P_1$ and $P_2$ are matrices instead of scalars).","I have the following (real) matrix which I need to be positive-semidefinite, $P = \begin{bmatrix} P_1 & -\frac{1}{2}(P_1+P_2)\\-\frac{1}{2}(P_1+P_2) & P_2\end{bmatrix} \succeq 0$, where $P_1, P_2 \in \mathbb{R}^{n\times n}$ and $P_1, P_2 \succeq 0$. I think this is only the case when, $P_1 = P_2$, but I couldn't find a way to prove this (only for the case where $n=1$, through the eigenvalues). I was therefore wondering if this is even the case and if so how to prove this. (I already asked this question before but forgot to mention that $P_1$ and $P_2$ are matrices instead of scalars).",,"['linear-algebra', 'matrices', 'positive-semidefinite']"
19,The set of matrices of form $A^3+B^3$ with multiplication is a monoid,The set of matrices of form  with multiplication is a monoid,A^3+B^3,"Consider the set   \begin{equation*} M=\{A^3+B^3|A,B\in\mathcal{M}_n(\mathbb{C})\} \end{equation*}   for $n\geq 1$. Prove that $(M,\cdot\;)$ is a monoid. This is a problem I found in a section of ""extra problems"" in the romanian magazine Gazeta Matematica, given as a high school problem. As mentioned in the comment, we need to show that for any complex matrices $A, B, C, D$, one needs to write $$(A^3 + B^3)(C^3+D^3)$$ as $E^3 + F^3$ for some complex matrices $E, F$. Even in the case $n=1$, I can't algebraically find $e, f\in \mathbb C$ so that  $$(a^3+ b^3)(c^3+d^3) = e^3+f^3$$ for any given $a, b, c, d\in \mathbb C$.","Consider the set   \begin{equation*} M=\{A^3+B^3|A,B\in\mathcal{M}_n(\mathbb{C})\} \end{equation*}   for $n\geq 1$. Prove that $(M,\cdot\;)$ is a monoid. This is a problem I found in a section of ""extra problems"" in the romanian magazine Gazeta Matematica, given as a high school problem. As mentioned in the comment, we need to show that for any complex matrices $A, B, C, D$, one needs to write $$(A^3 + B^3)(C^3+D^3)$$ as $E^3 + F^3$ for some complex matrices $E, F$. Even in the case $n=1$, I can't algebraically find $e, f\in \mathbb C$ so that  $$(a^3+ b^3)(c^3+d^3) = e^3+f^3$$ for any given $a, b, c, d\in \mathbb C$.",,"['abstract-algebra', 'matrices', 'monoid']"
20,Factorize $\det\left[\begin{smallmatrix}yz-x^2&zx-y^2&xy-z^2\\zx-y^2&xy-z^2&yz-x^2\\xy-z^2&yz-x^2&zx-y^2\end{smallmatrix}\right]$ using factor theorem,Factorize  using factor theorem,\det\left[\begin{smallmatrix}yz-x^2&zx-y^2&xy-z^2\\zx-y^2&xy-z^2&yz-x^2\\xy-z^2&yz-x^2&zx-y^2\end{smallmatrix}\right],"Factorize and prove that $$ \Delta=\begin{vmatrix} yz-x^2&zx-y^2&xy-z^2\\ zx-y^2&xy-z^2&yz-x^2\\ xy-z^2&yz-x^2&zx-y^2 \end{vmatrix}\\=\frac{1}{4}(x+y+z)^2\Big[(x-y)^2+(y-z)^2+(z-x)^2\Big]^2 $$   using factor theorem. My Attempt: $\Delta$ is a homogeneous symmetric polynomial of degree $6$. When $(x-y)^2+(y-z)^2+(z-x)^2=0$, i.e. $x=y=z$ $$ \Delta=\begin{vmatrix} 0&0&0\\ 0&0&0\\ 0&0&0\\ \end{vmatrix}=0 $$ Thus, $(x-y)^2+(y-z)^2+(z-x)^2$ is a factor. How do I extract the other $(x-y)^2+(y-z)^2+(z-x)^2$ from $\Delta$ $\color{red}{?}$ Does this have anything to do with all rows (or columns) being zero when $(x-y)^2+(y-z)^2+(z-x)^2=0$ $\color{red}{?}$ If I can extract that then i think I know how to proceed. The remaining factor must be a homogeneous quadratic symmetric polynomial, i.e. $p(x,y,z)=a(x^2+y^2+z^2)+b(xy+yz+zx)$ $$ \Delta(x,y,z)=\Big[(x-y)^2+(y-z)^2+(z-x)^2\Big]^2.a(x^2+y^2+z^2)+b(xy+yz+zx) $$ $$ \Delta(1,0,0)=\begin{vmatrix} -1&0&0\\ 0&0&-1\\ 0&-1&0\\ \end{vmatrix}=1=4.a\implies a=\frac{1}{4} $$ $$ \Delta(1,1,0)=\begin{vmatrix} -1&-1&1\\ -1&1&-1\\ 1&-1&-1\\ \end{vmatrix}=\begin{vmatrix} 0&0&1\\ -2&0&-1\\ 0&-2&-1\\ \end{vmatrix}\\ =\begin{vmatrix} -2&0\\ 0&-2\\ \end{vmatrix}=4=4.(2a+b)=4(1/2+b)=2+4b\\ \implies b=\frac{1}{2} $$ $$ \Delta(x,y,z)=\Big[(x-y)^2+(y-z)^2+(z-x)^2\Big]^2.\frac{1}{4}(x^2+y^2+z^2)+\frac{1}{2}(xy+yz+zx)\\ =\frac{1}{4}\Big[(x-y)^2+(y-z)^2+(z-x)^2\Big]^2.(x^2+y^2+z^2+2xy+2yz+2zx)\\ =\frac{1}{4}\Big[(x-y)^2+(y-z)^2+(z-x)^2\Big]^2(x+y+z)^2 $$ Note: I am trying to factorize the determinant using factor theorem given the fact that the determinant is a homogeneous symmetric polynomial of degree 6.","Factorize and prove that $$ \Delta=\begin{vmatrix} yz-x^2&zx-y^2&xy-z^2\\ zx-y^2&xy-z^2&yz-x^2\\ xy-z^2&yz-x^2&zx-y^2 \end{vmatrix}\\=\frac{1}{4}(x+y+z)^2\Big[(x-y)^2+(y-z)^2+(z-x)^2\Big]^2 $$   using factor theorem. My Attempt: $\Delta$ is a homogeneous symmetric polynomial of degree $6$. When $(x-y)^2+(y-z)^2+(z-x)^2=0$, i.e. $x=y=z$ $$ \Delta=\begin{vmatrix} 0&0&0\\ 0&0&0\\ 0&0&0\\ \end{vmatrix}=0 $$ Thus, $(x-y)^2+(y-z)^2+(z-x)^2$ is a factor. How do I extract the other $(x-y)^2+(y-z)^2+(z-x)^2$ from $\Delta$ $\color{red}{?}$ Does this have anything to do with all rows (or columns) being zero when $(x-y)^2+(y-z)^2+(z-x)^2=0$ $\color{red}{?}$ If I can extract that then i think I know how to proceed. The remaining factor must be a homogeneous quadratic symmetric polynomial, i.e. $p(x,y,z)=a(x^2+y^2+z^2)+b(xy+yz+zx)$ $$ \Delta(x,y,z)=\Big[(x-y)^2+(y-z)^2+(z-x)^2\Big]^2.a(x^2+y^2+z^2)+b(xy+yz+zx) $$ $$ \Delta(1,0,0)=\begin{vmatrix} -1&0&0\\ 0&0&-1\\ 0&-1&0\\ \end{vmatrix}=1=4.a\implies a=\frac{1}{4} $$ $$ \Delta(1,1,0)=\begin{vmatrix} -1&-1&1\\ -1&1&-1\\ 1&-1&-1\\ \end{vmatrix}=\begin{vmatrix} 0&0&1\\ -2&0&-1\\ 0&-2&-1\\ \end{vmatrix}\\ =\begin{vmatrix} -2&0\\ 0&-2\\ \end{vmatrix}=4=4.(2a+b)=4(1/2+b)=2+4b\\ \implies b=\frac{1}{2} $$ $$ \Delta(x,y,z)=\Big[(x-y)^2+(y-z)^2+(z-x)^2\Big]^2.\frac{1}{4}(x^2+y^2+z^2)+\frac{1}{2}(xy+yz+zx)\\ =\frac{1}{4}\Big[(x-y)^2+(y-z)^2+(z-x)^2\Big]^2.(x^2+y^2+z^2+2xy+2yz+2zx)\\ =\frac{1}{4}\Big[(x-y)^2+(y-z)^2+(z-x)^2\Big]^2(x+y+z)^2 $$ Note: I am trying to factorize the determinant using factor theorem given the fact that the determinant is a homogeneous symmetric polynomial of degree 6.",,"['matrices', 'polynomials', 'determinant', 'symmetric-polynomials']"
21,Examples of real $2\times2$ and complex $3\times3$ matrices with minimal polynomial $t^2+1$,Examples of real  and complex  matrices with minimal polynomial,2\times2 3\times3 t^2+1,"Consider the following corrolary (3.3.4 in Horn's and Johnson's Matrix Analysis book): For each $A\in\mathbb{C}^{n\times n}$, the minimal polynomial $q_A(t)$ divides the characteristic polynomial $p_A(t)$. Moreover, $q_A(\lambda)=0$ if and only if $\lambda$ is an eigenvalue of $A$, so every root of $p_A(t)=0$ is a root of $q_A(t)=0$. There is no real $3\times3$ matrix with minimal polynomial $t^2+1$ (proofs for that are here ). However, there is a real $2\times2$ matrix and a complex $3\times3$ matrix with minimal polynomial $q_A(t)=t^2+1$. I am trying to find such examples. For the $2\times2$ case consider $$A=\begin{bmatrix}1&-1\\2&-1\end{bmatrix}$$ Its characteristic polynomial is  $$p_A(t)=\det(tI-A)=\dots=t^2+1=(t-i)(t+i)$$ Then, by the above theorem and by the definition of the minimal polynomial (the unique monic polynomial of minimum degree, $q_A(t)$, that annihilates $A$ i.e. $q_A(A)=0$) we have  $$q_A(t)|p_A(t)$$ and every eigenvalue is a root of $q_A(t)$. Thus, the minimal polynomial is indeed $q_A(t)=t^2+1$. For the $3\times3$ case consider $$A=\begin{bmatrix}i&0&0\\0&1&-1\\0&2&-1\end{bmatrix}$$ Similarly, it is easy to see that  $$p_A(t)=\det(tI-A)=\dots=(t-i)^2(t+i)$$ and among the two possibilities for the minimal polynomial $$p_1(t)=(t-i)^2(t+i)\qquad\text{and}\qquad p_2(t)=(t-i)(t+i)$$ we see that $p_2(t)=t^2+1$ is the one with minimum degree that annihilates $A$ so that $q_A(t)=t^2+1$. Are the above examples correct? Any other examples, perhaps more interesting?","Consider the following corrolary (3.3.4 in Horn's and Johnson's Matrix Analysis book): For each $A\in\mathbb{C}^{n\times n}$, the minimal polynomial $q_A(t)$ divides the characteristic polynomial $p_A(t)$. Moreover, $q_A(\lambda)=0$ if and only if $\lambda$ is an eigenvalue of $A$, so every root of $p_A(t)=0$ is a root of $q_A(t)=0$. There is no real $3\times3$ matrix with minimal polynomial $t^2+1$ (proofs for that are here ). However, there is a real $2\times2$ matrix and a complex $3\times3$ matrix with minimal polynomial $q_A(t)=t^2+1$. I am trying to find such examples. For the $2\times2$ case consider $$A=\begin{bmatrix}1&-1\\2&-1\end{bmatrix}$$ Its characteristic polynomial is  $$p_A(t)=\det(tI-A)=\dots=t^2+1=(t-i)(t+i)$$ Then, by the above theorem and by the definition of the minimal polynomial (the unique monic polynomial of minimum degree, $q_A(t)$, that annihilates $A$ i.e. $q_A(A)=0$) we have  $$q_A(t)|p_A(t)$$ and every eigenvalue is a root of $q_A(t)$. Thus, the minimal polynomial is indeed $q_A(t)=t^2+1$. For the $3\times3$ case consider $$A=\begin{bmatrix}i&0&0\\0&1&-1\\0&2&-1\end{bmatrix}$$ Similarly, it is easy to see that  $$p_A(t)=\det(tI-A)=\dots=(t-i)^2(t+i)$$ and among the two possibilities for the minimal polynomial $$p_1(t)=(t-i)^2(t+i)\qquad\text{and}\qquad p_2(t)=(t-i)(t+i)$$ we see that $p_2(t)=t^2+1$ is the one with minimum degree that annihilates $A$ so that $q_A(t)=t^2+1$. Are the above examples correct? Any other examples, perhaps more interesting?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'minimal-polynomials']"
22,Derivative w.r.t orthogonal matrix,Derivative w.r.t orthogonal matrix,,"Let $A$ be an orthogonal matrix with elements $a_{ij}$ so that $\sum_k a_{ik} a_{jk}  = \delta_{ij}$. I'd like to know what is $\frac{ \partial a_{ij} }{ \partial a_{kl}}$. If $A$ was a generic matrix (with no constraints on its elements) then the answer would be $\delta_{ik} \delta_{jl}$. However, now we have the quadratic constraint on the matrix. What is the answer in this case?","Let $A$ be an orthogonal matrix with elements $a_{ij}$ so that $\sum_k a_{ik} a_{jk}  = \delta_{ij}$. I'd like to know what is $\frac{ \partial a_{ij} }{ \partial a_{kl}}$. If $A$ was a generic matrix (with no constraints on its elements) then the answer would be $\delta_{ik} \delta_{jl}$. However, now we have the quadratic constraint on the matrix. What is the answer in this case?",,"['matrices', 'derivatives', 'matrix-calculus', 'orthogonal-matrices']"
23,Cosine of a matrix,Cosine of a matrix,,"I came across this question, asked in a competitive exam. It  is as follows. Given a matrix $M = \begin{bmatrix}2&1\\1&2\end{bmatrix}$ what is the value of $cos(πM/6)$? I've tried series expansion but I think there is an alternative way doing it, any help is appreciated. Options given are \begin{bmatrix}1/2&1\\1&1/2\end{bmatrix} \begin{bmatrix}\sqrt3/4&-\sqrt3/4\\-\sqrt3/4&\sqrt3/4\end{bmatrix} \begin{bmatrix}\sqrt3/4&\sqrt3/4\\\sqrt3/4&\sqrt3/4\end{bmatrix} \begin{bmatrix}1/2&\sqrt3/2\\\sqrt3/2&1/2\end{bmatrix}","I came across this question, asked in a competitive exam. It  is as follows. Given a matrix $M = \begin{bmatrix}2&1\\1&2\end{bmatrix}$ what is the value of $cos(πM/6)$? I've tried series expansion but I think there is an alternative way doing it, any help is appreciated. Options given are \begin{bmatrix}1/2&1\\1&1/2\end{bmatrix} \begin{bmatrix}\sqrt3/4&-\sqrt3/4\\-\sqrt3/4&\sqrt3/4\end{bmatrix} \begin{bmatrix}\sqrt3/4&\sqrt3/4\\\sqrt3/4&\sqrt3/4\end{bmatrix} \begin{bmatrix}1/2&\sqrt3/2\\\sqrt3/2&1/2\end{bmatrix}",,"['matrices', 'matrix-calculus']"
24,Swapping row $n$ with row $m$ by using permutation matrix,Swapping row  with row  by using permutation matrix,n m,"Let $A$ be matrix ( finite or infinite). and We want to swap ( exchange ) location of row $n$ with row $m$. I know we need to use permutation matrix to do this, but my question do exactly we know which permutation matrix need to be used to perform this swapping? or we need to know the entries of the matrix too.","Let $A$ be matrix ( finite or infinite). and We want to swap ( exchange ) location of row $n$ with row $m$. I know we need to use permutation matrix to do this, but my question do exactly we know which permutation matrix need to be used to perform this swapping? or we need to know the entries of the matrix too.",,"['linear-algebra', 'matrices', 'linear-transformations', 'matrix-decomposition']"
25,Beginner troubleshooting an eigenvector calculation,Beginner troubleshooting an eigenvector calculation,,"I am having some difficulty identifying the error in my eigenvector calculation. I am trying to calculate the final eigenvector for $\lambda_3 = 1$ and am expecting the result $ X_3 = \left(\begin{smallmatrix}-2\\17\\7\end{smallmatrix}\right)$ To begin with, I set up the following equation (for the purpose of this question I will refer to the leftmost matrix here as A).  $$ \begin{bmatrix}    1 - \lambda & 0 & 0 \\     3 & 3 - \lambda  & -4\\     -2 & 1 & -\lambda -2 \\ \end{bmatrix} \begin{bmatrix}   x_1 \\     x_2\\     x_3 \\ \end{bmatrix} = \begin{bmatrix} 0\\  0\\  0\\ \end{bmatrix} $$ I) Substitute $\lambda_3 = 1$ $$ \begin{bmatrix}    0 & 0 & 0 \\     3 & 2  & -4\\     -2 & 1 & -3 \\ \end{bmatrix} \begin{bmatrix}   x_1 \\     x_2\\     x_3 \\ \end{bmatrix} = \begin{bmatrix} 0\\  0\\  0\\ \end{bmatrix} $$ II) Reduce the matrix with elementary row operations. $R_2 \leftarrow R_2 - 2R_3$ $$ A =  \begin{bmatrix}    0 & 0 & 0 \\     7 & 0  & 2\\     -2 & 1 & -3 \\ \end{bmatrix} $$ $R_3 \leftarrow  3R_2 +  2R_3$ $$ A =  \begin{bmatrix}    0 & 0 & 0 \\     7 & 0  & 2\\     17 & 2 & 0 \\ \end{bmatrix} $$ $R_2 \leftarrow  \frac{1}{7} R_2$ $R_3 \leftarrow \frac{1}{17} R_3$$ $$ A =  \begin{bmatrix}    0 & 0 & 0 \\     1 & 0  & 2/7\\     1 & 2/17 & 0 \\ \end{bmatrix} $$ III) multiply matrices to get a series of equations equal to 0 and rearrange them in terms of a common element. $x_1 + \frac{2}{7}x_3 = 0 \rightarrow x_1 = -\frac{2}{7}x_3$ $x_1 + \frac{2}{17}x_2 = 0 \rightarrow x_1 = -\frac{2}{17}x_2$ IV) Substitute a value into the vector to get an eigenvector. Let $\ x_1 = 1 \rightarrow X_3 =  \left(\begin{smallmatrix}1\\-2/17\\-2/7\end{smallmatrix}\right) $ Which at this point we can see is not a multiple of the expected $X_3$. Can anyone highlight my error for me? Many thanks in advance.","I am having some difficulty identifying the error in my eigenvector calculation. I am trying to calculate the final eigenvector for $\lambda_3 = 1$ and am expecting the result $ X_3 = \left(\begin{smallmatrix}-2\\17\\7\end{smallmatrix}\right)$ To begin with, I set up the following equation (for the purpose of this question I will refer to the leftmost matrix here as A).  $$ \begin{bmatrix}    1 - \lambda & 0 & 0 \\     3 & 3 - \lambda  & -4\\     -2 & 1 & -\lambda -2 \\ \end{bmatrix} \begin{bmatrix}   x_1 \\     x_2\\     x_3 \\ \end{bmatrix} = \begin{bmatrix} 0\\  0\\  0\\ \end{bmatrix} $$ I) Substitute $\lambda_3 = 1$ $$ \begin{bmatrix}    0 & 0 & 0 \\     3 & 2  & -4\\     -2 & 1 & -3 \\ \end{bmatrix} \begin{bmatrix}   x_1 \\     x_2\\     x_3 \\ \end{bmatrix} = \begin{bmatrix} 0\\  0\\  0\\ \end{bmatrix} $$ II) Reduce the matrix with elementary row operations. $R_2 \leftarrow R_2 - 2R_3$ $$ A =  \begin{bmatrix}    0 & 0 & 0 \\     7 & 0  & 2\\     -2 & 1 & -3 \\ \end{bmatrix} $$ $R_3 \leftarrow  3R_2 +  2R_3$ $$ A =  \begin{bmatrix}    0 & 0 & 0 \\     7 & 0  & 2\\     17 & 2 & 0 \\ \end{bmatrix} $$ $R_2 \leftarrow  \frac{1}{7} R_2$ $R_3 \leftarrow \frac{1}{17} R_3$$ $$ A =  \begin{bmatrix}    0 & 0 & 0 \\     1 & 0  & 2/7\\     1 & 2/17 & 0 \\ \end{bmatrix} $$ III) multiply matrices to get a series of equations equal to 0 and rearrange them in terms of a common element. $x_1 + \frac{2}{7}x_3 = 0 \rightarrow x_1 = -\frac{2}{7}x_3$ $x_1 + \frac{2}{17}x_2 = 0 \rightarrow x_1 = -\frac{2}{17}x_2$ IV) Substitute a value into the vector to get an eigenvector. Let $\ x_1 = 1 \rightarrow X_3 =  \left(\begin{smallmatrix}1\\-2/17\\-2/7\end{smallmatrix}\right) $ Which at this point we can see is not a multiple of the expected $X_3$. Can anyone highlight my error for me? Many thanks in advance.",,"['matrices', 'eigenvalues-eigenvectors']"
26,"Ax=0, why A must be singular matrix for having x different from 0?","Ax=0, why A must be singular matrix for having x different from 0?",,"For the given equation $Ax=0$ where $A$ is a square matrix and $x$ is a column vector, why $A$ must be a singular matrix (determinant $0$) in order to have $x$ different from null column vector?.","For the given equation $Ax=0$ where $A$ is a square matrix and $x$ is a column vector, why $A$ must be a singular matrix (determinant $0$) in order to have $x$ different from null column vector?.",,"['linear-algebra', 'matrices']"
27,When the inverse of a matrix with integer entries also has integer entries,When the inverse of a matrix with integer entries also has integer entries,,"Suppose $A\in\Bbb{GL}_n(\Bbb R)$ , (the space of all invertible matrices of order $n$ ), have integer entries. If $\det(A)=\pm 1$ then obviously $A^{-1}$ have integer entries. Is the converse true, that is, if $A^{-1}$ have integer entries does it imply that $\det(A)=\pm 1$ ? Obviously for that $|\det(A)|=\gcd\{|A_{ij}|,1\leq i,j\leq n\}$ where $A_{ij}$ is the $(i,j)^{th}$ cofactor. But is it necessary for the $\gcd$ to be $1$ for a matrix to be invertible?","Suppose , (the space of all invertible matrices of order ), have integer entries. If then obviously have integer entries. Is the converse true, that is, if have integer entries does it imply that ? Obviously for that where is the cofactor. But is it necessary for the to be for a matrix to be invertible?","A\in\Bbb{GL}_n(\Bbb R) n \det(A)=\pm 1 A^{-1} A^{-1} \det(A)=\pm 1 |\det(A)|=\gcd\{|A_{ij}|,1\leq i,j\leq n\} A_{ij} (i,j)^{th} \gcd 1","['linear-algebra', 'matrices']"
28,"Rank of $A=BC$, when ranks of $B,C$ are given.","Rank of , when ranks of  are given.","A=BC B,C","[NBHM-PhD Screening test-2015, Algebra(Q 1.7)] Let $B$ be a $5\times3$ matrix and let $C$ be a $3\times5$ matrix, both with real entries. Set $A=BC$ . Then what are the possible ranks of $A$ when (1) both $B$ and $C$ have rank $3$ (2) both $B$ and $C$ have rank $ 2$ I know that $\operatorname{rank}A\leq \min(\operatorname{rank}C,\operatorname{rank}B)$ . From this I can say in first case $\operatorname{rank}A\leq 3,$ and in second case $\leq2$ . What more we can say about rank of $A$ ? also is there any  general method to attack such kind of problems","[NBHM-PhD Screening test-2015, Algebra(Q 1.7)] Let be a matrix and let be a matrix, both with real entries. Set . Then what are the possible ranks of when (1) both and have rank (2) both and have rank I know that . From this I can say in first case and in second case . What more we can say about rank of ? also is there any  general method to attack such kind of problems","B 5\times3 C 3\times5 A=BC A B C 3 B C  2 \operatorname{rank}A\leq \min(\operatorname{rank}C,\operatorname{rank}B) \operatorname{rank}A\leq 3, \leq2 A","['linear-algebra', 'matrices']"
29,Notation for element-wise function application,Notation for element-wise function application,,"Is there some kind of specific notation I could use to specify that function $f$ is applied to each element of matrix $W$ and not to a matrix as a whole. Specifically, I am writing about applying activation function for a layer of neural network. Or just writing this is clear enough: $f(W)$?","Is there some kind of specific notation I could use to specify that function $f$ is applied to each element of matrix $W$ and not to a matrix as a whole. Specifically, I am writing about applying activation function for a layer of neural network. Or just writing this is clear enough: $f(W)$?",,"['matrices', 'functions', 'notation']"
30,Linear matrix equation $AXA^T=B$,Linear matrix equation,AXA^T=B,"What methods are there to solve the following linear matrix equation for $X$ $$AXA^T=B$$ where $X$ and $B$ are real square matrices, $X$ is symmetric and $A$ might not be square. OBS : I could reduce the problem to a more special case in which $B$ is diagonal.","What methods are there to solve the following linear matrix equation for where and are real square matrices, is symmetric and might not be square. OBS : I could reduce the problem to a more special case in which is diagonal.",X AXA^T=B X B X A B,"['linear-algebra', 'matrices', 'matrix-equations']"
31,Does $A^TA = \det(A) I$ imply anything significant?,Does  imply anything significant?,A^TA = \det(A) I,I was fiddling with some $n\times n$ square matrices and I came across a matrix where the transpose of $A$ multiplied by $A$ gives me the diagonal matrix with values of determinant of $A$. That is $A^TA = \det(A)I$. Does this mean anything significant? Would love to hear your thoughts.,I was fiddling with some $n\times n$ square matrices and I came across a matrix where the transpose of $A$ multiplied by $A$ gives me the diagonal matrix with values of determinant of $A$. That is $A^TA = \det(A)I$. Does this mean anything significant? Would love to hear your thoughts.,,"['linear-algebra', 'matrices', 'matrix-equations', 'transpose']"
32,Why are free variables free?,Why are free variables free?,,"I'm having trouble understanding this: The free variables are called free because they can take on any value; none of the equations relates any of them to each other. Why can we set free variables to an arbitrary s or t? Example: Say we have $A = \begin{bmatrix} 1 & 2 & 2 & 2 \\ 0 & 0 & 2 & 4 \\ 0 & 0 & 0 & 0 \end{bmatrix}$ and we are looking for solutions to $Ax = 0$ . Here, we have column $1$ and $3$ as our pivot columns (and $x_1$ , $x_3$ are pivot varibles ) and column $2$ and $4$ as our free columns (and $x_2$ , $x_4$ are free variables )[1]. To find a solution vector $x =\begin{bmatrix} x_1 \\ x_2 \\x_3 \\ x_4\end{bmatrix}$ , we can set any arbitrary values to $x_2$ and $x_4$ which will lead to some values for $x_1$ and $x_3$ , thus giving us our solution. The confusion: setting arbitrary initial values to the ""pivot variables"" ( $x_1$ and $x_3$ ) and then finding the values of ""free variables"" also leads to a solution. Then why do these ""free variables"" exist? And why are they called ""free""? [1] Page 80, Strang, Gilbert , Linear algebra and its applications, Boston, MA: Brooks/Cole, Cengage Learning (ISBN 978-0-534-42200-4). 496 p. (2007). ZBL1329.15004 . Lecture by Gilbert Strang illustrating this example: https://youtu.be/VqP2tREMvt0?t=322","I'm having trouble understanding this: The free variables are called free because they can take on any value; none of the equations relates any of them to each other. Why can we set free variables to an arbitrary s or t? Example: Say we have and we are looking for solutions to . Here, we have column and as our pivot columns (and , are pivot varibles ) and column and as our free columns (and , are free variables )[1]. To find a solution vector , we can set any arbitrary values to and which will lead to some values for and , thus giving us our solution. The confusion: setting arbitrary initial values to the ""pivot variables"" ( and ) and then finding the values of ""free variables"" also leads to a solution. Then why do these ""free variables"" exist? And why are they called ""free""? [1] Page 80, Strang, Gilbert , Linear algebra and its applications, Boston, MA: Brooks/Cole, Cengage Learning (ISBN 978-0-534-42200-4). 496 p. (2007). ZBL1329.15004 . Lecture by Gilbert Strang illustrating this example: https://youtu.be/VqP2tREMvt0?t=322","A = \begin{bmatrix}
1 & 2 & 2 & 2 \\
0 & 0 & 2 & 4 \\
0 & 0 & 0 & 0
\end{bmatrix} Ax = 0 1 3 x_1 x_3 2 4 x_2 x_4 x =\begin{bmatrix} x_1 \\ x_2 \\x_3 \\ x_4\end{bmatrix} x_2 x_4 x_1 x_3 x_1 x_3","['linear-algebra', 'matrices']"
33,query on diagonalizability of matrix,query on diagonalizability of matrix,,Here I think matrix A can be an identity matrix . But it's answer is that A is diagonalizable . It's a single correct question . How can it be diagonalizable only . Why can't A be an identity matrix?,Here I think matrix A can be an identity matrix . But it's answer is that A is diagonalizable . It's a single correct question . How can it be diagonalizable only . Why can't A be an identity matrix?,,"['linear-algebra', 'matrices', 'diagonalization', 'projection-matrices']"
34,Solve a matrix equation,Solve a matrix equation,,I need to find $X$ from $$\begin{pmatrix} 1 & 2\\  -3 &-6  \end{pmatrix} X \begin{pmatrix} 1 &2 \\  -1 &-2  \end{pmatrix}=\begin{pmatrix} 2 &4 \\  -6 & -12 \end{pmatrix}$$ I wrote $X$ as $$X=\begin{pmatrix} a & b\\  c &d  \end{pmatrix}$$ and I got $a+2c-b-2d=2$ but I do not know what to do next. Please help.,I need to find $X$ from $$\begin{pmatrix} 1 & 2\\  -3 &-6  \end{pmatrix} X \begin{pmatrix} 1 &2 \\  -1 &-2  \end{pmatrix}=\begin{pmatrix} 2 &4 \\  -6 & -12 \end{pmatrix}$$ I wrote $X$ as $$X=\begin{pmatrix} a & b\\  c &d  \end{pmatrix}$$ and I got $a+2c-b-2d=2$ but I do not know what to do next. Please help.,,"['matrices', 'matrix-equations']"
35,Determinant of odd matrix,Determinant of odd matrix,,"Given a matrix $A = \{{a_{i,j}}\} \in M_{7\times7}(\Bbb R)$ It is said that $a_{i,j} = 0$ if $i$,$j$ are both odd. Show that $det(A) = 0$ Any hints?","Given a matrix $A = \{{a_{i,j}}\} \in M_{7\times7}(\Bbb R)$ It is said that $a_{i,j} = 0$ if $i$,$j$ are both odd. Show that $det(A) = 0$ Any hints?",,"['linear-algebra', 'matrices']"
36,"Connecting Unitary, Orthogonal, Normal, and Self-Adjoint","Connecting Unitary, Orthogonal, Normal, and Self-Adjoint",,"I'm trying to connect these 4 properties to see the ""big picture,"" and would love some input or corrections. This is what I have so far: 1) If $T$ is unitary or orthogonal, then $T$ is also normal. 2) Normal operators are also self-adjoint. ($\implies T$ is unitary/orthgonal?) FALSE. 3) Unitary and orthogonal operators have eigenvalues where $|\lambda_i|$ = $1$. 4) Unitary and orthgonal operators are similar to a diagonal matrix (over $\mathbb{C}$ and $\mathbb{R}$, respectively), which means they are diagonalizable. 5) If $T$ is orthogonal, then all of its eigenvalues must be real. FALSE. 6) If $T$ is unitary, not all of its eigenvalues have to be imaginary (can be both complex and real, as long as its absolute values are $1$). I know not all of these are correct, and I will edit them accordingly. But these are the connections I came up with so far. Any input or corrections would be GREATLY helpful. Thank you!","I'm trying to connect these 4 properties to see the ""big picture,"" and would love some input or corrections. This is what I have so far: 1) If $T$ is unitary or orthogonal, then $T$ is also normal. 2) Normal operators are also self-adjoint. ($\implies T$ is unitary/orthgonal?) FALSE. 3) Unitary and orthogonal operators have eigenvalues where $|\lambda_i|$ = $1$. 4) Unitary and orthgonal operators are similar to a diagonal matrix (over $\mathbb{C}$ and $\mathbb{R}$, respectively), which means they are diagonalizable. 5) If $T$ is orthogonal, then all of its eigenvalues must be real. FALSE. 6) If $T$ is unitary, not all of its eigenvalues have to be imaginary (can be both complex and real, as long as its absolute values are $1$). I know not all of these are correct, and I will edit them accordingly. But these are the connections I came up with so far. Any input or corrections would be GREATLY helpful. Thank you!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations']"
37,Orthogonal matrix and orthonormal columns,Orthogonal matrix and orthonormal columns,,"Show that the columns of orthogonal matrix are always orthonormal. Hint: $A^TA=I$ Can't really get even started, I thought that it has to be orthonormal since the result is $I$ and not only a diagonal matrix with other numbers in them?","Show that the columns of orthogonal matrix are always orthonormal. Hint: $A^TA=I$ Can't really get even started, I thought that it has to be orthonormal since the result is $I$ and not only a diagonal matrix with other numbers in them?",,"['linear-algebra', 'matrices']"
38,Trace of product of three Pauli matrices,Trace of product of three Pauli matrices,,"Consider the four $2\times 2$ matrices $\{\sigma_\mu\}$, with $\mu = 0,1,2,3$, which are defined as follows $$ \sigma_0 =\left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array}\right) $$ $$ \sigma_1 =\left( \begin{array}{cc} 0 & 1 \\ 1 & 0 \end{array}\right) $$ $$ \sigma_2 =\left( \begin{array}{cc} 0 & -i \\ i & 0 \end{array}\right) $$ $$ \sigma_3 =\left( \begin{array}{cc} 1 & 0 \\ 0 & -1 \end{array}\right) $$ i.e. the identity matrix and the three Pauli matrices. For the trace of the product of any two matrices $\sigma_\mu$ one has the identity $\text{tr}(\sigma_\mu \sigma_\nu)= 2 \delta_{\mu \nu}$. I was wondering if a similar identity can be derived for the product of three sigma matrices,  $$ \text{tr}(\sigma_\mu \sigma_\nu \sigma_\lambda)= \;? $$","Consider the four $2\times 2$ matrices $\{\sigma_\mu\}$, with $\mu = 0,1,2,3$, which are defined as follows $$ \sigma_0 =\left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array}\right) $$ $$ \sigma_1 =\left( \begin{array}{cc} 0 & 1 \\ 1 & 0 \end{array}\right) $$ $$ \sigma_2 =\left( \begin{array}{cc} 0 & -i \\ i & 0 \end{array}\right) $$ $$ \sigma_3 =\left( \begin{array}{cc} 1 & 0 \\ 0 & -1 \end{array}\right) $$ i.e. the identity matrix and the three Pauli matrices. For the trace of the product of any two matrices $\sigma_\mu$ one has the identity $\text{tr}(\sigma_\mu \sigma_\nu)= 2 \delta_{\mu \nu}$. I was wondering if a similar identity can be derived for the product of three sigma matrices,  $$ \text{tr}(\sigma_\mu \sigma_\nu \sigma_\lambda)= \;? $$",,"['linear-algebra', 'matrices', 'trace']"
39,"Finding the minimal polynomial, the eigenvalues, and the characteristic polynomial of $T(A)=A+A^t$","Finding the minimal polynomial, the eigenvalues, and the characteristic polynomial of",T(A)=A+A^t,"Let $M_n$ denote the vector space of all $n\times n$ real valued matrices and for any matrix $A\in M_n$ let $A^t$ denote its transpose. Define the linear map $T:M_n\rightarrow M_n$ as $$T(A)=A+A^t.$$ What is the minimal polynomial of $T$? What are the eigenvalues and the corresponding eigenspaces of $T$? What is the characteristic polynomial of $T$? I learned about minimal polynomials of operators from $\textit{Linear Algebra Done Right}$ by Axler. In his description he explains how in order to find the minimal polynomial you must find the smallest positive integer $m$ such that $T^m$ is a linear combination of $I, T, T^2, \ldots, T^{m-1}$. That is, the smallest integer $m$ such that $$T^m = a_0I+a_1T+a_2T^2+ \cdots +a_{m-1}T^{m-1}.$$ As for the eigenvalues...since we are working with real-valued matrices, then $A$ and $A^t$ have the same eigenvalues, so if $\lambda \in \mathbb{R}$ is an eigenvalue of $A$ (and hence of $A^t$) wouldn't $2\lambda$ be an eigenvalue of $A+A^t$? I apologize if I am saying utter nonsense...","Let $M_n$ denote the vector space of all $n\times n$ real valued matrices and for any matrix $A\in M_n$ let $A^t$ denote its transpose. Define the linear map $T:M_n\rightarrow M_n$ as $$T(A)=A+A^t.$$ What is the minimal polynomial of $T$? What are the eigenvalues and the corresponding eigenspaces of $T$? What is the characteristic polynomial of $T$? I learned about minimal polynomials of operators from $\textit{Linear Algebra Done Right}$ by Axler. In his description he explains how in order to find the minimal polynomial you must find the smallest positive integer $m$ such that $T^m$ is a linear combination of $I, T, T^2, \ldots, T^{m-1}$. That is, the smallest integer $m$ such that $$T^m = a_0I+a_1T+a_2T^2+ \cdots +a_{m-1}T^{m-1}.$$ As for the eigenvalues...since we are working with real-valued matrices, then $A$ and $A^t$ have the same eigenvalues, so if $\lambda \in \mathbb{R}$ is an eigenvalue of $A$ (and hence of $A^t$) wouldn't $2\lambda$ be an eigenvalue of $A+A^t$? I apologize if I am saying utter nonsense...",,"['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations', 'minimal-polynomials']"
40,Counter example or proof that $\kappa(AB) \leq \kappa(A)\kappa(B) $,Counter example or proof that,\kappa(AB) \leq \kappa(A)\kappa(B) ,"I am stuggling to find a counter example or either proof (for general matrices $A, B \in \mathbb{C}^{m \times n}$) that  $$\kappa(AB) \leq \kappa(A)\kappa(B)\,,$$  where the condition number $\kappa(\cdot)$ is evaluated with respect to any submultiplicative norm $$\|AB\| \leq \|A\|\|B\|$$ and is given by $$ \kappa(A) = \|A\|\|A^\dagger\|\,, $$ with $A^\dagger$ the Moore–Penrose pseudoinverse of $A$. If the matrices $A, B$ are non singular the result can be easily proved, indeed $$ \|AB\|\|(AB)^\dagger\| = \|AB\|\|(AB)^{-1}\| = \|AB\|\|B^{-1}A^{-1}\|\leq \|A\|\|A^{-1}\|\|B\|\|B^{-1}\| = \kappa(A)\kappa(B)\,.  $$ But what about other cases? Added later: Similar problem is considered in this question , but no answer to a more general setting is given there.","I am stuggling to find a counter example or either proof (for general matrices $A, B \in \mathbb{C}^{m \times n}$) that  $$\kappa(AB) \leq \kappa(A)\kappa(B)\,,$$  where the condition number $\kappa(\cdot)$ is evaluated with respect to any submultiplicative norm $$\|AB\| \leq \|A\|\|B\|$$ and is given by $$ \kappa(A) = \|A\|\|A^\dagger\|\,, $$ with $A^\dagger$ the Moore–Penrose pseudoinverse of $A$. If the matrices $A, B$ are non singular the result can be easily proved, indeed $$ \|AB\|\|(AB)^\dagger\| = \|AB\|\|(AB)^{-1}\| = \|AB\|\|B^{-1}A^{-1}\|\leq \|A\|\|A^{-1}\|\|B\|\|B^{-1}\| = \kappa(A)\kappa(B)\,.  $$ But what about other cases? Added later: Similar problem is considered in this question , but no answer to a more general setting is given there.",,"['matrices', 'numerical-methods', 'numerical-linear-algebra', 'matrix-decomposition', 'pseudoinverse']"
41,Multiplicative functions on matrices,Multiplicative functions on matrices,,"What are some examples of multiplicative functions on matrices? More precisely, I'm looking for $f : M^{n \times n}, M^{n \times n} \to \mathbb R$ with the property $$f(AB) = f(A)f(B)$$ where A, B are $n \times n$ matrices. A particularly well-known example is the determinant, but I'm curious about other examples, and possibly a classification of them.","What are some examples of multiplicative functions on matrices? More precisely, I'm looking for $f : M^{n \times n}, M^{n \times n} \to \mathbb R$ with the property $$f(AB) = f(A)f(B)$$ where A, B are $n \times n$ matrices. A particularly well-known example is the determinant, but I'm curious about other examples, and possibly a classification of them.",,"['linear-algebra', 'matrices']"
42,Sorting rows then sorting columns preserves the sorting of rows,Sorting rows then sorting columns preserves the sorting of rows,,"From Peter Winkler's book: Given a matrix, prove that after first sorting each row, then sorting each column, each row remains sorted. For example: starting with $$\begin{bmatrix} 1 & -3 & 2 \\ 0 & 1 & -5 \\ 4 & -1 & 1 \end{bmatrix}$$ Sorting each row individually and in ascending order gives $$\begin{bmatrix} -3 & 1 & 2 \\ -5 & 0 & 1 \\ -1 & 1 & 4 \end{bmatrix}$$ Then sorting each column individually in ascending order gives $$\begin{bmatrix} -5 & 0 & 1 \\ -3 & 1 & 2 \\ -1 & 1 & 4 \end{bmatrix}$$ And notice the rows are still individually sorted, in ascending order. I was trying to find a 'nice'  proof that does not involve messy index comparisons... but I cannot find one!","From Peter Winkler's book: Given a matrix, prove that after first sorting each row, then sorting each column, each row remains sorted. For example: starting with $$\begin{bmatrix} 1 & -3 & 2 \\ 0 & 1 & -5 \\ 4 & -1 & 1 \end{bmatrix}$$ Sorting each row individually and in ascending order gives $$\begin{bmatrix} -3 & 1 & 2 \\ -5 & 0 & 1 \\ -1 & 1 & 4 \end{bmatrix}$$ Then sorting each column individually in ascending order gives $$\begin{bmatrix} -5 & 0 & 1 \\ -3 & 1 & 2 \\ -1 & 1 & 4 \end{bmatrix}$$ And notice the rows are still individually sorted, in ascending order. I was trying to find a 'nice'  proof that does not involve messy index comparisons... but I cannot find one!",,"['matrices', 'puzzle', 'sorting', 'popular-math']"
43,What is the significance of reversing the polarity of the negative eigenvalues of a symmetric matrix?,What is the significance of reversing the polarity of the negative eigenvalues of a symmetric matrix?,,"Consider a full rank $n\times n$ symmetric matrix $A$ (coming from a set of physical measurements). I do an eigendecomposition of this matrix as $$A = E V E^T$$ Most of the eigenvalues are positive, while a few are negative but with much smaller magnitude compared to the maximum eigenvalue. I want to convert this matrix into a positive definite one but with minimal damage, for the purpose of my experiments. So I naively invert the polarity of the negative eigenvalues and construct a PSD matrix $$B = E |V| E'$$ Later, I realized there is a whole lot of research and algorithms available to find the nearest PSD matrix to a given symmetric matrix and what I am doing is not the right way. So I got hold of a few of those algorithms (with code) and tried out. These algorithms give nearest PSD matrix $C$ to $A$ than $B$ in terms of $2$-norm and Frobenius norm. Surprising part is when I use them in my experiments, I get far better results with $B$ than any of those algorithms. My understanding is that those algorithms usually end up in a low rank matrix (semi definite) and they do $C+\lambda I$ to make it make it a PSD where we get to choose $\lambda$. I am not able to understand fully why $B$ is giving good results than $C$ and what is the significance of $B$. I'd like to understand this as i want better solution than naively going for $B$ albeit it gives results.","Consider a full rank $n\times n$ symmetric matrix $A$ (coming from a set of physical measurements). I do an eigendecomposition of this matrix as $$A = E V E^T$$ Most of the eigenvalues are positive, while a few are negative but with much smaller magnitude compared to the maximum eigenvalue. I want to convert this matrix into a positive definite one but with minimal damage, for the purpose of my experiments. So I naively invert the polarity of the negative eigenvalues and construct a PSD matrix $$B = E |V| E'$$ Later, I realized there is a whole lot of research and algorithms available to find the nearest PSD matrix to a given symmetric matrix and what I am doing is not the right way. So I got hold of a few of those algorithms (with code) and tried out. These algorithms give nearest PSD matrix $C$ to $A$ than $B$ in terms of $2$-norm and Frobenius norm. Surprising part is when I use them in my experiments, I get far better results with $B$ than any of those algorithms. My understanding is that those algorithms usually end up in a low rank matrix (semi definite) and they do $C+\lambda I$ to make it make it a PSD where we get to choose $\lambda$. I am not able to understand fully why $B$ is giving good results than $C$ and what is the significance of $B$. I'd like to understand this as i want better solution than naively going for $B$ albeit it gives results.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
44,$I_m - AB$ invertible if and only if $I_n - BA$ invertible,invertible if and only if  invertible,I_m - AB I_n - BA,"Let $A$ and $B$ be $m\times n$ and $n\times m$ matrices respectively. Prove that if $\lambda$ is a non-zero eigenvalue of $AB$ then it is also an eigenvalue of $BA$ Prove that $I_m-AB$ is invertible if and only if $I_n-BA$ is invertible. Part (1) is easy: $$ABx=\lambda x$$ By definition, $x\ne 0$ , and by assumption $\lambda \ne 0$ . So we have $Bx\ne 0$ Now, $$B(AB)x=(BA)Bx=\lambda Bx$$ and so $Bx$ and a non-zero vector with eigenvalue $\lambda$ . My problem is I have no idea how to use this to do (2). Any help is much appreciated. Thanks!","Let and be and matrices respectively. Prove that if is a non-zero eigenvalue of then it is also an eigenvalue of Prove that is invertible if and only if is invertible. Part (1) is easy: By definition, , and by assumption . So we have Now, and so and a non-zero vector with eigenvalue . My problem is I have no idea how to use this to do (2). Any help is much appreciated. Thanks!",A B m\times n n\times m \lambda AB BA I_m-AB I_n-BA ABx=\lambda x x\ne 0 \lambda \ne 0 Bx\ne 0 B(AB)x=(BA)Bx=\lambda Bx Bx \lambda,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'inverse']"
45,How do I prove that $AB = BA$?,How do I prove that ?,AB = BA,"Two $n × n$ matrices $A$ and $B$ are said to be simultaneously diagonalizable if there exists an invertible $n × n$ matrix $S$ such that $S^{−1}AS$ and $S^{−1}BS$ are both diagonal. Prove that if $A$ and $B$ are simultaneously diagonalizable $n × n$ matrices, then $AB = BA$ My attempt: if A and B are simultaneously diagonalizable, then there exists an invertible $n × n$ matrix $S$ such that $S^{−1}AS$ and $S^{−1}BS$ are both diagonal (given). $A = S^{−1}AS$ $B = S^{−1}BS$ $AB  = S^{−1}AS \times S^{−1}BS  = BA = S^{−1}BS \times S^{−1}AS$ Is that a sufficient proof? I think I am going wrong somewhere...","Two $n × n$ matrices $A$ and $B$ are said to be simultaneously diagonalizable if there exists an invertible $n × n$ matrix $S$ such that $S^{−1}AS$ and $S^{−1}BS$ are both diagonal. Prove that if $A$ and $B$ are simultaneously diagonalizable $n × n$ matrices, then $AB = BA$ My attempt: if A and B are simultaneously diagonalizable, then there exists an invertible $n × n$ matrix $S$ such that $S^{−1}AS$ and $S^{−1}BS$ are both diagonal (given). $A = S^{−1}AS$ $B = S^{−1}BS$ $AB  = S^{−1}AS \times S^{−1}BS  = BA = S^{−1}BS \times S^{−1}AS$ Is that a sufficient proof? I think I am going wrong somewhere...",,"['linear-algebra', 'matrices']"
46,Determine if a matrix can be transformed to a nonnegative matrix,Determine if a matrix can be transformed to a nonnegative matrix,,"Non-negative matrices come up in relation to the Perron-Frobenius theorem.  The definition of a nonnegative matrix is that all of the matrix elements are greater than or equal to zero.  From this, the Perron-Frobenius theorem gives information about the eigenvalues of the matrix. However, having all positive matrix elements is not a basis-independent statement.  Simply switching a basis vector $\mathbf{e}_i\rightarrow -\mathbf{e}_i$ flips the sign of all the matrix elements in the $i$ column and row (except the diagonal).  So I was wondering if there is a more basis-independent definition of a nonnegative matrix, from which I could immediately infer that a basis exists where the matrix elements are positive, and also that I can immediately apply the Perron-Frobenius theorem.","Non-negative matrices come up in relation to the Perron-Frobenius theorem.  The definition of a nonnegative matrix is that all of the matrix elements are greater than or equal to zero.  From this, the Perron-Frobenius theorem gives information about the eigenvalues of the matrix. However, having all positive matrix elements is not a basis-independent statement.  Simply switching a basis vector $\mathbf{e}_i\rightarrow -\mathbf{e}_i$ flips the sign of all the matrix elements in the $i$ column and row (except the diagonal).  So I was wondering if there is a more basis-independent definition of a nonnegative matrix, from which I could immediately infer that a basis exists where the matrix elements are positive, and also that I can immediately apply the Perron-Frobenius theorem.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
47,How useful are non-square matrices in maths or sciences?,How useful are non-square matrices in maths or sciences?,,"I know that a matrix will be either square or rectangular matrix. I know that square matrices are used to solve a system of linear equations. But what's the use of rectangular matrices, why do we study them? Are they used somewhere in Math or science? Please answer my questions.","I know that a matrix will be either square or rectangular matrix. I know that square matrices are used to solve a system of linear equations. But what's the use of rectangular matrices, why do we study them? Are they used somewhere in Math or science? Please answer my questions.",,"['matrices', 'soft-question']"
48,Determinant of $2 \times 2$ matrix such that $A = A^{-1}$,Determinant of  matrix such that,2 \times 2 A = A^{-1},"Let $A$ be a $2 \times 2$ matrix such that $A = A^{-1}$. The value of $\operatorname{det} (A)$ can be: $\operatorname{det} (A)=-2$ $\operatorname{det} (A)=-1$ $\operatorname{det} (A)=0$ $\operatorname{det} (A)=2$ My attempt: $$\begin{bmatrix} a &b \\   c& d \end{bmatrix} = \frac{1}{ad-bc} \times \begin{bmatrix} d &-b \\   -c& a \end{bmatrix}$$ Obviously the determinant cannot be $0$, since an inverse exists, but I'm not sure how to proceed. Any help would be appreciated.","Let $A$ be a $2 \times 2$ matrix such that $A = A^{-1}$. The value of $\operatorname{det} (A)$ can be: $\operatorname{det} (A)=-2$ $\operatorname{det} (A)=-1$ $\operatorname{det} (A)=0$ $\operatorname{det} (A)=2$ My attempt: $$\begin{bmatrix} a &b \\   c& d \end{bmatrix} = \frac{1}{ad-bc} \times \begin{bmatrix} d &-b \\   -c& a \end{bmatrix}$$ Obviously the determinant cannot be $0$, since an inverse exists, but I'm not sure how to proceed. Any help would be appreciated.",,"['linear-algebra', 'matrices', 'determinant']"
49,"$A$ has more columns than rows and has full row rank, show there exist infinitely many $B$ s.t. $AB=I$","has more columns than rows and has full row rank, show there exist infinitely many  s.t.",A B AB=I,"If A $\in M_{m\times n}(R)$ such that $n>m$. Prove that if $\text{rank} (A) = m$ then there are infinitely many matrices $B \in \ M_{n\times m} (R)$ such that $ AB = I_m$ So the question is defining a matrix $A$, in a set of matrices where the number of columns is always greater than the number of rows, and if $A$ has full row rank, then there are infinitely many matrices $B$ such that $AB$ creates the identity matrix. I'm not sure where to go with this question, could anyone please help? ETA Thank you to everyone for their answers, the question makes sense now.","If A $\in M_{m\times n}(R)$ such that $n>m$. Prove that if $\text{rank} (A) = m$ then there are infinitely many matrices $B \in \ M_{n\times m} (R)$ such that $ AB = I_m$ So the question is defining a matrix $A$, in a set of matrices where the number of columns is always greater than the number of rows, and if $A$ has full row rank, then there are infinitely many matrices $B$ such that $AB$ creates the identity matrix. I'm not sure where to go with this question, could anyone please help? ETA Thank you to everyone for their answers, the question makes sense now.",,"['linear-algebra', 'matrices']"
50,Example for adjacency matrix of a bipartite graph,Example for adjacency matrix of a bipartite graph,,Can someone explain to me with an example how to create the adjacency matrix of a bipartite graph? And why the diagonal elements of it are not zero? Thanks.,Can someone explain to me with an example how to create the adjacency matrix of a bipartite graph? And why the diagonal elements of it are not zero? Thanks.,,"['linear-algebra', 'matrices', 'graph-theory']"
51,Let $M = \frac{1}{2}(A + {A^T})$ be real symmetric nonnegative matrix. Why does $\rho (A) \le {\lambda _{\max }}(M)$?,Let  be real symmetric nonnegative matrix. Why does ?,M = \frac{1}{2}(A + {A^T}) \rho (A) \le {\lambda _{\max }}(M),"Let $A \in  M_n$ be nonnegative, and consider the real symmetric nonnegative matrix $M = \frac{1}{2}(A + {A^T})$. Why does $\rho (A) \le {\lambda _{\max }}(M)$?","Let $A \in  M_n$ be nonnegative, and consider the real symmetric nonnegative matrix $M = \frac{1}{2}(A + {A^T})$. Why does $\rho (A) \le {\lambda _{\max }}(M)$?",,"['linear-algebra', 'matrices']"
52,Hessian matrix for convexity of multidimensional function,Hessian matrix for convexity of multidimensional function,,"To prove that a one dimensional differentiable function $f(x)$ is convex, it is quite obvious to see why we would check whether or not its second derivative is $>0$ or $<0.$ What is the intuition behind the claim that, if the Hessian $H$ of a multidimensional differentiable function $f(x_1,...,x_n)$ is positive semi-definite, it must be convex$?$ Is there a way I interpret this requirement, $y'H(x)y \ge 0\, \forall y$, geometrically$?$ (as in the case of one dimensional $f(x)$) How to interpret vector $y$ here$?$","To prove that a one dimensional differentiable function $f(x)$ is convex, it is quite obvious to see why we would check whether or not its second derivative is $>0$ or $<0.$ What is the intuition behind the claim that, if the Hessian $H$ of a multidimensional differentiable function $f(x_1,...,x_n)$ is positive semi-definite, it must be convex$?$ Is there a way I interpret this requirement, $y'H(x)y \ge 0\, \forall y$, geometrically$?$ (as in the case of one dimensional $f(x)$) How to interpret vector $y$ here$?$",,"['calculus', 'matrices', 'convex-analysis', 'hessian-matrix', 'positive-semidefinite']"
53,Cofactor multiplied with another row [duplicate],Cofactor multiplied with another row [duplicate],,"This question already has answers here : Taking product of cofactor with different row (2 answers) Closed 5 years ago . Why is it that when I add up the product of cofactors for one row and a corresponding element of any other row , the answer is 0? For example: This seems to work for all matrices but I'm unable to figure out why.","This question already has answers here : Taking product of cofactor with different row (2 answers) Closed 5 years ago . Why is it that when I add up the product of cofactors for one row and a corresponding element of any other row , the answer is 0? For example: This seems to work for all matrices but I'm unable to figure out why.",,"['matrices', 'determinant']"
54,Determinant tridiagonal matrix [duplicate],Determinant tridiagonal matrix [duplicate],,"This question already has answers here : How to compute the determinant of a tridiagonal Toeplitz matrix? (5 answers) Closed 8 years ago . Can anybody help me out with getting an expression of the values of $\lambda$ for a matrix $A$ for which $det(A-\lambda I)$ equals the determinant of a matrix with on the main diagonal $-\lambda$, on the diagonal above the main diagonal $\dfrac{1}{2}$ and on the diagonal under the main diagonal $\frac{1}{2} \lambda$.","This question already has answers here : How to compute the determinant of a tridiagonal Toeplitz matrix? (5 answers) Closed 8 years ago . Can anybody help me out with getting an expression of the values of $\lambda$ for a matrix $A$ for which $det(A-\lambda I)$ equals the determinant of a matrix with on the main diagonal $-\lambda$, on the diagonal above the main diagonal $\dfrac{1}{2}$ and on the diagonal under the main diagonal $\frac{1}{2} \lambda$.",,"['linear-algebra', 'matrices', 'determinant']"
55,$g$ is coercive for $g(x)=x^TAx+b^Tx+c$,is coercive for,g g(x)=x^TAx+b^Tx+c,"Suppose $A$ is a symmetric positive definite matrix $A\in \Bbb{R}^{n\times n}$, $b \in \Bbb{R}^n$, and c is a real number. Let $$g(x)=x^TAx + b^Tx + c$$ Show that $g$ is coercive. Because $A$ is positive definite, then $x^TAx\gt0\ \ , \forall x \ne0$ I tried expanding out $g(x)$ and got $$g(x)=(a_{11}x_1^2+a_{22}x_2^2+...+a_{nn}x_n^2+2x_1(a_{12}x_2+a_{13}x_3+...+a_{1n}x_n)+2x_2(a_{23}x_3+...+a_{2n}x_n)+...+2a_{n-1,n}x_{n-1}x_n)+(b_1x_1+b_2x_2+...+b_nx_n)+c$$ Now I'm stuck as to how to show that $$\lim_{|x|\to\infty} g(x)=\infty$$","Suppose $A$ is a symmetric positive definite matrix $A\in \Bbb{R}^{n\times n}$, $b \in \Bbb{R}^n$, and c is a real number. Let $$g(x)=x^TAx + b^Tx + c$$ Show that $g$ is coercive. Because $A$ is positive definite, then $x^TAx\gt0\ \ , \forall x \ne0$ I tried expanding out $g(x)$ and got $$g(x)=(a_{11}x_1^2+a_{22}x_2^2+...+a_{nn}x_n^2+2x_1(a_{12}x_2+a_{13}x_3+...+a_{1n}x_n)+2x_2(a_{23}x_3+...+a_{2n}x_n)+...+2a_{n-1,n}x_{n-1}x_n)+(b_1x_1+b_2x_2+...+b_nx_n)+c$$ Now I'm stuck as to how to show that $$\lim_{|x|\to\infty} g(x)=\infty$$",,"['matrices', 'nonlinear-optimization']"
56,"If $AA^*=AA$, how to prove $A$ is an Hermitian? [duplicate]","If , how to prove  is an Hermitian? [duplicate]",AA^*=AA A,"This question already has answers here : $A^2=A^*A$. Why is matrix $A$ Hermitian? [duplicate] (2 answers) Closed 8 years ago . If $A$ is an $n \times n$ matrix and $AA^*=AA,$ how to prove $A$ is Hermitian?","This question already has answers here : $A^2=A^*A$. Why is matrix $A$ Hermitian? [duplicate] (2 answers) Closed 8 years ago . If $A$ is an $n \times n$ matrix and $AA^*=AA,$ how to prove $A$ is Hermitian?",,"['linear-algebra', 'matrices']"
57,"Does every projection operator satisfy $\|Px\| \leq \|x\|\,$?",Does every projection operator satisfy ?,"\|Px\| \leq \|x\|\,","It's well known that an orthogonal projection satisfies $\|Px\|\leq\|x\|$ . Does this property hold for any general projection operator $P$ , which is defined by $P^2=P$ ?","It's well known that an orthogonal projection satisfies . Does this property hold for any general projection operator , which is defined by ?",\|Px\|\leq\|x\| P P^2=P,"['linear-algebra', 'matrices', 'operator-theory', 'projection', 'idempotents']"
58,Find the range for $AX-XA$,Find the range for,AX-XA,"I don't know how to prove the following and need help. Let $M_{n\times n}$ be the vector space of matrices over $\mathbb{C}$. Let $A\in M_{n\times n}$ be fixed matrices, and $X\in M_{n\times n}$ be variable matrix. Define the linear transformation $T$ on $M_{n\times n}$ by $T(X)=AX-XA$. Prove that the range of $T$ is at most $n^2-n$. My idea is to use Kronecker Product to convert $AX-XA$ to a vector form equation like $\overline{A}{y}$ but got stuck on how to proceed.","I don't know how to prove the following and need help. Let $M_{n\times n}$ be the vector space of matrices over $\mathbb{C}$. Let $A\in M_{n\times n}$ be fixed matrices, and $X\in M_{n\times n}$ be variable matrix. Define the linear transformation $T$ on $M_{n\times n}$ by $T(X)=AX-XA$. Prove that the range of $T$ is at most $n^2-n$. My idea is to use Kronecker Product to convert $AX-XA$ to a vector form equation like $\overline{A}{y}$ but got stuck on how to proceed.",,['linear-algebra']
59,"Let $n$ be a positive integer. If $A∈\mathscr{M}_{n×n}(\mathbb{C})$, show that $A$ and $A^T$ are similar.","Let  be a positive integer. If , show that  and  are similar.",n A∈\mathscr{M}_{n×n}(\mathbb{C}) A A^T,"Let $n$ be a positive integer. If $A∈\mathscr{M}_{n×n}(\mathbb{C})$, show that $A$ and $A^T$ are similar. I have that $A=BC$ where $B,C$ are symmetric, then  $A^T=(BC)^T=C^TB^T=CB$ and then $AB=BCB=BA^T$ but I'm not sure if $B$ is nonsingular, or if I should try another way. Thanks.","Let $n$ be a positive integer. If $A∈\mathscr{M}_{n×n}(\mathbb{C})$, show that $A$ and $A^T$ are similar. I have that $A=BC$ where $B,C$ are symmetric, then  $A^T=(BC)^T=C^TB^T=CB$ and then $AB=BCB=BA^T$ but I'm not sure if $B$ is nonsingular, or if I should try another way. Thanks.",,"['linear-algebra', 'matrices']"
60,"Some questions about notation in ""$[T]_\alpha^\beta$""","Some questions about notation in """"",[T]_\alpha^\beta,"I just have a few questions about the general meaning of the notation ""$[T]_\alpha^\beta$"". I would really appreciate if someone would dumb it WAY down to the most basic level (no assumptions, no leaps of logic) because most of the literature I have read on this notation is very scattered. I want to mention that $\alpha$ and $\beta$ are the ordered bases for $R^n$ and $R^m$ respectively. $T$ is the linear transformation from $R^n \to R^m$. Questions: What is $[T]$? What are the subscript and superscript? Does the order of the subscript and superscript matter (which one is  on top or bottom)? What are the dimensions of $[T]_\alpha^\beta$? Thank you guys so much.","I just have a few questions about the general meaning of the notation ""$[T]_\alpha^\beta$"". I would really appreciate if someone would dumb it WAY down to the most basic level (no assumptions, no leaps of logic) because most of the literature I have read on this notation is very scattered. I want to mention that $\alpha$ and $\beta$ are the ordered bases for $R^n$ and $R^m$ respectively. $T$ is the linear transformation from $R^n \to R^m$. Questions: What is $[T]$? What are the subscript and superscript? Does the order of the subscript and superscript matter (which one is  on top or bottom)? What are the dimensions of $[T]_\alpha^\beta$? Thank you guys so much.",,"['linear-algebra', 'matrices', 'notation', 'linear-transformations']"
61,"Let $A,B \in {M_2}$ and $C=AB-BA$. Why is ${C^2} = \lambda I$ true?",Let  and . Why is  true?,"A,B \in {M_2} C=AB-BA {C^2} = \lambda I","Let $A,B \in {M_2}$ and $C=AB-BA$. Why does ${C^2} = \lambda I$?","Let $A,B \in {M_2}$ and $C=AB-BA$. Why does ${C^2} = \lambda I$?",,"['linear-algebra', 'matrices']"
62,Is $\text{rank} (A^*A)=\text{rank}(A)$ for all nonsquare matrices? [duplicate],Is  for all nonsquare matrices? [duplicate],\text{rank} (A^*A)=\text{rank}(A),"This question already has answers here : Prove $\operatorname{rank}A^TA=\operatorname{rank}A$ for any $A\in M_{m \times n}$ (4 answers) Closed 9 years ago . The community reviewed whether to reopen this question 5 months ago and left it closed: Original close reason(s) were not resolved If $A$ is a $m\times n$ type matrix with $m\geq n$ then $$ \mathrm{rank}(A^*A)=\mathrm{rank} (A). $$ Is maybe also true in general that $$ \mathrm{rank}(AA^*)=\mathrm{rank}(A) ? $$ Thanks Edit. My question is different from the question about $\mathrm{rank}(A^TA)=\mathrm{rank}(A)$ , because this concerns complex conjugate transposed matrix instead of transpose.","This question already has answers here : Prove $\operatorname{rank}A^TA=\operatorname{rank}A$ for any $A\in M_{m \times n}$ (4 answers) Closed 9 years ago . The community reviewed whether to reopen this question 5 months ago and left it closed: Original close reason(s) were not resolved If is a type matrix with then Is maybe also true in general that Thanks Edit. My question is different from the question about , because this concerns complex conjugate transposed matrix instead of transpose.","A m\times n m\geq n 
\mathrm{rank}(A^*A)=\mathrm{rank} (A).
 
\mathrm{rank}(AA^*)=\mathrm{rank}(A) ?
 \mathrm{rank}(A^TA)=\mathrm{rank}(A)","['linear-algebra', 'matrices', 'matrix-rank']"
63,What is a skew-symmetric matrix?,What is a skew-symmetric matrix?,,What is a skew-symmetric matrix? It came under types of matrices in my text. I have learned square matrices.,What is a skew-symmetric matrix? It came under types of matrices in my text. I have learned square matrices.,,"['matrices', 'skew-symmetric-matrices']"
64,Eigenvectors of almost-Toeplitz tridiagonal matrix.,Eigenvectors of almost-Toeplitz tridiagonal matrix.,,"I'm reading a book about semi-conductors, and when figuring out how dopants affect the energy-levels, one wishes to find the eigenvalues (and vectors) of a NxN tridiagonal matrix of the form $$\begin{pmatrix} E_1 & -A \\ -A & \ddots & -A \\ & -A & (E_1-F) & \ddots \\ & & \ddots & \ddots & -A \\ & & & -A & E_1 \end{pmatrix}$$ for $E_1,A,F \in \mathbb{R}$. That is, it's Toeplitz except for the value in the middle of the matrix being $E_1-F$ instead of $E_1$. The book mentions that this eigenvalue can be solved by hand, but doesn't do it. I've been trying and failing to solve it (except for $F=0$), and was hoping that someone here could shed some light on the problem!","I'm reading a book about semi-conductors, and when figuring out how dopants affect the energy-levels, one wishes to find the eigenvalues (and vectors) of a NxN tridiagonal matrix of the form $$\begin{pmatrix} E_1 & -A \\ -A & \ddots & -A \\ & -A & (E_1-F) & \ddots \\ & & \ddots & \ddots & -A \\ & & & -A & E_1 \end{pmatrix}$$ for $E_1,A,F \in \mathbb{R}$. That is, it's Toeplitz except for the value in the middle of the matrix being $E_1-F$ instead of $E_1$. The book mentions that this eigenvalue can be solved by hand, but doesn't do it. I've been trying and failing to solve it (except for $F=0$), and was hoping that someone here could shed some light on the problem!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'physics']"
65,"Calculate inverse of arbitrarily sized, lower triangle matrix with a specific pattern.","Calculate inverse of arbitrarily sized, lower triangle matrix with a specific pattern.",,"I have a matrix of the following form: $$A=\begin{bmatrix} 2 & 0 & 0 & 0 \\-1 & 2 & 0 & 0\\ 0 & -1 & 2 & 0\\ 0 & 0 & -1 & 2 \end{bmatrix}$$ which, in general, can be any size with the pattern continued parallel the diagonal.  (I didn't know how to show ellipsis in a matrix with LaTeX, feel free to edit if you know how). I want to find an inverse for this matrix in general for any given size of the matrix.  What should I do?  I had thought of maybe splitting the matrix up into two matrices, one with all the diagonal terms and one with all the off-diagonal terms and finding the inverse of their sum but I didn't know if that would be helpful/possible.","I have a matrix of the following form: $$A=\begin{bmatrix} 2 & 0 & 0 & 0 \\-1 & 2 & 0 & 0\\ 0 & -1 & 2 & 0\\ 0 & 0 & -1 & 2 \end{bmatrix}$$ which, in general, can be any size with the pattern continued parallel the diagonal.  (I didn't know how to show ellipsis in a matrix with LaTeX, feel free to edit if you know how). I want to find an inverse for this matrix in general for any given size of the matrix.  What should I do?  I had thought of maybe splitting the matrix up into two matrices, one with all the diagonal terms and one with all the off-diagonal terms and finding the inverse of their sum but I didn't know if that would be helpful/possible.",,"['matrices', 'inverse']"
66,Closest matrix with specific eigenvector,Closest matrix with specific eigenvector,,"Consider a vector ${\bf x}$ and a matrix $A_0$ with $A_0(i,j)\ge 0$. What is the best way of getting matrix $A$ s.t. $$A = \arg \min \|A-A_0\|_{\text F}$$ subject to $$A{\bf x} = \lambda {\bf x} \hspace{2mm} \mbox{and} \hspace{2mm} A(i,j)\ge0$$ where $\|\cdot\|_{\text F}$ denotes the Frobenius norm?","Consider a vector ${\bf x}$ and a matrix $A_0$ with $A_0(i,j)\ge 0$. What is the best way of getting matrix $A$ s.t. $$A = \arg \min \|A-A_0\|_{\text F}$$ subject to $$A{\bf x} = \lambda {\bf x} \hspace{2mm} \mbox{and} \hspace{2mm} A(i,j)\ge0$$ where $\|\cdot\|_{\text F}$ denotes the Frobenius norm?",,"['matrices', 'optimization', 'convex-optimization']"
67,"What is the matrix used to find the reflected (x, y) coordinate in the line y=mx?","What is the matrix used to find the reflected (x, y) coordinate in the line y=mx?",,"I hope this makes sense, I'm essentially looking for a matrix in which you can just substitute in the gradient m from y=mx and find the reflected coordinates? If this doesn't make any sense please say why? Regards Tom","I hope this makes sense, I'm essentially looking for a matrix in which you can just substitute in the gradient m from y=mx and find the reflected coordinates? If this doesn't make any sense please say why? Regards Tom",,['matrices']
68,$\det(I+A\bar{A}) \ge 0$ for all complex square matrices $A$? [closed],for all complex square matrices ? [closed],\det(I+A\bar{A}) \ge 0 A,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question $\det(I+A\bar{A}) \ge 0$ Is it possible to prove the inequality is true for all complex square matrices $A$ where $I$ is the identity matrix and $\bar{A}$ is the complex conjugated matrix.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question Is it possible to prove the inequality is true for all complex square matrices where is the identity matrix and is the complex conjugated matrix.",\det(I+A\bar{A}) \ge 0 A I \bar{A},"['linear-algebra', 'matrices']"
69,Finding determinant of $n \times n$ matrix,Finding determinant of  matrix,n \times n,"I need to find a determinant of the matrix: $$  A = \begin{pmatrix} 1 & 2 & 3 & \cdot & \cdot & \cdot & n \\ x & 1 & 2 & 3 & \cdot & \cdot & n-1 \\ x & x & 1 & 2 & 3 & \cdot & n-2 \\ \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\ x & x & \cdot & \cdot & x & 1 & 2 \\ x & x & \cdot & \cdot & \cdot & x & 1 \\      \end{pmatrix} $$ We know that $x \in R$ So far I managed to transform it to the form: $$  \begin{pmatrix} 1-x & 1 & 1 & \cdot & \cdot & \cdot & 1 \\ 0 & 1-x & 1 & 1 & \cdot & \cdot & 1 \\ 0 & 0 & 1-x & 1 & 1 & \cdot & 1 \\ \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\ 0 & 0 & \cdot & \cdot & 0 & 1-x & 1 \\ x & x & \cdot & \cdot & \cdot & x & 1 \\      \end{pmatrix} $$ by the operations: (Let's say $r_i$ is the ith row)  $$r_1 = r_1 - r_n,r_2 = r_2-r_n, r_3 = r_3 - r_n, ..., r_{n-1} = r_{n-1} - r_n$$ and then  $$r_1 = r_1 - r_2, r_2 = r_2 - r_3, r_3 = r_3 - r_4,...,r_{n-2} = r_{n-2} - r_{n-1}$$ Unfortunately, I have no idea how to eliminate the last row. Any hints?","I need to find a determinant of the matrix: $$  A = \begin{pmatrix} 1 & 2 & 3 & \cdot & \cdot & \cdot & n \\ x & 1 & 2 & 3 & \cdot & \cdot & n-1 \\ x & x & 1 & 2 & 3 & \cdot & n-2 \\ \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\ x & x & \cdot & \cdot & x & 1 & 2 \\ x & x & \cdot & \cdot & \cdot & x & 1 \\      \end{pmatrix} $$ We know that $x \in R$ So far I managed to transform it to the form: $$  \begin{pmatrix} 1-x & 1 & 1 & \cdot & \cdot & \cdot & 1 \\ 0 & 1-x & 1 & 1 & \cdot & \cdot & 1 \\ 0 & 0 & 1-x & 1 & 1 & \cdot & 1 \\ \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\ 0 & 0 & \cdot & \cdot & 0 & 1-x & 1 \\ x & x & \cdot & \cdot & \cdot & x & 1 \\      \end{pmatrix} $$ by the operations: (Let's say $r_i$ is the ith row)  $$r_1 = r_1 - r_n,r_2 = r_2-r_n, r_3 = r_3 - r_n, ..., r_{n-1} = r_{n-1} - r_n$$ and then  $$r_1 = r_1 - r_2, r_2 = r_2 - r_3, r_3 = r_3 - r_4,...,r_{n-2} = r_{n-2} - r_{n-1}$$ Unfortunately, I have no idea how to eliminate the last row. Any hints?",,"['matrices', 'determinant']"
70,Are $\vec{v}$ and $\vec{w}$ linearly independent?,Are  and  linearly independent?,\vec{v} \vec{w},"Am I correct in saying that $\vec{v}=\begin{pmatrix}1\\2\\0\end{pmatrix}$ and $\vec{w}=\begin{pmatrix}2\\4a\\a-1\end{pmatrix}$ are linearly independent $\forall a\in\mathbb R$ /{1}? It seems to me that the only way the second vector can be a multiple of the first is if $a=1$. How could I prove this? I tried something but I don't know if that is correct: 1) $\vec{v} ,\vec{w}$ are linearly independent if and only if the only solution to $\lambda\vec{v}+\mu\vec{w}=\vec{0}$ is $\lambda=\mu=0$ 2) $\lambda\begin{pmatrix}1\\2\\0\end{pmatrix}+\mu\begin{pmatrix}2\\4a\\a-1\end{pmatrix}=\begin{pmatrix}0\\0\\0\end{pmatrix}$ $\Rightarrow \lambda+2\mu=0$ $2\lambda+4a\mu=0$ $\mu a-\mu=0$ $\Rightarrow a=1$ I am not sure why I am getting $a=1$ because isn't that the solution where they are linearly dependent?","Am I correct in saying that $\vec{v}=\begin{pmatrix}1\\2\\0\end{pmatrix}$ and $\vec{w}=\begin{pmatrix}2\\4a\\a-1\end{pmatrix}$ are linearly independent $\forall a\in\mathbb R$ /{1}? It seems to me that the only way the second vector can be a multiple of the first is if $a=1$. How could I prove this? I tried something but I don't know if that is correct: 1) $\vec{v} ,\vec{w}$ are linearly independent if and only if the only solution to $\lambda\vec{v}+\mu\vec{w}=\vec{0}$ is $\lambda=\mu=0$ 2) $\lambda\begin{pmatrix}1\\2\\0\end{pmatrix}+\mu\begin{pmatrix}2\\4a\\a-1\end{pmatrix}=\begin{pmatrix}0\\0\\0\end{pmatrix}$ $\Rightarrow \lambda+2\mu=0$ $2\lambda+4a\mu=0$ $\mu a-\mu=0$ $\Rightarrow a=1$ I am not sure why I am getting $a=1$ because isn't that the solution where they are linearly dependent?",,"['linear-algebra', 'matrices', 'vectors']"
71,A symmetric matrix with eigenvalues all $0$ or all $1$: does it equal $0$ or identity?,A symmetric matrix with eigenvalues all  or all : does it equal  or identity?,0 1 0,"I have these general wondering about matrices but I don't know to proceed with a proof or a counter example. Suppose that $A$ (dimension $n\times n$) is a real symmetric matrix. If $A$ has $n$ eigenvalues that are all $1$'s, does $A$ equal the identity matrix? If $A$ has $n$ eigenvalues that are all $0$'s, does $A$ equal the zero matrix? Can someone elucidate things for me please? Edit : I learned/can look up diagonalization theorems for real matrices.","I have these general wondering about matrices but I don't know to proceed with a proof or a counter example. Suppose that $A$ (dimension $n\times n$) is a real symmetric matrix. If $A$ has $n$ eigenvalues that are all $1$'s, does $A$ equal the identity matrix? If $A$ has $n$ eigenvalues that are all $0$'s, does $A$ equal the zero matrix? Can someone elucidate things for me please? Edit : I learned/can look up diagonalization theorems for real matrices.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
72,"Determining the dimension of span$\{AB-BA : A,B \in M_{n \times n}(\mathbb R) \}$",Determining the dimension of span,"\{AB-BA : A,B \in M_{n \times n}(\mathbb R) \}","Let $S$ be the subspace , of  $M_{n \times n}(\mathbb R)$ (the vector space of all $n \times n$ real matrices ) , generated by matrices of the form $AB-BA$ , where $A,B \in M_{n \times n}(\mathbb R)$ , then how do we prove that $\dim S=n^2-1$ ? The only thing that I can determine is that the trace of all matrices of $S$ is $0$ . Please help","Let $S$ be the subspace , of  $M_{n \times n}(\mathbb R)$ (the vector space of all $n \times n$ real matrices ) , generated by matrices of the form $AB-BA$ , where $A,B \in M_{n \times n}(\mathbb R)$ , then how do we prove that $\dim S=n^2-1$ ? The only thing that I can determine is that the trace of all matrices of $S$ is $0$ . Please help",,"['linear-algebra', 'matrices', 'vector-spaces']"
73,"How prove this matrix $B^{-1}-A^{-1}$ is positive-semidefinite matrix,if $A-B$ is positive matrix","How prove this matrix  is positive-semidefinite matrix,if  is positive matrix",B^{-1}-A^{-1} A-B,"Question: Let $A,B$ be positive $n\times n$ matrices, and assume that $A-B$ is also a positive definite matrix. Show that $$B^{-1}-A^{-1}$$ is a positive definite matrix too. My idea: since $A,B$ be positive matrix,and then exist non-singular matrices $P,Q$ ,such $$A=P^{-1}\operatorname{diag}{(a_{1},a_{2},\cdots,a_{n})}P$$ $$B=Q^{-1}\operatorname{diag}{(b_{1},b_{2},\cdots,b_{n})}Q$$ where $a_{i},b_{i}>0$","Question: Let be positive matrices, and assume that is also a positive definite matrix. Show that is a positive definite matrix too. My idea: since be positive matrix,and then exist non-singular matrices ,such where","A,B n\times n A-B B^{-1}-A^{-1} A,B P,Q A=P^{-1}\operatorname{diag}{(a_{1},a_{2},\cdots,a_{n})}P B=Q^{-1}\operatorname{diag}{(b_{1},b_{2},\cdots,b_{n})}Q a_{i},b_{i}>0","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-calculus', 'matrix-decomposition']"
74,Proving definition of norms induced by vector norms,Proving definition of norms induced by vector norms,,"I know the title isn't very clear. The question is the following. Let $\|\cdot\|$ be a vector norm on $\mathbb{C}^n$. Define the induced norm on $\mathbb{C}^{n\times n}$ as: $$\|A\|_{op}=\max\limits_{x\in\mathbb{C}^n\smallsetminus\{0\}}\dfrac{\|Ax\|}{\|x\|}.$$ How do I prove that: If $\|\cdot\|_{2}$ is the Euclidean norm, then: $$\|A\|_{2}=\rho^{\frac12}(A^{*}A),$$ with $\rho(A)$ the spectral radius of $A$ and $A^{*}$ the conjugate transpose of $A$? If $\|\cdot\|_{\infty}$ is the max norm ($\|x\|_{\infty}=\max|x_i|$, $x_i$ being the components of $x$) then: $$\|A\|_{\infty}=\max_{i=1,\dots,n}\left(\sum\limits_{j=1}^n|a_{ij}|\right)?$$ And that if $\|\cdot\|_{1}$ is the sum norm ($\|x\|_{1}=\sum_{i=1}^n|x_i|$, $x_i$ being, again, the components of $x$), then: $$\|A\|_{1}=\max\limits_{j=1,\dots,n}\left(\sum\limits_{i=1}^n|a_{ij}|\right)?$$","I know the title isn't very clear. The question is the following. Let $\|\cdot\|$ be a vector norm on $\mathbb{C}^n$. Define the induced norm on $\mathbb{C}^{n\times n}$ as: $$\|A\|_{op}=\max\limits_{x\in\mathbb{C}^n\smallsetminus\{0\}}\dfrac{\|Ax\|}{\|x\|}.$$ How do I prove that: If $\|\cdot\|_{2}$ is the Euclidean norm, then: $$\|A\|_{2}=\rho^{\frac12}(A^{*}A),$$ with $\rho(A)$ the spectral radius of $A$ and $A^{*}$ the conjugate transpose of $A$? If $\|\cdot\|_{\infty}$ is the max norm ($\|x\|_{\infty}=\max|x_i|$, $x_i$ being the components of $x$) then: $$\|A\|_{\infty}=\max_{i=1,\dots,n}\left(\sum\limits_{j=1}^n|a_{ij}|\right)?$$ And that if $\|\cdot\|_{1}$ is the sum norm ($\|x\|_{1}=\sum_{i=1}^n|x_i|$, $x_i$ being, again, the components of $x$), then: $$\|A\|_{1}=\max\limits_{j=1,\dots,n}\left(\sum\limits_{i=1}^n|a_{ij}|\right)?$$",,"['linear-algebra', 'matrices', 'matrix-norms']"
75,"If matrix $A$ is invertible, then there is a permutation of its rows leaving no-zeros on the diagonal","If matrix  is invertible, then there is a permutation of its rows leaving no-zeros on the diagonal",A,"I need to prove this statement: "" If $A$ invertible, then exist a permutation of its rows leaving no-zeros on the diagonal "" and I tried using the definitos of invertible matrices and $LU$ factorization, but without results. Can you help me, please?","I need to prove this statement: "" If $A$ invertible, then exist a permutation of its rows leaving no-zeros on the diagonal "" and I tried using the definitos of invertible matrices and $LU$ factorization, but without results. Can you help me, please?",,"['linear-algebra', 'matrices', 'permutations']"
76,How prove this $|A||M|=A_{11}A_{nn}-A_{1n}A_{n1}$ [duplicate],How prove this  [duplicate],|A||M|=A_{11}A_{nn}-A_{1n}A_{n1},"This question already has answers here : Determinant identity: $\det M \det N = \det M_{ii} \det M_{jj} - \det M_{ij}\det M_{ji}$ (2 answers) Closed 9 years ago . Question: let the matrix $A=(a_{ij})_{n\times n},i=1,2,\cdots,n,j=1,2,\cdots,n$,  and the matrix $M=(a_{ij})_{(n-2)\times (n-2)},$ mean  that $$A=\begin{bmatrix} a_{11}&\cdots&a_{1n}\\ \vdots& M&\vdots\\ a_{n1}&\cdots&a_{nn} \end{bmatrix}$$ show that $$\det|A|\cdot \det |M|=A_{11}A_{nn}-A_{1n}A_{n1}$$ where $A_{ij}$ is cofactor with the matrix $A$. This problem is from linear problem book ,and this problem I can't deal it. because this value $$|A||M|$$ I can't choose something to  deal it?","This question already has answers here : Determinant identity: $\det M \det N = \det M_{ii} \det M_{jj} - \det M_{ij}\det M_{ji}$ (2 answers) Closed 9 years ago . Question: let the matrix $A=(a_{ij})_{n\times n},i=1,2,\cdots,n,j=1,2,\cdots,n$,  and the matrix $M=(a_{ij})_{(n-2)\times (n-2)},$ mean  that $$A=\begin{bmatrix} a_{11}&\cdots&a_{1n}\\ \vdots& M&\vdots\\ a_{n1}&\cdots&a_{nn} \end{bmatrix}$$ show that $$\det|A|\cdot \det |M|=A_{11}A_{nn}-A_{1n}A_{n1}$$ where $A_{ij}$ is cofactor with the matrix $A$. This problem is from linear problem book ,and this problem I can't deal it. because this value $$|A||M|$$ I can't choose something to  deal it?",,"['matrices', 'determinant', 'matrix-calculus']"
77,Prove that $A^{t}A$ is positive definite,Prove that  is positive definite,A^{t}A,"$A$ is an invertible matrix over $\mathbb{R}$ (nxn). Show that $A^{T}A$ is positive definite. I looked up for it and found this two relevent posts but still need help. positive definite and transpose help me understand a line in an “$A^TA$ is positive, semi-definite” proof Any suggestions? thanks (The whole excercise is about bilinear forms)","$A$ is an invertible matrix over $\mathbb{R}$ (nxn). Show that $A^{T}A$ is positive definite. I looked up for it and found this two relevent posts but still need help. positive definite and transpose help me understand a line in an “$A^TA$ is positive, semi-definite” proof Any suggestions? thanks (The whole excercise is about bilinear forms)",,"['linear-algebra', 'matrices', 'bilinear-form']"
78,Find maximal possible determinant value given constraint,Find maximal possible determinant value given constraint,,"Task is to find maximal possible determinant value for 2x2 and 3x3 matrices given following constraint:  $$\sum_{i,j=1}^na_{ij}^2 \le 1$$ I was able to come up with solution, but I received the test result, where I scored zero for this task. I still hasn't got a chance to take a look at my examined paper, but for now I just want to find whether or not my solution was actually true. Solution First trivial idea is that optimum would be reached in the point of exact equality $\sum_{i,j=1}^na_{ij}^2 = 1$. Then, terms which participate in det calculation with minus sign, must be negative or zero. $$\left( \begin{matrix}  \frac 1 {\sqrt 2} & 0 \\ 0 & \frac 1 {\sqrt 2} \end{matrix}\right)$$ $$\left( \begin{matrix} \frac 1 {\sqrt 3} & 0 & 0 \\ 0 & \frac 1 {\sqrt 3} & 0 \\ 0 & 0 & \frac 1 {\sqrt 3} \end{matrix}\right)$$ So, I came up with these matrices and respective det values are $\frac 1 2$ and $\frac 1 {3\sqrt 3}$. Is that correct? Thanks!","Task is to find maximal possible determinant value for 2x2 and 3x3 matrices given following constraint:  $$\sum_{i,j=1}^na_{ij}^2 \le 1$$ I was able to come up with solution, but I received the test result, where I scored zero for this task. I still hasn't got a chance to take a look at my examined paper, but for now I just want to find whether or not my solution was actually true. Solution First trivial idea is that optimum would be reached in the point of exact equality $\sum_{i,j=1}^na_{ij}^2 = 1$. Then, terms which participate in det calculation with minus sign, must be negative or zero. $$\left( \begin{matrix}  \frac 1 {\sqrt 2} & 0 \\ 0 & \frac 1 {\sqrt 2} \end{matrix}\right)$$ $$\left( \begin{matrix} \frac 1 {\sqrt 3} & 0 & 0 \\ 0 & \frac 1 {\sqrt 3} & 0 \\ 0 & 0 & \frac 1 {\sqrt 3} \end{matrix}\right)$$ So, I came up with these matrices and respective det values are $\frac 1 2$ and $\frac 1 {3\sqrt 3}$. Is that correct? Thanks!",,"['linear-algebra', 'matrices', 'proof-verification', 'solution-verification']"
79,Determinant of specially structured block matrix,Determinant of specially structured block matrix,,How do you compute the determinant of the following $nd \times nd$ block matrix? $$M = \begin{bmatrix}A+B & A & A & \dots & A & A\\ A & A+B & A & \dots & A & A\\ A & A & A+B & \dots & A & A\\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ A & A & A & \dots & A+B & A\\ A & A & A & \dots & A & A+B\end{bmatrix}$$ where $A$ and $B$ are $d \times d$ matrices.,How do you compute the determinant of the following block matrix? where and are matrices.,nd \times nd M = \begin{bmatrix}A+B & A & A & \dots & A & A\\ A & A+B & A & \dots & A & A\\ A & A & A+B & \dots & A & A\\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ A & A & A & \dots & A+B & A\\ A & A & A & \dots & A & A+B\end{bmatrix} A B d \times d,"['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
80,Eigenvectors of the Zero Matrix,Eigenvectors of the Zero Matrix,,"Given the following matrix: $ \begin{pmatrix}    -1 & 0 \\     0 & -1 \\     \end{pmatrix} $. I have to calculate the eigenvalues and eigenvectors for this matrix, and I have calculated that this matrix has an eigenvalue of $-1$ with multiplicity $2$ However, here is where my problem comes in: To calculate the eigenvector, I need to use: $$ \begin{pmatrix}    -1-\lambda & 0 \\               0 & -1-\lambda\ \\     \end{pmatrix} $$ Multiply it by $$ \begin{pmatrix}                                x \\                                y \\                                \end{pmatrix} $$ and set it equal to $$ \lambda\ \begin{pmatrix}                                x \\                                y \\                                \end{pmatrix} $$ Using my value of $\lambda = -1$, I end up having the following: $ \begin{pmatrix}    0 & 0 \\               0 & 0 \\     \end{pmatrix} $ which equals $ \begin{pmatrix}                                -x \\                                -y \\                                \end{pmatrix}. $ However, apparently I am meant to get an eigenvector of $  \begin{pmatrix}                                1 \\                                0 \\                                \end{pmatrix} $. I have no idea where I am going wrong","Given the following matrix: $ \begin{pmatrix}    -1 & 0 \\     0 & -1 \\     \end{pmatrix} $. I have to calculate the eigenvalues and eigenvectors for this matrix, and I have calculated that this matrix has an eigenvalue of $-1$ with multiplicity $2$ However, here is where my problem comes in: To calculate the eigenvector, I need to use: $$ \begin{pmatrix}    -1-\lambda & 0 \\               0 & -1-\lambda\ \\     \end{pmatrix} $$ Multiply it by $$ \begin{pmatrix}                                x \\                                y \\                                \end{pmatrix} $$ and set it equal to $$ \lambda\ \begin{pmatrix}                                x \\                                y \\                                \end{pmatrix} $$ Using my value of $\lambda = -1$, I end up having the following: $ \begin{pmatrix}    0 & 0 \\               0 & 0 \\     \end{pmatrix} $ which equals $ \begin{pmatrix}                                -x \\                                -y \\                                \end{pmatrix}. $ However, apparently I am meant to get an eigenvector of $  \begin{pmatrix}                                1 \\                                0 \\                                \end{pmatrix} $. I have no idea where I am going wrong",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
81,Left and right eigenvectors perpendicular to each other,Left and right eigenvectors perpendicular to each other,,"I just read in a textbook on numerical methods that you can always have that the right eigenvectors of a matrix can be taken as orthonormal to the left eigenvectors for a diagonalisable matrix. This seems suprising to me. I would have assumed that you need to have that the matrix is normal or hermitian, but is this also possible for all matrices?","I just read in a textbook on numerical methods that you can always have that the right eigenvectors of a matrix can be taken as orthonormal to the left eigenvectors for a diagonalisable matrix. This seems suprising to me. I would have assumed that you need to have that the matrix is normal or hermitian, but is this also possible for all matrices?",,"['linear-algebra', 'matrices']"
82,"determinant inequality, $AB=BA$, then $ \det(A^2+B^2)\ge \det(2AB) $","determinant inequality, , then",AB=BA  \det(A^2+B^2)\ge \det(2AB) ,"$A$ and $B$ are two  $n\times n $ real matrices, $AB=BA$.  Can we conclude that $$ \det \Big(A^2+B^2\Big)\ge \det(2AB) $$ is right? Well, the inequality is interesting. if $A,B$ are upper triangular matrices, it is obvious right. If $AB\ne BA$, $ \det \Big(A^2+B^2\Big)\ge \det(AB+BA) $ is wrong.","$A$ and $B$ are two  $n\times n $ real matrices, $AB=BA$.  Can we conclude that $$ \det \Big(A^2+B^2\Big)\ge \det(2AB) $$ is right? Well, the inequality is interesting. if $A,B$ are upper triangular matrices, it is obvious right. If $AB\ne BA$, $ \det \Big(A^2+B^2\Big)\ge \det(AB+BA) $ is wrong.",,"['linear-algebra', 'matrices', 'inequality', 'determinant']"
83,The rank of general inverse of $A$ times $A$?,The rank of general inverse of  times ?,A A,"Supposing $X$ is the general inverse of $A$, that $AXA = A$.  Then $XA$ is idempotent, that is $(XA)(XA) = XA$. Why is the rank of $XA$ equal to the rank of $A$ ? Thanks.","Supposing $X$ is the general inverse of $A$, that $AXA = A$.  Then $XA$ is idempotent, that is $(XA)(XA) = XA$. Why is the rank of $XA$ equal to the rank of $A$ ? Thanks.",,"['matrices', 'inverse']"
84,$T^*T=TT^*$ and $T^2=T$. Prove $T$ is self adjoint: $T=T^*$ [duplicate],and . Prove  is self adjoint:  [duplicate],T^*T=TT^* T^2=T T T=T^*,"This question already has answers here : A normal, idempotent linear operator must be self-adjoint (2 answers) Closed 10 years ago . $V$ is an inner product space of finite dimension over $\mathbb{R}$, and $T:V\to V$ a linear transformation which is normal, that is, $T^*T=TT^*$. In addition $T^2=T$.  Prove $T$ is self adjoint, that is, $T=T^*$. I tried to prove it algebraically by using the inner product but it didn't work for me. Then I tried to prove the statement that $T$ is self adjoint iff $\langle v,Tv\rangle$ is real for all $v$. I know that $T$ is diagonalizable, therefore there is a basis of $V$ consisting of eigenvectors. In addition every eigenvector of $T$ is an eigenvector of $T^*$. Can I prove that $\langle v,Tv\rangle$ is real only for the vectors in the basis and then it means it is for all $v$? Any help or further hints are very appreciated.","This question already has answers here : A normal, idempotent linear operator must be self-adjoint (2 answers) Closed 10 years ago . $V$ is an inner product space of finite dimension over $\mathbb{R}$, and $T:V\to V$ a linear transformation which is normal, that is, $T^*T=TT^*$. In addition $T^2=T$.  Prove $T$ is self adjoint, that is, $T=T^*$. I tried to prove it algebraically by using the inner product but it didn't work for me. Then I tried to prove the statement that $T$ is self adjoint iff $\langle v,Tv\rangle$ is real for all $v$. I know that $T$ is diagonalizable, therefore there is a basis of $V$ consisting of eigenvectors. In addition every eigenvector of $T$ is an eigenvector of $T^*$. Can I prove that $\langle v,Tv\rangle$ is real only for the vectors in the basis and then it means it is for all $v$? Any help or further hints are very appreciated.",,"['linear-algebra', 'matrices', 'operator-theory', 'inner-products']"
85,"If $A$ is a matrix with complex entries, then there exists a matrix $B$ such that $AB=0$ and $\operatorname{rank} A+ \operatorname{rank}B=n$.","If  is a matrix with complex entries, then there exists a matrix  such that  and .",A B AB=0 \operatorname{rank} A+ \operatorname{rank}B=n,"I have some difficulty writing the proof for this one.This problem appeared in a shortlist for a mathematical olympiad (for high-school): Let $A$ be a $n\times n$ matrix with complex entries. Prove that there exists a $n\times n$ matrix with complex entries $B$ such that $AB=0$ (the null matrix) and $\operatorname{rank} A + \operatorname{rank}B = n$ If $1 < \operatorname{rank} A < n$, prove that there exists a $n\times n$ matrix with complex entries $C$ such that $AC=0, CA \neq 0$ and $\operatorname{rank}A + \operatorname{rank} C = n$ Well, since this problem was proposed for the mathematical olympiad, I suppose that the solution involves basic linear algebra concepts i.e. without vector spaces, linear transformations etc. So far, I have only come up with the fact that if $A$ is invertible, then $B$ is the zero matrix, and using the Sylvester inequality and supposing $A$ singular implies: $\operatorname{rank} A + \operatorname{rank} B \leq n$ Also, I guess that stating that $\operatorname{rank} B$ exists from the above inequality doesn't imply that exists such $B$ so that $\operatorname{rank} B = n - \operatorname{rank} A$.","I have some difficulty writing the proof for this one.This problem appeared in a shortlist for a mathematical olympiad (for high-school): Let $A$ be a $n\times n$ matrix with complex entries. Prove that there exists a $n\times n$ matrix with complex entries $B$ such that $AB=0$ (the null matrix) and $\operatorname{rank} A + \operatorname{rank}B = n$ If $1 < \operatorname{rank} A < n$, prove that there exists a $n\times n$ matrix with complex entries $C$ such that $AC=0, CA \neq 0$ and $\operatorname{rank}A + \operatorname{rank} C = n$ Well, since this problem was proposed for the mathematical olympiad, I suppose that the solution involves basic linear algebra concepts i.e. without vector spaces, linear transformations etc. So far, I have only come up with the fact that if $A$ is invertible, then $B$ is the zero matrix, and using the Sylvester inequality and supposing $A$ singular implies: $\operatorname{rank} A + \operatorname{rank} B \leq n$ Also, I guess that stating that $\operatorname{rank} B$ exists from the above inequality doesn't imply that exists such $B$ so that $\operatorname{rank} B = n - \operatorname{rank} A$.",,"['linear-algebra', 'matrices', 'matrix-rank']"
86,Matrices - Understanding row echelon form and reduced echelon form,Matrices - Understanding row echelon form and reduced echelon form,,"I have the following two matrices: 1) $$\begin{bmatrix}1&0&0\\ 0&1&1\\ 0&0&0\\0&0&0 \end{bmatrix}$$ I believe this matrix is in the form of reduced row echelon form but I'm a bit unsure. 2) $$\begin{bmatrix}1&2&6&0&0\\0&0&1&2&-2\\0&0&0&0&1\end{bmatrix}$$ I believe this matrix is in row echelon form. I'm still not 100% sure what defines a row echelon form and reduced row echelon form. I've tried looking everywhere for a clear understanding of it, but I just could grasp it 100%.","I have the following two matrices: 1) $$\begin{bmatrix}1&0&0\\ 0&1&1\\ 0&0&0\\0&0&0 \end{bmatrix}$$ I believe this matrix is in the form of reduced row echelon form but I'm a bit unsure. 2) $$\begin{bmatrix}1&2&6&0&0\\0&0&1&2&-2\\0&0&0&0&1\end{bmatrix}$$ I believe this matrix is in row echelon form. I'm still not 100% sure what defines a row echelon form and reduced row echelon form. I've tried looking everywhere for a clear understanding of it, but I just could grasp it 100%.",,"['linear-algebra', 'matrices']"
87,How to solve matrix equation $AXH+AHX−BH=0$,How to solve matrix equation,AXH+AHX−BH=0,"How to solve matrix equation $AXH+AHX−BH=0$? All matrices are square, $A$, $B$ known constant matrices and invertible, $H$ can take any value, $X$ represent the solution to be found. I have seen about the Sylvester Equation like in this post Solving a matrix equation $AX=XB$ in a CAS , but I'm not sure how to apply it because of the presence of matrix H.","How to solve matrix equation $AXH+AHX−BH=0$? All matrices are square, $A$, $B$ known constant matrices and invertible, $H$ can take any value, $X$ represent the solution to be found. I have seen about the Sylvester Equation like in this post Solving a matrix equation $AX=XB$ in a CAS , but I'm not sure how to apply it because of the presence of matrix H.",,"['linear-algebra', 'matrices']"
88,"If $(1,1)$ is an eigenvector of $A=\begin{pmatrix}2 &5\\3&k\end{pmatrix}$,then one of the eigenvalues of $A$ is :-","If  is an eigenvector of ,then one of the eigenvalues of  is :-","(1,1) A=\begin{pmatrix}2 &5\\3&k\end{pmatrix} A","If $(1,1)$ is an eigenvector of $A=\begin{pmatrix}2 &5\\3&k\end{pmatrix}$,then one of the eigenvalues of $A$ is :- $0,-1,1,2$ Can I get some hints please.","If $(1,1)$ is an eigenvector of $A=\begin{pmatrix}2 &5\\3&k\end{pmatrix}$,then one of the eigenvalues of $A$ is :- $0,-1,1,2$ Can I get some hints please.",,"['linear-algebra', 'matrices']"
89,$A$ be a $2\times 2$ real matrix with trace $2$ and determinant $-3$,be a  real matrix with trace  and determinant,A 2\times 2 2 -3,"$A$ be a $2\times 2$ real matrix  with trace $2$ and determinant $-3$, consider the linear map $T:M_2(\mathbb{R})\to M_2(\mathbb{R}):=B\to AB$ Then which of the following are true? $T$ is diagonalizable $T$ is invertible $2$ is an eigen value of $T$ $T(B)=B$ for some $0\ne B\in M_2(\mathbb{R})$ if $ A=\begin{pmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{pmatrix}$ Then I have calculated that matrix of $T$ will be \begin{pmatrix}a_{11}&a_{12}&0&0\\a_{21}&a_{22}&0&0\\0&0&a_{11}&a_{12}\\0&0&a_{21}&a_{22}\end{pmatrix} so $T$ is invertible I can say as $\det T=(\det A)^2=9$, could any one help to find out others as true/false?","$A$ be a $2\times 2$ real matrix  with trace $2$ and determinant $-3$, consider the linear map $T:M_2(\mathbb{R})\to M_2(\mathbb{R}):=B\to AB$ Then which of the following are true? $T$ is diagonalizable $T$ is invertible $2$ is an eigen value of $T$ $T(B)=B$ for some $0\ne B\in M_2(\mathbb{R})$ if $ A=\begin{pmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{pmatrix}$ Then I have calculated that matrix of $T$ will be \begin{pmatrix}a_{11}&a_{12}&0&0\\a_{21}&a_{22}&0&0\\0&0&a_{11}&a_{12}\\0&0&a_{21}&a_{22}\end{pmatrix} so $T$ is invertible I can say as $\det T=(\det A)^2=9$, could any one help to find out others as true/false?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
90,transitivity of commutator,transitivity of commutator,,"I remember a quantum mechanics lecture where my professor said ""Two matrices $A, B$ which commute with a third matrix $C$, $[A,C]=[B,C]=0$, commute with each other: $[A,B]=0$."" I pointed out the identity $C=\mathbb{I}$ as an obvious counterexample, to which he restated the proposition to exclude my counterexample: ""Two matrices $A, B$ which commute with a third matrix $C\neq\mathbb{I}$, $[A,C]=[B,C]=0$, commute with each other: $[A,B]=0$."" He said further that he didn't know if this was a theorem, but he had never come across a counterexample. Thinking about it now, a straightforward exception is a block diagonal $C$ which is the identity in a given block, but not the other blocks so $C\neq\mathbb{I}$, and $A,B$ which are only nonzero in the given block. My question is whether, apart from these trivial cases, transitivity of the commutator is generally true or not? If not what is a counterexample? If it is generally true for matrices how far can it be extended to other systems?","I remember a quantum mechanics lecture where my professor said ""Two matrices $A, B$ which commute with a third matrix $C$, $[A,C]=[B,C]=0$, commute with each other: $[A,B]=0$."" I pointed out the identity $C=\mathbb{I}$ as an obvious counterexample, to which he restated the proposition to exclude my counterexample: ""Two matrices $A, B$ which commute with a third matrix $C\neq\mathbb{I}$, $[A,C]=[B,C]=0$, commute with each other: $[A,B]=0$."" He said further that he didn't know if this was a theorem, but he had never come across a counterexample. Thinking about it now, a straightforward exception is a block diagonal $C$ which is the identity in a given block, but not the other blocks so $C\neq\mathbb{I}$, and $A,B$ which are only nonzero in the given block. My question is whether, apart from these trivial cases, transitivity of the commutator is generally true or not? If not what is a counterexample? If it is generally true for matrices how far can it be extended to other systems?",,"['matrices', 'relations', 'noncommutative-algebra']"
91,"If $\operatorname{rk}(A) = 1$, when $B + A$ is invertible?","If , when  is invertible?",\operatorname{rk}(A) = 1 B + A,"I should know how do this problem, but I have troubles with it. Let $B$ be an invertible matrix and let $A$ be a matrix with $\operatorname{rk}(A) = 1$. Then $\exists \lambda$ such that $A^2 = \lambda A$ and the problem is for which values of $\lambda$ the matrix $B + A$ is invertible? When $B = I$ then $B + A$ is invertible iff $\lambda \neq -1$ and in the general case I suppose is $\lambda \neq -\det(B)$. I think it'll be better if I type my conclusion for $B = I$, if $I +A$ es invertible let $C$ such that $$(I + A)C = C(I + A) = I.$$ Then $AC = CA$ iff $C^{-1}AC = A$, but $C^{-1} = I + A$ and therefore we have $$A = (I + A)AC = (A +A^2)C=(A +\lambda A)C = (1 + \lambda)AC.$$ If $1 + \lambda = 0$ then $A = 0$ which has no rank 1, thereupon $\lambda \neq -1$ and if $\lambda \neq -1$ the inverse for $I+A$ is $I -\frac{1}{1+\lambda} A$.","I should know how do this problem, but I have troubles with it. Let $B$ be an invertible matrix and let $A$ be a matrix with $\operatorname{rk}(A) = 1$. Then $\exists \lambda$ such that $A^2 = \lambda A$ and the problem is for which values of $\lambda$ the matrix $B + A$ is invertible? When $B = I$ then $B + A$ is invertible iff $\lambda \neq -1$ and in the general case I suppose is $\lambda \neq -\det(B)$. I think it'll be better if I type my conclusion for $B = I$, if $I +A$ es invertible let $C$ such that $$(I + A)C = C(I + A) = I.$$ Then $AC = CA$ iff $C^{-1}AC = A$, but $C^{-1} = I + A$ and therefore we have $$A = (I + A)AC = (A +A^2)C=(A +\lambda A)C = (1 + \lambda)AC.$$ If $1 + \lambda = 0$ then $A = 0$ which has no rank 1, thereupon $\lambda \neq -1$ and if $\lambda \neq -1$ the inverse for $I+A$ is $I -\frac{1}{1+\lambda} A$.",,"['linear-algebra', 'matrices']"
92,Coefficients of the characteristic polynomial det($xI+A$) [duplicate],Coefficients of the characteristic polynomial det() [duplicate],xI+A,This question already has answers here : Coefficients of characteristic polynomial of a matrix (5 answers) Closed 4 years ago . Let $A$ be an $n\times n$ matrix. I want to know how to prove the fact that the coefficient of $x^k$ in $\det(xI+A)$ is the sum of the determinants of all $(n-k)\times (n-k)$ principal submatrices of $A$ .,This question already has answers here : Coefficients of characteristic polynomial of a matrix (5 answers) Closed 4 years ago . Let be an matrix. I want to know how to prove the fact that the coefficient of in is the sum of the determinants of all principal submatrices of .,A n\times n x^k \det(xI+A) (n-k)\times (n-k) A,"['matrices', 'determinant']"
93,Can a doubly stochastic matrix be asymmetric?,Can a doubly stochastic matrix be asymmetric?,,It's a fairly simple question but I cannot find the answer to it anywhere.,It's a fairly simple question but I cannot find the answer to it anywhere.,,"['matrices', 'markov-chains', 'stochastic-matrices']"
94,Solve for $X$ in a simple equation system.,Solve for  in a simple equation system.,X,"I cannot really understand how to read this question, so please help me out here. $$\left[ \begin{array}{ccc} 0 & -6 & 4\\ 1 & 2 & 7\end{array} \right] = 4X + 5 \left[ \begin{array}{ccc} 3 & 4 & 3\\ 8 & -4 & 8\end{array} \right]$$ First, how should i read this? Secondly how do I procede and solve for $X$, a full development would be very much appreciated! Thank you kindly for you help!","I cannot really understand how to read this question, so please help me out here. $$\left[ \begin{array}{ccc} 0 & -6 & 4\\ 1 & 2 & 7\end{array} \right] = 4X + 5 \left[ \begin{array}{ccc} 3 & 4 & 3\\ 8 & -4 & 8\end{array} \right]$$ First, how should i read this? Secondly how do I procede and solve for $X$, a full development would be very much appreciated! Thank you kindly for you help!",,"['linear-algebra', 'matrices']"
95,Factorize positive definite symmetric matrix,Factorize positive definite symmetric matrix,,"Let's start from the assumption of disposal of a positive definite symmetric matrix of size $\ (N,N) $. For some reason I have to factorize this matrix: I am already aware of the Cholesky factorization method, but I am wondering if there is any procedure allowing for matrix factorization based on eigen-vectors and eigen-values . I am not a mathematician, therefore my background knowledge on the topic is rather scarce and little. I need the factorization within a MATLAB code, and the Cholesky factorization is quite time-expensive, so that I'd more than appreciate moving towards a more practical (but effective) way to compute the lower triangular matrix. Any support will be more than welcome.","Let's start from the assumption of disposal of a positive definite symmetric matrix of size $\ (N,N) $. For some reason I have to factorize this matrix: I am already aware of the Cholesky factorization method, but I am wondering if there is any procedure allowing for matrix factorization based on eigen-vectors and eigen-values . I am not a mathematician, therefore my background knowledge on the topic is rather scarce and little. I need the factorization within a MATLAB code, and the Cholesky factorization is quite time-expensive, so that I'd more than appreciate moving towards a more practical (but effective) way to compute the lower triangular matrix. Any support will be more than welcome.",,"['matrices', 'matlab', 'matrix-decomposition', 'positive-definite', 'symmetric-matrices']"
96,"Three unknowns, the last row of the matrix contains all zeros.","Three unknowns, the last row of the matrix contains all zeros.",,"I'm in the process of solving a system of three linear equations in three unknowns, the last row of the matrix contains all zeros. How does this affect the solution of the system of equations?","I'm in the process of solving a system of three linear equations in three unknowns, the last row of the matrix contains all zeros. How does this affect the solution of the system of equations?",,"['linear-algebra', 'matrices']"
97,"For $n >1$,let $\displaystyle f(n)$ be the number of $n \times n$ real matrices $A$ such that $A^2+I=0.$","For ,let  be the number of  real matrices  such that",n >1 \displaystyle f(n) n \times n A A^2+I=0.,"I came across the following problem that says: For $n >1$,let $\displaystyle f(n)$ be the number of $n \times n$ real matrices $A$ such that $A^2+I=0.$ Then which of the following options is correct? $1.\displaystyle f(n) \equiv 0$ $2.\displaystyle f(n) \equiv \infty$ $3.\displaystyle f(n)=0$ iff $n$ is even $4.\displaystyle f(n)=0$ iff $n$ is odd. Can someone throw light on it?  Thanks in advance for your time.","I came across the following problem that says: For $n >1$,let $\displaystyle f(n)$ be the number of $n \times n$ real matrices $A$ such that $A^2+I=0.$ Then which of the following options is correct? $1.\displaystyle f(n) \equiv 0$ $2.\displaystyle f(n) \equiv \infty$ $3.\displaystyle f(n)=0$ iff $n$ is even $4.\displaystyle f(n)=0$ iff $n$ is odd. Can someone throw light on it?  Thanks in advance for your time.",,"['linear-algebra', 'matrices']"
98,"Definition of $\text{GL}(n,R)$",Definition of,"\text{GL}(n,R)","How do one usually define the general linear group over a ring $R$, denoted by $\text{GL}(n,R)$. I was told in a paper that $\text{GL}(n,R)$ is a group, and I presumed that $$\text{GL}(n,R)=\{A\in M_{n\times n}(R)|\text{det}(A)~\mbox{is a unit in}~R\}.$$However, I tried google it and found $$\text{GL}(n,R)=\{A\in M_{n\times n}(R)|\text{det}(A)\neq0\}.$$See for example, http://gmcninch.math.tufts.edu/Math215-Fall-2012/storage/HW4.pdf . As $R$ is not necessarily a unital ring, so it would happen that $\text{GL}(n,R)$ is not a group. Could any expert tell me which understanding is correct? And also, could you recommend any textbook which provides detailed discussion about this kind of group? I need to learn this more, thank you very much!","How do one usually define the general linear group over a ring $R$, denoted by $\text{GL}(n,R)$. I was told in a paper that $\text{GL}(n,R)$ is a group, and I presumed that $$\text{GL}(n,R)=\{A\in M_{n\times n}(R)|\text{det}(A)~\mbox{is a unit in}~R\}.$$However, I tried google it and found $$\text{GL}(n,R)=\{A\in M_{n\times n}(R)|\text{det}(A)\neq0\}.$$See for example, http://gmcninch.math.tufts.edu/Math215-Fall-2012/storage/HW4.pdf . As $R$ is not necessarily a unital ring, so it would happen that $\text{GL}(n,R)$ is not a group. Could any expert tell me which understanding is correct? And also, could you recommend any textbook which provides detailed discussion about this kind of group? I need to learn this more, thank you very much!",,"['linear-algebra', 'group-theory', 'matrices', 'ring-theory']"
99,Proof of equivalence theorem about left invertible matrices,Proof of equivalence theorem about left invertible matrices,,"I am taking a course in Matrix Theory and we have a theorem that states (among other things) that: The following conditions on the matrix $A$ of size $m \times n$ are equivalent: (1) A has left inverse (2) The system $Ax=b$ has at most one solution for any column vector $b$. ... The proof that (1) $\implies$ (2) goes like this: If $Ax=b$ and $V$ is a left inverse, then $VAx=Vb \implies x=Vb$, so we have at most one solution (if any). The thing is, left inverses are not unique right? Take  $A = \left(  \begin{matrix} 1 \\ 0 \end{matrix} \right)$ That has left inverses $V_1= \left( \begin{matrix} 1 & 0 \end{matrix} \right) $ and $ V_2 = \left(  \begin{matrix} 1 & 1 \end{matrix} \right)$ Does this mean that the proof is wrong or am I missing something?","I am taking a course in Matrix Theory and we have a theorem that states (among other things) that: The following conditions on the matrix $A$ of size $m \times n$ are equivalent: (1) A has left inverse (2) The system $Ax=b$ has at most one solution for any column vector $b$. ... The proof that (1) $\implies$ (2) goes like this: If $Ax=b$ and $V$ is a left inverse, then $VAx=Vb \implies x=Vb$, so we have at most one solution (if any). The thing is, left inverses are not unique right? Take  $A = \left(  \begin{matrix} 1 \\ 0 \end{matrix} \right)$ That has left inverses $V_1= \left( \begin{matrix} 1 & 0 \end{matrix} \right) $ and $ V_2 = \left(  \begin{matrix} 1 & 1 \end{matrix} \right)$ Does this mean that the proof is wrong or am I missing something?",,['matrices']
