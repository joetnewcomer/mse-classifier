,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Looking for a nonrecursive formula for the general derivatives of the quotient of functions,Looking for a nonrecursive formula for the general derivatives of the quotient of functions,,I want to prove that the $k$-th derivative $h^{(k)}(x)$ of the function $h(x)=\frac{1}{1+x^2}$ is zero at $x=0$ for all integer values $k>0$. My only idea was to go the stubborn way applying iteratively the elementary formula for the derivative of a quotient of functions. Alas I didnt find a general formula similar to the Leibniz formula for the derivatives of a product of functions neither in Wikipedia nor else in the web so far. This puzzles me. It wouldnt surprise me if a non-recursive closed expression (using iterated binomial coefficient sums) would be existing. I tackled the problem so far in using first the Leibniz formula on $h(x)=f(x)\frac{1}{g(x)}$ in the way $$h^{(k)}(x)=\sum_{r=0}^k f^{(k)}(x)\left( \frac{1}{g(x)}\right)^{(k-r)}$$ So I am in front of the problem calculating $\left( \frac{1}{g(x)}\right)^{(s)}$ which in the first step tackled by decreasing iteratively by $1$ $$\left( \frac{1}{g(x)}\right)^{(s)}=\left( -\frac{g^{(1)}(x)}{g^2(x)}\right)^{(s-1)}$$ If one applies to that quotient iteratively the Leibniz product rule the next calculation problem comes up for $$\left( \frac{1}{g^2(x)}\right)^{(t)}=\left( -\frac{(g^2)^{(1)}(x)}{g^4(x)}\right)^{(t-1)}$$ So I arrive at the problem calculating $$\left( -\frac {g^{{2^{m-1}}^{(1)}}(x)} {g^{2^m}(x)} \right)^{(1)}$$ Then I tried to go on by de-exponentiating and getting the square $$(g^{2^{m-1}})^{(1)}(x)=\left((g^{2^{m-2}}(x))^2\right)^{(1)}(x)=2g^{2^{m-2}}(x)g^{2^{m-2}})^{(1)}(x)$$ This leads me iteratively to the ( surprising/erroneous(?) ) result $$\left( -\frac {g^{{2^{m-1}}^{(1)}}(x)} {g^{2^m}(x)} \right)^{(1)}=2^{m-1}\frac{g^{\prime}}{g}$$ where the exponent at the $2$ is in doubt. Now I am overwhelmed at putting this all together and especially simplifying the nested iterative Leibniz sums with the binomials.,I want to prove that the $k$-th derivative $h^{(k)}(x)$ of the function $h(x)=\frac{1}{1+x^2}$ is zero at $x=0$ for all integer values $k>0$. My only idea was to go the stubborn way applying iteratively the elementary formula for the derivative of a quotient of functions. Alas I didnt find a general formula similar to the Leibniz formula for the derivatives of a product of functions neither in Wikipedia nor else in the web so far. This puzzles me. It wouldnt surprise me if a non-recursive closed expression (using iterated binomial coefficient sums) would be existing. I tackled the problem so far in using first the Leibniz formula on $h(x)=f(x)\frac{1}{g(x)}$ in the way $$h^{(k)}(x)=\sum_{r=0}^k f^{(k)}(x)\left( \frac{1}{g(x)}\right)^{(k-r)}$$ So I am in front of the problem calculating $\left( \frac{1}{g(x)}\right)^{(s)}$ which in the first step tackled by decreasing iteratively by $1$ $$\left( \frac{1}{g(x)}\right)^{(s)}=\left( -\frac{g^{(1)}(x)}{g^2(x)}\right)^{(s-1)}$$ If one applies to that quotient iteratively the Leibniz product rule the next calculation problem comes up for $$\left( \frac{1}{g^2(x)}\right)^{(t)}=\left( -\frac{(g^2)^{(1)}(x)}{g^4(x)}\right)^{(t-1)}$$ So I arrive at the problem calculating $$\left( -\frac {g^{{2^{m-1}}^{(1)}}(x)} {g^{2^m}(x)} \right)^{(1)}$$ Then I tried to go on by de-exponentiating and getting the square $$(g^{2^{m-1}})^{(1)}(x)=\left((g^{2^{m-2}}(x))^2\right)^{(1)}(x)=2g^{2^{m-2}}(x)g^{2^{m-2}})^{(1)}(x)$$ This leads me iteratively to the ( surprising/erroneous(?) ) result $$\left( -\frac {g^{{2^{m-1}}^{(1)}}(x)} {g^{2^m}(x)} \right)^{(1)}=2^{m-1}\frac{g^{\prime}}{g}$$ where the exponent at the $2$ is in doubt. Now I am overwhelmed at putting this all together and especially simplifying the nested iterative Leibniz sums with the binomials.,,"['calculus', 'derivatives', 'binomial-coefficients']"
1,Can $f''(x)$ exist if $f'(x)$ is undefined?,Can  exist if  is undefined?,f''(x) f'(x),"For example, the piecewise function $ f(x) =  \begin{cases}  \sqrt{1 - (x + 1)^2} &-2 \leq x \leq 0 \\  -\sqrt{1 - (x - 1)^2} &0 \leq x \leq 2 \end{cases} $ will, at $f(0)$ , give $f'(0) = $ undefined (vertical tangent).  Once deriving I can prove this algebraically, there is a zero in the denominator.  It looked like an inflection point, so I wondered if $f''(x)$ would equal zero or undefined.  After taking the (painstakingly ugly) second derivative, I ran into another zero in the denominator error, so $f''(0)$ is also undefined.  Is this a general rule that if $f'(x)$ is undefined $f''(x)$ will also be undefined? Thanks :)","For example, the piecewise function will, at , give undefined (vertical tangent).  Once deriving I can prove this algebraically, there is a zero in the denominator.  It looked like an inflection point, so I wondered if would equal zero or undefined.  After taking the (painstakingly ugly) second derivative, I ran into another zero in the denominator error, so is also undefined.  Is this a general rule that if is undefined will also be undefined? Thanks :)","
f(x) = 
\begin{cases} 
\sqrt{1 - (x + 1)^2} &-2 \leq x \leq 0 \\ 
-\sqrt{1 - (x - 1)^2} &0 \leq x \leq 2
\end{cases}
 f(0) f'(0) =  f''(x) f''(0) f'(x) f''(x)",['derivatives']
2,How to read 4th order mixed Leibniz derivative,How to read 4th order mixed Leibniz derivative,,"How exactly is the order of mixed partials read in Leibniz notation? In Lagrange notation, we just read from left to right. $$f_{xyzz} = \left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial y}\left(\frac {\partial f} {\partial x}\right)\right)\right)\right)$$ But what would the compacted form of Leibniz be? $$? = \left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial y}\left(\frac {\partial f} {\partial x}\right)\right)\right)\right)$$ Here are some options that I have seen, but don't know which one is right, it seems each person I ask/website I visit has its own convention for these rather odd/rarely used cases $$\frac {\partial^4f} {\partial z^2 \partial y \partial x} = \left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial y}\left(\frac {\partial f} {\partial x}\right)\right)\right)\right)$$ $$\frac {\partial^4f} {\partial x \partial y \partial z^2} = \left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial y}\left(\frac {\partial f} {\partial x}\right)\right)\right)\right)$$ I personally feel that the latter is more intuitive, since it would mean both Lagrange's subscripts and Leibniz's denominator have the same order $$f_{xyzz} = \frac {\partial^4f} {\partial x \partial y \partial z^2} =\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial y}\left(\frac {\partial f} {\partial x}\right)\right)\right)\right)$$","How exactly is the order of mixed partials read in Leibniz notation? In Lagrange notation, we just read from left to right. But what would the compacted form of Leibniz be? Here are some options that I have seen, but don't know which one is right, it seems each person I ask/website I visit has its own convention for these rather odd/rarely used cases I personally feel that the latter is more intuitive, since it would mean both Lagrange's subscripts and Leibniz's denominator have the same order",f_{xyzz} = \left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial y}\left(\frac {\partial f} {\partial x}\right)\right)\right)\right) ? = \left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial y}\left(\frac {\partial f} {\partial x}\right)\right)\right)\right) \frac {\partial^4f} {\partial z^2 \partial y \partial x} = \left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial y}\left(\frac {\partial f} {\partial x}\right)\right)\right)\right) \frac {\partial^4f} {\partial x \partial y \partial z^2} = \left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial y}\left(\frac {\partial f} {\partial x}\right)\right)\right)\right) f_{xyzz} = \frac {\partial^4f} {\partial x \partial y \partial z^2} =\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial z}\left(\frac {\partial} {\partial y}\left(\frac {\partial f} {\partial x}\right)\right)\right)\right),"['derivatives', 'notation', 'partial-derivative']"
3,Prove that $|f'(x)|\leq\sqrt{2MM''}$ [duplicate],Prove that  [duplicate],|f'(x)|\leq\sqrt{2MM''},"This question already has answers here : Prove that $\left| f'(x)\right| \leq \sqrt{2AC}$ using integration (2 answers) Closed 8 years ago . Let $f:\mathbb{R}\to\mathbb{R}$ be twice differentiable with $$|f(x)|\leq M, |f''(x)|\leq M'',\forall x\in\mathbb{R}$$ Prove that $|f'(x)|\leq\sqrt{2MM''},\forall x\in\mathbb{R}$ I am thinking about using Taylor's theorem: For any $x\in\mathbb{R}$ and $a>0$, by Taylor's theorem $\exists \xi\in(x,x+a)$ s.t. $$f(x+a)=f(x)+f'(x)a+\frac{f''(\xi)}{2}a^2$$ Thus $$|f'(x)|\leq\frac{|f(x)|+|f(x+a)|}{a}+\frac{|f''(\xi)|}{2}a$$ However with this approach the best bound we can get is $$|f'(x)|\leq 2\sqrt{MM''}$$ Thus I feel that there is probably a completely different trick.","This question already has answers here : Prove that $\left| f'(x)\right| \leq \sqrt{2AC}$ using integration (2 answers) Closed 8 years ago . Let $f:\mathbb{R}\to\mathbb{R}$ be twice differentiable with $$|f(x)|\leq M, |f''(x)|\leq M'',\forall x\in\mathbb{R}$$ Prove that $|f'(x)|\leq\sqrt{2MM''},\forall x\in\mathbb{R}$ I am thinking about using Taylor's theorem: For any $x\in\mathbb{R}$ and $a>0$, by Taylor's theorem $\exists \xi\in(x,x+a)$ s.t. $$f(x+a)=f(x)+f'(x)a+\frac{f''(\xi)}{2}a^2$$ Thus $$|f'(x)|\leq\frac{|f(x)|+|f(x+a)|}{a}+\frac{|f''(\xi)|}{2}a$$ However with this approach the best bound we can get is $$|f'(x)|\leq 2\sqrt{MM''}$$ Thus I feel that there is probably a completely different trick.",,"['real-analysis', 'derivatives', 'inequality']"
4,Differential Equation change of variable,Differential Equation change of variable,,"I'm investigating how you get from this: $$ z\frac{d^2y}{dz^2}+(1-a)\frac{dy}{dz}+a^2z^{2a-1}y=0 $$ to this: $$ \frac{d^2y}{dx^2}+y=0 $$ with a change of variable: $$ z=x^{1/a}, (x\ge0) $$ I'm not quite 'getting' the substitution in the derivative... In a normal situation where we have $ \frac{dy}{dx} $ and a function $ y(x) $, we basically perform $ \frac{d}{dx}y(x) $ to see how $ y $ changes with respect to $ x $. Now, with the variable substitution are we now saying that $ \frac{dy}{dz} $ is changed to $ \frac{d}{dz}y(z) = \frac{d}{dx}y(x^{1/a}) $. This leaves me with a derivative that can be solved using the Chain Rule? So, $$ \frac{d}{dx}y(x^{1/a})  = \frac{dy}{dx}\frac{x^{\frac{1}{a}-1}}{a}$$ I would then just proceed to take the second derivative and substitute $ \frac{dy}{dx} $ , $ \frac{d^2y}{dx^2} $ and $ y $ into the equation original equation above. Is my understanding correct on this please?  Thankyou.","I'm investigating how you get from this: $$ z\frac{d^2y}{dz^2}+(1-a)\frac{dy}{dz}+a^2z^{2a-1}y=0 $$ to this: $$ \frac{d^2y}{dx^2}+y=0 $$ with a change of variable: $$ z=x^{1/a}, (x\ge0) $$ I'm not quite 'getting' the substitution in the derivative... In a normal situation where we have $ \frac{dy}{dx} $ and a function $ y(x) $, we basically perform $ \frac{d}{dx}y(x) $ to see how $ y $ changes with respect to $ x $. Now, with the variable substitution are we now saying that $ \frac{dy}{dz} $ is changed to $ \frac{d}{dz}y(z) = \frac{d}{dx}y(x^{1/a}) $. This leaves me with a derivative that can be solved using the Chain Rule? So, $$ \frac{d}{dx}y(x^{1/a})  = \frac{dy}{dx}\frac{x^{\frac{1}{a}-1}}{a}$$ I would then just proceed to take the second derivative and substitute $ \frac{dy}{dx} $ , $ \frac{d^2y}{dx^2} $ and $ y $ into the equation original equation above. Is my understanding correct on this please?  Thankyou.",,"['derivatives', 'chain-rule']"
5,Non-standard version of Frechet derivative,Non-standard version of Frechet derivative,,"Non-standard analysis offers very convenient tools to prove facts about continuity or differentiability. I am looking for such tool in infinite-dimensional calculus. To be more precise, let $X$ and $Y$ be Banach spaces and let ${}^*\!X, {}^*Y$ be their non-standard extensions. Suppose we have function $f\colon X\to Y$ and $x_0\in X$. What is the non-standard statement about ${}^*\!f$ corresponding to saying that $f$ is Frechet-differentiable at $x_0$?","Non-standard analysis offers very convenient tools to prove facts about continuity or differentiability. I am looking for such tool in infinite-dimensional calculus. To be more precise, let $X$ and $Y$ be Banach spaces and let ${}^*\!X, {}^*Y$ be their non-standard extensions. Suppose we have function $f\colon X\to Y$ and $x_0\in X$. What is the non-standard statement about ${}^*\!f$ corresponding to saying that $f$ is Frechet-differentiable at $x_0$?",,"['real-analysis', 'derivatives', 'nonstandard-analysis', 'frechet-derivative']"
6,Derivative of exponential integral,Derivative of exponential integral,,"Take the derivative of $$y_t = e^{-\int_{0}^{t}r_s ds}x_t$$  by chain rule,  $$dy_t = d(e^{-\int_{0}^{t}r_s ds})x_t + e^{-\int_{0}^{t}r_s ds}dx_t$$ but what should the following equation be? $$d(e^{-\int_{0}^{t}r_s ds})$$ do I take the derivative wrt $x_t$ or $r_t$ ?","Take the derivative of $$y_t = e^{-\int_{0}^{t}r_s ds}x_t$$  by chain rule,  $$dy_t = d(e^{-\int_{0}^{t}r_s ds})x_t + e^{-\int_{0}^{t}r_s ds}dx_t$$ but what should the following equation be? $$d(e^{-\int_{0}^{t}r_s ds})$$ do I take the derivative wrt $x_t$ or $r_t$ ?",,"['calculus', 'derivatives']"
7,Show $f(t)$ is not injective in any neighbourhood of $0$,Show  is not injective in any neighbourhood of,f(t) 0,"This comes from Baby Rudin chapter 9's exercises: define  $$f(t)=t+2t^2\sin\frac1t,\,t\ne 0;\quad f(0)=0$$ show $f$  fails to be injective in any neighbourhood of $0$. My thought was to find points where $f'(t)=0$ but $f''(t)\ne 0$. Letting $t_n=1/(2n\pi)$ and $t'_n=1/(2n\pi+\pi)$ it's easy to see that $f'(t_n)\to -1$ while $f'(t'_n)\to 3$. So by the continuity of $f'(t),t\ne 0$ there exists   some $t_0$  between $t_n$ and $t'_n$ for each $n$ such that $f'(t_0)=0$. But it's much more troubling to show the second order derivative doesn't vanish. Is there any slicker way?","This comes from Baby Rudin chapter 9's exercises: define  $$f(t)=t+2t^2\sin\frac1t,\,t\ne 0;\quad f(0)=0$$ show $f$  fails to be injective in any neighbourhood of $0$. My thought was to find points where $f'(t)=0$ but $f''(t)\ne 0$. Letting $t_n=1/(2n\pi)$ and $t'_n=1/(2n\pi+\pi)$ it's easy to see that $f'(t_n)\to -1$ while $f'(t'_n)\to 3$. So by the continuity of $f'(t),t\ne 0$ there exists   some $t_0$  between $t_n$ and $t'_n$ for each $n$ such that $f'(t_0)=0$. But it's much more troubling to show the second order derivative doesn't vanish. Is there any slicker way?",,"['calculus', 'real-analysis', 'derivatives']"
8,Where is the definition of the derivative formula derived from?,Where is the definition of the derivative formula derived from?,,"I know what the definition of the derivative is , however, I am curious where this comes from mathematically.","I know what the definition of the derivative is , however, I am curious where this comes from mathematically.",,"['calculus', 'derivatives', 'math-history']"
9,"Proving that an inequality is true, from assuming that second derivatives exist, and first derivatives are zero on the boundary,","Proving that an inequality is true, from assuming that second derivatives exist, and first derivatives are zero on the boundary,",,"EDIT 2: I just posted my revised proof, where I used two Taylor expansions, and subtracting both equations to get something that's pretty close to what I want.  What do you think?  Please see below.  Thanks, EDIT:  I think that, with the help of Joey Zou and Claudeh5's attempts at a solution (please see below), I am pretty close to an answer.  At present there are some things of concern: a) Joey Zou's more technical proof seems to rely on $f''$ being continuous or at least Riemann-integrable.  Unfortunately, I don't think we can assume that. b) Claudeh5's cute one-line proof almost does the job.  It uses Taylor expansion, the Lagrange Remainder, and centering the expansion about  $x=a$, and evaluating the series at $x=b$.  However, the estimate is not quite good enough. Any hints or comments are welcome.  Thanks, The problem statement is: Assume that $f(x)$ has second derivatives on [a,b], and $f′(a)=f′(b)=0$. Prove that there exists a point $c∈[a,b]$ such that $f′′(c)≥\frac{4}{(a−b)^2}|f(b)−f(a)|$. Using Claudeh5's approach, here is my revised proof, which is perhaps closer to the desired upper bound: By Taylor expansion, and the Lagrange remainder, we have that $$f(b) = f(a) + \frac{f''(\psi_1)}{2!} (b-a)^2$$ Now again, but centering the expansion about $x=b$ gives us $$f(a) = f(b) + \frac{f''(\psi_2)}{2!} (a-b)^2$$ Note that the first derivatives vanish at $x=a$ and $x=b$, by assumption. Now subtracting the two Taylor series gives $$2[f(b)-f(a)]= \frac {[f''(\psi_1) - f''(\psi_2)]}{2!}(a-b)^2$$ $$\implies \frac  {4[f(b)-f(a)]}{(a-b)^2}= [f''(\psi_1) - f''(\psi_2)]$$ $$\implies \frac  {4[f(b)-f(a)]}{(a-b)^2} \le max \{f''(\psi_1), f''(\psi_2)\}$$ $$=:f''(c)$$ I am not confident about the last two lines of my proof. Am I on the right track?  I feel a bit closer now to achieving the upper bound ... Thanks,","EDIT 2: I just posted my revised proof, where I used two Taylor expansions, and subtracting both equations to get something that's pretty close to what I want.  What do you think?  Please see below.  Thanks, EDIT:  I think that, with the help of Joey Zou and Claudeh5's attempts at a solution (please see below), I am pretty close to an answer.  At present there are some things of concern: a) Joey Zou's more technical proof seems to rely on $f''$ being continuous or at least Riemann-integrable.  Unfortunately, I don't think we can assume that. b) Claudeh5's cute one-line proof almost does the job.  It uses Taylor expansion, the Lagrange Remainder, and centering the expansion about  $x=a$, and evaluating the series at $x=b$.  However, the estimate is not quite good enough. Any hints or comments are welcome.  Thanks, The problem statement is: Assume that $f(x)$ has second derivatives on [a,b], and $f′(a)=f′(b)=0$. Prove that there exists a point $c∈[a,b]$ such that $f′′(c)≥\frac{4}{(a−b)^2}|f(b)−f(a)|$. Using Claudeh5's approach, here is my revised proof, which is perhaps closer to the desired upper bound: By Taylor expansion, and the Lagrange remainder, we have that $$f(b) = f(a) + \frac{f''(\psi_1)}{2!} (b-a)^2$$ Now again, but centering the expansion about $x=b$ gives us $$f(a) = f(b) + \frac{f''(\psi_2)}{2!} (a-b)^2$$ Note that the first derivatives vanish at $x=a$ and $x=b$, by assumption. Now subtracting the two Taylor series gives $$2[f(b)-f(a)]= \frac {[f''(\psi_1) - f''(\psi_2)]}{2!}(a-b)^2$$ $$\implies \frac  {4[f(b)-f(a)]}{(a-b)^2}= [f''(\psi_1) - f''(\psi_2)]$$ $$\implies \frac  {4[f(b)-f(a)]}{(a-b)^2} \le max \{f''(\psi_1), f''(\psi_2)\}$$ $$=:f''(c)$$ I am not confident about the last two lines of my proof. Am I on the right track?  I feel a bit closer now to achieving the upper bound ... Thanks,",,"['calculus', 'real-analysis', 'derivatives']"
10,Globally Lipschitz if and only if derivative is bounded?,Globally Lipschitz if and only if derivative is bounded?,,"If I have a function $f:\mathbb R\to\mathbb R^n$ I can say that it is globally Lipschitz in $t$ if its Jacobian is bounded in $t$. However, does it work the other way around? If I find that the function Jacobian is not bounded, does this mean that the function is not globally Lipschitz?","If I have a function $f:\mathbb R\to\mathbb R^n$ I can say that it is globally Lipschitz in $t$ if its Jacobian is bounded in $t$. However, does it work the other way around? If I find that the function Jacobian is not bounded, does this mean that the function is not globally Lipschitz?",,"['real-analysis', 'derivatives', 'lipschitz-functions']"
11,"Vector Calculus Notation for ""Gradient of a Vector""","Vector Calculus Notation for ""Gradient of a Vector""",,"Given (differentiable) functions $\,n_{1,2}:\mathbb{R}\to\mathbb{R}\,$ we write vector $\renewcommand{\arraystretch}{2}$ \begin{align}   \vec{\boldsymbol{n}} = \begin{bmatrix} n_{1} \\ n_{2} \end{bmatrix} \end{align} and define ""gradient of a vector"" operation $\,:\mathbb{R}^2\to\mathbb{R}^{2\times2}\,$ as \begin{align}\label{1}\tag{$\boldsymbol{\ast}$}   \begin{bmatrix}     \,^{\partial}/_{\partial x}\, \\ \,^{\partial}/_{\partial y}\,   \end{bmatrix}   \begin{bmatrix}\,n_{1} & n_{2} \end{bmatrix} =   \begin{bmatrix}\,      \big(n_{1}\big)_{x} & \big(n_{2}\big)_{x} \\\,     \big(n_{1}\big)_{y} & \big(n_{2}\big)_{y}   \end{bmatrix}. \end{align} Is it possible to write the expression above in terms of nabla operator and vector $\,\vec{\boldsymbol{n}}$? For example, I was thinking about something like \begin{align}   \begin{bmatrix}\,      \big(n_{1}\big)_{x} & \big(n_{2}\big)_{x} \\\,     \big(n_{1}\big)_{y} & \big(n_{2}\big)_{y}   \end{bmatrix}   =   \nabla \circ \vec{\boldsymbol{n}}^{T} \end{align} where $\,\circ\,$ stands for some ( vector calculus? ) operation. What would be an appropriate symbol to write instead of $\,\circ\,$?  Is there standard notation for such operation? I know that the matrix on the right hand side of equation $\eqref{1}$ can be viewed as Jacobian matrix of vector-valued function $\,\vec{\boldsymbol{n}}\,(x,y):\mathbb{R}^{2}\to\mathbb{R}^{2},\,$ but I would like to be able to express in using just the vector $\,\vec{\boldsymbol{n}}\,$ and differential operations of vector calculus.","Given (differentiable) functions $\,n_{1,2}:\mathbb{R}\to\mathbb{R}\,$ we write vector $\renewcommand{\arraystretch}{2}$ \begin{align}   \vec{\boldsymbol{n}} = \begin{bmatrix} n_{1} \\ n_{2} \end{bmatrix} \end{align} and define ""gradient of a vector"" operation $\,:\mathbb{R}^2\to\mathbb{R}^{2\times2}\,$ as \begin{align}\label{1}\tag{$\boldsymbol{\ast}$}   \begin{bmatrix}     \,^{\partial}/_{\partial x}\, \\ \,^{\partial}/_{\partial y}\,   \end{bmatrix}   \begin{bmatrix}\,n_{1} & n_{2} \end{bmatrix} =   \begin{bmatrix}\,      \big(n_{1}\big)_{x} & \big(n_{2}\big)_{x} \\\,     \big(n_{1}\big)_{y} & \big(n_{2}\big)_{y}   \end{bmatrix}. \end{align} Is it possible to write the expression above in terms of nabla operator and vector $\,\vec{\boldsymbol{n}}$? For example, I was thinking about something like \begin{align}   \begin{bmatrix}\,      \big(n_{1}\big)_{x} & \big(n_{2}\big)_{x} \\\,     \big(n_{1}\big)_{y} & \big(n_{2}\big)_{y}   \end{bmatrix}   =   \nabla \circ \vec{\boldsymbol{n}}^{T} \end{align} where $\,\circ\,$ stands for some ( vector calculus? ) operation. What would be an appropriate symbol to write instead of $\,\circ\,$?  Is there standard notation for such operation? I know that the matrix on the right hand side of equation $\eqref{1}$ can be viewed as Jacobian matrix of vector-valued function $\,\vec{\boldsymbol{n}}\,(x,y):\mathbb{R}^{2}\to\mathbb{R}^{2},\,$ but I would like to be able to express in using just the vector $\,\vec{\boldsymbol{n}}\,$ and differential operations of vector calculus.",,"['derivatives', 'notation', 'vector-analysis']"
12,Who invented the notation $Df$ for the derivative?,Who invented the notation  for the derivative?,Df,"We are often taught that $f'$ came from Newton and $\frac{df}{dx}$ came from Leibniz, but who introduced $Df$? Are there other notations for this simple idea by famous mathematicians?","We are often taught that $f'$ came from Newton and $\frac{df}{dx}$ came from Leibniz, but who introduced $Df$? Are there other notations for this simple idea by famous mathematicians?",,"['derivatives', 'math-history']"
13,Derivative of power series,Derivative of power series,,"Let $$f(x) = \sum_{k=0}^\infty a_kx^k$$ be a power series mapping reals to reals, with radius of convergence $R$. Suppose $f'(x_0)$ exists in $[-R,R]$ (take the one-sided limit if $x_0 = R$ or $x_0 = -R$). Then is it always true that $$f'(x_0) = \sum_{k=0}^\infty ka_kx^{k-1}?$$ This clearly holds if $x_0 \in (-R,R)$. But what about $x_0 = R$? EDIT: Even though I have accepted the answer, I'm wondering if there still exists a counterexample if $a_k$ is restricted to the nonnegative reals for all $k$. It would be very helpful if someone can answer that.","Let $$f(x) = \sum_{k=0}^\infty a_kx^k$$ be a power series mapping reals to reals, with radius of convergence $R$. Suppose $f'(x_0)$ exists in $[-R,R]$ (take the one-sided limit if $x_0 = R$ or $x_0 = -R$). Then is it always true that $$f'(x_0) = \sum_{k=0}^\infty ka_kx^{k-1}?$$ This clearly holds if $x_0 \in (-R,R)$. But what about $x_0 = R$? EDIT: Even though I have accepted the answer, I'm wondering if there still exists a counterexample if $a_k$ is restricted to the nonnegative reals for all $k$. It would be very helpful if someone can answer that.",,"['calculus', 'real-analysis', 'derivatives', 'convergence-divergence', 'power-series']"
14,A Question about derivative.,A Question about derivative.,,"Suppose that $f:\mathbb{R} \to \mathbb{R}$ is differentiable, $f(0)= 0$, and $f’(x) > f(x)$ for all $x \in \mathbb{R}$. Prove that $f(x) > 0$ for $x >0$.  Clear, $f'(0)> 0$ and by defintion, $f'(0)> f(0)=0$. Any hints are welcome.","Suppose that $f:\mathbb{R} \to \mathbb{R}$ is differentiable, $f(0)= 0$, and $f’(x) > f(x)$ for all $x \in \mathbb{R}$. Prove that $f(x) > 0$ for $x >0$.  Clear, $f'(0)> 0$ and by defintion, $f'(0)> f(0)=0$. Any hints are welcome.",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
15,Differentiating geometric series,Differentiating geometric series,,On Wikipedia it is stated that by differentiating the following formula holds: $$ \sum_n n q^n = {1\over (1-q)^2}$$ Does this not require a proof? It seems to me because the series is infinite it is not clear that differentiation commutes with taking the limit. How to prove this?,On Wikipedia it is stated that by differentiating the following formula holds: $$ \sum_n n q^n = {1\over (1-q)^2}$$ Does this not require a proof? It seems to me because the series is infinite it is not clear that differentiation commutes with taking the limit. How to prove this?,,"['real-analysis', 'limits', 'derivatives', 'power-series']"
16,Derivates of function and limits,Derivates of function and limits,,I wonder if it's true $$\lim_{n \rightarrow \infty} \frac{d}{dx}f_n(x)=\frac{d}{dx} \lim_{n \rightarrow \infty} f_n(x)$$ When $\displaystyle f_n(x)$ is a sequence of function.,I wonder if it's true $$\lim_{n \rightarrow \infty} \frac{d}{dx}f_n(x)=\frac{d}{dx} \lim_{n \rightarrow \infty} f_n(x)$$ When $\displaystyle f_n(x)$ is a sequence of function.,,"['real-analysis', 'complex-analysis', 'analysis', 'limits', 'derivatives']"
17,Is continuity of first partials required for analyticity?,Is continuity of first partials required for analyticity?,,"Let's cast the complex function $f(z) = u(z) + iv(z), z = x+iy$, as the multivariable function $F(x,y) = U(x,y) + iV(x,y) ; x,y \in R$. Thus, $$dF = F_x\,dx + F_y\,dy = U_x\,dx + iV_x\,dx + U_y dy + iV_y\,dy$$ Making use of $$dz = dx + i\,dy,$$ $$dz∗ =dx−i\,dy,$$ We plug in $$dx = \frac 1 2 (dz + dz∗),$$ $$dy= \frac 1 {2i}(dz−dz∗)$$ into the equation of $dF$, thus we get: $$dF = \frac 1 2 (U_x + V_y + i(V_x - U_y))\, dz + \frac 1 2 (U_x - V_y + i(V_x + U_y))\, dz*$$ Now it can be seen that if the function $f(z)$ (and thus $F(x,y)$) satisfies the Cauchy Riemann Equations, then the factor in front of $dz*$ will be zero, and we will have the expression for the derivative of the function $F(x,y)$ (and hence $f(z)$). Now there are two assumptions for the derivative of a complex function to exist, namely that (1) it satisfies CR equations, and (2) it has continuous first partial derivatives. But we just found the derivative of $f(z)$, without assuming the condition of continuous first partials!! How is this?","Let's cast the complex function $f(z) = u(z) + iv(z), z = x+iy$, as the multivariable function $F(x,y) = U(x,y) + iV(x,y) ; x,y \in R$. Thus, $$dF = F_x\,dx + F_y\,dy = U_x\,dx + iV_x\,dx + U_y dy + iV_y\,dy$$ Making use of $$dz = dx + i\,dy,$$ $$dz∗ =dx−i\,dy,$$ We plug in $$dx = \frac 1 2 (dz + dz∗),$$ $$dy= \frac 1 {2i}(dz−dz∗)$$ into the equation of $dF$, thus we get: $$dF = \frac 1 2 (U_x + V_y + i(V_x - U_y))\, dz + \frac 1 2 (U_x - V_y + i(V_x + U_y))\, dz*$$ Now it can be seen that if the function $f(z)$ (and thus $F(x,y)$) satisfies the Cauchy Riemann Equations, then the factor in front of $dz*$ will be zero, and we will have the expression for the derivative of the function $F(x,y)$ (and hence $f(z)$). Now there are two assumptions for the derivative of a complex function to exist, namely that (1) it satisfies CR equations, and (2) it has continuous first partial derivatives. But we just found the derivative of $f(z)$, without assuming the condition of continuous first partials!! How is this?",,"['complex-analysis', 'derivatives', 'partial-derivative', 'analyticity']"
18,"$f(x) \ge f(x + \sin x)$, nonconstant functions, infinite number of solutions to $f'(x) = 0$.",", nonconstant functions, infinite number of solutions to .",f(x) \ge f(x + \sin x) f'(x) = 0,"Let $\mathcal{F}$ be the set of all the differentiable functions $f: \mathbb{R} \to \mathbb{R}$, which have the property $f(x) \ge f(x + \sin x)$, for all $x \in \mathbb{R}$. Prove that $\mathcal{F}$ contains also nonconstant functions. Prove that if $f \in \mathcal{F}$ then the set of the solutions of the equation $f'(x) = 0$ is infinite.","Let $\mathcal{F}$ be the set of all the differentiable functions $f: \mathbb{R} \to \mathbb{R}$, which have the property $f(x) \ge f(x + \sin x)$, for all $x \in \mathbb{R}$. Prove that $\mathcal{F}$ contains also nonconstant functions. Prove that if $f \in \mathcal{F}$ then the set of the solutions of the equation $f'(x) = 0$ is infinite.",,"['calculus', 'real-analysis']"
19,Notion of differentiability for complex functions,Notion of differentiability for complex functions,,"I understand that the Cauchy Riemann condition for complex function starts by defining differentiable complex functions as those whose derivative exists and is the same regardless of whether we go along the real axis or the imaginary. Let $z=x+iy$. The function $f(z)$ is differentiable if $\frac{f(x+\Delta x+i y) - f(x+i y)}{\Delta x} = \frac{f(x+i(y+\Delta y)) - f(x+i y)}{i\Delta y}$ As a definition and what follows, it's fine but can anyone explain (to a physicist) why derivatives should be direction independent? I imagine there are plenty of functions that have direction dependent derivatives but I'd like to understand the motivation behind this way of defining differentiability.","I understand that the Cauchy Riemann condition for complex function starts by defining differentiable complex functions as those whose derivative exists and is the same regardless of whether we go along the real axis or the imaginary. Let $z=x+iy$. The function $f(z)$ is differentiable if $\frac{f(x+\Delta x+i y) - f(x+i y)}{\Delta x} = \frac{f(x+i(y+\Delta y)) - f(x+i y)}{i\Delta y}$ As a definition and what follows, it's fine but can anyone explain (to a physicist) why derivatives should be direction independent? I imagine there are plenty of functions that have direction dependent derivatives but I'd like to understand the motivation behind this way of defining differentiability.",,"['complex-analysis', 'derivatives']"
20,"$\frac{d^{100}}{dx^{100}}\left[\frac{f(x)}{g(x)}\right]=\frac{p(x)}{q(x)}$,then find the degrees of the polynomials $p(x)$ and $q(x)$",",then find the degrees of the polynomials  and",\frac{d^{100}}{dx^{100}}\left[\frac{f(x)}{g(x)}\right]=\frac{p(x)}{q(x)} p(x) q(x),"Let $g(x)=x^3-x$,and $f(x)$ be a polynomial of degree $\leq100$.If $f(x)$ and $g(x)$ have no common factor and $\frac{d^{100}}{dx^{100}}\left[\frac{f(x)}{g(x)}\right]=\frac{p(x)}{q(x)}$,then find the degrees of the polynomials $p(x)$ and $q(x)$. I tried this problem.SInce the degree of $f(x)$ is atmost 100,and degree of $g(x)$ is 3,so the atmost degree of $\frac{f(x)}{g(x)}$ is 97 and after differentiation we get degree of $\frac{p(x)}{q(x)}$ as $-3$ but i cannot exactly pinpoint the degree of $p(x)$ and $q(x)$. Answer in my book says degree of $p(x)$ is $6\times 2^{99}-101$ and the degree of $q(x)$ is $6\times 2^{99}$.How can i get this answer.Please help me.","Let $g(x)=x^3-x$,and $f(x)$ be a polynomial of degree $\leq100$.If $f(x)$ and $g(x)$ have no common factor and $\frac{d^{100}}{dx^{100}}\left[\frac{f(x)}{g(x)}\right]=\frac{p(x)}{q(x)}$,then find the degrees of the polynomials $p(x)$ and $q(x)$. I tried this problem.SInce the degree of $f(x)$ is atmost 100,and degree of $g(x)$ is 3,so the atmost degree of $\frac{f(x)}{g(x)}$ is 97 and after differentiation we get degree of $\frac{p(x)}{q(x)}$ as $-3$ but i cannot exactly pinpoint the degree of $p(x)$ and $q(x)$. Answer in my book says degree of $p(x)$ is $6\times 2^{99}-101$ and the degree of $q(x)$ is $6\times 2^{99}$.How can i get this answer.Please help me.",,"['derivatives', 'polynomials']"
21,L'Hopital's rule and limiting variables,L'Hopital's rule and limiting variables,,"I'm working some problems from a calculus text and came across this question: If $f(x)$ is a function that's differentiable everywhere, what is the value of the limit $$\lim\limits_{h \to 0}\frac{f(x+3h^2)-f(x-h^2)}{2h^2}$$ I know that it is necessary to use L'Hopital's and at first I was taking derivatives with respect to x. However, the book's answer arrives at the solution by differentiating with respect to h. I'm confused as to why differentiating with respect to h is the proper way to approach this problem, I was under the impression that $f$ is a function of x. I did some research and the term ""limiting variable"" is used a lot, but I'm still a little mixed up.","I'm working some problems from a calculus text and came across this question: If $f(x)$ is a function that's differentiable everywhere, what is the value of the limit $$\lim\limits_{h \to 0}\frac{f(x+3h^2)-f(x-h^2)}{2h^2}$$ I know that it is necessary to use L'Hopital's and at first I was taking derivatives with respect to x. However, the book's answer arrives at the solution by differentiating with respect to h. I'm confused as to why differentiating with respect to h is the proper way to approach this problem, I was under the impression that $f$ is a function of x. I did some research and the term ""limiting variable"" is used a lot, but I'm still a little mixed up.",,"['calculus', 'limits', 'derivatives']"
22,Maxima/Minima of absolute function,Maxima/Minima of absolute function,,"Given $a_i=\{a_1,\dots,a_n\}$ and function $$f(x)=\sum_{i=1}^n{|x-a_i|}^3$$ I need to find minimum value of $f(x)$. As far my understanding goes the derivative is given by: $$f'(x) = \sum_{i=1}^n\frac{3*{(x-a_i)}^3}{{|x-a_i|}^3}$$ After this I have no clue how to solve $f'(x)=0$. Any suggestions?","Given $a_i=\{a_1,\dots,a_n\}$ and function $$f(x)=\sum_{i=1}^n{|x-a_i|}^3$$ I need to find minimum value of $f(x)$. As far my understanding goes the derivative is given by: $$f'(x) = \sum_{i=1}^n\frac{3*{(x-a_i)}^3}{{|x-a_i|}^3}$$ After this I have no clue how to solve $f'(x)=0$. Any suggestions?",,"['derivatives', 'absolute-value']"
23,Computing the integral of $-1/f''$,Computing the integral of,-1/f'',"I think this is a very silly question but I have some problems nonetheless. If I know that $g'=-\frac{1}{f''}$, is then $$ g=(f')^{-1}? $$","I think this is a very silly question but I have some problems nonetheless. If I know that $g'=-\frac{1}{f''}$, is then $$ g=(f')^{-1}? $$",,"['calculus', 'integration', 'derivatives', 'closed-form']"
24,Derivative of a function with respect to x containing integral over y,Derivative of a function with respect to x containing integral over y,,"does anyone know how to take a derivative of a function with respect to a variable if that function contains an integral over another variable? For example, what would be the derivative of the following function with respect to x? $$f = \int (2x^2y)dy$$ I think it would be  $$f’(x) =\int (4xy)dy$$ but I haven’t found any rule according which it should be so... just used intuition. Any formal explanation or link to such would be very appreciated. Thanks!","does anyone know how to take a derivative of a function with respect to a variable if that function contains an integral over another variable? For example, what would be the derivative of the following function with respect to x? $$f = \int (2x^2y)dy$$ I think it would be  $$f’(x) =\int (4xy)dy$$ but I haven’t found any rule according which it should be so... just used intuition. Any formal explanation or link to such would be very appreciated. Thanks!",,"['integration', 'derivatives']"
25,What is the derivative of the ReLu of a Matrix with respect to a matrix,What is the derivative of the ReLu of a Matrix with respect to a matrix,,"I want to compute $\frac{\partial r(ZZ^tY)}{\partial Z}$ where the ReLu function is a nonlinear operator $r(x)=max(0,x)$ and $Z \in\mathbb{R}^{n\times m}$ is a matrix. I am wondering also if the derivative of the transpose is the transpose of the derivative of my expression, i.e., $\frac{\partial r(ZZ^tY)^t}{\partial Z}=\big(\frac{\partial r(ZZ^tY)}{\partial Z}\big)^t.$","I want to compute $\frac{\partial r(ZZ^tY)}{\partial Z}$ where the ReLu function is a nonlinear operator $r(x)=max(0,x)$ and $Z \in\mathbb{R}^{n\times m}$ is a matrix. I am wondering also if the derivative of the transpose is the transpose of the derivative of my expression, i.e., $\frac{\partial r(ZZ^tY)^t}{\partial Z}=\big(\frac{\partial r(ZZ^tY)}{\partial Z}\big)^t.$",,"['matrices', 'derivatives', 'partial-derivative', 'matrix-calculus']"
26,Longest pipe that fits around a corner. [duplicate],Longest pipe that fits around a corner. [duplicate],,"This question already has answers here : Intuitive explanation for formula of maximum length of a pipe moving around a corner? (3 answers) Closed 8 years ago . While studying, I came upon the problem ""Two corridors of widths $a$ and $b$ intersect at right angle. What is the length of the longest pipe that can be carried across the two corridors, touching the corner of the wall where the corridors meet?"" The explanation is not detailed, but the answer is shown as  $(a^{2/3} + b^{2/3})^{3/2}$. I have attempted the problem using trigonometry and derivatives, but got stuck early in. Could someone help with this problem?","This question already has answers here : Intuitive explanation for formula of maximum length of a pipe moving around a corner? (3 answers) Closed 8 years ago . While studying, I came upon the problem ""Two corridors of widths $a$ and $b$ intersect at right angle. What is the length of the longest pipe that can be carried across the two corridors, touching the corner of the wall where the corridors meet?"" The explanation is not detailed, but the answer is shown as  $(a^{2/3} + b^{2/3})^{3/2}$. I have attempted the problem using trigonometry and derivatives, but got stuck early in. Could someone help with this problem?",,"['calculus', 'geometry', 'trigonometry', 'derivatives']"
27,"When it comes to using derivatives to graph, do I have all of these steps right?","When it comes to using derivatives to graph, do I have all of these steps right?",,"Perhaps this is a silly question, but I haven't been able to find a clear answer anywhere as to what exactly the steps are for using derivatives to find the shape of a graph (I'm having difficulty even expressing what I mean properly). I've done my best to take information from my textbook and on the internet and create a list of what I think is supposed to be going on. So here it goes: Original function: used to find $f'(x)$ produces the y-value for a critical number found using $f'(x)$ produces the y-value for the inflection point found using $f''(x)$ 1st Derivative: finds $f''(x)$ finds critical numbers shows what intervals a function is increasing or decreasing on by using the roots of $f'(x)$ First derivative test: finds a local maximum or minimum 2nd Derivative: finds where $f(x)$ is a maximum or minimum by evaluating $f''(x)$ by the roots of $f'(x)$ (which takes place of the first derivative test) by using the critical numbers of $f'(x)$, $f''(x)$ can find where the graph is concave up or concave down. gives where the inflection points are by finding where $f''(x)=0$, and then evaluating $f(x)$ at these numbers to find the point. Any help I can get on this will be greatly appreciated!","Perhaps this is a silly question, but I haven't been able to find a clear answer anywhere as to what exactly the steps are for using derivatives to find the shape of a graph (I'm having difficulty even expressing what I mean properly). I've done my best to take information from my textbook and on the internet and create a list of what I think is supposed to be going on. So here it goes: Original function: used to find $f'(x)$ produces the y-value for a critical number found using $f'(x)$ produces the y-value for the inflection point found using $f''(x)$ 1st Derivative: finds $f''(x)$ finds critical numbers shows what intervals a function is increasing or decreasing on by using the roots of $f'(x)$ First derivative test: finds a local maximum or minimum 2nd Derivative: finds where $f(x)$ is a maximum or minimum by evaluating $f''(x)$ by the roots of $f'(x)$ (which takes place of the first derivative test) by using the critical numbers of $f'(x)$, $f''(x)$ can find where the graph is concave up or concave down. gives where the inflection points are by finding where $f''(x)=0$, and then evaluating $f(x)$ at these numbers to find the point. Any help I can get on this will be greatly appreciated!",,"['calculus', 'derivatives', 'graphing-functions']"
28,What did I do wrong?,What did I do wrong?,,"So, I have found the following problem. This problem is a multiple-choice one, and I have to pick the correct answer. The problem, gives a function $f:D \to R$, $$f(x)=\frac{xe^x}{e^x-a}$$ with $a$ being a real number. I have managed to proof that if $a \in (0,1)$, the function is strictly increasing. I will describe how. If $a \in (0,1)$, the function is well defiend for all real numbers, except $\ln{a}$. The derivate is: $$f'(x)=e^x\frac{e^x-a-ax}{(e^x-a)^2}$$ To study its sign we only need to study the sign of $g(x) = e^x-a-ax$. Since $$g'(x)=e^x-a$$ I have found that $g(x)>0$, and $x_0=-a\ln{a}$ it's a minimum point. It follows that $f'(x)>0$, and hence f is strictly increasing. So, what's the mistake in my reasoning..?","So, I have found the following problem. This problem is a multiple-choice one, and I have to pick the correct answer. The problem, gives a function $f:D \to R$, $$f(x)=\frac{xe^x}{e^x-a}$$ with $a$ being a real number. I have managed to proof that if $a \in (0,1)$, the function is strictly increasing. I will describe how. If $a \in (0,1)$, the function is well defiend for all real numbers, except $\ln{a}$. The derivate is: $$f'(x)=e^x\frac{e^x-a-ax}{(e^x-a)^2}$$ To study its sign we only need to study the sign of $g(x) = e^x-a-ax$. Since $$g'(x)=e^x-a$$ I have found that $g(x)>0$, and $x_0=-a\ln{a}$ it's a minimum point. It follows that $f'(x)>0$, and hence f is strictly increasing. So, what's the mistake in my reasoning..?",,"['calculus', 'derivatives']"
29,When can I use the natural log to help solve an integral?,When can I use the natural log to help solve an integral?,,Why is it okay to do this: $\int \frac{1}{x-2}dx = \ln(x-2)$ but not this: $\int \frac{1}{1-x^2}dx   = \ln(1-x^2)$,Why is it okay to do this: $\int \frac{1}{x-2}dx = \ln(x-2)$ but not this: $\int \frac{1}{1-x^2}dx   = \ln(1-x^2)$,,"['calculus', 'integration', 'derivatives', 'logarithms']"
30,Proving a corollary of a corollary of the Mean Value Theorem (corollary-ception),Proving a corollary of a corollary of the Mean Value Theorem (corollary-ception),,"This is will a wordy question but here it goes: My analysis book states the mean-value theorem and then a corollary which we will label as (1): Let $f$ be a differentiable function on $(a,b)$ such that $f'(x) = 0$ for all $x \in (a,b)$. Then $f$ is a constant function on $(a,b)$. My book then goes on to state an additional corollary which we will label as (2): Let $f$ and $g$ be differentiable functions on $(a,b)$ such that $f'=g'$ on $(a,b)$. Then there exists a constant $c$ such that $f(x)    = g(x) + c$ for all $x \in (a,b)$. This corollary also makes sense. I interpreted it as two functions $f$ and $g$ with parallel tangent lines at every point so that one function is an exact copy of the other except it is at a different height. However, in the proof of (2), the book simply states ""Apply [1] to the function $f-g$. I don't see how (1) completely proves (2) because (1) only deals with functions with $f'(x) = 0$ and (2) deals with functions with f' = g' on a whole interval. Can anyone explain this? Thank you!","This is will a wordy question but here it goes: My analysis book states the mean-value theorem and then a corollary which we will label as (1): Let $f$ be a differentiable function on $(a,b)$ such that $f'(x) = 0$ for all $x \in (a,b)$. Then $f$ is a constant function on $(a,b)$. My book then goes on to state an additional corollary which we will label as (2): Let $f$ and $g$ be differentiable functions on $(a,b)$ such that $f'=g'$ on $(a,b)$. Then there exists a constant $c$ such that $f(x)    = g(x) + c$ for all $x \in (a,b)$. This corollary also makes sense. I interpreted it as two functions $f$ and $g$ with parallel tangent lines at every point so that one function is an exact copy of the other except it is at a different height. However, in the proof of (2), the book simply states ""Apply [1] to the function $f-g$. I don't see how (1) completely proves (2) because (1) only deals with functions with $f'(x) = 0$ and (2) deals with functions with f' = g' on a whole interval. Can anyone explain this? Thank you!",,"['real-analysis', 'derivatives', 'continuity']"
31,Real Analysis - differentiable,Real Analysis - differentiable,,"$f:[0,\infty]\rightarrow \mathbb{R}$ is twice differentiable. If $f''$ is bounded and exists the limit of $f(x)$ at infinity, then $\lim_{x\rightarrow \infty}f'(x)=0$. I tried to use the Taylor's formula but I couldn't prove that $\lim_{x\rightarrow \infty}f'(x)=0$.","$f:[0,\infty]\rightarrow \mathbb{R}$ is twice differentiable. If $f''$ is bounded and exists the limit of $f(x)$ at infinity, then $\lim_{x\rightarrow \infty}f'(x)=0$. I tried to use the Taylor's formula but I couldn't prove that $\lim_{x\rightarrow \infty}f'(x)=0$.",,"['real-analysis', 'derivatives']"
32,How can I determine the sequence which has this generating function?,How can I determine the sequence which has this generating function?,,"In a discrete mathematics past paper, I must find the first eight terms of the sequence whose generating function is $$\frac{x^2}{(1-x)(1-2x)}.$$ I have looked at both of the following posts: How can I determine the sequence generated by a generating function? how to determine the sequence generated by these generating functions? A comment from the first post states that 'To compute the term $a_n$ in the sequence generated by an ordinary generating function, take the $n$-th derivative of the function at $x=0$ and divide by $n!$.' I haven't tried to use this method in my case, because I would like to see a justification for it, rather than simply memorising and using it. An answer from the second post refers to updating the affected terms in a simpler generating function; however, I am not sure if that would be useful in my case. I would appreciate help to understand how to efficiently solve this problem.","In a discrete mathematics past paper, I must find the first eight terms of the sequence whose generating function is $$\frac{x^2}{(1-x)(1-2x)}.$$ I have looked at both of the following posts: How can I determine the sequence generated by a generating function? how to determine the sequence generated by these generating functions? A comment from the first post states that 'To compute the term $a_n$ in the sequence generated by an ordinary generating function, take the $n$-th derivative of the function at $x=0$ and divide by $n!$.' I haven't tried to use this method in my case, because I would like to see a justification for it, rather than simply memorising and using it. An answer from the second post refers to updating the affected terms in a simpler generating function; however, I am not sure if that would be useful in my case. I would appreciate help to understand how to efficiently solve this problem.",,"['discrete-mathematics', 'derivatives', 'generating-functions']"
33,mean value theorem sin(b) - sin(a),mean value theorem sin(b) - sin(a),,"It's too much hassle to post it here as latex, to so here's the screenshot . I don't understand why |cos(c)| = 1 Why 1? Why not $\frac {\sqrt{3}}{2}$? Why absolute value assumes the max value a function can take? Shouldn't it be like: $\cos(c) > 0$ and $-\cos(c) < 0$ ?","It's too much hassle to post it here as latex, to so here's the screenshot . I don't understand why |cos(c)| = 1 Why 1? Why not $\frac {\sqrt{3}}{2}$? Why absolute value assumes the max value a function can take? Shouldn't it be like: $\cos(c) > 0$ and $-\cos(c) < 0$ ?",,"['calculus', 'derivatives', 'absolute-value']"
34,How to find $ \frac{d (\tanh(kx))}{d x}=?$,How to find, \frac{d (\tanh(kx))}{d x}=?,"I am tried to resolve the problem  $$ \frac{d (\tanh(kx))}{d x}=?$$ where $k$ is positive value. I found one solution that is  $$ \frac{d (\tanh(kx))}{d x}=\frac{k}{2\cosh^2(kx)}$$ Is it right? If is not true, could you give me the true solution. Thanks","I am tried to resolve the problem  $$ \frac{d (\tanh(kx))}{d x}=?$$ where $k$ is positive value. I found one solution that is  $$ \frac{d (\tanh(kx))}{d x}=\frac{k}{2\cosh^2(kx)}$$ Is it right? If is not true, could you give me the true solution. Thanks",,"['derivatives', 'partial-derivative']"
35,"TI-84 gives 100 for $\frac{d}{dx}\sqrt[3]{x}\,\big|_{x=0}$",TI-84 gives 100 for,"\frac{d}{dx}\sqrt[3]{x}\,\big|_{x=0}","My TI-84 Silver Edition is doing something strange. If  $f(x)=\sqrt[3]{x}$, $\frac{d}{dx}\sqrt[3]{x}=\frac{1}{3\sqrt[3]{{x^2}}}$ At $x=0$, $\frac{d}{dx}f(0)$ is undefined. When I type $\frac{d}{dx}\sqrt[3]{x}|_{x=0}$, the calculator returns $100$. Any reason behind this?","My TI-84 Silver Edition is doing something strange. If  $f(x)=\sqrt[3]{x}$, $\frac{d}{dx}\sqrt[3]{x}=\frac{1}{3\sqrt[3]{{x^2}}}$ At $x=0$, $\frac{d}{dx}f(0)$ is undefined. When I type $\frac{d}{dx}\sqrt[3]{x}|_{x=0}$, the calculator returns $100$. Any reason behind this?",,"['calculus', 'derivatives', 'calculator']"
36,Derivative for numerical models.,Derivative for numerical models.,,"I am working in Mechanical engineering and Computer vision, in which I use a matlab code (or codes) to represent  a specific system or process. Of course such model has an input , an implimented  structure , and an output. In the way of studying the sensitivity analysis of such models, I think of using local sensitivity analysis tests, in which the partial derivative of the models is computed to represent the index of sensitivity of each parameter or variable. My question here is that, If my model cannot be represented by a function to evluate partial derivatives  explicitly, is it possible to use directly the numerical methods to derive the partial derivatives? For example, we may run the model for some values $x_1, x_2, ...,x_j,..., x_n$ to give  $y_j$, then for the values  $  x_1, x_2, ...,x_j+\Delta_j,... , x_n$ to give  $y_{j+\Delta}$, and then estimate the partial derivative wrt the variable $x_j$ by  $\frac{y_{j+\Delta}-y_j}{\Delta}$. I think this way is straightforward, but how to be assure that our model has a differentiable form i.e. is differentiable? Or let me say, can we speak about difference in variation without including the notion of partial derivatives and differentiability of the model? I respect any perspective and I highly appriciate your opinions. Thanks .","I am working in Mechanical engineering and Computer vision, in which I use a matlab code (or codes) to represent  a specific system or process. Of course such model has an input , an implimented  structure , and an output. In the way of studying the sensitivity analysis of such models, I think of using local sensitivity analysis tests, in which the partial derivative of the models is computed to represent the index of sensitivity of each parameter or variable. My question here is that, If my model cannot be represented by a function to evluate partial derivatives  explicitly, is it possible to use directly the numerical methods to derive the partial derivatives? For example, we may run the model for some values $x_1, x_2, ...,x_j,..., x_n$ to give  $y_j$, then for the values  $  x_1, x_2, ...,x_j+\Delta_j,... , x_n$ to give  $y_{j+\Delta}$, and then estimate the partial derivative wrt the variable $x_j$ by  $\frac{y_{j+\Delta}-y_j}{\Delta}$. I think this way is straightforward, but how to be assure that our model has a differentiable form i.e. is differentiable? Or let me say, can we speak about difference in variation without including the notion of partial derivatives and differentiability of the model? I respect any perspective and I highly appriciate your opinions. Thanks .",,"['derivatives', 'numerical-methods', 'partial-derivative', 'differential-forms']"
37,"Analyzing if function is ""onto""","Analyzing if function is ""onto""",,"I have some function $g$. I know that $g \in C^1[a,b]$, so $g'(x)$ exists. I want to know, if $g: [a,b] \to [a,b]$ is onto. How can I find out if this is true or not? P.S. I am not saying all $g$ have the said property, I want to have some kind of test to distinguish functions with this property from functions without it.","I have some function $g$. I know that $g \in C^1[a,b]$, so $g'(x)$ exists. I want to know, if $g: [a,b] \to [a,b]$ is onto. How can I find out if this is true or not? P.S. I am not saying all $g$ have the said property, I want to have some kind of test to distinguish functions with this property from functions without it.",,"['real-analysis', 'derivatives']"
38,How exactly is this happening?,How exactly is this happening?,,"I was studying Derivative and my book says if: Then its derivative is: I can't understand how the writer has changed the first derivative fraction into the second one. In other words, how did he simplify? Note: I'm really really basic, so please explain in details. Thank you.","I was studying Derivative and my book says if: Then its derivative is: I can't understand how the writer has changed the first derivative fraction into the second one. In other words, how did he simplify? Note: I'm really really basic, so please explain in details. Thank you.",,"['derivatives', 'fractions']"
39,The function $f(z)=|z|^2$ is only differentiable at the origin,The function  is only differentiable at the origin,f(z)=|z|^2,"Show that a complex function $f(z)=|z|^2$ is continuous on all complex plan $\mathbb{C}$, but it is only differentiable at the origin. I know that a complex function is continuous at $z_0$ $$\Leftrightarrow f(z_0) \space\exists\space\forall z_0 \in \mathbb{C} $$ $$\Leftrightarrow \lim_{z\to z_0}f(z)\space\exists \space\forall z_0 \in \mathbb{C}$$ $$\Leftrightarrow \lim_{z\to z_0}f(z)= f(z_0)$$ And $f(z_0)$ is continous in all complex plan $\Leftrightarrow$ is continuous at all $z_0 \in \mathbb{C}$ but I do not know how to formally demonstrate this, or that the function is differentiable only at the origin.","Show that a complex function $f(z)=|z|^2$ is continuous on all complex plan $\mathbb{C}$, but it is only differentiable at the origin. I know that a complex function is continuous at $z_0$ $$\Leftrightarrow f(z_0) \space\exists\space\forall z_0 \in \mathbb{C} $$ $$\Leftrightarrow \lim_{z\to z_0}f(z)\space\exists \space\forall z_0 \in \mathbb{C}$$ $$\Leftrightarrow \lim_{z\to z_0}f(z)= f(z_0)$$ And $f(z_0)$ is continous in all complex plan $\Leftrightarrow$ is continuous at all $z_0 \in \mathbb{C}$ but I do not know how to formally demonstrate this, or that the function is differentiable only at the origin.",,"['complex-analysis', 'derivatives', 'complex-numbers']"
40,Find values of b for which $f(x)=x^3+x^2+bx+6$ is increasing for all values of $x$,Find values of b for which  is increasing for all values of,f(x)=x^3+x^2+bx+6 x,"For the function defined by $f(x)$, find the values of $b$ that results in $f(x)$ increasing for all values of $x$. I found the derivative: $f'(x) = 3x^2+2x+b$ and I know that it should always be equal to a value greater than 0, I just don't know how to find $b$.","For the function defined by $f(x)$, find the values of $b$ that results in $f(x)$ increasing for all values of $x$. I found the derivative: $f'(x) = 3x^2+2x+b$ and I know that it should always be equal to a value greater than 0, I just don't know how to find $b$.",,"['calculus', 'derivatives']"
41,Property of a differentiable function,Property of a differentiable function,,"Which one of the following is true: 1.If a function real valued function $f$ satisfies $|f(x)-f(y)|\leq |x-y|^{\sqrt2}$ for all $x,y\in \mathbb R$ is $f$ a constant? 2.If $f$ is differentiable and its derivative is bounded then there exists $\epsilon_0>0$ such that $0<\epsilon\le\epsilon_0$,the function   $g(x)=x+\epsilon f(x)$ is injective. I think 1 is not true as $f(x)=x$ is a counter example for 2 let $g(x_1)=g(x_2)\implies x_1+\epsilon f(x_1)=x_2+\epsilon f(x_2)$ $\implies \frac{1}{\epsilon}=\dfrac{f(x_1)-f(x_2)}{x_1-x_2}$ which is no longer bounded when $\epsilon$ is very small contradiction Hence $x_1=x_2$ Is my solution correct?Hope someone helps","Which one of the following is true: 1.If a function real valued function $f$ satisfies $|f(x)-f(y)|\leq |x-y|^{\sqrt2}$ for all $x,y\in \mathbb R$ is $f$ a constant? 2.If $f$ is differentiable and its derivative is bounded then there exists $\epsilon_0>0$ such that $0<\epsilon\le\epsilon_0$,the function   $g(x)=x+\epsilon f(x)$ is injective. I think 1 is not true as $f(x)=x$ is a counter example for 2 let $g(x_1)=g(x_2)\implies x_1+\epsilon f(x_1)=x_2+\epsilon f(x_2)$ $\implies \frac{1}{\epsilon}=\dfrac{f(x_1)-f(x_2)}{x_1-x_2}$ which is no longer bounded when $\epsilon$ is very small contradiction Hence $x_1=x_2$ Is my solution correct?Hope someone helps",,"['real-analysis', 'derivatives']"
42,A Sequence of Functions Converging to the Derivative at a Point,A Sequence of Functions Converging to the Derivative at a Point,,"I'm reading Neal Carothers' Real Analysis and while in the process of constructing an everywhere continuous but nowhere differentiable function, he claims that $$\dfrac{f(v_n)-f(u_n)}{(v_n-u_n)} \to f'(x)$$ as $n\to \infty$ if $f$ is continuous over the entire real line and is differentiable at $x$, and if $u_n\le x \le v_n$, $u_n<v_n$, and $v_n-u_n\to 0$ as $n\to \infty$. I'm trying to show that this is true by using the following definition of the derivative: $$f'(x)=\lim_{t \mathop \to x}\dfrac{f(t)-f(x)}{t-x}$$ but I can't seem to make the math come out. I thought about using the mean value theorem but we're assuming differentiability at a point, not an interval. (Aside: $f$ is assumed to be uniformly continuous here, although I don't think we need that fact.) Any help at all would be much appreciated!","I'm reading Neal Carothers' Real Analysis and while in the process of constructing an everywhere continuous but nowhere differentiable function, he claims that $$\dfrac{f(v_n)-f(u_n)}{(v_n-u_n)} \to f'(x)$$ as $n\to \infty$ if $f$ is continuous over the entire real line and is differentiable at $x$, and if $u_n\le x \le v_n$, $u_n<v_n$, and $v_n-u_n\to 0$ as $n\to \infty$. I'm trying to show that this is true by using the following definition of the derivative: $$f'(x)=\lim_{t \mathop \to x}\dfrac{f(t)-f(x)}{t-x}$$ but I can't seem to make the math come out. I thought about using the mean value theorem but we're assuming differentiability at a point, not an interval. (Aside: $f$ is assumed to be uniformly continuous here, although I don't think we need that fact.) Any help at all would be much appreciated!",,"['calculus', 'real-analysis', 'sequences-and-series', 'derivatives', 'convergence-divergence']"
43,Derivative of a vector,Derivative of a vector,,"Let $p, v :$ real, positive $1\times n$ vectors, $c^T:$ real, non - negative $n\times 1$ vector, $I:$ the identity matrix. Assume that the following relationship holds true: $$p(v) = v\cdot ( I - c^Tv)^{-1}$$ How can we compute the derivative: $$\dfrac{dp}{dv}(v)?$$","Let $p, v :$ real, positive $1\times n$ vectors, $c^T:$ real, non - negative $n\times 1$ vector, $I:$ the identity matrix. Assume that the following relationship holds true: $$p(v) = v\cdot ( I - c^Tv)^{-1}$$ How can we compute the derivative: $$\dfrac{dp}{dv}(v)?$$",,"['matrices', 'derivatives']"
44,Function such that $f^{(n)}(0)=\frac{(n!)^2}{(2n+1)!}$,Function such that,f^{(n)}(0)=\frac{(n!)^2}{(2n+1)!},I was trying to solve another problem and come up with the problem if there is a function with closed form such that $$f^{(n)}(0)=\frac{(n!)^2}{(2n+1)!};(n\ge1).$$ I tried to check the condition for compositions of some elementary functions but could not find such function. Any hints and suggestions would be appreciated. Thanks!,I was trying to solve another problem and come up with the problem if there is a function with closed form such that $$f^{(n)}(0)=\frac{(n!)^2}{(2n+1)!};(n\ge1).$$ I tried to check the condition for compositions of some elementary functions but could not find such function. Any hints and suggestions would be appreciated. Thanks!,,"['derivatives', 'power-series']"
45,use fundamental theorem of calculus to find a function $f(x)$ and a number $a$,use fundamental theorem of calculus to find a function  and a number,f(x) a,I thought I understood the fundamental theorem of calculus but I'm confused on the following problem.. Use the Fundamental Theorem of Calculus to find a function $f(x)$ and a number $a$ so that $a+\int_{4}^{x}\frac{f(t)}{t^2}dt=2\sqrt{x}$ for all $x>0$. I don't have the answer to check but what I did was take the derivative of both sides of the equation which means $a$ could be any number because it's derivative will be zero and the derivative of the integral is $\frac{f(x)}{x^2}$ and the derivative of the right hand side is $\frac{1}{\sqrt{x}}$ so I determined $f(x)$ would have to equal $x^{3/2}$ Am I correct? or could someone please explain the method to solve this problem. Thanks!,I thought I understood the fundamental theorem of calculus but I'm confused on the following problem.. Use the Fundamental Theorem of Calculus to find a function $f(x)$ and a number $a$ so that $a+\int_{4}^{x}\frac{f(t)}{t^2}dt=2\sqrt{x}$ for all $x>0$. I don't have the answer to check but what I did was take the derivative of both sides of the equation which means $a$ could be any number because it's derivative will be zero and the derivative of the integral is $\frac{f(x)}{x^2}$ and the derivative of the right hand side is $\frac{1}{\sqrt{x}}$ so I determined $f(x)$ would have to equal $x^{3/2}$ Am I correct? or could someone please explain the method to solve this problem. Thanks!,,"['calculus', 'integration', 'derivatives']"
46,$d(\beta \wedge d\beta)=0$ if $k$ is even.,if  is even.,d(\beta \wedge d\beta)=0 k,Let $\beta$ be a $k$-form. Show that $d(\beta \wedge d\beta)=0$ if $k$ is even. I get that $d(\beta \wedge d\beta)=d\beta \wedge d \beta + (-1)^k\beta \wedge d^2\beta=d\beta \wedge d \beta$. Why doesnt this just equal $0$? I thought that if $dx^{(i)} = f^{(i)}$ then $dx^{(i)} \wedge dx^{(i)}=0 \ \forall i$?,Let $\beta$ be a $k$-form. Show that $d(\beta \wedge d\beta)=0$ if $k$ is even. I get that $d(\beta \wedge d\beta)=d\beta \wedge d \beta + (-1)^k\beta \wedge d^2\beta=d\beta \wedge d \beta$. Why doesnt this just equal $0$? I thought that if $dx^{(i)} = f^{(i)}$ then $dx^{(i)} \wedge dx^{(i)}=0 \ \forall i$?,,"['calculus', 'differential-geometry', 'derivatives', 'differential-forms']"
47,Show that $F$ and $G$ differ by a constant,Show that  and  differ by a constant,F G,"Suppose $F$ and $G$ are differentiable functions defined on $[a,b]$ such that $F'(x)=G'(x)$ for all $x\in[a,b]$. Using the fundamental theorem of calculus, show that $F$ and $G$ differ by a constant. That is, show that there exists a $C\in\mathbb R$ such that $F(x)-G(x)=C$. I'm assuming this is quite simple, but I can't seem to figure it out.","Suppose $F$ and $G$ are differentiable functions defined on $[a,b]$ such that $F'(x)=G'(x)$ for all $x\in[a,b]$. Using the fundamental theorem of calculus, show that $F$ and $G$ differ by a constant. That is, show that there exists a $C\in\mathbb R$ such that $F(x)-G(x)=C$. I'm assuming this is quite simple, but I can't seem to figure it out.",,"['real-analysis', 'derivatives']"
48,Does all function's domain stay the same\expands as we derivate them?,Does all function's domain stay the same\expands as we derivate them?,,"Lets define a funciton $f(x)$ with a domain of, lets say $a>x>b$. If I derivate this function, it's domain will always stay the same or expand? Or it can be ""reduced""? Is that mean that $f'(x)$ must be defined in the following domain?: $$g>x>t$$ $$g \leq a$$  $$t \geq b$$","Lets define a funciton $f(x)$ with a domain of, lets say $a>x>b$. If I derivate this function, it's domain will always stay the same or expand? Or it can be ""reduced""? Is that mean that $f'(x)$ must be defined in the following domain?: $$g>x>t$$ $$g \leq a$$  $$t \geq b$$",,['derivatives']
49,"If $f$ is a continuous odd function. Prove that if $f$ is differentiable at $0$, then there is a continuous even function $g$ such that $f(x) = xg(x)$","If  is a continuous odd function. Prove that if  is differentiable at , then there is a continuous even function  such that",f f 0 g f(x) = xg(x),"I'm working backwards to see if I can find the $g$, however, when I take the derivative of $xg(x)$  I have $f'(x) = g(x) + xg(x)'$ at $0$, then it will always ends up with $0$. Then I have no idea how to continue. Any help is appreciated","I'm working backwards to see if I can find the $g$, however, when I take the derivative of $xg(x)$  I have $f'(x) = g(x) + xg(x)'$ at $0$, then it will always ends up with $0$. Then I have no idea how to continue. Any help is appreciated",,"['calculus', 'derivatives', 'continuity']"
50,If $f''(x_0)$ exists then $\lim_{x \to x_0} \frac{f(x_0+h)-2f(x_0)+f(x_0-h)}{h^2} = f''(x_0)$,If  exists then,f''(x_0) \lim_{x \to x_0} \frac{f(x_0+h)-2f(x_0)+f(x_0-h)}{h^2} = f''(x_0),"Prove: if $f''(x_0)$ exists then $\lim\limits_{x \rightarrow x_0} \dfrac{f(x_0+h)-2f(x_0)+f(x_0-h)}{h^2} = f''(x_0)$. I'm not exactly sure how Taylor's theorem fits into all this, but I found the first derivative, and I haven't been able to progress past that.","Prove: if $f''(x_0)$ exists then $\lim\limits_{x \rightarrow x_0} \dfrac{f(x_0+h)-2f(x_0)+f(x_0-h)}{h^2} = f''(x_0)$. I'm not exactly sure how Taylor's theorem fits into all this, but I found the first derivative, and I haven't been able to progress past that.",,"['real-analysis', 'derivatives', 'taylor-expansion']"
51,How can I determine a general formula for the nth derivative of any continuous function $f(x)$ differentiable at least $n$ times?,How can I determine a general formula for the nth derivative of any continuous function  differentiable at least  times?,f(x) n,"I know how to do it with easier functions, but is there a universal method which can be applied to all continuous functions differentiable at least $n$ times(introduced to in a second year calculus class)? I can do it for easy ones like $\sin(x)$ and $\cos(x)$, but $\sin(x^2)$ and $\tan(x)$ and $\ln(1+x^2)$ are proving to be very difficult. Thanks.","I know how to do it with easier functions, but is there a universal method which can be applied to all continuous functions differentiable at least $n$ times(introduced to in a second year calculus class)? I can do it for easy ones like $\sin(x)$ and $\cos(x)$, but $\sin(x^2)$ and $\tan(x)$ and $\ln(1+x^2)$ are proving to be very difficult. Thanks.",,['derivatives']
52,Derivative of an exponential function,Derivative of an exponential function,,"I am trying to solve $$\frac{1}{e^{x}}$$ I first tried using the quotient rule, and ended up with: $$\frac{e^{x}}{(e^{x})^2}$$ That was not the right answer, so I took a look at wolfram, and wolfram rewrites $$\frac{1}{e^{x}}$$ as $$e^{-x}$$ And then apply the chain rule, but that is irrelevant to me right now. I am looking for someone to explain how $$\frac{1}{e^{x}} = e^{-x}$$ Any help would be great, I want to solve this but I need to understand what just happened.","I am trying to solve $$\frac{1}{e^{x}}$$ I first tried using the quotient rule, and ended up with: $$\frac{e^{x}}{(e^{x})^2}$$ That was not the right answer, so I took a look at wolfram, and wolfram rewrites $$\frac{1}{e^{x}}$$ as $$e^{-x}$$ And then apply the chain rule, but that is irrelevant to me right now. I am looking for someone to explain how $$\frac{1}{e^{x}} = e^{-x}$$ Any help would be great, I want to solve this but I need to understand what just happened.",,"['derivatives', 'exponential-function']"
53,Derivative notation?,Derivative notation?,,"I am getting a bit confused with the primed notation for derivatives, does  $$f'(g(x))$$ mean the first derivative of $f$ with respect to the spacial coordinate $x$ or with respect to $g(x)$. If it is the latter case, then, does the ' notation all ways represent the derivative with respect to the argument, in this case $g(x)$?","I am getting a bit confused with the primed notation for derivatives, does  $$f'(g(x))$$ mean the first derivative of $f$ with respect to the spacial coordinate $x$ or with respect to $g(x)$. If it is the latter case, then, does the ' notation all ways represent the derivative with respect to the argument, in this case $g(x)$?",,['derivatives']
54,Critical points for undefined fraction on closed interval,Critical points for undefined fraction on closed interval,,"I am told to find the absolute extrema of $$h(x) = \frac{8+x}{8-x},[4,6]$$ So I obtain the derivative of $$\frac{16}{(8-x)^2}$$ The trouble I am having is trying to determine the critical points. I know that a critical point is found where the derivative is equal to zero or does not exist. So my assumption would be that the critical point is $\frac{16}{(8-8)^2}$, i.e. x = 8 Looking for someone to shed some light on this. Would there be no critical points because the function is discontinuous (I think) and therefore I would just test the endpoints? Thank you for any help!","I am told to find the absolute extrema of $$h(x) = \frac{8+x}{8-x},[4,6]$$ So I obtain the derivative of $$\frac{16}{(8-x)^2}$$ The trouble I am having is trying to determine the critical points. I know that a critical point is found where the derivative is equal to zero or does not exist. So my assumption would be that the critical point is $\frac{16}{(8-8)^2}$, i.e. x = 8 Looking for someone to shed some light on this. Would there be no critical points because the function is discontinuous (I think) and therefore I would just test the endpoints? Thank you for any help!",,"['calculus', 'derivatives']"
55,evaluating derivative of $\log_4(2x^2+1)$,evaluating derivative of,\log_4(2x^2+1),Find the derivative and evaluate at $f\;'(2):$ $$\log_4(2x^2+1)$$ $\log_4(2x^2+1)=y$ $4^y=2x^2+1$ $4^y\ln4 \times y\;'=4x$ $y\;'=\dfrac{4x}{4^y\ln4}\implies \dfrac{4x}{(2x^2+1)\ln4}$ What am I doing wrong? I evaluated at $2$ and got $1.154$,Find the derivative and evaluate at $f\;'(2):$ $$\log_4(2x^2+1)$$ $\log_4(2x^2+1)=y$ $4^y=2x^2+1$ $4^y\ln4 \times y\;'=4x$ $y\;'=\dfrac{4x}{4^y\ln4}\implies \dfrac{4x}{(2x^2+1)\ln4}$ What am I doing wrong? I evaluated at $2$ and got $1.154$,,"['calculus', 'derivatives', 'logarithms']"
56,"Product rule proof for $f,g: U\subset\mathbb{R}^n \to \mathbb{R}$",Product rule proof for,"f,g: U\subset\mathbb{R}^n \to \mathbb{R}","I've been struggling with that proposition but I don't know how to prove it. Let $f: U\subset\mathbb{R}^n \to \mathbb{R}$, $g: U\subset\mathbb{R}^n \to \mathbb{R}$ be functions that are differentiable on a point $a\in U$. Let $h(x)=f(x)g(x)$, with $x \in \mathbb{R}^n$.Then $h: U\subset\mathbb{R}^n \to \mathbb{R}$ is differentiable on $a$ and \begin{equation} Dh(a)=g(a)Df(a)+f(a)Dg(a). \end{equation} Can anybody give me a hint or something? Thanks!","I've been struggling with that proposition but I don't know how to prove it. Let $f: U\subset\mathbb{R}^n \to \mathbb{R}$, $g: U\subset\mathbb{R}^n \to \mathbb{R}$ be functions that are differentiable on a point $a\in U$. Let $h(x)=f(x)g(x)$, with $x \in \mathbb{R}^n$.Then $h: U\subset\mathbb{R}^n \to \mathbb{R}$ is differentiable on $a$ and \begin{equation} Dh(a)=g(a)Df(a)+f(a)Dg(a). \end{equation} Can anybody give me a hint or something? Thanks!",,"['calculus', 'derivatives']"
57,How to derive $J_v(x)$,How to derive,J_v(x),"I've seen many sources say that $\frac{\text{d}}{\text{d}x}J_v(x) = J_{v-1}(x) - \frac{v}{x}J_v(x)$, but every time I try to derive it, I get the conjugate, $\frac{v}{x}J_v(x) - J_{v-1}(x)$. Could someone walk me through the steps to arrive at this derivative, so I may see where I went wrong? $$ J_v(x) = \sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v}}\\ \begin{align} \frac{\text{d}}{\text{d}x}J_v(x) & = \sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\frac{\text{d}}{\text{d}x}\left(\frac{x}{2}\right)^{2n+v}} \\ & = \sum_{n=0}^\infty{\frac{(-1)^n(2n+v)}{2\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v-1}} \\ & = \sum_{n=0}^\infty{\frac{(-1)^nn}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v-1}} + \frac{v}{2}\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v-1}} \\ & = \left(\frac{x}{2}\right)^{-1}\left[\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)(n-1)!}\left(\frac{x}{2}\right)^{2n+v}} + \frac{v}{2}\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v}}\right] \\ & = \left(\frac{2}{x}\right)\left[\sum_{n=1}^\infty{\frac{(-1)^{n+1}}{\Pi(v+n-1)n!}\left(\frac{x}{2}\right)^{2(n-1)+v}} + \frac{v}{2}\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v}}\right] \\ & = -\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi((v-1)+n)n!}\left(\frac{x}{2}\right)^{2n+(v-1)}} + \frac{v}{x}\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v}} \\ & = \frac{v}{x}J_v(x) - J_{v-1}(x) \end{align} $$","I've seen many sources say that $\frac{\text{d}}{\text{d}x}J_v(x) = J_{v-1}(x) - \frac{v}{x}J_v(x)$, but every time I try to derive it, I get the conjugate, $\frac{v}{x}J_v(x) - J_{v-1}(x)$. Could someone walk me through the steps to arrive at this derivative, so I may see where I went wrong? $$ J_v(x) = \sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v}}\\ \begin{align} \frac{\text{d}}{\text{d}x}J_v(x) & = \sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\frac{\text{d}}{\text{d}x}\left(\frac{x}{2}\right)^{2n+v}} \\ & = \sum_{n=0}^\infty{\frac{(-1)^n(2n+v)}{2\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v-1}} \\ & = \sum_{n=0}^\infty{\frac{(-1)^nn}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v-1}} + \frac{v}{2}\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v-1}} \\ & = \left(\frac{x}{2}\right)^{-1}\left[\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)(n-1)!}\left(\frac{x}{2}\right)^{2n+v}} + \frac{v}{2}\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v}}\right] \\ & = \left(\frac{2}{x}\right)\left[\sum_{n=1}^\infty{\frac{(-1)^{n+1}}{\Pi(v+n-1)n!}\left(\frac{x}{2}\right)^{2(n-1)+v}} + \frac{v}{2}\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v}}\right] \\ & = -\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi((v-1)+n)n!}\left(\frac{x}{2}\right)^{2n+(v-1)}} + \frac{v}{x}\sum_{n=0}^\infty{\frac{(-1)^n}{\Pi(v+n)n!}\left(\frac{x}{2}\right)^{2n+v}} \\ & = \frac{v}{x}J_v(x) - J_{v-1}(x) \end{align} $$",,"['derivatives', 'bessel-functions']"
58,Trigonometric Identity for tan?,Trigonometric Identity for tan?,,"Can somebody please help with this (probably simple) query. Given  $$dx = R\,d(\tan\theta)$$ this can be expressed as $$dx = R\,\sec^2\theta\,d\theta$$ I can't determine where the $\sec^2\theta\,d\theta$ terms are coming from.  I know there is the following Trigonometric Identity for $\tan\theta$ : $$\tan^2\theta = \sec^2\theta - 1$$ But for some reason I'm struggling! Not necessarily after the answer but some pointers would be appreciated. Thankyou.","Can somebody please help with this (probably simple) query. Given  $$dx = R\,d(\tan\theta)$$ this can be expressed as $$dx = R\,\sec^2\theta\,d\theta$$ I can't determine where the $\sec^2\theta\,d\theta$ terms are coming from.  I know there is the following Trigonometric Identity for $\tan\theta$ : $$\tan^2\theta = \sec^2\theta - 1$$ But for some reason I'm struggling! Not necessarily after the answer but some pointers would be appreciated. Thankyou.",,"['geometry', 'derivatives']"
59,$\displaystyle \frac{d}{dx}2^x$ where $x=0$,where,\displaystyle \frac{d}{dx}2^x x=0,"I put into Wolfram Alpha: d/dx 2^x Where it told me $f'(x)=2^x\log(2)$. Then I put in d/dx 2^x where x=0 and it said ""$\displaystyle \log(2)\approx0.693147$"" I know through Wolfram Alpha and a couple of calculators $\log(2)\approx0.30103$. But Wolfram Alpha and my graphing calculator agree the derivative at $0$ of $2^x$ is $0.693147$ and the log of 2 is $0.30103$. Why?","I put into Wolfram Alpha: d/dx 2^x Where it told me $f'(x)=2^x\log(2)$. Then I put in d/dx 2^x where x=0 and it said ""$\displaystyle \log(2)\approx0.693147$"" I know through Wolfram Alpha and a couple of calculators $\log(2)\approx0.30103$. But Wolfram Alpha and my graphing calculator agree the derivative at $0$ of $2^x$ is $0.693147$ and the log of 2 is $0.30103$. Why?",,['derivatives']
60,Algebraic issues with the calculation of the second derivative of $(a+be^x)/(ae^x+b)$,Algebraic issues with the calculation of the second derivative of,(a+be^x)/(ae^x+b),"I'm trying to work out the 2nd derivative of $\dfrac{a+be^x}{ae^x+b}$ I have $f''=\dfrac{(ae^x+b)^2(b^2-a^2)e^x-2ae^x(ae^x+b)(b^2-a^2)e^x}{(ae^x+b)^4}$ There are so many terms, and I'm seriously confused on how to cancel it down. The mark scheme says I should expect: $\dfrac{(b^2-a^2)(b-ae^x)e^x}{(ae^x+b)^3}$ How do I get from my working to the answer?","I'm trying to work out the 2nd derivative of $\dfrac{a+be^x}{ae^x+b}$ I have $f''=\dfrac{(ae^x+b)^2(b^2-a^2)e^x-2ae^x(ae^x+b)(b^2-a^2)e^x}{(ae^x+b)^4}$ There are so many terms, and I'm seriously confused on how to cancel it down. The mark scheme says I should expect: $\dfrac{(b^2-a^2)(b-ae^x)e^x}{(ae^x+b)^3}$ How do I get from my working to the answer?",,"['calculus', 'algebra-precalculus', 'derivatives', 'exponential-function']"
61,"If $f$ is continuous on $[a,b)$ and differentiable on $(a,b)$ such that $\lim_{x\to b^{-}}f(x)=\infty$, Then $f'$ is not bounded above in $(a,b)$.","If  is continuous on  and differentiable on  such that , Then  is not bounded above in .","f [a,b) (a,b) \lim_{x\to b^{-}}f(x)=\infty f' (a,b)","I got this problem: Let $f$ be a continuous function on the interval $[a,b)$ and differentiable on the interval $(a,b)$, Prove that if $\lim_{x\to b^{-}}f(x)=\infty$, Then $f'$ is not bounded above in $(a,b)$. I tried some ways but none led me to a solution. Any help will be appreciated.","I got this problem: Let $f$ be a continuous function on the interval $[a,b)$ and differentiable on the interval $(a,b)$, Prove that if $\lim_{x\to b^{-}}f(x)=\infty$, Then $f'$ is not bounded above in $(a,b)$. I tried some ways but none led me to a solution. Any help will be appreciated.",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
62,Proving the correctness of alternative derivative definition,Proving the correctness of alternative derivative definition,,"We know that the derivative exists as $f'(x)=\lim_{h \to 0}\dfrac{f(x+h)-f(x)}{h}$. Using this, how can we prove that it is $f'(x)=\lim_{h \to 0}\dfrac{f(x)-f(x-h)}{h}$ as well? I tried the definition of the limit: If it exists, the both one sided limits do exists and equal to it as well. $$f'(x)=\lim_{h \to 0}\dfrac{f(x+h)-f(x)}{h} = \lim_{h \to 0^+}\dfrac{f(x+h)-f(x)}{h} =\lim_{h \to 0^-}\dfrac{f(x+h)-f(x)}{h}$$ I think that this should easily lead to the proof, but somehow I am missing it.","We know that the derivative exists as $f'(x)=\lim_{h \to 0}\dfrac{f(x+h)-f(x)}{h}$. Using this, how can we prove that it is $f'(x)=\lim_{h \to 0}\dfrac{f(x)-f(x-h)}{h}$ as well? I tried the definition of the limit: If it exists, the both one sided limits do exists and equal to it as well. $$f'(x)=\lim_{h \to 0}\dfrac{f(x+h)-f(x)}{h} = \lim_{h \to 0^+}\dfrac{f(x+h)-f(x)}{h} =\lim_{h \to 0^-}\dfrac{f(x+h)-f(x)}{h}$$ I think that this should easily lead to the proof, but somehow I am missing it.",,"['calculus', 'limits', 'derivatives']"
63,Issue differentiating the Lambert W function,Issue differentiating the Lambert W function,,"I want to differentiate the Lambert W function (the inverse of $y = xe^x$), I didn't think it would be that difficult a problem but it's causing me some problems. I tried this method: (1.) Implicitly differentiating $f(g(x)) = x$ and solving for $g'(x)$ yields $g'(x) = \frac{1}{f'(g(x))}$, so substituting $f = xe^x$ and $g = W(x)$ gives us $W'(x) = \frac{1}{x + e^{W(x)}}$. I then got paranoid and tried a second method, (2.) Implicitly differentiating $W(x)e^{W(x)} = x$ directly gives us $W'(x)e^{W(x)} + W(x)e^{W(x)}W'(x) = 1$, or $W'(x)(e^{W(x)} + x) = 1$. Solving for $W'$ gives us the exact same answer as (1.) My issue arises from the fact that WolframAlpha tells me that $W'(x) = \frac{W(x)}{xW(x) + x}$, which is nothing like what I got. What is wrong with my method?","I want to differentiate the Lambert W function (the inverse of $y = xe^x$), I didn't think it would be that difficult a problem but it's causing me some problems. I tried this method: (1.) Implicitly differentiating $f(g(x)) = x$ and solving for $g'(x)$ yields $g'(x) = \frac{1}{f'(g(x))}$, so substituting $f = xe^x$ and $g = W(x)$ gives us $W'(x) = \frac{1}{x + e^{W(x)}}$. I then got paranoid and tried a second method, (2.) Implicitly differentiating $W(x)e^{W(x)} = x$ directly gives us $W'(x)e^{W(x)} + W(x)e^{W(x)}W'(x) = 1$, or $W'(x)(e^{W(x)} + x) = 1$. Solving for $W'$ gives us the exact same answer as (1.) My issue arises from the fact that WolframAlpha tells me that $W'(x) = \frac{W(x)}{xW(x) + x}$, which is nothing like what I got. What is wrong with my method?",,['derivatives']
64,Find the Derivative,Find the Derivative,,"I'm currently studying the product rule and have come across a section of questions that seems to make no sense. I'm sure there's just one little thing that I'm missing but I am unable to spot it. Anyhow, I was hoping someone could show me step-by-step how to solve the following, and hopefully I can get the rest: Differentiate $(x^2 - 1)(x^3 - 1)$. You may need both the chain rule and the product rule Thanks in advance","I'm currently studying the product rule and have come across a section of questions that seems to make no sense. I'm sure there's just one little thing that I'm missing but I am unable to spot it. Anyhow, I was hoping someone could show me step-by-step how to solve the following, and hopefully I can get the rest: Differentiate $(x^2 - 1)(x^3 - 1)$. You may need both the chain rule and the product rule Thanks in advance",,"['calculus', 'derivatives']"
65,Find volume of cask,Find volume of cask,,"I was given the following question: A wine cask has a radius at the top of $30 cm$ and a radius at the middle of $40 cm$. The height of the cask is $1m$. What is the volume of the cask in litres, assuming the shape of the side is parabolic? I have to come to parabolic function of $$y = \frac{-1}{250}(x-50)^2+40$$ The derivative of $y$ is: $$\frac{dy}{dx} = \frac{2}{5} - \frac{x}{125}$$ Then I integrate and end up with an expression the length of $\pi$. Am I on the right track?","I was given the following question: A wine cask has a radius at the top of $30 cm$ and a radius at the middle of $40 cm$. The height of the cask is $1m$. What is the volume of the cask in litres, assuming the shape of the side is parabolic? I have to come to parabolic function of $$y = \frac{-1}{250}(x-50)^2+40$$ The derivative of $y$ is: $$\frac{dy}{dx} = \frac{2}{5} - \frac{x}{125}$$ Then I integrate and end up with an expression the length of $\pi$. Am I on the right track?",,"['calculus', 'integration', 'derivatives']"
66,Show that the approximation to $f$'($x_0$) has discretization error $O$($h^2$),Show that the approximation to '() has discretization error (),f x_0 O h^2,"We Define 3 grid points $x_{-1}$, $x_0$, $x_1$ with $x_{-1}=x_0-h$ and $x_{1} = x_0 + h$ with $h$ > 0. Given a smooth function f, show that the approximation to $f'(x_0)$ given by the centered differences: $$f[x_{-1}, x_1] = \frac { f({ x }_{ 1 })-f({ x }_{ -1 }) }{ { x }_{ 1 }-{ x }_{ -1 } } $$ has discretization error $O(h^2)$ I know that performing a Taylor expansion of $f(x_1)$ and $f(x_{-1})$ at $x_0$ would be a good start here but I do not proceed with that. Could anyone show me how to solve this problem?","We Define 3 grid points $x_{-1}$, $x_0$, $x_1$ with $x_{-1}=x_0-h$ and $x_{1} = x_0 + h$ with $h$ > 0. Given a smooth function f, show that the approximation to $f'(x_0)$ given by the centered differences: $$f[x_{-1}, x_1] = \frac { f({ x }_{ 1 })-f({ x }_{ -1 }) }{ { x }_{ 1 }-{ x }_{ -1 } } $$ has discretization error $O(h^2)$ I know that performing a Taylor expansion of $f(x_1)$ and $f(x_{-1})$ at $x_0$ would be a good start here but I do not proceed with that. Could anyone show me how to solve this problem?",,"['derivatives', 'taylor-expansion', 'approximation']"
67,Prove that $f'(x_o) =0$,Prove that,f'(x_o) =0,"Let $f$ be a function defined on an interval $I$  differentiable at a point $x_o$ in the interior of $I$. Prove that if $\exists  a>0$   $ \ [x_o -a, x_o+a] \subset I$ and  $ \ \forall x \in [x_o -a, x_o+a]  \ \ f(x) \leq f(x_o)$, then $f'(x_o)=0$. I did it as follows: Let b>0. Since $f$ is differentiable at $x_o$, $$ \exists a_o>0 \ \ \text{s.t} \ \ \forall x \in I \ \ \ \ \  0<|x-x_o|<a_o \implies \left| \frac{f(x)-f(x_o)}{x-x_0} - f'(x_o)\right| <b$$ Let $x_1 \in (x_o,x_o+a) \forall x \in I; f(x_1) \leq f(x_o)$ $$ \left| \frac{f(x_1)-f(x_o)}{x_1-x_0} - f'(x_o)\right| <b \\ -b < f'(x_o)-\frac{f(x_1)-f(x_o)}{x_1-x_0} <b \\ f'(x_o) < b+ \frac{f(x_1)-f(x_o)}{x_1-x_0} < b$$ $$f'(x_o) < b \tag{1} $$ Similarly Let $x_2 \in (x_o-a,x_o) \forall x \in I; f(x_2) \leq f(x_o)$ $$ \left| \frac{f(x_2)-f(x_o)}{x_2-x_0} - f'(x_o)\right| <b \\ -b < \frac{f(x_2)-f(x_o)}{x_2-x_0} - f'(x_o) <b \\ -b< -b + \frac{f(x_2)-f(x_o)}{x_2-x_0} < f'(x_o)$$ $$-b<f'(x_o)  \tag{2} $$ From $(1)$ and $(2)$, $$ -b < f'(x_o) <b \\ |f'(x_o)|<b  $$ I'm stuck here, how can I go to $f'(x_o)=0$ from here? Any help?","Let $f$ be a function defined on an interval $I$  differentiable at a point $x_o$ in the interior of $I$. Prove that if $\exists  a>0$   $ \ [x_o -a, x_o+a] \subset I$ and  $ \ \forall x \in [x_o -a, x_o+a]  \ \ f(x) \leq f(x_o)$, then $f'(x_o)=0$. I did it as follows: Let b>0. Since $f$ is differentiable at $x_o$, $$ \exists a_o>0 \ \ \text{s.t} \ \ \forall x \in I \ \ \ \ \  0<|x-x_o|<a_o \implies \left| \frac{f(x)-f(x_o)}{x-x_0} - f'(x_o)\right| <b$$ Let $x_1 \in (x_o,x_o+a) \forall x \in I; f(x_1) \leq f(x_o)$ $$ \left| \frac{f(x_1)-f(x_o)}{x_1-x_0} - f'(x_o)\right| <b \\ -b < f'(x_o)-\frac{f(x_1)-f(x_o)}{x_1-x_0} <b \\ f'(x_o) < b+ \frac{f(x_1)-f(x_o)}{x_1-x_0} < b$$ $$f'(x_o) < b \tag{1} $$ Similarly Let $x_2 \in (x_o-a,x_o) \forall x \in I; f(x_2) \leq f(x_o)$ $$ \left| \frac{f(x_2)-f(x_o)}{x_2-x_0} - f'(x_o)\right| <b \\ -b < \frac{f(x_2)-f(x_o)}{x_2-x_0} - f'(x_o) <b \\ -b< -b + \frac{f(x_2)-f(x_o)}{x_2-x_0} < f'(x_o)$$ $$-b<f'(x_o)  \tag{2} $$ From $(1)$ and $(2)$, $$ -b < f'(x_o) <b \\ |f'(x_o)|<b  $$ I'm stuck here, how can I go to $f'(x_o)=0$ from here? Any help?",,['derivatives']
68,"Differentiating $\frac{t \, e^{\tan t}}{\ln(3t+1)}$?",Differentiating ?,"\frac{t \, e^{\tan t}}{\ln(3t+1)}","I've tried to differentiate the following function: $$f(t)=\frac{t \, e^{\tan (t)}}{\ln(3t+1)}$$ But I am confused at what I should do (and perhaps I forgot some identities too), I've learned the rudiments of differential calculus, I've learned about the sum/product/division of derivatives and the chain rule. My understanding of these operations is that they are tools that allow one to decompose complicated derivatives in various pieces so one can derive the simplest derivatives one by one. I've tried the following: Using the chain rule, I guess that I should differentiate $e^{\tan (t)}$ first, the derivative of $f(x)=e^x$ is $f'(x)=e^x$ , but I got confused at  how to proceed later, I've differentiated part of the $\tan x$ in a similar fashion to this . At the moment, my guess is that I should finishing doing the derivative of $\tan (x)$ , which is $\sec ^2(x)$ then apply the chain rule, obtaining: $$e^{\tan (t)}\sec^2(t)$$ Now I guess I should use the product's rule to multiply $t$ by $e^{\tan (t)}\sec^2(t)$ , obtaining: $$1 \cdot e^{\tan (t)}+t\cdot e^{\tan (t)}\sec^2(t)$$ And now I guess I should use the quotient rule on: $$\frac{1 \cdot e^{\tan (t)}+t\cdot e^{\tan (t)}\sec^2(t)}{\ln(3t+1)}$$ But I feel this is getting too lengthy and that perhaps I'm doing something wrong. Is my reasoning until this point correct?","I've tried to differentiate the following function: But I am confused at what I should do (and perhaps I forgot some identities too), I've learned the rudiments of differential calculus, I've learned about the sum/product/division of derivatives and the chain rule. My understanding of these operations is that they are tools that allow one to decompose complicated derivatives in various pieces so one can derive the simplest derivatives one by one. I've tried the following: Using the chain rule, I guess that I should differentiate first, the derivative of is , but I got confused at  how to proceed later, I've differentiated part of the in a similar fashion to this . At the moment, my guess is that I should finishing doing the derivative of , which is then apply the chain rule, obtaining: Now I guess I should use the product's rule to multiply by , obtaining: And now I guess I should use the quotient rule on: But I feel this is getting too lengthy and that perhaps I'm doing something wrong. Is my reasoning until this point correct?","f(t)=\frac{t \, e^{\tan (t)}}{\ln(3t+1)} e^{\tan (t)} f(x)=e^x f'(x)=e^x \tan x \tan (x) \sec ^2(x) e^{\tan (t)}\sec^2(t) t e^{\tan (t)}\sec^2(t) 1 \cdot e^{\tan (t)}+t\cdot e^{\tan (t)}\sec^2(t) \frac{1 \cdot e^{\tan (t)}+t\cdot e^{\tan (t)}\sec^2(t)}{\ln(3t+1)}","['calculus', 'derivatives']"
69,Variation on the Cauchy mean value theorem,Variation on the Cauchy mean value theorem,,"From Spivak's Calculus , 4th edition, problem 11-50: Prove that if $f$ and $g$ are continuous on $[a,b]$ and differentiable   on $(a,b)$, and $g'(x)\neq 0$ for $x$ in $(a,b)$, then there is some   $x$ in $(a,b)$ with $$\frac{f'(x)} {g'(x)} = \frac {f(x) - f(a)}  {g(b)-g(x)}.$$ Hint : Multiply out first, to see what this really   says. I've struggled with this one for a while, to no avail. I'm sure it's just something small I'm missing.","From Spivak's Calculus , 4th edition, problem 11-50: Prove that if $f$ and $g$ are continuous on $[a,b]$ and differentiable   on $(a,b)$, and $g'(x)\neq 0$ for $x$ in $(a,b)$, then there is some   $x$ in $(a,b)$ with $$\frac{f'(x)} {g'(x)} = \frac {f(x) - f(a)}  {g(b)-g(x)}.$$ Hint : Multiply out first, to see what this really   says. I've struggled with this one for a while, to no avail. I'm sure it's just something small I'm missing.",,"['calculus', 'derivatives']"
70,"Find the value of derivative, given that the tangent line passes through a particular point","Find the value of derivative, given that the tangent line passes through a particular point",,"If the line tangent to the graph of the function $f$ at the point $(2,7)$ passes through the point $(-3,-3)$ then $f'(2)$ is...? A. 5 B.1 C. 2 D.7 E. Undefined I don't understand how to do this. I know that I can find the slope of the line tangent to the point $(2,7)$ but after I find that equation of the line how do I find $f'(2)$?","If the line tangent to the graph of the function $f$ at the point $(2,7)$ passes through the point $(-3,-3)$ then $f'(2)$ is...? A. 5 B.1 C. 2 D.7 E. Undefined I don't understand how to do this. I know that I can find the slope of the line tangent to the point $(2,7)$ but after I find that equation of the line how do I find $f'(2)$?",,"['calculus', 'derivatives']"
71,"Prove $\sqrt{x}>\ln(x)$ in $[1,\infty)$",Prove  in,"\sqrt{x}>\ln(x) [1,\infty)","Well, i try to prove this statement. i choose to make function: $f\left(x\right)\:=\:\sqrt{x}-\ln x$ but the derivative is: $\dfrac{\sqrt{x}\:-\:2}{2\sqrt{x}}$ and it's not always greater than $ 0$. any ideas?","Well, i try to prove this statement. i choose to make function: $f\left(x\right)\:=\:\sqrt{x}-\ln x$ but the derivative is: $\dfrac{\sqrt{x}\:-\:2}{2\sqrt{x}}$ and it's not always greater than $ 0$. any ideas?",,"['calculus', 'derivatives']"
72,Trying to solve a Taylor series problem,Trying to solve a Taylor series problem,,"I have a Taylor series problem, well more precisely a Maclaurin series. I am trying to find convergence of: $f(x) = e^{x^3} + e^{{2x}^3}$ Okay here goes: $$f'(x) = 3xe^{x^3} + 6x e^{{2x}^3}$$ $$f''(x) = 9x^2e^{x^3} + 3e^{x^3} + 36x^2e^{{2x}^3} + 6e^{{2x}^3}=e^{x^3}(9x^2+3) + e^{{2x}^3}(36x^2+6)$$ $$f'''(x) = (9x^2+3)(3xe^{x^3}) + 18xe^{x^3} + (36x^2 + 6)(6xe^{2x^3}) + 72xe^{2x^3}$$ $$=27xe^{x^3}(x^2 + 1) + 108xe^{2{x^3}}(2x^2 + 1)$$ Now I don't see a pattern, but I can note at $a=0$ we have: $$f(0)=2$$ $$f'(0)=0$$ $$f''(0)=9$$ $$f'''(0)=0$$ I am not sure where this is going, or if this is the right way to attack the problem. Any advice?","I have a Taylor series problem, well more precisely a Maclaurin series. I am trying to find convergence of: $f(x) = e^{x^3} + e^{{2x}^3}$ Okay here goes: $$f'(x) = 3xe^{x^3} + 6x e^{{2x}^3}$$ $$f''(x) = 9x^2e^{x^3} + 3e^{x^3} + 36x^2e^{{2x}^3} + 6e^{{2x}^3}=e^{x^3}(9x^2+3) + e^{{2x}^3}(36x^2+6)$$ $$f'''(x) = (9x^2+3)(3xe^{x^3}) + 18xe^{x^3} + (36x^2 + 6)(6xe^{2x^3}) + 72xe^{2x^3}$$ $$=27xe^{x^3}(x^2 + 1) + 108xe^{2{x^3}}(2x^2 + 1)$$ Now I don't see a pattern, but I can note at $a=0$ we have: $$f(0)=2$$ $$f'(0)=0$$ $$f''(0)=9$$ $$f'''(0)=0$$ I am not sure where this is going, or if this is the right way to attack the problem. Any advice?",,"['calculus', 'real-analysis', 'derivatives', 'taylor-expansion']"
73,Finding the absolute minimum and maximum of a function,Finding the absolute minimum and maximum of a function,,"The function is  $$f(x)=x+\sin(2x)$$ I need to find the absolute maxima and minima of several different domains using this function. I have found that the derivative of this function is $$f'(x)=1+2\cos(2x)$$ Then, I set this derivative equal to zero and got $x=\dfrac{1}{2}\cos^{-1}(-\dfrac{1}{2})$. One of the domains was $[1,5]$ and I plugged into the x of the original function $1$, $\dfrac{1}{2}\cos^{-1}(-\dfrac{1}{2})$, $5$, and found out that $1$ was the minimum and $5$ was the maximum with the resulting values of $1.909297427$ and $4.455978889$, respectively. The resulting value of $x=\cos^{-1}(-\dfrac{1}{2})$ was $1.913222955$. But WebAssign says that $1$ and $5$ are wrong. I have applied the same concept to solving the answers to other intervals and got them all right. What could I be possibly doing wrong? Thanks.","The function is  $$f(x)=x+\sin(2x)$$ I need to find the absolute maxima and minima of several different domains using this function. I have found that the derivative of this function is $$f'(x)=1+2\cos(2x)$$ Then, I set this derivative equal to zero and got $x=\dfrac{1}{2}\cos^{-1}(-\dfrac{1}{2})$. One of the domains was $[1,5]$ and I plugged into the x of the original function $1$, $\dfrac{1}{2}\cos^{-1}(-\dfrac{1}{2})$, $5$, and found out that $1$ was the minimum and $5$ was the maximum with the resulting values of $1.909297427$ and $4.455978889$, respectively. The resulting value of $x=\cos^{-1}(-\dfrac{1}{2})$ was $1.913222955$. But WebAssign says that $1$ and $5$ are wrong. I have applied the same concept to solving the answers to other intervals and got them all right. What could I be possibly doing wrong? Thanks.",,"['derivatives', 'optimization']"
74,Show that function is partially differentiable,Show that function is partially differentiable,,"I have the following function: $$F: \mathbb{R}^2 \rightarrow \mathbb{R}, ~~ (x,y) \rightarrow xy\frac{x^2-y^2}{x^2+y^2}$$ for $(x,y) \ne 0$ and $F(0,0) = 0$. I want to show that $F$ is partially differentiable twice. So far I've figured out that for the function to be differentiable I have to show the differential quotient exists for both - $x$ and $y$. So starting with $x$ I'd treat $y$ as a constant and check for the limit: $$\lim_{h \rightarrow 0} \frac{F(x_0+h) -F(x_0)}{h} = y\frac{(x_0+h)}{h}\frac{(x_0+h)^2-y^2}{(x_0+h)^2+y^2} - \frac{x_0y}{h}\frac{x_0^2-y^2}{x_0^2+y^2}$$ And unfortunately that is where I am stuck at the moment - I don't know how to get rid of the $h$ after expanding - maybe I've done something wrong? Thanks for helping out. FunkyPeanut","I have the following function: $$F: \mathbb{R}^2 \rightarrow \mathbb{R}, ~~ (x,y) \rightarrow xy\frac{x^2-y^2}{x^2+y^2}$$ for $(x,y) \ne 0$ and $F(0,0) = 0$. I want to show that $F$ is partially differentiable twice. So far I've figured out that for the function to be differentiable I have to show the differential quotient exists for both - $x$ and $y$. So starting with $x$ I'd treat $y$ as a constant and check for the limit: $$\lim_{h \rightarrow 0} \frac{F(x_0+h) -F(x_0)}{h} = y\frac{(x_0+h)}{h}\frac{(x_0+h)^2-y^2}{(x_0+h)^2+y^2} - \frac{x_0y}{h}\frac{x_0^2-y^2}{x_0^2+y^2}$$ And unfortunately that is where I am stuck at the moment - I don't know how to get rid of the $h$ after expanding - maybe I've done something wrong? Thanks for helping out. FunkyPeanut",,"['real-analysis', 'analysis', 'limits', 'derivatives', 'partial-derivative']"
75,Dini Derivative [closed],Dini Derivative [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $f$ be defined on $\mathbb{R}$ such that $$ f(x) = \begin{cases} |x|, & \text{if }x \in \mathbb{Q} \\ |2x|, & \text{if }x \notin \mathbb{Q}  \end{cases} $$ Calculate $D^{+}f(0),D^{-}f(0),D_{+}f(0),D_{-}f(0)$ and $D^{+}f(1),D^{-}f(1),D_{+}f(1),D_{-}f(1)$.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $f$ be defined on $\mathbb{R}$ such that $$ f(x) = \begin{cases} |x|, & \text{if }x \in \mathbb{Q} \\ |2x|, & \text{if }x \notin \mathbb{Q}  \end{cases} $$ Calculate $D^{+}f(0),D^{-}f(0),D_{+}f(0),D_{-}f(0)$ and $D^{+}f(1),D^{-}f(1),D_{+}f(1),D_{-}f(1)$.",,"['real-analysis', 'analysis', 'derivatives', 'lebesgue-integral', 'lebesgue-measure']"
76,Calculating the derivative in terms of $y(x)$,Calculating the derivative in terms of,y(x),"It has been a few years since I have taken calculus, and I must have forgotten how to do this. I need to find the derivative of the Sigmoid function in terms of $y(x)$. I have found this page which shows the answer is $y(1-y)$. I know how to derive the final derivative using the derivative rules, but I don't know how to get  $y(1-y)$? I need to find the derivative in this form for several functions to be used in back propogation of a neural network. This is list of activation functions .","It has been a few years since I have taken calculus, and I must have forgotten how to do this. I need to find the derivative of the Sigmoid function in terms of $y(x)$. I have found this page which shows the answer is $y(1-y)$. I know how to derive the final derivative using the derivative rules, but I don't know how to get  $y(1-y)$? I need to find the derivative in this form for several functions to be used in back propogation of a neural network. This is list of activation functions .",,"['calculus', 'derivatives']"
77,"Differentiability at a point $(0,0)$",Differentiability at a point,"(0,0)","How would i show $$\frac{xy(x^2-y^2)}{(x^2+y^2)^{3/2}}$$ is not differentiable at $(0,0)$","How would i show $$\frac{xy(x^2-y^2)}{(x^2+y^2)^{3/2}}$$ is not differentiable at $(0,0)$",,['derivatives']
78,Properties of the function defined by $g(x) = \sum\limits_{n=0}^{\infty} \frac{1}{1+n^2x^2}$,Properties of the function defined by,g(x) = \sum\limits_{n=0}^{\infty} \frac{1}{1+n^2x^2},"I am looking at the function $g:\mathbb{R} \rightarrow \mathbb{R}$ defined as $$g(x) = \sum\limits_{n=0}^{\infty} \frac{1}{1+n^2x^2}$$ I would like to know if this function is convergent, continuous and differentiable. For convergence I have $g(0)$ is divergent because we get a sum of 1, but for $x \not = 0$ we get $$0< \sum\limits_{n=0}^{\infty} \frac{1}{1+n^2x^2} < \sum\limits_{n=0}^{\infty} \frac{1}{n^2x^2} = \frac{1}{x^2} \sum\limits_{n=0}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6x^2}$$ and since $g$ is monotonic and bounded it converges (right?) Since the pointwise limit doesn't exist for $x \not = 0$ then $g$ doesn't converge uniformly making proving continuity more difficult. For differentiability I know that if $g_m(x)$ (partial sum of g) is point-wise convergent and $g_m'(x)$ is uniformly convergent then $g'(x) = lim_{m \rightarrow \infty } g'_m(x)$ but again I can't use this due to pointwise limit not existing. For any of these properties could I look at $g(x)|_{(0,\infty)}$ and $g(x)|_{(-\infty,0)}$ and make conclusions from this? Any guidance or solutions would be helpful, Thanks.","I am looking at the function $g:\mathbb{R} \rightarrow \mathbb{R}$ defined as $$g(x) = \sum\limits_{n=0}^{\infty} \frac{1}{1+n^2x^2}$$ I would like to know if this function is convergent, continuous and differentiable. For convergence I have $g(0)$ is divergent because we get a sum of 1, but for $x \not = 0$ we get $$0< \sum\limits_{n=0}^{\infty} \frac{1}{1+n^2x^2} < \sum\limits_{n=0}^{\infty} \frac{1}{n^2x^2} = \frac{1}{x^2} \sum\limits_{n=0}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6x^2}$$ and since $g$ is monotonic and bounded it converges (right?) Since the pointwise limit doesn't exist for $x \not = 0$ then $g$ doesn't converge uniformly making proving continuity more difficult. For differentiability I know that if $g_m(x)$ (partial sum of g) is point-wise convergent and $g_m'(x)$ is uniformly convergent then $g'(x) = lim_{m \rightarrow \infty } g'_m(x)$ but again I can't use this due to pointwise limit not existing. For any of these properties could I look at $g(x)|_{(0,\infty)}$ and $g(x)|_{(-\infty,0)}$ and make conclusions from this? Any guidance or solutions would be helpful, Thanks.",,"['real-analysis', 'sequences-and-series']"
79,The Notation for Derivatives,The Notation for Derivatives,,""" The derivative of a sum is the sum of derivatives "" Above theorem can be mathematically expressed as: $$h'(x)=f'(x)+g'(x)$$ where $f(x)$ and $g(x)$ are two differentiable functions. What is the right way to express the statement of this theorem in Leibniz notation? Is it  $$\frac{d}{dx}h=\frac{d}{dx}(f+g)=\frac{d}{dx}f+\frac{d}{dx}g$$ OR $$\frac{d}{dx}h(x)=\frac{d}{dx}(f(x)+g(x))=\frac{d}{dx}f(x)+\frac{d}{dx}g(x)?$$ In other words, is it permissible to write the derivative of functions in such a way so as to express the variable(s) on which they depend?",""" The derivative of a sum is the sum of derivatives "" Above theorem can be mathematically expressed as: $$h'(x)=f'(x)+g'(x)$$ where $f(x)$ and $g(x)$ are two differentiable functions. What is the right way to express the statement of this theorem in Leibniz notation? Is it  $$\frac{d}{dx}h=\frac{d}{dx}(f+g)=\frac{d}{dx}f+\frac{d}{dx}g$$ OR $$\frac{d}{dx}h(x)=\frac{d}{dx}(f(x)+g(x))=\frac{d}{dx}f(x)+\frac{d}{dx}g(x)?$$ In other words, is it permissible to write the derivative of functions in such a way so as to express the variable(s) on which they depend?",,"['derivatives', 'notation']"
80,Definition of Derivative for $sgn(x)$,Definition of Derivative for,sgn(x),"When using the definition of the second derivative for $sgn(x)$, I'm  a little confused on evaluating something like $sgn(x+h)$. Since $h\rightarrow 0$ does that mean that I should treating $sgn(x+h)$ as $1$ when x is positive, or should I treat it as a $sgn(y)$ centered at $y=x+h$? I thought originally in order to evaluate something like $\frac{f(x-h)-2(x)+f(x+h)}{h^2}$ I would need to break up my evaluation over $x \lt -h, x \gt h, ...$ etc, but I think I got the wrong answer that way.","When using the definition of the second derivative for $sgn(x)$, I'm  a little confused on evaluating something like $sgn(x+h)$. Since $h\rightarrow 0$ does that mean that I should treating $sgn(x+h)$ as $1$ when x is positive, or should I treat it as a $sgn(y)$ centered at $y=x+h$? I thought originally in order to evaluate something like $\frac{f(x-h)-2(x)+f(x+h)}{h^2}$ I would need to break up my evaluation over $x \lt -h, x \gt h, ...$ etc, but I think I got the wrong answer that way.",,['derivatives']
81,Why are differential of $\sin^2(x)$ and integral of $\sin(2x)$ not the same?,Why are differential of  and integral of  not the same?,\sin^2(x) \sin(2x),I was working on a list of common integrals and differentials and I came across this question. If $${d\over d\theta}(\sin^2\theta) = \sin(2\theta)$$ Then why is $$\int \sin(2\theta) \space d\theta = -\frac12\cos(2\theta) + c$$ Isn't integration the opposite of differentiation?,I was working on a list of common integrals and differentials and I came across this question. If $${d\over d\theta}(\sin^2\theta) = \sin(2\theta)$$ Then why is $$\int \sin(2\theta) \space d\theta = -\frac12\cos(2\theta) + c$$ Isn't integration the opposite of differentiation?,,"['integration', 'trigonometry', 'derivatives']"
82,Differentiability of $x^\alpha \sin(x^{-\beta})$ at $x = 0$,Differentiability of  at,x^\alpha \sin(x^{-\beta}) x = 0,"\begin{align*} 	f(x) = \left\{\begin{array}{ll}        		0 &  \text{ if } x=0\\ 		x^\alpha \sin(x^{-\beta}) &  \text{ otherwise } 	\end{array}\right. 	\end{align*} Determine the values of $\alpha$ and $\beta$ for which this function is differentiable at $x=0$. I found the derivative, but I don't know what to do after...","\begin{align*} 	f(x) = \left\{\begin{array}{ll}        		0 &  \text{ if } x=0\\ 		x^\alpha \sin(x^{-\beta}) &  \text{ otherwise } 	\end{array}\right. 	\end{align*} Determine the values of $\alpha$ and $\beta$ for which this function is differentiable at $x=0$. I found the derivative, but I don't know what to do after...",,['derivatives']
83,Leibniz Notation Second Derivative Chain Rule?,Leibniz Notation Second Derivative Chain Rule?,,I believe I understand the chain rule better from a few tutorials as the following: $$\frac{d}{dx}(f(g(x)) ) = \frac{\partial f}{\partial g}\frac{\partial g}{\partial x}$$ But how would you represent equations that require the chain rule with leibniz notation for higher order derivatives. For example would the second derivative of a function requiring the chain rule be represented as follows: $$\frac{d^2}{dx^2}(f(g(x)) ) = \frac{\partial^2 f}{\partial g}\frac{\partial g}{\partial x^2}$$ Or will the product rule be required somewhere? Any information would be appreciated,I believe I understand the chain rule better from a few tutorials as the following: $$\frac{d}{dx}(f(g(x)) ) = \frac{\partial f}{\partial g}\frac{\partial g}{\partial x}$$ But how would you represent equations that require the chain rule with leibniz notation for higher order derivatives. For example would the second derivative of a function requiring the chain rule be represented as follows: $$\frac{d^2}{dx^2}(f(g(x)) ) = \frac{\partial^2 f}{\partial g}\frac{\partial g}{\partial x^2}$$ Or will the product rule be required somewhere? Any information would be appreciated,,"['calculus', 'derivatives']"
84,How to prove that $\frac{d}{dx}\sin(x)=\cos(x)$,How to prove that,\frac{d}{dx}\sin(x)=\cos(x),"I have to prove that $\dfrac{d}{dx}\sin(x)=\cos(x)$. I used the definition of a derivative: $$\dfrac{d}{dx}f(x)=\lim\limits_{h\to 0} \dfrac{f(x+h)-f(x)}{h}$$ $$\dfrac{d}{dx}\sin(x)=\lim\limits_{h\to 0} \dfrac{\sin(x+h)-\sin(x)}{h}$$ Using angle sum formula: $$\dfrac{d}{dx}\sin(x)=\lim\limits_{h\to 0} \dfrac{\sin (x)\cos (h)+\cos (x)\sin (h)-\sin (x)}{h}$$ Rearranging the terms: $$\dfrac{d}{dx}\sin(x)=\lim\limits_{h\to 0} \dfrac{\cos (h)\sin(h)-\sin(x)+\sin(x)\cos(h)}{h}$$ Factoring out $\sin(x)$ from the last two terms: $$\dfrac{d}{dx}\sin(x)=\lim\limits_{h\to 0} \dfrac{\cos(x)\sin(h)-\sin(x)[1-\cos(h)]}{h}$$ Separating the fraction: $$\dfrac{d}{dx}\sin(x)=\lim\limits_{h\to 0} \dfrac{\cos(x)\sin(h)}{h}-\lim\limits_{h\to 0} \dfrac{\sin(x)[1-\cos(h)]}{h}$$ Now I am stuck. Can anyone please tell me what to do next, or give me a hint? Thanks","I have to prove that $\dfrac{d}{dx}\sin(x)=\cos(x)$. I used the definition of a derivative: $$\dfrac{d}{dx}f(x)=\lim\limits_{h\to 0} \dfrac{f(x+h)-f(x)}{h}$$ $$\dfrac{d}{dx}\sin(x)=\lim\limits_{h\to 0} \dfrac{\sin(x+h)-\sin(x)}{h}$$ Using angle sum formula: $$\dfrac{d}{dx}\sin(x)=\lim\limits_{h\to 0} \dfrac{\sin (x)\cos (h)+\cos (x)\sin (h)-\sin (x)}{h}$$ Rearranging the terms: $$\dfrac{d}{dx}\sin(x)=\lim\limits_{h\to 0} \dfrac{\cos (h)\sin(h)-\sin(x)+\sin(x)\cos(h)}{h}$$ Factoring out $\sin(x)$ from the last two terms: $$\dfrac{d}{dx}\sin(x)=\lim\limits_{h\to 0} \dfrac{\cos(x)\sin(h)-\sin(x)[1-\cos(h)]}{h}$$ Separating the fraction: $$\dfrac{d}{dx}\sin(x)=\lim\limits_{h\to 0} \dfrac{\cos(x)\sin(h)}{h}-\lim\limits_{h\to 0} \dfrac{\sin(x)[1-\cos(h)]}{h}$$ Now I am stuck. Can anyone please tell me what to do next, or give me a hint? Thanks",,"['limits', 'trigonometry', 'derivatives']"
85,Prove that $ \int \limits_a^b f(x) dx$ = $ \int \limits_a^b f(a+b-x) dx$,Prove that  =, \int \limits_a^b f(x) dx  \int \limits_a^b f(a+b-x) dx,Hi everyone I have been trying to prove that that $ \int \limits_a^b f(x) dx$  = $ \int \limits_a^b f(a+b-x) dx$ .  Heres my attempt: LS: $ \int \limits_a^b f(x) dx$  = $ \int \limits f(b) - \int \limits f(a) $ RS: $ \int \limits_a^b f(a+b-x) dx$ = $ \int \limits f(a+b-b) - \int \limits f(a+b-a) $ = $ \int \limits f(a) - \int \limits f(b) $ as you can see I have really not been able to make the 2 sides equal each other. Any help on this i appreciated! Thank you,Hi everyone I have been trying to prove that that $ \int \limits_a^b f(x) dx$  = $ \int \limits_a^b f(a+b-x) dx$ .  Heres my attempt: LS: $ \int \limits_a^b f(x) dx$  = $ \int \limits f(b) - \int \limits f(a) $ RS: $ \int \limits_a^b f(a+b-x) dx$ = $ \int \limits f(a+b-b) - \int \limits f(a+b-a) $ = $ \int \limits f(a) - \int \limits f(b) $ as you can see I have really not been able to make the 2 sides equal each other. Any help on this i appreciated! Thank you,,"['calculus', 'real-analysis', 'integration', 'derivatives', 'definite-integrals']"
86,How to take the Limits of Logs [duplicate],How to take the Limits of Logs [duplicate],,This question already has answers here : Limit of $\frac{\log(n!)}{n\log(n)}$ as $n\to\infty$. (11 answers) Closed 3 years ago . How would you take the limit of $$\frac{\log(n!)}{\log(n^n)}$$ as $n\rightarrow\infty$. I believe you have to remove the log raising it to their base. Is this correct ? Thanks.,This question already has answers here : Limit of $\frac{\log(n!)}{n\log(n)}$ as $n\to\infty$. (11 answers) Closed 3 years ago . How would you take the limit of $$\frac{\log(n!)}{\log(n^n)}$$ as $n\rightarrow\infty$. I believe you have to remove the log raising it to their base. Is this correct ? Thanks.,,"['limits', 'derivatives', 'logarithms']"
87,Finding $y^{\prime \prime}$ of $2x^2+3y^2=4$,Finding  of,y^{\prime \prime} 2x^2+3y^2=4,Find $y^{\prime}$ and $y^{\prime \prime}$ of $2x^2+3y^2=4$ $$y^{\prime}=\dfrac{d}{dx}(2x^2)+\dfrac{d}{dx}(3y^2)=\dfrac{d}{dx}(4)$$ $$4x+6yy^{\prime}=0$$ $$y^{\prime}=\dfrac{-2x}{3y}$$ This is how I started finding $y^{\prime\prime}$: $$y^{\prime\prime}=\dfrac{3y \dfrac{d}{dx}(-2x) - (-2x)\dfrac{d}{dx}(3y)}{(3y^2)}$$ $$\dfrac{3y(-2)-[-2x(3y^{\prime})]}{9y^2}$$ $$\dfrac{-6y-[-6xy^{\prime}]}{9y^2}$$ This isn't right since the correct answer is $\dfrac{-6y^2+4x^2}{9y^3}$ Can you please show how to find $y^{\prime\prime}?$ Thank you.,Find $y^{\prime}$ and $y^{\prime \prime}$ of $2x^2+3y^2=4$ $$y^{\prime}=\dfrac{d}{dx}(2x^2)+\dfrac{d}{dx}(3y^2)=\dfrac{d}{dx}(4)$$ $$4x+6yy^{\prime}=0$$ $$y^{\prime}=\dfrac{-2x}{3y}$$ This is how I started finding $y^{\prime\prime}$: $$y^{\prime\prime}=\dfrac{3y \dfrac{d}{dx}(-2x) - (-2x)\dfrac{d}{dx}(3y)}{(3y^2)}$$ $$\dfrac{3y(-2)-[-2x(3y^{\prime})]}{9y^2}$$ $$\dfrac{-6y-[-6xy^{\prime}]}{9y^2}$$ This isn't right since the correct answer is $\dfrac{-6y^2+4x^2}{9y^3}$ Can you please show how to find $y^{\prime\prime}?$ Thank you.,,"['calculus', 'derivatives']"
88,Limits and derivatives - two questions,Limits and derivatives - two questions,,"I was asked to find two limits. Let $f$ be differentiable function at $x=1$ and $f(1)>0$. $$\lim_{n \rightarrow \infty}\left(\frac{f\left(1+\frac{1}{n}\right )}{f(1)} \right)^{\frac{1}{n}}$$ Let $\frac{1}{n}=h$ so the limit becomes $$\lim_{h \rightarrow 0}\left(\frac{f\left(1+h \right)}{f(1)} \right)^h=\lim_{h \rightarrow 0} \left(\frac{f(1+h)-f(1)}{f(1)}+1 \right)^h$$ How may I continue? Same for $\lim_{x \rightarrow 1} \left(\frac{f(x)}{f(1)} \right)^{\frac{1}{\log(x)}}$. I defined $h=\log(x)$ and tried to continue with no luck. Please help, thank you!","I was asked to find two limits. Let $f$ be differentiable function at $x=1$ and $f(1)>0$. $$\lim_{n \rightarrow \infty}\left(\frac{f\left(1+\frac{1}{n}\right )}{f(1)} \right)^{\frac{1}{n}}$$ Let $\frac{1}{n}=h$ so the limit becomes $$\lim_{h \rightarrow 0}\left(\frac{f\left(1+h \right)}{f(1)} \right)^h=\lim_{h \rightarrow 0} \left(\frac{f(1+h)-f(1)}{f(1)}+1 \right)^h$$ How may I continue? Same for $\lim_{x \rightarrow 1} \left(\frac{f(x)}{f(1)} \right)^{\frac{1}{\log(x)}}$. I defined $h=\log(x)$ and tried to continue with no luck. Please help, thank you!",,"['calculus', 'limits', 'derivatives']"
89,On the nature of a first derivative,On the nature of a first derivative,,"This is a very, very basic question. Never been very involved in math but I've been learning calculus in my free time, so here goes. I have observed some various things that happen with derivatives, but I don't know what it's telling me. I see that if I find the slope at two x-values with a derivative, then the difference in their slopes is the same as if I plugged the difference between the two x-values into the derivative. But what does the slope itself tell me? If my x value is 8, and f'(x) is 2x, then what does my slope of 16 at that point tell me? The slope at 7.99998 is going to be .00004 away from 16, but what do those two slopes actually tell me? I guess I'm having a hard time understanding the usefulness of the value given when you plug in x-values in a slope, what does the quantity (the slope) actually mean? Thanks for taking time to read this. I understand this board has lots of really juicy questions and this is hardly worth your time but I really appreciate the help!","This is a very, very basic question. Never been very involved in math but I've been learning calculus in my free time, so here goes. I have observed some various things that happen with derivatives, but I don't know what it's telling me. I see that if I find the slope at two x-values with a derivative, then the difference in their slopes is the same as if I plugged the difference between the two x-values into the derivative. But what does the slope itself tell me? If my x value is 8, and f'(x) is 2x, then what does my slope of 16 at that point tell me? The slope at 7.99998 is going to be .00004 away from 16, but what do those two slopes actually tell me? I guess I'm having a hard time understanding the usefulness of the value given when you plug in x-values in a slope, what does the quantity (the slope) actually mean? Thanks for taking time to read this. I understand this board has lots of really juicy questions and this is hardly worth your time but I really appreciate the help!",,"['calculus', 'derivatives', 'education']"
90,1st derivatives of $f(\alpha) = \frac{\sin(2\alpha)}{\sin(\alpha+1)}$,1st derivatives of,f(\alpha) = \frac{\sin(2\alpha)}{\sin(\alpha+1)},Could someone help me out with the following? I have to get a maximum using the derivative $$f(\alpha) = \frac{\sin(2\alpha)}{\sin(\alpha+1)}$$ $$f(\alpha) = \sin(2\alpha) \cdot (\sin(\alpha+1))^{-1}$$ $$f'(\alpha) = \sin(2\alpha) \cdot ((\sin(\alpha+1))^{-1})' + (\sin(2\alpha))' \cdot (\sin(\alpha+1))^{-1}$$ $$f'(\alpha) = \sin(2\alpha) \cdot \color{red}{\cdots} + 2 \cos(2\alpha) \cdot (\sin(\alpha+1))^{-1}$$ I can't get any furher then this,Could someone help me out with the following? I have to get a maximum using the derivative $$f(\alpha) = \frac{\sin(2\alpha)}{\sin(\alpha+1)}$$ $$f(\alpha) = \sin(2\alpha) \cdot (\sin(\alpha+1))^{-1}$$ $$f'(\alpha) = \sin(2\alpha) \cdot ((\sin(\alpha+1))^{-1})' + (\sin(2\alpha))' \cdot (\sin(\alpha+1))^{-1}$$ $$f'(\alpha) = \sin(2\alpha) \cdot \color{red}{\cdots} + 2 \cos(2\alpha) \cdot (\sin(\alpha+1))^{-1}$$ I can't get any furher then this,,"['algebra-precalculus', 'derivatives']"
91,Extremal problem,Extremal problem,,"The task is to calculate $a'$ of a square (you cut out) if the volume of a cube is maximum. (you cut out white squares and put together grey squares so you get a cube without a cover/cap) I don't know what I'm doing wrong, but when I try to calculate critical points I always get $0$, and there isn't even a $\max$ but a $\min$! $V={a'}^3$ $\implies V'=3{a'}^2$ $\Rightarrow \text{critical points}: 0=3{a'}^2$  $\implies a'=0 (?!)$ The solution is $\dfrac{a}{6}$.","The task is to calculate $a'$ of a square (you cut out) if the volume of a cube is maximum. (you cut out white squares and put together grey squares so you get a cube without a cover/cap) I don't know what I'm doing wrong, but when I try to calculate critical points I always get $0$, and there isn't even a $\max$ but a $\min$! $V={a'}^3$ $\implies V'=3{a'}^2$ $\Rightarrow \text{critical points}: 0=3{a'}^2$  $\implies a'=0 (?!)$ The solution is $\dfrac{a}{6}$.",,"['calculus', 'derivatives']"
92,Maximizing a product of factorials,Maximizing a product of factorials,,"I would like to maximize $n_1! n_2! \cdots n_k!$ under the constraint $n_1 + n_2 + \cdots + n_k = N$ and $n_i > 0$ for all $i$. Intuitively, I think the maximum occurs when all $n_i$ are $1$ except for one of them: $$(1!)( 1!) \cdots (1!)( (N-(k-1))!)$$ but I am unsure how to show this because I can't take the derivative of this kind of function. Could someone point me in the right direction? Thanks! Edit: perhaps induction on $k$ will work? If $k=2$, it is a little easier to see that the maximum of $n_1!n_2!=n_1! (N-n_1)!$ is $N!$ (how to rigorously show this still escapes me). Then it follows by induction, I believe.","I would like to maximize $n_1! n_2! \cdots n_k!$ under the constraint $n_1 + n_2 + \cdots + n_k = N$ and $n_i > 0$ for all $i$. Intuitively, I think the maximum occurs when all $n_i$ are $1$ except for one of them: $$(1!)( 1!) \cdots (1!)( (N-(k-1))!)$$ but I am unsure how to show this because I can't take the derivative of this kind of function. Could someone point me in the right direction? Thanks! Edit: perhaps induction on $k$ will work? If $k=2$, it is a little easier to see that the maximum of $n_1!n_2!=n_1! (N-n_1)!$ is $N!$ (how to rigorously show this still escapes me). Then it follows by induction, I believe.",,"['combinatorics', 'derivatives', 'optimization']"
93,Continuous but nowhere differentiable function on domain,Continuous but nowhere differentiable function on domain,,"I am trying to come up with an example of a function which is continuous on $[0,1]$ but nowhere differentiable but which is in a way simpler than Weierstrass function or ""more intuitive"" so to speak. Do you know any such example?","I am trying to come up with an example of a function which is continuous on $[0,1]$ but nowhere differentiable but which is in a way simpler than Weierstrass function or ""more intuitive"" so to speak. Do you know any such example?",,"['derivatives', 'continuity']"
94,Finding derivative of three variables,Finding derivative of three variables,,"Consider a box with dimensions x, y, and z. x is changing at a rate of 1 m/s, y at -2 m/s and z at 1 m/s. Find the rate that the volume, surface area and diagonal length ($s = \sqrt{x^2+y^2+z^2}$) are changing at the instant when $x = 4$, $y = 3$ and $z = 2$. I know I need to use to product rule. But my teacher didn't bother to teach it to us how to do it with three variables (ex. xyz for the volume). So I'm having trouble figuring out how to start this problem.","Consider a box with dimensions x, y, and z. x is changing at a rate of 1 m/s, y at -2 m/s and z at 1 m/s. Find the rate that the volume, surface area and diagonal length ($s = \sqrt{x^2+y^2+z^2}$) are changing at the instant when $x = 4$, $y = 3$ and $z = 2$. I know I need to use to product rule. But my teacher didn't bother to teach it to us how to do it with three variables (ex. xyz for the volume). So I'm having trouble figuring out how to start this problem.",,"['calculus', 'derivatives']"
95,Derivatives question help,Derivatives question help,,"The question is :Find the derivative of $f(x)=e^c + c^x$. Assume that c is a constant. Wouldn't $f'(x)=   ce^{c-1} + xc^{x-1}$.    It keeps saying this answer is incorrect, What am i doing wrong?","The question is :Find the derivative of $f(x)=e^c + c^x$. Assume that c is a constant. Wouldn't $f'(x)=   ce^{c-1} + xc^{x-1}$.    It keeps saying this answer is incorrect, What am i doing wrong?",,"['calculus', 'algebra-precalculus', 'derivatives', 'optimization']"
96,Derivative in interesting way,Derivative in interesting way,,"I am supposed to give a 15-20 minutes math lecture, where I am expecting around 20-30 people. The lecture is about derivative. Since this would be my first ""class"", I would appreciate any suggestions to how to make it interesting. Students could be bored with definitions, theorems, mathematical concepts that are told in abstract way, therefore my question is: How to tell a story about derivative in simple, interesting but also, mathematically based way? :) Any suggestions about mathematical questions, examples, fun problems related to this topic (which should grab their attention) are also welcome.","I am supposed to give a 15-20 minutes math lecture, where I am expecting around 20-30 people. The lecture is about derivative. Since this would be my first ""class"", I would appreciate any suggestions to how to make it interesting. Students could be bored with definitions, theorems, mathematical concepts that are told in abstract way, therefore my question is: How to tell a story about derivative in simple, interesting but also, mathematically based way? :) Any suggestions about mathematical questions, examples, fun problems related to this topic (which should grab their attention) are also welcome.",,"['calculus', 'reference-request', 'soft-question', 'derivatives', 'education']"
97,How prove this $f(x)$ is polynomial function,How prove this  is polynomial function,f(x),"we define $$f^{[n]}(x)=\lim_{h\to 0}\sum_{k=0}^{n}\binom{n}{k}\dfrac{(-1)^kf[(x+(n-2k)h]}{(2h)^n}$$   if $f(x)$ is continuous on $[a,b]$,and such $f^{[n]}(x)=0$ for all $x$. prove or disprove :$f(x)$ is  polynomial  function, My try: I find sometimes,and I find this following therom: (Schwarz,1870):If $F$ is continuous and $DF(x)=0$ for all $x$, then $F$ is a linear function where $$DF(x):=\lim_{h\to 0}\dfrac{F(x-h)-2F(x)+F(x+h)}{h^2}$$ and this solution can see: http://math.depaul.edu/mash/newharder.pdf I think my problem is true.But  I can't prove it,Thank you","we define $$f^{[n]}(x)=\lim_{h\to 0}\sum_{k=0}^{n}\binom{n}{k}\dfrac{(-1)^kf[(x+(n-2k)h]}{(2h)^n}$$   if $f(x)$ is continuous on $[a,b]$,and such $f^{[n]}(x)=0$ for all $x$. prove or disprove :$f(x)$ is  polynomial  function, My try: I find sometimes,and I find this following therom: (Schwarz,1870):If $F$ is continuous and $DF(x)=0$ for all $x$, then $F$ is a linear function where $$DF(x):=\lim_{h\to 0}\dfrac{F(x-h)-2F(x)+F(x+h)}{h^2}$$ and this solution can see: http://math.depaul.edu/mash/newharder.pdf I think my problem is true.But  I can't prove it,Thank you",,['analysis']
98,derivative of a summation with variable upper limit,derivative of a summation with variable upper limit,,"Is the following statement correct and if yes does it need to satisfy specific requirement to be correct: $${ d \over dt} \sum_{j=1}^{N(t)} f(t,j) = \sum_{j=1}^{N(t)} {df(t,j) \over dt} + f(t,N(t)) {dN(t) \over dt}$$","Is the following statement correct and if yes does it need to satisfy specific requirement to be correct: $${ d \over dt} \sum_{j=1}^{N(t)} f(t,j) = \sum_{j=1}^{N(t)} {df(t,j) \over dt} + f(t,N(t)) {dN(t) \over dt}$$",,"['derivatives', 'summation']"
99,Do monotone operators have positive Frechet derivatives?,Do monotone operators have positive Frechet derivatives?,,"If a scalar function $f\colon \mathbb R \to \mathbb R$ is monotone and differentiable, then $f'\geq 0$. Monotonicity is generalized for an operator $A\colon V \to V^*$, where $V$ is a Banach spaces with its dual space $V^*$, via: The operator $A\colon V \to V^*$ is called monotone if $$   \langle A(u) - A(v), u-v \rangle \geq 0,$$ for all $u$, $v \in V$. So my question is: Does monotonicity of $A$ imply, that the Frechet derivative of $A$ is positive, i.e. $\langle A'(w)v,v\rangle \geq 0$ for any $w \in V$ and for all $v\in V$. Any idea or reference is appreciated.","If a scalar function $f\colon \mathbb R \to \mathbb R$ is monotone and differentiable, then $f'\geq 0$. Monotonicity is generalized for an operator $A\colon V \to V^*$, where $V$ is a Banach spaces with its dual space $V^*$, via: The operator $A\colon V \to V^*$ is called monotone if $$   \langle A(u) - A(v), u-v \rangle \geq 0,$$ for all $u$, $v \in V$. So my question is: Does monotonicity of $A$ imply, that the Frechet derivative of $A$ is positive, i.e. $\langle A'(w)v,v\rangle \geq 0$ for any $w \in V$ and for all $v\in V$. Any idea or reference is appreciated.",,"['functional-analysis', 'derivatives', 'reference-request', 'banach-spaces', 'monotone-operator-theory']"
