,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why is matrix multiplication called 'multiplication' if it is non-commutative?,Why is matrix multiplication called 'multiplication' if it is non-commutative?,,"This question begins with the assumption that matrix multiplication was termed 'multiplication' as a form of comparison/parallel to multiplication of integers and real numbers. Why was matrix multiplication termed 'multiplication' if it does not share the commutativity that other methods of multiplication typically adhere to? And by extension, why was another term not adopted (such as 'matrix application') in respect to this lack of commutativity? Edit: To further clarify, it seems to me that the natural choice for the operation termed 'multiplication' performed with matrices ought to be the Hadamard Product given that it is a direct multiplication of the elements of the matrix (thus retaining the properties of multiplication of real numbers), and that the operation now termed "" Matrix Multiplication "" should have received a different name because it does not retain the properties exhibited by multiplication in more basic contexts. So perhaps my question would be better phrased as: ""How did an operation which does not have all the properties of multiplication on real numbers come to be termed 'multiplication'?"" or: ""Why is matrix multiplication called 'multiplication' when the operation it represents seems to have no analogy to multiplication of real numbers?""","This question begins with the assumption that matrix multiplication was termed 'multiplication' as a form of comparison/parallel to multiplication of integers and real numbers. Why was matrix multiplication termed 'multiplication' if it does not share the commutativity that other methods of multiplication typically adhere to? And by extension, why was another term not adopted (such as 'matrix application') in respect to this lack of commutativity? Edit: To further clarify, it seems to me that the natural choice for the operation termed 'multiplication' performed with matrices ought to be the Hadamard Product given that it is a direct multiplication of the elements of the matrix (thus retaining the properties of multiplication of real numbers), and that the operation now termed "" Matrix Multiplication "" should have received a different name because it does not retain the properties exhibited by multiplication in more basic contexts. So perhaps my question would be better phrased as: ""How did an operation which does not have all the properties of multiplication on real numbers come to be termed 'multiplication'?"" or: ""Why is matrix multiplication called 'multiplication' when the operation it represents seems to have no analogy to multiplication of real numbers?""",,"['matrices', 'terminology', 'math-history']"
1,Order of matrix and monic irreducible polynomial over finite field,Order of matrix and monic irreducible polynomial over finite field,,"I want to verify (and prove - in case it is true) the following proposition. Suppose $\mathbb{F}_p$ is a finite field and $m(x)$ is a monic irreducible polynomial over $\mathbb{F}_p$ with $\mathrm{deg}(m(x))=n$. If $\mathbf{A}$ is an $n \times n$ matrix over $\mathbb{F}_p$ whose characteristic polynomial is $m(x)$, then $\mathrm{ord}(\mathbf{A})=p^n-1$, where $\mathrm{ord}(\mathbf{A})$ denotes the least positive integer $k$ such that $\mathbf{A}^k = \mathbf{I}$, where $\mathbf{I}$ is the identity matrix. For example, in $\mathbb{F}_2$, $m(x)=x^3+x^2+1$ is a monic irreducible polynomial. The characteristic polynomial of matrix $\mathbf{A}=$ \begin{bmatrix}0&0&1\\1&0&0\\0&1&1\end{bmatrix} is equal to $m(x)$ and $\mathbf{A}$ satisfies $\mathrm{ord}(\mathbf{A})=7$. Note that $2^3-1=7$. I have already checked using SAGE mathematical software that the companion matrices of a monic irreducible polynomial have order $q^n-1$ for $q=2$ and $2\leq n \leq 21$. My question is, is the above proposition true in general? My attempt to prove the theorem is by using group isomorphism between $\mathbb{F}^{*}_p = (\mathbb{F}_p[x]/m(x))^{*}$, i.e. the non zero elements of the finite field, and the matrix group $S = \{\mathbf{A}^k | k \in \mathbb{Z}\}$. But a problem occurs, how to construct an isomorphism from $\mathbb{F}^{*}_p$ to $S$ ? I have tried the function $\theta: \alpha \mapsto \mathbf{A}$ where $\alpha$ is the primitive element in $\mathbb{F}^{*}_p$, but I can not complete the detail. I have read several references, this book, p. 65 by R. Ridl and H. Niedereitter and this article by W. P. Wardlaw. The proposition seems true, but I have not found any formal proof about it. ADDENDUM: Apparently, the proposition is not true in general. There is a counterexample in W. P. Wardlaw article for $m(x)=x^2+1$ in $\mathbb{F}_3$. (Credit to WimC ). However the condition is true for $m(x)=x^2+x+2$. So what is there any additional requirement for $m(x)$ in order to make the proposition true?","I want to verify (and prove - in case it is true) the following proposition. Suppose $\mathbb{F}_p$ is a finite field and $m(x)$ is a monic irreducible polynomial over $\mathbb{F}_p$ with $\mathrm{deg}(m(x))=n$. If $\mathbf{A}$ is an $n \times n$ matrix over $\mathbb{F}_p$ whose characteristic polynomial is $m(x)$, then $\mathrm{ord}(\mathbf{A})=p^n-1$, where $\mathrm{ord}(\mathbf{A})$ denotes the least positive integer $k$ such that $\mathbf{A}^k = \mathbf{I}$, where $\mathbf{I}$ is the identity matrix. For example, in $\mathbb{F}_2$, $m(x)=x^3+x^2+1$ is a monic irreducible polynomial. The characteristic polynomial of matrix $\mathbf{A}=$ \begin{bmatrix}0&0&1\\1&0&0\\0&1&1\end{bmatrix} is equal to $m(x)$ and $\mathbf{A}$ satisfies $\mathrm{ord}(\mathbf{A})=7$. Note that $2^3-1=7$. I have already checked using SAGE mathematical software that the companion matrices of a monic irreducible polynomial have order $q^n-1$ for $q=2$ and $2\leq n \leq 21$. My question is, is the above proposition true in general? My attempt to prove the theorem is by using group isomorphism between $\mathbb{F}^{*}_p = (\mathbb{F}_p[x]/m(x))^{*}$, i.e. the non zero elements of the finite field, and the matrix group $S = \{\mathbf{A}^k | k \in \mathbb{Z}\}$. But a problem occurs, how to construct an isomorphism from $\mathbb{F}^{*}_p$ to $S$ ? I have tried the function $\theta: \alpha \mapsto \mathbf{A}$ where $\alpha$ is the primitive element in $\mathbb{F}^{*}_p$, but I can not complete the detail. I have read several references, this book, p. 65 by R. Ridl and H. Niedereitter and this article by W. P. Wardlaw. The proposition seems true, but I have not found any formal proof about it. ADDENDUM: Apparently, the proposition is not true in general. There is a counterexample in W. P. Wardlaw article for $m(x)=x^2+1$ in $\mathbb{F}_3$. (Credit to WimC ). However the condition is true for $m(x)=x^2+x+2$. So what is there any additional requirement for $m(x)$ in order to make the proposition true?",,"['matrices', 'finite-fields', 'group-isomorphism']"
2,Existence of Jordan decomposition over finite field,Existence of Jordan decomposition over finite field,,"Prove that over finite field $\mathbb F$ exists additive Jordan-Chevalley decomposition: for all matrix $M$ there are semisimple matrix $M_{s}$ and nilpotent matrix $M_{n}$ such that $M=M_{s}+M_{n}$ . I proved that $M^n=M^k$ for some $n,k \in \mathbb N$ as $\mathbb F$ is finite. Then considering $M$ is inversible we get that $M^{n-k}=E$ . But, unfortunetly, polynomial $x^{n-k} - 1$ is not separable if $p|(n-k)$ . If $n-k$ is not even we have decomposition $M=E + (M-E)$ . In case $n-k$ is even $M-E$ is not nilpotent. And i got stucked. Also I proved it in cases characteristic polynomial $f$ of $M$ : all roots of $f$ belong $\mathbb F$ $f=gh$ , where all roots of $g$ belongs $\mathbb F$ and $h$ is irreducible.","Prove that over finite field exists additive Jordan-Chevalley decomposition: for all matrix there are semisimple matrix and nilpotent matrix such that . I proved that for some as is finite. Then considering is inversible we get that . But, unfortunetly, polynomial is not separable if . If is not even we have decomposition . In case is even is not nilpotent. And i got stucked. Also I proved it in cases characteristic polynomial of : all roots of belong , where all roots of belongs and is irreducible.","\mathbb F M M_{s} M_{n} M=M_{s}+M_{n} M^n=M^k n,k \in \mathbb N \mathbb F M M^{n-k}=E x^{n-k} - 1 p|(n-k) n-k M=E + (M-E) n-k M-E f M f \mathbb F f=gh g \mathbb F h","['matrices', 'field-theory', 'finite-fields', 'irreducible-polynomials', 'jordan-normal-form']"
3,Understanding the QR eigenvalue finding algorithm,Understanding the QR eigenvalue finding algorithm,,"I'm trying to code up a matrix library (purely as a learning exercise). This question is about the math I'm trying to understand in order to implement it. I just want to make sure I have a firm grasp of the underlying algorithm. For reference, I'm referring mainly to this book for the algorithms and mathematical understanding: http://people.inf.ethz.ch/arbenz/ewp/Lnotes/chapter3.pdf First, basic QR algorithm: Given an $N \times N$ non-symmetric real A, let QR(A) be the QR Decomposition into an orthogonal matrix Q and an upper triangular matrix R. Then, iterate $QR(A_{k-1}), A_k = R_k*Q_k$ until the last element in the first sub diagonal is close (enough) to zero. Take the last element (in the main diagonal) as an eigenvalue and deflate the matrix by removing its last row and column and begin the process again, until the entire matrix has been consumed. To handle complex eigenvalues, I've assumed that if this element does not converge to zero before a specified iteration_max, then the trailing $2 \times 2$ sub matrix contains two eigenvalues (which still may be real eigenvalues, I think, so I check for that). I apply a straight quadratic equation to that sub-matrix's characteristic polynomial to get its eigenvalues, and deflate the A matrix by two rows and columns, and iterate. Everything before this point seems to work when I implement it in code. Now, the problem is that the convergence is much too slow. I have to set the maximum iterations severely high in order to handle some of my test cases. So I went and read up on advanced QR tricks. One is to reduce the input matrix to upper Hessenberg form, which I believe I've done correctly through the transformation H = Hessenberg(A). However, for one of my test cases, this causes the required iterations to actually increase . That makes me scratch my head. More importantly, I tried to implement shifting, and this broke my algorithm. So I need to make sure I understand that. Let the shift S be defined as the last element on the main diagonal. Let I be defined as an appropriately sized identity matrix. Then, I do $QR(H_k - I * S_k)$ . And $H_{k+1} = R * Q + I * S_k$ . This causes my matrix to rapidly converge. Unfortunately, it also effectively zeroes the last element in the main diagonal of H in my test case, causing it to incorrectly report an eigenvalue near zero. What am I doing wrong? Edit: (please don't laugh at my little $4 \times 4$ matrix)  Test matrix: double[][] Matrix = {         {-2.476, -2.814, 4.29, -3.649},         {2.839, -2.859, 1.623, -2.926},         {-0.392, -3.206, -0.401, -2.174},         {2.241, -4.435, -3.963, 4.102}}; Edit: link to repository with code. Relevant function is large or I would embed it. Located in SquareMatrix.eigenvalues() https://github.com/rwthompsonii/matrix-java","I'm trying to code up a matrix library (purely as a learning exercise). This question is about the math I'm trying to understand in order to implement it. I just want to make sure I have a firm grasp of the underlying algorithm. For reference, I'm referring mainly to this book for the algorithms and mathematical understanding: http://people.inf.ethz.ch/arbenz/ewp/Lnotes/chapter3.pdf First, basic QR algorithm: Given an non-symmetric real A, let QR(A) be the QR Decomposition into an orthogonal matrix Q and an upper triangular matrix R. Then, iterate until the last element in the first sub diagonal is close (enough) to zero. Take the last element (in the main diagonal) as an eigenvalue and deflate the matrix by removing its last row and column and begin the process again, until the entire matrix has been consumed. To handle complex eigenvalues, I've assumed that if this element does not converge to zero before a specified iteration_max, then the trailing sub matrix contains two eigenvalues (which still may be real eigenvalues, I think, so I check for that). I apply a straight quadratic equation to that sub-matrix's characteristic polynomial to get its eigenvalues, and deflate the A matrix by two rows and columns, and iterate. Everything before this point seems to work when I implement it in code. Now, the problem is that the convergence is much too slow. I have to set the maximum iterations severely high in order to handle some of my test cases. So I went and read up on advanced QR tricks. One is to reduce the input matrix to upper Hessenberg form, which I believe I've done correctly through the transformation H = Hessenberg(A). However, for one of my test cases, this causes the required iterations to actually increase . That makes me scratch my head. More importantly, I tried to implement shifting, and this broke my algorithm. So I need to make sure I understand that. Let the shift S be defined as the last element on the main diagonal. Let I be defined as an appropriately sized identity matrix. Then, I do . And . This causes my matrix to rapidly converge. Unfortunately, it also effectively zeroes the last element in the main diagonal of H in my test case, causing it to incorrectly report an eigenvalue near zero. What am I doing wrong? Edit: (please don't laugh at my little matrix)  Test matrix: double[][] Matrix = {         {-2.476, -2.814, 4.29, -3.649},         {2.839, -2.859, 1.623, -2.926},         {-0.392, -3.206, -0.401, -2.174},         {2.241, -4.435, -3.963, 4.102}}; Edit: link to repository with code. Relevant function is large or I would embed it. Located in SquareMatrix.eigenvalues() https://github.com/rwthompsonii/matrix-java","N \times N QR(A_{k-1}), A_k = R_k*Q_k 2 \times 2 QR(H_k - I * S_k) H_{k+1} = R * Q + I * S_k 4 \times 4","['matrices', 'algorithms', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
4,Generalization of the Jordan form for infinite matrices,Generalization of the Jordan form for infinite matrices,,"Under what conditions is it the case that for a matrix $M$ whose rows and columns are indexed by a countably infinite set $S$ one has a Hamel basis consisting of generalized eigenvectors (i.e. $v \in \ker(M - \lambda I)^n$) of $M$? Must $M$ be a compact operator (I have a norm)? The matrix I am working with has non-negative entries, row sums not exceeding $1$ (substochastic), is irreducible and aperiodic. However, I suspect this question may be of general interest to others, so any solution not employing these properties would be all the more useful. EDIT Here is some more information: the matrix $M$ which I am working with is $R$-positive. This means that none of the sequences $\{ M^n_{ij}\}_{n \in \mathbb{N}}$, $i,j \in S$, converge to $0$, where  $$    R^{-1} := \lim_{n \to \infty} (M_{ij}^n)^{1/n}. $$ In such a case, it is known that $R^{-1}$ is the spectral radius of $M$, and moreover that $R^{-1}$ is an eigenvalue for $M$ for which there are unique left and right eigenvectors $\alpha,\beta$ which are strictly positive and satisfy  $$   \sum_{k \in S} \alpha(k) \beta(k) < \infty. $$ In particular the set of eigenvalues for $M$ cannot be empty. Thanks!","Under what conditions is it the case that for a matrix $M$ whose rows and columns are indexed by a countably infinite set $S$ one has a Hamel basis consisting of generalized eigenvectors (i.e. $v \in \ker(M - \lambda I)^n$) of $M$? Must $M$ be a compact operator (I have a norm)? The matrix I am working with has non-negative entries, row sums not exceeding $1$ (substochastic), is irreducible and aperiodic. However, I suspect this question may be of general interest to others, so any solution not employing these properties would be all the more useful. EDIT Here is some more information: the matrix $M$ which I am working with is $R$-positive. This means that none of the sequences $\{ M^n_{ij}\}_{n \in \mathbb{N}}$, $i,j \in S$, converge to $0$, where  $$    R^{-1} := \lim_{n \to \infty} (M_{ij}^n)^{1/n}. $$ In such a case, it is known that $R^{-1}$ is the spectral radius of $M$, and moreover that $R^{-1}$ is an eigenvalue for $M$ for which there are unique left and right eigenvectors $\alpha,\beta$ which are strictly positive and satisfy  $$   \sum_{k \in S} \alpha(k) \beta(k) < \infty. $$ In particular the set of eigenvalues for $M$ cannot be empty. Thanks!",,"['matrices', 'functional-analysis', 'eigenvalues-eigenvectors', 'markov-chains', 'jordan-normal-form']"
5,Eigenvalues appear when the dimension of the Prime Index Matrix is a prime-th prime. Why?,Eigenvalues appear when the dimension of the Prime Index Matrix is a prime-th prime. Why?,,"I had a look at the eigenvalues of the matrix, I called it Prime Index Matrix (is there a better name?), constructed like the following:  $$ P_{k,p_k}=P_{p_k,k}=1, $$ where $p_k$ is the $k$th prime. $P$ is symmetric and for $k_\max=6000$ looks like the left figure: The maximal eigenvalue behaves strange, since from time to time I get bumps, which I can't explain (see right figure). The largest value I got was $1.9021$ for $k_\max=6000$. I had a closer look at the bumps. Funnily I recognized that they happen to happen at: $1,2,3,5,11,31,127,709,5381,...$, but what funny about that? I was just about to post a question about the following recursion $\Phi: k \mapsto p_k$, which gives you $1 \mapsto p_1=2 \mapsto p_2=3\mapsto p_3=5\mapsto p_5=11\mapsto p_{11}=31\mapsto p_{31}=127\mapsto p_{127}=709\mapsto p_{709} $ How can this be explained? And how this help to answer my original question ? EDIT The sequence is known: OEIS/A007097 . It relates to the Matula-Goebel number of the rooted path tree on n+1 vertices. Does this give anybody a hint? EDIT 2.0 The same behaviour appears when you restrict to primes $\bmod 4 \equiv 1$ or $\bmod 4 \equiv 3$: The jumps in eigenvalues appear for primes $5 \mapsto p_{4,1;5}= 37 \mapsto p_{4,1;37}= 397\mapsto p_{4,1;397}= 6229$. For the other case it's $3, 11, 71, 787,...$. I also checked primes $\bmod 6\equiv 1$: same result...","I had a look at the eigenvalues of the matrix, I called it Prime Index Matrix (is there a better name?), constructed like the following:  $$ P_{k,p_k}=P_{p_k,k}=1, $$ where $p_k$ is the $k$th prime. $P$ is symmetric and for $k_\max=6000$ looks like the left figure: The maximal eigenvalue behaves strange, since from time to time I get bumps, which I can't explain (see right figure). The largest value I got was $1.9021$ for $k_\max=6000$. I had a closer look at the bumps. Funnily I recognized that they happen to happen at: $1,2,3,5,11,31,127,709,5381,...$, but what funny about that? I was just about to post a question about the following recursion $\Phi: k \mapsto p_k$, which gives you $1 \mapsto p_1=2 \mapsto p_2=3\mapsto p_3=5\mapsto p_5=11\mapsto p_{11}=31\mapsto p_{31}=127\mapsto p_{127}=709\mapsto p_{709} $ How can this be explained? And how this help to answer my original question ? EDIT The sequence is known: OEIS/A007097 . It relates to the Matula-Goebel number of the rooted path tree on n+1 vertices. Does this give anybody a hint? EDIT 2.0 The same behaviour appears when you restrict to primes $\bmod 4 \equiv 1$ or $\bmod 4 \equiv 3$: The jumps in eigenvalues appear for primes $5 \mapsto p_{4,1;5}= 37 \mapsto p_{4,1;37}= 397\mapsto p_{4,1;397}= 6229$. For the other case it's $3, 11, 71, 787,...$. I also checked primes $\bmod 6\equiv 1$: same result...",,"['matrices', 'prime-numbers', 'eigenvalues-eigenvectors', 'recursion']"
6,"Gelfand's formula, different field","Gelfand's formula, different field",,"Gelfand's formula says that for a complex matrix $A \in \mathbb{C}^{n \times n}$, $$\rho(A) = \lim_{m \rightarrow \infty} \|A^m\|^{1/m},$$ where $\rho$ is the spectral radius (norm of maximal eigenvalue / maximal norm of eigenvalues) and $\|\cdot \|$ is any matrix norm. I looked at the proof at http://en.wikipedia.org/wiki/Spectral_radius and to me it seems like this should be valid (maybe with slight changes to the proof) over any algebraically closed field $K = \overline{K}$ with a nontrivial valuation (in particular, over $\overline{\mathbb{Q}_p}$). Can anyone confirm this?","Gelfand's formula says that for a complex matrix $A \in \mathbb{C}^{n \times n}$, $$\rho(A) = \lim_{m \rightarrow \infty} \|A^m\|^{1/m},$$ where $\rho$ is the spectral radius (norm of maximal eigenvalue / maximal norm of eigenvalues) and $\|\cdot \|$ is any matrix norm. I looked at the proof at http://en.wikipedia.org/wiki/Spectral_radius and to me it seems like this should be valid (maybe with slight changes to the proof) over any algebraically closed field $K = \overline{K}$ with a nontrivial valuation (in particular, over $\overline{\mathbb{Q}_p}$). Can anyone confirm this?",,"['matrices', 'spectral-theory', 'p-adic-number-theory']"
7,Tall $m \times n$ matrices whose submatrices with $n$ rows have full rank,Tall  matrices whose submatrices with  rows have full rank,m \times n n,"I want to find some $m \times n$ (where $m>n$ ) matrices that have the property that any submatrix with $n$ rows has full rank. The Vandermonde and Cauchy matrices are the only two matrices I know. Can you please give me some other matrices? P.S. I forgot to mention that the entries in the matrices must be integers, and the base field is infinite field.","I want to find some (where ) matrices that have the property that any submatrix with rows has full rank. The Vandermonde and Cauchy matrices are the only two matrices I know. Can you please give me some other matrices? P.S. I forgot to mention that the entries in the matrices must be integers, and the base field is infinite field.",m \times n m>n n,"['matrices', 'matrix-rank']"
8,Slutsky's theorem for random matrices,Slutsky's theorem for random matrices,,This image is from Applied Multivariate Analysis . In this image plim means convergence in probability. I could not find the reference about the statement for random matrices. I'd highly appreciate if you cite the reference or give the proof. Thanks,This image is from Applied Multivariate Analysis . In this image plim means convergence in probability. I could not find the reference about the statement for random matrices. I'd highly appreciate if you cite the reference or give the proof. Thanks,,"['matrices', 'probability-theory', 'random-matrices']"
9,A matrix eigenvalue problem,A matrix eigenvalue problem,,"In my previous problem , I made a typo. Now I restate it as a new problem. Let  $  \begin{bmatrix} A& B   \\ B^*  &C \end{bmatrix}$ be positive semidefinite, $A,C$ are of size $n\times n$. Is it true that $$\quad \sum\limits_{i=1}^k\lambda_i\begin{bmatrix} A& B   \\ B^*  &C \end{bmatrix}\le \sum\limits_{i=1}^k\left(\lambda_i(A)+\lambda_i(C)\right)\quad, $$  where $1\le k\le n$? Here, $\lambda_i(\cdot)$ means the $i$th largest eigenvalue of $\cdot\quad$","In my previous problem , I made a typo. Now I restate it as a new problem. Let  $  \begin{bmatrix} A& B   \\ B^*  &C \end{bmatrix}$ be positive semidefinite, $A,C$ are of size $n\times n$. Is it true that $$\quad \sum\limits_{i=1}^k\lambda_i\begin{bmatrix} A& B   \\ B^*  &C \end{bmatrix}\le \sum\limits_{i=1}^k\left(\lambda_i(A)+\lambda_i(C)\right)\quad, $$  where $1\le k\le n$? Here, $\lambda_i(\cdot)$ means the $i$th largest eigenvalue of $\cdot\quad$",,['matrices']
10,Eigenvectors not stable under matrix perturbation,Eigenvectors not stable under matrix perturbation,,"Let $A,B$ be square matrices of the same size but arbitrary components. It is a fact that the eigenvalues of $A$ and $A+\epsilon B$ are close for $\epsilon$ small, but the eigenvectors may be drastically different. This latter point is what I am interested in. I am mainly looking for references or work that has been done on this. There is a lot on perturbation of eigenvalues, but I can't really find anything about how perturbations affect eigenvectors. A simple example is $A=\left(\begin{matrix}1 & 0\\0&1\end{matrix}\right)$ , $B=\left(\begin{matrix}-1 & 1\\1&-1\end{matrix}\right)$ and let $\epsilon>0$ be small. Then the eigenpairs for $A$ are just $1,(1,0)^\top$ and $1,(0,1)^\top$ . The eigenpairs for $A+\epsilon B$ are $1,(1,1)^\top$ and $1-2\epsilon,(1,-1)^\top$ . So the eigenvalues are just perturbed from the original ones for $A$ , but the eigenvectors are clearly very different. I saw this wikipedia article that showed some long calculations for the Hermitian, positive definite case with distinct eigenvalues, and, as far as I can tell, the eigenvectors are only perturbed slightly in that case. In my example, if I change $A_{1,1}=1+\delta$ for a small $\delta$ and make $\epsilon$ small enough, then I do get that the eigenvectors are only slightly perturbed, but it seems $\epsilon$ must be very small compared to $\delta$ (e.g. $\epsilon=\delta/100$ seems to work well). My question is this: (1) Are there other cases (other than Hermitian, positive definite case with distinct eigenvalues) where the eigenvectors are indeed only perturbed? I'm specifically interested in nonnegative matrices that are not symmetric (but maybe still with distinct eigenvalues), but also if this is ever possible with repeated eigenvalues. (2) If the eigenvectors are not simply perturbed but drastically different, is there some relation between the new eigenvectors and the old, e.g. something like the each new eigenvector being a linear combination of the old eigenvectors or some other tractable relationship? I find for the upper triangular $2\times2$ case with $A=\left(\begin{matrix}a & b\\0&d\end{matrix}\right)$ , $B=\left(\begin{matrix}0 & 0\\1&-1\end{matrix}\right)$ that we get $\lambda_\pm$ for $A$ with eigenvectors $(b, \lambda_\pm-a)^\top$ . For $A+\epsilon B$ we get the eigenvalues perturbed by $O(\epsilon)$ . The eigenvalues for $A$ , leaving unsimplified, are $$\lambda_\pm=\frac{a+d}2\pm\sqrt{\left(\frac{a-d}2\right)^2}$$ with eigenvectors $(b, \lambda_\pm-a)^\top$ . And the eigenvalues for $A+\epsilon B$ are $$\lambda_\pm=\frac{a+d}2-\frac\epsilon2\pm\sqrt{\left(\frac{a-d}2\right)^2}\cdot\sqrt{1+4\epsilon \frac{b+d}{(a-d)^2}+O(\epsilon^2)}=\lambda_\pm+O(\epsilon)$$ with eigenvectors $(b,\lambda_\pm-a+O(\epsilon))^\top$ So in this case, where the matrix $A$ is clearly not symmetric (at least for certain values of $a,b,d$ ), the eigenvectors are simply perturbed. Also, we can clearly see that as $a$ and $d$ get close, then $\epsilon$ needs to be much smaller to make the eigenvectors close. It's also not too hard to work out the same calculation for the $2\times2$ case of general $A=\left(\begin{matrix}a & b\\c&d\end{matrix}\right)$ , $B=\left(\begin{matrix}-1 & 1\\-1&1\end{matrix}\right)$ to get that the eigenvalues and eigenvectors are similarly changed by $O(\epsilon)$ . I have a feeling this $O(\epsilon)$ perturbation holds for the general $2\times2$ case as long as the eigenvalues are distinct (i.e. as long as $A$ isn't triangular with identical diagonal components), but I haven't worked it out carefully. I definitely don't want to try such a calculation for the $3\times3$ case! But maybe there is something already known for other cases?","Let be square matrices of the same size but arbitrary components. It is a fact that the eigenvalues of and are close for small, but the eigenvectors may be drastically different. This latter point is what I am interested in. I am mainly looking for references or work that has been done on this. There is a lot on perturbation of eigenvalues, but I can't really find anything about how perturbations affect eigenvectors. A simple example is , and let be small. Then the eigenpairs for are just and . The eigenpairs for are and . So the eigenvalues are just perturbed from the original ones for , but the eigenvectors are clearly very different. I saw this wikipedia article that showed some long calculations for the Hermitian, positive definite case with distinct eigenvalues, and, as far as I can tell, the eigenvectors are only perturbed slightly in that case. In my example, if I change for a small and make small enough, then I do get that the eigenvectors are only slightly perturbed, but it seems must be very small compared to (e.g. seems to work well). My question is this: (1) Are there other cases (other than Hermitian, positive definite case with distinct eigenvalues) where the eigenvectors are indeed only perturbed? I'm specifically interested in nonnegative matrices that are not symmetric (but maybe still with distinct eigenvalues), but also if this is ever possible with repeated eigenvalues. (2) If the eigenvectors are not simply perturbed but drastically different, is there some relation between the new eigenvectors and the old, e.g. something like the each new eigenvector being a linear combination of the old eigenvectors or some other tractable relationship? I find for the upper triangular case with , that we get for with eigenvectors . For we get the eigenvalues perturbed by . The eigenvalues for , leaving unsimplified, are with eigenvectors . And the eigenvalues for are with eigenvectors So in this case, where the matrix is clearly not symmetric (at least for certain values of ), the eigenvectors are simply perturbed. Also, we can clearly see that as and get close, then needs to be much smaller to make the eigenvectors close. It's also not too hard to work out the same calculation for the case of general , to get that the eigenvalues and eigenvectors are similarly changed by . I have a feeling this perturbation holds for the general case as long as the eigenvalues are distinct (i.e. as long as isn't triangular with identical diagonal components), but I haven't worked it out carefully. I definitely don't want to try such a calculation for the case! But maybe there is something already known for other cases?","A,B A A+\epsilon B \epsilon A=\left(\begin{matrix}1 & 0\\0&1\end{matrix}\right) B=\left(\begin{matrix}-1 & 1\\1&-1\end{matrix}\right) \epsilon>0 A 1,(1,0)^\top 1,(0,1)^\top A+\epsilon B 1,(1,1)^\top 1-2\epsilon,(1,-1)^\top A A_{1,1}=1+\delta \delta \epsilon \epsilon \delta \epsilon=\delta/100 2\times2 A=\left(\begin{matrix}a & b\\0&d\end{matrix}\right) B=\left(\begin{matrix}0 & 0\\1&-1\end{matrix}\right) \lambda_\pm A (b, \lambda_\pm-a)^\top A+\epsilon B O(\epsilon) A \lambda_\pm=\frac{a+d}2\pm\sqrt{\left(\frac{a-d}2\right)^2} (b, \lambda_\pm-a)^\top A+\epsilon B \lambda_\pm=\frac{a+d}2-\frac\epsilon2\pm\sqrt{\left(\frac{a-d}2\right)^2}\cdot\sqrt{1+4\epsilon \frac{b+d}{(a-d)^2}+O(\epsilon^2)}=\lambda_\pm+O(\epsilon) (b,\lambda_\pm-a+O(\epsilon))^\top A a,b,d a d \epsilon 2\times2 A=\left(\begin{matrix}a & b\\c&d\end{matrix}\right) B=\left(\begin{matrix}-1 & 1\\-1&1\end{matrix}\right) O(\epsilon) O(\epsilon) 2\times2 A 3\times3","['matrices', 'eigenvalues-eigenvectors', 'perturbation-theory']"
11,Numerically unstable matrix decompositions (to simple perturbations e.g. $A+ \varepsilon A$),Numerically unstable matrix decompositions (to simple perturbations e.g. ),A+ \varepsilon A,"So basically the title says it all. I would like to find a decomposition, or something similar, e.g. any transformation that would ""notice"" a slight perturbation in a given matrix; as in $f(A) - f(A+ \varepsilon * A)$ is relatively big. It would be desirable if it were reversible. Let's say the matrix has nonnegative values and they are bounded from above. An example could be LSB type algorithms. All of the normally used decompositions are stable. E.g. singular value decomposition: $U, S,V$ won't change much if I make slight changes to the original matrix.","So basically the title says it all. I would like to find a decomposition, or something similar, e.g. any transformation that would ""notice"" a slight perturbation in a given matrix; as in is relatively big. It would be desirable if it were reversible. Let's say the matrix has nonnegative values and they are bounded from above. An example could be LSB type algorithms. All of the normally used decompositions are stable. E.g. singular value decomposition: won't change much if I make slight changes to the original matrix.","f(A) - f(A+ \varepsilon * A) U, S,V","['matrices', 'perturbation-theory']"
12,How to get the characteristic polynomial of this matrix?,How to get the characteristic polynomial of this matrix?,,"Consider a $n\times n$ matrix: $$ M_n = \begin{pmatrix} a_1 & 1 & 0 & 0 & 0 & \cdots & 1 \\ 1 & a_2 & 1 & 0 & 0 & \cdots & 0 \\ 0 & 1 & a_3 & 1 & 0 & \cdots & 0 \\ \vdots & \vdots& \vdots& \vdots & \vdots& \vdots & \vdots \\ 0 & \cdots & \cdots & 0 & 1 & a_{n-1} & 1 \\ 1 & \cdots & \cdots & \cdots & \cdots & 1 & a_n \end{pmatrix} $$ where $a_k=2\cos(k\phi)+2\mathrm{i}\gamma\sin(k\phi)$ , with $\phi=2\pi/n$ and $0<\gamma<1$ . $~n\ge5$ , and can be assumed to be prime numbers if necessary. How to get its characteristic polynomial $P_n(x)=\det(M_n-xI)$ ? $x^n$ , $x^{n-2}$ and $x^0$ terms are easy to get, can you get other terms?","Consider a matrix: where , with and . , and can be assumed to be prime numbers if necessary. How to get its characteristic polynomial ? , and terms are easy to get, can you get other terms?","n\times n 
M_n = \begin{pmatrix}
a_1 & 1 & 0 & 0 & 0 & \cdots & 1 \\
1 & a_2 & 1 & 0 & 0 & \cdots & 0 \\
0 & 1 & a_3 & 1 & 0 & \cdots & 0 \\
\vdots & \vdots& \vdots& \vdots & \vdots& \vdots & \vdots \\
0 & \cdots & \cdots & 0 & 1 & a_{n-1} & 1 \\
1 & \cdots & \cdots & \cdots & \cdots & 1 & a_n
\end{pmatrix}
 a_k=2\cos(k\phi)+2\mathrm{i}\gamma\sin(k\phi) \phi=2\pi/n 0<\gamma<1 ~n\ge5 P_n(x)=\det(M_n-xI) x^n x^{n-2} x^0","['matrices', 'determinant', 'trace', 'characteristic-polynomial']"
13,binomial determinant,binomial determinant,,"Let $n > 0$ , then : $$ \det \left( {2n \choose n+i-j} \right)_{i,j=0}^{n-1} = \prod_{i=0}^{n-1} \frac{2n+i \choose n}{n+i \choose n}  $$ The LHS appears to be the determinant of a symetric Toeplitz matrix with binomial coefficients. Does anyone have an idea on how to proove this formula ?","Let , then : The LHS appears to be the determinant of a symetric Toeplitz matrix with binomial coefficients. Does anyone have an idea on how to proove this formula ?","n > 0  \det \left( {2n \choose n+i-j} \right)_{i,j=0}^{n-1} = \prod_{i=0}^{n-1} \frac{2n+i \choose n}{n+i \choose n}  ","['matrices', 'determinant']"
14,"""Almost Normal"" Matrix and Gap between Spectral Radius/Norm","""Almost Normal"" Matrix and Gap between Spectral Radius/Norm",,"Let's denote $$\Vert{A}\Vert := \max_{x\neq0}\frac{x^* Ax}{x^*x}$$ and let $\rho(A)$ denote the largest absolute value of the eigenvalues of matrix $A$ . From basic linear algebra, one could characterize normal matrices as those unitarily diagonalizable ones, namely, for $A^*A=AA^*$ , there exist $Q \in \mathsf{SU}(n)$ and $\Lambda \in \mathsf{diag}(n)$ such that $$ A = Q\Lambda Q^*. $$ Therefore, the spectral norm is exactly the spectral radius, $\rho(A)=\Vert A\Vert$ . On the other hand, when $A$ is not normal, even if it is diagonalizable, it is easy to construct some matrices with small spectral radius but large spectral norm, e.g., for $$A=\pmatrix{\epsilon&0\\\frac{1}{\epsilon}&2\epsilon}$$ we clearly have $\rho(A)=2|\epsilon|\rightarrow 0$ but $\Vert A\Vert\geq\frac{1}{|\epsilon|}\rightarrow +\infty$ as $\epsilon\rightarrow 0$ . It seems natural that, if we quantify the obstruction from a not-normal matrix to normal, we might be able bound the gap between spectral radius/norm. So here is my question: Suppose for $A\in\mathbb{C}^{n\times n}$ there is some $Q\in\mathsf{SU}(n)$ that $\Vert A-QA^*\Vert\leq\epsilon$ is small, could we have some upper bound on either the multiplicative gap $\Vert A\Vert/\rho(A)$ or additive gap $\Vert A\Vert-\rho(A)$ between its spectral norm and radius?","Let's denote and let denote the largest absolute value of the eigenvalues of matrix . From basic linear algebra, one could characterize normal matrices as those unitarily diagonalizable ones, namely, for , there exist and such that Therefore, the spectral norm is exactly the spectral radius, . On the other hand, when is not normal, even if it is diagonalizable, it is easy to construct some matrices with small spectral radius but large spectral norm, e.g., for we clearly have but as . It seems natural that, if we quantify the obstruction from a not-normal matrix to normal, we might be able bound the gap between spectral radius/norm. So here is my question: Suppose for there is some that is small, could we have some upper bound on either the multiplicative gap or additive gap between its spectral norm and radius?","\Vert{A}\Vert := \max_{x\neq0}\frac{x^* Ax}{x^*x} \rho(A) A A^*A=AA^* Q \in \mathsf{SU}(n) \Lambda \in \mathsf{diag}(n) 
A = Q\Lambda Q^*.
 \rho(A)=\Vert A\Vert A A=\pmatrix{\epsilon&0\\\frac{1}{\epsilon}&2\epsilon} \rho(A)=2|\epsilon|\rightarrow 0 \Vert A\Vert\geq\frac{1}{|\epsilon|}\rightarrow +\infty \epsilon\rightarrow 0 A\in\mathbb{C}^{n\times n} Q\in\mathsf{SU}(n) \Vert A-QA^*\Vert\leq\epsilon \Vert A\Vert/\rho(A) \Vert A\Vert-\rho(A)","['matrices', 'matrix-norms', 'spectral-radius', 'matrix-analysis', 'spectral-norm']"
15,"If $p+q+r=0$, find the value of the determinant","If , find the value of the determinant",p+q+r=0,"If $p+q+r=0$ , prove that the value of the determinant $$ \Delta= \begin{vmatrix} pa & qb &rc \\  qc & ra &pb\\  rb& pc & qa  \\  \end{vmatrix} =-pqr \begin{vmatrix} a & b &c \\  b & c &a\\  c& a & b  \\  \end{vmatrix}$$ My Try:Since $p+q+r=0$ we have $$a(p+q+r)+b(p+q+r)+c(p+q+r)=0$$ $\implies$ $$(ap+qc+rb)+(qb+ra+pc)+(rc+pb+qa)=0 \tag{1}$$ Now applying $C_1 \to C_1+C_2+C_3$ and then applying $R_1 \to R_1+R_2+R_3$ for $\Delta$ we get $$\Delta=  \begin{vmatrix} 0 & qb &rc \\  qc+ra+pb & ra &pb\\  rb+pc+qa& pc & qa  \\  \end{vmatrix}$$ Any clue here?","If , prove that the value of the determinant My Try:Since we have Now applying and then applying for we get Any clue here?","p+q+r=0  \Delta= \begin{vmatrix}
pa & qb &rc \\ 
qc & ra &pb\\ 
rb& pc & qa  \\ 
\end{vmatrix} =-pqr \begin{vmatrix}
a & b &c \\ 
b & c &a\\ 
c& a & b  \\ 
\end{vmatrix} p+q+r=0 a(p+q+r)+b(p+q+r)+c(p+q+r)=0 \implies (ap+qc+rb)+(qb+ra+pc)+(rc+pb+qa)=0 \tag{1} C_1 \to C_1+C_2+C_3 R_1 \to R_1+R_2+R_3 \Delta \Delta=  \begin{vmatrix}
0 & qb &rc \\ 
qc+ra+pb & ra &pb\\ 
rb+pc+qa& pc & qa  \\ 
\end{vmatrix}","['linear-algebra', 'matrices', 'determinant']"
16,The polarization of the determinant is invariant?,The polarization of the determinant is invariant?,,"Given $n \in \mathbb N$, I am asked to show that there is a multilinear symmetric $\operatorname{GL}_n$-invariant form $\phi : (M_{n \times n})^l \to \mathbb R$ (for some $l \geq 0$) such that $\phi(A,A,...,A) = \det A$ for all $A \in \mathbb M_{n \times n}(R)$. Using the idea of polarizing an algebraic form, I decided to do the following: by definition, we have $$ \det(A) = \sum_{\sigma \in S_n} \operatorname{sgn}(\sigma) \prod_{i=1}^n A_{i \sigma(i)}  $$ and therefore we could define $\phi$ with $l=n$, but taking $n$ independent copies of $A$ and adding another permutation into the mix, just like the polarization formula. This gives: $$ \phi(A^{1},A^2,...,A^n) = \frac 1{n!} \sum_{\pi \in S_n} \sum_{\sigma \in S_n} \operatorname{sgn}(\sigma)\prod_{i=1}^n A^{\pi(i)}_{i\sigma(i)} $$ This formula is multilinear, symmetric and $\phi(A,A,..,A) = \det A$. However, I am not quite getting how to prove invariance : $\phi(g^{-1}A^1g, g^{-1}A^2g,...,g^{-1}A^ng) = \phi(A^1,A^2,...,A^n)$ for all $A^i$, $i = 1 \to n$. While this may not work out, I am inclined to think it does, since I used the definition of polarization to obtain all the conditions, and got the one I need additionally. However, proceeding by simple expansion does not work (creating a bunch of $g^{-1}$ and $g$ indexed terms, and therefore discombobulation), and therefore I need some help on why this is the case.","Given $n \in \mathbb N$, I am asked to show that there is a multilinear symmetric $\operatorname{GL}_n$-invariant form $\phi : (M_{n \times n})^l \to \mathbb R$ (for some $l \geq 0$) such that $\phi(A,A,...,A) = \det A$ for all $A \in \mathbb M_{n \times n}(R)$. Using the idea of polarizing an algebraic form, I decided to do the following: by definition, we have $$ \det(A) = \sum_{\sigma \in S_n} \operatorname{sgn}(\sigma) \prod_{i=1}^n A_{i \sigma(i)}  $$ and therefore we could define $\phi$ with $l=n$, but taking $n$ independent copies of $A$ and adding another permutation into the mix, just like the polarization formula. This gives: $$ \phi(A^{1},A^2,...,A^n) = \frac 1{n!} \sum_{\pi \in S_n} \sum_{\sigma \in S_n} \operatorname{sgn}(\sigma)\prod_{i=1}^n A^{\pi(i)}_{i\sigma(i)} $$ This formula is multilinear, symmetric and $\phi(A,A,..,A) = \det A$. However, I am not quite getting how to prove invariance : $\phi(g^{-1}A^1g, g^{-1}A^2g,...,g^{-1}A^ng) = \phi(A^1,A^2,...,A^n)$ for all $A^i$, $i = 1 \to n$. While this may not work out, I am inclined to think it does, since I used the definition of polarization to obtain all the conditions, and got the one I need additionally. However, proceeding by simple expansion does not work (creating a bunch of $g^{-1}$ and $g$ indexed terms, and therefore discombobulation), and therefore I need some help on why this is the case.",,"['matrices', 'determinant', 'invariant-theory']"
17,Convex conjugate of a matrix function,Convex conjugate of a matrix function,,"Let $S$ be some (not necessarily convex) subset of positive semidefinite matrices. What is the convex conjugate of the function $$ f(x) = \sup_M \left\{ x^T M x \;:\; M \in S \right\},$$ that is, what is $f^*(y) = \sup_x \{ x^T y - f(x) \}$? I'm struggling with this one because there are essentially two maximization problems involved, and I don't really have any idea about how to approach this. I'd appreciate any help.","Let $S$ be some (not necessarily convex) subset of positive semidefinite matrices. What is the convex conjugate of the function $$ f(x) = \sup_M \left\{ x^T M x \;:\; M \in S \right\},$$ that is, what is $f^*(y) = \sup_x \{ x^T y - f(x) \}$? I'm struggling with this one because there are essentially two maximization problems involved, and I don't really have any idea about how to approach this. I'd appreciate any help.",,"['matrices', 'convex-analysis', 'convex-optimization', 'matrix-equations']"
18,Can any 2x2 matrix be written like sum of two squared matrices? [duplicate],Can any 2x2 matrix be written like sum of two squared matrices? [duplicate],,"This question already has answers here : Is it true that any matrix in $M_2(\mathbb R)$ is the sum of two squares? (4 answers) Closed 7 years ago . How to prove that for any $A$ is a $2\times2$ matrix with real elements exist $B$ and $C$ so that $A=B^2+C^2$? So far, I used Cayley-Hamilton theorem and I have: $A =$ $\frac{1}{Tr(A)}A^2 + \frac{det(A)}{Tr(A)}I_n$. I know that I need a positive trace, so I choose $A_1 = A + tI_n$ and $\lim_{t\to∞}(A + tI_n) = \infty$","This question already has answers here : Is it true that any matrix in $M_2(\mathbb R)$ is the sum of two squares? (4 answers) Closed 7 years ago . How to prove that for any $A$ is a $2\times2$ matrix with real elements exist $B$ and $C$ so that $A=B^2+C^2$? So far, I used Cayley-Hamilton theorem and I have: $A =$ $\frac{1}{Tr(A)}A^2 + \frac{det(A)}{Tr(A)}I_n$. I know that I need a positive trace, so I choose $A_1 = A + tI_n$ and $\lim_{t\to∞}(A + tI_n) = \infty$",,['matrices']
19,Numerical stability of Winograd short convolution algorithm,Numerical stability of Winograd short convolution algorithm,,"Similar to how Strassen matrix multiplication is an asymptotically faster matrix-multiplication algorithm, there exists a similar idea for convolution by (short) filters called Winograd convolution [1,2] that lowers the number of multiplications required below that of the naive, direct-form convolution. To do this, Winograd convolution uses the Chinese Remainder Theorem over polynomials to convert real-valued data and filters to a real-valued transform space, pointwise multiplies, then transforms back the output. The FFT convolution does something similar but uses Fourier space and complex numbers. It is worth noting at this point that like the FFT, Winograd convolution is linear and ""correct"" (if a convolution using these algorithms were symbolically performed, the exactly correct result, and not a mere approximation, would be obtained). And yet, a properly implemented FFT is numerically stable, while Winograd convolution acquires hideous roundoff errors. I've attempted to determine empirically using Numpy where, if anywhere, I could perform the steps for Winograd convolution (6, 3) (See [2]: indicates 6 outputs, 3 filter taps) in reduced precision while still maintaining high output precision. I used random, normally-distributed inputs and filter taps, and the $A, B, G$ matrices suggested by the authors on Github for NNPACK [3]. The answer was, nowhere : If IEEE 754-2008 float (23+1 bit mantissa) is used throughout, the output appears to have around 12 bits precision. If double (52+1 bit mantissa) is used throughout, the output appears to have about 25 bits precision. If I cheap out and use float anywhere in the double -precision evaluation, I get 12 bits precision. So my question is, Why does Winograd convolution halve the precision of the computation? Because of Winograd's (and FFT's) linearity and correctness, I don't know how to isolate what causes the numerical instability in Winograd. Whatever the input and filters, both algorithms nominally give 0 error, and the perturbation in the outputs is linear in the perturbations at the inputs and filters; Yet Winograd halves precision and FFT does not. I would love it if any answer could address how numerical analysts go about proving error bounds due to roundoff in such linear, ""correct"" algorithms. This would expose the flaws in my thinking that leave me so baffled. [1] Arithmetic Complexity of Computation , Schmuel Winograd [2] Fast Algorithms for Convolutional Neural Networks , Andrew Lavin & Scott Gray [3] https://github.com/Maratyszcza/NNPACK/issues/8","Similar to how Strassen matrix multiplication is an asymptotically faster matrix-multiplication algorithm, there exists a similar idea for convolution by (short) filters called Winograd convolution [1,2] that lowers the number of multiplications required below that of the naive, direct-form convolution. To do this, Winograd convolution uses the Chinese Remainder Theorem over polynomials to convert real-valued data and filters to a real-valued transform space, pointwise multiplies, then transforms back the output. The FFT convolution does something similar but uses Fourier space and complex numbers. It is worth noting at this point that like the FFT, Winograd convolution is linear and ""correct"" (if a convolution using these algorithms were symbolically performed, the exactly correct result, and not a mere approximation, would be obtained). And yet, a properly implemented FFT is numerically stable, while Winograd convolution acquires hideous roundoff errors. I've attempted to determine empirically using Numpy where, if anywhere, I could perform the steps for Winograd convolution (6, 3) (See [2]: indicates 6 outputs, 3 filter taps) in reduced precision while still maintaining high output precision. I used random, normally-distributed inputs and filter taps, and the $A, B, G$ matrices suggested by the authors on Github for NNPACK [3]. The answer was, nowhere : If IEEE 754-2008 float (23+1 bit mantissa) is used throughout, the output appears to have around 12 bits precision. If double (52+1 bit mantissa) is used throughout, the output appears to have about 25 bits precision. If I cheap out and use float anywhere in the double -precision evaluation, I get 12 bits precision. So my question is, Why does Winograd convolution halve the precision of the computation? Because of Winograd's (and FFT's) linearity and correctness, I don't know how to isolate what causes the numerical instability in Winograd. Whatever the input and filters, both algorithms nominally give 0 error, and the perturbation in the outputs is linear in the perturbations at the inputs and filters; Yet Winograd halves precision and FFT does not. I would love it if any answer could address how numerical analysts go about proving error bounds due to roundoff in such linear, ""correct"" algorithms. This would expose the flaws in my thinking that leave me so baffled. [1] Arithmetic Complexity of Computation , Schmuel Winograd [2] Fast Algorithms for Convolutional Neural Networks , Andrew Lavin & Scott Gray [3] https://github.com/Maratyszcza/NNPACK/issues/8",,"['matrices', 'algorithms', 'numerical-linear-algebra', 'convolution', 'rounding-error']"
20,Link between the largest eigenvalue and the largest entry of a symmetric matrix,Link between the largest eigenvalue and the largest entry of a symmetric matrix,,"Let $A$ be a symmetric (or Hermitian) matrix with size $n\times n$. If it helps, one can consider $A$ tridiagonal (and symmetric). By spectral theorem, there exists $\lambda_1\geq ... \geq \lambda_n$ real eigenvalues. Are there known connection (such as inequalities etc) between its largest eigenvalue $\lambda_1$ and its largest entry $\max_{i,j}|a_{i,j}|$ ?","Let $A$ be a symmetric (or Hermitian) matrix with size $n\times n$. If it helps, one can consider $A$ tridiagonal (and symmetric). By spectral theorem, there exists $\lambda_1\geq ... \geq \lambda_n$ real eigenvalues. Are there known connection (such as inequalities etc) between its largest eigenvalue $\lambda_1$ and its largest entry $\max_{i,j}|a_{i,j}|$ ?",,"['matrices', 'eigenvalues-eigenvectors']"
21,"Is a complex determinant still a ""volume""? [closed]","Is a complex determinant still a ""volume""? [closed]",,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I like the interpretation of the determinant of an $m\times n$ matrix as the volume of the $n$-dimensional parallelepiped whose sides are the columns of the matrix. Does this interpretation hold also for a complex matrix? If so what's the meaning of a complex volume?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I like the interpretation of the determinant of an $m\times n$ matrix as the volume of the $n$-dimensional parallelepiped whose sides are the columns of the matrix. Does this interpretation hold also for a complex matrix? If so what's the meaning of a complex volume?",,"['matrices', 'complex-numbers', 'determinant']"
22,Matrix equivalence independent of dimension,Matrix equivalence independent of dimension,,"I'm looking for a criterion on symmetric, positive definite matrices $A$ which ensures the constant $M$ in the lower bound of the norm equivalence $M\|A\|_\infty \le \|A\|_2 \le \|A\|_\infty$ does not depend on the dimension of the matrix. EDIT: I was thinking that it sufficient to constrain the eigenvalues of $A$, but so far I can't make this work. So far, I have only a quite unsatisfying attempt based on matrix sensitivity: supposing $A = Id + \Delta$, the matrix $\Delta$ should have $\|\Delta\|_2 \le \frac M{\sqrt n}$. One can generalize a bit by taking any diagonal (positive definite) matrix (EDIT: with bounded eigenvalues) in the place of the identity, but I'd be interested in a more general criterion. UPDATE: OK, something simple I missed: if an eigenvector of $A$ is $e = (1,...,1)$, then $$ \|A\|_\infty = \max_{\|v\|_\infty =1}\|Av\|_\infty = \|Ae\|_\infty = \lambda_{e}(A) \le \lambda_{max}(A) = \|A\|_2. $$ However, I need more generality than this. UPDATE 2: Just an idea, not sure that it helps: what one needs is a constant $M$ independent of $n$ such that $$  \|A\|_\infty = \max_{i} \sum_{j=1}^n |a_{ij}| \le \frac {\|A\|_2}{M} $$ for all matrices in the class.","I'm looking for a criterion on symmetric, positive definite matrices $A$ which ensures the constant $M$ in the lower bound of the norm equivalence $M\|A\|_\infty \le \|A\|_2 \le \|A\|_\infty$ does not depend on the dimension of the matrix. EDIT: I was thinking that it sufficient to constrain the eigenvalues of $A$, but so far I can't make this work. So far, I have only a quite unsatisfying attempt based on matrix sensitivity: supposing $A = Id + \Delta$, the matrix $\Delta$ should have $\|\Delta\|_2 \le \frac M{\sqrt n}$. One can generalize a bit by taking any diagonal (positive definite) matrix (EDIT: with bounded eigenvalues) in the place of the identity, but I'd be interested in a more general criterion. UPDATE: OK, something simple I missed: if an eigenvector of $A$ is $e = (1,...,1)$, then $$ \|A\|_\infty = \max_{\|v\|_\infty =1}\|Av\|_\infty = \|Ae\|_\infty = \lambda_{e}(A) \le \lambda_{max}(A) = \|A\|_2. $$ However, I need more generality than this. UPDATE 2: Just an idea, not sure that it helps: what one needs is a constant $M$ independent of $n$ such that $$  \|A\|_\infty = \max_{i} \sum_{j=1}^n |a_{ij}| \le \frac {\|A\|_2}{M} $$ for all matrices in the class.",,"['matrices', 'normed-spaces', 'equivalent-metrics']"
23,When is a generalized Vandermonde matrix over a finite field invertible?,When is a generalized Vandermonde matrix over a finite field invertible?,,"The generalized Vandermonde matrix that I am considering is one where the rows of a matrix correspond to the powers of different elements of the field, but the powers need not be consecutive integers as for the case of a traditional Vandermonde matrix. Is there a result that characterizes when such a matrix is invertible? Thanks!","The generalized Vandermonde matrix that I am considering is one where the rows of a matrix correspond to the powers of different elements of the field, but the powers need not be consecutive integers as for the case of a traditional Vandermonde matrix. Is there a result that characterizes when such a matrix is invertible? Thanks!",,"['matrices', 'finite-fields']"
24,Diagonalization of a big scary matrix,Diagonalization of a big scary matrix,,"I would need to diagonalize this tridiagonal block matrix $M$: $$M = \begin{bmatrix}  A & B & & \\ B^T & A & B & \\  & B^T & A & B \\  & & \ddots & \ddots & \ddots \\  & & & B^T & A & B \\  & & & & B^T & A \end{bmatrix}_{n \times n}$$ where matrices $A$ and $B$ are also $n \times n$ tridiagonal: $$A = \begin{bmatrix}  C & D & & \\ D & C & D & \\  & D & C & D \\  & & \ddots & \ddots & \ddots \\  & & & D & C & D \\  & & & & D & C \end{bmatrix}_{n \times n}$$ $$B = \begin{bmatrix}  E & F & & \\ G & E & F & \\  & G & E & F \\  & & \ddots & \ddots & \ddots \\  & & & G & E & F \\  & & & & G & E \end{bmatrix}_{n \times n}$$ where: $$C = \begin{bmatrix}  8 & 0\\ 0 & 8 \end{bmatrix} \quad D = \begin{bmatrix}  -2 & 0\\ 0 & 0 \end{bmatrix} \quad E = \begin{bmatrix}  0 & 0\\ 0 & -2 \end{bmatrix} \quad F = \begin{bmatrix}  -1 & 1\\ 1 & -1 \end{bmatrix} \quad G = \begin{bmatrix}  -1 & -1\\ -1 & -1 \end{bmatrix} $$ So essentially $M$ is $4n^2 \times 4n^2$ large. I would need to do this, because I need to solve this system of differential equations $\ddot{\vec{x}} = -M \vec{x}$. When I set $\vec{x} = \vec{u} e^{i \omega t}$, I got the problem of eigenvalues $M \vec{u} = \omega^2 \vec{u}$, where $\omega^2$ are the eigenvalues and $\vec{u}$ are the eigenvectors. It would be great, if this monstrosity could be solved analytically for an arbitrary $n$.","I would need to diagonalize this tridiagonal block matrix $M$: $$M = \begin{bmatrix}  A & B & & \\ B^T & A & B & \\  & B^T & A & B \\  & & \ddots & \ddots & \ddots \\  & & & B^T & A & B \\  & & & & B^T & A \end{bmatrix}_{n \times n}$$ where matrices $A$ and $B$ are also $n \times n$ tridiagonal: $$A = \begin{bmatrix}  C & D & & \\ D & C & D & \\  & D & C & D \\  & & \ddots & \ddots & \ddots \\  & & & D & C & D \\  & & & & D & C \end{bmatrix}_{n \times n}$$ $$B = \begin{bmatrix}  E & F & & \\ G & E & F & \\  & G & E & F \\  & & \ddots & \ddots & \ddots \\  & & & G & E & F \\  & & & & G & E \end{bmatrix}_{n \times n}$$ where: $$C = \begin{bmatrix}  8 & 0\\ 0 & 8 \end{bmatrix} \quad D = \begin{bmatrix}  -2 & 0\\ 0 & 0 \end{bmatrix} \quad E = \begin{bmatrix}  0 & 0\\ 0 & -2 \end{bmatrix} \quad F = \begin{bmatrix}  -1 & 1\\ 1 & -1 \end{bmatrix} \quad G = \begin{bmatrix}  -1 & -1\\ -1 & -1 \end{bmatrix} $$ So essentially $M$ is $4n^2 \times 4n^2$ large. I would need to do this, because I need to solve this system of differential equations $\ddot{\vec{x}} = -M \vec{x}$. When I set $\vec{x} = \vec{u} e^{i \omega t}$, I got the problem of eigenvalues $M \vec{u} = \omega^2 \vec{u}$, where $\omega^2$ are the eigenvalues and $\vec{u}$ are the eigenvectors. It would be great, if this monstrosity could be solved analytically for an arbitrary $n$.",,"['matrices', 'diagonalization', 'block-matrices', 'tridiagonal-matrices']"
25,Proving the max of a quadratic form ${\mathbf x}^T\mathbf A \mathbf x$ can be attained when $x$ is from $n$-dimensional hypercube,Proving the max of a quadratic form  can be attained when  is from -dimensional hypercube,{\mathbf x}^T\mathbf A \mathbf x x n,"updated : Maybe my original question is somewhat misleading. I rewrite some of the post. This is some research problem I'm working on. I have an $n\times n$ symmetric positive-definite matrix $\mathbf A$. I also have a set of $2^n$ vectors $\mathbf x_1,\cdots,\mathbf x_{2^n}$ from an $n$-dimensional hypercube $\gamma_n$ of $\{1, -1\}^n$. I already know the maximum of the [quadratic form](http://mathworld.wolfram.com/QuadraticForm.html) of the matrix $\mathbf A$ and vectors $\mathbf x\in\{1, -1\}^n$ is a known constant $c$: Using some kind of AM-GM like inequality, I know that the quadratic form is at most $c$ $$ \mathbf x^T \mathbf A \mathbf x \leq c \quad\text{subject to $\mathbf x \in \{1,-1\}^n$}  $$ In this case, I want to show that there exists a vector $\mathbf x\in\{1,-1\}^n$ that achieves $\mathbf x^T \mathbf A \mathbf x = c$, and if that's possible, I'd like to show that this maximizer can be computed efficiently . Some related problem is the '0-1 Positive-Definite Maximum Problem' from Girtzmann and Klee : Instance : a symmetric positive definite matrix $\mathbf B$ and an integer $\lambda$ Question : Does there exist a vector $\mathbf x\in\{0,1\}^n$ such that $\mathbf x^T \mathbf B \mathbf x \geq \lambda$? They showed that this problem is NP-Complete . I think my problem is different from '0-1 Positive-Definite Maximum Problem' because I already know the upper bound of the quadratic form and ask whether this upper bound can be achieved from a vector from a hypercube. My Question : Is there any trick for an existential proof that such a maximizer exists in my setting? One trick I can come up on top of my head right now is the Averaging Argument , but this seems too strong: I'm afraid the averaging argument works only when all the $2^n$ quadratic form values amount to the desired maximum $c$.","updated : Maybe my original question is somewhat misleading. I rewrite some of the post. This is some research problem I'm working on. I have an $n\times n$ symmetric positive-definite matrix $\mathbf A$. I also have a set of $2^n$ vectors $\mathbf x_1,\cdots,\mathbf x_{2^n}$ from an $n$-dimensional hypercube $\gamma_n$ of $\{1, -1\}^n$. I already know the maximum of the [quadratic form](http://mathworld.wolfram.com/QuadraticForm.html) of the matrix $\mathbf A$ and vectors $\mathbf x\in\{1, -1\}^n$ is a known constant $c$: Using some kind of AM-GM like inequality, I know that the quadratic form is at most $c$ $$ \mathbf x^T \mathbf A \mathbf x \leq c \quad\text{subject to $\mathbf x \in \{1,-1\}^n$}  $$ In this case, I want to show that there exists a vector $\mathbf x\in\{1,-1\}^n$ that achieves $\mathbf x^T \mathbf A \mathbf x = c$, and if that's possible, I'd like to show that this maximizer can be computed efficiently . Some related problem is the '0-1 Positive-Definite Maximum Problem' from Girtzmann and Klee : Instance : a symmetric positive definite matrix $\mathbf B$ and an integer $\lambda$ Question : Does there exist a vector $\mathbf x\in\{0,1\}^n$ such that $\mathbf x^T \mathbf B \mathbf x \geq \lambda$? They showed that this problem is NP-Complete . I think my problem is different from '0-1 Positive-Definite Maximum Problem' because I already know the upper bound of the quadratic form and ask whether this upper bound can be achieved from a vector from a hypercube. My Question : Is there any trick for an existential proof that such a maximizer exists in my setting? One trick I can come up on top of my head right now is the Averaging Argument , but this seems too strong: I'm afraid the averaging argument works only when all the $2^n$ quadratic form values amount to the desired maximum $c$.",,"['matrices', 'quadratic-forms']"
26,How to solve a distance problem inside of a picture?,How to solve a distance problem inside of a picture?,,"sorry for my bad english. I have the following problem: In the picture you can see 4 different positions. Every position is known to me (longitude, latitude with screen-x and screen-y). Now i want to know, where in the picture a specific position is. For example, i want to have a rectangle 20 meters in front and 5 meters to the left of me. I also got the height of the camera, the rotation around X axis from the camera, the position of the camera (longitude,latitude) and the horizontal and vertical view angle of the camera. Can i use simple mathematic operations to solve this problem?","sorry for my bad english. I have the following problem: In the picture you can see 4 different positions. Every position is known to me (longitude, latitude with screen-x and screen-y). Now i want to know, where in the picture a specific position is. For example, i want to have a rectangle 20 meters in front and 5 meters to the left of me. I also got the height of the camera, the rotation around X axis from the camera, the position of the camera (longitude,latitude) and the horizontal and vertical view angle of the camera. Can i use simple mathematic operations to solve this problem?",,"['matrices', 'approximation', '3d', 'coordinate-systems', 'computer-vision']"
27,Quadric surfaces and transformation matrices,Quadric surfaces and transformation matrices,,"I'm currently facing a problem and I think I have found a solution to it. However, I'd be interested to hear your opinion or hints on this. I have written geometric modeling software and I have users who want to define a number of different bodies (e.g. cylinders, spheres, ellipsoids etc.) by providing the coefficients of the general quadric surface equation: $$ax^2 + 2bxy + 2cxz + 2dx + \dots=0$$ which in general can be written also using a coefficient matrix $Q$ and a row vector $v:$ $$v Q v^t$$ My solid modeler already provides various different objects like cylinders, ellipsoids etc. but they are defined in a canonical system (e.g. cylinder parallel to the $\,z\,$ axis) and carry a transformation matrix to place them at the right position for rendering. In order to treat bodies defined by general quadrics I have to do two things: 1.) Determine which kind of surface has been defined 2.) Obtain the transformation matrix which will transform the respective surface to the position/orientation which is implicitly included in the quadrics coefficients. After doing a bit of algebra I came up with the following idea for 2.) I thought to tackle the issue via principal axis transformation. So I would first try to determine the eigenvalues and the eigenvectors for the coefficient matrix $Q.$ The matrix which is represented by the eigenvectors in its columns should transform my coefficient matrix into the canonical system. If I take the inverse of this eigenmatrix then this should be my transformation matrix, or am I wrong here? The eigenvalues would eventually determine the magnitude of parameters like the radius for spheres & cylinders, the major or minor halfaxes for ellipsoids etc. I'd appreciate to hear your thoughts on this or if somebody has a simpler solution I'd be happy to discuss this as well. Thanks a lot Chris","I'm currently facing a problem and I think I have found a solution to it. However, I'd be interested to hear your opinion or hints on this. I have written geometric modeling software and I have users who want to define a number of different bodies (e.g. cylinders, spheres, ellipsoids etc.) by providing the coefficients of the general quadric surface equation: which in general can be written also using a coefficient matrix and a row vector My solid modeler already provides various different objects like cylinders, ellipsoids etc. but they are defined in a canonical system (e.g. cylinder parallel to the axis) and carry a transformation matrix to place them at the right position for rendering. In order to treat bodies defined by general quadrics I have to do two things: 1.) Determine which kind of surface has been defined 2.) Obtain the transformation matrix which will transform the respective surface to the position/orientation which is implicitly included in the quadrics coefficients. After doing a bit of algebra I came up with the following idea for 2.) I thought to tackle the issue via principal axis transformation. So I would first try to determine the eigenvalues and the eigenvectors for the coefficient matrix The matrix which is represented by the eigenvectors in its columns should transform my coefficient matrix into the canonical system. If I take the inverse of this eigenmatrix then this should be my transformation matrix, or am I wrong here? The eigenvalues would eventually determine the magnitude of parameters like the radius for spheres & cylinders, the major or minor halfaxes for ellipsoids etc. I'd appreciate to hear your thoughts on this or if somebody has a simpler solution I'd be happy to discuss this as well. Thanks a lot Chris","ax^2 + 2bxy + 2cxz + 2dx + \dots=0 Q v: v Q v^t \,z\, Q.","['matrices', 'geometry', 'eigenvalues-eigenvectors']"
28,Is there a way to exploit the fact that the covariance matrix has a  blocked structure to more easily compute the multivariate normal density?,Is there a way to exploit the fact that the covariance matrix has a  blocked structure to more easily compute the multivariate normal density?,,"I'm trying to minimize the (negative) multivariate normal log likelihood (dropping constants): $$ \log |\boldsymbol\Sigma|\,+(\mathbf{x}-\boldsymbol\mu)^{\rm T}\boldsymbol\Sigma^{-1}(\mathbf{x}-\boldsymbol\mu)$$ where $$ \Sigma_{ij} = \sigma_1 \cdot \mathcal{I}\{i = j \} + \sigma_2 \cdot \mathcal{I}\{ L_i = L_j \} + \sigma_3 \cdot f(||L_i - L_j||)$$ as a function of $\{ \sigma_1, \sigma_2, \sigma_3, {\boldsymbol \mu} \}$. The $L_i$ are known values - in the context of the problem it is the ""location"" of unit $i$. The function $f(\cdot)$ is a known monotonically decreasing function. $\mathcal{I}(\cdot)$ is an indicator function and $|| \cdot ||$ is a distance measure (say, euclidean distance). Each of the $\sigma_i$ parameters are positive so that $\Sigma$ is certainly positive definite. In this problem the dimension of $\Sigma$ is very large, say $1000 \times 1000$, so naively evaluating the log-likelihood is relatively computationally expensive ($\approx$ 2 seconds per evaluation). I'm thinking that, since $\Sigma$ has this blocked structure, there may be some way exploit this fact to significantly speed up computation, but I'm having some trouble figuring how/if this will work. Any tips are appreciated. Update: I can see that $\Sigma$ can be written as $$ \Sigma = \sigma_1 {\bf I}+ {\bf C} \otimes {\bf 1} $$ where ${\bf 1}$ is a matrix of $1$s, ${\bf I}$ is the identity, $\otimes$ denotes the Kronecker product, and ${\bf C}$ is an $N \times N$ matrix, where $N$ is the number of ""locations"" and has the structure $$ {\bf C}_{nm} = \sigma_2 \cdot \mathcal{I} \{ n = m \} + \sigma_3 \cdot f( \delta_{nm} )$$ where $\delta_{nm}$ denotes the distance between location $n$ and $m$. Still not quite sure this helps me a ton. Will be back with more updates perhaps.","I'm trying to minimize the (negative) multivariate normal log likelihood (dropping constants): $$ \log |\boldsymbol\Sigma|\,+(\mathbf{x}-\boldsymbol\mu)^{\rm T}\boldsymbol\Sigma^{-1}(\mathbf{x}-\boldsymbol\mu)$$ where $$ \Sigma_{ij} = \sigma_1 \cdot \mathcal{I}\{i = j \} + \sigma_2 \cdot \mathcal{I}\{ L_i = L_j \} + \sigma_3 \cdot f(||L_i - L_j||)$$ as a function of $\{ \sigma_1, \sigma_2, \sigma_3, {\boldsymbol \mu} \}$. The $L_i$ are known values - in the context of the problem it is the ""location"" of unit $i$. The function $f(\cdot)$ is a known monotonically decreasing function. $\mathcal{I}(\cdot)$ is an indicator function and $|| \cdot ||$ is a distance measure (say, euclidean distance). Each of the $\sigma_i$ parameters are positive so that $\Sigma$ is certainly positive definite. In this problem the dimension of $\Sigma$ is very large, say $1000 \times 1000$, so naively evaluating the log-likelihood is relatively computationally expensive ($\approx$ 2 seconds per evaluation). I'm thinking that, since $\Sigma$ has this blocked structure, there may be some way exploit this fact to significantly speed up computation, but I'm having some trouble figuring how/if this will work. Any tips are appreciated. Update: I can see that $\Sigma$ can be written as $$ \Sigma = \sigma_1 {\bf I}+ {\bf C} \otimes {\bf 1} $$ where ${\bf 1}$ is a matrix of $1$s, ${\bf I}$ is the identity, $\otimes$ denotes the Kronecker product, and ${\bf C}$ is an $N \times N$ matrix, where $N$ is the number of ""locations"" and has the structure $$ {\bf C}_{nm} = \sigma_2 \cdot \mathcal{I} \{ n = m \} + \sigma_3 \cdot f( \delta_{nm} )$$ where $\delta_{nm}$ denotes the distance between location $n$ and $m$. Still not quite sure this helps me a ton. Will be back with more updates perhaps.",,"['matrices', 'statistics', 'normal-distribution', 'numerical-linear-algebra', 'block-matrices']"
29,Submatrix Notation,Submatrix Notation,,"I'm looking through some computer science papers and I see some notation that I'm just not familiar with. Consider an 5 x 6 matrix $$G =  \begin{pmatrix}          a_{0,0} & a_{0,1} & a_{0,2} & a_{0,3} & a_{0,4} & a_{0,5} \\          a_{1,0} & a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5} \\          a_{2,0} & a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5} \\          a_{3,0} & a_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5} \\          a_{4,0} & a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4} & a_{4,5} \\        \end{pmatrix}$$ If I wrote down $G[1,3; 2,5]$ does that mean row 1 to row3 inclusive and col 2 to col 5 inclusive: $$G[1,3; 2,5] = \begin{pmatrix}                   a_{0,1} & a_{0,2} & a_{0,3} & a_{0,4} \\                   a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\                   a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\                 \end{pmatrix}$$ Or (zero indexed version of previous): $$G[1,3; 2,5] = \begin{pmatrix}                   a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5} \\                   a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5} \\                   a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5} \\                 \end{pmatrix}$$ Or the rectangle made by the element at row 1 col 3 to the element at row 2 col 5: $$G[1,3; 2,5] = \begin{pmatrix}                   a_{0,2} & a_{0,3} & a_{0,4} \\                   a_{1,2} & a_{1,3} & a_{1,4} \\                 \end{pmatrix}$$ Or (zero indexed version of previous): $$G[1,3; 2,5] = \begin{pmatrix}                   a_{1,3} & a_{1,4} & a_{1,5} \\                   a_{2,3} & a_{2,4} & a_{2,5} \\                 \end{pmatrix}$$ Or the intersection of rows 1 and 3 with the intersection of rows 2 and 5: $$G[1,3; 2,5] = \begin{pmatrix}                   a_{0,1} & a_{0,4} \\                   a_{2,1} & a_{2,4} \\                 \end{pmatrix}$$ Or (zero indexed version of previous): $$G[1,3; 2,5] = \begin{pmatrix}                   a_{1,2} & a_{1,5} \\                   a_{3,2} & a_{3,5} \\                 \end{pmatrix}$$ Or the same thing but specified row, col; row, col (like possibility 3 and 4) $$G[1,3; 2,5] = \begin{pmatrix}                   a_{0,2} & a_{0,4} \\                   a_{1,2} & a_{1,4} \\                 \end{pmatrix}$$ Or (zero indexed version): $$G[1,3; 2,5] = \begin{pmatrix}                   a_{1,3} & a_{1,5} \\                   a_{2,3} & a_{2,5} \\                 \end{pmatrix}$$ Sorry, if this is a rather elementary question, I was simply unfamiliar with the notation and I couldn't find any information on the Internet about it.","I'm looking through some computer science papers and I see some notation that I'm just not familiar with. Consider an 5 x 6 matrix $$G =  \begin{pmatrix}          a_{0,0} & a_{0,1} & a_{0,2} & a_{0,3} & a_{0,4} & a_{0,5} \\          a_{1,0} & a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5} \\          a_{2,0} & a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5} \\          a_{3,0} & a_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5} \\          a_{4,0} & a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4} & a_{4,5} \\        \end{pmatrix}$$ If I wrote down $G[1,3; 2,5]$ does that mean row 1 to row3 inclusive and col 2 to col 5 inclusive: $$G[1,3; 2,5] = \begin{pmatrix}                   a_{0,1} & a_{0,2} & a_{0,3} & a_{0,4} \\                   a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\                   a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\                 \end{pmatrix}$$ Or (zero indexed version of previous): $$G[1,3; 2,5] = \begin{pmatrix}                   a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5} \\                   a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5} \\                   a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5} \\                 \end{pmatrix}$$ Or the rectangle made by the element at row 1 col 3 to the element at row 2 col 5: $$G[1,3; 2,5] = \begin{pmatrix}                   a_{0,2} & a_{0,3} & a_{0,4} \\                   a_{1,2} & a_{1,3} & a_{1,4} \\                 \end{pmatrix}$$ Or (zero indexed version of previous): $$G[1,3; 2,5] = \begin{pmatrix}                   a_{1,3} & a_{1,4} & a_{1,5} \\                   a_{2,3} & a_{2,4} & a_{2,5} \\                 \end{pmatrix}$$ Or the intersection of rows 1 and 3 with the intersection of rows 2 and 5: $$G[1,3; 2,5] = \begin{pmatrix}                   a_{0,1} & a_{0,4} \\                   a_{2,1} & a_{2,4} \\                 \end{pmatrix}$$ Or (zero indexed version of previous): $$G[1,3; 2,5] = \begin{pmatrix}                   a_{1,2} & a_{1,5} \\                   a_{3,2} & a_{3,5} \\                 \end{pmatrix}$$ Or the same thing but specified row, col; row, col (like possibility 3 and 4) $$G[1,3; 2,5] = \begin{pmatrix}                   a_{0,2} & a_{0,4} \\                   a_{1,2} & a_{1,4} \\                 \end{pmatrix}$$ Or (zero indexed version): $$G[1,3; 2,5] = \begin{pmatrix}                   a_{1,3} & a_{1,5} \\                   a_{2,3} & a_{2,5} \\                 \end{pmatrix}$$ Sorry, if this is a rather elementary question, I was simply unfamiliar with the notation and I couldn't find any information on the Internet about it.",,"['matrices', 'notation']"
30,Second eigenvalue of a stochastic block matrix,Second eigenvalue of a stochastic block matrix,,"Considering a stochastic block matrix in the form of, $$\textbf{$P_{}$} = \begin{pmatrix} \textbf{$A_{}$} & \textbf{$B_{}$} \\ \textbf{$B_{}$} & \textbf{$A_{}$} \end{pmatrix}$$ I found out that the second largest eigenvalue $\lambda_{2}(P_{})$ of $P$ can be derived from, $$\lambda_{2}(P_{}) = \max \Big\{\lambda_{2}{(A_{}+B_{})}, \lambda_{\max}(A_{}-B_{})\Big\}$$ This can be easily done using the characteristic polynomial and properties of determinants. Since I'm involved in a paper which uses this property, I'd like to find this out in any reference (if available) to minimize the use of proof in the paper by refering it which I didn't succeed in finding out. Does anyone know any reference which has this proof done?","Considering a stochastic block matrix in the form of, I found out that the second largest eigenvalue of can be derived from, This can be easily done using the characteristic polynomial and properties of determinants. Since I'm involved in a paper which uses this property, I'd like to find this out in any reference (if available) to minimize the use of proof in the paper by refering it which I didn't succeed in finding out. Does anyone know any reference which has this proof done?","\textbf{P_{}} = \begin{pmatrix} \textbf{A_{}} & \textbf{B_{}} \\ \textbf{B_{}} & \textbf{A_{}} \end{pmatrix} \lambda_{2}(P_{}) P \lambda_{2}(P_{}) = \max \Big\{\lambda_{2}{(A_{}+B_{})}, \lambda_{\max}(A_{}-B_{})\Big\}","['matrices', 'reference-request', 'eigenvalues-eigenvectors', 'block-matrices', 'stochastic-matrices']"
31,Smith normal form of graded modules. (Major edit),Smith normal form of graded modules. (Major edit),,"Ok, this is a major rewriting of my previous entry which no one answered. Let us have two graded $F[t]$-modules M and N with bases $m_1, \ldots, m_m$ and $n_1, \ldots, n_n$, respectively, and $F$ is a field ($F[t]$ is a PID). Note that $m_i$ and $n_j$ are homogeneous elements. We are now interested in calculating the image of a certain 0-degree graded homomorphism from $M$ to $N$. Note that  $$M \simeq F[t]\deg(m_1)\oplus\cdots\oplus F[t]\deg(m_m)$$ and similarly for $N$. The grading is the standard grading, i.e. $t(c_0, c_1, \ldots) = (0,c_0, c_1, \ldots)$. Now let $\partial$ be our homomorphism and the basis elements of $M$ and $N$ with degrees in () are given by:  $$ab(1), bc(1), cd(2), ad(2), ac(3)$$ for $M$ and  $$a(0), b(0), c(1), d(1)$$ for $N$. Then $\partial$ is defined by  $$\partial(ab) = t^{\deg(ab)-\deg(b)}b - t^{\deg(ab)-\deg(a)}a = tb-ta$$ and exactly the same for the rest,  $$\partial(bc) = c - tb$$ $$\partial(cd) = td - tc$$ $$\partial(ad) = td - t^2a$$ $$\partial(ac) = t^2c - t^3a$$ Now sorting the basis elements of $N$ in descending order $d,c,b,a$ we can represent $\partial$ by $$  \begin{pmatrix}   * & ab & bc & cd & ad & ac \\   d & 0 & 0 & t & t & 0 \\   c & 0 & 1 & t & 0 & t^2 \\   b & t& t & 0 & 0 & 0 \\   a & t & 0 & 0 & t^2 & t^3  \end{pmatrix} $$ Using column operations we can keep homogeneous bases and reduce the matrix to column echelon form $$  \begin{pmatrix}   * & cd & bc & ab & z_1 & z_2 \\   d & t & 0 & 0 & 0 & 0 \\   c & t & 1 & 0 & 0 & 0 \\   b & 0& t & t & 0 & 0 \\   a & 0 & 0 & t & 0 & 0  \end{pmatrix} $$ where $z_1 = ad - cd - t\cdot bc - t\cdot ab$ and $z_2 = ac - t^2\cdot bc - t^2\cdot ab$ form homogenous basis for the kernel. Note that we have that for an entry in the matrix that the degree of the element at that position + the degree of the row basis element = the degree of column basis element. Now the author of the paper argues: The pivots in column-echelon form are the same as the diagonal elements in Smith normal form. Moreover, the degree of the basis elements on pivot rows is the same in both forms. With proof: Because of our sort, the degree of row basis elements is monotonically decreasing from the top rown down. Within each fixed column $j$ the degree of the column basis element is constant equal to $c$ and therefore $\deg\partial_{i,j} = c - \deg(\text{row}~i)$. Therefore, the degree of the elements in each column is monotonically increasing with row. We may eliminate non-zero elements below pivots using row operations that do not change the pivot elements or the degrees of the row basis elements. We then place the matrix in diagonal form with row and column swaps. $\square$ How is it possible to do row operations WITHOUT altering the degree and keeping a homogeneous basis element? Am I missing something obvious? Note that this proof shall hold for any such $\partial$ where the degree of the row + degree of element is equal to the degree of the column. Another example would be  $$\begin{pmatrix} * & ab \\ a & t \\ b & t^2 \end{pmatrix}$$ where $\deg(ab) = 3, \deg(a) = 2, \deg(b) = 1$. How would even that be possible... What we are really interested in is the image of $\partial$. This becomes $$\deg(d)tF(t)\oplus \deg(c)F(t)\oplus \deg(b)tF(t)$$ according to the statement and matrix above. I do, however, believe that the result is true and I think I can give a proof for it: Assume that we have column-echelon-form. Then the degree of the homogenous elements along each column increases as we go top rown down. We may also assume that along each row the degree of the pivot element is greater than the other elements on the row, if not, use column operations to remove the element with greater degree. This gives us a matrix of the form (assuming just 2 elements for simplicitiy) $$ \begin{pmatrix} * & m_1 & m_2 \\ n_1 & t^{\beta_1^1}  & 0 \\ n_2 & t^{\beta_2^1} & t^{\beta_2^2} \end{pmatrix}$$ where $\beta_2^2 \geq \beta_2^1 \geq \beta_1^1$. Then use $n_1$ to remove first coordinate of $n_2$. The image then becomes: $$m_1 \to t^{\beta_1^1}n_1\cdot f(t)$$ where $f(t) \in F(t)$.  $$m_2 \to t^{\beta_2^2}(n_2 - n_1\cdot t^{\beta_2^1 - \beta_1^1})g(t)$$ where $g(t) \in F(t)$. Writing in terms of the basis elements of the codomain we have image equal to $$n_1(t^{\beta_1^1}f(t) - t^{\beta_2^2 +\beta_2^1-\beta_1^1}g(t))$$ $$n_2t^{\beta_2^2}$$ and since $\beta_2^2 \geq \beta_1^1$ we have that this is isomorphic to $$\deg(n_1)t^{\beta_1^1}F(t)\oplus \deg(n_2)t^{\beta_2^2}F(t)$$ and the proof generalizes in an obvious way to higher dimensions. Non-pivot rows are skipped. Source: http://comptop.stanford.edu/preprints/persistence1.pdf Chapter 4.1","Ok, this is a major rewriting of my previous entry which no one answered. Let us have two graded $F[t]$-modules M and N with bases $m_1, \ldots, m_m$ and $n_1, \ldots, n_n$, respectively, and $F$ is a field ($F[t]$ is a PID). Note that $m_i$ and $n_j$ are homogeneous elements. We are now interested in calculating the image of a certain 0-degree graded homomorphism from $M$ to $N$. Note that  $$M \simeq F[t]\deg(m_1)\oplus\cdots\oplus F[t]\deg(m_m)$$ and similarly for $N$. The grading is the standard grading, i.e. $t(c_0, c_1, \ldots) = (0,c_0, c_1, \ldots)$. Now let $\partial$ be our homomorphism and the basis elements of $M$ and $N$ with degrees in () are given by:  $$ab(1), bc(1), cd(2), ad(2), ac(3)$$ for $M$ and  $$a(0), b(0), c(1), d(1)$$ for $N$. Then $\partial$ is defined by  $$\partial(ab) = t^{\deg(ab)-\deg(b)}b - t^{\deg(ab)-\deg(a)}a = tb-ta$$ and exactly the same for the rest,  $$\partial(bc) = c - tb$$ $$\partial(cd) = td - tc$$ $$\partial(ad) = td - t^2a$$ $$\partial(ac) = t^2c - t^3a$$ Now sorting the basis elements of $N$ in descending order $d,c,b,a$ we can represent $\partial$ by $$  \begin{pmatrix}   * & ab & bc & cd & ad & ac \\   d & 0 & 0 & t & t & 0 \\   c & 0 & 1 & t & 0 & t^2 \\   b & t& t & 0 & 0 & 0 \\   a & t & 0 & 0 & t^2 & t^3  \end{pmatrix} $$ Using column operations we can keep homogeneous bases and reduce the matrix to column echelon form $$  \begin{pmatrix}   * & cd & bc & ab & z_1 & z_2 \\   d & t & 0 & 0 & 0 & 0 \\   c & t & 1 & 0 & 0 & 0 \\   b & 0& t & t & 0 & 0 \\   a & 0 & 0 & t & 0 & 0  \end{pmatrix} $$ where $z_1 = ad - cd - t\cdot bc - t\cdot ab$ and $z_2 = ac - t^2\cdot bc - t^2\cdot ab$ form homogenous basis for the kernel. Note that we have that for an entry in the matrix that the degree of the element at that position + the degree of the row basis element = the degree of column basis element. Now the author of the paper argues: The pivots in column-echelon form are the same as the diagonal elements in Smith normal form. Moreover, the degree of the basis elements on pivot rows is the same in both forms. With proof: Because of our sort, the degree of row basis elements is monotonically decreasing from the top rown down. Within each fixed column $j$ the degree of the column basis element is constant equal to $c$ and therefore $\deg\partial_{i,j} = c - \deg(\text{row}~i)$. Therefore, the degree of the elements in each column is monotonically increasing with row. We may eliminate non-zero elements below pivots using row operations that do not change the pivot elements or the degrees of the row basis elements. We then place the matrix in diagonal form with row and column swaps. $\square$ How is it possible to do row operations WITHOUT altering the degree and keeping a homogeneous basis element? Am I missing something obvious? Note that this proof shall hold for any such $\partial$ where the degree of the row + degree of element is equal to the degree of the column. Another example would be  $$\begin{pmatrix} * & ab \\ a & t \\ b & t^2 \end{pmatrix}$$ where $\deg(ab) = 3, \deg(a) = 2, \deg(b) = 1$. How would even that be possible... What we are really interested in is the image of $\partial$. This becomes $$\deg(d)tF(t)\oplus \deg(c)F(t)\oplus \deg(b)tF(t)$$ according to the statement and matrix above. I do, however, believe that the result is true and I think I can give a proof for it: Assume that we have column-echelon-form. Then the degree of the homogenous elements along each column increases as we go top rown down. We may also assume that along each row the degree of the pivot element is greater than the other elements on the row, if not, use column operations to remove the element with greater degree. This gives us a matrix of the form (assuming just 2 elements for simplicitiy) $$ \begin{pmatrix} * & m_1 & m_2 \\ n_1 & t^{\beta_1^1}  & 0 \\ n_2 & t^{\beta_2^1} & t^{\beta_2^2} \end{pmatrix}$$ where $\beta_2^2 \geq \beta_2^1 \geq \beta_1^1$. Then use $n_1$ to remove first coordinate of $n_2$. The image then becomes: $$m_1 \to t^{\beta_1^1}n_1\cdot f(t)$$ where $f(t) \in F(t)$.  $$m_2 \to t^{\beta_2^2}(n_2 - n_1\cdot t^{\beta_2^1 - \beta_1^1})g(t)$$ where $g(t) \in F(t)$. Writing in terms of the basis elements of the codomain we have image equal to $$n_1(t^{\beta_1^1}f(t) - t^{\beta_2^2 +\beta_2^1-\beta_1^1}g(t))$$ $$n_2t^{\beta_2^2}$$ and since $\beta_2^2 \geq \beta_1^1$ we have that this is isomorphic to $$\deg(n_1)t^{\beta_1^1}F(t)\oplus \deg(n_2)t^{\beta_2^2}F(t)$$ and the proof generalizes in an obvious way to higher dimensions. Non-pivot rows are skipped. Source: http://comptop.stanford.edu/preprints/persistence1.pdf Chapter 4.1",,"['matrices', 'algebraic-topology']"
32,"If two matrices have the same determinant, are they similar? [closed]","If two matrices have the same determinant, are they similar? [closed]",,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I am wondering if we have two square matrices $A$ and $B$ and if $\det A = \det B$, then does an invertible matrix $P$ exist with  $$A = P^{-1} B P$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I am wondering if we have two square matrices $A$ and $B$ and if $\det A = \det B$, then does an invertible matrix $P$ exist with  $$A = P^{-1} B P$$",,"['matrices', 'determinant']"
33,How can I prove that a square matrix is invertible if it satisfies this polynomial equation?,How can I prove that a square matrix is invertible if it satisfies this polynomial equation?,,"For a 3x3 matrix $C$, it is given that $$C^3+I=3C^2-C$$ I am then required to prove that $C$ is invertible. I have attempted a proof, below, but I am not sure it is valid or if there is a better solution. Attempted proof $$C^3 + I = 3C^2 - C$$ $$I = - C^3 +3C^2-C$$ If it is assumed that $C^{-1}$ exists then $$I = C^{-1}(-C^4+3C^3-C^2)$$ If $C^{-1}$ is defined, then $I=C^{-1}C$; therefore test whether $$C \stackrel{!}{=} -C^4 + 3C^3 - C^2$$ $$ 0 = -C^4 + 3C^3 - C^2 - C$$ $$ C = 0, 1, 1\pm\sqrt{2}$$ Is this at all in the right direction?","For a 3x3 matrix $C$, it is given that $$C^3+I=3C^2-C$$ I am then required to prove that $C$ is invertible. I have attempted a proof, below, but I am not sure it is valid or if there is a better solution. Attempted proof $$C^3 + I = 3C^2 - C$$ $$I = - C^3 +3C^2-C$$ If it is assumed that $C^{-1}$ exists then $$I = C^{-1}(-C^4+3C^3-C^2)$$ If $C^{-1}$ is defined, then $I=C^{-1}C$; therefore test whether $$C \stackrel{!}{=} -C^4 + 3C^3 - C^2$$ $$ 0 = -C^4 + 3C^3 - C^2 - C$$ $$ C = 0, 1, 1\pm\sqrt{2}$$ Is this at all in the right direction?",,['matrices']
34,How can we show that $(I-A)$ is invertible?,How can we show that  is invertible?,(I-A),$A$ is an $n\times n$ matrix with $\|A\|≤a<1 $ . I need to prove that the matrix $(I-A)$ is invertible with $\|(I-A)^{-1}\|\le\frac1{(1-a)}$. It doesn't say anything more. The norm makes me confuse. How can we start to solve this. Could you please help?,$A$ is an $n\times n$ matrix with $\|A\|≤a<1 $ . I need to prove that the matrix $(I-A)$ is invertible with $\|(I-A)^{-1}\|\le\frac1{(1-a)}$. It doesn't say anything more. The norm makes me confuse. How can we start to solve this. Could you please help?,,['matrices']
35,Why does $A^2=I$ imply $nullity(A)=0$?,Why does  imply ?,A^2=I nullity(A)=0,"$A$ is a square matrix, why does $A^2=I$ imply $nullity(A)=0$? This is the key step in the solution, which I can't get it. Please help","$A$ is a square matrix, why does $A^2=I$ imply $nullity(A)=0$? This is the key step in the solution, which I can't get it. Please help",,['matrices']
36,"$A^{64}=A^{27}=I$, prove $A=I$",", prove",A^{64}=A^{27}=I A=I,"$A^{64}=A^{27}=I$ , prove $A=I$ . I tried finding the eigenvalues of A, by factoring $A^{64}-I$ but this method is too long. I'm not really sure what to do, I thought of showing $A$ is similar to $I$ but I don't know how to do that either.",", prove . I tried finding the eigenvalues of A, by factoring but this method is too long. I'm not really sure what to do, I thought of showing is similar to but I don't know how to do that either.",A^{64}=A^{27}=I A=I A^{64}-I A I,['matrices']
37,"Inverse of a matrix exponential, ${(e^{At})}^{-1} = {e^{-At}}$","Inverse of a matrix exponential,",{(e^{At})}^{-1} = {e^{-At}},Consider the matrix exponential $$ e^{At} = \frac{1}{4} \begin{bmatrix}       -e^{-t} + 5e^{3t} &  e^{-t} - e^{3t} \\      -5e^{-t} + 5e^{3t} & 5e^{-t} - e^{3t}   \end{bmatrix} $$ And $$ {(e^{At})}^{-1} =  {e^{-At}} $$ What does that identity mean?  Can I just multiply the exponent by $-1$ to find  $e^{-At}$?,Consider the matrix exponential $$ e^{At} = \frac{1}{4} \begin{bmatrix}       -e^{-t} + 5e^{3t} &  e^{-t} - e^{3t} \\      -5e^{-t} + 5e^{3t} & 5e^{-t} - e^{3t}   \end{bmatrix} $$ And $$ {(e^{At})}^{-1} =  {e^{-At}} $$ What does that identity mean?  Can I just multiply the exponent by $-1$ to find  $e^{-At}$?,,"['matrices', 'exponential-function', 'matrix-exponential']"
38,Matrix multiplication: is C(AB) the same as (CA)B?,Matrix multiplication: is C(AB) the same as (CA)B?,,"I would like to show that $(\mathbf{A} \mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}$, where $\mathbf{A}$ and $\mathbf{B}$ are $N \times N$ square matrices. I think that this can be done as follows: First, note that $(\mathbf{A}\mathbf{B})^{-1} (\mathbf{A} \mathbf{B}) = \mathbf{1}$ and also that $\mathbf{B}^{-1} \mathbf{A}^{-1} \mathbf{A} \mathbf{B} = \mathbf{B}^{-1} \mathbf{1} \mathbf{B} = \mathbf{B}^{-1} \mathbf{B} = \mathbf{1}$ (where $\mathbf{1}$ is the unit matrix). Thus $(\mathbf{A} \mathbf{B})^{-1} \mathbf{A} \mathbf{B} = \mathbf{B}^{-1} \mathbf{A}^{-1} \mathbf{A} \mathbf{B}$ which implies that $(\mathbf{A} \mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}$. I am not sure that this is correct. It seems almost too easy! The correctness of my working is based on my assumption that $\mathbf{C} (\mathbf{A}\mathbf{B}) = (\mathbf{C} \mathbf{A}) \mathbf{B}$. Is this correct? In normal linear algebra, this is the case. But is it the case in matrix multiplication? I am not sure whether the brackets result in a different order of multiplication and thus a different result.","I would like to show that $(\mathbf{A} \mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}$, where $\mathbf{A}$ and $\mathbf{B}$ are $N \times N$ square matrices. I think that this can be done as follows: First, note that $(\mathbf{A}\mathbf{B})^{-1} (\mathbf{A} \mathbf{B}) = \mathbf{1}$ and also that $\mathbf{B}^{-1} \mathbf{A}^{-1} \mathbf{A} \mathbf{B} = \mathbf{B}^{-1} \mathbf{1} \mathbf{B} = \mathbf{B}^{-1} \mathbf{B} = \mathbf{1}$ (where $\mathbf{1}$ is the unit matrix). Thus $(\mathbf{A} \mathbf{B})^{-1} \mathbf{A} \mathbf{B} = \mathbf{B}^{-1} \mathbf{A}^{-1} \mathbf{A} \mathbf{B}$ which implies that $(\mathbf{A} \mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}$. I am not sure that this is correct. It seems almost too easy! The correctness of my working is based on my assumption that $\mathbf{C} (\mathbf{A}\mathbf{B}) = (\mathbf{C} \mathbf{A}) \mathbf{B}$. Is this correct? In normal linear algebra, this is the case. But is it the case in matrix multiplication? I am not sure whether the brackets result in a different order of multiplication and thus a different result.",,['matrices']
39,Conjugation is not expressible in terms of polynomials,Conjugation is not expressible in terms of polynomials,,"In order to convince myself that the set $U(n)$ of unitary matrices (matrices with columns that are orthonormal under the complex inner product) is not an affine variety in $\mathbb{C}^{n^2}$, I need that the conjugate of an entry $\overline{x}_{ij}$ cannot be expressed using polynomials. If we say that complex polynomials have to be finite sums of monomials of the form $cx_1^{e_1}\dots x_n^{e_n}$ where $c \in \mathbb{C}$, $e_i \geq 0$, then is this really enough? It seems a little slippery.","In order to convince myself that the set $U(n)$ of unitary matrices (matrices with columns that are orthonormal under the complex inner product) is not an affine variety in $\mathbb{C}^{n^2}$, I need that the conjugate of an entry $\overline{x}_{ij}$ cannot be expressed using polynomials. If we say that complex polynomials have to be finite sums of monomials of the form $cx_1^{e_1}\dots x_n^{e_n}$ where $c \in \mathbb{C}$, $e_i \geq 0$, then is this really enough? It seems a little slippery.",,"['matrices', 'algebraic-geometry']"
40,Full rank vs short rank matrix,Full rank vs short rank matrix,,"I am given the definition: ""A matrix A is of full rank if and only if the vector $d$ for which $Ad=0$ is $d=0$."" I don't understand: if we have the matrix $$\begin{pmatrix}1&2&3\\   4&5&6\\   13&19&88\end{pmatrix}$$ It is not of full rank, but what number other than $0$ can we multiply it by to get $0$? The last line is just an example that is independent of the first two.","I am given the definition: ""A matrix A is of full rank if and only if the vector $d$ for which $Ad=0$ is $d=0$."" I don't understand: if we have the matrix $$\begin{pmatrix}1&2&3\\   4&5&6\\   13&19&88\end{pmatrix}$$ It is not of full rank, but what number other than $0$ can we multiply it by to get $0$? The last line is just an example that is independent of the first two.",,['matrices']
41,"$(\mathbf{u}^T\mathbf{v})\mathbf{v} = \mathbf{u}^T(\mathbf{v}\mathbf{v})$ doesn't hold for $\mathbf{u}, \mathbf{v}\in\mathbb{R}^n$ - why?",doesn't hold for  - why?,"(\mathbf{u}^T\mathbf{v})\mathbf{v} = \mathbf{u}^T(\mathbf{v}\mathbf{v}) \mathbf{u}, \mathbf{v}\in\mathbb{R}^n","Suppose I have vectors $\mathbf{u}$ and $\mathbf{v}$ in $\mathbb{R}^n$. It is well defined to write $5\mathbf{v}$ or $c\mathbf{v}$ for scalar $c$. Since the inner product of $\mathbf{u}$ and $\mathbf{v}$ is just a scalar: $$ \mathbf{u}^T\mathbf{v}\in\mathbb{R} $$ I can write $(\mathbf{u}^T\mathbf{v})\mathbf{v}$ which is also well defined. Then, since matrix multiplication is associative: $$ (\mathbf{u}^T\mathbf{v})\mathbf{v} = \mathbf{u}^T\mathbf{v}\mathbf{v} = \mathbf{u}^T(\mathbf{v}\mathbf{v}) $$ Which is obviously not defined for a column vector $\mathbf{v}$. What went wrong? What notation should I use to indicate the vector $(\mathbf{u}^T\mathbf{v})\mathbf{v}$? Writing it like that looks like the brackets are arbitrary and therefore looks wrong to me.","Suppose I have vectors $\mathbf{u}$ and $\mathbf{v}$ in $\mathbb{R}^n$. It is well defined to write $5\mathbf{v}$ or $c\mathbf{v}$ for scalar $c$. Since the inner product of $\mathbf{u}$ and $\mathbf{v}$ is just a scalar: $$ \mathbf{u}^T\mathbf{v}\in\mathbb{R} $$ I can write $(\mathbf{u}^T\mathbf{v})\mathbf{v}$ which is also well defined. Then, since matrix multiplication is associative: $$ (\mathbf{u}^T\mathbf{v})\mathbf{v} = \mathbf{u}^T\mathbf{v}\mathbf{v} = \mathbf{u}^T(\mathbf{v}\mathbf{v}) $$ Which is obviously not defined for a column vector $\mathbf{v}$. What went wrong? What notation should I use to indicate the vector $(\mathbf{u}^T\mathbf{v})\mathbf{v}$? Writing it like that looks like the brackets are arbitrary and therefore looks wrong to me.",,"['matrices', 'notation']"
42,Proof for infinity matrix norm,Proof for infinity matrix norm,,"I need to prove the $\infty$ -norm of a matrix, which is here $$ \| A \|_\infty = \max_{x \neq 0} \left\{\frac{\max\limits_{1 \le i\le n} |(Ax)_i|}{\max\limits_{1 \le i \le n} |x_i|}\right\}.$$ I can't find the the proof anywhere and would appreciate if someone could explain every step of the proof. Thank you!","I need to prove the -norm of a matrix, which is here I can't find the the proof anywhere and would appreciate if someone could explain every step of the proof. Thank you!",\infty  \| A \|_\infty = \max_{x \neq 0} \left\{\frac{\max\limits_{1 \le i\le n} |(Ax)_i|}{\max\limits_{1 \le i \le n} |x_i|}\right\}.,"['matrices', 'solution-verification', 'proof-explanation', 'normed-spaces', 'matrix-norms']"
43,Question about eigenvectors of real matrix with real eigenvalues,Question about eigenvectors of real matrix with real eigenvalues,,I have two related questions: Can a real matrix with real eigenvalues have complex eigenvectors? Is it always the case that a real matrix with real eigenvalues is diagonalisable?,I have two related questions: Can a real matrix with real eigenvalues have complex eigenvectors? Is it always the case that a real matrix with real eigenvalues is diagonalisable?,,"['matrices', 'eigenvalues-eigenvectors']"
44,Flipping a matrix?,Flipping a matrix?,,"Real quick question: I was wondering, how would one denote mathemathically the flipping of a matrix, horizontally or vertically, around its own axis?","Real quick question: I was wondering, how would one denote mathemathically the flipping of a matrix, horizontally or vertically, around its own axis?",,"['matrices', 'matrix-equations', 'matrix-calculus']"
45,how do you recognize a positive (semi)definite matrix?,how do you recognize a positive (semi)definite matrix?,,"I understand that the definition of (semi-)definiteness of matrix $A$ is $$\forall z_{\neq0}\in\mathbb R^k: z^TAz>0$$ I also know that this doesn't mean that all elements of a negative definite matrix $A$ are negative (in fact, they may all be positive or 0, such as with a 180 degree rotation in $\mathbb R^2$). Nevertheless, I'm wondering if there is a way to recognize, just by looking at the matrix, whether it is likely going to be a positive definite matrix? Is there a way to see this just from the matrix itself, or does it always require some form of computation first?","I understand that the definition of (semi-)definiteness of matrix $A$ is $$\forall z_{\neq0}\in\mathbb R^k: z^TAz>0$$ I also know that this doesn't mean that all elements of a negative definite matrix $A$ are negative (in fact, they may all be positive or 0, such as with a 180 degree rotation in $\mathbb R^2$). Nevertheless, I'm wondering if there is a way to recognize, just by looking at the matrix, whether it is likely going to be a positive definite matrix? Is there a way to see this just from the matrix itself, or does it always require some form of computation first?",,['matrices']
46,Faithful Representations of C*-algebras,Faithful Representations of C*-algebras,,"Can anyone give me an example of a represetation of the algebra $M_n(\mathbb{C})$ that is not faithul? If it's not possible, could you explain me why it is not?","Can anyone give me an example of a represetation of the algebra $M_n(\mathbb{C})$ that is not faithul? If it's not possible, could you explain me why it is not?",,"['matrices', 'operator-algebras', 'c-star-algebras']"
47,How to find 2D rotation matrix that rotates vector $\mathbf{a}$ to $\mathbf{b}$,How to find 2D rotation matrix that rotates vector  to,\mathbf{a} \mathbf{b},"I have two 2D unit vectors a and b . I'd like to find the rotation matrix that rotates a to b . The formulas I see online are for a rotation matrix are $$      \left(     \begin{matrix}     \cos \theta & - \sin \theta \\     \sin \theta &  \cos \theta \\     \end{matrix}     \right) $$ And I can get the angle between a and b with $$ \theta = \cos^{-1} (\mathbf{a} \cdot \mathbf{b}) $$ My problem is that that doesn't give me the direction. For example, if $\theta$ is $ \pi/2 $ when maybe the matrix should use $ -\pi/2 $","I have two 2D unit vectors a and b . I'd like to find the rotation matrix that rotates a to b . The formulas I see online are for a rotation matrix are And I can get the angle between a and b with My problem is that that doesn't give me the direction. For example, if is when maybe the matrix should use"," 
    \left(
    \begin{matrix}
    \cos \theta & - \sin \theta \\
    \sin \theta &  \cos \theta \\
    \end{matrix}
    \right)
 
\theta = \cos^{-1} (\mathbf{a} \cdot \mathbf{b})
 \theta  \pi/2   -\pi/2 ","['matrices', 'vectors', 'rotations']"
48,How to show the von Neumann trace inequality?,How to show the von Neumann trace inequality?,,"Let $A,B$ have the appropriate size. How can we show the von Neumann trace inequality? $$ \mbox{Tr}(AB) \leq \sum_{i=1}^n \sigma_{A,i}\sigma_{B,i} $$ Also, what is the intuition behind this inequality?","Let have the appropriate size. How can we show the von Neumann trace inequality? Also, what is the intuition behind this inequality?","A,B  \mbox{Tr}(AB) \leq \sum_{i=1}^n \sigma_{A,i}\sigma_{B,i} ","['matrices', 'inequality']"
49,left and right eigenvalues,left and right eigenvalues,,"On the Stochastic Matrices article in Wikipedia there's a claim that left and right eigenvalues of a square matrix are the same. I tried looking this up, but can't find an explanation, only for hermitian matrices with real eigenvalues. Is it correct?","On the Stochastic Matrices article in Wikipedia there's a claim that left and right eigenvalues of a square matrix are the same. I tried looking this up, but can't find an explanation, only for hermitian matrices with real eigenvalues. Is it correct?",,"['matrices', 'eigenvalues-eigenvectors']"
50,Differentiating $\mbox{tr} (ABA^TC)$ w.r.t. $A$ [duplicate],Differentiating  w.r.t.  [duplicate],\mbox{tr} (ABA^TC) A,"This question already has answers here : Gradient of $A \mapsto \operatorname{trace} (A B A' C)$ (5 answers) Closed 3 years ago . Why is $\nabla_A \mbox{tr} (ABA^TC) = CAB + C^TAB^T$? Here $A, B, C, D$ are all $n \times n$ matrices. $$\nabla_A f(A) = \left[\begin{matrix} \frac{\partial f}{\partial A_{11}}... \frac{\partial f}{\partial A_{1n}}\\ ...\\ \frac{\partial f}{\partial A_{n1}}... \frac{\partial f}{\partial A_{nn}}\\ \end{matrix}\right]$$ I tried to prove it in this way: $$\begin{align} \nabla_A \mbox{tr} (ABA^TC) &= \nabla_Atr (BA^TC)A\\ &= \nabla_A \mbox{tr} DA ......let  \ D=BA^TC\\ &= \nabla_A \mbox{tr} AD\\ &=D^T\\ &=B^TAC^T\end{align}$$ Since $B^TAC^T \neq CAB + C^TAB^T$, there must be something wrong in my derivation. How to prove this property?","This question already has answers here : Gradient of $A \mapsto \operatorname{trace} (A B A' C)$ (5 answers) Closed 3 years ago . Why is $\nabla_A \mbox{tr} (ABA^TC) = CAB + C^TAB^T$? Here $A, B, C, D$ are all $n \times n$ matrices. $$\nabla_A f(A) = \left[\begin{matrix} \frac{\partial f}{\partial A_{11}}... \frac{\partial f}{\partial A_{1n}}\\ ...\\ \frac{\partial f}{\partial A_{n1}}... \frac{\partial f}{\partial A_{nn}}\\ \end{matrix}\right]$$ I tried to prove it in this way: $$\begin{align} \nabla_A \mbox{tr} (ABA^TC) &= \nabla_Atr (BA^TC)A\\ &= \nabla_A \mbox{tr} DA ......let  \ D=BA^TC\\ &= \nabla_A \mbox{tr} AD\\ &=D^T\\ &=B^TAC^T\end{align}$$ Since $B^TAC^T \neq CAB + C^TAB^T$, there must be something wrong in my derivation. How to prove this property?",,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
51,Is symmetry a necessary condition for positive (or negative) definiteness?,Is symmetry a necessary condition for positive (or negative) definiteness?,,"Is symmetry a necessary condition for positive (or negative) definiteness? If not: It can be proved that if $\mathbf{A} \in \mathbb{R}^{m\times m}$ is a square (non-symmetric) matrix, then  $$ \mathbf{z'Az=z'Bz},~~\mathbf{B=B'= \frac{A+A'}{2}} $$ On the other hand, a positive definite matrix is a symmetric matrix for which: $$\mathbf{z'Bz}>0,~~ \mathbf{z\ne 0}$$ Can we imply that $\mathbf{A}$ which is a non-symmetric matrix, is positive definite?","Is symmetry a necessary condition for positive (or negative) definiteness? If not: It can be proved that if $\mathbf{A} \in \mathbb{R}^{m\times m}$ is a square (non-symmetric) matrix, then  $$ \mathbf{z'Az=z'Bz},~~\mathbf{B=B'= \frac{A+A'}{2}} $$ On the other hand, a positive definite matrix is a symmetric matrix for which: $$\mathbf{z'Bz}>0,~~ \mathbf{z\ne 0}$$ Can we imply that $\mathbf{A}$ which is a non-symmetric matrix, is positive definite?",,"['matrices', 'quadratic-forms', 'symmetric-matrices']"
52,Order of operations for multiplying three matrices,Order of operations for multiplying three matrices,,"If I have a $1\times 2$ matrix $A$, a $2\times 2$ matrix $B$, and a $2\times 2$ matrix $C$, and am asked to calculate ABC, is there a specific order in which I have to carry out the multiplication? Would I be correct in assuming I have to calculate $AB$ first to get a $1\times 2$ matrix, and then multiply that result by the $2\times 2$ matrix $C$? Or can you also calculate $BC$ first and then multiply that result by $A$?","If I have a $1\times 2$ matrix $A$, a $2\times 2$ matrix $B$, and a $2\times 2$ matrix $C$, and am asked to calculate ABC, is there a specific order in which I have to carry out the multiplication? Would I be correct in assuming I have to calculate $AB$ first to get a $1\times 2$ matrix, and then multiply that result by the $2\times 2$ matrix $C$? Or can you also calculate $BC$ first and then multiply that result by $A$?",,"['matrices', 'multiplicative-function']"
53,"Where we have used the condition that $ST=TS$, i.e, commutativity?","Where we have used the condition that , i.e, commutativity?",ST=TS,"definition Let $A$ be an $n\times n$ matrix. Then for $t\in \mathbb R$, $$e^{At}=\sum_{k=0}^\infty \frac{A^kt^k}{k!}\tag{1}$$ Proposition If $S$ and $T$ are linear transformations on $\mathbb R^n$ which commute, then $e^{S+T}=e^Se^T$. Proof By binomial therem, $$(S+T)^n=\sum_{j+k=0}^n n! \frac{S^jT^k}{j!k!}$$ therefore, $$e^{S+T}=\sum_{n=0}^\infty\sum_{j+k=0}^n \frac{S^jT^k}{j!k!}=\sum_{j=0}^\infty \frac {S^j}{j!}\sum_{k=0}^\infty \frac{T^k}{k!}=e^Se^T$$ In the above steps I have used cauchy product and binomial theorem and the absolute convergence of the two series. But My doubt is.. Where we have used the condition that $ST=TS$, i.e, commutativity?","definition Let $A$ be an $n\times n$ matrix. Then for $t\in \mathbb R$, $$e^{At}=\sum_{k=0}^\infty \frac{A^kt^k}{k!}\tag{1}$$ Proposition If $S$ and $T$ are linear transformations on $\mathbb R^n$ which commute, then $e^{S+T}=e^Se^T$. Proof By binomial therem, $$(S+T)^n=\sum_{j+k=0}^n n! \frac{S^jT^k}{j!k!}$$ therefore, $$e^{S+T}=\sum_{n=0}^\infty\sum_{j+k=0}^n \frac{S^jT^k}{j!k!}=\sum_{j=0}^\infty \frac {S^j}{j!}\sum_{k=0}^\infty \frac{T^k}{k!}=e^Se^T$$ In the above steps I have used cauchy product and binomial theorem and the absolute convergence of the two series. But My doubt is.. Where we have used the condition that $ST=TS$, i.e, commutativity?",,"['matrices', 'exponential-function']"
54,Verifying eigenvalues,Verifying eigenvalues,,"How would you check whether eigenvalues $\lambda_1=8$, $\lambda_2=3$, $\lambda_3=-1$ belong to a matrix? $$         \begin{matrix}         7 & 1 & 1\\         3 & 1 & 2 \\         1 & 3 & 2 \\         \end{matrix} $$","How would you check whether eigenvalues $\lambda_1=8$, $\lambda_2=3$, $\lambda_3=-1$ belong to a matrix? $$         \begin{matrix}         7 & 1 & 1\\         3 & 1 & 2 \\         1 & 3 & 2 \\         \end{matrix} $$",,"['matrices', 'eigenvalues-eigenvectors']"
55,Gradient of $x^{T}Ax$ [duplicate],Gradient of  [duplicate],x^{T}Ax,"This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I just came across the following $$\nabla x^TAx = 2Ax$$ which seems like as good of a guess as any, but it certainly wasn't discussed in either my linear algebra class or my multivariable calculus class. Is there any intuitive way to see why this should be true?","This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I just came across the following which seems like as good of a guess as any, but it certainly wasn't discussed in either my linear algebra class or my multivariable calculus class. Is there any intuitive way to see why this should be true?",\nabla x^TAx = 2Ax,"['matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus', 'quadratic-forms']"
56,Can we have a matrix whose elements are other matrices as well as other things similar to sets?,Can we have a matrix whose elements are other matrices as well as other things similar to sets?,,"Basically, what I am asking is this: Is a matrix just like an ordered sequence, of which the elements can be anything? It would seem silly to restrict a matrix down to limited uses such as only for numbers, but I get the impression that I cant multiply two matrices whose elements are the planets in our solar system. So we only allow numbers in matrices. Does this also mean that I cannot have a matrix of matrices? I feel like I should be able to since if I wanted to multiply two matrices of matrices, then although tedious by hand it can be done since I can add/subtract and multiply normal matrices. Although I do not know many of the other operations so I do not know if everything else can be done with such constructs.","Basically, what I am asking is this: Is a matrix just like an ordered sequence, of which the elements can be anything? It would seem silly to restrict a matrix down to limited uses such as only for numbers, but I get the impression that I cant multiply two matrices whose elements are the planets in our solar system. So we only allow numbers in matrices. Does this also mean that I cannot have a matrix of matrices? I feel like I should be able to since if I wanted to multiply two matrices of matrices, then although tedious by hand it can be done since I can add/subtract and multiply normal matrices. Although I do not know many of the other operations so I do not know if everything else can be done with such constructs.",,['matrices']
57,How to find the sum of all entries in the matrix $A^5$,How to find the sum of all entries in the matrix,A^5,Let $A$ be a $4 × 4$ matrix with non-negative entries such that the sum of the entries in each row of $A$ equals $1$. Find the sum of all entries in the matrix $A^5$. If $A=I_4$ then $A^5=I_4$ and sum of all entries in the matrix $A^5=4$. But how I show the general result. Please help.,Let $A$ be a $4 × 4$ matrix with non-negative entries such that the sum of the entries in each row of $A$ equals $1$. Find the sum of all entries in the matrix $A^5$. If $A=I_4$ then $A^5=I_4$ and sum of all entries in the matrix $A^5=4$. But how I show the general result. Please help.,,['matrices']
58,Is the limit of power of a stochastic matrix still a stochastic matrix?,Is the limit of power of a stochastic matrix still a stochastic matrix?,,"Suppose $A$ is a right stochastic matrix, which is defined as a square matrix each of whose rows consists of nonnegative real numbers, with each row summing to 1. If $\lim_{n \rightarrow \infty} A^n$ exists, is the limit also a right stochastic matrix? If not, then if $\lim_{n \rightarrow     \infty} A^n$ exists and the rows in the limit are identical, is the limit still a right stochastic matrix? I am considering the first part of this theorem from Ross as a counterexample where every element in the limit is 0, and thus the sum in each row is not 1. But I guess in that case the dimension of the matrix $A$ must be countably infinite (although not written out explicitly there and my guess can be wrong) and wonder if a stochastic matrix can be defined for infinite dimension? if not to the first question in Part 2, is it wrong to say that the limit distribution of a discrete-time Markov chain with $A$ as its transition matrix is defined as the identical rows of $\lim_{n     \rightarrow \infty} A^n$ if the limit exists and its rows are identical? If yes, what is the proper definition for the limit distribution considering the counterexample in Part 2? Thanks and regards!","Suppose $A$ is a right stochastic matrix, which is defined as a square matrix each of whose rows consists of nonnegative real numbers, with each row summing to 1. If $\lim_{n \rightarrow \infty} A^n$ exists, is the limit also a right stochastic matrix? If not, then if $\lim_{n \rightarrow     \infty} A^n$ exists and the rows in the limit are identical, is the limit still a right stochastic matrix? I am considering the first part of this theorem from Ross as a counterexample where every element in the limit is 0, and thus the sum in each row is not 1. But I guess in that case the dimension of the matrix $A$ must be countably infinite (although not written out explicitly there and my guess can be wrong) and wonder if a stochastic matrix can be defined for infinite dimension? if not to the first question in Part 2, is it wrong to say that the limit distribution of a discrete-time Markov chain with $A$ as its transition matrix is defined as the identical rows of $\lim_{n     \rightarrow \infty} A^n$ if the limit exists and its rows are identical? If yes, what is the proper definition for the limit distribution considering the counterexample in Part 2? Thanks and regards!",,"['matrices', 'stochastic-processes']"
59,Rotate and scale a point around different origins,Rotate and scale a point around different origins,,"I am trying to rotate an arbitrary, 2D point (x,y) around another point (a,b), and at the same time, scale it from a different point (c,d). To transform the point, I must use a 3x3 transformation matrix. For example, say I have a rectangle. I want to rotate its topleft point around its center, and scale it around its topleft: Given point: \begin{pmatrix} x\\ y \end{pmatrix} rotational origin: \begin{pmatrix} a\\ b \end{pmatrix} and scaling origin: \begin{pmatrix} c\\ d \end{pmatrix} (1) rotate (x,y) around (a,b) (2) scale (x,y) from (c,d) I know I can transform it around one origin using homogeneous coordinates, but how I can incorporate different origins for scaling and rotating in just one transformation matrix?","I am trying to rotate an arbitrary, 2D point (x,y) around another point (a,b), and at the same time, scale it from a different point (c,d). To transform the point, I must use a 3x3 transformation matrix. For example, say I have a rectangle. I want to rotate its topleft point around its center, and scale it around its topleft: Given point: rotational origin: and scaling origin: (1) rotate (x,y) around (a,b) (2) scale (x,y) from (c,d) I know I can transform it around one origin using homogeneous coordinates, but how I can incorporate different origins for scaling and rotating in just one transformation matrix?",\begin{pmatrix} x\\ y \end{pmatrix} \begin{pmatrix} a\\ b \end{pmatrix} \begin{pmatrix} c\\ d \end{pmatrix},"['matrices', 'rotations', 'geometric-transformation']"
60,Find the matrix projection of a symmetric matrix onto the set of symmetric positive semi definite (PSD) matrices,Find the matrix projection of a symmetric matrix onto the set of symmetric positive semi definite (PSD) matrices,,"Consider $\mathbb{S}^n$, the set of all $n\times n$ real symmetric matrices. Let $A\in \mathbb{S}^n$ and $A=U\Lambda U^{\top}$ be its spectral decomposition. I want to know how to prove that $\Pi_{\Bbb S_+^n}(A)=U\Lambda ^+U^{\top}$, where $\Lambda^+$ is the $n\times n$ diagonal matrix given by $\Lambda_{ii}^+=\max\{\Lambda_{ii}, 0\}$, for $i=1,...,n$；$\Pi_{S}(x)=\arg\min_{z\in S}\|x-z\|^2_2$. This projection means that we can project any symmetric matrix on $\mathbb{S}_+^n$ and get the closest positive semi-definite matrix, but how can we prove that it is the closest one?","Consider $\mathbb{S}^n$, the set of all $n\times n$ real symmetric matrices. Let $A\in \mathbb{S}^n$ and $A=U\Lambda U^{\top}$ be its spectral decomposition. I want to know how to prove that $\Pi_{\Bbb S_+^n}(A)=U\Lambda ^+U^{\top}$, where $\Lambda^+$ is the $n\times n$ diagonal matrix given by $\Lambda_{ii}^+=\max\{\Lambda_{ii}, 0\}$, for $i=1,...,n$；$\Pi_{S}(x)=\arg\min_{z\in S}\|x-z\|^2_2$. This projection means that we can project any symmetric matrix on $\mathbb{S}_+^n$ and get the closest positive semi-definite matrix, but how can we prove that it is the closest one?",,"['matrices', 'convex-optimization', 'positive-semidefinite', 'projection']"
61,Solve for unknown matrix,Solve for unknown matrix,,Let $A = \begin{bmatrix} 2 & 3 \\ 4 & 5         \end{bmatrix}$ and let $B =  \begin{bmatrix} 3 & 4 \\ 5 & 6         \end{bmatrix}$ Solve $A X = B$ for a matrix $X$ My guess is that i: let $X =  \begin{bmatrix} x \\ y         \end{bmatrix}$ Then solve it using a linear equation but i'm not too sure. Any help would be appreciated.,Let $A = \begin{bmatrix} 2 & 3 \\ 4 & 5         \end{bmatrix}$ and let $B =  \begin{bmatrix} 3 & 4 \\ 5 & 6         \end{bmatrix}$ Solve $A X = B$ for a matrix $X$ My guess is that i: let $X =  \begin{bmatrix} x \\ y         \end{bmatrix}$ Then solve it using a linear equation but i'm not too sure. Any help would be appreciated.,,"['matrices', 'matrix-equations']"
62,$\det(A \otimes B - B \otimes A) = 0$ why? Why $rk(M) = n^2-n$ ? Why  x and -x in Spec(M) ?,why? Why  ? Why  x and -x in Spec(M) ?,\det(A \otimes B - B \otimes A) = 0 rk(M) = n^2-n,"Let $A$, $B$ be $n\times n$ matrices. It seems $\det(A \otimes B - B \otimes A) = 0$. Moreover it seems that the kernel of $A \otimes B - B \otimes A$ contains $n$ vectors. Here is MatLab code to check: n=4; a=randn(n,n);b=randn(n,n);svd(kron(a,b)-kron(b,a)) You will see that the are exactly n zeros in svd, so the rank is n^2-n, for generic matrices. This should be very simple, but I cannot see why .. [EDIT].  If matrices A,B commute and have joint eigenbasis $v_i$. Then $v_i\otimes v_i$ generate the kernel. David Speyer provided correct answer which I accept. The answer given before is wrong. AoB-BoA (xoy+yox)=AxoBy+AyoBx -BxoAy- ByoAx is no zero by no means. Moreover if it would be true the kernel would be n(n-1)/2 dimensional. However it is n-dimensional typically as it  can be seen by experiments. [END EDIT]. [EDIT 2 ].  Additional observation  non-zero eigenvalues of $M= A \otimes B - B \otimes A$, comes in pairs: x and -x. How to explain ? [End EDIT] [EDIT 2] Here is some numertical example n=2; a=diag(rand(n,1)),  b=rand(n,n), m= kron(a,b)-kron(b,a), [v d]=eig(m), diag(d) a = 0.4494         0      0    0.6596 b = 0.7532    0.0292 0.8047    0.7798 m = 0    0.0131   -0.0131         0  0.3617   -0.1464         0   -0.0192 -0.3617         0    0.1464    0.0192 0   -0.5308    0.5308         0 v = 0.0232   -0.0232    0.0531   -0.0007 -0.3305   -0.0711    0.0000   -0.1309 0.0711    0.3305   -0.0000   -0.1309 -0.9409    0.9409    0.9986    0.9827 d = -0.2265         0         0         0 0    0.2265         0         0       0         0    0.0000         0       0         0         0    0.0000 ans = -0.2265     0.2265     0.0000     0.0000 [end edit] Motivation comes from this question on MO, c1=a , c2=b: https://mathoverflow.net/questions/97036/relaxing-commutativity-for-c1-c2-find-q1-q2-1-c1-c2q1c2-q2c1-2-q1-q20/","Let $A$, $B$ be $n\times n$ matrices. It seems $\det(A \otimes B - B \otimes A) = 0$. Moreover it seems that the kernel of $A \otimes B - B \otimes A$ contains $n$ vectors. Here is MatLab code to check: n=4; a=randn(n,n);b=randn(n,n);svd(kron(a,b)-kron(b,a)) You will see that the are exactly n zeros in svd, so the rank is n^2-n, for generic matrices. This should be very simple, but I cannot see why .. [EDIT].  If matrices A,B commute and have joint eigenbasis $v_i$. Then $v_i\otimes v_i$ generate the kernel. David Speyer provided correct answer which I accept. The answer given before is wrong. AoB-BoA (xoy+yox)=AxoBy+AyoBx -BxoAy- ByoAx is no zero by no means. Moreover if it would be true the kernel would be n(n-1)/2 dimensional. However it is n-dimensional typically as it  can be seen by experiments. [END EDIT]. [EDIT 2 ].  Additional observation  non-zero eigenvalues of $M= A \otimes B - B \otimes A$, comes in pairs: x and -x. How to explain ? [End EDIT] [EDIT 2] Here is some numertical example n=2; a=diag(rand(n,1)),  b=rand(n,n), m= kron(a,b)-kron(b,a), [v d]=eig(m), diag(d) a = 0.4494         0      0    0.6596 b = 0.7532    0.0292 0.8047    0.7798 m = 0    0.0131   -0.0131         0  0.3617   -0.1464         0   -0.0192 -0.3617         0    0.1464    0.0192 0   -0.5308    0.5308         0 v = 0.0232   -0.0232    0.0531   -0.0007 -0.3305   -0.0711    0.0000   -0.1309 0.0711    0.3305   -0.0000   -0.1309 -0.9409    0.9409    0.9986    0.9827 d = -0.2265         0         0         0 0    0.2265         0         0       0         0    0.0000         0       0         0         0    0.0000 ans = -0.2265     0.2265     0.0000     0.0000 [end edit] Motivation comes from this question on MO, c1=a , c2=b: https://mathoverflow.net/questions/97036/relaxing-commutativity-for-c1-c2-find-q1-q2-1-c1-c2q1c2-q2c1-2-q1-q20/",,"['matrices', 'tensors']"
63,How to compute $e^{At}$ with $A=\left ( \begin{array}{cc} -3 & 4 \\ -4 & -3 \end{array} \right )$,How to compute  with,e^{At} A=\left ( \begin{array}{cc} -3 & 4 \\ -4 & -3 \end{array} \right ),"If I wanted to calculate the matrix exponential of $\left ( \begin{array}{cc}  -3 & 4 \\ -4 & -3 \end{array} \right )$, I could do this by calculating the eigenvectors and values and then use $e^{At}=P\ \mbox{diag}(e^{\lambda t}) P^{-1}$ where $P$ is the matrix with the eigenvectors of $A$ as its columns and I get $$e^{At}= e^{-3t}\left ( \begin{array}{cc}    \cos(4t) & \sin(4t) \\ -\sin(4t) & \cos(4t) \end{array} \right )$$ but I was wondering how I do this using the definition, I'm sure it shouldn't be tough but I'm getting a bit lost; I have: $e^{At}=\sum\limits^{\infty}_{m=0} \frac{1}{m!}(At)^m$ but then looking at the powers of this matrix I just get rubbish i think? Thanks very much for any help.","If I wanted to calculate the matrix exponential of $\left ( \begin{array}{cc}  -3 & 4 \\ -4 & -3 \end{array} \right )$, I could do this by calculating the eigenvectors and values and then use $e^{At}=P\ \mbox{diag}(e^{\lambda t}) P^{-1}$ where $P$ is the matrix with the eigenvectors of $A$ as its columns and I get $$e^{At}= e^{-3t}\left ( \begin{array}{cc}    \cos(4t) & \sin(4t) \\ -\sin(4t) & \cos(4t) \end{array} \right )$$ but I was wondering how I do this using the definition, I'm sure it shouldn't be tough but I'm getting a bit lost; I have: $e^{At}=\sum\limits^{\infty}_{m=0} \frac{1}{m!}(At)^m$ but then looking at the powers of this matrix I just get rubbish i think? Thanks very much for any help.",,"['matrices', 'exponential-function', 'matrix-exponential']"
64,determining orthonormal matrix of rank N with special first row,determining orthonormal matrix of rank N with special first row,,"Is there a more efficient algorithm besides Gram-Schmidt that would produce an orthonormal matrix of rank N, with first row equal to [1 1 1 1 1 ... 1] / sqrt(N)? e.g. for N = 3, the matrix $\begin{align} \mathsf A_3 &= \begin{bmatrix}   1/\sqrt 3 & 1/\sqrt 3 & 1/\sqrt 3 \\   2/\sqrt 6 & -1/\sqrt 6 & -1/\sqrt 6 \\   0 & 1/\sqrt 2 & -1/\sqrt 2 \end{bmatrix}\end{align}$ suffices, but I'm not sure how to generalize.","Is there a more efficient algorithm besides Gram-Schmidt that would produce an orthonormal matrix of rank N, with first row equal to [1 1 1 1 1 ... 1] / sqrt(N)? e.g. for N = 3, the matrix $\begin{align} \mathsf A_3 &= \begin{bmatrix}   1/\sqrt 3 & 1/\sqrt 3 & 1/\sqrt 3 \\   2/\sqrt 6 & -1/\sqrt 6 & -1/\sqrt 6 \\   0 & 1/\sqrt 2 & -1/\sqrt 2 \end{bmatrix}\end{align}$ suffices, but I'm not sure how to generalize.",,"['matrices', 'algorithms', 'orthonormal']"
65,Does the set of 2x2 complex matrices form a field,Does the set of 2x2 complex matrices form a field,,"Let $C$ be the set of all the matrices of the form $$ C = \{\begin{pmatrix} z & -w \\ w   & z \end{pmatrix} \; | \; z,\ w \in \mathbb{C}\}. $$ My question would be if the $C$ forms a field with an addition ( $+$ ) and matrix multiplication ( $\times$ )? If not, why not? I went through all of the field axioms and couldn´t find the issue but it doesn´t seem right.","Let be the set of all the matrices of the form My question would be if the forms a field with an addition ( ) and matrix multiplication ( )? If not, why not? I went through all of the field axioms and couldn´t find the issue but it doesn´t seem right.","C  C = \{\begin{pmatrix}
z & -w \\ w 
 & z
\end{pmatrix} \; | \; z,\ w \in \mathbb{C}\}.  C + \times",['matrices']
66,x$A^{100 }$ where $A = \begin{bmatrix} 1 &2 \\ 3& 4 \end{bmatrix}$ [duplicate],x where  [duplicate],A^{100 } A = \begin{bmatrix} 1 &2 \\ 3& 4 \end{bmatrix},"This question already has answers here : Finding a 2x2 Matrix raised to the power of 1000 (6 answers) Closed 5 years ago . Compute $A^{100 }$ where   $A = \begin{bmatrix}  1 &2 \\ 3& 4 \end{bmatrix}$. I can  calculate $A^{100}$  using a calculator, but  my question  is that  is  there  any  short formula/method   or  is their any trick  to find the  $A^{100}$?","This question already has answers here : Finding a 2x2 Matrix raised to the power of 1000 (6 answers) Closed 5 years ago . Compute $A^{100 }$ where   $A = \begin{bmatrix}  1 &2 \\ 3& 4 \end{bmatrix}$. I can  calculate $A^{100}$  using a calculator, but  my question  is that  is  there  any  short formula/method   or  is their any trick  to find the  $A^{100}$?",,['matrices']
67,Vector field contour for a linear system,Vector field contour for a linear system,,"Let's say $\boldsymbol{\vec F}$ is a field vector with a liner relationship $$\boldsymbol{\vec F}(\boldsymbol{\vec  x})=\boldsymbol A \boldsymbol{\vec  x}$$ where $\boldsymbol{\vec  x}$ is a vector of size $n$ and $\boldsymbol A$ is a constant $n\times n$ square matrix. For a given point $\boldsymbol{\vec  x}_0$, how can I find the hyper-surface $S(\boldsymbol{\vec  x})=0$ which crosses $\boldsymbol{\vec  x}_0$ and it is perpendicular to the vector field $\boldsymbol{\vec F}$? I believe this there is an explicit form of the surface which depends on $\boldsymbol{\vec  x}_0$ and $\boldsymbol A$. On hyper-surface $S$, there will be $n-1$ degree of freedom. I am looking for a solution for an $n$ dimension vector field. This image is just for illustration. PS. This question is a special case (linear form) of my previous question .","Let's say $\boldsymbol{\vec F}$ is a field vector with a liner relationship $$\boldsymbol{\vec F}(\boldsymbol{\vec  x})=\boldsymbol A \boldsymbol{\vec  x}$$ where $\boldsymbol{\vec  x}$ is a vector of size $n$ and $\boldsymbol A$ is a constant $n\times n$ square matrix. For a given point $\boldsymbol{\vec  x}_0$, how can I find the hyper-surface $S(\boldsymbol{\vec  x})=0$ which crosses $\boldsymbol{\vec  x}_0$ and it is perpendicular to the vector field $\boldsymbol{\vec F}$? I believe this there is an explicit form of the surface which depends on $\boldsymbol{\vec  x}_0$ and $\boldsymbol A$. On hyper-surface $S$, there will be $n-1$ degree of freedom. I am looking for a solution for an $n$ dimension vector field. This image is just for illustration. PS. This question is a special case (linear form) of my previous question .",,"['matrices', 'vectors', 'vector-analysis', 'control-theory', 'vector-fields']"
68,Can an empty array be useful?,Can an empty array be useful?,,"Most computer programming languages have constructs for managing arrays of data, including multiple-dimensional arrays, which are clearly useful when storing, manipulating and modelling mathematical arrays/matrices. Most of these languages also support empty arrays, i.e. ones with a length of zero in at least one dimension. Are such arrays useful in mathematics, or are they simply a programming nicety? And do they have real-world applications?","Most computer programming languages have constructs for managing arrays of data, including multiple-dimensional arrays, which are clearly useful when storing, manipulating and modelling mathematical arrays/matrices. Most of these languages also support empty arrays, i.e. ones with a length of zero in at least one dimension. Are such arrays useful in mathematics, or are they simply a programming nicety? And do they have real-world applications?",,['matrices']
69,Constructing a graph from a degree sequence,Constructing a graph from a degree sequence,,"Let's say I'm given several degree sequences like $$\begin{array}{l} \{4,3,3,2,2\} \\ \{3,3,3,3\} \\ \{5,3,3,2,2,1\} \end{array}$$ I can find the number of edges using the handshaking lemma $$ \sum_{v \in V} \operatorname{deg}(v)=2|E| $$ But how do I construct a graph just given these degree sequences? There are multiple types of graphs that satisfy the degree sequences, but beyond guess & checking, is there a logic/pattern to follow when making graphs? Right now I'm at the guess & check stage.","Let's say I'm given several degree sequences like I can find the number of edges using the handshaking lemma But how do I construct a graph just given these degree sequences? There are multiple types of graphs that satisfy the degree sequences, but beyond guess & checking, is there a logic/pattern to follow when making graphs? Right now I'm at the guess & check stage.","\begin{array}{l}
\{4,3,3,2,2\} \\
\{3,3,3,3\} \\
\{5,3,3,2,2,1\}
\end{array} 
\sum_{v \in V} \operatorname{deg}(v)=2|E|
","['matrices', 'graph-theory']"
70,How to make a matrix a magic square?,How to make a matrix a magic square?,,"Suppose I have a matrix $$\begin{pmatrix} *  & 3 & 6\\ 5 & *  & 5\\ 4 & 7 & * \end{pmatrix}$$ How can I find the three numbers on the main diagonal such that the sum of the numbers on every row and every column is equal (i.e., it's a magic square). Thank you.","Suppose I have a matrix $$\begin{pmatrix} *  & 3 & 6\\ 5 & *  & 5\\ 4 & 7 & * \end{pmatrix}$$ How can I find the three numbers on the main diagonal such that the sum of the numbers on every row and every column is equal (i.e., it's a magic square). Thank you.",,"['matrices', 'magic-square']"
71,"$ A^2 - B^2 = I_{2n+1} \implies det(AB-BA)=0 $ where A,B are complex matrices of odd size","where A,B are complex matrices of odd size", A^2 - B^2 = I_{2n+1} \implies det(AB-BA)=0 ,"Let $A, B$ be square matrices (with complex entries) of size $2n+1$ , where $n$ is a positive integer. I need help proving the following: $$A^2 - B^2 = I_{2n+1} \implies det(AB-BA)=0 $$ I've tried using characteristic polynomials, properties of eigenvalues, however to no avail. I feel like this kind of problem needs a little bit of experience in working with ranks of matrices (I've tried using Sylvester in more ways) and I would appreciate some help.","Let be square matrices (with complex entries) of size , where is a positive integer. I need help proving the following: I've tried using characteristic polynomials, properties of eigenvalues, however to no avail. I feel like this kind of problem needs a little bit of experience in working with ranks of matrices (I've tried using Sylvester in more ways) and I would appreciate some help.","A, B 2n+1 n A^2 - B^2 = I_{2n+1} \implies det(AB-BA)=0 ","['matrices', 'determinant', 'matrix-equations', 'matrix-rank', 'characteristic-polynomial']"
72,What is the intuition behind a low-rank covariance matrix?,What is the intuition behind a low-rank covariance matrix?,,"Let $X, Y$ be random vectors.  Let $K = \text{Cov}(X,Y) = E[XY^T] - E[X]E[Y]^T$ be the covariance matrix of $X$ and $Y$ .  Assume $K$ is low rank. I'm trying to come up with simple intuitive examples about what having a low rank covariance matrix means but I'm having trouble.  I understand that a low rank matrix means most of the column vectors are linearly dependent on other column vectors, and I understand that the covariance matrix shows the variance relationships between each random variable.  But from here I'm having trouble coming up with an intuitive explanation or example of where this would be useful.","Let be random vectors.  Let be the covariance matrix of and .  Assume is low rank. I'm trying to come up with simple intuitive examples about what having a low rank covariance matrix means but I'm having trouble.  I understand that a low rank matrix means most of the column vectors are linearly dependent on other column vectors, and I understand that the covariance matrix shows the variance relationships between each random variable.  But from here I'm having trouble coming up with an intuitive explanation or example of where this would be useful.","X, Y K = \text{Cov}(X,Y) = E[XY^T] - E[X]E[Y]^T X Y K","['matrices', 'intuition', 'matrix-rank', 'covariance']"
73,Is there any non zero matrix whose adjoint is a zero matrix,Is there any non zero matrix whose adjoint is a zero matrix,,"Just wanted to know whether their exits a non zero matrix whose adjoint is a zero matrix. And if so what would be inverse of a matrix whose adjoint as well as determinant is zero, as $$A^{-1}=\dfrac{1}{ |A|} adj(A)$$","Just wanted to know whether their exits a non zero matrix whose adjoint is a zero matrix. And if so what would be inverse of a matrix whose adjoint as well as determinant is zero, as",A^{-1}=\dfrac{1}{ |A|} adj(A),['matrices']
74,"Calculate $A^5 - 27A^3 + 65A^2$, where $A$ is the matrix defined below.","Calculate , where  is the matrix defined below.",A^5 - 27A^3 + 65A^2 A,"If $A=\begin{bmatrix} 0 & 0 & 1 \\ 3 & 1 & 0 \\  -2&1&4\end{bmatrix}$ , find $A^5 - 27A^3 + 65A^2$ $$A=\begin{bmatrix} 0 & 0 & 1 \\ 3 & 1 & 0 \\  -2&1&4\end{bmatrix}$$ Let $\lambda$ be its eigenvalue, then $$(A-\lambda I) = \begin{bmatrix} 0-\lambda & 0 & 1 \\ 3 & 1-\lambda & 0 \\  -2&1&4-\lambda\end{bmatrix}$$ $$|A-\lambda I| = -(\lambda)^3 + 5(\lambda)^2 - 6(\lambda) +5$$ Using Cayley-Hamilton theorem $$A^3-5^2+6A-5=0$$ How do I use this find $A^5 - 27A^3 + 65A^2$ ?","If , find Let be its eigenvalue, then Using Cayley-Hamilton theorem How do I use this find ?","A=\begin{bmatrix} 0 & 0 & 1 \\
3 & 1 & 0 \\ 
-2&1&4\end{bmatrix} A^5 - 27A^3 + 65A^2 A=\begin{bmatrix} 0 & 0 & 1 \\
3 & 1 & 0 \\ 
-2&1&4\end{bmatrix} \lambda (A-\lambda I) = \begin{bmatrix} 0-\lambda & 0 & 1 \\
3 & 1-\lambda & 0 \\ 
-2&1&4-\lambda\end{bmatrix} |A-\lambda I| = -(\lambda)^3 + 5(\lambda)^2 - 6(\lambda) +5 A^3-5^2+6A-5=0 A^5 - 27A^3 + 65A^2","['matrices', 'eigenvalues-eigenvectors']"
75,Relationships between 2 matrices yielding values of determinants,Relationships between 2 matrices yielding values of determinants,,"I got some homework in my school about matrix. These questions are seem so easy to solve but I always get stuck. Here they are: Let $A,B \in \mathbb{R}^{2017\times2017}$ matrices which satisfy the following equation.   $$A^{-1} = (A+B)^{-1}-B^{-1}$$   and $\det(A^{-1})=2017.$ Find $\det(B)$. My attempt: \begin{equation*} \begin{split} (A+B)A^{-1} &= (A+B)\left[(A+B)^{-1}-B^{-1}\right] \quad \quad \text{multiplying both sides by (A+B)} \\ A^{-1}A +BA^{-1} &= (A+B)(A+B)^{-1}-(A+B)B^{-1} \\ I+BA^{-1} &= I - AB^{-1}-I\\ I+BA^{-1} &=-AB^{-1}\\ BA^{-1} +AB^{-1} +I&= O \end{split} \end{equation*} then I don't know how to continue. 2.Let $A,B\in \mathbb{R}^{2017 \times 2017}$ matrices which satisfy the equation   $$AB^{2}-2BAB+B^{2}A=O$$ What is the largest eigenvalue of $AB-BA?$ $ABB+BBA=2BAB$ $ABB+BBA=BAB+BAB$ $ABB-BAB=BAB-BBA$ $(AB-BA)B=B(AB-BA)$ what is this means? I really need your thoughts, thanks in advance.","I got some homework in my school about matrix. These questions are seem so easy to solve but I always get stuck. Here they are: Let $A,B \in \mathbb{R}^{2017\times2017}$ matrices which satisfy the following equation.   $$A^{-1} = (A+B)^{-1}-B^{-1}$$   and $\det(A^{-1})=2017.$ Find $\det(B)$. My attempt: \begin{equation*} \begin{split} (A+B)A^{-1} &= (A+B)\left[(A+B)^{-1}-B^{-1}\right] \quad \quad \text{multiplying both sides by (A+B)} \\ A^{-1}A +BA^{-1} &= (A+B)(A+B)^{-1}-(A+B)B^{-1} \\ I+BA^{-1} &= I - AB^{-1}-I\\ I+BA^{-1} &=-AB^{-1}\\ BA^{-1} +AB^{-1} +I&= O \end{split} \end{equation*} then I don't know how to continue. 2.Let $A,B\in \mathbb{R}^{2017 \times 2017}$ matrices which satisfy the equation   $$AB^{2}-2BAB+B^{2}A=O$$ What is the largest eigenvalue of $AB-BA?$ $ABB+BBA=2BAB$ $ABB+BBA=BAB+BAB$ $ABB-BAB=BAB-BBA$ $(AB-BA)B=B(AB-BA)$ what is this means? I really need your thoughts, thanks in advance.",,"['matrices', 'eigenvalues-eigenvectors', 'determinant']"
76,Parity of rows and columns of a rectangular table of binary digits,Parity of rows and columns of a rectangular table of binary digits,,"I'm preparing for a Computer Science interview at Cambridge and I came across this question: With a rectangular table of binary digits it is always possible to add an extra row and an extra column so that every row and every column of the resulting table has an even number of ones and zeros in it. Explain why. Is it always possible to add a row and column to a table of binary digits where every row and every column of the resulting table has an odd number of binary digits? This is the answer: Parity Matrix Adding respectively an extra row or column to a table so that the parity of all columns or rows respectively is odd or even is always straightforward. Adding both at once adds one extra cell that is overspecified (at the intersection of extra column and row - at bottom right hand corner if we extend in length and to the right). The overspecification is not a conflict with even parity. This can be seen by considering every possible original table as a sum of a certain number of tables each with a single one in it and noting that addition of tables (using cell-by-cell xor) preserves even parity. Infact, it is soon clear that the conflicted cell is always clear. The overspecification is not a conflict with odd parity for certain table sizes. Another argument that works for even parity is based on the whether the tally of the unextended table is odd or even. The associativity of addition means that the same digit is required in the corner cell for even parity however you perform the tally. However, for odd parity, a virtual r or c additional ones have been summed, where r and c are the number of rows and columns in the original table. There is no conflict if r and c have the same parity. I can't understand the question or the answer. If we have the following table: ╔═════════════╗ ║ 1 0 0 0 1 1 ║ ║ 1 0 0 0 1 1 ║ ║ 1 0 0 0 1 1 ║ ║ 0 1 1 1 0 0 ║ ║ 0 1 1 1 0 0 ║ ║ 0 1 1 1 0 0 ║ ╚═════════════╝ How can we add an extra row and an extra column so that every row and every column of the resulting table has an even number of ones and zeros in it? Also, what exactly does the answer mean by ""parity"" (I'm vaguely familiar with it, but I'd like some more info) and how does it relate to the question? Thanks in advance. Edit: The question is found here .","I'm preparing for a Computer Science interview at Cambridge and I came across this question: With a rectangular table of binary digits it is always possible to add an extra row and an extra column so that every row and every column of the resulting table has an even number of ones and zeros in it. Explain why. Is it always possible to add a row and column to a table of binary digits where every row and every column of the resulting table has an odd number of binary digits? This is the answer: Parity Matrix Adding respectively an extra row or column to a table so that the parity of all columns or rows respectively is odd or even is always straightforward. Adding both at once adds one extra cell that is overspecified (at the intersection of extra column and row - at bottom right hand corner if we extend in length and to the right). The overspecification is not a conflict with even parity. This can be seen by considering every possible original table as a sum of a certain number of tables each with a single one in it and noting that addition of tables (using cell-by-cell xor) preserves even parity. Infact, it is soon clear that the conflicted cell is always clear. The overspecification is not a conflict with odd parity for certain table sizes. Another argument that works for even parity is based on the whether the tally of the unextended table is odd or even. The associativity of addition means that the same digit is required in the corner cell for even parity however you perform the tally. However, for odd parity, a virtual r or c additional ones have been summed, where r and c are the number of rows and columns in the original table. There is no conflict if r and c have the same parity. I can't understand the question or the answer. If we have the following table: ╔═════════════╗ ║ 1 0 0 0 1 1 ║ ║ 1 0 0 0 1 1 ║ ║ 1 0 0 0 1 1 ║ ║ 0 1 1 1 0 0 ║ ║ 0 1 1 1 0 0 ║ ║ 0 1 1 1 0 0 ║ ╚═════════════╝ How can we add an extra row and an extra column so that every row and every column of the resulting table has an even number of ones and zeros in it? Also, what exactly does the answer mean by ""parity"" (I'm vaguely familiar with it, but I'd like some more info) and how does it relate to the question? Thanks in advance. Edit: The question is found here .",,"['matrices', 'computer-science', 'binary']"
77,How to calculate the negative half power of a matrix,How to calculate the negative half power of a matrix,,I have a square matrix called A. How can I find $A ^ {-1/2}$. Should I compute $a_{ij} ^ {-1/2}$ for all of its elements? Thanks,I have a square matrix called A. How can I find $A ^ {-1/2}$. Should I compute $a_{ij} ^ {-1/2}$ for all of its elements? Thanks,,['matrices']
78,Matrix consisting of cosines of differences,Matrix consisting of cosines of differences,,"Consider the following matrix: $$\left[\begin{array}{cccc} \cos(x_1-y_1) & \cos(x_1-y_2) & \ldots & \cos(x_1-y_n)  \\ \cos(x_2-y_1) & \cos(x_2-y_2) & \ldots & \cos(x_2-y_n) \\ \cos(x_3-y_1) & \cos(x_3-y_2) & \ldots & \cos(x_3-y_n) \\ \vdots & \vdots & \ddots & \vdots\\ \cos(x_n-y_1) & \cos(x_n-y_2) & \ldots & \cos(x_n-y_n) \\ \end{array}  \right]$$ with $x_k, y_k$ being real numbers. I cannot really apply any kind of functional calculus, I believe, to get a closed form formula for the determinant of this matrix. Any hints or ideas how to proceed?","Consider the following matrix: $$\left[\begin{array}{cccc} \cos(x_1-y_1) & \cos(x_1-y_2) & \ldots & \cos(x_1-y_n)  \\ \cos(x_2-y_1) & \cos(x_2-y_2) & \ldots & \cos(x_2-y_n) \\ \cos(x_3-y_1) & \cos(x_3-y_2) & \ldots & \cos(x_3-y_n) \\ \vdots & \vdots & \ddots & \vdots\\ \cos(x_n-y_1) & \cos(x_n-y_2) & \ldots & \cos(x_n-y_n) \\ \end{array}  \right]$$ with $x_k, y_k$ being real numbers. I cannot really apply any kind of functional calculus, I believe, to get a closed form formula for the determinant of this matrix. Any hints or ideas how to proceed?",,"['matrices', 'determinant']"
79,Is this group finite?,Is this group finite?,,"Let $G$ be a sub-group of the invertible real matrices of size $n$ (usually noted $GL_n(\mathbb{R})$), such that $\forall M\in G,M^2=I_n$ Is $G$ finite ?","Let $G$ be a sub-group of the invertible real matrices of size $n$ (usually noted $GL_n(\mathbb{R})$), such that $\forall M\in G,M^2=I_n$ Is $G$ finite ?",,"['group-theory', 'matrices']"
80,Are matrices vectors?,Are matrices vectors?,,"This may sound like an obvious question but it has confused me! According to wikipedia ( https://en.m.wikipedia.org/wiki/Vector_(mathematics_and_physics) ) vectors are defined as: ""An element of a vector space"" But can't you have a vector space with elements of matrices, and for that matter numbers or even functions. Does this not mean that all of these are vectors to (along with the  normal arrow like vectors)?","This may sound like an obvious question but it has confused me! According to wikipedia ( https://en.m.wikipedia.org/wiki/Vector_(mathematics_and_physics) ) vectors are defined as: ""An element of a vector space"" But can't you have a vector space with elements of matrices, and for that matter numbers or even functions. Does this not mean that all of these are vectors to (along with the  normal arrow like vectors)?",,['matrices']
81,Do cyclic permutations of rows and column entries generate all permutations?,Do cyclic permutations of rows and column entries generate all permutations?,,"Background: I am interested in the group of permutations of the entries of a general $m\times n$ matrix. In particular, I am interested in (1) interesting sets of simple generators for this group that might be used in a puzzle or game and (2) algorithms for computing the smallest sequence of generators that produce a given permutation. This question focuses on one candidate set of generators. Question: Suppose $m,n\ge 1$ are integers. Consider the set of permutations of a general $m\times n$ matrix containing all single-row cyclic permutations and all single-column cyclic permutations. Is every possible permutation of matrix entries generated by this set? More formally, for all integers $r>0$, let $\sigma_r$ be the permutation of $1,\ldots,r$ defined by $1\mapsto 2\mapsto 3\mapsto \cdots \mapsto r \mapsto 1$. For each $k=1,\ldots,m$, let $r_k:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ be defined by $$ (r_k A)_{ij} = \begin{cases} A_{ij} &\text{if } i\ne k \\ A_{i\sigma_n(j)} &\text{if }i=k. \end{cases} $$ Similarly, for each $l=1,\ldots,n$, let $c_l:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ be defined by $$ (c_l A)_{ij} = \begin{cases} A_{ij} &\text{if } j\ne l \\ A_{\sigma_m(i) j} &\text{if }j=l. \end{cases} $$ Clearly, each $r_k$ and $c_l$ is a permutation of the entries of an $m\times n$ matrix. Are all such permutations generated by $\{r_1,\ldots,r_m,c_1,\ldots,c_n\}$ ? If not, is there any easy characterization of the generated group? EDIT: As @Omnomnomnom pointed out in his answer, a parity argument can be used to show all odd permutations of entries cannot be generated if both $m$ and $n$ are odd, because all the generators then have even parity. But that is (obviously) not a full characterization of the permutation group generated by these cycles.","Background: I am interested in the group of permutations of the entries of a general $m\times n$ matrix. In particular, I am interested in (1) interesting sets of simple generators for this group that might be used in a puzzle or game and (2) algorithms for computing the smallest sequence of generators that produce a given permutation. This question focuses on one candidate set of generators. Question: Suppose $m,n\ge 1$ are integers. Consider the set of permutations of a general $m\times n$ matrix containing all single-row cyclic permutations and all single-column cyclic permutations. Is every possible permutation of matrix entries generated by this set? More formally, for all integers $r>0$, let $\sigma_r$ be the permutation of $1,\ldots,r$ defined by $1\mapsto 2\mapsto 3\mapsto \cdots \mapsto r \mapsto 1$. For each $k=1,\ldots,m$, let $r_k:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ be defined by $$ (r_k A)_{ij} = \begin{cases} A_{ij} &\text{if } i\ne k \\ A_{i\sigma_n(j)} &\text{if }i=k. \end{cases} $$ Similarly, for each $l=1,\ldots,n$, let $c_l:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ be defined by $$ (c_l A)_{ij} = \begin{cases} A_{ij} &\text{if } j\ne l \\ A_{\sigma_m(i) j} &\text{if }j=l. \end{cases} $$ Clearly, each $r_k$ and $c_l$ is a permutation of the entries of an $m\times n$ matrix. Are all such permutations generated by $\{r_1,\ldots,r_m,c_1,\ldots,c_n\}$ ? If not, is there any easy characterization of the generated group? EDIT: As @Omnomnomnom pointed out in his answer, a parity argument can be used to show all odd permutations of entries cannot be generated if both $m$ and $n$ are odd, because all the generators then have even parity. But that is (obviously) not a full characterization of the permutation group generated by these cycles.",,"['group-theory', 'matrices', 'permutations']"
82,Invert a matrix.,Invert a matrix.,,"$$A=\begin{pmatrix}1 & -a_1 & -a_1 &\cdots & -a_1\\ -a_2 & 1 &-a_2 & \cdots &-a_2\\ \vdots & \vdots & \ddots & \vdots & \vdots\\ -a_{N-1} & -a_{N-1} & \cdots& 1 & -a_{N-1}\\ -a_N & -a_N & \cdots & -a_N & 1 \end{pmatrix}.$$ Where $a_i\geq0\;\forall\; i\in\{1, \cdots, N\}$ and $$\sum\limits_{i=1}^{N}\dfrac{a_i}{a_i+1}<1.\quad (1)$$ EDIT 1 : The condition $(1)$ must guarantee that the inverse exists. EDIT 2 In fact, there is no formula given for $A^{-1}$. The problem is to find $P_i$ in the following equation: $$P_i-a_i\sum\limits_{j\neq i}^{N}P_j=\alpha a_i\;\forall\;i\in\{1, \cdots, N\}.$$ This is equivalent to $AP=b$ and hence $P=A^{-1}b$. They said that $P_i$ is given by: $$P_i=\dfrac{\alpha}{1-\sum\limits_{j=1}^{N}\dfrac{a_j}{1+a_j}}\dfrac{a_i}{1+a_i}.$$ Where $b=[\alpha a_1, \alpha a_2, \cdots, \alpha a_N]^{\mathrm{T}}$ and $P=[P_1, P_2, \cdots, P_N]^{\mathrm{T}}.$ This matrix is given in a paper: the authors said that its inverse is given by $A^{-1}$ when $(1)$ is satisfied. I do not know how to proceed to invert it. How did they get $P$ without getting $A^{-1}$ ? Thank you very much.","$$A=\begin{pmatrix}1 & -a_1 & -a_1 &\cdots & -a_1\\ -a_2 & 1 &-a_2 & \cdots &-a_2\\ \vdots & \vdots & \ddots & \vdots & \vdots\\ -a_{N-1} & -a_{N-1} & \cdots& 1 & -a_{N-1}\\ -a_N & -a_N & \cdots & -a_N & 1 \end{pmatrix}.$$ Where $a_i\geq0\;\forall\; i\in\{1, \cdots, N\}$ and $$\sum\limits_{i=1}^{N}\dfrac{a_i}{a_i+1}<1.\quad (1)$$ EDIT 1 : The condition $(1)$ must guarantee that the inverse exists. EDIT 2 In fact, there is no formula given for $A^{-1}$. The problem is to find $P_i$ in the following equation: $$P_i-a_i\sum\limits_{j\neq i}^{N}P_j=\alpha a_i\;\forall\;i\in\{1, \cdots, N\}.$$ This is equivalent to $AP=b$ and hence $P=A^{-1}b$. They said that $P_i$ is given by: $$P_i=\dfrac{\alpha}{1-\sum\limits_{j=1}^{N}\dfrac{a_j}{1+a_j}}\dfrac{a_i}{1+a_i}.$$ Where $b=[\alpha a_1, \alpha a_2, \cdots, \alpha a_N]^{\mathrm{T}}$ and $P=[P_1, P_2, \cdots, P_N]^{\mathrm{T}}.$ This matrix is given in a paper: the authors said that its inverse is given by $A^{-1}$ when $(1)$ is satisfied. I do not know how to proceed to invert it. How did they get $P$ without getting $A^{-1}$ ? Thank you very much.",,['matrices']
83,How to prove that $\det(M) = (-1)^k \det(A) \det(B)?$,How to prove that,\det(M) = (-1)^k \det(A) \det(B)?,Let $\mathbf{A}$ and $\mathbf{B}$ be $k \times k$ matrices and $\mathbf{M}$ is the block matrix $$\mathbf{M} = \begin{pmatrix}0 & \mathbf{B} \\ \mathbf{A} & 0\end{pmatrix}.$$ How to prove that $\det(\mathbf{M}) = (-1)^k \det(\mathbf{A}) \det(\mathbf{B})$?,Let $\mathbf{A}$ and $\mathbf{B}$ be $k \times k$ matrices and $\mathbf{M}$ is the block matrix $$\mathbf{M} = \begin{pmatrix}0 & \mathbf{B} \\ \mathbf{A} & 0\end{pmatrix}.$$ How to prove that $\det(\mathbf{M}) = (-1)^k \det(\mathbf{A}) \det(\mathbf{B})$?,,"['matrices', 'determinant', 'block-matrices']"
84,Basis of complex matrix vector space over $\Bbb{R}$,Basis of complex matrix vector space over,\Bbb{R},"I understand that the basis of the vector space $$Mat_2(\Bbb{R}) = \begin{pmatrix}a_{11} & a_{12} \\ a_{21} & a_{22}\end{pmatrix}$$ over $\Bbb{R}$ is $$e = \left\{ \begin{pmatrix}1 & 0\\ 0 & 0\end{pmatrix}, \begin{pmatrix}0 & 1\\ 0 & 0\end{pmatrix},\begin{pmatrix}0 & 0\\ 1 & 0\end{pmatrix},\begin{pmatrix}0 & 0\\ 0 & 1\end{pmatrix} \right\}$$ However, I can't figure out the basis or the vector space $Mat_2(\Bbb{C}) = \begin{pmatrix}a_{11}+b_{11}i & a_{12}+b_{12}i \\ a_{21}+b_{21}i & a_{22}+b_{22}i\end{pmatrix}$ over $\Bbb{R}$. Thank You.","I understand that the basis of the vector space $$Mat_2(\Bbb{R}) = \begin{pmatrix}a_{11} & a_{12} \\ a_{21} & a_{22}\end{pmatrix}$$ over $\Bbb{R}$ is $$e = \left\{ \begin{pmatrix}1 & 0\\ 0 & 0\end{pmatrix}, \begin{pmatrix}0 & 1\\ 0 & 0\end{pmatrix},\begin{pmatrix}0 & 0\\ 1 & 0\end{pmatrix},\begin{pmatrix}0 & 0\\ 0 & 1\end{pmatrix} \right\}$$ However, I can't figure out the basis or the vector space $Mat_2(\Bbb{C}) = \begin{pmatrix}a_{11}+b_{11}i & a_{12}+b_{12}i \\ a_{21}+b_{21}i & a_{22}+b_{22}i\end{pmatrix}$ over $\Bbb{R}$. Thank You.",,"['matrices', 'vector-spaces']"
85,"Matrix inverse identity $\,(A-B)^{-1}=A^{-1}+A^{-1}(B^{-1}-A^{-1})^{-1}A^{-1}$",Matrix inverse identity,"\,(A-B)^{-1}=A^{-1}+A^{-1}(B^{-1}-A^{-1})^{-1}A^{-1}","Question: Assuming that all matrix inverses involved below exist, show that $$(\mathbf{A}-\mathbf{B})^{-1}=\mathbf{A}^{-1}+\mathbf{A}^{-1}(\mathbf{B}^{-1}-\mathbf{A}^{-1})^{-1}\mathbf{A}^{-1}$$ in particular $$(\mathbf{I}+\mathbf{A})^{-1}=\mathbf{I}-(\mathbf{A}^{-1}+I)^{-1}$$ and $$\det[(\mathbf{I}+\mathbf{A})^{-1}+(\mathbf{A}^{-1}+\mathbf{I})^{-1}]=1$$","Question: Assuming that all matrix inverses involved below exist, show that $$(\mathbf{A}-\mathbf{B})^{-1}=\mathbf{A}^{-1}+\mathbf{A}^{-1}(\mathbf{B}^{-1}-\mathbf{A}^{-1})^{-1}\mathbf{A}^{-1}$$ in particular $$(\mathbf{I}+\mathbf{A})^{-1}=\mathbf{I}-(\mathbf{A}^{-1}+I)^{-1}$$ and $$\det[(\mathbf{I}+\mathbf{A})^{-1}+(\mathbf{A}^{-1}+\mathbf{I})^{-1}]=1$$",,"['matrices', 'determinant', 'formal-power-series']"
86,Easiest way to determine all disconnected sets from a graph?,Easiest way to determine all disconnected sets from a graph?,,"Suppose that I have a un-directed graph of nodes and edges, I would like to know all sets of nodes that do not connect with any other nodes in the graph. Here is a concrete example to help you picture what I'm asking. In the following graph, all x nodes are connected to their adjacent (diagonal included) x nodes and the same goes for o nodes and b nodes. x o o b x o b b x I wrote an algorithm that does this by taking a node and using depth first search to find all nodes connected to it. Then I remove those nodes from the graph and repeat with a new node until there are no more nodes left in the graph. I'm starting to think that this isn't the most efficient method and that there has to be a way to do this using an adjacency matrix or something similar. If I were to translate the above graph into an adjacency matrix and name each node (1..9, left to right, top to bottom), it would look like this: ~~ 1 2 3 4 5 6 7 8 9 1 | 0 0 0 0 1 0 0 0 0 2 | 0 0 1 0 0 1 0 0 0 3 | 0 1 0 0 0 1 0 0 0 4 | 0 0 0 0 0 0 1 1 0 5 | 1 0 0 0 0 0 0 0 1 6 | 0 1 1 0 0 0 0 0 0 7 | 0 0 0 1 0 0 0 1 0 8 | 0 0 0 1 0 0 1 0 0 9 | 0 0 0 0 1 0 0 0 0 I put zeros down the diagonal, but I'm not sure if that's right notation for an adjacency matrix. Also, since it's an undirected graph, I know that the matrix is symmetrical down the diagonal. Beyond that, I'm stuck. I just have a feeling that something about this matrix will make it easier to identify the 3 distinct unconnected groups beyond what I've done already. Does anyone have an idea for an algorithm that will help me? Thanks in advance.","Suppose that I have a un-directed graph of nodes and edges, I would like to know all sets of nodes that do not connect with any other nodes in the graph. Here is a concrete example to help you picture what I'm asking. In the following graph, all x nodes are connected to their adjacent (diagonal included) x nodes and the same goes for o nodes and b nodes. x o o b x o b b x I wrote an algorithm that does this by taking a node and using depth first search to find all nodes connected to it. Then I remove those nodes from the graph and repeat with a new node until there are no more nodes left in the graph. I'm starting to think that this isn't the most efficient method and that there has to be a way to do this using an adjacency matrix or something similar. If I were to translate the above graph into an adjacency matrix and name each node (1..9, left to right, top to bottom), it would look like this: ~~ 1 2 3 4 5 6 7 8 9 1 | 0 0 0 0 1 0 0 0 0 2 | 0 0 1 0 0 1 0 0 0 3 | 0 1 0 0 0 1 0 0 0 4 | 0 0 0 0 0 0 1 1 0 5 | 1 0 0 0 0 0 0 0 1 6 | 0 1 1 0 0 0 0 0 0 7 | 0 0 0 1 0 0 0 1 0 8 | 0 0 0 1 0 0 1 0 0 9 | 0 0 0 0 1 0 0 0 0 I put zeros down the diagonal, but I'm not sure if that's right notation for an adjacency matrix. Also, since it's an undirected graph, I know that the matrix is symmetrical down the diagonal. Beyond that, I'm stuck. I just have a feeling that something about this matrix will make it easier to identify the 3 distinct unconnected groups beyond what I've done already. Does anyone have an idea for an algorithm that will help me? Thanks in advance.",,"['matrices', 'graph-theory']"
87,Forming equation of a plane by solving linear equation set,Forming equation of a plane by solving linear equation set,,"Given three points on the plane: $ A(x_1, y_1, z_1) $, $ B(x_2, y_2, z_2) $ and $ C(x_3, y_3, z_3) $. I'm trying to obtain the equation of the plane in this format: $ ax + by + cz + d = 0 $ I substituted given three points into the plane equation above to form this matrix equation below: \begin{equation}  \begin{bmatrix}     x_1 & y_1 & z_1 & 1 \\     x_2 & y_2 & z_2 & 1 \\     x_3 & y_3 & z_3 & 1 \\     ? & ? & ? & ?  \end{bmatrix}  \begin{bmatrix} a \\ b \\ c \\ d \end{bmatrix}  =  \begin{bmatrix} 0 \\ 0 \\ 0 \\ ? \end{bmatrix} \end{equation} My aim is to find the coefficients $ a $, $ b $, $ c $ and $ d $ by solving this matrix equation. However, I can't find a fourth equation to complete the equation set. Can you please write me a fourth equation to complete the set? Note: My aim is not just finding the plane equation. My aim is to find the plane equation by this method, by means of solving a linear set of equations. I know the other more practical way of finding the plane equation, but I'm trying to find it this way on purpose. There is no reason, I just like trying different methods and playing with numbers occasionally out of interest. So, please consider this not while writing your answers and don't suggest me other methods.","Given three points on the plane: $ A(x_1, y_1, z_1) $, $ B(x_2, y_2, z_2) $ and $ C(x_3, y_3, z_3) $. I'm trying to obtain the equation of the plane in this format: $ ax + by + cz + d = 0 $ I substituted given three points into the plane equation above to form this matrix equation below: \begin{equation}  \begin{bmatrix}     x_1 & y_1 & z_1 & 1 \\     x_2 & y_2 & z_2 & 1 \\     x_3 & y_3 & z_3 & 1 \\     ? & ? & ? & ?  \end{bmatrix}  \begin{bmatrix} a \\ b \\ c \\ d \end{bmatrix}  =  \begin{bmatrix} 0 \\ 0 \\ 0 \\ ? \end{bmatrix} \end{equation} My aim is to find the coefficients $ a $, $ b $, $ c $ and $ d $ by solving this matrix equation. However, I can't find a fourth equation to complete the equation set. Can you please write me a fourth equation to complete the set? Note: My aim is not just finding the plane equation. My aim is to find the plane equation by this method, by means of solving a linear set of equations. I know the other more practical way of finding the plane equation, but I'm trying to find it this way on purpose. There is no reason, I just like trying different methods and playing with numbers occasionally out of interest. So, please consider this not while writing your answers and don't suggest me other methods.",,"['geometry', 'matrices', 'intuition']"
88,How do you do a cross product of two $3 \times 3$ boolean matrices?,How do you do a cross product of two  boolean matrices?,3 \times 3,"I have two boolean matrices: A = |1 1 0|     |0 1 0|     |0 0 1|  and  B = |1 0 0|     |1 1 1|     |0 0 1| What is the result of A x B and what are the steps needed to attain the result? Note: My textbook says that the answer to the above is: A x B = |1 1 1|         |1 1 1|         |0 0 1| and that A * B is not equal to A x B . Unfortunately, it does not give the steps needed to find the solution.","I have two boolean matrices: A = |1 1 0|     |0 1 0|     |0 0 1|  and  B = |1 0 0|     |1 1 1|     |0 0 1| What is the result of A x B and what are the steps needed to attain the result? Note: My textbook says that the answer to the above is: A x B = |1 1 1|         |1 1 1|         |0 0 1| and that A * B is not equal to A x B . Unfortunately, it does not give the steps needed to find the solution.",,"['matrices', 'computer-science']"
89,Series of polynomials very nearly follows binomial coefficients but doesn't quite,Series of polynomials very nearly follows binomial coefficients but doesn't quite,,"I'm modelling a system using a Markov chain and by a few iterations of the transition matrix I can see a pattern emerging in the resulting polynomial that really looks like Pascal's triangle, but isn't quite. I've been out of the mathematics world for a little while now so I may be missing quite obvious, but this feels like the sort of thing that should be able to wrap very neatly up into a little summation or something. Ideally I want a simple expression in terms of $n$ and $\mu$ . $$T=\begin{bmatrix} \mu^2-2\mu+1 & \mu-\mu^2 & \mu-\mu^2 & \mu^2\\  0 & 1-\mu & 0 & \mu\\  0 & 0 & 1-\mu & \mu\\ 0 & 0 & 0 & 1 \end{bmatrix}$$ $$i=\begin{bmatrix} 1\\  0\\  0\\  0 \end{bmatrix}$$ Basically what I've done so far is take my transition matrix $T$ and raise it to the powers 1-4, then left multiplying by the initial state vector $i$ . The first term of the resulting state vector $p$ is pretty clearly $(\mu^2-2\mu+1)^n$ , but the other ones are much bigger. At the moment I'm only interested in the final term $p_4$ , so I'll only show that one. Here's the polynomials in expanded form: $$\mu^2\\ \mu^4-4\mu^3+4\mu^2\\ \mu^6-6\mu^5+15\mu^4-18\mu^3+9\mu^2\\ \mu^8-8\mu^7+28\mu^6-56\mu^5+68\mu^4-48\mu^3+16\mu^2$$ There's clearly a pattern here, the powers are going up by two each time, and each coefficient alternates sign, but what I'm trying to figure out is why the last few coefficients of each polynomial don't seem to follow the Pascal's triangle pattern. In each equation the middle coefficient is 2 smaller than the binomial coefficient should be, and then the ones after that are different in a way I can't seem to find a pattern for. I was hoping someone might be able to point me in the right direction on this, I feel like there must be a way to write a general expression for this but I can't put my finger on it.","I'm modelling a system using a Markov chain and by a few iterations of the transition matrix I can see a pattern emerging in the resulting polynomial that really looks like Pascal's triangle, but isn't quite. I've been out of the mathematics world for a little while now so I may be missing quite obvious, but this feels like the sort of thing that should be able to wrap very neatly up into a little summation or something. Ideally I want a simple expression in terms of and . Basically what I've done so far is take my transition matrix and raise it to the powers 1-4, then left multiplying by the initial state vector . The first term of the resulting state vector is pretty clearly , but the other ones are much bigger. At the moment I'm only interested in the final term , so I'll only show that one. Here's the polynomials in expanded form: There's clearly a pattern here, the powers are going up by two each time, and each coefficient alternates sign, but what I'm trying to figure out is why the last few coefficients of each polynomial don't seem to follow the Pascal's triangle pattern. In each equation the middle coefficient is 2 smaller than the binomial coefficient should be, and then the ones after that are different in a way I can't seem to find a pattern for. I was hoping someone might be able to point me in the right direction on this, I feel like there must be a way to write a general expression for this but I can't put my finger on it.","n \mu T=\begin{bmatrix}
\mu^2-2\mu+1 & \mu-\mu^2 & \mu-\mu^2 & \mu^2\\ 
0 & 1-\mu & 0 & \mu\\ 
0 & 0 & 1-\mu & \mu\\
0 & 0 & 0 & 1
\end{bmatrix} i=\begin{bmatrix}
1\\ 
0\\ 
0\\ 
0
\end{bmatrix} T i p (\mu^2-2\mu+1)^n p_4 \mu^2\\
\mu^4-4\mu^3+4\mu^2\\
\mu^6-6\mu^5+15\mu^4-18\mu^3+9\mu^2\\
\mu^8-8\mu^7+28\mu^6-56\mu^5+68\mu^4-48\mu^3+16\mu^2","['matrices', 'polynomials', 'binomial-coefficients', 'markov-chains', 'stochastic-matrices']"
90,Angle definition confusion in Rodrigues rotation matrix,Angle definition confusion in Rodrigues rotation matrix,,"The Rodrigues rotation formula gives us the following way to rotate a vector $\vec{v}$ by some angle $\theta$ about an arbitrary axis $\vec{k}$: $\vec{v}_\text{rot} = \vec{v}\cos(\theta) + (\vec{k} \times \vec{v})\sin(\theta) + \vec{k}(\vec{k}\cdot\vec{v})(1-\cos\theta)$ Let's call this the ""vector notation"" There is also a way to obtain the corresponding rotation matrix $\textbf{R}$, as such: $\textbf{R} = \textbf{I} + (\sin\theta)\textbf{K} + (1-\cos\theta)\textbf{K}^2$ $\>\>\>\>\>\downarrow$ $\vec{v}_\text{rot} = \textbf{R}\vec{v}$ where $\textbf{K}$ is the cross-product matrix of the rotation axis: $\textbf{K} = \begin{pmatrix} 0 & -k_z & k_y \\                                k_z & 0 & -k_x \\                                -k_y & k_x & 0\end{pmatrix}$ Let's call this the ""matrix notation"" My issue is that I cannot seem to get the ""vector"" and ""matrix"" notations to agree... Let's say we have the vector $\vec{v} = \begin{pmatrix} \sqrt{1/3} \\ \sqrt{1/3} \\ \sqrt{1/3} \end{pmatrix}$ That we would like to rotate to lie on the x-axis: $\vec{v}_\text{rot} = \begin{pmatrix} |\vec{v}| \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$ then our axis and angle of rotation are: $\vec{k} = \dfrac{\vec{v}\times\vec{v}_\text{rot}}{|\vec{v}\times\vec{v}_\text{rot}|} = \begin{pmatrix} 0 \\ \sqrt{2}/2 \\ \sqrt{2}/2 \end{pmatrix}\>\>\>\>, \>\>\theta = \cos^{-1}\left( \dfrac{\vec{v}\cdot\vec{v}_\text{rot}}{|\vec{v}||\vec{v}_\text{rot}|}\right) = 0.95532 \text{rad}$ The ""vector notation"" statement above does indeed give the expected answer (I've implemented it in Python with NumPy ): import numpy as np v = np.array([1/3, 1/3, 1/3])**(1/2) vr_desired = np.array([1, 0, 0]) theta = np.arccos(np.dot(v, vr_desired) / np.linalg.norm(v)*np.linalg.norm(vr_desired)) k = np.cross(v, vr_desired) / np.linalg.norm(np.cross(v, vr_desired))  vr_according_to_vector_form = v*np.cos(theta) + (np.cross(k,v)*np.sin(theta)) + k*(np.dot(k,v))*(1.0-np.cos(theta)) print(""v_rot = {}"".format(vr_according_to_vector_form)) where the result is v_rot = [ 1.  0.  0.] as expected. Now, trying the ""matrix notation"": K = np.array([[0, -k[2], k[1]],[k[2], 0, -k[0]],[-k[1], k[0], 0]]) I = np.eye(3) R = I + np.sin(theta)*K + (1-np.cos(theta))*(K**2)  vr_according_to_matrix_form = np.matmul(R,v) print(""v_rot = {}"".format(vr_according_to_matrix_form)) I get the wrong answer: v_rot = [ 1.48803387  0.3660254   0.3660254 ] I can only imagine that the angle $\theta$ is being defined differently in the vector and matrix forms of the Rodrigues formula... I am confident in this diagnosis because if I rotate the axis of rotation itself, I do get the same axis back, which seems to indeed indicate that it is simply an issue with $\theta$ that I'm not understanding, and that there isn't any issue in my declaration of $\textbf{R}$... print(""Rk - k = {}"".format(np.matmul(R,k) - k)) gives Rk - k = [ 0.  0.  0.] FYI, I've been using the this wikipedia resource: https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula There, it says that $\vec{v}$ in the vector formula is rotated ""by an angle θ according to the right hand rule,"", and in the matrix formula, it is rotated ""through an angle θ anticlockwise about the axis $\vec{k}$"". Those sound like identical statements to me, though.","The Rodrigues rotation formula gives us the following way to rotate a vector $\vec{v}$ by some angle $\theta$ about an arbitrary axis $\vec{k}$: $\vec{v}_\text{rot} = \vec{v}\cos(\theta) + (\vec{k} \times \vec{v})\sin(\theta) + \vec{k}(\vec{k}\cdot\vec{v})(1-\cos\theta)$ Let's call this the ""vector notation"" There is also a way to obtain the corresponding rotation matrix $\textbf{R}$, as such: $\textbf{R} = \textbf{I} + (\sin\theta)\textbf{K} + (1-\cos\theta)\textbf{K}^2$ $\>\>\>\>\>\downarrow$ $\vec{v}_\text{rot} = \textbf{R}\vec{v}$ where $\textbf{K}$ is the cross-product matrix of the rotation axis: $\textbf{K} = \begin{pmatrix} 0 & -k_z & k_y \\                                k_z & 0 & -k_x \\                                -k_y & k_x & 0\end{pmatrix}$ Let's call this the ""matrix notation"" My issue is that I cannot seem to get the ""vector"" and ""matrix"" notations to agree... Let's say we have the vector $\vec{v} = \begin{pmatrix} \sqrt{1/3} \\ \sqrt{1/3} \\ \sqrt{1/3} \end{pmatrix}$ That we would like to rotate to lie on the x-axis: $\vec{v}_\text{rot} = \begin{pmatrix} |\vec{v}| \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$ then our axis and angle of rotation are: $\vec{k} = \dfrac{\vec{v}\times\vec{v}_\text{rot}}{|\vec{v}\times\vec{v}_\text{rot}|} = \begin{pmatrix} 0 \\ \sqrt{2}/2 \\ \sqrt{2}/2 \end{pmatrix}\>\>\>\>, \>\>\theta = \cos^{-1}\left( \dfrac{\vec{v}\cdot\vec{v}_\text{rot}}{|\vec{v}||\vec{v}_\text{rot}|}\right) = 0.95532 \text{rad}$ The ""vector notation"" statement above does indeed give the expected answer (I've implemented it in Python with NumPy ): import numpy as np v = np.array([1/3, 1/3, 1/3])**(1/2) vr_desired = np.array([1, 0, 0]) theta = np.arccos(np.dot(v, vr_desired) / np.linalg.norm(v)*np.linalg.norm(vr_desired)) k = np.cross(v, vr_desired) / np.linalg.norm(np.cross(v, vr_desired))  vr_according_to_vector_form = v*np.cos(theta) + (np.cross(k,v)*np.sin(theta)) + k*(np.dot(k,v))*(1.0-np.cos(theta)) print(""v_rot = {}"".format(vr_according_to_vector_form)) where the result is v_rot = [ 1.  0.  0.] as expected. Now, trying the ""matrix notation"": K = np.array([[0, -k[2], k[1]],[k[2], 0, -k[0]],[-k[1], k[0], 0]]) I = np.eye(3) R = I + np.sin(theta)*K + (1-np.cos(theta))*(K**2)  vr_according_to_matrix_form = np.matmul(R,v) print(""v_rot = {}"".format(vr_according_to_matrix_form)) I get the wrong answer: v_rot = [ 1.48803387  0.3660254   0.3660254 ] I can only imagine that the angle $\theta$ is being defined differently in the vector and matrix forms of the Rodrigues formula... I am confident in this diagnosis because if I rotate the axis of rotation itself, I do get the same axis back, which seems to indeed indicate that it is simply an issue with $\theta$ that I'm not understanding, and that there isn't any issue in my declaration of $\textbf{R}$... print(""Rk - k = {}"".format(np.matmul(R,k) - k)) gives Rk - k = [ 0.  0.  0.] FYI, I've been using the this wikipedia resource: https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula There, it says that $\vec{v}$ in the vector formula is rotated ""by an angle θ according to the right hand rule,"", and in the matrix formula, it is rotated ""through an angle θ anticlockwise about the axis $\vec{k}$"". Those sound like identical statements to me, though.",,"['matrices', 'rotations', 'python']"
91,"What is the difference between a kernel, and kernel (Gram) matrix?","What is the difference between a kernel, and kernel (Gram) matrix?",,"Given a kernel , can we represent it as a Gram matrix ? For example, a linear kernel can be presented (in Python/MATLAB code) in a Gram matrix as follows: K = X*X.T . If this is true, how to represent other non-trivial kernels in their Gram matrix form, e.g., check the following link, page 5 , equation 17 showing Jensen-Shannon kernel : K(p,q) = exp(-JS(p||q)) https://pdfs.semanticscholar.org/3e43/4ca7cbd1869f41e338658f7ab4f954782ad8.pdf","Given a kernel , can we represent it as a Gram matrix ? For example, a linear kernel can be presented (in Python/MATLAB code) in a Gram matrix as follows: K = X*X.T . If this is true, how to represent other non-trivial kernels in their Gram matrix form, e.g., check the following link, page 5 , equation 17 showing Jensen-Shannon kernel : K(p,q) = exp(-JS(p||q)) https://pdfs.semanticscholar.org/3e43/4ca7cbd1869f41e338658f7ab4f954782ad8.pdf",,"['matrices', 'machine-learning', 'positive-definite', 'positive-semidefinite', 'python']"
92,How do I do the following matrix differentiation?,How do I do the following matrix differentiation?,,"If $\mathbf{A}$ is symmetric, my function $f:\mathbb{R}^{d}\mapsto \mathbb{R}^{d}$ is defined as $$ f(\mathbf{x}) = \mathbf{Ax}(\mathbf{x}^{T}\mathbf{Ax}). $$ What is the differentiation of $f$ with respect to $\mathbf{x}$, i.e., $\nabla_{\mathbf{x}}f(\mathbf{x})$? This is not a homework problem but rather something related to my research and I converted the notations so that it is more legible. The function $f$ itself is already a gradient of some vector function that maps to a scalar and I initially assumed the Hessian of the original function would be positive definite but my code keeps firing errors at me. Without proper education in matrix calculus, the answer I came up with is $$ \nabla_{\mathbf{x}}f(\mathbf{x}) = \mathbf{A}(\mathbf{x}^{T}\mathbf{Ax}) + 2\mathbf{Axx}^{T}\mathbf{A} $$ Is this correct?","If $\mathbf{A}$ is symmetric, my function $f:\mathbb{R}^{d}\mapsto \mathbb{R}^{d}$ is defined as $$ f(\mathbf{x}) = \mathbf{Ax}(\mathbf{x}^{T}\mathbf{Ax}). $$ What is the differentiation of $f$ with respect to $\mathbf{x}$, i.e., $\nabla_{\mathbf{x}}f(\mathbf{x})$? This is not a homework problem but rather something related to my research and I converted the notations so that it is more legible. The function $f$ itself is already a gradient of some vector function that maps to a scalar and I initially assumed the Hessian of the original function would be positive definite but my code keeps firing errors at me. Without proper education in matrix calculus, the answer I came up with is $$ \nabla_{\mathbf{x}}f(\mathbf{x}) = \mathbf{A}(\mathbf{x}^{T}\mathbf{Ax}) + 2\mathbf{Axx}^{T}\mathbf{A} $$ Is this correct?",,"['matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus', 'jacobian']"
93,Matrices over noncommutative rings,Matrices over noncommutative rings,,"I am wondering whether matrices over noncommutative rings have gone undergone a systematic study, particularly noncommutative group rings? I would appreciate sources, if any are available. Thanks!","I am wondering whether matrices over noncommutative rings have gone undergone a systematic study, particularly noncommutative group rings? I would appreciate sources, if any are available. Thanks!",,"['matrices', 'reference-request', 'noncommutative-algebra', 'reference-works']"
94,"Entries of the inverse of $\left[\frac{1}{x+i+j-1}\right]_{i,j\in\{1,2,\ldots,n\}}$ are polynomials in $x$.",Entries of the inverse of  are polynomials in .,"\left[\frac{1}{x+i+j-1}\right]_{i,j\in\{1,2,\ldots,n\}} x","Let $n$ be a positive integer.  Define $$\textbf{A}_n(x):= \left[\frac{1}{x+i+j-1}\right]_{i,j\in\{1,2,\ldots,n\}}$$ as a matrix over the field $\mathbb{Q}(x)$ of rational functions over $\mathbb{Q}$ in variable $x$. (a) Prove that the Hilbert matrix $\textbf{A}_n(0)$ is an invertible matrix over $\mathbb{Q}$ and all entries of the inverse of $\textbf{A}_n(0)$ are integers. (b) Determine the greatest common divisor (over $\mathbb{Z}$) of all the entries of $\big(\textbf{A}_n(0)\big)^{-1}$. (c) Show that $\textbf{A}_n(x)$ is an invertible matrix over $\mathbb{Q}(x)$ and every entry of the inverse of $\textbf{A}_n(x)$ is a polynomial in $x$. (d) Prove that $x+n$ is the greatest common divisor (over $\mathbb{Q}[x]$) of all the entries of $\big(\textbf{A}_n(x)\big)^{-1}$. Parts (a) and (c) are known. Parts (b) and (d) are open. Now, Part (d) is known (see i707107's solution below), but Part (b) remains open, although it seems like the answer is $n$. Recall that  $$\binom{t}{r}=\frac{t(t-1)(t-2)\cdots(t-r+1)}{r!}$$ for all $t\in\mathbb{Q}(x)$ and $r=0,1,2,\ldots$.  According to i707107, the $(i,j)$-entry of $\big(\textbf{A}_n(x)\big)^{-1}$ is given by $$\alpha_{i,j}(x)=(-1)^{i+j}\,(x+n)\,\binom{x+n+i-1}{i-1}\,\binom{x+n-1}{n-j}\,\binom{x+n+j-1}{n-i}\,\binom{x+i+j-2}{j-1}\,.\tag{*}$$ This means that, for all integers $k$ such that $k\notin\{-1,-2,\ldots,-2n+1\}$, the entries of $\big(\textbf{A}_n(k)\big)^{-1}$ are integers. I now have a new conjecture, which is the primary target for the bounty award. Conjecture: The greatest common divisor $\gamma_n(k)$ over $\mathbb{Z}$ of the entries of $\big(\textbf{A}_n(k)\big)^{-1}$, where $k$ is an integer not belonging in the set $\{-1,-2,\ldots,-2n+1\}$, is given by $$\gamma_n(k)=\mathrm{lcm}(n,n+k)\,.$$ It is clear from (*) that $n+k$ must divide $\gamma_n(k)$.  However, it is not yet clear to me why $n$ should divide $\gamma_n(k)$.  I would like to have a proof of this conjecture, or at least a proof that $n \mid \gamma_n(k)$. Let $M_n$ denote the (unitary) cyclic $\mathbb{Z}[x]$-module generated by $\dfrac{1}{\big((n-1)!\big)^2}\,(x+n)$.  Then, the (unitary)  $\mathbb{Z}[x]$-module $N_n$ generated by the entries of $\big(\textbf{A}_n(x)\big)^{-1}$ is a $\mathbb{Z}[x]$-submodule of $M_n$. We also denote by $\tilde{M}_n$ for the (unitary)  $\mathbb{Z}$-module generated by $\dfrac{1}{\big((n-1)!\big)^2}\,(x+n)\,x^l$ for $l=0,1,2,\ldots,2n-2$.  Then, the (unitary) $\mathbb{Z}$-module $\tilde{N}_n$ generated by the entries of $\big(\textbf{A}_n(x)\big)^{-1}$ is a $\mathbb{Z}$-submodule of $\tilde{M}_n$. For example, $M_2/N_2$ is isomorphic to the (unitary) $\mathbb{Z}[x]$-module $\mathbb{Z}/2\mathbb{Z}$ (in which $x$ acts trivially), and $\tilde{M}_2/\tilde{N}_2$ is isomorphic to the (unitary) $\mathbb{Z}$-module $\mathbb{Z}/2\mathbb{Z}$.  Hence, $\left|M_2/N_2\right|=2=\left|\tilde{M}_2/\tilde{N}_2\right|$.  For $n=3$, Mathematica yields $$\tilde{M}_3/\tilde{N}_3\cong (\mathbb{Z}/2\mathbb{Z})\oplus(\mathbb{Z}/3\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/4\mathbb{Z})^{\oplus 3}\,,$$ as abelian groups.  That is, $\left|\tilde{M}_3/\tilde{N}_3\right|=1152$.  On the other hand,  $$M_3/N_3\cong \mathbb{Z}[x] \big/\left(12,2x^2+6x+4,x^4-x^2\right)$$ as $\mathbb{Z}[x]$-modules, which gives $\left|M_3/N_3\right|=576$. Question: Describe the factor $\mathbb{Z}[x]$-module $M_n/N_n$ and the factor $\mathbb{Z}$-module $\tilde{M}_n/\tilde{N}_n$.  It is easily seen that $\left|M_n/N_n\right|\leq\left|\tilde{M}_n/\tilde{N}_n\right|$.  What are $\left|M_n/N_n\right|$ and $\left|\tilde{M}_n/\tilde{N}_n\right|$?  It can be shown also that the ratio $\dfrac{\left|\tilde{M}_n/\tilde{N}_n\right|}{\left|M_n/N_n\right|}$ is an integer, provided that $\left|\tilde{M}_n/\tilde{N}_n\right|$ is finite.  Compute $\dfrac{\left|\tilde{M}_n/\tilde{N}_n\right|}{\left|M_n/N_n\right|}$ for all integers $n>0$ such that $\left|\tilde{M}_n/\tilde{N}_n\right|<\infty$.  Is it always the case that $\left|\tilde{M}_n/\tilde{N}_n\right|$ is finite? Apart from the conjecture above, this question is also eligible for the bounty award. I have not yet fully tried to deal with any case involving $n>3$.  However, for $n=4$, the module $\tilde{M}_4/\tilde{N}_4$ is huge: $$  \tilde{M}_4/\tilde{N}_4\cong (\mathbb{Z}/2\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/3\mathbb{Z})^{\oplus 3}\oplus(\mathbb{Z}/8\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/9\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/16\mathbb{Z})\oplus(\mathbb{Z}/27\mathbb{Z})$$ as abelian groups.","Let $n$ be a positive integer.  Define $$\textbf{A}_n(x):= \left[\frac{1}{x+i+j-1}\right]_{i,j\in\{1,2,\ldots,n\}}$$ as a matrix over the field $\mathbb{Q}(x)$ of rational functions over $\mathbb{Q}$ in variable $x$. (a) Prove that the Hilbert matrix $\textbf{A}_n(0)$ is an invertible matrix over $\mathbb{Q}$ and all entries of the inverse of $\textbf{A}_n(0)$ are integers. (b) Determine the greatest common divisor (over $\mathbb{Z}$) of all the entries of $\big(\textbf{A}_n(0)\big)^{-1}$. (c) Show that $\textbf{A}_n(x)$ is an invertible matrix over $\mathbb{Q}(x)$ and every entry of the inverse of $\textbf{A}_n(x)$ is a polynomial in $x$. (d) Prove that $x+n$ is the greatest common divisor (over $\mathbb{Q}[x]$) of all the entries of $\big(\textbf{A}_n(x)\big)^{-1}$. Parts (a) and (c) are known. Parts (b) and (d) are open. Now, Part (d) is known (see i707107's solution below), but Part (b) remains open, although it seems like the answer is $n$. Recall that  $$\binom{t}{r}=\frac{t(t-1)(t-2)\cdots(t-r+1)}{r!}$$ for all $t\in\mathbb{Q}(x)$ and $r=0,1,2,\ldots$.  According to i707107, the $(i,j)$-entry of $\big(\textbf{A}_n(x)\big)^{-1}$ is given by $$\alpha_{i,j}(x)=(-1)^{i+j}\,(x+n)\,\binom{x+n+i-1}{i-1}\,\binom{x+n-1}{n-j}\,\binom{x+n+j-1}{n-i}\,\binom{x+i+j-2}{j-1}\,.\tag{*}$$ This means that, for all integers $k$ such that $k\notin\{-1,-2,\ldots,-2n+1\}$, the entries of $\big(\textbf{A}_n(k)\big)^{-1}$ are integers. I now have a new conjecture, which is the primary target for the bounty award. Conjecture: The greatest common divisor $\gamma_n(k)$ over $\mathbb{Z}$ of the entries of $\big(\textbf{A}_n(k)\big)^{-1}$, where $k$ is an integer not belonging in the set $\{-1,-2,\ldots,-2n+1\}$, is given by $$\gamma_n(k)=\mathrm{lcm}(n,n+k)\,.$$ It is clear from (*) that $n+k$ must divide $\gamma_n(k)$.  However, it is not yet clear to me why $n$ should divide $\gamma_n(k)$.  I would like to have a proof of this conjecture, or at least a proof that $n \mid \gamma_n(k)$. Let $M_n$ denote the (unitary) cyclic $\mathbb{Z}[x]$-module generated by $\dfrac{1}{\big((n-1)!\big)^2}\,(x+n)$.  Then, the (unitary)  $\mathbb{Z}[x]$-module $N_n$ generated by the entries of $\big(\textbf{A}_n(x)\big)^{-1}$ is a $\mathbb{Z}[x]$-submodule of $M_n$. We also denote by $\tilde{M}_n$ for the (unitary)  $\mathbb{Z}$-module generated by $\dfrac{1}{\big((n-1)!\big)^2}\,(x+n)\,x^l$ for $l=0,1,2,\ldots,2n-2$.  Then, the (unitary) $\mathbb{Z}$-module $\tilde{N}_n$ generated by the entries of $\big(\textbf{A}_n(x)\big)^{-1}$ is a $\mathbb{Z}$-submodule of $\tilde{M}_n$. For example, $M_2/N_2$ is isomorphic to the (unitary) $\mathbb{Z}[x]$-module $\mathbb{Z}/2\mathbb{Z}$ (in which $x$ acts trivially), and $\tilde{M}_2/\tilde{N}_2$ is isomorphic to the (unitary) $\mathbb{Z}$-module $\mathbb{Z}/2\mathbb{Z}$.  Hence, $\left|M_2/N_2\right|=2=\left|\tilde{M}_2/\tilde{N}_2\right|$.  For $n=3$, Mathematica yields $$\tilde{M}_3/\tilde{N}_3\cong (\mathbb{Z}/2\mathbb{Z})\oplus(\mathbb{Z}/3\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/4\mathbb{Z})^{\oplus 3}\,,$$ as abelian groups.  That is, $\left|\tilde{M}_3/\tilde{N}_3\right|=1152$.  On the other hand,  $$M_3/N_3\cong \mathbb{Z}[x] \big/\left(12,2x^2+6x+4,x^4-x^2\right)$$ as $\mathbb{Z}[x]$-modules, which gives $\left|M_3/N_3\right|=576$. Question: Describe the factor $\mathbb{Z}[x]$-module $M_n/N_n$ and the factor $\mathbb{Z}$-module $\tilde{M}_n/\tilde{N}_n$.  It is easily seen that $\left|M_n/N_n\right|\leq\left|\tilde{M}_n/\tilde{N}_n\right|$.  What are $\left|M_n/N_n\right|$ and $\left|\tilde{M}_n/\tilde{N}_n\right|$?  It can be shown also that the ratio $\dfrac{\left|\tilde{M}_n/\tilde{N}_n\right|}{\left|M_n/N_n\right|}$ is an integer, provided that $\left|\tilde{M}_n/\tilde{N}_n\right|$ is finite.  Compute $\dfrac{\left|\tilde{M}_n/\tilde{N}_n\right|}{\left|M_n/N_n\right|}$ for all integers $n>0$ such that $\left|\tilde{M}_n/\tilde{N}_n\right|<\infty$.  Is it always the case that $\left|\tilde{M}_n/\tilde{N}_n\right|$ is finite? Apart from the conjecture above, this question is also eligible for the bounty award. I have not yet fully tried to deal with any case involving $n>3$.  However, for $n=4$, the module $\tilde{M}_4/\tilde{N}_4$ is huge: $$  \tilde{M}_4/\tilde{N}_4\cong (\mathbb{Z}/2\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/3\mathbb{Z})^{\oplus 3}\oplus(\mathbb{Z}/8\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/9\mathbb{Z})^{\oplus 2}\oplus(\mathbb{Z}/16\mathbb{Z})\oplus(\mathbb{Z}/27\mathbb{Z})$$ as abelian groups.",,"['matrices', 'polynomials', 'modules', 'gcd-and-lcm', 'rational-functions']"
95,"Prove or disprove: For $2\times 2$ matrices $A$ and $B$, if $(AB)^2=0$, then $(BA)^2=0$.","Prove or disprove: For  matrices  and , if , then .",2\times 2 A B (AB)^2=0 (BA)^2=0,"My goal is to prove or disprove the following claim: For $2\times 2$ matrices $A$ and $B$ , if $(AB)^2=0$ , then $(BA)^2=0$ . My thoughts on the question: I know that $AB=O$ does not imply that $BA=O$ , so my first impression was that it is false. I tried the counter-example I know but it leads to $(BA)^2=O$ . ( edited ) As pointed out by Friedrich Philipp in the comments, $A$ or $B$ is not invertible. If one of them is invertible, then the question is easily shown to be true. I wish to avoid density arguments though, so I am still stuck with the case where $A$ and $B$ are both singular. The question is in a list of  prove or disprove questions for square matrices of any order. This specific one precise that $A$ and $B$ are of order $2$ , so maybe the property is true for these matrices. I seems to remember that if $M$ is a square matrix of order $n$ and is nilpotent, then the order of nilpotence (the smallest $p$ such that $M^p=O$ ) is at most $n$ , but I don't know how to use it here. Beside this, I am clueless. I am looking for ideas or hints on the problem.","My goal is to prove or disprove the following claim: For matrices and , if , then . My thoughts on the question: I know that does not imply that , so my first impression was that it is false. I tried the counter-example I know but it leads to . ( edited ) As pointed out by Friedrich Philipp in the comments, or is not invertible. If one of them is invertible, then the question is easily shown to be true. I wish to avoid density arguments though, so I am still stuck with the case where and are both singular. The question is in a list of  prove or disprove questions for square matrices of any order. This specific one precise that and are of order , so maybe the property is true for these matrices. I seems to remember that if is a square matrix of order and is nilpotent, then the order of nilpotence (the smallest such that ) is at most , but I don't know how to use it here. Beside this, I am clueless. I am looking for ideas or hints on the problem.",2\times 2 A B (AB)^2=0 (BA)^2=0 AB=O BA=O (BA)^2=O A B A B A B 2 M n p M^p=O n,['matrices']
96,"Find unit vector given Roll, Pitch and Yaw","Find unit vector given Roll, Pitch and Yaw",,"Is it possible to find the unit vector with: Roll € [-90 (banked to right), 90 (banked to left)] , Pitch € [-90 (all the way down), 90 (all the way up)] Yaw € [0, 360 (N)] I calculated it without the Roll and it is \begin{pmatrix} cos(Pitch)  sin(Yaw)\\  cos(Yaw)  cos(Pitch)\\  sin(Pitch) \end{pmatrix}. How should it be with the Roll rotation and how can I get to this result? My coordinate system is with +z up , +x right and +y forward Many thanks!","Is it possible to find the unit vector with: Roll € [-90 (banked to right), 90 (banked to left)] , Pitch € [-90 (all the way down), 90 (all the way up)] Yaw € [0, 360 (N)] I calculated it without the Roll and it is \begin{pmatrix} cos(Pitch)  sin(Yaw)\\  cos(Yaw)  cos(Pitch)\\  sin(Pitch) \end{pmatrix}. How should it be with the Roll rotation and how can I get to this result? My coordinate system is with +z up , +x right and +y forward Many thanks!",,"['matrices', 'geometry', 'trigonometry', 'vectors', 'rotations']"
97,Exponential of a matrix always converges,Exponential of a matrix always converges,,"I am trying to show that the exponential of a matrix converges for any given square matrix of size $n\times n$: $M\mapsto e^M$ e.g. $\displaystyle e^M = \sum_{n=0}^\infty \frac{M^n}{n!}$ Can I argue that: Since $n!$ necessarily grows faster than $k^n$ will, that this converges. This seems to be an obvious fact, since: $$n!=1\times 2\times 3\times \cdots \times k\times (k+1)\times (k+2)\times \cdots$$ $$k^n=k\times k\times k \times\cdots\times k \times k\times \cdots$$ If we have some $q\times q$ matrix, with $a$'s in each position(which will grow as fast as we make our $a$ and $q$ large) we still only get increasing at a rate of $q^{n-1}\times a^n$ In light of the comments, I know that in this banach space, I need only show that $\displaystyle e^M = \sum_{n=0}^\infty \frac{||M||^n}{n!}$ converges. Now I have many matrix norms to choose from, and I can't seem to get a good argument going rigorously. Any ideas?","I am trying to show that the exponential of a matrix converges for any given square matrix of size $n\times n$: $M\mapsto e^M$ e.g. $\displaystyle e^M = \sum_{n=0}^\infty \frac{M^n}{n!}$ Can I argue that: Since $n!$ necessarily grows faster than $k^n$ will, that this converges. This seems to be an obvious fact, since: $$n!=1\times 2\times 3\times \cdots \times k\times (k+1)\times (k+2)\times \cdots$$ $$k^n=k\times k\times k \times\cdots\times k \times k\times \cdots$$ If we have some $q\times q$ matrix, with $a$'s in each position(which will grow as fast as we make our $a$ and $q$ large) we still only get increasing at a rate of $q^{n-1}\times a^n$ In light of the comments, I know that in this banach space, I need only show that $\displaystyle e^M = \sum_{n=0}^\infty \frac{||M||^n}{n!}$ converges. Now I have many matrix norms to choose from, and I can't seem to get a good argument going rigorously. Any ideas?",,"['matrices', 'convergence-divergence']"
98,Is the determinant of a matrix Lipschitz continuous?,Is the determinant of a matrix Lipschitz continuous?,,"I want to know if the determinant of a matrix is Lipschitz continuous or not. To be precise, does there exist a constant $K$ such that $|\det(A)-\det(B)|\leq K||A-B||_F$, for all matrices $A,B\in \mathcal{C}^{n\times n}$? If the answer is no, then what about being Hölder continuous? Does $|\det(A)-\det(B)|\leq K||A-B||_F^\alpha$ hold for some constant $K$ and $\alpha$? Can anyone help me on this? Thank you in advance!","I want to know if the determinant of a matrix is Lipschitz continuous or not. To be precise, does there exist a constant $K$ such that $|\det(A)-\det(B)|\leq K||A-B||_F$, for all matrices $A,B\in \mathcal{C}^{n\times n}$? If the answer is no, then what about being Hölder continuous? Does $|\det(A)-\det(B)|\leq K||A-B||_F^\alpha$ hold for some constant $K$ and $\alpha$? Can anyone help me on this? Thank you in advance!",,"['real-analysis', 'complex-analysis', 'analysis', 'matrices']"
99,Formula for Nth Derivative of Matrix Inverse,Formula for Nth Derivative of Matrix Inverse,,"I was looking for an equation for the nth derivative of a matrix inverse, ie $\frac{d^n \bf{A}^{-1}}{dx^n}$ I know that the first derivative $\frac{\text{d} \bf{A}^{-1}}{\text{d}x} = -\bf{A}^{-1} \frac{\text{d} \bf{A}}{\text{d} x} \bf{A}^{-1}$ But is there some sort of generalization of this without having to chain/product rule it?","I was looking for an equation for the nth derivative of a matrix inverse, ie $\frac{d^n \bf{A}^{-1}}{dx^n}$ I know that the first derivative $\frac{\text{d} \bf{A}^{-1}}{\text{d}x} = -\bf{A}^{-1} \frac{\text{d} \bf{A}}{\text{d} x} \bf{A}^{-1}$ But is there some sort of generalization of this without having to chain/product rule it?",,"['calculus', 'matrices', 'derivatives', 'inverse']"
