,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Prove that the integral $\int_a^b \frac{\sin(x)}{x}dx$ is bounded uniformly.,Prove that the integral  is bounded uniformly.,\int_a^b \frac{\sin(x)}{x}dx,"How do I show that exists a constant $M>0$ such that, for all $0\leq a \leq b < \infty$ , $$\left|\int_a^b \frac{\sin(x)}{x}dx\right| \leq M.$$ I just read on Richard Bass book's that is enough to prove the uniformly boundeness of the integral $\int_0^b \frac{\sin(x)}{x}dx$ , but I can't see why...","How do I show that exists a constant such that, for all , I just read on Richard Bass book's that is enough to prove the uniformly boundeness of the integral , but I can't see why...",M>0 0\leq a \leq b < \infty \left|\int_a^b \frac{\sin(x)}{x}dx\right| \leq M. \int_0^b \frac{\sin(x)}{x}dx,"['real-analysis', 'calculus', 'analysis', 'measure-theory']"
1,"If $a$, $b$, $c \geq 1$; $y \geq x \geq 1$; $p$, $q$, $r > 0$ Prove the insane inequality","If , , ; ; , ,  Prove the insane inequality",a b c \geq 1 y \geq x \geq 1 p q r > 0,$$\left(\frac{1+y\left(a^pb^qc^r\right)^{\frac{1}{p+q+r}}}{1+x\left(a^pb^qc^r\right)^{\frac{1}{p+q+r}}}\right)^{\frac{p+q+r}{\left(a^pb^qc^r\right)^{\frac{1}{p+q+r}}}}\left(\frac{1+xa}{1+ya}\right)^{\frac{p}{a}}\left(\frac{1+xb}{1+yb}\right)^{\frac{q}{b}}\left(\frac{1+xc}{1+yc}\right)^{\frac{r}{c}}\geq \prod \limits_{cyc}\left(\frac{1+y\left(a^pb^q\right)^{\frac{1}{p+q}}}{1+x\left(a^pb^q\right)^{\frac{1}{p+q}}}\right)^{\frac{p+q}{\left(a^pb^q\right)^{\frac{1}{p+q}}}}$$ This is a inequality problem from 2019 Jozsef Wildt International Mathematics Competition . For this problem i almost tried in all ways with AM-GM and Cauchy-Schwarz but i am unable to do it. Honestly i don't even know how to start thinking for this problem. Please help. I know i have not uploaded my attempts. But what i shall attempt. I am completely blank about this problem,This is a inequality problem from 2019 Jozsef Wildt International Mathematics Competition . For this problem i almost tried in all ways with AM-GM and Cauchy-Schwarz but i am unable to do it. Honestly i don't even know how to start thinking for this problem. Please help. I know i have not uploaded my attempts. But what i shall attempt. I am completely blank about this problem,\left(\frac{1+y\left(a^pb^qc^r\right)^{\frac{1}{p+q+r}}}{1+x\left(a^pb^qc^r\right)^{\frac{1}{p+q+r}}}\right)^{\frac{p+q+r}{\left(a^pb^qc^r\right)^{\frac{1}{p+q+r}}}}\left(\frac{1+xa}{1+ya}\right)^{\frac{p}{a}}\left(\frac{1+xb}{1+yb}\right)^{\frac{q}{b}}\left(\frac{1+xc}{1+yc}\right)^{\frac{r}{c}}\geq \prod \limits_{cyc}\left(\frac{1+y\left(a^pb^q\right)^{\frac{1}{p+q}}}{1+x\left(a^pb^q\right)^{\frac{1}{p+q}}}\right)^{\frac{p+q}{\left(a^pb^q\right)^{\frac{1}{p+q}}}},"['analysis', 'multivariable-calculus', 'inequality', 'exponential-function', 'jensen-inequality']"
2,"For continuous, monotonically-increasing $f$ with $f(0)=0$ and $f(1)=1$, prove $\sum_{k=1}^{10}f(k/10)+f^{-1}(k/10)\leq 99/10$","For continuous, monotonically-increasing  with  and , prove",f f(0)=0 f(1)=1 \sum_{k=1}^{10}f(k/10)+f^{-1}(k/10)\leq 99/10,"A question from Leningrad Mathematical Olympiad 1991: Let $f$ be continuous and monotonically increasing, with $f(0)=0$ and $f(1)=1$ .   Prove that: $$ \text{f}\left( \frac{1}{10} \right) +\text{f}\left( \frac{2}{10} \right) +...+\text{f}\left( \frac{9}{10} \right) +\text{f}^{-1}\left( \frac{1}{10} \right) +\text{f}^{-1}\left( \frac{2}{10} \right) +...+\text{f}^{-1}\left( \frac{9}{10} \right) \leqslant \frac{99}{10} $$ I tried to express them in areas to find inequalities but failed.","A question from Leningrad Mathematical Olympiad 1991: Let be continuous and monotonically increasing, with and .   Prove that: I tried to express them in areas to find inequalities but failed.","f f(0)=0 f(1)=1 
\text{f}\left( \frac{1}{10} \right) +\text{f}\left( \frac{2}{10} \right) +...+\text{f}\left( \frac{9}{10} \right) +\text{f}^{-1}\left( \frac{1}{10} \right) +\text{f}^{-1}\left( \frac{2}{10} \right) +...+\text{f}^{-1}\left( \frac{9}{10} \right) \leqslant \frac{99}{10}
","['analysis', 'functions', 'inequality', 'contest-math']"
3,Solving an integral equation with Laplace transform and convolution,Solving an integral equation with Laplace transform and convolution,,"So I solved the integral equation $\int_{-\infty}^\infty f(t)f(x-t)\,dt = f(x)$ using the Fourier transform and convolution. With $F$ as $f$ 's Fourier transform, I found that $F²=F,$ then by using $F$ 's continuity and integrability I concluded that $F$ was the null function on $\Bbb R$ and so was $f$ . Now I've got a very similar equation to solve : $$\int_{0}^x f(t)f(x-t)\,dt = f(x).$$ Now it still looks like a convolution product but from Laplace's transform point of view, which use is strongly recommended in order to solve this. I fail to see why the conclusion would be any different, though; can someone push me in the right direction? Thanks a lot!","So I solved the integral equation using the Fourier transform and convolution. With as 's Fourier transform, I found that then by using 's continuity and integrability I concluded that was the null function on and so was . Now I've got a very similar equation to solve : Now it still looks like a convolution product but from Laplace's transform point of view, which use is strongly recommended in order to solve this. I fail to see why the conclusion would be any different, though; can someone push me in the right direction? Thanks a lot!","\int_{-\infty}^\infty f(t)f(x-t)\,dt = f(x) F f F²=F, F F \Bbb R f \int_{0}^x f(t)f(x-t)\,dt = f(x).","['analysis', 'laplace-transform', 'fourier-transform', 'convolution']"
4,a Differential inequality without integration,a Differential inequality without integration,,"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ a differentiable function with $f(x)+f^{'}(x)\leq1$ for all $x \in \mathbb{R}$ and $f(0)=0$ . Which is the maximum possible value of $f(1)$ ? The question is ""solved"" here : maximum value and a differential inequality but I did a mistake by supposing that $\frac{d}{d x} (e^x f(x)) $ is integrable in the interval $[0,1]$ . I don't know how to solve the problem without such condition. Someone could help me? thanks in advance","Let a differentiable function with for all and . Which is the maximum possible value of ? The question is ""solved"" here : maximum value and a differential inequality but I did a mistake by supposing that is integrable in the interval . I don't know how to solve the problem without such condition. Someone could help me? thanks in advance","f:\mathbb{R}\rightarrow\mathbb{R} f(x)+f^{'}(x)\leq1 x \in \mathbb{R} f(0)=0 f(1) \frac{d}{d x} (e^x f(x))  [0,1]","['real-analysis', 'analysis']"
5,$L^p$ estimates for scalar conservation laws,estimates for scalar conservation laws,L^p,"Consider the initial value problem for scalar conservation laws $$\begin{eqnarray} u_t+f(u)_x=0\\ u(x,0)=u_0(x) \end{eqnarray}$$ If $u_0 \in L^{\infty}(\mathbb{R})$ we have $\Vert u(.,t)\Vert_{\infty} \leq \Vert u_0\Vert_{\infty}$ If $u_0 \in L^{1}(\mathbb{R})$ we have $\Vert u(.,t)\Vert_{1} \leq \Vert u_0\Vert_{1}$ (which follows from the contraction principle) Do we have such results for other values of $p \in (0,\infty)$ ? If so how to prove it? Any comments or references would be greatly appreciated.",Consider the initial value problem for scalar conservation laws If we have If we have (which follows from the contraction principle) Do we have such results for other values of ? If so how to prove it? Any comments or references would be greatly appreciated.,"\begin{eqnarray} u_t+f(u)_x=0\\
u(x,0)=u_0(x) \end{eqnarray} u_0 \in L^{\infty}(\mathbb{R}) \Vert u(.,t)\Vert_{\infty} \leq \Vert u_0\Vert_{\infty} u_0 \in L^{1}(\mathbb{R}) \Vert u(.,t)\Vert_{1} \leq \Vert u_0\Vert_{1} p \in (0,\infty)","['analysis', 'partial-differential-equations', 'hyperbolic-equations']"
6,Distance to the boundary function in a disk,Distance to the boundary function in a disk,,"Let $B_1 = B(0, 1)$ denote the open unit disk in $\mathbb{R}^d$ . For $x \in B_1$ , the ""distance to the boundary $\partial B_1$ "" function is defined as \begin{equation} d(x) = \inf_{y\in \partial B_1} \|x-y\|. \end{equation} This definition is of course more general and not specific to $B_1$ . My question is the following: In the special case of the unit disk $B_1$ , is it true (and if yes, why) that \begin{equation} d(x) = 1-\|x\| \end{equation} for any $x \in B_1$ ?","Let denote the open unit disk in . For , the ""distance to the boundary "" function is defined as This definition is of course more general and not specific to . My question is the following: In the special case of the unit disk , is it true (and if yes, why) that for any ?","B_1 = B(0, 1) \mathbb{R}^d x \in B_1 \partial B_1 \begin{equation}
d(x) = \inf_{y\in \partial B_1} \|x-y\|.
\end{equation} B_1 B_1 \begin{equation}
d(x) = 1-\|x\|
\end{equation} x \in B_1","['analysis', 'metric-spaces']"
7,Conditions for two optimization problems to yield the same solution,Conditions for two optimization problems to yield the same solution,,"Problem: Consider the optimization problems $$\min_\beta \|y-X\beta\|^2+\alpha\|\beta\|^2 \tag 1$$ and $$\min_\beta \|\beta\|^2 \text{ subject to } \|y-X\beta\|^2 \le c \tag 2$$ where $\|x\|$ is the $2$ -norm. Fix $\alpha$ , and suppose $\beta^*$ is the solution to ( $1$ ), and let $c=\|y-X\beta^*\|^2$ . Is it true that the solution to ( $2$ ) is also $\beta^*$ ? Attempt: I believe this is true. The argument should be very similar to the one in Why are additional constraint and penalty term equivalent in ridge regression? . However, I was running some numerical experiments and it turns out the two problems have different solutions. Hence my question here: are the two problems really yielding the same solutions? Are there exceptions that I should be careful of?","Problem: Consider the optimization problems and where is the -norm. Fix , and suppose is the solution to ( ), and let . Is it true that the solution to ( ) is also ? Attempt: I believe this is true. The argument should be very similar to the one in Why are additional constraint and penalty term equivalent in ridge regression? . However, I was running some numerical experiments and it turns out the two problems have different solutions. Hence my question here: are the two problems really yielding the same solutions? Are there exceptions that I should be careful of?",\min_\beta \|y-X\beta\|^2+\alpha\|\beta\|^2 \tag 1 \min_\beta \|\beta\|^2 \text{ subject to } \|y-X\beta\|^2 \le c \tag 2 \|x\| 2 \alpha \beta^* 1 c=\|y-X\beta^*\|^2 2 \beta^*,"['analysis', 'optimization', 'numerical-methods', 'convex-optimization', 'regression']"
8,Every open subset $U\subseteq\mathbb{R}$ is countable union of disjoint open intervals.,Every open subset  is countable union of disjoint open intervals.,U\subseteq\mathbb{R},"Theorem. Every open subset $U\subseteq\mathbb{R}$ is countable union of disjoint open intervals. I was looking for proofs for this result and I came to this interesting post: Any open subset of $\Bbb R$ is a at most countable union of disjoint open intervals. [Collecting Proofs] . Among all the proofs I started from the simplest one: the one written by G.T. https://math.stackexchange.com/a/1949873/554978 . However, I am not convinced of the proof that $I_x$ is an interval. I do not know if I did not understand correctly or the proof in question is not valid, so I propose one myself and I would like you to tell me which one is valid. Definition. An interval is a subset $I\subseteq\mathbb{R}$ such that, for all $a<c<b$ in $\mathbb{R}$ , if $a,b\in I$ then $c\in I$ . Let $x\in U$ and we suppose that $x\in\mathbb{Q}$ , then define \begin{align} I_x = \bigcup\limits_{\substack{I\text{ an open interval} \\ x~\in~I~\subseteq~U}} I,\end{align} we must prove that $I_x$ is an interval. About that let $a,b\in I_x$ such that $a<c<b$ , then we must prove that $c\in I_x$ . Since $a,b\in I_x$ , then $a,b\in I$ for same open interval $I$ which contains $x$ . If $a$ and $b$ belong to the same $I$ , since $a<c<b$ and $I$ is an interval, $c\in I$ , therefore $c\in I_x$ . Now, we denote with $I_a$ the open interval of $I_x$ which contains $a$ , but not $b$ and we denote with $I_b$ the open interval of $I_x$ which contains $b$ , but not $a$ . First case: $[c=x]$ . If $c=x$ , then $c\in I_x$ by definition of $I_x$ ; Second case: $[c<x]$ . If $c<x$ , then either $a<c<x<b$ or $a<c<b<x$ . $(i)$ If $a<c<x<b$ , since $a\in I_a$ and $x\in I_a$ and $I_a$ is an interval, then $c\in I_a$ , therefore $c\in I_x$ . $(ii)$ If $a<c<b<x$ , since $x\in I_a$ and $a\in I_a$ , and $I_a$ is an interval we have that $b\in I_a$ , absurd. Third case $[c>x]$ . If $c>x$ , then either $a<c<x<b$ or $x<a<c<b$ . $(i)$ If $a<c<x<b$ , since $a\in I_a$ , $x\in I_a$ and $I_a$ is an interval we have that $c\in I_a$ therefore $c\in I_x$ . $(ii)$ If $x<a<c<b$ , since $x\in I_b$ and $b\in I_b$ , we have that $a\in I_b$ absurd. Then in general $c\in I_x$ , this prove that $I_x$ is an interval. Thanks!","Theorem. Every open subset is countable union of disjoint open intervals. I was looking for proofs for this result and I came to this interesting post: Any open subset of is a at most countable union of disjoint open intervals. [Collecting Proofs] . Among all the proofs I started from the simplest one: the one written by G.T. https://math.stackexchange.com/a/1949873/554978 . However, I am not convinced of the proof that is an interval. I do not know if I did not understand correctly or the proof in question is not valid, so I propose one myself and I would like you to tell me which one is valid. Definition. An interval is a subset such that, for all in , if then . Let and we suppose that , then define we must prove that is an interval. About that let such that , then we must prove that . Since , then for same open interval which contains . If and belong to the same , since and is an interval, , therefore . Now, we denote with the open interval of which contains , but not and we denote with the open interval of which contains , but not . First case: . If , then by definition of ; Second case: . If , then either or . If , since and and is an interval, then , therefore . If , since and , and is an interval we have that , absurd. Third case . If , then either or . If , since , and is an interval we have that therefore . If , since and , we have that absurd. Then in general , this prove that is an interval. Thanks!","U\subseteq\mathbb{R} \Bbb R I_x I\subseteq\mathbb{R} a<c<b \mathbb{R} a,b\in I c\in I x\in U x\in\mathbb{Q} \begin{align} I_x = \bigcup\limits_{\substack{I\text{ an open interval} \\ x~\in~I~\subseteq~U}} I,\end{align} I_x a,b\in I_x a<c<b c\in I_x a,b\in I_x a,b\in I I x a b I a<c<b I c\in I c\in I_x I_a I_x a b I_b I_x b a [c=x] c=x c\in I_x I_x [c<x] c<x a<c<x<b a<c<b<x (i) a<c<x<b a\in I_a x\in I_a I_a c\in I_a c\in I_x (ii) a<c<b<x x\in I_a a\in I_a I_a b\in I_a [c>x] c>x a<c<x<b x<a<c<b (i) a<c<x<b a\in I_a x\in I_a I_a c\in I_a c\in I_x (ii) x<a<c<b x\in I_b b\in I_b a\in I_b c\in I_x I_x","['real-analysis', 'analysis', 'proof-verification']"
9,"Solution verification post : Problem $12.1$, Mathematical Analysis, Apostol","Solution verification post : Problem , Mathematical Analysis, Apostol",12.1,"As a continuation of this post , I'm starting to post my solutions (or attempts) to the exercise problems of Chapter 12 (Multivariable Differential Calculus), Mathematical Analysis by Apostol (suggested by Masacroso ). Since I'm essentially self-studying, I'd really appreciate if anyone checks the solutions and let me know if there is any gap in my arguments or if there exists any cleverer solutions. Thank you. Problem $12.1.$ Let $S$ be an open subset of $\mathbb{R}^n$, and let $f:S \to \mathbb{R}$ be a real-valued function with finite partial derivative $D_1f,\dots,D_nf$ on $S$. If $f$ has a local maximum or a local minimum at a point $c$ in $S$, prove that $D_kf\left(c\right)=0 \,\,\forall \,k$. Solution. Suppose that $f$ has a local maximum at $c \in S$. This implies that $\exists \delta>0$ such that $$f(c) \geq f(x) \text{ for all } x \in B_{\delta}(c)=\left\{y \in S : \left\Vert y-c \right\Vert < \delta \right\} \tag1$$ Let $k \in \{1,\dots,n\}$. Now, $$D_kf(c)=\lim_{h \to 0} \frac{f(c+he_k)-f(c)}{h} \tag2$$ It is given that $(2)$ exists finitely, then $$D_kf(c)=\lim_{h \to 0;\,h \in (0,\delta)} \frac{f(c+he_k)-f(c)}{h} \leq 0 \tag3$$ On the other hand, $$D_kf(c)=\lim_{h \to 0;\,h \in (-\delta,0)} \frac{f(c+he_k)-f(c)}{h} \geq 0 \tag4$$ $(3)$ and $(4)$ $\Rightarrow$ $D_kf(c)=0$. This holds for every $k \in \{1,\dots,n\}$ and hence we are done.","As a continuation of this post , I'm starting to post my solutions (or attempts) to the exercise problems of Chapter 12 (Multivariable Differential Calculus), Mathematical Analysis by Apostol (suggested by Masacroso ). Since I'm essentially self-studying, I'd really appreciate if anyone checks the solutions and let me know if there is any gap in my arguments or if there exists any cleverer solutions. Thank you. Problem $12.1.$ Let $S$ be an open subset of $\mathbb{R}^n$, and let $f:S \to \mathbb{R}$ be a real-valued function with finite partial derivative $D_1f,\dots,D_nf$ on $S$. If $f$ has a local maximum or a local minimum at a point $c$ in $S$, prove that $D_kf\left(c\right)=0 \,\,\forall \,k$. Solution. Suppose that $f$ has a local maximum at $c \in S$. This implies that $\exists \delta>0$ such that $$f(c) \geq f(x) \text{ for all } x \in B_{\delta}(c)=\left\{y \in S : \left\Vert y-c \right\Vert < \delta \right\} \tag1$$ Let $k \in \{1,\dots,n\}$. Now, $$D_kf(c)=\lim_{h \to 0} \frac{f(c+he_k)-f(c)}{h} \tag2$$ It is given that $(2)$ exists finitely, then $$D_kf(c)=\lim_{h \to 0;\,h \in (0,\delta)} \frac{f(c+he_k)-f(c)}{h} \leq 0 \tag3$$ On the other hand, $$D_kf(c)=\lim_{h \to 0;\,h \in (-\delta,0)} \frac{f(c+he_k)-f(c)}{h} \geq 0 \tag4$$ $(3)$ and $(4)$ $\Rightarrow$ $D_kf(c)=0$. This holds for every $k \in \{1,\dots,n\}$ and hence we are done.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives', 'proof-verification']"
10,"$f((x,y)^T)=\frac{xy^2}{x^2+y^2}$ not differentiable in $(0,0)^T$",not differentiable in,"f((x,y)^T)=\frac{xy^2}{x^2+y^2} (0,0)^T","$f:\mathbb{R}^2 \to \mathbb{R}$ $f\Bigg(\begin{matrix}x\\y\end{matrix}\Bigg)=\begin{cases}\frac{xy^2}{x^2+y^2},(x,y)^T \neq(0,0)^T \\0 , (x,y)^T=(0,0)^T\end{cases}$ I need to determine all partial derivatives for $(x,y)^T \in \mathbb{R}^2$: $f_x=y^2/(x^2+y^2)-2x^2y^2/(x^2+y^2)^2$ for $(x,y)^T \neq (0,0)$ $f_y=2xy/(x^2+y^2)-2xy^3/(x^2+y^2)^2$ for $(x,y)^T \neq (0,0)$ and $f_x=f_y=0$ for $(x,y)^T = (0,0)$. Then I need to determine $\frac{\partial f}{\partial v}((0,0)^T)$ for all $v=(v_1,v_2)^T \in \mathbb{R}^2$. I tried: $\frac{1}{s} (f(x+sv)-f(x))$ at $x=(0,0)^T$ is equal to $\frac{1}{s} f(sv)$=$\frac{1}{s} f\Big(\begin{matrix}sv_1\\sv_2\end{matrix}\Big)$. Which is either equal to $0$ when the argument is $(0,0)^T$ or it is $\frac{1}{s}\frac{sv_1s^2v_2^2}{s^2v_1^2+s^2v_2^2}$ which converges to $\frac{v_1v_2^2}{v_1^2+v_2^2}$ as $s \to \infty$. Is that correct so far? And how do I know if $f$ is continuously partial differentiable on $\mathbb{R}^2$? According to our professor $f$ is not differentiable at $0$. How do I show that? As far as I know it has something to do with that something is not linear but I don't know what exactly. So I guess it can't be continuously partial differentiable on $\mathbb{R}^2$ as well but I am not sure about that. Thanks for your help!","$f:\mathbb{R}^2 \to \mathbb{R}$ $f\Bigg(\begin{matrix}x\\y\end{matrix}\Bigg)=\begin{cases}\frac{xy^2}{x^2+y^2},(x,y)^T \neq(0,0)^T \\0 , (x,y)^T=(0,0)^T\end{cases}$ I need to determine all partial derivatives for $(x,y)^T \in \mathbb{R}^2$: $f_x=y^2/(x^2+y^2)-2x^2y^2/(x^2+y^2)^2$ for $(x,y)^T \neq (0,0)$ $f_y=2xy/(x^2+y^2)-2xy^3/(x^2+y^2)^2$ for $(x,y)^T \neq (0,0)$ and $f_x=f_y=0$ for $(x,y)^T = (0,0)$. Then I need to determine $\frac{\partial f}{\partial v}((0,0)^T)$ for all $v=(v_1,v_2)^T \in \mathbb{R}^2$. I tried: $\frac{1}{s} (f(x+sv)-f(x))$ at $x=(0,0)^T$ is equal to $\frac{1}{s} f(sv)$=$\frac{1}{s} f\Big(\begin{matrix}sv_1\\sv_2\end{matrix}\Big)$. Which is either equal to $0$ when the argument is $(0,0)^T$ or it is $\frac{1}{s}\frac{sv_1s^2v_2^2}{s^2v_1^2+s^2v_2^2}$ which converges to $\frac{v_1v_2^2}{v_1^2+v_2^2}$ as $s \to \infty$. Is that correct so far? And how do I know if $f$ is continuously partial differentiable on $\mathbb{R}^2$? According to our professor $f$ is not differentiable at $0$. How do I show that? As far as I know it has something to do with that something is not linear but I don't know what exactly. So I guess it can't be continuously partial differentiable on $\mathbb{R}^2$ as well but I am not sure about that. Thanks for your help!",,"['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives', 'continuity']"
11,Smooth function on product manifolds,Smooth function on product manifolds,,"suppose $M, N$ are smooth manifolds and $f:M\times N\rightarrow \mathbb{ R }$ is a function such that $\forall y\in N$ the function $x\mapsto f (x, y)$ is smooth and $\forall x\in M$ the function $y\mapsto f (x, y)$ is smooth as well. Does this imply that $f$ is smooth jointly in x and y? The map should be differentiable as far as I know. But I can not see if the map is also differentiable of higher orders. I appreciate any help!","suppose $M, N$ are smooth manifolds and $f:M\times N\rightarrow \mathbb{ R }$ is a function such that $\forall y\in N$ the function $x\mapsto f (x, y)$ is smooth and $\forall x\in M$ the function $y\mapsto f (x, y)$ is smooth as well. Does this imply that $f$ is smooth jointly in x and y? The map should be differentiable as far as I know. But I can not see if the map is also differentiable of higher orders. I appreciate any help!",,"['analysis', 'multivariable-calculus', 'manifolds', 'smooth-manifolds']"
12,Extending the Definition of Asymptotic Density to rationals,Extending the Definition of Asymptotic Density to rationals,,"$\require{enclose}$ Edit: Replace rationals with $\enclose{horizontalstrike}{\mathbb{Z}\times\mathbb{Z}}$. And view fractions as $\enclose{horizontalstrike}{(\text{Numerator},\text{Denominator})}$ instead of $\enclose{horizontalstrike}{\frac{\text{Numerator}}{\text{Denominator}}}$. For more information look at Hurkyl's answer I want to extend the definition of asymptotic density to countably dense sets. The Asymptotic density of a subset of $\mathbb{N}$ gives the ratio of the number of elements from the subset, compared to the number of elements from $\mathbb{N}$, between $[0,n]$ as $n\to\infty$. I want to apply a similar concept to the subsets of rationals which gives a ratio of number of elements from the subset of $\mathbb{Q}$, compared to number of elements from $\mathbb{Q}$, based on restricted intervals. Note this is not the same as extending the definition of asymptotic density to $\mathbb{Z}\times\mathbb{Z}$, as this takes the density of the numerator and denominator separately and counts the same element more than once. This ""new density"" should act as an informal measure. If such a density exists and that density of set $X$ is $D(X)$, then the density for sets $A$ and $B$ should be meet the following requirements such that If set $A=B$ then $D(A)=D(B)$ If set $A\subset{B}$ then $D(A)\le D(B)$ However, I am not sure how to answer this question. So far I determined the following. The rationals or $\left\{\left.\frac{m}{n}\right|m,n\in\mathbb{Z}\right\}$ can be divided into groups of sets that contain eachother. \begin{equation} \left\{\left.\frac{2^{k}m}{2n+1}\right|m,n\in\mathbb{Z}\right\}\subset...\subset\left\{\left.\frac{2m}{2n+1}\right|m,n\in\mathbb{Z}\right\}\subset\left\{\left.\frac{m}{2n+1}\right|m,n\in\mathbb{Z}\right\}\subset\left\{\left.\frac{m}{4n+2}\right|m,n\in\mathbb{Z}\right\}\subset...\subset\left\{\left.\frac{m}{2^{k}(2n+1)}\right|m,n\in\mathbb{Z}\right\}=\left\{\left.\frac{m}{n}\right|m,n\in\mathbb{Z}\right\} \qquad (1) \end{equation} However, each set can be permuted diffferently. For example $\left\{\left.\frac{m}{2n+1}\right|m,n\in\mathbb{Z}\right\}=\left\{\left.\frac{m}{3(2n+1)}\right|m,n\in\mathbb{Z}\right\}=\left\{\left.\frac{m}{5(2n+1)}\right|m,n\in\mathbb{Z}\right\}$ Hence we need identical sets with different permutations to be permuted in the same permutation before taking their density. I believe all sets should be rearranged to have permutations similar to the sets in (1) for two reasons. One, the union of the numerator and denominator of all the set covers every integer that could be in the numeator and denominator. Second, due to their permutations, the sets can easily be shown as the subsets of one another. For example, we can convert $(1)$ into \begin{equation} \left\{\left.\frac{2^{2k}m}{2^k(2n+1)}\right|m,n\in\mathbb{Z}\right\}\subset...\subset\left\{\left.\frac{2^{k+1}m}{2n+1}\right|m,n\in\mathbb{Z}\right\}\subset\left\{\left.\frac{2^{k}m}{2n+1}\right|m,n\in\mathbb{Z}\right\}\subset\left\{\left.\frac{2^{k-1}m}{4n+2}\right|m,n\in\mathbb{Z}\right\}\subset...\subset\left\{\left.\frac{m}{2^{k}(2n+1)}\right|m,n\in\mathbb{Z}\right\}=\left\{\left.\frac{m}{n}\right|m,n\in\mathbb{Z}\right\} \qquad (1) \end{equation} then we can compare the density of the numerators. From here, I attempted an answer below this post but I am not sure if its correct.  If I'm wrong could there still be way of extending asymptotic density to subsets of rationals?","$\require{enclose}$ Edit: Replace rationals with $\enclose{horizontalstrike}{\mathbb{Z}\times\mathbb{Z}}$. And view fractions as $\enclose{horizontalstrike}{(\text{Numerator},\text{Denominator})}$ instead of $\enclose{horizontalstrike}{\frac{\text{Numerator}}{\text{Denominator}}}$. For more information look at Hurkyl's answer I want to extend the definition of asymptotic density to countably dense sets. The Asymptotic density of a subset of $\mathbb{N}$ gives the ratio of the number of elements from the subset, compared to the number of elements from $\mathbb{N}$, between $[0,n]$ as $n\to\infty$. I want to apply a similar concept to the subsets of rationals which gives a ratio of number of elements from the subset of $\mathbb{Q}$, compared to number of elements from $\mathbb{Q}$, based on restricted intervals. Note this is not the same as extending the definition of asymptotic density to $\mathbb{Z}\times\mathbb{Z}$, as this takes the density of the numerator and denominator separately and counts the same element more than once. This ""new density"" should act as an informal measure. If such a density exists and that density of set $X$ is $D(X)$, then the density for sets $A$ and $B$ should be meet the following requirements such that If set $A=B$ then $D(A)=D(B)$ If set $A\subset{B}$ then $D(A)\le D(B)$ However, I am not sure how to answer this question. So far I determined the following. The rationals or $\left\{\left.\frac{m}{n}\right|m,n\in\mathbb{Z}\right\}$ can be divided into groups of sets that contain eachother. \begin{equation} \left\{\left.\frac{2^{k}m}{2n+1}\right|m,n\in\mathbb{Z}\right\}\subset...\subset\left\{\left.\frac{2m}{2n+1}\right|m,n\in\mathbb{Z}\right\}\subset\left\{\left.\frac{m}{2n+1}\right|m,n\in\mathbb{Z}\right\}\subset\left\{\left.\frac{m}{4n+2}\right|m,n\in\mathbb{Z}\right\}\subset...\subset\left\{\left.\frac{m}{2^{k}(2n+1)}\right|m,n\in\mathbb{Z}\right\}=\left\{\left.\frac{m}{n}\right|m,n\in\mathbb{Z}\right\} \qquad (1) \end{equation} However, each set can be permuted diffferently. For example $\left\{\left.\frac{m}{2n+1}\right|m,n\in\mathbb{Z}\right\}=\left\{\left.\frac{m}{3(2n+1)}\right|m,n\in\mathbb{Z}\right\}=\left\{\left.\frac{m}{5(2n+1)}\right|m,n\in\mathbb{Z}\right\}$ Hence we need identical sets with different permutations to be permuted in the same permutation before taking their density. I believe all sets should be rearranged to have permutations similar to the sets in (1) for two reasons. One, the union of the numerator and denominator of all the set covers every integer that could be in the numeator and denominator. Second, due to their permutations, the sets can easily be shown as the subsets of one another. For example, we can convert $(1)$ into \begin{equation} \left\{\left.\frac{2^{2k}m}{2^k(2n+1)}\right|m,n\in\mathbb{Z}\right\}\subset...\subset\left\{\left.\frac{2^{k+1}m}{2n+1}\right|m,n\in\mathbb{Z}\right\}\subset\left\{\left.\frac{2^{k}m}{2n+1}\right|m,n\in\mathbb{Z}\right\}\subset\left\{\left.\frac{2^{k-1}m}{4n+2}\right|m,n\in\mathbb{Z}\right\}\subset...\subset\left\{\left.\frac{m}{2^{k}(2n+1)}\right|m,n\in\mathbb{Z}\right\}=\left\{\left.\frac{m}{n}\right|m,n\in\mathbb{Z}\right\} \qquad (1) \end{equation} then we can compare the density of the numerators. From here, I attempted an answer below this post but I am not sure if its correct.  If I'm wrong could there still be way of extending asymptotic density to subsets of rationals?",,"['real-analysis', 'analysis', 'asymptotics']"
13,$h$ is strictly monotone iff polynomials in $h$ are uniformly dense,is strictly monotone iff polynomials in  are uniformly dense,h h,"Suppose $h$ is continuous on $[0,1]$. Then, I have to prove the following: $h$ is strictly monotone iff every continuous function on $[0,1]$ can be uniformly approximated on $[0,1]$ by a polynomial in $h$. I know we get to use here the Stone—Weierstrass theorem. I proved one direction, that is if $h$ is continuous on $[0,1]$ and $h$ is strictly monotone then every function on $[0,1]$ can be uniformly approximated by a polynomial in $h$. But I am having a hard time doing the converse part.","Suppose $h$ is continuous on $[0,1]$. Then, I have to prove the following: $h$ is strictly monotone iff every continuous function on $[0,1]$ can be uniformly approximated on $[0,1]$ by a polynomial in $h$. I know we get to use here the Stone—Weierstrass theorem. I proved one direction, that is if $h$ is continuous on $[0,1]$ and $h$ is strictly monotone then every function on $[0,1]$ can be uniformly approximated by a polynomial in $h$. But I am having a hard time doing the converse part.",,"['real-analysis', 'analysis', 'weierstrass-approximation']"
14,"Prove that $m^*(A\cup B)=m^*(A)+m^*(B)$ whenever $\exists \alpha>0$ such that $|a-b|>\alpha$ for any $a\in A,b\in B$",Prove that  whenever  such that  for any,"m^*(A\cup B)=m^*(A)+m^*(B) \exists \alpha>0 |a-b|>\alpha a\in A,b\in B","Let $A,B$ be bounded sets in $\Bbb R$ for which $\exists \alpha>0$ such that $|a-b|>\alpha$ for any $a\in A,b\in B$ . Prove that $m^*(A\cup B)=m^*(A)+m^*(B)$ . By countable sub-additivity of outer measure $m^*(A\cup B)\le m^*(A)+m^*(B)$ . To show the reverse inequality let $\epsilon>0$ . Then there exists  a sequence of open intervals $\{I_n\}$ such that $A\cup B\subset \cup I_n$ and $m^*(A\cup B)+\epsilon >\sum l(I_n)$ . If I can show that for each sequence of intervals $\{I_n^{'}\}$ and $\{I_n^{""}\}$ covering $A,B$ respectively we have $\sum l(I_n)>\sum l(I_n^{'})+\sum l(I_n^{""})$ then we are done. But I am stuck .Any help.",Let be bounded sets in for which such that for any . Prove that . By countable sub-additivity of outer measure . To show the reverse inequality let . Then there exists  a sequence of open intervals such that and . If I can show that for each sequence of intervals and covering respectively we have then we are done. But I am stuck .Any help.,"A,B \Bbb R \exists \alpha>0 |a-b|>\alpha a\in A,b\in B m^*(A\cup B)=m^*(A)+m^*(B) m^*(A\cup B)\le m^*(A)+m^*(B) \epsilon>0 \{I_n\} A\cup B\subset \cup I_n m^*(A\cup B)+\epsilon >\sum l(I_n) \{I_n^{'}\} \{I_n^{""}\} A,B \sum l(I_n)>\sum l(I_n^{'})+\sum l(I_n^{""})","['analysis', 'measure-theory', 'lebesgue-measure', 'outer-measure']"
15,Proof of Almost uniform convergence implies Convergence almost everywhere,Proof of Almost uniform convergence implies Convergence almost everywhere,,"I read the same proof that almost uniform convergence implies convergence almost everywhere on several sources (Friedman's Foundations of Modern Analysis and online sources), and they all seem to use the same proof: Proof taken from Proofwiki, https://proofwiki.org/wiki/Convergence_a.u._Implies_Convergence_a.e. However, I have a problem with this proof. While the set on which $f_n$ does not converge to $f$ is definitely a subset of $B$, I do not see why the reverse inclusion (that $f_n$ does not converge to $f$ for every element of $B$) is true. Hence, might it not be possible that $f_n$ converges to $f$only on a proper subset of $B$ that just happens to be non-measurable? (Which is possible since the measure space is not specified to be complete.) Then the proof would be false. I would really appreciate help in understanding why my critique of the proof does not hold. Thanks in advance.","I read the same proof that almost uniform convergence implies convergence almost everywhere on several sources (Friedman's Foundations of Modern Analysis and online sources), and they all seem to use the same proof: Proof taken from Proofwiki, https://proofwiki.org/wiki/Convergence_a.u._Implies_Convergence_a.e. However, I have a problem with this proof. While the set on which $f_n$ does not converge to $f$ is definitely a subset of $B$, I do not see why the reverse inclusion (that $f_n$ does not converge to $f$ for every element of $B$) is true. Hence, might it not be possible that $f_n$ converges to $f$only on a proper subset of $B$ that just happens to be non-measurable? (Which is possible since the measure space is not specified to be complete.) Then the proof would be false. I would really appreciate help in understanding why my critique of the proof does not hold. Thanks in advance.",,"['analysis', 'measure-theory', 'convergence-divergence', 'uniform-convergence', 'almost-everywhere']"
16,Prove that the set $B_m$ is measurable,Prove that the set  is measurable,B_m,"Let $(X, \mathcal{A}, \mu)$ be a measure space and $\{A_n\}$ a family of measurable sets. For $m \in \mathbb{N}$ let $B_m$ be the set of points $x \in X$ that belong to at least $m$ of the sets $A_n$. Prove that $B_m$ is measurable and that $\mu (B_m) \leq \dfrac{1}{m} \sum_{n=1}^{\infty} \mu (A_n)$ I have been trying using the theorem that said that $\mu (\cup_{n=1}^\infty C_n)  \leq \sum_{n=1}^{\infty} \mu (C_n)$ for a family of measurable sets $\{C_n\}$ and I'm looking for the family that could help me, but I don't know how to use the factor $\dfrac{1}{m}$. I'm also trying to see the set $B_m$ as the intersection of other measurable sets that I know. I tried with induction over $m$ too.","Let $(X, \mathcal{A}, \mu)$ be a measure space and $\{A_n\}$ a family of measurable sets. For $m \in \mathbb{N}$ let $B_m$ be the set of points $x \in X$ that belong to at least $m$ of the sets $A_n$. Prove that $B_m$ is measurable and that $\mu (B_m) \leq \dfrac{1}{m} \sum_{n=1}^{\infty} \mu (A_n)$ I have been trying using the theorem that said that $\mu (\cup_{n=1}^\infty C_n)  \leq \sum_{n=1}^{\infty} \mu (C_n)$ for a family of measurable sets $\{C_n\}$ and I'm looking for the family that could help me, but I don't know how to use the factor $\dfrac{1}{m}$. I'm also trying to see the set $B_m$ as the intersection of other measurable sets that I know. I tried with induction over $m$ too.",,"['analysis', 'measure-theory']"
17,Show that for any $n\in\mathbb{N}$ we have $ |x-a_0.a_1a_2\cdots a_n|\leq \frac{1}{10^n}$,Show that for any  we have,n\in\mathbb{N}  |x-a_0.a_1a_2\cdots a_n|\leq \frac{1}{10^n},"Let $a_0\in \mathbb{N}\cup\{0\}$ and $\{a_1,a_2,\ldots\}\subset\{0,1,2,\ldots, 9\}$. Define the infinite decimal exansion $x=a_0.a_1a_2\cdots$. Show that for any $n\in\mathbb{N}$ we have $ |x-a_0.a_1a_2\cdots a_n|\leq \frac{1}{10^n}$. I was thinking of using the proof that $\sup(S+T)=\sup(S)+\sup(T)$ (I've done this in class already) and separating the infinite decimal sequence as $a_0 + a_1*(10^{-1}) + a_2*(10^{-2})+\cdots$ to prove this but I'm not exactly sure how.","Let $a_0\in \mathbb{N}\cup\{0\}$ and $\{a_1,a_2,\ldots\}\subset\{0,1,2,\ldots, 9\}$. Define the infinite decimal exansion $x=a_0.a_1a_2\cdots$. Show that for any $n\in\mathbb{N}$ we have $ |x-a_0.a_1a_2\cdots a_n|\leq \frac{1}{10^n}$. I was thinking of using the proof that $\sup(S+T)=\sup(S)+\sup(T)$ (I've done this in class already) and separating the infinite decimal sequence as $a_0 + a_1*(10^{-1}) + a_2*(10^{-2})+\cdots$ to prove this but I'm not exactly sure how.",,"['real-analysis', 'analysis']"
18,Baby Rudin - Theorem 1.35 Cauchy Schwartz,Baby Rudin - Theorem 1.35 Cauchy Schwartz,,"I'm stumped on the following difficulty while reading baby Rudin (p.15). Let $A=\sum|a_j|^2, B=\sum|b_j|^2, C=\sum a_j \overline{b}_j$: $$\begin{align} \sum|Ba_j-Cb_j|^2 &= \sum(Ba_j-Cb_j)(B\overline{a}_j-\overline{Cb_j}) \\ &= B^2\sum|a_j|^2{\bf -B\overline{C}\sum a_j\overline{b}_j-BC\sum \overline{a}_j b_j}+|C|^2\sum|b_j|^2 \\ &=B^2A-B|C|^2=B(AB-|C|^2) \end{align}$$ Note that the second line of the equation, the book indicates that the middle 2 term together to be zero. Why is that?","I'm stumped on the following difficulty while reading baby Rudin (p.15). Let $A=\sum|a_j|^2, B=\sum|b_j|^2, C=\sum a_j \overline{b}_j$: $$\begin{align} \sum|Ba_j-Cb_j|^2 &= \sum(Ba_j-Cb_j)(B\overline{a}_j-\overline{Cb_j}) \\ &= B^2\sum|a_j|^2{\bf -B\overline{C}\sum a_j\overline{b}_j-BC\sum \overline{a}_j b_j}+|C|^2\sum|b_j|^2 \\ &=B^2A-B|C|^2=B(AB-|C|^2) \end{align}$$ Note that the second line of the equation, the book indicates that the middle 2 term together to be zero. Why is that?",,"['real-analysis', 'analysis', 'inequality']"
19,Find all real and continuous functions that are a 3-involution.,Find all real and continuous functions that are a 3-involution.,,"Find all continuous functions $f: \mathbb R \rightarrow \mathbb R$such that $f(f(f(x)))=x$. Obviously one solution to this functional equation is $f(x)=x$. If the function is NOT continuous, there are also other solutions such as $f(x)=\frac{1}{1-x}$, but I'm not sure how to find all solutions that are continuous.","Find all continuous functions $f: \mathbb R \rightarrow \mathbb R$such that $f(f(f(x)))=x$. Obviously one solution to this functional equation is $f(x)=x$. If the function is NOT continuous, there are also other solutions such as $f(x)=\frac{1}{1-x}$, but I'm not sure how to find all solutions that are continuous.",,"['real-analysis', 'analysis', 'functions', 'continuity', 'functional-equations']"
20,Theorem 2.27 (a) in Baby Rudin: Is his proof complete enough?,Theorem 2.27 (a) in Baby Rudin: Is his proof complete enough?,,"Here's Theorem 2.27 (a) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $X$ is a metric space and $E \subset X$, then $\overline{E}$ is closed. Now here's Rudin's proof: If $p \in X$ and $p \not\in \overline{E}$ then $p$ is neither a point of $E$ nor a limit point of $E$. Hence $p$ has a neighborhood which does not intersect $E$. The complement of $\overline{E}$ is therefore open. Hence $\overline{E}$ is closed. Is the above proof good enough, especially at the level Rudin is intended for? Now here's the proof I propose: If $p \in X$ and $p \not\in \overline{E}$ then $p$ is neither a point of $E$ nor a limit point of $E$. Hence $p$ has a neighborhood which does not intersect $E$. Let $N_\epsilon (p)$ be this neighborhood. Now we show that no point of $N_\epsilon (p)$ can be in $\overline{E}$. Let $q \in N_\epsilon (p)$. Then $d(q,p) < \epsilon$, where $d$ denotes the metric on $X$. Let $\delta \colon= \epsilon - d(p,q)$. Then $0 < \delta < \epsilon$. Now if $a \in N_\delta (q)$, then $d(a,q) < \delta = \epsilon - d(q,p)$, which implies that $$d(a,p) \leq d(a,q) + d(q,p) < \epsilon,$$   and so $a \in N_\epsilon (p)$. Thus we have shown that $N_\delta (q) \subset N_\epsilon (p)$. Since    $ N_\epsilon (p) \cap E = \emptyset$, we have $N_\delta (q) \cap E = \emptyset$ as well. That is, the point  $q$ has a neighborhood --- namely  $N_\delta (q)$ --- which does not intersect $E$ at all. So $q \not\in \overline{E}$. But $q$ was an arbitrary point in $N_\epsilon (p)$. So $N_\epsilon (p) \subset \left( \overline{E} \right)^c$. But $p$ was an arbitrary point in $\left( \overline{E} \right)^c$. Thus, we can conclude that every point of $\left( \overline{E} \right)^c$ is an interior point. Hence $\left( \overline{E} \right)^c$ is open. Now is my proof any better than Rudin's? Are there any extra advantages to be had from inclusion or exclusion of extra details?","Here's Theorem 2.27 (a) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $X$ is a metric space and $E \subset X$, then $\overline{E}$ is closed. Now here's Rudin's proof: If $p \in X$ and $p \not\in \overline{E}$ then $p$ is neither a point of $E$ nor a limit point of $E$. Hence $p$ has a neighborhood which does not intersect $E$. The complement of $\overline{E}$ is therefore open. Hence $\overline{E}$ is closed. Is the above proof good enough, especially at the level Rudin is intended for? Now here's the proof I propose: If $p \in X$ and $p \not\in \overline{E}$ then $p$ is neither a point of $E$ nor a limit point of $E$. Hence $p$ has a neighborhood which does not intersect $E$. Let $N_\epsilon (p)$ be this neighborhood. Now we show that no point of $N_\epsilon (p)$ can be in $\overline{E}$. Let $q \in N_\epsilon (p)$. Then $d(q,p) < \epsilon$, where $d$ denotes the metric on $X$. Let $\delta \colon= \epsilon - d(p,q)$. Then $0 < \delta < \epsilon$. Now if $a \in N_\delta (q)$, then $d(a,q) < \delta = \epsilon - d(q,p)$, which implies that $$d(a,p) \leq d(a,q) + d(q,p) < \epsilon,$$   and so $a \in N_\epsilon (p)$. Thus we have shown that $N_\delta (q) \subset N_\epsilon (p)$. Since    $ N_\epsilon (p) \cap E = \emptyset$, we have $N_\delta (q) \cap E = \emptyset$ as well. That is, the point  $q$ has a neighborhood --- namely  $N_\delta (q)$ --- which does not intersect $E$ at all. So $q \not\in \overline{E}$. But $q$ was an arbitrary point in $N_\epsilon (p)$. So $N_\epsilon (p) \subset \left( \overline{E} \right)^c$. But $p$ was an arbitrary point in $\left( \overline{E} \right)^c$. Thus, we can conclude that every point of $\left( \overline{E} \right)^c$ is an interior point. Hence $\left( \overline{E} \right)^c$ is open. Now is my proof any better than Rudin's? Are there any extra advantages to be had from inclusion or exclusion of extra details?",,"['real-analysis', 'analysis', 'metric-spaces', 'proof-writing', 'education']"
21,Continuity of min function,Continuity of min function,,"I would like to ask a question about the continuity of this min function. Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be continuous. Define $$F(x):=\min\{f(t):t \in [-x,x]\}.$$ I believe that the function $F$ is continuous on $\mathbb{R}.$ However, I do not know how to use the $\delta$-$\epsilon$ definition to show its continuity. Could somebody help me? Thank you. Masih","I would like to ask a question about the continuity of this min function. Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be continuous. Define $$F(x):=\min\{f(t):t \in [-x,x]\}.$$ I believe that the function $F$ is continuous on $\mathbb{R}.$ However, I do not know how to use the $\delta$-$\epsilon$ definition to show its continuity. Could somebody help me? Thank you. Masih",,"['calculus', 'real-analysis', 'analysis']"
22,Prove: f is surjective -->$ f(f^{-1}(S)) = S$,Prove: f is surjective -->, f(f^{-1}(S)) = S,"I have to prove this exercise for my math-study: Let $f: X \rightarrow Y$ be a function and $S \subseteq Y$ Prove: $f$ is surjective $\Rightarrow$ $f(f^{-1}(S)) = S$ I divided this exercise in two parts, first proving that $S \subseteq f(f^{-1}(S))$. This is what I did: Assume $f$ is surjective $\Rightarrow$ $\forall s$  $\in S$ $\exists x \in$ $f^{-1}(S)$ such that $f(x) = s \Rightarrow s$ $\in$ $f(f^{-1}(S))$ $\Rightarrow$ $$S \subseteq f(f^{-1}(S))$$ Is this part right, or did I make any mistakes? For the second part, I have to prove that $f(f^{-1}(S)) \subseteq S$ I began with this: Assume $x$ $\in$ $f(f^{-1}(S))$. $f^{-1}(S)$ = {$x$ $\in$ X | $f(x)$ $\in$ S} But I don't know how to prove from that that $x \in S$. Could you please help me with these two questions? Thanks in advance!","I have to prove this exercise for my math-study: Let $f: X \rightarrow Y$ be a function and $S \subseteq Y$ Prove: $f$ is surjective $\Rightarrow$ $f(f^{-1}(S)) = S$ I divided this exercise in two parts, first proving that $S \subseteq f(f^{-1}(S))$. This is what I did: Assume $f$ is surjective $\Rightarrow$ $\forall s$  $\in S$ $\exists x \in$ $f^{-1}(S)$ such that $f(x) = s \Rightarrow s$ $\in$ $f(f^{-1}(S))$ $\Rightarrow$ $$S \subseteq f(f^{-1}(S))$$ Is this part right, or did I make any mistakes? For the second part, I have to prove that $f(f^{-1}(S)) \subseteq S$ I began with this: Assume $x$ $\in$ $f(f^{-1}(S))$. $f^{-1}(S)$ = {$x$ $\in$ X | $f(x)$ $\in$ S} But I don't know how to prove from that that $x \in S$. Could you please help me with these two questions? Thanks in advance!",,"['analysis', 'elementary-set-theory']"
23,Winding number integral/index in plane,Winding number integral/index in plane,,"Let $B(x_0,y_0)$ be an open unit disk.  Assume that $F(x,y)=(f_1 (x,y),f_2 (x,y)):\overline{B(x_0,y_0)}\rightarrow \mathbb{R}^2$ is a diffeomorphism. Assume also that $F(x,y) \ne (s_0,t_0) $, for all $(x,y)\in \partial B(x_0,y_0)$. What is the elementary way to evaluate the following result? $$\frac{1}{2\pi}\int_{\partial B(x_0,y_0)}\frac{(f_1-s_0) df_2-(f_2-t_0)df_1}{(f_1-s_0) ^2+(f_2-t_0) ^2}=sign \det F'(x_0,y_0)=±1. $$ Can that be shown by using Green's theorem and then change of variables formula? Since $F$ is a diffeomorphism, due to inverse function theorem $\det F'(x,y) \ne 0$, for all $(x,y) \in B(x_0,y_0)$, and by Jordan curve theorem $F(\partial B(x_0,y_0))$ is a Jordan curve. Therefore inner domain is simply connected and we can use Green's theorem. But I'm not sure how to calculate that integral.","Let $B(x_0,y_0)$ be an open unit disk.  Assume that $F(x,y)=(f_1 (x,y),f_2 (x,y)):\overline{B(x_0,y_0)}\rightarrow \mathbb{R}^2$ is a diffeomorphism. Assume also that $F(x,y) \ne (s_0,t_0) $, for all $(x,y)\in \partial B(x_0,y_0)$. What is the elementary way to evaluate the following result? $$\frac{1}{2\pi}\int_{\partial B(x_0,y_0)}\frac{(f_1-s_0) df_2-(f_2-t_0)df_1}{(f_1-s_0) ^2+(f_2-t_0) ^2}=sign \det F'(x_0,y_0)=±1. $$ Can that be shown by using Green's theorem and then change of variables formula? Since $F$ is a diffeomorphism, due to inverse function theorem $\det F'(x,y) \ne 0$, for all $(x,y) \in B(x_0,y_0)$, and by Jordan curve theorem $F(\partial B(x_0,y_0))$ is a Jordan curve. Therefore inner domain is simply connected and we can use Green's theorem. But I'm not sure how to calculate that integral.",,"['analysis', 'multivariable-calculus', 'differential-topology', 'vector-analysis', 'winding-number']"
24,"Terence Tao, Analysis 1. Exercise 5.3.2. Real Numbers and Cauchy Sequences.","Terence Tao, Analysis 1. Exercise 5.3.2. Real Numbers and Cauchy Sequences.",,"Let $ x = \lim_{n\rightarrow\infty}a_n, y = \lim_{n\rightarrow\infty}b_n$, and $ x' = \lim_{n\rightarrow\infty}a'_n$ be real numbers. Then $xy$ is also a real number. Furthermore, is $x=x'$, then $xy = x'y$. Here is my attempt. We need to show that $xy = \lim_{n\rightarrow\infty}a_n b_n$ is a real number. from the hypothesis we know that $a_n$ and $b_n$. are eventually $\delta$ -steady sequences for every $\delta> 0 $. so we can choose $a_n$ and $b_n$ to be eventually $\sqrt{\epsilon}$-steady sequences. since $(a_n)_{n=1}$ is eventually $\sqrt{\epsilon}$-steady, we know there is an $N \geq 1$ such that $d(a_n,a_m) < \sqrt{\epsilon}$  for every $n,m > N$. By a similar argument we can say the same for $(b_n)_{n=1}$. by proposition 4.3.7 (h). $d(a_nb_n,a_mb_m) < \sqrt{\epsilon}|b_n| + \sqrt{\epsilon}|a_n| + \epsilon $ From here i am not sure where to go, I am pretty sure i am going the right way, as this sort of resembles the definition of a cauchy sequence that is eventually $\sqrt{\epsilon}|b_n| + \sqrt{\epsilon}|a_n| + \epsilon $-close. I think I require something that just involves epsilon. The second part of the question I havent attempted yet, but I thought I'd include it just in case anyone would like to lend a helping hand. by the way propostion 4.3.7 (h). states that if x and y are $\epsilon$-close and z and w and $\delta$ close then xz and yw are $(\epsilon|z| + \delta|x| + \epsilon \delta$)-close. EDIT. A real number is defined to be an object of the form $ x = \lim_{n\rightarrow\infty}a_n$, where $a_n$ is a cauchy sequence of rational numbers.","Let $ x = \lim_{n\rightarrow\infty}a_n, y = \lim_{n\rightarrow\infty}b_n$, and $ x' = \lim_{n\rightarrow\infty}a'_n$ be real numbers. Then $xy$ is also a real number. Furthermore, is $x=x'$, then $xy = x'y$. Here is my attempt. We need to show that $xy = \lim_{n\rightarrow\infty}a_n b_n$ is a real number. from the hypothesis we know that $a_n$ and $b_n$. are eventually $\delta$ -steady sequences for every $\delta> 0 $. so we can choose $a_n$ and $b_n$ to be eventually $\sqrt{\epsilon}$-steady sequences. since $(a_n)_{n=1}$ is eventually $\sqrt{\epsilon}$-steady, we know there is an $N \geq 1$ such that $d(a_n,a_m) < \sqrt{\epsilon}$  for every $n,m > N$. By a similar argument we can say the same for $(b_n)_{n=1}$. by proposition 4.3.7 (h). $d(a_nb_n,a_mb_m) < \sqrt{\epsilon}|b_n| + \sqrt{\epsilon}|a_n| + \epsilon $ From here i am not sure where to go, I am pretty sure i am going the right way, as this sort of resembles the definition of a cauchy sequence that is eventually $\sqrt{\epsilon}|b_n| + \sqrt{\epsilon}|a_n| + \epsilon $-close. I think I require something that just involves epsilon. The second part of the question I havent attempted yet, but I thought I'd include it just in case anyone would like to lend a helping hand. by the way propostion 4.3.7 (h). states that if x and y are $\epsilon$-close and z and w and $\delta$ close then xz and yw are $(\epsilon|z| + \delta|x| + \epsilon \delta$)-close. EDIT. A real number is defined to be an object of the form $ x = \lim_{n\rightarrow\infty}a_n$, where $a_n$ is a cauchy sequence of rational numbers.",,[]
25,A problem on the sum of the reciprocals of two derivatives,A problem on the sum of the reciprocals of two derivatives,,"If $f(x)$ is continuous in the closed interval $[a,b]$ and differentiable in the open interval $a<x<b$, and if $f(a)=a$, $f(b)=b$, prove there exist points $x_1$ and $x_2$ with $a<x_1<x_2<b$ for which the following equation is true: $1/f'(x_1)+1/f'(x_2)=2$.","If $f(x)$ is continuous in the closed interval $[a,b]$ and differentiable in the open interval $a<x<b$, and if $f(a)=a$, $f(b)=b$, prove there exist points $x_1$ and $x_2$ with $a<x_1<x_2<b$ for which the following equation is true: $1/f'(x_1)+1/f'(x_2)=2$.",,['analysis']
26,Iteration of continuous function on compact interval,Iteration of continuous function on compact interval,,"Let $f:[0,1]\to [0,1]$ be continuous function. Moreover assume that for every $u\in [0,1]$ there exists $n(u)\in \mathbb{N}$ such that the nth iteration $f^{n(u)} (u) =0.$ Is this true that there exists $m\in\mathbb{N}$ such that for all $t\in [0,1]$ we have $f^m (t) =0$?","Let $f:[0,1]\to [0,1]$ be continuous function. Moreover assume that for every $u\in [0,1]$ there exists $n(u)\in \mathbb{N}$ such that the nth iteration $f^{n(u)} (u) =0.$ Is this true that there exists $m\in\mathbb{N}$ such that for all $t\in [0,1]$ we have $f^m (t) =0$?",,['real-analysis']
27,integration in five dimensions space,integration in five dimensions space,,"I am doing this problem: Consider the differential form $$a=p_1 \, dq_1+p_2 \, dq_2-(p_1^2+p_2^2+q_1^2+q_2^2) \, dt\text{ in }\mathbb R^5=(p_1,p_2,q_1,q_2,t).$$ (a) Compute the differential $da$ and the form $a\wedge da$. (b) Evaluate the integral $\int_S da\wedge da$ where $S$ is the 4-dim surface in $R^5$ defined by the equation $p_1^2+p_2^2+q_1^2+q_2^2=t$ and the inequality $1\le t\le 2$. Here is part of my solution: (a) $da=dp_1\wedge dq_1+dp_2\wedge dq_2-2p_1dp_1\wedge dt-2p_2dp_2\wedge dt-2q_1dq_1\wedge dt-2q_2dq_2\wedge dt$ $a\wedge da=p_1dq_1\wedge(dp_2\wedge dq_2-2p_1dp_1\wedge dt-2p_2dp_2\wedge dt-2q_2dq_2\wedge dt)$ ${}+p_2dp_2\wedge(dp_1\wedge dq_1-2p_1dp_1\wedge dt-2p_2dp_2\wedge dt-2q_1dq_1\wedge dt)$  ${}-(p_1^2+p_2^2+q_1^2+q_2^2)dt\wedge(dp_1\wedge dq_1+dp_2\wedge dq_2)$ I stopped here, since I think this expression is too long. I cannot imagine this problem is so complicated. So my first question is that is there any quick way to calculate $a\wedge da$? Or the only way is to continue the above calculation, and leave so many terms there(I guess only two pairs can be collected)? (b) I realized that the integrand $$da\wedge da=d(a\wedge da)$$, so I think this is a good indicator for us to use the Stokes's theorem $$\int_Sda\wedge da=\int_{\partial S}a\wedge da$$. And $\partial S$ is just $p_1^2+p_2^2+q_1^2+q_2^2=1$ and $p_1^2+p_2^2+q_1^2+q_2^2=2$, which I guess is what the people who made this problem want us to do. So my second question is how do we calculate $\int_{\partial S}a\wedge da$. By this I mean: Do I really need to calculate the integral one by one(I guess there are at least eight terms in the expression of $a\wedge da$ after collecting the terms)? Any orientation issues I need to be careful about the two ""balls"" $p_1^2+p_2^2+q_1^2+q_2^2=1$ and $p_1^2+p_2^2+q_1^2+q_2^2=2$? I think I need to use the parametric form to calculate the integral, right? By the way, do I really need to use the polar coordinate, which is four layers of $sin, cos$? or there is some other way to calculate the final integral? Thank you very much!","I am doing this problem: Consider the differential form $$a=p_1 \, dq_1+p_2 \, dq_2-(p_1^2+p_2^2+q_1^2+q_2^2) \, dt\text{ in }\mathbb R^5=(p_1,p_2,q_1,q_2,t).$$ (a) Compute the differential $da$ and the form $a\wedge da$. (b) Evaluate the integral $\int_S da\wedge da$ where $S$ is the 4-dim surface in $R^5$ defined by the equation $p_1^2+p_2^2+q_1^2+q_2^2=t$ and the inequality $1\le t\le 2$. Here is part of my solution: (a) $da=dp_1\wedge dq_1+dp_2\wedge dq_2-2p_1dp_1\wedge dt-2p_2dp_2\wedge dt-2q_1dq_1\wedge dt-2q_2dq_2\wedge dt$ $a\wedge da=p_1dq_1\wedge(dp_2\wedge dq_2-2p_1dp_1\wedge dt-2p_2dp_2\wedge dt-2q_2dq_2\wedge dt)$ ${}+p_2dp_2\wedge(dp_1\wedge dq_1-2p_1dp_1\wedge dt-2p_2dp_2\wedge dt-2q_1dq_1\wedge dt)$  ${}-(p_1^2+p_2^2+q_1^2+q_2^2)dt\wedge(dp_1\wedge dq_1+dp_2\wedge dq_2)$ I stopped here, since I think this expression is too long. I cannot imagine this problem is so complicated. So my first question is that is there any quick way to calculate $a\wedge da$? Or the only way is to continue the above calculation, and leave so many terms there(I guess only two pairs can be collected)? (b) I realized that the integrand $$da\wedge da=d(a\wedge da)$$, so I think this is a good indicator for us to use the Stokes's theorem $$\int_Sda\wedge da=\int_{\partial S}a\wedge da$$. And $\partial S$ is just $p_1^2+p_2^2+q_1^2+q_2^2=1$ and $p_1^2+p_2^2+q_1^2+q_2^2=2$, which I guess is what the people who made this problem want us to do. So my second question is how do we calculate $\int_{\partial S}a\wedge da$. By this I mean: Do I really need to calculate the integral one by one(I guess there are at least eight terms in the expression of $a\wedge da$ after collecting the terms)? Any orientation issues I need to be careful about the two ""balls"" $p_1^2+p_2^2+q_1^2+q_2^2=1$ and $p_1^2+p_2^2+q_1^2+q_2^2=2$? I think I need to use the parametric form to calculate the integral, right? By the way, do I really need to use the polar coordinate, which is four layers of $sin, cos$? or there is some other way to calculate the final integral? Thank you very much!",,"['analysis', 'differential-geometry']"
28,Show a non-empty open and closed set in R must be equal to R [duplicate],Show a non-empty open and closed set in R must be equal to R [duplicate],,"This question already has answers here : If a nonempty set of real numbers is open and closed, is it $\mathbb{R}$? Why/Why not? (10 answers) Closed 8 years ago . I did this in class, and got no credit. We are now supposed to find a proof that works, can anyone help me with this? Thanks!","This question already has answers here : If a nonempty set of real numbers is open and closed, is it $\mathbb{R}$? Why/Why not? (10 answers) Closed 8 years ago . I did this in class, and got no credit. We are now supposed to find a proof that works, can anyone help me with this? Thanks!",,['analysis']
29,Relation between convex set and convex function,Relation between convex set and convex function,,"Let $E$ be an normed vector space and $A\subset E$ be a closed nonempty set. Define $$\phi(x)=\operatorname{dist}(x,A)=\inf_{a\in A}\|x-a\|$$ Prove that if $\phi$ is convex then $A$ is convex. Definition of convex set: We will say $A$ is convex if for e very $x,y\in A$ and $\lambda \in [0,1]$ we have $\lambda x+(1-\lambda)y\in A$ Definition of convex function: We will say that $\phi$ is convex if for every $x,y\in A$ and $\lambda\in[0,1]$ we have $\phi(\lambda x+(1-\lambda)y)\leq\lambda\phi(x)+(1-\lambda)\phi(y)$.","Let $E$ be an normed vector space and $A\subset E$ be a closed nonempty set. Define $$\phi(x)=\operatorname{dist}(x,A)=\inf_{a\in A}\|x-a\|$$ Prove that if $\phi$ is convex then $A$ is convex. Definition of convex set: We will say $A$ is convex if for e very $x,y\in A$ and $\lambda \in [0,1]$ we have $\lambda x+(1-\lambda)y\in A$ Definition of convex function: We will say that $\phi$ is convex if for every $x,y\in A$ and $\lambda\in[0,1]$ we have $\phi(\lambda x+(1-\lambda)y)\leq\lambda\phi(x)+(1-\lambda)\phi(y)$.",,"['real-analysis', 'analysis', 'convex-analysis']"
30,Properties of $||f||_{\infty}$ - the infinity norm,Properties of  - the infinity norm,||f||_{\infty},"Prove that $||f||_{\infty}$ is the smallest of all numbers of the form $\sup\{|g(x)|: x\in X\}$, where $f=g$ ($\mu$ almost everywhere). In addition, if $f$ is a continuous function on the measure space $\mathbb{R}^n,\mathcal{L},\lambda$ (i.e., Lebesgue measure), prove that $||f||_{\infty}=\sup\{|f(x)|:x\in\mathbb{R}^n\}$ Here's the book's definition: $L^{\infty}(X,\mathcal{M},\mu)$ is the collection of all essentially bounded $\mathcal{M}$-measurable functions on $X$. Moreover, the norm of a function $f\in L^{\infty}$ is the number $$||f||_{\infty}=\inf\{M:|f(x)|\le M\quad \text{for } \mu \text{-almost everywhere  } x\in X\}$$ I think the statement in the second paragraph (i.e., assuming continuity of $f$ on the measure space) is a consequence of the first paragraph. If $f=g$ almost everywhere, and $f$ is continuous, does it follow that $f=g$ identically?","Prove that $||f||_{\infty}$ is the smallest of all numbers of the form $\sup\{|g(x)|: x\in X\}$, where $f=g$ ($\mu$ almost everywhere). In addition, if $f$ is a continuous function on the measure space $\mathbb{R}^n,\mathcal{L},\lambda$ (i.e., Lebesgue measure), prove that $||f||_{\infty}=\sup\{|f(x)|:x\in\mathbb{R}^n\}$ Here's the book's definition: $L^{\infty}(X,\mathcal{M},\mu)$ is the collection of all essentially bounded $\mathcal{M}$-measurable functions on $X$. Moreover, the norm of a function $f\in L^{\infty}$ is the number $$||f||_{\infty}=\inf\{M:|f(x)|\le M\quad \text{for } \mu \text{-almost everywhere  } x\in X\}$$ I think the statement in the second paragraph (i.e., assuming continuity of $f$ on the measure space) is a consequence of the first paragraph. If $f=g$ almost everywhere, and $f$ is continuous, does it follow that $f=g$ identically?",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lp-spaces']"
31,Clarification on a proof involving cluster point,Clarification on a proof involving cluster point,,"Definition of cluster point- Let $A \subseteq \mathbb{R}$. A point $c\in\mathbb{R}$ is a cluster point of $A$ if for evert $\delta>0$ there exists at least one point $x\in A$, $x\neq c$ such that $|x-c|<\delta$. Theorem- A number $c\in\mathbb{R}$ is a cluster point of a subset $A$ of $\mathbb{R}$ iff there exists a sequence $(a_n)$ in A such that lim$(a_n)=c$ and $a_n \neq c$ for all $n\in\mathbb{N}$. I'm stuck on understanding the foward direction. Proof: $(\rightarrow)$""If $c$ is a cluster point of $A$ then for any $n\in\mathbb{N}$ the $\frac{1}{n}$-neighborhood $V_{\frac{1}{n}}(c)$ contains atleast one point $a_n\in A$ distinct from $c$. Then  $a_n\in A$, $a_n \neq c$, and $|a_n-c|<\frac{1}{n}$ implies  lim$(a_n)=c$."" What I understand is that if we let $\delta>0$ then it follows by the Archimdean property that there exists a $k\in\mathbb{N}$ such that $\frac{1}{k}<\delta$. It follows if $n\geq k$ then $\frac{1}{n}<\frac{1}{k}<\delta$. Since $c$ is a cluster point there exists a $x_{n}\in A$ where $x_{n}\neq c$ such that $|x_{n}-c|<\frac{1}{n}<\delta$ since $\frac{1}{n}>0$. Thus by the definition of convergence we have $lim (a_n)=c$. Would this be a correct way of understanding the proof to this theorem?","Definition of cluster point- Let $A \subseteq \mathbb{R}$. A point $c\in\mathbb{R}$ is a cluster point of $A$ if for evert $\delta>0$ there exists at least one point $x\in A$, $x\neq c$ such that $|x-c|<\delta$. Theorem- A number $c\in\mathbb{R}$ is a cluster point of a subset $A$ of $\mathbb{R}$ iff there exists a sequence $(a_n)$ in A such that lim$(a_n)=c$ and $a_n \neq c$ for all $n\in\mathbb{N}$. I'm stuck on understanding the foward direction. Proof: $(\rightarrow)$""If $c$ is a cluster point of $A$ then for any $n\in\mathbb{N}$ the $\frac{1}{n}$-neighborhood $V_{\frac{1}{n}}(c)$ contains atleast one point $a_n\in A$ distinct from $c$. Then  $a_n\in A$, $a_n \neq c$, and $|a_n-c|<\frac{1}{n}$ implies  lim$(a_n)=c$."" What I understand is that if we let $\delta>0$ then it follows by the Archimdean property that there exists a $k\in\mathbb{N}$ such that $\frac{1}{k}<\delta$. It follows if $n\geq k$ then $\frac{1}{n}<\frac{1}{k}<\delta$. Since $c$ is a cluster point there exists a $x_{n}\in A$ where $x_{n}\neq c$ such that $|x_{n}-c|<\frac{1}{n}<\delta$ since $\frac{1}{n}>0$. Thus by the definition of convergence we have $lim (a_n)=c$. Would this be a correct way of understanding the proof to this theorem?",,"['real-analysis', 'analysis']"
32,Scalar potential and vector potential,Scalar potential and vector potential,,"Let $D$ be a simply connected open subset of $\mathbb{R}^3$. It is well-known that if ${\bf v}(x,y,z)=[P(x,y,z), Q(x,y,z), R(x,y,z)]$ is a $C^1$ function on $D$ such that $$\mbox{curl }\bf{v}=\bf{0}$$ then there exists $C^2$ function $f(x,y,z)$ on $D$ such that $${\bf v}=\mbox{grad }f$$ on $D$. My question is: Can we use this fact to prove that if $\bf{w}$ is a vector field with $\mbox{div }\bf{w}=0$ on $D$, then there exists a vector field $\bf{t}$ on $D$ such that $\mbox{curl }\bf{t}=\bf{w}$? I can do this for 2-d case.","Let $D$ be a simply connected open subset of $\mathbb{R}^3$. It is well-known that if ${\bf v}(x,y,z)=[P(x,y,z), Q(x,y,z), R(x,y,z)]$ is a $C^1$ function on $D$ such that $$\mbox{curl }\bf{v}=\bf{0}$$ then there exists $C^2$ function $f(x,y,z)$ on $D$ such that $${\bf v}=\mbox{grad }f$$ on $D$. My question is: Can we use this fact to prove that if $\bf{w}$ is a vector field with $\mbox{div }\bf{w}=0$ on $D$, then there exists a vector field $\bf{t}$ on $D$ such that $\mbox{curl }\bf{t}=\bf{w}$? I can do this for 2-d case.",,"['analysis', 'multivariable-calculus']"
33,Mean value theorem application for multivariable functions,Mean value theorem application for multivariable functions,,"Define the function $f\colon \Bbb R^3\to \Bbb R$ by $$f(x,y,z)=xyz+x^2+y^2$$   The Mean Value Theorem implies that there is a number $\theta$ with $0<\theta <1$ for which   $$f(1,1,1)-f(0,0,0)=\frac{\partial f}{\partial x}(\theta, \theta, \theta)+\frac{\partial f}{\partial y}(\theta, \theta, \theta)+\frac{\partial f}{\partial z}(\theta, \theta, \theta)$$ This is the last question. I don't have any idea. Sorry for not writing any idea. How can we show MVT for this question?","Define the function $f\colon \Bbb R^3\to \Bbb R$ by $$f(x,y,z)=xyz+x^2+y^2$$   The Mean Value Theorem implies that there is a number $\theta$ with $0<\theta <1$ for which   $$f(1,1,1)-f(0,0,0)=\frac{\partial f}{\partial x}(\theta, \theta, \theta)+\frac{\partial f}{\partial y}(\theta, \theta, \theta)+\frac{\partial f}{\partial z}(\theta, \theta, \theta)$$ This is the last question. I don't have any idea. Sorry for not writing any idea. How can we show MVT for this question?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
34,Is there a version of mean value property for nonharmonic funcions?,Is there a version of mean value property for nonharmonic funcions?,,"We know by mean value property that harmonic functions satisfies the equalities \begin{equation} u(x) = \dfrac{1}{|B_r|}\int_{B_r}f dx = \dfrac{1}{|\partial B_r|}\int_{ \partial B_r}udS. \end{equation} I'm wondering if we do not have harmonic functions, instead we have $\Delta u =f$. Is there some equality like above? For example adding some term depending on $f$ or its intergral on the right hand side?","We know by mean value property that harmonic functions satisfies the equalities \begin{equation} u(x) = \dfrac{1}{|B_r|}\int_{B_r}f dx = \dfrac{1}{|\partial B_r|}\int_{ \partial B_r}udS. \end{equation} I'm wondering if we do not have harmonic functions, instead we have $\Delta u =f$. Is there some equality like above? For example adding some term depending on $f$ or its intergral on the right hand side?",,"['analysis', 'partial-differential-equations']"
35,Proof of weak maximum principle.,Proof of weak maximum principle.,,"I read a proof in my book on pde which I find a bit strange. Let $\Omega$ be some bounded domain. For $f\in\mathscr C^2(\Omega)\cap\mathscr C^0(\Omega)$ satisfying $\Delta f\geq0$ it holds that $\max_\bar\Omega(f)=\max_{\partial\Omega}(f)$. The proof follows by showing that it holds for $f$ with $\Delta f>0$ based on the second derivative test, and then generalizes it by considering the function $v(x)=f(x)+\epsilon|x|^2$ for $\epsilon$>0. Now since the norm function is strictly subharmonic, we have $\Delta v>0$. The author proceeds as follows: $$\max_{\bar\Omega} (v)=\max_{\partial\Omega}(v) \Longrightarrow \max_\bar\Omega (f)+\epsilon\min_\bar\Omega(|x|^2)\leq \max_{\partial\Omega}(f)+\epsilon\max_{\partial\Omega}(|x|^2) \Longrightarrow \max_\bar\Omega(f)=\max_{\partial\Omega}(f)$$ since epsilon was arbitrary. My question is, why would it not work to simply let epsilon be 1 in the beginning and reason as follows. $$\max_\bar\Omega(f)+\max_\bar\Omega(|x|^2)=\max_{\bar\Omega} (v)=\max_{\partial\Omega}(v)=\max_{\partial\Omega}(f)+\max_{\partial\Omega}(|x|^2)$$ which implies the result since $\max_{\bar\Omega}(|x|^2)=\max_{\partial\Omega}(|x|^2)$ since $|x|$ is strictly subharmonic. Wouldn't this be easier? What ridiculous mistake am i making? Thanks","I read a proof in my book on pde which I find a bit strange. Let $\Omega$ be some bounded domain. For $f\in\mathscr C^2(\Omega)\cap\mathscr C^0(\Omega)$ satisfying $\Delta f\geq0$ it holds that $\max_\bar\Omega(f)=\max_{\partial\Omega}(f)$. The proof follows by showing that it holds for $f$ with $\Delta f>0$ based on the second derivative test, and then generalizes it by considering the function $v(x)=f(x)+\epsilon|x|^2$ for $\epsilon$>0. Now since the norm function is strictly subharmonic, we have $\Delta v>0$. The author proceeds as follows: $$\max_{\bar\Omega} (v)=\max_{\partial\Omega}(v) \Longrightarrow \max_\bar\Omega (f)+\epsilon\min_\bar\Omega(|x|^2)\leq \max_{\partial\Omega}(f)+\epsilon\max_{\partial\Omega}(|x|^2) \Longrightarrow \max_\bar\Omega(f)=\max_{\partial\Omega}(f)$$ since epsilon was arbitrary. My question is, why would it not work to simply let epsilon be 1 in the beginning and reason as follows. $$\max_\bar\Omega(f)+\max_\bar\Omega(|x|^2)=\max_{\bar\Omega} (v)=\max_{\partial\Omega}(v)=\max_{\partial\Omega}(f)+\max_{\partial\Omega}(|x|^2)$$ which implies the result since $\max_{\bar\Omega}(|x|^2)=\max_{\partial\Omega}(|x|^2)$ since $|x|$ is strictly subharmonic. Wouldn't this be easier? What ridiculous mistake am i making? Thanks",,"['real-analysis', 'analysis', 'partial-differential-equations', 'harmonic-analysis', 'harmonic-functions']"
36,Chain rule proof,Chain rule proof,,"Let $a \in E \subset R^n, E \mbox{ open}, f: E \to R^m, f(E) \subset U  \subset R^m, U \mbox{ open}, g: U \to R^l, F:= g \circ f.$ If $f$ is    differentiable in $a$ and $g$ differentiable in $f(a)$, then $F$ is    differentiable in $a$ and $F'(a)=g'(f(a)) f'(a)$. I have been given a short proof, but I do not understand every step of it. We have $f(a+h) = f(a) + f'(a)h + o(|h|)$ and $g(f(a)+k) = g(f(a)) + g'(f(a))k + o(|k|)$ as $h,k \to 0$. Thus $\begin{align} F(a+h)=(g \circ f)(a+h) &= g(f(a+h)) \\  &= g(f(a)+f'(a)h+o(|h|)) \\ &= g(f(a)) + g'(f(a))(f'(a)h+o(|h|)) + o(|f'(a)h+o(|h|)|) \\ &= (g\circ f)(a) + g'(f(a))f'(a)h+o(|h|) + o(O(|h|)) \\ &= F(a) + g'(f(a))f'(a)h+o(|h|) + o(|h|). \end{align}$ That's all. I feel like there is some trickery involved with the little o en big o. My problems are: Why does $f'(a)h+o(|h|) \to 0$ when $h\to 0$. I see that the first term tends to $0$, but how come $o(|h|) \to 0$? $g'(f(a))o(|h|))=o(|h|)$. $o(|f'(a)h + o(|h|)|)=o(O(|h|))$ $o(|h|) + o(|h|) = o(|h|)$. This last one isn't included in the proof, but I expect that's would have been something trivial. I've been trying to work this out with the definitions of big o en little o, but to no prevail. It's confusion to me how I can do computations with big o and little o, since you can't really perform algebraic operations with them and it's more like a function property. If anyone could show me how this would be done I'd be so thankful.","Let $a \in E \subset R^n, E \mbox{ open}, f: E \to R^m, f(E) \subset U  \subset R^m, U \mbox{ open}, g: U \to R^l, F:= g \circ f.$ If $f$ is    differentiable in $a$ and $g$ differentiable in $f(a)$, then $F$ is    differentiable in $a$ and $F'(a)=g'(f(a)) f'(a)$. I have been given a short proof, but I do not understand every step of it. We have $f(a+h) = f(a) + f'(a)h + o(|h|)$ and $g(f(a)+k) = g(f(a)) + g'(f(a))k + o(|k|)$ as $h,k \to 0$. Thus $\begin{align} F(a+h)=(g \circ f)(a+h) &= g(f(a+h)) \\  &= g(f(a)+f'(a)h+o(|h|)) \\ &= g(f(a)) + g'(f(a))(f'(a)h+o(|h|)) + o(|f'(a)h+o(|h|)|) \\ &= (g\circ f)(a) + g'(f(a))f'(a)h+o(|h|) + o(O(|h|)) \\ &= F(a) + g'(f(a))f'(a)h+o(|h|) + o(|h|). \end{align}$ That's all. I feel like there is some trickery involved with the little o en big o. My problems are: Why does $f'(a)h+o(|h|) \to 0$ when $h\to 0$. I see that the first term tends to $0$, but how come $o(|h|) \to 0$? $g'(f(a))o(|h|))=o(|h|)$. $o(|f'(a)h + o(|h|)|)=o(O(|h|))$ $o(|h|) + o(|h|) = o(|h|)$. This last one isn't included in the proof, but I expect that's would have been something trivial. I've been trying to work this out with the definitions of big o en little o, but to no prevail. It's confusion to me how I can do computations with big o and little o, since you can't really perform algebraic operations with them and it's more like a function property. If anyone could show me how this would be done I'd be so thankful.",,"['analysis', 'asymptotics']"
37,Showing a function is not one-to-one near the origin,Showing a function is not one-to-one near the origin,,"Let $$f(x)=\begin{cases} x+2x^2\sin\left(\frac{1}{x}\right) \text{ if } x \neq 0 \\ 0 \text{ if } x=0 \end{cases}$$ I'm trying to show this is not one-to-one near $0$.  I was given a hint to consider three sequences: $x_n=\frac{2}{(4n-1)\pi}$, $y_n=\frac{2}{(4n+1)\pi}$, and $z_n=\frac{2}{(4n-3)\pi}$ and evaluate them using $f$.  I'm able to evaluate them, i.e., just plug them in.  I'm not sure what to do after I have my $f(x_n), f(y_n), \text{and }  f(z_n)$. The functions evaluated at each sequence: $f(x_n)=\frac{2}{(4n-1)\pi}+2\left(\frac{2}{(4n-1)\pi}\right)^2\sin\left(\frac{(4n-1)\pi}{2}\right)$ $f(y_n)=\frac{2}{(4n+1)\pi}+2\left(\frac{2}{(4n+1)\pi}\right)^2\sin\left(\frac{(4n+1)\pi}{2}\right)$ $f(z_n)=\frac{2}{(4n-3)\pi}+2\left(\frac{2}{(4n-3)\pi}\right)^2\sin\left(\frac{(4n-3)\pi}{2}\right)$","Let $$f(x)=\begin{cases} x+2x^2\sin\left(\frac{1}{x}\right) \text{ if } x \neq 0 \\ 0 \text{ if } x=0 \end{cases}$$ I'm trying to show this is not one-to-one near $0$.  I was given a hint to consider three sequences: $x_n=\frac{2}{(4n-1)\pi}$, $y_n=\frac{2}{(4n+1)\pi}$, and $z_n=\frac{2}{(4n-3)\pi}$ and evaluate them using $f$.  I'm able to evaluate them, i.e., just plug them in.  I'm not sure what to do after I have my $f(x_n), f(y_n), \text{and }  f(z_n)$. The functions evaluated at each sequence: $f(x_n)=\frac{2}{(4n-1)\pi}+2\left(\frac{2}{(4n-1)\pi}\right)^2\sin\left(\frac{(4n-1)\pi}{2}\right)$ $f(y_n)=\frac{2}{(4n+1)\pi}+2\left(\frac{2}{(4n+1)\pi}\right)^2\sin\left(\frac{(4n+1)\pi}{2}\right)$ $f(z_n)=\frac{2}{(4n-3)\pi}+2\left(\frac{2}{(4n-3)\pi}\right)^2\sin\left(\frac{(4n-3)\pi}{2}\right)$",,"['calculus', 'analysis']"
38,A sequel for Elementary Analysis by Ross?,A sequel for Elementary Analysis by Ross?,,"I've been learning real analysis from this book: Elementary Analysis, K.A. Ross I really liked the style of this book. It is quite old, and sometimes very difficult, but I guess I liked the way it lays out analysis very precise and rigorously. Although, I'm a first-year math student and I don't have much to compare it with. We just finished this book at my university, and now we are studying: ""Analysis from $ℝ$ to $ℝ^n$ : multivariable functions"" as it is called. The ""book"" we use is written by the university, and to be fair, I just don't understand it. I think that if I had attended all the lectures then it would have worked out otherwise. But now I'm quite sure that some definitions/theorems/tricks to solve the questions are just missing in the text. I get completely frustrated by this. Would anybody know a good sequel to Elementary Analysis by Ross? These are the names of the chapters of the book we use: 1. Normed vector spaces and limits 1.1 Normed vector spaces 1.2 Limits of functions and continuity 1.3 Operator norm 2. Differentiation of functions from $ℝ^n$ to $ℝ^m$ 3. The chain rule 4. Geometric interpretation of derivatives 5. Mean value theorem: Higher partial derivatives 6. Taylor series in use 7. Extremes 8. Higher total derivatives: Taylor series","I've been learning real analysis from this book: Elementary Analysis, K.A. Ross I really liked the style of this book. It is quite old, and sometimes very difficult, but I guess I liked the way it lays out analysis very precise and rigorously. Although, I'm a first-year math student and I don't have much to compare it with. We just finished this book at my university, and now we are studying: ""Analysis from to : multivariable functions"" as it is called. The ""book"" we use is written by the university, and to be fair, I just don't understand it. I think that if I had attended all the lectures then it would have worked out otherwise. But now I'm quite sure that some definitions/theorems/tricks to solve the questions are just missing in the text. I get completely frustrated by this. Would anybody know a good sequel to Elementary Analysis by Ross? These are the names of the chapters of the book we use: 1. Normed vector spaces and limits 1.1 Normed vector spaces 1.2 Limits of functions and continuity 1.3 Operator norm 2. Differentiation of functions from to 3. The chain rule 4. Geometric interpretation of derivatives 5. Mean value theorem: Higher partial derivatives 6. Taylor series in use 7. Extremes 8. Higher total derivatives: Taylor series",ℝ ℝ^n ℝ^n ℝ^m,"['real-analysis', 'analysis', 'reference-request', 'multivariable-calculus']"
39,What are norms used for?,What are norms used for?,,"These two questions are quite similar to this one , so I apologise if this irritates anyone. Also, I suspect that a lot can be said in the answer, so I am really just looking for some main points (maybe a reference?). If we have a vector space $V$ with a norm $||\cdot||$ then it implies that $(V,d)$ is a metric space with metric $d(x,y)=||x-y||$. In turn, this gives access to huge parts of analysis (by using the metric to define open/closed sets, limits, continuity, etc.). But apart from inducing metrics what else are norms good for? If applicable - in how many of these uses could $d(x,0)$, where $0$ denotes additive identity of $V$, replace $||\cdot||$? If $||\cdot||$ cannot be replaced with $d(x,0)$, why? Is it maybe that homogeniety of $||\cdot||$ is particularly important? Thanks.","These two questions are quite similar to this one , so I apologise if this irritates anyone. Also, I suspect that a lot can be said in the answer, so I am really just looking for some main points (maybe a reference?). If we have a vector space $V$ with a norm $||\cdot||$ then it implies that $(V,d)$ is a metric space with metric $d(x,y)=||x-y||$. In turn, this gives access to huge parts of analysis (by using the metric to define open/closed sets, limits, continuity, etc.). But apart from inducing metrics what else are norms good for? If applicable - in how many of these uses could $d(x,0)$, where $0$ denotes additive identity of $V$, replace $||\cdot||$? If $||\cdot||$ cannot be replaced with $d(x,0)$, why? Is it maybe that homogeniety of $||\cdot||$ is particularly important? Thanks.",,"['analysis', 'metric-spaces', 'normed-spaces']"
40,d'alembert's formula,d'alembert's formula,,"I'm studying the Cauchy problem for the wave equation $n=2$; $$\begin{cases}u_{tt}=\alpha^{2} u_{xx}, x \in\mathbb{R}, t>0\\[8pt] u(x,0)=f(x), x\in\mathbb{R}\\[8pt] u_{t}(x,0)=g(x), x\in\mathbb{R} \end{cases}$$ By d´Alembert's formula we know that $u(x,t)=\frac{f(x+\alpha t)+f(x-\alpha t)}{2} +\frac{1}{2\alpha}\int^{x+\alpha t}_{x-\alpha t}g(s)ds$ is only solution of the above problem. But in the proof, the uniqueness of $u$, second my book reference, is given by the uniqueness of the functions $f(x)\in C^{2}(\mathbb{R})$ and $g(x)\in C^{1}(\mathbb{R}).$ I do not understand this statement!  Can anyone help me? thank you very much.","I'm studying the Cauchy problem for the wave equation $n=2$; $$\begin{cases}u_{tt}=\alpha^{2} u_{xx}, x \in\mathbb{R}, t>0\\[8pt] u(x,0)=f(x), x\in\mathbb{R}\\[8pt] u_{t}(x,0)=g(x), x\in\mathbb{R} \end{cases}$$ By d´Alembert's formula we know that $u(x,t)=\frac{f(x+\alpha t)+f(x-\alpha t)}{2} +\frac{1}{2\alpha}\int^{x+\alpha t}_{x-\alpha t}g(s)ds$ is only solution of the above problem. But in the proof, the uniqueness of $u$, second my book reference, is given by the uniqueness of the functions $f(x)\in C^{2}(\mathbb{R})$ and $g(x)\in C^{1}(\mathbb{R}).$ I do not understand this statement!  Can anyone help me? thank you very much.",,"['analysis', 'partial-differential-equations']"
41,"Taylor's Theorem Application Question, $f(x)$ smooth and $f(0)=0$ implies $f(x)/x$ smooth.","Taylor's Theorem Application Question,  smooth and  implies  smooth.",f(x) f(0)=0 f(x)/x,"I am wondering the following fact, and I believe I know the answer, but I am not sure why. If $f(x)$ is a smooth function from $\mathbb{R}$ to $\mathbb{R}$, if $f(0)=0$, is it true that $f(x)/x$ is smooth? I believe this is an application of Taylor's theorem (a text I was reading used Taylor's theorem at some part of a proof, and I believe it to be this part), however, I do not see why. Any help is much appreciated.","I am wondering the following fact, and I believe I know the answer, but I am not sure why. If $f(x)$ is a smooth function from $\mathbb{R}$ to $\mathbb{R}$, if $f(0)=0$, is it true that $f(x)/x$ is smooth? I believe this is an application of Taylor's theorem (a text I was reading used Taylor's theorem at some part of a proof, and I believe it to be this part), however, I do not see why. Any help is much appreciated.",,"['calculus', 'real-analysis', 'analysis', 'taylor-expansion']"
42,Explicit bijection between Jordan curves and real numbers,Explicit bijection between Jordan curves and real numbers,,"It is my understanding that the set of all Jordan curves and the set of real numbers are of the same cardinality.  So, it follows that there should exist a bijection between them.  Is there a known, explicit bijection between these two sets?  If so, what is it?","It is my understanding that the set of all Jordan curves and the set of real numbers are of the same cardinality.  So, it follows that there should exist a bijection between them.  Is there a known, explicit bijection between these two sets?  If so, what is it?",,"['analysis', 'elementary-set-theory']"
43,How to prove $ \sum_{n=1}^{\infty}\left|\frac{a_{1}+\cdots+a_{n}}{n}\right|^{p}\leq\left(\frac{p}{p-1}\right)^{p}\sum_{n=1}^{\infty}|a_{n}|^{p} $,How to prove, \sum_{n=1}^{\infty}\left|\frac{a_{1}+\cdots+a_{n}}{n}\right|^{p}\leq\left(\frac{p}{p-1}\right)^{p}\sum_{n=1}^{\infty}|a_{n}|^{p} ,"Let define $(a_n)_{n\geq1}$ as real series. Prove, that $$ \sum_{n=1}^{\infty}\left|\frac{a_{1}+\cdots+a_{n}}{n}\right|^{p}\leq\left(\frac{p}{p-1}\right)^{p}\sum_{n=1}^{\infty}|a_{n}|^{p} $$ (*) Extended level question - is the constant $\left(\frac{p}{p-1}\right)^p$   optimal? I've tried induction methods, getting the logarithm of each sides, but it seems to be not working...","Let define $(a_n)_{n\geq1}$ as real series. Prove, that $$ \sum_{n=1}^{\infty}\left|\frac{a_{1}+\cdots+a_{n}}{n}\right|^{p}\leq\left(\frac{p}{p-1}\right)^{p}\sum_{n=1}^{\infty}|a_{n}|^{p} $$ (*) Extended level question - is the constant $\left(\frac{p}{p-1}\right)^p$   optimal? I've tried induction methods, getting the logarithm of each sides, but it seems to be not working...",,"['real-analysis', 'analysis', 'inequality']"
44,smooth approximations of indicator function,smooth approximations of indicator function,,"How would I construct Schwartz functions $f_1^\epsilon$, $f_2^\epsilon$ on $\mathbb{R}$ such that $f_1^\epsilon(x)\leq\mathbb{1}_{[a,b]}(x)\leq f_2^\epsilon(x)$, and $f_1^\epsilon\rightarrow\mathbb{1}_{[a,b]}$, $f_2^\epsilon\rightarrow\mathbb{1}_{[a,b]}$ as $\epsilon\rightarrow0$, where $a<b$ are real numbers and $\mathbb{1}_{[a,b]}$ is the indicator function?","How would I construct Schwartz functions $f_1^\epsilon$, $f_2^\epsilon$ on $\mathbb{R}$ such that $f_1^\epsilon(x)\leq\mathbb{1}_{[a,b]}(x)\leq f_2^\epsilon(x)$, and $f_1^\epsilon\rightarrow\mathbb{1}_{[a,b]}$, $f_2^\epsilon\rightarrow\mathbb{1}_{[a,b]}$ as $\epsilon\rightarrow0$, where $a<b$ are real numbers and $\mathbb{1}_{[a,b]}$ is the indicator function?",,['analysis']
45,Verifying that there is a piecewise linear function on a closed set that approximates it well,Verifying that there is a piecewise linear function on a closed set that approximates it well,,"Question: Let $f$ be a continuous function on $[a,b]$.  Show that there is a piecewise linear function $\phi$ on $[a,b]$ with $|f(x)-\phi(x)| < \varepsilon$ for $x \in [a,b]$. My proof: For any $\varepsilon > 0$ there is a $\delta >0$ such that $|x-y|<\delta$ implies $|f(x)-f(y)|<\varepsilon$ for all $x,y \in [a,b]$.  Let $b=a+N\delta$ and partition the interval into subintervals as follows: $x_n=a+n\delta$ for $n=1,2,3,...,N$.  Now let $f(x_i)=\phi(x_i)$ and let $\phi(x)$ be piecewise linear between $(x_i,x_{x+1})$. Then for $x \in (x_i, x_{i+1})$: $$|f(x)-\phi(x)|=|f(x)-f(x_k)+\phi(x_k)-\phi(x)|\leq|f(x)-f(x_k)|+|\phi(x_k)+\phi(x)| < 2\epsilon,$$ since $$|x-x_k|<\delta.$$","Question: Let $f$ be a continuous function on $[a,b]$.  Show that there is a piecewise linear function $\phi$ on $[a,b]$ with $|f(x)-\phi(x)| < \varepsilon$ for $x \in [a,b]$. My proof: For any $\varepsilon > 0$ there is a $\delta >0$ such that $|x-y|<\delta$ implies $|f(x)-f(y)|<\varepsilon$ for all $x,y \in [a,b]$.  Let $b=a+N\delta$ and partition the interval into subintervals as follows: $x_n=a+n\delta$ for $n=1,2,3,...,N$.  Now let $f(x_i)=\phi(x_i)$ and let $\phi(x)$ be piecewise linear between $(x_i,x_{x+1})$. Then for $x \in (x_i, x_{i+1})$: $$|f(x)-\phi(x)|=|f(x)-f(x_k)+\phi(x_k)-\phi(x)|\leq|f(x)-f(x_k)|+|\phi(x_k)+\phi(x)| < 2\epsilon,$$ since $$|x-x_k|<\delta.$$",,['analysis']
46,"equivalence of two definitions of norm equivalence: ""$|\cdot|_1=|\cdot|_2^\alpha$"" vs. ""being a Cauchy sequence is the same for both norms""","equivalence of two definitions of norm equivalence: """" vs. ""being a Cauchy sequence is the same for both norms""",|\cdot|_1=|\cdot|_2^\alpha,"Let $|\cdot|_1$ and $|\cdot|_2$ be two norms on a field $\mathbb F$. We call the two norms equivalent if every Cauchy-sequence with respect to $|\cdot|_1$ is also a Cauchy-sequence with respect to $|\cdot|_2$. Prove the following statement: $$|\cdot|_1\sim|\cdot|_2\quad\Leftrightarrow\quad\exists \alpha\in\mathbb{R}_{>0}: \forall x\in\mathbb F: |x|_1=|x|_1^\alpha.$$ The direction ""$\Leftarrow$"" is straightforward: Let there an $\alpha$ with the property above, $(a_i)_{i\in\mathbb N}$ be a Cauchy sequence with respect to $|\cdot|_1$ and $\varepsilon_2\in\mathbb{R}_{>0}$ be arbitrary. Set $\varepsilon_1:=\varepsilon_2^\alpha$ and follow from the fact that $(a_i)_{i\in\mathbb N}$ is a Cauchy sequence, that there exists an $N\in\mathbb N$ such that $$\forall n,m>N: |a_m-a_n|_1<\varepsilon_1.$$ Since $|\cdot|_1\sim|\cdot|_2$ and the definition of $\varepsilon_1$ we get that $$\forall n,m>N: |a_m-a_n|_2^\alpha<\varepsilon_1^\alpha$$ which is equivalent to  $$\forall n,m>N: |a_m-a_n|_2<\varepsilon_1$$ which means that $(a_i)_{i\in\mathbb N}$ is a Cauchy sequence with respect to $|\cdot|_2$. For the other direction (which is probably harder) we may assume that the being a Cauchy sequence is the same property for both norms but I don't see how I can construct such an $\alpha$ from this fact. Remark 1: This is an exercise number 5 on page 7 in the book ""p-adic Numbers, p-adic Analysis and Zeta-Functions"" (Second Edition) by Neal Koblitz. Remark 2: The right side is the notion of norm-equivalence that I am familiar with, but in this book it is explicitly defined in the way from this post.","Let $|\cdot|_1$ and $|\cdot|_2$ be two norms on a field $\mathbb F$. We call the two norms equivalent if every Cauchy-sequence with respect to $|\cdot|_1$ is also a Cauchy-sequence with respect to $|\cdot|_2$. Prove the following statement: $$|\cdot|_1\sim|\cdot|_2\quad\Leftrightarrow\quad\exists \alpha\in\mathbb{R}_{>0}: \forall x\in\mathbb F: |x|_1=|x|_1^\alpha.$$ The direction ""$\Leftarrow$"" is straightforward: Let there an $\alpha$ with the property above, $(a_i)_{i\in\mathbb N}$ be a Cauchy sequence with respect to $|\cdot|_1$ and $\varepsilon_2\in\mathbb{R}_{>0}$ be arbitrary. Set $\varepsilon_1:=\varepsilon_2^\alpha$ and follow from the fact that $(a_i)_{i\in\mathbb N}$ is a Cauchy sequence, that there exists an $N\in\mathbb N$ such that $$\forall n,m>N: |a_m-a_n|_1<\varepsilon_1.$$ Since $|\cdot|_1\sim|\cdot|_2$ and the definition of $\varepsilon_1$ we get that $$\forall n,m>N: |a_m-a_n|_2^\alpha<\varepsilon_1^\alpha$$ which is equivalent to  $$\forall n,m>N: |a_m-a_n|_2<\varepsilon_1$$ which means that $(a_i)_{i\in\mathbb N}$ is a Cauchy sequence with respect to $|\cdot|_2$. For the other direction (which is probably harder) we may assume that the being a Cauchy sequence is the same property for both norms but I don't see how I can construct such an $\alpha$ from this fact. Remark 1: This is an exercise number 5 on page 7 in the book ""p-adic Numbers, p-adic Analysis and Zeta-Functions"" (Second Edition) by Neal Koblitz. Remark 2: The right side is the notion of norm-equivalence that I am familiar with, but in this book it is explicitly defined in the way from this post.",,"['analysis', 'normed-spaces']"
47,Expressing the wave equation solution by separation of variables as a superposition of forward and backward waves.,Expressing the wave equation solution by separation of variables as a superposition of forward and backward waves.,,"(From an exercise in Pinchover's Introduction to Partial Differential Equations ). $$u(x,t)=\frac{A_0 + B_0 t}{2}+\sum_{n=1}^{\infty} \left(A_n\cos{\frac{c\pi nt}{L}}+ B_n\sin{\frac{c\pi nt}{L}}\right)\cos{\frac{n\pi x}{L}}$$ is a general (and formal, at least) solution to the vibrating string with fixed ends. How to write this as a superposition of a forward and a backward wave? That is, as $f(x+ct)+f(x-ct)$ for some $f$. (No need to worry about rigour here, an heuristic will do.) I know, by elementary trigonometry, that $$\left(A_n\cos{\frac{c\pi nt}{L}}+ B_n\sin{\frac{c\pi nt}{L}}\right)\cos{\frac{n\pi x}{L}} =\\= (1/2)(A_n\cos +B_n\sin)\left(\frac{c\pi nt}{L} + \frac{n\pi x}{L}\right)+(1/2)(A_n\cos +B_n\sin)\left(\frac{c\pi nt}{L} - \frac{n\pi x}{L}\right), $$ but this doesn't seem to work because the variable $x$ is the one that changes sign, so apparently this cannot be interpreted as a sum of forward and backward waves. Is there a workaround to this? EDIT. The second wave is from another function $g$. The answer is then straightforward after oen's comment.","(From an exercise in Pinchover's Introduction to Partial Differential Equations ). $$u(x,t)=\frac{A_0 + B_0 t}{2}+\sum_{n=1}^{\infty} \left(A_n\cos{\frac{c\pi nt}{L}}+ B_n\sin{\frac{c\pi nt}{L}}\right)\cos{\frac{n\pi x}{L}}$$ is a general (and formal, at least) solution to the vibrating string with fixed ends. How to write this as a superposition of a forward and a backward wave? That is, as $f(x+ct)+f(x-ct)$ for some $f$. (No need to worry about rigour here, an heuristic will do.) I know, by elementary trigonometry, that $$\left(A_n\cos{\frac{c\pi nt}{L}}+ B_n\sin{\frac{c\pi nt}{L}}\right)\cos{\frac{n\pi x}{L}} =\\= (1/2)(A_n\cos +B_n\sin)\left(\frac{c\pi nt}{L} + \frac{n\pi x}{L}\right)+(1/2)(A_n\cos +B_n\sin)\left(\frac{c\pi nt}{L} - \frac{n\pi x}{L}\right), $$ but this doesn't seem to work because the variable $x$ is the one that changes sign, so apparently this cannot be interpreted as a sum of forward and backward waves. Is there a workaround to this? EDIT. The second wave is from another function $g$. The answer is then straightforward after oen's comment.",,"['analysis', 'partial-differential-equations', 'physics', 'fourier-series', 'wave-equation']"
48,Intuitive explanation for Jacobian matrix having max. rank,Intuitive explanation for Jacobian matrix having max. rank,,"As part of a longer definition I came across the following: $f: X\subseteq\mathbb{R}^m \rightarrow \mathbb{R}^n$ ($X$ open, $m<n$) with $rank(Df_{x}) = m$ for all $x \in X$. My question is now if you can give me an intuitive explanation of what it means for $f$ that its Jacobian matrix has rank $m$ for all $x\in W$. I don't know if it matters but $f$ was also to be injective and smooth.","As part of a longer definition I came across the following: $f: X\subseteq\mathbb{R}^m \rightarrow \mathbb{R}^n$ ($X$ open, $m<n$) with $rank(Df_{x}) = m$ for all $x \in X$. My question is now if you can give me an intuitive explanation of what it means for $f$ that its Jacobian matrix has rank $m$ for all $x\in W$. I don't know if it matters but $f$ was also to be injective and smooth.",,['analysis']
49,An Exercise on Inverse Function Theorem,An Exercise on Inverse Function Theorem,,"Consider the mapping $f:R^2\rightarrow R^2$ given componentwise by: $f_1(x,y)=x+a_1x^2+2b_1xy+c_1y^2\\ f_2(x,y)=y+a_2x^2+2b_2xy+c_2y^2$ Determine a neighbourhood of $(0,0)$ as large as possible on which $f$ is invertible and bijective. Inverse function theorem assures the existence of such neighbourhood. However, how do I choose the largest neighbourhood such that this holds?","Consider the mapping $f:R^2\rightarrow R^2$ given componentwise by: $f_1(x,y)=x+a_1x^2+2b_1xy+c_1y^2\\ f_2(x,y)=y+a_2x^2+2b_2xy+c_2y^2$ Determine a neighbourhood of $(0,0)$ as large as possible on which $f$ is invertible and bijective. Inverse function theorem assures the existence of such neighbourhood. However, how do I choose the largest neighbourhood such that this holds?",,['analysis']
50,"How to show $\int_{\mathbb{R}^d}  u(x) e^{-|x|^2}  (e^{-|x|^2 / n} - 1)^2 dx \rightarrow 0$, if $u\in L^2(\Bbb R^n)$?","How to show , if ?",\int_{\mathbb{R}^d}  u(x) e^{-|x|^2}  (e^{-|x|^2 / n} - 1)^2 dx \rightarrow 0 u\in L^2(\Bbb R^n),"How can I prove that  $$ \int_{\mathbb{R}^d} u(x) e^{-|x|^2} (e^{-|x|^2 / n} - 1)^2 dx \rightarrow 0 $$ as $n \rightarrow \infty$?  Here $u \in L^2(\mathbb{R}^n)$. I'm thinking the dominated convergence theorem, but I don't know how to bound the integrand. Thanks.","How can I prove that  $$ \int_{\mathbb{R}^d} u(x) e^{-|x|^2} (e^{-|x|^2 / n} - 1)^2 dx \rightarrow 0 $$ as $n \rightarrow \infty$?  Here $u \in L^2(\mathbb{R}^n)$. I'm thinking the dominated convergence theorem, but I don't know how to bound the integrand. Thanks.",,"['real-analysis', 'analysis']"
51,Sequence question from Rudin [duplicate],Sequence question from Rudin [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: In this case, does $\{x_n\}$ converge given that $\{x_{2m}\}$ and $\{x_{2m+1}\}$ converge? Fix $\alpha > 1$. Take $x_1 > \sqrt{\alpha}$, and define $$x_{n + 1} = \frac{\alpha + x_n}{1 + x_n} = x_n + \frac{\alpha - x_n^2}{1 + x_n}.$$ (a) Prove that $x_1 > x_3 > x_5 > \cdots$. (b) Prove that $x_2 < x_4 < x_6 < \cdots$. (c) Prove that $\lim x_n = \sqrt{\alpha}$. I think once I get (a), I will be able to solve the rest of the problem. However, I have tried a few different things and I can't seem to figure it out. As suggested below $$ x_{n + 2} = \frac{2 \alpha + (1 + \alpha)x_n}{(1 + \alpha) + 2x_n}$$ Another idea I had using the second form of the equation was to write $x_{n + 1}$ as $$ x_{n + 1} = x_1 + \sum_{k = 1}^n f(k) $$ where  $$f(k) = \frac{\alpha - x_k^2}{1 + x_k}$$ But once again I don't see where to go. I've been looking at the problem Martin posted and this is what I have gotten. Since $x_1 > \sqrt{\alpha}$, so $x_1 = \sqrt{\alpha} + \epsilon$. $$x_2 = \frac{\alpha + x_1}{1 + x_1}  = \frac{\alpha + \sqrt{\alpha}(1 + \epsilon)}{1 + \sqrt{\alpha}(1 + \epsilon} = \sqrt{\alpha}\left( \frac{\sqrt{\alpha} + 1 + \epsilon}{\sqrt{\alpha} + 1 + \sqrt{\alpha}\epsilon} \right) \le \sqrt{\alpha}\left( 1 - \left( \frac{\sqrt{\alpha} - 1}{\sqrt{\alpha} + 1} \right)\epsilon \right)$$ Then they use 3/2, but I dont see a way to base that on $\alpha$ instead.","This question already has answers here : Closed 12 years ago . Possible Duplicate: In this case, does $\{x_n\}$ converge given that $\{x_{2m}\}$ and $\{x_{2m+1}\}$ converge? Fix $\alpha > 1$. Take $x_1 > \sqrt{\alpha}$, and define $$x_{n + 1} = \frac{\alpha + x_n}{1 + x_n} = x_n + \frac{\alpha - x_n^2}{1 + x_n}.$$ (a) Prove that $x_1 > x_3 > x_5 > \cdots$. (b) Prove that $x_2 < x_4 < x_6 < \cdots$. (c) Prove that $\lim x_n = \sqrt{\alpha}$. I think once I get (a), I will be able to solve the rest of the problem. However, I have tried a few different things and I can't seem to figure it out. As suggested below $$ x_{n + 2} = \frac{2 \alpha + (1 + \alpha)x_n}{(1 + \alpha) + 2x_n}$$ Another idea I had using the second form of the equation was to write $x_{n + 1}$ as $$ x_{n + 1} = x_1 + \sum_{k = 1}^n f(k) $$ where  $$f(k) = \frac{\alpha - x_k^2}{1 + x_k}$$ But once again I don't see where to go. I've been looking at the problem Martin posted and this is what I have gotten. Since $x_1 > \sqrt{\alpha}$, so $x_1 = \sqrt{\alpha} + \epsilon$. $$x_2 = \frac{\alpha + x_1}{1 + x_1}  = \frac{\alpha + \sqrt{\alpha}(1 + \epsilon)}{1 + \sqrt{\alpha}(1 + \epsilon} = \sqrt{\alpha}\left( \frac{\sqrt{\alpha} + 1 + \epsilon}{\sqrt{\alpha} + 1 + \sqrt{\alpha}\epsilon} \right) \le \sqrt{\alpha}\left( 1 - \left( \frac{\sqrt{\alpha} - 1}{\sqrt{\alpha} + 1} \right)\epsilon \right)$$ Then they use 3/2, but I dont see a way to base that on $\alpha$ instead.",,['analysis']
52,uniform convergence of series of functions,uniform convergence of series of functions,,"Prove that $f(x)=\displaystyle{\sum_{n=1}^{\infty}} \dfrac{e^x \sin (n^2x)}{n^2}$ is convergent for every $x \in \mathbb{R}$ and that its sum $f(x)$ is a continuous function on $\mathbb{R}$. This is my tentative to solve the problem: $f(x)= e^x \sum {\frac{\sin (n^2x)}{n^2}}$. So, to prove that $f(x)$ is convergent, I only need to prove that $\sum {\frac{\sin(n^2x)}{n^2}}$ is convergent.  since  absolute value of $\frac {\sin (n^2x)}{n^2} \le \frac{1}{n^2}$ because absolute value of $\sin (n^2x)$ is $\le 1$ and the series: $\sum {\frac{1}{n^2}}$ is convergent, then by the $M$-test the series $\sum {\frac{\sin (n^2x)}{n^2}}$ is uniformly convergent and thus $f(x)$ is continuous on $\mathbb{R}$. Please let me know whether my solution is true? Also, do I have to distinguish the two cases where $x=0$ and $x \ne 0$? Do I have to prove that $f$ is continuous at $x=0$ separately?","Prove that $f(x)=\displaystyle{\sum_{n=1}^{\infty}} \dfrac{e^x \sin (n^2x)}{n^2}$ is convergent for every $x \in \mathbb{R}$ and that its sum $f(x)$ is a continuous function on $\mathbb{R}$. This is my tentative to solve the problem: $f(x)= e^x \sum {\frac{\sin (n^2x)}{n^2}}$. So, to prove that $f(x)$ is convergent, I only need to prove that $\sum {\frac{\sin(n^2x)}{n^2}}$ is convergent.  since  absolute value of $\frac {\sin (n^2x)}{n^2} \le \frac{1}{n^2}$ because absolute value of $\sin (n^2x)$ is $\le 1$ and the series: $\sum {\frac{1}{n^2}}$ is convergent, then by the $M$-test the series $\sum {\frac{\sin (n^2x)}{n^2}}$ is uniformly convergent and thus $f(x)$ is continuous on $\mathbb{R}$. Please let me know whether my solution is true? Also, do I have to distinguish the two cases where $x=0$ and $x \ne 0$? Do I have to prove that $f$ is continuous at $x=0$ separately?",,"['calculus', 'real-analysis', 'analysis']"
53,Infinite gradient and continuity,Infinite gradient and continuity,,"Would I be right in thinking that although the function $f(x)=x^2\sin({1\over x^2})$ has infinite gradient, it still uniformly continuous? Thanks.","Would I be right in thinking that although the function $f(x)=x^2\sin({1\over x^2})$ has infinite gradient, it still uniformly continuous? Thanks.",,['analysis']
54,A sine product with (almost) integer values,A sine product with (almost) integer values,,"Let n, k be integers, $n>1$ and $k \perp n$ denote that k, n  are coprime and let $S_n = \{1 \le k \le \lfloor n / 2 \rfloor :  k  \perp n \}.$ Then  $$ n \left( \prod_{k \in S_{n}} \sin \left( k \frac {\pi}{n} \right) \right)^{-2}  \in \mathbb{Z}. $$ I think this is surprising but I have no proof.","Let n, k be integers, $n>1$ and $k \perp n$ denote that k, n  are coprime and let $S_n = \{1 \le k \le \lfloor n / 2 \rfloor :  k  \perp n \}.$ Then  $$ n \left( \prod_{k \in S_{n}} \sin \left( k \frac {\pi}{n} \right) \right)^{-2}  \in \mathbb{Z}. $$ I think this is surprising but I have no proof.",,"['number-theory', 'analysis']"
55,Mean Value theorem and Newton's Method,Mean Value theorem and Newton's Method,,"I am trying to prove that: given $x_0, x_1, x_2 \ldots$ the sequence of approximations to $\pi$, use the mean value theorem to show that $|\pi-x_{j+1}| = |\tan c_j||\pi - x_j|$, where $c_j$ is some number between $x_j$ and $\pi$. So I do the following, but I am not sure if they are right. Let $g(x) = \sin x$. Then let $f(x) = x - \frac{g'(x)}{g(x)} = x - \tan x$. As $f(x)$ is continuous on $(\pi/2,3\pi/2)$ and differentiable on this interval excluding the endpoints, then the mean value theorem says that $\exists c \in [\frac{\pi}{2}+\epsilon, \frac{3\pi}{2}-\epsilon]$ such that $f'(c) = \frac{f(b) - f(a)}{b-a}$ where $b = \frac{3\pi}{2} - \epsilon$, $a= \frac{\pi}{2} +\epsilon$ for some $\epsilon>0$. Now (is this right?) that if $x_0$ is in this interval, then $\exists c_0$ such that $f'(c_0) = \frac{f(\pi)-f(x_0)}{\pi - x_0}$, as $(x_0,\pi)\subset(\frac{\pi}{2} +\epsilon,\frac{2\pi}{2}+\epsilon)$. (b) Now as $x_0,x_1,\ldots$ are the sequence of approximations of $\pi$ then either $x_0 < x_1 \ldots$ or $x_0 > x_1 \ldots$, depending on whether the $x_i's$ approach $\pi$ from the left or right, as $x - \tan(x)$ is strictly increasing on $(\pi/2,3\pi/2)$. In other words, this means that $(x_0,\pi) \supset (x_1,\pi) \supset \ldots$. (Can I deduce this from the above?) So here are several bits that are bothering me. If I look at (b) above, it is sort of assuming that my sequence is strictly increasing and bounded above, and hence is convergent (which is what i am supposed to prove later on on). Also, if $f(x)$ is continuous and differentiable on $(\pi/2,3\pi/2)$, then does it mean that $f(x)$ is continuous and differentiable on a subinterval of $(\pi/2,3\pi/2)$? Thanks, Ben","I am trying to prove that: given $x_0, x_1, x_2 \ldots$ the sequence of approximations to $\pi$, use the mean value theorem to show that $|\pi-x_{j+1}| = |\tan c_j||\pi - x_j|$, where $c_j$ is some number between $x_j$ and $\pi$. So I do the following, but I am not sure if they are right. Let $g(x) = \sin x$. Then let $f(x) = x - \frac{g'(x)}{g(x)} = x - \tan x$. As $f(x)$ is continuous on $(\pi/2,3\pi/2)$ and differentiable on this interval excluding the endpoints, then the mean value theorem says that $\exists c \in [\frac{\pi}{2}+\epsilon, \frac{3\pi}{2}-\epsilon]$ such that $f'(c) = \frac{f(b) - f(a)}{b-a}$ where $b = \frac{3\pi}{2} - \epsilon$, $a= \frac{\pi}{2} +\epsilon$ for some $\epsilon>0$. Now (is this right?) that if $x_0$ is in this interval, then $\exists c_0$ such that $f'(c_0) = \frac{f(\pi)-f(x_0)}{\pi - x_0}$, as $(x_0,\pi)\subset(\frac{\pi}{2} +\epsilon,\frac{2\pi}{2}+\epsilon)$. (b) Now as $x_0,x_1,\ldots$ are the sequence of approximations of $\pi$ then either $x_0 < x_1 \ldots$ or $x_0 > x_1 \ldots$, depending on whether the $x_i's$ approach $\pi$ from the left or right, as $x - \tan(x)$ is strictly increasing on $(\pi/2,3\pi/2)$. In other words, this means that $(x_0,\pi) \supset (x_1,\pi) \supset \ldots$. (Can I deduce this from the above?) So here are several bits that are bothering me. If I look at (b) above, it is sort of assuming that my sequence is strictly increasing and bounded above, and hence is convergent (which is what i am supposed to prove later on on). Also, if $f(x)$ is continuous and differentiable on $(\pi/2,3\pi/2)$, then does it mean that $f(x)$ is continuous and differentiable on a subinterval of $(\pi/2,3\pi/2)$? Thanks, Ben",,[]
56,Fourier Transforms,Fourier Transforms,,"I'm having a terrible time trying to understand Fourier transforms.  I'm very visual so leaving the $X,Y,Z,t$ domain is not working form me :) I'm trying to figure out the basics at the moment.  Like, taking a Sine wave (they're odd right?) and converting it into its real and imaginary numbers.  I'm pretty sure I got that working, but to make sure, what should the plotted data look like? Also, how do I find the power spectrum of a transform?  How do I use FT to identify the $n$ most significant frequencies in a signal?  That last question shows how lost I am! What I have: I know how to get the real and imaginary numbers from a signal.  I know how to get the phase and the magnitude.  What I need to get is the power spectrum and the most significant frequencies.  Also any dumbed down explanation of what's going on would be very helpful! Thanks!","I'm having a terrible time trying to understand Fourier transforms.  I'm very visual so leaving the $X,Y,Z,t$ domain is not working form me :) I'm trying to figure out the basics at the moment.  Like, taking a Sine wave (they're odd right?) and converting it into its real and imaginary numbers.  I'm pretty sure I got that working, but to make sure, what should the plotted data look like? Also, how do I find the power spectrum of a transform?  How do I use FT to identify the $n$ most significant frequencies in a signal?  That last question shows how lost I am! What I have: I know how to get the real and imaginary numbers from a signal.  I know how to get the phase and the magnitude.  What I need to get is the power spectrum and the most significant frequencies.  Also any dumbed down explanation of what's going on would be very helpful! Thanks!",,"['analysis', 'fourier-analysis', 'signal-processing']"
57,Continuous bijections from the open unit disc to itself - existence of fixed points,Continuous bijections from the open unit disc to itself - existence of fixed points,,"I'm wondering about the following: Let $f:D \mapsto D$ be a continuous real-valued bijection from the open unit disc $\{(x,y): x^2 + y^2 <1\}$ to itself. Does f necessarily have a fixed point? I am aware that without the bijective property, it is not necessarily true - indeed, I have constructed a counterexample without any trouble. However, I suspect with bijectivity it may be the case. I'm aware of the Brouwer Fixed Point Theorem and I imagine these two are intricately linked. However, i'm not certain where the bijectivity comes in - I believe we can argue something along the lines f now necessarily maps boundary to boundary - something about how if $x^2+y^2 \to 1$, $\|f(x,y)\| \to 1$ maybe. However, how does this help? Even if we could definitely define a limit to f(x,y) along the whole boundary and apply Brouwer, we can't guarantee the fixed points aren't all on the boundary anyway. Conversely however, I still can't construct a counterexample. Could anyone help me finish this off please? Thanks!","I'm wondering about the following: Let $f:D \mapsto D$ be a continuous real-valued bijection from the open unit disc $\{(x,y): x^2 + y^2 <1\}$ to itself. Does f necessarily have a fixed point? I am aware that without the bijective property, it is not necessarily true - indeed, I have constructed a counterexample without any trouble. However, I suspect with bijectivity it may be the case. I'm aware of the Brouwer Fixed Point Theorem and I imagine these two are intricately linked. However, i'm not certain where the bijectivity comes in - I believe we can argue something along the lines f now necessarily maps boundary to boundary - something about how if $x^2+y^2 \to 1$, $\|f(x,y)\| \to 1$ maybe. However, how does this help? Even if we could definitely define a limit to f(x,y) along the whole boundary and apply Brouwer, we can't guarantee the fixed points aren't all on the boundary anyway. Conversely however, I still can't construct a counterexample. Could anyone help me finish this off please? Thanks!",,"['real-analysis', 'analysis', 'examples-counterexamples', 'fixed-point-theorems']"
58,"Understanding Proof: Simple Functions In $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ Form A Dense Subspace of $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$",Understanding Proof: Simple Functions In  Form A Dense Subspace of,"\mathscr{L}^{\infty}(X,\mathscr{A},\mu) \mathscr{L}^{\infty}(X,\mathscr{A},\mu)","I have difficulties understanding the proof of the following proposition: Proposition $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $p$ satisfy $1\leq p$ . Then the simple functions in $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ form a dense subspace of $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ The book defines $\mathscr{L}^{\infty}$ as follows: Definition $\quad$ Let $\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R})$ be the set of all bounded real-valued $\mathscr{A}$ -measurable functions on $X$ . Define $\|\cdot\|_{\infty}$ by letting $\|f\|_{\infty}$ be the infimum of those nonnegative numbers $M$ such that $\{x\in X:|f(x)|>M\}$ is locally $\mu$ -null. We only consider real-valued functions. Here is the proof: Proof $\quad$ Let $f$ belong to $\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R})$ , and let $\epsilon$ be a positive number. Choose real numbers $a_0,a_1,\dots,a_n$ such that \begin{align}     a_0<a_1<\dots<a_n \end{align} and such that the intervals $(a_{i-1},a_i]$ cover the interval $[-\|f\|_{\infty},\|f\|_{\infty}]$ and have lengths at most $\epsilon$ . Let $A_i=f^{-1}((a_{i-1},a_i])$ for $i=1,\dots,n$ , and let $f_{\epsilon}=\sum_{i=1}^na_i\chi_{A_i}$ . Then $f_{\epsilon}$ is a simple $\mathscr{A}$ -measurable function that satisfies $\|f-f_{\epsilon}\|_{\infty}<\epsilon$ . Since $f$ and $\epsilon$ are arbitrary, the proof is complete. So my understanding of the idea of this proof is the following: For each $f\in\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R})$ and each $\epsilon>0$ , we want to construct a simple function $f_{\epsilon}$ in $\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R})$ so that $$ \|f-f_{\epsilon}\|_{\infty} = \inf\Big\{M\in\mathbb{R}_+:\{x: X:|f(x) - f_{\epsilon}(x)|>M\}\ \text{is locally $\mu$-null}\Big\}<\epsilon. $$ So, if we can construct $f_{\epsilon}$ such that $\{x\in X:|f(x)-f_{\epsilon}(x)|>\epsilon\}$ is locally $\mu$ -null, then we are done. I can see that in the proof, if $x\in A_i$ for some $i$ , then $f(x)\in(a_{i-1},a_i]$ and $f_{\epsilon}(x)=a_i$ , and so $|f(x)-f_{\epsilon}(x)|<\epsilon$ . But I don't understand why we need to let the intervals $(a_{i-1},a_i]$ cover the interval $[-\|f\|_{\infty},\|f\|_{\infty}]$ . I don't think I understand how $f_{\epsilon}$ is constructed either. Could someone please help me out? Thanks a lot in advance! Reference: $\quad$ Proposition 3.4.2 from Measure Theory by Donald Cohn.","I have difficulties understanding the proof of the following proposition: Proposition Let be a measure space, and let satisfy . Then the simple functions in form a dense subspace of The book defines as follows: Definition Let be the set of all bounded real-valued -measurable functions on . Define by letting be the infimum of those nonnegative numbers such that is locally -null. We only consider real-valued functions. Here is the proof: Proof Let belong to , and let be a positive number. Choose real numbers such that and such that the intervals cover the interval and have lengths at most . Let for , and let . Then is a simple -measurable function that satisfies . Since and are arbitrary, the proof is complete. So my understanding of the idea of this proof is the following: For each and each , we want to construct a simple function in so that So, if we can construct such that is locally -null, then we are done. I can see that in the proof, if for some , then and , and so . But I don't understand why we need to let the intervals cover the interval . I don't think I understand how is constructed either. Could someone please help me out? Thanks a lot in advance! Reference: Proposition 3.4.2 from Measure Theory by Donald Cohn.","\quad (X,\mathscr{A},\mu) p 1\leq p \mathscr{L}^{\infty}(X,\mathscr{A},\mu) \mathscr{L}^{\infty}(X,\mathscr{A},\mu) \mathscr{L}^{\infty} \quad \mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R}) \mathscr{A} X \|\cdot\|_{\infty} \|f\|_{\infty} M \{x\in X:|f(x)|>M\} \mu \quad f \mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R}) \epsilon a_0,a_1,\dots,a_n \begin{align}
    a_0<a_1<\dots<a_n
\end{align} (a_{i-1},a_i] [-\|f\|_{\infty},\|f\|_{\infty}] \epsilon A_i=f^{-1}((a_{i-1},a_i]) i=1,\dots,n f_{\epsilon}=\sum_{i=1}^na_i\chi_{A_i} f_{\epsilon} \mathscr{A} \|f-f_{\epsilon}\|_{\infty}<\epsilon f \epsilon f\in\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R}) \epsilon>0 f_{\epsilon} \mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R}) 
\|f-f_{\epsilon}\|_{\infty} = \inf\Big\{M\in\mathbb{R}_+:\{x: X:|f(x) - f_{\epsilon}(x)|>M\}\ \text{is locally \mu-null}\Big\}<\epsilon.
 f_{\epsilon} \{x\in X:|f(x)-f_{\epsilon}(x)|>\epsilon\} \mu x\in A_i i f(x)\in(a_{i-1},a_i] f_{\epsilon}(x)=a_i |f(x)-f_{\epsilon}(x)|<\epsilon (a_{i-1},a_i] [-\|f\|_{\infty},\|f\|_{\infty}] f_{\epsilon} \quad","['real-analysis', 'analysis', 'measure-theory', 'proof-explanation', 'dense-subspaces']"
59,Simplify/re-express a potential expression for the inverse of $\frac{\sinh(x)}x$,Simplify/re-express a potential expression for the inverse of,\frac{\sinh(x)}x,"$\DeclareMathOperator\M M \DeclareMathOperator\csch {csch}$ An attempt to invert the sinhc function $\frac{\sinh(x)}x$ uses Mellin inversion . Define $f(x)$ as the  inverse of $y=x\csch(x),0<y\le1$ and $0$ otherwise, graphed here . Therefore, $f(x)$ ’s Mellin transform is: $$\M_t(f(t))=\int_0^1t^{s-1}f(t)dt=\int_0^\infty t(t\csch(t))^{s-1} d(t\csch(t))=-\frac1s\int_0^\infty (t\csch(t))^sdt$$ after substituting $f(t)\to t$ as well as integration by parts on $t$ and $(t\csch(t))^{s-1} d(t\csch(t))$ with $\left.\frac ts(t\csch(t))^s\right|_0^\infty=0$ . The final integral appears in: Compute integral of general form $ \int_0^\infty \left(\frac{x}{\sinh x}\right)^n d x $ We apply binomial series, converging on $e^x>1\iff 0<x$ , to get: $$\M_t(f(t))=-\frac{2^s}s\int_0^\infty\left(\frac t{e^t-e^{-t}}\right)^sdt=-\frac{2^s}s\sum_{n=0}^\infty\binom{-s}n (-1)^n\int_0^\infty t^s e^{-(2n+s)t}dt=-2^s\Gamma(s)\sum_{n=0}^\infty\binom{-s}n \frac{(-1)^n}{(2n+s)^{s+1}}$$ Expanding/simplifying the binomial and applying the inverse Mellin transform finally gives: $$\boxed{\operatorname{sinhc}^{-1}(x)=\pm\frac1{2\pi i}\int_{c-i\infty}^{c+i\infty}(2x)^sg(s)ds;g(s)=2^{-s}\M_t(f(t))(s)=\sum_{n=0}^\infty\frac{\Gamma(n+s)}{(2n+s)^{s+1}n!},x\ge1}$$ shown here: The result is accurate to $2$ decimal places if one truncates the integral/sum at $\pm130$ . The boxed result is an integral of a sum which looks a bit cumbersome to use. However, it could possibly be a  sum of a sum if Ramanujan master theorem , which also uses the Mellin transform, is applied. Another idea is a closed form for $\M_t(f(t))$ . Both would make the boxed result cleaner. If the boxed result it correct, what is a cleaner expression for the inverse of $\frac{\sinh(x)}x$ possibly using the Ramanujan master theorem or getting a closed form for $\M_t(f(t))$ ?","An attempt to invert the sinhc function uses Mellin inversion . Define as the  inverse of and otherwise, graphed here . Therefore, ’s Mellin transform is: after substituting as well as integration by parts on and with . The final integral appears in: Compute integral of general form We apply binomial series, converging on , to get: Expanding/simplifying the binomial and applying the inverse Mellin transform finally gives: shown here: The result is accurate to decimal places if one truncates the integral/sum at . The boxed result is an integral of a sum which looks a bit cumbersome to use. However, it could possibly be a  sum of a sum if Ramanujan master theorem , which also uses the Mellin transform, is applied. Another idea is a closed form for . Both would make the boxed result cleaner. If the boxed result it correct, what is a cleaner expression for the inverse of possibly using the Ramanujan master theorem or getting a closed form for ?","\DeclareMathOperator\M M \DeclareMathOperator\csch {csch} \frac{\sinh(x)}x f(x) y=x\csch(x),0<y\le1 0 f(x) \M_t(f(t))=\int_0^1t^{s-1}f(t)dt=\int_0^\infty t(t\csch(t))^{s-1} d(t\csch(t))=-\frac1s\int_0^\infty (t\csch(t))^sdt f(t)\to t t (t\csch(t))^{s-1} d(t\csch(t)) \left.\frac ts(t\csch(t))^s\right|_0^\infty=0  \int_0^\infty \left(\frac{x}{\sinh x}\right)^n d x  e^x>1\iff 0<x \M_t(f(t))=-\frac{2^s}s\int_0^\infty\left(\frac t{e^t-e^{-t}}\right)^sdt=-\frac{2^s}s\sum_{n=0}^\infty\binom{-s}n (-1)^n\int_0^\infty t^s e^{-(2n+s)t}dt=-2^s\Gamma(s)\sum_{n=0}^\infty\binom{-s}n \frac{(-1)^n}{(2n+s)^{s+1}} \boxed{\operatorname{sinhc}^{-1}(x)=\pm\frac1{2\pi i}\int_{c-i\infty}^{c+i\infty}(2x)^sg(s)ds;g(s)=2^{-s}\M_t(f(t))(s)=\sum_{n=0}^\infty\frac{\Gamma(n+s)}{(2n+s)^{s+1}n!},x\ge1} 2 \pm130 \M_t(f(t)) \frac{\sinh(x)}x \M_t(f(t))","['analysis', 'closed-form', 'inverse-function', 'mellin-transform', 'ramanujan-master-theorem']"
60,Limit inferior of bounded sequence [duplicate],Limit inferior of bounded sequence [duplicate],,"This question already has an answer here : Proof $\limsup_{n\to\infty}\max\{a_n,b_n\} =\max\{\limsup_{n\to\infty} a_{n}, \limsup_{n\to\infty} b_{n}\}$ (1 answer) Closed last year . I found an interesting problem that I can't tackle as I am studying real analysis on my own. Let there be bounded sequences $(a_n)$ and $(b_n)$ . Proove that $\varliminf_{n\to\infty} (\min \{ a_n , b_n \} ) = \min \{ \varliminf_{n\to\infty} a_n , \varliminf_{n\to\infty} b_n \}$ . So far I have figured out that I should use lower limit monotonicity and then use definition of subsequential sequence. Am I on the right track? I have been stuck on this problem for quite some time.","This question already has an answer here : Proof $\limsup_{n\to\infty}\max\{a_n,b_n\} =\max\{\limsup_{n\to\infty} a_{n}, \limsup_{n\to\infty} b_{n}\}$ (1 answer) Closed last year . I found an interesting problem that I can't tackle as I am studying real analysis on my own. Let there be bounded sequences and . Proove that . So far I have figured out that I should use lower limit monotonicity and then use definition of subsequential sequence. Am I on the right track? I have been stuck on this problem for quite some time.","(a_n) (b_n) \varliminf_{n\to\infty} (\min \{ a_n , b_n \} ) = \min \{ \varliminf_{n\to\infty} a_n , \varliminf_{n\to\infty} b_n \}","['real-analysis', 'calculus', 'analysis', 'limsup-and-liminf']"
61,Proving the equivalence of two inequalities,Proving the equivalence of two inequalities,,"Let $f:\mathbb{R} \to \mathbb{R}$ be a function that admits primitives on $\mathbb{R}$ and $F:\mathbb{R} \to \mathbb{R}$ one primitive (i.e. $F'(x)=f(x), \forall x \in \mathbb{R}$ ). Prove that the following two inequalities are equivalent: \begin{align} a)&& (f(x)-f(y))(x-y) &\ge (x-y)^2, && \forall x, y \in \mathbb{R} \\b)&& F(x)-F(y) &\ge \frac{1}{2}(x-y)^2+f(y)(x-y), &&\forall x, y \in \mathbb{R} \end{align} My approach was to prove $b) \implies a)$ . It is obvious, because we if $b)$ is true for all pairs $(x, y)$ then it is true for $(y, x)$ by adding the two relations we obtain "" $a)$ "". But I have not found a way to prove the converse statement. My first thought was to use the mean value theorem to get $F$ into the equation, but the mean value theorem implies the existence of a point, which would not satisfy the $\forall$ quantifier.","Let be a function that admits primitives on and one primitive (i.e. ). Prove that the following two inequalities are equivalent: My approach was to prove . It is obvious, because we if is true for all pairs then it is true for by adding the two relations we obtain "" "". But I have not found a way to prove the converse statement. My first thought was to use the mean value theorem to get into the equation, but the mean value theorem implies the existence of a point, which would not satisfy the quantifier.","f:\mathbb{R} \to \mathbb{R} \mathbb{R} F:\mathbb{R} \to \mathbb{R} F'(x)=f(x), \forall x \in \mathbb{R} \begin{align}
a)&& (f(x)-f(y))(x-y) &\ge (x-y)^2, && \forall x, y \in \mathbb{R}
\\b)&& F(x)-F(y) &\ge \frac{1}{2}(x-y)^2+f(y)(x-y), &&\forall x, y \in \mathbb{R}
\end{align} b) \implies a) b) (x, y) (y, x) a) F \forall","['real-analysis', 'calculus', 'analysis', 'derivatives', 'indefinite-integrals']"
62,Does there always exist a unique point with minimal average distance to any curve?,Does there always exist a unique point with minimal average distance to any curve?,,"Given any curve $\gamma:[0,1]\to\mathbb{R}^2$ , we can consider the average distance from any point $x\in\mathbb{R}^2$ to the curve, $\int_0^1d(x,\gamma(t))dt$ . I am interested in the points $x\in\mathbb{R}^2$ which minimize this distance. It is easy to prove that such points exist by a compactness argument, but are they always unique? This problem can be expressed much more generally: given a compactly supported probability measure $m$ on $\mathbb{R}^2$ , is there always a unique point $x$ that minimizes $\int_{\mathbb{R}^2}d(x,y)dm(y)$ ? In this case the general answer is no, for example, we can consider a measure where the points $(0,0)$ and $(0,1)$ each have probability $\frac{1}{2}$ : then any point between them minimizes the average distance. But are there some simple sufficient conditions that eliminate ""trivial"" counterexamples like that one? (e.g. the measure not being supported in two points or something more restrictive if necessary). The case of the first paragraph with a curve $\gamma$ can be seen as a concrete case of a compactly supported probability measure, just by defining a measure $m$ in $\mathbb{R}^2$ by $m(A)=\mu(\gamma^{-1}(A))$ , where $\mu$ is the usual Lebesgue measure in $[0,1]$ . If the uniqueness part is true, it would have some cool consequences: for example, if the curve is a regular polygon parametrized by arc length, then the unique point at minimal average distance has to be the center of the polygon. The same would apply to other curves whose isometry group (that is, the group of isometries $\phi:\mathbb{R}^2\to\mathbb{R}^2$ such that $\phi\circ\gamma=\gamma$ ) fixes just $1$ point.","Given any curve , we can consider the average distance from any point to the curve, . I am interested in the points which minimize this distance. It is easy to prove that such points exist by a compactness argument, but are they always unique? This problem can be expressed much more generally: given a compactly supported probability measure on , is there always a unique point that minimizes ? In this case the general answer is no, for example, we can consider a measure where the points and each have probability : then any point between them minimizes the average distance. But are there some simple sufficient conditions that eliminate ""trivial"" counterexamples like that one? (e.g. the measure not being supported in two points or something more restrictive if necessary). The case of the first paragraph with a curve can be seen as a concrete case of a compactly supported probability measure, just by defining a measure in by , where is the usual Lebesgue measure in . If the uniqueness part is true, it would have some cool consequences: for example, if the curve is a regular polygon parametrized by arc length, then the unique point at minimal average distance has to be the center of the polygon. The same would apply to other curves whose isometry group (that is, the group of isometries such that ) fixes just point.","\gamma:[0,1]\to\mathbb{R}^2 x\in\mathbb{R}^2 \int_0^1d(x,\gamma(t))dt x\in\mathbb{R}^2 m \mathbb{R}^2 x \int_{\mathbb{R}^2}d(x,y)dm(y) (0,0) (0,1) \frac{1}{2} \gamma m \mathbb{R}^2 m(A)=\mu(\gamma^{-1}(A)) \mu [0,1] \phi:\mathbb{R}^2\to\mathbb{R}^2 \phi\circ\gamma=\gamma 1","['analysis', 'measure-theory', 'probability-distributions', 'curves', 'geometric-measure-theory']"
63,A question about consistency,A question about consistency,,"These Definitions come from Analysis I textbook of Tao, My question is: what is mean of ' Definition 6.1.2 is consistent with Definition 4.3.4 ',and why is it so clear that they are consistent. How do we verify this 'consistency'? Thank you!","These Definitions come from Analysis I textbook of Tao, My question is: what is mean of ' Definition 6.1.2 is consistent with Definition 4.3.4 ',and why is it so clear that they are consistent. How do we verify this 'consistency'? Thank you!",,['analysis']
64,How should I prove this map to be injective and surjective,How should I prove this map to be injective and surjective,,"I am reading from course notes on Smooth Manifolds and I was unable to prove this which Define $T_v : C^{\infty}(U) \to \mathbb{R} $ by $T_v(f)= v(f) = \frac{d}{dt} f(p+tv)|_{t=0}=df_p(v)=\frac{df}{dx_1}(p) v_1 +...+ \frac{df}{dx_n} (p)v_n$ . Let $M, M_1, M_2$ be manifolds and $M= M_1 \times M_2$ , $P= (P_1,P_2)$ Then Prove that $T_p M \approx T_{P_1} M_1 \oplus T_{P_2}M_2$ If $\pi_1 : M \to M_1$ , $\pi_2 : M\to M_2$ , $(d\pi_1)_P: T_P M \to T_{P_1}M_1$ , $(d\pi_2)_P: T_PM \to T_{P_2}M_2$ , $\alpha :T_P M \to T_{P_1} M_1 \oplus T_{P_2}M_2$ , I thought of the map $\alpha(v)=((d\pi_1)_P(v), (d\pi_2)_P (v))$ I need help in checking map 1-1 and onto. Let $\alpha(v)=0$ , to show v=0. $\alpha(v)=0$ => $(d\pi_1)_P(v)=0 $ and $ (d\pi_2)_P(v)=0$ but  how does definition of $df_p$ now implies that v=0. Can you please tell ? Similarly , I am not able to think how should  I approach the surjective part. Kindly give a rigorious proof so that I can learn the method for smooth manifolds as I think I am not much comfortable in this.","I am reading from course notes on Smooth Manifolds and I was unable to prove this which Define by . Let be manifolds and , Then Prove that If , , , , , I thought of the map I need help in checking map 1-1 and onto. Let , to show v=0. => and but  how does definition of now implies that v=0. Can you please tell ? Similarly , I am not able to think how should  I approach the surjective part. Kindly give a rigorious proof so that I can learn the method for smooth manifolds as I think I am not much comfortable in this.","T_v : C^{\infty}(U) \to \mathbb{R}  T_v(f)= v(f) = \frac{d}{dt} f(p+tv)|_{t=0}=df_p(v)=\frac{df}{dx_1}(p) v_1 +...+ \frac{df}{dx_n} (p)v_n M, M_1, M_2 M= M_1 \times M_2 P= (P_1,P_2) T_p M \approx T_{P_1} M_1 \oplus T_{P_2}M_2 \pi_1 : M \to M_1 \pi_2 : M\to M_2 (d\pi_1)_P: T_P M \to T_{P_1}M_1 (d\pi_2)_P: T_PM \to T_{P_2}M_2 \alpha :T_P M \to T_{P_1} M_1 \oplus T_{P_2}M_2 \alpha(v)=((d\pi_1)_P(v), (d\pi_2)_P (v)) \alpha(v)=0 \alpha(v)=0 (d\pi_1)_P(v)=0   (d\pi_2)_P(v)=0 df_p","['analysis', 'manifolds']"
65,Showing two metric spaces are not bi-Lipschitz equivalent but are uniformly isomorphic,Showing two metric spaces are not bi-Lipschitz equivalent but are uniformly isomorphic,,"How would one go about generating examples of metric spaces that are uniformly isomorphic (i.e. there is a uniformly continuous bijection between them with uniformly continuous inverse), but also fail to be bi-Lipschitz equivalent (bi-Lipschitz equivalent meaning there is a Lipschitz bijection between them with a Lipschitz inverse)? Furthermore, how could this be shown from first principles i.e. definitions of Lipschitz and uniform continuity? I can think of examples e.g. R,d and R,f(d) where d is the Euclidean norm and f is not Lipschitz (or its inverse is not Lipschitz). However, I struggle to find a way to prove these spaces are not bi-Lipschitz equivalent i.e. there is no Lipschitz bijection with a Lipschitz inverse between them.","How would one go about generating examples of metric spaces that are uniformly isomorphic (i.e. there is a uniformly continuous bijection between them with uniformly continuous inverse), but also fail to be bi-Lipschitz equivalent (bi-Lipschitz equivalent meaning there is a Lipschitz bijection between them with a Lipschitz inverse)? Furthermore, how could this be shown from first principles i.e. definitions of Lipschitz and uniform continuity? I can think of examples e.g. R,d and R,f(d) where d is the Euclidean norm and f is not Lipschitz (or its inverse is not Lipschitz). However, I struggle to find a way to prove these spaces are not bi-Lipschitz equivalent i.e. there is no Lipschitz bijection with a Lipschitz inverse between them.",,"['analysis', 'metric-spaces', 'lipschitz-functions']"
66,Finding the derivative of the given piecewise function,Finding the derivative of the given piecewise function,,"I'm given the following function $$f(x)= \begin{cases} \left(x-a\right)^2\left(x-b\right)^2\;,\quad x \in[a;b]\\ 0\;,\qquad\qquad\qquad\;\; x \notin[a;b] \end{cases}$$ I tried finding the derivative by using the definition, but I couldn't. I am not very familiar with the idea of taking the derivative of a piecewise function, that's why I'm stuck and don't know what to do. I would be great help for me if you could show how to take the derivatives of this kinds of functions or give some hints. Thank you very much.","I'm given the following function I tried finding the derivative by using the definition, but I couldn't. I am not very familiar with the idea of taking the derivative of a piecewise function, that's why I'm stuck and don't know what to do. I would be great help for me if you could show how to take the derivatives of this kinds of functions or give some hints. Thank you very much.","f(x)= \begin{cases} \left(x-a\right)^2\left(x-b\right)^2\;,\quad x \in[a;b]\\ 0\;,\qquad\qquad\qquad\;\; x \notin[a;b] \end{cases}","['real-analysis', 'calculus', 'analysis', 'functions', 'derivatives']"
67,Convergence of $\sum^\infty_{n=0} \frac{\cos(n + \frac{1}{n^2})}{n \cdot \ln(n^2 + 1)}$ Is my idea correct?,Convergence of  Is my idea correct?,\sum^\infty_{n=0} \frac{\cos(n + \frac{1}{n^2})}{n \cdot \ln(n^2 + 1)},Convergence: $$\sum^\infty_{n=0}\frac{\cos \left(n + \frac{1}{n^2} \right)}{n \cdot \ln \left( n^2 + 1 \right)}$$ Edit (I tried to follow the idea written by @Daniel Fischer): $$\sum_{n=0}^{\infty} \frac{cos(n + \frac{1}{n^2})}{n\cdot ln(n^2 + 1)} = \sum_{n=0}^{\infty} \frac{cos(n)}{n \cdot ln(n^2 + 1)} + \sum_{n=0}^{\infty} \frac{cos(n + \frac{1}{n^2}) - cos(n)}{n \cdot ln(n^2 + 1)}$$ $\sum_{n=0}^{\infty} \frac{cos(n)}{n \cdot ln(n^2 + 1)}$ is convergent because of Dirichlet test and How can we sum up $\sin$ and $\cos$ series when the angles are in arithmetic progression? $\sum_{n=0}^{\infty} \frac{cos(n + \frac{1}{n^2}) - cos(n)}{n \cdot ln(n^2 + 1)} = \sum_{n=0}^{\infty} \frac{-2 \left(sin \left( \frac{2n+\frac{1}{n^2}}{2} \right) \cdot sin \left( \frac{\frac{1}{n^2}}{2} \right) \right)}{n \cdot ln \left(n^2 + 1 \right)} = \sum_{n=0}^{\infty} \frac{-2 \left(sin \left( n+\frac{1}{2n^2} \right) \cdot sin \left( \frac{1}{2n^2} \right) \right)}{n \cdot ln \left(n^2 + 1 \right)}$ Then: $$\sum_{n=0}^{\infty} \left| \frac{-2 \left(sin \left( n+\frac{1}{2n^2} \right) \cdot sin \left( \frac{1}{2n^2} \right) \right)}{n \cdot ln \left(n^2 + 1 \right)} \right| \leq \sum_{n=0}^{\infty} \left| 2 \cdot \frac{ \frac{1}{2n^2} }{n \cdot ln \left(n^2 + 1 \right)} \right| \leq \sum_{n=0}^{\infty} \left| \frac{ 1 }{n^3 \cdot ln \left(n^2 + 1 \right)} \right| $$ For $n \geq 3$ : $$\sum_{n=0}^{\infty} \left| \frac{ 1 }{n^3 \cdot ln \left(n^2 + 1 \right)} \right| \leq \sum_{n=0}^{\infty} \left| \frac{1}{n^2} \right|$$ Therefore we know that $\sum_{n=0}^{\infty} \frac{cos(n + \frac{1}{n^2}) - cos(n)}{n \cdot ln(n^2 + 1)}$ it is also convergent. Is that correct?,Convergence: Edit (I tried to follow the idea written by @Daniel Fischer): is convergent because of Dirichlet test and How can we sum up $\sin$ and $\cos$ series when the angles are in arithmetic progression? Then: For : Therefore we know that it is also convergent. Is that correct?,\sum^\infty_{n=0}\frac{\cos \left(n + \frac{1}{n^2} \right)}{n \cdot \ln \left( n^2 + 1 \right)} \sum_{n=0}^{\infty} \frac{cos(n + \frac{1}{n^2})}{n\cdot ln(n^2 + 1)} = \sum_{n=0}^{\infty} \frac{cos(n)}{n \cdot ln(n^2 + 1)} + \sum_{n=0}^{\infty} \frac{cos(n + \frac{1}{n^2}) - cos(n)}{n \cdot ln(n^2 + 1)} \sum_{n=0}^{\infty} \frac{cos(n)}{n \cdot ln(n^2 + 1)} \sum_{n=0}^{\infty} \frac{cos(n + \frac{1}{n^2}) - cos(n)}{n \cdot ln(n^2 + 1)} = \sum_{n=0}^{\infty} \frac{-2 \left(sin \left( \frac{2n+\frac{1}{n^2}}{2} \right) \cdot sin \left( \frac{\frac{1}{n^2}}{2} \right) \right)}{n \cdot ln \left(n^2 + 1 \right)} = \sum_{n=0}^{\infty} \frac{-2 \left(sin \left( n+\frac{1}{2n^2} \right) \cdot sin \left( \frac{1}{2n^2} \right) \right)}{n \cdot ln \left(n^2 + 1 \right)} \sum_{n=0}^{\infty} \left| \frac{-2 \left(sin \left( n+\frac{1}{2n^2} \right) \cdot sin \left( \frac{1}{2n^2} \right) \right)}{n \cdot ln \left(n^2 + 1 \right)} \right| \leq \sum_{n=0}^{\infty} \left| 2 \cdot \frac{ \frac{1}{2n^2} }{n \cdot ln \left(n^2 + 1 \right)} \right| \leq \sum_{n=0}^{\infty} \left| \frac{ 1 }{n^3 \cdot ln \left(n^2 + 1 \right)} \right|  n \geq 3 \sum_{n=0}^{\infty} \left| \frac{ 1 }{n^3 \cdot ln \left(n^2 + 1 \right)} \right| \leq \sum_{n=0}^{\infty} \left| \frac{1}{n^2} \right| \sum_{n=0}^{\infty} \frac{cos(n + \frac{1}{n^2}) - cos(n)}{n \cdot ln(n^2 + 1)},"['real-analysis', 'analysis']"
68,Homotopy of Jordan Curves?,Homotopy of Jordan Curves?,,"A Jordan curve is an injective continuous map from $S^1$ to $\mathbb{R}^2$ . If $\gamma_1,\gamma_2,\gamma_3,\gamma_4$ are four counter clockwise Jordan curves, such that $\gamma_{i+1}$ is contained in the exterior region of $\gamma_i$ for each $i=1,2,3$ . Let $A$ be the intersection of the of exterior region of $\gamma_1$ and the interior region of $\gamma_4$ , is it true that there is a homotopy from $\gamma_2$ to $\gamma_3$ in $A$ ? Here homotopy means homotopy as maps from $S^1$ to $\mathbb{R}^2$ . I thought this seems reasonable, but I couldn't come up with a proof myself, although I do think we may assume that $\gamma_i$ 's are all polygonal curves, but it is still hard after that. Maybe it is possible to use the Jordan Schoenflies theorem, generalized to four curves?","A Jordan curve is an injective continuous map from to . If are four counter clockwise Jordan curves, such that is contained in the exterior region of for each . Let be the intersection of the of exterior region of and the interior region of , is it true that there is a homotopy from to in ? Here homotopy means homotopy as maps from to . I thought this seems reasonable, but I couldn't come up with a proof myself, although I do think we may assume that 's are all polygonal curves, but it is still hard after that. Maybe it is possible to use the Jordan Schoenflies theorem, generalized to four curves?","S^1 \mathbb{R}^2 \gamma_1,\gamma_2,\gamma_3,\gamma_4 \gamma_{i+1} \gamma_i i=1,2,3 A \gamma_1 \gamma_4 \gamma_2 \gamma_3 A S^1 \mathbb{R}^2 \gamma_i","['analysis', 'algebraic-topology', 'homotopy-theory']"
69,"Every harmonic function is real analytic, Evans p.31-32","Every harmonic function is real analytic, Evans p.31-32",,"In L.Evans book 'Partial Differential Equations' theorem $10$ page $31$ proves that every harmonic function is real analytic. I am stuck in a certain point of the proof. He mentions: To verify this, let us compute for each $N$ the remainder term $$ \begin{align} R_N(x) &= u(x)-  \sum_{k=0}^{N-1} { \sum_{|a|=k}{\dfrac{D^a u(x_0)}{a!}}(x-x_0)^a} \\&= \sum_{|a|=N} \dfrac{D^a u(x_0+t(x-x_0))}{a!} (x-x_0)^a    \end{align}\tag{*}$$ for some $t \in [0,1]$ , $t$ depending on $x$ . We establish this formula by writing out the first $N$ terms and the error in the Taylor expansion about $0$ for the function of one variable $$g(t):=u(x_0+t(x-x_0))\quad \text{at $t=1$.}$$ Question: Can someone provide a detailed proof of the second equality above in $(*)$ ? My approach: As the hint suggests I computed using the chain rule that $$g^{(k)}(t)=\sum_{|a|=k} D^a u(x_0+t(x-x_0)) (x-x_0)^a $$ and so writing $$g(t)= \sum_{k=0}^{\infty} \frac{g^{(k)}(0)}{k!}t^k  $$ one gets by plugging in $t=1$ that $$u(x)= \sum_{k=0}^{\infty} { \sum_{|a|=k}{\dfrac{D^a u(x_0)}{k!}}(x-x_0)^a} $$ Then, I split the sum from $k=0$ to $N-1$ and from $N$ to $\infty$ but I don't know how to continue.","In L.Evans book 'Partial Differential Equations' theorem page proves that every harmonic function is real analytic. I am stuck in a certain point of the proof. He mentions: To verify this, let us compute for each the remainder term for some , depending on . We establish this formula by writing out the first terms and the error in the Taylor expansion about for the function of one variable Question: Can someone provide a detailed proof of the second equality above in ? My approach: As the hint suggests I computed using the chain rule that and so writing one gets by plugging in that Then, I split the sum from to and from to but I don't know how to continue.","10 31 N 
\begin{align}
R_N(x) &= u(x)-  \sum_{k=0}^{N-1} { \sum_{|a|=k}{\dfrac{D^a u(x_0)}{a!}}(x-x_0)^a} \\&= \sum_{|a|=N} \dfrac{D^a u(x_0+t(x-x_0))}{a!} (x-x_0)^a   
\end{align}\tag{*} t \in [0,1] t x N 0 g(t):=u(x_0+t(x-x_0))\quad \text{at t=1.} (*) g^{(k)}(t)=\sum_{|a|=k} D^a u(x_0+t(x-x_0)) (x-x_0)^a  g(t)= \sum_{k=0}^{\infty} \frac{g^{(k)}(0)}{k!}t^k   t=1 u(x)= \sum_{k=0}^{\infty} { \sum_{|a|=k}{\dfrac{D^a u(x_0)}{k!}}(x-x_0)^a}  k=0 N-1 N \infty","['analysis', 'partial-differential-equations', 'taylor-expansion']"
70,Green's theorem (integration by parts) on the unit sphere,Green's theorem (integration by parts) on the unit sphere,,"List item I am missing something here and I need help to find it: Since the unit sphere $\mathbb{S}^{n-1}$ in $\mathbb{R}^{n}$ has no boundary, then given a smooth function $\phi$ and a smooth vector field $\psi$ we can integrate by parts $$\int_{\mathbb{S}^{n-1}}\phi \nabla_{\mathbb{S}^{n-1}}\cdot\psi\, d\omega_n=- \int_{\mathbb{S}^{n-1}} \nabla_{\mathbb{S}^{n-1}} \phi\cdot\psi\, d\omega_n \qquad (1)$$ where $\nabla_{\mathbb{S}^{n-1}}$ is the surface gradient on the sphere, and $\omega_n$ is the standard Surface measure on $\mathbb{S}^{n-1}$ . Therefore we have that $$\int_{\mathbb{S}^{n-1}} \nabla_{\mathbb{S}^{n-1}}\cdot\psi d\omega_n=0\qquad \qquad\qquad\qquad (2)$$ for any smooth vector field $\psi$ . Obviously, the smoothness condition is not necessary in the aforementioned statements. It can be relaxed to some appropriate integrability conditions.     Now, take the simple explicit example of the  unit sphere $\mathbb{S}^{2}$ in $\mathbb{R}^{3}$ , and for each point $(x,y,z)\in \mathbb{S}^{2}$ , consider the parametric representation $(x,y,z)=(\cos{\theta},\sin{\theta}\cos{\varphi},\sin{\theta}\sin{\varphi})$ , $0\leq \theta \leq \pi$ , $0\leq \varphi< 2\pi$ . Then, we have $$d\omega_3=\sin{\theta} d\theta d \varphi,$$ $$\nabla_{\mathbb{S}^{2}}=\frac{\partial}{\partial \theta} \widehat{\theta}+\frac{1}{\sin{\theta}}\frac{\partial}{\partial \varphi} \widehat{\varphi}$$ where $\widehat{\theta}$ and $\widehat{\varphi}$ are the standard orthonormal unit vectors tangent to the sphere pointing in the direction of increase of $\theta$ and $\varphi$ respectively. We can write $$\frac{1}{\sin{\theta}}=\nabla_{\mathbb{S}^{2}}\cdot \left(\frac{\theta}{\sin{\theta}} \nabla_{\mathbb{S}^{2}} \theta\right).$$ To verify this, one needs to recall that $\nabla_{\mathbb{S}^{2}} \cdot\nabla_{\mathbb{S}^{2}} \theta= \Delta_{\mathbb{S}^{2}} \theta$ , where $\Delta_{\mathbb{S}^{2}}=\frac{1}{\sin{\theta}}\frac{\partial}{\partial \theta} \left(\sin{\theta}\frac{\partial}{\partial \theta}\right)+ \frac{1}{\sin^2{\theta}}\frac{\partial^2}{\partial \varphi^2}$ is the Laplace Beltrami operator on $\mathbb{S}^{2}$ . On the other hand $$\int_{\mathbb{S}^{2}} \frac{1}{\sin{\theta}}d\omega_3= \int_{0}^{2\pi}\int_{0}^{\pi} \frac{1}{\sin{\theta}}\sin{\theta}d \theta d\varphi=2\pi^2\neq 0.$$ Where is my mistake ?","List item I am missing something here and I need help to find it: Since the unit sphere in has no boundary, then given a smooth function and a smooth vector field we can integrate by parts where is the surface gradient on the sphere, and is the standard Surface measure on . Therefore we have that for any smooth vector field . Obviously, the smoothness condition is not necessary in the aforementioned statements. It can be relaxed to some appropriate integrability conditions.     Now, take the simple explicit example of the  unit sphere in , and for each point , consider the parametric representation , , . Then, we have where and are the standard orthonormal unit vectors tangent to the sphere pointing in the direction of increase of and respectively. We can write To verify this, one needs to recall that , where is the Laplace Beltrami operator on . On the other hand Where is my mistake ?","\mathbb{S}^{n-1} \mathbb{R}^{n} \phi \psi \int_{\mathbb{S}^{n-1}}\phi \nabla_{\mathbb{S}^{n-1}}\cdot\psi\, d\omega_n=-
\int_{\mathbb{S}^{n-1}} \nabla_{\mathbb{S}^{n-1}} \phi\cdot\psi\, d\omega_n \qquad (1) \nabla_{\mathbb{S}^{n-1}} \omega_n \mathbb{S}^{n-1} \int_{\mathbb{S}^{n-1}} \nabla_{\mathbb{S}^{n-1}}\cdot\psi d\omega_n=0\qquad \qquad\qquad\qquad (2) \psi \mathbb{S}^{2} \mathbb{R}^{3} (x,y,z)\in \mathbb{S}^{2} (x,y,z)=(\cos{\theta},\sin{\theta}\cos{\varphi},\sin{\theta}\sin{\varphi}) 0\leq \theta \leq \pi 0\leq \varphi< 2\pi d\omega_3=\sin{\theta} d\theta d \varphi, \nabla_{\mathbb{S}^{2}}=\frac{\partial}{\partial \theta} \widehat{\theta}+\frac{1}{\sin{\theta}}\frac{\partial}{\partial \varphi} \widehat{\varphi} \widehat{\theta} \widehat{\varphi} \theta \varphi \frac{1}{\sin{\theta}}=\nabla_{\mathbb{S}^{2}}\cdot
\left(\frac{\theta}{\sin{\theta}} \nabla_{\mathbb{S}^{2}} \theta\right). \nabla_{\mathbb{S}^{2}} \cdot\nabla_{\mathbb{S}^{2}} \theta=
\Delta_{\mathbb{S}^{2}} \theta \Delta_{\mathbb{S}^{2}}=\frac{1}{\sin{\theta}}\frac{\partial}{\partial \theta} \left(\sin{\theta}\frac{\partial}{\partial \theta}\right)+
\frac{1}{\sin^2{\theta}}\frac{\partial^2}{\partial \varphi^2} \mathbb{S}^{2} \int_{\mathbb{S}^{2}} \frac{1}{\sin{\theta}}d\omega_3=
\int_{0}^{2\pi}\int_{0}^{\pi} \frac{1}{\sin{\theta}}\sin{\theta}d \theta d\varphi=2\pi^2\neq 0.","['analysis', 'multivariable-calculus', 'spherical-coordinates', 'spherical-geometry', 'greens-theorem']"
71,The Yosida transform and its properties.,The Yosida transform and its properties.,,"Let $\lambda \gt 0$ and $f:\Bbb R \to \Bbb R$ . Define the Yosida transform of $f$ by $$T_\lambda f(x) = \inf_{y \in \Bbb R}\{f(y) +\lambda |x-y|\}$$ So far I have showed that $T_\lambda f = \max\{g:g\le f$ and g is $\lambda$ -Lipschitz $\}$ . Now, I want to show a few things about this operator: if $f_n\to f$ pointwise on $\Bbb R$ then $T_\lambda f_n\to T_\lambda f$ pointwise on $\Bbb R$ . if $f_n$ has a growth condition such as $f_n(x)\ge c|x|^p$ for some $p\gt 1$ and $f_n$ is convex for each $n$ then the convergence is uniform. Suppose $f$ is convex and have the same growth as in (2). I'm wondering whether or not $T_\lambda f \to f$ as $\lambda \to \infty$ . I would really appreciate any help since I wasn't able to prove either claims. My attempt: let $x\in \Bbb R$ we want to show that $\lim_n T_\lambda f_n(x) = T_\lambda f(x)$ . For each $n\in \Bbb N$ by the definition of the infimum, there is $(y_k^n)_{k=1}^{\infty}$ such that $\lim_{k \to \infty}(f(y_k^n) +\lambda |x-y_k^n| )= T_\lambda f_n(x)$ . I thought maybe looking at the ""diagonal"" $(y_n^n)_{n=1}^{\infty}$ but not sure if that helps since im not sure if this sequence converge. Thanks for helping.","Let and . Define the Yosida transform of by So far I have showed that and g is -Lipschitz . Now, I want to show a few things about this operator: if pointwise on then pointwise on . if has a growth condition such as for some and is convex for each then the convergence is uniform. Suppose is convex and have the same growth as in (2). I'm wondering whether or not as . I would really appreciate any help since I wasn't able to prove either claims. My attempt: let we want to show that . For each by the definition of the infimum, there is such that . I thought maybe looking at the ""diagonal"" but not sure if that helps since im not sure if this sequence converge. Thanks for helping.",\lambda \gt 0 f:\Bbb R \to \Bbb R f T_\lambda f(x) = \inf_{y \in \Bbb R}\{f(y) +\lambda |x-y|\} T_\lambda f = \max\{g:g\le f \lambda \} f_n\to f \Bbb R T_\lambda f_n\to T_\lambda f \Bbb R f_n f_n(x)\ge c|x|^p p\gt 1 f_n n f T_\lambda f \to f \lambda \to \infty x\in \Bbb R \lim_n T_\lambda f_n(x) = T_\lambda f(x) n\in \Bbb N (y_k^n)_{k=1}^{\infty} \lim_{k \to \infty}(f(y_k^n) +\lambda |x-y_k^n| )= T_\lambda f_n(x) (y_n^n)_{n=1}^{\infty},"['calculus', 'analysis']"
72,How to show that this function is continuous?,How to show that this function is continuous?,,"Let $\psi(x,y)$ be a continuous function in two real variables, and then define $$f(x) = \sup_{t \in [-x,x]} \psi(x,t)$$ It seems to me that the function $f$ should also be continuous for $x \geq 0$ , but I'm not sure how to prove it. Is this true and how would you prove it?","Let be a continuous function in two real variables, and then define It seems to me that the function should also be continuous for , but I'm not sure how to prove it. Is this true and how would you prove it?","\psi(x,y) f(x) = \sup_{t \in [-x,x]} \psi(x,t) f x \geq 0","['real-analysis', 'analysis', 'continuity']"
73,Integral Expression of Legendre Polynomials,Integral Expression of Legendre Polynomials,,"a) Verify that for $x > 1$ , $n \in \mathbb{N}$ the function $$P_n(x) = \frac{1}{\pi} \int_0 ^ \pi (x + \sqrt{x ^ 2- 1} \cos \phi) ^ n d \phi$$ is a polynomial of degree $n$ (the $n$ th Legendre polynomial). b) Show that $$P_n(x) = \frac{1}{\pi} \int_0 ^ \pi \frac{d\varphi}{(x - \sqrt{x ^ 2- 1}\cos \varphi) ^ n}.$$ I have solved the first part of the problem but I am stuck on the second part. I have tried making the substitution $r = \varphi - \pi$ in the second integral and using the oddity of $y = \cos x$ to bring the denominator to $(x + \sqrt{x ^ 2 - 1} \cos r) ^ n$ , but I have little idea how to proceed from there. Can sombody help? Thanks in advance!","a) Verify that for , the function is a polynomial of degree (the th Legendre polynomial). b) Show that I have solved the first part of the problem but I am stuck on the second part. I have tried making the substitution in the second integral and using the oddity of to bring the denominator to , but I have little idea how to proceed from there. Can sombody help? Thanks in advance!",x > 1 n \in \mathbb{N} P_n(x) = \frac{1}{\pi} \int_0 ^ \pi (x + \sqrt{x ^ 2- 1} \cos \phi) ^ n d \phi n n P_n(x) = \frac{1}{\pi} \int_0 ^ \pi \frac{d\varphi}{(x - \sqrt{x ^ 2- 1}\cos \varphi) ^ n}. r = \varphi - \pi y = \cos x (x + \sqrt{x ^ 2 - 1} \cos r) ^ n,"['analysis', 'legendre-polynomials']"
74,Question about Kallenberg's proof of Doob-Dynkin Lemma,Question about Kallenberg's proof of Doob-Dynkin Lemma,,"The following is the proof for the Doob Dynkin Lemma from Kallenberg's Foundations of Modern Probability. In the theorem, $(S,\mathscr{S})$ is assumed to be Borel, i.e. Borel isomorphic to a Borel subset of $[0,1]$ . Thus, it is natural to assume that $S \in \mathscr{B}([0,1])$ . But how can we modify $h$ , to further reduce to the case when $S = [0,1]$ ?","The following is the proof for the Doob Dynkin Lemma from Kallenberg's Foundations of Modern Probability. In the theorem, is assumed to be Borel, i.e. Borel isomorphic to a Borel subset of . Thus, it is natural to assume that . But how can we modify , to further reduce to the case when ?","(S,\mathscr{S}) [0,1] S \in \mathscr{B}([0,1]) h S = [0,1]","['real-analysis', 'analysis', 'measure-theory', 'borel-sets']"
75,Doubt in Understanding Lebesgue measure.,Doubt in Understanding Lebesgue measure.,,"I am studying Measure theory form Stein and Shakarachi:Real Analysis. I come across observation regarding the outer measure. For any $E\in R^d$ $m_*(E)=\inf m_*(O)$ where $O $ is the open set containg E. i.e $\forall \epsilon>0 \exists O\in R^d$ such that $m_*(O)<m_*(E)+\epsilon$ and $E\subset O$ . Also By Lebesgue measurable set definition E is said to be Lebesgue measurable iff $\forall \epsilon>0,\exists O\in R^d$ such that $m_*(O\setminus E)<\epsilon$ and $E\subset O$ From this, I thought both definitions are the same. then every set in $R^d$ become Lebesgue measurable which is of course not true .where is my interpretation fails? When it is possible that $m_*(O \setminus E)\neq m_*(O)-m_*(E)$ Please help me out to solve this misinterpretation. Any help will be appreciated","I am studying Measure theory form Stein and Shakarachi:Real Analysis. I come across observation regarding the outer measure. For any where is the open set containg E. i.e such that and . Also By Lebesgue measurable set definition E is said to be Lebesgue measurable iff such that and From this, I thought both definitions are the same. then every set in become Lebesgue measurable which is of course not true .where is my interpretation fails? When it is possible that Please help me out to solve this misinterpretation. Any help will be appreciated","E\in R^d m_*(E)=\inf m_*(O) O  \forall \epsilon>0 \exists O\in R^d m_*(O)<m_*(E)+\epsilon E\subset O \forall \epsilon>0,\exists O\in R^d m_*(O\setminus E)<\epsilon E\subset O R^d m_*(O \setminus E)\neq m_*(O)-m_*(E)","['real-analysis', 'analysis', 'measure-theory', 'examples-counterexamples']"
76,"Why is ""antiderivative"" also known as ""primitive""?","Why is ""antiderivative"" also known as ""primitive""?",,"If I had to guess, I would say that calling the antiderivative as primitive is of French origin. Is one term more popular than the other?","If I had to guess, I would say that calling the antiderivative as primitive is of French origin. Is one term more popular than the other?",,"['calculus', 'analysis', 'terminology', 'math-history']"
77,Looking for a sigmoid-like function with convex segment around origin,Looking for a sigmoid-like function with convex segment around origin,,I am looking for a function with some specific properties (this is for a probabilistic simulation). It should be a function that is runs though (and is symmetric) the origin and asymptotically approaches $-1$ and $1$ as its parameter goes from negative to positive. Any sigmoid function would fit the bill but I ideally want it to have a convex component around the zero (i.e. I want its value to be more similar around zero) and all sigmoid functions that I am aware of change rather quickly. See the attached badly drawn picture for an illustration. Would appreciate any pointers! Computationally less expensive functions are prefered.,I am looking for a function with some specific properties (this is for a probabilistic simulation). It should be a function that is runs though (and is symmetric) the origin and asymptotically approaches $-1$ and $1$ as its parameter goes from negative to positive. Any sigmoid function would fit the bill but I ideally want it to have a convex component around the zero (i.e. I want its value to be more similar around zero) and all sigmoid functions that I am aware of change rather quickly. See the attached badly drawn picture for an illustration. Would appreciate any pointers! Computationally less expensive functions are prefered.,,"['analysis', 'functions', 'special-functions']"
78,Chain rule for higher Fréchet derivatives?,Chain rule for higher Fréchet derivatives?,,"I'm having trouble with the proof of the following fact. Let $E,F,G$ be Banach spaces. Suppose $X$ is open in $E$ and $Y$ is open in $F$. Given functions $f\in C^m(X,F)$, $g\in C^m(Y,G)$ such that $f(X)\subseteq Y$. Then $g\circ f\in C^m(X,G)$. Here $C^m$ means $m$-times Fréchet differentiable. We write $\mathcal{L}(E,F)$ for the space of bounded linear operators from $E$ to $F$. My book (Amann & Escher, Analysis II , p.185) says the proof is induction on $m$ and left as an exercise. While I understand that $\partial(g\circ f)(x_0)=\partial g(f(x_0))\partial f(x_0)$, I got stuck for the case $m\geq2$. Here are my thoughts. So $\partial(g\circ f):X\to\mathcal{L}(E,G)$ is the composition of $(\partial g)\circ f$ and $\partial f$, where $X\xrightarrow{f}F\xrightarrow{\partial g}\mathcal{L}(F,G)$ and $X\xrightarrow{\partial f}\mathcal{L}(E,F)$. However, this does not mean $\partial(g\circ f)=(\partial g)\circ f\circ\partial f$, because the composition of $(\partial g)\circ f$ and $\partial f$ is actually the composition of the values of these functions. So we shall write $\partial(g\circ f)=\big((\partial g)\circ f\big)(\partial f)$. We can decompose the map like this:\begin{matrix}X&\xrightarrow{\varphi}&\mathcal{L}(E,F)\times\mathcal{L}(F,G)&\xrightarrow{\psi}&\mathcal{L}(E,G)\\x_0&\mapsto&(\partial f(x_0),\partial g(f(x_0)))&\mapsto&\partial g(f(x_0))\partial f(x_0)\end{matrix} By assumption the map $x_0\mapsto\partial f(x_0)$ belongs to $C^{m-1}(X,\mathcal{L}(E,F))$. By induction the map $x_0\mapsto\partial g(f(x_0))$, being a composition of $\partial g$ and $f$, both of which are $C^{m-1}$, belongs to $C^{m-1}(X,\mathcal{L}(F,G))$. Hence $\varphi\in C^{m-1}(X,\mathcal{L}(E,F)\times\mathcal{L}(F,G))$. It suffices to show that $\psi\in C^{m-1}(\mathcal{L}(E,F)\times\mathcal{L}(F,G),\mathcal{L}(F,G))$, and then induction completes the proof. But how do I show that $\psi\in C^{m-1}(\mathcal{L}(E,F)\times\mathcal{L}(F,G),\mathcal{L}(F,G))$? And are there other proofs?","I'm having trouble with the proof of the following fact. Let $E,F,G$ be Banach spaces. Suppose $X$ is open in $E$ and $Y$ is open in $F$. Given functions $f\in C^m(X,F)$, $g\in C^m(Y,G)$ such that $f(X)\subseteq Y$. Then $g\circ f\in C^m(X,G)$. Here $C^m$ means $m$-times Fréchet differentiable. We write $\mathcal{L}(E,F)$ for the space of bounded linear operators from $E$ to $F$. My book (Amann & Escher, Analysis II , p.185) says the proof is induction on $m$ and left as an exercise. While I understand that $\partial(g\circ f)(x_0)=\partial g(f(x_0))\partial f(x_0)$, I got stuck for the case $m\geq2$. Here are my thoughts. So $\partial(g\circ f):X\to\mathcal{L}(E,G)$ is the composition of $(\partial g)\circ f$ and $\partial f$, where $X\xrightarrow{f}F\xrightarrow{\partial g}\mathcal{L}(F,G)$ and $X\xrightarrow{\partial f}\mathcal{L}(E,F)$. However, this does not mean $\partial(g\circ f)=(\partial g)\circ f\circ\partial f$, because the composition of $(\partial g)\circ f$ and $\partial f$ is actually the composition of the values of these functions. So we shall write $\partial(g\circ f)=\big((\partial g)\circ f\big)(\partial f)$. We can decompose the map like this:\begin{matrix}X&\xrightarrow{\varphi}&\mathcal{L}(E,F)\times\mathcal{L}(F,G)&\xrightarrow{\psi}&\mathcal{L}(E,G)\\x_0&\mapsto&(\partial f(x_0),\partial g(f(x_0)))&\mapsto&\partial g(f(x_0))\partial f(x_0)\end{matrix} By assumption the map $x_0\mapsto\partial f(x_0)$ belongs to $C^{m-1}(X,\mathcal{L}(E,F))$. By induction the map $x_0\mapsto\partial g(f(x_0))$, being a composition of $\partial g$ and $f$, both of which are $C^{m-1}$, belongs to $C^{m-1}(X,\mathcal{L}(F,G))$. Hence $\varphi\in C^{m-1}(X,\mathcal{L}(E,F)\times\mathcal{L}(F,G))$. It suffices to show that $\psi\in C^{m-1}(\mathcal{L}(E,F)\times\mathcal{L}(F,G),\mathcal{L}(F,G))$, and then induction completes the proof. But how do I show that $\psi\in C^{m-1}(\mathcal{L}(E,F)\times\mathcal{L}(F,G),\mathcal{L}(F,G))$? And are there other proofs?",,"['analysis', 'multivariable-calculus', 'chain-rule', 'frechet-derivative']"
79,How to compute $\sum_n(2n - \sqrt{n^2+1}-\sqrt{n^2-1})$?,How to compute ?,\sum_n(2n - \sqrt{n^2+1}-\sqrt{n^2-1}),"How to compute $\sum_n (2n - \sqrt{n^2+1}-\sqrt{n^2-1})$? I tried two ways: 1. \begin{align*} (2n - \sqrt{n^2+1}-\sqrt{n^2-1}) &= n - \sqrt{n^2+1} + n -\sqrt{n^2-1} \\ &= \frac{1}{n+\sqrt{n^2-1}}-\frac{1}{n-\sqrt{n^2+1}}, \end{align*} but I don't know how to do later. 2. \begin{align*} (2n - \sqrt{n^2+1}-\sqrt{n^2-1}) &= 2n - \frac{(\sqrt{n^2+1} + \sqrt{n^2-1})}{1} \\ &= 2n - \frac{2}{\sqrt{n^2+1} - \sqrt{n^2-1}}, \end{align*} but I don't know how to do later too.","How to compute $\sum_n (2n - \sqrt{n^2+1}-\sqrt{n^2-1})$? I tried two ways: 1. \begin{align*} (2n - \sqrt{n^2+1}-\sqrt{n^2-1}) &= n - \sqrt{n^2+1} + n -\sqrt{n^2-1} \\ &= \frac{1}{n+\sqrt{n^2-1}}-\frac{1}{n-\sqrt{n^2+1}}, \end{align*} but I don't know how to do later. 2. \begin{align*} (2n - \sqrt{n^2+1}-\sqrt{n^2-1}) &= 2n - \frac{(\sqrt{n^2+1} + \sqrt{n^2-1})}{1} \\ &= 2n - \frac{2}{\sqrt{n^2+1} - \sqrt{n^2-1}}, \end{align*} but I don't know how to do later too.",,"['calculus', 'analysis']"
80,"Positive Integer Multiple of Convergent, Decreasing Sequence Can Be Made Arbitrarily Small","Positive Integer Multiple of Convergent, Decreasing Sequence Can Be Made Arbitrarily Small",,"I'm working on the following problem, and for some reason I can't seem to grasp it intuitively: Let $\{ a_n \}_{n=1}^{\infty}$ be a strictly decreasing sequence of positive numbers. Assume $\sum_{n=1}^{\infty} a_n$ converges. Prove that for every $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that for all $n > N$, $(n-N)a_n < \epsilon$. Is it correct to say that we are proving that $a_n$ eventually becomes so small that even a large number of copies of the $n^{th}$ term is arbitrarily close to 0? My attempt: First, note that as $\sum_{n=1}^{\infty} a_n$ converges, it must hold that the sequence of partial sums converges, and thus is Cauchy. That is, if $s_n = \sum_{k=1}^{n} a_k$, then for fixed $\epsilon > 0$, $\exists N$ such that $$ |s_n - s_m| < \epsilon \qquad \forall n, m > N.$$ Equivalently, w.l.o.g. assume that $n > m$. Then, since $a_n$ is a sequence of positive numbers,  \begin{align*} |s_n - s_m| &= \bigg| \sum_{k=1}^n a_k - \sum_{k=1}^m a_k \bigg| \\ &= \sum_{k=m+1}^n a_k \\ &< \epsilon \qquad \forall n > m > N \\  \end{align*} Now, since $a_n$ is decreasing,  \begin{align*} (n-N)a_n &= \sum_{N+1}^n a_n \\ &= a_n + a_n + \dots + a_n \\ &< a_{N+1} + a_{N+2} + \dots + a_n \qquad n>m\geq N+1 \\ &= \sum_{k=N+1}^n a_k \\ &< \epsilon  \qquad \forall n > N \\ \end{align*} Since $\epsilon > 0$ was arbitrary, $\forall \epsilon > 0$, $\exists N \in \mathbb{N}$ such that  $$ (n-N)a_n < \epsilon \qquad \forall n > N.$$ Is this a valid proof? I feel as if I might be overcomplicating things.","I'm working on the following problem, and for some reason I can't seem to grasp it intuitively: Let $\{ a_n \}_{n=1}^{\infty}$ be a strictly decreasing sequence of positive numbers. Assume $\sum_{n=1}^{\infty} a_n$ converges. Prove that for every $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that for all $n > N$, $(n-N)a_n < \epsilon$. Is it correct to say that we are proving that $a_n$ eventually becomes so small that even a large number of copies of the $n^{th}$ term is arbitrarily close to 0? My attempt: First, note that as $\sum_{n=1}^{\infty} a_n$ converges, it must hold that the sequence of partial sums converges, and thus is Cauchy. That is, if $s_n = \sum_{k=1}^{n} a_k$, then for fixed $\epsilon > 0$, $\exists N$ such that $$ |s_n - s_m| < \epsilon \qquad \forall n, m > N.$$ Equivalently, w.l.o.g. assume that $n > m$. Then, since $a_n$ is a sequence of positive numbers,  \begin{align*} |s_n - s_m| &= \bigg| \sum_{k=1}^n a_k - \sum_{k=1}^m a_k \bigg| \\ &= \sum_{k=m+1}^n a_k \\ &< \epsilon \qquad \forall n > m > N \\  \end{align*} Now, since $a_n$ is decreasing,  \begin{align*} (n-N)a_n &= \sum_{N+1}^n a_n \\ &= a_n + a_n + \dots + a_n \\ &< a_{N+1} + a_{N+2} + \dots + a_n \qquad n>m\geq N+1 \\ &= \sum_{k=N+1}^n a_k \\ &< \epsilon  \qquad \forall n > N \\ \end{align*} Since $\epsilon > 0$ was arbitrary, $\forall \epsilon > 0$, $\exists N \in \mathbb{N}$ such that  $$ (n-N)a_n < \epsilon \qquad \forall n > N.$$ Is this a valid proof? I feel as if I might be overcomplicating things.",,"['analysis', 'proof-verification']"
81,How to prove that $\sum_{m\leqslant x}\sum_{n\leqslant x}\Big\{\frac{x}{m+n}\Big\}=\Big(2\log2-\frac{\pi^2}{12}\Big)x^2+O(x\log x)$?,How to prove that ?,\sum_{m\leqslant x}\sum_{n\leqslant x}\Big\{\frac{x}{m+n}\Big\}=\Big(2\log2-\frac{\pi^2}{12}\Big)x^2+O(x\log x),"Prove that $$\sum_{m\leqslant x}\sum_{n\leqslant x}\Big\{\frac{x}{m+n}\Big\}=\Big(2\log2-\frac{\pi^2}{12}\Big)x^2+O(x\log x),$$ where $\{x\}$ is the fractional part of the real number $x$. I know $$\sum_{n\leqslant x} \Big\{\frac{x}{n} \Big\}=(1-\gamma)x+O\big(x^{1/2}\big),$$ where $\gamma$ is Euler's constant. But I don't know whether it is useful. Can you help me?","Prove that $$\sum_{m\leqslant x}\sum_{n\leqslant x}\Big\{\frac{x}{m+n}\Big\}=\Big(2\log2-\frac{\pi^2}{12}\Big)x^2+O(x\log x),$$ where $\{x\}$ is the fractional part of the real number $x$. I know $$\sum_{n\leqslant x} \Big\{\frac{x}{n} \Big\}=(1-\gamma)x+O\big(x^{1/2}\big),$$ where $\gamma$ is Euler's constant. But I don't know whether it is useful. Can you help me?",,"['analysis', 'analytic-number-theory']"
82,Bump function inequality,Bump function inequality,,"Let $f_1, f_2, f_3\in C_0^\infty(\mathbb{R}^2)$, $\psi\in C_0^\infty(\mathbb{R})$, and $\psi_t(x)=t\psi(tx)$ for all $t>0$. Show that if $\tfrac{1}{p_1}+\tfrac{1}{p_2}+\tfrac{1}{p_3}=1$ for some $1\le p_1,p_2,p_3\le\infty$, then $$\left|\int_{\mathbb{R}^3}f_1(x,y)f_2(y,z)f_3(z,x)\psi_t(x+y+z)dxdydz\right|\le\lVert f_1\rVert_{L^{p_1}(\mathbb{R}^2)}\lVert f_2\rVert_{L^{p_2}(\mathbb{R}^2)}\lVert f_3\rVert_{L^{p_3}(\mathbb{R}^2)}\lVert\psi\rVert_{L^1(\mathbb{R})}$$ for all $t>0$. See https://en.wikipedia.org/wiki/Bump_function and http://mathworld.wolfram.com/Lp-Space.html for notation explanation. My first instinct is to use Holder's Inequality, but the pairings of the variables on the LHS and the additional $\psi$ terms make it difficult to apply. I don't have any other ideas.","Let $f_1, f_2, f_3\in C_0^\infty(\mathbb{R}^2)$, $\psi\in C_0^\infty(\mathbb{R})$, and $\psi_t(x)=t\psi(tx)$ for all $t>0$. Show that if $\tfrac{1}{p_1}+\tfrac{1}{p_2}+\tfrac{1}{p_3}=1$ for some $1\le p_1,p_2,p_3\le\infty$, then $$\left|\int_{\mathbb{R}^3}f_1(x,y)f_2(y,z)f_3(z,x)\psi_t(x+y+z)dxdydz\right|\le\lVert f_1\rVert_{L^{p_1}(\mathbb{R}^2)}\lVert f_2\rVert_{L^{p_2}(\mathbb{R}^2)}\lVert f_3\rVert_{L^{p_3}(\mathbb{R}^2)}\lVert\psi\rVert_{L^1(\mathbb{R})}$$ for all $t>0$. See https://en.wikipedia.org/wiki/Bump_function and http://mathworld.wolfram.com/Lp-Space.html for notation explanation. My first instinct is to use Holder's Inequality, but the pairings of the variables on the LHS and the additional $\psi$ terms make it difficult to apply. I don't have any other ideas.",,"['analysis', 'lebesgue-integral', 'lp-spaces', 'holder-inequality']"
83,Radon-Nikodym derivative can depend on the $\sigma$-algebra,Radon-Nikodym derivative can depend on the -algebra,\sigma,"Suppose $X$ is a set and $\mathcal{E} \subset \mathcal{F}$ are two $\sigma$-algebras of subsets of $X$. Let $\mu,\nu$ be two finite positive measures on $(X,\mathcal{F})$ and suppose $\nu \ll \mu$. Let $\bar{\mu}$ be the restriction of $\mu$ to $(X,\mathcal{E})$ and $\bar{\nu}$ the restriction of $\nu$ to $\mathcal{E}$. Find an example of the above framework where $\frac{d\bar{\nu}}{d\bar{\mu}} \ne \frac{d\nu}{d\mu}$, i.e, where the Radon-Nikodym derivative of $\bar{\nu}$ with respect to $\bar{\mu}(\text{in terms of $\mathcal{E}$)} $ is not the same as the Radon-Nikodym derivative of $\nu$ with respect to $\mu$ (in terms of $\mathcal{F}$). For this I took $X=\{0,1,2,3\},\mathcal{E}=\{\{0,1\},X,\phi,\{3,4\}\}, \mathcal{F}=\mathcal{P}(X)$. I chose $\mu$ as counting measure and $\nu(E)=\mu(E \cap\{1\})$. Then $\nu \ll \mu$. Then $\frac{d\nu}{d\mu}=\chi_{\{1\}}$ where as $\frac{d\bar{\nu}}{d\bar{\mu}}=\chi_{\{0,1\}}$. Is this alright? I was wondering if there are examples which look a little more beautiful. Thanks for the help!!","Suppose $X$ is a set and $\mathcal{E} \subset \mathcal{F}$ are two $\sigma$-algebras of subsets of $X$. Let $\mu,\nu$ be two finite positive measures on $(X,\mathcal{F})$ and suppose $\nu \ll \mu$. Let $\bar{\mu}$ be the restriction of $\mu$ to $(X,\mathcal{E})$ and $\bar{\nu}$ the restriction of $\nu$ to $\mathcal{E}$. Find an example of the above framework where $\frac{d\bar{\nu}}{d\bar{\mu}} \ne \frac{d\nu}{d\mu}$, i.e, where the Radon-Nikodym derivative of $\bar{\nu}$ with respect to $\bar{\mu}(\text{in terms of $\mathcal{E}$)} $ is not the same as the Radon-Nikodym derivative of $\nu$ with respect to $\mu$ (in terms of $\mathcal{F}$). For this I took $X=\{0,1,2,3\},\mathcal{E}=\{\{0,1\},X,\phi,\{3,4\}\}, \mathcal{F}=\mathcal{P}(X)$. I chose $\mu$ as counting measure and $\nu(E)=\mu(E \cap\{1\})$. Then $\nu \ll \mu$. Then $\frac{d\nu}{d\mu}=\chi_{\{1\}}$ where as $\frac{d\bar{\nu}}{d\bar{\mu}}=\chi_{\{0,1\}}$. Is this alright? I was wondering if there are examples which look a little more beautiful. Thanks for the help!!",,"['real-analysis', 'analysis', 'measure-theory']"
84,Why is $\mathbf{d}^{t} H \mathbf{d}$ the second directional derivative?,Why is  the second directional derivative?,\mathbf{d}^{t} H \mathbf{d},"Given a function $f: \mathbb{R}^m \to \mathbb{R}$, why does $\mathbf{d}^{t} H \mathbf{d}$ give the second derivative to $f$ along the unit vector $\mathbf{d}$? Here $H$ is the Hessian, so that $H_{ij} = \frac{\partial f}{\partial x_i \partial x_j}$. I suppose I don't have the right definition to work with, so any intuition would be helpful, as well as formal proof. (e.g. why this works in the second order taylor series expansion)","Given a function $f: \mathbb{R}^m \to \mathbb{R}$, why does $\mathbf{d}^{t} H \mathbf{d}$ give the second derivative to $f$ along the unit vector $\mathbf{d}$? Here $H$ is the Hessian, so that $H_{ij} = \frac{\partial f}{\partial x_i \partial x_j}$. I suppose I don't have the right definition to work with, so any intuition would be helpful, as well as formal proof. (e.g. why this works in the second order taylor series expansion)",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
85,Determinant of the Jacobian of a short map,Determinant of the Jacobian of a short map,,"Let $U\subset \mathbb{R}^2$ and $f:U\rightarrow \mathbb{R}^2$ be an everywhere differentiable short map (i.e. a Lipschitz function with upper Lipschitz constant =1, the upper Lipschitz constant is defined as $\sup\limits_{x\in U}\sup\limits_{v\in\mathbb{R}^2,||v||=1}||df_xv||$). Suppose that $f$ is also bi-Lipschitz and that its lower Lipschitz constant is equal to $c$, $0<c<1$ (the lower Lipschitz constant is defined as $\inf\limits_{x\in U}\inf\limits_{v\in\mathbb{R}^2,||v||=1}||df_xv||$). Let $|J_f(p)|$ be the determinant of the Jacobian of $f$ in $p\in U$. In this case I've read that it should be $|J_f(p)|\ge c$ for every $p\in U$, but I can't understand why. Can you explain it to me?","Let $U\subset \mathbb{R}^2$ and $f:U\rightarrow \mathbb{R}^2$ be an everywhere differentiable short map (i.e. a Lipschitz function with upper Lipschitz constant =1, the upper Lipschitz constant is defined as $\sup\limits_{x\in U}\sup\limits_{v\in\mathbb{R}^2,||v||=1}||df_xv||$). Suppose that $f$ is also bi-Lipschitz and that its lower Lipschitz constant is equal to $c$, $0<c<1$ (the lower Lipschitz constant is defined as $\inf\limits_{x\in U}\inf\limits_{v\in\mathbb{R}^2,||v||=1}||df_xv||$). Let $|J_f(p)|$ be the determinant of the Jacobian of $f$ in $p\in U$. In this case I've read that it should be $|J_f(p)|\ge c$ for every $p\in U$, but I can't understand why. Can you explain it to me?",,"['real-analysis', 'analysis', 'lipschitz-functions', 'jacobian']"
86,Uniform convergence in convex set,Uniform convergence in convex set,,"I have some questions about the following theorem and it's proof. Theorem . Let $X\subset \mathbb R^n$, open, convex, bounded and $f_n:X \to \mathbb R^m$ differentiable.And let's consider the following: There exist $H$: $\mathbb R^n\rightarrow L(\mathbb R^n,\mathbb R^m)$ such that $Df_n \rightarrow H$ uniformly in X, where $Df_n$ is the differential of $f_n$. There exist $x_0 \in $ X such that the sequence $\{f_n(x_0)\}$ converges. Then there exist $f:X  \rightarrow \mathbb R^m$ such that $f_n \rightarrow f$ uniformly over X, $f$ is differentiable and $Df(x)=H(x)$, for all $x\in X$. Proof . Notice that \begin{align*} &  \frac{f(x)-f(x_{0})-H(x_{0})\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}% \color{fuchsia}=\frac{f(x)-f(x_{0})-[f_{n}(x)-f_{n}(x_{0})]}{\Vert x-x_{0\Vert}}\\ &  +\frac{f_{n}(x)-f_{n}(x_{0})-\nabla f_{n}(x_{0})\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}+\frac{(\nabla f_{n}(x_{0})-H(x_{0}))\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}\\ &  =:I+II+III. \end{align*} Since $X$ is convex, by applying the mean value theorem to the function $$ g_{n,m}(t)=f_{m}(tx+(1-t)x_{0})-f_{n}(tx+(1-t)x_{0}),\quad t\in\lbrack0,1] $$ $\color{fuchsia}{**...(2)**}$ there is $t_{0}$ such that \begin{align*} &  f_{m}(x)-f_{m}(x_{0})-[f_{n}(x)-f_{n}(x_{0})]=g_{n,m}(1)-g_{n,m}(0)\\ &  =g_{n,m}^{\prime}(t_{0})=(\nabla f_{m}(z_{0})-\nabla f_{n}(z_{0}% ))\cdot(x-x_{0}), \end{align*} where $z_{0}=t_{0}x+(1-t_{0})x_{0}$. By uniform convergence of the gradients, ...(1) $$ \Vert\nabla f_{m}(z)-\nabla f_{n}(z)\Vert\leq\Vert\nabla f_{m}(z)-H(z)\Vert +\Vert\nabla f_{n}(z)-H(z)\Vert\leq2\varepsilon $$ for all $n,m\geq n_{\varepsilon}$ and all $\color{fuchsia}z\in X$. Hence, by Cauchy's inequality \begin{align*} \left\vert \frac{f_{m}(x)-f_{m}(x_{0})-[f_{n}(x)-f_{n}(x_{0})]}{\Vert x-x_{0\Vert}}\right\vert  & =\left\vert \frac{(\nabla f_{m}(z_{0})-\nabla f_{n}(z_{0}))\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}\right\vert \\ & \leq\Vert\nabla f_{m}(z_{0})-\nabla f_{n}(z_{0})\Vert\leq2\varepsilon. \end{align*} Since $X$ is bounded, this inequality implies that \begin{align*} \vert f_{m}(x)-f_{n}(x)\vert\le|f_{m}(x_{0})-f_{n}(x_{0})|+\Vert x-x_{0}\Vert  2\varepsilon\le |f_{m}(x_{0})-f_{n}(x_{0})|+2M\varepsilon. \end{align*} and so $\{f_n\}$ is a uniform Cauchy sequence ... (2) and so it converges uniformly to a function $f$. Letting $m\rightarrow\infty$ $\color{fuchsia}{**...(3)**}$ we get $$ \left\vert \frac{f(x)-f(x_{0})-[f_{n}(x)-f_{n}(x_{0})]}{\Vert x-x_{0\Vert}% }\right\vert \leq2\varepsilon $$ for all $n \geq n_{\varepsilon}$. This takes care of $I$. Taking $n =n_{\varepsilon}\color{fuchsia}{**...(4)**} $ and using the fact that $f_{n_{\varepsilon}}$ is differentiable at $x_{0}$ we get that $$ \left\vert \frac{f_{n_{\varepsilon}}(x)-f_{n_{\varepsilon}}(x_{0})-\nabla f_{n_{\varepsilon}}(x_{0})\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}\right\vert \leq\varepsilon $$ for all $x\in X$ with $0<\Vert x-x_{0}\Vert\leq\delta_{\varepsilon}$. This takes care of $II$. Lastly, by Cauchy's inequality $$ \left\vert \frac{(\nabla f_{n}(x_{0})-H(x_{0}))\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}\right\vert \leq\Vert\nabla f_{n}(x_{0})-H(x_{0})\Vert \leq\varepsilon $$ for all $n\geq n_{\varepsilon}$. In conclusion we have that for all $x\in A$ with $0<\Vert x-x_{0}\Vert\leq\delta_{\varepsilon}$, $$ \left\vert \frac{f(x)-f(x_{0})-H(x_{0})\cdot(x-x_{0})}{\Vert x-x_{0\Vert}% }\right\vert \leq4\varepsilon $$ which implies that $f$ is differentiable at $x_{0}$ with $\nabla f(x_{0})=H(x_{0})$.$\color{fuchsia}{**...(5)**}$ By repeating the proof with $x_0$ replaced by any other point, we get that $f$ is differentiable in $X$. ...(4) My questions are in $\color{fuchsia}{pink}$ color. At the beginning why it is that $H(x_0) (x-x_0)= f_{n}(x)-f_{n}(x_{0})$? How is $g_{m,n}$ defined? I think its domain is $[0,1]$ but I don't know which codomain has. What is the form of $z$, is it a simple vector in X, or has the form of $z_0$? Why do we take $m\to\infty$? Could have been $n$? And why taking $m\to\infty$ implies the next inequality? Why do we take the $n=n_{\epsilon}$? I really don't see this step. Why do we need to repeat the proof?? Isn't $x_0$ arbitrary? How can the proof be formal? At the beginning, it is stated that it must be always $\varepsilon>0$, and maybe the $x,x_0\in X$ must be given too? Or they must be given in the middle of the proof? Or where should they be? Or how?","I have some questions about the following theorem and it's proof. Theorem . Let $X\subset \mathbb R^n$, open, convex, bounded and $f_n:X \to \mathbb R^m$ differentiable.And let's consider the following: There exist $H$: $\mathbb R^n\rightarrow L(\mathbb R^n,\mathbb R^m)$ such that $Df_n \rightarrow H$ uniformly in X, where $Df_n$ is the differential of $f_n$. There exist $x_0 \in $ X such that the sequence $\{f_n(x_0)\}$ converges. Then there exist $f:X  \rightarrow \mathbb R^m$ such that $f_n \rightarrow f$ uniformly over X, $f$ is differentiable and $Df(x)=H(x)$, for all $x\in X$. Proof . Notice that \begin{align*} &  \frac{f(x)-f(x_{0})-H(x_{0})\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}% \color{fuchsia}=\frac{f(x)-f(x_{0})-[f_{n}(x)-f_{n}(x_{0})]}{\Vert x-x_{0\Vert}}\\ &  +\frac{f_{n}(x)-f_{n}(x_{0})-\nabla f_{n}(x_{0})\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}+\frac{(\nabla f_{n}(x_{0})-H(x_{0}))\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}\\ &  =:I+II+III. \end{align*} Since $X$ is convex, by applying the mean value theorem to the function $$ g_{n,m}(t)=f_{m}(tx+(1-t)x_{0})-f_{n}(tx+(1-t)x_{0}),\quad t\in\lbrack0,1] $$ $\color{fuchsia}{**...(2)**}$ there is $t_{0}$ such that \begin{align*} &  f_{m}(x)-f_{m}(x_{0})-[f_{n}(x)-f_{n}(x_{0})]=g_{n,m}(1)-g_{n,m}(0)\\ &  =g_{n,m}^{\prime}(t_{0})=(\nabla f_{m}(z_{0})-\nabla f_{n}(z_{0}% ))\cdot(x-x_{0}), \end{align*} where $z_{0}=t_{0}x+(1-t_{0})x_{0}$. By uniform convergence of the gradients, ...(1) $$ \Vert\nabla f_{m}(z)-\nabla f_{n}(z)\Vert\leq\Vert\nabla f_{m}(z)-H(z)\Vert +\Vert\nabla f_{n}(z)-H(z)\Vert\leq2\varepsilon $$ for all $n,m\geq n_{\varepsilon}$ and all $\color{fuchsia}z\in X$. Hence, by Cauchy's inequality \begin{align*} \left\vert \frac{f_{m}(x)-f_{m}(x_{0})-[f_{n}(x)-f_{n}(x_{0})]}{\Vert x-x_{0\Vert}}\right\vert  & =\left\vert \frac{(\nabla f_{m}(z_{0})-\nabla f_{n}(z_{0}))\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}\right\vert \\ & \leq\Vert\nabla f_{m}(z_{0})-\nabla f_{n}(z_{0})\Vert\leq2\varepsilon. \end{align*} Since $X$ is bounded, this inequality implies that \begin{align*} \vert f_{m}(x)-f_{n}(x)\vert\le|f_{m}(x_{0})-f_{n}(x_{0})|+\Vert x-x_{0}\Vert  2\varepsilon\le |f_{m}(x_{0})-f_{n}(x_{0})|+2M\varepsilon. \end{align*} and so $\{f_n\}$ is a uniform Cauchy sequence ... (2) and so it converges uniformly to a function $f$. Letting $m\rightarrow\infty$ $\color{fuchsia}{**...(3)**}$ we get $$ \left\vert \frac{f(x)-f(x_{0})-[f_{n}(x)-f_{n}(x_{0})]}{\Vert x-x_{0\Vert}% }\right\vert \leq2\varepsilon $$ for all $n \geq n_{\varepsilon}$. This takes care of $I$. Taking $n =n_{\varepsilon}\color{fuchsia}{**...(4)**} $ and using the fact that $f_{n_{\varepsilon}}$ is differentiable at $x_{0}$ we get that $$ \left\vert \frac{f_{n_{\varepsilon}}(x)-f_{n_{\varepsilon}}(x_{0})-\nabla f_{n_{\varepsilon}}(x_{0})\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}\right\vert \leq\varepsilon $$ for all $x\in X$ with $0<\Vert x-x_{0}\Vert\leq\delta_{\varepsilon}$. This takes care of $II$. Lastly, by Cauchy's inequality $$ \left\vert \frac{(\nabla f_{n}(x_{0})-H(x_{0}))\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}\right\vert \leq\Vert\nabla f_{n}(x_{0})-H(x_{0})\Vert \leq\varepsilon $$ for all $n\geq n_{\varepsilon}$. In conclusion we have that for all $x\in A$ with $0<\Vert x-x_{0}\Vert\leq\delta_{\varepsilon}$, $$ \left\vert \frac{f(x)-f(x_{0})-H(x_{0})\cdot(x-x_{0})}{\Vert x-x_{0\Vert}% }\right\vert \leq4\varepsilon $$ which implies that $f$ is differentiable at $x_{0}$ with $\nabla f(x_{0})=H(x_{0})$.$\color{fuchsia}{**...(5)**}$ By repeating the proof with $x_0$ replaced by any other point, we get that $f$ is differentiable in $X$. ...(4) My questions are in $\color{fuchsia}{pink}$ color. At the beginning why it is that $H(x_0) (x-x_0)= f_{n}(x)-f_{n}(x_{0})$? How is $g_{m,n}$ defined? I think its domain is $[0,1]$ but I don't know which codomain has. What is the form of $z$, is it a simple vector in X, or has the form of $z_0$? Why do we take $m\to\infty$? Could have been $n$? And why taking $m\to\infty$ implies the next inequality? Why do we take the $n=n_{\epsilon}$? I really don't see this step. Why do we need to repeat the proof?? Isn't $x_0$ arbitrary? How can the proof be formal? At the beginning, it is stated that it must be always $\varepsilon>0$, and maybe the $x,x_0\in X$ must be given too? Or they must be given in the middle of the proof? Or where should they be? Or how?",,"['analysis', 'convergence-divergence', 'proof-explanation', 'uniform-convergence', 'cauchy-sequences']"
87,"Prob. 5, Chap. 5 in Baby Rudin: If $f^\prime(x) \to 0$ as $x \to +\infty$, then $f(x+1) - f(x) \to 0$ as well.","Prob. 5, Chap. 5 in Baby Rudin: If  as , then  as well.",f^\prime(x) \to 0 x \to +\infty f(x+1) - f(x) \to 0,"Here is Prob. 5, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is defined and differentiable for every $x > 0$, and $f^\prime(x) \to 0$ as $x \to +\infty$. Put $g(x) = f(x+1)-f(x)$. Prove that $g(x) \to 0$ as $x \to +\infty$. Here is an attempt of mine. As $$\lim_{x \to +\infty} f^\prime(x) = 0,$$    so, given any real number $\varepsilon > 0$, we can find a real number $r$ such that $$\left\vert f^\prime(x) - 0 \right\vert < \varepsilon  \tag{1} $$   for all real numbers $x$ which satisfy $x > r$. Let  $x$ be a real number such that $x > r$. Then as $f$ is continuous on $[ x, x+1]$ and differentiable on $(x, x+1)$, so by the Mean Value Theorem there is some point $p \in (x, x+1)$ for which    $$g(x) = f(x+1) - f(x) = \left( \ (x+1) - x \ \right) f^\prime(p) = f^\prime(p), $$    and as $p > x > r$, so by (1) we can conclude that    $$\left\vert g(x) \right\vert = \left\vert f^\prime(p) \right\vert < \varepsilon.$$    Thus, given a real number $\varepsilon > 0$, we can find a real number $r$ such that    $$\left\vert g(x) - 0 \right\vert < \varepsilon$$    for all real numbers $x$ which satisfy $x > r$. Hence $$\lim_{x \to +\infty} g(x) = 0.$$ Is this proof correct?","Here is Prob. 5, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is defined and differentiable for every $x > 0$, and $f^\prime(x) \to 0$ as $x \to +\infty$. Put $g(x) = f(x+1)-f(x)$. Prove that $g(x) \to 0$ as $x \to +\infty$. Here is an attempt of mine. As $$\lim_{x \to +\infty} f^\prime(x) = 0,$$    so, given any real number $\varepsilon > 0$, we can find a real number $r$ such that $$\left\vert f^\prime(x) - 0 \right\vert < \varepsilon  \tag{1} $$   for all real numbers $x$ which satisfy $x > r$. Let  $x$ be a real number such that $x > r$. Then as $f$ is continuous on $[ x, x+1]$ and differentiable on $(x, x+1)$, so by the Mean Value Theorem there is some point $p \in (x, x+1)$ for which    $$g(x) = f(x+1) - f(x) = \left( \ (x+1) - x \ \right) f^\prime(p) = f^\prime(p), $$    and as $p > x > r$, so by (1) we can conclude that    $$\left\vert g(x) \right\vert = \left\vert f^\prime(p) \right\vert < \varepsilon.$$    Thus, given a real number $\varepsilon > 0$, we can find a real number $r$ such that    $$\left\vert g(x) - 0 \right\vert < \varepsilon$$    for all real numbers $x$ which satisfy $x > r$. Hence $$\lim_{x \to +\infty} g(x) = 0.$$ Is this proof correct?",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
88,When is a matrix function the Jacobian matrix of another mapping,When is a matrix function the Jacobian matrix of another mapping,,"Suppose $J(x)$ is a continuous matrix function $\mathbb{R}^D \to \mathbb{R}^D \times \mathbb{R}^D$. Do there always exist a mapping $f: \mathbb{R}^D \to \mathbb{R}^D$ so that $J = \nabla f$. If not, are there well-known conditions such that this mapping exists?","Suppose $J(x)$ is a continuous matrix function $\mathbb{R}^D \to \mathbb{R}^D \times \mathbb{R}^D$. Do there always exist a mapping $f: \mathbb{R}^D \to \mathbb{R}^D$ so that $J = \nabla f$. If not, are there well-known conditions such that this mapping exists?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'functional-calculus']"
89,Equivalence between norms in $H_0^1(\Omega)\cap H^2(\Omega)$.,Equivalence between norms in .,H_0^1(\Omega)\cap H^2(\Omega),"Based in many questions and answers like [1 , 2 , 3 ] and a comment a good comment here [4] .  I would like to know that the space $H=H_0^1(\Omega)\cap H^2(\Omega)$ can be equiped with this norm  $$\tag{1}\|\cdot\|_H=||\Delta\cdot||_{L^2}+||\nabla\cdot||_{L^2}+||\cdot||_{L^2},$$ this is the $H^2$-norm. Or, is it equipped with this one $$\tag{2}\|\cdot\|_H=||\Delta\cdot||_{L^2}+||\cdot||_{L^2}.$$ Can I say that if $H$ is equipped with (1) then, normes (1), (2) and $||\Delta\cdot||_{L^2}$ are equivalent in $H$.","Based in many questions and answers like [1 , 2 , 3 ] and a comment a good comment here [4] .  I would like to know that the space $H=H_0^1(\Omega)\cap H^2(\Omega)$ can be equiped with this norm  $$\tag{1}\|\cdot\|_H=||\Delta\cdot||_{L^2}+||\nabla\cdot||_{L^2}+||\cdot||_{L^2},$$ this is the $H^2$-norm. Or, is it equipped with this one $$\tag{2}\|\cdot\|_H=||\Delta\cdot||_{L^2}+||\cdot||_{L^2}.$$ Can I say that if $H$ is equipped with (1) then, normes (1), (2) and $||\Delta\cdot||_{L^2}$ are equivalent in $H$.",,"['analysis', 'reference-request', 'partial-differential-equations', 'sobolev-spaces', 'regularity-theory-of-pdes']"
90,Proving linearity from differentiability,Proving linearity from differentiability,,"I dont know how to prove that if $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ is differentiable and $f(x/2) = f(x)/2$ for each $x \in \mathbb{R}^n$ , then $f$ is linear. Anyone could give me a hint?","I dont know how to prove that if is differentiable and for each , then is linear. Anyone could give me a hint?",f: \mathbb{R}^{n} \rightarrow \mathbb{R} f(x/2) = f(x)/2 x \in \mathbb{R}^n f,['analysis']
91,Trying to prove that $\lim_{N \rightarrow \infty} \frac{1}{N} \Sigma_{n=1}^N f(n\alpha) = \int_0^1 f(x) dx$ [duplicate],Trying to prove that  [duplicate],\lim_{N \rightarrow \infty} \frac{1}{N} \Sigma_{n=1}^N f(n\alpha) = \int_0^1 f(x) dx,"This question already has answers here : Prove that $\lim_{N\rightarrow\infty}(1/N)\sum_{n=1}^N f(nx)=\int_{0}^1f(t)dt$ (2 answers) Closed 7 years ago . Let $\alpha$ be an irrational number. Let $f: \mathbb{R} \rightarrow \mathbb{C}$ be a continuous periodic function with period 1. Show that $\lim_{N \rightarrow \infty} \frac{1}{N} \Sigma_{n=1}^N f(n\alpha) = \int_0^1 f(x)\,dx$ The beginning (but probably not the end) of my confusion with this problem has to do with the irrational inputs. Why would that be necessary? Any help is appreciated!","This question already has answers here : Prove that $\lim_{N\rightarrow\infty}(1/N)\sum_{n=1}^N f(nx)=\int_{0}^1f(t)dt$ (2 answers) Closed 7 years ago . Let $\alpha$ be an irrational number. Let $f: \mathbb{R} \rightarrow \mathbb{C}$ be a continuous periodic function with period 1. Show that $\lim_{N \rightarrow \infty} \frac{1}{N} \Sigma_{n=1}^N f(n\alpha) = \int_0^1 f(x)\,dx$ The beginning (but probably not the end) of my confusion with this problem has to do with the irrational inputs. Why would that be necessary? Any help is appreciated!",,"['real-analysis', 'analysis', 'equidistribution']"
92,Show uniform convergence of a function series,Show uniform convergence of a function series,,Show that the function series $$\large\sum_{k = 1}^\infty \frac1{\sqrt k((x - k)^2 + 1)}$$ converges uniformly in $\mathbb{R}$. I would like to use the Weierstrass M-test but I don't know where to start.,Show that the function series $$\large\sum_{k = 1}^\infty \frac1{\sqrt k((x - k)^2 + 1)}$$ converges uniformly in $\mathbb{R}$. I would like to use the Weierstrass M-test but I don't know where to start.,,"['real-analysis', 'analysis']"
93,Measurable set of real numbers with arbitrarily small periods,Measurable set of real numbers with arbitrarily small periods,,"I am trying to prove the following exercise (exercise 3, chapter 7 of Rudins Book ""Real and Complex Analysis""): Suppose that $ E $ is a measurable set of real numbers with arbitrarily small periods. Explicitly, this means that there are positive numbers $ p_i $, converging to $ 0 $ as $ i\rightarrow \infty $, so that    \begin{align*} E + p_i = E \ \ (i = 1, 2, 3, . . . ). \end{align*}   Prove that then either $ E $ or its complement has measure $ 0 $. I have seen the following answer: Measure zero sets ,  but I tried it by my own and followed the hints Rudin give in his book. That is what I have: Given $ \alpha\in\mathbb{R}$, we define $F(x)=m(E\cap [\alpha, x])$, where $x>\alpha$. Then we get:    \begin{align*} F(x+p_i)-F(x-p_i) &=m(E\cap [\alpha, x+p_i])-m(E\cap [\alpha, x-p_i])\\ &=m((E\cap [\alpha, x+p_i])-p_i)-m((E\cap [\alpha, x-p_i])+p_i) \\ &=m(E\cap [\alpha-p_i, x)-m(E\cap [\alpha+p_i, x]) \\ &=m(E\cap [\alpha-p_i,\alpha+p_i]).  \end{align*}     This implies that for every points $\alpha+p_i<x<y$, we get that   \begin{align} F(x+p_i)-F(x-p_i)=F(y+p_i)-F(y-p_i).  \end{align} So the hint he gives now is to think about what does this imply for $F'(x)$ if $m(E) > 0$. I just have in mind that we could apply the Lebesgue differentiation theorem for $f=1_E$: \begin{align*} \lim_{m(I)\rightarrow 0}\frac{m(E\cap I)}{m(I)}=1_E(x) \end{align*} but this do not give enough information to conclude the density of $E$. Nevertheless, by having $m(E)>0$, we now that it must have some density point. I would appreciate any help!","I am trying to prove the following exercise (exercise 3, chapter 7 of Rudins Book ""Real and Complex Analysis""): Suppose that $ E $ is a measurable set of real numbers with arbitrarily small periods. Explicitly, this means that there are positive numbers $ p_i $, converging to $ 0 $ as $ i\rightarrow \infty $, so that    \begin{align*} E + p_i = E \ \ (i = 1, 2, 3, . . . ). \end{align*}   Prove that then either $ E $ or its complement has measure $ 0 $. I have seen the following answer: Measure zero sets ,  but I tried it by my own and followed the hints Rudin give in his book. That is what I have: Given $ \alpha\in\mathbb{R}$, we define $F(x)=m(E\cap [\alpha, x])$, where $x>\alpha$. Then we get:    \begin{align*} F(x+p_i)-F(x-p_i) &=m(E\cap [\alpha, x+p_i])-m(E\cap [\alpha, x-p_i])\\ &=m((E\cap [\alpha, x+p_i])-p_i)-m((E\cap [\alpha, x-p_i])+p_i) \\ &=m(E\cap [\alpha-p_i, x)-m(E\cap [\alpha+p_i, x]) \\ &=m(E\cap [\alpha-p_i,\alpha+p_i]).  \end{align*}     This implies that for every points $\alpha+p_i<x<y$, we get that   \begin{align} F(x+p_i)-F(x-p_i)=F(y+p_i)-F(y-p_i).  \end{align} So the hint he gives now is to think about what does this imply for $F'(x)$ if $m(E) > 0$. I just have in mind that we could apply the Lebesgue differentiation theorem for $f=1_E$: \begin{align*} \lim_{m(I)\rightarrow 0}\frac{m(E\cap I)}{m(I)}=1_E(x) \end{align*} but this do not give enough information to conclude the density of $E$. Nevertheless, by having $m(E)>0$, we now that it must have some density point. I would appreciate any help!",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
94,Characterizing sets with a function,Characterizing sets with a function,,"Given a set $A=\{z_1,\ldots,z_n\},\epsilon>0$ where $z_i \in \mathbb{N}$, I wonder if there is an easy way to choose numbers $0<\lambda_1,\ldots,\lambda_n<\epsilon$ explicitly such that for all subsets $T\subseteq\{1,\ldots,n\}$ we have that the numbers $$f(T)=\sum_{t \in T}\sqrt{z_{t}+\lambda_{t}}$$ are pairwise different? That is $f(T)\neq f(T')$ if $T \neq T'$. The square root seems to make things difficult, otherwise one could choose something like $\lambda_i=2^{-i -k}$ for some constant $k>0$.","Given a set $A=\{z_1,\ldots,z_n\},\epsilon>0$ where $z_i \in \mathbb{N}$, I wonder if there is an easy way to choose numbers $0<\lambda_1,\ldots,\lambda_n<\epsilon$ explicitly such that for all subsets $T\subseteq\{1,\ldots,n\}$ we have that the numbers $$f(T)=\sum_{t \in T}\sqrt{z_{t}+\lambda_{t}}$$ are pairwise different? That is $f(T)\neq f(T')$ if $T \neq T'$. The square root seems to make things difficult, otherwise one could choose something like $\lambda_i=2^{-i -k}$ for some constant $k>0$.",,"['analysis', 'number-theory']"
95,"$ x+y+z = 3, \; \sum\limits_{cyc} \frac{x}{2x^2+x+1} \leq \frac{3}{4} $",," x+y+z = 3, \; \sum\limits_{cyc} \frac{x}{2x^2+x+1} \leq \frac{3}{4} ","For positive variables $ x+y+z=3 $ , show that $ \displaystyle \sum_{cyc} \dfrac{x}{2x^2+x+1} \leq \dfrac{3}{4} $ . Apart from $ (n-1)$ EV, I could not prove this inequality. I've tried transforming it into a more generic problem - it looks fairly more interesting to me. Consider a continuous function $ f(x) \geq 0, \; 0 < x < \alpha $ , with a unique value of $ \gamma $ such that $ f'(\gamma) = 0 $ and a unique value of $ \theta $ such that $ f''(\theta) = 0 $ , being $ \gamma < \theta $ . If $ \displaystyle \sum_{i=1}^{n} x_i = \alpha, \; f(0)=0$ and $ f \left ( \frac{\alpha}{n} \right ) = \frac{\beta}{n} $ , is it always correct to conclude that $ \displaystyle \sum_{i=1}^{n} f (x_i) \leq \beta $ , being $ f(\gamma) > \frac{\beta}{n} $ ? Provide an example other than $ f(x) = \frac{x}{2x^2+x+1} $ and a counter-example.","For positive variables , show that . Apart from EV, I could not prove this inequality. I've tried transforming it into a more generic problem - it looks fairly more interesting to me. Consider a continuous function , with a unique value of such that and a unique value of such that , being . If and , is it always correct to conclude that , being ? Provide an example other than and a counter-example."," x+y+z=3   \displaystyle \sum_{cyc} \dfrac{x}{2x^2+x+1} \leq \dfrac{3}{4}   (n-1)  f(x) \geq 0, \; 0 < x < \alpha   \gamma   f'(\gamma) = 0   \theta   f''(\theta) = 0   \gamma < \theta   \displaystyle \sum_{i=1}^{n} x_i = \alpha, \; f(0)=0  f \left ( \frac{\alpha}{n} \right ) = \frac{\beta}{n}   \displaystyle \sum_{i=1}^{n} f (x_i) \leq \beta   f(\gamma) > \frac{\beta}{n}   f(x) = \frac{x}{2x^2+x+1} ","['analysis', 'multivariable-calculus', 'inequality', 'tangent-line-method']"
96,What is the role of fixed point theorems in modern mathematics?,What is the role of fixed point theorems in modern mathematics?,,"About Fixed Point Theorems , Wikipedia says: Results of this kind are amongst the most generally useful in   mathematics. This seems an accurate statement: indeed, there are many journals dedicated exclusively to this topic and many monographs have been published. Could you elaborate on why this kind of theorem is important to the development of current mathematical research? In particular, what is the relationship between fixed point theory and analysis? Also, could you recommend some self-contained but comprehensive monographs?","About Fixed Point Theorems , Wikipedia says: Results of this kind are amongst the most generally useful in   mathematics. This seems an accurate statement: indeed, there are many journals dedicated exclusively to this topic and many monographs have been published. Could you elaborate on why this kind of theorem is important to the development of current mathematical research? In particular, what is the relationship between fixed point theory and analysis? Also, could you recommend some self-contained but comprehensive monographs?",,"['analysis', 'reference-request', 'partial-differential-equations', 'soft-question', 'fixed-point-theorems']"
97,Closed Sets are Measurable Proof in Stein/Shakarchi Text,Closed Sets are Measurable Proof in Stein/Shakarchi Text,,"Had a question about the proof laid out in Stein/Shakarchi's text that closed sets are measurable. Im hoping someone can explain the part in bold below, which is the only thing that's bugging me. They first assume F to be compact and show this is equivalent to it being closed, so take F to be compact: Suppose $F$ is compact, which implies we can find a $O$ such that $F_*\subset O$ and $m_*(O)\leq m_*(F)+\epsilon$. Since $F$ is closed, then $O\backslash F$ is open and we write it as a countable union of almost disjoint cubes $\implies O\backslash F = \bigcup_i Q_i$ Fix an $N$. The finite union $K=\bigcup_i^{N}Qi$ is compact, which means that $d(K,F)>0$ This implies that $m_*(O)\geq m_*(F)+m_*(K)= m_*(F)+\sum_i^{N}m_*(Q_i)$ Which works out to be equivalent to $\sum_i^{N}m_*(Q_i)\leq m_*(O)-m_*(F)\leq \epsilon$ Here's where i get lost: The authors then claim that ""this holds as N goes to infinity""...Why? Which gives us $m_*(O\backslash F)\leq \sum_i^{\infty}m_*(Q_i)\leq m_*(O)-m_*(F)\leq \epsilon$","Had a question about the proof laid out in Stein/Shakarchi's text that closed sets are measurable. Im hoping someone can explain the part in bold below, which is the only thing that's bugging me. They first assume F to be compact and show this is equivalent to it being closed, so take F to be compact: Suppose $F$ is compact, which implies we can find a $O$ such that $F_*\subset O$ and $m_*(O)\leq m_*(F)+\epsilon$. Since $F$ is closed, then $O\backslash F$ is open and we write it as a countable union of almost disjoint cubes $\implies O\backslash F = \bigcup_i Q_i$ Fix an $N$. The finite union $K=\bigcup_i^{N}Qi$ is compact, which means that $d(K,F)>0$ This implies that $m_*(O)\geq m_*(F)+m_*(K)= m_*(F)+\sum_i^{N}m_*(Q_i)$ Which works out to be equivalent to $\sum_i^{N}m_*(Q_i)\leq m_*(O)-m_*(F)\leq \epsilon$ Here's where i get lost: The authors then claim that ""this holds as N goes to infinity""...Why? Which gives us $m_*(O\backslash F)\leq \sum_i^{\infty}m_*(Q_i)\leq m_*(O)-m_*(F)\leq \epsilon$",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
98,"Proving the Cantor set is closed (without using the fact ""the intersection of closed sets is closed"")","Proving the Cantor set is closed (without using the fact ""the intersection of closed sets is closed"")",,"I am trying to prove that the Cantor set (C) is closed without using the fact ""the intersection of closed sets is closed"". My proof is as follows. Proof: Let $ \{x_{n}\} $ be a sequence of elements of $ C $ such that $ \{x_{n}\} $ converges for some $ x\in [0,1] $. Notice that for each $ n\in \mathbb{N} $, $ x_{n} $ can be written as $ \sum \limits_{k=1}^{\infty}\frac{x_{n,k}}{3^{k}} $ where $ x_{n,k}\in \{0,2\} $ for each $ k\in \mathbb{N} $. Since $ x\in [0,1] $ we have that $ x=\sum \limits_{k=1}^{\infty}\frac{x_{k}}{3^{k}} $ where $ x_{k}\in \{0,1,2\} $ for each $ k\in \mathbb{N} $. Since $ \{x_{n}\} $ converges to $ x $, for each $ k\in \mathbb{N} $, there exists $ n_{k}\in \mathbb{N} $ such that for each $ n\geq n_{k} $, $ |x_{n}-x|<\frac{1}{3^{k}} $. This implies for each $ k\in \mathbb{N} $, there exists $ n_{k}\in \mathbb{N} $ such that for each $ n\geq n_{k} $, $ x_{k}=x_{n,k} $. Therefore for each $ k\in \mathbb{N} $, $ x_{k}\in \{0,2\} $ and hence $ x\in C $. Thus the Cantor set (C) is closed. Can someone verify my proof? Is there something missing? Is this proof depend on the consideration of two different ternary expansion of some numbers(end points of the removed middle third intervals in the construction of C) of $[0,1]$? Thanks for the feedback.","I am trying to prove that the Cantor set (C) is closed without using the fact ""the intersection of closed sets is closed"". My proof is as follows. Proof: Let $ \{x_{n}\} $ be a sequence of elements of $ C $ such that $ \{x_{n}\} $ converges for some $ x\in [0,1] $. Notice that for each $ n\in \mathbb{N} $, $ x_{n} $ can be written as $ \sum \limits_{k=1}^{\infty}\frac{x_{n,k}}{3^{k}} $ where $ x_{n,k}\in \{0,2\} $ for each $ k\in \mathbb{N} $. Since $ x\in [0,1] $ we have that $ x=\sum \limits_{k=1}^{\infty}\frac{x_{k}}{3^{k}} $ where $ x_{k}\in \{0,1,2\} $ for each $ k\in \mathbb{N} $. Since $ \{x_{n}\} $ converges to $ x $, for each $ k\in \mathbb{N} $, there exists $ n_{k}\in \mathbb{N} $ such that for each $ n\geq n_{k} $, $ |x_{n}-x|<\frac{1}{3^{k}} $. This implies for each $ k\in \mathbb{N} $, there exists $ n_{k}\in \mathbb{N} $ such that for each $ n\geq n_{k} $, $ x_{k}=x_{n,k} $. Therefore for each $ k\in \mathbb{N} $, $ x_{k}\in \{0,2\} $ and hence $ x\in C $. Thus the Cantor set (C) is closed. Can someone verify my proof? Is there something missing? Is this proof depend on the consideration of two different ternary expansion of some numbers(end points of the removed middle third intervals in the construction of C) of $[0,1]$? Thanks for the feedback.",,"['real-analysis', 'analysis', 'proof-verification', 'cantor-set']"
99,How prove this limits is exsit $\displaystyle\lim_{n\to\infty}x_{n}$,How prove this limits is exsit,\displaystyle\lim_{n\to\infty}x_{n},"let $f:[a,b]\to [a,b]$ be Continuous function,Assmue that sequence $\{x_{n}\}(n\ge 0)$ such   $$x_{0}=x,x_{1}=f(x_{0}),x_{2}=f(x_{1}),\cdots,x_{n+1}=f(x_{n}),\forall n\in N^{+}$$   and $$\lim_{n\to\infty}(x_{n+1}-x_{n})=0$$   show that:   $$\lim_{n\to\infty}x_{n}$$ is exsit My idea: first  we note $$x_{n+1}-x_{n}=f(x_{n})-x_{n}$$ so we have $$\lim_{n\to\infty}(f(x_{n})-x_{n})=0$$ then I can't prove this limits is exsit?I fell can't easy prove it.?","let $f:[a,b]\to [a,b]$ be Continuous function,Assmue that sequence $\{x_{n}\}(n\ge 0)$ such   $$x_{0}=x,x_{1}=f(x_{0}),x_{2}=f(x_{1}),\cdots,x_{n+1}=f(x_{n}),\forall n\in N^{+}$$   and $$\lim_{n\to\infty}(x_{n+1}-x_{n})=0$$   show that:   $$\lim_{n\to\infty}x_{n}$$ is exsit My idea: first  we note $$x_{n+1}-x_{n}=f(x_{n})-x_{n}$$ so we have $$\lim_{n\to\infty}(f(x_{n})-x_{n})=0$$ then I can't prove this limits is exsit?I fell can't easy prove it.?",,['analysis']
