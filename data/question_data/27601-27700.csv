,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Prove matrix AB is invertible.,Prove matrix AB is invertible.,,"Suppose $A$ and $B$ are $n \times n$ matrices such that $||I-AB||$ is less than $1$, then show that $AB$ is invertible. I started to prove this by contradiction. Assuming it wasn't invertible so $(I-AB)x=0$ and $||x|| = 1$ thus $x=ABx$. I tried to find a contradiction but didn't have any luck. Any suggestions?","Suppose $A$ and $B$ are $n \times n$ matrices such that $||I-AB||$ is less than $1$, then show that $AB$ is invertible. I started to prove this by contradiction. Assuming it wasn't invertible so $(I-AB)x=0$ and $||x|| = 1$ thus $x=ABx$. I tried to find a contradiction but didn't have any luck. Any suggestions?",,"['linear-algebra', 'matrices']"
1,Is $\mathbb{R}^1$ a subspace of $\mathbb{R}^2?$,Is  a subspace of,\mathbb{R}^1 \mathbb{R}^2?,"My intuition tells me it is. But in terms of vectors, the span of a vector with only one component (a vector in $\mathbb{R}^1$) is not said to be a subspace of $\mathbb{R}^2$","My intuition tells me it is. But in terms of vectors, the span of a vector with only one component (a vector in $\mathbb{R}^1$) is not said to be a subspace of $\mathbb{R}^2$",,"['linear-algebra', 'vector-spaces']"
2,"For $V$ vector space of dimension $n$ under $\mathbb C$ and $T: V \to V $ linear transformation , show $V= \ker T^n \oplus $ Im $T^n$","For  vector space of dimension  under  and  linear transformation , show  Im",V n \mathbb C T: V \to V  V= \ker T^n \oplus  T^n,"I'm looking for the shortest and the clearest proof for this following theorem: For $V$ vector space of dimension $n$ under $\mathbb C$ and $T: V \to V $ linear transformation , I need to show $V= \ker T^n \oplus  $ Im $T^n$. Any hints? I don't know where to start from. Thank you.","I'm looking for the shortest and the clearest proof for this following theorem: For $V$ vector space of dimension $n$ under $\mathbb C$ and $T: V \to V $ linear transformation , I need to show $V= \ker T^n \oplus  $ Im $T^n$. Any hints? I don't know where to start from. Thank you.",,[]
3,Characterization of linear independence by wedge product,Characterization of linear independence by wedge product,,"Let $V$ be a vector space of finite dimension. Show that $x_1,...,x_k$ is linearly independent iff  $x_1\wedge ... \wedge x_k \neq 0$.","Let $V$ be a vector space of finite dimension. Show that $x_1,...,x_k$ is linearly independent iff  $x_1\wedge ... \wedge x_k \neq 0$.",,"['linear-algebra', 'abstract-algebra']"
4,"Ideals of $\mathscr{I}(V) = \text{Hom}(V,V)$",Ideals of,"\mathscr{I}(V) = \text{Hom}(V,V)","I have been trying this notoriously difficult problem for quite some time but i haven't made any progress. Let $\mathscr{I}(V)$ denote the set of all homomorphisms $f : V \to V$. That is $\mathscr{I}(V) = \text{Hom}(V,V)$. Suppose $V$ is a vector space over a field $K$ and $\text{dim}_{K}(V)>1$, then prove that $\mathscr{I}(V)$ has no proper two sided ideals . This means we have to show that $\mathscr{I}(V)$ has no two sided ideals other than $(0)$ and $\mathscr{I}(V)$. Next, does the conclusion of the above problem remain true if $V$ is infinite dimensional . Moreover, since Field's are the ubiquitous algebraic structures to have trivial ideals i tried thinking of proving $\mathscr{I}(V)$ to be a field. But i haven't yet taken a single step forward in terms of this problem.","I have been trying this notoriously difficult problem for quite some time but i haven't made any progress. Let $\mathscr{I}(V)$ denote the set of all homomorphisms $f : V \to V$. That is $\mathscr{I}(V) = \text{Hom}(V,V)$. Suppose $V$ is a vector space over a field $K$ and $\text{dim}_{K}(V)>1$, then prove that $\mathscr{I}(V)$ has no proper two sided ideals . This means we have to show that $\mathscr{I}(V)$ has no two sided ideals other than $(0)$ and $\mathscr{I}(V)$. Next, does the conclusion of the above problem remain true if $V$ is infinite dimensional . Moreover, since Field's are the ubiquitous algebraic structures to have trivial ideals i tried thinking of proving $\mathscr{I}(V)$ to be a field. But i haven't yet taken a single step forward in terms of this problem.",,['linear-algebra']
5,"Prove $[x,y,z]$ such that $|x-y|=|y-z|$ is a subspace of $\mathbb{R}^3$",Prove  such that  is a subspace of,"[x,y,z] |x-y|=|y-z| \mathbb{R}^3","Let S be the collection of vectors $\begin{pmatrix} x \\ y \\ z \end{pmatrix}$ in $\mathbb{R}^3$ such that $|x-y|=|y-z|$. Either prove that S forms a subspace of $\mathbb{R}^3$ or give a counterexample to show that it does not. (from Poole, Linear Algebra - A Modern Introduction 2nd ed., exercise 3.5.8) I'm working on this book by myself, so if you can just give me a hint to show me the right direction, that would be great. I have been trying to solve this by going through the three requirements of a subspace. The zero vector is in S. If u and v are in S, then u + v is in S. If u is in S and c is a scalar, then c u is in S. It's easy to see the first one holds. For the third one I noticed that by adding c as multiplier on both sides of $|x-y|=|y-z|$ and fiddling a bit it becomes $|$ c $u_x$- c $u_y|$ = $|$ c $u_y$- c $u_z|$, which is the form I need to show it is closed under scalar multiplication. But how to show the second one I have no idea. Stuff I tried tended to split into multiple cases because absolute values are taken of the left and right sides, it didn't feel like the right direction. Is there something really easy I'm missing here? I also tried imagining what the subspace would look like, but it's too difficult to visualize.","Let S be the collection of vectors $\begin{pmatrix} x \\ y \\ z \end{pmatrix}$ in $\mathbb{R}^3$ such that $|x-y|=|y-z|$. Either prove that S forms a subspace of $\mathbb{R}^3$ or give a counterexample to show that it does not. (from Poole, Linear Algebra - A Modern Introduction 2nd ed., exercise 3.5.8) I'm working on this book by myself, so if you can just give me a hint to show me the right direction, that would be great. I have been trying to solve this by going through the three requirements of a subspace. The zero vector is in S. If u and v are in S, then u + v is in S. If u is in S and c is a scalar, then c u is in S. It's easy to see the first one holds. For the third one I noticed that by adding c as multiplier on both sides of $|x-y|=|y-z|$ and fiddling a bit it becomes $|$ c $u_x$- c $u_y|$ = $|$ c $u_y$- c $u_z|$, which is the form I need to show it is closed under scalar multiplication. But how to show the second one I have no idea. Stuff I tried tended to split into multiple cases because absolute values are taken of the left and right sides, it didn't feel like the right direction. Is there something really easy I'm missing here? I also tried imagining what the subspace would look like, but it's too difficult to visualize.",,['linear-algebra']
6,Calculating determinant from LUDecomposition,Calculating determinant from LUDecomposition,,"As far as i know LU decomposition allow you to calculate matrix determinant following easy and cheap formula: Det[A] = Det[L] Det[U] = Det[U] Trying this out in Mathematica 7 gives me correct result by absolute value, i.e. it ignores negative determinants and transforms them to positive ones. Sample code: matrixA = {{-1, 2, 3, 5}, {-7, -4, 5, 4}, {-89, 7, 8, -6}, {8, 6, -1, 4}}; Det[matrixA] gives out -2067 but {lu, p, c} = LUDecomposition[matrixA]  u = lu SparseArray[{i_, j_} /; j >= i -> 1, {4, 4}] Det[u] is 2067 Well, the question is obvious - how to get correct determinant in Mathematica using LU decomposition?","As far as i know LU decomposition allow you to calculate matrix determinant following easy and cheap formula: Det[A] = Det[L] Det[U] = Det[U] Trying this out in Mathematica 7 gives me correct result by absolute value, i.e. it ignores negative determinants and transforms them to positive ones. Sample code: matrixA = {{-1, 2, 3, 5}, {-7, -4, 5, 4}, {-89, 7, 8, -6}, {8, 6, -1, 4}}; Det[matrixA] gives out -2067 but {lu, p, c} = LUDecomposition[matrixA]  u = lu SparseArray[{i_, j_} /; j >= i -> 1, {4, 4}] Det[u] is 2067 Well, the question is obvious - how to get correct determinant in Mathematica using LU decomposition?",,"['linear-algebra', 'matrices', 'mathematica']"
7,Linearity of differential forms,Linearity of differential forms,,"I put my hands on ""Linear Algebra"" by Serge Lang, second edition, and I noticed that it contains some sections that were later removed in the following third one. In one of the removed parts from that edition there was a section about tensor products and differential forms. Here, assuming that $\{e^1,e^2,e^3\}$ is a basis of $V$ and $\{dx,dy,dz\}$ is a basis of $V^*$ , it is stated [XIII, §3, p.314]: [...] By a differential form on $\mathbb R^3$ of degree $2$ , one means a map ( not necessarily linear !) $$\omega: \mathbb R^3\rightarrow V^*\land V^*$$ form $\mathbb R^3$ into the alternating product of the dual space with itself. [...] Now, I am a bit confused as I thought differential forms were linear, am I wrong? Can you make one/some examples?","I put my hands on ""Linear Algebra"" by Serge Lang, second edition, and I noticed that it contains some sections that were later removed in the following third one. In one of the removed parts from that edition there was a section about tensor products and differential forms. Here, assuming that is a basis of and is a basis of , it is stated [XIII, §3, p.314]: [...] By a differential form on of degree , one means a map ( not necessarily linear !) form into the alternating product of the dual space with itself. [...] Now, I am a bit confused as I thought differential forms were linear, am I wrong? Can you make one/some examples?","\{e^1,e^2,e^3\} V \{dx,dy,dz\} V^* \mathbb R^3 2 \omega: \mathbb R^3\rightarrow V^*\land V^* \mathbb R^3","['linear-algebra', 'differential-geometry', 'tensor-products', 'differential-forms', 'tensors']"
8,Differentiation of the Vandermonde determinant,Differentiation of the Vandermonde determinant,,"Prove that the operator $$D=\frac{\partial^2}{\partial x_1^2}+\frac{\partial^2}{\partial x_2^2}+\cdots+\frac{\partial^2}{\partial x_n^2},$$ annihilates the Vandermonde determinant. $$ V_n= \begin{vmatrix} x_1^{n-1} & x_2^{n-1} & \ldots & x_n^{n-1} \\  x_1^{n-2} & x_2^{n-2} & \ldots & x_n^{n-2} \\  \vdots & \vdots & \ldots & \vdots \\ x_1 & x_1 & \ldots & x_n \\ 1 & 1 & \ldots & 1 \end{vmatrix}. $$ Unfortunately, the operator $D$ is not a differentiation, and I cannot apply it to each row at a time, keeping other rows unchanged. By sequentially performing differentiation, I have got that the sum of $n$ determinants $$ D(V_n)=\begin{vmatrix} (n-1)(n-2)x_1^{n-3} & x_2^{n-1} & \ldots & x_n^{n-1} \\ (n-2)(n-3) x_1^{n-4} & x_2^{n-2} & \ldots & x_n^{n-2} \\  \vdots & \vdots & \ldots & \vdots \\ 2 & x_2^2 & \ldots & x_n^2 \\ 0 & x_2 & \ldots & x_n \\ 0 & 1 & \ldots & 1 \end{vmatrix}+\cdots+\begin{vmatrix} x_1^{n-1} & x_2^{n-1} & \ldots &  (n-1)(n-2)x_n^{n-1} \\  x_1^{n-4} & x_2^{n-2} & \ldots & (n-2)(n-3)x_n^{n-2} \\  \vdots & \vdots & \ldots & \vdots \\ x_1^2 & x_2^2 & \ldots & 2 \\ x_1 & x_2 & \ldots & 0 \\ 1 & 1 & \ldots & 0 \end{vmatrix} $$ But I do not yet see how to form a zero from this. Is there another way to prove it?","Prove that the operator annihilates the Vandermonde determinant. Unfortunately, the operator is not a differentiation, and I cannot apply it to each row at a time, keeping other rows unchanged. By sequentially performing differentiation, I have got that the sum of determinants But I do not yet see how to form a zero from this. Is there another way to prove it?","D=\frac{\partial^2}{\partial x_1^2}+\frac{\partial^2}{\partial x_2^2}+\cdots+\frac{\partial^2}{\partial x_n^2}, 
V_n= \begin{vmatrix} x_1^{n-1} & x_2^{n-1} & \ldots & x_n^{n-1} \\  x_1^{n-2} & x_2^{n-2} & \ldots & x_n^{n-2} \\ 
\vdots & \vdots & \ldots & \vdots \\
x_1 & x_1 & \ldots & x_n \\
1 & 1 & \ldots & 1
\end{vmatrix}.
 D n 
D(V_n)=\begin{vmatrix} (n-1)(n-2)x_1^{n-3} & x_2^{n-1} & \ldots & x_n^{n-1} \\ (n-2)(n-3) x_1^{n-4} & x_2^{n-2} & \ldots & x_n^{n-2} \\ 
\vdots & \vdots & \ldots & \vdots \\
2 & x_2^2 & \ldots & x_n^2 \\
0 & x_2 & \ldots & x_n \\
0 & 1 & \ldots & 1
\end{vmatrix}+\cdots+\begin{vmatrix} x_1^{n-1} & x_2^{n-1} & \ldots &  (n-1)(n-2)x_n^{n-1} \\  x_1^{n-4} & x_2^{n-2} & \ldots & (n-2)(n-3)x_n^{n-2} \\ 
\vdots & \vdots & \ldots & \vdots \\
x_1^2 & x_2^2 & \ldots & 2 \\
x_1 & x_2 & \ldots & 0 \\
1 & 1 & \ldots & 0
\end{vmatrix}
","['linear-algebra', 'combinatorics']"
9,Raise a Matrix to Arbitrary Power,Raise a Matrix to Arbitrary Power,,"I have a $k\times k$ matrix $$ A_{k}=   \begin{pmatrix} 1 & 1 & \cdots & 1 & 1 & 1 \\ 1 & 1 & \cdots & 1 &1 & 0\\  &\vdots & &\vdots \\ 1 & 1 & \cdots & 0 & 0 & 0\\ 1 & 0 & \cdots & 0 & 0 & 0   \end{pmatrix} $$ where $k\ge 2$ . I would like to raise this matrix to arbitrary power $n$ . Does anyone have any idea on how to proceed? The only method I know is to convert the matrix in diagonal form, i.e. find the eigenvectors. However, I don't think that this method works here, because $k$ is arbitrary. In addition, as long as $k\ge 3$ , the eigenvalues are extremely complicated. I would appreciate it if anyone can suggest some ideas! In addition, it would be also helpful if you can provide some idea in calculating $A_{k}^{n}$ , even for some small $n$ (preferably large than $5$ ). Thank you every much!","I have a matrix where . I would like to raise this matrix to arbitrary power . Does anyone have any idea on how to proceed? The only method I know is to convert the matrix in diagonal form, i.e. find the eigenvectors. However, I don't think that this method works here, because is arbitrary. In addition, as long as , the eigenvalues are extremely complicated. I would appreciate it if anyone can suggest some ideas! In addition, it would be also helpful if you can provide some idea in calculating , even for some small (preferably large than ). Thank you every much!","k\times k 
A_{k}=
  \begin{pmatrix}
1 & 1 & \cdots & 1 & 1 & 1 \\
1 & 1 & \cdots & 1 &1 & 0\\
 &\vdots & &\vdots \\
1 & 1 & \cdots & 0 & 0 & 0\\
1 & 0 & \cdots & 0 & 0 & 0
  \end{pmatrix}
 k\ge 2 n k k\ge 3 A_{k}^{n} n 5","['linear-algebra', 'matrices', 'numerical-linear-algebra', 'computational-algebra']"
10,Why do cross products not change their sign under an inversion of coordinates?,Why do cross products not change their sign under an inversion of coordinates?,,"I always understood that the direction of a cross product was determined by the right-hand rule regardless of the handedness of the coordinate system we're working in. The only way a cross product wouldn't change its sign under a coordinate inversion is if we were to assume a left-hand rule for computing cross products in a left-handed coordinate system. But if we were to do this then we would have to specifically mention that the right-hand rule only holds true in a right-handed coordinate system (which almost no one ever does). Also, this means that the directions of quantities like angular momentum depend upon the coordinate system being used which just seems wrong. Also also, if we were to use a left-hand rule for left-handed coordinate systems, $\hat{i} \times \hat{j} = \hat{k}$ even in a left-handed coordinate system which is incorrect. This question was prompted by Problem 1.10 from Griffiths and its solution .","I always understood that the direction of a cross product was determined by the right-hand rule regardless of the handedness of the coordinate system we're working in. The only way a cross product wouldn't change its sign under a coordinate inversion is if we were to assume a left-hand rule for computing cross products in a left-handed coordinate system. But if we were to do this then we would have to specifically mention that the right-hand rule only holds true in a right-handed coordinate system (which almost no one ever does). Also, this means that the directions of quantities like angular momentum depend upon the coordinate system being used which just seems wrong. Also also, if we were to use a left-hand rule for left-handed coordinate systems, even in a left-handed coordinate system which is incorrect. This question was prompted by Problem 1.10 from Griffiths and its solution .",\hat{i} \times \hat{j} = \hat{k},"['electromagnetism', 'coordinate-systems', 'linear-algebra']"
11,Basis for infinite dimensional vector space definition,Basis for infinite dimensional vector space definition,,I'm reading Luenberger's Optimization by Vector Space Methods which has the following definition: A finite set $S$ of linearly independent vectors is said to be a basis for the space $X$ if $S$ generates $X$ . A vector space having a finite basis is said to be finite dimensional . All other vector spaces are said to be infinite dimensional . Am I going crazy for thinking that this definition does not really allow for infinite dimensional spaces since $S$ is defined as a finite set? Maybe there's some subtlety I'm missing here?,I'm reading Luenberger's Optimization by Vector Space Methods which has the following definition: A finite set of linearly independent vectors is said to be a basis for the space if generates . A vector space having a finite basis is said to be finite dimensional . All other vector spaces are said to be infinite dimensional . Am I going crazy for thinking that this definition does not really allow for infinite dimensional spaces since is defined as a finite set? Maybe there's some subtlety I'm missing here?,S X S X S,"['linear-algebra', 'vector-spaces']"
12,Prove a matrix is idempotent using algebra,Prove a matrix is idempotent using algebra,,"I'd like to prove that this matrix is idempotent using a more algebraic proof for matrices with a similar definition to A, rather than deriving its eigenvalues or calculating $A^2$ . $A=$ $\begin{bmatrix}0.5&0&-0.5\\0&1&0\\ -0.5&0&0.5\end{bmatrix}$ I know that A is symmetric (so $A'=A$ ) and positive semi-definite, so how could I use these properties to prove idempotency?","I'd like to prove that this matrix is idempotent using a more algebraic proof for matrices with a similar definition to A, rather than deriving its eigenvalues or calculating . I know that A is symmetric (so ) and positive semi-definite, so how could I use these properties to prove idempotency?",A^2 A= \begin{bmatrix}0.5&0&-0.5\\0&1&0\\ -0.5&0&0.5\end{bmatrix} A'=A,"['linear-algebra', 'matrices', 'symmetric-matrices', 'idempotents']"
13,Matrix exponential weirdness in WolframAlpha - it fails on diagonal matrices?,Matrix exponential weirdness in WolframAlpha - it fails on diagonal matrices?,,"So Wikipedia says this, which makes sense: But Wolfram computed this: Is this a bug or am I just extremely tired?","So Wikipedia says this, which makes sense: But Wolfram computed this: Is this a bug or am I just extremely tired?",,"['linear-algebra', 'wolfram-alpha', 'matrix-exponential']"
14,Finding smallest vector satisfying an equation with a dot product,Finding smallest vector satisfying an equation with a dot product,,"I was wondering whether you could help me understand a solution to one of the problems from the 6.036 Introduction to Machine Learning from MIT Open Learning Library . The task is to find the smallest vector (with respect to the L2 norm) $\theta$ which satisfies the equation $$\theta \cdot x = \frac{1}{y}$$ where $x$ is an arbitrary vector (with the same dimension as $\theta$ ), y is a constant taking either value $+1$ or $-1$ and $\cdot$ is the dot product. The provided solution is: $$\theta = \frac{x}{\| x \|^2} \times \frac{1}{y}$$ Even after looking at the solution, I am still not sure how this problem was supposed to be solved. It seems that the solution could be derived using manipulations similar to: $\theta \times (x \cdot x) = x \times {1 \over y}$ $\theta \times \| x \| ^ 2 = x \times {1 \over y}$ $\theta = {x \over \| x \| ^ 2} \times {1 \over y}$ However, I struggle to see what could be the justification for transforming the original equation to the form in the first step, so I suspect this might not be the right way.","I was wondering whether you could help me understand a solution to one of the problems from the 6.036 Introduction to Machine Learning from MIT Open Learning Library . The task is to find the smallest vector (with respect to the L2 norm) which satisfies the equation where is an arbitrary vector (with the same dimension as ), y is a constant taking either value or and is the dot product. The provided solution is: Even after looking at the solution, I am still not sure how this problem was supposed to be solved. It seems that the solution could be derived using manipulations similar to: However, I struggle to see what could be the justification for transforming the original equation to the form in the first step, so I suspect this might not be the right way.",\theta \theta \cdot x = \frac{1}{y} x \theta +1 -1 \cdot \theta = \frac{x}{\| x \|^2} \times \frac{1}{y} \theta \times (x \cdot x) = x \times {1 \over y} \theta \times \| x \| ^ 2 = x \times {1 \over y} \theta = {x \over \| x \| ^ 2} \times {1 \over y},"['linear-algebra', 'vectors', 'products']"
15,"Let $U$ and $V$ be unitary matrices and $A$ a positive definite matrix , for which $AU = VA$. Show that...","Let  and  be unitary matrices and  a positive definite matrix , for which . Show that...",U V A AU = VA,"a) Show that $UA  = AV$ . b) Show that $VA^2  = A^2V$ . c) Show that $VA  = AV$ . d) Show that $U = V$ . I did a, b and d, but I'm having trouble with c. I would appreciate some help. a) $AU  = VA => (AU)^*  = (VA)^* => U^*A^* = A^*V^*$ , since A is positive definite $A = A^*$ , so $U^*A = AV^* => UU^*AV = UAV^*V$ , since $U$ and $V$ are unitary $ UU^* = I$ and $V^*V = I$ so $AV = UA$ . b) $AU  = VA => AUA = VAA$ , since $UA = AV => AAV = VAA => A^2V = VA^2$ . d) $AU  = VA$ and $VA  = AV$ so $AU = AV$ . Since A is positive definite it has an inverse, so $A^{-1}AU = A^{-1}AV => U = V$ .","a) Show that . b) Show that . c) Show that . d) Show that . I did a, b and d, but I'm having trouble with c. I would appreciate some help. a) , since A is positive definite , so , since and are unitary and so . b) , since . d) and so . Since A is positive definite it has an inverse, so .",UA  = AV VA^2  = A^2V VA  = AV U = V AU  = VA => (AU)^*  = (VA)^* => U^*A^* = A^*V^* A = A^* U^*A = AV^* => UU^*AV = UAV^*V U V  UU^* = I V^*V = I AV = UA AU  = VA => AUA = VAA UA = AV => AAV = VAA => A^2V = VA^2 AU  = VA VA  = AV AU = AV A^{-1}AU = A^{-1}AV => U = V,"['linear-algebra', 'matrices', 'positive-definite', 'unitary-matrices']"
16,$A \in M_n(\mathbb{C})$ invertible and $A^2$ is diagonalizable. Prove $A$ is diagonalizable,invertible and  is diagonalizable. Prove  is diagonalizable,A \in M_n(\mathbb{C}) A^2 A,"I'm interested to know if it's true because I saw that if $A^2$ is diagonalizable then $A$ is not necessary Diagonalizable. I have a feeling it's true but I'm not sure how to prove it. This is my proof but I'm not sure if it's good to assume that : My try By Diagonalizable we know that exist invertible matrix $P$ such that : $P^{-1}AAP=diag(\lambda_1,...,\lambda_n)$ $AA=Pdiag(\lambda_1,...,\lambda_n)P^{-1}$ Set $D=Pdiag(\sqrt{\lambda_1},...,\sqrt{\lambda_n})P^{-1}$ So : $D^2=(Pdiag(\sqrt{\lambda_1},...,\sqrt{\lambda_n})P^{-1})(Pdiag(\sqrt{\lambda_1},...,\sqrt{\lambda_n})P^{-1})=Pdiag(\lambda_1,...,\lambda_n)P^{-1}=AA$ Here I wonder - $A=D=Pdiag(\lambda_1,...,\lambda_n)P^{-1} \rightarrow A$ is diagonalizable.",I'm interested to know if it's true because I saw that if is diagonalizable then is not necessary Diagonalizable. I have a feeling it's true but I'm not sure how to prove it. This is my proof but I'm not sure if it's good to assume that : My try By Diagonalizable we know that exist invertible matrix such that : Set So : Here I wonder - is diagonalizable.,"A^2 A P P^{-1}AAP=diag(\lambda_1,...,\lambda_n) AA=Pdiag(\lambda_1,...,\lambda_n)P^{-1} D=Pdiag(\sqrt{\lambda_1},...,\sqrt{\lambda_n})P^{-1} D^2=(Pdiag(\sqrt{\lambda_1},...,\sqrt{\lambda_n})P^{-1})(Pdiag(\sqrt{\lambda_1},...,\sqrt{\lambda_n})P^{-1})=Pdiag(\lambda_1,...,\lambda_n)P^{-1}=AA A=D=Pdiag(\lambda_1,...,\lambda_n)P^{-1} \rightarrow A","['linear-algebra', 'diagonalization']"
17,Probability that $\text{det}(A)$ is an even number.,Probability that  is an even number.,\text{det}(A),"I'm struggling to solve the following problem: If a matrix $A$ is chosen randomly in the set of all $2\times 2$ matrices with coefficients in $\mathbb{Z}$ , i.e. $A\in\text{Mat}_2(\mathbb{Z})$ , what is the probability that $\text{det}(A)$ is an even number? How can I solve this problem, why is the probability a finite number, even thought I'm dealing with an infinite set. Any help is appreciated.","I'm struggling to solve the following problem: If a matrix is chosen randomly in the set of all matrices with coefficients in , i.e. , what is the probability that is an even number? How can I solve this problem, why is the probability a finite number, even thought I'm dealing with an infinite set. Any help is appreciated.",A 2\times 2 \mathbb{Z} A\in\text{Mat}_2(\mathbb{Z}) \text{det}(A),"['linear-algebra', 'probability']"
18,How do we decide whether to visualize a matrix with its rows or columns?,How do we decide whether to visualize a matrix with its rows or columns?,,"Should one visualize a matrix by its rows, columns, or both depending on the situation? I see both used and it seems arbitrary. It would be nice if only one was used consistently. Shouldn't a graph of a matrix be denoted as being a row or column representation somehow to avoid confusion? Example where author switches: https://intuitive-math.club/linear-algebra/matrices [Example I] Given the transformation: $$ \begin{bmatrix} 1 & 1\\ 2 & 0 \end{bmatrix} + \begin{bmatrix} 2 & 1\\ 1 & 1 \end{bmatrix} = \begin{bmatrix} 3 & 2\\ 3 & 1 \end{bmatrix} $$ The author represents the matrix after the transformation visually by its rows , using the following row vectors: $$ v_1 =  \begin{bmatrix} 3\\ 2 \end{bmatrix} v_2 =  \begin{bmatrix} 3\\ 1 \end{bmatrix} $$ [Example II] Given the transformation: $$ \begin{bmatrix} 0 & 1\\ -1 & 0 \end{bmatrix} \begin{bmatrix} 3 & 1\\ 1 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 1\\ -3 & 1 \end{bmatrix} $$ The author represents the matrix after the transformation visually by its columns , using the following column vectors: $$ v_1 =  \begin{bmatrix} 1\\ -3 \end{bmatrix} v_2 =  \begin{bmatrix} 1\\ -1 \end{bmatrix} $$ Question: Why is did they author seemingly arbitrarily switch from a row → column visual representation? What is the intuition behind this – if any?","Should one visualize a matrix by its rows, columns, or both depending on the situation? I see both used and it seems arbitrary. It would be nice if only one was used consistently. Shouldn't a graph of a matrix be denoted as being a row or column representation somehow to avoid confusion? Example where author switches: https://intuitive-math.club/linear-algebra/matrices [Example I] Given the transformation: The author represents the matrix after the transformation visually by its rows , using the following row vectors: [Example II] Given the transformation: The author represents the matrix after the transformation visually by its columns , using the following column vectors: Question: Why is did they author seemingly arbitrarily switch from a row → column visual representation? What is the intuition behind this – if any?","
\begin{bmatrix}
1 & 1\\
2 & 0
\end{bmatrix}
+
\begin{bmatrix}
2 & 1\\
1 & 1
\end{bmatrix}
=
\begin{bmatrix}
3 & 2\\
3 & 1
\end{bmatrix}
 
v_1 = 
\begin{bmatrix}
3\\
2
\end{bmatrix}
v_2 = 
\begin{bmatrix}
3\\
1
\end{bmatrix}
 
\begin{bmatrix}
0 & 1\\
-1 & 0
\end{bmatrix}
\begin{bmatrix}
3 & 1\\
1 & 1
\end{bmatrix}
=
\begin{bmatrix}
1 & 1\\
-3 & 1
\end{bmatrix}
 
v_1 = 
\begin{bmatrix}
1\\
-3
\end{bmatrix}
v_2 = 
\begin{bmatrix}
1\\
-1
\end{bmatrix}
","['linear-algebra', 'linear-transformations']"
19,What is the function fsolve in python doing mathematically?,What is the function fsolve in python doing mathematically?,,"In the Python documentation for fsolve it says ""Return the roots of the (non-linear) equations defined by func(x) = 0 given a starting estimate"" f(x, *args). I wondered if anyone knew the mathematical mechanics behind what fsolve is actually doing?  Thanks.","In the Python documentation for fsolve it says ""Return the roots of the (non-linear) equations defined by func(x) = 0 given a starting estimate"" f(x, *args). I wondered if anyone knew the mathematical mechanics behind what fsolve is actually doing?  Thanks.",,"['linear-algebra', 'roots', 'numerical-linear-algebra', 'nonlinear-system', 'python']"
20,Evaluating $\sum_{r=1}^{3n-1}\dfrac{(-1)^{r-1}\cdot r}{\binom{3n}r}$,Evaluating,\sum_{r=1}^{3n-1}\dfrac{(-1)^{r-1}\cdot r}{\binom{3n}r},"$$\sum_{r = 1}^{3n-1}\left(-1\right)^{r - 1}\,\,\dfrac{r}{{3n \choose r}},\quad n \in 2k,\ k\in \mathbb{Z^+}$$ Answer given (much simpler than expected) $\dfrac{3n}{3n+2}$ I tried adding and subtracting 1 to $r$ so could use $\dfrac{\binom{n}r}{r+1}=\dfrac{\binom{n+1}{r+1}}{n+1}$ , but didn't prove to be useful. I know summation and double summation of binomial coefficients to quite to good extent. If you could help...","Answer given (much simpler than expected) I tried adding and subtracting 1 to so could use , but didn't prove to be useful. I know summation and double summation of binomial coefficients to quite to good extent. If you could help...","\sum_{r = 1}^{3n-1}\left(-1\right)^{r - 1}\,\,\dfrac{r}{{3n \choose r}},\quad n \in 2k,\ k\in \mathbb{Z^+} \dfrac{3n}{3n+2} r \dfrac{\binom{n}r}{r+1}=\dfrac{\binom{n+1}{r+1}}{n+1}","['linear-algebra', 'summation', 'binomial-coefficients', 'binomial-theorem']"
21,Products of matrices in either order have the same characteristic polynomial,Products of matrices in either order have the same characteristic polynomial,,"Let $A, B$ be square matrices over $\Bbb C$ . Prove that matrices $AB$ and $BA$ have the same characteristic polynomial. I know it's a famous problem and found various answers. However, I am at my first year of math degree and my knowledge is very limited. I have never seen matrix which the entires of the matrix is matrices themselves. We never spoke in the class about limits of matrices (those the sort of solutions I saw online). So, this question is kind of ""challenge"" for us to prove with our basic linear algebra knowledge. If any one knows a solution (complicated as it may be as long as it dosent require more than the basic knowledge) it would help a lot. thank you very much","Let be square matrices over . Prove that matrices and have the same characteristic polynomial. I know it's a famous problem and found various answers. However, I am at my first year of math degree and my knowledge is very limited. I have never seen matrix which the entires of the matrix is matrices themselves. We never spoke in the class about limits of matrices (those the sort of solutions I saw online). So, this question is kind of ""challenge"" for us to prove with our basic linear algebra knowledge. If any one knows a solution (complicated as it may be as long as it dosent require more than the basic knowledge) it would help a lot. thank you very much","A, B \Bbb C AB BA","['linear-algebra', 'matrices', 'characteristic-polynomial']"
22,Rank of SO(3) and SO(4)?,Rank of SO(3) and SO(4)?,,"The rank of SO(3) is 1, the rank of SO(4) is 2. I'm trying to understand the definition of rank of a group with those two examples. The rank of a group is the cardinality of the smallest generating set. The definition from Wikipedia is given in the first sentence. (Link to wikipedia: https://en.wikipedia.org/wiki/Rank_of_a_group ) Definition of generating set: ""a generating set of a group is a subset such that every element of the group can be expressed as a combination (under the group operation) of finitely many elements of the subset and their inverses. "" In the case of SO(3), the group operation would be (matrix-)multiplication and there is no way one could express all the uncountably many rotations in the xy-plane with a finite product of matrices.","The rank of SO(3) is 1, the rank of SO(4) is 2. I'm trying to understand the definition of rank of a group with those two examples. The rank of a group is the cardinality of the smallest generating set. The definition from Wikipedia is given in the first sentence. (Link to wikipedia: https://en.wikipedia.org/wiki/Rank_of_a_group ) Definition of generating set: ""a generating set of a group is a subset such that every element of the group can be expressed as a combination (under the group operation) of finitely many elements of the subset and their inverses. "" In the case of SO(3), the group operation would be (matrix-)multiplication and there is no way one could express all the uncountably many rotations in the xy-plane with a finite product of matrices.",,"['linear-algebra', 'group-theory', 'rotations']"
23,Is this matrix surjective? Textbook dispute,Is this matrix surjective? Textbook dispute,,"\begin{bmatrix}  1 & -2 & 3 & 0 \\ 0 & 0 & 0 & 1 \\ \end{bmatrix} Is this matrix surjective? My textbook says no, but by all logic I've read it should be yes. For example, Check if $f$ is injective / surjective (matrix) says that if Rank is equal to number of rows then a matrix is surjective.","Is this matrix surjective? My textbook says no, but by all logic I've read it should be yes. For example, Check if $f$ is injective / surjective (matrix) says that if Rank is equal to number of rows then a matrix is surjective.","\begin{bmatrix} 
1 & -2 & 3 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}","['linear-algebra', 'matrices', 'linear-transformations']"
24,How can I prove mathematically the reflection matrix has only the eigenvalues 1 or -1?,How can I prove mathematically the reflection matrix has only the eigenvalues 1 or -1?,,Specifically where S is a subspace of $R^n$ . $P_S$ is the orthogonal projection onto $S$ . and the reflection matrix $ M = I - 2P_S$ I understand a similar proof where the eigenvalues of the projection matrix is either 0 or 1. Now trying to get the intuition for the reflection matrix (M) case. This is the proof for projection matrices that I have seen: $$Px =  \lambda x $$ $$P^2 = P$$ $$P^2x = \lambda x$$ $$P(Px) = \lambda x$$ $$\lambda^2x = \lambda x$$ $$\lambda(\lambda -1)x = 0$$,Specifically where S is a subspace of . is the orthogonal projection onto . and the reflection matrix I understand a similar proof where the eigenvalues of the projection matrix is either 0 or 1. Now trying to get the intuition for the reflection matrix (M) case. This is the proof for projection matrices that I have seen:,"R^n P_S S  M = I - 2P_S Px = 
\lambda x  P^2 = P P^2x = \lambda x P(Px) = \lambda x \lambda^2x = \lambda x \lambda(\lambda -1)x = 0","['linear-algebra', 'proof-writing', 'eigenvalues-eigenvectors', 'reflection']"
25,Find all real matrices such that $X^{3}-4X^{2}+5X=\begin{pmatrix} 10 & 20 \\ 5 & 10 \end{pmatrix}$,Find all real matrices such that,X^{3}-4X^{2}+5X=\begin{pmatrix} 10 & 20 \\ 5 & 10 \end{pmatrix},The following question come from the 1998 Romanian Mathematical Competition: Find all matrices in $M_2(\mathbb R)$ such that $$X^{3}-4X^{2}+5X=\begin{pmatrix} 10 & 20 \\ 5 & 10 \end{pmatrix}$$ Can you guy please help me? Thanks a lot!,The following question come from the 1998 Romanian Mathematical Competition: Find all matrices in such that Can you guy please help me? Thanks a lot!,M_2(\mathbb R) X^{3}-4X^{2}+5X=\begin{pmatrix} 10 & 20 \\ 5 & 10 \end{pmatrix},"['linear-algebra', 'matrices', 'contest-math', 'matrix-equations']"
26,Two matrices that are not similar have (almost) same eigenvalues,Two matrices that are not similar have (almost) same eigenvalues,,"I have two matrices $$ A=\begin{pmatrix}  a & 0 & 0  \\ 0 & b & 0  \\ 0 & 0 & c  \end{pmatrix} \quad \text{  and  } \quad B=\begin{pmatrix}  d & e & f  \\ d & e & f  \\ d & e & f  \end{pmatrix} $$ In reality mine are more like 1000 x 1000 matrices but the only thing that is important for now is that the left matrix is diagonal and the right one has one row that repeats itself. Obviously the eigenvalues of the left matrix are its diagonal components. I want to create a new matrix C $$C = A+B=\begin{pmatrix}  a & 0 & 0  \\0 & b & 0  \\0 & 0 & c \end{pmatrix}+\begin{pmatrix} d & e & f \\d & e & f  \\d & e & f \end{pmatrix}=\begin{pmatrix} a+d & e & f  \\d & b+e & f  \\d & e & c+f \end{pmatrix}$$ I am now wondering how the eigenvalues of this new matrix C are related to the eigenvalues of the diagonal matrix A. Can I use an argument that uses row reduction in order to relate the eigenvalues of both matrices? The reason why I am asking is that my 1000 x 1000 matrix (implemented in mathematica) that is described as above gives me almost the same eigenvalues as the corresponding diagonal matrix (only a few eigenvalues differ) and I really cannot think of any reason why that should be the case. EDIT: I implemented a simple code in mathematica to illustrate what I mean. One can see that every eigenvalue of the diagonal matrix A appears in C: dim = 50;      A = DiagonalMatrix[Flatten[RandomInteger[{0, 10}, {1, dim}]]];      mat = RandomReal[{0, 100}, {1, dim}];     B = ArrayFlatten[ConstantArray[{mat}, dim]];      c = A + B;      Abs[Eigenvalues[A]]     Round[Abs[Eigenvalues[c]], 0.01]      (*{10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 7, 7, 7, 7, 7,      6, 6, 6, 6, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2,      1, 1, 1, 0, 0, 0}*)      (*{2084.89, 10., 10., 10., 10., 10., 9.71, 9., 9., 9., 9., 9., 8.54,      8., 8., 8., 7.72, 7., 7., 7., 7., 6.61, 6., 6., 6., 5.44, 5., 5., 5.,      5., 4.29, 4., 4., 4., 3.51, 3., 3., 3., 3., 2.28, 2., 2., 2., 2.,      1.21, 1., 1., 0.33, 0., 0.}*)","I have two matrices In reality mine are more like 1000 x 1000 matrices but the only thing that is important for now is that the left matrix is diagonal and the right one has one row that repeats itself. Obviously the eigenvalues of the left matrix are its diagonal components. I want to create a new matrix C I am now wondering how the eigenvalues of this new matrix C are related to the eigenvalues of the diagonal matrix A. Can I use an argument that uses row reduction in order to relate the eigenvalues of both matrices? The reason why I am asking is that my 1000 x 1000 matrix (implemented in mathematica) that is described as above gives me almost the same eigenvalues as the corresponding diagonal matrix (only a few eigenvalues differ) and I really cannot think of any reason why that should be the case. EDIT: I implemented a simple code in mathematica to illustrate what I mean. One can see that every eigenvalue of the diagonal matrix A appears in C: dim = 50;      A = DiagonalMatrix[Flatten[RandomInteger[{0, 10}, {1, dim}]]];      mat = RandomReal[{0, 100}, {1, dim}];     B = ArrayFlatten[ConstantArray[{mat}, dim]];      c = A + B;      Abs[Eigenvalues[A]]     Round[Abs[Eigenvalues[c]], 0.01]      (*{10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 7, 7, 7, 7, 7,      6, 6, 6, 6, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2,      1, 1, 1, 0, 0, 0}*)      (*{2084.89, 10., 10., 10., 10., 10., 9.71, 9., 9., 9., 9., 9., 8.54,      8., 8., 8., 7.72, 7., 7., 7., 7., 6.61, 6., 6., 6., 5.44, 5., 5., 5.,      5., 4.29, 4., 4., 4., 3.51, 3., 3., 3., 3., 2.28, 2., 2., 2., 2.,      1.21, 1., 1., 0.33, 0., 0.}*)","
A=\begin{pmatrix} 
a & 0 & 0  \\
0 & b & 0  \\
0 & 0 & c 
\end{pmatrix}
\quad
\text{  and  }
\quad
B=\begin{pmatrix} 
d & e & f  \\
d & e & f  \\
d & e & f 
\end{pmatrix}
 C = A+B=\begin{pmatrix} 
a & 0 & 0  \\0 & b & 0  \\0 & 0 & c \end{pmatrix}+\begin{pmatrix} d & e & f \\d & e & f  \\d & e & f \end{pmatrix}=\begin{pmatrix} a+d & e & f  \\d & b+e & f  \\d & e & c+f \end{pmatrix}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
27,What are necessary and sufficient conditions such that $X^2+Y^2$ is invertible?,What are necessary and sufficient conditions such that  is invertible?,X^2+Y^2,"Let $X,Y$ be $n\times n$ matrices such that $X^3=Y^3$ and $X^2Y=Y^2X$ . What are necessary and sufficient conditions on $X$ and $Y$ such that $X^2+Y^2$ is invertible? I believe $X=Y$ is a sufficient condition, but how does one find all the necessary and sufficient conditions? I tried writing it out individually as $XXX=YYY$ etc, but just found jumbles of $X, Y, X^{-1}$ , and $Y^{-1}$ which really don't help me much. The problem is that as far as I know there is no simple way to link $\det(A+B)$ to $\det A$ and $\det B$ . However, I am not looking for a full answer, but rather for hints... I already have the answer but am choosing not to look.","Let be matrices such that and . What are necessary and sufficient conditions on and such that is invertible? I believe is a sufficient condition, but how does one find all the necessary and sufficient conditions? I tried writing it out individually as etc, but just found jumbles of , and which really don't help me much. The problem is that as far as I know there is no simple way to link to and . However, I am not looking for a full answer, but rather for hints... I already have the answer but am choosing not to look.","X,Y n\times n X^3=Y^3 X^2Y=Y^2X X Y X^2+Y^2 X=Y XXX=YYY X, Y, X^{-1} Y^{-1} \det(A+B) \det A \det B","['linear-algebra', 'determinant']"
28,The exponential of a Jordan block,The exponential of a Jordan block,,"Is it true that the exponential of a Jordan block is an upper triangular matrix? I tried two examples and got just diagonal matrices which may be a coincidence, as diagonal matrices are also upper/lower triangular.","Is it true that the exponential of a Jordan block is an upper triangular matrix? I tried two examples and got just diagonal matrices which may be a coincidence, as diagonal matrices are also upper/lower triangular.",,"['linear-algebra', 'matrices', 'jordan-normal-form', 'matrix-exponential']"
29,"What is the ""antonym of determinant"" that does $f\left(\begin{bmatrix}a & b\\c & d\end{bmatrix}\right)=ad+cb$ called?","What is the ""antonym of determinant"" that does  called?",f\left(\begin{bmatrix}a & b\\c & d\end{bmatrix}\right)=ad+cb,"I'm looking for the name of the function $f\left(\begin{bmatrix}a & b\\c & d\end{bmatrix}\right)=ad+cb$ which I had encountered a long ago on the internet. It is the same as determinant, except that (in Laplace Expansion) even numbered rows/columns don't have the negative sign. I looked at many webpages about matrix operations, but this one wasn't in any of the lists. Maybe it's because it's used rarely?","I'm looking for the name of the function which I had encountered a long ago on the internet. It is the same as determinant, except that (in Laplace Expansion) even numbered rows/columns don't have the negative sign. I looked at many webpages about matrix operations, but this one wasn't in any of the lists. Maybe it's because it's used rarely?",f\left(\begin{bmatrix}a & b\\c & d\end{bmatrix}\right)=ad+cb,"['linear-algebra', 'determinant']"
30,Is it true that $A^TA=A \implies A^2=A$? Is the converse true?,Is it true that ? Is the converse true?,A^TA=A \implies A^2=A,"I was asked this question in an exam. Let $A$ be a square matrix. $A^TA=A \implies A^2=A$ , true or false? $A^2=A \implies A^TA=A$ , true or false? I rewrote the equations as $(A^T-I)A=0$ and $(A-I)A=0$ , but I am unsure how to proceed. I also tried to consider it in terms of columns and rows, A^2=A means that the dot product of row i and column j equals $A_{ij}$ , but that doesn't get me anywhere. I know that if I assume $A$ to be symmetric, both statements are true. My hunch would be that 1 is false, 2 is true.","I was asked this question in an exam. Let be a square matrix. , true or false? , true or false? I rewrote the equations as and , but I am unsure how to proceed. I also tried to consider it in terms of columns and rows, A^2=A means that the dot product of row i and column j equals , but that doesn't get me anywhere. I know that if I assume to be symmetric, both statements are true. My hunch would be that 1 is false, 2 is true.",A A^TA=A \implies A^2=A A^2=A \implies A^TA=A (A^T-I)A=0 (A-I)A=0 A_{ij} A,"['linear-algebra', 'matrices']"
31,"$\operatorname{rank}(A) = \operatorname{rank}(B)$, Prove there exist $U, V$ invertible matrices such that: $A = UBV$",", Prove there exist  invertible matrices such that:","\operatorname{rank}(A) = \operatorname{rank}(B) U, V A = UBV","$\DeclareMathOperator{\rank}{rank}$$\DeclareMathOperator{\Mat}{Mat}$ Given two matrices $A, B \in \Mat_{m \times n}$ , as $\rank(A) = \rank(B)$ . Prove there exist two invertible matrices: $$U \in \Mat_{m \times m}, V \in \Mat_{n \times n}$$ such that: $$A = UBV$$ My attempt:  this question essentially is to prove that multiplying a matrix of the left side is equivalent to be preforming operations on the rows, and multiplying a matrix to the right side is  equivalent to be preforming operations on the columns. I don't know how to prove this - so I tried using Linear maps and to prove that using linear mapps, which was so effective - as this does not ""proves"" that for every $A, B$ with an equal rank, there exist $U, V$ so that $A = UBV$ .","Given two matrices , as . Prove there exist two invertible matrices: such that: My attempt:  this question essentially is to prove that multiplying a matrix of the left side is equivalent to be preforming operations on the rows, and multiplying a matrix to the right side is  equivalent to be preforming operations on the columns. I don't know how to prove this - so I tried using Linear maps and to prove that using linear mapps, which was so effective - as this does not ""proves"" that for every with an equal rank, there exist so that .","\DeclareMathOperator{\rank}{rank}\DeclareMathOperator{\Mat}{Mat} A, B \in \Mat_{m \times n} \rank(A) = \rank(B) U \in \Mat_{m \times m}, V \in \Mat_{n \times n} A = UBV A, B U, V A = UBV","['linear-algebra', 'matrices', 'matrix-rank']"
32,"If $f \circ f = 0 $, show that transformations $f + id_x$ and $f - id_x$ are isomorphisms of $X$","If , show that transformations  and  are isomorphisms of",f \circ f = 0  f + id_x f - id_x X,"I have a problem with this task: The linear transformation $f \in L(X,X)$ has property $f \circ f = 0 $ Show that transformations $f + id_x$ and $f - id_x$ are isomorphisms of $X$ space with itself If I need to be honest I have no idea how to prove this. I was tryinging something like that: If $f + id_x$ is isomorphism it must be both injective and surjective. Ok, but $id_x$ is injective and surjective. I thought to show that f is surjective but unfortunately sum of two surjectives can give me something what is not surjective... Maybe the key is in $f \circ f = 0 $ ? Thanks for your time!","I have a problem with this task: The linear transformation has property Show that transformations and are isomorphisms of space with itself If I need to be honest I have no idea how to prove this. I was tryinging something like that: If is isomorphism it must be both injective and surjective. Ok, but is injective and surjective. I thought to show that f is surjective but unfortunately sum of two surjectives can give me something what is not surjective... Maybe the key is in ? Thanks for your time!","f \in L(X,X) f \circ f = 0  f + id_x f - id_x X f + id_x id_x f \circ f = 0 ",['linear-algebra']
33,"If $a$, $b$ are the roots of $x^2-2x+3$.Then the equation whose roots are $a^3-3a^2+5a-2$ and $b^3-b^2+b+5$ is:","If ,  are the roots of .Then the equation whose roots are  and  is:",a b x^2-2x+3 a^3-3a^2+5a-2 b^3-b^2+b+5,"If $a$ , $b$ are the roots of $x^2-2x+3$ .Then the equation whose roots are $a^3-3a^2+5a-2$ and $b^3-b^2+b+5$ is: I have not been able to find a better method than to calculate $a$ and $b$ then substitute them into the roots for the new polynomial. I believe this question can't be transformed in a similar manner as mentioned in this question as the new roots are asymmetrical. Does a better method than the lackluster substitution, exist? The answer is: $x^2-3x+2$","If , are the roots of .Then the equation whose roots are and is: I have not been able to find a better method than to calculate and then substitute them into the roots for the new polynomial. I believe this question can't be transformed in a similar manner as mentioned in this question as the new roots are asymmetrical. Does a better method than the lackluster substitution, exist? The answer is:",a b x^2-2x+3 a^3-3a^2+5a-2 b^3-b^2+b+5 a b x^2-3x+2,[]
34,Solving independent linear equations,Solving independent linear equations,,"\begin{align} &{-}2y+2z-1=0 \tag{4} \\[4px] &{-}2x+4y-2z-2=0 \tag{5} \\[4px] &\phantom{-2}x-y+3/2=0 \tag{6} \end{align} Equation (6) is the sum of (4) and (5). There are only two independent equations.   Putting $z=0$ in (5) and (6) and solving for x and y, we have \begin{align} x&=-2 \\[4px] y&=-1/2 \end{align} equation (6) is the sum of (4) and (5): OK, I see it There are only two independent equations: I didn't get; what does this sentence mean? Putting $z=0$ in (5) and (6): why putting z=0 in equation (5) and (6)? Please help","Equation (6) is the sum of (4) and (5). There are only two independent equations.   Putting in (5) and (6) and solving for x and y, we have equation (6) is the sum of (4) and (5): OK, I see it There are only two independent equations: I didn't get; what does this sentence mean? Putting in (5) and (6): why putting z=0 in equation (5) and (6)? Please help","\begin{align}
&{-}2y+2z-1=0 \tag{4} \\[4px]
&{-}2x+4y-2z-2=0 \tag{5} \\[4px]
&\phantom{-2}x-y+3/2=0 \tag{6}
\end{align} z=0 \begin{align}
x&=-2 \\[4px]
y&=-1/2
\end{align} z=0",['linear-algebra']
35,Proof of the derivative of a quadratic form [duplicate],Proof of the derivative of a quadratic form [duplicate],,This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I was reading this pdf and on page 6 proposition 8 states: I don't really understand the steps that bring from $$\alpha = \sum_{j=1}^n\sum_{i=1}^n a_{ij} x_{i} x_{j}$$ to its derivative $$\frac{\partial \alpha}{\partial \bf{x}} = \sum_{j=1}^n a_{kj} x_J + \sum_{i=1}^n a_{ik}x_i$$ and then back to the final result: $$\frac{\partial \alpha}{\partial \bf{x}} = \bf{x}^T A^T + \bf{x}^T A$$ Can someone please help me?,This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I was reading this pdf and on page 6 proposition 8 states: I don't really understand the steps that bring from to its derivative and then back to the final result: Can someone please help me?,\alpha = \sum_{j=1}^n\sum_{i=1}^n a_{ij} x_{i} x_{j} \frac{\partial \alpha}{\partial \bf{x}} = \sum_{j=1}^n a_{kj} x_J + \sum_{i=1}^n a_{ik}x_i \frac{\partial \alpha}{\partial \bf{x}} = \bf{x}^T A^T + \bf{x}^T A,"['calculus', 'linear-algebra', 'matrices', 'derivatives', 'matrix-calculus']"
36,"What are the maximum and minimum values of $\langle u, v\rangle + \langle v, w\rangle + \langle w, u\rangle$?",What are the maximum and minimum values of ?,"\langle u, v\rangle + \langle v, w\rangle + \langle w, u\rangle","Let $V$ be an inner product space over $\mathbb{R}$.   Suppose that $u$, $v$ and $w $ are three unit vectors in the $xy$-plane.   What are the maximum and minimum values that   $$\langle u, v\rangle  + \langle v, w\rangle  + \langle w, u\rangle$$   can attain, and under what conditions? My attempt: I can say that the maximum value is  $\langle u, v\rangle  + \langle v, w\rangle  + \langle w, u\rangle=  \langle 1, 1\rangle  + \langle 1, 1\rangle  + \langle 1, 1\rangle= 3$. The minimum value  is $\langle u, v\rangle  + \langle v, w\rangle  + \langle w, u\rangle=  \langle 0, 0\rangle  + \langle 0, 0\rangle  + \langle 0, 0\rangle= 0$. Please tell me if my answers is correct or not, and help me. Thanks in advance.","Let $V$ be an inner product space over $\mathbb{R}$.   Suppose that $u$, $v$ and $w $ are three unit vectors in the $xy$-plane.   What are the maximum and minimum values that   $$\langle u, v\rangle  + \langle v, w\rangle  + \langle w, u\rangle$$   can attain, and under what conditions? My attempt: I can say that the maximum value is  $\langle u, v\rangle  + \langle v, w\rangle  + \langle w, u\rangle=  \langle 1, 1\rangle  + \langle 1, 1\rangle  + \langle 1, 1\rangle= 3$. The minimum value  is $\langle u, v\rangle  + \langle v, w\rangle  + \langle w, u\rangle=  \langle 0, 0\rangle  + \langle 0, 0\rangle  + \langle 0, 0\rangle= 0$. Please tell me if my answers is correct or not, and help me. Thanks in advance.",,"['linear-algebra', 'inequality', 'inner-products']"
37,When are $V$ and $V^{*}$ canonically isomorphic?,When are  and  canonically isomorphic?,V V^{*},"Generally speaking, for a finite dimensional vector space $V$, $V$ and $V^*$ are not canonically isomorphic after we fix a basis for $V$ . One of my questions is: If we do not fix a basis for $V$, can we say that $V$   and $V^*$ are canonical isomorphic? For example, the following is a special case. Let $L$ be a finite dimensional vector space over a field $K$, and let $V:=\text{Hom}(L,L)$. We know that $$\text{Hom}(L,L)\cong L^*\otimes L\cong L^*\otimes L^{**}\cong(L\otimes L^*)^*\cong\text{Hom}(L,L)^*,$$ and the above isomorphisms are canonical, so $V\cong V^*$ canonicaly. The other question is: Are there other vector spaces $V$ such that $V$ and $V^*$ are   canonically isomorphic?   Or, is the above the only case such that $V$ and $V^*$ are canonically isomorphic?","Generally speaking, for a finite dimensional vector space $V$, $V$ and $V^*$ are not canonically isomorphic after we fix a basis for $V$ . One of my questions is: If we do not fix a basis for $V$, can we say that $V$   and $V^*$ are canonical isomorphic? For example, the following is a special case. Let $L$ be a finite dimensional vector space over a field $K$, and let $V:=\text{Hom}(L,L)$. We know that $$\text{Hom}(L,L)\cong L^*\otimes L\cong L^*\otimes L^{**}\cong(L\otimes L^*)^*\cong\text{Hom}(L,L)^*,$$ and the above isomorphisms are canonical, so $V\cong V^*$ canonicaly. The other question is: Are there other vector spaces $V$ such that $V$ and $V^*$ are   canonically isomorphic?   Or, is the above the only case such that $V$ and $V^*$ are canonically isomorphic?",,"['linear-algebra', 'dual-spaces']"
38,How to prove that if $AB = A + B$ then $\frac{\lambda}{\lambda - 1}$ is an eigenvalue for $A$?,How to prove that if  then  is an eigenvalue for ?,AB = A + B \frac{\lambda}{\lambda - 1} A,"I'm struggling with a problem in linear algebra where I have to prove that: Given $AB = A + B$ and $ 1 \notin \sigma (A) \cup \sigma(B)$ where $\sigma$ represent the spectrum of a Matrix and $\lambda \in \sigma(B)$ then the quotient :  $\frac{\lambda}{\lambda - 1}$ is an eigenvalue for A (  $\frac{\lambda}{\lambda - 1} \in \sigma(A)$ ) I have proved before this that A is invertible iff B is invertible, it has also asked me to prove that : $$\prod\limits_{\lambda_i \in \sigma(A)}(\lambda_i - 1)  \prod\limits_{\lambda_i \in \sigma(B)}(\lambda_i - 1) = 1$$ But I couldn't find a way to do it, how can I prove both questions ?","I'm struggling with a problem in linear algebra where I have to prove that: Given $AB = A + B$ and $ 1 \notin \sigma (A) \cup \sigma(B)$ where $\sigma$ represent the spectrum of a Matrix and $\lambda \in \sigma(B)$ then the quotient :  $\frac{\lambda}{\lambda - 1}$ is an eigenvalue for A (  $\frac{\lambda}{\lambda - 1} \in \sigma(A)$ ) I have proved before this that A is invertible iff B is invertible, it has also asked me to prove that : $$\prod\limits_{\lambda_i \in \sigma(A)}(\lambda_i - 1)  \prod\limits_{\lambda_i \in \sigma(B)}(\lambda_i - 1) = 1$$ But I couldn't find a way to do it, how can I prove both questions ?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
39,How to find the eigenvalues of $A+I$ given the eigenvalues of $A$?,How to find the eigenvalues of  given the eigenvalues of ?,A+I A,"Say the $3 \times 3$ matrix $A$ has eigenvalues $a$, $b$ and $c$. Given $I$ is the $3 \times 3$ identity matrix, how to find the eigenvalues of $A+I$? I think it could be $a+1$, $b+1$ and $c+1$ but I don't have any basis at all. Thanks in advance.","Say the $3 \times 3$ matrix $A$ has eigenvalues $a$, $b$ and $c$. Given $I$ is the $3 \times 3$ identity matrix, how to find the eigenvalues of $A+I$? I think it could be $a+1$, $b+1$ and $c+1$ but I don't have any basis at all. Thanks in advance.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
40,Proof for norm of sum of vectors is less/equal to sum of norm of vectors,Proof for norm of sum of vectors is less/equal to sum of norm of vectors,,"Prove $||v+w|| \le ||v|| + ||w||$ My proof so far is: $$ ||v+w|| = \sqrt{\langle v+w,v+w\rangle}$$ $$= \sqrt{\langle v,v\rangle +2\langle v,w\rangle +\langle w,w\rangle }$$ So, $||v+w||^2$ $$=||v||^2 +2\langle v,w\rangle + ||w||^2$$ When I squared and expanded the right side, I got $$(||v||+||w||)^2=||v||^2 +2||v||\ ||w||+||w||^2$$ Which means I need to prove that $\langle v,w \rangle \le ||v||\ ||w||$. I'm not sure how to prove this. Is there a different answer to the question that sidesteps this? How can I go about proving that statement?","Prove $||v+w|| \le ||v|| + ||w||$ My proof so far is: $$ ||v+w|| = \sqrt{\langle v+w,v+w\rangle}$$ $$= \sqrt{\langle v,v\rangle +2\langle v,w\rangle +\langle w,w\rangle }$$ So, $||v+w||^2$ $$=||v||^2 +2\langle v,w\rangle + ||w||^2$$ When I squared and expanded the right side, I got $$(||v||+||w||)^2=||v||^2 +2||v||\ ||w||+||w||^2$$ Which means I need to prove that $\langle v,w \rangle \le ||v||\ ||w||$. I'm not sure how to prove this. Is there a different answer to the question that sidesteps this? How can I go about proving that statement?",,"['linear-algebra', 'proof-verification']"
41,Is the system of linear equations always solvable if the matrix is of full rank,Is the system of linear equations always solvable if the matrix is of full rank,,"I know that for an $n×n$ matrix $A$, the system $Ax=b$ has a unique solution for all $b$ in $\mathbb{R}^{n}$.  My question is : what will happen when $A$ is an $m\times n$ matrix?Does the system is solvable for every $b$ if rank of $A$ is $n$?","I know that for an $n×n$ matrix $A$, the system $Ax=b$ has a unique solution for all $b$ in $\mathbb{R}^{n}$.  My question is : what will happen when $A$ is an $m\times n$ matrix?Does the system is solvable for every $b$ if rank of $A$ is $n$?",,['linear-algebra']
42,Computing the Moore-Penrose pseudoinverse of a $2 \times 2$ matrix,Computing the Moore-Penrose pseudoinverse of a  matrix,2 \times 2,"I am facing difficulties in calculating the Moore-Pensore pseudoinverse of a positive semidefinite matrix $A$, where $A$ is self-adjoint and $\langle A u, u \rangle \geq 0$ for all $u \in \mathcal{H}$, where $\mathcal{H}$ is a complex Hilbert space. For example, $$A = \begin{bmatrix} 1&-1\\ -1&1\end{bmatrix}$$ is a positive semidefinite matrix. How to calculate the Moore-Penrose pseudoinverse of $A$?","I am facing difficulties in calculating the Moore-Pensore pseudoinverse of a positive semidefinite matrix $A$, where $A$ is self-adjoint and $\langle A u, u \rangle \geq 0$ for all $u \in \mathcal{H}$, where $\mathcal{H}$ is a complex Hilbert space. For example, $$A = \begin{bmatrix} 1&-1\\ -1&1\end{bmatrix}$$ is a positive semidefinite matrix. How to calculate the Moore-Penrose pseudoinverse of $A$?",,"['linear-algebra', 'matrices', 'pseudoinverse']"
43,Find the determinant of the linear transformation $L(A)=A^T$ from $\mathbb{R}^{n\times n}$ to $\mathbb{R}^{n\times n}$,Find the determinant of the linear transformation  from  to,L(A)=A^T \mathbb{R}^{n\times n} \mathbb{R}^{n\times n},"Find the determinant of the linear transformation $L(A)=A^T$ from $\mathbb{R}^{n\times n}$ to $\mathbb{R}^{n\times n}$. The solution is $(-1)^{\frac{n(n-1)}{2}}$. I've found some resources to do this when for the case from $\mathbb{R}^{2\times 2}$ to $\mathbb{R}^{2\times 2}$ (in fact from stackexchange ), but in the $n$ case, I'm really confused about the formula. Using the same logic as the 2x2 case, I get that in an equivalent mapping of $L:\mathbb{R}^{n^2} \rightarrow \mathbb{R}^{n^2}$ is: $L(e_{11},e_{12},e_{nn})=\begin{bmatrix}     1 & 0 & 0 & \dots  & 0& 0 \\     0 & 0 & 0 & \dots  & 1& 0 \\     \vdots & \vdots & \vdots & \ddots & \vdots& \vdots \\ 0 & 1 & 0 & \dots  & 0& 0 \\     0 & 0 & 0 & \dots  & 0& 1 \end{bmatrix} \begin{bmatrix}     e_{11} \\     e_{21} \\     \vdots\\ \vdots \\     e_{nn} \end{bmatrix}$ Where we can find the determinant of the large $0$ and $1$ matrix above (let's call it matrix B) to find the determinant of the linear transformation L. I know that $det(B)=(-1)^s$ where $s=$the number of swaps made to get B into rref form. However, I just can't see how we need $\frac{n(n-1)}{2}$ swaps and not $n$ swaps?","Find the determinant of the linear transformation $L(A)=A^T$ from $\mathbb{R}^{n\times n}$ to $\mathbb{R}^{n\times n}$. The solution is $(-1)^{\frac{n(n-1)}{2}}$. I've found some resources to do this when for the case from $\mathbb{R}^{2\times 2}$ to $\mathbb{R}^{2\times 2}$ (in fact from stackexchange ), but in the $n$ case, I'm really confused about the formula. Using the same logic as the 2x2 case, I get that in an equivalent mapping of $L:\mathbb{R}^{n^2} \rightarrow \mathbb{R}^{n^2}$ is: $L(e_{11},e_{12},e_{nn})=\begin{bmatrix}     1 & 0 & 0 & \dots  & 0& 0 \\     0 & 0 & 0 & \dots  & 1& 0 \\     \vdots & \vdots & \vdots & \ddots & \vdots& \vdots \\ 0 & 1 & 0 & \dots  & 0& 0 \\     0 & 0 & 0 & \dots  & 0& 1 \end{bmatrix} \begin{bmatrix}     e_{11} \\     e_{21} \\     \vdots\\ \vdots \\     e_{nn} \end{bmatrix}$ Where we can find the determinant of the large $0$ and $1$ matrix above (let's call it matrix B) to find the determinant of the linear transformation L. I know that $det(B)=(-1)^s$ where $s=$the number of swaps made to get B into rref form. However, I just can't see how we need $\frac{n(n-1)}{2}$ swaps and not $n$ swaps?",,['linear-algebra']
44,"$A\in M_{3\times3} $ has eigenvalues $\lambda=1,-1. \Rightarrow A^3=A$?",has eigenvalues ?,"A\in M_{3\times3}  \lambda=1,-1. \Rightarrow A^3=A","My first intuition was that the statement in the title is false, however I have failed to find a counterexample. Iv'e managed to show this is true if $A$ is diagonalizable but cannot see why this is necessarily true. Thanks. Clarification: $\lambda=1,-1$ are the only eigenvalues","My first intuition was that the statement in the title is false, however I have failed to find a counterexample. Iv'e managed to show this is true if $A$ is diagonalizable but cannot see why this is necessarily true. Thanks. Clarification: $\lambda=1,-1$ are the only eigenvalues",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
45,Tangent Space of the Heisenberg Group,Tangent Space of the Heisenberg Group,,"Let H$_{3}$(R) denote the Heisenberg group of 3 x 3 real matrices. For any A $\in$ H$_{3}$(R) such that A = $\left( \begin{array}{ccc} 1 & a & b \\ 0 & 1 & c \\ 0 & 0 & 1 \end{array} \right)$ ,  $\exists$ a continuous path A(t), 0 $\leq$ t $\leq$ 1, from I to A given by A(t) = $ \left( \begin{array}{ccc} 1 & a(t) & b(t) \\ 0 & 1 & c(t) \\ 0 & 0 & 1 \end{array} \right)$ . Let v be the tangent vector of A(t) at identity (t=0), then v= $\frac{d}{dt}$ A(t)|$_{t=o}$. Then shouldn't v = A'(0) = $ \left( \begin{array}{ccc} 1 & \frac {d}{dt}0 & \frac{d}{dt}0 \\ 0 & 1 & \frac{d}{dt}0 \\ 0 & 0 & 1 \end{array} \right)$ = $ \left( \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right)$ ? If this is correct, then is the tangent space of H$_{3}$(R) the standard, orthonormal basis of R$^3$? (Apologies if this is horribly fallacious, I'm new to lie algebra)","Let H$_{3}$(R) denote the Heisenberg group of 3 x 3 real matrices. For any A $\in$ H$_{3}$(R) such that A = $\left( \begin{array}{ccc} 1 & a & b \\ 0 & 1 & c \\ 0 & 0 & 1 \end{array} \right)$ ,  $\exists$ a continuous path A(t), 0 $\leq$ t $\leq$ 1, from I to A given by A(t) = $ \left( \begin{array}{ccc} 1 & a(t) & b(t) \\ 0 & 1 & c(t) \\ 0 & 0 & 1 \end{array} \right)$ . Let v be the tangent vector of A(t) at identity (t=0), then v= $\frac{d}{dt}$ A(t)|$_{t=o}$. Then shouldn't v = A'(0) = $ \left( \begin{array}{ccc} 1 & \frac {d}{dt}0 & \frac{d}{dt}0 \\ 0 & 1 & \frac{d}{dt}0 \\ 0 & 0 & 1 \end{array} \right)$ = $ \left( \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right)$ ? If this is correct, then is the tangent space of H$_{3}$(R) the standard, orthonormal basis of R$^3$? (Apologies if this is horribly fallacious, I'm new to lie algebra)",,"['linear-algebra', 'differential-geometry']"
46,Finding the determinant of a skew-symmetric matrix $K$,Finding the determinant of a skew-symmetric matrix,K,"Find the determinant of the skew-symmetric matrix $K$ $$K = \begin{bmatrix} 0 & 1 & 3\\ -1 & 0 & 4 \\ -3 & -4 & 0 \\ \end{bmatrix}$$ My Attempted Solution: I performed the following row operations to reduce $K$ into upper-triangular form $U$ $R_2 \leftrightarrow R_1$ $R_3 - (l_{31} = 3)R_1$ $R_3 \leftrightarrow R_2$ $R_3 - (l_{32} = -4) R_2$ $$U = \begin{bmatrix} -1 & 0 & 4 \\ 0 & -4 & -12 \\ 0 & 0 & -45 \end{bmatrix}$$ From this I got  $$\begin{align} \det(K) &= \pm \  \det(U) \\ &= + \det(U) & \text{(Even no. of row exchanges)} \\ & = (-1)(-4)(-45) \\ &= 180 \end{align}$$ However the correct answer is $\det(K) = 0$. What could I have done wrong, I wouldn't think it would've been the row operations as the row operations apart from the row exchanges don't affect the $\det(K)$? Any hints or suggestiong are greatly appreciated","Find the determinant of the skew-symmetric matrix $K$ $$K = \begin{bmatrix} 0 & 1 & 3\\ -1 & 0 & 4 \\ -3 & -4 & 0 \\ \end{bmatrix}$$ My Attempted Solution: I performed the following row operations to reduce $K$ into upper-triangular form $U$ $R_2 \leftrightarrow R_1$ $R_3 - (l_{31} = 3)R_1$ $R_3 \leftrightarrow R_2$ $R_3 - (l_{32} = -4) R_2$ $$U = \begin{bmatrix} -1 & 0 & 4 \\ 0 & -4 & -12 \\ 0 & 0 & -45 \end{bmatrix}$$ From this I got  $$\begin{align} \det(K) &= \pm \  \det(U) \\ &= + \det(U) & \text{(Even no. of row exchanges)} \\ & = (-1)(-4)(-45) \\ &= 180 \end{align}$$ However the correct answer is $\det(K) = 0$. What could I have done wrong, I wouldn't think it would've been the row operations as the row operations apart from the row exchanges don't affect the $\det(K)$? Any hints or suggestiong are greatly appreciated",,"['linear-algebra', 'matrices', 'determinant']"
47,How to determine the matrix in the following case [duplicate],How to determine the matrix in the following case [duplicate],,"This question already has an answer here : Solving for $A$ in $Ax = b$ (1 answer) Closed 7 years ago . Say we have a vector $\textbf{b}$ and $\textbf u$ such that:  $$A \mathbf b= \mathbf u$$ Where $A$ is a square matrix. If $\mathbf b$ and $\mathbf u$ are known and $A$ is the unknown, How to get the matrix $A$  (perhaps it is not unique but how can one proceed to get it)?","This question already has an answer here : Solving for $A$ in $Ax = b$ (1 answer) Closed 7 years ago . Say we have a vector $\textbf{b}$ and $\textbf u$ such that:  $$A \mathbf b= \mathbf u$$ Where $A$ is a square matrix. If $\mathbf b$ and $\mathbf u$ are known and $A$ is the unknown, How to get the matrix $A$  (perhaps it is not unique but how can one proceed to get it)?",,"['linear-algebra', 'matrices']"
48,Let $A$ be a real $4\times 4$ matrix with characteristic polynomial $(x^2+1)^2$. Which of the following is true?,Let  be a real  matrix with characteristic polynomial . Which of the following is true?,A 4\times 4 (x^2+1)^2,Let $A \in \mathbb R^{4\times 4}$ have characteristic polynomial $(x^2+1)^2$. Which of the following is true? $A$ is diagonalizable over $\mathbb C$ but not over $\mathbb R$ $A$ is nilpotent $A$ is invertible There is no such matrix $A$.,Let $A \in \mathbb R^{4\times 4}$ have characteristic polynomial $(x^2+1)^2$. Which of the following is true? $A$ is diagonalizable over $\mathbb C$ but not over $\mathbb R$ $A$ is nilpotent $A$ is invertible There is no such matrix $A$.,,"['linear-algebra', 'matrices']"
49,Dual space of polynomial algebra,Dual space of polynomial algebra,,"Let $k$ be an infinite field and let's consider the ring $R=k[x_1,\dots,x_n]$. This ring has a structure of $k$-vector space (or a $k$-algebra). I am interested to know about the structure of the vector space $R^*$. It would be enough to know about the structure of the vector space $k[x]^*$ since $R\cong k[x]^{\otimes n}=\stackrel{n\text{ times}}{k[x]\otimes\cdots\otimes k[x]}$ (this, as a $k$-algebra) and $(V\otimes W)^*\cong V^*\otimes W^*$ as here (as a vector space). So these are my questions: Is there any known vector space which is isomorphic to $k[x]^*$? If $A$ is a $k$-algebra, does this extra structure induce a $k$-algebra structure over the dual space $A^*$? If 2. is true, does this structure preserve the property $(V\otimes W)^*\cong V^*\otimes W^*$? While I was writing I found this . This answers 1: $k[x]^*\cong k[[x]]$. About 2, the homomorphism defined there doesn't seem to be an algebra homomorphism (if I am not mistaken $$\sum_{i=0}^\infty f(x^i)g(x^i)x^i\neq \sum_{i=0}^\infty \sum_{n=0}^i f(x^n)g(x^{i-n})x^i$$ in general) so it just preserves the structure as a vector space. So Is there any way to save 2. and/or 3.? And last: What can we say about the structure of $k[[x]]^{\otimes n}$? Note: I am not sure if the notation $R^{\otimes n}$ is correct, I just thought it would be shorter that way.","Let $k$ be an infinite field and let's consider the ring $R=k[x_1,\dots,x_n]$. This ring has a structure of $k$-vector space (or a $k$-algebra). I am interested to know about the structure of the vector space $R^*$. It would be enough to know about the structure of the vector space $k[x]^*$ since $R\cong k[x]^{\otimes n}=\stackrel{n\text{ times}}{k[x]\otimes\cdots\otimes k[x]}$ (this, as a $k$-algebra) and $(V\otimes W)^*\cong V^*\otimes W^*$ as here (as a vector space). So these are my questions: Is there any known vector space which is isomorphic to $k[x]^*$? If $A$ is a $k$-algebra, does this extra structure induce a $k$-algebra structure over the dual space $A^*$? If 2. is true, does this structure preserve the property $(V\otimes W)^*\cong V^*\otimes W^*$? While I was writing I found this . This answers 1: $k[x]^*\cong k[[x]]$. About 2, the homomorphism defined there doesn't seem to be an algebra homomorphism (if I am not mistaken $$\sum_{i=0}^\infty f(x^i)g(x^i)x^i\neq \sum_{i=0}^\infty \sum_{n=0}^i f(x^n)g(x^{i-n})x^i$$ in general) so it just preserves the structure as a vector space. So Is there any way to save 2. and/or 3.? And last: What can we say about the structure of $k[[x]]^{\otimes n}$? Note: I am not sure if the notation $R^{\otimes n}$ is correct, I just thought it would be shorter that way.",,"['linear-algebra', 'abstract-algebra', 'tensor-products']"
50,what is the difference between cross product and exterior product?,what is the difference between cross product and exterior product?,,"I have learn that the exterior product is an oriented plane called bivector given as  $A \times B = |A||B| \sin x (i \times j)$ For $x \in(-\pi,\pi)$. I will like someone to derive the cross product and the exterior product with a geometry picture to show the difference.","I have learn that the exterior product is an oriented plane called bivector given as  $A \times B = |A||B| \sin x (i \times j)$ For $x \in(-\pi,\pi)$. I will like someone to derive the cross product and the exterior product with a geometry picture to show the difference.",,"['linear-algebra', 'cross-product']"
51,"Without calculating $A^4$ prove that $A^4\in Span\{A,I\}.$",Without calculating  prove that,"A^4 A^4\in Span\{A,I\}.","Let  $$A=         \begin{bmatrix}         -1 & 6 & -9 \\         -11 & 24 & -33 \\         -6 & 12 & -16 \\         \end{bmatrix} $$ a) Without calculating $A^4$ prove that $A^4\in Span\{A,I\}.$ b) Write $A^n$ in a form of $a_nA+b_nI$ If matrix $A^4\in Span\{A,I\}$ then $A^4=\alpha A+ \beta B$. I had an idea to find eigenvalues and eigenvectors of $A$, so I can diagonalize the matrix and say that $A^4=SD^4S^{-1}$. However, I am not sure is that done without calculating $A^4$. Thank you all in advance.","Let  $$A=         \begin{bmatrix}         -1 & 6 & -9 \\         -11 & 24 & -33 \\         -6 & 12 & -16 \\         \end{bmatrix} $$ a) Without calculating $A^4$ prove that $A^4\in Span\{A,I\}.$ b) Write $A^n$ in a form of $a_nA+b_nI$ If matrix $A^4\in Span\{A,I\}$ then $A^4=\alpha A+ \beta B$. I had an idea to find eigenvalues and eigenvectors of $A$, so I can diagonalize the matrix and say that $A^4=SD^4S^{-1}$. However, I am not sure is that done without calculating $A^4$. Thank you all in advance.",,"['linear-algebra', 'matrices']"
52,Solving an equation with logarithms: $x^{\log_2(\sqrt{x})-1} = \sqrt{8}$,Solving an equation with logarithms:,x^{\log_2(\sqrt{x})-1} = \sqrt{8},The equation I'm given is $$\large x^{\log_2(\sqrt{x})-1} = \sqrt{8}$$ I've tried on solving it and my best try is on the photo. Got stuck there and not sure how to proceed any further.,The equation I'm given is $$\large x^{\log_2(\sqrt{x})-1} = \sqrt{8}$$ I've tried on solving it and my best try is on the photo. Got stuck there and not sure how to proceed any further.,,['linear-algebra']
53,why adding or subtracting linear equations finds their intersection point?,why adding or subtracting linear equations finds their intersection point?,,"I was trying to understand the logic behind linear algebra but got stuck at this point. Why does adding or subtracting two linear equations with one another result in their intersection point ? After searching i came across a method which goes like this . Lets say we have two equations x+y=5 and 2x+y=8. Now they do this  y=5-x y=8-2x and then 5-x=8-2x and then find the value of x. I understood the logic behind this method. But what about the other method where they just subtract those equations , and find x value first and then find y value . What is going on there ? What is the logic behind it ?","I was trying to understand the logic behind linear algebra but got stuck at this point. Why does adding or subtracting two linear equations with one another result in their intersection point ? After searching i came across a method which goes like this . Lets say we have two equations x+y=5 and 2x+y=8. Now they do this  y=5-x y=8-2x and then 5-x=8-2x and then find the value of x. I understood the logic behind this method. But what about the other method where they just subtract those equations , and find x value first and then find y value . What is going on there ? What is the logic behind it ?",,['linear-algebra']
54,Metric tensor second derivative in terms of riemann curvature tensor NOT in a local coordinate system,Metric tensor second derivative in terms of riemann curvature tensor NOT in a local coordinate system,,"I know that the Riemann curvature tensor, in a general coordinate system is: $$ R_{abcd}=\frac{1}{2}(g_{ad,bc}+g_{bc,ad}-g_{ac,bd}-g_{bd,ac})+(...) $$ The dots mean lower derivatives of $g$. I am interested in getting the second derivative of the metric tensor in terms of the Riemann curvature tensor. I don't care about the terms in place of the dots or how they combine in the final result, or if they appear or not.I'd like something like this: $$ g_{ab,cd}=R_{abcd}....+(...) $$ Is this is even possible? Thank you in advance.","I know that the Riemann curvature tensor, in a general coordinate system is: $$ R_{abcd}=\frac{1}{2}(g_{ad,bc}+g_{bc,ad}-g_{ac,bd}-g_{bd,ac})+(...) $$ The dots mean lower derivatives of $g$. I am interested in getting the second derivative of the metric tensor in terms of the Riemann curvature tensor. I don't care about the terms in place of the dots or how they combine in the final result, or if they appear or not.I'd like something like this: $$ g_{ab,cd}=R_{abcd}....+(...) $$ Is this is even possible? Thank you in advance.",,"['linear-algebra', 'geometry', 'ordinary-differential-equations', 'differential-geometry', 'riemannian-geometry']"
55,"Simpler definition for kernel, image, and rank of transformation matrix?","Simpler definition for kernel, image, and rank of transformation matrix?",,"I'm a beginner at linear algebra and I keep running into such complicated definitions for kernel, image, and rank of a matrix and I would appreciate it if someone could simplify the definitions more into English rather than math symbols. I generally know how to find these qualities but...what am I finding? Thanks in advance!","I'm a beginner at linear algebra and I keep running into such complicated definitions for kernel, image, and rank of a matrix and I would appreciate it if someone could simplify the definitions more into English rather than math symbols. I generally know how to find these qualities but...what am I finding? Thanks in advance!",,"['linear-algebra', 'matrices', 'definition', 'matrix-rank']"
56,Product of nilpotent matrices invertible,Product of nilpotent matrices invertible,,"Do there exist a positive integer $n$ and two nilpotent $n\times n$ matrices $A$ and $B$ such that  $AB$ is invertible? If yes, can you give me an example, if no, can you prove why?","Do there exist a positive integer $n$ and two nilpotent $n\times n$ matrices $A$ and $B$ such that  $AB$ is invertible? If yes, can you give me an example, if no, can you prove why?",,['linear-algebra']
57,$\mathbb{C}$-algebra automorphism of $M_n(\mathbb{C})$ has form $X \mapsto AXA^{-1}$.,-algebra automorphism of  has form .,\mathbb{C} M_n(\mathbb{C}) X \mapsto AXA^{-1},"As the title suggests, what is the easiest way to see that any $\mathbb{C}$-algebra automorphism of $M_n(\mathbb{C})$ has the form $X \mapsto AXA^{-1}$ for some fixed $A \in GL_n(\mathbb{C})$?","As the title suggests, what is the easiest way to see that any $\mathbb{C}$-algebra automorphism of $M_n(\mathbb{C})$ has the form $X \mapsto AXA^{-1}$ for some fixed $A \in GL_n(\mathbb{C})$?",,['linear-algebra']
58,Can a nonsingular square matrix be made singular by changing exactly one element or vice versa?,Can a nonsingular square matrix be made singular by changing exactly one element or vice versa?,,"Given a nonsingular square  matrix $A$ , can changing just one element  make it singular? Given a singular square  matrix $A$ , can changing just one element  make it nonsingular? For $1$ )  I  was  thinking  that  the  reduced  row  echelon  form  of $A$ ,say $E$ ( $E$ = $E_{1}E_{2}E_{3}.....E_{n}A$ ), must  be  identity  so  changing  just  one $1$ to $0$ would make  it  singular $\bar E$ . But  then , does  that  mean  alteration  in  just one  element  in  the  original  matrix  when the  same  row  operations  are  inverted  on  the  new  matrix  i.e ${E_{n}}^{-1}.....{E_{1}}^{-1}\bar E = \bar A$ is singular  but  do $A$ and $\bar A$ differ  by  only  one  element? For $2$ )  May  be  wrong  because  in this  case  one  or  more  than  one  rows  are zero  rows so  putting  one  pivotal $1$ in  each  row  will  make it non-singular and  reversing  the  row  operations  will  give a non-singular  matrix.But  the  same  problem  here .  How many  of  original  elements  are  altered? If  the  RRE  form  had more  than $1$ zero  rows  then  I  guess  it  is not  just $1$ element. Am  I  going  in   the  right  direction  or   totally  messed  up? Thanks   for  the  answers @Servaes  and  @Dustan Lavenstein. Those  were  really  helpful.  Can  anybody  please  throw  some  light  on  the  process  I  was trying ? Can  the  answer  be  obtained  in  that  way, especially  for  the  first  question?  Thanks.","Given a nonsingular square  matrix , can changing just one element  make it singular? Given a singular square  matrix , can changing just one element  make it nonsingular? For )  I  was  thinking  that  the  reduced  row  echelon  form  of ,say ( = ), must  be  identity  so  changing  just  one to would make  it  singular . But  then , does  that  mean  alteration  in  just one  element  in  the  original  matrix  when the  same  row  operations  are  inverted  on  the  new  matrix  i.e is singular  but  do and differ  by  only  one  element? For )  May  be  wrong  because  in this  case  one  or  more  than  one  rows  are zero  rows so  putting  one  pivotal in  each  row  will  make it non-singular and  reversing  the  row  operations  will  give a non-singular  matrix.But  the  same  problem  here .  How many  of  original  elements  are  altered? If  the  RRE  form  had more  than zero  rows  then  I  guess  it  is not  just element. Am  I  going  in   the  right  direction  or   totally  messed  up? Thanks   for  the  answers @Servaes  and  @Dustan Lavenstein. Those  were  really  helpful.  Can  anybody  please  throw  some  light  on  the  process  I  was trying ? Can  the  answer  be  obtained  in  that  way, especially  for  the  first  question?  Thanks.",A A 1 A E E E_{1}E_{2}E_{3}.....E_{n}A 1 0 \bar E {E_{n}}^{-1}.....{E_{1}}^{-1}\bar E = \bar A A \bar A 2 1 1 1,"['linear-algebra', 'matrices', 'gaussian-elimination', 'singular-values']"
59,How to find eigenvalues of matrix $\begin{bmatrix} 3& a+1\\a+1&3 \end{bmatrix}$,How to find eigenvalues of matrix,\begin{bmatrix} 3& a+1\\a+1&3 \end{bmatrix},I want to find the eigenvalues of the following matrix $\begin{bmatrix} 3& a+1\\a+1&3 \end{bmatrix}$ expressed in $a$ using $\begin{bmatrix} \lambda - 3& a+1\\a+1&\lambda-3 \end{bmatrix}$. But the $a$-term makes it difficult for me to find it. I hope someone can show me how to do this. Thanks in advance ! EDIT: Determinant = $4a^2+8a+4$ Using the abc rule I get $(6\overset{+}{-} \sqrt{4a^2+8a+4})/2$,I want to find the eigenvalues of the following matrix $\begin{bmatrix} 3& a+1\\a+1&3 \end{bmatrix}$ expressed in $a$ using $\begin{bmatrix} \lambda - 3& a+1\\a+1&\lambda-3 \end{bmatrix}$. But the $a$-term makes it difficult for me to find it. I hope someone can show me how to do this. Thanks in advance ! EDIT: Determinant = $4a^2+8a+4$ Using the abc rule I get $(6\overset{+}{-} \sqrt{4a^2+8a+4})/2$,,"['linear-algebra', 'eigenvalues-eigenvectors']"
60,Is this a circulant matrix?,Is this a circulant matrix?,,"It's symmetric, but I'm not sure whether it is circulant.  In a question that I had asked on MSE a couple of weeks ago, several commenters had said that this is a circulant matrix, and to study the explicit formulas to find its eigenvalues and eigenvectors. However, I was reading up on circulant matrices again last night and noticed that the ""constant diagonals"" are actually from left to right, at least according to Wikipedia -- although the page does mention that circulant matrices can be defined in other ways, with different shifts in direction. This matrix has constant diagonal, but from right to left .  Is this still a circulant matrix, and hence the formulas to compute its eigenvalues and eigenvectors are still the same? Thanks, $$ A=         \begin{bmatrix}         a & b & c \\         b & c & a \\         c & a & b \\         \end{bmatrix} $$","It's symmetric, but I'm not sure whether it is circulant.  In a question that I had asked on MSE a couple of weeks ago, several commenters had said that this is a circulant matrix, and to study the explicit formulas to find its eigenvalues and eigenvectors. However, I was reading up on circulant matrices again last night and noticed that the ""constant diagonals"" are actually from left to right, at least according to Wikipedia -- although the page does mention that circulant matrices can be defined in other ways, with different shifts in direction. This matrix has constant diagonal, but from right to left .  Is this still a circulant matrix, and hence the formulas to compute its eigenvalues and eigenvectors are still the same? Thanks, $$ A=         \begin{bmatrix}         a & b & c \\         b & c & a \\         c & a & b \\         \end{bmatrix} $$",,"['linear-algebra', 'matrices', 'numerical-methods', 'fourier-analysis', 'numerical-linear-algebra']"
61,Why do we treat dot product like a square?,Why do we treat dot product like a square?,,"I am trying to figure out the reason for this line of deduction ( It is a proof for Householder's transformation on some vector)  $$\|\mathbf{v}-\mathbf{u}\|^2= \langle \mathbf{v},\mathbf{v}\rangle- \langle \mathbf{v},\mathbf{u} \rangle-\langle \mathbf{u},\mathbf{v} \rangle+\langle \mathbf{u},\mathbf{u} \rangle = -2\langle\mathbf{v}-\mathbf{u},\mathbf{u} \rangle $$ How did they deduce the first and the second equality? Do we use a geometrical aid to help us think about this?","I am trying to figure out the reason for this line of deduction ( It is a proof for Householder's transformation on some vector)  $$\|\mathbf{v}-\mathbf{u}\|^2= \langle \mathbf{v},\mathbf{v}\rangle- \langle \mathbf{v},\mathbf{u} \rangle-\langle \mathbf{u},\mathbf{v} \rangle+\langle \mathbf{u},\mathbf{u} \rangle = -2\langle\mathbf{v}-\mathbf{u},\mathbf{u} \rangle $$ How did they deduce the first and the second equality? Do we use a geometrical aid to help us think about this?",,['linear-algebra']
62,Find all constants where a matrix is symmetric,Find all constants where a matrix is symmetric,,"I have a matrix like below: $$M = \begin{bmatrix} 2 && a-2b+c && 2a+b+c \\ 3 && 5 && -2 \\ 0 && a+c && 7\end{bmatrix}$$ In order for the matrix to be symmetric, the following constraints on $a$, $b$, and $c$ must hold: $$a-2b+c = 3 \\ 2a+b+c = 0 \\ a+c = -2$$ How would I find the domain of $a$, $b$, and $c$? The set of equations above can form a matrix, and solving the system would give an individual value for each, but this gives a lone example of values that work, how would I find the range? Am I reading too much into the question? Are the set of equations I've provided the ranges?","I have a matrix like below: $$M = \begin{bmatrix} 2 && a-2b+c && 2a+b+c \\ 3 && 5 && -2 \\ 0 && a+c && 7\end{bmatrix}$$ In order for the matrix to be symmetric, the following constraints on $a$, $b$, and $c$ must hold: $$a-2b+c = 3 \\ 2a+b+c = 0 \\ a+c = -2$$ How would I find the domain of $a$, $b$, and $c$? The set of equations above can form a matrix, and solving the system would give an individual value for each, but this gives a lone example of values that work, how would I find the range? Am I reading too much into the question? Are the set of equations I've provided the ranges?",,"['linear-algebra', 'matrices']"
63,Linear dependence of these functions?,Linear dependence of these functions?,,"How can I check if these three functions (which belong to vector space $R^R$) are linearly dependent: $$e^{2x}, e^{3x}, x$$ If I take $\alpha, \beta, \gamma ∈ R$ and write the linear combination as: $$\alpha e^{2x}+\beta e^{3x}+\gamma x = 0$$ How can I know if the statement is only correct if all $\alpha, \beta$ and $\gamma$ are $zero$?","How can I check if these three functions (which belong to vector space $R^R$) are linearly dependent: $$e^{2x}, e^{3x}, x$$ If I take $\alpha, \beta, \gamma ∈ R$ and write the linear combination as: $$\alpha e^{2x}+\beta e^{3x}+\gamma x = 0$$ How can I know if the statement is only correct if all $\alpha, \beta$ and $\gamma$ are $zero$?",,['linear-algebra']
64,A Linear Operator of Rank 1,A Linear Operator of Rank 1,,Let $T$ be a linear operator with rank $1$ on a finite dimensional vector space $V$.Then Which of the following are true? 1)either $T$ is diagonalizable or  $T$ is nilpotent. 2)$T$ is both diagonalizable and  nilpotent. I take $T$ as constant mapping and got it as diagonalizable. So can we say that 1) is true and 2) is false? Is there any other method to solve the problem?,Let $T$ be a linear operator with rank $1$ on a finite dimensional vector space $V$.Then Which of the following are true? 1)either $T$ is diagonalizable or  $T$ is nilpotent. 2)$T$ is both diagonalizable and  nilpotent. I take $T$ as constant mapping and got it as diagonalizable. So can we say that 1) is true and 2) is false? Is there any other method to solve the problem?,,['linear-algebra']
65,Skew-symmetric matrix subspace dimension and basis,Skew-symmetric matrix subspace dimension and basis,,"If $M$ is the vector space of $2\times 2$ real matrices, then I can show that $$ \{A \in M \mid A^\mathrm{T}=-A \} $$ is a subspace of $M$, since $$ \left[ \begin{array}{cc} x & z \\ -z & y \end{array} \right]+\left[ \begin{array}{cc} x' & z' \\ -z' & y' \end{array} \right] = \left[ \begin{array}{cc} x+x' & z+z' \\ -(z+z') & y+y' \end{array} \right] $$ and for some $\lambda\in\mathbb{R}$ $$ \lambda\left[ \begin{array}{cc} x & z \\ -z & y \end{array} \right] = \left[ \begin{array}{cc} \lambda x & \lambda z \\ -\lambda z & \lambda y \end{array} \right] $$ But I'm not sure if I'm correct on finding the dimension and a basis of the subspace: $$ \left[ \begin{array}{cc} x & z \\ -z & y \end{array} \right] = x\left[ \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right]+y\left[ \begin{array}{cc} 0 & 0 \\ 0 & 1 \end{array} \right]+z\left[ \begin{array}{cc} 0 & 1 \\ -1 & 0 \end{array} \right] $$ This makes me think that a basis is made up of $$ \left(\left[ \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right],\left[ \begin{array}{cc} 0 & 0 \\ 0 & 1 \end{array} \right],\left[ \begin{array}{cc} 0 & 1 \\ -1 & 0 \end{array} \right]  \right) $$ and so the dimension is three. Is that right?","If $M$ is the vector space of $2\times 2$ real matrices, then I can show that $$ \{A \in M \mid A^\mathrm{T}=-A \} $$ is a subspace of $M$, since $$ \left[ \begin{array}{cc} x & z \\ -z & y \end{array} \right]+\left[ \begin{array}{cc} x' & z' \\ -z' & y' \end{array} \right] = \left[ \begin{array}{cc} x+x' & z+z' \\ -(z+z') & y+y' \end{array} \right] $$ and for some $\lambda\in\mathbb{R}$ $$ \lambda\left[ \begin{array}{cc} x & z \\ -z & y \end{array} \right] = \left[ \begin{array}{cc} \lambda x & \lambda z \\ -\lambda z & \lambda y \end{array} \right] $$ But I'm not sure if I'm correct on finding the dimension and a basis of the subspace: $$ \left[ \begin{array}{cc} x & z \\ -z & y \end{array} \right] = x\left[ \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right]+y\left[ \begin{array}{cc} 0 & 0 \\ 0 & 1 \end{array} \right]+z\left[ \begin{array}{cc} 0 & 1 \\ -1 & 0 \end{array} \right] $$ This makes me think that a basis is made up of $$ \left(\left[ \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right],\left[ \begin{array}{cc} 0 & 0 \\ 0 & 1 \end{array} \right],\left[ \begin{array}{cc} 0 & 1 \\ -1 & 0 \end{array} \right]  \right) $$ and so the dimension is three. Is that right?",,"['linear-algebra', 'matrices']"
66,What is a basis for the vector space $ \Bbb{C}^{n} $ (a complex vector space)?,What is a basis for the vector space  (a complex vector space)?, \Bbb{C}^{n} ,"I know that a basis for $ \Bbb{C} $ is $ \{ 1,i \} $. This set is linearly independent in $ \Bbb{C} $ and spans $ \Bbb{C} $. I think that the dimension of $ \Bbb{C}^{n} $ may be $ 2 n $, but I’m just failing to understand what kind of vectors should be in a basis. Also, please correct me if the dimension of $ \Bbb{C}^{n} $ is not $ 2 n $.","I know that a basis for $ \Bbb{C} $ is $ \{ 1,i \} $. This set is linearly independent in $ \Bbb{C} $ and spans $ \Bbb{C} $. I think that the dimension of $ \Bbb{C}^{n} $ may be $ 2 n $, but I’m just failing to understand what kind of vectors should be in a basis. Also, please correct me if the dimension of $ \Bbb{C}^{n} $ is not $ 2 n $.",,"['linear-algebra', 'vector-spaces', 'complex-numbers']"
67,What is the difference between a vector and its transpose?,What is the difference between a vector and its transpose?,,"I have seen a definition of orthogonality between two vectors something like this: $$<\vec{u}, \vec{v}> = u^T \cdot v = 0$$ I am wondering what is the purpose of using a transpose of a vector (in this case and in general). I have also seen this in the formula to find the projection of a vector over another, but I have used just the normal vector instead of its transpose and everything seems to work.","I have seen a definition of orthogonality between two vectors something like this: $$<\vec{u}, \vec{v}> = u^T \cdot v = 0$$ I am wondering what is the purpose of using a transpose of a vector (in this case and in general). I have also seen this in the formula to find the projection of a vector over another, but I have used just the normal vector instead of its transpose and everything seems to work.",,['linear-algebra']
68,Exercise about Matrix diagonalization,Exercise about Matrix diagonalization,,"Well I have to diagonalize this matrix : $$ \begin{pmatrix} 5 & 0 & -1 \\ 1 & 4 & -1 \\ -1 & 0 & 5 \end{pmatrix} $$ I find the polynome witch is $P=-(\lambda-4)^2(\lambda-6)$ Now I want to know eignevectors so I solve $AX=4X$ and $AX=6X$ with $X=\begin{pmatrix} x \\ y \\ z \end{pmatrix}$but I have a problem with the first system ! In the correction they say ""after an elementary calculus we have $E_4=Vect\left(\begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix},\begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}\right)$"" And I don't know how and why because with my system I just find that $x=z$. Can you explain to me, please ?","Well I have to diagonalize this matrix : $$ \begin{pmatrix} 5 & 0 & -1 \\ 1 & 4 & -1 \\ -1 & 0 & 5 \end{pmatrix} $$ I find the polynome witch is $P=-(\lambda-4)^2(\lambda-6)$ Now I want to know eignevectors so I solve $AX=4X$ and $AX=6X$ with $X=\begin{pmatrix} x \\ y \\ z \end{pmatrix}$but I have a problem with the first system ! In the correction they say ""after an elementary calculus we have $E_4=Vect\left(\begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix},\begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}\right)$"" And I don't know how and why because with my system I just find that $x=z$. Can you explain to me, please ?",,"['linear-algebra', 'diagonalization']"
69,"For matrices, if $AB=BA$, then does it follow that $B^{2}A=AB^{2}$?","For matrices, if , then does it follow that ?",AB=BA B^{2}A=AB^{2},"Suppose $AB=BA$ ($A, B$ are $n\times n$ matrices). Does that mean $B^{2}A=AB^{2}$ ? I looked for counter cases and couldn't find any. I tried to prove this by multiplying both sides and comparing, but I got stuck since I don't know how to effectively use the fact that $AB = BA$. Any advice or general direction would be greatly appreciated.","Suppose $AB=BA$ ($A, B$ are $n\times n$ matrices). Does that mean $B^{2}A=AB^{2}$ ? I looked for counter cases and couldn't find any. I tried to prove this by multiplying both sides and comparing, but I got stuck since I don't know how to effectively use the fact that $AB = BA$. Any advice or general direction would be greatly appreciated.",,"['linear-algebra', 'matrices']"
70,"Why $\max \left\{ {{x^T}Ax:x \in {R^n},{x^T}x = 1} \right\}$ is the largest real eigenvalue of A?",Why  is the largest real eigenvalue of A?,"\max \left\{ {{x^T}Ax:x \in {R^n},{x^T}x = 1} \right\}","Let $A \in {M_n}(R)$ and A is symmetric.Why $\max \left\{ {{x^T}Ax:x \in {R^n},{x^T}x = 1} \right\}$ is the largest real eigenvalue of A?","Let $A \in {M_n}(R)$ and A is symmetric.Why $\max \left\{ {{x^T}Ax:x \in {R^n},{x^T}x = 1} \right\}$ is the largest real eigenvalue of A?",,"['linear-algebra', 'matrices', 'matrix-calculus']"
71,A determinant coming out from the computation of a volume form,A determinant coming out from the computation of a volume form,,"I am convinced that the following identity is true: \begin{equation} \det\begin{bmatrix}  1+a_1^2 & a_1 a_2 & a_1 a_3 & \ldots & a_1a_n \\  a_1a_2 & 1+a_2^2 & a_2a_3 & \ldots & a_2a_n \\  \ldots &\ldots &\dots & \ldots & \ldots \\ a_na_1 &a_na_2 & a_na_3 &\ldots &1+a_n^2 \end{bmatrix}= 1+ a_1^2+a_2^2+\dots+a_n^2. \end{equation} Can you help me proving it? This determinant comes out in the computation of the volume form on a n-dimensional surface on $\mathbb{R}^{n+1}$ described by the equation  \begin{equation} x_{n+1}=f(x_1\ldots x_n). \end{equation} The volume form is given by $\sqrt{g}dx_1\ldots dx_{n}$, where $g$ is the determinant above with $a_j=\partial_{x_j}f$. The result proven in this question & answer shows that the volume form is  $$ \sqrt{1+\sum_{j=1}^n \left(\frac{\partial f}{\partial x_j}\right)^2}\, dx_1\ldots dx_n.$$","I am convinced that the following identity is true: \begin{equation} \det\begin{bmatrix}  1+a_1^2 & a_1 a_2 & a_1 a_3 & \ldots & a_1a_n \\  a_1a_2 & 1+a_2^2 & a_2a_3 & \ldots & a_2a_n \\  \ldots &\ldots &\dots & \ldots & \ldots \\ a_na_1 &a_na_2 & a_na_3 &\ldots &1+a_n^2 \end{bmatrix}= 1+ a_1^2+a_2^2+\dots+a_n^2. \end{equation} Can you help me proving it? This determinant comes out in the computation of the volume form on a n-dimensional surface on $\mathbb{R}^{n+1}$ described by the equation  \begin{equation} x_{n+1}=f(x_1\ldots x_n). \end{equation} The volume form is given by $\sqrt{g}dx_1\ldots dx_{n}$, where $g$ is the determinant above with $a_j=\partial_{x_j}f$. The result proven in this question & answer shows that the volume form is  $$ \sqrt{1+\sum_{j=1}^n \left(\frac{\partial f}{\partial x_j}\right)^2}\, dx_1\ldots dx_n.$$",,"['linear-algebra', 'determinant', 'riemannian-geometry']"
72,"If I have a given matrix A and I perform it a row/column operation on it to get a new matrix B, are A and B similar? [duplicate]","If I have a given matrix A and I perform it a row/column operation on it to get a new matrix B, are A and B similar? [duplicate]",,"This question already has answers here : Why does Gaussian elimination not preserve similarity of a matrix? (3 answers) Closed 9 years ago . I wonder if the matrices are similar, and if this is true, then if I want to solve a minimal polynomial problem on a matrix A, if I can simplify the matrix A until it has the form of a diagonal matrix by blocks B.","This question already has answers here : Why does Gaussian elimination not preserve similarity of a matrix? (3 answers) Closed 9 years ago . I wonder if the matrices are similar, and if this is true, then if I want to solve a minimal polynomial problem on a matrix A, if I can simplify the matrix A until it has the form of a diagonal matrix by blocks B.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'minimal-polynomials']"
73,Linear Algebra - Suppose $CA=I_n$. Show that the equation $Ax = 0$ has only the trivial solution.,Linear Algebra - Suppose . Show that the equation  has only the trivial solution.,CA=I_n Ax = 0,Suppose $CA=I_n$.  Show that the equation $Ax = 0$ has only the trivial solution.  Explain why $A$ cannot have more columns than rows. I really don't even know where to begin with this one.,Suppose $CA=I_n$.  Show that the equation $Ax = 0$ has only the trivial solution.  Explain why $A$ cannot have more columns than rows. I really don't even know where to begin with this one.,,['linear-algebra']
74,Derivative of a quadratic form,Derivative of a quadratic form,,"There is a Hermitian matrix $X$ and a complex vector $a$. I know that $a^HXa$ is a real scalar but derivative of $a^HXa$ with respect to $a$ is complex,  $$\frac{\partial a^HXa}{\partial a}=Xa^*$$ Why is the derivative complex? Is it possible that the derivative of a real variable be complex? (matrix $X$ is complex).","There is a Hermitian matrix $X$ and a complex vector $a$. I know that $a^HXa$ is a real scalar but derivative of $a^HXa$ with respect to $a$ is complex,  $$\frac{\partial a^HXa}{\partial a}=Xa^*$$ Why is the derivative complex? Is it possible that the derivative of a real variable be complex? (matrix $X$ is complex).",,"['linear-algebra', 'multivariable-calculus', 'complex-numbers', 'quadratic-forms', 'scalar-fields']"
75,trace inequality of positive definite matrices.,trace inequality of positive definite matrices.,,"Assume $A,B \in M_n(\Bbb{R})$ are positive definite matrices, show that $$\text{Tr}(AB)\leq \text{Tr}(A)\text{Tr}(B) $$ I only prove it for $n=2$, it is straightforward calculate.but when $n \geq 3$. I have no idea to use the condition positive definite. I know it is equal to its principal minors are positive. when $n=2$, is very useful .but when $n \geq 3$. it seems useless : ( please help me ,thanks","Assume $A,B \in M_n(\Bbb{R})$ are positive definite matrices, show that $$\text{Tr}(AB)\leq \text{Tr}(A)\text{Tr}(B) $$ I only prove it for $n=2$, it is straightforward calculate.but when $n \geq 3$. I have no idea to use the condition positive definite. I know it is equal to its principal minors are positive. when $n=2$, is very useful .but when $n \geq 3$. it seems useless : ( please help me ,thanks",,"['linear-algebra', 'matrices', 'inequality']"
76,"Vector space of continuous $[0,1] \to [0,1]$ over $\Bbb R$",Vector space of continuous  over,"[0,1] \to [0,1] \Bbb R","Is there a way to define a vector space of continuous functions of type $[0,1] \to [0,1]$ over $\mathbb R$? If no, how to prove that such a vector space does not exist?","Is there a way to define a vector space of continuous functions of type $[0,1] \to [0,1]$ over $\mathbb R$? If no, how to prove that such a vector space does not exist?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
77,Is a norm on $R^n$ linear?,Is a norm on  linear?,R^n,"I was reading the book Linear Algebra Done Right by Axler. In the chapter on inner product space (Ch.6), he defines the norm of x on $R^n$ space as: $||x|| = \sqrt{x_1^2 + ... + x_n^2}$ and says: ""The norm is not linear on $R^n$.  To inject linearity into the discussion, we introduce the dot product."" I don't see why the norm is not linear. If I check a multiplicity for $R^2$, using a scalar of 3 for example $3||x|| =? ||3x||$ $3 \sqrt{x_1^2 + x_2^2} =? \sqrt{(3 x_1)^2 + (3 x_2)^2} $ $3 \sqrt{x_1^2 + x_2^2} =? \sqrt{9(x_1^2 + x_2)^2} $ $3 \sqrt{x_1^2 + x_2^2} =  3 \sqrt{x_1^2 + x_2^2} $ Why is the norm not linear? Axler then says: ""Also, if $y \in R^n$ is fixed, then clearly the map from $R^n$ to $R$ that sends $x \in R^n$ to $x \cdot y$ is linear."" Why is the dot product linear if the norm isn't? Regards, Madeleine.","I was reading the book Linear Algebra Done Right by Axler. In the chapter on inner product space (Ch.6), he defines the norm of x on $R^n$ space as: $||x|| = \sqrt{x_1^2 + ... + x_n^2}$ and says: ""The norm is not linear on $R^n$.  To inject linearity into the discussion, we introduce the dot product."" I don't see why the norm is not linear. If I check a multiplicity for $R^2$, using a scalar of 3 for example $3||x|| =? ||3x||$ $3 \sqrt{x_1^2 + x_2^2} =? \sqrt{(3 x_1)^2 + (3 x_2)^2} $ $3 \sqrt{x_1^2 + x_2^2} =? \sqrt{9(x_1^2 + x_2)^2} $ $3 \sqrt{x_1^2 + x_2^2} =  3 \sqrt{x_1^2 + x_2^2} $ Why is the norm not linear? Axler then says: ""Also, if $y \in R^n$ is fixed, then clearly the map from $R^n$ to $R$ that sends $x \in R^n$ to $x \cdot y$ is linear."" Why is the dot product linear if the norm isn't? Regards, Madeleine.",,"['linear-algebra', 'functions', 'normed-spaces', 'inner-products']"
78,Closest points on two line segments,Closest points on two line segments,,"I am looking for a general formulation to find the closest points on two line segments. What I was thinking about is to define our lines as: $$ P1 + s  (P2-P1)$$ $$ Q1 + t  (Q2-Q1)$$ Where $P1 , P2, Q1$ and $Q2$ are the beginning and the end points on each segment. Now we should go through an optimization problem as: $\min f(s,t)$ such that $0<s<1$ and $0<t<1$.  Where $f(s,t)$ is the point-to-point distance function. Is there any straight forward solution?","I am looking for a general formulation to find the closest points on two line segments. What I was thinking about is to define our lines as: $$ P1 + s  (P2-P1)$$ $$ Q1 + t  (Q2-Q1)$$ Where $P1 , P2, Q1$ and $Q2$ are the beginning and the end points on each segment. Now we should go through an optimization problem as: $\min f(s,t)$ such that $0<s<1$ and $0<t<1$.  Where $f(s,t)$ is the point-to-point distance function. Is there any straight forward solution?",,"['linear-algebra', 'optimization']"
79,Prove or disprove that trace of matrix $X$ is zero,Prove or disprove that trace of matrix  is zero,X,"I was trying to solve a question  from a competitive exam paper. This is a part of that question. Let $I_n$ and $O_n$ be $n\times n$ identity and null matrices respectively.Let $S$ be $2n\times 2n$ matrix given in block form by $$S=\begin{bmatrix} O_n & I_n \\ -I_n & O_n\end{bmatrix}$$ If $X$ is a $2n\times 2n$ matrix such that $X^tS+SX=O_{2n}$ then determine wheather trace of $X$ is zero or not. From the given information in the question I just managed to find that $S^t=-S$ (i.e $S$ is skew symmetric) and det( $S$ )= $1$ . Then $SX=-X^tS=X^tS^t=(SX)^t\Rightarrow SX$ is symmetric. Also det( $SX$ )=det( $X$ ). Am I right upto this?and I do not know whether these are necessary or not to solve the problem. I can not proceed further,completely stuck. Please help.Thnx in advance.","I was trying to solve a question  from a competitive exam paper. This is a part of that question. Let and be identity and null matrices respectively.Let be matrix given in block form by If is a matrix such that then determine wheather trace of is zero or not. From the given information in the question I just managed to find that (i.e is skew symmetric) and det( )= . Then is symmetric. Also det( )=det( ). Am I right upto this?and I do not know whether these are necessary or not to solve the problem. I can not proceed further,completely stuck. Please help.Thnx in advance.",I_n O_n n\times n S 2n\times 2n S=\begin{bmatrix} O_n & I_n \\ -I_n & O_n\end{bmatrix} X 2n\times 2n X^tS+SX=O_{2n} X S^t=-S S S 1 SX=-X^tS=X^tS^t=(SX)^t\Rightarrow SX SX X,"['linear-algebra', 'matrices', 'block-matrices', 'trace']"
80,A basis of a subspace is subset of a basis of the whole space,A basis of a subspace is subset of a basis of the whole space,,"If $X$ is a vector space with a basis $B$ and $A$ is a subspace of $X$. Does $A $always has a basis subset of $B$? If yes, how should I prove this? If no, we should give an example of a vector space $X$ with a basis $B$ and a subspace $A$ of $X$ such that any basis of $A$ is not subset of $B$.","If $X$ is a vector space with a basis $B$ and $A$ is a subspace of $X$. Does $A $always has a basis subset of $B$? If yes, how should I prove this? If no, we should give an example of a vector space $X$ with a basis $B$ and a subspace $A$ of $X$ such that any basis of $A$ is not subset of $B$.",,['linear-algebra']
81,Prove determinant of $n \times n$ matrix is $(a+(n-1)b)(a-b)^{n-1}$? [duplicate],Prove determinant of  matrix is ? [duplicate],n \times n (a+(n-1)b)(a-b)^{n-1},"This question already has answers here : Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ [duplicate] (9 answers) Closed 10 years ago . Prove $\det(A)$ is $(a+(n-1)b)(a-b)^{n-1}$ where $A$ is $n \times n$ matrix with $a$'s on diagonal and all other elements $b$, off diagonal.","This question already has answers here : Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ [duplicate] (9 answers) Closed 10 years ago . Prove $\det(A)$ is $(a+(n-1)b)(a-b)^{n-1}$ where $A$ is $n \times n$ matrix with $a$'s on diagonal and all other elements $b$, off diagonal.",,"['linear-algebra', 'matrices', 'proof-writing', 'determinant']"
82,"If $A \in \mathbb{C}^{m\times n}$ is full-column rank matrix, then is rank($AB$) = rank ($BA$) = rank($B$)?","If  is full-column rank matrix, then is rank() = rank () = rank()?",A \in \mathbb{C}^{m\times n} AB BA B,"Let $A \in \mathbb{C}^{m\times n}$, and $B \in \mathbb{C}^{n\times k}$ complex matrices. If A is full-column rank matrix then can we say that rank($AB$) = rank ($BA$) = rank($B$)? What can we say about $N(AB)$? Under what condition $N(AB) = N(B)$? I need help to understand this. I would be very much helpful for any kind of help and suggestions.","Let $A \in \mathbb{C}^{m\times n}$, and $B \in \mathbb{C}^{n\times k}$ complex matrices. If A is full-column rank matrix then can we say that rank($AB$) = rank ($BA$) = rank($B$)? What can we say about $N(AB)$? Under what condition $N(AB) = N(B)$? I need help to understand this. I would be very much helpful for any kind of help and suggestions.",,"['linear-algebra', 'matrices', 'matrix-rank']"
83,How do rotational matrices work? [duplicate],How do rotational matrices work? [duplicate],,"This question already has answers here : Rotation of Matrices and their interpretation (2 answers) Closed 10 years ago . I am confuse on the how exactly rotational matrices work. So I understand that you can rotate a point around the x, y and z axis but if asked how you find a single matrix that will show the same rotation if you were to rotate it along the x, y and z axis in that order. Any help would be appreciated!","This question already has answers here : Rotation of Matrices and their interpretation (2 answers) Closed 10 years ago . I am confuse on the how exactly rotational matrices work. So I understand that you can rotate a point around the x, y and z axis but if asked how you find a single matrix that will show the same rotation if you were to rotate it along the x, y and z axis in that order. Any help would be appreciated!",,"['linear-algebra', 'matrices']"
84,"$x,y$ are linearly depending iff $|\langle x,y\rangle|=\|x\| \cdot \|y\|$",are linearly depending iff,"x,y |\langle x,y\rangle|=\|x\| \cdot \|y\|","I tried to prove a special case of Cauchy-Schwarz: $$x,y \text{ are linearly depending vectors} \Leftrightarrow |\langle x,y\rangle|=||x|| \cdot ||y||$$ $\Rightarrow$ is simple: \begin{eqnarray*} x= \lambda y \Leftrightarrow (x_1,\ldots,x_n)&=&(\lambda x_1,\ldots,\lambda x_n),\\ |\langle x,y\rangle|=||x|| \cdot ||y|| &\Leftrightarrow& |\langle x,\lambda x\rangle|=||x|| \cdot ||\lambda x|| \\ & \Leftrightarrow& |x_1 \lambda x_1+\ldots +x_n \lambda x_n|=\sqrt{x_1^2+\ldots+x_n^2} \cdot \sqrt{(\lambda x_1)^2+\ldots+(\lambda x_n)^2}\\ &\Leftrightarrow& |\lambda x_1^2+\ldots+\lambda x_n^2|= \sqrt{x_1^2+\ldots+x_n^2} \cdot \sqrt{\lambda^2(x_1^2+\ldots+x_n^2}\\ &\Leftrightarrow& |\lambda| \cdot |x_1^2+\ldots+x_n^2|=|\lambda| \cdot \sqrt{x_1^2+\ldots+x_n^2}^2\\ &\Leftrightarrow& |\lambda| \cdot |x_1^2+\ldots+x_n^2|=|\lambda| \cdot |x_1^2+\ldots+x_n^2| \end{eqnarray*} Now I have absolutly no idea how to prove $\Leftarrow$. Anyone got in idea?","I tried to prove a special case of Cauchy-Schwarz: $$x,y \text{ are linearly depending vectors} \Leftrightarrow |\langle x,y\rangle|=||x|| \cdot ||y||$$ $\Rightarrow$ is simple: \begin{eqnarray*} x= \lambda y \Leftrightarrow (x_1,\ldots,x_n)&=&(\lambda x_1,\ldots,\lambda x_n),\\ |\langle x,y\rangle|=||x|| \cdot ||y|| &\Leftrightarrow& |\langle x,\lambda x\rangle|=||x|| \cdot ||\lambda x|| \\ & \Leftrightarrow& |x_1 \lambda x_1+\ldots +x_n \lambda x_n|=\sqrt{x_1^2+\ldots+x_n^2} \cdot \sqrt{(\lambda x_1)^2+\ldots+(\lambda x_n)^2}\\ &\Leftrightarrow& |\lambda x_1^2+\ldots+\lambda x_n^2|= \sqrt{x_1^2+\ldots+x_n^2} \cdot \sqrt{\lambda^2(x_1^2+\ldots+x_n^2}\\ &\Leftrightarrow& |\lambda| \cdot |x_1^2+\ldots+x_n^2|=|\lambda| \cdot \sqrt{x_1^2+\ldots+x_n^2}^2\\ &\Leftrightarrow& |\lambda| \cdot |x_1^2+\ldots+x_n^2|=|\lambda| \cdot |x_1^2+\ldots+x_n^2| \end{eqnarray*} Now I have absolutly no idea how to prove $\Leftarrow$. Anyone got in idea?",,"['linear-algebra', 'vector-spaces']"
85,Diagonalization of $M=ab^t+ba^t$,Diagonalization of,M=ab^t+ba^t,"Given $a=(a_i)_{i=1}^n$ and $b=(b_i)_{i=1}^n$ vectors in $\mathbb{R}^n$, we define the matrix $M=(m_{ij})_{i,j=1}^n$ as: $$ m_{ij}=a_ib_j + a_jb_i, $$ or equivalently  $$ M=ab^t+ba^t. $$  Note that $a,b$ are also $n\times 1$ matrices, $a^t,b^t$ are $1\times n$ matrices. What are the eigenvalues and eigenvectors of $M$?","Given $a=(a_i)_{i=1}^n$ and $b=(b_i)_{i=1}^n$ vectors in $\mathbb{R}^n$, we define the matrix $M=(m_{ij})_{i,j=1}^n$ as: $$ m_{ij}=a_ib_j + a_jb_i, $$ or equivalently  $$ M=ab^t+ba^t. $$  Note that $a,b$ are also $n\times 1$ matrices, $a^t,b^t$ are $1\times n$ matrices. What are the eigenvalues and eigenvectors of $M$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'diagonalization']"
86,Transcendental Basis,Transcendental Basis,,"Can you say that because $\pi$ is transcendental, that a basis of $\{\pi, \pi^2, \pi^3, \dots\}$ in the rational numbers $\mathbb{Q}$ spans the entire real numbers? It seems likely, although I can't think of a proof of the fact.","Can you say that because $\pi$ is transcendental, that a basis of $\{\pi, \pi^2, \pi^3, \dots\}$ in the rational numbers $\mathbb{Q}$ spans the entire real numbers? It seems likely, although I can't think of a proof of the fact.",,"['linear-algebra', 'polynomials', 'transcendental-numbers']"
87,"Given two similar matrices $A$, $B$, is there a way to find an invertible matrix $P$ such that $A=P^{-1}BP$?","Given two similar matrices , , is there a way to find an invertible matrix  such that ?",A B P A=P^{-1}BP,I was wondering if given two similar square matrices $A$ and $B$ would always be possible to find an matrix $P\in GL(n)$ such that $A=P^{-1}BP$. thank you!,I was wondering if given two similar square matrices $A$ and $B$ would always be possible to find an matrix $P\in GL(n)$ such that $A=P^{-1}BP$. thank you!,,"['linear-algebra', 'matrices']"
88,The isomorphisms between two vector spaces,The isomorphisms between two vector spaces,,"Let $V$ and $W$ be two vector spaces over real number field, if they are isomorphic as vector spaces over rational number field, are they isomorphic as real  vector spaces ?","Let $V$ and $W$ be two vector spaces over real number field, if they are isomorphic as vector spaces over rational number field, are they isomorphic as real  vector spaces ?",,"['linear-algebra', 'vector-spaces', 'abstract-algebra']"
89,Function that maps straight lines into straight lines,Function that maps straight lines into straight lines,,"Consider two vector spaces $V$ and $V'$ with the same dimension. Let $f: V\longrightarrow V'$ be a bijection such that it maps straight lines into straight lines; I don't know if the following statement is true: There exists a unique linear function $L:V\longrightarrow V'$ such that $f(v)=L(v)+v_0$ for any $v\in V$ and for a certain fixed $v_0\in V$. Maybe this is a stupid question, but I can't find the (probably) easy proof of this fact.","Consider two vector spaces $V$ and $V'$ with the same dimension. Let $f: V\longrightarrow V'$ be a bijection such that it maps straight lines into straight lines; I don't know if the following statement is true: There exists a unique linear function $L:V\longrightarrow V'$ such that $f(v)=L(v)+v_0$ for any $v\in V$ and for a certain fixed $v_0\in V$. Maybe this is a stupid question, but I can't find the (probably) easy proof of this fact.",,"['linear-algebra', 'geometry']"
90,"Given a matrix with non-negative real entries, can you algebraically prove that it has a non-negative eigenvalue?","Given a matrix with non-negative real entries, can you algebraically prove that it has a non-negative eigenvalue?",,"I am looking specifically for an algebraic proof, but if you can offer both algebraic and topological proofs, I will appreciate it even more.","I am looking specifically for an algebraic proof, but if you can offer both algebraic and topological proofs, I will appreciate it even more.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
91,Prove that $L_A(X)=AX-XA$ is symmetric if and only if $A$ is symmetric.,Prove that  is symmetric if and only if  is symmetric.,L_A(X)=AX-XA A,"Define for a fixed $A \in \mathbb{M}^{2 \times 2}(\mathbb{R})$ the   mapping: $$L_A : \mathbb{M}^{2 \times 2}(\mathbb{R}) \to \mathbb{M}^{2 \times 2}(\mathbb{R}) : X \mapsto AX-XA. $$ Define on $\mathbb{M}^{2 \times 2}(\mathbb{R})$ the dotproduct $\langle \cdot , \cdot \rangle$ as follows:   $ \langle X, Y \rangle = [X]_{\xi}^t [Y]_{\xi}$. Here is $[\cdot ]_\xi$ the coordinate map   that belongs to the standard basis $$\xi = \{ E_1 = {\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}}, E_2={\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}}, E_3={\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}}, E_4={\begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}} \}$$ of   $\mathbb{M}^{2 \times 2}(\mathbb{R})$. Write $M_A$ for the matrix such   that for all $X \in \mathbb{M}^{2 \times 2}(\mathbb{R})$ it satisfies   $[L_A (X)]_\xi=M_A [X]_\xi$. Prove that $L_A$ is symmetric if and only if $A$ is symmetric. In a previous exercise I had to determine all the matrices $M_{E_1}, M_{E_2}, M_{E_3}, M_{E_4}$. And this is the result: $M_{E_1} = {\begin{pmatrix} 0 & 0 & 0 & 0 \\ 0 & -1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 \end{pmatrix}},~M_{E_2}={\begin{pmatrix} 0 & 0 & -1 & 0 \\ 1 & 0 & 0 & -1 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{pmatrix}}, ~M_{E_3}={\begin{pmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ -1 & -1 & 0 & 1 \\ 0 & 0 & 0 & 0 \end{pmatrix}}, \\M_{E_4}={\begin{pmatrix} 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & 0 \end{pmatrix}}$ How am I to use this to prove the given problem? I already showed that $L_A$ is symmetric if and only if $M_A$ is symmetric, meaning $\langle L_A(X), Y \rangle = \langle X, L_A(Y) \rangle$ if and only if $M_A = M_A^t$. I am at a loss. Thanks in advance.","Define for a fixed $A \in \mathbb{M}^{2 \times 2}(\mathbb{R})$ the   mapping: $$L_A : \mathbb{M}^{2 \times 2}(\mathbb{R}) \to \mathbb{M}^{2 \times 2}(\mathbb{R}) : X \mapsto AX-XA. $$ Define on $\mathbb{M}^{2 \times 2}(\mathbb{R})$ the dotproduct $\langle \cdot , \cdot \rangle$ as follows:   $ \langle X, Y \rangle = [X]_{\xi}^t [Y]_{\xi}$. Here is $[\cdot ]_\xi$ the coordinate map   that belongs to the standard basis $$\xi = \{ E_1 = {\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}}, E_2={\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}}, E_3={\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}}, E_4={\begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}} \}$$ of   $\mathbb{M}^{2 \times 2}(\mathbb{R})$. Write $M_A$ for the matrix such   that for all $X \in \mathbb{M}^{2 \times 2}(\mathbb{R})$ it satisfies   $[L_A (X)]_\xi=M_A [X]_\xi$. Prove that $L_A$ is symmetric if and only if $A$ is symmetric. In a previous exercise I had to determine all the matrices $M_{E_1}, M_{E_2}, M_{E_3}, M_{E_4}$. And this is the result: $M_{E_1} = {\begin{pmatrix} 0 & 0 & 0 & 0 \\ 0 & -1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 \end{pmatrix}},~M_{E_2}={\begin{pmatrix} 0 & 0 & -1 & 0 \\ 1 & 0 & 0 & -1 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{pmatrix}}, ~M_{E_3}={\begin{pmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ -1 & -1 & 0 & 1 \\ 0 & 0 & 0 & 0 \end{pmatrix}}, \\M_{E_4}={\begin{pmatrix} 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & 0 \end{pmatrix}}$ How am I to use this to prove the given problem? I already showed that $L_A$ is symmetric if and only if $M_A$ is symmetric, meaning $\langle L_A(X), Y \rangle = \langle X, L_A(Y) \rangle$ if and only if $M_A = M_A^t$. I am at a loss. Thanks in advance.",,"['linear-algebra', 'matrices', 'inner-products']"
92,The matrix I mentioned below is irreducible and primitive or isn't?,The matrix I mentioned below is irreducible and primitive or isn't?,,$$ \left[ \begin{array}{@{}ccccc@{}} 0.9&	0.1&	0&	0&	0&	0& \\ 0&	0.9&	0.1&	0&	0&	0& \\ 0&	0&	0.9&	0&	0&	0.1& \\ 0&	0&	0&	0.9&	0.1&	0& \\ 0&	0&	0&	0.1&	0.9&	0& \\ 0.1&	0&	0&	0&	0&	0.9& \\ \end{array} \right] $$ I think this matrix is irreducible and primitive but after I looked properties of irreducible i was confused and don't know how can to prove that the matrix irreducible and primitive,$$ \left[ \begin{array}{@{}ccccc@{}} 0.9&	0.1&	0&	0&	0&	0& \\ 0&	0.9&	0.1&	0&	0&	0& \\ 0&	0&	0.9&	0&	0&	0.1& \\ 0&	0&	0&	0.9&	0.1&	0& \\ 0&	0&	0&	0.1&	0.9&	0& \\ 0.1&	0&	0&	0&	0&	0.9& \\ \end{array} \right] $$ I think this matrix is irreducible and primitive but after I looked properties of irreducible i was confused and don't know how can to prove that the matrix irreducible and primitive,,"['linear-algebra', 'algebra-precalculus', 'matrices']"
93,Linear map between duals induced by linear maps between vector spaces,Linear map between duals induced by linear maps between vector spaces,,"Let $V, W$ be vector spaces over a field $F$ and let $\psi: V \to W$. Show that $\psi$ induces a linear map $\psi^{*}: W^{*} \to V^{*}$ naturally. Although the question asks for a naturally induced linear map, it does not seem at all that easy to me. Any suggestions will be greatly appreciated!","Let $V, W$ be vector spaces over a field $F$ and let $\psi: V \to W$. Show that $\psi$ induces a linear map $\psi^{*}: W^{*} \to V^{*}$ naturally. Although the question asks for a naturally induced linear map, it does not seem at all that easy to me. Any suggestions will be greatly appreciated!",,['linear-algebra']
94,How to randomly construct a square full-ranked matrix with low determinant?,How to randomly construct a square full-ranked matrix with low determinant?,,"How to randomly construct a square (1000*1000) full-ranked matrix with low determinant? I have tried the following method, but it failed. In MATLAB, I just use: n=100; A=randi([0 1], n, n); while rank(A)~=n A=randi([0 1], n, n); end The above code generates a random binary matrix, with the hope that the corresponding determinant can be small. However, the determinant is usually about 10^49, a huge number. Not to mention when n>200, the determinant is usually overflowed in MATLAB. Could anyone have comments how I can generate matrix (could be non-binary) with very low determinant (e.g. <10^3)?","How to randomly construct a square (1000*1000) full-ranked matrix with low determinant? I have tried the following method, but it failed. In MATLAB, I just use: n=100; A=randi([0 1], n, n); while rank(A)~=n A=randi([0 1], n, n); end The above code generates a random binary matrix, with the hope that the corresponding determinant can be small. However, the determinant is usually about 10^49, a huge number. Not to mention when n>200, the determinant is usually overflowed in MATLAB. Could anyone have comments how I can generate matrix (could be non-binary) with very low determinant (e.g. <10^3)?",,"['linear-algebra', 'random', 'matlab', 'determinant']"
95,Poles and Zeros of Linear Systems,Poles and Zeros of Linear Systems,,"This period I follow a course in System and Control Theory. This is all about linear systems $$\frac{dx}{dt}= Ax + Bu $$ $$y = Cx + Du $$ where A,B,C,D are matrices, and x, u and y are vectors. To describe my (little) background, I should know things like linearization, Impulse/Step Response, Equillibrium points, (asymptotic) stability (routh, lyapunov), controllability, observability, stabilizability, realization, hankel, etc. Given my background, I would like to ask some questions about poles and zeros of linear systems. If you know the answer of just one of the questions, please dont hesitate to write it down :-) 1.) What are poles and zeros of linear system {A,B,C,D} exactly? What does it mean for a system to have a pole at a certain value, or a zero at certain value? 2.) The author writes about 'poles of a transfer function matrix H(s)'. What is a transfer function matrix? The only thing I know is how to compute it and that it describes some relation between input/output of the system. But why do we need tranfser function matrices? 3.) To calculate the poles and zeros, the author says that we need the Smith and Smith-McMillan Forms. These are matrices that have only diagonal entries. What is exactly the algorithm to calculate the Smith-(McMillan)-form of a transfer matrix? 4.) What is the relation between the poles of a system and the controllability, observability, stability and stabilizability ? The same for a zero ? 5.) What is an invariant zero polynomial of the system {A,B,C,D} ? 6.) What is 'a realization of a system'? 7.) Where can I find more good information about this subject?","This period I follow a course in System and Control Theory. This is all about linear systems $$\frac{dx}{dt}= Ax + Bu $$ $$y = Cx + Du $$ where A,B,C,D are matrices, and x, u and y are vectors. To describe my (little) background, I should know things like linearization, Impulse/Step Response, Equillibrium points, (asymptotic) stability (routh, lyapunov), controllability, observability, stabilizability, realization, hankel, etc. Given my background, I would like to ask some questions about poles and zeros of linear systems. If you know the answer of just one of the questions, please dont hesitate to write it down :-) 1.) What are poles and zeros of linear system {A,B,C,D} exactly? What does it mean for a system to have a pole at a certain value, or a zero at certain value? 2.) The author writes about 'poles of a transfer function matrix H(s)'. What is a transfer function matrix? The only thing I know is how to compute it and that it describes some relation between input/output of the system. But why do we need tranfser function matrices? 3.) To calculate the poles and zeros, the author says that we need the Smith and Smith-McMillan Forms. These are matrices that have only diagonal entries. What is exactly the algorithm to calculate the Smith-(McMillan)-form of a transfer matrix? 4.) What is the relation between the poles of a system and the controllability, observability, stability and stabilizability ? The same for a zero ? 5.) What is an invariant zero polynomial of the system {A,B,C,D} ? 6.) What is 'a realization of a system'? 7.) Where can I find more good information about this subject?",,"['linear-algebra', 'dynamical-systems', 'control-theory']"
96,Orthogonal Projection Proof,Orthogonal Projection Proof,,"Let $w_1,...,w_n$ be any basis of the subspace $W \subset  \mathbb{R^m}$. Let $A = (w_1,...,w_n)$ be the $m$ x $n$ matrix whose   columns are the basis vectors, so that $W = rngA$ and $rankA=n$. Let   $P = A(A^TA)^{-1}A^T$ be the corresponding projection matrix. a.) Prove that the orthogonal projection of $v \in \mathbb{R^n}$ onto   $w \in W$ is obtained by multiplying by the projection matrix: $w=Pv$. b.) Show that if $A=QR$, then $P = QQ^T$. Why is $P \ne I$? How will I be able to prove these?","Let $w_1,...,w_n$ be any basis of the subspace $W \subset  \mathbb{R^m}$. Let $A = (w_1,...,w_n)$ be the $m$ x $n$ matrix whose   columns are the basis vectors, so that $W = rngA$ and $rankA=n$. Let   $P = A(A^TA)^{-1}A^T$ be the corresponding projection matrix. a.) Prove that the orthogonal projection of $v \in \mathbb{R^n}$ onto   $w \in W$ is obtained by multiplying by the projection matrix: $w=Pv$. b.) Show that if $A=QR$, then $P = QQ^T$. Why is $P \ne I$? How will I be able to prove these?",,['linear-algebra']
97,How to prove the sum of 2 linearly independent vectors is also linearly independent?,How to prove the sum of 2 linearly independent vectors is also linearly independent?,,"Suppose $a,b$ and $c$ are linearly independent vectors in a vector space $V$. How can I prove that $a+b$ or $b+c$ are also linearly independent?","Suppose $a,b$ and $c$ are linearly independent vectors in a vector space $V$. How can I prove that $a+b$ or $b+c$ are also linearly independent?",,"['linear-algebra', 'vector-spaces']"
98,Similarity of matrices over $\Bbb{Z}$,Similarity of matrices over,\Bbb{Z},Let $A$ be a $2 \times 2$ matrix over $\Bbb{Z}$ with characteristic polynomial $x^2 + 1$. Determine whether $A$ is similar to  $$\pmatrix{0 & -1 \\ 1 & 0}$$ over $\Bbb{Z}$. I'm so used to working with matrices over fields $\Bbb{Q}$ and $\Bbb{C}$ that I'm not sure how it differs when we switch to non-fields like $\Bbb{Z}$. Any advice would be much appreciated.,Let $A$ be a $2 \times 2$ matrix over $\Bbb{Z}$ with characteristic polynomial $x^2 + 1$. Determine whether $A$ is similar to  $$\pmatrix{0 & -1 \\ 1 & 0}$$ over $\Bbb{Z}$. I'm so used to working with matrices over fields $\Bbb{Q}$ and $\Bbb{C}$ that I'm not sure how it differs when we switch to non-fields like $\Bbb{Z}$. Any advice would be much appreciated.,,"['linear-algebra', 'matrices']"
99,Conjugation of Matrices and Conjugation of Complex Numbers,Conjugation of Matrices and Conjugation of Complex Numbers,,"Are conjugation of matrices and conjugation of complex numbers related? What I mean is that if $A$ is an $n \times n$ matrix then the conjugation of $A$ by an invertible $n \times n$ matrix $C$ is given by $CAC^{-1}$. On the other hand, if $a + bi$ is a complex number then it's conjugate is $a-bi$. These two operations don't really seem to have anything to do with one another but if they're unrelated why is the same term used to describe the operation?","Are conjugation of matrices and conjugation of complex numbers related? What I mean is that if $A$ is an $n \times n$ matrix then the conjugation of $A$ by an invertible $n \times n$ matrix $C$ is given by $CAC^{-1}$. On the other hand, if $a + bi$ is a complex number then it's conjugate is $a-bi$. These two operations don't really seem to have anything to do with one another but if they're unrelated why is the same term used to describe the operation?",,['linear-algebra']
