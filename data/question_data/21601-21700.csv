,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,infinite matrix leading eigenvalue problem,infinite matrix leading eigenvalue problem,,"I'm trying to find the leading eigenvalue and corresponding left and right eigenvectors of the following infinite matrix, for $\lambda>0$: $$ \mathrm{A}=\left( \begin{array}{cccccc} 1 &e^{-\lambda} & 0 &0 &0 & \dots\\ 1 &e^{-\lambda} & e^{-2\lambda} &0 &0 & \dots\\ 1 &e^{-\lambda} & e^{-2\lambda} &e^{-3\lambda} &0 & \dots\\ \vdots & \vdots & \vdots & & \ddots \end{array} \right) $$ Note that there are terms above the main diagonal. I know that in general infinite matrices aren't really a self-consistent idea, but from doing it numerically with $n\times n$ matrices using power iteration it looks like the problem converges in the limit of infinite $n$. The convergence is slower for smaller values of $\lambda$, but it looks like it probably converges for all $\lambda>0$. Note that I only care about the leading eigenvalue, i.e. the one with the largest magnitude, which should be real and positive. Its corresponding eigenvectors should have only positive entries, due to the Perron-Frobenius theorem. Alternatively, if it's easier, a solution for the following matrix will be just as useful to me: $$ \mathrm{B}=\left( \begin{array}{cccccc} 1 & 1& 0 &0 &0 & \dots\\ e^{-\lambda} &e^{-\lambda} & e^{-\lambda} &0 &0 & \dots\\ e^{-2\lambda} & e^{-2\lambda} &e^{-2\lambda} &e^{-2\lambda} &0 & \dots\\ \vdots & \vdots & \vdots & & \ddots \end{array} \right) $$ Again note the terms above the diagonal. (The two problems are not equivalent, it's just that either one of them will help me solve a larger problem.) The problem is, I just don't have much of an idea how to do this. I've tried a variety of naive methods, along the lines of writing the eigenvalue equation $\mathrm{A}\mathbf{x} = \eta \mathbf{x}$ as a system of equations involving infinite sums and then trying to find $\{x_i >0\}$ and $\eta>0$ to satisfy them, but this doesn't seem to lead anywhere nice. It could be that there is no analytical solution. Or even worse it could be that these matrices have unbounded spectra after all (in which case I'd really like to know!), but if anyone has any insight into how to solve one of these two problems I'd really appreciate it.","I'm trying to find the leading eigenvalue and corresponding left and right eigenvectors of the following infinite matrix, for $\lambda>0$: $$ \mathrm{A}=\left( \begin{array}{cccccc} 1 &e^{-\lambda} & 0 &0 &0 & \dots\\ 1 &e^{-\lambda} & e^{-2\lambda} &0 &0 & \dots\\ 1 &e^{-\lambda} & e^{-2\lambda} &e^{-3\lambda} &0 & \dots\\ \vdots & \vdots & \vdots & & \ddots \end{array} \right) $$ Note that there are terms above the main diagonal. I know that in general infinite matrices aren't really a self-consistent idea, but from doing it numerically with $n\times n$ matrices using power iteration it looks like the problem converges in the limit of infinite $n$. The convergence is slower for smaller values of $\lambda$, but it looks like it probably converges for all $\lambda>0$. Note that I only care about the leading eigenvalue, i.e. the one with the largest magnitude, which should be real and positive. Its corresponding eigenvectors should have only positive entries, due to the Perron-Frobenius theorem. Alternatively, if it's easier, a solution for the following matrix will be just as useful to me: $$ \mathrm{B}=\left( \begin{array}{cccccc} 1 & 1& 0 &0 &0 & \dots\\ e^{-\lambda} &e^{-\lambda} & e^{-\lambda} &0 &0 & \dots\\ e^{-2\lambda} & e^{-2\lambda} &e^{-2\lambda} &e^{-2\lambda} &0 & \dots\\ \vdots & \vdots & \vdots & & \ddots \end{array} \right) $$ Again note the terms above the diagonal. (The two problems are not equivalent, it's just that either one of them will help me solve a larger problem.) The problem is, I just don't have much of an idea how to do this. I've tried a variety of naive methods, along the lines of writing the eigenvalue equation $\mathrm{A}\mathbf{x} = \eta \mathbf{x}$ as a system of equations involving infinite sums and then trying to find $\{x_i >0\}$ and $\eta>0$ to satisfy them, but this doesn't seem to lead anywhere nice. It could be that there is no analytical solution. Or even worse it could be that these matrices have unbounded spectra after all (in which case I'd really like to know!), but if anyone has any insight into how to solve one of these two problems I'd really appreciate it.",,"['linear-algebra', 'sequences-and-series', 'systems-of-equations']"
1,"Prove $\frac{c_n(a_1,\ldots,a_n)}{c_{n-1}(a_2,\ldots,a_n)}=a_1 + \frac{1}{a_2 + \frac{1}{\ddots + \frac{1}{a_{n-1}+\frac{1}{a_n}}}}$",Prove,"\frac{c_n(a_1,\ldots,a_n)}{c_{n-1}(a_2,\ldots,a_n)}=a_1 + \frac{1}{a_2 + \frac{1}{\ddots + \frac{1}{a_{n-1}+\frac{1}{a_n}}}}","For $n>0$ and $a_1,...,a_n \in K$ let $c_n(a_1,...,a_n)$ be the determinant of the matrix $$   \begin{pmatrix}   a_1 & 1 & 0 & \cdots & 0 \\   -1 & a_2 & \ddots & \ddots & \vdots \\   0 & \ddots & \ddots & \ddots & 0 \\   \vdots & \ddots & \ddots & \ddots & 1 \\   0 & \cdots & 0 & -1 & a_n   \end{pmatrix} $$ Show for $n \ge 2$ that following holds: $$   \frac{c_n(a_1,...,a_n)}{c_{n-1}(a_2,...,a_n)}   =    a_1 + \cfrac{1}{a_2 + \cfrac{1}{\ddots + \cfrac{1}{a_{n-1}+\frac{1}{a_n}}}} $$ I want to show it using induction over n but I already fail at the initial step: For $n = 2$ I have to show: $$   \frac{c_2(a_1,a_2)}{c_{2-1}(a_2)}   =    a_1 + \frac{1}{a_2} $$ which is $$   \frac{a_1a_2 + 1}{a_2} \neq a_1 + \frac{1}{a_2} $$ My also have no idea for the induction step where I have to show: $$   \frac{c_{n+1}(a_1,...,a_{n+1})}{c_n(a_2,...,a_{n+1})}   =    a_1 + \cfrac{1}{a_2 + \cfrac{1}{\ddots + \cfrac{1}{a_n+\frac{1}{a_{n+1}}}}} $$ So first I calculate $c_{n+1}(...)$ by developing after the first column which gives me: $$   a_1 \cdot \det   \begin{pmatrix}   a_2 & 1 & & & & \\   -1 & a_3 & 1 & & & \\   & -1 & \ddots & & & \\   & & & \ddots & & \\   & & & & & 1 \\   & & & & -1 & a_{n+1}   \end{pmatrix}   +   \det   \begin{pmatrix}   1 & 0 & & & & \\   -1 & a_3 & 1 & & & \\   & -1 & a_4 & & & \\   & & & \ddots & & \\   & & & & & 0 \\   & & & & 0 & a_{n+1}   \end{pmatrix} $$ which after developing after the first row gives me $$   a_1 \cdot c_n(a_2,...,a_{n+1}) + c_{n-2}(a_3,...,a_{n+1}). $$ Analog for $c_n(...)$: $$   c_n(...) = a_2 c_{n-1}(...) + c_{n-3}(...) $$ So I have $$   \frac{c_{n+1}(...)}{c_n(...)} = \frac{a_1c_n(...) + c_{n-2}(...)}{a_2c_{n-1}(...) + c_{n-3}(...)} $$ written in another way it is $$   \frac{a_1c_n(...)}{c_n(...)} + \frac{c_{n-2}}{a_2c_{n-1}(...) + c_{n-3}(...)} $$ I write it as $$   a_1 + \frac{1}{a_2 \frac{c_{n-1}(...)}{c_{n-2}(...)} + \frac{c_{n-3}(...)}{c_{n-2}(...)}} $$ and then $$   a_1 + \frac{1}{a_2 \frac{c_{n-1}(...)}{c_{n-2}(...)} + \frac{1}{\frac{c_{n-2}(...)}{c_{n-3}(...)}}} $$ so $$   a_1 + \frac{1}{a_2 \cdot \left( a_3 + \cfrac{1}{a_4 + \cfrac{1}{\ddots + \cfrac{1}{a_n+\frac{1}{a_{n+1}}}}} \right) + \frac{1}{\left( a_4 + \cfrac{1}{a_5 + \cfrac{1}{\ddots + \cfrac{1}{a_n+\frac{1}{a_{n+1}}}}} \right)}} $$ I dont know how to preced from here, any help or simpler solutions are appreciated!","For $n>0$ and $a_1,...,a_n \in K$ let $c_n(a_1,...,a_n)$ be the determinant of the matrix $$   \begin{pmatrix}   a_1 & 1 & 0 & \cdots & 0 \\   -1 & a_2 & \ddots & \ddots & \vdots \\   0 & \ddots & \ddots & \ddots & 0 \\   \vdots & \ddots & \ddots & \ddots & 1 \\   0 & \cdots & 0 & -1 & a_n   \end{pmatrix} $$ Show for $n \ge 2$ that following holds: $$   \frac{c_n(a_1,...,a_n)}{c_{n-1}(a_2,...,a_n)}   =    a_1 + \cfrac{1}{a_2 + \cfrac{1}{\ddots + \cfrac{1}{a_{n-1}+\frac{1}{a_n}}}} $$ I want to show it using induction over n but I already fail at the initial step: For $n = 2$ I have to show: $$   \frac{c_2(a_1,a_2)}{c_{2-1}(a_2)}   =    a_1 + \frac{1}{a_2} $$ which is $$   \frac{a_1a_2 + 1}{a_2} \neq a_1 + \frac{1}{a_2} $$ My also have no idea for the induction step where I have to show: $$   \frac{c_{n+1}(a_1,...,a_{n+1})}{c_n(a_2,...,a_{n+1})}   =    a_1 + \cfrac{1}{a_2 + \cfrac{1}{\ddots + \cfrac{1}{a_n+\frac{1}{a_{n+1}}}}} $$ So first I calculate $c_{n+1}(...)$ by developing after the first column which gives me: $$   a_1 \cdot \det   \begin{pmatrix}   a_2 & 1 & & & & \\   -1 & a_3 & 1 & & & \\   & -1 & \ddots & & & \\   & & & \ddots & & \\   & & & & & 1 \\   & & & & -1 & a_{n+1}   \end{pmatrix}   +   \det   \begin{pmatrix}   1 & 0 & & & & \\   -1 & a_3 & 1 & & & \\   & -1 & a_4 & & & \\   & & & \ddots & & \\   & & & & & 0 \\   & & & & 0 & a_{n+1}   \end{pmatrix} $$ which after developing after the first row gives me $$   a_1 \cdot c_n(a_2,...,a_{n+1}) + c_{n-2}(a_3,...,a_{n+1}). $$ Analog for $c_n(...)$: $$   c_n(...) = a_2 c_{n-1}(...) + c_{n-3}(...) $$ So I have $$   \frac{c_{n+1}(...)}{c_n(...)} = \frac{a_1c_n(...) + c_{n-2}(...)}{a_2c_{n-1}(...) + c_{n-3}(...)} $$ written in another way it is $$   \frac{a_1c_n(...)}{c_n(...)} + \frac{c_{n-2}}{a_2c_{n-1}(...) + c_{n-3}(...)} $$ I write it as $$   a_1 + \frac{1}{a_2 \frac{c_{n-1}(...)}{c_{n-2}(...)} + \frac{c_{n-3}(...)}{c_{n-2}(...)}} $$ and then $$   a_1 + \frac{1}{a_2 \frac{c_{n-1}(...)}{c_{n-2}(...)} + \frac{1}{\frac{c_{n-2}(...)}{c_{n-3}(...)}}} $$ so $$   a_1 + \frac{1}{a_2 \cdot \left( a_3 + \cfrac{1}{a_4 + \cfrac{1}{\ddots + \cfrac{1}{a_n+\frac{1}{a_{n+1}}}}} \right) + \frac{1}{\left( a_4 + \cfrac{1}{a_5 + \cfrac{1}{\ddots + \cfrac{1}{a_n+\frac{1}{a_{n+1}}}}} \right)}} $$ I dont know how to preced from here, any help or simpler solutions are appreciated!",,"['linear-algebra', 'induction', 'determinant']"
2,What's a good book on advanced linear algebra?,What's a good book on advanced linear algebra?,,"I'm taking an advanced linear algebra course and I'm a little confused about books. The teacher said we could use any book we wanted to, but he recomended just Hoffman and Kunze and also Kostrikin, however, those seem to be not quite sufficient since it seems he's teaching in a little more advanced way. Indeed in the first class he proved from Zorn's Lemma that every vector space admits a basis and in the second class he defined direct product, direct sums (both internal and external), proved the rank nullity theorem and some other things. Most of things he did in a very general context (families of vector spaces indexed by some arbitry set of indexes and with dimensions being finite or infinite). I'm looking for a good advanced book on linear algebra to review what he did in class, but the books he recommend do not go so far if I'm right. I've studied multilinear algebra once with Kostrikin's book, but he never generalized things to arbitrary families of vector spaces and things like that. Which books at this kind of approach are recommended? Thanks very much in advance!","I'm taking an advanced linear algebra course and I'm a little confused about books. The teacher said we could use any book we wanted to, but he recomended just Hoffman and Kunze and also Kostrikin, however, those seem to be not quite sufficient since it seems he's teaching in a little more advanced way. Indeed in the first class he proved from Zorn's Lemma that every vector space admits a basis and in the second class he defined direct product, direct sums (both internal and external), proved the rank nullity theorem and some other things. Most of things he did in a very general context (families of vector spaces indexed by some arbitry set of indexes and with dimensions being finite or infinite). I'm looking for a good advanced book on linear algebra to review what he did in class, but the books he recommend do not go so far if I'm right. I've studied multilinear algebra once with Kostrikin's book, but he never generalized things to arbitrary families of vector spaces and things like that. Which books at this kind of approach are recommended? Thanks very much in advance!",,"['linear-algebra', 'reference-request', 'book-recommendation']"
3,Does having real eigenvalues have a geometric meaning?,Does having real eigenvalues have a geometric meaning?,,"Is it possible to characterize the set of real matrices which have real eigenvalues geometrically? That is, is it possible to say that a linear map $T$ has real eigenvalues if and only if it satisfies some property ${\cal P}$ which has a geometric flavor? I understand, of course, that different people will disagree on what ``geometric'' means. Still, I'm quite interested in seeing any possible answers. It seems plausible that properties of matrices which are independent of choice of basis have geometric interpretations. Examples of the sort of thing I'm looking for: A linear map $T$ has eigenvalues within the unit circle if and only if $\lim_k T^k x = 0$ for any $x$. A linear map on $\mathbb{R}^n$ has all of its eigenvalues equal to zero if $T^n x = 0$ for any $x \in \mathbb{R}^n$. A linear map $T$ has $1$ in its set of eigenvalues if and only if it has a fixed point. A linear map $T$ will have a root of unity in its set of eigenvalues if and only if it has a periodic orbit.","Is it possible to characterize the set of real matrices which have real eigenvalues geometrically? That is, is it possible to say that a linear map $T$ has real eigenvalues if and only if it satisfies some property ${\cal P}$ which has a geometric flavor? I understand, of course, that different people will disagree on what ``geometric'' means. Still, I'm quite interested in seeing any possible answers. It seems plausible that properties of matrices which are independent of choice of basis have geometric interpretations. Examples of the sort of thing I'm looking for: A linear map $T$ has eigenvalues within the unit circle if and only if $\lim_k T^k x = 0$ for any $x$. A linear map on $\mathbb{R}^n$ has all of its eigenvalues equal to zero if $T^n x = 0$ for any $x \in \mathbb{R}^n$. A linear map $T$ has $1$ in its set of eigenvalues if and only if it has a fixed point. A linear map $T$ will have a root of unity in its set of eigenvalues if and only if it has a periodic orbit.",,['linear-algebra']
4,Eigenvalues of a tridiagonal trigonometric matrix,Eigenvalues of a tridiagonal trigonometric matrix,,"Let $D$ be the diagonal matrix w/alternating in sign diagonal entries:  $$D_{kk}=(-1)^{k+1}\tan(\frac{k\pi}{2n+1}),$$ where $k=1,2,\dots n\in N$, and let $B$ be the $n$ by $n$ square $(0,1)$-matrix $$B= \begin{pmatrix}  0     & 1 & 0 & \ldots & 0 \\  1     & 0 & 1 & \ldots & 0 \\  0     & 1 & \ddots & \ddots & \vdots \\  \vdots    & \vdots & \ddots & 0 & 1 \\  0     & 0  & \ldots & 1 & 1 \\ \end{pmatrix}. $$ (a) Prove, that the eigenvalues of the product matrix $(-1)^{n+1}DB$ are $$  2\sin\left(\frac{k\pi}{2n+1}\right), \,\, k=1,2,\dots,n. $$ The result follows from a continued fraction identity w/a lengthy proof and an exercise from the open Wiki book ""On 2D Inverse Problems"" ( http://en.wikibooks.org/wiki/On_2D_Inverse_Problems ), but a direct shorter proof w/some geometric intuition would be very useful. (b) The matrix $D$ is a discrete version of the operator $\frac{d^2}{dx^2}+2+\delta$. Is there differential/continuous/limiting equation version of the result in (a)?","Let $D$ be the diagonal matrix w/alternating in sign diagonal entries:  $$D_{kk}=(-1)^{k+1}\tan(\frac{k\pi}{2n+1}),$$ where $k=1,2,\dots n\in N$, and let $B$ be the $n$ by $n$ square $(0,1)$-matrix $$B= \begin{pmatrix}  0     & 1 & 0 & \ldots & 0 \\  1     & 0 & 1 & \ldots & 0 \\  0     & 1 & \ddots & \ddots & \vdots \\  \vdots    & \vdots & \ddots & 0 & 1 \\  0     & 0  & \ldots & 1 & 1 \\ \end{pmatrix}. $$ (a) Prove, that the eigenvalues of the product matrix $(-1)^{n+1}DB$ are $$  2\sin\left(\frac{k\pi}{2n+1}\right), \,\, k=1,2,\dots,n. $$ The result follows from a continued fraction identity w/a lengthy proof and an exercise from the open Wiki book ""On 2D Inverse Problems"" ( http://en.wikibooks.org/wiki/On_2D_Inverse_Problems ), but a direct shorter proof w/some geometric intuition would be very useful. (b) The matrix $D$ is a discrete version of the operator $\frac{d^2}{dx^2}+2+\delta$. Is there differential/continuous/limiting equation version of the result in (a)?",,"['linear-algebra', 'matrices', 'trigonometry', 'numerical-methods', 'continued-fractions']"
5,"If $A$ is orthogonal, for any vector $x$ such that $Ax = b$, $\Vert x \Vert = \Vert b \Vert$","If  is orthogonal, for any vector  such that ,",A x Ax = b \Vert x \Vert = \Vert b \Vert,"Is this statment true: For any vector $x$ such that $Ax = b$, $\Vert x \Vert = \Vert b \Vert$, if $A$ is orthogonal. I was working on a proof for my linear algebra class, when I noticed that the entire proof could be reduced to simple algebraic work conditional on the following statement being true: $$\text{For any vector $x$ such that $Ax = b$, $\Vert x \Vert_2 = \Vert b \Vert_2$, if $A$ is orthogonal.}$$ I have not found a straight answer to this question. I believe the statement is true but I am not sure about how to prove it. Thanks in advance.","Is this statment true: For any vector $x$ such that $Ax = b$, $\Vert x \Vert = \Vert b \Vert$, if $A$ is orthogonal. I was working on a proof for my linear algebra class, when I noticed that the entire proof could be reduced to simple algebraic work conditional on the following statement being true: $$\text{For any vector $x$ such that $Ax = b$, $\Vert x \Vert_2 = \Vert b \Vert_2$, if $A$ is orthogonal.}$$ I have not found a straight answer to this question. I believe the statement is true but I am not sure about how to prove it. Thanks in advance.",,['linear-algebra']
6,Property of partial traces,Property of partial traces,,"Consider the Kronecker product of $A \in M_m, B \in M_n$: $A \otimes B = \left( \begin{matrix} a_{11}B&...&a_{1m}B\\ \vdots&\ddots\\a_{m1}B&...&a_{mm}B \end{matrix} \right)$ $A \otimes B$ can also be thought of as an $n \times n$ block matrix. This representation can be seen as $\left( \begin{matrix} T_{11}&...&T_{1n}\\ \vdots&\ddots\\T_{n1}&...&T_{nn} \end{matrix} \right)$, where each $T$ entry in the block is an $m \times m$ matrix. If we define the two partial traces of a Kronecker product between two matrices as follows: $Tr_1(A \otimes B) = \sum_{i=1}^n T_{ii}$. We have $Tr_1 \in M_m$. $Tr_2(A \otimes B) = \left( \begin{matrix} Tr(T_{11})&...&Tr(T_{1n})\\ \vdots&\ddots\\Tr(T_{n1})&...&Tr(T_{nn}) \end{matrix} \right)$. We have $Tr_2 \in M_n$. How does one show that for any $X \geq 0$, $Im(Tr_2(X) \otimes Tr_1(X)) \supset Im(X)$ (where $X$ is Hermitian and has the form $A \otimes B$ for some $A,B$)?","Consider the Kronecker product of $A \in M_m, B \in M_n$: $A \otimes B = \left( \begin{matrix} a_{11}B&...&a_{1m}B\\ \vdots&\ddots\\a_{m1}B&...&a_{mm}B \end{matrix} \right)$ $A \otimes B$ can also be thought of as an $n \times n$ block matrix. This representation can be seen as $\left( \begin{matrix} T_{11}&...&T_{1n}\\ \vdots&\ddots\\T_{n1}&...&T_{nn} \end{matrix} \right)$, where each $T$ entry in the block is an $m \times m$ matrix. If we define the two partial traces of a Kronecker product between two matrices as follows: $Tr_1(A \otimes B) = \sum_{i=1}^n T_{ii}$. We have $Tr_1 \in M_m$. $Tr_2(A \otimes B) = \left( \begin{matrix} Tr(T_{11})&...&Tr(T_{1n})\\ \vdots&\ddots\\Tr(T_{n1})&...&Tr(T_{nn}) \end{matrix} \right)$. We have $Tr_2 \in M_n$. How does one show that for any $X \geq 0$, $Im(Tr_2(X) \otimes Tr_1(X)) \supset Im(X)$ (where $X$ is Hermitian and has the form $A \otimes B$ for some $A,B$)?",,"['linear-algebra', 'tensor-products', 'operator-algebras', 'trace']"
7,How prove this linear algebra $AB=BA$?,How prove this linear algebra ?,AB=BA,"Suppose $A,B\in M_{n}(\Bbb C)$ satisfies for $\forall a,b\in \Bbb C,aA+bB$ is always diagonalizable. Show that $$AB=BA.$$",Suppose satisfies for is always diagonalizable. Show that,"A,B\in M_{n}(\Bbb C) \forall a,b\in \Bbb C,aA+bB AB=BA.","['linear-algebra', 'matrices', 'diagonalization']"
8,How fast can you determine if vectors are linearly independent?,How fast can you determine if vectors are linearly independent?,,Let us suppose you have $m$ real-valued vectors of length $n$ where $n \geq m$. How fast can you determine if they are linearly independent? In the case where $m = n$ one way to determine independence would be to compute the determinant of the matrix whose rows are the vectors. I tried some googling and found that the best known algorithm to compute the determinant of a square matrix with $n$ rows runs in $O \left ( n^{2.373} \right )$. That puts an upper bound on the case where $m = n$. But computing the determinant seems like an overkill. Furthermore it does not solve the case where $n > m$. Is there a better algorithm? What is the known theoretical lower bound on the complexity of such an algorithm?,Let us suppose you have $m$ real-valued vectors of length $n$ where $n \geq m$. How fast can you determine if they are linearly independent? In the case where $m = n$ one way to determine independence would be to compute the determinant of the matrix whose rows are the vectors. I tried some googling and found that the best known algorithm to compute the determinant of a square matrix with $n$ rows runs in $O \left ( n^{2.373} \right )$. That puts an upper bound on the case where $m = n$. But computing the determinant seems like an overkill. Furthermore it does not solve the case where $n > m$. Is there a better algorithm? What is the known theoretical lower bound on the complexity of such an algorithm?,,['linear-algebra']
9,Uniqueness of sum of nilpotent and diagonalizable matrices.,Uniqueness of sum of nilpotent and diagonalizable matrices.,,"I have the following question: Let $V$ be a vector space over a field $F$, and let $A$ be an endomorphism $V\rightarrow V$. Prove there is at most one pair of linear maps $D$ and $N$ such that $D+N=A$, $D$ is diagonalizable, $N$ is nilpotent, and every map that commutes with $D+N$ commutes with $D$ and $N$. If there are two pairs, $D_1$, $N_1$ and $D_2$, $N_2$ that satisfy these properties, then $D_1-D_2 = N_2-N_1$. Since the sum of commuting diagonalizable and nilpotent matrices are diagonalizable and nilpotent respectively, the theorem is proved if I can show that $D_1, D_2$ and $N_1$, $N_2$ commute. But this is where I'm stuck. Any help would be greatly appreciated. Thank you!","I have the following question: Let $V$ be a vector space over a field $F$, and let $A$ be an endomorphism $V\rightarrow V$. Prove there is at most one pair of linear maps $D$ and $N$ such that $D+N=A$, $D$ is diagonalizable, $N$ is nilpotent, and every map that commutes with $D+N$ commutes with $D$ and $N$. If there are two pairs, $D_1$, $N_1$ and $D_2$, $N_2$ that satisfy these properties, then $D_1-D_2 = N_2-N_1$. Since the sum of commuting diagonalizable and nilpotent matrices are diagonalizable and nilpotent respectively, the theorem is proved if I can show that $D_1, D_2$ and $N_1$, $N_2$ commute. But this is where I'm stuck. Any help would be greatly appreciated. Thank you!",,['linear-algebra']
10,When $\operatorname{span} (S_1 \cap S_2) = \operatorname{span}(S_1) \cap \operatorname{span}(S_2)$holds,When holds,\operatorname{span} (S_1 \cap S_2) = \operatorname{span}(S_1) \cap \operatorname{span}(S_2),"Let $S_1$ and $S_2$ be subsets of a vector space. When does the equality $$\operatorname{span} (S_1 \cap S_2) = \operatorname{span}(S_1) \cap \operatorname{span}(S_2)$$ hold? I have found two sufficient conditions: They are vector spaces; one is a subset of the other. Any others? But what is the necessary and sufficient condition?  Can it be that there are no necessary conditions, or in other words we cannot necessarily characterize $S_1$ and $S_2$? What are some other problems where this situation arises?","Let $S_1$ and $S_2$ be subsets of a vector space. When does the equality $$\operatorname{span} (S_1 \cap S_2) = \operatorname{span}(S_1) \cap \operatorname{span}(S_2)$$ hold? I have found two sufficient conditions: They are vector spaces; one is a subset of the other. Any others? But what is the necessary and sufficient condition?  Can it be that there are no necessary conditions, or in other words we cannot necessarily characterize $S_1$ and $S_2$? What are some other problems where this situation arises?",,['linear-algebra']
11,Pathological linear functionals and ZF,Pathological linear functionals and ZF,,"Let $S$ be an infinite set. Let $C(S)$ be the vector space of all functions $S \to \mathbb{R}$, and let $C_c(S)$ be the subspace of functions of finite support. Is the existence of a nonzero linear functional on $C(S)/C_c(S)$ independent of ZF? Does it follow from a choice principle that is known to be strictly weaker than AC? What about $\ell^1(S)/C_c(S)$? (A closely related question is to ask for nonzero linear functionals on $\ell^{\infty}(S)/C_0(S)$, and here I know that examples can be constructed using the Hahn-Banach theorem or the ultrafilter lemma .)","Let $S$ be an infinite set. Let $C(S)$ be the vector space of all functions $S \to \mathbb{R}$, and let $C_c(S)$ be the subspace of functions of finite support. Is the existence of a nonzero linear functional on $C(S)/C_c(S)$ independent of ZF? Does it follow from a choice principle that is known to be strictly weaker than AC? What about $\ell^1(S)/C_c(S)$? (A closely related question is to ask for nonzero linear functionals on $\ell^{\infty}(S)/C_0(S)$, and here I know that examples can be constructed using the Hahn-Banach theorem or the ultrafilter lemma .)",,"['linear-algebra', 'set-theory', 'axiom-of-choice']"
12,Eigenvalues of the block matrix $C=\begin{bmatrix}−I &-I\\L&0\end{bmatrix}$,Eigenvalues of the block matrix,C=\begin{bmatrix}−I &-I\\L&0\end{bmatrix},"Consider the following matrix $$C=\begin{bmatrix}−I &-I\\L&0\end{bmatrix}$$ where for $L$ we have: $$L\mathbf{1}=0$$ $$\mathbf{1}^TL=0$$ $$\text{rank}(L)=\dim(L)-1$$ $$L+L^T\geq 0$$ zero is a simple eigenvalue of $L$ and $L+L^T$ . We can show that $C$ has a zero eigenvalue and another one equal to $-1$ , what does we can say about the rest of eigenvalues? For example, we know that when $L=L^T$ , $C$ has only one simple zero and the rest of the eigenvalues have negative real parts. Numerical examples show that most of the time $C$ has only one simple zero and the rest of the eigenvalues have negative real part, but sometimes there is a pair of eigenvalues with zero real part also appearing. Under what conditions does this pair appear?","Consider the following matrix where for we have: zero is a simple eigenvalue of and . We can show that has a zero eigenvalue and another one equal to , what does we can say about the rest of eigenvalues? For example, we know that when , has only one simple zero and the rest of the eigenvalues have negative real parts. Numerical examples show that most of the time has only one simple zero and the rest of the eigenvalues have negative real part, but sometimes there is a pair of eigenvalues with zero real part also appearing. Under what conditions does this pair appear?",C=\begin{bmatrix}−I &-I\\L&0\end{bmatrix} L L\mathbf{1}=0 \mathbf{1}^TL=0 \text{rank}(L)=\dim(L)-1 L+L^T\geq 0 L L+L^T C -1 L=L^T C C,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
13,Uniform limit of finite-rank operators with the same rank.,Uniform limit of finite-rank operators with the same rank.,,"Let $\{T_n\in\mathcal{B}(X)\,|\,\text{rank}(T_n)=R\,\}^{\infty}_{n=1}$ is a sequence of linear bounded finite-rank operators on a Banach space with the same rank $R$. Let it converge uniformly to an operator $T\in\mathcal{B}(X)$, that is $\Vert T_n - T\Vert_{\mathcal{B}(X)}\longrightarrow 0\;(n \rightarrow\infty)$, that is $\forall\epsilon>0\;\exists N\in\mathbb N\;\forall n\ge N\;\forall x\in X\;|\;\Vert x\Vert_{X}\le 1$ holds $\Vert T_n x - T x\Vert_X\le\epsilon$. Prove that $\text{rank}(T)\le R$.","Let $\{T_n\in\mathcal{B}(X)\,|\,\text{rank}(T_n)=R\,\}^{\infty}_{n=1}$ is a sequence of linear bounded finite-rank operators on a Banach space with the same rank $R$. Let it converge uniformly to an operator $T\in\mathcal{B}(X)$, that is $\Vert T_n - T\Vert_{\mathcal{B}(X)}\longrightarrow 0\;(n \rightarrow\infty)$, that is $\forall\epsilon>0\;\exists N\in\mathbb N\;\forall n\ge N\;\forall x\in X\;|\;\Vert x\Vert_{X}\le 1$ holds $\Vert T_n x - T x\Vert_X\le\epsilon$. Prove that $\text{rank}(T)\le R$.",,"['linear-algebra', 'functional-analysis', 'operator-theory']"
14,Why is there no simpler form of a matrix than the Jordan or Frobenius normal form?,Why is there no simpler form of a matrix than the Jordan or Frobenius normal form?,,"The Jordan and Frobenius normal forms of a linear map $A:\Bbb R^n \rightarrow \Bbb R^n$ seem to be maximally simple representations of $A$ in the sense that one of them contains as few nonzero entries as possible. But how do you prove that? More precise, show that for every $A:\Bbb R^n \rightarrow \Bbb R^n$ and every Basis $B$ of $\Bbb R^n$, the transformation matrix $_B A _B$ has at least as many nonzero entries as the Jordan normal form or the Frobenius normal form of $A$ or, otherwise, give a counterexample.","The Jordan and Frobenius normal forms of a linear map $A:\Bbb R^n \rightarrow \Bbb R^n$ seem to be maximally simple representations of $A$ in the sense that one of them contains as few nonzero entries as possible. But how do you prove that? More precise, show that for every $A:\Bbb R^n \rightarrow \Bbb R^n$ and every Basis $B$ of $\Bbb R^n$, the transformation matrix $_B A _B$ has at least as many nonzero entries as the Jordan normal form or the Frobenius normal form of $A$ or, otherwise, give a counterexample.",,"['linear-algebra', 'matrices']"
15,Normal subgroups of the Special Linear Group,Normal subgroups of the Special Linear Group,,"What are some normal subgroups of SL $(2, \mathbb{R})$ ?  I tried to check SO $(2, \mathbb{R})$ , UT $(2, \mathbb{R})$ , linear algebraic group and some scalar and diagonal matrices, but still couldn't come up with any. So can anyone give me an idea to continue on, please?","What are some normal subgroups of SL ?  I tried to check SO , UT , linear algebraic group and some scalar and diagonal matrices, but still couldn't come up with any. So can anyone give me an idea to continue on, please?","(2, \mathbb{R}) (2, \mathbb{R}) (2, \mathbb{R})","['linear-algebra', 'abstract-algebra', 'group-theory', 'topological-groups']"
16,Proof of Cauchy-Schwarz inequality.,Proof of Cauchy-Schwarz inequality.,,"I'm trying to understand the proof of the Cauchy-Schwarz inequality: for two elements x and y of an inner product space we have $$\lvert \langle x,y\rangle\rvert \leq\lVert x \rVert \cdot\lVert y\lVert$$ The proof I am reading goes as follows: We may assume that $y\neq0$ and $\lVert y\rVert=1$. Indeed, the Cauchy-Schwarz inequality holds when y=0. If $y\neq0$ then $z=\frac{y}{\lVert y\rVert}$ has length 1. So if $\lvert \langle x,z\rangle\rvert\leq \lVert x\rVert$ holds then    $$\lvert \langle x,z\rangle\rvert=\frac{\langle x,y\rangle}{\lVert y\rVert}\leq\lVert x\rVert$$   from which $\lvert \langle x,y\rangle\rvert \leq\lVert x\rVert \cdot\lVert y\rVert$ follows. The confusing part is in bold, why does this inequality hold in general?","I'm trying to understand the proof of the Cauchy-Schwarz inequality: for two elements x and y of an inner product space we have $$\lvert \langle x,y\rangle\rvert \leq\lVert x \rVert \cdot\lVert y\lVert$$ The proof I am reading goes as follows: We may assume that $y\neq0$ and $\lVert y\rVert=1$. Indeed, the Cauchy-Schwarz inequality holds when y=0. If $y\neq0$ then $z=\frac{y}{\lVert y\rVert}$ has length 1. So if $\lvert \langle x,z\rangle\rvert\leq \lVert x\rVert$ holds then    $$\lvert \langle x,z\rangle\rvert=\frac{\langle x,y\rangle}{\lVert y\rVert}\leq\lVert x\rVert$$   from which $\lvert \langle x,y\rangle\rvert \leq\lVert x\rVert \cdot\lVert y\rVert$ follows. The confusing part is in bold, why does this inequality hold in general?",,['linear-algebra']
17,Inducing a Linear Transformation on the Quotient Space,Inducing a Linear Transformation on the Quotient Space,,"Consider $W\subseteq V$, a subspace over a field $\mathbb{F}$ and $T:V\rightarrow V$ a linear transformation with the stipulation that $T(W)\subseteq W$.  Then we have the induced linear transformation $\overline{T}:V/W \rightarrow V/W$ such that $\overline{T}=T(v)+W$. I'm supposed to show that this induced transformation is well-defined, and that given $V$ finite and $T$ an isomorphism that $\overline{T}$ is an isomorphism.  I'm having a little trouble with this part.  Namely, I want to show that the $ker(\overline{T})=W$, and I'm to the point where I realize that this means $T(v)\in W$.  How do I know there's not some random $v\in V\setminus W $ such that $ T(v)\in W$. In particular, is all this true if $V$ is not finite dimensional?  I can't immediately think of a counterexample...","Consider $W\subseteq V$, a subspace over a field $\mathbb{F}$ and $T:V\rightarrow V$ a linear transformation with the stipulation that $T(W)\subseteq W$.  Then we have the induced linear transformation $\overline{T}:V/W \rightarrow V/W$ such that $\overline{T}=T(v)+W$. I'm supposed to show that this induced transformation is well-defined, and that given $V$ finite and $T$ an isomorphism that $\overline{T}$ is an isomorphism.  I'm having a little trouble with this part.  Namely, I want to show that the $ker(\overline{T})=W$, and I'm to the point where I realize that this means $T(v)\in W$.  How do I know there's not some random $v\in V\setminus W $ such that $ T(v)\in W$. In particular, is all this true if $V$ is not finite dimensional?  I can't immediately think of a counterexample...",,"['linear-algebra', 'abstract-algebra']"
18,"Question on finite Vector Spaces, injective, surjective and if $V$ is not finite","Question on finite Vector Spaces, injective, surjective and if  is not finite",V,"Let $V$ be a vector space and $\alpha \in \operatorname{End}(V)$ (i) If $V$ is finite dimensional, then $\alpha$ is injective iff $\alpha$ is surjective. (ii) Give example showing (i) is false if $V$ is not finite dimensional. So on (i), since $V$ is finite dimensional, then $V$ has a basis with finite cardinality and hence $\dim(V)=n$. Also, the following holds, $\dim(V)=\dim(\ker(\alpha) + \dim(\operatorname{im}(\alpha))$ Since $\alpha$ is injective, then the $\ker(\alpha)$ is $0$ which implies that the $\dim(\ker(\alpha)$ is $0$ which implies that $\dim(\operatorname{im}(\alpha))=\dim(V)$ and that shows that $\alpha$ is surjective. (Is that right?) On the second one, (ii), I just have no idea. I mean shouldn't Axiom of Choice be able to work here?","Let $V$ be a vector space and $\alpha \in \operatorname{End}(V)$ (i) If $V$ is finite dimensional, then $\alpha$ is injective iff $\alpha$ is surjective. (ii) Give example showing (i) is false if $V$ is not finite dimensional. So on (i), since $V$ is finite dimensional, then $V$ has a basis with finite cardinality and hence $\dim(V)=n$. Also, the following holds, $\dim(V)=\dim(\ker(\alpha) + \dim(\operatorname{im}(\alpha))$ Since $\alpha$ is injective, then the $\ker(\alpha)$ is $0$ which implies that the $\dim(\ker(\alpha)$ is $0$ which implies that $\dim(\operatorname{im}(\alpha))=\dim(V)$ and that shows that $\alpha$ is surjective. (Is that right?) On the second one, (ii), I just have no idea. I mean shouldn't Axiom of Choice be able to work here?",,"['linear-algebra', 'vector-spaces']"
19,Eigenvalues of certain block Hermitian matrix,Eigenvalues of certain block Hermitian matrix,,"Suppose I have a special block, Hermitian matrix $$H = \begin{bmatrix} A & B \\ B^* & A^* \end{bmatrix}$$ where $*$ denotes conjugate transpose. The blocks $A$ and $B$ are themself Hermitian in this case. Are there any theorems considering the eigenvalues and eigenvectors for this special matrix?","Suppose I have a special block, Hermitian matrix where denotes conjugate transpose. The blocks and are themself Hermitian in this case. Are there any theorems considering the eigenvalues and eigenvectors for this special matrix?",H = \begin{bmatrix} A & B \\ B^* & A^* \end{bmatrix} * A B,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices', 'hermitian-matrices']"
20,"Why is the following map an isomorphism between $Cl(V,\omega)$ and $\operatorname{End}(\Lambda(V))$?",Why is the following map an isomorphism between  and ?,"Cl(V,\omega) \operatorname{End}(\Lambda(V))","Suppose you have a vector space $V$ of dimension $2n$. I know that there exists a basis $x_1,\dots,x_n,y_1,\dots,y_m$ such that $\omega(x_i,x_j)=\omega(y_i,y_j)=0$ and $\omega(x_i,y_j)=\delta_{ij}$, where $\omega$ is a bilinear symmetric nondegenerate form. Now let $W=\operatorname{span}\{x_1,\dots,x_n\}$, and $W'=\operatorname{span}\{y_1,\dots,y_n\}$. If $Cl(V,\omega)$ denotes the corresponding Clifford algebra, then one can define a $Cl(V,\omega)$-module structure on the exterior algebra $\Lambda(W)$ as follows. For $x\in W$, let $\sigma(x)\in\operatorname{End}(\Lambda(W))$ as wedge multiplication by $x$ on the left. Also, for $y\in W'$, one can define $\sigma(y)x=\omega(y,x)$ for $x\in W$, which then extends to $\sigma(y)\in\operatorname{End}(\Lambda(W))$ by $$ y(a\wedge b)=y(a)\wedge b+(-1)^{\deg a}a\wedge y(b). $$ My question is, why does the $\sigma$ defined above in fact give an isomorphism between $Cl(V,\omega)$ and $\operatorname{End}(\Lambda(W))$? I'm viewing the endomorphisms as over the underlying field $k$, by the way.","Suppose you have a vector space $V$ of dimension $2n$. I know that there exists a basis $x_1,\dots,x_n,y_1,\dots,y_m$ such that $\omega(x_i,x_j)=\omega(y_i,y_j)=0$ and $\omega(x_i,y_j)=\delta_{ij}$, where $\omega$ is a bilinear symmetric nondegenerate form. Now let $W=\operatorname{span}\{x_1,\dots,x_n\}$, and $W'=\operatorname{span}\{y_1,\dots,y_n\}$. If $Cl(V,\omega)$ denotes the corresponding Clifford algebra, then one can define a $Cl(V,\omega)$-module structure on the exterior algebra $\Lambda(W)$ as follows. For $x\in W$, let $\sigma(x)\in\operatorname{End}(\Lambda(W))$ as wedge multiplication by $x$ on the left. Also, for $y\in W'$, one can define $\sigma(y)x=\omega(y,x)$ for $x\in W$, which then extends to $\sigma(y)\in\operatorname{End}(\Lambda(W))$ by $$ y(a\wedge b)=y(a)\wedge b+(-1)^{\deg a}a\wedge y(b). $$ My question is, why does the $\sigma$ defined above in fact give an isomorphism between $Cl(V,\omega)$ and $\operatorname{End}(\Lambda(W))$? I'm viewing the endomorphisms as over the underlying field $k$, by the way.",,"['linear-algebra', 'abstract-algebra', 'multilinear-algebra']"
21,Is support of an operator same as row space?,Is support of an operator same as row space?,,Support of an operator is vector space that is orthogonal to its kernel. Does this mean support is same as row space? How to calculate support for any matrix?,Support of an operator is vector space that is orthogonal to its kernel. Does this mean support is same as row space? How to calculate support for any matrix?,,['linear-algebra']
22,An optimization problem involving orthogonal matrices,An optimization problem involving orthogonal matrices,,"Let $X\in\mathbb{R}^{3\times 3}$ be an orthogonal matrix . Then $\mathrm{vec}X\in\mathbb{R}^9$ is a 9 by 1 vector formed by stacking the columns of the matrix $X$ on top of one another. Given a matrix $A\in\mathbb{R}^{9\times 9}$, find the optimal orthogonal matrix $X$ minimizing the following objective function. $$J=\left(\mathrm{vec}X\right)^T A \mathrm{vec}X$$ I think Kronecker product may be useful for solving this problem. Does a closed-form solution exist? If not, is it possible to solve it iteratively? Thanks. EDIT ： a) Here $A$ doesn't have any special property. But it is also acceptable if solutions can be obtained by adding some properties on $A$. b) In the original problem, $X$ is constrained as a rotation matrix. But I think that would be even harder, so I put $X$ as an orthogonal matrix herein. Of course, optimal rotation matrices are better.","Let $X\in\mathbb{R}^{3\times 3}$ be an orthogonal matrix . Then $\mathrm{vec}X\in\mathbb{R}^9$ is a 9 by 1 vector formed by stacking the columns of the matrix $X$ on top of one another. Given a matrix $A\in\mathbb{R}^{9\times 9}$, find the optimal orthogonal matrix $X$ minimizing the following objective function. $$J=\left(\mathrm{vec}X\right)^T A \mathrm{vec}X$$ I think Kronecker product may be useful for solving this problem. Does a closed-form solution exist? If not, is it possible to solve it iteratively? Thanks. EDIT ： a) Here $A$ doesn't have any special property. But it is also acceptable if solutions can be obtained by adding some properties on $A$. b) In the original problem, $X$ is constrained as a rotation matrix. But I think that would be even harder, so I put $X$ as an orthogonal matrix herein. Of course, optimal rotation matrices are better.",,"['linear-algebra', 'matrices', 'optimization', 'numerical-methods']"
23,"$A,B\in M_{n}(\mathbb{R})$ so that $A>0, B>0$, prove that $\det (A+B)>\max (\det(A), \det(B))$","so that , prove that","A,B\in M_{n}(\mathbb{R}) A>0, B>0 \det (A+B)>\max (\det(A), \det(B))","Let $A,B\in M_{n}(\mathbb{R})$ be symmetric, with $A>0$ and $B>0$. I need to prove that $\det (A+B)>\max (\det(A), \det(B))$. I want to use Sylvester theorem of having a matrix $D$ so that $D=\operatorname{diag}(1,1,\ldots, 1,-1,-1, \ldots-1,0,0,\ldots,0)$. Do I need to use it? How do I use it here? Thank you.","Let $A,B\in M_{n}(\mathbb{R})$ be symmetric, with $A>0$ and $B>0$. I need to prove that $\det (A+B)>\max (\det(A), \det(B))$. I want to use Sylvester theorem of having a matrix $D$ so that $D=\operatorname{diag}(1,1,\ldots, 1,-1,-1, \ldots-1,0,0,\ldots,0)$. Do I need to use it? How do I use it here? Thank you.",,"['linear-algebra', 'inequality']"
24,Is equidistant points an open problem?,Is equidistant points an open problem?,,"This post asks whether for any $n$ -dimensional (presumably real) normed vector space, you can find $n+1$ equidistant points. They receive two answers saying that it is possible, but neither give much detail. Wikipedia states that this is an open problem, though its source is from 1971. Neither of these seem particularly convincing as to whether the problem is open or not. They also seem to contradict each other. So I would like to know whether it is an open problem whether for any $n$ -dimensional normed vector space $(\mathbb{R}^n,\|\cdot\|)$ there is always a set of $n+1$ equidistant points.","This post asks whether for any -dimensional (presumably real) normed vector space, you can find equidistant points. They receive two answers saying that it is possible, but neither give much detail. Wikipedia states that this is an open problem, though its source is from 1971. Neither of these seem particularly convincing as to whether the problem is open or not. They also seem to contradict each other. So I would like to know whether it is an open problem whether for any -dimensional normed vector space there is always a set of equidistant points.","n n+1 n (\mathbb{R}^n,\|\cdot\|) n+1","['linear-algebra', 'normed-spaces', 'open-problem']"
25,"Proving if $v, T(v)\, ..., T^{k}(v)$ are linearly dependent for every $v$, then $I, T, ..., T^{k}$ are linearly dependent.","Proving if  are linearly dependent for every , then  are linearly dependent.","v, T(v)\, ..., T^{k}(v) v I, T, ..., T^{k}","Suppose $V$ is a finite-dimensional vector space. Take a linear operator $T \in L(V)$ . Now suppose that we know for every $v \in V$ , the set of vectors $\{v, T(v)\, ..., T^{k}(v)\}$ is linearly dependent. I want to show that this would imply that the set of linear operators $\{I, T, ..., T^{k}\}$ is also linearly dependent. Here are my thoughts so far: Define $A_v = \{ p(x) \in F[x]:p(T)(v)=0\}$ where $F$ is the scalar field of $V$ . Since $A_v$ is an ideal of $F[x],$ there exists a unique monic polynomial $g_v$ , such that $g_v$ generates $A_v$ : $$\langle g_v\rangle=A_v.$$ Now define $G=\{g_v(x): v \in V\}$ . If we take $q(x)=\operatorname{lcm}(G)$ (that is, if such a $q$ exists), then it would suffice to show that $\deg(q(x)) \leq k$ . If that's proven, then $\{I, T, ..., T^{k}\}$ would be linearly dependent. Edit : This statement actually follows from the Cyclic Decomposition Theorem (Linear Algebra (Ed2), Hoffman, Kunze, p233). As a corollary to this theorem, we have that there exists a vector $\alpha \in V$ such that $g_\alpha$ is the minimal polynomial of $T$ (Again, Hoffman, p237). Now by the hypothesis, for this $\alpha$ we have a polynomial $p(x)$ of degree at most $k$ such that $p(T)(\alpha)=0$ . By the definition of $g_\alpha$ , $g_\alpha(x)$ divides $p(x)$ . We also know that $g_\alpha(x)=m_{T}(x)$ . Thus $m_{T}(x)$ divides $p(x)$ and has a degree of at most $k$ . Hence, there exists a polynomial of degree less than or equal to $k$ such that its value at $T$ would be zero, which is equivalent to what we are trying to prove. Although this completes the implication, I'm hoping to find a more elementary proof. Edit 2: Statement of the Cyclic Decomposition Theorem: Let $T$ be a linear operator on a finite-dimensional vector space $V$ and let $W_0$ be a proper $T$ -admissible subspace of $V$ . There exist non-zero vectors $\alpha_1, ..., \alpha_r$ in $V$ with respective $T$ -annihilators $p_1, ..., p_r$ such that (i) $V=W_0 \bigoplus Z(\alpha_1; T) \bigoplus ... \bigoplus Z(\alpha_r; T)$ ; (ii) $p_k$ divides $p_{k-1}$ , $k=2, ..., r$ ( $Z(\alpha_i; T)$ is the cyclic subspace of $\alpha_i$ (smallest $T$ -invariant subspace of $V$ including $\alpha_i$ )). Edit 3: By a $T$ -admissible $W$ , we mean a $T$ -invariant subspace such that for every polynomial $f(x) \in F(x)$ , if $f(T)\beta$ is in $W$ , then there exists $\gamma \in W$ such that $f(T)\beta=f(T)\gamma$ . Statement of the CDT corollary: Let $T$ be a linear operator on a finite-dimensional vector space $V$ . There exists a vector $\alpha$ in $V$ such that the $T$ -annihilator of $\alpha$ is the minimal polynomial for $T$ .","Suppose is a finite-dimensional vector space. Take a linear operator . Now suppose that we know for every , the set of vectors is linearly dependent. I want to show that this would imply that the set of linear operators is also linearly dependent. Here are my thoughts so far: Define where is the scalar field of . Since is an ideal of there exists a unique monic polynomial , such that generates : Now define . If we take (that is, if such a exists), then it would suffice to show that . If that's proven, then would be linearly dependent. Edit : This statement actually follows from the Cyclic Decomposition Theorem (Linear Algebra (Ed2), Hoffman, Kunze, p233). As a corollary to this theorem, we have that there exists a vector such that is the minimal polynomial of (Again, Hoffman, p237). Now by the hypothesis, for this we have a polynomial of degree at most such that . By the definition of , divides . We also know that . Thus divides and has a degree of at most . Hence, there exists a polynomial of degree less than or equal to such that its value at would be zero, which is equivalent to what we are trying to prove. Although this completes the implication, I'm hoping to find a more elementary proof. Edit 2: Statement of the Cyclic Decomposition Theorem: Let be a linear operator on a finite-dimensional vector space and let be a proper -admissible subspace of . There exist non-zero vectors in with respective -annihilators such that (i) ; (ii) divides , ( is the cyclic subspace of (smallest -invariant subspace of including )). Edit 3: By a -admissible , we mean a -invariant subspace such that for every polynomial , if is in , then there exists such that . Statement of the CDT corollary: Let be a linear operator on a finite-dimensional vector space . There exists a vector in such that the -annihilator of is the minimal polynomial for .","V T \in L(V) v \in V \{v, T(v)\, ..., T^{k}(v)\} \{I, T, ..., T^{k}\} A_v = \{ p(x) \in F[x]:p(T)(v)=0\} F V A_v F[x], g_v g_v A_v \langle g_v\rangle=A_v. G=\{g_v(x): v \in V\} q(x)=\operatorname{lcm}(G) q \deg(q(x)) \leq k \{I, T, ..., T^{k}\} \alpha \in V g_\alpha T \alpha p(x) k p(T)(\alpha)=0 g_\alpha g_\alpha(x) p(x) g_\alpha(x)=m_{T}(x) m_{T}(x) p(x) k k T T V W_0 T V \alpha_1, ..., \alpha_r V T p_1, ..., p_r V=W_0 \bigoplus Z(\alpha_1; T) \bigoplus ... \bigoplus Z(\alpha_r; T) p_k p_{k-1} k=2, ..., r Z(\alpha_i; T) \alpha_i T V \alpha_i T W T f(x) \in F(x) f(T)\beta W \gamma \in W f(T)\beta=f(T)\gamma T V \alpha V T \alpha T",['linear-algebra']
26,How to prove the following determinant identity,How to prove the following determinant identity,,"Prove: $$ \begin{array}{|cccccccccc|} 1 & 0 & 0 & \cdots & 0 & 1 & 0 & 0 & \cdots & 0 \\ x & x & x & \cdots & x & y & y & y & \cdots & y \\ x^{2} & 2 x^{2} & 2^{2} x^{2} & \cdots & 2^{m-1} x^{2} & y^{2} & 2 y^{2} & 2^{2} y^{2} & \cdots & 2^{m-1} y^{2} \\ x^{3} & 3 x^{3} & 3^{2} x^{3} & \cdots & 3^{m-1} x^{3} & y^{3} & 3 y^{3} & 3^{2} y^{3} & \cdots & 3^{m-1} y^{3} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ x^{n} & n x^{n} & n^{2} x^{n} & \cdots & n^{m-1} x^{n} & y^{n} & n y^{n} & n^{2} y^{n} & \cdots & n^{m-1} y^{n} \end{array}=(x-y)^{m^{2}}(x y)^{\frac{m^{2}-m}{2}}\left(\prod_{i=0}^{m-1} i !\right)^{2} $$ where $n=2m-1$ . A friend of mine gave me this problem. He claimed that he solved this by a very complicated method(which is too long to type here, and to be frank, I didn't get it at all). The following part is  my progress. First, we want to extract $y$ so that we can let $z=\frac{x}{y}$ , and then the determinant should be a polynomial $f(z)$ of variable $z$ . Thus we can use calculus to simplify it. This trick  usually works nicely on two-varible homogeneous determinant, but it doesn't kill this problem. Since we know the original problem is equivalent to $$ \begin{array}{|cccccccccc|} 1 & 0 & 0 & \cdots & 0 & 1 & 0 & 0 & \cdots & 0 \\ x & x & x & \cdots & x & 1 & 1 & 1 & \cdots & 1 \\ x^{2} & 2 x^{2} & 2^{2} x^{2} & \cdots & 2^{m-1} x^{2} & 1 & 2  & 2^{2}  & \cdots & 2^{m-1}  \\ x^{3} & 3 x^{3} & 3^{2} x^{3} & \cdots & 3^{m-1} x^{3} & 1 & 3  & 3^{2}  & \cdots & 3^{m-1}  \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ x^{n} & n x^{n} & n^{2} x^{n} & \cdots & n^{m-1} x^{n} & 1 & n  & n^{2}  & \cdots & n^{m-1}  \end{array}=(x-1)^{m^{2}}x^{\frac{m^{2}-m}{2}}\left(\prod_{i=0}^{m-1} i !\right)^{2} $$ Note $f(x)=LHS$ , calculate the derivative of $f(x)$ , we have $$ x\frac{\,\mathrm{d}}{\,\mathrm{d}x}f(x)=\begin{array}{|cccccccccc|} 1 & 0 & \cdots & 0 & 0 & 1 & 0 & 0 & \cdots & 0 \\ x & x & \cdots & x & x & 1 & 1 & 1 & \cdots & 1 \\ x^{2} & 2 x^{2} & \cdots & 2^{m-2} x^{2} & 2^{m} x^{2} & 1 & 2 & 2^{2} & \cdots & 2^{m-1} \\ x^{3} & 3 x^{3} & \cdots & 3^{m-2} x^{3} & 3^{m} x^{3} & 1 & 3 & 3^{2} & \cdots & 3^{m-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\ x^{n} & n x^{n} & \cdots & n^{m-2} x^{n} & n^{m} x^{n} & 1 & n & n^{2} & \cdots & n^{m-1} \end{array} $$ which doesn't help at all. I hope you can share some thoughts of this problem.","Prove: where . A friend of mine gave me this problem. He claimed that he solved this by a very complicated method(which is too long to type here, and to be frank, I didn't get it at all). The following part is  my progress. First, we want to extract so that we can let , and then the determinant should be a polynomial of variable . Thus we can use calculus to simplify it. This trick  usually works nicely on two-varible homogeneous determinant, but it doesn't kill this problem. Since we know the original problem is equivalent to Note , calculate the derivative of , we have which doesn't help at all. I hope you can share some thoughts of this problem.","
\begin{array}{|cccccccccc|} 1 & 0 & 0 & \cdots & 0 & 1 & 0 & 0 & \cdots & 0 \\ x & x & x & \cdots & x & y & y & y & \cdots & y \\ x^{2} & 2 x^{2} & 2^{2} x^{2} & \cdots & 2^{m-1} x^{2} & y^{2} & 2 y^{2} & 2^{2} y^{2} & \cdots & 2^{m-1} y^{2} \\ x^{3} & 3 x^{3} & 3^{2} x^{3} & \cdots & 3^{m-1} x^{3} & y^{3} & 3 y^{3} & 3^{2} y^{3} & \cdots & 3^{m-1} y^{3} \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ x^{n} & n x^{n} & n^{2} x^{n} & \cdots & n^{m-1} x^{n} & y^{n} & n y^{n} & n^{2} y^{n} & \cdots & n^{m-1} y^{n} \end{array}=(x-y)^{m^{2}}(x y)^{\frac{m^{2}-m}{2}}\left(\prod_{i=0}^{m-1} i !\right)^{2}
 n=2m-1 y z=\frac{x}{y} f(z) z 
\begin{array}{|cccccccccc|} 1 & 0 & 0 & \cdots & 0 & 1 & 0 & 0 & \cdots & 0 \\ x & x & x & \cdots & x & 1 & 1 & 1 & \cdots & 1 \\ x^{2} & 2 x^{2} & 2^{2} x^{2} & \cdots & 2^{m-1} x^{2} & 1 & 2  & 2^{2}  & \cdots & 2^{m-1}  \\ x^{3} & 3 x^{3} & 3^{2} x^{3} & \cdots & 3^{m-1} x^{3} & 1 & 3  & 3^{2}  & \cdots & 3^{m-1}  \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ x^{n} & n x^{n} & n^{2} x^{n} & \cdots & n^{m-1} x^{n} & 1 & n  & n^{2}  & \cdots & n^{m-1}  \end{array}=(x-1)^{m^{2}}x^{\frac{m^{2}-m}{2}}\left(\prod_{i=0}^{m-1} i !\right)^{2}
 f(x)=LHS f(x) 
x\frac{\,\mathrm{d}}{\,\mathrm{d}x}f(x)=\begin{array}{|cccccccccc|}
1 & 0 & \cdots & 0 & 0 & 1 & 0 & 0 & \cdots & 0 \\
x & x & \cdots & x & x & 1 & 1 & 1 & \cdots & 1 \\
x^{2} & 2 x^{2} & \cdots & 2^{m-2} x^{2} & 2^{m} x^{2} & 1 & 2 & 2^{2} & \cdots & 2^{m-1} \\
x^{3} & 3 x^{3} & \cdots & 3^{m-2} x^{3} & 3^{m} x^{3} & 1 & 3 & 3^{2} & \cdots & 3^{m-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
x^{n} & n x^{n} & \cdots & n^{m-2} x^{n} & n^{m} x^{n} & 1 & n & n^{2} & \cdots & n^{m-1}
\end{array}
","['linear-algebra', 'algebra-precalculus', 'determinant', 'multilinear-algebra']"
27,If a vector space and one subspace are both infinite-dimensional. Are they equal?,If a vector space and one subspace are both infinite-dimensional. Are they equal?,,"From linear algebra I know this theorem: If V is a finite-dimensional vector space and W is a subspace of V, then $$ i)~ dim~ W \leq dim ~V\\ ii) ~dim~ W = dim~ V \longrightarrow W=V $$ As a lemma: If $dim ~W = \infty$ $\longrightarrow$ $dim~V = \infty$ My question is: Being W a subspace of V, Is this valid? $$ dim~W=dim~V = \infty \longrightarrow W = V $$ My tentative answer is no. If V is the vector space of continuous functions, and W is the vector space of polynomials, then W is a subspace of V and is infinite-dimensional, as well as V (by the last lemma). Here $dim~W=dim~V=\infty$ but $W\neq V$ . I would like to ensure this proof is correct, thanks.","From linear algebra I know this theorem: If V is a finite-dimensional vector space and W is a subspace of V, then As a lemma: If My question is: Being W a subspace of V, Is this valid? My tentative answer is no. If V is the vector space of continuous functions, and W is the vector space of polynomials, then W is a subspace of V and is infinite-dimensional, as well as V (by the last lemma). Here but . I would like to ensure this proof is correct, thanks.","
i)~ dim~ W \leq dim ~V\\
ii) ~dim~ W = dim~ V \longrightarrow W=V
 dim ~W = \infty \longrightarrow dim~V = \infty 
dim~W=dim~V = \infty \longrightarrow W = V
 dim~W=dim~V=\infty W\neq V",['linear-algebra']
28,Basis for $\mathbb{R}^\mathbb{N}$ implies axiom of choice?,Basis for  implies axiom of choice?,\mathbb{R}^\mathbb{N},"Let $\mathbb{R}^\mathbb{N}$ denote the vector space over $\mathbb{R}$ of sequences of real numbers, with multiplication and addition defined by component. It's well-known that though the subspace $\mathbb{R}^\infty$ of sequences with only a finite number of nonzero terms has a basis $\mathbf{e}_1 = (1, 0, 0, 0, \ldots), \mathbf{e}_2 = (0, 1, 0, 0, \ldots)$ , this is not a basis of $\mathbb{R}^\mathbb{N}$ (expressing the constant sequence $(1, 1, 1, \ldots)$ would require an infinite sum $\mathbf{e}_1 + \mathbf{e}_2 + \mathbf{e}_3 + \cdots$ , and infinite sums in generic vector spaces are undefined). It's also been proved that the statement that all vector spaces have a basis is equivalent to the axiom of choice. I'm interested, though, in the specific space $\mathbb{R}^\mathbb{N}$ . Has it been proved that a basis for this set requires the axiom of choice and cannot be described explicitly? This isn't a homework question or anything; I'm just curious.","Let denote the vector space over of sequences of real numbers, with multiplication and addition defined by component. It's well-known that though the subspace of sequences with only a finite number of nonzero terms has a basis , this is not a basis of (expressing the constant sequence would require an infinite sum , and infinite sums in generic vector spaces are undefined). It's also been proved that the statement that all vector spaces have a basis is equivalent to the axiom of choice. I'm interested, though, in the specific space . Has it been proved that a basis for this set requires the axiom of choice and cannot be described explicitly? This isn't a homework question or anything; I'm just curious.","\mathbb{R}^\mathbb{N} \mathbb{R} \mathbb{R}^\infty \mathbf{e}_1 = (1, 0, 0, 0, \ldots), \mathbf{e}_2 = (0, 1, 0, 0, \ldots) \mathbb{R}^\mathbb{N} (1, 1, 1, \ldots) \mathbf{e}_1 + \mathbf{e}_2 + \mathbf{e}_3 + \cdots \mathbb{R}^\mathbb{N}","['linear-algebra', 'vector-spaces', 'set-theory', 'axiom-of-choice', 'hamel-basis']"
29,Is the product $AB$ invertible if $A$ is invertible and $B$ is non-invertible?,Is the product  invertible if  is invertible and  is non-invertible?,AB A B,"$A$ is an invertible matrix and $B$ is a non-invertible matrix. Can $AB$ be invertible? I have the following idea: Sup. $AB$ is invertible, then: $B=IB=(A^{-1}A)B=A^-1(AB)$ , then apply inverse both sides: $B^{-1}=(AB)^{-1}(A^{-1})^{-1}=(AB)^{-1}A$ , but $B$ is non-invertible (hip). This leads to a contradiction, as we supposed $AB$ is invertible. Therefore $AB$ is non-invertible. I'm not sure if the step where I apply ""inverse both sides"" is right. Otherwise I'm not sure how to prove this. Note 1: I CAN'T use $(AB)^{-1}=B^{-1}A^{-1}$ since the hypothesis for that theorem is $A, B$ invertible matrices and this is not the case. Note 2: I CAN'T use determinants yet.","is an invertible matrix and is a non-invertible matrix. Can be invertible? I have the following idea: Sup. is invertible, then: , then apply inverse both sides: , but is non-invertible (hip). This leads to a contradiction, as we supposed is invertible. Therefore is non-invertible. I'm not sure if the step where I apply ""inverse both sides"" is right. Otherwise I'm not sure how to prove this. Note 1: I CAN'T use since the hypothesis for that theorem is invertible matrices and this is not the case. Note 2: I CAN'T use determinants yet.","A B AB AB B=IB=(A^{-1}A)B=A^-1(AB) B^{-1}=(AB)^{-1}(A^{-1})^{-1}=(AB)^{-1}A B AB AB (AB)^{-1}=B^{-1}A^{-1} A, B","['linear-algebra', 'abstract-algebra', 'matrices', 'inverse']"
30,There is no continuous surjective multiplicative map from $M_n(\mathbb H)$ to $\mathbb H$,There is no continuous surjective multiplicative map from  to,M_n(\mathbb H) \mathbb H,"Let $\mathbb H$ denote the field of quaternions. I would like to prove that there does not exist any function $f:M_n(\mathbb H)\rightarrow \mathbb H$ for $n\geq 2$ that is continous surjective and multiplicative. I have been thinking about this problem for a while but I can't find any contradiction assuming that such a function does exist. I tried considering preimages for $1,i,j,k$ and toying with them, I also tried infering the values of some specific matrices (the $\lambda I_n$ , the nilpotent matrices, etc...) but I couldn't reach any conclusion. Mostly, I fail to see how to make use of the continuity here. Would somebody have a hint as to how to proceed with this problem?","Let denote the field of quaternions. I would like to prove that there does not exist any function for that is continous surjective and multiplicative. I have been thinking about this problem for a while but I can't find any contradiction assuming that such a function does exist. I tried considering preimages for and toying with them, I also tried infering the values of some specific matrices (the , the nilpotent matrices, etc...) but I couldn't reach any conclusion. Mostly, I fail to see how to make use of the continuity here. Would somebody have a hint as to how to proceed with this problem?","\mathbb H f:M_n(\mathbb H)\rightarrow \mathbb H n\geq 2 1,i,j,k \lambda I_n","['linear-algebra', 'matrices', 'quaternions']"
31,"Determine, by its action on an orthonormal basis, whether a linear operator can be continuous","Determine, by its action on an orthonormal basis, whether a linear operator can be continuous",,"We have a set of scalars $(A_{ij}\mid i,j\in\mathbb N)$ , which are supposed to be the coefficients of a continuous linear operator $A$ on a real Hilbert space, with respect to an orthonormal basis $(e_i\mid i\in\mathbb N)$ . $$x=\sum_ix_ie_i\mapsto Ax=\sum_{i,j}A_{ij}x_je_i$$ $$A_{ij}=e_i\cdot Ae_j$$ When is this possible? How can we tell from $A_{ij}$ whether this produces a well-defined continuous operator? Here are some necessary results of continuity: $$\sup_{i,j}|A_{ij}|<\infty$$ $$\forall j,\quad\lVert Ae_j\rVert^2=\sum_iA_{ij}^2<\infty$$ $$\sup_j\lVert Ae_j\rVert^2=\sup_j\sum_iA_{ij}^2<\infty$$ Here are some sufficient conditions: $$\max\{i+j\mid A_{ij}\neq0\}<\infty$$ $$\sum_{i,j}A_{ij}^2<\infty$$ $$\sum_{i,j}|A_{ij}|<\infty$$ $$\bigg(\sup_j\sum_i|A_{ij}|\bigg)\bigg(\sup_i\sum_j|A_{ij}|\bigg)<\infty$$ Is any such expression necessary and sufficient? Of course we need precisely $$\lVert A\rVert^2=\sup_{x\neq0}\frac{\lVert Ax\rVert^2}{\lVert x\rVert^2}=\sup_{x\neq0}\frac{\sum_i\left(\sum_jA_{ij}x_j\right)^2}{\sum_ix_i^2}<\infty$$ and we can take $x$ to be in the countable dense set ( $\cong c_{00}\cap\mathbb Q^\mathbb N\subset\ell^2$ ) of finite, rational, linear combinations of $e_i$ . But this seems difficult to work with. The adjoint $A^*$ has the same operator norm $\lVert A^*\rVert=\lVert A\rVert$ , so $A$ is bounded if and only if $A^*$ is bounded. And $\lVert A^*A\rVert=\lVert A\rVert^2$ , so $A$ is bounded if and only if $A^*A$ is bounded. Thus, we need only to consider symmetric positive-semidefinite operators. $$S=A^*A;\quad S_{ij}=\sum_kA_{ki}A_{kj}$$ Re la ted .","We have a set of scalars , which are supposed to be the coefficients of a continuous linear operator on a real Hilbert space, with respect to an orthonormal basis . When is this possible? How can we tell from whether this produces a well-defined continuous operator? Here are some necessary results of continuity: Here are some sufficient conditions: Is any such expression necessary and sufficient? Of course we need precisely and we can take to be in the countable dense set ( ) of finite, rational, linear combinations of . But this seems difficult to work with. The adjoint has the same operator norm , so is bounded if and only if is bounded. And , so is bounded if and only if is bounded. Thus, we need only to consider symmetric positive-semidefinite operators. Re la ted .","(A_{ij}\mid i,j\in\mathbb N) A (e_i\mid i\in\mathbb N) x=\sum_ix_ie_i\mapsto Ax=\sum_{i,j}A_{ij}x_je_i A_{ij}=e_i\cdot Ae_j A_{ij} \sup_{i,j}|A_{ij}|<\infty \forall j,\quad\lVert Ae_j\rVert^2=\sum_iA_{ij}^2<\infty \sup_j\lVert Ae_j\rVert^2=\sup_j\sum_iA_{ij}^2<\infty \max\{i+j\mid A_{ij}\neq0\}<\infty \sum_{i,j}A_{ij}^2<\infty \sum_{i,j}|A_{ij}|<\infty \bigg(\sup_j\sum_i|A_{ij}|\bigg)\bigg(\sup_i\sum_j|A_{ij}|\bigg)<\infty \lVert A\rVert^2=\sup_{x\neq0}\frac{\lVert Ax\rVert^2}{\lVert x\rVert^2}=\sup_{x\neq0}\frac{\sum_i\left(\sum_jA_{ij}x_j\right)^2}{\sum_ix_i^2}<\infty x \cong c_{00}\cap\mathbb Q^\mathbb N\subset\ell^2 e_i A^* \lVert A^*\rVert=\lVert A\rVert A A^* \lVert A^*A\rVert=\lVert A\rVert^2 A A^*A S=A^*A;\quad S_{ij}=\sum_kA_{ki}A_{kj}","['linear-algebra', 'functional-analysis', 'hilbert-spaces', 'upper-lower-bounds', 'unbounded-operators']"
32,Upper bound CP tensor rank,Upper bound CP tensor rank,,"I have a question about CP tensor ranks. In the following, $\mathcal X \in \mathbb R^{n_1 \times n_2 \times n_3}$ is a third-order tensor of CP rank $R$ , i.e., there exist vectors $a_i$ , $b_i$ and $c_i$ for $i = 1, \ldots, R$ of appropriate dimensions such that $$\mathrm{vec}(\mathcal X) = c_1 \otimes b_1 \otimes a_1 + \ldots + c_R \otimes b_R \otimes a_R.$$ The following theorem holds. Theorem. Let $R_\mu$ be the rank of the matricization $X^{(\mu)}$ of $\mathcal X$ for $\mu = 1, 2, 3$ . Then $$ \max\{R_1,R_2, R_3\} \leq R \leq \min\{R_1R_2, R_1R_3, R_2R_3\}.$$ While I have no issues with the lower bound (i.e., left), I can neither prove nor find a reference for the proof of the upper bound. I can prove that $R \leq \min\{n_1n_2, n_1n_3, n_2n_3\}$ , but that is not enough, as $R_\mu \leq n_\mu$ for $\mu = 1, 2, 3$ . EDIT: Proof of $R \leq \min\{n_1n_2, n_1n_3, n_2n_3\}$ (w.l.o.g. prove $R \leq n_1 n_2$ .) Consider $X^{(1)} \in \mathbb R^{n_1\times n_2n_3}$ the first matricization of $\mathcal X$ , and write $$ X^{(1)} = [x_1, \ldots, x_{n_2n_3}] $$ Then we have for the canonical basis vectors $e_i \in \mathbb{R}^{n_2n_3}$ $$ X^{(1)} = \sum_{i = 1}^{n_2 n_3} x_i e_i^\top $$ And by vectorization $$ \mathrm{vec}(\mathcal X) = \mathrm{vec}(X^{(1)}) = \sum_{i = 1}^{n_2 n_3} e_i \otimes x_i, $$ which implies that $R \leq n_2 n_3$ by constructing vectors $g_i$ and $h_i$ such that $g_i \otimes h_i = e_i$ .","I have a question about CP tensor ranks. In the following, is a third-order tensor of CP rank , i.e., there exist vectors , and for of appropriate dimensions such that The following theorem holds. Theorem. Let be the rank of the matricization of for . Then While I have no issues with the lower bound (i.e., left), I can neither prove nor find a reference for the proof of the upper bound. I can prove that , but that is not enough, as for . EDIT: Proof of (w.l.o.g. prove .) Consider the first matricization of , and write Then we have for the canonical basis vectors And by vectorization which implies that by constructing vectors and such that .","\mathcal X \in \mathbb R^{n_1 \times n_2 \times n_3} R a_i b_i c_i i = 1, \ldots, R \mathrm{vec}(\mathcal X) = c_1 \otimes b_1 \otimes a_1 + \ldots + c_R \otimes b_R \otimes a_R. R_\mu X^{(\mu)} \mathcal X \mu = 1, 2, 3  \max\{R_1,R_2, R_3\} \leq R \leq \min\{R_1R_2, R_1R_3, R_2R_3\}. R \leq \min\{n_1n_2, n_1n_3, n_2n_3\} R_\mu \leq n_\mu \mu = 1, 2, 3 R \leq \min\{n_1n_2, n_1n_3, n_2n_3\} R \leq n_1 n_2 X^{(1)} \in \mathbb R^{n_1\times n_2n_3} \mathcal X 
X^{(1)} = [x_1, \ldots, x_{n_2n_3}]
 e_i \in \mathbb{R}^{n_2n_3} 
X^{(1)} = \sum_{i = 1}^{n_2 n_3} x_i e_i^\top
 
\mathrm{vec}(\mathcal X) = \mathrm{vec}(X^{(1)}) = \sum_{i = 1}^{n_2 n_3} e_i \otimes x_i,
 R \leq n_2 n_3 g_i h_i g_i \otimes h_i = e_i","['linear-algebra', 'numerical-linear-algebra', 'tensors', 'matrix-rank', 'tensor-rank']"
33,Find all matrices which satisfy $M^2-3M+3I = 0$,Find all matrices which satisfy,M^2-3M+3I = 0,"I am trying to find all matrices which solve the matrix equation $$M^2 -3M +3I=0$$ Since this doesn't factor I tried expanding this in terms of the coordinates of the matrix.  It also occurs to me to put it into ""vertex"" form: $$M^2 - 3M + \frac{9}{4}I+\frac{3}{4}I=0$$ $$(M-\frac{3}{2}I)^2 = -\frac{3}{4}I$$ but this doesn't look much better. What I found from expanding by coordinates was, if $M=\pmatrix{a & b \\ c & d}$ then $$\pmatrix{a^2+bc -3a + 3& ab + bd - 3b \\ ac+cd-3c & bc+d^2-3d+3} = \pmatrix{0&0\\0&0}$$ From the off-diagonal entries I get that either $$a+d-3=0$$ or $$b=c=0$$ If $a+d-3\not=0$ then $a^2-3a+3=0$ and likewise for $d$ .  Then we get more cases for $a$ and $d$ . If $a+d-3=0$ the upper-left is unchanged and the lower-right is $$bc + (3-a)^2-3(3-a)+3 = 0$$ which simplifies to the same thing from the upper-left and so is redundant.  In the off-diagonals $$ac+c(a-3)-3c = 0 \Rightarrow $$ $$2ac-6c = 0$$ We again get cases, and I suppose after chasing cases enough you get the solution set. However, it just feels like this can't be the intended solution given how tedious and uninformative all of this case-chasing is.  Is there some bigger idea I'm missing?","I am trying to find all matrices which solve the matrix equation Since this doesn't factor I tried expanding this in terms of the coordinates of the matrix.  It also occurs to me to put it into ""vertex"" form: but this doesn't look much better. What I found from expanding by coordinates was, if then From the off-diagonal entries I get that either or If then and likewise for .  Then we get more cases for and . If the upper-left is unchanged and the lower-right is which simplifies to the same thing from the upper-left and so is redundant.  In the off-diagonals We again get cases, and I suppose after chasing cases enough you get the solution set. However, it just feels like this can't be the intended solution given how tedious and uninformative all of this case-chasing is.  Is there some bigger idea I'm missing?",M^2 -3M +3I=0 M^2 - 3M + \frac{9}{4}I+\frac{3}{4}I=0 (M-\frac{3}{2}I)^2 = -\frac{3}{4}I M=\pmatrix{a & b \\ c & d} \pmatrix{a^2+bc -3a + 3& ab + bd - 3b \\ ac+cd-3c & bc+d^2-3d+3} = \pmatrix{0&0\\0&0} a+d-3=0 b=c=0 a+d-3\not=0 a^2-3a+3=0 d a d a+d-3=0 bc + (3-a)^2-3(3-a)+3 = 0 ac+c(a-3)-3c = 0 \Rightarrow  2ac-6c = 0,"['linear-algebra', 'matrices', 'examples-counterexamples', 'matrix-equations', 'minimal-polynomials']"
34,How do we know you can only flip something’s orientation two times?,How do we know you can only flip something’s orientation two times?,,"I was exploring what the determinant’s sign means geometrically. For 2-D, you can swap the axes and you’ve flipped orientation, and once you swap them again, you get the original orientation. Why should this be the case for higher dimensions? Why is it not possible that in 3-D, I can swap i and j, then swap j and k, and I get a “third orientation”? How is orientation defined anyway?","I was exploring what the determinant’s sign means geometrically. For 2-D, you can swap the axes and you’ve flipped orientation, and once you swap them again, you get the original orientation. Why should this be the case for higher dimensions? Why is it not possible that in 3-D, I can swap i and j, then swap j and k, and I get a “third orientation”? How is orientation defined anyway?",,['linear-algebra']
35,"Geometric meaning of the quantity $|a|^2 |b|^2 |c|^2 - (a \cdot b)(b \cdot c)(c \cdot a)$ for non-coplanar vectors $a$, $b$, $c$","Geometric meaning of the quantity  for non-coplanar vectors , ,",|a|^2 |b|^2 |c|^2 - (a \cdot b)(b \cdot c)(c \cdot a) a b c,"For two non-collinear vectors, $a$ and $b$ , the quantity $$|a|^2 |b|^2 - (a \cdot b)(b \cdot a) = |a \times b|^2$$ is the square of the area of the parallelogram spanned by these two vectors. For three non-coplanar vectors, $a$ , $b$ and $c$ , we can form a similar expression $$B = |a|^2 |b|^2 |c|^2 - (a \cdot b) (b \cdot c) (c \cdot a)$$ which is not equal to the square of the volume of the parallelepiped spanned by these three vectors. Does $B$ have any geometrical (or other) meaning?","For two non-collinear vectors, and , the quantity is the square of the area of the parallelogram spanned by these two vectors. For three non-coplanar vectors, , and , we can form a similar expression which is not equal to the square of the volume of the parallelepiped spanned by these three vectors. Does have any geometrical (or other) meaning?",a b |a|^2 |b|^2 - (a \cdot b)(b \cdot a) = |a \times b|^2 a b c B = |a|^2 |b|^2 |c|^2 - (a \cdot b) (b \cdot c) (c \cdot a) B,"['linear-algebra', 'geometry', 'vectors', 'inner-products']"
36,Construct traceless symmetric tensors,Construct traceless symmetric tensors,,"I understand how to create a traceless symmetric tensor, like $$ \hat{X}_{ij} = X_{ij} - \frac{1}{N}\delta_{ij}X_{hh} $$ with Einstein convention of summing over repeated indices. (By the way, I'm following here the book ""Group Theory in a Nutshell for Physicists"", by A. Zee). But for a 3-tensor I understand that you contract by pairs of indices, but why this normalization factor? $$ \hat{X}_{ijk} = X_{ijk} - \frac{1}{N + 2}(\delta_{ij}X_{hhk} + \delta_{ik}X_{hhj} + \delta_{jk}X_{hhi}) $$ What is the pattern here? How do you generalize to even higher order tensors? Thanks in advance.","I understand how to create a traceless symmetric tensor, like $$ \hat{X}_{ij} = X_{ij} - \frac{1}{N}\delta_{ij}X_{hh} $$ with Einstein convention of summing over repeated indices. (By the way, I'm following here the book ""Group Theory in a Nutshell for Physicists"", by A. Zee). But for a 3-tensor I understand that you contract by pairs of indices, but why this normalization factor? $$ \hat{X}_{ijk} = X_{ijk} - \frac{1}{N + 2}(\delta_{ij}X_{hhk} + \delta_{ik}X_{hhj} + \delta_{jk}X_{hhi}) $$ What is the pattern here? How do you generalize to even higher order tensors? Thanks in advance.",,"['linear-algebra', 'tensors']"
37,What is the name for the third-order tensor of third-order partial derivatives?,What is the name for the third-order tensor of third-order partial derivatives?,,"Given a sufficiently nice function $$f:\mathbb R^n\rightarrow\mathbb R$$ one can define a first-order tensor of all first-order partial derivatives in the standard way to obtain the gradient , and the same kind of idea allows one to construct a second-order tensor of all second-order partial derivatives to obtain the Hessian . Is there a name for the analogously obtained third-order tensor of third-order partial derivatives ? I'm writing a bit of code that uses that object to compute the Hessian of a matrix's eigenvalues with respect to some other voodoo that's probably irrelevant to this question, and if there is a standard nomenclature I'd prefer to use it.","Given a sufficiently nice function $$f:\mathbb R^n\rightarrow\mathbb R$$ one can define a first-order tensor of all first-order partial derivatives in the standard way to obtain the gradient , and the same kind of idea allows one to construct a second-order tensor of all second-order partial derivatives to obtain the Hessian . Is there a name for the analogously obtained third-order tensor of third-order partial derivatives ? I'm writing a bit of code that uses that object to compute the Hessian of a matrix's eigenvalues with respect to some other voodoo that's probably irrelevant to this question, and if there is a standard nomenclature I'd prefer to use it.",,"['linear-algebra', 'tensors', 'matrix-calculus']"
38,Simultaneous eigenvectors of symmetric and antisymmetric parts,Simultaneous eigenvectors of symmetric and antisymmetric parts,,"This question will seem over-specific and obscure, but the motivation comes from a problem I am trying to solve in game theory. I hope someone can help, as it requires only linear algebra! Let $H$ be a real invertible matrix, decomposed into symmetric and antisymmetric parts as $H = S+A$. Assume that $S$ is positive semi-definite, and that there exists a simultaneous eigenvector $u$ of $S$ and $A$ such that $Su = 0$ and $Au = \lambda u$ with non-zero (pure imaginary) $\lambda$. Prove or disprove that $u$ is also an eigenvector of $S_d$, the sub-matrix of $S$ consisting of its diagonal part only. I can neither prove this nor find a counter-example. It is definitely true for $2 \times 2$ matrices, and I think also for $3 \times 3$. In the general case, notice that $u$ is also an eigenvalue of $H$ since $Hu = Au = \lambda u$. [The assumption that $\lambda \neq 0$ is superfluous since $H$ is assumed invertible.] I tried to use a criterion on the possibility of a matrix having pure imaginary eigenvalues, e.g. that there exists a rank-1 matrix $M$ such that $HM$ is antisymmetric ( ref ). I have also tried to use a relationship between eigenvectors and diagonal elements of a diagonalisable matrix ( ref ). I didn't get very far. EDIT: Thanks to fedja's counter-example below, the answer to this question is no. Any such $u$ is not necessarily an eigenvector of $S_d$. The question I am really interested in, however, is the following. If any two such eigenvectors $u_i$ and $u_j$ exist with distinct eigenvalues, they must be orthogonal since $A$ is anti-symmetric. Can we also prove that $u_i$ and $S_d u_j$ are orthogonal, namely $$ u_i^* S_d u_j = 0 \ ? $$ As you can see, if we could have shown that $u_i$ is an eigenvector of $S_d$, we would be done. This is not true, but fedja's counter-example does not contradict this more restrictive claim.","This question will seem over-specific and obscure, but the motivation comes from a problem I am trying to solve in game theory. I hope someone can help, as it requires only linear algebra! Let $H$ be a real invertible matrix, decomposed into symmetric and antisymmetric parts as $H = S+A$. Assume that $S$ is positive semi-definite, and that there exists a simultaneous eigenvector $u$ of $S$ and $A$ such that $Su = 0$ and $Au = \lambda u$ with non-zero (pure imaginary) $\lambda$. Prove or disprove that $u$ is also an eigenvector of $S_d$, the sub-matrix of $S$ consisting of its diagonal part only. I can neither prove this nor find a counter-example. It is definitely true for $2 \times 2$ matrices, and I think also for $3 \times 3$. In the general case, notice that $u$ is also an eigenvalue of $H$ since $Hu = Au = \lambda u$. [The assumption that $\lambda \neq 0$ is superfluous since $H$ is assumed invertible.] I tried to use a criterion on the possibility of a matrix having pure imaginary eigenvalues, e.g. that there exists a rank-1 matrix $M$ such that $HM$ is antisymmetric ( ref ). I have also tried to use a relationship between eigenvectors and diagonal elements of a diagonalisable matrix ( ref ). I didn't get very far. EDIT: Thanks to fedja's counter-example below, the answer to this question is no. Any such $u$ is not necessarily an eigenvector of $S_d$. The question I am really interested in, however, is the following. If any two such eigenvectors $u_i$ and $u_j$ exist with distinct eigenvalues, they must be orthogonal since $A$ is anti-symmetric. Can we also prove that $u_i$ and $S_d u_j$ are orthogonal, namely $$ u_i^* S_d u_j = 0 \ ? $$ As you can see, if we could have shown that $u_i$ is an eigenvector of $S_d$, we would be done. This is not true, but fedja's counter-example does not contradict this more restrictive claim.",,['linear-algebra']
39,Can a vector space have more than one definition of an inner product?,Can a vector space have more than one definition of an inner product?,,"If so, can I define an inner product in our usual vector space where two vectors orthogonal with respect to the dot product are no longer orthogonal in the new inner product I define?","If so, can I define an inner product in our usual vector space where two vectors orthogonal with respect to the dot product are no longer orthogonal in the new inner product I define?",,"['linear-algebra', 'vector-spaces']"
40,Geometric Proof of Perron-Frobenius,Geometric Proof of Perron-Frobenius,,"I am reading this paper ( A Geometric Proof of the Perron-Frobenius Theorem , A. Borobia, U. R. Trias, Revista Mathematica de la Universidad Conplutense de Madrid, Vol. 5, 1992) where a short geometric proof of the Perron-Frobenius theorem is given. I am having trouble at one place, which I articulate below. I sacrifice some generality in service of simplicity. Let $A$ be an $n\times n$ matrix with positive real entries and $T:\mathbf R^n\to \mathbf R^n$ be the linear map whose matrix representation with respect to the standard basis is same as $A$ . Define $$C=\{(x_1, \ldots, x_n)\in \mathbf R^n:\ x_i>0\}, \quad \bar C=\{(x_1, \ldots, x_n)\in \mathbf R^n:\ x_i\geq 0\}$$ The following are true: Theorem 1. There is a positive eigenvalue of $T$ with corresponding eigenvector in $C$ . Theorem 2. If $\lambda$ is an eigenvalue as in Theorem 1, then the geometric multiplicity of $\lambda$ is $1$ . Theorem 3. If $\lambda$ is an eigenvalue as in Theorem 1, then the algebraic multiplicity of $\lambda$ is $1$ . (Clearly, Theorem $3$ subsumes Theorem 2.) Theorem 1 can be proved using Brouwer's fixed point theorem (BFPT). We notice that if $R$ is the collection of all rays in $\mathbf R^n$ of the form $\{av:\ a\geq 0\}$ for some $v\in \mathbf R^n$ having all entries non-negative, then $R$ is fixed, as a set, by $T$ . But $R$ is homeomorphic to the $n-1$ disc, and thus by BFPT we see that there is some ray in $R$ that is fixed by $T$ . This immediately gives $1$ . (The fixed ray cannot lie in $\partial C$ because all entries of $A$ are positive.) For Theorem 2 we argue as follows. Let $\lambda$ be a positive eigenvalue of $T$ with $v$ as a corresponding eigenvector, all of whose entries are positive. If the geometric multiplicity of $\lambda$ is not $1$ , then there is a vector $u\notin \text{span}(v)$ with $Tu=\lambda u$ . Let $V$ be the plane spanned by $u$ and $v$ . Each ray in $V$ is fixed by $T$ . But there is a ray in $V$ spanned by a vector in $\partial C$ , which cannot remain fixed under $T$ , giving a contradiction. I am stuck with the proof of Theorem 3 . The proof in the above-cited paper proceeds as follows. Let $\lambda$ be a positive eigenvalue with the corresponding eigenvector $v$ having all entries positive. Assume that the algebraic multiplicity of $\lambda$ is more than $1$ . Then we can find a $T$ -invariant plane $U$ containing $\text{span}(v)$ . Let $S^1$ be identified with the set of rays in $U$ . Let $r$ and $-r$ denote the rays spanned by $v$ and $-v$ respectively. By Theorme 2, $S^1$ is not point-wise fixed under the action of $T$ . And here is what I don't follow: The set of points in $S^1$ which are fixed by the action of $T^2$ does not consist only of $r$ and $-r$ . Other wise the dynamics of the action of $T^2$ over $S^1$ looks like this Here $L$ is the arc of $S^1$ formed by the intersection of $S^1$ with the set of rays in $\bar C$ . EDIT: Why $T^2$ needs to have a fixed point apart from $r$ and $-r$ can be argued as follows: Assume on the contrary. Note that $T^2$ is orientation preserving. So the arc ""above"" the points $-r$ and $r$ (including $-r$ and $r$ ) is mapped to itself under $T^2$ . Since this is homeomorphic to the closed interval, either all points in the open arc converge to $r$ under iterates of $T^2$ , or all points of the open arc converge to $-r$ under iterates of $T^2$ . But the sets $C$ and $-C:=\{-x:\ x\in C\}$ are invariant under $T^2$ so we get a contradiction. Can somebody explain this last piece of reasoning. And how is it helping us deduce that the algebraic multiplicity of $\lambda$ is $1$ . This argument is at the bottom of the second page of the paper I mentioned.","I am reading this paper ( A Geometric Proof of the Perron-Frobenius Theorem , A. Borobia, U. R. Trias, Revista Mathematica de la Universidad Conplutense de Madrid, Vol. 5, 1992) where a short geometric proof of the Perron-Frobenius theorem is given. I am having trouble at one place, which I articulate below. I sacrifice some generality in service of simplicity. Let be an matrix with positive real entries and be the linear map whose matrix representation with respect to the standard basis is same as . Define The following are true: Theorem 1. There is a positive eigenvalue of with corresponding eigenvector in . Theorem 2. If is an eigenvalue as in Theorem 1, then the geometric multiplicity of is . Theorem 3. If is an eigenvalue as in Theorem 1, then the algebraic multiplicity of is . (Clearly, Theorem subsumes Theorem 2.) Theorem 1 can be proved using Brouwer's fixed point theorem (BFPT). We notice that if is the collection of all rays in of the form for some having all entries non-negative, then is fixed, as a set, by . But is homeomorphic to the disc, and thus by BFPT we see that there is some ray in that is fixed by . This immediately gives . (The fixed ray cannot lie in because all entries of are positive.) For Theorem 2 we argue as follows. Let be a positive eigenvalue of with as a corresponding eigenvector, all of whose entries are positive. If the geometric multiplicity of is not , then there is a vector with . Let be the plane spanned by and . Each ray in is fixed by . But there is a ray in spanned by a vector in , which cannot remain fixed under , giving a contradiction. I am stuck with the proof of Theorem 3 . The proof in the above-cited paper proceeds as follows. Let be a positive eigenvalue with the corresponding eigenvector having all entries positive. Assume that the algebraic multiplicity of is more than . Then we can find a -invariant plane containing . Let be identified with the set of rays in . Let and denote the rays spanned by and respectively. By Theorme 2, is not point-wise fixed under the action of . And here is what I don't follow: The set of points in which are fixed by the action of does not consist only of and . Other wise the dynamics of the action of over looks like this Here is the arc of formed by the intersection of with the set of rays in . EDIT: Why needs to have a fixed point apart from and can be argued as follows: Assume on the contrary. Note that is orientation preserving. So the arc ""above"" the points and (including and ) is mapped to itself under . Since this is homeomorphic to the closed interval, either all points in the open arc converge to under iterates of , or all points of the open arc converge to under iterates of . But the sets and are invariant under so we get a contradiction. Can somebody explain this last piece of reasoning. And how is it helping us deduce that the algebraic multiplicity of is . This argument is at the bottom of the second page of the paper I mentioned.","A n\times n T:\mathbf R^n\to \mathbf R^n A C=\{(x_1, \ldots, x_n)\in \mathbf R^n:\ x_i>0\}, \quad \bar C=\{(x_1, \ldots, x_n)\in \mathbf R^n:\ x_i\geq 0\} T C \lambda \lambda 1 \lambda \lambda 1 3 R \mathbf R^n \{av:\ a\geq 0\} v\in \mathbf R^n R T R n-1 R T 1 \partial C A \lambda T v \lambda 1 u\notin \text{span}(v) Tu=\lambda u V u v V T V \partial C T \lambda v \lambda 1 T U \text{span}(v) S^1 U r -r v -v S^1 T S^1 T^2 r -r T^2 S^1 L S^1 S^1 \bar C T^2 r -r T^2 -r r -r r T^2 r T^2 -r T^2 C -C:=\{-x:\ x\in C\} T^2 \lambda 1","['linear-algebra', 'matrices', 'algebraic-topology', 'markov-chains', 'positive-matrices']"
41,What is the use/significance of Farkas' lemma?,What is the use/significance of Farkas' lemma?,,"I worked on an exercise to prove Farkas' lemma, which states that for $A \in \mathbb{R}^{m,n}$ and $b \in \mathbb{R}^n$ exactly one of the following is true: There exists $x \ge 0$ such that $Ax=b$. There exists $y$ such that $A^T y \ge 0$ and $y^T b < 0$. This is simply a trivial fact about the separation between two closed convex sets: $S_1 = \{ b \}$ and $S_2 = \{ Ax \mid x \ge 0 \}$. Given its simplicity, there must be some broader significance or application of this fact. Can anyone enlighten me as to what it is?","I worked on an exercise to prove Farkas' lemma, which states that for $A \in \mathbb{R}^{m,n}$ and $b \in \mathbb{R}^n$ exactly one of the following is true: There exists $x \ge 0$ such that $Ax=b$. There exists $y$ such that $A^T y \ge 0$ and $y^T b < 0$. This is simply a trivial fact about the separation between two closed convex sets: $S_1 = \{ b \}$ and $S_2 = \{ Ax \mid x \ge 0 \}$. Given its simplicity, there must be some broader significance or application of this fact. Can anyone enlighten me as to what it is?",,"['linear-algebra', 'convex-analysis', 'linear-programming']"
42,Linear system $Ax=0$ with invertible $A$ has unique solution $x=0$?,Linear system  with invertible  has unique solution ?,Ax=0 A x=0,"If $Ax=0$ and $A$ is an invertible matrix, then $x=0$ is the unique solution? I think it is a very basic question but I forget some knowledge in linear algebra. For an invertible $A$, the inverse $A^{-1}$ is well-defined, thus we can left-multiply $A^{-1}$ to each side of the equation to get $A^{-1}Ax=A^{-1}0$, which leads to $Ix=0$ and thus $x=0$. I think this calculation says $x=0$ must be a solution but dose not guarantee the uniqueness. It is not a proof.","If $Ax=0$ and $A$ is an invertible matrix, then $x=0$ is the unique solution? I think it is a very basic question but I forget some knowledge in linear algebra. For an invertible $A$, the inverse $A^{-1}$ is well-defined, thus we can left-multiply $A^{-1}$ to each side of the equation to get $A^{-1}Ax=A^{-1}0$, which leads to $Ix=0$ and thus $x=0$. I think this calculation says $x=0$ must be a solution but dose not guarantee the uniqueness. It is not a proof.",,"['linear-algebra', 'matrices']"
43,The first fundamental form is enough to determine the geometry of a hypersurface?,The first fundamental form is enough to determine the geometry of a hypersurface?,,"Question: As we know, For two dimensional surfaces there are many examples for which their first fundamental forms are the same, but their second fundamental forms are not. However, it seems that for hypersurfaces (dimension $\ge 3$) their first fundamental forms are enough to determine the geometry of the hypersurfaces completely, provided that $Rank(L) \ge 3$, where $L$ is the shape operator. I want to know whether this is true or not. If the answer is yes, why? Definitions: The first fundamental form $I$ of a surface element is just the restriction of the Euclidean inner product in $\mathbb R^n$ to all tangent hyperplanes $T_uf$, i.e., $$I(X,Y):=\langle X,Y \rangle$$ for any two tangent vectors $X,Y \in T_uf$ or for vectors $X,Y \in \mathbb R^n$ which are tangent to the surface element$.^1$ Shape operator of a surface is the minus derivative of the unit normal vectors on the surface. Formally speaking, let $f:U \to \mathbb {R}^3$ be a surface element with unit normal vector map $\nu$, $\nu: U \to S^2$ is defined by $$\nu (u_1,u_2):=\frac{\frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2}}{\left \Vert \frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2} \right \Vert},$$ then for every $u\in U$ we have the linear map $$D\nu|_u:T_uU \to T_uf,$$ where $T_uU=\{u\} \times \mathbb R^2$ and $T_uf=Df|_u\left(T_uU\right)$, and $$Df|_u:T_uU \to T_uf$$ is a linear isomorphism. Then the shape operator $L:=-D\nu \circ (Df)^{-1}$ is defined pointwise by $$L_u:=-\left(D\nu|_u \right) \circ \left(Df|_u\right)^{-1}:T_uf \to T_uf\,.^2$$ The above definition can be easily generalized to the general $\mathbb R^n$ space. Let $f:U \to \mathbb R^3$ be given. Then for tangent vectors $X$ and $Y$, one defines: the second fundamental form $I\!I$ of $f$ by $$I\!I(X,Y):=I(LX,Y),$$ where $L$ is the shape operator$.^3$ The above definition can be easily generalized to the general $\mathbb R^n$ space. [1], [2], [3] Wolfgang Kühnel, ""Differential Geometry Curves-Surfaces-Manifolds"", Second Edition, American Mathematical Society, 2006.","Question: As we know, For two dimensional surfaces there are many examples for which their first fundamental forms are the same, but their second fundamental forms are not. However, it seems that for hypersurfaces (dimension $\ge 3$) their first fundamental forms are enough to determine the geometry of the hypersurfaces completely, provided that $Rank(L) \ge 3$, where $L$ is the shape operator. I want to know whether this is true or not. If the answer is yes, why? Definitions: The first fundamental form $I$ of a surface element is just the restriction of the Euclidean inner product in $\mathbb R^n$ to all tangent hyperplanes $T_uf$, i.e., $$I(X,Y):=\langle X,Y \rangle$$ for any two tangent vectors $X,Y \in T_uf$ or for vectors $X,Y \in \mathbb R^n$ which are tangent to the surface element$.^1$ Shape operator of a surface is the minus derivative of the unit normal vectors on the surface. Formally speaking, let $f:U \to \mathbb {R}^3$ be a surface element with unit normal vector map $\nu$, $\nu: U \to S^2$ is defined by $$\nu (u_1,u_2):=\frac{\frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2}}{\left \Vert \frac{\partial f}{\partial u_1} \times \frac{\partial f}{\partial u_2} \right \Vert},$$ then for every $u\in U$ we have the linear map $$D\nu|_u:T_uU \to T_uf,$$ where $T_uU=\{u\} \times \mathbb R^2$ and $T_uf=Df|_u\left(T_uU\right)$, and $$Df|_u:T_uU \to T_uf$$ is a linear isomorphism. Then the shape operator $L:=-D\nu \circ (Df)^{-1}$ is defined pointwise by $$L_u:=-\left(D\nu|_u \right) \circ \left(Df|_u\right)^{-1}:T_uf \to T_uf\,.^2$$ The above definition can be easily generalized to the general $\mathbb R^n$ space. Let $f:U \to \mathbb R^3$ be given. Then for tangent vectors $X$ and $Y$, one defines: the second fundamental form $I\!I$ of $f$ by $$I\!I(X,Y):=I(LX,Y),$$ where $L$ is the shape operator$.^3$ The above definition can be easily generalized to the general $\mathbb R^n$ space. [1], [2], [3] Wolfgang Kühnel, ""Differential Geometry Curves-Surfaces-Manifolds"", Second Edition, American Mathematical Society, 2006.",,"['linear-algebra', 'geometry']"
44,Determinant inequality about Toeplitz matrix,Determinant inequality about Toeplitz matrix,,"Given, Toeplitz matrix $T \in R^{n \times n}$ : $$         T=         \begin{bmatrix}         \tau_0 & \tau_1 & \cdots & \tau_{n-1} \\         \tau_1 & \tau_0 & \ddots & \vdots \\         \vdots & \ddots & \ddots & \tau_1 \\         \tau_{n-1} & \cdots & \tau_1 & \tau_0 \\         \end{bmatrix} $$ and denoted the $k$ -th order leading principal submatrix of $T$ by $T_k$ . If $T$ is a positive definite matrix, how to prove the following inequality: $$         \det T_{k+1} \le \frac{(\det T_k)^2}{\det T_{k-1}} $$ ,where $\forall k \in \{1, \cdots, n\}$ . And, when the equality is attained?","Given, Toeplitz matrix : and denoted the -th order leading principal submatrix of by . If is a positive definite matrix, how to prove the following inequality: ,where . And, when the equality is attained?","T \in R^{n \times n} 
        T=
        \begin{bmatrix}
        \tau_0 & \tau_1 & \cdots & \tau_{n-1} \\
        \tau_1 & \tau_0 & \ddots & \vdots \\
        \vdots & \ddots & \ddots & \tau_1 \\
        \tau_{n-1} & \cdots & \tau_1 & \tau_0 \\
        \end{bmatrix}
 k T T_k T 
        \det T_{k+1} \le \frac{(\det T_k)^2}{\det T_{k-1}}
 \forall k \in \{1, \cdots, n\}","['linear-algebra', 'matrices', 'inequality', 'determinant', 'toeplitz-matrices']"
45,How to prove the orthogonality of the Fourier basis from the dot product?,How to prove the orthogonality of the Fourier basis from the dot product?,,"In order to prove the discrete fourier basis $$ w^{(k)}_n = e^{-j\frac{2\pi}{N}nk}$$ is orthogonal, the following was stated But I am confused why it is 0 when $k \neq h$ , How can the sum from $n=0$ to $ N-1 $ be $0$ ? I tried substituting values for $h-k$ by $2$ , and $N$ by $5$ , but in the summation there is always that $e^0$ which equals to 1 and no negative term to cancel it. Can someone clear this please?","In order to prove the discrete fourier basis is orthogonal, the following was stated But I am confused why it is 0 when , How can the sum from to be ? I tried substituting values for by , and by , but in the summation there is always that which equals to 1 and no negative term to cancel it. Can someone clear this please?", w^{(k)}_n = e^{-j\frac{2\pi}{N}nk} k \neq h n=0  N-1  0 h-k 2 N 5 e^0,"['linear-algebra', 'fourier-analysis']"
46,Orthogonal complement to subspace of $n \times n$ matrices with trace = $0$,Orthogonal complement to subspace of  matrices with trace =,n \times n 0,"For an inner product space $$W = \{A \in M_{n \times n}(\mathbb{R}) \mid tr(A)=0\}$$ with inner product $$\langle A,B\rangle:=tr(A^TB)=\sum_{i=1}^n\sum_{j=1}^na_{ij}b_{ij}$$ find $W^{\perp}$ . I know that $$\dim(W) = n^2 -1$$ and that $$W \oplus W^{\perp} = M$$ But I'm not sure of how to proceed. Would you be able to point me in the right direction? Many thanks!",For an inner product space with inner product find . I know that and that But I'm not sure of how to proceed. Would you be able to point me in the right direction? Many thanks!,"W = \{A \in M_{n \times n}(\mathbb{R}) \mid tr(A)=0\} \langle A,B\rangle:=tr(A^TB)=\sum_{i=1}^n\sum_{j=1}^na_{ij}b_{ij} W^{\perp} \dim(W) = n^2 -1 W \oplus W^{\perp} = M","['linear-algebra', 'orthogonality']"
47,Showing that $T:V\to V$ has a cyclic vector if its eigenspaces all have dimension one.,Showing that  has a cyclic vector if its eigenspaces all have dimension one.,T:V\to V,"Let $V$ be an $n$-dimensional complex vector space and $T:V\to V$. Suppose that $$\{v\in V: Tv = \lambda v\}$$ has dimension $1$ or $0$ for all $\lambda\in \mathbb{C}$. Show that there exists some $w\in V$ such that $\{w,Tw,\dots, T^{n-1}w\}$ is linearly independent. I tried using the canonical forms to answer this question but I don't think that's the right way to go.","Let $V$ be an $n$-dimensional complex vector space and $T:V\to V$. Suppose that $$\{v\in V: Tv = \lambda v\}$$ has dimension $1$ or $0$ for all $\lambda\in \mathbb{C}$. Show that there exists some $w\in V$ such that $\{w,Tw,\dots, T^{n-1}w\}$ is linearly independent. I tried using the canonical forms to answer this question but I don't think that's the right way to go.",,"['linear-algebra', 'abstract-algebra']"
48,Why is there no natural isomorphism between $V$ and its dual?,Why is there no natural isomorphism between  and its dual?,V,"Question While looking over the exercise $3.F-34$ in Linear Algebra Done Right , I encountered the following paragraph Suppose $V$ is finite dimensional. Then $V$ and $V'$ are isomorphic, but finding an isomorphism from $V$ onto $V'$ , generally, requires choosing a basis of $V$ . In contrast, the isomorphism from $V$ to $V''$ does not require a choice of basis and thus is considered more natural . and these questions showed up in my mind: $1$ . Does the word natural just means that we don't need to choose a basis? I have seen the word canonical is used in the similar manner too. Is there a more precise definition for natural or canonical ? $2$ . Assuming the answer to question $1$ is Yes , then why there is no natural isomorphism from $V$ onto $V'$ ? $3$ . I think that there is a relation between the answer to question $2$ and the proof of Riesz representation theorem. So, if we cannot find a natural isomorphism between $V$ and $V'$ then it means that we cannot prove Riesz representation theorem without choosing a basis of $V$ . Is this true? Complementary Information The Isomorphism from $V$ onto $V^{''}$ . Suppose $V$ is a finite dimensional vector space. Consider the following map $$ \Lambda(v)(\phi)=\phi(v),  \qquad \forall v \in V, \,\, \forall \phi \in V^{'} $$ then $\Lambda$ is an isomorphism from $V$ onto $V''$ . Riesz Representation Theorem . Suppose $V$ is a finite dimensional linear space equipped with an inner product and $\phi$ is a linear functional on $V$ . Then there is a unique vector $v_0 \in V$ such that $$\phi(v) = {\langle v,v_0 \rangle}_{V}, \qquad \forall v \in V$$ Other Related Posts I found the following posts related to this question on MSE and MO. Post $1$ , Post $2$ , Post $3$ , Post $4$ .","Question While looking over the exercise in Linear Algebra Done Right , I encountered the following paragraph Suppose is finite dimensional. Then and are isomorphic, but finding an isomorphism from onto , generally, requires choosing a basis of . In contrast, the isomorphism from to does not require a choice of basis and thus is considered more natural . and these questions showed up in my mind: . Does the word natural just means that we don't need to choose a basis? I have seen the word canonical is used in the similar manner too. Is there a more precise definition for natural or canonical ? . Assuming the answer to question is Yes , then why there is no natural isomorphism from onto ? . I think that there is a relation between the answer to question and the proof of Riesz representation theorem. So, if we cannot find a natural isomorphism between and then it means that we cannot prove Riesz representation theorem without choosing a basis of . Is this true? Complementary Information The Isomorphism from onto . Suppose is a finite dimensional vector space. Consider the following map then is an isomorphism from onto . Riesz Representation Theorem . Suppose is a finite dimensional linear space equipped with an inner product and is a linear functional on . Then there is a unique vector such that Other Related Posts I found the following posts related to this question on MSE and MO. Post , Post , Post , Post .","3.F-34 V V V' V V' V V V'' 1 2 1 V V' 3 2 V V' V V V^{''} V 
\Lambda(v)(\phi)=\phi(v),  \qquad \forall v \in V, \,\, \forall \phi \in V^{'}
 \Lambda V V'' V \phi V v_0 \in V \phi(v) = {\langle v,v_0 \rangle}_{V}, \qquad \forall v \in V 1 2 3 4","['linear-algebra', 'duality-theorems', 'dual-spaces']"
49,Stable resolution of a $2\times2$ linear system,Stable resolution of a  linear system,2\times2,"Cramer’s method for the resolution of linear systems is known to be unstable, even in the $2\times2$ case. For general systems, stability can be improved by partial or full pivoting. When you transpose the full pivoting principle to a $2\times2$ , the procedure essentially amounts to finding the L.H.S. coefficient with the largest magnitude, let it be $a_{11}$ W.L.O.G.; computing $x_2$ by determinants*, computing $x_1$ by elimination of $x_2$ from equation $1$ . Can this improve stability? Is there a more stable solution? *Whatever the choice of the pivot, the formula amounts to a ratio of $2\times2$ determinants. I wonder if first normalizing the pivot coefficient to $1$ makes any difference. $$\frac{b_1-b_2\dfrac{a_{21}}{a_{11}}}{a_{22}-a_{12}\dfrac{a_{21}}{a_{11}}}\text{ vs. }\frac{b_1a_{11}-b_2a_{21}}{a_{11}a_{22}-a_{12}a_{21}}$$","Cramer’s method for the resolution of linear systems is known to be unstable, even in the case. For general systems, stability can be improved by partial or full pivoting. When you transpose the full pivoting principle to a , the procedure essentially amounts to finding the L.H.S. coefficient with the largest magnitude, let it be W.L.O.G.; computing by determinants*, computing by elimination of from equation . Can this improve stability? Is there a more stable solution? *Whatever the choice of the pivot, the formula amounts to a ratio of determinants. I wonder if first normalizing the pivot coefficient to makes any difference.",2\times2 2\times2 a_{11} x_2 x_1 x_2 1 2\times2 1 \frac{b_1-b_2\dfrac{a_{21}}{a_{11}}}{a_{22}-a_{12}\dfrac{a_{21}}{a_{11}}}\text{ vs. }\frac{b_1a_{11}-b_2a_{21}}{a_{11}a_{22}-a_{12}a_{21}},['linear-algebra']
50,Find the eigenvalues of a 3 x 3 matrix,Find the eigenvalues of a 3 x 3 matrix,,"I have a question on determining eigenvalues for a given matrix A: $$ A=  \begin{bmatrix}  2 & 1 & 2 \\  0 & 2 & -1 \\  0 & 1 & 0 \\  \end{bmatrix} $$ Here's what I have so far: \begin{align} & det(A-\lambda I_3) = 0\\ & =det \begin{bmatrix}  2-\lambda & 1 & 2 \\  0 & 2-\lambda & -1 \\  0 & 1 & 0-\lambda \\  \end{bmatrix} = 0 \\ & = (2-\lambda) \begin{vmatrix}  2-\lambda & -1 \\  1 & 0-\lambda \\  \end{vmatrix} \\ & = (2-\lambda)(-2\lambda+\lambda^2+1) \\ & = (2-\lambda)(\lambda-1)^2 = 0\end{align} So my eigenvalues are $2$ and $1$. I am almost postitive this is correct. Here's my confusion/question. I know that the determinant of an upper triangular matrix is the product of the terms along the diagonal. Some of my classmates used a different approach where from the beginning they row reduced $A$ into an upper triangular matrix by eliminating the $1$ from the $3$rd row, then subtracted $\lambda I$ and then instead of solving the determinant like I did they just multiplied the diagonal terms together. The problem I see with this is that you can obtain different matrices depending on how you choose to row reduce, which means you can get different eigenvalues. Is it possible to solve for the eigenvalues by using the approach of making $A$ into an upper triangular matrix first? (So that you can solve the determinant by multiplying the diagonal entries) I hope that makes sense. Thanks!","I have a question on determining eigenvalues for a given matrix A: $$ A=  \begin{bmatrix}  2 & 1 & 2 \\  0 & 2 & -1 \\  0 & 1 & 0 \\  \end{bmatrix} $$ Here's what I have so far: \begin{align} & det(A-\lambda I_3) = 0\\ & =det \begin{bmatrix}  2-\lambda & 1 & 2 \\  0 & 2-\lambda & -1 \\  0 & 1 & 0-\lambda \\  \end{bmatrix} = 0 \\ & = (2-\lambda) \begin{vmatrix}  2-\lambda & -1 \\  1 & 0-\lambda \\  \end{vmatrix} \\ & = (2-\lambda)(-2\lambda+\lambda^2+1) \\ & = (2-\lambda)(\lambda-1)^2 = 0\end{align} So my eigenvalues are $2$ and $1$. I am almost postitive this is correct. Here's my confusion/question. I know that the determinant of an upper triangular matrix is the product of the terms along the diagonal. Some of my classmates used a different approach where from the beginning they row reduced $A$ into an upper triangular matrix by eliminating the $1$ from the $3$rd row, then subtracted $\lambda I$ and then instead of solving the determinant like I did they just multiplied the diagonal terms together. The problem I see with this is that you can obtain different matrices depending on how you choose to row reduce, which means you can get different eigenvalues. Is it possible to solve for the eigenvalues by using the approach of making $A$ into an upper triangular matrix first? (So that you can solve the determinant by multiplying the diagonal entries) I hope that makes sense. Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
51,Convert a piecewise linear non-convex function into a linear optimisation problem.,Convert a piecewise linear non-convex function into a linear optimisation problem.,,"Update: Problem and solution found here (p. 17, 61), although my prof's solution (formulation) is different. Convert $$\min z = f(x)$$ where $$f(x) = \left\{\begin{matrix} 1-x, & 0 \le x < 1\\  x-1, & 1 \le x < 2\\  \frac{x}{2}, &  2 \le x \le 3 \end{matrix}\right.$$ s.t. $$x \ge 0$$ into a linear integer programming problem. What I tried: It seems that according to this , $$\min \ f(x) = \max(a_1x + b_1, a_2x + b_2, ..., a_mx + b_m), x \ge 0$$ is equivalent to $$\min \ t \ \text{s.t.}$$ $$a_1x + b_1 \le t$$ $$a_2x + b_2 \le t$$ $$\vdots$$ $$a_mx + b_m \le t$$ $$x \ge 0$$ Following that, I tried $$g(x) = \max(1-x, x-1, x/2) = \left\{\begin{matrix} 1-x, & 0 \le x \le 2/3\\  x-1, & x \ge 2\\  \frac{x}{2}, &  2/3 \le x \le 2 \end{matrix}\right.$$ If we allow only integer $x$, then we have $$g(x) = \max(1-x, x-1, x/2) = \left\{\begin{matrix} 1-x, & x=0\\  x-1, & x=2\\  \frac{x}{2}, &  x=1 or 2 \end{matrix}\right.$$ If we allow only integer $x$ for $f$, then we have $$f(x) = \left\{\begin{matrix} 1-x, & x=0\\  x-1, & x=1\\  \frac{x}{2}, &  x=2 or 3 \end{matrix}\right.$$ It doesn't look like $f(x) = g(x)$, w/ or w/o integer constraint. How can I approach this? (The following is copied from an answer I deleted and comments on it) Prof's answer (assuming I remembered question right): Let Xi = X for ith constraint. Minimise $$z = (1-x_1)+(x_2-1)+(1/2)(x_3)$$ s.t. $$0 \le x_1 \le y_1$$ $$y_2 \le x_2 \le 2y_2$$ $$2y_3 \le x_3 \le 3y_3$$ $$y_1, y_2, y_3 \in \{0,1\}$$ $$x_1,x_2,x_3 \ge 0$$ Comments below it: (You should probably annotate this to indicate the source.) Anyway, if you suspected something was off about this solution, then you're right: it's incorrect. This formulation is similar to Kuifje's Option 3, but it incorrectly encodes the objective function. The minimum value occurs at $(y_1,y_2,y_3)=(1,0,0)$, presumably as expected, but here $(x_1,x_2,x_3)=(1,0,0)$ giving $z=−1$. The value $−1$ is never taken by the original function! The $(x_2−1)$ term should have been chosen to minimize at $0$ in the case that $y_2=0$, but it wasn't. – Erick Wong 1 hour ago @ErickWong I was lacking one thing: 'let xi=X for the ith constraint.' what about now? Thanks for the feedback XD honestly I haven't yet bothered to analyse any of these. We didn't discuss boolean logic. I'm about to ask my prof about this. I could have remembered the question wrong. I do remember that function and something about an integer linear programming problem – BCLC 11 mins ago That extra line shouldn't make a difference: it just declares the Intent of the variable $x_i$ (as an aid to the reader) but it doesn't change its value. Good luck, I do believe if the function is exactly as you remember then this answer is flawed. I haven't carefully analyzed Kuifje's answers and there may be minor typos there too :) – Erick Wong 5 mins ago @ErickWong Edit: Thanks for the feedback XD honestly I haven't yet bothered to analyse any of these. I mean I guess I could understand if I analysed, but the point is that I don't think average student in my class can come up with this without boolean logic because we didn't discuss boolean logic. I'm about to ask my prof about this. I could have remembered the question wrong. I do remember that function and something about an integer linear programming problem. – BCLC 2 mins ago   edit @ErickWong THANK YOU XD – BCLC 2 mins ago   edit","Update: Problem and solution found here (p. 17, 61), although my prof's solution (formulation) is different. Convert $$\min z = f(x)$$ where $$f(x) = \left\{\begin{matrix} 1-x, & 0 \le x < 1\\  x-1, & 1 \le x < 2\\  \frac{x}{2}, &  2 \le x \le 3 \end{matrix}\right.$$ s.t. $$x \ge 0$$ into a linear integer programming problem. What I tried: It seems that according to this , $$\min \ f(x) = \max(a_1x + b_1, a_2x + b_2, ..., a_mx + b_m), x \ge 0$$ is equivalent to $$\min \ t \ \text{s.t.}$$ $$a_1x + b_1 \le t$$ $$a_2x + b_2 \le t$$ $$\vdots$$ $$a_mx + b_m \le t$$ $$x \ge 0$$ Following that, I tried $$g(x) = \max(1-x, x-1, x/2) = \left\{\begin{matrix} 1-x, & 0 \le x \le 2/3\\  x-1, & x \ge 2\\  \frac{x}{2}, &  2/3 \le x \le 2 \end{matrix}\right.$$ If we allow only integer $x$, then we have $$g(x) = \max(1-x, x-1, x/2) = \left\{\begin{matrix} 1-x, & x=0\\  x-1, & x=2\\  \frac{x}{2}, &  x=1 or 2 \end{matrix}\right.$$ If we allow only integer $x$ for $f$, then we have $$f(x) = \left\{\begin{matrix} 1-x, & x=0\\  x-1, & x=1\\  \frac{x}{2}, &  x=2 or 3 \end{matrix}\right.$$ It doesn't look like $f(x) = g(x)$, w/ or w/o integer constraint. How can I approach this? (The following is copied from an answer I deleted and comments on it) Prof's answer (assuming I remembered question right): Let Xi = X for ith constraint. Minimise $$z = (1-x_1)+(x_2-1)+(1/2)(x_3)$$ s.t. $$0 \le x_1 \le y_1$$ $$y_2 \le x_2 \le 2y_2$$ $$2y_3 \le x_3 \le 3y_3$$ $$y_1, y_2, y_3 \in \{0,1\}$$ $$x_1,x_2,x_3 \ge 0$$ Comments below it: (You should probably annotate this to indicate the source.) Anyway, if you suspected something was off about this solution, then you're right: it's incorrect. This formulation is similar to Kuifje's Option 3, but it incorrectly encodes the objective function. The minimum value occurs at $(y_1,y_2,y_3)=(1,0,0)$, presumably as expected, but here $(x_1,x_2,x_3)=(1,0,0)$ giving $z=−1$. The value $−1$ is never taken by the original function! The $(x_2−1)$ term should have been chosen to minimize at $0$ in the case that $y_2=0$, but it wasn't. – Erick Wong 1 hour ago @ErickWong I was lacking one thing: 'let xi=X for the ith constraint.' what about now? Thanks for the feedback XD honestly I haven't yet bothered to analyse any of these. We didn't discuss boolean logic. I'm about to ask my prof about this. I could have remembered the question wrong. I do remember that function and something about an integer linear programming problem – BCLC 11 mins ago That extra line shouldn't make a difference: it just declares the Intent of the variable $x_i$ (as an aid to the reader) but it doesn't change its value. Good luck, I do believe if the function is exactly as you remember then this answer is flawed. I haven't carefully analyzed Kuifje's answers and there may be minor typos there too :) – Erick Wong 5 mins ago @ErickWong Edit: Thanks for the feedback XD honestly I haven't yet bothered to analyse any of these. I mean I guess I could understand if I analysed, but the point is that I don't think average student in my class can come up with this without boolean logic because we didn't discuss boolean logic. I'm about to ask my prof about this. I could have remembered the question wrong. I do remember that function and something about an integer linear programming problem. – BCLC 2 mins ago   edit @ErickWong THANK YOU XD – BCLC 2 mins ago   edit",,"['linear-algebra', 'optimization', 'linear-programming', 'integers', 'operations-research']"
52,Determinant of a block matrix including non-square matrices,Determinant of a block matrix including non-square matrices,,"I am trying to find a nice way of computing the determinant of the matrix \begin{equation} M= \begin{bmatrix} A & B \\ C & D \end{bmatrix} \in \mathbb{R}^{T\times T} \end{equation} where $A \in \mathbb{R}^{M\times N}$, $B \in \mathbb{R}^{M\times (T-N)}$, $C \in \mathbb{R}^{(T-M)\times N}$ and $D \in \mathbb{R}^{(T-M)\times (T-N)}$. Furthermore, $(A)_{i,j} = f_i(x_j)$, $(C)_{i,j} = g_i(x_j)$ where $f$ and $g$ are differentiable functions. I know there are nice ways to compute it when either $A$ or $D$ are invertible but is there a way to do it in the more general case above? When $A$ is invertible,  $$|M|=|A||D-CA^{-1}B|$$  A similar formula holds when $D$ is invertible. The question is specifically if such formulas can be extended to give $|M|$ in the case where neither $A$ nor $D$ is invertible (indeed, both could be non-square).","I am trying to find a nice way of computing the determinant of the matrix \begin{equation} M= \begin{bmatrix} A & B \\ C & D \end{bmatrix} \in \mathbb{R}^{T\times T} \end{equation} where $A \in \mathbb{R}^{M\times N}$, $B \in \mathbb{R}^{M\times (T-N)}$, $C \in \mathbb{R}^{(T-M)\times N}$ and $D \in \mathbb{R}^{(T-M)\times (T-N)}$. Furthermore, $(A)_{i,j} = f_i(x_j)$, $(C)_{i,j} = g_i(x_j)$ where $f$ and $g$ are differentiable functions. I know there are nice ways to compute it when either $A$ or $D$ are invertible but is there a way to do it in the more general case above? When $A$ is invertible,  $$|M|=|A||D-CA^{-1}B|$$  A similar formula holds when $D$ is invertible. The question is specifically if such formulas can be extended to give $|M|$ in the case where neither $A$ nor $D$ is invertible (indeed, both could be non-square).",,"['linear-algebra', 'matrices', 'determinant']"
53,What is the difference between linear manifold and linear vector subspace?,What is the difference between linear manifold and linear vector subspace?,,"Does linear manifold need to be closed in summation and multiplication? If it needs to be, then it seems it is the same as a linear vector subspace. However, some people mention in infinite dimension case, they are different.","Does linear manifold need to be closed in summation and multiplication? If it needs to be, then it seems it is the same as a linear vector subspace. However, some people mention in infinite dimension case, they are different.",,"['linear-algebra', 'terminology']"
54,Slick proof of cross product identities,Slick proof of cross product identities,,"The cross product between vectors in $\mathbb{R}^3$ obeys two pleasant identities (sometimes named after Lagrange), namely $a\times(b\times c)=b(a\cdot c)-c(a\cdot b)$ $(a\times b)\cdot(c\times d)=(a\cdot c)(b\cdot d)-(a\cdot d)(b\cdot c)$. The first one can be proved by invoking multilinearity and thus reducing oneself to a tedious check when $\{a,b,c\}\subseteq\{e_1,e_2,e_3\}$. The second one can be deduced from the first using properties of the triple product. I am looking for some slick/conceptual proof of the two identities (or one of them); maybe exterior algebra has something to tell us?","The cross product between vectors in $\mathbb{R}^3$ obeys two pleasant identities (sometimes named after Lagrange), namely $a\times(b\times c)=b(a\cdot c)-c(a\cdot b)$ $(a\times b)\cdot(c\times d)=(a\cdot c)(b\cdot d)-(a\cdot d)(b\cdot c)$. The first one can be proved by invoking multilinearity and thus reducing oneself to a tedious check when $\{a,b,c\}\subseteq\{e_1,e_2,e_3\}$. The second one can be deduced from the first using properties of the triple product. I am looking for some slick/conceptual proof of the two identities (or one of them); maybe exterior algebra has something to tell us?",,"['linear-algebra', 'vectors', 'cross-product', 'exterior-algebra']"
55,What is the best way to compute the pseudoinverse of a matrix?,What is the best way to compute the pseudoinverse of a matrix?,,"Mathematica gives the pseudo-inverse of a matrix almost instantaneously, so I suspect it is calculating the pseudo-inverse of a matrix not by doing singular value decomposition. Since the pseudo-inverse of a matrix is unique, is there a good formula that we can use to simplify our calculation in obtaining the pseudo-inverse, in place of compact singular value decomposition? I'm thinking about the property that $A^\dagger = (A^\ast A)^{-1} A$, and I think this should give the unique pseudo-inverse of $A$. But I'm not too sure. Thanks for any insight!","Mathematica gives the pseudo-inverse of a matrix almost instantaneously, so I suspect it is calculating the pseudo-inverse of a matrix not by doing singular value decomposition. Since the pseudo-inverse of a matrix is unique, is there a good formula that we can use to simplify our calculation in obtaining the pseudo-inverse, in place of compact singular value decomposition? I'm thinking about the property that $A^\dagger = (A^\ast A)^{-1} A$, and I think this should give the unique pseudo-inverse of $A$. But I'm not too sure. Thanks for any insight!",,"['linear-algebra', 'matrices', 'svd', 'pseudoinverse']"
56,Sum of two kronecker products as a kronecker product,Sum of two kronecker products as a kronecker product,,"I seek for the following relationship (if there is one so): $$C \otimes D = (A_1 \otimes B_1) + (A_2 \otimes B_2)$$ I would like to obtain $C = f(A_1,A_2)$ (in terms of $A$'s) and $D = g(B_1,B_2)$ (in terms of $B$'s). For simplicity, we can assume $A_i$ and $B_i$ are covariance matrices, so positive-definite, square, and symmetric. Any help is greatly appreciated! PS: For more simplification (if so), we can assume $\dim(A_1) = \dim(A_2)$ and same for $B_i$'s.","I seek for the following relationship (if there is one so): $$C \otimes D = (A_1 \otimes B_1) + (A_2 \otimes B_2)$$ I would like to obtain $C = f(A_1,A_2)$ (in terms of $A$'s) and $D = g(B_1,B_2)$ (in terms of $B$'s). For simplicity, we can assume $A_i$ and $B_i$ are covariance matrices, so positive-definite, square, and symmetric. Any help is greatly appreciated! PS: For more simplification (if so), we can assume $\dim(A_1) = \dim(A_2)$ and same for $B_i$'s.",,['linear-algebra']
57,Is every family of almost commuting matrices close to some family of commuting matrices?,Is every family of almost commuting matrices close to some family of commuting matrices?,,"Suppose that there are many matrices $A_i\in M_n(C)$, $i=1,2,3,\cdots,m$, that almost commute with each other (what I means is that they are not commute with each other, but $||[A_i, A_j]||$ is very small). Can I slightly change the matrices, e.g., $A_i^\prime:=A_i+\delta_i$ with $||\delta_i||$ very small, such that the new matrices $A_i^\prime$ commute with each other?","Suppose that there are many matrices $A_i\in M_n(C)$, $i=1,2,3,\cdots,m$, that almost commute with each other (what I means is that they are not commute with each other, but $||[A_i, A_j]||$ is very small). Can I slightly change the matrices, e.g., $A_i^\prime:=A_i+\delta_i$ with $||\delta_i||$ very small, such that the new matrices $A_i^\prime$ commute with each other?",,['linear-algebra']
58,Real matrix without real eigenvalues commutes with some matrix of square $-I$.,Real matrix without real eigenvalues commutes with some matrix of square .,-I,"Let $A\in M_n(\mathbb R)$ be such that, the minimal polynomial of $A$, has not any real root. Prove that there exist some $B\in M_n(\mathbb R)$ which: $B^2=-I_n$ and $AB=BA$. Suppose that $p(x)=x^k+a_{k-1}x^{k-1}+...+a_1+a_0$ be the minimal polynomial of $A$. By the hypothesis,  $a_0\neq 0$ and over ring $\mathbb R[x]$, the polynomial  $p(x)$ has the following decomposition: $$p(x)=(x^2+b_1x+b_2)^{d_1}(x^2+b_3x+b_4)^{d_2}...(x^2+b_mx+b_{m+1})^{ds}$$ With $b_j\in \mathbb R$ for $j=1,2,...,m+1$, and $d_1+d_2+...+d_s=k$. If I prove that there exist $q(x),h(x)\in \mathbb R[x]$ such that: $$(h(x))^2=p(x)q(x)-1$$ Then I am done by putting $B=h(A)$. But how can I prove that $q$ exists?","Let $A\in M_n(\mathbb R)$ be such that, the minimal polynomial of $A$, has not any real root. Prove that there exist some $B\in M_n(\mathbb R)$ which: $B^2=-I_n$ and $AB=BA$. Suppose that $p(x)=x^k+a_{k-1}x^{k-1}+...+a_1+a_0$ be the minimal polynomial of $A$. By the hypothesis,  $a_0\neq 0$ and over ring $\mathbb R[x]$, the polynomial  $p(x)$ has the following decomposition: $$p(x)=(x^2+b_1x+b_2)^{d_1}(x^2+b_3x+b_4)^{d_2}...(x^2+b_mx+b_{m+1})^{ds}$$ With $b_j\in \mathbb R$ for $j=1,2,...,m+1$, and $d_1+d_2+...+d_s=k$. If I prove that there exist $q(x),h(x)\in \mathbb R[x]$ such that: $$(h(x))^2=p(x)q(x)-1$$ Then I am done by putting $B=h(A)$. But how can I prove that $q$ exists?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'minimal-polynomials']"
59,Finding the vertices of a convex set of matrices,Finding the vertices of a convex set of matrices,,"I'm a little new here so wasn't sure if this was the right area. I've been trying to figure out how to generate a set of random $K \times N$ (for $K<N$) matrices that are subject to a several constraints: all columns sum to 1, all rows sum to $N/K$, and all elements of the matrix are non-negative. I've managed to reduce a problem a bit; because the set of matrices bound by these properties is convex, one could take the vertices of this set and take random weighted averages of the vertices (and use these averages for the pool, and so on); you should end up populating the space. What I'm trying to figure out is how to find the vertices of this set in the first place. So for example for the set of $2 \times 3$ matrices the vertices are the following set of pairs on the 2-D simplex (I think): (if you're reducing the 6-D space to ordered pairs in 3-D space then technically it also includes the re-ordered pairs, but this was just for illustration). Many thanks in advance.","I'm a little new here so wasn't sure if this was the right area. I've been trying to figure out how to generate a set of random $K \times N$ (for $K<N$) matrices that are subject to a several constraints: all columns sum to 1, all rows sum to $N/K$, and all elements of the matrix are non-negative. I've managed to reduce a problem a bit; because the set of matrices bound by these properties is convex, one could take the vertices of this set and take random weighted averages of the vertices (and use these averages for the pool, and so on); you should end up populating the space. What I'm trying to figure out is how to find the vertices of this set in the first place. So for example for the set of $2 \times 3$ matrices the vertices are the following set of pairs on the 2-D simplex (I think): (if you're reducing the 6-D space to ordered pairs in 3-D space then technically it also includes the re-ordered pairs, but this was just for illustration). Many thanks in advance.",,"['linear-algebra', 'convex-analysis']"
60,Continuity of the basis of the null space,Continuity of the basis of the null space,,"Consider a real $n\times n$ matrix $A(\alpha)$ continuously dependent of a real vector $\alpha$, i.e. a real matrix-valued continuous function $A: \mathbb{R}^m \rightarrow \mathbb{R}^{n \times n}$. If the rank of $A(\alpha)$ is the same for every $\alpha \in \mathbb{R}^m$, it is true that it is always possible to continuously choose a basis for the null space of $A(\alpha)$?  That is, there exists a continuous matrix valued function $N(\alpha)$ such that its columns form a basis for $A(\alpha)$ for every $\alpha \in \mathbb{R}^m$? Thanks for any help! Update Just to add more context for this question, for the case that the rank of $A(\alpha)$ is different for some $\alpha$, there are many counterexamples. More information about the non-constant rank case can be seen in the following MO questions: Conditions for smooth dependence of the eigenvalues and eigenvectors of a matrix on a set of parameters How to find/define eigenvectors as a continuous function of matrix?","Consider a real $n\times n$ matrix $A(\alpha)$ continuously dependent of a real vector $\alpha$, i.e. a real matrix-valued continuous function $A: \mathbb{R}^m \rightarrow \mathbb{R}^{n \times n}$. If the rank of $A(\alpha)$ is the same for every $\alpha \in \mathbb{R}^m$, it is true that it is always possible to continuously choose a basis for the null space of $A(\alpha)$?  That is, there exists a continuous matrix valued function $N(\alpha)$ such that its columns form a basis for $A(\alpha)$ for every $\alpha \in \mathbb{R}^m$? Thanks for any help! Update Just to add more context for this question, for the case that the rank of $A(\alpha)$ is different for some $\alpha$, there are many counterexamples. More information about the non-constant rank case can be seen in the following MO questions: Conditions for smooth dependence of the eigenvalues and eigenvectors of a matrix on a set of parameters How to find/define eigenvectors as a continuous function of matrix?",,"['linear-algebra', 'matrices', 'analysis']"
61,The action of a Lie algebra on a manifold is a Lie algebra homomorphism. How to show it?,The action of a Lie algebra on a manifold is a Lie algebra homomorphism. How to show it?,,"By definition, the action of a Lie algebra $\mathfrak g$ on a manifold $M$ is a Lie algebra homomorphism, $\mathcal A: \mathfrak g\rightarrow\mathfrak X(M), \xi\mapsto\xi_M$ such that the action map $\mathfrak g\times M\rightarrow TM, (\xi,m)\mapsto\xi_M(m)$ is smooth. Now, if we know the action of a Lie group $G$ on the manifold $M$, $G\times M\rightarrow M, (g,m)\mapsto gm$, then an action of the corresponding Lie algebra $\mathfrak g$ is thus $\xi_M=\frac{d}{dt}|_{t=0}\exp(-t\xi)m$ First, I would like to show that this action is linear in $\xi$. Pick $a\in\mathbb R$, one obtains $\frac{d}{dt}|_{t=0}\exp(-ta\xi)m=a\frac{d}{dt'}|_{t'=ta=0}\exp(-t'\xi)m=a\xi_M$. Then, if you pick a second vector $\eta\in\mathfrak g$, form a 3rd one $\xi+\eta=\zeta$ and consider the induced vector on $M$, $\zeta_M=\frac{d}{dt}|_{t=0}\exp(-t\zeta)m=\frac{d}{dt}|_{t=0}\exp(-t\xi-t\eta)m$ I want to prove $\zeta_M=\xi_M+\eta_M$, but got stuck. How can I proceed from here? Thank you very much!","By definition, the action of a Lie algebra $\mathfrak g$ on a manifold $M$ is a Lie algebra homomorphism, $\mathcal A: \mathfrak g\rightarrow\mathfrak X(M), \xi\mapsto\xi_M$ such that the action map $\mathfrak g\times M\rightarrow TM, (\xi,m)\mapsto\xi_M(m)$ is smooth. Now, if we know the action of a Lie group $G$ on the manifold $M$, $G\times M\rightarrow M, (g,m)\mapsto gm$, then an action of the corresponding Lie algebra $\mathfrak g$ is thus $\xi_M=\frac{d}{dt}|_{t=0}\exp(-t\xi)m$ First, I would like to show that this action is linear in $\xi$. Pick $a\in\mathbb R$, one obtains $\frac{d}{dt}|_{t=0}\exp(-ta\xi)m=a\frac{d}{dt'}|_{t'=ta=0}\exp(-t'\xi)m=a\xi_M$. Then, if you pick a second vector $\eta\in\mathfrak g$, form a 3rd one $\xi+\eta=\zeta$ and consider the induced vector on $M$, $\zeta_M=\frac{d}{dt}|_{t=0}\exp(-t\zeta)m=\frac{d}{dt}|_{t=0}\exp(-t\xi-t\eta)m$ I want to prove $\zeta_M=\xi_M+\eta_M$, but got stuck. How can I proceed from here? Thank you very much!",,"['linear-algebra', 'differential-geometry', 'lie-groups', 'lie-algebras', 'group-actions']"
62,The rank and eigenvalues of the operator $T(M) = AM - MA$ on the space of matrices,The rank and eigenvalues of the operator  on the space of matrices,T(M) = AM - MA,"This problem is from Artin Algebra Second edition, 5.2.3. Let $A$ be an $n\times n$ complex matrix. $(a)$ Consider the linear operator $T$ defined on the space $\mathbb{C}^{n\times n}$ of all complex $n\times n$ matrices by the rule $T(M) = AM - MA$ . Prove that the rank of this operator is at most $n^2-n$ $(b)$ Determine the eigenvalues of $T$ in terms of the eigenvalues $\lambda_1,\cdots,\lambda_n$ of $A$ . For part $(a)$ , I tried to use Dimension Formula. But, I don't know how to show that $\dim(\ker(T))$ is greater than equal to $n$ . For part $(b)$ , I really don't know... Can someone help me?","This problem is from Artin Algebra Second edition, 5.2.3. Let be an complex matrix. Consider the linear operator defined on the space of all complex matrices by the rule . Prove that the rank of this operator is at most Determine the eigenvalues of in terms of the eigenvalues of . For part , I tried to use Dimension Formula. But, I don't know how to show that is greater than equal to . For part , I really don't know... Can someone help me?","A n\times n (a) T \mathbb{C}^{n\times n} n\times n T(M) = AM - MA n^2-n (b) T \lambda_1,\cdots,\lambda_n A (a) \dim(\ker(T)) n (b)","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
63,"Matrix inner product, and operator and trace norm inequality","Matrix inner product, and operator and trace norm inequality",,"I have trouble proving the following inequality. Let a matrix $A \in \mathbb{R}^{M \times N}$ , and $\sigma_i(A)$ be the i-largest singular value of A. Define the operator norm and the trace norm as follow: $$ \|A\|_2 := \sigma_1(A),\ \|A\|_{tr} = \sum_{i = 1}^{\min\{M, N\}} \sigma_i(A) $$ Could anyone provide me how to prove the below statement? $$ \langle X,Y\rangle \le \|X\|_{tr}\,\|Y\|_2 $$ Note that $\langle X,Y\rangle $ is the matrix inner product.","I have trouble proving the following inequality. Let a matrix , and be the i-largest singular value of A. Define the operator norm and the trace norm as follow: Could anyone provide me how to prove the below statement? Note that is the matrix inner product.","A \in \mathbb{R}^{M \times N} \sigma_i(A) 
\|A\|_2 := \sigma_1(A),\ \|A\|_{tr} = \sum_{i = 1}^{\min\{M, N\}} \sigma_i(A)
 
\langle X,Y\rangle \le \|X\|_{tr}\,\|Y\|_2
 \langle X,Y\rangle ","['linear-algebra', 'inequality', 'normed-spaces']"
64,"Is there any difference between linear dependence, collinearity and coplanarity?","Is there any difference between linear dependence, collinearity and coplanarity?",,"The way it seems to me, linearly dependent vectors have to be collinear, and collinear vectors have to be coplanar. However, since a plane doesn't really have a direction, I'm assuming coplanar vectors can point in different directions as long as their lines exist on the same plane. Or do coplanar vectors/points also have to point in the same direction? If so, what's the practical difference between these concepts? I'm wondering this in terms of orientation and position in three-space, not in terms of whether the math is done differently or not. For example, what would be the difference between 3 linearly dependent vectors, 3 collinear vectors and 3 coplanar vectors? EDIT: So far I still can't visualize the difference in 3D space. It's not that I don't understand that the math is different, I just want to be able to clearly visualize what the similarities and differences are, because if I don't then the math won't make sense to me. I need to understand what the math does in order to make it stick.","The way it seems to me, linearly dependent vectors have to be collinear, and collinear vectors have to be coplanar. However, since a plane doesn't really have a direction, I'm assuming coplanar vectors can point in different directions as long as their lines exist on the same plane. Or do coplanar vectors/points also have to point in the same direction? If so, what's the practical difference between these concepts? I'm wondering this in terms of orientation and position in three-space, not in terms of whether the math is done differently or not. For example, what would be the difference between 3 linearly dependent vectors, 3 collinear vectors and 3 coplanar vectors? EDIT: So far I still can't visualize the difference in 3D space. It's not that I don't understand that the math is different, I just want to be able to clearly visualize what the similarities and differences are, because if I don't then the math won't make sense to me. I need to understand what the math does in order to make it stick.",,"['linear-algebra', 'vectors']"
65,Why does the Cholesky decomposition exist?,Why does the Cholesky decomposition exist?,,"From wikipedia , given any matrix $A$, we can sometimes decompose $A = LU$ using Gaussian elimination. Other times, a permutation matrix is needed, giving $PA = LU$. If $A$ is Hermitian positive-definite, I can show that IF no permutation matrix is needed, then Gaussian elimination gives $A=LU$ which I can eventually massage and get the Cholesky decomposition $A=LL^*$. However, it seems that Hermitian positive-definite matrices are special in that no permutaiton matrix is ever needed, and hence the Cholesky decomposition always exist. Why?","From wikipedia , given any matrix $A$, we can sometimes decompose $A = LU$ using Gaussian elimination. Other times, a permutation matrix is needed, giving $PA = LU$. If $A$ is Hermitian positive-definite, I can show that IF no permutation matrix is needed, then Gaussian elimination gives $A=LU$ which I can eventually massage and get the Cholesky decomposition $A=LL^*$. However, it seems that Hermitian positive-definite matrices are special in that no permutaiton matrix is ever needed, and hence the Cholesky decomposition always exist. Why?",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'cholesky-decomposition']"
66,Product of vector spaces,Product of vector spaces,,"Let $V$ be a vector space over a fixed field $k$. Under what circumstances do we have $V\times V\cong V$? I think this should be true if $\mathrm{dim} \ V=\infty$, isn't it?","Let $V$ be a vector space over a fixed field $k$. Under what circumstances do we have $V\times V\cong V$? I think this should be true if $\mathrm{dim} \ V=\infty$, isn't it?",,['linear-algebra']
67,"When solving a linear differential equation by factoring the operator, how does one guarantee no solutions are lost?","When solving a linear differential equation by factoring the operator, how does one guarantee no solutions are lost?",,"I think the best way to make this question clear is with an example. Lets say we want to solve the differential equation $(\Delta^2 - \lambda^4)\phi=0,$ calculations are greatly simplified if  we factor the operator as  $(\Delta -  \lambda^2)(\Delta + \lambda^2) \phi=0.$ Since the factors commute, it is clear that we can find solutions by solving each of the equations $$ (\Delta -  \lambda^2)\phi_1 =0\\(\Delta +  \lambda^2)\phi_2=0  $$ separately. However it is not obvious that any solution to the original problem can be given in the form $\phi = \phi_1 + \phi_2,$ which is what texts concerning vibrations of plates claim. For example the book from Leissa, Vibrating Plates, NASA. I have not found yet any reference that addresses this issue. How can one guarantee no solutions are lost by solving the equation through this method? That is, how can one guarantee there are no other solutions $\phi$ that can't be written as $\phi=\phi_1+\phi_2$? I would greatly appreciate help with this problem. Thanks in advance.","I think the best way to make this question clear is with an example. Lets say we want to solve the differential equation $(\Delta^2 - \lambda^4)\phi=0,$ calculations are greatly simplified if  we factor the operator as  $(\Delta -  \lambda^2)(\Delta + \lambda^2) \phi=0.$ Since the factors commute, it is clear that we can find solutions by solving each of the equations $$ (\Delta -  \lambda^2)\phi_1 =0\\(\Delta +  \lambda^2)\phi_2=0  $$ separately. However it is not obvious that any solution to the original problem can be given in the form $\phi = \phi_1 + \phi_2,$ which is what texts concerning vibrations of plates claim. For example the book from Leissa, Vibrating Plates, NASA. I have not found yet any reference that addresses this issue. How can one guarantee no solutions are lost by solving the equation through this method? That is, how can one guarantee there are no other solutions $\phi$ that can't be written as $\phi=\phi_1+\phi_2$? I would greatly appreciate help with this problem. Thanks in advance.",,"['linear-algebra', 'partial-differential-equations', 'operator-theory', 'factoring']"
68,Making non-singular matrices singular,Making non-singular matrices singular,,What is the minimum value of $k$ such that every non-singular $n\times n$ real matrices can be made singular by switching EXACTLY $k$ entries  with ZERO ?,What is the minimum value of $k$ such that every non-singular $n\times n$ real matrices can be made singular by switching EXACTLY $k$ entries  with ZERO ?,,"['linear-algebra', 'optimization']"
69,What graph Laplacians commute,What graph Laplacians commute,,"I know that the graph Laplacian of a fully connected graph commutes with the Laplacian of any other graph. Is there any theorem stating something similar about some more general family of graphs? I'm specially interested to know if there's any result about sparse random graphs. Thanks. --EDIT (Re-statement): What Laplacians commute with a -given- Laplacian?  One answer is all of its powers and their linear combination (see: Given a matrix, is there always another matrix which commutes with it? ) Are there other graphs (a more general family) with this property?","I know that the graph Laplacian of a fully connected graph commutes with the Laplacian of any other graph. Is there any theorem stating something similar about some more general family of graphs? I'm specially interested to know if there's any result about sparse random graphs. Thanks. --EDIT (Re-statement): What Laplacians commute with a -given- Laplacian?  One answer is all of its powers and their linear combination (see: Given a matrix, is there always another matrix which commutes with it? ) Are there other graphs (a more general family) with this property?",,"['linear-algebra', 'matrices', 'spectral-graph-theory', 'graph-laplacian']"
70,orthogonal group of a quadratic vector space,orthogonal group of a quadratic vector space,,"I am reading about the orthogonal group $O(V)$ of a real finite dimensional quadratic vector space $(V,Q)$ with $Q$ nondegenerate. By definition $$O(V)=\{f:V\mapsto V |\quad Q(f(v))=Q(v) \quad \forall v\in V\}.$$ I don't know if this definition is enough to derive that $O(V)\subset GL(V)$. Also can we imply from definition that $|\det(f)|=1$ for every $f\in V$ ? (For the case $V$ is positive definite, i.e. the bilinear form associated to $Q$ is an inner product, it's true, but I don't know in general case.) Thanks for any help!","I am reading about the orthogonal group $O(V)$ of a real finite dimensional quadratic vector space $(V,Q)$ with $Q$ nondegenerate. By definition $$O(V)=\{f:V\mapsto V |\quad Q(f(v))=Q(v) \quad \forall v\in V\}.$$ I don't know if this definition is enough to derive that $O(V)\subset GL(V)$. Also can we imply from definition that $|\det(f)|=1$ for every $f\in V$ ? (For the case $V$ is positive definite, i.e. the bilinear form associated to $Q$ is an inner product, it's true, but I don't know in general case.) Thanks for any help!",,"['linear-algebra', 'lie-groups', 'topological-groups']"
71,Is sub-vector an established mathematical entity?,Is sub-vector an established mathematical entity?,,"Reading the following paragraph I was wondering how this entity that the authors [1] call a sub-vector should be named. In Matlab a sub-vector has to be contiguous. Let $\mathbf{X}=(X_1,\ldots,X_n)$ be a vector of random variables and $\mathbf{x}=(x_1,\ldots,x_n)$ a possible value setting (configuration) for these variables. $x_i$ denotes a possible value of $X_i$, the $i$th component of $\mathbf{X}$, and $\mathbf{y}$ denotes a possible value setting for the sub-vector $\mathbf{Y}=(X_{J_1},\ldots,X_{J_k})$, $J=\{J_1,\ldots,J_k\}\subseteq\{1,\ldots,n\}$. Is sub-vector indeed the proper mathematical term for this entity? [1] A review of probabilistic graphical models in evolutionary computation [Larrañaga et al.], 2012","Reading the following paragraph I was wondering how this entity that the authors [1] call a sub-vector should be named. In Matlab a sub-vector has to be contiguous. Let $\mathbf{X}=(X_1,\ldots,X_n)$ be a vector of random variables and $\mathbf{x}=(x_1,\ldots,x_n)$ a possible value setting (configuration) for these variables. $x_i$ denotes a possible value of $X_i$, the $i$th component of $\mathbf{X}$, and $\mathbf{y}$ denotes a possible value setting for the sub-vector $\mathbf{Y}=(X_{J_1},\ldots,X_{J_k})$, $J=\{J_1,\ldots,J_k\}\subseteq\{1,\ldots,n\}$. Is sub-vector indeed the proper mathematical term for this entity? [1] A review of probabilistic graphical models in evolutionary computation [Larrañaga et al.], 2012",,['linear-algebra']
72,Norm inequality for sum and difference of positive-definite matrices,Norm inequality for sum and difference of positive-definite matrices,,"If $X_{1}$ and $X_{2}$ are positive definite matrices, how to show that $\left\Vert X_{1}-X_{2}\right\Vert \le\left\Vert X_{1}+X_{2}\right\Vert$ for the spectral norm? and how about for the nuclear norm? Thanks in advance.","If $X_{1}$ and $X_{2}$ are positive definite matrices, how to show that $\left\Vert X_{1}-X_{2}\right\Vert \le\left\Vert X_{1}+X_{2}\right\Vert$ for the spectral norm? and how about for the nuclear norm? Thanks in advance.",,"['linear-algebra', 'matrices', 'inequality', 'normed-spaces', 'nuclear-norm']"
73,Positive definiteness of a matrix,Positive definiteness of a matrix,,"Let ${x_1,x_2,...x_n}$ be positive numbers. Consider the matrix $C$ whose $(i,j)$-th entry is $$\min\left\{\frac{x_i}{x_j},\frac{x_j}{x_i}\right\}$$ Show that $C$ is non-negative definite (or positive semidefinite, meaning $z^t C z\geq 0$ for all $z\in \mathbb{R}^n$).When is $C$ positive definite?","Let ${x_1,x_2,...x_n}$ be positive numbers. Consider the matrix $C$ whose $(i,j)$-th entry is $$\min\left\{\frac{x_i}{x_j},\frac{x_j}{x_i}\right\}$$ Show that $C$ is non-negative definite (or positive semidefinite, meaning $z^t C z\geq 0$ for all $z\in \mathbb{R}^n$).When is $C$ positive definite?",,['linear-algebra']
74,Number of solutions - please check my solution?,Number of solutions - please check my solution?,,"Determine, in accordance to $k$, how many solutions does the given   system of equations have:    $$ \begin{cases}kx+(k+1)y=k-1\\4x+(k+4)y=k\end{cases} $$    And check, for which values of $k$ this system has exactly one solution lying   within the boundaries of a triangle of vertexes: $A=(0, 0), B=(\frac{2}{3},  0), C=(0, 2)$. Can I just solve it using determinants? I mean: $$D=k(k+4)-4(k+1)=k^2-4=(k+2)(k-2)$$ $$D_x=(k-1)(k+4)-(k+1)k=2k-4$$ $$D_y=k^2-4(k-1)=k^2-4k+4$$ Our system has one solution if $D$ is not $0$. This happens when $k$ is neither $2$ nor $-2$. It has no solutions when $D=0$ and ($D_x \neq 0$ or $D_y \neq 0$). $D=0$ for $k=2$ or $k=-2$,  $D_x \neq 0$ when $k \neq 2$ and $D_y \neq 0$ when $k \neq 2$. Though for $k=2$ our $D$ equals $0$, neither $D_x$ nor $D_y$ is $\neq 0$ so it has no solutions only when $k=-2$. It has infinitely many solutions for $D=0$ (which means $k=2$ or $k=-2$), $D_x = D_y = 0$. This happens when $k=2$. Summing up:  $$ \begin{cases} \text{no solution}, &\text{for } k=-2, \\ \text{infinitely many solutions}, &\text{for } k=2, \\  \text{one solution}, &\text{for every other } k. \end{cases} $$ Is this all right? I need to know that before proceeding to the triangle-thing.","Determine, in accordance to $k$, how many solutions does the given   system of equations have:    $$ \begin{cases}kx+(k+1)y=k-1\\4x+(k+4)y=k\end{cases} $$    And check, for which values of $k$ this system has exactly one solution lying   within the boundaries of a triangle of vertexes: $A=(0, 0), B=(\frac{2}{3},  0), C=(0, 2)$. Can I just solve it using determinants? I mean: $$D=k(k+4)-4(k+1)=k^2-4=(k+2)(k-2)$$ $$D_x=(k-1)(k+4)-(k+1)k=2k-4$$ $$D_y=k^2-4(k-1)=k^2-4k+4$$ Our system has one solution if $D$ is not $0$. This happens when $k$ is neither $2$ nor $-2$. It has no solutions when $D=0$ and ($D_x \neq 0$ or $D_y \neq 0$). $D=0$ for $k=2$ or $k=-2$,  $D_x \neq 0$ when $k \neq 2$ and $D_y \neq 0$ when $k \neq 2$. Though for $k=2$ our $D$ equals $0$, neither $D_x$ nor $D_y$ is $\neq 0$ so it has no solutions only when $k=-2$. It has infinitely many solutions for $D=0$ (which means $k=2$ or $k=-2$), $D_x = D_y = 0$. This happens when $k=2$. Summing up:  $$ \begin{cases} \text{no solution}, &\text{for } k=-2, \\ \text{infinitely many solutions}, &\text{for } k=2, \\  \text{one solution}, &\text{for every other } k. \end{cases} $$ Is this all right? I need to know that before proceeding to the triangle-thing.",,['linear-algebra']
75,Can Lawson's proof (that the canonical inclusion into a Clifford algebra is injective) be fixed?,Can Lawson's proof (that the canonical inclusion into a Clifford algebra is injective) be fixed?,,"The first proof in Lawson & Michelsohn's Spin Geometry is known to be wrong. The claim, which appears in a paragraph on page 8 (not in an official proposition), is that the projection map $\pi_q|_V:\mathscr T(V)\to \text{Cl}(V,q)$ is injective. For completeness, here is their ""proof."" $(V,q)$ is a vector space with a quadratic form, $\mathscr T(V)$ is the tensor algebra over $V$ , and $\mathscr J_q(V)$ is the ideal of $\mathscr T(V)$ generated by elements of the form $v\otimes v+q(v)1$ . We prove that $\pi_q|_V$ is injective as follows. We say that an element $\varphi\in\mathscr T(V)$ is of pure degree $s$ if $\varphi\in\bigotimes^sV$ . (Every element of $\mathscr T(V)$ is a finite sum of elements of pure degree.) We want to show that any element $\mathscr J_q(V)\cap V$ is zero. Any such element can be written as a finite sum $\varphi=\sum a_i\otimes(v_i\otimes v_i+q(v_i))\otimes b_i$ where we may assume that the $a_i$ 's and $b_i$ 's are of pure degree. Since $\varphi\in V=\bigotimes^1V$ , we conclude that $\sum a_{i'}\otimes(v_{i'}\otimes v_{i'})\otimes b_{i'}=0$ , where this sum is taken over those indices with $\deg a_i+\deg b_i$ maximal. This equation implies, by contraction with $q$ , that $\sum a_{i'}q(v_{i'})b_{i'}=0$ . Proceeding inductively, we prove that $\varphi=0$ . I don't know what they mean by ""contraction with $q$ ,"" so I can't even tell how this proof is supposed to work, much less how it goes wrong. That said, I don't really care what they intended or what's wrong with it, but rather I'd like to have a correct version of this proof in the same ""spirit"" as was intended here. A correct proof is given in the above-linked MO post, but it resorts to a representation of the Clifford algebra acting on the exterior algebra, which feels like a very different approach to me. We essentially need to show that the ideal $\mathscr J_q(V)$ is not equal to all of $\mathscr T(V)$ , and I like the idea of showing this directly using the definition of $\mathscr J_q(V)$ and the structure of $\mathscr T(V)$ , which I think is the approach that Lawson & Michelsohn were intending.","The first proof in Lawson & Michelsohn's Spin Geometry is known to be wrong. The claim, which appears in a paragraph on page 8 (not in an official proposition), is that the projection map is injective. For completeness, here is their ""proof."" is a vector space with a quadratic form, is the tensor algebra over , and is the ideal of generated by elements of the form . We prove that is injective as follows. We say that an element is of pure degree if . (Every element of is a finite sum of elements of pure degree.) We want to show that any element is zero. Any such element can be written as a finite sum where we may assume that the 's and 's are of pure degree. Since , we conclude that , where this sum is taken over those indices with maximal. This equation implies, by contraction with , that . Proceeding inductively, we prove that . I don't know what they mean by ""contraction with ,"" so I can't even tell how this proof is supposed to work, much less how it goes wrong. That said, I don't really care what they intended or what's wrong with it, but rather I'd like to have a correct version of this proof in the same ""spirit"" as was intended here. A correct proof is given in the above-linked MO post, but it resorts to a representation of the Clifford algebra acting on the exterior algebra, which feels like a very different approach to me. We essentially need to show that the ideal is not equal to all of , and I like the idea of showing this directly using the definition of and the structure of , which I think is the approach that Lawson & Michelsohn were intending.","\pi_q|_V:\mathscr T(V)\to \text{Cl}(V,q) (V,q) \mathscr T(V) V \mathscr J_q(V) \mathscr T(V) v\otimes v+q(v)1 \pi_q|_V \varphi\in\mathscr T(V) s \varphi\in\bigotimes^sV \mathscr T(V) \mathscr J_q(V)\cap V \varphi=\sum a_i\otimes(v_i\otimes v_i+q(v_i))\otimes b_i a_i b_i \varphi\in V=\bigotimes^1V \sum a_{i'}\otimes(v_{i'}\otimes v_{i'})\otimes b_{i'}=0 \deg a_i+\deg b_i q \sum a_{i'}q(v_{i'})b_{i'}=0 \varphi=0 q \mathscr J_q(V) \mathscr T(V) \mathscr J_q(V) \mathscr T(V)","['linear-algebra', 'alternative-proof', 'quotient-spaces', 'multilinear-algebra', 'clifford-algebras']"
76,Find $\mathbf{A}$ minimizing $\lVert \mathbf{A}^{\top}\mathbf{AX}-\mathbf{X} \rVert _{F}^{2}$ with a given $\mathbf{X}$,Find  minimizing  with a given,\mathbf{A} \lVert \mathbf{A}^{\top}\mathbf{AX}-\mathbf{X} \rVert _{F}^{2} \mathbf{X},"$1.$ Problem: There are three subproblems: $(1.1)$ Given a data matrix $\mathbf{X}\in \mathbb{R}^{N\times D}$ , find a matrix $\mathbf{A}\in \mathbb{R}^{M\times N} (M\ll N)$ satisfying: $$ \mathbf{A}=\underset{\mathbf{A}}{\arg\min}\lVert\mathbf{A}^\top\mathbf{A}\mathbf{X}-\mathbf{X}\rVert_F^2. $$ $(1.2)$ Given a data matrix $\mathbf{X}\in \mathbb{R}^{N\times D}$ , find a bipolar matrix $\mathbf{B}\in \mathbb{Z}^{M\times N} (M\ll N)$ and a scaling factor $\alpha\in\mathbb{R}$ satisfying: $$ \mathbf{C}=\underset{\mathbf{C}}{\arg\min}\lVert\mathbf{C}^\top\mathbf{C}\mathbf{X}-\mathbf{X}\rVert_F^2,~~~~\text{s.t.}~\mathbf{C}=\alpha\mathbf{B}, $$ $~~~~~~$ where each element of $\mathbf{B}$ can only be choosed from $\{-1,+1\}$ . $(1.3)$ Based on $(1.2)$ , could you please get an optimal binary matrix $\mathbf{C}$ ( i.e. , each element of $\mathbf{B}$ can only be choosed from $\{0,1\}$ )? $2.$ Background: (Compressed Sensing) $(2.1)$ I project an $N$ -dimensional dataset (with $D$ data points) into an $M$ -dimensional space by $\mathbf{Y}=\mathbf{AX}$ and then reconstruct it by $\mathbf{\hat{X}}=\mathbf{A^\top Y}=\mathbf{A^\top AX}$ . I want to get an optimal $\mathbf{A}$ such that the $\ell_2$ -error between $\mathbf{X}$ and $\mathbf{\hat{X}}$ is minimized. $(2.2)$ For higher compression efficiency and lower memory complexity of $\mathbf{A}$ , I want to get a bipolar $\mathbf{C}$ that may works well or close to floating-point $\mathbf{A}$ . $3.$ My Efforts: $(3.1)$ I think I can solve subproblem $(1.1)$ by singular value decomposition (SVD) of $\mathbf{X}=\mathbf{U\Sigma V^\top}$ with singular values $\sigma_i=\mathbf{\Sigma}_{i,i}$ satisfying $\sigma_1\ge \sigma_2\ge\cdots\ge \sigma_N$ . The optimal $\mathbf{A}$ should be formed by the first $M$ rows of $\mathbf{U}^\top$ [ Reference ]. $(3.2)$ For subproblem $(1.2)$ , I guess that the optimal scaling factor $\alpha=1/\sqrt{N}$ since the optimal $\mathbf{C}$ should be row-normalized, and I do not know how to get the optimal $\mathbf{B}$ . $(3.3)$ I guess that I can get a bipolar $\mathbf{B}$ by discretizing each element of $\mathbf{A}$ with $$ b_{i,j}=f\left( a_{i,j} \right) =\left\{ \begin{array}{l} 	-1,\ a_{i,j}<0\\ 	1,\ a_{i,j}\ge 0\\ \end{array} \right., $$ $~~~~~~~~~$ which may be better than random ones (by Bernoulli sampling) but not optimal? $4.$ Some Other Minor Questions: $(4.1)$ I do not know if I can directly write that “ $\mathbf{B}\in\{-1,+1\}^{M\times N}$ ”?","Problem: There are three subproblems: Given a data matrix , find a matrix satisfying: Given a data matrix , find a bipolar matrix and a scaling factor satisfying: where each element of can only be choosed from . Based on , could you please get an optimal binary matrix ( i.e. , each element of can only be choosed from )? Background: (Compressed Sensing) I project an -dimensional dataset (with data points) into an -dimensional space by and then reconstruct it by . I want to get an optimal such that the -error between and is minimized. For higher compression efficiency and lower memory complexity of , I want to get a bipolar that may works well or close to floating-point . My Efforts: I think I can solve subproblem by singular value decomposition (SVD) of with singular values satisfying . The optimal should be formed by the first rows of [ Reference ]. For subproblem , I guess that the optimal scaling factor since the optimal should be row-normalized, and I do not know how to get the optimal . I guess that I can get a bipolar by discretizing each element of with which may be better than random ones (by Bernoulli sampling) but not optimal? Some Other Minor Questions: I do not know if I can directly write that “ ”?","1. (1.1) \mathbf{X}\in \mathbb{R}^{N\times D} \mathbf{A}\in \mathbb{R}^{M\times N} (M\ll N) 
\mathbf{A}=\underset{\mathbf{A}}{\arg\min}\lVert\mathbf{A}^\top\mathbf{A}\mathbf{X}-\mathbf{X}\rVert_F^2.
 (1.2) \mathbf{X}\in \mathbb{R}^{N\times D} \mathbf{B}\in \mathbb{Z}^{M\times N} (M\ll N) \alpha\in\mathbb{R} 
\mathbf{C}=\underset{\mathbf{C}}{\arg\min}\lVert\mathbf{C}^\top\mathbf{C}\mathbf{X}-\mathbf{X}\rVert_F^2,~~~~\text{s.t.}~\mathbf{C}=\alpha\mathbf{B},
 ~~~~~~ \mathbf{B} \{-1,+1\} (1.3) (1.2) \mathbf{C} \mathbf{B} \{0,1\} 2. (2.1) N D M \mathbf{Y}=\mathbf{AX} \mathbf{\hat{X}}=\mathbf{A^\top Y}=\mathbf{A^\top AX} \mathbf{A} \ell_2 \mathbf{X} \mathbf{\hat{X}} (2.2) \mathbf{A} \mathbf{C} \mathbf{A} 3. (3.1) (1.1) \mathbf{X}=\mathbf{U\Sigma V^\top} \sigma_i=\mathbf{\Sigma}_{i,i} \sigma_1\ge \sigma_2\ge\cdots\ge \sigma_N \mathbf{A} M \mathbf{U}^\top (3.2) (1.2) \alpha=1/\sqrt{N} \mathbf{C} \mathbf{B} (3.3) \mathbf{B} \mathbf{A} 
b_{i,j}=f\left( a_{i,j} \right) =\left\{ \begin{array}{l}
	-1,\ a_{i,j}<0\\
	1,\ a_{i,j}\ge 0\\
\end{array} \right.,
 ~~~~~~~~~ 4. (4.1) \mathbf{B}\in\{-1,+1\}^{M\times N}","['linear-algebra', 'matrices', 'discrete-mathematics', 'optimization', 'convex-optimization']"
77,Understanding properties of sequential orthogonalization methods like Gram-Schmidt.,Understanding properties of sequential orthogonalization methods like Gram-Schmidt.,,"We all know the Gram-Schmidt orthogonalization is done recursively and takes the linearly independent set of vectors one-by-one. And it can be distinguished from democratic orthogonalization like Löwdin and Wigner, which handles all the given vectors simultaneously and treat them on equal footing 1 2 . I want to understand a particular property of sequential orthogonalization methods like Gram-Schmidt. Suppose we have a linearly independent set of vectors $A:=\{v_,\cdots,v_i,\cdots,v_n\}$ and we apply the Gram-Schmidt orthogonalization method to get the orthonormal set of vectors $B:=\{w_1,\cdots,w_i,\cdots,w_n\}$ . Now suppose we replace $v_i$ to $\tilde{v}_i$ $(\tilde{v}_i \neq v_i)$ such that the new initial set $\tilde{A}$ is still linearly independent. Then after applying the Gram-Schmidt orthogonalization we get $\tilde{B}:=\{w_1,\cdots,\tilde{w}_i,\cdots,\tilde{w}_n\}$ . Can we show that $w_j \neq \tilde{w}_j$ for some $i \leq j \leq n$ . Or may be there are some special conditions for which the equality holds. Can we mathematically explore such conditions. A similar condition is when we exchange positions of $v_i$ and $v_k$ $(1\leq k<i\leq n)$ , then again can we answer the above question. I know that the Gram-Schmidt orthogonalization is not one-one. So most probably the inequality doesn't hold always. But what conditions will result in equality and what conditions to inequality. It would be helpful even if you could point me to some references or explanations.","We all know the Gram-Schmidt orthogonalization is done recursively and takes the linearly independent set of vectors one-by-one. And it can be distinguished from democratic orthogonalization like Löwdin and Wigner, which handles all the given vectors simultaneously and treat them on equal footing 1 2 . I want to understand a particular property of sequential orthogonalization methods like Gram-Schmidt. Suppose we have a linearly independent set of vectors and we apply the Gram-Schmidt orthogonalization method to get the orthonormal set of vectors . Now suppose we replace to such that the new initial set is still linearly independent. Then after applying the Gram-Schmidt orthogonalization we get . Can we show that for some . Or may be there are some special conditions for which the equality holds. Can we mathematically explore such conditions. A similar condition is when we exchange positions of and , then again can we answer the above question. I know that the Gram-Schmidt orthogonalization is not one-one. So most probably the inequality doesn't hold always. But what conditions will result in equality and what conditions to inequality. It would be helpful even if you could point me to some references or explanations.","A:=\{v_,\cdots,v_i,\cdots,v_n\} B:=\{w_1,\cdots,w_i,\cdots,w_n\} v_i \tilde{v}_i (\tilde{v}_i \neq v_i) \tilde{A} \tilde{B}:=\{w_1,\cdots,\tilde{w}_i,\cdots,\tilde{w}_n\} w_j \neq \tilde{w}_j i \leq j \leq n v_i v_k (1\leq k<i\leq n)","['linear-algebra', 'orthogonality', 'gram-schmidt']"
78,Why does Inverse of Linear/Linear Function have same structure as Inverse of Matrix,Why does Inverse of Linear/Linear Function have same structure as Inverse of Matrix,,"The inverse of a $\frac{\text{linear}}{\text{linear}} $ function $$f(x)=\frac{ax+b}{cx+d}\implies f^{-1}(x)=\frac{dx-b}{-cx+a}, ad-bc\neq0$$ has a very similar structure to that of the inverse of a matrix (one is a multiple of the other) $$X=\begin{bmatrix}a & b\\c & d\end{bmatrix}\implies X^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d & -b\\-c & a\end{bmatrix}, ad-bc\neq0$$ I understand these two properties, and their proofs, individually .I understand how to use matrices as representations of systems of linear equations, but am not sure that is the correct interpretation here. What would be an appropriate interpretation of the matrix, and what causes this connection?","The inverse of a function has a very similar structure to that of the inverse of a matrix (one is a multiple of the other) I understand these two properties, and their proofs, individually .I understand how to use matrices as representations of systems of linear equations, but am not sure that is the correct interpretation here. What would be an appropriate interpretation of the matrix, and what causes this connection?","\frac{\text{linear}}{\text{linear}}  f(x)=\frac{ax+b}{cx+d}\implies f^{-1}(x)=\frac{dx-b}{-cx+a}, ad-bc\neq0 X=\begin{bmatrix}a & b\\c & d\end{bmatrix}\implies X^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d & -b\\-c & a\end{bmatrix}, ad-bc\neq0","['linear-algebra', 'contest-math', 'projective-geometry', 'inverse-function']"
79,About the remark of the proof that every operator on non-zero finite dimensional complex vector space has an eigenvalue in Linear Algebra Done Right,About the remark of the proof that every operator on non-zero finite dimensional complex vector space has an eigenvalue in Linear Algebra Done Right,,"In the 2nd Edition of Linear algebra Done Right, theorem 5.10 (Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue), the author writes as a side note: Compare the simple proof of this theorem given here with the standard proof using determinants. With the standard proof, first the difficult concept of determinants must be defined, then an operator with 0 determinant must be shown to be not invertible, then the characteristic polynomial needs to be defined, and by the time the proof of this theorem is reached, no insight remains about why it is true. However, the approach given by the author does not seem to give me any more insight for why this theorem is true than the standard proof using determinants. Both proofs seem to just find some trick to transform this into a problem about roots of polynomials, in order to apply FTA. Am i missing some intuition about this Here's the proof for reference:","In the 2nd Edition of Linear algebra Done Right, theorem 5.10 (Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue), the author writes as a side note: Compare the simple proof of this theorem given here with the standard proof using determinants. With the standard proof, first the difficult concept of determinants must be defined, then an operator with 0 determinant must be shown to be not invertible, then the characteristic polynomial needs to be defined, and by the time the proof of this theorem is reached, no insight remains about why it is true. However, the approach given by the author does not seem to give me any more insight for why this theorem is true than the standard proof using determinants. Both proofs seem to just find some trick to transform this into a problem about roots of polynomials, in order to apply FTA. Am i missing some intuition about this Here's the proof for reference:",,"['linear-algebra', 'soft-question']"
80,Let $A$ and $B$ be complex $n\times n$ matrices such that $AB−BA$ is invertible and such that $A^2+B^2=c(AB−BA)$ for some rational number $c$.,Let  and  be complex  matrices such that  is invertible and such that  for some rational number .,A B n\times n AB−BA A^2+B^2=c(AB−BA) c,"Let $A$ and $B$ be complex $n\times n$ matrices such that $AB−BA$ is invertible and such that $A^2+B^2=c(AB−BA)$ for some rational number $c$ . Prove $c\in\{−1,0,1\}$ and show that $n$ is a multiple of $4$ when $c\neq0$ . Please give some hints for this problem. I could not find a way to proceed. I had taken determinant on both sides. But it does not helping much",Let and be complex matrices such that is invertible and such that for some rational number . Prove and show that is a multiple of when . Please give some hints for this problem. I could not find a way to proceed. I had taken determinant on both sides. But it does not helping much,"A B n\times n AB−BA A^2+B^2=c(AB−BA) c c\in\{−1,0,1\} n 4 c\neq0","['linear-algebra', 'matrices']"
81,Lower bound on the rank of a 0-1 matrix: $\mathrm {rank}_\mathbb R(A)\cdot |A|\geq n^2$,Lower bound on the rank of a 0-1 matrix:,\mathrm {rank}_\mathbb R(A)\cdot |A|\geq n^2,"Let $A$ be a square matrix of size $n \times n$ whose entries are all $0$ or $1$ , and its diagonal entries are all $1$ . Denote the total number of $1$ s in the matrix by $|A|$ . So $|A|$ is the sum of all entries. I want to prove the following lower bound on the rank of $A$ over the reals. $$\mathrm {rank}_\mathbb R(A)\cdot |A|\geq n^2.$$ Thoughts. If $A$ is the identity matrix or the all-ones matrix then we get equality. An equivalent interpretation: start with the identity matrix and then try to add more $1$ s efficiently to reduce the rank. The claim is that to reduce the rank by $k$ we must add at least $$\frac {n^2}{n-k}-n = \frac{kn}{n-k}$$ new $1$ s. For small $k$ this can be verified manually. The claim is that the geomtric mean of the rank and the sum is at least $n$ . If we replace the geometric mean by arithmetic mean, meaning $\mathrm {rank}_\mathbb R(A) + |A|\geq 2n$ , then the claim is immediate from the preceding interpretation, because adding $1$ somewhere can reduce the rank by at most $1$ .","Let be a square matrix of size whose entries are all or , and its diagonal entries are all . Denote the total number of s in the matrix by . So is the sum of all entries. I want to prove the following lower bound on the rank of over the reals. Thoughts. If is the identity matrix or the all-ones matrix then we get equality. An equivalent interpretation: start with the identity matrix and then try to add more s efficiently to reduce the rank. The claim is that to reduce the rank by we must add at least new s. For small this can be verified manually. The claim is that the geomtric mean of the rank and the sum is at least . If we replace the geometric mean by arithmetic mean, meaning , then the claim is immediate from the preceding interpretation, because adding somewhere can reduce the rank by at most .",A n \times n 0 1 1 1 |A| |A| A \mathrm {rank}_\mathbb R(A)\cdot |A|\geq n^2. A 1 k \frac {n^2}{n-k}-n = \frac{kn}{n-k} 1 k n \mathrm {rank}_\mathbb R(A) + |A|\geq 2n 1 1,"['linear-algebra', 'combinatorics', 'matrices', 'inequality', 'matrix-rank']"
82,A matrix defined by an operation in a finite group,A matrix defined by an operation in a finite group,,"Let $G$ be a finite group and $x_1,..., x_n$ be an enumeration of its elements. We consider the matrix $(a_{ij})_{1\le i,j \le n}$ where $a_{ij}=0$ if $x_i x_j^{-1}=x_jx_i^{-1}$ and $a_{ij}=1$ otherwise. Find the parity of $\det(a_{ij})$ . This problem comes from the 2019 District stage of the Romanian Mathematics Olympiad. Let $A=(a_{ij})_{1\le i,j \le n}$ . One of the $x_i$ s is going to be the identity element of $G$ . WLOG we may consider it to be $x_1$ , since changing rows and columns only affects the sign of a determinant. I managed to obseve that $A$ is symmetric since $a_{ij}=a_{ji}$ always. Furthermore, $A$ 's principal diagonal is going to be $0$ because $x_i x_i^{-1}=x_i^{-1}x_i$ , $\forall i=\overline{1,n}$ . Hence, $A$ is a symmetric hollow matrix whose entries are either $0$ or $1$ . Here I got stuck and I would like to know if it is possible to continue along these lines. EDIT: As requested, I will translate the official solution: $\det(a_{ij})$ is even. To prove this, we will show that $\det(a_{ij})$ is divisible by $|S|$ , where $S=\{x | x\in G, x\ne x^{-1}\}$ . Since an element of $G$ is in $S$ if and only if its inverse is in $S$ , $|S|$ is even (possibly zero), so $\det(a_{ij})$ is even. The value of a determinant is not changed if a column is replaced by the sum of all the columns. Hence, to prove the divisibility it is enough to show that every row contains exactly $|S|$ units. If $S$ is empty, then $(a_{ij})=O_n$ , so $\det(a_{ij})=0$ . If $S$ isn't empty, we fix a row $i$ and we consider the set $J_i=\{j |a_{ij}=1\}$ . Since $j \to x_i x_j^{-1}$ defines a bijection from $J_i$ to $S$ , it follows that $|J_i|=|S|$ .","Let be a finite group and be an enumeration of its elements. We consider the matrix where if and otherwise. Find the parity of . This problem comes from the 2019 District stage of the Romanian Mathematics Olympiad. Let . One of the s is going to be the identity element of . WLOG we may consider it to be , since changing rows and columns only affects the sign of a determinant. I managed to obseve that is symmetric since always. Furthermore, 's principal diagonal is going to be because , . Hence, is a symmetric hollow matrix whose entries are either or . Here I got stuck and I would like to know if it is possible to continue along these lines. EDIT: As requested, I will translate the official solution: is even. To prove this, we will show that is divisible by , where . Since an element of is in if and only if its inverse is in , is even (possibly zero), so is even. The value of a determinant is not changed if a column is replaced by the sum of all the columns. Hence, to prove the divisibility it is enough to show that every row contains exactly units. If is empty, then , so . If isn't empty, we fix a row and we consider the set . Since defines a bijection from to , it follows that .","G x_1,..., x_n (a_{ij})_{1\le i,j \le n} a_{ij}=0 x_i x_j^{-1}=x_jx_i^{-1} a_{ij}=1 \det(a_{ij}) A=(a_{ij})_{1\le i,j \le n} x_i G x_1 A a_{ij}=a_{ji} A 0 x_i x_i^{-1}=x_i^{-1}x_i \forall i=\overline{1,n} A 0 1 \det(a_{ij}) \det(a_{ij}) |S| S=\{x | x\in G, x\ne x^{-1}\} G S S |S| \det(a_{ij}) |S| S (a_{ij})=O_n \det(a_{ij})=0 S i J_i=\{j |a_{ij}=1\} j \to x_i x_j^{-1} J_i S |J_i|=|S|","['linear-algebra', 'matrices', 'group-theory', 'contest-math']"
83,Subring of $\text{End}(V)$ generated by $\text{GL}(V)$,Subring of  generated by,\text{End}(V) \text{GL}(V),"Let $V$ be a $\mathbb{C}$ -vector space of arbitrary dimension and let $R$ be its endomorphism ring. Can we describe the subring $S$ generated by $R^\times = \text{GL}(V)$ ? If $\text{dim}(V) < \infty$ , it's easy to see $R = S$ . But in general?","Let be a -vector space of arbitrary dimension and let be its endomorphism ring. Can we describe the subring generated by ? If , it's easy to see . But in general?",V \mathbb{C} R S R^\times = \text{GL}(V) \text{dim}(V) < \infty R = S,"['linear-algebra', 'ring-theory']"
84,Powers of a Positive Matrix in the Limit,Powers of a Positive Matrix in the Limit,,"I'm trying to prove a standard result: for a positive $n \times n$ matrix $A$ , the powers of $A$ scaled by its leading eigenvalue $\lambda$ converge to a matrix whose columns are just scalar multiples of $A$ 's leading eigenvector $\mathbf{v}$ . More precisely, $$ \lim_{k \rightarrow \infty} \left(\frac{A}{\lambda}\right)^k = \mathbf{v}\mathbf{u},$$ where $\mathbf{v}$ and $\mathbf{u}$ are the leading right and left eigenvectors of $A$ , respectively (scaled so that $\mathbf{u}\mathbf{v} = 1$ ). These notes give a nice compact proof, pictured below. (It covers the more general case where $A$ is primitive, but I'm happy to assume it's positive for my purposes.) I'm stuck on the highlighted step. Having established the existence of a matrix $M$ that (a) fixes $\mathbf{u}$ and $\mathbf{v}$ , and (b) annihilates all other generalized eigenvalues of $A$ , how do we know $M$ is unique? Why couldn't there be other matrices satisfying (a) and (b)? I don't know much about generalized eigenvectors. I gather they're linearly independent, hence form a basis for $\mathbb{R}^n$ . So each column of $M$ must be a unique linear combination of $A$ 's generalized right eigenvectors. Is there some path I'm not seeing from there to the conclusion that only one matrix can satisfy both (a) and (b)?","I'm trying to prove a standard result: for a positive matrix , the powers of scaled by its leading eigenvalue converge to a matrix whose columns are just scalar multiples of 's leading eigenvector . More precisely, where and are the leading right and left eigenvectors of , respectively (scaled so that ). These notes give a nice compact proof, pictured below. (It covers the more general case where is primitive, but I'm happy to assume it's positive for my purposes.) I'm stuck on the highlighted step. Having established the existence of a matrix that (a) fixes and , and (b) annihilates all other generalized eigenvalues of , how do we know is unique? Why couldn't there be other matrices satisfying (a) and (b)? I don't know much about generalized eigenvectors. I gather they're linearly independent, hence form a basis for . So each column of must be a unique linear combination of 's generalized right eigenvectors. Is there some path I'm not seeing from there to the conclusion that only one matrix can satisfy both (a) and (b)?","n \times n A A \lambda A \mathbf{v}  \lim_{k \rightarrow \infty} \left(\frac{A}{\lambda}\right)^k = \mathbf{v}\mathbf{u}, \mathbf{v} \mathbf{u} A \mathbf{u}\mathbf{v} = 1 A M \mathbf{u} \mathbf{v} A M \mathbb{R}^n M A","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'generalized-eigenvector', 'positive-matrices']"
85,Is my proof that $l^1$ is isometrically isomorphic to $c_0^*$ correct?,Is my proof that  is isometrically isomorphic to  correct?,l^1 c_0^*,"This is a classic exercise of functional analysis, but I do not fully understand it after reading many answers in textbooks. So I am trying to reorganize the proof step by step in details. I am hoping that someone may review my proof very carefully and give comments or corrections. Then I will revise the proof and hopefully it could be helpful to the beginners of functional analysis. Let $c_0(\mathbb{N})$ be the space of sequences converging to $0$ . Show that there is a well-defined, isometric isomorphism \begin{align} T: l^1(\mathbb{N}) \to \left(c_0(\mathbb{N})\right)^*,  \qquad T(g)(f) := \sum_{n\in\mathbb{N}}f(n)g(n).  \end{align} That is, show that $T(g)$ is a bounded linear functional $c_0(\mathbb{N}) \to  \mathbb{C}$ with $\|T(g)\| = \|g\|$ and that any bounded linear functional on $c_0(\mathbb{N})$ is of this form for a unique $g \in  l^1(\mathbb{N})$ . My proof: First of all, we denote the sequences $f \in c_0(\mathbb{N})$ and $g \in l^1(\mathbb{N})$ , thus $T(g) \in c_0^*: c_0(\mathbb{N}) \to \mathbb{C}$ . By the way, can we say $T(f): l^1(\mathbb{N}) \to \mathbb{C}$ ? I think it is not well-defined. Boundedness: To show $T(g)$ is a bounded linear functional $c_0(\mathbb{N}) \to \mathbb{C}$ with $\|T(g)\| = \|g\|_{l^1}$ , we first show the boundedness. \begin{align} |T(g)(f)| = \left|\sum_{n\in\mathbb{N}}f(n)g(n)\right| \le \sum_{n\in\mathbb{N}}|f(n)||g(n)| \le \sup_{n\in\mathbb{N}}|f(n)|\sum_{n\in\mathbb{N}}|g(n)| = \|f\|_{l^\infty}\|g\|_{l^1} \end{align} Therefore, $T(g)$ is bounded by \begin{align} \|T(g)\| = \sup\{|T(g)(f)|: \forall f \in c_0(\mathbb{N}), \|f\|_{l^\infty}\le 1\} \le \|g\|_{l^1} \end{align} Linearity: To show the linearity, we define $f_1, f_2 \in c_0(\mathbb{N})$ and $a_1, a_2 \in \mathbb{C}$ . Then \begin{align} T(g)(a_1 f_1 + a_2 f_2)  &= \sum_{n\in\mathbb{N}} \left(a_1 f_1(n) + a_2 f_2(n)\right) g(n) \\ &= \sum_{n\in\mathbb{N}} \left(a_1 f_1(n) g(n) + a_2 f_2(n) g(n)\right) \\ &= a_1 \sum_{n\in\mathbb{N}} f_1(n) g(n) + a_2 \sum_{n\in\mathbb{N}} f_2(n) g(n) \\ &= a_1 T(g)(f_1) + a_2 T(g)(f_2)  \end{align} implies that $T(g)$ is linear. Isometry: We already proved $T(g): c_0(\mathbb{N}) \to \mathbb{C}$ a bounded linear functional for all $g \in l^1(\mathbb{N})$ , now can we say the operator $T: l^1(\mathbb{N}) \to \left(c_0(\mathbb{N})\right)^*$ is therefore well-defined? Next we need to show that $T$ is an isometry for which $\|T(g)\| = \|g\|_{l^1}$ . Since we already have $\|T(g)\| \le \|g\|_{l^1}$ from the boundedness, if we are able to show that there exists some $f \in c_0(\mathbb{N})$ for which $\|T(g)\| \ge \|g\|_{l^1}$ , then $\|T(g)\| = \|g\|_{l^1}$ . Let $g$ be a sequence in $l^1(\mathbb{N})$ . If $g = 0$ , then $\|T(g)\| = \|g\|_{l^1}$ holds trivially. Assuming $g \ne 0$ , we define \begin{align} f(n) :=  \begin{cases} \frac{\overline{g(n)}}{|g(n)|} &n \le N \\ 0 &n > N \end{cases} \end{align} which is a sequence in $c_0(\mathbb{N})$ , with $T(g)(f) := \sum_{n\in\mathbb{N}}f(n)g(n) = \sum_{n\in\mathbb{N}}|g(n)| =: \|g\|_{l^1}$ and $\|f\|_{l^\infty} = 1$ by definition. Therefore, we have \begin{align} \|g\|_{l^1} = |T(g)(f)| \le \|T(g)\|\|f\|_{l^\infty} = \|T(g)\| \end{align} in addition to $\|T(g)\| \le \|g\|_{l^1}$ , which implies $\|T(g)\| = \|g\|_{l^1}$ , i.e., $T$ is an isometry and thus injective (one-to-one). Surjectivity: To prove that $T: l^1(\mathbb{N}) \to \left(c_0(\mathbb{N})\right)^*$ is surjective (onto), we need to show for any bounded linear functional $\forall S \in \left(c_0(\mathbb{N})\right)^*$ there exists $T(g) = S$ that has some preimage $g \in l^1(\mathbb{N})$ . Let us build a basis $\{e_n: n = 1, 2,...\}$ of sequences for $c_0(\mathbb{N})$ , where $e_n = (\delta_n)_{n \in \mathbb{N}} \in c_0(\mathbb{N})$ . Any $f \in c_0(\mathbb{N})$ can be written coordinate-wisely by the linear combination of the sequences of the basis $f = \sum_{n\in\mathbb{N}} f(n) e_n$ . Then by the linearity of $S$ we have \begin{align} S(f) = \sum_{n\in\mathbb{N}} f(n) S(e_n) \end{align} If we define $g(n) := S(e_n)$ , then we find \begin{align} S(f) = \sum_{n\in\mathbb{N}} f(n) S(e_n) = \sum_{n\in\mathbb{N}} f(n) g(n) =: T(g)(f) \end{align} that implies $S = T(g)$ . By definition, $g(n)$ maps each sequence of basis to $T(g)(e_n)$ . In addition, we need to show this $g \in l^1(\mathbb{N})$ by $\|g\|_{l^1} \le \|S\|$ which I am not able to finish . In conclusion, we find $\exists g \in l^1(\mathbb{N})$ for $T(g)$ corresponding to $\forall S \in \left(c_0(\mathbb{N})\right)^*$ , therefore $T$ is surjective. Finally, $T$ is a well-defined isometric isomorphism. Please show me that $\|g\|_{l^1} \le \|S\|$ How to show this $g$ is unique?","This is a classic exercise of functional analysis, but I do not fully understand it after reading many answers in textbooks. So I am trying to reorganize the proof step by step in details. I am hoping that someone may review my proof very carefully and give comments or corrections. Then I will revise the proof and hopefully it could be helpful to the beginners of functional analysis. Let be the space of sequences converging to . Show that there is a well-defined, isometric isomorphism That is, show that is a bounded linear functional with and that any bounded linear functional on is of this form for a unique . My proof: First of all, we denote the sequences and , thus . By the way, can we say ? I think it is not well-defined. Boundedness: To show is a bounded linear functional with , we first show the boundedness. Therefore, is bounded by Linearity: To show the linearity, we define and . Then implies that is linear. Isometry: We already proved a bounded linear functional for all , now can we say the operator is therefore well-defined? Next we need to show that is an isometry for which . Since we already have from the boundedness, if we are able to show that there exists some for which , then . Let be a sequence in . If , then holds trivially. Assuming , we define which is a sequence in , with and by definition. Therefore, we have in addition to , which implies , i.e., is an isometry and thus injective (one-to-one). Surjectivity: To prove that is surjective (onto), we need to show for any bounded linear functional there exists that has some preimage . Let us build a basis of sequences for , where . Any can be written coordinate-wisely by the linear combination of the sequences of the basis . Then by the linearity of we have If we define , then we find that implies . By definition, maps each sequence of basis to . In addition, we need to show this by which I am not able to finish . In conclusion, we find for corresponding to , therefore is surjective. Finally, is a well-defined isometric isomorphism. Please show me that How to show this is unique?","c_0(\mathbb{N}) 0 \begin{align} T: l^1(\mathbb{N}) \to \left(c_0(\mathbb{N})\right)^*,
 \qquad T(g)(f) := \sum_{n\in\mathbb{N}}f(n)g(n).  \end{align} T(g) c_0(\mathbb{N}) \to
 \mathbb{C} \|T(g)\| = \|g\| c_0(\mathbb{N}) g \in
 l^1(\mathbb{N}) f \in c_0(\mathbb{N}) g \in l^1(\mathbb{N}) T(g) \in c_0^*: c_0(\mathbb{N}) \to \mathbb{C} T(f): l^1(\mathbb{N}) \to \mathbb{C} T(g) c_0(\mathbb{N}) \to \mathbb{C} \|T(g)\| = \|g\|_{l^1} \begin{align}
|T(g)(f)| = \left|\sum_{n\in\mathbb{N}}f(n)g(n)\right| \le \sum_{n\in\mathbb{N}}|f(n)||g(n)| \le \sup_{n\in\mathbb{N}}|f(n)|\sum_{n\in\mathbb{N}}|g(n)| = \|f\|_{l^\infty}\|g\|_{l^1}
\end{align} T(g) \begin{align}
\|T(g)\| = \sup\{|T(g)(f)|: \forall f \in c_0(\mathbb{N}), \|f\|_{l^\infty}\le 1\} \le \|g\|_{l^1}
\end{align} f_1, f_2 \in c_0(\mathbb{N}) a_1, a_2 \in \mathbb{C} \begin{align}
T(g)(a_1 f_1 + a_2 f_2) 
&= \sum_{n\in\mathbb{N}} \left(a_1 f_1(n) + a_2 f_2(n)\right) g(n) \\
&= \sum_{n\in\mathbb{N}} \left(a_1 f_1(n) g(n) + a_2 f_2(n) g(n)\right) \\
&= a_1 \sum_{n\in\mathbb{N}} f_1(n) g(n) + a_2 \sum_{n\in\mathbb{N}} f_2(n) g(n) \\
&= a_1 T(g)(f_1) + a_2 T(g)(f_2) 
\end{align} T(g) T(g): c_0(\mathbb{N}) \to \mathbb{C} g \in l^1(\mathbb{N}) T: l^1(\mathbb{N}) \to \left(c_0(\mathbb{N})\right)^* T \|T(g)\| = \|g\|_{l^1} \|T(g)\| \le \|g\|_{l^1} f \in c_0(\mathbb{N}) \|T(g)\| \ge \|g\|_{l^1} \|T(g)\| = \|g\|_{l^1} g l^1(\mathbb{N}) g = 0 \|T(g)\| = \|g\|_{l^1} g \ne 0 \begin{align}
f(n) := 
\begin{cases}
\frac{\overline{g(n)}}{|g(n)|} &n \le N \\
0 &n > N
\end{cases}
\end{align} c_0(\mathbb{N}) T(g)(f) := \sum_{n\in\mathbb{N}}f(n)g(n) = \sum_{n\in\mathbb{N}}|g(n)| =: \|g\|_{l^1} \|f\|_{l^\infty} = 1 \begin{align}
\|g\|_{l^1} = |T(g)(f)| \le \|T(g)\|\|f\|_{l^\infty} = \|T(g)\|
\end{align} \|T(g)\| \le \|g\|_{l^1} \|T(g)\| = \|g\|_{l^1} T T: l^1(\mathbb{N}) \to \left(c_0(\mathbb{N})\right)^* \forall S \in \left(c_0(\mathbb{N})\right)^* T(g) = S g \in l^1(\mathbb{N}) \{e_n: n = 1, 2,...\} c_0(\mathbb{N}) e_n = (\delta_n)_{n \in \mathbb{N}} \in c_0(\mathbb{N}) f \in c_0(\mathbb{N}) f = \sum_{n\in\mathbb{N}} f(n) e_n S \begin{align}
S(f) = \sum_{n\in\mathbb{N}} f(n) S(e_n)
\end{align} g(n) := S(e_n) \begin{align}
S(f) = \sum_{n\in\mathbb{N}} f(n) S(e_n) = \sum_{n\in\mathbb{N}} f(n) g(n) =: T(g)(f)
\end{align} S = T(g) g(n) T(g)(e_n) g \in l^1(\mathbb{N}) \|g\|_{l^1} \le \|S\| \exists g \in l^1(\mathbb{N}) T(g) \forall S \in \left(c_0(\mathbb{N})\right)^* T T \|g\|_{l^1} \le \|S\| g","['linear-algebra', 'functional-analysis', 'analysis', 'isometry', 'vector-space-isomorphism']"
86,Inverse of Gaussian Kernel Matrix,Inverse of Gaussian Kernel Matrix,,"Let a gaussian kernel be defined as $K(x_i, x_j) \equiv \exp(-\alpha |x_i-x_j|^2)+\beta \delta_{ij}$, and define the kernel matrix of some set of datapoints $\{x_i\}_{i=1}^n$ as the $n\times n$ matrix $K$ with $K_{ij} = K(x_i, x_j)$. This is a common construction in various fields, e.g. Gaussian Processes. Is there a fast way of calculating the inverse of the kernel matrix? Thoughts: if we could break down the matrix $K$ into the form $\beta I + u u^T$ for some column vector $u$, we could use the Sherman–Morrison formula to quickly calculate the inverse, however, we know such a $u$ would be infinite dimensional. Is there another trick one could use?","Let a gaussian kernel be defined as $K(x_i, x_j) \equiv \exp(-\alpha |x_i-x_j|^2)+\beta \delta_{ij}$, and define the kernel matrix of some set of datapoints $\{x_i\}_{i=1}^n$ as the $n\times n$ matrix $K$ with $K_{ij} = K(x_i, x_j)$. This is a common construction in various fields, e.g. Gaussian Processes. Is there a fast way of calculating the inverse of the kernel matrix? Thoughts: if we could break down the matrix $K$ into the form $\beta I + u u^T$ for some column vector $u$, we could use the Sherman–Morrison formula to quickly calculate the inverse, however, we know such a $u$ would be infinite dimensional. Is there another trick one could use?",,"['linear-algebra', 'inverse']"
87,Patterns for eigenvalues of Vandermonde matrix,Patterns for eigenvalues of Vandermonde matrix,,"Let $A$ be a Vandermonde type matrix $A = \begin{bmatrix}  1 & 1 & \dots & 1 \\ x_1 & x_2 &\dots & x_n \\ \dots& \dots & \dots&  \dots\\ x_1^{n-1} &x_2^{n-1} &\dots & x_n^{n-1} \end{bmatrix}$ When I was testing some such real matrices with positive entries for their eigenvalues I have noticed that eigenvalues are also real and positive. How such property (if true in general case) can be explained ? Moreover in examples   which I tested (when it was assumed for $i<j$ that  $x_i <x_j$) the greatest eigenvalue was always close to the greatest value in this matrix and the smallest one always less than $1$. How to explain also these facts ? Examples of $3 \times 3$ matrices generated from natural numbers $A = \begin{bmatrix}  1 & 1 &    1 \\ 2 & 3   & 5 \\ 4 & 9   & 25 \end{bmatrix}$ Eigenvalues: $\{ 27.09 , \ 0.12  , \ 1.79 \} $ $A = \begin{bmatrix}  1 & 1 &    1 \\ 3 & 7   &  11 \\ 9 & 49   & 121 \end{bmatrix}$ Eigenvalues: $\{ 125.63,  \ 0.37 , \ 3.03  \} $ Of course if two eigenvalues in the examples above are rather small then the third must be rather great from the trace of the matrix, but why these two must be small ? If the answer for general case is hard to find let dimension of a matrix be specific i.e. $ 3 \times 3$ and entries only natural.","Let $A$ be a Vandermonde type matrix $A = \begin{bmatrix}  1 & 1 & \dots & 1 \\ x_1 & x_2 &\dots & x_n \\ \dots& \dots & \dots&  \dots\\ x_1^{n-1} &x_2^{n-1} &\dots & x_n^{n-1} \end{bmatrix}$ When I was testing some such real matrices with positive entries for their eigenvalues I have noticed that eigenvalues are also real and positive. How such property (if true in general case) can be explained ? Moreover in examples   which I tested (when it was assumed for $i<j$ that  $x_i <x_j$) the greatest eigenvalue was always close to the greatest value in this matrix and the smallest one always less than $1$. How to explain also these facts ? Examples of $3 \times 3$ matrices generated from natural numbers $A = \begin{bmatrix}  1 & 1 &    1 \\ 2 & 3   & 5 \\ 4 & 9   & 25 \end{bmatrix}$ Eigenvalues: $\{ 27.09 , \ 0.12  , \ 1.79 \} $ $A = \begin{bmatrix}  1 & 1 &    1 \\ 3 & 7   &  11 \\ 9 & 49   & 121 \end{bmatrix}$ Eigenvalues: $\{ 125.63,  \ 0.37 , \ 3.03  \} $ Of course if two eigenvalues in the examples above are rather small then the third must be rather great from the trace of the matrix, but why these two must be small ? If the answer for general case is hard to find let dimension of a matrix be specific i.e. $ 3 \times 3$ and entries only natural.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
88,Roles of $\bf A^TA$ ($\text {A transpose A}$) matrices in orthogonal projection,Roles of  () matrices in orthogonal projection,\bf A^TA \text {A transpose A},"$\bf A^TA$ forms (or equivalently (?) positive semidefinite matrices , or more particularly, covariance matrices($\bf \Sigma$)) are linked in practice to many operations in which data points are orthogonally projected: In ordinary linear regression (OLS) is part of the projection matrix $\bf P = X(\color{blue}{X^TX})^{−1}X^T$ of the ""dependent variable"" on the column space of the model matrix. In principal component analysis (PCA) the data is projected on the eigenvectors of the covariance matrix. The covariance matrix informs white random ""white"" samples into diagonal projections in Gaussian processes , which seems intuitively to correspond to a way of projecting. But I am looking at a unifying explanation. A more generic concept. In this regard, I have come across the sentence, ""It is as if the covariance matrix stored all possible projection variances in all directions,"" a statement seemingly supported by the fact that a for data cloud in $\mathbb R^n$, the variance of the projection of the points onto a unit vector $\bf u$ will be given by $\bf u^T \Sigma u$. So is there a way of unify all these inter-related properties into a single set of principles from which all the applications and geometric derivations can be seen? I believe that the unifying theme is related to the the orthogonal diagonalization $\bf A^T A = U^T D U$ as mentioned here , but I'd like to see this idea explained a bit further. EXEGETICAL APPENDIX for novices: It was far from self-evident, but after some help by Michael Hardy and @stewbasic, the answer by Étienne Bézout may be starting to click. So like in the move Memento , I'd better tattoo what I got so far here in case it is blurry in the morning: Concept One: Block matrix multiplication: \begin{align} A^\top A & = \begin{bmatrix}  \vdots & \vdots & \vdots & \cdots & \vdots \\                    a_1^\top    & a_2^\top    &  a_3^\top   & \cdots & a_{\color{blue}{\bf n}}^\top\\                    \vdots & \vdots & \vdots & \cdots & \vdots\end{bmatrix}           \begin{bmatrix}                    \cdots & a_1 & \cdots\\                    \cdots & a_2 & \cdots \\                    \cdots & a_3 & \cdots \\                    & \vdots&\\                     \cdots & a_{\color{blue}{\bf n}} & \cdots              \end{bmatrix}\\             &= a_1^\top a_1 + a_2^\top a_2 + a_3^\top a_3 + \cdots+a_n^\top a_n\tag{1} \end{align} where $a_i$'s are $[\color{blue}{1 \times \bf n}]$ row vectors. Concept Two: The $\color{blue}{\bf n}$. We have the same dimensions for the block matrix multiplication $\bf \underset{[\text{many rows} \times \color{blue}{\bf n}]}{\bf A^\top}\underset{[\color{blue}{\bf n} \times \text{many rows}]}{\bf A} =\large [\color{blue}{\bf n} \times \color{blue}{\bf n}] \small \text{ matrix}$, as for each individual summand $\bf a_i^\top a_i$ in Eq. 1. Concept Three: $\bf a_i^\top a_i$ is deceptive because of the key definition: row vector. Because $\bf a_i$ was defined as a row vector, and the $\bf a_i$ vectors are normalized ($\vert a_i \vert =1$), $\bf a_i^\top a_i$ is really a matrix of the form $\bf XX^\top$, which is a projection matrix provided the $a_i$ vectors are independent (check: ""...are linearly independent""), and orthonormal (not a requisite in the answer (""I'm no longer saying they are orthogonal"")) - $\color{red}{\text{Do these vectors actually need to be defined as orthonormal?}}$ Or can this constraint of orthonormality of the vectors $a_i$ be relaxed, or implicitly fulfilled by virtue of other considerations? Otherwise we have a rather specific $\bf A$ matrix, making the results less generalizable. Concept Four: A projection onto what? Onto the subspace spanned by the column space of $\bf X$ (think OLS projection ${\bf A}\color{gray}{(A^\top A)^{-1}} {\bf A^\top}$). But what is $\bf X$ here? None other than $\bf a_i^\top$, and since $\bf a_i$ is a row vector, $\bf a_i^\top$ is a column vector. So we are doing ortho-projections onto the column space of $\bf A^\top$, which is in $\mathbb R^{\color{blue}{\bf n}}$. I was hoping that the last sentence could have been, ""... onto the column space of $\bf A$... What are the implications?","$\bf A^TA$ forms (or equivalently (?) positive semidefinite matrices , or more particularly, covariance matrices($\bf \Sigma$)) are linked in practice to many operations in which data points are orthogonally projected: In ordinary linear regression (OLS) is part of the projection matrix $\bf P = X(\color{blue}{X^TX})^{−1}X^T$ of the ""dependent variable"" on the column space of the model matrix. In principal component analysis (PCA) the data is projected on the eigenvectors of the covariance matrix. The covariance matrix informs white random ""white"" samples into diagonal projections in Gaussian processes , which seems intuitively to correspond to a way of projecting. But I am looking at a unifying explanation. A more generic concept. In this regard, I have come across the sentence, ""It is as if the covariance matrix stored all possible projection variances in all directions,"" a statement seemingly supported by the fact that a for data cloud in $\mathbb R^n$, the variance of the projection of the points onto a unit vector $\bf u$ will be given by $\bf u^T \Sigma u$. So is there a way of unify all these inter-related properties into a single set of principles from which all the applications and geometric derivations can be seen? I believe that the unifying theme is related to the the orthogonal diagonalization $\bf A^T A = U^T D U$ as mentioned here , but I'd like to see this idea explained a bit further. EXEGETICAL APPENDIX for novices: It was far from self-evident, but after some help by Michael Hardy and @stewbasic, the answer by Étienne Bézout may be starting to click. So like in the move Memento , I'd better tattoo what I got so far here in case it is blurry in the morning: Concept One: Block matrix multiplication: \begin{align} A^\top A & = \begin{bmatrix}  \vdots & \vdots & \vdots & \cdots & \vdots \\                    a_1^\top    & a_2^\top    &  a_3^\top   & \cdots & a_{\color{blue}{\bf n}}^\top\\                    \vdots & \vdots & \vdots & \cdots & \vdots\end{bmatrix}           \begin{bmatrix}                    \cdots & a_1 & \cdots\\                    \cdots & a_2 & \cdots \\                    \cdots & a_3 & \cdots \\                    & \vdots&\\                     \cdots & a_{\color{blue}{\bf n}} & \cdots              \end{bmatrix}\\             &= a_1^\top a_1 + a_2^\top a_2 + a_3^\top a_3 + \cdots+a_n^\top a_n\tag{1} \end{align} where $a_i$'s are $[\color{blue}{1 \times \bf n}]$ row vectors. Concept Two: The $\color{blue}{\bf n}$. We have the same dimensions for the block matrix multiplication $\bf \underset{[\text{many rows} \times \color{blue}{\bf n}]}{\bf A^\top}\underset{[\color{blue}{\bf n} \times \text{many rows}]}{\bf A} =\large [\color{blue}{\bf n} \times \color{blue}{\bf n}] \small \text{ matrix}$, as for each individual summand $\bf a_i^\top a_i$ in Eq. 1. Concept Three: $\bf a_i^\top a_i$ is deceptive because of the key definition: row vector. Because $\bf a_i$ was defined as a row vector, and the $\bf a_i$ vectors are normalized ($\vert a_i \vert =1$), $\bf a_i^\top a_i$ is really a matrix of the form $\bf XX^\top$, which is a projection matrix provided the $a_i$ vectors are independent (check: ""...are linearly independent""), and orthonormal (not a requisite in the answer (""I'm no longer saying they are orthogonal"")) - $\color{red}{\text{Do these vectors actually need to be defined as orthonormal?}}$ Or can this constraint of orthonormality of the vectors $a_i$ be relaxed, or implicitly fulfilled by virtue of other considerations? Otherwise we have a rather specific $\bf A$ matrix, making the results less generalizable. Concept Four: A projection onto what? Onto the subspace spanned by the column space of $\bf X$ (think OLS projection ${\bf A}\color{gray}{(A^\top A)^{-1}} {\bf A^\top}$). But what is $\bf X$ here? None other than $\bf a_i^\top$, and since $\bf a_i$ is a row vector, $\bf a_i^\top$ is a column vector. So we are doing ortho-projections onto the column space of $\bf A^\top$, which is in $\mathbb R^{\color{blue}{\bf n}}$. I was hoping that the last sentence could have been, ""... onto the column space of $\bf A$... What are the implications?",,"['linear-algebra', 'projection-matrices']"
89,Why are there no $16$ by $32$ Hadamard circulant matrices?,Why are there no  by  Hadamard circulant matrices?,16 32,"Two rows of a matrix are orthogonal if their inner product equals zero. Call a matrix with all rows pairwise orthogonal an orthogonal matrix . A circulant matrix is one where each row vector is rotated one element to the right relative to the preceding row vector.  We will only consider matrices whose entries are either $-1$ or $1$. For number of columns $n= 4,8,12,16,20,24,28, 36$ there exist $n/2$ by $n$ orthogonal circulant matrices. Why are there no circulant matrices with $16$ rows and $32$ columns which are orthogonal? Or to phrase it differently, is it possible to prove they don't exist without enumerating them all? Example 6 by 12 matrix \begin{pmatrix}   -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1 &\phantom{-}1 & -1 & -1 & -1 & -1\\   -1 & -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1 &\phantom{-}1 & -1 & -1 & -1\\   -1 & -1 & -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1 &\phantom{-}1 & -1 & -1\\   -1 & -1 & -1 & -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1 &\phantom{-}1 & -1\\   -1 & -1 & -1 & -1 & -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1 &\phantom{-}1\\   \phantom{-}1 & -1 & -1 & -1 & -1 & -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1\\\end{pmatrix}","Two rows of a matrix are orthogonal if their inner product equals zero. Call a matrix with all rows pairwise orthogonal an orthogonal matrix . A circulant matrix is one where each row vector is rotated one element to the right relative to the preceding row vector.  We will only consider matrices whose entries are either $-1$ or $1$. For number of columns $n= 4,8,12,16,20,24,28, 36$ there exist $n/2$ by $n$ orthogonal circulant matrices. Why are there no circulant matrices with $16$ rows and $32$ columns which are orthogonal? Or to phrase it differently, is it possible to prove they don't exist without enumerating them all? Example 6 by 12 matrix \begin{pmatrix}   -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1 &\phantom{-}1 & -1 & -1 & -1 & -1\\   -1 & -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1 &\phantom{-}1 & -1 & -1 & -1\\   -1 & -1 & -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1 &\phantom{-}1 & -1 & -1\\   -1 & -1 & -1 & -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1 &\phantom{-}1 & -1\\   -1 & -1 & -1 & -1 & -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1 &\phantom{-}1\\   \phantom{-}1 & -1 & -1 & -1 & -1 & -1 &\phantom{-}1 &\phantom{-}1 & -1 & -1 &\phantom{-}1 & -1\\\end{pmatrix}",,"['linear-algebra', 'combinatorics']"
90,Classification theorem for vector spaces,Classification theorem for vector spaces,,"As I was going over the classification theorem for closed surfaces today, the text I'm reading gave another example of a classification theorem: finite dimensional vector spaces are classified by their dimension.  As a point of fact, I think that this is slightly wrong -- if I understand what the author was trying to say I think that finite dimensional vector spaces are classified by their dimension and their base field.  Then he's just talking about that theorem that says that any $\Bbb F$-vector space with dimension $n$ is isomorphic to $\Bbb F^n$. His use of the word ""finite"" though has me wondering, does the same thing hold for infinite dimensional vector spaces?  Can we ""classify"" infinite dimensional vector spaces by their dimensions (meaning $\aleph_0$, $\aleph_1$, etc) and base fields?  Or is there something more complex that happens for infinite dimensional spaces?","As I was going over the classification theorem for closed surfaces today, the text I'm reading gave another example of a classification theorem: finite dimensional vector spaces are classified by their dimension.  As a point of fact, I think that this is slightly wrong -- if I understand what the author was trying to say I think that finite dimensional vector spaces are classified by their dimension and their base field.  Then he's just talking about that theorem that says that any $\Bbb F$-vector space with dimension $n$ is isomorphic to $\Bbb F^n$. His use of the word ""finite"" though has me wondering, does the same thing hold for infinite dimensional vector spaces?  Can we ""classify"" infinite dimensional vector spaces by their dimensions (meaning $\aleph_0$, $\aleph_1$, etc) and base fields?  Or is there something more complex that happens for infinite dimensional spaces?",,['linear-algebra']
91,Why more than 3 dimensions in linear algebra?,Why more than 3 dimensions in linear algebra?,,"This might seem a silly question, but I was wondering why mathematicians came out with more than 3 dimensions when studying vector spaces, matrices, etc. I cannot visualise more than 3 dimensions, so I am not seeing the goal of having more than 3 dimensions. Except from the fact that the general rules of linear algebra can be applied mathematically to vectors spaces of higher dimensions, what is the practical purpose of having more than 3 dimensions? Linear algebra is also the study of system of linear equations that can have more than 3 unknowns, is this maybe related?","This might seem a silly question, but I was wondering why mathematicians came out with more than 3 dimensions when studying vector spaces, matrices, etc. I cannot visualise more than 3 dimensions, so I am not seeing the goal of having more than 3 dimensions. Except from the fact that the general rules of linear algebra can be applied mathematically to vectors spaces of higher dimensions, what is the practical purpose of having more than 3 dimensions? Linear algebra is also the study of system of linear equations that can have more than 3 unknowns, is this maybe related?",,['linear-algebra']
92,Origin of the term dual space?,Origin of the term dual space?,,"Basically, why is a dual vector space called as such? Is the reason for the term ""dual"" simply because the two vector spaces are related by a one-to-one mapping, or is there something more to it? Sorry for such a basic question, but I've never been able to find a definitive answer so far.","Basically, why is a dual vector space called as such? Is the reason for the term ""dual"" simply because the two vector spaces are related by a one-to-one mapping, or is there something more to it? Sorry for such a basic question, but I've never been able to find a definitive answer so far.",,"['linear-algebra', 'terminology']"
93,How many matrices can commute with a given matrix?,How many matrices can commute with a given matrix?,,"I'm trying to learn linear and abstract algebra on my own and have been attempting textbook exercises and problem sets I find online. I've been doing okay so far but I found this problem and I'm having a lot of trouble with it: Let $A$ be an $n \times n$ complex matrix. a) Show that the set of matrices commuting with $A$ is a subspace. b) What is the dimension of this subspace? I think I got the first part. It wasn't that bad.  But I'm having trouble with the second part. I feel like this is supposed to be an easy question, but I just don't know how to start it. I was thinking about using Jordan form somehow. If $A$ ~ $J_A$ and $B$ ~ $J_B$ , is it true that if $J_A J_B = J_B J_A$ then $AB = BA$ ? If it is, then we'd only have to look at the Jordan blocks of these and see when those commute with each other. Then the problem wouldn't be so bad, I think. I'd love some hints.","I'm trying to learn linear and abstract algebra on my own and have been attempting textbook exercises and problem sets I find online. I've been doing okay so far but I found this problem and I'm having a lot of trouble with it: Let be an complex matrix. a) Show that the set of matrices commuting with is a subspace. b) What is the dimension of this subspace? I think I got the first part. It wasn't that bad.  But I'm having trouble with the second part. I feel like this is supposed to be an easy question, but I just don't know how to start it. I was thinking about using Jordan form somehow. If ~ and ~ , is it true that if then ? If it is, then we'd only have to look at the Jordan blocks of these and see when those commute with each other. Then the problem wouldn't be so bad, I think. I'd love some hints.",A n \times n A A J_A B J_B J_A J_B = J_B J_A AB = BA,"['linear-algebra', 'matrices']"
94,"Cholesky, Inverse, and Determinant when updating the diagonal of a symmetric positive definite matrix","Cholesky, Inverse, and Determinant when updating the diagonal of a symmetric positive definite matrix",,"Suppose that $A$ is a symmetric positive definite matrix and assume its dimension $n$ is large. Let $I$ be the $n \times n$ identity matrix and $m \neq 0$ be a scalar. I'm interested in computing as many of the following as possible: det$(A + mI)$ $(A + mI)^{-1}$ $(A + mI)^{-1}B$, where $B$ is an $n \times m$ matrix the Cholesky decomposition of $A + mI$, I'd like to do this for many values of $m$. However, because $n$ is large, I'd like to know if there is some update trick based on det$(A)$, $A^{-1}$, and the Cholesky decomposition of $A$. $A$ will likely not be sparse. I've been researching this for quite a while and the results haven't been all that encouraging. Any help, hints, suggestions, or references would be much appreciated!","Suppose that $A$ is a symmetric positive definite matrix and assume its dimension $n$ is large. Let $I$ be the $n \times n$ identity matrix and $m \neq 0$ be a scalar. I'm interested in computing as many of the following as possible: det$(A + mI)$ $(A + mI)^{-1}$ $(A + mI)^{-1}B$, where $B$ is an $n \times m$ matrix the Cholesky decomposition of $A + mI$, I'd like to do this for many values of $m$. However, because $n$ is large, I'd like to know if there is some update trick based on det$(A)$, $A^{-1}$, and the Cholesky decomposition of $A$. $A$ will likely not be sparse. I've been researching this for quite a while and the results haven't been all that encouraging. Any help, hints, suggestions, or references would be much appreciated!",,"['linear-algebra', 'matrices', 'determinant', 'inverse', 'matrix-decomposition']"
95,Conic hull of outer products,Conic hull of outer products,,"Consider the set of rank- $k$ outer products, defined as $$\left\{ X X^T \mid X \in \Bbb R^{n \times k}, \mbox{rank} (X) = k \right\}$$ Describe its conic hull in simple terms. I have found the solution of this exercise but I have quesions on that. So I will start writing the solution step by step and make the relevant questions for each step. Why $XX^T$ is expressed as an outer product? We have $XX^T \geq 0$ and $rank(XX^T) = k$ (using SVD on X). Why it is that? A positive linear combination of such matrices can have $rank$ up to n, but never less than k. Is there specific theorem about that? Then a proof follows on 3: Let $A,B\geq 0$ of $rank$ k. (This came from the exercise definition.) With $rank(A+B)=r\leq k$ . We know that $rank(A+B) \leq rank(A) + rank(B) = 2k$ . Why $rank(A+B)=r\leq k$ holds? Let $V\in R^{n \times (n-r)}$ such that $\mathcal{R}(V) = \mathcal{N(A+B)}$ . Why it is that? We have $V: R^{n-r}\to R^n$ and $A: R^k \to R^n$ , $\mathcal{N(A+B) \subseteq R^n}$ and $\mathcal{R(V) \subseteq R^n}$ . Since $A,B \geq 0$ , this means $V^TAV = V^TBV = 0$ , which implies that $rank(A)\leq r$ and $rank(A)\leq r$ , which is a contradiction. Why the last two inequalities hold? Then they conclude that $rank(A+B) \geq k$ for any $A$ , $B$ such that $rank(A)=rank(B)=k$ and $A,B \geq 0$ . (This is because of 4.) Conversely, any non-zero matrix of $rank$ at least k can be written as the sum of several matrices of $rank$ k (using SVD decomposition for of $A = XX^T = UΣV^T$ and note that the diagonal of Σ has at least k positive entries). How is that contribute to the proof? Concludes that, that the conic hull of the set of $rank$ -k outer products is the set of positive semi-definite matrices of $rank$ greater than or equal to k, along with the zero matrix. How they come up with his idea? And more specifically, how the zero matrix is involved on that. Any help on each of them is highly appreciated. It is even more appreciated a good reference, i.e., a book. Thank you! These steps mentioned here .","Consider the set of rank- outer products, defined as Describe its conic hull in simple terms. I have found the solution of this exercise but I have quesions on that. So I will start writing the solution step by step and make the relevant questions for each step. Why is expressed as an outer product? We have and (using SVD on X). Why it is that? A positive linear combination of such matrices can have up to n, but never less than k. Is there specific theorem about that? Then a proof follows on 3: Let of k. (This came from the exercise definition.) With . We know that . Why holds? Let such that . Why it is that? We have and , and . Since , this means , which implies that and , which is a contradiction. Why the last two inequalities hold? Then they conclude that for any , such that and . (This is because of 4.) Conversely, any non-zero matrix of at least k can be written as the sum of several matrices of k (using SVD decomposition for of and note that the diagonal of Σ has at least k positive entries). How is that contribute to the proof? Concludes that, that the conic hull of the set of -k outer products is the set of positive semi-definite matrices of greater than or equal to k, along with the zero matrix. How they come up with his idea? And more specifically, how the zero matrix is involved on that. Any help on each of them is highly appreciated. It is even more appreciated a good reference, i.e., a book. Thank you! These steps mentioned here .","k \left\{ X X^T \mid X \in \Bbb R^{n \times k}, \mbox{rank} (X) = k \right\} XX^T XX^T \geq 0 rank(XX^T) = k rank A,B\geq 0 rank rank(A+B)=r\leq k rank(A+B) \leq rank(A) + rank(B) = 2k rank(A+B)=r\leq k V\in R^{n \times (n-r)} \mathcal{R}(V) = \mathcal{N(A+B)} V: R^{n-r}\to R^n A: R^k \to R^n \mathcal{N(A+B) \subseteq R^n} \mathcal{R(V) \subseteq R^n} A,B \geq 0 V^TAV = V^TBV = 0 rank(A)\leq r rank(A)\leq r rank(A+B) \geq k A B rank(A)=rank(B)=k A,B \geq 0 rank rank A = XX^T = UΣV^T rank rank","['linear-algebra', 'matrices', 'convex-optimization', 'matrix-rank', 'positive-semidefinite']"
96,When is the equation $Ax = b$ solvable in the integers?,When is the equation  solvable in the integers?,Ax = b,"Let $A$ be an $m\times n$ matrix with integer entries, $b$ a column-vector with $m$ integer   entries. Suppose the equation $Ax = b$ has infinitely many solutions. It is clear that the general solution of the equation $Ax = 0$ can be written as a   linear combination of vectors with integer values. But what about a special solution of $Ax = b$ ? When does a vector $x$ with $n$ integer entries exist with $Ax = b$? In other words,    when is $Ax = b$ solveable in the integers ? If it is, how can I find the integer    solution ? I tried to implement the function linsolve in PARI/GP. PARI/GP deliveres the  kernel of A with integral entries. Using matsupplement, I can find a special  solution, if no column-exchanges are required. But for the martrix $$\begin{pmatrix} 1 & 1 &3 \\ 0 & 0 &1 \\ 0 &0 &0 \end{pmatrix}$$ and the column vector $\begin{pmatrix}8 \\ 1 \\0\end{pmatrix}$ , the wrong  solution $\begin{pmatrix}5 \\ 1 \\0\end{pmatrix}$ is given instead of the correct $\begin{pmatrix}5 \\ 0 \\1\end{pmatrix}$. Is there a way to  always find a correct special solution with PARI/GP, assuming the system is  solveable ?","Let $A$ be an $m\times n$ matrix with integer entries, $b$ a column-vector with $m$ integer   entries. Suppose the equation $Ax = b$ has infinitely many solutions. It is clear that the general solution of the equation $Ax = 0$ can be written as a   linear combination of vectors with integer values. But what about a special solution of $Ax = b$ ? When does a vector $x$ with $n$ integer entries exist with $Ax = b$? In other words,    when is $Ax = b$ solveable in the integers ? If it is, how can I find the integer    solution ? I tried to implement the function linsolve in PARI/GP. PARI/GP deliveres the  kernel of A with integral entries. Using matsupplement, I can find a special  solution, if no column-exchanges are required. But for the martrix $$\begin{pmatrix} 1 & 1 &3 \\ 0 & 0 &1 \\ 0 &0 &0 \end{pmatrix}$$ and the column vector $\begin{pmatrix}8 \\ 1 \\0\end{pmatrix}$ , the wrong  solution $\begin{pmatrix}5 \\ 1 \\0\end{pmatrix}$ is given instead of the correct $\begin{pmatrix}5 \\ 0 \\1\end{pmatrix}$. Is there a way to  always find a correct special solution with PARI/GP, assuming the system is  solveable ?",,"['linear-algebra', 'matrices', 'systems-of-equations']"
97,Cubic convergence of Rayleigh quotient iteration?,Cubic convergence of Rayleigh quotient iteration?,,"Trefethen and Bau, Numerical Linear Algebra, p. 208 states that Rayleigh quotient iteration (combining Rayleigh quotient estimate for eigenvalues and inverse power iteration) converges cubically ...the convergence is ultimately cubic in the sense that if   $\lambda_J$ is an eigenvalue of $A$ and $v^{(0)}$ is sufficiently   close to the eigenvector $q_J$,    then $$\|v^{(k+1)} - (\pm q_J)\| =  O(\|v^{(k)} - (\pm q_J)\|^3)$$ and $$|\lambda^{(k+1)} - \lambda_J| = > O(|\lambda^{(k)} - \lambda_J|^3)$$ as $k \rightarrow \infty$. Their argument is as follows. Suppose that convergence occurs, and that $\|v^{(k)} - q_J\| \leq \epsilon$ for some small $\epsilon$. Then the Rayleigh quotient estimation for an eigenvalue gives an eigenvalue estimate $\lambda^{(k)}$ with $|\lambda^{(k)} - \lambda_J| = O(\epsilon^2)$. Then apply the proof of the inverse power iteration for one step to obtain $v^{(k+1)}$ from $v^{(k)}$ and $\lambda^{(k)}$, so that $$\|v^{(k+1)} - q_J \|= O(|\lambda^{(k)}-\lambda_J| \|v^{(k)} - q_J\|) = O(\epsilon^3)$$ with the constants in the big-Oh symbols uniform in sufficiently small neighborhoods. I don't see the left most equality above, nor why the constants are uniform. It seems that all power iteration gives is $$O(|\lambda^{(k)}-\lambda_J|/|\lambda^{(k)}-\lambda_K|)$$ where $\lambda_K$ is the second closest eigenvalue.","Trefethen and Bau, Numerical Linear Algebra, p. 208 states that Rayleigh quotient iteration (combining Rayleigh quotient estimate for eigenvalues and inverse power iteration) converges cubically ...the convergence is ultimately cubic in the sense that if   $\lambda_J$ is an eigenvalue of $A$ and $v^{(0)}$ is sufficiently   close to the eigenvector $q_J$,    then $$\|v^{(k+1)} - (\pm q_J)\| =  O(\|v^{(k)} - (\pm q_J)\|^3)$$ and $$|\lambda^{(k+1)} - \lambda_J| = > O(|\lambda^{(k)} - \lambda_J|^3)$$ as $k \rightarrow \infty$. Their argument is as follows. Suppose that convergence occurs, and that $\|v^{(k)} - q_J\| \leq \epsilon$ for some small $\epsilon$. Then the Rayleigh quotient estimation for an eigenvalue gives an eigenvalue estimate $\lambda^{(k)}$ with $|\lambda^{(k)} - \lambda_J| = O(\epsilon^2)$. Then apply the proof of the inverse power iteration for one step to obtain $v^{(k+1)}$ from $v^{(k)}$ and $\lambda^{(k)}$, so that $$\|v^{(k+1)} - q_J \|= O(|\lambda^{(k)}-\lambda_J| \|v^{(k)} - q_J\|) = O(\epsilon^3)$$ with the constants in the big-Oh symbols uniform in sufficiently small neighborhoods. I don't see the left most equality above, nor why the constants are uniform. It seems that all power iteration gives is $$O(|\lambda^{(k)}-\lambda_J|/|\lambda^{(k)}-\lambda_K|)$$ where $\lambda_K$ is the second closest eigenvalue.",,"['linear-algebra', 'numerical-methods']"
98,Is it possible to use the imaginary components of quaternions to facilitate calculation of vector cross products?,Is it possible to use the imaginary components of quaternions to facilitate calculation of vector cross products?,,"It has come to my attention that the cross products of the vectors $\mathbf{i}$, $\mathbf{j}$, and $\mathbf{k}$ are almost identical to the products of the imaginary components of quaternions $i$, $j$, and $k$. If one were to ignore the real portion of quaternions, could one find cross products by representing the vectors as quaternions? Another question: would it be possible to use quaternions to find cross products of Cartesian vectors in $(x, y, z, t)$? If so, could this be used as a representation of physical entities such as space-time?","It has come to my attention that the cross products of the vectors $\mathbf{i}$, $\mathbf{j}$, and $\mathbf{k}$ are almost identical to the products of the imaginary components of quaternions $i$, $j$, and $k$. If one were to ignore the real portion of quaternions, could one find cross products by representing the vectors as quaternions? Another question: would it be possible to use quaternions to find cross products of Cartesian vectors in $(x, y, z, t)$? If so, could this be used as a representation of physical entities such as space-time?",,"['linear-algebra', 'quaternions']"
99,Geometric intuition of adjoint,Geometric intuition of adjoint,,For a linear operator it holds that $\ker (T^\ast ) = (\operatorname{ran} (T))^\perp$. The star denote the adjoint of $T$ and $\perp$ the orthogonal complement. Is there a geometric intuition for the meaning of $\ker (T^\ast ) = (\operatorname{ran} (T))^\perp$?,For a linear operator it holds that $\ker (T^\ast ) = (\operatorname{ran} (T))^\perp$. The star denote the adjoint of $T$ and $\perp$ the orthogonal complement. Is there a geometric intuition for the meaning of $\ker (T^\ast ) = (\operatorname{ran} (T))^\perp$?,,['linear-algebra']
