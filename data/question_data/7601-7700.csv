,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to prove that $f(x)=e^{x}$ is not a polynomial.,How to prove that  is not a polynomial.,f(x)=e^{x},It is obvious that if we differentiate $f(x) = e^{x}$ with respect to x we will get again and again $e^{x}$. Can we conclude anything by considering the behavior at $\pm\alpha$,It is obvious that if we differentiate $f(x) = e^{x}$ with respect to x we will get again and again $e^{x}$. Can we conclude anything by considering the behavior at $\pm\alpha$,,"['real-analysis', 'polynomials', 'convergence-divergence', 'uniform-convergence', 'uniform-continuity']"
1,Construct a sequence of measureable sets $E_1\supseteq E_2 \supseteq E_3 \supseteq \cdots$ such that $\mu(E_n)=\infty$ for each $n$ but ...,Construct a sequence of measureable sets  such that  for each  but ...,E_1\supseteq E_2 \supseteq E_3 \supseteq \cdots \mu(E_n)=\infty n,"Construct a sequence of measureable sets $E_1\supseteq E_2 \supseteq E_3 \supseteq \cdots$ such that $\mu(E_n)=\infty$ for each $n$ but $$\mu\left(\bigcap_{n=1}^\infty E_n\right)=0$$ Claim: Let  \begin{equation*} \begin{aligned} E_1= & \left(\frac{1}{i},1\right]\cup \left(\frac{1}{i+1},2\right] \cup \cdots \\ E_2= & \left(\frac{1}{i},2\right]\cup \left(\frac{1}{i+1},3\right] \cup \cdots \\ \vdots & \vdots \\ E_n= & \left(\frac{1}{i},n\right]\cup \left(\frac{1}{i+1},n+1\right] \cup \cdots \\ \vdots & \vdots \\ \end{aligned} \end{equation*} where $i$ is an arbitrary positive integer. I believe that this sequence of sets satisfies the conditions above, but I want to formally write it out.","Construct a sequence of measureable sets $E_1\supseteq E_2 \supseteq E_3 \supseteq \cdots$ such that $\mu(E_n)=\infty$ for each $n$ but $$\mu\left(\bigcap_{n=1}^\infty E_n\right)=0$$ Claim: Let  \begin{equation*} \begin{aligned} E_1= & \left(\frac{1}{i},1\right]\cup \left(\frac{1}{i+1},2\right] \cup \cdots \\ E_2= & \left(\frac{1}{i},2\right]\cup \left(\frac{1}{i+1},3\right] \cup \cdots \\ \vdots & \vdots \\ E_n= & \left(\frac{1}{i},n\right]\cup \left(\frac{1}{i+1},n+1\right] \cup \cdots \\ \vdots & \vdots \\ \end{aligned} \end{equation*} where $i$ is an arbitrary positive integer. I believe that this sequence of sets satisfies the conditions above, but I want to formally write it out.",,"['real-analysis', 'proof-writing']"
2,What is the limit of $\lim_{n\to \infty} \frac{1}{n^{k+1}}\left(k!+\frac{(k+1)!}{1!}+\frac{(k+2)!}{2!}+\cdots+\frac{(k+n)!}{n!}\right)=?$,What is the limit of,\lim_{n\to \infty} \frac{1}{n^{k+1}}\left(k!+\frac{(k+1)!}{1!}+\frac{(k+2)!}{2!}+\cdots+\frac{(k+n)!}{n!}\right)=?,"For $k\in\mathbb{N},$ $$\lim_{n\to \infty} \frac{1}{n^{k+1}}\left(k!+\frac{(k+1)!}{1!}+\frac{(k+2)!}{2!}+\cdots+\frac{(k+n)!}{n!}\right)=?$$ So I am trying to find this limit. Try to apply limit comparison but don't know where to start. I observe that $(k+n)!\geq n!$, is this fact going to help me somehow? Help me out. Thanks.","For $k\in\mathbb{N},$ $$\lim_{n\to \infty} \frac{1}{n^{k+1}}\left(k!+\frac{(k+1)!}{1!}+\frac{(k+2)!}{2!}+\cdots+\frac{(k+n)!}{n!}\right)=?$$ So I am trying to find this limit. Try to apply limit comparison but don't know where to start. I observe that $(k+n)!\geq n!$, is this fact going to help me somehow? Help me out. Thanks.",,"['real-analysis', 'sequences-and-series', 'limits']"
3,Is outer measure a measure?,Is outer measure a measure?,,"In my text book Lebesgue measure is shown to have countable additivity on disjoint sets, i.e. if $A_k$ is a countable sequence of disjoint measurable sets, then $\mu(\bigcup A_k)=\sum\mu(A_k)$. Thus Lebesgue measure is a measure with an additional property that the empty set has zero measure. But for outer measure, my text book never says outer measure is a measure, and it does not say anything if outer measure has countable additivity on disjoint sets. So my question is does outer measure has such countable additivity ? If not, is there a counter example?","In my text book Lebesgue measure is shown to have countable additivity on disjoint sets, i.e. if $A_k$ is a countable sequence of disjoint measurable sets, then $\mu(\bigcup A_k)=\sum\mu(A_k)$. Thus Lebesgue measure is a measure with an additional property that the empty set has zero measure. But for outer measure, my text book never says outer measure is a measure, and it does not say anything if outer measure has countable additivity on disjoint sets. So my question is does outer measure has such countable additivity ? If not, is there a counter example?",,"['real-analysis', 'measure-theory', 'outer-measure']"
4,How can I visualize the discrete metric?,How can I visualize the discrete metric?,,"I saw that in a real analysis proof, they used a proof by contradiction where the metric was a discrete metric. That is, distance is defined to be 1 if the points ARENT the same and 0 if the points are the same. I was trying to visualize how this would look for a unit circle. Does such a visualization exist? What is the correct way of viewing it in 2 dimensions? Thank you!","I saw that in a real analysis proof, they used a proof by contradiction where the metric was a discrete metric. That is, distance is defined to be 1 if the points ARENT the same and 0 if the points are the same. I was trying to visualize how this would look for a unit circle. Does such a visualization exist? What is the correct way of viewing it in 2 dimensions? Thank you!",,['real-analysis']
5,Convergence of $\sum \frac{\sqrt{a_n}}{n^p}$,Convergence of,\sum \frac{\sqrt{a_n}}{n^p},"For $a_n \geq 0$, and $\sum a_n$ convergent, show that  $\sum \frac{\sqrt{a_n}}{n^p}$ is also convergent for $p > 1/2$? What bugs me more is why isn't $\sum \sqrt{\frac{a_n}{n}}$ convergent?? Clearly it converges for $p > 1$, and $a_n$ somehow helps out in $p \in (1/2,1)$. Only hints are welcomed Thanks in advance..","For $a_n \geq 0$, and $\sum a_n$ convergent, show that  $\sum \frac{\sqrt{a_n}}{n^p}$ is also convergent for $p > 1/2$? What bugs me more is why isn't $\sum \sqrt{\frac{a_n}{n}}$ convergent?? Clearly it converges for $p > 1$, and $a_n$ somehow helps out in $p \in (1/2,1)$. Only hints are welcomed Thanks in advance..",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
6,How to properly construct an $\epsilon-N$ proof,How to properly construct an  proof,\epsilon-N,"I've asked a couple of questions now on this type of proof. My question is: could someone give me a step-by-step set of steps to follow in the general case (e.g. for proof by induction, I'd say the 3 main steps are: check the statement's true in the base cas, assume the result is true for $n$, and then finally show $P(n)\implies P(n+1)$). Is there a general recipe for $\epsilon-N$ proofs? All I know is that we need to show that $|a_n-L|<\epsilon$.","I've asked a couple of questions now on this type of proof. My question is: could someone give me a step-by-step set of steps to follow in the general case (e.g. for proof by induction, I'd say the 3 main steps are: check the statement's true in the base cas, assume the result is true for $n$, and then finally show $P(n)\implies P(n+1)$). Is there a general recipe for $\epsilon-N$ proofs? All I know is that we need to show that $|a_n-L|<\epsilon$.",,['real-analysis']
7,Does the series $\sum^\infty_{n=1}\frac{n!}{\sqrt{(2n)!}}$ converge/diverge?,Does the series  converge/diverge?,\sum^\infty_{n=1}\frac{n!}{\sqrt{(2n)!}},Does the series $\displaystyle\sum^\infty_{n=1}\frac{n!}{\sqrt{(2n)!}}$ converge/diverge? I used the ratio test but I'm not sure: $\begin{align} \frac{\frac{(n+1)!}{\sqrt{(2n+2)!}}}{\frac{n!}{\sqrt{(2n)!}}} &=\frac{n+1}{\sqrt{(2n+1)(2n+2)}}\\ &=\frac{n+1}{4(n+1)^2\sqrt{2n+1}}\\ &=\frac{1}{4\sqrt{2n+1}} \end{align}$ The limit of that is smaller than $1$ so the series does converge. Is it correct ?,Does the series $\displaystyle\sum^\infty_{n=1}\frac{n!}{\sqrt{(2n)!}}$ converge/diverge? I used the ratio test but I'm not sure: $\begin{align} \frac{\frac{(n+1)!}{\sqrt{(2n+2)!}}}{\frac{n!}{\sqrt{(2n)!}}} &=\frac{n+1}{\sqrt{(2n+1)(2n+2)}}\\ &=\frac{n+1}{4(n+1)^2\sqrt{2n+1}}\\ &=\frac{1}{4\sqrt{2n+1}} \end{align}$ The limit of that is smaller than $1$ so the series does converge. Is it correct ?,,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
8,Question About Dedekind Cuts,Question About Dedekind Cuts,,"Rudin gives the definition of a Dedekind Cut to be: A set of rational numbers is said to be a cut if (I) $\alpha$ contains at least one rational, but not every rational; (II) if $p\in\alpha$ and $q<p$ (q rational), then $q\in\alpha$; (III) $\alpha$ contains no largest rational. I'm confused as to how a set of rationals can contain no largest rational, yet not contain every rational.","Rudin gives the definition of a Dedekind Cut to be: A set of rational numbers is said to be a cut if (I) $\alpha$ contains at least one rational, but not every rational; (II) if $p\in\alpha$ and $q<p$ (q rational), then $q\in\alpha$; (III) $\alpha$ contains no largest rational. I'm confused as to how a set of rationals can contain no largest rational, yet not contain every rational.",,['real-analysis']
9,Must a Borel set B of nonzero measure contain an interval as a subset?,Must a Borel set B of nonzero measure contain an interval as a subset?,,"Assume we are working in the reals under the standard Lebesgue measure. Must a Borel set B of nonzero measure contain an interval as a subset? I conjecture yes. Is the following line of reasoning valid? Borel sets are generated by countable intersections, countable unions, and complements of open sets, but all sets of nonzero measure must be a union of uncountably many points. The union may contain an interval or be composed entirely of discrete points (each with some neighborhood containing no other point from the union). Since Borel sets only allow for the mentioned countable operations and B has nonzero measure, the nonzero measure cannot be due to an uncountable union of discrete points, so the nonzero measure must be due to some interval contained in B.","Assume we are working in the reals under the standard Lebesgue measure. Must a Borel set B of nonzero measure contain an interval as a subset? I conjecture yes. Is the following line of reasoning valid? Borel sets are generated by countable intersections, countable unions, and complements of open sets, but all sets of nonzero measure must be a union of uncountably many points. The union may contain an interval or be composed entirely of discrete points (each with some neighborhood containing no other point from the union). Since Borel sets only allow for the mentioned countable operations and B has nonzero measure, the nonzero measure cannot be due to an uncountable union of discrete points, so the nonzero measure must be due to some interval contained in B.",,"['real-analysis', 'measure-theory']"
10,Irrational Numbers Containing Other Irrational Numbers,Irrational Numbers Containing Other Irrational Numbers,,Does $ \sqrt{2} $ contain all the digits of $ \pi $ in order? Does it contain all the digits of $ \pi $ in order an infinite number of times? Does $ \pi $ contain all the digits of $ \sqrt{2} $ in order?,Does $ \sqrt{2} $ contain all the digits of $ \pi $ in order? Does it contain all the digits of $ \pi $ in order an infinite number of times? Does $ \pi $ contain all the digits of $ \sqrt{2} $ in order?,,"['real-analysis', 'sequences-and-series', 'irrational-numbers', 'pi']"
11,Why are bump functions compactly supported?,Why are bump functions compactly supported?,,"Smooth and compactly supported functions are called bump functions. They play an important role in mathematics and physics. In $\mathbb{R}^n$ and $\mathbb{C}^n$, a set is compact if and only if it is closed and bounded. It is clear why we like to work with functions that have a bounded support. But what is the advantage of working with functions that have a support that is also closed? Why do we often work with compactly supported functions, and not just functions with bounded support?","Smooth and compactly supported functions are called bump functions. They play an important role in mathematics and physics. In $\mathbb{R}^n$ and $\mathbb{C}^n$, a set is compact if and only if it is closed and bounded. It is clear why we like to work with functions that have a bounded support. But what is the advantage of working with functions that have a support that is also closed? Why do we often work with compactly supported functions, and not just functions with bounded support?",,"['real-analysis', 'compactness']"
12,Rudin Principles Theorem 2.40: Every k-cell is compact.,Rudin Principles Theorem 2.40: Every k-cell is compact.,,"In the proof $I$ is a $k$-cell whose coordinates are bounded by $a_{j}\le x_{j}\le b_{j}$ where $1\le j\le k$. From the proof: Put $c_{j}=(a_{j}+b_{j})/2$. The intervals $[a_{j},c_{j}]$ and $[c_{j},b_{j}]$ then determine $2^{k}$ $k$-cells $Q_{i}$ whose union is $I$. What does each of the $Q_{i}$ look like?","In the proof $I$ is a $k$-cell whose coordinates are bounded by $a_{j}\le x_{j}\le b_{j}$ where $1\le j\le k$. From the proof: Put $c_{j}=(a_{j}+b_{j})/2$. The intervals $[a_{j},c_{j}]$ and $[c_{j},b_{j}]$ then determine $2^{k}$ $k$-cells $Q_{i}$ whose union is $I$. What does each of the $Q_{i}$ look like?",,['real-analysis']
13,Banach theorem example,Banach theorem example,,"By Banach fixed point theorem, if a metric on a metric space $X$ is such that $d(f(x),f(y))\leq K d(x,y)$ for $K\in (0,1)$ then $f$ has one unique fixed point. Is there an example where $d(f(x),f(y))\leq K d(x,y)$ does not have a fixed point if $K=1$? What if $X$ is a compact space?","By Banach fixed point theorem, if a metric on a metric space $X$ is such that $d(f(x),f(y))\leq K d(x,y)$ for $K\in (0,1)$ then $f$ has one unique fixed point. Is there an example where $d(f(x),f(y))\leq K d(x,y)$ does not have a fixed point if $K=1$? What if $X$ is a compact space?",,"['real-analysis', 'fixed-point-theorems']"
14,what is the cardinality of set of all smooth functions in $L^1$?,what is the cardinality of set of all smooth functions in ?,L^1,What is the cardinality of set of all smooth functions belonging to $L^1$ or $L^2$ ? What is that of set of all integrable or square integrable functions ?,What is the cardinality of set of all smooth functions belonging to $L^1$ or $L^2$ ? What is that of set of all integrable or square integrable functions ?,,"['real-analysis', 'cardinals']"
15,"Find $\,\lim\limits_{t\to0^+}t\!\int_{t^2}^t\frac{\cos(x)}{x^{3/2}}\,\mathrm{d}x$",Find,"\,\lim\limits_{t\to0^+}t\!\int_{t^2}^t\frac{\cos(x)}{x^{3/2}}\,\mathrm{d}x","Question: Find $\;\displaystyle\lim_{t\to0^+}\,t\!\!\int_{t^2}^t\frac{\cos(x)}{x^{3/2}}\,\mathrm{d}x$ . Attempted solution: Let: $$f(x)=\begin{cases}\frac{\cos x-1}{x^{3/2}}&\text{if }x\in(0,\pi/2)\\0&\text{otherwise}\end{cases}$$ $\lim\limits_{x\to0^+}f(x)=0\;$ so $\,f(x)\,$ is continuous on $[0,\pi/2]$ . So, $\;\displaystyle\left|\int_{t^2}^t\frac{\cos x-1}{x^{3/2}}\,\mathrm{d}x\right|\leq\int_0^t \frac{1-\cos x}{x^{3/2}}\,\mathrm{d}x\;,$ for $t\in[0,\pi/2]$ , which tends to zero as $t\to0^+$ . $$\begin{align}t\int_{t^2}^t\frac{\cos x}{x^{3/2}}\,\mathrm{d}x&=t\left[\int_{t^2}^t\frac{\cos x-1}{x^{3/2}}\,\mathrm{d}x+\int_{t^2}^t\frac{1}{x^{3/2}}\,\mathrm{d}x\right]\\&=O(t)+2 (1-\sqrt{t})\end{align}$$ So the answer is $2$ . I just wanted to make sure is my approach correct.","Question: Find . Attempted solution: Let: so is continuous on . So, for , which tends to zero as . So the answer is . I just wanted to make sure is my approach correct.","\;\displaystyle\lim_{t\to0^+}\,t\!\!\int_{t^2}^t\frac{\cos(x)}{x^{3/2}}\,\mathrm{d}x f(x)=\begin{cases}\frac{\cos x-1}{x^{3/2}}&\text{if }x\in(0,\pi/2)\\0&\text{otherwise}\end{cases} \lim\limits_{x\to0^+}f(x)=0\; \,f(x)\, [0,\pi/2] \;\displaystyle\left|\int_{t^2}^t\frac{\cos x-1}{x^{3/2}}\,\mathrm{d}x\right|\leq\int_0^t \frac{1-\cos x}{x^{3/2}}\,\mathrm{d}x\;, t\in[0,\pi/2] t\to0^+ \begin{align}t\int_{t^2}^t\frac{\cos x}{x^{3/2}}\,\mathrm{d}x&=t\left[\int_{t^2}^t\frac{\cos x-1}{x^{3/2}}\,\mathrm{d}x+\int_{t^2}^t\frac{1}{x^{3/2}}\,\mathrm{d}x\right]\\&=O(t)+2 (1-\sqrt{t})\end{align} 2","['real-analysis', 'integration', 'analysis', 'solution-verification']"
16,Integration $\int_{0}^{\infty} \frac{2x-1}{x^{2/3}\sqrt{(x+1)((x+1)^3-x)}}\text{d}x=0.$,Integration,\int_{0}^{\infty} \frac{2x-1}{x^{2/3}\sqrt{(x+1)((x+1)^3-x)}}\text{d}x=0.,"Here is the integral: $$\int_{0}^{\infty} \frac{2x-1}{x^{2/3}\sqrt{(x+1)((x+1)^3-x)}}\text{d}x=0.$$ This is an elliptic integral, with such an easy result, maybe some clever substitutions or integrating methods can solve it, but so far I don't know exactly how to attack it. Thought 1 : To separate the integration interval into two parts: $(0,\frac12]$ and $[\frac12,+\infty)$ , for the second one, doing a substitution $x\mapsto \frac1x$ is my first thought(possibly not valid). Thought 2 : It looks 100%, even 90%, like pseudo elliptic integral (its anti-derivative is elementary). I think this one is elementary as well. THOUGHT 3 : Maybe this is a very STUPID question. Perhaps some mathematical softwares can done it(especially for Mathematica). For some good reasons, I can't use them. Waiting for your replies.","Here is the integral: This is an elliptic integral, with such an easy result, maybe some clever substitutions or integrating methods can solve it, but so far I don't know exactly how to attack it. Thought 1 : To separate the integration interval into two parts: and , for the second one, doing a substitution is my first thought(possibly not valid). Thought 2 : It looks 100%, even 90%, like pseudo elliptic integral (its anti-derivative is elementary). I think this one is elementary as well. THOUGHT 3 : Maybe this is a very STUPID question. Perhaps some mathematical softwares can done it(especially for Mathematica). For some good reasons, I can't use them. Waiting for your replies.","\int_{0}^{\infty} \frac{2x-1}{x^{2/3}\sqrt{(x+1)((x+1)^3-x)}}\text{d}x=0. (0,\frac12] [\frac12,+\infty) x\mapsto \frac1x","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'complex-integration']"
17,Folland: Why is the product measure well-defined?,Folland: Why is the product measure well-defined?,,"Consider the following fragment from Folland's ""Real analysis"" on p64: I don't understand why $\pi(E)$ is well-defined, i.e. assume that $$E = \bigcup_i A_i \times B_i= \bigcup_j C_j \times D_j$$ where the unions are disjoint. Then why is it true that $$\sum_i \mu(A_i)\nu(B_i) = \sum_j \mu(C_j)\nu(D_j)?$$ Folland talks about a common refinement, but I don't see how that works.","Consider the following fragment from Folland's ""Real analysis"" on p64: I don't understand why is well-defined, i.e. assume that where the unions are disjoint. Then why is it true that Folland talks about a common refinement, but I don't see how that works.",\pi(E) E = \bigcup_i A_i \times B_i= \bigcup_j C_j \times D_j \sum_i \mu(A_i)\nu(B_i) = \sum_j \mu(C_j)\nu(D_j)?,"['real-analysis', 'integration']"
18,Solutions to Spring 2020 UCLA Analysis Qual Problem 1? Or: an identity implies function is odd,Solutions to Spring 2020 UCLA Analysis Qual Problem 1? Or: an identity implies function is odd,,"I would like to see an elegant/simple solution  to the following problem from the Spring 2020 UCLA Analysis Qual. Suppose that $f\in C_c^{\infty}(\mathbb R)$ satisfies \begin{equation} \int_{\mathbb R}e^{-tx^2}f(x)\,dx=0\qquad\text{for any }t\geq0. \end{equation} Show that $f$ is odd; that is, show that $f(x)=-f(-x)$ for each $x\in\mathbb R$ . The proof that I have in mind is quite long. Any help is appreciated!","I would like to see an elegant/simple solution  to the following problem from the Spring 2020 UCLA Analysis Qual. Suppose that satisfies Show that is odd; that is, show that for each . The proof that I have in mind is quite long. Any help is appreciated!","f\in C_c^{\infty}(\mathbb R) \begin{equation}
\int_{\mathbb R}e^{-tx^2}f(x)\,dx=0\qquad\text{for any }t\geq0.
\end{equation} f f(x)=-f(-x) x\in\mathbb R","['real-analysis', 'even-and-odd-functions']"
19,Two equivalent series converge on different limits,Two equivalent series converge on different limits,,"Consider the series: $$ \sum_{n=0}^{\infty} \left({\dfrac{(-1)^n}{2n+1}}\right)^3  = \dfrac{1}{1^3}-\dfrac{1}{3^3}+\dfrac{1}{5^3}-\dfrac{1}{7^3} \dots = \dfrac{\pi^3}{32} $$ Now suppose I wish to represent this series with the following equivalent summation: $$ \sum_{n=1}^{\infty} \left[\left({\dfrac{1}{4n-3}}\right)^3-\left({\dfrac{1}{4n-1}}\right)^3 \right]=  \left[\dfrac{1}{1^3}-\dfrac{1}{3^3}\right]+\left[\dfrac{1}{5^3}-\dfrac{1}{7^3}\right] \dots  $$ I know that changing order of summation in infinite series might affect the limit, but in this example the order of summation is not really changed, and all of the terms are exactly the same.One can even prove by induction that both series term by term are equivalent - i.e that the second is a compressed version of the first. So can I state that the limit of the second series is equal to $\dfrac{\pi^3}{32}$ ? If not, I would be happy to find some explanation as to why we even pretend to assign values to infinite series if we can see that certain examples of infinite series break basic axioms of arithmetic.","Consider the series: Now suppose I wish to represent this series with the following equivalent summation: I know that changing order of summation in infinite series might affect the limit, but in this example the order of summation is not really changed, and all of the terms are exactly the same.One can even prove by induction that both series term by term are equivalent - i.e that the second is a compressed version of the first. So can I state that the limit of the second series is equal to ? If not, I would be happy to find some explanation as to why we even pretend to assign values to infinite series if we can see that certain examples of infinite series break basic axioms of arithmetic.","
\sum_{n=0}^{\infty} \left({\dfrac{(-1)^n}{2n+1}}\right)^3  = \dfrac{1}{1^3}-\dfrac{1}{3^3}+\dfrac{1}{5^3}-\dfrac{1}{7^3} \dots = \dfrac{\pi^3}{32}
 
\sum_{n=1}^{\infty} \left[\left({\dfrac{1}{4n-3}}\right)^3-\left({\dfrac{1}{4n-1}}\right)^3 \right]=  \left[\dfrac{1}{1^3}-\dfrac{1}{3^3}\right]+\left[\dfrac{1}{5^3}-\dfrac{1}{7^3}\right] \dots 
 \dfrac{\pi^3}{32}","['real-analysis', 'sequences-and-series']"
20,How can I show the incompleteness of the irrational numbers?,How can I show the incompleteness of the irrational numbers?,,"To show the incompleteness of the rational numbers, we just had to find a set of rational numbers, that does not have an supremum / infimum which is element of $\mathbb{Q}$ . For example, we could show that the set $\{x\in\mathbb{Q}|x²<2\}$ does not have a supremum in $\mathbb{Q}$ , because $\sqrt{2}\notin\mathbb{Q}$ . So we simply found a counter example and we were done, right? Now I'm wondering, how can we show the same thing for $\mathbb{R}$ \ $\mathbb{Q}$ (let's call it $\mathbb{I}$ for simplicity reasons). It feels like, you could just do the same thing we did for $\mathbb{Q}$ also with $\mathbb{I}$ , for example we know that $\frac{3}{2}\notin\mathbb{I}$ , but how can I actually prove that? In $\mathbb{Q}$ we could atleast construct numbers, but in $\mathbb{I}$ this gets kinda hard. Does someone have a tip for me? Thanks in advance!","To show the incompleteness of the rational numbers, we just had to find a set of rational numbers, that does not have an supremum / infimum which is element of . For example, we could show that the set does not have a supremum in , because . So we simply found a counter example and we were done, right? Now I'm wondering, how can we show the same thing for \ (let's call it for simplicity reasons). It feels like, you could just do the same thing we did for also with , for example we know that , but how can I actually prove that? In we could atleast construct numbers, but in this gets kinda hard. Does someone have a tip for me? Thanks in advance!",\mathbb{Q} \{x\in\mathbb{Q}|x²<2\} \mathbb{Q} \sqrt{2}\notin\mathbb{Q} \mathbb{R} \mathbb{Q} \mathbb{I} \mathbb{Q} \mathbb{I} \frac{3}{2}\notin\mathbb{I} \mathbb{Q} \mathbb{I},"['real-analysis', 'analysis', 'real-numbers', 'irrational-numbers', 'complete-spaces']"
21,Entropy Lower Bound in Terms of $\ell_2$ norm,Entropy Lower Bound in Terms of  norm,\ell_2,"Define $$ \begin{align} H(p_1, \dots, p_n) &= \sum_{i=1}^n p_i\log1/p_i\\ &=\log n+\sum_{i=1}^n\sum_{k=2}^\infty (-1)^{k + 1} n^{k - 1} \frac{(p_i - 1/n)^k}{k (k - 1)}, \end{align} $$ where $p_1,\dots,p_n\ge0$ sum to $1$ . Then we have the classic inequality $H(p_1,p_2)\ge(\log2)(1-2((p_1-1/2)^2+2(p_2-1/2)^2))=(\log 2)(1-2\|p-1/2\|^2)$ , and we might wonder if that could be extended for $n>2$ . In particular with something like $$\begin{align} H(p_1,\dots,p_n)&\ge(\log n)(1-c_n\|p-1/n\|^2_2). \end{align}$$ From experiments with $n=3$ , it seems like $c_n\ge\frac{2 n (\log n/2)}{(n-2) \log n}=2(1-O(1/\log n))$ suffices, but I don't have a proof of this. It is also slightly inconvenient that it can go below $0$ , something that wasn't the case with the $n=2$ case. Bounding the terms individually, we can get $H(p_1,\dots,p_n)\ge-2+4\sum_{i=1}^n\frac{p_i}{1+p_i}$ , which is non-negative, but not as relatable to the $\ell_2$ norm. We can also bound $H\ge n/4-\|p-1/2\|_2^2$ , but somehow bounding centered in $1/n$ seems more natural. Is there a well known lower bound like this, relating $H(p)$ with $\|p\|_2$ ? Ideally, one that is asymptotically tight at $p_1=\dots=p_n=1/n$ and is always positive.","Define where sum to . Then we have the classic inequality , and we might wonder if that could be extended for . In particular with something like From experiments with , it seems like suffices, but I don't have a proof of this. It is also slightly inconvenient that it can go below , something that wasn't the case with the case. Bounding the terms individually, we can get , which is non-negative, but not as relatable to the norm. We can also bound , but somehow bounding centered in seems more natural. Is there a well known lower bound like this, relating with ? Ideally, one that is asymptotically tight at and is always positive.","
\begin{align}
H(p_1, \dots, p_n) &= \sum_{i=1}^n p_i\log1/p_i\\
&=\log n+\sum_{i=1}^n\sum_{k=2}^\infty (-1)^{k + 1} n^{k - 1} \frac{(p_i - 1/n)^k}{k (k - 1)},
\end{align}
 p_1,\dots,p_n\ge0 1 H(p_1,p_2)\ge(\log2)(1-2((p_1-1/2)^2+2(p_2-1/2)^2))=(\log 2)(1-2\|p-1/2\|^2) n>2 \begin{align}
H(p_1,\dots,p_n)&\ge(\log n)(1-c_n\|p-1/n\|^2_2).
\end{align} n=3 c_n\ge\frac{2 n (\log n/2)}{(n-2) \log n}=2(1-O(1/\log n)) 0 n=2 H(p_1,\dots,p_n)\ge-2+4\sum_{i=1}^n\frac{p_i}{1+p_i} \ell_2 H\ge n/4-\|p-1/2\|_2^2 1/n H(p) \|p\|_2 p_1=\dots=p_n=1/n","['real-analysis', 'inequality', 'summation', 'information-theory', 'entropy']"
22,"How to show that the natural logarithm is Lipschitz on $[\beta, \infty)$",How to show that the natural logarithm is Lipschitz on,"[\beta, \infty)","I want to show the following result: Let $\ln(x)$ have domain $D = [\beta, \infty)$ then $|\ln(x) - \ln(y)|  \leq \dfrac{1}{\beta} |x-y|, \forall x,y \in D$ I am confused as to how to prove this seemingly simple looking claim. Can someone please help?",I want to show the following result: Let have domain then I am confused as to how to prove this seemingly simple looking claim. Can someone please help?,"\ln(x) D = [\beta, \infty) |\ln(x) - \ln(y)|
 \leq \dfrac{1}{\beta} |x-y|, \forall x,y \in D","['calculus', 'real-analysis', 'logarithms', 'lipschitz-functions']"
23,"Every open cover of $[0,1]$ has finite subcover",Every open cover of  has finite subcover,"[0,1]","I am understanding proof of theorem stated in title from Spivak's calculus. It is as below. (0) Let $\mathcal{O}$ be an open cover of $[0,1]$. (1) Let $A=\{x\in [0,1]:[0,x] \mbox{ has finite subcover from } \mathcal{O}\}$. (2) Then $A$ is non-empty, bounded above by $1$; let $\alpha$ be its supremum. (3) Since $\mathcal{O}$ is open cover of $[0,1]$,  $\alpha$ is in some $U$ from $\mathcal{O}$. (4) There is an open interval $J$, $\alpha\in J\subseteq U$ s.t.all points of $J$  to the left of $\alpha$ are also in $U$. (5) Since $\alpha$ is supremum of $A$, there is an $x\in J$ such that $x\in A$. How? (6) Then $[0,x]$ is covered by finite subcover; this together with $U$ covers $[0,\alpha]$; so $\alpha\in A$. (7) One tries to prove that $\alpha=1$, and proof will complete. Q.1 It is in step 5, which I don't understand. Q.2 Are there  different proofs of this theorem? (I don't find other  in 5-6 standard books than this).","I am understanding proof of theorem stated in title from Spivak's calculus. It is as below. (0) Let $\mathcal{O}$ be an open cover of $[0,1]$. (1) Let $A=\{x\in [0,1]:[0,x] \mbox{ has finite subcover from } \mathcal{O}\}$. (2) Then $A$ is non-empty, bounded above by $1$; let $\alpha$ be its supremum. (3) Since $\mathcal{O}$ is open cover of $[0,1]$,  $\alpha$ is in some $U$ from $\mathcal{O}$. (4) There is an open interval $J$, $\alpha\in J\subseteq U$ s.t.all points of $J$  to the left of $\alpha$ are also in $U$. (5) Since $\alpha$ is supremum of $A$, there is an $x\in J$ such that $x\in A$. How? (6) Then $[0,x]$ is covered by finite subcover; this together with $U$ covers $[0,\alpha]$; so $\alpha\in A$. (7) One tries to prove that $\alpha=1$, and proof will complete. Q.1 It is in step 5, which I don't understand. Q.2 Are there  different proofs of this theorem? (I don't find other  in 5-6 standard books than this).",,"['real-analysis', 'metric-spaces', 'compactness', 'proof-explanation']"
24,Calculate $\lim_{n\rightarrow \infty} \int_{-\infty}^{\infty}\frac{1}{1+\frac{x^{4}}{n}}dx$ if it exists.,Calculate  if it exists.,\lim_{n\rightarrow \infty} \int_{-\infty}^{\infty}\frac{1}{1+\frac{x^{4}}{n}}dx,"Calculate $$\lim_{n\rightarrow \infty} \int_{-\infty}^\infty \frac{1}{1+\frac{x^4}{n}} \, dx$$ if it exists. If this limit does not exist, show why it does not exist. My attemp: Consider $f_n(x):=\frac{1}{1+\frac{x^{4}}{n}}$,  since $f_n$ is continuous in $\mathbb{R}$, then $f_n$ is $f_n$ are Lebesgue measurable in $\mathbb{R}$, futhermore, note that $f_n\leq f_{n+1}$ for all $n\in\mathbb{N}$. Moreover, we have $$\lim_{n\rightarrow \infty }f_n(x)=1.$$ Therefore,. by Monotone convergence Theorem we have $$\lim_{n\rightarrow \infty} \int_{-\infty}^\infty\frac{1}{1+\frac{x^4}{n}} \, dx=\int_{-\infty}^\infty\lim_{n\rightarrow \infty}\frac{1}{1+\frac{x^4}{n}} \, dx= \int_{-\infty}^\infty 1\,dx=\infty.$$ Quiestion: This last conclusion brought me doubts. Does the monotone convergence theorem guarantee that the integral exists? I have read over and over the theorem and I do not find guarantee the existence, it only allows to enter the limit in the integral. I would like to know if I am correct. Additionally, know if there is any error in my attempt and know another way to do it.","Calculate $$\lim_{n\rightarrow \infty} \int_{-\infty}^\infty \frac{1}{1+\frac{x^4}{n}} \, dx$$ if it exists. If this limit does not exist, show why it does not exist. My attemp: Consider $f_n(x):=\frac{1}{1+\frac{x^{4}}{n}}$,  since $f_n$ is continuous in $\mathbb{R}$, then $f_n$ is $f_n$ are Lebesgue measurable in $\mathbb{R}$, futhermore, note that $f_n\leq f_{n+1}$ for all $n\in\mathbb{N}$. Moreover, we have $$\lim_{n\rightarrow \infty }f_n(x)=1.$$ Therefore,. by Monotone convergence Theorem we have $$\lim_{n\rightarrow \infty} \int_{-\infty}^\infty\frac{1}{1+\frac{x^4}{n}} \, dx=\int_{-\infty}^\infty\lim_{n\rightarrow \infty}\frac{1}{1+\frac{x^4}{n}} \, dx= \int_{-\infty}^\infty 1\,dx=\infty.$$ Quiestion: This last conclusion brought me doubts. Does the monotone convergence theorem guarantee that the integral exists? I have read over and over the theorem and I do not find guarantee the existence, it only allows to enter the limit in the integral. I would like to know if I am correct. Additionally, know if there is any error in my attempt and know another way to do it.",,"['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
25,find $\lim\limits_{n \rightarrow \infty }(n+1)\int\limits_{0}^{1} x^n f(x)$,find,\lim\limits_{n \rightarrow \infty }(n+1)\int\limits_{0}^{1} x^n f(x),"Let $f(x)$ , $x \in [0,1]$ ,be any positive real valued continuous function. Then find $$\lim\limits_{n \rightarrow \infty }(n+1)\int\limits_{0}^{1} x^n f(x)$$ I tried by integrating by parts to get the result $f(1)-(n+1)\int\limits_{0}^{1} x^{n+1}f(x) dx$ But i think this does not help us anyway to find the limit.","Let , ,be any positive real valued continuous function. Then find I tried by integrating by parts to get the result But i think this does not help us anyway to find the limit.","f(x) x \in [0,1] \lim\limits_{n \rightarrow \infty }(n+1)\int\limits_{0}^{1} x^n f(x) f(1)-(n+1)\int\limits_{0}^{1} x^{n+1}f(x) dx",[]
26,Is mapping from a countable set to an uncountable set never surjective?,Is mapping from a countable set to an uncountable set never surjective?,,"Suppose $f: X \rightarrow \mathbb{R}$, where $X$ is a countable set. Does it mean that it is always not surjective? (Sorry for such a basic question, but we never dealt with this question in elementary real analysis class).","Suppose $f: X \rightarrow \mathbb{R}$, where $X$ is a countable set. Does it mean that it is always not surjective? (Sorry for such a basic question, but we never dealt with this question in elementary real analysis class).",,"['real-analysis', 'elementary-set-theory']"
27,Proof of 'sandwich theorem' for sequences,Proof of 'sandwich theorem' for sequences,,"Please bear with me here and please try to read it all and spot any mistakes or errors as I'm trying to prove this result but I'm unsure of whether I have done it or not. THANK YOU. Suppose we have the following statement $(a_n)\rightarrow \ell , (b_n)\rightarrow \ell $ and we have $$a_n\leq c_n\leq b_n$$ then $(c_n)\rightarrow \ell $. I think I have a proof which goes as follow ; $$a_n\leq c_n\leq b_n\leq  \ \Rightarrow 0\leq c_n-a_n\leq b_n-a_n $$, as the terms are all larger than 0, taking the absolute value will not change any of the signs of the inequalities. So we have $$0\leq |c_n-a_n|\leq|b_n-a_n|. $$ Now consider $$|b_n-a_n|=|(b_n-\ell )+(\ell - a_n)|\leq |b_n-\ell |+|a_n - \ell | \text{ (by triangle inequality)} .$$ Using the definition of a sequence tending to a value, if $(a_n)\rightarrow \ell $ then $\exists N_1\in \mathbb{N} \text{ s.t} \ \forall n>N, |a_n-\ell |<\epsilon \ ,\forall \epsilon >0 .$ We do the same for $(b_n) $ but replacing $N_1 $ with $N_2$ and using the same $\epsilon $ without loss of generality.  So we can now say that $$|b_n-a_n|\leq |b_n-\ell |+|a_n - \ell |<2\epsilon . $$ So we have $$0\leq |c_n-a_n |\leq|b_n-a_n|<\epsilon _0, \text{ where } \epsilon _0=2\epsilon .$$ So we can conclude (using sandwich theorem for null sequences) that $(c_n-a_n)\rightarrow 0 \Rightarrow (c_n)\rightarrow (a_n)\Rightarrow (c_n)\rightarrow \ell $ since $(a_n)\rightarrow \ell . \ \ \ \square $","Please bear with me here and please try to read it all and spot any mistakes or errors as I'm trying to prove this result but I'm unsure of whether I have done it or not. THANK YOU. Suppose we have the following statement $(a_n)\rightarrow \ell , (b_n)\rightarrow \ell $ and we have $$a_n\leq c_n\leq b_n$$ then $(c_n)\rightarrow \ell $. I think I have a proof which goes as follow ; $$a_n\leq c_n\leq b_n\leq  \ \Rightarrow 0\leq c_n-a_n\leq b_n-a_n $$, as the terms are all larger than 0, taking the absolute value will not change any of the signs of the inequalities. So we have $$0\leq |c_n-a_n|\leq|b_n-a_n|. $$ Now consider $$|b_n-a_n|=|(b_n-\ell )+(\ell - a_n)|\leq |b_n-\ell |+|a_n - \ell | \text{ (by triangle inequality)} .$$ Using the definition of a sequence tending to a value, if $(a_n)\rightarrow \ell $ then $\exists N_1\in \mathbb{N} \text{ s.t} \ \forall n>N, |a_n-\ell |<\epsilon \ ,\forall \epsilon >0 .$ We do the same for $(b_n) $ but replacing $N_1 $ with $N_2$ and using the same $\epsilon $ without loss of generality.  So we can now say that $$|b_n-a_n|\leq |b_n-\ell |+|a_n - \ell |<2\epsilon . $$ So we have $$0\leq |c_n-a_n |\leq|b_n-a_n|<\epsilon _0, \text{ where } \epsilon _0=2\epsilon .$$ So we can conclude (using sandwich theorem for null sequences) that $(c_n-a_n)\rightarrow 0 \Rightarrow (c_n)\rightarrow (a_n)\Rightarrow (c_n)\rightarrow \ell $ since $(a_n)\rightarrow \ell . \ \ \ \square $",,"['real-analysis', 'sequences-and-series', 'analysis']"
28,Discontinuous at infinitely many points,Discontinuous at infinitely many points,,"While doing a worksheet on real analysis I came across the following problem. $Q$. Let $f$ be a function defined on $[0,1]$ with the following property. For every $y \in R$, either there is no $x$ in $[0,1]$ for which $f(x)=y$ or there are exactly two values of $x$ in $[0,1]$ for which $f(x)=y$. (a) Prove that $f$ cannot be continuous on $[0,1]$. (b) Construct a function $f$ which has the above property. (c) Prove that any such function with this property has infinitely many discontinuous on $[0,1]$. I really have absolutely no idea how to solve the problem. Even constructing the function is proving pretty difficult. Any help would be appreciated asap.","While doing a worksheet on real analysis I came across the following problem. $Q$. Let $f$ be a function defined on $[0,1]$ with the following property. For every $y \in R$, either there is no $x$ in $[0,1]$ for which $f(x)=y$ or there are exactly two values of $x$ in $[0,1]$ for which $f(x)=y$. (a) Prove that $f$ cannot be continuous on $[0,1]$. (b) Construct a function $f$ which has the above property. (c) Prove that any such function with this property has infinitely many discontinuous on $[0,1]$. I really have absolutely no idea how to solve the problem. Even constructing the function is proving pretty difficult. Any help would be appreciated asap.",,"['real-analysis', 'continuity']"
29,"Is there a continuous function such that $\int_0^{\infty} f(x)dx$ converges, yet $\lim_{x\rightarrow \infty}f(x) \ne 0$? [duplicate]","Is there a continuous function such that  converges, yet ? [duplicate]",\int_0^{\infty} f(x)dx \lim_{x\rightarrow \infty}f(x) \ne 0,"This question already has answers here : Improper integral from 1 to infinity $\Rightarrow$ integrated function converges towards zero? (3 answers) Closed 5 years ago . Is there a continuous function such that $\int_0^{\infty} f(x)dx$ converges, yet $\lim_{x\rightarrow \infty}f(x) \ne 0$? I know there are such functions, but I just can't think of any example.","This question already has answers here : Improper integral from 1 to infinity $\Rightarrow$ integrated function converges towards zero? (3 answers) Closed 5 years ago . Is there a continuous function such that $\int_0^{\infty} f(x)dx$ converges, yet $\lim_{x\rightarrow \infty}f(x) \ne 0$? I know there are such functions, but I just can't think of any example.",,"['calculus', 'real-analysis', 'examples-counterexamples']"
30,all possible inner products in $\mathbb R^2$,all possible inner products in,\mathbb R^2,"Suppose $\langle., .\rangle: \mathbb R^2\times \mathbb R^2\to \mathbb R$ is an inner product. What would be  all possible function forms of the inner products, i.e. would all of them have the forms either $\langle x, y\rangle=ax_1y_1+bx_2y_2$ or   $\langle x, y\rangle=ax_1 y_2+b x_2y_1,   a,b\in \mathbb R$ or other forms are also possible? How about $\mathbb R^n$$?$","Suppose $\langle., .\rangle: \mathbb R^2\times \mathbb R^2\to \mathbb R$ is an inner product. What would be  all possible function forms of the inner products, i.e. would all of them have the forms either $\langle x, y\rangle=ax_1y_1+bx_2y_2$ or   $\langle x, y\rangle=ax_1 y_2+b x_2y_1,   a,b\in \mathbb R$ or other forms are also possible? How about $\mathbb R^n$$?$",,"['real-analysis', 'linear-algebra', 'hilbert-spaces', 'inner-products']"
31,How do I convince students in high school for which this equation: $2^x=4x$ have only one solution in integers that is $x=4$?,How do I convince students in high school for which this equation:  have only one solution in integers that is ?,2^x=4x x=4,I would like to convince my student in high school level using a simple mathematical way  to solve this equation: $$2^x=4x$$ in $\mathbb{Z}$ which have only one integer solution  that is $x=4$ . My question here :How do I convince students in high school for which this equation: $$2^x=4x$$ have only one solution that is $x=4$ ? Note : I do not want to use substitution to convince them and by numerical methods can't give us exactly $x=4$ EDIT : I edited the question as it is  very related to the precedent Thank you for any help.,I would like to convince my student in high school level using a simple mathematical way  to solve this equation: in which have only one integer solution  that is . My question here :How do I convince students in high school for which this equation: have only one solution that is ? Note : I do not want to use substitution to convince them and by numerical methods can't give us exactly EDIT : I edited the question as it is  very related to the precedent Thank you for any help.,2^x=4x \mathbb{Z} x=4 2^x=4x x=4 x=4,"['real-analysis', 'logarithms', 'education', 'exponentiation']"
32,"Condition to guarantee $f=0$ on $[a,b]$",Condition to guarantee  on,"f=0 [a,b]","I have been stuck for several days on this old Analysis problem (I am doing some study on my own).  I have tried several things (which I'll indicate below), but I cannot seem to figure it out.  Here is how the problem is presented: Problem: ""Let $f$ be a continuous real-valued function on $[a,b]$ .  Suppose there exists a constant $M \geq 0 $ such that $$|f(x)| \leq M \int_a^x |f(t)| dt$$ for all $x \in [a,b]$ .  Show that $f(x)=0$ for all $x \in [a,b]$ ."" My Thoughts: I have tried using the mean value theorem iteratively, but that seems to always lead me down a dead end road.  I deduced that $f(a)=0$ .  If only $f$ were assumed to be differentiable, then maybe I could play with trying to get the derivative to be $0$ , but unfortunately it's only continuous.  My other thought was to (somehow) use the condition to show that $\int_a^b |f(x)| dx = 0$ .  I also played around a bit with contradiction, but to no avail.  Even if one of these hair-brained thoughts is correct, I am not really sure what to do next. If you have any ideas, suggestions, or solutions, I would really appreciate it if you are willing to share them.  Thank you for your time.","I have been stuck for several days on this old Analysis problem (I am doing some study on my own).  I have tried several things (which I'll indicate below), but I cannot seem to figure it out.  Here is how the problem is presented: Problem: ""Let be a continuous real-valued function on .  Suppose there exists a constant such that for all .  Show that for all ."" My Thoughts: I have tried using the mean value theorem iteratively, but that seems to always lead me down a dead end road.  I deduced that .  If only were assumed to be differentiable, then maybe I could play with trying to get the derivative to be , but unfortunately it's only continuous.  My other thought was to (somehow) use the condition to show that .  I also played around a bit with contradiction, but to no avail.  Even if one of these hair-brained thoughts is correct, I am not really sure what to do next. If you have any ideas, suggestions, or solutions, I would really appreciate it if you are willing to share them.  Thank you for your time.","f [a,b] M \geq 0  |f(x)| \leq M \int_a^x |f(t)| dt x \in [a,b] f(x)=0 x \in [a,b] f(a)=0 f 0 \int_a^b |f(x)| dx = 0",[]
33,"Not the toughest integral, not the easiest one","Not the toughest integral, not the easiest one",,"Perhaps it's not amongst the toughest integrals, but it's interesting  to try to find an elegant approach for the integral  $$I_1=\int_0^1 \frac{\log (x)}{\sqrt{x (x+1)}} \, dx$$ $$=4 \text{Li}_2\left(-\sqrt{2}\right)-4 \text{Li}_2\left(-1-\sqrt{2}\right)+2 \log ^2\left(1+\sqrt{2}\right)-4 \log \left(2+\sqrt{2}\right) \log \left(1+\sqrt{2}\right)-\frac{\pi ^2}{3}$$ Do you see any such a way? Then I wonder if we can think of some elegant ways for the evaluation of the quadratic and cubic versions, that is $$I_2=\int_0^1 \frac{\log^2 (x)}{\sqrt{x (x+1)}} \, dx$$ $$I_3=\int_0^1 \frac{\log^3 (x)}{\sqrt{x (x+1)}} \, dx.$$ How far can we possibly go with the generalization such that we can get integrals in closed form?","Perhaps it's not amongst the toughest integrals, but it's interesting  to try to find an elegant approach for the integral  $$I_1=\int_0^1 \frac{\log (x)}{\sqrt{x (x+1)}} \, dx$$ $$=4 \text{Li}_2\left(-\sqrt{2}\right)-4 \text{Li}_2\left(-1-\sqrt{2}\right)+2 \log ^2\left(1+\sqrt{2}\right)-4 \log \left(2+\sqrt{2}\right) \log \left(1+\sqrt{2}\right)-\frac{\pi ^2}{3}$$ Do you see any such a way? Then I wonder if we can think of some elegant ways for the evaluation of the quadratic and cubic versions, that is $$I_2=\int_0^1 \frac{\log^2 (x)}{\sqrt{x (x+1)}} \, dx$$ $$I_3=\int_0^1 \frac{\log^3 (x)}{\sqrt{x (x+1)}} \, dx.$$ How far can we possibly go with the generalization such that we can get integrals in closed form?",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'special-functions']"
34,A recursive formula to approximate $e$. Prove or disprove.,A recursive formula to approximate . Prove or disprove.,e,"Let the sequence $\{x_n, n=1,2,...\}$ be defined as follows: Let $x_2=x_1=1$ and for $n>2$ let $$x_{n+1}=x_n-\frac{1}{n}x_{n-1}.$$ This sequence, generated by the recursion above, tends to zero extremely fast. My guess is that $$\sum_{i=1}^{\infty}x_i=e.$$ I came across this problem here on MSE while working on an infinite Markov chain related problem . The state transition probability matrix was given and the task was to find the stationary probabilities. The stationary probabilities could be generated by the recursion above but with an unknown $c=x_1=x_2$. I did some experiments with $c$ and the results lead me believe that $$c=e.$$ I would be surprised if this recursion was not known to somebody around here.","Let the sequence $\{x_n, n=1,2,...\}$ be defined as follows: Let $x_2=x_1=1$ and for $n>2$ let $$x_{n+1}=x_n-\frac{1}{n}x_{n-1}.$$ This sequence, generated by the recursion above, tends to zero extremely fast. My guess is that $$\sum_{i=1}^{\infty}x_i=e.$$ I came across this problem here on MSE while working on an infinite Markov chain related problem . The state transition probability matrix was given and the task was to find the stationary probabilities. The stationary probabilities could be generated by the recursion above but with an unknown $c=x_1=x_2$. I did some experiments with $c$ and the results lead me believe that $$c=e.$$ I would be surprised if this recursion was not known to somebody around here.",,"['real-analysis', 'sequences-and-series', 'probability-theory', 'markov-chains']"
35,Algebraic proof of $\tan x>x$,Algebraic proof of,\tan x>x,"I'm looking for a non-calculus proof of the statement that $\tan x>x$ on $(0,\pi/2)$, meaning ""not using derivatives or integrals."" (The calculus proof: if $f(x)=\tan x-x$ then $f'(x)=\sec^2 x-1>0$ so $f$ is increasing, and $f(0)=0$.) $\tan x$ is defined to be $\frac{\sin x}{\cos x}$ where these are defined by their infinite series. What I have so far: $$|z|\le1\implies\left|\sum_{n=4}^\infty\frac{z^n}{n!}\right|<\sum_{n=0}^\infty\frac{|z|^4}{4!\,5^n}=\frac{5|z|^4}{4\cdot 4!}$$ $$\left|\sin x-\Big(x-\frac{x^3}6\Big)\right|=\Im\left[\sum_{n=4}^\infty\frac{(ix)^n}{n!}\right]<\frac{5x^4}{4\cdot 4!}<\frac{x^3}6$$ $$\left|\cos x-\Big(1-\frac{x^2}2\Big)\right|=\Re\left[\sum_{n=4}^\infty\frac{(ix)^n}{n!}\right]<\frac{5x^4}{4\cdot 4!}<\frac{x^2}6$$ Thus $\sin x>x-\frac{x^3}3$ and $\cos x<1-\frac{x^2}3$, so $\tan x>x$. However, this only covers the region $x\le1$, and I still need to bound $\tan x$ on $(1,\pi/2)$. My best approximation to $\pi$ is the very crude $2<\pi<4$, derived by combining the above bounds with the double angle formulas (note that $\pi$ is defined as the smallest positive root of $\sin x$), so I can't quite finish the proof with a bound like $\sin x>1/\sqrt 2$, $\cos x\le\pi/2-x$ (assuming now $x\ge1\ge\pi/4$) because the bound is too tight. Any ideas?","I'm looking for a non-calculus proof of the statement that $\tan x>x$ on $(0,\pi/2)$, meaning ""not using derivatives or integrals."" (The calculus proof: if $f(x)=\tan x-x$ then $f'(x)=\sec^2 x-1>0$ so $f$ is increasing, and $f(0)=0$.) $\tan x$ is defined to be $\frac{\sin x}{\cos x}$ where these are defined by their infinite series. What I have so far: $$|z|\le1\implies\left|\sum_{n=4}^\infty\frac{z^n}{n!}\right|<\sum_{n=0}^\infty\frac{|z|^4}{4!\,5^n}=\frac{5|z|^4}{4\cdot 4!}$$ $$\left|\sin x-\Big(x-\frac{x^3}6\Big)\right|=\Im\left[\sum_{n=4}^\infty\frac{(ix)^n}{n!}\right]<\frac{5x^4}{4\cdot 4!}<\frac{x^3}6$$ $$\left|\cos x-\Big(1-\frac{x^2}2\Big)\right|=\Re\left[\sum_{n=4}^\infty\frac{(ix)^n}{n!}\right]<\frac{5x^4}{4\cdot 4!}<\frac{x^2}6$$ Thus $\sin x>x-\frac{x^3}3$ and $\cos x<1-\frac{x^2}3$, so $\tan x>x$. However, this only covers the region $x\le1$, and I still need to bound $\tan x$ on $(1,\pi/2)$. My best approximation to $\pi$ is the very crude $2<\pi<4$, derived by combining the above bounds with the double angle formulas (note that $\pi$ is defined as the smallest positive root of $\sin x$), so I can't quite finish the proof with a bound like $\sin x>1/\sqrt 2$, $\cos x\le\pi/2-x$ (assuming now $x\ge1\ge\pi/4$) because the bound is too tight. Any ideas?",,"['real-analysis', 'sequences-and-series', 'trigonometry', 'inequality']"
36,Integral $\int_0^{\pi/4}\frac{x^2\tan x}{\cos^2 x}dx=\frac{\log 2}{2}-\frac{\pi}{4}+\frac{\pi^2}{16}$,Integral,\int_0^{\pi/4}\frac{x^2\tan x}{\cos^2 x}dx=\frac{\log 2}{2}-\frac{\pi}{4}+\frac{\pi^2}{16},"Hi I am trying to evaluate the definite integral which has a closed form given by:  $$ \mathcal{I}=\int_0^{\pi/4}\frac{x^2\tan x}{\cos^2 x}dx=\frac{\log 2}{2}-\frac{\pi}{4}+\frac{\pi^2}{16}. $$ We can possibly write $y=\cos x$ $$ \mathcal{I}=\int_0^{\pi/4}\frac{x^2\sin x}{\cos^3x}dx=\int_{\frac{1}{\sqrt 2}}^1\frac{(\cos^{-1}y)^2}{y^3}dy $$ however I am now sure how that will help us. We can also try $u=\tan x, du=\frac{dx}{\cos^2x}$ to obtain $$ \mathcal{I}=\int_0^1 (\tan^{-1}u)^2 u\, du $$ however I am unsure where to go from here.  How can we solve $\mathcal{I}$?  Thank you","Hi I am trying to evaluate the definite integral which has a closed form given by:  $$ \mathcal{I}=\int_0^{\pi/4}\frac{x^2\tan x}{\cos^2 x}dx=\frac{\log 2}{2}-\frac{\pi}{4}+\frac{\pi^2}{16}. $$ We can possibly write $y=\cos x$ $$ \mathcal{I}=\int_0^{\pi/4}\frac{x^2\sin x}{\cos^3x}dx=\int_{\frac{1}{\sqrt 2}}^1\frac{(\cos^{-1}y)^2}{y^3}dy $$ however I am now sure how that will help us. We can also try $u=\tan x, du=\frac{dx}{\cos^2x}$ to obtain $$ \mathcal{I}=\int_0^1 (\tan^{-1}u)^2 u\, du $$ however I am unsure where to go from here.  How can we solve $\mathcal{I}$?  Thank you",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
37,How to integrate $\int_0^\pi \frac{1}{\sqrt{1+k^2\sin^2\phi}} d \phi$?,How to integrate ?,\int_0^\pi \frac{1}{\sqrt{1+k^2\sin^2\phi}} d \phi,"I am currently dealing with the integral $$\int_{0}^{\large\pi}\frac{{\rm d}\phi} {\,\sqrt{\vphantom{\Large A}\,1 + k^{2}\sin^{2} \phi \,}\,} $$ I know that if I had a minus sign in the denominator, then this would be similar to an elliptic integral, but in this case, I don't really know what this is.","I am currently dealing with the integral $$\int_{0}^{\large\pi}\frac{{\rm d}\phi} {\,\sqrt{\vphantom{\Large A}\,1 + k^{2}\sin^{2} \phi \,}\,} $$ I know that if I had a minus sign in the denominator, then this would be similar to an elliptic integral, but in this case, I don't really know what this is.",,"['calculus', 'real-analysis']"
38,"If $f$ is differentiable and $\lim_{x→0} f'(x) = L$, then $f'(0) = L$.","If  is differentiable and , then .",f \lim_{x→0} f'(x) = L f'(0) = L,"True/False. (c) If $f$ is differentiable on an interval containing zero and if $\lim_{x→0} f'(x) = L$, then $f'(0) = L$. 1. How to presage proof by contradiction? Proof by contradiction. True. Assume that $L  \neq f'(0)$ and choose e > 0 so that $|f'(0) − L| > e$. From the hypothesis that $\lim_{x→0} f'(x) = L$, we know there exists a $δ > 0$ such that $0 < |x| < δ$ implies that $|f'(x) − L| < e$. Now our choice of $e$ guarantees that there exists a point $f'(0) < a < \color{red}L$ but $|a - L| > e$. 2. How? We chose $e$ guarantee this? However, by Darboux’s Theorem , there exists a point x such that $|x| < d$  such that $f(x) = a$. 3. How? $\color{red}L$ not a derivative of $f$? Ergo what sanctions use of Darboux's Theorem here? This suggests that $a - L< e$, a contradiction. 4. What suggests this?","True/False. (c) If $f$ is differentiable on an interval containing zero and if $\lim_{x→0} f'(x) = L$, then $f'(0) = L$. 1. How to presage proof by contradiction? Proof by contradiction. True. Assume that $L  \neq f'(0)$ and choose e > 0 so that $|f'(0) − L| > e$. From the hypothesis that $\lim_{x→0} f'(x) = L$, we know there exists a $δ > 0$ such that $0 < |x| < δ$ implies that $|f'(x) − L| < e$. Now our choice of $e$ guarantees that there exists a point $f'(0) < a < \color{red}L$ but $|a - L| > e$. 2. How? We chose $e$ guarantee this? However, by Darboux’s Theorem , there exists a point x such that $|x| < d$  such that $f(x) = a$. 3. How? $\color{red}L$ not a derivative of $f$? Ergo what sanctions use of Darboux's Theorem here? This suggests that $a - L< e$, a contradiction. 4. What suggests this?",,"['calculus', 'real-analysis']"
39,Continuous bounded function $f:\mathbb{R}\rightarrow \mathbb{R}$,Continuous bounded function,f:\mathbb{R}\rightarrow \mathbb{R},"Question is to check which of the following holds (only one option is correct) for a continuous bounded function  $f:\mathbb{R}\rightarrow \mathbb{R}$. $f$ has to be uniformly continuous. there exists a $x\in \mathbb{R}$ such that $f(x)=x$. $f$ can not be increasing. $\lim_{x\rightarrow \infty}f(x)$ exists. What all i have done is : $f(x)=\sin(x^3)$ is a continuous function which is bounded by $1$ which is not uniformly continuous. suppose $f$ is bounded by $M>0$ then restrict $f: [-M,M]\rightarrow [-M,M]$ this function is bounded ad continuous so has fixed point. I could not say much about the third option ""$f$ can not be increasing"". I think this is also true as for an increasing function $f$ can not be bounded but i am not sure. I also believe that $\lim_{x\rightarrow \infty}f(x)$ exists as $f$ is bounded it should have limit at infinity.But then I feel the function can be so fluctuating so limit need not exists. I am not so sure. So, I am sure second option is correct and fourth option may probably wrong but i am not so sure about third option. Please help me to clear this. Thank You. :)","Question is to check which of the following holds (only one option is correct) for a continuous bounded function  $f:\mathbb{R}\rightarrow \mathbb{R}$. $f$ has to be uniformly continuous. there exists a $x\in \mathbb{R}$ such that $f(x)=x$. $f$ can not be increasing. $\lim_{x\rightarrow \infty}f(x)$ exists. What all i have done is : $f(x)=\sin(x^3)$ is a continuous function which is bounded by $1$ which is not uniformly continuous. suppose $f$ is bounded by $M>0$ then restrict $f: [-M,M]\rightarrow [-M,M]$ this function is bounded ad continuous so has fixed point. I could not say much about the third option ""$f$ can not be increasing"". I think this is also true as for an increasing function $f$ can not be bounded but i am not sure. I also believe that $\lim_{x\rightarrow \infty}f(x)$ exists as $f$ is bounded it should have limit at infinity.But then I feel the function can be so fluctuating so limit need not exists. I am not so sure. So, I am sure second option is correct and fourth option may probably wrong but i am not so sure about third option. Please help me to clear this. Thank You. :)",,[]
40,"Cardinality of $\{ f\in C'[0,1] : f(0)=0, f(1)=1, |f'(t)|\leq 1 \forall t\in[0,1]\}$... NBHM $2007$",Cardinality of ... NBHM,"\{ f\in C'[0,1] : f(0)=0, f(1)=1, |f'(t)|\leq 1 \forall t\in[0,1]\} 2007","Question is to find : What is the cardinality of the following set : $$A=\{ f\in C'[0,1] : f(0)=0, f(1)=1, |f'(t)|\leq 1 \forall t\in[0,1]\}$$ I would like to see for some time that all $f\in C'[0,1]$ which i am looking for are just polynomials as any continuous function can be approximated uniformly by polynomials. with $f(0)=0$ condition I would like to say $f$ do not have constant term. with the condition that $f'(t)$ being bounded by $1$ I would like to say that $f$ do not have higher powers ie., suppose $f=x^3+x^2+1$ then, $f'=3x^2+2$ whihch is not bounded by $1$ I do not really mean this should how the polynomials of higher degrees look like but if $f$ has degree $n>1$ then, $f(x)=nx^{n-1}+g(x)$ and this would be not bounded by $1$. I have not yet used $|f'(t)|\leq 1 \forall t\in[0,1]$ but i belive this would also give same result. Just for simplicity Suppose $f(x)=x^3$ then, $|f'(t)|=\frac{|f(x)-f(y)|}{|x-y|}=\frac{|x^3-y^3|}{|x-y|}=|x^2+xy+y^2|$ But i can choose $x,y$ accordingly suhc that this is not bounded by $1$ So, $f(x)$ do not have higher powers and because $f(1)=1$, coefficient of $x$ has to be $1$ i.e., $$A=\{ f\in C'[0,1] : f(0)=0, f(1)=1, |f'(t)|\leq 1 \forall t\in[0,1]\}=\{x\}$$ I would like some one to verify if this is sufficient or do i have to rewrite the arguments (I am sure i have to, but i am not sure how to do so). Please help me to clear this ambiguity.. Thank you","Question is to find : What is the cardinality of the following set : $$A=\{ f\in C'[0,1] : f(0)=0, f(1)=1, |f'(t)|\leq 1 \forall t\in[0,1]\}$$ I would like to see for some time that all $f\in C'[0,1]$ which i am looking for are just polynomials as any continuous function can be approximated uniformly by polynomials. with $f(0)=0$ condition I would like to say $f$ do not have constant term. with the condition that $f'(t)$ being bounded by $1$ I would like to say that $f$ do not have higher powers ie., suppose $f=x^3+x^2+1$ then, $f'=3x^2+2$ whihch is not bounded by $1$ I do not really mean this should how the polynomials of higher degrees look like but if $f$ has degree $n>1$ then, $f(x)=nx^{n-1}+g(x)$ and this would be not bounded by $1$. I have not yet used $|f'(t)|\leq 1 \forall t\in[0,1]$ but i belive this would also give same result. Just for simplicity Suppose $f(x)=x^3$ then, $|f'(t)|=\frac{|f(x)-f(y)|}{|x-y|}=\frac{|x^3-y^3|}{|x-y|}=|x^2+xy+y^2|$ But i can choose $x,y$ accordingly suhc that this is not bounded by $1$ So, $f(x)$ do not have higher powers and because $f(1)=1$, coefficient of $x$ has to be $1$ i.e., $$A=\{ f\in C'[0,1] : f(0)=0, f(1)=1, |f'(t)|\leq 1 \forall t\in[0,1]\}=\{x\}$$ I would like some one to verify if this is sufficient or do i have to rewrite the arguments (I am sure i have to, but i am not sure how to do so). Please help me to clear this ambiguity.. Thank you",,['real-analysis']
41,"Proof that for any interval (a,b) with a<b in the real numbers contains both rational and irrational numbers?","Proof that for any interval (a,b) with a<b in the real numbers contains both rational and irrational numbers?",,"Background: We are assuming that the elements of $\mathbb{R}\setminus\mathbb{Q}$ are irrational number . If $x$ is irrational and $r$ is rational then $y=x+r$ is irrational. Also, if $r\neq 0$ then $rx$ is irrational as well. Likewise, if a number is irrational then its reciprocal is irrational as well. Theorem: Every interval $(a,b)$, no matter how small, contains both rational and irrational numbers. Proof . First we can see that the interval $(0,1)$   contains both rational and irrational, just select the numbers $1/2$   and $1/\sqrt{2}$. For the general interval $(a,b)$, think of $a$   and $b$   as cuts, that is $a=A\mid A^{'}$   and $b=B\mid B^{'}$  , such that $a<b$  . The fact that $a<b$   implies that $B\setminus A\neq\emptyset$  , moreover since $B$   has no greatest value if we select an a rational element $r\in B\setminus A$   then we can find another rational element $s\in B\setminus A$   such that $r<s$  . Thus $a\leqslant r<s\leqslant b$  . The transformation $$T:t\mapsto r+(s-r)t$$ sends the interval $(0,1)$   to the interval $(r,s)$  . Since $r,s,$   and $s-r$   are all rational, the transformation $T$   sends rationals to rationals and irrationals to irrationals. That is, $(r,s)$   contains and irrationals, and so does the larger interval $(a,b)$.$\space\space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \square$ My Confusion: My first confusion is what does the transformation  $T$ map from and to? If $t\in \mathbb R$ then for any value of  $t$  we should have $r\leqslant r+(s-r)t \leqslant s$ right? However, if  $t<0$, $r<0$, and $s<0$ then the preceding inequality shouldn't hold (unless I am missing something); and if the preceding inequality doesn't hold then what good is the transformation $T$ for  showing that there is an irrational in the interval $[r,s]$?","Background: We are assuming that the elements of $\mathbb{R}\setminus\mathbb{Q}$ are irrational number . If $x$ is irrational and $r$ is rational then $y=x+r$ is irrational. Also, if $r\neq 0$ then $rx$ is irrational as well. Likewise, if a number is irrational then its reciprocal is irrational as well. Theorem: Every interval $(a,b)$, no matter how small, contains both rational and irrational numbers. Proof . First we can see that the interval $(0,1)$   contains both rational and irrational, just select the numbers $1/2$   and $1/\sqrt{2}$. For the general interval $(a,b)$, think of $a$   and $b$   as cuts, that is $a=A\mid A^{'}$   and $b=B\mid B^{'}$  , such that $a<b$  . The fact that $a<b$   implies that $B\setminus A\neq\emptyset$  , moreover since $B$   has no greatest value if we select an a rational element $r\in B\setminus A$   then we can find another rational element $s\in B\setminus A$   such that $r<s$  . Thus $a\leqslant r<s\leqslant b$  . The transformation $$T:t\mapsto r+(s-r)t$$ sends the interval $(0,1)$   to the interval $(r,s)$  . Since $r,s,$   and $s-r$   are all rational, the transformation $T$   sends rationals to rationals and irrationals to irrationals. That is, $(r,s)$   contains and irrationals, and so does the larger interval $(a,b)$.$\space\space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \space \square$ My Confusion: My first confusion is what does the transformation  $T$ map from and to? If $t\in \mathbb R$ then for any value of  $t$  we should have $r\leqslant r+(s-r)t \leqslant s$ right? However, if  $t<0$, $r<0$, and $s<0$ then the preceding inequality shouldn't hold (unless I am missing something); and if the preceding inequality doesn't hold then what good is the transformation $T$ for  showing that there is an irrational in the interval $[r,s]$?",,['real-analysis']
42,The series expansion of $\frac{1}{\sqrt{e^{x}-1}}$ at $x=0$,The series expansion of  at,\frac{1}{\sqrt{e^{x}-1}} x=0,"The function $ \displaystyle\frac{1}{\sqrt{e^{x}-1}}$ doesn't have a Laurent series expansion at $x=0$ . But according to Wolfram Alpha, it does have a series expansion with terms raised to non-integer powers: $$ \frac{1}{\sqrt{e^{x}-1}} = \frac{1}{\sqrt{x}}- \frac{\sqrt{x}}{4} + \cdots$$ How is that series derived? My initial thought was to use the general binomial theorem, but I'm not sure how.","The function doesn't have a Laurent series expansion at . But according to Wolfram Alpha, it does have a series expansion with terms raised to non-integer powers: How is that series derived? My initial thought was to use the general binomial theorem, but I'm not sure how.", \displaystyle\frac{1}{\sqrt{e^{x}-1}} x=0  \frac{1}{\sqrt{e^{x}-1}} = \frac{1}{\sqrt{x}}- \frac{\sqrt{x}}{4} + \cdots,"['real-analysis', 'sequences-and-series']"
43,Every uniformly continuous real function has at most linear growth at infinity,Every uniformly continuous real function has at most linear growth at infinity,,"Assuming $f:\mathbb R\to\mathbb R $ be an uniform continuous function, how to prove $$\exists a,b\in \mathbb R^+ \quad \text{such that}\quad |f(x)|\le a|x|+b.$$","Assuming $f:\mathbb R\to\mathbb R $ be an uniform continuous function, how to prove $$\exists a,b\in \mathbb R^+ \quad \text{such that}\quad |f(x)|\le a|x|+b.$$",,"['real-analysis', 'continuity', 'asymptotics', 'uniform-continuity']"
44,Prove that $\int_0^1 \psi{(x) \sin(2 n \pi x)} \space\mathrm{dx}=-\frac{\pi}{2}$,Prove that,\int_0^1 \psi{(x) \sin(2 n \pi x)} \space\mathrm{dx}=-\frac{\pi}{2},"Prove that $$\int_0^1 \psi{(x) \sin(2 n \pi x)} \space\mathrm{dx}=-\frac{\pi}{2}, \space n\ge1$$ where $\psi(x)$ - digamma function","Prove that $$\int_0^1 \psi{(x) \sin(2 n \pi x)} \space\mathrm{dx}=-\frac{\pi}{2}, \space n\ge1$$ where $\psi(x)$ - digamma function",,"['calculus', 'real-analysis', 'integration', 'special-functions', 'definite-integrals']"
45,"Does one of $L^\infty$ and $L^p, p \in (0, \infty)$ contain the other?",Does one of  and  contain the other?,"L^\infty L^p, p \in (0, \infty)","I think $L^\infty(\Omega, \mathcal{F}, \mu)\supseteq L^p(\Omega, \mathcal{F}, \mu), \forall p \in (0, \infty)$? My reason is $L^\infty$ is defined as the set of measurable functions that are bounded up to a set of measure zero, and if $f \notin L^\infty$, then there exists a subset of measure nonzero on which $|f|$ is $\infty$, so $f \notin L^p, \forall p \in (0, \infty)$. So I wonder why ""If $\mu$ is finite, then $L^\infty(\Omega, \mathcal{F}, \mu)\subseteq L^p(\Omega, \mathcal{F}, \mu)$ for each $p$""? If we both are right, then If $\mu$ is finite, $L^\infty = L^p$? This is a spinoff of a reply to my earlier question . Thanks!","I think $L^\infty(\Omega, \mathcal{F}, \mu)\supseteq L^p(\Omega, \mathcal{F}, \mu), \forall p \in (0, \infty)$? My reason is $L^\infty$ is defined as the set of measurable functions that are bounded up to a set of measure zero, and if $f \notin L^\infty$, then there exists a subset of measure nonzero on which $|f|$ is $\infty$, so $f \notin L^p, \forall p \in (0, \infty)$. So I wonder why ""If $\mu$ is finite, then $L^\infty(\Omega, \mathcal{F}, \mu)\subseteq L^p(\Omega, \mathcal{F}, \mu)$ for each $p$""? If we both are right, then If $\mu$ is finite, $L^\infty = L^p$? This is a spinoff of a reply to my earlier question . Thanks!",,['real-analysis']
46,Show that this limit is equal to $\liminf  a_{n}^{1/n}$ for positive terms.,Show that this limit is equal to  for positive terms.,\liminf  a_{n}^{1/n},"Show that if $a_{n}$ is a sequence of positive terms such that $\lim\limits_{n\to\infty} (a_{n+1}/a_n) $ exists, then this limit is equal to $\liminf\limits_{n\to\infty} a_n^{1/n}$. I am not event sure where to start from, any help would be much appreciated.","Show that if $a_{n}$ is a sequence of positive terms such that $\lim\limits_{n\to\infty} (a_{n+1}/a_n) $ exists, then this limit is equal to $\liminf\limits_{n\to\infty} a_n^{1/n}$. I am not event sure where to start from, any help would be much appreciated.",,"['real-analysis', 'sequences-and-series', 'limits']"
47,"If $f$ is differentiable on $[a,b]$, then it is also Lipschitz on it.","If  is differentiable on , then it is also Lipschitz on it.","f [a,b]","He guys, I am trying to show that a differentiable function defined on a closed interval is also Lipschitz on it. I managed to weave the below proof, but I have a feeling that it may be just a tad too general for this purpose: Theorem. If $f$ is differentiable on $[a,b]$, then it is also Lipschitz on it. Recall that $f:A\to\mathbb{R}$ is Lipschitz on $A$ if there exists an $M>0$ such that$$\left|\frac{f(x)-f(y)}{x-y}\right|\leq M$$for all $x,y\in A$. Proof. Let $f$ be differentiable on $[a,b]$. Because $f$ is continuous and $[a,b]$ is compact, by the Extreme Value Theorem , it follows that $f$ attains a maximum value $M$. Moreover, since $f$ is differentiable on $[a,b]$,$$f'(y)=\lim_{x\to y}\left|\frac{f(x)-f(y)}{x-y}\right|\leq M,$$for all $x,y\in[a,b]$, as required. $\square$ What do you guys think? Edit: What if we were to add that $f'$ is also continuous on $[a,b]$?","He guys, I am trying to show that a differentiable function defined on a closed interval is also Lipschitz on it. I managed to weave the below proof, but I have a feeling that it may be just a tad too general for this purpose: Theorem. If $f$ is differentiable on $[a,b]$, then it is also Lipschitz on it. Recall that $f:A\to\mathbb{R}$ is Lipschitz on $A$ if there exists an $M>0$ such that$$\left|\frac{f(x)-f(y)}{x-y}\right|\leq M$$for all $x,y\in A$. Proof. Let $f$ be differentiable on $[a,b]$. Because $f$ is continuous and $[a,b]$ is compact, by the Extreme Value Theorem , it follows that $f$ attains a maximum value $M$. Moreover, since $f$ is differentiable on $[a,b]$,$$f'(y)=\lim_{x\to y}\left|\frac{f(x)-f(y)}{x-y}\right|\leq M,$$for all $x,y\in[a,b]$, as required. $\square$ What do you guys think? Edit: What if we were to add that $f'$ is also continuous on $[a,b]$?",,['real-analysis']
48,Existence of irrationals in arbitrary intervals,Existence of irrationals in arbitrary intervals,,"I was studying for my analysis mid-term paper and was going over the properties of real numbers. I was wondering how to prove the following statement: (Not a textbook problem, it just popped into my head.) Given rational numbers $p$ and $q$ such that $p < q$, show that there exists an irrational number $r$ such that $p < r < q$. I know some ways of proving it, like picking a known irrational and shifting it into the open interval $(p,q)$. I was wondering whether there is a way to prove it without referencing to any previously known irrationals. Specifically I am trying to construct a sequence of rational numbers which converges to a irrational in the interval $(p,q)$. Is there any way to do that?","I was studying for my analysis mid-term paper and was going over the properties of real numbers. I was wondering how to prove the following statement: (Not a textbook problem, it just popped into my head.) Given rational numbers $p$ and $q$ such that $p < q$, show that there exists an irrational number $r$ such that $p < r < q$. I know some ways of proving it, like picking a known irrational and shifting it into the open interval $(p,q)$. I was wondering whether there is a way to prove it without referencing to any previously known irrationals. Specifically I am trying to construct a sequence of rational numbers which converges to a irrational in the interval $(p,q)$. Is there any way to do that?",,"['real-analysis', 'irrational-numbers']"
49,"One-to-one function from the interval $[0,1]$ to $\mathbb{R}\setminus\mathbb{Q}$?",One-to-one function from the interval  to ?,"[0,1] \mathbb{R}\setminus\mathbb{Q}","This question might turn out to be really trivial. $f$ is a one-to-one function from the interval $[0,1]$ to $\mathbb{R}$. Is it necessary that $\exists q \in \mathbb{Q}$ such that $f(x) = q$ for some $x \in [0,1]$ i.e. is it necessary that the image of $f$ contains a rational number? I came across this question when I was browsing through some website. I think this is false. But I am unable to come up with a counter example.","This question might turn out to be really trivial. $f$ is a one-to-one function from the interval $[0,1]$ to $\mathbb{R}$. Is it necessary that $\exists q \in \mathbb{Q}$ such that $f(x) = q$ for some $x \in [0,1]$ i.e. is it necessary that the image of $f$ contains a rational number? I came across this question when I was browsing through some website. I think this is false. But I am unable to come up with a counter example.",,['real-analysis']
50,Analytic guess for integral $\int_0^\infty \frac{\ln^2(\sqrt{x^2+1}+x) }{\sqrt{x^2 + 1}} \ln \frac{\cosh w+\sqrt{x^2+1} }{1+\sqrt{x^2+1} } dx$,Analytic guess for integral,\int_0^\infty \frac{\ln^2(\sqrt{x^2+1}+x) }{\sqrt{x^2 + 1}} \ln \frac{\cosh w+\sqrt{x^2+1} }{1+\sqrt{x^2+1} } dx,"(I was directed here from mathoverflow.) I am not able to do the following integral analytically, but after numerical evaluations was able to guess the result for all real $w$ values. Not only is the full $w$ -dependence clear numerically, the actual coefficients are also correct to very high precision. $$\int_0^\infty \frac{1}{\sqrt{x^2 + 1}} \log^2(\sqrt{x^2+1}+x) \log\left(\frac{\cosh(w)+\sqrt{x^2+1} }{1+\sqrt{x^2+1} } \right) dx = \frac{1}{12} \left( 2\pi^2 w^2 + w^4 \right)$$ The fact that the result is a simple low order polynomial in $w$ is surprising enough to me. I tried all kinds of tricks (new variables, partial integrations, etc.) to prove the above but was not able to. How would I go about proving it analytically?","(I was directed here from mathoverflow.) I am not able to do the following integral analytically, but after numerical evaluations was able to guess the result for all real values. Not only is the full -dependence clear numerically, the actual coefficients are also correct to very high precision. The fact that the result is a simple low order polynomial in is surprising enough to me. I tried all kinds of tricks (new variables, partial integrations, etc.) to prove the above but was not able to. How would I go about proving it analytically?",w w \int_0^\infty \frac{1}{\sqrt{x^2 + 1}} \log^2(\sqrt{x^2+1}+x) \log\left(\frac{\cosh(w)+\sqrt{x^2+1} }{1+\sqrt{x^2+1} } \right) dx = \frac{1}{12} \left( 2\pi^2 w^2 + w^4 \right) w,"['real-analysis', 'calculus', 'integration', 'definite-integrals']"
51,$\lim_{n \to \infty} \int_{0}^{1} e^{x^2} \sin(nx) dx$ [duplicate],[duplicate],\lim_{n \to \infty} \int_{0}^{1} e^{x^2} \sin(nx) dx,This question already has answers here : How to prove that $\lim\limits_{n\to\infty}\int\limits _{a}^{b}\sin\left(nt\right)f\left(t\right)dt=0\text { ? }$ (3 answers) Closed 3 years ago . $$\lim_{n \to \infty} \int_{0}^{1} e^{x^2} \sin(nx) dx$$ I am trying to find above limit without using the concept of Lebesgue integration. I know that $e^{x^2} sin(nx)$ is not a uniformly convergent sequence of functions so i can not take limit inside integration. So give me any idea that is it really possible to evaluate the limit without using the concept of lebesgue integration. Thank you,This question already has answers here : How to prove that $\lim\limits_{n\to\infty}\int\limits _{a}^{b}\sin\left(nt\right)f\left(t\right)dt=0\text { ? }$ (3 answers) Closed 3 years ago . I am trying to find above limit without using the concept of Lebesgue integration. I know that is not a uniformly convergent sequence of functions so i can not take limit inside integration. So give me any idea that is it really possible to evaluate the limit without using the concept of lebesgue integration. Thank you,\lim_{n \to \infty} \int_{0}^{1} e^{x^2} \sin(nx) dx e^{x^2} sin(nx),"['real-analysis', 'integration', 'limits']"
52,Prove or disprove that there does not exist a monotone function $f:\mathbb{R}\rightarrow\mathbb{Q}$ which is onto.,Prove or disprove that there does not exist a monotone function  which is onto.,f:\mathbb{R}\rightarrow\mathbb{Q},Prove or disprove that there does not exist a monotone function $f:\mathbb{R}\rightarrow\mathbb{Q}$ which is onto. Clearly $f$ can not be continuous. Suppose $f$ is discontinuous. Then it can have only countably many points of discontinuity. From this how to proceed?,Prove or disprove that there does not exist a monotone function which is onto. Clearly can not be continuous. Suppose is discontinuous. Then it can have only countably many points of discontinuity. From this how to proceed?,f:\mathbb{R}\rightarrow\mathbb{Q} f f,"['real-analysis', 'monotone-functions']"
53,Proving $(n+1)$th differential is $0$ given lower differentials are $0$,Proving th differential is  given lower differentials are,(n+1) 0 0,"Following is a question I am stuck in. Let $f : \Bbb R \to \Bbb R$ be an infinitely differentiable function and suppose that for some $n ≥ 1$ , $$f(1) = f(0) = f^{(1)}(0) = f^{(2)}(0) = · · · = f^{(n)}(0) = 0$$ Prove that there exists $x \in (0, 1)$ such that $f^{(n+1)}(x) = 0$ . It is a past question of an entrance exam. I thought to use Rolle's Theorem, but this requires information about $f^{(n)} (1)$ that I am unable to get. Only information about behaviour at $1$ I have is $f(1)=0$","Following is a question I am stuck in. Let be an infinitely differentiable function and suppose that for some , Prove that there exists such that . It is a past question of an entrance exam. I thought to use Rolle's Theorem, but this requires information about that I am unable to get. Only information about behaviour at I have is","f : \Bbb R \to \Bbb R n ≥ 1 f(1) = f(0) = f^{(1)}(0) = f^{(2)}(0) = · · · = f^{(n)}(0) = 0 x \in (0, 1) f^{(n+1)}(x) = 0 f^{(n)} (1) 1 f(1)=0","['real-analysis', 'calculus']"
54,Is the Set of Continuous Functions that are the Sum of Even and Odd Functions Meager?,Is the Set of Continuous Functions that are the Sum of Even and Odd Functions Meager?,,"Consider $X = \mathcal{C}([−1,1])$ with the usual norm $\|f\|_{\infty} = \sup_{t\in [−1,1]}|f(t)|.$ Define $$\mathcal{A}_{+}=\{ f \in X : f(t)=f(−t) \space \forall t\in [−1,1]\},$$ $$\mathcal{A}_{−}=\{ f \in X : f(t)=−f(−t) \space \forall t \in [−1,1]\}. $$ Is $\mathcal{A}_{+} +\mathcal{A}_{−} = \{f +g : f \in \mathcal{A}_{+},g \in \mathcal{A}_{−}\}$ meager? I know this set is dense by the Stone-Weierstrass Theorem. However, that doesn't really help. I also know that if the set is closed, then it is meager, but I have difficulties deciding whether it is closed or not. I know the exponential function is a limit of a sequence of a sum of even and odd functions, however one could define it to be that, in which case it doesn't help. Any hints on how to get going on this problem, and on whether the set $\mathcal{A}_{+}+{A}_{-} $ is closed or not? Thank you in advance.","Consider with the usual norm Define Is meager? I know this set is dense by the Stone-Weierstrass Theorem. However, that doesn't really help. I also know that if the set is closed, then it is meager, but I have difficulties deciding whether it is closed or not. I know the exponential function is a limit of a sequence of a sum of even and odd functions, however one could define it to be that, in which case it doesn't help. Any hints on how to get going on this problem, and on whether the set is closed or not? Thank you in advance.","X = \mathcal{C}([−1,1]) \|f\|_{\infty} = \sup_{t\in [−1,1]}|f(t)|. \mathcal{A}_{+}=\{ f \in X : f(t)=f(−t) \space \forall t\in [−1,1]\}, \mathcal{A}_{−}=\{ f \in X : f(t)=−f(−t) \space \forall t \in [−1,1]\}.  \mathcal{A}_{+} +\mathcal{A}_{−} = \{f +g : f \in \mathcal{A}_{+},g \in \mathcal{A}_{−}\} \mathcal{A}_{+}+{A}_{-} ","['real-analysis', 'general-topology', 'functional-analysis', 'metric-spaces', 'baire-category']"
55,Is there a function $f(x)$ such that $\lim_{x\rightarrow x_0}f(x)=\infty$ for all $x_0$ in some interval? [closed],Is there a function  such that  for all  in some interval? [closed],f(x) \lim_{x\rightarrow x_0}f(x)=\infty x_0,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let $f(x)$ be a real valued function with its domain in $\mathbb{R}$ . Is there an example of $f(x)$ such that $$\lim_{x\rightarrow x_0}f(x)=\infty$$ for all $x_0$ in some interval?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let be a real valued function with its domain in . Is there an example of such that for all in some interval?",f(x) \mathbb{R} f(x) \lim_{x\rightarrow x_0}f(x)=\infty x_0,"['calculus', 'real-analysis', 'examples-counterexamples']"
56,My Proof: Every convergent sequence is a Cauchy sequence.,My Proof: Every convergent sequence is a Cauchy sequence.,,"Let $(x_n)_{n\in\Bbb N}$ be a real sequence. $\textbf{Definition 1.}$ $(x_n)$ is $\textit{convergent}$ iff there is an $x\in\Bbb R$ such that, for every $\varepsilon\in\Bbb R$ with $\varepsilon>0$ , there is an $N\in\Bbb N$ such that, for every $n\in\Bbb N$ with $n>N$ , we have $|x_n-x|<\varepsilon$ . $\textbf{Definition 2.}$ $(x_n)$ is a $\textit{Cauchy sequence}$ iff, for every $\varepsilon \in\Bbb R$ with $\varepsilon > 0$ , there is an $N\in\Bbb N$ such that, for every $m,n\in\Bbb N$ with $m,n > N$ , we have $|x_m - x_n| < \varepsilon$ . $\textbf{Theorem.}$ If $(x_n)$ is convergent, then it is a Cauchy sequence. Proof : Since $(x_n)\to x$ we have the following for for some $\varepsilon_1, \varepsilon_2 > 0$ there exists $N_1, N_2 \in \Bbb N$ such for all $n_1>N_1$ and $n_2>N_2$ following holds $$|x_{n_1}-x|<\varepsilon_1\\ |x_{n_2}-x|<\varepsilon_2$$ So both will hold for all $n_1, n_2 >\max(N_1, N_2)=N$ , say $\varepsilon = \max(\varepsilon_1, \varepsilon_2)$ then $$|x_{n_1}-x-(x_{n_2}-x)|<\varepsilon\\\implies |x_{n_1}-x_{n_2}|<\varepsilon$$ Hence all convergent sequences are Cauchy. Is this proof correct? I also saw this question and copied some of the content(definition and theorem) from there.https://math.stackexchange.com/q/1105255","Let be a real sequence. is iff there is an such that, for every with , there is an such that, for every with , we have . is a iff, for every with , there is an such that, for every with , we have . If is convergent, then it is a Cauchy sequence. Proof : Since we have the following for for some there exists such for all and following holds So both will hold for all , say then Hence all convergent sequences are Cauchy. Is this proof correct? I also saw this question and copied some of the content(definition and theorem) from there.https://math.stackexchange.com/q/1105255","(x_n)_{n\in\Bbb N} \textbf{Definition 1.} (x_n) \textit{convergent} x\in\Bbb R \varepsilon\in\Bbb R \varepsilon>0 N\in\Bbb N n\in\Bbb N n>N |x_n-x|<\varepsilon \textbf{Definition 2.} (x_n) \textit{Cauchy sequence} \varepsilon \in\Bbb R \varepsilon > 0 N\in\Bbb N m,n\in\Bbb N m,n > N |x_m - x_n| < \varepsilon \textbf{Theorem.} (x_n) (x_n)\to x \varepsilon_1, \varepsilon_2 > 0 N_1, N_2 \in \Bbb N n_1>N_1 n_2>N_2 |x_{n_1}-x|<\varepsilon_1\\ |x_{n_2}-x|<\varepsilon_2 n_1, n_2 >\max(N_1, N_2)=N \varepsilon = \max(\varepsilon_1, \varepsilon_2) |x_{n_1}-x-(x_{n_2}-x)|<\varepsilon\\\implies |x_{n_1}-x_{n_2}|<\varepsilon","['real-analysis', 'solution-verification', 'cauchy-sequences']"
57,Uniform Convergence of $(\sin x)^n$,Uniform Convergence of,(\sin x)^n,"Is the sequence of functions $(\sin(x))^n$ uniformly convergent in $[0,\pi]$? Can you give me a hint or solution, I have already already prove that it is UC in $[0,1]$ but I don't know how to proceed in $[1,\pi]$.","Is the sequence of functions $(\sin(x))^n$ uniformly convergent in $[0,\pi]$? Can you give me a hint or solution, I have already already prove that it is UC in $[0,1]$ but I don't know how to proceed in $[1,\pi]$.",,"['real-analysis', 'uniform-convergence', 'sequence-of-function']"
58,Convergence of $\sum\frac{1}{n(\ln n)^c}$,Convergence of,\sum\frac{1}{n(\ln n)^c},"What are the values of the positive constant, $c$, for which    $$ \sum_{n=2}^\infty \frac{1}{n(\ln n)^c}$$   is convergent or divergent? I am a bit confused here, because usually the $c$ value is given, so I am not really sure how to approach this.","What are the values of the positive constant, $c$, for which    $$ \sum_{n=2}^\infty \frac{1}{n(\ln n)^c}$$   is convergent or divergent? I am a bit confused here, because usually the $c$ value is given, so I am not really sure how to approach this.",,"['real-analysis', 'calculus', 'convergence-divergence']"
59,Is all normed space also inner product space?,Is all normed space also inner product space?,,"1) I know that all inner product space is also a normed space with the norm induce by the scalar product, but is the reciprocal true ? I mean, is all normed space also a inner product space ? 2) I know that all normed space is a metric space with the metric induced by the norm. Is the reciprocal true ? I mean, is all metric space also a normed space ?","1) I know that all inner product space is also a normed space with the norm induce by the scalar product, but is the reciprocal true ? I mean, is all normed space also a inner product space ? 2) I know that all normed space is a metric space with the metric induced by the norm. Is the reciprocal true ? I mean, is all metric space also a normed space ?",,"['real-analysis', 'linear-algebra', 'general-topology', 'metric-spaces', 'normed-spaces']"
60,"Prove that if $12a+6b+4c+3d=0$, then $a+bx+cx^2+dx^3=0$ has a real solution in $(0,1)$","Prove that if , then  has a real solution in","12a+6b+4c+3d=0 a+bx+cx^2+dx^3=0 (0,1)","Assume that $a,b,c,d$ are real numbers such that $12a+6b+4c+3d=0$. Prove that $a+bx+cx^2+dx^3=0$ has a real solution in $(0,1)$. Note : I have no idea ! How is the assumption even related to the statement that the question wants us to prove? I don't understand. Second Note ( Edited ) : I know that if $x$ is too large, the equation goes to $+\infty$ and when $x$ is so much below $0$, the equation goes to $-\infty$. Then we can apply mean value theorem and say there exists some point such that on that point, the equation becomes zero. It's ok. But how to show that the root is in $(0,1)$ and what is the use of knowing $12a+6b+4c+3d=0$? Thanks in advance.","Assume that $a,b,c,d$ are real numbers such that $12a+6b+4c+3d=0$. Prove that $a+bx+cx^2+dx^3=0$ has a real solution in $(0,1)$. Note : I have no idea ! How is the assumption even related to the statement that the question wants us to prove? I don't understand. Second Note ( Edited ) : I know that if $x$ is too large, the equation goes to $+\infty$ and when $x$ is so much below $0$, the equation goes to $-\infty$. Then we can apply mean value theorem and say there exists some point such that on that point, the equation becomes zero. It's ok. But how to show that the root is in $(0,1)$ and what is the use of knowing $12a+6b+4c+3d=0$? Thanks in advance.",,['real-analysis']
61,How to prove that $\int_{-\infty}^{\infty} \frac{1-\cos x}{x^2} dx$ equal to $\pi $?,How to prove that  equal to ?,\int_{-\infty}^{\infty} \frac{1-\cos x}{x^2} dx \pi ,How to prove that $\int_{-\infty}^{\infty} \frac{1-\cos x}{x^2} dx$ equal to $\pi $? Is there any simple approach that does not require knowledge in Fourier Analysis or Complex analysis?,How to prove that $\int_{-\infty}^{\infty} \frac{1-\cos x}{x^2} dx$ equal to $\pi $? Is there any simple approach that does not require knowledge in Fourier Analysis or Complex analysis?,,"['calculus', 'real-analysis', 'integration']"
62,"Show $A\cap B \neq \varnothing \Rightarrow \operatorname{dist}(A,B) = 0$, and $\operatorname{dist}(A, B) = 0 \not\Rightarrow A\cap B \neq \varnothing$","Show , and","A\cap B \neq \varnothing \Rightarrow \operatorname{dist}(A,B) = 0 \operatorname{dist}(A, B) = 0 \not\Rightarrow A\cap B \neq \varnothing","I have a question Let $d$ be a metric on $X$ , and define the set to set distance $$\operatorname{dist}(A,B) = \inf\{d(x,y): x\in A, y \in B\}$$ where $A,B \subseteq X$ are nonempty sets Show that $A\cap B \neq \varnothing \Rightarrow \operatorname{dist}(A,B) = 0$ , and $\operatorname{dist}(A, B) = 0 \not\Rightarrow A\cap B \neq \varnothing$ First: ( $A\cap B \neq \varnothing \Rightarrow \operatorname{dist}(A,B) = 0$ ) Trivial, since $A \cap B \neq \varnothing \implies \exists z \in A$ and $B$ , so $\operatorname{dist}(A,B) = \inf\{d(z,z)\} = 0$ Second: ( $\operatorname{dist}(A, B) = 0 \not\Rightarrow A\cap B \neq \varnothing$ ) We want to produce $A \cap B = \varnothing$ such that $\operatorname{dist}(A,B) = 0$ . Is there a metric space where this can happen? I've checked the discrete metric, all the $\ell_p$ metrics. I don't think you can have disjoint sets with their distance zero.","I have a question Let be a metric on , and define the set to set distance where are nonempty sets Show that , and First: ( ) Trivial, since and , so Second: ( ) We want to produce such that . Is there a metric space where this can happen? I've checked the discrete metric, all the metrics. I don't think you can have disjoint sets with their distance zero.","d X \operatorname{dist}(A,B) = \inf\{d(x,y): x\in A, y \in B\} A,B \subseteq X A\cap B \neq \varnothing \Rightarrow \operatorname{dist}(A,B) = 0 \operatorname{dist}(A, B) = 0 \not\Rightarrow A\cap B \neq \varnothing A\cap B \neq \varnothing \Rightarrow \operatorname{dist}(A,B) = 0 A \cap B \neq \varnothing \implies \exists z \in A B \operatorname{dist}(A,B) = \inf\{d(z,z)\} = 0 \operatorname{dist}(A, B) = 0 \not\Rightarrow A\cap B \neq \varnothing A \cap B = \varnothing \operatorname{dist}(A,B) = 0 \ell_p","['real-analysis', 'metric-spaces']"
63,"Is there a connection between the concepts of limits in ordinals, functions and categories?","Is there a connection between the concepts of limits in ordinals, functions and categories?",,In set theory there is the concept of a limit ordinal: Nonzero ordinals that are the supermum of all ordinals below them. In functional analysis there are the concepts of limits of functions (and sequences) a value that the function comes arbitrarily close to at a point. And in category theory there is a concept of a limit which is a universal cone. Is there something common about all these ideas that justifies them all being called limits or is it a coincidence of language ?,In set theory there is the concept of a limit ordinal: Nonzero ordinals that are the supermum of all ordinals below them. In functional analysis there are the concepts of limits of functions (and sequences) a value that the function comes arbitrarily close to at a point. And in category theory there is a concept of a limit which is a universal cone. Is there something common about all these ideas that justifies them all being called limits or is it a coincidence of language ?,,"['real-analysis', 'functional-analysis', 'limits', 'elementary-set-theory', 'category-theory']"
64,Proving every bounded infinite set has a limit point without using Bolzano-Weierstrass.,Proving every bounded infinite set has a limit point without using Bolzano-Weierstrass.,,"I am trying to prove that every bounded, infinite set has at least one limit point through the concept of open covers and without using Bolzano Weierstrass. I have no idea where to start. So far I've only assumed that a set A is bounded and infinite, so for some real number M, the set A is entirely contained in the closed and bounded set [-M, M]. By Heine Bornel I know that each open cover for [-M, M] has a finite subcover, but honestly I don't know where to go from there. Am I in the right direction?","I am trying to prove that every bounded, infinite set has at least one limit point through the concept of open covers and without using Bolzano Weierstrass. I have no idea where to start. So far I've only assumed that a set A is bounded and infinite, so for some real number M, the set A is entirely contained in the closed and bounded set [-M, M]. By Heine Bornel I know that each open cover for [-M, M] has a finite subcover, but honestly I don't know where to go from there. Am I in the right direction?",,['real-analysis']
65,"Pointwise convergence to zero, with integrals converging to a nonzero value","Pointwise convergence to zero, with integrals converging to a nonzero value",,"For $n\in{\mathbb{N}}$ let $$f_n(x)=nx(1-x^2)^n\qquad(0\le x\le 1).$$ Show that $\{f_n\}_{n=1}^\infty$ converges pointwise to $0$ on $[0,1]$. Show that $\{\int_0^1f_n\}_{n=1}^\infty$ converges to $\frac12$. I've already shown both of these statements to be true. What I don't understand is this: how can $f_n$ converge pointwise to $0$, yet the sequence $\{\int_0^1f_n\}_{n=1}^\infty$ converges to $\frac12$? Isn't that almost like saying $f(x)=0\qquad(a\le x\le b)$, but $\int_a^bf(x)=\frac12$? Clearly that would be false. I know this has something to do with that this is a sequence of functions but it still baffles my mind. Thanks in advance.","For $n\in{\mathbb{N}}$ let $$f_n(x)=nx(1-x^2)^n\qquad(0\le x\le 1).$$ Show that $\{f_n\}_{n=1}^\infty$ converges pointwise to $0$ on $[0,1]$. Show that $\{\int_0^1f_n\}_{n=1}^\infty$ converges to $\frac12$. I've already shown both of these statements to be true. What I don't understand is this: how can $f_n$ converge pointwise to $0$, yet the sequence $\{\int_0^1f_n\}_{n=1}^\infty$ converges to $\frac12$? Isn't that almost like saying $f(x)=0\qquad(a\le x\le b)$, but $\int_a^bf(x)=\frac12$? Clearly that would be false. I know this has something to do with that this is a sequence of functions but it still baffles my mind. Thanks in advance.",,['real-analysis']
66,Why are there more Irrationals than Rationals given the density of $Q$ in $R$?,Why are there more Irrationals than Rationals given the density of  in ?,Q R,"I'm reading ""Understanding Analysis"" by Abbott, and I'm confused about the density of $Q$ in $R$ and how that ties to the cardinality of rational vs irrational numbers. First, on page 20, Theorem 1.4.3 ""Density of $Q$ in $R$"" Abbot states: For every two real numbers a and b   with a < b, there exists a rational number r satisfying a < r < b. For which he provides a proof. Later, on page 22, in the section titled ""Countable and Uncountable Sets"" he states: Mentally, there is a temptation to think of $Q$ and $I$ as being intricately mixed together in equal proportions, but this turns out not to be the case...the irrational numbers far outnumber the rational numbers in making   up the real line. My question is: how are these two statements not in direct contradiction? Given any closed interval of irrational numbers of cardinality $X$, $A$, shouldn't be the case that we would have corresponding set of $X-1$ rational numbers, $B$, where each rational in $B$ falls ""between"" two other irrationals in $A$? If this is not the case, how do we have so many more irrationals than rationals while still satisfying our theorem that between every two reals there is a rational number? I know there are other questions similar to this, but I haven't found an answer that explains this very well, and none that address this (perceived) contradiction.","I'm reading ""Understanding Analysis"" by Abbott, and I'm confused about the density of $Q$ in $R$ and how that ties to the cardinality of rational vs irrational numbers. First, on page 20, Theorem 1.4.3 ""Density of $Q$ in $R$"" Abbot states: For every two real numbers a and b   with a < b, there exists a rational number r satisfying a < r < b. For which he provides a proof. Later, on page 22, in the section titled ""Countable and Uncountable Sets"" he states: Mentally, there is a temptation to think of $Q$ and $I$ as being intricately mixed together in equal proportions, but this turns out not to be the case...the irrational numbers far outnumber the rational numbers in making   up the real line. My question is: how are these two statements not in direct contradiction? Given any closed interval of irrational numbers of cardinality $X$, $A$, shouldn't be the case that we would have corresponding set of $X-1$ rational numbers, $B$, where each rational in $B$ falls ""between"" two other irrationals in $A$? If this is not the case, how do we have so many more irrationals than rationals while still satisfying our theorem that between every two reals there is a rational number? I know there are other questions similar to this, but I haven't found an answer that explains this very well, and none that address this (perceived) contradiction.",,"['real-analysis', 'irrational-numbers']"
67,"Prove sum of the lengths of intervals in a finite covering of $\mathbb{Q}\cap [0,1]$ is $\geq 1$",Prove sum of the lengths of intervals in a finite covering of  is,"\mathbb{Q}\cap [0,1] \geq 1","I have the following proof, but I don't think it's right. Could somebody please tell me what's wrong with it, and how to fix it?  Thanks :) Let $B$ be the set of rational numbers in the interval $[0,1]$ Let $I_{k}$ , $k=1,2,\cdots, n$ be a finite collection of open intervals that covers $B$ .  Prove that $\sum_{k=1}^{n}l(I_{k})\geq 1$ . ( $l(I)$ denotes the length of the interval $I$ ). $\underline{\mathbf{\text{Proof}}}$ : Suppose $a,b \in [0,1]$ such that $a < b$ . By the density of the rational numbers in $\mathbb{R}$ , $\exists q_{i} \in \mathbb{Q}$ such that $\forall a_{i} < b_{i}$ , $q_{i}\in(a_{i},b_{i})$ . Since the rationals are dense in $\mathbb{R}$ and hence in $[0,1]$ , we can cover $[0,1]$ as follows: $[0,1] \subseteq \cup_{k=1}^{\infty}I_{k}=\cup_{k=1}^{\infty} (a_{k},b_{k}) $ . (In the case where the $(a_{k},b_{k})$ do not completely cover $[0,1]$ completely, $\exists$ a point between $(a_{i},b_{i})$ and $(a_{i+1}, b_{i+1})$ that remains uncovered.  However, because there are countably many intervals, there will be only countably many such points.  Thus, the set of points, call it $S$ , being a countable set will have $m^{*}(S)=0$ .  So, these points will be negligible in terms of their effect on the sum of the lengths of the intervals.) By Heine-Borel, since $[0,1]$ is closed and bounded, every such open cover has a finite subcover. So, $[0,1] \subset \cup_{k=1}^{n}(a_{k},b_{k}) = \cup_{k=1}^{n}I_{k}$ . Now, the outer measure of an interval is its length.  So $m^{*}([0,1])=l([0,1]) = 1$ , and by the definition of the outer measure, $1 = m^{*}([0,1]) = \inf_{k=1}^{n}l(I_{k}) \leq \sum_{k=1}^{n}l(I_{k})$ Q.E.D. I also realize this question has been asked before on here, but I am terribly dissatisfied with the answers that have thus far been given.","I have the following proof, but I don't think it's right. Could somebody please tell me what's wrong with it, and how to fix it?  Thanks :) Let be the set of rational numbers in the interval Let , be a finite collection of open intervals that covers .  Prove that . ( denotes the length of the interval ). : Suppose such that . By the density of the rational numbers in , such that , . Since the rationals are dense in and hence in , we can cover as follows: . (In the case where the do not completely cover completely, a point between and that remains uncovered.  However, because there are countably many intervals, there will be only countably many such points.  Thus, the set of points, call it , being a countable set will have .  So, these points will be negligible in terms of their effect on the sum of the lengths of the intervals.) By Heine-Borel, since is closed and bounded, every such open cover has a finite subcover. So, . Now, the outer measure of an interval is its length.  So , and by the definition of the outer measure, Q.E.D. I also realize this question has been asked before on here, but I am terribly dissatisfied with the answers that have thus far been given.","B [0,1] I_{k} k=1,2,\cdots, n B \sum_{k=1}^{n}l(I_{k})\geq 1 l(I) I \underline{\mathbf{\text{Proof}}} a,b \in [0,1] a < b \mathbb{R} \exists q_{i} \in \mathbb{Q} \forall a_{i} < b_{i} q_{i}\in(a_{i},b_{i}) \mathbb{R} [0,1] [0,1] [0,1] \subseteq \cup_{k=1}^{\infty}I_{k}=\cup_{k=1}^{\infty} (a_{k},b_{k})  (a_{k},b_{k}) [0,1] \exists (a_{i},b_{i}) (a_{i+1}, b_{i+1}) S m^{*}(S)=0 [0,1] [0,1] \subset \cup_{k=1}^{n}(a_{k},b_{k}) = \cup_{k=1}^{n}I_{k} m^{*}([0,1])=l([0,1]) = 1 1 = m^{*}([0,1]) = \inf_{k=1}^{n}l(I_{k}) \leq \sum_{k=1}^{n}l(I_{k})","['real-analysis', 'measure-theory']"
68,How can I prove $\lim \limits_{n \to \infty}\left(1+\frac{1}{a_n}\right)^{a_n}=e$ without involving function limit?,How can I prove  without involving function limit?,\lim \limits_{n \to \infty}\left(1+\frac{1}{a_n}\right)^{a_n}=e,"If I already know that $$\lim \limits_{n \to \infty} a_n=+\infty$$ Then how can I prove $$\lim \limits_{n \to \infty}\left(1+\frac{1}{a_n}\right)^{a_n}=e$$  without involving function limit? This question comes because you may find some books on calculus or analysis (maybe they are badly written) require you to prove something like $$\lim \limits_{n \to \infty}\left(1+\frac{2}{n}\right)^{n}=e^2$$ or something more complex even before they formally introduce the definition of limit of a function, They are hard to prove because you can't simply take something like $\frac{n}{2}$ as a subsequence of $n$. The definition of limit of a function (at infinity) here mean: For a real function $f$ which is well-defined on $[a, +\infty)$, if for any $\epsilon >0$, there is a positive number $M \geq a$ such that when $x>M$ we can say $|f(x)-A|<\epsilon$, then   $$\lim \limits_{x \to \infty}f(x)=A.$$ While the definition of limit of a sequence here mean: For a sequence $\{a_n\}$, if for any $\epsilon>0$, there is a positive integer $N$ such that when $n>N$ we can say $|a_n-A|<\epsilon$, then $$\lim \limits_{n \to \infty}a_n=A.$$ I know sequence is a ""special"" kind of function whose domain is $\mathbb{N}$ and thus sequence limit is but a special case of function limit. Here I say avoid involving the idea of function limit means not to use the idea above but only to prove it by the ""special case"" below. After all, $(1+\frac{1}{a_n})^{a_n}$ is still a ""special"" function - a sequence. p.s. $e$ is defined by $$\lim \limits_{n \to \infty}\left(1+\frac{1}{n}\right)^n=e.$$ My try so far: Since $$\lim \limits_{n \to \infty}\left(1+\frac{1}{n}\right)^n=e,$$ for every $\epsilon>0$, there is $N \in \mathbb{N}$ s.t. for all $n>N$,$$|\left(1+\frac{1}{n}\right)^n-e|<\epsilon.$$ Meanwhile, since $$\lim \limits_{n \to \infty} a_n=+\infty,$$ for $N' \in \mathbb{N}$ and $N'>N$, there is $N'' \in \mathbb{N}$ s.t. for all $n>N''$, $a_n>N'>N.$ However, if $a_n$ become bigger then $1+\frac{1}{a_n}$ will be smaller, and vise versa, so I don't know how to deal with $\left(1+\frac{1}{a_n}\right)^{a_n}.$","If I already know that $$\lim \limits_{n \to \infty} a_n=+\infty$$ Then how can I prove $$\lim \limits_{n \to \infty}\left(1+\frac{1}{a_n}\right)^{a_n}=e$$  without involving function limit? This question comes because you may find some books on calculus or analysis (maybe they are badly written) require you to prove something like $$\lim \limits_{n \to \infty}\left(1+\frac{2}{n}\right)^{n}=e^2$$ or something more complex even before they formally introduce the definition of limit of a function, They are hard to prove because you can't simply take something like $\frac{n}{2}$ as a subsequence of $n$. The definition of limit of a function (at infinity) here mean: For a real function $f$ which is well-defined on $[a, +\infty)$, if for any $\epsilon >0$, there is a positive number $M \geq a$ such that when $x>M$ we can say $|f(x)-A|<\epsilon$, then   $$\lim \limits_{x \to \infty}f(x)=A.$$ While the definition of limit of a sequence here mean: For a sequence $\{a_n\}$, if for any $\epsilon>0$, there is a positive integer $N$ such that when $n>N$ we can say $|a_n-A|<\epsilon$, then $$\lim \limits_{n \to \infty}a_n=A.$$ I know sequence is a ""special"" kind of function whose domain is $\mathbb{N}$ and thus sequence limit is but a special case of function limit. Here I say avoid involving the idea of function limit means not to use the idea above but only to prove it by the ""special case"" below. After all, $(1+\frac{1}{a_n})^{a_n}$ is still a ""special"" function - a sequence. p.s. $e$ is defined by $$\lim \limits_{n \to \infty}\left(1+\frac{1}{n}\right)^n=e.$$ My try so far: Since $$\lim \limits_{n \to \infty}\left(1+\frac{1}{n}\right)^n=e,$$ for every $\epsilon>0$, there is $N \in \mathbb{N}$ s.t. for all $n>N$,$$|\left(1+\frac{1}{n}\right)^n-e|<\epsilon.$$ Meanwhile, since $$\lim \limits_{n \to \infty} a_n=+\infty,$$ for $N' \in \mathbb{N}$ and $N'>N$, there is $N'' \in \mathbb{N}$ s.t. for all $n>N''$, $a_n>N'>N.$ However, if $a_n$ become bigger then $1+\frac{1}{a_n}$ will be smaller, and vise versa, so I don't know how to deal with $\left(1+\frac{1}{a_n}\right)^{a_n}.$",,"['real-analysis', 'limits', 'proof-writing', 'exponential-function']"
69,"Show that $f(x) = x^2$ is not uniformly continuous on $[0,\infty)$",Show that  is not uniformly continuous on,"f(x) = x^2 [0,\infty)","Ok, I know the same question has already been asked here , and I am not looking for an answer even though my proof looks kind of the same. But, I need to know whether or not I am on the right track. Also, the choice of $y$ on the other proof doesn't many much sense to me. So, here it goes: Show that $f(x) = x^2$ is not uniformly continuous on $[0,\infty)$ This is what I did: Suppose, it is. Then, fix $\epsilon = 1 > 0.$ Let, $x < \delta \in [0, \infty)$ and $y = 2x \in [0,\infty)$. Then, according to the definition, $$\forall \epsilon > 0, \quad\exists \delta > 0\quad\text{such that} \quad\forall x,y \in [0,\infty),\qquad\mid x-y\mid < \delta\quad \implies\quad\mid f(x) - f(y)\mid < \epsilon$$ If we replace $y = 2x$, then $$\mid x -2x\mid = \mid -x\mid = x < \delta.$$ So, that holds. Now, $$\mid f(x) - f(y)\mid = \mid x^2 - 4x^2\mid = \mid -3x^2\mid = 3x^2 > \epsilon = 1,$$ which is a contradiction depending on the choice of $x$. Is it correct? Can I do that? Thanks.","Ok, I know the same question has already been asked here , and I am not looking for an answer even though my proof looks kind of the same. But, I need to know whether or not I am on the right track. Also, the choice of $y$ on the other proof doesn't many much sense to me. So, here it goes: Show that $f(x) = x^2$ is not uniformly continuous on $[0,\infty)$ This is what I did: Suppose, it is. Then, fix $\epsilon = 1 > 0.$ Let, $x < \delta \in [0, \infty)$ and $y = 2x \in [0,\infty)$. Then, according to the definition, $$\forall \epsilon > 0, \quad\exists \delta > 0\quad\text{such that} \quad\forall x,y \in [0,\infty),\qquad\mid x-y\mid < \delta\quad \implies\quad\mid f(x) - f(y)\mid < \epsilon$$ If we replace $y = 2x$, then $$\mid x -2x\mid = \mid -x\mid = x < \delta.$$ So, that holds. Now, $$\mid f(x) - f(y)\mid = \mid x^2 - 4x^2\mid = \mid -3x^2\mid = 3x^2 > \epsilon = 1,$$ which is a contradiction depending on the choice of $x$. Is it correct? Can I do that? Thanks.",,"['real-analysis', 'proof-verification', 'solution-verification', 'uniform-continuity']"
70,Percentage of rational numbers on an interval,Percentage of rational numbers on an interval,,"Today, I just came up with this random question: What is the percentage of of rational numbers on an interval? Let $\mathbb{Q}$ be the set of the rational numbers: 1- Take an interval on the real axis: $A=[a,b]$, then define two sets: $S_1=\{x\mid x\in A\cap\mathbb{Q}  \},~~ S_2=\{x\mid x\in A ~\text{and} ~x\notin\mathbb{Q}  \}$, what is the value of the ratio $\frac{\#S1}{\#S1+\#S2}$, where $\#S1$ and $\#S2$ are the cardinality of the respective sets. 2- How is this ratio related to the choices of $a$ and $b$? Thanks!","Today, I just came up with this random question: What is the percentage of of rational numbers on an interval? Let $\mathbb{Q}$ be the set of the rational numbers: 1- Take an interval on the real axis: $A=[a,b]$, then define two sets: $S_1=\{x\mid x\in A\cap\mathbb{Q}  \},~~ S_2=\{x\mid x\in A ~\text{and} ~x\notin\mathbb{Q}  \}$, what is the value of the ratio $\frac{\#S1}{\#S1+\#S2}$, where $\#S1$ and $\#S2$ are the cardinality of the respective sets. 2- How is this ratio related to the choices of $a$ and $b$? Thanks!",,"['real-analysis', 'real-numbers']"
71,Show that the function $f$ is continuous only at the irrational points,Show that the function  is continuous only at the irrational points,f,"Prove that the function $f$ is continuous only at the irrational points. $f(x)=\begin{cases}        0 & ;x \in \mathbb R-\mathbb Q \\       \dfrac{1}{n} & ;x=\dfrac {m}{n} :\gcd(m,n)=1;m,n \in \mathbb Z \\       1 & ;x=0     \end{cases}$ Attempt: Let us suppose $a \in [0,1]$ such that $f$ is continuous in $[0,1]$ Then : $\forall \epsilon >0, \exists \delta >0 $ such that $|g(x)-g(a)|< \epsilon$ whenever $|x-a|< \delta$ Case $1$ :Suppose $a$ is a rational element in the interval $[0,1]$ Then if $a=\dfrac {p}{q} \implies g(a) = \dfrac {1}{q}$. Hence : $|g(x)-\dfrac {1}{q} |< \epsilon~~~........(1)$. $(a)$ Now, if $x$ is a rational element, then $(1)$ reduces to $\dfrac {1}{q} < \epsilon$ which is not true always. Hence, there is no continuity for any rational point $a$ Case $2$ :Suppose $a$ is an irrational element in the interval $[0,1]$ $\forall \epsilon >0, \exists \delta >0 $ such that $|g(x)-g(a)|< \epsilon$ whenever $|x-a|< \delta$ Hence, $|g(x)-0|< \epsilon ~~~...........(2)$ $(a)$ Now, if $x= \dfrac {r}{s}$ is a rational element: $g(x) =\dfrac {1}{s}$ There always exist an integer $s$ such that $(2)$ holds But, how do we find the value of $\delta$? $(b)$ If $x$ is an irrational element, then $|g(x)|=0< \epsilon$ Which is always true for any $\delta$. Did I attempt this problem correctly? Thank you for your help.","Prove that the function $f$ is continuous only at the irrational points. $f(x)=\begin{cases}        0 & ;x \in \mathbb R-\mathbb Q \\       \dfrac{1}{n} & ;x=\dfrac {m}{n} :\gcd(m,n)=1;m,n \in \mathbb Z \\       1 & ;x=0     \end{cases}$ Attempt: Let us suppose $a \in [0,1]$ such that $f$ is continuous in $[0,1]$ Then : $\forall \epsilon >0, \exists \delta >0 $ such that $|g(x)-g(a)|< \epsilon$ whenever $|x-a|< \delta$ Case $1$ :Suppose $a$ is a rational element in the interval $[0,1]$ Then if $a=\dfrac {p}{q} \implies g(a) = \dfrac {1}{q}$. Hence : $|g(x)-\dfrac {1}{q} |< \epsilon~~~........(1)$. $(a)$ Now, if $x$ is a rational element, then $(1)$ reduces to $\dfrac {1}{q} < \epsilon$ which is not true always. Hence, there is no continuity for any rational point $a$ Case $2$ :Suppose $a$ is an irrational element in the interval $[0,1]$ $\forall \epsilon >0, \exists \delta >0 $ such that $|g(x)-g(a)|< \epsilon$ whenever $|x-a|< \delta$ Hence, $|g(x)-0|< \epsilon ~~~...........(2)$ $(a)$ Now, if $x= \dfrac {r}{s}$ is a rational element: $g(x) =\dfrac {1}{s}$ There always exist an integer $s$ such that $(2)$ holds But, how do we find the value of $\delta$? $(b)$ If $x$ is an irrational element, then $|g(x)|=0< \epsilon$ Which is always true for any $\delta$. Did I attempt this problem correctly? Thank you for your help.",,"['real-analysis', 'proof-verification', 'continuity']"
72,Is the distance function differentiable?,Is the distance function differentiable?,,"Let $I=[0,1]\subset \mathbb{R}$ and for $x\in \mathbb{R}$ define $f(x)= \operatorname{dist}(x,I)$ then I need to find out its points of differentiability. I could see that $f(x)$ is continuous everywhere, but for differentiability at some point $x=c$ what has to be done?","Let $I=[0,1]\subset \mathbb{R}$ and for $x\in \mathbb{R}$ define $f(x)= \operatorname{dist}(x,I)$ then I need to find out its points of differentiability. I could see that $f(x)$ is continuous everywhere, but for differentiability at some point $x=c$ what has to be done?",,['real-analysis']
73,"How can I show that $L^{1}([0,1])$ is a separable metric space?",How can I show that  is a separable metric space?,"L^{1}([0,1])","I am trying to show that $L^{1}([0,1])$ is separable. I know by definition that a space is separable if I can prove the existence of a countable and dense subset. However, I really dont know even how to start. I would appreciate any help, thank you!","I am trying to show that $L^{1}([0,1])$ is separable. I know by definition that a space is separable if I can prove the existence of a countable and dense subset. However, I really dont know even how to start. I would appreciate any help, thank you!",,"['real-analysis', 'functional-analysis', 'measure-theory', 'lp-spaces', 'separable-spaces']"
74,"Integral $\iint \limits_{{x,y \ \in \ [0,1]}} \frac{\log(1-x)\log(1-y)}{1-xy}dx\,dy=\frac{17\pi^4}{360}$",Integral,"\iint \limits_{{x,y \ \in \ [0,1]}} \frac{\log(1-x)\log(1-y)}{1-xy}dx\,dy=\frac{17\pi^4}{360}","Hi I am trying to integrate $$ \mathcal{I}:=\iint \limits_{{x,y \ \in \ [0,1]}} \frac{\log(1-x)\log(1-y)}{1-xy}dx\,dy=\int_0^1\int_0^1 \frac{\log(1-x)\log(1-y)}{1-xy}dx \,dy $$ A closed form does exist. I tried to write \begin{align} \mathcal{I} &=\int_0^1 \log(1-y)dy\int_0^1 \log(1-x)\frac{dx}{1-xy} \\ &= \int_0^1\log(1-y)dy \ \int_0^1 \sum_{n\geq0}(xy)^n\, \ln(1-x) \ dx \\ &= \sum_{n\geq 0}\frac{1}{n+1}\int_0^1 \log(1-y) y^n\, dy \\ &= \sum_{n\geq 0}\frac{1}{n+1}\int_0^1 \log(1-y)y^n\, dy = ? \end{align} I was able to realize that  $$ \mathcal{I}=\sum_{n\geq 1}\left(\frac{H_n}{n}\right)^2=\frac{17\zeta_4}{4}=\frac{17\pi^4}{360},\qquad \zeta_4=\sum_{n\geq 1} n^{-4}  $$ however this does not help me solve the problem.  How can we calculate $\mathcal{I}$? Thanks.","Hi I am trying to integrate $$ \mathcal{I}:=\iint \limits_{{x,y \ \in \ [0,1]}} \frac{\log(1-x)\log(1-y)}{1-xy}dx\,dy=\int_0^1\int_0^1 \frac{\log(1-x)\log(1-y)}{1-xy}dx \,dy $$ A closed form does exist. I tried to write \begin{align} \mathcal{I} &=\int_0^1 \log(1-y)dy\int_0^1 \log(1-x)\frac{dx}{1-xy} \\ &= \int_0^1\log(1-y)dy \ \int_0^1 \sum_{n\geq0}(xy)^n\, \ln(1-x) \ dx \\ &= \sum_{n\geq 0}\frac{1}{n+1}\int_0^1 \log(1-y) y^n\, dy \\ &= \sum_{n\geq 0}\frac{1}{n+1}\int_0^1 \log(1-y)y^n\, dy = ? \end{align} I was able to realize that  $$ \mathcal{I}=\sum_{n\geq 1}\left(\frac{H_n}{n}\right)^2=\frac{17\zeta_4}{4}=\frac{17\pi^4}{360},\qquad \zeta_4=\sum_{n\geq 1} n^{-4}  $$ however this does not help me solve the problem.  How can we calculate $\mathcal{I}$? Thanks.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
75,Cantor's Set Has No Intervals,Cantor's Set Has No Intervals,,"I'm asked to prove that the Cantor set has no intervals. This is what I've got: Let $(a,b)\subseteq [0,1]$, where $a<b$. Let $T=\{-\log_3(b-a)<n:n\in\mathbb{N}\}$. Now if $(a,b)\subseteq[0,1]$, then $b-a<1$, and so $-\log_3(b-a)>0$. This would then mean that $-\log_3(b-a)\in\mathbb{R}$, and so by the Archimidean property there exists some $\alpha\in T$ such that $a\leq \beta$ for all $\beta\in T$. Hence $$-\log_3(b-a)<\alpha,$$ so we have know that $$3^{-(-\log_3(b-a))}>3^{-\alpha}.$$ Of course the LHS here reduces the inequality as $$b-a=\lvert b-a \rvert < 3^{-\alpha}.$$ Now $A_{\alpha}$ is the union of subsets of $[0,1]$ having length $3^{-\alpha}$, so $(a,b)\not\subseteq A_{\alpha}$; hence $(a,b)\not\subseteq C$, as was to be shown. I'm wondering what your opinion on the proof is. Good? Bad? Incorrect? Throw it away! There's a faster way! You know, all of this.","I'm asked to prove that the Cantor set has no intervals. This is what I've got: Let $(a,b)\subseteq [0,1]$, where $a<b$. Let $T=\{-\log_3(b-a)<n:n\in\mathbb{N}\}$. Now if $(a,b)\subseteq[0,1]$, then $b-a<1$, and so $-\log_3(b-a)>0$. This would then mean that $-\log_3(b-a)\in\mathbb{R}$, and so by the Archimidean property there exists some $\alpha\in T$ such that $a\leq \beta$ for all $\beta\in T$. Hence $$-\log_3(b-a)<\alpha,$$ so we have know that $$3^{-(-\log_3(b-a))}>3^{-\alpha}.$$ Of course the LHS here reduces the inequality as $$b-a=\lvert b-a \rvert < 3^{-\alpha}.$$ Now $A_{\alpha}$ is the union of subsets of $[0,1]$ having length $3^{-\alpha}$, so $(a,b)\not\subseteq A_{\alpha}$; hence $(a,b)\not\subseteq C$, as was to be shown. I'm wondering what your opinion on the proof is. Good? Bad? Incorrect? Throw it away! There's a faster way! You know, all of this.",,"['real-analysis', 'proof-verification', 'compactness']"
76,Prove $f$ is constant if $|f(x)-f(y)|\leq M|x-y|^\alpha$ [duplicate],Prove  is constant if  [duplicate],f |f(x)-f(y)|\leq M|x-y|^\alpha,"This question already has answers here : How to show that every $\alpha$-Hölder function, with $\alpha>1$, is constant? (5 answers) Closed 8 years ago . Let $\alpha > 1$ and $M \geq 0$. Suppose $f: \mathbb{R} \longrightarrow \mathbb{R}$ satisfies $|f(x)-f(y)|\leq M|x-y|^\alpha$ for all $x, y\in \mathbb{R}$. How can we prove that $f$ is a constant function? I don't even know where to start.","This question already has answers here : How to show that every $\alpha$-Hölder function, with $\alpha>1$, is constant? (5 answers) Closed 8 years ago . Let $\alpha > 1$ and $M \geq 0$. Suppose $f: \mathbb{R} \longrightarrow \mathbb{R}$ satisfies $|f(x)-f(y)|\leq M|x-y|^\alpha$ for all $x, y\in \mathbb{R}$. How can we prove that $f$ is a constant function? I don't even know where to start.",,['real-analysis']
77,Bathtub principle proof,Bathtub principle proof,,"Let $ (\Omega, \Sigma, \mu) $ be a measure space and let $f$ be a real valued function on $\Omega $ such that  $\mu (x :f(x)<t) $ is finite for all t $\in \mathbb{R}$. Let the number $G>0$ be and given and defined a class of measurable functions $\Omega$ by $C= (g: 0\le g(x) \le 1  $ for all $x$ and $ \int g(x)\mu(dx)=G) $ Then the minimization problem I= $\inf_{g \in C} \int f(x)g(x) \mu(dx)$ is solved by $g(x)= \chi_{(f<s)}(x) + c\chi_{(f=s)}(x)$ where $s=\sup(t: \mu((x :f(x)<t)) \le G) $ and $c\mu ((x :f(x)=s))=G-\mu ((x :f(x)<s))$ My question is, how can one show that g(x) given above is the minimizer. Thanks","Let $ (\Omega, \Sigma, \mu) $ be a measure space and let $f$ be a real valued function on $\Omega $ such that  $\mu (x :f(x)<t) $ is finite for all t $\in \mathbb{R}$. Let the number $G>0$ be and given and defined a class of measurable functions $\Omega$ by $C= (g: 0\le g(x) \le 1  $ for all $x$ and $ \int g(x)\mu(dx)=G) $ Then the minimization problem I= $\inf_{g \in C} \int f(x)g(x) \mu(dx)$ is solved by $g(x)= \chi_{(f<s)}(x) + c\chi_{(f=s)}(x)$ where $s=\sup(t: \mu((x :f(x)<t)) \le G) $ and $c\mu ((x :f(x)=s))=G-\mu ((x :f(x)<s))$ My question is, how can one show that g(x) given above is the minimizer. Thanks",,"['real-analysis', 'measure-theory']"
78,Is there a minimal diverging series?,Is there a minimal diverging series?,,Is there a function $f:\mathbb{N} \to \mathbb{R}^+$ s.t. its series $\Sigma_{i=0}^\infty f(n)$ diverges but the series for all function in $o(f)$ converge?,Is there a function $f:\mathbb{N} \to \mathbb{R}^+$ s.t. its series $\Sigma_{i=0}^\infty f(n)$ diverges but the series for all function in $o(f)$ converge?,,"['real-analysis', 'sequences-and-series', 'asymptotics', 'divergent-series']"
79,An inequality involving integrals,An inequality involving integrals,,"Let be $f:[0,1] \longrightarrow R $, $f$ is an integrable function such that: $$\int_{0}^{1} f(x) \space dx = \int_{0}^{1} xf(x) \space dx=1$$ I need to prove that: $$\int_{0}^{1} f^2(x) \space dx\geq4$$","Let be $f:[0,1] \longrightarrow R $, $f$ is an integrable function such that: $$\int_{0}^{1} f(x) \space dx = \int_{0}^{1} xf(x) \space dx=1$$ I need to prove that: $$\int_{0}^{1} f^2(x) \space dx\geq4$$",,"['real-analysis', 'analysis', 'integration']"
80,How to prove that a continuous function is identically zero over $\mathbb{R}$?,How to prove that a continuous function is identically zero over ?,\mathbb{R},"The following problem is given at the level of a senior undergrad analysis course: We are given a continuous function $g:\mathbb{R}\rightarrow \mathbb{R}$. Assume that $\mathbb{R}$ contains a countably infinite subset $G$ such that:$\int_{a}^{b}g(x)dx=0$ if $a$ and $b$ are not in $G$. Prove that $g$ is the zero function. My attempt: Without loss of generality, I assume that my function has only one positive part and one negative part over $\mathbb{R}$.  I broke my function $g(x)$ into two functions: $g_{+}(x)$ which is equal to $0$ when $g(x)$ is negative and equals $g(x)$ when $g(x)$ is positive. Similarly, I define $g_{-}(x)$ to be zero when $g(x)$ is positive and equals $g(x)$ when $g(x)$ is negative. Now, I call $A_{+}$ the subset of $\mathbb{R}$ where $g(x)$ is positive, and $A_{-}$ the subset of $\mathbb{R}$ where $g(x)$ is negative. Then, I am trying to prove that for any $a< b$ in $A_{+}$: $\int_{a}^{b}g_{+}(x)dx=0$ (This obvious if $a$ and $b$ are not in $G$, but the problem is when one or both of them is/are not in $G$) and since $g_{+}(x)\geq 0$ for any $x\in \left [ a,b \right ]$, then $g_{+}(x)=0 $ for all $x\in \left [ a,b \right ]$. The same thing applies to the negative part. So, I am stuck at this point. Can anyone tell how to move forward and solve the problem? Also, if anyone has an easier way to solve the problem, please share.","The following problem is given at the level of a senior undergrad analysis course: We are given a continuous function $g:\mathbb{R}\rightarrow \mathbb{R}$. Assume that $\mathbb{R}$ contains a countably infinite subset $G$ such that:$\int_{a}^{b}g(x)dx=0$ if $a$ and $b$ are not in $G$. Prove that $g$ is the zero function. My attempt: Without loss of generality, I assume that my function has only one positive part and one negative part over $\mathbb{R}$.  I broke my function $g(x)$ into two functions: $g_{+}(x)$ which is equal to $0$ when $g(x)$ is negative and equals $g(x)$ when $g(x)$ is positive. Similarly, I define $g_{-}(x)$ to be zero when $g(x)$ is positive and equals $g(x)$ when $g(x)$ is negative. Now, I call $A_{+}$ the subset of $\mathbb{R}$ where $g(x)$ is positive, and $A_{-}$ the subset of $\mathbb{R}$ where $g(x)$ is negative. Then, I am trying to prove that for any $a< b$ in $A_{+}$: $\int_{a}^{b}g_{+}(x)dx=0$ (This obvious if $a$ and $b$ are not in $G$, but the problem is when one or both of them is/are not in $G$) and since $g_{+}(x)\geq 0$ for any $x\in \left [ a,b \right ]$, then $g_{+}(x)=0 $ for all $x\in \left [ a,b \right ]$. The same thing applies to the negative part. So, I am stuck at this point. Can anyone tell how to move forward and solve the problem? Also, if anyone has an easier way to solve the problem, please share.",,"['calculus', 'real-analysis', 'analysis', 'measure-theory']"
81,Cauchy Criterion on the series $\frac{1}{n}$,Cauchy Criterion on the series,\frac{1}{n},"I have seen the statement a million times and I know it implies convergence but I've never actually seen how to use it. It seems much harder than proving a limit since there is an $m$ and an $n$. Can someone give an example, or even use this Criterion to show that $\frac{1}{n}$ is a convergent sequence? Suppose I have reduced the Cauchy Criterion inequality in a problem to $\frac{n}{m}$, $m\gt n$, and $\frac{n}{m} > \epsilon$. If I want to find a lower bound for $n$, can I choose anything I want that will satisfy the above inequalities?","I have seen the statement a million times and I know it implies convergence but I've never actually seen how to use it. It seems much harder than proving a limit since there is an $m$ and an $n$. Can someone give an example, or even use this Criterion to show that $\frac{1}{n}$ is a convergent sequence? Suppose I have reduced the Cauchy Criterion inequality in a problem to $\frac{n}{m}$, $m\gt n$, and $\frac{n}{m} > \epsilon$. If I want to find a lower bound for $n$, can I choose anything I want that will satisfy the above inequalities?",,"['real-analysis', 'sequences-and-series', 'analysis']"
82,Convergence of a kind of difference quotient,Convergence of a kind of difference quotient,,"Let $f:\mathbb{R}\to \mathbb{R}$ be a measurable function and $(h_j)_j$ be a sequence of nonzero real numbers converging to zero as $j\to \infty$. Is it true that for almost every $x\in \mathbb{R}$: $$\frac{1}{h_j}(\cos(f(x+h_j)-f(x))-1)\to 0\quad\mathrm{as}~~ j\to \infty~~?$$ It feels that the Lebesgue differentiation theorem or maybe some other measure-theoretic theorem must be used to prove it, if it is true at all.","Let $f:\mathbb{R}\to \mathbb{R}$ be a measurable function and $(h_j)_j$ be a sequence of nonzero real numbers converging to zero as $j\to \infty$. Is it true that for almost every $x\in \mathbb{R}$: $$\frac{1}{h_j}(\cos(f(x+h_j)-f(x))-1)\to 0\quad\mathrm{as}~~ j\to \infty~~?$$ It feels that the Lebesgue differentiation theorem or maybe some other measure-theoretic theorem must be used to prove it, if it is true at all.",,"['real-analysis', 'measure-theory']"
83,"Convergence of sequence of functions, $f_n(x) = n^2 x(1-nx) \dots $","Convergence of sequence of functions,",f_n(x) = n^2 x(1-nx) \dots ,"Doing an exercise for exam preparation, I stumbled across the following function: $f_n(x)=  n^2x(1-nx),  \quad \text{if }0 \leq x \leq \frac{1}{n} $ $f_n(x)=  0, \quad \text{if } \frac{1}{n} < x \leq 1$ The task is to find the limit of this function series and to determine whether this function converges uniformly in $[0,1]$ On the one hand $\frac{1}{n}$ approaches $0$ for $n \to \infty$. So one would just have to insert $0$ in $n^2x(1-nx)$. Thereby gaining $f_n(x) = 0$ for $n \to \infty$. On the other hand the function has a maximum for $x=\frac{n}{2}$. Putting this into $n^2x(1-nx)$ and calculating $f_n(x)$ for $n \to \infty$ afterwards one gets $f_n(x) = \infty$. So whats correct? How does one approach such a problem? Thanks in advance ftiaronsem","Doing an exercise for exam preparation, I stumbled across the following function: $f_n(x)=  n^2x(1-nx),  \quad \text{if }0 \leq x \leq \frac{1}{n} $ $f_n(x)=  0, \quad \text{if } \frac{1}{n} < x \leq 1$ The task is to find the limit of this function series and to determine whether this function converges uniformly in $[0,1]$ On the one hand $\frac{1}{n}$ approaches $0$ for $n \to \infty$. So one would just have to insert $0$ in $n^2x(1-nx)$. Thereby gaining $f_n(x) = 0$ for $n \to \infty$. On the other hand the function has a maximum for $x=\frac{n}{2}$. Putting this into $n^2x(1-nx)$ and calculating $f_n(x)$ for $n \to \infty$ afterwards one gets $f_n(x) = \infty$. So whats correct? How does one approach such a problem? Thanks in advance ftiaronsem",,"['real-analysis', 'analysis', 'sequences-and-series']"
84,Can a norm only be defined on vector spaces?,Can a norm only be defined on vector spaces?,,All examples I have come across so far deal with norm defined on a vector space. Can norm only be defined on vector spaces?,All examples I have come across so far deal with norm defined on a vector space. Can norm only be defined on vector spaces?,,[]
85,"With some conditions on $f$, resolve the claim $|f(x)-f(y)|\le|x-y|$ for all $x,y\in[0,1]$.","With some conditions on , resolve the claim  for all .","f |f(x)-f(y)|\le|x-y| x,y\in[0,1]","Here's the problem . . . Prove or disprove: $\;$ If $f:[0,1]\to\mathbb{R}$ is a twice differentiable function such that $f(0)=f(1)=0$ . $|f''(x)|\le 2$ for all $x\in[0,1]$ . then $|f(x)-f(y)|\le|x-y|$ for all $x,y\in[0,1]$ . Some context . . . This problem was posted yesterdayby @Alex (user 1258161) https://math.stackexchange.com/questions/4829557/ but was closed for lack of context, and then deleted by the post author. I found the problem interesting so I tried it just for fun. Considering the possibility that the claim might be false, I first tried to construct a counterexample. Not finding one, I tried to prove the claim, but so far without success. Perhaps I'm missing something simple, a commonly used approach or a standard trick. In any case, as shown below, I made some progress, and perhaps my partial results could be of some use in the quest for a complete solution. Attempting to prove the claim, here's what I tried . . . Assume the hypothesis and suppose $|f(u)-f(v)| > |u-v|$ for some $u,v\in[0,1]$ . Our goal is to derive a contradiction. We can assume $u < v$ , else replace $f(x)$ by $f(1-x)$ . $f(u) > f(v)$ , else replace $f(x)$ by $-f(x)$ . Thus we have $f(u)-f(v) > v-u$ . By the MVT, there exists $a\in(u,v)$ such that $f(u)-f(v)=f'(a)(u-v)$ , hence $f'(a) < -1$ . By the MVT, for arbitrary $x\in[0,1]{\setminus}\{a\}$ , there exists $b$ with $\min(x,a) <  b < \max(x,a)$ such that $f'(x)-f'(a)=f''(b)(x-a)$ , so $f'(x) < 1$ . Thus we have $f'(x) < 1$ for all $x\in[0,1]$ . Then by the MVT Since $f(0)=0$ , it follows that $f(x) < x$ for all $x\in(0,1]$ . Since $f(1)=0$ , it follows that $f(x) > x-1$ for all $x\in[0,1)$ . By Rolle's Theorem, there exists $c\in(0,1)$ such that $f'(c)=0$ . By the MVT, for arbitrary $x\in[0,1]{\setminus}\{c\}$ , there exists $d$ with $\min(x,c) < d < \max(x,c)$ such that $f'(x)-f'(c)=f''(d)(x-c)$ , so $f'(x) > -2$ . Thus we have $f'(x) > -2$ for all $x\in[0,1]$ . Then by the MVT Since $f(0)=0$ , it follows that $f(x) > -2x$ for all $x\in(0,1]$ . Since $f(1)=0$ , it follows that $f(x) < 2-2x$ for all $x\in[0,1)$ . Then we have $f(x) < \min(x,2-2x)$ for all $x\in(0,1)$ , hence $f(x) < 2/3$ for all $x\in[0,1]$ . $f(x) > \max(-2x,x-1)$ for all $x\in(0,1)$ , hence $f(x) > -2/3$ for all $x\in[0,1]$ . That's my progress so far. How to resolve the problem?","Here's the problem . . . Prove or disprove: If is a twice differentiable function such that . for all . then for all . Some context . . . This problem was posted yesterdayby @Alex (user 1258161) https://math.stackexchange.com/questions/4829557/ but was closed for lack of context, and then deleted by the post author. I found the problem interesting so I tried it just for fun. Considering the possibility that the claim might be false, I first tried to construct a counterexample. Not finding one, I tried to prove the claim, but so far without success. Perhaps I'm missing something simple, a commonly used approach or a standard trick. In any case, as shown below, I made some progress, and perhaps my partial results could be of some use in the quest for a complete solution. Attempting to prove the claim, here's what I tried . . . Assume the hypothesis and suppose for some . Our goal is to derive a contradiction. We can assume , else replace by . , else replace by . Thus we have . By the MVT, there exists such that , hence . By the MVT, for arbitrary , there exists with such that , so . Thus we have for all . Then by the MVT Since , it follows that for all . Since , it follows that for all . By Rolle's Theorem, there exists such that . By the MVT, for arbitrary , there exists with such that , so . Thus we have for all . Then by the MVT Since , it follows that for all . Since , it follows that for all . Then we have for all , hence for all . for all , hence for all . That's my progress so far. How to resolve the problem?","\; f:[0,1]\to\mathbb{R} f(0)=f(1)=0 |f''(x)|\le 2 x\in[0,1] |f(x)-f(y)|\le|x-y| x,y\in[0,1] |f(u)-f(v)| > |u-v| u,v\in[0,1] u < v f(x) f(1-x) f(u) > f(v) f(x) -f(x) f(u)-f(v) > v-u a\in(u,v) f(u)-f(v)=f'(a)(u-v) f'(a) < -1 x\in[0,1]{\setminus}\{a\} b \min(x,a) <  b < \max(x,a) f'(x)-f'(a)=f''(b)(x-a) f'(x) < 1 f'(x) < 1 x\in[0,1] f(0)=0 f(x) < x x\in(0,1] f(1)=0 f(x) > x-1 x\in[0,1) c\in(0,1) f'(c)=0 x\in[0,1]{\setminus}\{c\} d \min(x,c) < d < \max(x,c) f'(x)-f'(c)=f''(d)(x-c) f'(x) > -2 f'(x) > -2 x\in[0,1] f(0)=0 f(x) > -2x x\in(0,1] f(1)=0 f(x) < 2-2x x\in[0,1) f(x) < \min(x,2-2x) x\in(0,1) f(x) < 2/3 x\in[0,1] f(x) > \max(-2x,x-1) x\in(0,1) f(x) > -2/3 x\in[0,1]","['real-analysis', 'calculus', 'mean-value-theorem']"
86,Convergence analysis of a quotient of two sequences $x_{n+1}^2 = x_n^2 + \frac{c}{x_n^2}$.,Convergence analysis of a quotient of two sequences .,x_{n+1}^2 = x_n^2 + \frac{c}{x_n^2},"Given two recurrence relations $x_{n+1}^2 = x_n^2 + \frac{c_1^2}{x_n^2}$ and $y_{n+1}^2 = y_n^2 + \frac{c_2^2}{y_n^2}$ for $c_1, c_2\in \mathbb{R}^+$ and $x_1=y_1=c_3\in\mathbb{R}^+$ . We want to study the convergence properties of $r_n := \frac{y_n^2 c_1}{x_n^2 c_2}$ as $n\rightarrow \infty$ . I strongly suspect that $r_n$ converges to 1 as $n\rightarrow \infty$ but am unable to prove it yet. I was also able to show that 1 is its the only fixed point and an attractor in the sense that $r_n > 1 \Rightarrow r_{n+1} < r_n$ and $r_n < 1 \Rightarrow r_{n+1} > r_n$ . I was then able to prove that the $r_n$ does converge, as $r_n$ is strictly monotonous for $n\geq 2$ . I would like to show that $r_n$ cannot asymptotically near another limit than 1 or find a counter-example for it.","Given two recurrence relations and for and . We want to study the convergence properties of as . I strongly suspect that converges to 1 as but am unable to prove it yet. I was also able to show that 1 is its the only fixed point and an attractor in the sense that and . I was then able to prove that the does converge, as is strictly monotonous for . I would like to show that cannot asymptotically near another limit than 1 or find a counter-example for it.","x_{n+1}^2 = x_n^2 + \frac{c_1^2}{x_n^2} y_{n+1}^2 = y_n^2 + \frac{c_2^2}{y_n^2} c_1, c_2\in \mathbb{R}^+ x_1=y_1=c_3\in\mathbb{R}^+ r_n := \frac{y_n^2 c_1}{x_n^2 c_2} n\rightarrow \infty r_n n\rightarrow \infty r_n > 1 \Rightarrow r_{n+1} < r_n r_n < 1 \Rightarrow r_{n+1} > r_n r_n r_n n\geq 2 r_n","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'recurrence-relations']"
87,Does it imply that $\sum_{n=1}^{\infty}a_n$ converges absolutely?,Does it imply that  converges absolutely?,\sum_{n=1}^{\infty}a_n,"Let $(a_n)_{n=1}^{\infty}$ be a sequence of reals and $\lim\limits_{n\to\infty}a_n = 0$ . Suppose that there exists $M \in (0, 1)$ s.t for each $n \in \mathbb{N}$ : $|a_{n+2}-a_{n+1}| \leq M|a_{n+1}-a_{n}|$ . Does it imply that $\sum_{n=1}^{\infty}a_n$ converges absolutely? This questions follows my previous one: Is it necessary that $\sum{a_{2^k}}$ converges? . I struggle to either prove or find a counter-example to this argument and I'm not sure how to proceed (I could only find a counter-example for the case where $M=1$ ). I'd like to have some insights\guidance, thanks.","Let be a sequence of reals and . Suppose that there exists s.t for each : . Does it imply that converges absolutely? This questions follows my previous one: Is it necessary that $\sum{a_{2^k}}$ converges? . I struggle to either prove or find a counter-example to this argument and I'm not sure how to proceed (I could only find a counter-example for the case where ). I'd like to have some insights\guidance, thanks.","(a_n)_{n=1}^{\infty} \lim\limits_{n\to\infty}a_n = 0 M \in (0, 1) n \in \mathbb{N} |a_{n+2}-a_{n+1}| \leq M|a_{n+1}-a_{n}| \sum_{n=1}^{\infty}a_n M=1","['real-analysis', 'sequences-and-series']"
88,Does $\lim_{k\to\infty}\sum_{n\ge1}\left(\frac{n!}{n^n}\right)^k$ converges to 1?,Does  converges to 1?,\lim_{k\to\infty}\sum_{n\ge1}\left(\frac{n!}{n^n}\right)^k,"Context: I was randomly putting infinite sums of this form ( $k\in\mathbb N)$ in online calculators. $$\sum_{n\ge1}\left(\frac{n!}{n^n}\right)^k$$ I noticed that as $k$ increases, the sums becomes more and more closer to $1$ . Question: Can we prove if $$\lim_{k\to\infty}\sum_{n\ge1}\left(\frac{n!}{n^n}\right)^k = 1 $$ Thanks !","Context: I was randomly putting infinite sums of this form ( in online calculators. I noticed that as increases, the sums becomes more and more closer to . Question: Can we prove if Thanks !",k\in\mathbb N) \sum_{n\ge1}\left(\frac{n!}{n^n}\right)^k k 1 \lim_{k\to\infty}\sum_{n\ge1}\left(\frac{n!}{n^n}\right)^k = 1 ,"['real-analysis', 'sequences-and-series', 'limits', 'summation']"
89,How to prove: $\lim_{y\to\infty}\int_{1}^{\infty}\frac{f(x)}{x^2+y^2}dx=0 $,How to prove:,\lim_{y\to\infty}\int_{1}^{\infty}\frac{f(x)}{x^2+y^2}dx=0 ,"Let $f:\left[0,\infty\right]\to \left[0,\infty\right]$ be locally integrable. It is integrable on every compact subinterval $I \subset  \left[0,\infty\right]$ , and assume that the improper integral: $$\int_{1}^{\infty}\frac{f(x)}{x^2}\,dx$$ converges and is finite. Compute: $$\lim_{y\to\infty}\int_{1}^{\infty}\frac{f(x)}{x^2+y^2}dx $$ The limit exists because the integral exists by comparison test for each $y$ and is decreasing function of $y$ . I am sure that the answer is zero but I am not allowed to use the Dominated convergence theorem to solve this question, therefore I need either use $\epsilon$ $\delta$ definition or need to bound this integral and apply squeeze theorem somehow.","Let be locally integrable. It is integrable on every compact subinterval , and assume that the improper integral: converges and is finite. Compute: The limit exists because the integral exists by comparison test for each and is decreasing function of . I am sure that the answer is zero but I am not allowed to use the Dominated convergence theorem to solve this question, therefore I need either use definition or need to bound this integral and apply squeeze theorem somehow.","f:\left[0,\infty\right]\to \left[0,\infty\right] I \subset  \left[0,\infty\right] \int_{1}^{\infty}\frac{f(x)}{x^2}\,dx \lim_{y\to\infty}\int_{1}^{\infty}\frac{f(x)}{x^2+y^2}dx  y y \epsilon \delta","['real-analysis', 'limits', 'analysis', 'convergence-divergence', 'improper-integrals']"
90,Analytic definition implies geometric definition of trigonometric functions,Analytic definition implies geometric definition of trigonometric functions,,"It is well known how we can arrive to the power series definition of trigonometric functions starting from their definition in terms of the unit circle. I'm trying to do the converse, i.e. start from the definition of trigonometric functions by their power series and prove that we can parametrize the unit circle with them and that an angle of $\theta$ radians subtends an arc of length $\theta$ (in the unit circle, of course). Here is what I have done so far. Define $\displaystyle \pi :=2\int \limits _{-1}^1\sqrt{1-x^2}\,dx$ the area of the unit circle, and define $$C(x):=\sum \limits _{n=0}^\infty \frac{(-1)^n}{(2n)!}x^{2n}$$ and $$S(x):=\sum \limits _{n=0}^\infty \frac{(-1)^n}{(2n+1)!}x^{2n+1}.$$ The ratio test shows that this functions have infinite convergence radius, so they are in $\mathcal{C}^\infty (\mathbb{R})$ , and it is clear that $C$ is an even function and $S$ is an odd function. Differentiating gives $\dfrac{d}{dx}C(x)=-S(x)$ and $\dfrac{d}{dx}S(x)=C(x)$ . Now we prove that they verify the addition formulae for $\sin$ and $\cos$ , i.e. that $S(x+y)=S(x)C(y)+C(x)S(y)$ and $C(x+y)=C(x)C(y)-S(x)S(y)$ for all $x,y\in \mathbb{R}$ . Fix $y\in \mathbb{R}$ and define $$F(x):=S(x+y)-S(x)C(y)-C(x)S(y).$$ Then we have that $$F'(x)=C(x+y)-C(x)C(y)+S(x)S(y).$$ Now observe that $F''(x)=-F(x)$ , hence $$2F'(x)[F(x)+F''(x)]=0$$ and therefore $$\frac{d}{dx}\left [(F(x))^2+(F'(x))^2\right ]=0$$ for all $x\in \mathbb{R}$ , hence $F(x)=0$ for all $x\in \mathbb{R}$ and $F'(x)=0$ for all $x\in \mathbb{R}$ , this proves that $S(x+y)=S(x)C(y)+C(x)S(y)$ and $C(x+y)=C(x)C(y)-S(x)S(y)$ for all $x,y\in \mathbb{R}$ . Using that $C$ is even and $S$ is odd we find that $S(x\pm y)=S(x)C(y)\pm C(x)S(y)$ and that $C(x\pm y)=C(x)C(y)\mp S(x)S(y)$ , this also gives the Pythagorean Identity $$(C(x))^2+(S(x))^2=C(x-x)=C(0)=1$$ which implies that $C(x),S(x)\in [-1,1]$ for all $x\in \mathbb{R}$ . Now I want to prove that we can parametrize the unit circle using $C$ and $S$ . The Pythagorean Identity implies that $(C(x),S(x))$ is in the unit circle for all $x\in \mathbb{R}$ , but I don't know how to prove that every point in the unit circle has the form $(C(x_0),S(x_0))$ for some $x_0\in \mathbb{R}$ . We now prove that the perimeter of the unit circle is $2\pi$ . By symmetry, it is enough to prove that the perimeter of the upper half of the unit circle is $\pi$ . This curve can be parametrized as $\alpha (t):=\left (t,\sqrt{1-t^2}\right )$ for $t\in [-1,1]$ , hence $\alpha '(t)=\left (1,-\dfrac{t}{\sqrt{1-t^2}}\right )$ , and therefore $\|\alpha '(t)\|=\dfrac{1}{\sqrt{1-t^2}}$ . To prove that the perimeter of the upper half of the unit circle is $\pi$ , we have to prove that $$\int \limits _{-1}^1\frac{1}{\sqrt{1-t^2}}\,dt=2\int \limits _{-1}^1\sqrt{1-t^2}\,dt$$ i.e. we have to prove that $$\int \limits _{-1}^1\frac{1}{\sqrt{1-t^2}}-2\sqrt{1-t^2}\,dt=0.$$ But $$\frac{d}{dt}\left (-t\sqrt{1-t^2}\right )=\frac{1}{\sqrt{1-t^2}}-2\sqrt{1-t^2}$$ and therefore $$\int \limits _{-1}^1\frac{1}{\sqrt{1-t^2}}-2\sqrt{1-t^2}\,dt=\left .-t\sqrt{1-t^2}\right |_{-1}^1=0$$ as wanted. Now assuming that we can parametrize the unit circle using $C$ and $S$ , then we can show with line integration that the length of the portion of the unit circle that goes from $(1,0)$ to $(C(x),S(x))$ in counterclockwise sense is exactly $x$ . In particular, this would prove that $C$ and $S$ are periodic with period $2\pi$ and that $C(x)=\cos x$ and $S(x)=\sin x$ for $x\in [0,2\pi ]$ , and therefore over all of $\mathbb{R}$ . So my question here is: How can we prove that the functions $C$ and $S$ defined as the power series of (what then are going to be) the cosine and the sine functions can be used to parametrize the unit circle?","It is well known how we can arrive to the power series definition of trigonometric functions starting from their definition in terms of the unit circle. I'm trying to do the converse, i.e. start from the definition of trigonometric functions by their power series and prove that we can parametrize the unit circle with them and that an angle of radians subtends an arc of length (in the unit circle, of course). Here is what I have done so far. Define the area of the unit circle, and define and The ratio test shows that this functions have infinite convergence radius, so they are in , and it is clear that is an even function and is an odd function. Differentiating gives and . Now we prove that they verify the addition formulae for and , i.e. that and for all . Fix and define Then we have that Now observe that , hence and therefore for all , hence for all and for all , this proves that and for all . Using that is even and is odd we find that and that , this also gives the Pythagorean Identity which implies that for all . Now I want to prove that we can parametrize the unit circle using and . The Pythagorean Identity implies that is in the unit circle for all , but I don't know how to prove that every point in the unit circle has the form for some . We now prove that the perimeter of the unit circle is . By symmetry, it is enough to prove that the perimeter of the upper half of the unit circle is . This curve can be parametrized as for , hence , and therefore . To prove that the perimeter of the upper half of the unit circle is , we have to prove that i.e. we have to prove that But and therefore as wanted. Now assuming that we can parametrize the unit circle using and , then we can show with line integration that the length of the portion of the unit circle that goes from to in counterclockwise sense is exactly . In particular, this would prove that and are periodic with period and that and for , and therefore over all of . So my question here is: How can we prove that the functions and defined as the power series of (what then are going to be) the cosine and the sine functions can be used to parametrize the unit circle?","\theta \theta \displaystyle \pi :=2\int \limits _{-1}^1\sqrt{1-x^2}\,dx C(x):=\sum \limits _{n=0}^\infty \frac{(-1)^n}{(2n)!}x^{2n} S(x):=\sum \limits _{n=0}^\infty \frac{(-1)^n}{(2n+1)!}x^{2n+1}. \mathcal{C}^\infty (\mathbb{R}) C S \dfrac{d}{dx}C(x)=-S(x) \dfrac{d}{dx}S(x)=C(x) \sin \cos S(x+y)=S(x)C(y)+C(x)S(y) C(x+y)=C(x)C(y)-S(x)S(y) x,y\in \mathbb{R} y\in \mathbb{R} F(x):=S(x+y)-S(x)C(y)-C(x)S(y). F'(x)=C(x+y)-C(x)C(y)+S(x)S(y). F''(x)=-F(x) 2F'(x)[F(x)+F''(x)]=0 \frac{d}{dx}\left [(F(x))^2+(F'(x))^2\right ]=0 x\in \mathbb{R} F(x)=0 x\in \mathbb{R} F'(x)=0 x\in \mathbb{R} S(x+y)=S(x)C(y)+C(x)S(y) C(x+y)=C(x)C(y)-S(x)S(y) x,y\in \mathbb{R} C S S(x\pm y)=S(x)C(y)\pm C(x)S(y) C(x\pm y)=C(x)C(y)\mp S(x)S(y) (C(x))^2+(S(x))^2=C(x-x)=C(0)=1 C(x),S(x)\in [-1,1] x\in \mathbb{R} C S (C(x),S(x)) x\in \mathbb{R} (C(x_0),S(x_0)) x_0\in \mathbb{R} 2\pi \pi \alpha (t):=\left (t,\sqrt{1-t^2}\right ) t\in [-1,1] \alpha '(t)=\left (1,-\dfrac{t}{\sqrt{1-t^2}}\right ) \|\alpha '(t)\|=\dfrac{1}{\sqrt{1-t^2}} \pi \int \limits _{-1}^1\frac{1}{\sqrt{1-t^2}}\,dt=2\int \limits _{-1}^1\sqrt{1-t^2}\,dt \int \limits _{-1}^1\frac{1}{\sqrt{1-t^2}}-2\sqrt{1-t^2}\,dt=0. \frac{d}{dt}\left (-t\sqrt{1-t^2}\right )=\frac{1}{\sqrt{1-t^2}}-2\sqrt{1-t^2} \int \limits _{-1}^1\frac{1}{\sqrt{1-t^2}}-2\sqrt{1-t^2}\,dt=\left .-t\sqrt{1-t^2}\right |_{-1}^1=0 C S (1,0) (C(x),S(x)) x C S 2\pi C(x)=\cos x S(x)=\sin x x\in [0,2\pi ] \mathbb{R} C S","['real-analysis', 'geometry', 'trigonometry', 'power-series']"
91,In an attempt to find $I = \int_0^\infty \frac{t}{e^t-1}dt$,In an attempt to find,I = \int_0^\infty \frac{t}{e^t-1}dt,"I was trying to solve $I = \int_0^\infty \frac{t}{e^t-1}dt$ My approach I took the more general form of integral $f(s) = \int_0^{\infty}\frac{e^{-st}}{e^t-1}dt$ the same way as How to evaluate the integral $I = \int_o^{\infty} \frac{x}{\sqrt{e^{2\pi\sqrt{x}}-1}}dx$? So, my answer $$I = -\left[\frac{\partial f(s)}{\partial s}\right]_{s =0}$$ but that simply doesn't work. However, $\int_0^\infty \frac{t}{e^t-1}dt$ without using series doesn't answer my question as I'm interested in finding why finding derivative at $s = 0$ doesn't work Also, I'm interested in finding a number of different ways I can solve this problem","I was trying to solve My approach I took the more general form of integral the same way as How to evaluate the integral $I = \int_o^{\infty} \frac{x}{\sqrt{e^{2\pi\sqrt{x}}-1}}dx$? So, my answer but that simply doesn't work. However, without using series doesn't answer my question as I'm interested in finding why finding derivative at doesn't work Also, I'm interested in finding a number of different ways I can solve this problem",I = \int_0^\infty \frac{t}{e^t-1}dt f(s) = \int_0^{\infty}\frac{e^{-st}}{e^t-1}dt I = -\left[\frac{\partial f(s)}{\partial s}\right]_{s =0} \int_0^\infty \frac{t}{e^t-1}dt s = 0,"['real-analysis', 'calculus']"
92,Prove $\int_0^1 \ln(x+1) \ln(x)dx=2-2\ln(2)-\frac{\pi^2}{12}$ [duplicate],Prove  [duplicate],\int_0^1 \ln(x+1) \ln(x)dx=2-2\ln(2)-\frac{\pi^2}{12},"This question already has answers here : Evaluate $ \int_{0}^{1} \ln(x)\ln(1-x)\,dx $ (9 answers) Closed 2 years ago . Prove that $$I:=\int_0^1 \ln(x+1) \ln(x)dx=2-2\ln(2)-\frac{\pi^2}{12}$$ I've tried integration by parts and u substitution and cannot make it work but I have one potential path I use $$\ln(x+1)=\sum^{\infty}_{n=1} \frac{(-1)^{n-1}x^n}{n} $$ $$\Rightarrow I=\int_0^1 \ln(x)\sum^{\infty}_{n=1} \frac{(-1)^{n-1}x^n}{n}dx=\sum^{\infty}_{n=1} \frac{(-1)^{n-1}}{n} \int_0^1 x^{n-1} \ln(x)dx=  $$ How does one proceed from here? Thank you for your time","This question already has answers here : Evaluate $ \int_{0}^{1} \ln(x)\ln(1-x)\,dx $ (9 answers) Closed 2 years ago . Prove that I've tried integration by parts and u substitution and cannot make it work but I have one potential path I use How does one proceed from here? Thank you for your time","I:=\int_0^1 \ln(x+1) \ln(x)dx=2-2\ln(2)-\frac{\pi^2}{12} \ln(x+1)=\sum^{\infty}_{n=1} \frac{(-1)^{n-1}x^n}{n}
 \Rightarrow I=\int_0^1 \ln(x)\sum^{\infty}_{n=1} \frac{(-1)^{n-1}x^n}{n}dx=\sum^{\infty}_{n=1} \frac{(-1)^{n-1}}{n} \int_0^1 x^{n-1} \ln(x)dx= 
","['real-analysis', 'calculus', 'integration']"
93,Find $\limsup\limits_{x\rightarrow\infty}\ (\sin(x)+\sin(\pi x))$,Find,\limsup\limits_{x\rightarrow\infty}\ (\sin(x)+\sin(\pi x)),I'm studying for my Qualifying exams and this was one of the questions in the question bank under real analysis section. I'm currently stuck on this question. I think the answer is 2 but don't have a rigorous proof. Find $\limsup\limits_{x\rightarrow\infty}\ (\sin(x)+\sin(\pi x))$ . My attempt: I tried to look at the sequence $x_n=\frac{1}{2}+2n$ but not sure how to calculate further and find the $\limsup$,I'm studying for my Qualifying exams and this was one of the questions in the question bank under real analysis section. I'm currently stuck on this question. I think the answer is 2 but don't have a rigorous proof. Find . My attempt: I tried to look at the sequence but not sure how to calculate further and find the,\limsup\limits_{x\rightarrow\infty}\ (\sin(x)+\sin(\pi x)) x_n=\frac{1}{2}+2n \limsup,"['real-analysis', 'sequences-and-series', 'limsup-and-liminf']"
94,Why has there been no unification of topology axioms and measure theory axioms?,Why has there been no unification of topology axioms and measure theory axioms?,,"Related thread here . The axioms of a topological space and a measure space at the outset seem very similar. They differ in the closure axioms of unions and intersections. The uncanny resemblance between a metric and a measure makes me wonder as to why these axioms have been defined separately. Couldn't they develop a theory with just the concept of a measure and a measure space? The one issue I see is that it might create circular logic. If we need topological space axioms to develop concepts in measure theory, that is a reason why we'd need to separate the two concepts. Closure of arbitrary unions versus countable unions, and finite intersections versus countable intersections, is not something I'd like to see as the only difference between the two concepts. Why have two separate systems when they are, at least from the outset, very similar concepts?","Related thread here . The axioms of a topological space and a measure space at the outset seem very similar. They differ in the closure axioms of unions and intersections. The uncanny resemblance between a metric and a measure makes me wonder as to why these axioms have been defined separately. Couldn't they develop a theory with just the concept of a measure and a measure space? The one issue I see is that it might create circular logic. If we need topological space axioms to develop concepts in measure theory, that is a reason why we'd need to separate the two concepts. Closure of arbitrary unions versus countable unions, and finite intersections versus countable intersections, is not something I'd like to see as the only difference between the two concepts. Why have two separate systems when they are, at least from the outset, very similar concepts?",,"['real-analysis', 'general-topology', 'measure-theory', 'soft-question']"
95,"UC Berkeley Integral Problem: Show that $\int_0^{2\pi} \frac{\min(\sin x, \cos x)}{\max(e^{\sin x},e^{\cos x})}\ {\rm d}x = -4\sinh(1/{\sqrt2})$.",UC Berkeley Integral Problem: Show that .,"\int_0^{2\pi} \frac{\min(\sin x, \cos x)}{\max(e^{\sin x},e^{\cos x})}\ {\rm d}x = -4\sinh(1/{\sqrt2})","Show that $$\int_0^{2\pi} \frac{\mathrm{min}(\sin{x},\, \cos{x})}{\mathrm{max}\left(e^{\sin{x}},\, e^{\cos{x}}\right)}\ \mathrm{d}x = -4\sinh\left(\frac{1}{\sqrt{2}}\right).$$ this problem comes from the 2020 UC Berkeley Integration Bee and was not solved by either of the contestants. Any hints? My initial approach was to compute the maximum and minimum of the specified function by observing the graph for $x\in (0, 2\pi)$ but could not get very far. Thank you!",Show that this problem comes from the 2020 UC Berkeley Integration Bee and was not solved by either of the contestants. Any hints? My initial approach was to compute the maximum and minimum of the specified function by observing the graph for but could not get very far. Thank you!,"\int_0^{2\pi} \frac{\mathrm{min}(\sin{x},\, \cos{x})}{\mathrm{max}\left(e^{\sin{x}},\, e^{\cos{x}}\right)}\ \mathrm{d}x = -4\sinh\left(\frac{1}{\sqrt{2}}\right). x\in (0, 2\pi)","['real-analysis', 'integration', 'definite-integrals', 'contest-math', 'maxima-minima']"
96,How to compute $\sum_{n=1}^\infty\frac{(-1)^nH_n}{n^4}$ by real integration only?,How to compute  by real integration only?,\sum_{n=1}^\infty\frac{(-1)^nH_n}{n^4},"How to prove, by real methods that $$\sum_{n=1}^\infty\frac{(-1)^nH_n}{n^4}=\frac12\zeta(2)\zeta(3)-\frac{59}{32}\zeta(5)$$ where $H_n$ is the harmonic number and $\zeta$ is the Riemann zeta function. This alternating Euler sum was already evaluated by M.N.C.E here using complex analysis and also by Cornel using series manipulation. My question here is can we do it by integration only? The integral representation of the sum is $\ \frac16\int_0^1\frac{\ln^3x\ln(1+x)}{x(1+x)}dx$ . Thanks.","How to prove, by real methods that where is the harmonic number and is the Riemann zeta function. This alternating Euler sum was already evaluated by M.N.C.E here using complex analysis and also by Cornel using series manipulation. My question here is can we do it by integration only? The integral representation of the sum is . Thanks.",\sum_{n=1}^\infty\frac{(-1)^nH_n}{n^4}=\frac12\zeta(2)\zeta(3)-\frac{59}{32}\zeta(5) H_n \zeta \ \frac16\int_0^1\frac{\ln^3x\ln(1+x)}{x(1+x)}dx,"['real-analysis', 'integration', 'sequences-and-series', 'alternative-proof', 'harmonic-numbers']"
97,Every Cover of a Compact Real Interval by Open Intervals Has a Finite Subcover where only Consecutive Sets Overlap?,Every Cover of a Compact Real Interval by Open Intervals Has a Finite Subcover where only Consecutive Sets Overlap?,,"Intuitively this seems true and would be a useful lemma in proving the fundamental theorem of calculus without assuming continuity of the derivative, i.e. that if $f$ is differentiable on $[a, b$ and the derivative is Riemann integrable then $\int_a^b f' = f(b) - f(a)$ . But is it true and if so is there any standard terminology for such a cover ? My attempt at a proof follows. Let $I = [a, b]$ be a closed bounded interval in $\mathbb R$ . Then (Hiene-Borel theorem) every open cover of open intervals has a finite sub-cover. We can assume that the open intervals have distinct R-endpoints, as for any two with the same R-endpoint one must be contained within the other and the smaller one (or either if the same L-endpoint) can be discarded without affecting the cover. Then these open intervals $\{O_i\ = (a_i, b_i)\}_{i = 1, n}$ can be ordered by their R-endpoints $\{b_i\}_{i = 1, n}$ in a strictly ascending sequence $b_1 < b_2, ...< b_n$ . Claim : from such a set $\{O_i = (a_i, b_i)\}_{i = 1, n}$ we can select a subset which covers $[a, b]$ , renumbered as $\{O'_j = (a'_i, b'_1)\}_{j = 1, m}$ in strictly ascending sequence $b'_1 < b'_2, ...< b'_m$ such that for $|i - j| > 1$ then $O'_i \cap O'_j = \emptyset$ and for $|i - j| = 1$ then $O'_i \cap O'_j \not= \emptyset$ . Construction: Choose $O'_1$ from intervals $O_i$ having $a \in O_i$ and maximizing $b_i$ among such intervals. Iteratively, stop if $b$ is in the last chosen interval $O'_j$ , otherwise, ... Chose $O'_{j+1}$ from intervals $O_i$ having $b'_j \in O_i$ and maximizing $b_i$ among such intervals. Then the set $\{O'_j\}  $ fulfills the requirements. Proof: Since the set $\{O_i\}  $ covers $[a, b]$ then for $a$ in step1 and for every $b'_j$ in the iteration there is an $O_i$ which contains it and since the endpoints are unique there is exactly one which maximizes $b_j$ . Since the $O_i$ chosen as $O'_{j+1}$ contains $b'_j$ and is open then it has a non-empty intersection with $O'_j$ . I.e. consecutive open intervals intersect. The $O_i$ chosen as $O'_{j+1}$ cannot intersect any interval prior to $O'_j$ as this would require it to have been chosen previously in order to maximize $b_j$ .","Intuitively this seems true and would be a useful lemma in proving the fundamental theorem of calculus without assuming continuity of the derivative, i.e. that if is differentiable on and the derivative is Riemann integrable then . But is it true and if so is there any standard terminology for such a cover ? My attempt at a proof follows. Let be a closed bounded interval in . Then (Hiene-Borel theorem) every open cover of open intervals has a finite sub-cover. We can assume that the open intervals have distinct R-endpoints, as for any two with the same R-endpoint one must be contained within the other and the smaller one (or either if the same L-endpoint) can be discarded without affecting the cover. Then these open intervals can be ordered by their R-endpoints in a strictly ascending sequence . Claim : from such a set we can select a subset which covers , renumbered as in strictly ascending sequence such that for then and for then . Construction: Choose from intervals having and maximizing among such intervals. Iteratively, stop if is in the last chosen interval , otherwise, ... Chose from intervals having and maximizing among such intervals. Then the set fulfills the requirements. Proof: Since the set covers then for in step1 and for every in the iteration there is an which contains it and since the endpoints are unique there is exactly one which maximizes . Since the chosen as contains and is open then it has a non-empty intersection with . I.e. consecutive open intervals intersect. The chosen as cannot intersect any interval prior to as this would require it to have been chosen previously in order to maximize .","f [a, b \int_a^b f' = f(b) - f(a) I = [a, b] \mathbb R \{O_i\ = (a_i, b_i)\}_{i = 1, n} \{b_i\}_{i = 1, n} b_1 < b_2, ...< b_n \{O_i = (a_i, b_i)\}_{i = 1, n} [a, b] \{O'_j = (a'_i, b'_1)\}_{j = 1, m} b'_1 < b'_2, ...< b'_m |i - j| > 1 O'_i \cap O'_j = \emptyset |i - j| = 1 O'_i \cap O'_j \not= \emptyset O'_1 O_i a \in O_i b_i b O'_j O'_{j+1} O_i b'_j \in O_i b_i \{O'_j\}   \{O_i\}   [a, b] a b'_j O_i b_j O_i O'_{j+1} b'_j O'_j O_i O'_{j+1} O'_j b_j","['real-analysis', 'general-topology', 'proof-verification']"
98,Is an infinite set with no limit point unbounded in an arbitrary metric space?,Is an infinite set with no limit point unbounded in an arbitrary metric space?,,"Given an infinite set $X$ with no limit points, is $X$ unbounded? (In an arbitrary metric space) I only know how to do this in $\mathbb{R}^k$ . Since $X$ has no limit points, $X$ is closed. An infinite set with no limit point also cannot be compact, because we can choose a ball around each point of $X$ with no other points in it, and such cover has no finite subcover. Then in $\mathbb{R}^k$ we have $\text{closed} \land \text{bounded} \implies \text{compact}$ (Heine-Borel theorem), so by simple logic knowing it is not compact we have: $$\neg\text{compact} \implies \left( \neg\text{closed} \lor \neg\text{bounded} \right),$$ and since it is closed by definition, it must be unbounded. The question is, how can I show this in an arbitrary metric space, and not just in $\mathbb{R}^k$ ? Just to clarify, I'm asking this because of my own curiosity, I only stumbled upon this in relation to trying to solve something else (in $\mathbb{R}^k$ ) and was wondering if it works in general.","Given an infinite set with no limit points, is unbounded? (In an arbitrary metric space) I only know how to do this in . Since has no limit points, is closed. An infinite set with no limit point also cannot be compact, because we can choose a ball around each point of with no other points in it, and such cover has no finite subcover. Then in we have (Heine-Borel theorem), so by simple logic knowing it is not compact we have: and since it is closed by definition, it must be unbounded. The question is, how can I show this in an arbitrary metric space, and not just in ? Just to clarify, I'm asking this because of my own curiosity, I only stumbled upon this in relation to trying to solve something else (in ) and was wondering if it works in general.","X X \mathbb{R}^k X X X \mathbb{R}^k \text{closed} \land \text{bounded} \implies \text{compact} \neg\text{compact} \implies \left( \neg\text{closed} \lor \neg\text{bounded} \right), \mathbb{R}^k \mathbb{R}^k","['real-analysis', 'general-topology']"
99,Prove $\sum_{k=1}^n \frac{(-1)^{k-1}}{k }\binom{n}{k}= H_n$ without the Beta function,Prove  without the Beta function,\sum_{k=1}^n \frac{(-1)^{k-1}}{k }\binom{n}{k}= H_n,"I know how to prove $$\sum_{k=1}^n \frac{(-1)^{k-1}}{k }\binom{n}{k}= H_n$$ by tackling it with the beta function. I was actually wondering if there is a proof of this fact without using the property of the Beta function $$B(x,y) = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}$$",I know how to prove by tackling it with the beta function. I was actually wondering if there is a proof of this fact without using the property of the Beta function,"\sum_{k=1}^n \frac{(-1)^{k-1}}{k }\binom{n}{k}= H_n B(x,y) = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}","['calculus', 'real-analysis', 'sequences-and-series', 'binomial-coefficients']"
