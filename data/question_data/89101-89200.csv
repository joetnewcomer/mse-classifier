,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Kernel of a bilinear form - Structural Mechanics,Kernel of a bilinear form - Structural Mechanics,,"If I have a bilinear form $$a : (H^1(\Omega))^3 \times (H^1(\Omega))^3 \mapsto \mathbb{R} \hspace{0.9in} a(\vec{u}, \vec{v}) = \int_{\Omega}{(D\vec{u})^T C (D\vec{v})\, d\Omega} $$ I would like to find the kernel of this bilinear form when $u \in V$ such that, $$V = \{ (u_x, u_y, u_z) \in (H^1(\Omega))^3 | u_x(0,0,0) = u_y(0,0,0) = u_z(0,0,0) = 0 \}$$ I would like to comment what  are C and D operators. D is Symmetric derivative operator. $$D\vec{u} = \begin{bmatrix} \frac{\partial u_x}{\partial x} \\ \frac{\partial u_y}{\partial y} \\ \frac{\partial u_z}{\partial z} \\ \frac{1}{2}( \frac{\partial u_x}{\partial y} + \frac{\partial u_y}{\partial x}) \\ \frac{1}{2}( \frac{\partial u_y}{\partial z} + \frac{\partial u_z}{\partial y}) \\ \frac{1}{2}( \frac{\partial u_x}{\partial z} + \frac{\partial u_z}{\partial x})\end{bmatrix} : \Omega \mapsto \mathbb{R}^6$$ C is fourth order isotropic elasticity tensor in voigt notation. http://web.mit.edu/16.20/homepage/3_Constitutive/Constitutive_files/module_3_with_solutions.pdf pg no - 18","If I have a bilinear form $$a : (H^1(\Omega))^3 \times (H^1(\Omega))^3 \mapsto \mathbb{R} \hspace{0.9in} a(\vec{u}, \vec{v}) = \int_{\Omega}{(D\vec{u})^T C (D\vec{v})\, d\Omega} $$ I would like to find the kernel of this bilinear form when $u \in V$ such that, $$V = \{ (u_x, u_y, u_z) \in (H^1(\Omega))^3 | u_x(0,0,0) = u_y(0,0,0) = u_z(0,0,0) = 0 \}$$ I would like to comment what  are C and D operators. D is Symmetric derivative operator. $$D\vec{u} = \begin{bmatrix} \frac{\partial u_x}{\partial x} \\ \frac{\partial u_y}{\partial y} \\ \frac{\partial u_z}{\partial z} \\ \frac{1}{2}( \frac{\partial u_x}{\partial y} + \frac{\partial u_y}{\partial x}) \\ \frac{1}{2}( \frac{\partial u_y}{\partial z} + \frac{\partial u_z}{\partial y}) \\ \frac{1}{2}( \frac{\partial u_x}{\partial z} + \frac{\partial u_z}{\partial x})\end{bmatrix} : \Omega \mapsto \mathbb{R}^6$$ C is fourth order isotropic elasticity tensor in voigt notation. http://web.mit.edu/16.20/homepage/3_Constitutive/Constitutive_files/module_3_with_solutions.pdf pg no - 18",,"['functional-analysis', 'bilinear-form']"
1,Reconstruction after Unitary Operator on Paley-Wiener space,Reconstruction after Unitary Operator on Paley-Wiener space,,"Background: According to the Sampling Theorem for Reproducing Kernel Hilbert Spaces (RKHS), for any RKHS $H_{rk}$ with reproducing kernel $k(\cdot, \cdot)$, if I can find a set of points $\{t_{n}\}_{n \in \mathbb{Z}} \in \mathbb{R}$ such that $\{k(\cdot, t_{n}) \}_{n \in \mathbb{Z}}$ is an orthogonal basis for the space, I can reproduce any $g \in H_{rk}$ via kernel interpolation at $\{t_{n}\}_{n \in \mathbb{Z}}$. So now suppose I have a function $f \in PW_{\pi}$, where $PW_{\pi}$ is the Paley-Weiner space with Fourier support $[-\pi, \pi]$. I then apply a unitary operator $U$ to the function and get $g = Uf$. It is well known that $PW_{\pi}$ is a RKHS, with $\{sinc_{\pi}(t-n)\}_{n \in \mathbb{Z}}$ as both the orthogonal basis and the reproducing kernel. My Questions: 1 .Is $UPW_{\pi}$ an RKHS? 2 .If it is, is it possible to reconstruct $f$ via interpolation from the samples of $g$? If so, can we find the samples of $f$ that are required to reconstruct $f$ from the samples of $g$? My Answers: Yes. My intuition is that since open sets on $L^{2}[\mathbb{R}]$ are unit circles, and since $U$ is unitary, it is just rotating these balls, which means $UPW_{\pi} = PW_{\pi}$. I am not sure how to prove this though, or if this is even the way to do so. Yes. All we have to do is interpolate $g$ and then map it back to $f$ via $f = U^{-1}g$. So if we want to find a sample of $f$, we first find all the needed samples of $g$, interpolate $g$ via the sampling theorem, and then resample $U^{-1}g$ at the points $\{U^{-1}t_{n}\}_{n \in \mathbb{Z}}$. These points $\{U^{-1}t_{n}\}_{n \in \mathbb{Z}}$ will be the corresponding sampling points of $f$. Any idea on how to start proving this? Apologies: My functional analysis is very weak. I am just trying to piece things together as I go, so I may be missing some fundamental concepts or parts here. If anybody has any suggestions on things to study to help me start to understand how to prove this, I would be grateful.","Background: According to the Sampling Theorem for Reproducing Kernel Hilbert Spaces (RKHS), for any RKHS $H_{rk}$ with reproducing kernel $k(\cdot, \cdot)$, if I can find a set of points $\{t_{n}\}_{n \in \mathbb{Z}} \in \mathbb{R}$ such that $\{k(\cdot, t_{n}) \}_{n \in \mathbb{Z}}$ is an orthogonal basis for the space, I can reproduce any $g \in H_{rk}$ via kernel interpolation at $\{t_{n}\}_{n \in \mathbb{Z}}$. So now suppose I have a function $f \in PW_{\pi}$, where $PW_{\pi}$ is the Paley-Weiner space with Fourier support $[-\pi, \pi]$. I then apply a unitary operator $U$ to the function and get $g = Uf$. It is well known that $PW_{\pi}$ is a RKHS, with $\{sinc_{\pi}(t-n)\}_{n \in \mathbb{Z}}$ as both the orthogonal basis and the reproducing kernel. My Questions: 1 .Is $UPW_{\pi}$ an RKHS? 2 .If it is, is it possible to reconstruct $f$ via interpolation from the samples of $g$? If so, can we find the samples of $f$ that are required to reconstruct $f$ from the samples of $g$? My Answers: Yes. My intuition is that since open sets on $L^{2}[\mathbb{R}]$ are unit circles, and since $U$ is unitary, it is just rotating these balls, which means $UPW_{\pi} = PW_{\pi}$. I am not sure how to prove this though, or if this is even the way to do so. Yes. All we have to do is interpolate $g$ and then map it back to $f$ via $f = U^{-1}g$. So if we want to find a sample of $f$, we first find all the needed samples of $g$, interpolate $g$ via the sampling theorem, and then resample $U^{-1}g$ at the points $\{U^{-1}t_{n}\}_{n \in \mathbb{Z}}$. These points $\{U^{-1}t_{n}\}_{n \in \mathbb{Z}}$ will be the corresponding sampling points of $f$. Any idea on how to start proving this? Apologies: My functional analysis is very weak. I am just trying to piece things together as I go, so I may be missing some fundamental concepts or parts here. If anybody has any suggestions on things to study to help me start to understand how to prove this, I would be grateful.",,"['functional-analysis', 'operator-theory', 'sampling-theory']"
2,"Is $\sum_{m\in \mathbb Z} f(x-m)f(x-n) \in L^2(a,b)$ if $f\in L^2(\mathbb R)$?",Is  if ?,"\sum_{m\in \mathbb Z} f(x-m)f(x-n) \in L^2(a,b) f\in L^2(\mathbb R)","Let $f\in L^2(\mathbb R)$ and $0<a<b< \infty,$ put $A= \{x\in \mathbb R: a<|x|<b\}.$ Fix $n\in \mathbb Z.$ Define  $$ F_n(x)=\sum_{m\in \mathbb Z} f(x-n-m)f(x-m)$$ We note that $F_n$ is a periodic function with period 1.  And hence we cannot expect $F_n$ belongs to $L^2(\mathbb R).$ But can we expect  $F_n\in L^2(A)$? I'm also curios to know what happens of summation over $n$? Specifically, define  $$G(x)= \sum_{n\in \mathbb Z} F_n(x).$$ Can we expect    $G\in L^2(A)$?","Let $f\in L^2(\mathbb R)$ and $0<a<b< \infty,$ put $A= \{x\in \mathbb R: a<|x|<b\}.$ Fix $n\in \mathbb Z.$ Define  $$ F_n(x)=\sum_{m\in \mathbb Z} f(x-n-m)f(x-m)$$ We note that $F_n$ is a periodic function with period 1.  And hence we cannot expect $F_n$ belongs to $L^2(\mathbb R).$ But can we expect  $F_n\in L^2(A)$? I'm also curios to know what happens of summation over $n$? Specifically, define  $$G(x)= \sum_{n\in \mathbb Z} F_n(x).$$ Can we expect    $G\in L^2(A)$?",,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'examples-counterexamples', 'lp-spaces']"
3,Real content of Chebyshev sum inequality,Real content of Chebyshev sum inequality,,"There's well-known Chebyshev's sum inequality . I'm looking for possible generalization of this statement; problem is that I'm not sure what is the right direction, but here's one which I'd prefer. Let $A$ be some $\Bbb R$ - or $\Bbb C$ -algebra with seminorm $|\cdot|$ and ""positive cone"" $P$ . $T$ is some operator (or parametric family $T_{\alpha}$ ) on $A$ satisfying several unknown properties ; maybe, it looks like a shift operator on $l^1(\Bbb Z)$ or parametric shift on $l^1(R)$ . Template of theorem (Generalized Chebyshev's sum inequality) If $a, b \in A$ are elements such that $(T-1)a$ and $(T-1)b$ are positive, then $|ab| \geq |a||b|$ If $x, y \in A$ are elements such that $(T-1)x$ and $(1-T)y$ are positive, then $|xy| \leq |x||y|$ Original inequality is obtained by taking $A = l^1(\Bbb Z), T$ being equal to shift, $P$ usual positive cone in $C^*$ sense and seminorm being stadard or $|x|_n := |x \cdot \chi_{[0, n]}|$ . With obvious tweaks one can adjust this to integral version on something like $L^1(\Bbb R)$ with limiting positivity condition for small forward difference operators. Question 1 . What's so special about shift operator? Question 2 . Is there some statement fitting the template and where $T$ does not look like a shift at all?","There's well-known Chebyshev's sum inequality . I'm looking for possible generalization of this statement; problem is that I'm not sure what is the right direction, but here's one which I'd prefer. Let be some - or -algebra with seminorm and ""positive cone"" . is some operator (or parametric family ) on satisfying several unknown properties ; maybe, it looks like a shift operator on or parametric shift on . Template of theorem (Generalized Chebyshev's sum inequality) If are elements such that and are positive, then If are elements such that and are positive, then Original inequality is obtained by taking being equal to shift, usual positive cone in sense and seminorm being stadard or . With obvious tweaks one can adjust this to integral version on something like with limiting positivity condition for small forward difference operators. Question 1 . What's so special about shift operator? Question 2 . Is there some statement fitting the template and where does not look like a shift at all?","A \Bbb R \Bbb C |\cdot| P T T_{\alpha} A l^1(\Bbb Z) l^1(R) a, b \in A (T-1)a (T-1)b |ab| \geq |a||b| x, y \in A (T-1)x (1-T)y |xy| \leq |x||y| A = l^1(\Bbb Z), T P C^* |x|_n := |x \cdot \chi_{[0, n]}| L^1(\Bbb R) T","['functional-analysis', 'inequality', 'operator-theory', 'normed-spaces', 'c-star-algebras']"
4,Finding eigenfunctions for Sturm-Liouville problems.,Finding eigenfunctions for Sturm-Liouville problems.,,Many PDE textbooks contain theoretical results regarding existence of eigenvalues and eigenfunctions for Sturm-Liouville problems but I haven't seen any that actually tell how to compute either making the theory seem not very useful. I was wondering how is can you find the eigenfunctions?,Many PDE textbooks contain theoretical results regarding existence of eigenvalues and eigenfunctions for Sturm-Liouville problems but I haven't seen any that actually tell how to compute either making the theory seem not very useful. I was wondering how is can you find the eigenfunctions?,,"['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
5,Approximating Sobolev functions on unbounded domains of class $C^1$ by compactly supported smooth functions,Approximating Sobolev functions on unbounded domains of class  by compactly supported smooth functions,C^1,"In the lecture we saw the following statement Corollary : Let $\Omega$ be open and of class $C^1$, $1 \leq p < \infty$ and $u \in W^{1,p}(\Omega)$. Then there exists a sequence $(u_k)_{k \in \mathbb{N}} \subset C^{\infty}_c(\mathbb{R}^n)$ with $||u_k|_{\Omega}-u||_{W^{1,p}(\Omega)} \rightarrow 0, k \rightarrow \infty$ The proof went as follows: First we set $\delta>0$ and constructed a $\overline{u} \in W^{1,p}(\Omega)$ with $supp(\overline{u}) \subset\subset \mathbb{R}^n$, i.e. compact support in $\mathbb{R}^n$, with $||\overline{u}-u||_{W^{1,p}(\Omega)} < \delta$. Then the proof said to use a extension operator for Sobolev functions, which can be constructed, since for the boundary $\Gamma$ of $\Omega$ it holds that $\Gamma \cap supp(\overline{u})$ is compact. This was the part I did not understand . For reference our lecturer used the following notes . The above stated corollary is Korollar 8.4.2. Unfortunatly the notes are written in german, but since the notes are mainly based on Brezi's book on functional analysis, the same statement can be found in Corollary 9.8 in Brezi's book. Thanks a lot in advance!","In the lecture we saw the following statement Corollary : Let $\Omega$ be open and of class $C^1$, $1 \leq p < \infty$ and $u \in W^{1,p}(\Omega)$. Then there exists a sequence $(u_k)_{k \in \mathbb{N}} \subset C^{\infty}_c(\mathbb{R}^n)$ with $||u_k|_{\Omega}-u||_{W^{1,p}(\Omega)} \rightarrow 0, k \rightarrow \infty$ The proof went as follows: First we set $\delta>0$ and constructed a $\overline{u} \in W^{1,p}(\Omega)$ with $supp(\overline{u}) \subset\subset \mathbb{R}^n$, i.e. compact support in $\mathbb{R}^n$, with $||\overline{u}-u||_{W^{1,p}(\Omega)} < \delta$. Then the proof said to use a extension operator for Sobolev functions, which can be constructed, since for the boundary $\Gamma$ of $\Omega$ it holds that $\Gamma \cap supp(\overline{u})$ is compact. This was the part I did not understand . For reference our lecturer used the following notes . The above stated corollary is Korollar 8.4.2. Unfortunatly the notes are written in german, but since the notes are mainly based on Brezi's book on functional analysis, the same statement can be found in Corollary 9.8 in Brezi's book. Thanks a lot in advance!",,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
6,Determination of the unitary irreps of $\mathbb{R}$ using Stone's theorem,Determination of the unitary irreps of  using Stone's theorem,\mathbb{R},"I've tried to find the unitary irreducible representations of the additive group $(\mathbb{R},+)$ and came up with a pair of results, which I want to verify if are correct. They are: Theorem: Let the additive group $G=(\mathbb{R},+)$ be given. Then any unitary representation $U : G\to \mathfrak{U}(\mathscr{H})$ acting on the Hilbert space $\mathscr{H}$ is isomorphic to a unitary representation $U^\diamond : G\to \mathfrak{U}(L^2(X,d\mu))$ for some measure space $(X,\mu)$ given by $[U^\diamond(t)\Psi](x)=e^{-ita(x)}\Psi(x)$ for a real $a\in L^2(X,d\mu)$. Proof: Since a unitary representation is given by a continuous homomorphism $U$ on the strong topology, this means that $U$ is actually a strongly continuous $1$-parameter group of unitary operators, after all, $U(t+s)=U(t)U(s)$. Hence Stone's theorem guarantees that there is one hermitian operator $A$ in $\mathscr{H}$ such that $U(t) = e^{-itA}$. This in turn can be rigorously characterized by the spectral theorem. Since $A$ is hermitian, there is a measure space $(X,\mu)$ and a unitary isomorphism $\chi : \mathscr{H}\to L^2(X,d\mu)$ such that if we define $A^\diamond = \chi A\chi^\dagger$ then $A^\diamond$ is a multiplication operator in the sense that there is $a\in L^2(X,d\mu)$ which is actualy real, satisfying $$[A^\diamond\Psi](x)=a(x)\Psi(x),\quad \forall x\in X.$$ In that case we can perfectly define $e^{-itA^\diamond}$ as $$[e^{-itA^\diamond}\Psi](x)=e^{-ita(x)}\Psi(x),\quad \forall x\in X.$$ Hence we may as well define $$e^{-itA}=\chi^\dagger e^{-itA^\diamond}\chi.$$ But here $U^\diamond(t)=e^{-itA^\diamond}$ provides a unitary representation of $G$ on $L^2(X,d\mu)$ and so the last equation shows that $\chi$ becomes a representation isomorphism between it and the original one, completing the proof. Corolary: Let the additive group $G = (\mathbb{R},+)$ be given. Then the irreducible representations are parametrized by $\lambda \in \mathbb{R}$ and given by $U^\diamond : G\to \mathfrak{U}(L^2(X,d\mu))$ given by $U^\diamond(t)=e^{-i\lambda t}\mathbf{1}$ where $\mathbf{1}$ is the identity operator and $(X,\mu)$ is a measure space. Proof : Let an irreducible representation $U : G\to \mathfrak{U}(\mathscr{H})$ be given. By the previous theorem it is isomorphic to a representation $U^\diamond : G\to \mathfrak{U}(L^2(X,d\mu))$ characterized by a real function $a\in L^2(X,d\mu)$ so that $U^\diamond(t)= e^{-itA^\diamond}$ for the associate multiplication operator $A^\diamond$. It should be obvious that $A^\diamond$ commutes with $U^\diamond(t)$ for every $t$. Hence this means $$[U^\diamond(t),A^\diamond]=U^\diamond(t)\circ A^\diamond - A^\diamond \circ U^\diamond(t) = 0.$$ Hence by Schur's lemma, since the representation is irreducible, $A^\diamond$ acts by a multiple of the identity. Thus $a(x) = \lambda$ for some $\lambda \in \mathbb{R}$ and for all $x\in X$. In turn $U^\diamond(t) = e^{-i\lambda t}\mathbf{1}$ completing the proof. Now I'm unsure if this is correct (I'm actually a bit new to functional analysis). Is the result correct, or have I made some remarkably wrong mistake? Finaly, I feel strange that one arbitrary measure space $(X,\mu)$ is left lurking around. Wasn't it required to have a fully determined $(X,\mu)$ in the irreducible representations?","I've tried to find the unitary irreducible representations of the additive group $(\mathbb{R},+)$ and came up with a pair of results, which I want to verify if are correct. They are: Theorem: Let the additive group $G=(\mathbb{R},+)$ be given. Then any unitary representation $U : G\to \mathfrak{U}(\mathscr{H})$ acting on the Hilbert space $\mathscr{H}$ is isomorphic to a unitary representation $U^\diamond : G\to \mathfrak{U}(L^2(X,d\mu))$ for some measure space $(X,\mu)$ given by $[U^\diamond(t)\Psi](x)=e^{-ita(x)}\Psi(x)$ for a real $a\in L^2(X,d\mu)$. Proof: Since a unitary representation is given by a continuous homomorphism $U$ on the strong topology, this means that $U$ is actually a strongly continuous $1$-parameter group of unitary operators, after all, $U(t+s)=U(t)U(s)$. Hence Stone's theorem guarantees that there is one hermitian operator $A$ in $\mathscr{H}$ such that $U(t) = e^{-itA}$. This in turn can be rigorously characterized by the spectral theorem. Since $A$ is hermitian, there is a measure space $(X,\mu)$ and a unitary isomorphism $\chi : \mathscr{H}\to L^2(X,d\mu)$ such that if we define $A^\diamond = \chi A\chi^\dagger$ then $A^\diamond$ is a multiplication operator in the sense that there is $a\in L^2(X,d\mu)$ which is actualy real, satisfying $$[A^\diamond\Psi](x)=a(x)\Psi(x),\quad \forall x\in X.$$ In that case we can perfectly define $e^{-itA^\diamond}$ as $$[e^{-itA^\diamond}\Psi](x)=e^{-ita(x)}\Psi(x),\quad \forall x\in X.$$ Hence we may as well define $$e^{-itA}=\chi^\dagger e^{-itA^\diamond}\chi.$$ But here $U^\diamond(t)=e^{-itA^\diamond}$ provides a unitary representation of $G$ on $L^2(X,d\mu)$ and so the last equation shows that $\chi$ becomes a representation isomorphism between it and the original one, completing the proof. Corolary: Let the additive group $G = (\mathbb{R},+)$ be given. Then the irreducible representations are parametrized by $\lambda \in \mathbb{R}$ and given by $U^\diamond : G\to \mathfrak{U}(L^2(X,d\mu))$ given by $U^\diamond(t)=e^{-i\lambda t}\mathbf{1}$ where $\mathbf{1}$ is the identity operator and $(X,\mu)$ is a measure space. Proof : Let an irreducible representation $U : G\to \mathfrak{U}(\mathscr{H})$ be given. By the previous theorem it is isomorphic to a representation $U^\diamond : G\to \mathfrak{U}(L^2(X,d\mu))$ characterized by a real function $a\in L^2(X,d\mu)$ so that $U^\diamond(t)= e^{-itA^\diamond}$ for the associate multiplication operator $A^\diamond$. It should be obvious that $A^\diamond$ commutes with $U^\diamond(t)$ for every $t$. Hence this means $$[U^\diamond(t),A^\diamond]=U^\diamond(t)\circ A^\diamond - A^\diamond \circ U^\diamond(t) = 0.$$ Hence by Schur's lemma, since the representation is irreducible, $A^\diamond$ acts by a multiple of the identity. Thus $a(x) = \lambda$ for some $\lambda \in \mathbb{R}$ and for all $x\in X$. In turn $U^\diamond(t) = e^{-i\lambda t}\mathbf{1}$ completing the proof. Now I'm unsure if this is correct (I'm actually a bit new to functional analysis). Is the result correct, or have I made some remarkably wrong mistake? Finaly, I feel strange that one arbitrary measure space $(X,\mu)$ is left lurking around. Wasn't it required to have a fully determined $(X,\mu)$ in the irreducible representations?",,"['group-theory', 'functional-analysis', 'proof-verification', 'representation-theory', 'spectral-theory']"
7,Is this operator pseudodifferential or trace-class?,Is this operator pseudodifferential or trace-class?,,"Let $M$ be a closed manifold and $D$ a Dirac operator on $M$. Then (the closure of) $D^2+1$ has a bounded inverse  $$(D^2+1)^{-1}:L^2\rightarrow H^2,$$  where $H^2$ is the second Sobolev space. In fact, $(D^2+1)^{-1}$ is a compact operator on $L^2$ by Rellich's lemma. Question 1: Is $(D^2+1)^{-1}$ a properly supported pseudodifferential operator if $M$ is non-compact? Question 2: Is $(D^2+1)^{-1}$ a trace-class operator on $L^2$ (when $M$ is compact)? Thoughts: I am not sure about the first question, but I believe the answer to the second question is no (although certainly one can raise $(D^2+1)^{-1}$ to a high enough power to get a trace-class operator), although I don't know how to show explicitly that it's not trace-class. Thanks!","Let $M$ be a closed manifold and $D$ a Dirac operator on $M$. Then (the closure of) $D^2+1$ has a bounded inverse  $$(D^2+1)^{-1}:L^2\rightarrow H^2,$$  where $H^2$ is the second Sobolev space. In fact, $(D^2+1)^{-1}$ is a compact operator on $L^2$ by Rellich's lemma. Question 1: Is $(D^2+1)^{-1}$ a properly supported pseudodifferential operator if $M$ is non-compact? Question 2: Is $(D^2+1)^{-1}$ a trace-class operator on $L^2$ (when $M$ is compact)? Thoughts: I am not sure about the first question, but I believe the answer to the second question is no (although certainly one can raise $(D^2+1)^{-1}$ to a high enough power to get a trace-class operator), although I don't know how to show explicitly that it's not trace-class. Thanks!",,"['functional-analysis', 'differential-geometry', 'operator-theory', 'spin-geometry', 'pseudo-differential-operators']"
8,Unital homomorphism to semisimple Banach algebra is automatically continuous,Unital homomorphism to semisimple Banach algebra is automatically continuous,,"I need to prove that any unital  homomorphism $\phi: A \to B$, where $A$ is unital  Banach algebra and $B$ is semisimple Banach algebra is continuous. The definition of ""semisimple"" I know is that the kernel of Gelfand transform which is the same as $ \{b \in B: \sigma(b) = 0 \}$ equals $\{0 \}$. I am asking for some hint. Also may be I am not aware of some result needed.","I need to prove that any unital  homomorphism $\phi: A \to B$, where $A$ is unital  Banach algebra and $B$ is semisimple Banach algebra is continuous. The definition of ""semisimple"" I know is that the kernel of Gelfand transform which is the same as $ \{b \in B: \sigma(b) = 0 \}$ equals $\{0 \}$. I am asking for some hint. Also may be I am not aware of some result needed.",,"['functional-analysis', 'banach-algebras', 'gelfand-representation']"
9,Why is this compact operator a Fredholm operator?,Why is this compact operator a Fredholm operator?,,"Let $X$ be a Banach space with the $L^{\infty}$ norm and let $A$: $X \rightarrow X$ be an integral operator of the following form, \begin{equation} Ax(s) = \lambda\int^{b}_{a}K(s,t)x(t)dt, \end{equation} where $K$: $[a,b] \times [a,b]\rightarrow \mathbb{R}$ is a continuous map such that $K \neq 0$ and where $\lambda$ is a constant. I know that due to the continuity of $K$, Arzelà–Ascoli theorem implies that $A$ is compact. But in the uploaded image they say $A$ is Fredholm as well. I fail to see why, could someone help me out?","Let $X$ be a Banach space with the $L^{\infty}$ norm and let $A$: $X \rightarrow X$ be an integral operator of the following form, \begin{equation} Ax(s) = \lambda\int^{b}_{a}K(s,t)x(t)dt, \end{equation} where $K$: $[a,b] \times [a,b]\rightarrow \mathbb{R}$ is a continuous map such that $K \neq 0$ and where $\lambda$ is a constant. I know that due to the continuity of $K$, Arzelà–Ascoli theorem implies that $A$ is compact. But in the uploaded image they say $A$ is Fredholm as well. I fail to see why, could someone help me out?",,"['functional-analysis', 'spectral-theory', 'compact-operators']"
10,Compact embedding between $H^{m+1}(\Omega)$ and $H^{m}(\Omega)$ for $\Omega$ bounded,Compact embedding between  and  for  bounded,H^{m+1}(\Omega) H^{m}(\Omega) \Omega,"I know that we have Rellich-Kondrachov Theorem that says that there is a compact embedding between $H^{1}(\Omega)$ and $H^{0}(\Omega)$, or more generally as Adams states (pag 168 theorem 6.3) we have a compact embedding between $H^{j+k}(\Omega)$ and $H^{j}(\Omega)$ for $j\geq 0$ and $k>0$ integers. So, we are excluding negative exponents, so i would like to know if we can have the same result for every Sobolev Space,i.e, i would like to know if we can have the compact embedding between $H^{m+1}(\Omega)$ and $H^{m}(\Omega)$, $m \in \mathbb{R}$, just knowing Rellich-Kondrachov theorem. I'm particularly interested in knowing if there is a compact embedding for $H^{-1}(\Omega)$ and $H^{0}(\Omega)$. Thank you in advance, and any reference or help would be useful","I know that we have Rellich-Kondrachov Theorem that says that there is a compact embedding between $H^{1}(\Omega)$ and $H^{0}(\Omega)$, or more generally as Adams states (pag 168 theorem 6.3) we have a compact embedding between $H^{j+k}(\Omega)$ and $H^{j}(\Omega)$ for $j\geq 0$ and $k>0$ integers. So, we are excluding negative exponents, so i would like to know if we can have the same result for every Sobolev Space,i.e, i would like to know if we can have the compact embedding between $H^{m+1}(\Omega)$ and $H^{m}(\Omega)$, $m \in \mathbb{R}$, just knowing Rellich-Kondrachov theorem. I'm particularly interested in knowing if there is a compact embedding for $H^{-1}(\Omega)$ and $H^{0}(\Omega)$. Thank you in advance, and any reference or help would be useful",,"['functional-analysis', 'sobolev-spaces', 'compact-operators']"
11,Closeness of points in the irreducible decomposition of a C$^{*}$-algebra representation,Closeness of points in the irreducible decomposition of a C-algebra representation,^{*},"Suppose $X$ and $Y$ are compact metric spaces. Let $\varphi\colon C(X)\to M_{n}(C(Y))$ be any $*$-homomorphism. If $\pi$ is an irreducible representation of $M_{n}(C(Y))$, then $\pi$ is unitarily equivalent to a point evaluation $\textrm{ev}_{y}$. The $*$-homomorphism $\textrm{ev}_{y}\circ\varphi\colon C(X)\to M_{n}(\mathbb{C})$ is a representation of $C(X)$. Since it's a finite-dimensional representation, we can find a unitary $u_{y}\in M_{n}(\mathbb{C})$ and a set of points $X_{y}=\{x^{y}_{1},\ldots,x^{y}_{n}\}\subset X$ such that for all $f\in C(X)$, $$ (\varphi\circ f)(y)=(\textrm{ev}_{y}\circ\varphi)(f)=u_{y} \begin{pmatrix} f(x^{y}_{1}) & 0 & \cdots & 0\\ 0 & f(x^{y}_{2}) & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\ 0 & 0 & \cdots & f(x^{y}_{n}) \end{pmatrix} u_{y}^{*}. $$ My question is: Does the Hausdorff distance of the set $\{x_{1}^{y},\ldots,x_{n}^{y}\}$ vary continuously in $y$? More precisely, if $y_{m}\to y$ in $Y$, does $D_{m}:=\max_{1\leq i\leq n}\min_{1\leq j\leq n}d_{X}(x^{y}_{i},x^{y_{m}}_{j})\to 0$ as $m\to \infty$? This question is motivated by the simpler case where $\varphi$ gives rise to continuous functions $\lambda_{1},\ldots,\lambda_{n}:Y\to X$ satisfying $$ \varphi(f)=\begin{pmatrix} f\circ\lambda_{1} & 0 & \cdots & 0\\ 0 & f\circ\lambda_{2} & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\ 0 & 0 & \cdots & f\circ\lambda_{n} \end{pmatrix}. $$ In this case for each $y\in Y$, $X_{y}=\{\lambda_{1}(y),\ldots,\lambda_{n}(y)\}$, and the result holds.","Suppose $X$ and $Y$ are compact metric spaces. Let $\varphi\colon C(X)\to M_{n}(C(Y))$ be any $*$-homomorphism. If $\pi$ is an irreducible representation of $M_{n}(C(Y))$, then $\pi$ is unitarily equivalent to a point evaluation $\textrm{ev}_{y}$. The $*$-homomorphism $\textrm{ev}_{y}\circ\varphi\colon C(X)\to M_{n}(\mathbb{C})$ is a representation of $C(X)$. Since it's a finite-dimensional representation, we can find a unitary $u_{y}\in M_{n}(\mathbb{C})$ and a set of points $X_{y}=\{x^{y}_{1},\ldots,x^{y}_{n}\}\subset X$ such that for all $f\in C(X)$, $$ (\varphi\circ f)(y)=(\textrm{ev}_{y}\circ\varphi)(f)=u_{y} \begin{pmatrix} f(x^{y}_{1}) & 0 & \cdots & 0\\ 0 & f(x^{y}_{2}) & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\ 0 & 0 & \cdots & f(x^{y}_{n}) \end{pmatrix} u_{y}^{*}. $$ My question is: Does the Hausdorff distance of the set $\{x_{1}^{y},\ldots,x_{n}^{y}\}$ vary continuously in $y$? More precisely, if $y_{m}\to y$ in $Y$, does $D_{m}:=\max_{1\leq i\leq n}\min_{1\leq j\leq n}d_{X}(x^{y}_{i},x^{y_{m}}_{j})\to 0$ as $m\to \infty$? This question is motivated by the simpler case where $\varphi$ gives rise to continuous functions $\lambda_{1},\ldots,\lambda_{n}:Y\to X$ satisfying $$ \varphi(f)=\begin{pmatrix} f\circ\lambda_{1} & 0 & \cdots & 0\\ 0 & f\circ\lambda_{2} & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\ 0 & 0 & \cdots & f\circ\lambda_{n} \end{pmatrix}. $$ In this case for each $y\in Y$, $X_{y}=\{\lambda_{1}(y),\ldots,\lambda_{n}(y)\}$, and the result holds.",,"['functional-analysis', 'operator-theory', 'representation-theory', 'operator-algebras', 'c-star-algebras']"
12,Proving completeness of space of converging sequences,Proving completeness of space of converging sequences,,"Assume $c_{0, \lambda}$ is a metric space of converging to zero sequences, where $c_{0, \lambda} =\{ x = (x_i):\lim_{i \rightarrow \infty} i^{\lambda} x_i = 0 \}$, here $\lambda > 0$ imposes a certain restriction on the convergence speed for the sequences; with the following metric $d(x,y) = \sup_{i}i^{\lambda}|x_i - y_i|$, I'm trying to prove that such space is complete. My approach: Assume that $(x_n)$ is a Cauchy sequence from $c_{0, \lambda}$. It is clear that $(x_n)$ is also from $c_{0}$ (with $\lambda = 0$, which is known to be complete). If we construct a sequence $z_n = (z_{n,i}) = ( i^{\lambda} x_{n, i} )$, by definition it should hold that $z_n \in c_0$ for some fixed $\lambda > 0$, and it's also a Cauchy sequence in $c_0$. Hence, there exists a limit $z_0 \in c_0$, which implies that $\lim_{i \rightarrow \infty} z_{0, i}= 0$. Therefore, it should hold that $\lim_{i \rightarrow \infty} z_{0,i}/{i^{\lambda}} = 0$ for the same fixed $\lambda > 0$, suggesting that $x_0 := (x_i) = (z_{0,i}/i^{\lambda}) \in c_{0, \lambda}$. Since $d(x_n, x_0) = \sup_{i}i^{\lambda}|x_{n,i} - x_{0,i}| = sup_i | i^\lambda x_{n,i} - i^{\lambda} x_{0, i}| = sup_{i} | z_{n, i} - z_{0, i} | \rightarrow 0$, $i \rightarrow \infty$, it seems that $x_n \rightarrow x_0, n \rightarrow \infty$, suggesting that $c_{0, \lambda}$ is complete. Here the last result holds due to convergence in $c_0$ under supremum metric. My question: Is such approach suitable for proving completeness? If not, what would be the right way to proceed in this case (and in more general ones, where the restriction $i^{\lambda}$ may have various other expressions (and hence the metric would be constructed accordingly))? The goal I'm trying to achieve here is not only to just prove the completeness for this particular example, but to better understand proving completeness for various spaces, where it is clear that it is a some sort of a restricted version of a larger, well known space the properties of which is known. Would be grateful for any hints or corrections!","Assume $c_{0, \lambda}$ is a metric space of converging to zero sequences, where $c_{0, \lambda} =\{ x = (x_i):\lim_{i \rightarrow \infty} i^{\lambda} x_i = 0 \}$, here $\lambda > 0$ imposes a certain restriction on the convergence speed for the sequences; with the following metric $d(x,y) = \sup_{i}i^{\lambda}|x_i - y_i|$, I'm trying to prove that such space is complete. My approach: Assume that $(x_n)$ is a Cauchy sequence from $c_{0, \lambda}$. It is clear that $(x_n)$ is also from $c_{0}$ (with $\lambda = 0$, which is known to be complete). If we construct a sequence $z_n = (z_{n,i}) = ( i^{\lambda} x_{n, i} )$, by definition it should hold that $z_n \in c_0$ for some fixed $\lambda > 0$, and it's also a Cauchy sequence in $c_0$. Hence, there exists a limit $z_0 \in c_0$, which implies that $\lim_{i \rightarrow \infty} z_{0, i}= 0$. Therefore, it should hold that $\lim_{i \rightarrow \infty} z_{0,i}/{i^{\lambda}} = 0$ for the same fixed $\lambda > 0$, suggesting that $x_0 := (x_i) = (z_{0,i}/i^{\lambda}) \in c_{0, \lambda}$. Since $d(x_n, x_0) = \sup_{i}i^{\lambda}|x_{n,i} - x_{0,i}| = sup_i | i^\lambda x_{n,i} - i^{\lambda} x_{0, i}| = sup_{i} | z_{n, i} - z_{0, i} | \rightarrow 0$, $i \rightarrow \infty$, it seems that $x_n \rightarrow x_0, n \rightarrow \infty$, suggesting that $c_{0, \lambda}$ is complete. Here the last result holds due to convergence in $c_0$ under supremum metric. My question: Is such approach suitable for proving completeness? If not, what would be the right way to proceed in this case (and in more general ones, where the restriction $i^{\lambda}$ may have various other expressions (and hence the metric would be constructed accordingly))? The goal I'm trying to achieve here is not only to just prove the completeness for this particular example, but to better understand proving completeness for various spaces, where it is clear that it is a some sort of a restricted version of a larger, well known space the properties of which is known. Would be grateful for any hints or corrections!",,"['functional-analysis', 'convergence-divergence', 'metric-spaces', 'complete-spaces']"
13,Application of the open mapping theorem Brezis exercise 2.12,Application of the open mapping theorem Brezis exercise 2.12,,"I am trying to understand the answer given in Brezis' book for this particular  exercise: Let $E$ and $F$ be two Banach spaces with norms $\|\ \|_E$ and $\|\ \|_F$ . Let $T\in\mathcal L(E,F)$ be such that $R(T)$ is closed and $\dim N(T)<\infty$ . Let $|\ |$ denote another norm on $E$ that is weaker that $\|\ \|_E$ , i.e., $|x|\leqslant M\|x\|_E\ \forall x\in E$ . Prove that there exists a constant $C$ such that $$\|x|_E\leqslant C\left(\|Tx\|_F+|x|\right)\quad\forall x\in E.$$ Hint: Argue by contradiction In the back of the book there is an answer to the exercise and I am trying to fill in the details, however there is a very important step here Without loss of generality we may assume that $T$ is surjective (otherwise, replace $E$ by $R(T)$ ). Assume by contradiction that there is a sequence $(x_n)$ in $E$ such that $$ \|x_n\|_E=1\quad\text{ and } \|Tx_n\|_F+|x_n|<1/n.$$ By the open mapping theorem there is a constant $c>0$ such that $T(B_E)\supset cB_F$ . Since $\|Tx_n\|_F<1/n$ , there exists some $y_n\in E$ such that $$Tx_n = Ty_n \quad \text{ and } \|y_n\|_E < 1/nc. $$ Write $x_n+y_n+z_n$ with $z_n\in N(T)$ , $\|y_n\|_E\to 0$ , and $\|z_n\|_E\to1$ . On the other hand, $|x_n|<1/n$ ; hence $|z_n|<(1/n)+|y_n\leqslant (1/n)+M\|y_n\|_E$ , and consequently $|z_n|\to0$ . This is impossible, since the norms $\|\ \|_E$ and $|\ |$ are equivalent on the finite-dimensional space $N(T)$ . which I do not understand. Why can we assume $\|y_n\|_E \leq \frac{1}{nc}$ ? The open mapping provides us with the inequality $\|y_n\|_E \leq \frac{1}{c}$ since $cB_F \subseteq T(B_E)$ , but without the 'n' in the denominator we can no longer assume that $y_n \to 0$ and we need that to get the contradiction.","I am trying to understand the answer given in Brezis' book for this particular  exercise: Let and be two Banach spaces with norms and . Let be such that is closed and . Let denote another norm on that is weaker that , i.e., . Prove that there exists a constant such that Hint: Argue by contradiction In the back of the book there is an answer to the exercise and I am trying to fill in the details, however there is a very important step here Without loss of generality we may assume that is surjective (otherwise, replace by ). Assume by contradiction that there is a sequence in such that By the open mapping theorem there is a constant such that . Since , there exists some such that Write with , , and . On the other hand, ; hence , and consequently . This is impossible, since the norms and are equivalent on the finite-dimensional space . which I do not understand. Why can we assume ? The open mapping provides us with the inequality since , but without the 'n' in the denominator we can no longer assume that and we need that to get the contradiction.","E F \|\ \|_E \|\ \|_F T\in\mathcal L(E,F) R(T) \dim N(T)<\infty |\ | E \|\ \|_E |x|\leqslant M\|x\|_E\ \forall x\in E C \|x|_E\leqslant C\left(\|Tx\|_F+|x|\right)\quad\forall x\in E. T E R(T) (x_n) E  \|x_n\|_E=1\quad\text{ and } \|Tx_n\|_F+|x_n|<1/n. c>0 T(B_E)\supset cB_F \|Tx_n\|_F<1/n y_n\in E Tx_n = Ty_n \quad \text{ and } \|y_n\|_E < 1/nc.  x_n+y_n+z_n z_n\in N(T) \|y_n\|_E\to 0 \|z_n\|_E\to1 |x_n|<1/n |z_n|<(1/n)+|y_n\leqslant (1/n)+M\|y_n\|_E |z_n|\to0 \|\ \|_E |\ | N(T) \|y_n\|_E \leq \frac{1}{nc} \|y_n\|_E \leq \frac{1}{c} cB_F \subseteq T(B_E) y_n \to 0","['functional-analysis', 'operator-theory', 'banach-spaces']"
14,"What is ""Geometry of Banach spaces"" precisely?","What is ""Geometry of Banach spaces"" precisely?",,"I've been seeing this around lately in books that study classical Banach spaces. I can't find an exact description of this subject anywhere so I figured somebody on SE would know how to describe it; the word ""Geometry"" in the title is what confuses me.","I've been seeing this around lately in books that study classical Banach spaces. I can't find an exact description of this subject anywhere so I figured somebody on SE would know how to describe it; the word ""Geometry"" in the title is what confuses me.",,"['functional-analysis', 'soft-question']"
15,Paving property,Paving property,,"In their famed paper ( https://arxiv.org/abs/math-ph/0011053 ), Bourgain and Goldstein conjecture what they call the paving property : Let $H_{jk}=\delta_{j,k+1}+\delta_{j,k-1}+v(\theta+j\omega)\delta_{jk}$ be the matrix of the Schrödinger operator $H$ for a quasi-periodic nearest-neighbour lattice system with $j,k\in\mathbb Z$ and $\theta,\omega\in\mathbb T$. Denote by $H_\Delta$ the restriction of $H$ to $\Delta\subset\mathbb Z$, i.e. $H_\Delta=P_\Delta HP_\Delta$ with $P_\Delta$ the projection onto $\Delta$, and by $G_\Delta=(H_\Delta-E)^{-1}$ its resolvent for $E\notin\sigma(H_\Delta)$. Then: let $\Delta\subset\mathbb Z$ be an interval of length $N>n$ such that for each $x\in\Delta$ there is an interval $\Delta'\subset\Delta$ of length $n$ satisfying \begin{equation*} \{y\in\Delta:|x-y|<\frac n{10}\}\subset\Delta' \end{equation*} and \begin{equation*} |G_{\Delta'}(n_1,n_2)|<e^{-c|n_1-n_2|+o(n)} \end{equation*} for some constant $c>0$ and $n$ sufficiently large. Then also \begin{equation*} |G_{\Delta}(n_1,n_2)|<e^{-\frac c2|n_1-n_2|+o(n)}. \end{equation*} The authors promise a proof of this statement in Section IV of their paper which does not exist. Is the statement obvious and if so, why and how?","In their famed paper ( https://arxiv.org/abs/math-ph/0011053 ), Bourgain and Goldstein conjecture what they call the paving property : Let $H_{jk}=\delta_{j,k+1}+\delta_{j,k-1}+v(\theta+j\omega)\delta_{jk}$ be the matrix of the Schrödinger operator $H$ for a quasi-periodic nearest-neighbour lattice system with $j,k\in\mathbb Z$ and $\theta,\omega\in\mathbb T$. Denote by $H_\Delta$ the restriction of $H$ to $\Delta\subset\mathbb Z$, i.e. $H_\Delta=P_\Delta HP_\Delta$ with $P_\Delta$ the projection onto $\Delta$, and by $G_\Delta=(H_\Delta-E)^{-1}$ its resolvent for $E\notin\sigma(H_\Delta)$. Then: let $\Delta\subset\mathbb Z$ be an interval of length $N>n$ such that for each $x\in\Delta$ there is an interval $\Delta'\subset\Delta$ of length $n$ satisfying \begin{equation*} \{y\in\Delta:|x-y|<\frac n{10}\}\subset\Delta' \end{equation*} and \begin{equation*} |G_{\Delta'}(n_1,n_2)|<e^{-c|n_1-n_2|+o(n)} \end{equation*} for some constant $c>0$ and $n$ sufficiently large. Then also \begin{equation*} |G_{\Delta}(n_1,n_2)|<e^{-\frac c2|n_1-n_2|+o(n)}. \end{equation*} The authors promise a proof of this statement in Section IV of their paper which does not exist. Is the statement obvious and if so, why and how?",,"['functional-analysis', 'mathematical-physics', 'greens-function']"
16,About multiplication operators on fractional Sobolev space,About multiplication operators on fractional Sobolev space,,"Let $\Gamma$ be a regular boundary of a $C^{k,1}$ domain $\Omega$ and  $H^s(\Gamma)$, $s\in(0,1)$, denote the fractional Sobolev space on $\Gamma$. Suppose I define a multiplication operator $M_\phi:H^s(\Gamma)\to H^s(\Gamma)$ where $M_\phi v=\phi v$. What should be the minimal regularity of $v$ for the map to be continuous? I am thinking that $v$ being Lipschitz (i.e., $v \in C^{0,1}(\Gamma)$) is enough for $M_\phi$ to be continuous. Can someone please give me a reference or study that deals with this kind of problem. Thanks! Edit I am only interested on the case when the map $H^\frac12(\Gamma) \to H^\frac12(\Gamma)$ is continuous. More precisely, I want to know whether the following statement is true. Let $\Omega \subset \mathbb{R}^2$ be a bounded Lipschitz domain and $\Gamma$ be a non-empty subset of $\Omega$. Then, the map $v \mapsto \phi v$ is continuous in $H^\frac12(\Gamma)$ for any $v \in H^\frac12(\Gamma)$ and $\phi \in C^{0,1}(\Gamma)$. I tried to argue the validity of the above statement as follows. By McShane-Whitney extension theorem, we know that we can find a $\tilde{\phi} \in C^{0,1}(\bar{\Omega})$ such that $\tilde{\phi}|_{\Gamma} = \phi$. Then, using [Grisvard, Elliptic Problems in Nonsmooth Domains, Theorem 1.4.1.1, p. 21] and [McLean, Strongly Elliptic Systems and Boundary Integral Equations, Theorem 3.37, p. 102], we have that $v \mapsto \phi v$ a continuous linear map in $H^\frac12(\Gamma)$. Can someone confirm if my argument is correct?","Let $\Gamma$ be a regular boundary of a $C^{k,1}$ domain $\Omega$ and  $H^s(\Gamma)$, $s\in(0,1)$, denote the fractional Sobolev space on $\Gamma$. Suppose I define a multiplication operator $M_\phi:H^s(\Gamma)\to H^s(\Gamma)$ where $M_\phi v=\phi v$. What should be the minimal regularity of $v$ for the map to be continuous? I am thinking that $v$ being Lipschitz (i.e., $v \in C^{0,1}(\Gamma)$) is enough for $M_\phi$ to be continuous. Can someone please give me a reference or study that deals with this kind of problem. Thanks! Edit I am only interested on the case when the map $H^\frac12(\Gamma) \to H^\frac12(\Gamma)$ is continuous. More precisely, I want to know whether the following statement is true. Let $\Omega \subset \mathbb{R}^2$ be a bounded Lipschitz domain and $\Gamma$ be a non-empty subset of $\Omega$. Then, the map $v \mapsto \phi v$ is continuous in $H^\frac12(\Gamma)$ for any $v \in H^\frac12(\Gamma)$ and $\phi \in C^{0,1}(\Gamma)$. I tried to argue the validity of the above statement as follows. By McShane-Whitney extension theorem, we know that we can find a $\tilde{\phi} \in C^{0,1}(\bar{\Omega})$ such that $\tilde{\phi}|_{\Gamma} = \phi$. Then, using [Grisvard, Elliptic Problems in Nonsmooth Domains, Theorem 1.4.1.1, p. 21] and [McLean, Strongly Elliptic Systems and Boundary Integral Equations, Theorem 3.37, p. 102], we have that $v \mapsto \phi v$ a continuous linear map in $H^\frac12(\Gamma)$. Can someone confirm if my argument is correct?",,"['functional-analysis', 'fractional-sobolev-spaces']"
17,Condition insuring a convex body is a ball,Condition insuring a convex body is a ball,,"Let $K$ be a convex centered body in $\mathbb{R}^{n}$ and suppose that for every $\theta\in{S^{n-1}}$ we have $|K\cap{\theta^{\perp}}|=C$. Does this imply that $K$ is the euclidian ball? One can consider a refinement of the first version, assuming moreover that all such sections are identical. Is it now the euclidian ball? BTW, centered means that $\int_{K}xdx=0$ Thanks.","Let $K$ be a convex centered body in $\mathbb{R}^{n}$ and suppose that for every $\theta\in{S^{n-1}}$ we have $|K\cap{\theta^{\perp}}|=C$. Does this imply that $K$ is the euclidian ball? One can consider a refinement of the first version, assuming moreover that all such sections are identical. Is it now the euclidian ball? BTW, centered means that $\int_{K}xdx=0$ Thanks.",,"['geometry', 'functional-analysis', 'analysis', 'convex-analysis', 'convex-geometry']"
18,Convexity of a set defined by integrals of a function on measurable sets,Convexity of a set defined by integrals of a function on measurable sets,,"While studying measurable function, I want to know if the following statement is true. Let $\mu$ be a measure on $\mathbb R^n$ with a support with nonempty interior, $f$ polynomial function and $B$ a compact subset of $\mathbb R^n$ : $$A = \left\{\int_{\Omega} f \mathrm d\mu : \Omega \subset B \text{ $\mu$-measurable}\right\}$$ is convex. Can any one help me to prove or disprove this statement ?","While studying measurable function, I want to know if the following statement is true. Let $\mu$ be a measure on $\mathbb R^n$ with a support with nonempty interior, $f$ polynomial function and $B$ a compact subset of $\mathbb R^n$ : $$A = \left\{\int_{\Omega} f \mathrm d\mu : \Omega \subset B \text{ $\mu$-measurable}\right\}$$ is convex. Can any one help me to prove or disprove this statement ?",,"['functional-analysis', 'measure-theory']"
19,Difference quotients uniformly bounded in Holder space,Difference quotients uniformly bounded in Holder space,,"I have trouble with proving the following: Let $u \in C^{k, \alpha}$. For each $h > 0$, denote the  difference quotient of u in the $j$-direction as $\delta_{h, j}u$. That is,  \begin{equation}    \delta_{h, j} u (x) = \frac{u(x + h e_j) - u(x)}{h}. \end{equation} Suppose $| \delta_{h, j} u |_{k, \alpha} \leq M$ uniformly in $h$.  Then $D_j u \in C^{k+1, \alpha}$ and $| D_j u |_{k+1, \alpha} \leq M$. The Sobolev space version of this argument is easy because we can take a weakly convergent subsequence and pair it with test functions to obtain existence of a higher derivative. However, in the Holder space case, we do not have any test functions to make our lives easy. So I suspect we have to use certain Arzela-Ascoli type compactness, but somehow I am stuck and cannot proceed. I would really appreciate it if somebody could give me some idea or proof.  Thank you!","I have trouble with proving the following: Let $u \in C^{k, \alpha}$. For each $h > 0$, denote the  difference quotient of u in the $j$-direction as $\delta_{h, j}u$. That is,  \begin{equation}    \delta_{h, j} u (x) = \frac{u(x + h e_j) - u(x)}{h}. \end{equation} Suppose $| \delta_{h, j} u |_{k, \alpha} \leq M$ uniformly in $h$.  Then $D_j u \in C^{k+1, \alpha}$ and $| D_j u |_{k+1, \alpha} \leq M$. The Sobolev space version of this argument is easy because we can take a weakly convergent subsequence and pair it with test functions to obtain existence of a higher derivative. However, in the Holder space case, we do not have any test functions to make our lives easy. So I suspect we have to use certain Arzela-Ascoli type compactness, but somehow I am stuck and cannot proceed. I would really appreciate it if somebody could give me some idea or proof.  Thank you!",,"['real-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations']"
20,L2 norm of a function w.r.t. its gradient,L2 norm of a function w.r.t. its gradient,,"I was wondering whether it is possible to generalise the statement of this post , i.e. I would like to show something like $$\|f\|^2_{L^2(\Omega)}\leq C \|\nabla f\|^2_{L^2(\Omega)},$$ where $\Omega=[0,1]^n$ and $f:\Omega\to\mathbb{R}$ is a suitable function with $f=0$ on the vertices of $\Omega$ (and $C$ does not depend of $f$ ). Although this seems to be a standard question I was not able to find anything on this for $n\geq2$ . My approach was the following (given that everything is well-defined). By $f(0)=0$ we know that: $$f(x)=\int^1_0\langle\nabla f(hx),x\rangle dh=\int^{|x|}_0\langle\nabla f(uw),w\rangle du,$$ where $w=x/|x|$ . If I am not mistaken, using polar coordinates (cf. this post ) should give the identity $$\|f\|^2_{L^2(\Omega)}=\int^{\rho_n}_0\int_{A_n}f(rw)^2dwr^{n-1}dr$$ with $A_n=S^{n-1}\cap[0,1]^n$ , $S^{n-1}$ sphere and $\rho_n=\sqrt{n}$ being the ""diagonal"" (or diameter) of $\Omega$ . Next I would like to combine the two preceding equalities to obtain the desired inequality. This should give (if I did not mix anything up with the gradient) the equation $$\|f\|^2_{L^2(\Omega)}=\int^{\rho_n}_0\int_{A_n}\Big(\int^r_0\langle\nabla f(uw),w\rangle du\Big)^2dwr^{n-1}dr.$$ Cauchy-Schwarz now yields $$\|f\|^2_{L^2(\Omega)}\leq\int^{\rho_n}_0\int_{A_n}\int^r_0|\nabla f(uw)|^2dudwr^ndr$$ but I do not see how I could end up with $\|\nabla f\|^2_{L^2(\Omega)}$ since I would need the term $u^{n-1}$ to pop-up somewhere. Any ideas are appreciated especially for the special case $n=2$ . In particular, I would like to consider Sobolev functions with $s\in(1,2)$ to obtain $$\|f\|^2_{L^2(\Omega)}\leq C \|\nabla f\|^2_{L^2(\Omega)}\leq C' \sum_{|\alpha|=1}\| f^{(\alpha)}\|^2_{H^{s-1}(\Omega)},$$ such that $C'$ does not depend on $f$ and I think that I already showed that the second inequality holds.","I was wondering whether it is possible to generalise the statement of this post , i.e. I would like to show something like where and is a suitable function with on the vertices of (and does not depend of ). Although this seems to be a standard question I was not able to find anything on this for . My approach was the following (given that everything is well-defined). By we know that: where . If I am not mistaken, using polar coordinates (cf. this post ) should give the identity with , sphere and being the ""diagonal"" (or diameter) of . Next I would like to combine the two preceding equalities to obtain the desired inequality. This should give (if I did not mix anything up with the gradient) the equation Cauchy-Schwarz now yields but I do not see how I could end up with since I would need the term to pop-up somewhere. Any ideas are appreciated especially for the special case . In particular, I would like to consider Sobolev functions with to obtain such that does not depend on and I think that I already showed that the second inequality holds.","\|f\|^2_{L^2(\Omega)}\leq C \|\nabla f\|^2_{L^2(\Omega)}, \Omega=[0,1]^n f:\Omega\to\mathbb{R} f=0 \Omega C f n\geq2 f(0)=0 f(x)=\int^1_0\langle\nabla f(hx),x\rangle dh=\int^{|x|}_0\langle\nabla f(uw),w\rangle du, w=x/|x| \|f\|^2_{L^2(\Omega)}=\int^{\rho_n}_0\int_{A_n}f(rw)^2dwr^{n-1}dr A_n=S^{n-1}\cap[0,1]^n S^{n-1} \rho_n=\sqrt{n} \Omega \|f\|^2_{L^2(\Omega)}=\int^{\rho_n}_0\int_{A_n}\Big(\int^r_0\langle\nabla f(uw),w\rangle du\Big)^2dwr^{n-1}dr. \|f\|^2_{L^2(\Omega)}\leq\int^{\rho_n}_0\int_{A_n}\int^r_0|\nabla f(uw)|^2dudwr^ndr \|\nabla f\|^2_{L^2(\Omega)} u^{n-1} n=2 s\in(1,2) \|f\|^2_{L^2(\Omega)}\leq C \|\nabla f\|^2_{L^2(\Omega)}\leq C' \sum_{|\alpha|=1}\| f^{(\alpha)}\|^2_{H^{s-1}(\Omega)}, C' f","['functional-analysis', 'measure-theory', 'multivariable-calculus', 'normed-spaces', 'sobolev-spaces']"
21,Soft question - a subset of a Hilbert space endowed with subspace topology,Soft question - a subset of a Hilbert space endowed with subspace topology,,"I am considering a Hilbert space $X$, endowed with its weak topology. I need to work with a subset (but not a subspace) $S$ of $X$. However I need to endow $S$ with the subspace topology (so $U$ is weakly open in $S$ if and only if there is a weakly open set $V$ in $X$ such that $U=S \cap V$). I have quite a lot to write about this, and I am worried that the reader will misunderstand when I speak about $S$ with the subspace topology, thinking that I have misunderstood $S$ to be a vector subpspace. I am also worried that when I say subspace topology the reader will not realise it's the one induced from $X$ with its weak topology. Is there a better way to phrase myself? Perhaps another way to refer to the subspace topology as to not confuse $S$ with a vector subspace of $X$? I am struggling to find a concise and clear way to phrase it.","I am considering a Hilbert space $X$, endowed with its weak topology. I need to work with a subset (but not a subspace) $S$ of $X$. However I need to endow $S$ with the subspace topology (so $U$ is weakly open in $S$ if and only if there is a weakly open set $V$ in $X$ such that $U=S \cap V$). I have quite a lot to write about this, and I am worried that the reader will misunderstand when I speak about $S$ with the subspace topology, thinking that I have misunderstood $S$ to be a vector subpspace. I am also worried that when I say subspace topology the reader will not realise it's the one induced from $X$ with its weak topology. Is there a better way to phrase myself? Perhaps another way to refer to the subspace topology as to not confuse $S$ with a vector subspace of $X$? I am struggling to find a concise and clear way to phrase it.",,"['general-topology', 'functional-analysis', 'soft-question', 'weak-topology']"
22,"When can we interchange $\partial/\partial x$ with integral sign, or commute two such operators in general?","When can we interchange  with integral sign, or commute two such operators in general?",\partial/\partial x,"In applied mathematics, when can we assume that we are allowed to do this: $$ \frac{\partial}{\partial x}\int_{x}\int_{y}\cdots\int f(x, y,\cdots)\ dx\ dy\ d(\cdots)= \int_{x}\int_{y}\cdots\int \frac{\partial}{\partial x}\left[f(x, y,\cdots)\right]\ dx\ dy\ d(\cdots),$$ where $f(x,y,\cdots)$ is a general continuous and differentiable function over the domain of the variables $(x, y, \cdots)$? If we view differentiation and integral here as operators, then a more general question would be about the conditions (or properties) that would make two general operators $T_{1}[\cdot]$ and $T_{2}[\cdot]$ commute? I am interested here in applying such rules to practical calculations (e.g. in physical sciences), so any relevant practical notes or observations about such conditions would be nice.","In applied mathematics, when can we assume that we are allowed to do this: $$ \frac{\partial}{\partial x}\int_{x}\int_{y}\cdots\int f(x, y,\cdots)\ dx\ dy\ d(\cdots)= \int_{x}\int_{y}\cdots\int \frac{\partial}{\partial x}\left[f(x, y,\cdots)\right]\ dx\ dy\ d(\cdots),$$ where $f(x,y,\cdots)$ is a general continuous and differentiable function over the domain of the variables $(x, y, \cdots)$? If we view differentiation and integral here as operators, then a more general question would be about the conditions (or properties) that would make two general operators $T_{1}[\cdot]$ and $T_{2}[\cdot]$ commute? I am interested here in applying such rules to practical calculations (e.g. in physical sciences), so any relevant practical notes or observations about such conditions would be nice.",,"['calculus', 'real-analysis', 'integration', 'functional-analysis', 'operator-theory']"
23,Delta distribution is not induced by and tempered function,Delta distribution is not induced by and tempered function,,"Call a function $f: \mathbb{R} \to \mathbb{C}$ a $\textit{tempered}$ function if there exists $N \in \mathbb{N}$ such that $$\int_{\mathbb{R}} |f(x)| \, (1+|x|)^{-N} < \infty.$$ Then it is known that the $\textit{tempered distribution}$ induced by $f$, $$T_f: \mathcal{S}(\mathbb{R}) \to \mathbb{C}$$ $$T_f(\phi) = \int_{\mathbb{R}} f(x) \, \phi(x) \, dx$$ is indeed a tempered distribution, where $\mathcal{S}(\mathbb{R})$ is the space of Schwartz functions and a tempered distribution has the property that if $\phi_j \to 0$ in $\mathcal{S}(\mathbb{R})$, then $T_f(\phi_j) \to 0$ . I would like to show that the (a priori) tempered distribution  $$\delta: \mathcal{S}(\mathbb{R}) \to \mathbb{C}$$ $$\delta(\phi) = \phi(0)$$ is not induced by any tempered function $f$. My argument is intuitively clear to me, but the rigor in some steps, I am not sure. On the contrary, suppose that we have some $f$ so that $T_f = \delta$. Let $\varphi$ be some symmetric bump function such that $\mathrm{Supp}(\varphi) \subseteq [-1,1]$ and $\varphi(0) = 1$. Define a sequence $\varphi_j = \varphi(jx)$. Then for all $j \in \mathbb{N}$ we have $$\delta(\varphi_j)=\delta(\varphi(jx)) = \varphi(0) = 1.$$ Thus $\delta(\phi_j) \to 1$. On then other hand, since $\mathrm{Supp}(\varphi_j) = [-1/j, 1/j]$ we have $$T_f(\varphi_j) = \int_{-1/j}^{1/j} f(x) \varphi_j(x) dx$$ so that $T_f(\varphi_j) \to 0$. By uniqueness of limits we must have $T_f \neq \delta$.","Call a function $f: \mathbb{R} \to \mathbb{C}$ a $\textit{tempered}$ function if there exists $N \in \mathbb{N}$ such that $$\int_{\mathbb{R}} |f(x)| \, (1+|x|)^{-N} < \infty.$$ Then it is known that the $\textit{tempered distribution}$ induced by $f$, $$T_f: \mathcal{S}(\mathbb{R}) \to \mathbb{C}$$ $$T_f(\phi) = \int_{\mathbb{R}} f(x) \, \phi(x) \, dx$$ is indeed a tempered distribution, where $\mathcal{S}(\mathbb{R})$ is the space of Schwartz functions and a tempered distribution has the property that if $\phi_j \to 0$ in $\mathcal{S}(\mathbb{R})$, then $T_f(\phi_j) \to 0$ . I would like to show that the (a priori) tempered distribution  $$\delta: \mathcal{S}(\mathbb{R}) \to \mathbb{C}$$ $$\delta(\phi) = \phi(0)$$ is not induced by any tempered function $f$. My argument is intuitively clear to me, but the rigor in some steps, I am not sure. On the contrary, suppose that we have some $f$ so that $T_f = \delta$. Let $\varphi$ be some symmetric bump function such that $\mathrm{Supp}(\varphi) \subseteq [-1,1]$ and $\varphi(0) = 1$. Define a sequence $\varphi_j = \varphi(jx)$. Then for all $j \in \mathbb{N}$ we have $$\delta(\varphi_j)=\delta(\varphi(jx)) = \varphi(0) = 1.$$ Thus $\delta(\phi_j) \to 1$. On then other hand, since $\mathrm{Supp}(\varphi_j) = [-1/j, 1/j]$ we have $$T_f(\varphi_j) = \int_{-1/j}^{1/j} f(x) \varphi_j(x) dx$$ so that $T_f(\varphi_j) \to 0$. By uniqueness of limits we must have $T_f \neq \delta$.",,"['functional-analysis', 'fourier-analysis', 'sobolev-spaces', 'distribution-theory', 'harmonic-analysis']"
24,Range of an integral operator,Range of an integral operator,,"I tried to determine the range of the integral operator $T\in\mathcal{L}(L^2(0,1))$ given by $$(Tx)(t) = \int^1_t x(s)ds \text{ for } x \in L^2(0,1), t\in[0,1]$$ but it doesn't sound very logical. This is my approach to the solution: \begin{align*} y(t) = Tx(t) &= \int^1_t x(s)ds\\              &= \alpha - \int^t_0 x(s)ds ~~~~~(\alpha = \int^1_0 x(s)ds<\infty) \\ \Rightarrow -y'&=x \end{align*} Then \begin{align*} (Tx)(0) = \alpha <\infty~~~~~ &, ~~~~~(Tx)(1) = 0\\ (Tx)(t) = -(Ty')(t) &= \int^t_1 y'(s)ds = y(t) - y(1)  \end{align*} if the integral exist, i.e. $y'(s)$ exists, $y' \in L^2(0,1)$ and $y(1) = 0$. Therefore $$\mathcal{R}(T) = \lbrace y \in H^1(0,1)~ \vert ~y(1) = 0\rbrace$$","I tried to determine the range of the integral operator $T\in\mathcal{L}(L^2(0,1))$ given by $$(Tx)(t) = \int^1_t x(s)ds \text{ for } x \in L^2(0,1), t\in[0,1]$$ but it doesn't sound very logical. This is my approach to the solution: \begin{align*} y(t) = Tx(t) &= \int^1_t x(s)ds\\              &= \alpha - \int^t_0 x(s)ds ~~~~~(\alpha = \int^1_0 x(s)ds<\infty) \\ \Rightarrow -y'&=x \end{align*} Then \begin{align*} (Tx)(0) = \alpha <\infty~~~~~ &, ~~~~~(Tx)(1) = 0\\ (Tx)(t) = -(Ty')(t) &= \int^t_1 y'(s)ds = y(t) - y(1)  \end{align*} if the integral exist, i.e. $y'(s)$ exists, $y' \in L^2(0,1)$ and $y(1) = 0$. Therefore $$\mathcal{R}(T) = \lbrace y \in H^1(0,1)~ \vert ~y(1) = 0\rbrace$$",,"['functional-analysis', 'operator-theory', 'lebesgue-integral']"
25,"Let $G$ be a locally compact group with Haar measure $\mu$. Is right-translation continuous on $L^1(G,\mu)$?",Let  be a locally compact group with Haar measure . Is right-translation continuous on ?,"G \mu L^1(G,\mu)","Let $G$ be a locally compact group with right-invariant Haar measure $\mu$. I know that the space of compactly supported continuous functions $C_c(G)$ is dense in $L^1(G,\mu)$. If $G$ is 1st-countable, I can show that whenever $y_n\to y$ in $G$, we have that for any $\varphi\in C_c(G)$, if we let $\varphi_{y_n}(x)=\varphi(x y_n)$, then $$\varphi_{y_n}\xrightarrow[L^1(G,\mu)]{n\to\infty}\varphi_y$$ by dominated convergence, so that the map $$G\to L^1(G,\mu),\quad \phi\to\phi_y$$ is continuous. But is this still true if $G$ is not 1st countable?","Let $G$ be a locally compact group with right-invariant Haar measure $\mu$. I know that the space of compactly supported continuous functions $C_c(G)$ is dense in $L^1(G,\mu)$. If $G$ is 1st-countable, I can show that whenever $y_n\to y$ in $G$, we have that for any $\varphi\in C_c(G)$, if we let $\varphi_{y_n}(x)=\varphi(x y_n)$, then $$\varphi_{y_n}\xrightarrow[L^1(G,\mu)]{n\to\infty}\varphi_y$$ by dominated convergence, so that the map $$G\to L^1(G,\mu),\quad \phi\to\phi_y$$ is continuous. But is this still true if $G$ is not 1st countable?",,"['real-analysis', 'group-theory', 'functional-analysis', 'harmonic-analysis']"
26,Half quadratic splitting (alternating optimization with penalty),Half quadratic splitting (alternating optimization with penalty),,"Hello I'm working on an optimization problem: $$\hat{x}=\text{arg min}_{x} \frac{1}{2}\parallel y - Hx \parallel^{2} + \lambda \Phi (x), \quad x, y \in \mathbb{R}^{N\times M}.$$ where $H$ is a matrix and $\Phi$ an application. To solve this problem, my idea is to split in two subproblems like in ADMM ( http://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf ). The problem of minimizating the before equation is equivalent to $$\hat{x}=\text{arg min} _{x} \frac{1}{2}\parallel y - Hx \parallel^{2} + \lambda \Phi (z)\quad \text{s.t.}\quad z=x, \qquad z, x \in \mathbb{R}^{N\times M}.$$ The method of HQS (half splitting quadratic) seeks to minizmize the following cost function: $$L_{\mu}(x,z) = \frac{1}{2}\parallel y-Hx\parallel^{2} +\lambda \Phi(z) + \frac{\mu}{2} \parallel z-x\parallel^{2}, \qquad z, x \in \mathbb{R}^{N\times M}, \mu\in\mathbb{R^{+}} .$$ And we apply the HQS method, that solves it iterativetely: \begin{equation}  \left\{  \begin{aligned}  x_{k+1} &= \text{arg min} _{x} {\frac{1}{2}}\parallel {y-}Hx \parallel^{2} + \mu \parallel x-z_{k}\parallel^{2}, \\[1pt]    z_{k+1} &= \text{arg min} _{z} \frac{\mu}{2} \parallel z-x_{k+1}\parallel^{2} + \lambda \Phi (z). \end{aligned}   \right. \end{equation} My question is: Is this equivalent to the original problem? for convergence and obtaining a solution of the original problem, what do we have to supose about $\mu$? And the most important, which convergence results are there? Any textbook to learn this?","Hello I'm working on an optimization problem: $$\hat{x}=\text{arg min}_{x} \frac{1}{2}\parallel y - Hx \parallel^{2} + \lambda \Phi (x), \quad x, y \in \mathbb{R}^{N\times M}.$$ where $H$ is a matrix and $\Phi$ an application. To solve this problem, my idea is to split in two subproblems like in ADMM ( http://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf ). The problem of minimizating the before equation is equivalent to $$\hat{x}=\text{arg min} _{x} \frac{1}{2}\parallel y - Hx \parallel^{2} + \lambda \Phi (z)\quad \text{s.t.}\quad z=x, \qquad z, x \in \mathbb{R}^{N\times M}.$$ The method of HQS (half splitting quadratic) seeks to minizmize the following cost function: $$L_{\mu}(x,z) = \frac{1}{2}\parallel y-Hx\parallel^{2} +\lambda \Phi(z) + \frac{\mu}{2} \parallel z-x\parallel^{2}, \qquad z, x \in \mathbb{R}^{N\times M}, \mu\in\mathbb{R^{+}} .$$ And we apply the HQS method, that solves it iterativetely: \begin{equation}  \left\{  \begin{aligned}  x_{k+1} &= \text{arg min} _{x} {\frac{1}{2}}\parallel {y-}Hx \parallel^{2} + \mu \parallel x-z_{k}\parallel^{2}, \\[1pt]    z_{k+1} &= \text{arg min} _{z} \frac{\mu}{2} \parallel z-x_{k+1}\parallel^{2} + \lambda \Phi (z). \end{aligned}   \right. \end{equation} My question is: Is this equivalent to the original problem? for convergence and obtaining a solution of the original problem, what do we have to supose about $\mu$? And the most important, which convergence results are there? Any textbook to learn this?",,"['functional-analysis', 'optimization', 'lagrange-multiplier']"
27,Completeness of Continuous Functions on Uniform Spaces,Completeness of Continuous Functions on Uniform Spaces,,"So I'm trying to find the most general setting in which I can talk about completeness of function spaces. In metric spaces, it's simple to show that the space $C(X,Y)$ of continuous functions $X\longrightarrow Y$ is complete whenever $X$ is compact and $Y$ is complete. I have a hunch that $C(X,Y)$ is complete if $X$ is compact and $Y,\mathcal U$ is a complete uniform space. First of all, we can generate a uniform structure $\mathcal F$ on $C(X,Y)$ by declaring that for each entourage $U\in\mathcal U$, there is an entourage $F\in\mathcal F$ defined by  $$(f,g)\in F \iff \forall x\in X, (f(x),g(x))\in U.$$ (Note: I am open to the fact that another formulation of uniform spaces might be more appropriate here, particularly the pseudometric formulation. Feel free to demonstrate this!) My questions are: (i) under what conditions is $C(X,Y)$ complete, and (ii) does the uniform topology on $C(X,Y)$ coincide with the compact-open topology?","So I'm trying to find the most general setting in which I can talk about completeness of function spaces. In metric spaces, it's simple to show that the space $C(X,Y)$ of continuous functions $X\longrightarrow Y$ is complete whenever $X$ is compact and $Y$ is complete. I have a hunch that $C(X,Y)$ is complete if $X$ is compact and $Y,\mathcal U$ is a complete uniform space. First of all, we can generate a uniform structure $\mathcal F$ on $C(X,Y)$ by declaring that for each entourage $U\in\mathcal U$, there is an entourage $F\in\mathcal F$ defined by  $$(f,g)\in F \iff \forall x\in X, (f(x),g(x))\in U.$$ (Note: I am open to the fact that another formulation of uniform spaces might be more appropriate here, particularly the pseudometric formulation. Feel free to demonstrate this!) My questions are: (i) under what conditions is $C(X,Y)$ complete, and (ii) does the uniform topology on $C(X,Y)$ coincide with the compact-open topology?",,"['general-topology', 'functional-analysis', 'metric-spaces', 'uniform-spaces']"
28,Question about linear bounded operator in Banach spaces.,Question about linear bounded operator in Banach spaces.,,"Let $T:X\to Y$ a linear bounded operator between Banach spaces. Let $U$ a neighbourhood of $0\in Y$, $t\in(0,1)$ and suppose that $\forall u\in U$ $\exists \bar x\in X$ with $\|\bar x\|\le1$ and $\bar u\in U$ such that $$u=T\bar x +t\bar u.$$ Then if I take $u\in U$, I can write $$u=T\left(\sum_{i=0}^{\infty} t^ix_i\right), \ \ \ \|x_i\|\le1$$ and that series has sense since $$\|\sum_{i=0}^{\infty} t^ix_i\|\le \sum_{i=0}^{\infty} t^i=C<+\infty.$$ In this way I obtain that $$U\subset T\left(B_X(0,C)\right).$$ Is it correct?","Let $T:X\to Y$ a linear bounded operator between Banach spaces. Let $U$ a neighbourhood of $0\in Y$, $t\in(0,1)$ and suppose that $\forall u\in U$ $\exists \bar x\in X$ with $\|\bar x\|\le1$ and $\bar u\in U$ such that $$u=T\bar x +t\bar u.$$ Then if I take $u\in U$, I can write $$u=T\left(\sum_{i=0}^{\infty} t^ix_i\right), \ \ \ \|x_i\|\le1$$ and that series has sense since $$\|\sum_{i=0}^{\infty} t^ix_i\|\le \sum_{i=0}^{\infty} t^i=C<+\infty.$$ In this way I obtain that $$U\subset T\left(B_X(0,C)\right).$$ Is it correct?",,"['functional-analysis', 'banach-spaces']"
29,Hausdorff methods of summation,Hausdorff methods of summation,,"From the book of Boss ""Classical and modern methods in summability"": ""The class of Hausdorff methods includes the Hölder, Cesaro and Euler methods. A large number of other matrix methods which play an essential role in summability are Hausdorff methods too."" I wonder if there is any well-known example of Hausdorff methods? For example, circle methods such as Meyer-Konig or Taylor is an Hausdorff method?","From the book of Boss ""Classical and modern methods in summability"": ""The class of Hausdorff methods includes the Hölder, Cesaro and Euler methods. A large number of other matrix methods which play an essential role in summability are Hausdorff methods too."" I wonder if there is any well-known example of Hausdorff methods? For example, circle methods such as Meyer-Konig or Taylor is an Hausdorff method?",,"['sequences-and-series', 'functional-analysis', 'reference-request', 'summation', 'summation-method']"
30,How do I use Fubini to show a set has the measure 0?,How do I use Fubini to show a set has the measure 0?,,"I am trying to show, that for fixed $v$ and $y \in \mathbb{R}^n$ the intersection of $L_y = \{y+tv ; t \in \mathbb{R}\}$ and a set E with  n-dimensional measure 0, has 1-dimensional measure 0 for almost every $y \in \mathbb{R}^n$. Since $L_y$ is a line the intersection $L_y \cap E$ is one dimensional. I am supposed to use Fubini's theoreme for the proof. My Idea was to define $\varphi : \mathbb{R}^n \to \mathbb{R}$ with $\varphi(y) := \lambda^1[L_y \cap E ] = \int_\mathbb{R} \chi_{E}(y+tv) dt$   and  $\psi:\mathbb{R}^{n-1} \to \mathbb{R}$ with $ \psi(y) := \lambda^1[L_y \cap E] = \int_\mathbb{R} \chi_{E }(y+tv) \ dt.$ Than we know: $ 0=\lambda^n[E] = \int_{\mathbb{R}^n} \chi_{E } \ d\lambda^n = \int_{\mathbb{R}^{n-1}}\int_\mathbb{R} \chi_{E}(y+tv)  dtdy = \int_{\mathbb{R}^{n-1}} \psi(y)  dy$. Now we know that $\psi = 0$ almost everywhere on $\mathbb{R}^{n-1}$. I want to use Fubini again to show that $\varphi = 0$ almost everywhere on $\mathbb{R}^{n}$ Edit: Therefore I define a function $h :\mathbb{R}^{n-1} \times \mathbb{R}$ by $h(x, z):= \varphi(y)$ for $(x, z) =y$ Now $\int_{\mathbb{R}^n} \varphi \ d\lambda^n = \int_{\mathbb{R}^n} h(x,z) \ d\lambda^n\overset{\text{Fubini}}{=} \int_\mathbb{R}\int_{\mathbb{R}^{n-1}}  h(x,z) \ dx dz = 0$ Because for fixed z $h(x,z) = \psi(x)$.","I am trying to show, that for fixed $v$ and $y \in \mathbb{R}^n$ the intersection of $L_y = \{y+tv ; t \in \mathbb{R}\}$ and a set E with  n-dimensional measure 0, has 1-dimensional measure 0 for almost every $y \in \mathbb{R}^n$. Since $L_y$ is a line the intersection $L_y \cap E$ is one dimensional. I am supposed to use Fubini's theoreme for the proof. My Idea was to define $\varphi : \mathbb{R}^n \to \mathbb{R}$ with $\varphi(y) := \lambda^1[L_y \cap E ] = \int_\mathbb{R} \chi_{E}(y+tv) dt$   and  $\psi:\mathbb{R}^{n-1} \to \mathbb{R}$ with $ \psi(y) := \lambda^1[L_y \cap E] = \int_\mathbb{R} \chi_{E }(y+tv) \ dt.$ Than we know: $ 0=\lambda^n[E] = \int_{\mathbb{R}^n} \chi_{E } \ d\lambda^n = \int_{\mathbb{R}^{n-1}}\int_\mathbb{R} \chi_{E}(y+tv)  dtdy = \int_{\mathbb{R}^{n-1}} \psi(y)  dy$. Now we know that $\psi = 0$ almost everywhere on $\mathbb{R}^{n-1}$. I want to use Fubini again to show that $\varphi = 0$ almost everywhere on $\mathbb{R}^{n}$ Edit: Therefore I define a function $h :\mathbb{R}^{n-1} \times \mathbb{R}$ by $h(x, z):= \varphi(y)$ for $(x, z) =y$ Now $\int_{\mathbb{R}^n} \varphi \ d\lambda^n = \int_{\mathbb{R}^n} h(x,z) \ d\lambda^n\overset{\text{Fubini}}{=} \int_\mathbb{R}\int_{\mathbb{R}^{n-1}}  h(x,z) \ dx dz = 0$ Because for fixed z $h(x,z) = \psi(x)$.",,"['functional-analysis', 'measure-theory', 'lebesgue-integral']"
31,When is the pointwise convergence on $A$ equivalent to the uniform one?,When is the pointwise convergence on  equivalent to the uniform one?,A,"Let $A$ be a finite set of $[0,1]$ and $F$ a sub-vector-space of $C([0,1],\mathbb R^n)$. When is the pointwise convergence on $A$ equivalent to the uniform one. And with $A$ be countable? I suppose the finite dimension of $F$ is sufficient but is it necessary? Not sure for the second case. Is there a more general condition to have this equivalence (with any vector-space)?","Let $A$ be a finite set of $[0,1]$ and $F$ a sub-vector-space of $C([0,1],\mathbb R^n)$. When is the pointwise convergence on $A$ equivalent to the uniform one. And with $A$ be countable? I suppose the finite dimension of $F$ is sufficient but is it necessary? Not sure for the second case. Is there a more general condition to have this equivalence (with any vector-space)?",,"['general-topology', 'functional-analysis']"
32,Versions of Lusin theorem,Versions of Lusin theorem,,"I found this two version of Lusin theorem, without reference Can someone know in any book i can found it : Theorem 1: Let $(X,\beta,\mu)$ a measurable space satisfying: $\bullet$ $X$ is a metric space locally compact, $\bullet$  $\beta$ is a borelian  $\sigma-$algebra. $\bullet$ $\mu$ is a regular measure. Suppose that $A\in \beta$ such that $\mu(A)<+\infty.$ Then, given   $\varepsilon>0$ there exists $g\in C_0(X)$ such that $\bullet$ $\mu(\{x\in X; g(x)\neq \chi_{A}(x)\})<\varepsilon;$ $\bullet$ $g(x)\in [0,1], x\in X.$ and the second: Theorem 2: $(i)$ $Y$ be a Hausdorff topological space with a countable  bases; $(ii)$ $(\Omega,\beta,\mu)$ a finite measurable space, where    $\Omega\subset Y$; $(iii)$ $f:\Omega\to \mathbb{R}$ a measurable  function.    Given   $\varepsilon>0$, there exists a compact set $K\subset \Omega$  such   that $(I)$ $f$ is continuous on $K$; $(II)$  $\mu(\Omega\setminus K)<\varepsilon.$","I found this two version of Lusin theorem, without reference Can someone know in any book i can found it : Theorem 1: Let $(X,\beta,\mu)$ a measurable space satisfying: $\bullet$ $X$ is a metric space locally compact, $\bullet$  $\beta$ is a borelian  $\sigma-$algebra. $\bullet$ $\mu$ is a regular measure. Suppose that $A\in \beta$ such that $\mu(A)<+\infty.$ Then, given   $\varepsilon>0$ there exists $g\in C_0(X)$ such that $\bullet$ $\mu(\{x\in X; g(x)\neq \chi_{A}(x)\})<\varepsilon;$ $\bullet$ $g(x)\in [0,1], x\in X.$ and the second: Theorem 2: $(i)$ $Y$ be a Hausdorff topological space with a countable  bases; $(ii)$ $(\Omega,\beta,\mu)$ a finite measurable space, where    $\Omega\subset Y$; $(iii)$ $f:\Omega\to \mathbb{R}$ a measurable  function.    Given   $\varepsilon>0$, there exists a compact set $K\subset \Omega$  such   that $(I)$ $f$ is continuous on $K$; $(II)$  $\mu(\Omega\setminus K)<\varepsilon.$",,"['functional-analysis', 'analysis', 'measure-theory', 'reference-request', 'borel-measures']"
33,Which theorem tells us that RKHS is dense in L2?,Which theorem tells us that RKHS is dense in L2?,,"I have heard this phrase quite a lot now, that RKHS is dense in the space of bounded continuous functions ($\mathcal L_2$). For example, this would be true with $$f(x) = \sum_{i=1}^N \alpha_i K(x_i,x)$$ for some $\alpha_i$, and $K$ the RBF kernel $$K(x,y) = \text{exp}\left(-\frac{\|x-y\|_2^2}{\sigma}\right)$$ I guess this also means for any $x_i$ as well, or for infinite $N$. But which theorem / proof is this referring to? Some options: Mercer's representation theorem: $K(x,y)$ can be written as a linear combination of $\phi(x)\phi(y)$ where $\phi$ are the eigenfunctions of $K$ Riesz representer theorem: If $H$ is an RKHS, there exists a positive definite kernel function $K$ in which all functions in $H$ can be written this way. Moore–Aronszajn: Any positive definite function  $K$ defines an RKHS. None of these quite seem like what I need though... Edit: Here is a promising cuprit! The nonparametric representor theorem in Scholkopf, Herbrich, and Smola:  A Generalized Representer Theorem (2001) The nonparametric version gives the functional class $$\mathcal F = \left\{f | f(x) = \sum_{i=1}^\infty \beta_i k(x,z_i), \beta_i\in \mathbb R, z_i \in \mathcal X \subseteq \mathbb R^n, \|f\|< \infty \right\}$$ Then any $f\in \mathcal F$ minimizing some risk function with regularization $g(\|f\|)$ (for any monotonic $g$) has a representation $$f(x) = \sum_{i=1}^n \alpha_i K(x_i,x).$$ However, it seems a nontrivial extension is needed to show that $\mathcal F$ is dense in $\mathcal L_2$.","I have heard this phrase quite a lot now, that RKHS is dense in the space of bounded continuous functions ($\mathcal L_2$). For example, this would be true with $$f(x) = \sum_{i=1}^N \alpha_i K(x_i,x)$$ for some $\alpha_i$, and $K$ the RBF kernel $$K(x,y) = \text{exp}\left(-\frac{\|x-y\|_2^2}{\sigma}\right)$$ I guess this also means for any $x_i$ as well, or for infinite $N$. But which theorem / proof is this referring to? Some options: Mercer's representation theorem: $K(x,y)$ can be written as a linear combination of $\phi(x)\phi(y)$ where $\phi$ are the eigenfunctions of $K$ Riesz representer theorem: If $H$ is an RKHS, there exists a positive definite kernel function $K$ in which all functions in $H$ can be written this way. Moore–Aronszajn: Any positive definite function  $K$ defines an RKHS. None of these quite seem like what I need though... Edit: Here is a promising cuprit! The nonparametric representor theorem in Scholkopf, Herbrich, and Smola:  A Generalized Representer Theorem (2001) The nonparametric version gives the functional class $$\mathcal F = \left\{f | f(x) = \sum_{i=1}^\infty \beta_i k(x,z_i), \beta_i\in \mathbb R, z_i \in \mathcal X \subseteq \mathbb R^n, \|f\|< \infty \right\}$$ Then any $f\in \mathcal F$ minimizing some risk function with regularization $g(\|f\|)$ (for any monotonic $g$) has a representation $$f(x) = \sum_{i=1}^n \alpha_i K(x_i,x).$$ However, it seems a nontrivial extension is needed to show that $\mathcal F$ is dense in $\mathcal L_2$.",,"['functional-analysis', 'reproducing-kernel-hilbert-spaces']"
34,Equivalent norms in polynomial space,Equivalent norms in polynomial space,,"Let $\mathbb{P}_k(\mathbb{R}^d)$ be the space of multivariate polynomials of degree $\le k$ defined on $\mathbb{R}^d$. Let $Q=[0,1]^d$ and let $B(0,2)$ be the ball centred at the original with radius 2. Using the fact that $\mathbb{P}_k(\mathbb{R}^d)$ is a finite dimensional vector space and that polynomials are real analytic functions (hence they admit unique continuation), we know \begin{equation} c\|p \|_{L^p(Q)}\le \|p \|_{L^p(B(0,2))}\le C \|p \|_{L^p(Q)} \end{equation} for $1\le p \le \infty$, for every $p\in\mathbb{P}_k(\mathbb{R}^d)$. I'm wondering on which parameters does $C$ depend.","Let $\mathbb{P}_k(\mathbb{R}^d)$ be the space of multivariate polynomials of degree $\le k$ defined on $\mathbb{R}^d$. Let $Q=[0,1]^d$ and let $B(0,2)$ be the ball centred at the original with radius 2. Using the fact that $\mathbb{P}_k(\mathbb{R}^d)$ is a finite dimensional vector space and that polynomials are real analytic functions (hence they admit unique continuation), we know \begin{equation} c\|p \|_{L^p(Q)}\le \|p \|_{L^p(B(0,2))}\le C \|p \|_{L^p(Q)} \end{equation} for $1\le p \le \infty$, for every $p\in\mathbb{P}_k(\mathbb{R}^d)$. I'm wondering on which parameters does $C$ depend.",,"['functional-analysis', 'inequality', 'polynomials', 'lp-spaces', 'normed-spaces']"
35,Yang-Mills energy gap in dimension 2,Yang-Mills energy gap in dimension 2,,"Let $(\Sigma,g)$ be a smooth genus $k$ closed oriented surface endowed with a Riemannian metric $g$. Let $P_\Sigma$ be a (necessarily) trivial $\mathrm{SU}(2)$-principal bundle over $\Sigma$. Let $\mathcal{A}_\Sigma$ be the space of connexions on $P_\Sigma$. Define $S:\mathcal{A}_\Sigma\to \mathbb R_{\geq 0}$ be the Yang-Mills functional $S(A):=\frac12\|F_A\|^2_{L^2}$ where $F_A\in \Omega^2(\Sigma;\mathrm{AdP}_\Sigma)$ is $A$'s curvature 2-form. It is well known that the critical set of $S$ is given by : $$ \mathrm{crit}(S) = \{A\in \mathcal{A}_\Sigma | \delta_A F_A = 0\} $$ where $\delta_A$ is the covariant codifferential (a.k.a $\mathrm{d}_A^*$ the $L^2$-adjoint of the covariant exterior derivative $\mathrm{d}_A$). Question : is there an $m\in \mathbb R_{>0}$ such that $$ \mathrm{crit}(S) \cap \{A\in \mathcal{A}_\Sigma | 0<S(A)<m\} = \emptyset \qquad \text{?} $$","Let $(\Sigma,g)$ be a smooth genus $k$ closed oriented surface endowed with a Riemannian metric $g$. Let $P_\Sigma$ be a (necessarily) trivial $\mathrm{SU}(2)$-principal bundle over $\Sigma$. Let $\mathcal{A}_\Sigma$ be the space of connexions on $P_\Sigma$. Define $S:\mathcal{A}_\Sigma\to \mathbb R_{\geq 0}$ be the Yang-Mills functional $S(A):=\frac12\|F_A\|^2_{L^2}$ where $F_A\in \Omega^2(\Sigma;\mathrm{AdP}_\Sigma)$ is $A$'s curvature 2-form. It is well known that the critical set of $S$ is given by : $$ \mathrm{crit}(S) = \{A\in \mathcal{A}_\Sigma | \delta_A F_A = 0\} $$ where $\delta_A$ is the covariant codifferential (a.k.a $\mathrm{d}_A^*$ the $L^2$-adjoint of the covariant exterior derivative $\mathrm{d}_A$). Question : is there an $m\in \mathbb R_{>0}$ such that $$ \mathrm{crit}(S) \cap \{A\in \mathcal{A}_\Sigma | 0<S(A)<m\} = \emptyset \qquad \text{?} $$",,"['functional-analysis', 'differential-geometry', 'gauge-theory']"
36,"Compact embedding in $L^2((0,\infty),rdr)$",Compact embedding in,"L^2((0,\infty),rdr)","Let $H$ be the closure of $C^{\infty}_0(0,\infty)$ with respect to the inner product $$(f,g)=\int_0^{\infty}\left(f'(r)g'(r)+h(r)f(r)g(r)\right)rdr,$$ and induced norm $||f||_H^2=(f,f)$, and where $h(r)=1/r^2+\lambda$ with $\lambda>0$. Is this space compactly embedded in $L^2((0,\infty),rdr)$? Here is what I am able to show: $H$ is continuously embedded in $L^2((0,\infty),rdr)$ and in $W^{1,2}((0,\infty),rdr)$, i.e.,  \begin{align} \int_0^{\infty}f^2(r)rdr\leq\int_0^{\infty}\left(f_r^2(r)+f^2(r)\right)rdr\leq \max\left\{1,\dfrac{1}{\lambda}\right\}||f||_H^2. \end{align} $H$ is embedded in $W_{loc}^{1,2}(0,\infty)$. So $f$ is continuous on $(0,\infty)$. Every $f\in H$ satisfies $f(0)=0$ and $f(\infty)=0$. Using $f(0)=0$, we have that $H$ is embedded in $L^{\infty}[0,\infty)$.","Let $H$ be the closure of $C^{\infty}_0(0,\infty)$ with respect to the inner product $$(f,g)=\int_0^{\infty}\left(f'(r)g'(r)+h(r)f(r)g(r)\right)rdr,$$ and induced norm $||f||_H^2=(f,f)$, and where $h(r)=1/r^2+\lambda$ with $\lambda>0$. Is this space compactly embedded in $L^2((0,\infty),rdr)$? Here is what I am able to show: $H$ is continuously embedded in $L^2((0,\infty),rdr)$ and in $W^{1,2}((0,\infty),rdr)$, i.e.,  \begin{align} \int_0^{\infty}f^2(r)rdr\leq\int_0^{\infty}\left(f_r^2(r)+f^2(r)\right)rdr\leq \max\left\{1,\dfrac{1}{\lambda}\right\}||f||_H^2. \end{align} $H$ is embedded in $W_{loc}^{1,2}(0,\infty)$. So $f$ is continuous on $(0,\infty)$. Every $f\in H$ satisfies $f(0)=0$ and $f(\infty)=0$. Using $f(0)=0$, we have that $H$ is embedded in $L^{\infty}[0,\infty)$.",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
37,"""Chains"" of Dual Spaces","""Chains"" of Dual Spaces",,"I have seen examples of the following: A sequence of duals extending forever$$S \subset S^*  \subset S^{**}  \subset S^{***} \subset ... \\ S = L^0 $$ A sequence of duals flipping between 2 spaces $$S \subset S^*  \subset S  \subset S^{*} \subset ... \\ S = L^p $$ A sequence of duals which is isometric to itself $$S = S^* \\ S = H $$ Are there sequences of duals that ""return"" every third space or something? Is there some ""deep"" algebraic fact that these are the only 3 possibilities?","I have seen examples of the following: A sequence of duals extending forever$$S \subset S^*  \subset S^{**}  \subset S^{***} \subset ... \\ S = L^0 $$ A sequence of duals flipping between 2 spaces $$S \subset S^*  \subset S  \subset S^{*} \subset ... \\ S = L^p $$ A sequence of duals which is isometric to itself $$S = S^* \\ S = H $$ Are there sequences of duals that ""return"" every third space or something? Is there some ""deep"" algebraic fact that these are the only 3 possibilities?",,"['functional-analysis', 'soft-question']"
38,Restriction of adjoint map on measure space,Restriction of adjoint map on measure space,,"I am stuck at a certain computation in solving a problem. The problem can be formulated as the following: Let $X, Y$ be locally compact spaces and $C_b(X)$ denotes the space of bounded continuous functions on $X$ and $M(X)$ denotes the space of regular complex Borel measures on $X$. We have a linear map  $T: C_b(Y) \rightarrow C_b(X)$. Then we can get the adjoint map $T^*: C_b(X)^* \rightarrow C_b(Y)^*$ given by $$T^*(m)(f) = m(Tf)$$ for each $m\in C_b(X)^*, f\in C_b(Y)$. We know that $M(X) \subset C_b(X)^*$. My question is the following: If for every $x\in X$, we have that $$T^*(\delta_x) = \delta_{x'}$$ for some $x'\in Y$ (here $\delta_x$ is the Dirac measure at the point $x$), then can we conclude that $$T^*({M(X)}) \subset M(Y) \ \ ?$$ I believe it should be true since $C_0(X)^* = M(X)$ and since $C_0(X) \subset C_b(X) = C(\tilde{X})$ where $\tilde{X}$ is the one-point compactification of $X$. But I am not sure if this is the correct line of thought or if I am missing something here. Any help is greatly appreciated !","I am stuck at a certain computation in solving a problem. The problem can be formulated as the following: Let $X, Y$ be locally compact spaces and $C_b(X)$ denotes the space of bounded continuous functions on $X$ and $M(X)$ denotes the space of regular complex Borel measures on $X$. We have a linear map  $T: C_b(Y) \rightarrow C_b(X)$. Then we can get the adjoint map $T^*: C_b(X)^* \rightarrow C_b(Y)^*$ given by $$T^*(m)(f) = m(Tf)$$ for each $m\in C_b(X)^*, f\in C_b(Y)$. We know that $M(X) \subset C_b(X)^*$. My question is the following: If for every $x\in X$, we have that $$T^*(\delta_x) = \delta_{x'}$$ for some $x'\in Y$ (here $\delta_x$ is the Dirac measure at the point $x$), then can we conclude that $$T^*({M(X)}) \subset M(Y) \ \ ?$$ I believe it should be true since $C_0(X)^* = M(X)$ and since $C_0(X) \subset C_b(X) = C(\tilde{X})$ where $\tilde{X}$ is the one-point compactification of $X$. But I am not sure if this is the correct line of thought or if I am missing something here. Any help is greatly appreciated !",,"['functional-analysis', 'measure-theory', 'operator-theory', 'adjoint-operators']"
39,Is the open mapping theorem true for all complete metrizable topological vector spaces?,Is the open mapping theorem true for all complete metrizable topological vector spaces?,,"Recall that the open mapping theorem is true for Banach spaces. Furthermore, it is also true for Frechet spaces, which are complete metric spaces(with extra properties). It makes me wonder whether we can say that open mapping theorem applies to all complete metrizable topological vector spaces, since the key ingredient Baire category theorem certainly applies to all complete metric spaces and the norm in the proof of Banach spaces can be adapted to metric hopefully.","Recall that the open mapping theorem is true for Banach spaces. Furthermore, it is also true for Frechet spaces, which are complete metric spaces(with extra properties). It makes me wonder whether we can say that open mapping theorem applies to all complete metrizable topological vector spaces, since the key ingredient Baire category theorem certainly applies to all complete metric spaces and the norm in the proof of Banach spaces can be adapted to metric hopefully.",,['functional-analysis']
40,Prove $(\oplus_{n=1}^{\infty}\ell_1^n)_{\ell_2}$ cannot admit a norm which is uniformly convex.,Prove  cannot admit a norm which is uniformly convex.,(\oplus_{n=1}^{\infty}\ell_1^n)_{\ell_2},"Just as the title, prove $(\oplus_{n=1}^{\infty}\ell_1^n)_{\ell_2}$ cannot admit a norm which is uniformly convex. I find in the answer of A reflexive space which does not have an equivalent uniformly convex norm , a user noticed using Enflo's theorem, but he did not mention what this theorem says, and I cannot find a version of this theorem on the internet.","Just as the title, prove $(\oplus_{n=1}^{\infty}\ell_1^n)_{\ell_2}$ cannot admit a norm which is uniformly convex. I find in the answer of A reflexive space which does not have an equivalent uniformly convex norm , a user noticed using Enflo's theorem, but he did not mention what this theorem says, and I cannot find a version of this theorem on the internet.",,"['real-analysis', 'functional-analysis', 'banach-spaces', 'convex-geometry']"
41,Spectral decomposition of hilbert modules,Spectral decomposition of hilbert modules,,"Let $A$ be a $C^*$-algebra, $E$ a Hilbert $A$-module and $T:E\rightarrow E$ a bounded self-adjoint regular operator. Is there something like a spectral theorem for Hilbert modules that gives a decomposition of $E$ in relation to the spectrum of $T$? For example, is there an analogue of the spectral theorem for compact self-adjoint operators? Edit: clarified question with Martin's comments below.","Let $A$ be a $C^*$-algebra, $E$ a Hilbert $A$-module and $T:E\rightarrow E$ a bounded self-adjoint regular operator. Is there something like a spectral theorem for Hilbert modules that gives a decomposition of $E$ in relation to the spectrum of $T$? For example, is there an analogue of the spectral theorem for compact self-adjoint operators? Edit: clarified question with Martin's comments below.",,"['functional-analysis', 'operator-theory', 'operator-algebras', 'c-star-algebras', 'hilbert-modules']"
42,Elliptic pseudodifferential operator has at most finite number of non-positive eigenvalues,Elliptic pseudodifferential operator has at most finite number of non-positive eigenvalues,,Let $P \in OPS^m(M)$ ($m$ can be negative of positive) be a self-adjoint elliptic pseudodifferential operator on a compact manifold $M$. How to show that $P$ has only finitely many non-positive eigenvalues (in $L^2(M)$)? Does it somehow follow from the Gårding inequality?,Let $P \in OPS^m(M)$ ($m$ can be negative of positive) be a self-adjoint elliptic pseudodifferential operator on a compact manifold $M$. How to show that $P$ has only finitely many non-positive eigenvalues (in $L^2(M)$)? Does it somehow follow from the Gårding inequality?,,"['functional-analysis', 'spectral-theory', 'pseudo-differential-operators']"
43,Change of coordinates for a first order linear PDE,Change of coordinates for a first order linear PDE,,"Given a linear PDE of order 1 with $C^ \infty$ real coefficients in a neighborhood of the origin in $\mathbb{R}^n$ : $$L=\sum_{j=1}^{n}\alpha_j(x)\frac{\partial}{\partial x_j}$$ Suppose that at least one of the coefficients $\alpha_j$ does not vanish at the origin. Show that there is a $C^\infty$ change of variables $x \to y$ in a nbhd of the origin s.t. $L$ in y-coordinates is $w(y)\frac{\partial}{\partial y_1}$ with $w(0)\neq 0$. Is it always possible to choose the  coordinates y so as to have $w(y) \equiv 1$ near $y=0$ ? ATTEMPT:I have solved the above problem for the constant coefficients case that is $\alpha_j$ are constant. For the variable case, how do I proceed ?","Given a linear PDE of order 1 with $C^ \infty$ real coefficients in a neighborhood of the origin in $\mathbb{R}^n$ : $$L=\sum_{j=1}^{n}\alpha_j(x)\frac{\partial}{\partial x_j}$$ Suppose that at least one of the coefficients $\alpha_j$ does not vanish at the origin. Show that there is a $C^\infty$ change of variables $x \to y$ in a nbhd of the origin s.t. $L$ in y-coordinates is $w(y)\frac{\partial}{\partial y_1}$ with $w(0)\neq 0$. Is it always possible to choose the  coordinates y so as to have $w(y) \equiv 1$ near $y=0$ ? ATTEMPT:I have solved the above problem for the constant coefficients case that is $\alpha_j$ are constant. For the variable case, how do I proceed ?",,"['functional-analysis', 'partial-differential-equations', 'coordinate-systems']"
44,projection-valued measure and bounded self-adjoint operator,projection-valued measure and bounded self-adjoint operator,,"Let $E$ be a projection-valued measure on Borel sets of $\mathbb{R}$ and assume $E([-R,R]) = \text{id}$ for some $R$. For each $x\in \mathcal{H}$, define the Boreal measure $\nu_x(B) = \left\lVert E(B)x\right\rVert^2$. How to show that there exists a bounded self-adjoint operator $T$ in $\mathcal{H}$ such that $$\langle Tx,\, x\rangle = \int_\mathbb{R} \lambda \;dv_x(\lambda)$$","Let $E$ be a projection-valued measure on Borel sets of $\mathbb{R}$ and assume $E([-R,R]) = \text{id}$ for some $R$. For each $x\in \mathcal{H}$, define the Boreal measure $\nu_x(B) = \left\lVert E(B)x\right\rVert^2$. How to show that there exists a bounded self-adjoint operator $T$ in $\mathcal{H}$ such that $$\langle Tx,\, x\rangle = \int_\mathbb{R} \lambda \;dv_x(\lambda)$$",,"['functional-analysis', 'operator-theory', 'spectral-theory']"
45,Norm of K less than or equal to norm of $\phi$,Norm of K less than or equal to norm of,\phi,"The following problem has been presented to me: Let $k : [a, b] × [a, b] → \mathbb{F}$ be continuous, and consider the integral operator $K : C_{\mathbb{F}}[a, b] → C_{\mathbb{F}}[a, b]$, defined by $$ Kf(s) = \int_{a}^{b} k(s, t)f(t) dt \hspace{2cm} (s ∈ [a, b], f ∈ C_{\mathbb{F}}[a, b]) $$ Define $φ: [a, b] → \mathbb{R}$ by $$ φ(s) = \int_{a}^{b} |k(s, t)| dt (s ∈ [a, b]).$$ Prove that $||K|| \leq ||\phi||_{\infty}=\max_{s \in [a,b]}\{|\phi(s)|\}$. I have trouble with two parts of this problem: What does $||K||$ mean? Is it the supremum norm over both $f$ and $s$, such that $|K(f(s))|$ is greater than any other combination of $f$ and $s$ for $K$? When assuming that that is the case, I found: $$ |K(f(s))| \leq \int_{a}^{b} |k(s,t)||f(t)| dt $$ But how do I proceed? I need to get the $f(t)$ out of the integral somehow, I need to ""seperate"" the f(t) from the rest to obtain the supremum norm of $\phi$, but how do I do that? I know that $|f(x)| \leq \|f\|_{\infty}$, but when considering the supremum norm for $K$, the supremum norm of $f$ does not come into play, correct? Any and all help would be very much appreciated.","The following problem has been presented to me: Let $k : [a, b] × [a, b] → \mathbb{F}$ be continuous, and consider the integral operator $K : C_{\mathbb{F}}[a, b] → C_{\mathbb{F}}[a, b]$, defined by $$ Kf(s) = \int_{a}^{b} k(s, t)f(t) dt \hspace{2cm} (s ∈ [a, b], f ∈ C_{\mathbb{F}}[a, b]) $$ Define $φ: [a, b] → \mathbb{R}$ by $$ φ(s) = \int_{a}^{b} |k(s, t)| dt (s ∈ [a, b]).$$ Prove that $||K|| \leq ||\phi||_{\infty}=\max_{s \in [a,b]}\{|\phi(s)|\}$. I have trouble with two parts of this problem: What does $||K||$ mean? Is it the supremum norm over both $f$ and $s$, such that $|K(f(s))|$ is greater than any other combination of $f$ and $s$ for $K$? When assuming that that is the case, I found: $$ |K(f(s))| \leq \int_{a}^{b} |k(s,t)||f(t)| dt $$ But how do I proceed? I need to get the $f(t)$ out of the integral somehow, I need to ""seperate"" the f(t) from the rest to obtain the supremum norm of $\phi$, but how do I do that? I know that $|f(x)| \leq \|f\|_{\infty}$, but when considering the supremum norm for $K$, the supremum norm of $f$ does not come into play, correct? Any and all help would be very much appreciated.",,['functional-analysis']
46,"Original Proof of Riesz Representation Theorem for $C([0,1])^*$",Original Proof of Riesz Representation Theorem for,"C([0,1])^*","It is well known that Riesz Representation Theorem states that every positive linear functional $\Psi$ on $C_c(X),$ where $X$ is a locally compact Hausdorff space, can be realize as integration  $$\Psi(f)=\int_X f(x)d\mu(x)$$ for a unique regular Borel measure $\mu$ on $X.$ Question: In the Wiki article linked above, there is this sentence: The theorem is named for Frigyes Riesz (1909) who introduced it for continuous functions on the unit interval. I would like to know that how Riesz proved the statement himself. It would be good if someone can provide me a paper where he provided a proof.","It is well known that Riesz Representation Theorem states that every positive linear functional $\Psi$ on $C_c(X),$ where $X$ is a locally compact Hausdorff space, can be realize as integration  $$\Psi(f)=\int_X f(x)d\mu(x)$$ for a unique regular Borel measure $\mu$ on $X.$ Question: In the Wiki article linked above, there is this sentence: The theorem is named for Frigyes Riesz (1909) who introduced it for continuous functions on the unit interval. I would like to know that how Riesz proved the statement himself. It would be good if someone can provide me a paper where he provided a proof.",,"['functional-analysis', 'measure-theory', 'reference-request', 'duality-theorems', 'riesz-representation-theorem']"
47,Invariance of the domain of self-adjoint operator by nonlinear map.,Invariance of the domain of self-adjoint operator by nonlinear map.,,"Let $H$ be a Hilbert space and $N: D(N)\subset H \to H$ be some  self-adjoint operator with domain $D(N)$ dense in $H$ and spectrum  $$\sigma(N) \subset (-\infty, \alpha]$$ for some fixed $\alpha <0.$ We assume further that  $$e^{tN}(D(N))\subset D(N), \quad \forall t\in (0, 1].$$  Let now $g: H \to H$ such that $g(D(N)) \subset D(N)$ and  there exist $C,s>0$  $$\|g(u)\| \leq C\|u\|^s\, \forall u \in H. $$ Define  $$\psi(u):= \int_0^\infty e^{tN}g(tu)\, dt.$$ Can we  prove that $\psi (D(N))\subset D(N)$  and that  $$N\psi(u)= \int_0^\infty N e^{tN}g(tu)\, dt ?$$ If the answer on the above question is yes, can this result be generalised by taking $N$ to be normal with spectrum  $$\sigma(N) \subset \{\lambda \in \mathbb{C}: {\rm Re}(\lambda) \leq \alpha\}?$$ The assumption on $e^{tN}$ on the interval $(0,1]$ implies that  $$e^{tN}(D(N))\subset D(N), \quad \forall t\in (0, \infty).$$ Then I don't know if it makes sense to write  $$\psi(u) = \sum_{n =O}^{\infty}\int_{n}^{n+1} e^{tN}g(tu)\, dt$$ Thank you for any hint.","Let $H$ be a Hilbert space and $N: D(N)\subset H \to H$ be some  self-adjoint operator with domain $D(N)$ dense in $H$ and spectrum  $$\sigma(N) \subset (-\infty, \alpha]$$ for some fixed $\alpha <0.$ We assume further that  $$e^{tN}(D(N))\subset D(N), \quad \forall t\in (0, 1].$$  Let now $g: H \to H$ such that $g(D(N)) \subset D(N)$ and  there exist $C,s>0$  $$\|g(u)\| \leq C\|u\|^s\, \forall u \in H. $$ Define  $$\psi(u):= \int_0^\infty e^{tN}g(tu)\, dt.$$ Can we  prove that $\psi (D(N))\subset D(N)$  and that  $$N\psi(u)= \int_0^\infty N e^{tN}g(tu)\, dt ?$$ If the answer on the above question is yes, can this result be generalised by taking $N$ to be normal with spectrum  $$\sigma(N) \subset \{\lambda \in \mathbb{C}: {\rm Re}(\lambda) \leq \alpha\}?$$ The assumption on $e^{tN}$ on the interval $(0,1]$ implies that  $$e^{tN}(D(N))\subset D(N), \quad \forall t\in (0, \infty).$$ Then I don't know if it makes sense to write  $$\psi(u) = \sum_{n =O}^{\infty}\int_{n}^{n+1} e^{tN}g(tu)\, dt$$ Thank you for any hint.",,"['functional-analysis', 'operator-theory', 'operator-algebras']"
48,Separable Eigenfunctions,Separable Eigenfunctions,,"Consider a differential operators acting on differentiable functions of two real variables. Under what conditions does such an operator admit a basis of separable eigenfunctions, i.e. eigenfunctions of the form $f_n(x,y)= a_n(x) b_n(y)$? I've heard this is true when the operator is itself separable, i.e. $\hat{D} = \hat{A}_x \hat{B}_y$, where $A_x$ is a differential operator acting on the $x$ variable, and $B_y$ is a differential operator acting on the variable $y$. Is it true even if the operator to be hermitian? And how about the case $\hat{D}  = \hat{A}_x \hat{B}_y + \hat{F}_x \hat{G}_y$, where $\hat{F}$ and $\hat{G}$ are two more differential operators acting each on a single variable? Thanks for sharing any tips or references you may think of.","Consider a differential operators acting on differentiable functions of two real variables. Under what conditions does such an operator admit a basis of separable eigenfunctions, i.e. eigenfunctions of the form $f_n(x,y)= a_n(x) b_n(y)$? I've heard this is true when the operator is itself separable, i.e. $\hat{D} = \hat{A}_x \hat{B}_y$, where $A_x$ is a differential operator acting on the $x$ variable, and $B_y$ is a differential operator acting on the variable $y$. Is it true even if the operator to be hermitian? And how about the case $\hat{D}  = \hat{A}_x \hat{B}_y + \hat{F}_x \hat{G}_y$, where $\hat{F}$ and $\hat{G}$ are two more differential operators acting each on a single variable? Thanks for sharing any tips or references you may think of.",,"['functional-analysis', 'partial-differential-equations', 'operator-theory', 'operator-algebras', 'eigenfunctions']"
49,"Absolute value of a function in $W^{1,2}_0$?",Absolute value of a function in ?,"W^{1,2}_0","Let $u\in W^{1,2}_0(\Omega)$. Can we conclude that $|u|\in W^{1,2}_0(\Omega)$? Here $\Omega$ is an arbitrary area in $\mathbb{R}^n$. This should hold, can you help me proving this? I have issues with the not-necessary bounded domain and Lebesgue's Dominated Convergence Theorem.. Thanks","Let $u\in W^{1,2}_0(\Omega)$. Can we conclude that $|u|\in W^{1,2}_0(\Omega)$? Here $\Omega$ is an arbitrary area in $\mathbb{R}^n$. This should hold, can you help me proving this? I have issues with the not-necessary bounded domain and Lebesgue's Dominated Convergence Theorem.. Thanks",,"['real-analysis', 'functional-analysis', 'sobolev-spaces', 'weak-derivatives']"
50,Weak-to-weak continuous operator which is not norm-continuous,Weak-to-weak continuous operator which is not norm-continuous,,"Can one give a ""relatively easy"" example of a linear mapping $T\colon X\to X$ ($X$ a Banach space) which is a) weak-to-weak continuous b) weak*-to-weak* continuous ($X=Y^*$) but not norm-to-norm continuous (not bounded). This needs some choice I guess.","Can one give a ""relatively easy"" example of a linear mapping $T\colon X\to X$ ($X$ a Banach space) which is a) weak-to-weak continuous b) weak*-to-weak* continuous ($X=Y^*$) but not norm-to-norm continuous (not bounded). This needs some choice I guess.",,"['functional-analysis', 'banach-spaces']"
51,Term by term proof for a special case of the Baker-Campbell-Hausdorff formula,Term by term proof for a special case of the Baker-Campbell-Hausdorff formula,,"In physics, a typical situation one is confronted with is to compute the product of an exponential of operators. Let $A$ and $B$ be some (possibly unbounded) self adjoint operators on a Hilbert space $\mathcal{H}$, if the operators $A$ and $B$ satisfy $$ [A,B] = c I$$ for some $c \in \mathbb{C}$, then it follows that $$ e^A e^B = e^{A + B - \frac{1}{2} [A,B]} \ .$$ The usual ""proof"" in physics, is to consider a function defined by $$ F(t) = e^{- (A + B)t}e^{A t} e^{B t}$$ then take (strong) derivatives, and the conlusion follows from some additional calculations. The problem with this approach, and all other similar approaches, is that we are either assuming $A$ and $B$ to be bounded operators or, in the context of the actual BCH formula, to be elements of some Lie algebra. The problem is that almost all practical operators for which we need this formula, the operators $A$ and $B$ are unbounded. Because of the unboundedness, there are countless issues which cannot fully be resolved without additional regularity properties added to the operators. One such regularity ""trick"" is the usage of so-called analytic vectors. If $A$ is an unbounded self adjoint operator, we say that $\psi \in \mathcal{H}$ is an analytic vector of $A$ if $\psi \in \cap_{n \in \mathbb{N}} D(A^n)$, and $$ \sum_{n \geq 0} \frac{t^n || A^n \psi ||}{n!} < \infty \ .$$ In this case, one has $$ e^{i A t} \psi = \sum_{n \geq 0} \frac{(itA)^n}{n!} \psi \ .$$ The exponential here is defined via the spectral theorem for normal operators. To make a long story short, one eventually comes to the conclusion, that if $A$ and $B$ are self-adjoint operators satisfying $[A,B] = c I$, equipped with some additional conditions akin to the analytic vector condition, then we are able to conclude that $$ e^{i A t} e^{ i Bt} \psi = \lim_{N \to \infty}  \lim_{M \to \infty} \sum_{n=0}^N \sum_{m=0}^M \frac{(i A t)^n (i B t)^m}{n! m!} \psi \ .$$ Despite searching for this computation, I have not yet found anywhere where this might have been computed term by term. So, my question is how does one approach such a computation? Trying to write out the terms term by term, the individual terms quickly become too intractable, at least for me, to deal with. Any hints, comments, or references on this computation?","In physics, a typical situation one is confronted with is to compute the product of an exponential of operators. Let $A$ and $B$ be some (possibly unbounded) self adjoint operators on a Hilbert space $\mathcal{H}$, if the operators $A$ and $B$ satisfy $$ [A,B] = c I$$ for some $c \in \mathbb{C}$, then it follows that $$ e^A e^B = e^{A + B - \frac{1}{2} [A,B]} \ .$$ The usual ""proof"" in physics, is to consider a function defined by $$ F(t) = e^{- (A + B)t}e^{A t} e^{B t}$$ then take (strong) derivatives, and the conlusion follows from some additional calculations. The problem with this approach, and all other similar approaches, is that we are either assuming $A$ and $B$ to be bounded operators or, in the context of the actual BCH formula, to be elements of some Lie algebra. The problem is that almost all practical operators for which we need this formula, the operators $A$ and $B$ are unbounded. Because of the unboundedness, there are countless issues which cannot fully be resolved without additional regularity properties added to the operators. One such regularity ""trick"" is the usage of so-called analytic vectors. If $A$ is an unbounded self adjoint operator, we say that $\psi \in \mathcal{H}$ is an analytic vector of $A$ if $\psi \in \cap_{n \in \mathbb{N}} D(A^n)$, and $$ \sum_{n \geq 0} \frac{t^n || A^n \psi ||}{n!} < \infty \ .$$ In this case, one has $$ e^{i A t} \psi = \sum_{n \geq 0} \frac{(itA)^n}{n!} \psi \ .$$ The exponential here is defined via the spectral theorem for normal operators. To make a long story short, one eventually comes to the conclusion, that if $A$ and $B$ are self-adjoint operators satisfying $[A,B] = c I$, equipped with some additional conditions akin to the analytic vector condition, then we are able to conclude that $$ e^{i A t} e^{ i Bt} \psi = \lim_{N \to \infty}  \lim_{M \to \infty} \sum_{n=0}^N \sum_{m=0}^M \frac{(i A t)^n (i B t)^m}{n! m!} \psi \ .$$ Despite searching for this computation, I have not yet found anywhere where this might have been computed term by term. So, my question is how does one approach such a computation? Trying to write out the terms term by term, the individual terms quickly become too intractable, at least for me, to deal with. Any hints, comments, or references on this computation?",,"['functional-analysis', 'mathematical-physics', 'unbounded-operators']"
52,Equivalence of hermitian forms under subgroups of $\textrm{GL}_n(\mathbb{C})$,Equivalence of hermitian forms under subgroups of,\textrm{GL}_n(\mathbb{C}),"Let $X \in \textrm{GL}_n(\mathbb{C})$ be a hermitian matrix ($\space ^t \overline{X} = X$).  For another hermitian matrix $Y$, let's say that $X \sim Y$ if there exists a $g \in \textrm{GL}_n(\mathbb{C})$ such that $gX \space ^t \overline{g} = Y$. I was wondering if there is anything in the literature about modifying the equivalence relation $\sim$ to restrict the $g$ to certain subgroups of $\textrm{GL}_n(\mathbb{C})$.  Specifically, I want to know if a complete set of representatives for the equivalence classes are known if we restrict $g$ to be an upper triangular unipotent matrix. More generally, I'm interested in the following problem: let $\sigma$ be an automorphism of a field $E$ with $\sigma^2 = 1_E$.  Give a complete set of representatives for the equivalence classes of $\{ X \in \textrm{GL}_n(E) : \space ^t \overline{X} = X \}$ under the relation: $X \sim Y$ if and only if there exists an upper triangular unipotent matrix $g \in \textrm{GL}_n(E)$ such that $gX \space ^t \overline{g} = Y$. If the case $E = \mathbb{C}$ has been done before, understanding that may be a good start for an investigation into arbitrary fields. Special case $\textrm{GL}_2(\mathbb{C})$: Let $X = \begin{pmatrix} x & y \\ \overline{y} & w \end{pmatrix}$ be a Hermitian matrix.  So $x, w \in \mathbb{R}$ and $xw \neq |y|^2$.  For $a \in \mathbb{C}$, we have $$\begin{pmatrix} 1 & a \\ 0 & 1 \end{pmatrix} \begin{pmatrix} x & y \\ \overline{y} & w \end{pmatrix} \begin{pmatrix} 1 & 0 \\ \overline{a} & 1 \end{pmatrix} = \begin{pmatrix} x + 2 \textrm{Re}(ay) + w |a|^2 & y + aw \\ \overline{y + aw} & w \end{pmatrix}$$","Let $X \in \textrm{GL}_n(\mathbb{C})$ be a hermitian matrix ($\space ^t \overline{X} = X$).  For another hermitian matrix $Y$, let's say that $X \sim Y$ if there exists a $g \in \textrm{GL}_n(\mathbb{C})$ such that $gX \space ^t \overline{g} = Y$. I was wondering if there is anything in the literature about modifying the equivalence relation $\sim$ to restrict the $g$ to certain subgroups of $\textrm{GL}_n(\mathbb{C})$.  Specifically, I want to know if a complete set of representatives for the equivalence classes are known if we restrict $g$ to be an upper triangular unipotent matrix. More generally, I'm interested in the following problem: let $\sigma$ be an automorphism of a field $E$ with $\sigma^2 = 1_E$.  Give a complete set of representatives for the equivalence classes of $\{ X \in \textrm{GL}_n(E) : \space ^t \overline{X} = X \}$ under the relation: $X \sim Y$ if and only if there exists an upper triangular unipotent matrix $g \in \textrm{GL}_n(E)$ such that $gX \space ^t \overline{g} = Y$. If the case $E = \mathbb{C}$ has been done before, understanding that may be a good start for an investigation into arbitrary fields. Special case $\textrm{GL}_2(\mathbb{C})$: Let $X = \begin{pmatrix} x & y \\ \overline{y} & w \end{pmatrix}$ be a Hermitian matrix.  So $x, w \in \mathbb{R}$ and $xw \neq |y|^2$.  For $a \in \mathbb{C}$, we have $$\begin{pmatrix} 1 & a \\ 0 & 1 \end{pmatrix} \begin{pmatrix} x & y \\ \overline{y} & w \end{pmatrix} \begin{pmatrix} 1 & 0 \\ \overline{a} & 1 \end{pmatrix} = \begin{pmatrix} x + 2 \textrm{Re}(ay) + w |a|^2 & y + aw \\ \overline{y + aw} & w \end{pmatrix}$$",,"['linear-algebra', 'functional-analysis', 'inner-products', 'algebraic-groups', 'bilinear-form']"
53,Embedding of some function spaces,Embedding of some function spaces,,"Consider the strictly monotone continuous function $d:\mathbb{R^+}\to\mathbb{R^+}$, and denote by $\mathcal{D}$ the space of all measurable functions $f:[0,1]\to\mathbb{R}$ such that: $$\int_0^1 d(|f(x)|) dx<\infty$$ I am wondering if there is any result on the (dense and continuous) embedding of this function space in the $L_p$ spaces?","Consider the strictly monotone continuous function $d:\mathbb{R^+}\to\mathbb{R^+}$, and denote by $\mathcal{D}$ the space of all measurable functions $f:[0,1]\to\mathbb{R}$ such that: $$\int_0^1 d(|f(x)|) dx<\infty$$ I am wondering if there is any result on the (dense and continuous) embedding of this function space in the $L_p$ spaces?",,"['real-analysis', 'functional-analysis', 'lebesgue-integral', 'lp-spaces', 'measurable-functions']"
54,"Linear map $K : C[0,1] \to C[0,1]$ is given as $(Kf)(x) = \int^1_0 k(x,y)f(y)\,dy$. Find $\|K\|$.",Linear map  is given as . Find .,"K : C[0,1] \to C[0,1] (Kf)(x) = \int^1_0 k(x,y)f(y)\,dy \|K\|","I'm solving the following problem. Let $(C[0,1], \|\cdot\|_\infty)$ be the space of real continuous functions equipped with the uniform norm. Let $k : [0,1]^2 \to \mathbb{R}$ be a nonzero continuous function. Define $K : C[0,1] \to C[0,1]$ as: $$(Kf)(x) = \int^1_0 k(x,y)f(y)\,dy, \quad f \in C[0,1]$$ Show that $K$ is bounded and compute $\|K\|$. This question has beed asked several times before: here and here . However, no answer actually constructs the sequence of functions which is needed to prove that $\|K\|\ge \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$, nor at least precisely and elementary specifies the density argument which was used. So I have written a complete proof with an explicit construction of the approximating functions. Could you verify it? $$\|Kf\|_\infty = \sup_{x\in [0,1]}\left|\int^1_0 k(x,y)f(y)\,dy\right| \le \sup_{x\in [0,1]} \int^1_0 |k(x,y)||f(y)|\,dy \\ \le \left(\sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy\right)\|f\|_\infty$$ Thus, $K$ is bounded and $\|K\|\le \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$. Let show that $\|K\|= \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$. Since $x \mapsto \int^1_0 |k(x,y)|\,dy$ is continuous, there exists $x_0 \in [0,1]$ such that $$\int^1_0 |k(x_0,y)|\,dy= \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$$ Now we $\DeclareMathOperator{\sgn}{sgn}$could take $f(y) = \sgn k(x_0,y)$ and notice that $\|f\|_\infty = 1$ and $$Kf = \int^1_0 |k(x_0,y)|\,dy$$ so we are done. However, $f$ is not necessarily continuous so let's try to approximate it with continuous functions. Let $\varepsilon > 0$, and $M = \max_{(x,y)\in [0,1]^2} |k(x,y)|$. Since $k(x_0, \cdot)$ is continuous, $k(x_0, \cdot)^{-1}(\langle 0, +\infty\rangle)$ is an open set so it can be written as a countable disjoint union of intervals: $$k(x_0, \cdot)^{-1}\big(\langle 0, +\infty\rangle\big) = \bigcup_{n=1}^\infty \langle a_n, b_n\rangle$$ Similarly: $$k(x_0, \cdot)^{-1}\big(\langle -\infty, 0\rangle\big) = \bigcup_{n=1}^\infty \langle c_n, d_n\rangle$$ Now define $$A = \bigcup_{n=1}^\infty \left\langle a_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\}, b_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\}\right\rangle \\ \cup \bigcup_{n=1}^\infty \left\langle c_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\}, d_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\}\right\rangle\\ \cup k(x_0, \cdot)^{-1}({0})$$ and define $f_\varepsilon \in C[0,1]$ to be equal to $\sgn k(x_0, \cdot)$ on $A$, and to an affine function on $[0,1]\setminus A$ connecting $\pm 1$ with $0$ so that $f_\varepsilon$ is continuous. Now we have: $$\begin{align}(Kf_\varepsilon)(x_0) &= \int_{[0,1]} k(x_0, y)f_\varepsilon(y)\,dy\\ &= \int_{A} \underbrace{k(x_0, y)f_\varepsilon(y)}_{=|k(x_0, y)|}\,dy + \underbrace{\int_{A^c} k(x_0, y)f_\varepsilon(y)\,dy}_{\geq 0}\\ &\geq \int_{A} |k(x_0, y)|\,dy\\ &= \int_{[0,1]} |k(x_0, y)|\,dy - \int_{A^c} |k(x_0, y)|\,dy\\ &\geq \int_{[0,1]} |k(x_0, y)|\,dy - M\lambda(A^c)\\ &\geq \int_{[0,1]} |k(x_0, y)|\,dy - \varepsilon\\ \end{align}$$ since $$\lambda(A^c) = \lambda\left( \bigcup_{n=1}^\infty \left[a_n, a_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\} \right] \cup \left[b_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\}, b_n \right] \right)\\ +\lambda\left( \bigcup_{n=1}^\infty \left[a_n, c_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\} \right] \cup \left[d_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\}, d_n \right] \right)\\ \le \sum_{n=1}^\infty \lambda\left(\left[a_n, a_n+\frac{\varepsilon}{2^{n+3}M}\right]\right) + \lambda\left(\left[b_n-\frac{\varepsilon}{2^{n+3}M}, b_n\right]\right) + \lambda\left(\left[c_n, c_n+\frac{\varepsilon}{2^{n+3}M}\right]\right) + \lambda\left(\left[d_n- \frac{\varepsilon}{2^{n+3}M}, d_n\right]\right)\\ =\frac{\varepsilon}{M}\sum_{n=1}^\infty\frac{1}{2^{n+1}} = \frac{\varepsilon}{M}$$ Thus: $$\|K\| \geq \frac{\|Kf_\varepsilon\|_\infty}{\|f_\varepsilon\|_\infty} \ge |(Kf)(x_0)| = \int_{[0,1]} |k(x_0, y)|\,dy - \varepsilon \xrightarrow{\varepsilon\to 0} \int_{[0,1]} |k(x_0, y)|\,dy$$ So, $\|K\| = \int_{[0,1]} |k(x_0, y)|\,dy$ follows. The situation would be similar (only easier) if $k(x_0, \cdot)$ had only finitely many points where it changes sign.","I'm solving the following problem. Let $(C[0,1], \|\cdot\|_\infty)$ be the space of real continuous functions equipped with the uniform norm. Let $k : [0,1]^2 \to \mathbb{R}$ be a nonzero continuous function. Define $K : C[0,1] \to C[0,1]$ as: $$(Kf)(x) = \int^1_0 k(x,y)f(y)\,dy, \quad f \in C[0,1]$$ Show that $K$ is bounded and compute $\|K\|$. This question has beed asked several times before: here and here . However, no answer actually constructs the sequence of functions which is needed to prove that $\|K\|\ge \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$, nor at least precisely and elementary specifies the density argument which was used. So I have written a complete proof with an explicit construction of the approximating functions. Could you verify it? $$\|Kf\|_\infty = \sup_{x\in [0,1]}\left|\int^1_0 k(x,y)f(y)\,dy\right| \le \sup_{x\in [0,1]} \int^1_0 |k(x,y)||f(y)|\,dy \\ \le \left(\sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy\right)\|f\|_\infty$$ Thus, $K$ is bounded and $\|K\|\le \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$. Let show that $\|K\|= \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$. Since $x \mapsto \int^1_0 |k(x,y)|\,dy$ is continuous, there exists $x_0 \in [0,1]$ such that $$\int^1_0 |k(x_0,y)|\,dy= \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$$ Now we $\DeclareMathOperator{\sgn}{sgn}$could take $f(y) = \sgn k(x_0,y)$ and notice that $\|f\|_\infty = 1$ and $$Kf = \int^1_0 |k(x_0,y)|\,dy$$ so we are done. However, $f$ is not necessarily continuous so let's try to approximate it with continuous functions. Let $\varepsilon > 0$, and $M = \max_{(x,y)\in [0,1]^2} |k(x,y)|$. Since $k(x_0, \cdot)$ is continuous, $k(x_0, \cdot)^{-1}(\langle 0, +\infty\rangle)$ is an open set so it can be written as a countable disjoint union of intervals: $$k(x_0, \cdot)^{-1}\big(\langle 0, +\infty\rangle\big) = \bigcup_{n=1}^\infty \langle a_n, b_n\rangle$$ Similarly: $$k(x_0, \cdot)^{-1}\big(\langle -\infty, 0\rangle\big) = \bigcup_{n=1}^\infty \langle c_n, d_n\rangle$$ Now define $$A = \bigcup_{n=1}^\infty \left\langle a_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\}, b_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\}\right\rangle \\ \cup \bigcup_{n=1}^\infty \left\langle c_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\}, d_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\}\right\rangle\\ \cup k(x_0, \cdot)^{-1}({0})$$ and define $f_\varepsilon \in C[0,1]$ to be equal to $\sgn k(x_0, \cdot)$ on $A$, and to an affine function on $[0,1]\setminus A$ connecting $\pm 1$ with $0$ so that $f_\varepsilon$ is continuous. Now we have: $$\begin{align}(Kf_\varepsilon)(x_0) &= \int_{[0,1]} k(x_0, y)f_\varepsilon(y)\,dy\\ &= \int_{A} \underbrace{k(x_0, y)f_\varepsilon(y)}_{=|k(x_0, y)|}\,dy + \underbrace{\int_{A^c} k(x_0, y)f_\varepsilon(y)\,dy}_{\geq 0}\\ &\geq \int_{A} |k(x_0, y)|\,dy\\ &= \int_{[0,1]} |k(x_0, y)|\,dy - \int_{A^c} |k(x_0, y)|\,dy\\ &\geq \int_{[0,1]} |k(x_0, y)|\,dy - M\lambda(A^c)\\ &\geq \int_{[0,1]} |k(x_0, y)|\,dy - \varepsilon\\ \end{align}$$ since $$\lambda(A^c) = \lambda\left( \bigcup_{n=1}^\infty \left[a_n, a_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\} \right] \cup \left[b_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\}, b_n \right] \right)\\ +\lambda\left( \bigcup_{n=1}^\infty \left[a_n, c_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\} \right] \cup \left[d_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\}, d_n \right] \right)\\ \le \sum_{n=1}^\infty \lambda\left(\left[a_n, a_n+\frac{\varepsilon}{2^{n+3}M}\right]\right) + \lambda\left(\left[b_n-\frac{\varepsilon}{2^{n+3}M}, b_n\right]\right) + \lambda\left(\left[c_n, c_n+\frac{\varepsilon}{2^{n+3}M}\right]\right) + \lambda\left(\left[d_n- \frac{\varepsilon}{2^{n+3}M}, d_n\right]\right)\\ =\frac{\varepsilon}{M}\sum_{n=1}^\infty\frac{1}{2^{n+1}} = \frac{\varepsilon}{M}$$ Thus: $$\|K\| \geq \frac{\|Kf_\varepsilon\|_\infty}{\|f_\varepsilon\|_\infty} \ge |(Kf)(x_0)| = \int_{[0,1]} |k(x_0, y)|\,dy - \varepsilon \xrightarrow{\varepsilon\to 0} \int_{[0,1]} |k(x_0, y)|\,dy$$ So, $\|K\| = \int_{[0,1]} |k(x_0, y)|\,dy$ follows. The situation would be similar (only easier) if $k(x_0, \cdot)$ had only finitely many points where it changes sign.",,"['functional-analysis', 'proof-verification', 'normed-spaces']"
55,Almost periodic function,Almost periodic function,,"I am trying to do this problem but could figure it out. Show that $f(x) = \cos 2\pi x +\cos 2\pi \sqrt{2} x$ is almost-periodic by showing directly   that given $\varepsilon > 0$ there exists an integer $M$ such that at least one of any $M$ consecutive integers lies within $\varepsilon$ from an integral multiple of $2\pi$. The definition of almost periodic is based on the notion of relatively dense set. A set $\mathcal{F}\subset \mathbb{R}$ is relatively dense in   $\mathbb{R}$ if there exists a constant $L>0$ such that $(x,x+L)\cap F  \neq \emptyset$ for all $x\in \mathbb{R}$. Then a bounded uniformly   continuous function $f:\mathbb{R}\longrightarrow \mathbb{R}$ is   ""almost periodic"" iff for any $\varepsilon >0$, the set  $$  \mathcal{F}_\varepsilon = \left\lbrace h\in \mathbb{R}:\sup_{x\in > \mathbb{R}} \left|f(x+h) - f(x)\right| <\varepsilon \right\rbrace$$ is   relatively dense in $\mathbb{R}$. My attempt is based on the observation that for $m\in \mathbb{Z}$ then  $$ f(x+m) - f(x) = \cos(2\pi \sqrt{2} x + 2\pi \sqrt{2}m) - \cos (2\pi \sqrt{2} x)$$ and thus $$ |f(x+m) - f(x)| \leq 2\pi\Big( \inf_{n\in \mathbb{Z}} \left|m\sqrt{2}+n\right| \Big)$$ We know that the set $\left\lbrace m\sqrt{2} + n: m,n\in \mathbb{Z}\right\rbrace$ is dense in $\mathbb{R}$ (by a simple Pidgeonhole argument). Thus the problem will be soved if we can show that for any $\varepsilon>0$, the set  $$ \mathcal{F} = \left\lbrace m\in  \mathbb{Z}: \;\text{there exists}\;n\in \mathbb{Z}\;\text{such that}\quad |m\sqrt{2}+n| < \varepsilon\right\rbrace$$ is ""relatively dense"" in $\mathbb{R}$, where relatively dense means there exists a constant $M_\varepsilon>0$ such that any interval $(x,x+M_\varepsilon)$ contains at least one element of $\mathcal{F}$. This somehow relates to the problem of approximation $\sqrt{2}$ by rational numbers $\frac{p}{q}$ but we want the set of $q$ is relatively dense. One well-known result is the Dirichlet approximation theorem, which states that  showing that any real number has a sequence of good rational approximations: in fact an immediate consequence is that for a given irrational $\alpha$, the inequality $$\left|\alpha - \frac{p}{q}\right| < \frac{1}{q^2}$$ is satisfied by infinitely many integers $p$ and $q$. But the problem how to control the distance between two consecutive denumerators still cannot be solved. Can anyone help me? Thank you very much.","I am trying to do this problem but could figure it out. Show that $f(x) = \cos 2\pi x +\cos 2\pi \sqrt{2} x$ is almost-periodic by showing directly   that given $\varepsilon > 0$ there exists an integer $M$ such that at least one of any $M$ consecutive integers lies within $\varepsilon$ from an integral multiple of $2\pi$. The definition of almost periodic is based on the notion of relatively dense set. A set $\mathcal{F}\subset \mathbb{R}$ is relatively dense in   $\mathbb{R}$ if there exists a constant $L>0$ such that $(x,x+L)\cap F  \neq \emptyset$ for all $x\in \mathbb{R}$. Then a bounded uniformly   continuous function $f:\mathbb{R}\longrightarrow \mathbb{R}$ is   ""almost periodic"" iff for any $\varepsilon >0$, the set  $$  \mathcal{F}_\varepsilon = \left\lbrace h\in \mathbb{R}:\sup_{x\in > \mathbb{R}} \left|f(x+h) - f(x)\right| <\varepsilon \right\rbrace$$ is   relatively dense in $\mathbb{R}$. My attempt is based on the observation that for $m\in \mathbb{Z}$ then  $$ f(x+m) - f(x) = \cos(2\pi \sqrt{2} x + 2\pi \sqrt{2}m) - \cos (2\pi \sqrt{2} x)$$ and thus $$ |f(x+m) - f(x)| \leq 2\pi\Big( \inf_{n\in \mathbb{Z}} \left|m\sqrt{2}+n\right| \Big)$$ We know that the set $\left\lbrace m\sqrt{2} + n: m,n\in \mathbb{Z}\right\rbrace$ is dense in $\mathbb{R}$ (by a simple Pidgeonhole argument). Thus the problem will be soved if we can show that for any $\varepsilon>0$, the set  $$ \mathcal{F} = \left\lbrace m\in  \mathbb{Z}: \;\text{there exists}\;n\in \mathbb{Z}\;\text{such that}\quad |m\sqrt{2}+n| < \varepsilon\right\rbrace$$ is ""relatively dense"" in $\mathbb{R}$, where relatively dense means there exists a constant $M_\varepsilon>0$ such that any interval $(x,x+M_\varepsilon)$ contains at least one element of $\mathcal{F}$. This somehow relates to the problem of approximation $\sqrt{2}$ by rational numbers $\frac{p}{q}$ but we want the set of $q$ is relatively dense. One well-known result is the Dirichlet approximation theorem, which states that  showing that any real number has a sequence of good rational approximations: in fact an immediate consequence is that for a given irrational $\alpha$, the inequality $$\left|\alpha - \frac{p}{q}\right| < \frac{1}{q^2}$$ is satisfied by infinitely many integers $p$ and $q$. But the problem how to control the distance between two consecutive denumerators still cannot be solved. Can anyone help me? Thank you very much.",,"['real-analysis', 'functional-analysis', 'number-theory', 'diophantine-approximation', 'almost-periodic-functions']"
56,Equivalent definition of $C^k(\overline \Omega)$,Equivalent definition of,C^k(\overline \Omega),"I have found some posts about equivalent definitions of $C^k(\overline \Omega)$: Equivalent definitions of $C^r(\Omega)$? Definition of convergence in $C^\infty(\Omega)$ Relationship among the function spaces $C_c^\infty(\Omega)$, $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Bbb{R}^d)$ However, they all haven't answered the following question: Let $\Omega$ be an open, bounded subset in $\mathbb{R}^N$. Denote   $$C^k(\overline{\Omega}):= \{ u \in C^k(\Omega): D^\alpha f \text{ is uniformly continuous on } \Omega, \text{ for all }|\alpha|\leq k \}$$   and   $$D^k(\overline {\Omega}):= \{ u: \text{ there exists }\Omega' \supset \overline{\Omega} \text{ and }g \in C^k(\Omega') \text{ such that }u = g|_{\Omega}\}.$$   Under which conditions (of $\Omega$), one has $C^k(\overline{\Omega)} = D^k(\overline{\Omega})$? Thanks for your helps.","I have found some posts about equivalent definitions of $C^k(\overline \Omega)$: Equivalent definitions of $C^r(\Omega)$? Definition of convergence in $C^\infty(\Omega)$ Relationship among the function spaces $C_c^\infty(\Omega)$, $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Bbb{R}^d)$ However, they all haven't answered the following question: Let $\Omega$ be an open, bounded subset in $\mathbb{R}^N$. Denote   $$C^k(\overline{\Omega}):= \{ u \in C^k(\Omega): D^\alpha f \text{ is uniformly continuous on } \Omega, \text{ for all }|\alpha|\leq k \}$$   and   $$D^k(\overline {\Omega}):= \{ u: \text{ there exists }\Omega' \supset \overline{\Omega} \text{ and }g \in C^k(\Omega') \text{ such that }u = g|_{\Omega}\}.$$   Under which conditions (of $\Omega$), one has $C^k(\overline{\Omega)} = D^k(\overline{\Omega})$? Thanks for your helps.",,"['real-analysis', 'functional-analysis']"
57,Any reference concerning factorization of partial isometries in $B(H)$,Any reference concerning factorization of partial isometries in,B(H),"Let $H$ be a Hilbert space. An operator $x\in B(H)$ is called a partial isometry if it is an isometry on Ker$x^{\perp}$. Two well-behaved classes of partial isometries are maximal partial isometries (i.e., isometries and co-isometries) and power partial isometries . Indeed we have that: ( Wold-decomposition theorem 1954. ) Every isometry is a direct sum of unitary and unilateral shifts. (Halmos and Wallen 1970. ) Every power partial isometry is a direct sum of unitary, isometry, co-isometry and truncated shifts. B. Fishel also obtained a characterization of those partial isometries which are sums of shifts: ( Fishel 1975. ) A necessary and sufficient condition that a partial isometry $x$ (without zero or unitary parts) be the orthogonal direct sum of left and right shifts is that $x^nxH^{\perp}\subseteq x^nH$ or  $x^{*n}xH^{\perp}\subseteq x^{*n}H$. Q. Does there exist any other factorization(s)  of a partial isometry into some nicer operators?","Let $H$ be a Hilbert space. An operator $x\in B(H)$ is called a partial isometry if it is an isometry on Ker$x^{\perp}$. Two well-behaved classes of partial isometries are maximal partial isometries (i.e., isometries and co-isometries) and power partial isometries . Indeed we have that: ( Wold-decomposition theorem 1954. ) Every isometry is a direct sum of unitary and unilateral shifts. (Halmos and Wallen 1970. ) Every power partial isometry is a direct sum of unitary, isometry, co-isometry and truncated shifts. B. Fishel also obtained a characterization of those partial isometries which are sums of shifts: ( Fishel 1975. ) A necessary and sufficient condition that a partial isometry $x$ (without zero or unitary parts) be the orthogonal direct sum of left and right shifts is that $x^nxH^{\perp}\subseteq x^nH$ or  $x^{*n}xH^{\perp}\subseteq x^{*n}H$. Q. Does there exist any other factorization(s)  of a partial isometry into some nicer operators?",,"['functional-analysis', 'hilbert-spaces', 'operator-algebras', 'c-star-algebras', 'invariant-subspace']"
58,Completeness of Wasserstein space,Completeness of Wasserstein space,,"In this MO question it is said that: On a complete, non necessarily separable metric space $E$, the set $P_r(E)$ of all Radon probability measures with the Wasserstein-Kantorovich metric $W_d$ is complete; and that the above is a known fact.  However I can't seem to find a published reference. Could someone please point me to one? I need in particular a reference of the result above where separability is not required.","In this MO question it is said that: On a complete, non necessarily separable metric space $E$, the set $P_r(E)$ of all Radon probability measures with the Wasserstein-Kantorovich metric $W_d$ is complete; and that the above is a known fact.  However I can't seem to find a published reference. Could someone please point me to one? I need in particular a reference of the result above where separability is not required.",,"['functional-analysis', 'probability-theory', 'measure-theory', 'reference-request', 'optimal-transport']"
59,Every Pseudo-Differential Operator is Pseudo-Local operator.,Every Pseudo-Differential Operator is Pseudo-Local operator.,,"This is the theorem 8.9 of the book Introduction to PDE, Folland, G. B. I'm trying to complete the proof details. I would like to know how to justify the ""Afirmation"", with more rigor. Every pseudo-differential operator is pseudo-local, that is, if $P\in \Psi^m(\Omega)$, then, for all $u\in \mathcal{E}'(\Omega)$, $\hbox{ sing  supp }  Pu \subset \hbox{ sing supp } u $. Proof:  Let $P\in \Psi^m(\Omega)$ and $u\in \mathcal{E}'(\Omega)$.  Given $y_0 \notin \hbox{ sing supp } u$, choose a neighborhood $V$ of $\hbox{sing supp} u$ in $\Omega$ which do not meets the set $\{y_0\}$. Choose $\varphi \in C_0^\infty(\Omega)$ verifying $\hbox{supp }  \varphi \subset V$, such that $\varphi =1$ in a  neighborhood $U \subset V$ of $\hbox{ sing supp } u$ and define $u_1=\varphi u$ and $u_2 = (1-\varphi)u$. Then $u = u_1+u_2$, $\hbox{ supp } u_1 \subset V$ and $u_2= (1-\varphi)u\in C_0^\infty(\Omega)$. Indeed, $\hbox{  supp } u_1 = \hbox{ supp }(\varphi u) \subset \hbox{ supp } (\varphi)\cap \hbox{  supp } u\subset V$ and, in addition, $\hbox{ supp }(u_2) \subset \hbox{ supp }((1-\varphi))\cap \hbox{  supp } u\subset \hbox{  supp } u$ and $\hbox{ supp } u$ is a compact set because $u\in \mathcal{E}'(\Omega)$. To show that $u_2 \in C^\infty(\Omega)$, observe that if $x \in \Omega \setminus \hbox{ sing supp } u$, then $u_2$ is $C^\infty$ in $x$. Thus, it is sufficient to prove that $u_2$ is $C^\infty$ in $U$. Let $\psi \in \mathcal{D}(U)$, then $\langle u_2|_{U}, \psi \rangle = \langle u, (1- \varphi)|_{U} \widetilde{\psi}|_{U} \rangle =0$, since $(1-\varphi)|_{U}=0$, where $\widetilde{\psi}$ denotes the extension of $\psi$ zero outside of $U$. Thus, $u_2|_{U}=0$ and consequently $u_2 \in C^\infty(\Omega)$. Hence, $Pu=Pu_1 + Pu_2$, and $P u_2$ is a $C^\infty(\Omega)$ function since $u_2\in C_0^\infty(\Omega)$. If $k_p$ is the distributional kernel of $P$, the Theorem 8.8 gives that         $k_p(x, y)$ is a $C^\infty$ function in $(\Omega\times \Omega) \backslash \Delta_{\Omega \times \Omega}$. In particular, $k_p(x,y)$ is a $C^\infty$ function for $x \notin V$ and $y \in V$, that is, if $Z$ is a neighborhood of $x$ which do not meets the set $V$, then $k_p \in C^\infty(Z \times V)$. Thus, for $x \notin V$  we have         \begin{eqnarray*} 			P u_1(x) = \left<k_p(x,\cdot),u_1 \right>_{\mathcal{E}(V),\mathcal{E}'(V)}. 		\end{eqnarray*} In fact, let $u \in  \mathcal{E}'(V)$.  Since $C_0^\infty(V)$ is dense in $ \mathcal{E}'(V)$, there exists $(u_n) \subset C_0^\infty(V)$ such that $u_n \rightarrow u$ in $ \mathcal{E}'(V)$.  Then,         \begin{eqnarray*} 			Pu_{n}(x)& = &  \int_{\mathbb{R}^{n}} e^{2 \pi i x \cdot \xi} p(x, \xi) \hat{u}_{n}(\xi) d \xi\\ 			& = & \int_{\mathbb{R}^{n}} \int_{\mathbb{R}^{n}} e^{2 \pi i (x - y) \cdot\xi} 			p(x, \xi) u_{n}(y) dy d \xi \\ 			& = & \int_{\mathbb{R}^{n}} \left[\int_{\mathbb{R}^{n}} e^{2 \pi i (x - y)\cdot \xi}p(x, \xi) d \xi \right]  u_{n}(y) dy \\ 			& = &  \int_{\mathbb{R}^{n}}{k_p (x, y)u_{n}(y)}dy \\ 			& = &  \left< k_p (x,\cdot),u_n \right>_{\mathcal{E}(V),\mathcal{E}'(V)}. 		\end{eqnarray*}         Passing to the limit when $n$ goes to infinity, we obtain the desired equality. Afirmation: This implies that $D^\alpha P u_1(x) = \left<D_x^\alpha k_p(x,\cdot), u_1\right>_{\mathcal{E}(V), \mathcal{E}'(V)}$, and hence we deduce that the aplication $u_1 \mapsto Pu_1|_Z$ is linear and continous from $\mathcal{E}'(V)$ into $C^\infty(Z)$. I do not know if it is possible to identify a distribution with compact support defined in $\mathcal{E}'(\Omega)$ with a distribution defined in $\mathcal{E}'(V)$, even if $\hbox{ supp } u_1 \subset V \subset \Omega$. Justify this it's important to the next step ""$Pu_1$ is $C^\infty$ in $W$ "". Next, we shall prove that $\hbox{ sing supp } Pu_1 \subset V$.  To this end, let $U_{Pu_1}$ be the largest open set on which $Pu_1\in C^\infty(U_{P u_1})$. So, we have to prove that $\Omega \backslash U_{Pu_1} \subset V$, i.e., $\Omega\backslash V \subset U_{Pu_1}$. We will argue by absurd.  Let's assume that $x_0\in \Omega\backslash V $ and that $x_0 \notin U_{Pu_1}$. If $x_0 \in \Omega$ and $x_0\notin V$, then there exists a neighbourhood $W$ of $x_0$ such that $W\cap V=\emptyset$. Then, $P u_1$ is also a $C^\infty$ function in $W$. Note that $W \subset \Omega\setminus V$ and by the above comments we have the desired. Thus, $x_0 \in U_{Pu_1}$, which is a absurd.  Then,  $\hbox{ sing supp } Pu_1 \subset V$ as we desired to prove. Since $y_0 \notin V$, then $y_0 \notin \hbox{ sing supp } Pu_1$. Hence, $y_0 \in U_{Pu_1}$. From this, we deduce that there exists a open set $\omega \subset \Omega$ such that $Pu_1|_{\omega} \in C^\infty(\Omega)$. So, $Pu|_\omega=Pu_1|_{\omega}+Pu_2|_{\omega} \in C^\infty(\omega)$, since $Pu_2 \in C^\infty(\omega)$. Hence, $y_0 \notin\hbox{ sing supp } Pu$.         $\blacksquare$","This is the theorem 8.9 of the book Introduction to PDE, Folland, G. B. I'm trying to complete the proof details. I would like to know how to justify the ""Afirmation"", with more rigor. Every pseudo-differential operator is pseudo-local, that is, if $P\in \Psi^m(\Omega)$, then, for all $u\in \mathcal{E}'(\Omega)$, $\hbox{ sing  supp }  Pu \subset \hbox{ sing supp } u $. Proof:  Let $P\in \Psi^m(\Omega)$ and $u\in \mathcal{E}'(\Omega)$.  Given $y_0 \notin \hbox{ sing supp } u$, choose a neighborhood $V$ of $\hbox{sing supp} u$ in $\Omega$ which do not meets the set $\{y_0\}$. Choose $\varphi \in C_0^\infty(\Omega)$ verifying $\hbox{supp }  \varphi \subset V$, such that $\varphi =1$ in a  neighborhood $U \subset V$ of $\hbox{ sing supp } u$ and define $u_1=\varphi u$ and $u_2 = (1-\varphi)u$. Then $u = u_1+u_2$, $\hbox{ supp } u_1 \subset V$ and $u_2= (1-\varphi)u\in C_0^\infty(\Omega)$. Indeed, $\hbox{  supp } u_1 = \hbox{ supp }(\varphi u) \subset \hbox{ supp } (\varphi)\cap \hbox{  supp } u\subset V$ and, in addition, $\hbox{ supp }(u_2) \subset \hbox{ supp }((1-\varphi))\cap \hbox{  supp } u\subset \hbox{  supp } u$ and $\hbox{ supp } u$ is a compact set because $u\in \mathcal{E}'(\Omega)$. To show that $u_2 \in C^\infty(\Omega)$, observe that if $x \in \Omega \setminus \hbox{ sing supp } u$, then $u_2$ is $C^\infty$ in $x$. Thus, it is sufficient to prove that $u_2$ is $C^\infty$ in $U$. Let $\psi \in \mathcal{D}(U)$, then $\langle u_2|_{U}, \psi \rangle = \langle u, (1- \varphi)|_{U} \widetilde{\psi}|_{U} \rangle =0$, since $(1-\varphi)|_{U}=0$, where $\widetilde{\psi}$ denotes the extension of $\psi$ zero outside of $U$. Thus, $u_2|_{U}=0$ and consequently $u_2 \in C^\infty(\Omega)$. Hence, $Pu=Pu_1 + Pu_2$, and $P u_2$ is a $C^\infty(\Omega)$ function since $u_2\in C_0^\infty(\Omega)$. If $k_p$ is the distributional kernel of $P$, the Theorem 8.8 gives that         $k_p(x, y)$ is a $C^\infty$ function in $(\Omega\times \Omega) \backslash \Delta_{\Omega \times \Omega}$. In particular, $k_p(x,y)$ is a $C^\infty$ function for $x \notin V$ and $y \in V$, that is, if $Z$ is a neighborhood of $x$ which do not meets the set $V$, then $k_p \in C^\infty(Z \times V)$. Thus, for $x \notin V$  we have         \begin{eqnarray*} 			P u_1(x) = \left<k_p(x,\cdot),u_1 \right>_{\mathcal{E}(V),\mathcal{E}'(V)}. 		\end{eqnarray*} In fact, let $u \in  \mathcal{E}'(V)$.  Since $C_0^\infty(V)$ is dense in $ \mathcal{E}'(V)$, there exists $(u_n) \subset C_0^\infty(V)$ such that $u_n \rightarrow u$ in $ \mathcal{E}'(V)$.  Then,         \begin{eqnarray*} 			Pu_{n}(x)& = &  \int_{\mathbb{R}^{n}} e^{2 \pi i x \cdot \xi} p(x, \xi) \hat{u}_{n}(\xi) d \xi\\ 			& = & \int_{\mathbb{R}^{n}} \int_{\mathbb{R}^{n}} e^{2 \pi i (x - y) \cdot\xi} 			p(x, \xi) u_{n}(y) dy d \xi \\ 			& = & \int_{\mathbb{R}^{n}} \left[\int_{\mathbb{R}^{n}} e^{2 \pi i (x - y)\cdot \xi}p(x, \xi) d \xi \right]  u_{n}(y) dy \\ 			& = &  \int_{\mathbb{R}^{n}}{k_p (x, y)u_{n}(y)}dy \\ 			& = &  \left< k_p (x,\cdot),u_n \right>_{\mathcal{E}(V),\mathcal{E}'(V)}. 		\end{eqnarray*}         Passing to the limit when $n$ goes to infinity, we obtain the desired equality. Afirmation: This implies that $D^\alpha P u_1(x) = \left<D_x^\alpha k_p(x,\cdot), u_1\right>_{\mathcal{E}(V), \mathcal{E}'(V)}$, and hence we deduce that the aplication $u_1 \mapsto Pu_1|_Z$ is linear and continous from $\mathcal{E}'(V)$ into $C^\infty(Z)$. I do not know if it is possible to identify a distribution with compact support defined in $\mathcal{E}'(\Omega)$ with a distribution defined in $\mathcal{E}'(V)$, even if $\hbox{ supp } u_1 \subset V \subset \Omega$. Justify this it's important to the next step ""$Pu_1$ is $C^\infty$ in $W$ "". Next, we shall prove that $\hbox{ sing supp } Pu_1 \subset V$.  To this end, let $U_{Pu_1}$ be the largest open set on which $Pu_1\in C^\infty(U_{P u_1})$. So, we have to prove that $\Omega \backslash U_{Pu_1} \subset V$, i.e., $\Omega\backslash V \subset U_{Pu_1}$. We will argue by absurd.  Let's assume that $x_0\in \Omega\backslash V $ and that $x_0 \notin U_{Pu_1}$. If $x_0 \in \Omega$ and $x_0\notin V$, then there exists a neighbourhood $W$ of $x_0$ such that $W\cap V=\emptyset$. Then, $P u_1$ is also a $C^\infty$ function in $W$. Note that $W \subset \Omega\setminus V$ and by the above comments we have the desired. Thus, $x_0 \in U_{Pu_1}$, which is a absurd.  Then,  $\hbox{ sing supp } Pu_1 \subset V$ as we desired to prove. Since $y_0 \notin V$, then $y_0 \notin \hbox{ sing supp } Pu_1$. Hence, $y_0 \in U_{Pu_1}$. From this, we deduce that there exists a open set $\omega \subset \Omega$ such that $Pu_1|_{\omega} \in C^\infty(\Omega)$. So, $Pu|_\omega=Pu_1|_{\omega}+Pu_2|_{\omega} \in C^\infty(\omega)$, since $Pu_2 \in C^\infty(\omega)$. Hence, $y_0 \notin\hbox{ sing supp } Pu$.         $\blacksquare$",,"['functional-analysis', 'topological-vector-spaces', 'pseudo-differential-operators']"
60,What do the Stone-Cech compactification's youger brothers look like?,What do the Stone-Cech compactification's youger brothers look like?,,"$X$ is a non-compact completely-regular space. $C(X)$ is the ring of continuous functions $X \to \mathbb R$. The Stone-Cech compactification $\beta(X)$ is the space of all multiplicative linear functionals $C(X) \to \mathbb R$, under the pointwise topology. $\beta(X) \  -$ in particular $\beta(\mathbb R) \ -$ is a well-studied space. It is compact and satisfies a universal property about extending continuous functions on $X$. Many of its topological properties are known to be independent of ZFC. But what if we only consider a subring $A \subset C(X)$ and the space of all multiplicative linear functionals $A \to \mathbb R$? Call the associated compactification $Y$. Then by Gelfand duality there is a continuous surjection $\beta(X) \to Y$ and so $Y$ is compact, and connected if $X$ is connected. But what else is known about $Y$? What properties does it share with $\beta(X)$ and what properties can we change by choosing $A$ differently? For example what is known when $X = \mathbb R$ and $A$ is any of the following subrings: $A = C^1(X)$ the ring of once-differentiable functions $A = C^\infty(X)$ the ring  of smooth functions $A = P(X)$ the ring of polynomials $A = L(X)$ the ring of Lipschitz functions I can easily believe all the $Y$'s satisfy a corresponding property about lifting the elements of $A$ to $Y$. But what else? Or does all of these $A$'s being dense in $C(X)$ just turn out that $Y = \beta(X)$?","$X$ is a non-compact completely-regular space. $C(X)$ is the ring of continuous functions $X \to \mathbb R$. The Stone-Cech compactification $\beta(X)$ is the space of all multiplicative linear functionals $C(X) \to \mathbb R$, under the pointwise topology. $\beta(X) \  -$ in particular $\beta(\mathbb R) \ -$ is a well-studied space. It is compact and satisfies a universal property about extending continuous functions on $X$. Many of its topological properties are known to be independent of ZFC. But what if we only consider a subring $A \subset C(X)$ and the space of all multiplicative linear functionals $A \to \mathbb R$? Call the associated compactification $Y$. Then by Gelfand duality there is a continuous surjection $\beta(X) \to Y$ and so $Y$ is compact, and connected if $X$ is connected. But what else is known about $Y$? What properties does it share with $\beta(X)$ and what properties can we change by choosing $A$ differently? For example what is known when $X = \mathbb R$ and $A$ is any of the following subrings: $A = C^1(X)$ the ring of once-differentiable functions $A = C^\infty(X)$ the ring  of smooth functions $A = P(X)$ the ring of polynomials $A = L(X)$ the ring of Lipschitz functions I can easily believe all the $Y$'s satisfy a corresponding property about lifting the elements of $A$ to $Y$. But what else? Or does all of these $A$'s being dense in $C(X)$ just turn out that $Y = \beta(X)$?",,"['general-topology', 'functional-analysis', 'ring-theory']"
61,"Is it true, that for $m$-accretive operators in a reflexive Banach space the generalized domain is equal to the domain of the operator?","Is it true, that for -accretive operators in a reflexive Banach space the generalized domain is equal to the domain of the operator?",m,"For $m$-accretive operators $A$ in a Banach space $(X,\|\cdot\|)$ we define the generalized domain by \begin{align} \hat{D}(A) := \{ x \in X : \exists (x_n,y_n) \in A , n \in \mathbb{N}: x_n \rightarrow x \,\, \text{in} \,\,  X, \sup_{n \in \mathbb{N}} \|y_n\| < \infty \}. \end{align} In general we have $D(A) \subseteq \hat{D}(A) \subseteq \overline{D(A)} $. If $X$ is reflexive and $A: D(A) \rightarrow X$ is linear, we have $D(A) = \hat{D}(A)$. Sketch of proof: Since $A$ is $m$-accretive, $A$ is closed. Therefore its graph is closed, moreover $A$ is linear, so we get that its graph is convex. Because closed and convex subsets of a Banach space are weakly closed, the graph of $A$ is also weakly closed. So we now have that $A$ is weakly closed. Now let $x \in \hat{D}(A)$ and $(x_n)_{n} \subseteq D(A)$, such that $x_n \rightarrow x$ and  $\sup_{n \in \mathbb{N}} \|y_n\| < \infty$. Because $X$ is reflexive we can assume that $Ax_n \rightharpoonup y$ for some $y \in X$ after extraction of a subsequence because of the Eberlein-Shmulyan theorem. Since $A$ is weakly closed we get $x \in D(A)$ and $Ax = y$ and therefore $D(A) = \hat{D}(A)$. Now assume $A$ is a nonlinear and $m$-accretive operator in the reflexive space $X$. We then do not have the convexity of the graph of $A$ and therefore the proof does not hold. Is the equation $D(A) = \hat{D}(A)$ still true? If yes, how to proof it?","For $m$-accretive operators $A$ in a Banach space $(X,\|\cdot\|)$ we define the generalized domain by \begin{align} \hat{D}(A) := \{ x \in X : \exists (x_n,y_n) \in A , n \in \mathbb{N}: x_n \rightarrow x \,\, \text{in} \,\,  X, \sup_{n \in \mathbb{N}} \|y_n\| < \infty \}. \end{align} In general we have $D(A) \subseteq \hat{D}(A) \subseteq \overline{D(A)} $. If $X$ is reflexive and $A: D(A) \rightarrow X$ is linear, we have $D(A) = \hat{D}(A)$. Sketch of proof: Since $A$ is $m$-accretive, $A$ is closed. Therefore its graph is closed, moreover $A$ is linear, so we get that its graph is convex. Because closed and convex subsets of a Banach space are weakly closed, the graph of $A$ is also weakly closed. So we now have that $A$ is weakly closed. Now let $x \in \hat{D}(A)$ and $(x_n)_{n} \subseteq D(A)$, such that $x_n \rightarrow x$ and  $\sup_{n \in \mathbb{N}} \|y_n\| < \infty$. Because $X$ is reflexive we can assume that $Ax_n \rightharpoonup y$ for some $y \in X$ after extraction of a subsequence because of the Eberlein-Shmulyan theorem. Since $A$ is weakly closed we get $x \in D(A)$ and $Ax = y$ and therefore $D(A) = \hat{D}(A)$. Now assume $A$ is a nonlinear and $m$-accretive operator in the reflexive space $X$. We then do not have the convexity of the graph of $A$ and therefore the proof does not hold. Is the equation $D(A) = \hat{D}(A)$ still true? If yes, how to proof it?",,"['functional-analysis', 'partial-differential-equations', 'nonlinear-analysis']"
62,Positive part of functions from Sobolev space involving time,Positive part of functions from Sobolev space involving time,,"Assume that $u \in W^{1,2}(0,T; W_0^{1,2}(\Omega))$, where $\Omega$ is a bounded domain in $\mathbb{R}^n$ and $T <+\infty$. Let $A \subset \Omega \times (0, T)$ be a Lipschitz domain such that $u(\cdot,t_0) \leq 0$ on $\partial (A \cap \{t=t_0\})$ for a.a. $t_0 \in (0, T)$ in the sense of traces of functions from $W^{1,2}(\Omega)$. Is it true that $\max(u, 0)|_{A} \in W^{1,2}(0,T;W_0^{1,2}(\Omega))$? (Here under $\max(u, 0)|_{A}$ I mean a function $w$ such that $w = \max(u,0)$ on $A$ and $w=0$ on $(\Omega \times (0, T)) \setminus A$). P.S. Analogous property is, of course, valid for functions from $W_0^{1,2}(\Omega)$. [Added] The question is also actual for the space $W^{1,2}(0,T;L^2(\Omega)) \cap L^2(0,T;W_0^{1,2}(\Omega))$.","Assume that $u \in W^{1,2}(0,T; W_0^{1,2}(\Omega))$, where $\Omega$ is a bounded domain in $\mathbb{R}^n$ and $T <+\infty$. Let $A \subset \Omega \times (0, T)$ be a Lipschitz domain such that $u(\cdot,t_0) \leq 0$ on $\partial (A \cap \{t=t_0\})$ for a.a. $t_0 \in (0, T)$ in the sense of traces of functions from $W^{1,2}(\Omega)$. Is it true that $\max(u, 0)|_{A} \in W^{1,2}(0,T;W_0^{1,2}(\Omega))$? (Here under $\max(u, 0)|_{A}$ I mean a function $w$ such that $w = \max(u,0)$ on $A$ and $w=0$ on $(\Omega \times (0, T)) \setminus A$). P.S. Analogous property is, of course, valid for functions from $W_0^{1,2}(\Omega)$. [Added] The question is also actual for the space $W^{1,2}(0,T;L^2(\Omega)) \cap L^2(0,T;W_0^{1,2}(\Omega))$.",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'bochner-spaces']"
63,A funtion and its fourier transformation cannot both be compactly supported unless f=0 [duplicate],A funtion and its fourier transformation cannot both be compactly supported unless f=0 [duplicate],,"This question already has an answer here : A function and its Fourier transform cannot both be compactly supported (1 answer) Closed 7 years ago . Problem : Suppose that $f$ is continuous on $\mathbb{R}$. Show that $f$ and $\hat f$ cannot both be compactly supported unless $f=0$. Hint : Assume $f$ is supported in [0,1/2]. Expand $f$ in a Fourier series in the interval [-,1], and note that as a result, f is a trigonometric polynomial. I proved that f is trigonometric polynomial by using hint. But, I don't know how to prove function's fourier transform cannot compactly supported function.  Can I get some hints?","This question already has an answer here : A function and its Fourier transform cannot both be compactly supported (1 answer) Closed 7 years ago . Problem : Suppose that $f$ is continuous on $\mathbb{R}$. Show that $f$ and $\hat f$ cannot both be compactly supported unless $f=0$. Hint : Assume $f$ is supported in [0,1/2]. Expand $f$ in a Fourier series in the interval [-,1], and note that as a result, f is a trigonometric polynomial. I proved that f is trigonometric polynomial by using hint. But, I don't know how to prove function's fourier transform cannot compactly supported function.  Can I get some hints?",,['fourier-analysis']
64,Definition of class of continuously differentiable functions on a closed interval,Definition of class of continuously differentiable functions on a closed interval,,"This is a technical simple question.  The space $C^1([a,b])$ is defined as $$C^1([a,b])=\{f:[a,b]\to R: \exists\; f' \textrm{ and is continuous on } [a,b]  \}.$$ I think there is some explaining to do here, and is the fact that $f'$ cannot be defined at $a$ or $b$ because those points do not belong to the interior of the domain. The question is: In the above definition, $f'(a)$ and $f'(b)$ are understood in the sense of right and left derivatives respectively? If not, in which sense?","This is a technical simple question.  The space $C^1([a,b])$ is defined as $$C^1([a,b])=\{f:[a,b]\to R: \exists\; f' \textrm{ and is continuous on } [a,b]  \}.$$ I think there is some explaining to do here, and is the fact that $f'$ cannot be defined at $a$ or $b$ because those points do not belong to the interior of the domain. The question is: In the above definition, $f'(a)$ and $f'(b)$ are understood in the sense of right and left derivatives respectively? If not, in which sense?",,"['calculus', 'real-analysis', 'functional-analysis', 'derivatives', 'continuity']"
65,Control higher derivatives of approximating sequence,Control higher derivatives of approximating sequence,,"Let $\Omega \subset \mathbb R^d$ be a bounded open set. Let $H_0^1(\Omega)$ be the usual Sobolev space. From the definition it follows, that each $u \in H_0^1(\Omega)$ can be approximated by a sequence $\{u_n\} \subset C_c^\infty(\Omega)$ (smooth and compactly supported functions). Is it possible to somehow control the higher derivatives of the approximating sequence? Of course, we must have $\|u_n\|_{H^2(\Omega)} \to \infty$ if $u \not\in H^2(\Omega)$. But is it, e.g., possible to obtain a sequence $\{u_n\} \subset H^2(\Omega) \cap H_0^1(\Omega)$ with $$\|u - u_n\|_{L^2(\Omega)} \le \frac{C}n \quad\text{and}\quad \|u_n\|_{H^2(\Omega)} \le C \, n\quad?$$ One possibility would be to extend $u$ by zero and use a mollification argument. This should yield a sequence $\{u_n\}\subset H^2(\Omega)$ satisfying the above properties. However, I do not see a reason how to construct the approximating sequence in $H_0^1(\Omega)$.","Let $\Omega \subset \mathbb R^d$ be a bounded open set. Let $H_0^1(\Omega)$ be the usual Sobolev space. From the definition it follows, that each $u \in H_0^1(\Omega)$ can be approximated by a sequence $\{u_n\} \subset C_c^\infty(\Omega)$ (smooth and compactly supported functions). Is it possible to somehow control the higher derivatives of the approximating sequence? Of course, we must have $\|u_n\|_{H^2(\Omega)} \to \infty$ if $u \not\in H^2(\Omega)$. But is it, e.g., possible to obtain a sequence $\{u_n\} \subset H^2(\Omega) \cap H_0^1(\Omega)$ with $$\|u - u_n\|_{L^2(\Omega)} \le \frac{C}n \quad\text{and}\quad \|u_n\|_{H^2(\Omega)} \le C \, n\quad?$$ One possibility would be to extend $u$ by zero and use a mollification argument. This should yield a sequence $\{u_n\}\subset H^2(\Omega)$ satisfying the above properties. However, I do not see a reason how to construct the approximating sequence in $H_0^1(\Omega)$.",,"['functional-analysis', 'numerical-methods', 'sobolev-spaces', 'approximation-theory']"
66,Identity theorem in higher dimensions,Identity theorem in higher dimensions,,"The following is the identity theorem in complex analysis: Let $f,g:U\to\Bbb C$ be holomorphic functions with $U\subset\Bbb C$ open and connected, if $f\lvert_A=g\lvert_A$ where $A\subset U$ is a set with an accumulation point in $U$, then $f=g$. Is there a version of this theorem that works in complex Banach spaces or Banach algebras? I think this should be standard knowledge, but internet searches aren't helpful. One idea to reduce to the other case would be ""lay in"" one dimensional subspaces in $U$ and then compose the difference of $f-g$ with an element of the dual space. This would be a differentiable function with an accumulation point at which it evaluates to zero. Doing it for arbitrary affine 1d subspaces and linear functionals would be enough to see $f=g$ on all of $U$. However unless $U$ is convex the domain of this guy need not be connected and unless $f-g=0$ on an open subset there is no guarantee that every point in $U$ lies on an affine subspace that has an accumulation point on which we know $f-g=0$.","The following is the identity theorem in complex analysis: Let $f,g:U\to\Bbb C$ be holomorphic functions with $U\subset\Bbb C$ open and connected, if $f\lvert_A=g\lvert_A$ where $A\subset U$ is a set with an accumulation point in $U$, then $f=g$. Is there a version of this theorem that works in complex Banach spaces or Banach algebras? I think this should be standard knowledge, but internet searches aren't helpful. One idea to reduce to the other case would be ""lay in"" one dimensional subspaces in $U$ and then compose the difference of $f-g$ with an element of the dual space. This would be a differentiable function with an accumulation point at which it evaluates to zero. Doing it for arbitrary affine 1d subspaces and linear functionals would be enough to see $f=g$ on all of $U$. However unless $U$ is convex the domain of this guy need not be connected and unless $f-g=0$ on an open subset there is no guarantee that every point in $U$ lies on an affine subspace that has an accumulation point on which we know $f-g=0$.",,"['complex-analysis', 'functional-analysis']"
67,A weakly closed set in $X$ that remains weakly-star closed in $X^{**}$,A weakly closed set in  that remains weakly-star closed in,X X^{**},"I have $X$ a (non-reflexive) Banach space and $B\subset X$ a weakly closed convex subset. I wonder under what additional conditions (other than weak compactness) $B$ remains weakly-star closed in $X^{**}$. My take on this:- the canonical embedding $J:(X,w)\to (X^{**},w^*)$ is linear continuous and my question refers to finding on what sets $B$ is  $J$ a closed map. Another remark: - because $B$  is strongly closed in $X$ it is so in $X^{**}$. Combined with $B$ convex that yields that $B$ is weakly closed in $X^{**}$. It remains to get to the weak-star topology on $X^{**}$.","I have $X$ a (non-reflexive) Banach space and $B\subset X$ a weakly closed convex subset. I wonder under what additional conditions (other than weak compactness) $B$ remains weakly-star closed in $X^{**}$. My take on this:- the canonical embedding $J:(X,w)\to (X^{**},w^*)$ is linear continuous and my question refers to finding on what sets $B$ is  $J$ a closed map. Another remark: - because $B$  is strongly closed in $X$ it is so in $X^{**}$. Combined with $B$ convex that yields that $B$ is weakly closed in $X^{**}$. It remains to get to the weak-star topology on $X^{**}$.",,"['functional-analysis', 'banach-spaces', 'locally-convex-spaces']"
68,weakly compact in metric space,weakly compact in metric space,,"We know a Borel probability measure $m$ on a complete metric space $X$ is a Radon measure if and only if there exists a separable subset $Y \in B(X)$ (Borel subset of $X$) such that $m(Y)=1$. Now, if  $X$ is a weakly compact set in a Banach space and $m$ is a Radon measure on it, is there a separable subset $Y \in B(X)$ (Borel subset of $X$) such that $m(Y)=1$?","We know a Borel probability measure $m$ on a complete metric space $X$ is a Radon measure if and only if there exists a separable subset $Y \in B(X)$ (Borel subset of $X$) such that $m(Y)=1$. Now, if  $X$ is a weakly compact set in a Banach space and $m$ is a Radon measure on it, is there a separable subset $Y \in B(X)$ (Borel subset of $X$) such that $m(Y)=1$?",,"['functional-analysis', 'measure-theory']"
69,Gauss map and Minkowski functionals,Gauss map and Minkowski functionals,,"Let $K$ be a convex body and let $\| \cdot \|_{K}$ be the correspoding Minkowski functional $$\| x \|_{K} = \inf\{\lambda > 0 : x \in \lambda K \}$$ Let us consider the following map $f: K \rightarrow \partial \mathring K$   such that $f(x) = \nabla \| x \|_{K}$ Here $\partial \mathring K$ stands for the polar set, i.e. $$\partial \mathring K = \{ y \in \partial K : \sup_{x \in \partial K}{\langle x, y \rangle} \leq 1 \}$$ It is pointed out that $f$ pretends to be a Gauss map $v_{K}: \partial K \rightarrow S^{n-1}$ that maps the outer unit normal to the boundary to the unit sphere. Are there any easy ways to recover it geometrically? It looks as if there is a direct relationship between the fact that the subgradients of convex functions are presicely the outer normal vector of supporting hyperplanes of sublevel sets and the statement above, but i can't see any fast ways to figure it out.","Let $K$ be a convex body and let $\| \cdot \|_{K}$ be the correspoding Minkowski functional $$\| x \|_{K} = \inf\{\lambda > 0 : x \in \lambda K \}$$ Let us consider the following map $f: K \rightarrow \partial \mathring K$   such that $f(x) = \nabla \| x \|_{K}$ Here $\partial \mathring K$ stands for the polar set, i.e. $$\partial \mathring K = \{ y \in \partial K : \sup_{x \in \partial K}{\langle x, y \rangle} \leq 1 \}$$ It is pointed out that $f$ pretends to be a Gauss map $v_{K}: \partial K \rightarrow S^{n-1}$ that maps the outer unit normal to the boundary to the unit sphere. Are there any easy ways to recover it geometrically? It looks as if there is a direct relationship between the fact that the subgradients of convex functions are presicely the outer normal vector of supporting hyperplanes of sublevel sets and the statement above, but i can't see any fast ways to figure it out.",,"['functional-analysis', 'convex-analysis', 'optimal-transport']"
70,Why the denseness of norm attaining operator is important?,Why the denseness of norm attaining operator is important?,,"I plan for studying norm attaining operators, Lindenstrauss property A and B, property $\alpha$, $\beta$, and so on... Before do it, I wanna understand why these properties are important. I know it starts from Bishop-Phelps theorem, but I don't know why the denseness of the set of norm attaining operators in the set of linear operator is important. What I want is something like: if we know the denseness, then we can characterize the original space by only considering norm attaining operators or something else. Any useful links or comments should be appreciated","I plan for studying norm attaining operators, Lindenstrauss property A and B, property $\alpha$, $\beta$, and so on... Before do it, I wanna understand why these properties are important. I know it starts from Bishop-Phelps theorem, but I don't know why the denseness of the set of norm attaining operators in the set of linear operator is important. What I want is something like: if we know the denseness, then we can characterize the original space by only considering norm attaining operators or something else. Any useful links or comments should be appreciated",,"['functional-analysis', 'operator-theory', 'banach-spaces']"
71,Can Sobolev spaces with $1 \le p < 2$ be defined in terms of Fourier transforms?,Can Sobolev spaces with  be defined in terms of Fourier transforms?,1 \le p < 2,"By Plancherel's Theorem, we can define the Sobolev space $W^{k,2}(\mathbb{R})$ using Fourier transforms as $$W^{k,2} (\mathbb{R}) = \left\{ f \in \mathcal{S}' \ | \ \mathscr{F}^{-1}\left[ \left( 1 + \lvert \xi \rvert^2 \right)^{k/2} \mathscr{F}[f]\right] \in L^2(\mathbb{R}) \right\}$$ equivalently to the definition for general $p\ge 1$ $$W^{k,p}(\mathbb{R}) = \left\{ f \in L^p(\mathbb{R}) \ | \ \frac{d^if}{dx^i} \in L^p(\mathbb{R}), \ 1 \le i \le k \right\} \, ,$$ if we set $p=2$. My question is whether it is possible to define $W^{k,p}(\mathbb{R})$ using Fourier transforms when $p \ne 2$.  Using the Riesz-Thorin interpolation theorem as discussed in this MathSE question and answer , we can extend the Fourier transform to a (non-surjective) bounded linear transformation $L^p \to L^q$ for $1 \le p \le 2$ (where $q$ is the Holder conjugate of $p$).  As discussed in this book chapter (p178) , we can invert the Fourier transform on its range in $L^q$. Is this enough that we can define Sobolev spaces for $1 \le p < 2$ in terms of Fourier transforms as we can when $p=2$?","By Plancherel's Theorem, we can define the Sobolev space $W^{k,2}(\mathbb{R})$ using Fourier transforms as $$W^{k,2} (\mathbb{R}) = \left\{ f \in \mathcal{S}' \ | \ \mathscr{F}^{-1}\left[ \left( 1 + \lvert \xi \rvert^2 \right)^{k/2} \mathscr{F}[f]\right] \in L^2(\mathbb{R}) \right\}$$ equivalently to the definition for general $p\ge 1$ $$W^{k,p}(\mathbb{R}) = \left\{ f \in L^p(\mathbb{R}) \ | \ \frac{d^if}{dx^i} \in L^p(\mathbb{R}), \ 1 \le i \le k \right\} \, ,$$ if we set $p=2$. My question is whether it is possible to define $W^{k,p}(\mathbb{R})$ using Fourier transforms when $p \ne 2$.  Using the Riesz-Thorin interpolation theorem as discussed in this MathSE question and answer , we can extend the Fourier transform to a (non-surjective) bounded linear transformation $L^p \to L^q$ for $1 \le p \le 2$ (where $q$ is the Holder conjugate of $p$).  As discussed in this book chapter (p178) , we can invert the Fourier transform on its range in $L^q$. Is this enough that we can define Sobolev spaces for $1 \le p < 2$ in terms of Fourier transforms as we can when $p=2$?",,"['functional-analysis', 'sobolev-spaces', 'fourier-transform']"
72,The Hardy space is a Hilbert space,The Hardy space is a Hilbert space,,"The Hardy space $H^2(\mathbb{D})$ is defined to be the space of all functions $f$ holomorphic on the unit disk $\mathbb{D}$ with the norm $\lVert \cdot \rVert_H$ $\lVert f \rVert_H^2=\sup_{0<r<1}\int_0^{2\pi}|f(re^{i\theta})|^2 d\theta$ is finite. Show that $H^2(\mathbb{D})$ is a Hilbert space. I have shown that if $f(z)=\sum_n c_nz^n$ , then $\lVert f \rVert_H^2=2\pi \sum_n|c_n|^2$ . How does this imply $H^2(\mathbb{D})$ is a Hilbert space? What is the inner product induced by the norm?","The Hardy space is defined to be the space of all functions holomorphic on the unit disk with the norm is finite. Show that is a Hilbert space. I have shown that if , then . How does this imply is a Hilbert space? What is the inner product induced by the norm?",H^2(\mathbb{D}) f \mathbb{D} \lVert \cdot \rVert_H \lVert f \rVert_H^2=\sup_{0<r<1}\int_0^{2\pi}|f(re^{i\theta})|^2 d\theta H^2(\mathbb{D}) f(z)=\sum_n c_nz^n \lVert f \rVert_H^2=2\pi \sum_n|c_n|^2 H^2(\mathbb{D}),"['functional-analysis', 'hilbert-spaces']"
73,Prove $T:X\to X$ is bounded,Prove  is bounded,T:X\to X,"Let $X$ be a Banach space and $T:X\to X$ be linear operator. Let $A$ be s subset of $X^*$ which separates the points in $X$. Suppose $f\circ T$ is bounded $\forall f\in A$, show that $T:X\to X$ is bounded. I think it is kind of using Uniform Boundedness theorem, but I have no idea how to do it. Could you please give me some hints please? Thank you.","Let $X$ be a Banach space and $T:X\to X$ be linear operator. Let $A$ be s subset of $X^*$ which separates the points in $X$. Suppose $f\circ T$ is bounded $\forall f\in A$, show that $T:X\to X$ is bounded. I think it is kind of using Uniform Boundedness theorem, but I have no idea how to do it. Could you please give me some hints please? Thank you.",,"['functional-analysis', 'banach-spaces']"
74,"Proof verification that $\sup_{||x||=1} |\langle Tx, x \rangle | = ||T||$",Proof verification that,"\sup_{||x||=1} |\langle Tx, x \rangle | = ||T||","I have been trying to solve the following exercise: Suppose that for an operator $T$ on a Hilbert space $\left\||Tx_n - \lambda x_n \right\| \to 0$ with $||x_n|| = 1$ so that $\lambda$ is an approximate eigenvalue of $T$. If $|\lambda| = ||T||$ show that $\sup_{||x||=1} |\langle Tx, x \rangle | = ||T||$. Could you please verify my proof? The first direction is easy as for a unit vector $x$ $$|\langle Tx, x \rangle | \leq \left\|Tx\right\| \left\|x\right\| \leq \left\| T \right\| \left\|  x\right\|^2$$ so  $\sup_{||x||=1} |\langle Tx, x \rangle | \leq ||T|| $. For the other direction I was thinking I could again use the Cauchy-Schwartz inequality to get $$\left| \langle T x_n - \lambda x_n, x_n \rangle  \right| =  \left| \langle T x_n,x_n \rangle - \lambda  \right|< \epsilon$$ for large enough $n$. Hence by the reverse triangle inequality $$|\lambda| < \left| \langle T x_n,x_n \rangle  \right| + \epsilon \leq \sup_{||x||=1} |\langle Tx, x \rangle | + \epsilon $$ so $ |\lambda| = ||T|| \leq \sup_{||x||=1} |\langle Tx, x \rangle | $ and assuming this is correct, the proof is complete. Is everything alright in the above? Thank you.","I have been trying to solve the following exercise: Suppose that for an operator $T$ on a Hilbert space $\left\||Tx_n - \lambda x_n \right\| \to 0$ with $||x_n|| = 1$ so that $\lambda$ is an approximate eigenvalue of $T$. If $|\lambda| = ||T||$ show that $\sup_{||x||=1} |\langle Tx, x \rangle | = ||T||$. Could you please verify my proof? The first direction is easy as for a unit vector $x$ $$|\langle Tx, x \rangle | \leq \left\|Tx\right\| \left\|x\right\| \leq \left\| T \right\| \left\|  x\right\|^2$$ so  $\sup_{||x||=1} |\langle Tx, x \rangle | \leq ||T|| $. For the other direction I was thinking I could again use the Cauchy-Schwartz inequality to get $$\left| \langle T x_n - \lambda x_n, x_n \rangle  \right| =  \left| \langle T x_n,x_n \rangle - \lambda  \right|< \epsilon$$ for large enough $n$. Hence by the reverse triangle inequality $$|\lambda| < \left| \langle T x_n,x_n \rangle  \right| + \epsilon \leq \sup_{||x||=1} |\langle Tx, x \rangle | + \epsilon $$ so $ |\lambda| = ||T|| \leq \sup_{||x||=1} |\langle Tx, x \rangle | $ and assuming this is correct, the proof is complete. Is everything alright in the above? Thank you.",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'normed-spaces']"
75,Existence of translations dilations compactifying set,Existence of translations dilations compactifying set,,"I am reading the book J. Krieger, W. Schlag Concentration Compactness for critical Wave Maps , in which one of the key preliminary reductions is Corollary 9.36 on pg. 315, which I have reproduced below. Corollary 9.36 There exist continuous functions $\bar{x} : I \rightarrow\mathbb{R}^{2}$ and $\lambda : I \rightarrow\mathbb{R}^{+}$ so that the family of functions $\{\lambda(t)^{-1}\Psi_{\alpha}^{\infty}(t,(\cdot-\bar{x}(t))\lambda(t)^{-1})\}_{t\in I}\subset L_{x}^{2}$ is pre-compact. Since I believe my issue can be stated without specifying exactly what the function $\Psi_{\alpha}^{\infty}$ defined on the interval $I$ is, I will not do so. Also, I am content with showing the existence of (possibly) discontinuous functions $\bar{x}$ and $\lambda$. The proof of Corollary 9.36 is by contradiction. I.e. the authors first assume that there do not even exist discontinuous such functions $\bar{x}$, $\lambda$, from which they claim there exists $\varepsilon>0$ and a sequence of times $\{t_{n}\}\subset I$ so that   $$\inf_{\lambda>0, \bar{x}\in\mathbb{R}^{2}}\|\lambda^{-1}\Psi_{\alpha}^{\infty}(t_{n}, (\cdot-\bar{x})\lambda^{-1})-\Psi_{\alpha}^{\infty}(t_{m},\cdot)\|_{2}\geq\varepsilon \tag{*}$$   for any $n\neq m$. I do not see why ( * ) follows from the authors' assumption. I looked at the cited works [14] and [15] in the proof, but these papers did not answer my question, as they state ( * ). I have been struggling with this issue for a while now and would appreciate any clarification.","I am reading the book J. Krieger, W. Schlag Concentration Compactness for critical Wave Maps , in which one of the key preliminary reductions is Corollary 9.36 on pg. 315, which I have reproduced below. Corollary 9.36 There exist continuous functions $\bar{x} : I \rightarrow\mathbb{R}^{2}$ and $\lambda : I \rightarrow\mathbb{R}^{+}$ so that the family of functions $\{\lambda(t)^{-1}\Psi_{\alpha}^{\infty}(t,(\cdot-\bar{x}(t))\lambda(t)^{-1})\}_{t\in I}\subset L_{x}^{2}$ is pre-compact. Since I believe my issue can be stated without specifying exactly what the function $\Psi_{\alpha}^{\infty}$ defined on the interval $I$ is, I will not do so. Also, I am content with showing the existence of (possibly) discontinuous functions $\bar{x}$ and $\lambda$. The proof of Corollary 9.36 is by contradiction. I.e. the authors first assume that there do not even exist discontinuous such functions $\bar{x}$, $\lambda$, from which they claim there exists $\varepsilon>0$ and a sequence of times $\{t_{n}\}\subset I$ so that   $$\inf_{\lambda>0, \bar{x}\in\mathbb{R}^{2}}\|\lambda^{-1}\Psi_{\alpha}^{\infty}(t_{n}, (\cdot-\bar{x})\lambda^{-1})-\Psi_{\alpha}^{\infty}(t_{m},\cdot)\|_{2}\geq\varepsilon \tag{*}$$   for any $n\neq m$. I do not see why ( * ) follows from the authors' assumption. I looked at the cited works [14] and [15] in the proof, but these papers did not answer my question, as they state ( * ). I have been struggling with this issue for a while now and would appreciate any clarification.",,"['functional-analysis', 'partial-differential-equations', 'compactness']"
76,"Suppose that $H$ is an infinite dimensional normed vector space, is every infinite dimensional subspace of $H$ with a basis closed?","Suppose that  is an infinite dimensional normed vector space, is every infinite dimensional subspace of  with a basis closed?",H H,"Suppose that $H$ is an infinite dimensional normed vector space, is every infinite dimensional subspace of $H$ with a basis closed? In finite dimension, this is correct, but in infinite dimension, is this right?","Suppose that $H$ is an infinite dimensional normed vector space, is every infinite dimensional subspace of $H$ with a basis closed? In finite dimension, this is correct, but in infinite dimension, is this right?",,['functional-analysis']
77,In Calculus of Variation: Problem applying variational principle theorem,In Calculus of Variation: Problem applying variational principle theorem,,"Let $f:\mathbb R^m \rightarrow [0,+\infty)\;$ be a smooth function    that vanishes on a finite set $A\;$ where $\vert A \vert\; \ge 2$ and    the maps $v:(l^{-},l^{+}) \rightarrow \mathbb R^m\;$ defined by $\mathcal M= \{\;v\in W^{1,2}_{loc} (l^{-},l^{+});\;-\infty \le  l^{-}  \lt l^{+} \le +\infty\;,\;\lim_{x \to l^{-}}  v(x)=a_1 \in  A\;,\;\lim_{x \to l^{+}} v(x)=a_2 \neq  a_1 \in  A\;\;,\;v((l^{-},l^{+}))\subseteq \mathbb R \setminus A \}\;$. Show that a minimizer of the functional $\;I(v)=\int_{l^{-}}^{l^{+}} \frac{{\dot v}^2}{2} + f(v) \;dx$ on $\;\mathcal M\;$ exists. I found after some research the following theorem: Let $\;X\;$ be reflexive, $\;M\subset  X\;$ nonempty and weakly sequentially closed,$\; F:M\rightarrow \mathbb R \;$ coercive and weakly sequentially lower semi-continuous. Then there exist $\;x_0 \in M\;$ such that ; $\;F(x_0)=\inf_{x \in M} F(x)\;$ I know $\;W^{1,2}\;$ is reflexive and for the coerciveness of $\;I\;$ there is a hint : ""Assume $\limsup_{\vert v \vert \to +\infty} f(v) \gt 0\;$"" which I don't completely understand why is needed. My question : How do I prove that $\; \mathcal M\;$ and $\;I\;$ satisfy the above theorem? I'm really new to Sobolev Spaces and I hadn't seen before the term ""weakly sequentially closed"". I would appreciate if somebody could help me through this. Furthermore, any suggestions about useful books related to this topic, would be valuable. Thanks in advance!!!","Let $f:\mathbb R^m \rightarrow [0,+\infty)\;$ be a smooth function    that vanishes on a finite set $A\;$ where $\vert A \vert\; \ge 2$ and    the maps $v:(l^{-},l^{+}) \rightarrow \mathbb R^m\;$ defined by $\mathcal M= \{\;v\in W^{1,2}_{loc} (l^{-},l^{+});\;-\infty \le  l^{-}  \lt l^{+} \le +\infty\;,\;\lim_{x \to l^{-}}  v(x)=a_1 \in  A\;,\;\lim_{x \to l^{+}} v(x)=a_2 \neq  a_1 \in  A\;\;,\;v((l^{-},l^{+}))\subseteq \mathbb R \setminus A \}\;$. Show that a minimizer of the functional $\;I(v)=\int_{l^{-}}^{l^{+}} \frac{{\dot v}^2}{2} + f(v) \;dx$ on $\;\mathcal M\;$ exists. I found after some research the following theorem: Let $\;X\;$ be reflexive, $\;M\subset  X\;$ nonempty and weakly sequentially closed,$\; F:M\rightarrow \mathbb R \;$ coercive and weakly sequentially lower semi-continuous. Then there exist $\;x_0 \in M\;$ such that ; $\;F(x_0)=\inf_{x \in M} F(x)\;$ I know $\;W^{1,2}\;$ is reflexive and for the coerciveness of $\;I\;$ there is a hint : ""Assume $\limsup_{\vert v \vert \to +\infty} f(v) \gt 0\;$"" which I don't completely understand why is needed. My question : How do I prove that $\; \mathcal M\;$ and $\;I\;$ satisfy the above theorem? I'm really new to Sobolev Spaces and I hadn't seen before the term ""weakly sequentially closed"". I would appreciate if somebody could help me through this. Furthermore, any suggestions about useful books related to this topic, would be valuable. Thanks in advance!!!",,"['functional-analysis', 'sobolev-spaces', 'calculus-of-variations', 'weak-convergence', 'coercive']"
78,Generalizations of the Robbins lemma and Gaussian integration by parts,Generalizations of the Robbins lemma and Gaussian integration by parts,,"The Robbins lemma, named after Herbert Robbins, says that if $X\sim\operatorname{Poisson}(\lambda)$ and $g$ is a function for which $\operatorname{E}(|X g(X)|) < \infty,$ then $$\operatorname{E}(Xg(X)) = \lambda \operatorname{E}(g(X+1)).$$ ""Gaussian integration by parts"" is an identity that says that under suitable assumptions about the function $g$, if $X\sim N(0,\sigma^2),$ then $$ \operatorname{E}(Xg(X)) = \sigma^2\operatorname{E} (g\,'(X)). $$ Both of these propositions are used in empirical Bayes methods. Both of these are of the form $$ \operatorname{E}(Xg(X)) = \operatorname{var}(X) \cdot \operatorname{E} ((Tg)(X)) $$ where $T$ is a linear operator on functions $g$. QUESTION: Might there be, for each linear operator $T$, some probability distribution for which this holds? And might all of these be useful in empirical Bayes methods?","The Robbins lemma, named after Herbert Robbins, says that if $X\sim\operatorname{Poisson}(\lambda)$ and $g$ is a function for which $\operatorname{E}(|X g(X)|) < \infty,$ then $$\operatorname{E}(Xg(X)) = \lambda \operatorname{E}(g(X+1)).$$ ""Gaussian integration by parts"" is an identity that says that under suitable assumptions about the function $g$, if $X\sim N(0,\sigma^2),$ then $$ \operatorname{E}(Xg(X)) = \sigma^2\operatorname{E} (g\,'(X)). $$ Both of these propositions are used in empirical Bayes methods. Both of these are of the form $$ \operatorname{E}(Xg(X)) = \operatorname{var}(X) \cdot \operatorname{E} ((Tg)(X)) $$ where $T$ is a linear operator on functions $g$. QUESTION: Might there be, for each linear operator $T$, some probability distribution for which this holds? And might all of these be useful in empirical Bayes methods?",,"['functional-analysis', 'probability-distributions', 'empirical-bayes']"
79,Find the measure $\mu$ such that $f = \int g d\mu$,Find the measure  such that,\mu f = \int g d\mu,"I have the following equation: $$P(x)=\int_0^\infty f(x,y) g(y)dy $$ This is, I have an equation relating P to the weighted integral of f with respect to the weight $g(y)$. I want to somehow invert this equation to get an expression for $g$ in terms of $P$ and $f$ (which I know). I though of expressing this equation as: $$P = \int f d\mu$$ Where the measure $\mu$ is given by $g(y)dy$. Then the problem would be for a given $P$ and $f$ to find the measure $\mu$ verifying the above equality. Is this doable? If it is, how can I do that? I also tried applying some kind of transform to my original equation to see if I can solve it in an easier way. I was thinking of applying a Laplace transform (because of the limits of my original integral) but I have the problem that I have no delay when integrating over the $y$ variable so I can't get a simplified equation as I would get in the case of a convolution. How can I find $g$ in terms of $P$ and $f$? EDIT: I have been reading this last few days and I've found what is exactly what I want to solve. I have a Fredholm equation of the first kind with a continuous (kind of Gaussian) kernel. Is this kind of equations solvable analytically or the best thing that I can get is a numerical solution?","I have the following equation: $$P(x)=\int_0^\infty f(x,y) g(y)dy $$ This is, I have an equation relating P to the weighted integral of f with respect to the weight $g(y)$. I want to somehow invert this equation to get an expression for $g$ in terms of $P$ and $f$ (which I know). I though of expressing this equation as: $$P = \int f d\mu$$ Where the measure $\mu$ is given by $g(y)dy$. Then the problem would be for a given $P$ and $f$ to find the measure $\mu$ verifying the above equality. Is this doable? If it is, how can I do that? I also tried applying some kind of transform to my original equation to see if I can solve it in an easier way. I was thinking of applying a Laplace transform (because of the limits of my original integral) but I have the problem that I have no delay when integrating over the $y$ variable so I can't get a simplified equation as I would get in the case of a convolution. How can I find $g$ in terms of $P$ and $f$? EDIT: I have been reading this last few days and I've found what is exactly what I want to solve. I have a Fredholm equation of the first kind with a continuous (kind of Gaussian) kernel. Is this kind of equations solvable analytically or the best thing that I can get is a numerical solution?",,"['integration', 'functional-analysis', 'measure-theory']"
80,Density in function space,Density in function space,,"Let $$A = \{ f\colon{\mathbb{R}^2} \to \mathbb{R}\} $$ be the set of all function with two varibles and let $$B = \{ f(x,y) = X(x)Y(y)\} $$ represent the set of all function with two variables that can be written as the product of two separate functions. The question is: is $B$ dense in $A$? Thanks.","Let $$A = \{ f\colon{\mathbb{R}^2} \to \mathbb{R}\} $$ be the set of all function with two varibles and let $$B = \{ f(x,y) = X(x)Y(y)\} $$ represent the set of all function with two variables that can be written as the product of two separate functions. The question is: is $B$ dense in $A$? Thanks.",,"['real-analysis', 'functional-analysis', 'density-function']"
81,Convex hull of a compact set in a normed linear space,Convex hull of a compact set in a normed linear space,,"I want to show that the closure of the convex hull of a compact set in a normed linear space is compact. This came up in a book on Functional Analysis where the author wants to make sense of vector valued integration. In particular the problem is if $\varphi: [0,1]\rightarrow V$ be a continuous map from the unit interval to a real normed linear space then the closure of the convex hull of $\varphi([0,1])$ is compact. There are questions on stakexchange, for example, this and this , which deal with similar issues but they do not answer my question.","I want to show that the closure of the convex hull of a compact set in a normed linear space is compact. This came up in a book on Functional Analysis where the author wants to make sense of vector valued integration. In particular the problem is if $\varphi: [0,1]\rightarrow V$ be a continuous map from the unit interval to a real normed linear space then the closure of the convex hull of $\varphi([0,1])$ is compact. There are questions on stakexchange, for example, this and this , which deal with similar issues but they do not answer my question.",,"['general-topology', 'functional-analysis']"
82,Convergent sequence under linear extension (Hahn-Banach theorem),Convergent sequence under linear extension (Hahn-Banach theorem),,"In Functional Analysis, Hahn - Banach Theorem can be stated as follows: "" Let $X$ be a real or complex vector spave and $p$ a real-valued functional on X which is additive, that is, for all $x$, $y$ in $X$, $$p(x+y) \le p(x) + p(y),\tag1\label1$$ and for every scalar $\alpha$ satisfies $$p(\alpha x) = |\alpha| p (x).\tag2\label2$$ Furthermore, let $f$ be a linear functional which is defined on a subspace $Z$ of $X$ and satisfies $$|f(x)| \le p(x)\quad \text{for all } x \in Z.\tag3\label3$$ Then $f$ has a linear extension $\widetilde{f}$ from $Z$ to $X$ satisfying $$|\widetilde{f}(x)| \le p(x)\quad \text{for all } x \in X.\tag{3*}\label{3*}""$$ Introductory to Functional Analysis with Applications, Erwin Kreyszig, Theorem 4.3 -1, p291. My question is the following: Suppose that $\{f_n\}$ is a convergent sequence of linear functionals under the uniformly convergence in the subspace $Z$ of $X$ satisfying all conditions \eqref{1}, \eqref{2}, \eqref{3}. Can we extend $\{f_n\}$ to become another convergent sequence in the whole space $X$, which is still satisfying \eqref{3*}? Thank you so much for your attention.","In Functional Analysis, Hahn - Banach Theorem can be stated as follows: "" Let $X$ be a real or complex vector spave and $p$ a real-valued functional on X which is additive, that is, for all $x$, $y$ in $X$, $$p(x+y) \le p(x) + p(y),\tag1\label1$$ and for every scalar $\alpha$ satisfies $$p(\alpha x) = |\alpha| p (x).\tag2\label2$$ Furthermore, let $f$ be a linear functional which is defined on a subspace $Z$ of $X$ and satisfies $$|f(x)| \le p(x)\quad \text{for all } x \in Z.\tag3\label3$$ Then $f$ has a linear extension $\widetilde{f}$ from $Z$ to $X$ satisfying $$|\widetilde{f}(x)| \le p(x)\quad \text{for all } x \in X.\tag{3*}\label{3*}""$$ Introductory to Functional Analysis with Applications, Erwin Kreyszig, Theorem 4.3 -1, p291. My question is the following: Suppose that $\{f_n\}$ is a convergent sequence of linear functionals under the uniformly convergence in the subspace $Z$ of $X$ satisfying all conditions \eqref{1}, \eqref{2}, \eqref{3}. Can we extend $\{f_n\}$ to become another convergent sequence in the whole space $X$, which is still satisfying \eqref{3*}? Thank you so much for your attention.",,['functional-analysis']
83,Is a closed subset of a Banach lattice complete?,Is a closed subset of a Banach lattice complete?,,"Let $(E, || \cdot||)$ be a Banach lattice. Let $E_{+}$ denote the positive cone of $E$. The metric $\rho$ on $E_{+}$ is induced by the complete lattice norm $|| \cdot ||$, which is defined by $\rho(x,y) := || x - y||$ for all $x,y \in E_{+}$. My question is that whether the metric space $(E_{+}, \rho)$ is complete? I think it is complete. Because we know that a closed subset of Banach space is complete, and the positive cone of a Banach lattice is closed, so that  $(E_{+}, \rho)$ is complete. But I read a textbook wrote a thing that ""The metric induced by a complete lattice norm need not be complete"". So I am confused now and would like to seek for your help. Thanks in advance!","Let $(E, || \cdot||)$ be a Banach lattice. Let $E_{+}$ denote the positive cone of $E$. The metric $\rho$ on $E_{+}$ is induced by the complete lattice norm $|| \cdot ||$, which is defined by $\rho(x,y) := || x - y||$ for all $x,y \in E_{+}$. My question is that whether the metric space $(E_{+}, \rho)$ is complete? I think it is complete. Because we know that a closed subset of Banach space is complete, and the positive cone of a Banach lattice is closed, so that  $(E_{+}, \rho)$ is complete. But I read a textbook wrote a thing that ""The metric induced by a complete lattice norm need not be complete"". So I am confused now and would like to seek for your help. Thanks in advance!",,"['real-analysis', 'functional-analysis', 'banach-lattices']"
84,Inverse of a square root operator.,Inverse of a square root operator.,,"Specifically, let $B$ be a unbounded self-adjoint positive-definite operator on a complex Hilbert space $H$. Denote by $B^{1/2}$ the square root operator of $B$. I am interested in showing that there exists the bounded inverse operator of $B^{1/2}$. I would appreciate if anyone can give me a hint to start with. My intention here is to prove the result without the knowledge of how $B^{1/2}$ is obtained if possible......... If it helps at all, I am reading this book Operator Approach to Linear Problems of Hydrodynamics. , in particular Section 1.4.2.","Specifically, let $B$ be a unbounded self-adjoint positive-definite operator on a complex Hilbert space $H$. Denote by $B^{1/2}$ the square root operator of $B$. I am interested in showing that there exists the bounded inverse operator of $B^{1/2}$. I would appreciate if anyone can give me a hint to start with. My intention here is to prove the result without the knowledge of how $B^{1/2}$ is obtained if possible......... If it helps at all, I am reading this book Operator Approach to Linear Problems of Hydrodynamics. , in particular Section 1.4.2.",,"['functional-analysis', 'partial-differential-equations', 'hilbert-spaces', 'unbounded-operators']"
85,Existence of a subsequence in $L^{2}$ [closed],Existence of a subsequence in  [closed],L^{2},"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Let $\Omega \subset \mathbb{R}^{n}$ an open set and $(f_{n})_{n=1}^{\infty}$ a sequence in $L^{2}(\Omega)$ such that $||f_{n}||_{2} \to ||f||_{2}$, then there exists a subsequence $(f_{n_{k}})_{k}$ of $(f_{n})_{n=1}^{\infty}$ such that $\int_{\Omega} |f_{n_{k}}  - f|^{2} \to 0$, when $k\to \infty$. The only thing I thought: How the sequence $(||f_{n}||_2)_{n=1}^{\infty}$ is limited because is convergent and $L^{2}(\Omega)$ is reflexive, then there exists a subsequence $f_{n_{k}} \rightharpoonup g$, but i don't know how to continue. Thanks","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Let $\Omega \subset \mathbb{R}^{n}$ an open set and $(f_{n})_{n=1}^{\infty}$ a sequence in $L^{2}(\Omega)$ such that $||f_{n}||_{2} \to ||f||_{2}$, then there exists a subsequence $(f_{n_{k}})_{k}$ of $(f_{n})_{n=1}^{\infty}$ such that $\int_{\Omega} |f_{n_{k}}  - f|^{2} \to 0$, when $k\to \infty$. The only thing I thought: How the sequence $(||f_{n}||_2)_{n=1}^{\infty}$ is limited because is convergent and $L^{2}(\Omega)$ is reflexive, then there exists a subsequence $f_{n_{k}} \rightharpoonup g$, but i don't know how to continue. Thanks",,"['functional-analysis', 'convergence-divergence', 'hilbert-spaces', 'lp-spaces']"
86,"Prove $\int_a^b f(x)g(x)~d\alpha(x)=f(a)\int_a^{x_0}g~d\alpha+f(b)\int_{x_0}^bg ~d\alpha,$",Prove,"\int_a^b f(x)g(x)~d\alpha(x)=f(a)\int_a^{x_0}g~d\alpha+f(b)\int_{x_0}^bg ~d\alpha,","Let $α ∈ C([a, b]) ∩ BV([a, b])$. Assume $g ∈ R(α)$ on $[a, b]$ and define $β(x) = \int_a^xg(t)dα(t)$ if $x ∈ [a, b]$. Show that (a) if $f$ is increasing  on $[a, b]$, there exists $x0 ∈ [a, b]$ such that $$\int_a^b f~d\beta=f(a)\int_x^{x_0}g~d\alpha+f(b)\int_{x_0}^bg~d\alpha,$$ (b) if, in addition, $f$ is continuous on $[a,b],$ we also have $$\int_a^b f(x)g(x)~d\alpha(x)=f(a)\int_a^{x_0}g~d\alpha+f(b)\int_{x_0}^bg ~d\alpha,$$ I have proved the first problem using the integration by parts formula and the first mean value theorem. However, I got stuck on the second problem. We don't know anything about $\alpha'(x)$, so we cannot use differential formula. I also tried to define a function $F(x)=f(a)\int_a^{x}g~d\alpha+f(b)\int_{x}^bg ~d\alpha$ to prove that $\int_a^b f(x)g(x)~d\alpha(x)$ lies between $F(a)$ and $F(b)$, but it failed because $\int_a^b f(x)g(x)~d\alpha(x)$ lies between $f(a)\int_a^bg~d\alpha$ and $f(b)\int_a^bg~d\alpha$ is not true. I have tried to use the definition to prove it. It still did not work. I totally have no idea about the second problem right now.","Let $α ∈ C([a, b]) ∩ BV([a, b])$. Assume $g ∈ R(α)$ on $[a, b]$ and define $β(x) = \int_a^xg(t)dα(t)$ if $x ∈ [a, b]$. Show that (a) if $f$ is increasing  on $[a, b]$, there exists $x0 ∈ [a, b]$ such that $$\int_a^b f~d\beta=f(a)\int_x^{x_0}g~d\alpha+f(b)\int_{x_0}^bg~d\alpha,$$ (b) if, in addition, $f$ is continuous on $[a,b],$ we also have $$\int_a^b f(x)g(x)~d\alpha(x)=f(a)\int_a^{x_0}g~d\alpha+f(b)\int_{x_0}^bg ~d\alpha,$$ I have proved the first problem using the integration by parts formula and the first mean value theorem. However, I got stuck on the second problem. We don't know anything about $\alpha'(x)$, so we cannot use differential formula. I also tried to define a function $F(x)=f(a)\int_a^{x}g~d\alpha+f(b)\int_{x}^bg ~d\alpha$ to prove that $\int_a^b f(x)g(x)~d\alpha(x)$ lies between $F(a)$ and $F(b)$, but it failed because $\int_a^b f(x)g(x)~d\alpha(x)$ lies between $f(a)\int_a^bg~d\alpha$ and $f(b)\int_a^bg~d\alpha$ is not true. I have tried to use the definition to prove it. It still did not work. I totally have no idea about the second problem right now.",,"['calculus', 'real-analysis', 'integration', 'functional-analysis', 'analysis']"
87,Does the operator $A^*A$ have a name?,Does the operator  have a name?,A^*A,"The self-adjoint operator $A^*A$ (for a generic linear operator $A$) is used in functional analysis, linear algebra, statistics, physics, and probably many other fields. I am curious if there is a standard way to refer to this operator? There's the Gramian of course, but that seems specifically to refer to a collection of finite-dimensional vectors. ""Gram operator"" seems natural to me, but Googling suggests that it's not widely used.","The self-adjoint operator $A^*A$ (for a generic linear operator $A$) is used in functional analysis, linear algebra, statistics, physics, and probably many other fields. I am curious if there is a standard way to refer to this operator? There's the Gramian of course, but that seems specifically to refer to a collection of finite-dimensional vectors. ""Gram operator"" seems natural to me, but Googling suggests that it's not widely used.",,"['linear-algebra', 'functional-analysis', 'terminology', 'operator-theory']"
88,Injectivity of a multiplication operator implies that the corresponding function is supported almost everywhere,Injectivity of a multiplication operator implies that the corresponding function is supported almost everywhere,,"Suppose that we have given any measure space $(\Omega, \Sigma, \mu)$ (such that $L^2(\mu)$ is not trivial) and consider the multiplication operator $M_g : L^2(\mu) \rightarrow L^2(\mu)$ given by $M_g (\phi) (x) = g(x) \phi (x)$. Here $g$ is a bounded, measurable function from $\Omega$ to $\mathbb{C}$. Now suppose that $M_g$ is injective, i.e. we have that for every $f \in L^2(\mu)$ $$ g \cdot f = 0  $$ implies $$ f = 0. $$ Now we look at the set $M=\{ \omega \in \Omega : g(\omega)=0 \}$ where $g$ vanishes. Question : Does $M$ always have $\mu$-measure zero? Partial Answer : Suppose that $\mu$ is $\sigma$-finite and suppose that $M$ does not have measure zero. Then we can find a subset $M_0$ of $M$ which has positive but finite measure, i.e. the characteristic function $\chi_{M_0}$ is in $L^2(\mu)$. But we have $M_g \chi_{M_0} = 0$, so by the assumption $\chi_{M_0}$ is the zero function. This is a contradiction. The problem with this is that in general we do not have $\sigma$-finiteness of $\mu$. So this argument won't work. Maybe some context: This statement is needed in a proof of the spectral theorem for unbounded self-adjoint operators on a (not necessarily separable) Hilbert space. The function $g$ is given by the spectral theorem for bounded, normal operators, i.e. we are given a non-trivial Hilbert space $H$ and a normal operator $A$ on it which then is unitarily equivalent to multiplication by $g$ on $L^2(\mu)$. If the Hilbert space were separable, the function $g$ would live on a $\sigma$-finite measure space; but this is not assumed.","Suppose that we have given any measure space $(\Omega, \Sigma, \mu)$ (such that $L^2(\mu)$ is not trivial) and consider the multiplication operator $M_g : L^2(\mu) \rightarrow L^2(\mu)$ given by $M_g (\phi) (x) = g(x) \phi (x)$. Here $g$ is a bounded, measurable function from $\Omega$ to $\mathbb{C}$. Now suppose that $M_g$ is injective, i.e. we have that for every $f \in L^2(\mu)$ $$ g \cdot f = 0  $$ implies $$ f = 0. $$ Now we look at the set $M=\{ \omega \in \Omega : g(\omega)=0 \}$ where $g$ vanishes. Question : Does $M$ always have $\mu$-measure zero? Partial Answer : Suppose that $\mu$ is $\sigma$-finite and suppose that $M$ does not have measure zero. Then we can find a subset $M_0$ of $M$ which has positive but finite measure, i.e. the characteristic function $\chi_{M_0}$ is in $L^2(\mu)$. But we have $M_g \chi_{M_0} = 0$, so by the assumption $\chi_{M_0}$ is the zero function. This is a contradiction. The problem with this is that in general we do not have $\sigma$-finiteness of $\mu$. So this argument won't work. Maybe some context: This statement is needed in a proof of the spectral theorem for unbounded self-adjoint operators on a (not necessarily separable) Hilbert space. The function $g$ is given by the spectral theorem for bounded, normal operators, i.e. we are given a non-trivial Hilbert space $H$ and a normal operator $A$ on it which then is unitarily equivalent to multiplication by $g$ on $L^2(\mu)$. If the Hilbert space were separable, the function $g$ would live on a $\sigma$-finite measure space; but this is not assumed.",,"['functional-analysis', 'operator-theory', 'lp-spaces', 'spectral-theory']"
89,Poisson Equation - $L^2$ boundary regularity,Poisson Equation -  boundary regularity,L^2,"Let $\mathbb{R}_{+}^n = \{(x',x_n): x' \in \mathbb{R}^{n-1}, x_n > 0 \}$ be the half space. It is known that if $u \in H_0^1(\mathbb{R}_{+}^n)$ is a weak solution to  $-\Delta u = f$ with $f \in L^2(\mathbb{R}_{+}^n)$ then in fact $u \in H^2(\mathbb{R}_{+}^n)$. Furthermore if $f \in H^1$ then $u \in H^3$, etc. This second part can be shown as follows: first show that for $u\in H_0^1\cap H^2$, the tangential derivatives $\partial_j u \in H_0^1(\mathbb{R}_+^n)$ (ie for $j\neq n$) is a weak solution to poisson equation, thus by first part $\partial_j u \in H^2$. Secondly, we need to just show $\partial_n^2 u\in H^1$, which comes from $-\partial_n^2 u = \partial_1^2 u + \cdot \cdot \cdot + \partial_{n-1}^2u + f \in H^1$. Now suppose we only have that $f \in L^2$ with $\partial_j f \in L^2$ for some $j$. I would like $\partial_j u \in H^2$. In the case when the derivative is tangential to boundary, ie $j\neq n$ then this extra smoothness is carried over to $u$ as explained above. But what happens if $j=n$, ie the extra smoothness is normal to the boundary? The argument above requires tangent derivatives to control the normal derivative so it doesn't work. Is there some other way to show that that the additional normal smoothness on $f$ carries over to $u$, or is this in fact false?","Let $\mathbb{R}_{+}^n = \{(x',x_n): x' \in \mathbb{R}^{n-1}, x_n > 0 \}$ be the half space. It is known that if $u \in H_0^1(\mathbb{R}_{+}^n)$ is a weak solution to  $-\Delta u = f$ with $f \in L^2(\mathbb{R}_{+}^n)$ then in fact $u \in H^2(\mathbb{R}_{+}^n)$. Furthermore if $f \in H^1$ then $u \in H^3$, etc. This second part can be shown as follows: first show that for $u\in H_0^1\cap H^2$, the tangential derivatives $\partial_j u \in H_0^1(\mathbb{R}_+^n)$ (ie for $j\neq n$) is a weak solution to poisson equation, thus by first part $\partial_j u \in H^2$. Secondly, we need to just show $\partial_n^2 u\in H^1$, which comes from $-\partial_n^2 u = \partial_1^2 u + \cdot \cdot \cdot + \partial_{n-1}^2u + f \in H^1$. Now suppose we only have that $f \in L^2$ with $\partial_j f \in L^2$ for some $j$. I would like $\partial_j u \in H^2$. In the case when the derivative is tangential to boundary, ie $j\neq n$ then this extra smoothness is carried over to $u$ as explained above. But what happens if $j=n$, ie the extra smoothness is normal to the boundary? The argument above requires tangent derivatives to control the normal derivative so it doesn't work. Is there some other way to show that that the additional normal smoothness on $f$ carries over to $u$, or is this in fact false?",,"['functional-analysis', 'partial-differential-equations', 'regularity-theory-of-pdes', 'elliptic-equations']"
90,Cauchy-Schwarz inequality and homogeneous Sobolev space,Cauchy-Schwarz inequality and homogeneous Sobolev space,,"Let $f\in \dot{H}^s$ with $s\in (0,1)$ and $g\in \dot{H}^{-s}$, where $\dot{H}^s$ denote the fractional homogeneous Sobolev space $\dot{W}^{p,s}$with $p=2$. Does one have the following Cauchy-Schwarz type inequality? $$ (f,g)_{L^2}\leq \| f \|_{\dot{H}^s} \| g \|_{\dot{H}^{-s}}, $$ where $(.,.)_{L^2}$ is the $L^2$-inner product and on the right are the seminorms corresponding to the homogeneous spaces (given by the Gagliardo seminorms). I have seen an inequality of this type for fractional Sobolev spaces but without any proof. Does one have a reference (also for the case above)? Thanks for your help!","Let $f\in \dot{H}^s$ with $s\in (0,1)$ and $g\in \dot{H}^{-s}$, where $\dot{H}^s$ denote the fractional homogeneous Sobolev space $\dot{W}^{p,s}$with $p=2$. Does one have the following Cauchy-Schwarz type inequality? $$ (f,g)_{L^2}\leq \| f \|_{\dot{H}^s} \| g \|_{\dot{H}^{-s}}, $$ where $(.,.)_{L^2}$ is the $L^2$-inner product and on the right are the seminorms corresponding to the homogeneous spaces (given by the Gagliardo seminorms). I have seen an inequality of this type for fractional Sobolev spaces but without any proof. Does one have a reference (also for the case above)? Thanks for your help!",,"['functional-analysis', 'inequality', 'sobolev-spaces']"
91,Limit in a sequence of Hilbert spaces,Limit in a sequence of Hilbert spaces,,"Let $H$ be an complex infinite dimensional Hilbert space Let $\{H_n\}_{n \in \Bbb N}$ be a sequence of subspaces of $H$ such that $\cap_{n=1}^\infty H_n = H_0$, where $H_0$ is a one dimensional subspace of $H$ and such that $H_{n+1}\subsetneq H_n$ Let, for each $n \in \Bbb N$, be $v_n \in H_n$. My questions are: is $\{v_n\}$ convergent ? if it is convergent, is $v_n \to v_0 \in H_0$ ? Thanks for any suggestion","Let $H$ be an complex infinite dimensional Hilbert space Let $\{H_n\}_{n \in \Bbb N}$ be a sequence of subspaces of $H$ such that $\cap_{n=1}^\infty H_n = H_0$, where $H_0$ is a one dimensional subspace of $H$ and such that $H_{n+1}\subsetneq H_n$ Let, for each $n \in \Bbb N$, be $v_n \in H_n$. My questions are: is $\{v_n\}$ convergent ? if it is convergent, is $v_n \to v_0 \in H_0$ ? Thanks for any suggestion",,"['functional-analysis', 'metric-spaces', 'hilbert-spaces']"
92,A question about uniformly bounded set of operators,A question about uniformly bounded set of operators,,"Let $ \ (E, \lVert \cdot \rVert_E) \ $ and $ \ (F , \lVert \cdot \rVert_F ) \ $ be real normed vector spaces, $ \ \big( \mathcal{L}(E,F) , \lVert \cdot \rVert_L \big) \ $ be the real normed vector space of continuous linear transformations from $E$ to $F$ with the usual norm induced by $ \ \lVert \cdot \rVert_E \ $ and $ \ \lVert \cdot \rVert_F \ $ and $ \ \mathcal{C} \subset \mathcal{L}(E,F) \ $. Suppose that $ \ \mathcal{C} \neq \varnothing \ $ and $\mathcal{C}$ is (uniformly) bounded, ie, there exists $ \ R > 0 \ $ such that $ \ \mathcal{C} \subset B_L (0_L , R) \subset \mathcal{L}(E,F)$. Where $ \ 0_L : E \to F \ $ is the zero function ⏤ the origin of $ \ L = \mathcal{L}(E,F)$ ⏤ and $ \ B_L (0_L , R) \ $ is the open ball of $L$ centered in $0_L \, $. Let $ \ \alpha > 0 \, $. I want to know if (with possibly some restrictions on $\alpha$) the set $$G = \bigcap_{T \in \mathcal{C}} T^{-1} \big[ B_F(0_F, \alpha) \big]$$ is open in $E$. Here are my efforts. First it is obvious that $ \ T^{-1} \big[ B_F(0_F, \alpha) \big] \ $ is an open set of $E$, $ \forall T \in \mathcal{C} \, $. Hence the answer is clearly positive in the case where $\mathcal{C}$ is finite. From now on let us suppose that $ \, \mathcal{C} \, $ is infinite. So we have that $L$ is infinite and $ \ \mathcal{C} \setminus \{ 0_L \} \neq \varnothing \, $. It follows that $ \ \mathcal{C} \neq \{ 0_L \}$, $ \ E \neq \{ 0_E \} \ $ and $ \ F \neq \{ 0_F \}$. Obs .: when $\mathcal{C}$ is countable, the set $G$ is a $G_{\delta}$ of $E$. I was able to prove that: $ \ B_E \left( 0_E , \alpha/R \right) \subset G \ $; For all $ \ x \in G \ $ and all $ \ T \in \mathcal{C} \, $, one has $ \ \lVert T(x) \rVert_F < \alpha \ $; For all $ \ x \in E \ $ and all $ \ T \in \mathcal{C} \, $, one has $ \ \lVert T(x) \rVert_F \leq \lVert T \rVert_L \cdot \lVert x \rVert_E < R \cdot \lVert x \rVert_E \ $. Let $ \ D = \Big\{ \lVert T(x) \rVert_F \in \mathbb{R}_{^+} \ : \ T \in \mathcal{C} \ \ \text{ and } \ \ x \in G \Big\} \, $. Since $ \ 0_E \in G \ $ and $ \ \mathcal{C} \neq \varnothing \, $, we have that $ \ 0 \in D \ $. Thus $ \ D \neq \varnothing \, $. Moreover, by the second item above, $D$ is bounded. So there exists the supremum (least upper bound) of $D$, say $ \ s = \sup(D) = \text{lub}(D) \, $, with $ \ 0 \leq s \leq \alpha \, $. Here is a digression: actually $ \ s>0 \, $. Suppose that $ \ s = 0 \ $. Then one has $ \ D = {0} \, $. Let $ \ x \in G \, $. We have $ \ \lVert T(x) \rVert_F = 0 \ \Rightarrow \ T(x) = 0_F \ \Rightarrow \ x \in ker(T)$, $ \forall T \in \mathcal{C} \, $. It follows that $ \ x \in \bigcap_{T \in \mathcal{C}} ker(T) \, $. Consequently, $ \ B_E \left( 0_E , \alpha/R \right) \subset G \subset \bigcap_{T \in \mathcal{C}} ker(T) \, $. Now $ \ \bigcap_{T \in \mathcal{C}} ker(T) \ $ is a vector subspace of $E$ that contains an open ball. Whence $ \ \bigcap_{T \in \mathcal{C}} ker(T) = E \, $. Thereafter, for all $ \ T \in \mathcal{C} \, $, we have that $ \ E = \bigcap_{A \in \mathcal{C}} ker(A) \subset ker(T) \ \Rightarrow \ ker(T) = E \ \Rightarrow \ T = 0_L \, $. So $ \ \mathcal{C} = \{ 0_L \} \, $, a contradiction with the fact that $\mathcal{C}$ is infinite. Similarly, fix $ \ p \in G \ $ and let $ \ D_p = \Big\{ \lVert T(p) \rVert_F \in \mathbb{R}_{^+} \ : \ T \in \mathcal{C} \Big\} \, $. Since $ \ \mathcal{C} \neq \varnothing \, $, we have that $ \ D_p \neq \varnothing \, $. Moreover, $\forall T \in \mathcal{C}$, again by the second item above, we have that $ \ \lVert T(p) \rVert_F < \alpha \, $. Hence $D_p \, $ is bounded. So there exists the supremum $ \ s_p = \sup(D_p) \, $, with $ \ 0 \leq s_p \leq \alpha \, $. Since $ \ D_p \subset D \, $, we have that $ \ 0 \leq s_p \leq s \leq \alpha \, $. Indeed it is easy to show that $ \ \displaystyle \sup_{p \in G} s_p = s \, $. If $ \ p = 0_E \, $, then $ \ D_p = \{ 0 \} \ \Rightarrow \ s_p = 0 \, $. But we can choose $ \ p \neq 0_E \, $, since $ \ B_E \left( 0_E , \alpha/R \right) \subset G \ $ and $ \ B_E \left( 0_E , \alpha/R \right) \neq \{ 0_E \} \, $, as $ \ E \neq \{ 0_E \} \, $. In that case, $ \ 0 < s_p \leq s \leq \alpha \, $. Like above, again we proved that $ \ s>0 \, $. My first attempt was restricting $ \ \alpha \neq s \, $. Then $ \ 0 < s_p \leq s < \alpha \, $ and we have that $$0< \frac{\alpha - s}{R} \leq \frac{\alpha - s_p}{R} \ \ . $$ But then I am afraid that I arrived at a contradiction. I will explain it below. Let $ \ p \in G \ $ and $ \ x \in B_E \left( p , \frac{\alpha - s_p}{R} \right) \, $. Hence, for all $ \ T \in \mathcal{C} \, $, one has $ \ \lVert T(p) \rVert_F \leq s_p \ $ and then \begin{eqnarray*} \lVert T(x) \rVert_F & = & \lVert T(x - p) + T(p) \rVert_F \\ & \leq & \lVert T(x - p) \rVert_F + \lVert T(p) \rVert_F \\ & \leq & \lVert T \rVert_L \cdot \lVert x - p \rVert_E + \lVert T(p) \rVert_F \\ & \leq & \lVert T \rVert_L \cdot \lVert x - p \rVert_E + s_p \\ & < & R \cdot \lVert x - p \rVert_E + s_p \\ & < & R \cdot \frac{(\alpha - s_p)}{R} + s_p \\ &=& \alpha \ . \end{eqnarray*} Thus $ \ x \in T^{-1} \big[ B_F (0_F , \alpha) \big] \, $, $\forall T \in \mathcal{C}$, that is, $x \in G$. Therefore $ \ B_E \left( p , \frac{\alpha - s_p}{R} \right) \subset G \ \Rightarrow \ p \in int(G) \, $. Since $p$ is arbitrary, it follows that $G$ is open. But a similar argument shows a stronger fact: Let $ \ x \in G \ $ and $ \ u \in B_E \left( x , \frac{\alpha - s}{R} \right) \, $. Hence, for all $ \ T \in \mathcal{C} \, $, one has $ \ \lVert T(x) \rVert_F \leq s \ $ and then \begin{eqnarray*} \lVert T(u) \rVert_F & = & \lVert T(u - x) + T(x) \rVert_F \\ & \leq & \lVert T(u - x) \rVert_F + \lVert T(x) \rVert_F \\ & \leq & \lVert T \rVert_L \cdot \lVert u - x \rVert_E + \lVert T(x) \rVert_F \\ & \leq & \lVert T \rVert_L \cdot \lVert u - x \rVert_E + s \\ & < & R \cdot \lVert u - x \rVert_E + s \\ & < & R \cdot \frac{(\alpha - s)}{R} + s \\ &=& \alpha \ . \end{eqnarray*} Thus $ \ u \in T^{-1} \big[ B_F (0_F , \alpha) \big] \, $, $\forall T \in \mathcal{C}$, that is, $u \in G$. Therefore $ \ B_E \left( x , \frac{\alpha - s}{R} \right) \subset G \, $. This reasoning gives a broader result because the radius $ \ \frac{\alpha - s}{R} \ $ do not depend on $x \, $: For all $ \ x \in G \ $ we have that $ \ B_E \left( x , \frac{\alpha - s}{R} \right) \subset G \, $. Let $ \ x \in E$. If $ \ x \in B_E (0_E , \alpha/R)$, then $ \ x \in G$. Suppose that $ \ x \notin B_E (0_E , \alpha/R) \, $. Then $ \ x \neq 0_E \ $ and $ \ 0 < \frac{\alpha}{2R} < \frac{\alpha}{R} \leq \lVert x \rVert_E \ \Rightarrow \ 2R \lVert x \rVert_E - \alpha > 0 \, $. Let $ \ (x_n) \in E^{\mathbb{N}} \ $ be such that, $\forall n \in \mathbb{N}$, $$x_n = \frac{[\alpha + n(\alpha - s)]}{2R \lVert x \rVert_E} \cdot x$$ So we have $ \ \lVert x_0 \rVert_E = \frac{\alpha}{2R} < \frac{\alpha}{R} \ \Rightarrow \ x_0 \in B_E \left( 0_E , \frac{\alpha}{R} \right) \subset G \ \Rightarrow \ x_0 \in G \ \Rightarrow \ B_E \left( x_0 , \frac{\alpha - s}{R} \right) \subset G \, $. Let $ \ N = \Big\{ n \in \mathbb{N} \ : \ B_E \left( x_n , \frac{\alpha - s}{R} \right) \subset G \Big\}$. Hence $ \ 0 \in N$. Let $ \ n \in N$, ie, $B_E \left( x_n , \frac{\alpha - s}{R} \right) \subset G$. We have that \begin{eqnarray*} \lVert x_{n+1} - x_n \rVert_E & = & \left\lVert \frac{[\alpha + (n+1)(\alpha - s)]}{2R \lVert x \rVert_E} \cdot x - \frac{[\alpha + n(\alpha - s)]}{2R \lVert x \rVert_E} \cdot x \right\rVert_E \\ & = & \frac{[\alpha + (n+1)(\alpha - s)] - [\alpha + n(\alpha - s)]}{2R} \\ & = & \frac{\alpha - s}{2R} \\ & < & \frac{\alpha - s}{R} \\ & \Rightarrow & x_{n+1} \in B_E \left( x_n , \frac{\alpha - s}{R} \right) \subset G \\ & \Rightarrow & x_{n+1} \in G \\ & \Rightarrow & B_E \left( x_{n+1} , \frac{\alpha - s}{R} \right) \subset G \\ & \Rightarrow & n+1 \in N \ . \end{eqnarray*} By induction, we conclude that $ \ N = \mathbb{N}$, that is, $ \ (x_n) \in G^{\mathbb{N}} \, $, with $ \ B_E \left( x_n , \frac{\alpha - s}{R} \right) \subset G$, $\forall n \in \mathbb{N}$. Note that $ \ \frac{2R \lVert x \rVert_E - \alpha}{\alpha - s} > 0 \, $. Since $\mathbb{N}$ is unbounded, the set $ \ J = \Big\{ n \in \mathbb{N} \ : \ n > \frac{2R \lVert x \rVert_E - \alpha}{\alpha - s} \Big\} \ $ is nonempty. Off course we have that $ \ 0 \notin J$. Since $\mathbb{N}$ is well-ordered, there exists the least element (minimum) of $J$, say $ \ j = \min(J)$, with $ \ j \in J$, that is, $j > \frac{2R \lVert x \rVert_E - \alpha}{\alpha - s} > 0 \, $. Thus $ \ m=j-1 \in \mathbb{N} \ $ and $ \ j = m+1 \in \mathbb{N}^*$. By the minimality of $j$ we have either $ \ m \notin J \ \Rightarrow \ m \leq \frac{2R \lVert x \rVert_E - \alpha}{\alpha - s} \, $. Therefore $ \ \lVert x_m \rVert_E = \frac{\alpha + m(\alpha - s)}{2R} \leq \lVert x \rVert_E < \frac{\alpha + (m+1)(\alpha - s)}{2R} = \lVert x_{m+1} \rVert_E \ $ and we are left with $ \ \lVert x_m \rVert_E = \frac{\alpha + m(\alpha - s)}{2R} \leq \lVert x \rVert_E < \frac{\alpha + (m+1)(\alpha - s)}{2R} = \frac{\alpha + m(\alpha - s)}{2R} + \frac{\alpha - s}{2R} = \lVert x_{m} \rVert_E + \frac{\alpha - s}{2R} \ $, that is, $ \ \lVert x_m \rVert_E \leq \lVert x \rVert_E < \lVert x_{m} \rVert_E + \frac{\alpha - s}{2R} \ $. Then $ \ \lVert x - x_m \rVert_E \leq \big| \lVert x \rVert_E - \lVert x_m \rVert_E \big| < \frac{\alpha - s}{2R} \ $, wich implies $ \ x \in B_E \left( x_m , \frac{\alpha - s}{R} \right) \subset G \ \Rightarrow \ x \in G$. As $x$ was arbitrarily chosen, we conclude that $ \ G=E$. Let $ \ T \in \mathcal{C} \ $ and $ \ y \in im(T)$. There exists $ \ x \in E \ $ such that $ \ y = T(x)$. But $ \ x \in E = G = \bigcap_{A \in \mathcal{C}} A^{-1} \big[ B_F (0_F , \alpha) \big] \subset T^{-1} \big[ B_F (0_F , \alpha) \big] \ $ and we have that $ \ y = T(x) \in B_F(0_F, \alpha)$. Then $ \ im(T) \subset B_F (0_F , \alpha) \ $ and $ \, im(T) \, $ is a bounded vector subspace of $F$. Hence $ \ im(T) = \{ 0_F \} \ \Rightarrow \ T = 0_L \, $. Therefore we have that $ \ \mathcal{C} = \{ 0_L \}$, a contradiction. Is there anything wrong with this reasoning? Are my calculation steps correct? If it is right, then we cannot have $ \ \alpha \neq s \ $ and we are left with $ \ s= \alpha \, $. That is exactly the case which I was not able to handle. Like always, any hint is appreciated. Thanks in advance.","Let $ \ (E, \lVert \cdot \rVert_E) \ $ and $ \ (F , \lVert \cdot \rVert_F ) \ $ be real normed vector spaces, $ \ \big( \mathcal{L}(E,F) , \lVert \cdot \rVert_L \big) \ $ be the real normed vector space of continuous linear transformations from $E$ to $F$ with the usual norm induced by $ \ \lVert \cdot \rVert_E \ $ and $ \ \lVert \cdot \rVert_F \ $ and $ \ \mathcal{C} \subset \mathcal{L}(E,F) \ $. Suppose that $ \ \mathcal{C} \neq \varnothing \ $ and $\mathcal{C}$ is (uniformly) bounded, ie, there exists $ \ R > 0 \ $ such that $ \ \mathcal{C} \subset B_L (0_L , R) \subset \mathcal{L}(E,F)$. Where $ \ 0_L : E \to F \ $ is the zero function ⏤ the origin of $ \ L = \mathcal{L}(E,F)$ ⏤ and $ \ B_L (0_L , R) \ $ is the open ball of $L$ centered in $0_L \, $. Let $ \ \alpha > 0 \, $. I want to know if (with possibly some restrictions on $\alpha$) the set $$G = \bigcap_{T \in \mathcal{C}} T^{-1} \big[ B_F(0_F, \alpha) \big]$$ is open in $E$. Here are my efforts. First it is obvious that $ \ T^{-1} \big[ B_F(0_F, \alpha) \big] \ $ is an open set of $E$, $ \forall T \in \mathcal{C} \, $. Hence the answer is clearly positive in the case where $\mathcal{C}$ is finite. From now on let us suppose that $ \, \mathcal{C} \, $ is infinite. So we have that $L$ is infinite and $ \ \mathcal{C} \setminus \{ 0_L \} \neq \varnothing \, $. It follows that $ \ \mathcal{C} \neq \{ 0_L \}$, $ \ E \neq \{ 0_E \} \ $ and $ \ F \neq \{ 0_F \}$. Obs .: when $\mathcal{C}$ is countable, the set $G$ is a $G_{\delta}$ of $E$. I was able to prove that: $ \ B_E \left( 0_E , \alpha/R \right) \subset G \ $; For all $ \ x \in G \ $ and all $ \ T \in \mathcal{C} \, $, one has $ \ \lVert T(x) \rVert_F < \alpha \ $; For all $ \ x \in E \ $ and all $ \ T \in \mathcal{C} \, $, one has $ \ \lVert T(x) \rVert_F \leq \lVert T \rVert_L \cdot \lVert x \rVert_E < R \cdot \lVert x \rVert_E \ $. Let $ \ D = \Big\{ \lVert T(x) \rVert_F \in \mathbb{R}_{^+} \ : \ T \in \mathcal{C} \ \ \text{ and } \ \ x \in G \Big\} \, $. Since $ \ 0_E \in G \ $ and $ \ \mathcal{C} \neq \varnothing \, $, we have that $ \ 0 \in D \ $. Thus $ \ D \neq \varnothing \, $. Moreover, by the second item above, $D$ is bounded. So there exists the supremum (least upper bound) of $D$, say $ \ s = \sup(D) = \text{lub}(D) \, $, with $ \ 0 \leq s \leq \alpha \, $. Here is a digression: actually $ \ s>0 \, $. Suppose that $ \ s = 0 \ $. Then one has $ \ D = {0} \, $. Let $ \ x \in G \, $. We have $ \ \lVert T(x) \rVert_F = 0 \ \Rightarrow \ T(x) = 0_F \ \Rightarrow \ x \in ker(T)$, $ \forall T \in \mathcal{C} \, $. It follows that $ \ x \in \bigcap_{T \in \mathcal{C}} ker(T) \, $. Consequently, $ \ B_E \left( 0_E , \alpha/R \right) \subset G \subset \bigcap_{T \in \mathcal{C}} ker(T) \, $. Now $ \ \bigcap_{T \in \mathcal{C}} ker(T) \ $ is a vector subspace of $E$ that contains an open ball. Whence $ \ \bigcap_{T \in \mathcal{C}} ker(T) = E \, $. Thereafter, for all $ \ T \in \mathcal{C} \, $, we have that $ \ E = \bigcap_{A \in \mathcal{C}} ker(A) \subset ker(T) \ \Rightarrow \ ker(T) = E \ \Rightarrow \ T = 0_L \, $. So $ \ \mathcal{C} = \{ 0_L \} \, $, a contradiction with the fact that $\mathcal{C}$ is infinite. Similarly, fix $ \ p \in G \ $ and let $ \ D_p = \Big\{ \lVert T(p) \rVert_F \in \mathbb{R}_{^+} \ : \ T \in \mathcal{C} \Big\} \, $. Since $ \ \mathcal{C} \neq \varnothing \, $, we have that $ \ D_p \neq \varnothing \, $. Moreover, $\forall T \in \mathcal{C}$, again by the second item above, we have that $ \ \lVert T(p) \rVert_F < \alpha \, $. Hence $D_p \, $ is bounded. So there exists the supremum $ \ s_p = \sup(D_p) \, $, with $ \ 0 \leq s_p \leq \alpha \, $. Since $ \ D_p \subset D \, $, we have that $ \ 0 \leq s_p \leq s \leq \alpha \, $. Indeed it is easy to show that $ \ \displaystyle \sup_{p \in G} s_p = s \, $. If $ \ p = 0_E \, $, then $ \ D_p = \{ 0 \} \ \Rightarrow \ s_p = 0 \, $. But we can choose $ \ p \neq 0_E \, $, since $ \ B_E \left( 0_E , \alpha/R \right) \subset G \ $ and $ \ B_E \left( 0_E , \alpha/R \right) \neq \{ 0_E \} \, $, as $ \ E \neq \{ 0_E \} \, $. In that case, $ \ 0 < s_p \leq s \leq \alpha \, $. Like above, again we proved that $ \ s>0 \, $. My first attempt was restricting $ \ \alpha \neq s \, $. Then $ \ 0 < s_p \leq s < \alpha \, $ and we have that $$0< \frac{\alpha - s}{R} \leq \frac{\alpha - s_p}{R} \ \ . $$ But then I am afraid that I arrived at a contradiction. I will explain it below. Let $ \ p \in G \ $ and $ \ x \in B_E \left( p , \frac{\alpha - s_p}{R} \right) \, $. Hence, for all $ \ T \in \mathcal{C} \, $, one has $ \ \lVert T(p) \rVert_F \leq s_p \ $ and then \begin{eqnarray*} \lVert T(x) \rVert_F & = & \lVert T(x - p) + T(p) \rVert_F \\ & \leq & \lVert T(x - p) \rVert_F + \lVert T(p) \rVert_F \\ & \leq & \lVert T \rVert_L \cdot \lVert x - p \rVert_E + \lVert T(p) \rVert_F \\ & \leq & \lVert T \rVert_L \cdot \lVert x - p \rVert_E + s_p \\ & < & R \cdot \lVert x - p \rVert_E + s_p \\ & < & R \cdot \frac{(\alpha - s_p)}{R} + s_p \\ &=& \alpha \ . \end{eqnarray*} Thus $ \ x \in T^{-1} \big[ B_F (0_F , \alpha) \big] \, $, $\forall T \in \mathcal{C}$, that is, $x \in G$. Therefore $ \ B_E \left( p , \frac{\alpha - s_p}{R} \right) \subset G \ \Rightarrow \ p \in int(G) \, $. Since $p$ is arbitrary, it follows that $G$ is open. But a similar argument shows a stronger fact: Let $ \ x \in G \ $ and $ \ u \in B_E \left( x , \frac{\alpha - s}{R} \right) \, $. Hence, for all $ \ T \in \mathcal{C} \, $, one has $ \ \lVert T(x) \rVert_F \leq s \ $ and then \begin{eqnarray*} \lVert T(u) \rVert_F & = & \lVert T(u - x) + T(x) \rVert_F \\ & \leq & \lVert T(u - x) \rVert_F + \lVert T(x) \rVert_F \\ & \leq & \lVert T \rVert_L \cdot \lVert u - x \rVert_E + \lVert T(x) \rVert_F \\ & \leq & \lVert T \rVert_L \cdot \lVert u - x \rVert_E + s \\ & < & R \cdot \lVert u - x \rVert_E + s \\ & < & R \cdot \frac{(\alpha - s)}{R} + s \\ &=& \alpha \ . \end{eqnarray*} Thus $ \ u \in T^{-1} \big[ B_F (0_F , \alpha) \big] \, $, $\forall T \in \mathcal{C}$, that is, $u \in G$. Therefore $ \ B_E \left( x , \frac{\alpha - s}{R} \right) \subset G \, $. This reasoning gives a broader result because the radius $ \ \frac{\alpha - s}{R} \ $ do not depend on $x \, $: For all $ \ x \in G \ $ we have that $ \ B_E \left( x , \frac{\alpha - s}{R} \right) \subset G \, $. Let $ \ x \in E$. If $ \ x \in B_E (0_E , \alpha/R)$, then $ \ x \in G$. Suppose that $ \ x \notin B_E (0_E , \alpha/R) \, $. Then $ \ x \neq 0_E \ $ and $ \ 0 < \frac{\alpha}{2R} < \frac{\alpha}{R} \leq \lVert x \rVert_E \ \Rightarrow \ 2R \lVert x \rVert_E - \alpha > 0 \, $. Let $ \ (x_n) \in E^{\mathbb{N}} \ $ be such that, $\forall n \in \mathbb{N}$, $$x_n = \frac{[\alpha + n(\alpha - s)]}{2R \lVert x \rVert_E} \cdot x$$ So we have $ \ \lVert x_0 \rVert_E = \frac{\alpha}{2R} < \frac{\alpha}{R} \ \Rightarrow \ x_0 \in B_E \left( 0_E , \frac{\alpha}{R} \right) \subset G \ \Rightarrow \ x_0 \in G \ \Rightarrow \ B_E \left( x_0 , \frac{\alpha - s}{R} \right) \subset G \, $. Let $ \ N = \Big\{ n \in \mathbb{N} \ : \ B_E \left( x_n , \frac{\alpha - s}{R} \right) \subset G \Big\}$. Hence $ \ 0 \in N$. Let $ \ n \in N$, ie, $B_E \left( x_n , \frac{\alpha - s}{R} \right) \subset G$. We have that \begin{eqnarray*} \lVert x_{n+1} - x_n \rVert_E & = & \left\lVert \frac{[\alpha + (n+1)(\alpha - s)]}{2R \lVert x \rVert_E} \cdot x - \frac{[\alpha + n(\alpha - s)]}{2R \lVert x \rVert_E} \cdot x \right\rVert_E \\ & = & \frac{[\alpha + (n+1)(\alpha - s)] - [\alpha + n(\alpha - s)]}{2R} \\ & = & \frac{\alpha - s}{2R} \\ & < & \frac{\alpha - s}{R} \\ & \Rightarrow & x_{n+1} \in B_E \left( x_n , \frac{\alpha - s}{R} \right) \subset G \\ & \Rightarrow & x_{n+1} \in G \\ & \Rightarrow & B_E \left( x_{n+1} , \frac{\alpha - s}{R} \right) \subset G \\ & \Rightarrow & n+1 \in N \ . \end{eqnarray*} By induction, we conclude that $ \ N = \mathbb{N}$, that is, $ \ (x_n) \in G^{\mathbb{N}} \, $, with $ \ B_E \left( x_n , \frac{\alpha - s}{R} \right) \subset G$, $\forall n \in \mathbb{N}$. Note that $ \ \frac{2R \lVert x \rVert_E - \alpha}{\alpha - s} > 0 \, $. Since $\mathbb{N}$ is unbounded, the set $ \ J = \Big\{ n \in \mathbb{N} \ : \ n > \frac{2R \lVert x \rVert_E - \alpha}{\alpha - s} \Big\} \ $ is nonempty. Off course we have that $ \ 0 \notin J$. Since $\mathbb{N}$ is well-ordered, there exists the least element (minimum) of $J$, say $ \ j = \min(J)$, with $ \ j \in J$, that is, $j > \frac{2R \lVert x \rVert_E - \alpha}{\alpha - s} > 0 \, $. Thus $ \ m=j-1 \in \mathbb{N} \ $ and $ \ j = m+1 \in \mathbb{N}^*$. By the minimality of $j$ we have either $ \ m \notin J \ \Rightarrow \ m \leq \frac{2R \lVert x \rVert_E - \alpha}{\alpha - s} \, $. Therefore $ \ \lVert x_m \rVert_E = \frac{\alpha + m(\alpha - s)}{2R} \leq \lVert x \rVert_E < \frac{\alpha + (m+1)(\alpha - s)}{2R} = \lVert x_{m+1} \rVert_E \ $ and we are left with $ \ \lVert x_m \rVert_E = \frac{\alpha + m(\alpha - s)}{2R} \leq \lVert x \rVert_E < \frac{\alpha + (m+1)(\alpha - s)}{2R} = \frac{\alpha + m(\alpha - s)}{2R} + \frac{\alpha - s}{2R} = \lVert x_{m} \rVert_E + \frac{\alpha - s}{2R} \ $, that is, $ \ \lVert x_m \rVert_E \leq \lVert x \rVert_E < \lVert x_{m} \rVert_E + \frac{\alpha - s}{2R} \ $. Then $ \ \lVert x - x_m \rVert_E \leq \big| \lVert x \rVert_E - \lVert x_m \rVert_E \big| < \frac{\alpha - s}{2R} \ $, wich implies $ \ x \in B_E \left( x_m , \frac{\alpha - s}{R} \right) \subset G \ \Rightarrow \ x \in G$. As $x$ was arbitrarily chosen, we conclude that $ \ G=E$. Let $ \ T \in \mathcal{C} \ $ and $ \ y \in im(T)$. There exists $ \ x \in E \ $ such that $ \ y = T(x)$. But $ \ x \in E = G = \bigcap_{A \in \mathcal{C}} A^{-1} \big[ B_F (0_F , \alpha) \big] \subset T^{-1} \big[ B_F (0_F , \alpha) \big] \ $ and we have that $ \ y = T(x) \in B_F(0_F, \alpha)$. Then $ \ im(T) \subset B_F (0_F , \alpha) \ $ and $ \, im(T) \, $ is a bounded vector subspace of $F$. Hence $ \ im(T) = \{ 0_F \} \ \Rightarrow \ T = 0_L \, $. Therefore we have that $ \ \mathcal{C} = \{ 0_L \}$, a contradiction. Is there anything wrong with this reasoning? Are my calculation steps correct? If it is right, then we cannot have $ \ \alpha \neq s \ $ and we are left with $ \ s= \alpha \, $. That is exactly the case which I was not able to handle. Like always, any hint is appreciated. Thanks in advance.",,"['functional-analysis', 'normed-spaces']"
93,"$c_0$, $l_1$, $l_\infty$ and duals",", ,  and duals",c_0 l_1 l_\infty,"Why $$ (l_1)^{**}=l_1 \oplus (l_\infty/c_0)^*   $$ Is there any general isomorphism between $X$,$X$** and $X$*** in  Banach spaces?","Why $$ (l_1)^{**}=l_1 \oplus (l_\infty/c_0)^*   $$ Is there any general isomorphism between $X$,$X$** and $X$*** in  Banach spaces?",,"['sequences-and-series', 'functional-analysis']"
94,"Distribution on $(0, \infty)$ is not a restriction of another distribution",Distribution on  is not a restriction of another distribution,"(0, \infty)","Given the linear form $$ \langle u, \phi\rangle = \sum_{k=1}^\infty \partial^k \phi(1/k) $$ which is a distribution on $(0, \infty)$, show that there is no $v \in \mathscr{D}(\mathbb{R})$ whose restriction to $(0, \infty)$ equals $u$. This question is similar to On the extension of distribution but I didn't understand that solution and I'm wondering if there is an argument that involves making a partition of unity. My attempt so far: For any test function $\phi \in C_0^\infty((0,  \infty))$, for some $N$, $\operatorname{supp} (\phi) \subset (\frac{1}{N}, \infty)$.  So, $$\left|\langle u, \phi\rangle\right| \leq \sum_{k=1}^{N+1} \left|\partial^k \phi(1/k)\right|.$$ Using that estimate it is not hard to show that $u \in \mathscr{D}((0, \infty))$.  Suppose $v$ is a distribution on $\mathbb{R}$ that restricts to $u$ on $(0, \infty)$.  Let $\rho$ be a test function identically equal to 1 on $[-1, 1]$ and support in a small neighborhood of that set. For all $n$, let $\mathbb{O_n} = \{(-2, \frac{1}{n-1}), (\frac{1}{n}, 2)\}$ and let $\psi_1$ and $\psi_2$ be the partition of unity of $supp(\rho e^x)$ with respect to $\mathbb{O_n}$.  Then  $$\langle v, \rho e^x\rangle  = \langle v, \psi_1 \rho e^x\rangle  +\langle v, \psi_2 \rho e^x\rangle  = \langle v, \psi_1 \rho e^x\rangle  + \sum_{k=1}^{n-1} e^{1/k}.$$ The last sum goes off to infinity as $n \to \infty$, but that doesn't finish the problem and I'm not sure what to do.","Given the linear form $$ \langle u, \phi\rangle = \sum_{k=1}^\infty \partial^k \phi(1/k) $$ which is a distribution on $(0, \infty)$, show that there is no $v \in \mathscr{D}(\mathbb{R})$ whose restriction to $(0, \infty)$ equals $u$. This question is similar to On the extension of distribution but I didn't understand that solution and I'm wondering if there is an argument that involves making a partition of unity. My attempt so far: For any test function $\phi \in C_0^\infty((0,  \infty))$, for some $N$, $\operatorname{supp} (\phi) \subset (\frac{1}{N}, \infty)$.  So, $$\left|\langle u, \phi\rangle\right| \leq \sum_{k=1}^{N+1} \left|\partial^k \phi(1/k)\right|.$$ Using that estimate it is not hard to show that $u \in \mathscr{D}((0, \infty))$.  Suppose $v$ is a distribution on $\mathbb{R}$ that restricts to $u$ on $(0, \infty)$.  Let $\rho$ be a test function identically equal to 1 on $[-1, 1]$ and support in a small neighborhood of that set. For all $n$, let $\mathbb{O_n} = \{(-2, \frac{1}{n-1}), (\frac{1}{n}, 2)\}$ and let $\psi_1$ and $\psi_2$ be the partition of unity of $supp(\rho e^x)$ with respect to $\mathbb{O_n}$.  Then  $$\langle v, \rho e^x\rangle  = \langle v, \psi_1 \rho e^x\rangle  +\langle v, \psi_2 \rho e^x\rangle  = \langle v, \psi_1 \rho e^x\rangle  + \sum_{k=1}^{n-1} e^{1/k}.$$ The last sum goes off to infinity as $n \to \infty$, but that doesn't finish the problem and I'm not sure what to do.",,"['functional-analysis', 'distribution-theory']"
95,Example of Holmested formula for real interpolation spaces [closed],Example of Holmested formula for real interpolation spaces [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Can anybody give me an example of Holmested formula for real interpolation spaces.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Can anybody give me an example of Holmested formula for real interpolation spaces.",,"['functional-analysis', 'interpolation-theory']"
96,Projection properties in Hilbert space,Projection properties in Hilbert space,,"Let $L$ and $M$ be closed subspaces of a Hilbert space $H$ and $P_L,P_M\in \mathcal{L}(H,H)$ are orthoprojections to $L$ and $M$. I want to prove that then $$P_M\geq P_L \Leftrightarrow L\subset M.$$ Partial order $P_M\geq P_L$ is defined as $$P_M\geq P_L \Leftrightarrow ((P_M-P_L)x,x)\geq 0 \ \ \ \forall x\in X.$$ Any ideas?","Let $L$ and $M$ be closed subspaces of a Hilbert space $H$ and $P_L,P_M\in \mathcal{L}(H,H)$ are orthoprojections to $L$ and $M$. I want to prove that then $$P_M\geq P_L \Leftrightarrow L\subset M.$$ Partial order $P_M\geq P_L$ is defined as $$P_M\geq P_L \Leftrightarrow ((P_M-P_L)x,x)\geq 0 \ \ \ \forall x\in X.$$ Any ideas?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces']"
97,Approximations of functionals,Approximations of functionals,,The distance between a point and a set in a metric space or in a normed space is used in approximation of functionals (i read it in functional analysis book) can any body explain this please as i am unaware of approximation of functions,The distance between a point and a set in a metric space or in a normed space is used in approximation of functionals (i read it in functional analysis book) can any body explain this please as i am unaware of approximation of functions,,['functional-analysis']
98,Importance (applications) of functionals,Importance (applications) of functionals,,Yes functional analysis is the study of functionals (up to some extend). Why we are so curious to study functionals? Some applications please for the motivations to me.,Yes functional analysis is the study of functionals (up to some extend). Why we are so curious to study functionals? Some applications please for the motivations to me.,,['functional-analysis']
99,tensorial product of C*-algebras and adjointness,tensorial product of C*-algebras and adjointness,,"Given $M,N$, $R$-modules, it is standard verification that $-\otimes Z$ is left adjoint to $Hom(Z,-)$. If we have more structure, particularly if $\mathcal{A}$ and $\mathcal{B}$ are C*-algebras, choosing any suitable norm in the purely algebraic tensor product (so that the product is still a C*-algebra), what more adjunctions arise from this construction ?","Given $M,N$, $R$-modules, it is standard verification that $-\otimes Z$ is left adjoint to $Hom(Z,-)$. If we have more structure, particularly if $\mathcal{A}$ and $\mathcal{B}$ are C*-algebras, choosing any suitable norm in the purely algebraic tensor product (so that the product is still a C*-algebra), what more adjunctions arise from this construction ?",,"['functional-analysis', 'category-theory', 'operator-algebras', 'c-star-algebras']"
