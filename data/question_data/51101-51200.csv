,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Writing sums as integrals,Writing sums as integrals,,"There are many proofs of the Basel problem (see this wonderful thread Different ways to prove $\sum_{k=1}^\infty \frac{1}{k^2}=\frac{\pi^2}{6}$ (the Basel problem) ), many of which require a step writing either $\log (1-x)$ or $1/(1-x)$ as their power series to go from the sum $\sum 1/n^2$ to an integral. Is there a way to go from the sum to an integral without using these power series representations, but rather writing the sum directly as an integral? By ""writing a sum as integral"" I mean calculus-first-course-style things like $$\lim _{N\rightarrow \infty }\frac {1}{N}\sum _{n=1}^N\frac {1}{1+n/N}=\int _0^1\frac {dx}{1+x}.$$","There are many proofs of the Basel problem (see this wonderful thread Different ways to prove $\sum_{k=1}^\infty \frac{1}{k^2}=\frac{\pi^2}{6}$ (the Basel problem) ), many of which require a step writing either or as their power series to go from the sum to an integral. Is there a way to go from the sum to an integral without using these power series representations, but rather writing the sum directly as an integral? By ""writing a sum as integral"" I mean calculus-first-course-style things like",\log (1-x) 1/(1-x) \sum 1/n^2 \lim _{N\rightarrow \infty }\frac {1}{N}\sum _{n=1}^N\frac {1}{1+n/N}=\int _0^1\frac {dx}{1+x}.,"['calculus', 'integration', 'sequences-and-series', 'definite-integrals', 'power-series']"
1,Integration by parts does not work for this complex integral. Why?,Integration by parts does not work for this complex integral. Why?,,"There is a longer integral for which integration by parts $\displaystyle\int udv=uv-\int vdu$ was attempted as it came across in research: $$ \frac i{2\pi}\int_0^{2\pi}\underbrace{\ln\left(1+\frac{e^{-i t}+1}a\ln\left(1-\frac1be^{\frac{e^{it}+1}a}\right)\right)}_u \;d\bigg[\underbrace{\ln\left(e^\frac{e^{it}+1}c-b\right)}_v\bigg] \tag{0} $$ as it has the below problem, but here is a simpler one to better get the point across $$ \frac1{2\pi}\int_0^{2\pi} \ln(e^{it}+a)\; d\left[\ln\left(e^{e^{it}}-2\right)\right] \tag{1} $$ Integrating $(1)$ by parts should give: $$\frac1{2\pi}\int_0^{2\pi}\ln(e^{it}+a) \;d\left[\ln\left(e^{e^{it}}-2\right)\right] \\  = \underbrace{\left[\ln(e^{it}+a)\ln\left(e^{e^{it}}-2\right)\middle]\right|_0^{2\pi}}_\text{should equal zero when plugging in directly}-\frac1{2\pi}\int_0^{2\pi}\ln\left(e^{e^{it}}-2\right)  \;d\left[\ln(e^{it}+a)\right]  \ . \tag2 $$ Directly substituting, we should get $\left[\ln(e^{it}+a)\ln\left(e^{e^{it}}-2\right)\middle]\right|_0^{2\pi} =0$ . Checking numerically shows $(2)$ to be false. However, instead, we numerically get $\ln(a-1)i$ instead of $0$ : $$ \frac1{2\pi}\int_0^{2\pi}\ln(e^{it}+a) \;d\left[\ln\left(e^{e^{it}}-2\right)\right] \\ =\ln(a-1)i -\frac1{2\pi}\int_0^{2\pi}\ln\left(e^{e^{it}}-2\right) d\left[\ln(e^{it}+a)\right] \tag{3} \ . $$ What is the correct way to find the extra $[uv]|_a^b$ term, like $\ln(a-1)i$ here?","There is a longer integral for which integration by parts was attempted as it came across in research: as it has the below problem, but here is a simpler one to better get the point across Integrating by parts should give: Directly substituting, we should get . Checking numerically shows to be false. However, instead, we numerically get instead of : What is the correct way to find the extra term, like here?","\displaystyle\int udv=uv-\int vdu 
\frac i{2\pi}\int_0^{2\pi}\underbrace{\ln\left(1+\frac{e^{-i t}+1}a\ln\left(1-\frac1be^{\frac{e^{it}+1}a}\right)\right)}_u
\;d\bigg[\underbrace{\ln\left(e^\frac{e^{it}+1}c-b\right)}_v\bigg] \tag{0}
 
\frac1{2\pi}\int_0^{2\pi}
\ln(e^{it}+a)\;
d\left[\ln\left(e^{e^{it}}-2\right)\right] \tag{1}
 (1) \frac1{2\pi}\int_0^{2\pi}\ln(e^{it}+a)
\;d\left[\ln\left(e^{e^{it}}-2\right)\right]
\\
 = \underbrace{\left[\ln(e^{it}+a)\ln\left(e^{e^{it}}-2\right)\middle]\right|_0^{2\pi}}_\text{should equal zero when plugging in directly}-\frac1{2\pi}\int_0^{2\pi}\ln\left(e^{e^{it}}-2\right) 
\;d\left[\ln(e^{it}+a)\right] 
\ .
\tag2
 \left[\ln(e^{it}+a)\ln\left(e^{e^{it}}-2\right)\middle]\right|_0^{2\pi} =0 (2) \ln(a-1)i 0 
\frac1{2\pi}\int_0^{2\pi}\ln(e^{it}+a)
\;d\left[\ln\left(e^{e^{it}}-2\right)\right]
\\
=\ln(a-1)i -\frac1{2\pi}\int_0^{2\pi}\ln\left(e^{e^{it}}-2\right) d\left[\ln(e^{it}+a)\right] \tag{3}
\ .
 [uv]|_a^b \ln(a-1)i","['integration', 'solution-verification', 'complex-numbers', 'complex-integration', 'constants']"
2,Evaluating $\int_0^{\pi} \frac{x}{1+\sin^2{x}}dx$,Evaluating,\int_0^{\pi} \frac{x}{1+\sin^2{x}}dx,"Evaluate: $$\int_0^{\pi} \frac{x}{1+\sin^2{x}}dx$$ My attempt: Let $I = \int_0^{\pi} \frac{x}{1+\sin^2{x}}dx$ Using Identity: $$\int_0^af(x)dx=\int_0^af(a-x)dx$$ I got: $$I=\int_0^{\pi} \frac{\pi -x}{1+\sin^2{(\pi -x})}dx$$ $$I=\frac{\pi}{2}\int_0^{\pi} \frac{dx}{1+\sin^2{x}}$$ Dividing numerator and denominator by $\cos^2x$ $$I=\frac{\pi}{2}\int_0^{\pi} \frac{\sec^2x}{\sec^2x+\tan^2x}dx$$ $$I=\frac{\pi}{2}\int_0^{\pi} \frac{\sec^2x}{1+2\tan^2x}dx$$ Using Substitution method, let: $$t =\sqrt2\tan x$$ $$\frac{dt}{\sqrt2} =\sec^2xdx$$ The limits also change upon substitution, so the integral I get is: $$I=\frac{\pi}{2\sqrt2}\int_0^{0} \frac{1}{1+t^2}dt$$ But the answer isn't $I(0)-I(0)=0$ . So where did my solution go wrong? I do know that I could have changed the upper limit of the integral to be $\frac{\pi}{2}$ , and somehow it all works out. But why doesn't the integral work in the way I did?","Evaluate: My attempt: Let Using Identity: I got: Dividing numerator and denominator by Using Substitution method, let: The limits also change upon substitution, so the integral I get is: But the answer isn't . So where did my solution go wrong? I do know that I could have changed the upper limit of the integral to be , and somehow it all works out. But why doesn't the integral work in the way I did?",\int_0^{\pi} \frac{x}{1+\sin^2{x}}dx I = \int_0^{\pi} \frac{x}{1+\sin^2{x}}dx \int_0^af(x)dx=\int_0^af(a-x)dx I=\int_0^{\pi} \frac{\pi -x}{1+\sin^2{(\pi -x})}dx I=\frac{\pi}{2}\int_0^{\pi} \frac{dx}{1+\sin^2{x}} \cos^2x I=\frac{\pi}{2}\int_0^{\pi} \frac{\sec^2x}{\sec^2x+\tan^2x}dx I=\frac{\pi}{2}\int_0^{\pi} \frac{\sec^2x}{1+2\tan^2x}dx t =\sqrt2\tan x \frac{dt}{\sqrt2} =\sec^2xdx I=\frac{\pi}{2\sqrt2}\int_0^{0} \frac{1}{1+t^2}dt I(0)-I(0)=0 \frac{\pi}{2},"['calculus', 'integration', 'trigonometry']"
3,"Evaluate $\int_{0}^{\pi/2}x\ln\left(\tan x\right)\,dx$ [duplicate]",Evaluate  [duplicate],"\int_{0}^{\pi/2}x\ln\left(\tan x\right)\,dx","This question already has answers here : How to evaluate $\int_{0}^{\pi }\theta \ln\tan\frac{\theta }{2} \, \mathrm{d}\theta$ (6 answers) Closed 6 months ago . Evaluate $$\int_{0}^{\pi/2}x\ln\left(\tan x\right)\,dx$$ First we will work out the complex integral of the function $$\displaystyle{f\left( z \right) = \frac{{{z^2}}}{{{e^z} - 1}},{\text{ }}z \ne 0} and \displaystyle{f\left( 0 \right) = 0}$$ in the rectangle $\displaystyle{c:{\text{ }}OABC}$ with $$\displaystyle{O\left( {0,0} \right){\text{, }}A\left( {R,0} \right){\text{, }}B\left( {R,i \pi } \right){\text{ \& }}D\left( {i\pi ,0} \right)}$$ Obviously f(z) is analytic without poles in the above rectangle, so $\displaystyle{\int\limits_c {f\left( z \right)dz}  = 0}$ For $$\displaystyle{z = R + iy{\text{  with  }}y \in \left[ {0,\pi } \right]:\mathop {\lim }\limits_{R \to \infty } \left| {f\left( z \right)} \right| = \mathop {\lim }\limits_{R \to \infty } \left| {\frac{{{{\left( {R + iy} \right)}^2}}}{{{e^{R + iy}} - 1}}} \right| \leqslant \mathop {\lim }\limits_{R \to \infty } \frac{{\left| {{{\left( {R + iy} \right)}^2}} \right|}}{{{e^R} - 1}}\xrightarrow{{R \to  + \infty }}0}$$ And $$\displaystyle{0 = \int\limits_c {f\left( z \right)dz}  = \int\limits_0^\infty  {\frac{{{x^2}}}{{{e^x} - 1}}dx}  + \int\limits_\infty ^0 {\frac{{{{\left( {x + i\pi } \right)}^2}}}{{{e^{x + i\pi }} - 1}}dx}  + i \cdot \int\limits_\pi ^0 {\frac{{{{\left( {iy} \right)}^2}}}{{{e^{iy}} - 1}}dy}  \Rightarrow }$$ $$\displaystyle{ \Rightarrow \int\limits_0^\infty  {\frac{{{x^2}}}{{{e^x} - 1}}dx}  + \int\limits_0^\infty  {\frac{{{x^2}}}{{{e^x} + 1}}dx}  - {\pi ^2}\int\limits_0^\infty  {\frac{1}{{{e^x} + 1}}dx}  + 2i\pi \int\limits_0^\infty  {\frac{x}{{{e^x} + 1}}dx}  - i \cdot \int\limits_\pi ^0 {\frac{{{y^2}}}{{\cos \left( y \right) - 1 + i\sin \left( y \right)}}dy}  = 0}$$ $$\displaystyle{\int\limits_0^\infty  {\frac{1}{{{e^x} + 1}}dx}  = \int\limits_0^\infty  {\frac{{{e^{ - x}}}}{{1 + {e^{ - x}}}}dx}  =  - \left[ {\ln \left( {1 + {e^{ - x}}} \right)} \right]_0^\infty  = \ln \left( 2 \right)}$$ $$\displaystyle{\int\limits_0^\infty  {\frac{x}{{{e^x} + 1}}dx}  = \int\limits_0^\infty  {\frac{{x{e^{ - x}}}}{{1 + {e^{ - x}}}}dx}  = \int\limits_0^\infty  {x\left( {\sum\limits_{k = 1}^\infty  {{{\left( { - 1} \right)}^{k + 1}}{e^{ - kx}}} } \right)dx}  = \sum\limits_1^\infty  {{{\left( { - 1} \right)}^{k + 1}}\int\limits_0^\infty  {x{e^{ - kx}}dx} }  = \sum\limits_1^\infty  {\frac{{{{\left( { - 1} \right)}^{k + 1}}}}{{{k^2}}}}  = .. = \frac{{{\pi ^2}}}{{12}}}$$ $$\displaystyle{\int\limits_\pi ^0 {\frac{{{y^2}}}{{\cos \left( y \right) - 1 + i\sin \left( y \right)}}dy}  = \int\limits_0^\pi  {\frac{{{y^2}}}{{2{{\sin }^2}\left( {\dfrac{y}{2}} \right) - 2i \cdot \sin \left( {\dfrac{y}{2}} \right)\cos \left( {\dfrac{y}{2}} \right)}}dy}  = \int\limits_0^\pi  {\frac{{{y^2}}}{{ - 2i\sin \left( {\dfrac{y}{2}} \right)\left( {\cos \left( {\dfrac{y}{2}} \right) + i\sin \left( {\dfrac{y}{2}} \right)} \right)}}dy}  = }$$ $$\displaystyle{ = \frac{i}{2} \cdot \int\limits_0^\pi  {\frac{{{y^2}\left( {\cos \left( {\dfrac{y}{2}} \right) - i\sin \left( {\dfrac{y}{2}} \right)} \right)}}{{\sin \left( {\dfrac{y}{2}} \right)}}dy}  = \frac{i}{2} \cdot \int\limits_0^\pi  {\frac{{{y^2}}}{{\tan \left( {\dfrac{y}{2}} \right)}}dy}  + \frac{1}{2} \cdot \int\limits_0^\pi  {{y^2}dy}  = \mathop  = \limits^{y/2 = x}  = 4i \cdot \int\limits_0^{\pi /2} {\frac{{{x^2}}}{{\tan \left( x \right)}}dx}  + \frac{{{\pi ^3}}}{6}}$$","This question already has answers here : How to evaluate $\int_{0}^{\pi }\theta \ln\tan\frac{\theta }{2} \, \mathrm{d}\theta$ (6 answers) Closed 6 months ago . Evaluate First we will work out the complex integral of the function in the rectangle with Obviously f(z) is analytic without poles in the above rectangle, so For And","\int_{0}^{\pi/2}x\ln\left(\tan x\right)\,dx \displaystyle{f\left( z \right) = \frac{{{z^2}}}{{{e^z} - 1}},{\text{ }}z \ne 0} and \displaystyle{f\left( 0 \right) = 0} \displaystyle{c:{\text{ }}OABC} \displaystyle{O\left( {0,0} \right){\text{, }}A\left( {R,0} \right){\text{, }}B\left( {R,i \pi } \right){\text{ \& }}D\left( {i\pi ,0} \right)} \displaystyle{\int\limits_c {f\left( z \right)dz}  = 0} \displaystyle{z = R + iy{\text{  with  }}y \in \left[ {0,\pi } \right]:\mathop {\lim }\limits_{R \to \infty } \left| {f\left( z \right)} \right| = \mathop {\lim }\limits_{R \to \infty } \left| {\frac{{{{\left( {R + iy} \right)}^2}}}{{{e^{R + iy}} - 1}}} \right| \leqslant \mathop {\lim }\limits_{R \to \infty } \frac{{\left| {{{\left( {R + iy} \right)}^2}} \right|}}{{{e^R} - 1}}\xrightarrow{{R \to  + \infty }}0} \displaystyle{0 = \int\limits_c {f\left( z \right)dz}  = \int\limits_0^\infty  {\frac{{{x^2}}}{{{e^x} - 1}}dx}  + \int\limits_\infty ^0 {\frac{{{{\left( {x + i\pi } \right)}^2}}}{{{e^{x + i\pi }} - 1}}dx}  + i \cdot \int\limits_\pi ^0 {\frac{{{{\left( {iy} \right)}^2}}}{{{e^{iy}} - 1}}dy}  \Rightarrow } \displaystyle{ \Rightarrow \int\limits_0^\infty  {\frac{{{x^2}}}{{{e^x} - 1}}dx}  + \int\limits_0^\infty  {\frac{{{x^2}}}{{{e^x} + 1}}dx}  - {\pi ^2}\int\limits_0^\infty  {\frac{1}{{{e^x} + 1}}dx}  + 2i\pi \int\limits_0^\infty  {\frac{x}{{{e^x} + 1}}dx}  - i \cdot \int\limits_\pi ^0 {\frac{{{y^2}}}{{\cos \left( y \right) - 1 + i\sin \left( y \right)}}dy}  = 0} \displaystyle{\int\limits_0^\infty  {\frac{1}{{{e^x} + 1}}dx}  = \int\limits_0^\infty  {\frac{{{e^{ - x}}}}{{1 + {e^{ - x}}}}dx}  =  - \left[ {\ln \left( {1 + {e^{ - x}}} \right)} \right]_0^\infty  = \ln \left( 2 \right)} \displaystyle{\int\limits_0^\infty  {\frac{x}{{{e^x} + 1}}dx}  = \int\limits_0^\infty  {\frac{{x{e^{ - x}}}}{{1 + {e^{ - x}}}}dx}  = \int\limits_0^\infty  {x\left( {\sum\limits_{k = 1}^\infty  {{{\left( { - 1} \right)}^{k + 1}}{e^{ - kx}}} } \right)dx}  = \sum\limits_1^\infty  {{{\left( { - 1} \right)}^{k + 1}}\int\limits_0^\infty  {x{e^{ - kx}}dx} }  = \sum\limits_1^\infty  {\frac{{{{\left( { - 1} \right)}^{k + 1}}}}{{{k^2}}}}  = .. = \frac{{{\pi ^2}}}{{12}}} \displaystyle{\int\limits_\pi ^0 {\frac{{{y^2}}}{{\cos \left( y \right) - 1 + i\sin \left( y \right)}}dy}  = \int\limits_0^\pi  {\frac{{{y^2}}}{{2{{\sin }^2}\left( {\dfrac{y}{2}} \right) - 2i \cdot \sin \left( {\dfrac{y}{2}} \right)\cos \left( {\dfrac{y}{2}} \right)}}dy}  = \int\limits_0^\pi  {\frac{{{y^2}}}{{ - 2i\sin \left( {\dfrac{y}{2}} \right)\left( {\cos \left( {\dfrac{y}{2}} \right) + i\sin \left( {\dfrac{y}{2}} \right)} \right)}}dy}  = } \displaystyle{ = \frac{i}{2} \cdot \int\limits_0^\pi  {\frac{{{y^2}\left( {\cos \left( {\dfrac{y}{2}} \right) - i\sin \left( {\dfrac{y}{2}} \right)} \right)}}{{\sin \left( {\dfrac{y}{2}} \right)}}dy}  = \frac{i}{2} \cdot \int\limits_0^\pi  {\frac{{{y^2}}}{{\tan \left( {\dfrac{y}{2}} \right)}}dy}  + \frac{1}{2} \cdot \int\limits_0^\pi  {{y^2}dy}  = \mathop  = \limits^{y/2 = x}  = 4i \cdot \int\limits_0^{\pi /2} {\frac{{{x^2}}}{{\tan \left( x \right)}}dx}  + \frac{{{\pi ^3}}}{6}}","['calculus', 'integration']"
4,"I know that $ \int_0^\infty \frac{|f(x)|^2}{x} < \infty $, which condition should I have to prove $ \int_0^\infty \frac{|f(x)|^2}{x^2} < \infty $?","I know that , which condition should I have to prove ?", \int_0^\infty \frac{|f(x)|^2}{x} < \infty   \int_0^\infty \frac{|f(x)|^2}{x^2} < \infty ,"I'm working with the wavelet transform, and I'm facing a problem proving that $$ \int_0^\infty \frac{|f(x)|^2}{x^2}\mathrm{d}x $$ converges. The only thing I know is that $f(x)$ is the Fourier transform of some analytical mother wavelet which indicates that $$ \int_0^\infty \frac{|f(x)|^2}{x}\mathrm{d}x  < \infty $$ . I have no idea about what conditions should be added to prove that and the ideas to prove that. Thanks for helping.","I'm working with the wavelet transform, and I'm facing a problem proving that converges. The only thing I know is that is the Fourier transform of some analytical mother wavelet which indicates that . I have no idea about what conditions should be added to prove that and the ideas to prove that. Thanks for helping.", \int_0^\infty \frac{|f(x)|^2}{x^2}\mathrm{d}x  f(x)  \int_0^\infty \frac{|f(x)|^2}{x}\mathrm{d}x  < \infty ,"['integration', 'wavelets']"
5,General formula for the definite integral of form $\int \frac{dx}{\tan(ax)^n}$,General formula for the definite integral of form,\int \frac{dx}{\tan(ax)^n},"I am not entirely sure where I went wrong with my approach so I wanted to share it with you guys, thanks for any insights or for spotting some silly mistake. $$\int \frac{dx}{\tan(ax)^n}$$ First, I made a substitution as such: $u = \tan(ax)$ thus obtaining $$a\sec(ax)^2dx = du$$ rewriting it gives us $$a(\tan(ax)^2+1)dx = du$$ Then I rewrote the integral with the substitution as follows: $$\frac{1}{a}\int \frac{du}{u^n(u^2 + 1)}$$ Then I integrate as follows  (following step is wrong, this should be done with partial decomposition but is it possible?): $$\frac{1}{a}\int \frac{du}{u^n(u^2 + 1)} = \frac{1}{a}\ln \left| u^{n+2} + u^n \right| \cdot \frac{1}{(n+1)u^{n+1} + (n-1)u^{n-1}} + C$$ Then substituting back results in: $$\frac{1}{a}\ln \left| u^{n+2} + u^n \right| \cdot \frac{1}{(n+1)u^{n+1}} + (n-1)u^{n-1} + C = \frac{1}{a}\ln \left| \tan(ax)^{n+2} + \tan(ax)^n \right| \cdot \frac{1}{(n+1)\tan(ax)^{n+1} + (n-1)\tan(ax)^{n-1}} + C$$ Testing my answer with $\cot(3x)^4$ I get a very similar graph to the answer in the book but it is slightly off I can't seem to find the mistake. Thanks for any insights. It seems that my mistake is the integration step as I do not get a log integral, it is wrong. I should consider partial decomposition, but then is it possible to generalise it? $\textbf{Update:}$ so far I have only realised that if we play around with different powers $n$ we notice that if there is a general expression, it is different for odd and even powers; I conjecture that we indeed can find it, but two separate expressions.","I am not entirely sure where I went wrong with my approach so I wanted to share it with you guys, thanks for any insights or for spotting some silly mistake. First, I made a substitution as such: thus obtaining rewriting it gives us Then I rewrote the integral with the substitution as follows: Then I integrate as follows  (following step is wrong, this should be done with partial decomposition but is it possible?): Then substituting back results in: Testing my answer with I get a very similar graph to the answer in the book but it is slightly off I can't seem to find the mistake. Thanks for any insights. It seems that my mistake is the integration step as I do not get a log integral, it is wrong. I should consider partial decomposition, but then is it possible to generalise it? so far I have only realised that if we play around with different powers we notice that if there is a general expression, it is different for odd and even powers; I conjecture that we indeed can find it, but two separate expressions.",\int \frac{dx}{\tan(ax)^n} u = \tan(ax) a\sec(ax)^2dx = du a(\tan(ax)^2+1)dx = du \frac{1}{a}\int \frac{du}{u^n(u^2 + 1)} \frac{1}{a}\int \frac{du}{u^n(u^2 + 1)} = \frac{1}{a}\ln \left| u^{n+2} + u^n \right| \cdot \frac{1}{(n+1)u^{n+1} + (n-1)u^{n-1}} + C \frac{1}{a}\ln \left| u^{n+2} + u^n \right| \cdot \frac{1}{(n+1)u^{n+1}} + (n-1)u^{n-1} + C = \frac{1}{a}\ln \left| \tan(ax)^{n+2} + \tan(ax)^n \right| \cdot \frac{1}{(n+1)\tan(ax)^{n+1} + (n-1)\tan(ax)^{n-1}} + C \cot(3x)^4 \textbf{Update:} n,"['real-analysis', 'calculus', 'integration', 'analysis', 'indefinite-integrals']"
6,What is the Expected Time of Crystallization?,What is the Expected Time of Crystallization?,,"Consider the process of periodic 1D crystallization, where multiple sites initiate crystallizing waves at random with speed $v$ . The fraction of the crystallized substance, at position $x$ and time $t$ , is given by $$ f(x,t)=1-e^{-\int_{V_X(v)}p(Y) \,dY}  $$ where $p(x,t)$ is the crystallization initiation rate at position $x$ and time $t$ , and $V_X(v)$ is the past light-cone of the spacetime point $X=(x,t)$ (see figure below, where $x_\pm = x\mp vt$ ). My goal is to find an expression for $p$ in terms of the expected time of crystallization at each space point, which can be given by $$ t_E(x)=\int_0^\infty t \frac{\partial f(x,t')}{\partial t'}|_{t'=t} \,dt $$ Some ideas: From this paper (in the context of DNA replication, analogous to the process of 1D crystallization, see also this publication and chapter II of this thesis ), defining the fraction of uncrystallized substance $s(x,t)=1-f(x,t)$ , $p$ is shown to satisfy $$ p(x,t)=-\frac{v}{2}\square \log(s(x,t)), $$ where $\square =\frac{1}{v^2}\partial_t^2-\partial_x^2$ is the d'Alembert operator . Could I use this to write it in terms of the expected time of crystallization at each space point? I was thinking that, given the shape of $f$ , perhaps a Laplace Transform could be used to rewrite the fraction $s$ in terms of the expected time at each space point, which would be our data. A simple example: When $p(x,t)=p$ is constant in spacetime, we have $f(x,t)=1-e^{-pvt^2} $ and thus $$ t_E(x)=2pv\int_0^\infty t^2 e^{-pv t^2}\,dt=\frac12\sqrt{\frac{\pi}{pv}} $$ which can easily be inverted. Any ideas for the general case?","Consider the process of periodic 1D crystallization, where multiple sites initiate crystallizing waves at random with speed . The fraction of the crystallized substance, at position and time , is given by where is the crystallization initiation rate at position and time , and is the past light-cone of the spacetime point (see figure below, where ). My goal is to find an expression for in terms of the expected time of crystallization at each space point, which can be given by Some ideas: From this paper (in the context of DNA replication, analogous to the process of 1D crystallization, see also this publication and chapter II of this thesis ), defining the fraction of uncrystallized substance , is shown to satisfy where is the d'Alembert operator . Could I use this to write it in terms of the expected time of crystallization at each space point? I was thinking that, given the shape of , perhaps a Laplace Transform could be used to rewrite the fraction in terms of the expected time at each space point, which would be our data. A simple example: When is constant in spacetime, we have and thus which can easily be inverted. Any ideas for the general case?","v x t 
f(x,t)=1-e^{-\int_{V_X(v)}p(Y) \,dY} 
 p(x,t) x t V_X(v) X=(x,t) x_\pm = x\mp vt p 
t_E(x)=\int_0^\infty t \frac{\partial f(x,t')}{\partial t'}|_{t'=t} \,dt
 s(x,t)=1-f(x,t) p 
p(x,t)=-\frac{v}{2}\square \log(s(x,t)),
 \square =\frac{1}{v^2}\partial_t^2-\partial_x^2 f s p(x,t)=p f(x,t)=1-e^{-pvt^2}  
t_E(x)=2pv\int_0^\infty t^2 e^{-pv t^2}\,dt=\frac12\sqrt{\frac{\pi}{pv}}
","['integration', 'analysis', 'expected-value', 'physics', 'laplace-transform']"
7,An interesting integral $\int_0^{\infty}\{e^x\}-\frac {1}{2}dx$,An interesting integral,\int_0^{\infty}\{e^x\}-\frac {1}{2}dx,"So I recently thought of this random integral, as I was thinking of integrals that have nice cancellation properties involving the fractional part. I believe I have an answer but my method is rather non-rigorous as I do not prove convergence, nor even know if it converges. Here is what I did: Let $$J=\int_0^{\infty}\{e^x\}-\frac {1}{2}dx$$ Expressing it in terms of the floor function and noticing that for $x\in(\log(n),\log(n+1))$ , you have $\lfloor e^x \rfloor=n$ yields $J=\sum_{n=1}^\infty\int_{\log(n)}^{\log(n+1)}(e^x-\lfloor e^x\rfloor-\frac {1}{2} )  dx= \sum_{n=1}^\infty\int_{\log(n)}^{\log(n+1)}(e^x-n-\frac {1}{2} )  dx \\= \sum_{n=1}^\infty (1+(n+\frac{1}{2})\log(\frac{n}{n+1}))$ This can be expressed as $$J= \lim_{N\to\infty}\sum_{n=1}^N((n-\frac{1}{2})\log(n)-(n+\frac{1}{2})\log(n+1)+\log(n)+1)$$ The first two terms are telescoping which gives $J=\lim_{N\to\infty}(\log(N!)+N-(N+\frac{1}{2})\log(N+1))$ $ =\lim_{N\to\infty}(N\log(N)+\frac{1}{2}\log(2\pi N)-(N+\frac{1}{2})\log(N+1))$ (using Stirling's formula) $ = \lim_{N\to\infty}((N+\frac{1}{2})\log(N)-(N+\frac{1}{2})\log(N+1))+\frac{1}{2}\log(2 \pi)=\frac{1}{2}\log(2 \pi)+\log(\frac{1}{e})=\frac{1}{2}\log(2 \pi)-1.$ It would be much appreciated if anyone could rectify any mistakes I made, make this rigorous and/or correct me on why the integral diverges.","So I recently thought of this random integral, as I was thinking of integrals that have nice cancellation properties involving the fractional part. I believe I have an answer but my method is rather non-rigorous as I do not prove convergence, nor even know if it converges. Here is what I did: Let Expressing it in terms of the floor function and noticing that for , you have yields This can be expressed as The first two terms are telescoping which gives (using Stirling's formula) It would be much appreciated if anyone could rectify any mistakes I made, make this rigorous and/or correct me on why the integral diverges.","J=\int_0^{\infty}\{e^x\}-\frac {1}{2}dx x\in(\log(n),\log(n+1)) \lfloor e^x \rfloor=n J=\sum_{n=1}^\infty\int_{\log(n)}^{\log(n+1)}(e^x-\lfloor e^x\rfloor-\frac {1}{2} )  dx=
\sum_{n=1}^\infty\int_{\log(n)}^{\log(n+1)}(e^x-n-\frac {1}{2} )  dx
\\= \sum_{n=1}^\infty (1+(n+\frac{1}{2})\log(\frac{n}{n+1})) J= \lim_{N\to\infty}\sum_{n=1}^N((n-\frac{1}{2})\log(n)-(n+\frac{1}{2})\log(n+1)+\log(n)+1) J=\lim_{N\to\infty}(\log(N!)+N-(N+\frac{1}{2})\log(N+1))  =\lim_{N\to\infty}(N\log(N)+\frac{1}{2}\log(2\pi N)-(N+\frac{1}{2})\log(N+1))  = \lim_{N\to\infty}((N+\frac{1}{2})\log(N)-(N+\frac{1}{2})\log(N+1))+\frac{1}{2}\log(2 \pi)=\frac{1}{2}\log(2 \pi)+\log(\frac{1}{e})=\frac{1}{2}\log(2 \pi)-1.","['calculus', 'integration', 'sequences-and-series', 'definite-integrals', 'ceiling-and-floor-functions']"
8,Generalization of the result of $\int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x$.,Generalization of the result of .,\int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x,"When I came across the integral $$\int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x,$$ I didn’t know how to deal with it. After struggling, I thought of Feynman’s trick and Euler formula and tried by letting $$I(a)=\int_0^{\infty} \frac{e^{-x^2} \sin \left(a x^2\right)}{x^2} d x$$ with $I(0)=0.$ Differentiating $I(a)$ w.r.t. $a$ yields $$ I^{\prime}(a)=\int_0^{\infty} e^{-x^2} \cos \left(a x^2\right) d x $$ Inevitably, I used the Euler formula $e^{i x}=\cos x+i \sin x$ to group the functions in terms of exponential function. $$ \begin{aligned} I^{\prime}(a) & =\int_0^{\infty} e^{-x^2} \cos \left(a x^2\right) d x \\ & =\operatorname{Re} \int_0^{\infty} e^{-x^2} e^{i a x^2} d x \\ & =\operatorname{Re} \int_0^{\infty} e^{-(1-i a) x^2} d x \end{aligned} $$ which is a Gaussian integral : $\int_{-\infty}^{\infty} e^{-a(x+b)^2} d x=\sqrt{\frac{\pi}{a}}$ for any $Re(a)>0$ . $$ \begin{aligned} I’(a) &=\operatorname{Re}\left(\frac{\sqrt{\pi}}{2 \sqrt{1-i a}}\right)\\ I(a)&=\frac{\sqrt{\pi}}{2} \operatorname{Re}\left[\frac{(1-i a)^{\frac{1}{2}}}{-\frac{1}{2} i}\right]\\ \therefore \int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x&= \sqrt{\pi}\operatorname{Re}\left[i \sqrt{\sqrt{2} e^{-\frac{\pi i}{4}}}\right]\\&   = \sqrt{\sqrt{2} \pi} \operatorname{Re}\left[i\left(\cos \frac{\pi}{8}-i \sin \frac{\pi}{8}\right)\right]\\&= \sqrt{\sqrt{2} \pi} \sin \frac{\pi}{8} \end{aligned} $$ As a bonus, $$\int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x = \sqrt{\sqrt{2} \pi} \cos \frac{\pi}{8} $$ My Question :Can we generalise the method to the integral $$ I_n=\int_0^{\infty} \frac{e^{-x^n} \sin \left(x^n\right)}{x^n} d x? $$ Fortunately, the answer is positive and decent as: $$ \int_0^{\infty} \frac{e^{-x^n} \sin \left(x^n\right)}{x^n} d x= \frac{2^{\frac{n-1}{2 n}}}{n-1} \Gamma\left(\frac{1}{n}\right) \sin \left(\frac{\pi(n-1)}{4 n}\right) $$ where $n$ is any real number greater than $1$ . Proof: Replacing the number 2 in the original integral by $n$ gives $$ I_n^{\prime}(a)=\operatorname{Re} \int_0^{\infty} e^{-(1-i a) x^n} d x $$ Letting $(1-i a) x^n \mapsto x$ transforms the derivative into $$ \begin{aligned} I_n ^{\prime}(a) & =\operatorname{Re}\left[\frac{1}{n(1-i a)^{\frac{1}{n}}} \int_0^{\infty} x^{\frac{1}{n}-1} e^{-x} d x\right] \\ & =\frac{1}{n} \operatorname{Re}\left[\frac{1}{(1-i a)^{\frac{1}{2}}}\Gamma\left(\frac{1}{n}\right)\right] \\ & =\frac{1}{n} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[\frac{1}{(1-i a)^{\frac{1}{n}}}\right] \end{aligned} $$ Integrating back yields $$ \begin{aligned} I_n(1)-I_n(0) & =\frac{1}{n} \Gamma\left(\frac{1}{n}\right) \operatorname{Re} \int_0^1(1-i a)^{-\frac{1}{n}} d a \\ I(1)& =\frac{1}{n} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[\frac{(1-i a)^{-\frac{1}{n}+1}}{-i\left(\frac{1}{n}+1\right)}\right]_0^1 \\ & =\frac{1}{n-1} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left(i(1-i)^{\frac{n-1}{n}}\right) \end{aligned} $$ Now we can conclude that $$ \begin{aligned} I& =\frac{1}{n-1} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[i\left(\sqrt{2} e^{-\frac{\pi}{4}}\right)^{\frac{n-1}{n}}\right] \\ & =\frac{1}{n-1} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[i 2^{\frac{n-1}{2 n}} e^{-\frac{\pi(n-1)}{4 n}}\right] \\ & =\frac{2^{\frac{n-1}{2 n}}}{n-1} \Gamma\left(\frac{1}{n}\right) \sin \left(\frac{\pi(n-1)}{4 n}\right) \end{aligned} $$ Any comments and alternative methods are highly appreciated.","When I came across the integral I didn’t know how to deal with it. After struggling, I thought of Feynman’s trick and Euler formula and tried by letting with Differentiating w.r.t. yields Inevitably, I used the Euler formula to group the functions in terms of exponential function. which is a Gaussian integral : for any . As a bonus, My Question :Can we generalise the method to the integral Fortunately, the answer is positive and decent as: where is any real number greater than . Proof: Replacing the number 2 in the original integral by gives Letting transforms the derivative into Integrating back yields Now we can conclude that Any comments and alternative methods are highly appreciated.","\int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x, I(a)=\int_0^{\infty} \frac{e^{-x^2} \sin \left(a x^2\right)}{x^2} d x I(0)=0. I(a) a 
I^{\prime}(a)=\int_0^{\infty} e^{-x^2} \cos \left(a x^2\right) d x
 e^{i x}=\cos x+i \sin x 
\begin{aligned}
I^{\prime}(a) & =\int_0^{\infty} e^{-x^2} \cos \left(a x^2\right) d x \\
& =\operatorname{Re} \int_0^{\infty} e^{-x^2} e^{i a x^2} d x \\
& =\operatorname{Re} \int_0^{\infty} e^{-(1-i a) x^2} d x
\end{aligned}
 \int_{-\infty}^{\infty} e^{-a(x+b)^2} d x=\sqrt{\frac{\pi}{a}} Re(a)>0 
\begin{aligned}
I’(a) &=\operatorname{Re}\left(\frac{\sqrt{\pi}}{2 \sqrt{1-i a}}\right)\\ I(a)&=\frac{\sqrt{\pi}}{2} \operatorname{Re}\left[\frac{(1-i a)^{\frac{1}{2}}}{-\frac{1}{2} i}\right]\\ \therefore \int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x&= \sqrt{\pi}\operatorname{Re}\left[i \sqrt{\sqrt{2} e^{-\frac{\pi i}{4}}}\right]\\&   = \sqrt{\sqrt{2} \pi} \operatorname{Re}\left[i\left(\cos \frac{\pi}{8}-i \sin \frac{\pi}{8}\right)\right]\\&= \sqrt{\sqrt{2} \pi} \sin \frac{\pi}{8}
\end{aligned}
 \int_0^{\infty} \frac{e^{-x^2} \sin \left(x^2\right)}{x^2} d x = \sqrt{\sqrt{2} \pi} \cos \frac{\pi}{8}  
I_n=\int_0^{\infty} \frac{e^{-x^n} \sin \left(x^n\right)}{x^n} d x?
 
\int_0^{\infty} \frac{e^{-x^n} \sin \left(x^n\right)}{x^n} d x= \frac{2^{\frac{n-1}{2 n}}}{n-1} \Gamma\left(\frac{1}{n}\right) \sin \left(\frac{\pi(n-1)}{4 n}\right)
 n 1 n 
I_n^{\prime}(a)=\operatorname{Re} \int_0^{\infty} e^{-(1-i a) x^n} d x
 (1-i a) x^n \mapsto x 
\begin{aligned}
I_n ^{\prime}(a) & =\operatorname{Re}\left[\frac{1}{n(1-i a)^{\frac{1}{n}}} \int_0^{\infty} x^{\frac{1}{n}-1} e^{-x} d x\right] \\
& =\frac{1}{n} \operatorname{Re}\left[\frac{1}{(1-i a)^{\frac{1}{2}}}\Gamma\left(\frac{1}{n}\right)\right] \\
& =\frac{1}{n} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[\frac{1}{(1-i a)^{\frac{1}{n}}}\right]
\end{aligned}
 
\begin{aligned}
I_n(1)-I_n(0) & =\frac{1}{n} \Gamma\left(\frac{1}{n}\right) \operatorname{Re} \int_0^1(1-i a)^{-\frac{1}{n}} d a \\
I(1)& =\frac{1}{n} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[\frac{(1-i a)^{-\frac{1}{n}+1}}{-i\left(\frac{1}{n}+1\right)}\right]_0^1 \\
& =\frac{1}{n-1} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left(i(1-i)^{\frac{n-1}{n}}\right)
\end{aligned}
 
\begin{aligned}
I& =\frac{1}{n-1} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[i\left(\sqrt{2} e^{-\frac{\pi}{4}}\right)^{\frac{n-1}{n}}\right] \\
& =\frac{1}{n-1} \Gamma\left(\frac{1}{n}\right) \operatorname{Re}\left[i 2^{\frac{n-1}{2 n}} e^{-\frac{\pi(n-1)}{4 n}}\right] \\
& =\frac{2^{\frac{n-1}{2 n}}}{n-1} \Gamma\left(\frac{1}{n}\right) \sin \left(\frac{\pi(n-1)}{4 n}\right)
\end{aligned}
","['calculus', 'integration', 'definite-integrals', 'trigonometric-integrals', 'gaussian-integral']"
9,Proving an integral identity involving binomial coefficients,Proving an integral identity involving binomial coefficients,,"I'm having a lot of trouble proving or disproving the following statement: $$\int_0^1(1-x^{\alpha})^n\ dx=\frac{n!\alpha^n}{\prod_{k=1}^n(\alpha k+1)}$$ $\alpha\in\mathbb{R}^{+*}$ and $n\in\mathbb{N}^*$ . I've tried proving it via the binomial coefficients and an induction, but this has lead to nothing: $$\int_0^1(1-x^{\alpha})^n\ dx=\int_0^1 \sum_{k=0}^n\begin{pmatrix} n\\ k \end{pmatrix}(-x^\alpha)^k\ dx=\sum_{k=0}^n\begin{pmatrix} n\\ k \end{pmatrix}(-1)^k\int_0^1x^{\alpha k}\ dx$$ $$=\sum_{k=0}^n\begin{pmatrix} n\\ k \end{pmatrix}(-1)^k(\alpha k+1)^{-1}$$ The induction blocks beccause of the fact that: $$\begin{pmatrix} n\\ k \end{pmatrix}=\frac{n!}{k!(n-k)!}$$ Making it impossible to factor out $(n-k+1)$ from the denominator in the hereditary step of the proof because this term isn't constant. Attacking the problem via series has also proven unfruitful on my end; the series can't be evaluated at $x=1$ or $x=0$ , and evaluating it at other points leads to a disgusting series. I'm just hoping someone can give me some indicators on what to do! This is a purely recereational question and I thought it would be interesting to tie this conjecture to integrals of this form as I believe there was a similar method for deriving the Wallis product for $\pi$ via integrals like this. For the reference to this integral and the wallis product for $\pi$ , see the ""Historical motivation"" paragraph of this article: https://mindyourdecisions.com/blog/2016/10/12/the-wallis-product-formula-for-pi-and-its-proof/","I'm having a lot of trouble proving or disproving the following statement: and . I've tried proving it via the binomial coefficients and an induction, but this has lead to nothing: The induction blocks beccause of the fact that: Making it impossible to factor out from the denominator in the hereditary step of the proof because this term isn't constant. Attacking the problem via series has also proven unfruitful on my end; the series can't be evaluated at or , and evaluating it at other points leads to a disgusting series. I'm just hoping someone can give me some indicators on what to do! This is a purely recereational question and I thought it would be interesting to tie this conjecture to integrals of this form as I believe there was a similar method for deriving the Wallis product for via integrals like this. For the reference to this integral and the wallis product for , see the ""Historical motivation"" paragraph of this article: https://mindyourdecisions.com/blog/2016/10/12/the-wallis-product-formula-for-pi-and-its-proof/","\int_0^1(1-x^{\alpha})^n\ dx=\frac{n!\alpha^n}{\prod_{k=1}^n(\alpha k+1)} \alpha\in\mathbb{R}^{+*} n\in\mathbb{N}^* \int_0^1(1-x^{\alpha})^n\ dx=\int_0^1 \sum_{k=0}^n\begin{pmatrix}
n\\
k
\end{pmatrix}(-x^\alpha)^k\ dx=\sum_{k=0}^n\begin{pmatrix}
n\\
k
\end{pmatrix}(-1)^k\int_0^1x^{\alpha k}\ dx =\sum_{k=0}^n\begin{pmatrix}
n\\
k
\end{pmatrix}(-1)^k(\alpha k+1)^{-1} \begin{pmatrix}
n\\
k
\end{pmatrix}=\frac{n!}{k!(n-k)!} (n-k+1) x=1 x=0 \pi \pi","['calculus', 'integration']"
10,Criterion for local integrability of $1/f$ for $f$ a smooth function from $\mathbb R^2$ to $\mathbb C$,Criterion for local integrability of  for  a smooth function from  to,1/f f \mathbb R^2 \mathbb C,"Suppose I have a function $f=u+iv$ , $f:\mathbb{R}^2\to \mathbb{C}$ which is smooth, in the sense that $u$ and $v$ are smooth. I am wondering what some criteria are for $$ \int_{B_r(x_0,y_0)}\frac1 f\mathrm dA(x,y) $$ to converge in some suitable sense, where $\mathrm dA(x,y)$ is the area measure. For example, if $f(x_0,y_0)=0$ but $f$ has no other zeros in the ball, is it enough to have $\nabla f(x_0,y_0)\ne 0$ ? Presumably, what I am asking is equivalent to asking if $$ \int_{B_r(x_0,y_0)}\frac {u} {u^2+v^2}\mathrm dA(x,y) $$ and $$ \int_{B_r(x_0,y_0)}\frac {v} {u^2+v^2}\mathrm dA(x,y) $$ converge separately.","Suppose I have a function , which is smooth, in the sense that and are smooth. I am wondering what some criteria are for to converge in some suitable sense, where is the area measure. For example, if but has no other zeros in the ball, is it enough to have ? Presumably, what I am asking is equivalent to asking if and converge separately.","f=u+iv f:\mathbb{R}^2\to \mathbb{C} u v 
\int_{B_r(x_0,y_0)}\frac1 f\mathrm dA(x,y)
 \mathrm dA(x,y) f(x_0,y_0)=0 f \nabla f(x_0,y_0)\ne 0 
\int_{B_r(x_0,y_0)}\frac {u} {u^2+v^2}\mathrm dA(x,y)
 
\int_{B_r(x_0,y_0)}\frac {v} {u^2+v^2}\mathrm dA(x,y)
","['real-analysis', 'integration', 'complex-analysis', 'measure-theory', 'multivariable-calculus']"
11,The closed form for the integral: $\int_{0}^{1}\frac{x^{2a}\arctan{x}}{x^2+1}dx$,The closed form for the integral:,\int_{0}^{1}\frac{x^{2a}\arctan{x}}{x^2+1}dx,"I am trying to find a closed form for this integral: $$I=\int_{0}^{1}\frac{x^{2a}\arctan{x}}{x^2+1}dx$$ Where $a$ is any constant $\in R$ . I use this approach, but still don't get the result: First, use the series of $\arctan{x}=\sum_{k=0}^{\infty}\frac{(-1)^k}{2k+1}x^{2k+1}$ then $$I=\int_{0}^{1}\frac{x^{2a}\arctan{x}}{x^2+1}dx=\sum_{k=0}^{\infty}\frac{(-1)^k}{2k+1}\int_{0}^{1}\frac{x^{2a+2k+1}}{1+x^2}dx=\frac{1}{4}\sum_{k=0}^{\infty}\frac{(-1)^k}{2k+1}\left(H_{\frac{a+k}{2}}-H_{\frac{1}{2} (a+k-1)}\right)$$ But I got stuck at this step. Can you give me some hints, or another approaches? Thank you so much.","I am trying to find a closed form for this integral: Where is any constant . I use this approach, but still don't get the result: First, use the series of then But I got stuck at this step. Can you give me some hints, or another approaches? Thank you so much.",I=\int_{0}^{1}\frac{x^{2a}\arctan{x}}{x^2+1}dx a \in R \arctan{x}=\sum_{k=0}^{\infty}\frac{(-1)^k}{2k+1}x^{2k+1} I=\int_{0}^{1}\frac{x^{2a}\arctan{x}}{x^2+1}dx=\sum_{k=0}^{\infty}\frac{(-1)^k}{2k+1}\int_{0}^{1}\frac{x^{2a+2k+1}}{1+x^2}dx=\frac{1}{4}\sum_{k=0}^{\infty}\frac{(-1)^k}{2k+1}\left(H_{\frac{a+k}{2}}-H_{\frac{1}{2} (a+k-1)}\right),"['calculus', 'integration']"
12,How to get the following integral which is the Laplace transform of the modified Bessel function?,How to get the following integral which is the Laplace transform of the modified Bessel function?,,"How to get the following integral $$\int_0^\infty E_{\lambda\sim \mu}[\lambda e^{2 r\lambda}]e^{-4r}dr$$ where $\mu$ is the semi-circle law $d\mu(x)=\frac{1}{2\pi}\sqrt{4-x^2}1_{|x|\le 2}dx$ . Note that if define the moment generating function $M(t)=E[e^{t\lambda}]$ and then $E_{\lambda\sim \mu}[\lambda e^{2 t\lambda}]=M'(2t)$ . Also, we can express it by the modified Bessel function $$ M'(2t)=\frac{I_1(2t)}{t} $$ So that integral becomes the Laplace transform of the modified Bessel function $$ \int_0^\infty \frac{I_1(2r)}{r}e^{-4r}dr $$ But how to get the result?","How to get the following integral where is the semi-circle law . Note that if define the moment generating function and then . Also, we can express it by the modified Bessel function So that integral becomes the Laplace transform of the modified Bessel function But how to get the result?","\int_0^\infty E_{\lambda\sim \mu}[\lambda e^{2 r\lambda}]e^{-4r}dr \mu d\mu(x)=\frac{1}{2\pi}\sqrt{4-x^2}1_{|x|\le 2}dx M(t)=E[e^{t\lambda}] E_{\lambda\sim \mu}[\lambda e^{2 t\lambda}]=M'(2t) 
M'(2t)=\frac{I_1(2t)}{t}
 
\int_0^\infty \frac{I_1(2r)}{r}e^{-4r}dr
","['real-analysis', 'integration', 'probability-distributions', 'laplace-transform', 'bessel-functions']"
13,prove an integral inequality on the unit square,prove an integral inequality on the unit square,,"Let $f$ be a continuous function on the unit square. Prove that $\int_0^1 (\int_0^1 f(x,y) dx)^2 dy + \int_0^1 (\int_0^1 f(x,y) dy)^2 dx \leq (\int_0^1 \int_0^1 f(x,y)dx dy)^2 + \int_0^1 \int_0^1 f(x,y)^2 dx dy.$ I think writing the integrals as Riemann sums and taking limits should yield the result. If one divides the unit square into $n^2$ equal squares and picks a point $(x_i, y_j)$ in each square and defines $a_{ij} = f(x_i, y_j)$ , then the corresponding inequality in terms of Riemann sums is $\frac{1}{n^3} \sum_i ((\sum_j a_{ij})^2 + (\sum_j a_{ji})^2) \leq \frac{1}{n^4} (\sum_{ij} a_{ij})^2 + \frac{1}{n^2}\sum_{ij} a_{ij}^2.$ How can I manipulate this inequality to get the result I want (e.g. maybe by writing it as $\sum_{ij} x_{ij}^2 \ge 0$ where the $x_{ij}$ 's are carefully selected)?","Let be a continuous function on the unit square. Prove that I think writing the integrals as Riemann sums and taking limits should yield the result. If one divides the unit square into equal squares and picks a point in each square and defines , then the corresponding inequality in terms of Riemann sums is How can I manipulate this inequality to get the result I want (e.g. maybe by writing it as where the 's are carefully selected)?","f \int_0^1 (\int_0^1 f(x,y) dx)^2 dy + \int_0^1 (\int_0^1 f(x,y) dy)^2 dx \leq (\int_0^1 \int_0^1 f(x,y)dx dy)^2 + \int_0^1 \int_0^1 f(x,y)^2 dx dy. n^2 (x_i, y_j) a_{ij} = f(x_i, y_j) \frac{1}{n^3} \sum_i ((\sum_j a_{ij})^2 + (\sum_j a_{ji})^2) \leq \frac{1}{n^4} (\sum_{ij} a_{ij})^2 + \frac{1}{n^2}\sum_{ij} a_{ij}^2. \sum_{ij} x_{ij}^2 \ge 0 x_{ij}","['real-analysis', 'calculus', 'integration', 'inequality', 'contest-math']"
14,Are these two recursive formulas known in the literature?,Are these two recursive formulas known in the literature?,,"First let me introduce the two recursive relations: $$\int_0^1 x^{n-1}\ln^a(1-x)dx=f(a,n),$$ where $$f(a,n)=(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}f(j,n)  H_n^{(a-j)},\quad f(0,n)=\frac1n.\tag{1}$$ Cases using Mathematica: $$(-1)^a\frac{\ln^a(1-x)}{1-x}=\sum_{n=0}^\infty g(a,n) x^n,$$ where $$g(a,n)=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}g(j,n)H_n^{(a-j)},\quad g(0,n)=1.\tag{2}$$ Cases using Mathematica: Question : Are $(1)$ and $(2)$ known in the literature? If so, any reference? Proof of $(1)$ : Take the logarithm of both sides of $$\operatorname{B}(m,n)=\Gamma(n)\prod_{k=0}^{n-1} \frac{1}{k+m},$$ we have \begin{gather*} \ln\operatorname{B}(m,n)=\ln\Gamma(n)+\ln\prod_{k=0}^{n-1}\frac{1}{k+m}\\ \left\{\text{ use $\ln\prod a_n=\sum \ln(a_n)$}\right\}\\ =\ln\Gamma(n)-\sum_{k=0}^{n-1} \ln(k+m). \end{gather*} Differentiate both sides with respect to $m$ , $$\frac{\frac{\partial}{\partial m}\operatorname{B}(m,n)}{\operatorname{B}(m,n)}=-\sum_{k=0}^{n-1} \frac{1}{k+m}$$ or \begin{equation} \frac{\partial}{\partial m}\operatorname{B}(m,n)=-\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m}. \end{equation} Let's keep differentiating w.r.t $m$ : \begin{equation} \frac{\partial^2}{\partial m^2}\operatorname{B}(m,n)=\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^2}-\frac{\partial}{\partial m}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m}, \end{equation} \begin{equation} \frac{\partial^3}{\partial m^3}\operatorname{B}(m,n)=-2\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^3}+2\frac{\partial}{\partial m}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^2}\\ -\frac{\partial^2}{\partial m^2}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m}, \end{equation} \begin{equation} \frac{\partial^4}{\partial m^4}\operatorname{B}(m,n)=6\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^4}-6\frac{\partial}{\partial m}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^3}\\ +3\frac{\partial^2}{\partial m^2}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^2} -\frac{\partial^3}{\partial m^3}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m}, \end{equation} so in general $$\frac{\partial^a}{\partial m^a}\operatorname{B}(m,n)=(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}\frac{\partial^j}{\partial m^j}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^{a-j}}.$$ Let $m$ approach $1$ and call $\displaystyle\left.\frac{\partial^a}{\partial m^a}\operatorname{B}(m,n)\right|_{m\to1}=f(a,n)$ , $$f(a,n)=(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}f(j,n)H_n^{(a-j)}$$ and the proof finishes on observing \begin{gather*} \left.\frac{\partial^a}{\partial m^a}\operatorname{B}(m,n)\right|_{m=1}=\left.\int_0^1\frac{\partial^a}{\partial m^a} x^{n-1}(1-x)^{m-1}\mathrm{d}x\right|_{m=1}\\ =\left.\int_0^1 x^{n-1}\ln^a(1-x)(1-x)^{m-1}\mathrm{d}x\right|_{m=1}\\ =\int_0^1 x^{n-1}\ln^a(1-x)\mathrm{d}x=f(a,n)	 \end{gather*} and $f(0,n)=\int_0^1 x^{n-1}\mathrm{d}x=\frac1n.$ Proof of $(2)$ : We have $$\frac{1}{(1-x)^m}=(1-x)^{-m}=\sum_{n=0}^\infty \binom{m+n-1}{n}x^n$$ Take the $a$ -th derivative of both sides w.r.t $m$ $$(-1)^a\frac{\ln^a(1-x)}{(1-x)^m}=\sum_{n=0}^\infty \frac{\partial^a}{\partial^am}\binom{m+n-1}{n} x^n$$ Let $m\to 1$ and call $\displaystyle\left.\frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}\right|_{m\to 1}=g(a,n)$ $$(-1)^a\frac{\ln^a(1-x)}{1-x}=\sum_{n=0}^\infty g(a,n) x^n$$ Note that $$\frac{\partial}{\partial m}\binom{m+n-1}{n}=\binom{m+n-1}{n}\left(\psi(m+n)-\psi(m)\right)=\binom{m+n-1}{n}\sum_{k=0}^{n-1}\frac{1}{k+m}$$ which can be generalized to $$\frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}\frac{\partial^j}{\partial m^j}\binom{m+n-1}{n}\sum_{k=0}^{n-1} \frac{1}{(k+m)^{a-j}}.$$ and so $$\left.\frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}\right|_{m\to 1}$$ $$=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}\left.\frac{\partial^j}{\partial m^j}\binom{m+n-1}{n}\right|_{m\to 1}\,\left.\sum_{k=0}^{n-1} \frac{1}{(k+m)^{a-j}}\right|_{m\to 1}$$ or $$g(a,n)=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}g(j,n)H_n^{(a-j)}$$ and the proof completes on observing $$g(0,n)=\left.\binom{m+n-1}{n}\right|_{m\to 1}=\binom{n}{n}=1$$ Note: A recursive relation, similar to $(2)$ , was introduced by @Marko Riedel here .","First let me introduce the two recursive relations: where Cases using Mathematica: where Cases using Mathematica: Question : Are and known in the literature? If so, any reference? Proof of : Take the logarithm of both sides of we have Differentiate both sides with respect to , or Let's keep differentiating w.r.t : so in general Let approach and call , and the proof finishes on observing and Proof of : We have Take the -th derivative of both sides w.r.t Let and call Note that which can be generalized to and so or and the proof completes on observing Note: A recursive relation, similar to , was introduced by @Marko Riedel here .","\int_0^1 x^{n-1}\ln^a(1-x)dx=f(a,n), f(a,n)=(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}f(j,n)  H_n^{(a-j)},\quad f(0,n)=\frac1n.\tag{1} (-1)^a\frac{\ln^a(1-x)}{1-x}=\sum_{n=0}^\infty g(a,n) x^n, g(a,n)=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}g(j,n)H_n^{(a-j)},\quad g(0,n)=1.\tag{2} (1) (2) (1) \operatorname{B}(m,n)=\Gamma(n)\prod_{k=0}^{n-1} \frac{1}{k+m}, \begin{gather*}
\ln\operatorname{B}(m,n)=\ln\Gamma(n)+\ln\prod_{k=0}^{n-1}\frac{1}{k+m}\\
\left\{\text{ use \ln\prod a_n=\sum \ln(a_n)}\right\}\\
=\ln\Gamma(n)-\sum_{k=0}^{n-1} \ln(k+m).
\end{gather*} m \frac{\frac{\partial}{\partial m}\operatorname{B}(m,n)}{\operatorname{B}(m,n)}=-\sum_{k=0}^{n-1} \frac{1}{k+m} \begin{equation}
\frac{\partial}{\partial m}\operatorname{B}(m,n)=-\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m}.
\end{equation} m \begin{equation}
\frac{\partial^2}{\partial m^2}\operatorname{B}(m,n)=\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^2}-\frac{\partial}{\partial m}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m},
\end{equation} \begin{equation}
\frac{\partial^3}{\partial m^3}\operatorname{B}(m,n)=-2\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^3}+2\frac{\partial}{\partial m}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^2}\\
-\frac{\partial^2}{\partial m^2}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m},
\end{equation} \begin{equation}
\frac{\partial^4}{\partial m^4}\operatorname{B}(m,n)=6\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^4}-6\frac{\partial}{\partial m}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^3}\\
+3\frac{\partial^2}{\partial m^2}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^2}
-\frac{\partial^3}{\partial m^3}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m},
\end{equation} \frac{\partial^a}{\partial m^a}\operatorname{B}(m,n)=(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}\frac{\partial^j}{\partial m^j}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^{a-j}}. m 1 \displaystyle\left.\frac{\partial^a}{\partial m^a}\operatorname{B}(m,n)\right|_{m\to1}=f(a,n) f(a,n)=(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}f(j,n)H_n^{(a-j)} \begin{gather*}
\left.\frac{\partial^a}{\partial m^a}\operatorname{B}(m,n)\right|_{m=1}=\left.\int_0^1\frac{\partial^a}{\partial m^a} x^{n-1}(1-x)^{m-1}\mathrm{d}x\right|_{m=1}\\
=\left.\int_0^1 x^{n-1}\ln^a(1-x)(1-x)^{m-1}\mathrm{d}x\right|_{m=1}\\
=\int_0^1 x^{n-1}\ln^a(1-x)\mathrm{d}x=f(a,n)	
\end{gather*} f(0,n)=\int_0^1 x^{n-1}\mathrm{d}x=\frac1n. (2) \frac{1}{(1-x)^m}=(1-x)^{-m}=\sum_{n=0}^\infty \binom{m+n-1}{n}x^n a m (-1)^a\frac{\ln^a(1-x)}{(1-x)^m}=\sum_{n=0}^\infty \frac{\partial^a}{\partial^am}\binom{m+n-1}{n} x^n m\to 1 \displaystyle\left.\frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}\right|_{m\to 1}=g(a,n) (-1)^a\frac{\ln^a(1-x)}{1-x}=\sum_{n=0}^\infty g(a,n) x^n \frac{\partial}{\partial m}\binom{m+n-1}{n}=\binom{m+n-1}{n}\left(\psi(m+n)-\psi(m)\right)=\binom{m+n-1}{n}\sum_{k=0}^{n-1}\frac{1}{k+m} \frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}\frac{\partial^j}{\partial m^j}\binom{m+n-1}{n}\sum_{k=0}^{n-1} \frac{1}{(k+m)^{a-j}}. \left.\frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}\right|_{m\to 1} =-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}\left.\frac{\partial^j}{\partial m^j}\binom{m+n-1}{n}\right|_{m\to 1}\,\left.\sum_{k=0}^{n-1} \frac{1}{(k+m)^{a-j}}\right|_{m\to 1} g(a,n)=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}g(j,n)H_n^{(a-j)} g(0,n)=\left.\binom{m+n-1}{n}\right|_{m\to 1}=\binom{n}{n}=1 (2)","['integration', 'reference-request', 'logarithms', 'recurrence-relations', 'harmonic-numbers']"
15,Apply Dominated Convergence Theorem: $\lim_{n\rightarrow\infty}\int_0^\infty \dfrac{1+nx^2+n^2x^4}{(1+x^2)^n}d\mu.$,Apply Dominated Convergence Theorem:,\lim_{n\rightarrow\infty}\int_0^\infty \dfrac{1+nx^2+n^2x^4}{(1+x^2)^n}d\mu.,"I am working on a problem which I assume is an application of Dominated Convergence Theorem. I am supposed to find $$\lim_{n\rightarrow\infty}\int_0^\infty \dfrac{1+nx^2+n^2x^4}{(1+x^2)^n}d\mu.$$ I guess I need to find a bound on $$\frac{1+nx^2+n^2x^4}{(1+x^2)^n}=\frac{1+nx^2}{(1+x^2)^n}+\frac{n^2x^4}{(1+x^2)^n}.$$ I can bound the first part, $ \dfrac{1+nx^2}{(1+x^2)^n}$ , by using the binomial expression of ${(1+x^2)^n}$ . But I am stuck with dealing with the second part, $\dfrac{n^2x^4}{(1+x^2)^n}$ . Thank you in advance for any suggestions and help.","I am working on a problem which I assume is an application of Dominated Convergence Theorem. I am supposed to find I guess I need to find a bound on I can bound the first part, , by using the binomial expression of . But I am stuck with dealing with the second part, . Thank you in advance for any suggestions and help.",\lim_{n\rightarrow\infty}\int_0^\infty \dfrac{1+nx^2+n^2x^4}{(1+x^2)^n}d\mu. \frac{1+nx^2+n^2x^4}{(1+x^2)^n}=\frac{1+nx^2}{(1+x^2)^n}+\frac{n^2x^4}{(1+x^2)^n}.  \dfrac{1+nx^2}{(1+x^2)^n} {(1+x^2)^n} \dfrac{n^2x^4}{(1+x^2)^n},"['real-analysis', 'integration', 'measure-theory', 'improper-integrals']"
16,How to show that the integral inequality holds for vector-valued functions. [duplicate],How to show that the integral inequality holds for vector-valued functions. [duplicate],,"This question already has an answer here : Inequality involving the integral of vector valued function (1 answer) Closed 2 years ago . If I define the integral for $\int_a^b\textbf{F}(t)dt$ as $(\int_a^b F_1(t)dt, \int_a^bF_2(t)dt,\ldots, \int_a^bF_n(t)dt )$ . How do I then show that $$\left|\int_a^b\textbf{F}(t)dt \right|\le\int_a^b\left|\textbf{F}(t) \right|dt?$$ If we write out the inequality we have $$\sqrt{\left(\int_a^b F_1(t)dt\right)^2+\left(\int_a^bF_2(t)dt\right)^2+\cdots+ \left(\int_a^bF_n(t)dt\right)^2 }\le\int_a^b\sqrt{F_1(t)^2+F_2(t)^2+\cdots+F_n(t)^2}dt.$$ attempt: If I use Jenssen's inequality I get for the left side: $$\sqrt{\left(\int_a^b F_1(t)dt\right)^2+\left(\int_a^bF_2(t)dt\right)^2+\cdots+ \left(\int_a^bF_n(t)dt\right)^2 }\le\sqrt{\int_a^b F_1^2(t)dt+\int_a^bF_2^2(t)dt+\cdots+ \int_a^bF_n^2(t)dt }\\=\sqrt{\sum\limits_{i=1}^n\int_a^bF_i^2(t)dt}=\sqrt{\int_a^b\sum\limits_{i=1}^nF_i^2(t)dt}.$$ The problem I have now is that the square root function is concave, so if I use Jenssen again, I get the opposite inequality as the one I need. Any idea on how to solve this?","This question already has an answer here : Inequality involving the integral of vector valued function (1 answer) Closed 2 years ago . If I define the integral for as . How do I then show that If we write out the inequality we have attempt: If I use Jenssen's inequality I get for the left side: The problem I have now is that the square root function is concave, so if I use Jenssen again, I get the opposite inequality as the one I need. Any idea on how to solve this?","\int_a^b\textbf{F}(t)dt (\int_a^b F_1(t)dt, \int_a^bF_2(t)dt,\ldots, \int_a^bF_n(t)dt ) \left|\int_a^b\textbf{F}(t)dt \right|\le\int_a^b\left|\textbf{F}(t) \right|dt? \sqrt{\left(\int_a^b F_1(t)dt\right)^2+\left(\int_a^bF_2(t)dt\right)^2+\cdots+ \left(\int_a^bF_n(t)dt\right)^2 }\le\int_a^b\sqrt{F_1(t)^2+F_2(t)^2+\cdots+F_n(t)^2}dt. \sqrt{\left(\int_a^b F_1(t)dt\right)^2+\left(\int_a^bF_2(t)dt\right)^2+\cdots+ \left(\int_a^bF_n(t)dt\right)^2 }\le\sqrt{\int_a^b F_1^2(t)dt+\int_a^bF_2^2(t)dt+\cdots+ \int_a^bF_n^2(t)dt }\\=\sqrt{\sum\limits_{i=1}^n\int_a^bF_i^2(t)dt}=\sqrt{\int_a^b\sum\limits_{i=1}^nF_i^2(t)dt}.","['real-analysis', 'calculus', 'integration', 'measure-theory', 'inequality']"
17,Real and complex line integrals:,Real and complex line integrals:,,"From complex analysis I know that we define the complex line integral as follows: $$\int_\gamma f(z)dz = \int_a^bf(\gamma(t))\cdot\gamma'(t)dt \tag 1$$ Assuming that $\gamma$ is continuously differentiable with endpoints $a$ and $b$ . Back then I did not give this too much thought, as it seems like we are simply substituting $\gamma(t)$ for $z$ . Now we are defining the line integral over a real valued scalar field $\Omega$ as follows: $$\int_C \Omega(r) = \int_a^b\Omega(r(s))ds$$ Assuming that $r(s) = (x(s), y(s), z(s))$ is a curve parametrized by arc length. To generalise this definition for curves not parametrized by arc length we use the formula for the arc length $s$ : $$s = \int \lvert r'(t)\rvert dt \implies \frac{ds}{dt}=\lvert r'(t)\rvert$$ $$\therefore \quad \int_C \Omega(r) = \int_a^b\Omega(r(s))ds \stackrel{?}{=} \int_a^b\Omega(r(t))\frac{ds}{dt}dt = \int_a^b\Omega(r(t))\lvert r'(t)\rvert dt \tag 2$$ My two questions are: Apart from the modulus, $(1)$ and $(2)$ look very similar. Is there any intuitive connection between the the complex and real line integral? Or is it just a coincidence. After all, the multiplication $\cdot$ in $(1)$ is the complex multiplication, so these definitions may be not as similar as I think. What exactly is happening at the penultimate step in $(2)$ ? My chain of equal signs seems very handwavy, so I am assuming there is some rigour missing in my explanation.","From complex analysis I know that we define the complex line integral as follows: Assuming that is continuously differentiable with endpoints and . Back then I did not give this too much thought, as it seems like we are simply substituting for . Now we are defining the line integral over a real valued scalar field as follows: Assuming that is a curve parametrized by arc length. To generalise this definition for curves not parametrized by arc length we use the formula for the arc length : My two questions are: Apart from the modulus, and look very similar. Is there any intuitive connection between the the complex and real line integral? Or is it just a coincidence. After all, the multiplication in is the complex multiplication, so these definitions may be not as similar as I think. What exactly is happening at the penultimate step in ? My chain of equal signs seems very handwavy, so I am assuming there is some rigour missing in my explanation.","\int_\gamma f(z)dz = \int_a^bf(\gamma(t))\cdot\gamma'(t)dt \tag 1 \gamma a b \gamma(t) z \Omega \int_C \Omega(r) = \int_a^b\Omega(r(s))ds r(s) = (x(s), y(s), z(s)) s s = \int \lvert r'(t)\rvert dt \implies \frac{ds}{dt}=\lvert r'(t)\rvert \therefore \quad \int_C \Omega(r) = \int_a^b\Omega(r(s))ds \stackrel{?}{=} \int_a^b\Omega(r(t))\frac{ds}{dt}dt = \int_a^b\Omega(r(t))\lvert r'(t)\rvert dt \tag 2 (1) (2) \cdot (1) (2)","['integration', 'vector-analysis', 'contour-integration']"
18,Egoroff's Theorem problem (Folland Real Analysis Ex 40),Egoroff's Theorem problem (Folland Real Analysis Ex 40),,"The problem is to show that the theorem holds if we replace the condition that $\mu(X)<\infty$ with $|f_n|\leq g$ for all $n\ge 1$ , and $g\in L^1$ . I have completed half the problem but I am stuck on the other half. This is what I have so far Let $\epsilon>0$ . For $N\geq1$ , define $$G_N = \{x\;|\;g(x)\geq N^{-1}\}.$$ Then, since $g\in L^1$ , we have $\mu(G_n)$ is finite. Now, we can apply the original Egoroff's theroem, and get that there exists $E_N \subset G_N$ s.t. $\mu(E_N)<\frac{\epsilon}{2^N}$ and $f_n$ converges uniformly to $f$ on $G_N \setminus E_N.$ Let $$E=\bigcup_{N=1}^{\infty} E_N.$$ Then $$ \mu(E)\leq \displaystyle \sum \mu(E_N)=\epsilon$$ This is where I am stuck, I don't know how to show the uniform convergence on $E^c$ . Clearly $f_n$ converges uniformly to $f$ on $G^c$ , where $\displaystyle G=\bigcup_{N=1}^{\infty} G_N$ . So, at this point it would be enough to show uniform convergence on $G\setminus E$ . On any finite union of the $G_k \setminus E_k$ we have uniform convergence, so I imagine we need to use that. The one idea I have had is this: Define $$h_n(x)=\chi_{\bigcup\limits_{a=1}^n E_a \setminus G_a}(x)f(x)$$ The idea is to show that $h_n \rightarrow f$ uniformily, which, I think would be equivalent to showing that $f_n \rightarrow f$ uniformily, since $f_n \rightarrow f$ on every finite union of these sets, and also $h_n$ is in a sense equivalent to f restricted to $\displaystyle \bigcup_{a=1}^n E_a \setminus G_a$ . (please check this logic) Let $\epsilon > 0$ . Then, if $x\in \displaystyle\bigcup_{a=1}^N E_a \setminus G_a$ for some $N$ , $|h_n(x)-f(x)|=0<\epsilon$ for all $n\geq N$ . However, if $x$ is not in that union, then we need to show that $|f|<\epsilon$ . However, all we know is that $|f|\leq g$ . We do know that $g(x)\geq N^{-1}$ by construction but this doesn't seem helpful. What seems more helpful is that $g\in L^1$ , and hence $$\int_E g < \infty$$ For any subset $E\subset X$ . Perhaps we could use this? Although I don't know how.","The problem is to show that the theorem holds if we replace the condition that with for all , and . I have completed half the problem but I am stuck on the other half. This is what I have so far Let . For , define Then, since , we have is finite. Now, we can apply the original Egoroff's theroem, and get that there exists s.t. and converges uniformly to on Let Then This is where I am stuck, I don't know how to show the uniform convergence on . Clearly converges uniformly to on , where . So, at this point it would be enough to show uniform convergence on . On any finite union of the we have uniform convergence, so I imagine we need to use that. The one idea I have had is this: Define The idea is to show that uniformily, which, I think would be equivalent to showing that uniformily, since on every finite union of these sets, and also is in a sense equivalent to f restricted to . (please check this logic) Let . Then, if for some , for all . However, if is not in that union, then we need to show that . However, all we know is that . We do know that by construction but this doesn't seem helpful. What seems more helpful is that , and hence For any subset . Perhaps we could use this? Although I don't know how.",\mu(X)<\infty |f_n|\leq g n\ge 1 g\in L^1 \epsilon>0 N\geq1 G_N = \{x\;|\;g(x)\geq N^{-1}\}. g\in L^1 \mu(G_n) E_N \subset G_N \mu(E_N)<\frac{\epsilon}{2^N} f_n f G_N \setminus E_N. E=\bigcup_{N=1}^{\infty} E_N.  \mu(E)\leq \displaystyle \sum \mu(E_N)=\epsilon E^c f_n f G^c \displaystyle G=\bigcup_{N=1}^{\infty} G_N G\setminus E G_k \setminus E_k h_n(x)=\chi_{\bigcup\limits_{a=1}^n E_a \setminus G_a}(x)f(x) h_n \rightarrow f f_n \rightarrow f f_n \rightarrow f h_n \displaystyle \bigcup_{a=1}^n E_a \setminus G_a \epsilon > 0 x\in \displaystyle\bigcup_{a=1}^N E_a \setminus G_a N |h_n(x)-f(x)|=0<\epsilon n\geq N x |f|<\epsilon |f|\leq g g(x)\geq N^{-1} g\in L^1 \int_E g < \infty E\subset X,"['real-analysis', 'integration', 'analysis', 'measure-theory']"
19,Integration operator appears inside an integration operator as I try $~\mathcal{L}\left[x \cdot \sin^{}\left(x\right) \right]\left(s\right)~$,Integration operator appears inside an integration operator as I try,~\mathcal{L}\left[x \cdot \sin^{}\left(x\right) \right]\left(s\right)~,"$$  A:=\mathcal{L}\left[x \cdot \sin^{}\left(x\right) \right]\left(s\right) \tag{1}  $$ $$=\lim_{\beta\to\infty}\int_{0}^{\beta}\left(x\cdot\sin^{}\left(x\right)\right)\cdot\exp\left(-sx\right)\,dx$$ $$ = \lim_{\beta\to\infty}\int_{0}^{\beta}\left(x\right)\cdot \underbrace{\left( \sin^{}\left(x\right) \exp\left(-sx\right) \right)}_{\text{This part is to be integrated} }   \,dx  $$ $$ = \lim_{ \beta \to \infty} \left\{ \left[ x \cdot \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx   \right)  \right]_{0}^{\beta} - \int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx  \right)  \,dx   \right\}  $$ $$  \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx = - \frac{ s \cdot  \exp\left(-sx\right)   }{ \left( s^2+1 \right)    } \left( \sin^{}\left(x\right) + \frac{1}{s}\cos^{}\left(x\right)  \right) +\text{const} \tag{2}   $$ Should I have written the derivation of the above equation? $$ A= \lim_{ \beta \to \infty} \left\{ \left[ -\frac{  x  \cdot s \cdot \exp\left(-sx\right)   }{ \left( s^2+1 \right)    } \left( \sin^{}\left(x\right) + \frac{1}{s}\cos^{}\left(x\right)  \right)  \right]_{0}^{\beta} -  \int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx  \right)  \,dx    \right\}  $$ $$ = \lim_{ \beta \to \infty} \left\{ \underbrace{\left[ -\frac{  x  \cdot s   }{ \left( s^2+1 \right) e^{sx}   } \left( \sin^{}\left(x\right) + \frac{1}{s}\cos^{}\left(x\right)  \right)  \right]_{0}^{\beta}}_{\text{About}~\beta~ \text{,it converges to }~0   }  -  \int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx  \right)  \,dx    \right\}  $$ $$ = -\lim_{ \beta \to \infty} \underbrace{\int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx   \right)  \,dx}_\text{What can be done at here?}    $$",Should I have written the derivation of the above equation?,"  A:=\mathcal{L}\left[x \cdot \sin^{}\left(x\right) \right]\left(s\right) \tag{1}   =\lim_{\beta\to\infty}\int_{0}^{\beta}\left(x\cdot\sin^{}\left(x\right)\right)\cdot\exp\left(-sx\right)\,dx  = \lim_{\beta\to\infty}\int_{0}^{\beta}\left(x\right)\cdot \underbrace{\left( \sin^{}\left(x\right) \exp\left(-sx\right) \right)}_{\text{This part is to be integrated} }   \,dx    = \lim_{ \beta \to \infty} \left\{ \left[ x \cdot \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx   \right)  \right]_{0}^{\beta} - \int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx  \right)  \,dx   \right\}     \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx = - \frac{ s \cdot  \exp\left(-sx\right)   }{ \left( s^2+1 \right)    } \left( \sin^{}\left(x\right) + \frac{1}{s}\cos^{}\left(x\right)  \right) +\text{const} \tag{2}     A= \lim_{ \beta \to \infty} \left\{ \left[ -\frac{  x  \cdot s \cdot \exp\left(-sx\right)   }{ \left( s^2+1 \right)    } \left( \sin^{}\left(x\right) + \frac{1}{s}\cos^{}\left(x\right)  \right)  \right]_{0}^{\beta} -  \int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx  \right)  \,dx    \right\}    = \lim_{ \beta \to \infty} \left\{ \underbrace{\left[ -\frac{  x  \cdot s   }{ \left( s^2+1 \right) e^{sx}   } \left( \sin^{}\left(x\right) + \frac{1}{s}\cos^{}\left(x\right)  \right)  \right]_{0}^{\beta}}_{\text{About}~\beta~ \text{,it converges to }~0   }  -  \int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx  \right)  \,dx    \right\}    = -\lim_{ \beta \to \infty} \underbrace{\int_{0 }^{\beta } \left( \int_{ }^{ } \sin^{}\left(x\right) \exp\left(-sx\right)  \,dx   \right)  \,dx}_\text{What can be done at here?}    ","['integration', 'systems-of-equations', 'laplace-transform']"
20,"Asymptotic behavior of $\int_0^\pi\int_0^R e^{-at\cos\theta}\sin^2\theta J_0(at\sin\theta)\sin\theta \, \mathrm{d}t\mathrm{d}\theta$ as $R \to \infty$",Asymptotic behavior of  as,"\int_0^\pi\int_0^R e^{-at\cos\theta}\sin^2\theta J_0(at\sin\theta)\sin\theta \, \mathrm{d}t\mathrm{d}\theta R \to \infty","Consider the two following functions defined by double integrals \begin{align} \varphi_a (R) &= \int_0^\frac{\pi}{2} \int_0^R e^{-at\cos\theta} \sin^2\theta \, J_0(at\sin\theta) \sin\theta \, \mathrm{d}t \, \mathrm{d}\theta \, , \\ \psi_a (R) &= \int_0^\frac{\pi}{2} \int_0^R e^{-at\cos\theta} \sin^2\theta \,  J_1(at\sin\theta) \cos\theta \, \mathrm{d}t \, \mathrm{d}\theta \, , \end{align} wherein $a, R \in \mathbb{R}_+^*$ . It can be shown that $\lim_{R\to\infty} \varphi_a = 2/ (3a)$ and that $\lim_{R\to\infty} \psi_a = -1/ (3a)$ But how do $\varphi_a(R)$ and $\psi_a(R)$ behave asymptotically to leading order as $R \to \infty$ ? Any help or suggestions are highly appreciated. Thank you,","Consider the two following functions defined by double integrals wherein . It can be shown that and that But how do and behave asymptotically to leading order as ? Any help or suggestions are highly appreciated. Thank you,","\begin{align}
\varphi_a (R) &= \int_0^\frac{\pi}{2} \int_0^R e^{-at\cos\theta} \sin^2\theta \,
J_0(at\sin\theta) \sin\theta \, \mathrm{d}t \, \mathrm{d}\theta \, , \\
\psi_a (R) &= \int_0^\frac{\pi}{2} \int_0^R e^{-at\cos\theta} \sin^2\theta \,
 J_1(at\sin\theta) \cos\theta \, \mathrm{d}t \, \mathrm{d}\theta \, ,
\end{align} a, R \in \mathbb{R}_+^* \lim_{R\to\infty} \varphi_a = 2/ (3a) \lim_{R\to\infty} \psi_a = -1/ (3a) \varphi_a(R) \psi_a(R) R \to \infty","['real-analysis', 'integration', 'complex-analysis', 'asymptotics', 'indefinite-integrals']"
21,Proving $\pi$ is irrational with recursive integrals,Proving  is irrational with recursive integrals,\pi,"Given $$I_n = \int_{-1}^1 (1-x^2)^n\cos(\theta x)dx,$$ I have proven the relation $$\theta^2I_n = 2n(2n-1)I_{n-1} - 4n(n-1))I_{n-2}.$$ I am supposed to now notice that $$\theta^{2n+1}I_n = n!(P_n(\theta)\sin(\theta) + Q_n(\theta)\cos(\theta)),$$ where $P_n$ and $Q_n$ are polynomials of degree at most $2n$ with integer coefficients, then use this relation to prove that $\pi$ is irrational. I can sort of see the relation, but can't see a clear way to prove it besides ''the coefficients seem to decrease by one each time so the $n!$ makes sense, and $I_1,I_0$ have the desired $\sin\theta$ and $\cos\theta$ , and the plus $1$ makes sense since we apply the relation either $n$ or $n-1$ times on the terms (giving us $\theta^{2n}$ or $\theta^{2n-2}$ ) but then $I_1 = \frac{4}{\theta^3}(\sin\theta - \theta\cos\theta)$ and $I_0 = \frac{2}{\theta}\sin\theta$ ''. So, I can vaguely see how all the parts could fit together but am not convinced with my argument and don't see an obvious way to prove it carefully by actually applying the relation by hand (I've tried expanding a few of the term by hand to see if a particularly nice pattern appears, but I don't see one). Can someone explain how to do this a bit more carefully? I see how to prove that $\pi$ is irrational given this relation (plug in $\theta = \pi/2$ and find an integer between $0$ and $1$ assuming $\pi$ is rational). I got the problem from this practice sheet: [https://www.dpmms.cam.ac.uk/study/IA/AnalysisI/2020-2021/aI_4_21.pdf]","Given I have proven the relation I am supposed to now notice that where and are polynomials of degree at most with integer coefficients, then use this relation to prove that is irrational. I can sort of see the relation, but can't see a clear way to prove it besides ''the coefficients seem to decrease by one each time so the makes sense, and have the desired and , and the plus makes sense since we apply the relation either or times on the terms (giving us or ) but then and ''. So, I can vaguely see how all the parts could fit together but am not convinced with my argument and don't see an obvious way to prove it carefully by actually applying the relation by hand (I've tried expanding a few of the term by hand to see if a particularly nice pattern appears, but I don't see one). Can someone explain how to do this a bit more carefully? I see how to prove that is irrational given this relation (plug in and find an integer between and assuming is rational). I got the problem from this practice sheet: [https://www.dpmms.cam.ac.uk/study/IA/AnalysisI/2020-2021/aI_4_21.pdf]","I_n = \int_{-1}^1 (1-x^2)^n\cos(\theta x)dx, \theta^2I_n = 2n(2n-1)I_{n-1} - 4n(n-1))I_{n-2}. \theta^{2n+1}I_n = n!(P_n(\theta)\sin(\theta) + Q_n(\theta)\cos(\theta)), P_n Q_n 2n \pi n! I_1,I_0 \sin\theta \cos\theta 1 n n-1 \theta^{2n} \theta^{2n-2} I_1 = \frac{4}{\theta^3}(\sin\theta - \theta\cos\theta) I_0 = \frac{2}{\theta}\sin\theta \pi \theta = \pi/2 0 1 \pi","['real-analysis', 'integration', 'polynomials', 'pi']"
22,Integration by rational substitution,Integration by rational substitution,,"I was taking a look at a proof of the Riemann-Liouville Integral of consecutive integrations, and at some point I reached a step where it shows the following substitution: $$_{a}I_{x}^\alpha(_{a}I_{x}^\beta f(x))=\frac{1}{\Gamma(\alpha)\Gamma(\beta)}\int_{a}^x \int_\zeta^x(x-t)^{\alpha-1}(t-\zeta)^{\beta-1}f(\zeta)\mathop{dt}\mathop{d\zeta}$$ using the substitution $u=\frac{t-\zeta}{x-\zeta}$ we finally obtain $$_{a}I_{x}^\alpha(_{a}I_{x}^\beta f(x))=\frac{B(\beta,\alpha)}{\Gamma(\alpha)\Gamma(\beta)}\int_{a}^x (x-\zeta)^{\alpha+\beta-1}f(\zeta)\mathop{d\zeta}=\,_{a}I_{x}^{\alpha+\beta} f(x)$$ My question is, is there any motivation to make such a substitution as $u=\frac{t-\zeta}{x-\zeta}$ ? I mean, is some kind of special rational substitution? it reminds me to Möbius transformation somewhow but i can't connect the ideas. Edit: After reading Markus Scheuer reply, I've found the next document: Aygören, Aysel - Fractional Derivative and Integral [2014], pp 27-29","I was taking a look at a proof of the Riemann-Liouville Integral of consecutive integrations, and at some point I reached a step where it shows the following substitution: using the substitution we finally obtain My question is, is there any motivation to make such a substitution as ? I mean, is some kind of special rational substitution? it reminds me to Möbius transformation somewhow but i can't connect the ideas. Edit: After reading Markus Scheuer reply, I've found the next document: Aygören, Aysel - Fractional Derivative and Integral [2014], pp 27-29","_{a}I_{x}^\alpha(_{a}I_{x}^\beta f(x))=\frac{1}{\Gamma(\alpha)\Gamma(\beta)}\int_{a}^x \int_\zeta^x(x-t)^{\alpha-1}(t-\zeta)^{\beta-1}f(\zeta)\mathop{dt}\mathop{d\zeta} u=\frac{t-\zeta}{x-\zeta} _{a}I_{x}^\alpha(_{a}I_{x}^\beta f(x))=\frac{B(\beta,\alpha)}{\Gamma(\alpha)\Gamma(\beta)}\int_{a}^x (x-\zeta)^{\alpha+\beta-1}f(\zeta)\mathop{d\zeta}=\,_{a}I_{x}^{\alpha+\beta} f(x) u=\frac{t-\zeta}{x-\zeta}","['integration', 'substitution', 'fractional-calculus']"
23,"Is there a better solution for $\mathrm{\int (a^t)^{(a^t)}dt= C+t+\frac1{\,ln(a)}\sum_{n=0}^\infty \frac{(-1)^n Q(n+1,-nt\ln(a))}{n^{n+1}}\,dt}$?",Is there a better solution for ?,"\mathrm{\int (a^t)^{(a^t)}dt= C+t+\frac1{\,ln(a)}\sum_{n=0}^\infty \frac{(-1)^n Q(n+1,-nt\ln(a))}{n^{n+1}}\,dt}","I know there exist functions like this one for simplifying tetration based sums. There may be a way to simplify this type of sum at least using a lesser known and widely accepted functions. Here are some results as proof: graphical visualization of results and calculation of special case and integral version of special case . It was a nice idea to find the sophomore’s dream , but I thought that it would be more interesting if I could find a “generalized exponential sophomore’s dream”. This post uses the following functions: regularized gamma functions , the exponential integral function, and tetration . There was an annoying discontinuity at n=0 hence the constant term: $$\mathrm{\int_0^b \, ^2\left(a^t\right) \, dt=\int_0^b a^{ta^t} \, dt = b + \sum_{n=1}^\infty\frac{\ln^n(a)}{n!}\int_0^b t^n a^{tn} \, dt = \boxed{\mathrm{b+\frac1{\ln(a)}\sum_{n=1}^\infty\frac{(-1)^nP\big(n+1,-n\,b\ln(a)\big)}{n^{n+1}}}}\implies \int_{-\frac1e}^0 e^{{te}^t} \, dt = 1+\sum_{n=1}^\infty \frac{(-1)^nP(n+1,n)}{n^{n+1}}=0.77215…}$$ $$\mathrm{\implies A(a,t)\mathop=^\text{def}\int \,^2\left(a^t\right)dt=C+t+\frac1{ln(a)}\sum_{n=0}^\infty \frac{(-1)^n Q(n+1,-nt\,ln(a))}{n^{n+1}}=\quad C+t-t\sum_{n=0}^\infty \frac{(t\,ln(a))^n E_{-n}(-nt\,ln(a))}{n!}}$$ This series reminds me of the Marcum Q function for non negative integers: $$\mathrm{Q_m(a,b)=1-e^{-\frac{a^2}2}\sum_{n=0}^\infty\left(\frac{a^2}{2}\right)^n\frac{P\left(m+n,\frac{b^2}2\right)}{n!}}$$ This representation is good, but is quite tedious to use as the formula requires summing an infinite amount of regularized gamma functions. I would like to find a way to get rid of the summation .I see the gamma function with powers which reminds me of the summation definition of a hypergeometric function . I would even like to see the use of a generalized hypergeometric function like Meijer G or Kampé de Fériet functions seen in the link. Please correct me and give me feedback!","I know there exist functions like this one for simplifying tetration based sums. There may be a way to simplify this type of sum at least using a lesser known and widely accepted functions. Here are some results as proof: graphical visualization of results and calculation of special case and integral version of special case . It was a nice idea to find the sophomore’s dream , but I thought that it would be more interesting if I could find a “generalized exponential sophomore’s dream”. This post uses the following functions: regularized gamma functions , the exponential integral function, and tetration . There was an annoying discontinuity at n=0 hence the constant term: This series reminds me of the Marcum Q function for non negative integers: This representation is good, but is quite tedious to use as the formula requires summing an infinite amount of regularized gamma functions. I would like to find a way to get rid of the summation .I see the gamma function with powers which reminds me of the summation definition of a hypergeometric function . I would even like to see the use of a generalized hypergeometric function like Meijer G or Kampé de Fériet functions seen in the link. Please correct me and give me feedback!","\mathrm{\int_0^b \, ^2\left(a^t\right) \, dt=\int_0^b a^{ta^t} \, dt = b + \sum_{n=1}^\infty\frac{\ln^n(a)}{n!}\int_0^b t^n a^{tn} \, dt = \boxed{\mathrm{b+\frac1{\ln(a)}\sum_{n=1}^\infty\frac{(-1)^nP\big(n+1,-n\,b\ln(a)\big)}{n^{n+1}}}}\implies \int_{-\frac1e}^0 e^{{te}^t} \, dt = 1+\sum_{n=1}^\infty \frac{(-1)^nP(n+1,n)}{n^{n+1}}=0.77215…} \mathrm{\implies A(a,t)\mathop=^\text{def}\int \,^2\left(a^t\right)dt=C+t+\frac1{ln(a)}\sum_{n=0}^\infty \frac{(-1)^n Q(n+1,-nt\,ln(a))}{n^{n+1}}=\quad C+t-t\sum_{n=0}^\infty \frac{(t\,ln(a))^n E_{-n}(-nt\,ln(a))}{n!}} \mathrm{Q_m(a,b)=1-e^{-\frac{a^2}2}\sum_{n=0}^\infty\left(\frac{a^2}{2}\right)^n\frac{P\left(m+n,\frac{b^2}2\right)}{n!}}","['integration', 'exponential-function', 'hypergeometric-function', 'tetration', 'power-towers']"
24,Is it true that the derivative of an integral function is integrable?,Is it true that the derivative of an integral function is integrable?,,"Let $f(x)$ be a Riemann integrable function (not neccesarily continuous) defined on some interval I. Let $F(x) = \int_a^xf(t)dt$ be it's integral function. Suppose that $F(x)$ is differentiable in the interval where it is defined, and let $F'(x)$ be its derivative. Is it true that $F'(x)$ is Riemann integrable on the interval I? I was wondering if this proposition is true, since the symmetric proposition is true by Barrow's Rule (FTC) Let $F(x)$ be a differentiable function defined on some interval I. Let $f(x) = F'(x)$ be it's derivative. Suppose that $f(x)$ is integrable in the interval where it is defined. Then $\int_a^x f(t)dt = F(x) + c$ , where $c = -F(a)$ If the first proposition its true, applying the second one we would have $\int_a^x F'(t)dt = F(x) - F(a)$ I would be grateful if the the proof is ""elementary"" and does not use any Measure Theory or Lebesgue integral results.","Let be a Riemann integrable function (not neccesarily continuous) defined on some interval I. Let be it's integral function. Suppose that is differentiable in the interval where it is defined, and let be its derivative. Is it true that is Riemann integrable on the interval I? I was wondering if this proposition is true, since the symmetric proposition is true by Barrow's Rule (FTC) Let be a differentiable function defined on some interval I. Let be it's derivative. Suppose that is integrable in the interval where it is defined. Then , where If the first proposition its true, applying the second one we would have I would be grateful if the the proof is ""elementary"" and does not use any Measure Theory or Lebesgue integral results.",f(x) F(x) = \int_a^xf(t)dt F(x) F'(x) F'(x) F(x) f(x) = F'(x) f(x) \int_a^x f(t)dt = F(x) + c c = -F(a) \int_a^x F'(t)dt = F(x) - F(a),"['real-analysis', 'calculus', 'integration', 'derivatives', 'definite-integrals']"
25,Integral $\int^\infty_0 \frac{\cos(\omega x)}{\cosh(ax)-\cos\beta}\text{d}x$,Integral,\int^\infty_0 \frac{\cos(\omega x)}{\cosh(ax)-\cos\beta}\text{d}x,"Maybe you can help me, understanding an arising ambiguity: Consider the integral, which is on page 30 integral (6) of the Bateman Project( see link below) $$\int^\infty_0 \frac{\cos(\omega x)}{\cosh(ax)+\cos(\beta)}\text{d}x=\frac{\pi}{a\sin(\beta)}\frac{\sinh(\frac{\beta \omega}{a})}{\sinh(\frac{\pi \omega}{a})}$$ which holds for $\text{Re}(a)\pi>\text{Im}(a^*\beta) $ . Say I want to calculate the integral with a minus in the denominatorfor real $a, \beta$ (hence the above restriction on the parameters is always satisfied), which I have naively done by $\int^\infty_0 \frac{\cos(\omega x)}{\cosh(ax)-\cos(\beta)}\text{d}x=\int^\infty_0 \frac{\cos(\omega x)}{\cosh(ax)+\cos(\beta\pm\pi)}\text{d}x= \frac{\pi}{a\sin(\beta\pm \pi)}\frac{\sinh(\frac{(\beta\pm\pi) \omega}{a})}{\sinh(\frac{\pi \omega}{a})}=-\frac{\pi}{a\sin(\beta)}\frac{\sinh(\frac{(\beta\pm\pi) \omega}{a})}{\sinh(\frac{\pi \omega}{a})}$ , then I get the ambiguity of the choosen sign infront of the $\pi$ in the $\sinh(\frac{(\beta\pm\pi) \omega}{a})$ . Can anyone explain to me what it is the correct way (sign of $\pi$ ) to solve this integral with the minus in the denominator? Link for formula: https://authors.library.caltech.edu/43489/1/Volume%201.pdf","Maybe you can help me, understanding an arising ambiguity: Consider the integral, which is on page 30 integral (6) of the Bateman Project( see link below) which holds for . Say I want to calculate the integral with a minus in the denominatorfor real (hence the above restriction on the parameters is always satisfied), which I have naively done by , then I get the ambiguity of the choosen sign infront of the in the . Can anyone explain to me what it is the correct way (sign of ) to solve this integral with the minus in the denominator? Link for formula: https://authors.library.caltech.edu/43489/1/Volume%201.pdf","\int^\infty_0 \frac{\cos(\omega x)}{\cosh(ax)+\cos(\beta)}\text{d}x=\frac{\pi}{a\sin(\beta)}\frac{\sinh(\frac{\beta \omega}{a})}{\sinh(\frac{\pi \omega}{a})} \text{Re}(a)\pi>\text{Im}(a^*\beta)  a, \beta \int^\infty_0 \frac{\cos(\omega x)}{\cosh(ax)-\cos(\beta)}\text{d}x=\int^\infty_0 \frac{\cos(\omega x)}{\cosh(ax)+\cos(\beta\pm\pi)}\text{d}x=
\frac{\pi}{a\sin(\beta\pm \pi)}\frac{\sinh(\frac{(\beta\pm\pi) \omega}{a})}{\sinh(\frac{\pi \omega}{a})}=-\frac{\pi}{a\sin(\beta)}\frac{\sinh(\frac{(\beta\pm\pi) \omega}{a})}{\sinh(\frac{\pi \omega}{a})} \pi \sinh(\frac{(\beta\pm\pi) \omega}{a}) \pi","['integration', 'definite-integrals', 'fourier-transform']"
26,Where is my mistake $\int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx$,Where is my mistake,\int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx,"Edit As commented bellow by @Donald Splutterwit and @ Elliot Yu, it seems that my computation is numerically correct and the post is wrong! I also added a corollary from this computation. I saw the following statement here $$\boxed{\int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx=\frac{7}{4}\zeta(3)}$$ And I wanted to proof it. My approach was the following $$I=\int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx=\int_{0}^{1}\frac{\log(1-x)\log[(1-x)(1+x)]}{x}dx$$ $$=\int_{0}^{1}\frac{\log(1-x)\log(1-x)}{x}dx+\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx$$ $$=\underbrace{\int_{0}^{1}\frac{\log^2(1-x)}{x}dx}_{I_{1}}+\underbrace{\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx}_{I_{2}}$$ $$I_{1}=\int_{0}^{1}\frac{\log^2(1-x)}{x}dx$$ let $1-x=t$ $$I_{1}=\int_{0}^{1}\frac{\log^2(t)}{1-t} dt$$ $$=\int_{0}^{1}\log^2(t)\sum_{k=0}^{\infty}t^kdt$$ $$=\sum_{k=0}^{\infty}\int_{0}^{1}t^k\log^2(t)dt$$ Now use the fact that $$\boxed{\int_{0}^{1}x^m \log^n(x)dx=\frac{(-1)^{n}n! }{(m+1)^{n+1}}}$$ for $$n=2 \,\,\text{and} \,\, m=k $$ $$I_{1}=\sum_{k=0}^{\infty}\frac{(-1)^{2}2! }{(k+1)^{3}}$$ $$\boxed{\int_{0}^{1}\frac{\log^2(1-x)}{x}dx=2\zeta(3)}$$ The second integral is a little trickier $$I_{2}=\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx$$ Observe the following $$\Big(\log(1-x)+\log(1+x) \Big)^2=\log^2(1-x)+2\log(1-x)\log(1+x)+\log^2(1+x)$$ $$\log(1-x)\log(1+x)=\frac{\Big(\log[(1-x)(1+x)] \Big)^2-\log^2(1-x)-\log^2(1+x)}{2}$$ $$\log(1-x)\log(1+x)=\frac{\log^2(1-x^2)-\log^2(1-x)-\log^2(1+x)}{2}$$ dividing both sides by $x$ and integrating from $0$ to $1$ $$\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=\frac{1}{2}\int_{0}^{1}\frac{\log^2(1-x^2)}{x}dx-\frac{1}{2}\int_{0}^{1}\frac{\log^2(1-x)}{x}dx-\frac{1}{2}\int_{0}^{1}\frac{\log^2(1+x)}{x}dx$$ Now we have to evaluate the three integrals on the RHS and we are done. I´ll state only the value of each  integral since their proof is easy to find on this forum. $$\int_{0}^{1}\frac{\log^2(1-x^2)}{x}dx=\zeta(3)$$ $$\int_{0}^{1}\frac{\log^2(1-x)}{x}dx=2\zeta(3)$$ $$\int_{0}^{1}\frac{\log^2(1+x)}{x}dx=\frac{1}{4}\zeta(3)$$ Putting all together we get $$I_{2}=\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=-\frac{5}{8}\zeta(3)$$ Now summing the results of $I_{1}$ and $I_{2}$ we get the final result which differs form the above statement! $$\int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx=2\zeta(3)-\frac{5}{8}\zeta(3)=\frac{11}{8}\zeta(3)$$ A Corollary We just computed the integral $$\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=-\frac{5}{8}\zeta(3)$$ On the other hand we can show that this integral equals $$\sum_{n=1}^{\infty}\frac{(-1)^n H_{n}}{n^2}$$ and conclude that $$\sum_{n=1}^{\infty}\frac{(-1)^{n-1} H_{n}}{n^2}=\frac{5}{8}\zeta(3)$$ Proof: Expanding $\log(1+x)$ in Taylor series we get $$I=\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\int_{0}^{1}x^{n-1}\log(1-x)dx$$ Integrating by parts we get: $$I=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\bigg\{\frac{\log(1-x)(x^n-1)}{n}\Big|_{0}^{1}+\frac{1}{n}\int_{0}^{1}\frac{x^n-1}{1-x}dx \bigg\}$$ $$I=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\bigg\{-\frac{1}{n}\int_{0}^{1}\frac{1-x^n}{1-x}dx \bigg\}$$ $$I=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\bigg\{-\frac{1}{n}H_{n} \bigg\}$$ $$\boxed{\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=-\sum_{n=1}^{\infty}\frac{(-1)^{n-1}H_{n} }{n^2}}$$ and therefore $$\boxed{\sum_{n=1}^{\infty}\frac{(-1)^{n-1} H_{n}}{n^2}=\frac{5}{8}\zeta(3)}$$","Edit As commented bellow by @Donald Splutterwit and @ Elliot Yu, it seems that my computation is numerically correct and the post is wrong! I also added a corollary from this computation. I saw the following statement here And I wanted to proof it. My approach was the following let Now use the fact that for The second integral is a little trickier Observe the following dividing both sides by and integrating from to Now we have to evaluate the three integrals on the RHS and we are done. I´ll state only the value of each  integral since their proof is easy to find on this forum. Putting all together we get Now summing the results of and we get the final result which differs form the above statement! A Corollary We just computed the integral On the other hand we can show that this integral equals and conclude that Proof: Expanding in Taylor series we get Integrating by parts we get: and therefore","\boxed{\int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx=\frac{7}{4}\zeta(3)} I=\int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx=\int_{0}^{1}\frac{\log(1-x)\log[(1-x)(1+x)]}{x}dx =\int_{0}^{1}\frac{\log(1-x)\log(1-x)}{x}dx+\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx =\underbrace{\int_{0}^{1}\frac{\log^2(1-x)}{x}dx}_{I_{1}}+\underbrace{\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx}_{I_{2}} I_{1}=\int_{0}^{1}\frac{\log^2(1-x)}{x}dx 1-x=t I_{1}=\int_{0}^{1}\frac{\log^2(t)}{1-t} dt =\int_{0}^{1}\log^2(t)\sum_{k=0}^{\infty}t^kdt =\sum_{k=0}^{\infty}\int_{0}^{1}t^k\log^2(t)dt \boxed{\int_{0}^{1}x^m \log^n(x)dx=\frac{(-1)^{n}n! }{(m+1)^{n+1}}} n=2 \,\,\text{and} \,\, m=k  I_{1}=\sum_{k=0}^{\infty}\frac{(-1)^{2}2! }{(k+1)^{3}} \boxed{\int_{0}^{1}\frac{\log^2(1-x)}{x}dx=2\zeta(3)} I_{2}=\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx \Big(\log(1-x)+\log(1+x) \Big)^2=\log^2(1-x)+2\log(1-x)\log(1+x)+\log^2(1+x) \log(1-x)\log(1+x)=\frac{\Big(\log[(1-x)(1+x)] \Big)^2-\log^2(1-x)-\log^2(1+x)}{2} \log(1-x)\log(1+x)=\frac{\log^2(1-x^2)-\log^2(1-x)-\log^2(1+x)}{2} x 0 1 \int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=\frac{1}{2}\int_{0}^{1}\frac{\log^2(1-x^2)}{x}dx-\frac{1}{2}\int_{0}^{1}\frac{\log^2(1-x)}{x}dx-\frac{1}{2}\int_{0}^{1}\frac{\log^2(1+x)}{x}dx \int_{0}^{1}\frac{\log^2(1-x^2)}{x}dx=\zeta(3) \int_{0}^{1}\frac{\log^2(1-x)}{x}dx=2\zeta(3) \int_{0}^{1}\frac{\log^2(1+x)}{x}dx=\frac{1}{4}\zeta(3) I_{2}=\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=-\frac{5}{8}\zeta(3) I_{1} I_{2} \int_{0}^{1}\frac{\log(1-x)\log(1-x^2)}{x}dx=2\zeta(3)-\frac{5}{8}\zeta(3)=\frac{11}{8}\zeta(3) \int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=-\frac{5}{8}\zeta(3) \sum_{n=1}^{\infty}\frac{(-1)^n H_{n}}{n^2} \sum_{n=1}^{\infty}\frac{(-1)^{n-1} H_{n}}{n^2}=\frac{5}{8}\zeta(3) \log(1+x) I=\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\int_{0}^{1}x^{n-1}\log(1-x)dx I=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\bigg\{\frac{\log(1-x)(x^n-1)}{n}\Big|_{0}^{1}+\frac{1}{n}\int_{0}^{1}\frac{x^n-1}{1-x}dx \bigg\} I=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\bigg\{-\frac{1}{n}\int_{0}^{1}\frac{1-x^n}{1-x}dx \bigg\} I=\sum_{n=1}^{\infty}\frac{(-1)^{n-1} }{n}\bigg\{-\frac{1}{n}H_{n} \bigg\} \boxed{\int_{0}^{1}\frac{\log(1-x)\log(1+x)}{x}dx=-\sum_{n=1}^{\infty}\frac{(-1)^{n-1}H_{n} }{n^2}} \boxed{\sum_{n=1}^{\infty}\frac{(-1)^{n-1} H_{n}}{n^2}=\frac{5}{8}\zeta(3)}","['integration', 'sequences-and-series', 'harmonic-numbers', 'zeta-functions']"
27,"Get asymptotic behaviour of the equation $y' = C e^{-(\log x)^s}$, as $x \rightarrow \infty$","Get asymptotic behaviour of the equation , as",y' = C e^{-(\log x)^s} x \rightarrow \infty,"I have the following differential equation $$y''+ s \frac{(\log x)^{s-1}}{x}y'=0$$ where $s>0$ . It  is very easy to see that $$y' = C_1 e^{-(\log x)^s},$$ which cannot be integrated in closed form, as far as I know. What I am interested in is finding how $y$ behaves as $x \rightarrow \infty$ , that is, it's asymptotic behaviour. A first idea would be to write the integrand as a series in $(\log x )^s$ and integrate term by term, but that doesn't really help since as $x$ is arbitrarily large, every term is larger than the previous one (in absolute terms), and so it doesn't make sense to terminate the series somewhere. How can I do that in this case? EDIT: One idea might be to make the ansatz $$y(x) = \sum_{n=0}^\infty y_n(x) s^n,$$ and solve for the $y_n(x)$ term by term. The zeroth order term is $$y_0(x) = C_1 e^{-d} x+C_2,$$ while the first order term is $$y_1(x) = C_1 d e^{-d} \int^x \log (\log u) du.$$ In general, it is easy to see that $$y_n(x) \propto \int^x[ \log (\log u)]^n du.$$ If we are able to compute the limit of the ratio $$ \lim_{x \rightarrow \infty} \frac{y_{n+1}(x)}{y_n(x)} \propto \lim_{x \rightarrow \infty}\frac{\int^x[ \log (\log u)]^{n+1} du }{\int^x[ \log (\log u)]^n du} $$ and show that it vanishes (I don't know what it is equal to, but it'd be lovely if it vanished ), then indeed we have the asymptotic behaviour. If it diverges, then we haven't made much progress. I think it probably diverges, given that it is easy to see that the limit is $$ \lim_{x \rightarrow \infty} \frac{y'_{n+1}(x)}{y'_n(x)} \propto \lim_{x \rightarrow \infty}\frac{[ \log (\log x)]^{n+1} }{[ \log (\log x)]^n}=\lim_{x \rightarrow \infty} \log (\log x) = \infty. $$ This of course assumes that the original limit is indeterminate which is likely but not known yet. NOTE: I set all integration constants for the terms $y_n(x)$ with $n \geq 1$ equal to zero, and keep them only for the zeroth term.","I have the following differential equation where . It  is very easy to see that which cannot be integrated in closed form, as far as I know. What I am interested in is finding how behaves as , that is, it's asymptotic behaviour. A first idea would be to write the integrand as a series in and integrate term by term, but that doesn't really help since as is arbitrarily large, every term is larger than the previous one (in absolute terms), and so it doesn't make sense to terminate the series somewhere. How can I do that in this case? EDIT: One idea might be to make the ansatz and solve for the term by term. The zeroth order term is while the first order term is In general, it is easy to see that If we are able to compute the limit of the ratio and show that it vanishes (I don't know what it is equal to, but it'd be lovely if it vanished ), then indeed we have the asymptotic behaviour. If it diverges, then we haven't made much progress. I think it probably diverges, given that it is easy to see that the limit is This of course assumes that the original limit is indeterminate which is likely but not known yet. NOTE: I set all integration constants for the terms with equal to zero, and keep them only for the zeroth term.","y''+ s \frac{(\log x)^{s-1}}{x}y'=0 s>0 y' = C_1 e^{-(\log x)^s}, y x \rightarrow \infty (\log x )^s x y(x) = \sum_{n=0}^\infty y_n(x) s^n, y_n(x) y_0(x) = C_1 e^{-d} x+C_2, y_1(x) = C_1 d e^{-d} \int^x \log (\log u) du. y_n(x) \propto \int^x[ \log (\log u)]^n du.  \lim_{x \rightarrow \infty} \frac{y_{n+1}(x)}{y_n(x)} \propto \lim_{x \rightarrow \infty}\frac{\int^x[ \log (\log u)]^{n+1} du
}{\int^x[ \log (\log u)]^n du}
  \lim_{x \rightarrow \infty} \frac{y'_{n+1}(x)}{y'_n(x)} \propto \lim_{x \rightarrow \infty}\frac{[ \log (\log x)]^{n+1}
}{[ \log (\log x)]^n}=\lim_{x \rightarrow \infty} \log (\log x) = \infty.
 y_n(x) n \geq 1","['integration', 'ordinary-differential-equations', 'asymptotics']"
28,Condition for an integral to be zero,Condition for an integral to be zero,,"For a bounded function $\operatorname{F}: \mathbb{R}_{\,\ge\ 0} \to \mathbb{R}$ ( not necessarily non-negative ), is it true that $$ \int_{0}^{\infty}\frac{x^{k}\,s}{(s^{2} + x^{2})^{\left(k + 3\right)/2}\,\,}\, \operatorname{F}\left(x\right)\,{\rm d}x = 0\quad \forall s > 0  \iff \operatorname{F} \equiv 0 $$ where $k \in \mathbb{N}$ is a positive constant $?$ Of course, one implication ( $\leftarrow$ ) is true. What about the other one $?$ .","For a bounded function ( not necessarily non-negative ), is it true that where is a positive constant Of course, one implication ( ) is true. What about the other one .","\operatorname{F}: \mathbb{R}_{\,\ge\ 0} \to \mathbb{R} 
\int_{0}^{\infty}\frac{x^{k}\,s}{(s^{2} + x^{2})^{\left(k + 3\right)/2}\,\,}\, \operatorname{F}\left(x\right)\,{\rm d}x = 0\quad
\forall s > 0  \iff \operatorname{F} \equiv 0
 k \in \mathbb{N} ? \leftarrow ?","['real-analysis', 'calculus', 'integration']"
29,Prove that $\int^1_0|f'(x)|dx \le 2\int^1_0 |f(x)|dx+\int^1_0|f''(x)|dx.$,Prove that,\int^1_0|f'(x)|dx \le 2\int^1_0 |f(x)|dx+\int^1_0|f''(x)|dx.,"$f(x),f'(x),f''(x)$ are  continuous functions, $f(0)f(1) \ge 0$ , prove that $$\int^1_0|f'(x)|dx \le 2\int^1_0 |f(x)|dx+\int^1_0|f''(x)|dx.$$ This problem comes from here .","are  continuous functions, , prove that This problem comes from here .","f(x),f'(x),f''(x) f(0)f(1) \ge 0 \int^1_0|f'(x)|dx \le 2\int^1_0 |f(x)|dx+\int^1_0|f''(x)|dx.","['calculus', 'integration', 'definite-integrals', 'integral-inequality']"
30,How to evaluate $\int _0^1\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx$,How to evaluate,\int _0^1\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx,"I want to evaluate $$\int _0^1\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx$$ But I've not been successful in doing so, what I tried is $$\int _0^1\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx=\int _0^1\left(x\right)'\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx$$ $$\overset{\operatorname{IBP}}=\ln \left(\zeta (2)\right)+\int _0^1\frac{\ln \left(1-x\right)}{\operatorname{Li}_2\left(x\right)}\:dx$$ I also tried using identities for the dilogarithm but they dont do much. I'm not sure what to do now, any help will be very well regarded, thank you.","I want to evaluate But I've not been successful in doing so, what I tried is I also tried using identities for the dilogarithm but they dont do much. I'm not sure what to do now, any help will be very well regarded, thank you.",\int _0^1\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx \int _0^1\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx=\int _0^1\left(x\right)'\ln \left(\operatorname{Li}_2\left(x\right)\right)\:dx \overset{\operatorname{IBP}}=\ln \left(\zeta (2)\right)+\int _0^1\frac{\ln \left(1-x\right)}{\operatorname{Li}_2\left(x\right)}\:dx,"['integration', 'definite-integrals']"
31,Justifying the change of variables formula $\int_{g(a)}^{g(b)} f(y)dy = \int_a^b f(g(x))g'(x)dx$ for Lebesgue Integration,Justifying the change of variables formula  for Lebesgue Integration,\int_{g(a)}^{g(b)} f(y)dy = \int_a^b f(g(x))g'(x)dx,"This is a problem from Royden & Fitzpatrick 4th ed, page 129 problem 59. I am struggling proving it and was wondering if someone can help prove it please? Thank you For a nonnegative integrable function $f$ over $[c,d],$ and a strictly increasing absolutely continuous function $g$ on $[a,b]$ such that $g([a,b]) \subseteq [c,d],$ is it possible to justify the change of variables formula $$\int_{g(a)}^{g(b)} f(y)dy = \int_a^b f(g(x))g'(x)dx,$$ by showing that $$\frac{d}{dx} \left[\int_{g(a)}^{g(x)} f(s)ds - \int_a^x f(g(t))g'(t)dt \right] = 0 \text{ for almost all } x\in (a,b)?$$","This is a problem from Royden & Fitzpatrick 4th ed, page 129 problem 59. I am struggling proving it and was wondering if someone can help prove it please? Thank you For a nonnegative integrable function over and a strictly increasing absolutely continuous function on such that is it possible to justify the change of variables formula by showing that","f [c,d], g [a,b] g([a,b]) \subseteq [c,d], \int_{g(a)}^{g(b)} f(y)dy = \int_a^b f(g(x))g'(x)dx, \frac{d}{dx} \left[\int_{g(a)}^{g(x)} f(s)ds - \int_a^x f(g(t))g'(t)dt \right] = 0 \text{ for almost all } x\in (a,b)?","['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral', 'absolute-continuity']"
32,"Showing a summation identity for $1$, possibly tied to Legendre polynomials","Showing a summation identity for , possibly tied to Legendre polynomials",1,"The Problem: Consider the sign function on $(-1,0)\cup(0,1)$ defined by $$ \sigma(x) := \left. \text{sgn}(x) \right|_{(-1,0)\cup(0,1)} = \begin{cases} 1 & x \in (0,1) \\ -1 & x \in (-1,0) \end{cases}$$ The problem is to show that $$\int_{-1}^1 (\sigma(x))^2 dx = 2 \sum_{n=0}^\infty (4n+3) \left( \frac{(2n-1)!!}{(2n+2)!!} \right)^2$$ Context: This is (in its essence) problem $15.2.8$ in Mathematical Methods for Physicists by Arfken, Weber, & Harris. It was assigned to me as a homework problem for one of my classes. (In that vein I would prefer to only have nudges in the right direction, rather than full solutions.) The discussion in this section ( $\S 15.2$ ) is essentially on Legendre polynomials and Fourier-Legendre series. It is quite obvious that the integral evaluates to $2$ , so the problem is ultimately showing that $$\sum_{n=0}^\infty (4n+3) \left( \frac{(2n-1)!!}{(2n+2)!!} \right)^2 = 1$$ However, browsing the text, Wikipedia , and MathWorld don't give me any enlightening ideas on what identities to leverage. Expanding $f(x) = 1$ as a Fourier-Legendre series $$f(x) = \sum_{n=0}^\infty c_n P_n(x) \; \text{where} \; c_n = \int_{-1}^1 f(x)P_n(x)dx$$ doesn't really lead me anywhere (for the integral in $c_n$ is zero whenever $n \ge 1$ ) - which is obvious enough, since $P_0(x) = 1$ anyways, so of course we'd get a finite series. The identity does seem true. Taking the equivalent formulation of the problem (as a series equaling $1$ ) and summing $n=0$ to $n=100$ yields a result of about $0.996$ according to Wolfram , and up to $n=10,000$ yields about $0.999354$ ( Wolfram ), so it seems reasonable it converges to $1$ , albeit somewhat slowly. The original problem is in multiple parts: this is part (a), and part (c) notes, as I did, the integral $\int_{-1}^1 \sigma^2(x)dx = 2$ . So it also seems plausible that I'm not even meant to calculate the integral at the outset, but instead utilize some other method. I suppose one could rewrite $\sigma$ as $$ \sigma(x) = \begin{cases} P_0(x) & x \in (0,1) \\ -P_0(x) & x \in (-1,0) \end{cases}$$ and perhaps utilize some sort of identity used in the motivations/derivations tied to Legendre polynomials (a lot of integrals of $P_n^2$ seem to come up), but this rewriting doesn't give me anything more enlightening to work with. Does anyone have some ideas as to how I might at least get started with this?","The Problem: Consider the sign function on defined by The problem is to show that Context: This is (in its essence) problem in Mathematical Methods for Physicists by Arfken, Weber, & Harris. It was assigned to me as a homework problem for one of my classes. (In that vein I would prefer to only have nudges in the right direction, rather than full solutions.) The discussion in this section ( ) is essentially on Legendre polynomials and Fourier-Legendre series. It is quite obvious that the integral evaluates to , so the problem is ultimately showing that However, browsing the text, Wikipedia , and MathWorld don't give me any enlightening ideas on what identities to leverage. Expanding as a Fourier-Legendre series doesn't really lead me anywhere (for the integral in is zero whenever ) - which is obvious enough, since anyways, so of course we'd get a finite series. The identity does seem true. Taking the equivalent formulation of the problem (as a series equaling ) and summing to yields a result of about according to Wolfram , and up to yields about ( Wolfram ), so it seems reasonable it converges to , albeit somewhat slowly. The original problem is in multiple parts: this is part (a), and part (c) notes, as I did, the integral . So it also seems plausible that I'm not even meant to calculate the integral at the outset, but instead utilize some other method. I suppose one could rewrite as and perhaps utilize some sort of identity used in the motivations/derivations tied to Legendre polynomials (a lot of integrals of seem to come up), but this rewriting doesn't give me anything more enlightening to work with. Does anyone have some ideas as to how I might at least get started with this?","(-1,0)\cup(0,1)  \sigma(x) := \left. \text{sgn}(x) \right|_{(-1,0)\cup(0,1)} = \begin{cases} 1 & x \in (0,1) \\ -1 & x \in (-1,0) \end{cases} \int_{-1}^1 (\sigma(x))^2 dx = 2 \sum_{n=0}^\infty (4n+3) \left( \frac{(2n-1)!!}{(2n+2)!!} \right)^2 15.2.8 \S 15.2 2 \sum_{n=0}^\infty (4n+3) \left( \frac{(2n-1)!!}{(2n+2)!!} \right)^2 = 1 f(x) = 1 f(x) = \sum_{n=0}^\infty c_n P_n(x) \; \text{where} \; c_n = \int_{-1}^1 f(x)P_n(x)dx c_n n \ge 1 P_0(x) = 1 1 n=0 n=100 0.996 n=10,000 0.999354 1 \int_{-1}^1 \sigma^2(x)dx = 2 \sigma  \sigma(x) = \begin{cases} P_0(x) & x \in (0,1) \\ -P_0(x) & x \in (-1,0) \end{cases} P_n^2","['integration', 'sequences-and-series', 'fourier-series', 'legendre-polynomials', 'legendre-functions']"
33,"If $\mathbb E_{\mathbb P} \vert f(X,Y)\vert <\infty$, is also $\mathbb E_{\mathbb P_1\times \mathbb P_2} \vert f(X,Y)\vert <\infty$?","If , is also ?","\mathbb E_{\mathbb P} \vert f(X,Y)\vert <\infty \mathbb E_{\mathbb P_1\times \mathbb P_2} \vert f(X,Y)\vert <\infty","Let $(E_1,\mathcal E_1)$ and $(E_2,\mathcal E_2)$ be two measurable spaces. Let $(E=E_1\times E_2,\mathcal E=\mathcal E_1\times \mathcal E_2)$ . Let $\mathbb P$ a valid probability distribution on $(E,\mathcal E)$ . Define the marginal distributions $\mathbb P_1(A)=\mathbb P (A\times E_2)$ and $\mathbb P_2(A)=\mathbb P (E_1\times A)$ , and finally denote the product distribution by $\mathbb P_1\times \mathbb P_2$ . Finally assume that we know \begin{align*} \mathbb E_{\mathbb P} \vert f(X,Y)\vert <\infty.\end{align*} My question is if it also holds that \begin{align*} \mathbb E_{\mathbb P_1\times \mathbb P_2} \vert f(X,Y)\vert <\infty?\end{align*} In particular, if it is false I hope to see a counterexample and I'd also be interested to know if it is true in the case $f(X,Y)=g(X)h(Y)$ for some functions $g$ and $h$ .","Let and be two measurable spaces. Let . Let a valid probability distribution on . Define the marginal distributions and , and finally denote the product distribution by . Finally assume that we know My question is if it also holds that In particular, if it is false I hope to see a counterexample and I'd also be interested to know if it is true in the case for some functions and .","(E_1,\mathcal E_1) (E_2,\mathcal E_2) (E=E_1\times E_2,\mathcal E=\mathcal E_1\times \mathcal E_2) \mathbb P (E,\mathcal E) \mathbb P_1(A)=\mathbb P (A\times E_2) \mathbb P_2(A)=\mathbb P (E_1\times A) \mathbb P_1\times \mathbb P_2 \begin{align*} \mathbb E_{\mathbb P} \vert f(X,Y)\vert <\infty.\end{align*} \begin{align*} \mathbb E_{\mathbb P_1\times \mathbb P_2} \vert f(X,Y)\vert <\infty?\end{align*} f(X,Y)=g(X)h(Y) g h","['real-analysis', 'integration', 'probability-theory', 'measure-theory', 'probability-distributions']"
34,Ramanujan Identity related to JacobiFunction [duplicate],Ramanujan Identity related to JacobiFunction [duplicate],,"This question already has answers here : Proof of a Ramanujan Integral (4 answers) Closed 3 years ago . The following identity is allegedly due to Ramanujan $$\int_0^\infty \frac{{\rm d}x}{(1+x^2)(1+r^2x^2)(1+r^4x^2)\cdots} = \frac{\pi/2}{\sum_{n=0}^\infty r^{\frac{n(n+1)}{2}}} \, $$ but how do you prove this? The denominator of the right side is related to the Jacobi Function, so maybe one could proceed via modular forms?","This question already has answers here : Proof of a Ramanujan Integral (4 answers) Closed 3 years ago . The following identity is allegedly due to Ramanujan but how do you prove this? The denominator of the right side is related to the Jacobi Function, so maybe one could proceed via modular forms?","\int_0^\infty \frac{{\rm d}x}{(1+x^2)(1+r^2x^2)(1+r^4x^2)\cdots} = \frac{\pi/2}{\sum_{n=0}^\infty r^{\frac{n(n+1)}{2}}} \, ","['integration', 'modular-function']"
35,Proof of Stokes' theorem for differentiable manifolds,Proof of Stokes' theorem for differentiable manifolds,,"In Evan's well-known PDE book, he states the following in an appendix, without proof: Let $U\subset\mathbb{R}^n$ be an open, bounded set with $\partial U$ being $C^1$ . Suppose $u\in C^1(\bar{U})$ , then $$\int_U \frac{\partial u}{\partial x_i} \, dx=\int_{\partial U} u\nu^i \, dS\;\;\;\;(i=1,\ldots,n),$$ where $\nu=(\nu^1,\ldots\nu^n)$ denotes the outward-pointing unit normal vector field to the region $U$ . This is the Green–Gauss Theorem, and it can be derived from the divergence theorem, which in turn is a special case of Stokes' theorem. I cannot find a reference which gives a (complete, rigorous) proof of Stokes' theorem in this generality (i.e. not requiring $C^\infty$ ). Where can I find such a proof? Note: a very similar question is asked here , but careful inspection show that the supposed answers do not actually resolve the problem. A proof is given for $n=3$ , $U$ convex, but there is some handwaviness in generalizing to the nonconvex case. All references given point to special cases of the theorem (e.g. Rudin) or to $C^\infty$ presentations. Note 2: If possible, I would prefer an analytic proof as I am not so familiar with differential forms. But any complete reference would be appreciated.","In Evan's well-known PDE book, he states the following in an appendix, without proof: Let be an open, bounded set with being . Suppose , then where denotes the outward-pointing unit normal vector field to the region . This is the Green–Gauss Theorem, and it can be derived from the divergence theorem, which in turn is a special case of Stokes' theorem. I cannot find a reference which gives a (complete, rigorous) proof of Stokes' theorem in this generality (i.e. not requiring ). Where can I find such a proof? Note: a very similar question is asked here , but careful inspection show that the supposed answers do not actually resolve the problem. A proof is given for , convex, but there is some handwaviness in generalizing to the nonconvex case. All references given point to special cases of the theorem (e.g. Rudin) or to presentations. Note 2: If possible, I would prefer an analytic proof as I am not so familiar with differential forms. But any complete reference would be appreciated.","U\subset\mathbb{R}^n \partial U C^1 u\in C^1(\bar{U}) \int_U \frac{\partial u}{\partial x_i} \, dx=\int_{\partial U} u\nu^i \, dS\;\;\;\;(i=1,\ldots,n), \nu=(\nu^1,\ldots\nu^n) U C^\infty n=3 U C^\infty","['integration', 'differential-geometry', 'stokes-theorem']"
36,"Evaluate $\int_0^1 \log (1-x)\ _3F_2\left(1,1,1;\frac{3}{2},\frac{3}{2};x\right) \, dx$",Evaluate,"\int_0^1 \log (1-x)\ _3F_2\left(1,1,1;\frac{3}{2},\frac{3}{2};x\right) \, dx","I encountered a hypergeometric integral while investigating harmonic sums $$\int_0^1 \log (1-x)\ _3F_2\left(1,1,1;\frac{3}{2},\frac{3}{2};x\right) \, dx$$ Based on my experience I suspect a nice closed-form exists but have found none.  Any kind of help will be appreciated. Update: To complete the solution following @Jack D'Aurizio's derivation, $1$ . Let $uz\to u$ in expression $f(z)=\int_{0}^{1}\frac{\arcsin\sqrt{uz}}{\sqrt{uz(1-uz)}}\log(1-u)\,du$ $2$ . Apply Fubini to $\int_0^1 f(4x(1-x))dx$ , then it become $\int_0^1 du \int_{\frac{1-\sqrt{1-u}}2}^{\frac{1+\sqrt{1-u}}2}dx\cdots$ $3$ . Integrate w.r.t $x$ by brute force, then let $u\to \frac{4t^2}{1+2t^2+t^4}$ $4$ . These integrals are evaluated using method of arXiv $2007.03957$ . Whence $$-\frac{1}4 \sum _{n=1}^{\infty } \left(\frac{4^n}{\binom{2 n}{n}}\right)^2\frac{ H_n}{n^3}=\int_0^1 \log (1-x)\ _3F_2\left(1,1,1;\frac{3}{2},\frac{3}{2};x\right) \, dx=-8 C^2+8 \pi  C \log (2)-32 \pi  \Im(\text{Li}_3(1+i))-16 \text{Li}_4\left(\frac{1}{2}\right)+\frac{413 \pi ^4}{360}-\frac{2}{3}  \log ^4(2)+\frac{8}{3} \pi ^2 \log ^2(2)$$","I encountered a hypergeometric integral while investigating harmonic sums Based on my experience I suspect a nice closed-form exists but have found none.  Any kind of help will be appreciated. Update: To complete the solution following @Jack D'Aurizio's derivation, . Let in expression . Apply Fubini to , then it become . Integrate w.r.t by brute force, then let . These integrals are evaluated using method of arXiv . Whence","\int_0^1 \log (1-x)\ _3F_2\left(1,1,1;\frac{3}{2},\frac{3}{2};x\right) \, dx 1 uz\to u f(z)=\int_{0}^{1}\frac{\arcsin\sqrt{uz}}{\sqrt{uz(1-uz)}}\log(1-u)\,du 2 \int_0^1 f(4x(1-x))dx \int_0^1 du \int_{\frac{1-\sqrt{1-u}}2}^{\frac{1+\sqrt{1-u}}2}dx\cdots 3 x u\to \frac{4t^2}{1+2t^2+t^4} 4 2007.03957 -\frac{1}4 \sum _{n=1}^{\infty } \left(\frac{4^n}{\binom{2 n}{n}}\right)^2\frac{ H_n}{n^3}=\int_0^1 \log (1-x)\ _3F_2\left(1,1,1;\frac{3}{2},\frac{3}{2};x\right) \, dx=-8 C^2+8 \pi  C \log (2)-32 \pi  \Im(\text{Li}_3(1+i))-16 \text{Li}_4\left(\frac{1}{2}\right)+\frac{413 \pi ^4}{360}-\frac{2}{3}  \log ^4(2)+\frac{8}{3} \pi ^2 \log ^2(2)","['integration', 'definite-integrals', 'closed-form', 'hypergeometric-function', 'polylogarithm']"
37,"Solutions for $ \int_0^{\infty} \frac{1}{\sqrt{t^3\left(t+\tau\right)^3}}\exp\left[- \frac{2t+\tau}{t\left(t+\tau\right)}\right] \, dt $",Solutions for," \int_0^{\infty} \frac{1}{\sqrt{t^3\left(t+\tau\right)^3}}\exp\left[- \frac{2t+\tau}{t\left(t+\tau\right)}\right] \, dt ","I am trying to find an analytical solution to the above integral. The context is as follows: I am interested in obtaining an expression for the autocovariance function of the change in groundwater level in an idealised aquifer following a unit pulse input. The governing equation for fluid motion in this idealised aquifer is given by: \begin{equation} \frac{\partial h}{\partial t} = \frac{T}{S}\frac{\partial^2 h}{\partial x^2} \end{equation} where $T$ and $S$ are characteristics of the aquifer system. The unit impulse-response function is given by: \begin{equation} h^{\delta}\left(x,\, t\right) = \frac{Sx^2}{4T}\left(t^*\right)^{-\frac{3}{2}}\mathrm{e}^{-1/t^*} \quad \mbox{where} \quad t^* = \frac{4Tt}{Sx^2} \end{equation} The autocovariance function, $\gamma\left(x,\,\tau\right)$ for the causal system is given by: \begin{equation} \gamma\left(x,\, \tau\right) = \int_0^{\infty} h^{\delta}\left(x,\,t\right)h^{\delta}\left(x,\,t+\tau\right) dt = \left(\frac{Sx^2}{4T}\right)^2 \int_0^{\infty} \left(t^*\right)^{-\frac{3}{2}}\mathrm{e}^{-1/t^*}\left(t^*+\tau\right)^{-\frac{3}{2}}\mathrm{e}^{-1/(t^*+\tau)}\,  dt^* \end{equation} I have noted that: \begin{equation} \int_0^{\infty} \left(t^*\right)^{-\frac{3}{2}}\mathrm{e}^{-1/t^*} \, dt^* = \sqrt{\pi} \quad \mbox{and} \quad \int_0^{\infty} \left(t^*+\tau\right)^{-\frac{3}{2}}\mathrm{e}^{-1/(t^*+\tau)} \, dt^* = \sqrt{\pi} \,\mathrm{erf}\left[\frac{1}{\tau}\right] \end{equation} but can get no further. Any assistance or thoughts would be much appreciated.","I am trying to find an analytical solution to the above integral. The context is as follows: I am interested in obtaining an expression for the autocovariance function of the change in groundwater level in an idealised aquifer following a unit pulse input. The governing equation for fluid motion in this idealised aquifer is given by: where and are characteristics of the aquifer system. The unit impulse-response function is given by: The autocovariance function, for the causal system is given by: I have noted that: but can get no further. Any assistance or thoughts would be much appreciated.","\begin{equation}
\frac{\partial h}{\partial t} = \frac{T}{S}\frac{\partial^2 h}{\partial x^2}
\end{equation} T S \begin{equation}
h^{\delta}\left(x,\, t\right) = \frac{Sx^2}{4T}\left(t^*\right)^{-\frac{3}{2}}\mathrm{e}^{-1/t^*} \quad \mbox{where} \quad t^* = \frac{4Tt}{Sx^2}
\end{equation} \gamma\left(x,\,\tau\right) \begin{equation}
\gamma\left(x,\, \tau\right) = \int_0^{\infty} h^{\delta}\left(x,\,t\right)h^{\delta}\left(x,\,t+\tau\right) dt = \left(\frac{Sx^2}{4T}\right)^2 \int_0^{\infty} \left(t^*\right)^{-\frac{3}{2}}\mathrm{e}^{-1/t^*}\left(t^*+\tau\right)^{-\frac{3}{2}}\mathrm{e}^{-1/(t^*+\tau)}\,  dt^*
\end{equation} \begin{equation}
\int_0^{\infty} \left(t^*\right)^{-\frac{3}{2}}\mathrm{e}^{-1/t^*} \, dt^* = \sqrt{\pi} \quad \mbox{and} \quad \int_0^{\infty} \left(t^*+\tau\right)^{-\frac{3}{2}}\mathrm{e}^{-1/(t^*+\tau)} \, dt^* = \sqrt{\pi} \,\mathrm{erf}\left[\frac{1}{\tau}\right]
\end{equation}","['integration', 'convolution']"
38,Necessary and sufficient condition for weak convergence and convergence of density,Necessary and sufficient condition for weak convergence and convergence of density,,"Let $(\mu_n)_n$ and $\mu$ be two probability measure, having respectively density $(f_n)_n$ and $f$ for the measure $\lambda$ on $(\mathbb{R},B(\mathbb{R})).$ Prove that the following statement are equivalent: a) $(\mu_n)_n$ converges weakly to $\mu$ and $$\forall \epsilon>0,\exists \delta>0;\forall n \in \mathbb{N}, \forall E \in B(\mathbb{R}),\lambda(E)\leq \delta\implies\int_Ef_n(x)dx \leq \epsilon$$ b) $(\mu_n)_n$ converges weakly to $\mu$ and $$\lim_{k\to+\infty}\sup_{n \in \mathbb{N}}\int_{\left\{f_n>k \right\}}f_n(x)dx=0.$$ c) $\forall E \in B(\mathbb{R}),\lim_{n\to+\infty}\mu_n(E)=\mu(E).$ If $(\mu_n)_n$ converges weakly to a probability measure $\sigma$ and for all $\epsilon>0,$ there exist $\delta>0$ such that for all $n \in \mathbb{N},$ for all $E \in B(\mathbb{R})$ such that $\lambda(E)\leq \delta,\int_Ef_n(x)dx \leq \epsilon.$ Is it true that $\sigma$ have a probability density ? (There exist $\phi:\mathbb{R}\to\mathbb{R}^+,$ such that $\int_{\mathbb{R}}\phi(x)dx=1, \sigma(U)=\int_U\phi(x)dx,$ for all $U \in B(\mathbb{R})$ ) This is the attempt so far. a) $\implies$ b). Take $\epsilon>0.$ there exist $\delta>0$ such that $$\forall n \in \mathbb{N},\forall E \in B(\mathbb{R}),\lambda(E) \leq \delta \implies \int_Ef_ndx \leq \epsilon.$$ Let $k \geq \frac{1}{\delta}.$ So $$\forall n \in \mathbb{N},\lambda(\left\{f_n>k \right\}) \leq \frac{1}{k} \leq \delta$$ which means that $$\forall n \in \mathbb{N},\int_{\left\{f_n>k \right\}}f_n \leq\epsilon,$$ Then $\sup_n\int_{\left\{f_n>k \right\}}f_n(x)dx \leq \epsilon.$ b) $\implies$ a). Let $\epsilon>0.$ there exist $k>0$ such that $$\sup_n \int_{\left\{f_n>k \right\}}f_n(x)dx \leq \epsilon/2.$$ Let $n \in \mathbb{N},E \in B(\mathbb{R})$ such that $\lambda(E) \leq \frac{\epsilon}{2(k+1)}.$ $$\int_E f_n(x)dx \leq k\lambda(E)+\int_{\left\{f_n>k \right\}}f_n(x)dx \leq \epsilon.$$ How can we proceed with c) $\implies$ a)? 2) is the statement correct?","Let and be two probability measure, having respectively density and for the measure on Prove that the following statement are equivalent: a) converges weakly to and b) converges weakly to and c) If converges weakly to a probability measure and for all there exist such that for all for all such that Is it true that have a probability density ? (There exist such that for all ) This is the attempt so far. a) b). Take there exist such that Let So which means that Then b) a). Let there exist such that Let such that How can we proceed with c) a)? 2) is the statement correct?","(\mu_n)_n \mu (f_n)_n f \lambda (\mathbb{R},B(\mathbb{R})). (\mu_n)_n \mu \forall \epsilon>0,\exists \delta>0;\forall n \in \mathbb{N}, \forall E \in B(\mathbb{R}),\lambda(E)\leq \delta\implies\int_Ef_n(x)dx \leq \epsilon (\mu_n)_n \mu \lim_{k\to+\infty}\sup_{n \in \mathbb{N}}\int_{\left\{f_n>k \right\}}f_n(x)dx=0. \forall E \in B(\mathbb{R}),\lim_{n\to+\infty}\mu_n(E)=\mu(E). (\mu_n)_n \sigma \epsilon>0, \delta>0 n \in \mathbb{N}, E \in B(\mathbb{R}) \lambda(E)\leq \delta,\int_Ef_n(x)dx \leq \epsilon. \sigma \phi:\mathbb{R}\to\mathbb{R}^+, \int_{\mathbb{R}}\phi(x)dx=1, \sigma(U)=\int_U\phi(x)dx, U \in B(\mathbb{R}) \implies \epsilon>0. \delta>0 \forall n \in \mathbb{N},\forall E \in B(\mathbb{R}),\lambda(E) \leq \delta \implies \int_Ef_ndx \leq \epsilon. k \geq \frac{1}{\delta}. \forall n \in \mathbb{N},\lambda(\left\{f_n>k \right\}) \leq \frac{1}{k} \leq \delta \forall n \in \mathbb{N},\int_{\left\{f_n>k \right\}}f_n \leq\epsilon, \sup_n\int_{\left\{f_n>k \right\}}f_n(x)dx \leq \epsilon. \implies \epsilon>0. k>0 \sup_n \int_{\left\{f_n>k \right\}}f_n(x)dx \leq \epsilon/2. n \in \mathbb{N},E \in B(\mathbb{R}) \lambda(E) \leq \frac{\epsilon}{2(k+1)}. \int_E f_n(x)dx \leq k\lambda(E)+\int_{\left\{f_n>k \right\}}f_n(x)dx \leq \epsilon. \implies","['real-analysis', 'integration', 'probability-theory', 'measure-theory', 'weak-convergence']"
39,Differentiability of an integral accumulation function,Differentiability of an integral accumulation function,,"Is $$H(x) = \int_0^x \left\lvert\sin\left(\frac{1}t\right)\right\rvert\,\mathrm dt$$ differentiable at $x = 0$ ? I claim that $H(x)$ is differentiable at $x=0.$ Observe that \begin{align}H(-x) &= \displaystyle\int_0^{-x}|\sin(\frac{1}t)|dt=\displaystyle\int_0^x |\sin(-\frac{1}u)|(-1)du,\text{ where $u = -t,$}\\ &=-\displaystyle\int_0^x |\sin(\frac{1}t)|dt = -H(x),\end{align} so $H(x)$ is odd. Also, $H(0) = 0.$ It suffices to evaluate $\lim\limits_{x\to 0^+}\dfrac{H(x)}x,$ since if $\lim\limits_{x\to 0^+} H(x)$ exists, it must equal $-\lim\limits_{x\to 0^-}H(x),$ which implies that $\lim\limits_{x\to 0^+}\dfrac{H(x)}x = \lim\limits_{x\to 0^-}\dfrac{H(x)}x.$ So assume $x>0.$ Since $|\sin(\frac{1}t)|$ is bounded and continuous on $(0, x], H(x) = \displaystyle\int_0^x |\sin(\frac{1}t)|dt = \lim\limits_{u\to 0^+}\displaystyle\int_u^x |\sin(\frac{1}t)|dt=\lim\limits_{n\to\infty}\displaystyle\int_{1/((n+1)\pi)}^{1/(k_x\pi)}|\sin(\frac{1}t)|dt + \displaystyle\int_{1/(k_x\pi)}^x |\sin(\frac{1}t)|dt \\ = \displaystyle\sum_{k=k_x}^\infty \displaystyle\int_{1/((k+1)\pi)}^{1/(k\pi)}|\sin(\frac{1}t)|dt+\displaystyle\int_{1/(k_x\pi)}^x|\sin(\frac{1}t)|dt,$ where $\frac{1}{k_x\pi} \leq x \leq \frac{1}{(k_x-1)\pi}\Rightarrow k_x\pi \geq \frac{1}{x} \geq (k_x - 1)\pi \Rightarrow k_x =  \lceil \frac{1}{x\pi} \rceil.$ Now, observe that $0 \leq |\dfrac{H(x)}x|\leq \dfrac{1}x \left|\displaystyle\sum_{k=k_x}^\infty \displaystyle\int_{1/((k+1)\pi)}^{1/(k\pi)}|\sin(\frac{1}t)|dt + \displaystyle\int_{1/(k_x\pi)}^x |\sin(1/t)|dt\right|\\ \leq \dfrac{1}x(\left|\displaystyle\sum_{k=k_x}^\infty \displaystyle\int_{1/((k+1)\pi)}^{1/(k\pi)}|\sin(\frac{1}t)|dt\right|+\left|\displaystyle\int_{1/(k_x\pi)}^x |\sin(1/t)|dt\right|)\leq \dfrac{1}x(\lim\limits_{n\to\infty} \dfrac{1}{k_x\pi} - \dfrac{1}{(n+1)\pi}+x-\dfrac{1}{k_x\pi})\leq 1,$ however, here I am stuck. Also, $\dfrac{H(x)}{x}$ is not monotone, so I think I should use a different approach. I know that for $t\in [\dfrac{1}{n\pi+\frac{3\pi}4}, \dfrac{1}{n\pi+\frac\pi4}], |\sin(\dfrac{1}t)| \geq \dfrac{1}2,$ but I am not sure if this is useful.","Is differentiable at ? I claim that is differentiable at Observe that so is odd. Also, It suffices to evaluate since if exists, it must equal which implies that So assume Since is bounded and continuous on where Now, observe that however, here I am stuck. Also, is not monotone, so I think I should use a different approach. I know that for but I am not sure if this is useful.","H(x) = \int_0^x \left\lvert\sin\left(\frac{1}t\right)\right\rvert\,\mathrm dt x = 0 H(x) x=0. \begin{align}H(-x) &= \displaystyle\int_0^{-x}|\sin(\frac{1}t)|dt=\displaystyle\int_0^x |\sin(-\frac{1}u)|(-1)du,\text{ where u = -t,}\\
&=-\displaystyle\int_0^x |\sin(\frac{1}t)|dt = -H(x),\end{align} H(x) H(0) = 0. \lim\limits_{x\to 0^+}\dfrac{H(x)}x, \lim\limits_{x\to 0^+} H(x) -\lim\limits_{x\to 0^-}H(x), \lim\limits_{x\to 0^+}\dfrac{H(x)}x = \lim\limits_{x\to 0^-}\dfrac{H(x)}x. x>0. |\sin(\frac{1}t)| (0, x], H(x) = \displaystyle\int_0^x |\sin(\frac{1}t)|dt = \lim\limits_{u\to 0^+}\displaystyle\int_u^x |\sin(\frac{1}t)|dt=\lim\limits_{n\to\infty}\displaystyle\int_{1/((n+1)\pi)}^{1/(k_x\pi)}|\sin(\frac{1}t)|dt + \displaystyle\int_{1/(k_x\pi)}^x |\sin(\frac{1}t)|dt \\
= \displaystyle\sum_{k=k_x}^\infty \displaystyle\int_{1/((k+1)\pi)}^{1/(k\pi)}|\sin(\frac{1}t)|dt+\displaystyle\int_{1/(k_x\pi)}^x|\sin(\frac{1}t)|dt, \frac{1}{k_x\pi} \leq x \leq \frac{1}{(k_x-1)\pi}\Rightarrow k_x\pi \geq \frac{1}{x} \geq (k_x - 1)\pi \Rightarrow k_x =  \lceil \frac{1}{x\pi} \rceil. 0 \leq |\dfrac{H(x)}x|\leq \dfrac{1}x \left|\displaystyle\sum_{k=k_x}^\infty \displaystyle\int_{1/((k+1)\pi)}^{1/(k\pi)}|\sin(\frac{1}t)|dt + \displaystyle\int_{1/(k_x\pi)}^x |\sin(1/t)|dt\right|\\
\leq \dfrac{1}x(\left|\displaystyle\sum_{k=k_x}^\infty \displaystyle\int_{1/((k+1)\pi)}^{1/(k\pi)}|\sin(\frac{1}t)|dt\right|+\left|\displaystyle\int_{1/(k_x\pi)}^x |\sin(1/t)|dt\right|)\leq \dfrac{1}x(\lim\limits_{n\to\infty} \dfrac{1}{k_x\pi} - \dfrac{1}{(n+1)\pi}+x-\dfrac{1}{k_x\pi})\leq 1, \dfrac{H(x)}{x} t\in [\dfrac{1}{n\pi+\frac{3\pi}4}, \dfrac{1}{n\pi+\frac\pi4}], |\sin(\dfrac{1}t)| \geq \dfrac{1}2,",[]
40,"Integral $\mathcal{P}\int_{-\infty}^{\infty}\frac{\tanh\left(\frac{1}{x^2}\right)}{x-k}\,dx$",Integral,"\mathcal{P}\int_{-\infty}^{\infty}\frac{\tanh\left(\frac{1}{x^2}\right)}{x-k}\,dx","I need help solving the integral $$\mathcal{I}(k)=\mathcal{P}\int_{-\infty}^{\infty}\frac{\tanh\left(\frac{1}{x^2}\right)}{x-k}\,dx$$ for $k>0$ . The $\mathcal{P}$ denotes the Cauchy principal value. Mathematica has been unhelpful, but numerical tests show that it converges. I would also be happy with a series or asymptotic solution. Edit: I attempt to solve for the asymptotic behaviour using Maxim's useful comment. Note that $$\mathcal{I}(k)\sim-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx$$ for large $k$ . Then $$\mathcal{I}(k)=-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx+\int_{-\infty}^{\infty}\frac{\tanh\left(\frac{1}{x^2}\right)}{x-k}\,dx+\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx$$ $$\mathcal{I}(k)=-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx+\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\left[\frac{1}{x-k}+\frac{1}{k}\right]\,dx$$ $$\mathcal{I}(k)=-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx+\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\left[\frac{x/k}{x/k-1}\right]\,dx$$ $$\mathcal{I}(k)=-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx-\frac{1}{k^2}\int_{-\infty}^{\infty}x\tanh\left(\frac{1}{x^2}\right)\left[\frac{1}{1-x/k}\right]\,dx$$ $$\mathcal{I}(k)\sim-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx-\frac{1}{k^2}\int_{-\infty}^{\infty}x\tanh\left(\frac{1}{x^2}\right)\sum_{j=0}^{\infty}\left(\frac{x}{k}\right)^j\,dx$$ $$\mathcal{I}(k)\sim-\sum_{j=0}^{\infty}\frac{\int_{-\infty}^{\infty}x^j\tanh\left(\frac{1}{x^2}\right)}{k^{j+1}}$$ $$\mathcal{I}(k)\sim-\sum_{j=0}^{\infty}\frac{\int_{-\infty}^{\infty}x^{2j}\tanh\left(\frac{1}{x^2}\right)}{k^{2j+1}}$$ $$\mathcal{I}(k)\sim-2\sum_{j=0}^{\infty}\frac{\int_{0}^{\infty}x^{2j}\tanh\left(\frac{1}{x^2}\right)}{k^{2j+1}}$$ Edit: This still doesn't work. My integrals diverge for $j>0$ . Perhaps useful: $$\tanh(x)=\sum_{j=0}^{\infty}2\frac{(-1)^{j}}{\pi^{2j+2}}\left(4^{j+1}-1\right)\zeta(2j+2)x^{2j+1}$$","I need help solving the integral for . The denotes the Cauchy principal value. Mathematica has been unhelpful, but numerical tests show that it converges. I would also be happy with a series or asymptotic solution. Edit: I attempt to solve for the asymptotic behaviour using Maxim's useful comment. Note that for large . Then Edit: This still doesn't work. My integrals diverge for . Perhaps useful:","\mathcal{I}(k)=\mathcal{P}\int_{-\infty}^{\infty}\frac{\tanh\left(\frac{1}{x^2}\right)}{x-k}\,dx k>0 \mathcal{P} \mathcal{I}(k)\sim-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx k \mathcal{I}(k)=-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx+\int_{-\infty}^{\infty}\frac{\tanh\left(\frac{1}{x^2}\right)}{x-k}\,dx+\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx \mathcal{I}(k)=-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx+\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\left[\frac{1}{x-k}+\frac{1}{k}\right]\,dx \mathcal{I}(k)=-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx+\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\left[\frac{x/k}{x/k-1}\right]\,dx \mathcal{I}(k)=-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx-\frac{1}{k^2}\int_{-\infty}^{\infty}x\tanh\left(\frac{1}{x^2}\right)\left[\frac{1}{1-x/k}\right]\,dx \mathcal{I}(k)\sim-\frac{1}{k}\int_{-\infty}^{\infty}\tanh\left(\frac{1}{x^2}\right)\,dx-\frac{1}{k^2}\int_{-\infty}^{\infty}x\tanh\left(\frac{1}{x^2}\right)\sum_{j=0}^{\infty}\left(\frac{x}{k}\right)^j\,dx \mathcal{I}(k)\sim-\sum_{j=0}^{\infty}\frac{\int_{-\infty}^{\infty}x^j\tanh\left(\frac{1}{x^2}\right)}{k^{j+1}} \mathcal{I}(k)\sim-\sum_{j=0}^{\infty}\frac{\int_{-\infty}^{\infty}x^{2j}\tanh\left(\frac{1}{x^2}\right)}{k^{2j+1}} \mathcal{I}(k)\sim-2\sum_{j=0}^{\infty}\frac{\int_{0}^{\infty}x^{2j}\tanh\left(\frac{1}{x^2}\right)}{k^{2j+1}} j>0 \tanh(x)=\sum_{j=0}^{\infty}2\frac{(-1)^{j}}{\pi^{2j+2}}\left(4^{j+1}-1\right)\zeta(2j+2)x^{2j+1}","['integration', 'definite-integrals', 'asymptotics', 'improper-integrals', 'contour-integration']"
41,Is there closed form for $\sum_{n=1}^\infty\frac{\overline{H}_nH_{n/2}}{n^3}\ ?$,Is there closed form for,\sum_{n=1}^\infty\frac{\overline{H}_nH_{n/2}}{n^3}\ ?,Is it possible to compute $$\sum_{n=1}^\infty\frac{\overline{H}_nH_{n/2}}{n^3}\ ?$$ where $\overline{H}_n=\sum_{k=1}^n\frac{(-1)^{k-1}}{k}$ is the alternating harmonic number and $H_n=\int_0^1\frac{1-x^n}{1-x}\ dx$ is the harmonic number. The reason I wrote the harmonic number in integral representation instead of series representation is due to the non-integer argument $n/2$ of the harmonic number and as we know $H_n=\sum_{k=1}^n\frac1k$ works for only integer $n$ . A similar version $\displaystyle\small\sum_{n=1}^\infty\frac{\overline{H}_nH_{n/2}}{n^2}$ was computed here,Is it possible to compute where is the alternating harmonic number and is the harmonic number. The reason I wrote the harmonic number in integral representation instead of series representation is due to the non-integer argument of the harmonic number and as we know works for only integer . A similar version was computed here,\sum_{n=1}^\infty\frac{\overline{H}_nH_{n/2}}{n^3}\ ? \overline{H}_n=\sum_{k=1}^n\frac{(-1)^{k-1}}{k} H_n=\int_0^1\frac{1-x^n}{1-x}\ dx n/2 H_n=\sum_{k=1}^n\frac1k n \displaystyle\small\sum_{n=1}^\infty\frac{\overline{H}_nH_{n/2}}{n^2},"['calculus', 'integration', 'sequences-and-series', 'closed-form', 'harmonic-numbers']"
42,Fourier series of elliptic F integral,Fourier series of elliptic F integral,,"Question. How to expand $$\mathbf{F}(\theta\mid k)=\int_0^\theta\frac{dt}{\sqrt{1-k^2\sin^2t}} \text{ (where $|k|<1$ and $|\theta|<\pi$)}$$ as a Fourier series w.r.t. $\theta$ ? I noticed that the question boils down to finding the Fourier expansion of $$f(\theta)=\frac1{\sqrt{1-k^2\sin^2\theta}}$$ as we can change the order of integration and differentiation. We can see clearly that $f$ is even, so $$\int_{-\pi}^\pi f(\theta)\sin n\theta d\theta=0.$$ Also, $$\int_{-\pi}^{\pi}f(\theta)\cos(2n+1)\theta d\theta=2\int_0^\pi f(\theta)\cos(2n+1)\theta d\theta\\=2\int_0^\pi f(\theta)\cos((2n+1)(\pi-\theta))d\theta=0$$ So the only hard part left is $$\int_0^{\pi/2}\frac{\cos 2n\theta}{\sqrt{1-k^2\sin^2\theta}}d\theta.$$ For $n=0$ I can clearly see it's $\mathbf{K}(k)$ , the elliptic K integral. Clearly there is a polynomial $P_n(x)$ s.t. $\cos 2nx=P_n(\sin^2x)$ with $\deg P_n=n$ , but note that this does not make the question boil down to evaluating $\displaystyle\int_0^{\pi/2}\frac{\sin^{2n}(t)}{\sqrt{1-k^2\sin^2 t}}dt$ because after evaluating this elliptic-like integral, which I ensure that it evaluates to a form of $a\mathbf{K}(k)+b\mathbf{E}(k)$ by partial fraction expansion, where $a,b\in\mathbb Q$ , there is a hard finite summation left involving the coefficients of $P$ . When $n=1$ , $a_n$ equals $2\mathbf{E}/k^2+(1-2/k^2)\mathbf{K}$ . I don't have any further thoughts. If we cannot find the general term with $k$ varying, can we at least find the coefficients when $k^2=-1$ ?","Question. How to expand as a Fourier series w.r.t. ? I noticed that the question boils down to finding the Fourier expansion of as we can change the order of integration and differentiation. We can see clearly that is even, so Also, So the only hard part left is For I can clearly see it's , the elliptic K integral. Clearly there is a polynomial s.t. with , but note that this does not make the question boil down to evaluating because after evaluating this elliptic-like integral, which I ensure that it evaluates to a form of by partial fraction expansion, where , there is a hard finite summation left involving the coefficients of . When , equals . I don't have any further thoughts. If we cannot find the general term with varying, can we at least find the coefficients when ?","\mathbf{F}(\theta\mid k)=\int_0^\theta\frac{dt}{\sqrt{1-k^2\sin^2t}} \text{ (where |k|<1 and |\theta|<\pi)} \theta f(\theta)=\frac1{\sqrt{1-k^2\sin^2\theta}} f \int_{-\pi}^\pi f(\theta)\sin n\theta d\theta=0. \int_{-\pi}^{\pi}f(\theta)\cos(2n+1)\theta d\theta=2\int_0^\pi f(\theta)\cos(2n+1)\theta d\theta\\=2\int_0^\pi f(\theta)\cos((2n+1)(\pi-\theta))d\theta=0 \int_0^{\pi/2}\frac{\cos 2n\theta}{\sqrt{1-k^2\sin^2\theta}}d\theta. n=0 \mathbf{K}(k) P_n(x) \cos 2nx=P_n(\sin^2x) \deg P_n=n \displaystyle\int_0^{\pi/2}\frac{\sin^{2n}(t)}{\sqrt{1-k^2\sin^2 t}}dt a\mathbf{K}(k)+b\mathbf{E}(k) a,b\in\mathbb Q P n=1 a_n 2\mathbf{E}/k^2+(1-2/k^2)\mathbf{K} k k^2=-1","['integration', 'definite-integrals', 'fourier-series', 'elliptic-integrals']"
43,integrating $\int_{x_0}^{x}\arccos(a-\cos(x'))dx'$,integrating,\int_{x_0}^{x}\arccos(a-\cos(x'))dx',"I've been looking at an integral of the form $$\int_{x_0}^{x}\arccos(a-\cos(x'))dx'.$$ We can set $x_0$ as to give no contribution from the lower limit. I've tried to set $\arccos(x)=\pi/2-\arcsin(x)$ , to simplify it. I achieve the form $$\int\frac{v\cos(v)}{\sqrt{1-(a-\sin(v))^2}}dv,$$ by setting $\sin(v)=a-\cos(x)$ , I have also ignored the limits to focus on the integrand. I have looked at the table of integrals and series to see if there was a solution for this integral, but no luck. Does anybody possibly know a technique for solving it?","I've been looking at an integral of the form We can set as to give no contribution from the lower limit. I've tried to set , to simplify it. I achieve the form by setting , I have also ignored the limits to focus on the integrand. I have looked at the table of integrals and series to see if there was a solution for this integral, but no luck. Does anybody possibly know a technique for solving it?","\int_{x_0}^{x}\arccos(a-\cos(x'))dx'. x_0 \arccos(x)=\pi/2-\arcsin(x) \int\frac{v\cos(v)}{\sqrt{1-(a-\sin(v))^2}}dv, \sin(v)=a-\cos(x)","['integration', 'elliptic-integrals']"
44,Check my work: General solution of a PDE,Check my work: General solution of a PDE,,"I have been asked to find the ""Most general solution"" for $u(x,y)$ of the PDE $$\frac{\partial u}{\partial x} = \frac{x}{\sqrt{x^2+y^2}} + 3y\cos(3xy) + 3x^2y^2$$ I know you must take the integral of both sides to ""undo"" the partial derivative. Is it reasonable to then split the right hand side into three separate integrals as below? $$u(x, y) = \int\frac{x}{\sqrt{x^2+y^2}}dx + \int3y\cos(3xy)dx + \int3x^2y^2dx.$$ From here I got: $$\int\frac{x}{\sqrt{x^2+y^2}}dx = \frac{1}{2}\int\frac{2x}{\sqrt{x^2+y^2}}dx = \sqrt{x^2+y^2} + C(y)$$ $$\int3y\cos(3xy)dx = \sin(3xy) + C(y)$$ $$\int3x^2y^2dx = y^2x^3+C(y)$$ The final answer is: $$u(x, y) = \sqrt{x^2+y^2} + \sin(3xy) + y^2x^3+C(y)$$ Is this correct?","I have been asked to find the ""Most general solution"" for of the PDE I know you must take the integral of both sides to ""undo"" the partial derivative. Is it reasonable to then split the right hand side into three separate integrals as below? From here I got: The final answer is: Is this correct?","u(x,y) \frac{\partial u}{\partial x} = \frac{x}{\sqrt{x^2+y^2}} + 3y\cos(3xy) + 3x^2y^2 u(x, y) = \int\frac{x}{\sqrt{x^2+y^2}}dx + \int3y\cos(3xy)dx + \int3x^2y^2dx. \int\frac{x}{\sqrt{x^2+y^2}}dx = \frac{1}{2}\int\frac{2x}{\sqrt{x^2+y^2}}dx = \sqrt{x^2+y^2} + C(y) \int3y\cos(3xy)dx = \sin(3xy) + C(y) \int3x^2y^2dx = y^2x^3+C(y) u(x, y) = \sqrt{x^2+y^2} + \sin(3xy) + y^2x^3+C(y)","['integration', 'partial-differential-equations', 'partial-derivative', 'indefinite-integrals']"
45,"How do you solve this $\int\limits ^{\infty }_{0}\frac{\cos( x)}{x^{n} +1} dx,\ n >0$",How do you solve this,"\int\limits ^{\infty }_{0}\frac{\cos( x)}{x^{n} +1} dx,\ n >0","Me and my friend have tried a wedge,a triangle, and we even tried Feynman's technique. None of these things got us an answer to the integral $\int\limits ^{\infty }_{0}\frac{\cos( x)}{x^{n} +1} dx,\ n >0$ Can someone show the process of solving this?","Me and my friend have tried a wedge,a triangle, and we even tried Feynman's technique. None of these things got us an answer to the integral Can someone show the process of solving this?","\int\limits ^{\infty }_{0}\frac{\cos( x)}{x^{n} +1} dx,\ n >0","['integration', 'definite-integrals', 'contour-integration']"
46,Integration and a function increasing,Integration and a function increasing,,"Let $g_{1},g_{2}\in\mathcal{R}([a,b])$ (Riemann-integrable) such that $$\int_{a}^{x}{g_{2}(t)dt}\leq\int_{a}^{x}{g_{1}(t)dt}\phantom{a}\text{for each}\phantom{a}x\in [a,b]$$ and $$\int_{a}^{b}{g_{1}(t)dt}=\int_{a}^{b}{g_{2}(t)dt}$$ Show that if $f: [a,b]\rightarrow{\mathbb{R}}$ is increasing, then $$\int_{a}^{b}{f(t)g_{1}(t)dt}\leq\int_{a}^{b}{f(t)g_{2}(t)dt}$$ Anyone have a suggestion?","Let (Riemann-integrable) such that and Show that if is increasing, then Anyone have a suggestion?","g_{1},g_{2}\in\mathcal{R}([a,b]) \int_{a}^{x}{g_{2}(t)dt}\leq\int_{a}^{x}{g_{1}(t)dt}\phantom{a}\text{for each}\phantom{a}x\in [a,b] \int_{a}^{b}{g_{1}(t)dt}=\int_{a}^{b}{g_{2}(t)dt} f: [a,b]\rightarrow{\mathbb{R}} \int_{a}^{b}{f(t)g_{1}(t)dt}\leq\int_{a}^{b}{f(t)g_{2}(t)dt}","['real-analysis', 'integration', 'analysis', 'definite-integrals', 'riemann-integration']"
47,Find out the value of the integral $\int_{-2}^{2} \lfloor x^2-1\rfloor dx$,Find out the value of the integral,\int_{-2}^{2} \lfloor x^2-1\rfloor dx,"Find out the value of the integral $$\int_{-2}^{2} \lfloor x^2-1\rfloor dx$$ where $[x]$ denotes the floor function (i.e., $[x]$ is the greatest integer $\le x$ .) My attempt ..... $$\int_{-2}^2 \lfloor x^2 – 1\rfloor dx = 2\int_0^2 \lfloor x^2-1\rfloor dx$$ Because $\lfloor x^2 – 1\rfloor$ is even. $$2\int_0^2 \lfloor x^2-1\rfloor dx =\\ 2\int_0^1 \lfloor x^2-1\rfloor dx+2\int_1^{\sqrt{2}} \lfloor x^2-1\rfloor dx +2\int_{\sqrt{2}}^{\sqrt{3}} \lfloor x^2-1\rfloor dx + 2\int_{\sqrt{3}}^2 \lfloor x^2-1\rfloor dx$$ But how to evaluate this or am I wrong in the whole assumption?","Find out the value of the integral where denotes the floor function (i.e., is the greatest integer .) My attempt ..... Because is even. But how to evaluate this or am I wrong in the whole assumption?",\int_{-2}^{2} \lfloor x^2-1\rfloor dx [x] [x] \le x \int_{-2}^2 \lfloor x^2 – 1\rfloor dx = 2\int_0^2 \lfloor x^2-1\rfloor dx \lfloor x^2 – 1\rfloor 2\int_0^2 \lfloor x^2-1\rfloor dx =\\ 2\int_0^1 \lfloor x^2-1\rfloor dx+2\int_1^{\sqrt{2}} \lfloor x^2-1\rfloor dx +2\int_{\sqrt{2}}^{\sqrt{3}} \lfloor x^2-1\rfloor dx + 2\int_{\sqrt{3}}^2 \lfloor x^2-1\rfloor dx,"['integration', 'definite-integrals', 'ceiling-and-floor-functions']"
48,Calculating probability using joint density,Calculating probability using joint density,,"I'm given the family $(X, Y)$ of random variables with join density: $$f^{X, Y}(x, y) := 2 \cdot e^{-(x+2y)} \cdot 1_{[0,\infty)}(x) \cdot 1_{[0, \infty)}(y)$$ and I'm required to calculate $P(X > Y)$ . I already calculated the marginal density $f^X, f^Y$ and established that $X$ and $Y$ are independent, but didn't use this information, but calculated: $$\int_{\{(x, y)\,\in\,\mathbb{R}^2 : x\,>\,y\}} f^{X, Y}(x,y) \,d(x,y) \\ = \int_\mathbb{R}\int_\mathbb{R} 2\cdot e^{-(x+2y)} \cdot 1_{[0, \infty)}(x) \cdot 1_{[0, x)}(y) \,dy\,dx \\= \int_\mathbb{R} 1_{[0, \infty)}(x) \int_0^x 2e^{-(x+2y)} dy\,dx \\= \int_0^\infty e^{-x} - e^{-3x}dx = 1 - \frac{1}{3} = \frac{2}{3}$$ Is this approach correct? Do I need to change $x$ in the second equality (since it's part of the bounds) or is it fine, since I integrated $dy$ ? Can this problem be solves quicker using $f^X, f^Y$ and independence?","I'm given the family of random variables with join density: and I'm required to calculate . I already calculated the marginal density and established that and are independent, but didn't use this information, but calculated: Is this approach correct? Do I need to change in the second equality (since it's part of the bounds) or is it fine, since I integrated ? Can this problem be solves quicker using and independence?","(X, Y) f^{X, Y}(x, y) := 2 \cdot e^{-(x+2y)} \cdot 1_{[0,\infty)}(x) \cdot 1_{[0, \infty)}(y) P(X > Y) f^X, f^Y X Y \int_{\{(x, y)\,\in\,\mathbb{R}^2 : x\,>\,y\}} f^{X, Y}(x,y) \,d(x,y) \\ = \int_\mathbb{R}\int_\mathbb{R} 2\cdot e^{-(x+2y)} \cdot 1_{[0, \infty)}(x) \cdot 1_{[0, x)}(y) \,dy\,dx \\= \int_\mathbb{R} 1_{[0, \infty)}(x) \int_0^x 2e^{-(x+2y)} dy\,dx \\= \int_0^\infty e^{-x} - e^{-3x}dx = 1 - \frac{1}{3} = \frac{2}{3} x dy f^X, f^Y","['integration', 'probability-theory', 'density-function']"
49,Definite Integration ( a little query),Definite Integration ( a little query),,"$$\int_0^π \frac{xdx}{a^2\cos^2x+b^2\sin^2x} \,dx$$ Using property $$\int_a^b f(x) \,dx= \int_a^b f(a+b-x) \,dx$$ (i can't write it correctly,please check it) I get, $2I=\pi\int_0^\pi \frac{dx}{a^2\cos^2x+b^2\sin^2x} \,dx$ On dividing numerator and denominator of R.H.S by $\cos^2x$ I get, $2I=\pi\int_0^\pi \frac{\sec^2xdx}{a^2+b^2\tan^2x} \,dx$ Now, solving by substitution method  (taking $b\tan x=t$ ) I get (i have added the image because i was not able to type this correctly) As the upper limit and lower limit on the  function are zero So, answer should be zero. But in the solution ( after getting this $2I=\pi\int_0^\pi \frac{dx}{a^2\cos^2x+b^2\sin^2x} \,dx$ )they have used the property $$\int_0^2a f(x) \,dx= 2\left(\int_0^a f(x) \,dx\right)$$ Why they didn't ended the solution in the direction in which i did pardon for my mathjax errors","Using property (i can't write it correctly,please check it) I get, On dividing numerator and denominator of R.H.S by I get, Now, solving by substitution method  (taking ) I get (i have added the image because i was not able to type this correctly) As the upper limit and lower limit on the  function are zero So, answer should be zero. But in the solution ( after getting this )they have used the property Why they didn't ended the solution in the direction in which i did pardon for my mathjax errors","\int_0^π \frac{xdx}{a^2\cos^2x+b^2\sin^2x} \,dx \int_a^b f(x) \,dx= \int_a^b f(a+b-x) \,dx 2I=\pi\int_0^\pi \frac{dx}{a^2\cos^2x+b^2\sin^2x} \,dx \cos^2x 2I=\pi\int_0^\pi \frac{\sec^2xdx}{a^2+b^2\tan^2x} \,dx b\tan x=t 2I=\pi\int_0^\pi \frac{dx}{a^2\cos^2x+b^2\sin^2x} \,dx \int_0^2a f(x) \,dx= 2\left(\int_0^a f(x) \,dx\right)","['calculus', 'integration', 'definite-integrals']"
50,Average of a function that comes out of a complicated DE,Average of a function that comes out of a complicated DE,,"I've to find (the average of a function over a particular interval, where $t_1>0$ , $t_2>0$ and $t_2>t_1$ ): $$\frac{1}{t_2-t_1}\int_{t_1}^{t_2}x(t)dt\tag1$$ Where $x(t)$ is the solution to the following DE (with intial condition $x(0)=x_0)$ : $$x(t)\cdot r+x'(t)\cdot l+a\cdot\ln\left(1+\frac{x(t)}{b}\right)=0\space\Longleftrightarrow\space x(t)=\dots\tag2$$ Now, according to the answer of @JJacquelin on my previuous question , I could write $x(t)$ as follows: $$t=-l\int_{x_0}^{x(t)}\frac{d\xi}{r\xi+a\ln\left(1+\frac{\xi}{b}\right)}\tag3$$ But I do not see how that can help me find $(1)$ ?!","I've to find (the average of a function over a particular interval, where , and ): Where is the solution to the following DE (with intial condition : Now, according to the answer of @JJacquelin on my previuous question , I could write as follows: But I do not see how that can help me find ?!",t_1>0 t_2>0 t_2>t_1 \frac{1}{t_2-t_1}\int_{t_1}^{t_2}x(t)dt\tag1 x(t) x(0)=x_0) x(t)\cdot r+x'(t)\cdot l+a\cdot\ln\left(1+\frac{x(t)}{b}\right)=0\space\Longleftrightarrow\space x(t)=\dots\tag2 x(t) t=-l\int_{x_0}^{x(t)}\frac{d\xi}{r\xi+a\ln\left(1+\frac{\xi}{b}\right)}\tag3 (1),"['integration', 'ordinary-differential-equations', 'definite-integrals', 'physics', 'average']"
51,"Contour for $\int_0^\infty \arctan(z) e^{-z^2}\,dz$ or some variant",Contour for  or some variant,"\int_0^\infty \arctan(z) e^{-z^2}\,dz","I'm trying to practice my contour integration skills and got interested in the following integral: $$\int_0^\infty \arctan(z) e^{-z^2}\,dz$$ I know that the usual way to calculate integrals on $[0,\infty)$ is to use the keyhole-contour, but the problem in this case is that $\arctan(z)$ has branch points at $z=\pm i$ , with branch cuts usually chosen on $[i,i\infty)$ and $[-i,-i\infty)$ , which I think means that the keyhole contour doesn't work in this case. I have tried a rectangle contour made up of $C:[0,R]\cup [R+i/2] \cup [R+i/2,i/2] \cup [i/2,0]$ but that didn't work out. Because my function is holomorphic on $\mathbb{C}\setminus [i,i\infty) \cap [-i,-i\infty)$ there also wouldn't be any residues to calculate, which doesn't necessarily have to be a problem as Cauchy's theorem could be used to try and compute the integral. If I'm not mistaken, I would then get somerhing like $\int_0^\infty f(x)-f(x+i/2)\,dx=0$ . One difficulty I ran into is simplify $f(z+i/2)$ (where $f(z)=\arctan(x) e^{-z^2}$ ) into some other form such as $\alpha f(z)+\beta g(z)$ for some $g(z)$ whose integral can be calculated on $[0,\infty)$ and $\alpha, \beta\in\mathbb{C}$ . This would let me then solve for $\int_0^\infty f(x)\,dx$ . I also thought that, if that makes it easier, we could extend the range of integration to $\mathbb{R}$ , as long as we found an odd function $q(a,x)$ , such that $q(0,x)=1$ and compute $$\lim_{a\to 0}  \frac{1}{2} \int_{-\infty}^\infty  q(a, x) \arctan(x)e^{-x^2}\,dx$$ We could either use Cauchy's/the Residue theorem depending on whether $q(z)$ has poles or not. I have not been able to use this approach. Any ideas?","I'm trying to practice my contour integration skills and got interested in the following integral: I know that the usual way to calculate integrals on is to use the keyhole-contour, but the problem in this case is that has branch points at , with branch cuts usually chosen on and , which I think means that the keyhole contour doesn't work in this case. I have tried a rectangle contour made up of but that didn't work out. Because my function is holomorphic on there also wouldn't be any residues to calculate, which doesn't necessarily have to be a problem as Cauchy's theorem could be used to try and compute the integral. If I'm not mistaken, I would then get somerhing like . One difficulty I ran into is simplify (where ) into some other form such as for some whose integral can be calculated on and . This would let me then solve for . I also thought that, if that makes it easier, we could extend the range of integration to , as long as we found an odd function , such that and compute We could either use Cauchy's/the Residue theorem depending on whether has poles or not. I have not been able to use this approach. Any ideas?","\int_0^\infty \arctan(z) e^{-z^2}\,dz [0,\infty) \arctan(z) z=\pm i [i,i\infty) [-i,-i\infty) C:[0,R]\cup [R+i/2] \cup [R+i/2,i/2] \cup [i/2,0] \mathbb{C}\setminus [i,i\infty) \cap [-i,-i\infty) \int_0^\infty f(x)-f(x+i/2)\,dx=0 f(z+i/2) f(z)=\arctan(x) e^{-z^2} \alpha f(z)+\beta g(z) g(z) [0,\infty) \alpha, \beta\in\mathbb{C} \int_0^\infty f(x)\,dx \mathbb{R} q(a,x) q(0,x)=1 \lim_{a\to 0}  \frac{1}{2} \int_{-\infty}^\infty  q(a, x) \arctan(x)e^{-x^2}\,dx q(z)","['integration', 'complex-analysis', 'definite-integrals', 'contour-integration', 'complex-integration']"
52,An Integral Designed to be on the Very Cusp of Convergence,An Integral Designed to be on the Very Cusp of Convergence,,"Say we have an integer $n$ ∊ℕ₀ & a sequence of $n+1$ real numbers $\alpha_k\in[0,\infty)∀k$ , where $k=0\dots n$ , and using $\log^{[k]}$ to denote $k$ functionings of the logarithm ( $\log^{[0]}x\equiv x$ , $\log^{[1]}x\equiv \log x$ , $\log^{[2]}x\equiv \log\log x$ , etc), I would conjecture that, $∀n, $ the integral $$\int_{e\uparrow\uparrow n}^\infty{dx\over\prod_{k=0}^n(\log^{[k]}x)^{\alpha_k}}$$ diverges , when $\alpha_k=1∀k\leq n$ , or when with ascending $k$ the first $\alpha_k≠1$ is $<1$ ; and converges when with ascending $k$ the first $\alpha_k≠1$ is $>1$ . In the integral given the lower limit is chosen simply to keep the function in the denominator well clear of taking any argument that would result in a negative value being fed into the logarithm - the convergence|divergence of the integral is determined purely by the behaviour of the integrand as its argument $\to\infty$ . I am wondering whether this surmise is correct. My reasoning for supposing it is is that if the variable $y$ be substituted for $\log^{[n]}x$ , then in the denominator we shall have successive orders of functioning of the exponential of $y$ from right to left ... but each raised to the power of its index $\alpha_k$ in order from left to right; and in the numerator we shall have the same product of the same factors, by reason of the chain rule, but each with unit exponent. So that considering the factors from left to right, the first one that does not completely cancel will be the first one at which $\alpha_k$ differs from unity; and also the one with the highest order of application of the exponential function: and if that $a_k$ is $<1$ the remnant will be in the numerator, and if $>1$ , in the denominator. And the integral will diverge in the former case & converge in the latter, as subsequent remnants will be completely overruled, regardless of the size of their exponent, as an exponential of a variable always overrules a mere power of a variable, regardless of the relative sizes of the scaling of the exponential and the degree of the power ... and the comparison will be at least that . Finally, in the case of all the $\alpha_k$ till the last being $=1$ , there will be complete cancellation of the exponentials; and we shall be left with $$\int_{e\uparrow\uparrow n}^\infty{dy\over y^{\alpha_n}} ,$$ the convergence|divergence of which depends on $\alpha_n$ in the well-familiar way. I would also surmise that this theorem - if it indeed is one (and the question here is essentially whether it is one, and not merely a surmise, or incorrectly infererred) - translates into sum over integers. I'll refrain from fully explicating the logic of that surmise; but basically it's that if the correspondence between Σ & ∫ of $1/x$ holds by reason of the asymptotically-flat -ness of the logarithm, then it could reasonably be expected to hold when functions that are progressively yet asymptotically-flatter are factored-in.","Say we have an integer ∊ℕ₀ & a sequence of real numbers , where , and using to denote functionings of the logarithm ( , , , etc), I would conjecture that, the integral diverges , when , or when with ascending the first is ; and converges when with ascending the first is . In the integral given the lower limit is chosen simply to keep the function in the denominator well clear of taking any argument that would result in a negative value being fed into the logarithm - the convergence|divergence of the integral is determined purely by the behaviour of the integrand as its argument . I am wondering whether this surmise is correct. My reasoning for supposing it is is that if the variable be substituted for , then in the denominator we shall have successive orders of functioning of the exponential of from right to left ... but each raised to the power of its index in order from left to right; and in the numerator we shall have the same product of the same factors, by reason of the chain rule, but each with unit exponent. So that considering the factors from left to right, the first one that does not completely cancel will be the first one at which differs from unity; and also the one with the highest order of application of the exponential function: and if that is the remnant will be in the numerator, and if , in the denominator. And the integral will diverge in the former case & converge in the latter, as subsequent remnants will be completely overruled, regardless of the size of their exponent, as an exponential of a variable always overrules a mere power of a variable, regardless of the relative sizes of the scaling of the exponential and the degree of the power ... and the comparison will be at least that . Finally, in the case of all the till the last being , there will be complete cancellation of the exponentials; and we shall be left with the convergence|divergence of which depends on in the well-familiar way. I would also surmise that this theorem - if it indeed is one (and the question here is essentially whether it is one, and not merely a surmise, or incorrectly infererred) - translates into sum over integers. I'll refrain from fully explicating the logic of that surmise; but basically it's that if the correspondence between Σ & ∫ of holds by reason of the asymptotically-flat -ness of the logarithm, then it could reasonably be expected to hold when functions that are progressively yet asymptotically-flatter are factored-in.","n n+1 \alpha_k\in[0,\infty)∀k k=0\dots n \log^{[k]} k \log^{[0]}x\equiv x \log^{[1]}x\equiv \log x \log^{[2]}x\equiv \log\log x ∀n,  \int_{e\uparrow\uparrow n}^\infty{dx\over\prod_{k=0}^n(\log^{[k]}x)^{\alpha_k}} \alpha_k=1∀k\leq n k \alpha_k≠1 <1 k \alpha_k≠1 >1 \to\infty y \log^{[n]}x y \alpha_k \alpha_k a_k <1 >1 \alpha_k =1 \int_{e\uparrow\uparrow n}^\infty{dy\over y^{\alpha_n}} , \alpha_n 1/x","['integration', 'limits', 'tetration']"
53,Facing difficulty in working $\int_{0}^{1}\frac{\arctan\left(\frac{ax}{1-x}\right)}{\sqrt{1-x}}\frac{dx}{x^{3/2}}$,Facing difficulty in working,\int_{0}^{1}\frac{\arctan\left(\frac{ax}{1-x}\right)}{\sqrt{1-x}}\frac{dx}{x^{3/2}},"I would like to evaluate this integral,$$\int_{0}^{1}\frac{\arctan\left(\frac{ax}{1-x}\right)}{\sqrt{1-x}}\frac{dx}{x^{3/2}}\tag1$$ This is the approach I will take: We can begin with a sub: $y=\sqrt{x}$, $dx=2\sqrt{x}dy$ $$-2\int_{0}^{1}\frac{\arctan\left(\frac{ay^2}{y^2-1}\right)}{\sqrt{1-y^2}}\frac{dy}{y^2}\tag2$$ Not sure, but we can try integration by parts: $u=\arctan\left(\frac{ay^2}{y^2-1}\right)$, $$du=\frac{2ay}{y^2-1}-\frac{2ay^3}{(y^2-1)^2}\times \frac{1}{\frac{a^2y^4}{(y^2-1)^2}+1}dy$$ $dv=\frac{1}{y^2\sqrt{1-y^2}}dy$, $$v=-\frac{\sqrt{1-y^2}}{y}$$ $$2a\int_{0}^{1}\frac{(1-y^2)^{3/2}}{(y^2-1)+a^2y^4}+2a\int_{0}^{1}\frac{y^2(1-y^2)^{1/2}}{(y^2-1)^2+a^2y^4}dy=2a\left(I+J\right)\tag3$$ Integral I: Making another sub: $y=\sin(u)$, $u=\arcsin(y)$, $dy=\cos(u) du$ It is too much to write everything down, finally got to: $$I=\int_{0}^{\pi/2}\frac{cos^4(u)}{a^2\sin^4(u)+\cos^4(u)}du$$ Using trig identities we can rewrite $$I=\int_{0}^{\pi/2}\sec^2(u)\frac{du}{(1+\tan^2(u))(1+a^2\tan^4(u))}$$ Make another sub: $s=\tan(u)$, $du=\frac{1}{\sec^2(s)}ds$ $$I=\int_{0}^{\infty}\frac{ds}{(1+s^2)(1+a^2s^4)}$$ Using partial fraction decomp: $$I=\frac{\pi}{2(a^2+1)}-\frac{a^2}{a^2+1}\int_{0}^{\infty}\frac{s^2-1}{a^2s^4+1}ds$$ Integral J: Making another sub: $y=\sin(v)$, $v=\arcsin(y)$, $dy=\cos(v) dv$ $$J=\int_{0}^{\pi/2}\frac{\cos^2(v)\sin^2(v)}{(a^2+1)\sin^4(v)-2\sin^2(v)+1}$$ Using trig identities to rewrite $$J=\int_{0}^{\pi/2}\sec^2(v)\cdot \frac{\tan^2(v)}{(1+\tan^2(v))(a^2\tan^4(v)+1)}$$ Make another sub: $t=\tan(v)$, $dv=\frac{1}{\sec^2(v)}dt$ $$J=\int_{0}^{\infty}\frac{t^2}{(1+t^2)(1+a^2t^4)}dt$$ Using partial fraction decomp: $$J=\frac{1}{1+a^2}\color{red}{\int_{0}^{\infty}\frac{a^2t^2+1}{a^2t^4+1}dt}-\frac{\pi}{2(1+a^2)}$$ The red integral it is definitely way out of my reach! $$\int_{0}^{\infty}\frac{a^2t^2+1}{a^2t^4+1}dt=\int_{0}^{\infty}\frac{t^2}{t^4+a^{-2}}dt+\int_{0}^{\infty}\frac{dt}{a^2t^4+1}$$ The above approach seem to be not helping in evaluating the question. After simplification I got to: $$2a(I+J)=\int_{0}^{1}\frac{\arctan\left(\frac{ax}{1-x}\right)}{\sqrt{1-x}}\frac{dx}{x^{3/2}} =\frac{1}{1+a^2}\int_{0}^{\infty}\frac{a^2+1}{a^2t^4+1}dt$$ If my work so far it is correct, then I am shruggle in solving this integral $$\int_{0}^{\infty}\frac{1}{a^2t^4+1}dt=\int_{0}^{\infty}\frac{dt}{(at^2-i)(at^2+i)}dt$$","I would like to evaluate this integral,$$\int_{0}^{1}\frac{\arctan\left(\frac{ax}{1-x}\right)}{\sqrt{1-x}}\frac{dx}{x^{3/2}}\tag1$$ This is the approach I will take: We can begin with a sub: $y=\sqrt{x}$, $dx=2\sqrt{x}dy$ $$-2\int_{0}^{1}\frac{\arctan\left(\frac{ay^2}{y^2-1}\right)}{\sqrt{1-y^2}}\frac{dy}{y^2}\tag2$$ Not sure, but we can try integration by parts: $u=\arctan\left(\frac{ay^2}{y^2-1}\right)$, $$du=\frac{2ay}{y^2-1}-\frac{2ay^3}{(y^2-1)^2}\times \frac{1}{\frac{a^2y^4}{(y^2-1)^2}+1}dy$$ $dv=\frac{1}{y^2\sqrt{1-y^2}}dy$, $$v=-\frac{\sqrt{1-y^2}}{y}$$ $$2a\int_{0}^{1}\frac{(1-y^2)^{3/2}}{(y^2-1)+a^2y^4}+2a\int_{0}^{1}\frac{y^2(1-y^2)^{1/2}}{(y^2-1)^2+a^2y^4}dy=2a\left(I+J\right)\tag3$$ Integral I: Making another sub: $y=\sin(u)$, $u=\arcsin(y)$, $dy=\cos(u) du$ It is too much to write everything down, finally got to: $$I=\int_{0}^{\pi/2}\frac{cos^4(u)}{a^2\sin^4(u)+\cos^4(u)}du$$ Using trig identities we can rewrite $$I=\int_{0}^{\pi/2}\sec^2(u)\frac{du}{(1+\tan^2(u))(1+a^2\tan^4(u))}$$ Make another sub: $s=\tan(u)$, $du=\frac{1}{\sec^2(s)}ds$ $$I=\int_{0}^{\infty}\frac{ds}{(1+s^2)(1+a^2s^4)}$$ Using partial fraction decomp: $$I=\frac{\pi}{2(a^2+1)}-\frac{a^2}{a^2+1}\int_{0}^{\infty}\frac{s^2-1}{a^2s^4+1}ds$$ Integral J: Making another sub: $y=\sin(v)$, $v=\arcsin(y)$, $dy=\cos(v) dv$ $$J=\int_{0}^{\pi/2}\frac{\cos^2(v)\sin^2(v)}{(a^2+1)\sin^4(v)-2\sin^2(v)+1}$$ Using trig identities to rewrite $$J=\int_{0}^{\pi/2}\sec^2(v)\cdot \frac{\tan^2(v)}{(1+\tan^2(v))(a^2\tan^4(v)+1)}$$ Make another sub: $t=\tan(v)$, $dv=\frac{1}{\sec^2(v)}dt$ $$J=\int_{0}^{\infty}\frac{t^2}{(1+t^2)(1+a^2t^4)}dt$$ Using partial fraction decomp: $$J=\frac{1}{1+a^2}\color{red}{\int_{0}^{\infty}\frac{a^2t^2+1}{a^2t^4+1}dt}-\frac{\pi}{2(1+a^2)}$$ The red integral it is definitely way out of my reach! $$\int_{0}^{\infty}\frac{a^2t^2+1}{a^2t^4+1}dt=\int_{0}^{\infty}\frac{t^2}{t^4+a^{-2}}dt+\int_{0}^{\infty}\frac{dt}{a^2t^4+1}$$ The above approach seem to be not helping in evaluating the question. After simplification I got to: $$2a(I+J)=\int_{0}^{1}\frac{\arctan\left(\frac{ax}{1-x}\right)}{\sqrt{1-x}}\frac{dx}{x^{3/2}} =\frac{1}{1+a^2}\int_{0}^{\infty}\frac{a^2+1}{a^2t^4+1}dt$$ If my work so far it is correct, then I am shruggle in solving this integral $$\int_{0}^{\infty}\frac{1}{a^2t^4+1}dt=\int_{0}^{\infty}\frac{dt}{(at^2-i)(at^2+i)}dt$$",,['calculus']
54,"Solving : $\frac{\mathrm{d}x}{y-z} = \frac{\mathrm{d}y}{z-x} = \frac{\mathrm{d}z}{x-y}$ over $\mathbf{V} = (y-z,z-x,x-y)$",Solving :  over,"\frac{\mathrm{d}x}{y-z} = \frac{\mathrm{d}y}{z-x} = \frac{\mathrm{d}z}{x-y} \mathbf{V} = (y-z,z-x,x-y)","I can't seem how to proceed with finding two curves $u_1$ and $u_2$ by solving the integral problem  $$\frac{\mathrm{d}x}{y-z} = \frac{\mathrm{d}y}{z-x} = \frac{\mathrm{d}z}{x-y}$$ over the vector field $\mathbf{V} = (y-z,z-x,x-y)$. This is a part of proving that an integral surface is contained within $\mathbf{V}$. I would really appreciate any help given. Also Wolfram Alpha doesn't yield a solution for the problem : $$(y-z)u_x + (z-x)u_y + (x-y)u_z = 0$$","I can't seem how to proceed with finding two curves $u_1$ and $u_2$ by solving the integral problem  $$\frac{\mathrm{d}x}{y-z} = \frac{\mathrm{d}y}{z-x} = \frac{\mathrm{d}z}{x-y}$$ over the vector field $\mathbf{V} = (y-z,z-x,x-y)$. This is a part of proving that an integral surface is contained within $\mathbf{V}$. I would really appreciate any help given. Also Wolfram Alpha doesn't yield a solution for the problem : $$(y-z)u_x + (z-x)u_y + (x-y)u_z = 0$$",,"['integration', 'ordinary-differential-equations', 'partial-differential-equations']"
55,Integrate $\sin(\cos x)\text{d}x$,Integrate,\sin(\cos x)\text{d}x,"Soo.. a question appeared in my exam in which I had to compare the values of the following integrals $$J=\int_0^{\pi/2} \sin(\cos x)\,\mathrm dx\\I=\int_0^{\pi/2} \cos(\sin x)\,\mathrm dx \\K=\int_0^{\pi/2} \cos x\,\mathrm dx$$ I was able solve it using an indirect method but it left me wondering whether I could find the exact values of the three integrals. PS: I have linked a picture of the problem and its solution for your reference.","Soo.. a question appeared in my exam in which I had to compare the values of the following integrals $$J=\int_0^{\pi/2} \sin(\cos x)\,\mathrm dx\\I=\int_0^{\pi/2} \cos(\sin x)\,\mathrm dx \\K=\int_0^{\pi/2} \cos x\,\mathrm dx$$ I was able solve it using an indirect method but it left me wondering whether I could find the exact values of the three integrals. PS: I have linked a picture of the problem and its solution for your reference.",,"['integration', 'trigonometry', 'definite-integrals', 'trigonometric-integrals']"
56,maximum of iid exponential random variables,maximum of iid exponential random variables,,"I have a sequence of iid r.v. $(X_n,n\geq 1), X_i\sim \mathcal{E}(1)$. I am studying properties of the random variable $T_n = \max({X_1,\ldots,X_n})$. Computing the cdf I get $$\mathbb{P}\left(\{T_n\leq t\}\right)= \prod_{i=1}^n\mathbb{P}(\{X_i\leq t\}) = F_X(t)^n = (1-e^{-t})^n$$ I was asked to study the convergence in distribution of $T_n-\log(n)$ I get that $$ \begin{split} \mathbb{P}(\{T_n-\log n\leq t\})   &=\mathbb{P}(\{T_n\leq t+\log n\}) \\   &= \left(1-e^{-t-\log n}\right)^n \\   &= \left(1+\frac{-e^{-t}}{n}\right)^n \to e^{((-e)^{-t})}, \quad n\to +\infty \end{split}$$ But I was also asked to compute the expectation of $T_n$. Now, I know that since the $X_i$ are positive then the $T_n$ will also be positive and thus I can compute $$\mathbb{E}(T_n) = \int_{0}^{+\infty}\mathbb{P}(\{T_n>t\})dt$$ But $$\int_{0}^{+\infty}\mathbb{P}(\{T_n>t\})dt = \int_{0}^{+\infty}1-(1-e^{-t})^n dt = x\big|^{+\infty}_0 - \int_{0}^{+\infty}(1-e^{-t})^n dt$$ and I tried, for instance, to solve the last integral by substitution $y = (1-e^{-t})$ but I don't get something ""easy"" to compute, and wolfram alpha also does not provide me an answer. Am I on the wrong track?","I have a sequence of iid r.v. $(X_n,n\geq 1), X_i\sim \mathcal{E}(1)$. I am studying properties of the random variable $T_n = \max({X_1,\ldots,X_n})$. Computing the cdf I get $$\mathbb{P}\left(\{T_n\leq t\}\right)= \prod_{i=1}^n\mathbb{P}(\{X_i\leq t\}) = F_X(t)^n = (1-e^{-t})^n$$ I was asked to study the convergence in distribution of $T_n-\log(n)$ I get that $$ \begin{split} \mathbb{P}(\{T_n-\log n\leq t\})   &=\mathbb{P}(\{T_n\leq t+\log n\}) \\   &= \left(1-e^{-t-\log n}\right)^n \\   &= \left(1+\frac{-e^{-t}}{n}\right)^n \to e^{((-e)^{-t})}, \quad n\to +\infty \end{split}$$ But I was also asked to compute the expectation of $T_n$. Now, I know that since the $X_i$ are positive then the $T_n$ will also be positive and thus I can compute $$\mathbb{E}(T_n) = \int_{0}^{+\infty}\mathbb{P}(\{T_n>t\})dt$$ But $$\int_{0}^{+\infty}\mathbb{P}(\{T_n>t\})dt = \int_{0}^{+\infty}1-(1-e^{-t})^n dt = x\big|^{+\infty}_0 - \int_{0}^{+\infty}(1-e^{-t})^n dt$$ and I tried, for instance, to solve the last integral by substitution $y = (1-e^{-t})$ but I don't get something ""easy"" to compute, and wolfram alpha also does not provide me an answer. Am I on the wrong track?",,"['integration', 'probability-theory', 'probability-distributions', 'definite-integrals', 'expectation']"
57,Convolution of a function and a measure.,Convolution of a function and a measure.,,"Consider a locally compact group $\mathrm{G}$ and a left-invariant Haar measure $\lambda$ on it. Let $\mu$ be a probability measure. Suppose $f$ is a function continuous and bounded. Denote by $\Delta$ the modulus of the group $\mathrm{G}.$ I want to show that the function $$(f \ast \mu)(x) = \int\limits_\mathrm{G} f(xs^{-1}) \Delta(s^{-1})\ d\mu(s)$$ is (1) defined everywhere , (2) continuous and (3) bounded. EDIT: I am looking at the integral on a locally compact space, the usual way they handle this is by means of the Daniell integral. So, I guess the measures here are called Radon measures. In particular, they are regular (both inner and outer) and finite on every compact set. Also, if $\mathscr{A}_\mu$ denotes the $\mu$-integrable sets, then $\mathscr{A}_\mu$ contains the topology of $\mathrm{X}$ (remark $\mu$ is a probability measure, so there are no issues of infinite measure here). In the main text the proof of the same statement is given for the convolution $\mu \ast f$ which is given by $$(\mu \ast f)(x) = \int\limits_\mathrm{G} f(s^{-1} x)\ d\mu(s).$$ I tried to immitate the proof, but the are problems that arise. The easiest way to illustrate this is when they show boundedness. For the case $\mu \ast f$ the author uses a well-known inequality and boundedness of $f$ $$|(\mu \ast f)(x)| \leq \int\limits_\mathrm{G} |f(s^{-1} x)|\ d\mu(s) \leq \|f\|,$$ since $\mu$ is a probability measure. Obviously, the same proof can't proceed for the convolution $f \ast \mu$ since one would get $$|(\mu \ast f)(x)| \leq \|f\| \int\limits_\mathrm{G} \Delta(s^{-1})\ d\mu(s)$$ and I don't think the modulus function is integrable. Any suggestions is greatly appreciated.","Consider a locally compact group $\mathrm{G}$ and a left-invariant Haar measure $\lambda$ on it. Let $\mu$ be a probability measure. Suppose $f$ is a function continuous and bounded. Denote by $\Delta$ the modulus of the group $\mathrm{G}.$ I want to show that the function $$(f \ast \mu)(x) = \int\limits_\mathrm{G} f(xs^{-1}) \Delta(s^{-1})\ d\mu(s)$$ is (1) defined everywhere , (2) continuous and (3) bounded. EDIT: I am looking at the integral on a locally compact space, the usual way they handle this is by means of the Daniell integral. So, I guess the measures here are called Radon measures. In particular, they are regular (both inner and outer) and finite on every compact set. Also, if $\mathscr{A}_\mu$ denotes the $\mu$-integrable sets, then $\mathscr{A}_\mu$ contains the topology of $\mathrm{X}$ (remark $\mu$ is a probability measure, so there are no issues of infinite measure here). In the main text the proof of the same statement is given for the convolution $\mu \ast f$ which is given by $$(\mu \ast f)(x) = \int\limits_\mathrm{G} f(s^{-1} x)\ d\mu(s).$$ I tried to immitate the proof, but the are problems that arise. The easiest way to illustrate this is when they show boundedness. For the case $\mu \ast f$ the author uses a well-known inequality and boundedness of $f$ $$|(\mu \ast f)(x)| \leq \int\limits_\mathrm{G} |f(s^{-1} x)|\ d\mu(s) \leq \|f\|,$$ since $\mu$ is a probability measure. Obviously, the same proof can't proceed for the convolution $f \ast \mu$ since one would get $$|(\mu \ast f)(x)| \leq \|f\| \int\limits_\mathrm{G} \Delta(s^{-1})\ d\mu(s)$$ and I don't think the modulus function is integrable. Any suggestions is greatly appreciated.",,"['integration', 'measure-theory', 'convolution', 'locally-compact-groups', 'haar-measure']"
58,$\int \frac{x^2+4x+4}{(x^2+5x+7)\sqrt{x+2} } dx$,,\int \frac{x^2+4x+4}{(x^2+5x+7)\sqrt{x+2} } dx,The question is to evaluate $$\int \frac{x^2+4x+4}{(x^2+5x+7)\sqrt{x+2} } dx$$ I tried rewriting the integral as $$\int \frac{dx}{\sqrt{x+2}} - \int\frac{x+3}{(x^2+5x+7)\sqrt{x+2}} dx$$ the first integral is simply $2\sqrt{x+2}$.I tried using Integration by parts on second integral but it got complicated.Any ideas?,The question is to evaluate $$\int \frac{x^2+4x+4}{(x^2+5x+7)\sqrt{x+2} } dx$$ I tried rewriting the integral as $$\int \frac{dx}{\sqrt{x+2}} - \int\frac{x+3}{(x^2+5x+7)\sqrt{x+2}} dx$$ the first integral is simply $2\sqrt{x+2}$.I tried using Integration by parts on second integral but it got complicated.Any ideas?,,"['calculus', 'integration']"
59,"""Point-wise"" value of Bochner integral of $L^2$ functions","""Point-wise"" value of Bochner integral of  functions",L^2,"Let $T:L^2(\mathbb{R})\rightarrow L^2(\mathbb{R})$ be a linear operator and $\phi:[a,b]\rightarrow L^2(\mathbb{R})$ be a parameterised family of $L^2$ functions. I sometimes see an integral (which seems to be called a Bochner integral) like $$\int_a^b T\phi(t)\,dt,$$ which is supposed to give an element of $L^2(\mathbb{R})$, call it $f$. Question: How does one interpret $f$ as an actual function (up to a set of measure $0$)? For instance, it is possible to make a statement like: the point-wise value of $f$ at $x$ (up to a set of measure $0$) is the same as the value $$\int_a^b (T\phi(t))(x)\,dt\in\mathbb{R}$$ where $T\phi(t)(x)$ is the value at $x$ of some representative of $T\phi(t)\in L^2(\mathbb{R})$? I'm not sure if the last question makes sense in general, but it certainly should if $\phi(t)$ was point-wise well-defined, for example if for all $t$ we have $\phi(t)\in C_c^\infty(\mathbb{R})\subseteq L^2(\mathbb{R})$.","Let $T:L^2(\mathbb{R})\rightarrow L^2(\mathbb{R})$ be a linear operator and $\phi:[a,b]\rightarrow L^2(\mathbb{R})$ be a parameterised family of $L^2$ functions. I sometimes see an integral (which seems to be called a Bochner integral) like $$\int_a^b T\phi(t)\,dt,$$ which is supposed to give an element of $L^2(\mathbb{R})$, call it $f$. Question: How does one interpret $f$ as an actual function (up to a set of measure $0$)? For instance, it is possible to make a statement like: the point-wise value of $f$ at $x$ (up to a set of measure $0$) is the same as the value $$\int_a^b (T\phi(t))(x)\,dt\in\mathbb{R}$$ where $T\phi(t)(x)$ is the value at $x$ of some representative of $T\phi(t)\in L^2(\mathbb{R})$? I'm not sure if the last question makes sense in general, but it certainly should if $\phi(t)$ was point-wise well-defined, for example if for all $t$ we have $\phi(t)\in C_c^\infty(\mathbb{R})\subseteq L^2(\mathbb{R})$.",,"['real-analysis', 'integration', 'functional-analysis', 'analysis', 'measure-theory']"
60,Fourier Transform of $\displaystyle \frac{x}{x+ic}$,Fourier Transform of,\displaystyle \frac{x}{x+ic},"$$\int_{-\infty} ^\infty \frac{x}{x+ic}e^{ikx}dx$$ where $c$ is some positive real constant. I've tried substituton $u=x+ic$ giving: $$\int_{-\infty}^\infty \frac{u-ic}{u}e^{ik(u-ic)}du \\ = \int_{-\infty} ^\infty \frac{u-ic}{u}e^{iku}e^{kc}du \\ = e^{kc}\left(\int_{-\infty} ^\infty e^{iku}du-ic\int_{-\infty} ^\infty\frac{e^{iku}}{u}du\right) \\ = e^{kc}\left(\left.\frac{e^{iku}}{ik}\right|_{-\infty} ^\infty -ic\int_{-\infty} ^\infty\frac{e^{iku}}{u}du\right) $$ I don't know how to evaluate the second integral, and I'm not sure if there's just an easier way to do the whole calculation that I'm missing.","$$\int_{-\infty} ^\infty \frac{x}{x+ic}e^{ikx}dx$$ where $c$ is some positive real constant. I've tried substituton $u=x+ic$ giving: $$\int_{-\infty}^\infty \frac{u-ic}{u}e^{ik(u-ic)}du \\ = \int_{-\infty} ^\infty \frac{u-ic}{u}e^{iku}e^{kc}du \\ = e^{kc}\left(\int_{-\infty} ^\infty e^{iku}du-ic\int_{-\infty} ^\infty\frac{e^{iku}}{u}du\right) \\ = e^{kc}\left(\left.\frac{e^{iku}}{ik}\right|_{-\infty} ^\infty -ic\int_{-\infty} ^\infty\frac{e^{iku}}{u}du\right) $$ I don't know how to evaluate the second integral, and I'm not sure if there's just an easier way to do the whole calculation that I'm missing.",,"['integration', 'fourier-analysis', 'fourier-transform']"
61,The inverse Laplace transform of $e^{-z}\textrm{Ei}(z)z^{-1}\log(z).$,The inverse Laplace transform of,e^{-z}\textrm{Ei}(z)z^{-1}\log(z).,"For a work I need to evaluate the following integral: $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}e^{uz}e^{-z}\textrm{Ei}\left(z\right)z^{-1}\log\left(z\right)dz,\ c>0,\,u>2$$ where $\mathrm{Ei}\left(z\right)$ is the exponential integral function . I know that $$\mathfrak{L}^{-1}\left(z^{-1}\log\left(z\right)\right)\left(u\right)=(-\log\left(u\right)-\gamma)1_{u>0}$$ where $\gamma$ is the Euler-Mascheroni constant , so my idea was to find the inverse Laplace transform of $e^{-z}\textrm{Ei}\left(z\right)$ and then to use the convolution theorem. I found that $$e^{-z}\textrm{Ei}\left(z\right)=PV\int_{0}^{\infty}\frac{e^{-uz}}{1-u}du\tag{1}$$ so my question is: Can I use the convolution theorem even if the integral in $(1)$ exists only in the sense of the Cauchy Principal Value ? In other words, can I write $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}e^{uz}e^{-z}\textrm{Ei}\left(z\right)z^{-1}\log\left(z\right)dz=PV\int_{0}^{u}\frac{\log\left(t\right)+\gamma}{u-t-1}dt?$$ I don't know if it is a standard property or is a idiocy. I searched in my textbooks but I didn't find anything like that. Thank you for your time.","For a work I need to evaluate the following integral: $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}e^{uz}e^{-z}\textrm{Ei}\left(z\right)z^{-1}\log\left(z\right)dz,\ c>0,\,u>2$$ where $\mathrm{Ei}\left(z\right)$ is the exponential integral function . I know that $$\mathfrak{L}^{-1}\left(z^{-1}\log\left(z\right)\right)\left(u\right)=(-\log\left(u\right)-\gamma)1_{u>0}$$ where $\gamma$ is the Euler-Mascheroni constant , so my idea was to find the inverse Laplace transform of $e^{-z}\textrm{Ei}\left(z\right)$ and then to use the convolution theorem. I found that $$e^{-z}\textrm{Ei}\left(z\right)=PV\int_{0}^{\infty}\frac{e^{-uz}}{1-u}du\tag{1}$$ so my question is: Can I use the convolution theorem even if the integral in $(1)$ exists only in the sense of the Cauchy Principal Value ? In other words, can I write $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}e^{uz}e^{-z}\textrm{Ei}\left(z\right)z^{-1}\log\left(z\right)dz=PV\int_{0}^{u}\frac{\log\left(t\right)+\gamma}{u-t-1}dt?$$ I don't know if it is a standard property or is a idiocy. I searched in my textbooks but I didn't find anything like that. Thank you for your time.",,"['integration', 'complex-analysis', 'special-functions', 'laplace-transform', 'integral-transforms']"
62,What are the values of the following integrals?,What are the values of the following integrals?,,"For each $k = 1,2,3,\dots, $, let $I_k=\left( \frac{1}{k+1}, \frac{1}{k}\right)$, $|I_k| = \frac{1}{k(k+1)}$. Let   $$g_k(x)=k(k+1)\chi_{I_k} \, : \, \int_0^1 g_k(x)dx=1$$   for each $k$. Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ be defined as follows   $$f(x,y) :=  \begin{cases} 0, \quad &\text{if}\ (x,y)\not\in[0,1]^2\\ \sum_{k=1}^{\infty}[g_k(x)-g_{k+1}(x)]g_k(y) \quad &\text{if}\ (x,y)\in [0,1]^2 \end{cases}$$   Find the values of $$\int_0^1\int_0^1f(x,y)dxdy, \, \int_0^1\int_0^1 f(x,y)dydx,\, \text{and}\, \iint_{[0,1]^2}f(x,y)dxdy$$ The values of the integrals are supposed to be $1, 0$ and $\infty$ respectively, but I don't know why (I get that both the iterates are $0$). For example:  \begin{align*} \int_0^1 \int_0^1 f(x,y)dydx &= \int_0^1 \int_0^1 \sum_{k=1}^{\infty} [g_k(x)-g_k(x)]g_k(y)dydx\\ & =\int_0^1 \sum_{k=1}^{\infty}\int_0^1[g_k(x)-g_{k+1}]g_k(y)didx\\ & =\sum_{k=1}^{\infty} \int_0^1 g_k(y) \int_0^1 [g_k(x)-g_{k+1}(x)]dx dy\\ & = \sum_{k=1}^{\infty}1\cdot 0 = 0 \end{align*}  and with the same calculations, I get the same result for the other iterate. Where were my mistakes? If someone could point them out and explain to me how to get to the desire result, I'd be more than happy. Thank you!","For each $k = 1,2,3,\dots, $, let $I_k=\left( \frac{1}{k+1}, \frac{1}{k}\right)$, $|I_k| = \frac{1}{k(k+1)}$. Let   $$g_k(x)=k(k+1)\chi_{I_k} \, : \, \int_0^1 g_k(x)dx=1$$   for each $k$. Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ be defined as follows   $$f(x,y) :=  \begin{cases} 0, \quad &\text{if}\ (x,y)\not\in[0,1]^2\\ \sum_{k=1}^{\infty}[g_k(x)-g_{k+1}(x)]g_k(y) \quad &\text{if}\ (x,y)\in [0,1]^2 \end{cases}$$   Find the values of $$\int_0^1\int_0^1f(x,y)dxdy, \, \int_0^1\int_0^1 f(x,y)dydx,\, \text{and}\, \iint_{[0,1]^2}f(x,y)dxdy$$ The values of the integrals are supposed to be $1, 0$ and $\infty$ respectively, but I don't know why (I get that both the iterates are $0$). For example:  \begin{align*} \int_0^1 \int_0^1 f(x,y)dydx &= \int_0^1 \int_0^1 \sum_{k=1}^{\infty} [g_k(x)-g_k(x)]g_k(y)dydx\\ & =\int_0^1 \sum_{k=1}^{\infty}\int_0^1[g_k(x)-g_{k+1}]g_k(y)didx\\ & =\sum_{k=1}^{\infty} \int_0^1 g_k(y) \int_0^1 [g_k(x)-g_{k+1}(x)]dx dy\\ & = \sum_{k=1}^{\infty}1\cdot 0 = 0 \end{align*}  and with the same calculations, I get the same result for the other iterate. Where were my mistakes? If someone could point them out and explain to me how to get to the desire result, I'd be more than happy. Thank you!",,"['integration', 'sequences-and-series', 'measure-theory', 'definite-integrals']"
63,Approximating $\int_0^1 \cos(x^2)dx$ with power series,Approximating  with power series,\int_0^1 \cos(x^2)dx,"I would like to calculate $\int_0^1 \cos(x^2)dx$ with an error smaller than $10^{-6}$ (this error should be proven.). I have a strategy, but I am not quite sure if this is a valid one. Here is what I did: I know that the series representation of the cosine is:$$\cos(x) = \sum_{k=0}^\infty (-1)^k \frac{x^{2k}}{(2k)!}$$ Since the integral boundaries I want to calculate only range from $0$ to $1$, I can use the fact that $a_k :=\frac{x^{2k}}{(2k)!}$ is a falling sequence for $x \in [0,1]$. Then I would use the estimation from the Leibnitz criterion, that for positive falling $a_k$ with $$s_n := \sum_{k=1}^n (-1)^k a_k$$ the inequality $|s - s_n|\le a_{n+1}$ holds, whereas $s$ denotes the limit of $s_k$. Then I could say that for $n=9$: $$|\cos(x)-s_9| \le \frac{1}{(2\cdot 10)!}$$ Then I would calculate $\int_0^1 \sum_{k=0}^9 (-1)^k \frac{x^{4k}}{(2k)!}dx$ The estimate I get seems to be correct, however I am not sure whether or not my prove above is sufficient. Any help would be greatly appreciated!","I would like to calculate $\int_0^1 \cos(x^2)dx$ with an error smaller than $10^{-6}$ (this error should be proven.). I have a strategy, but I am not quite sure if this is a valid one. Here is what I did: I know that the series representation of the cosine is:$$\cos(x) = \sum_{k=0}^\infty (-1)^k \frac{x^{2k}}{(2k)!}$$ Since the integral boundaries I want to calculate only range from $0$ to $1$, I can use the fact that $a_k :=\frac{x^{2k}}{(2k)!}$ is a falling sequence for $x \in [0,1]$. Then I would use the estimation from the Leibnitz criterion, that for positive falling $a_k$ with $$s_n := \sum_{k=1}^n (-1)^k a_k$$ the inequality $|s - s_n|\le a_{n+1}$ holds, whereas $s$ denotes the limit of $s_k$. Then I could say that for $n=9$: $$|\cos(x)-s_9| \le \frac{1}{(2\cdot 10)!}$$ Then I would calculate $\int_0^1 \sum_{k=0}^9 (-1)^k \frac{x^{4k}}{(2k)!}dx$ The estimate I get seems to be correct, however I am not sure whether or not my prove above is sufficient. Any help would be greatly appreciated!",,"['integration', 'fresnel-integrals']"
64,Investigate on convergence and absolute convergence,Investigate on convergence and absolute convergence,,"How to investigate on convergence and absolute convergence the following integral: $$\int \limits_0^1 \frac{\sin(1/x)}{(\sqrt{x} - x)^n}$$ for all real values of $n$? I've tried to make a comparison test. In order to do this I needed some upper bound, but I didn't know how to pick it. UPD As @zhw. pointed I made substitution $x = 1/y$ and got $$\int \limits_1^{\pi/2} \frac{\sin y \cdot y^{n - 2}}{(\sqrt{y} - 1)^n}dy + \int \limits_{\pi/2}^{\infty} \frac{\sin y \cdot y^{n - 2}}{(\sqrt{y} - 1)^n}dy$$ For the first integral I want to use Taylor expansion for $\sqrt{y}$ at $y = 1$ and also the numerator can be bounded by some constant: $$\int \limits_1^{\pi/2} \frac{\sin y \cdot y^{n - 2}}{(\sqrt{y} - 1)^n}dy < \int \limits_1^{\pi/2} \frac{c}{(\sqrt{y} - 1)^n}dy \sim \int \limits_1^{\pi/2} \frac{c}{(1+ \frac{y - 1}{2} + O((y - 1)^2) - 1)^n}dy = \int \limits_1^{\pi/2} \frac{c}{(\frac{y - 1}{2} + O((y - 1)^2))^n}dy$$ . I don't really know how to proceed further. And also I don't like the jump with equivalence. For the right integral I probably could say that as $y \to \infty \Rightarrow (\sqrt{y} - 1)^n \sim y^{n/2}$, but again I want to be fully correct with this step. It seems like it is converge for $n < 1$. I can't say anything about absolute convergence.","How to investigate on convergence and absolute convergence the following integral: $$\int \limits_0^1 \frac{\sin(1/x)}{(\sqrt{x} - x)^n}$$ for all real values of $n$? I've tried to make a comparison test. In order to do this I needed some upper bound, but I didn't know how to pick it. UPD As @zhw. pointed I made substitution $x = 1/y$ and got $$\int \limits_1^{\pi/2} \frac{\sin y \cdot y^{n - 2}}{(\sqrt{y} - 1)^n}dy + \int \limits_{\pi/2}^{\infty} \frac{\sin y \cdot y^{n - 2}}{(\sqrt{y} - 1)^n}dy$$ For the first integral I want to use Taylor expansion for $\sqrt{y}$ at $y = 1$ and also the numerator can be bounded by some constant: $$\int \limits_1^{\pi/2} \frac{\sin y \cdot y^{n - 2}}{(\sqrt{y} - 1)^n}dy < \int \limits_1^{\pi/2} \frac{c}{(\sqrt{y} - 1)^n}dy \sim \int \limits_1^{\pi/2} \frac{c}{(1+ \frac{y - 1}{2} + O((y - 1)^2) - 1)^n}dy = \int \limits_1^{\pi/2} \frac{c}{(\frac{y - 1}{2} + O((y - 1)^2))^n}dy$$ . I don't really know how to proceed further. And also I don't like the jump with equivalence. For the right integral I probably could say that as $y \to \infty \Rightarrow (\sqrt{y} - 1)^n \sim y^{n/2}$, but again I want to be fully correct with this step. It seems like it is converge for $n < 1$. I can't say anything about absolute convergence.",,"['calculus', 'real-analysis', 'integration', 'convergence-divergence', 'definite-integrals']"
65,Regularization of integral,Regularization of integral,,"How does one regularize a divergent integral of the form, $$ I = \int_0^\infty dx \, \cosh^4 x \hspace{0.5 in} ? $$ Regularizing via simple subtraction of divergences (as we commonly do in physics) is not a viable option, since expanding $\cosh^4x$ in a power series reveals an infinite number of divergent contributions, with no finite piece to remain. However, I do know that there must be some regularization technique out there enabling us to evaluate this, since we have a similar result $$ \int_0^\infty dx \, \cosh^4 x \sinh x = -{8 \pi \over 15} $$ in common use. Any tips on how to obtain this sort of thing  (or even reference to a source where this is calculated) would be very helpful!","How does one regularize a divergent integral of the form, $$ I = \int_0^\infty dx \, \cosh^4 x \hspace{0.5 in} ? $$ Regularizing via simple subtraction of divergences (as we commonly do in physics) is not a viable option, since expanding $\cosh^4x$ in a power series reveals an infinite number of divergent contributions, with no finite piece to remain. However, I do know that there must be some regularization technique out there enabling us to evaluate this, since we have a similar result $$ \int_0^\infty dx \, \cosh^4 x \sinh x = -{8 \pi \over 15} $$ in common use. Any tips on how to obtain this sort of thing  (or even reference to a source where this is calculated) would be very helpful!",,"['integration', 'definite-integrals', 'divergence-operator', 'regularization']"
66,Evaluate $\int_{|z|=1} \frac{1}{z^2 -\frac{3}{2}z + 1} dz$,Evaluate,\int_{|z|=1} \frac{1}{z^2 -\frac{3}{2}z + 1} dz,Evaluate : $$\int_{|z|=1} \frac{1}{z^2 -\frac{3}{2}z + 1} dz$$ Using residue method : $$z=\frac{3}{4} \pm i \frac{\sqrt{7}}{4}$$ The problem is the two roots on the boundary $|z|=1$,Evaluate : $$\int_{|z|=1} \frac{1}{z^2 -\frac{3}{2}z + 1} dz$$ Using residue method : $$z=\frac{3}{4} \pm i \frac{\sqrt{7}}{4}$$ The problem is the two roots on the boundary $|z|=1$,,"['integration', 'complex-analysis', 'residue-calculus']"
67,Integrals of Infinitely Iterated Functions,Integrals of Infinitely Iterated Functions,,"As a curiosity, I was looking at functions such as $y = x^{x^{x^{.^{.^{.}}}}}$ and finding their derivatives. I realize this is quite easy. For this problem, we can write $y = x^y$ and use implicit differentiation, and essentially the same concept can be used for any infinitely iterated function like this. I then tried to integrate one of these and am finding it much more difficult. Is anything known about $\int x^{x^{x^{.^{.^{.}}}}}dx$ or any other functions involving some sort of infinite iteration? (like $\sqrt{x + \sqrt{x + ...}}$)?","As a curiosity, I was looking at functions such as $y = x^{x^{x^{.^{.^{.}}}}}$ and finding their derivatives. I realize this is quite easy. For this problem, we can write $y = x^y$ and use implicit differentiation, and essentially the same concept can be used for any infinitely iterated function like this. I then tried to integrate one of these and am finding it much more difficult. Is anything known about $\int x^{x^{x^{.^{.^{.}}}}}dx$ or any other functions involving some sort of infinite iteration? (like $\sqrt{x + \sqrt{x + ...}}$)?",,"['calculus', 'integration']"
68,Compute $\int_0^{\infty} \frac{1}{e^{x}+x}dx$,Compute,\int_0^{\infty} \frac{1}{e^{x}+x}dx,"I am trying to compute this integral: $$I = \int_0^{\infty} \frac{1}{e^x + x}dx$$ I don't see any obvious ways to integrate this using real methods. So, now I'm trying to integrate with using complex analysis, I tried to transform the equation with $z = e^{-x}$ and get $$I = \int_0^1 \frac{1}{1-z\log(z)}dz$$ I see that a pole of the above is $z = \frac{1}{W(1)}$, which is the Lambert W function When I evaluate the residue, I get the value 0. $$\text{Res}_{z \rightarrow \frac{1}{W(1)}}\left(\frac{1}{1-z \log (z)},f(z)\right) = 0$$ Assistance on next steps and a solution would be appreciated.","I am trying to compute this integral: $$I = \int_0^{\infty} \frac{1}{e^x + x}dx$$ I don't see any obvious ways to integrate this using real methods. So, now I'm trying to integrate with using complex analysis, I tried to transform the equation with $z = e^{-x}$ and get $$I = \int_0^1 \frac{1}{1-z\log(z)}dz$$ I see that a pole of the above is $z = \frac{1}{W(1)}$, which is the Lambert W function When I evaluate the residue, I get the value 0. $$\text{Res}_{z \rightarrow \frac{1}{W(1)}}\left(\frac{1}{1-z \log (z)},f(z)\right) = 0$$ Assistance on next steps and a solution would be appreciated.",,"['integration', 'complex-analysis', 'definite-integrals', 'improper-integrals', 'lambert-w']"
69,Interesting examples of differentiation under the integral sign?,Interesting examples of differentiation under the integral sign?,,"I was recently looking through integration techniques when I came upon differentiation under the integral sign (DUIS).  It seems to be pretty powerful, for example: $$f(t)=\int_0^1\frac{x^t-1}{\ln(x)}\ dx\implies f'(t)=\int_0^1x^t\ dx=\frac1{t+1}\\\implies f(t)=C+\ln(t+1)\\f(0)=0\implies C=0\\\implies\int_0^1\frac{x^t-1}{\ln(x)}\ dx=\ln(t+1)$$ Now, on my own, proving that $\int_0^1\frac{x^t-1}{\ln(x)}\ dx=\ln(t+1)$ would've been a hefty task without DUIS, and so I wanted to ask this question: It's hard to produce interesting examples where applying DUIS is almost magical, so what are some good example uses of DUIS?","I was recently looking through integration techniques when I came upon differentiation under the integral sign (DUIS).  It seems to be pretty powerful, for example: $$f(t)=\int_0^1\frac{x^t-1}{\ln(x)}\ dx\implies f'(t)=\int_0^1x^t\ dx=\frac1{t+1}\\\implies f(t)=C+\ln(t+1)\\f(0)=0\implies C=0\\\implies\int_0^1\frac{x^t-1}{\ln(x)}\ dx=\ln(t+1)$$ Now, on my own, proving that $\int_0^1\frac{x^t-1}{\ln(x)}\ dx=\ln(t+1)$ would've been a hefty task without DUIS, and so I wanted to ask this question: It's hard to produce interesting examples where applying DUIS is almost magical, so what are some good example uses of DUIS?",,"['calculus', 'integration', 'big-list', 'leibniz-integral-rule']"
70,challenging sum,challenging sum,,"Let $\phi$ the Euler totient, and $f : [0,1] \to \mathbb{C}$ piecewise continuous. Prove that    $$ \lim_{n\to\infty} \frac{1}{\phi(n)} \sum_{\substack{1\leq k \leq n\\ \gcd(k,n)=1}} f\Bigl(\frac{k}{n}\Bigr) = \int_{0}^{1} f(x)\,dx $$","Let $\phi$ the Euler totient, and $f : [0,1] \to \mathbb{C}$ piecewise continuous. Prove that    $$ \lim_{n\to\infty} \frac{1}{\phi(n)} \sum_{\substack{1\leq k \leq n\\ \gcd(k,n)=1}} f\Bigl(\frac{k}{n}\Bigr) = \int_{0}^{1} f(x)\,dx $$",,"['integration', 'number-theory']"
71,How to integrate this arcsin integral? [closed],How to integrate this arcsin integral? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How to integrate $$\int_a^1 \frac{\arcsin x}{\sqrt{x^2-a^2}} dx?$$ This integral appeared in Demkov Yu. N., Ostrovsky V. N., Berezina (Avdonina) N. B., ""Uniqueness of the Firsov Inversion Method and Focusing Potentials"", Sov. Phys. JETP 33, 867-70, (1971).","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How to integrate $$\int_a^1 \frac{\arcsin x}{\sqrt{x^2-a^2}} dx?$$ This integral appeared in Demkov Yu. N., Ostrovsky V. N., Berezina (Avdonina) N. B., ""Uniqueness of the Firsov Inversion Method and Focusing Potentials"", Sov. Phys. JETP 33, 867-70, (1971).",,['integration']
72,Are there real definite integrals which can only be evaluated by contour integration?,Are there real definite integrals which can only be evaluated by contour integration?,,"Contour integration is a great tool for definite integrals, especially of the following type: $$\int_{-\infty}^\infty \frac{f(x)dx}{P(x)}$$ where $P(x)$ is a polynomial with no zeroes on the real axis (as far as I remember) and $f(x)$ is some appropriate regular function, which vanishes fast enough in the upper half-plane of $\mathbb{C}$. I'm not going to go into details here and refer everyone to a textbook definition. Contour integration seems like magic to be, and I'm sure to most of the regular people who use it either during their education or for various applications like physics. It's simple for me to see how it works in the complex plane, but not how it applies to the real functions. Why would some poles which are not on the real line affect the value of a real integral which still can be seen as the area under the curve defined by the integrand? I know that some integrals which can be solved by contour integration can be also solved in other ways, for example the most simple one: $$I=\int_{-\infty}^\infty \frac{dx}{1+x^2}=\pi$$ Using $(1+i)(1-i)=1+x^2$ makes the contour integration very simple, however it's also simple to show that $I=4\int_0^1 \frac{dx}{1+x^2}$ and connect the following integral to the definition of arcangent and Lebniz series. But not all of the integrals can be easily solved that way. What about: $$\int_{-\infty}^\infty \frac{\cos x dx}{1+x^2}=\frac{\pi}{e}$$ Can this integral be solved by real methods? Are there real integrals with closed forms, which can only be evaluated by contour integration? And if they exist, then how can it be explained? To be clear, I understand that we can numerically confirm the value to any precision by just using numerical methods for the real integral in question. Except for some highly oscillatory integrands (probably).","Contour integration is a great tool for definite integrals, especially of the following type: $$\int_{-\infty}^\infty \frac{f(x)dx}{P(x)}$$ where $P(x)$ is a polynomial with no zeroes on the real axis (as far as I remember) and $f(x)$ is some appropriate regular function, which vanishes fast enough in the upper half-plane of $\mathbb{C}$. I'm not going to go into details here and refer everyone to a textbook definition. Contour integration seems like magic to be, and I'm sure to most of the regular people who use it either during their education or for various applications like physics. It's simple for me to see how it works in the complex plane, but not how it applies to the real functions. Why would some poles which are not on the real line affect the value of a real integral which still can be seen as the area under the curve defined by the integrand? I know that some integrals which can be solved by contour integration can be also solved in other ways, for example the most simple one: $$I=\int_{-\infty}^\infty \frac{dx}{1+x^2}=\pi$$ Using $(1+i)(1-i)=1+x^2$ makes the contour integration very simple, however it's also simple to show that $I=4\int_0^1 \frac{dx}{1+x^2}$ and connect the following integral to the definition of arcangent and Lebniz series. But not all of the integrals can be easily solved that way. What about: $$\int_{-\infty}^\infty \frac{\cos x dx}{1+x^2}=\frac{\pi}{e}$$ Can this integral be solved by real methods? Are there real integrals with closed forms, which can only be evaluated by contour integration? And if they exist, then how can it be explained? To be clear, I understand that we can numerically confirm the value to any precision by just using numerical methods for the real integral in question. Except for some highly oscillatory integrands (probably).",,"['calculus', 'integration', 'definite-integrals', 'contour-integration']"
73,Do Contour Integrals and Infinite Series Commute? Fubini's Theorem,Do Contour Integrals and Infinite Series Commute? Fubini's Theorem,,"I ask this question because similar questions don't address contour integrals and infinite series specifically, only evaluating a contour integral by converting it into an infinite series. I'm trying to take the contour integral of an infinite series.  My understanding is that under Fubini's theorem that under certain conditions, integrals can commute with each other, summations can commute with each other, and integrals can commute with summations. However, the requirement (as I understand it) is that the summation must converge for the summation and integral to commute; but if I knew whether or not the summation converges, I would not need to integrate first. Here's an example: $$\oint_C  \sum_{n=0}^ \infty  a_{n} \cdot f_{n}  \big(x\big) \stackrel{?}{=} \sum_{n=0}^ \infty  \oint_C  a_{n} \cdot f_{n}  \big(x\big)$$ EDIT: I'd like to further explain my question and perhaps why it's confusing.  I want to find an analytical solution to the area of a simply-connected space.  I found that I could conveniently describe this space using parametric equations, and therefore could use Green's Theorem and integrate around the boundary to obtain the area.  The parametric equations are: $$x[\theta] =  a \cdot Cos[\theta]$$ $$y[\theta] =  \sum_{n=0}^ \infty \frac{b_{n}}{n} \cdot Sin[(2n+1) \cdot \theta] + \frac{c_{n}}{n} \cdot Cos[(2n+1) \cdot  \theta ]$$ From there you can calculate $dx/d\theta$ and $dy/d\theta$ and from there $ A = \frac{1}{2} \oint_C x \cdot dy-y \cdot dx$. However, summing the integral means summing the individual areas of each term's space, while integrating the sum means integrating the area from the sum of the contribution of each term to the coordinates at a given point on the contour. These seem to be different operations to me and therefore I'm confused as to under what circumstances contour integral's and infinite series commute. EDIT 2:  I'm asking this question because I can do the integral on the right hand side (the sum of the contour integrals) in the example, but not the other (the contour integral of the sum), and need to know (1) how to test to see if both expressions are equivalent.  The suggestions so far have been to integrate both sides and see if they are equivalent, but I can't do that, hence the need for Fubini's Theorem.","I ask this question because similar questions don't address contour integrals and infinite series specifically, only evaluating a contour integral by converting it into an infinite series. I'm trying to take the contour integral of an infinite series.  My understanding is that under Fubini's theorem that under certain conditions, integrals can commute with each other, summations can commute with each other, and integrals can commute with summations. However, the requirement (as I understand it) is that the summation must converge for the summation and integral to commute; but if I knew whether or not the summation converges, I would not need to integrate first. Here's an example: $$\oint_C  \sum_{n=0}^ \infty  a_{n} \cdot f_{n}  \big(x\big) \stackrel{?}{=} \sum_{n=0}^ \infty  \oint_C  a_{n} \cdot f_{n}  \big(x\big)$$ EDIT: I'd like to further explain my question and perhaps why it's confusing.  I want to find an analytical solution to the area of a simply-connected space.  I found that I could conveniently describe this space using parametric equations, and therefore could use Green's Theorem and integrate around the boundary to obtain the area.  The parametric equations are: $$x[\theta] =  a \cdot Cos[\theta]$$ $$y[\theta] =  \sum_{n=0}^ \infty \frac{b_{n}}{n} \cdot Sin[(2n+1) \cdot \theta] + \frac{c_{n}}{n} \cdot Cos[(2n+1) \cdot  \theta ]$$ From there you can calculate $dx/d\theta$ and $dy/d\theta$ and from there $ A = \frac{1}{2} \oint_C x \cdot dy-y \cdot dx$. However, summing the integral means summing the individual areas of each term's space, while integrating the sum means integrating the area from the sum of the contribution of each term to the coordinates at a given point on the contour. These seem to be different operations to me and therefore I'm confused as to under what circumstances contour integral's and infinite series commute. EDIT 2:  I'm asking this question because I can do the integral on the right hand side (the sum of the contour integrals) in the example, but not the other (the contour integral of the sum), and need to know (1) how to test to see if both expressions are equivalent.  The suggestions so far have been to integrate both sides and see if they are equivalent, but I can't do that, hence the need for Fubini's Theorem.",,"['integration', 'sequences-and-series', 'contour-integration']"
74,Is integrating $f(x)= x\exp(-x^2/2)$ with substitution $u = x^2$ well defined?,Is integrating  with substitution  well defined?,f(x)= x\exp(-x^2/2) u = x^2,"I have the following question, which is: I know how to integrate $f(x) = x\exp(-x^2)$ using the standard method of subsitution, however I was wondering if this method is well defined, and if so why exactly. The reason I ask this, is because a coordinate transformation must be a bijective and continuously partially differentiable mapping, from say an set U to another open set V. Now formally $u\colon \mathbb{R} \to \mathbb{R}$, $u(x) = x^2$ is not a coordinate transfer, so I ask: why would the substitution $u(x) = x^2$ be a well defined one? Is it because we do not use definite integrals?","I have the following question, which is: I know how to integrate $f(x) = x\exp(-x^2)$ using the standard method of subsitution, however I was wondering if this method is well defined, and if so why exactly. The reason I ask this, is because a coordinate transformation must be a bijective and continuously partially differentiable mapping, from say an set U to another open set V. Now formally $u\colon \mathbb{R} \to \mathbb{R}$, $u(x) = x^2$ is not a coordinate transfer, so I ask: why would the substitution $u(x) = x^2$ be a well defined one? Is it because we do not use definite integrals?",,['integration']
75,How to do this two complex integrals?,How to do this two complex integrals?,,"The first integral reads: $$ f(\vec{r})=\iiint_{-\infty}^\infty \mathrm{d}k_x \, \mathrm{d}k_y \, \mathrm{d}k_z \, \frac{e^{i\vec{k}\cdot\vec{r}}}{(k_0-\alpha k_z)^2-k^2+i(k_0-\alpha k_z)\eta} $$ the second integral is very similar, except the infinitesimal parts: $$ f(\vec{r})=\iiint_{-\infty}^\infty \mathrm{d}k_x \, \mathrm{d}k_y \, \mathrm{d}k_z \, \frac{e^{i\vec{k}\cdot\vec{r}}}{(k_0-\alpha k_z)^2-k^2+i\eta} $$ where $\alpha>1$, $k_0>0$ is finite positive number, $\eta = 0^+$ is the infinitesimal positive number. $\vec{k}=(k_x,k_y,k_z), \vec{r}=(x,y,z), k=\sqrt{k_x^2+k_y^2+k_z^2}$. As a hint , the integral for $\alpha=0$ can be done as following: \begin{align} &\int\mathrm{d}^3k \frac{e^{i\vec{k}\cdot\vec{r}}}{k_0^2-k^2+i\eta} \quad\text{--- making $\vec{r}$ as $z$ axis for $\vec{k}$ } \\ =& 2\pi\int_{-1}^1\mathrm{d}\xi\int_0^\infty \mathrm{d}k k^2\frac{e^{ikr\xi}}{k_0^2-k^2+i\eta}\quad\text{--- integral over $\phi_k$, and let $\xi=\cos\theta_k$}\\ =&-\frac{2\pi}{i r} \int_0^\infty \frac{k\,\mathrm{d}k}{k^2-(k_0^2+i\eta)}(e^{ikr}-e^{-ikr})\\ =&-\frac{2\pi}{i r}\int_{-\infty}^\infty \frac{k\,\mathrm{d}k}{k^2-(k_0^2+i\eta)}e^{ikr} \quad\text{--- use the upper semicircle as contour.} \\ =&-(2\pi)^2\frac{e^{ik_0r}}{2r} \end{align} It seems that the first integral is zero as pointed by @Fabian. As for the second integral, @Felix Martin makes a variable substitution and integrate out one dimension, it seems that it's hard to proceed and get an analytical form. Therefore, is it possible to get the asymptotic form of the second integral when $r\to\infty$? (i.e. keep only $1/r$ part, ignore $1/r^2$ etc.)","The first integral reads: $$ f(\vec{r})=\iiint_{-\infty}^\infty \mathrm{d}k_x \, \mathrm{d}k_y \, \mathrm{d}k_z \, \frac{e^{i\vec{k}\cdot\vec{r}}}{(k_0-\alpha k_z)^2-k^2+i(k_0-\alpha k_z)\eta} $$ the second integral is very similar, except the infinitesimal parts: $$ f(\vec{r})=\iiint_{-\infty}^\infty \mathrm{d}k_x \, \mathrm{d}k_y \, \mathrm{d}k_z \, \frac{e^{i\vec{k}\cdot\vec{r}}}{(k_0-\alpha k_z)^2-k^2+i\eta} $$ where $\alpha>1$, $k_0>0$ is finite positive number, $\eta = 0^+$ is the infinitesimal positive number. $\vec{k}=(k_x,k_y,k_z), \vec{r}=(x,y,z), k=\sqrt{k_x^2+k_y^2+k_z^2}$. As a hint , the integral for $\alpha=0$ can be done as following: \begin{align} &\int\mathrm{d}^3k \frac{e^{i\vec{k}\cdot\vec{r}}}{k_0^2-k^2+i\eta} \quad\text{--- making $\vec{r}$ as $z$ axis for $\vec{k}$ } \\ =& 2\pi\int_{-1}^1\mathrm{d}\xi\int_0^\infty \mathrm{d}k k^2\frac{e^{ikr\xi}}{k_0^2-k^2+i\eta}\quad\text{--- integral over $\phi_k$, and let $\xi=\cos\theta_k$}\\ =&-\frac{2\pi}{i r} \int_0^\infty \frac{k\,\mathrm{d}k}{k^2-(k_0^2+i\eta)}(e^{ikr}-e^{-ikr})\\ =&-\frac{2\pi}{i r}\int_{-\infty}^\infty \frac{k\,\mathrm{d}k}{k^2-(k_0^2+i\eta)}e^{ikr} \quad\text{--- use the upper semicircle as contour.} \\ =&-(2\pi)^2\frac{e^{ik_0r}}{2r} \end{align} It seems that the first integral is zero as pointed by @Fabian. As for the second integral, @Felix Martin makes a variable substitution and integrate out one dimension, it seems that it's hard to proceed and get an analytical form. Therefore, is it possible to get the asymptotic form of the second integral when $r\to\infty$? (i.e. keep only $1/r$ part, ignore $1/r^2$ etc.)",,"['integration', 'complex-analysis', 'definite-integrals', 'asymptotics', 'contour-integration']"
76,Integration of a cohomology class over a homology class.,Integration of a cohomology class over a homology class.,,"Can anyone explain to me what it is said in the following article : http://indico.ictp.it/event/a06114/material/0/0.pdf , page : $3$, by Mr. Aroldo Kaplan : The paragraph says : Stokes :   $$ \int_M d \omega = \int_{ \partial M }\omega $$   implies :   $$ d \omega = 0 \ \ \Longleftrightarrow \ \ \int_{ \mathrm{boundary} } \omega = 0 \ \ \ \ \ \ \ \ \  \mathrm{and} \ \ \ \ \ \ \ \ \partial M = 0 \ \ \Longleftrightarrow \ \ \int_M \mathrm{exact} = 0 $$   so, defining : $ H^k ( X ) = \dfrac{ \{ \omega \in \bigwedge^k \ : \ d \omega = 0 \} }{ \{ \omega \in \bigwedge^{k} \ : \ \omega = d \phi \} } = \dfrac{ \mathrm{closed} }{ \mathrm{exact} } $ , $ \ H_k ( X ) = \dfrac{ \{ M \subset X \ : \ \partial M = 0 \} }{ \{ M \subset X \ : \ M = \partial N \} } = \dfrac{ \mathrm{cycles} }{ \mathrm{boundaries} } $ the bilinear (function?):   $$ H^k ( X ) \times H_k ( X ) \to \mathbb{R}, \ \ \ \ \ \ \ \ \ \ \ \  ( [ \omega ] , [M] ) = \int_M \omega $$   ( = flux of $ \omega $ through $ M $ ) is well defined. So, I still do not understand why: $$ H^k ( X ) \times H_k ( X ) \to \mathbb{R}, \ \ \ \ \ \ \ \ \ \ \ \  ( [ \omega ] , [M] ) = \int_M \omega $$ is well defined. Can you explain to me that please ? Thanks in advance for your help.","Can anyone explain to me what it is said in the following article : http://indico.ictp.it/event/a06114/material/0/0.pdf , page : $3$, by Mr. Aroldo Kaplan : The paragraph says : Stokes :   $$ \int_M d \omega = \int_{ \partial M }\omega $$   implies :   $$ d \omega = 0 \ \ \Longleftrightarrow \ \ \int_{ \mathrm{boundary} } \omega = 0 \ \ \ \ \ \ \ \ \  \mathrm{and} \ \ \ \ \ \ \ \ \partial M = 0 \ \ \Longleftrightarrow \ \ \int_M \mathrm{exact} = 0 $$   so, defining : $ H^k ( X ) = \dfrac{ \{ \omega \in \bigwedge^k \ : \ d \omega = 0 \} }{ \{ \omega \in \bigwedge^{k} \ : \ \omega = d \phi \} } = \dfrac{ \mathrm{closed} }{ \mathrm{exact} } $ , $ \ H_k ( X ) = \dfrac{ \{ M \subset X \ : \ \partial M = 0 \} }{ \{ M \subset X \ : \ M = \partial N \} } = \dfrac{ \mathrm{cycles} }{ \mathrm{boundaries} } $ the bilinear (function?):   $$ H^k ( X ) \times H_k ( X ) \to \mathbb{R}, \ \ \ \ \ \ \ \ \ \ \ \  ( [ \omega ] , [M] ) = \int_M \omega $$   ( = flux of $ \omega $ through $ M $ ) is well defined. So, I still do not understand why: $$ H^k ( X ) \times H_k ( X ) \to \mathbb{R}, \ \ \ \ \ \ \ \ \ \ \ \  ( [ \omega ] , [M] ) = \int_M \omega $$ is well defined. Can you explain to me that please ? Thanks in advance for your help.",,"['integration', 'algebraic-topology', 'homology-cohomology']"
77,"Compute $\int_{0}^{2\pi}\frac{\sin^2\theta}{5+3\cos\theta}\,d\theta$",Compute,"\int_{0}^{2\pi}\frac{\sin^2\theta}{5+3\cos\theta}\,d\theta","I''m stuck in a exercise in complex analysis concerning integration of rational trigonometric functions. I have the solution but I don't understand a specific part. Here it goes: We want to find $\int_{0}^{2\pi}\frac{\sin^2\theta}{5+3\cos\theta}\,d\theta$. Let $z=e^{i\theta}$ so that $d\theta=dz/iz$, $\cos \theta = \frac12 (z+z^{-1})$ and $\sin \theta = \frac {1}{2i}(z-z^{-1})$.  We have $$\begin{align} I&=\int_0^{2\pi} \frac{\sin^2\theta}{5+3\cos \theta}d\theta\\\\ &=\oint_C \frac{-\frac14(z-z^{-1})^2}{5+3\frac12 (z+z^{-1})}\frac{dz}{iz}\\\\ &=\oint_C \frac{\frac i4 (z^2-2+z^{-2})}{5z+ \frac32 z^2 + \frac32}dz\\\\ &=\oint_C \frac{\frac i2 (z^2-2+z^{-2})}{3z^2 + 10z + 3}dz\\\\ &=\frac i2 \oint_C \frac{z^4-2z^2+1}{z^2(3z^2 + 10z + 3)}dz \quad (1) \end{align}$$ where $C$ is the unit circle in the complex $z$-plane. If I understand the last step correctly we multipliy numerator/denominator by $z^2$ in order to get rid of the negative power of $z$. Now for the part that I don't understand in the solution. It is said that $$(1) = \frac i2 \oint_C \frac{z^4-2z^2+1}{z^23(z+3)(z+\frac13)}dz = \frac i6 \oint_C \frac{z^4-2z^2+1}{z^2(z+3)(z+\frac13)}dz$$ Where does this $3$ come from in the denominator? If I factor out $3z^2 + 10z + 3$ I simply get $(z+3)(z+\frac13)$. What is even stranger to me is that, after the computation of residues, the author is getting the correct solution of $\frac{2\pi}{9}$ (I verified with Wolfram) while I'm getting $\frac{2\pi}{3}$  so I'm missing a factor of $\frac13$ (i.e. I'm missing that $3$ in the denominator). Can someone help me with this?","I''m stuck in a exercise in complex analysis concerning integration of rational trigonometric functions. I have the solution but I don't understand a specific part. Here it goes: We want to find $\int_{0}^{2\pi}\frac{\sin^2\theta}{5+3\cos\theta}\,d\theta$. Let $z=e^{i\theta}$ so that $d\theta=dz/iz$, $\cos \theta = \frac12 (z+z^{-1})$ and $\sin \theta = \frac {1}{2i}(z-z^{-1})$.  We have $$\begin{align} I&=\int_0^{2\pi} \frac{\sin^2\theta}{5+3\cos \theta}d\theta\\\\ &=\oint_C \frac{-\frac14(z-z^{-1})^2}{5+3\frac12 (z+z^{-1})}\frac{dz}{iz}\\\\ &=\oint_C \frac{\frac i4 (z^2-2+z^{-2})}{5z+ \frac32 z^2 + \frac32}dz\\\\ &=\oint_C \frac{\frac i2 (z^2-2+z^{-2})}{3z^2 + 10z + 3}dz\\\\ &=\frac i2 \oint_C \frac{z^4-2z^2+1}{z^2(3z^2 + 10z + 3)}dz \quad (1) \end{align}$$ where $C$ is the unit circle in the complex $z$-plane. If I understand the last step correctly we multipliy numerator/denominator by $z^2$ in order to get rid of the negative power of $z$. Now for the part that I don't understand in the solution. It is said that $$(1) = \frac i2 \oint_C \frac{z^4-2z^2+1}{z^23(z+3)(z+\frac13)}dz = \frac i6 \oint_C \frac{z^4-2z^2+1}{z^2(z+3)(z+\frac13)}dz$$ Where does this $3$ come from in the denominator? If I factor out $3z^2 + 10z + 3$ I simply get $(z+3)(z+\frac13)$. What is even stranger to me is that, after the computation of residues, the author is getting the correct solution of $\frac{2\pi}{9}$ (I verified with Wolfram) while I'm getting $\frac{2\pi}{3}$  so I'm missing a factor of $\frac13$ (i.e. I'm missing that $3$ in the denominator). Can someone help me with this?",,['integration']
78,Limit of the minimum value of an integral,Limit of the minimum value of an integral,,Let $$f(a)=\frac{1}{2}\int_{0}^{1}|ax^n-1|dx+\frac{1}{2}$$ Here $n$ is a natural number. Let $b_n$ be the minimum value of $f(a)$ for $a>1$. Evaluate $$\lim_{m \to \infty}b_mb_{m+1}\ldots b_{2m}$$ Some starters please. Thanks.,Let $$f(a)=\frac{1}{2}\int_{0}^{1}|ax^n-1|dx+\frac{1}{2}$$ Here $n$ is a natural number. Let $b_n$ be the minimum value of $f(a)$ for $a>1$. Evaluate $$\lim_{m \to \infty}b_mb_{m+1}\ldots b_{2m}$$ Some starters please. Thanks.,,['calculus']
79,L-series through integrals of rational functions,L-series through integrals of rational functions,,"Recently I stumbled upon this short proof here : $$L(1,\chi_2)=\sum_{j=0}^{+\infty}\left(\frac{1}{3j+1}-\frac{1}{3j+2}\right)=\int_{0}^{1}\frac{1-x}{1-x^3}\,dx=\int_{0}^{1}\frac{dx}{1+x+x^2}$$ so: $$\color{red}{L(1,\chi_2)}=\int_{0}^{1/2}\frac{dx}{x^2+3/4}=\frac{1}{\sqrt{3}}\arctan\sqrt{3}=\color{red}{\frac{\pi}{3\sqrt{3}}.}$$ So I was wondering: what if we have a polynomial $$Q(X)$$ in place of 1+x+x^2 in the denominator.Using partial fractions, the integral can be evaluated $$\sum_{j=0}^{\infty}a_n,$$ where  $$a_n = \sum_{j=0}^{k}-\frac{1}{\alpha_i^n.n.Q'(\alpha_i)}$$ In the case of $$Q=x^2+x+1$$ we have that $$a_n/n$$ is actually a character mod 3, sending 1mod 3 to 1, 2 mod 3 to -1 and 0 mod 3 to 0.Can this be generalised?Also, we needn't stop at having just one polynomial, we can have a polynomial P at the numerator.If we can do this, we can evaluate L-functions as integrals of rational functions.","Recently I stumbled upon this short proof here : $$L(1,\chi_2)=\sum_{j=0}^{+\infty}\left(\frac{1}{3j+1}-\frac{1}{3j+2}\right)=\int_{0}^{1}\frac{1-x}{1-x^3}\,dx=\int_{0}^{1}\frac{dx}{1+x+x^2}$$ so: $$\color{red}{L(1,\chi_2)}=\int_{0}^{1/2}\frac{dx}{x^2+3/4}=\frac{1}{\sqrt{3}}\arctan\sqrt{3}=\color{red}{\frac{\pi}{3\sqrt{3}}.}$$ So I was wondering: what if we have a polynomial $$Q(X)$$ in place of 1+x+x^2 in the denominator.Using partial fractions, the integral can be evaluated $$\sum_{j=0}^{\infty}a_n,$$ where  $$a_n = \sum_{j=0}^{k}-\frac{1}{\alpha_i^n.n.Q'(\alpha_i)}$$ In the case of $$Q=x^2+x+1$$ we have that $$a_n/n$$ is actually a character mod 3, sending 1mod 3 to 1, 2 mod 3 to -1 and 0 mod 3 to 0.Can this be generalised?Also, we needn't stop at having just one polynomial, we can have a polynomial P at the numerator.If we can do this, we can evaluate L-functions as integrals of rational functions.",,"['integration', 'complex-analysis', 'polynomials', 'dirichlet-series', 'l-functions']"
80,Proof of $\sum_{n=1}^\infty \frac{1}{n(n!)} = \int_0^1\frac{e^x-1}{x}dx$.,Proof of .,\sum_{n=1}^\infty \frac{1}{n(n!)} = \int_0^1\frac{e^x-1}{x}dx,"I want to prove that $$\sum_{n=1}^\infty \frac{1}{n\cdot n!} = \int_0^1\frac{e^x-1}{x} \,dx.$$ I tried this: $$g(x)= \begin{cases} \frac{e^x-1}{x} , x\neq0\\ 1,x=0\\ \end{cases}$$ $\lim_{n\to\infty}\frac{e^x-1}{x}=1$, so $g(x)$ is continuous in $[0,1]$. Therfore $$(1)\int_0^1g(x)dx=\int_0^1\frac{e^x-1}{x}dx$$ Next,  $$e^x-1=\sum_{n=1}^\infty\frac{x^n}{n!}$$ And this series converges uniformly in $[0,1]$. If $0\lt x\le1$ divide by $x$: $$\Rightarrow(2)\frac{e^x-1}{x}=\sum_{n=1}^\infty\frac{x^{n-1}}{n!}$$ In addition, $$(3)\sum_{n=1}^\infty\frac{0^{n-1}}{n!}=0^0=1$$ From $(2),(3)$ we can conclude that for every $x\in[0,1]$: $$(4)g(x)=\sum_{n=1}^\infty\frac{x^{n-1}}{n!}$$ This series converges uniformly in $[0,1]$, so from $(1),(4)$: $$\int_0^1\frac{e^x-1}{x} \, dx = \int_0^1 g(x) \, dx =\int_0^1\sum_{n=1}^\infty\frac{x^{n-1}}{n!} =\sum_{n=1}^\infty \int_0^1 \frac{x^{n-1}}{n!} =\sum_{n=1}^\infty\frac{1}{n\cdot n!}$$ I'm not so sure that all the steps i took in my proof are 100% correct,","I want to prove that $$\sum_{n=1}^\infty \frac{1}{n\cdot n!} = \int_0^1\frac{e^x-1}{x} \,dx.$$ I tried this: $$g(x)= \begin{cases} \frac{e^x-1}{x} , x\neq0\\ 1,x=0\\ \end{cases}$$ $\lim_{n\to\infty}\frac{e^x-1}{x}=1$, so $g(x)$ is continuous in $[0,1]$. Therfore $$(1)\int_0^1g(x)dx=\int_0^1\frac{e^x-1}{x}dx$$ Next,  $$e^x-1=\sum_{n=1}^\infty\frac{x^n}{n!}$$ And this series converges uniformly in $[0,1]$. If $0\lt x\le1$ divide by $x$: $$\Rightarrow(2)\frac{e^x-1}{x}=\sum_{n=1}^\infty\frac{x^{n-1}}{n!}$$ In addition, $$(3)\sum_{n=1}^\infty\frac{0^{n-1}}{n!}=0^0=1$$ From $(2),(3)$ we can conclude that for every $x\in[0,1]$: $$(4)g(x)=\sum_{n=1}^\infty\frac{x^{n-1}}{n!}$$ This series converges uniformly in $[0,1]$, so from $(1),(4)$: $$\int_0^1\frac{e^x-1}{x} \, dx = \int_0^1 g(x) \, dx =\int_0^1\sum_{n=1}^\infty\frac{x^{n-1}}{n!} =\sum_{n=1}^\infty \int_0^1 \frac{x^{n-1}}{n!} =\sum_{n=1}^\infty\frac{1}{n\cdot n!}$$ I'm not so sure that all the steps i took in my proof are 100% correct,",,"['real-analysis', 'integration', 'sequences-and-series', 'proof-verification']"
81,Solve $\frac{dx}{dt}=\frac{at-\cos{x}}{at^2\tan{x}+t}$,Solve,\frac{dx}{dt}=\frac{at-\cos{x}}{at^2\tan{x}+t},"Solve $\begin{align*}\frac{dx}{dt}=\frac{at-\cos{x}}{at^2\tan{x}+t}\end{align*}\\\\ $ Am I justified in doing the following substitution? If not, can a closed-form solution be found? Let $t=r\cos{x}$ and $dt=-r\sin{x}dx$ $\begin{align} \frac{dx}{dt}&=\frac{(ar-1)\cos{x}}{ar^2\cos^2{x}\tan{x}+r\cos{x}}\\\\ \frac{dx}{dt}&=\frac{(ar-1)\cos{x}}{(ar\sin{x}+1)r\cos{x}}\\\\ dx&=-\frac{(ar-1)}{(ar\sin{x}+1)r}r\sin{x}dx\\\\ 1&=-\frac{(ar-1)\sin{x}}{(ar\sin{x}+1)}\\\\ ar\sin{x}+1&=-(ar-1)\sin{x}\\\\ 1&=(1-2ar)\sin{x}\end{align}$ Therefore, $\begin{align} 1&=-(1-2ar)\frac{dt}{rdx}\\\\ \int{rdx}&=\int(2ar-1){dt}\\\\ x&=(2a-\frac{1}{r})t+c \end{align}$ EDIT: Here is some background on the above differential equation. The following equations describe the requirements of a curve $f$ which elastically reflects particles in a desired fashion, the details of which I will not cover. $\begin{equation}f(x,t)=t \tan{x}+\frac{a}{2}t^2 \sec^2{x}+h, \ \ \ \ \ 0<x<\pi \\\\ \frac{\partial f(x,t)}{\partial t}=\tan{\left(\frac{x}{2}-\frac{\pi}{4}\right)} \end{equation}$ where $x$ is t-dependent. We can differentiate the first equation above with respect to t, taking care to evaluate the derivative of $x$, as well. \begin{align*}\frac{\partial f\left(x, t\right)}{\partial t}&=\tan{x}+t\sec^2{x}\frac{dx}{dt}+at\sec^2{x}+at^2\tan{x}\sec^2{x}\frac{dx}{dt}\\\\ \tan{\left(\frac{x}{2}-\frac{\pi}{4}\right)}&=\tan{x}+\left[\left(t+a t^2\tan{x}\right)\frac{d x}{dt}+at\right]\sec^2{x}\\\\ \tan{x}-\sec{x}&=\tan{x}+\left[\left(t+a t^2\tan{x}\right)\frac{d x}{dt}+at\right]\sec^2{x}\\\\ \frac{dx}{dt}&=\frac{at-\cos{x}}{at^2\tan{x}+t}\\\\ \end{align*}","Solve $\begin{align*}\frac{dx}{dt}=\frac{at-\cos{x}}{at^2\tan{x}+t}\end{align*}\\\\ $ Am I justified in doing the following substitution? If not, can a closed-form solution be found? Let $t=r\cos{x}$ and $dt=-r\sin{x}dx$ $\begin{align} \frac{dx}{dt}&=\frac{(ar-1)\cos{x}}{ar^2\cos^2{x}\tan{x}+r\cos{x}}\\\\ \frac{dx}{dt}&=\frac{(ar-1)\cos{x}}{(ar\sin{x}+1)r\cos{x}}\\\\ dx&=-\frac{(ar-1)}{(ar\sin{x}+1)r}r\sin{x}dx\\\\ 1&=-\frac{(ar-1)\sin{x}}{(ar\sin{x}+1)}\\\\ ar\sin{x}+1&=-(ar-1)\sin{x}\\\\ 1&=(1-2ar)\sin{x}\end{align}$ Therefore, $\begin{align} 1&=-(1-2ar)\frac{dt}{rdx}\\\\ \int{rdx}&=\int(2ar-1){dt}\\\\ x&=(2a-\frac{1}{r})t+c \end{align}$ EDIT: Here is some background on the above differential equation. The following equations describe the requirements of a curve $f$ which elastically reflects particles in a desired fashion, the details of which I will not cover. $\begin{equation}f(x,t)=t \tan{x}+\frac{a}{2}t^2 \sec^2{x}+h, \ \ \ \ \ 0<x<\pi \\\\ \frac{\partial f(x,t)}{\partial t}=\tan{\left(\frac{x}{2}-\frac{\pi}{4}\right)} \end{equation}$ where $x$ is t-dependent. We can differentiate the first equation above with respect to t, taking care to evaluate the derivative of $x$, as well. \begin{align*}\frac{\partial f\left(x, t\right)}{\partial t}&=\tan{x}+t\sec^2{x}\frac{dx}{dt}+at\sec^2{x}+at^2\tan{x}\sec^2{x}\frac{dx}{dt}\\\\ \tan{\left(\frac{x}{2}-\frac{\pi}{4}\right)}&=\tan{x}+\left[\left(t+a t^2\tan{x}\right)\frac{d x}{dt}+at\right]\sec^2{x}\\\\ \tan{x}-\sec{x}&=\tan{x}+\left[\left(t+a t^2\tan{x}\right)\frac{d x}{dt}+at\right]\sec^2{x}\\\\ \frac{dx}{dt}&=\frac{at-\cos{x}}{at^2\tan{x}+t}\\\\ \end{align*}",,"['integration', 'ordinary-differential-equations', 'substitution']"
82,Unusual integral notation,Unusual integral notation,,"When I was learning analysis, I often wondered why I couldn't seem to find anything like $$\iint f(x) (dx)^2$$ in a standard calculus text, and concluded that it should be meaningless – even though, since we can differentiate functions multiple times, it would make sense that we can also integrate them repeatedly. But then I stumbled upon this blog entry by the creator of Mathematica , showing that Leibniz had similar notation in mind when he was developing the calculus, and found out about the differintegral operator, using which the above expression looks like $D^{-2}[f(x)]$. My question is, why don't we see this notation that often in basic analysis courses? What is the graphical meaning of such an expression – i.e. how would its behavior affect the shape of $f(x)$? And how would one solve it? Is there even a definite analog of it, and if so what is its geometrical meaning?","When I was learning analysis, I often wondered why I couldn't seem to find anything like $$\iint f(x) (dx)^2$$ in a standard calculus text, and concluded that it should be meaningless – even though, since we can differentiate functions multiple times, it would make sense that we can also integrate them repeatedly. But then I stumbled upon this blog entry by the creator of Mathematica , showing that Leibniz had similar notation in mind when he was developing the calculus, and found out about the differintegral operator, using which the above expression looks like $D^{-2}[f(x)]$. My question is, why don't we see this notation that often in basic analysis courses? What is the graphical meaning of such an expression – i.e. how would its behavior affect the shape of $f(x)$? And how would one solve it? Is there even a definite analog of it, and if so what is its geometrical meaning?",,"['calculus', 'integration', 'notation']"
83,Which is easier to integrate?,Which is easier to integrate?,,"My calculus teacher gave us this problem in class: Which is easier to integrate? $$\int \sin^{100}x\cos x dx$$ or $$\int \sin^{50}xdx$$ By easier, I assume the teacher means which integral would take less work. I'm unsure of how to approach this problem because of the relatively large exponents. I would guess the second because it has smaller exponents but I'm not sure.","My calculus teacher gave us this problem in class: Which is easier to integrate? $$\int \sin^{100}x\cos x dx$$ or $$\int \sin^{50}xdx$$ By easier, I assume the teacher means which integral would take less work. I'm unsure of how to approach this problem because of the relatively large exponents. I would guess the second because it has smaller exponents but I'm not sure.",,"['calculus', 'integration']"
84,"$\int_{a}^b f(x)dx=\int_{a}^b x f(x)dx=0$, how to prove there are more than two zeros in $[a,b]$",", how to prove there are more than two zeros in","\int_{a}^b f(x)dx=\int_{a}^b x f(x)dx=0 [a,b]","$f(x)$ is continuous on $[a,b]$ One zero point follows easy from Intermediate Value Theorem and $\int_{a}^b f(x)dx=0$, but similar trick does not work for more zeros.","$f(x)$ is continuous on $[a,b]$ One zero point follows easy from Intermediate Value Theorem and $\int_{a}^b f(x)dx=0$, but similar trick does not work for more zeros.",,"['integration', 'analysis']"
85,Asymptotic behavior of an integral involving the gamma function,Asymptotic behavior of an integral involving the gamma function,,"I'm trying to obtain an asymptotic large-$k$ approximation for the integral $$I(k) := e^{-k^2}\int_0^1 \frac{(1 + \xi^2)\Gamma(0, \xi^2 k^2) - 2\Gamma(0, k^2)}{1 - \xi} d\xi$$ where $\Gamma$ is the incomplete gamma function $$\Gamma(0, x) = \int_x^\infty \frac{e^{-t}}{t}\,dt$$ (also expressible in terms of the exponential integral ). Specifically, I would like to show that as $k\to\infty$, $I(k)$ becomes negligible with respect to $e^{-k^2}\ln k^2$. My first thought was to use the series expansion $$\Gamma(0, z) = e^{-z}\biggl(\frac{1}{z} - \frac{1}{z^2} + \cdots\biggr)$$ but the problem is that $\xi^2 k^2$ goes all the way down to zero, so it takes values where this series will converge arbitrarily slowly. It might be possible to show that the region of the integral where this effect is significant is small enough not to matter, but I can't think of a way to do that. I also thought to do a series expansion of the integrand around $\xi = 1$, or $\xi = 0$, and integrate that, or perhaps to split the integral in half and use a different series for each. But the series coefficients I obtained (using Mathematica) depend on positive powers of $k$, increasing with each order of $\xi$ (or $1 - \xi$), which does not bode well for convergence. Inspired by another question , I tried transforming the expression into a form that would allow me to apply Watson's lemma , but I couldn't find a transformation that would satisfy the conditions. So, can anyone offer a way to derive the asymptotic behavior? It seems like it shouldn't be that hard, but I'm too tired to keep thinking about this at the moment. For what it's worth, numerical evaluation suggests that $$I(k) \sim \frac{C e^{-k^2}}{k}$$ where $C = 1.77245\ldots \overset{?}{=} \sqrt{\pi}$. But I would much prefer to have a symbolic demonstration of this, and not just rely on numerics.","I'm trying to obtain an asymptotic large-$k$ approximation for the integral $$I(k) := e^{-k^2}\int_0^1 \frac{(1 + \xi^2)\Gamma(0, \xi^2 k^2) - 2\Gamma(0, k^2)}{1 - \xi} d\xi$$ where $\Gamma$ is the incomplete gamma function $$\Gamma(0, x) = \int_x^\infty \frac{e^{-t}}{t}\,dt$$ (also expressible in terms of the exponential integral ). Specifically, I would like to show that as $k\to\infty$, $I(k)$ becomes negligible with respect to $e^{-k^2}\ln k^2$. My first thought was to use the series expansion $$\Gamma(0, z) = e^{-z}\biggl(\frac{1}{z} - \frac{1}{z^2} + \cdots\biggr)$$ but the problem is that $\xi^2 k^2$ goes all the way down to zero, so it takes values where this series will converge arbitrarily slowly. It might be possible to show that the region of the integral where this effect is significant is small enough not to matter, but I can't think of a way to do that. I also thought to do a series expansion of the integrand around $\xi = 1$, or $\xi = 0$, and integrate that, or perhaps to split the integral in half and use a different series for each. But the series coefficients I obtained (using Mathematica) depend on positive powers of $k$, increasing with each order of $\xi$ (or $1 - \xi$), which does not bode well for convergence. Inspired by another question , I tried transforming the expression into a form that would allow me to apply Watson's lemma , but I couldn't find a transformation that would satisfy the conditions. So, can anyone offer a way to derive the asymptotic behavior? It seems like it shouldn't be that hard, but I'm too tired to keep thinking about this at the moment. For what it's worth, numerical evaluation suggests that $$I(k) \sim \frac{C e^{-k^2}}{k}$$ where $C = 1.77245\ldots \overset{?}{=} \sqrt{\pi}$. But I would much prefer to have a symbolic demonstration of this, and not just rely on numerics.",,"['integration', 'asymptotics', 'gamma-function']"
86,Integrating Dirichlet Distribution,Integrating Dirichlet Distribution,,"Let's say that $(X_1, \dots, X_4) \sim Dirichlet(\alpha_1, \dots, \alpha_4)$ with $\sum_{i=1}^4 X_i=1$. I want to find the distribution of $(X_1, X_2)$. I know the marginal distributions of the $X_i$ (answered in other questions on this site) but it doesn't seem that this has been answered. Denote the full joint pdf by $f_0$ so that $f_0(x_1, x_2, x_3, x_4) = \frac{1}{B(\vec \alpha)} \prod_{i=1}^4 x_i^{\alpha_i-1}$. I'll denote the joint pdf of $(X_1, X_2)$ by $g$. First of all, since $X_4 = 1 - X_1 - X_2 - X_3$ it seems that I should only consider $X_1$, $X_2$, and $X_3$ and therefore I'll only need to do a single integration. This means that I'll be working with  $$ f(x_1, x_2, x_3) := \frac{x_1^{\alpha_1 - 1} x_2^{\alpha_2 - 1} x_3^{\alpha_3 - 1}(1-x_1-x_2-x_3)^{\alpha_4-1}}{B(\vec \alpha)}. $$ From this it follows that $$ g(x_1, x_2) = \int_{x_3}  f(x_1, x_2, x_3) dx_3 $$ $$ = \frac{x_1^{\alpha_1-1} x_2^{\alpha_2-1}}{B(\vec \alpha)} \int_{x_3}  x_3^{\alpha_3-1} (1-x_1-x_2-x_3)^{\alpha_4-1} dx_3. $$ My questions: What are the limits of integration here? How do I actually do this integral? I assume that I need to shoehorn it into a beta integral but I don't see how. For the limits of integration, certainly $x_3 \geq 0$, but I don't know what the upper bound is. Would it just be $0 \leq x_3 \leq 1 - x_1 - x_2$? As for the actual integral, if I'm correct about $0 \leq x_3 \leq 1 - x_1 - x_2$ then I just need to be able to do $$ I = \int_{0}^{1-k} t^{\alpha-1}(1-k-t)^{\beta-1} dt $$ where $k = x_1 + x_2$, $\alpha = \alpha_3$, and $\beta = \alpha_4$. This looks really close to an incomplete beta but not quite.","Let's say that $(X_1, \dots, X_4) \sim Dirichlet(\alpha_1, \dots, \alpha_4)$ with $\sum_{i=1}^4 X_i=1$. I want to find the distribution of $(X_1, X_2)$. I know the marginal distributions of the $X_i$ (answered in other questions on this site) but it doesn't seem that this has been answered. Denote the full joint pdf by $f_0$ so that $f_0(x_1, x_2, x_3, x_4) = \frac{1}{B(\vec \alpha)} \prod_{i=1}^4 x_i^{\alpha_i-1}$. I'll denote the joint pdf of $(X_1, X_2)$ by $g$. First of all, since $X_4 = 1 - X_1 - X_2 - X_3$ it seems that I should only consider $X_1$, $X_2$, and $X_3$ and therefore I'll only need to do a single integration. This means that I'll be working with  $$ f(x_1, x_2, x_3) := \frac{x_1^{\alpha_1 - 1} x_2^{\alpha_2 - 1} x_3^{\alpha_3 - 1}(1-x_1-x_2-x_3)^{\alpha_4-1}}{B(\vec \alpha)}. $$ From this it follows that $$ g(x_1, x_2) = \int_{x_3}  f(x_1, x_2, x_3) dx_3 $$ $$ = \frac{x_1^{\alpha_1-1} x_2^{\alpha_2-1}}{B(\vec \alpha)} \int_{x_3}  x_3^{\alpha_3-1} (1-x_1-x_2-x_3)^{\alpha_4-1} dx_3. $$ My questions: What are the limits of integration here? How do I actually do this integral? I assume that I need to shoehorn it into a beta integral but I don't see how. For the limits of integration, certainly $x_3 \geq 0$, but I don't know what the upper bound is. Would it just be $0 \leq x_3 \leq 1 - x_1 - x_2$? As for the actual integral, if I'm correct about $0 \leq x_3 \leq 1 - x_1 - x_2$ then I just need to be able to do $$ I = \int_{0}^{1-k} t^{\alpha-1}(1-k-t)^{\beta-1} dt $$ where $k = x_1 + x_2$, $\alpha = \alpha_3$, and $\beta = \alpha_4$. This looks really close to an incomplete beta but not quite.",,"['integration', 'probability-distributions']"
87,Taking inverse Fourier transform of $\frac{\sin^2(\pi s)}{(\pi s)^2}$ [duplicate],Taking inverse Fourier transform of  [duplicate],\frac{\sin^2(\pi s)}{(\pi s)^2},"This question already has answers here : Evaluating the integral $\int_{-\infty}^\infty \frac{\sin^2(x)}{x^2}e^{i t x} dx$ (5 answers) Closed 8 years ago . How do I show that $$\int_{-\infty}^\infty \frac{\sin^2(\pi s)}{(\pi s)^2} e^{2\pi isx} \, ds = \begin{cases} 1+x & \text{if }-1 \le x \le 0 \\ 1-x & \text{if }0 \le x \le 1 \\ 0 & \text{otherwise} \end{cases}$$ I know that $\sin^2(\pi s)=\frac{1-\cos(2\pi s)}{2}=\frac{1-(e^{2\pi i s}-e^{-2\pi i s}))/2}{2}$, so $$\int_{-\infty}^\infty \frac{\sin^2(\pi s)}{(\pi s)^2} e^{2\pi isx} \, ds=2\int_0^\infty \frac{2e^{2\pi isx}-(e^{2\pi is(1+x)}+e^{2\pi i s(-1+x)})}{4\pi^2s^2} \, ds$$ I am also allowed to use the known identity $$\int_{-\infty}^\infty \frac{1-\cos(a \pi x)}{(\pi x)^2} \, dx = |a|$$ for some real number $a$.","This question already has answers here : Evaluating the integral $\int_{-\infty}^\infty \frac{\sin^2(x)}{x^2}e^{i t x} dx$ (5 answers) Closed 8 years ago . How do I show that $$\int_{-\infty}^\infty \frac{\sin^2(\pi s)}{(\pi s)^2} e^{2\pi isx} \, ds = \begin{cases} 1+x & \text{if }-1 \le x \le 0 \\ 1-x & \text{if }0 \le x \le 1 \\ 0 & \text{otherwise} \end{cases}$$ I know that $\sin^2(\pi s)=\frac{1-\cos(2\pi s)}{2}=\frac{1-(e^{2\pi i s}-e^{-2\pi i s}))/2}{2}$, so $$\int_{-\infty}^\infty \frac{\sin^2(\pi s)}{(\pi s)^2} e^{2\pi isx} \, ds=2\int_0^\infty \frac{2e^{2\pi isx}-(e^{2\pi is(1+x)}+e^{2\pi i s(-1+x)})}{4\pi^2s^2} \, ds$$ I am also allowed to use the known identity $$\int_{-\infty}^\infty \frac{1-\cos(a \pi x)}{(\pi x)^2} \, dx = |a|$$ for some real number $a$.",,"['integration', 'fourier-analysis']"
88,integration of $\ln \ln x$,integration of,\ln \ln x,"I would like to compute the following integral : $$\int_{2}^{\frac{\ln a}{\ln \ln a}} \ln \ln x \, \mathrm{d}x$$ where $a$ is a positive constant. Is this possible ?","I would like to compute the following integral : $$\int_{2}^{\frac{\ln a}{\ln \ln a}} \ln \ln x \, \mathrm{d}x$$ where $a$ is a positive constant. Is this possible ?",,"['calculus', 'integration', 'definite-integrals', 'logarithms']"
89,Absolute value in integrating factor of First-Order Linear Differential Equation,Absolute value in integrating factor of First-Order Linear Differential Equation,,"Question states: $$ y' + \frac{y}{x} = 6x+2$$ Obviously x cannot be zero. If we assume that $x$ is positive (i.e. $x>0$), we find the integrating factor as $$u(x)=e^{\int \frac{1}{x} dx}$$ which is equal to $x$. Then the solution is $$y(x)= \frac{1}{u(x)} \int (6x+2)(u(x)) dx = \frac {1}{x} \int 6x^2+2x \ dx = 2x^2+x+\frac{C}{x}.$$ Now, we assumed that $x$ is positive. But I couldn't get the same answer when I didn't make this assumption; that is, the integrating factor is $$u(x)=e^{\int \frac{1}{x} dx} = e^{ \ln \lvert x\rvert} = \lvert x\rvert.$$ Then this problem gets way more complicated, as the solution becomes $$y(x)= \frac{1}{u(x)} \int (6x+2)(u(x)) dx = \frac {1}{\lvert x\rvert} \int 6x \lvert x\rvert +2\lvert x\rvert \ dx.$$ My calculus textbook omitted the absolute value altogether; that is, the textbook indicated that the integrating factor was just $x$. Because the textbook is written by quite reputable and trustworthy authors (Ron Larson and Bruce Edwards), I was wondering (1) if treating the integrating factor as just $x$ is acceptable, and/or (2) How the solution is still correct if we must use the integrating factor as $\lvert x\rvert$. If we can omit the absolute value sometimes, how do we know when we can omit the absolute value sign and when we shouldn't? (As a side note, I fully understand why there's absolute value sign for the antidervative of $ \frac{1}{x} $).","Question states: $$ y' + \frac{y}{x} = 6x+2$$ Obviously x cannot be zero. If we assume that $x$ is positive (i.e. $x>0$), we find the integrating factor as $$u(x)=e^{\int \frac{1}{x} dx}$$ which is equal to $x$. Then the solution is $$y(x)= \frac{1}{u(x)} \int (6x+2)(u(x)) dx = \frac {1}{x} \int 6x^2+2x \ dx = 2x^2+x+\frac{C}{x}.$$ Now, we assumed that $x$ is positive. But I couldn't get the same answer when I didn't make this assumption; that is, the integrating factor is $$u(x)=e^{\int \frac{1}{x} dx} = e^{ \ln \lvert x\rvert} = \lvert x\rvert.$$ Then this problem gets way more complicated, as the solution becomes $$y(x)= \frac{1}{u(x)} \int (6x+2)(u(x)) dx = \frac {1}{\lvert x\rvert} \int 6x \lvert x\rvert +2\lvert x\rvert \ dx.$$ My calculus textbook omitted the absolute value altogether; that is, the textbook indicated that the integrating factor was just $x$. Because the textbook is written by quite reputable and trustworthy authors (Ron Larson and Bruce Edwards), I was wondering (1) if treating the integrating factor as just $x$ is acceptable, and/or (2) How the solution is still correct if we must use the integrating factor as $\lvert x\rvert$. If we can omit the absolute value sometimes, how do we know when we can omit the absolute value sign and when we shouldn't? (As a side note, I fully understand why there's absolute value sign for the antidervative of $ \frac{1}{x} $).",,[]
90,"Use Cauchy's Theorem to show that if $\int_{0}^{\infty}f(x)dx$ exists, then so does $\int_{L}f(z)dz$","Use Cauchy's Theorem to show that if  exists, then so does",\int_{0}^{\infty}f(x)dx \int_{L}f(z)dz,"Suppose that $f(z)$ is analytic at every point of the closed domain $0 \leq arg z \leq \alpha$ $(0 \leq \alpha \leq 2 \pi)$, and that $\lim_{z \to \infty}z f(z) = 0$. I need to prove that if the integral $\displaystyle J_{1}=\int_{0}^{\infty}f(x) dx$ exists, then the integral $\displaystyle J_{2}=\int_{L}f(z)dz$, where $L$ is the ray $z=r e^{i \alpha}$, $0 \leq r \leq \infty$. Moreover, I need to show that $J_{1} = J_{2}$ I have been given the hint to use Cauchy's Theorem (not the Cauchy integral formula or residues - answers using either of those things are useless to me), and the result of the previous problem, which states as follows: If $f(z)$ is continuous in the closed domain $|z|\geq R_{0}$, $0 \leq arg z \leq \alpha$ $(0 \leq \alpha \leq 2 \pi)$, and if the limit $\displaystyle \lim_{z \to \infty} zf(z) = A$ exists, then $\displaystyle \lim_{R \to \infty}\int_{\displaystyle \Gamma_{R}}f(z)dz = i A \alpha$, where $\Gamma_{R}$ is the arc of the circle $|z|=R$ lying in the given domain. So, for this problem, I can use the fact that $\lim_{z \to \infty}zf(z) = 0$ to show that $\displaystyle \lim_{r \to \infty}\int_{\displaystyle \Gamma_{r}}f(z)dz = 0$ at some point, I guess. Thus far, I've tried approaching this problem in two different ways. The first way was to start out with $J_{2} = \int_{L}f(z)dz$ and then try to get $L_{1}$ to pop out somewhere. Didn't get too far with that, and anyway, I'm not sure that it is correct to write $\int_{L}f(z)dz = \lim_{r \to \infty}\int_{0}^{2\pi}f(re^{i\alpha})ire^{i \alpha}d \alpha$. All of these angles and args are confusing me, and I'm not even entirely sure what the domain on which $f(z)$ is analytic looks like. The second way was to start out with $J_{1} = \int_{0}^{\infty}f(x) dx$, and try to parametrize it in terms of $z = re^{i \alpha}$. But, I'm not sure exactly how to do this (again, the domain is confusing. Tried to draw it; didn't help. Maybe I'm just not visualizing it right). Then, at some point, I assume I can apply Cauchy's Theorem and the given limit. I'm guessing that since Cauchy's Theorem is involved and that the given limit goes to $0$, I'm probably going to wind up with $0 = J_{1} = J_{2}$, but I need a lot of help and guidance to show this. I'm at my wits end, don't have a lot of time to figure this out, and am starting to panic. Please help.","Suppose that $f(z)$ is analytic at every point of the closed domain $0 \leq arg z \leq \alpha$ $(0 \leq \alpha \leq 2 \pi)$, and that $\lim_{z \to \infty}z f(z) = 0$. I need to prove that if the integral $\displaystyle J_{1}=\int_{0}^{\infty}f(x) dx$ exists, then the integral $\displaystyle J_{2}=\int_{L}f(z)dz$, where $L$ is the ray $z=r e^{i \alpha}$, $0 \leq r \leq \infty$. Moreover, I need to show that $J_{1} = J_{2}$ I have been given the hint to use Cauchy's Theorem (not the Cauchy integral formula or residues - answers using either of those things are useless to me), and the result of the previous problem, which states as follows: If $f(z)$ is continuous in the closed domain $|z|\geq R_{0}$, $0 \leq arg z \leq \alpha$ $(0 \leq \alpha \leq 2 \pi)$, and if the limit $\displaystyle \lim_{z \to \infty} zf(z) = A$ exists, then $\displaystyle \lim_{R \to \infty}\int_{\displaystyle \Gamma_{R}}f(z)dz = i A \alpha$, where $\Gamma_{R}$ is the arc of the circle $|z|=R$ lying in the given domain. So, for this problem, I can use the fact that $\lim_{z \to \infty}zf(z) = 0$ to show that $\displaystyle \lim_{r \to \infty}\int_{\displaystyle \Gamma_{r}}f(z)dz = 0$ at some point, I guess. Thus far, I've tried approaching this problem in two different ways. The first way was to start out with $J_{2} = \int_{L}f(z)dz$ and then try to get $L_{1}$ to pop out somewhere. Didn't get too far with that, and anyway, I'm not sure that it is correct to write $\int_{L}f(z)dz = \lim_{r \to \infty}\int_{0}^{2\pi}f(re^{i\alpha})ire^{i \alpha}d \alpha$. All of these angles and args are confusing me, and I'm not even entirely sure what the domain on which $f(z)$ is analytic looks like. The second way was to start out with $J_{1} = \int_{0}^{\infty}f(x) dx$, and try to parametrize it in terms of $z = re^{i \alpha}$. But, I'm not sure exactly how to do this (again, the domain is confusing. Tried to draw it; didn't help. Maybe I'm just not visualizing it right). Then, at some point, I assume I can apply Cauchy's Theorem and the given limit. I'm guessing that since Cauchy's Theorem is involved and that the given limit goes to $0$, I'm probably going to wind up with $0 = J_{1} = J_{2}$, but I need a lot of help and guidance to show this. I'm at my wits end, don't have a lot of time to figure this out, and am starting to panic. Please help.",,"['integration', 'complex-analysis']"
91,Solving $\int {\frac{1}{(-x^{2}+6x-5)^{1/2}}}\;dx$ such that $(1\lt x\lt5)$ using trigonometric substitutions and pythagorean identities,Solving  such that  using trigonometric substitutions and pythagorean identities,\int {\frac{1}{(-x^{2}+6x-5)^{1/2}}}\;dx (1\lt x\lt5),"First post so I'll get right to the question; $$\int {\frac{1}{(-x^{2}+6x-5)^{1/2}}}\;dx,\qquad(1\lt x\lt5)$$ To begin with I completed the square which yields: $$-(x-3)^{2}+4$$ substituting this completed square into the integrand yields: $$\int {\frac{1}{(-(x-3)^{2}+4)^{1/2}}}\,\,dx$$ solving by substituting the trig substitution: let $$x = 3+2\tan u$$ hence: $$\frac{dx}{du} = 2\sec^{2}u$$ thus: $$dx = (2\sec^{2}u) du$$ Rearranging $$x = 3+2\tan u$$ yields $$u=\tan^{-1}(\frac{x-3}{2})$$ Now rewriting the integral using the value of x yields: $$\int {\frac{1}{(-(3+2\tan u-3)^{2}+4)^{1/2}}}\,\,\,(2\sec^{2}u) du$$ which yields:$$\int {\frac{1}{(-(2\tan u)^{2}+4)^{1/2}}}\,\,(2\sec^{2}u) du$$ which is the same as saying: $$\int {\frac{1}{(-4\tan^{2}u+4)^{1/2}}}\,\,\,(2\sec^{2}u) du$$ which can be rearranged to give: $$\int {\frac{1}{(4-4\tan^{2}u)^{1/2}}}\,\,\,(2\sec^{2}u) du$$ Applying the constant multiple rule to the integral yields: $$\frac 12\int {\frac{1}{(1-\tan^{2}u)^{1/2}}}\,\,\,(\sec^{2}u) du$$ Now we know that pythagorean identity: $$\tan^{2}u=\sec^{2}u-1$$ so substituting I end up with an indefinite integral that looks like this: $$\frac 12\int {\frac{1}{(1-\sec^{2}u+1)^{1/2}}}\,\,\,(\sec^{2}u) du$$ which gives: $$\frac 12\int {\frac{1}{(2-\sec^{2}u)^{1/2}}}\,\,\,(\sec^{2}u) du$$ However this is obviously wrong because I think it is unsolvable. So where did I veer from the right path?","First post so I'll get right to the question; $$\int {\frac{1}{(-x^{2}+6x-5)^{1/2}}}\;dx,\qquad(1\lt x\lt5)$$ To begin with I completed the square which yields: $$-(x-3)^{2}+4$$ substituting this completed square into the integrand yields: $$\int {\frac{1}{(-(x-3)^{2}+4)^{1/2}}}\,\,dx$$ solving by substituting the trig substitution: let $$x = 3+2\tan u$$ hence: $$\frac{dx}{du} = 2\sec^{2}u$$ thus: $$dx = (2\sec^{2}u) du$$ Rearranging $$x = 3+2\tan u$$ yields $$u=\tan^{-1}(\frac{x-3}{2})$$ Now rewriting the integral using the value of x yields: $$\int {\frac{1}{(-(3+2\tan u-3)^{2}+4)^{1/2}}}\,\,\,(2\sec^{2}u) du$$ which yields:$$\int {\frac{1}{(-(2\tan u)^{2}+4)^{1/2}}}\,\,(2\sec^{2}u) du$$ which is the same as saying: $$\int {\frac{1}{(-4\tan^{2}u+4)^{1/2}}}\,\,\,(2\sec^{2}u) du$$ which can be rearranged to give: $$\int {\frac{1}{(4-4\tan^{2}u)^{1/2}}}\,\,\,(2\sec^{2}u) du$$ Applying the constant multiple rule to the integral yields: $$\frac 12\int {\frac{1}{(1-\tan^{2}u)^{1/2}}}\,\,\,(\sec^{2}u) du$$ Now we know that pythagorean identity: $$\tan^{2}u=\sec^{2}u-1$$ so substituting I end up with an indefinite integral that looks like this: $$\frac 12\int {\frac{1}{(1-\sec^{2}u+1)^{1/2}}}\,\,\,(\sec^{2}u) du$$ which gives: $$\frac 12\int {\frac{1}{(2-\sec^{2}u)^{1/2}}}\,\,\,(\sec^{2}u) du$$ However this is obviously wrong because I think it is unsolvable. So where did I veer from the right path?",,"['calculus', 'integration', 'trigonometry', 'indefinite-integrals']"
92,Integral with trigonometric function,Integral with trigonometric function,,"I have a problem with this integral $$\int_\ \frac{\sin 2x }{  \sqrt{4-\cos^2 x}} \, dx$$ We can transform it to $$\int_\ \frac{2\sin x \cos x }{  \sqrt{4-\cos^2 x}} \, dx$$ Using substitution $u^2 = 4 - \cos^2 x $ we get $$\int_\ \frac{2u }{\ u } \, du$$ And it gives bad result. Can you point when did i make the mistake ?","I have a problem with this integral $$\int_\ \frac{\sin 2x }{  \sqrt{4-\cos^2 x}} \, dx$$ We can transform it to $$\int_\ \frac{2\sin x \cos x }{  \sqrt{4-\cos^2 x}} \, dx$$ Using substitution $u^2 = 4 - \cos^2 x $ we get $$\int_\ \frac{2u }{\ u } \, du$$ And it gives bad result. Can you point when did i make the mistake ?",,['integration']
93,Using 1-forms to integrate along curves,Using 1-forms to integrate along curves,,"The following is written in my lecture notes: 'Suppose that $\gamma:[a,b]\rightarrow\mathcal{M}$ is a smooth curve and $\omega$ is a 1-form on $\mathcal{M}$. Then we get a smooth function $[t\rightarrow(\omega(\gamma(t)))(\gamma'(t))]$, evaluating $\omega(\gamma(t)))\in{T}^*_{\gamma(t)}\mathcal{M}$ at the vector $\gamma'(t)\in{T}_{\gamma(t)}\mathcal{M}$. We write $\int_\gamma\omega=\int_a^b{(\omega(\gamma(t)))(\gamma'(t))}dt$.' As an example, I'm trying to show that for a smooth function $f\in{C^{\infty}}(\mathcal{M})$, $\int_\gamma{df}=f(\gamma(b))-f(\gamma(a))$. This is my first attempt at using 1-forms in integration, and whilst I understand the definition above to the extent of what is being mapped where, I don't have much intuition yet and so am quite lost. Anyway, I have the fact that $df=\sum_{i=1}^{m}\frac{\partial{f}}{\partial{x_i}}dx_i$ and I've substituted this into the expression above to get: \begin{equation} \int_\gamma{df}=\int_a^b\bigg{(}\sum_{i=1}^m{\frac{\partial{f}}{\partial{x_i}}dx_i}(\gamma(t))\bigg{)}(\gamma'(t))dt \end{equation} I'm aware there are a few similar questions that have been asked, but they all use slightly different notation and I hope that seeing what do with this specific example would clear up the general theory for me as well. Any advice on how to proceed would be much appreciated!","The following is written in my lecture notes: 'Suppose that $\gamma:[a,b]\rightarrow\mathcal{M}$ is a smooth curve and $\omega$ is a 1-form on $\mathcal{M}$. Then we get a smooth function $[t\rightarrow(\omega(\gamma(t)))(\gamma'(t))]$, evaluating $\omega(\gamma(t)))\in{T}^*_{\gamma(t)}\mathcal{M}$ at the vector $\gamma'(t)\in{T}_{\gamma(t)}\mathcal{M}$. We write $\int_\gamma\omega=\int_a^b{(\omega(\gamma(t)))(\gamma'(t))}dt$.' As an example, I'm trying to show that for a smooth function $f\in{C^{\infty}}(\mathcal{M})$, $\int_\gamma{df}=f(\gamma(b))-f(\gamma(a))$. This is my first attempt at using 1-forms in integration, and whilst I understand the definition above to the extent of what is being mapped where, I don't have much intuition yet and so am quite lost. Anyway, I have the fact that $df=\sum_{i=1}^{m}\frac{\partial{f}}{\partial{x_i}}dx_i$ and I've substituted this into the expression above to get: \begin{equation} \int_\gamma{df}=\int_a^b\bigg{(}\sum_{i=1}^m{\frac{\partial{f}}{\partial{x_i}}dx_i}(\gamma(t))\bigg{)}(\gamma'(t))dt \end{equation} I'm aware there are a few similar questions that have been asked, but they all use slightly different notation and I hope that seeing what do with this specific example would clear up the general theory for me as well. Any advice on how to proceed would be much appreciated!",,"['integration', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
94,How to solve the differential equation: $y'=\sqrt{|y|}$,How to solve the differential equation:,y'=\sqrt{|y|},"While trying to solve the differential equation: $y'=\sqrt{|y|}$. I got confused how to deal with the absolute value. I want to draw a sketch for the direction field for that equation, and see for what initial values does this equation fulfill the conditions of the existence and uniqueness Theorem. I tried to do the integral according to the sign, depends if $(y>0)$ or $(y<0)$. but I'm still not sure if my result is right. result that I got: while $y>0$ : $y=({\frac{x}{2}+c})^{2}$ while $y<0$ : $y=({-\frac{x}{2}+c})^{2}$ if that's the case, Its hard for me to imagine the Direction field by these two equations, because they are always above axis: $x$. How does it look? any hints?","While trying to solve the differential equation: $y'=\sqrt{|y|}$. I got confused how to deal with the absolute value. I want to draw a sketch for the direction field for that equation, and see for what initial values does this equation fulfill the conditions of the existence and uniqueness Theorem. I tried to do the integral according to the sign, depends if $(y>0)$ or $(y<0)$. but I'm still not sure if my result is right. result that I got: while $y>0$ : $y=({\frac{x}{2}+c})^{2}$ while $y<0$ : $y=({-\frac{x}{2}+c})^{2}$ if that's the case, Its hard for me to imagine the Direction field by these two equations, because they are always above axis: $x$. How does it look? any hints?",,"['calculus', 'integration', 'ordinary-differential-equations']"
95,Calculating an Exponential Integral,Calculating an Exponential Integral,,"Calculate $\int_0^{\infty} \frac{x^{2N+1}}{a+x^{-b}} e^{-c x^2} dx$ where $a,c > 0$ and $b>1$. The best I could do about this integral is to find an upperbound for it: $\int_0^{\infty} \frac{x^{2N+1}}{a+x^{-b}} e^{-c x^2} dx \leq \int_0^{\infty} \frac{x^{2N+1}}{x^{-b}} e^{-c x^2} dx= \int_0^{\infty} x^{2N+b+1} e^{-c x^2} dx$ Then I can use $\int_{0}^{\infty} x^{n} e^{-ax^2}\,\mathrm{d}x =  \begin{cases}        \frac{1}{2}\Gamma \left(\frac{n+1}{2}\right)/a^{\frac{n+1}{2}} & (n>-1,a>0) \\        \frac{(2k-1)!!}{2^{k+1}a^k}\sqrt{\frac{\pi}{a}} & (n=2k, k \;\text{integer}, a>0) \\        \frac{k!}{2a^{k+1}} & (n=2k+1,k \;\text{integer}, a>0) \end{cases} $ Any idea that how I can calculate the integral, not jut an upper bound?","Calculate $\int_0^{\infty} \frac{x^{2N+1}}{a+x^{-b}} e^{-c x^2} dx$ where $a,c > 0$ and $b>1$. The best I could do about this integral is to find an upperbound for it: $\int_0^{\infty} \frac{x^{2N+1}}{a+x^{-b}} e^{-c x^2} dx \leq \int_0^{\infty} \frac{x^{2N+1}}{x^{-b}} e^{-c x^2} dx= \int_0^{\infty} x^{2N+b+1} e^{-c x^2} dx$ Then I can use $\int_{0}^{\infty} x^{n} e^{-ax^2}\,\mathrm{d}x =  \begin{cases}        \frac{1}{2}\Gamma \left(\frac{n+1}{2}\right)/a^{\frac{n+1}{2}} & (n>-1,a>0) \\        \frac{(2k-1)!!}{2^{k+1}a^k}\sqrt{\frac{\pi}{a}} & (n=2k, k \;\text{integer}, a>0) \\        \frac{k!}{2a^{k+1}} & (n=2k+1,k \;\text{integer}, a>0) \end{cases} $ Any idea that how I can calculate the integral, not jut an upper bound?",,"['integration', 'definite-integrals', 'normal-distribution', 'gaussian-integral']"
96,Convert Riemann sum to a definite integral: $\lim_{n\to \infty} \sum_{k=1}^n\sqrt{{1\over n^2}\left(1+{2k\over n}\right)} $,Convert Riemann sum to a definite integral:,\lim_{n\to \infty} \sum_{k=1}^n\sqrt{{1\over n^2}\left(1+{2k\over n}\right)} ,"I'm between 2 answers for this question, but I am not sure if either of them are right. $$\lim_{n\to \infty} \sum_{k=1}^n\sqrt{{1\over n^2}\left(1+{2k\over n}\right)} $$ It has to be rewritten as a definite integral. I am between $$\int_0 ^1\sqrt{1+2x}dx$$ and $$\int_1^2\sqrt{x}dx$$ Not sure if either of them or right though, help please.","I'm between 2 answers for this question, but I am not sure if either of them are right. $$\lim_{n\to \infty} \sum_{k=1}^n\sqrt{{1\over n^2}\left(1+{2k\over n}\right)} $$ It has to be rewritten as a definite integral. I am between $$\int_0 ^1\sqrt{1+2x}dx$$ and $$\int_1^2\sqrt{x}dx$$ Not sure if either of them or right though, help please.",,"['calculus', 'integration', 'definite-integrals']"
97,Confusion regarding The Fundamental Theorem of Calculus,Confusion regarding The Fundamental Theorem of Calculus,,"I'm currently studying Calculus from Stewart's book, and for the The Fundamental Theorem of Calculus Pt. 1, he defined a function $g(x) = \int_0^x f(t) dt$ which represented the area under $f(x)$ from $0$ up to $x$ and proved that $g(x)$ is the antiderivative of $f(x)$ and, in this case, if I plugged in an $x$ for $g(x)$, it would give me the area under the curve from $0$ til that $x$ since $g(x) = \int_0^x f(t) dt$ However, for an arbitrary function $f(x)$, if I found the antiderivative and plugged in an $x$, it would give me the area under the curve of $f(x)$ from which point upto $x$?","I'm currently studying Calculus from Stewart's book, and for the The Fundamental Theorem of Calculus Pt. 1, he defined a function $g(x) = \int_0^x f(t) dt$ which represented the area under $f(x)$ from $0$ up to $x$ and proved that $g(x)$ is the antiderivative of $f(x)$ and, in this case, if I plugged in an $x$ for $g(x)$, it would give me the area under the curve from $0$ til that $x$ since $g(x) = \int_0^x f(t) dt$ However, for an arbitrary function $f(x)$, if I found the antiderivative and plugged in an $x$, it would give me the area under the curve of $f(x)$ from which point upto $x$?",,"['calculus', 'integration']"
98,Using Cauchy's integral theorem to prove a inequality,Using Cauchy's integral theorem to prove a inequality,,"I am trying to solve this question: Let $f(z) = c_0 + c_1z + \ldots + c_nz^n$ be a polynomial. If the $c_k$'s are real, show that   $$\int_{-1}^1 f(x)^2 dx \le \pi \int_0^{2\pi} \left | f(e^{i\theta})\right |^2 \frac{d\theta}{2\pi} = \pi\sum_{k=0}^n\left | c_k\right |^2\ .$$ $Hint.$ For the first inequality, apply Cauchy's theorem to the function $f(z)^2$ separately on the top half and the bottom half of the unit disk. I have taken the top half of the unit disk to be the domain $D$ as given in the hint. Since $f(z)^2$ is analytic in $D$, by Cauchy's integral theorem, $\int_{\partial D}f(z)dz = 0$. I can't understand how to proceed from here. Can I break the integral into the sum of the integrals along the semicircle, $|Z| = 1 $ and $Im(Z) \gt 0$, and along the straight line, $-1 \lt Re(Z) \lt 1$ and $Im(Z) = 0$?","I am trying to solve this question: Let $f(z) = c_0 + c_1z + \ldots + c_nz^n$ be a polynomial. If the $c_k$'s are real, show that   $$\int_{-1}^1 f(x)^2 dx \le \pi \int_0^{2\pi} \left | f(e^{i\theta})\right |^2 \frac{d\theta}{2\pi} = \pi\sum_{k=0}^n\left | c_k\right |^2\ .$$ $Hint.$ For the first inequality, apply Cauchy's theorem to the function $f(z)^2$ separately on the top half and the bottom half of the unit disk. I have taken the top half of the unit disk to be the domain $D$ as given in the hint. Since $f(z)^2$ is analytic in $D$, by Cauchy's integral theorem, $\int_{\partial D}f(z)dz = 0$. I can't understand how to proceed from here. Can I break the integral into the sum of the integrals along the semicircle, $|Z| = 1 $ and $Im(Z) \gt 0$, and along the straight line, $-1 \lt Re(Z) \lt 1$ and $Im(Z) = 0$?",,"['integration', 'complex-analysis']"
99,To prove the integral inequality $\int_\overline{\theta}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}\gt\pi$,To prove the integral inequality,\int_\overline{\theta}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}\gt\pi,"The following inequality comes up in connection with motion in a dipole field. One has to show that  $$\int_\overline{\theta}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}\gt\pi$$ where $$\overline{\theta}= \begin{cases}0, & 0\lt\lambda \lt1\\ \arccos(\frac{1}{\lambda}), & \lambda \gt 1\end{cases}.$$ The case $0\lt\lambda\lt 1$ is easily verified. For then $$\int_{0}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}=\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}+\int_{\pi/2}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}$$ $$\implies\int_{0}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}=\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}+\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-\lambda \sin{\theta}}}\gt2\int_{0}^{\pi/2}d\theta=\pi.$$ However I am finding it difficult to prove the second case. Any pointers would be helpful. Thanks. NOTE: The inequality does not hold for all values of $\lambda > 1$, as has been noted in the following responses.","The following inequality comes up in connection with motion in a dipole field. One has to show that  $$\int_\overline{\theta}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}\gt\pi$$ where $$\overline{\theta}= \begin{cases}0, & 0\lt\lambda \lt1\\ \arccos(\frac{1}{\lambda}), & \lambda \gt 1\end{cases}.$$ The case $0\lt\lambda\lt 1$ is easily verified. For then $$\int_{0}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}=\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}+\int_{\pi/2}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}$$ $$\implies\int_{0}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}=\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}+\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-\lambda \sin{\theta}}}\gt2\int_{0}^{\pi/2}d\theta=\pi.$$ However I am finding it difficult to prove the second case. Any pointers would be helpful. Thanks. NOTE: The inequality does not hold for all values of $\lambda > 1$, as has been noted in the following responses.",,"['integration', 'integral-inequality']"
