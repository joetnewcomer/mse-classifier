,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Eigenvalues of block matrix $\begin{bmatrix}0 &A\\A&0\end{bmatrix}$,Eigenvalues of block matrix,\begin{bmatrix}0 &A\\A&0\end{bmatrix},"If the eigenvalues of a matrix $A$ are $\lambda_1, \lambda_2, \dots, \lambda_n$ , what are the eigenvalues of the following matrix? $$\begin{bmatrix}0 &A\\A&0\end{bmatrix}$$ From some numerical examples, I have found that the eigenvalues of this block matrix are just $\lambda_1, \lambda_2, \dots, \lambda_n$ and $-\lambda_1, -\lambda_2, \dots, -\lambda_n$ , but I am not sure how to go about proving it. I have tried taking $ \det(A - \lambda I) = 0 $ , but I could not find a way to work with the results. Where is a good place to start?","If the eigenvalues of a matrix are , what are the eigenvalues of the following matrix? From some numerical examples, I have found that the eigenvalues of this block matrix are just and , but I am not sure how to go about proving it. I have tried taking , but I could not find a way to work with the results. Where is a good place to start?","A \lambda_1, \lambda_2, \dots, \lambda_n \begin{bmatrix}0 &A\\A&0\end{bmatrix} \lambda_1, \lambda_2, \dots, \lambda_n -\lambda_1, -\lambda_2, \dots, -\lambda_n  \det(A - \lambda I) = 0 ","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
1,How to think of $\vec{u}-\vec{v}$,How to think of,\vec{u}-\vec{v},"Assume I have two vectors, $\vec{u}$ and $\vec{v}$. I know that I can think of their sum via Triangle or Parallelogram Law, but I'm having trouble knowing which way the vector would point depending on if it was $\vec{u}-\vec{v}$ or if it was $\vec{v}-\vec{u}$. Is there an easy way to remember which way the arrow would point?","Assume I have two vectors, $\vec{u}$ and $\vec{v}$. I know that I can think of their sum via Triangle or Parallelogram Law, but I'm having trouble knowing which way the vector would point depending on if it was $\vec{u}-\vec{v}$ or if it was $\vec{v}-\vec{u}$. Is there an easy way to remember which way the arrow would point?",,"['linear-algebra', 'analytic-geometry']"
2,Can a vector space have more than one zero vector?,Can a vector space have more than one zero vector?,,"The question above is really it. The reason I ask is that my text says a vector space can have more than one zero vector (It's a true/false question: A vector space may have more than one zero vector). But if the zero vector in any space is unique, then it has only one zero vector, no? Or am I reading ""unique"" wrong?","The question above is really it. The reason I ask is that my text says a vector space can have more than one zero vector (It's a true/false question: A vector space may have more than one zero vector). But if the zero vector in any space is unique, then it has only one zero vector, no? Or am I reading ""unique"" wrong?",,"['linear-algebra', 'vector-spaces']"
3,The role of maps in algebraic structures?,The role of maps in algebraic structures?,,"In group theory we have encountered the concepts of homomorphisms as maps from one group to another, and the same thing with ring and field theory and we ave called them respectively homomorphisms of groups (of rings, and of fields). Also in linear algebra we have linear maps. So my question is this: What is the role of such maps in this algebraic structures?","In group theory we have encountered the concepts of homomorphisms as maps from one group to another, and the same thing with ring and field theory and we ave called them respectively homomorphisms of groups (of rings, and of fields). Also in linear algebra we have linear maps. So my question is this: What is the role of such maps in this algebraic structures?",,"['linear-algebra', 'abstract-algebra', 'morphism']"
4,Inverse matrix of matrix (all rows equal) plus identity matrix,Inverse matrix of matrix (all rows equal) plus identity matrix,,"Let $A$ be a matrix where all rows are equal, for example, $$A=\left[\begin{array}{ccc} a_{1} & a_{2} & a_{3} \\ a_{1} & a_{2} & a_{3} \\ a_{1} & a_{2} & a_{3} \end{array}\right]$$ Then what is the inverse of the matrix $B=I+A$, where $I$ is the identity matrix? For example, $$B=\left[\begin{array}{ccc} a_{1}+1 & a_{2} & a_{3} \\ a_{1} & a_{2}+1 & a_{3} \\ a_{1} & a_{2} & a_{3}+1 \end{array}\right]$$ I have a conjecture, which computation has so far confirmed: $$B^{-1}=I-\frac{A}{\mbox{tr}(A)+1}$$ Why is this true?","Let $A$ be a matrix where all rows are equal, for example, $$A=\left[\begin{array}{ccc} a_{1} & a_{2} & a_{3} \\ a_{1} & a_{2} & a_{3} \\ a_{1} & a_{2} & a_{3} \end{array}\right]$$ Then what is the inverse of the matrix $B=I+A$, where $I$ is the identity matrix? For example, $$B=\left[\begin{array}{ccc} a_{1}+1 & a_{2} & a_{3} \\ a_{1} & a_{2}+1 & a_{3} \\ a_{1} & a_{2} & a_{3}+1 \end{array}\right]$$ I have a conjecture, which computation has so far confirmed: $$B^{-1}=I-\frac{A}{\mbox{tr}(A)+1}$$ Why is this true?",,"['linear-algebra', 'matrices', 'inverse']"
5,Motivation behind matrix diagonalisation,Motivation behind matrix diagonalisation,,"I'm going to give a 50 minutes lecture about matrix diagonalization for first year college students and I would like to give some applications of it. I've been thinking about saying the calculation of matrix exponential become simpler in a diagonal matrix, but this subject is a higher level for them and I have only 10-15 minutes to talk about it. So what do you think I can say to motivate them to study matrix diagonalization?","I'm going to give a 50 minutes lecture about matrix diagonalization for first year college students and I would like to give some applications of it. I've been thinking about saying the calculation of matrix exponential become simpler in a diagonal matrix, but this subject is a higher level for them and I have only 10-15 minutes to talk about it. So what do you think I can say to motivate them to study matrix diagonalization?",,"['linear-algebra', 'diagonalization']"
6,"How to Show that the Only Subspaces of $R^2$ are the zero subspace, $R^2$ itself, and the lines through the origin","How to Show that the Only Subspaces of  are the zero subspace,  itself, and the lines through the origin",R^2 R^2,"I'm having trouble with a question from an introductory Linear Algebra book. It goes: ""Show that the Only Subspaces of $R^2$ are the zero subspace, $R^2$ itself, and the lines through the origin."" I'm thinking the easiest way to do this is to show that if $W$ is a subspace of $R^2$ containing $2$ different lines through the origin then $W$ is all of $R^2$. Is this a good way to go about it? and how could I show this?","I'm having trouble with a question from an introductory Linear Algebra book. It goes: ""Show that the Only Subspaces of $R^2$ are the zero subspace, $R^2$ itself, and the lines through the origin."" I'm thinking the easiest way to do this is to show that if $W$ is a subspace of $R^2$ containing $2$ different lines through the origin then $W$ is all of $R^2$. Is this a good way to go about it? and how could I show this?",,"['linear-algebra', 'vector-spaces']"
7,the determinant function is an open function?,the determinant function is an open function?,,"The determinant function $\det:M(n,\mathbb R)\rightarrow \mathbb R$ is an open mapping or a closed mapping? The determinant function $\det:M(n,\mathbb C)\rightarrow \mathbb C$ is an open mapping or a closed mapping?","The determinant function $\det:M(n,\mathbb R)\rightarrow \mathbb R$ is an open mapping or a closed mapping? The determinant function $\det:M(n,\mathbb C)\rightarrow \mathbb C$ is an open mapping or a closed mapping?",,['linear-algebra']
8,Is the standard scalar product in a coordinate space basis independent?,Is the standard scalar product in a coordinate space basis independent?,,"Would you say that the standard scalar product in $K^n$, $\left< x,y \right>=\sum_i x_i y_i$, is basis-independent or not ? I would argue that it is, because we don't use the components of the vectors $x,y,$ to define this function, but the vectors themselves (although there is a basis, namely the standart basis, which, if we were to define the standard scaler product from the beginning in a basis-dependent manner - i.e. as $\left< x,y \right>_ {\mathcal{B}}=\sum_i ([x]_{\mathcal{B}})_i ([y]_{\mathcal{B}})_i$, where $\mathcal{B}$ denotes some basis of $K^n$ and $([x]_{\mathcal{B}})_i$ the $i$-th component of the coordinate vector of $x$ w.r.t. the basis $\mathcal{B}$ - would give the same function).","Would you say that the standard scalar product in $K^n$, $\left< x,y \right>=\sum_i x_i y_i$, is basis-independent or not ? I would argue that it is, because we don't use the components of the vectors $x,y,$ to define this function, but the vectors themselves (although there is a basis, namely the standart basis, which, if we were to define the standard scaler product from the beginning in a basis-dependent manner - i.e. as $\left< x,y \right>_ {\mathcal{B}}=\sum_i ([x]_{\mathcal{B}})_i ([y]_{\mathcal{B}})_i$, where $\mathcal{B}$ denotes some basis of $K^n$ and $([x]_{\mathcal{B}})_i$ the $i$-th component of the coordinate vector of $x$ w.r.t. the basis $\mathcal{B}$ - would give the same function).",,['linear-algebra']
9,Meaning of math symbol ~,Meaning of math symbol ~,,"Segment of Example from Textbook: $t=\dots$ More usefully, we have: $t\sim n\log n$ I recall $\sim$ means ""similarity"" in geometry,  same shape but not same size. How is it interpreted here?","Segment of Example from Textbook: $t=\dots$ More usefully, we have: $t\sim n\log n$ I recall $\sim$ means ""similarity"" in geometry,  same shape but not same size. How is it interpreted here?",,"['linear-algebra', 'abstract-algebra', 'notation']"
10,Consistency of matrix norm — $\| A x \|_2 \leq \| A \|_{F} \| x \|_2$,Consistency of matrix norm —,\| A x \|_2 \leq \| A \|_{F} \| x \|_2,"I'm trying to show that $$\| A x \|_2 \leq \| A \|_{F} \| x \|_2$$ where $A$ is an $n \times n$ matrix, $x \in \mathbb R^n$ , $\| \cdot \|_2$ is the Euclidean norm, and $\| \cdot \|_F$ is the Frobenius norm. I actually wrote down what $Ax$ is, it is $$Ax =\begin{pmatrix} \sum_{i=1}^n a_{1i}x_i \\\sum_{i=1}^n a_{2i}x_i \\ \vdots \\ \sum_{i=1}^n a_{ni}x_i \end{pmatrix}$$ And so: $$\|Ax\|_2 = \sqrt{\sum_{j=1}^n \left|\sum_{i=1}^n a_{ji}x_i \right|^2}$$ I need to show that that is smaller than $$\sqrt{\sum_{j=1}^n \sum_{i=1}^n |a_{ij}|^2\sum_{k=1}^n |x_k|^2}$$ But this seems very complicated. I'd love a hint in the right direction.","I'm trying to show that where is an matrix, , is the Euclidean norm, and is the Frobenius norm. I actually wrote down what is, it is And so: I need to show that that is smaller than But this seems very complicated. I'd love a hint in the right direction.",\| A x \|_2 \leq \| A \|_{F} \| x \|_2 A n \times n x \in \mathbb R^n \| \cdot \|_2 \| \cdot \|_F Ax Ax =\begin{pmatrix} \sum_{i=1}^n a_{1i}x_i \\\sum_{i=1}^n a_{2i}x_i \\ \vdots \\ \sum_{i=1}^n a_{ni}x_i \end{pmatrix} \|Ax\|_2 = \sqrt{\sum_{j=1}^n \left|\sum_{i=1}^n a_{ji}x_i \right|^2} \sqrt{\sum_{j=1}^n \sum_{i=1}^n |a_{ij}|^2\sum_{k=1}^n |x_k|^2},"['linear-algebra', 'matrices', 'vector-spaces', 'normed-spaces']"
11,formula for square of absolute value of difference of two variables $|a-b|^{2}$,formula for square of absolute value of difference of two variables,|a-b|^{2},Is there any formula for the expansion of $|a-b|^{2}$ ? Can I expand it in the same way as $(a-b)^{2}$?,Is there any formula for the expansion of $|a-b|^{2}$ ? Can I expand it in the same way as $(a-b)^{2}$?,,['linear-algebra']
12,how does addition of identity matrix to a square matrix changes determinant?,how does addition of identity matrix to a square matrix changes determinant?,,"Suppose there is $n \times n$ matrix $A$. If we form matrix $B = A+I$ where $I$ is $n \times n$ identity matrix, how does $|B|$ - determinant of $B$ - change compared to $|A|$? And what about the case where $B = A - I$?","Suppose there is $n \times n$ matrix $A$. If we form matrix $B = A+I$ where $I$ is $n \times n$ identity matrix, how does $|B|$ - determinant of $B$ - change compared to $|A|$? And what about the case where $B = A - I$?",,"['linear-algebra', 'matrices', 'determinant']"
13,A question about an $n$-dimensional subspace of $\mathbb{F}^{S}$.,A question about an -dimensional subspace of .,n \mathbb{F}^{S},"I am self-studying Hoffman and Kunze's book Linear Algebra . This is Exercise 3.6.3(Linear Transformation-The Double Dual) from page 111. Let $S$ be a set, $\mathbb{F}$ a field, and $V(S,\mathbb{F})$ the   space of all functions from $S$ into $\mathbb{F}:$ $$(f+g)(x)=f(x)+g(x)\hspace{0.5cm}(\alpha f)(x)=\alpha f(x).$$ Let $W$ be any $n$ -dimensional subspace of $V(S,\mathbb F)$ . Show   that there exist points $x_{1},\ldots,x_{n}\in S$ and functions $f_{1},\ldots, f_{n}\in W$ such that $f_{i}(x_{j})=\delta_{ij}$ . Since $W$ is an $n$ -dimensional subspace of $V(S,\mathbb{F})$ we can say find a basis $\mathcal{B}=\{f_{1},\ldots, f_{n}\}$ . But I got stuck here. I don't know what to do from now on. I mean, what should I do in order to find those points $x_{1},\ldots,x_{n}\in S$ such that $f_{i}(x_{j})=\delta_{ij}$ .","I am self-studying Hoffman and Kunze's book Linear Algebra . This is Exercise 3.6.3(Linear Transformation-The Double Dual) from page 111. Let be a set, a field, and the   space of all functions from into Let be any -dimensional subspace of . Show   that there exist points and functions such that . Since is an -dimensional subspace of we can say find a basis . But I got stuck here. I don't know what to do from now on. I mean, what should I do in order to find those points such that .","S \mathbb{F} V(S,\mathbb{F}) S \mathbb{F}: (f+g)(x)=f(x)+g(x)\hspace{0.5cm}(\alpha f)(x)=\alpha f(x). W n V(S,\mathbb F) x_{1},\ldots,x_{n}\in S f_{1},\ldots, f_{n}\in W f_{i}(x_{j})=\delta_{ij} W n V(S,\mathbb{F}) \mathcal{B}=\{f_{1},\ldots, f_{n}\} x_{1},\ldots,x_{n}\in S f_{i}(x_{j})=\delta_{ij}",[]
14,Criterion for deciding whether matrix is diagonalizable,Criterion for deciding whether matrix is diagonalizable,,"Let $B \in$ GL$_n(\mathbb{C})$. In a paper I'm reading someone probably claims the following: Lemma : For showing that $B$ is diagonalizable it suffices to show the following: Let $\lambda$ be an eigenvalue of $B$ with algebraic multiplicity $\geq 2$ and assume $x, y \in \mathbb{C}^n$ with \begin{align*} & (B - \lambda I)x = y  \\ & (B - \lambda I)y = 0 \end{align*} then we have $y = 0$. Why is this true? Does it have to do with the Jordan normal form?","Let $B \in$ GL$_n(\mathbb{C})$. In a paper I'm reading someone probably claims the following: Lemma : For showing that $B$ is diagonalizable it suffices to show the following: Let $\lambda$ be an eigenvalue of $B$ with algebraic multiplicity $\geq 2$ and assume $x, y \in \mathbb{C}^n$ with \begin{align*} & (B - \lambda I)x = y  \\ & (B - \lambda I)y = 0 \end{align*} then we have $y = 0$. Why is this true? Does it have to do with the Jordan normal form?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization', 'jordan-normal-form']"
15,Should a linear function always fix the origin? [duplicate],Should a linear function always fix the origin? [duplicate],,"This question already has answers here : What is the difference between linear and affine function? (5 answers) Closed 7 years ago . I became very confused about linear functions after reading this question What is the difference between linear and affine function? In the comments it says that $F(x)=2 \cdot x+4$ is not a linear function, (but an affine one). All my professors gave such examples when teaching linear functions. I am really confused now. Should a linear function always be of the form $f(x)=t \cdot x$ , where t is a constant? I think this could help me understand better linear transformations. I think one of the reasons I did not understand them is because I had a slightly wrong definition of linear functions. However, on Wikipedia, the definition of linear functions seems to accepts functions that also have a constant added or subtracted from the first (linear?) part. https://en.wikipedia.org/wiki/Linear_function https://upload.wikimedia.org/wikipedia/commons/0/0e/Linear_Function_Graph.svg So is Wikipedia wrong on this one?","This question already has answers here : What is the difference between linear and affine function? (5 answers) Closed 7 years ago . I became very confused about linear functions after reading this question What is the difference between linear and affine function? In the comments it says that is not a linear function, (but an affine one). All my professors gave such examples when teaching linear functions. I am really confused now. Should a linear function always be of the form , where t is a constant? I think this could help me understand better linear transformations. I think one of the reasons I did not understand them is because I had a slightly wrong definition of linear functions. However, on Wikipedia, the definition of linear functions seems to accepts functions that also have a constant added or subtracted from the first (linear?) part. https://en.wikipedia.org/wiki/Linear_function https://upload.wikimedia.org/wikipedia/commons/0/0e/Linear_Function_Graph.svg So is Wikipedia wrong on this one?",F(x)=2 \cdot x+4 f(x)=t \cdot x,"['linear-algebra', 'functions', 'terminology', 'linear-transformations']"
16,"Use of word ""axiom"" in definition of vector spaces","Use of word ""axiom"" in definition of vector spaces",,"Consider the following definition of vector spaces: Why are the listed conditions called ""axioms""? My understanding of axioms is that they are base assumptions which are taken to be true. Thus, they're not really meant to be proven. Yet from this definition, it's necessary to show that the axioms are ""satisfied"" for a specific set in order to conclude that the set is a vector space. Is that somehow different than ""proving"" the axioms are true for the given set?","Consider the following definition of vector spaces: Why are the listed conditions called ""axioms""? My understanding of axioms is that they are base assumptions which are taken to be true. Thus, they're not really meant to be proven. Yet from this definition, it's necessary to show that the axioms are ""satisfied"" for a specific set in order to conclude that the set is a vector space. Is that somehow different than ""proving"" the axioms are true for the given set?",,"['linear-algebra', 'definition', 'axioms']"
17,Are $\mathbb{C} \otimes _\mathbb{R} \mathbb{C}$ and $\mathbb{C} \otimes _\mathbb{C} \mathbb{C}$ isomorphic as $\mathbb{R}$-vector spaces?,Are  and  isomorphic as -vector spaces?,\mathbb{C} \otimes _\mathbb{R} \mathbb{C} \mathbb{C} \otimes _\mathbb{C} \mathbb{C} \mathbb{R},"Are $\mathbb{C} \otimes _\mathbb{R} \mathbb{C}$ and $\mathbb{C} \otimes _\mathbb{C} \mathbb{C}$ isomorphic as $\mathbb{R}$-vector spaces? I am having a very hard time at digesting tensor products and I do not know how to ""compare"" tensor products over different rings. My hunch is that they are not isomorphic. It is easy to see that $\mathbb{C} \otimes _\mathbb{R} \mathbb{C}$ is an $\mathbb{R}$-vector space of dimension $4$. I suspect that $\mathbb{C} \otimes _\mathbb{C} \mathbb{C}$ is also an $\mathbb{R}$-vector space but of lower dimension, but I have no idea how to show this or if indeed this intuition is correct. Thank you.","Are $\mathbb{C} \otimes _\mathbb{R} \mathbb{C}$ and $\mathbb{C} \otimes _\mathbb{C} \mathbb{C}$ isomorphic as $\mathbb{R}$-vector spaces? I am having a very hard time at digesting tensor products and I do not know how to ""compare"" tensor products over different rings. My hunch is that they are not isomorphic. It is easy to see that $\mathbb{C} \otimes _\mathbb{R} \mathbb{C}$ is an $\mathbb{R}$-vector space of dimension $4$. I suspect that $\mathbb{C} \otimes _\mathbb{C} \mathbb{C}$ is also an $\mathbb{R}$-vector space but of lower dimension, but I have no idea how to show this or if indeed this intuition is correct. Thank you.",,"['linear-algebra', 'abstract-algebra']"
18,Are the eigenvectors of a real symmetric matrix always an orthonormal basis without change?,Are the eigenvectors of a real symmetric matrix always an orthonormal basis without change?,,"I was reading the wikipedia page for symmetric matrices, and I noticed this part: a real n×n matrix A is symmetric if and only if there is an orthonormal basis of   Rn consisting of eigenvectors for A Does this mean the eigenvectors of a symmetric matrix with real values always form an orthonormal basis, meaning that without changing them at all, they're always orthogonal and always have a norm of 1? Or does it mean that based on the eigenvectors, we can manipulate them (e.g. divide them by their current norm) and turn them into vectors with a norm of 1?","I was reading the wikipedia page for symmetric matrices, and I noticed this part: a real n×n matrix A is symmetric if and only if there is an orthonormal basis of   Rn consisting of eigenvectors for A Does this mean the eigenvectors of a symmetric matrix with real values always form an orthonormal basis, meaning that without changing them at all, they're always orthogonal and always have a norm of 1? Or does it mean that based on the eigenvectors, we can manipulate them (e.g. divide them by their current norm) and turn them into vectors with a norm of 1?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
19,"Why am I getting wrong solution to the system $2x+6y-3z=10$, $5x+2y-1z=12$?","Why am I getting wrong solution to the system , ?",2x+6y-3z=10 5x+2y-1z=12,"Suppose we have this system of equations: $$\begin{align} 2x+6y-3z&=10\tag{1} \\ 5x+2y-1z&=12\tag{2} \end{align}$$ Why does solving as follows give an incorrect solution? $$\begin{align} 2x+6y-3z-10&=5x+2y-1z-12 \\ -3x+4y-2z&=-2\tag{3} \end{align}$$ According to this equation, $(0,0,1)$ is a solution. However, when I plug this into the two starting equations, the result is inconsistent. What is wrong with the logic I used in finding the solution? By the way, I was only looking for a single solution of the two starting equations, not a general solution. I know how to use matrices to solve the system, but I am still curious as to what is wrong with the approach I demonstrated above.","Suppose we have this system of equations: Why does solving as follows give an incorrect solution? According to this equation, is a solution. However, when I plug this into the two starting equations, the result is inconsistent. What is wrong with the logic I used in finding the solution? By the way, I was only looking for a single solution of the two starting equations, not a general solution. I know how to use matrices to solve the system, but I am still curious as to what is wrong with the approach I demonstrated above.","\begin{align}
2x+6y-3z&=10\tag{1} \\
5x+2y-1z&=12\tag{2}
\end{align} \begin{align}
2x+6y-3z-10&=5x+2y-1z-12 \\
-3x+4y-2z&=-2\tag{3}
\end{align} (0,0,1)","['linear-algebra', 'algebra-precalculus', 'solution-verification']"
20,Calculating dimension of the intersection of two subspaces,Calculating dimension of the intersection of two subspaces,,"Q. Let $U$ be the vector subspace of $\Bbb R^5$ generated by $\{(1,3,-3,-1,-4),(1,4,-1,-2,-2),(2,9,0,-5,-2)\}$, and let $V$ be the vector subspace of $\Bbb R^5$ generated by $\{(1,6,2,-2,3),(2,8,-1,-6,-5),(1,3,-1,-5,-6)\}$. What is the vector space dimension of $U \cap V$? I found that the basis of $U$ is just $\{(1,3,-3,-1,-4),(1,4,-1,-2,-2)\}$ and that of $V$ is $\{(1,6,2,-2,3),(2,8,-1,-6,-5),(1,3,-1,-5,-6)\}$. This means $\dim U=2$ and $\dim V=3$. I know that the dimension of $U \cap V$ is either $0,1$ or $2$ since $\dim (U \cap V) \le \dim U$. Attempt : Using the method of top answer from here , I got this equation $a(1,3,-3,-1,-4)+b(1,4,-1,-2,-2)-x(1,6,2,-2,3)-y(2,8,-1,-6,-5)-z(1,3,-1,-5,-6)=0$ then solving the arising system of linear equations and reducing to row-echelon form I get the following system of linear equations : $$ \left\{  \begin{array}{c} a+b-x-2y+z=0 \\  b-3x-2y=0 \\  x-y-2z=0 \end{array} \right.  $$ Which gives me null space as $[-2y-5z \;\;\; 5y+6z \;\;\; y+2z \;\;\; y \;\;\; z]^T$. Setting $y=-\frac 65$ and $z=1$ we get a vector in null space as $(-\frac {13}5,0,\frac 45,-\frac 65,1)$ Hence our $\textbf {v}=-\frac {13}5 (1,3,-3,-1,-4)=\frac 45(1,6,2,-2,3)+\frac {-6}5 (2,8,-1,-6,-5)+(1,3,-1,-5,-6)$. Does this mean that $\{(1,3,-3,-1,-4)\}$ is a basis of $U \cap V$ implying that $\dim (U \cap V)=1$? Have I done it right through out? Thanks. PS : How I set $y=-\frac 65,z=1$ ? I let $5y+6z=0 \; \text{in null space} \; \Rightarrow y=-\frac {6z}5$. Which gives me $[-\frac {13z}5 \;\;\; 0 \;\;\; \frac {4z}5 \;\;\; -\frac {6z}5 \;\;\; z]$. Then I took $z=1$. If I vary $z$ all over the $\Bbb R$ then, $U \cap V=\{-\frac {13z}5 (1,3,-3,-1,-4)\ : z \in \Bbb R\}$.","Q. Let $U$ be the vector subspace of $\Bbb R^5$ generated by $\{(1,3,-3,-1,-4),(1,4,-1,-2,-2),(2,9,0,-5,-2)\}$, and let $V$ be the vector subspace of $\Bbb R^5$ generated by $\{(1,6,2,-2,3),(2,8,-1,-6,-5),(1,3,-1,-5,-6)\}$. What is the vector space dimension of $U \cap V$? I found that the basis of $U$ is just $\{(1,3,-3,-1,-4),(1,4,-1,-2,-2)\}$ and that of $V$ is $\{(1,6,2,-2,3),(2,8,-1,-6,-5),(1,3,-1,-5,-6)\}$. This means $\dim U=2$ and $\dim V=3$. I know that the dimension of $U \cap V$ is either $0,1$ or $2$ since $\dim (U \cap V) \le \dim U$. Attempt : Using the method of top answer from here , I got this equation $a(1,3,-3,-1,-4)+b(1,4,-1,-2,-2)-x(1,6,2,-2,3)-y(2,8,-1,-6,-5)-z(1,3,-1,-5,-6)=0$ then solving the arising system of linear equations and reducing to row-echelon form I get the following system of linear equations : $$ \left\{  \begin{array}{c} a+b-x-2y+z=0 \\  b-3x-2y=0 \\  x-y-2z=0 \end{array} \right.  $$ Which gives me null space as $[-2y-5z \;\;\; 5y+6z \;\;\; y+2z \;\;\; y \;\;\; z]^T$. Setting $y=-\frac 65$ and $z=1$ we get a vector in null space as $(-\frac {13}5,0,\frac 45,-\frac 65,1)$ Hence our $\textbf {v}=-\frac {13}5 (1,3,-3,-1,-4)=\frac 45(1,6,2,-2,3)+\frac {-6}5 (2,8,-1,-6,-5)+(1,3,-1,-5,-6)$. Does this mean that $\{(1,3,-3,-1,-4)\}$ is a basis of $U \cap V$ implying that $\dim (U \cap V)=1$? Have I done it right through out? Thanks. PS : How I set $y=-\frac 65,z=1$ ? I let $5y+6z=0 \; \text{in null space} \; \Rightarrow y=-\frac {6z}5$. Which gives me $[-\frac {13z}5 \;\;\; 0 \;\;\; \frac {4z}5 \;\;\; -\frac {6z}5 \;\;\; z]$. Then I took $z=1$. If I vary $z$ all over the $\Bbb R$ then, $U \cap V=\{-\frac {13z}5 (1,3,-3,-1,-4)\ : z \in \Bbb R\}$.",,"['linear-algebra', 'proof-verification', 'vector-spaces', 'systems-of-equations']"
21,Can infinite-dimensional vector spaces be decomposed into direct sum of its subspaces? [duplicate],Can infinite-dimensional vector spaces be decomposed into direct sum of its subspaces? [duplicate],,"This question already has an answer here : Vector space can be written as direct sum of subspaces (1 answer) Closed 3 years ago . I'm reading Axler ""Linear agebra done right"" and in Chapter 1 he discusses subspaces and direct sum. My question is, are there subspaces of the infinite-dimensional vector spaces, e.g. a functional Banach space with sup norm $V$ that directly sum to the entire space $$V = U_1\oplus U_2$$ (except for trivial $U_1=\{0\}$, $U_2 = V$).","This question already has an answer here : Vector space can be written as direct sum of subspaces (1 answer) Closed 3 years ago . I'm reading Axler ""Linear agebra done right"" and in Chapter 1 he discusses subspaces and direct sum. My question is, are there subspaces of the infinite-dimensional vector spaces, e.g. a functional Banach space with sup norm $V$ that directly sum to the entire space $$V = U_1\oplus U_2$$ (except for trivial $U_1=\{0\}$, $U_2 = V$).",,['linear-algebra']
22,Proving that a linear isometry on $\mathbb{R}^{n}$ is an orthogonal matrix,Proving that a linear isometry on  is an orthogonal matrix,\mathbb{R}^{n},"I wish to prove that if $T:\mathbb{R}^{n}\to\mathbb{R}^{n}$ is defined by $T(v)=Av$ (where $A\in M_{n}(\mathbb{R})$) is an isometry then $A$ is an orthogonal matrix. I am familiar with many equivalent definition for $A\in M_{n}(\mathbb{R})$ to be orthogonal, and it doesn't matter to me which one to show. What I tried to do is the following: $||x-y||=||Ax-Ay||\implies\langle x-y,x-y\rangle=\langle Ax-Ay,Ax-Ay\rangle\implies\langle x-y,x-y\rangle=\langle x-y,A^{t}A(x-y)\rangle$, from here I thought that I will be able to deduce $A^{t}A=I$ and complete the proof, but I was unable to do so. How can I complete the proof, or prove this in another fashion ? Help is appreciated!","I wish to prove that if $T:\mathbb{R}^{n}\to\mathbb{R}^{n}$ is defined by $T(v)=Av$ (where $A\in M_{n}(\mathbb{R})$) is an isometry then $A$ is an orthogonal matrix. I am familiar with many equivalent definition for $A\in M_{n}(\mathbb{R})$ to be orthogonal, and it doesn't matter to me which one to show. What I tried to do is the following: $||x-y||=||Ax-Ay||\implies\langle x-y,x-y\rangle=\langle Ax-Ay,Ax-Ay\rangle\implies\langle x-y,x-y\rangle=\langle x-y,A^{t}A(x-y)\rangle$, from here I thought that I will be able to deduce $A^{t}A=I$ and complete the proof, but I was unable to do so. How can I complete the proof, or prove this in another fashion ? Help is appreciated!",,"['linear-algebra', 'linear-transformations', 'orthogonality']"
23,How is the Laplace Transform a Change of basis?,How is the Laplace Transform a Change of basis?,,"This question is primarily based on the following answer's way of reasoning, https://math.stackexchange.com/a/2156002/525644 If you want to write a new answer to the question; ""How is the Laplace Transform a Change of basis?"" Please do. In jnez71's answer, he concludes that laplace transform is a change of basis, Now lets look at that mysterious laplace transform. $$\mathscr{L}(f(x)) = \int_{-\infty}^\infty e^{-sx}f(x) \, dx$$ Imagine all possible values of $e^{-sx}$ in a big matrix $^1$ , where each   row corresponds to plugging in a specific $s$ and each column   corresponds to plugging in a specific $x$ . (This matrix is orthonormal   if $s=i\omega$ , i.e. the Fourier transform). If you select some $s$ ,   you are plucking out a specific value of the function that resulted   from the multiplication of this matrix with the vector $f(x)$ , a   function we call $F(s):=\mathscr{L}(f(x))$ . Specifically, $$F(s=3) = > f(x) \cdot e^{-3x}$$ (where that dot is an inner product, not ordinary multiplication). We   say that $F(s)$ is just $f(x)$ expressed on a basis of exponential   functions $^2$ . Choosing a specific value of $s=s_1$ is picking out the   value of $f(x)$ in the $e^{-s_1x}$ direction . The entire $e^{-sx}$ can be viewed as the change of basis matrix. Wait, what basis were we on before if we're on the exponentials now?   The dirac   deltas . Take an   inner product of some function with a dirac delta and notice how you   get back that function at the action point of the dirac delta $^3$ . This is   sometimes called the sifting   theorem , but it   should be clear that if we can project a vector (via inner product)   and just get back some component of that vector, that component was how much the vector had in the direction we projected it onto. Questions: 1. Can someone literally write the matrix he's referring to? 2. Please elucidate on ""how"" and where exactly does this change of basis occurs?(even though this information exists in his answer I couldn't get a good grasp) 3. Take an   inner product of some function with a dirac delta and notice how you   get back that function at the action point of the dirac delta How does this imply that previously we were on the dirac delta function's basis?","This question is primarily based on the following answer's way of reasoning, https://math.stackexchange.com/a/2156002/525644 If you want to write a new answer to the question; ""How is the Laplace Transform a Change of basis?"" Please do. In jnez71's answer, he concludes that laplace transform is a change of basis, Now lets look at that mysterious laplace transform. Imagine all possible values of in a big matrix , where each   row corresponds to plugging in a specific and each column   corresponds to plugging in a specific . (This matrix is orthonormal   if , i.e. the Fourier transform). If you select some ,   you are plucking out a specific value of the function that resulted   from the multiplication of this matrix with the vector , a   function we call . Specifically, (where that dot is an inner product, not ordinary multiplication). We   say that is just expressed on a basis of exponential   functions . Choosing a specific value of is picking out the   value of in the direction . The entire can be viewed as the change of basis matrix. Wait, what basis were we on before if we're on the exponentials now?   The dirac   deltas . Take an   inner product of some function with a dirac delta and notice how you   get back that function at the action point of the dirac delta . This is   sometimes called the sifting   theorem , but it   should be clear that if we can project a vector (via inner product)   and just get back some component of that vector, that component was how much the vector had in the direction we projected it onto. Questions: 1. Can someone literally write the matrix he's referring to? 2. Please elucidate on ""how"" and where exactly does this change of basis occurs?(even though this information exists in his answer I couldn't get a good grasp) 3. Take an   inner product of some function with a dirac delta and notice how you   get back that function at the action point of the dirac delta How does this imply that previously we were on the dirac delta function's basis?","\mathscr{L}(f(x)) = \int_{-\infty}^\infty e^{-sx}f(x) \, dx e^{-sx} ^1 s x s=i\omega s f(x) F(s):=\mathscr{L}(f(x)) F(s=3) =
> f(x) \cdot e^{-3x} F(s) f(x) ^2 s=s_1 f(x) e^{-s_1x} e^{-sx} ^3","['linear-algebra', 'eigenvalues-eigenvectors', 'laplace-transform', 'change-of-basis', 'eigenfunctions']"
24,Coordinate free proof for $a\times (b\times c) = b(a\cdot c) - c(a\cdot b)$,Coordinate free proof for,a\times (b\times c) = b(a\cdot c) - c(a\cdot b),"The vector triple product is defined as $\mathbf{a}\times (\mathbf{b}\times \mathbf{c})$. This is often re-written in the following way: \begin{align*}\mathbf{a}\times (\mathbf{b}\times \mathbf{c}) = \mathbf{b}(\mathbf{a}\cdot\mathbf{c}) - \mathbf{c}(\mathbf{a}\cdot\mathbf{b})\end{align*} This is a very useful identity for integrating over vector fields and so on (usually in physics). Every proof I have encountered splits the vectors into components first. This is understandable, because the cross product is purely a three dimensional construct. However, I'm curious as to whether or not there is a coordinate free proof of this identity. Although I don't know much differential geometry, I feel that tensors and so on may form a suitable framework for a coordinate free proof.","The vector triple product is defined as $\mathbf{a}\times (\mathbf{b}\times \mathbf{c})$. This is often re-written in the following way: \begin{align*}\mathbf{a}\times (\mathbf{b}\times \mathbf{c}) = \mathbf{b}(\mathbf{a}\cdot\mathbf{c}) - \mathbf{c}(\mathbf{a}\cdot\mathbf{b})\end{align*} This is a very useful identity for integrating over vector fields and so on (usually in physics). Every proof I have encountered splits the vectors into components first. This is understandable, because the cross product is purely a three dimensional construct. However, I'm curious as to whether or not there is a coordinate free proof of this identity. Although I don't know much differential geometry, I feel that tensors and so on may form a suitable framework for a coordinate free proof.",,"['linear-algebra', 'differential-geometry', 'vector-spaces', 'differential-topology', 'cross-product']"
25,Gram Matrices Rank [duplicate],Gram Matrices Rank [duplicate],,"This question already has answers here : Prove $\operatorname{rank}A^TA=\operatorname{rank}A$ for any $A\in M_{m \times n}$ (4 answers) Closed 9 years ago . Let $A$ be an $m \times n$ matrix. Show that, even though they may be of different sizes, both Gram matrices $K = A^TA$ and $L = AA^T$ have the same rank. My attempt: We have that $K$ and $L$ are Gram matrices so $K = A^TA = (A^TA)^T = AA^T = L$ and by definition we have that $\mathrm{rank}(A) = A^T$ .","This question already has answers here : Prove $\operatorname{rank}A^TA=\operatorname{rank}A$ for any $A\in M_{m \times n}$ (4 answers) Closed 9 years ago . Let be an matrix. Show that, even though they may be of different sizes, both Gram matrices and have the same rank. My attempt: We have that and are Gram matrices so and by definition we have that .",A m \times n K = A^TA L = AA^T K L K = A^TA = (A^TA)^T = AA^T = L \mathrm{rank}(A) = A^T,"['linear-algebra', 'matrices']"
26,Eigenvector of magic square,Eigenvector of magic square,,"I'm trying to show: A ""magic square"" $A$ is a matrix $n\times n$ with slots $1,2,\cdots, n^2$ such that the sum of  the elements of each row (and column) is the same . Prove that $\frac{n(n^2+1)}{2}$ is a eigenvalue of the matrix $A$. I was trying to make a proof with a proposition: ""$\beta$ is  a eigenvalue of $A$ if and only if $\det(A-\beta I_n)=0$"", I is the matrix idetity $n\times n$. But I can not do it. Thanks for your help.","I'm trying to show: A ""magic square"" $A$ is a matrix $n\times n$ with slots $1,2,\cdots, n^2$ such that the sum of  the elements of each row (and column) is the same . Prove that $\frac{n(n^2+1)}{2}$ is a eigenvalue of the matrix $A$. I was trying to make a proof with a proposition: ""$\beta$ is  a eigenvalue of $A$ if and only if $\det(A-\beta I_n)=0$"", I is the matrix idetity $n\times n$. But I can not do it. Thanks for your help.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'magic-square']"
27,"If a matrix is invertible, is its multiplication commutative?","If a matrix is invertible, is its multiplication commutative?",,"The question is prompted by change of basis problems -- the book keeps multiplying the bases by matrix $S$ from the left in order to keep subscripts nice and obviously matching, but in examples bases are multiplied by $S$ (the change of basis matrix) from whatever side. So is matrix multiplication commutative if at least one matrix is invertible?","The question is prompted by change of basis problems -- the book keeps multiplying the bases by matrix $S$ from the left in order to keep subscripts nice and obviously matching, but in examples bases are multiplied by $S$ (the change of basis matrix) from whatever side. So is matrix multiplication commutative if at least one matrix is invertible?",,['linear-algebra']
28,The determinant is the integral of algebra. The integral is the determinant of analysis,The determinant is the integral of algebra. The integral is the determinant of analysis,,"This is probably an obvious parallel that most people are aware of, but I only just noticed it the other day and it made me quite excited. The determinant in algebra has a lot in common with the integral in analysis. For example: They are both applied to functions, the integral to integrable functions, the determinant to linear transformations $T:V \rightarrow V$. They are both ""sums of products."" They both can be used to give a scalar result. (Not always, of course, but this is how they are first developed.) They are both important major structures in algebra and analysis. They are both defined in ways that feel 'backwards'-- the formal definition isn't always useful for calculating them-- then they come to represent multiple important concepts acting as a fulcrum in their fields.  (ie. AREA is connected to ANTI-DERIVATIVES... or that SOLUTIONS TO AX=B are connected to LINEAR TRANSFORMATIONS.) They can both be used to give area and volume. (under a curve, or of a parallelepiped) Question: What mathematical structure encompasses both? (If the answer is category theory, please go slowly with me, I don't understand that stuff yet.) What else could we add to this list? Are there any problems or proofs that bring these parallels in to the light? Are there any other mathematical structures that follow the pattern established by these two structures ? Were they developed independently (what I suspect) or is the determinant in some way patterned after the integral or vice versa? (I know my math history and have not come across anything about this.)","This is probably an obvious parallel that most people are aware of, but I only just noticed it the other day and it made me quite excited. The determinant in algebra has a lot in common with the integral in analysis. For example: They are both applied to functions, the integral to integrable functions, the determinant to linear transformations $T:V \rightarrow V$. They are both ""sums of products."" They both can be used to give a scalar result. (Not always, of course, but this is how they are first developed.) They are both important major structures in algebra and analysis. They are both defined in ways that feel 'backwards'-- the formal definition isn't always useful for calculating them-- then they come to represent multiple important concepts acting as a fulcrum in their fields.  (ie. AREA is connected to ANTI-DERIVATIVES... or that SOLUTIONS TO AX=B are connected to LINEAR TRANSFORMATIONS.) They can both be used to give area and volume. (under a curve, or of a parallelepiped) Question: What mathematical structure encompasses both? (If the answer is category theory, please go slowly with me, I don't understand that stuff yet.) What else could we add to this list? Are there any problems or proofs that bring these parallels in to the light? Are there any other mathematical structures that follow the pattern established by these two structures ? Were they developed independently (what I suspect) or is the determinant in some way patterned after the integral or vice versa? (I know my math history and have not come across anything about this.)",,"['linear-algebra', 'analysis', 'intuition']"
29,Why $\det\begin{pmatrix} 1&1&0&...&0 \\ -1 & 1 & 0 &...&0 \\ -1 & 0 & 1 &...&0 \\ \vdots&\vdots&\vdots&\ddots&\vdots \\ -1&0&0&...&1\end{pmatrix}=2$?,Why ?,\det\begin{pmatrix} 1&1&0&...&0 \\ -1 & 1 & 0 &...&0 \\ -1 & 0 & 1 &...&0 \\ \vdots&\vdots&\vdots&\ddots&\vdots \\ -1&0&0&...&1\end{pmatrix}=2,"Let $$A = \begin{pmatrix} 1 & 1 & 0 & 0 &0 &... & 0 \\ -1 & 1 & 0 & 0 &0&...&0 \\ -1 & 0 & 1 & 0 &0&...&0 \\ -1 & 0 & 0 & 1 &0&...&0 \\ -1 & 0 & 0 & 0 &1&...&0 \\ \vdots & \vdots & \vdots & \vdots &\vdots&\ddots&\vdots \\ -1 & 0 & 0 & 0 &0&...&1\end{pmatrix}$$ be an $n \times n$ matrix consisting of $1$ 's on its diagonal, a $1$ in the entry located on row $1$ and column $2$ , and $-1$ 's from  the second entry of the first column all the way to $n$ . Prove that the determinant of $A$ is always $2$ . How should I begin this? I tried taking the determinants of the matrix when $n=2, 3, $ and $ 4$ and saw that they were all $2$ , but am not sure how to proceed.","Let be an matrix consisting of 's on its diagonal, a in the entry located on row and column , and 's from  the second entry of the first column all the way to . Prove that the determinant of is always . How should I begin this? I tried taking the determinants of the matrix when and and saw that they were all , but am not sure how to proceed.","A = \begin{pmatrix}
1 & 1 & 0 & 0 &0 &... & 0
\\ -1 & 1 & 0 & 0 &0&...&0
\\ -1 & 0 & 1 & 0 &0&...&0
\\ -1 & 0 & 0 & 1 &0&...&0
\\ -1 & 0 & 0 & 0 &1&...&0
\\ \vdots & \vdots & \vdots & \vdots &\vdots&\ddots&\vdots
\\ -1 & 0 & 0 & 0 &0&...&1\end{pmatrix} n \times n 1 1 1 2 -1 n A 2 n=2, 3,   4 2","['linear-algebra', 'matrices', 'determinant']"
30,Rank of transformation $Y=AX-XA$,Rank of transformation,Y=AX-XA,"Consider vector space $V$ consist all $n \times n$ matrix (real or complex). What is the rank of the linear transformation $f(X)=AX-XA$ ($A\in V$)? ($A$ is a given matrix, which means we can have information about it) I have tried to consider the basis of $V$ but it doesn't work. EDITED: $\operatorname{rank} f = n^2 - \operatorname{dim}N(f)$, which means we just need to find the demension of $N(f)$, which I think is much easier. It turns out that it's not that easier. EDIT 2: In the first answer it has been shown that the matrix version of the transformation has at least $n$ zero roots in the characteristic polynomial, however it's not true that these roots are accompanied with eigenvectors (Consider a nilpotent matrix of degree $n$ always have $n$ zero roots but may have only $1$ eigenvector). Therefore unless $A$ is diagonalisable, I think the problem is not solved yet.","Consider vector space $V$ consist all $n \times n$ matrix (real or complex). What is the rank of the linear transformation $f(X)=AX-XA$ ($A\in V$)? ($A$ is a given matrix, which means we can have information about it) I have tried to consider the basis of $V$ but it doesn't work. EDITED: $\operatorname{rank} f = n^2 - \operatorname{dim}N(f)$, which means we just need to find the demension of $N(f)$, which I think is much easier. It turns out that it's not that easier. EDIT 2: In the first answer it has been shown that the matrix version of the transformation has at least $n$ zero roots in the characteristic polynomial, however it's not true that these roots are accompanied with eigenvectors (Consider a nilpotent matrix of degree $n$ always have $n$ zero roots but may have only $1$ eigenvector). Therefore unless $A$ is diagonalisable, I think the problem is not solved yet.",,"['linear-algebra', 'matrix-rank']"
31,What is a basis for the space of $n\times n$ Hermitian matrices?,What is a basis for the space of  Hermitian matrices?,n\times n,"So I was working on a specific problem related to Hermitian matrices. If we let $H_n$ denote the set of n x n Hermitian matrices. We're told that $H_n$ is a real vector space under matrix addition and scalar multiplication by a real number. I don't understand why though, because what would prevent you from adding two imaginary numbers? Then we're told to write a basis for $H_2$. Now for a Hermitian matrix, it's equal to its conjugate transpose, so a basis would be: $$\left(\begin{array}{ccc} 0 & 1 \\ 1 & 0\\ \end{array}\right) , \left(\begin{array}{ccc} 0 & 0 \\ 0 & 1\\ \end{array}\right), \left(\begin{array}{ccc} 1 & 0 \\ 0 & 0\\ \end{array}\right), \left(\begin{array}{ccc} 0 & i \\ -i & 0\\ \end{array}\right)$$ Which should be the same as the basis for a symmetric matrix, correct? From there though, we have $sH_n$ represent skew-Hermitian matrices. I believe the basis for $sH_n$ is: Edit: updated my basis here. $$\left(\begin{array}{ccc} 0 & -1 \\ 1 & 0\\ \end{array}\right), \left(\begin{array}{ccc} 0 & i \\ i & 0\\ \end{array}\right)$$ because the diagonals would have to be zero right, to make the conjugate transpose of A be equal to -A? From there though, they ask if $sH_n$ is a real vector space, and I'm not sure what to answer. It's real if we only use reals? I'm asked the same thing for $U_n$, where $U_n$ represents n x n unitary matrices. I'm not even sure how to come up with a basis for this set?? Edit: Technically, the question says ""Is $U_n$ a real vector space? Write a basis if possible."" I suppose I couldn't write a basis because $U_n$ is not a real vector space then. How would I show that though? Thanks in advance for your help.","So I was working on a specific problem related to Hermitian matrices. If we let $H_n$ denote the set of n x n Hermitian matrices. We're told that $H_n$ is a real vector space under matrix addition and scalar multiplication by a real number. I don't understand why though, because what would prevent you from adding two imaginary numbers? Then we're told to write a basis for $H_2$. Now for a Hermitian matrix, it's equal to its conjugate transpose, so a basis would be: $$\left(\begin{array}{ccc} 0 & 1 \\ 1 & 0\\ \end{array}\right) , \left(\begin{array}{ccc} 0 & 0 \\ 0 & 1\\ \end{array}\right), \left(\begin{array}{ccc} 1 & 0 \\ 0 & 0\\ \end{array}\right), \left(\begin{array}{ccc} 0 & i \\ -i & 0\\ \end{array}\right)$$ Which should be the same as the basis for a symmetric matrix, correct? From there though, we have $sH_n$ represent skew-Hermitian matrices. I believe the basis for $sH_n$ is: Edit: updated my basis here. $$\left(\begin{array}{ccc} 0 & -1 \\ 1 & 0\\ \end{array}\right), \left(\begin{array}{ccc} 0 & i \\ i & 0\\ \end{array}\right)$$ because the diagonals would have to be zero right, to make the conjugate transpose of A be equal to -A? From there though, they ask if $sH_n$ is a real vector space, and I'm not sure what to answer. It's real if we only use reals? I'm asked the same thing for $U_n$, where $U_n$ represents n x n unitary matrices. I'm not even sure how to come up with a basis for this set?? Edit: Technically, the question says ""Is $U_n$ a real vector space? Write a basis if possible."" I suppose I couldn't write a basis because $U_n$ is not a real vector space then. How would I show that though? Thanks in advance for your help.",,"['linear-algebra', 'matrices', 'vector-spaces', 'hermitian-matrices']"
32,Proving a linear transformation is unique,Proving a linear transformation is unique,,"In Axler's Linear Algebra Done Right, there is this theorem. (3.5) Suppose $v_1. . .  v_n $ is a basis of $V$ and $w_ , . . . w_n \in W$ . Then there exists   a unique linear map $T: V \rightarrow  W$ such that $$T (v_j) = w_j$$ for each $j\in   1, . . . , n $ . I understood the first part of the proof proving existence of such a transformation, but didn't understand the uniqueness part of the proof. To prove uniqueness, now suppose that $T \in \cal L $$(L,V)$ ;and that $T( v_j)=  w_j$ for each $j\in   1, . . . , n $ . Let $c_1,. . . ,c_n \in F$ . The homogeneity of $T$ implies that $T(c_j v_j) =  c_jw_j$ for each $j\in   1, . . . , n $ . The additivity of T now implies that $T(c_1v_1 + . . . + c_nv_n) = c_1w_1 + . . . + c_nw_n$ . Thus $T$ is uniquely determined on span( $v_1, . . . ,v_n$ ) by the equation above.   Because $v_1,  . . . v_n$ is a basis of $V$ , this implies that $T$ is uniquely determined   on $V$ . How does ""equation above"" show that $T$ is uniquely determined on span( $v_1, . . . ,v_n$ )? Is it because there is one way to get each of the basis vectors using the equation? How do we know  there isn't another transformation that will work?","In Axler's Linear Algebra Done Right, there is this theorem. (3.5) Suppose is a basis of and . Then there exists   a unique linear map such that for each . I understood the first part of the proof proving existence of such a transformation, but didn't understand the uniqueness part of the proof. To prove uniqueness, now suppose that ;and that for each . Let . The homogeneity of implies that for each . The additivity of T now implies that . Thus is uniquely determined on span( ) by the equation above.   Because is a basis of , this implies that is uniquely determined   on . How does ""equation above"" show that is uniquely determined on span( )? Is it because there is one way to get each of the basis vectors using the equation? How do we know  there isn't another transformation that will work?","v_1. . .  v_n  V w_ , . . . w_n \in W T: V \rightarrow  W T (v_j) = w_j j\in   1, . . . , n  T \in \cal L (L,V) T( v_j)=  w_j j\in   1, . . . , n  c_1,. . . ,c_n \in F T T(c_j v_j) =  c_jw_j j\in   1, . . . , n  T(c_1v_1 + . . . + c_nv_n) = c_1w_1 + . . . + c_nw_n T v_1, . . . ,v_n v_1,  . . . v_n V T V T v_1, . . . ,v_n","['linear-algebra', 'linear-transformations', 'proof-explanation']"
33,"How can I tell if a matrix is diagonalizable knowing only the trace, one eigenvalue, and a result of the characteristic polynomial?","How can I tell if a matrix is diagonalizable knowing only the trace, one eigenvalue, and a result of the characteristic polynomial?",,"Given $A$, a $3 \times 3$ matrix, and: $\mathrm{tr}(A) = −2$, $\mathrm{rk}(A−2I)< 3$, $\chi_A(1) = −8$ ($\mathrm{tr}$: trace, $\mathrm{rk}$: rank, and $\chi_A(x)$: characteristic polynomial) How can I tell if the matrix is diagonalizable? What are the eigenvalues? I know that given $\mathrm{rk}(A−2I)< 3$, $2$ must an eigenvalue with multiplicity $1$, at least. But I don't know what information does ""$\chi_A(1)=−8$"" provide me.","Given $A$, a $3 \times 3$ matrix, and: $\mathrm{tr}(A) = −2$, $\mathrm{rk}(A−2I)< 3$, $\chi_A(1) = −8$ ($\mathrm{tr}$: trace, $\mathrm{rk}$: rank, and $\chi_A(x)$: characteristic polynomial) How can I tell if the matrix is diagonalizable? What are the eigenvalues? I know that given $\mathrm{rk}(A−2I)< 3$, $2$ must an eigenvalue with multiplicity $1$, at least. But I don't know what information does ""$\chi_A(1)=−8$"" provide me.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'matrix-rank']"
34,What does this characterization of the determinant theorem show?,What does this characterization of the determinant theorem show?,,"I don't understand what this theorem for a characterization of the determinant actually means, and what use it has. Could you please explain it? Let $D$ be a function mapping from the set of square matrices $M$ with $n$ rows/columns over the field $F$, to a field $F$. Also let $D$ be a function that is multilinear and alternating in columns. Then $$D(A) = D(I) \det A$$","I don't understand what this theorem for a characterization of the determinant actually means, and what use it has. Could you please explain it? Let $D$ be a function mapping from the set of square matrices $M$ with $n$ rows/columns over the field $F$, to a field $F$. Also let $D$ be a function that is multilinear and alternating in columns. Then $$D(A) = D(I) \det A$$",,"['linear-algebra', 'matrices', 'determinant']"
35,Number of $k$-dimensional subspaces in $V$ [duplicate],Number of -dimensional subspaces in  [duplicate],k V,This question already has an answer here : How to count number of bases and subspaces of a given dimension in a vector space over a finite field? [duplicate] (1 answer) Closed 8 years ago . I know my following question is somewhat similar to this one but still I need help . How many $k$-dimensional subspaces of a $n$-dimensional vector space $V$  over the finite field $F$ with $q$ elements are there?,This question already has an answer here : How to count number of bases and subspaces of a given dimension in a vector space over a finite field? [duplicate] (1 answer) Closed 8 years ago . I know my following question is somewhat similar to this one but still I need help . How many $k$-dimensional subspaces of a $n$-dimensional vector space $V$  over the finite field $F$ with $q$ elements are there?,,"['linear-algebra', 'combinatorics', 'vector-spaces']"
36,"$A,B$ be Hermitian.Is this true that $tr[(AB)^2]\le tr(A^2B^2)$?",be Hermitian.Is this true that ?,"A,B tr[(AB)^2]\le tr(A^2B^2)","Suppose $A,B \in {M_n}$ be Hermitian.Is this true that $tr[(AB)^2]\le tr(A^2B^2)$?","Suppose $A,B \in {M_n}$ be Hermitian.Is this true that $tr[(AB)^2]\le tr(A^2B^2)$?",,"['linear-algebra', 'matrices', 'inequality', 'trace']"
37,Taking the Inner Product with the Zero Vector,Taking the Inner Product with the Zero Vector,,"Is it possible for the inner product of any vector with the zero vector $ \mathbf{0} $ to be nonzero? Or must it always be zero? I'm struggling to find a counterexample. That is, is the following statement correct? $$ \langle \mathbf{v}, \mathbf{0}\rangle = 0 \space \forall \space v \in V$$","Is it possible for the inner product of any vector with the zero vector $ \mathbf{0} $ to be nonzero? Or must it always be zero? I'm struggling to find a counterexample. That is, is the following statement correct? $$ \langle \mathbf{v}, \mathbf{0}\rangle = 0 \space \forall \space v \in V$$",,['linear-algebra']
38,Interior product between differential forms and vector fields,Interior product between differential forms and vector fields,,"I don't understand what is meant when someone writes that forms (or form fields) ""eat"" vectors (or vector fields). For example when I have a one form field ω=3dx+5dy+3xdz and a vector field X=3x∂x+5y∂y+3∂z, then their interior product $\iota_{X}\omega=\omega(X)= (3dx+5dy+3xdz)(3x\partial_{x}+5y\partial_{y}+3\partial_{z})=6x(\partial_{x}dx)+25y(\partial_{y}dy)+6x(\partial_{z}dz)=9x+25y+6x=15x+25y$ I think this is right. So in this case ω(X) doesn’t mean that ω is function of X, it just means multiplication. Now how does this work with two forms? For example X=y∂x+2z∂y+3xy∂z and the two form ω=3dx∧dy−(14zx+2)dx∧dz $\iota_{X}\omega=\omega(X,V)=(3dx\wedge dy-(14zx+2)dx\wedge dz)(y\partial_{x}+2z\partial_{y}+3xy\partial_{z}, V)$ Now how do I proceed from here?","I don't understand what is meant when someone writes that forms (or form fields) ""eat"" vectors (or vector fields). For example when I have a one form field ω=3dx+5dy+3xdz and a vector field X=3x∂x+5y∂y+3∂z, then their interior product $\iota_{X}\omega=\omega(X)= (3dx+5dy+3xdz)(3x\partial_{x}+5y\partial_{y}+3\partial_{z})=6x(\partial_{x}dx)+25y(\partial_{y}dy)+6x(\partial_{z}dz)=9x+25y+6x=15x+25y$ I think this is right. So in this case ω(X) doesn’t mean that ω is function of X, it just means multiplication. Now how does this work with two forms? For example X=y∂x+2z∂y+3xy∂z and the two form ω=3dx∧dy−(14zx+2)dx∧dz $\iota_{X}\omega=\omega(X,V)=(3dx\wedge dy-(14zx+2)dx\wedge dz)(y\partial_{x}+2z\partial_{y}+3xy\partial_{z}, V)$ Now how do I proceed from here?",,"['linear-algebra', 'differential-geometry', 'differential-forms']"
39,"On the proof: $\exp(A)\exp(B)=\exp(A+B)$ , where uses the hypothesis $AB=BA$?","On the proof:  , where uses the hypothesis ?",\exp(A)\exp(B)=\exp(A+B) AB=BA,I was seeing the proof that $\exp(A)\exp(B)=\exp(A+B)$ on link Show that $ e^{A+B}=e^A e^B$ where uses the hypothesis $AB=BA$? Thanks!,I was seeing the proof that $\exp(A)\exp(B)=\exp(A+B)$ on link Show that $ e^{A+B}=e^A e^B$ where uses the hypothesis $AB=BA$? Thanks!,,"['linear-algebra', 'exponential-function']"
40,Total unimodularity of matrix with consecutive ones property,Total unimodularity of matrix with consecutive ones property,,"A matrix has the consecutive ones property (often abbreviated C1P) if its every row (or column, for column-oriented C1P) is of the form $(0,\ldots,0,1,\ldots,1,0,\ldots,0)$. There is a theorem which says that any such matrix is totally unimodular , i.e. its every square submatrix has determinant $-1$, $0$ or $1$. Naturally, this also holds if we could permute and/or transpose the matrix into one with consecutive ones property, however, how do I prove the theorem? Context: The question was asked here , but the answer was to long for a comment, and didn't fit as edit into the original post.","A matrix has the consecutive ones property (often abbreviated C1P) if its every row (or column, for column-oriented C1P) is of the form $(0,\ldots,0,1,\ldots,1,0,\ldots,0)$. There is a theorem which says that any such matrix is totally unimodular , i.e. its every square submatrix has determinant $-1$, $0$ or $1$. Naturally, this also holds if we could permute and/or transpose the matrix into one with consecutive ones property, however, how do I prove the theorem? Context: The question was asked here , but the answer was to long for a comment, and didn't fit as edit into the original post.",,"['linear-algebra', 'matrices']"
41,Extension of linearly independent set to a basis in an infinite dimensional vector space,Extension of linearly independent set to a basis in an infinite dimensional vector space,,"Is it always possible to extend a linearly independent set to a basis in infinite dimensional vector space? I was proving with the following argument: If S is a linearly independent set, if it spans the vector space then done else keep on adding the elements such that the resultant set is also linearly independent, till it spans the vector space . But the problem is how can we guarantee that the process will stop?","Is it always possible to extend a linearly independent set to a basis in infinite dimensional vector space? I was proving with the following argument: If S is a linearly independent set, if it spans the vector space then done else keep on adding the elements such that the resultant set is also linearly independent, till it spans the vector space . But the problem is how can we guarantee that the process will stop?",,"['linear-algebra', 'hamel-basis']"
42,Finding eigenvalues of an unknown matrix subtracted by the identity matrix,Finding eigenvalues of an unknown matrix subtracted by the identity matrix,,"The question: If the eigenvalues of $A$ are 0, 1, and 3, find the eigenvalues of $A-I$. Explain how you obtained them. My intuition is telling me that I just subtract one from each of the eigenvalues since they are related to the diagonal, and we are subtracting one from each of the members on the diagonal.","The question: If the eigenvalues of $A$ are 0, 1, and 3, find the eigenvalues of $A-I$. Explain how you obtained them. My intuition is telling me that I just subtract one from each of the eigenvalues since they are related to the diagonal, and we are subtracting one from each of the members on the diagonal.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
43,Can a matrix have a null space that is equal to its column space?,Can a matrix have a null space that is equal to its column space?,,"I had a question in a recent assignment that asked if a $3\times 3$ matrix could have a null space equal to its column space... clearly no, by the rank+nullity theorem... but I have a hard time wrapping my head around the concept of such a matrix, no matter what size, even existing. How could this be possible, and does anybody have an example of such a $m\times n$ matrix?","I had a question in a recent assignment that asked if a $3\times 3$ matrix could have a null space equal to its column space... clearly no, by the rank+nullity theorem... but I have a hard time wrapping my head around the concept of such a matrix, no matter what size, even existing. How could this be possible, and does anybody have an example of such a $m\times n$ matrix?",,['linear-algebra']
44,The dimensions of $V$ and $V^\perp$ are complementary,The dimensions of  and  are complementary,V V^\perp,"Statement: Let $V$ a vector subspace of $\mathbb{R}^4$. The bilinear form $f(x,y) = x_1y_1 + x_2y_2 + x_3y_3 - x_4y_4$ where $x,y \in \mathbb{R}$. Let $V^\bot = \{ y \in \mathbb{R}^4 : f(x, y) = 0 \ \ \ \forall x \in V \}$. Prove that $ \text{dim}(V) + \text{dim}(V^\bot) = 4$. -- This problem currently had multiple subproblems which I've already solved successfully (e.g. prove that $V^\bot$ is a subspace, find $V$ that $ V \cap V^\bot \ne \{ 0 \} $ ). I've tried giving a basis of $V \cap V^\bot$ and extending it to a basis of $V$ and a basis of $V^\bot$ but I couldn't go further. I'll appreciate any hint. Thanks.","Statement: Let $V$ a vector subspace of $\mathbb{R}^4$. The bilinear form $f(x,y) = x_1y_1 + x_2y_2 + x_3y_3 - x_4y_4$ where $x,y \in \mathbb{R}$. Let $V^\bot = \{ y \in \mathbb{R}^4 : f(x, y) = 0 \ \ \ \forall x \in V \}$. Prove that $ \text{dim}(V) + \text{dim}(V^\bot) = 4$. -- This problem currently had multiple subproblems which I've already solved successfully (e.g. prove that $V^\bot$ is a subspace, find $V$ that $ V \cap V^\bot \ne \{ 0 \} $ ). I've tried giving a basis of $V \cap V^\bot$ and extending it to a basis of $V$ and a basis of $V^\bot$ but I couldn't go further. I'll appreciate any hint. Thanks.",,"['linear-algebra', 'vector-spaces']"
45,Existence of least squares solution to $Ax=b$,Existence of least squares solution to,Ax=b,Does a least squares solution to $Ax=b$ always exist?,Does a least squares solution to $Ax=b$ always exist?,,"['linear-algebra', 'matrices']"
46,What is a Hamel basis?,What is a Hamel basis?,,"According to Mathworld , a Hamel basis is a basis for $\mathbb R$ considered as a vector space over $\mathbb Q$ . According to Wikipedia , the term is used in the context of infinite-dimensional vector spaces over $\mathbb R$ or $\mathbb C$ . According to the description of the Mathematics Stack Exchange tag hamel-basis , a Hamel basis of a vector space $V$ over a field $F$ is a linearly independent subset of $V$ that spans it. (That is often called simply a basis, and there is no mention of infinite dimension.) I find it difficult to reconcile these three different explanations of the term Hamel basis, though the first two seem to be different particular examples of the third (and for finite-dimensional vector spaces different kinds of bases are the same).","According to Mathworld , a Hamel basis is a basis for considered as a vector space over . According to Wikipedia , the term is used in the context of infinite-dimensional vector spaces over or . According to the description of the Mathematics Stack Exchange tag hamel-basis , a Hamel basis of a vector space over a field is a linearly independent subset of that spans it. (That is often called simply a basis, and there is no mention of infinite dimension.) I find it difficult to reconcile these three different explanations of the term Hamel basis, though the first two seem to be different particular examples of the third (and for finite-dimensional vector spaces different kinds of bases are the same).",\mathbb R \mathbb Q \mathbb R \mathbb C V F V,"['linear-algebra', 'terminology', 'hamel-basis']"
47,"Does $\text{Span}(\{1, e^x, e^{2x}, . . . e^{nx}, . . .\})=C(\mathbb{R})$?",Does ?,"\text{Span}(\{1, e^x, e^{2x}, . . . e^{nx}, . . .\})=C(\mathbb{R})","Does the set of functions $A = \{1, e^x, e^{2x}, . . . e^{nx}, . . .\}\subseteq C(\mathbb{R})$ span $C(\mathbb{R})$? Here $C(\mathbb{R})$ is the vector space of all continuous functions $f : \mathbb{R} \rightarrow \mathbb{R}$. Will the answer change if we consider $A$ as a subset of the vector space of all differentiable functions from $\mathbb{R}$ to $\mathbb{R}$? I think the answer to the first question is no, and I think that $f(x)=2^x$ is a counterexample. I am not sure about the second question. Any help is appreciated, thank you.","Does the set of functions $A = \{1, e^x, e^{2x}, . . . e^{nx}, . . .\}\subseteq C(\mathbb{R})$ span $C(\mathbb{R})$? Here $C(\mathbb{R})$ is the vector space of all continuous functions $f : \mathbb{R} \rightarrow \mathbb{R}$. Will the answer change if we consider $A$ as a subset of the vector space of all differentiable functions from $\mathbb{R}$ to $\mathbb{R}$? I think the answer to the first question is no, and I think that $f(x)=2^x$ is a counterexample. I am not sure about the second question. Any help is appreciated, thank you.",,"['linear-algebra', 'vector-spaces']"
48,Compute $\det{T}$ where $T(X)=AX+XA$,Compute  where,\det{T} T(X)=AX+XA,"Consider the linear transformation $T:V\to V$ given by $T(X) = AX + XA$, where    $$A = \begin{pmatrix}1&1&0\\0&2&0\\0&0&-1 \end{pmatrix}.$$   Compute the determinant $\det T$. I know there was a similar problem with a different $A$, but that was a diagonal matrix, which made the situation easier. I computed $XA + AX$ using an arbitrary $X$, but I'm not sure where to go from there.","Consider the linear transformation $T:V\to V$ given by $T(X) = AX + XA$, where    $$A = \begin{pmatrix}1&1&0\\0&2&0\\0&0&-1 \end{pmatrix}.$$   Compute the determinant $\det T$. I know there was a similar problem with a different $A$, but that was a diagonal matrix, which made the situation easier. I computed $XA + AX$ using an arbitrary $X$, but I'm not sure where to go from there.",,"['linear-algebra', 'matrices', 'determinant', 'linear-transformations']"
49,Is Cramer's rule efficient for computational point of view?,Is Cramer's rule efficient for computational point of view?,,I am not sure if Cramer's rule is used for computation purposes. Your help would mean a lot. Thanks!,I am not sure if Cramer's rule is used for computation purposes. Your help would mean a lot. Thanks!,,"['linear-algebra', 'computational-complexity']"
50,Is there a relationship between vector spaces and fields/rings/groups?,Is there a relationship between vector spaces and fields/rings/groups?,,"I understand from a comment under Vector Spaces and Groups that every vector space is a group, but not every group is a vector space. Specifically, I would like to know, can I make a statement like: ""All fields are rings, and all rings are groups""? At this point in my studies, I see various lists of axioms, and I'm trying to see the relationship between them all. This all because I have a headache, so I went to lie down with a linear algebra book. It's not helping.","I understand from a comment under Vector Spaces and Groups that every vector space is a group, but not every group is a vector space. Specifically, I would like to know, can I make a statement like: ""All fields are rings, and all rings are groups""? At this point in my studies, I see various lists of axioms, and I'm trying to see the relationship between them all. This all because I have a headache, so I went to lie down with a linear algebra book. It's not helping.",,"['linear-algebra', 'group-theory', 'ring-theory', 'vector-spaces', 'field-theory']"
51,Degree of minimum polynomial at most $n$ without Cayley-Hamilton?,Degree of minimum polynomial at most  without Cayley-Hamilton?,n,"Let $T$ be a linear transformation of an $n$-dimensional vector space $V$ over a field $k$. It's pretty easy to define the minimum polynomial of $T$ and make sure its degree is between $1$ and $n^2$, inclusive. Observe $ I = \{ p(x) \in k[x] : p(T) =0\}$ is an ideal in $k[x]$. Indeed, $I$ is the kernel of the evaluation homomorphism $\mathrm{eval}_T: k[x] \to \mathrm{End}(V)$.  Notice also that: $\mathrm{eval}_T$ is unital homomorphism, so $I$ is a proper ideal. The $n^2 + 1$ transformations $I, T,T^2,T^3,\ldots, T^{n^2}$ must be linearly dependent, since $\mathrm{dim}(\mathrm{End}(V)) = n^2$, so there exist scalars $a_0,\ldots,a_{n^2}$, not all zero, such that $a_0I + a_1 T + \ldots + a_{n^2}T^{n^2} = 0$, whence the nonzero polynomial $p(x) = a_0 + a_1 x + \ldots + a_{n^2}x^{n^2}$ belongs to $I$. Since $k[x]$ is a p.i.d., we may define the minimum polynomial $m(x)$ of $T$ to be the monic generator of the ideal $I$. By the preceding two observations, we have $1 \leq \mathrm{deg}(m(x)) \leq n^2$. Now, of course, we know that the degree of $m(x)$ actually satisfies $1 \leq \mathrm{deg}(m(x)) \leq n$. One way to see this is to use the Cayley-Hamilton theorem which shows that the characteristic polynomial $c(x)=\det(xI - T)$, whose degree is $n$, annihilates $T$, whence $m(x)$ divides $c(x)$. Question: Is there another way to see that $T$ is annihilated by a polynomial of degree $\leq n$ which does not require use of the characteristic polynomial?","Let $T$ be a linear transformation of an $n$-dimensional vector space $V$ over a field $k$. It's pretty easy to define the minimum polynomial of $T$ and make sure its degree is between $1$ and $n^2$, inclusive. Observe $ I = \{ p(x) \in k[x] : p(T) =0\}$ is an ideal in $k[x]$. Indeed, $I$ is the kernel of the evaluation homomorphism $\mathrm{eval}_T: k[x] \to \mathrm{End}(V)$.  Notice also that: $\mathrm{eval}_T$ is unital homomorphism, so $I$ is a proper ideal. The $n^2 + 1$ transformations $I, T,T^2,T^3,\ldots, T^{n^2}$ must be linearly dependent, since $\mathrm{dim}(\mathrm{End}(V)) = n^2$, so there exist scalars $a_0,\ldots,a_{n^2}$, not all zero, such that $a_0I + a_1 T + \ldots + a_{n^2}T^{n^2} = 0$, whence the nonzero polynomial $p(x) = a_0 + a_1 x + \ldots + a_{n^2}x^{n^2}$ belongs to $I$. Since $k[x]$ is a p.i.d., we may define the minimum polynomial $m(x)$ of $T$ to be the monic generator of the ideal $I$. By the preceding two observations, we have $1 \leq \mathrm{deg}(m(x)) \leq n^2$. Now, of course, we know that the degree of $m(x)$ actually satisfies $1 \leq \mathrm{deg}(m(x)) \leq n$. One way to see this is to use the Cayley-Hamilton theorem which shows that the characteristic polynomial $c(x)=\det(xI - T)$, whose degree is $n$, annihilates $T$, whence $m(x)$ divides $c(x)$. Question: Is there another way to see that $T$ is annihilated by a polynomial of degree $\leq n$ which does not require use of the characteristic polynomial?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'polynomials', 'determinant']"
52,Why null space and column space?,Why null space and column space?,,"I am not asking this question for WHAT is null space or WHAT is column space. I have finished learning about the definitions of these two concepts for a while. However, to install these concepts in my mind forever, I really want to know what the purposes are for null space and column space of a vector. Thanks!","I am not asking this question for WHAT is null space or WHAT is column space. I have finished learning about the definitions of these two concepts for a while. However, to install these concepts in my mind forever, I really want to know what the purposes are for null space and column space of a vector. Thanks!",,['linear-algebra']
53,Proving that orthogonal vectors are linearly independent,Proving that orthogonal vectors are linearly independent,,"I am trying to prove that $a_1$, $a_2$, $a_3$ are linearly independent. I am asked to use vector product and prove that if $c_{1}a_{1} + c_{2}a_{2} + c_{3}a_{3} = 0$ then $c_1 = c_2 = c_3 = 0$ I am completely stuck on where to go with this problem. I would think that linearly independent then the null space of the space can only be equal to $0$. Would this prove it?","I am trying to prove that $a_1$, $a_2$, $a_3$ are linearly independent. I am asked to use vector product and prove that if $c_{1}a_{1} + c_{2}a_{2} + c_{3}a_{3} = 0$ then $c_1 = c_2 = c_3 = 0$ I am completely stuck on where to go with this problem. I would think that linearly independent then the null space of the space can only be equal to $0$. Would this prove it?",,"['linear-algebra', 'matrices']"
54,Diagonalizable properties of triangular matrix,Diagonalizable properties of triangular matrix,,How to show that an upper triangular matrix with identical diagonal entries is diagonalizable iff it is already diagonal?,How to show that an upper triangular matrix with identical diagonal entries is diagonalizable iff it is already diagonal?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
55,Is matrix diagonalization unique?,Is matrix diagonalization unique?,,"From the following statement, it seems matrix diagonalization is just eigen decomposition. Diagonalizing a matrix is also equivalent to finding the matrix's eigenvalues, which turn out to be precisely the entries of the diagonalized matrix. Similarly, the eigenvectors make up the new set of axes corresponding to the diagonal matrix. http://mathworld.wolfram.com/MatrixDiagonalization.html However, from what I have learned, Spectral Theorem is closest to this conclusion. But how the spectral theorem is related to it, or is there some other theorem grants this statement? Spectral Theorem: Suppose that $V$ is a complex inner-product space and $T \in L(V)$. Then $V$ has an orthonormal basis consisting of eigenvectors of $T$ if and only if $T$ is normal.","From the following statement, it seems matrix diagonalization is just eigen decomposition. Diagonalizing a matrix is also equivalent to finding the matrix's eigenvalues, which turn out to be precisely the entries of the diagonalized matrix. Similarly, the eigenvectors make up the new set of axes corresponding to the diagonal matrix. http://mathworld.wolfram.com/MatrixDiagonalization.html However, from what I have learned, Spectral Theorem is closest to this conclusion. But how the spectral theorem is related to it, or is there some other theorem grants this statement? Spectral Theorem: Suppose that $V$ is a complex inner-product space and $T \in L(V)$. Then $V$ has an orthonormal basis consisting of eigenvectors of $T$ if and only if $T$ is normal.",,['linear-algebra']
56,Usage of inverse Laplace transform,Usage of inverse Laplace transform,,"At my current study level in college, use of inverse Laplace transform is not mentioned well - textbooks say ""use tables."" So, can anyone show me how to use inverse Lapalce transform? And also proof? Edit: I am not asking how to use tables to solve inverse Laplace transform - I ACTUALLY want to know how to solve inverse Laplace transform  using the inverse Laplace transform formula: $$f(t) = \mathcal{L}^{-1} \{F(s)\} = \frac{1}{2 \pi i} \lim_{T\to\infty}\int_{ \gamma - i T}^{ \gamma + i T} e^{st} F(s)\,ds$$","At my current study level in college, use of inverse Laplace transform is not mentioned well - textbooks say ""use tables."" So, can anyone show me how to use inverse Lapalce transform? And also proof? Edit: I am not asking how to use tables to solve inverse Laplace transform - I ACTUALLY want to know how to solve inverse Laplace transform  using the inverse Laplace transform formula: $$f(t) = \mathcal{L}^{-1} \{F(s)\} = \frac{1}{2 \pi i} \lim_{T\to\infty}\int_{ \gamma - i T}^{ \gamma + i T} e^{st} F(s)\,ds$$",,"['linear-algebra', 'signal-processing', 'laplace-transform', 'integral-transforms']"
57,Induced Exact Sequence of Dual Spaces,Induced Exact Sequence of Dual Spaces,,"So given a short exact sequence of vector spaces $$0\longrightarrow U\longrightarrow V \longrightarrow W\longrightarrow 0$$  With linear transformations $S$ and $T$ from left to right in the non-trivial places. I want to show that the corresponding sequence of duals is also exact, namely that $$0\longleftarrow U^*\longleftarrow V^* \longleftarrow W^*\longleftarrow 0$$ with functions $\circ S$ and $\circ T$ again from left to right in the non-trivial spots.  So I'm a bit lost here.  Namely, I'm not chasing with particular effectiveness.  Certainly this ""circle"" notation is pretty suggestive, and I suspect that this is a generalization of the ordinary transpose, but I'm not entirely sure there either. Any hints and tips are much appreciated.","So given a short exact sequence of vector spaces $$0\longrightarrow U\longrightarrow V \longrightarrow W\longrightarrow 0$$  With linear transformations $S$ and $T$ from left to right in the non-trivial places. I want to show that the corresponding sequence of duals is also exact, namely that $$0\longleftarrow U^*\longleftarrow V^* \longleftarrow W^*\longleftarrow 0$$ with functions $\circ S$ and $\circ T$ again from left to right in the non-trivial spots.  So I'm a bit lost here.  Namely, I'm not chasing with particular effectiveness.  Certainly this ""circle"" notation is pretty suggestive, and I suspect that this is a generalization of the ordinary transpose, but I'm not entirely sure there either. Any hints and tips are much appreciated.",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'duality-theorems']"
58,"Characteristic polynomial equals minimal polynomial iff $x, Mx, \ldots, M^{n-1} x$ are linearly independent",Characteristic polynomial equals minimal polynomial iff  are linearly independent,"x, Mx, \ldots, M^{n-1} x","I have been trying to compile conditions for when characteristic polynomials equal minimal polynomials and I have found a result that I think is fairly standard but I have not been able to come up with a proof for.  Any references to a proof would be greatly appreciated. Let $M\in M_n(\Bbb F)$ and let $c_M$ be the characteristic polynomial of $M$ and $p_M$ be the minimal polynomial of $M$ . How do we show that $p_M = c_M$ if and only if there exists a column vector $x$ such that $x, Mx, \ldots M^{n-1} x$ are linearly independent ?",I have been trying to compile conditions for when characteristic polynomials equal minimal polynomials and I have found a result that I think is fairly standard but I have not been able to come up with a proof for.  Any references to a proof would be greatly appreciated. Let and let be the characteristic polynomial of and be the minimal polynomial of . How do we show that if and only if there exists a column vector such that are linearly independent ?,"M\in M_n(\Bbb F) c_M M p_M M p_M = c_M x x, Mx, \ldots M^{n-1} x","['linear-algebra', 'matrices']"
59,On isometric affine transformations,On isometric affine transformations,,"Somewhat prompted by the discussions of Qiaochu Yuan and Aryabhata in this question , I realized that my understanding of linear/affine transformations thus far had been built on a convoluted series of circular arguments. I will now be asking a question in order to patch the gaps in my knowledge. Due to my innate tendency to view things geometrically, I had always taken for granted the fact that rotations, reflections, and translations, among other affine transformations, are isometric (That is to say, no matter how you move, spin, or reflect an object, all the lengths and angles do not change at all). To put it another way, I considered these transformations being isometries as postulates. My question, then, is if there is a rigorous (so probably non-geometric) way of justifying that these transformations are isometries.","Somewhat prompted by the discussions of Qiaochu Yuan and Aryabhata in this question , I realized that my understanding of linear/affine transformations thus far had been built on a convoluted series of circular arguments. I will now be asking a question in order to patch the gaps in my knowledge. Due to my innate tendency to view things geometrically, I had always taken for granted the fact that rotations, reflections, and translations, among other affine transformations, are isometric (That is to say, no matter how you move, spin, or reflect an object, all the lengths and angles do not change at all). To put it another way, I considered these transformations being isometries as postulates. My question, then, is if there is a rigorous (so probably non-geometric) way of justifying that these transformations are isometries.",,"['linear-algebra', 'geometry', 'transformational-geometry']"
60,Coordinate-Free Proof of the Orthogonal Decomposition Theorem,Coordinate-Free Proof of the Orthogonal Decomposition Theorem,,"The Orthogonal Decomposition Theorem says that, if $V$ is a subspace of $\mathbb R^n$ , then for any vector $v$ in $\mathbb R^n$ , $$ v = v_1 + v_2, $$ where $v_1$ and $v_2$ are the projections of $v$ onto $V$ and $V^{\perp}$ , respectively. The only proof that I have seen constructs a basis that can be partitioned into a basis for $V$ and a basis for $V^{\perp}$ . Is there a proof that does not require constructing a basis but just uses coordinate-free geometric arguments?","The Orthogonal Decomposition Theorem says that, if is a subspace of , then for any vector in , where and are the projections of onto and , respectively. The only proof that I have seen constructs a basis that can be partitioned into a basis for and a basis for . Is there a proof that does not require constructing a basis but just uses coordinate-free geometric arguments?","V \mathbb R^n v \mathbb R^n 
v = v_1 + v_2,
 v_1 v_2 v V V^{\perp} V V^{\perp}","['linear-algebra', 'functional-analysis']"
61,Determinant of a matrix after changes,Determinant of a matrix after changes,,"If $\det \begin{pmatrix}a&1&d\\ b&1&e\\ c&1&f\end{pmatrix}=1$ and $\det \begin{pmatrix}a&1&d\\ b&2&e\\ c&3&f\end{pmatrix}=1$, what is $\det \begin{pmatrix}a&-4&d\\ b&-5&e\\ c&-6&f\end{pmatrix}$? So I am aware about all the different operations and what changes they bring to the value of the determinant, but I am not exactly sure which one of them is being applied here. There is no constant multiplication or an addition or subtraction by a row. I have tried to add and subtract various multiples of the matrices from each other as well but not to any avail. I believe I am just not spotting something. Any help?","If $\det \begin{pmatrix}a&1&d\\ b&1&e\\ c&1&f\end{pmatrix}=1$ and $\det \begin{pmatrix}a&1&d\\ b&2&e\\ c&3&f\end{pmatrix}=1$, what is $\det \begin{pmatrix}a&-4&d\\ b&-5&e\\ c&-6&f\end{pmatrix}$? So I am aware about all the different operations and what changes they bring to the value of the determinant, but I am not exactly sure which one of them is being applied here. There is no constant multiplication or an addition or subtraction by a row. I have tried to add and subtract various multiples of the matrices from each other as well but not to any avail. I believe I am just not spotting something. Any help?",,"['linear-algebra', 'matrices', 'determinant']"
62,Matrix determinant lemma derivation,Matrix determinant lemma derivation,,"While reading this wikipedia article on the determinant lemma, I stumbled upon this expression (in a proof section): \begin{equation} \begin{pmatrix} \mathbf{I} & 0 \\ \mathbf{v}^\mathrm{T} & 1 \end{pmatrix} \begin{pmatrix} \mathbf{I}+\mathbf{uv}^\mathrm{T} & \mathbf{u} \\ 0 & 1 \end{pmatrix} \begin{pmatrix} \mathbf{I} & 0 \\ -\mathbf{v}^\mathrm{T} & 1 \end{pmatrix} = \begin{pmatrix} \mathbf{I} & \mathbf{u} \\ 0 & 1 + \mathbf{v}^\mathrm{T}\mathbf{u} \end{pmatrix}. \end{equation} Although I see that this equation ""works"", I'm interested in HOW this thing was invented. For example, why we have $u$ term in a central block matrix of the left side? UPD A little clarification of the question above. Let \begin{equation}L = \begin{pmatrix} \mathbf{I} & 0 \\ \mathbf{v}^\mathrm{T} & 1 \end{pmatrix} \end{equation} I see that \begin{equation} L^{-1}= \begin{pmatrix} \mathbf{I} & 0 \\ -\mathbf{v}^\mathrm{T} & 1 \end{pmatrix}  \end{equation} and hence the first equation looks like  \begin{equation} L\begin{pmatrix}\mathbf{I + uv^T} & \mathbf{u} \\ 0 & 1\end{pmatrix}L^{-1} =  \begin{pmatrix} \mathbf{I} & \mathbf{u} \\ 0 & 1 + \mathbf{v}^\mathrm{T}\mathbf{u} \end{pmatrix}. \end{equation} I see that $\det(L) = \det(L^{-1}) = 1 $. Hence determinants of RHS and LHS are equal as well. What I do not understand is how we jumped from simple $\begin{pmatrix}\mathbf{I + uv^T} & \mathbf{0} \\ 0 & 1\end{pmatrix}$ or $\begin{pmatrix}\mathbf{I} & \mathbf{u} \\ \mathbf{-v}^T & 1\end{pmatrix}$ to $\begin{pmatrix}\mathbf{I + uv^T} & \mathbf{u} \\ 0 & 1\end{pmatrix}$ for a central part of LHS. Thank you.","While reading this wikipedia article on the determinant lemma, I stumbled upon this expression (in a proof section): \begin{equation} \begin{pmatrix} \mathbf{I} & 0 \\ \mathbf{v}^\mathrm{T} & 1 \end{pmatrix} \begin{pmatrix} \mathbf{I}+\mathbf{uv}^\mathrm{T} & \mathbf{u} \\ 0 & 1 \end{pmatrix} \begin{pmatrix} \mathbf{I} & 0 \\ -\mathbf{v}^\mathrm{T} & 1 \end{pmatrix} = \begin{pmatrix} \mathbf{I} & \mathbf{u} \\ 0 & 1 + \mathbf{v}^\mathrm{T}\mathbf{u} \end{pmatrix}. \end{equation} Although I see that this equation ""works"", I'm interested in HOW this thing was invented. For example, why we have $u$ term in a central block matrix of the left side? UPD A little clarification of the question above. Let \begin{equation}L = \begin{pmatrix} \mathbf{I} & 0 \\ \mathbf{v}^\mathrm{T} & 1 \end{pmatrix} \end{equation} I see that \begin{equation} L^{-1}= \begin{pmatrix} \mathbf{I} & 0 \\ -\mathbf{v}^\mathrm{T} & 1 \end{pmatrix}  \end{equation} and hence the first equation looks like  \begin{equation} L\begin{pmatrix}\mathbf{I + uv^T} & \mathbf{u} \\ 0 & 1\end{pmatrix}L^{-1} =  \begin{pmatrix} \mathbf{I} & \mathbf{u} \\ 0 & 1 + \mathbf{v}^\mathrm{T}\mathbf{u} \end{pmatrix}. \end{equation} I see that $\det(L) = \det(L^{-1}) = 1 $. Hence determinants of RHS and LHS are equal as well. What I do not understand is how we jumped from simple $\begin{pmatrix}\mathbf{I + uv^T} & \mathbf{0} \\ 0 & 1\end{pmatrix}$ or $\begin{pmatrix}\mathbf{I} & \mathbf{u} \\ \mathbf{-v}^T & 1\end{pmatrix}$ to $\begin{pmatrix}\mathbf{I + uv^T} & \mathbf{u} \\ 0 & 1\end{pmatrix}$ for a central part of LHS. Thank you.",,"['linear-algebra', 'matrices', 'determinant']"
63,Why is the set of matrices with determinant zero not a subspace?,Why is the set of matrices with determinant zero not a subspace?,,"I'm reading my linear algebra textbook, and it says word for word: The following set is not a subspace: the set of all $2\times 2$ matrices $B$ such that $\det(B)=0$. I just need help trying to understand what that means.","I'm reading my linear algebra textbook, and it says word for word: The following set is not a subspace: the set of all $2\times 2$ matrices $B$ such that $\det(B)=0$. I just need help trying to understand what that means.",,"['linear-algebra', 'matrices', 'determinant']"
64,Is the zero vector in $\mathbb{R}^n$ by itself a subspace of $\mathbb{R}^n$?,Is the zero vector in  by itself a subspace of ?,\mathbb{R}^n \mathbb{R}^n,"W is a subspace of $\mathbb{R}^n$ iff The zero vector ∈ W. X + Y ∈ W for any X, Y ∈ W. aX ∈ W for any X ∈ W and a ∈ R. So, given W = { X : X = [x1...], x1 = 0, x2 = 0, ... xn = 0 } ∈ Rn The zero vector ∈ W because each X in W is the zero vector by definition, X + Y ∈ W because [0...] + [0...] = [0...] aX ∈ W because a[0...] = [0...] So if I understand this correctly, the zero vector is itself a subspace of $\mathbb{R}^n$. Is this correct? In addition, can this be extended to say that for any W = { X : X = [$x_1$...] } ∈ $\mathbb{R}^n$, assuming W is a subspace of $\mathbb{R}^n$, $x_i$ (an arbitrary component of W) can only be a constant if it is 0? (I.e., $x_i$ can't be 1 or 2, but can be 0)","W is a subspace of $\mathbb{R}^n$ iff The zero vector ∈ W. X + Y ∈ W for any X, Y ∈ W. aX ∈ W for any X ∈ W and a ∈ R. So, given W = { X : X = [x1...], x1 = 0, x2 = 0, ... xn = 0 } ∈ Rn The zero vector ∈ W because each X in W is the zero vector by definition, X + Y ∈ W because [0...] + [0...] = [0...] aX ∈ W because a[0...] = [0...] So if I understand this correctly, the zero vector is itself a subspace of $\mathbb{R}^n$. Is this correct? In addition, can this be extended to say that for any W = { X : X = [$x_1$...] } ∈ $\mathbb{R}^n$, assuming W is a subspace of $\mathbb{R}^n$, $x_i$ (an arbitrary component of W) can only be a constant if it is 0? (I.e., $x_i$ can't be 1 or 2, but can be 0)",,"['linear-algebra', 'vector-spaces']"
65,Show $A$ and $B$ have a common eigenvector if $A^2=B^2=I$,Show  and  have a common eigenvector if,A B A^2=B^2=I,"Let $n$ be a positive odd integer and let $A,B\in M_n(R)$ such that $A^2=B^2=I$. Prove that $A$ and $B$ have a common eigenvector.","Let $n$ be a positive odd integer and let $A,B\in M_n(R)$ such that $A^2=B^2=I$. Prove that $A$ and $B$ have a common eigenvector.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
66,Why don't $A^TAx = A^Tb$ and $Ax=b$ have the same solution?,Why don't  and  have the same solution?,A^TAx = A^Tb Ax=b,"Suppose $A$ is a $m\times n$ matrix, $x$ is a vector of unkowns and $b$ is a vector of know entries. Why don't $$Ax=b$$ and $$A^TAx = A^Tb$$ have the same solution ($x$)? It seems to me that I could get from the first equation to the second equation by simply multiplying both sides by $A^T$, so in my conception this wouldn't change the solutions. Am I missing something?","Suppose $A$ is a $m\times n$ matrix, $x$ is a vector of unkowns and $b$ is a vector of know entries. Why don't $$Ax=b$$ and $$A^TAx = A^Tb$$ have the same solution ($x$)? It seems to me that I could get from the first equation to the second equation by simply multiplying both sides by $A^T$, so in my conception this wouldn't change the solutions. Am I missing something?",,"['linear-algebra', 'systems-of-equations', 'least-squares']"
67,Eigenvalues of a bipartite graph,Eigenvalues of a bipartite graph,,"Let $X$ be a connected graph with maximum eigenvalue $k$. Assume that $-k$ is also an eigenvalue. I wish to prove that $X$ is bipartite. Now if $\vec{x}=(x_1,\cdots ,x_n)$ is the eigenvector for $-k$ then I can show that for the vector $\vec{y}$ whose entries are $(|x_1|,\cdots ,|x_n|)$ we have $y'Ay=ky'y$. From here can I conclude that $\vec{y}$ is an eigenvector with eigenvalue $k$? How to proceed to prove this result? Thanks.","Let $X$ be a connected graph with maximum eigenvalue $k$. Assume that $-k$ is also an eigenvalue. I wish to prove that $X$ is bipartite. Now if $\vec{x}=(x_1,\cdots ,x_n)$ is the eigenvector for $-k$ then I can show that for the vector $\vec{y}$ whose entries are $(|x_1|,\cdots ,|x_n|)$ we have $y'Ay=ky'y$. From here can I conclude that $\vec{y}$ is an eigenvector with eigenvalue $k$? How to proceed to prove this result? Thanks.",,"['linear-algebra', 'graph-theory']"
68,Can we say that there exist an integer $n$ such $A+nB$ invertible?,Can we say that there exist an integer  such  invertible?,n A+nB,"If $A$ and $B$ are $3\times 3$ matrices and $A$ is invertible, then can we say that there exists an integer $n$ such that $A+nB$ invertible? I was trying to show this by choosing $n$ such that eignevalues of $A+nB$ are non-zero. In the case where $B = I$ we can find the eigenvalues of $A+nB$ that would be $\lambda + nB$ (though I am not certain about its proof). This choosing of $n$ such that $\lambda$ is not equal to $-n$ times an eigenvalue of $B$ will serve the purpose. But I am not sure about general $B$. What if I take arbitrary matrices $A$ and $B$.","If $A$ and $B$ are $3\times 3$ matrices and $A$ is invertible, then can we say that there exists an integer $n$ such that $A+nB$ invertible? I was trying to show this by choosing $n$ such that eignevalues of $A+nB$ are non-zero. In the case where $B = I$ we can find the eigenvalues of $A+nB$ that would be $\lambda + nB$ (though I am not certain about its proof). This choosing of $n$ such that $\lambda$ is not equal to $-n$ times an eigenvalue of $B$ will serve the purpose. But I am not sure about general $B$. What if I take arbitrary matrices $A$ and $B$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
69,"$\{0,1\}^n$ and $[0,1]^n$ notations",and  notations,"\{0,1\}^n [0,1]^n","Can someone please help me clarify the notations/definitions below: Does $\{0,1\}^n$ mean a $n$-length vector consisting of $0$s and/or $1$s? Does $[0,1]^n$ ($(0,1)^n$) mean a $n$-length vector consisting of any number between $0$ and $1$ inclusive (exclusive)? As a related question, is there a reference web page for all such definitions/notations? Or do we just need to take note of them individually as we learn. Thanks.","Can someone please help me clarify the notations/definitions below: Does $\{0,1\}^n$ mean a $n$-length vector consisting of $0$s and/or $1$s? Does $[0,1]^n$ ($(0,1)^n$) mean a $n$-length vector consisting of any number between $0$ and $1$ inclusive (exclusive)? As a related question, is there a reference web page for all such definitions/notations? Or do we just need to take note of them individually as we learn. Thanks.",,"['linear-algebra', 'notation', 'definition']"
70,"Finding a point along a line in three dimensions, given two points","Finding a point along a line in three dimensions, given two points",,"I need to find a point along a line segment in three-dimensional space, given two points. For example: Find a point along a line segment between point $a(-2, -2, -2)$ and $b(3, 3, 3)$ which is at distance $3$ from point $a$, in the direction of point $b$. I've been trying to figure out a way to do it with three parametric equations, or with similar triangles, but I can't seem to make sense of it. Any help is greatly appreciated.","I need to find a point along a line segment in three-dimensional space, given two points. For example: Find a point along a line segment between point $a(-2, -2, -2)$ and $b(3, 3, 3)$ which is at distance $3$ from point $a$, in the direction of point $b$. I've been trying to figure out a way to do it with three parametric equations, or with similar triangles, but I can't seem to make sense of it. Any help is greatly appreciated.",,"['linear-algebra', 'geometry', 'vector-spaces']"
71,difference between eigenspace and generalized eigenspace,difference between eigenspace and generalized eigenspace,,What are the differences between eigenspace and generalized eigenspace? Why do we need generalized eigenspace? Can an arbitrary matrix (not necessarily over $\mathbb{C}$) have a Jordan form? Thank you very much.,What are the differences between eigenspace and generalized eigenspace? Why do we need generalized eigenspace? Can an arbitrary matrix (not necessarily over $\mathbb{C}$) have a Jordan form? Thank you very much.,,['linear-algebra']
72,"What does ""well defined"" mean?","What does ""well defined"" mean?",,"On page 153 of Linear Algebra Done Right the second edition, it says: Define a linear map $S_1: \text{range}(\sqrt{T^*T} ) \to \text{range}(T)$ by: 7.43: $S_1 (\sqrt{T^* T}v)=Tv$ First we must check that $S_1$ is well defined . To do this, suppose $v_1, v_2 \in V$ are such that $\sqrt {T^*T}v_1 = \sqrt{T^*T}v_2$ . For the definition given by 7.43 to make sense, we must show that $Tv_1=T v_2$ . It is not entirely clear to me what the term 'well-defined' means here. Can someone clarify? Thanks","On page 153 of Linear Algebra Done Right the second edition, it says: Define a linear map by: 7.43: First we must check that is well defined . To do this, suppose are such that . For the definition given by 7.43 to make sense, we must show that . It is not entirely clear to me what the term 'well-defined' means here. Can someone clarify? Thanks","S_1: \text{range}(\sqrt{T^*T} ) \to \text{range}(T) S_1 (\sqrt{T^* T}v)=Tv S_1 v_1, v_2 \in V \sqrt {T^*T}v_1 = \sqrt{T^*T}v_2 Tv_1=T v_2","['linear-algebra', 'definition']"
73,How to prove that every orthogonal matrix has determinant $\pm1$ using limits (Strang 5.1.8)?,How to prove that every orthogonal matrix has determinant  using limits (Strang 5.1.8)?,\pm1,"The problem of interest from Gilbert Strang's Introduction to Linear Algebra Section 5.1 is as follows. Prove that every orthogonal matrix ( $Q^TQ = I$ ) has determinant $1$ or $-1$ . (b) Use only the product rule. If $|\det(Q)| > 1$ , then $\det(Q^n) = (\det(Q))^n$ blows up. How do you know this can't happen to $Q^n$ . Anyone who has even sniffed a Strang textbook knows that the words inside are filled with ambiguity; this problem is no exception. From what I can interpret, Strang is giving the first steps to prove the desired result. Assume $|\det(Q)| \neq 1$ . If $|\det(Q)| > 1$ , then $\det(Q^n) = (\det(Q))^n$ and thus $|\det(Q^n)| \to \infty$ as $n \to \infty$ . And this contradicts... what exactly? In the solution manual (by Strang), this is the solution. $Q^n$ stays orthogonal so its determinant can't blow up as $n \to \infty$ . This sounds like circular logic to me. Since $Q$ is orthogonal, so is $Q^n$ . Since $Q^n$ is orthogonal, $|\det(Q^n)| \not\to \infty$ as $n \to \infty$ . The only way this is true is if $|\det(Q^n)| \le 1$ , which is what we are trying to prove in the first place. Is my interpretation correct? Is Strang using circular logic here or am I missing something? Also, what is Strang getting at with his hint? Thanks for the help.","The problem of interest from Gilbert Strang's Introduction to Linear Algebra Section 5.1 is as follows. Prove that every orthogonal matrix ( ) has determinant or . (b) Use only the product rule. If , then blows up. How do you know this can't happen to . Anyone who has even sniffed a Strang textbook knows that the words inside are filled with ambiguity; this problem is no exception. From what I can interpret, Strang is giving the first steps to prove the desired result. Assume . If , then and thus as . And this contradicts... what exactly? In the solution manual (by Strang), this is the solution. stays orthogonal so its determinant can't blow up as . This sounds like circular logic to me. Since is orthogonal, so is . Since is orthogonal, as . The only way this is true is if , which is what we are trying to prove in the first place. Is my interpretation correct? Is Strang using circular logic here or am I missing something? Also, what is Strang getting at with his hint? Thanks for the help.",Q^TQ = I 1 -1 |\det(Q)| > 1 \det(Q^n) = (\det(Q))^n Q^n |\det(Q)| \neq 1 |\det(Q)| > 1 \det(Q^n) = (\det(Q))^n |\det(Q^n)| \to \infty n \to \infty Q^n n \to \infty Q Q^n Q^n |\det(Q^n)| \not\to \infty n \to \infty |\det(Q^n)| \le 1,"['linear-algebra', 'determinant', 'orthogonal-matrices']"
74,Meaning of matrix 'diag' operator with matrix arguments,Meaning of matrix 'diag' operator with matrix arguments,,"I came across this definition in a paper and can't figure out what it is supposed to represent: I understand that there is a 'diag' operator which when given a vector argument creates a matrix with the vector values along the diagonal, but I can't understand how such an operator would work on a set of matrices.","I came across this definition in a paper and can't figure out what it is supposed to represent: I understand that there is a 'diag' operator which when given a vector argument creates a matrix with the vector values along the diagonal, but I can't understand how such an operator would work on a set of matrices.",,"['linear-algebra', 'matrices', 'notation', 'block-matrices']"
75,Are there non-square matrices that are both left and right invertible?,Are there non-square matrices that are both left and right invertible?,,"I am aware that invertible square matrices are left invertible and right invertible, and that the left and right inverses are equal. However, I was wondering whether exists a non square $m\times n$ matrice $A$ , so that exist both: An $n\times m$ matrix $B$ so that $AB = I_m$ An $n\times m$ matrix $C$ so that $CA = I_n$ I just couldn't think of an example nor of a proof that these two conditions provide that A is necessarily square.","I am aware that invertible square matrices are left invertible and right invertible, and that the left and right inverses are equal. However, I was wondering whether exists a non square matrice , so that exist both: An matrix so that An matrix so that I just couldn't think of an example nor of a proof that these two conditions provide that A is necessarily square.",m\times n A n\times m B AB = I_m n\times m C CA = I_n,"['linear-algebra', 'matrices']"
76,Linear equations given by $Ax = b$ have a solution if and only if $\operatorname{rank}(A|b) = \operatorname{rank}(A)$,Linear equations given by  have a solution if and only if,Ax = b \operatorname{rank}(A|b) = \operatorname{rank}(A),"Theorem: Given a system of linear equations $Ax = b$ where $A \in M_{m \times n}\left(\mathbb{R}\right)$ , $ x \in \mathbb{R}^{n}_\text{col} $ , $ b \in \mathbb{R}^{m}_\text{col}$ Deduce that a solution $x$ exists if and only if $\operatorname{rank}\left(A|b\right) = \operatorname{rank}\left(A\right)$ where $A|b$ is the augmented coefficient matrix of this system I am having trouble proving the above theorem from my Linear Algebra course, I understand that A|b must reduce under elementary row operations to a form which is consistent but I don't understand exactly why the matrix A|b need have the same rank as A for this to happen. Please correct me if I am mistaken","Theorem: Given a system of linear equations where , , Deduce that a solution exists if and only if where is the augmented coefficient matrix of this system I am having trouble proving the above theorem from my Linear Algebra course, I understand that A|b must reduce under elementary row operations to a form which is consistent but I don't understand exactly why the matrix A|b need have the same rank as A for this to happen. Please correct me if I am mistaken",Ax = b A \in M_{m \times n}\left(\mathbb{R}\right)  x \in \mathbb{R}^{n}_\text{col}   b \in \mathbb{R}^{m}_\text{col} x \operatorname{rank}\left(A|b\right) = \operatorname{rank}\left(A\right) A|b,"['linear-algebra', 'matrices', 'systems-of-equations']"
77,Pattern in twin primes,Pattern in twin primes,,"I recently noticed a pattern in twin primes. My questions are: does this pattern continue to hold indefinitely, and how would I prove it? Here's the pattern: For the $n$ th prime, there exists exactly $n-2$ twin prime pairs that can be created as follows: $p_n$ is the $n$ th prime, $P_p=\prod_{1<m<n}p_m$ $(p_nP_p-4, p_nP_p-2)$ Here's what I've worked up to: $n=3$ $(p_n=5)$ has $3-2=1$ twin prime pairs $P_p=3$ gives $(15-4,15-2)=(11,13)$ $n=4$ has 2 $P_p=3$ gives $(17,19)$ $P_p=3\cdot 5$ gives $(101,103)$ $n=5$ has 3 (29,31), (227,229), (1151,1153) $n=6$ has 4 (191,193), (269,271), (2141,2143), (2999,3001) $n=7$ has 5 (659,661), (2801,2803), (4637,4639), (23201,23203), (255251,255253) Again, my questions are: does this pattern continue to hold indefinitely, and how would I prove it? P.S. How about a pic of a brute force Mathematica script up to like 20?","I recently noticed a pattern in twin primes. My questions are: does this pattern continue to hold indefinitely, and how would I prove it? Here's the pattern: For the th prime, there exists exactly twin prime pairs that can be created as follows: is the th prime, Here's what I've worked up to: has twin prime pairs gives has 2 gives gives has 3 (29,31), (227,229), (1151,1153) has 4 (191,193), (269,271), (2141,2143), (2999,3001) has 5 (659,661), (2801,2803), (4637,4639), (23201,23203), (255251,255253) Again, my questions are: does this pattern continue to hold indefinitely, and how would I prove it? P.S. How about a pic of a brute force Mathematica script up to like 20?","n n-2 p_n n P_p=\prod_{1<m<n}p_m (p_nP_p-4, p_nP_p-2) n=3 (p_n=5) 3-2=1 P_p=3 (15-4,15-2)=(11,13) n=4 P_p=3 (17,19) P_p=3\cdot 5 (101,103) n=5 n=6 n=7","['linear-algebra', 'proof-writing', 'solution-verification', 'prime-numbers']"
78,Linear Algebra: Difference Matrix,Linear Algebra: Difference Matrix,,"The above part of my linear algebra is giving me trouble. It says: This A is a ""difference matrix""... I have colored in yellow what I think the book is meaning by the difference matrix. It is a difference matrix because it only has subtraction occurring. Is this true?","The above part of my linear algebra is giving me trouble. It says: This A is a ""difference matrix""... I have colored in yellow what I think the book is meaning by the difference matrix. It is a difference matrix because it only has subtraction occurring. Is this true?",,"['linear-algebra', 'terminology']"
79,Is it possible to diagonalize a singular matrix?,Is it possible to diagonalize a singular matrix?,,"I have not seen anywhere written that it is impossible, but it seems impossible, so I want to check if I missed something. According to a theorem, an $n\times n$ matrix is diagonalizable if it has $n$ independent eigenvectors. Let's say, the matrix has $1$ row with only zeros (worst singular case). As it has one row with only zeros, it will zero out the corresponding row of any vector it is multiplied by. That means, the corresponding rows of its eigenvectors have to be zero. And if one of the rows is fixed, there cannot be n independent eigenvectors. Am I correct? Did I miss anything?","I have not seen anywhere written that it is impossible, but it seems impossible, so I want to check if I missed something. According to a theorem, an $n\times n$ matrix is diagonalizable if it has $n$ independent eigenvectors. Let's say, the matrix has $1$ row with only zeros (worst singular case). As it has one row with only zeros, it will zero out the corresponding row of any vector it is multiplied by. That means, the corresponding rows of its eigenvectors have to be zero. And if one of the rows is fixed, there cannot be n independent eigenvectors. Am I correct? Did I miss anything?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
80,How to find $n+1$ equidistant vectors on an $n$-sphere?,How to find  equidistant vectors on an -sphere?,n+1 n,"Following this question, Proving the existence of a set of vectors , I'm looking for a way to find $n+1$ equidistant vectors on a Euclidean $n$-sphere. For $n=2$, you can pick the vertices of any equilateral triangle. For $n=3$, pick a tetrahedron. What about larger dimensions?","Following this question, Proving the existence of a set of vectors , I'm looking for a way to find $n+1$ equidistant vectors on a Euclidean $n$-sphere. For $n=2$, you can pick the vertices of any equilateral triangle. For $n=3$, pick a tetrahedron. What about larger dimensions?",,"['linear-algebra', 'geometry']"
81,$\Bbb{C} \otimes_{\Bbb{R}} \Bbb{C}$ and $\Bbb{C} \otimes_{\Bbb{C}} \Bbb{C}$ are not isomorphic as $\Bbb{R}$-vector spaces [duplicate],and  are not isomorphic as -vector spaces [duplicate],\Bbb{C} \otimes_{\Bbb{R}} \Bbb{C} \Bbb{C} \otimes_{\Bbb{C}} \Bbb{C} \Bbb{R},"This question already has answers here : Are $\mathbb{C} \otimes _\mathbb{R} \mathbb{C}$ and $\mathbb{C} \otimes _\mathbb{C} \mathbb{C}$ isomorphic as $\mathbb{R}$-vector spaces? (3 answers) Closed 5 years ago . $\Bbb{C} \otimes_{\Bbb{R}} \Bbb{C}$ and $\Bbb{C} \otimes_{\Bbb{C}} \Bbb{C}$ are not isomorphic as $\Bbb{R}$-vector spaces. Clearly each tensor product is both a left and right $R$-module. But how do I show that they're not isomorphic? I'm trying to argue as in other examples, so in $M = \Bbb{C} \otimes_{\Bbb{C}} \Bbb{C}$, there's the simple tensor $(-1)\otimes i = (i^2) \otimes i = i \otimes(-1)$.   But those two simple tensors are not equal in $N = \Bbb{C} \otimes_{\Bbb{R}} \Bbb{C}$.  I'm not sure how to show that.  Let $\phi : M \to N$ be an isomorphism.    Then $\phi((-1)\otimes i)$ must equal $\phi (i \otimes (-1))$.","This question already has answers here : Are $\mathbb{C} \otimes _\mathbb{R} \mathbb{C}$ and $\mathbb{C} \otimes _\mathbb{C} \mathbb{C}$ isomorphic as $\mathbb{R}$-vector spaces? (3 answers) Closed 5 years ago . $\Bbb{C} \otimes_{\Bbb{R}} \Bbb{C}$ and $\Bbb{C} \otimes_{\Bbb{C}} \Bbb{C}$ are not isomorphic as $\Bbb{R}$-vector spaces. Clearly each tensor product is both a left and right $R$-module. But how do I show that they're not isomorphic? I'm trying to argue as in other examples, so in $M = \Bbb{C} \otimes_{\Bbb{C}} \Bbb{C}$, there's the simple tensor $(-1)\otimes i = (i^2) \otimes i = i \otimes(-1)$.   But those two simple tensors are not equal in $N = \Bbb{C} \otimes_{\Bbb{R}} \Bbb{C}$.  I'm not sure how to show that.  Let $\phi : M \to N$ be an isomorphism.    Then $\phi((-1)\otimes i)$ must equal $\phi (i \otimes (-1))$.",,"['linear-algebra', 'abstract-algebra', 'tensor-products']"
82,How do you orthogonally diagonalize the matrix?,How do you orthogonally diagonalize the matrix?,,How do you orthogonally diagonalize the matrix A? Matrix A = $$ \begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1  \end{bmatrix} $$,How do you orthogonally diagonalize the matrix A? Matrix A = $$ \begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1  \end{bmatrix} $$,,['linear-algebra']
83,Calculating number of lines and planes in affine space over finite field,Calculating number of lines and planes in affine space over finite field,,"Consider a finite field $F_p$ with $p$ elements; how would one calculate the number of lines and the number of planes in the affine space $F^3_p$? If one knew the number of lines through a particular point, the number of planes could then be calculated by multiplying the number of lines through the origin by the number of points on any line.","Consider a finite field $F_p$ with $p$ elements; how would one calculate the number of lines and the number of planes in the affine space $F^3_p$? If one knew the number of lines through a particular point, the number of planes could then be calculated by multiplying the number of lines through the origin by the number of points on any line.",,"['linear-algebra', 'abstract-algebra', 'geometry']"
84,How much ought a first course in Linear Algebra emphasize proofs?,How much ought a first course in Linear Algebra emphasize proofs?,,I sometimes feel that proofs crowd out a coherent vision for linear algebra.  However I also think a central theme of a Linear Algebra course is to learn reasoning even though it does not always succeed. The audience is first year undergraduate students studying mathematics and physics but maybe extended to engineers. They generally struggle with the idea of proof.,I sometimes feel that proofs crowd out a coherent vision for linear algebra.  However I also think a central theme of a Linear Algebra course is to learn reasoning even though it does not always succeed. The audience is first year undergraduate students studying mathematics and physics but maybe extended to engineers. They generally struggle with the idea of proof.,,"['linear-algebra', 'soft-question', 'education']"
85,Eigenvalues of a matrix $A$ and $e^{A}$,Eigenvalues of a matrix  and,A e^{A},"If I know the eigenvalues of $e^{A}$, what can I say about the eigenvalues of $A$ itself?","If I know the eigenvalues of $e^{A}$, what can I say about the eigenvalues of $A$ itself?",,"['linear-algebra', 'matrices']"
86,Applications of systems of linear equations,Applications of systems of linear equations,,"Sorry if this questions is overly simplistic. It's just something I haven't been able to figure out. I've been reading through quite a few linear algebra books and have gone through the various methods of solving linear systems of equations, in particular, $n$ systems in $n$ unknowns. While I understand the techniques used to solve these for the most part, I don't understand how these situations present themselves. I was wondering if anyone could provide a simple real-world example or two from data analysis, finance, economics, etc. in which the problem they were working on led to a system of $n$ equations in $n$ unknowns. I don't need the solution worked out. I just need to know the problem that resulted in the system.","Sorry if this questions is overly simplistic. It's just something I haven't been able to figure out. I've been reading through quite a few linear algebra books and have gone through the various methods of solving linear systems of equations, in particular, $n$ systems in $n$ unknowns. While I understand the techniques used to solve these for the most part, I don't understand how these situations present themselves. I was wondering if anyone could provide a simple real-world example or two from data analysis, finance, economics, etc. in which the problem they were working on led to a system of $n$ equations in $n$ unknowns. I don't need the solution worked out. I just need to know the problem that resulted in the system.",,['linear-algebra']
87,Invariant Subspace of Two Linear Involutions,Invariant Subspace of Two Linear Involutions,,I'd love some help with this practice qualifier problem: If $A$ and $B$ are two linear operators on a finite dimensional complex vector space $V$ such that $A^2=B^2=I$ then show that $V$ has a one or two dimensional subspace invariant under $A$ and $B$. Thanks!,I'd love some help with this practice qualifier problem: If $A$ and $B$ are two linear operators on a finite dimensional complex vector space $V$ such that $A^2=B^2=I$ then show that $V$ has a one or two dimensional subspace invariant under $A$ and $B$. Thanks!,,['linear-algebra']
88,How is linear algebra taught through abstract algebra?,How is linear algebra taught through abstract algebra?,,"I have a numerical (so to speak) understanding of linear algebra . I have read a proof-based book on linear algebra where the determinant is defined using adjoints of a row or column..., where the Cramer's rule and the method of Gauss to found an inverse matrix is presented and that sort of stuff. However, when I took a glimpse at Lang's treatment of matrices and linear maps in chapter XIII of his algebra book , I began to notice a lot of uses of abstract algebra definitions (such as groups, homomorphism or isomorphisms - or some similar word), and to sum up, a different approach to it (at least that is what I think - note that I am just beginning my journey on higher mathematics). My question is then whether this approach and use of different tools from abstract algebra is used to correctly and fully prove the statements of linear algebra (something like what real analysis may be in this regard to calculus) or whether these abstract algebra tools are used to actually develop new and different tools. I am therefore asking what is the purpose of treating linear algebra with abstract algebra (if that makes sense). If the latter option is the case, can you give me a broad view of these tools (like names and their purposes...)?","I have a numerical (so to speak) understanding of linear algebra . I have read a proof-based book on linear algebra where the determinant is defined using adjoints of a row or column..., where the Cramer's rule and the method of Gauss to found an inverse matrix is presented and that sort of stuff. However, when I took a glimpse at Lang's treatment of matrices and linear maps in chapter XIII of his algebra book , I began to notice a lot of uses of abstract algebra definitions (such as groups, homomorphism or isomorphisms - or some similar word), and to sum up, a different approach to it (at least that is what I think - note that I am just beginning my journey on higher mathematics). My question is then whether this approach and use of different tools from abstract algebra is used to correctly and fully prove the statements of linear algebra (something like what real analysis may be in this regard to calculus) or whether these abstract algebra tools are used to actually develop new and different tools. I am therefore asking what is the purpose of treating linear algebra with abstract algebra (if that makes sense). If the latter option is the case, can you give me a broad view of these tools (like names and their purposes...)?",,"['linear-algebra', 'abstract-algebra', 'soft-question']"
89,Intuitively Understanding Double Dual of a Vector Space,Intuitively Understanding Double Dual of a Vector Space,,"I am trying to see if someone can help me understand the isomorphism between $V$ and $V''$ a bit more intuitively . I understand that the dual space of $V$ is the set of linear maps from $V$ to $\mathbb{F}$ . i.e. $V' = \mathcal{L}(V, \mathbb{F})$ . Therefore, double dual of $V$ , is the set of linear maps from $V'$ to $\mathbb{F}$ , or $V'' = \mathcal{L}(V', \mathbb{F})$ . That is to say, the $V''$ is the set of linear functionals on linear functionals on $V$ . The part that gets me tripped up is the natural isomorphism $\varphi: V \rightarrow  V''$ , where $\varphi(v)(f)=f(v)$ for $f \in V'$ . I know how the proof that this is a isomorphism goes, but I am having trouble understanding it intuitively. I think of an isomorphism as a bijective map that tells me how to ""relabel"" elements in the domain to elements in the codomain. For example, the subspace $\{(0,y) | y \in \mathbb{R} \} \subset \mathbb{R}^2$ is isomorphic with the subspace $\{(x,0) | x \in \mathbb{R} \} \subset \mathbb{R^2}$ . One particular isomorphism is the map $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ defined by $(0,y) \mapsto (y,0)$ . It's clear that the rule says: take the input, and flip the coordinates. In particular, it tells me how to go from one vector space to the other clearly. However, when I try to figure out what the rule is for $\varphi: V \rightarrow  V''$ in words, I'm a little stuck. $\varphi$ takes any $v \in V$ and finds a unique map $g \in \mathcal{L}(V', \mathbb{F})$ . How does it ""find"" this unique map $g$ ? The definition $\varphi(v)(f)=f(v)$ seems to only describe what you do with $g$ , which is evaluate it with the input $f$ and $v$ - it doesn't tell me what this $g$ is, in way that's equally satisfying like the example with $\mathbb{R}^2$ above. Another way to pose my question is, how would you define $\varphi:V \rightarrow V''$ using the ""maps to"" symbol? $v \mapsto .....?$ I'm not sure what should be in the place of the .....","I am trying to see if someone can help me understand the isomorphism between and a bit more intuitively . I understand that the dual space of is the set of linear maps from to . i.e. . Therefore, double dual of , is the set of linear maps from to , or . That is to say, the is the set of linear functionals on linear functionals on . The part that gets me tripped up is the natural isomorphism , where for . I know how the proof that this is a isomorphism goes, but I am having trouble understanding it intuitively. I think of an isomorphism as a bijective map that tells me how to ""relabel"" elements in the domain to elements in the codomain. For example, the subspace is isomorphic with the subspace . One particular isomorphism is the map defined by . It's clear that the rule says: take the input, and flip the coordinates. In particular, it tells me how to go from one vector space to the other clearly. However, when I try to figure out what the rule is for in words, I'm a little stuck. takes any and finds a unique map . How does it ""find"" this unique map ? The definition seems to only describe what you do with , which is evaluate it with the input and - it doesn't tell me what this is, in way that's equally satisfying like the example with above. Another way to pose my question is, how would you define using the ""maps to"" symbol? I'm not sure what should be in the place of the .....","V V'' V V \mathbb{F} V' = \mathcal{L}(V, \mathbb{F}) V V' \mathbb{F} V'' = \mathcal{L}(V', \mathbb{F}) V'' V \varphi: V \rightarrow  V'' \varphi(v)(f)=f(v) f \in V' \{(0,y) | y \in \mathbb{R} \} \subset \mathbb{R}^2 \{(x,0) | x \in \mathbb{R} \} \subset \mathbb{R^2} T: \mathbb{R}^2 \rightarrow \mathbb{R}^2 (0,y) \mapsto (y,0) \varphi: V \rightarrow  V'' \varphi v \in V g \in \mathcal{L}(V', \mathbb{F}) g \varphi(v)(f)=f(v) g f v g \mathbb{R}^2 \varphi:V \rightarrow V'' v \mapsto .....?","['linear-algebra', 'dual-spaces']"
90,What is the basis of the vector space $l^\infty$?,What is the basis of the vector space ?,l^\infty,"We know that every vector space has a Hamel basis and also every normed space need not have a Schauder basis. As the normed space $l^\infty$ is not Separable so can't have the Schauder basis, but on the other side $l^\infty$ is also a vector space so what will be the Hamel basis of $l^\infty$?","We know that every vector space has a Hamel basis and also every normed space need not have a Schauder basis. As the normed space $l^\infty$ is not Separable so can't have the Schauder basis, but on the other side $l^\infty$ is also a vector space so what will be the Hamel basis of $l^\infty$?",,"['linear-algebra', 'functional-analysis', 'lp-spaces', 'normed-spaces', 'hamel-basis']"
91,What does this symbol $\otimes$ mean?,What does this symbol  mean?,\otimes,"I came across the symbol $\otimes$ as below and I would like to know what this symbol $\otimes$ means: $\text{.... the projection operator P is given by: }$   $$P = I_nd - \nabla G^T(\nabla G \nabla G^T)^{-1} \nabla G= I_{nd} - I_d \otimes uu^T,$$ where $I_x$ denotes the identity matrix of size $x\times x$ and $\mathbf{u}$ is the unit vector , $\mathbf{u} = (1,1,1,\dots,1)/\sqrt{n})$ in $\mathbb{R}^n$","I came across the symbol $\otimes$ as below and I would like to know what this symbol $\otimes$ means: $\text{.... the projection operator P is given by: }$   $$P = I_nd - \nabla G^T(\nabla G \nabla G^T)^{-1} \nabla G= I_{nd} - I_d \otimes uu^T,$$ where $I_x$ denotes the identity matrix of size $x\times x$ and $\mathbf{u}$ is the unit vector , $\mathbf{u} = (1,1,1,\dots,1)/\sqrt{n})$ in $\mathbb{R}^n$",,"['linear-algebra', 'notation']"
92,"Show that if $X \succeq Y$, then $\det{(X)}\ge\det{(Y)}$","Show that if , then",X \succeq Y \det{(X)}\ge\det{(Y)},"Assume that two symmetric positive definite matrices $X$ and $Y$ are such   that $X-Y$ is a positive semidefinite matrix. Show that $$\det{(X)}\ge\det{(Y)}$$ I felt this result is clear, but I can't use mathematics methods explain this why?","Assume that two symmetric positive definite matrices $X$ and $Y$ are such   that $X-Y$ is a positive semidefinite matrix. Show that $$\det{(X)}\ge\det{(Y)}$$ I felt this result is clear, but I can't use mathematics methods explain this why?",,"['linear-algebra', 'inequality', 'determinant', 'positive-definite']"
93,"How to find a transformation matrix, given coordinates of two triangles in $R^2$","How to find a transformation matrix, given coordinates of two triangles in",R^2,"I am an undergraduate student, and today I was given two triangles, $T_1$ (green) and $T_2$ (blue) in $R^2$: I was then asked to find the transformation matrix transforming $T_1$ to $T_2$.  What I understand from this is, that I need to find $F$ in the following matrix equation: $T_2 = F \cdot T_1$. where $T_1= \begin{bmatrix}2&6&8\\2&-2&6\end{bmatrix}$ $T_2= \begin{bmatrix}-2&-10&-14\\-2&-4&10\end{bmatrix}$ which are the coordinates of the corners of the triangles. The above matrix equation however, is inconsistent, so how can I find $F$? It has to be a linear mapping, not an affine one. Have tried a lot, any help greatly appreciated!","I am an undergraduate student, and today I was given two triangles, $T_1$ (green) and $T_2$ (blue) in $R^2$: I was then asked to find the transformation matrix transforming $T_1$ to $T_2$.  What I understand from this is, that I need to find $F$ in the following matrix equation: $T_2 = F \cdot T_1$. where $T_1= \begin{bmatrix}2&6&8\\2&-2&6\end{bmatrix}$ $T_2= \begin{bmatrix}-2&-10&-14\\-2&-4&10\end{bmatrix}$ which are the coordinates of the corners of the triangles. The above matrix equation however, is inconsistent, so how can I find $F$? It has to be a linear mapping, not an affine one. Have tried a lot, any help greatly appreciated!",,"['linear-algebra', 'matrices']"
94,What does it mean for a subspace to be stable?,What does it mean for a subspace to be stable?,,"I'm looking through a proof for a spectral theorem, but I can't figure out what it means for a subspace to be stable. $\dots \mathbb{C}v$ is $T$-stable (for some $v$ that is an   eigenvector of an endomorphism $T: V \to V$, where $V$ is a vector   space over a field $k$). Then the orthogonal complement $(\mathbb{C}v)^{\perp}$ is also $T$-stable...","I'm looking through a proof for a spectral theorem, but I can't figure out what it means for a subspace to be stable. $\dots \mathbb{C}v$ is $T$-stable (for some $v$ that is an   eigenvector of an endomorphism $T: V \to V$, where $V$ is a vector   space over a field $k$). Then the orthogonal complement $(\mathbb{C}v)^{\perp}$ is also $T$-stable...",,"['linear-algebra', 'abstract-algebra']"
95,What is the transformation representation/interpretation of symmetric matrices?,What is the transformation representation/interpretation of symmetric matrices?,,"I know that a matrix stands for some kind of linear transformation. such as  $$ \left( \begin{matrix} 1&m\\ 0&1 \end{matrix} \right) $$ as a shear mapping matrix. There are all kinds of transformations including rotation, reflection, scaling, shear mapping, squeeze mapping and projection.(Are there any more? Please list them out if you can.) I try to apply some imagination to symmetric matrices, and I need more geometrical or visualizable interpretation, for this specific kind of matrix has so many useful properties. But as for such a big category of matrices (symmetric matrices), I can't figure out a common interpretation or imagination. For example, $$ \left( \begin{matrix} \frac{1}{2}&\frac{1}{2}\\ \frac{1}{2}&\frac{1}{2}\\ \end{matrix} \right) $$  is a symmetric matrix, and it's a projection matrix.  $$ \left( \begin{matrix} \frac{1}{2}&0\\ 0&\frac{1}{2}\\ \end{matrix} \right) $$  is also a symmetric matrix, but it's a scaling one. May be there are some more common and stronger interpretation(imagination/representation, anyway) for symmetric matrices, I don't know. May be you have some idea?","I know that a matrix stands for some kind of linear transformation. such as  $$ \left( \begin{matrix} 1&m\\ 0&1 \end{matrix} \right) $$ as a shear mapping matrix. There are all kinds of transformations including rotation, reflection, scaling, shear mapping, squeeze mapping and projection.(Are there any more? Please list them out if you can.) I try to apply some imagination to symmetric matrices, and I need more geometrical or visualizable interpretation, for this specific kind of matrix has so many useful properties. But as for such a big category of matrices (symmetric matrices), I can't figure out a common interpretation or imagination. For example, $$ \left( \begin{matrix} \frac{1}{2}&\frac{1}{2}\\ \frac{1}{2}&\frac{1}{2}\\ \end{matrix} \right) $$  is a symmetric matrix, and it's a projection matrix.  $$ \left( \begin{matrix} \frac{1}{2}&0\\ 0&\frac{1}{2}\\ \end{matrix} \right) $$  is also a symmetric matrix, but it's a scaling one. May be there are some more common and stronger interpretation(imagination/representation, anyway) for symmetric matrices, I don't know. May be you have some idea?",,"['linear-algebra', 'matrices']"
96,Multiplying Cardinal Numbers,Multiplying Cardinal Numbers,,"I was just reading a proof of the dimension theorem in Steven Roman's Advanced Linear Algebra. In addressing the cases of infinite bases, Roman proceeds to show that if $\mathcal{B}$ and $\mathcal{C}$ are bases of a space $V$, then $|\mathcal{B}|\leq |\mathcal{C}|$, working up to an application of the BSC-Theorem. Anyway, he uses the string $$|\mathcal{B}|\leq\aleph_0|\mathcal{C}|=|\mathcal{C}|.$$ Sorry if it's an elementary question, but why does the equality follow? Here $\mathcal{C}$ is any infinite basis. Is it definition? I tried looking up multiplication of ordinals, but didn't find anything useful. Thanks.","I was just reading a proof of the dimension theorem in Steven Roman's Advanced Linear Algebra. In addressing the cases of infinite bases, Roman proceeds to show that if $\mathcal{B}$ and $\mathcal{C}$ are bases of a space $V$, then $|\mathcal{B}|\leq |\mathcal{C}|$, working up to an application of the BSC-Theorem. Anyway, he uses the string $$|\mathcal{B}|\leq\aleph_0|\mathcal{C}|=|\mathcal{C}|.$$ Sorry if it's an elementary question, but why does the equality follow? Here $\mathcal{C}$ is any infinite basis. Is it definition? I tried looking up multiplication of ordinals, but didn't find anything useful. Thanks.",,['linear-algebra']
97,"Exercise 6.A.17 in ""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler. I am worried if my solution is ok.","Exercise 6.A.17 in ""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler. I am worried if my solution is ok.",,"I am reading ""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler. 6.A.17 Prove or disprove: there is an inner product on $\mathbb{R}^2$ such that the associated norm is given by $$||(x,y)||=\max\{x,y\}$$ for all $(x,y)\in\mathbb{R}^2$ . I solved this exercise but I am worried if my solution is ok because this exercise appears to be unnaturally too easy. My solution is the following: If there is an inner product on $\mathbb{R}^2$ such that the associated norm is given by $$||(x,y)||=\max\{x,y\}$$ for all $(x,y)\in\mathbb{R}^2$ , then $0<||(-1,-1)||=\max\{-1,-1\}=-1$ . This is a contradiction.","I am reading ""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler. 6.A.17 Prove or disprove: there is an inner product on such that the associated norm is given by for all . I solved this exercise but I am worried if my solution is ok because this exercise appears to be unnaturally too easy. My solution is the following: If there is an inner product on such that the associated norm is given by for all , then . This is a contradiction.","\mathbb{R}^2 ||(x,y)||=\max\{x,y\} (x,y)\in\mathbb{R}^2 \mathbb{R}^2 ||(x,y)||=\max\{x,y\} (x,y)\in\mathbb{R}^2 0<||(-1,-1)||=\max\{-1,-1\}=-1","['linear-algebra', 'solution-verification', 'normed-spaces', 'inner-products']"
98,Problem from the shortlist of the Romanian Mathematical olympiad,Problem from the shortlist of the Romanian Mathematical olympiad,,"Let $A,B\in\mathit{M_{n}\left(\mathbb{C}\right)}$ and $c\in\mathbb{C}^{*}$ such that $AB-BA=c\left(A-B\right)$ . Prove that $A$ and $B$ have the same eigenvalues. My idea was to prove that $\operatorname{Tr}(A^k)=\operatorname{Tr}(B^k)$ , $\forall k\in \mathbb{N}$ . For $k=1$ this is obvious since $\operatorname{Tr}(AB-BA)=0$ . I could prove this for $k=2$ by multiplying the given relation by $A$ to the left and to the right respectively and then doing the same thing for $B$ . However, I was not able to use the same technique for higher powers and mathematical induction didn't work either. I think that my idea works because this was shortlisted for Grade 11 students from Romania, who only learn linear algebra, so a solution without abstract algebra should be possible, and the result I mentioned seems suitable for such a question. Furthermore, since it works even for $k=2$ it is just a matter of finding a generalisation.","Let and such that . Prove that and have the same eigenvalues. My idea was to prove that , . For this is obvious since . I could prove this for by multiplying the given relation by to the left and to the right respectively and then doing the same thing for . However, I was not able to use the same technique for higher powers and mathematical induction didn't work either. I think that my idea works because this was shortlisted for Grade 11 students from Romania, who only learn linear algebra, so a solution without abstract algebra should be possible, and the result I mentioned seems suitable for such a question. Furthermore, since it works even for it is just a matter of finding a generalisation.","A,B\in\mathit{M_{n}\left(\mathbb{C}\right)} c\in\mathbb{C}^{*} AB-BA=c\left(A-B\right) A B \operatorname{Tr}(A^k)=\operatorname{Tr}(B^k) \forall k\in \mathbb{N} k=1 \operatorname{Tr}(AB-BA)=0 k=2 A B k=2","['linear-algebra', 'matrices', 'contest-math']"
99,Matrix Multiplication not associative when matrices are vectors?,Matrix Multiplication not associative when matrices are vectors?,,"Wikipedia states: Given three matrices A, B and C, the products (AB)C and A(BC) are   defined if and only the number of columns of A equals the number of   rows of B and the number of columns of B equals the number of rows of   C (in particular, if one of the product is defined, the other is also   defined) Row and column vectors can be thought of as just special cases of matrices. So given the above I would expect: $$(a^Tb)c = a^T(bc)$$ However the right side is undefined because you can’t multiply two column vectors, seemingly contradicting Wikipedia. Am I mistaken? If not, can we only consider matrix multiplication to be associative in contexts where we know no intermediate matrix becomes 1x1?","Wikipedia states: Given three matrices A, B and C, the products (AB)C and A(BC) are   defined if and only the number of columns of A equals the number of   rows of B and the number of columns of B equals the number of rows of   C (in particular, if one of the product is defined, the other is also   defined) Row and column vectors can be thought of as just special cases of matrices. So given the above I would expect: $$(a^Tb)c = a^T(bc)$$ However the right side is undefined because you can’t multiply two column vectors, seemingly contradicting Wikipedia. Am I mistaken? If not, can we only consider matrix multiplication to be associative in contexts where we know no intermediate matrix becomes 1x1?",,"['linear-algebra', 'matrices', 'vectors', 'associativity']"
