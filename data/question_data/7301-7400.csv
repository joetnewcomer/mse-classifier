,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Eigenvalues gone wild,Eigenvalues gone wild,,"I added some significant details to this problem, as it was apparently not clear to everyone what I want to know: This is a question about convergence of eigenvalues which essentially came up in studying the spectrum of St.-Liouville operators.  We want to look at matrices that agree in most of their entries and want to investigate whether this implies convergence of the eigenvalues. We start with two matrices $$ A_1:=\left[ \begin {array}{cc}  3.5&- 0.5\\ - 0.5& 0.75 \end {array} \right] $$ with eigenvalues $$\lambda_{1,1} := 0.661912511160047 \quad \lambda_{1,2}:=3.58808748883995  $$ and the matrix $$ B_1:=\left[ \begin {array}{ccc}  3.5&- 0.5&-1/4\,\sqrt {2} \\ - 0.5& 0.25&-1/2\,\sqrt {2}\\ - 1/4\,\sqrt {2}&-1/2\,\sqrt {2}&- 0.5\end {array} \right]  $$ with eigenvalues $$\mu_{1,0}:=-0.9958877876 \quad \mu_{1,1}:= 0.6554756723 \quad  \mu_{1,2}:=3.590412115.$$ We observe that $\lambda_{1,1} \approx \mu_{1,1} $ and $\lambda_{1,2} \approx \mu_{1,2}$. Now, we extend our matrices to larger dimensions, denoting them as $A_{i},B_{i}$ ,in the following way: So we get $A_i$ from $A_1$ by doing the following: (i) we use $A_1$ as the $A_i[n-1:n,n-1:n]$ submatrix of $A_i$. The elements down the diagonal are found from bottom to top by successive iterations in steps of two:  So $A_i(n-2,n-2) = A_i(n-1,n-1) + 5$, $A_i(n-3,n-3) = A_i(n-2,n-2) + 7$ $A_i(n-4,n-4) = A_i(n-3,n-3) + 9$ and so on. (ii)Down the first subdiagonal all entries are $-0.5$ and (iii)down the subsubdiagonal all entries are $-0.25$. All other entries are zero! For the $B_i$ we use the same extension, but use the different basis matrix $$ B_i[n-2:n,n-2:n]:=\left[ \begin {array}{ccc}  3.5&- 0.5&-1/4\,\sqrt {2} \\ - 0.5& 0.25&-1/2\,\sqrt {2}\\ - 1/4\,\sqrt {2}&-1/2\,\sqrt {2}&- 0.5\end {array} \right] . $$ Notice that due to the fact that we use THE SAME iterative scheme to define $A_i$ and $B_i$ we get very similar matrices $A_i,B_i$. So we get for example $$A_4:=\left[ \begin {array}{ccccc}  24.5&- 0.5&- 0.25&0&0 \\ - 0.5& 15.5&- 0.5&- 0.25&0\\ -  0.25&- 0.5& 8.5&- 0.5&- 0.25\\ 0&- 0.25&- 0.5& 3.5& - 0.5\\ 0&0&- 0.25&- 0.5& 0.75\end {array} \right]  $$ with eigenvalues $$\lambda_{4,5 }:= 24.5307920815531 \quad \lambda_{4,4}:= 15.5136493593423 \quad \lambda_{4,3}:= 8.51760322347614 \quad \lambda_{4,2}:=3.54058988050425 \quad\lambda_{4,1}:=0.647365455124154$$ and $$ B_4:= \left[ \begin {array}{cccccc}  24.5&- 0.5&- 0.25&0&0&0 \\- 0.5& 15.5&- 0.5&- 0.25&0&0\\  - 0.25&- 0.5& 8.5&- 0.5&- 0.25&0\\ 0&- 0.25&- 0.5&  3.5&- 0.5&-1/4\,\sqrt {2}\\ 0&0&- 0.25&- 0.5& 0.25& -1/2\,\sqrt {2}\\ 0&0&0&-1/4\,\sqrt {2}&-1/2\,\sqrt  {2}&- 0.5\end {array} \right]  $$ This matrix has the eigenvalues $$\mu_{4,1} = 0.6473654185 \quad \mu_{4,2} =3.540589910 \quad \mu_{4,3} =8.517603211 \quad \mu_{4,4} =15.51364936 \quad \mu_{4,5} =24.53079208,\mu_{4,0}=-0.9999999836$$ Obviously, the eigenvalues of $A_4$ and $B_4$ are extremely close together. Though, $B_4$ has an additional eigenvalue $\mu_{4,0}$ without a partner in the spectrum of $A_4$. So what I want to do is the following: By the iterative definition of these matrices we get sequences $(A_i)_i$ and $(B_i)_i$ with eigenvalue sequences $(\lambda_{i,1})_{i \ge 1}$,$(\lambda_{i,2})_{i \ge 1}$,$(\lambda_{i,3})_{i \ge 2}$,$(\lambda_{i,4})_{i \ge 3}$ and so on and eigenvalues $(\mu_{i,0})_{i \ge 1}$,$(\mu_{i,1})_{i \ge 1}$,$(\mu_{i,2})_{i \ge 1}$,$(\mu_{i,3})_{i \ge 2}$ and $(\mu_{i,4})_{i \ge 3}$.. . I want to show that $\mu_{i,0} \rightarrow -1$ and all the other eigenvalues converge to their partner value, so $\lambda_{i,k} \rightarrow c_k \in \mathbb{R}$(for i approaching infinity, hence going over to larger extended matrices) and accordingly $\mu_{i,k} \rightarrow c_k.$ Numerical simulations actually suggest that this happens ( I calculated up to $A_{150}$ and $B_{150}$ where I reached a pretty good convergence to the values already strongly suggested by $A_4$ and $B_4$, but I am not able to show it.) I will award a 300 points bounty to the person answering this question :-). Hope my problem is clearer now!","I added some significant details to this problem, as it was apparently not clear to everyone what I want to know: This is a question about convergence of eigenvalues which essentially came up in studying the spectrum of St.-Liouville operators.  We want to look at matrices that agree in most of their entries and want to investigate whether this implies convergence of the eigenvalues. We start with two matrices $$ A_1:=\left[ \begin {array}{cc}  3.5&- 0.5\\ - 0.5& 0.75 \end {array} \right] $$ with eigenvalues $$\lambda_{1,1} := 0.661912511160047 \quad \lambda_{1,2}:=3.58808748883995  $$ and the matrix $$ B_1:=\left[ \begin {array}{ccc}  3.5&- 0.5&-1/4\,\sqrt {2} \\ - 0.5& 0.25&-1/2\,\sqrt {2}\\ - 1/4\,\sqrt {2}&-1/2\,\sqrt {2}&- 0.5\end {array} \right]  $$ with eigenvalues $$\mu_{1,0}:=-0.9958877876 \quad \mu_{1,1}:= 0.6554756723 \quad  \mu_{1,2}:=3.590412115.$$ We observe that $\lambda_{1,1} \approx \mu_{1,1} $ and $\lambda_{1,2} \approx \mu_{1,2}$. Now, we extend our matrices to larger dimensions, denoting them as $A_{i},B_{i}$ ,in the following way: So we get $A_i$ from $A_1$ by doing the following: (i) we use $A_1$ as the $A_i[n-1:n,n-1:n]$ submatrix of $A_i$. The elements down the diagonal are found from bottom to top by successive iterations in steps of two:  So $A_i(n-2,n-2) = A_i(n-1,n-1) + 5$, $A_i(n-3,n-3) = A_i(n-2,n-2) + 7$ $A_i(n-4,n-4) = A_i(n-3,n-3) + 9$ and so on. (ii)Down the first subdiagonal all entries are $-0.5$ and (iii)down the subsubdiagonal all entries are $-0.25$. All other entries are zero! For the $B_i$ we use the same extension, but use the different basis matrix $$ B_i[n-2:n,n-2:n]:=\left[ \begin {array}{ccc}  3.5&- 0.5&-1/4\,\sqrt {2} \\ - 0.5& 0.25&-1/2\,\sqrt {2}\\ - 1/4\,\sqrt {2}&-1/2\,\sqrt {2}&- 0.5\end {array} \right] . $$ Notice that due to the fact that we use THE SAME iterative scheme to define $A_i$ and $B_i$ we get very similar matrices $A_i,B_i$. So we get for example $$A_4:=\left[ \begin {array}{ccccc}  24.5&- 0.5&- 0.25&0&0 \\ - 0.5& 15.5&- 0.5&- 0.25&0\\ -  0.25&- 0.5& 8.5&- 0.5&- 0.25\\ 0&- 0.25&- 0.5& 3.5& - 0.5\\ 0&0&- 0.25&- 0.5& 0.75\end {array} \right]  $$ with eigenvalues $$\lambda_{4,5 }:= 24.5307920815531 \quad \lambda_{4,4}:= 15.5136493593423 \quad \lambda_{4,3}:= 8.51760322347614 \quad \lambda_{4,2}:=3.54058988050425 \quad\lambda_{4,1}:=0.647365455124154$$ and $$ B_4:= \left[ \begin {array}{cccccc}  24.5&- 0.5&- 0.25&0&0&0 \\- 0.5& 15.5&- 0.5&- 0.25&0&0\\  - 0.25&- 0.5& 8.5&- 0.5&- 0.25&0\\ 0&- 0.25&- 0.5&  3.5&- 0.5&-1/4\,\sqrt {2}\\ 0&0&- 0.25&- 0.5& 0.25& -1/2\,\sqrt {2}\\ 0&0&0&-1/4\,\sqrt {2}&-1/2\,\sqrt  {2}&- 0.5\end {array} \right]  $$ This matrix has the eigenvalues $$\mu_{4,1} = 0.6473654185 \quad \mu_{4,2} =3.540589910 \quad \mu_{4,3} =8.517603211 \quad \mu_{4,4} =15.51364936 \quad \mu_{4,5} =24.53079208,\mu_{4,0}=-0.9999999836$$ Obviously, the eigenvalues of $A_4$ and $B_4$ are extremely close together. Though, $B_4$ has an additional eigenvalue $\mu_{4,0}$ without a partner in the spectrum of $A_4$. So what I want to do is the following: By the iterative definition of these matrices we get sequences $(A_i)_i$ and $(B_i)_i$ with eigenvalue sequences $(\lambda_{i,1})_{i \ge 1}$,$(\lambda_{i,2})_{i \ge 1}$,$(\lambda_{i,3})_{i \ge 2}$,$(\lambda_{i,4})_{i \ge 3}$ and so on and eigenvalues $(\mu_{i,0})_{i \ge 1}$,$(\mu_{i,1})_{i \ge 1}$,$(\mu_{i,2})_{i \ge 1}$,$(\mu_{i,3})_{i \ge 2}$ and $(\mu_{i,4})_{i \ge 3}$.. . I want to show that $\mu_{i,0} \rightarrow -1$ and all the other eigenvalues converge to their partner value, so $\lambda_{i,k} \rightarrow c_k \in \mathbb{R}$(for i approaching infinity, hence going over to larger extended matrices) and accordingly $\mu_{i,k} \rightarrow c_k.$ Numerical simulations actually suggest that this happens ( I calculated up to $A_{150}$ and $B_{150}$ where I reached a pretty good convergence to the values already strongly suggested by $A_4$ and $B_4$, but I am not able to show it.) I will award a 300 points bounty to the person answering this question :-). Hope my problem is clearer now!",,"['real-analysis', 'linear-algebra']"
1,"Every $x \in (0,1]$ can be represented as $x = \sum_{k=1}^{\infty} 1/{n_k}$, such that $n_{k+1}/n_k\in \{2,3,4\}$","Every  can be represented as , such that","x \in (0,1] x = \sum_{k=1}^{\infty} 1/{n_k} n_{k+1}/n_k\in \{2,3,4\}","Show that every $x \in (0,1]$ can be represented as $x = \sum_{k=1}^{\infty} 1/{n_k}$, where $(n_k)$ is a sequence of positive integers such that $n_{k+1}/n_k\in \{2,3,4\}$. Please do NOT reveal the full solution, I'm looking for hints. So, the first strategy that came to my mind was this: I must start with a representation of $x$ in a base-$n$ expansion and then I show that every term $a_k = x_k/n^k$ where $x \in \{0,1,\cdots,n-1\}$ can be written as a finite sum of $1/{n_k}$'s with the given condition, i.e. $n_{k+1}/n_k \in \{2,3,4\}$. It doesn't sound like a good strategy because even if I replace every $a_k = x_k/n^k$ with a finite sum of the form $a_k=\sum_{i=1}^{m_k}1/n_{i,k}$ and write the number as $$\sum_{k=1}^{\infty}(\sum_{i=1}^{m_k}1/{n_{i,k}})$$ with the condition that for each fixed $k: {n_{i+1,k}/n_{i,k}} \in {2,3,4}$ there is no guarantee that the same holds when we jump from the last term in the sum of $a_k$ to the first term of the sum of $a_{k+1}$. Please give me some ideas about how one could tackle a problem like this and what are some good strategies to try. Thanks in advance EDIT: A new strategy that just came to my mind: Start like this: Take $x \in (0,1]$, then by Archemedean property of the reals, we can find $n_1$ such that $1/n_1 < x$. Now choose $n_2$ such that $n_2/n_1 = \min\{2,3,4\}$ provided that $1/n_1 + 1/n_2 < x$. If such a $n_2$ didn't exist, then take $n_1$ smaller enough that $n_2$ exists.. Keep going on this way and you'll have an increasing sequence of positive terms that is bounded above by $x$.. Therefore the sequence must be convergent.. So it has a supremum and now we must somehow control the supremum to be $x$ and it can be done by decreasing the difference between the sum and $x$. Is this idea good?","Show that every $x \in (0,1]$ can be represented as $x = \sum_{k=1}^{\infty} 1/{n_k}$, where $(n_k)$ is a sequence of positive integers such that $n_{k+1}/n_k\in \{2,3,4\}$. Please do NOT reveal the full solution, I'm looking for hints. So, the first strategy that came to my mind was this: I must start with a representation of $x$ in a base-$n$ expansion and then I show that every term $a_k = x_k/n^k$ where $x \in \{0,1,\cdots,n-1\}$ can be written as a finite sum of $1/{n_k}$'s with the given condition, i.e. $n_{k+1}/n_k \in \{2,3,4\}$. It doesn't sound like a good strategy because even if I replace every $a_k = x_k/n^k$ with a finite sum of the form $a_k=\sum_{i=1}^{m_k}1/n_{i,k}$ and write the number as $$\sum_{k=1}^{\infty}(\sum_{i=1}^{m_k}1/{n_{i,k}})$$ with the condition that for each fixed $k: {n_{i+1,k}/n_{i,k}} \in {2,3,4}$ there is no guarantee that the same holds when we jump from the last term in the sum of $a_k$ to the first term of the sum of $a_{k+1}$. Please give me some ideas about how one could tackle a problem like this and what are some good strategies to try. Thanks in advance EDIT: A new strategy that just came to my mind: Start like this: Take $x \in (0,1]$, then by Archemedean property of the reals, we can find $n_1$ such that $1/n_1 < x$. Now choose $n_2$ such that $n_2/n_1 = \min\{2,3,4\}$ provided that $1/n_1 + 1/n_2 < x$. If such a $n_2$ didn't exist, then take $n_1$ smaller enough that $n_2$ exists.. Keep going on this way and you'll have an increasing sequence of positive terms that is bounded above by $x$.. Therefore the sequence must be convergent.. So it has a supremum and now we must somehow control the supremum to be $x$ and it can be done by decreasing the difference between the sum and $x$. Is this idea good?",,"['real-analysis', 'analysis', 'problem-solving']"
2,Sum of squares at integer points for $L^2$ function,Sum of squares at integer points for  function,L^2,"Let $f\in L^2(\mathbb{R})$ be a continuous function such that $f(x)\rightarrow 0$ as $x\rightarrow\pm\infty$. Is it true that $\sum_{n=1}^\infty |f(n)|^2$ is finite? If the continuity and going to zero conditions are dropped, the statement is not true, because $f(n)$ could have very high values only at the integer points.","Let $f\in L^2(\mathbb{R})$ be a continuous function such that $f(x)\rightarrow 0$ as $x\rightarrow\pm\infty$. Is it true that $\sum_{n=1}^\infty |f(n)|^2$ is finite? If the continuity and going to zero conditions are dropped, the statement is not true, because $f(n)$ could have very high values only at the integer points.",,"['real-analysis', 'sequences-and-series', 'continuity']"
3,How can we prove this function must be linear?,How can we prove this function must be linear?,,"Let $f:[a,b]\to\mathbb{R}$ be continuous. Suppose for any sequence $(r_n)_{n=0}^{\infty}$ with $\lim_{n\to\infty}r_n=0$, and any $x\in(a,b)$: $$\lim_{n\to\infty}\frac{f(x-r_n)+f(x+r_n)-2f(x)}{r_n^2}=0$$ Show that $f$ is a linear function. Clearly the limit of the numerator is zero, I want to conclude that the numerator must be zero  for all $n$, then from this to conclude $f$ is linear. I am not sure if this is the right approach. Are there any hints?","Let $f:[a,b]\to\mathbb{R}$ be continuous. Suppose for any sequence $(r_n)_{n=0}^{\infty}$ with $\lim_{n\to\infty}r_n=0$, and any $x\in(a,b)$: $$\lim_{n\to\infty}\frac{f(x-r_n)+f(x+r_n)-2f(x)}{r_n^2}=0$$ Show that $f$ is a linear function. Clearly the limit of the numerator is zero, I want to conclude that the numerator must be zero  for all $n$, then from this to conclude $f$ is linear. I am not sure if this is the right approach. Are there any hints?",,"['calculus', 'real-analysis', 'functions']"
4,How does the fundamental theorem of algebra follow from Weierstrass’s theorem.,How does the fundamental theorem of algebra follow from Weierstrass’s theorem.,,Can anyone please explain to me how the fundamental theorem of algebra follows from or is related to the Bolzano Weierstrass’s theorem?,Can anyone please explain to me how the fundamental theorem of algebra follows from or is related to the Bolzano Weierstrass’s theorem?,,"['real-analysis', 'abstract-algebra', 'general-topology', 'soft-question']"
5,Series convergence/divergence,Series convergence/divergence,,"I was trying to prove the following question. Part a is intuitive but couldn't give a clear mathematical argument. For parts b and c It seems there is something I am not seeing. Any help ? If $\sum_{k=1}^{\infty }a_{k}$ diverges, $a_{k}\geq 0$, and $% A_{n}=a_{1}+...+a_{n}$, then , (a) $\displaystyle \sum_{n=1}^{\infty }\frac{a_{n}}{1+a_{n}} $ diverges, (b) $\displaystyle \sum_{k=2}^{\infty }\frac{a_{k}}{A_{k}A_{k-1}}$ diverges, but (c) $\displaystyle \sum_{k=2}^{\infty }\frac{a_{k}}{A_{k}^{\alpha }}$ converges for each $\alpha >2$.","I was trying to prove the following question. Part a is intuitive but couldn't give a clear mathematical argument. For parts b and c It seems there is something I am not seeing. Any help ? If $\sum_{k=1}^{\infty }a_{k}$ diverges, $a_{k}\geq 0$, and $% A_{n}=a_{1}+...+a_{n}$, then , (a) $\displaystyle \sum_{n=1}^{\infty }\frac{a_{n}}{1+a_{n}} $ diverges, (b) $\displaystyle \sum_{k=2}^{\infty }\frac{a_{k}}{A_{k}A_{k-1}}$ diverges, but (c) $\displaystyle \sum_{k=2}^{\infty }\frac{a_{k}}{A_{k}^{\alpha }}$ converges for each $\alpha >2$.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'divergent-series']"
6,Dominated convergence and $\sigma$-finiteness,Dominated convergence and -finiteness,\sigma,"I am curious about the Dominated Convergence Theorem for a sequence of functions that converges in measure. Theorem: Let $(X,\mathcal{S},\mu)$ be a measure space. If $\{f_n\}, f$ are measurable, real-valued (i.e. finite) and such that $f_n \to f$ in measure and $|f_n| \leq g$ with $\int g \,\mathrm{d}\mu < \infty$, then $$ \int f_n \,\mathrm{d}\mu \to \int f \,\mathrm{d}\mu $$ Proof: Since $f_n \to f$ in measure, then for any subsequence of $\{f_n\}$, call it $\{f_k\}$, we have also $f_k \to f$ in measure. Now we can extract a further subsequence $\{f_{k_j}\}$ such that $f_{k_j} \to f$ almost everywhere. Applying the a.e. version of dominated convergence to this subsequence gives: $$ \int f_{k_j} \,\mathrm{d}\mu \to \int f \,\mathrm{d}\mu $$ Now defining the sequence of real numbers $\{a_n\}$ by $a_n = \int f_n \,\mathrm{d}\mu$, we want to show that $$a_n \to \int f \,\mathrm{d}\mu$$ But we have just shown that for any subsequence $\{a_k\}$ of $\{a_n\}$, there exists a further subsequence $\{a_{k_j}\}$ that converges to $\int f \,\mathrm{d}\mu$. Thus $a_n$ converges to $\int f \,\mathrm{d}\mu$ as well. QED. Question: So, nowhere in this proof did I use the fact that $\mu$ is $\sigma$-finite. However, everywhere that I look, I keep seeing this result with the condition that $\mu$ be $\sigma$-finite (e.g: Generalisation of Dominated Convergence Theorem ). So I must be doing something wrong? The only place I can think of where $\sigma$-finiteness might be required is in extracting an a.e. convergent subsequence from the ""in measure"" convergent sequence $\{f_k\}$. But I am pretty sure that $\sigma$-finiteness is not required to extract an almost uniformly convergent subsequence from an ""in measure"" convergent subsequence. And since almost uniformly convergent subsequences are also almost everywhere convergent, then I'm stumped. Any pointers?","I am curious about the Dominated Convergence Theorem for a sequence of functions that converges in measure. Theorem: Let $(X,\mathcal{S},\mu)$ be a measure space. If $\{f_n\}, f$ are measurable, real-valued (i.e. finite) and such that $f_n \to f$ in measure and $|f_n| \leq g$ with $\int g \,\mathrm{d}\mu < \infty$, then $$ \int f_n \,\mathrm{d}\mu \to \int f \,\mathrm{d}\mu $$ Proof: Since $f_n \to f$ in measure, then for any subsequence of $\{f_n\}$, call it $\{f_k\}$, we have also $f_k \to f$ in measure. Now we can extract a further subsequence $\{f_{k_j}\}$ such that $f_{k_j} \to f$ almost everywhere. Applying the a.e. version of dominated convergence to this subsequence gives: $$ \int f_{k_j} \,\mathrm{d}\mu \to \int f \,\mathrm{d}\mu $$ Now defining the sequence of real numbers $\{a_n\}$ by $a_n = \int f_n \,\mathrm{d}\mu$, we want to show that $$a_n \to \int f \,\mathrm{d}\mu$$ But we have just shown that for any subsequence $\{a_k\}$ of $\{a_n\}$, there exists a further subsequence $\{a_{k_j}\}$ that converges to $\int f \,\mathrm{d}\mu$. Thus $a_n$ converges to $\int f \,\mathrm{d}\mu$ as well. QED. Question: So, nowhere in this proof did I use the fact that $\mu$ is $\sigma$-finite. However, everywhere that I look, I keep seeing this result with the condition that $\mu$ be $\sigma$-finite (e.g: Generalisation of Dominated Convergence Theorem ). So I must be doing something wrong? The only place I can think of where $\sigma$-finiteness might be required is in extracting an a.e. convergent subsequence from the ""in measure"" convergent sequence $\{f_k\}$. But I am pretty sure that $\sigma$-finiteness is not required to extract an almost uniformly convergent subsequence from an ""in measure"" convergent subsequence. And since almost uniformly convergent subsequences are also almost everywhere convergent, then I'm stumped. Any pointers?",,"['real-analysis', 'probability', 'probability-theory', 'convergence-divergence']"
7,Lebesgue measure as a fixpoint: change of variables formulas,Lebesgue measure as a fixpoint: change of variables formulas,,"This question is inspired by several others on a similar topic: see e.g. this one and a sequence of linked questions. Let us so far focus on $\Bbb R^n$ endowed with standard Borel structure. For any map $\phi\in \mathcal C^1(\Bbb R^n,\Bbb R^n)$ we have the following formula for the change of variables in the integral $$   \int_{\phi(\Omega)}f\;\mathrm d\lambda = \int_\Omega (f\circ\phi)\;|\phi'|\mathrm d\lambda \tag{1} $$ where $\Omega$ is any Borel set, $f$ is any Borel and bounded, $\phi'$ is the Jacobian of $\phi$ and $\lambda$ is the $n$-dimensional Lebesgue measure on $\Bbb R^n$. On the other hand, for any Borel measure $\mu$ the following formula applies $$   \int_\Omega (f\circ\phi)\;\mathrm d\mu = \int_{\phi(\Omega)}f\;\mathrm d(\phi_*\mu) \tag{2}. $$ where $(\phi_*\mu)(A) = \mu(\phi^{-1}(A))$ is the pushforward measure. As a result, provided the fact that $   \frac{\mathrm d\mu}{\mathrm d\lambda} = |\phi'| $ we obtain for any admissible $\Omega$ and $f$ that $$   \int_{\phi(\Omega)}f\;\mathrm d\lambda = \int_{\phi(\Omega)}f\;\mathrm d(\phi_*\mu) $$ which implies that $\lambda|_{\phi(\Omega)} = (\phi_*\mu)|_{\phi(\Omega)}$. Moreover, if $\phi(\Omega) = \Bbb R^n$ then $\lambda = \phi_*\mu$. Let $\mathcal P(\Bbb R^n)$ be the set of all Borel measures on $\Bbb R^n$, and for any $\phi\in \mathcal C^1(\Bbb R^n,\Bbb R^n)$ with the range $\Bbb R^n$ let us define an operator $\phi'$ on $\mathcal P(\Bbb R^n)$ given by $\mathrm d\phi'(\mu) := |\phi'|\mathrm d\mu.$ As a result, from the discussion above we obtain that $\lambda$ solves the equation $$   \phi_*(\phi'(\mu)) = \mu \tag{3}. $$ for any such $\phi$. Clearly, for some $\phi$ there may be multiple solution: e.g. if $\phi =\mathrm{id}_{\Bbb R^n}$ then $\phi_*\circ \phi' = \mathrm{id}_{\mathcal P(\Bbb R^n)}$ so that any $\mu$ satisfies $(3)$ in this case. Q1: Is that true, that $\lambda$ is the only positive measure (up to scaling) that satisfies $(3)$ for all $\phi$ that range over $\Bbb R^n$? Q2: If such a measure is not unique, what similar properties do they have? Perhaps, it has to be equivalent to the Lebesgue measure. Q3: Is there a ""characteristic"" map $\hat\phi$ such that if $\mu$ satisfies $(3)$ for $\hat\phi$, then it satisfies it for all $\phi$? Q4: Is there any intuitive reason, why the Lebesgue measure satisfies this equation? P.S. Feel free to fix my notation (especially $\phi'$ as an operator), or comment for which $\Omega$, $f$ and $\phi$ formulas above are well-defined - I didn't have a chance to study geometric measure theory. I hope that the current formulation may shed some light on the similarities between two different formulas for the change of coordinates, which are not clear to me at the moment. Also, I know that similar formulas for the change of coordinates are available for smooth manifolds endowed e.g. with Hausdorff measures, so perhaps this feature applies not only to the Lebesgue measure, but to $n$-dimensional Hausdorff measures as well. Another consequence of $(1)$ and $(2)$ is that for any $\mathcal C^1$-bijection $\phi$ it holds that $$   \frac{\mathrm d(\phi_*\lambda)}{\mathrm d\lambda} = |(\phi^{-1})'|\circ\phi^{-1}. $$","This question is inspired by several others on a similar topic: see e.g. this one and a sequence of linked questions. Let us so far focus on $\Bbb R^n$ endowed with standard Borel structure. For any map $\phi\in \mathcal C^1(\Bbb R^n,\Bbb R^n)$ we have the following formula for the change of variables in the integral $$   \int_{\phi(\Omega)}f\;\mathrm d\lambda = \int_\Omega (f\circ\phi)\;|\phi'|\mathrm d\lambda \tag{1} $$ where $\Omega$ is any Borel set, $f$ is any Borel and bounded, $\phi'$ is the Jacobian of $\phi$ and $\lambda$ is the $n$-dimensional Lebesgue measure on $\Bbb R^n$. On the other hand, for any Borel measure $\mu$ the following formula applies $$   \int_\Omega (f\circ\phi)\;\mathrm d\mu = \int_{\phi(\Omega)}f\;\mathrm d(\phi_*\mu) \tag{2}. $$ where $(\phi_*\mu)(A) = \mu(\phi^{-1}(A))$ is the pushforward measure. As a result, provided the fact that $   \frac{\mathrm d\mu}{\mathrm d\lambda} = |\phi'| $ we obtain for any admissible $\Omega$ and $f$ that $$   \int_{\phi(\Omega)}f\;\mathrm d\lambda = \int_{\phi(\Omega)}f\;\mathrm d(\phi_*\mu) $$ which implies that $\lambda|_{\phi(\Omega)} = (\phi_*\mu)|_{\phi(\Omega)}$. Moreover, if $\phi(\Omega) = \Bbb R^n$ then $\lambda = \phi_*\mu$. Let $\mathcal P(\Bbb R^n)$ be the set of all Borel measures on $\Bbb R^n$, and for any $\phi\in \mathcal C^1(\Bbb R^n,\Bbb R^n)$ with the range $\Bbb R^n$ let us define an operator $\phi'$ on $\mathcal P(\Bbb R^n)$ given by $\mathrm d\phi'(\mu) := |\phi'|\mathrm d\mu.$ As a result, from the discussion above we obtain that $\lambda$ solves the equation $$   \phi_*(\phi'(\mu)) = \mu \tag{3}. $$ for any such $\phi$. Clearly, for some $\phi$ there may be multiple solution: e.g. if $\phi =\mathrm{id}_{\Bbb R^n}$ then $\phi_*\circ \phi' = \mathrm{id}_{\mathcal P(\Bbb R^n)}$ so that any $\mu$ satisfies $(3)$ in this case. Q1: Is that true, that $\lambda$ is the only positive measure (up to scaling) that satisfies $(3)$ for all $\phi$ that range over $\Bbb R^n$? Q2: If such a measure is not unique, what similar properties do they have? Perhaps, it has to be equivalent to the Lebesgue measure. Q3: Is there a ""characteristic"" map $\hat\phi$ such that if $\mu$ satisfies $(3)$ for $\hat\phi$, then it satisfies it for all $\phi$? Q4: Is there any intuitive reason, why the Lebesgue measure satisfies this equation? P.S. Feel free to fix my notation (especially $\phi'$ as an operator), or comment for which $\Omega$, $f$ and $\phi$ formulas above are well-defined - I didn't have a chance to study geometric measure theory. I hope that the current formulation may shed some light on the similarities between two different formulas for the change of coordinates, which are not clear to me at the moment. Also, I know that similar formulas for the change of coordinates are available for smooth manifolds endowed e.g. with Hausdorff measures, so perhaps this feature applies not only to the Lebesgue measure, but to $n$-dimensional Hausdorff measures as well. Another consequence of $(1)$ and $(2)$ is that for any $\mathcal C^1$-bijection $\phi$ it holds that $$   \frac{\mathrm d(\phi_*\lambda)}{\mathrm d\lambda} = |(\phi^{-1})'|\circ\phi^{-1}. $$",,"['real-analysis', 'measure-theory', 'geometric-measure-theory']"
8,$\frac{dS}{d\rho}$ Factor arising,Factor arising,\frac{dS}{d\rho},"To get details see: equations 29,30,31,34,44,50,51 We have known some solitary wave solutions, given by(equations 1 to 5)  $$ \phi_1=p_1\cos \tau \tag{1}$$ $$\phi_2=\frac16 g_2p_1^2\left(\cos(2\tau)-3\right)\tag{2}$$ $$\phi_3=p_3\cos \tau+\frac{1}{72}(4g_2^2-3\lambda)p_1^3\cos(3\tau)\tag{3}$$ $$\phi_4= \frac{1}{360}p_1^4\left(3g_4-5g_2\lambda+5g_2^3\right)\cos(4\tau) -\frac{1}{72}\left(8g_2(\nabla p_1)^2-12g_4p_1^4+16g_2^3p_1^4 -24g_2p_1p_3-23g_2\lambda p_1^4-8g_2p_1^2\right)\cos(2\tau) -g_2p_1^2-g_2p_1p_3+\frac{1}{6}g_2\lambda p_1^4-g_2(\nabla p_1)^2 +\frac{31}{72}g_2^3p_1^4-\frac{3}{8}g_4p_1^4  \tag{4}$$ \begin{equation} p_5=\frac{\sqrt 2}{9\sqrt 3}\left( Y-\frac{1235}{32}S^2Z+\frac{1503}{16}Z-24S-\frac{17}{3}S^3 +\frac{11525}{384}S^5\right)  \end{equation} Now we will set $\tau=0$ in these above equation. Now consider some conditions: $$S=p_1\sqrt{\lambda}$$ \begin{equation} p_3=\frac{\sqrt{2}}{3\sqrt{3}}\left( \frac{65}{8}Z-\frac{8}{3}S-\frac{19}{12}S^3 \right)\,. \end{equation} 3.value of the  some constants $g_2=-\frac32$, $g_3=\frac12$ and $g_i=0$ for $i\geq 4$ ,$ \lambda= \frac{3}{2}$ and equations of spherical symmetry \begin{equation} \frac{d^2S}{d\rho^2}+\frac{D-1}{\rho}\,\frac{dS}{d\rho} -S+S^3=0 \end{equation} Now I need write the equations (4)and (5) as  $$\phi_4^{(\tau=0)}=\frac{1}{9}\left[ \frac{65}{4}SZ+10\left(\frac{dS}{d\rho}\right)^2 +\frac{8}{3}S^2-\frac{125}{12}S^4 \right]$$ $$\phi_5^{(\tau=0)}=\frac{1}{9}\sqrt{\frac{2}{3}}\Biggl[ Y-\frac{2275}{64}S^2Z+\frac{1503}{16}Z -\frac{15}{32}S\left(\frac{dS}{d\rho}\right)^2 -24S-\frac{595}{96}S^3+\frac{11285}{384}S^5 \biggr]$$ My Problem is how the factor $\frac{dS}{d\rho}$ arises in  the above two equations? For further details: equations 29,30,31,34,44,50,51 If you have problem to understand the questions then ask me please. Thanks in advance","To get details see: equations 29,30,31,34,44,50,51 We have known some solitary wave solutions, given by(equations 1 to 5)  $$ \phi_1=p_1\cos \tau \tag{1}$$ $$\phi_2=\frac16 g_2p_1^2\left(\cos(2\tau)-3\right)\tag{2}$$ $$\phi_3=p_3\cos \tau+\frac{1}{72}(4g_2^2-3\lambda)p_1^3\cos(3\tau)\tag{3}$$ $$\phi_4= \frac{1}{360}p_1^4\left(3g_4-5g_2\lambda+5g_2^3\right)\cos(4\tau) -\frac{1}{72}\left(8g_2(\nabla p_1)^2-12g_4p_1^4+16g_2^3p_1^4 -24g_2p_1p_3-23g_2\lambda p_1^4-8g_2p_1^2\right)\cos(2\tau) -g_2p_1^2-g_2p_1p_3+\frac{1}{6}g_2\lambda p_1^4-g_2(\nabla p_1)^2 +\frac{31}{72}g_2^3p_1^4-\frac{3}{8}g_4p_1^4  \tag{4}$$ \begin{equation} p_5=\frac{\sqrt 2}{9\sqrt 3}\left( Y-\frac{1235}{32}S^2Z+\frac{1503}{16}Z-24S-\frac{17}{3}S^3 +\frac{11525}{384}S^5\right)  \end{equation} Now we will set $\tau=0$ in these above equation. Now consider some conditions: $$S=p_1\sqrt{\lambda}$$ \begin{equation} p_3=\frac{\sqrt{2}}{3\sqrt{3}}\left( \frac{65}{8}Z-\frac{8}{3}S-\frac{19}{12}S^3 \right)\,. \end{equation} 3.value of the  some constants $g_2=-\frac32$, $g_3=\frac12$ and $g_i=0$ for $i\geq 4$ ,$ \lambda= \frac{3}{2}$ and equations of spherical symmetry \begin{equation} \frac{d^2S}{d\rho^2}+\frac{D-1}{\rho}\,\frac{dS}{d\rho} -S+S^3=0 \end{equation} Now I need write the equations (4)and (5) as  $$\phi_4^{(\tau=0)}=\frac{1}{9}\left[ \frac{65}{4}SZ+10\left(\frac{dS}{d\rho}\right)^2 +\frac{8}{3}S^2-\frac{125}{12}S^4 \right]$$ $$\phi_5^{(\tau=0)}=\frac{1}{9}\sqrt{\frac{2}{3}}\Biggl[ Y-\frac{2275}{64}S^2Z+\frac{1503}{16}Z -\frac{15}{32}S\left(\frac{dS}{d\rho}\right)^2 -24S-\frac{595}{96}S^3+\frac{11285}{384}S^5 \biggr]$$ My Problem is how the factor $\frac{dS}{d\rho}$ arises in  the above two equations? For further details: equations 29,30,31,34,44,50,51 If you have problem to understand the questions then ask me please. Thanks in advance",,"['real-analysis', 'linear-algebra']"
9,Egoroff's theorem analogous for infinite measures,Egoroff's theorem analogous for infinite measures,,"Instead of $\mu (X)<\infty$ suppose that $|f_n|\leqslant g, \forall  n\in\mathbb N,$ and $g\in L^1(μ) $. $$$$ If $f_n\longrightarrow f $    a.e. in X , then prove that:$$\forall \epsilon>0, \exists E\subset  X,s.t. \mu (E)<\epsilon$$ and $$f_n\longrightarrow f $$uniformly on   $E^c$. A hint which accompanies this exercise and I want to use is : Use the sets $\left\{ g>1\right\} $ and $\left\{ 2^{-k}<g\leqslant2^{1-k}\right\}$ which have finite measures due to the integrability of g. I am thinking of applying Egoroff's theorem to each of these sets. We can write: $$X=\{ g>1\}\cup(\bigcup\limits_{k=1}^{\infty}\left\{ 2^{-k}<g\leqslant2^{1-k}\right\})\cup \{g=0\}$$ Then there exist $A\subset G=\{g>1\}$ ,and  $A_k\subset G_k=\left\{ 2^{-k}<g\leqslant2^{1-k}\right\}, $ such that: $$\mu (A)<\varepsilon ,   \mu (A_k)<\varepsilon  $$ and $$f_n\longrightarrow f$$uniformly in $E^c=(G\setminus A)\cup(\bigcup\limits_{k=1}^{\infty}G_k\setminus A_k)\cup \{f_n=0,\forall n\}$, where the last set in the union is a superset of $\{g=0\}$. However, I fail to prove that $\mu (E)<\varepsilon$. Maybe it is a simple calculation, or a better choice of sufficiently small $\varepsilon$'s . I would appreciate your help.","Instead of $\mu (X)<\infty$ suppose that $|f_n|\leqslant g, \forall  n\in\mathbb N,$ and $g\in L^1(μ) $. $$$$ If $f_n\longrightarrow f $    a.e. in X , then prove that:$$\forall \epsilon>0, \exists E\subset  X,s.t. \mu (E)<\epsilon$$ and $$f_n\longrightarrow f $$uniformly on   $E^c$. A hint which accompanies this exercise and I want to use is : Use the sets $\left\{ g>1\right\} $ and $\left\{ 2^{-k}<g\leqslant2^{1-k}\right\}$ which have finite measures due to the integrability of g. I am thinking of applying Egoroff's theorem to each of these sets. We can write: $$X=\{ g>1\}\cup(\bigcup\limits_{k=1}^{\infty}\left\{ 2^{-k}<g\leqslant2^{1-k}\right\})\cup \{g=0\}$$ Then there exist $A\subset G=\{g>1\}$ ,and  $A_k\subset G_k=\left\{ 2^{-k}<g\leqslant2^{1-k}\right\}, $ such that: $$\mu (A)<\varepsilon ,   \mu (A_k)<\varepsilon  $$ and $$f_n\longrightarrow f$$uniformly in $E^c=(G\setminus A)\cup(\bigcup\limits_{k=1}^{\infty}G_k\setminus A_k)\cup \{f_n=0,\forall n\}$, where the last set in the union is a superset of $\{g=0\}$. However, I fail to prove that $\mu (E)<\varepsilon$. Maybe it is a simple calculation, or a better choice of sufficiently small $\varepsilon$'s . I would appreciate your help.",,"['real-analysis', 'measure-theory', 'uniform-convergence']"
10,"$f$ is measurable and $g$ is monotonic continuous, is $f \circ g$ Lebesgue measurable?","is measurable and  is monotonic continuous, is  Lebesgue measurable?",f g f \circ g,Let $f$ be a measurable function on real numbers and $g$ is a monotonic continuous function on real numbers. Is the function composition $f \circ g$ Lebesgue measurable? Thanks.,Let $f$ be a measurable function on real numbers and $g$ is a monotonic continuous function on real numbers. Is the function composition $f \circ g$ Lebesgue measurable? Thanks.,,"['real-analysis', 'measure-theory']"
11,Interpretation of the Laplace transform,Interpretation of the Laplace transform,,"Here's my intuitive understanding of the Fourier transform of $f:{\mathbb R}\rightarrow{\mathbb C}$, defined by $$\mathcal{F}(f)(\omega) = \int_{-\infty}^{\infty}e^{-2 \pi i \, \omega \,x}f(x)dx$$ I visualize the complex unit factor $e^{-2 \pi i \, \omega \,x}$ as a rotating probe that sweeps the unit circle with frequency $\omega$, as $x$ sweeps from $-\infty$ to $\infty$, and thereby ""picks out"" those features of the curve $f(x) \subset {\mathbb C}$ that contribute to the component of its spectrum at frequency $\omega$.  (The integral sums all these contributions to produce the $\omega$ component of $f\;$'s spectrum.) Is it possible to give a similar ""intuitive interpretation"" of what the Laplace transform $$\mathcal{L}(f)(s) = \int_{0}^\infty e^{-sx}f(x)dx, \;\;\; s\in {\mathbb C}$$ is doing?","Here's my intuitive understanding of the Fourier transform of $f:{\mathbb R}\rightarrow{\mathbb C}$, defined by $$\mathcal{F}(f)(\omega) = \int_{-\infty}^{\infty}e^{-2 \pi i \, \omega \,x}f(x)dx$$ I visualize the complex unit factor $e^{-2 \pi i \, \omega \,x}$ as a rotating probe that sweeps the unit circle with frequency $\omega$, as $x$ sweeps from $-\infty$ to $\infty$, and thereby ""picks out"" those features of the curve $f(x) \subset {\mathbb C}$ that contribute to the component of its spectrum at frequency $\omega$.  (The integral sums all these contributions to produce the $\omega$ component of $f\;$'s spectrum.) Is it possible to give a similar ""intuitive interpretation"" of what the Laplace transform $$\mathcal{L}(f)(s) = \int_{0}^\infty e^{-sx}f(x)dx, \;\;\; s\in {\mathbb C}$$ is doing?",,"['real-analysis', 'complex-analysis', 'analysis', 'fourier-analysis', 'laplace-transform']"
12,Bolzano-Weierstrass and measures,Bolzano-Weierstrass and measures,,"Let $\{\mathcal{A}_n\}$ be an infinite sequence of sets with $\mathcal{A}_n \subset \mathcal{M}$, where $\mathcal{M}$ is a bounded subset of $\mathbb{R}$ (for simplicity). Is there a ""nice"" limit definition $\lim^{\circ}$ and a ""nice"" measure $\mu^{\circ}$ such that the following two holds: 1) Every $\{\mathcal{A}_n\}$ (as discussed above) has a converging subsequence. 2) If $\{\mathcal{A}_n\}$ converges, and $\mathcal{A} = \lim^{\circ}\mathcal{A}_n$, then $\mu^{\circ}(\mathcal{A}) = \lim_{n\rightarrow\infty}\mu^{\circ}(\mathcal{A}_n)$ (here the last limit is the usual limit). What I mean by ""nice:"" For the measure, it should satisfy e.g. $\mu^{\circ}([a,b]) = b-a$ (e.g. like the Lebesgue measure). For the limit, I do not know exactly what I want, but it should not be something utterly useless and trivial. Let me try to be more clear. For example, set $\mu^{\circ}$ to be the Lebesgue measure, and the limits defined in the standard manner, i.e. let  \begin{align} \liminf_{n\rightarrow\infty} \mathcal{A}_n = \{x:x\in\mathcal{A}_n\mbox{ for all but finitely many }n\}, \end{align} \begin{align} \limsup_{n\rightarrow\infty} \mathcal{A}_n = \{x:x\in\mathcal{A}_n\mbox{ for infinitely many }n\}, \end{align} and say that the limit $\lim\mathcal{A}_n$ exists is these values agree.  With this setup, the second condition is satisfied, but the first is not (see my earlier question today: Bolzano-Weierstrass for sequences of sets for which I got great responses thanks to many people) For the sake of experimenting, let $\mu^{\circ}$ be the Lebesgue measure again, but use the Kuratowski convergence for the limits (see e.g. the Wikipedia page http://en.wikipedia.org/wiki/Kuratowski_convergence ). Then, the first condition is satisfied (I read the proof somewhere, but now forgot where), but the second condition is not (it is easy to construct a counterexample). This is just a thought experiment that has been bothering me for a while, and I would greatly appreciate any responses.","Let $\{\mathcal{A}_n\}$ be an infinite sequence of sets with $\mathcal{A}_n \subset \mathcal{M}$, where $\mathcal{M}$ is a bounded subset of $\mathbb{R}$ (for simplicity). Is there a ""nice"" limit definition $\lim^{\circ}$ and a ""nice"" measure $\mu^{\circ}$ such that the following two holds: 1) Every $\{\mathcal{A}_n\}$ (as discussed above) has a converging subsequence. 2) If $\{\mathcal{A}_n\}$ converges, and $\mathcal{A} = \lim^{\circ}\mathcal{A}_n$, then $\mu^{\circ}(\mathcal{A}) = \lim_{n\rightarrow\infty}\mu^{\circ}(\mathcal{A}_n)$ (here the last limit is the usual limit). What I mean by ""nice:"" For the measure, it should satisfy e.g. $\mu^{\circ}([a,b]) = b-a$ (e.g. like the Lebesgue measure). For the limit, I do not know exactly what I want, but it should not be something utterly useless and trivial. Let me try to be more clear. For example, set $\mu^{\circ}$ to be the Lebesgue measure, and the limits defined in the standard manner, i.e. let  \begin{align} \liminf_{n\rightarrow\infty} \mathcal{A}_n = \{x:x\in\mathcal{A}_n\mbox{ for all but finitely many }n\}, \end{align} \begin{align} \limsup_{n\rightarrow\infty} \mathcal{A}_n = \{x:x\in\mathcal{A}_n\mbox{ for infinitely many }n\}, \end{align} and say that the limit $\lim\mathcal{A}_n$ exists is these values agree.  With this setup, the second condition is satisfied, but the first is not (see my earlier question today: Bolzano-Weierstrass for sequences of sets for which I got great responses thanks to many people) For the sake of experimenting, let $\mu^{\circ}$ be the Lebesgue measure again, but use the Kuratowski convergence for the limits (see e.g. the Wikipedia page http://en.wikipedia.org/wiki/Kuratowski_convergence ). Then, the first condition is satisfied (I read the proof somewhere, but now forgot where), but the second condition is not (it is easy to construct a counterexample). This is just a thought experiment that has been bothering me for a while, and I would greatly appreciate any responses.",,"['real-analysis', 'measure-theory', 'elementary-set-theory']"
13,How does one determine which variables to do induction on?,How does one determine which variables to do induction on?,,"I have been struggling with this all day.  When one does mathematical induction, how does one choose when to induct with one variable, or with more than one? I have been working through Tao's Analysis I and got up to Lemma 2.2.3 . At this point, addition of natural numbers is only defined as follows: Let $n,m \in \mathbb N$.  Then $0+m:=m$, and $(n^{++})+m:=(n+m)^{++}$ (""$:=$"" means ""defined as"", and ""${}^{++}$"" stands for ""increment of"".  At this point in the text basically only Peano's Axioms are defined.  Commutativity of addition has not been defined yet.) Lemma 2.2.3 states: if $n,m\in \mathbb N$, then $n+(m^{++})=(n+m)^{++}$ I was able to prove it in the way that was shown in the text, but what puzzled me was the method of induction.  In the text, Tao decides to induct on $n$ while leaving $m$ constant, which was great. However, after he finishes this induction, he says that the lemma is proven.  I assumed that afterwards he would have to somehow induct on $m$ (two-dimensional induction), but seemingly this isn't needed.  Why is this? edit: Here is the proof: Proof: Induct on $n$ (keeping $m$ fixed). Consider the base case: $0+(m^{++})=(0+m)^{++}$.  Applying the Def'n of Addition to each side implies that $m^{++}=m^{++}$, which is true. Suppose inductively that $n+(m^{++})=(n+m)^{++}$. We have to prove the inductive step, namely that $(n^{++})+(m^{++})=((n^{++})+m)^{++}$. Using the Def'n of Addition on both sides gives $(n+(m^{++}))^{++}=((n+m)^{++})^{++}$. Using the supposition on the left side gives $((n+m)^{++})^{++}=((n+m)^{++})^{++}$, which is true.   Thus, whenever $P(n)$ is true, $P(n^{++})$ is true.  With the base case established, this proves Lemma 2.2.3 . edit2: I found this blog post with a good example of 2D induction.  It seems in this example, 2D induction is needed only because there are two separate cases: one for $f(m+1,n)$ and another for $f(m,n+1)$.  I'm currently searching for (and trying to understand) other examples.","I have been struggling with this all day.  When one does mathematical induction, how does one choose when to induct with one variable, or with more than one? I have been working through Tao's Analysis I and got up to Lemma 2.2.3 . At this point, addition of natural numbers is only defined as follows: Let $n,m \in \mathbb N$.  Then $0+m:=m$, and $(n^{++})+m:=(n+m)^{++}$ (""$:=$"" means ""defined as"", and ""${}^{++}$"" stands for ""increment of"".  At this point in the text basically only Peano's Axioms are defined.  Commutativity of addition has not been defined yet.) Lemma 2.2.3 states: if $n,m\in \mathbb N$, then $n+(m^{++})=(n+m)^{++}$ I was able to prove it in the way that was shown in the text, but what puzzled me was the method of induction.  In the text, Tao decides to induct on $n$ while leaving $m$ constant, which was great. However, after he finishes this induction, he says that the lemma is proven.  I assumed that afterwards he would have to somehow induct on $m$ (two-dimensional induction), but seemingly this isn't needed.  Why is this? edit: Here is the proof: Proof: Induct on $n$ (keeping $m$ fixed). Consider the base case: $0+(m^{++})=(0+m)^{++}$.  Applying the Def'n of Addition to each side implies that $m^{++}=m^{++}$, which is true. Suppose inductively that $n+(m^{++})=(n+m)^{++}$. We have to prove the inductive step, namely that $(n^{++})+(m^{++})=((n^{++})+m)^{++}$. Using the Def'n of Addition on both sides gives $(n+(m^{++}))^{++}=((n+m)^{++})^{++}$. Using the supposition on the left side gives $((n+m)^{++})^{++}=((n+m)^{++})^{++}$, which is true.   Thus, whenever $P(n)$ is true, $P(n^{++})$ is true.  With the base case established, this proves Lemma 2.2.3 . edit2: I found this blog post with a good example of 2D induction.  It seems in this example, 2D induction is needed only because there are two separate cases: one for $f(m+1,n)$ and another for $f(m,n+1)$.  I'm currently searching for (and trying to understand) other examples.",,"['real-analysis', 'induction']"
14,one to one mapping between the floor function and the Riemann prime counting function,one to one mapping between the floor function and the Riemann prime counting function,,"We have the following 'transform' of a real valued, piecewise continuous function $f(x)$ : $$T[f(x)]=1+\sum_{n=1}^{\infty}\int_{\mathbb{R}^{n}_{+}}f\left(\frac{x}{\Lambda _{n}} \right )\frac{1}{n!}\left(\prod_{i=1}^{n-1}f(u_{i}) \right )\Theta^{n-1}_{u_{n}}f(u_{n})\frac{d\Lambda_{n}}{\Lambda _{n}}$$ where : $$\Lambda_{n} =\prod_{i=1}^{n}u_{i}$$ $$d\Lambda_{n}=\prod_{n=1}^{n}du_{i}$$ $$\Theta_{u_{k}}=u_{k}\frac{d}{du_{k}}$$ and we wish to recover $f(x)$ from $T[f(x)]$. What kind of mathematics should be used to study this problem? EDIT: $$J(x)=\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}\ln\zeta(s)\frac{x^{s}}{s}ds$$ where $J(x)$ is the Riemann prime counting function .   The above relation is the well known Perron's formula for Dirichlet series. This induced me to express $\zeta(s)$ as an exponential series expansion in terms of $\ln\zeta(s)$: $$\zeta(s)=\sum_{n=0}^{\infty}\frac{(\ln\zeta(s))^{n}}{n!}$$ Applying Perron's formula : $$\left \lfloor x \right \rfloor=\frac{1}{2\pi i}\int_{\alpha-i\infty}^{\alpha+i\infty}\zeta(s)\frac{x^{s}}{s}ds=\frac{1}{2\pi i}\sum_{n=0}^{\infty}\frac{1}{n!}\int_{c-i\infty}^{c+i\infty}(\ln\zeta(s))^{n}\frac{x^{s}}{s}ds$$ The first two terms corresponding to $n=0,n=1$ are trivial. The other terms starting at $n=2$ could be done using Mellin convolution. Namely, if two functions, say $F(s)$ and $G(s)$ are given by: $$F(s)=\int_{0}^{\infty}f(x)x^{-s-1}dx$$  $$G(s)=\int_{0}^{\infty}g(x)x^{-s-1}dx$$    then the following holds : $$F(s)G(s)=\int_{0}^{\infty}f(x)\bigstar g(x) x^{-s-1}dx$$  Where the star stands for Mellin convolution, and is defined by :  $$f(x)\bigstar g(x)=\int_{0}^{\infty}f\left( \frac{x}{u}\right)g(u)\frac{du}{u}$$ Another property of the Mellin transform we will need is that, if a function, say $h(x)$ is a Mellin inverse of of $H(s)$, then :  $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}sH(s)x^{s}ds=x\frac{d}{dx}h(x)$$ Using these facts about the Mellin transform, we can evaluate the integrals : $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}(\ln\zeta(s))^{n}\frac{x^{s}}{s}ds=J(x)\bigstar \left( x\frac{d}{dx}J(x)\right)^{\bigstar n-1}$$ Where the star and the power $n-1$ mean repeated convolution for $n-1$ times . Furthermore, the Mellin convolution has the property : $$\left(x\frac{d}{dx}\right)^{n}(f(x)\bigstar g(x))=f(x)\bigstar \left(x\frac{d}{dx}\right)^{n}g(x)=g(x) \bigstar\left(x\frac{d}{dx}\right)^{n}f(x) $$ Therefore : $$\left \lfloor x \right \rfloor=\frac{1}{2\pi i}\int_{\sigma-i\infty}^{\sigma+i\infty}\zeta(s)\frac{x^{s}}{s}ds$$ Therefore : $$\left \lfloor x \right \rfloor=1+  \sum_{n=0}^{\infty}\frac{J(x)^{\bigstar n}}{(n+1)!} \bigstar \left(x\frac{d}{dx}\right)^{n}J(x)$$ This relation could be given more explicitly by: $$\left \lfloor x \right \rfloor=1+\sum_{n=1}^{\infty}\int_{\mathbb{R}^{n}_{+}}J\left(\frac{x}{\Lambda _{n}} \right )\frac{1}{n!}\left(\prod_{i=1}^{n-1}J(u_{i}) \right )\Theta^{n-1}_{u_{n}}J(u_{n})\frac{d\Lambda_{n}}{\Lambda _{n}}$$ where : $$\Lambda_{n} =\prod_{i=1}^{n}u_{i}$$ $$d\Lambda_{n}=\prod_{n=1}^{n}du_{i}$$ $$\Theta_{u_{k}}=u_{k}\frac{d}{du_{k}}$$ We define the following operator acting on a distribution $f(x)$: $$T[f(x)]=1+\sum_{n=1}^{\infty}\int_{\mathbb{R}^{n}_{+}}f\left(\frac{x}{\Lambda _{n}} \right )\frac{1}{n!}\left(\prod_{i=1}^{n-1}f(u_{i}) \right )\Theta^{n-1}_{u_{n}}f(u_{n})\frac{d\Lambda_{n}}{\Lambda _{n}}$$ if $f(x)$ is given by : $$f(x)=\frac{1}{2\pi i}\int_{\sigma-i\infty}^{\sigma+i\infty}\ln g(s)\frac{x^{s}}{s}ds$$ then, the following holds: $$T[f(x)]=\frac{1}{2\pi i}\int_{\sigma-i\infty}^{\sigma+i\infty} g(s)\frac{x^{s}}{s}ds$$ The plan here is to investigate the geometric and algebraic properties of the mapping above. One thing that comes in mind is trying to find an inverse of the operator $T[f(x)]$, such that :  $$T^{-1}T[f(x)]=f(x)$$ assuming such an operator exists, and applying the operator to $\left \lfloor x \right \rfloor$: $$T^{-1}[\left \lfloor x \right \rfloor]=J(x)$$  and the prime counting function $\pi(x)$ could be given by: $$\pi(x)=\sum_{n=1}^{\infty}\frac{\mu(n)}{n}T^{-1}\left[\left \lfloor x^{1/n} \right \rfloor\right]$$","We have the following 'transform' of a real valued, piecewise continuous function $f(x)$ : $$T[f(x)]=1+\sum_{n=1}^{\infty}\int_{\mathbb{R}^{n}_{+}}f\left(\frac{x}{\Lambda _{n}} \right )\frac{1}{n!}\left(\prod_{i=1}^{n-1}f(u_{i}) \right )\Theta^{n-1}_{u_{n}}f(u_{n})\frac{d\Lambda_{n}}{\Lambda _{n}}$$ where : $$\Lambda_{n} =\prod_{i=1}^{n}u_{i}$$ $$d\Lambda_{n}=\prod_{n=1}^{n}du_{i}$$ $$\Theta_{u_{k}}=u_{k}\frac{d}{du_{k}}$$ and we wish to recover $f(x)$ from $T[f(x)]$. What kind of mathematics should be used to study this problem? EDIT: $$J(x)=\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}\ln\zeta(s)\frac{x^{s}}{s}ds$$ where $J(x)$ is the Riemann prime counting function .   The above relation is the well known Perron's formula for Dirichlet series. This induced me to express $\zeta(s)$ as an exponential series expansion in terms of $\ln\zeta(s)$: $$\zeta(s)=\sum_{n=0}^{\infty}\frac{(\ln\zeta(s))^{n}}{n!}$$ Applying Perron's formula : $$\left \lfloor x \right \rfloor=\frac{1}{2\pi i}\int_{\alpha-i\infty}^{\alpha+i\infty}\zeta(s)\frac{x^{s}}{s}ds=\frac{1}{2\pi i}\sum_{n=0}^{\infty}\frac{1}{n!}\int_{c-i\infty}^{c+i\infty}(\ln\zeta(s))^{n}\frac{x^{s}}{s}ds$$ The first two terms corresponding to $n=0,n=1$ are trivial. The other terms starting at $n=2$ could be done using Mellin convolution. Namely, if two functions, say $F(s)$ and $G(s)$ are given by: $$F(s)=\int_{0}^{\infty}f(x)x^{-s-1}dx$$  $$G(s)=\int_{0}^{\infty}g(x)x^{-s-1}dx$$    then the following holds : $$F(s)G(s)=\int_{0}^{\infty}f(x)\bigstar g(x) x^{-s-1}dx$$  Where the star stands for Mellin convolution, and is defined by :  $$f(x)\bigstar g(x)=\int_{0}^{\infty}f\left( \frac{x}{u}\right)g(u)\frac{du}{u}$$ Another property of the Mellin transform we will need is that, if a function, say $h(x)$ is a Mellin inverse of of $H(s)$, then :  $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}sH(s)x^{s}ds=x\frac{d}{dx}h(x)$$ Using these facts about the Mellin transform, we can evaluate the integrals : $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}(\ln\zeta(s))^{n}\frac{x^{s}}{s}ds=J(x)\bigstar \left( x\frac{d}{dx}J(x)\right)^{\bigstar n-1}$$ Where the star and the power $n-1$ mean repeated convolution for $n-1$ times . Furthermore, the Mellin convolution has the property : $$\left(x\frac{d}{dx}\right)^{n}(f(x)\bigstar g(x))=f(x)\bigstar \left(x\frac{d}{dx}\right)^{n}g(x)=g(x) \bigstar\left(x\frac{d}{dx}\right)^{n}f(x) $$ Therefore : $$\left \lfloor x \right \rfloor=\frac{1}{2\pi i}\int_{\sigma-i\infty}^{\sigma+i\infty}\zeta(s)\frac{x^{s}}{s}ds$$ Therefore : $$\left \lfloor x \right \rfloor=1+  \sum_{n=0}^{\infty}\frac{J(x)^{\bigstar n}}{(n+1)!} \bigstar \left(x\frac{d}{dx}\right)^{n}J(x)$$ This relation could be given more explicitly by: $$\left \lfloor x \right \rfloor=1+\sum_{n=1}^{\infty}\int_{\mathbb{R}^{n}_{+}}J\left(\frac{x}{\Lambda _{n}} \right )\frac{1}{n!}\left(\prod_{i=1}^{n-1}J(u_{i}) \right )\Theta^{n-1}_{u_{n}}J(u_{n})\frac{d\Lambda_{n}}{\Lambda _{n}}$$ where : $$\Lambda_{n} =\prod_{i=1}^{n}u_{i}$$ $$d\Lambda_{n}=\prod_{n=1}^{n}du_{i}$$ $$\Theta_{u_{k}}=u_{k}\frac{d}{du_{k}}$$ We define the following operator acting on a distribution $f(x)$: $$T[f(x)]=1+\sum_{n=1}^{\infty}\int_{\mathbb{R}^{n}_{+}}f\left(\frac{x}{\Lambda _{n}} \right )\frac{1}{n!}\left(\prod_{i=1}^{n-1}f(u_{i}) \right )\Theta^{n-1}_{u_{n}}f(u_{n})\frac{d\Lambda_{n}}{\Lambda _{n}}$$ if $f(x)$ is given by : $$f(x)=\frac{1}{2\pi i}\int_{\sigma-i\infty}^{\sigma+i\infty}\ln g(s)\frac{x^{s}}{s}ds$$ then, the following holds: $$T[f(x)]=\frac{1}{2\pi i}\int_{\sigma-i\infty}^{\sigma+i\infty} g(s)\frac{x^{s}}{s}ds$$ The plan here is to investigate the geometric and algebraic properties of the mapping above. One thing that comes in mind is trying to find an inverse of the operator $T[f(x)]$, such that :  $$T^{-1}T[f(x)]=f(x)$$ assuming such an operator exists, and applying the operator to $\left \lfloor x \right \rfloor$: $$T^{-1}[\left \lfloor x \right \rfloor]=J(x)$$  and the prime counting function $\pi(x)$ could be given by: $$\pi(x)=\sum_{n=1}^{\infty}\frac{\mu(n)}{n}T^{-1}\left[\left \lfloor x^{1/n} \right \rfloor\right]$$",,"['real-analysis', 'analytic-number-theory']"
15,Showing $f\left(A\right)=e^{A^{2}}$ is differentiable.,Showing  is differentiable.,f\left(A\right)=e^{A^{2}},"Let $f\left(A\right)=e^{A^{2}}$  where $A$  is an $n\times n$  matrix. Show that $f$  is differentiable and compute its derivative. I know this is kind of a basic question, but I am not sure how to solve it. By definition, we want to show that there exists a matrix $B$  such that $$\lim_{H\to0}\frac{\left|f\left(A+H\right)-f\left(A\right)-BH\right|}{\left|H\right|}=0.$$ Is this right? But since we're working with matrices I am not sure what to do. Any guidence would be appreciated.","Let $f\left(A\right)=e^{A^{2}}$  where $A$  is an $n\times n$  matrix. Show that $f$  is differentiable and compute its derivative. I know this is kind of a basic question, but I am not sure how to solve it. By definition, we want to show that there exists a matrix $B$  such that $$\lim_{H\to0}\frac{\left|f\left(A+H\right)-f\left(A\right)-BH\right|}{\left|H\right|}=0.$$ Is this right? But since we're working with matrices I am not sure what to do. Any guidence would be appreciated.",,"['real-analysis', 'analysis']"
16,Limit: How to Conclude,Limit: How to Conclude,,"I have difficulty to conclude this limit ....; place of my attempts and results, can anyone help? tanks in advance $$\lim_{x\to +a}\, \left(1+6\left(\frac{\sin x}{x^2}\right)^x\cdot\frac{\log(1+10^x)}{x}\right),\quad a=+\infty,\,\,\,0,\,\,\,\,-\infty$$ 1):$\,\,\,{a=+\infty}$ $$\lim_{x\to +\infty}\left(1+6\left(\frac{\sin x}{x^2}\right)^x\cdot\frac{\log(1+10^x)}{x}\right)$$ $$\sim\lim_{x\to +\infty}\, 1+6\left(  \frac{\sin^x x}{x^{2x}} \cdot\frac{\log 10^x }{x}\right)=\lim_{x\to +\infty}\, 1+6\left(  \frac{\sin^x x}{x^{2x}}   \right)$$ $$\text{observing}\,\,\,\,\,\left|\frac{\sin x}{x^2} \right|<\frac{1}{x^2}, \text{so} \left|\left(\frac{\sin x}{x^2}\right)^x\right|<\frac{1}{x^{2x}}\to 0\,\,\,\, x\to+\infty:$$ $$\text{infact we have} \lim_{x\to+\infty} \frac{1}{x^{2x}}=\lim_{x\to +\infty} e^{-2x\log x}=0$$ $$=\lim_{x\to +\infty}\, 1+6\left(  \frac{\sin^x x}{x^{2x}}   \right)=1+6\cdot0=1 $$ 2):$\,\,\,{a=0}$ $$\lim_{x\to 0}\, \left(1+6\left(\frac{\sin x}{x^2}\right)^x\cdot\frac{\log(1+10^x)}{x}\right) \sim\lim_{x\to 0}\, 1+\frac{6\log2}{x}\left(\frac{ x}{x^2}\right)^x$$ $$\sim\lim_{x\to0}1+\frac{6\log2}{x}\left(\frac{1}{x}\right)^x$$ $$=\lim_{x\to 0} 1+\frac{6\log2}{x}e^{ x\ln \left(\frac{1}{x}\right)}\to \lim_{x\to 0}\, x\ln \left(\frac{1}{x}\right)=\lim_{x\to 0}\, \frac{\ln \left(\frac{1}{x }\right)}{\frac{1}{x}}=0\to e^0=1$$ $$=\lim_{x\to 0}\,1+\frac{6\log2}{x}\cdot1=+\infty$$ 3):$\,\,\,{a=-\infty}$ let $x=-t,\,\,\,$if$\,\,\,\ x\to -\infty\,\,\,$we have$ \,\,\,\ t\to+\infty,\,\,\,$ and so : $$\lim_{t\to +\infty}\, \left(1+6\left(\frac{\sin(-t)}{t^2}\right)^{-t}\cdot\frac{\log(1+10^{-t})}{-t}\right)$$ $$=\lim_{t\to +\infty}\, \left(1-6\left(-\frac{\sin t }{t^2}\right)^{-t}\cdot\frac{\log\left(1+\frac{1}{10^t}\right)}{t}\right)$$ $$=1-6\cdot\lim_{t\to +\infty}\, \left[\left(-\frac{t^2}{\sin t }\right)^{t}\cdot\frac{\log\left(1+\frac{1}{10^t}\right)}{t}\right]$$ $$\stackrel{(\bf T)}{=}1-6\cdot\lim_{t\to +\infty}\, \left[\left(-\frac{t^2}{\sin t }\right)^{t}\cdot \left(\frac{1}{10^t}-\frac{1}{2\cdot10^{2t}}\cdot\frac{1}{t}\right) \right]$$ .... but here i'm lost ....","I have difficulty to conclude this limit ....; place of my attempts and results, can anyone help? tanks in advance $$\lim_{x\to +a}\, \left(1+6\left(\frac{\sin x}{x^2}\right)^x\cdot\frac{\log(1+10^x)}{x}\right),\quad a=+\infty,\,\,\,0,\,\,\,\,-\infty$$ 1):$\,\,\,{a=+\infty}$ $$\lim_{x\to +\infty}\left(1+6\left(\frac{\sin x}{x^2}\right)^x\cdot\frac{\log(1+10^x)}{x}\right)$$ $$\sim\lim_{x\to +\infty}\, 1+6\left(  \frac{\sin^x x}{x^{2x}} \cdot\frac{\log 10^x }{x}\right)=\lim_{x\to +\infty}\, 1+6\left(  \frac{\sin^x x}{x^{2x}}   \right)$$ $$\text{observing}\,\,\,\,\,\left|\frac{\sin x}{x^2} \right|<\frac{1}{x^2}, \text{so} \left|\left(\frac{\sin x}{x^2}\right)^x\right|<\frac{1}{x^{2x}}\to 0\,\,\,\, x\to+\infty:$$ $$\text{infact we have} \lim_{x\to+\infty} \frac{1}{x^{2x}}=\lim_{x\to +\infty} e^{-2x\log x}=0$$ $$=\lim_{x\to +\infty}\, 1+6\left(  \frac{\sin^x x}{x^{2x}}   \right)=1+6\cdot0=1 $$ 2):$\,\,\,{a=0}$ $$\lim_{x\to 0}\, \left(1+6\left(\frac{\sin x}{x^2}\right)^x\cdot\frac{\log(1+10^x)}{x}\right) \sim\lim_{x\to 0}\, 1+\frac{6\log2}{x}\left(\frac{ x}{x^2}\right)^x$$ $$\sim\lim_{x\to0}1+\frac{6\log2}{x}\left(\frac{1}{x}\right)^x$$ $$=\lim_{x\to 0} 1+\frac{6\log2}{x}e^{ x\ln \left(\frac{1}{x}\right)}\to \lim_{x\to 0}\, x\ln \left(\frac{1}{x}\right)=\lim_{x\to 0}\, \frac{\ln \left(\frac{1}{x }\right)}{\frac{1}{x}}=0\to e^0=1$$ $$=\lim_{x\to 0}\,1+\frac{6\log2}{x}\cdot1=+\infty$$ 3):$\,\,\,{a=-\infty}$ let $x=-t,\,\,\,$if$\,\,\,\ x\to -\infty\,\,\,$we have$ \,\,\,\ t\to+\infty,\,\,\,$ and so : $$\lim_{t\to +\infty}\, \left(1+6\left(\frac{\sin(-t)}{t^2}\right)^{-t}\cdot\frac{\log(1+10^{-t})}{-t}\right)$$ $$=\lim_{t\to +\infty}\, \left(1-6\left(-\frac{\sin t }{t^2}\right)^{-t}\cdot\frac{\log\left(1+\frac{1}{10^t}\right)}{t}\right)$$ $$=1-6\cdot\lim_{t\to +\infty}\, \left[\left(-\frac{t^2}{\sin t }\right)^{t}\cdot\frac{\log\left(1+\frac{1}{10^t}\right)}{t}\right]$$ $$\stackrel{(\bf T)}{=}1-6\cdot\lim_{t\to +\infty}\, \left[\left(-\frac{t^2}{\sin t }\right)^{t}\cdot \left(\frac{1}{10^t}-\frac{1}{2\cdot10^{2t}}\cdot\frac{1}{t}\right) \right]$$ .... but here i'm lost ....",,"['calculus', 'real-analysis', 'sequences-and-series']"
17,Generalization Limit,Generalization Limit,,"I would like to know if it is possible generalize this result and how it could be shown: we know that $$ \begin{align} \lim_{x\to 0^+}x^x & =1;\\  \\ \lim_{x\to 0^+}x^{x^x} & =0;\\  \\ \lim_{x\to 0^+}x^{x^{x^x}}& =1 \end{align} $$ $$\begin{matrix}\displaystyle\lim_{x\to 0^+} \overbrace{x^{x^{x^{x^{\cdots^{x}}}}}}^{n\text{ times}} \end{matrix}\quad$$ I would like to conclude that if $n$ equal limit is $1,$ and if $n$ is odd limit is $0$ is it Possible? tanks in advances","I would like to know if it is possible generalize this result and how it could be shown: we know that $$ \begin{align} \lim_{x\to 0^+}x^x & =1;\\  \\ \lim_{x\to 0^+}x^{x^x} & =0;\\  \\ \lim_{x\to 0^+}x^{x^{x^x}}& =1 \end{align} $$ $$\begin{matrix}\displaystyle\lim_{x\to 0^+} \overbrace{x^{x^{x^{x^{\cdots^{x}}}}}}^{n\text{ times}} \end{matrix}\quad$$ I would like to conclude that if $n$ equal limit is $1,$ and if $n$ is odd limit is $0$ is it Possible? tanks in advances",,"['calculus', 'real-analysis', 'limits']"
18,"If $f$ has a vanishing (first or higher) derivative at every point, $f$ is a polynomial","If  has a vanishing (first or higher) derivative at every point,  is a polynomial",f f,"There's a problem from calculus I remember: $$\forall x\ \exists n.\ f^{(n)}(x) = 0 \iff \exists n\ \forall x.\ f^{(n)}(x) = 0\,.$$ Function $f \in C^\infty(\mathbb{R})$, and the notation $f^{(n)}$ means differentiation. Right side is just curious statement that $f$ is a polynomial. Of course $(\Leftarrow)$ is just trivial, however, $(\Rightarrow)$ is far from obvious. Have anybody seen this, maybe somebody knows where it comes from? What about the proof of $(\Rightarrow)$?","There's a problem from calculus I remember: $$\forall x\ \exists n.\ f^{(n)}(x) = 0 \iff \exists n\ \forall x.\ f^{(n)}(x) = 0\,.$$ Function $f \in C^\infty(\mathbb{R})$, and the notation $f^{(n)}$ means differentiation. Right side is just curious statement that $f$ is a polynomial. Of course $(\Leftarrow)$ is just trivial, however, $(\Rightarrow)$ is far from obvious. Have anybody seen this, maybe somebody knows where it comes from? What about the proof of $(\Rightarrow)$?",,"['calculus', 'real-analysis', 'analysis', 'polynomials']"
19,Bernstein's theorem for non-finite measures,Bernstein's theorem for non-finite measures,,"Bernstein's theorem states the following: Let $g: [0, \infty) \to [0, \infty)$ be a smooth function. Then the following are equivalent: (a) $g$ is a completely monotone function, i.e. $(-1)^{n}g^{(n)}(x) \geq 0$ for all $n \geq 0$ and $x \geq 0$ . (b) $g(x) = \int_{0}^{\infty} e^{-tx} d\mu(t)$ for some (unique) finite Borel measure $\mu$ on $[0, \infty)$ . I wonder if we can modify this theorem slightly to the functions on $(0, \infty)$ (exclude $0$ ) and allow non-finite measures. For example, we have $\frac{1}{x} = \int_{0}^{\infty} e^{-tx} dt$ , and the function $g(x) = \frac{1}{x}$ is completely monotone on $(0, \infty)$ , and the corresponding measure is $dt$ , which is not finite. Another example is $g(x) = 1 + e^{-x} + e^{-2x} + \cdots = \frac{1}{1 - e^{-x}}$ , which is completely monotone on $(0, \infty)$ and a Laplace transform of the measure $\mu = \sum_{n \geq 0} \delta_{n}$ , which is again not finite.","Bernstein's theorem states the following: Let be a smooth function. Then the following are equivalent: (a) is a completely monotone function, i.e. for all and . (b) for some (unique) finite Borel measure on . I wonder if we can modify this theorem slightly to the functions on (exclude ) and allow non-finite measures. For example, we have , and the function is completely monotone on , and the corresponding measure is , which is not finite. Another example is , which is completely monotone on and a Laplace transform of the measure , which is again not finite.","g: [0, \infty) \to [0, \infty) g (-1)^{n}g^{(n)}(x) \geq 0 n \geq 0 x \geq 0 g(x) = \int_{0}^{\infty} e^{-tx} d\mu(t) \mu [0, \infty) (0, \infty) 0 \frac{1}{x} = \int_{0}^{\infty} e^{-tx} dt g(x) = \frac{1}{x} (0, \infty) dt g(x) = 1 + e^{-x} + e^{-2x} + \cdots = \frac{1}{1 - e^{-x}} (0, \infty) \mu = \sum_{n \geq 0} \delta_{n}","['real-analysis', 'measure-theory']"
20,"Show that if the ratios of coefficients is unbounded, then the power series converges only at 0","Show that if the ratios of coefficients is unbounded, then the power series converges only at 0",,"I have a question about Exercise 6.5.8(d) from Understanding Analysis by Stephen Abbott. Problem: Let $\sum a_nx^n$ be a power series with $a_n\neq0$ . Show that if $|\frac{a_{n+1}}{a_n}|$ is unbounded, then the original series $\sum a_nx^n$ converges only when $x = 0$ . I have come up with a possible counterexample. Let $(a_n)=(1,\frac{1}{2},1,\frac{1}{3},1,\frac{1}{4},\dots)$ . Then $|\frac{a_{n+1}}{a_n}|$ is unbounded. For $x=\frac{1}{2}$ , $$ \sum a_n x^n =\sum a_n \frac{1}{2^n} $$ Since each $0<a_n\leq1$ , the partial sums of the series is bounded by the infinite geometric series $\sum \frac{1}{2^n}$ . As each term is positive, we can use the monotone convergence theorem to show the series converges at $x=\frac{1}{2}$ . However, this contradicts the claim in the question that the series only converges when $x=0$ . So, my question is: why does this counterexample not work? Thanks","I have a question about Exercise 6.5.8(d) from Understanding Analysis by Stephen Abbott. Problem: Let be a power series with . Show that if is unbounded, then the original series converges only when . I have come up with a possible counterexample. Let . Then is unbounded. For , Since each , the partial sums of the series is bounded by the infinite geometric series . As each term is positive, we can use the monotone convergence theorem to show the series converges at . However, this contradicts the claim in the question that the series only converges when . So, my question is: why does this counterexample not work? Thanks","\sum a_nx^n a_n\neq0 |\frac{a_{n+1}}{a_n}| \sum a_nx^n x = 0 (a_n)=(1,\frac{1}{2},1,\frac{1}{3},1,\frac{1}{4},\dots) |\frac{a_{n+1}}{a_n}| x=\frac{1}{2} 
\sum a_n x^n =\sum a_n \frac{1}{2^n}
 0<a_n\leq1 \sum \frac{1}{2^n} x=\frac{1}{2} x=0","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
21,Series with numbers $\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}$,Series with numbers,\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k},"I'm looking for series alike in the literature containing in their summands numbers of the type $\displaystyle Q_n=\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}$ . The following three series have recently been presented by C.I.Valean in this short exposition , \begin{equation*} i) \ \sum _{n=1}^{\infty } \frac{ 2^n}{\displaystyle n^2 \binom{2 n}{n}}\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty } \frac{ 2^n Q_n}{\displaystyle n^2 \binom{2 n}{n}}=2 G; \end{equation*} \begin{equation*} \small ii) \ \sum _{n=1}^{\infty } (-1)^{n-1} \frac{ 2^n}{\displaystyle n^2 \binom{2 n}{n}}\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty } (-1)^{n-1} \frac{ 2^n Q_n}{\displaystyle n^2 \binom{2 n}{n}}=\frac{\pi}{3}  \log(2+\sqrt{3})-\frac{2}{3}G; \end{equation*} \begin{equation*} iii) \ \sum _{n=1}^{\infty } \frac{ 4^n}{\displaystyle n^2 \binom{4 n}{2n}}\sum _{k=0}^{2n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty } \frac{ 4^n Q_{2n}}{\displaystyle n^2 \binom{4 n}{2n}}=\frac{16}{3}G-\frac{2}{3}\pi \log(2+\sqrt{3}), \end{equation*} where here $\displaystyle G=\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{(2n-1)^2}$ is the Catalan's constant. For example, in the case of the second series, we may proceed as follows: exploiting that $\displaystyle \int_0^{\pi/2} \sin(n \theta) \cos^{n-1}(\theta) \textrm{d}\theta=\frac{2^n}{\displaystyle n \binom{2n}{n}}\sum_{k=0}^{n-1} \frac{1}{2^k}\binom{2k}{k}$ , which is found and evaluated in More (Almost) Impossible Integrals, Sums, and Series (2023) , page $16$ (exploiting simple recurrence relations is enough to extract a solution), we have $$\sum _{n=1}^{\infty } (-1)^{n-1} \frac{ 2^n}{\displaystyle n^2 \binom{2 n}{n}}\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty} (-1)^{n-1}\frac{1}{n} \int_0^{\pi/2} \sin(n \theta) \cos^{n-1}(\theta) \textrm{d}\theta$$ $$=\int_0^{\pi/2}\frac{1}{\cos(\theta)}\Im \biggr\{\sum_{n=1}^{\infty} (-1)^{n-1}\frac{(\cos(\theta) e^{i \theta})^n}{n}\biggr\}\textrm{d}\theta=\int_0^{\pi/2} \frac{\Im \{\log(1+\cos(\theta) e^{i \theta})\}}{\cos(\theta)}\textrm{d}\theta$$ $$\small =\int_0^{\pi/2}\frac{1}{\cos(\theta)} \arctan\left(\frac{\cot(\theta)}{1+2\cot^2(\theta)}\right)\textrm{d}\theta\overset{\theta\mapsto \pi/2-\theta}{=}\int_0^{\pi/2}\frac{1}{\sin(\theta)} \arctan\left(\frac{\tan(\theta)}{1+2\tan^2(\theta)}\right)\textrm{d}\theta$$ $$\overset{\theta \mapsto 2 \arctan(\theta)}{=}\int_0^1 \frac{1}{\theta}\arctan\left(\frac{2\theta (1-\theta^2)}{1+6 \theta^2+\theta^4}\right)\textrm{d}\theta$$ $$=\int_0^1 \frac{1}{\theta}\arctan\left(\frac{4\theta/(1-\theta^2)-2\theta/(1-\theta^2)}{1+(4 \theta/(1-\theta^2)) \cdot (2 \theta/(1-\theta^2))}\right)\textrm{d}\theta$$ $$=\int_0^1 \frac{1}{\theta}\arctan\left(\frac{4\theta}{1-\theta^2}\right)\textrm{d}\theta-\int_0^1 \frac{1}{\theta}\arctan\left(\frac{2\theta}{1-\theta^2}\right)\textrm{d}\theta$$ $$=\int_0^1 \frac{1}{\theta}\arctan\left(\frac{(2-\sqrt{3})\theta+(2+\sqrt{3})\theta}{1-(2-\sqrt{3})\theta\cdot (2+\sqrt{3})\theta}\right)\textrm{d}\theta-2\int_0^1 \frac{\arctan(\theta)}{\theta}\textrm{d}\theta$$ $$=\int_0^1 \frac{\arctan((2-\sqrt{3})\theta)}{\theta}\textrm{d}\theta+\int_0^1 \frac{\arctan((2+\sqrt{3})\theta)}{\theta}\textrm{d}\theta-2\int_0^1 \frac{\arctan(\theta)}{\theta}\textrm{d}\theta$$ $$=\operatorname{Ti}_2(2-\sqrt{3}) +\operatorname{Ti}_2(2+\sqrt{3}) -2 \operatorname{Ti}_2(1)=\frac{\pi}{3}  \log(2+\sqrt{3})-\frac{2}{3}G,$$ where we also needed to employ the well-known special values of the Inverse tangent integral like $\displaystyle \operatorname{Ti}_2(1)=G$ , $\displaystyle \operatorname{Ti}_2(2-\sqrt{3})=\frac{2}{3}G+\log(2-\sqrt{3})\frac{\pi}{12}$ , and $\displaystyle \operatorname{Ti}_2(2+\sqrt{3})=\frac{2 }{3}G-\frac{5}{12}\log \left(2-\sqrt{3}\right)\pi$ . For another solution, one might think of using Wallis' integrals. Question 1: Given the simplicity of the first result I wonder if that series already appeared in the literature (if so, the presentation will be updated accordingly). Have you ever met it before (together with a reference)? Question 2: More generally, I'm interested in finding more series involving those $Q_n$ numbers if they are available in the literature. Any references? Personally, I would also find very interesting more general cases like $$S_m=\sum _{n=1}^{\infty } \frac{ 2^n Q_n}{\displaystyle n^m \binom{2 n}{n}}, \ m \ge3,$$ and note that for $m=2$ we get the series from the point $i)$ . Update 1: Another fantastic example , given the generalization above, is the case $m=3$ . Cornel says that by using a strategy similar to the one in the reference above, we get that $$\sum _{n=1}^{\infty } \frac{ 2^n}{\displaystyle n^3 \binom{2 n}{n}}\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty } \frac{ 2^n Q_n}{\displaystyle n^3 \binom{2 n}{n}}=2 \log(2)G,$$ and it looks too beautiful to be real! This closed form makes me wonder if (there is any chance) we can relate this series to this integral problem in this post How to show $\int_0^1\frac{\operatorname{Li}_2\left(\frac{1+x^2}{2}\right)}{1+x^2}dx=\ln(2)G$ . Update 2: Okay, I've just found an answer to my thoughts in the previous update, and, Yes , they can be perfectly related. So, we can, for example, evaluate this series at this point by three solutions (a bit subtle this part in the sense that they are different only from the perspective of getting the core integral evaluated differently), and now I refer to the integral at the reference in the previous update (it represents the toughest obstacle in the evaluation). So, for that integral we already have two solutions in More (Almost) Impossible Integrals, Sums, and Series (2023) and one by Sujeethan Balendran available at that link. Update 3: The version $m=4$ has just been derived by Cornel, and together with the closed form all looks more than amazing, $$\sum _{n=1}^{\infty } \frac{ 2^n}{\displaystyle n^4 \binom{2 n}{n}}\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty } \frac{ 2^n Q_n}{\displaystyle n^4 \binom{2 n}{n}}$$ $$=\log^3(2)\frac{\pi}{12}-\frac{\pi ^2 }{3}G-\frac{5 }{48}\pi ^4   +\frac{5 }{384}\psi ^{(3)}\left(\frac{1}{4}\right)$$ $$-4 \log (2) \Im\left \{\text{Li}_3\left(\frac{1+i}{2}\right)\right\}-8 \Im\left \{\text{Li}_4\left(\frac{1+i}{2}\right)\right\}.$$ In the derivation process, the same starting ideas in the paper above were exploited.","I'm looking for series alike in the literature containing in their summands numbers of the type . The following three series have recently been presented by C.I.Valean in this short exposition , where here is the Catalan's constant. For example, in the case of the second series, we may proceed as follows: exploiting that , which is found and evaluated in More (Almost) Impossible Integrals, Sums, and Series (2023) , page (exploiting simple recurrence relations is enough to extract a solution), we have where we also needed to employ the well-known special values of the Inverse tangent integral like , , and . For another solution, one might think of using Wallis' integrals. Question 1: Given the simplicity of the first result I wonder if that series already appeared in the literature (if so, the presentation will be updated accordingly). Have you ever met it before (together with a reference)? Question 2: More generally, I'm interested in finding more series involving those numbers if they are available in the literature. Any references? Personally, I would also find very interesting more general cases like and note that for we get the series from the point . Update 1: Another fantastic example , given the generalization above, is the case . Cornel says that by using a strategy similar to the one in the reference above, we get that and it looks too beautiful to be real! This closed form makes me wonder if (there is any chance) we can relate this series to this integral problem in this post How to show $\int_0^1\frac{\operatorname{Li}_2\left(\frac{1+x^2}{2}\right)}{1+x^2}dx=\ln(2)G$ . Update 2: Okay, I've just found an answer to my thoughts in the previous update, and, Yes , they can be perfectly related. So, we can, for example, evaluate this series at this point by three solutions (a bit subtle this part in the sense that they are different only from the perspective of getting the core integral evaluated differently), and now I refer to the integral at the reference in the previous update (it represents the toughest obstacle in the evaluation). So, for that integral we already have two solutions in More (Almost) Impossible Integrals, Sums, and Series (2023) and one by Sujeethan Balendran available at that link. Update 3: The version has just been derived by Cornel, and together with the closed form all looks more than amazing, In the derivation process, the same starting ideas in the paper above were exploited.","\displaystyle Q_n=\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k} \begin{equation*}
i) \ \sum _{n=1}^{\infty } \frac{ 2^n}{\displaystyle n^2 \binom{2 n}{n}}\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty } \frac{ 2^n Q_n}{\displaystyle n^2 \binom{2 n}{n}}=2 G;
\end{equation*} \begin{equation*}
\small ii) \ \sum _{n=1}^{\infty } (-1)^{n-1} \frac{ 2^n}{\displaystyle n^2 \binom{2 n}{n}}\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty } (-1)^{n-1} \frac{ 2^n Q_n}{\displaystyle n^2 \binom{2 n}{n}}=\frac{\pi}{3}  \log(2+\sqrt{3})-\frac{2}{3}G;
\end{equation*} \begin{equation*}
iii) \ \sum _{n=1}^{\infty } \frac{ 4^n}{\displaystyle n^2 \binom{4 n}{2n}}\sum _{k=0}^{2n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty } \frac{ 4^n Q_{2n}}{\displaystyle n^2 \binom{4 n}{2n}}=\frac{16}{3}G-\frac{2}{3}\pi \log(2+\sqrt{3}),
\end{equation*} \displaystyle G=\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{(2n-1)^2} \displaystyle \int_0^{\pi/2} \sin(n \theta) \cos^{n-1}(\theta) \textrm{d}\theta=\frac{2^n}{\displaystyle n \binom{2n}{n}}\sum_{k=0}^{n-1} \frac{1}{2^k}\binom{2k}{k} 16 \sum _{n=1}^{\infty } (-1)^{n-1} \frac{ 2^n}{\displaystyle n^2 \binom{2 n}{n}}\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty} (-1)^{n-1}\frac{1}{n} \int_0^{\pi/2} \sin(n \theta) \cos^{n-1}(\theta) \textrm{d}\theta =\int_0^{\pi/2}\frac{1}{\cos(\theta)}\Im \biggr\{\sum_{n=1}^{\infty} (-1)^{n-1}\frac{(\cos(\theta) e^{i \theta})^n}{n}\biggr\}\textrm{d}\theta=\int_0^{\pi/2} \frac{\Im \{\log(1+\cos(\theta) e^{i \theta})\}}{\cos(\theta)}\textrm{d}\theta \small =\int_0^{\pi/2}\frac{1}{\cos(\theta)} \arctan\left(\frac{\cot(\theta)}{1+2\cot^2(\theta)}\right)\textrm{d}\theta\overset{\theta\mapsto \pi/2-\theta}{=}\int_0^{\pi/2}\frac{1}{\sin(\theta)} \arctan\left(\frac{\tan(\theta)}{1+2\tan^2(\theta)}\right)\textrm{d}\theta \overset{\theta \mapsto 2 \arctan(\theta)}{=}\int_0^1 \frac{1}{\theta}\arctan\left(\frac{2\theta (1-\theta^2)}{1+6 \theta^2+\theta^4}\right)\textrm{d}\theta =\int_0^1 \frac{1}{\theta}\arctan\left(\frac{4\theta/(1-\theta^2)-2\theta/(1-\theta^2)}{1+(4 \theta/(1-\theta^2)) \cdot (2 \theta/(1-\theta^2))}\right)\textrm{d}\theta =\int_0^1 \frac{1}{\theta}\arctan\left(\frac{4\theta}{1-\theta^2}\right)\textrm{d}\theta-\int_0^1 \frac{1}{\theta}\arctan\left(\frac{2\theta}{1-\theta^2}\right)\textrm{d}\theta =\int_0^1 \frac{1}{\theta}\arctan\left(\frac{(2-\sqrt{3})\theta+(2+\sqrt{3})\theta}{1-(2-\sqrt{3})\theta\cdot (2+\sqrt{3})\theta}\right)\textrm{d}\theta-2\int_0^1 \frac{\arctan(\theta)}{\theta}\textrm{d}\theta =\int_0^1 \frac{\arctan((2-\sqrt{3})\theta)}{\theta}\textrm{d}\theta+\int_0^1 \frac{\arctan((2+\sqrt{3})\theta)}{\theta}\textrm{d}\theta-2\int_0^1 \frac{\arctan(\theta)}{\theta}\textrm{d}\theta =\operatorname{Ti}_2(2-\sqrt{3}) +\operatorname{Ti}_2(2+\sqrt{3}) -2 \operatorname{Ti}_2(1)=\frac{\pi}{3}  \log(2+\sqrt{3})-\frac{2}{3}G, \displaystyle \operatorname{Ti}_2(1)=G \displaystyle \operatorname{Ti}_2(2-\sqrt{3})=\frac{2}{3}G+\log(2-\sqrt{3})\frac{\pi}{12} \displaystyle \operatorname{Ti}_2(2+\sqrt{3})=\frac{2 }{3}G-\frac{5}{12}\log \left(2-\sqrt{3}\right)\pi Q_n S_m=\sum _{n=1}^{\infty } \frac{ 2^n Q_n}{\displaystyle n^m \binom{2 n}{n}}, \ m \ge3, m=2 i) m=3 \sum _{n=1}^{\infty } \frac{ 2^n}{\displaystyle n^3 \binom{2 n}{n}}\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty } \frac{ 2^n Q_n}{\displaystyle n^3 \binom{2 n}{n}}=2 \log(2)G, m=4 \sum _{n=1}^{\infty } \frac{ 2^n}{\displaystyle n^4 \binom{2 n}{n}}\sum _{k=0}^{n-1}\frac{1}{2^k }\binom{2 k}{k}=\sum _{n=1}^{\infty } \frac{ 2^n Q_n}{\displaystyle n^4 \binom{2 n}{n}} =\log^3(2)\frac{\pi}{12}-\frac{\pi ^2 }{3}G-\frac{5 }{48}\pi ^4   +\frac{5 }{384}\psi ^{(3)}\left(\frac{1}{4}\right) -4 \log (2) \Im\left \{\text{Li}_3\left(\frac{1+i}{2}\right)\right\}-8 \Im\left \{\text{Li}_4\left(\frac{1+i}{2}\right)\right\}.","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'reference-request']"
22,Must injective path-connected maps/“path-Darboux” functions be continuous?,Must injective path-connected maps/“path-Darboux” functions be continuous?,,"(Inspired by https://mathoverflow.net/questions/235893/does-there-exist-a-bijection-of-mathbbrn-to-itself-such-that-the-forward-m?rq=1 ) Let us define a Darboux function or connected map to be a map between topological spaces that preserves connected sets (i.e. the image of any connected set is connected); and a path-Darboux function or path-connected map to be a map that preserves path-connected sets. Conjecture: every injective path-connected map $f:[0,1]\hookrightarrow \mathbb R^2$ has to be continuous. I make several pathetic attempts at this Conjecture below and draw some pictures, but honestly I do not expect nor wish anyone to do anything more than barely skim through them. Preliminary Remarks: The converse is of course true: all continuous functions are path-connected maps. Injectivity is important because of the example of ""everywhere locally surjective"" functions (functions whose images of any nonempty open set is the entire codomain), like the Conway Base-13 function, or another example using the Riemann rearrangement theorem on the alternating harmonic series . The Conjecture is true for $f: [0,1] \hookrightarrow \mathbb R$ , because we know all discontinuities of Darboux functions are essential , but essential discontinuities contradict injectivity. If one considers ""connected"" instead of ""path-connected"", the Conjecture is false, by considering the graph of the topologist sine curve $g$ with $g(0):=(0,0)$ ( $g(t):= (t,\sin(\frac 1t))$ for $t\in (0,1]$ ). Let us now assume $f$ is not (right-)continuous at $0$ , and normalize $f(0)=0$ . I think user TheEmptyFunction's answer to the above linked MO post https://mathoverflow.net/a/353018/112504 also applies here to tell us that there are arbitrarily small radii $\epsilon>0$ s.t. there is a sequence $y_n^\epsilon \searrow 0$ s.t. $f(y_n^\epsilon) \in C(0,\epsilon)$ ---- (illustrated below as the blue points, which I think we can normalize via pre-composing with a homeomorphism of $[0,1]$ and post-composing with a homeomorphism of $\mathbb R^2$ to be $\{\frac 1m: m \in \mathbb N^+\}$ in the domain, and $i\cdot e^{-\pi/2 \cdot \frac 1m})$ in the codomain, with $\epsilon=1$ ) By mapping $(\frac 1{m+1},\frac 1m]$ to the line segment between $0\in \mathbb C$ (not inclusive) and $i\cdot e^{-\pi/2 \cdot \frac 1m} \in \mathbb C$ (inclusive) in a linear manner say (illustrated above), we get an injective map $[0,1]\to \mathbb R^2$ that maps any interval $[0,a\rangle$ (where the right-hand bracket $\rangle$ denotes both "" $)$ "" and "" $]$ "") to a path-connected set in $\mathbb R^2$ (perhaps we could say this function is a ""initial-segment path-connected map""); but any interval $(0,a\rangle$ would not map to a path-connected set. This example is a ""near miss"" to the Conjecture. $$$$ One idea: projection onto 1st and 2nd coordinates are both continuous maps, hence preserve path-connectedness. Thus if $f:=[t\mapsto (x(t),y(t))]:[0,1] \to \mathbb R^2$ is a path-connected map, $x,y:[0,1]\to \mathbb R$ must be as well. As I mentioned above, a well-known class of non-continuous path-connected maps $[0,1]\to \mathbb R$ are the ""everywhere locally surjective"" functions. By postcomposing with a homeomorphism $\mathbb R \to (0,1)$ , we can think of $x,y$ as ""everywhere locally surjective"" maps $x,y:[0,1]\to (0,1)$ . Then to make $f$ bijective, we need $y$ to be injective when restricted on each fiber $x^{-1}(r)$ for all $r\in \mathbb R$ (where each such fiber is dense in the domain), and vice versa ( $x$ injective when restricted to each fiber $y^{-1}(r)$ for all $r\in \mathbb R$ ). Maybe something like this could work if we can partition $[0,1]$ into disjoint classes $C_\alpha$ where each $C_\alpha \cap x^{-1}(r)$ contains exactly 1 element (so each $C_\alpha$ contains exactly one element that gets mapped to $r$ , for each real number $r$ ), and if $y$ attains a different constant value $c_\alpha$ on each $C_\alpha$ . And to make $y$ ""everywhere locally surjective"" we need each $C_\alpha$ to be dense in $[0,1]$ . However I doubt that such a function preserves path-connectedness; it sends any interval $(a,b)$ (or $[a,b)$ , etc.) to a set with lots of pieces of horizontal and vertical lines; if the pieces are too fragmented, we won't get path connectivity. I can imagine that if $y$ was monotone on $x^{-1}(r)$ or something, then we would see a longer vertical line piece at $x=r$ ; but unfortunately as I commented here this desired monotonicity is not possible. $$\newcommand{\im}{\operatorname{im}} \newcommand{\harmonicint}[1]{[\frac 1{#1+1},\frac 1{#1}]} \newcommand{\harmonicintt}[1]{[1/({#1+1}),1/{#1}]}$$ Infinitely Many Discontinuities, and Trees In working with the previous near-miss example, I got the impression that in fact MiniConjecture: one point $p$ of discontinuity of $f$ begets infinitely many points of discontinuity converging to $p$ , making such a potential discontinuous $f$ quite difficult to imagine. Unfortunately I could not prove this MiniConjecture . Attempted proof that 1 discontinuity begets infinitely many: Since we know that each $[0, \frac 1m]$ maps to a path-connected set containing $f(\frac 1m)$ and $f(0)=0$ , let us define : paths $P_m : [-\frac 1m, 0] \to f([0, \frac 1m])\subseteq \mathbb R^2$ , which we can assume to be injective by e.g. Equivalence of Path-Connectedness and Arc-Connectedness for Hausdorff Spaces , or using Zorn's lemma (article by Jeremy Brazas) --- illustrated below in orange. and paths $F_m: [-\frac 1m, - \frac 1{m+1}] \to f([\frac 1{m+1},\frac 1m]) \subseteq \mathbb R^2$ going from $f(\frac 1m)$ to $f(\frac 1{m+1})$ (which we can again assume injective; and furthermore because $f$ is injective, the paths $F_m$ will be disjoint except touching perhaps at endpoints $f(\frac 1m)$ ) --- illustrated below in green. Now if $f|_{[1/(m+1),1/m]}$ is continuous as a function $[\frac 1{m+1},\frac 1m]\hookrightarrow \mathbb R^2$ , then $f|_{[1/(m+1),1/m]}$ is in fact a homeomorphism onto its image $f(\harmonicint m) \supseteq \im(F_m)$ . So its inverse is a continuous map, mapping the path-connected set $\im(F_m)$ to path-connected set in $\harmonicint m$ containing $\frac 1{m+1},\frac 1m$ (since the path $F_m$ goes through those points), meaning $f^{-1}$ maps $\im(F_m) \subseteq f(\harmonicint m)$ to all of $\harmonicint m$ , implying that $f|_{\harmonicintt m}$ is in fact a continuous bijection ( and hence homeomorphism) $\harmonicint m \to \im(F_m) = f(\harmonicint m)$ . So if $f$ is continuous on $(0,\eta)$ for some $\eta>\frac 1M >0$ , then all $\harmonicint m$ inside $(0,\eta)$ will be mapped by $f$ to a path from $f(\frac 1{m+1})$ to $f(\frac 1m)$ ; in other words, $f$ is composed of all these paths $F_m$ stitched together at the endpoints. Now if I had the injective continuous path $P_M :[-\frac 1M, 0] \hookrightarrow f([0,\frac 1M])$ that goes through $f(\frac 1M)$ and $f(0)=0$ , the fact that $f((0,\frac 1M])$ is the continuous image of $(0,1]$ (so a path/""ray"" with a perhaps infinite ""tail"" that may wiggle a lot, but still pass through all of the points $f(\frac 1m)$ illustrated below), intuitively $P_M$ must end up travelling along this ""ray"". Then one would hope to get a contradiction because if $P_M$ travels along this ray, then it would be forced to continue to come across the points $f(\frac 1m)$ (blue points in below pictures), and because the points $f(\frac 1m)$ are so far from $f(0)=0$ , this would contradict that $P_M$ is a continuous path $[-\frac 1M, 0] \hookrightarrow f([0,\frac 1M])$ . Sadly after making many attempts in which I kept finding mistakes, I gave up trying to prove this MiniConjecture , though I suspect that it can be proven by making the above intuition/hand-waving rigorous. $$$$ In any case, assuming $f:[0,1] \hookrightarrow \mathbb R^2$ has a discontinuity at $p\in [0,1]$ (e.g. $p=0$ as I normalized above), the paths $F_m$ and $P_m$ that are guaranteed (by path-connectedness of the map $f$ ) make us have a picture that looks like what follows, with the green $F_m$ paths and orange $P_m$ paths. If indeed we have infinitely many points of discontinuity (illustrated in red) converging to $p$ from the right, then each of those points will lead to another version of this tree structure in the codomain (which I have suggested in the small circles around the red points in the codomain). All images of intervals $[p, p+\eta\rangle$ (where "" $\rangle$ "" refers to both "" $)$ "" and "" $]$ "") will map to such a tree. Subtracting 2 such intervals leads to general intervals $\langle p+\delta, p+\eta\rangle$ which get mapped to a tree like above, with some portion (portions on the left in the above picture) cut off. But if the tree is super ""well-connected"" with threads weaving throughout the ""canopy"", I can imagine that the set remains path-connected. So perhaps the Conjecture above is false, if one can construct an injective map from $[0,1]$ to this fractal tree-like structure. $%[![simplified path connected tree thingy][8]][8]$","(Inspired by https://mathoverflow.net/questions/235893/does-there-exist-a-bijection-of-mathbbrn-to-itself-such-that-the-forward-m?rq=1 ) Let us define a Darboux function or connected map to be a map between topological spaces that preserves connected sets (i.e. the image of any connected set is connected); and a path-Darboux function or path-connected map to be a map that preserves path-connected sets. Conjecture: every injective path-connected map has to be continuous. I make several pathetic attempts at this Conjecture below and draw some pictures, but honestly I do not expect nor wish anyone to do anything more than barely skim through them. Preliminary Remarks: The converse is of course true: all continuous functions are path-connected maps. Injectivity is important because of the example of ""everywhere locally surjective"" functions (functions whose images of any nonempty open set is the entire codomain), like the Conway Base-13 function, or another example using the Riemann rearrangement theorem on the alternating harmonic series . The Conjecture is true for , because we know all discontinuities of Darboux functions are essential , but essential discontinuities contradict injectivity. If one considers ""connected"" instead of ""path-connected"", the Conjecture is false, by considering the graph of the topologist sine curve with ( for ). Let us now assume is not (right-)continuous at , and normalize . I think user TheEmptyFunction's answer to the above linked MO post https://mathoverflow.net/a/353018/112504 also applies here to tell us that there are arbitrarily small radii s.t. there is a sequence s.t. ---- (illustrated below as the blue points, which I think we can normalize via pre-composing with a homeomorphism of and post-composing with a homeomorphism of to be in the domain, and in the codomain, with ) By mapping to the line segment between (not inclusive) and (inclusive) in a linear manner say (illustrated above), we get an injective map that maps any interval (where the right-hand bracket denotes both "" "" and "" "") to a path-connected set in (perhaps we could say this function is a ""initial-segment path-connected map""); but any interval would not map to a path-connected set. This example is a ""near miss"" to the Conjecture. One idea: projection onto 1st and 2nd coordinates are both continuous maps, hence preserve path-connectedness. Thus if is a path-connected map, must be as well. As I mentioned above, a well-known class of non-continuous path-connected maps are the ""everywhere locally surjective"" functions. By postcomposing with a homeomorphism , we can think of as ""everywhere locally surjective"" maps . Then to make bijective, we need to be injective when restricted on each fiber for all (where each such fiber is dense in the domain), and vice versa ( injective when restricted to each fiber for all ). Maybe something like this could work if we can partition into disjoint classes where each contains exactly 1 element (so each contains exactly one element that gets mapped to , for each real number ), and if attains a different constant value on each . And to make ""everywhere locally surjective"" we need each to be dense in . However I doubt that such a function preserves path-connectedness; it sends any interval (or , etc.) to a set with lots of pieces of horizontal and vertical lines; if the pieces are too fragmented, we won't get path connectivity. I can imagine that if was monotone on or something, then we would see a longer vertical line piece at ; but unfortunately as I commented here this desired monotonicity is not possible. Infinitely Many Discontinuities, and Trees In working with the previous near-miss example, I got the impression that in fact MiniConjecture: one point of discontinuity of begets infinitely many points of discontinuity converging to , making such a potential discontinuous quite difficult to imagine. Unfortunately I could not prove this MiniConjecture . Attempted proof that 1 discontinuity begets infinitely many: Since we know that each maps to a path-connected set containing and , let us define : paths , which we can assume to be injective by e.g. Equivalence of Path-Connectedness and Arc-Connectedness for Hausdorff Spaces , or using Zorn's lemma (article by Jeremy Brazas) --- illustrated below in orange. and paths going from to (which we can again assume injective; and furthermore because is injective, the paths will be disjoint except touching perhaps at endpoints ) --- illustrated below in green. Now if is continuous as a function , then is in fact a homeomorphism onto its image . So its inverse is a continuous map, mapping the path-connected set to path-connected set in containing (since the path goes through those points), meaning maps to all of , implying that is in fact a continuous bijection ( and hence homeomorphism) . So if is continuous on for some , then all inside will be mapped by to a path from to ; in other words, is composed of all these paths stitched together at the endpoints. Now if I had the injective continuous path that goes through and , the fact that is the continuous image of (so a path/""ray"" with a perhaps infinite ""tail"" that may wiggle a lot, but still pass through all of the points illustrated below), intuitively must end up travelling along this ""ray"". Then one would hope to get a contradiction because if travels along this ray, then it would be forced to continue to come across the points (blue points in below pictures), and because the points are so far from , this would contradict that is a continuous path . Sadly after making many attempts in which I kept finding mistakes, I gave up trying to prove this MiniConjecture , though I suspect that it can be proven by making the above intuition/hand-waving rigorous. In any case, assuming has a discontinuity at (e.g. as I normalized above), the paths and that are guaranteed (by path-connectedness of the map ) make us have a picture that looks like what follows, with the green paths and orange paths. If indeed we have infinitely many points of discontinuity (illustrated in red) converging to from the right, then each of those points will lead to another version of this tree structure in the codomain (which I have suggested in the small circles around the red points in the codomain). All images of intervals (where "" "" refers to both "" "" and "" "") will map to such a tree. Subtracting 2 such intervals leads to general intervals which get mapped to a tree like above, with some portion (portions on the left in the above picture) cut off. But if the tree is super ""well-connected"" with threads weaving throughout the ""canopy"", I can imagine that the set remains path-connected. So perhaps the Conjecture above is false, if one can construct an injective map from to this fractal tree-like structure.","f:[0,1]\hookrightarrow \mathbb R^2 f: [0,1] \hookrightarrow \mathbb R g g(0):=(0,0) g(t):= (t,\sin(\frac 1t)) t\in (0,1] f 0 f(0)=0 \epsilon>0 y_n^\epsilon \searrow 0 f(y_n^\epsilon) \in C(0,\epsilon) [0,1] \mathbb R^2 \{\frac 1m: m \in \mathbb N^+\} i\cdot e^{-\pi/2 \cdot \frac 1m}) \epsilon=1 (\frac 1{m+1},\frac 1m] 0\in \mathbb C i\cdot e^{-\pi/2 \cdot \frac 1m} \in \mathbb C [0,1]\to \mathbb R^2 [0,a\rangle \rangle ) ] \mathbb R^2 (0,a\rangle  f:=[t\mapsto (x(t),y(t))]:[0,1] \to \mathbb R^2 x,y:[0,1]\to \mathbb R [0,1]\to \mathbb R \mathbb R \to (0,1) x,y x,y:[0,1]\to (0,1) f y x^{-1}(r) r\in \mathbb R x y^{-1}(r) r\in \mathbb R [0,1] C_\alpha C_\alpha \cap x^{-1}(r) C_\alpha r r y c_\alpha C_\alpha y C_\alpha [0,1] (a,b) [a,b) y x^{-1}(r) x=r \newcommand{\im}{\operatorname{im}} \newcommand{\harmonicint}[1]{[\frac 1{#1+1},\frac 1{#1}]} \newcommand{\harmonicintt}[1]{[1/({#1+1}),1/{#1}]} p f p f [0, \frac 1m] f(\frac 1m) f(0)=0 P_m : [-\frac 1m, 0] \to f([0, \frac 1m])\subseteq \mathbb R^2 F_m: [-\frac 1m, - \frac 1{m+1}] \to f([\frac 1{m+1},\frac 1m]) \subseteq \mathbb R^2 f(\frac 1m) f(\frac 1{m+1}) f F_m f(\frac 1m) f|_{[1/(m+1),1/m]} [\frac 1{m+1},\frac 1m]\hookrightarrow \mathbb R^2 f|_{[1/(m+1),1/m]} f(\harmonicint m) \supseteq \im(F_m) \im(F_m) \harmonicint m \frac 1{m+1},\frac 1m F_m f^{-1} \im(F_m) \subseteq f(\harmonicint m) \harmonicint m f|_{\harmonicintt m} \harmonicint m \to \im(F_m) = f(\harmonicint m) f (0,\eta) \eta>\frac 1M >0 \harmonicint m (0,\eta) f f(\frac 1{m+1}) f(\frac 1m) f F_m P_M :[-\frac 1M, 0] \hookrightarrow f([0,\frac 1M]) f(\frac 1M) f(0)=0 f((0,\frac 1M]) (0,1] f(\frac 1m) P_M P_M f(\frac 1m) f(\frac 1m) f(0)=0 P_M [-\frac 1M, 0] \hookrightarrow f([0,\frac 1M])  f:[0,1] \hookrightarrow \mathbb R^2 p\in [0,1] p=0 F_m P_m f F_m P_m p [p, p+\eta\rangle \rangle ) ] \langle p+\delta, p+\eta\rangle [0,1] %[![simplified path connected tree thingy][8]][8]","['real-analysis', 'general-topology', 'continuity', 'set-theory', 'path-connected']"
23,Adjoint to the continuity equation,Adjoint to the continuity equation,,"Consider the continuity equation on $[0,T] \times \mathbb{R}$ $$ \partial_{t}u + \partial_{x}(bu)=0$$ with initial data $u(0,x) =u_{0}(x)$ which we assume to be smooth for now (also assume $b$ is smooth). I have read in papers that the formal adjoint to the forward continuity equation is the backward transport equation, i.e. $$\partial_{t}v + b\partial_{x}v=0 $$ with final data $v(T,x) = v^{T}(x)$ . I would like to know if there is a precise way to show this? In other words how can we make sense of the backward transport equation as the adjoint of the forward continuity equation? Here is my attempt but I am not totally convinced by it. Consider $u,v \in C^{\infty}_{c}(\mathbb{R})$ for convenience and define the operator $L$ by $Lu = \partial_{t} u + \partial_{x}(bu)$ . Then we look for an operator $L^{*}$ (the adjoint) which satisfies $$ \langle Lu, v \rangle = \langle u, L^{*}v \rangle$$ where the duality bracket can be assumed to be $L^{2}([0,T]; L^{2}(\mathbb{R}))$ (i'm not actually sure why we can assume this). Then integrating by parts we see that \begin{aligned}\langle Lu, v \rangle &= \int_{t}\int_{x} v (\partial_{t}u + \partial_{x}(bu))~dxdt \\ &= - \int_{t}\int_{x} u(\partial_{t}v+b\partial_{x}v)~dxdt +  \int_{\mathbb{R}} u(x,T)v(x,T)-u(x,0)v(x,0)~dxdt. \end{aligned} So formally the forward continuity equation will be adjoint to the transport equation if the boundary term vanishes, for which we can simply require that $u(x,t):=v(x,T-t)$ , i.e. take $u$ to be the time-reversal of $v$ .  So the two equations will be adjoint if one is posed backwards in time and the other one is posed forwards in time. But this doesn't really make sense since I am defining $u$ in terms of $v$ and the duality equality should hold for any $u,v$ .","Consider the continuity equation on with initial data which we assume to be smooth for now (also assume is smooth). I have read in papers that the formal adjoint to the forward continuity equation is the backward transport equation, i.e. with final data . I would like to know if there is a precise way to show this? In other words how can we make sense of the backward transport equation as the adjoint of the forward continuity equation? Here is my attempt but I am not totally convinced by it. Consider for convenience and define the operator by . Then we look for an operator (the adjoint) which satisfies where the duality bracket can be assumed to be (i'm not actually sure why we can assume this). Then integrating by parts we see that So formally the forward continuity equation will be adjoint to the transport equation if the boundary term vanishes, for which we can simply require that , i.e. take to be the time-reversal of .  So the two equations will be adjoint if one is posed backwards in time and the other one is posed forwards in time. But this doesn't really make sense since I am defining in terms of and the duality equality should hold for any .","[0,T] \times \mathbb{R}  \partial_{t}u + \partial_{x}(bu)=0 u(0,x) =u_{0}(x) b \partial_{t}v + b\partial_{x}v=0  v(T,x) = v^{T}(x) u,v \in C^{\infty}_{c}(\mathbb{R}) L Lu = \partial_{t} u + \partial_{x}(bu) L^{*}  \langle Lu, v \rangle = \langle u, L^{*}v \rangle L^{2}([0,T]; L^{2}(\mathbb{R})) \begin{aligned}\langle Lu, v \rangle &= \int_{t}\int_{x} v (\partial_{t}u + \partial_{x}(bu))~dxdt \\ &= - \int_{t}\int_{x} u(\partial_{t}v+b\partial_{x}v)~dxdt +  \int_{\mathbb{R}} u(x,T)v(x,T)-u(x,0)v(x,0)~dxdt. \end{aligned} u(x,t):=v(x,T-t) u v u v u,v","['real-analysis', 'functional-analysis', 'partial-differential-equations', 'operator-theory']"
24,On the conditions of convergence in the generalized Riemann-Lebesgue lemma,On the conditions of convergence in the generalized Riemann-Lebesgue lemma,,"The following generalizations of the Riemann-Lebesgue lemma are rather well known (see for example the paper Kahane, C. S., Generalizations of the Riemann-Lebesgue and Cantor-Lebesgue lemmas , Czechoslovak Mathematical Journal, Vol 30 (1980), No. 1, 108-117): Theorem I: Suppose $g\in L_\infty([0,\infty),m)$ . A necessary and sufficient condition for \begin{align} I(f; g):=\lim_{\lambda\rightarrow\infty}\int^\infty_0 f(t) g(\lambda t)\,dt\tag{0}\label{zero} \end{align} to exists for every $f\in L_1([0,\infty),m)$ is that $g$ has the mean value property, i.e. \begin{align} I(g)=\lim_{T\rightarrow\infty}\frac1T\int^T_0 g(t)\,dt\tag{1}\label{one} \end{align} exists. Furthermore, when this is the case, \begin{align} I(f; g)=\Big(\int^\infty_0 f(t)\,dt\Big) I(g)\tag{2}\label{two} \end{align} This extends to $L_1(\mathbb{R},m)$ as follows: Corollary I: Suppose $g\in L_\infty(\mathbb{R},m)$ . A necessary and sufficient condition for \begin{align} I(f; g):=\lim_{\lambda\rightarrow\infty}\int_\mathbb{R} f(t) g(\lambda t)\,dt\tag{3}\label{three} \end{align} to exists for every $f\in L_1(\mathbb{R},m)$ is that $g_-(t)=g(-t)\mathbb{1}_{[0,\infty)}(t)$ and $g_+(t)=g(t)\mathbb{1}_{[0,\infty)}(t)$ have the mean value property.  If  this is the case, \begin{align} I(f; g)=\Big(\int^0_{-\infty} f(t)\,dt\Big) I(g_-) + \Big(\int^\infty_0 f(t)\,dt\Big) I(g_+)\tag{4}\label{four} \end{align} Perhaps the  most common version of  these results is the case where $g\in L_\infty(\mathbb{R},m)$ and $g$ is $P$ -periodic function ( $g(x+P)=g(x)$ for all $x$ and some $P>0$ ). Then  \eqref{four} has the form \begin{align} \lim_{\lambda\rightarrow\infty}\int_{\mathbb{R}}f(t)g(\lambda t)\,dt=\Big(\frac{1}{P}\int^P_0 g(t)\,dt\Big)\int_\mathbb{R} f(t)\,dt\tag{5}\label{five} \end{align} The assumption that $g\in L_\infty(\mathbb{R},m)$ can be relaxed by considering integrals over finite intervals and some duality assumptions: Theorem II: Suppose $g\in L^{loc}_q([0,\infty),m)$ , $q>1$ , and let $0\leq a< b<\infty$ . For the limit \begin{align}  I(f; g,[a,b]):=\lim_{\lambda\rightarrow\infty}\int^b_a f(t) g(\lambda t)\,dt\tag{6}\label{six} \end{align} to exists for every $f\in L_p([a,b],m)$ , $\frac1p+\frac1q=1$ , it is necessary and sufficient that (i) $\frac1T\int^T_0|g(t)|^q\,dt=O(1)$ as $T\rightarrow\infty$ , (ii) $g$ has the mean value property \eqref{one}. If (i) and (ii) hold, then the limit \eqref{six} takes the form \begin{align} I(f; g, [a,b])=\Big(\int^b_a f(t)\,dt\Big) I(g)\tag{7}\label{seven} \end{align} The validity of the results presented above is based on duality between $L_p$ spaces.  Holder's inequality provides uniform bounds on some linear operators,  uniform boundedness principle and density arguments then give the desired results. Question: Motivated by a recent posting , I would like to know if there is a counterexample to Theorem I if the assumption $g\in L_\infty([0,\infty),m)$ is relaxed while maintaining the integrability assumption for $f$ . To be more precise and to simplify matters, Are there measurable functions $g$ and $f$ , such that, $g$ is $1$ -periodic, $g\in L_1([0,1],m)\setminus  L_\infty([0,1],m)$ , $f\in L_1(\mathbb{R},m)$ for any $\lambda>0$ , $t\mapsto f(t) g(\lambda t)\in L_1(\mathbb{R},m),$ but either $I(f; g)=\lim_{\lambda\rightarrow\infty}\int^\infty_0 f(t) g(t\lambda)\,dt$ does not exist (as a real number), or if it does, $I(f; g)\neq I(g) \int^\infty_0 f$ . If anybody knows a counterexample or may be able to build one, that would also be fantastic!","The following generalizations of the Riemann-Lebesgue lemma are rather well known (see for example the paper Kahane, C. S., Generalizations of the Riemann-Lebesgue and Cantor-Lebesgue lemmas , Czechoslovak Mathematical Journal, Vol 30 (1980), No. 1, 108-117): Theorem I: Suppose . A necessary and sufficient condition for to exists for every is that has the mean value property, i.e. exists. Furthermore, when this is the case, This extends to as follows: Corollary I: Suppose . A necessary and sufficient condition for to exists for every is that and have the mean value property.  If  this is the case, Perhaps the  most common version of  these results is the case where and is -periodic function ( for all and some ). Then  \eqref{four} has the form The assumption that can be relaxed by considering integrals over finite intervals and some duality assumptions: Theorem II: Suppose , , and let . For the limit to exists for every , , it is necessary and sufficient that (i) as , (ii) has the mean value property \eqref{one}. If (i) and (ii) hold, then the limit \eqref{six} takes the form The validity of the results presented above is based on duality between spaces.  Holder's inequality provides uniform bounds on some linear operators,  uniform boundedness principle and density arguments then give the desired results. Question: Motivated by a recent posting , I would like to know if there is a counterexample to Theorem I if the assumption is relaxed while maintaining the integrability assumption for . To be more precise and to simplify matters, Are there measurable functions and , such that, is -periodic, , for any , but either does not exist (as a real number), or if it does, . If anybody knows a counterexample or may be able to build one, that would also be fantastic!","g\in L_\infty([0,\infty),m) \begin{align}
I(f; g):=\lim_{\lambda\rightarrow\infty}\int^\infty_0 f(t) g(\lambda t)\,dt\tag{0}\label{zero}
\end{align} f\in L_1([0,\infty),m) g \begin{align}
I(g)=\lim_{T\rightarrow\infty}\frac1T\int^T_0 g(t)\,dt\tag{1}\label{one}
\end{align} \begin{align}
I(f; g)=\Big(\int^\infty_0 f(t)\,dt\Big) I(g)\tag{2}\label{two}
\end{align} L_1(\mathbb{R},m) g\in L_\infty(\mathbb{R},m) \begin{align}
I(f; g):=\lim_{\lambda\rightarrow\infty}\int_\mathbb{R} f(t) g(\lambda t)\,dt\tag{3}\label{three}
\end{align} f\in L_1(\mathbb{R},m) g_-(t)=g(-t)\mathbb{1}_{[0,\infty)}(t) g_+(t)=g(t)\mathbb{1}_{[0,\infty)}(t) \begin{align}
I(f; g)=\Big(\int^0_{-\infty} f(t)\,dt\Big) I(g_-) + \Big(\int^\infty_0 f(t)\,dt\Big) I(g_+)\tag{4}\label{four}
\end{align} g\in L_\infty(\mathbb{R},m) g P g(x+P)=g(x) x P>0 \begin{align}
\lim_{\lambda\rightarrow\infty}\int_{\mathbb{R}}f(t)g(\lambda t)\,dt=\Big(\frac{1}{P}\int^P_0 g(t)\,dt\Big)\int_\mathbb{R} f(t)\,dt\tag{5}\label{five}
\end{align} g\in L_\infty(\mathbb{R},m) g\in L^{loc}_q([0,\infty),m) q>1 0\leq a< b<\infty \begin{align}
 I(f; g,[a,b]):=\lim_{\lambda\rightarrow\infty}\int^b_a f(t) g(\lambda t)\,dt\tag{6}\label{six}
\end{align} f\in L_p([a,b],m) \frac1p+\frac1q=1 \frac1T\int^T_0|g(t)|^q\,dt=O(1) T\rightarrow\infty g \begin{align}
I(f; g, [a,b])=\Big(\int^b_a f(t)\,dt\Big) I(g)\tag{7}\label{seven}
\end{align} L_p g\in L_\infty([0,\infty),m) f g f g 1 g\in L_1([0,1],m)\setminus  L_\infty([0,1],m) f\in L_1(\mathbb{R},m) \lambda>0 t\mapsto f(t) g(\lambda t)\in L_1(\mathbb{R},m), I(f; g)=\lim_{\lambda\rightarrow\infty}\int^\infty_0 f(t) g(t\lambda)\,dt I(f; g)\neq I(g) \int^\infty_0 f","['real-analysis', 'integration', 'analysis', 'fourier-analysis', 'lebesgue-integral']"
25,Equimeasurable functions of $p(\cdot) :[0;1] \to [1;\infty)$ and $p(\cdot\cdot) : [0;1]^2 \to [1; \infty)$.,Equimeasurable functions of  and .,p(\cdot) :[0;1] \to [1;\infty) p(\cdot\cdot) : [0;1]^2 \to [1; \infty),"Let $p(\cdot) :[0;1] \to [1;\infty)$ be a measurable function. Define $p^{*}(x)$ as the decreasing rearrangement of the measurable function $p(\cdot)$ . Hence we  know that $p(\cdot)$ and $ p^{*}(x)$ are equimeasurable to each other. ( Since $p(\cdot)$ is positive-valued). Let $p(\cdot\cdot) : [0;1]^2 \to [1; \infty)$ be a measurable function. Define $p^{*}(x,y)$ as the decreasing rearrangement of the measurable function $p(\cdot\cdot)$ . Thereby we know that $p(\cdot\cdot)$ and $p^{*}(x,y)$ are equimeasurable to each other.(Since $p(\cdot\cdot)$ is positive-valued). Define $\bar p(x,y) = p^{*}(x) + p^{*}(y)$ . There is the question: Will $\bar p(x,y)$ be equimeasurable to $p(\cdot\cdot)$ ? I'm trying to figure out something from these definitions: Note that : $(1)$ Nonnegative functions $f$ and $g$ are called equimeasurable if $\eta_f = \eta_g$ , i.e., $$m\{f \gt y \} = m\{g \gt y \} .$$ $(2)$ Functions $|f|$ and $f^{*}$ are equimeasurable. Any help would be appreciated.","Let be a measurable function. Define as the decreasing rearrangement of the measurable function . Hence we  know that and are equimeasurable to each other. ( Since is positive-valued). Let be a measurable function. Define as the decreasing rearrangement of the measurable function . Thereby we know that and are equimeasurable to each other.(Since is positive-valued). Define . There is the question: Will be equimeasurable to ? I'm trying to figure out something from these definitions: Note that : Nonnegative functions and are called equimeasurable if , i.e., Functions and are equimeasurable. Any help would be appreciated.","p(\cdot) :[0;1] \to [1;\infty) p^{*}(x) p(\cdot) p(\cdot)  p^{*}(x) p(\cdot) p(\cdot\cdot) : [0;1]^2 \to [1; \infty) p^{*}(x,y) p(\cdot\cdot) p(\cdot\cdot) p^{*}(x,y) p(\cdot\cdot) \bar p(x,y) = p^{*}(x) + p^{*}(y) \bar p(x,y) p(\cdot\cdot) (1) f g \eta_f = \eta_g m\{f \gt y \} = m\{g \gt y \} . (2) |f| f^{*}","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
26,"What conditions ensure that $\lim_{x \to 0+} \log(x) \int_0^x f(t) \, dt = 0$?",What conditions ensure that ?,"\lim_{x \to 0+} \log(x) \int_0^x f(t) \, dt = 0","In a recent post , a question arose as to the requirements on $f:[0,1] \to \mathbb{R}$ in order to ensure that $$\tag{1}\lim_{x \to 0+}\log(x) \int_0^x f(t) \, dt = 0$$ I showed that Riemann integrability of $f$ is a sufficient condition. Assuming $f$ is bounded (and all Riemann integrable functions must be bounded) we have $f(t) \leqslant M$ for some $M \geqslant 0$ and, hence, $$\tag{2}0 \leqslant \left|\log(x) \int_0^x f(t) \, dt \right|= -\log(x)\left|\int_0^x f(t) \, dt\right|\leqslant -\log(x)\int_0^x|f(t)| \, dt\leqslant -Mx\log(x)\underset{x \to 0+}\longrightarrow 0$$ My questions is what is the most general class of functions for which (1) holds? It clearly holds for some unbounded functions like $x \mapsto x^{-1/2}$ and I believe it is even true for $x \mapsto x^{-1}\sin x^{-1}$ which is not Lebesgue integrable. Furthermore, since $x \log(x) \to 0$ as $x \to 0+$ , it really is a question of the convergence or boundedness of $\frac{1}{x}\int_0^x f(t) \, dt$ and perhaps an application of the Lebesgue differentiation theorem.","In a recent post , a question arose as to the requirements on in order to ensure that I showed that Riemann integrability of is a sufficient condition. Assuming is bounded (and all Riemann integrable functions must be bounded) we have for some and, hence, My questions is what is the most general class of functions for which (1) holds? It clearly holds for some unbounded functions like and I believe it is even true for which is not Lebesgue integrable. Furthermore, since as , it really is a question of the convergence or boundedness of and perhaps an application of the Lebesgue differentiation theorem.","f:[0,1] \to \mathbb{R} \tag{1}\lim_{x \to 0+}\log(x) \int_0^x f(t) \, dt = 0 f f f(t) \leqslant M M \geqslant 0 \tag{2}0 \leqslant \left|\log(x) \int_0^x f(t) \, dt \right|= -\log(x)\left|\int_0^x f(t) \, dt\right|\leqslant -\log(x)\int_0^x|f(t)| \, dt\leqslant -Mx\log(x)\underset{x \to 0+}\longrightarrow 0 x \mapsto x^{-1/2} x \mapsto x^{-1}\sin x^{-1} x \log(x) \to 0 x \to 0+ \frac{1}{x}\int_0^x f(t) \, dt","['real-analysis', 'integration', 'lebesgue-integral', 'riemann-integration']"
27,How to prove Picard's existence and uniqueness theorem by Tonelli sequence instead of Picard sequence? For O.D.E./ODE.,How to prove Picard's existence and uniqueness theorem by Tonelli sequence instead of Picard sequence? For O.D.E./ODE.,,"$\qquad$ First of all, this question for O.D.E. comes from an end-of-book exercise with no answer. $\qquad$ Secondly, allow me to give the definitions of the relevant contents in the question to avoid ambiguity. You can also skip or skim this section until you get to the dividing line if you are familiar with it. Lipschitz condition $\qquad$ Say the function $f(x,y)$ satisfies the Lipschitz condition for $y$ on the region $G$ , if there is a constant $L$ , such that for any $(x,y_1),(x,y_2)\in G$ , there goes $|f(x,y_1)-f(x,y_2)|\le L|y_1-y_2|$ . Say the constant $L$ Lipschitz constant . Cauchy problem $\qquad$ The initial value problem $$\frac{\mathrm{d} y}{\mathrm{d} x} =f(x,y),\qquad y(x_0)=y_0,$$ $\qquad$ where the function $f(x,y)$ is continuous on the rectangle closed region $D:|x-x_0|\le a,|y-y_0|\le b$ . Picard's existence and uniqueness theorem $\qquad$ Call it Picard theorem for short. Assuming that the function $f(x,y)$ is continuous on the closed region $D$ and satisfies the Lipschitz condition for $y$ , then the solution of the Cauchy problem exists and is unique on the interval $|x-x_0|\le h$ , where $$h=\min{\left\{ a,\frac{b}{M}\right\} },\qquad M=\max_{(x,y)\in D}{|f(x,y)|}.$$ Picard sequence $$y_n(x)=y_0+\int_{x_0}^{x}f\left(s,y_{n-1}(s)\right)\mathrm{d}s,\qquad |x-x_0|\le h,$$ where $h$ is defined the same as in Picard theorem . Tonelli sequence Note: Only the right side of the initial condition is discussed. $\qquad$ On the interval $I=[x_0,x_0+h]$ , where $h$ is defined the same as in Picard theorem , construct the sequence $\{y_n(x)\}$ as follows: For each positive integer $n$ , divide the interval $I$ into $n$ parts, taking the points as $x_k=x_0+kd_n$ , where $d_n=\frac{h}{n},k=1,2,...,n$ , and then defining $$\begin{matrix}  y_n(x)=\begin{cases}  y_0, & x\in\left[x_0,x_1\right], \\  y_0+\displaystyle \int_{x_0}^{x-d_n}f\left(s,y_n(s)\right)\mathrm{d}s,  & x\in\left(x_1,x_n\right], \end{cases} & n=1,2,\cdots \end{matrix}.$$ We call the sequence $\{y_n(x)\}$ the Tonelli sequence . $\qquad$ In preparation for the exchange of integrals and limits in the later steps, one step in the proof of Picard theorem is to prove the uniform convergence of Picard sequences , using the difference $$|y_n(x)-y_{n-1}(x)|\le L\left|\int_{x_0}^{x}|y_{n-1}(s)-y_{n-2}(s)|\mathrm{d}s\right|,$$ treating the function column as a series of function terms, and applying the Weierstrass M discriminance. $\qquad$ In fact, the core of my problem is how to prove that the Tonelli sequence is uniformly convergent under the conditions of Picard theorem , which is that $f$ continuous on the closed region $D$ and has Lipschitz condition on $y$ . $\qquad$ I had a hard time doing the similar thing on the Tonelli sequence . When I take the difference and use the Lipschitz condition , I don't get a recursive inequality, but I get an inequality involving integration between B and B itself. $$\begin{align}|y_n(x)-y_{n-1}(x)| & = \left | \int_{x_0}^{x-d_n}f\left ( s,y_n(s) \right )\mathrm{d}s - \int_{x_0}^{x-d_{n-1}}f\left ( s,y_{n-1}(s) \right )\mathrm{d}s  \right | \\&=\left | \int_{x_0}^{x-d_{n-1}}\Big ( f\big ( s,y_n(s) \big )-f\big ( s,y_{n-1}(s) \big ) \Big )\mathrm{d}s + \int_{x-d_{n-1}}^{x-d_{n}}f\left ( s,y_n(s) \right )\mathrm{d}s  \right | \\&\le \left |\int_{x_0}^{x-d_{n-1}}L\left|y_n(x)-y_{n-1}(x)\right |\mathrm{d}s\right |+M|d_{n-1}-d_n|\\&= L \left |\int_{x_0}^{x-d_{n-1}}\left|y_n(x)-y_{n-1}(x)\right |\mathrm{d}s\right |+\frac{Mh}{n(n-1)}\end{align}$$ $\qquad$ In addition, I know the Arzela-Ascoli theorem, and I also learned that Tonelli sequence have uniform boundedness and  equicontinuity, and it seems that equicontinuity plus point-by-point convergence can lead to uniform convergence (sorry but idk if it is right), but in the process I still face problems like the above. It seems that any attempt to apply the Lipschitz condition would create such problems making the proof could not continue. $\qquad$ I don't know if I'm thinking in the right direction. If you know how to prove it or anything related to it, please discuss below, tks!","First of all, this question for O.D.E. comes from an end-of-book exercise with no answer. Secondly, allow me to give the definitions of the relevant contents in the question to avoid ambiguity. You can also skip or skim this section until you get to the dividing line if you are familiar with it. Lipschitz condition Say the function satisfies the Lipschitz condition for on the region , if there is a constant , such that for any , there goes . Say the constant Lipschitz constant . Cauchy problem The initial value problem where the function is continuous on the rectangle closed region . Picard's existence and uniqueness theorem Call it Picard theorem for short. Assuming that the function is continuous on the closed region and satisfies the Lipschitz condition for , then the solution of the Cauchy problem exists and is unique on the interval , where Picard sequence where is defined the same as in Picard theorem . Tonelli sequence Note: Only the right side of the initial condition is discussed. On the interval , where is defined the same as in Picard theorem , construct the sequence as follows: For each positive integer , divide the interval into parts, taking the points as , where , and then defining We call the sequence the Tonelli sequence . In preparation for the exchange of integrals and limits in the later steps, one step in the proof of Picard theorem is to prove the uniform convergence of Picard sequences , using the difference treating the function column as a series of function terms, and applying the Weierstrass M discriminance. In fact, the core of my problem is how to prove that the Tonelli sequence is uniformly convergent under the conditions of Picard theorem , which is that continuous on the closed region and has Lipschitz condition on . I had a hard time doing the similar thing on the Tonelli sequence . When I take the difference and use the Lipschitz condition , I don't get a recursive inequality, but I get an inequality involving integration between B and B itself. In addition, I know the Arzela-Ascoli theorem, and I also learned that Tonelli sequence have uniform boundedness and  equicontinuity, and it seems that equicontinuity plus point-by-point convergence can lead to uniform convergence (sorry but idk if it is right), but in the process I still face problems like the above. It seems that any attempt to apply the Lipschitz condition would create such problems making the proof could not continue. I don't know if I'm thinking in the right direction. If you know how to prove it or anything related to it, please discuss below, tks!","\qquad \qquad \qquad f(x,y) y G L (x,y_1),(x,y_2)\in G |f(x,y_1)-f(x,y_2)|\le L|y_1-y_2| L \qquad \frac{\mathrm{d} y}{\mathrm{d} x} =f(x,y),\qquad y(x_0)=y_0, \qquad f(x,y) D:|x-x_0|\le a,|y-y_0|\le b \qquad f(x,y) D y |x-x_0|\le h h=\min{\left\{ a,\frac{b}{M}\right\} },\qquad M=\max_{(x,y)\in D}{|f(x,y)|}. y_n(x)=y_0+\int_{x_0}^{x}f\left(s,y_{n-1}(s)\right)\mathrm{d}s,\qquad |x-x_0|\le h, h \qquad I=[x_0,x_0+h] h \{y_n(x)\} n I n x_k=x_0+kd_n d_n=\frac{h}{n},k=1,2,...,n \begin{matrix}
 y_n(x)=\begin{cases}
 y_0, & x\in\left[x_0,x_1\right], \\
 y_0+\displaystyle \int_{x_0}^{x-d_n}f\left(s,y_n(s)\right)\mathrm{d}s,  & x\in\left(x_1,x_n\right],
\end{cases} & n=1,2,\cdots
\end{matrix}. \{y_n(x)\} \qquad |y_n(x)-y_{n-1}(x)|\le L\left|\int_{x_0}^{x}|y_{n-1}(s)-y_{n-2}(s)|\mathrm{d}s\right|, \qquad f D y \qquad \begin{align}|y_n(x)-y_{n-1}(x)| & = \left | \int_{x_0}^{x-d_n}f\left ( s,y_n(s) \right )\mathrm{d}s - \int_{x_0}^{x-d_{n-1}}f\left ( s,y_{n-1}(s) \right )\mathrm{d}s  \right | \\&=\left | \int_{x_0}^{x-d_{n-1}}\Big ( f\big ( s,y_n(s) \big )-f\big ( s,y_{n-1}(s) \big ) \Big )\mathrm{d}s + \int_{x-d_{n-1}}^{x-d_{n}}f\left ( s,y_n(s) \right )\mathrm{d}s  \right | \\&\le \left |\int_{x_0}^{x-d_{n-1}}L\left|y_n(x)-y_{n-1}(x)\right |\mathrm{d}s\right |+M|d_{n-1}-d_n|\\&= L \left |\int_{x_0}^{x-d_{n-1}}\left|y_n(x)-y_{n-1}(x)\right |\mathrm{d}s\right |+\frac{Mh}{n(n-1)}\end{align} \qquad \qquad","['real-analysis', 'ordinary-differential-equations', 'lipschitz-functions', 'cauchy-problem']"
28,"Do there exist countinuously differentiable functions $f,g$ such that $f(x)g(x) = x$ and $f(0) = g(0) = 0$?",Do there exist countinuously differentiable functions  such that  and ?,"f,g f(x)g(x) = x f(0) = g(0) = 0","Do there exist continuously differentiable functions $f,g : \mathbb{R} \rightarrow \mathbb{R}$ such that $f(0) = g(0) = 0$ and for every $x {\in} \mathbb{R}, \,f(x)g(x) = x$ ? My solution: No. Since $f$ and $g$ are differentiable functions and $f(x)g(x) = x$ , we have $f'(0)g(0) + f(0)g'(0)= 1$ , which is not possible if $f(0) = g(0) = 0$ . Why does the exercise specify that $f$ and $g$ must be CONTINUOUSLY differentiable? It seems weird to add that if we don't need it; did I miss something?","Do there exist continuously differentiable functions such that and for every ? My solution: No. Since and are differentiable functions and , we have , which is not possible if . Why does the exercise specify that and must be CONTINUOUSLY differentiable? It seems weird to add that if we don't need it; did I miss something?","f,g : \mathbb{R} \rightarrow \mathbb{R} f(0) = g(0) = 0 x {\in} \mathbb{R}, \,f(x)g(x) = x f g f(x)g(x) = x f'(0)g(0) + f(0)g'(0)= 1 f(0) = g(0) = 0 f g","['real-analysis', 'derivatives']"
29,How can I improve my proof of Stirling's Theorem?,How can I improve my proof of Stirling's Theorem?,,"I'm trying to prove Robbin's inequality: $$ n! \le \sqrt{2 \pi n}(n/e)^n e^{1/(12n)}. $$ Step 1 : I start from the integral formulation \begin{align} n! = \int_0^\infty x^n e^{-x} dx &= (n/e)^n\int_0^\infty (x/n)^n e^{n-x} dx \\&= (n/e)^n\int_{-n}^\infty (1+x/n)^n e^{-x} dx \\&= (n/e)^n\int_{-n}^\infty \exp[-x + n\log(1+x/n)] dx \\&= (n/e)^n n \int_{-1}^\infty \exp[-x n + n\log(1+x)] dx. \end{align} Step 2 : I want to use Laplace's method, so I pull out Taylor's theorem $$ \log(1+x)=x - x^2/2 + \frac{2}{(1 + \xi_x)^3}x^3/6, $$ where $\xi_x \in [-x,x]$ . (Here we used $\frac{d^3\log(1+x)}{dx^3}=\frac{2}{(1+x)^3}$ .) Step 3 : I split the integral into segments: $[-1, 0]$ , $[0, L]$ and $[L, \infty)$ . In the first interval I guess I should use $\xi_x=-1$ , but actually we know $\log(1+x)\le x - x^2/2$ , so we don't really need Taylor's theorem. (Good, since we would have had a zero divisor.) In the interval $[0,L]$ we set $\xi_x=0$ since $x^3>0$ . Finally in $[L,\infty)$ in use a simple linear bound: $$\log(1+x) \le \log(1+L) + \frac{x-L}{1+L}$$ since $\log(1+x)$ is a concave function. (We have $f(x) \le f(L) + (x-L)f'(L)$ for any concave function and constant $L$ .) Step 4 : I now do the integrals: Integral one from $-1$ to $0$ : \begin{align} \int_{-1}^0 \exp[-x n + n\log(1+x)] dx &\le \int_{-1}^0 \exp[-nx^2/2] dx \end{align} Integral two from $0$ to $L$ : \begin{align} \int_{0}^L \exp[-x n + n\log(1+x)] dx &\le \int_{0}^L \exp[-nx^2/2 + nL^3/3] dx \\&= e^{nL^3/3} \int_{0}^L \exp[-nx^2/2] dx \end{align} Integral from $L$ to $\infty$ : \begin{align} \int_{L}^\infty \exp[-x n + n\log(1+x)] dx &\le\int_{L}^\infty \exp[n \log(1+L) - n(1+x)L/(1+L)] dx \\&\le(1+L)^n \int_{L}^\infty \exp[- n(1+x)L/(1+L)] dx \\&=(1+L)^n e^{-L n} \frac{1+L}{L n} \\&\le \exp(-n L^2/2 + n L^3/3) \frac{1+L}{L n}. \end{align} Step 5 : Combine the integrals. First combining (1) and (2): \begin{align} \int_{-1}^0 \exp[-nx^2/2] dx +e^{nL^3/3} \int_{0}^L \exp[-nx^2/2] dx &\le e^{nL^3/3} \int_{-1}^L \exp[-nx^2/2] dx \\&\le e^{nL^3/3} \int_{-\infty}^\infty \exp[-nx^2/2] dx \\&= e^{nL^3/3} \sqrt{2\pi/n}. \end{align} Then combining (1) and (2) with (3): \begin{align} \int_{-1}^\infty \exp[-x n + n\log(1+x)] dx \le e^{nL^3/3}\left(\sqrt{2\pi/n} + \exp(-n L^2/2) \frac{1+L}{L n}\right) \end{align} Step 6 : Choose $L$ . I choose $L=n^{-2/3}$ . Putting it all together we have shown $$ n! \le (n/e)^n \left( \sqrt{2\pi n} +  (1+n^{2/3})e^{-n^{1/3}/2}\right) e^{1/(3n)}. $$ That is nearly the $(n/e)^n \sqrt{2\pi n} e^{1/(12n)}$ bound we wanted, but it is off by a factor $1/4$ in the error exponent, and we have that extra annoying $o(1)$ term added onto $\sqrt{2\pi n}$ . Question : How can I improve my proof? I would like to get the full strength of Robbins's inequality, ideally  with similar methods. If there are any tricks I can use to make the derivation easier, I'm also very interested.","I'm trying to prove Robbin's inequality: Step 1 : I start from the integral formulation Step 2 : I want to use Laplace's method, so I pull out Taylor's theorem where . (Here we used .) Step 3 : I split the integral into segments: , and . In the first interval I guess I should use , but actually we know , so we don't really need Taylor's theorem. (Good, since we would have had a zero divisor.) In the interval we set since . Finally in in use a simple linear bound: since is a concave function. (We have for any concave function and constant .) Step 4 : I now do the integrals: Integral one from to : Integral two from to : Integral from to : Step 5 : Combine the integrals. First combining (1) and (2): Then combining (1) and (2) with (3): Step 6 : Choose . I choose . Putting it all together we have shown That is nearly the bound we wanted, but it is off by a factor in the error exponent, and we have that extra annoying term added onto . Question : How can I improve my proof? I would like to get the full strength of Robbins's inequality, ideally  with similar methods. If there are any tricks I can use to make the derivation easier, I'm also very interested.","
n! \le \sqrt{2 \pi n}(n/e)^n e^{1/(12n)}.
 \begin{align}
n! = \int_0^\infty x^n e^{-x} dx
&=
(n/e)^n\int_0^\infty (x/n)^n e^{n-x} dx
\\&=
(n/e)^n\int_{-n}^\infty (1+x/n)^n e^{-x} dx
\\&=
(n/e)^n\int_{-n}^\infty \exp[-x + n\log(1+x/n)] dx
\\&=
(n/e)^n n \int_{-1}^\infty \exp[-x n + n\log(1+x)] dx.
\end{align} 
\log(1+x)=x - x^2/2 + \frac{2}{(1 + \xi_x)^3}x^3/6,
 \xi_x \in [-x,x] \frac{d^3\log(1+x)}{dx^3}=\frac{2}{(1+x)^3} [-1, 0] [0, L] [L, \infty) \xi_x=-1 \log(1+x)\le x - x^2/2 [0,L] \xi_x=0 x^3>0 [L,\infty) \log(1+x) \le \log(1+L) + \frac{x-L}{1+L} \log(1+x) f(x) \le f(L) + (x-L)f'(L) L -1 0 \begin{align}
\int_{-1}^0 \exp[-x n + n\log(1+x)] dx
&\le \int_{-1}^0 \exp[-nx^2/2] dx
\end{align} 0 L \begin{align}
\int_{0}^L \exp[-x n + n\log(1+x)] dx
&\le \int_{0}^L \exp[-nx^2/2 + nL^3/3] dx
\\&= e^{nL^3/3} \int_{0}^L \exp[-nx^2/2] dx
\end{align} L \infty \begin{align}
\int_{L}^\infty \exp[-x n + n\log(1+x)] dx
&\le\int_{L}^\infty \exp[n \log(1+L) - n(1+x)L/(1+L)] dx
\\&\le(1+L)^n \int_{L}^\infty \exp[- n(1+x)L/(1+L)] dx
\\&=(1+L)^n e^{-L n} \frac{1+L}{L n}
\\&\le \exp(-n L^2/2 + n L^3/3) \frac{1+L}{L n}.
\end{align} \begin{align}
\int_{-1}^0 \exp[-nx^2/2] dx
+e^{nL^3/3} \int_{0}^L \exp[-nx^2/2] dx
&\le e^{nL^3/3} \int_{-1}^L \exp[-nx^2/2] dx
\\&\le e^{nL^3/3} \int_{-\infty}^\infty \exp[-nx^2/2] dx
\\&= e^{nL^3/3} \sqrt{2\pi/n}.
\end{align} \begin{align}
\int_{-1}^\infty \exp[-x n + n\log(1+x)] dx
\le e^{nL^3/3}\left(\sqrt{2\pi/n} + \exp(-n L^2/2) \frac{1+L}{L n}\right)
\end{align} L L=n^{-2/3} 
n! \le (n/e)^n \left(
\sqrt{2\pi n} +  (1+n^{2/3})e^{-n^{1/3}/2}\right) e^{1/(3n)}.
 (n/e)^n \sqrt{2\pi n} e^{1/(12n)} 1/4 o(1) \sqrt{2\pi n}","['real-analysis', 'inequality', 'definite-integrals', 'gamma-function', 'laplace-method']"
30,Exploring the continuous nowhere differentiable function $g(x) = \sum_{n=0}^{\infty} \frac{\cos {2^n x}}{2^n}$,Exploring the continuous nowhere differentiable function,g(x) = \sum_{n=0}^{\infty} \frac{\cos {2^n x}}{2^n},"I am self-learning Real Analysis from the text Understanding Analysis by Stephen Abbott. I would like someone to verify if my proof for the below exercise problem on a continuous nowhere differentiable function is correct. [Abbott 6.4.3] (a) Show that: \begin{equation*} g( x) =\sum _{n=0}^{\infty }\frac{\cos\left( 2^{n} x\right)}{2^{n}} \end{equation*} is continuous on all of $\displaystyle \mathbf{R}$ . Proof. Since, $\displaystyle ( \exists M_{n})$ such that \begin{equation*} |g_{n}( x) |=\left| \frac{\cos\left( 2^{n} x\right)}{2^{n}}\right| \leq \frac{1}{2^{n}} =M_{n} \end{equation*} and $\displaystyle \sum _{n=1}^{\infty } M_{n}$ converges, by the Weierstrass M-Test, $\displaystyle \sum _{n=1}^{\infty } g_{n}$ converges uniformly on $\displaystyle \mathbf{R}$ . Since each $\displaystyle g_{n}( x)$ is continuous on $\displaystyle \mathbf{R}$ , by the term-by-term Continuity theorem, $\displaystyle \sum _{n=1}^{\infty } g_{n}( x)$ is continuous on $\displaystyle \mathbf{R}$ . (b) The function $\displaystyle g$ was cited in section 5.4 as an example of a continuous nowhere differentiable function. What happens if we try to use the theorem 6.4.3 to explore whether $\displaystyle g$ is differentiable? Proof. Let \begin{equation*} g_{n}( x) =\frac{\cos\left( 2^{n} x\right)}{2^{n}} \end{equation*} So, \begin{equation*} g_{n} '( x) =-\sin\left( 2^{n} x\right) \end{equation*} Since $\displaystyle g$ is nowhere differentiable, by the contrapositive of the term-by-term differentiability theorem, we have that: If $\displaystyle g$ is not differentiable on $\displaystyle A$ , then either $\displaystyle \sum g_{n}( x)$ converges NOT pointwise for all $\displaystyle x\in A$ , or $\displaystyle \sum g_{n} '( x)$ converges NOT uniformly on $\displaystyle A$ . Since $\displaystyle \sum g_{n}$ converges uniformly on $\displaystyle \mathbf{R}$ , the only possibility is that $\displaystyle \sum _{n=1}^{\infty } g_{n} '=\sum -\sin\left( 2^{n} x\right)$ does not converge uniformly on $\displaystyle \mathbf{R}$ . The question I have is, are we actually required prove this result? And how to go about proving it anyway? Does the below check out? If a function $\sum h_n$ is uniformly convergent on $A$ , it is uniformly convergent $(\forall S) \subseteq A$ . If $(\exists S \subseteq A)$ where $\sum h_n$ converges NOT uniformly, then $\sum h_n$ converges NOT uniformly on $A$ . Consider the point $x_0 = \frac{\pi}{3}$ . The sequence $g_n'(x_0)=(-1)^n \frac{\sqrt{3}}{2}$ . Thus, $\lim g_n'(x_0) \neq 0$ . Consequently, by the $n$ th term test, $\sum g_n'(x_0)$ does not converge pointwise on any interval containing $x_0 = \frac{\pi}{3}$ . Therefore, $\sum g_n'$ converges NOT uniformly on $\mathbf{R}$ .","I am self-learning Real Analysis from the text Understanding Analysis by Stephen Abbott. I would like someone to verify if my proof for the below exercise problem on a continuous nowhere differentiable function is correct. [Abbott 6.4.3] (a) Show that: is continuous on all of . Proof. Since, such that and converges, by the Weierstrass M-Test, converges uniformly on . Since each is continuous on , by the term-by-term Continuity theorem, is continuous on . (b) The function was cited in section 5.4 as an example of a continuous nowhere differentiable function. What happens if we try to use the theorem 6.4.3 to explore whether is differentiable? Proof. Let So, Since is nowhere differentiable, by the contrapositive of the term-by-term differentiability theorem, we have that: If is not differentiable on , then either converges NOT pointwise for all , or converges NOT uniformly on . Since converges uniformly on , the only possibility is that does not converge uniformly on . The question I have is, are we actually required prove this result? And how to go about proving it anyway? Does the below check out? If a function is uniformly convergent on , it is uniformly convergent . If where converges NOT uniformly, then converges NOT uniformly on . Consider the point . The sequence . Thus, . Consequently, by the th term test, does not converge pointwise on any interval containing . Therefore, converges NOT uniformly on .","\begin{equation*}
g( x) =\sum _{n=0}^{\infty }\frac{\cos\left( 2^{n} x\right)}{2^{n}}
\end{equation*} \displaystyle \mathbf{R} \displaystyle ( \exists M_{n}) \begin{equation*}
|g_{n}( x) |=\left| \frac{\cos\left( 2^{n} x\right)}{2^{n}}\right| \leq \frac{1}{2^{n}} =M_{n}
\end{equation*} \displaystyle \sum _{n=1}^{\infty } M_{n} \displaystyle \sum _{n=1}^{\infty } g_{n} \displaystyle \mathbf{R} \displaystyle g_{n}( x) \displaystyle \mathbf{R} \displaystyle \sum _{n=1}^{\infty } g_{n}( x) \displaystyle \mathbf{R} \displaystyle g \displaystyle g \begin{equation*}
g_{n}( x) =\frac{\cos\left( 2^{n} x\right)}{2^{n}}
\end{equation*} \begin{equation*}
g_{n} '( x) =-\sin\left( 2^{n} x\right)
\end{equation*} \displaystyle g \displaystyle g \displaystyle A \displaystyle \sum g_{n}( x) \displaystyle x\in A \displaystyle \sum g_{n} '( x) \displaystyle A \displaystyle \sum g_{n} \displaystyle \mathbf{R} \displaystyle \sum _{n=1}^{\infty } g_{n} '=\sum -\sin\left( 2^{n} x\right) \displaystyle \mathbf{R} \sum h_n A (\forall S) \subseteq A (\exists S \subseteq A) \sum h_n \sum h_n A x_0 = \frac{\pi}{3} g_n'(x_0)=(-1)^n \frac{\sqrt{3}}{2} \lim g_n'(x_0) \neq 0 n \sum g_n'(x_0) x_0 = \frac{\pi}{3} \sum g_n' \mathbf{R}","['real-analysis', 'sequences-and-series', 'solution-verification', 'uniform-convergence', 'sequence-of-function']"
31,"Trouble with ""very simple"" lemma from Kenig.","Trouble with ""very simple"" lemma from Kenig.",,"In Carlos Kenig's 1986 paper Elliptic Boundary Value Problems on Lipschitz Domains , he uses (but does not prove) the following result which he says is ""very simple"". Fix $n\geq 2$ . Let $\lambda$ be the function satisfying $\lambda(0)=0$ and $\lambda'(t) = \frac{1}{(1+t^2)^{n/2}}$ . Let $\varphi:\mathbb{R}^{n-1}\to \mathbb{R}$ be a Lipschitz function and let $f\in C^\infty_c(\mathbb{R}^{n-1})$ . Then he claims that \begin{align*}  &\lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{(\|x-y\|^2+|\varphi(x)-\varphi(y)|^2)^{n/2}} f(x)  \, dx\\  &= - \lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{1}{\|x-y\|^{n-1}}\lambda\left( \frac{\varphi(x)-\varphi(y)}{\|x-y\|} \right) \nabla f(x)\cdot(y-x) \, dx \qquad(*) \end{align*} for each point $y\in \mathbb{R}^{n-1}$ where $\varphi$ is differentiable. This looks scary, not simple really, but I gave it a shot before asking here. Let $\epsilon>0$ and let $y\in \mathbb{R}^{n-1}$ be such that $\nabla\varphi(y)$ exists. Then \begin{align*} &\int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{(\|x-y\|^2+|\varphi(x)-\varphi(y)|^2)^{n/2}} f(x)  \, dx \\ &=  \int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{\|x-y\|^n\left(1+\left(\frac{\varphi(x)-\varphi(y)}{\|x-y\|}\right)^2\right)^{n/2}} f(x)  \, dx \\ &= \int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{\|x-y\|}\frac{1}{\|x-y\|^{n-1}}\lambda'\left(\frac{\varphi(x)-\varphi(y)}{\|x-y\|}\right) f(x)  \, dx. \end{align*} With $y$ fixed, we define a function $g$ on $\mathbb{R}^{n-1}\setminus\{y\}$ by $$ g(x) = \frac{\varphi(x)-\varphi(y)}{\|x-y\|}. $$ Notice that for almost every $x\in \mathbb{R}^{n-1}$ we have $$ \partial _jg(x) = \frac{\partial_j\varphi(x)\|x-y\|-(\varphi(x)-\varphi(y))\frac{x_j-y_j}{\|x-y\|}}{\|x-y\|^2} = \frac{\partial_i\varphi(x)}{\|x-y\|} - (\varphi(x)-\varphi(y))\frac{x_j-y_j}{\|x-y\|^3} $$ which implies $$ \nabla g(x)\cdot(x-y) = \frac{\nabla\varphi(x)}{\|x-y\|}\cdot(x-y) - (\varphi(x)-\varphi(y))\frac{\|x-y\|^2}{\|x-y\|^3} = \frac{\nabla \varphi(x)(x-y) - \varphi(x) + \varphi(y)}{\|x-y\|}. $$ Hence $$ \nabla(\lambda\circ g)(x)\cdot(y-x) = \lambda'\left(\frac{\varphi(x)-\varphi(y)}{\|x-y\|}\right)\frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{\|x-y\|} $$ by Chain Rule. Thus the left hand side of $(*)$ is $$ \lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{1}{\|x-y\|^{n-1}}\nabla(\lambda\circ g)(x)\cdot(y-x) f(x)  \, dx  $$ and the right hand side is $$ - \lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{1}{\|x-y\|^{n-1}}(\lambda\circ g)(x)  \nabla f(x)\cdot(y-x) \, dx. $$ However, I am now stuck. I am guessing there should be an integration by parts to make the derivative on $\lambda\circ g$ disappear, get the negative sign and get a derivative on $f$ . I can't make this work. Also I don't think I have used the fact that $\lambda(0)=0$ or that $\varphi$ is differentiable at $y$ yet.","In Carlos Kenig's 1986 paper Elliptic Boundary Value Problems on Lipschitz Domains , he uses (but does not prove) the following result which he says is ""very simple"". Fix . Let be the function satisfying and . Let be a Lipschitz function and let . Then he claims that for each point where is differentiable. This looks scary, not simple really, but I gave it a shot before asking here. Let and let be such that exists. Then With fixed, we define a function on by Notice that for almost every we have which implies Hence by Chain Rule. Thus the left hand side of is and the right hand side is However, I am now stuck. I am guessing there should be an integration by parts to make the derivative on disappear, get the negative sign and get a derivative on . I can't make this work. Also I don't think I have used the fact that or that is differentiable at yet.","n\geq 2 \lambda \lambda(0)=0 \lambda'(t) = \frac{1}{(1+t^2)^{n/2}} \varphi:\mathbb{R}^{n-1}\to \mathbb{R} f\in C^\infty_c(\mathbb{R}^{n-1}) \begin{align*} 
&\lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{(\|x-y\|^2+|\varphi(x)-\varphi(y)|^2)^{n/2}} f(x)  \, dx\\ 
&= - \lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{1}{\|x-y\|^{n-1}}\lambda\left( \frac{\varphi(x)-\varphi(y)}{\|x-y\|} \right) \nabla f(x)\cdot(y-x) \, dx \qquad(*)
\end{align*} y\in \mathbb{R}^{n-1} \varphi \epsilon>0 y\in \mathbb{R}^{n-1} \nabla\varphi(y) \begin{align*}
&\int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{(\|x-y\|^2+|\varphi(x)-\varphi(y)|^2)^{n/2}} f(x)  \, dx \\
&= 
\int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{\|x-y\|^n\left(1+\left(\frac{\varphi(x)-\varphi(y)}{\|x-y\|}\right)^2\right)^{n/2}} f(x)  \, dx \\
&= \int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{\|x-y\|}\frac{1}{\|x-y\|^{n-1}}\lambda'\left(\frac{\varphi(x)-\varphi(y)}{\|x-y\|}\right) f(x)  \, dx.
\end{align*} y g \mathbb{R}^{n-1}\setminus\{y\}  g(x) = \frac{\varphi(x)-\varphi(y)}{\|x-y\|}.  x\in \mathbb{R}^{n-1}  \partial _jg(x) = \frac{\partial_j\varphi(x)\|x-y\|-(\varphi(x)-\varphi(y))\frac{x_j-y_j}{\|x-y\|}}{\|x-y\|^2} = \frac{\partial_i\varphi(x)}{\|x-y\|} - (\varphi(x)-\varphi(y))\frac{x_j-y_j}{\|x-y\|^3}   \nabla g(x)\cdot(x-y) = \frac{\nabla\varphi(x)}{\|x-y\|}\cdot(x-y) - (\varphi(x)-\varphi(y))\frac{\|x-y\|^2}{\|x-y\|^3} = \frac{\nabla \varphi(x)(x-y) - \varphi(x) + \varphi(y)}{\|x-y\|}.   \nabla(\lambda\circ g)(x)\cdot(y-x) = \lambda'\left(\frac{\varphi(x)-\varphi(y)}{\|x-y\|}\right)\frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{\|x-y\|}  (*)  \lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{1}{\|x-y\|^{n-1}}\nabla(\lambda\circ g)(x)\cdot(y-x) f(x)  \, dx    - \lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{1}{\|x-y\|^{n-1}}(\lambda\circ g)(x)  \nabla f(x)\cdot(y-x) \, dx.  \lambda\circ g f \lambda(0)=0 \varphi y","['real-analysis', 'harmonic-analysis', 'singular-integrals']"
32,"Is there always a subsequence, whose intersection has a positive measure?","Is there always a subsequence, whose intersection has a positive measure?",,"I have the following problem: A number $0<C<1$ is given. On the interval $[0,1]$ there is a sequence of compacts with measures at least $C$ . Is it true that there is always a subsequence, whose intersection has a positive measure? I took some examples: $$a_n = \left[\frac{1}{2n+1}, \frac{1}{2n+1}+\frac{1}{2}\right]$$ The measure here is Lebesgue measure, that means: $\mu([a, b]) = b-a$ , if $b \geq a$ $\mu([a, b]) = 0$ , if $b \leq a$ Here $C = \frac{1}{2}$ . But it seems here the intersection of each subsequence has the positive measure. How can I solve this problem?","I have the following problem: A number is given. On the interval there is a sequence of compacts with measures at least . Is it true that there is always a subsequence, whose intersection has a positive measure? I took some examples: The measure here is Lebesgue measure, that means: , if , if Here . But it seems here the intersection of each subsequence has the positive measure. How can I solve this problem?","0<C<1 [0,1] C a_n = \left[\frac{1}{2n+1}, \frac{1}{2n+1}+\frac{1}{2}\right] \mu([a, b]) = b-a b \geq a \mu([a, b]) = 0 b \leq a C = \frac{1}{2}","['real-analysis', 'functional-analysis']"
33,Finding a smooth curve $\alpha: \Bbb R\to \Bbb R^n$ such that $\overline{\alpha(\Bbb R)} = \Bbb R^n$,Finding a smooth curve  such that,\alpha: \Bbb R\to \Bbb R^n \overline{\alpha(\Bbb R)} = \Bbb R^n,"I was reviewing my notes from a Differential Geometry course I took last year, and I started thinking of the following problem $\color{blue}{^1}$ . Let $n \ge 2$ . Does there exist a smooth curve $\alpha: \Bbb R\to \Bbb R^n$ satisfying $\overline{\alpha(\Bbb R)} = \Bbb R^n$ , i.e., the trajectory of the curve is dense in $\Bbb R^n$ ? It is not immediately clear that such a curve exists in the first place, but an explicit construction shall take that problem off our hands. Some Thoughts: Since $\Bbb R^n$ is separable, let us choose a countable dense subset $S:= \{x_k\}_{k\in \Bbb Z}$ of $\Bbb R^n$ . For every $k\in \Bbb Z$ , define $\alpha$ on the closed interval $[k,k+1]$ as the segment joining $x_k$ to $x_{k+1}$ . Then, the trajectory of $\alpha$ is dense in $\Bbb R^n$ , but the curve obtained is not necessarily smooth. Perhaps there is some way to modify the $\alpha$ so obtained, to ""smoothen"" it around the corners? Is the problem easier to solve when $n = 2$ ? Maybe we can take $n = 2$ as our guiding light to generalize to higher dimensions. I'd appreciate any help! Thanks a lot. P.S. By a ""curve"", I really mean a regular curve, i.e., $\alpha'(t) \ne 0$ for all $t\in \Bbb R$ , but please feel free to drop this assumption if that is really needed. Furthermore, it would be good if we can prevent the curve from intersecting itself. Smooth means $\mathcal C^\infty$ . $\color{blue}{1.}$ The source of this problem is my imagination , so I cannot comment on the difficulty and/or solvability of the same. It doesn't seem out of the scope of a first course in differential geometry, though.","I was reviewing my notes from a Differential Geometry course I took last year, and I started thinking of the following problem . Let . Does there exist a smooth curve satisfying , i.e., the trajectory of the curve is dense in ? It is not immediately clear that such a curve exists in the first place, but an explicit construction shall take that problem off our hands. Some Thoughts: Since is separable, let us choose a countable dense subset of . For every , define on the closed interval as the segment joining to . Then, the trajectory of is dense in , but the curve obtained is not necessarily smooth. Perhaps there is some way to modify the so obtained, to ""smoothen"" it around the corners? Is the problem easier to solve when ? Maybe we can take as our guiding light to generalize to higher dimensions. I'd appreciate any help! Thanks a lot. P.S. By a ""curve"", I really mean a regular curve, i.e., for all , but please feel free to drop this assumption if that is really needed. Furthermore, it would be good if we can prevent the curve from intersecting itself. Smooth means . The source of this problem is my imagination , so I cannot comment on the difficulty and/or solvability of the same. It doesn't seem out of the scope of a first course in differential geometry, though.","\color{blue}{^1} n \ge 2 \alpha: \Bbb R\to \Bbb R^n \overline{\alpha(\Bbb R)} = \Bbb R^n \Bbb R^n \Bbb R^n S:= \{x_k\}_{k\in \Bbb Z} \Bbb R^n k\in \Bbb Z \alpha [k,k+1] x_k x_{k+1} \alpha \Bbb R^n \alpha n = 2 n = 2 \alpha'(t) \ne 0 t\in \Bbb R \mathcal C^\infty \color{blue}{1.}","['real-analysis', 'differential-geometry', 'curves']"
34,Prove Implicit Function Theorem directly from Constant Rank Theorem,Prove Implicit Function Theorem directly from Constant Rank Theorem,,"For reference: ( $\textbf{Constant Rank Theorem}$ ) Suppose $U_0\subset\mathbb{R}^m$ is open and $F:U_0\rightarrow \mathbb{R}^n$ is a $C^r$ map with constant rank $k$ (that is, its Jacobian matrix has constant rank $k$ on $U_0$ ). Then for any $p\in U_0$ there exist $C^r$ charts $(U,\phi)$ for $\mathbb{R}^m$ and $(V,\psi)$ for $\mathbb{R}^n$ , with $p\in U$ and $F(U)\subset V$ , such that $$\psi\circ F\circ \phi^{-1}(u_1,\ldots,u_k,u_{k+1},\ldots,u_m)=(u_1,\ldots,u_k,0,\ldots,0)$$ ( $\textbf{Implicit Function Theorem}$ ) Suppose $m>n$ , $U_0\subset\mathbb{R}^m$ is open and $F:U_0\rightarrow \mathbb{R}^n$ is a $C^r$ map. Let $p=(a,b)\in U_0$ , $a\in\mathbb{R}^n$ , $b\in\mathbb{R}^{m-n}$ , with $F(p)=q$ , and let the Jacobian matrix of $F$ with respect to the first $n$ coordinates of $\mathbb{R}^m$ be invertible at $p$ . Then there exist neighborhoods $N_a\subset\mathbb{R}^n$ of $a$ and $N_b\subset\mathbb{R}^{m-n}$ of $b$ and a $C^r$ function $h:N_b\rightarrow N_a$ that assigns to each $y\in N_b$ the unique $x\in N_a$ satisfying $F(x,y)=q$ . ( $\textbf{Inverse Function Theorem}$ ) Suppose $U_0\subset\mathbb{R}^n$ is open and $F:U_0\rightarrow \mathbb{R}^n$ is a $C^r$ map. Let $p\in U_0$ and let the Jacobian matrix of $F$ be invertible at $p$ . Then there exist neighborhoods $U\subset\mathbb{R}^n$ of $p$ and $V\subset\mathbb{R}^{n}$ of $F(p)$ such that $F|_U:U\rightarrow V$ is a $C^r$ diffeomorphism. I understand how to derive the Inverse Function Theorem from the Constant Rank Theorem: the rank of $F$ stays maximal on some neighborhood of $p$ . Then, applying the Constant Rank Theorem, $\psi\circ F\circ \phi^{-1}$ becomes the identity on $\phi(U)$ and we get $F|_U:U\rightarrow F(U)$ is a $C^r$ diffeomorphism, with $F(U)\subset\mathbb{R}^n$ open. My question is: how do we derive the Implicit Function Theorem directly from the Constant Rank Theorem? I do not want to go Constant Rank => Inverse => Implicit Function, since that is well-known. I know that the Jacobian matrix of $F$ with respect to the first $n$ coordinates of $\mathbb{R}^m$ stays invertible around $p$ (constant rank $k=n$ ), and applying the Constant Rank Thm. gives $$\psi\circ F\circ \phi^{-1}(u_1,\ldots,u_n,u_{n+1},\ldots,u_m)=(u_1,\ldots,u_n),$$ a projection map onto the first $n$ coordinates. But I don't see how to go from there to arrive at the implicit function $h$ . Thanks a lot!","For reference: ( ) Suppose is open and is a map with constant rank (that is, its Jacobian matrix has constant rank on ). Then for any there exist charts for and for , with and , such that ( ) Suppose , is open and is a map. Let , , , with , and let the Jacobian matrix of with respect to the first coordinates of be invertible at . Then there exist neighborhoods of and of and a function that assigns to each the unique satisfying . ( ) Suppose is open and is a map. Let and let the Jacobian matrix of be invertible at . Then there exist neighborhoods of and of such that is a diffeomorphism. I understand how to derive the Inverse Function Theorem from the Constant Rank Theorem: the rank of stays maximal on some neighborhood of . Then, applying the Constant Rank Theorem, becomes the identity on and we get is a diffeomorphism, with open. My question is: how do we derive the Implicit Function Theorem directly from the Constant Rank Theorem? I do not want to go Constant Rank => Inverse => Implicit Function, since that is well-known. I know that the Jacobian matrix of with respect to the first coordinates of stays invertible around (constant rank ), and applying the Constant Rank Thm. gives a projection map onto the first coordinates. But I don't see how to go from there to arrive at the implicit function . Thanks a lot!","\textbf{Constant Rank Theorem} U_0\subset\mathbb{R}^m F:U_0\rightarrow \mathbb{R}^n C^r k k U_0 p\in U_0 C^r (U,\phi) \mathbb{R}^m (V,\psi) \mathbb{R}^n p\in U F(U)\subset V \psi\circ F\circ \phi^{-1}(u_1,\ldots,u_k,u_{k+1},\ldots,u_m)=(u_1,\ldots,u_k,0,\ldots,0) \textbf{Implicit Function Theorem} m>n U_0\subset\mathbb{R}^m F:U_0\rightarrow \mathbb{R}^n C^r p=(a,b)\in U_0 a\in\mathbb{R}^n b\in\mathbb{R}^{m-n} F(p)=q F n \mathbb{R}^m p N_a\subset\mathbb{R}^n a N_b\subset\mathbb{R}^{m-n} b C^r h:N_b\rightarrow N_a y\in N_b x\in N_a F(x,y)=q \textbf{Inverse Function Theorem} U_0\subset\mathbb{R}^n F:U_0\rightarrow \mathbb{R}^n C^r p\in U_0 F p U\subset\mathbb{R}^n p V\subset\mathbb{R}^{n} F(p) F|_U:U\rightarrow V C^r F p \psi\circ F\circ \phi^{-1} \phi(U) F|_U:U\rightarrow F(U) C^r F(U)\subset\mathbb{R}^n F n \mathbb{R}^m p k=n \psi\circ F\circ \phi^{-1}(u_1,\ldots,u_n,u_{n+1},\ldots,u_m)=(u_1,\ldots,u_n), n h","['real-analysis', 'manifolds', 'implicit-function-theorem', 'inverse-function-theorem']"
35,What sequences do we need to prove continuity,What sequences do we need to prove continuity,,"The continuity of a real function $f$ at the point $x_0$ can be characterized with sequences as $$x_n \to x_0 \implies f(x_n) \to f(x_0) \space \space \forall (x_n)$$ But can we restrict the set of considered sequences to be the ones satisfying $$ \frac{c}{2^{n+1}} \leq |x_{n+1} - x_n| \leq \frac{C}{2^{n+1}} $$ for some positive constants $c$ and $C$ ? If any sequence converging to $x_0$ can be sparsened and/or filled to satisfy this condition, then it works. But is this true?","The continuity of a real function at the point can be characterized with sequences as But can we restrict the set of considered sequences to be the ones satisfying for some positive constants and ? If any sequence converging to can be sparsened and/or filled to satisfy this condition, then it works. But is this true?",f x_0 x_n \to x_0 \implies f(x_n) \to f(x_0) \space \space \forall (x_n)  \frac{c}{2^{n+1}} \leq |x_{n+1} - x_n| \leq \frac{C}{2^{n+1}}  c C x_0,"['real-analysis', 'sequences-and-series', 'continuity']"
36,Existence of self-Laplace transforms,Existence of self-Laplace transforms,,"There are many functions that are self-Fourier transforms, such as $e^{-\pi x^2}$ or $\frac{1}{\cosh(\pi x)}$ , and this property may be used to prove some interesting theorems such as the functional equation for the theta function or an integral relation like this . I am wondering if the same can be said of self-Laplace transforms. Are there any useful functions that are their own Laplace transform, and can this property be exploited to give any interesting consequences? Here is one example of such a function that may be constructed, but it seems artificial and of no significance: Suppose $f$ is a function of the form $f(t) = C_1t^{s-1} +C_2 t^{-s}$ , where $0<\text{Re}(s)<1$ so that the Laplace transform exists and $C_1$ and $C_2$ are some constants. We may now assume $f$ is its own Laplace transform and solve for $s$ and the constants: $$ C_1 x^{s-1} + C_2 x^{-s} =\mathcal{L}(f(t))=\int_0^{\infty} f(t) e^{-xt} \, dt = C_1\int_0^{\infty} t^{s-1} e^{-xt} \, dt + C_2 \int_0^{\infty} t^{-s} e^{-xt} \, dt $$ $$ = C_1 \Gamma(s) x^{-s} + C_2 \Gamma(1-s) x^{s-1} $$ We therefore need $C_1 =C_2 \Gamma(1-s)$ and $C_2 = C_1 \Gamma(s)$ , and so $ 1= \Gamma(s) \Gamma(1-s) = \pi \csc(\pi s)$ , which has the unique solution $s=\frac{1}{2} \pm \frac{i}{\pi} \log(\pi + \sqrt{\pi^2-1}) $ in the strip $0<\text{Re}(s)<1$ , and $\frac{C_2}{C_1}=\Gamma(s)$ . Choosing $C_1=1$ , our self-Laplace transform is $$f(t) = t^{s-1} + \Gamma(s) t^{-s}, \text{ where } s = \frac{1}{2} \pm \frac{i}{\pi} \log \left(\pi+\sqrt{\pi^2-1} \right) $$","There are many functions that are self-Fourier transforms, such as or , and this property may be used to prove some interesting theorems such as the functional equation for the theta function or an integral relation like this . I am wondering if the same can be said of self-Laplace transforms. Are there any useful functions that are their own Laplace transform, and can this property be exploited to give any interesting consequences? Here is one example of such a function that may be constructed, but it seems artificial and of no significance: Suppose is a function of the form , where so that the Laplace transform exists and and are some constants. We may now assume is its own Laplace transform and solve for and the constants: We therefore need and , and so , which has the unique solution in the strip , and . Choosing , our self-Laplace transform is","e^{-\pi x^2} \frac{1}{\cosh(\pi x)} f f(t) = C_1t^{s-1} +C_2 t^{-s} 0<\text{Re}(s)<1 C_1 C_2 f s  C_1 x^{s-1} + C_2 x^{-s} =\mathcal{L}(f(t))=\int_0^{\infty} f(t) e^{-xt} \, dt = C_1\int_0^{\infty} t^{s-1} e^{-xt} \, dt + C_2 \int_0^{\infty} t^{-s} e^{-xt} \, dt   = C_1 \Gamma(s) x^{-s} + C_2 \Gamma(1-s) x^{s-1}  C_1 =C_2 \Gamma(1-s) C_2 = C_1 \Gamma(s)  1= \Gamma(s) \Gamma(1-s) = \pi \csc(\pi s) s=\frac{1}{2} \pm \frac{i}{\pi} \log(\pi + \sqrt{\pi^2-1})  0<\text{Re}(s)<1 \frac{C_2}{C_1}=\Gamma(s) C_1=1 f(t) = t^{s-1} + \Gamma(s) t^{-s}, \text{ where } s = \frac{1}{2} \pm \frac{i}{\pi} \log \left(\pi+\sqrt{\pi^2-1} \right) ","['real-analysis', 'laplace-transform']"
37,Prove that operator between Banach spaces is continuous [duplicate],Prove that operator between Banach spaces is continuous [duplicate],,"This question already has an answer here : Weakly continuous operators are continuous (1 answer) Closed 2 years ago . Let $X$ be a Banach space and $A : X \rightarrow X$ linear operator such that for any $\phi\in X'$ operator $\phi \;\circ A$ is continuous. I want to prove that $A$ is also continuous. My work so far I wanted to use closed graph theorem (this situation is quite suitable for this theorem. We have linear operator between Banach spaces) To show that $A$ is continuous using closed graph theorem I should prove that for any $(x_n) \subset X$ , $x_n \rightarrow 0$ , $A(x_n) \rightarrow y$ we have $y = 0$ I tried some tricks, firstly, becuase $\phi \circ A$ is bounded: $$0 \le \|(\phi \circ A)(x_n)\| \le \|\phi\circ A\|\cdot \|x_n\| $$ but becuse $x_n \rightarrow 0$ then $(\phi\circ A )(x_n) \rightarrow 0$ Also because $A(x_n) \rightarrow y$ then $\phi(A(x_n)) \rightarrow \phi(y)$ (becuase $\phi$ is continuous). Out of these two facts we have that $\phi(y) = 0$ And then I tried to somehow prove that $\phi(y) = y$ but I couldn't. I also tried to rewrite somehow $\|y\| \le $ something that tends to $0$ but also I didn't figure out anything. Could you please give me a hand?","This question already has an answer here : Weakly continuous operators are continuous (1 answer) Closed 2 years ago . Let be a Banach space and linear operator such that for any operator is continuous. I want to prove that is also continuous. My work so far I wanted to use closed graph theorem (this situation is quite suitable for this theorem. We have linear operator between Banach spaces) To show that is continuous using closed graph theorem I should prove that for any , , we have I tried some tricks, firstly, becuase is bounded: but becuse then Also because then (becuase is continuous). Out of these two facts we have that And then I tried to somehow prove that but I couldn't. I also tried to rewrite somehow something that tends to but also I didn't figure out anything. Could you please give me a hand?",X A : X \rightarrow X \phi\in X' \phi \;\circ A A A (x_n) \subset X x_n \rightarrow 0 A(x_n) \rightarrow y y = 0 \phi \circ A 0 \le \|(\phi \circ A)(x_n)\| \le \|\phi\circ A\|\cdot \|x_n\|  x_n \rightarrow 0 (\phi\circ A )(x_n) \rightarrow 0 A(x_n) \rightarrow y \phi(A(x_n)) \rightarrow \phi(y) \phi \phi(y) = 0 \phi(y) = y \|y\| \le  0,"['real-analysis', 'functional-analysis', 'convergence-divergence', 'operator-theory']"
38,Do lower Riemann sums form a connected set?,Do lower Riemann sums form a connected set?,,"Let $f:[a, b] \rightarrow \mathbb{R}$ be a bounded function. The usual way of defining the Riemann integral is to take lower sums and upper sums of partitions of $[a, b]$ and going from there. However, I stumbled upon the following question: The lower sums form a bounded set, since it is bounded from above by any upper sum and bounded from below by $C(b-a)$ , where $C$ is a lower bound for $f$ on $[a, b]$ . Is this set connected? In other words, given any real number $c$ between the bounds of the lower sum, can you take a partition $P$ such that the lower sum of $f$ with respect to $P$ is $c$ ? My intuition tells me that this would hold for a continuous function, but I couldn't even prove it for the identity function on $[0, 1]$ . Is my intuition correct? If it is, can we relax any conditions on $f$ ? I would really like to work on this myself, so any hints would be appreciated.","Let be a bounded function. The usual way of defining the Riemann integral is to take lower sums and upper sums of partitions of and going from there. However, I stumbled upon the following question: The lower sums form a bounded set, since it is bounded from above by any upper sum and bounded from below by , where is a lower bound for on . Is this set connected? In other words, given any real number between the bounds of the lower sum, can you take a partition such that the lower sum of with respect to is ? My intuition tells me that this would hold for a continuous function, but I couldn't even prove it for the identity function on . Is my intuition correct? If it is, can we relax any conditions on ? I would really like to work on this myself, so any hints would be appreciated.","f:[a, b] \rightarrow \mathbb{R} [a, b] C(b-a) C f [a, b] c P f P c [0, 1] f","['real-analysis', 'connectedness', 'riemann-sum']"
39,Must a differentiable function $f$ be constant if $f'$ is always $0$ and the domain is connected?,Must a differentiable function  be constant if  is always  and the domain is connected?,f f' 0,"Let $m,n\in\Bbb N$ , with $n\ne1$ , let $A$ be a non-empty open subset of $\Bbb R^n$ , let $f\colon A\longrightarrow\Bbb R^m$ be a differentiable function, and let $C$ be a connected subset of $A$ such that, for each $p\in C$ , $f'(p)$ is the null function. Does it follow that $f$ is constant on $C$ ? In the previous question, $n\in\Bbb N\setminus\{1\}$ since the statement is clearly true when $n=1$ ; it's a classical theorem that follows from the mean value theorem for vector-valued functions . And if $C$ is open, it is also true. The function $f$ will have to be locally constant since (from which it follows that it is constant, since $C$ is connected), if $p\in C$ and $r>0$ are such that $B_r(p)\subset C$ , then, for each $q\in B_r(p)$ , the map $$\begin{array}{rccc}\hat f:&[0,1]&\longrightarrow&\Bbb R^m\\&t&\mapsto&f\bigl(tp+(1-t)q\bigr)\end{array}$$ is differentiable and $\hat f'$ is the null function. More generally, by a similar argument, the statement is true if $C$ is arcwise connected, as long as those arcs are differentiable or, at least, piecewise differentiable. But I would like to know whether or not it holds in general.","Let , with , let be a non-empty open subset of , let be a differentiable function, and let be a connected subset of such that, for each , is the null function. Does it follow that is constant on ? In the previous question, since the statement is clearly true when ; it's a classical theorem that follows from the mean value theorem for vector-valued functions . And if is open, it is also true. The function will have to be locally constant since (from which it follows that it is constant, since is connected), if and are such that , then, for each , the map is differentiable and is the null function. More generally, by a similar argument, the statement is true if is arcwise connected, as long as those arcs are differentiable or, at least, piecewise differentiable. But I would like to know whether or not it holds in general.","m,n\in\Bbb N n\ne1 A \Bbb R^n f\colon A\longrightarrow\Bbb R^m C A p\in C f'(p) f C n\in\Bbb N\setminus\{1\} n=1 C f C p\in C r>0 B_r(p)\subset C q\in B_r(p) \begin{array}{rccc}\hat f:&[0,1]&\longrightarrow&\Bbb R^m\\&t&\mapsto&f\bigl(tp+(1-t)q\bigr)\end{array} \hat f' C","['real-analysis', 'derivatives']"
40,Convergence of a Generalized Taylor Expansion,Convergence of a Generalized Taylor Expansion,,"I have been playing around with the fundamental theorem of calculus: Starting with $$f(x)=f(x_0)+\int_{x_0}^x f'(t_1) \mathrm{d}t_1, \tag{1}$$ one can apply the FTC again, this time to the derivative, to get $$f'(t_1)=f'(x_0)+\int_{x_0}^{t_1} f''(t_2) \mathrm{d}t_2. \tag{2}$$ Substituting $(2)$ in $(1)$ gives $$f(x)=f(x_0) + f'(x_0) (x-x_0)+\int_{x_0}^x \int_{x_0}^{t_1} f''(t_2) \mathrm{d}t_2\, \mathrm{d}t_1.$$ This is one way to get the classical Taylor expansion, with the remainder term here in iterated integral form. I wondered what would happen if different ""centers"" are used at every step. For example, replacing Equation $(2)$ with $$f'(t_1) = f'(x_1)+ \int_{x_1}^{t_1} f''(t_2) \mathrm{d}t_2,  $$ gives $$f(x)=f(x_0)+f'(x_1)(x-x_0)+\int_{x_0}^x \int_{x_1}^{t_1} f''(t_2) \mathrm{d}t_2 \, \mathrm{d} t_1. $$ What I got after $n$ steps was $$f(x)=P_n(x;x_0,x_1,\dots,x_n)+R_n(x;x_0,x_1,\dots,x_n),$$ where $$P_n(x;x_0,x_1,\dots,x_{n})=\sum_{k=0}^n f^{(k)}(x_k) \int_{x_0}^x \int_{x_1}^{t_1} \cdots \int_{x_{k-1}}^{t_{k-1}} \mathrm{d}t_k \, \cdots \mathrm{d}t_2 \, \mathrm{d} t_1, $$ and $$R_n(x;x_0,x_1,\dots,x_{n}) = \int_{x_0}^x \int_{x_1}^{t_1} \cdots \int_{x_{n-1}}^{t_{n-1}} \int_{x_n}^{t_n} f^{(n+1)}(t_{n+1}) \mathrm{d}t_{n+1}\,\mathrm{d}t_n \, \cdots \mathrm{d}t_2 \, \mathrm{d} t_1.$$ Some of the low-order polynomials are: $$\begin{align} P_0 &= f(x_0), \\ P_1 &= f(x_0)+f'(x_1)(x-x_0), \\ P_2 &= f(x_0)+f'(x_1)(x-x_0)+ \frac{f''(x_2)}{2}(x - x_0) (x + x_0 - 2 x_1),\\ P_3 &= P_2+ \frac{f'''(x_3)}{6} (x-x_0) \left(x^2-3 x_1^2+x_0 \left(x+x_0\right)-3 \left(x+x_0-2 x_1\right) x_2 \right). \end{align} $$ My questions are: Do the multivariate polynomials $\int_{x_0}^x \int_{x_1}^{t_1} \cdots \int_{x_{k-1}}^{t_{k-1}} \mathrm{d}t_k \, \cdots \mathrm{d}t_2 \, \mathrm{d} t_1$ have a closed form? Is this expansion well-known? Aside from polynomials, and analytic functions where $x_0=x_1=\dots$ , are there criteria for which an infinite series expansion exists? (that is $\lim_{n\to \infty} R_n =0 $ .) Thanks.","I have been playing around with the fundamental theorem of calculus: Starting with one can apply the FTC again, this time to the derivative, to get Substituting in gives This is one way to get the classical Taylor expansion, with the remainder term here in iterated integral form. I wondered what would happen if different ""centers"" are used at every step. For example, replacing Equation with gives What I got after steps was where and Some of the low-order polynomials are: My questions are: Do the multivariate polynomials have a closed form? Is this expansion well-known? Aside from polynomials, and analytic functions where , are there criteria for which an infinite series expansion exists? (that is .) Thanks.","f(x)=f(x_0)+\int_{x_0}^x f'(t_1) \mathrm{d}t_1, \tag{1} f'(t_1)=f'(x_0)+\int_{x_0}^{t_1} f''(t_2) \mathrm{d}t_2. \tag{2} (2) (1) f(x)=f(x_0) + f'(x_0) (x-x_0)+\int_{x_0}^x \int_{x_0}^{t_1} f''(t_2) \mathrm{d}t_2\, \mathrm{d}t_1. (2) f'(t_1) = f'(x_1)+ \int_{x_1}^{t_1} f''(t_2) \mathrm{d}t_2,   f(x)=f(x_0)+f'(x_1)(x-x_0)+\int_{x_0}^x \int_{x_1}^{t_1} f''(t_2) \mathrm{d}t_2 \, \mathrm{d} t_1.  n f(x)=P_n(x;x_0,x_1,\dots,x_n)+R_n(x;x_0,x_1,\dots,x_n), P_n(x;x_0,x_1,\dots,x_{n})=\sum_{k=0}^n f^{(k)}(x_k) \int_{x_0}^x \int_{x_1}^{t_1} \cdots \int_{x_{k-1}}^{t_{k-1}} \mathrm{d}t_k \, \cdots \mathrm{d}t_2 \, \mathrm{d} t_1,  R_n(x;x_0,x_1,\dots,x_{n}) = \int_{x_0}^x \int_{x_1}^{t_1} \cdots \int_{x_{n-1}}^{t_{n-1}} \int_{x_n}^{t_n} f^{(n+1)}(t_{n+1}) \mathrm{d}t_{n+1}\,\mathrm{d}t_n \, \cdots \mathrm{d}t_2 \, \mathrm{d} t_1. \begin{align}
P_0 &= f(x_0), \\
P_1 &= f(x_0)+f'(x_1)(x-x_0), \\
P_2 &= f(x_0)+f'(x_1)(x-x_0)+ \frac{f''(x_2)}{2}(x - x_0) (x + x_0 - 2 x_1),\\
P_3 &= P_2+ \frac{f'''(x_3)}{6} (x-x_0) \left(x^2-3 x_1^2+x_0 \left(x+x_0\right)-3 \left(x+x_0-2 x_1\right) x_2 \right).
\end{align}  \int_{x_0}^x \int_{x_1}^{t_1} \cdots \int_{x_{k-1}}^{t_{k-1}} \mathrm{d}t_k \, \cdots \mathrm{d}t_2 \, \mathrm{d} t_1 x_0=x_1=\dots \lim_{n\to \infty} R_n =0 ","['real-analysis', 'sequences-and-series', 'taylor-expansion', 'closed-form']"
41,Does a positive definite and radial function imply its Fourier transform is nonnegative?,Does a positive definite and radial function imply its Fourier transform is nonnegative?,,"I am thinking about this question: Does a positive definite and radial function imply its Fourier transform is nonnegative? I know that the converse is correct. That is, we can apply the inverse Fourier transform formula, and the definition of the positive definite function to show the double finite sum is positive. But for the above statement, I have no further idea. Someone told me considering the Bochner theorem and I searched it that can be stated as below: Bochner's theorem ：  In order that a function $f:\mathbb{R}^d\rightarrow \mathbb{C}$ be positive definite and continuous, it is necessary and sufficient that it be the    Fourier transform of a nonnegative finite-valued Borel measure on $\mathbb{R}^d$ . But I have no idea to prove the Fourier transform of $f$ is positive. Any suggestions would be welcome! Thank you！","I am thinking about this question: Does a positive definite and radial function imply its Fourier transform is nonnegative? I know that the converse is correct. That is, we can apply the inverse Fourier transform formula, and the definition of the positive definite function to show the double finite sum is positive. But for the above statement, I have no further idea. Someone told me considering the Bochner theorem and I searched it that can be stated as below: Bochner's theorem ：  In order that a function be positive definite and continuous, it is necessary and sufficient that it be the    Fourier transform of a nonnegative finite-valued Borel measure on . But I have no idea to prove the Fourier transform of is positive. Any suggestions would be welcome! Thank you！",f:\mathbb{R}^d\rightarrow \mathbb{C} \mathbb{R}^d f,"['real-analysis', 'fourier-analysis', 'fourier-transform', 'positive-definite', 'radial-basis-functions']"
42,A question on Triangle Inequality in $\mathbb{R}^n$,A question on Triangle Inequality in,\mathbb{R}^n,"I'm reading a textbook on Topology. We know that $(\rho,\mathbb{R}^n)$ is a metric space, where $$\rho(x,y)=\sqrt{\sum_{i=1}^n(x_i-y_i)^2}$$ for any $x=(x_1,x_2,\ldots,x_n),y=(y_1,y_2,\ldots,y_n)\in\mathbb{R}^n$ . When proving that $\rho(x,z)\le \rho(x,y)+\rho(y,z)$ , the author uses Schwarz Inequality. I can understand the method, but I wonder if we can do it directly. We know that three non-collinear points can determine a plane. If those three points $x,y,z$ are on a single line, then of course we can apply the Triangle Inequality on $\mathbb{R}$ ; if they are not, then they are on a same plane, still we can apply the Triangle Inequality. Isn't it just a question on $\mathbb{R}^2$ essentially? Maybe I'm missing something, but I can't find it myself. Is my reasoning correct? Thank you!","I'm reading a textbook on Topology. We know that is a metric space, where for any . When proving that , the author uses Schwarz Inequality. I can understand the method, but I wonder if we can do it directly. We know that three non-collinear points can determine a plane. If those three points are on a single line, then of course we can apply the Triangle Inequality on ; if they are not, then they are on a same plane, still we can apply the Triangle Inequality. Isn't it just a question on essentially? Maybe I'm missing something, but I can't find it myself. Is my reasoning correct? Thank you!","(\rho,\mathbb{R}^n) \rho(x,y)=\sqrt{\sum_{i=1}^n(x_i-y_i)^2} x=(x_1,x_2,\ldots,x_n),y=(y_1,y_2,\ldots,y_n)\in\mathbb{R}^n \rho(x,z)\le \rho(x,y)+\rho(y,z) x,y,z \mathbb{R} \mathbb{R}^2","['real-analysis', 'general-topology']"
43,Prove that $\sum{\frac{1}{n_k}}$ is convergent .,Prove that  is convergent .,\sum{\frac{1}{n_k}},"The sequence $(n_k)$ is a strictly increasing positive sequence of integers which satisfies the condition $\lim_{k\to\infty}\frac{n_k}{n_1...n_{k-1}}=\infty$ . Now this has been my attempt, $\frac{n_k}{n_1...n_{k-1}}>1$ for all $k \ge N_1$ . So $n_k > n_1...n_{k-1}$ for all $k \ge N_1$ . Since the sequence is strictly increasing so $(n_k) >M_{N_1}$ for all $k \ge N_1$ ,then $\frac{1}{n_k}<\frac{1}{M_{N_1}}$ for all $k \ge N_1$ .Now let us chose $\min m=(n_1,...,n_{N_1-1},{M_{N_1}})$ . Then $n_k> m$ for all $k \in N$ Now $\sum{\frac{1}{n_k}}<\sum{\frac{1}{n_1...n_{k-1}}}<\sum{\frac{1}{m^{k-1}}}$ for $ k \ge N_1$ which leads to a geometric sequence. I think this method should work.It would be very helpful if someone goes through my attempt and point out my mistake. Edit1:The question also has a subpart which ask to prove that the sum is irrational.How do I proceed?","The sequence is a strictly increasing positive sequence of integers which satisfies the condition . Now this has been my attempt, for all . So for all . Since the sequence is strictly increasing so for all ,then for all .Now let us chose . Then for all Now for which leads to a geometric sequence. I think this method should work.It would be very helpful if someone goes through my attempt and point out my mistake. Edit1:The question also has a subpart which ask to prove that the sum is irrational.How do I proceed?","(n_k) \lim_{k\to\infty}\frac{n_k}{n_1...n_{k-1}}=\infty \frac{n_k}{n_1...n_{k-1}}>1 k \ge N_1 n_k > n_1...n_{k-1} k \ge N_1 (n_k) >M_{N_1} k \ge N_1 \frac{1}{n_k}<\frac{1}{M_{N_1}} k \ge N_1 \min m=(n_1,...,n_{N_1-1},{M_{N_1}}) n_k> m k \in N \sum{\frac{1}{n_k}}<\sum{\frac{1}{n_1...n_{k-1}}}<\sum{\frac{1}{m^{k-1}}}  k \ge N_1","['real-analysis', 'sequences-and-series']"
44,Prove that $\left|\int_0^1 f(x)dx\right|\ge 1\text{ or }\left|\int_0^1 g(x)dx\right|\ge 1.$,Prove that,\left|\int_0^1 f(x)dx\right|\ge 1\text{ or }\left|\int_0^1 g(x)dx\right|\ge 1.,"Question: Let $f,g:[0,1]\to\mathbb{R}$ be two continuous functions such that $f(x)g(x)\ge 4x^2, \forall x\in[0,1].$ Prove that $$\left|\int_0^1 f(x)dx\right|\ge 1\text{ or }\left|\int_0^1 g(x)dx\right|\ge 1.$$ Solution: Since we have $f(x)g(x)\ge 4x^2\ge 0, \forall x\in[0,1]\implies f(x)g(x)\ge 0, \forall x\in[0,1].$ This in turn implies that $f(x)$ and $g(x)$ has the same sign at each $x\in[0,1]$ . Claim: $f$ does not change it's sign in the interval $[0,1]$ . Proof: Let us assume that $f(x)>0$ at some $x=a$ and $f(x)<0$ at some $x=b$ , where $a,b\in[0,1]$ and $b>a$ . Now since $f$ is continuous in $[0,1]$ , thus by IVT we can conclude that $\exists c\in(a,b),$ such that $f(c)=0$ . Hence, we have $f(c)g(c)=0\ge 4c^2\implies 4c^2\le 0,$ but $c>0\implies 4c^2>0.$ Hence we arrive at a clear contradiction. Thus, we can conclude that $f$ does not change it's sign in the interval $[0,1]$ . Using our claim, we can also conclude that, $g$ does not change it's sign in the interval $[0,1]$ . Thus, either $f(x),g(x)>0, \forall x\in[0,1]$ or $f(x),g(x)<0, \forall x\in[0,1]$ . Observe that in any case $$\left|\int_0^1 f(x)dx\right|=\int_0^1|f(x)|dx\text{ and }\left|\int_0^1 g(x)dx\right|=\int_0^1|g(x)|dx.$$ Thus, by Cauchy-Schwarz inequality we have $$\left|\int_0^1 f(x)dx\right|\left|\int_0^1 g(x)dx\right|=\int_0^1|f(x)|dx\int_0^1|g(x)|dx\\=\int_0^1\left(\sqrt{|f(x)|}\right)^2dx\int_0^1\left(\sqrt{|g(x)|}\right)^2dx\\\ge \left(\int_0^1\sqrt{|f(x)|}.\sqrt{|g(x)|}dx\right)^2=\left(\int_0^1\sqrt{|f(x)g(x)|}dx\right)^2=\left(\int_0^1\sqrt{f(x)g(x)}dx\right)^2.$$ Now since $\forall x\in[0,1]$ , we have $$f(x)g(x)\ge 4x^2\implies \sqrt{f(x)g(x)}\ge 2x\\\implies\int_0^1\sqrt{f(x)g(x)}dx\ge \int_0^1 2x dx=1\\\implies \left(\int_0^1\sqrt{f(x)g(x)}dx\right)^2\ge 1.$$ Hence, we can conclude that $$\left|\int_0^1 f(x)dx\right|\left|\int_0^1 g(x)dx\right|\ge 1.$$ Thus, we can conclude that $$\left|\int_0^1 f(x)dx\right|\ge 1\text{ or }\left|\int_0^1 g(x)dx\right|\ge 1.$$ Hence, we are done. Is this solution correct and rigorous enough? Are there any alternative solutions?","Question: Let be two continuous functions such that Prove that Solution: Since we have This in turn implies that and has the same sign at each . Claim: does not change it's sign in the interval . Proof: Let us assume that at some and at some , where and . Now since is continuous in , thus by IVT we can conclude that such that . Hence, we have but Hence we arrive at a clear contradiction. Thus, we can conclude that does not change it's sign in the interval . Using our claim, we can also conclude that, does not change it's sign in the interval . Thus, either or . Observe that in any case Thus, by Cauchy-Schwarz inequality we have Now since , we have Hence, we can conclude that Thus, we can conclude that Hence, we are done. Is this solution correct and rigorous enough? Are there any alternative solutions?","f,g:[0,1]\to\mathbb{R} f(x)g(x)\ge 4x^2, \forall x\in[0,1]. \left|\int_0^1 f(x)dx\right|\ge 1\text{ or }\left|\int_0^1 g(x)dx\right|\ge 1. f(x)g(x)\ge 4x^2\ge 0, \forall x\in[0,1]\implies f(x)g(x)\ge 0, \forall x\in[0,1]. f(x) g(x) x\in[0,1] f [0,1] f(x)>0 x=a f(x)<0 x=b a,b\in[0,1] b>a f [0,1] \exists c\in(a,b), f(c)=0 f(c)g(c)=0\ge 4c^2\implies 4c^2\le 0, c>0\implies 4c^2>0. f [0,1] g [0,1] f(x),g(x)>0, \forall x\in[0,1] f(x),g(x)<0, \forall x\in[0,1] \left|\int_0^1 f(x)dx\right|=\int_0^1|f(x)|dx\text{ and }\left|\int_0^1 g(x)dx\right|=\int_0^1|g(x)|dx. \left|\int_0^1 f(x)dx\right|\left|\int_0^1 g(x)dx\right|=\int_0^1|f(x)|dx\int_0^1|g(x)|dx\\=\int_0^1\left(\sqrt{|f(x)|}\right)^2dx\int_0^1\left(\sqrt{|g(x)|}\right)^2dx\\\ge \left(\int_0^1\sqrt{|f(x)|}.\sqrt{|g(x)|}dx\right)^2=\left(\int_0^1\sqrt{|f(x)g(x)|}dx\right)^2=\left(\int_0^1\sqrt{f(x)g(x)}dx\right)^2. \forall x\in[0,1] f(x)g(x)\ge 4x^2\implies \sqrt{f(x)g(x)}\ge 2x\\\implies\int_0^1\sqrt{f(x)g(x)}dx\ge \int_0^1 2x dx=1\\\implies \left(\int_0^1\sqrt{f(x)g(x)}dx\right)^2\ge 1. \left|\int_0^1 f(x)dx\right|\left|\int_0^1 g(x)dx\right|\ge 1. \left|\int_0^1 f(x)dx\right|\ge 1\text{ or }\left|\int_0^1 g(x)dx\right|\ge 1.","['real-analysis', 'solution-verification']"
45,"Commuting second-order partial derivatives at some point, but exactly one of them is continuous at the point","Commuting second-order partial derivatives at some point, but exactly one of them is continuous at the point",,"Question. Does there exist a function $f(x,y)$ and a point $(x_0,y_0)$ such that: Both $f_{xy}$ and $f_{yx}$ exist in a neighbourhood of $(x_0,y_0)$ . $f_{xy}(x_0, y_0)=f_{yx}(x_0, y_0)$ . $f_{xy}$ is continuous at $(x_0,y_0)$ . $f_{yx}$ is not continuous at $(x_0,y_0)$ . My thought. Since there are functions $g(x,y)$ with $g_{xy}(0,0)=a\ne b= g_{yx}(0,0)$ (e.g. see this post ), a possible way would be to construct a sequence of disjoint balls $B_n$ centered at $(1/n,1/n)$ with radius $1/(n+1)^3$ , then smoothly restrict $g(x-1/n, y-1/n)$ in $B_n$ to get a function $$h_{n}(x,y)=\begin{cases} g(x-1/n,y-1/n) & \text{in a neighbourhood of}~ (1/n,1/n) \\ 0, &\text{outside}~B_n\end{cases}$$ and finally let $f(x,y)=\sum_{n\geq 1} h_n(x,y)$ . In this way $$f_{xy}(1/n,1/n)=(h_n)_{xy}(1/n,1/n)\to a \quad\text{and}\quad f_{yx}(1/n,1/n)=(h_n)_{yx}(1/n,1/n)\to b $$ and the requirement 4 is met so long as $f_{xy}(0,0)=f_{yx}(0,0)=a$ . But as to fulfill requirement 3, I cannot prove $f_{xy}(x_n,y_n)\to a$ when $(x_n,y_n)$ tends to $(0,0)$ arbitrarily, not just taking on points $(1/n,1/n)$ .","Question. Does there exist a function and a point such that: Both and exist in a neighbourhood of . . is continuous at . is not continuous at . My thought. Since there are functions with (e.g. see this post ), a possible way would be to construct a sequence of disjoint balls centered at with radius , then smoothly restrict in to get a function and finally let . In this way and the requirement 4 is met so long as . But as to fulfill requirement 3, I cannot prove when tends to arbitrarily, not just taking on points .","f(x,y) (x_0,y_0) f_{xy} f_{yx} (x_0,y_0) f_{xy}(x_0, y_0)=f_{yx}(x_0, y_0) f_{xy} (x_0,y_0) f_{yx} (x_0,y_0) g(x,y) g_{xy}(0,0)=a\ne b= g_{yx}(0,0) B_n (1/n,1/n) 1/(n+1)^3 g(x-1/n, y-1/n) B_n h_{n}(x,y)=\begin{cases} g(x-1/n,y-1/n) & \text{in a neighbourhood of}~ (1/n,1/n) \\ 0, &\text{outside}~B_n\end{cases} f(x,y)=\sum_{n\geq 1} h_n(x,y) f_{xy}(1/n,1/n)=(h_n)_{xy}(1/n,1/n)\to a \quad\text{and}\quad f_{yx}(1/n,1/n)=(h_n)_{yx}(1/n,1/n)\to b  f_{xy}(0,0)=f_{yx}(0,0)=a f_{xy}(x_n,y_n)\to a (x_n,y_n) (0,0) (1/n,1/n)","['real-analysis', 'partial-derivative']"
46,Does there exist a volume-preserving diffeomorphism of the disk without conformal points?,Does there exist a volume-preserving diffeomorphism of the disk without conformal points?,,"This question is related to this one , though is supposed to be easier. Let $D \subseteq \mathbb{R}^2$ be the closed unit disk. Does there exist a smooth volume-preserving diffeomorphism $f:D \to D$ such that the singular values of $df$ are everywhere distinct? i.e. I want $\sigma_1(df_p) \neq \sigma_2(df_p)$ for every $p \in D$ , and the product $\sigma_1(df)\sigma_2(df)=1$ to be constant. This answer provides the following example for such a diffeomorphism $D\setminus \{0\} \to D \setminus \{0\}$ with the required properties: $f_c: (r,\theta)\to r\big(\cos(\theta+c\log(r)), \sin(\theta+c\log (r))\big),\;\; $ (for every non-zero $c ֿ\in \mathbb R$ we get an example). Edit-a description of a possible topological obstruction: Set $\mathcal{NC}:=\{ A \in M_2(\mathbb{R}) \, |    \det A \ge 0 \, \,\text{ and } \, A \text{ is not conformal} \,\}$ , where by a non-conformal matrix, I refer to a matrix whose singular values are distinct. (i.e. I allow non-zero singular matrices in $\mathcal{NC}$ ). Suppose that such an $f \in \text{Diff}(D)$ exists. Then $df|_{\partial D}:\partial D \to \mathcal{NC}$ is homotopic to a constant. $df|_{\partial D}$ maps $T\partial D$ to itself, and in particular, at a specific point $\theta \in \mathbb{S}^1$ , $(df|_{\partial D})_{\theta}(T_{\theta}\partial D)=T_{f(\theta)}\partial D$ . So, thinking on $e_2$ as an element of $T_{(0,1)}\partial D$ , we have $R_{f(\theta)}^{-1} \circ df_{\theta} \circ R_{\theta}(e_2)=\lambda(\theta) e_2$ , for some positive factor $\lambda(\theta)$ . Setting $A_{\theta}:=R_{f(\theta)}^{-1} \circ df_{\theta} \circ R_{\theta}$ , and $$\mathcal{F}:=\{ A \in \text{SL}_2(\mathbb{R}) \, | \, Ae_2 \in \operatorname{span}(e_2) \, \, \text{ and } \, \, A \, \text{ is not conformal} \,\},$$ we get $$ df_{\theta} =R_{f(\theta)} \circ A_{\theta} \circ R_{-\theta}, \, \, \, A_{\theta}: \partial D \to \mathcal{F}. \tag{1}$$ If $\mathcal{F}$ were contractible in $\mathcal{NC}$ , we could deform $A_{\theta}$ to a constant map $ \partial D \to \mathcal{NC}$ . Thus, by equation $(1)$ , $df|_{\partial D}$ would be homotopic to the map $\theta \to R_{f(\theta)} \circ A \circ R_{-\theta}$ for some constant non-conformal matrix $A \in \mathcal{NC}$ . Writing $A=R_{\alpha} \Sigma R_{\beta}$ where $\Sigma$ is non-negative and diagonal, we would get that $df|_{\partial D}$ is homotopic to $\theta \to R_{f(\theta)+\alpha} \circ \Sigma \circ R_{-\theta+\beta}.$ On the space of non-conformal matrices $\mathcal{NC}$ , there is a continuous map* $H:\mathcal{NC} \to \mathbb{S}^1$ , given by $H(R_{\phi} \Sigma R_{\theta})= R_{2\theta}$ . This leads to a contradiction to the contractibility of $df|_{\partial D}$ : Indeed, if it were homotopic to a constant, then so would the map $\theta \to R_{f(\theta)+\alpha} \circ \Sigma \circ R_{-\theta+\beta}.$ Composing it with $H$ , we obtain the map $\theta \to R_{-2\theta+2\beta}$ , or $\theta \to -2\theta$ , which is not homotopic to a constant. Since $\mathcal{F}$ is not contractible in $\mathcal{NC}$ , this argument fails. However, perhaps a more refined topological argument could obtain more, I don't know. *The map $H$ is well-defined, since $U\Sigma V^T=(-U)\Sigma (-V)^T$ , and this is the only ambiguity in $U,V$ for a matrix in $\mathcal{NC}$ . Thus $\theta$ is well defined up to an addition of $\pi$ .","This question is related to this one , though is supposed to be easier. Let be the closed unit disk. Does there exist a smooth volume-preserving diffeomorphism such that the singular values of are everywhere distinct? i.e. I want for every , and the product to be constant. This answer provides the following example for such a diffeomorphism with the required properties: (for every non-zero we get an example). Edit-a description of a possible topological obstruction: Set , where by a non-conformal matrix, I refer to a matrix whose singular values are distinct. (i.e. I allow non-zero singular matrices in ). Suppose that such an exists. Then is homotopic to a constant. maps to itself, and in particular, at a specific point , . So, thinking on as an element of , we have , for some positive factor . Setting , and we get If were contractible in , we could deform to a constant map . Thus, by equation , would be homotopic to the map for some constant non-conformal matrix . Writing where is non-negative and diagonal, we would get that is homotopic to On the space of non-conformal matrices , there is a continuous map* , given by . This leads to a contradiction to the contractibility of : Indeed, if it were homotopic to a constant, then so would the map Composing it with , we obtain the map , or , which is not homotopic to a constant. Since is not contractible in , this argument fails. However, perhaps a more refined topological argument could obtain more, I don't know. *The map is well-defined, since , and this is the only ambiguity in for a matrix in . Thus is well defined up to an addition of .","D \subseteq \mathbb{R}^2 f:D \to D df \sigma_1(df_p) \neq \sigma_2(df_p) p \in D \sigma_1(df)\sigma_2(df)=1 D\setminus \{0\} \to D \setminus \{0\} f_c: (r,\theta)\to r\big(\cos(\theta+c\log(r)), \sin(\theta+c\log (r))\big),\;\;  c ֿ\in \mathbb R \mathcal{NC}:=\{ A \in M_2(\mathbb{R}) \, |    \det A \ge 0 \, \,\text{ and } \, A \text{ is not conformal} \,\} \mathcal{NC} f \in \text{Diff}(D) df|_{\partial D}:\partial D \to \mathcal{NC} df|_{\partial D} T\partial D \theta \in \mathbb{S}^1 (df|_{\partial D})_{\theta}(T_{\theta}\partial D)=T_{f(\theta)}\partial D e_2 T_{(0,1)}\partial D R_{f(\theta)}^{-1} \circ df_{\theta} \circ R_{\theta}(e_2)=\lambda(\theta) e_2 \lambda(\theta) A_{\theta}:=R_{f(\theta)}^{-1} \circ df_{\theta} \circ R_{\theta} \mathcal{F}:=\{ A \in \text{SL}_2(\mathbb{R}) \, | \, Ae_2 \in \operatorname{span}(e_2) \, \, \text{ and } \, \, A \, \text{ is not conformal} \,\},  df_{\theta} =R_{f(\theta)} \circ A_{\theta} \circ R_{-\theta}, \, \, \, A_{\theta}: \partial D \to \mathcal{F}. \tag{1} \mathcal{F} \mathcal{NC} A_{\theta}  \partial D \to \mathcal{NC} (1) df|_{\partial D} \theta \to R_{f(\theta)} \circ A \circ R_{-\theta} A \in \mathcal{NC} A=R_{\alpha} \Sigma R_{\beta} \Sigma df|_{\partial D} \theta \to R_{f(\theta)+\alpha} \circ \Sigma \circ R_{-\theta+\beta}. \mathcal{NC} H:\mathcal{NC} \to \mathbb{S}^1 H(R_{\phi} \Sigma R_{\theta})= R_{2\theta} df|_{\partial D} \theta \to R_{f(\theta)+\alpha} \circ \Sigma \circ R_{-\theta+\beta}. H \theta \to R_{-2\theta+2\beta} \theta \to -2\theta \mathcal{F} \mathcal{NC} H U\Sigma V^T=(-U)\Sigma (-V)^T U,V \mathcal{NC} \theta \pi","['real-analysis', 'differential-geometry', 'algebraic-topology', 'differential-topology', 'quasiconformal-maps']"
47,Quotient of measurable subspace measurable?,Quotient of measurable subspace measurable?,,"Let $E$ be a Banach space, $N\subset E$ a closed linear subspace and $q:E\rightarrow E/N$ the natural quotient map. Question. If $X\subset E$ is a Borel-measurable linear subspace, is $q(X)\subset E/N$ also Borel-measurable (w.r.t. the quotient topology)? I am not sure whether the condition that $X$ is a linear subspace is really needed. Ideally it could be left away By the open mapping theorem, $q$ is an open map and thus  the collection of sets $\mathcal{S}=\{A\subset E: q(A) \text{ is Borel measurable }\}$ contains all open sets. If $\mathcal{S}$ was a Dynkin system, then we would be done, but this does not seem to be the case (e.g. I don't see why $q(A^c)$ should be measurable, when $q(A)$ is). The example I have in mind is $E=C(M)$ for a closed manifold $M$ . Then for $\alpha>\dim M/2$ one can show that the Sobolev space $X=H^\alpha(M)\subset E$ is measurable. I am interested whether this generalises to a smooth domain $\Omega\subset M$ , i.e. after taking the quotient with respect to $N=\{u: u\vert_{M\backslash \Omega} = 0\}$ .","Let be a Banach space, a closed linear subspace and the natural quotient map. Question. If is a Borel-measurable linear subspace, is also Borel-measurable (w.r.t. the quotient topology)? I am not sure whether the condition that is a linear subspace is really needed. Ideally it could be left away By the open mapping theorem, is an open map and thus  the collection of sets contains all open sets. If was a Dynkin system, then we would be done, but this does not seem to be the case (e.g. I don't see why should be measurable, when is). The example I have in mind is for a closed manifold . Then for one can show that the Sobolev space is measurable. I am interested whether this generalises to a smooth domain , i.e. after taking the quotient with respect to .",E N\subset E q:E\rightarrow E/N X\subset E q(X)\subset E/N X q \mathcal{S}=\{A\subset E: q(A) \text{ is Borel measurable }\} \mathcal{S} q(A^c) q(A) E=C(M) M \alpha>\dim M/2 X=H^\alpha(M)\subset E \Omega\subset M N=\{u: u\vert_{M\backslash \Omega} = 0\},"['real-analysis', 'functional-analysis', 'probability-theory', 'measure-theory', 'borel-sets']"
48,prove $ \sum_{n=1}^\infty \frac{1}{n^\alpha\sqrt{n |x-x_n|}} $ converges almost everywhere,prove  converges almost everywhere, \sum_{n=1}^\infty \frac{1}{n^\alpha\sqrt{n |x-x_n|}} ,"Let $ \{x_n\}_{n=1}^\infty \subset \mathbb{R} $ be a sequence. Prove for $\alpha>1$ that $\sum_{n=1}^\infty \frac{1}{n^\alpha\sqrt{n|x-x_n|}}$ converges for almost every $x$ with regard to Lebesgue measure on $\mathbb{R}$ . I tried solving by finding an integrable function such that this series is bounded below the function's integral, but I didn't find a suitable function.","Let be a sequence. Prove for that converges for almost every with regard to Lebesgue measure on . I tried solving by finding an integrable function such that this series is bounded below the function's integral, but I didn't find a suitable function.", \{x_n\}_{n=1}^\infty \subset \mathbb{R}  \alpha>1 \sum_{n=1}^\infty \frac{1}{n^\alpha\sqrt{n|x-x_n|}} x \mathbb{R},"['real-analysis', 'lebesgue-integral', 'lebesgue-measure']"
49,How to bound this integral with the Hardy-Littlewood maximal function,How to bound this integral with the Hardy-Littlewood maximal function,,"Let $\Omega$ be a bounded Lipschitz domain in $\mathbb{R}^d$ and consider the following situation. $$\phi(x) = \int_{\partial \Omega} \frac{x-y}{|x-y|^d} f(y) d\sigma(y)$$ Where $\partial \Omega$ denotes the boundary of $\Omega$ and $\sigma$ denotes the surface measure. I want to show that the nontangential maximal function of $\phi$ is bounded on $L^p(\partial \Omega)$ when $f \in L^p(\partial \Omega)$ for $1 < p < \infty$ . To do so we look at the definition of the nontangential maximal function (denoted by $(\phi)^{*})$ . Let $P \in \partial \Omega$ denote a point on the boundary. Then $$ (\phi)^*(P) = \sup_{\substack{x \in \Omega \\ |x-P| < C_1 \operatorname{dist}(x,\partial \Omega)}} \left| \int_{\partial \Omega} \frac{x-y}{|x-y|^d} f(y) d\sigma(y) \right|$$ I have split the integral in multiple parts and managed to bound some of them by the maximal function of $f$ in $P$ . However I'm missing one part. I want to show that $$ \sup_{\substack{x \in \Omega \\ |x-P| < C_1\operatorname{dist}(x,\partial \Omega)}} \left| \int_{|P-y| > 2|P-x|} \frac{|P-y|}{|P-y|^d} f(y) d\sigma(y) \right|  \leq C_2 \mathcal{M}_{\partial \Omega}(f)(P)$$ With the maximal function defined as $$\mathcal{M}_{\partial \Omega}(f)(P) = \sup_{(B\cap \partial \Omega) \ni P}\frac{1}{\sigma\{B\cap \partial \Omega\}}\int_{B \cap \partial \Omega} |f(y)| d\sigma(y)$$",Let be a bounded Lipschitz domain in and consider the following situation. Where denotes the boundary of and denotes the surface measure. I want to show that the nontangential maximal function of is bounded on when for . To do so we look at the definition of the nontangential maximal function (denoted by . Let denote a point on the boundary. Then I have split the integral in multiple parts and managed to bound some of them by the maximal function of in . However I'm missing one part. I want to show that With the maximal function defined as,"\Omega \mathbb{R}^d \phi(x) = \int_{\partial \Omega} \frac{x-y}{|x-y|^d} f(y) d\sigma(y) \partial \Omega \Omega \sigma \phi L^p(\partial \Omega) f \in L^p(\partial \Omega) 1 < p < \infty (\phi)^{*}) P \in \partial \Omega  (\phi)^*(P) = \sup_{\substack{x \in \Omega \\ |x-P| < C_1 \operatorname{dist}(x,\partial \Omega)}} \left| \int_{\partial \Omega} \frac{x-y}{|x-y|^d} f(y) d\sigma(y) \right| f P  \sup_{\substack{x \in \Omega \\ |x-P| < C_1\operatorname{dist}(x,\partial \Omega)}} \left| \int_{|P-y| > 2|P-x|} \frac{|P-y|}{|P-y|^d} f(y) d\sigma(y) \right|  \leq C_2 \mathcal{M}_{\partial \Omega}(f)(P) \mathcal{M}_{\partial \Omega}(f)(P) = \sup_{(B\cap \partial \Omega) \ni P}\frac{1}{\sigma\{B\cap \partial \Omega\}}\int_{B \cap \partial \Omega} |f(y)| d\sigma(y)","['real-analysis', 'harmonic-analysis', 'potential-theory']"
50,"Finding $\lim_{x,y \rightarrow 0,0} \frac{x \ln(1+y)}{2x^2+y^2}$",Finding,"\lim_{x,y \rightarrow 0,0} \frac{x \ln(1+y)}{2x^2+y^2}","Find bounds for $\lim_{x,y \rightarrow 0,0} \frac{x \ln(1+y)}{2x^2+y^2}$ I am finding maximum and minimum for function and one of critical case is to find possible minimal and maximal value of given function in $0,0$ . But how can I do this due to this limit doesn't exists (for example we can take $x,y = {1\over n},{1\over n}$ and $x,y = {2\over n},{1 \over n}$",Find bounds for I am finding maximum and minimum for function and one of critical case is to find possible minimal and maximal value of given function in . But how can I do this due to this limit doesn't exists (for example we can take and,"\lim_{x,y \rightarrow 0,0} \frac{x \ln(1+y)}{2x^2+y^2} 0,0 x,y = {1\over n},{1\over n} x,y = {2\over n},{1 \over n}",['real-analysis']
51,Heuristic on Sobolev and BV functions,Heuristic on Sobolev and BV functions,,"Let $f: \Omega \subset \mathbb{R}^N \to \mathbb{R}^M$ be a Sobolev or BV vector field. A heuristic that I've heard frequently is the following: $f$ is almost Lipschitz on a large ""good"" set but there is a small ""bad"" set where $Df$ is very large. What theorems make this heuristic rigorous?","Let be a Sobolev or BV vector field. A heuristic that I've heard frequently is the following: is almost Lipschitz on a large ""good"" set but there is a small ""bad"" set where is very large. What theorems make this heuristic rigorous?",f: \Omega \subset \mathbb{R}^N \to \mathbb{R}^M f Df,"['real-analysis', 'functional-analysis', 'reference-request', 'soft-question', 'sobolev-spaces']"
52,"(How) can one visualise the derivative of the function $A \mapsto A^{-1}$, where $A$ is a matrix?","(How) can one visualise the derivative of the function , where  is a matrix?",A \mapsto A^{-1} A,"Our definition of differentiable is: Let $V$ and $W$ be Banachspaces, $V$ finite dimensional, and $G \subset V$ an open subset. We call a function $f: G \to W$ differentiable in $p \in G$ if there exists a linear map $F: V \to W$ , so that for the remainder function $R:G \to W$ , defined by $$ f(x) = f(p) + F(x - p) + R(x) $$ we have $\frac{R(x)}{\| x - p \|} \xrightarrow{x \to p} 0$ . Then, the function $F$ is unique and called the differential of $f$ in $p$ , we write $F = D_p f$ . Lemma: With all names from above we have for all $v \in V$ $$ F(v) = \lim_{t \to 0} \frac{f(p + tv) - f(p)}{t} =: \partial_v f(p), $$ if the limit exists and call it the directional derivative. In our lecture we have shown the following Let $V$ be a finite-dimensional Banach-space, then the general linear group $$\mbox{GL}(V) := \{ A \in L(V, V): A \text{ is invertible } \}$$ is open in $L(V,V)$ and the mapping $$\mbox{inv}: \mbox{GL}(V) \to L(V,V), \quad A \mapsto A^{-1}$$ is differentiable and its derivative is $D_{A} \mbox{inv}(B) = - A^{-1} B A^{-1}$ . This is the case because $$ \mbox{inv}(B) = \mbox{inv}(A) \underbrace{- A^{-1}(B - A)A^{-1}}_{D_A \text{inv}(B - A)} + \underbrace{A^{-1}(B - A)(A^{-1} - B^{-1})}_{= R(B)}. $$ Is there any way to visualise what $\mbox{inv}$ looks like and to picture its derivative? Using @RodrigodeAzevedo's hint, the vector field of a the inverse of a symmetric $2 \times 2$ matrix looks like this, but I don't really know what that ""shows"" me.","Our definition of differentiable is: Let and be Banachspaces, finite dimensional, and an open subset. We call a function differentiable in if there exists a linear map , so that for the remainder function , defined by we have . Then, the function is unique and called the differential of in , we write . Lemma: With all names from above we have for all if the limit exists and call it the directional derivative. In our lecture we have shown the following Let be a finite-dimensional Banach-space, then the general linear group is open in and the mapping is differentiable and its derivative is . This is the case because Is there any way to visualise what looks like and to picture its derivative? Using @RodrigodeAzevedo's hint, the vector field of a the inverse of a symmetric matrix looks like this, but I don't really know what that ""shows"" me.","V W V G \subset V f: G \to W p \in G F: V \to W R:G \to W 
f(x) = f(p) + F(x - p) + R(x)
 \frac{R(x)}{\| x - p \|} \xrightarrow{x \to p} 0 F f p F = D_p f v \in V 
F(v)
= \lim_{t \to 0} \frac{f(p + tv) - f(p)}{t}
=: \partial_v f(p),
 V \mbox{GL}(V) := \{ A \in L(V, V): A \text{ is invertible } \} L(V,V) \mbox{inv}: \mbox{GL}(V) \to L(V,V), \quad A \mapsto A^{-1} D_{A} \mbox{inv}(B) = - A^{-1} B A^{-1} 
\mbox{inv}(B) = \mbox{inv}(A) \underbrace{- A^{-1}(B - A)A^{-1}}_{D_A \text{inv}(B - A)} + \underbrace{A^{-1}(B - A)(A^{-1} - B^{-1})}_{= R(B)}.
 \mbox{inv} 2 \times 2","['real-analysis', 'matrices', 'derivatives', 'intuition', 'matrix-calculus']"
53,Question on a step of the proof of Theorem 1.25 of Introduction to Fourier Analysis on Euclidean Spaces,Question on a step of the proof of Theorem 1.25 of Introduction to Fourier Analysis on Euclidean Spaces,,"Theorem 1.25: Suppose $ \phi \in L^1(\mathbb{R^n}) $ and $ \int_{\mathbb{R^n}} \phi =1 $ .   Also, let $\phi_{\epsilon}(x)=\frac{\phi\left(\frac{x}{\epsilon}\right)}{\epsilon^n}$ .Moreover , suppose that : $ \psi(x)= \operatorname{esssup}_{|y|\geq |x|} |\phi(y)|$ is in $ L^1(\mathbb{R^n}) $ and $ f \in L^p(\mathbb{R^n}) $ for some $ p \in [1,\infty] $ Then $ \lim_{\epsilon \to 0} (f \ast \phi_{\epsilon})(x)=f(x)$ for every $ x \in L_f $ where $L_f$ is the Lebesgue set of $ f $ . In the course of the proof the author obtains for $ x \in L_f$ and for an arbitrary $\delta > 0 $ an $h>0 $ such that : $$ \frac{1}{r^n} \int_{|t|<r} |f(x-t)-f(x)|\mathrm{d}t < \delta , \quad \forall 0<r \leq h $$ Then the author considers the function $$ g(r)= \int_{S^{n-1}} |f(x-ry)-f(x)| \mathrm{d} \sigma_y $$ where $ \sigma $ is the surface measure on $ S^{n-1} $ . Then he considers the function $$G(r)=\int_{0}^{r} g(s)s^{n-1} \mathrm{d}s  $$ Now the step that I do not quite get is why the following calculation is fully justified : $$ \begin{split}  \int_{ |t|<h} |f(x-t)-f(x)| &\frac{1}{\epsilon^n} \psi\left(\frac{t}{ \epsilon}\right) \mathrm{d}t = \int_{0}^{h} r^{n-1}g(r) \epsilon^{-n} \psi\left(\frac{r}{\epsilon}\right) \mathrm{d}r \\ &= \left. G(r) \epsilon^{-n} \psi\left(\frac{r}{\epsilon}\right) \right|_0^h-\int_0^h G(r)\,\mathrm{d}\left( \epsilon^{-n} \psi\left(\frac{r}{\epsilon}\right)\right) \end{split}$$ While it clear for me why $ G $ and $ \psi $ is differentiable ( $ G $ because of the Lebesgue's differentiation theorem and $ \psi $ because it is decreasing [ in $ r $ ] ) I am not sure why the  integration by parts formula here is valid. For example I know that if R and Q are both absolutely continuous then we do have that $ \int_{a}^{b} RQ^{\prime}+QR^{\prime}=R(b)Q(b)-R(a)Q(a) $ but I am not sure that  this applies here. Thank you in advance for your help.","Theorem 1.25: Suppose and .   Also, let .Moreover , suppose that : is in and for some Then for every where is the Lebesgue set of . In the course of the proof the author obtains for and for an arbitrary an such that : Then the author considers the function where is the surface measure on . Then he considers the function Now the step that I do not quite get is why the following calculation is fully justified : While it clear for me why and is differentiable ( because of the Lebesgue's differentiation theorem and because it is decreasing [ in ] ) I am not sure why the  integration by parts formula here is valid. For example I know that if R and Q are both absolutely continuous then we do have that but I am not sure that  this applies here. Thank you in advance for your help."," \phi \in L^1(\mathbb{R^n})   \int_{\mathbb{R^n}} \phi =1  \phi_{\epsilon}(x)=\frac{\phi\left(\frac{x}{\epsilon}\right)}{\epsilon^n}  \psi(x)= \operatorname{esssup}_{|y|\geq |x|} |\phi(y)|  L^1(\mathbb{R^n})   f \in L^p(\mathbb{R^n})   p \in [1,\infty]   \lim_{\epsilon \to 0} (f \ast \phi_{\epsilon})(x)=f(x)  x \in L_f  L_f  f   x \in L_f \delta > 0  h>0  
\frac{1}{r^n} \int_{|t|<r} |f(x-t)-f(x)|\mathrm{d}t < \delta , \quad \forall 0<r \leq h
 
g(r)= \int_{S^{n-1}} |f(x-ry)-f(x)| \mathrm{d} \sigma_y
  \sigma   S^{n-1}  G(r)=\int_{0}^{r} g(s)s^{n-1} \mathrm{d}s 
 
\begin{split}
 \int_{ |t|<h} |f(x-t)-f(x)| &\frac{1}{\epsilon^n} \psi\left(\frac{t}{ \epsilon}\right) \mathrm{d}t = \int_{0}^{h} r^{n-1}g(r) \epsilon^{-n} \psi\left(\frac{r}{\epsilon}\right) \mathrm{d}r \\
&= \left. G(r) \epsilon^{-n} \psi\left(\frac{r}{\epsilon}\right) \right|_0^h-\int_0^h G(r)\,\mathrm{d}\left( \epsilon^{-n} \psi\left(\frac{r}{\epsilon}\right)\right)
\end{split}  G   \psi   G   \psi   r   \int_{a}^{b} RQ^{\prime}+QR^{\prime}=R(b)Q(b)-R(a)Q(a) ","['real-analysis', 'functional-analysis', 'fourier-analysis', 'lp-spaces', 'harmonic-analysis']"
54,Convergence/Divergence of infinite series $\sum_{n=1}^{\infty} \frac{(\sin n+2)^n}{n3^n}$,Convergence/Divergence of infinite series,\sum_{n=1}^{\infty} \frac{(\sin n+2)^n}{n3^n},"$$ \sum_{n=1}^{\infty} \frac{(\sin n+2)^n}{n3^n}$$ Does it converge or diverge? Can we have a rigorous proof that is not probabilistic? For reference, this question is supposedly a mix of real analysis and calculus.","Does it converge or diverge? Can we have a rigorous proof that is not probabilistic? For reference, this question is supposedly a mix of real analysis and calculus.", \sum_{n=1}^{\infty} \frac{(\sin n+2)^n}{n3^n},"['real-analysis', 'calculus', 'sequences-and-series', 'convergence-divergence']"
55,Function such that $f^{(n)}(x) = \frac{x}{f(x)^n}$,Function such that,f^{(n)}(x) = \frac{x}{f(x)^n},"Let $n$ be a fixed positive integer. Find all function $f:(0, \infty) \to \mathbb{R}$ that can be differentiated $n$ times such that $f^{(n)}(x) = \frac{x}{f(x)^n}$ if $f^{(n)}(x)$ is the $n$ -th derivative of $f$ . I tried to differentiate the given identity and I wrote $$f^{(n+1)}(x)=\frac{f(x)^n-nxf(x)^{n-1}f'(x)}{f(x)^{2n}}=\frac{f(x)-nxf'(x)}{f(x)^{n+1}}=\frac{1}{f(x)^n}-\frac{nxf'(x)}{f(x)^{n+1}}$$ I tried to connect this with $f^{(n)}(x)$ but the relations didn't get to anything. Also, if $n=2$ , I am not able to find any example of a function $f$ that satisfies the equation. It appears from the comments that the solutions are very complicated. Does the problem become more easy if we replace $f(x)^n$ with $f^n(x)=(f \circ f \circ...\circ f)(x)$ ?","Let be a fixed positive integer. Find all function that can be differentiated times such that if is the -th derivative of . I tried to differentiate the given identity and I wrote I tried to connect this with but the relations didn't get to anything. Also, if , I am not able to find any example of a function that satisfies the equation. It appears from the comments that the solutions are very complicated. Does the problem become more easy if we replace with ?","n f:(0, \infty) \to \mathbb{R} n f^{(n)}(x) = \frac{x}{f(x)^n} f^{(n)}(x) n f f^{(n+1)}(x)=\frac{f(x)^n-nxf(x)^{n-1}f'(x)}{f(x)^{2n}}=\frac{f(x)-nxf'(x)}{f(x)^{n+1}}=\frac{1}{f(x)^n}-\frac{nxf'(x)}{f(x)^{n+1}} f^{(n)}(x) n=2 f f(x)^n f^n(x)=(f \circ f \circ...\circ f)(x)","['real-analysis', 'calculus', 'ordinary-differential-equations', 'derivatives']"
56,"Weak analogues of gradient, divergence, and curl (collecting examples)","Weak analogues of gradient, divergence, and curl (collecting examples)",,"This question is mostly to help me understand the idea behind the ""weak curl"", but I also hope to accomplish other objectives with this question/post as well, partially inspired from some of the ""proof collecting"" posts I've seen. $1)$ I want to better understand the notion of ""weak curl"" with some examples. $2)$ To hopefully discuss theorems surrounding when the weak versions of gradient, divergence, and curl (if possible), are equal to their strong counterparts and what this implies for solutions for PDEs. $3)$ Collect illustrative examples of weak gradient, weak curl, and weak divergence in any number of dimensions or subsets. Maybe we can consider compact vs. non-compact subsets, upper/lower bounds on these quantities, disconnected spaces, and any related topic of interest. WEAK GRADIENT : let $\Omega\subset \mathbb{R}^n$ , and let $u\in L^1_{loc}(\Omega)$ and $\phi\in C^{\infty}_c(\Omega)$ . The function $v$ is called the ""weak gradient"" of $u$ if $\int_{\Omega}u\phi' d\mu=-\int_{\Omega}v\phi d\mu$ . The ""a-th"" weak gradient is just $\int_{\Omega}uD^{a}\phi d\mu=-(1)^a\int_{\Omega}v\phi d\mu$ $\forall \phi \in C^{\infty}_c(\Omega)$ WEAK DIVERGENCE: v is called the ""weak divergence"" for $u\in L^2(\Omega)$ if we have $\int_{\Omega}u\phi d\mu=-\int_{\Omega}\langle v, \nabla \phi \rangle$ $\forall \phi \in C^{\infty}_c(\Omega)$ EDIT : I also realized I need clarification on the notation $\int_{\Omega}(v,\nabla \phi)$ . I think this means we integrate w.r.t. each vector component of $\nabla \phi$ , so for $\mathbb{R}^2$ we have $\int_{\mathbb{R}}\int_{\mathbb{R}}|v_1 \nabla \phi_1 v_2 \nabla \phi_2|^2 d\mu d\lambda$ . WEAK CURL: This is a bit more delicate, and I am only aware of the definition where $\Omega\subset \mathbb{R}^3$ . First, we need to define $u:\mathbb{R}^2\rightarrow \mathbb{R}^2$ with $u=(u_0, u_1)$ where $u_0\in L^2(\Omega)$ , and $u_1\in L^2(\partial \Omega)$ . Let $n$ be a normal vector to the boundary $\partial \Omega$ . $v$ is called the ""weak curl"" if we have: $v=curl(u)=\langle u_0, (\nabla \times \phi) \rangle + \langle u_1 \times n, \phi \rangle $ $\forall \phi \in C^{\infty}_c(\Omega)$ where the inner product here is the standard $L^2$ inner product. Any interesting remarks/theorems are also welcome.","This question is mostly to help me understand the idea behind the ""weak curl"", but I also hope to accomplish other objectives with this question/post as well, partially inspired from some of the ""proof collecting"" posts I've seen. I want to better understand the notion of ""weak curl"" with some examples. To hopefully discuss theorems surrounding when the weak versions of gradient, divergence, and curl (if possible), are equal to their strong counterparts and what this implies for solutions for PDEs. Collect illustrative examples of weak gradient, weak curl, and weak divergence in any number of dimensions or subsets. Maybe we can consider compact vs. non-compact subsets, upper/lower bounds on these quantities, disconnected spaces, and any related topic of interest. WEAK GRADIENT : let , and let and . The function is called the ""weak gradient"" of if . The ""a-th"" weak gradient is just WEAK DIVERGENCE: v is called the ""weak divergence"" for if we have EDIT : I also realized I need clarification on the notation . I think this means we integrate w.r.t. each vector component of , so for we have . WEAK CURL: This is a bit more delicate, and I am only aware of the definition where . First, we need to define with where , and . Let be a normal vector to the boundary . is called the ""weak curl"" if we have: where the inner product here is the standard inner product. Any interesting remarks/theorems are also welcome.","1) 2) 3) \Omega\subset \mathbb{R}^n u\in L^1_{loc}(\Omega) \phi\in C^{\infty}_c(\Omega) v u \int_{\Omega}u\phi' d\mu=-\int_{\Omega}v\phi d\mu \int_{\Omega}uD^{a}\phi d\mu=-(1)^a\int_{\Omega}v\phi d\mu \forall \phi \in C^{\infty}_c(\Omega) u\in L^2(\Omega) \int_{\Omega}u\phi d\mu=-\int_{\Omega}\langle v, \nabla \phi \rangle \forall \phi \in C^{\infty}_c(\Omega) \int_{\Omega}(v,\nabla \phi) \nabla \phi \mathbb{R}^2 \int_{\mathbb{R}}\int_{\mathbb{R}}|v_1 \nabla \phi_1 v_2 \nabla \phi_2|^2 d\mu d\lambda \Omega\subset \mathbb{R}^3 u:\mathbb{R}^2\rightarrow \mathbb{R}^2 u=(u_0, u_1) u_0\in L^2(\Omega) u_1\in L^2(\partial \Omega) n \partial \Omega v v=curl(u)=\langle u_0, (\nabla \times \phi) \rangle + \langle u_1 \times n, \phi \rangle  \forall \phi \in C^{\infty}_c(\Omega) L^2","['real-analysis', 'vector-fields']"
57,Solving a dual integral equation involving a zeroth-order Bessel function,Solving a dual integral equation involving a zeroth-order Bessel function,,"Consider the following dual integral equations \begin{align} 	 \int_0^\infty  q^3 f_0(q) J_0 (qr) \, \mathrm{d} q &= g(r) \qquad\qquad\quad (0<r<1) , \\  	 \int_0^\infty f_0(q) J_0 (qr) \, \mathrm{d} q &= 0 \,\quad\qquad\qquad\quad (r>1) \, , \end{align} where $$ g(r) = \frac{9}{4\pi} \frac{16h^6-72h^4 r^2 + 18h^2 r^4 +r^6}{(h^2+r^2)^{11/2}} \, .  $$ We search a solution of the integral form \begin{equation}  f_0 (q) = \int_0^1 \lambda(t) \sin (qt) \, \mathrm{d} t \, ,  \label{integralWithHeaviside_sin} \end{equation} which clearly satisfies the integral equation for $r>1$ by making use of the relation \begin{equation}  \int_0^\infty J_0 (qr) \sin(qt) \, \mathrm{d} q = \frac{H(t-r)}{(t^2-r^2)^{1/2}} \, ,   \end{equation} where $H(\cdot)$ denotes Heaviside function. For $0<r<1$ , it follows from three successive integration by parts that \begin{equation}\label{longEqBending} 	  \begin{split} 	   \int_0^\infty & J_0(qr) \, \mathrm{d} q \int_0^1 q^3 \lambda(t)\sin(qt) \, \mathrm{d} t 	   \\ 	   &= \int_0^\infty J_0(qr) \, \mathrm{d} q \bigg( \left(\lambda''(1)-q^2 \lambda(1)\right) \cos(q) + q \lambda'(1) \sin(q)  	   -\lambda''(0) + q^2 \lambda(0) \\ &\qquad- \int_0^1 \lambda'''(t) \cos(qt) \, \mathrm{d} t \bigg) \, . 	  \end{split} 	\end{equation} For the integral on the right-hand side of the latter equation to be convergent, we require that $\lambda(0)=\lambda(1) =\lambda'(1)=0$ . Thus, the latter equation becomes \begin{equation}\label{secondTermBending} 	\int_0^\infty J_0(qr) \, \mathrm{d} q \int_0^1 q^3 \lambda(t)\sin(qt) 	= -  \frac{\lambda''(0)}{r} - \int_0^r \frac{\lambda'''(t) \, \mathrm{d} t}{(r^2-t^2)^{1/2}}   \, , \end{equation} after using identity \begin{equation}  \int_0^\infty J_0 (qr) \cos(qt) \, \mathrm{d} q = \frac{H(r-t)}{(r^2-t^2)^{1/2}} \, . \label{integralWithHeaviside_cos} \end{equation} Thus, the integral equation can be simplified as \begin{equation} 		  \frac{\lambda''(0)}{r} +  \int_0^r \frac{\lambda'''(t) \, \mathrm{d} t}{(r^2-t^2)^{1/2}} = -g(r) \, , \end{equation} By multiplying both members of the latter equation by $r/(s^2-r^2)^{1/2}$ and integrating with respect to $r$ from 0 to $s$ , the resulting equation reads \begin{equation}  \lambda''(s) = - \frac{24 h^3}{\pi^2 } \frac{s(3h^2 - 5s^2)}{(s^2+h^2)^5} \, . \end{equation} The latter equation can now be easily solved.      But the problem is that we have required that $\lambda(0)=\lambda(1)=0$ but also $\lambda'(1)=0$ .     As the final equation is a second order ODE, only 2 boundary conditions are in principal required.      I would be grateful if someone here could be of help and clarify how this could be explained. Thank you!","Consider the following dual integral equations where We search a solution of the integral form which clearly satisfies the integral equation for by making use of the relation where denotes Heaviside function. For , it follows from three successive integration by parts that For the integral on the right-hand side of the latter equation to be convergent, we require that . Thus, the latter equation becomes after using identity Thus, the integral equation can be simplified as By multiplying both members of the latter equation by and integrating with respect to from 0 to , the resulting equation reads The latter equation can now be easily solved.      But the problem is that we have required that but also .     As the final equation is a second order ODE, only 2 boundary conditions are in principal required.      I would be grateful if someone here could be of help and clarify how this could be explained. Thank you!","\begin{align}
	 \int_0^\infty  q^3 f_0(q) J_0 (qr) \, \mathrm{d} q &= g(r) \qquad\qquad\quad (0<r<1) , \\ 
	 \int_0^\infty f_0(q) J_0 (qr) \, \mathrm{d} q &= 0 \,\quad\qquad\qquad\quad (r>1) \, ,
\end{align} 
g(r) = \frac{9}{4\pi} \frac{16h^6-72h^4 r^2 + 18h^2 r^4 +r^6}{(h^2+r^2)^{11/2}} \, . 
 \begin{equation}
 f_0 (q) = \int_0^1 \lambda(t) \sin (qt) \, \mathrm{d} t \, , 
\label{integralWithHeaviside_sin}
\end{equation} r>1 \begin{equation}
 \int_0^\infty J_0 (qr) \sin(qt) \, \mathrm{d} q = \frac{H(t-r)}{(t^2-r^2)^{1/2}} \, ,  
\end{equation} H(\cdot) 0<r<1 \begin{equation}\label{longEqBending}
	  \begin{split}
	   \int_0^\infty & J_0(qr) \, \mathrm{d} q \int_0^1 q^3 \lambda(t)\sin(qt) \, \mathrm{d} t
	   \\
	   &= \int_0^\infty J_0(qr) \, \mathrm{d} q \bigg( \left(\lambda''(1)-q^2 \lambda(1)\right) \cos(q) + q \lambda'(1) \sin(q) 
	   -\lambda''(0) + q^2 \lambda(0) \\
&\qquad- \int_0^1 \lambda'''(t) \cos(qt) \, \mathrm{d} t \bigg) \, .
	  \end{split}
	\end{equation} \lambda(0)=\lambda(1) =\lambda'(1)=0 \begin{equation}\label{secondTermBending}
	\int_0^\infty J_0(qr) \, \mathrm{d} q \int_0^1 q^3 \lambda(t)\sin(qt)
	= -  \frac{\lambda''(0)}{r} - \int_0^r \frac{\lambda'''(t) \, \mathrm{d} t}{(r^2-t^2)^{1/2}}   \, ,
\end{equation} \begin{equation}
 \int_0^\infty J_0 (qr) \cos(qt) \, \mathrm{d} q = \frac{H(r-t)}{(r^2-t^2)^{1/2}} \, . \label{integralWithHeaviside_cos}
\end{equation} \begin{equation}
		  \frac{\lambda''(0)}{r} +  \int_0^r \frac{\lambda'''(t) \, \mathrm{d} t}{(r^2-t^2)^{1/2}} = -g(r) \, ,
\end{equation} r/(s^2-r^2)^{1/2} r s \begin{equation}
 \lambda''(s) = - \frac{24 h^3}{\pi^2 } \frac{s(3h^2 - 5s^2)}{(s^2+h^2)^5} \, .
\end{equation} \lambda(0)=\lambda(1)=0 \lambda'(1)=0","['real-analysis', 'integration', 'complex-analysis', 'ordinary-differential-equations', 'improper-integrals']"
58,How to prove $\sum_{n=1}^{\infty} \frac{\sin n\theta \sin \sqrt{n}}{n}$ is convergent or not,How to prove  is convergent or not,\sum_{n=1}^{\infty} \frac{\sin n\theta \sin \sqrt{n}}{n},"I want to check whether $$ \sum_{n=1}^{\infty} \frac{\sin n\theta \sin \sqrt{n}}{n} $$ is convergent or not. $\theta$ is a real number.  What I know is $$ |\sum_{n=1}^{N}\sin n\theta| = |\frac{\cos \frac{\theta}{2} - \cos(N+\frac{1}{2})\theta}{2\sin\frac{\theta}{2}}| \leq \frac{1}{|\sin \frac{\theta}{2}|}$$ for $\theta\neq 2k\pi$ . Let's assume $\theta \neq 2k\pi$ . So by Dirichlet test, $\sum_{n=1}^{\infty} \frac{\sin n\theta}{n}$ is convergent. But I don't quite know how to solve the original one. Any hint or something? Thank you so much!","I want to check whether is convergent or not. is a real number.  What I know is for . Let's assume . So by Dirichlet test, is convergent. But I don't quite know how to solve the original one. Any hint or something? Thank you so much!","
\sum_{n=1}^{\infty} \frac{\sin n\theta \sin \sqrt{n}}{n}
 \theta 
|\sum_{n=1}^{N}\sin n\theta| = |\frac{\cos \frac{\theta}{2} - \cos(N+\frac{1}{2})\theta}{2\sin\frac{\theta}{2}}| \leq \frac{1}{|\sin \frac{\theta}{2}|} \theta\neq 2k\pi \theta \neq 2k\pi \sum_{n=1}^{\infty} \frac{\sin n\theta}{n}","['real-analysis', 'sequences-and-series']"
59,"Check simple proof that $\lim\limits_{s\to0^+}\sum\limits_{n=1}^\infty(-1)^nf(n)^{-s}=-\frac12$ if $f>0$, $f''\le0$ and $f(+\infty)=+\infty$","Check simple proof that  if ,  and",\lim\limits_{s\to0^+}\sum\limits_{n=1}^\infty(-1)^nf(n)^{-s}=-\frac12 f>0 f''\le0 f(+\infty)=+\infty,"$f:[1,+\infty)\to \mathbb{R}_+$ satisfies  $\ f''\leq0,\ f(+\infty)=+\infty $. Prove    $$\lim_{s\to0^+}\sum_{n=1}^\infty (-1)^n[f(n)]^{-s}=-\frac{1}{2}$$ Some of my friends showed me this question and declared it's difficult. However I've found a quite simple proof. Could you help me examine if it's correct? Proof: Obviously, $\ f$ is strictly increasing. For $s\in(0,1)$, due to Leibniz's Test, $\sum_{n=1}^\infty (-1)^n[f(n)]^{-s}$ converges and we denote it by $(-1)S(s)$.  $$f''\leq0\ \&\ x^{-s}\ \text{is convex}\Rightarrow\ x^{-s}\circ f=[f]^{-s}\ \text{is convex}$$ $$\Rightarrow\ f^{-s}(n)-f^{-s}(n+1)\geq f^{-s}(n+1)-f^{-s}(n+2),\quad n\in\mathbb{N}_+$$ Let $a_n^s$ represents $f^{-s}(n)$, now we have, $$S(s)=a_1^s-a_2^s+a_3^s-a_4^s\cdots\ .$$ Define $S\tilde(s)$ as $$S\tilde(s)=a_2^s-a_3^s+a_4^s-a_5^s\cdots\ .$$ Notice that  $$S(s)\geq S\tilde(s),\ S(s)\leq S\tilde(s)+a_1^s-a_2^s, \ S(s)+S\tilde(s)=a_1^s,$$ which implies $$a_1^s/2\leq S(s)\leq a_1^s-a_2^s/2\ .$$ Let $s\to 0^+$, now we proved $\lim_{s\to 0^+}S(s)=1/2$, that is the conclusion.","$f:[1,+\infty)\to \mathbb{R}_+$ satisfies  $\ f''\leq0,\ f(+\infty)=+\infty $. Prove    $$\lim_{s\to0^+}\sum_{n=1}^\infty (-1)^n[f(n)]^{-s}=-\frac{1}{2}$$ Some of my friends showed me this question and declared it's difficult. However I've found a quite simple proof. Could you help me examine if it's correct? Proof: Obviously, $\ f$ is strictly increasing. For $s\in(0,1)$, due to Leibniz's Test, $\sum_{n=1}^\infty (-1)^n[f(n)]^{-s}$ converges and we denote it by $(-1)S(s)$.  $$f''\leq0\ \&\ x^{-s}\ \text{is convex}\Rightarrow\ x^{-s}\circ f=[f]^{-s}\ \text{is convex}$$ $$\Rightarrow\ f^{-s}(n)-f^{-s}(n+1)\geq f^{-s}(n+1)-f^{-s}(n+2),\quad n\in\mathbb{N}_+$$ Let $a_n^s$ represents $f^{-s}(n)$, now we have, $$S(s)=a_1^s-a_2^s+a_3^s-a_4^s\cdots\ .$$ Define $S\tilde(s)$ as $$S\tilde(s)=a_2^s-a_3^s+a_4^s-a_5^s\cdots\ .$$ Notice that  $$S(s)\geq S\tilde(s),\ S(s)\leq S\tilde(s)+a_1^s-a_2^s, \ S(s)+S\tilde(s)=a_1^s,$$ which implies $$a_1^s/2\leq S(s)\leq a_1^s-a_2^s/2\ .$$ Let $s\to 0^+$, now we proved $\lim_{s\to 0^+}S(s)=1/2$, that is the conclusion.",,"['real-analysis', 'sequences-and-series', 'proof-verification']"
60,One-sided Taylor's expansion,One-sided Taylor's expansion,,"Suppose a function $f(t)$ is defined only on $[t_{0},\infty)$. Suppose all ""right'' derivatives $f^{(n)}(t)$ exist, that is,  $$f^{(1)}(t_{0})=\lim_{\delta\rightarrow 0+}\frac{f(t_{0})-f(t_{0}+\delta)}{\delta}<\infty,$$ and  in general, $$f^{(n)}(t_{0})=\lim_{\delta\rightarrow 0+}\frac{f^{(n-1)}(t_{0})-f^{(n-1)}(t_{0}+\delta)}{\delta}<\infty,$$ for every $n\geq 1$. Is Taylor's expansion, $$     f(t_{0})+\sum_{n=1}^{\infty} \frac{f^{(n)}(t_{0})}{n!}(t-t_{0})^n, $$ defined on $[t_{0},t_{0}+\varepsilon)$ for some $\varepsilon$? I am not asking whether the expansion converges to $f(t)$ (that is a different question) in a neighborhood of $t_{0}$. I am just asking if $f(t)$ can be expanded only to the right hand side without the left hand side having been defined. Intuitively one would think so, but I cannot find any literature items specifically on this point. All Taylor's expansions seem to assume all derivatives exist in an open neighborhood of $t_{0}$. Please enlighten me.","Suppose a function $f(t)$ is defined only on $[t_{0},\infty)$. Suppose all ""right'' derivatives $f^{(n)}(t)$ exist, that is,  $$f^{(1)}(t_{0})=\lim_{\delta\rightarrow 0+}\frac{f(t_{0})-f(t_{0}+\delta)}{\delta}<\infty,$$ and  in general, $$f^{(n)}(t_{0})=\lim_{\delta\rightarrow 0+}\frac{f^{(n-1)}(t_{0})-f^{(n-1)}(t_{0}+\delta)}{\delta}<\infty,$$ for every $n\geq 1$. Is Taylor's expansion, $$     f(t_{0})+\sum_{n=1}^{\infty} \frac{f^{(n)}(t_{0})}{n!}(t-t_{0})^n, $$ defined on $[t_{0},t_{0}+\varepsilon)$ for some $\varepsilon$? I am not asking whether the expansion converges to $f(t)$ (that is a different question) in a neighborhood of $t_{0}$. I am just asking if $f(t)$ can be expanded only to the right hand side without the left hand side having been defined. Intuitively one would think so, but I cannot find any literature items specifically on this point. All Taylor's expansions seem to assume all derivatives exist in an open neighborhood of $t_{0}$. Please enlighten me.",,['real-analysis']
61,Let $X$ be a separable Banach space. Let $Y$ be the space formed by adding a point at infinity. Is $Y$ homeomorphic to the unit sphere of $X$?,Let  be a separable Banach space. Let  be the space formed by adding a point at infinity. Is  homeomorphic to the unit sphere of ?,X Y Y X,"More specifically, let $Y= X \cup \{\infty\}$ and declare open sets to be the usual open sets in $X$ together with those that are of the form $(X-C)\cup \{\infty\}$, where $C$ is norm closed and norm bounded. So $Y$ is formed in the same way that the one point compactification is when $X$ is finite dimensional. Clearly if $X = \Bbb R^n$ we have $Y \cong S^n$, which motivates the question. Note that all separable Banach spaces are homeomorphic, so I think $Y$ should not depend on which Banach space is chosen (which makes the Hilbert space the natural candidate to try to work with.) Any references to a paper/text with a proof or a sketch of a proof (or disproof) are greatly appreciated. Thanks in advance!","More specifically, let $Y= X \cup \{\infty\}$ and declare open sets to be the usual open sets in $X$ together with those that are of the form $(X-C)\cup \{\infty\}$, where $C$ is norm closed and norm bounded. So $Y$ is formed in the same way that the one point compactification is when $X$ is finite dimensional. Clearly if $X = \Bbb R^n$ we have $Y \cong S^n$, which motivates the question. Note that all separable Banach spaces are homeomorphic, so I think $Y$ should not depend on which Banach space is chosen (which makes the Hilbert space the natural candidate to try to work with.) Any references to a paper/text with a proof or a sketch of a proof (or disproof) are greatly appreciated. Thanks in advance!",,"['real-analysis', 'general-topology', 'functional-analysis']"
62,"The difference between two periodic functions converges to zero, is this two functions identical?","The difference between two periodic functions converges to zero, is this two functions identical?",,"If $f(x)$ and $g(x)$ are two periodic functions, that is, $f(x+T_1)=f(x)$ and $g(x+T_2)=g(x)$ for every $x \in \Bbb R$ . Now that $\displaystyle\lim_{x\to\infty}(f(x)-g(x))=0$ . Conjecture: $f(x) \equiv g(x)$ .","If and are two periodic functions, that is, and for every . Now that . Conjecture: .",f(x) g(x) f(x+T_1)=f(x) g(x+T_2)=g(x) x \in \Bbb R \displaystyle\lim_{x\to\infty}(f(x)-g(x))=0 f(x) \equiv g(x),"['calculus', 'limits', 'periodic-functions']"
63,How to derive this continued fraction formula of $\ln(2)$?,How to derive this continued fraction formula of ?,\ln(2),"On wiki page Naturel logarithhm of 2, other representations section it lists $$\ln\,2=\cfrac 1{1+\cfrac 1{2+\cfrac 1{3+\cfrac 2{2+\cfrac 2{5+\cfrac 3{2+\cfrac 3{7+\cfrac 4{2+\cfrac 4{9+\cdots}}}}}}}}}$$ How to derive this? I checked Euler's continued fraction formula page but still couldn't figure out how the $a_i$ and $b_i$ are derived. A math stackexchange answer mentioned $$\;\left[\matrix {n_k\\d_k}\right]=\left[\matrix {n_{k-1}\;n_{k-2}\\d_{k-1}\;d_{k-2}}\right]\left[\matrix {b_k\\a_k}\right],\quad\left[\matrix {n_1\\d_1}\right]=\left[\matrix {b_0\,b_1+a_1\\b_1}\right]=\left[\matrix {1\\1}\right],\;\left[\matrix {n_0\\d_0}\right]=\left[\matrix {b_0\\1}\right]=\left[\matrix {0\\1}\right]$$ but didn't explain where this is from.","On wiki page Naturel logarithhm of 2, other representations section it lists $$\ln\,2=\cfrac 1{1+\cfrac 1{2+\cfrac 1{3+\cfrac 2{2+\cfrac 2{5+\cfrac 3{2+\cfrac 3{7+\cfrac 4{2+\cfrac 4{9+\cdots}}}}}}}}}$$ How to derive this? I checked Euler's continued fraction formula page but still couldn't figure out how the $a_i$ and $b_i$ are derived. A math stackexchange answer mentioned $$\;\left[\matrix {n_k\\d_k}\right]=\left[\matrix {n_{k-1}\;n_{k-2}\\d_{k-1}\;d_{k-2}}\right]\left[\matrix {b_k\\a_k}\right],\quad\left[\matrix {n_1\\d_1}\right]=\left[\matrix {b_0\,b_1+a_1\\b_1}\right]=\left[\matrix {1\\1}\right],\;\left[\matrix {n_0\\d_0}\right]=\left[\matrix {b_0\\1}\right]=\left[\matrix {0\\1}\right]$$ but didn't explain where this is from.",,"['real-analysis', 'continued-fractions']"
64,Heisenberg's inequality,Heisenberg's inequality,,"The Heisenberg's inequality in $\Bbb{R}$ reads $$\|f\|_{L^2}^4\leq \int_{\Bbb{R}}x^2f(x)^2dx\int_{\Bbb{R}}\xi^2\hat{f}(\xi)^2d\xi$$ where by $\hat{f}$ we refer to the Fourier transform of $f$. The aformentioned inequality refered to as Heisenberg's since it is in consistency with the Heisenberg uncertainty principle which states that $$\sigma_x\sigma_\xi\geq \frac{\hbar}{2}$$ where $\hbar$ is the reduced Planck constant, $h/(2\pi)$). A simple interpretation of that is: $f$ and $\hat{f}$ can't both be concentrated in a small region . Now, consider the following which holds in periodic domain $\Bbb{T}$ $$\|f\|_{L^2(\Bbb{T})}^2=\sum_{k\in \mathbb{Z}}|c_k(f)|^2\leq \sup_{k\in \Bbb{Z}}|c_k(f)|\sum_{k\in \mathbb{Z}}|c_k(f)|\leq \underbrace{\|f\|_{L^1(\Bbb{T})}}_{space}\times \underbrace{\sum_{k\in \mathbb{Z}}|c_k(f)|}_{frequency}$$ does the aformentioned has anything to do with Heisenberg inequality and its interpretation since it involves as i indicated above the $L^2$ norm of $f$ controlled by a measurement in space multiplied by its counterpart in frequency space?","The Heisenberg's inequality in $\Bbb{R}$ reads $$\|f\|_{L^2}^4\leq \int_{\Bbb{R}}x^2f(x)^2dx\int_{\Bbb{R}}\xi^2\hat{f}(\xi)^2d\xi$$ where by $\hat{f}$ we refer to the Fourier transform of $f$. The aformentioned inequality refered to as Heisenberg's since it is in consistency with the Heisenberg uncertainty principle which states that $$\sigma_x\sigma_\xi\geq \frac{\hbar}{2}$$ where $\hbar$ is the reduced Planck constant, $h/(2\pi)$). A simple interpretation of that is: $f$ and $\hat{f}$ can't both be concentrated in a small region . Now, consider the following which holds in periodic domain $\Bbb{T}$ $$\|f\|_{L^2(\Bbb{T})}^2=\sum_{k\in \mathbb{Z}}|c_k(f)|^2\leq \sup_{k\in \Bbb{Z}}|c_k(f)|\sum_{k\in \mathbb{Z}}|c_k(f)|\leq \underbrace{\|f\|_{L^1(\Bbb{T})}}_{space}\times \underbrace{\sum_{k\in \mathbb{Z}}|c_k(f)|}_{frequency}$$ does the aformentioned has anything to do with Heisenberg inequality and its interpretation since it involves as i indicated above the $L^2$ norm of $f$ controlled by a measurement in space multiplied by its counterpart in frequency space?",,"['real-analysis', 'inequality']"
65,Proving the trichotomy of order for the natural numbers,Proving the trichotomy of order for the natural numbers,,"Is my proof correct? The trichotomy of order for natural numbers states: Let $a,b$ be natural numbers. Then exactly one of the following statements is true: I. $a < b$ II. $a = b$ III. $a > b$ Proof: If $a < b$ then by definition $a \neq b$. If $a > b$ then $a \neq b$ by definition as well. Therefore it is not possible to have I and II be true at the same time, nor II and III be true at the same time, nor I, II, and III be true at the same time. If $a < b$ and $a > b$ then it means we can write $a = b + m$ and $b = a + n$ for natural numbers $n,m$ where $a \neq b$. By substitution, $a = a + n + m$, which by cancellation law is $0 = n + m$. This also means $n = m = 0$, suggesting that $a = b$, a contradiction. Therefore it is not possible for I and III to be true at the same time. Now we will show that at least one of I, II, or III is true via induction on $a$. Base case: Let $a=0$. If $b=0$, then $a=b$, statement II, is true. If $b \neq 0$, then statement I is true, $0 < b$, since we can write $0 + m = m = b$ where $0 \neq b$. Inductive step: Suppose either $a < b$, $a = b$, or $a > b$ is true. We need to show that $S(a) < b$, $S(a) = b$, or $S(a) > b$ is true. If $a > b$, then $S(a) > b$ is true, since $a = b + m$ where $a \neq b$, so we also have $S(a) = S(b + m) = b + S(m)$ which satisfies the definition for $S(a) > b$ as long as $S(a) \neq b$. And $S(a) \neq b$ is true, since if it weren't, we'd have $b = b + S(m)$ or $0 = S(m)$ by additive identity and cancellation law, and $0$ is not a successor of any natural number. If $a = b$, then $S(a) > b$ is true. This is because if $a = b$ then $S(a) = S(b) = S(b + 0) = b + S(0)$ which satisfies the definition for $S(a) > b$ as long as $S(a) \neq b$. And $S(a) \neq b$ is still true, since if it weren't, we'd have $b = b + S(0)$ or $0 = S(0)$ by additive identity and cancellation law, and $0$ is not a successor of any natural number. If $a < b$, then either $S(a) < b$ or $S(a) = b$ is true. We can write $b = a + m$ for some $m$ where $a \neq b$ (so $m \neq 0$). Suppose $m=S(0)$. Then $b = a + S(0) = S(a + 0) = S(a)$. If $m \neq S(0)$, then $S(a) \neq b$, so it suffices to show that $b = S(a) + k$ for some $k$. If we let $S(k) = m$ (which is valid since we have $m \neq 0$), then $b = a + S(k) = S(a + k) = S(a) + k$ holds. This closes the induction.","Is my proof correct? The trichotomy of order for natural numbers states: Let $a,b$ be natural numbers. Then exactly one of the following statements is true: I. $a < b$ II. $a = b$ III. $a > b$ Proof: If $a < b$ then by definition $a \neq b$. If $a > b$ then $a \neq b$ by definition as well. Therefore it is not possible to have I and II be true at the same time, nor II and III be true at the same time, nor I, II, and III be true at the same time. If $a < b$ and $a > b$ then it means we can write $a = b + m$ and $b = a + n$ for natural numbers $n,m$ where $a \neq b$. By substitution, $a = a + n + m$, which by cancellation law is $0 = n + m$. This also means $n = m = 0$, suggesting that $a = b$, a contradiction. Therefore it is not possible for I and III to be true at the same time. Now we will show that at least one of I, II, or III is true via induction on $a$. Base case: Let $a=0$. If $b=0$, then $a=b$, statement II, is true. If $b \neq 0$, then statement I is true, $0 < b$, since we can write $0 + m = m = b$ where $0 \neq b$. Inductive step: Suppose either $a < b$, $a = b$, or $a > b$ is true. We need to show that $S(a) < b$, $S(a) = b$, or $S(a) > b$ is true. If $a > b$, then $S(a) > b$ is true, since $a = b + m$ where $a \neq b$, so we also have $S(a) = S(b + m) = b + S(m)$ which satisfies the definition for $S(a) > b$ as long as $S(a) \neq b$. And $S(a) \neq b$ is true, since if it weren't, we'd have $b = b + S(m)$ or $0 = S(m)$ by additive identity and cancellation law, and $0$ is not a successor of any natural number. If $a = b$, then $S(a) > b$ is true. This is because if $a = b$ then $S(a) = S(b) = S(b + 0) = b + S(0)$ which satisfies the definition for $S(a) > b$ as long as $S(a) \neq b$. And $S(a) \neq b$ is still true, since if it weren't, we'd have $b = b + S(0)$ or $0 = S(0)$ by additive identity and cancellation law, and $0$ is not a successor of any natural number. If $a < b$, then either $S(a) < b$ or $S(a) = b$ is true. We can write $b = a + m$ for some $m$ where $a \neq b$ (so $m \neq 0$). Suppose $m=S(0)$. Then $b = a + S(0) = S(a + 0) = S(a)$. If $m \neq S(0)$, then $S(a) \neq b$, so it suffices to show that $b = S(a) + k$ for some $k$. If we let $S(k) = m$ (which is valid since we have $m \neq 0$), then $b = a + S(k) = S(a + k) = S(a) + k$ holds. This closes the induction.",,"['real-analysis', 'inequality', 'proof-verification', 'logic', 'peano-axioms']"
66,Every k-cell is compact / alternative proof,Every k-cell is compact / alternative proof,,"I have become really interested in trying to prove directly things that are more easily proved by contradiction. The below seems to be a good example of this. Rudin's proof makes perfect sense to me. In fact I think it is a very elegant proof. But nonetheless I am very much interested in a constructive proof. Essentially Rudin proves that every k-cell is compact by assuming the opposite: that there is an open cover of a k-cell $I$ which does not contain a finite subcover. I would like to assume $\{G_\alpha \}$ is an arbitrary open cover of $I$ and show directly that $G_\alpha$ must have a finite subcover. Here is an outline of Rudin's proof(Theorem 2.40): Every k-cell is compact. Proof. Let $I$ be a k-cell consisting of all points $x = (x_1, \dots, x_k)$ such that $a_j \leq x_j \leq b_j$ for $1 \leq j \leq k$. Put $\delta = \{ \sum\limits_1^k(b_j - a_j)^2\}^{1/2}$ Then $|x-y| \leq \delta$ if $x, y \in I$. Suppose there is an open cover $\{G_\alpha \}$ which contains no finite subcover. Put $c_j = \frac{a_j + b_j}{2}$. The intervals $[a_j, c_j]$ and $[c_j, b_j]$ determine $2^k$ k-cells whose union is $I$. At least one of these subsets of $I$, say $I_1$, cannot be covered by any finite subcollection of $\{ G_\alpha \}$. So we begin again with the k-cell $I_1$ and subdivide further to achieve a sequence of k-cells such that (a) $I \supset I_1 \supset I_2 \supset I_3 \supset \dots$ (b) $I_n$ is not covered by any finite subcollection of $G_\alpha$ (c) If $x \in I_n$ and $y \in I_n$ then $|y-x| \leq 2^{-n}\delta$ Hence there is a point $x^* \in \cap I_n$ and for some $\alpha$ $x^* \in G_\alpha$. Since $G$ is open there is a neighborhood $N_r(x^*) \subset G_\alpha$. If $n$ is large enough that $2^{-n}\delta < r$ then given $p \in I_n$ $|x^* - p | < 2^{-n}\delta < r \implies p \in N_r(x^*) \implies I_n \subset G_\alpha$ contradicting the fact that $I_n$ cannot be covered by a finite subcollection of $\{ G_\alpha\}$","I have become really interested in trying to prove directly things that are more easily proved by contradiction. The below seems to be a good example of this. Rudin's proof makes perfect sense to me. In fact I think it is a very elegant proof. But nonetheless I am very much interested in a constructive proof. Essentially Rudin proves that every k-cell is compact by assuming the opposite: that there is an open cover of a k-cell $I$ which does not contain a finite subcover. I would like to assume $\{G_\alpha \}$ is an arbitrary open cover of $I$ and show directly that $G_\alpha$ must have a finite subcover. Here is an outline of Rudin's proof(Theorem 2.40): Every k-cell is compact. Proof. Let $I$ be a k-cell consisting of all points $x = (x_1, \dots, x_k)$ such that $a_j \leq x_j \leq b_j$ for $1 \leq j \leq k$. Put $\delta = \{ \sum\limits_1^k(b_j - a_j)^2\}^{1/2}$ Then $|x-y| \leq \delta$ if $x, y \in I$. Suppose there is an open cover $\{G_\alpha \}$ which contains no finite subcover. Put $c_j = \frac{a_j + b_j}{2}$. The intervals $[a_j, c_j]$ and $[c_j, b_j]$ determine $2^k$ k-cells whose union is $I$. At least one of these subsets of $I$, say $I_1$, cannot be covered by any finite subcollection of $\{ G_\alpha \}$. So we begin again with the k-cell $I_1$ and subdivide further to achieve a sequence of k-cells such that (a) $I \supset I_1 \supset I_2 \supset I_3 \supset \dots$ (b) $I_n$ is not covered by any finite subcollection of $G_\alpha$ (c) If $x \in I_n$ and $y \in I_n$ then $|y-x| \leq 2^{-n}\delta$ Hence there is a point $x^* \in \cap I_n$ and for some $\alpha$ $x^* \in G_\alpha$. Since $G$ is open there is a neighborhood $N_r(x^*) \subset G_\alpha$. If $n$ is large enough that $2^{-n}\delta < r$ then given $p \in I_n$ $|x^* - p | < 2^{-n}\delta < r \implies p \in N_r(x^*) \implies I_n \subset G_\alpha$ contradicting the fact that $I_n$ cannot be covered by a finite subcollection of $\{ G_\alpha\}$",,"['real-analysis', 'general-topology', 'alternative-proof']"
67,A naive conjecture about Taylor series convergence,A naive conjecture about Taylor series convergence,,"Conjecture. Let $f$ be a real (or complex) analytic function defined on some open subset $U$ of real (complex) numbers and assume that $p,q,x\in U$ are such that $x\in B(p,r_p)\cap B(q, r_q)$, where $r_p$ and $r_q$ are radiuses of convergence of $f$ expanded at $p$ and $q$ respectively and where $B$ stands for an open ball. If   $$\frac{|x-p|}{r_p}<\frac{|x-q|}{r_q},$$   then $P_n = o(Q_n)$, where   $$P_n = \left|f(x) - \sum_{i=0}^n\frac{f^{(i)}(p)}{i!}(x-p)^i\right|\quad\text{and}\quad  Q_n = \left|f(x) - \sum_{i=0}^n\frac{f^{(i)}(q)}{i!}(x-q)^i\right|$$ I came up with this conjecture while I was playing with an approximation of the natural logarithm using Taylor series. So, below I present the path of reasoning which lead me to this conjecture. I put a more detailed anaysis in this jupyter notebook . If $p>0$ and $x\in (0, 2p)$, then $$\ln(x)=\ln(p) - \sum_{i=1}^\infty\frac{1}{i}\left(\frac{p-x}{p}\right)^i.$$ By looking at the above formula, we notice that the very first term is $\ln(p)$. Let us say that we want to approximate $\ln(x)$, so we forbid ourselves to use logarithm in this series. Hence, we constrain ourselves to $p$ of the form $e^K$, where $K$ can be any integer. Problem. For a given point $x>0$, find $p=e^K$ such that Taylor series of $\ln$ expanded at $p$ converges to $\ln(x)$ in the fastest way. We know that if $x$ close to an edge of the interval of convergence, then the Taylor series converges very slowly. Thus, using this basic fact, I found the answer heuristically. In a nutshell, the idea is to compare distance from $x$ to $p$ but also take into account the size of the interval of convergence. Thus, we just look for best $p$ such that $$\frac{|x-p|}{r_p}$$ is the smallest, where $r_p$ is the radius of the convergence of $\ln$ a $p$. So, if $x\in (e^K, 2e^K)$, then we should compare  $$\frac{x-e^K}{e^K}\quad\text{with}\quad\frac{e^{K+1}-x}{e^{K+1}}$$ and put $p = e^K$ if the first one is smaller and put $p = e^{K+1}$ otherwise. For $x = C_0e^K$, where $C_0 = \frac{2e}{1+e}$, we have the equality of the above two numbers. Thus, in order to compute $\ln(x)$ using Taylor series, it is best to start with $p=e^K$, where $K$ is such that $x\in[C_0e^{K−1},C_0e^K)$. I have done some numerical tests whether this $C_0$ is the optimal one and it looks like it is. You are welcome to see the details in this already mentioned jupyter notebook . So, I wonder if this property generalizes to an arbitrarty analytic function and the conjecture is a manifestation of my wonderings.","Conjecture. Let $f$ be a real (or complex) analytic function defined on some open subset $U$ of real (complex) numbers and assume that $p,q,x\in U$ are such that $x\in B(p,r_p)\cap B(q, r_q)$, where $r_p$ and $r_q$ are radiuses of convergence of $f$ expanded at $p$ and $q$ respectively and where $B$ stands for an open ball. If   $$\frac{|x-p|}{r_p}<\frac{|x-q|}{r_q},$$   then $P_n = o(Q_n)$, where   $$P_n = \left|f(x) - \sum_{i=0}^n\frac{f^{(i)}(p)}{i!}(x-p)^i\right|\quad\text{and}\quad  Q_n = \left|f(x) - \sum_{i=0}^n\frac{f^{(i)}(q)}{i!}(x-q)^i\right|$$ I came up with this conjecture while I was playing with an approximation of the natural logarithm using Taylor series. So, below I present the path of reasoning which lead me to this conjecture. I put a more detailed anaysis in this jupyter notebook . If $p>0$ and $x\in (0, 2p)$, then $$\ln(x)=\ln(p) - \sum_{i=1}^\infty\frac{1}{i}\left(\frac{p-x}{p}\right)^i.$$ By looking at the above formula, we notice that the very first term is $\ln(p)$. Let us say that we want to approximate $\ln(x)$, so we forbid ourselves to use logarithm in this series. Hence, we constrain ourselves to $p$ of the form $e^K$, where $K$ can be any integer. Problem. For a given point $x>0$, find $p=e^K$ such that Taylor series of $\ln$ expanded at $p$ converges to $\ln(x)$ in the fastest way. We know that if $x$ close to an edge of the interval of convergence, then the Taylor series converges very slowly. Thus, using this basic fact, I found the answer heuristically. In a nutshell, the idea is to compare distance from $x$ to $p$ but also take into account the size of the interval of convergence. Thus, we just look for best $p$ such that $$\frac{|x-p|}{r_p}$$ is the smallest, where $r_p$ is the radius of the convergence of $\ln$ a $p$. So, if $x\in (e^K, 2e^K)$, then we should compare  $$\frac{x-e^K}{e^K}\quad\text{with}\quad\frac{e^{K+1}-x}{e^{K+1}}$$ and put $p = e^K$ if the first one is smaller and put $p = e^{K+1}$ otherwise. For $x = C_0e^K$, where $C_0 = \frac{2e}{1+e}$, we have the equality of the above two numbers. Thus, in order to compute $\ln(x)$ using Taylor series, it is best to start with $p=e^K$, where $K$ is such that $x\in[C_0e^{K−1},C_0e^K)$. I have done some numerical tests whether this $C_0$ is the optimal one and it looks like it is. You are welcome to see the details in this already mentioned jupyter notebook . So, I wonder if this property generalizes to an arbitrarty analytic function and the conjecture is a manifestation of my wonderings.",,"['real-analysis', 'complex-analysis', 'numerical-methods', 'asymptotics', 'taylor-expansion']"
68,Characterization of sets of differentiability,Characterization of sets of differentiability,,"If $f : \mathbb{R} \to \mathbb{R}$, define $C(f) = \{ x : f \text{ is continuous at } x \}$ and $D(f) = \{ x : f \text{ is differentiable at } x \}$. I have seen it proved that: $C(f)$ is a $G_\delta$ set. For any $G_\delta$ set $A \subset \mathbb{R}$, there exists a function $f : \mathbb{R} \to \mathbb{R}$ such that $C(f)=A$. I suspect there is a related characterization for $D(f)$. I see that a related question was asked at Continuous functions are differentiable on a measurable set? , where the accepted answer implies that $D(f)$ is a $G_{\delta \sigma \delta}$ set. Is this optimal, that is, does there exist $f$ such that $D(f)$ is $G_{\delta \sigma \delta}$ and not $G_{\delta \sigma}$? If so, can an example be given? Conversely, given any $G_{\delta \sigma \delta}$ set $A \subset \mathbb{R}$, does there exist $f$ such that $D(f)=A$?","If $f : \mathbb{R} \to \mathbb{R}$, define $C(f) = \{ x : f \text{ is continuous at } x \}$ and $D(f) = \{ x : f \text{ is differentiable at } x \}$. I have seen it proved that: $C(f)$ is a $G_\delta$ set. For any $G_\delta$ set $A \subset \mathbb{R}$, there exists a function $f : \mathbb{R} \to \mathbb{R}$ such that $C(f)=A$. I suspect there is a related characterization for $D(f)$. I see that a related question was asked at Continuous functions are differentiable on a measurable set? , where the accepted answer implies that $D(f)$ is a $G_{\delta \sigma \delta}$ set. Is this optimal, that is, does there exist $f$ such that $D(f)$ is $G_{\delta \sigma \delta}$ and not $G_{\delta \sigma}$? If so, can an example be given? Conversely, given any $G_{\delta \sigma \delta}$ set $A \subset \mathbb{R}$, does there exist $f$ such that $D(f)=A$?",,['real-analysis']
69,"Are absolutely convergent series ""many"" or ""few"" compared to conditionally convergent series?","Are absolutely convergent series ""many"" or ""few"" compared to conditionally convergent series?",,"We can identify absolutely convergent series with the $l^1$ space and conditionally convergent series with a subspace of $c_0 = \{\{a_n\} \in l^\infty : a_n \to 0\}$ Since $l^1$ contains all finite sequences, it is dense in $c_0$, so in this sense absolutely convergent series are dense, or ""common"" among all series. However, this result is not terribly illuminating, since finite sequences are also dense in $c_0$. Are there other ways to look at the relationship between absolutely and conditionally convergent series? Maybe absolutely convergent series are meager, open or dense when we consider a more appropriate topology on the set of convergent sequences.","We can identify absolutely convergent series with the $l^1$ space and conditionally convergent series with a subspace of $c_0 = \{\{a_n\} \in l^\infty : a_n \to 0\}$ Since $l^1$ contains all finite sequences, it is dense in $c_0$, so in this sense absolutely convergent series are dense, or ""common"" among all series. However, this result is not terribly illuminating, since finite sequences are also dense in $c_0$. Are there other ways to look at the relationship between absolutely and conditionally convergent series? Maybe absolutely convergent series are meager, open or dense when we consider a more appropriate topology on the set of convergent sequences.",,"['real-analysis', 'general-topology', 'functional-analysis', 'baire-category']"
70,"a consequence of a linear transformations $\mathcal{L} (X, Y) $ between two real Banach spaces $X$ and $Y$",a consequence of a linear transformations  between two real Banach spaces  and,"\mathcal{L} (X, Y)  X Y","I read a following statement in an academic paper from Journal of Mathematical Analysis and Applications . Please refer to Lemma 3 in https://ac.els-cdn.com/S0022247X05001897/1-s2.0-S0022247X05001897-main.pdf?_tid=75c416d8-c0c8-11e7-9e29-00000aacb362&acdnat=1509735466_ce1f132e2c3285a21b65e184e2630ecd . Let $E$ be a real Banach space endowed with complete norm $\| \cdot \|$ and $P$ be a total cone of $E$ . Suppose $B \colon P \to P$ is a bounded linear operator. Therefore this operator $B$ can be uniquely extended to a bounded linear operator on $\overline{B} \colon \overline{P-P} = E \to E$ such that $\| \overline{B} \| = \| B\|$ . Since there is no proof or any comments regarding this statement in that paper, I did not get why it is true. I was thinking that this statement might be a consequence of the Hahn Banach theorem for linear transformations $\mathcal{L} (X, Y) $ between two real Banach spaces $X$ and $Y$ . In fact, the precondition for such consequences may require the space $Y$ having the extensible property, please refer to Section 10 in this note http://www-personal.umich.edu/~romanv/teaching/2009-10/602/short-history-of-analysis.pdf . However, regarding the statement I wrote here, they only assumed that $E$ is a real Banach space with a total positive cone $P$ . I did not get why is that. So, could anyone please help me out and explain it? or could anyone please prove the statement I wrote above? Any idea or suggestion would be much appreciated! Thanks in advance!","I read a following statement in an academic paper from Journal of Mathematical Analysis and Applications . Please refer to Lemma 3 in https://ac.els-cdn.com/S0022247X05001897/1-s2.0-S0022247X05001897-main.pdf?_tid=75c416d8-c0c8-11e7-9e29-00000aacb362&acdnat=1509735466_ce1f132e2c3285a21b65e184e2630ecd . Let be a real Banach space endowed with complete norm and be a total cone of . Suppose is a bounded linear operator. Therefore this operator can be uniquely extended to a bounded linear operator on such that . Since there is no proof or any comments regarding this statement in that paper, I did not get why it is true. I was thinking that this statement might be a consequence of the Hahn Banach theorem for linear transformations between two real Banach spaces and . In fact, the precondition for such consequences may require the space having the extensible property, please refer to Section 10 in this note http://www-personal.umich.edu/~romanv/teaching/2009-10/602/short-history-of-analysis.pdf . However, regarding the statement I wrote here, they only assumed that is a real Banach space with a total positive cone . I did not get why is that. So, could anyone please help me out and explain it? or could anyone please prove the statement I wrote above? Any idea or suggestion would be much appreciated! Thanks in advance!","E \| \cdot \| P E B \colon P \to P B \overline{B} \colon \overline{P-P} = E \to E \| \overline{B} \| = \| B\| \mathcal{L} (X, Y)  X Y Y E P","['real-analysis', 'linear-algebra', 'functional-analysis', 'analysis', 'linear-transformations']"
71,Fixing a wobbly table - Revisited,Fixing a wobbly table - Revisited,,"There is this video on the Youtube channel Numberphile in which it is argued that each wobbly four legged square table can be made stable by just minor adjustments in position, namely an at most $90^°$ turn around its center. Too long to read : I doubt the completeness of the presented solution. Hence I am not sure if every table can be stabelized in the demonstrated way. Of course Numberphile is a popular math channel and cannot go into much detail, but I think there might be much more than just some simple gap filling and formalization work to complete the proof. Question : Is the problem indeed harder than demonstrated in the video? Or is  the solution complete and I can just not see how? How to fix it otherwise? There were some assumptions made and I will start here with formalizing the problem. At first, I assume that the ground on which the table is placed is given as a continuous function $f:\Bbb R^2\to\Bbb R$. For some point $(x,y)=p\in\Bbb R^2$ I will write $\bar p:=(x,y,f(p))\in\Bbb R^3$ for its lifting onto the ground in three dimensions. Now the problem of finding a stable position for the table can be given as follows: Conjecture . Given a continuous function $f:\Bbb R^2\to \Bbb R$. There are four points $(x_i,y_i)=p_i\in\Bbb R^2,i=1,2,3,4$ so that the points $\bar p_i$ form a unit-square. However, the demonstrated solution of the problem in the video aims for a stronger statement. For this, call a set $\{p_i\}$ of points $p_i\in\Bbb R^2,i=1,2,3$ squarable if there is a point $\diamond(p_i)\in\Bbb R^3$ so that $$\{\bar p_1,\bar p_2,\bar p_3,\diamond(p_i)\}$$ form the corners of a unit-square. So the video states Conjecture . Given a continuous function $f:\Bbb R^2\to \Bbb R$ and a squarable set $\{p_i^0\}$ of three points $p_i^0\in\Bbb R^2,i=1,2,3$. Then there are continuous functions $p_i:[0,1]\to\Bbb R^2,i=1,2,3$ so that it holds $p_i(0)=p_i^0$, the set $\{p_i(t)\}$ is squarable for all $t\in[0,1]$, there is a point $p_4\in\Bbb R^2$ with $\bar p_4=\diamond(p_i(1))$. Stated informally this means that a table in a possibly wobbly initial position (see 1.) can be moved continuously (see 2.) into a stable position (see 3.). Now the video explains how to choose the paths $p_i(t)$ in a clever way, so that 3. is satisfied automatically by the intermediate value theorem. In detail, they choose path $p_i(t),i=1,2,3$ to start in $p_i^0$ and aim for $p_{i+1}^0$ (assume $p_4^0$ is the projection of $\diamond(p_i^0)$ into the plane). They call this a rotation of the table around its center. Then they argue that condition 3. is satisfied for some time $t\in[0,1]$. I do not have any objections against this latter argument. I just realized that they completely skipped to argue that such paths even exist! In the end, condition 2. is a strong restriction and the existence is not obvious. The rest of this post contains some arguments with which I convinced myself that the problem might be non-trivial. At first I went one diemnsion lower. I tried to move a two dimensional table along a function-graph by keeping the legs on the ground. I asked: is it always possible to move the table from the left to the right without loosing touch with the ground? Lets formalize this. Again the ground is a continuous function $f:\Bbb R\to\Bbb R$. A point $p\in\Bbb R$ is lifted to $\bar p=(p,f(p))\in\Bbb R^2$. Instead of a unit-square, here we have to keep the touching point of the legs with the ground in the form of a unit interval, i.e. at distance one. Question : Given a continuous function $f:\Bbb R\to\Bbb R$ and $x_0,x_1,y_0\in\Bbb R$ with $\|\bar x_0-\bar y_0\|=1$. Are there continuous functions $x,y:[0,1]\to\Bbb R$ so that it holds $x(0)=x_0$, $y(0)=y_0$ and $x(1)=x_1$, it holds $\|\bar x(t)-\bar y(t)\|=1$ for all $t\in[0,1]$. Looking at the above sketch, this might seem trivial. However, when we choose functions like $f(x)=100 \sin(x^2)$ and want to move the table from $x_0=0$ to $x_1=100$, it is not clear that this can be done. Try it! I will now demonstrate you my approach to solve this question and the problems I encountered on the way. Define the function $d(x,y):=\|\bar x- \bar y\|$. Note that $d$ is a function $\Bbb R^2\to\Bbb R$, hence describes a height map over a two-dimensional area. The $x(t)$ and $y(t)$ in the above formulation are the positions of the table legs at each point in time. Condition 2. is then equivalent to $d(x(t),y(t))=1$. Hence we are interested in paths of constant ""height 1"" in the height map given by $d$. For example, the following figure shows on the left a plot of $f(x)=2\sin(x^2)$, and on the right the associated hightmap. The black points are all the points with $d(x,y)=1$. It looks pretty complicated. But the good thing is that there is indeed a path (the black curve) connecting the bottom of the plot with the top. The $x$- and $y$-coordinates of this path are the desired $x(t)$ and $y(t)$ functions of how to move the tables legs. The hard part is to show that such a path exists always. I tried to show this using some generalizations of the intermediate value theorem but there are sitations where this approach fails. And I was not able to proof that such situations cannot occure for this problem. The second observations are the closed loops in the right figure. They indicate positions in which the table can be placed, but from which he cannot escape. This was strong evidence for me that the problem is highly non-trivial. How do I know that such locked situations do not occure in the case of a square table? Why should I be able to rotate the table while keeping the legs on the ground? What if I am in such a separate connected component as in the above simplified problem?","There is this video on the Youtube channel Numberphile in which it is argued that each wobbly four legged square table can be made stable by just minor adjustments in position, namely an at most $90^°$ turn around its center. Too long to read : I doubt the completeness of the presented solution. Hence I am not sure if every table can be stabelized in the demonstrated way. Of course Numberphile is a popular math channel and cannot go into much detail, but I think there might be much more than just some simple gap filling and formalization work to complete the proof. Question : Is the problem indeed harder than demonstrated in the video? Or is  the solution complete and I can just not see how? How to fix it otherwise? There were some assumptions made and I will start here with formalizing the problem. At first, I assume that the ground on which the table is placed is given as a continuous function $f:\Bbb R^2\to\Bbb R$. For some point $(x,y)=p\in\Bbb R^2$ I will write $\bar p:=(x,y,f(p))\in\Bbb R^3$ for its lifting onto the ground in three dimensions. Now the problem of finding a stable position for the table can be given as follows: Conjecture . Given a continuous function $f:\Bbb R^2\to \Bbb R$. There are four points $(x_i,y_i)=p_i\in\Bbb R^2,i=1,2,3,4$ so that the points $\bar p_i$ form a unit-square. However, the demonstrated solution of the problem in the video aims for a stronger statement. For this, call a set $\{p_i\}$ of points $p_i\in\Bbb R^2,i=1,2,3$ squarable if there is a point $\diamond(p_i)\in\Bbb R^3$ so that $$\{\bar p_1,\bar p_2,\bar p_3,\diamond(p_i)\}$$ form the corners of a unit-square. So the video states Conjecture . Given a continuous function $f:\Bbb R^2\to \Bbb R$ and a squarable set $\{p_i^0\}$ of three points $p_i^0\in\Bbb R^2,i=1,2,3$. Then there are continuous functions $p_i:[0,1]\to\Bbb R^2,i=1,2,3$ so that it holds $p_i(0)=p_i^0$, the set $\{p_i(t)\}$ is squarable for all $t\in[0,1]$, there is a point $p_4\in\Bbb R^2$ with $\bar p_4=\diamond(p_i(1))$. Stated informally this means that a table in a possibly wobbly initial position (see 1.) can be moved continuously (see 2.) into a stable position (see 3.). Now the video explains how to choose the paths $p_i(t)$ in a clever way, so that 3. is satisfied automatically by the intermediate value theorem. In detail, they choose path $p_i(t),i=1,2,3$ to start in $p_i^0$ and aim for $p_{i+1}^0$ (assume $p_4^0$ is the projection of $\diamond(p_i^0)$ into the plane). They call this a rotation of the table around its center. Then they argue that condition 3. is satisfied for some time $t\in[0,1]$. I do not have any objections against this latter argument. I just realized that they completely skipped to argue that such paths even exist! In the end, condition 2. is a strong restriction and the existence is not obvious. The rest of this post contains some arguments with which I convinced myself that the problem might be non-trivial. At first I went one diemnsion lower. I tried to move a two dimensional table along a function-graph by keeping the legs on the ground. I asked: is it always possible to move the table from the left to the right without loosing touch with the ground? Lets formalize this. Again the ground is a continuous function $f:\Bbb R\to\Bbb R$. A point $p\in\Bbb R$ is lifted to $\bar p=(p,f(p))\in\Bbb R^2$. Instead of a unit-square, here we have to keep the touching point of the legs with the ground in the form of a unit interval, i.e. at distance one. Question : Given a continuous function $f:\Bbb R\to\Bbb R$ and $x_0,x_1,y_0\in\Bbb R$ with $\|\bar x_0-\bar y_0\|=1$. Are there continuous functions $x,y:[0,1]\to\Bbb R$ so that it holds $x(0)=x_0$, $y(0)=y_0$ and $x(1)=x_1$, it holds $\|\bar x(t)-\bar y(t)\|=1$ for all $t\in[0,1]$. Looking at the above sketch, this might seem trivial. However, when we choose functions like $f(x)=100 \sin(x^2)$ and want to move the table from $x_0=0$ to $x_1=100$, it is not clear that this can be done. Try it! I will now demonstrate you my approach to solve this question and the problems I encountered on the way. Define the function $d(x,y):=\|\bar x- \bar y\|$. Note that $d$ is a function $\Bbb R^2\to\Bbb R$, hence describes a height map over a two-dimensional area. The $x(t)$ and $y(t)$ in the above formulation are the positions of the table legs at each point in time. Condition 2. is then equivalent to $d(x(t),y(t))=1$. Hence we are interested in paths of constant ""height 1"" in the height map given by $d$. For example, the following figure shows on the left a plot of $f(x)=2\sin(x^2)$, and on the right the associated hightmap. The black points are all the points with $d(x,y)=1$. It looks pretty complicated. But the good thing is that there is indeed a path (the black curve) connecting the bottom of the plot with the top. The $x$- and $y$-coordinates of this path are the desired $x(t)$ and $y(t)$ functions of how to move the tables legs. The hard part is to show that such a path exists always. I tried to show this using some generalizations of the intermediate value theorem but there are sitations where this approach fails. And I was not able to proof that such situations cannot occure for this problem. The second observations are the closed loops in the right figure. They indicate positions in which the table can be placed, but from which he cannot escape. This was strong evidence for me that the problem is highly non-trivial. How do I know that such locked situations do not occure in the case of a square table? Why should I be able to rotate the table while keeping the legs on the ground? What if I am in such a separate connected component as in the above simplified problem?",,"['real-analysis', 'general-topology', 'proof-explanation', 'puzzle']"
72,Difference between Gâteaux and Frêchet derivatives,Difference between Gâteaux and Frêchet derivatives,,"I can't seem to grasp the difference between the definitions of gâteaux and Frêchet derivatives I am wokring with: A function $f: U \rightarrow \mathbb{R}^{n}$ ($U$ open) is said to be Gâteaux diff. in $x_0$ if every directional derivative $D_vf(x_0)$ exists ($\forall v$) and the map $v \mapsto D_vf(x_0)$ is linear. A function is said to be Frêchet diff. in $x_0$ if a linear map $L:\mathbb{R}^n \rightarrow \mathbb{R}$ such that $$\lim_{x \to x_0} \frac{f(x)-f(x_0)-L(x-x_0)}{|x-x_0|} = 0$$ Now by my understanding, as we work over $\mathbb{R}^{n}$, $L$ is equal to the total derivative $df(x_0)$. If a function is frêchet diff., we have $df(x_0) = \langle\nabla f(x_0), \cdot\rangle$, I understand that if a function is frêchet diff in $x_{0}$, it is also gâteaux diff in $x_0$ as exist (whereas $v\mapsto \langle \nabla f(x_0), v\rangle$) But how does the map $v \mapsto D_vf(x_0)$ from the gâteaux definition differ from the total derivative $df(x_0)$ ? I mean the map in the gâteaux definition is also linear so what is difference between both? I know there are similar questions out there, but I could not find one that used the same definitions.","I can't seem to grasp the difference between the definitions of gâteaux and Frêchet derivatives I am wokring with: A function $f: U \rightarrow \mathbb{R}^{n}$ ($U$ open) is said to be Gâteaux diff. in $x_0$ if every directional derivative $D_vf(x_0)$ exists ($\forall v$) and the map $v \mapsto D_vf(x_0)$ is linear. A function is said to be Frêchet diff. in $x_0$ if a linear map $L:\mathbb{R}^n \rightarrow \mathbb{R}$ such that $$\lim_{x \to x_0} \frac{f(x)-f(x_0)-L(x-x_0)}{|x-x_0|} = 0$$ Now by my understanding, as we work over $\mathbb{R}^{n}$, $L$ is equal to the total derivative $df(x_0)$. If a function is frêchet diff., we have $df(x_0) = \langle\nabla f(x_0), \cdot\rangle$, I understand that if a function is frêchet diff in $x_{0}$, it is also gâteaux diff in $x_0$ as exist (whereas $v\mapsto \langle \nabla f(x_0), v\rangle$) But how does the map $v \mapsto D_vf(x_0)$ from the gâteaux definition differ from the total derivative $df(x_0)$ ? I mean the map in the gâteaux definition is also linear so what is difference between both? I know there are similar questions out there, but I could not find one that used the same definitions.",,"['real-analysis', 'differential-geometry', 'definition']"
73,Prove property of integral of continuous functions,Prove property of integral of continuous functions,,"Going over some old notes, I came across the following problem: Let $f,g$ denote continuous real functions on $[a,b]\subset\mathbb{R}$. Prove      that      $$f(x)\int_{a}^{x}g(t)\,dt \;=\; g(x)\int_{x}^{b}f(t)\,dt$$      has at least one solution on $(a,b)$. Unfortunately, I have a note (in the margin no less!) that just says ""proof follows from Rolle's theorem"". Fortunately, this proof is easier than Fermat's Last Theorem, but I wouldn't mind someone checking my logic since it's a few months since doing the course. Proof : We introduce the function: $$\begin{aligned} F:[a,b] &\;\longrightarrow\; \mathbb{R} \\ x &\;\longmapsto\; F(x) \;\equiv\; \left(\int_{a}^{x}g(t)\,dt\right)\left(\int_{x}^{b}f(t)\,dt\right). \end{aligned}$$ From the Fundamental Theorem of Calculus, the integral of a function continuous on $[a,b]$ is continuous on $[a,b]$ and differentiable on $(a,b)$. It follows from the product of continuous and differentiable functions that $F$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Furthermore, $F(a)=F(b)=0$ since the limits of one of the integrals in $F$ will be the same, and hence vanish, at $x=a$ and $x=b$. Thus, $F$ satisfies the requirements of Rolle's theorem and $$\exists\, x_{0}\in(a,b) \quad\text{s.t.}\quad F'(x_{0})\,=\, 0.$$ From the Product Rule and the Leibniz Integral Rule, we have $$\begin{aligned}F'(x) &\;=\; \frac{d}{dx}\left(\int_{a}^{x}g(t)\,dt\right)\cdot\left(\int_{x}^{b}f(t)\,dt\right) + \left(\int_{a}^{x}g(t)\,dt\right)\cdot\frac{d}{dx}\left(\int_{x}^{b}f(t)\,dt\right) \\[0.3cm] &\;=\;  g(x)\int_{x}^{b}f(t)\,dt - f(x)\int_{a}^{x}g(t)\,dt \end{aligned}$$ and hence there exists at least one $x_{0}\in(a,b)$ such that $$\begin{aligned} F'(x_{0})\,=\, 0 \quad&\Longrightarrow\quad  g(x_{0})\int_{x_{0}}^{b}f(t)\,dt - f(x_{0})\int_{a}^{x_{0}}g(t)\,dt \;=\; 0 \\[0.3cm] &\Longrightarrow\quad  g(x_{0})\int_{x_{0}}^{b}f(t)\,dt \;=\; f(x_{0})\int_{a}^{x_{0}}g(t)\,dt \end{aligned}$$ and the result follows. $\;\blacksquare$ I get the feeling that I probably put too much into a proof - but it's good for my own sanity so that I know precisely what's going on and I don't run into the problem (like now!) of remembering precisely the logic. Thoughts welcome!","Going over some old notes, I came across the following problem: Let $f,g$ denote continuous real functions on $[a,b]\subset\mathbb{R}$. Prove      that      $$f(x)\int_{a}^{x}g(t)\,dt \;=\; g(x)\int_{x}^{b}f(t)\,dt$$      has at least one solution on $(a,b)$. Unfortunately, I have a note (in the margin no less!) that just says ""proof follows from Rolle's theorem"". Fortunately, this proof is easier than Fermat's Last Theorem, but I wouldn't mind someone checking my logic since it's a few months since doing the course. Proof : We introduce the function: $$\begin{aligned} F:[a,b] &\;\longrightarrow\; \mathbb{R} \\ x &\;\longmapsto\; F(x) \;\equiv\; \left(\int_{a}^{x}g(t)\,dt\right)\left(\int_{x}^{b}f(t)\,dt\right). \end{aligned}$$ From the Fundamental Theorem of Calculus, the integral of a function continuous on $[a,b]$ is continuous on $[a,b]$ and differentiable on $(a,b)$. It follows from the product of continuous and differentiable functions that $F$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Furthermore, $F(a)=F(b)=0$ since the limits of one of the integrals in $F$ will be the same, and hence vanish, at $x=a$ and $x=b$. Thus, $F$ satisfies the requirements of Rolle's theorem and $$\exists\, x_{0}\in(a,b) \quad\text{s.t.}\quad F'(x_{0})\,=\, 0.$$ From the Product Rule and the Leibniz Integral Rule, we have $$\begin{aligned}F'(x) &\;=\; \frac{d}{dx}\left(\int_{a}^{x}g(t)\,dt\right)\cdot\left(\int_{x}^{b}f(t)\,dt\right) + \left(\int_{a}^{x}g(t)\,dt\right)\cdot\frac{d}{dx}\left(\int_{x}^{b}f(t)\,dt\right) \\[0.3cm] &\;=\;  g(x)\int_{x}^{b}f(t)\,dt - f(x)\int_{a}^{x}g(t)\,dt \end{aligned}$$ and hence there exists at least one $x_{0}\in(a,b)$ such that $$\begin{aligned} F'(x_{0})\,=\, 0 \quad&\Longrightarrow\quad  g(x_{0})\int_{x_{0}}^{b}f(t)\,dt - f(x_{0})\int_{a}^{x_{0}}g(t)\,dt \;=\; 0 \\[0.3cm] &\Longrightarrow\quad  g(x_{0})\int_{x_{0}}^{b}f(t)\,dt \;=\; f(x_{0})\int_{a}^{x_{0}}g(t)\,dt \end{aligned}$$ and the result follows. $\;\blacksquare$ I get the feeling that I probably put too much into a proof - but it's good for my own sanity so that I know precisely what's going on and I don't run into the problem (like now!) of remembering precisely the logic. Thoughts welcome!",,"['real-analysis', 'proof-verification']"
74,A sufficient condition for a sequence to converge if arithmetic mean of the sequence converges?,A sufficient condition for a sequence to converge if arithmetic mean of the sequence converges?,,"We have a well-known conclusion: If a sequence $\{a_n\}_{n\in\mathbb{N}}$ converges, then the arithmetic mean $\frac{S_n}{n}$ (where $S_n=\sum\limits_{k=1}^na_k$ is the nth partial sum)  converges to the same limit. $\lim\limits_{n\to\infty}a_n=a\implies\lim\limits_{n\to\infty}\frac{S_n}{n}=a$ I hope the inverse process also works, so I add a condition: Sequence $\{n(a_n-a_{n-1})\}_{n\in\mathbb{N}}$ is bounded, i.e, $\exists M>0,\forall n\in\mathbb{N},|n(a_n-a_{n-1})|<M$ With such condition, can $\frac{S_n}{n}$ converges implies $a_n$ converges? I want to prove that $\lim\limits_{n\to\infty}\frac{S_n}{n}=a,\ |n(a_n-a_{n-1})|<M\implies \lim\limits_{n\to\infty}a_n=a$ I see that $|a_n-\frac{S_n}{n}|=\frac{1}{n}|\sum\limits_{k=2}^n (k-1)(a_{k}-a_{k-1})|  \\\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \leq\frac{1}{n}\sum\limits_{k=2}^n |(k-1)(a_{k} - a_{k-1})|\\\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \leq\frac{1}{n}\sum\limits_{k=2}^n |k(a_{k} - a_{k-1})|\\\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ < \frac{(n-1)M}{n}<M$ then $|a_n|\leq |\frac{S_n}{n}|+M$ means that $\{a_n\}$ is bounded. Now I can't continue the proof (Maybe consider a convergent subsequence of$\{a_n\}$? I failed..). Does the added condition works? Could you prove that? Sincerely looking forward to your help. Thanks!","We have a well-known conclusion: If a sequence $\{a_n\}_{n\in\mathbb{N}}$ converges, then the arithmetic mean $\frac{S_n}{n}$ (where $S_n=\sum\limits_{k=1}^na_k$ is the nth partial sum)  converges to the same limit. $\lim\limits_{n\to\infty}a_n=a\implies\lim\limits_{n\to\infty}\frac{S_n}{n}=a$ I hope the inverse process also works, so I add a condition: Sequence $\{n(a_n-a_{n-1})\}_{n\in\mathbb{N}}$ is bounded, i.e, $\exists M>0,\forall n\in\mathbb{N},|n(a_n-a_{n-1})|<M$ With such condition, can $\frac{S_n}{n}$ converges implies $a_n$ converges? I want to prove that $\lim\limits_{n\to\infty}\frac{S_n}{n}=a,\ |n(a_n-a_{n-1})|<M\implies \lim\limits_{n\to\infty}a_n=a$ I see that $|a_n-\frac{S_n}{n}|=\frac{1}{n}|\sum\limits_{k=2}^n (k-1)(a_{k}-a_{k-1})|  \\\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \leq\frac{1}{n}\sum\limits_{k=2}^n |(k-1)(a_{k} - a_{k-1})|\\\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \leq\frac{1}{n}\sum\limits_{k=2}^n |k(a_{k} - a_{k-1})|\\\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ < \frac{(n-1)M}{n}<M$ then $|a_n|\leq |\frac{S_n}{n}|+M$ means that $\{a_n\}$ is bounded. Now I can't continue the proof (Maybe consider a convergent subsequence of$\{a_n\}$? I failed..). Does the added condition works? Could you prove that? Sincerely looking forward to your help. Thanks!",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits']"
75,"Theorem 6.12 (e) in Baby Rudin: If $f \in \mathscr{R}\left(\alpha_1\right)$ and $f \in \mathscr{R}\left(\alpha_2\right)$, then $\ldots$","Theorem 6.12 (e) in Baby Rudin: If  and , then",f \in \mathscr{R}\left(\alpha_1\right) f \in \mathscr{R}\left(\alpha_2\right) \ldots,"If $f \in \mathscr{R}\left(\alpha_1\right)$ and $f \in \mathscr{R}\left(\alpha_2\right)$, then $f \in \mathscr{R}\left(\alpha_1 + \alpha_2 \right)$   and    $$ \int_a^b f d\left(\alpha_1 + \alpha_2 \right) = \int_a^b f d\alpha_1 +  \int_a^b f d\alpha_2.$$ This is part of Theorem 6.12 (e) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition. Here is my proof: Let $$\alpha \colon= \alpha_1 + \alpha_2. $$ As $\alpha_1$ and $\alpha_2$ are monotonically increasing functions defined on $[a, b]$, so is $\alpha$. Moreover, if $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$ is any partition of $[a, b]$, then    $$\begin{align}  L(P, f, \alpha) &= \sum_{i=1}^n \left( \inf_{x_{i-1}\leq x \leq x_i } f(x) \right) \left[ \alpha \left(x_i \right) - \alpha \left( x_{i-1} \right) \right] \\ &=  \sum_{i=1}^n \left( \inf_{x_{i-1}\leq x \leq x_i } f(x) \right) \left[ \alpha_1 \left(x_i \right) - \alpha_1 \left( x_{i-1} \right) \right] \\ & \qquad +  \sum_{i=1}^n \left( \inf_{x_{i-1}\leq x \leq x_i } f(x) \right) \left[ \alpha_2 \left(x_i \right) - \alpha_2 \left( x_{i-1} \right) \right] \\ &= L \left( P, f, \alpha_1 \right) + L \left( P, f, \alpha_2 \right), \tag{1} \end{align} $$   and similarly,    $$  U (P, f, \alpha) = U \left( P, f, \alpha_1 \right) + U \left( P, f, \alpha_2 \right). \tag{2}$$ Now as $f \in \mathscr{R}\left(\alpha_1\right)$ and $f \in \mathscr{R}\left(\alpha_2\right)$, so for every real number $\varepsilon > 0$ we can find partitions $P_1$ and $P_2$ of $[a, b]$ such that    $$ U \left( P_1, f, \alpha_1 \right) - L \left( P_1, f, \alpha_1 \right) < { \varepsilon \over 2 } \  \mbox{ and } \  U \left( P_2, f, \alpha_2 \right) - L \left( P_2, f, \alpha_2 \right) < { \varepsilon \over 2 }. \tag{3} $$ Now if $P$ is any partition of $[a, b]$ such that $P \supset P_1$ and $P \supset P_2$, then we have, for each $j = 1, 2$,    $$ L \left( P_j, f, \alpha_j \right) \leq L \left( P, f, \alpha_j \right) \leq U \left( P, f, \alpha_j \right) \leq U \left( P_j, f, \alpha_j \right),$$   and so    $$  U \left( P, f, \alpha_j \right) - L  \left( P, f, \alpha_j \right) \leq  U \left( P_j, f, \alpha_j \right) - L \left( P_j, f, \alpha_j \right). \tag{4} $$   From (3) and (4), we see that, for each $j = 1, 2$, we have    $$  U \left( P, f, \alpha_j \right) - L  \left( P, f, \alpha_j \right) < { \varepsilon \over 2 }, $$   which together with (1) and  (2) yields    $$ \begin{align}  U (P, f, \alpha) - L (P, f, \alpha)  &= \left[  U \left( P, f, \alpha_1 \right) + U \left( P, f, \alpha_2 \right) \right] - \left[  L \left( P, f, \alpha_1 \right) + L \left( P, f, \alpha_2 \right) \right] \\ &=  \left[  U \left( P, f, \alpha_1 \right) - L \left( P, f, \alpha_1 \right) \right] + \left[  U \left( P, f, \alpha_2 \right) -  L \left( P, f, \alpha_2 \right) \right] \\ &< { \varepsilon \over 2 } + { \varepsilon \over 2 } \\ &= \varepsilon, \end{align} $$   and thus it follows that $f \in \mathscr{R}\left(\alpha_1 + \alpha_2 \right)$. Finally,    $$ \begin{align} \int_a^b f d\alpha &= \int_a^b f d \left( \alpha_1 + \alpha_2 \right) \\ &= \inf \left\{ \ U(P, f, \alpha) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &= \inf \left\{ \ U \left(P, f, \alpha_1 \right) + U \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &\geq \inf \left\{ \ U \left(P, f, \alpha_1 \right) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} + \inf \left\{ U \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &= \int_a^b f d \alpha_1 + \int_a^b f d \alpha_2, \tag{5} \end{align}  $$   and    $$ \begin{align} \int_a^b f d\alpha &= \int_a^b f d \left( \alpha_1 + \alpha_2 \right) \\ &= \sup \left\{ \ L(P, f, \alpha) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &= \sup \left\{ \ L \left(P, f, \alpha_1 \right) + L \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &\leq \sup \left\{ \ L \left(P, f, \alpha_1 \right) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} + \sup \left\{ L \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &= \int_a^b f d \alpha_1 + \int_a^b f d \alpha_2. \tag{6} \end{align}  $$   From (5) and (6), we can conclude that    $$ \int_a^b f d \left( \alpha_1 + \alpha_2 \right) = \int_a^b f d \alpha_1 +  \int_a^b f d \alpha_2,$$   as required. Is this proof sound enough in terms of logic and rigor?","If $f \in \mathscr{R}\left(\alpha_1\right)$ and $f \in \mathscr{R}\left(\alpha_2\right)$, then $f \in \mathscr{R}\left(\alpha_1 + \alpha_2 \right)$   and    $$ \int_a^b f d\left(\alpha_1 + \alpha_2 \right) = \int_a^b f d\alpha_1 +  \int_a^b f d\alpha_2.$$ This is part of Theorem 6.12 (e) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition. Here is my proof: Let $$\alpha \colon= \alpha_1 + \alpha_2. $$ As $\alpha_1$ and $\alpha_2$ are monotonically increasing functions defined on $[a, b]$, so is $\alpha$. Moreover, if $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$ is any partition of $[a, b]$, then    $$\begin{align}  L(P, f, \alpha) &= \sum_{i=1}^n \left( \inf_{x_{i-1}\leq x \leq x_i } f(x) \right) \left[ \alpha \left(x_i \right) - \alpha \left( x_{i-1} \right) \right] \\ &=  \sum_{i=1}^n \left( \inf_{x_{i-1}\leq x \leq x_i } f(x) \right) \left[ \alpha_1 \left(x_i \right) - \alpha_1 \left( x_{i-1} \right) \right] \\ & \qquad +  \sum_{i=1}^n \left( \inf_{x_{i-1}\leq x \leq x_i } f(x) \right) \left[ \alpha_2 \left(x_i \right) - \alpha_2 \left( x_{i-1} \right) \right] \\ &= L \left( P, f, \alpha_1 \right) + L \left( P, f, \alpha_2 \right), \tag{1} \end{align} $$   and similarly,    $$  U (P, f, \alpha) = U \left( P, f, \alpha_1 \right) + U \left( P, f, \alpha_2 \right). \tag{2}$$ Now as $f \in \mathscr{R}\left(\alpha_1\right)$ and $f \in \mathscr{R}\left(\alpha_2\right)$, so for every real number $\varepsilon > 0$ we can find partitions $P_1$ and $P_2$ of $[a, b]$ such that    $$ U \left( P_1, f, \alpha_1 \right) - L \left( P_1, f, \alpha_1 \right) < { \varepsilon \over 2 } \  \mbox{ and } \  U \left( P_2, f, \alpha_2 \right) - L \left( P_2, f, \alpha_2 \right) < { \varepsilon \over 2 }. \tag{3} $$ Now if $P$ is any partition of $[a, b]$ such that $P \supset P_1$ and $P \supset P_2$, then we have, for each $j = 1, 2$,    $$ L \left( P_j, f, \alpha_j \right) \leq L \left( P, f, \alpha_j \right) \leq U \left( P, f, \alpha_j \right) \leq U \left( P_j, f, \alpha_j \right),$$   and so    $$  U \left( P, f, \alpha_j \right) - L  \left( P, f, \alpha_j \right) \leq  U \left( P_j, f, \alpha_j \right) - L \left( P_j, f, \alpha_j \right). \tag{4} $$   From (3) and (4), we see that, for each $j = 1, 2$, we have    $$  U \left( P, f, \alpha_j \right) - L  \left( P, f, \alpha_j \right) < { \varepsilon \over 2 }, $$   which together with (1) and  (2) yields    $$ \begin{align}  U (P, f, \alpha) - L (P, f, \alpha)  &= \left[  U \left( P, f, \alpha_1 \right) + U \left( P, f, \alpha_2 \right) \right] - \left[  L \left( P, f, \alpha_1 \right) + L \left( P, f, \alpha_2 \right) \right] \\ &=  \left[  U \left( P, f, \alpha_1 \right) - L \left( P, f, \alpha_1 \right) \right] + \left[  U \left( P, f, \alpha_2 \right) -  L \left( P, f, \alpha_2 \right) \right] \\ &< { \varepsilon \over 2 } + { \varepsilon \over 2 } \\ &= \varepsilon, \end{align} $$   and thus it follows that $f \in \mathscr{R}\left(\alpha_1 + \alpha_2 \right)$. Finally,    $$ \begin{align} \int_a^b f d\alpha &= \int_a^b f d \left( \alpha_1 + \alpha_2 \right) \\ &= \inf \left\{ \ U(P, f, \alpha) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &= \inf \left\{ \ U \left(P, f, \alpha_1 \right) + U \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &\geq \inf \left\{ \ U \left(P, f, \alpha_1 \right) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} + \inf \left\{ U \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &= \int_a^b f d \alpha_1 + \int_a^b f d \alpha_2, \tag{5} \end{align}  $$   and    $$ \begin{align} \int_a^b f d\alpha &= \int_a^b f d \left( \alpha_1 + \alpha_2 \right) \\ &= \sup \left\{ \ L(P, f, \alpha) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &= \sup \left\{ \ L \left(P, f, \alpha_1 \right) + L \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &\leq \sup \left\{ \ L \left(P, f, \alpha_1 \right) \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} + \sup \left\{ L \left(P, f, \alpha_2 \right)  \ \colon \ \mbox{ P is a partition of } [a, b] \ \right\} \\ &= \int_a^b f d \alpha_1 + \int_a^b f d \alpha_2. \tag{6} \end{align}  $$   From (5) and (6), we can conclude that    $$ \int_a^b f d \left( \alpha_1 + \alpha_2 \right) = \int_a^b f d \alpha_1 +  \int_a^b f d \alpha_2,$$   as required. Is this proof sound enough in terms of logic and rigor?",,"['real-analysis', 'integration', 'analysis', 'proof-verification', 'definite-integrals']"
76,Truncated taylor series inequality,Truncated taylor series inequality,,"I came across the following fact in a paper and am having trouble understanding why it is true: Consider the error made when truncating the expansion for $e^a$ at the $K$th term. By choosing $K = O(\frac{\log N}{\log \log N})$, we can upper bound the error by $1/N$, in other words $$\sum_{j={K+1}}^\infty \frac{a^j}{j!} \leq \frac{1}{N}.$$ Here, the $O$ is ""big-O"" notation. I'm thinking a Stirling approximation probably has to be used in the denominator but still can't reproduce this result. Maybe this is some well-known result that I'm not aware of?","I came across the following fact in a paper and am having trouble understanding why it is true: Consider the error made when truncating the expansion for $e^a$ at the $K$th term. By choosing $K = O(\frac{\log N}{\log \log N})$, we can upper bound the error by $1/N$, in other words $$\sum_{j={K+1}}^\infty \frac{a^j}{j!} \leq \frac{1}{N}.$$ Here, the $O$ is ""big-O"" notation. I'm thinking a Stirling approximation probably has to be used in the denominator but still can't reproduce this result. Maybe this is some well-known result that I'm not aware of?",,"['calculus', 'real-analysis', 'asymptotics', 'taylor-expansion']"
77,Using contour line integrals to prove identities,Using contour line integrals to prove identities,,I am looking for examples of how contour line integrals may be used to prove certain identities (that may seem unrelated to complex analysis) like this combinatoric one. Can anyone suggest more examples or links to them?,I am looking for examples of how contour line integrals may be used to prove certain identities (that may seem unrelated to complex analysis) like this combinatoric one. Can anyone suggest more examples or links to them?,,"['real-analysis', 'complex-analysis']"
78,Is this a convergent series? [closed],Is this a convergent series? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Put $a_{0}= \pi/2$  and  $a_{n}=\sin(a_{n-1})$. Is  $\sum a_{n}$ a convergent  series?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Put $a_{0}= \pi/2$  and  $a_{n}=\sin(a_{n-1})$. Is  $\sum a_{n}$ a convergent  series?",,"['real-analysis', 'sequences-and-series', 'divergent-series']"
79,Lipschitz function maps set of measure zero to set of measure zero,Lipschitz function maps set of measure zero to set of measure zero,,"Let the function $f: [a, b] \rightarrow \mathbb{R}$ be Lipschitz, that is, there is a constant $c \geq 0$ such that for all $u, v \in [a, b]$, $|f( u) - f( v)| \le c |u - v|$. Show that $f$ maps a set of measure zero onto a set of measure zero. The following is my attempt: Suppose that $A \subseteq [0,1]$ has measure zero and $\varepsilon > 0$ be given. By definition of measure, there exists a countable collection of open intervals $\{ I_k \}_{k \geq 1}$ such that $A \subseteq \cup_{k=1}^{\infty}{I_k}$ and $\sum_{k=1}^{\infty}{l(I_k)} < \frac{\varepsilon}{c}. $     Note that $f(A) \subseteq f(\cup_{k=1}^{\infty}{I_k}) = \cup_{k=1}^{\infty}{f(I_k)}.$     For each $k$, denote $I_k = (x_k, y_k).$     Since $f$ is Lipschitz, for each $k$, $|f(x_k) - f(y_k)| \leq c \cdot |x_k - y_k|.$     Therefore, $\sum_{k=1}^{\infty}{|f(x_k) - f(y_k)|} \leq c \cdot \sum_{k=1}^{\infty}{|x_k - y_k|} < \varepsilon.$     Hence, $f(A)$ has measure zero.","Let the function $f: [a, b] \rightarrow \mathbb{R}$ be Lipschitz, that is, there is a constant $c \geq 0$ such that for all $u, v \in [a, b]$, $|f( u) - f( v)| \le c |u - v|$. Show that $f$ maps a set of measure zero onto a set of measure zero. The following is my attempt: Suppose that $A \subseteq [0,1]$ has measure zero and $\varepsilon > 0$ be given. By definition of measure, there exists a countable collection of open intervals $\{ I_k \}_{k \geq 1}$ such that $A \subseteq \cup_{k=1}^{\infty}{I_k}$ and $\sum_{k=1}^{\infty}{l(I_k)} < \frac{\varepsilon}{c}. $     Note that $f(A) \subseteq f(\cup_{k=1}^{\infty}{I_k}) = \cup_{k=1}^{\infty}{f(I_k)}.$     For each $k$, denote $I_k = (x_k, y_k).$     Since $f$ is Lipschitz, for each $k$, $|f(x_k) - f(y_k)| \leq c \cdot |x_k - y_k|.$     Therefore, $\sum_{k=1}^{\infty}{|f(x_k) - f(y_k)|} \leq c \cdot \sum_{k=1}^{\infty}{|x_k - y_k|} < \varepsilon.$     Hence, $f(A)$ has measure zero.",,['real-analysis']
80,Does the reciprocal of a polynomial define a tempered distribution when it is locally integrable?,Does the reciprocal of a polynomial define a tempered distribution when it is locally integrable?,,"Consider a complex polynomial in $n$ variables $z=(z_1,\dots,z_n)$: \begin{equation} P(z)=\sum_{|\alpha| \leq N} c_{\alpha} z^{\alpha}, \end{equation} where as usual for every $\alpha=(\alpha_1,\dots,\alpha_n) \in \mathbb{N}^{n}$ we set $|\alpha|=\alpha_1+\dots+\alpha_n$, and $z^{\alpha}=z_1^{\alpha_1}\dots z_n^{\alpha_n}$. Let $Z= \{ x \in \mathbb{R}^n : P(x) = 0 \}$ and define $H:\mathbb{R}^n \rightarrow \mathbb{C}$ as \begin{equation} H(x)= \begin{cases} \frac{1}{P(x)} & \textit{if } x \in \mathbb{R}^n \backslash Z,\\ 0 & \textit{if }x \in Z. \end{cases} \end{equation} Assume that $H \in L_{loc}^{1}(\mathbb{R}^n)$. Is it then true that $H$ defines a tempered distribution? More explicitly, if we set \begin{equation} T(\phi)=\int_{\mathbb{R}^n} H(x) \phi(x) dx, \quad (\phi \in \mathscr{D}(\mathbb{R}^n)), \end{equation} does $T$ extends to a continuous linear functional on $\mathscr{S}(\mathbb{R}^n)$? I guess the answer is positive, but I have no idea of a possible proof. NOTE (1). Let $x \in \mathbb{R}^n$, and let $Q(x)$ and $R(x)$ be respectvely the real and imaginary part of $P(x)$. $Z$ is the intersection of the zero sets of the two real polynomials $Q(x)$ and $R(x)$. Since the zero set of a non null real polynomial has zero Lebesgue measure (for a very simple proof see Daniel Fischer's answer in Zero Set of a Polynomial ), we conclude that $Z$ has zero Lebesgue measure, and for our question is totally irrelevant how we define $H$ on $Z$. NOTE (2). Let us note that $H$ can be locally integrable even if $Z$ is not empty when $n \geq 2$. Take e.g. for $n=2$ the polynomial $P(z_1,z_2)=z_1+ i z_2$, or for $n=3$ consider $P(z_1,z_2,z_3)=z_1^{2}+z_2^{2}+z_3^{2}$ or even $P(z_1,z_2,z_3)=z_1+ i z_2$. NOTE (3). If $Z= \emptyset$, then clearly $H \in L_{loc}^{1}(\mathbb{R}^n)$. In this case $H$ defines a tempered distribution. Indeed, by a remarkable result of Hörmander (see Lemma (2) in On the Division of Distributions by Polynomials ) there exist $C > 0$ and $\mu > 0$ such that \begin{equation} |P(x)| \geq C (1+|x|^2)^{-\mu} \quad \forall x \in \mathbb{R}^n. \end{equation} So for $M > 0$ big enough we have in this case \begin{equation} \int_{\mathbb{R}^n} (1+|x|^2)^{-M} |H(x)| dx < \infty, \end{equation} and we conclude that $H$ defines a temepered distribution. When $Z \neq \emptyset$, then Hörmander proves that there exists positive constants $C, \mu, \nu$ such that \begin{equation} |P(x)| \geq C (1+|x|^2)^{-\mu} [d(x,Z)]^{\nu} \quad \forall x \in \mathbb{R}^n, \end{equation} where  \begin{equation} d(x,Z)=\inf_{y \in Z} |x - y| \quad (x \in \mathbb{R}^n). \end{equation} I don't know if this remarkable inequality together with the assumption that $H \in L_{loc}^{1}(\mathbb{R}^n)$ implies that $H$ defines a tempered distribution.","Consider a complex polynomial in $n$ variables $z=(z_1,\dots,z_n)$: \begin{equation} P(z)=\sum_{|\alpha| \leq N} c_{\alpha} z^{\alpha}, \end{equation} where as usual for every $\alpha=(\alpha_1,\dots,\alpha_n) \in \mathbb{N}^{n}$ we set $|\alpha|=\alpha_1+\dots+\alpha_n$, and $z^{\alpha}=z_1^{\alpha_1}\dots z_n^{\alpha_n}$. Let $Z= \{ x \in \mathbb{R}^n : P(x) = 0 \}$ and define $H:\mathbb{R}^n \rightarrow \mathbb{C}$ as \begin{equation} H(x)= \begin{cases} \frac{1}{P(x)} & \textit{if } x \in \mathbb{R}^n \backslash Z,\\ 0 & \textit{if }x \in Z. \end{cases} \end{equation} Assume that $H \in L_{loc}^{1}(\mathbb{R}^n)$. Is it then true that $H$ defines a tempered distribution? More explicitly, if we set \begin{equation} T(\phi)=\int_{\mathbb{R}^n} H(x) \phi(x) dx, \quad (\phi \in \mathscr{D}(\mathbb{R}^n)), \end{equation} does $T$ extends to a continuous linear functional on $\mathscr{S}(\mathbb{R}^n)$? I guess the answer is positive, but I have no idea of a possible proof. NOTE (1). Let $x \in \mathbb{R}^n$, and let $Q(x)$ and $R(x)$ be respectvely the real and imaginary part of $P(x)$. $Z$ is the intersection of the zero sets of the two real polynomials $Q(x)$ and $R(x)$. Since the zero set of a non null real polynomial has zero Lebesgue measure (for a very simple proof see Daniel Fischer's answer in Zero Set of a Polynomial ), we conclude that $Z$ has zero Lebesgue measure, and for our question is totally irrelevant how we define $H$ on $Z$. NOTE (2). Let us note that $H$ can be locally integrable even if $Z$ is not empty when $n \geq 2$. Take e.g. for $n=2$ the polynomial $P(z_1,z_2)=z_1+ i z_2$, or for $n=3$ consider $P(z_1,z_2,z_3)=z_1^{2}+z_2^{2}+z_3^{2}$ or even $P(z_1,z_2,z_3)=z_1+ i z_2$. NOTE (3). If $Z= \emptyset$, then clearly $H \in L_{loc}^{1}(\mathbb{R}^n)$. In this case $H$ defines a tempered distribution. Indeed, by a remarkable result of Hörmander (see Lemma (2) in On the Division of Distributions by Polynomials ) there exist $C > 0$ and $\mu > 0$ such that \begin{equation} |P(x)| \geq C (1+|x|^2)^{-\mu} \quad \forall x \in \mathbb{R}^n. \end{equation} So for $M > 0$ big enough we have in this case \begin{equation} \int_{\mathbb{R}^n} (1+|x|^2)^{-M} |H(x)| dx < \infty, \end{equation} and we conclude that $H$ defines a temepered distribution. When $Z \neq \emptyset$, then Hörmander proves that there exists positive constants $C, \mu, \nu$ such that \begin{equation} |P(x)| \geq C (1+|x|^2)^{-\mu} [d(x,Z)]^{\nu} \quad \forall x \in \mathbb{R}^n, \end{equation} where  \begin{equation} d(x,Z)=\inf_{y \in Z} |x - y| \quad (x \in \mathbb{R}^n). \end{equation} I don't know if this remarkable inequality together with the assumption that $H \in L_{loc}^{1}(\mathbb{R}^n)$ implies that $H$ defines a tempered distribution.",,"['real-analysis', 'polynomials', 'distribution-theory']"
81,Prove that $ \inf_{-1 < x< 1}|f^{k}| \leq \frac{2^{k(k+1)/2}k^k}{\lambda^k} $ for $|f_{-1 < x < 1}(x)| \leq 1$ and $\lambda$ is length of $x_1 - x_0$,Prove that  for  and  is length of, \inf_{-1 < x< 1}|f^{k}| \leq \frac{2^{k(k+1)/2}k^k}{\lambda^k}  |f_{-1 < x < 1}(x)| \leq 1 \lambda x_1 - x_0,"Let $$  |f_{-1 < x < 1}(x)| \leq 1 $$ $\lambda $ is length of conventional interval $I$ in $(-1, 1)$. Prove that on this interval: $$ \inf_{x \in I}|f^{k}| \leq \frac{2^{k(k+1)/2}k^k}{\lambda^k} $$ Where $k$ is k-th derivative, and  $f^{k-1}(x)$ for $1 ...k-1$ is continuous function. Since if function is not monotonic on this interval, it's derivative  $\inf_{x \in i}|f'(x)| = 0$, so let's consider the critical case when function and all it's derivatives until $k$ are monotonically increasing (decreasing) functions. Using Lagrange's formula for Taylor polynomial's reminder $$ f(x) = f(x_0) + f'(x_0)(x-x_0) + ... \frac{f^k(x_0)(x-x_0)^k}{k!} + \frac{f^{k+1}(\xi)(x - x_o)^{k+1}}{(k+1)!}$$ Take $x_0$ at $i$ left point, and choosing x at $i$ right point, for $k=0$ $$ f(x_o) + f'(\xi)\lambda \leq 1 $$ Since we consider increasing function, $f(x_0)$ minimum value is $-1$ $$ inf|f(x)| \leq 2/\lambda $$ This leads to formula: $$ \inf_{-1 < x< 1}|f^{k}| \leq \frac{2^{k+1}k!}{\lambda^k} $$ EDIT However, we can't just interchange derivative value of some order at point with obtained bound from above for $|\inf f^k(x)|$ Other thoughts This may be somehow related to $e$. At $n\to \infty$  $\frac{(n+1)^{(n+1)}}{(n+1)!} * \frac{n!}{n^{n}} \to e $ Take a look at different variation of Taylor polynomial reminder's formula: $$ r_k = \frac{1}{(k+1)!}f^{k+1}(\xi)(x - \xi)^k(x - x_0) $$ So if we could bound $(x-\xi)^k \leq(1 - \frac{1}{k})^k \to e^{-1}$, then $k^k$ starts to make sense, as we increase some previous term by $e * (k+1)$ So if we devide distance $ x - x_0$ by 2 at each term, starting with $\lambda$, it looks very close.","Let $$  |f_{-1 < x < 1}(x)| \leq 1 $$ $\lambda $ is length of conventional interval $I$ in $(-1, 1)$. Prove that on this interval: $$ \inf_{x \in I}|f^{k}| \leq \frac{2^{k(k+1)/2}k^k}{\lambda^k} $$ Where $k$ is k-th derivative, and  $f^{k-1}(x)$ for $1 ...k-1$ is continuous function. Since if function is not monotonic on this interval, it's derivative  $\inf_{x \in i}|f'(x)| = 0$, so let's consider the critical case when function and all it's derivatives until $k$ are monotonically increasing (decreasing) functions. Using Lagrange's formula for Taylor polynomial's reminder $$ f(x) = f(x_0) + f'(x_0)(x-x_0) + ... \frac{f^k(x_0)(x-x_0)^k}{k!} + \frac{f^{k+1}(\xi)(x - x_o)^{k+1}}{(k+1)!}$$ Take $x_0$ at $i$ left point, and choosing x at $i$ right point, for $k=0$ $$ f(x_o) + f'(\xi)\lambda \leq 1 $$ Since we consider increasing function, $f(x_0)$ minimum value is $-1$ $$ inf|f(x)| \leq 2/\lambda $$ This leads to formula: $$ \inf_{-1 < x< 1}|f^{k}| \leq \frac{2^{k+1}k!}{\lambda^k} $$ EDIT However, we can't just interchange derivative value of some order at point with obtained bound from above for $|\inf f^k(x)|$ Other thoughts This may be somehow related to $e$. At $n\to \infty$  $\frac{(n+1)^{(n+1)}}{(n+1)!} * \frac{n!}{n^{n}} \to e $ Take a look at different variation of Taylor polynomial reminder's formula: $$ r_k = \frac{1}{(k+1)!}f^{k+1}(\xi)(x - \xi)^k(x - x_0) $$ So if we could bound $(x-\xi)^k \leq(1 - \frac{1}{k})^k \to e^{-1}$, then $k^k$ starts to make sense, as we increase some previous term by $e * (k+1)$ So if we devide distance $ x - x_0$ by 2 at each term, starting with $\lambda$, it looks very close.",,['real-analysis']
82,Choice of $q$ in Baby Rudin's Example 1.1,Choice of  in Baby Rudin's Example 1.1,q,"First, my apologies if this has already been asked/answered.  I wasn't able to find this question via search. My question comes from Rudin's ""Principles of Mathematical Analysis,"" or ""Baby Rudin,"" Ch 1, Example 1.1 on p. 2.  In the second version of the proof, showing that sets A and B do not have greatest or lowest elements respectively, he presents a seemingly arbitrary assignment of a number $q$ that satisfies equations (3) and (4), plus other conditions needed to show that $q$ is the right number for the proof.  As an exercise, I tried to derive his choice of $q$ so that I may learn more about the problem. If we write equations (3) as $q = p - (p^2 - 2)x$ , we can write (4) as $$ q^2 - 2 = (p^2 - 2)[1 - 2px + (p^2 - 2)x^2]. $$ Here, we need a rational $x > 0$ , chosen such that the expression in $[...]$ is positive.  Using the quadratic formula and the sign of $(p^2 - 2)$ , it can be shown that we need $$ x \in \left(0, \frac{1}{p + \sqrt{2}}\right) \mbox{ for } p \in A, $$ or, for $p \in B$ , $x < 1/\left(p + \sqrt{2}\right)$ or $x > 1/\left(p - \sqrt{2}\right)$ . Notice that there are MANY solutions to these equations! The easiest to see, perhaps, is letting $x = 1/(p + n)$ for $n \geq 2$ .  Notice that Rudin chooses $n = 2$ for his answer, but it checks out easily for other $n$ . The Question: Why does Rudin choose $x = 1/(p + 2)$ specifically?  Is it just to make the expressions work out clearly algebraically?  Why doesn't he comment on his particular choice or the nature of the set of solutions that will work for the proof?  Is there a simpler derivation for the number $q$ that I am missing?","First, my apologies if this has already been asked/answered.  I wasn't able to find this question via search. My question comes from Rudin's ""Principles of Mathematical Analysis,"" or ""Baby Rudin,"" Ch 1, Example 1.1 on p. 2.  In the second version of the proof, showing that sets A and B do not have greatest or lowest elements respectively, he presents a seemingly arbitrary assignment of a number that satisfies equations (3) and (4), plus other conditions needed to show that is the right number for the proof.  As an exercise, I tried to derive his choice of so that I may learn more about the problem. If we write equations (3) as , we can write (4) as Here, we need a rational , chosen such that the expression in is positive.  Using the quadratic formula and the sign of , it can be shown that we need or, for , or . Notice that there are MANY solutions to these equations! The easiest to see, perhaps, is letting for .  Notice that Rudin chooses for his answer, but it checks out easily for other . The Question: Why does Rudin choose specifically?  Is it just to make the expressions work out clearly algebraically?  Why doesn't he comment on his particular choice or the nature of the set of solutions that will work for the proof?  Is there a simpler derivation for the number that I am missing?","q q q q = p - (p^2 - 2)x 
q^2 - 2 = (p^2 - 2)[1 - 2px + (p^2 - 2)x^2].
 x > 0 [...] (p^2 - 2) 
x \in \left(0, \frac{1}{p + \sqrt{2}}\right) \mbox{ for } p \in A,
 p \in B x < 1/\left(p + \sqrt{2}\right) x > 1/\left(p - \sqrt{2}\right) x = 1/(p + n) n \geq 2 n = 2 n x = 1/(p + 2) q","['real-analysis', 'analysis', 'self-learning']"
83,A method for evaluating sums/discrete functions by assuming they can be made continuous and differentiable?,A method for evaluating sums/discrete functions by assuming they can be made continuous and differentiable?,,"Suppose I had a function that satisfied the property $f(x)=f(x-1)+g(x)$.  For any $x\in\mathbb N$, it is easy enough to see that this boils down to the statement $$f(x)=f(0)+\sum_{k=1}^xg(k)$$ If we return to our functional equation and differentiate it $n$ times, we get $$f^{(n)}(x)=f^{(n)}(x-1)+g^{(n)}(x)$$ Once again, for any $x\in\mathbb N$, we have $$f^{(n)}(x)=f^{(n)}(0)+\sum_{k=1}^xg^{(n)}(k)$$ One could then integrate both sides.  For example, this asserts that $$\sum_{k=1}^xg(k)=xf'(0)+\int_0^x\sum_{k=1}^tg'(k)dt$$ If the sum inside the intagral is generalized to any upper bound. How would I justify this for discrete functions? An example of this is letting $f_p(x)=\sum_{k=1}^xk^p$, as it seems to follow the above result.  We can see from the above that $$f_p(x)=a_px+p\int_0^xf_{p-1}(t)dt$$ $$a_p=1-p\int_0^1f_{p-1}(t)dt$$ which appears to be a true recursive formula. Under what conditions can I extend discrete functions $f(x)$ and $g(x)$ into continuous differentiable functions and apply this method?","Suppose I had a function that satisfied the property $f(x)=f(x-1)+g(x)$.  For any $x\in\mathbb N$, it is easy enough to see that this boils down to the statement $$f(x)=f(0)+\sum_{k=1}^xg(k)$$ If we return to our functional equation and differentiate it $n$ times, we get $$f^{(n)}(x)=f^{(n)}(x-1)+g^{(n)}(x)$$ Once again, for any $x\in\mathbb N$, we have $$f^{(n)}(x)=f^{(n)}(0)+\sum_{k=1}^xg^{(n)}(k)$$ One could then integrate both sides.  For example, this asserts that $$\sum_{k=1}^xg(k)=xf'(0)+\int_0^x\sum_{k=1}^tg'(k)dt$$ If the sum inside the intagral is generalized to any upper bound. How would I justify this for discrete functions? An example of this is letting $f_p(x)=\sum_{k=1}^xk^p$, as it seems to follow the above result.  We can see from the above that $$f_p(x)=a_px+p\int_0^xf_{p-1}(t)dt$$ $$a_p=1-p\int_0^1f_{p-1}(t)dt$$ which appears to be a true recursive formula. Under what conditions can I extend discrete functions $f(x)$ and $g(x)$ into continuous differentiable functions and apply this method?",,"['real-analysis', 'functions', 'summation', 'summation-method']"
84,Roots of combination of trigonometric functions,Roots of combination of trigonometric functions,,"Consider the following function of $\mathbb{R^+}^2$: $$f(s_1,s_2)=r_1^2\sin\big(\tfrac{1}{2}(s_2-s_1)\omega_1\big)\sin\big(\tfrac{1}{2}(s_2+s_1)\omega_2\big) + r_2^2\sin\big(\tfrac{1}{2}(s_2+s_1)\omega_1\big)\sin\big(\tfrac{1}{2}(s_2-s_1)\omega_2\big)$$ where $r_1^2+r_2^2=1$. My goal is to find the solution curves of $f$ on $\mathbb{R^+}^2$. There is certainly no closed-form solution, so I'm doing it numerically, for some given values of $(r_1,r_2,\omega_1,\omega_2)$. Symmetries Obviously, $f(s_1,s_2)=-f(s_2,s_1)$ so the solution curves of $f$ are symmetric w.r.t. to $s_2=s_1$: it suffices to focus on $s_1\geq s_2$. Note that also $f(-s_1,s_2)=-f(-s_2,s_1)$ so $s_2=-s_1$ is another axis of symmetry for the solutions of the extension of $f$ to $\mathbb{R}^2$, but it is a priori not intersting on $\mathbb{R^+}^2$. $f$ is a scalar function of two variables so its solution curves are (generically) curves. That's what we can observe to some numeric values of $(r_1,r_2,\omega_1,\omega_2)$: (Note the axial symmetry mentionned above.) Calculating some points on the solution curves For some reason (reduction of computational cost in more complex cases), I'd like to get points on the solution curves of $f$ and then do numerical continuation to compute each curve. By curve , I mean a connex curve, and the solution curves is a family of such connex curves. The idea is that when $\tfrac{1}{2}(s_2+s_1)\omega_i$ approaches $\pi\mathbb{N}$, only one term is the sum remains. For example, when $s_2+s_1=2\pi/\omega_1$, $$f(s_1,2\pi/\omega_1-s_1)=r_1^2 \sin(\pi\tfrac{\omega_2}{\omega_1})\sin(s_1\omega_1)$$ and so the roots of $s_1\mapsto f(s_1,2\pi/\omega_1-s_1)$ are the $\pi/\omega_1\mathbb{N}$. In the, we know that the set of points $$\mathcal{S}=\Big\{ \frac{\pi}{\omega_i}(p,2p-q),\ p,q\in\mathbb{N}, i\in\{1,2\}\Big\} \cap {\mathbb{R}^+}^2$$ is a subset of the solution curves of $f$, as illustrated below: or, for another set of parameters $(r_1,r_2,\omega_1,\omega_2)$: Red and green points correspond to $i=1$ and $i=2$. Question On both figures, all the curves pass through the set of points $\mathcal{S}$. Is this always true, or can there be some curves which do not pass through any green or red points? I am also interested in references or keywords for mathematical tools which could be useful for the study of solution curves of functions of the ""type"" of $f$ (i.e. sum of products of sines), even though I don't think there are miraculous simplification.","Consider the following function of $\mathbb{R^+}^2$: $$f(s_1,s_2)=r_1^2\sin\big(\tfrac{1}{2}(s_2-s_1)\omega_1\big)\sin\big(\tfrac{1}{2}(s_2+s_1)\omega_2\big) + r_2^2\sin\big(\tfrac{1}{2}(s_2+s_1)\omega_1\big)\sin\big(\tfrac{1}{2}(s_2-s_1)\omega_2\big)$$ where $r_1^2+r_2^2=1$. My goal is to find the solution curves of $f$ on $\mathbb{R^+}^2$. There is certainly no closed-form solution, so I'm doing it numerically, for some given values of $(r_1,r_2,\omega_1,\omega_2)$. Symmetries Obviously, $f(s_1,s_2)=-f(s_2,s_1)$ so the solution curves of $f$ are symmetric w.r.t. to $s_2=s_1$: it suffices to focus on $s_1\geq s_2$. Note that also $f(-s_1,s_2)=-f(-s_2,s_1)$ so $s_2=-s_1$ is another axis of symmetry for the solutions of the extension of $f$ to $\mathbb{R}^2$, but it is a priori not intersting on $\mathbb{R^+}^2$. $f$ is a scalar function of two variables so its solution curves are (generically) curves. That's what we can observe to some numeric values of $(r_1,r_2,\omega_1,\omega_2)$: (Note the axial symmetry mentionned above.) Calculating some points on the solution curves For some reason (reduction of computational cost in more complex cases), I'd like to get points on the solution curves of $f$ and then do numerical continuation to compute each curve. By curve , I mean a connex curve, and the solution curves is a family of such connex curves. The idea is that when $\tfrac{1}{2}(s_2+s_1)\omega_i$ approaches $\pi\mathbb{N}$, only one term is the sum remains. For example, when $s_2+s_1=2\pi/\omega_1$, $$f(s_1,2\pi/\omega_1-s_1)=r_1^2 \sin(\pi\tfrac{\omega_2}{\omega_1})\sin(s_1\omega_1)$$ and so the roots of $s_1\mapsto f(s_1,2\pi/\omega_1-s_1)$ are the $\pi/\omega_1\mathbb{N}$. In the, we know that the set of points $$\mathcal{S}=\Big\{ \frac{\pi}{\omega_i}(p,2p-q),\ p,q\in\mathbb{N}, i\in\{1,2\}\Big\} \cap {\mathbb{R}^+}^2$$ is a subset of the solution curves of $f$, as illustrated below: or, for another set of parameters $(r_1,r_2,\omega_1,\omega_2)$: Red and green points correspond to $i=1$ and $i=2$. Question On both figures, all the curves pass through the set of points $\mathcal{S}$. Is this always true, or can there be some curves which do not pass through any green or red points? I am also interested in references or keywords for mathematical tools which could be useful for the study of solution curves of functions of the ""type"" of $f$ (i.e. sum of products of sines), even though I don't think there are miraculous simplification.",,"['real-analysis', 'trigonometry', 'algebraic-geometry', 'roots']"
85,Positivity of a convolution integral,Positivity of a convolution integral,,"Let $f\in\mathcal{S}(\mathbb{R}^3)$ a real function. Consider the following integral $$I_f:=\int_{\mathbb{R}^3}\int_{\mathbb{R}^3}\frac{f(x)f(y)}{|x-y|^2}dydx$$ Observe that,  either if $f$ is positive or negative, the integral becomes positive. Is it true that $I_f\geq 0$ for a generic $f$?","Let $f\in\mathcal{S}(\mathbb{R}^3)$ a real function. Consider the following integral $$I_f:=\int_{\mathbb{R}^3}\int_{\mathbb{R}^3}\frac{f(x)f(y)}{|x-y|^2}dydx$$ Observe that,  either if $f$ is positive or negative, the integral becomes positive. Is it true that $I_f\geq 0$ for a generic $f$?",,"['real-analysis', 'integration', 'convolution']"
86,Showing that a function is discontinuous,Showing that a function is discontinuous,,"Consider the function $c(x)$ defined by $c(x)=1+x-\left\lceil x+\frac{1}{2}\right\rceil $ . I want to show that the function $$F(x)=\sum_{n=1}^\infty\frac{c(nx)}{n^2}  $$ is discontinuous in each point $\frac{m}{2n}$ , where $m$ is odd and $n\neq0$ . So far I have proved that the only possible points of discontinuity are these ones, and I've realized to that the discontinuous points of $c(nx)$ are the points $\frac{2k-1}{2n}$ , $k\in\mathbb N$ , so this will be the problematic points in $F(x)$ , but I don't know how to write this final part of the proof.","Consider the function defined by . I want to show that the function is discontinuous in each point , where is odd and . So far I have proved that the only possible points of discontinuity are these ones, and I've realized to that the discontinuous points of are the points , , so this will be the problematic points in , but I don't know how to write this final part of the proof.",c(x) c(x)=1+x-\left\lceil x+\frac{1}{2}\right\rceil  F(x)=\sum_{n=1}^\infty\frac{c(nx)}{n^2}   \frac{m}{2n} m n\neq0 c(nx) \frac{2k-1}{2n} k\in\mathbb N F(x),"['real-analysis', 'continuity', 'ceiling-and-floor-functions']"
87,"A possible norm on a subspace of $C^\infty([0,1])$?",A possible norm on a subspace of ?,"C^\infty([0,1])","My question is related to this one : Take the vector space of infinitely differentiable functions on $[0,1]$. The standard norm of $C^k([0,1])$ is just the $\ell^1$-norm of the vector $(\|f\|_\infty, \|f'\|_\infty,\ldots,\|f^{(k)}\|_\infty)$, but of course this idea cannot be further pursued to define a norm on $C^\infty([0,1])$. However, what if one would consider the space $$ \{f\in C^\infty([0,1]):(\|f^{(n)}\|_\infty)_{n\in\mathbb N}\in \ell^p \} $$ for $p\in [1,\infty]$? This space is certainly small - in particular, it contains neither the exponential function, nor $\sin$ and $\cos$ - but at least it does contain the polynomials and it seems to be a Banach space - in fact even a Banach lattice algebra. Does this space appear in applications (PDEs?)? Has anybody ever studied its functional analytical properties and if this is not the case, what are this space's obvious drawbacks?","My question is related to this one : Take the vector space of infinitely differentiable functions on $[0,1]$. The standard norm of $C^k([0,1])$ is just the $\ell^1$-norm of the vector $(\|f\|_\infty, \|f'\|_\infty,\ldots,\|f^{(k)}\|_\infty)$, but of course this idea cannot be further pursued to define a norm on $C^\infty([0,1])$. However, what if one would consider the space $$ \{f\in C^\infty([0,1]):(\|f^{(n)}\|_\infty)_{n\in\mathbb N}\in \ell^p \} $$ for $p\in [1,\infty]$? This space is certainly small - in particular, it contains neither the exponential function, nor $\sin$ and $\cos$ - but at least it does contain the polynomials and it seems to be a Banach space - in fact even a Banach lattice algebra. Does this space appear in applications (PDEs?)? Has anybody ever studied its functional analytical properties and if this is not the case, what are this space's obvious drawbacks?",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
88,prove uniqueness from orthogonality relation,prove uniqueness from orthogonality relation,,"The problem: we have two functions $f(x), g(x)\in C^{1}[-\pi, \pi]$, and we know that \begin{align} \int_{-\pi}^{\pi} \int_{-\pi}^{\pi} \cos\left(ky\right) \sin\left(k\left\lvert y-z\right\rvert\right)\:f\left(y\right)g\left(z\right) \,dy\, dz &= 0,  \\ \int_{-\pi}^{\pi} \int_{-\pi}^{\pi} \sin\left(ky\right) \sin\left(k\left\lvert y-z\right\rvert\right)\:f\left(y\right)g\left(z\right) \,dy\, dz &= 0,  \end{align} for all $k\in\mathbb{R}$. What I want to prove is: if $g(z)$ is not zero function, then $f(y)$ must be $0$. Some results: the problem's difficult part is the absolute value, otherwise it is simply Fourier transform. if we take the above first equation as $H(k)$, then differentiating at $k=0$ will give $\forall p\in \mathbb{N}$, the first equation gives $$\iint \sum_{m+n=p} \frac{y^{2m} \left\lvert y-z\right\rvert^{2n+1}}{\left(2m\right)!\left(2n+1\right)!} \:f\left(y\right)g\left(z\right) \,dy\, dz = 0,$$ the second gives a similar one. Thanks in advance.","The problem: we have two functions $f(x), g(x)\in C^{1}[-\pi, \pi]$, and we know that \begin{align} \int_{-\pi}^{\pi} \int_{-\pi}^{\pi} \cos\left(ky\right) \sin\left(k\left\lvert y-z\right\rvert\right)\:f\left(y\right)g\left(z\right) \,dy\, dz &= 0,  \\ \int_{-\pi}^{\pi} \int_{-\pi}^{\pi} \sin\left(ky\right) \sin\left(k\left\lvert y-z\right\rvert\right)\:f\left(y\right)g\left(z\right) \,dy\, dz &= 0,  \end{align} for all $k\in\mathbb{R}$. What I want to prove is: if $g(z)$ is not zero function, then $f(y)$ must be $0$. Some results: the problem's difficult part is the absolute value, otherwise it is simply Fourier transform. if we take the above first equation as $H(k)$, then differentiating at $k=0$ will give $\forall p\in \mathbb{N}$, the first equation gives $$\iint \sum_{m+n=p} \frac{y^{2m} \left\lvert y-z\right\rvert^{2n+1}}{\left(2m\right)!\left(2n+1\right)!} \:f\left(y\right)g\left(z\right) \,dy\, dz = 0,$$ the second gives a similar one. Thanks in advance.",,"['calculus', 'real-analysis', 'analysis', 'functional-analysis', 'partial-differential-equations']"
89,The study of the properties of the solution of Heat equation.,The study of the properties of the solution of Heat equation.,,"I am studying the following basic heat equation. (All notations follows Evans book) \begin{align} u_t -\Delta u =0 &\text{ in }\mathbb R^2\times(0,\infty)\\ u=u_0 &\text{ on }\mathbb R^2\times\{t=0\} \end{align} i.e., the Cauchy problem. Here we assume $u_0\in C_c^\infty(\mathbb R^2)$ and $u_0$ is non-negative. Let $u(x,t)$ denote the standard solution so that  $$ u(x,t):=\frac{1}{4\pi t}\int_{\mathbb R^2}e^{-\frac{|x-y|^2}{4t}}u_0(y)dy \tag 1 $$ Now given a function $a(x)\in C_c^\infty(\mathbb R^2)$ so that  $$ \int_{\mathbb R^2}a(x)\,dx=0,\,\,\int_{\mathbb R^2}a(x)u_0(x)\,dx=C = \int_{\mathbb R^2}(a(x))^2dx, $$ where $C>0$ is a constant. (Note the above equation does not mean $u_0=a$) Now I need to study the following equation: $$ f(t):=\int_{\mathbb R^2} [u(x,t)-u_0(x)]\cdot\partial_t u(x,t)\,dx - \int_{\mathbb R^2} \partial_tu(x,t)\cdot a(x)\,dx. \tag 2 $$ Notice that  $$f(t) = \frac{d}{dt} \frac{1}{2}\int (u(x,t)-u_0+a)^2dx$$ My questions: Assume that there exist $t_0>0$ so that $f(t_0)=0$. Prove that for all $0<t<t_0$, $f(t)<0$; and for all $t>t_0$, $f(t)>0$. And I expect $f(t)\to 0$ as $t\to\infty$. Give a condition on $u_0$ so that there exists such $t_0>0$. (the condition can not be $u_0$ is a constant.) What I tried so far is put $(1)$ into $(2)$ and see what's going on. But so far I got nothing... I suspect  $$ 0\leq \int_{\mathbb R^2}a(x)u(x,t_2)\,dx\leq \int_{\mathbb R^2}a(x)u(x,t_1)\,dx\leq \int_{\mathbb R^2}a(x)u_0(x)\,dx $$ for $0<t_1<t_2$ but I can not prove it. Any help is really welcome!","I am studying the following basic heat equation. (All notations follows Evans book) \begin{align} u_t -\Delta u =0 &\text{ in }\mathbb R^2\times(0,\infty)\\ u=u_0 &\text{ on }\mathbb R^2\times\{t=0\} \end{align} i.e., the Cauchy problem. Here we assume $u_0\in C_c^\infty(\mathbb R^2)$ and $u_0$ is non-negative. Let $u(x,t)$ denote the standard solution so that  $$ u(x,t):=\frac{1}{4\pi t}\int_{\mathbb R^2}e^{-\frac{|x-y|^2}{4t}}u_0(y)dy \tag 1 $$ Now given a function $a(x)\in C_c^\infty(\mathbb R^2)$ so that  $$ \int_{\mathbb R^2}a(x)\,dx=0,\,\,\int_{\mathbb R^2}a(x)u_0(x)\,dx=C = \int_{\mathbb R^2}(a(x))^2dx, $$ where $C>0$ is a constant. (Note the above equation does not mean $u_0=a$) Now I need to study the following equation: $$ f(t):=\int_{\mathbb R^2} [u(x,t)-u_0(x)]\cdot\partial_t u(x,t)\,dx - \int_{\mathbb R^2} \partial_tu(x,t)\cdot a(x)\,dx. \tag 2 $$ Notice that  $$f(t) = \frac{d}{dt} \frac{1}{2}\int (u(x,t)-u_0+a)^2dx$$ My questions: Assume that there exist $t_0>0$ so that $f(t_0)=0$. Prove that for all $0<t<t_0$, $f(t)<0$; and for all $t>t_0$, $f(t)>0$. And I expect $f(t)\to 0$ as $t\to\infty$. Give a condition on $u_0$ so that there exists such $t_0>0$. (the condition can not be $u_0$ is a constant.) What I tried so far is put $(1)$ into $(2)$ and see what's going on. But so far I got nothing... I suspect  $$ 0\leq \int_{\mathbb R^2}a(x)u(x,t_2)\,dx\leq \int_{\mathbb R^2}a(x)u(x,t_1)\,dx\leq \int_{\mathbb R^2}a(x)u_0(x)\,dx $$ for $0<t_1<t_2$ but I can not prove it. Any help is really welcome!",,"['real-analysis', 'functional-analysis', 'partial-differential-equations']"
90,"Equivalent, finite measures if and only if strictly positive Radon-Nikodym derivative exists","Equivalent, finite measures if and only if strictly positive Radon-Nikodym derivative exists",,"Problem statement: Let $(X,\mathcal{A})$ be a measurable space, and let $\mu$ and $\nu$ be two finite measures. We say $\mu$ and $\nu$ are equivalent measures if $\mu \ll \nu$ and $\nu \ll \mu$ (if $\nu$ and $\mu$ are each absolutely continuous w.r.t. the other). Show that $\mu$ and $\nu$ are equivalent if and only if there exists a $\mu$-integrable function $f$ is that is strictly positive a.e.  w.r.t. $\mu$ such that $d \nu = f d\mu$. My attempt at a solution: For the reverse direction, I tried to use the Radon-Nikodym theorem. Since $\mu$ is $\sigma$-finite, and $\nu$ is finite, and $\nu \ll \mu$, there exists a $\mu$-integrable, non-negative function, measurable function, call it $f$, such that $\nu(A) = \int_A f d\mu$ for all $A \in \mathcal{A}$. Now, we just need to show that this function is strictly positive a.e. to get our conclusion. What I tried was to set $B = \{x : f(x) = 0\}$, and noting that  $$\nu(B) = \int_B f d\mu,$$ we find that $\nu(B) = 0$. But $\mu \ll \nu$, so we have that $\mu(B) = 0$, as well, and we are done. For the forward direction, we assume that there exists a $\mu$-integrable, strictly positive a.e. function w.r.t. $\mu$ such that $d\nu = f d\mu$. Now, this implies that for any measurable set $A$,  $$ \nu(A) = \int_Af d\mu.$$ It is clear that $\nu(A) \ll \mu(A)$. Now if $\nu(A) = 0$, we have that $0 = \int_A fd\mu$. What I would like to say there is that this implies that $f = 0$ a.e. on $A$, which I believe I can prove, but that since $f$ is strictly positive a.e., that therefore $\mu(A) = 0$. Here is my problem: I don't think that I have used that $\mu$ is a finite measure anywhere, which makes me very uncomfortable. I also am a little unsure regarding the argument for the forward direction. My apologies for posting so many homework-sounding questions in the analysis section lately - I'm studying for a qual in September, so I'm trying to do a bunch of problems before then, and this seems like a great place to get help when I get stuck - hopefully I'm not being too annoying.","Problem statement: Let $(X,\mathcal{A})$ be a measurable space, and let $\mu$ and $\nu$ be two finite measures. We say $\mu$ and $\nu$ are equivalent measures if $\mu \ll \nu$ and $\nu \ll \mu$ (if $\nu$ and $\mu$ are each absolutely continuous w.r.t. the other). Show that $\mu$ and $\nu$ are equivalent if and only if there exists a $\mu$-integrable function $f$ is that is strictly positive a.e.  w.r.t. $\mu$ such that $d \nu = f d\mu$. My attempt at a solution: For the reverse direction, I tried to use the Radon-Nikodym theorem. Since $\mu$ is $\sigma$-finite, and $\nu$ is finite, and $\nu \ll \mu$, there exists a $\mu$-integrable, non-negative function, measurable function, call it $f$, such that $\nu(A) = \int_A f d\mu$ for all $A \in \mathcal{A}$. Now, we just need to show that this function is strictly positive a.e. to get our conclusion. What I tried was to set $B = \{x : f(x) = 0\}$, and noting that  $$\nu(B) = \int_B f d\mu,$$ we find that $\nu(B) = 0$. But $\mu \ll \nu$, so we have that $\mu(B) = 0$, as well, and we are done. For the forward direction, we assume that there exists a $\mu$-integrable, strictly positive a.e. function w.r.t. $\mu$ such that $d\nu = f d\mu$. Now, this implies that for any measurable set $A$,  $$ \nu(A) = \int_Af d\mu.$$ It is clear that $\nu(A) \ll \mu(A)$. Now if $\nu(A) = 0$, we have that $0 = \int_A fd\mu$. What I would like to say there is that this implies that $f = 0$ a.e. on $A$, which I believe I can prove, but that since $f$ is strictly positive a.e., that therefore $\mu(A) = 0$. Here is my problem: I don't think that I have used that $\mu$ is a finite measure anywhere, which makes me very uncomfortable. I also am a little unsure regarding the argument for the forward direction. My apologies for posting so many homework-sounding questions in the analysis section lately - I'm studying for a qual in September, so I'm trying to do a bunch of problems before then, and this seems like a great place to get help when I get stuck - hopefully I'm not being too annoying.",,"['real-analysis', 'measure-theory']"
91,Theorem 12.13 of Apostol's Mathematical Analysis (2nd Ed),Theorem 12.13 of Apostol's Mathematical Analysis (2nd Ed),,"I had a little difficulty understanding Theorem 12.13 of Apostol's Mathematical Analysis, and was wondering if someone who had read this book could give me some pointers.  Specifically, on page 360, the author writes "" As a consequence of Theorem 12.11 and 12.12 we have Theorem 12.13 If both partial derivatives $D_{r}f$ and $D_{k}f$ exist in an n-ball $B(c)$ and if both $D_{k,r}f$ and $D_{r,k}f$ are continuous at c, then $D_{k,r}f(c) = D_{r,k}f(c)$. "" My understanding is that we're using Theorem 12.11 to establish the differentiability of $D_{r}f$ and $D_{k}f$ so that we can use Theorem 12.12 to assert the equality of $D_{k,r}f(c) = D_{r,k}f(c)$.  If so, my question is: why shouldn't we require $D_{r,r}f$ and $D_{k,k}f$ also exist at c? For convenience, I typed Theorem 12.11 & 12.12 below: "" Theorem 12.11 Assume that one of the partial derivatives $D_{1}f,...,D_{n}f$ exists at c and that the remaining n-1 partial derivatives exist in some n-ball $B(c)$ and are continuous at c.  Then $f$ is differentiable at c. Theorem 12.12 If both partial derivatives $D_{r}f$ and $D_{k}f$ exist in an n-ball $B(c)$ and if both are differentiable at c, then $D_{k,r}f(c) = D_{r,k}f(c)$. ""","I had a little difficulty understanding Theorem 12.13 of Apostol's Mathematical Analysis, and was wondering if someone who had read this book could give me some pointers.  Specifically, on page 360, the author writes "" As a consequence of Theorem 12.11 and 12.12 we have Theorem 12.13 If both partial derivatives $D_{r}f$ and $D_{k}f$ exist in an n-ball $B(c)$ and if both $D_{k,r}f$ and $D_{r,k}f$ are continuous at c, then $D_{k,r}f(c) = D_{r,k}f(c)$. "" My understanding is that we're using Theorem 12.11 to establish the differentiability of $D_{r}f$ and $D_{k}f$ so that we can use Theorem 12.12 to assert the equality of $D_{k,r}f(c) = D_{r,k}f(c)$.  If so, my question is: why shouldn't we require $D_{r,r}f$ and $D_{k,k}f$ also exist at c? For convenience, I typed Theorem 12.11 & 12.12 below: "" Theorem 12.11 Assume that one of the partial derivatives $D_{1}f,...,D_{n}f$ exists at c and that the remaining n-1 partial derivatives exist in some n-ball $B(c)$ and are continuous at c.  Then $f$ is differentiable at c. Theorem 12.12 If both partial derivatives $D_{r}f$ and $D_{k}f$ exist in an n-ball $B(c)$ and if both are differentiable at c, then $D_{k,r}f(c) = D_{r,k}f(c)$. """,,['real-analysis']
92,"When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$?","When , what are the solutions for ?",f(x+1)-f(x)=f'(x) f(x),"The question is: When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? The most obvious solution is a linear function of the form $f(x)=ax+b$. Is this the only solution? Edit I should add that $f:\mathbb R\to\mathbb R$ to the question.","The question is: When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? The most obvious solution is a linear function of the form $f(x)=ax+b$. Is this the only solution? Edit I should add that $f:\mathbb R\to\mathbb R$ to the question.",,"['real-analysis', 'ordinary-differential-equations', 'delay-differential-equations']"
93,"Prove that $a_{n}=0$ for all $n$, if $\sum a_{kn}=0$ for all $k\geq 1$","Prove that  for all , if  for all",a_{n}=0 n \sum a_{kn}=0 k\geq 1,Let $\sum a_{n}$ be an absolutely convergent series such that $$\sum a_{kn}=0$$ for all $k\geq 1$.   Help me prove that $a_{n}=0$ for all $n$. Thank you!,Let $\sum a_{n}$ be an absolutely convergent series such that $$\sum a_{kn}=0$$ for all $k\geq 1$.   Help me prove that $a_{n}=0$ for all $n$. Thank you!,,['sequences-and-series']
94,Smooth version of Tietze extension theorem,Smooth version of Tietze extension theorem,,"Thanks to the properties of mollifications, it is very easy to prove Urysohn's lemma in the euclidean space with the big plus that the constructed function is smooth. I was wondering if something of the kind could be achieved with Tietze extension theorem in the euclidean setting (i.e, can we always find smooth extensions of smooth functions in a closed set of the euclidean space?) The proof of Tietze's theorem runs by constructing iteratively a family of functions with the aid of Urysohn's lemma. This family extends the original function and converges uniformly. This is very nice, but I if I try to replicate the proof, I have no way to prove the uniform convergence of the derivatives, so I guess that if the theorem is true it will not follow these lines. Thanks","Thanks to the properties of mollifications, it is very easy to prove Urysohn's lemma in the euclidean space with the big plus that the constructed function is smooth. I was wondering if something of the kind could be achieved with Tietze extension theorem in the euclidean setting (i.e, can we always find smooth extensions of smooth functions in a closed set of the euclidean space?) The proof of Tietze's theorem runs by constructing iteratively a family of functions with the aid of Urysohn's lemma. This family extends the original function and converges uniformly. This is very nice, but I if I try to replicate the proof, I have no way to prove the uniform convergence of the derivatives, so I guess that if the theorem is true it will not follow these lines. Thanks",,"['real-analysis', 'analysis']"
95,How to integrate $\int_0^\infty\frac{dx}{1+x^n}$ [duplicate],How to integrate  [duplicate],\int_0^\infty\frac{dx}{1+x^n},"This question already has answers here : Improper integration involving complex analytic arguments: $\int_{0}^{\infty} \frac{1}{1+x^a}dx$, where $a>1$ and $a \in \mathbb{R}$ (3 answers) A few improper integral (1 answer) Closed 9 years ago . I was playing around with the function $\dfrac{1}{1+x^2}$, and knowing that the integral over $(0,\infty)$ was $\dfrac{\pi}{2}$, I was hoping to see if there was some neat pattern to determining the value of the following generalized form: $$I=\int_0^\infty \frac{dx}{1+x^n}$$ for natural $n\ge2$. I ran some computations in Mathematica and got a consistent result of $\dfrac{\pi}{n}\csc\left(\dfrac{\pi}{n}\right)$, which also happens to be equivalent to $\dfrac{1}{n}\Gamma\left(\dfrac{1}{n}\right)\Gamma\left(1-\dfrac{1}{n}\right)=\mathrm{B}\left(\dfrac{1}{n},1-\dfrac{1}{n}\right)$. Is there some neat trick I can use to get this result? I've considered integrating along a contour, but I'm somewhat rusty with complex methods. Also, the result was posted previously here . I tried something like this instead: $$\frac{1}{1+x^n}=\begin{cases}\displaystyle \sum_{k=0}^\infty(-1)^kx^{nk}&\text{for }|x|<1\\\\ \displaystyle\sum_{k=0}^\infty (-1)^k x^{-n(k+1)}&\text{for }|x|>1\end{cases}$$ then integrating along the respective intervals $(0,1)$ and $(1,\infty)$. Here's my attempt: $$\begin{align*}I &=\left\{\int_0^1+\int_1^\infty\right\}\frac{dx}{1+x^n}\\\\ &=\sum_{k=0}^\infty \left\{\int_0^1 (-1)^kx^{nk}\,dx+\int_1^\infty (-1)^kx^{-n(k+1)}\,dx\right\}\\\\ &=\sum_{k=0}^\infty \left\{(-1)^k\left[\frac{x^{nk+1}}{nk+1}\right]_{0}^{1}+(-1)^k\left[\frac{x^{-n(k+1)+1}}{-n(k+1)+1}\right]_1^\infty\right\}\\\\ &=\sum_{k=0}^\infty \left\{\frac{(-1)^k}{nk+1}-\frac{(-1)^{k}}{1-n(k+1)}\right\}\\\\ &=\left(1-\frac{1}{1-n}\right)+\left(-\frac{1}{n+1}+\frac{1}{1-2n}\right)\\ &\quad\quad+\left(\frac{1}{2n+1}-\frac{1}{1-3n}\right)+\left(-\frac{1}{3n+1}+\frac{1}{1-4n}\right)+\cdots\\\\ &=1-\frac{2}{1-n^2}+\frac{2}{1-4n^2}-\frac{2}{1-9n^2}+\cdots\\\\ &=\sum_{k=0}^\infty \frac{(-2)^k}{1-(kn)^2}\end{align*}$$ However, this series diverges by the ratio test, so I'm not sure if I made a mistake in my calculation or used some flawed reasoning.","This question already has answers here : Improper integration involving complex analytic arguments: $\int_{0}^{\infty} \frac{1}{1+x^a}dx$, where $a>1$ and $a \in \mathbb{R}$ (3 answers) A few improper integral (1 answer) Closed 9 years ago . I was playing around with the function $\dfrac{1}{1+x^2}$, and knowing that the integral over $(0,\infty)$ was $\dfrac{\pi}{2}$, I was hoping to see if there was some neat pattern to determining the value of the following generalized form: $$I=\int_0^\infty \frac{dx}{1+x^n}$$ for natural $n\ge2$. I ran some computations in Mathematica and got a consistent result of $\dfrac{\pi}{n}\csc\left(\dfrac{\pi}{n}\right)$, which also happens to be equivalent to $\dfrac{1}{n}\Gamma\left(\dfrac{1}{n}\right)\Gamma\left(1-\dfrac{1}{n}\right)=\mathrm{B}\left(\dfrac{1}{n},1-\dfrac{1}{n}\right)$. Is there some neat trick I can use to get this result? I've considered integrating along a contour, but I'm somewhat rusty with complex methods. Also, the result was posted previously here . I tried something like this instead: $$\frac{1}{1+x^n}=\begin{cases}\displaystyle \sum_{k=0}^\infty(-1)^kx^{nk}&\text{for }|x|<1\\\\ \displaystyle\sum_{k=0}^\infty (-1)^k x^{-n(k+1)}&\text{for }|x|>1\end{cases}$$ then integrating along the respective intervals $(0,1)$ and $(1,\infty)$. Here's my attempt: $$\begin{align*}I &=\left\{\int_0^1+\int_1^\infty\right\}\frac{dx}{1+x^n}\\\\ &=\sum_{k=0}^\infty \left\{\int_0^1 (-1)^kx^{nk}\,dx+\int_1^\infty (-1)^kx^{-n(k+1)}\,dx\right\}\\\\ &=\sum_{k=0}^\infty \left\{(-1)^k\left[\frac{x^{nk+1}}{nk+1}\right]_{0}^{1}+(-1)^k\left[\frac{x^{-n(k+1)+1}}{-n(k+1)+1}\right]_1^\infty\right\}\\\\ &=\sum_{k=0}^\infty \left\{\frac{(-1)^k}{nk+1}-\frac{(-1)^{k}}{1-n(k+1)}\right\}\\\\ &=\left(1-\frac{1}{1-n}\right)+\left(-\frac{1}{n+1}+\frac{1}{1-2n}\right)\\ &\quad\quad+\left(\frac{1}{2n+1}-\frac{1}{1-3n}\right)+\left(-\frac{1}{3n+1}+\frac{1}{1-4n}\right)+\cdots\\\\ &=1-\frac{2}{1-n^2}+\frac{2}{1-4n^2}-\frac{2}{1-9n^2}+\cdots\\\\ &=\sum_{k=0}^\infty \frac{(-2)^k}{1-(kn)^2}\end{align*}$$ However, this series diverges by the ratio test, so I'm not sure if I made a mistake in my calculation or used some flawed reasoning.",,"['real-analysis', 'integration', 'sequences-and-series', 'complex-analysis', 'improper-integrals']"
96,An a.e.-defined derivative which is not Lebesgue integrable on any interval?,An a.e.-defined derivative which is not Lebesgue integrable on any interval?,,"If the derivative $f'$ exists everywhere then it is shown here that there exist intervals on which $f'$ is Lebesgue integrable. But perhaps there is a function $f$ such that $f'$ only exists almost everywhere and $f'$ is not Lebesgue integrable on any interval? Basically I'm trying to make sense of a remark on p. 111 of Folland's Real Analysis : ""it is not hard to construct examples in which the singularities of $f'$ are so complicated that $f'$ is not Lebesgue integrable on any interval. In this situation the Lebesgue integral is simply insufficient. However, the Henstock-Kurzweil integral [...] is powerful enough to integrate such $f'$ [...].""","If the derivative $f'$ exists everywhere then it is shown here that there exist intervals on which $f'$ is Lebesgue integrable. But perhaps there is a function $f$ such that $f'$ only exists almost everywhere and $f'$ is not Lebesgue integrable on any interval? Basically I'm trying to make sense of a remark on p. 111 of Folland's Real Analysis : ""it is not hard to construct examples in which the singularities of $f'$ are so complicated that $f'$ is not Lebesgue integrable on any interval. In this situation the Lebesgue integral is simply insufficient. However, the Henstock-Kurzweil integral [...] is powerful enough to integrate such $f'$ [...].""",,"['real-analysis', 'integration', 'derivatives', 'lebesgue-integral', 'examples-counterexamples']"
97,Proving another digammabinomial series result,Proving another digammabinomial series result,,"This series is related to some extent to the previous question of mine, that is Computing $\sum_{n=1}^{\infty} \frac{\psi\left(\frac{n+1}{2}\right)}{ \binom{2n}{n}}$ , where an approach by series only is expected. To answer one of the questions in the previous post, no CAS is able to compute these series, and I suppose this won't be possible in the near future either. Prove that  $$\sum_{n=1}^{\infty} \left(\frac{\displaystyle \psi(n)-\psi\left(n-\frac{1}{2}\right)}{\displaystyle \binom{4n-2}{2n-1}}+\frac{\displaystyle \psi\left(n+\frac{1}{2}\right)-\psi(n)}{\displaystyle \binom{4n}{2n}}\right) =$$ $$\frac{\pi}{3\sqrt{3}}+\frac{1}{75}\left(\log(1073741824)-3\sqrt{5} \pi^2+\pi\left(\Re \{{4 i \sqrt{5} \log(5- \sqrt{5} + i (\sqrt{15}- \sqrt{3})  ) -   2 i \sqrt{5} \log(5 +\sqrt{5}+ i (\sqrt{3} + \sqrt{15}))\}}+6\sqrt{5}\arctan\left(\sqrt{\frac{5}{3}}\right)-5\sqrt{3}\right)+\sqrt{5}\left(\log(79228162514264337593543950336) \log\left(\frac{3-\sqrt{5}}{2}\right) +  \log(324518553658426726783156020576256)\log\left(\frac{3+\sqrt{5}}{2}\right)+24\Re \left\{\operatorname{Li}_2\left(\frac{\sqrt{3}+i}{\sqrt{3}+i\sqrt{5}}\right)-\operatorname{Li}_2\left(\frac{\sqrt{3}+i}{\sqrt{3}-i\sqrt{5}}\right)\right\}\right)\right)$$ A supplementary question -  Find the closed form of $$\sum_{n=1}^{\infty} (-1)^{n+1}\left(\frac{\displaystyle \psi(n)-\psi\left(n-\frac{1}{2}\right)}{\displaystyle \binom{4n-2}{2n-1}}+\frac{\displaystyle \psi\left(n+\frac{1}{2}\right)-\psi(n)}{\displaystyle \binom{4n}{2n}}\right)$$ Hopefully Cleo might help with the alternating version.","This series is related to some extent to the previous question of mine, that is Computing $\sum_{n=1}^{\infty} \frac{\psi\left(\frac{n+1}{2}\right)}{ \binom{2n}{n}}$ , where an approach by series only is expected. To answer one of the questions in the previous post, no CAS is able to compute these series, and I suppose this won't be possible in the near future either. Prove that  $$\sum_{n=1}^{\infty} \left(\frac{\displaystyle \psi(n)-\psi\left(n-\frac{1}{2}\right)}{\displaystyle \binom{4n-2}{2n-1}}+\frac{\displaystyle \psi\left(n+\frac{1}{2}\right)-\psi(n)}{\displaystyle \binom{4n}{2n}}\right) =$$ $$\frac{\pi}{3\sqrt{3}}+\frac{1}{75}\left(\log(1073741824)-3\sqrt{5} \pi^2+\pi\left(\Re \{{4 i \sqrt{5} \log(5- \sqrt{5} + i (\sqrt{15}- \sqrt{3})  ) -   2 i \sqrt{5} \log(5 +\sqrt{5}+ i (\sqrt{3} + \sqrt{15}))\}}+6\sqrt{5}\arctan\left(\sqrt{\frac{5}{3}}\right)-5\sqrt{3}\right)+\sqrt{5}\left(\log(79228162514264337593543950336) \log\left(\frac{3-\sqrt{5}}{2}\right) +  \log(324518553658426726783156020576256)\log\left(\frac{3+\sqrt{5}}{2}\right)+24\Re \left\{\operatorname{Li}_2\left(\frac{\sqrt{3}+i}{\sqrt{3}+i\sqrt{5}}\right)-\operatorname{Li}_2\left(\frac{\sqrt{3}+i}{\sqrt{3}-i\sqrt{5}}\right)\right\}\right)\right)$$ A supplementary question -  Find the closed form of $$\sum_{n=1}^{\infty} (-1)^{n+1}\left(\frac{\displaystyle \psi(n)-\psi\left(n-\frac{1}{2}\right)}{\displaystyle \binom{4n-2}{2n-1}}+\frac{\displaystyle \psi\left(n+\frac{1}{2}\right)-\psi(n)}{\displaystyle \binom{4n}{2n}}\right)$$ Hopefully Cleo might help with the alternating version.",,"['calculus', 'real-analysis', 'sequences-and-series']"
98,Construction of a Borel set with positive but not full measure in each interval,Construction of a Borel set with positive but not full measure in each interval,,"I was wondering how one can construct a Borel set that doesn't have full measure on any interval of the real line but does have positive measure everywhere. To be precise, if $\mu$ denotes Lebesgue measure, how would one construct a Borel set $A \subset \mathbb{R}$ such that $$0 < \mu(A \cap I) < \mu(I)$$ for every interval $I$ in $\mathbb{R}$? Moreover, would such a set necessarily have to contain infinite measure?","I was wondering how one can construct a Borel set that doesn't have full measure on any interval of the real line but does have positive measure everywhere. To be precise, if $\mu$ denotes Lebesgue measure, how would one construct a Borel set $A \subset \mathbb{R}$ such that $$0 < \mu(A \cap I) < \mu(I)$$ for every interval $I$ in $\mathbb{R}$? Moreover, would such a set necessarily have to contain infinite measure?",,"['real-analysis', 'measure-theory']"
99,Another way of expressing $\sum_{k=0}^{n} (-1)^k\frac{H_{k+1}}{n-k+1}$,Another way of expressing,\sum_{k=0}^{n} (-1)^k\frac{H_{k+1}}{n-k+1},In this post Another way of expressing $\sum_{k=0}^{n} \frac{H_{k+1}}{n-k+1}$ I asked for a solution of the non-alternating series. How about the alternating series? Can we find a nice way of expressing this sum? $$\sum_{k=0}^{n} (-1)^k\frac{H_{k+1}}{n-k+1}$$,In this post Another way of expressing $\sum_{k=0}^{n} \frac{H_{k+1}}{n-k+1}$ I asked for a solution of the non-alternating series. How about the alternating series? Can we find a nice way of expressing this sum? $$\sum_{k=0}^{n} (-1)^k\frac{H_{k+1}}{n-k+1}$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'complex-analysis']"
