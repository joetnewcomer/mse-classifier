,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Elementary proof of Wright-Fisher's time until fixation formula?,Elementary proof of Wright-Fisher's time until fixation formula?,,"Consider the Markov process $\def\Z{\mathbb{Z}}\def\N{\mathbb{N}}\def\E{\text{E}}\newcommand{\bp}[1]{{\left({#1}\right)}}(X_t^n)_{t\in\Z_+}$ given by $X_0^n = p \in [0,1]$ and $X_{t+1}^n|X_t^n \sim \frac1n \text{Binomial}(n, X_t)$ , where $n\in\N$ . In other words, $X_t^n \in \{0, \frac1n, \ldots, \frac{n-1}n, 1\}$ for $t\geq 1$ , and $$\Pr\bp{X_{t+1}^n = \frac kn \mid X_t^n = p} = \binom{n}{k} p^k (1-p)^{n-k}.$$ It's easy to see that $X_t^n$ is a martingale and it converges almost surely to either $0$ or $1$ . Let $\tau_n = \min\{t\geq1 : X_t^n \in \{0, 1\}\}$ be the time until fixation. It's known that $$\lim_{n\to\infty} \E\bp{\frac{\tau_n}n \mid X_0=x} = -2x\log x - 2(1-x)\log(1-x).$$ The proof of that formula requires using a diffusion approximation, which I find super hard to construct (it's done rigorously in this book , for example). Can you help me find an elementary proof? My idea is the following: let $f_n(x) := \E(\frac1n\tau_n|X_0=x)$ . We have $f_n(x) = \frac1n + \sum_{k=1}^{n-1} p_n(x,k) f_n(k/n)$ for $x\in[0,1]$ , where $p_n(x,k) := \binom{n}{k} x^k (1-x)^{n-k}$ . For each $n$ , $f_n(x)$ is a polynomial in $x$ , so we get $$\sum_{k=0}^n p_n(x,k) \bp{f_n\bp{\frac kn} - f_n(x)} + \frac1n\bp{1 - p_n(x,0) - p_n(x,1)} = 0.$$ We can do a Taylor expansion $$\sum_{k=0}^n p_n(x,k) \bp{f_n'(x)\bp{\frac kn - x} + \frac12 f_n''(x) \bp{\frac kn - x}^2 + \frac16 f_n'''(\xi_k) \bp{\frac kn - x}^3} + \frac1n\bp{1 - p_n(x,0) - p_n(x,1)} = 0.$$ We have $\sum_{k=0}^n p_n(x,k)\bp{\frac kn - x} = 0$ , $\sum_{k=0}^n p_n(x,k) \bp{\frac kn - x}^2 = \frac1n x(1-x)$ and $\sum_{k=0}^n p_n(x,k) \bp{\frac kn - x}^3 = \frac{1}{n^2} x(1-x)(1-2x)$ , so replacing we get $$\frac12 f_n''(x) x(1-x) = -1 + R_n(x),$$ where $|R_n(x)| \leq \frac1n \|f_n'''\|_\infty x(1-x)(1-2x) + p_n(x,0) + p_n(x,1)$ . If we can prove that $f_n, f_n', f_n'', f_n'''$ are all bounded we can pass to the limit, and we get the result. This doesn't sound so hard.","Consider the Markov process given by and , where . In other words, for , and It's easy to see that is a martingale and it converges almost surely to either or . Let be the time until fixation. It's known that The proof of that formula requires using a diffusion approximation, which I find super hard to construct (it's done rigorously in this book , for example). Can you help me find an elementary proof? My idea is the following: let . We have for , where . For each , is a polynomial in , so we get We can do a Taylor expansion We have , and , so replacing we get where . If we can prove that are all bounded we can pass to the limit, and we get the result. This doesn't sound so hard.","\def\Z{\mathbb{Z}}\def\N{\mathbb{N}}\def\E{\text{E}}\newcommand{\bp}[1]{{\left({#1}\right)}}(X_t^n)_{t\in\Z_+} X_0^n = p \in [0,1] X_{t+1}^n|X_t^n \sim \frac1n \text{Binomial}(n, X_t) n\in\N X_t^n \in \{0, \frac1n, \ldots, \frac{n-1}n, 1\} t\geq 1 \Pr\bp{X_{t+1}^n = \frac kn \mid X_t^n = p} = \binom{n}{k} p^k (1-p)^{n-k}. X_t^n 0 1 \tau_n = \min\{t\geq1 : X_t^n \in \{0, 1\}\} \lim_{n\to\infty} \E\bp{\frac{\tau_n}n \mid X_0=x} = -2x\log x - 2(1-x)\log(1-x). f_n(x) := \E(\frac1n\tau_n|X_0=x) f_n(x) = \frac1n + \sum_{k=1}^{n-1} p_n(x,k) f_n(k/n) x\in[0,1] p_n(x,k) := \binom{n}{k} x^k (1-x)^{n-k} n f_n(x) x \sum_{k=0}^n p_n(x,k) \bp{f_n\bp{\frac kn} - f_n(x)} + \frac1n\bp{1 - p_n(x,0) - p_n(x,1)} = 0. \sum_{k=0}^n p_n(x,k) \bp{f_n'(x)\bp{\frac kn - x} + \frac12 f_n''(x) \bp{\frac kn - x}^2 + \frac16 f_n'''(\xi_k) \bp{\frac kn - x}^3} + \frac1n\bp{1 - p_n(x,0) - p_n(x,1)} = 0. \sum_{k=0}^n p_n(x,k)\bp{\frac kn - x} = 0 \sum_{k=0}^n p_n(x,k) \bp{\frac kn - x}^2 = \frac1n x(1-x) \sum_{k=0}^n p_n(x,k) \bp{\frac kn - x}^3 = \frac{1}{n^2} x(1-x)(1-2x) \frac12 f_n''(x) x(1-x) = -1 + R_n(x), |R_n(x)| \leq \frac1n \|f_n'''\|_\infty x(1-x)(1-2x) + p_n(x,0) + p_n(x,1) f_n, f_n', f_n'', f_n'''","['probability', 'stochastic-processes', 'martingales']"
1,Estimate true probabilities from weighted sampling without replacement,Estimate true probabilities from weighted sampling without replacement,,"Suppose we have a random event with $n$ outcomes, with (unknown) true probabilities $p_1, p_2, \dots, p_n$ . We have performed a study of this event,  sampling it in batches of $k < n$ . From this we have found a good estimation of $q_i$ : the probability that outcome $i$ occurs in a batch of $k$ events. However, there is a crucial issue with our study. Each event $i$ can only occur once in a batch. Once an event $i$ has occurred, that event's probability becomes $0$ for the remainder of the batch (and the rest of the probability distribution normalized to sum to $1$ ). If we'd assume $\hat p_i = q_i$ then the most frequent outcomes are underestimated, and the lesser frequent overestimated. Can we do better, that is, find a good corrected estimate $\hat {\bf p}$ from $\bf q$ and $k$ ? Note that we don't know the order of the events within each batch, otherwise we could use the first event as an unbiased indicator. Nevertheless even if we did know the order within each batch it would be nice to also use the information in the events after the first in each batch effectively and correctly.","Suppose we have a random event with outcomes, with (unknown) true probabilities . We have performed a study of this event,  sampling it in batches of . From this we have found a good estimation of : the probability that outcome occurs in a batch of events. However, there is a crucial issue with our study. Each event can only occur once in a batch. Once an event has occurred, that event's probability becomes for the remainder of the batch (and the rest of the probability distribution normalized to sum to ). If we'd assume then the most frequent outcomes are underestimated, and the lesser frequent overestimated. Can we do better, that is, find a good corrected estimate from and ? Note that we don't know the order of the events within each batch, otherwise we could use the first event as an unbiased indicator. Nevertheless even if we did know the order within each batch it would be nice to also use the information in the events after the first in each batch effectively and correctly.","n p_1, p_2, \dots, p_n k < n q_i i k i i 0 1 \hat p_i = q_i \hat {\bf p} \bf q k","['probability', 'independence']"
2,Is this an okay way to calculate covid-19 death rates?,Is this an okay way to calculate covid-19 death rates?,,"I'm a tad rusty on my math skills but this isn't too hard. Can you confirm that I am doing this right. I wanted to know what percent of people die from covid-19 so I started looking for statistics. To my surprise the CDC , the WHO , and Google publish statistics: But then I found this article in Science which says that 86% of all infections go undocumented. To adjust for that I do: (total)(1-.86)=confirmed total = confirmed/.14 Which works out to: Then I wanted to see how that works out by age. To my surprise (again) the CDC has an awesome tool for that (Thank you CDC!). Here's a link to the tool . I can't link to my results because you have to login to save your search and I don't want to mess with that right now. Anyways I created the following report: Dimension: Age Group Measure: COVID-19 Deaths Filters:     From 2/1/2020-5/10/2020     Sex (Male and Female) And the results are : Which for the particular date range that I selected amounts to 87,116 deaths. If you transform that into what percent of people are dying by age you get: Next, I used the Census information for 2019 to get what percent each age range is of the total U.S. population (super lucky the age ranges matched up!!!): So right off the bat you can see it has a massively disproportionate effect on older folks ( interesting graph ). The last thing then is to figure out what the odds are of dying from covid-19 for each age range. I had to get some help for this part. I think I have all the numbers plugged in right. It just uses Bayes' Theorem (which sadly I never studied): *using 35-44 as example: P(age range death rate | population death rate) = (age range death rate)(population death rate) / P(age range) = (.016)(.0097)/.126 = .0012 = .12% Filling out the rest of the table the death rates by age are: I would be much obliged if you could check that the mathematical operations that I performed support the conclusions I drew in my final table. Especially if you could double check that I used Bayes' Theorem the right way. Thank you!","I'm a tad rusty on my math skills but this isn't too hard. Can you confirm that I am doing this right. I wanted to know what percent of people die from covid-19 so I started looking for statistics. To my surprise the CDC , the WHO , and Google publish statistics: But then I found this article in Science which says that 86% of all infections go undocumented. To adjust for that I do: (total)(1-.86)=confirmed total = confirmed/.14 Which works out to: Then I wanted to see how that works out by age. To my surprise (again) the CDC has an awesome tool for that (Thank you CDC!). Here's a link to the tool . I can't link to my results because you have to login to save your search and I don't want to mess with that right now. Anyways I created the following report: Dimension: Age Group Measure: COVID-19 Deaths Filters:     From 2/1/2020-5/10/2020     Sex (Male and Female) And the results are : Which for the particular date range that I selected amounts to 87,116 deaths. If you transform that into what percent of people are dying by age you get: Next, I used the Census information for 2019 to get what percent each age range is of the total U.S. population (super lucky the age ranges matched up!!!): So right off the bat you can see it has a massively disproportionate effect on older folks ( interesting graph ). The last thing then is to figure out what the odds are of dying from covid-19 for each age range. I had to get some help for this part. I think I have all the numbers plugged in right. It just uses Bayes' Theorem (which sadly I never studied): *using 35-44 as example: P(age range death rate | population death rate) = (age range death rate)(population death rate) / P(age range) = (.016)(.0097)/.126 = .0012 = .12% Filling out the rest of the table the death rates by age are: I would be much obliged if you could check that the mathematical operations that I performed support the conclusions I drew in my final table. Especially if you could double check that I used Bayes' Theorem the right way. Thank you!",,"['probability', 'statistics']"
3,Infinite coin tosses- same number of heads and tails will occur infinitely many times,Infinite coin tosses- same number of heads and tails will occur infinitely many times,,"Dear math stack exchange community, It's my first post, so hello and please let me know if I haven't posted this question in a proper way- happy to edit it. So here's an exercise which I'm trying to solve: We are tossing a fair coin infinitely many times. Let $A_n$ be an event that in first $n$ tosses we had same number of heads and tails. Show that with Probability 1 events $A_n$ will occur for infinitely many values of $n$ . This is an exercise from Borel-Cantelli Lemma's chapter, so I assume I should use it somehow to prove it. What I already know: For each $n \in \mathbb{N}:$ $P(A_{2n+1}) = 0$ $P(A_{2n}) = \binom{2n}{n}p^n(1-p)^n$ (in our case $p = 1/2$ ) But I cannot use those events in order to show divergence of series of those probabilities, because they are not independent and assumption for B-C Lemma will not apply. I believe I should construct independent events somehow, but here's where I am stuck- I have no clue how to do it. Any hints on that? Just in case it's rather expected to do it without prior knowledge about random walks, but any hints/answers are very welcome. Thank you a lot!","Dear math stack exchange community, It's my first post, so hello and please let me know if I haven't posted this question in a proper way- happy to edit it. So here's an exercise which I'm trying to solve: We are tossing a fair coin infinitely many times. Let be an event that in first tosses we had same number of heads and tails. Show that with Probability 1 events will occur for infinitely many values of . This is an exercise from Borel-Cantelli Lemma's chapter, so I assume I should use it somehow to prove it. What I already know: For each (in our case ) But I cannot use those events in order to show divergence of series of those probabilities, because they are not independent and assumption for B-C Lemma will not apply. I believe I should construct independent events somehow, but here's where I am stuck- I have no clue how to do it. Any hints on that? Just in case it's rather expected to do it without prior knowledge about random walks, but any hints/answers are very welcome. Thank you a lot!",A_n n A_n n n \in \mathbb{N}: P(A_{2n+1}) = 0 P(A_{2n}) = \binom{2n}{n}p^n(1-p)^n p = 1/2,"['probability', 'borel-cantelli-lemmas']"
4,"$X_1,..,X_n$ i.i.d random variable and discrete, local limit theorem","i.i.d random variable and discrete, local limit theorem","X_1,..,X_n","Let $X_1,..., X_n$ be i.i.d discrete random variables which take their value in $\mathbb{Z}$ with a non trivial and finite support. Let $S_n = X_1+...+ X_n$ Prove the existence of $ 0 < C_1 < C_2 < \infty$ such that for all $ n \geq 1$ : $$ C_1/\sqrt{n} \leq \sup_{k \in \mathbb{Z}} \mathbb{P}(S_n = k) \leq C_2/\sqrt{n}$$ I know that this can be proved using the central limit theorem yet we didn’t see this theorem in class so this exercise can be solved without using CLT. So far here are my thoughts : The fact that it’s $\sqrt{n}$ comes from the fact that the standard deviation of $S_n$ is $\theta \sqrt{n}$ . Hence one possible strategy is to study the random variable : $\frac{S_n- \mu}{\theta/\sqrt{n}}$ . Yet from now on I dind’t manage finding an upper bound on the probability. For example using Markov inequality I get that : $$\mathbb{P}( \mid \frac{S_n}{n} -\mu \mid \geq \theta/\sqrt{n}) \leq \frac{1}{n}$$ The problem is that it doesn’t help since I can’t say anything when $\mid S_n/n -\mu \mid \leq \theta/\sqrt{n}$ . Thank you !",Let be i.i.d discrete random variables which take their value in with a non trivial and finite support. Let Prove the existence of such that for all : I know that this can be proved using the central limit theorem yet we didn’t see this theorem in class so this exercise can be solved without using CLT. So far here are my thoughts : The fact that it’s comes from the fact that the standard deviation of is . Hence one possible strategy is to study the random variable : . Yet from now on I dind’t manage finding an upper bound on the probability. For example using Markov inequality I get that : The problem is that it doesn’t help since I can’t say anything when . Thank you !,"X_1,..., X_n \mathbb{Z} S_n = X_1+...+ X_n  0 < C_1 < C_2 < \infty  n \geq 1  C_1/\sqrt{n} \leq \sup_{k \in \mathbb{Z}} \mathbb{P}(S_n = k) \leq C_2/\sqrt{n} \sqrt{n} S_n \theta \sqrt{n} \frac{S_n- \mu}{\theta/\sqrt{n}} \mathbb{P}( \mid \frac{S_n}{n} -\mu \mid \geq \theta/\sqrt{n}) \leq \frac{1}{n} \mid S_n/n -\mu \mid \leq \theta/\sqrt{n}","['probability', 'probability-theory', 'probability-limit-theorems']"
5,Jacobian Transformation to find joint pdf of $Y_1$ and $Y_2$,Jacobian Transformation to find joint pdf of  and,Y_1 Y_2,"Let $X_1$ and $X_2$ have the following joint density: $$f_{X_{1},X_{2}}(x_1,x_2) = \begin{cases}  {\frac{4}{\pi}e^{-(x_1^2+x_2^2)}}  & \text{$x_1 \gt 0$, $x_2 \gt 0$}  \\{0} & \text{otherwise}\end{cases}$$ Letting $Y_1=X_1^2$ and $Y_2 =X_1^2 + X_2^2$, give the joint pdf of   $Y_1$ and $Y_2$. I have avoided using Jacobian Transformations in the past because it seemed complicated, but I think using it would be much easier than alternative methods in this case. I wanted to make sure I'm doing this correctly. $$\begin{align*} J(x_1,x_2) &= \begin{vmatrix}\frac{\partial x_1^2}{\partial x_1}&&\frac{\partial x_1^2}{\partial x_2}\\\frac{\partial x_1^2+x_2^2}{\partial x_1}&&\frac{\partial x_1^2+x_2^2}{\partial x_2}\end{vmatrix}\\\\ &= \begin{vmatrix}2x_1&&0\\2x_1&&2x_2\end{vmatrix}\\\\ &= 4x_1x_2 \end{align*}$$ For $x_1$ and $x_2$ we have $x_1=\sqrt{y_1}$ and $x_2=\sqrt{y_2-y_1}$ Thus I have, $$\begin{align*} f_{Y_1,Y_2}(y_1,y_2) &= f_{X_1,X_2}\left(\sqrt{y_1},\sqrt{y_2-y_1}\right) \cdot \left |J(x_1,x_2)\right|^{-1}\\\\ &= \frac{4}{\pi}\cdot e^{-(\sqrt{y_1}^2+\sqrt{y_2-y_1}^2)}\cdot\frac{1}{4x_1x_2}\\\\ &= \frac{1}{\pi\sqrt{y_1}\sqrt{y_2-y_1}}\cdot{e^{-y_2}} \\\\ \end{align*}$$ So the pdf I obtained is $$f_{Y_{1},Y_{2}}(y_1,y_2) = \begin{cases}  \frac{e^{-y_2}}{\pi\sqrt{y_1}\sqrt{y_2-y_1}}  & \text{$y_1 \gt 0$, $y_2 \gt 0$}  \\{0} & \text{otherwise}\end{cases}$$ Is this correct? Edit: If $Y_1 = X_1^2$ and $Y_2=X_1^2+X_2^2$ then $Y_2 \gt Y_1$ so $$f_{Y_{1},Y_{2}}(y_1,y_2) = \begin{cases}  \frac{e^{-y_2}}{\pi\sqrt{y_1}\sqrt{y_2-y_1}}  & \text{$y_1 \gt 0$, $y_2 \gt y_1$}  \\{0} & \text{otherwise}\end{cases}$$ Checking, I get that $$\int_0^\infty \int_x^\infty \frac{e^{-y_2}}{\pi\sqrt{y_1}\sqrt{y_2-y_1}} = 1$$","Let $X_1$ and $X_2$ have the following joint density: $$f_{X_{1},X_{2}}(x_1,x_2) = \begin{cases}  {\frac{4}{\pi}e^{-(x_1^2+x_2^2)}}  & \text{$x_1 \gt 0$, $x_2 \gt 0$}  \\{0} & \text{otherwise}\end{cases}$$ Letting $Y_1=X_1^2$ and $Y_2 =X_1^2 + X_2^2$, give the joint pdf of   $Y_1$ and $Y_2$. I have avoided using Jacobian Transformations in the past because it seemed complicated, but I think using it would be much easier than alternative methods in this case. I wanted to make sure I'm doing this correctly. $$\begin{align*} J(x_1,x_2) &= \begin{vmatrix}\frac{\partial x_1^2}{\partial x_1}&&\frac{\partial x_1^2}{\partial x_2}\\\frac{\partial x_1^2+x_2^2}{\partial x_1}&&\frac{\partial x_1^2+x_2^2}{\partial x_2}\end{vmatrix}\\\\ &= \begin{vmatrix}2x_1&&0\\2x_1&&2x_2\end{vmatrix}\\\\ &= 4x_1x_2 \end{align*}$$ For $x_1$ and $x_2$ we have $x_1=\sqrt{y_1}$ and $x_2=\sqrt{y_2-y_1}$ Thus I have, $$\begin{align*} f_{Y_1,Y_2}(y_1,y_2) &= f_{X_1,X_2}\left(\sqrt{y_1},\sqrt{y_2-y_1}\right) \cdot \left |J(x_1,x_2)\right|^{-1}\\\\ &= \frac{4}{\pi}\cdot e^{-(\sqrt{y_1}^2+\sqrt{y_2-y_1}^2)}\cdot\frac{1}{4x_1x_2}\\\\ &= \frac{1}{\pi\sqrt{y_1}\sqrt{y_2-y_1}}\cdot{e^{-y_2}} \\\\ \end{align*}$$ So the pdf I obtained is $$f_{Y_{1},Y_{2}}(y_1,y_2) = \begin{cases}  \frac{e^{-y_2}}{\pi\sqrt{y_1}\sqrt{y_2-y_1}}  & \text{$y_1 \gt 0$, $y_2 \gt 0$}  \\{0} & \text{otherwise}\end{cases}$$ Is this correct? Edit: If $Y_1 = X_1^2$ and $Y_2=X_1^2+X_2^2$ then $Y_2 \gt Y_1$ so $$f_{Y_{1},Y_{2}}(y_1,y_2) = \begin{cases}  \frac{e^{-y_2}}{\pi\sqrt{y_1}\sqrt{y_2-y_1}}  & \text{$y_1 \gt 0$, $y_2 \gt y_1$}  \\{0} & \text{otherwise}\end{cases}$$ Checking, I get that $$\int_0^\infty \int_x^\infty \frac{e^{-y_2}}{\pi\sqrt{y_1}\sqrt{y_2-y_1}} = 1$$",,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
6,"Proof of $Y=F_X(X)$ being uniformly distributed on $[0,1]$ for arbitrary continuous $F_X$",Proof of  being uniformly distributed on  for arbitrary continuous,"Y=F_X(X) [0,1] F_X","This question is related to Showing that Y has a uniform distribution if Y=F(X) where F is the cdf of continuous X , with the difference being that $F_X$ (the probability distribution function of random variable $X$) is an arbitrary continuous distribution function, not necessarily strictly increasing. I think the proof is similar, but we have to take care of the possibility that $F_X$ may not be $1$-to-$1$.  I list my attempt below, and would appreciate it if someone can confirm if it's correct, and, in particular, if it can be improved.  Thanks a lot! The goal is to show $F_Y(y)=y$ for any $y \in [0,1]$.  To do so, note that $F_Y(y)\triangleq\mathbb P(\{Y\le y\})$, and $\{Y\le y\}=\{F_X(X)\le y\}=\{X\in F_X^{-1}([0, y])\}.$ Since $F_X$ is continuous, $F_X^{-1}([0,y])$ must be closed.  So it follows that $\sup F_X^{-1}([0, y])=\max F_X^{-1}([0, y])=\max F_X^{-1}(\{y\}),$ which let's denote by $a$. Therefore, $F_Y(y)=\mathbb P(\{X\le a\})=F_X(a)=y.\quad$  (Q.E.D.)","This question is related to Showing that Y has a uniform distribution if Y=F(X) where F is the cdf of continuous X , with the difference being that $F_X$ (the probability distribution function of random variable $X$) is an arbitrary continuous distribution function, not necessarily strictly increasing. I think the proof is similar, but we have to take care of the possibility that $F_X$ may not be $1$-to-$1$.  I list my attempt below, and would appreciate it if someone can confirm if it's correct, and, in particular, if it can be improved.  Thanks a lot! The goal is to show $F_Y(y)=y$ for any $y \in [0,1]$.  To do so, note that $F_Y(y)\triangleq\mathbb P(\{Y\le y\})$, and $\{Y\le y\}=\{F_X(X)\le y\}=\{X\in F_X^{-1}([0, y])\}.$ Since $F_X$ is continuous, $F_X^{-1}([0,y])$ must be closed.  So it follows that $\sup F_X^{-1}([0, y])=\max F_X^{-1}([0, y])=\max F_X^{-1}(\{y\}),$ which let's denote by $a$. Therefore, $F_Y(y)=\mathbb P(\{X\le a\})=F_X(a)=y.\quad$  (Q.E.D.)",,"['probability', 'analysis', 'probability-theory', 'proof-verification']"
7,Why are random walks in dimensions 3 or higher transient?,Why are random walks in dimensions 3 or higher transient?,,I watched this PBS video a while ago (relevant part here ) and have been trying to get my head around the idea of transient walks. The video says that a recurrent random walk is one that is guaranteed to return to it's starting position - all 1D and 2D walks - and a walk is transient if there is a positive probability that it never returns - 3D or higher. I've tried to have a think about this and looked some stuff up but I haven't had any breakthroughs. What confuses me is this: A random walk in 3 dimensions can be split up into 3 independent random 1D walks. If each of these walks is guaranteed to return to the starting position infinitely many times we can say that there is a finite positive probability that they will return to the starting point on a given 'turn'. The product of the three finite probabilities is finite so isn't there a finite chance that any random walk in three dimensions will return to the start on any given 'turn' and hence they are guaranteed to return at some point? I imagine I am just making incorrect assumptions about the nature of these infinite systems as is too easy to do but I'd like to know exactly where my intuition is wrong.,I watched this PBS video a while ago (relevant part here ) and have been trying to get my head around the idea of transient walks. The video says that a recurrent random walk is one that is guaranteed to return to it's starting position - all 1D and 2D walks - and a walk is transient if there is a positive probability that it never returns - 3D or higher. I've tried to have a think about this and looked some stuff up but I haven't had any breakthroughs. What confuses me is this: A random walk in 3 dimensions can be split up into 3 independent random 1D walks. If each of these walks is guaranteed to return to the starting position infinitely many times we can say that there is a finite positive probability that they will return to the starting point on a given 'turn'. The product of the three finite probabilities is finite so isn't there a finite chance that any random walk in three dimensions will return to the start on any given 'turn' and hence they are guaranteed to return at some point? I imagine I am just making incorrect assumptions about the nature of these infinite systems as is too easy to do but I'd like to know exactly where my intuition is wrong.,,"['probability', 'random-walk']"
8,How did Mohan Srivastava crack Ontario scratchcards?,How did Mohan Srivastava crack Ontario scratchcards?,,"Wired ran a 2011 article about how a statistician, Mohan Srivastava, cracked Ontario scratchcards such as this one . First, he thought about the program that produced the numbers on the cards. 'Of course, it would be really nice if the computer could just spit out   random digits. But that’s not possible, since the lottery corporation   needs to control the number of winning tickets. The game can’t be truly   random. Instead, it has to generate the illusion of randomness while   actually being carefully determined.' He realised that if a card had a certain feature, it was likely profitable. Srivastava was looking for singletons, numbers that appear only a single   time on the visible tic-tac-toe boards. He realized that the singletons   were almost always repeated under the latex coating. If three singletons   appeared in a row on one of the eight boards, that ticket was probably a   winner. How might a program that produced the numbers work? And how did Srivastava infer that consecutive singletons would be predictive of winning cards?","Wired ran a 2011 article about how a statistician, Mohan Srivastava, cracked Ontario scratchcards such as this one . First, he thought about the program that produced the numbers on the cards. 'Of course, it would be really nice if the computer could just spit out   random digits. But that’s not possible, since the lottery corporation   needs to control the number of winning tickets. The game can’t be truly   random. Instead, it has to generate the illusion of randomness while   actually being carefully determined.' He realised that if a card had a certain feature, it was likely profitable. Srivastava was looking for singletons, numbers that appear only a single   time on the visible tic-tac-toe boards. He realized that the singletons   were almost always repeated under the latex coating. If three singletons   appeared in a row on one of the eight boards, that ticket was probably a   winner. How might a program that produced the numbers work? And how did Srivastava infer that consecutive singletons would be predictive of winning cards?",,"['probability', 'gambling', 'lotteries']"
9,Gambler's Ruin - Probability of Losing in t Steps,Gambler's Ruin - Probability of Losing in t Steps,,"I would be surprised if this hasn't been asked before, but I cannot find it anywhere. Suppose we're given an instance of the gambler's ruin problem where the gambler starts off with $i$ dollars and at every step she wins 1 dollar with probability $p$ and loses a dollar with probability $q = 1 - p$. The gambler stops when she has lost all her money, or when she has $n$ dollars. I am interested in the probability that the gambler loses in $t$ steps. I know how to find the expected number of steps before reaching either absorbing state, and how to solve the probability that she loses before winning $n$ dollars, but this one is eluding me. Let $P_{i, t}$ be the probability that the gambler goes broke in $t$ steps given that she started with $i$ dollars. I have set up the recurrence: $$ P_{i, t} = qP_{i-1, t-1} + pP_{i+1, t-1}$$ and we know that $P_{0, j} = 1$ and $P_{n, j} = 0$for all $j$, and $P_{i, 0} = 0$ for all $i > 0$. I'm struggling to solve this two dimensional recurrence. If it turns out to be too hard to give closed form solutions for this, can we give tighter bounds than just the probability that the gambler ever loses?","I would be surprised if this hasn't been asked before, but I cannot find it anywhere. Suppose we're given an instance of the gambler's ruin problem where the gambler starts off with $i$ dollars and at every step she wins 1 dollar with probability $p$ and loses a dollar with probability $q = 1 - p$. The gambler stops when she has lost all her money, or when she has $n$ dollars. I am interested in the probability that the gambler loses in $t$ steps. I know how to find the expected number of steps before reaching either absorbing state, and how to solve the probability that she loses before winning $n$ dollars, but this one is eluding me. Let $P_{i, t}$ be the probability that the gambler goes broke in $t$ steps given that she started with $i$ dollars. I have set up the recurrence: $$ P_{i, t} = qP_{i-1, t-1} + pP_{i+1, t-1}$$ and we know that $P_{0, j} = 1$ and $P_{n, j} = 0$for all $j$, and $P_{i, 0} = 0$ for all $i > 0$. I'm struggling to solve this two dimensional recurrence. If it turns out to be too hard to give closed form solutions for this, can we give tighter bounds than just the probability that the gambler ever loses?",,"['probability', 'markov-chains', 'martingales', 'random-walk', 'gambling']"
10,Flies in a cube,Flies in a cube,,"Two flies sit in the corners $(1, 1, 1)$ and $(n, n, n)$ of a discrete cube $M^3$, where $M=\{1,\ldots, n\}$. After every unit of time both flies decide randomly and independently of each other whether or not to fly to some neighbouring point; here two points are neighbours if they differ in precisely one coordinate and that difference is precisely $1$. What's more, all events are equally likely: if a fly currently sits in a point with $k$ neighbours, then with prob. $\frac{1}{k+1}$ it does not move and with the same prob. it also moves to any of the neighbours. Now both flies live precisely $2n$ units of time. What is the probability that they will ever meet? This problem is made up and so far I don't know the answer, maybe there is an elegant solution?","Two flies sit in the corners $(1, 1, 1)$ and $(n, n, n)$ of a discrete cube $M^3$, where $M=\{1,\ldots, n\}$. After every unit of time both flies decide randomly and independently of each other whether or not to fly to some neighbouring point; here two points are neighbours if they differ in precisely one coordinate and that difference is precisely $1$. What's more, all events are equally likely: if a fly currently sits in a point with $k$ neighbours, then with prob. $\frac{1}{k+1}$ it does not move and with the same prob. it also moves to any of the neighbours. Now both flies live precisely $2n$ units of time. What is the probability that they will ever meet? This problem is made up and so far I don't know the answer, maybe there is an elegant solution?",,['probability']
11,Exponential is to Poisson as Normal is to ???,Exponential is to Poisson as Normal is to ???,,"In a time series, if the gap between successive events follows an exponential distribution with PDF $\lambda e^{-\lambda}$, then a Poisson distribution with parameter $\lambda$ tells you the probability of finding 0, 1, 2, etc events in time frames of width 1. Now suppose the gap between successive events follows a normal distribution with parameters $\mu$ and $\sigma$. Is there a corresponding discrete distribution telling us the probability of finding 0, 1, 2, etc events in time frames of width $\mu$?","In a time series, if the gap between successive events follows an exponential distribution with PDF $\lambda e^{-\lambda}$, then a Poisson distribution with parameter $\lambda$ tells you the probability of finding 0, 1, 2, etc events in time frames of width 1. Now suppose the gap between successive events follows a normal distribution with parameters $\mu$ and $\sigma$. Is there a corresponding discrete distribution telling us the probability of finding 0, 1, 2, etc events in time frames of width $\mu$?",,"['probability', 'statistics']"
12,Probability that at least one of the bullets will go on forever.,Probability that at least one of the bullets will go on forever.,,"There's a gun located on an infinite line, let's say at 0 on number line. It starts shooting bullets along that line, +X axis , at the rate of one bullet per second. Each bullet has a velocity in the range [0, 1] m/s randomly chosen from a uniform distribution. If two bullets collide (are at the same spot at the same time) they explode and disappear. What is the probability that at least one of the infinite bullets will infinitely fly without colliding with another bullet?","There's a gun located on an infinite line, let's say at 0 on number line. It starts shooting bullets along that line, +X axis , at the rate of one bullet per second. Each bullet has a velocity in the range [0, 1] m/s randomly chosen from a uniform distribution. If two bullets collide (are at the same spot at the same time) they explode and disappear. What is the probability that at least one of the infinite bullets will infinitely fly without colliding with another bullet?",,['probability']
13,A matrix with a dense submatrix - application of Chernoff’s Inequality,A matrix with a dense submatrix - application of Chernoff’s Inequality,,"I am trying to solve an exercise from this book, which I will post here for convenience. I have a bit of a problem understanding how the hint of using Chernoff's bound implies the claim. Specifically let $X = \sum_{i=1}^n X_i$ where $X_i$ are indicator random variables defined in the hint and $E[X] \leq p t$. We are asked to show that $$Pr[ X \geq pt + t\delta] \leq 1/m^2,$$ where $t = \lceil 4p\log{m/\delta^2} \rceil.$ Using the suggested Chernoff's bound we obtain that the above probability is bounded by $$e^{-t^2 \delta^2/2t} = e^{-4p \log{(m/\delta^2)} \delta^2}.$$ And it does not seem to follow that the last expression is bounded by $1/m^2.$ So either one needs to use a different approach or I am missing a crucial detail. I am tempted to think that perhaps the author meant to take a different value of $t$ but the next exericse builds uppon this specific order of $t$. Hence I am wondering Where is the mistake in my reasoning? How to solve this problem correctly?","I am trying to solve an exercise from this book, which I will post here for convenience. I have a bit of a problem understanding how the hint of using Chernoff's bound implies the claim. Specifically let $X = \sum_{i=1}^n X_i$ where $X_i$ are indicator random variables defined in the hint and $E[X] \leq p t$. We are asked to show that $$Pr[ X \geq pt + t\delta] \leq 1/m^2,$$ where $t = \lceil 4p\log{m/\delta^2} \rceil.$ Using the suggested Chernoff's bound we obtain that the above probability is bounded by $$e^{-t^2 \delta^2/2t} = e^{-4p \log{(m/\delta^2)} \delta^2}.$$ And it does not seem to follow that the last expression is bounded by $1/m^2.$ So either one needs to use a different approach or I am missing a crucial detail. I am tempted to think that perhaps the author meant to take a different value of $t$ but the next exericse builds uppon this specific order of $t$. Hence I am wondering Where is the mistake in my reasoning? How to solve this problem correctly?",,"['probability', 'combinatorics', 'probability-theory', 'inequality']"
14,Probability of two zero inner products,Probability of two zero inner products,,"Consider a random $n+1$-dimensional vector $v$. Each $v_1,\dots,v_n$ equals $1$ with probability $1/2$ and $-1$ with probability $1/2$ independently.  We also set $v_{n+1} = v_1$. Now consider a random $n$-dimensional vector $w$. Each $w_i$ equals $1$ with probability $1/4$ and $-1$ with probability $1/4$ and equals $0$ with probability $1/2$, all sampled independently. We know $$P\left(\sum_{i=1}^n v_i w_i = 0\right) = \sum_{k=0}^{\lfloor \frac n2 \rfloor}\frac{n!}{k!(n-2k)!k!}\left(\frac14\right)^k\left(\frac12\right)^{n-2k}\left(\frac14\right)^k \sim \frac{1}{\sqrt{\pi n}}.$$ I wrote computer code to compute  probabilities exactly for smallish $n$. It seems numerically that  $$P\left(\sum_{i=1}^n v_{i+1} w_i = 0 \; \land \sum_{i=1}^n v_{i} w_i = 0\right) = \frac{\sum _{i=0}^{\lfloor n/2 \rfloor} {2(n-2i) \choose n-2i} {n \choose 2i} {4i \choose 2i}}{   2^{3n - 1}}.$$ Is this equality true and how could one prove it?","Consider a random $n+1$-dimensional vector $v$. Each $v_1,\dots,v_n$ equals $1$ with probability $1/2$ and $-1$ with probability $1/2$ independently.  We also set $v_{n+1} = v_1$. Now consider a random $n$-dimensional vector $w$. Each $w_i$ equals $1$ with probability $1/4$ and $-1$ with probability $1/4$ and equals $0$ with probability $1/2$, all sampled independently. We know $$P\left(\sum_{i=1}^n v_i w_i = 0\right) = \sum_{k=0}^{\lfloor \frac n2 \rfloor}\frac{n!}{k!(n-2k)!k!}\left(\frac14\right)^k\left(\frac12\right)^{n-2k}\left(\frac14\right)^k \sim \frac{1}{\sqrt{\pi n}}.$$ I wrote computer code to compute  probabilities exactly for smallish $n$. It seems numerically that  $$P\left(\sum_{i=1}^n v_{i+1} w_i = 0 \; \land \sum_{i=1}^n v_{i} w_i = 0\right) = \frac{\sum _{i=0}^{\lfloor n/2 \rfloor} {2(n-2i) \choose n-2i} {n \choose 2i} {4i \choose 2i}}{   2^{3n - 1}}.$$ Is this equality true and how could one prove it?",,['probability']
15,Spivak alike books for Probability and/or Statistics,Spivak alike books for Probability and/or Statistics,,"I am looking for a Probability/Statistics book with an style alike to that of Spivak's Calculus, that is, a book with the following characteristics: Directed more towards Math majors rather than applied majors(engineers, economists, physicists). Every theorem, or almost every theorem is proven rigorously. Even though the book is very rigourous mathmatically, it gives a very deep intuitive understanding of the subject. I don't want merely to have a collection of formulas that work, but I want to know why they work, and what they mean. Also, I when I say rigourous I don't mean technical or formal, the book doesn't need to use lots of technical words, it can be informal, as long as the proofs are proved rigorously. Rigorous proofs and deep understanding... Could you please reference me to a book like that? Thanks in advance","I am looking for a Probability/Statistics book with an style alike to that of Spivak's Calculus, that is, a book with the following characteristics: Directed more towards Math majors rather than applied majors(engineers, economists, physicists). Every theorem, or almost every theorem is proven rigorously. Even though the book is very rigourous mathmatically, it gives a very deep intuitive understanding of the subject. I don't want merely to have a collection of formulas that work, but I want to know why they work, and what they mean. Also, I when I say rigourous I don't mean technical or formal, the book doesn't need to use lots of technical words, it can be informal, as long as the proofs are proved rigorously. Rigorous proofs and deep understanding... Could you please reference me to a book like that? Thanks in advance",,['probability']
16,Prove the density of this SDE is not smooth in a parameter,Prove the density of this SDE is not smooth in a parameter,,"Consider the following, 1-dimensional, equation $$X_t^x = x + \int_0^t \mathbb{E} |X_s^x| \, ds + B_t , $$ where $B$ is a Brownian motion. This a McKean-Vlasov equation, sometimes called a nonlinear diffusion or mean-field equation. I can prove that it has a unique strong solution. What I would like to show is that its density is not smooth, i.e. $C^{\infty}$, in $x$. Or alternatively, I would like to show that for fixed $t$, the map $$ x \mapsto \mathbb{E} |X_t^x| $$ is not smooth. Some McKean-Vlasov SDEs can be solved explicitly, but in this case the presence of the modulus in the coefficients makes it hard to find an explicit solution. I therefore do not know the density or an expression for $\mathbb{E} |X_t^x|$ explicitly. Without knowing the density or an expression for $\mathbb{E} |X_t^x|$ explicitly, how can I show that they are not smooth?","Consider the following, 1-dimensional, equation $$X_t^x = x + \int_0^t \mathbb{E} |X_s^x| \, ds + B_t , $$ where $B$ is a Brownian motion. This a McKean-Vlasov equation, sometimes called a nonlinear diffusion or mean-field equation. I can prove that it has a unique strong solution. What I would like to show is that its density is not smooth, i.e. $C^{\infty}$, in $x$. Or alternatively, I would like to show that for fixed $t$, the map $$ x \mapsto \mathbb{E} |X_t^x| $$ is not smooth. Some McKean-Vlasov SDEs can be solved explicitly, but in this case the presence of the modulus in the coefficients makes it hard to find an explicit solution. I therefore do not know the density or an expression for $\mathbb{E} |X_t^x|$ explicitly. Without knowing the density or an expression for $\mathbb{E} |X_t^x|$ explicitly, how can I show that they are not smooth?",,"['probability', 'analysis', 'stochastic-processes', 'stochastic-calculus']"
17,Probability that a sequence repeats itself,Probability that a sequence repeats itself,,"Given an infinite sequence $a_n$ of uniformly random integers $0$ to $9$, what is the probability there exist an integer $m$ such that the sequence $a_1$ to $a_m$ is equal to that from $a_{m+1}$ to $a_{2m}$? What if we restrict to two symbols, or $k$ symbols?","Given an infinite sequence $a_n$ of uniformly random integers $0$ to $9$, what is the probability there exist an integer $m$ such that the sequence $a_1$ to $a_m$ is equal to that from $a_{m+1}$ to $a_{2m}$? What if we restrict to two symbols, or $k$ symbols?",,"['probability', 'combinatorics', 'sequences-and-series', 'random']"
18,Sufficient statistics vs. Bayesian sufficient statistics,Sufficient statistics vs. Bayesian sufficient statistics,,"Given sample data $x_1, \ldots, x_n$ generated from a probability distribution $f(x|\theta)$ ($\theta$ being an unknown parameter), a statistic $T(x_1, \ldots, x_n)$ of the sample data is called sufficient if $f(x|\theta, t) = f(x|t)$. However, I'm always kinda confused by this definition, since I think of a sufficient statistic as a function that gives just as much information about $\theta$ as the original data itself (which seems a little different from the definition above). The definition of Bayesian sufficiency, on the other hand, does mesh with my intuition: $T$ is a Bayesian sufficient statistic if $f(\theta|t) = f(\theta|x)$. So why is the first definition of sufficiency important? What does it capture that Bayesian sufficiency doesn't, and how should I think about it? [Note: I believe that every sufficient statistic is also Bayesian sufficient, but not conversely (the reverse implication doesn't hold in the infinite-dimensional case, according to Wikipedia).]","Given sample data $x_1, \ldots, x_n$ generated from a probability distribution $f(x|\theta)$ ($\theta$ being an unknown parameter), a statistic $T(x_1, \ldots, x_n)$ of the sample data is called sufficient if $f(x|\theta, t) = f(x|t)$. However, I'm always kinda confused by this definition, since I think of a sufficient statistic as a function that gives just as much information about $\theta$ as the original data itself (which seems a little different from the definition above). The definition of Bayesian sufficiency, on the other hand, does mesh with my intuition: $T$ is a Bayesian sufficient statistic if $f(\theta|t) = f(\theta|x)$. So why is the first definition of sufficiency important? What does it capture that Bayesian sufficiency doesn't, and how should I think about it? [Note: I believe that every sufficient statistic is also Bayesian sufficient, but not conversely (the reverse implication doesn't hold in the infinite-dimensional case, according to Wikipedia).]",,"['probability', 'statistics']"
19,"Probability, potential theory and complex analysis","Probability, potential theory and complex analysis",,"The connection between Markov processes and Potential Theory is well known, as is conformal invariance of Brownian motion which allows probabilistic proofs of statements in Complex Analysis, like Picard's theorem. What are some results in these areas (Potential Theory, Complex Analysis, maybe Complex or Differential Geometry) that admit probabilistic proofs, and possibly for which no other proof is known? To what extent are probabilistic methods in these areas actually crucial in proving new theory and to what extent are they more of an interpretation of facts that are already known? It would be great if there were some survey about this. Thanks in advance for your answers.","The connection between Markov processes and Potential Theory is well known, as is conformal invariance of Brownian motion which allows probabilistic proofs of statements in Complex Analysis, like Picard's theorem. What are some results in these areas (Potential Theory, Complex Analysis, maybe Complex or Differential Geometry) that admit probabilistic proofs, and possibly for which no other proof is known? To what extent are probabilistic methods in these areas actually crucial in proving new theory and to what extent are they more of an interpretation of facts that are already known? It would be great if there were some survey about this. Thanks in advance for your answers.",,"['probability', 'complex-analysis', 'probability-theory', 'differential-geometry', 'potential-theory']"
20,Distributions with 'Gaussian Tails',Distributions with 'Gaussian Tails',,"In a paper I was reading, the following seemingly artificial assumption is used: suppose $f$ is some probability density function on $\mathbb{R}^d$ , and let $\phi$ denote the density of a $N(0,I_d)$ random variable. Then they assume either: for any $x \in \mathbb{R}^d$ , there exists a finite positive constant (possibly depending on $f$ such that $f(x) \le C_f \phi(x)$ . for any $x \in \mathbb{R}^d$ , there exists finite positive constants $c_1,c_2$ (not depending on $f$ ) such that $c_1 \phi(x) \le f(x) \le c_2 \phi(x)$ . I am trying to understand what kind of densities belong to either class. For example, the standard sub-gaussian assumption does not belong to the class of densities in 1. since any unbounded density does not satisfy 1, even though there are sub-gaussian distributions that are unbounded. Questions: what kind of distributions belong to either class? Do gaussian mixtures belong to either class? What does the dependence of the constants on $f$ change ? or how does the class of densities that satisfy either of the conditions change if the constants were/were not allowed to depend on $f$ ?","In a paper I was reading, the following seemingly artificial assumption is used: suppose is some probability density function on , and let denote the density of a random variable. Then they assume either: for any , there exists a finite positive constant (possibly depending on such that . for any , there exists finite positive constants (not depending on ) such that . I am trying to understand what kind of densities belong to either class. For example, the standard sub-gaussian assumption does not belong to the class of densities in 1. since any unbounded density does not satisfy 1, even though there are sub-gaussian distributions that are unbounded. Questions: what kind of distributions belong to either class? Do gaussian mixtures belong to either class? What does the dependence of the constants on change ? or how does the class of densities that satisfy either of the conditions change if the constants were/were not allowed to depend on ?","f \mathbb{R}^d \phi N(0,I_d) x \in \mathbb{R}^d f f(x) \le C_f \phi(x) x \in \mathbb{R}^d c_1,c_2 f c_1 \phi(x) \le f(x) \le c_2 \phi(x) f f","['probability', 'probability-theory', 'statistics', 'probability-distributions']"
21,"What is the probability that $(1+u_1)(1+u_1 u_2)(1+u_1 u_2 u_3)...>e$, where each $u$ is a uniformly random real number in $(0,1)$?","What is the probability that , where each  is a uniformly random real number in ?","(1+u_1)(1+u_1 u_2)(1+u_1 u_2 u_3)...>e u (0,1)","What is the probability that $\prod\limits_{k=1}^\infty \left(1+\prod\limits_{i=1}^k u_i\right)>e$ , where the $u$ 's are i.i.d. $\text{Uniform}(0,1)$ -variables ? The product, $\prod\limits_{k=1}^\infty \left(1+\prod\limits_{i=1}^k u_i\right)$ , has an expectation of $e$ . I am wondering, what is the probability that it is greater than its expectation. Excel simulations suggest that the answer is (simply) $\frac13$ . EDIT: After reading @joriki's comment, I ran more Excel simulations, and now it seems the answer is more like $0.328$ .","What is the probability that , where the 's are i.i.d. -variables ? The product, , has an expectation of . I am wondering, what is the probability that it is greater than its expectation. Excel simulations suggest that the answer is (simply) . EDIT: After reading @joriki's comment, I ran more Excel simulations, and now it seems the answer is more like .","\prod\limits_{k=1}^\infty \left(1+\prod\limits_{i=1}^k u_i\right)>e u \text{Uniform}(0,1) \prod\limits_{k=1}^\infty \left(1+\prod\limits_{i=1}^k u_i\right) e \frac13 0.328","['probability', 'random-variables', 'infinite-product']"
22,"How can you measure how ""shuffled"" a deck of cards is?","How can you measure how ""shuffled"" a deck of cards is?",,"A few days ago I asked for some methods of measuring how shuffled a deck of cards was. Predictably there were a lot of suggested methods, which got me thinking, which is the best one? I think it'd be a cool experiment to test a bunch of methods, and then compare them. My idea is to start with a sorted deck of cards, and then progressively shuffle it, and after each shuffle calculate how ""shuffled"" the deck is using a number of methods. I plan to test many different types of shuffles as well, but that's getting into unnecessary detail. So my question is, what ideas do you have for measuring how shuffled the deck is? I've got a lot of ideas from my previous post, and I'll be throwing in some more basic methods. Here's what I have so far: Calculating the entropy of the deck as described in this wonderful post. Counting the number of pairs in the deck. Counting the number of inversions in the deck after a shuffle. Counting the number of rising or falling sequences in the deck. Tracking the position of each card in the deck and analyzing the change in position after shuffling. Then calculate the entropy of this information. This may have near identical results to the first method, but that's part of the experiment! Recording the number of high and low cards in a series of fixed ranges, and determining the randomness of the shuffle using this data (I don't think I need to go into full detail here but if requested I can give it). Those are the main methods I have at the moment, but I'm looking for as many as possible. I will be using a program to run all of these, so I can check a nearly unlimited number of methods, of any complexity. So go for it! Give me whatever methods you'd like to see. They can be as easy and flawed as you like (like the 2nd example I gave), or as complicated as you want. The whole idea is to test as many as I can. Also I'm very interested in using entropy, but it's pretty new to me, so any links to methods involving entropy would be fantastic. Once I run the experiment and get the results (in maybe a week or so) I'll update this post with the results I found (lots of pretty graphs to be expected!). That's the main gist of it, but for some of the more astute of you I'll give a quick explanation for a key issue I found when researching this. A big idea I kept running across is that you don't want to measure the randomness of the shuffled deck, but rather the randomness of the shuffling method itself, checking for a uniform distribution of final deck permutations. However, my objective is to look at a shuffled deck of cards that was originally sorted, and using some method, calculate a quantitative value that represents how shuffled the deck of cards is. The shuffling method itself will be a mystery to the program calculating the randomness (in reality I'll be using a lot of methods from a riffle shuffle to cutting the deck to the Fisher-Yates method). I'm going to look into a lot of statistics on each method like finding the standard deviation or consistency of each one. In the end I'll have a lot of conclusions, but the one I most want to answer is: if handed a shuffled deck that was once sorted, what would be the best method for calculating how well shuffled the deck is? I probably have misunderstood something here so feel free to comment any advice or changes you'd make. EDIT: I completely forgot to post the results of this test from almost a year ago. The final paper had some very interesting results, and I'd like to show them here. Here is a PDF of the full paper: https://drive.google.com/file/d/1EoJhtHAO5iFjikkH35KDVrmmJQpXVb5q/view?usp=sharing","A few days ago I asked for some methods of measuring how shuffled a deck of cards was. Predictably there were a lot of suggested methods, which got me thinking, which is the best one? I think it'd be a cool experiment to test a bunch of methods, and then compare them. My idea is to start with a sorted deck of cards, and then progressively shuffle it, and after each shuffle calculate how ""shuffled"" the deck is using a number of methods. I plan to test many different types of shuffles as well, but that's getting into unnecessary detail. So my question is, what ideas do you have for measuring how shuffled the deck is? I've got a lot of ideas from my previous post, and I'll be throwing in some more basic methods. Here's what I have so far: Calculating the entropy of the deck as described in this wonderful post. Counting the number of pairs in the deck. Counting the number of inversions in the deck after a shuffle. Counting the number of rising or falling sequences in the deck. Tracking the position of each card in the deck and analyzing the change in position after shuffling. Then calculate the entropy of this information. This may have near identical results to the first method, but that's part of the experiment! Recording the number of high and low cards in a series of fixed ranges, and determining the randomness of the shuffle using this data (I don't think I need to go into full detail here but if requested I can give it). Those are the main methods I have at the moment, but I'm looking for as many as possible. I will be using a program to run all of these, so I can check a nearly unlimited number of methods, of any complexity. So go for it! Give me whatever methods you'd like to see. They can be as easy and flawed as you like (like the 2nd example I gave), or as complicated as you want. The whole idea is to test as many as I can. Also I'm very interested in using entropy, but it's pretty new to me, so any links to methods involving entropy would be fantastic. Once I run the experiment and get the results (in maybe a week or so) I'll update this post with the results I found (lots of pretty graphs to be expected!). That's the main gist of it, but for some of the more astute of you I'll give a quick explanation for a key issue I found when researching this. A big idea I kept running across is that you don't want to measure the randomness of the shuffled deck, but rather the randomness of the shuffling method itself, checking for a uniform distribution of final deck permutations. However, my objective is to look at a shuffled deck of cards that was originally sorted, and using some method, calculate a quantitative value that represents how shuffled the deck of cards is. The shuffling method itself will be a mystery to the program calculating the randomness (in reality I'll be using a lot of methods from a riffle shuffle to cutting the deck to the Fisher-Yates method). I'm going to look into a lot of statistics on each method like finding the standard deviation or consistency of each one. In the end I'll have a lot of conclusions, but the one I most want to answer is: if handed a shuffled deck that was once sorted, what would be the best method for calculating how well shuffled the deck is? I probably have misunderstood something here so feel free to comment any advice or changes you'd make. EDIT: I completely forgot to post the results of this test from almost a year ago. The final paper had some very interesting results, and I'd like to show them here. Here is a PDF of the full paper: https://drive.google.com/file/d/1EoJhtHAO5iFjikkH35KDVrmmJQpXVb5q/view?usp=sharing",,"['probability', 'statistics', 'random', 'entropy', 'card-games']"
23,$P(X_{(n-k_n)}>X_1\mid X_1>u_n)=0$?,?,P(X_{(n-k_n)}>X_1\mid X_1>u_n)=0,"Let $X_1,X_2,\dots$ be continuous random variables with full support (I need the result when they follow AR time series $X_i=\alpha X_{i-1}+\varepsilon_i$ for iid epsilons. But if you will consider iid case, it may also help with the time series case) with their stationary distribution function $F$ . Notation $X_{n; (k)}$ represents the $k-$ th order statistic, i.e. $X_{n; (1)}=\min_{i\leq n} X_i$ . Let $k_n\in\mathbb{N}$ fulfill $$k_n\to\infty, \frac{k_n}{n}\to 0 \text{, for } n\to\infty.$$ Let $$u_n=quantile(X_1, 1-\frac{k_n}{n}), \text{ i.e. } P(X_1>u_n)=\frac{k_n}{n}.$$ Is the following true $$\frac{n}{k_n}P(X_{n; (n-k_n)}>X_1>u_n)\overset{n\to\infty}{\to}0?$$ Maybe a helpful observation is that this is equal to $$ \frac{n}{k_n}P(X_{n; (n-k_n)}>X_1>u_n)=P(X_{n; (n-k_n)}>X_1\mid X_1>u_n)=1-P(\hat{F}(X_1)>1-\frac{k_n}{n}\mid F(X_1)>1-\frac{k_n}{n}). $$","Let be continuous random variables with full support (I need the result when they follow AR time series for iid epsilons. But if you will consider iid case, it may also help with the time series case) with their stationary distribution function . Notation represents the th order statistic, i.e. . Let fulfill Let Is the following true Maybe a helpful observation is that this is equal to","X_1,X_2,\dots X_i=\alpha X_{i-1}+\varepsilon_i F X_{n; (k)} k- X_{n; (1)}=\min_{i\leq n} X_i k_n\in\mathbb{N} k_n\to\infty, \frac{k_n}{n}\to 0 \text{, for } n\to\infty. u_n=quantile(X_1, 1-\frac{k_n}{n}), \text{ i.e. } P(X_1>u_n)=\frac{k_n}{n}. \frac{n}{k_n}P(X_{n; (n-k_n)}>X_1>u_n)\overset{n\to\infty}{\to}0? 
\frac{n}{k_n}P(X_{n; (n-k_n)}>X_1>u_n)=P(X_{n; (n-k_n)}>X_1\mid X_1>u_n)=1-P(\hat{F}(X_1)>1-\frac{k_n}{n}\mid F(X_1)>1-\frac{k_n}{n}).
","['probability', 'conditional-probability', 'order-statistics', 'large-deviation-theory', 'empirical-processes']"
24,Expected value of the maximum minimum set,Expected value of the maximum minimum set,,"Pick $m$ numbers in $[0,1]$ , independently and uniformly at random. It is known that the expected value of the smallest number is $1/(m+1)$ and of the largest number is $m/(m+1)$ . Now, partition the numbers into two sets, such that sum of numbers in the set with the smaller sum is as large as possible (in other words: solve the partition problem on the numbers). What is the expected value of the smaller sum?","Pick numbers in , independently and uniformly at random. It is known that the expected value of the smallest number is and of the largest number is . Now, partition the numbers into two sets, such that sum of numbers in the set with the smaller sum is as large as possible (in other words: solve the partition problem on the numbers). What is the expected value of the smaller sum?","m [0,1] 1/(m+1) m/(m+1)","['probability', 'expected-value', 'set-partition']"
25,Proof of one-side version of Bennett-Bernstein inequality,Proof of one-side version of Bennett-Bernstein inequality,,"I'm going to prove the following: For independent random variables $X_i$ , $i \in [m]$ satisfying $X_i-E[X_i] \le b$ for some constant $b > 0$ . Let $\bar{X} = \dfrac{1}{m}\sum_{i=1}^m X_i$ , we have \begin{equation} 		P(\bar{X}\ge E[\bar{X}]+\varepsilon)  		\le \exp\left(\dfrac{-m\varepsilon^2}{2(\frac{1}{m}\sum_{i=1}^m Var[X_i]+\frac{1}{3}b\varepsilon)}\right) 	\end{equation} and here is my trial: Lemma: Let $f(u) = 2\cdot\frac{e^u-u-1}{u^2}$ and $f(0) := 1$ . $f'(u) \ge 0$ and for $u \in (0, 3)$ , $f(u) \le \left(1-\frac{u}{3}\right)^{-1}$ . For random variable $X$ with $EX=0$ , $P(X \le b)=1$ and $\lambda \in \left(0, \frac{3}{b}\right)$ , $E[X^2] = Var[X]$ , $f(\lambda X) \le f(\lambda b)$ , \begin{equation} 	E[\exp(\lambda X)] 	=1+\lambda E[X]+\frac{1}{2}\lambda^2E[X^2f(\lambda X)] 	\le 1+\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} Var[X]  	\le \exp\left(\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} Var[X]\right) \end{equation} Proof: For independent random variables $X_i$ , $i \in [m]$ satisfying $X_i-E{X_i} \le b$ and $\bar{X} = \frac{1}{m}\sum_{i=1}^m X_i$ , we have \begin{align} 		P\left(\bar{X}\ge E[\bar{X}]+\varepsilon\right) 		&= P\left(\sum_{i=1}^m(X_i-E{X_i})\ge m\varepsilon\right) \\ 		&\le E[\exp(\lambda\sum_{i=1}^m(X_i-E{X_i}))] \cdot \exp(-\lambda m\varepsilon) \\ 		&= \prod_{i=1}^m E[\exp(\lambda(X_i-E{X_i}))] \cdot \exp(-\lambda m\varepsilon) \\ 		&\le \prod_{i=1}^m \exp\left(\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} Var{[X_i]}\right) \cdot \exp(-\lambda m\varepsilon) \\ 		&= \exp\left(\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} \sum_{i=1}^m Var[X_i] -\lambda m\varepsilon\right) 	\end{align} Let $\lambda = \dfrac{\varepsilon}{\frac{1}{m}\sum_{i=1}^m Var[X_i]+\frac{1}{3}b\varepsilon}$ (Note that this time $\lambda^2(1-\frac{\lambda b}{3})^{-1} \sum_{i=1}^m Var[X_i] = \lambda m\varepsilon$ and the right hand side takes $\exp\left(\dfrac{-m\varepsilon^2}{2(\frac{1}{m}\sum_{i=1}^m Var[X_i]+\frac{1}{3}b\varepsilon)}\right)$ . $\Box$ This seems fine. But when I try to prove it more directly, I meet some difficulty. $X_i-E{X_i} \le b$ and $\bar{X} = \frac{1}{m}\sum_{i=1}^m X_i$ , so we have $E[\bar{X}] \le b$ and $Var[\bar{X}] = \frac{1}{m^2}\sum_{i=1}^m Var[X_i]$ and \begin{align} P\left(\bar{X}\ge E[\bar{X}]+\varepsilon\right) &\le \exp\left(\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} Var[\bar{X}]\right) \cdot \exp(-\lambda \varepsilon) \\ &= \exp\left(\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} \sum_{i=1}^m \frac{1}{m^2} Var[\bar{X}_i]-\lambda \varepsilon\right) \end{align} If we want to get the same result, we may consider multiply $\lambda$ by a factor $m$ , but this is invalid since the $(1-\frac{\lambda b}{3})^{-1}$ term is unchanged and $\lambda$ is bounded by $\lambda < \frac{3}{b}$ . So where's the problem and how should I proceed?","I'm going to prove the following: For independent random variables , satisfying for some constant . Let , we have and here is my trial: Lemma: Let and . and for , . For random variable with , and , , , Proof: For independent random variables , satisfying and , we have Let (Note that this time and the right hand side takes . This seems fine. But when I try to prove it more directly, I meet some difficulty. and , so we have and and If we want to get the same result, we may consider multiply by a factor , but this is invalid since the term is unchanged and is bounded by . So where's the problem and how should I proceed?","X_i i \in [m] X_i-E[X_i] \le b b > 0 \bar{X} = \dfrac{1}{m}\sum_{i=1}^m X_i \begin{equation}
		P(\bar{X}\ge E[\bar{X}]+\varepsilon) 
		\le \exp\left(\dfrac{-m\varepsilon^2}{2(\frac{1}{m}\sum_{i=1}^m Var[X_i]+\frac{1}{3}b\varepsilon)}\right)
	\end{equation} f(u) = 2\cdot\frac{e^u-u-1}{u^2} f(0) := 1 f'(u) \ge 0 u \in (0, 3) f(u) \le \left(1-\frac{u}{3}\right)^{-1} X EX=0 P(X \le b)=1 \lambda \in \left(0, \frac{3}{b}\right) E[X^2] = Var[X] f(\lambda X) \le f(\lambda b) \begin{equation}
	E[\exp(\lambda X)]
	=1+\lambda E[X]+\frac{1}{2}\lambda^2E[X^2f(\lambda X)]
	\le 1+\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} Var[X] 
	\le \exp\left(\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} Var[X]\right)
\end{equation} X_i i \in [m] X_i-E{X_i} \le b \bar{X} = \frac{1}{m}\sum_{i=1}^m X_i \begin{align}
		P\left(\bar{X}\ge E[\bar{X}]+\varepsilon\right)
		&= P\left(\sum_{i=1}^m(X_i-E{X_i})\ge m\varepsilon\right) \\
		&\le E[\exp(\lambda\sum_{i=1}^m(X_i-E{X_i}))] \cdot \exp(-\lambda m\varepsilon) \\
		&= \prod_{i=1}^m E[\exp(\lambda(X_i-E{X_i}))] \cdot \exp(-\lambda m\varepsilon) \\
		&\le \prod_{i=1}^m \exp\left(\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} Var{[X_i]}\right) \cdot \exp(-\lambda m\varepsilon) \\
		&= \exp\left(\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} \sum_{i=1}^m Var[X_i] -\lambda m\varepsilon\right)
	\end{align} \lambda = \dfrac{\varepsilon}{\frac{1}{m}\sum_{i=1}^m Var[X_i]+\frac{1}{3}b\varepsilon} \lambda^2(1-\frac{\lambda b}{3})^{-1} \sum_{i=1}^m Var[X_i] = \lambda m\varepsilon \exp\left(\dfrac{-m\varepsilon^2}{2(\frac{1}{m}\sum_{i=1}^m Var[X_i]+\frac{1}{3}b\varepsilon)}\right) \Box X_i-E{X_i} \le b \bar{X} = \frac{1}{m}\sum_{i=1}^m X_i E[\bar{X}] \le b Var[\bar{X}] = \frac{1}{m^2}\sum_{i=1}^m Var[X_i] \begin{align}
P\left(\bar{X}\ge E[\bar{X}]+\varepsilon\right) &\le \exp\left(\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} Var[\bar{X}]\right) \cdot \exp(-\lambda \varepsilon) \\
&= \exp\left(\frac{1}{2}\lambda^2(1-\frac{\lambda b}{3})^{-1} \sum_{i=1}^m \frac{1}{m^2} Var[\bar{X}_i]-\lambda \varepsilon\right)
\end{align} \lambda m (1-\frac{\lambda b}{3})^{-1} \lambda \lambda < \frac{3}{b}","['probability', 'probability-theory', 'inequality', 'solution-verification', 'moment-generating-functions']"
26,Is there a name for this apparent paradox?,Is there a name for this apparent paradox?,,"I was recently reminded of this probability ""paradox"" (in quotes because in fact there's nothing really paradoxical about it, but it's surprising to the intuition), which I first encountered back in the rec.puzzles Usenet group.  I want to know if it has a name.  Here it is: Suppose I write any two distinct (real) numbers on two blank cards that I then put face down. You get to look at the number on one of the cards, of your choice.  Now, I ask you whether that number is the lower or the higher of the two. Can you answer in such a way that your probability of being correct is strictly greater than $\frac12$ ? At first, it seems incredible that this is even being asked. I can use any method I choose to select the two numbers, and you have no way of knowing what the other number is. Nonetheless, you can answer in such a way that your response is more likely than not to be correct. Method: Select any cumulative distribution function $F(x)$ that is strictly increasing; that is, pick a function $F(x)$ such that $F(x) > F(y)$ for all $x > y$ $\lim_{x \to -\infty} F(x) = 0$ $\lim_{x \to +\infty} F(x) = 1$ To make things simple, a suitable function is $F(x) = \frac{2^x}{1 + 2^x}$ .  (This is similar to the standard logistic function, but with base $2$ instead of base $e$ .) Now, if the number you look at is $x$ , you say ""higher"" with probability $F(x)$ , and ""lower"" with probability $1-F(x)$ .  For instance, if you turn over a card and see the number $3$ , you say ""higher"" with probability $F(3) = \frac{2^3}{1+2^3} = \frac89$ , and ""lower"" with probability $1 – F(3) = \frac19$ . Your probability of getting the right answer can now be determined as follows: Suppose the two numbers I selected were $a$ and $b$ , with $b > a$ .  Your answer is correct if either you looked at $a$ and said ""lower"" (with probability $1-F(a)$ ), or you looked at $b$ and said ""higher"" (with probability $F(b)$ ).  Since each of these two scenarios is equally likely—remember, you got to choose the card you looked at, with no prior information—your overall probability of guessing correctly is $$ P(\text{correct}) = \frac{1-F(a)+F(b)}{2} = \frac12 + \frac{F(b)-F(a)}{2} $$ But since $F(x)$ is strictly increasing, and $b > a$ , we must have $F(b) > F(a)$ , and so your probability of being correct must be strictly greater than $\frac12$ . Has anyone else heard of this seeming paradox, along with a name? Update (2019-10-16-0413Z): Math.SE didn't provide this tip before I posted the question, but now this shows up as a ""related question"": Who discovered this number-guessing paradox? I'll have to take a closer look at that!","I was recently reminded of this probability ""paradox"" (in quotes because in fact there's nothing really paradoxical about it, but it's surprising to the intuition), which I first encountered back in the rec.puzzles Usenet group.  I want to know if it has a name.  Here it is: Suppose I write any two distinct (real) numbers on two blank cards that I then put face down. You get to look at the number on one of the cards, of your choice.  Now, I ask you whether that number is the lower or the higher of the two. Can you answer in such a way that your probability of being correct is strictly greater than ? At first, it seems incredible that this is even being asked. I can use any method I choose to select the two numbers, and you have no way of knowing what the other number is. Nonetheless, you can answer in such a way that your response is more likely than not to be correct. Method: Select any cumulative distribution function that is strictly increasing; that is, pick a function such that for all To make things simple, a suitable function is .  (This is similar to the standard logistic function, but with base instead of base .) Now, if the number you look at is , you say ""higher"" with probability , and ""lower"" with probability .  For instance, if you turn over a card and see the number , you say ""higher"" with probability , and ""lower"" with probability . Your probability of getting the right answer can now be determined as follows: Suppose the two numbers I selected were and , with .  Your answer is correct if either you looked at and said ""lower"" (with probability ), or you looked at and said ""higher"" (with probability ).  Since each of these two scenarios is equally likely—remember, you got to choose the card you looked at, with no prior information—your overall probability of guessing correctly is But since is strictly increasing, and , we must have , and so your probability of being correct must be strictly greater than . Has anyone else heard of this seeming paradox, along with a name? Update (2019-10-16-0413Z): Math.SE didn't provide this tip before I posted the question, but now this shows up as a ""related question"": Who discovered this number-guessing paradox? I'll have to take a closer look at that!","\frac12 F(x) F(x) F(x) > F(y) x > y \lim_{x \to -\infty} F(x) = 0 \lim_{x \to +\infty} F(x) = 1 F(x) = \frac{2^x}{1 + 2^x} 2 e x F(x) 1-F(x) 3 F(3) = \frac{2^3}{1+2^3} = \frac89 1 – F(3) = \frac19 a b b > a a 1-F(a) b F(b) 
P(\text{correct}) = \frac{1-F(a)+F(b)}{2} = \frac12 + \frac{F(b)-F(a)}{2}
 F(x) b > a F(b) > F(a) \frac12","['probability', 'puzzle']"
27,Bound on Expectation and Variance of Random variables ratio with positive weights,Bound on Expectation and Variance of Random variables ratio with positive weights,,"Let $X_1, X_2, \dots, X_n$ be $n$ strictly positive iid random variables with bounded variance. Let $w_1, w_2, \dots, w_n$ be non-negative deterministic constants such that $\sum_{i=1}^{n} w_i = 1$ . Consider the random variable $$Y_j = \frac{w_jX_j}{\sum_{i=1}^{n} w_i X_i}$$ What are the upper and lower bounds on the expectation and variance of $Y_j$ ? An easy upperbound on variance can be established by Popoviciu's inequality, but I am looking for bounds that use moment information of $X_i$ 's. For the case where $w_i = \frac{1}{n}$ , it is easy to show that the $E(Y_j) = \frac{1}{n}$ and have been asked numerous times here (see, e.g. this question ). For the general case, there are counterexamples to show $E(Y_j) \neq w_j$ in general (see this question ). However, based on my observations the expectation is very close to $w_j$ , at least for the case where $X_i = \alpha+$ Bernoulli( $p$ ) for some deterministic constant $\alpha>0$ .","Let be strictly positive iid random variables with bounded variance. Let be non-negative deterministic constants such that . Consider the random variable What are the upper and lower bounds on the expectation and variance of ? An easy upperbound on variance can be established by Popoviciu's inequality, but I am looking for bounds that use moment information of 's. For the case where , it is easy to show that the and have been asked numerous times here (see, e.g. this question ). For the general case, there are counterexamples to show in general (see this question ). However, based on my observations the expectation is very close to , at least for the case where Bernoulli( ) for some deterministic constant .","X_1, X_2, \dots, X_n n w_1, w_2, \dots, w_n \sum_{i=1}^{n} w_i = 1 Y_j = \frac{w_jX_j}{\sum_{i=1}^{n} w_i X_i} Y_j X_i w_i = \frac{1}{n} E(Y_j) = \frac{1}{n} E(Y_j) \neq w_j w_j X_i = \alpha+ p \alpha>0","['probability', 'random-variables', 'conditional-expectation', 'expected-value', 'variance']"
28,Accounting for uncertainty in an Elo rating system for Foosball,Accounting for uncertainty in an Elo rating system for Foosball,,"For a Foosball game at work we implemented a rating system based on the Elo system. Allthough we achieved a sensible result so far, which provided us with a lot of fun (which is the goal) we feel we can do better by: accounting for uncertainty for new players removing/ignoring inactive players Game description A game is played by 4 players, 2 versus 2. The first team to reach 10 goals with a two point difference wins. Current rating system Each player gets an initial rating of 1000 points. When a match is played the outcome of the match is predicted by an adaptation of Elo's rating system. First we predict the probability of team A scoring a single goal before team B (team A wins one 'ball') by $$ E_A = \frac{1}{1 + 10^{(R_B-R_A)/800}}$$ where $E_A$ is the probability that team A will win this ball and $R_{A,B}$ the sum of ratings of the players in team $A,B$ respectively. The predicted outcome of the match is then found by 'translating' this prediction to 20 balls: $$E_{\Delta S} = 20*(E_A-\frac{1}{2}) $$ with $E_{\Delta S}$ the predicted difference in score and $E_A$ the probability of team A winning a ball. If $E_{\Delta S} > 0$ team A is expected to win with a difference of $E_{\Delta S}$ goals, if $E_{\Delta S} < 0$ team B is expected to win with a difference of $E_{\Delta S}$. The change in rating for the players in this match is then calculated from the difference between the prediction and the actual outcome $\Delta S$: $$ \Delta r = k\cdot (\Delta S - E_{\Delta S})) $$ With $k$ a parameter to make the changes in rating more agressive. For us a value of $k=10$ works nicely since every goal different from the predicted score earns you $10$ points. A nice round number to remember in the heat of the game. The ratings for the players in the match are then updated by adding $\Delta r$ to their old rating for the players in team A, and subtracting $\Delta r$ for the players in team B. Experience I have not thoroughly examined the performance of this algorithm since I know it has some flaws. However the expected score predictions 'feel' good. They match relatively well with what we expect when playing with/against people we know. Furthermore in the last 50 of 170 the error in expected score $\epsilon = \mathrm{abs}(E_{\Delta S} - \Delta S)$ was on average 3. This may not be as accurate as it gets, but we consider this reasonable (especially since we have not accounted for the fact that games must end with a 2 point difference). Question 1 We do notice however that the most inaccurate predictions are given when a player's rating is insanely high or low because he/she has recently achieved a very improbable result (lost or won with a very large score difference). This we would like to solve by predicting how accurate the rating of a player is, and adapting the change in rating accordingly. This could also help the fact that new players start with a 1000 points which is in most cases not accurate at all. What do you think is a good measure for the accuracy of the current rating and what is an appropriate relation to the ratingchange? Question 2 We have a high throughput of employees, and so there are a lot of players that no longer play. We can however not simply delete their entries since the average amount of points per player must remain at 1000 so as to counteract inflation/deflation. What are your ideas on removing players and what should I do with their points? Thanks Also rough thoughts and ideas are welcome, we may be able to translate your thoughts and ideas to formulas ourselves.","For a Foosball game at work we implemented a rating system based on the Elo system. Allthough we achieved a sensible result so far, which provided us with a lot of fun (which is the goal) we feel we can do better by: accounting for uncertainty for new players removing/ignoring inactive players Game description A game is played by 4 players, 2 versus 2. The first team to reach 10 goals with a two point difference wins. Current rating system Each player gets an initial rating of 1000 points. When a match is played the outcome of the match is predicted by an adaptation of Elo's rating system. First we predict the probability of team A scoring a single goal before team B (team A wins one 'ball') by $$ E_A = \frac{1}{1 + 10^{(R_B-R_A)/800}}$$ where $E_A$ is the probability that team A will win this ball and $R_{A,B}$ the sum of ratings of the players in team $A,B$ respectively. The predicted outcome of the match is then found by 'translating' this prediction to 20 balls: $$E_{\Delta S} = 20*(E_A-\frac{1}{2}) $$ with $E_{\Delta S}$ the predicted difference in score and $E_A$ the probability of team A winning a ball. If $E_{\Delta S} > 0$ team A is expected to win with a difference of $E_{\Delta S}$ goals, if $E_{\Delta S} < 0$ team B is expected to win with a difference of $E_{\Delta S}$. The change in rating for the players in this match is then calculated from the difference between the prediction and the actual outcome $\Delta S$: $$ \Delta r = k\cdot (\Delta S - E_{\Delta S})) $$ With $k$ a parameter to make the changes in rating more agressive. For us a value of $k=10$ works nicely since every goal different from the predicted score earns you $10$ points. A nice round number to remember in the heat of the game. The ratings for the players in the match are then updated by adding $\Delta r$ to their old rating for the players in team A, and subtracting $\Delta r$ for the players in team B. Experience I have not thoroughly examined the performance of this algorithm since I know it has some flaws. However the expected score predictions 'feel' good. They match relatively well with what we expect when playing with/against people we know. Furthermore in the last 50 of 170 the error in expected score $\epsilon = \mathrm{abs}(E_{\Delta S} - \Delta S)$ was on average 3. This may not be as accurate as it gets, but we consider this reasonable (especially since we have not accounted for the fact that games must end with a 2 point difference). Question 1 We do notice however that the most inaccurate predictions are given when a player's rating is insanely high or low because he/she has recently achieved a very improbable result (lost or won with a very large score difference). This we would like to solve by predicting how accurate the rating of a player is, and adapting the change in rating accordingly. This could also help the fact that new players start with a 1000 points which is in most cases not accurate at all. What do you think is a good measure for the accuracy of the current rating and what is an appropriate relation to the ratingchange? Question 2 We have a high throughput of employees, and so there are a lot of players that no longer play. We can however not simply delete their entries since the average amount of points per player must remain at 1000 so as to counteract inflation/deflation. What are your ideas on removing players and what should I do with their points? Thanks Also rough thoughts and ideas are welcome, we may be able to translate your thoughts and ideas to formulas ourselves.",,['probability']
29,Asymptotic value of card drawing game,Asymptotic value of card drawing game,,"A deck consisting of $r_0$ red cards and $b_0$ black cards is randomly shuffled. The host turns up the cards one at a time; if it is red, you get $\$1$; otherwise you pay the host $\$1$ (and you're allowed to go into debt).  You can stop the game at any time (even at the beginning of the game). Let $f(r_0,b_0)$ denote the value of the game. Question: What is the asymptotic behavior of $f(n,n)$?  More generally, for fixed $\Delta\in\mathbb{Z}$, what is the asymptotic behavior of $f(n+\Delta,n)$? Partial analysis. The state of the game at any point is a pair $(r,b)$ indicating the number of cards of each color remaining.  Let $f_{r_0,b_0}(r,b)$ denote the current value of the game if the initial state was $(r_0,b_0)$ and the current state is $(r,b)$.  Note that our accumulated balance at $(r,b)$ is $(r_0-r)-(b_0-b)=(b-r)+\Delta$ dollars.  Since we can either stop there or keep playing, we have the recurrence $$f_{r_0,b_0}(r,b)=\max\left(b-r+\Delta,\frac{r}{r+b}f_{r_0,b_0}(r-1,b)+\frac{b}{r+b}f_{r_0,b_0}(r,b-1)\right).$$ Since the recurrence only depends on $\Delta$, not on $r_0$ or $b_0$ individually, it makes sense to define $f_\Delta(r,b)$ to be the value of the game at state $(r,b)$ if the initial state $(r_0,b_0)$ satisfied $r_0-b_0=\Delta$.  We have $$f_\Delta(r,b)=\max\left(b-r+\Delta,\frac{r}{r+b}f_\Delta(r-1,b)+\frac{b}{r+b}f_\Delta(r,b-1)\right),$$ with $f(r_0,b_0)=f_\Delta(r_0,b_0)$ where $\Delta=r_0-b_0$. The boundary conditions are $f_\Delta(0,0)=\Delta$, and $f_\Delta(r,b)=0$ if either $r$ or $b$ is negative.  [Thanks to Julian Rosen for catching an error in my original posting here.]  It's now easy to check that $f_\Delta(r,b)=\Delta+f_0(r,b)$, so it suffices to compute $f_0$.  Calculating individual values of $f_0(r,b)$ is easy using the recurrence and dynamic programming.  Here are values of $f_0(r,b)$ for small $r$ and $b$: $$ \begin{array}{c|ccccccc}  r\,\backslash\,b & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline  0 & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\  1 & 0 & \frac{1}{2} & 1 & 2 & 3 & 4 & 5 \\  2 & 0 & \frac{1}{3} & \frac{2}{3} & \frac{6}{5} & 2 & 3 & 4 \\  3 & 0 & \frac{1}{4} & \frac{1}{2} & \frac{17}{20} & \frac{47}{35} & 2 & 3 \\  4 & 0 & \frac{1}{5} & \frac{2}{5} & \frac{23}{35} & 1 & \frac{13}{9} & \frac{31}{15} \\  5 & 0 & \frac{1}{6} & \frac{1}{3} & \frac{15}{28} & \frac{50}{63} & \frac{47}{42} & \frac{358}{231} \\  6 & 0 & \frac{1}{7} & \frac{2}{7} & \frac{19}{42} & \frac{23}{35} & \frac{10}{11} & \frac{284}{231} \\ \end{array} $$ The diagonal entries in the table above lead to OEIS sequences A108883 -A108886, which describe this game (as an urn game), but no asymptotics are given. The value of the game from the starting point $(r_0,b_0)$ is $f_{r_0-b_0}(r_0,b_0)=f_0(r_0,b_0)+r_0-b_0$; numerically, this is: \begin{array}{c|ccccccc}  r_0\,\backslash\,b_0 & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline  0 & 0 & 0. & 0. & 0. & 0. & 0. & 0. \\  1 & 1. & 0.5 & 0. & 0. & 0. & 0. & 0. \\  2 & 2. & 1.33333 & 0.666667 & 0.2 & 0. & 0. & 0. \\  3 & 3. & 2.25 & 1.5 & 0.85 & 0.342857 & 0. & 0. \\  4 & 4. & 3.2 & 2.4 & 1.65714 & 1. & 0.444444 & 0.0666667 \\  5 & 5. & 4.16667 & 3.33333 & 2.53571 & 1.79365 & 1.11905 & 0.549784 \\  6 & 6. & 5.14286 & 4.28571 & 3.45238 & 2.65714 & 1.90909 & 1.22944 \\ \end{array} Here are plots of $\ln f(n,n)$ and $\ln f(n,n) / \ln n$ for $n\le 1000$: Remark. This question arose based on my misreading of this question .","A deck consisting of $r_0$ red cards and $b_0$ black cards is randomly shuffled. The host turns up the cards one at a time; if it is red, you get $\$1$; otherwise you pay the host $\$1$ (and you're allowed to go into debt).  You can stop the game at any time (even at the beginning of the game). Let $f(r_0,b_0)$ denote the value of the game. Question: What is the asymptotic behavior of $f(n,n)$?  More generally, for fixed $\Delta\in\mathbb{Z}$, what is the asymptotic behavior of $f(n+\Delta,n)$? Partial analysis. The state of the game at any point is a pair $(r,b)$ indicating the number of cards of each color remaining.  Let $f_{r_0,b_0}(r,b)$ denote the current value of the game if the initial state was $(r_0,b_0)$ and the current state is $(r,b)$.  Note that our accumulated balance at $(r,b)$ is $(r_0-r)-(b_0-b)=(b-r)+\Delta$ dollars.  Since we can either stop there or keep playing, we have the recurrence $$f_{r_0,b_0}(r,b)=\max\left(b-r+\Delta,\frac{r}{r+b}f_{r_0,b_0}(r-1,b)+\frac{b}{r+b}f_{r_0,b_0}(r,b-1)\right).$$ Since the recurrence only depends on $\Delta$, not on $r_0$ or $b_0$ individually, it makes sense to define $f_\Delta(r,b)$ to be the value of the game at state $(r,b)$ if the initial state $(r_0,b_0)$ satisfied $r_0-b_0=\Delta$.  We have $$f_\Delta(r,b)=\max\left(b-r+\Delta,\frac{r}{r+b}f_\Delta(r-1,b)+\frac{b}{r+b}f_\Delta(r,b-1)\right),$$ with $f(r_0,b_0)=f_\Delta(r_0,b_0)$ where $\Delta=r_0-b_0$. The boundary conditions are $f_\Delta(0,0)=\Delta$, and $f_\Delta(r,b)=0$ if either $r$ or $b$ is negative.  [Thanks to Julian Rosen for catching an error in my original posting here.]  It's now easy to check that $f_\Delta(r,b)=\Delta+f_0(r,b)$, so it suffices to compute $f_0$.  Calculating individual values of $f_0(r,b)$ is easy using the recurrence and dynamic programming.  Here are values of $f_0(r,b)$ for small $r$ and $b$: $$ \begin{array}{c|ccccccc}  r\,\backslash\,b & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline  0 & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\  1 & 0 & \frac{1}{2} & 1 & 2 & 3 & 4 & 5 \\  2 & 0 & \frac{1}{3} & \frac{2}{3} & \frac{6}{5} & 2 & 3 & 4 \\  3 & 0 & \frac{1}{4} & \frac{1}{2} & \frac{17}{20} & \frac{47}{35} & 2 & 3 \\  4 & 0 & \frac{1}{5} & \frac{2}{5} & \frac{23}{35} & 1 & \frac{13}{9} & \frac{31}{15} \\  5 & 0 & \frac{1}{6} & \frac{1}{3} & \frac{15}{28} & \frac{50}{63} & \frac{47}{42} & \frac{358}{231} \\  6 & 0 & \frac{1}{7} & \frac{2}{7} & \frac{19}{42} & \frac{23}{35} & \frac{10}{11} & \frac{284}{231} \\ \end{array} $$ The diagonal entries in the table above lead to OEIS sequences A108883 -A108886, which describe this game (as an urn game), but no asymptotics are given. The value of the game from the starting point $(r_0,b_0)$ is $f_{r_0-b_0}(r_0,b_0)=f_0(r_0,b_0)+r_0-b_0$; numerically, this is: \begin{array}{c|ccccccc}  r_0\,\backslash\,b_0 & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline  0 & 0 & 0. & 0. & 0. & 0. & 0. & 0. \\  1 & 1. & 0.5 & 0. & 0. & 0. & 0. & 0. \\  2 & 2. & 1.33333 & 0.666667 & 0.2 & 0. & 0. & 0. \\  3 & 3. & 2.25 & 1.5 & 0.85 & 0.342857 & 0. & 0. \\  4 & 4. & 3.2 & 2.4 & 1.65714 & 1. & 0.444444 & 0.0666667 \\  5 & 5. & 4.16667 & 3.33333 & 2.53571 & 1.79365 & 1.11905 & 0.549784 \\  6 & 6. & 5.14286 & 4.28571 & 3.45238 & 2.65714 & 1.90909 & 1.22944 \\ \end{array} Here are plots of $\ln f(n,n)$ and $\ln f(n,n) / \ln n$ for $n\le 1000$: Remark. This question arose based on my misreading of this question .",,"['probability', 'recurrence-relations', 'asymptotics', 'card-games']"
30,Density given by variable-coefficient PDE,Density given by variable-coefficient PDE,,"I am looking for a time-dependent probability density $f(x,y,t)$ solving the equation $$-\frac{\partial f}{\partial t} = \alpha\cdot \big(y - F(x)\big)\frac{\partial f}{\partial x}+\beta\cdot \big(G(y)-x\big)\frac{\partial f}{\partial y},$$ where the coefficient functions $F$ and $G$ given by\begin{align}F(x) &= a_0 + a_1 x + o(x - x^{0}),\\ G(y) &=b_0 + b_1y + o(y-y^{0}).\end{align} My question is: What general class of equations does this PDE belong to, and what would be your angle of attack on such problem (if it makes sense at all)? Remark I: Under the assumption of existence of a solution, my initial angle of attack was to rewrite the equation as $$i \frac{\partial f}{\partial t} = L\,f$$ where $L$ is an operator acting on $f$ given by $L = -i\left[\alpha\big(y - F(x)\big)\frac{\partial}{\partial x}+\beta\big(G(y)-x\big)\frac{\partial}{\partial y}\right]$. Then - I suppose - we can formally express the density $f$ as $$f(x,y,t)=\sum_k c_ke^{-i\lambda_k t}\varphi_k(x,y),$$ where $\lambda_k$ and $\varphi_k$ are the eigenvalues and eigenfunctions corresponding to the operator $L$. I am thus led to considering the equations  $$L(\varphi_k) = \lambda_k\,\varphi_k$$ and swap the solutions $\varphi_k$ and $\lambda_k$ back the into the formal solution. But I have not been able to find the eigenfunctions, perhaps my patience was too short. Remark II: A necessary condition, I suppose, for the existence of a density solving the PDE is the existence of curves $x$ and $y$ solving the ""predator-prey"" equations $$\frac{dx}{dt} = \alpha\cdot (y - F(x)),\qquad \frac{dy}{dt} = \beta\cdot (G(y)-x).$$ If we neglect the little-o terms, by putting them equal to zero  (is that completely illegal?), the curves $x$ and $y$ are given by \begin{align}x(t) &=-\frac{a^0}{a^1} + C_1 e^{-a^1\mu t} + \frac{1}{1-a^1}\left[\frac{b^0}{b^1}+\frac{a^0}{a^1b^1}-C_2 e^{b^1\lambda t}-\frac{C_4}{b^1}e^{-a^1\mu t}\right]\\ y(t) &= \frac{a^1}{1-a^1}\left[\frac{b^0}{b^1}+\frac{a^0}{a^1b^1}-C_5 e^{b^1\lambda t}-\frac{C_6}{b^1}e^{-a^1\mu t}\right] \end{align} where $C_1$, $C_2$, $C_3$, $C_4$, $C_5$, and $C_6$ are arbitrary constants.","I am looking for a time-dependent probability density $f(x,y,t)$ solving the equation $$-\frac{\partial f}{\partial t} = \alpha\cdot \big(y - F(x)\big)\frac{\partial f}{\partial x}+\beta\cdot \big(G(y)-x\big)\frac{\partial f}{\partial y},$$ where the coefficient functions $F$ and $G$ given by\begin{align}F(x) &= a_0 + a_1 x + o(x - x^{0}),\\ G(y) &=b_0 + b_1y + o(y-y^{0}).\end{align} My question is: What general class of equations does this PDE belong to, and what would be your angle of attack on such problem (if it makes sense at all)? Remark I: Under the assumption of existence of a solution, my initial angle of attack was to rewrite the equation as $$i \frac{\partial f}{\partial t} = L\,f$$ where $L$ is an operator acting on $f$ given by $L = -i\left[\alpha\big(y - F(x)\big)\frac{\partial}{\partial x}+\beta\big(G(y)-x\big)\frac{\partial}{\partial y}\right]$. Then - I suppose - we can formally express the density $f$ as $$f(x,y,t)=\sum_k c_ke^{-i\lambda_k t}\varphi_k(x,y),$$ where $\lambda_k$ and $\varphi_k$ are the eigenvalues and eigenfunctions corresponding to the operator $L$. I am thus led to considering the equations  $$L(\varphi_k) = \lambda_k\,\varphi_k$$ and swap the solutions $\varphi_k$ and $\lambda_k$ back the into the formal solution. But I have not been able to find the eigenfunctions, perhaps my patience was too short. Remark II: A necessary condition, I suppose, for the existence of a density solving the PDE is the existence of curves $x$ and $y$ solving the ""predator-prey"" equations $$\frac{dx}{dt} = \alpha\cdot (y - F(x)),\qquad \frac{dy}{dt} = \beta\cdot (G(y)-x).$$ If we neglect the little-o terms, by putting them equal to zero  (is that completely illegal?), the curves $x$ and $y$ are given by \begin{align}x(t) &=-\frac{a^0}{a^1} + C_1 e^{-a^1\mu t} + \frac{1}{1-a^1}\left[\frac{b^0}{b^1}+\frac{a^0}{a^1b^1}-C_2 e^{b^1\lambda t}-\frac{C_4}{b^1}e^{-a^1\mu t}\right]\\ y(t) &= \frac{a^1}{1-a^1}\left[\frac{b^0}{b^1}+\frac{a^0}{a^1b^1}-C_5 e^{b^1\lambda t}-\frac{C_6}{b^1}e^{-a^1\mu t}\right] \end{align} where $C_1$, $C_2$, $C_3$, $C_4$, $C_5$, and $C_6$ are arbitrary constants.",,"['probability', 'functional-analysis', 'ordinary-differential-equations', 'probability-theory', 'partial-differential-equations']"
31,Finding an upper bound for $\frac{d}{d\theta}\beta^*(\theta)|_{\theta=\theta_0}$,Finding an upper bound for,\frac{d}{d\theta}\beta^*(\theta)|_{\theta=\theta_0},"Suppose that a random variable X has a distribution depending on a parameter $\theta$, $\theta \in \Theta$, and consider a test of hypothesis $H_0: \theta = \theta_0$ versus the alternative $H_1: \theta \in \Theta_1$, where  $\theta_0 \in \Theta$ and $\Theta_1$ is a subset of $\Theta$ such that $\theta_0 \notin \Theta_1$, at size $\alpha$. Suppose that a uniformly most powerful test $\delta^*$ exists and let $\beta^*$ denote its power function. Assume the model is regular, that all power functions are defined for $\theta \in \Theta$, that $\theta_0$ is an interior point of $\Theta$, and that all power functions are differentiable at $\Theta_0$. Also assume that $\beta^*(\theta)=\alpha$. Suppose that $\Theta = R$ and that $\Theta_1 =(\theta_0,\infty)$. Find an upper bound for $\frac{d}{d\theta}\beta^*(\theta)|_{\theta=\theta_0}$. Attempted answer: The probability of a Type I error is given as $$\beta^*(\theta) \leq \int_k^\infty \frac{df(x;\theta)}{d\theta}dx = \int_k^\infty \frac{f(x;\theta)}{f(x;\theta)}* \frac{df(x;\theta)}{d\theta}dx= E_{\theta}\left[\delta(x)\frac{d\log f(x;\theta)}{d\theta}\right].$$ Here $\delta$ is a function that is 1 when the range is from $k$ to infinity, and zero otherwise. The expression needs to be evaluated at $\theta_0$. The final result is $E_{\theta_0}[\delta \frac{d\log f(x;\theta)}{d\theta}].$","Suppose that a random variable X has a distribution depending on a parameter $\theta$, $\theta \in \Theta$, and consider a test of hypothesis $H_0: \theta = \theta_0$ versus the alternative $H_1: \theta \in \Theta_1$, where  $\theta_0 \in \Theta$ and $\Theta_1$ is a subset of $\Theta$ such that $\theta_0 \notin \Theta_1$, at size $\alpha$. Suppose that a uniformly most powerful test $\delta^*$ exists and let $\beta^*$ denote its power function. Assume the model is regular, that all power functions are defined for $\theta \in \Theta$, that $\theta_0$ is an interior point of $\Theta$, and that all power functions are differentiable at $\Theta_0$. Also assume that $\beta^*(\theta)=\alpha$. Suppose that $\Theta = R$ and that $\Theta_1 =(\theta_0,\infty)$. Find an upper bound for $\frac{d}{d\theta}\beta^*(\theta)|_{\theta=\theta_0}$. Attempted answer: The probability of a Type I error is given as $$\beta^*(\theta) \leq \int_k^\infty \frac{df(x;\theta)}{d\theta}dx = \int_k^\infty \frac{f(x;\theta)}{f(x;\theta)}* \frac{df(x;\theta)}{d\theta}dx= E_{\theta}\left[\delta(x)\frac{d\log f(x;\theta)}{d\theta}\right].$$ Here $\delta$ is a function that is 1 when the range is from $k$ to infinity, and zero otherwise. The expression needs to be evaluated at $\theta_0$. The final result is $E_{\theta_0}[\delta \frac{d\log f(x;\theta)}{d\theta}].$",,"['probability', 'statistics', 'probability-theory', 'expectation', 'hypothesis-testing']"
32,Proving that Markov Chain Monte Carlo converges,Proving that Markov Chain Monte Carlo converges,,"I am trying to understand how the very basic Markov Chain Monte Carlo approach works: We try to approximately calculate the expected value $E_{\pi(x)}[X]$ by drawing sequential samples from a Markov Chain $(x_0,x_1,...)$ with the stationary distribution $\pi(x)$ and  transition matrix $T(x_i|x_{i-1})$. So, according to the MCMC approach, it should be $E_{\pi(x)}[X] \approx \frac{1}{N-N_0} \sum_{i=N_0}^{N} x_i$ where $N_0$ is the point where we assume that $p(x_{N_0})$ is close enough to the stationary distribution $\pi(x)$. We simulate the Markov Chain such that $x_0$ comes from an initial distribution $p(x_0)$ and each $x_i$ comes from $T(x_i|x_{i-1})$ . I am trying to show that $\frac{1}{N-N_0} \sum_{i=N_0}^{N} x_i \rightarrow E_{\pi(x)}[X]$ as $N \rightarrow \infty$. What I am doing currently is the following: 1-In order to simplify the problem I assume that $x_0$ comes already from the stationary distribution so we won't need a $N_0$ value. I have $S_N = \frac{1}{N} \sum_{i=0}^{N-1} x_i$ where $x_0$ comes from $\pi(x_0)$ and each $x_i$ comes from $T(x_i|x_{i-1})$ and $(x_0,x_1,...)$ is a Markov Chain. 2-The convergence of the regular Monte Carlo is shown with the Law of Large Numbers, so I try the same approach. By using the Weak Law of Large Numbers, I want to show that $P(|S_N - E_{\pi(x)}[X]| \geq \epsilon) \rightarrow 0$ as $ N \rightarrow \infty$ where $\epsilon > 0$. 3-Weak Law of Large Numbers can be proven by using Chebyshev's Inequality. For $S_N$, I write the inequality as $P(|S_N - E[S_N]| \geq \epsilon) \leq \frac{V[S_N]}{\epsilon^2}$ where $V[S_N]$ is the variance of $S_N$ and again $\epsilon > 0$. 4-I first want to show that the expected value of $S_N$, $E[S_N]$ is equal to the expected value we are after: $E_{\pi(x)}[X]$. I showed this by using the fact that the marginal of each $x_i$ in the Markov Chain is equal to the stationary distribution $\pi(x)$: $E[S_N]=E[\frac{1}{N} (x_0 + x_1 + ... + x_{N-1})] = \frac{1}{N} (E[x_0] + E[x_1] + ... + E[x_{N-1}]) = \frac{1}{N} (E_{\pi(x)}[x] + E_{\pi(x)}[x] + ... + E_{\pi(x)}[x]) = E_{\pi(x)}[x]$ 5-Now I need to evaluate the variance of $S_N$, $V[S_N]$ in order to complete the Chebyshev's Inequality. But I failed to form a closed form expression for $V[S_N]$ as I did for $E[S_N]$ and I became stuck. I have actually two questions. First one: Is my way of proving the convergence of Markov Chain Monte Carlo is correct to begin with? Second one is, how can I carry on with the proof from the 5. step? It seems that the variance of the Monte Carlo sum doesn't have a closed form solution since $(x_0,x_1,...)$ are not i.i.d and come from a Markov Chain instead. Please note that I am a Computer Engineer, not exactly from a Mathematician background, so I could have done something very naive. Thanks in advance.","I am trying to understand how the very basic Markov Chain Monte Carlo approach works: We try to approximately calculate the expected value $E_{\pi(x)}[X]$ by drawing sequential samples from a Markov Chain $(x_0,x_1,...)$ with the stationary distribution $\pi(x)$ and  transition matrix $T(x_i|x_{i-1})$. So, according to the MCMC approach, it should be $E_{\pi(x)}[X] \approx \frac{1}{N-N_0} \sum_{i=N_0}^{N} x_i$ where $N_0$ is the point where we assume that $p(x_{N_0})$ is close enough to the stationary distribution $\pi(x)$. We simulate the Markov Chain such that $x_0$ comes from an initial distribution $p(x_0)$ and each $x_i$ comes from $T(x_i|x_{i-1})$ . I am trying to show that $\frac{1}{N-N_0} \sum_{i=N_0}^{N} x_i \rightarrow E_{\pi(x)}[X]$ as $N \rightarrow \infty$. What I am doing currently is the following: 1-In order to simplify the problem I assume that $x_0$ comes already from the stationary distribution so we won't need a $N_0$ value. I have $S_N = \frac{1}{N} \sum_{i=0}^{N-1} x_i$ where $x_0$ comes from $\pi(x_0)$ and each $x_i$ comes from $T(x_i|x_{i-1})$ and $(x_0,x_1,...)$ is a Markov Chain. 2-The convergence of the regular Monte Carlo is shown with the Law of Large Numbers, so I try the same approach. By using the Weak Law of Large Numbers, I want to show that $P(|S_N - E_{\pi(x)}[X]| \geq \epsilon) \rightarrow 0$ as $ N \rightarrow \infty$ where $\epsilon > 0$. 3-Weak Law of Large Numbers can be proven by using Chebyshev's Inequality. For $S_N$, I write the inequality as $P(|S_N - E[S_N]| \geq \epsilon) \leq \frac{V[S_N]}{\epsilon^2}$ where $V[S_N]$ is the variance of $S_N$ and again $\epsilon > 0$. 4-I first want to show that the expected value of $S_N$, $E[S_N]$ is equal to the expected value we are after: $E_{\pi(x)}[X]$. I showed this by using the fact that the marginal of each $x_i$ in the Markov Chain is equal to the stationary distribution $\pi(x)$: $E[S_N]=E[\frac{1}{N} (x_0 + x_1 + ... + x_{N-1})] = \frac{1}{N} (E[x_0] + E[x_1] + ... + E[x_{N-1}]) = \frac{1}{N} (E_{\pi(x)}[x] + E_{\pi(x)}[x] + ... + E_{\pi(x)}[x]) = E_{\pi(x)}[x]$ 5-Now I need to evaluate the variance of $S_N$, $V[S_N]$ in order to complete the Chebyshev's Inequality. But I failed to form a closed form expression for $V[S_N]$ as I did for $E[S_N]$ and I became stuck. I have actually two questions. First one: Is my way of proving the convergence of Markov Chain Monte Carlo is correct to begin with? Second one is, how can I carry on with the proof from the 5. step? It seems that the variance of the Monte Carlo sum doesn't have a closed form solution since $(x_0,x_1,...)$ are not i.i.d and come from a Markov Chain instead. Please note that I am a Computer Engineer, not exactly from a Mathematician background, so I could have done something very naive. Thanks in advance.",,"['probability', 'self-learning', 'markov-chains', 'monte-carlo', 'simulation']"
33,Relation betweeen Hoeffding inequality and Chernoff bound?,Relation betweeen Hoeffding inequality and Chernoff bound?,,"If I am correct, both Hoeffding inequality and Chernoff bound are about bounds on the probability of sample mean deviates from the true mean. Besides that, I wonder how Hoeffding inequality and Cernoff bound are related? Is the conditioning Hoeffding inequality more restricted than Chernoff bound's? Is the bound in Hoeffding inequality tighter than Chernoff bound's? Is one the special case of the other? I.e. one can be derived from the other? Thanks!","If I am correct, both Hoeffding inequality and Chernoff bound are about bounds on the probability of sample mean deviates from the true mean. Besides that, I wonder how Hoeffding inequality and Cernoff bound are related? Is the conditioning Hoeffding inequality more restricted than Chernoff bound's? Is the bound in Hoeffding inequality tighter than Chernoff bound's? Is one the special case of the other? I.e. one can be derived from the other? Thanks!",,"['probability', 'inequality', 'random-variables']"
34,"Random graph connectivity, and the existence of isolated vertices","Random graph connectivity, and the existence of isolated vertices",,"Here $G_{n,p}$ represents the Erdős-Rényi random graph model, where the graph has order $n$ and each edge is added independently with probability $p$ . I am faced with proving the following claim: Show that there is a constant $c>0$ such that, for every $p$ we have: $\mathbb{P}(G_{n,p}$ is disconnected) $\leq c \mathbb{P}(G_{n,p}$ has an isolated vertex). $\,\,\,(*)$ From the appearance of the question I think it is meant to be interpreted as asking ''in the limit $n \to \infty$ ''. It is clear that $\mathbb{P}($ a fixed vertex of $G_{n,p}$ is isolated $)=(1-p)^{n-1}$ . It is easy to calculate the expected number of isolated vertices using this, but I'm not convinced that helps. As a last thought, a followup to the question asks ""What value of $c$ would be acceptable""? It is therefore probably not the case that a valid choice of $c$ will actually be obtained in the proof, although it may be reasonably clear how to calculate one; perhaps that clarifies the nature of the solution a little. Many thanks for your help. Edit: Update - I have thought a little more about it, and I have the following theorem we can hopefully make use of (if anyone is willing to help me!): suppose $p = \frac{\log{n}+\gamma(n)}{n}$ and $\gamma(n)$ grows at most slowly (say $o(\log \log n)$ ); then if $\gamma(n) \to +\infty$ , $\mathbb{P}(G_{n,p}$ disconnected) $\to 0$ , if $\gamma(n) \to -\infty$ , $\mathbb{P}(G_{n,p}$ disconnected) $\to 1$ , if $\gamma(n) \to k$ , $\mathbb{P}(G_{n,p}$ disconnected) $\to 1-e^{-e^{-k}}$ . Now in the first case, being connected implies no isolated vertex, so $(*)$ holds with any constant $c$ since both probabilities are 0. Likewise, in the second case, the graph is almost surely disconnected: while this doesn't immediately imply that an isolated vertex exists, we can hopefully say for $X:=\#$ of isolated vertices, $\mathbb{E}(X)=n(1-p)^{n-1} = n(1-\frac{\log{n}+\gamma(n)}{n})^{n-1} \sim ne^{-(\log{n}+\gamma(n))} = e^{-\gamma(n)} \to \infty$ . I think this last step holds but it may depend on $\gamma$ : in general I'm not sure for which functions $(1+\frac{f(n)}{n})^n \to e^{f(n)}$ , I know this is true for the log term but maybe not if $\gamma$ grows very fast (though obviously it can't grow any faster than $1-\frac{\log{n}}{n}$ otherwise we would have $p>1$ ). We can also calculate the second moment and get $\mathbb{E}(X^2)-\mathbb{E}(X)^2 \sim e^{-\gamma(n)}$ and deduce that with high probability there is an isolated vertex. Thus again, both probabilities are equal and we can take (e.g.) $c=1$ . The hard case is where $\gamma(n) \to k$ : in this case we can reapply the same method to get $\mathbb{E}(X) \sim e^{-k}$ , a constant. We can calculate again $\mathbb{E}(X^2) -\mathbb{E}(X)^2 \sim e^{-k}$ , and use Chebyshev's inequality to calculate $\mathbb{P}(X=0) \leq e^{-k}/e^{-2k} = e^k$ . If $k<0$ , then this gives us an actual bound on the probability; otherwise we just get $\mathbb{P}(X=0) \leq 1$ . Supposing $k<0$ then; we can rewrite as $s:=\mathbb{P}(X>0)=\mathbb{P}$ (isolated vertex) $\geq 1-e^k$ , $d:=\mathbb{P}($ disconnected) and using the fact $d =1- e^{-e^{-k}}$ and a little rearranging I think we get out the inequality $d \leq 1-\exp\left(\frac{1}{s-1}\right)$ . We can then find a $c$ which works by applying the lower bound to $s$ in terms of $k$ then looking at the values of $c$ such that $cx \geq 1-\exp\left(\frac{1}{s-1}\right),\,x \in [1-e^k,1]$ . However, this is only for fixed $k$ ! If we try and do this for every $k<0$ (i.e. every probability of this type) simultaneously, then we find that $c$ must be arbitrarily large. What's worse, this method doesn't work at all for $k \geq 0$ where we don't have a lower bound for $s$ : in this case $s$ can be arbitrarily small and we can't pick a $c$ big enough to always work. So close and yet so far. I am aware this question's length has spiralled out of control, so apologies for that - I know there's a good chance Math.SE is not going to provide me with an answer to this one. Nevertheless, it says add your working and this is what I managed to do! A proof which works for all slowly decreasing $\gamma$ or $\gamma \to k \in (-\infty,-\epsilon],$ any $\epsilon > 0$ . I have a strong suspicion this is not how I was meant to try and tackle the question, but tragically this is the best I could do so far. Thank you in advance to anyone who actually reads through all this!","Here represents the Erdős-Rényi random graph model, where the graph has order and each edge is added independently with probability . I am faced with proving the following claim: Show that there is a constant such that, for every we have: is disconnected) has an isolated vertex). From the appearance of the question I think it is meant to be interpreted as asking ''in the limit ''. It is clear that a fixed vertex of is isolated . It is easy to calculate the expected number of isolated vertices using this, but I'm not convinced that helps. As a last thought, a followup to the question asks ""What value of would be acceptable""? It is therefore probably not the case that a valid choice of will actually be obtained in the proof, although it may be reasonably clear how to calculate one; perhaps that clarifies the nature of the solution a little. Many thanks for your help. Edit: Update - I have thought a little more about it, and I have the following theorem we can hopefully make use of (if anyone is willing to help me!): suppose and grows at most slowly (say ); then if , disconnected) , if , disconnected) , if , disconnected) . Now in the first case, being connected implies no isolated vertex, so holds with any constant since both probabilities are 0. Likewise, in the second case, the graph is almost surely disconnected: while this doesn't immediately imply that an isolated vertex exists, we can hopefully say for of isolated vertices, . I think this last step holds but it may depend on : in general I'm not sure for which functions , I know this is true for the log term but maybe not if grows very fast (though obviously it can't grow any faster than otherwise we would have ). We can also calculate the second moment and get and deduce that with high probability there is an isolated vertex. Thus again, both probabilities are equal and we can take (e.g.) . The hard case is where : in this case we can reapply the same method to get , a constant. We can calculate again , and use Chebyshev's inequality to calculate . If , then this gives us an actual bound on the probability; otherwise we just get . Supposing then; we can rewrite as (isolated vertex) , disconnected) and using the fact and a little rearranging I think we get out the inequality . We can then find a which works by applying the lower bound to in terms of then looking at the values of such that . However, this is only for fixed ! If we try and do this for every (i.e. every probability of this type) simultaneously, then we find that must be arbitrarily large. What's worse, this method doesn't work at all for where we don't have a lower bound for : in this case can be arbitrarily small and we can't pick a big enough to always work. So close and yet so far. I am aware this question's length has spiralled out of control, so apologies for that - I know there's a good chance Math.SE is not going to provide me with an answer to this one. Nevertheless, it says add your working and this is what I managed to do! A proof which works for all slowly decreasing or any . I have a strong suspicion this is not how I was meant to try and tackle the question, but tragically this is the best I could do so far. Thank you in advance to anyone who actually reads through all this!","G_{n,p} n p c>0 p \mathbb{P}(G_{n,p} \leq c \mathbb{P}(G_{n,p} \,\,\,(*) n \to \infty \mathbb{P}( G_{n,p} )=(1-p)^{n-1} c c p = \frac{\log{n}+\gamma(n)}{n} \gamma(n) o(\log \log n) \gamma(n) \to +\infty \mathbb{P}(G_{n,p} \to 0 \gamma(n) \to -\infty \mathbb{P}(G_{n,p} \to 1 \gamma(n) \to k \mathbb{P}(G_{n,p} \to 1-e^{-e^{-k}} (*) c X:=\# \mathbb{E}(X)=n(1-p)^{n-1} = n(1-\frac{\log{n}+\gamma(n)}{n})^{n-1} \sim ne^{-(\log{n}+\gamma(n))} = e^{-\gamma(n)} \to \infty \gamma (1+\frac{f(n)}{n})^n \to e^{f(n)} \gamma 1-\frac{\log{n}}{n} p>1 \mathbb{E}(X^2)-\mathbb{E}(X)^2 \sim e^{-\gamma(n)} c=1 \gamma(n) \to k \mathbb{E}(X) \sim e^{-k} \mathbb{E}(X^2) -\mathbb{E}(X)^2 \sim e^{-k} \mathbb{P}(X=0) \leq e^{-k}/e^{-2k} = e^k k<0 \mathbb{P}(X=0) \leq 1 k<0 s:=\mathbb{P}(X>0)=\mathbb{P} \geq 1-e^k d:=\mathbb{P}( d =1- e^{-e^{-k}} d \leq 1-\exp\left(\frac{1}{s-1}\right) c s k c cx \geq 1-\exp\left(\frac{1}{s-1}\right),\,x \in [1-e^k,1] k k<0 c k \geq 0 s s c \gamma \gamma \to k \in (-\infty,-\epsilon], \epsilon > 0","['probability', 'graph-theory', 'random']"
35,"Does this calculation have a name, or a generic formulation?","Does this calculation have a name, or a generic formulation?",,"Background Informatiom I would appreciate help in identifying or explaining this operation: To calculate each of the $n$ values of $f(\Phi)$ : Sample from the distribution of each of $i$ parameters, $\phi_i$ Calculate the $i$ values of $g(\phi_{i})$ Subtract each $g_i(\phi_{i,n})$ from $g_i(\hat{\phi_i})$ (these are deviation) Take the sum of these deviations Add the sum of these deviations to the median in summary, this is the calculation: $$f(\Phi_n)=g(\hat{\phi}) + \sum_i(g_i(\phi_{i,n})-g_i(\hat{\phi_i}))$$ $\phi_i$ is the distribution of each of $i$ parameters $\hat{\phi}$ is a vector of the parameter medians $g$ is a vector of $i$ univariate splines, one for each parameter estimated by evaluating a multivariate model across the range of $\phi_i$ while all $\phi_{\text{not}i}$ held at their medians (a univariate sensitivity analysis of a computationally intensive prognostic model) $g_i(\hat{\phi_i})$ is the $i^{th}$ spline evaluated at the median of $\phi_i$ Questions: Is there a name or simple way to describe this calculation? My first attempt to describe the above operation: The spline ensemble is calculated as the sum of deviations from the median for for each parameter added to the median. An alternative suggestion: The spline ensemble is calculated based on the univariate anomalies for each parameter. Is there a simplified form of this computation, or expression of the equation?","Background Informatiom I would appreciate help in identifying or explaining this operation: To calculate each of the values of : Sample from the distribution of each of parameters, Calculate the values of Subtract each from (these are deviation) Take the sum of these deviations Add the sum of these deviations to the median in summary, this is the calculation: is the distribution of each of parameters is a vector of the parameter medians is a vector of univariate splines, one for each parameter estimated by evaluating a multivariate model across the range of while all held at their medians (a univariate sensitivity analysis of a computationally intensive prognostic model) is the spline evaluated at the median of Questions: Is there a name or simple way to describe this calculation? My first attempt to describe the above operation: The spline ensemble is calculated as the sum of deviations from the median for for each parameter added to the median. An alternative suggestion: The spline ensemble is calculated based on the univariate anomalies for each parameter. Is there a simplified form of this computation, or expression of the equation?","n f(\Phi) i \phi_i i g(\phi_{i}) g_i(\phi_{i,n}) g_i(\hat{\phi_i}) f(\Phi_n)=g(\hat{\phi}) + \sum_i(g_i(\phi_{i,n})-g_i(\hat{\phi_i})) \phi_i i \hat{\phi} g i \phi_i \phi_{\text{not}i} g_i(\hat{\phi_i}) i^{th} \phi_i","['probability', 'statistics', 'functions', 'algorithms', 'terminology']"
36,Cox proof of product rule - step explanation,Cox proof of product rule - step explanation,,"I'm going through ""Probability Theory - The logic of science"" written by E.T. Jaynes and I have a problem with one step on page 27/28 in the proof of the product rule. The idea here is that we have a function $F$ for which: $$(AB|C)=F[(B|C), (A|BC)]$$ We want to proof its associativity: $$F[F(x, y), z] = F[x, F(y,z)]$$ We assume that F is differentiable. We denote: $$u = F(x,y)$$ $$v = F(y,z)$$ $$F_1(x,y) = \frac{\partial F}{\partial x}$$ $$F_2(x,y) = \frac{\partial F}{\partial y}$$ With this assumptions, we want to prove: $$F(x, v) = F(u, z)$$ We differentiate w.r.t. x and y: $$F_1(x,v)=F_1(u,z)F_1(x,y)$$ $$F_2(x,v)F_1(y,z)=F_1(u,z)F_2(x,y)$$ We then use: $$G(x,y)=\frac{F_2(x,y)}{F_1(x,y)}$$ And get equation E1: $$G(x,v)F_1(y,z)=G(x,y)$$ Which can be rewritten as equation E2: $$G(x,v)F_2(y,z)=G(x,y)G(y,z)$$ Now there's the part is hard for me to understand. We differentiate E1 w.r.t. z and E2 w.r.t. y. The proof states that left hand sides of both derivatives are the same. How is that? When I'm using product rule of calculus and chain rule, I get summation in E1 left side: $$G_2(x,v)F_2(y,z)F_1(y,z)+G(x,v)F_{12}(y,z)$$ and for equation E2, left side equals: $$G_2(x,v)F_1(y,z)F_2(y,z)+G(x,v)F_{21}(y,z)$$ So there's the quation - where's my mistake? I can see that first product of both equations derivative are the same but how this equations are the same? I can see two options: first, I shouldn't use product rule here (why?), second, for some reason  $$F_{12}(y,z)=F_{21}(y,z)$$ but I can't see reason for any of these. I've checked original Cox proof in his book ""The algebra of probable inference"" but it's quite the same as in Jaynes book.","I'm going through ""Probability Theory - The logic of science"" written by E.T. Jaynes and I have a problem with one step on page 27/28 in the proof of the product rule. The idea here is that we have a function $F$ for which: $$(AB|C)=F[(B|C), (A|BC)]$$ We want to proof its associativity: $$F[F(x, y), z] = F[x, F(y,z)]$$ We assume that F is differentiable. We denote: $$u = F(x,y)$$ $$v = F(y,z)$$ $$F_1(x,y) = \frac{\partial F}{\partial x}$$ $$F_2(x,y) = \frac{\partial F}{\partial y}$$ With this assumptions, we want to prove: $$F(x, v) = F(u, z)$$ We differentiate w.r.t. x and y: $$F_1(x,v)=F_1(u,z)F_1(x,y)$$ $$F_2(x,v)F_1(y,z)=F_1(u,z)F_2(x,y)$$ We then use: $$G(x,y)=\frac{F_2(x,y)}{F_1(x,y)}$$ And get equation E1: $$G(x,v)F_1(y,z)=G(x,y)$$ Which can be rewritten as equation E2: $$G(x,v)F_2(y,z)=G(x,y)G(y,z)$$ Now there's the part is hard for me to understand. We differentiate E1 w.r.t. z and E2 w.r.t. y. The proof states that left hand sides of both derivatives are the same. How is that? When I'm using product rule of calculus and chain rule, I get summation in E1 left side: $$G_2(x,v)F_2(y,z)F_1(y,z)+G(x,v)F_{12}(y,z)$$ and for equation E2, left side equals: $$G_2(x,v)F_1(y,z)F_2(y,z)+G(x,v)F_{21}(y,z)$$ So there's the quation - where's my mistake? I can see that first product of both equations derivative are the same but how this equations are the same? I can see two options: first, I shouldn't use product rule here (why?), second, for some reason  $$F_{12}(y,z)=F_{21}(y,z)$$ but I can't see reason for any of these. I've checked original Cox proof in his book ""The algebra of probable inference"" but it's quite the same as in Jaynes book.",,"['probability', 'proof-explanation']"
37,Fingerprint match probability,Fingerprint match probability,,"I am trying to use the formula for the birthday paradox as a reference to figure out an equation that represents the probability of a fingerprint match. Here's the equation for probability of a matching birthday. $$ p(n) = 1-\frac{364}{365}^{\frac{n(n-1)}{2}} $$ Where n is the number of people in the room. There are obviously a few things that are different for fingerprint probability. The available different fingerprints is theoretically infinite but I am going to go off of the assumption Apple made that there is a 1 in 50,000 chance of a match. Each person has more than one fingerprint (10 total) that can be used The secured device can have one or more fingerprints registered as secure (up to 10) which will also increase probability of a match I tried to adapt this equation myself and ended up with this. Fr represents the number of registered fingers, n the amount of people in the room (multiplied by 10 to include all fingers), and Pr is the match probability (1/50000) $$ 1-Pr^{\frac{Fr*((n*10)-Fr)}{2}} $$ This equation doesn't work for a couple reasons though (there's probably more I'm missing too) It doesn't remove the registered fingerprints from a match per phone. e.g. if two people, each with their own phone, have registered 1 finger each then the total available fingerprints for a match is 19 per phone. The equation assumes each person has a phone with registered fingerprints (I'm OK with that assumption) When I work out this equation with 50 people each registering 3 fingerprints I get $$ 1-\frac{49999}{50000}^{\frac{150*((50*10)-150)}{2}} $$ Which gives me 52.9% chance of a match which seems way too high. Can someone help me figure out what I'm doing wrong?","I am trying to use the formula for the birthday paradox as a reference to figure out an equation that represents the probability of a fingerprint match. Here's the equation for probability of a matching birthday. $$ p(n) = 1-\frac{364}{365}^{\frac{n(n-1)}{2}} $$ Where n is the number of people in the room. There are obviously a few things that are different for fingerprint probability. The available different fingerprints is theoretically infinite but I am going to go off of the assumption Apple made that there is a 1 in 50,000 chance of a match. Each person has more than one fingerprint (10 total) that can be used The secured device can have one or more fingerprints registered as secure (up to 10) which will also increase probability of a match I tried to adapt this equation myself and ended up with this. Fr represents the number of registered fingers, n the amount of people in the room (multiplied by 10 to include all fingers), and Pr is the match probability (1/50000) $$ 1-Pr^{\frac{Fr*((n*10)-Fr)}{2}} $$ This equation doesn't work for a couple reasons though (there's probably more I'm missing too) It doesn't remove the registered fingerprints from a match per phone. e.g. if two people, each with their own phone, have registered 1 finger each then the total available fingerprints for a match is 19 per phone. The equation assumes each person has a phone with registered fingerprints (I'm OK with that assumption) When I work out this equation with 50 people each registering 3 fingerprints I get $$ 1-\frac{49999}{50000}^{\frac{150*((50*10)-150)}{2}} $$ Which gives me 52.9% chance of a match which seems way too high. Can someone help me figure out what I'm doing wrong?",,"['probability', 'paradoxes']"
38,A six-sided die is rolled 5 times. The roller wins if the last roll is the same as one of the previous rolls.,A six-sided die is rolled 5 times. The roller wins if the last roll is the same as one of the previous rolls.,,"What is the probability of winning this game? I tried some but the closest I get was $\frac 16 \cdot \frac 16$, but it's not right.","What is the probability of winning this game? I tried some but the closest I get was $\frac 16 \cdot \frac 16$, but it's not right.",,"['probability', 'discrete-mathematics', 'dice']"
39,"By means of an example, show that $P(A) + P(B) = 1$ does not mean that $B$ is the complement of $A$","By means of an example, show that  does not mean that  is the complement of",P(A) + P(B) = 1 B A,"I'm in grade 10, and I've just started to learn about complementary events. I am rather perplexed with this question. Isn't this question kinda contradictory, since $P(A) + P(A') = 1$ ? This is what I got to: $P(A) + P(B) = 1$ $P(A) + P(A') = 1$ How could it be proven that $B$ isn't the complement of $A$ ? Help would be greatly appreciated.","I'm in grade 10, and I've just started to learn about complementary events. I am rather perplexed with this question. Isn't this question kinda contradictory, since ? This is what I got to: How could it be proven that isn't the complement of ? Help would be greatly appreciated.",P(A) + P(A') = 1 P(A) + P(B) = 1 P(A) + P(A') = 1 B A,['probability']
40,"Draw cards repeatedly, until we find the ace of spades. Probability that we draw between 20 and 30 cards?","Draw cards repeatedly, until we find the ace of spades. Probability that we draw between 20 and 30 cards?",,"consider this problem: Draw cards repeatedly, without replacement, from a standard 52-card deck until we find the ace of spades. What is the probability that we draw between 20 and 30 cards? The solution I came across: $$P(A)=\frac{11}{52}$$ where the numerator denotes the sum from the 20th to the 30th draw, and the denominator the sum of all possible draws from the 1st until the 52nd. I've thought about this problem a lot, and I just do not understand the thinking put behind it, this problem clearly requires more attention, order does matter and the fact that the cards drawn can not be replaced made me doubt the above solution. The solution that I believe is correct: $$P(A)=\frac{1}{33}+\frac{1}{32}+\frac{1}{31}+...+\frac{1}{23}$$ Why?  Well the first 19 drawn cards must have resulted in other values than the ones required and considering the fact that cards drawn can not be replaced, the denominator or sample space always decreases by 1 until exactly the criterion of having to draw between 20 and 30 cards is met. I would like to be assured if first of all the solution I gave is correct, or corrected if I am wrong, I would also appreciate if you could provide other helpful ideas, methods which I could to tackle these kind of problems.","consider this problem: Draw cards repeatedly, without replacement, from a standard 52-card deck until we find the ace of spades. What is the probability that we draw between 20 and 30 cards? The solution I came across: $$P(A)=\frac{11}{52}$$ where the numerator denotes the sum from the 20th to the 30th draw, and the denominator the sum of all possible draws from the 1st until the 52nd. I've thought about this problem a lot, and I just do not understand the thinking put behind it, this problem clearly requires more attention, order does matter and the fact that the cards drawn can not be replaced made me doubt the above solution. The solution that I believe is correct: $$P(A)=\frac{1}{33}+\frac{1}{32}+\frac{1}{31}+...+\frac{1}{23}$$ Why?  Well the first 19 drawn cards must have resulted in other values than the ones required and considering the fact that cards drawn can not be replaced, the denominator or sample space always decreases by 1 until exactly the criterion of having to draw between 20 and 30 cards is met. I would like to be assured if first of all the solution I gave is correct, or corrected if I am wrong, I would also appreciate if you could provide other helpful ideas, methods which I could to tackle these kind of problems.",,['probability']
41,Finding the probability that an ace is found in every pile when a deck of cards is split into 4,Finding the probability that an ace is found in every pile when a deck of cards is split into 4,,"I'm trying to answer this question and you are supposed to use the multiplication rule to solve it: A deck of 52 playing cards is randomly divided into four piles of 13 cards each. Compute the probability that each pile has exactly 1 ace. I started off by defining following 4 events: $A_{1}, A_{2}, A_{3}$ and $A_{4}$ where $A_{i}$ denotes the event that exactly one ace is found in the i$^{th}$ pile - so to find the probability I need to find the probability of the intersection of all these events which is where I can use the multiplication rule. The multiplication rule says that $$\mathbb{P}(A_{1} \cap A_{2} \cap A_{3} \cap A_{4})= \mathbb{P}(A_{1}) \mathbb{P}(A_{2}|A_{1}) \mathbb{P}(A_{3}|A_{2} \cap A_{1}) \mathbb{P}(A_{4}|A_{3} \cap A_{2} \cap A_{1})$$ to find each of the probabilities on the RHS I looked compared the possible combinations allowed for each situation: The number of possible card combinations such that $A_{1}$ holds is $\binom{48}{12}$ and the total number of possible card combinations for the first pile is $\binom{52}{13}$. It then follows that $$\mathbb{P}(A_{1})= \frac{\binom{48}{12}}{\binom{52}{13}}=\frac{1406}{4165}$$ When moving on to the second pile, it follows that we now have 39 cards remaining so in order for the pile 2 to have exactly one ace, it leads to $\binom{36}{12}$ possible combinations out of the $\binom{39}{13}$ total number of combinations and so we get that $$\mathbb{P}(A_{2}|A_{1}) = \frac{\binom{36}{12}} {\binom{39}{13}} =\frac{225}{703}$$. Continuing on in this way I got that $$\mathbb{P}(A_{3}|A_{2} \cap A_{1}) = \frac{\binom{24}{12}}{\binom{26}{13}}=\frac{13}{50}$$ and by the way I have defined my events, it means that  $$\mathbb{P}(A_{4}|A_{3} \cap A_{2} \cap A_{1}) = 1$$ so by the multiplication rule I get that $$\mathbb{P}(A_{1} \cap A_{2} \cap A_{3} \cap A_{4}) = \frac{1406}{4165} \frac{225}{703} \frac{13}{50} \approx 0.0281$$ However the answer I am given says it should be $\approx 0.105$. Can anyone help me to see where I have gone wrong? Would it perhaps be that defining the events differently lead to different probabilities? Thanks!","I'm trying to answer this question and you are supposed to use the multiplication rule to solve it: A deck of 52 playing cards is randomly divided into four piles of 13 cards each. Compute the probability that each pile has exactly 1 ace. I started off by defining following 4 events: $A_{1}, A_{2}, A_{3}$ and $A_{4}$ where $A_{i}$ denotes the event that exactly one ace is found in the i$^{th}$ pile - so to find the probability I need to find the probability of the intersection of all these events which is where I can use the multiplication rule. The multiplication rule says that $$\mathbb{P}(A_{1} \cap A_{2} \cap A_{3} \cap A_{4})= \mathbb{P}(A_{1}) \mathbb{P}(A_{2}|A_{1}) \mathbb{P}(A_{3}|A_{2} \cap A_{1}) \mathbb{P}(A_{4}|A_{3} \cap A_{2} \cap A_{1})$$ to find each of the probabilities on the RHS I looked compared the possible combinations allowed for each situation: The number of possible card combinations such that $A_{1}$ holds is $\binom{48}{12}$ and the total number of possible card combinations for the first pile is $\binom{52}{13}$. It then follows that $$\mathbb{P}(A_{1})= \frac{\binom{48}{12}}{\binom{52}{13}}=\frac{1406}{4165}$$ When moving on to the second pile, it follows that we now have 39 cards remaining so in order for the pile 2 to have exactly one ace, it leads to $\binom{36}{12}$ possible combinations out of the $\binom{39}{13}$ total number of combinations and so we get that $$\mathbb{P}(A_{2}|A_{1}) = \frac{\binom{36}{12}} {\binom{39}{13}} =\frac{225}{703}$$. Continuing on in this way I got that $$\mathbb{P}(A_{3}|A_{2} \cap A_{1}) = \frac{\binom{24}{12}}{\binom{26}{13}}=\frac{13}{50}$$ and by the way I have defined my events, it means that  $$\mathbb{P}(A_{4}|A_{3} \cap A_{2} \cap A_{1}) = 1$$ so by the multiplication rule I get that $$\mathbb{P}(A_{1} \cap A_{2} \cap A_{3} \cap A_{4}) = \frac{1406}{4165} \frac{225}{703} \frac{13}{50} \approx 0.0281$$ However the answer I am given says it should be $\approx 0.105$. Can anyone help me to see where I have gone wrong? Would it perhaps be that defining the events differently lead to different probabilities? Thanks!",,"['probability', 'problem-solving']"
42,Should I throw the dice again if I have rolled 4?,Should I throw the dice again if I have rolled 4?,,"My math skills are very basic so it might be a stupid question, I had a discussion with my brother in law and now we have a 'math problem'. We were playing a game with dices and he threw 4. The challenge was to throw the highest number, you can stop or throw again once, you cant see what the opponent has thrown , you both reveal after finishing. He said if you have 4 you have a 50% change the next time to throw the same or higher. (4,5,6) vs (1,2,3) and should throw again. But I said that I won't throw again on 4 because you already threw 4 and you are not likely to throw 4, 2 times in a row and therefore I would stop at 4. Am I right or is he and do you have a 50 % chance on throwing 4 or higher again? Game rules You throw a normal dice 1/6 You can choose to throw again or keep the current value Your opponent cant see your value, you cant see his. The one with the highest value wins. Short version: If I threw 4, how big is the chance I throw 4 or more the next time I throw the dice, and should i take that chance?. Thoughts Average of 1 dice is 3.5, if i throw 4 im above Average and am more likely to throw below average next time.","My math skills are very basic so it might be a stupid question, I had a discussion with my brother in law and now we have a 'math problem'. We were playing a game with dices and he threw 4. The challenge was to throw the highest number, you can stop or throw again once, you cant see what the opponent has thrown , you both reveal after finishing. He said if you have 4 you have a 50% change the next time to throw the same or higher. (4,5,6) vs (1,2,3) and should throw again. But I said that I won't throw again on 4 because you already threw 4 and you are not likely to throw 4, 2 times in a row and therefore I would stop at 4. Am I right or is he and do you have a 50 % chance on throwing 4 or higher again? Game rules You throw a normal dice 1/6 You can choose to throw again or keep the current value Your opponent cant see your value, you cant see his. The one with the highest value wins. Short version: If I threw 4, how big is the chance I throw 4 or more the next time I throw the dice, and should i take that chance?. Thoughts Average of 1 dice is 3.5, if i throw 4 im above Average and am more likely to throw below average next time.",,['probability']
43,Find the probability that the thirteenth spade will appear before the thirteenth diamond?,Find the probability that the thirteenth spade will appear before the thirteenth diamond?,,"The $52$ cards of an ordinary deck of cards are placed successively one after the other from left to right. Find the probability that the thirteenth spade will appear before the thirteenth diamond? This appears to be a hard problem since counting the favorable cases are hard to count. Clearly, the position of the $13$ th diamond must be between $26$ and $N$ . Now, I tried to count for specific values, but I find this difficult, perhaps there is a better way? Note: I checked the answer and the favorable cases are supposed to be: $$\sum_{k=26}^{52}(n-1)!(52-n)!\binom{26}{n-26}\cdot 13.$$ So I guess this ugly sum divided by $(52)!$ is equal to $1/2,$ right? PS: Question : Answer:","The cards of an ordinary deck of cards are placed successively one after the other from left to right. Find the probability that the thirteenth spade will appear before the thirteenth diamond? This appears to be a hard problem since counting the favorable cases are hard to count. Clearly, the position of the th diamond must be between and . Now, I tried to count for specific values, but I find this difficult, perhaps there is a better way? Note: I checked the answer and the favorable cases are supposed to be: So I guess this ugly sum divided by is equal to right? PS: Question : Answer:","52 13 26 N \sum_{k=26}^{52}(n-1)!(52-n)!\binom{26}{n-26}\cdot 13. (52)! 1/2,","['probability', 'combinatorics']"
44,Find the probability that the white ball labelled $1$ is drawn before all the black balls.,Find the probability that the white ball labelled  is drawn before all the black balls.,1,"Suppose in an urn there are $20$ black balls labelled $1,2, \ldots , 20$ and $10$ white balls labelled $1,2, \ldots ,10$. Balls are drawn one by one without replacement. Find the probability that the white ball labelled $1$ is drawn before all the black balls. My attempt $:$ If we want to draw the first white ball before all the black balls then I have to draw the first white ball in one of first $10$ steps. Suppose I draw the first white ball in $k$-th step. Then in order to fulfil my requirement I have to draw white balls in first $k-1$ steps. That can be done in $\binom 9 {k-1}  (k-1)!$ ways. For each of these ways remaining $30-k$ balls can be drawn in $(30-k)!$ ways. This $k$ can run from $1$ to $10$. So the total number of ways to draw the first white ball before all the black balls is $$\sum\limits_{k=1}^{10} \binom 9 {k-1}(k-1)! (30-k)!$$ So the required probability is $$\frac1{30!}{\sum\limits_{k=1}^{10} \binom 9 {k-1}(k-1)! (30-k)!} =\sum\limits_{k=1}^{10} {\frac {9!(30-k)!} {30!(10-k)!}}$$ Now my instructor has given it's answer which is $\frac {1} {21}$. Does the above sum evaluate to $\frac {1} {21}$? Is there any other simpler way to do this? Please help me in this regard. Thank you very much.","Suppose in an urn there are $20$ black balls labelled $1,2, \ldots , 20$ and $10$ white balls labelled $1,2, \ldots ,10$. Balls are drawn one by one without replacement. Find the probability that the white ball labelled $1$ is drawn before all the black balls. My attempt $:$ If we want to draw the first white ball before all the black balls then I have to draw the first white ball in one of first $10$ steps. Suppose I draw the first white ball in $k$-th step. Then in order to fulfil my requirement I have to draw white balls in first $k-1$ steps. That can be done in $\binom 9 {k-1}  (k-1)!$ ways. For each of these ways remaining $30-k$ balls can be drawn in $(30-k)!$ ways. This $k$ can run from $1$ to $10$. So the total number of ways to draw the first white ball before all the black balls is $$\sum\limits_{k=1}^{10} \binom 9 {k-1}(k-1)! (30-k)!$$ So the required probability is $$\frac1{30!}{\sum\limits_{k=1}^{10} \binom 9 {k-1}(k-1)! (30-k)!} =\sum\limits_{k=1}^{10} {\frac {9!(30-k)!} {30!(10-k)!}}$$ Now my instructor has given it's answer which is $\frac {1} {21}$. Does the above sum evaluate to $\frac {1} {21}$? Is there any other simpler way to do this? Please help me in this regard. Thank you very much.",,"['probability', 'combinatorics', 'permutations']"
45,The probability of Bus A arriving before Bus B,The probability of Bus A arriving before Bus B,,"Bus A arrives at a random time between 2pm and 4pm, and Bus B arrives at a random time between 3pm and 5pm. What are the odds that Bus A arrives before Bus B? My understanding is that since Bus B cannot possibly arrive between 2 and 3, we can only talk about the time between 3 and 4 pm, when there is an equal probability for both buses arriving. But in this case, the probability of Bus A arriving before B is 50%. No? What am I missing here? Or I should look at the entire timeline, 2 pm - 5 pm? But then in this case, it is still 50%. Where is my thinking wrong?","Bus A arrives at a random time between 2pm and 4pm, and Bus B arrives at a random time between 3pm and 5pm. What are the odds that Bus A arrives before Bus B? My understanding is that since Bus B cannot possibly arrive between 2 and 3, we can only talk about the time between 3 and 4 pm, when there is an equal probability for both buses arriving. But in this case, the probability of Bus A arriving before B is 50%. No? What am I missing here? Or I should look at the entire timeline, 2 pm - 5 pm? But then in this case, it is still 50%. Where is my thinking wrong?",,['probability']
46,Probability: A flaw in logic? The emperor's proposition with marbles and two urns,Probability: A flaw in logic? The emperor's proposition with marbles and two urns,,"I've tried searching for this question but couldn't find it on stackexchange. This is a common type of interview question; I ran into it doing brain teasers on a probability puzzles app, and if you fine people agree with my logic, I will inform the app developer that his/her answers are incorrect. The problem is essentially this: You are sentenced to death for thievery. The King is magnanimous and decides to put your fate in the hands of chance. You are given $100$ white marbles and 100 black marbles, and $2$ urns. The king will choose an urn at random and pull out a single marble at random; if the marble is white, you live, if its black, you die. If you place the marbles in the best way possible, what is your probability of survival? I started with the base case: $100$ white marbles in one urn, $100$ black marbles in the other. This comes down to a $50$-$50$ chance of survival. I then worked my way to deciding that placing $1$ white marble in one urn and $99$ white marbles + $100$ black marbles in the other urn would be the ""best way possible"", which yields the following: $$P(\text{Survival}) = \frac{1}{2}(1+\frac{99}{199}) \approx .749$$ Selecting $1$ of $2$ urns at random gives $\frac{1}{2}$, the urn containing $1$ marble gives $1$, and the other that contains $99$ white marbles and $100$ black marbles gives $\frac{99}{199}$ because there are $99$ possible white marbles to select out of $199$ total marbles. The app claims that the correct answer is $\frac{1}{2}(1+\frac{99}{200}) \approx .748$ I see where the $200$ comes from, but I do not think it is right to say that there are $200$ marbles in the other urn. Who is correct?","I've tried searching for this question but couldn't find it on stackexchange. This is a common type of interview question; I ran into it doing brain teasers on a probability puzzles app, and if you fine people agree with my logic, I will inform the app developer that his/her answers are incorrect. The problem is essentially this: You are sentenced to death for thievery. The King is magnanimous and decides to put your fate in the hands of chance. You are given $100$ white marbles and 100 black marbles, and $2$ urns. The king will choose an urn at random and pull out a single marble at random; if the marble is white, you live, if its black, you die. If you place the marbles in the best way possible, what is your probability of survival? I started with the base case: $100$ white marbles in one urn, $100$ black marbles in the other. This comes down to a $50$-$50$ chance of survival. I then worked my way to deciding that placing $1$ white marble in one urn and $99$ white marbles + $100$ black marbles in the other urn would be the ""best way possible"", which yields the following: $$P(\text{Survival}) = \frac{1}{2}(1+\frac{99}{199}) \approx .749$$ Selecting $1$ of $2$ urns at random gives $\frac{1}{2}$, the urn containing $1$ marble gives $1$, and the other that contains $99$ white marbles and $100$ black marbles gives $\frac{99}{199}$ because there are $99$ possible white marbles to select out of $199$ total marbles. The app claims that the correct answer is $\frac{1}{2}(1+\frac{99}{200}) \approx .748$ I see where the $200$ comes from, but I do not think it is right to say that there are $200$ marbles in the other urn. Who is correct?",,['probability']
47,Expected value in coin flipping process,Expected value in coin flipping process,,"You flip a coin, and if the result is tails, you lose. If the result is heads, you get to play again. What is the expected value of throws before you lose? My Approach The expected value is the sum of all the outcomes multiplied by their respective probabilities: $$\sum_{i=1}^{n}V_iP_i$$ So for this problem: $$\sum_{i=1}^{\infty}i(\frac{1}{2^i})=0.5+0.5+0.375+…$$ I can’t figure out how to find the sum, even though I know it converges.","You flip a coin, and if the result is tails, you lose. If the result is heads, you get to play again. What is the expected value of throws before you lose? My Approach The expected value is the sum of all the outcomes multiplied by their respective probabilities: So for this problem: I can’t figure out how to find the sum, even though I know it converges.",\sum_{i=1}^{n}V_iP_i \sum_{i=1}^{\infty}i(\frac{1}{2^i})=0.5+0.5+0.375+…,"['probability', 'sequences-and-series', 'convergence-divergence']"
48,"We have two red, two white and two green marbles in an urn","We have two red, two white and two green marbles in an urn",,"We have two red, two white and two green marbles in an urn. We pick them one by one out of the urn and record their colors. Find the probability that at some point we will pick the same color back to back. For example, this happens   when we get the sequence red, white, white, green, red, green, but also if we get red, red, white, white, green, green.) So far I have the following. Note that if we think about this problem as a sequence $(x_1,x_2,x_3,x_4,x_5,x_6)$ of the balls. Note that if we fix one of the balls, then we have a $\large\frac{4}{5}\cdot\frac{3}{4}$ chance of not picking the same color next to it. hence the probability of picking two consecutive balls of the same color is equal to $\large 1-\frac{3}{5}=\frac{2}{5}.$ However I am not confident in my reasoning and I think I have made a mistake. Any help is much appreciated.","We have two red, two white and two green marbles in an urn. We pick them one by one out of the urn and record their colors. Find the probability that at some point we will pick the same color back to back. For example, this happens   when we get the sequence red, white, white, green, red, green, but also if we get red, red, white, white, green, green.) So far I have the following. Note that if we think about this problem as a sequence of the balls. Note that if we fix one of the balls, then we have a chance of not picking the same color next to it. hence the probability of picking two consecutive balls of the same color is equal to However I am not confident in my reasoning and I think I have made a mistake. Any help is much appreciated.","(x_1,x_2,x_3,x_4,x_5,x_6) \large\frac{4}{5}\cdot\frac{3}{4} \large 1-\frac{3}{5}=\frac{2}{5}.",['probability']
49,What is the probablity of sitting next to my friend?,What is the probablity of sitting next to my friend?,,"Let's say you are at a table with $5$ others, everyone is seated randomly around a $6$ person table, and you only know $1$ person at this party. What is the likelihood you sit next to the individual that you know? What is the likelihood you are seated opposite to the person that you know? What is the likelihood that you sit next to two strangers? The table has $6$ seats so if you sit in any one seat then there are $5$ chairs left over. Since your friend can be seated on either side of you that leaves 3 chairs.  With that reasoning would it be $1/3$ ( $2/6$ )?","Let's say you are at a table with others, everyone is seated randomly around a person table, and you only know person at this party. What is the likelihood you sit next to the individual that you know? What is the likelihood you are seated opposite to the person that you know? What is the likelihood that you sit next to two strangers? The table has seats so if you sit in any one seat then there are chairs left over. Since your friend can be seated on either side of you that leaves 3 chairs.  With that reasoning would it be ( )?",5 6 1 6 5 1/3 2/6,['probability']
50,Probability that at least one of 2 balls taken randomly from a pile of 2 red and 3 black is red,Probability that at least one of 2 balls taken randomly from a pile of 2 red and 3 black is red,,"I get 2 different answers, depending how I approach this, and I need help to see why the error arises. One solution is to calculate unfavorable combinations probability and substract from 1: $$1-\frac{C_3^2}{C_5^2}=\frac{7}{10}$$ The other solution is when calculating favorable combinations, to first choose one of the 2 reds, and then choose one of the remaining 4: $$\frac{C_2^1 \times C_4^1}{C_5^2} = \frac{8}{10}$$ Which is obviously $\neq \frac{7}{10}$.","I get 2 different answers, depending how I approach this, and I need help to see why the error arises. One solution is to calculate unfavorable combinations probability and substract from 1: $$1-\frac{C_3^2}{C_5^2}=\frac{7}{10}$$ The other solution is when calculating favorable combinations, to first choose one of the 2 reds, and then choose one of the remaining 4: $$\frac{C_2^1 \times C_4^1}{C_5^2} = \frac{8}{10}$$ Which is obviously $\neq \frac{7}{10}$.",,"['probability', 'combinatorics']"
51,What is the purpose of $\frac{1}{\sigma \sqrt{2 \pi}}$ in $\frac{1}{\sigma \sqrt{2 \pi}}e^{\frac{(-(x - \mu ))^2}{2\sigma ^2}}$?,What is the purpose of  in ?,\frac{1}{\sigma \sqrt{2 \pi}} \frac{1}{\sigma \sqrt{2 \pi}}e^{\frac{(-(x - \mu ))^2}{2\sigma ^2}},"I have been studying the probability density function... $$\frac{1}{\sigma \sqrt{2 \pi}}e^{\frac{(-(x - \mu ))^2}{2\sigma ^2}}$$ For now I remove the constant, and using the following proof , I prove that... $$\int_{-\infty}^{\infty}e^{\frac{-x^2}{2}} = \sqrt{2 \pi }$$ The way I interpret this is that the area under the gaussian distribution is $\sqrt{2 \pi }$ . But I am still having a hard time figuring out what the constant is doing. It seems to divide by the area itself and by $\sigma$ as well. Why is this done?","I have been studying the probability density function... For now I remove the constant, and using the following proof , I prove that... The way I interpret this is that the area under the gaussian distribution is . But I am still having a hard time figuring out what the constant is doing. It seems to divide by the area itself and by as well. Why is this done?",\frac{1}{\sigma \sqrt{2 \pi}}e^{\frac{(-(x - \mu ))^2}{2\sigma ^2}} \int_{-\infty}^{\infty}e^{\frac{-x^2}{2}} = \sqrt{2 \pi } \sqrt{2 \pi } \sigma,"['probability', 'statistics', 'probability-distributions', 'normal-distribution', 'gaussian-integral']"
52,Product of cdf and pdf of normal distribution.,Product of cdf and pdf of normal distribution.,,"A continuos random variable $X$ has the density   $$ f(x) = 2\phi(x)\Phi(x), ~x\in\mathbb{R} $$   then ( A ) $E(X) > 0$ ( B ) $E(X) < 0$ ( C ) $P(X\leq 0) > 0.5$ ( D ) $P(X\ge0) < 0.25$ \begin{eqnarray} \Phi(x) &=& \text{Cumulative distribution function of } N(0,1)\\ \phi(x) &=& \text{Density function of } N(0, 1) \end{eqnarray} I don't have a slightest clue where to start with. Can someone give me a little push. I saw some answers on same question like this but I didn't understand how should I integrate it when calculating expectation.","A continuos random variable $X$ has the density   $$ f(x) = 2\phi(x)\Phi(x), ~x\in\mathbb{R} $$   then ( A ) $E(X) > 0$ ( B ) $E(X) < 0$ ( C ) $P(X\leq 0) > 0.5$ ( D ) $P(X\ge0) < 0.25$ \begin{eqnarray} \Phi(x) &=& \text{Cumulative distribution function of } N(0,1)\\ \phi(x) &=& \text{Density function of } N(0, 1) \end{eqnarray} I don't have a slightest clue where to start with. Can someone give me a little push. I saw some answers on same question like this but I didn't understand how should I integrate it when calculating expectation.",,"['probability', 'statistics', 'probability-distributions', 'normal-distribution']"
53,Probability of Sets,Probability of Sets,,"I need some help on this one: We have sets $X$ and $Y$ chosen independently and uniformly at random from among all subsets of $\{1,2,\ldots,100\}$. Determine the probability that $X$ is a subset of $Y$.","I need some help on this one: We have sets $X$ and $Y$ chosen independently and uniformly at random from among all subsets of $\{1,2,\ldots,100\}$. Determine the probability that $X$ is a subset of $Y$.",,['probability']
54,"A bag contains 3 red, 4 blue, and 5 green balls. [closed]","A bag contains 3 red, 4 blue, and 5 green balls. [closed]",,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Peter draws a ball from the bag, and then Angelina draws a ball. What is the probability that Angelina got a green ball? So far I have this: Scenario A: 1st ball is not green, 2nd green: 7/12 * 5/11 = 35/132 Scenario B: 1st ball is green, 2nd green: 5/12 * 4/11 = 20/132 --> $$\frac{55}{132} = \frac{5}{12}$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Peter draws a ball from the bag, and then Angelina draws a ball. What is the probability that Angelina got a green ball? So far I have this: Scenario A: 1st ball is not green, 2nd green: 7/12 * 5/11 = 35/132 Scenario B: 1st ball is green, 2nd green: 5/12 * 4/11 = 20/132 --> $$\frac{55}{132} = \frac{5}{12}$$",,"['probability', 'combinatorics']"
55,What does $\sim$ mean in math and statistics?,What does  mean in math and statistics?,\sim,"For example in the following picture above, the red sentence means what exactly with this $\sim$ symbol?","For example in the following picture above, the red sentence means what exactly with this $\sim$ symbol?",,"['probability', 'statistics', 'notation']"
56,counting probability with multiple cases,counting probability with multiple cases,,"There are four different colors of paint one can use for four different houses. If one color can be used up to three times, how many total possibilities are there? I approached the problem by splitting it into cases: 1) each color used once 2) one color used twice 3) two colors used twice 4) one color used thrice but i dont know how to move on from here or if im right","There are four different colors of paint one can use for four different houses. If one color can be used up to three times, how many total possibilities are there? I approached the problem by splitting it into cases: 1) each color used once 2) one color used twice 3) two colors used twice 4) one color used thrice but i dont know how to move on from here or if im right",,"['probability', 'combinatorics', 'probability-distributions']"
57,Finding $E\left[\frac{\sum_{i=1}^n X_i^2}{(\sum_{i=1}^n X_i)^2}\right]$ of a sample of gamma random variables,Finding  of a sample of gamma random variables,E\left[\frac{\sum_{i=1}^n X_i^2}{(\sum_{i=1}^n X_i)^2}\right],"Suppose $X_1,\ldots,X_n$ is a random sample from the $\Gamma(k,\lambda)$ distribution where $\lambda$ is unknown and $k$ is a positive integer and known. How can I find $$E\left[\frac{\sum_{i=1}^n X_i^2}{(\sum_{i=1}^n X_i)^2}\right] \>?$$","Suppose $X_1,\ldots,X_n$ is a random sample from the $\Gamma(k,\lambda)$ distribution where $\lambda$ is unknown and $k$ is a positive integer and known. How can I find $$E\left[\frac{\sum_{i=1}^n X_i^2}{(\sum_{i=1}^n X_i)^2}\right] \>?$$",,"['probability', 'statistics']"
58,What percentage of a population that has survived one standard deviation will survive two?,What percentage of a population that has survived one standard deviation will survive two?,,"This is a problem I had in school that I think I remember how to solve except my son does not agree with me. Given a population with a life expectancy that fits a normal distribution, mean of 75 and a standard deviation of 5.  What percentage of the people that live to age 80 will live to age 85? As soon as I'll figure out how to produce suitable graphics to answer the question but I'll post it now in case anybody can come up with a good explanation.","This is a problem I had in school that I think I remember how to solve except my son does not agree with me. Given a population with a life expectancy that fits a normal distribution, mean of 75 and a standard deviation of 5.  What percentage of the people that live to age 80 will live to age 85? As soon as I'll figure out how to produce suitable graphics to answer the question but I'll post it now in case anybody can come up with a good explanation.",,"['probability', 'statistics']"
59,"Given enough time, what are the chances I can come out ahead in a coin toss contest?","Given enough time, what are the chances I can come out ahead in a coin toss contest?",,"Assuming I can play forever, what are my chances of coming out ahead in a coin flipping series? Let's say I want ""heads""...then if I flip once, and get heads, then I win, because I've reached a point where I have more heads than tails (1-0).  If it was tails, I can flip again.  If I'm lucky, and I get two heads in a row after this, this is another way for me to win (2-1). Obviously, if I can play forever, my chances are probably pretty decent.  They are at least greater than 50%, since I can get that from the first flip.  After that, though, it starts getting sticky. I've drawn a tree graph to try to get to the point where I could start see the formula hopefully dropping out, but so far it's eluding me. Your chances of coming out ahead after 1 flip are 50%.  Fine.  Assuming you don't win, you have to flip at least twice more.  This step gives you 1 chance out of 4.  The next level would be after 5 flips, where you have an addtional 2 chances out of 12, followed by 7 flips, giving you 4 out of 40. I suspect I may be able to work through this given some time, but I'd like to see what other people think...is there an easy way to approach this?  Is this a known problem?","Assuming I can play forever, what are my chances of coming out ahead in a coin flipping series? Let's say I want ""heads""...then if I flip once, and get heads, then I win, because I've reached a point where I have more heads than tails (1-0).  If it was tails, I can flip again.  If I'm lucky, and I get two heads in a row after this, this is another way for me to win (2-1). Obviously, if I can play forever, my chances are probably pretty decent.  They are at least greater than 50%, since I can get that from the first flip.  After that, though, it starts getting sticky. I've drawn a tree graph to try to get to the point where I could start see the formula hopefully dropping out, but so far it's eluding me. Your chances of coming out ahead after 1 flip are 50%.  Fine.  Assuming you don't win, you have to flip at least twice more.  This step gives you 1 chance out of 4.  The next level would be after 5 flips, where you have an addtional 2 chances out of 12, followed by 7 flips, giving you 4 out of 40. I suspect I may be able to work through this given some time, but I'd like to see what other people think...is there an easy way to approach this?  Is this a known problem?",,['probability']
60,Understanding Black-Scholes,Understanding Black-Scholes,,"Assume I have only basic math knowledge, what specific areas of math would I need to learn in order to understand the following webpage: Black-Scholes Many thanks.","Assume I have only basic math knowledge, what specific areas of math would I need to learn in order to understand the following webpage: Black-Scholes Many thanks.",,"['probability', 'applications', 'stochastic-processes', 'finance']"
61,Probability of three dice falling in the same quadrant of a box,Probability of three dice falling in the same quadrant of a box,,"This is surely really basic for most people here but it's tripping me up. You get a box and draw lines to split it up into 4 parts.  I got asked what the probability was that when rolling three dice, all three dices would end up in the same quadrant. My first take on this was a 1/4 chance of die 1 in quadrant x a 1/4 chance of die 2 in same quadrant x a 1/4 chance of die 3 in same quadrant x => 1/4*1/4*1/4 = 1/64 chance My second take on this was that the first die doesn't matter at all so all that's left is a 1/4 chance of die 2 in same quadrant a 1/4 chance of die 3 in same quadrant => 1/4*1/4 = 1/16 chance But I have been given a solution where all possible combinations are drawn out and as there are 20 possible combinations, the odds are 1/20. What is correct (if any) and why ?","This is surely really basic for most people here but it's tripping me up. You get a box and draw lines to split it up into 4 parts.  I got asked what the probability was that when rolling three dice, all three dices would end up in the same quadrant. My first take on this was a 1/4 chance of die 1 in quadrant x a 1/4 chance of die 2 in same quadrant x a 1/4 chance of die 3 in same quadrant x => 1/4*1/4*1/4 = 1/64 chance My second take on this was that the first die doesn't matter at all so all that's left is a 1/4 chance of die 2 in same quadrant a 1/4 chance of die 3 in same quadrant => 1/4*1/4 = 1/16 chance But I have been given a solution where all possible combinations are drawn out and as there are 20 possible combinations, the odds are 1/20. What is correct (if any) and why ?",,"['probability', 'combinatorics', 'dice']"
62,Probability of obtaining triangle when choosing $3$ points from $3\times3$ array,Probability of obtaining triangle when choosing  points from  array,3 3\times3,"$9$ points are placed in a $3\times3$ array.  If $3$ points are randomly selected, what is the probability that they are the vertices of a triangle?","$9$ points are placed in a $3\times3$ array.  If $3$ points are randomly selected, what is the probability that they are the vertices of a triangle?",,['probability']
63,Probability puzzle about arrival time,Probability puzzle about arrival time,,"I recently came up with an interesting probability puzzle and wondered what would be the best way to solve it. It goes as follows: Suppose your friend is to arrive sometime on Monday, with a uniform   distribution over the full 24 hour day. Whatever time they arrive   dictates how long they will stay at your place. For example: If your friend arrives exactly at midnight (00:00) on Monday, they won't stay at all, and simply say ""hello"" while passing by. If they arrive at 1:30am (01:30) they will stay for an hour and a half, until 3:00am (03:00). If they arrive at 2:00pm (14:00) they will stay for 14 hours, until 4:00am the next day, Tuesday. If $t$ is a moment in time sometime between the beginning of Monday   and the end of Tuesday, what is the probability your friend is at your   house at time $t$? Stated a little more formally, if $a \in [0, 86400)$ is the arrival time (in seconds) of your friend, then the times they will be staying at your house will be given by $[a,2a)$. So the question becomes: Given $t\in[0, 172800)$, what is the probability that your friend is   at your house at time $t$? Naturally I am interested in the method for arriving at a solution more so than the actual solution itself. I would also like to know how to approach the problem if the distribution on $[0,86400)$ was not uniform, but instead was given by some specific probability density function. Thanks, in advance.","I recently came up with an interesting probability puzzle and wondered what would be the best way to solve it. It goes as follows: Suppose your friend is to arrive sometime on Monday, with a uniform   distribution over the full 24 hour day. Whatever time they arrive   dictates how long they will stay at your place. For example: If your friend arrives exactly at midnight (00:00) on Monday, they won't stay at all, and simply say ""hello"" while passing by. If they arrive at 1:30am (01:30) they will stay for an hour and a half, until 3:00am (03:00). If they arrive at 2:00pm (14:00) they will stay for 14 hours, until 4:00am the next day, Tuesday. If $t$ is a moment in time sometime between the beginning of Monday   and the end of Tuesday, what is the probability your friend is at your   house at time $t$? Stated a little more formally, if $a \in [0, 86400)$ is the arrival time (in seconds) of your friend, then the times they will be staying at your house will be given by $[a,2a)$. So the question becomes: Given $t\in[0, 172800)$, what is the probability that your friend is   at your house at time $t$? Naturally I am interested in the method for arriving at a solution more so than the actual solution itself. I would also like to know how to approach the problem if the distribution on $[0,86400)$ was not uniform, but instead was given by some specific probability density function. Thanks, in advance.",,"['probability', 'probability-distributions', 'recreational-mathematics']"
64,Application of the pigeon hole principle,Application of the pigeon hole principle,,"There's a problem in my textbook that I'm having difficulties understanding. The solution given skips steps and is hard to decipher. The problem is as follows: ""During a month with 30 days, a baseball team plays at least one game a day, but no more than 45 games. Show that there must be a period of some number of consecutive days during which the team must play exactly 14 games."" I don't know how to arrive at the conclusion of 14 games. Anyone who can help me understand this application of the pigeon-hole principle would be greatly appreciated, thank you.","There's a problem in my textbook that I'm having difficulties understanding. The solution given skips steps and is hard to decipher. The problem is as follows: ""During a month with 30 days, a baseball team plays at least one game a day, but no more than 45 games. Show that there must be a period of some number of consecutive days during which the team must play exactly 14 games."" I don't know how to arrive at the conclusion of 14 games. Anyone who can help me understand this application of the pigeon-hole principle would be greatly appreciated, thank you.",,"['probability', 'discrete-mathematics', 'pigeonhole-principle']"
65,Probability - two balls in the box: one we don't know its color and the other is red. What's the probability it's white?,Probability - two balls in the box: one we don't know its color and the other is red. What's the probability it's white?,,"Bob has a black box (you can't see what's inside the box). A long time ago Bob put one ball into the box but he doesn't remember what color the ball was. With equal probability it can be a white ball or a red ball. A. Bob takes a red ball and puts it in the same box.  Now there are two balls in the box: one ball Red that Bob just put in and a ball that was in the box earlier (Bob doesn't remember its color). Now Bob draws Randomly one ball out of the box and it turned out to be a red ball. Calculate the probability that the ball that has been in the box for a long time is a white ball given the action taken by Bob. My attempt: There are two options, since we already know of them is red, $A_1= \{\text{White, Red}\}$ or $A_2= \{\text{Red, Red}\}$ , so $\Pr[A_1 \cup A_2] = 1/2$ ?","Bob has a black box (you can't see what's inside the box). A long time ago Bob put one ball into the box but he doesn't remember what color the ball was. With equal probability it can be a white ball or a red ball. A. Bob takes a red ball and puts it in the same box.  Now there are two balls in the box: one ball Red that Bob just put in and a ball that was in the box earlier (Bob doesn't remember its color). Now Bob draws Randomly one ball out of the box and it turned out to be a red ball. Calculate the probability that the ball that has been in the box for a long time is a white ball given the action taken by Bob. My attempt: There are two options, since we already know of them is red, or , so ?","A_1= \{\text{White, Red}\} A_2= \{\text{Red, Red}\} \Pr[A_1 \cup A_2] = 1/2",['probability']
66,Probability of being dealt four-of-a-kind in a set of $5$ cards?,Probability of being dealt four-of-a-kind in a set of  cards?,5,"You are dealt a hand of five cards from a standard deck of playing cards. Find the probability of being dealt a hand consisting of four-of-a-kind. If possible, please provide a hint first before the answer. One of the first things that came to me was $\dfrac{52}{52} \cdot \dfrac{51}{51} \cdot \dfrac{3}{50} \cdot \dfrac{2}{49} \cdot \dfrac{1}{48}$ but this was of course wrong. Then, I realized that the third card could be the same as the first OR second card, so I tried $\dfrac{52}{52} \cdot \dfrac{51}{51} \cdot \dfrac{7}{50} \cdot \dfrac{2}{49} \cdot \dfrac{1}{48}$, which is also wrong. Then I realized that we probably need to add up the probabilities of situations where the first card is the one that doesn't match, or the second is the one that doesn't match, etc. I think this method leads to the solution, but I don't think we're intended to solve it this way. I think the solution looks something like $\dfrac{x}{ {{52}\choose{5}}}$, but I'm not sure what should be in the numerator.","You are dealt a hand of five cards from a standard deck of playing cards. Find the probability of being dealt a hand consisting of four-of-a-kind. If possible, please provide a hint first before the answer. One of the first things that came to me was $\dfrac{52}{52} \cdot \dfrac{51}{51} \cdot \dfrac{3}{50} \cdot \dfrac{2}{49} \cdot \dfrac{1}{48}$ but this was of course wrong. Then, I realized that the third card could be the same as the first OR second card, so I tried $\dfrac{52}{52} \cdot \dfrac{51}{51} \cdot \dfrac{7}{50} \cdot \dfrac{2}{49} \cdot \dfrac{1}{48}$, which is also wrong. Then I realized that we probably need to add up the probabilities of situations where the first card is the one that doesn't match, or the second is the one that doesn't match, etc. I think this method leads to the solution, but I don't think we're intended to solve it this way. I think the solution looks something like $\dfrac{x}{ {{52}\choose{5}}}$, but I'm not sure what should be in the numerator.",,"['probability', 'combinatorics']"
67,"Simple example of ""Maximum A Posteriori""","Simple example of ""Maximum A Posteriori""",,"I've been immersing myself into Bayesian statistics in school and I'm having a very difficult time grasping argmax and maximum a posteriori .  A quick explanation of this can be found: https://www.cs.utah.edu/~suyash/Dissertation_html/node8.html Basically theta is a set of parameters and x is the data, P of theta given x (posterior) equals P of x given theta (likelihood term) multiplied by P of theta (prior term) and all of that divided by P of x (to normalize).  Not sure exactly how dividing by P(x) normalizes this but thats not my main question. Then, you maximize the posterior with argmax I believe your maximizing the set of parameters to get the most likely posterior ... Can someone please give a simple example of this so I can visualize what is happening?","I've been immersing myself into Bayesian statistics in school and I'm having a very difficult time grasping argmax and maximum a posteriori .  A quick explanation of this can be found: https://www.cs.utah.edu/~suyash/Dissertation_html/node8.html Basically theta is a set of parameters and x is the data, P of theta given x (posterior) equals P of x given theta (likelihood term) multiplied by P of theta (prior term) and all of that divided by P of x (to normalize).  Not sure exactly how dividing by P(x) normalizes this but thats not my main question. Then, you maximize the posterior with argmax I believe your maximizing the set of parameters to get the most likely posterior ... Can someone please give a simple example of this so I can visualize what is happening?",,"['probability', 'probability-theory', 'statistics', 'bayesian', 'maximum-likelihood']"
68,Calculating the variance of the ratio of random variables,Calculating the variance of the ratio of random variables,,"I want to calculate $\newcommand{\var}{\mathrm{var}}\var(X/Y)$ . I know: $$\var(X - Y) = \var(X) + \var(Y) - 2 \var(X) \var(Y) \mathrm{corr}(X,Y) \>,$$ What is the equivalent for $\newcommand{\var}{\mathrm{var}}\var(X/Y)$ ?",I want to calculate . I know: What is the equivalent for ?,"\newcommand{\var}{\mathrm{var}}\var(X/Y) \var(X - Y) = \var(X) + \var(Y) - 2 \var(X) \var(Y) \mathrm{corr}(X,Y) \>, \newcommand{\var}{\mathrm{var}}\var(X/Y)","['probability', 'statistics', 'correlation']"
69,"Asking for an ""intuitive"" explanation of a probability problem [duplicate]","Asking for an ""intuitive"" explanation of a probability problem [duplicate]",,"This question already has answers here : Why are all subset sizes equiprobable if elements are independently included with probability uniform over $[0,1]$? (2 answers) Closed last year . The problem is as follows: We pick a real number p in $(0,1)$ randomly and uniformly, then construct a coin such that when tossed, $P(H) =p$ and $P(T) = 1- p$ . Now fix a positive integer $n$ , if we were to toss the coin $n$ times (independently), what is the probability that exactly $k$ ( $0 \leq k \leq n$ ) heads occur? I have done this by evaluating the integral $$\int_0^1{{n \choose k}p^k(1-p)^{n-k}}dp = \frac {1}{n+1}.$$ However, the evaluation was far from easy and I had to use the ""snake oil"" method mentioned in the book ""generatingfunctionology"" by Wilf. Yet this result is so simple and beautiful, in particular, it is very suprising to me that the probability is independent of $k$ . I tend to believe there must be a simpler reasoning for this, something I failed to notice that can draw the conclusion ""the probability is independdent of $k$ "". Can someone show me such a way(if there is one)?","This question already has answers here : Why are all subset sizes equiprobable if elements are independently included with probability uniform over $[0,1]$? (2 answers) Closed last year . The problem is as follows: We pick a real number p in randomly and uniformly, then construct a coin such that when tossed, and . Now fix a positive integer , if we were to toss the coin times (independently), what is the probability that exactly ( ) heads occur? I have done this by evaluating the integral However, the evaluation was far from easy and I had to use the ""snake oil"" method mentioned in the book ""generatingfunctionology"" by Wilf. Yet this result is so simple and beautiful, in particular, it is very suprising to me that the probability is independent of . I tend to believe there must be a simpler reasoning for this, something I failed to notice that can draw the conclusion ""the probability is independdent of "". Can someone show me such a way(if there is one)?","(0,1) P(H) =p P(T) = 1- p n n k 0 \leq k \leq n \int_0^1{{n \choose k}p^k(1-p)^{n-k}}dp = \frac {1}{n+1}. k k",['probability']
70,"Are the random variables $X + Y$ and $X - Y$ independent if $X, Y$ are distributed normal?",Are the random variables  and  independent if  are distributed normal?,"X + Y X - Y X, Y","Let $X, Y$ be independent random variables such that $X,Y \sim N(\mu,\sigma^2)$, show that $X+Y$ and $X-Y$ are independent using the moment generating function. I know that the moment generating function of a sum of independent random variables is the product of the MGF. So, I´m trying to solve that but i don´t know if my process is correct $M_{X+Y}(t_1,t_2)=M_X(t_1)M_Y(t_2)=M^2_{N(\mu,\sigma^2)}(t)$ ?","Let $X, Y$ be independent random variables such that $X,Y \sim N(\mu,\sigma^2)$, show that $X+Y$ and $X-Y$ are independent using the moment generating function. I know that the moment generating function of a sum of independent random variables is the product of the MGF. So, I´m trying to solve that but i don´t know if my process is correct $M_{X+Y}(t_1,t_2)=M_X(t_1)M_Y(t_2)=M^2_{N(\mu,\sigma^2)}(t)$ ?",,"['probability', 'statistics', 'moment-generating-functions']"
71,What's the expectation of square root of Chi-square variable?,What's the expectation of square root of Chi-square variable?,,"Setting $$X_1,X_2,\ldots \overset{d}{\sim} \mathcal{N}(0,1)$$ $$V = X_1^2 + \ldots + X_n^2$$ Now I would like to find the expectation of $\sqrt{V}$. My biggest problem is I don't know how to write down the density of $\sqrt{V}$. Could someone help me derive it?","Setting $$X_1,X_2,\ldots \overset{d}{\sim} \mathcal{N}(0,1)$$ $$V = X_1^2 + \ldots + X_n^2$$ Now I would like to find the expectation of $\sqrt{V}$. My biggest problem is I don't know how to write down the density of $\sqrt{V}$. Could someone help me derive it?",,"['probability', 'probability-distributions']"
72,What does it mean to say a random variable is non-negative?,What does it mean to say a random variable is non-negative?,,How would you define a random variable to be non-negative ??? What are some examples of a Negative random variable ???,How would you define a random variable to be non-negative ??? What are some examples of a Negative random variable ???,,"['probability', 'statistics', 'probability-theory']"
73,Coin tosses with unknown success probability,Coin tosses with unknown success probability,,"Suppose I have a coin with unknown probability of success (let say Heads) $p$ which is uniformly distributed on $[0, 1]$ . I toss a coin $N$ times. What is the probability that I have got $n \leq N$ Heads? Ok. I've calculated (using Wolfram) that it is exactly $\frac{1}{N+1}$ , i.e. every $n$ is equiprobable. What is the intuitive explanation for this fact? I can understand by symmetry argument, why getting $k$ Heads should be exactly the same as getting $k$ Tails. But why getting $0$ Heads is the same as getting $7$ Heads (in case $N \neq 7$ ) is well beyond me. Update: Calculations involved: $$P(\text{# of Heads} = k) = \int\limits_0^1 {N \choose k} p^k (1-p)^{N-k} dp, $$ solving it gives: $$P(\text{# of Heads} = k) = {N \choose k} \frac{\Gamma(k+1)\Gamma(N-k+1)}{\Gamma(N+2)},$$ which turns out to be $\frac{1}{N+1}$ .","Suppose I have a coin with unknown probability of success (let say Heads) which is uniformly distributed on . I toss a coin times. What is the probability that I have got Heads? Ok. I've calculated (using Wolfram) that it is exactly , i.e. every is equiprobable. What is the intuitive explanation for this fact? I can understand by symmetry argument, why getting Heads should be exactly the same as getting Tails. But why getting Heads is the same as getting Heads (in case ) is well beyond me. Update: Calculations involved: solving it gives: which turns out to be .","p [0, 1] N n \leq N \frac{1}{N+1} n k k 0 7 N \neq 7 P(\text{# of Heads} = k) = \int\limits_0^1 {N \choose k} p^k (1-p)^{N-k} dp,  P(\text{# of Heads} = k) = {N \choose k} \frac{\Gamma(k+1)\Gamma(N-k+1)}{\Gamma(N+2)}, \frac{1}{N+1}",['probability']
74,Rolling 3 dice and getting exactly 2 to be of the same number,Rolling 3 dice and getting exactly 2 to be of the same number,,"I am reading about rolling a six-sided die and I came up with this equation myself. I know this is wrong but couldn't really figure out why. Could someone please help me understand? The probability of rolling $3$ dice and getting $2$ to be the same number $$=\frac{|E|}{|S|}= \frac{6 \cdot 5 \cdot 3!}{6^3}$$ The first $6$ is for having $6$ different possible choices of our target(AAX results), $5$ is for $5$ possible choices for the non-pair number and $3!$ is the number of possible ways to order the three numbers we rolled (and I think this might be the incorrect part of my equation). Finally, $6^3$ is just all possible outcomes.","I am reading about rolling a six-sided die and I came up with this equation myself. I know this is wrong but couldn't really figure out why. Could someone please help me understand? The probability of rolling dice and getting to be the same number The first is for having different possible choices of our target(AAX results), is for possible choices for the non-pair number and is the number of possible ways to order the three numbers we rolled (and I think this might be the incorrect part of my equation). Finally, is just all possible outcomes.",3 2 =\frac{|E|}{|S|}= \frac{6 \cdot 5 \cdot 3!}{6^3} 6 6 5 5 3! 6^3,"['probability', 'combinatorics']"
75,Unordered sampling with replacement,Unordered sampling with replacement,,"I have difficulties with understanding of how do we count this. Why can't we just divide number of ways of ordered sample with replacement $n^k$ by $k!$, i.e. to get rid of permutations since we cannot distinguish elements? What so different about that in comparison to the case without replacement? I read the derivation based on counting ""walls"" between integers representing number of times particular ball was drawn. I do understand how to count number of combinations in this case but I do not understand how we get there, namely, why these numbers (number of combinations of placing ""walls"" and number of unordered samples with replacement) are the same?","I have difficulties with understanding of how do we count this. Why can't we just divide number of ways of ordered sample with replacement $n^k$ by $k!$, i.e. to get rid of permutations since we cannot distinguish elements? What so different about that in comparison to the case without replacement? I read the derivation based on counting ""walls"" between integers representing number of times particular ball was drawn. I do understand how to count number of combinations in this case but I do not understand how we get there, namely, why these numbers (number of combinations of placing ""walls"" and number of unordered samples with replacement) are the same?",,"['probability', 'combinatorics', 'probability-theory']"
76,Example to illustrate difference between support and domain,Example to illustrate difference between support and domain,,"Assume that X is a discrete random variable, I understand that the domain of X is the set of values it can take and the support of X is the set of values it takes with non-zero probability.  Can someone give me an example of where 1) $S \subset D$ 2) $S = D$","Assume that X is a discrete random variable, I understand that the domain of X is the set of values it can take and the support of X is the set of values it takes with non-zero probability.  Can someone give me an example of where 1) $S \subset D$ 2) $S = D$",,['probability']
77,"Probability of exactly $7$ unreplaced draws from $\{1,...,10\}$ to get $1,2,3$?",Probability of exactly  unreplaced draws from  to get ?,"7 \{1,...,10\} 1,2,3","A box contains tickets numbered $1$ to $10$ . Tickets are drawn at random without replacement until the numbers $1, 2$ and $3$ are drawn. Find the probability that exactly $7$ draws are required. I am a little confused with this question. My first thought was to define the numbers $1,2,$ and $3$ as successes and the rest of the numbers as failures. Then, $$P(\text{in $6$ draws you get $2$ successes}) = \frac{\binom{3}{2} \binom{7}{4}}{\binom{10}{6}}.$$ But I don't know where to go next or if this is even needed. Can anyone help me? Thanks.","A box contains tickets numbered to . Tickets are drawn at random without replacement until the numbers and are drawn. Find the probability that exactly draws are required. I am a little confused with this question. My first thought was to define the numbers and as successes and the rest of the numbers as failures. Then, But I don't know where to go next or if this is even needed. Can anyone help me? Thanks.","1 10 1, 2 3 7 1,2, 3 P(\text{in 6 draws you get 2 successes}) = \frac{\binom{3}{2} \binom{7}{4}}{\binom{10}{6}}.",['probability']
78,How to show P(A|A∪B) ≥ P(A|B)?,How to show P(A|A∪B) ≥ P(A|B)?,,"How would one show $P(A\,|\,A\cup B)\ge P(A\,|\,B)$ ? I first tried to expand the LHS using the definition of conditional probability and therefore it becomes: $\frac{P(A \cap (A \cup B))}{P(A \cup B)}$ Then I tried to use distribution law to further expand the numerator and get: $P((A \cap A) \cup (B \cap A))$ Since $A \cap A$ is just A and it becomes $P(A \cup (B \cap A))$ . However, if I use distribution law again, it looks like it will go back to the previous step and I'm just repeating doing the same thing. In the solution, it mentioned the total probability law, but I'm wondering how to link these two together and what should be the correct way to prove this.","How would one show ? I first tried to expand the LHS using the definition of conditional probability and therefore it becomes: Then I tried to use distribution law to further expand the numerator and get: Since is just A and it becomes . However, if I use distribution law again, it looks like it will go back to the previous step and I'm just repeating doing the same thing. In the solution, it mentioned the total probability law, but I'm wondering how to link these two together and what should be the correct way to prove this.","P(A\,|\,A\cup B)\ge P(A\,|\,B) \frac{P(A \cap (A \cup B))}{P(A \cup B)} P((A \cap A) \cup (B \cap A)) A \cap A P(A \cup (B \cap A))","['probability', 'statistics', 'conditional-probability']"
79,"After 6n roll of dice, what is the probability each face was rolled exactly n times?","After 6n roll of dice, what is the probability each face was rolled exactly n times?",,"This is closely related to the question "" If you toss an even number of coins, what is the probability of 50% head and 50% tail? "", but for dice with 6 possible results instead of coins (with 2 possible results). Actually I would like a more general approximation formula, for dice with m faces.","This is closely related to the question "" If you toss an even number of coins, what is the probability of 50% head and 50% tail? "", but for dice with 6 possible results instead of coins (with 2 possible results). Actually I would like a more general approximation formula, for dice with m faces.",,"['probability', 'probability-distributions', 'binomial-coefficients', 'approximation', 'factorial']"
80,Prove that if $A \subset B$ then $P(A) \leq P(B)$,Prove that if  then,A \subset B P(A) \leq P(B),"I'm supposed to prove that if $A \subset B$, then $P(A) \leq P(B)$. The hint it gives is confusing me even more. It says use a venn diagram to convince yourself $ B = A \cup (A^c \cap B)$ and $A$ and $A^c \cap B$ are disjoint. I know that I can do this: $A \cup (A^c \cap B) = (A \cup A^c) \cap (A \cup B)$ Since $(A \cup A^c) = U$ that means if it intersects with $A \cup B$ we just have a venn diagram with everything shaded in. How do I use this to my advantage to prove the original question? And wouldn't the two sets $A$ and $A^c \cap B$ being disjoint hurt me? Since I want to show that everything in $A$ is in $B$?","I'm supposed to prove that if $A \subset B$, then $P(A) \leq P(B)$. The hint it gives is confusing me even more. It says use a venn diagram to convince yourself $ B = A \cup (A^c \cap B)$ and $A$ and $A^c \cap B$ are disjoint. I know that I can do this: $A \cup (A^c \cap B) = (A \cup A^c) \cap (A \cup B)$ Since $(A \cup A^c) = U$ that means if it intersects with $A \cup B$ we just have a venn diagram with everything shaded in. How do I use this to my advantage to prove the original question? And wouldn't the two sets $A$ and $A^c \cap B$ being disjoint hurt me? Since I want to show that everything in $A$ is in $B$?",,"['probability', 'statistics']"
81,"Is the product of uniformly distributed numbers, uniformly distributed too?","Is the product of uniformly distributed numbers, uniformly distributed too?",,"My question is simple, I think. If we took two random natural numbers $a$ and $b$ uniformly distributed in a specific range $[c,d]$, is $ab$ a uniformly distributed too? What if $a$ and $b$ are not natural numbers, but real numbers? What if $a$ is a natural number and $b$ a real number?","My question is simple, I think. If we took two random natural numbers $a$ and $b$ uniformly distributed in a specific range $[c,d]$, is $ab$ a uniformly distributed too? What if $a$ and $b$ are not natural numbers, but real numbers? What if $a$ is a natural number and $b$ a real number?",,"['probability', 'elementary-number-theory', 'random', 'natural-numbers', 'real-numbers']"
82,How do you differentiate the likelihood function for the uniform distribution in finding the M.L.E.?,How do you differentiate the likelihood function for the uniform distribution in finding the M.L.E.?,,"There is a classic problem: Suppose that $X_1,\ldots,X_n$ form an i.i.d. sample from a uniform distribution on the interval $(0,\theta)$, where $\theta>0$ is unknown. I would like to find the MLE of $\theta$. The pdf of each observation will have the form: $$ f(x\mid\theta) =  \begin{cases} 1/\theta\quad&\text{for }\, 0\leq x\leq \theta\\ 0 &\text{otherwise}. \end{cases} $$ The likelihood function therefore has the form: $$ L(\theta) = \begin{cases} 1/\theta^n \quad&\text{for }\; 0\leq x_i \leq \theta\;\; \text{for all }i,\\ 0 &\text{otherwise}. \end{cases} $$ The general solution is usually that the MLE of theta must be a value of $\theta$ for which $\theta \geq x_i$ and which maximizes $1/\theta^n$ among all such values. The reasoning is that since $1/\theta^n$ is a decreasing function of $\theta$, the estimate will be the smallest possible value of $\theta$ such that $\theta\geq x_i$. Therefore, the mle of $\theta$, $\hat{\theta}$, is $\max(X_1,\ldots,X_n)$. Here, I do not understand why we cannot just differentiate the likelihood function with respect to theta and then set it equal to $0$? Thanks!","There is a classic problem: Suppose that $X_1,\ldots,X_n$ form an i.i.d. sample from a uniform distribution on the interval $(0,\theta)$, where $\theta>0$ is unknown. I would like to find the MLE of $\theta$. The pdf of each observation will have the form: $$ f(x\mid\theta) =  \begin{cases} 1/\theta\quad&\text{for }\, 0\leq x\leq \theta\\ 0 &\text{otherwise}. \end{cases} $$ The likelihood function therefore has the form: $$ L(\theta) = \begin{cases} 1/\theta^n \quad&\text{for }\; 0\leq x_i \leq \theta\;\; \text{for all }i,\\ 0 &\text{otherwise}. \end{cases} $$ The general solution is usually that the MLE of theta must be a value of $\theta$ for which $\theta \geq x_i$ and which maximizes $1/\theta^n$ among all such values. The reasoning is that since $1/\theta^n$ is a decreasing function of $\theta$, the estimate will be the smallest possible value of $\theta$ such that $\theta\geq x_i$. Therefore, the mle of $\theta$, $\hat{\theta}$, is $\max(X_1,\ldots,X_n)$. Here, I do not understand why we cannot just differentiate the likelihood function with respect to theta and then set it equal to $0$? Thanks!",,"['probability', 'statistics']"
83,Expectation of product of indicator functions,Expectation of product of indicator functions,,"I was asked this question and somehow I cannot relate the expectation with the given probability. If $A,B$ are events and $1_A$,$1_B$ are the indicator variables for these events, then how do I show that $$ E\big(1_{A}\times 1_{B}\big) = Pr\big(A\cap B\big). $$","I was asked this question and somehow I cannot relate the expectation with the given probability. If $A,B$ are events and $1_A$,$1_B$ are the indicator variables for these events, then how do I show that $$ E\big(1_{A}\times 1_{B}\big) = Pr\big(A\cap B\big). $$",,"['probability', 'expectation']"
84,"X,Y are independent exponentially distributed then what is the distribution of X/(X+Y)","X,Y are independent exponentially distributed then what is the distribution of X/(X+Y)",,"Been crushing my head with this exercise. I know how to get the distribution of a ratio of exponential variables and of the sum of them, but i can't piece everything together. The exercise goes as this: If X,Y are independent exponentially distributed with beta = 1 (parameter of the exponential distribution = 1) then what is the distribution of X/(X+Y) Any ideas? Thanks a lot.","Been crushing my head with this exercise. I know how to get the distribution of a ratio of exponential variables and of the sum of them, but i can't piece everything together. The exercise goes as this: If X,Y are independent exponentially distributed with beta = 1 (parameter of the exponential distribution = 1) then what is the distribution of X/(X+Y) Any ideas? Thanks a lot.",,"['probability', 'statistics', 'functions', 'probability-distributions']"
85,Variance of binomial distribution,Variance of binomial distribution,,"Why for $X\sim B(n,p)$ is $Var(X)=np(1-p)$? $Var(X)=\sum x_i^2 p_i -(\sum x_i p_i)^2=\sum_{r=0}^n r^2 \binom{n}{r}p^r(1-p)^{n-r}+( \sum_{r=0}^n r \binom{n}{r}p^r(1-p)^{n-r} )^2$ In my short-sightedness, I don't see any viable ways to derive the variance from this.","Why for $X\sim B(n,p)$ is $Var(X)=np(1-p)$? $Var(X)=\sum x_i^2 p_i -(\sum x_i p_i)^2=\sum_{r=0}^n r^2 \binom{n}{r}p^r(1-p)^{n-r}+( \sum_{r=0}^n r \binom{n}{r}p^r(1-p)^{n-r} )^2$ In my short-sightedness, I don't see any viable ways to derive the variance from this.",,"['probability', 'combinatorics', 'intuition']"
86,Sequence which converges in probability but neither almost surely nor in $L^p$,Sequence which converges in probability but neither almost surely nor in,L^p,"Is it possible to find a sequence of r.v.'s $\{X_n\}$ that converge in probability but not almost surely nor in $L^p$? Does anyone know of a single example that illustrates this point? I've been trying to think of one for ages and I've not come up with anything. I'm sure such a sequence must exist, but can't think what it could be.","Is it possible to find a sequence of r.v.'s $\{X_n\}$ that converge in probability but not almost surely nor in $L^p$? Does anyone know of a single example that illustrates this point? I've been trying to think of one for ages and I've not come up with anything. I'm sure such a sequence must exist, but can't think what it could be.",,"['probability', 'probability-theory', 'convergence-divergence']"
87,Expected number of coin streaks,Expected number of coin streaks,,"Source: Quant interview. You have a coin which comes heads with probability $1/5$ , and you toss it $700$ times. If there are multiple coins in a row (could be just 1), we call them/it a streak. For example, if we get HTHTT, we have 4 streaks: first H, second T, third H, and last two T. What is the expected number of streaks in $700$ tosses? Attempt : I interpreted the number of streaks as the number of switch between either H to T or T to H in the sequence + 1. The problem has a markov chain structure and I was able to obtain the following equations, given $S(n)$ number of strike in sequence with n coins: $S(n|H)=0.2S(n-1)+0.8(1+S(n-1))$ and $S(n|T)=0.8S(n-1)+0.2(1+S(n-1))$ This implies $S(n) = S(n|T)p(T) + S(n|H)p(H) = S(n-1) + 8/25$ and therefore $E[S(700)]=224$ . This solution matches some numerical simulations but always underestimates a bit (e.g. with $10^6$ iterations I get $224.64$ and also smaller $n$ tend to underestimate).","Source: Quant interview. You have a coin which comes heads with probability , and you toss it times. If there are multiple coins in a row (could be just 1), we call them/it a streak. For example, if we get HTHTT, we have 4 streaks: first H, second T, third H, and last two T. What is the expected number of streaks in tosses? Attempt : I interpreted the number of streaks as the number of switch between either H to T or T to H in the sequence + 1. The problem has a markov chain structure and I was able to obtain the following equations, given number of strike in sequence with n coins: and This implies and therefore . This solution matches some numerical simulations but always underestimates a bit (e.g. with iterations I get and also smaller tend to underestimate).",1/5 700 700 S(n) S(n|H)=0.2S(n-1)+0.8(1+S(n-1)) S(n|T)=0.8S(n-1)+0.2(1+S(n-1)) S(n) = S(n|T)p(T) + S(n|H)p(H) = S(n-1) + 8/25 E[S(700)]=224 10^6 224.64 n,"['probability', 'statistics', 'markov-chains']"
88,Distribution of the Maximum of a (infinite) Random Walk,Distribution of the Maximum of a (infinite) Random Walk,,"Let $S_0 = 0$ and define $S_n = \sum^n_{i = 1} X_i$ such that \begin{align*} \mathbb P(X_i = 1) &= p \\ \mathbb P(X_i = -1) &= 1 - p = q \end{align*} for $p < \frac{1}{2}$. Find the distribution of $Y = \max \{S_0, S_1, S_2, ...\}$. My attempt at a solution: One known result (and a nice application of path counting/the reflection principle) is that if $Y_n = \max \{S_0, S_1, ..., S_n\}$ then \begin{equation*} \mathbb P(Y_n \geq r, S_n = b) = \begin{cases}  \mathbb P(S_n = b) & b \geq r \\  \left(\frac{q}{p}\right)^{r - b} \mathbb P(S_n = 2r - b) & b < r \end{cases} \end{equation*} and so, for $r \geq 1$, we find \begin{align*} \mathbb P(Y_n \geq r) &= \mathbb P(S_n \geq r) + \sum^{r - 1}_{b = -\infty} \left(\frac{q}{p}\right)^{r-b} \mathbb P(S_n = 2r - b) \\ &= \mathbb P(S_n = r) + \sum^\infty_{c = r + 1} \left[1 + \left(\frac{q}{p}\right)^{c - r}\right] \mathbb P(S_n = c) \end{align*} However, this was for the maximum over a finite random walk $S_n = \sum^n_{i = 1} X_i$. In the present case we're interested in the maximum over all $n \in \mathbb N$, say $S_\infty$, and it's not immediately obvious to me how to solve such a case. Thank you for any input!","Let $S_0 = 0$ and define $S_n = \sum^n_{i = 1} X_i$ such that \begin{align*} \mathbb P(X_i = 1) &= p \\ \mathbb P(X_i = -1) &= 1 - p = q \end{align*} for $p < \frac{1}{2}$. Find the distribution of $Y = \max \{S_0, S_1, S_2, ...\}$. My attempt at a solution: One known result (and a nice application of path counting/the reflection principle) is that if $Y_n = \max \{S_0, S_1, ..., S_n\}$ then \begin{equation*} \mathbb P(Y_n \geq r, S_n = b) = \begin{cases}  \mathbb P(S_n = b) & b \geq r \\  \left(\frac{q}{p}\right)^{r - b} \mathbb P(S_n = 2r - b) & b < r \end{cases} \end{equation*} and so, for $r \geq 1$, we find \begin{align*} \mathbb P(Y_n \geq r) &= \mathbb P(S_n \geq r) + \sum^{r - 1}_{b = -\infty} \left(\frac{q}{p}\right)^{r-b} \mathbb P(S_n = 2r - b) \\ &= \mathbb P(S_n = r) + \sum^\infty_{c = r + 1} \left[1 + \left(\frac{q}{p}\right)^{c - r}\right] \mathbb P(S_n = c) \end{align*} However, this was for the maximum over a finite random walk $S_n = \sum^n_{i = 1} X_i$. In the present case we're interested in the maximum over all $n \in \mathbb N$, say $S_\infty$, and it's not immediately obvious to me how to solve such a case. Thank you for any input!",,"['probability', 'probability-theory', 'random-walk']"
89,Does $\mathbb{E}\left[X\right]=\infty$ imply $\mathbb{E}\left[X^{2}\right]=\infty$?,Does  imply ?,\mathbb{E}\left[X\right]=\infty \mathbb{E}\left[X^{2}\right]=\infty,I'm trying to prove that if $\mathbb{E}\left[X\right]=\infty$ then $\mathbb{E}\left[X^{2}\right]=\infty$ for every random variable $X$. I know that if $X(w)>1$ I'll get that $X^2(w)>X(w)$ so $\mathbb{E}\left[X^{2}\right]\ge\mathbb{E}\left[X\right]$ and if $X(w)\le1$ then $X^2(w)\le X(w)$  so $\mathbb{E}\left[X^{2}\right]=\sum\nolimits _{w\in\Omega}X^{2}\left(w\right)\cdot p(w)\le\sum\nolimits _{w\in\Omega}1\cdot p(w)=1$ But how do I prove it when some of the $w\in \Omega$ are larger than 1 and some are less ? Is this even the right way to prove this ?,I'm trying to prove that if $\mathbb{E}\left[X\right]=\infty$ then $\mathbb{E}\left[X^{2}\right]=\infty$ for every random variable $X$. I know that if $X(w)>1$ I'll get that $X^2(w)>X(w)$ so $\mathbb{E}\left[X^{2}\right]\ge\mathbb{E}\left[X\right]$ and if $X(w)\le1$ then $X^2(w)\le X(w)$  so $\mathbb{E}\left[X^{2}\right]=\sum\nolimits _{w\in\Omega}X^{2}\left(w\right)\cdot p(w)\le\sum\nolimits _{w\in\Omega}1\cdot p(w)=1$ But how do I prove it when some of the $w\in \Omega$ are larger than 1 and some are less ? Is this even the right way to prove this ?,,"['probability', 'random-variables', 'expectation']"
90,Weighted sum of two dice such that the result is a random integer between $0$ and $35$,Weighted sum of two dice such that the result is a random integer between  and,0 35,"The numbers shown by 2 dice are labelled $d$ and $e$. $A, B$ and $C$ are constants, giving a score $S=Ad + Be + C$. Find $A, B$ and $C$ such that the range of possible values for $S$ covers all integers from $0$ to $35$, with an equal probability of each score. [Oxford PAT exam, 2011] I have tried to find equations in terms of $A, B, C$ for different values of $S$.","The numbers shown by 2 dice are labelled $d$ and $e$. $A, B$ and $C$ are constants, giving a score $S=Ad + Be + C$. Find $A, B$ and $C$ such that the range of possible values for $S$ covers all integers from $0$ to $35$, with an equal probability of each score. [Oxford PAT exam, 2011] I have tried to find equations in terms of $A, B, C$ for different values of $S$.",,"['probability', 'dice']"
91,Two random variables from the same probability density function: how can they be different?,Two random variables from the same probability density function: how can they be different?,,"The definition of $X$ as a random variable according to Wiki is as follows: $Let (\Omega, \mathcal{F}, P)$ be a probability space and $(E, > \mathcal{E})$ a measurable space. Then an $(E, \mathcal{E})$-valued   random variable is a function $X\colon \Omega \to E$ which is   $(\mathcal{F}, \mathcal{E})$-measurable. The latter means that, for   every subset $B\in\mathcal{E}$, its preimage $X^{-1}(B)\in > \mathcal{F}$ where $X^{-1}(B) = \{\omega : X(\omega)\in B\}$. This   definition enables us to measure any subset B in the target space by   looking at its preimage, which by assumption is measurable. And for real-valued random variables: In this case the observation space is the real numbers. Recall,   $(\Omega, \mathcal{F}, P)$ is the probability space. For real   observation space, the function $X\colon \Omega \rightarrow > \mathbb{R}$ is a real-valued random variable if: $\{ \omega : X(\omega) \le r \} \in \mathcal{F} \qquad \forall r \in > \mathbb{R}$. Now in statistics and fields alike, they introduce random variables like $X \sim p(x)$ where $p(x)$ is a probability distribution. My question is if you say that $X\sim p(x)$ and $Y\sim p(x)$ how can these two represent two different random variables (like two different standard normal random variables) when they are sampled from the same $p(x)$, viz. how should you translate this to the formal measure theoretic definition that could differentiate between these two?","The definition of $X$ as a random variable according to Wiki is as follows: $Let (\Omega, \mathcal{F}, P)$ be a probability space and $(E, > \mathcal{E})$ a measurable space. Then an $(E, \mathcal{E})$-valued   random variable is a function $X\colon \Omega \to E$ which is   $(\mathcal{F}, \mathcal{E})$-measurable. The latter means that, for   every subset $B\in\mathcal{E}$, its preimage $X^{-1}(B)\in > \mathcal{F}$ where $X^{-1}(B) = \{\omega : X(\omega)\in B\}$. This   definition enables us to measure any subset B in the target space by   looking at its preimage, which by assumption is measurable. And for real-valued random variables: In this case the observation space is the real numbers. Recall,   $(\Omega, \mathcal{F}, P)$ is the probability space. For real   observation space, the function $X\colon \Omega \rightarrow > \mathbb{R}$ is a real-valued random variable if: $\{ \omega : X(\omega) \le r \} \in \mathcal{F} \qquad \forall r \in > \mathbb{R}$. Now in statistics and fields alike, they introduce random variables like $X \sim p(x)$ where $p(x)$ is a probability distribution. My question is if you say that $X\sim p(x)$ and $Y\sim p(x)$ how can these two represent two different random variables (like two different standard normal random variables) when they are sampled from the same $p(x)$, viz. how should you translate this to the formal measure theoretic definition that could differentiate between these two?",,"['probability', 'measure-theory', 'probability-theory', 'random-variables']"
92,Does convergence in probability implies convergence of the mean?,Does convergence in probability implies convergence of the mean?,,"Let $\{X_n\}_{n=1}^\infty$ be a sequence of random variables converging to a random variable $X$. Does this fact implies that $\lim\limits_{n\to \infty} EX_n = EX$? If not, is there any other sufficient condition?","Let $\{X_n\}_{n=1}^\infty$ be a sequence of random variables converging to a random variable $X$. Does this fact implies that $\lim\limits_{n\to \infty} EX_n = EX$? If not, is there any other sufficient condition?",,"['probability', 'convergence-divergence']"
93,"Expected number of (0,1) distributed continuous random variables required to sum upto 1","Expected number of (0,1) distributed continuous random variables required to sum upto 1",,"I define $X_i$ as a random variable that is uniformly distributed between (0,1). What is the expected number of such variables I require to make the sum go just higher than 1.  Thanks","I define $X_i$ as a random variable that is uniformly distributed between (0,1). What is the expected number of such variables I require to make the sum go just higher than 1.  Thanks",,['probability']
94,How to assign 3 rooms to 3 people with 1 coin randomly?,How to assign 3 rooms to 3 people with 1 coin randomly?,,We are 3 friends; just checked into an airbnb flat with 3 rooms. And to make things truly random we decided to use 1 coin to assign each person a room. What's the best way to do this? (Assuming coin flip is truly random.) (One way to define 'best' is minimum number of flips),We are 3 friends; just checked into an airbnb flat with 3 rooms. And to make things truly random we decided to use 1 coin to assign each person a room. What's the best way to do this? (Assuming coin flip is truly random.) (One way to define 'best' is minimum number of flips),,"['probability', 'combinatorics', 'random']"
95,Probability that polygon formed by n points on a circle contain the center of the circle?,Probability that polygon formed by n points on a circle contain the center of the circle?,,"I have seen similar questions being asked on this forum, but couldn't find this exact problem. So there are n points selected uniformly randomly on a circle. What is the probability that the polygon of these n points contains the center of the circle? Now, taking cue from a similar question of probability that all these n points lie within a semicircle, Suppose we mark the bottom most point of the circle as zero. From there on, we move to the right and find the first point, lets say point i, at a distance x along the circumference. Now, the probability that the next n-1 points lie within the arc length $(x, x+\frac12)$ is $P = { (\frac { 1 }{ 2 } ) }^{ n-1 }$, which is the probability that these n points lie within a semicircle. The probability that they don't lie in the same semicircle then becomes $1-P.$ Clearly, if these n points lie within a semicircle, their polygon doesn't contain the center of the circle. Next, the point i could be any of those n points. So we need to account for all the n possibilities being the first point. But, should the final probability be $1-nP$, or $n(1-P)$?","I have seen similar questions being asked on this forum, but couldn't find this exact problem. So there are n points selected uniformly randomly on a circle. What is the probability that the polygon of these n points contains the center of the circle? Now, taking cue from a similar question of probability that all these n points lie within a semicircle, Suppose we mark the bottom most point of the circle as zero. From there on, we move to the right and find the first point, lets say point i, at a distance x along the circumference. Now, the probability that the next n-1 points lie within the arc length $(x, x+\frac12)$ is $P = { (\frac { 1 }{ 2 } ) }^{ n-1 }$, which is the probability that these n points lie within a semicircle. The probability that they don't lie in the same semicircle then becomes $1-P.$ Clearly, if these n points lie within a semicircle, their polygon doesn't contain the center of the circle. Next, the point i could be any of those n points. So we need to account for all the n possibilities being the first point. But, should the final probability be $1-nP$, or $n(1-P)$?",,"['probability', 'geometric-probability']"
96,Find joint distribution of minimum and maximum of iid random variables,Find joint distribution of minimum and maximum of iid random variables,,"$(X_n)$ sequence of iid random variables with uniform distribution $U([0,1])$ . $m=\min(X_1,...X_n), M=\max(X_1,...X_n)$ . I want to find $f_{m,M}(s,t)$ . $$ \begin{split} P(m<s,M<t)  &= P(m<s)P(M<t)1_{m\ne M}+P(X_1<\min(s,t))1_{m=M} \\  &= (1-(1-s))^nt^n1_{m \ne M}+\min(s,t)1_{m=M} \\  &=((st)^n+s)1_{s<t}+((st)^n+t)1_{s \ge t} \end{split} $$ When I differentiate it, I get $f_{m,M}(s,t)=n^2t^{n-1}s^{n-1}$ . Is this okay? And does it mean that $M$ and $m$ are independent and $f_{m,M}(s,t)=f_m(s)f_M(t)$ ?","sequence of iid random variables with uniform distribution . . I want to find . When I differentiate it, I get . Is this okay? And does it mean that and are independent and ?","(X_n) U([0,1]) m=\min(X_1,...X_n), M=\max(X_1,...X_n) f_{m,M}(s,t) 
\begin{split}
P(m<s,M<t)
 &= P(m<s)P(M<t)1_{m\ne M}+P(X_1<\min(s,t))1_{m=M} \\
 &= (1-(1-s))^nt^n1_{m \ne M}+\min(s,t)1_{m=M} \\
 &=((st)^n+s)1_{s<t}+((st)^n+t)1_{s \ge t}
\end{split}
 f_{m,M}(s,t)=n^2t^{n-1}s^{n-1} M m f_{m,M}(s,t)=f_m(s)f_M(t)","['probability', 'probability-distributions', 'uniform-distribution']"
97,Is there a quick way to justify that this elementary probability is equal to $\frac23$?,Is there a quick way to justify that this elementary probability is equal to ?,\frac23,"I just solved this problem with the conditional probability formula and after a while the answer was surprisingly $\frac23$. I believe there must be a tricky short way to calculate it. Can somebody help me? There are $n$ urns of which the $r$th contains $r-1$ red balls and $n-r$ magenta balls.  You pick an urn at random and remove two balls at random without replacement.  Find the probability that: the second ball is magenta, given that the first is magenta.","I just solved this problem with the conditional probability formula and after a while the answer was surprisingly $\frac23$. I believe there must be a tricky short way to calculate it. Can somebody help me? There are $n$ urns of which the $r$th contains $r-1$ red balls and $n-r$ magenta balls.  You pick an urn at random and remove two balls at random without replacement.  Find the probability that: the second ball is magenta, given that the first is magenta.",,"['probability', 'combinatorics', 'discrete-mathematics']"
98,Integral of Wiener Squared process,Integral of Wiener Squared process,,"I don't have a background of stochastic calculus. It is known fact that definite integral of standard Wiener process from $0$ to $t$ results in another Gaussian process with slice distribution that is normal distributed with mean equal to $0$ and variance $\frac{T^3}{3}$ i-e $$ \int_0^{t} W_s ds \sim \mathcal{N}(0,\frac{t^3}{3})    $$ Question: What if we square the standard Wiener process and then integrate i-e $$ \int_0^{t} W_s^2 ds \sim ?    $$ Would that be scaled Chi-square distributed ?","I don't have a background of stochastic calculus. It is known fact that definite integral of standard Wiener process from $0$ to $t$ results in another Gaussian process with slice distribution that is normal distributed with mean equal to $0$ and variance $\frac{T^3}{3}$ i-e $$ \int_0^{t} W_s ds \sim \mathcal{N}(0,\frac{t^3}{3})    $$ Question: What if we square the standard Wiener process and then integrate i-e $$ \int_0^{t} W_s^2 ds \sim ?    $$ Would that be scaled Chi-square distributed ?",,"['probability', 'stochastic-processes', 'brownian-motion', 'stochastic-integrals']"
99,What is the expected value of the number of circles formed?,What is the expected value of the number of circles formed?,,"There are $99$ identical square tiles, each with a quarter-circle drawn on it. When the tiles are randomly arranged in a $9$ by $11$ rectangle, what is the expected value of the number of full circles formed?","There are $99$ identical square tiles, each with a quarter-circle drawn on it. When the tiles are randomly arranged in a $9$ by $11$ rectangle, what is the expected value of the number of full circles formed?",,['probability']
