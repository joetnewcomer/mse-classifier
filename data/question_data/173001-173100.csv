,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Convolution of normal probability distribution and Gaussian filter,Convolution of normal probability distribution and Gaussian filter,,"I'm interested in generating a random scalar field according to a probability density function with a correlation length. This might correspond to say a spatial distribution of strength of a material or a temporal distribution of a signal. My plan for doing so is to generate normally-distributed random numbers $R$ at each sampling point $x_i$ and introduce the spatial correlation by convolving with a Gaussian filter. Specifically, I perform this in the frequency domain by multiplying the Fourier transforms element-wise and then performing the inverse transform to reconstruct the filtered variable. I'm using numpy 's multi-dimensional Fourier transforms to do the calculation ( numpy.fft.fftn , numpy.fft.ifftn ). $$ R(x_i) \sim N(\mu=0,\sigma^2=1) $$ $$ f(x_i) = \exp \left( \frac{-x_i^2}{\ell^2/2}\right) $$ $$ \tilde{R} = \alpha \cdot IFFT_n (FFT_n(R) \cdots FFT_n(f)) $$ Where I am not sure is about how to scale the filtered random field $\tilde{R}$ such that it is still normally distributed ($\sigma^2(\tilde{R})=1$). The mean is zero, but the variance is generally not one and nonlinearly varies with the correlation length $\ell$. What scaling factor $\alpha$  should I use, and how is this derived? I'm interested primarily in three-dimensional fields ($n=3$, $x_i \in R^3$).","I'm interested in generating a random scalar field according to a probability density function with a correlation length. This might correspond to say a spatial distribution of strength of a material or a temporal distribution of a signal. My plan for doing so is to generate normally-distributed random numbers $R$ at each sampling point $x_i$ and introduce the spatial correlation by convolving with a Gaussian filter. Specifically, I perform this in the frequency domain by multiplying the Fourier transforms element-wise and then performing the inverse transform to reconstruct the filtered variable. I'm using numpy 's multi-dimensional Fourier transforms to do the calculation ( numpy.fft.fftn , numpy.fft.ifftn ). $$ R(x_i) \sim N(\mu=0,\sigma^2=1) $$ $$ f(x_i) = \exp \left( \frac{-x_i^2}{\ell^2/2}\right) $$ $$ \tilde{R} = \alpha \cdot IFFT_n (FFT_n(R) \cdots FFT_n(f)) $$ Where I am not sure is about how to scale the filtered random field $\tilde{R}$ such that it is still normally distributed ($\sigma^2(\tilde{R})=1$). The mean is zero, but the variance is generally not one and nonlinearly varies with the correlation length $\ell$. What scaling factor $\alpha$  should I use, and how is this derived? I'm interested primarily in three-dimensional fields ($n=3$, $x_i \in R^3$).",,"['statistics', 'normal-distribution', 'convolution']"
1,Statistical Analysis 1: What is the probablity that this sample proportion is less than the population proportion by 0.06 or more?,Statistical Analysis 1: What is the probablity that this sample proportion is less than the population proportion by 0.06 or more?,,"I have not gotten this wording before in regards to this question: ""Mong Corporation makes auto batteries. The company claims that 80% of its LL70 batteries are good for 70 months or longer. Assume that this claim is true. Let $\hat{p}$ be the proportion in a sample of 100 such batteries that are good for 70 months or longer."" Now I cannot figure out whether it is asking: $p(\hat{p} < 0.06)$ $p(\hat{p} < .80 - 0.06) = p(\hat{p} < .74)$","I have not gotten this wording before in regards to this question: ""Mong Corporation makes auto batteries. The company claims that 80% of its LL70 batteries are good for 70 months or longer. Assume that this claim is true. Let $\hat{p}$ be the proportion in a sample of 100 such batteries that are good for 70 months or longer."" Now I cannot figure out whether it is asking: $p(\hat{p} < 0.06)$ $p(\hat{p} < .80 - 0.06) = p(\hat{p} < .74)$",,"['probability', 'statistics']"
2,The number of parking tickets issued in a certain city...,The number of parking tickets issued in a certain city...,,"The number of parking tickets issued in a certain city on any given day has Poisson distribution with parameter $\mu = 50.$ Calculate the approximate probability that between $35$ and $80$ tickets are given on a day. I'm not sure how to approach this, but the wording in the problem gives me a hint that this is a normal distribution problem, that's all I know. Any help will be appreciated.","The number of parking tickets issued in a certain city on any given day has Poisson distribution with parameter $\mu = 50.$ Calculate the approximate probability that between $35$ and $80$ tickets are given on a day. I'm not sure how to approach this, but the wording in the problem gives me a hint that this is a normal distribution problem, that's all I know. Any help will be appreciated.",,"['statistics', 'normal-distribution']"
3,Variance of normal cdf?,Variance of normal cdf?,,"Suppose $Z \sim N(0,1)$, how to calculate $Var(\Phi(\frac{a - bZ}{c}))$ with $a,b,c >0$? I know how to solve $E[\Phi(\frac{a - bZ}{c})]$, but how to find $E[\Phi^2(\frac{a- bZ}{c})]$? Thanks! Here is my attempt to solve $E[\Phi(\frac{a-bZ}{c})]$: $E[\Phi(\frac{a-bZ}{c})] = \int_{-\infty}^{\infty} P(X \le \frac{a-bz}{c})\phi(z)dz \\ = \int_{-\infty}^{\infty}P(X \le \frac{a- bZ}{c}| Z = z) \phi(z)dz \\ = P(X \le \frac{a-bZ}{c}) \\ = P(bZ + cX \le a) \\ = P(N(0,c^2+b^2) \le a) \\ = \Phi(\frac{a}{\sqrt{c^2 + b^2}})$","Suppose $Z \sim N(0,1)$, how to calculate $Var(\Phi(\frac{a - bZ}{c}))$ with $a,b,c >0$? I know how to solve $E[\Phi(\frac{a - bZ}{c})]$, but how to find $E[\Phi^2(\frac{a- bZ}{c})]$? Thanks! Here is my attempt to solve $E[\Phi(\frac{a-bZ}{c})]$: $E[\Phi(\frac{a-bZ}{c})] = \int_{-\infty}^{\infty} P(X \le \frac{a-bz}{c})\phi(z)dz \\ = \int_{-\infty}^{\infty}P(X \le \frac{a- bZ}{c}| Z = z) \phi(z)dz \\ = P(X \le \frac{a-bZ}{c}) \\ = P(bZ + cX \le a) \\ = P(N(0,c^2+b^2) \le a) \\ = \Phi(\frac{a}{\sqrt{c^2 + b^2}})$",,['probability']
4,Kolmogorov–Smirnov 2-sample test with large sample sizes always significant,Kolmogorov–Smirnov 2-sample test with large sample sizes always significant,,"I'm aware that the probability of a traditional statistical test such as student's t or mann-whitney u being deemed significant approaches 1.0 as sample size increases (i.e. >10,000) but I'm getting the same issue with a Kolmogorov–Smirnov 2-sample test which I'm having trouble understanding. Doesn't it always evaluate the difference between two sets of 100 cumulative probability values? I don't understand how sample size affects the result.","I'm aware that the probability of a traditional statistical test such as student's t or mann-whitney u being deemed significant approaches 1.0 as sample size increases (i.e. >10,000) but I'm getting the same issue with a Kolmogorov–Smirnov 2-sample test which I'm having trouble understanding. Doesn't it always evaluate the difference between two sets of 100 cumulative probability values? I don't understand how sample size affects the result.",,['statistics']
5,Variance of log Survival Odds,Variance of log Survival Odds,,I'm trying to undestand why $${Var}\left(\log\frac{\hat S(t)}{1-\hat S(t)}\right) \approx \frac{1}{\left(1-S(t)\right)^2}\sum_{i:Y_{(i)}\le t}\frac{d_i}{r_i(r_i-d_i)}$$ I know that by Greenwood formula: $${Var}\left({\hat S(t)}\right) = {\left(\hat S(t)\right)^2}\sum_{i:Y_{(i)}\le t}\frac{d_i}{r_i(r_i-d_i)}$$ So I know how to get to second part by I don't see why: $$\log\left(\frac{\hat S(t)}{1-\hat S(t)}\right) \approx \frac{1}{\left(1-S(t)\right)^2}$$,I'm trying to undestand why $${Var}\left(\log\frac{\hat S(t)}{1-\hat S(t)}\right) \approx \frac{1}{\left(1-S(t)\right)^2}\sum_{i:Y_{(i)}\le t}\frac{d_i}{r_i(r_i-d_i)}$$ I know that by Greenwood formula: $${Var}\left({\hat S(t)}\right) = {\left(\hat S(t)\right)^2}\sum_{i:Y_{(i)}\le t}\frac{d_i}{r_i(r_i-d_i)}$$ So I know how to get to second part by I don't see why: $$\log\left(\frac{\hat S(t)}{1-\hat S(t)}\right) \approx \frac{1}{\left(1-S(t)\right)^2}$$,,"['probability', 'statistics', 'self-learning']"
6,Explain conditional distribution given sufficient statistics.,Explain conditional distribution given sufficient statistics.,,"I understood proof of sufficient statistics by factorization theorem. But I can't solve fllowing two questions about conditional distribution. When $X_1, X_2, ..., X_n \sim N(0, \sigma^2)$ are i.i.d., expain conditional distribution of $X=(X_1, X_2, ..., X_n)$ given $T=\sum_{i=1}^{n}X_i^2$. When $X_1, X_2, ..., X_n \sim N(\mu, \sigma^2)$ are i.i.d., expain conditional distribution of $X=(X_1, X_2, ..., X_n)$ given $T=(\bar{X}, \sum_{i=1}^{n}(X_i-\bar{X})^2)$. I was given a hint of Q1, ""Consider uniformaly distribution on a hyper-sphere  $\sum x_i^2=t$.""","I understood proof of sufficient statistics by factorization theorem. But I can't solve fllowing two questions about conditional distribution. When $X_1, X_2, ..., X_n \sim N(0, \sigma^2)$ are i.i.d., expain conditional distribution of $X=(X_1, X_2, ..., X_n)$ given $T=\sum_{i=1}^{n}X_i^2$. When $X_1, X_2, ..., X_n \sim N(\mu, \sigma^2)$ are i.i.d., expain conditional distribution of $X=(X_1, X_2, ..., X_n)$ given $T=(\bar{X}, \sum_{i=1}^{n}(X_i-\bar{X})^2)$. I was given a hint of Q1, ""Consider uniformaly distribution on a hyper-sphere  $\sum x_i^2=t$.""",,['probability']
7,supervised version of TF-IDF [term frequency inverse document frequency],supervised version of TF-IDF [term frequency inverse document frequency],,"In classification tasks involving documents, the documents will commonly be preprocessed by doing things like getting counts / tf-idf weights for each term-document pair. I understand how tf-idf works but it seems like one shortcoming is that it doesn't directly look at how the tf-idf weightings affect classification ability. Instead it creates tf-idf weights and then as a separate step, you can use things like cosine similarity and train your classifier on the feature vectors produced this way, and then transofrm the test data in the same way [using same vocabulary]. So e.g. consider binary classification task (like spam / not spam). You can apply tf-idf in an unsupervised way, just getting tf-idf weights for each term in each document. Then you take this matrix of feature vectors and train your classifier on it. If a word occurs across many documents, it will be downweighted by the idf term because it is a ubiquitous word. But if you have labeled training data [documents each have category label], and you saw that almost all of the time the word occurred it was in one of the document classes, then actually this is a strong feature. So it seems like you could better build your vocabulary by directly seeing how it impacts classification ability. So my question is if there any techniques that turn tf-idf into a supervised version of tf-idf by directly taking advantage of the known document labels? Basically making TF-IDF a supervised approach?","In classification tasks involving documents, the documents will commonly be preprocessed by doing things like getting counts / tf-idf weights for each term-document pair. I understand how tf-idf works but it seems like one shortcoming is that it doesn't directly look at how the tf-idf weightings affect classification ability. Instead it creates tf-idf weights and then as a separate step, you can use things like cosine similarity and train your classifier on the feature vectors produced this way, and then transofrm the test data in the same way [using same vocabulary]. So e.g. consider binary classification task (like spam / not spam). You can apply tf-idf in an unsupervised way, just getting tf-idf weights for each term in each document. Then you take this matrix of feature vectors and train your classifier on it. If a word occurs across many documents, it will be downweighted by the idf term because it is a ubiquitous word. But if you have labeled training data [documents each have category label], and you saw that almost all of the time the word occurred it was in one of the document classes, then actually this is a strong feature. So it seems like you could better build your vocabulary by directly seeing how it impacts classification ability. So my question is if there any techniques that turn tf-idf into a supervised version of tf-idf by directly taking advantage of the known document labels? Basically making TF-IDF a supervised approach?",,"['statistics', 'machine-learning', 'clustering']"
8,Why the sample method of mixture distribution works?,Why the sample method of mixture distribution works?,,For example this thread: Generating random variables from a mixture of Normal distributions First choose a distribution according to the weights. Then sample from the chosen distribution. How to prove the correctness of this method?,For example this thread: Generating random variables from a mixture of Normal distributions First choose a distribution according to the weights. Then sample from the chosen distribution. How to prove the correctness of this method?,,"['probability', 'statistics', 'sampling', 'simulation']"
9,Normal distribution is not single-parameter exponential family,Normal distribution is not single-parameter exponential family,,"I want to show that the family of normal distributions is not a single-parameter exponential family, i.e. there aren't functions $h,g,\eta,T$ such that $$h(x)g(\mu,\sigma^2)\exp(\eta(\mu,\sigma^2) T(x)) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)$$ for all $x\in\mathbb R$ and $(\mu,\sigma^2)\in\mathbb R\times\mathbb R_{>0}$. I tried fixing one argument, in order to get some information about the other functions. For example $\mu=0$ looks much simpler but it didn't lead me anywhere. Can anybody give me a hint?","I want to show that the family of normal distributions is not a single-parameter exponential family, i.e. there aren't functions $h,g,\eta,T$ such that $$h(x)g(\mu,\sigma^2)\exp(\eta(\mu,\sigma^2) T(x)) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)$$ for all $x\in\mathbb R$ and $(\mu,\sigma^2)\in\mathbb R\times\mathbb R_{>0}$. I tried fixing one argument, in order to get some information about the other functions. For example $\mu=0$ looks much simpler but it didn't lead me anywhere. Can anybody give me a hint?",,"['probability-theory', 'statistics', 'probability-distributions']"
10,Compute/approximate percentile from percentiles of smaller time windows,Compute/approximate percentile from percentiles of smaller time windows,,"As the title says, I am interested in computing or approximating a percentile (e.g. 95%) for a large time interval (e.g., 1h) based on the percentiles of smaller time windows (30s). I am aware that the correct way would be to compute the percentile from scratch starting from the raw data. However, the data set is very large and would take way too much time (a 30s window can have 100k-100M data points). To simplify things, assume that the distribution for all the time intervals are the same but the constants might be somewhat different. For example, values in a window follow a Poisson distribution but the parameters of the distribution might vary somewhat between windows. To put it in more visual terms, the shape of all the distributions is the same but there might be a ""scaling"" factor that changes. I do not require a mathematical proof but some insight on what would be a reasonable way to proceed. Right now I am computing the median value of all the percentiles I am interested in. Could I improve on this?","As the title says, I am interested in computing or approximating a percentile (e.g. 95%) for a large time interval (e.g., 1h) based on the percentiles of smaller time windows (30s). I am aware that the correct way would be to compute the percentile from scratch starting from the raw data. However, the data set is very large and would take way too much time (a 30s window can have 100k-100M data points). To simplify things, assume that the distribution for all the time intervals are the same but the constants might be somewhat different. For example, values in a window follow a Poisson distribution but the parameters of the distribution might vary somewhat between windows. To put it in more visual terms, the shape of all the distributions is the same but there might be a ""scaling"" factor that changes. I do not require a mathematical proof but some insight on what would be a reasonable way to proceed. Right now I am computing the median value of all the percentiles I am interested in. Could I improve on this?",,"['statistics', 'percentile']"
11,Show that $\operatorname E\left(\frac{\widehat{E} \widehat{E}^T}{n-q}\right) = \Sigma$,Show that,\operatorname E\left(\frac{\widehat{E} \widehat{E}^T}{n-q}\right) = \Sigma,"We have $Y = X\beta+E,$ $X\in\mathbb R^{n\times q}$ and has rank $q,$ $n\gg q,$ $\beta\in\mathbb R^{q\times1},$ $E$ is a random vector taking values in $\mathbb R^n,$ $\operatorname{E}(E)=0\in\mathbb R^n,$ $\operatorname{var}(E) = \Sigma,$ so $\Sigma$ is a symmetric $n\times n$ real matrix. The variance $\Sigma$ is strictly positive-definite. As usual the hat matrix is the $n\times n$ matrix $H = X(X^T X)^{-1} X^T,$ of rank $q,$ which is the matrix of the orthogonal projection onto the column space of $X.$ And the vector of observable residuals is $\widehat E = (I-H)Y.$ The question is how to show that $$\operatorname E \left(\frac{\widehat{E} \widehat{E}^T}{n-q} \right) = \Sigma$$ My Process: I know the $\operatorname E(\frac{1}{n-q}) = \frac{1}{n-q}$ So I am left with $\operatorname E\left[\widehat{E} \widehat{E}^T \right]$ This simplifies to $\operatorname E[Y^T (I-H) Y]$ I don't believe I need to expand the hat-matrix $H$. What I want to do at this point is introduce $\Sigma$ somehow. The only thing I can think of is to introduce something like $\Sigma^{1/2} \Sigma^{-1/2}$ or $\Sigma \Sigma^{-1}$ but I'm not sure where or how to introduce this. EDIT: Sorry for lack of clarity. This is a MULTIVARIATE STATISTICS problem. I've been so focused on just this that I forgot that other math exists :'D $\Sigma$ is the Covariance Matrix. $n$ is the number of observations. $q$ independent variables. $\widehat{E}$ is the estimator for the error matrix.","We have $Y = X\beta+E,$ $X\in\mathbb R^{n\times q}$ and has rank $q,$ $n\gg q,$ $\beta\in\mathbb R^{q\times1},$ $E$ is a random vector taking values in $\mathbb R^n,$ $\operatorname{E}(E)=0\in\mathbb R^n,$ $\operatorname{var}(E) = \Sigma,$ so $\Sigma$ is a symmetric $n\times n$ real matrix. The variance $\Sigma$ is strictly positive-definite. As usual the hat matrix is the $n\times n$ matrix $H = X(X^T X)^{-1} X^T,$ of rank $q,$ which is the matrix of the orthogonal projection onto the column space of $X.$ And the vector of observable residuals is $\widehat E = (I-H)Y.$ The question is how to show that $$\operatorname E \left(\frac{\widehat{E} \widehat{E}^T}{n-q} \right) = \Sigma$$ My Process: I know the $\operatorname E(\frac{1}{n-q}) = \frac{1}{n-q}$ So I am left with $\operatorname E\left[\widehat{E} \widehat{E}^T \right]$ This simplifies to $\operatorname E[Y^T (I-H) Y]$ I don't believe I need to expand the hat-matrix $H$. What I want to do at this point is introduce $\Sigma$ somehow. The only thing I can think of is to introduce something like $\Sigma^{1/2} \Sigma^{-1/2}$ or $\Sigma \Sigma^{-1}$ but I'm not sure where or how to introduce this. EDIT: Sorry for lack of clarity. This is a MULTIVARIATE STATISTICS problem. I've been so focused on just this that I forgot that other math exists :'D $\Sigma$ is the Covariance Matrix. $n$ is the number of observations. $q$ independent variables. $\widehat{E}$ is the estimator for the error matrix.",,"['linear-algebra', 'statistics']"
12,Autocorrelation Function Properties,Autocorrelation Function Properties,,For what values of $k$ and $N$ can $f(x) = \frac{5-|k|}{A}$ be considered an autocorrelation function? $K$ is the number of autoregressive lags from $k = 0 \ldots \pm N$,For what values of $k$ and $N$ can $f(x) = \frac{5-|k|}{A}$ be considered an autocorrelation function? $K$ is the number of autoregressive lags from $k = 0 \ldots \pm N$,,"['statistics', 'stationary-processes']"
13,Evaluating the expected value of $\log(g(x))$.,Evaluating the expected value of .,\log(g(x)),"I am trying to evaluate the expected value of $\log(x+r)$ with respect to a negative binomial distribution.  In other words, I am searching for a closed form or approximation to the quantity \begin{equation} \sum_{x=0}^{\infty}\log{\left(x+r\right)}\frac{\Gamma(x+r)}{x!\Gamma(r)}{(1-p)^r}{p^x} \end{equation} where $p\in(0,1)$.  So far, I have tried to use the result in this post from 2012 , however this approximation does not seem that accurate when I compare it to the numerical computation of this quantity(truncating the sum at some large value), particularly for smaller values of $p$.  Any help would be appreciated, thanks for your time.","I am trying to evaluate the expected value of $\log(x+r)$ with respect to a negative binomial distribution.  In other words, I am searching for a closed form or approximation to the quantity \begin{equation} \sum_{x=0}^{\infty}\log{\left(x+r\right)}\frac{\Gamma(x+r)}{x!\Gamma(r)}{(1-p)^r}{p^x} \end{equation} where $p\in(0,1)$.  So far, I have tried to use the result in this post from 2012 , however this approximation does not seem that accurate when I compare it to the numerical computation of this quantity(truncating the sum at some large value), particularly for smaller values of $p$.  Any help would be appreciated, thanks for your time.",,"['probability', 'statistics', 'summation']"
14,Simple hypothesis testing problem Beta distribution,Simple hypothesis testing problem Beta distribution,,"Let $X$ be a random variable that is Beta distributed with parameters $\alpha=\beta=\theta>0$, so the given frequency function is $f(x|\theta)=\dfrac{\Gamma(2\theta)}{\Gamma(\theta)^2}x^{\theta-1}(1-x)^{\theta-1}1_{(0,1)}(x)$. Where $\Gamma(\theta)$ is the gamma function. I want to test $H_0:\theta=1$ against $H_1:\theta=2$. With the LR-test I found that you reject $H_0$ for large values of $X(1-X)$. Now I'm supposed to show that we should reject the null hypothesis for values of  $X$ within the interval $[1/2-c,1/2+c]$ for a certain $c$ (constant) that is later determined. Can someone help me out with this?","Let $X$ be a random variable that is Beta distributed with parameters $\alpha=\beta=\theta>0$, so the given frequency function is $f(x|\theta)=\dfrac{\Gamma(2\theta)}{\Gamma(\theta)^2}x^{\theta-1}(1-x)^{\theta-1}1_{(0,1)}(x)$. Where $\Gamma(\theta)$ is the gamma function. I want to test $H_0:\theta=1$ against $H_1:\theta=2$. With the LR-test I found that you reject $H_0$ for large values of $X(1-X)$. Now I'm supposed to show that we should reject the null hypothesis for values of  $X$ within the interval $[1/2-c,1/2+c]$ for a certain $c$ (constant) that is later determined. Can someone help me out with this?",,"['probability', 'statistics', 'hypothesis-testing']"
15,What is the relationship between open and connected sets and statistical independence?,What is the relationship between open and connected sets and statistical independence?,,"$\newcommand{\vol}{\operatorname{vol}}$Suppose that $U \sim \operatorname{Unif}(C)$, $0 < \vol(C) < \infty$ with $U = (X, Y)$ on $\mathbb{R}^p = \mathbb{R}^r \times \mathbb{R}^s $. Verify that if $C$ is open and connected then $X\mathrel{\perp\!\!\!\perp}Y \iff C = A \times B$ I've come up with a proof but as far as I can tell it doesn't matter if the sets are open and connected which makes me suspicious. Where is that necessary? $\Rightarrow$ $$X\mathrel{\perp\!\!\!\perp}Y \Rightarrow P(X \in E_1, Y \in E_2) = P(X \in E_1)P(Y \in E_2)$$ Since $U = (X, Y)$ $$P(X \in E_1, Y \in E_2) = P(U \in E_1 \times E_2)=P(X \in E_1)P(Y \in E_2)$$ where for clarity I assumed $E_1 \times E_2 \subset C$, but this could be relaxed. Then, because $U$ has a uniform distribution on $C$, $$P(U \in E_1 \times E_2)=\frac{\vol(E_1 \times E_2)}{\vol(C)} = \frac{\vol(E_1)}{\vol(A)}\frac{\vol(E_2)}{\vol(B)}$$ where $A$ and $B$ are the sets where $X$ and $Y$ are defined, respectively. Now turning this into densities we have that $$\frac{I(E_1 \times E_2)}{\vol(C)} = \frac{I(E_1)}{\vol(A)}\frac{I(E_2)}{\vol(B)}$$ $$\frac{I(E_1)I(E_2)}{\vol(C)} = \frac{I(E_1)}{\vol(A)}\frac{I(E_2)}{\vol(B)}$$ where $I$ is the indicator function. Since the numerators are the same this means that $\vol(C) = \vol(A)\vol(B)$. Applying Fubini's theorem, $$\int_Cdx = \int_Adx \int_Bdy = \int_{A \times B}d(x, y)$$ and so the set $C$ is $A \times B$. $\Leftarrow$ Since $C = A \times B$, $$\frac{I(E_1 \times E_2)}{\vol(C)} = \frac{I(E_1)I(E_2)}{\vol(A)\vol(B)} $$ hence $X$ and $Y$ are independent.","$\newcommand{\vol}{\operatorname{vol}}$Suppose that $U \sim \operatorname{Unif}(C)$, $0 < \vol(C) < \infty$ with $U = (X, Y)$ on $\mathbb{R}^p = \mathbb{R}^r \times \mathbb{R}^s $. Verify that if $C$ is open and connected then $X\mathrel{\perp\!\!\!\perp}Y \iff C = A \times B$ I've come up with a proof but as far as I can tell it doesn't matter if the sets are open and connected which makes me suspicious. Where is that necessary? $\Rightarrow$ $$X\mathrel{\perp\!\!\!\perp}Y \Rightarrow P(X \in E_1, Y \in E_2) = P(X \in E_1)P(Y \in E_2)$$ Since $U = (X, Y)$ $$P(X \in E_1, Y \in E_2) = P(U \in E_1 \times E_2)=P(X \in E_1)P(Y \in E_2)$$ where for clarity I assumed $E_1 \times E_2 \subset C$, but this could be relaxed. Then, because $U$ has a uniform distribution on $C$, $$P(U \in E_1 \times E_2)=\frac{\vol(E_1 \times E_2)}{\vol(C)} = \frac{\vol(E_1)}{\vol(A)}\frac{\vol(E_2)}{\vol(B)}$$ where $A$ and $B$ are the sets where $X$ and $Y$ are defined, respectively. Now turning this into densities we have that $$\frac{I(E_1 \times E_2)}{\vol(C)} = \frac{I(E_1)}{\vol(A)}\frac{I(E_2)}{\vol(B)}$$ $$\frac{I(E_1)I(E_2)}{\vol(C)} = \frac{I(E_1)}{\vol(A)}\frac{I(E_2)}{\vol(B)}$$ where $I$ is the indicator function. Since the numerators are the same this means that $\vol(C) = \vol(A)\vol(B)$. Applying Fubini's theorem, $$\int_Cdx = \int_Adx \int_Bdy = \int_{A \times B}d(x, y)$$ and so the set $C$ is $A \times B$. $\Leftarrow$ Since $C = A \times B$, $$\frac{I(E_1 \times E_2)}{\vol(C)} = \frac{I(E_1)I(E_2)}{\vol(A)\vol(B)} $$ hence $X$ and $Y$ are independent.",,"['real-analysis', 'probability', 'probability-theory', 'statistics', 'measure-theory']"
16,Approximation of region of rejection,Approximation of region of rejection,,"Let $X_1,...,X_n$ be a random sample of size $n$ from the beta distribution $B(\theta, 1)$ whose pdf is given by $f(x;\theta)=\theta x^{\theta-1}(0<x<1)$ and the hypotheses are given by: $H_0: \theta=1$   vs.   $H_1: \theta \neq 1$ Using likelihood ratio test with significance level $\alpha$ ($0<\alpha<1$), I ""already found"" that$-2\sum_{i=1}^n \log X_i \sim \chi^2 (2n)$ and the rejection region is given by: $$-2\sum_{i=1}^n \log X_i \le 2nc_1 \text{ or } -2\sum_{i=1}^n \log X_i \ge 2nc_2$$ where the constants $c_1, c_2$ satisfy $\int_{2nc_1}^{2nc_2}f_{2n}(x) \, dx = 1-\alpha$ and $c_1- \log{c_1} =c_2-\log{c_2} $, $f_{2n}$ is the pdf of $\chi^2(2n)$ distribution. Here is the problem: Show that the constants $c_1,c_2$ can be approximated as $c_1 \approx \dfrac{ \chi^2_{1-\alpha/2}(2n)}{2n},c_2 \approx \dfrac{ \chi^2_{\alpha/2}(2n)}{2n}$ when $n$ is sufficiently large. I attempted to prove that the two integrals $\int_0^{2nc_1}f_{2n}(x) \, dx$ and $\int_{2nc_2}^\infty f_{2n}(x) \, dx$ both converge to $\alpha/2$ when $n \to \infty$. Since the sum of the two integrals is $\alpha$, it suffices to show that the first integral goes to $\alpha/2$. Here is where I stuck. Does anyone have ideas? Any hints or advice will help a lot! Thanks.","Let $X_1,...,X_n$ be a random sample of size $n$ from the beta distribution $B(\theta, 1)$ whose pdf is given by $f(x;\theta)=\theta x^{\theta-1}(0<x<1)$ and the hypotheses are given by: $H_0: \theta=1$   vs.   $H_1: \theta \neq 1$ Using likelihood ratio test with significance level $\alpha$ ($0<\alpha<1$), I ""already found"" that$-2\sum_{i=1}^n \log X_i \sim \chi^2 (2n)$ and the rejection region is given by: $$-2\sum_{i=1}^n \log X_i \le 2nc_1 \text{ or } -2\sum_{i=1}^n \log X_i \ge 2nc_2$$ where the constants $c_1, c_2$ satisfy $\int_{2nc_1}^{2nc_2}f_{2n}(x) \, dx = 1-\alpha$ and $c_1- \log{c_1} =c_2-\log{c_2} $, $f_{2n}$ is the pdf of $\chi^2(2n)$ distribution. Here is the problem: Show that the constants $c_1,c_2$ can be approximated as $c_1 \approx \dfrac{ \chi^2_{1-\alpha/2}(2n)}{2n},c_2 \approx \dfrac{ \chi^2_{\alpha/2}(2n)}{2n}$ when $n$ is sufficiently large. I attempted to prove that the two integrals $\int_0^{2nc_1}f_{2n}(x) \, dx$ and $\int_{2nc_2}^\infty f_{2n}(x) \, dx$ both converge to $\alpha/2$ when $n \to \infty$. Since the sum of the two integrals is $\alpha$, it suffices to show that the first integral goes to $\alpha/2$. Here is where I stuck. Does anyone have ideas? Any hints or advice will help a lot! Thanks.",,"['statistics', 'approximation', 'hypothesis-testing']"
17,Cauchy distribution maximum likelihood estimator: previous sources?,Cauchy distribution maximum likelihood estimator: previous sources?,,"On Wikipedia's page for the wrapped Cauchy distribution , there's a fixed-point algorithm to calculate the maximum-likelihood estimator using a Möbius transformation in the Poincaré disc that the circular Cauchy distribution can be considered to lie in. By using the parameterization $\theta=x_0+i\gamma$ on the regular Cauchy distribution and treating $\theta,\overline{\theta}$ as independent variables, the maximum likelihood equation becomes $$\sum_{j=1}^n\frac{1}{\theta-x_j}=\frac{n}{\theta-\overline{\theta}} $$ and there's a similar (likely the same after Cayley transform, even) estimator $$\theta_{k+1}=\overline{\theta_k-n\left(\sum_{j=1}^n \frac{1}{\theta_k-x_j} \right)^{-1}} $$ which can also be shown to be a contraction in the Poincaré upper half-plane metric by Schwarz-Pick (and that $z \mapsto -\overline{z}$ is an isometry).  However, the Wikipedia page's sources do not mention these and I've been unable to find a reference showing either algorithm, aside from Wikipedia itself.  But, the calculation is simple enough that I'd be legitimately surprised if neither had been done before.  Since I'm not hugely familiar with the statistics literature, I figure this should be aired out.","On Wikipedia's page for the wrapped Cauchy distribution , there's a fixed-point algorithm to calculate the maximum-likelihood estimator using a Möbius transformation in the Poincaré disc that the circular Cauchy distribution can be considered to lie in. By using the parameterization $\theta=x_0+i\gamma$ on the regular Cauchy distribution and treating $\theta,\overline{\theta}$ as independent variables, the maximum likelihood equation becomes $$\sum_{j=1}^n\frac{1}{\theta-x_j}=\frac{n}{\theta-\overline{\theta}} $$ and there's a similar (likely the same after Cayley transform, even) estimator $$\theta_{k+1}=\overline{\theta_k-n\left(\sum_{j=1}^n \frac{1}{\theta_k-x_j} \right)^{-1}} $$ which can also be shown to be a contraction in the Poincaré upper half-plane metric by Schwarz-Pick (and that $z \mapsto -\overline{z}$ is an isometry).  However, the Wikipedia page's sources do not mention these and I've been unable to find a reference showing either algorithm, aside from Wikipedia itself.  But, the calculation is simple enough that I'd be legitimately surprised if neither had been done before.  Since I'm not hugely familiar with the statistics literature, I figure this should be aired out.",,"['probability-theory', 'statistics', 'reference-request']"
18,Kolmogorow-Smirnow-Test with estimated parameters,Kolmogorow-Smirnow-Test with estimated parameters,,"I think the most popular alternative to the ""normal"" Kolmogorow-Smirnow-Test for normal distribution (if the parameters are unknown and therefore have to be estimated) is the Lilliefors-Test. I am thinking about this (maybe too simple) alternative: first, you calculate the empirical mean and empirical variance of the given data (or of a subset of it). Afterwards, you generate a sample of normally distributed random variables with these estimated parameters. Then, you can test, whether the original sample and the generated sample have the same distribution using the 2-sample-kolmogorow-smirnow-test. This would be a simple method to check, whether a given sample follows a normal distribution. I don't see problems but it seems to be too simple. Does anybody see problems/mistakes in this idea? Thank you very much!","I think the most popular alternative to the ""normal"" Kolmogorow-Smirnow-Test for normal distribution (if the parameters are unknown and therefore have to be estimated) is the Lilliefors-Test. I am thinking about this (maybe too simple) alternative: first, you calculate the empirical mean and empirical variance of the given data (or of a subset of it). Afterwards, you generate a sample of normally distributed random variables with these estimated parameters. Then, you can test, whether the original sample and the generated sample have the same distribution using the 2-sample-kolmogorow-smirnow-test. This would be a simple method to check, whether a given sample follows a normal distribution. I don't see problems but it seems to be too simple. Does anybody see problems/mistakes in this idea? Thank you very much!",,"['statistics', 'normal-distribution', 'hypothesis-testing']"
19,Causal inference calculus (Bayesian Probability),Causal inference calculus (Bayesian Probability),,"Here is my problem : There is a causal Markovian model as follows. By the definition of interventional probability , since $\text{do}(x)$ makes no edges between $X$ and $Z_1, Z_2$ , we have $$  P(y\mid \text{do}(x)) =\sum_{z_1,z_2,z_3}P(z_1)P(z_2)P(z_3\mid z_1, z_2)P(y \mid z_2, z_3, x), > $$ where $\text{do}(\cdot)$ is so-called do-calculus (Judea pearl, 2010). Then show that the summation over $z_2$ is $$  P(y\mid \text{do}(x)) =\sum_{z_1, z_3} P(z_1)  P(z_3\mid z_1)P(y \mid z_1, z_3, x). $$ Here is what is tried . To hold the equality of the above problem, we must have $$   \sum_{z_2} P(z_3 \mid z_1,z_2)P(y,z_2,z_3,x)  $$ should be same with $$   P(y,z_1,z_3,x), $$ but I don't know how this can happen. Thanks, Reference: Judea Pearl, 2010, An Introduction to Causal Inference, The International Journal of Biostatistics, pp15-16","Here is my problem : There is a causal Markovian model as follows. By the definition of interventional probability , since makes no edges between and , we have where is so-called do-calculus (Judea pearl, 2010). Then show that the summation over is Here is what is tried . To hold the equality of the above problem, we must have should be same with but I don't know how this can happen. Thanks, Reference: Judea Pearl, 2010, An Introduction to Causal Inference, The International Journal of Biostatistics, pp15-16","\text{do}(x) X Z_1, Z_2 
 P(y\mid \text{do}(x)) =\sum_{z_1,z_2,z_3}P(z_1)P(z_2)P(z_3\mid z_1, z_2)P(y \mid z_2, z_3, x),
>  \text{do}(\cdot) z_2 
 P(y\mid \text{do}(x)) =\sum_{z_1, z_3} P(z_1)  P(z_3\mid z_1)P(y \mid z_1, z_3, x).
 
  \sum_{z_2} P(z_3 \mid z_1,z_2)P(y,z_2,z_3,x) 
 
  P(y,z_1,z_3,x),
","['probability', 'statistics', 'bayesian', 'causal-diagrams', 'causality']"
20,Sum of three normal distributed random variables [closed],Sum of three normal distributed random variables [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Whether the sum of three normal distributed random variables can be nontrivial and discountinuous distributied or not, for example, Bernoulli, Poisson, or binomial distributied? If they are independent or have joint normal distribution, the distribution of the sum is normal distribution. But if they are dependent and have no joint normal distribution, the distribution of the sum may be nontrivial and discontinous. Is there any example?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Whether the sum of three normal distributed random variables can be nontrivial and discountinuous distributied or not, for example, Bernoulli, Poisson, or binomial distributied? If they are independent or have joint normal distribution, the distribution of the sum is normal distribution. But if they are dependent and have no joint normal distribution, the distribution of the sum may be nontrivial and discontinous. Is there any example?",,"['probability', 'statistics', 'normal-distribution']"
21,Finite variance of a linear combination,Finite variance of a linear combination,,"$\newcommand{\E}{\operatorname{E}}\newcommand{\Var}{\operatorname{Var}}\newcommand{\Cov}{\operatorname{Cov}}$Let be $X$ and $Y$ two random variables, such that $\E[X^2]<\infty$ and $\E[Y^2]<\infty$, can we conclude that $\Var[aX+bY]$, with $a,b\in{\mathbb{R}}$ is finite? My attemp: \begin{align} & \left|\Var[aX+bY]\right|=|a^2\Var[X]+b^2\Var[Y]+ab\Cov(X,Y)| \\[10pt] \leq {} & a^2\Var[X]+b^2\Var[Y]+|a||b| \left|\Cov(X,Y)\right| \\[10pt] \leq {} & a^2\Var[X]+b^2\Var[Y]+|a||b|\sqrt{(\Var[X])}\sqrt{(\Var[Y]})<+\infty \end{align} So,in particular, we can conclude that $\E[(aX+bY)^2]<+\infty$","$\newcommand{\E}{\operatorname{E}}\newcommand{\Var}{\operatorname{Var}}\newcommand{\Cov}{\operatorname{Cov}}$Let be $X$ and $Y$ two random variables, such that $\E[X^2]<\infty$ and $\E[Y^2]<\infty$, can we conclude that $\Var[aX+bY]$, with $a,b\in{\mathbb{R}}$ is finite? My attemp: \begin{align} & \left|\Var[aX+bY]\right|=|a^2\Var[X]+b^2\Var[Y]+ab\Cov(X,Y)| \\[10pt] \leq {} & a^2\Var[X]+b^2\Var[Y]+|a||b| \left|\Cov(X,Y)\right| \\[10pt] \leq {} & a^2\Var[X]+b^2\Var[Y]+|a||b|\sqrt{(\Var[X])}\sqrt{(\Var[Y]})<+\infty \end{align} So,in particular, we can conclude that $\E[(aX+bY)^2]<+\infty$",,"['statistics', 'statistical-inference']"
22,Conventional notation for samples of random variables?,Conventional notation for samples of random variables?,,"Is there any conventional notation to separate population values from sample values in statistics? For example, how would one differentiate $\mathbb{E}(X) = \mu$, the population mean, from $\mathbb{E}(X) = \bar{x}$, the sample mean? Would you use a small $x$ like this: $\mathbb{E}(x) = \bar{x}$ ?","Is there any conventional notation to separate population values from sample values in statistics? For example, how would one differentiate $\mathbb{E}(X) = \mu$, the population mean, from $\mathbb{E}(X) = \bar{x}$, the sample mean? Would you use a small $x$ like this: $\mathbb{E}(x) = \bar{x}$ ?",,"['probability', 'statistics', 'notation']"
23,Actuary Exam Question dealing with Poisson distribution,Actuary Exam Question dealing with Poisson distribution,,"Assume a policyholder is four times more likely to file two claims as to file exactly three claims. Assume also that the number X of claims of this policyholder is Poisson. What is $E(X^2)$? Here's what I have so far: $ 4(P(X=3))=P(X=2)=(\lambda^2*e^{-2})/2!\\ P(X=3)=(\lambda^3*e^{-3})/3!$ Solving this algebra gives us that $\lambda=2.039$ Since we're asked to find the 2nd moment about X, isn't that just $\lambda$? The answer to$ E(X^2)$ is 21/16. why is that and what procedure do you use to even get a fraction here?","Assume a policyholder is four times more likely to file two claims as to file exactly three claims. Assume also that the number X of claims of this policyholder is Poisson. What is $E(X^2)$? Here's what I have so far: $ 4(P(X=3))=P(X=2)=(\lambda^2*e^{-2})/2!\\ P(X=3)=(\lambda^3*e^{-3})/3!$ Solving this algebra gives us that $\lambda=2.039$ Since we're asked to find the 2nd moment about X, isn't that just $\lambda$? The answer to$ E(X^2)$ is 21/16. why is that and what procedure do you use to even get a fraction here?",,"['probability', 'statistics', 'probability-distributions', 'poisson-distribution']"
24,Finding constants in terms of correlation coefficients and variance,Finding constants in terms of correlation coefficients and variance,,"The Problem: Let $X_1, X_2, X_3$ be random variables with means $\mu_1, \mu_2, \mu_3$, variances $\sigma_1^2, \sigma_2^2, \sigma_3^2$ and correlation coefficients $\rho_{12}, \rho_{13}, \rho_{23}$ Suppose the following: $$E(X_1 - \mu_1|X_2,X_3)=b_2(x_2-\mu_2)-b_3(x_3-\mu_3)$$ Determine $b_2$ and $b_3$ in terms of the variances and correlation coefficients. My Work: I see that on the right hand side of the equation, the makings of $\sigma_2^2$ and $\sigma_3^2$ are multiplied by their respective constants, so I started by isolating the $b_2$ term, squaring both sides, and then taking the expectation of both sides with respect to $X_2$: $$E_{x_2}([E(X_1 -\mu_1|X_2,X_3)-b_3(x_3-\mu_3)]^2)= b_2^2E([x_2-\mu_2]^2) = b_2^2\sigma_2^2$$ This seems very, very messy so I doubt I am heading in the right direction. Also, I have no idea how to pull the correlation coefficients out.","The Problem: Let $X_1, X_2, X_3$ be random variables with means $\mu_1, \mu_2, \mu_3$, variances $\sigma_1^2, \sigma_2^2, \sigma_3^2$ and correlation coefficients $\rho_{12}, \rho_{13}, \rho_{23}$ Suppose the following: $$E(X_1 - \mu_1|X_2,X_3)=b_2(x_2-\mu_2)-b_3(x_3-\mu_3)$$ Determine $b_2$ and $b_3$ in terms of the variances and correlation coefficients. My Work: I see that on the right hand side of the equation, the makings of $\sigma_2^2$ and $\sigma_3^2$ are multiplied by their respective constants, so I started by isolating the $b_2$ term, squaring both sides, and then taking the expectation of both sides with respect to $X_2$: $$E_{x_2}([E(X_1 -\mu_1|X_2,X_3)-b_3(x_3-\mu_3)]^2)= b_2^2E([x_2-\mu_2]^2) = b_2^2\sigma_2^2$$ This seems very, very messy so I doubt I am heading in the right direction. Also, I have no idea how to pull the correlation coefficients out.",,"['probability', 'probability-theory', 'statistics', 'random-variables']"
25,decreasing random process depending on previous draw,decreasing random process depending on previous draw,,"I draw a number $x_i \in [n]$ where $n \in \mathbb{N}$. I draw a number again $x_{i+1} \in [x_i]$. I repeat this drawing process till i got an 1. This generates a random sequence $(x_i)_{i=1}^k$. All draws are uniformly at random. Question 1: What is $P[x_i = 1] $ meaning the probability of stopping at a specific index i. Question 2: $P[x_i = k] $ where k is some number in N. I already solved Question 2 to some recurrence $\forall k > 1$: $s_{i,k}(n) = \sum_{l=k}^n \dfrac{1}{l} s_{i-1,l}(n) = P[x_i = k] $ with base case $s_{1,l} = \dfrac{1}{n}$. But its quite a messy way and not able to be in closed form so im wondering if anyone else has a better way of solving this.","I draw a number $x_i \in [n]$ where $n \in \mathbb{N}$. I draw a number again $x_{i+1} \in [x_i]$. I repeat this drawing process till i got an 1. This generates a random sequence $(x_i)_{i=1}^k$. All draws are uniformly at random. Question 1: What is $P[x_i = 1] $ meaning the probability of stopping at a specific index i. Question 2: $P[x_i = k] $ where k is some number in N. I already solved Question 2 to some recurrence $\forall k > 1$: $s_{i,k}(n) = \sum_{l=k}^n \dfrac{1}{l} s_{i-1,l}(n) = P[x_i = k] $ with base case $s_{1,l} = \dfrac{1}{n}$. But its quite a messy way and not able to be in closed form so im wondering if anyone else has a better way of solving this.",,"['statistics', 'probability-distributions', 'random-walk', 'random-functions']"
26,"Maximum Likelihood Estimation of Multivariate Gaussian Density, where the number of samples is smaller than the unknown parameters","Maximum Likelihood Estimation of Multivariate Gaussian Density, where the number of samples is smaller than the unknown parameters",,"If we want to estimate the $p\times p$ (full rank) covariance matrix $\Sigma$ of multivariate normal density, using $n$ sample vectors $\mathbf{x}_1, \ldots, \mathbf{x}_n$, then the empirical covariance matrix is known to be the maximum likelihood estimation $\Sigma_\text{ML}$ (at least when $n\gg p$) of $\Sigma$. But when $n\propto p$, or in particular when $n<p$, then ML estimation is not reliable, and becomes singular, since $\Sigma_\text{ML} = \dfrac{1}{n} XX'$, where $X=[\mathbf{x}_1,\ldots,\mathbf{x}_n]$ is a $p\times n$ matrix. Now, since $n<p$ the rank of this $p\times p$ matrix $\Sigma_\text{ML}$ is at most $n$, making it singular. Which obviously is not a good estimation, since we have samples of a full rank matrix $\Sigma$. But the covariance matrix simply consists of pairwise covariance values $\Sigma=\sigma_{ij}$, and to compute each, we can use the sample vectors, (using components $i$ of each sample vector and the whole $\mathbf{x}_n$), which are apparently enough to estimate $\sigma_{ij}$. We can do this for all the covariance elements. But what is the reason that we still cannot rely on ML estimation? Is it because, we use those $n$ samples $p\times (p+1)/2$ times (to compute each $\sigma_{ij}$), and this results in having somehow dependent rows (columns), and hence a singular $\Sigma_\text{ML}$? But how we can rigorously show this?","If we want to estimate the $p\times p$ (full rank) covariance matrix $\Sigma$ of multivariate normal density, using $n$ sample vectors $\mathbf{x}_1, \ldots, \mathbf{x}_n$, then the empirical covariance matrix is known to be the maximum likelihood estimation $\Sigma_\text{ML}$ (at least when $n\gg p$) of $\Sigma$. But when $n\propto p$, or in particular when $n<p$, then ML estimation is not reliable, and becomes singular, since $\Sigma_\text{ML} = \dfrac{1}{n} XX'$, where $X=[\mathbf{x}_1,\ldots,\mathbf{x}_n]$ is a $p\times n$ matrix. Now, since $n<p$ the rank of this $p\times p$ matrix $\Sigma_\text{ML}$ is at most $n$, making it singular. Which obviously is not a good estimation, since we have samples of a full rank matrix $\Sigma$. But the covariance matrix simply consists of pairwise covariance values $\Sigma=\sigma_{ij}$, and to compute each, we can use the sample vectors, (using components $i$ of each sample vector and the whole $\mathbf{x}_n$), which are apparently enough to estimate $\sigma_{ij}$. We can do this for all the covariance elements. But what is the reason that we still cannot rely on ML estimation? Is it because, we use those $n$ samples $p\times (p+1)/2$ times (to compute each $\sigma_{ij}$), and this results in having somehow dependent rows (columns), and hence a singular $\Sigma_\text{ML}$? But how we can rigorously show this?",,"['matrices', 'statistics', 'statistical-inference', 'matrix-calculus', 'stochastic-approximation']"
27,Generating a list of numbers whose combinations are mathematically unique,Generating a list of numbers whose combinations are mathematically unique,,"I'm trying to design a puzzle for a game I'm making, and the puzzle is fairly similar to the one from the power plant in Myst . The idea is that I have 9 buttons, and each button supplies an amount of power to the ""system"". The solution to the puzzle is to press three specific buttons (order doesn't matter) to reach a specific power threshold. Obviously, I have to be careful about how I choose how much power each button provides since if a number can be reached by more than a single combination, that can destroy the complexity of the puzzle somewhat. My first naive idea was to use the first 9 prime numbers, but a quick check through a program showed that that wasn't going to fly. It had occurred to me that I could just choose a number that only occurs once in the list of results, but I would prefer it if I could choose which 3 buttons to press at random. How do I go about generating this list of 9 numbers such that adding any 3 of the numbers together results in a unique value? Is there some statistics/probability thing that I can use or do I just have to trial-and-error it?","I'm trying to design a puzzle for a game I'm making, and the puzzle is fairly similar to the one from the power plant in Myst . The idea is that I have 9 buttons, and each button supplies an amount of power to the ""system"". The solution to the puzzle is to press three specific buttons (order doesn't matter) to reach a specific power threshold. Obviously, I have to be careful about how I choose how much power each button provides since if a number can be reached by more than a single combination, that can destroy the complexity of the puzzle somewhat. My first naive idea was to use the first 9 prime numbers, but a quick check through a program showed that that wasn't going to fly. It had occurred to me that I could just choose a number that only occurs once in the list of results, but I would prefer it if I could choose which 3 buttons to press at random. How do I go about generating this list of 9 numbers such that adding any 3 of the numbers together results in a unique value? Is there some statistics/probability thing that I can use or do I just have to trial-and-error it?",,['statistics']
28,Definition of Expectation,Definition of Expectation,,"I found in a textbook this definition of expectation for a random variable $X$ $$E(X)=\lim_{n\to\infty}\sum_{i=1}^n\frac{i-1}{\sqrt{n}}P\left(\frac{i-1}{\sqrt{n}}<X\leq \frac{i}{\sqrt{n}}\right)$$ However, the textbook doesn't particularly mention where this definition comes from, when it's from, and how it's equivalent to the current definition to expectation.  Also, I cannot find any other references to this anywhere.  If anyone knows anywhere to further read about this, it would be highly appreciated.  Thanks!","I found in a textbook this definition of expectation for a random variable $X$ $$E(X)=\lim_{n\to\infty}\sum_{i=1}^n\frac{i-1}{\sqrt{n}}P\left(\frac{i-1}{\sqrt{n}}<X\leq \frac{i}{\sqrt{n}}\right)$$ However, the textbook doesn't particularly mention where this definition comes from, when it's from, and how it's equivalent to the current definition to expectation.  Also, I cannot find any other references to this anywhere.  If anyone knows anywhere to further read about this, it would be highly appreciated.  Thanks!",,"['probability', 'probability-theory', 'statistics', 'self-learning']"
29,Probability of a biased coin obtaining its second heads or seconds tails on the $6^\text{th}$ toss,Probability of a biased coin obtaining its second heads or seconds tails on the  toss,6^\text{th},"Consider a coin for which the $P(\text{heads}) = {1\over3}$ and $P(\text{tails}) = {2\over3}$. Suppose that the coin will be repeatedly flipped  until at least two heads and at least two tails are obtained. Letting $X$ be the number of flips required, give the value of $P(X= 6)$ Attempted Solution: In order to get the second head on the $6^\text{th}$ toss, you need to get $4$ tails in the first $5$ tosses, and heads on the $6^\text{th}$ toss. Similarly, in order to get the second tail on the $6^\text{th}$ toss, you need to get $4$ heads in the first $5$ tosses, and tails on the $6^\text{th}$ toss. So would the answer just be $${5\choose4} \left(\frac{2}{3}\right)^4 \left(\frac{1}{3}\right)^2 + {5\choose4} \left(\frac{1}{3}\right)^4 \left(\frac{2}{3}\right)^2 = 0.1372$$","Consider a coin for which the $P(\text{heads}) = {1\over3}$ and $P(\text{tails}) = {2\over3}$. Suppose that the coin will be repeatedly flipped  until at least two heads and at least two tails are obtained. Letting $X$ be the number of flips required, give the value of $P(X= 6)$ Attempted Solution: In order to get the second head on the $6^\text{th}$ toss, you need to get $4$ tails in the first $5$ tosses, and heads on the $6^\text{th}$ toss. Similarly, in order to get the second tail on the $6^\text{th}$ toss, you need to get $4$ heads in the first $5$ tosses, and tails on the $6^\text{th}$ toss. So would the answer just be $${5\choose4} \left(\frac{2}{3}\right)^4 \left(\frac{1}{3}\right)^2 + {5\choose4} \left(\frac{1}{3}\right)^4 \left(\frac{2}{3}\right)^2 = 0.1372$$",,"['probability', 'combinatorics', 'statistics']"
30,Model for probability of choosing k successful marbles from an urn of a + b marbles,Model for probability of choosing k successful marbles from an urn of a + b marbles,,"I am wondering if there exists a model for the probability of choosing at least k successful marbles from an urn of a (successful) and b marbles with replacement given n draws, where if an a marble is selected, it is replaced as a b marble but not vice versa (i.e., a marbles become b marbles if selected, but b marbles do not become a marbles). I have come across beta distributions and beta-binomial distributions, but these don't seem to be exactly what I need. Any help is much appreciated.","I am wondering if there exists a model for the probability of choosing at least k successful marbles from an urn of a (successful) and b marbles with replacement given n draws, where if an a marble is selected, it is replaced as a b marble but not vice versa (i.e., a marbles become b marbles if selected, but b marbles do not become a marbles). I have come across beta distributions and beta-binomial distributions, but these don't seem to be exactly what I need. Any help is much appreciated.",,"['probability', 'statistics', 'game-theory']"
31,Expected Value of neural network output,Expected Value of neural network output,,"I'm attempting to find a closed form expected value of a particularly messy expression involving dot products of multiple neural network outputs. A sub-problem of this is to consider just a single feed-forward network. Consider the feed-forward neural network with weight matrices $W_1 \in \mathbb{R^{k_2 \times k_1}}$, $W_2\in \mathbb{R^{k_3 \times k_2}}$, and input $x\in \mathbb{R^{k_1}}$. Treat each component of these as iid random variables normally distributed with the following means, $\mu_{W_1}, \mu_{W_2}, \mu_{x}$ and variances, $\sigma_{W_1}, \sigma_{W_2}, \sigma_{x}$, respectably. E.g. $[W_1]_{i,j} \sim \mathcal{N}(\mu_{W_1},\sigma_{W_1})$. The neural network output would be $$ o(W_1,W_2,x) := g(W_2 g(W_1x))$$ where $g$ is the relu function. What then would be its expected value? $$ E[g(W_2 g(W_1x))]$$ I know this can fairly easily be calculated through Monte Carlo methods, but is it possible to come up with a closed-form expression or a closed-form approximation?","I'm attempting to find a closed form expected value of a particularly messy expression involving dot products of multiple neural network outputs. A sub-problem of this is to consider just a single feed-forward network. Consider the feed-forward neural network with weight matrices $W_1 \in \mathbb{R^{k_2 \times k_1}}$, $W_2\in \mathbb{R^{k_3 \times k_2}}$, and input $x\in \mathbb{R^{k_1}}$. Treat each component of these as iid random variables normally distributed with the following means, $\mu_{W_1}, \mu_{W_2}, \mu_{x}$ and variances, $\sigma_{W_1}, \sigma_{W_2}, \sigma_{x}$, respectably. E.g. $[W_1]_{i,j} \sim \mathcal{N}(\mu_{W_1},\sigma_{W_1})$. The neural network output would be $$ o(W_1,W_2,x) := g(W_2 g(W_1x))$$ where $g$ is the relu function. What then would be its expected value? $$ E[g(W_2 g(W_1x))]$$ I know this can fairly easily be calculated through Monte Carlo methods, but is it possible to come up with a closed-form expression or a closed-form approximation?",,"['probability', 'integration', 'statistics', 'machine-learning', 'neural-networks']"
32,Hypothesis Testing of Comparison of $2$ Treatments,Hypothesis Testing of Comparison of  Treatments,2,"A blanching process currently used in the canning industry consists of treating vegetables with a large volume of boiling water before canning. A newly developed method, called Steam Blanching Process (SBP), is expected to remove less vitamins and minerals from vegetables, because it is more of a steam wash than a flowing water wash. Ten batches of string beans from different farms are to be used to compare the SBP and the standard process. One-half of each batch of beans is treated with the standard process; the other half of each batch is treated with the SBP. Measurements of the vitamin content per pound of canned beans are: SBP Standard 35 33 48 40 65 55 33 41 61 62 54 54 49 40 37 35 58 59 65 56 a) Do the data provide strong evidence that SBP removes less vitamins in canned beans than the standard method of blanching? Use t-test and sign-test. b) Construct a $98$% confidence interval for the difference between the mean vitamin contents per pound using the two methods of blanching. Attempted Solution a) t-test $H_0$: $\delta$ = $0$ $H_1$: $\delta$ $\gt$ $0$ Average difference $\bar{D}$ = $3$ $S_D$ = $5.8689$ t = $\frac{\bar{D}-\delta_0}{S_D\over{\sqrt{n}}}$ = $1.616$ $t_{9, .05}$ = $1.83313$ Since $1.616 \lt 1.833113$ we fail to reject $H_0$ at $\alpha = .05$ Sign Test Proportion of positive $D_i$'s = $2\over3$ $P(X \geq 6)$ = $.5^{9}$[$9\choose{6}$ + $9\choose{7}$ + $9\choose{8}$ + $9\choose{9}$] = $.254 \gt .05$ so we fail to reject $H_0$ at $\alpha = .05$ b) $\bar{D}$ $\pm$ $t_{n-1, \alpha/2}$$\frac{S_D}{\sqrt{n}}$ =$3$ $\pm$ $t_{9, .01}$$\frac{5.8689}{\sqrt{10}}$ = $(-2.236, 8.236)$ Did I do these correctly? Particularly the sign test?","A blanching process currently used in the canning industry consists of treating vegetables with a large volume of boiling water before canning. A newly developed method, called Steam Blanching Process (SBP), is expected to remove less vitamins and minerals from vegetables, because it is more of a steam wash than a flowing water wash. Ten batches of string beans from different farms are to be used to compare the SBP and the standard process. One-half of each batch of beans is treated with the standard process; the other half of each batch is treated with the SBP. Measurements of the vitamin content per pound of canned beans are: SBP Standard 35 33 48 40 65 55 33 41 61 62 54 54 49 40 37 35 58 59 65 56 a) Do the data provide strong evidence that SBP removes less vitamins in canned beans than the standard method of blanching? Use t-test and sign-test. b) Construct a $98$% confidence interval for the difference between the mean vitamin contents per pound using the two methods of blanching. Attempted Solution a) t-test $H_0$: $\delta$ = $0$ $H_1$: $\delta$ $\gt$ $0$ Average difference $\bar{D}$ = $3$ $S_D$ = $5.8689$ t = $\frac{\bar{D}-\delta_0}{S_D\over{\sqrt{n}}}$ = $1.616$ $t_{9, .05}$ = $1.83313$ Since $1.616 \lt 1.833113$ we fail to reject $H_0$ at $\alpha = .05$ Sign Test Proportion of positive $D_i$'s = $2\over3$ $P(X \geq 6)$ = $.5^{9}$[$9\choose{6}$ + $9\choose{7}$ + $9\choose{8}$ + $9\choose{9}$] = $.254 \gt .05$ so we fail to reject $H_0$ at $\alpha = .05$ b) $\bar{D}$ $\pm$ $t_{n-1, \alpha/2}$$\frac{S_D}{\sqrt{n}}$ =$3$ $\pm$ $t_{9, .01}$$\frac{5.8689}{\sqrt{10}}$ = $(-2.236, 8.236)$ Did I do these correctly? Particularly the sign test?",,"['probability', 'statistics', 'hypothesis-testing']"
33,MOM estimator same as MLE estimator for normal distribution,MOM estimator same as MLE estimator for normal distribution,,"Q: Let X1,...,Xn be IID N(μ, σ²) r.v. Is the MOM estimators for σ² and μ same as the MLE estimators? For Uniform distribution, i remembered that the MOM and MLE estimators corresponds and are the same, but is it the same for normal distribution?","Q: Let X1,...,Xn be IID N(μ, σ²) r.v. Is the MOM estimators for σ² and μ same as the MLE estimators? For Uniform distribution, i remembered that the MOM and MLE estimators corresponds and are the same, but is it the same for normal distribution?",,"['statistics', 'moment-generating-functions']"
34,Distribution of green bean lengths before and after cutting,Distribution of green bean lengths before and after cutting,,"Say I have a bag of beans, with different lengths, but following a normal distribution with some mean and some standard deviation. If I were to take out the beans in pairs, and cut the longer one in half, leave the shorter one alone, and put them both in a different pot (so they won't be subject to more than one inspection or cut), and continue doing this for all the beans, how will the distribution change? (An acceptable result is finding the new mean and standard deviation as functions of the old ones - a simulation is interesting but I can do that myself, and wouldn't give that satisfaction of understanding that a real solution would bring). Context: I thought of this problem a number of years ago while doing a first year undergraduate course in statistics, while chopping beans for dinner. I later asked my tutor, but he didn't know how to solve it (or didn't have time). I thought of it again a few days ago, again while chopping beans. I never did any further studies in statistics and have forgotten 90% of what I learned so I'm not really equipped to begin solving it myself. Hoping I can get closure on this topic :-)","Say I have a bag of beans, with different lengths, but following a normal distribution with some mean and some standard deviation. If I were to take out the beans in pairs, and cut the longer one in half, leave the shorter one alone, and put them both in a different pot (so they won't be subject to more than one inspection or cut), and continue doing this for all the beans, how will the distribution change? (An acceptable result is finding the new mean and standard deviation as functions of the old ones - a simulation is interesting but I can do that myself, and wouldn't give that satisfaction of understanding that a real solution would bring). Context: I thought of this problem a number of years ago while doing a first year undergraduate course in statistics, while chopping beans for dinner. I later asked my tutor, but he didn't know how to solve it (or didn't have time). I thought of it again a few days ago, again while chopping beans. I never did any further studies in statistics and have forgotten 90% of what I learned so I'm not really equipped to begin solving it myself. Hoping I can get closure on this topic :-)",,"['probability', 'statistics', 'normal-distribution']"
35,Variance with specific PMF [closed],Variance with specific PMF [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I'm having issues finding the variance of the following PMF: $$\mathbb P(X=k) = C \frac{p^k} k,\quad k=1,2,\ldots$$ where $C$ is a normalizing constant and $p$ is between $0$ and $1$ (inclusive). I have gotten the expected value-- which is $(Cp/q)$ -- but I am having issues calculating the $\operatorname{Var}(X)$, specifically with $E(X^2)$. Help please?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I'm having issues finding the variance of the following PMF: $$\mathbb P(X=k) = C \frac{p^k} k,\quad k=1,2,\ldots$$ where $C$ is a normalizing constant and $p$ is between $0$ and $1$ (inclusive). I have gotten the expected value-- which is $(Cp/q)$ -- but I am having issues calculating the $\operatorname{Var}(X)$, specifically with $E(X^2)$. Help please?",,"['probability', 'statistics']"
36,Sufficient statistic: prove Fisher–Neyman factorization theorem using conditional expectation,Sufficient statistic: prove Fisher–Neyman factorization theorem using conditional expectation,,"I am studying the following problem: Consider $X = (X^{(1)}, \ldots, X^{(n)})$ a random variable drawn from a distribution with density $p_{\theta}$ for some $\theta \in \Theta$, where $\theta$ is itself a random variable. Assume that the joint distribution of $(X, \theta)$ has the density $p(x, \theta) = p(x\mid\theta)p(\theta)$. The statistic $T(X)$ is said to be a sufficient statistic if there exists functions $f$ and $h$ such that for any $x$ $$  p(x\mid\theta) = h(x, T(x))f(T(x), \theta) $$ Show that $T$ is a sufficient statistic if and only if $\theta$ and $X$ are conditionally independent given $T$. I have found a few proofs online, and they usually suppose that either all variables are discrete or all of them are continuous. I was thinking about proving a similar result in a more general case, in order to take into account all the possibilities (for example, $X$ and $T(X)$ continuous and $\theta$ discrete, or $X$ and $\theta$ continuous and $T(X)$ discrete). For that, I reformulated the problem using conditional expectations: Prove that there exists measurable functions $g$ and $h$ such that, for all measurable $f$:   $$ E[f(X)\mid\theta] = h(X, T(X))g(T(X), \theta) $$ if and only if $$ E[\phi(X)\psi(\theta)\mid T(X)] = E[\phi(X)\mid T(X)] E[\psi(\theta)\mid T(X)] $$ for all measurable $\psi$, $\phi$. I would like to know if it is true (and how to prove it, if it is), as I couldn't prove it myself. Thank you for your help!","I am studying the following problem: Consider $X = (X^{(1)}, \ldots, X^{(n)})$ a random variable drawn from a distribution with density $p_{\theta}$ for some $\theta \in \Theta$, where $\theta$ is itself a random variable. Assume that the joint distribution of $(X, \theta)$ has the density $p(x, \theta) = p(x\mid\theta)p(\theta)$. The statistic $T(X)$ is said to be a sufficient statistic if there exists functions $f$ and $h$ such that for any $x$ $$  p(x\mid\theta) = h(x, T(x))f(T(x), \theta) $$ Show that $T$ is a sufficient statistic if and only if $\theta$ and $X$ are conditionally independent given $T$. I have found a few proofs online, and they usually suppose that either all variables are discrete or all of them are continuous. I was thinking about proving a similar result in a more general case, in order to take into account all the possibilities (for example, $X$ and $T(X)$ continuous and $\theta$ discrete, or $X$ and $\theta$ continuous and $T(X)$ discrete). For that, I reformulated the problem using conditional expectations: Prove that there exists measurable functions $g$ and $h$ such that, for all measurable $f$:   $$ E[f(X)\mid\theta] = h(X, T(X))g(T(X), \theta) $$ if and only if $$ E[\phi(X)\psi(\theta)\mid T(X)] = E[\phi(X)\mid T(X)] E[\psi(\theta)\mid T(X)] $$ for all measurable $\psi$, $\phi$. I would like to know if it is true (and how to prove it, if it is), as I couldn't prove it myself. Thank you for your help!",,"['probability-theory', 'statistics', 'conditional-expectation']"
37,"If X is a random variable with a Poisson distribution, rate parameter $\lambda$ show that...","If X is a random variable with a Poisson distribution, rate parameter  show that...",\lambda,"If X is a random variable with a Poisson distribution, rate parameter   $\lambda$ show that $$E(X(X-1)(X-2)...(X-k)) = \lambda^{k+1}$$ I know that for the probability generating function $G(N)$ of a Poisson random variable, $$G''(1)=E(X(X-1))=\lambda^2$$ and I'm guessing that with each proceeding derivative we add $(X-2), (X-3)$, etc inside the expectation, eg: $G'''(1)=E(X(X-1)(X-2))=\lambda^3$. But I don't really know how to show it.","If X is a random variable with a Poisson distribution, rate parameter   $\lambda$ show that $$E(X(X-1)(X-2)...(X-k)) = \lambda^{k+1}$$ I know that for the probability generating function $G(N)$ of a Poisson random variable, $$G''(1)=E(X(X-1))=\lambda^2$$ and I'm guessing that with each proceeding derivative we add $(X-2), (X-3)$, etc inside the expectation, eg: $G'''(1)=E(X(X-1)(X-2))=\lambda^3$. But I don't really know how to show it.",,"['statistics', 'stochastic-processes', 'generating-functions']"
38,Conditional Probability of Train Arriving on Time,Conditional Probability of Train Arriving on Time,,"Suppose that a train is scheduled to arrive at its destination at noon. Also suppose that if it experiences no major problems it will arrive before noon with probability $0.9$, if it experiences one major problem it will arrive before noon with probability $0.5$, if it experiences two major problems it will arrive before noon with probability $0.1$, and if it experiences more than two major problems it will not arrive before noon. If the probability of no major problems is $0.5$, the probability of one major problem is $0.2$, the probability of two major problems is $0.2$, and the probability of more than two major problems is $0.1$, given that the train will arrive before noon, what is the probability of no major delays? Attempted Solution: P(no major problems | arrives before noon) = P(no major problems $\cap$ arrives before noon)/P(arrives before noon) $${.5(.9)\over .5(.9)+.2(.5)+.2(.1)+.1(0)} = .7895$$ Did I do this correctly?","Suppose that a train is scheduled to arrive at its destination at noon. Also suppose that if it experiences no major problems it will arrive before noon with probability $0.9$, if it experiences one major problem it will arrive before noon with probability $0.5$, if it experiences two major problems it will arrive before noon with probability $0.1$, and if it experiences more than two major problems it will not arrive before noon. If the probability of no major problems is $0.5$, the probability of one major problem is $0.2$, the probability of two major problems is $0.2$, and the probability of more than two major problems is $0.1$, given that the train will arrive before noon, what is the probability of no major delays? Attempted Solution: P(no major problems | arrives before noon) = P(no major problems $\cap$ arrives before noon)/P(arrives before noon) $${.5(.9)\over .5(.9)+.2(.5)+.2(.1)+.1(0)} = .7895$$ Did I do this correctly?",,"['probability', 'statistics']"
39,"If 9 people play against each other twice for one point each, what is the minimum number of points you would need to be guaranteed in the top 6?","If 9 people play against each other twice for one point each, what is the minimum number of points you would need to be guaranteed in the top 6?",,"If 9 people were going to play a game against each other, two times each (each game would be 1 point), how many points would an individual person need to get to be guaranteed to be in the top 6? I am trying to figure out the magic number for a tournament at work and we haven't been able to come to a conclusion so I'm turning to you.","If 9 people were going to play a game against each other, two times each (each game would be 1 point), how many points would an individual person need to get to be guaranteed to be in the top 6? I am trying to figure out the magic number for a tournament at work and we haven't been able to come to a conclusion so I'm turning to you.",,['statistics']
40,"In machine learning and statistics, why does the lasso path slope down to the right?","In machine learning and statistics, why does the lasso path slope down to the right?",,"In Lasso regression, for a sparse estimate of coefficients $\beta$, we have: $$ \hat{\beta}(\lambda) = \arg \min_b \Bigl\{\frac{1}{2} \|y-Xb\|^2_{2} + \lambda\|b\|_1\Bigl\} $$ One graph I saw that plotted out the coefficient values of $\beta$ vs. the $\lambda$ parameter is: My question is why the graphs slope downwards to zero? In other words, why is it that when we increase $\lambda$, our coefficient estimates tend to zero? I did a thought experiment where I let the $\lambda||b||_1$ term get large, but I fail to see the connection. In other words: 1) Is it the case Lasso Paths always slope to zero as $\lambda$ gets large? 2) Do coefficient values always start positive? (if $\lambda = 0$, then we have OLS). 3) What is the intuition here? Thanks!","In Lasso regression, for a sparse estimate of coefficients $\beta$, we have: $$ \hat{\beta}(\lambda) = \arg \min_b \Bigl\{\frac{1}{2} \|y-Xb\|^2_{2} + \lambda\|b\|_1\Bigl\} $$ One graph I saw that plotted out the coefficient values of $\beta$ vs. the $\lambda$ parameter is: My question is why the graphs slope downwards to zero? In other words, why is it that when we increase $\lambda$, our coefficient estimates tend to zero? I did a thought experiment where I let the $\lambda||b||_1$ term get large, but I fail to see the connection. In other words: 1) Is it the case Lasso Paths always slope to zero as $\lambda$ gets large? 2) Do coefficient values always start positive? (if $\lambda = 0$, then we have OLS). 3) What is the intuition here? Thanks!",,"['statistics', 'machine-learning']"
41,A probability distribution problem,A probability distribution problem,,"Is this a negative binomial distribution? How to solve this particular problem? Two ball players, denoted A and B , are practicing their scoring skill in their respective sports. The probability that A will score in any attempt is $p$, and A tries until they have scored $r$ times. The probability that B will score in any attempt is $mp$, where $m$ is a given integer $(m = 2, 3, . . .)$ such that $mp < 1$, and B tries until they have scored $mr$ times. (a) For which player ( A or B ) is the expected number of failed attempts smaller? Solved. (b) For which player is the variance of the total number of attempts smaller?","Is this a negative binomial distribution? How to solve this particular problem? Two ball players, denoted A and B , are practicing their scoring skill in their respective sports. The probability that A will score in any attempt is $p$, and A tries until they have scored $r$ times. The probability that B will score in any attempt is $mp$, where $m$ is a given integer $(m = 2, 3, . . .)$ such that $mp < 1$, and B tries until they have scored $mr$ times. (a) For which player ( A or B ) is the expected number of failed attempts smaller? Solved. (b) For which player is the variance of the total number of attempts smaller?",,"['probability', 'statistics', 'discrete-mathematics', 'probability-distributions']"
42,probability density function and expected value of Y,probability density function and expected value of Y,,"Consider a random variable Y whose probability density function (PDF) is given by In integral $$ f_Y(y) = \begin{cases} \frac43 y^3 & 0 < y < 1 \\ \frac83 - \frac43 y & 1 < y < 2 \end{cases} $$ $$ E(Y) = \int_{y=0}^2 y \, f_Y(y) \, dy $$ First term between 0 and 1: $$ \int_{y=0}^1 y \, \frac43 y^3 \, dy = \frac{4}{15} $$ Second term between 1 and 2: $$ \int_{y=1}^2 y \, \left(\frac83 - \frac43 y \right) \, dy = \frac89 $$ $$ E(Y)= \frac{52}{45} = 1.15 $$ Did I make any mistakes?","Consider a random variable Y whose probability density function (PDF) is given by In integral $$ f_Y(y) = \begin{cases} \frac43 y^3 & 0 < y < 1 \\ \frac83 - \frac43 y & 1 < y < 2 \end{cases} $$ $$ E(Y) = \int_{y=0}^2 y \, f_Y(y) \, dy $$ First term between 0 and 1: $$ \int_{y=0}^1 y \, \frac43 y^3 \, dy = \frac{4}{15} $$ Second term between 1 and 2: $$ \int_{y=1}^2 y \, \left(\frac83 - \frac43 y \right) \, dy = \frac89 $$ $$ E(Y)= \frac{52}{45} = 1.15 $$ Did I make any mistakes?",,"['probability', 'statistics', 'probability-distributions']"
43,Correct me if I am wrong : Bound the absolute value of sum of products using central limit theorem.,Correct me if I am wrong : Bound the absolute value of sum of products using central limit theorem.,,"I need to bind the value of a variable $\alpha$. $\alpha$ is defined as, $\alpha=a_0\cdot b_0 + a_1\cdot b_1 + a_2\cdot b_2 + \cdots +a_{n-1}\cdot b_{n-1}$. -  ( eq. 1 ) $a_i$'s are selected from a certain distribution with standard deviation $\sigma$ (for simplicity we can assume it a Gaussian distribution). And $b_i$'s are uniform random variable in $(-1/2,1/2]$. $a_i$'s and $b_i$'s are all independently sampled. Now, $E[a^2]=\sigma^2$, as $\mu=0$ and similarly, $E[a^2]=\sigma_b^2=1/12$, also $\mu=0$. Hence, $a_ib_i$ has mean $=0$ and $E[a^2b^2]=\sigma^2/12$. Hence, from central limit theorem we can claim that $\alpha$ follows a Gaussian distribution with mean $0$ and variance $=n\cdot\sigma^2/12$ standard deviation $=\sqrt{n/12}\cdot\sigma$. - (Eq. 2 ) The above part was explained nicely by Henry in this question . Now as $\alpha$ follows a Gaussian distribution we can claim using the Lemma 4.4 in this paper , that for $k>0$ $Pr[|\alpha|>k\sigma_\alpha : \alpha\leftarrow D_{\sigma_\alpha}]\leq 2e^{-k^2/2}$, Here, $\sigma_\alpha=\sqrt{n/12}\cdot\sigma$. - (eq. 3 ). Now, My question is suppose that there are two collections of $t\geq n$ numbers $A$ and $B$. Each element of $A$ is sampled independently from Gaussian with standard deviation $\sigma$ and each element of $B$ is chosen independently from the uniform distribution $(-1/2,1/2]$. Now, can I still claim that the relation in $eq. 3$ $\textit{i.e}$ $\alpha>k\sigma$ with probability $2\cdot e^{-k^2/2}$, holds if $\alpha$ is generated by choosing $n$ $a_i$'s and $b_i$'s from collection $A$ and $B$ respectively. As, each $a_i$ and $b_i$ is still independent of each other. What will be the effect of 1) $t=n$ or 2) $t=n^2$ I appreciate your help.","I need to bind the value of a variable $\alpha$. $\alpha$ is defined as, $\alpha=a_0\cdot b_0 + a_1\cdot b_1 + a_2\cdot b_2 + \cdots +a_{n-1}\cdot b_{n-1}$. -  ( eq. 1 ) $a_i$'s are selected from a certain distribution with standard deviation $\sigma$ (for simplicity we can assume it a Gaussian distribution). And $b_i$'s are uniform random variable in $(-1/2,1/2]$. $a_i$'s and $b_i$'s are all independently sampled. Now, $E[a^2]=\sigma^2$, as $\mu=0$ and similarly, $E[a^2]=\sigma_b^2=1/12$, also $\mu=0$. Hence, $a_ib_i$ has mean $=0$ and $E[a^2b^2]=\sigma^2/12$. Hence, from central limit theorem we can claim that $\alpha$ follows a Gaussian distribution with mean $0$ and variance $=n\cdot\sigma^2/12$ standard deviation $=\sqrt{n/12}\cdot\sigma$. - (Eq. 2 ) The above part was explained nicely by Henry in this question . Now as $\alpha$ follows a Gaussian distribution we can claim using the Lemma 4.4 in this paper , that for $k>0$ $Pr[|\alpha|>k\sigma_\alpha : \alpha\leftarrow D_{\sigma_\alpha}]\leq 2e^{-k^2/2}$, Here, $\sigma_\alpha=\sqrt{n/12}\cdot\sigma$. - (eq. 3 ). Now, My question is suppose that there are two collections of $t\geq n$ numbers $A$ and $B$. Each element of $A$ is sampled independently from Gaussian with standard deviation $\sigma$ and each element of $B$ is chosen independently from the uniform distribution $(-1/2,1/2]$. Now, can I still claim that the relation in $eq. 3$ $\textit{i.e}$ $\alpha>k\sigma$ with probability $2\cdot e^{-k^2/2}$, holds if $\alpha$ is generated by choosing $n$ $a_i$'s and $b_i$'s from collection $A$ and $B$ respectively. As, each $a_i$ and $b_i$ is still independent of each other. What will be the effect of 1) $t=n$ or 2) $t=n^2$ I appreciate your help.",,"['statistics', 'probability-distributions', 'normal-distribution', 'central-limit-theorem']"
44,Measure function similarity,Measure function similarity,,"I need functional $F(f,g)$ to measure similarity of two real-valued functions $f$, $g$ defined on real segment [a, b] As a simple reasonable example we can consider functional $F(f,g) = \frac{1}{b-a}\int_a^b e^{-(f(x)-g(x))^2} dx$ which have desired properties $F(f,f) = 1$; $F(f,g)\lt 1$ I wandering about a bit sophisticated $F$ that can reveal similarity of two function if: $g(x) = \alpha f(x)$ $g(x) = f(\alpha x)$ $g(x) = \theta (x-a_1) \theta (b_1-x) f(x)$ ($g$ is only 'part' of function $f$) Maybe problem is simpler if functions $f,g$ are defined on finite set ${x_i}$","I need functional $F(f,g)$ to measure similarity of two real-valued functions $f$, $g$ defined on real segment [a, b] As a simple reasonable example we can consider functional $F(f,g) = \frac{1}{b-a}\int_a^b e^{-(f(x)-g(x))^2} dx$ which have desired properties $F(f,f) = 1$; $F(f,g)\lt 1$ I wandering about a bit sophisticated $F$ that can reveal similarity of two function if: $g(x) = \alpha f(x)$ $g(x) = f(\alpha x)$ $g(x) = \theta (x-a_1) \theta (b_1-x) f(x)$ ($g$ is only 'part' of function $f$) Maybe problem is simpler if functions $f,g$ are defined on finite set ${x_i}$",,"['statistics', 'information-theory']"
45,Constructing joint confidence intervals/multiple confidence intervals,Constructing joint confidence intervals/multiple confidence intervals,,"Consider $n$ iid observations $X_1,X_2,\dots ,X_n$ from a $Uniform(a,b)$ distribution, where $a$ and $b$ are both unknown. How do we construct a joint confidence interval for $(a,b)$? I would prefer a rectangular shape confidence interval, which can be obtained by using Bonferroni's method, so I guess the question can also be reformed as: How do we get a confidence interval for $a$?","Consider $n$ iid observations $X_1,X_2,\dots ,X_n$ from a $Uniform(a,b)$ distribution, where $a$ and $b$ are both unknown. How do we construct a joint confidence interval for $(a,b)$? I would prefer a rectangular shape confidence interval, which can be obtained by using Bonferroni's method, so I guess the question can also be reformed as: How do we get a confidence interval for $a$?",,"['statistics', 'probability-distributions', 'uniform-distribution', 'confidence-interval']"
46,How many people should be on a jury to be representative?,How many people should be on a jury to be representative?,,"Assuming a jury is like a sample of a population. Assume the following: The population is a million people. There are X percent who think the defendant is guilty. Choose N random people to be a jury. Given X, how many jurors (what sample size) would you need to give 95% accuracy of the juror decision being the same majority decision as the entire population. I would guess that if X is closer to 50% like 50.1% then you would need a very large jury to make sure you got the accurate result. Whereas if X is closer to 0% or 100% you might need just one juror. So assume the everyone in the population chooses guilty or innocent at random 50/50. (So they might do this and 60% have randomly chosen guilty.) On average how many jurors would you need for them to give the same result as a census 95% of the time? [I added this assumption because we need to know how often the population is split and how often the population is in agreement which will affect the results. But thinking about it this condition would mean the jury would only agree with the population 50% of the time! Right??? So perhaps the condition should be "" Provided above 60% of the population is in agreement the jury should agree with the population 95% of the time. How many jurors are needed in the worst case scenario? "" ]. i.e. is there some way to calculate the best number of people for a jury such that the population as a whole will be happy most (95%) of the time? And justice is ""seen"" to be done?","Assuming a jury is like a sample of a population. Assume the following: The population is a million people. There are X percent who think the defendant is guilty. Choose N random people to be a jury. Given X, how many jurors (what sample size) would you need to give 95% accuracy of the juror decision being the same majority decision as the entire population. I would guess that if X is closer to 50% like 50.1% then you would need a very large jury to make sure you got the accurate result. Whereas if X is closer to 0% or 100% you might need just one juror. So assume the everyone in the population chooses guilty or innocent at random 50/50. (So they might do this and 60% have randomly chosen guilty.) On average how many jurors would you need for them to give the same result as a census 95% of the time? [I added this assumption because we need to know how often the population is split and how often the population is in agreement which will affect the results. But thinking about it this condition would mean the jury would only agree with the population 50% of the time! Right??? So perhaps the condition should be "" Provided above 60% of the population is in agreement the jury should agree with the population 95% of the time. How many jurors are needed in the worst case scenario? "" ]. i.e. is there some way to calculate the best number of people for a jury such that the population as a whole will be happy most (95%) of the time? And justice is ""seen"" to be done?",,['statistics']
47,Bootstrap confidence interval in R using 'replicate' and 'quantile',Bootstrap confidence interval in R using 'replicate' and 'quantile',,"I have around one thousand measurements (numbers). All these measurements are my observations and I have calculated a 95% confidence interval for the mean and for the variance by using the normal formulas (without software). Then, I have used the replicate function in R with around one hundred thousand simulations for my measurements (observations) and the parameter ""replace"" has been set to ""true"". After that, I used the apply function with the three parameters: my measurements as the data, 2 as the margin and mean as the function. Then to get a 95% confidence interval this way, I used the quantile function containing the variable for the apply function I used, and then 0.025 and 0.975 combined as the second parameter for the quantile function. In that way, I got almost exactly the same 95% confidens interval as calculated with the normal formula (without software). So now I wanted to do exactly the same thing for the variance, i.e. use replicate, apply and quantile to get a 95% confidence interval for the variance. So I just changed the third parameter ""mean"" to ""var"" in the apply function. I then noticed that the outputted 95% confidence interval for the variance (from the quantile function) is a little bit different (it is wider) than the one I calculated by the normal formula without software. So my question is: Did I use the replicate, apply and quantile function correctly for the variance confidence interval? I know I did use the functions right for the mean, since I got almost exactly the same result there as calculated by normal formula. If I did use the functions correctly for the variance, why is the confidence interval a little bit different? Is it because as sample size increases, there might be more observations far away from the mean resulting in a bigger variance?","I have around one thousand measurements (numbers). All these measurements are my observations and I have calculated a 95% confidence interval for the mean and for the variance by using the normal formulas (without software). Then, I have used the replicate function in R with around one hundred thousand simulations for my measurements (observations) and the parameter ""replace"" has been set to ""true"". After that, I used the apply function with the three parameters: my measurements as the data, 2 as the margin and mean as the function. Then to get a 95% confidence interval this way, I used the quantile function containing the variable for the apply function I used, and then 0.025 and 0.975 combined as the second parameter for the quantile function. In that way, I got almost exactly the same 95% confidens interval as calculated with the normal formula (without software). So now I wanted to do exactly the same thing for the variance, i.e. use replicate, apply and quantile to get a 95% confidence interval for the variance. So I just changed the third parameter ""mean"" to ""var"" in the apply function. I then noticed that the outputted 95% confidence interval for the variance (from the quantile function) is a little bit different (it is wider) than the one I calculated by the normal formula without software. So my question is: Did I use the replicate, apply and quantile function correctly for the variance confidence interval? I know I did use the functions right for the mean, since I got almost exactly the same result there as calculated by normal formula. If I did use the functions correctly for the variance, why is the confidence interval a little bit different? Is it because as sample size increases, there might be more observations far away from the mean resulting in a bigger variance?",,"['statistics', 'math-software', 'confidence-interval']"
48,Calculating median (second quartile) using formula for quartiles in an interval,Calculating median (second quartile) using formula for quartiles in an interval,,"I have the following formula in my lecture notes for calculating the $j^{th}$ quartile in an interval: $$Q_j=L_l+I\frac{N \frac{j}{4}-F_l}{f_l}, j=1,2,3$$ Where $L_l$ is the left side of the interval in which the quartile is. The interval in which the quartile is, is defined as the interval before the sum of the frequencies is bellow $N\frac{j}{4}$. $F_l$ is the sum of the frequencies in the intervals before $L_l$ $f_l$ is the frequency in the interval of the median I is the length of the interval which contains the median My question is: If this formula is correct, how do I calculate the second quartile which is the median, when the formula is actually using information about the median ($f_l$ is the frequency in the interval of the median)? Also, what is the intuition behind this formula (if it's correct of course)?","I have the following formula in my lecture notes for calculating the $j^{th}$ quartile in an interval: $$Q_j=L_l+I\frac{N \frac{j}{4}-F_l}{f_l}, j=1,2,3$$ Where $L_l$ is the left side of the interval in which the quartile is. The interval in which the quartile is, is defined as the interval before the sum of the frequencies is bellow $N\frac{j}{4}$. $F_l$ is the sum of the frequencies in the intervals before $L_l$ $f_l$ is the frequency in the interval of the median I is the length of the interval which contains the median My question is: If this formula is correct, how do I calculate the second quartile which is the median, when the formula is actually using information about the median ($f_l$ is the frequency in the interval of the median)? Also, what is the intuition behind this formula (if it's correct of course)?",,"['statistics', 'descriptive-statistics']"
49,how to find The median of a Grouped data when the sum of the frequency is odd?,how to find The median of a Grouped data when the sum of the frequency is odd?,,how to find The median of a Grouped data when the sum of the frequency is odd? Can anyone explain me with an example. I have searched a lot. But each example in the internet is done with even number. I want to know what to do for the case of odd.,how to find The median of a Grouped data when the sum of the frequency is odd? Can anyone explain me with an example. I have searched a lot. But each example in the internet is done with even number. I want to know what to do for the case of odd.,,['statistics']
50,PDF from MGF inverse not triviAL,PDF from MGF inverse not triviAL,,"$$\frac{128 r t \, _1F_2\left(2;\frac{5}{2},\frac{7}{2};r^2 t^2\right)}{45 \pi } + \frac{2 r t+4 r t I_2(2 r t)-2 I_1(2 r t)}{r^3 t^3}$$ My computer and I found that this is the moment generating function of a distribution rooted in the geometric and statistical process of finding the mean distance between two points inside and/or on a disk of radius $r$, $r>0.$ I can factor: $$\frac{2 \left(64 r^4 t^4 \, _1F_2\left(2;\frac{5}{2},\frac{7}{2};r^2 t^2\right)+45 \pi  r t+90 \pi  r t I_2(2 r t)-45 \pi  I_1(2 r t)\right)}{45 \pi  r^3 t^3}$$ That makes it at least the product of moments so I can look for the PDF for the denominator.  Question is this: What PDF corresponds to the MGF I found?","$$\frac{128 r t \, _1F_2\left(2;\frac{5}{2},\frac{7}{2};r^2 t^2\right)}{45 \pi } + \frac{2 r t+4 r t I_2(2 r t)-2 I_1(2 r t)}{r^3 t^3}$$ My computer and I found that this is the moment generating function of a distribution rooted in the geometric and statistical process of finding the mean distance between two points inside and/or on a disk of radius $r$, $r>0.$ I can factor: $$\frac{2 \left(64 r^4 t^4 \, _1F_2\left(2;\frac{5}{2},\frac{7}{2};r^2 t^2\right)+45 \pi  r t+90 \pi  r t I_2(2 r t)-45 \pi  I_1(2 r t)\right)}{45 \pi  r^3 t^3}$$ That makes it at least the product of moments so I can look for the PDF for the denominator.  Question is this: What PDF corresponds to the MGF I found?",,['statistics']
51,Choosing the right correlation test for the given variables [closed],Choosing the right correlation test for the given variables [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 years ago . Improve this question I need to choose between Pearson, Spearman and Kendall's tau correlations for the given varibles $A$ and $B$: $A,B$ are continuous with importance to value $A,B$ are ordinal variables $A$ is continuous and $B$ is ordinal $A,B$ are continuous with importance to order My try: The only correlation test which uses values is Pearson . It seems to me that Kendall's tau best fits though Spearman could be used as well. I've understood that Spearman is good for both continuous and ordinal, but technically we could use Kendall's tau, can't we? Kendall's tau - since the order is important So as you can see I'm somewhat confused regarding choosing the right test and I'd be glad if you could help me understand what are the appropriate choices. Thanks","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 years ago . Improve this question I need to choose between Pearson, Spearman and Kendall's tau correlations for the given varibles $A$ and $B$: $A,B$ are continuous with importance to value $A,B$ are ordinal variables $A$ is continuous and $B$ is ordinal $A,B$ are continuous with importance to order My try: The only correlation test which uses values is Pearson . It seems to me that Kendall's tau best fits though Spearman could be used as well. I've understood that Spearman is good for both continuous and ordinal, but technically we could use Kendall's tau, can't we? Kendall's tau - since the order is important So as you can see I'm somewhat confused regarding choosing the right test and I'd be glad if you could help me understand what are the appropriate choices. Thanks",,"['statistics', 'machine-learning', 'correlation']"
52,Poisson process with stochastic intensity correlated with a Brownian Motion,Poisson process with stochastic intensity correlated with a Brownian Motion,,"I am currently confused with the moment of non-homogeneous compound Poisson process and a Brownian Motion. I know that generally Poisson Process and Brownian Motion are independent if they are adapted to the same filtration. But what if the intensity of the Poisson Process and the Brownian Motion are correlated? For example, we have a CIR process $dX(t)=\kappa_1\Big(\theta_1-X(t)\Big)dt+\sigma_1\sqrt{X(t)}dW_1(t)$, and a non-homogeneous compound Poisson Process $J(t)=\int_0^t\int_{\mathbb{R}^+}Q\mu(dx,ds)$, with a stochastic intensity $\lambda$ satisfies $d\lambda(t)=\kappa_2\Big(\theta_2-\lambda(t)\Big)dt+\sigma_2\sqrt{\lambda(t)}dW_2(t)$ (another CIR process), and its jump size $Q$ has a normal distribution $N(\mu_Q, \sigma_Q^2)$, the correlation coefficient between $W_1(t)$ and $W_2(t)$ is $\rho$, but the jump size is independent of them. BTW, $\mu(dx,ds)$ is a Poisson random measure, $\tilde{\mu}(dx,ds)$ is a compensated Poisson measure. Question is, what exactly are the mixed moment of $X(t)$ and $J(t)$, i.e. $\mathbb{E}[X(t)J(t)]$, and $\mathbb{E}[X(t)\int_0^t\int_{\mathbb{R}^+}Q\tilde{\mu}(dx,ds)]$?","I am currently confused with the moment of non-homogeneous compound Poisson process and a Brownian Motion. I know that generally Poisson Process and Brownian Motion are independent if they are adapted to the same filtration. But what if the intensity of the Poisson Process and the Brownian Motion are correlated? For example, we have a CIR process $dX(t)=\kappa_1\Big(\theta_1-X(t)\Big)dt+\sigma_1\sqrt{X(t)}dW_1(t)$, and a non-homogeneous compound Poisson Process $J(t)=\int_0^t\int_{\mathbb{R}^+}Q\mu(dx,ds)$, with a stochastic intensity $\lambda$ satisfies $d\lambda(t)=\kappa_2\Big(\theta_2-\lambda(t)\Big)dt+\sigma_2\sqrt{\lambda(t)}dW_2(t)$ (another CIR process), and its jump size $Q$ has a normal distribution $N(\mu_Q, \sigma_Q^2)$, the correlation coefficient between $W_1(t)$ and $W_2(t)$ is $\rho$, but the jump size is independent of them. BTW, $\mu(dx,ds)$ is a Poisson random measure, $\tilde{\mu}(dx,ds)$ is a compensated Poisson measure. Question is, what exactly are the mixed moment of $X(t)$ and $J(t)$, i.e. $\mathbb{E}[X(t)J(t)]$, and $\mathbb{E}[X(t)\int_0^t\int_{\mathbb{R}^+}Q\tilde{\mu}(dx,ds)]$?",,"['probability', 'statistics', 'stochastic-processes']"
53,What is the cutting point of two distributions?,What is the cutting point of two distributions?,,"I just want to know what is the cutting point between the predictive bayesian distribution and the predictive frequentist distribution (plugin) (the one that uses the MLE). I will only consider distributions with positive support. This is my attempt: Let $x_f^{*}$ be the point such that \begin{align} & \int_0^\infty f (x_f^{*} \mid \theta) \pi(\theta\mid \textbf{x}) \, d\theta = f(x_f^{*}\mid\hat{\theta}) \\[10pt] \iff & \int_0^\infty f (x_f^{*} \mid \theta)\pi(\theta\mid\textbf{x}) \, d\theta - f(x_f^{*}\mid\hat{\theta}) = 0 \\[10pt] \iff & \int_0^\infty f (x_f^{*} \mid \theta)\pi(\theta\mid\textbf{x}) \, d\theta - \int_0^\infty f(x_f^{*}\mid\hat{\theta})\pi(\theta\mid\textbf{x})\,d\theta = 0 \\[10pt] \iff & \int_0^\infty [f (x_f^{*} \mid \theta) - f(x_f^{*} \mid \hat{\theta})] \pi(\theta\mid\textbf{x})\,d\theta = 0 \end{align} Hence, $f(x^{*}_f\mid\theta) = f(x^{*}_f\mid\hat{\theta})$. Also by the multivariate and unbounded version of the mean value theorem (Apostol analysis book) $$ \int_0^\infty f (x_f \mid \theta)\pi(\theta\mid\textbf{x})\,d\theta = f(x_f \mid c) \int_0^\infty\pi(\theta\mid\textbf{x}) \, d\theta \text{ for some } c \in [\inf f(x_f \mid \theta), \sup f(x_f\mid\theta)] $$ Since $\pi(\theta\mid\textbf{x})$ is a posterior distribution and integrates one, $$ f(x_f\mid c)\int_0^\infty \pi(\theta\mid\textbf{x}) \, d\theta = f(x_f\mid c) = f(x_f\mid \hat{\theta}) \Rightarrow c = \hat{\theta} $$ But I make an example (the exponential and they do not cut at the MLE). What is wrong here ?","I just want to know what is the cutting point between the predictive bayesian distribution and the predictive frequentist distribution (plugin) (the one that uses the MLE). I will only consider distributions with positive support. This is my attempt: Let $x_f^{*}$ be the point such that \begin{align} & \int_0^\infty f (x_f^{*} \mid \theta) \pi(\theta\mid \textbf{x}) \, d\theta = f(x_f^{*}\mid\hat{\theta}) \\[10pt] \iff & \int_0^\infty f (x_f^{*} \mid \theta)\pi(\theta\mid\textbf{x}) \, d\theta - f(x_f^{*}\mid\hat{\theta}) = 0 \\[10pt] \iff & \int_0^\infty f (x_f^{*} \mid \theta)\pi(\theta\mid\textbf{x}) \, d\theta - \int_0^\infty f(x_f^{*}\mid\hat{\theta})\pi(\theta\mid\textbf{x})\,d\theta = 0 \\[10pt] \iff & \int_0^\infty [f (x_f^{*} \mid \theta) - f(x_f^{*} \mid \hat{\theta})] \pi(\theta\mid\textbf{x})\,d\theta = 0 \end{align} Hence, $f(x^{*}_f\mid\theta) = f(x^{*}_f\mid\hat{\theta})$. Also by the multivariate and unbounded version of the mean value theorem (Apostol analysis book) $$ \int_0^\infty f (x_f \mid \theta)\pi(\theta\mid\textbf{x})\,d\theta = f(x_f \mid c) \int_0^\infty\pi(\theta\mid\textbf{x}) \, d\theta \text{ for some } c \in [\inf f(x_f \mid \theta), \sup f(x_f\mid\theta)] $$ Since $\pi(\theta\mid\textbf{x})$ is a posterior distribution and integrates one, $$ f(x_f\mid c)\int_0^\infty \pi(\theta\mid\textbf{x}) \, d\theta = f(x_f\mid c) = f(x_f\mid \hat{\theta}) \Rightarrow c = \hat{\theta} $$ But I make an example (the exponential and they do not cut at the MLE). What is wrong here ?",,"['statistics', 'bayesian', 'distribution-tails']"
54,When is Pinsker’s inequality tight?,When is Pinsker’s inequality tight?,,"Let $P(x)$ and $Q(x)$ be two distributions of $\mathbf{x}$. Pinsker’s inequality says $$D(P(x)\|Q(x)) \geq \frac{1}{2 \ln 2} \|P(x) - Q(x)\|_1^2,$$ where $\|\cdot\|_1$ is the $\mathcal{L}^1$ norm, and $D(P(x)\|Q(x))$ is the KL-divergence $$D(P(x)\|Q(x)) = \int_{\mathcal{X}} P(x)\log_2 \frac{P(x)}{Q(x)} dx.$$ Given that $\int x^2 P(x) dx \neq \int x^2 Q(x) dx$ (Second moment constraint). My question is when Pinsker’s inequality is tight. Thanks! PS: My previous question is without the second moment contraint. In that case, $P = Q$ (a.e.) can make Pinsker's inequality tight.","Let $P(x)$ and $Q(x)$ be two distributions of $\mathbf{x}$. Pinsker’s inequality says $$D(P(x)\|Q(x)) \geq \frac{1}{2 \ln 2} \|P(x) - Q(x)\|_1^2,$$ where $\|\cdot\|_1$ is the $\mathcal{L}^1$ norm, and $D(P(x)\|Q(x))$ is the KL-divergence $$D(P(x)\|Q(x)) = \int_{\mathcal{X}} P(x)\log_2 \frac{P(x)}{Q(x)} dx.$$ Given that $\int x^2 P(x) dx \neq \int x^2 Q(x) dx$ (Second moment constraint). My question is when Pinsker’s inequality is tight. Thanks! PS: My previous question is without the second moment contraint. In that case, $P = Q$ (a.e.) can make Pinsker's inequality tight.",,"['probability-theory', 'statistics', 'hypothesis-testing']"
55,Method to estimate variance of sample mean for correlated data,Method to estimate variance of sample mean for correlated data,,"I have $m$ weakly stationary observations $X_1,X_2,\cdots,X_m$. I don't know anything else about the observations. I want to estimate the variance of the sample mean. At first, my idea was to use nonparametric bootstrapping to do this. But I learnt that this method doesn't work for correlated data. What are the most easy, standard ways of doing this to get reliable estimates?","I have $m$ weakly stationary observations $X_1,X_2,\cdots,X_m$. I don't know anything else about the observations. I want to estimate the variance of the sample mean. At first, my idea was to use nonparametric bootstrapping to do this. But I learnt that this method doesn't work for correlated data. What are the most easy, standard ways of doing this to get reliable estimates?",,"['statistics', 'estimation', 'parameter-estimation']"
56,Proving the independence of the given statistics,Proving the independence of the given statistics,,"Let $(X_1, Y_1), ..., (X_n, Y_n)   (n>2)$ be random samples from the bivariate normal distribution $N_2 (\mu_1, \mu_2; \sigma_1^2, \sigma_2^2, \rho)$, where $\sigma_1>0, \sigma_2>0, -1<\rho<1$. Let $r$ be the sample correlation coefficient defined as $r= \frac{\sum_1^n (X_i - \bar X )(Y_i - \bar Y)}{\sqrt{\sum_1^n (X_i-\bar X)^2}\sqrt{\sum_1^n (Y_i -\bar Y)^2}}$ where $\bar X, \bar Y$ are sample means of $X_i$'s and $Y_i$'s, respectively. Also, let $Z_i = (X_i-\mu_1)/\sigma_1$, $W_i=\{(Y_i-\mu_2)/\sigma_2-\rho(X_i-\mu_1)/\sigma_1\}/\sqrt{1-\rho^2}$ for $i=1, ..., n$. Now prove the followings: (a) $S_{WW}-S_{ZW}^2/S_{ZZ} $ and $Z=(Z_1, ..., Z_n)$ are independent. (b) $S_{ZW}/\sqrt{S_{ZZ}} $ and $Z$ are independent. (c) $S_{WW}-S_{ZW}^2/S_{ZZ}, S_{ZW}/\sqrt{S_{ZZ}}, S_{ZZ}$ are indenepdent. Where $S_{ZZ} = \sum_1^n (Z_i-\bar Z)^2$, $S_{WW} = \sum_1^n (W_i-\bar W)^2$, $S_{ZW} = \sum_1^n (Z_i-\bar Z)(W_i-\bar W)$. What I know is that if $T=((Z_1-\bar Z)/\sqrt{S_{ZZ}}, ..., (Z_n-\bar Z)/\sqrt{S_{ZZ}}$), then $S_{WW}-S_{ZW}^2/S_{ZZ}=W^t(I-n^{-1} 11^t-TT^t)W$, where $1$ is the vector where all entries are 1. Also, it is known that $Z=(Z_1, ..., Z_n)$ and $W=(W_1, ..., W_n)$ are independent. How should I prove the independence this case? Using pdf or mgf to prove independence here seems ridiculous.","Let $(X_1, Y_1), ..., (X_n, Y_n)   (n>2)$ be random samples from the bivariate normal distribution $N_2 (\mu_1, \mu_2; \sigma_1^2, \sigma_2^2, \rho)$, where $\sigma_1>0, \sigma_2>0, -1<\rho<1$. Let $r$ be the sample correlation coefficient defined as $r= \frac{\sum_1^n (X_i - \bar X )(Y_i - \bar Y)}{\sqrt{\sum_1^n (X_i-\bar X)^2}\sqrt{\sum_1^n (Y_i -\bar Y)^2}}$ where $\bar X, \bar Y$ are sample means of $X_i$'s and $Y_i$'s, respectively. Also, let $Z_i = (X_i-\mu_1)/\sigma_1$, $W_i=\{(Y_i-\mu_2)/\sigma_2-\rho(X_i-\mu_1)/\sigma_1\}/\sqrt{1-\rho^2}$ for $i=1, ..., n$. Now prove the followings: (a) $S_{WW}-S_{ZW}^2/S_{ZZ} $ and $Z=(Z_1, ..., Z_n)$ are independent. (b) $S_{ZW}/\sqrt{S_{ZZ}} $ and $Z$ are independent. (c) $S_{WW}-S_{ZW}^2/S_{ZZ}, S_{ZW}/\sqrt{S_{ZZ}}, S_{ZZ}$ are indenepdent. Where $S_{ZZ} = \sum_1^n (Z_i-\bar Z)^2$, $S_{WW} = \sum_1^n (W_i-\bar W)^2$, $S_{ZW} = \sum_1^n (Z_i-\bar Z)(W_i-\bar W)$. What I know is that if $T=((Z_1-\bar Z)/\sqrt{S_{ZZ}}, ..., (Z_n-\bar Z)/\sqrt{S_{ZZ}}$), then $S_{WW}-S_{ZW}^2/S_{ZZ}=W^t(I-n^{-1} 11^t-TT^t)W$, where $1$ is the vector where all entries are 1. Also, it is known that $Z=(Z_1, ..., Z_n)$ and $W=(W_1, ..., W_n)$ are independent. How should I prove the independence this case? Using pdf or mgf to prove independence here seems ridiculous.",,"['statistics', 'normal-distribution', 'independence']"
57,Make a covariance matrix have a larger eigenvalue for one eigenvector,Make a covariance matrix have a larger eigenvalue for one eigenvector,,"Suppose I have a covariance matrix $\Sigma$, with eigenvalues $u_1,u_2,u_3$ and eigenvectors $v_1,v_2,v_3$. How can I use that information to generate a new covariance matrix $\Sigma_2$ with the same eigenvectors but with some new eigenvalues $\alpha u_1, u_2, u_3$?","Suppose I have a covariance matrix $\Sigma$, with eigenvalues $u_1,u_2,u_3$ and eigenvectors $v_1,v_2,v_3$. How can I use that information to generate a new covariance matrix $\Sigma_2$ with the same eigenvectors but with some new eigenvalues $\alpha u_1, u_2, u_3$?",,"['linear-algebra', 'statistics', 'eigenvalues-eigenvectors', 'covariance']"
58,Representation of arbitrary probability distributions by latent variable models with gaussian posteriors and priors,Representation of arbitrary probability distributions by latent variable models with gaussian posteriors and priors,,"A probability distribution of a random variable $\mathbf x$ may be expressed as $ P(x)=\int P(x|z) P(z) dz$ where $\mathbf z$ is a latent random variable. In models such as Variational Autoencoders (VAEs) it is common to let $\mathbf z \sim \mathcal{N}(0,I)$ and $P(x|z) = \mathcal{N}\left(x;\mu(z),\Sigma(z)\right)$. However, it is not obvious to me whether those kind of models are able to represent any arbitrary probability distribution $P(x)$ by varying $\mu(z)$ and $\Sigma(z)$, and if that is the case, why it is possible. Do you have any intuition or references where this is proven? Thank you very much","A probability distribution of a random variable $\mathbf x$ may be expressed as $ P(x)=\int P(x|z) P(z) dz$ where $\mathbf z$ is a latent random variable. In models such as Variational Autoencoders (VAEs) it is common to let $\mathbf z \sim \mathcal{N}(0,I)$ and $P(x|z) = \mathcal{N}\left(x;\mu(z),\Sigma(z)\right)$. However, it is not obvious to me whether those kind of models are able to represent any arbitrary probability distribution $P(x)$ by varying $\mu(z)$ and $\Sigma(z)$, and if that is the case, why it is possible. Do you have any intuition or references where this is proven? Thank you very much",,"['statistics', 'probability-distributions', 'statistical-inference', 'machine-learning']"
59,Is it true that the marginal expectation of a probablity density function is still a probablity density?,Is it true that the marginal expectation of a probablity density function is still a probablity density?,,"Let $p(x,y),x,y\in \Bbb R$ be a probability density function. Let $q(x),x \in \Bbb R$ be another density. Is $f(y)=\Bbb E_q[p(x,y)] = \int q(x)p(x,y)dx$ a density function? Is $f(y)=\exp\{\Bbb E_q [\log p(x,y)]\}$ a density function? I am not sure, but my hunch is that it is not necessarily true. Take 1.  for example, if it is probability density, then $\int f(y)dy=\int{\int q(x)p(x,y)dxdy}=\int q(x) (\int p(x,y)dy) dx=\int q(x) p_X(x) dx=1$ where $p_X$ is the marginal density of $p$. Since $p,q$ has no relation, I think the product of two arbitrary density functions might not be a density. I would appreciate it if someone can help with concrete counterexamples.","Let $p(x,y),x,y\in \Bbb R$ be a probability density function. Let $q(x),x \in \Bbb R$ be another density. Is $f(y)=\Bbb E_q[p(x,y)] = \int q(x)p(x,y)dx$ a density function? Is $f(y)=\exp\{\Bbb E_q [\log p(x,y)]\}$ a density function? I am not sure, but my hunch is that it is not necessarily true. Take 1.  for example, if it is probability density, then $\int f(y)dy=\int{\int q(x)p(x,y)dxdy}=\int q(x) (\int p(x,y)dy) dx=\int q(x) p_X(x) dx=1$ where $p_X$ is the marginal density of $p$. Since $p,q$ has no relation, I think the product of two arbitrary density functions might not be a density. I would appreciate it if someone can help with concrete counterexamples.",,"['probability', 'statistics']"
60,Recursive formula for Multivariate Normal distribution,Recursive formula for Multivariate Normal distribution,,"i have a multivariate normal distribution with pdf as is shown here: https://en.wikipedia.org/wiki/Multivariate_normal_distribution i want to find the cdf, but there is no close type. I'm trying to find a recursive formula for the multivariate normal distribution to use instead but still, no close type found. Can someone help me? I want to use it to find the inverse $F(x)$.","i have a multivariate normal distribution with pdf as is shown here: https://en.wikipedia.org/wiki/Multivariate_normal_distribution i want to find the cdf, but there is no close type. I'm trying to find a recursive formula for the multivariate normal distribution to use instead but still, no close type found. Can someone help me? I want to use it to find the inverse $F(x)$.",,"['probability', 'statistics', 'functions']"
61,Calculating First Passage time in a Birth-Death Process,Calculating First Passage time in a Birth-Death Process,,"I am trying to find the mean first passage time in a birth-death process to get from state 2 to state 1. Here is the question in detail: We are in a birth-death system, so the state space corresponds to the natural numbers. (The states are 0,1,2,3,...) Additionally, the system can only move from its current state to an adjacent state (If in state n, the next state can only be n-1 or n+1), unless its current state is 0, in which case it can only go up to 1. We have a birthrate $\lambda$ and deathrate $\mu$ for the current state n. $\lambda=A$ $\mu=nB$ Where A and B are real numbers. What I want to find is the mean first passage time to get from state 2 to state 1. I'm not highly familiar with Markov processes or statistics in general, but I am math-literate and would like to understand the derivation. If someone could explain it in a way that is thorough, but doesn't assume too much foreknowledge, that would be greatly appreciated. Thanks!","I am trying to find the mean first passage time in a birth-death process to get from state 2 to state 1. Here is the question in detail: We are in a birth-death system, so the state space corresponds to the natural numbers. (The states are 0,1,2,3,...) Additionally, the system can only move from its current state to an adjacent state (If in state n, the next state can only be n-1 or n+1), unless its current state is 0, in which case it can only go up to 1. We have a birthrate $\lambda$ and deathrate $\mu$ for the current state n. $\lambda=A$ $\mu=nB$ Where A and B are real numbers. What I want to find is the mean first passage time to get from state 2 to state 1. I'm not highly familiar with Markov processes or statistics in general, but I am math-literate and would like to understand the derivation. If someone could explain it in a way that is thorough, but doesn't assume too much foreknowledge, that would be greatly appreciated. Thanks!",,"['statistics', 'markov-process', 'birth-death-process']"
62,"MVUE of Bernoulli r.v. when $p\in [1/2,1)$",MVUE of Bernoulli r.v. when,"p\in [1/2,1)","$X_1, X_2\dots , X_n$ are independent Bernoulli r.v. s.t. $P(X_i=1)=p$ , $i=1,2,\dots , n$ , where $p\in [1/2,1)$ . (a) Is $T=\frac{1}{n} \sum_{i=1}^{n}X_i$ MVUE of $p$ ? Justify it. It can be easily shown that $T$ is MVUE if $p\in (0,1)$ , since $T$ is complete sufficient statistics(as belongs to exponential family) and unbiased estimator by Lehmann-Scheffe theorem. But here $p\in(1/2,1]$ . I don't think the result would be same(or may be same). I am confused!! What I did is that: Let $T^*=a\space \text{if $T<.5$ and}\\ \space\space\space\space\space\space\space\space bT\space \text{if $T\geq.5$}$ $a,b$ are such that $T^*$ is u.e. of $p.$ Then I tried to find $a,b$ such that $Var(T^*)<Var(T)$ . But I don't think this is right.","are independent Bernoulli r.v. s.t. , , where . (a) Is MVUE of ? Justify it. It can be easily shown that is MVUE if , since is complete sufficient statistics(as belongs to exponential family) and unbiased estimator by Lehmann-Scheffe theorem. But here . I don't think the result would be same(or may be same). I am confused!! What I did is that: Let are such that is u.e. of Then I tried to find such that . But I don't think this is right.","X_1, X_2\dots , X_n P(X_i=1)=p i=1,2,\dots , n p\in [1/2,1) T=\frac{1}{n} \sum_{i=1}^{n}X_i p T p\in (0,1) T p\in(1/2,1] T^*=a\space \text{if T<.5 and}\\ \space\space\space\space\space\space\space\space bT\space \text{if T\geq.5} a,b T^* p. a,b Var(T^*)<Var(T)","['statistics', 'statistical-inference', 'parameter-estimation']"
63,Sampling from a truncated PDF,Sampling from a truncated PDF,,"I have a PDF $f$ that I know how to sample from, and I want to sample from the PDF $$ g(x) = \frac{f(x)}{\bar{F}(s)} \mathbb{1}_{(x>s)}$$ where $s > 0$, $\bar{F}(s) = \mathbb{P}(X>s)$, and $\mathbb{1}_{(\cdot)}$ is the indicator function. Can I sample from $f$ and accept only samples that are greater than $s$ to be samples form $g$?","I have a PDF $f$ that I know how to sample from, and I want to sample from the PDF $$ g(x) = \frac{f(x)}{\bar{F}(s)} \mathbb{1}_{(x>s)}$$ where $s > 0$, $\bar{F}(s) = \mathbb{P}(X>s)$, and $\mathbb{1}_{(\cdot)}$ is the indicator function. Can I sample from $f$ and accept only samples that are greater than $s$ to be samples form $g$?",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'sampling']"
64,How to prove that some combinatorical function has minimum on borders?,How to prove that some combinatorical function has minimum on borders?,,"One maximum likelihood estimation is calculated according to equation: $$\widehat{\ell}=\operatorname*{argmin}_\ell\mathbb{E}f(\ell,k),$$ where $f(\ell,k)$ is a function of two variables: $$f(\ell,k)=\binom \ell k \binom {n-\ell} {m-k}.$$ One of arguments $k$ is random variable with hypergeometric distribution: $$P(k)=\frac{\binom {\ell-g_1}{k-g_2}\binom{h_1-\ell+g_1}{h_2-k+g_2}}{\binom {h_1}{h_2}}.$$ This probability has values more then zero on interval $k\in[g_2,g_2+h_2]$. I want to prove that if $\ell\in[g_1,g_1+h_1]$, then minimum of objective function locates on one of the borders: $$\min\limits_\ell\mathbb{E}f(\ell,k)=\min\left(\binom {g_1} {g_2}\binom {n-g_1} {m-g_2},\binom {g_1+h_1} {g_2+h_2}\binom {n-g_1-h_1} {m-g_2-h_2}\right),$$ where all variables and constants in binomial cooefs are nonnegative integers. To prove it I tried to calculate: $$\mathbb{E}f(\ell,k)=\sum\limits_k f(\ell,k)P(k)$$ $$\mathbb{E}f(\ell,k)={\binom {h_1}{h_2}}^{-1} \sum\limits_{k=\max(g_2,g_2+h_2+\ell-g_1-h_1)}^{\min(\ell-g_1,h_2)} \binom \ell k \binom {n-\ell} {m-k}\binom {\ell-g_1}{k-g_2}\binom{h_1-\ell+g_1}{h_2-k+g_2}$$ But when I read ""Concrete mathematics"" I can not find some formula or trick to use in this sum. It looks like the some modification of Vandermonde's identity, but ... how to use this to get closed form of this? It is possible that closed form doesn't exist. Another question is how to find this descrete function extremum? Am I going right way trying to calculate last expression? May be, there exist another way to prove that minimum is on borders. Actually, I use log-likelihood instead likelihood (first equation) to estimate $\ell$ beacuse this helps to avoids big integers. I'm not sure about the equation: $$\operatorname*{argmin}\limits_\ell\mathbb{E}\log{f(\ell,k)}=\operatorname*{argmin}\limits_\ell \mathbb{E}f(\ell,k)$$ To ilustrate this problem in terms of log-likelihood I posted this function realization and sample mean over 100 realizations. We can see that hypothesis i want to prove can be statistically accepted. Existing of proof is my question to mathematical community. Thank you for any help or comments!","One maximum likelihood estimation is calculated according to equation: $$\widehat{\ell}=\operatorname*{argmin}_\ell\mathbb{E}f(\ell,k),$$ where $f(\ell,k)$ is a function of two variables: $$f(\ell,k)=\binom \ell k \binom {n-\ell} {m-k}.$$ One of arguments $k$ is random variable with hypergeometric distribution: $$P(k)=\frac{\binom {\ell-g_1}{k-g_2}\binom{h_1-\ell+g_1}{h_2-k+g_2}}{\binom {h_1}{h_2}}.$$ This probability has values more then zero on interval $k\in[g_2,g_2+h_2]$. I want to prove that if $\ell\in[g_1,g_1+h_1]$, then minimum of objective function locates on one of the borders: $$\min\limits_\ell\mathbb{E}f(\ell,k)=\min\left(\binom {g_1} {g_2}\binom {n-g_1} {m-g_2},\binom {g_1+h_1} {g_2+h_2}\binom {n-g_1-h_1} {m-g_2-h_2}\right),$$ where all variables and constants in binomial cooefs are nonnegative integers. To prove it I tried to calculate: $$\mathbb{E}f(\ell,k)=\sum\limits_k f(\ell,k)P(k)$$ $$\mathbb{E}f(\ell,k)={\binom {h_1}{h_2}}^{-1} \sum\limits_{k=\max(g_2,g_2+h_2+\ell-g_1-h_1)}^{\min(\ell-g_1,h_2)} \binom \ell k \binom {n-\ell} {m-k}\binom {\ell-g_1}{k-g_2}\binom{h_1-\ell+g_1}{h_2-k+g_2}$$ But when I read ""Concrete mathematics"" I can not find some formula or trick to use in this sum. It looks like the some modification of Vandermonde's identity, but ... how to use this to get closed form of this? It is possible that closed form doesn't exist. Another question is how to find this descrete function extremum? Am I going right way trying to calculate last expression? May be, there exist another way to prove that minimum is on borders. Actually, I use log-likelihood instead likelihood (first equation) to estimate $\ell$ beacuse this helps to avoids big integers. I'm not sure about the equation: $$\operatorname*{argmin}\limits_\ell\mathbb{E}\log{f(\ell,k)}=\operatorname*{argmin}\limits_\ell \mathbb{E}f(\ell,k)$$ To ilustrate this problem in terms of log-likelihood I posted this function realization and sample mean over 100 realizations. We can see that hypothesis i want to prove can be statistically accepted. Existing of proof is my question to mathematical community. Thank you for any help or comments!",,"['combinatorics', 'algebra-precalculus', 'statistics', 'combinatorial-proofs']"
65,How to express a hypergeometric function as beta and/or gamma functions,How to express a hypergeometric function as beta and/or gamma functions,,"The CDF of the student t distribution can be represented by $$\frac{1}{2} \cdot \beta_{x^2/(x^2+v)}\left(\frac 1 2,\frac v 2\right); \qquad x\in[-\sqrt{v},0]$$ Where we have a t distribution with $v>0$ degrees of freedom and $\beta_z(a,b)$ is the incomplete beta function. I'm interested in it's derivative w.r.t. $v$, which happens to contain the term $$_3\tilde{F}_2\left(\frac{v}{2},\frac{v}{2},-\frac{1}{2};\frac{v}{2}+1,\frac{v}{2}+1;1-\frac{x^2}{x^2+v}\right)$$ I was able to get to this point using wolfram's derivatives of the incomplete beta function . However, for this to be useful to me, I would like to express it in terms of functions I'm more familiar with such as the gamma, polygamma, and beta functions. How might this be done? Also, how do the $\tilde{F}$ functions differ from the $F$ functions, and what are the $\tilde{F}$ functions called? I couldn't find any information on them at all.","The CDF of the student t distribution can be represented by $$\frac{1}{2} \cdot \beta_{x^2/(x^2+v)}\left(\frac 1 2,\frac v 2\right); \qquad x\in[-\sqrt{v},0]$$ Where we have a t distribution with $v>0$ degrees of freedom and $\beta_z(a,b)$ is the incomplete beta function. I'm interested in it's derivative w.r.t. $v$, which happens to contain the term $$_3\tilde{F}_2\left(\frac{v}{2},\frac{v}{2},-\frac{1}{2};\frac{v}{2}+1,\frac{v}{2}+1;1-\frac{x^2}{x^2+v}\right)$$ I was able to get to this point using wolfram's derivatives of the incomplete beta function . However, for this to be useful to me, I would like to express it in terms of functions I'm more familiar with such as the gamma, polygamma, and beta functions. How might this be done? Also, how do the $\tilde{F}$ functions differ from the $F$ functions, and what are the $\tilde{F}$ functions called? I couldn't find any information on them at all.",,"['complex-analysis', 'statistics']"
66,Probabilistic or Statistical Interepretation of Central Binomial Identity,Probabilistic or Statistical Interepretation of Central Binomial Identity,,What is probabilistic or statistical interpretation of the identity given below?  \begin{equation}\label{BIIntro1} \sum_{k=0}^{n}\binom{2k}{k}\binom{2n-2k}{n-k}=4^{n} \end{equation},What is probabilistic or statistical interpretation of the identity given below?  \begin{equation}\label{BIIntro1} \sum_{k=0}^{n}\binom{2k}{k}\binom{2n-2k}{n-k}=4^{n} \end{equation},,"['probability', 'combinatorics', 'statistics', 'permutations', 'combinations']"
67,variance/confidence interval of development patterns,variance/confidence interval of development patterns,,"I was wondering if there is a technique to calculate the variance of developments. eg. I have paired data eg: 100 110, 200 225, 78 92, 33 20... This means the developments are: 1.1, 1.125, 1.18, 0.6... I was wondering if there is a technique to calculate a confidence interval around this. And I was also wondering if there is a way to weight this as well (eg. instead of assuming each development is equal, weigh the 200-225 development twice as much as the 100-110 one.","I was wondering if there is a technique to calculate the variance of developments. eg. I have paired data eg: 100 110, 200 225, 78 92, 33 20... This means the developments are: 1.1, 1.125, 1.18, 0.6... I was wondering if there is a technique to calculate a confidence interval around this. And I was also wondering if there is a way to weight this as well (eg. instead of assuming each development is equal, weigh the 200-225 development twice as much as the 100-110 one.",,"['probability', 'statistics']"
68,Convergence in distribution to the standard normal using Cramer-Rao,Convergence in distribution to the standard normal using Cramer-Rao,,"Given is a a random sample $X_1,\ldots,X_n$ from a distribution with a pdf $f(x;\theta) = 2\theta^{-1} x^{(2/\theta)-1}$ for $0<x<1$, zero otherwise. We know that maximum likelihood estimator is $\hat{\theta} = -\dfrac 2 n \sum_i \log X_i$. Determine $c$ such that $c\left(\dfrac{n}{-2\sum_i \log X_i} - \dfrac{1}{\theta}\right) \xrightarrow{\text{d}} Z  \sim N(0,1)$, i.e. converges in distribution. So in the correction sheet it says the answer is $\dfrac{1}{\sqrt{CRLB}}$ where CRLB stands for the Cramer Rao Lower Bound. I don't really understand this. I have found in my book that for large $n$ $\hat{\theta}_n \sim N(\theta, CRLB)$, but how can we normalise this result to a $N(0,1)$?","Given is a a random sample $X_1,\ldots,X_n$ from a distribution with a pdf $f(x;\theta) = 2\theta^{-1} x^{(2/\theta)-1}$ for $0<x<1$, zero otherwise. We know that maximum likelihood estimator is $\hat{\theta} = -\dfrac 2 n \sum_i \log X_i$. Determine $c$ such that $c\left(\dfrac{n}{-2\sum_i \log X_i} - \dfrac{1}{\theta}\right) \xrightarrow{\text{d}} Z  \sim N(0,1)$, i.e. converges in distribution. So in the correction sheet it says the answer is $\dfrac{1}{\sqrt{CRLB}}$ where CRLB stands for the Cramer Rao Lower Bound. I don't really understand this. I have found in my book that for large $n$ $\hat{\theta}_n \sim N(\theta, CRLB)$, but how can we normalise this result to a $N(0,1)$?",,"['statistics', 'probability-distributions', 'normal-distribution']"
69,Distribution of elements of inverse matrix,Distribution of elements of inverse matrix,,"Suppose $M$ is an $n \times n$ matrix whose elements are independent random values that follow a normal distribution whose mean is $0$ and whose standard deviation is $1$. What distribution will the elements of $M^{-1}$ follow? If the distribution of $M$ follows a uniform distribution, how does this change the distribution of the elements of $M^{-1}$? To clarify, $M$ is almost surely an invertible matrix simply because of the way it's generated. When you have a matrix filled with independent random real values, it's almost surely invertible even if it's really large.","Suppose $M$ is an $n \times n$ matrix whose elements are independent random values that follow a normal distribution whose mean is $0$ and whose standard deviation is $1$. What distribution will the elements of $M^{-1}$ follow? If the distribution of $M$ follows a uniform distribution, how does this change the distribution of the elements of $M^{-1}$? To clarify, $M$ is almost surely an invertible matrix simply because of the way it's generated. When you have a matrix filled with independent random real values, it's almost surely invertible even if it's really large.",,"['linear-algebra', 'statistics']"
70,Why do we need a Borel function in the definition of exponential family?,Why do we need a Borel function in the definition of exponential family?,,"See the following definition. I was expecting $h$ to be a $\cal F$-measurable function, but the definition says it is a Borel function. Anyone can help explain why we need a Borel function here ? What difference does it make?","See the following definition. I was expecting $h$ to be a $\cal F$-measurable function, but the definition says it is a Borel function. Anyone can help explain why we need a Borel function here ? What difference does it make?",,"['probability', 'probability-theory', 'statistics']"
71,Which biased coin is better?,Which biased coin is better?,,"My friend has 2 biased coins. He flips the first one a number of times and tells my how many heads and tails he saw, then repeats this for the second one. I have no influence on how many trials he does, though I am guranteed at least one per coin. My job now is to tell him which coin is better; i.e. which one has a higher probability of showing heads. The obvious way to do this would be to estimate the probabilities from the samples, however this seems wrong to me because I am not taking into account how accurate my estimates are. E.g., consider the case where: Coin 1 has shown 501 heads and 499 tails. Coin 2 has shown 2 heads and 1 tails. I tried approaching this in a few ways, and all of those quickly showed me how little I know about probability theory and statistics. My question has four main points: Does this problem depend on how the coin probabilities are distributed? (If so, assume a uniform distribution.) How do I calculate how likely either answer is correct or wrong, given the two samples? Is there a strategy that is more often correct (or less often wrong) than the obvious one? How does this generalize to any number of coins?","My friend has 2 biased coins. He flips the first one a number of times and tells my how many heads and tails he saw, then repeats this for the second one. I have no influence on how many trials he does, though I am guranteed at least one per coin. My job now is to tell him which coin is better; i.e. which one has a higher probability of showing heads. The obvious way to do this would be to estimate the probabilities from the samples, however this seems wrong to me because I am not taking into account how accurate my estimates are. E.g., consider the case where: Coin 1 has shown 501 heads and 499 tails. Coin 2 has shown 2 heads and 1 tails. I tried approaching this in a few ways, and all of those quickly showed me how little I know about probability theory and statistics. My question has four main points: Does this problem depend on how the coin probabilities are distributed? (If so, assume a uniform distribution.) How do I calculate how likely either answer is correct or wrong, given the two samples? Is there a strategy that is more often correct (or less often wrong) than the obvious one? How does this generalize to any number of coins?",,"['probability', 'statistics']"
72,Proving that a specific Bayes rule is least favourable,Proving that a specific Bayes rule is least favourable,,"Suppose $\pi$ is a prior distribution on $\Theta$ such that the Bayes risk of the Bayes rule equals $\sup_{\theta\in \Theta}R(\delta_\pi,\theta)$, where $R(\delta,\theta)$ is the risk function associated to the decision problem. Prove that $\delta_\pi$ is least favourable. Okay this means that $\sup_{\theta\in \Theta}R(\delta_\pi,\theta)=\int_\Theta R(\delta_\pi,\theta)\pi(\theta)\,d\theta\le\int_\Theta R(\delta',\theta)\pi(\theta)\,d\theta$ for any $\delta'$, and we need to show that $\int_\Theta R(\delta_\pi,\theta)\pi(\theta)\,d\theta\ge\int_\Theta R(\delta_\lambda,\theta)\lambda(\theta)\,d\theta$ for any prior distribution $\lambda$.","Suppose $\pi$ is a prior distribution on $\Theta$ such that the Bayes risk of the Bayes rule equals $\sup_{\theta\in \Theta}R(\delta_\pi,\theta)$, where $R(\delta,\theta)$ is the risk function associated to the decision problem. Prove that $\delta_\pi$ is least favourable. Okay this means that $\sup_{\theta\in \Theta}R(\delta_\pi,\theta)=\int_\Theta R(\delta_\pi,\theta)\pi(\theta)\,d\theta\le\int_\Theta R(\delta',\theta)\pi(\theta)\,d\theta$ for any $\delta'$, and we need to show that $\int_\Theta R(\delta_\pi,\theta)\pi(\theta)\,d\theta\ge\int_\Theta R(\delta_\lambda,\theta)\lambda(\theta)\,d\theta$ for any prior distribution $\lambda$.",,"['statistics', 'probability-distributions', 'bayesian', 'decision-theory']"
73,About the proof of the Lindeberg-Feller theorem.,About the proof of the Lindeberg-Feller theorem.,,"I found this pdf online that proves the Lindeberg-Feller theorem (Theorem 3.4.5) in probability Theory with the proof on page 2. At an early stage in the proof the author defines the Independent random variables $$Y_{n,i}$$ such that they are Independent of $$X_{n,i}$$ and with the property that $$Y_{n,i} \sim \mathcal{N}(0,\mathbb{E}(X_{n,i})).$$ Why do such $Y_{n,i}$'s exist? I read in my own lecture notes that one can expand the probability space $(\Omega,\mathcal{A},\mathbb{P})$ to assure existence, but I cannot figure out why. Thanks a lot in advance!","I found this pdf online that proves the Lindeberg-Feller theorem (Theorem 3.4.5) in probability Theory with the proof on page 2. At an early stage in the proof the author defines the Independent random variables $$Y_{n,i}$$ such that they are Independent of $$X_{n,i}$$ and with the property that $$Y_{n,i} \sim \mathcal{N}(0,\mathbb{E}(X_{n,i})).$$ Why do such $Y_{n,i}$'s exist? I read in my own lecture notes that one can expand the probability space $(\Omega,\mathcal{A},\mathbb{P})$ to assure existence, but I cannot figure out why. Thanks a lot in advance!",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'normal-distribution']"
74,Esscher transform of unimodal distribution,Esscher transform of unimodal distribution,,"Let $X$ be a continuous random variable with distribution function $F_X$, and suppose $F_X$ is unimodal. (Here, unimodal is taken to mean that the density function of $X$ is monotone increasing (resp. decreasing) for all $x<x^*$ (resp. $x >x^*$), where $x^*$ is the argmax of the density function.) The Esscher transform of $X$ can be defined as \begin{align*}     dF_X^{(t)}(x) := \frac{e^{t x} dF_X(x)}{\mathbb{E}_X[e^{t X}]} = \exp \left(tx - K_X(t) \right)  dF_X(x) , \qquad t\in \mathcal{T}, \end{align*} with $\mathcal{T} = \{t \in \mathbb{R}: \mathbb{E}_X[e^{t X}] < \infty  \}$, and where $K_X$ is the cumulant generating function. Does it follow that the transformed distribution function, $F_X^{(t)}$, is also unimodal in $x$? If so, how does one go about showing this? If this is not possible, can we impose further constraints to get the desired result, ex. $F_X$ is a unimodal member of the exponential family ?","Let $X$ be a continuous random variable with distribution function $F_X$, and suppose $F_X$ is unimodal. (Here, unimodal is taken to mean that the density function of $X$ is monotone increasing (resp. decreasing) for all $x<x^*$ (resp. $x >x^*$), where $x^*$ is the argmax of the density function.) The Esscher transform of $X$ can be defined as \begin{align*}     dF_X^{(t)}(x) := \frac{e^{t x} dF_X(x)}{\mathbb{E}_X[e^{t X}]} = \exp \left(tx - K_X(t) \right)  dF_X(x) , \qquad t\in \mathcal{T}, \end{align*} with $\mathcal{T} = \{t \in \mathbb{R}: \mathbb{E}_X[e^{t X}] < \infty  \}$, and where $K_X$ is the cumulant generating function. Does it follow that the transformed distribution function, $F_X^{(t)}$, is also unimodal in $x$? If so, how does one go about showing this? If this is not possible, can we impose further constraints to get the desired result, ex. $F_X$ is a unimodal member of the exponential family ?",,"['real-analysis', 'probability', 'probability-theory', 'statistics', 'probability-distributions']"
75,Average of ratios - I don't get it,Average of ratios - I don't get it,,"I'm calculating ratios from paired samples and there is a point I don't understand. Supposed that I measured the values of 2 paired samples: A and B, and then I calculate the ratios from those values. Ratio 1: $\frac{A}{B} = 0.5$ Ratio 2: $\frac{A}{B} = 2.0$ Normally one would calculate the average $ratio = \frac{(Ratio 1 + Ratio 2)}{2} = 1.25$. Then the conclusion would be: the value of A is 1.25 times higher than that of B. But, the ratios can be understood as: Ratio 1 = 0.5 --> the value of B is double the value of A Ratio 2 = 2 --> the value of A is double the value of B Then, the average ratio of A and B should be equal 1. Does that make sense to you? Where is the flaw? Thanks all,","I'm calculating ratios from paired samples and there is a point I don't understand. Supposed that I measured the values of 2 paired samples: A and B, and then I calculate the ratios from those values. Ratio 1: $\frac{A}{B} = 0.5$ Ratio 2: $\frac{A}{B} = 2.0$ Normally one would calculate the average $ratio = \frac{(Ratio 1 + Ratio 2)}{2} = 1.25$. Then the conclusion would be: the value of A is 1.25 times higher than that of B. But, the ratios can be understood as: Ratio 1 = 0.5 --> the value of B is double the value of A Ratio 2 = 2 --> the value of A is double the value of B Then, the average ratio of A and B should be equal 1. Does that make sense to you? Where is the flaw? Thanks all,",,"['algebra-precalculus', 'statistics', 'average']"
76,Moment-generating function,Moment-generating function,,Let X~Exponential(1). I know that the moment-generating function of an exponential distribution is defined as $(1-t\lambda)^{-1}$. And hence $E[e^{tx}]=(1-t)^{-1}$. But what is $E[Xe^{tx}]$? Would that be the first moment of the moment-generating function?,Let X~Exponential(1). I know that the moment-generating function of an exponential distribution is defined as $(1-t\lambda)^{-1}$. And hence $E[e^{tx}]=(1-t)^{-1}$. But what is $E[Xe^{tx}]$? Would that be the first moment of the moment-generating function?,,"['statistics', 'moment-generating-functions']"
77,Distributional result in NIW model,Distributional result in NIW model,,"Suppose $$ \Sigma^{-1} \sim \text{Wishart}_p \left(\frac{\Sigma_0^{-1}}{d_0},d_0\right) $$ That is $\Sigma$ has a inverse Wishart distribution. Furthermore $$ W \mid \Sigma \sim \operatorname{Wishart}_p(\Sigma, d) $$ $$ Z \mid \Sigma \sim \mathcal{N}_p(0,\Sigma) $$ and $W$ and $Z$ are conditionally independent given $\Sigma$. I want to prove (or disprove) that  $$ \frac{Z^T\Sigma^{-1}Z}{Z^T(W+\Sigma_0d_0)^{-1}Z} \sim \chi^2(d+d_0-p+1). $$ I can show that the result holds in a few special cases: Special case 1: p=1 When $p=1$ the Wishart distributions reduces to $\chi^2$ distributions: $$\Sigma^{-1} \sim \frac{1}{d_0\Sigma_0}\chi^2(d_0)$$ $$ W \mid \Sigma \sim \Sigma\chi^2(d) $$ It now follows that $$ \begin{split} \frac{Z^T\Sigma^{-1}Z}{Z^T(W+\Sigma_0d_0)^{-1}Z} &=  \frac{W+\Sigma_0d_0}{\Sigma} \\ &= \frac{W}{\Sigma} + \Sigma_0d_0\Sigma^{-1} \\ &\sim \chi^2(d+d_0) \end{split} $$ since $\frac{W}{\Sigma}\sim \chi^2(d)$ and is independent of $\Sigma_0d_0\Sigma^{-1} \sim \chi^2(d_0)$ Special case 2: ""$d_0 = 0$"" If $V \sim \operatorname{Wishart}(\Sigma, d)$ then $$ \frac{1}{(V^{-1})_{ii}} \sim \frac{1}{(\Sigma^{-1})_{ii}}\chi^2(d-p+1) $$ see Inverse-Wishart . Now for any fixed vector $a$ of length $p$. Take an orthogonal matrix $B=\{b_{(1)},\ldots ,b_{(p-1)},\frac{a}{|a|}\}$. Then $B^TV^{-1}B \sim \text{Wishart}(B^T\Sigma^{-1}B,d)$. We now have $$ \begin{split} (B^TVB)^{-1} &= B^{-1}V^{-1}B^{T-1} = B^TV^{-1}B \\ (B^T\Sigma B)^{-1} &= B^T\Sigma^{-1}B \\ (B^T\Sigma^{-1}B)_{pp} &= \frac{1}{|a|^2}a^T\Sigma^{-1}a \\ (B^TV^{-1}B)_{pp} &= \frac{1}{|a|^2}a^TV^{-1}a \end{split} $$ Using the above result we have that for any $a$ $$ \frac{Z^T\Sigma^{-1}Z}{Z^TW^{-1}Z} \mid Z = a \sim \chi^2(d-p+1). $$ Since the conditional distribution does not depend on $a$ it holds unconditionally as well.","Suppose $$ \Sigma^{-1} \sim \text{Wishart}_p \left(\frac{\Sigma_0^{-1}}{d_0},d_0\right) $$ That is $\Sigma$ has a inverse Wishart distribution. Furthermore $$ W \mid \Sigma \sim \operatorname{Wishart}_p(\Sigma, d) $$ $$ Z \mid \Sigma \sim \mathcal{N}_p(0,\Sigma) $$ and $W$ and $Z$ are conditionally independent given $\Sigma$. I want to prove (or disprove) that  $$ \frac{Z^T\Sigma^{-1}Z}{Z^T(W+\Sigma_0d_0)^{-1}Z} \sim \chi^2(d+d_0-p+1). $$ I can show that the result holds in a few special cases: Special case 1: p=1 When $p=1$ the Wishart distributions reduces to $\chi^2$ distributions: $$\Sigma^{-1} \sim \frac{1}{d_0\Sigma_0}\chi^2(d_0)$$ $$ W \mid \Sigma \sim \Sigma\chi^2(d) $$ It now follows that $$ \begin{split} \frac{Z^T\Sigma^{-1}Z}{Z^T(W+\Sigma_0d_0)^{-1}Z} &=  \frac{W+\Sigma_0d_0}{\Sigma} \\ &= \frac{W}{\Sigma} + \Sigma_0d_0\Sigma^{-1} \\ &\sim \chi^2(d+d_0) \end{split} $$ since $\frac{W}{\Sigma}\sim \chi^2(d)$ and is independent of $\Sigma_0d_0\Sigma^{-1} \sim \chi^2(d_0)$ Special case 2: ""$d_0 = 0$"" If $V \sim \operatorname{Wishart}(\Sigma, d)$ then $$ \frac{1}{(V^{-1})_{ii}} \sim \frac{1}{(\Sigma^{-1})_{ii}}\chi^2(d-p+1) $$ see Inverse-Wishart . Now for any fixed vector $a$ of length $p$. Take an orthogonal matrix $B=\{b_{(1)},\ldots ,b_{(p-1)},\frac{a}{|a|}\}$. Then $B^TV^{-1}B \sim \text{Wishart}(B^T\Sigma^{-1}B,d)$. We now have $$ \begin{split} (B^TVB)^{-1} &= B^{-1}V^{-1}B^{T-1} = B^TV^{-1}B \\ (B^T\Sigma B)^{-1} &= B^T\Sigma^{-1}B \\ (B^T\Sigma^{-1}B)_{pp} &= \frac{1}{|a|^2}a^T\Sigma^{-1}a \\ (B^TV^{-1}B)_{pp} &= \frac{1}{|a|^2}a^TV^{-1}a \end{split} $$ Using the above result we have that for any $a$ $$ \frac{Z^T\Sigma^{-1}Z}{Z^TW^{-1}Z} \mid Z = a \sim \chi^2(d-p+1). $$ Since the conditional distribution does not depend on $a$ it holds unconditionally as well.",,"['statistics', 'probability-distributions', 'chi-squared']"
78,Universal Donsker classes and bounded variation,Universal Donsker classes and bounded variation,,"I just read in a paper A Donsker Theorem for Lévy Measures the following statement $BV$-balls are universal Donsker classes (page 7, Examples 3.2 - Compound Poisson Processes) $BV$ stands here for bounded variation. Unfortunately there is no reference for this result. I was wondering, is this some sort of trivial? Does anyone know a reference?","I just read in a paper A Donsker Theorem for Lévy Measures the following statement $BV$-balls are universal Donsker classes (page 7, Examples 3.2 - Compound Poisson Processes) $BV$ stands here for bounded variation. Unfortunately there is no reference for this result. I was wondering, is this some sort of trivial? Does anyone know a reference?",,"['statistics', 'stochastic-processes', 'bounded-variation', 'empirical-processes']"
79,Does a one tailed test statistic will always fail to reject the null if observed value is in the tail opposite to the alternative hypothesis?,Does a one tailed test statistic will always fail to reject the null if observed value is in the tail opposite to the alternative hypothesis?,,"Suppose: $H_0: \beta \le 0.5$ $H_1: \beta > 0.5$ $\widehat \beta = 0.49$ If $\widehat \beta = 0.49$ and I want to test the alternative hypothesis that $\beta > 0.5$ (versus the null that $\beta \le 0.5$). Won't it be the case that a one tailed t statistic will always fail to reject the null, given that the t-stat will be negative and to reject the null, the t-stat would need to be positive? This seems counterintuitive that we would never reject the null even if $\widehat\beta$ had a high standard error. Furthermore wouldn't a constructed confidence interval then suggest there are null hypotheses where beta $> 0.5$, which we would fail to reject (contrary to the calculated t-value)?","Suppose: $H_0: \beta \le 0.5$ $H_1: \beta > 0.5$ $\widehat \beta = 0.49$ If $\widehat \beta = 0.49$ and I want to test the alternative hypothesis that $\beta > 0.5$ (versus the null that $\beta \le 0.5$). Won't it be the case that a one tailed t statistic will always fail to reject the null, given that the t-stat will be negative and to reject the null, the t-stat would need to be positive? This seems counterintuitive that we would never reject the null even if $\widehat\beta$ had a high standard error. Furthermore wouldn't a constructed confidence interval then suggest there are null hypotheses where beta $> 0.5$, which we would fail to reject (contrary to the calculated t-value)?",,['statistics']
80,"Comparing $x*f(x)$ with C.D.F., $F(x)$ [closed]","Comparing  with C.D.F.,  [closed]",x*f(x) F(x),"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question If $x$ is a continuous r.v., with pdf as $f(x)$ and cdf as $F(x)$, then say I take $x=A$, and wish to compare the quantities ($A*f(A)$) with $F(A)$. Since $F(A)=\int_0^Af(x)dx$ doesn't it follow that $F(A)>A*f(A)$? However, I also have this argument: Consider a discrete distribution where $f(x)$ has value 4 in the range 0.95 to 1.00.  The value of $xf(x)$ in this range will be around 4, but the contribution to $F(x)$ is only 0.05*4 = 0.20.  The value of $F(x)$ meanwhile is 1.00.  So $xf(x) > F(x)$. Where am I going wrong? Please guide. Thanks.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question If $x$ is a continuous r.v., with pdf as $f(x)$ and cdf as $F(x)$, then say I take $x=A$, and wish to compare the quantities ($A*f(A)$) with $F(A)$. Since $F(A)=\int_0^Af(x)dx$ doesn't it follow that $F(A)>A*f(A)$? However, I also have this argument: Consider a discrete distribution where $f(x)$ has value 4 in the range 0.95 to 1.00.  The value of $xf(x)$ in this range will be around 4, but the contribution to $F(x)$ is only 0.05*4 = 0.20.  The value of $F(x)$ meanwhile is 1.00.  So $xf(x) > F(x)$. Where am I going wrong? Please guide. Thanks.",,"['probability-theory', 'statistics', 'probability-distributions']"
81,Characterize joint distribution from marginals,Characterize joint distribution from marginals,,"Let $Z$ be an arbitrary set. Let $X=\prod_{i=1}^n Z_i$ and $Z_i=Z$ for each $i=1,\ldots,n$, $n$ fixed. Consider the $n$-tuple $(\mu_1,\ldots,\mu_n)$ with $\mu_i\in\Delta_s(Z)$ for each $i=1,\ldots,n$, i.e., each $\mu_i$ is a simple probability measure on $Z$ (it has finite support). Let $J=\{\mu\in \Delta_s(X): \operatorname*{marg}_{Z_i}\mu=\mu_i \;\;\forall $i$ \}$, i.e. the set of all simple probability measures on $X$ that have $(\mu_1,\ldots,\mu_n)$ as marginals. Is there any way to characterize the set $J$? I know that for real valued random variables this problem can be tackled with copulas, but I don't know if copulas can be used in this case. We can assume that there is some complete and transitive order $\succeq$ on $Z$.","Let $Z$ be an arbitrary set. Let $X=\prod_{i=1}^n Z_i$ and $Z_i=Z$ for each $i=1,\ldots,n$, $n$ fixed. Consider the $n$-tuple $(\mu_1,\ldots,\mu_n)$ with $\mu_i\in\Delta_s(Z)$ for each $i=1,\ldots,n$, i.e., each $\mu_i$ is a simple probability measure on $Z$ (it has finite support). Let $J=\{\mu\in \Delta_s(X): \operatorname*{marg}_{Z_i}\mu=\mu_i \;\;\forall $i$ \}$, i.e. the set of all simple probability measures on $X$ that have $(\mu_1,\ldots,\mu_n)$ as marginals. Is there any way to characterize the set $J$? I know that for real valued random variables this problem can be tackled with copulas, but I don't know if copulas can be used in this case. We can assume that there is some complete and transitive order $\succeq$ on $Z$.",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'decision-theory']"
82,Kollo Skewness - Calculation and meaning,Kollo Skewness - Calculation and meaning,,"I would like to calculate the Kollo Skewness which is defined here in Definition 2. $b(x) = E\left [ \sum (Y_{i}Y_{j})Y \right ]$ where $Y = \Sigma ^{-1/2} (X-\mu)$ with $E[X] = \mu$ and $\Sigma$ is the dispersion matrix (Covariance Matrix). I am strugling to grasp the intuition behind the Kollo Skewness. A good first step would be understanding the thinking behind $Y$. What is the effect of premultiplying with the inverse of the cholesky decomposition of the Covariance Matrix? I have also written a Matlab function to calculate the sample Kollo Skewness. $\widehat{b(X)} = \frac{1}{n}1_{pxp}\star\sum_{n}^{i=1}(y_{i}y_{i}'\otimes y_{i})$ function [ B ] = KolloSkew( X ) %sample estimate for Kollo Skewness %    %   Calculation according to Def. 3 Kollo (2008) % %   Input: %   X: mxn of sample data %    %   Output: %   B: Kollo Skewness Vector  [m, n] = size(X); S = cov(X);  %%  Y Y = zeros(size(X,2)); for j=1:n     % yi = S^(-1/2)*(xi-x.mean)     Y(:,j) = (chol(S)')^(-1)*(X(1,:)'-mean(X(:,1))); end  %% B = 1/n 1pxp star sum temp1 = 1/m * ones(n); %temp1: 1/n 1pxp temp2 = zeros(4,2); %temp2: sum yi*yi' kron yi*yi' for j=1:n     temp2 = temp2 + kron(Y(:,j)*Y(:,j)',Y(:,j));  end   B = star(temp1, temp2);  end  %% star product function [ starprod ] = star( A, B ) % star product according to Def. 1 Kollo r = size(B,1) / size(A,1); s = size(B,2) / size(A,2);  n = size(A,2); m = size(A,1);  temp3 = zeros(n,1); for i=1:m    for j=1:n        temp3 = temp3 + A(i,j) * B(1+(i-1)*r:i*r, 1+(j-1)*s:j*s);    end  end  starprod = temp3; end","I would like to calculate the Kollo Skewness which is defined here in Definition 2. $b(x) = E\left [ \sum (Y_{i}Y_{j})Y \right ]$ where $Y = \Sigma ^{-1/2} (X-\mu)$ with $E[X] = \mu$ and $\Sigma$ is the dispersion matrix (Covariance Matrix). I am strugling to grasp the intuition behind the Kollo Skewness. A good first step would be understanding the thinking behind $Y$. What is the effect of premultiplying with the inverse of the cholesky decomposition of the Covariance Matrix? I have also written a Matlab function to calculate the sample Kollo Skewness. $\widehat{b(X)} = \frac{1}{n}1_{pxp}\star\sum_{n}^{i=1}(y_{i}y_{i}'\otimes y_{i})$ function [ B ] = KolloSkew( X ) %sample estimate for Kollo Skewness %    %   Calculation according to Def. 3 Kollo (2008) % %   Input: %   X: mxn of sample data %    %   Output: %   B: Kollo Skewness Vector  [m, n] = size(X); S = cov(X);  %%  Y Y = zeros(size(X,2)); for j=1:n     % yi = S^(-1/2)*(xi-x.mean)     Y(:,j) = (chol(S)')^(-1)*(X(1,:)'-mean(X(:,1))); end  %% B = 1/n 1pxp star sum temp1 = 1/m * ones(n); %temp1: 1/n 1pxp temp2 = zeros(4,2); %temp2: sum yi*yi' kron yi*yi' for j=1:n     temp2 = temp2 + kron(Y(:,j)*Y(:,j)',Y(:,j));  end   B = star(temp1, temp2);  end  %% star product function [ starprod ] = star( A, B ) % star product according to Def. 1 Kollo r = size(B,1) / size(A,1); s = size(B,2) / size(A,2);  n = size(A,2); m = size(A,1);  temp3 = zeros(n,1); for i=1:m    for j=1:n        temp3 = temp3 + A(i,j) * B(1+(i-1)*r:i*r, 1+(j-1)*s:j*s);    end  end  starprod = temp3; end",,"['linear-algebra', 'statistics']"
83,"If $f(y\mid\theta)\sim N(\theta, 1)$, how to find the asymptotic distribution $f\left(y\mid\hat{\theta}_n^{MLE}\right)$?","If , how to find the asymptotic distribution ?","f(y\mid\theta)\sim N(\theta, 1) f\left(y\mid\hat{\theta}_n^{MLE}\right)","If I have that $f(y\mid\theta)$ has a $N(\theta, 1)$ distribution, and $\widehat{\theta}_n^{MLE} \to \theta^{\star}$ in probability, where $Y_1, \ldots, Y_n \sim N(\theta^{\star},1)$ , how can I find the asymptotic distribution of $p\left(y\mid\widehat{\theta}_n^{MLE}\right)$ ? I know that we should have $N\left(\widehat{\theta}_n^{MLE},1\right)$ converging to $N\left(\theta^{\star},1\right)$ but am not sure how to show this. Does anyone have any ideas?","If I have that has a distribution, and in probability, where , how can I find the asymptotic distribution of ? I know that we should have converging to but am not sure how to show this. Does anyone have any ideas?","f(y\mid\theta) N(\theta, 1) \widehat{\theta}_n^{MLE} \to \theta^{\star} Y_1, \ldots, Y_n \sim N(\theta^{\star},1) p\left(y\mid\widehat{\theta}_n^{MLE}\right) N\left(\widehat{\theta}_n^{MLE},1\right) N\left(\theta^{\star},1\right)","['probability-theory', 'statistics', 'probability-distributions', 'asymptotics', 'normal-distribution']"
84,How do you handle errors when rotating vector data in 2D?,How do you handle errors when rotating vector data in 2D?,,"I have a vector data set in component-form in two columns with error (x-component and y-component: $x+\delta x$ and $y+\delta y$). I need to rotate this data by an angle $\theta$ to a new coordinate system $x'$ and $y'$. To do this with perfect data, I understand that you can use the 2D rotation matrix: $$\begin{bmatrix}x' \\ y'  \end{bmatrix} = \begin{bmatrix} \cos (\theta) & -\sin(\theta)\\ \sin(\theta) & \cos(\theta)\end{bmatrix}\begin{bmatrix}x \\ y \end{bmatrix}.$$ However, for imprecise data where I have columns for $\delta x$ and $\delta y$, how can I handle this error? Is it as simple as: $$\begin{bmatrix}\delta x' \\ \delta y'  \end{bmatrix} = \begin{bmatrix} \cos (\theta) & -\sin(\theta)\\ \sin(\theta) & \cos(\theta)\end{bmatrix}\begin{bmatrix}\delta x \\ \delta y \end{bmatrix}$$ or something more complicated?","I have a vector data set in component-form in two columns with error (x-component and y-component: $x+\delta x$ and $y+\delta y$). I need to rotate this data by an angle $\theta$ to a new coordinate system $x'$ and $y'$. To do this with perfect data, I understand that you can use the 2D rotation matrix: $$\begin{bmatrix}x' \\ y'  \end{bmatrix} = \begin{bmatrix} \cos (\theta) & -\sin(\theta)\\ \sin(\theta) & \cos(\theta)\end{bmatrix}\begin{bmatrix}x \\ y \end{bmatrix}.$$ However, for imprecise data where I have columns for $\delta x$ and $\delta y$, how can I handle this error? Is it as simple as: $$\begin{bmatrix}\delta x' \\ \delta y'  \end{bmatrix} = \begin{bmatrix} \cos (\theta) & -\sin(\theta)\\ \sin(\theta) & \cos(\theta)\end{bmatrix}\begin{bmatrix}\delta x \\ \delta y \end{bmatrix}$$ or something more complicated?",,"['statistics', 'vectors', 'vector-analysis', 'rotations', 'error-propagation']"
85,How is this histogram correct?,How is this histogram correct?,,"Looking at this histogram in Wikipedia (one on the right and not the ones at the bottom), I am unable to understand how it is correct or is there a flaw in my understanding. Bin   Count −3.5     23 −2.5     32 −1.5    109 −0.5    180  0.5    132  1.5     34  2.5      4  3.5     90 Given the above information, I can infer that the bins are -: -3.5 to less than 2.5 -2.5 to less than -1.5 and so on. Is this inference correct or am I missing something here ? If the above inference is correct then the looking at the first column is for 23 values occurring in the first bin. What is weird is the last bin(for 3.5, the last element in the table) for which there are 90 values in the table but the values shown in the histogram are lesser than the 23 values in the first column. How is that possible ? Is that correct ? Or are my concepts lacking somewhere ?","Looking at this histogram in Wikipedia (one on the right and not the ones at the bottom), I am unable to understand how it is correct or is there a flaw in my understanding. Bin   Count −3.5     23 −2.5     32 −1.5    109 −0.5    180  0.5    132  1.5     34  2.5      4  3.5     90 Given the above information, I can infer that the bins are -: -3.5 to less than 2.5 -2.5 to less than -1.5 and so on. Is this inference correct or am I missing something here ? If the above inference is correct then the looking at the first column is for 23 values occurring in the first bin. What is weird is the last bin(for 3.5, the last element in the table) for which there are 90 values in the table but the values shown in the histogram are lesser than the 23 values in the first column. How is that possible ? Is that correct ? Or are my concepts lacking somewhere ?",,"['statistics', 'visualization']"
86,How to determine the probability that two variables are related?,How to determine the probability that two variables are related?,,"I have a set of observations. Each observation has two variables: # | VarA  | VarB --|-------|------- 1 | True  | False 2 | False | False 3 | True  | True 4 | True  | False ... (143,804 rows) From this, I've got the following table: | VarB true | VarB false VarA true |   729     |    1296 VarA false|   1753    |   140026 I want to know the chance that VarA and VarB are related, in the sense that their likelihood of occurring together is significantly higher than suggested by chance. It's rare that an observation contains VarA or VarB (1.98% and 1.41% respectively); if the variables were randomly distributed I'd expect to see (0.0198*0.0141)*143804 = 40 co-occurrences, but instead I see 729. My question is: for this and other observations, what test(s) can I run to check whether the results are independent or related? I've looked at chi-squared, but I can't work out whether it's valid.","I have a set of observations. Each observation has two variables: # | VarA  | VarB --|-------|------- 1 | True  | False 2 | False | False 3 | True  | True 4 | True  | False ... (143,804 rows) From this, I've got the following table: | VarB true | VarB false VarA true |   729     |    1296 VarA false|   1753    |   140026 I want to know the chance that VarA and VarB are related, in the sense that their likelihood of occurring together is significantly higher than suggested by chance. It's rare that an observation contains VarA or VarB (1.98% and 1.41% respectively); if the variables were randomly distributed I'd expect to see (0.0198*0.0141)*143804 = 40 co-occurrences, but instead I see 729. My question is: for this and other observations, what test(s) can I run to check whether the results are independent or related? I've looked at chi-squared, but I can't work out whether it's valid.",,"['probability', 'statistics']"
87,"Polar form representation of $aX+bY+cZ$ ($X$, $Y$, and $Z$ are complex Gaussian random variable)","Polar form representation of  (, , and  are complex Gaussian random variable)",aX+bY+cZ X Y Z,"I need some help with the following problem: Let $X$, $Y$, and $Z$ are independent circularly symmetric complex Gaussian random variable with zero mean and unit variance, i.e., $X$, $Y$, and $Z \sim CN(0, 1)$. These random variables can be represented in polar form as    \begin{equation} X=r_1 e^{i \alpha}\\ Y=r_2 e^{i \beta}\\ Z=r_3 e^{i \gamma} \end{equation}   where $r_1$, $r_2$ and $r_3$ are the magnitude (Rayleigh distributed) and $\alpha$, $\beta$ and $\gamma$ are the phase of the variables (Uniformly distributed over $[0, 2\pi]$). How can I represent the  random variable $aX+bY+cZ$ (where a,b, c are constant) in polar form as a function of the variables $a$, $b$, $c$, $r_1$, $r_2$, $r_3$, $\alpha$, $\beta$ and $\gamma$. Particularly I am interested to know the phase of $aX+bY+cZ$ in terms of these variables. Thank you very much","I need some help with the following problem: Let $X$, $Y$, and $Z$ are independent circularly symmetric complex Gaussian random variable with zero mean and unit variance, i.e., $X$, $Y$, and $Z \sim CN(0, 1)$. These random variables can be represented in polar form as    \begin{equation} X=r_1 e^{i \alpha}\\ Y=r_2 e^{i \beta}\\ Z=r_3 e^{i \gamma} \end{equation}   where $r_1$, $r_2$ and $r_3$ are the magnitude (Rayleigh distributed) and $\alpha$, $\beta$ and $\gamma$ are the phase of the variables (Uniformly distributed over $[0, 2\pi]$). How can I represent the  random variable $aX+bY+cZ$ (where a,b, c are constant) in polar form as a function of the variables $a$, $b$, $c$, $r_1$, $r_2$, $r_3$, $\alpha$, $\beta$ and $\gamma$. Particularly I am interested to know the phase of $aX+bY+cZ$ in terms of these variables. Thank you very much",,"['statistics', 'random-variables', 'normal-distribution', 'polar-coordinates', 'random-functions']"
88,Unbiased estimate for $\mu$ and standard error for Poisson random variables [closed],Unbiased estimate for  and standard error for Poisson random variables [closed],\mu,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Suppose that computer failure data $x_1, x_2, \ldots, x_n$ are modelled as observations of i.i.d. Poisson random variables $X_1, X_2, \ldots, X_n$ with common mean $\mu$. Recalling that, for this data, $n = 104$ and $\bar{x} = 3.75$, calculate an unbiased estimate for $\mu$, together with its standard error, under the Poisson model.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Suppose that computer failure data $x_1, x_2, \ldots, x_n$ are modelled as observations of i.i.d. Poisson random variables $X_1, X_2, \ldots, X_n$ with common mean $\mu$. Recalling that, for this data, $n = 104$ and $\bar{x} = 3.75$, calculate an unbiased estimate for $\mu$, together with its standard error, under the Poisson model.",,"['statistics', 'poisson-distribution']"
89,"For the expectation of a random variable $Y$, what does the $dy$ term imply in: $\mathbb{E}[Y] = \int_{\mathcal{Y}}y \ p(dy)$?","For the expectation of a random variable , what does the  term imply in: ?",Y dy \mathbb{E}[Y] = \int_{\mathcal{Y}}y \ p(dy),"For a random variable $Y$, assume that it has distribution $p$ on a space $\mathcal{Y}$. Then, I have seen the expectation of $Y$ written as: $$ \mathbb{E}[Y] = \int_{\mathcal{Y}}y \ p(dy) $$ I am wondering why there is a $dy$ inside of the distribution $p$. Is this perhaps referencing the Lebesgue measure?","For a random variable $Y$, assume that it has distribution $p$ on a space $\mathcal{Y}$. Then, I have seen the expectation of $Y$ written as: $$ \mathbb{E}[Y] = \int_{\mathcal{Y}}y \ p(dy) $$ I am wondering why there is a $dy$ inside of the distribution $p$. Is this perhaps referencing the Lebesgue measure?",,"['probability', 'probability-theory', 'statistics']"
90,Identify covariance-like formula,Identify covariance-like formula,,"This might be a weird question, but in the process of some computation I obtained the following formula: $$\text{E}[X^2]\text{E}[Y^2] - \text{E}[XY]^2$$ It kind of looks like a ""quadratic"" covariance, but I don't know what it actually means. Is there a mathematical meaning to this quantity? Specifically, what kind of relationship between $X$ and $Y$ would ensure the above quantity is non-zero? UPDATE: Seems this is equal to $\text{Var}(XY) - \text{Cov}(X^2, Y^2)$. Is there a clearer meaning to this anyway?","This might be a weird question, but in the process of some computation I obtained the following formula: $$\text{E}[X^2]\text{E}[Y^2] - \text{E}[XY]^2$$ It kind of looks like a ""quadratic"" covariance, but I don't know what it actually means. Is there a mathematical meaning to this quantity? Specifically, what kind of relationship between $X$ and $Y$ would ensure the above quantity is non-zero? UPDATE: Seems this is equal to $\text{Var}(XY) - \text{Cov}(X^2, Y^2)$. Is there a clearer meaning to this anyway?",,"['probability', 'statistics']"
91,The expected value as an integral,The expected value as an integral,,"Let $X \geq 0$ be continuous random variable with the CDF $F(x) := \displaystyle\int_0^x f(x) \; \mathrm dx$ where $f(x)$ is the PDF. I want to express the (finite) expected value $E[X] := \displaystyle\int_0^\infty x f(x) \; \mathrm dx < \infty$ as an integral that includes the failure rate function $r(t) = \frac{f(x)}{1 - F(x)}$ but not the PDF, CDF or survival function $G(x) = 1 - F(x)$.","Let $X \geq 0$ be continuous random variable with the CDF $F(x) := \displaystyle\int_0^x f(x) \; \mathrm dx$ where $f(x)$ is the PDF. I want to express the (finite) expected value $E[X] := \displaystyle\int_0^\infty x f(x) \; \mathrm dx < \infty$ as an integral that includes the failure rate function $r(t) = \frac{f(x)}{1 - F(x)}$ but not the PDF, CDF or survival function $G(x) = 1 - F(x)$.",,"['real-analysis', 'probability', 'statistics', 'means']"
92,convergence of pdfs vs cdfs,convergence of pdfs vs cdfs,,"Let $X_n$ be a continuous rv with cdf $F_n(x)$ and pdf $f_n(x)$. Let $X$ be a continuous rv with cdf $F(x)$ and pdf $f(x)$. It is known that convergence in distribution, i.e., $F_n(x)\to F(x)$ as $n\to\infty$ at every continuity point $x$ of $F(x)$, does not generally imply that $f_n(x)\to f(x)$ as $n\to\infty$. This question offers a counterexample. My question is: Is there a class of continuous rvs for which $F_n(x)\to F(x)$ does imply $f_n(x)\to f(x)$? What additional conditions are these rvs to satisfy? Incidentally, according to this question , pointwise convergence of $f_n(x)$ to $f(x)$ is enough to have $F_n(x)\to F(x)$. I'm particularly interested in the case when $f_n(x)$ is supported on $[0,n]$ while $f(x)$ is supported on $[0,\infty)$. To boot, $f_n(x)$ is infinitely differentiable at every $x\in[0,n]$ for each $n$, and $f(x)$ is infinitely differentiable at every $x\ge0$. In fact, in my specific case, $f(x)=e^{-x}$, $x\ge0$.","Let $X_n$ be a continuous rv with cdf $F_n(x)$ and pdf $f_n(x)$. Let $X$ be a continuous rv with cdf $F(x)$ and pdf $f(x)$. It is known that convergence in distribution, i.e., $F_n(x)\to F(x)$ as $n\to\infty$ at every continuity point $x$ of $F(x)$, does not generally imply that $f_n(x)\to f(x)$ as $n\to\infty$. This question offers a counterexample. My question is: Is there a class of continuous rvs for which $F_n(x)\to F(x)$ does imply $f_n(x)\to f(x)$? What additional conditions are these rvs to satisfy? Incidentally, according to this question , pointwise convergence of $f_n(x)$ to $f(x)$ is enough to have $F_n(x)\to F(x)$. I'm particularly interested in the case when $f_n(x)$ is supported on $[0,n]$ while $f(x)$ is supported on $[0,\infty)$. To boot, $f_n(x)$ is infinitely differentiable at every $x\in[0,n]$ for each $n$, and $f(x)$ is infinitely differentiable at every $x\ge0$. In fact, in my specific case, $f(x)=e^{-x}$, $x\ge0$.",,"['probability-theory', 'statistics', 'weak-convergence', 'probability-limit-theorems']"
93,Expectation of nonlinear combination of t-distributions,Expectation of nonlinear combination of t-distributions,,"Let $\varepsilon$ be distributed according to the following $t$-distribution: $$p(\varepsilon) = \frac{\Gamma\left(\frac{\nu+1} 2\right)}{\Gamma\left(\frac \nu 2\right)[(\nu-2) \pi\sigma^2]^{1/2}}\left[1+\frac 1 {\nu-2}\left(\frac \varepsilon \sigma \right)^2\right]^{-\frac{\nu+1} 2}$$ so that $\operatorname{E}[\varepsilon]=0$ and $\operatorname{Var}[\varepsilon] = \sigma^2$. Consider the r.v. $\zeta = \frac \varepsilon \sigma$ which has unit variance. Is it possible to compute in closed form $\operatorname{E}\left[\frac{\zeta^2}{(a+\zeta^2)^2}\right],\quad a\in\mathbb{R}$?","Let $\varepsilon$ be distributed according to the following $t$-distribution: $$p(\varepsilon) = \frac{\Gamma\left(\frac{\nu+1} 2\right)}{\Gamma\left(\frac \nu 2\right)[(\nu-2) \pi\sigma^2]^{1/2}}\left[1+\frac 1 {\nu-2}\left(\frac \varepsilon \sigma \right)^2\right]^{-\frac{\nu+1} 2}$$ so that $\operatorname{E}[\varepsilon]=0$ and $\operatorname{Var}[\varepsilon] = \sigma^2$. Consider the r.v. $\zeta = \frac \varepsilon \sigma$ which has unit variance. Is it possible to compute in closed form $\operatorname{E}\left[\frac{\zeta^2}{(a+\zeta^2)^2}\right],\quad a\in\mathbb{R}$?",,"['probability-theory', 'statistics', 'probability-distributions', 'computational-mathematics']"
94,Why doen't we consider nonlinear estimators for the parameters of linear regression models?,Why doen't we consider nonlinear estimators for the parameters of linear regression models?,,"The Gauss-Markov theorem tells us that the ordinary least-squares (OLS) estimator is the best linear unbiased estimator (BLUE) for the coefficients in a linear regression (given some conditions on the errors). I can understand why we want an unbiased and minimum-variance (""best"") estimator, but why linear? Why not an estimator that depends on any other power (square, square root, etc) of the data? More specifically, for an $n\times m$ data matrix $X$ predicting an $n \times 1$ response vector $y$ in the model $y = \beta X + \epsilon$, the OLS estimator for the coefficients $\beta$ is, $$\hat\beta = (X^TX)^{-1}X^Ty = Cy = c_0 y_0 + c_1 y_1 + c_2 y_2 + \cdots $$ Note that $\hat\beta$ is defined linearly in terms of $y_i$ and thus a linear estimator. Is there a particular reason we don't consider estimators of the form, $$ \tilde\beta = Cy^a = c_0 y_0^a + c_1 y_1^a + c_2 y_2^a + \cdots $$","The Gauss-Markov theorem tells us that the ordinary least-squares (OLS) estimator is the best linear unbiased estimator (BLUE) for the coefficients in a linear regression (given some conditions on the errors). I can understand why we want an unbiased and minimum-variance (""best"") estimator, but why linear? Why not an estimator that depends on any other power (square, square root, etc) of the data? More specifically, for an $n\times m$ data matrix $X$ predicting an $n \times 1$ response vector $y$ in the model $y = \beta X + \epsilon$, the OLS estimator for the coefficients $\beta$ is, $$\hat\beta = (X^TX)^{-1}X^Ty = Cy = c_0 y_0 + c_1 y_1 + c_2 y_2 + \cdots $$ Note that $\hat\beta$ is defined linearly in terms of $y_i$ and thus a linear estimator. Is there a particular reason we don't consider estimators of the form, $$ \tilde\beta = Cy^a = c_0 y_0^a + c_1 y_1^a + c_2 y_2^a + \cdots $$",,['statistics']
95,Distribution of logistic regression estimators,Distribution of logistic regression estimators,,"It is well known that for OLS estimators, the parameters are asymptotically normal, i.e. for the regression $y_i = \beta_i x_i$, $$\hat{\beta_i} \sim \mathcal{N}(\beta_i, \sigma^2 (X_i^T X_i)^{-1})$$ Consequently, for any fixed $x_i$, the product $\hat{\beta_i} x_i$ is also normally distributed. Is there any analogous relationship for the parameters of a logistic regression? For context, I am looking to give a bound on the probability that the resultant output from a LR is within a certain interval.","It is well known that for OLS estimators, the parameters are asymptotically normal, i.e. for the regression $y_i = \beta_i x_i$, $$\hat{\beta_i} \sim \mathcal{N}(\beta_i, \sigma^2 (X_i^T X_i)^{-1})$$ Consequently, for any fixed $x_i$, the product $\hat{\beta_i} x_i$ is also normally distributed. Is there any analogous relationship for the parameters of a logistic regression? For context, I am looking to give a bound on the probability that the resultant output from a LR is within a certain interval.",,"['statistics', 'probability-distributions', 'normal-distribution', 'linear-regression', 'logistic-regression']"
96,Likelihood function; Indicator Function approach,Likelihood function; Indicator Function approach,,"I have been solving some problems regarding finding the likelihood function; i.e determining the MLE of a parameter from a statistical distribution, i.e the Normal distribution. Now, I have an expression for the likelihood function of the parameter $a, -\infty < a < \infty$; $x_i$ are iid $i = 0,1,...,n$ $$\frac{1}{a^{9n}}I{\{max (x_i) < a, min (x_i) > -a}\}$$ I know that the MLE is where the graph of this likelihood function is maximum. Obviously this is a decreasing function in $a,$ my thoughts are that the mle is $max(x_i)$ if $|max(x_i)| > |min(x_i)|$, and $min(x_i)$ if $|min(x_i)| > |max(x_i)|$, but I'm unsure whether this is true or not as I haven't seen an example where the MLE depends on the values of the extremum.","I have been solving some problems regarding finding the likelihood function; i.e determining the MLE of a parameter from a statistical distribution, i.e the Normal distribution. Now, I have an expression for the likelihood function of the parameter $a, -\infty < a < \infty$; $x_i$ are iid $i = 0,1,...,n$ $$\frac{1}{a^{9n}}I{\{max (x_i) < a, min (x_i) > -a}\}$$ I know that the MLE is where the graph of this likelihood function is maximum. Obviously this is a decreasing function in $a,$ my thoughts are that the mle is $max(x_i)$ if $|max(x_i)| > |min(x_i)|$, and $min(x_i)$ if $|min(x_i)| > |max(x_i)|$, but I'm unsure whether this is true or not as I haven't seen an example where the MLE depends on the values of the extremum.",,"['statistics', 'statistical-inference', 'maximum-likelihood']"
97,Convergence in distribution yields something peculiar.,Convergence in distribution yields something peculiar.,,"I'm solving a question that wants me to prove that for a sample X1, X2,X3, .. from a given distribution with density:   f(x) = $\frac{1}{2}(1+x)e^{-x}$, for positive x, and 0 elsewhere, the sequence $n\cdot Y_{n}$, where $Y_{n} = min\{X1,X2,..,Xn\}$  converges in distribution to a certain random variable. I've used independence to show  that $$P(n\cdot Y_{n}\leq z) = (P(X\leq z/n))^{n} =  (1-\frac{1}{2}e^{-z/n}(\frac{z}{n}+2))^{n}$$ but when I take n to infinity this goes to zero. Did I miscalculate something? Does this mean the limit doesn't converge?","I'm solving a question that wants me to prove that for a sample X1, X2,X3, .. from a given distribution with density:   f(x) = $\frac{1}{2}(1+x)e^{-x}$, for positive x, and 0 elsewhere, the sequence $n\cdot Y_{n}$, where $Y_{n} = min\{X1,X2,..,Xn\}$  converges in distribution to a certain random variable. I've used independence to show  that $$P(n\cdot Y_{n}\leq z) = (P(X\leq z/n))^{n} =  (1-\frac{1}{2}e^{-z/n}(\frac{z}{n}+2))^{n}$$ but when I take n to infinity this goes to zero. Did I miscalculate something? Does this mean the limit doesn't converge?",,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
98,Find the distribution of $R=\bar X / \bar Y$ where $X$ and $Y$ are exponentially distributed,Find the distribution of  where  and  are exponentially distributed,R=\bar X / \bar Y X Y,"Find the distribution of $R=\bar X / \bar Y$ given that exponential   distribution with mean parameter 2 is equivalent to Chi squared with 2   dof. I'm given 2 samples $x_i$ and $y_i$ of sizes $n$ and $m$ respectevely such that  $$f(x,\theta)=\frac{1}{\theta}e^{-x/\theta}$$ and $$f(y,\theta)=\frac{1}{\theta}e^{-y/\theta}$$ Construct confidence interval of $R$? I think it should also me exponentially distributed, but I'm struggling even starting this question. In particular, the different sizes of samples put me off doing $f(R,\theta)=f(x)/f(y)$ I though about about finding the mean $E(X)=\int^{\infty}_0 x f(x,\theta)dx$ and finding $E(R)=E(X)/E(Y)$","Find the distribution of $R=\bar X / \bar Y$ given that exponential   distribution with mean parameter 2 is equivalent to Chi squared with 2   dof. I'm given 2 samples $x_i$ and $y_i$ of sizes $n$ and $m$ respectevely such that  $$f(x,\theta)=\frac{1}{\theta}e^{-x/\theta}$$ and $$f(y,\theta)=\frac{1}{\theta}e^{-y/\theta}$$ Construct confidence interval of $R$? I think it should also me exponentially distributed, but I'm struggling even starting this question. In particular, the different sizes of samples put me off doing $f(R,\theta)=f(x)/f(y)$ I though about about finding the mean $E(X)=\int^{\infty}_0 x f(x,\theta)dx$ and finding $E(R)=E(X)/E(Y)$",,"['statistics', 'exponential-distribution']"
99,Probability of being alergic given that does not pratice sports,Probability of being alergic given that does not pratice sports,,"In a city, it's estimated that $20\%$ of people have some type of   alergy. I'ts known that $50\%$ of alergic people pratice sports, while   this percentage for the non alergic people is $40\%$. For an   individual chosen randomly in ths city, find the probability that   this person is alergic given that it does not pratice sports While I was writing this exercise I found my error so I'm just gonna leave it here to help people I denoted by $A$ the event ""alergic"" and $S$ for sports. So, the exercise wants to know $P(A|S^c)$ which is by bayes theorem: $$P(A|S^c) = \frac{P(S^c|A)P(A)}{P(S^c)}$$ Well, $P(S^c|A) = 0.5$ since if $50\%$ of alergic people pratice, $50\%$ does not. The probability of being alergic is $20\%$ as said in the exercise. Now, $P(S^c)$ is the probability of: being alergic and do not pratice sports + not being alergic and do not pratice sports = $0.2\cdot 0.5 + 0.8 + 0.6$, so the result is: $$P(A|S^c) = \frac{P(S^c|A)P(A)}{P(S^c)} = \frac{0.5\cdot 0.2}{0.2\cdot 0.5 + 0.8 \cdot 0.6} = 0.172413\cdots$$ Turns out my error was that I was writing $P(S^c|A)$ as $0.5\cdot 0.2$ instead of just $0.5$","In a city, it's estimated that $20\%$ of people have some type of   alergy. I'ts known that $50\%$ of alergic people pratice sports, while   this percentage for the non alergic people is $40\%$. For an   individual chosen randomly in ths city, find the probability that   this person is alergic given that it does not pratice sports While I was writing this exercise I found my error so I'm just gonna leave it here to help people I denoted by $A$ the event ""alergic"" and $S$ for sports. So, the exercise wants to know $P(A|S^c)$ which is by bayes theorem: $$P(A|S^c) = \frac{P(S^c|A)P(A)}{P(S^c)}$$ Well, $P(S^c|A) = 0.5$ since if $50\%$ of alergic people pratice, $50\%$ does not. The probability of being alergic is $20\%$ as said in the exercise. Now, $P(S^c)$ is the probability of: being alergic and do not pratice sports + not being alergic and do not pratice sports = $0.2\cdot 0.5 + 0.8 + 0.6$, so the result is: $$P(A|S^c) = \frac{P(S^c|A)P(A)}{P(S^c)} = \frac{0.5\cdot 0.2}{0.2\cdot 0.5 + 0.8 \cdot 0.6} = 0.172413\cdots$$ Turns out my error was that I was writing $P(S^c|A)$ as $0.5\cdot 0.2$ instead of just $0.5$",,"['probability', 'statistics']"
