,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Remarquable identities $f(n) = \frac{a^n}{(a-b)(a-c)} + \frac{b^n}{(b-a)(b-c)} + \frac{c^n}{(c-a)(c-b)}$,Remarquable identities,f(n) = \frac{a^n}{(a-b)(a-c)} + \frac{b^n}{(b-a)(b-c)} + \frac{c^n}{(c-a)(c-b)},"Let $n$ be an integer, and \begin{equation} f(n) = \frac{a^n}{(a-b)(a-c)} + \frac{b^n}{(b-a)(b-c)} + \frac{c^n}{(c-a)(c-b)} \end{equation} \begin{equation} g(n) = \frac{(bc)^n}{(a-b)(a-c)} + \frac{(ac)^n}{(b-a)(b-c)} + \frac{(ab)^n}{(c-a)(c-b)} \end{equation} We have the following impressive identities, for all $a,b,c$ , \begin{align} f(0) &= 0 \\  f(1) &= 0 \\ f(2) &= 1 \\ f(3) &= a+b+c \\ f(4) &= a^2 + b^2 + c^2 + ab + ac + bc \\ f(5) &= a^3 + b^3 + c^3 + a^2b + a^2c + b^2c + ab^2 + ac^2 + bc^2 \\ f(6) &= a^4 + b^4 + c^4 + a^3b + a^3c + b^3c + ab^3 + ac^3 + bc^3 + a^2bc + ab^2c + abc^2 +a^2b^2 + a^2c^2 + b^2c^2 \\  \\ g(0) &= 0 \\  g(1) &= 1 \\ g(2) &= ab + ac + bc \\ g(3) &= a^2b^2 + a^2c^2 + b^2c^2 + a^2bc + ab^2c + abc^2 \\ g(4) &= a^3b^3 + a^3c^3 + b^3c^3 + a^3b^2c + a^3bc^2 + a^2b^3c + ab^3c^2 + a^2bc^3 + ab^2c^3 + a^2b^2c^2      \end{align} which I have verified by plugging the expressions into Wolfram Alpha. It seems that the general form should be, for $n > 2$ . \begin{align} f(n) &= \sum_{i+j+k = n-2}a^ib^jc^k \\ g(n) &= \sum_{\substack{i+j+k = 2(n-1)\\1\leq i,j,k \leq n-1}}a^ib^jc^k \end{align} The questions are : How to demonstrate the statements for $n$ general using induction. Intuitively, we should use induction, however I do not see the induction step. Could we demonstrate the general case without using induction ? There is a link, between these formulas and Vandermondt matrices (see below), would there be a nice demonstration using matrices ? ======================================================================= I arrived at such identities when working with partial fractions decomposition, and after some related work I realized that the Vandermondt matrices where almost the inverse of the matrices which appear when we do partial fractions decomposition, Then I realised that the Vandermondt matrices have very nice inverse : \begin{equation} \begin{pmatrix} 1&1 \\ a&b  \end{pmatrix} \begin{pmatrix} -\frac{b}{(a - b)} & \frac{1}{(a - b)} \\ -\frac{a}{(b - a)} & \frac{1}{(b - a)} \\ \end{pmatrix} = I_{2} \end{equation} \begin{equation} \begin{pmatrix} 1&1&1 \\ a&b&c \\ a^2&b^2&c^2 \end{pmatrix} \begin{pmatrix} \frac{b c}{(a - b) (a - c)} & -\frac{b + c}{(a - b) (a - c)} & \frac{1}{(a - b) (a - c)} \\ \frac{a c}{(b - a) (b - c)} & -\frac{a + c}{(b - a) (b - c)} & \frac{1}{(b - a) (b - c)} \\ \frac{a b}{(c - a) (c - b)} & -\frac{a + b}{(c - a) (c - b)} & \frac{1}{(c - a) (c - b)} \end{pmatrix} = I_{3} \end{equation} \begin{equation} \begin{pmatrix} 1&1&1&1 \\ a&b&c&d \\ a^2&b^2&c^2&d^2 \\ a^3&b^3&c^3&d^3 \end{pmatrix} \begin{pmatrix} -\frac{bcd}{(a - b) (a - c)(a-d)} & \frac{bc + cd + bd}{(a - b) (a - c)(a-d)} &-\frac{b+c+d}{(a - b) (a - c)(a-d)} &  \frac{1}{(a - b) (a - c)(a-d)}\\ -\frac{a cd}{(b - a) (b - c)(b-d)} & \frac{ac + ad + cd}{(b - a) (b - c)(b-d)} & -\frac{a + c + d}{(b - a) (b - c)(b-d)}& \frac{1}{(b - a) (b - c)(b-d)}\\ -\frac{a bd}{(c - a) (c - b)(c-d)} & \frac{ab + ad + bd}{(c - a) (c - b)(c-d)} & -\frac{a + b + d}{(c - a) (c - b)(c-d)}&\frac{1}{(c - a) (c - b)(c-d)}\\ -\frac{a bc}{(d - a) (d - b)(d-c)} & \frac{ab + ac + bc}{(d - a) (d - b)(d-c)} & -\frac{a + b + c}{(d - a) (d - b)(d-c)}&\frac{1}{(d - a) (d - b)(d-c)} \end{pmatrix} = I_{4} \end{equation} The identities $f(0),f(1), f(2)$ are the last column of the inverse equation for 3-dim matrices. However, it seems that such matrix argument is not sufficient to prove the case for $n$ general, and that many similar identities (the other places in the matrices) should exist.","Let be an integer, and We have the following impressive identities, for all , which I have verified by plugging the expressions into Wolfram Alpha. It seems that the general form should be, for . The questions are : How to demonstrate the statements for general using induction. Intuitively, we should use induction, however I do not see the induction step. Could we demonstrate the general case without using induction ? There is a link, between these formulas and Vandermondt matrices (see below), would there be a nice demonstration using matrices ? ======================================================================= I arrived at such identities when working with partial fractions decomposition, and after some related work I realized that the Vandermondt matrices where almost the inverse of the matrices which appear when we do partial fractions decomposition, Then I realised that the Vandermondt matrices have very nice inverse : The identities are the last column of the inverse equation for 3-dim matrices. However, it seems that such matrix argument is not sufficient to prove the case for general, and that many similar identities (the other places in the matrices) should exist.","n \begin{equation}
f(n) = \frac{a^n}{(a-b)(a-c)} + \frac{b^n}{(b-a)(b-c)} + \frac{c^n}{(c-a)(c-b)}
\end{equation} \begin{equation}
g(n) = \frac{(bc)^n}{(a-b)(a-c)} + \frac{(ac)^n}{(b-a)(b-c)} + \frac{(ab)^n}{(c-a)(c-b)}
\end{equation} a,b,c \begin{align}
f(0) &= 0 \\ 
f(1) &= 0 \\
f(2) &= 1 \\
f(3) &= a+b+c \\
f(4) &= a^2 + b^2 + c^2 + ab + ac + bc \\
f(5) &= a^3 + b^3 + c^3 + a^2b + a^2c + b^2c + ab^2 + ac^2 + bc^2 \\
f(6) &= a^4 + b^4 + c^4 + a^3b + a^3c + b^3c + ab^3 + ac^3 + bc^3 + a^2bc + ab^2c + abc^2 +a^2b^2 + a^2c^2 + b^2c^2 \\
 \\
g(0) &= 0 \\ 
g(1) &= 1 \\
g(2) &= ab + ac + bc \\
g(3) &= a^2b^2 + a^2c^2 + b^2c^2 + a^2bc + ab^2c + abc^2 \\
g(4) &= a^3b^3 + a^3c^3 + b^3c^3 + a^3b^2c + a^3bc^2 + a^2b^3c + ab^3c^2 + a^2bc^3 + ab^2c^3 + a^2b^2c^2     
\end{align} n > 2 \begin{align}
f(n) &= \sum_{i+j+k = n-2}a^ib^jc^k \\
g(n) &= \sum_{\substack{i+j+k = 2(n-1)\\1\leq i,j,k \leq n-1}}a^ib^jc^k
\end{align} n \begin{equation}
\begin{pmatrix}
1&1 \\
a&b 
\end{pmatrix}
\begin{pmatrix}
-\frac{b}{(a - b)} & \frac{1}{(a - b)} \\
-\frac{a}{(b - a)} & \frac{1}{(b - a)} \\
\end{pmatrix}
= I_{2}
\end{equation} \begin{equation}
\begin{pmatrix}
1&1&1 \\
a&b&c \\
a^2&b^2&c^2
\end{pmatrix}
\begin{pmatrix}
\frac{b c}{(a - b) (a - c)} & -\frac{b + c}{(a - b) (a - c)} & \frac{1}{(a - b) (a - c)} \\
\frac{a c}{(b - a) (b - c)} & -\frac{a + c}{(b - a) (b - c)} & \frac{1}{(b - a) (b - c)} \\
\frac{a b}{(c - a) (c - b)} & -\frac{a + b}{(c - a) (c - b)} & \frac{1}{(c - a) (c - b)}
\end{pmatrix}
= I_{3}
\end{equation} \begin{equation}
\begin{pmatrix}
1&1&1&1 \\
a&b&c&d \\
a^2&b^2&c^2&d^2 \\
a^3&b^3&c^3&d^3
\end{pmatrix}
\begin{pmatrix}
-\frac{bcd}{(a - b) (a - c)(a-d)} & \frac{bc + cd + bd}{(a - b) (a - c)(a-d)} &-\frac{b+c+d}{(a - b) (a - c)(a-d)} &  \frac{1}{(a - b) (a - c)(a-d)}\\
-\frac{a cd}{(b - a) (b - c)(b-d)} & \frac{ac + ad + cd}{(b - a) (b - c)(b-d)} & -\frac{a + c + d}{(b - a) (b - c)(b-d)}& \frac{1}{(b - a) (b - c)(b-d)}\\
-\frac{a bd}{(c - a) (c - b)(c-d)} & \frac{ab + ad + bd}{(c - a) (c - b)(c-d)} & -\frac{a + b + d}{(c - a) (c - b)(c-d)}&\frac{1}{(c - a) (c - b)(c-d)}\\
-\frac{a bc}{(d - a) (d - b)(d-c)} & \frac{ab + ac + bc}{(d - a) (d - b)(d-c)} & -\frac{a + b + c}{(d - a) (d - b)(d-c)}&\frac{1}{(d - a) (d - b)(d-c)}
\end{pmatrix}
= I_{4}
\end{equation} f(0),f(1), f(2) n","['matrices', 'algebra-precalculus', 'inverse']"
1,Proof that any element of a free abelian group which is not divisible by any $k>1$ can be extended to a basis,Proof that any element of a free abelian group which is not divisible by any  can be extended to a basis,k>1,"Let $A$ be a free abelian group of rank $n$ , and let $\alpha_1 \in A\setminus \{0\}$ such that $\alpha_1 \not \in kA$ for all $k > 1$ . Do there always exist $\alpha_2,\ldots, \alpha_n \in A$ such that $\alpha_1,\ldots, \alpha_n$ is a basis for $A$ ? This question implies that the answer is yes, but it doesn't give any justification, and there's no proof in the accepted answer. Can anyone point me to a proof of the claim? Attempting to generalise the example in the linked question, we might try letting $e_1,\ldots, e_n$ be a basis and writing $\alpha_1 = c_1e_1 + \ldots + c_ne_n$ for integers $c_i$ . Then we would need a series of elementary column operations taking the vector $(c_1,\ldots, c_n)$ to a vector containing one $1$ and all other entries $0$ . Since $k\nmid \alpha_1$ for all $k>1$ , we have that the $c_i$ are mutually coprime, so there exist integers $x_1,\ldots, x_n$ with $c_1x_1 + \ldots c_nx_n = 1$ . Maybe we can somehow use this combination to construct the desired $1$ . Beyond that, I'm not too sure how to proceed.","Let be a free abelian group of rank , and let such that for all . Do there always exist such that is a basis for ? This question implies that the answer is yes, but it doesn't give any justification, and there's no proof in the accepted answer. Can anyone point me to a proof of the claim? Attempting to generalise the example in the linked question, we might try letting be a basis and writing for integers . Then we would need a series of elementary column operations taking the vector to a vector containing one and all other entries . Since for all , we have that the are mutually coprime, so there exist integers with . Maybe we can somehow use this combination to construct the desired . Beyond that, I'm not too sure how to proceed.","A n \alpha_1 \in A\setminus \{0\} \alpha_1 \not \in kA k > 1 \alpha_2,\ldots, \alpha_n \in A \alpha_1,\ldots, \alpha_n A e_1,\ldots, e_n \alpha_1 = c_1e_1 + \ldots + c_ne_n c_i (c_1,\ldots, c_n) 1 0 k\nmid \alpha_1 k>1 c_i x_1,\ldots, x_n c_1x_1 + \ldots c_nx_n = 1 1","['matrices', 'abelian-groups', 'integer-lattices', 'free-modules', 'free-abelian-group']"
2,How to prove a matrix $[a_{ij}]$ with $a_{ji} = 1/a_{ij}$ has no complex eigenvalues?,How to prove a matrix  with  has no complex eigenvalues?,[a_{ij}] a_{ji} = 1/a_{ij},"I've been given a matrix of the form $$\begin{pmatrix} -1 & 1/a_{21} & \cdots & 1/a_{n1} \\ a_{21} & -1 &\cdots & 1/a_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & -1 \end{pmatrix}$$ which satisfies $$ \sum_{j=1}^n a_{ij} = 0,\space\space\forall\space i $$ and I've been asked to prove whether it can have complex eigenvalues when all its coefficients are real. I've been trying to throw properties of matrices and eigevalues to it for a week but I haven't gotten anything. For $n = 2$ , I know its eigenvalues are $0$ and $-2$ , its determinant is $0$ and its trace is $-2$ . For $n = 3$ it also has a $0$ determinant, but this time only after applying the extra property that the sum of all the coefficients of any column must be $0$ . I haven't been able to prove whether the determinant should be $0$ for any natural $n$ or not. I don't know if there's an easy way to prove this, maybe it's possible to prove they don't even without that extra constraint. Any hint or idea I could try would be very welcome!","I've been given a matrix of the form which satisfies and I've been asked to prove whether it can have complex eigenvalues when all its coefficients are real. I've been trying to throw properties of matrices and eigevalues to it for a week but I haven't gotten anything. For , I know its eigenvalues are and , its determinant is and its trace is . For it also has a determinant, but this time only after applying the extra property that the sum of all the coefficients of any column must be . I haven't been able to prove whether the determinant should be for any natural or not. I don't know if there's an easy way to prove this, maybe it's possible to prove they don't even without that extra constraint. Any hint or idea I could try would be very welcome!","\begin{pmatrix}
-1 & 1/a_{21} & \cdots & 1/a_{n1}
\\
a_{21} & -1 &\cdots & 1/a_{n2}
\\
\vdots & \vdots & \ddots & \vdots
\\
a_{n1} & a_{n2} & \cdots & -1
\end{pmatrix} 
\sum_{j=1}^n a_{ij} = 0,\space\space\forall\space i
 n = 2 0 -2 0 -2 n = 3 0 0 0 n","['matrices', 'eigenvalues-eigenvectors']"
3,"Can every symmetric, unimodular and positive definite $G\in\mathbb{Z}^{n\times n}$ be written as $G=U^TU$?","Can every symmetric, unimodular and positive definite  be written as ?",G\in\mathbb{Z}^{n\times n} G=U^TU,"Let $G\in\mathbb{Z}^{n\times n}$ be symmetric, unimodular and positive definite. Does there exist a unimodular matrix $U\in\mathbb{Z}^{n\times n}$ such that $G=U^TU$ ? I now that the result is true if $n=2$ , but I have a feeling that it fails if $n$ gets larger.","Let be symmetric, unimodular and positive definite. Does there exist a unimodular matrix such that ? I now that the result is true if , but I have a feeling that it fails if gets larger.",G\in\mathbb{Z}^{n\times n} U\in\mathbb{Z}^{n\times n} G=U^TU n=2 n,"['matrices', 'positive-definite', 'unimodular-matrices']"
4,SVD of a matrix when its columns are shuffled and when certain columns are removed,SVD of a matrix when its columns are shuffled and when certain columns are removed,,"as far as I can tell, if the columns of a matrix are shuffled, an SVD would still give the same left singular vectors and eigenvalues, and a permutation (based on which column is now in which place) of the right singular vectors. I only made several trials on R, and I'm not sure if this is theoretically valid. Moreover, and more importantly, I want to find out what happens or if there is any relationship between the SVDs of a certain matrix, and when some of its columns are removed. Basically, here is what I'm trying to find out. For the first part of my question, let's say I have a matrix $\mathbf{X}$ . $\mathbf{X} = UDV^\top$ . If $\mathbf{X} = [x_1 | x_2 | x_3 | x_4 | \dots | x_{2n-1} | x_{2n}]$ , when I move every ""even"" column to the end, I get $\mathbf{Y} = [x_1 | x_3 | x_5 | \dots | x_{2n-1} \mathbf{|} x_2 | x_4 | x_6 | \dots | x_{2n}]$ . SVD of it leads to $\mathbf{Y} = UDQ^\top$ . Moreover, it seems that the rows of $Q$ are the (respectively) reshuffled rows of $V$ . I assume these are also theoretically correct. For the second part of my question, let's say, again, I have a matrix $\mathbf{X}$ . Let's also say I want to remove every second column, and perform an SVD. So, if $\mathbf{X} = [x_1 | x_2 | x_3 | x_4 | \dots | x_{2n-1} | x_{2n}]$ , and I remove every ""even"" column, I end up with $\mathbf{Y} = [x_1 | x_3 | x_5 | \dots | x_{2n-1}]$ . If, originally, $\mathbf{X} = UDV^\top$ , I want to know now what is the SVD of $\mathbf{Y}$ , if it's possible to relate it to the original? To be concise, I am trying to infer something about the components of the SVD of $\mathbf{X}$ without actually calculating its SVD, but rather only concern myself with the SVD of $\mathbf{Y}$ , because in my problem, it's extremely easier to get the SVD of $\mathbf{Y}$ , but I still kind of need information on the SVD of $\mathbf{X}$ . In short, I want to know what can be inferred from the SVD of $\mathbf{Y}$ regarding the SVD of $\mathbf{X}$ .","as far as I can tell, if the columns of a matrix are shuffled, an SVD would still give the same left singular vectors and eigenvalues, and a permutation (based on which column is now in which place) of the right singular vectors. I only made several trials on R, and I'm not sure if this is theoretically valid. Moreover, and more importantly, I want to find out what happens or if there is any relationship between the SVDs of a certain matrix, and when some of its columns are removed. Basically, here is what I'm trying to find out. For the first part of my question, let's say I have a matrix . . If , when I move every ""even"" column to the end, I get . SVD of it leads to . Moreover, it seems that the rows of are the (respectively) reshuffled rows of . I assume these are also theoretically correct. For the second part of my question, let's say, again, I have a matrix . Let's also say I want to remove every second column, and perform an SVD. So, if , and I remove every ""even"" column, I end up with . If, originally, , I want to know now what is the SVD of , if it's possible to relate it to the original? To be concise, I am trying to infer something about the components of the SVD of without actually calculating its SVD, but rather only concern myself with the SVD of , because in my problem, it's extremely easier to get the SVD of , but I still kind of need information on the SVD of . In short, I want to know what can be inferred from the SVD of regarding the SVD of .",\mathbf{X} \mathbf{X} = UDV^\top \mathbf{X} = [x_1 | x_2 | x_3 | x_4 | \dots | x_{2n-1} | x_{2n}] \mathbf{Y} = [x_1 | x_3 | x_5 | \dots | x_{2n-1} \mathbf{|} x_2 | x_4 | x_6 | \dots | x_{2n}] \mathbf{Y} = UDQ^\top Q V \mathbf{X} \mathbf{X} = [x_1 | x_2 | x_3 | x_4 | \dots | x_{2n-1} | x_{2n}] \mathbf{Y} = [x_1 | x_3 | x_5 | \dots | x_{2n-1}] \mathbf{X} = UDV^\top \mathbf{Y} \mathbf{X} \mathbf{Y} \mathbf{Y} \mathbf{X} \mathbf{Y} \mathbf{X},"['matrices', 'svd']"
5,Max eigenvalue of symmetric matrix and its relation to diagonal values,Max eigenvalue of symmetric matrix and its relation to diagonal values,,"I saw few questions about it, but still can't understand. Let $A$ be a symmetric matrix and $\lambda_{\max}$ its largest eigenvalue. Is the following true for all $A$ ? $$ \lambda_{\max} \ge a_{ii} \forall i $$ That is, is the largest eigenvalue of a symmetric matrix always greater than any of its diagonal entries? Is it somehow related to spectral radius and the following equation? $$ \rho(A)=\max|\lambda_i|. $$","I saw few questions about it, but still can't understand. Let be a symmetric matrix and its largest eigenvalue. Is the following true for all ? That is, is the largest eigenvalue of a symmetric matrix always greater than any of its diagonal entries? Is it somehow related to spectral radius and the following equation?","A \lambda_{\max} A 
\lambda_{\max} \ge a_{ii} \forall i
 
\rho(A)=\max|\lambda_i|.
","['matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'spectral-radius']"
6,$\operatorname{Tr}(AB)=\operatorname{Tr}(A)\operatorname{Tr}(B)$,,\operatorname{Tr}(AB)=\operatorname{Tr}(A)\operatorname{Tr}(B),"Problem. Show that $\operatorname{Tr}(AB)=\operatorname{Tr}(A)\operatorname{Tr}(B)$ , if $A^2+3AB+B^2=BA$ and $\det(A)=0$ , where all matrices involved is real valued $2\times2$ matrices. First I rewrote the $A^2+3AB+B^2=BA$ as $$ A(A+3B)=B(A-B) $$ and I took the determinant. Then I obtained that $$ \det(B)\det(A-B)=0, $$ because $\det(A)=0$ . So, $\det(B)=0$ or $\det(A-B)=0$ . Case I. Then, by Cayley-Hamiton theorem, I wrote that $$ A^2=(\operatorname{Tr}(A))A \text{ and } B^2=(\operatorname{Tr}(B))B. $$ Multiplying the above two equations, I got $(AB)^2=(\operatorname{Tr}(A)\operatorname{Tr}(B))AB$ . Another use of Cayley theorem got $(AB)^2=(\operatorname{Tr}(AB))AB$ . So, $\operatorname{Tr}(A)\operatorname{Tr}(B)$ must be equal with $\operatorname{Tr}(AB)$ only if $AB=BA$ . But I don't have such a hypothesis. Case II. For the case where $\det(A-B)$ I don't know how to handle this one in order to get the answear.","Problem. Show that , if and , where all matrices involved is real valued matrices. First I rewrote the as and I took the determinant. Then I obtained that because . So, or . Case I. Then, by Cayley-Hamiton theorem, I wrote that Multiplying the above two equations, I got . Another use of Cayley theorem got . So, must be equal with only if . But I don't have such a hypothesis. Case II. For the case where I don't know how to handle this one in order to get the answear.","\operatorname{Tr}(AB)=\operatorname{Tr}(A)\operatorname{Tr}(B) A^2+3AB+B^2=BA \det(A)=0 2\times2 A^2+3AB+B^2=BA  A(A+3B)=B(A-B)   \det(B)\det(A-B)=0,  \det(A)=0 \det(B)=0 \det(A-B)=0  A^2=(\operatorname{Tr}(A))A \text{ and } B^2=(\operatorname{Tr}(B))B.  (AB)^2=(\operatorname{Tr}(A)\operatorname{Tr}(B))AB (AB)^2=(\operatorname{Tr}(AB))AB \operatorname{Tr}(A)\operatorname{Tr}(B) \operatorname{Tr}(AB) AB=BA \det(A-B)",['matrices']
7,Is there an easy way to compute the determinant of this block matrix?,Is there an easy way to compute the determinant of this block matrix?,,"$$M = \begin{bmatrix} X & \begin{bmatrix} I\\I\\-I\\-I\end{bmatrix}\\ \begin{bmatrix}-I&-I&I&I\end{bmatrix}&O\end{bmatrix}$$ Is there an easy way to compute the determinant of the block matrix above? Or, at least, can I know if $\det (M) > 0$ if $X$ is positive definite? $X$ is, for example, an $8 \times 8$ matrix while $I$ is the $2 \times 2$ identity and $O$ is the the $2 \times 2$ zero matrix.","$$M = \begin{bmatrix} X & \begin{bmatrix} I\\I\\-I\\-I\end{bmatrix}\\ \begin{bmatrix}-I&-I&I&I\end{bmatrix}&O\end{bmatrix}$$ Is there an easy way to compute the determinant of the block matrix above? Or, at least, can I know if $\det (M) > 0$ if $X$ is positive definite? $X$ is, for example, an $8 \times 8$ matrix while $I$ is the $2 \times 2$ identity and $O$ is the the $2 \times 2$ zero matrix.",,"['matrices', 'determinant', 'numerical-linear-algebra', 'positive-definite', 'block-matrices']"
8,Trace permutation of product of symmetric matrices,Trace permutation of product of symmetric matrices,,"It is well known that the trace of a product of matrices is invariant under cyclic permutations. However, in general this is not true for arbitrary permutations. So, if $A,B,C$ are $n \times n$ matrices then $\mathrm{Tr}(ABC)$ is not necessarily equal to $\mathrm{Tr}(ACB)$. However, if we assume $A,B,C$ to also be symmetric, then using the fact that the trace is invariant to taking transpose (as well as cyclic permutations) we obtain $$\mathrm{Tr}(A^TB^TC^T) = \mathrm{Tr}(A^T(CB)^T) = \mathrm{Tr}((CB)^TA^T) =\mathrm{Tr}((ACB)^T) = \mathrm{Tr}(ACB).$$ And, in fact, this allows us to show that the trace of a product of 3 matrices is preserved by taking any permutation of the product. For products of more than 4 matrices, some permutations will not preserve the trace. Still, when all matrices involved in the product are symmetric there are more permutations preserving the trace, than just the cyclic permutations. Is there a complete characterization for which permutations preserve the trace of a product of $n$ symmetric matrices?","It is well known that the trace of a product of matrices is invariant under cyclic permutations. However, in general this is not true for arbitrary permutations. So, if $A,B,C$ are $n \times n$ matrices then $\mathrm{Tr}(ABC)$ is not necessarily equal to $\mathrm{Tr}(ACB)$. However, if we assume $A,B,C$ to also be symmetric, then using the fact that the trace is invariant to taking transpose (as well as cyclic permutations) we obtain $$\mathrm{Tr}(A^TB^TC^T) = \mathrm{Tr}(A^T(CB)^T) = \mathrm{Tr}((CB)^TA^T) =\mathrm{Tr}((ACB)^T) = \mathrm{Tr}(ACB).$$ And, in fact, this allows us to show that the trace of a product of 3 matrices is preserved by taking any permutation of the product. For products of more than 4 matrices, some permutations will not preserve the trace. Still, when all matrices involved in the product are symmetric there are more permutations preserving the trace, than just the cyclic permutations. Is there a complete characterization for which permutations preserve the trace of a product of $n$ symmetric matrices?",,"['matrices', 'permutations']"
9,Is dot product the only rotation invariant function?,Is dot product the only rotation invariant function?,,"I am looking for rotation invariant scalar functions $f(x,y): x,y \in R^3$ that are not some scalar function over the dot product (or norm), i.e. $ f \neq g(x\cdot y, \Vert x \Vert, \Vert y \Vert ) $ Do they exist ? Edit: Edited to clarify that the norm is just another form of the dot product, and the norm being rotation invariant is really just the dot product being invariant.","I am looking for rotation invariant scalar functions $f(x,y): x,y \in R^3$ that are not some scalar function over the dot product (or norm), i.e. $ f \neq g(x\cdot y, \Vert x \Vert, \Vert y \Vert ) $ Do they exist ? Edit: Edited to clarify that the norm is just another form of the dot product, and the norm being rotation invariant is really just the dot product being invariant.",,"['matrices', 'functions', 'rotations', 'geometric-invariant']"
10,Det(AB)=0: what is the determinant of A and B,Det(AB)=0: what is the determinant of A and B,,"True or false. If the determinant of AB is zero, then the determinant of A is zero or the determinant of B is zero. I put true in my exam. After all det(A)det(B)=det(AB). Why was I wrong? The answer is apparently is wrong.","True or false. If the determinant of AB is zero, then the determinant of A is zero or the determinant of B is zero. I put true in my exam. After all det(A)det(B)=det(AB). Why was I wrong? The answer is apparently is wrong.",,"['matrices', 'determinant']"
11,Matrix (correct) notation,Matrix (correct) notation,,"Say I have a real $m \times n$ matrix $\mathbf{M}$. Shall I write $\mathbf{M} \in \mathbb{R}^{m \times n}$ or $\mathbf{M} \in \mathbb{R}^{m,n}$? What is commonly accepted and most beautiful and correct version? Moreover, shall I use an italic bold font like $\boldsymbol{M}$ or vertical bold $\mathbf{M}$?  And same question for vectors: I personally dislike $\mathbf{x}$ and prefer $\boldsymbol{x}$. What is commonly and beautifully accepted? In conclusion, if I got it right, $\boldsymbol{M}\in\mathbb{R}^{m \times n}$ and  $M_{i,j}\in\mathbb{R}$.","Say I have a real $m \times n$ matrix $\mathbf{M}$. Shall I write $\mathbf{M} \in \mathbb{R}^{m \times n}$ or $\mathbf{M} \in \mathbb{R}^{m,n}$? What is commonly accepted and most beautiful and correct version? Moreover, shall I use an italic bold font like $\boldsymbol{M}$ or vertical bold $\mathbf{M}$?  And same question for vectors: I personally dislike $\mathbf{x}$ and prefer $\boldsymbol{x}$. What is commonly and beautifully accepted? In conclusion, if I got it right, $\boldsymbol{M}\in\mathbb{R}^{m \times n}$ and  $M_{i,j}\in\mathbb{R}$.",,"['matrices', 'notation']"
12,Solving a second-order matrix differential equation,Solving a second-order matrix differential equation,,"I have the differential equation $\frac{d^2 x}{dt^2}+Ax=0$ where $A$ is a matrix and $$\frac{d^2 x}{dt^2}=\left(\frac{d^2 x_1}{dt^2},\frac{d^2 x_2}{dt^2},\ldots,\frac{d^2 x_n}{dt^2}\right)^T\text{ for }x=(x_1,x_2,\ldots,x_n).$$ I know the solution is $x=\cos(\sqrt{A}(t-t_0))x_0+(\sqrt{A})^{-1}\sin(\sqrt{A}(t-t_0))\dot{x}_0$, but I have no idea why this is. How would I go about deriving this solution? The literature I have looked at simply states the solution with little or no explanation.","I have the differential equation $\frac{d^2 x}{dt^2}+Ax=0$ where $A$ is a matrix and $$\frac{d^2 x}{dt^2}=\left(\frac{d^2 x_1}{dt^2},\frac{d^2 x_2}{dt^2},\ldots,\frac{d^2 x_n}{dt^2}\right)^T\text{ for }x=(x_1,x_2,\ldots,x_n).$$ I know the solution is $x=\cos(\sqrt{A}(t-t_0))x_0+(\sqrt{A})^{-1}\sin(\sqrt{A}(t-t_0))\dot{x}_0$, but I have no idea why this is. How would I go about deriving this solution? The literature I have looked at simply states the solution with little or no explanation.",,"['matrices', 'ordinary-differential-equations']"
13,Help understanding $e^{it}=\cos t+i\sin t$ by way of matrices and vector fields,Help understanding  by way of matrices and vector fields,e^{it}=\cos t+i\sin t,"I was brushing up on my complex arithmetic in preparation for a class in ODE's this semester and I found myself looking at Exercise 2.7.5 in Introduction to Complex Analysis for Engineers by Michael Alder, which reads The exponential function is a procedure for turning vector fields into flows; if you take the vector field which is given by$$V\begin{bmatrix} x \\ y \end{bmatrix}=\begin{bmatrix} 0 & -1\\ 1 & 0 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$$   you call the matrix $A$ and then the flow is given as $e^{tA}$. [...] Draw a picture of the vector field. Identify the matrix as a complex number. Deduce that $e^{it}=\cos t+i\sin t$ is little more than the observation that exponentiation is about solving ODE's by Euler's method taken to the limit. I would like very much to understand this very well. I've actually done most of it and perhaps the problem is that I haven't actually taken the ODE's class yet, but I've read ahead enough to understand most of what's being said. I drew the vector field (by hand) and got some lovely circley looking things. When he says, ""identify the matrix as a complex number,"" I understand that he is referring to the fact that in the book he defines a complex number $a+bi$ to be the matrix $$\begin{bmatrix} a & -b\\ b & a \end{bmatrix}$$and so $A$ is $i$. I also managed to do the exponentiation $e^{tA}$ and got $$\begin{bmatrix} \cos t & -\sin t\\ \sin t & \cos t \end{bmatrix}$$ Which is, of course, the complex number $\cos t + i\sin t$. So so far so good, I've shown that $e^{it}=\cos t + i\sin t$. I'm just having problems understanding the last little bit, and maybe that's cause I haven't taken the ODE's class yet, but I see that there are tangent lines to radiuses of circles somewhere in there since the vector field makes tangent lines to circles around the origin and $e^{At}$ ends up being the rotation matrix with angle $t$, so we have some notion of a radius rotating around the origin somewhere? How does this relate to Euler's method for solving ODE's? Is the idea that there is an ODE which produces that vector field as a direction field, and $e^{it}$ gives solutions? I get a little bit lost at this point, maybe someone can help me finish putting the pieces together.","I was brushing up on my complex arithmetic in preparation for a class in ODE's this semester and I found myself looking at Exercise 2.7.5 in Introduction to Complex Analysis for Engineers by Michael Alder, which reads The exponential function is a procedure for turning vector fields into flows; if you take the vector field which is given by$$V\begin{bmatrix} x \\ y \end{bmatrix}=\begin{bmatrix} 0 & -1\\ 1 & 0 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$$   you call the matrix $A$ and then the flow is given as $e^{tA}$. [...] Draw a picture of the vector field. Identify the matrix as a complex number. Deduce that $e^{it}=\cos t+i\sin t$ is little more than the observation that exponentiation is about solving ODE's by Euler's method taken to the limit. I would like very much to understand this very well. I've actually done most of it and perhaps the problem is that I haven't actually taken the ODE's class yet, but I've read ahead enough to understand most of what's being said. I drew the vector field (by hand) and got some lovely circley looking things. When he says, ""identify the matrix as a complex number,"" I understand that he is referring to the fact that in the book he defines a complex number $a+bi$ to be the matrix $$\begin{bmatrix} a & -b\\ b & a \end{bmatrix}$$and so $A$ is $i$. I also managed to do the exponentiation $e^{tA}$ and got $$\begin{bmatrix} \cos t & -\sin t\\ \sin t & \cos t \end{bmatrix}$$ Which is, of course, the complex number $\cos t + i\sin t$. So so far so good, I've shown that $e^{it}=\cos t + i\sin t$. I'm just having problems understanding the last little bit, and maybe that's cause I haven't taken the ODE's class yet, but I see that there are tangent lines to radiuses of circles somewhere in there since the vector field makes tangent lines to circles around the origin and $e^{At}$ ends up being the rotation matrix with angle $t$, so we have some notion of a radius rotating around the origin somewhere? How does this relate to Euler's method for solving ODE's? Is the idea that there is an ODE which produces that vector field as a direction field, and $e^{it}$ gives solutions? I get a little bit lost at this point, maybe someone can help me finish putting the pieces together.",,"['matrices', 'ordinary-differential-equations', 'complex-numbers']"
14,"Proof of 2 Matrix identities (Traces, Logs, Determinants)","Proof of 2 Matrix identities (Traces, Logs, Determinants)",,"I am working through a derivation in someone's thesis at the moment to understand an important result, but I am more than a bit rusty on matrices. Could anyone give me some tips on these identities? They are stated without proof and I'm having a hard time finding a derivation online. Below, X is a matrix and E is a scalar, and X is a function of E. 1) $Tr(X' X^{-1}) = \frac{d}{dE} Tr(ln(X))$ When I first saw this I thought it would be the same as treating X as a scalar, then by the definition of the ln function the above would be true. Is the fact that there is a trace and that X is a matrix important in the derivation? He does mention that """" $Tr(log X)' = Tr[X' X] = Tr[X X']$ """" but I think this was probably a typo, since an expression of the form $Tr[X' X]$ does not appear in his calculation. 2) $Tr(ln(X)) = ln(det(X))$ This one I am a bit stuck on, I would guess that it has something to do with the definitions of the trace and determinant but not sure where to go from there. I haven't done anything with matrices in about 3 years, and I'm a physicist, so keep it basic :) EDIT OK here is my working for proof 1 using Robert's guide below: $$ X' = \frac{d}{dE} \sum_{n=0}^{\infty} \frac{L^n}{n!} $$ Using the chain rule, $$ X' = \sum_{n=0}^{\infty} \frac{n L' L^{n-1}}{n!} = \sum_{n=0}^{\infty} \frac{L' L^{n-1}}{(n-1)!} = \sum_{n=1}^{\infty} \frac{L' L^{n-1}}{n!}$$ Here is the bit I don't quite follow, regarding the introduction of the dummy j which seems to cancel later on in the calculation without being used. $$ \sum_{j=0}^{n-1} \sum_{n=1}^{\infty} \frac{L' L^j L^{n-1 - j}}{n!} $$ Now using this expression for X': $$ X' X^{-1} = X' e^{-L} = \sum_{j=0}^{n-1} \sum_{n=1}^{\infty} \frac{L' L^j L^{n-1 - j}}{n!} e^{-L} $$ Here is where I find a problem now, since  $$ \sum_{j=0}^{n-1} (\sum_{n=1}^{\infty} \frac{L^{n-1}}{n!}) L' e^{-L} $$ Now my part in the brackets in that last expression isn't $e^L$ so doesn't cancel nicely. I am pretty sure I am missing something with commutativity and when you introduced the sum over j!? EDIT 2 Just realised my last step on the chain rule, changing the sum from n=0 to n=1 doesn't make much sense.","I am working through a derivation in someone's thesis at the moment to understand an important result, but I am more than a bit rusty on matrices. Could anyone give me some tips on these identities? They are stated without proof and I'm having a hard time finding a derivation online. Below, X is a matrix and E is a scalar, and X is a function of E. 1) $Tr(X' X^{-1}) = \frac{d}{dE} Tr(ln(X))$ When I first saw this I thought it would be the same as treating X as a scalar, then by the definition of the ln function the above would be true. Is the fact that there is a trace and that X is a matrix important in the derivation? He does mention that """" $Tr(log X)' = Tr[X' X] = Tr[X X']$ """" but I think this was probably a typo, since an expression of the form $Tr[X' X]$ does not appear in his calculation. 2) $Tr(ln(X)) = ln(det(X))$ This one I am a bit stuck on, I would guess that it has something to do with the definitions of the trace and determinant but not sure where to go from there. I haven't done anything with matrices in about 3 years, and I'm a physicist, so keep it basic :) EDIT OK here is my working for proof 1 using Robert's guide below: $$ X' = \frac{d}{dE} \sum_{n=0}^{\infty} \frac{L^n}{n!} $$ Using the chain rule, $$ X' = \sum_{n=0}^{\infty} \frac{n L' L^{n-1}}{n!} = \sum_{n=0}^{\infty} \frac{L' L^{n-1}}{(n-1)!} = \sum_{n=1}^{\infty} \frac{L' L^{n-1}}{n!}$$ Here is the bit I don't quite follow, regarding the introduction of the dummy j which seems to cancel later on in the calculation without being used. $$ \sum_{j=0}^{n-1} \sum_{n=1}^{\infty} \frac{L' L^j L^{n-1 - j}}{n!} $$ Now using this expression for X': $$ X' X^{-1} = X' e^{-L} = \sum_{j=0}^{n-1} \sum_{n=1}^{\infty} \frac{L' L^j L^{n-1 - j}}{n!} e^{-L} $$ Here is where I find a problem now, since  $$ \sum_{j=0}^{n-1} (\sum_{n=1}^{\infty} \frac{L^{n-1}}{n!}) L' e^{-L} $$ Now my part in the brackets in that last expression isn't $e^L$ so doesn't cancel nicely. I am pretty sure I am missing something with commutativity and when you introduced the sum over j!? EDIT 2 Just realised my last step on the chain rule, changing the sum from n=0 to n=1 doesn't make much sense.",,['matrices']
15,How do I determine if a matrix is contained in another matrix?,How do I determine if a matrix is contained in another matrix?,,"Is there a clever way of determining if one matrix is contained within another larger matrix? Iterating over the larger matrix to check each item until potential matches show up is straightforward but gets slow for large matrices. Example, a smaller matrix: $$\begin{pmatrix}4&3&2\\2&3&4\end{pmatrix}$$ which is ""within"" (I'm probably not using the right terminology) this larger matrix: $$\begin{pmatrix}1&2&3&4&5\\5&\color{red}4&\color{red}3&\color{red}2&1\\1&\color{red}2&\color{red}3&\color{red}4&5\end{pmatrix}$$ It feels like a problem that could have a smart mathematics trick for determining if this is the case - is there one?","Is there a clever way of determining if one matrix is contained within another larger matrix? Iterating over the larger matrix to check each item until potential matches show up is straightforward but gets slow for large matrices. Example, a smaller matrix: which is ""within"" (I'm probably not using the right terminology) this larger matrix: It feels like a problem that could have a smart mathematics trick for determining if this is the case - is there one?",\begin{pmatrix}4&3&2\\2&3&4\end{pmatrix} \begin{pmatrix}1&2&3&4&5\\5&\color{red}4&\color{red}3&\color{red}2&1\\1&\color{red}2&\color{red}3&\color{red}4&5\end{pmatrix},"['matrices', 'algorithms', 'decision-problems']"
16,Scaling at an arbitrary point and figuring out the distance from origin,Scaling at an arbitrary point and figuring out the distance from origin,,"Suppose I have a $8 \times 6$ rectangle, with its lower left corner at the origin $\left(0, 0\right)$. I want to scale this rectangle by $\frac{1}{2}$ at an anchor point $\left(3, 3\right)$. So the resulting rectangle is $4 \times 3$, but I cannot figure out how to compute the distance from the origin to the lower left corner of the new rectangle. Help is appreciated.","Suppose I have a $8 \times 6$ rectangle, with its lower left corner at the origin $\left(0, 0\right)$. I want to scale this rectangle by $\frac{1}{2}$ at an anchor point $\left(3, 3\right)$. So the resulting rectangle is $4 \times 3$, but I cannot figure out how to compute the distance from the origin to the lower left corner of the new rectangle. Help is appreciated.",,"['geometry', 'matrices', 'analytic-geometry', 'transformation']"
17,What is the Hessian of $x \mapsto\log \det \left( A^T A + R^T \operatorname{diag}(x)^{-1} R \right)$?,What is the Hessian of ?,x \mapsto\log \det \left( A^T A + R^T \operatorname{diag}(x)^{-1} R \right),"This is a follow-up to a previous question I asked regarding the hessian of a similar log determinant. The log determinant I am considering is given by $$ L(\vec{x}) = \log \det \left( A^T A + R^T D_x^{-1} R \right), $$ where $x \in \mathbb{R}^q$ with all positive entries, $D_x = \operatorname{diag}(x)$ is a diagonal matrix, and $A \in \mathbb{R}^{p \times n}$ , $B \in \mathbb{R}^{q \times n}$ . What is the Hessian of $L(x)$ with respect to $x = (x_1, \ldots, x_q)^T$ ? I have determined that the gradient can be written as $$ \nabla_x L(x) = - \operatorname{diag}\left( D_x^{-1} R \left( A^T A + R^T D_x^{-1} R \right)^{-1} R^T D_x^{-1}  \right). $$ Using an identity for the $\operatorname{diag}$ operator, this can also be written as $$ \nabla_x L(x) = - \left( \left( D_x^{-1} R  \right) \odot \left( D_x^{-1} R  \right) \right) \operatorname{diag}\left( \left( A^T A + R^T D_x^{-1} R \right)^{-1}  \right). $$ Any advice on how to proceed from here? I am thinking the next step to find the gradient is to apply a product rule to this expression.","This is a follow-up to a previous question I asked regarding the hessian of a similar log determinant. The log determinant I am considering is given by where with all positive entries, is a diagonal matrix, and , . What is the Hessian of with respect to ? I have determined that the gradient can be written as Using an identity for the operator, this can also be written as Any advice on how to proceed from here? I am thinking the next step to find the gradient is to apply a product rule to this expression.","
L(\vec{x}) = \log \det \left( A^T A + R^T D_x^{-1} R \right),
 x \in \mathbb{R}^q D_x = \operatorname{diag}(x) A \in \mathbb{R}^{p \times n} B \in \mathbb{R}^{q \times n} L(x) x = (x_1, \ldots, x_q)^T 
\nabla_x L(x) = - \operatorname{diag}\left( D_x^{-1} R \left( A^T A + R^T D_x^{-1} R \right)^{-1} R^T D_x^{-1}  \right).
 \operatorname{diag} 
\nabla_x L(x) = - \left( \left( D_x^{-1} R  \right) \odot \left( D_x^{-1} R  \right) \right) \operatorname{diag}\left( \left( A^T A + R^T D_x^{-1} R \right)^{-1}  \right).
","['matrices', 'multivariable-calculus', 'matrix-calculus', 'hessian-matrix', 'scalar-fields']"
18,Minimize trace of quadratic inverse + LASSO,Minimize trace of quadratic inverse + LASSO,,"Given a symmetric positive definite matrix $S \in \mathbb R^{d\times d}$ and $\lambda > 0$ , I would like to find $$X^\star := \underset{{X\in\mathbb R^{d\times d}}}{\operatorname{argmin}} \operatorname{tr}\left(X^{-T}SX^{-1}\right) + \lambda \|X\|_1.$$ where $$\|X\|_1 := \sum_{i=1}^d\sum_{j=1}^d\left\vert X_{ij}\right\vert$$ Has anyone seen this kind of objective function? In particular, it has proven to be quite tricky as it seems to be locally convex.","Given a symmetric positive definite matrix and , I would like to find where Has anyone seen this kind of objective function? In particular, it has proven to be quite tricky as it seems to be locally convex.",S \in \mathbb R^{d\times d} \lambda > 0 X^\star := \underset{{X\in\mathbb R^{d\times d}}}{\operatorname{argmin}} \operatorname{tr}\left(X^{-T}SX^{-1}\right) + \lambda \|X\|_1. \|X\|_1 := \sum_{i=1}^d\sum_{j=1}^d\left\vert X_{ij}\right\vert,"['matrices', 'optimization', 'regularization']"
19,What is the determinant of the Pascal matrix $M_{ij} = \binom{2j+1}{i}$?,What is the determinant of the Pascal matrix ?,M_{ij} = \binom{2j+1}{i},"For $0\le i,j\le N-1$ , define $M_{ij} = \binom{2j+1}{i}$ . For example, with $N=6$ , we have $$M_6=\left(\begin{array}{cccccc}1&1&1&1&1&1\\1&3&5&7&9&11\\0&3&10&21&36&55\\0&1&10&35&84&165\\0&0&5&35&126&330\\0&0&1&21&126&462\\\end{array}\right)$$ For the first few $N$ s, I have $\det(M_2)=2$ , $\det(M_3)=8$ , $\det(M_4)=64$ , $\det(M_5)=1024$ , $\det(M_6)=32768$ , so it is easy to conjecture $$\det(M_N) = 2^{\binom{n}{2}}$$ This would suggest an induction approach, perhaps similar to the one proposed by Marc in this question . If the same approach worked in this case, we could apply some elimination matrices to reduce $M_N$ to something of the form $$M_N=\left(\begin{array}{cc} 1 & 0 \\ 0 & 2M_{N-1}\end{array}\right)$$ at which point induction would finish the proof. However, the elimination matrices required to obtain this reduction are not as easy to find given that the elements of the matrix satisfy the recursion $$M_{ij}=M_{i,j-1}+2M_{i-1,j-1}+M_{i-2,j-1}$$ where the last term is the one which causes most of the problems. Another idea would be to try to row reduce $M_N$ to a lower triangular matrix. For instance, with $N=6$ , we would have: $$M_6 \to \left(\begin{array}{cccccc}1&1&1&1&1&1\\0&2&4&6&8&10\\0&0&4&12&24&40\\0&0&0&8&32&80\\0&0&0&0&16&80\\0&0&0&0&0&32\\\end{array}\right)$$ from which the result is readily apparent. Dividing the $n$ th row by $2^n$ reveals a Pascal matrix, so I was hoping one could transform the Pascal matrix to only include its odd-numbered columns. However, I have not been able to come up with the required manipulations. Is there something trivial I am missing?","For , define . For example, with , we have For the first few s, I have , , , , , so it is easy to conjecture This would suggest an induction approach, perhaps similar to the one proposed by Marc in this question . If the same approach worked in this case, we could apply some elimination matrices to reduce to something of the form at which point induction would finish the proof. However, the elimination matrices required to obtain this reduction are not as easy to find given that the elements of the matrix satisfy the recursion where the last term is the one which causes most of the problems. Another idea would be to try to row reduce to a lower triangular matrix. For instance, with , we would have: from which the result is readily apparent. Dividing the th row by reveals a Pascal matrix, so I was hoping one could transform the Pascal matrix to only include its odd-numbered columns. However, I have not been able to come up with the required manipulations. Is there something trivial I am missing?","0\le i,j\le N-1 M_{ij} = \binom{2j+1}{i} N=6 M_6=\left(\begin{array}{cccccc}1&1&1&1&1&1\\1&3&5&7&9&11\\0&3&10&21&36&55\\0&1&10&35&84&165\\0&0&5&35&126&330\\0&0&1&21&126&462\\\end{array}\right) N \det(M_2)=2 \det(M_3)=8 \det(M_4)=64 \det(M_5)=1024 \det(M_6)=32768 \det(M_N) = 2^{\binom{n}{2}} M_N M_N=\left(\begin{array}{cc} 1 & 0 \\ 0 & 2M_{N-1}\end{array}\right) M_{ij}=M_{i,j-1}+2M_{i-1,j-1}+M_{i-2,j-1} M_N N=6 M_6 \to \left(\begin{array}{cccccc}1&1&1&1&1&1\\0&2&4&6&8&10\\0&0&4&12&24&40\\0&0&0&8&32&80\\0&0&0&0&16&80\\0&0&0&0&0&32\\\end{array}\right) n 2^n","['matrices', 'binomial-coefficients', 'determinant']"
20,Let $G$ be a finite matrix group in $GL_2(Q)$ such that every matrix $A\in G$ has integer entries. Prove $A^{12}= I$ for each $A$.,Let  be a finite matrix group in  such that every matrix  has integer entries. Prove  for each .,G GL_2(Q) A\in G A^{12}= I A,"Let $G$ be a finite matrix group in $GL_2(Q)$ (general linear group of $2$ by $2$ matrices with rational entries) such that every matrix $A\in G$ has integer entries. Prove that $A^{12} = I$ for every $A \in G$ . Attempt : We have that $A^k = I$ for some natural $k$ since $G$ is finite. Then the minimal polynomial of $A$ divides $x^{k} - 1$ . Also, the characteristic polynomial is of the form $x^2 + ax + b$ for some $a,b$ . Thus, the minimal polynomial has either a root of the form $x-c$ where $c$ is an integer, or it is the characteristic polynomial itself. In the first case, we get $A = I$ or $A =-I$ since $1,-1$ are the only integer roots of unity. Hence $A^2 = I$ . In the second case, we have that either the roots of the characteristic polynomial are $1,-1$ in which case we get $x^2 - 1$ , so $A^2 = I$ again. Otherwise, we have a complex root of unity and it's conjugate. This gives us $b = 1$ as it is the product of these roots, and $a$ is $2 *$ the real part. Hence we get $A^2 + aA + I = 0 $ so $A(A + aI) = -I$ . How do I use this to show $A^{12} = I$ ?","Let be a finite matrix group in (general linear group of by matrices with rational entries) such that every matrix has integer entries. Prove that for every . Attempt : We have that for some natural since is finite. Then the minimal polynomial of divides . Also, the characteristic polynomial is of the form for some . Thus, the minimal polynomial has either a root of the form where is an integer, or it is the characteristic polynomial itself. In the first case, we get or since are the only integer roots of unity. Hence . In the second case, we have that either the roots of the characteristic polynomial are in which case we get , so again. Otherwise, we have a complex root of unity and it's conjugate. This gives us as it is the product of these roots, and is the real part. Hence we get so . How do I use this to show ?","G GL_2(Q) 2 2 A\in G A^{12} = I A \in G A^k = I k G A x^{k} - 1 x^2 + ax + b a,b x-c c A = I A =-I 1,-1 A^2 = I 1,-1 x^2 - 1 A^2 = I b = 1 a 2 * A^2 + aA + I = 0  A(A + aI) = -I A^{12} = I","['matrices', 'group-theory', 'polynomials', 'finite-groups']"
21,Number of $4 \times 4$ matrices with sum along row or column $0$,Number of  matrices with sum along row or column,4 \times 4 0,Find the number of $4 \times 4$ matrices whose entries are each $2018$ or $—2018$ such that the sum of the  entries in each row and in each column is $0$. Now to get sum $0$ along a row we need two $2018$ and two $-2018$ and number of arrangement will be $\frac{4!}{2! 2!}$ but how adjust column while adjusting the rows or vice-versa? Could someone help me with this,Find the number of $4 \times 4$ matrices whose entries are each $2018$ or $—2018$ such that the sum of the  entries in each row and in each column is $0$. Now to get sum $0$ along a row we need two $2018$ and two $-2018$ and number of arrangement will be $\frac{4!}{2! 2!}$ but how adjust column while adjusting the rows or vice-versa? Could someone help me with this,,"['matrices', 'permutations']"
22,Number of coniugacy classes of a group.,Number of coniugacy classes of a group.,,"I was thinking about how to compute the number of coniugacy classes of a group $G$, in particular I'm interested in the number of coniugacy classes of $GL_n(\mathbb{F}_p)$. This is clearly related to the number of irreducible representations of this group, but searching on the internet, I found out that this is actually an ""open"" problem. If the field in which we are considering our matrices is algebrically closed, we can use an argument like the Jordan form to attack this problem, but this doesn't work in $\mathbb{F}_p$. Can you suggest me some papers about this topic? I found that there are only upper and lower bounds for this number, but not an asymptotic behaviour or something like this. Given a general group $GL_n(\mathbb{F}_p)$ how many coniugacy classes   are there? Do you think this is a question for StackExchange or should I ask this on Overflow? Any hint/paper/actual research on this topic? Thanks in advance.","I was thinking about how to compute the number of coniugacy classes of a group $G$, in particular I'm interested in the number of coniugacy classes of $GL_n(\mathbb{F}_p)$. This is clearly related to the number of irreducible representations of this group, but searching on the internet, I found out that this is actually an ""open"" problem. If the field in which we are considering our matrices is algebrically closed, we can use an argument like the Jordan form to attack this problem, but this doesn't work in $\mathbb{F}_p$. Can you suggest me some papers about this topic? I found that there are only upper and lower bounds for this number, but not an asymptotic behaviour or something like this. Given a general group $GL_n(\mathbb{F}_p)$ how many coniugacy classes   are there? Do you think this is a question for StackExchange or should I ask this on Overflow? Any hint/paper/actual research on this topic? Thanks in advance.",,"['matrices', 'group-theory', 'finite-groups', 'representation-theory', 'finite-fields']"
23,Vandermond matrix generalisation for non-integer degrees,Vandermond matrix generalisation for non-integer degrees,,"For a $n \times n$ Vandermonde matrix $$V:=\begin{bmatrix}1 & c_1 & c_1^2 & \cdots & c_1^{n-1} \\ 1 & c_2 & c_2^2 & \cdots & c_2^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots\\ 1 & c_n & c_n^2 & \cdots & c_n^{n-1}\end{bmatrix}$$ we know that it is nonsingular if and only if $c_i \ne c_j$ for $i\ne j$. I am curious if this property can be generalized for non-integer degrees. Suppose I am given a $n \times n$ matrix $$W:=\begin{bmatrix}c_1^{d_1} & c_1^{d_2} & c_1^{d_3} & \cdots & c_1^{d_n} \\ c_2^{d_1} & c_2^{d_2} & c_2^{d_3} & \cdots & c_2^{d_n} \\ \vdots & \vdots & \vdots & \ddots & \vdots\\ c_n^{d_1} & c_n^{d_2} & c_n^{d_3} & \cdots & c_n^{d_n}\end{bmatrix}.$$ Is it true that $W$ is nonsingular if and only if $c_i \ne c_j$ for $i\ne j$? If it matters, then I consider complex values for $c_i$ and real values for $d_i$. If (as I guess) it is something well-known, could you, please, give me a reference? Update: Obviously, I assume that $c_i \ne 0$ for all $i$. Let us also assume that $d_i \ne d_j$ for $i \ne j$. Update 2: I have tried the following condition: for all $d_k\ne 0$ we have $c_i^{d_k} \ne c_j^{d_k}$ for $i \ne j$. It does not work. Actually, for $n=2$ the condition is $c_1^{d_2-d_1} \ne c_2^{d_2-d_1}$. This is satisfied, particularly, for $|c_1| \ne |c_2|$. Update 3: I have the following intuition. Let $\Delta_{ij}:=d_i-d_j$. The hypothesis: if for all $i\ne j$ we have $c_k^{\Delta_{ij}} \ne c_l^{\Delta_{ij}}$ for $k\ne l$, then $\det{W} \ne 0$. For integer $d$ we have exactly the Vandermond condition.","For a $n \times n$ Vandermonde matrix $$V:=\begin{bmatrix}1 & c_1 & c_1^2 & \cdots & c_1^{n-1} \\ 1 & c_2 & c_2^2 & \cdots & c_2^{n-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots\\ 1 & c_n & c_n^2 & \cdots & c_n^{n-1}\end{bmatrix}$$ we know that it is nonsingular if and only if $c_i \ne c_j$ for $i\ne j$. I am curious if this property can be generalized for non-integer degrees. Suppose I am given a $n \times n$ matrix $$W:=\begin{bmatrix}c_1^{d_1} & c_1^{d_2} & c_1^{d_3} & \cdots & c_1^{d_n} \\ c_2^{d_1} & c_2^{d_2} & c_2^{d_3} & \cdots & c_2^{d_n} \\ \vdots & \vdots & \vdots & \ddots & \vdots\\ c_n^{d_1} & c_n^{d_2} & c_n^{d_3} & \cdots & c_n^{d_n}\end{bmatrix}.$$ Is it true that $W$ is nonsingular if and only if $c_i \ne c_j$ for $i\ne j$? If it matters, then I consider complex values for $c_i$ and real values for $d_i$. If (as I guess) it is something well-known, could you, please, give me a reference? Update: Obviously, I assume that $c_i \ne 0$ for all $i$. Let us also assume that $d_i \ne d_j$ for $i \ne j$. Update 2: I have tried the following condition: for all $d_k\ne 0$ we have $c_i^{d_k} \ne c_j^{d_k}$ for $i \ne j$. It does not work. Actually, for $n=2$ the condition is $c_1^{d_2-d_1} \ne c_2^{d_2-d_1}$. This is satisfied, particularly, for $|c_1| \ne |c_2|$. Update 3: I have the following intuition. Let $\Delta_{ij}:=d_i-d_j$. The hypothesis: if for all $i\ne j$ we have $c_k^{\Delta_{ij}} \ne c_l^{\Delta_{ij}}$ for $k\ne l$, then $\det{W} \ne 0$. For integer $d$ we have exactly the Vandermond condition.",,['matrices']
24,Kronecker product of positive definite matrices,Kronecker product of positive definite matrices,,"I am looking for a reference where it is proved that given two positive definite matrices $A\in M_n$, $B \in M_m$, their Kronecker product $A\otimes B$ is positive definite. More precisely, I am looking for a computation showing that  $$\langle (A\otimes B)v,v\rangle \ge 0$$ for every $v\in \mathbb{C}^{mn}$. I specifically don't want to use the argument about the eigenvalues or the mixed-product and square roots, but a very direct computation of the inner product above. I would appreciate the help.","I am looking for a reference where it is proved that given two positive definite matrices $A\in M_n$, $B \in M_m$, their Kronecker product $A\otimes B$ is positive definite. More precisely, I am looking for a computation showing that  $$\langle (A\otimes B)v,v\rangle \ge 0$$ for every $v\in \mathbb{C}^{mn}$. I specifically don't want to use the argument about the eigenvalues or the mixed-product and square roots, but a very direct computation of the inner product above. I would appreciate the help.",,"['matrices', 'reference-request']"
25,"Is it possible to exist a ""three-dimensional matrix""?","Is it possible to exist a ""three-dimensional matrix""?",,"We all have seen matrices that are ""bi-dimensional"" (i'm using the quotes here because i'm not talking about the number of lines, but about the way you represent a matrix, as a rectangle). I was wondering if we can define a space where we have similar objects, but instead of being a rectangle it would be a box, cubic or non-cubic.  Imagine for a example a space $M$ of ""3D"" Matrices, and a 3D matrix $A$ would have numbers represented by $a_{ijk}$, where $i$ is equivalent to a column, $j$ to a line, and $k$ to another line, but this time in a 3-dimensional way. We can imagine a simple $2\times2\times2$ ""3D-Matrix"" we would have eight entries: $a_{111}, a_{112}, a_{121}, a_{122}, a_{211}, a_{212}, a_{222}$ instead of 4 in a normal matrix. Does this concept exist? I have tried looking in this forum for ""3D matrices"" but didn't found anything. Also looked on google but couldn't find anything. Thanks.","We all have seen matrices that are ""bi-dimensional"" (i'm using the quotes here because i'm not talking about the number of lines, but about the way you represent a matrix, as a rectangle). I was wondering if we can define a space where we have similar objects, but instead of being a rectangle it would be a box, cubic or non-cubic.  Imagine for a example a space $M$ of ""3D"" Matrices, and a 3D matrix $A$ would have numbers represented by $a_{ijk}$, where $i$ is equivalent to a column, $j$ to a line, and $k$ to another line, but this time in a 3-dimensional way. We can imagine a simple $2\times2\times2$ ""3D-Matrix"" we would have eight entries: $a_{111}, a_{112}, a_{121}, a_{122}, a_{211}, a_{212}, a_{222}$ instead of 4 in a normal matrix. Does this concept exist? I have tried looking in this forum for ""3D matrices"" but didn't found anything. Also looked on google but couldn't find anything. Thanks.",,['matrices']
26,Probability of having complex eigenvalue?,Probability of having complex eigenvalue?,,"Let $A$ be a real $n \times n$ matrix with coefficients randomly chosen from Uniform Distribution [-1,1]. What's the probability that A has a complex eigenvalue with non-zero imaginary part? If you'd like to know the motivation, I've been doing QR factorization and I notice that the algorithm seems to fail at times, producing blocks along the main diagonal but zero elsewhere. While I'm sure the fault lies with my code, it got me interested in the question. Props to you if you can generalize from Uniform distribution to any probability distribution.","Let $A$ be a real $n \times n$ matrix with coefficients randomly chosen from Uniform Distribution [-1,1]. What's the probability that A has a complex eigenvalue with non-zero imaginary part? If you'd like to know the motivation, I've been doing QR factorization and I notice that the algorithm seems to fail at times, producing blocks along the main diagonal but zero elsewhere. While I'm sure the fault lies with my code, it got me interested in the question. Props to you if you can generalize from Uniform distribution to any probability distribution.",,"['matrices', 'probability-distributions', 'random-matrices']"
27,Finding minimum from matrix,Finding minimum from matrix,,"Consider following $3\times 3$ matrix. $\begin{pmatrix}3&6&9\\     2& 4 &8\\     1 &5& 7     \end{pmatrix}$ I need to find combination of three numbers where each number comes from unique column & row.  For example $3,6,8$ is not what I want as $3$ and $6$ are from same row. $6,4,7$ is also not right because $6$ and $4$ are from same columns. $3,4,7$ is valid as each number comes from different column and row. Next thing I want, is to find combination whose sum is minimum compared to sum of all other possible combinations. For example $3,4,7$ will be final answer if $3+4+7 = \min$. How do I find this for $n \times n$ matrix?","Consider following $3\times 3$ matrix. $\begin{pmatrix}3&6&9\\     2& 4 &8\\     1 &5& 7     \end{pmatrix}$ I need to find combination of three numbers where each number comes from unique column & row.  For example $3,6,8$ is not what I want as $3$ and $6$ are from same row. $6,4,7$ is also not right because $6$ and $4$ are from same columns. $3,4,7$ is valid as each number comes from different column and row. Next thing I want, is to find combination whose sum is minimum compared to sum of all other possible combinations. For example $3,4,7$ will be final answer if $3+4+7 = \min$. How do I find this for $n \times n$ matrix?",,"['matrices', 'summation', 'matrix-equations', 'matrix-decomposition', 'discrete-optimization']"
28,When should matrices have units of measurement?,When should matrices have units of measurement?,,"As a mathematician I think of matrices as $\mathbb{F}^{m\times n}$, where $\mathbb{F}$ is a field and usually $\mathbb{F} = \mathbb{R}$ or $\mathbb{F} = \mathbb{C}$. Units are not necessary. However, some engineers have told me that they prefer matrices to have units such as metres, kg, pounds, dollars, etc. Assigning a unit of measurement to each entry to me seems restrictive (for instance if working with dollars then is $A^2$ allowed?). Here are a few things that I would like to understand more deeply: Are there examples where it is more appropriate to work with matrices that have units? If units can only restrict the algebra, why should one assign units at all? Is there anything exciting here, or is it just engineers trying to put physical interpretations on to matrix algebra? Also, see: https://stackoverflow.com/questions/11975658/how-do-units-flow-through-matrix-operations","As a mathematician I think of matrices as $\mathbb{F}^{m\times n}$, where $\mathbb{F}$ is a field and usually $\mathbb{F} = \mathbb{R}$ or $\mathbb{F} = \mathbb{C}$. Units are not necessary. However, some engineers have told me that they prefer matrices to have units such as metres, kg, pounds, dollars, etc. Assigning a unit of measurement to each entry to me seems restrictive (for instance if working with dollars then is $A^2$ allowed?). Here are a few things that I would like to understand more deeply: Are there examples where it is more appropriate to work with matrices that have units? If units can only restrict the algebra, why should one assign units at all? Is there anything exciting here, or is it just engineers trying to put physical interpretations on to matrix algebra? Also, see: https://stackoverflow.com/questions/11975658/how-do-units-flow-through-matrix-operations",,"['matrices', 'unit-of-measure']"
29,About integral binary quadratic forms fixed by $\operatorname{GL_2(\mathbb Z)}$ matrices of order $3$,About integral binary quadratic forms fixed by  matrices of order,\operatorname{GL_2(\mathbb Z)} 3,"I am reading this paper of Manjul Bhargava and Ariel Shnidman, and I want to prove this claim, which appear at the first paragraph of Theorem $14$: Up to $\operatorname{SL_2}(\mathbb Z)$ equivalence and scaling, there is only one integral binary quadratic form having an $\operatorname{SL_2}(\mathbb Z)$-automorphism of order three, namely $Q(x,y)=x^2+xy+y^2$. Now the pertinent definitions in order to understand the problem. Consider the following action of $\operatorname{GL_2}(\mathbb Z)$ on the set of integral binary quadratic forms: for $\gamma\in$$\operatorname{GL_2}(\mathbb Z)$ and $f(x,y)=Px^2+Qxy+Ry^2$, with $P,Q,R\in\mathbb Z$, we define $(\gamma f)(x,y)=f\bigl((x,y)\gamma\bigr)$. Identifying each such $f$ with the triplet $(P,Q,R)^t$, we have that the triplet associated to $\gamma f$ is given by the matricial product $$M_{\gamma}\,\cdot\begin{bmatrix} P\\ Q\\ R\end{bmatrix},\ \text{where}\ \ \gamma=\begin{bmatrix} A & B\\ C & D \end{bmatrix}\,\ \text{and}\ M_{\gamma}=\begin{bmatrix} A^2 & AB    & B^2\\ 2AC & AD+BC & 2BD\\ C^2 & CD    & D^2 \end{bmatrix}.$$ With these notations the authors' claim can be restated as follows: Suppose that $f=(P,Q,R)^t$ is fixed by $\gamma\in\operatorname{GL_2}(\mathbb Z)$ of order $3$ and $Q\ne0$. Show that there exist $\theta\in\operatorname{GL_2}(\mathbb Z)$ and $n\in\mathbb Z$ such that $\theta f=(n,n,n)^t.$ I have had a very bad time trying to prove this fact. Perhaps my understanding of the problem is wrong, but I honestly don't think so (I successfully completed the previous details of the paper). My work so far: Let $a=A+D$ be the trace of $\gamma$. Since $\gamma^3=I$ then $\det\gamma=1$, so the characteristic polynomial of $\gamma$ is $X^2-aX+1$. Now $$X^3-1=(X+a)(X^2-aX+1)+(a+1)\bigl[(a-1)X-1\bigr]\,,$$ and since $\gamma$ also satisfies $\gamma^3-I=0$, then after evaluating the equality above at $\gamma$ we obtain that either $a=-1$ or $(a-1)\gamma=I$; but $(a-1)\gamma=I$ implies $(a-1)^3=1$, that is $a=2$, and so $I=(a-1)\gamma=\gamma$, contradicting the fact that $\gamma$ has order $3$ in $\operatorname{GL_2}(\mathbb Z)$. On the other hand $\gamma^{-1}$ also fixes $f$, so denoting $(P,Q,R)^t$ by $v$ we obtain $(M_{\gamma^{-1}}-M_\gamma)v=0$. Using the formula $\gamma^{-1}=\binom{\ \ D\ \ -B}{\!\!\!-C\ \ \ \ A}$ together with the equalities $AD-BC=1$ and $A+D=-1$ we get $$\begin{align*} M_{\gamma^{-1}}-M_{\gamma}=&\,\begin{bmatrix} D^2  & -BD   &  B^2\\ -2CD & AD+BC & -2AB\\ C^2  & -AC   &  A^2 \end{bmatrix}-\begin{bmatrix} A^2 & AB    & B^2\\ 2AC & AD+BC & 2BD\\ C^2 & CD    & D^2 \end{bmatrix} \\[5mm] =&\,\begin{bmatrix} A-D & B & 0 \\ 2C  & 0 & 2B\\  0  & C & D-A \end{bmatrix}\,, \end{align*}$$ and so $v$ has the specific form $v=\frac Q{A-D}(-B,A-D,C)^t$. Now I am stuck at this point: If $\theta=\binom{B\ \ \ 0}{D\ \ \ 1}$ then $M_\theta\cdot(n,n,n)^t=v$, where $n=\frac{-Q}{B(A-D)}$ (it is easy to see from the equalities $AD-BC=1$ and $A+D=-1$ that $B(A-D)\ne0$). The problem is, of course, that $n$ is not necessarily an integer and that $\theta$ not necessarily belong to $\operatorname{GL_2}(\mathbb Z)$. Changing $B$ by $1$ in $\theta$ does not work, because scaling the first row of $\theta$ amounts to a corresponding quadratic scaling at the first row of $M_\theta$, but keeping the same scaling at the second row of $M_\theta$, so the desired equality is not preserved. I tried every possible matrix obtained from $\gamma$ with no success, so any help is welcomed!!!","I am reading this paper of Manjul Bhargava and Ariel Shnidman, and I want to prove this claim, which appear at the first paragraph of Theorem $14$: Up to $\operatorname{SL_2}(\mathbb Z)$ equivalence and scaling, there is only one integral binary quadratic form having an $\operatorname{SL_2}(\mathbb Z)$-automorphism of order three, namely $Q(x,y)=x^2+xy+y^2$. Now the pertinent definitions in order to understand the problem. Consider the following action of $\operatorname{GL_2}(\mathbb Z)$ on the set of integral binary quadratic forms: for $\gamma\in$$\operatorname{GL_2}(\mathbb Z)$ and $f(x,y)=Px^2+Qxy+Ry^2$, with $P,Q,R\in\mathbb Z$, we define $(\gamma f)(x,y)=f\bigl((x,y)\gamma\bigr)$. Identifying each such $f$ with the triplet $(P,Q,R)^t$, we have that the triplet associated to $\gamma f$ is given by the matricial product $$M_{\gamma}\,\cdot\begin{bmatrix} P\\ Q\\ R\end{bmatrix},\ \text{where}\ \ \gamma=\begin{bmatrix} A & B\\ C & D \end{bmatrix}\,\ \text{and}\ M_{\gamma}=\begin{bmatrix} A^2 & AB    & B^2\\ 2AC & AD+BC & 2BD\\ C^2 & CD    & D^2 \end{bmatrix}.$$ With these notations the authors' claim can be restated as follows: Suppose that $f=(P,Q,R)^t$ is fixed by $\gamma\in\operatorname{GL_2}(\mathbb Z)$ of order $3$ and $Q\ne0$. Show that there exist $\theta\in\operatorname{GL_2}(\mathbb Z)$ and $n\in\mathbb Z$ such that $\theta f=(n,n,n)^t.$ I have had a very bad time trying to prove this fact. Perhaps my understanding of the problem is wrong, but I honestly don't think so (I successfully completed the previous details of the paper). My work so far: Let $a=A+D$ be the trace of $\gamma$. Since $\gamma^3=I$ then $\det\gamma=1$, so the characteristic polynomial of $\gamma$ is $X^2-aX+1$. Now $$X^3-1=(X+a)(X^2-aX+1)+(a+1)\bigl[(a-1)X-1\bigr]\,,$$ and since $\gamma$ also satisfies $\gamma^3-I=0$, then after evaluating the equality above at $\gamma$ we obtain that either $a=-1$ or $(a-1)\gamma=I$; but $(a-1)\gamma=I$ implies $(a-1)^3=1$, that is $a=2$, and so $I=(a-1)\gamma=\gamma$, contradicting the fact that $\gamma$ has order $3$ in $\operatorname{GL_2}(\mathbb Z)$. On the other hand $\gamma^{-1}$ also fixes $f$, so denoting $(P,Q,R)^t$ by $v$ we obtain $(M_{\gamma^{-1}}-M_\gamma)v=0$. Using the formula $\gamma^{-1}=\binom{\ \ D\ \ -B}{\!\!\!-C\ \ \ \ A}$ together with the equalities $AD-BC=1$ and $A+D=-1$ we get $$\begin{align*} M_{\gamma^{-1}}-M_{\gamma}=&\,\begin{bmatrix} D^2  & -BD   &  B^2\\ -2CD & AD+BC & -2AB\\ C^2  & -AC   &  A^2 \end{bmatrix}-\begin{bmatrix} A^2 & AB    & B^2\\ 2AC & AD+BC & 2BD\\ C^2 & CD    & D^2 \end{bmatrix} \\[5mm] =&\,\begin{bmatrix} A-D & B & 0 \\ 2C  & 0 & 2B\\  0  & C & D-A \end{bmatrix}\,, \end{align*}$$ and so $v$ has the specific form $v=\frac Q{A-D}(-B,A-D,C)^t$. Now I am stuck at this point: If $\theta=\binom{B\ \ \ 0}{D\ \ \ 1}$ then $M_\theta\cdot(n,n,n)^t=v$, where $n=\frac{-Q}{B(A-D)}$ (it is easy to see from the equalities $AD-BC=1$ and $A+D=-1$ that $B(A-D)\ne0$). The problem is, of course, that $n$ is not necessarily an integer and that $\theta$ not necessarily belong to $\operatorname{GL_2}(\mathbb Z)$. Changing $B$ by $1$ in $\theta$ does not work, because scaling the first row of $\theta$ amounts to a corresponding quadratic scaling at the first row of $M_\theta$, but keeping the same scaling at the second row of $M_\theta$, so the desired equality is not preserved. I tried every possible matrix obtained from $\gamma$ with no success, so any help is welcomed!!!",,"['group-theory', 'matrices', 'representation-theory', 'quadratic-forms', 'group-actions']"
30,Understanding and interpreting graph spectra,Understanding and interpreting graph spectra,,"I'm not a mathematician, but a geographer trying to get a grasp on some network analysis I'm experimenting with. I have a few questions related to spectral graph theory that a mathematician could help me with: I'm generating random graphs of 50 nodes and density 0.17 that have particular average path length, modularity, etc. Each of these graphs produce spectra (distribution of eigenvalue) resembling this: From a novice point of view, I am tempted to tell that a semi-circle shape, if weak, is forming between eigenvalues 0.6 and 1.55. What would that mean? Is this related to the typical semi-circle seen in random graphs? What about the peak at eigenvalue 0.4? Could it be the second eigenvalue, which is often related to algebraic connectivity? Are the values close to zero typical of anything? To broaden the question, how much can one conclude, in terms of graph structure, from looking at the spectrum of a graph (laplacian or otherwise)? This may seem naive, but can one ever use a graph spectra (perhaps averaged over many graphs) as a structural signature?","I'm not a mathematician, but a geographer trying to get a grasp on some network analysis I'm experimenting with. I have a few questions related to spectral graph theory that a mathematician could help me with: I'm generating random graphs of 50 nodes and density 0.17 that have particular average path length, modularity, etc. Each of these graphs produce spectra (distribution of eigenvalue) resembling this: From a novice point of view, I am tempted to tell that a semi-circle shape, if weak, is forming between eigenvalues 0.6 and 1.55. What would that mean? Is this related to the typical semi-circle seen in random graphs? What about the peak at eigenvalue 0.4? Could it be the second eigenvalue, which is often related to algebraic connectivity? Are the values close to zero typical of anything? To broaden the question, how much can one conclude, in terms of graph structure, from looking at the spectrum of a graph (laplacian or otherwise)? This may seem naive, but can one ever use a graph spectra (perhaps averaged over many graphs) as a structural signature?",,"['matrices', 'graph-theory', 'eigenvalues-eigenvectors', 'spectral-graph-theory', 'graph-laplacian']"
31,Zeilberger's determinant evaluation problem,Zeilberger's determinant evaluation problem,,"Doron Zeilberger posted a question here: http://arxiv.org/abs/1401.1532 of evaluating the determinant of a very sparse matrix. The $2d \times 2d$ matrix $M(d)$ has ones in the pattern $1,0,1,0,1,0,\dots$ in both the subdiagonals and the superdiagonals. And $M_{b-1,2b}=M_{3b+1,2b-1}=-1$, for all $b$. The entries are zero everywhere else. This matrix happens to be related to the Collatz Conjecture, in that the matrix is nonsingular for all $d$ iff there are no nontrivial cycles of the Collatz function iteration. See http://www.math.rutgers.edu/~zeilberg/mamarim/mamarimhtml/detconj.html If one swaps all odd and even columns, $2b-1$ and $2b$, one gets all ones on the diagonal of this matrix. Then if one performs Gaussian elimination, one gets an upper-triangular matrix with all ones on the diagonal (for any $d$); therefore, the matrix is nonsingular. I verified this on a spreadsheet. If this doesn't convince you, here is a formal proof: (Invalid formal proof) We'll prove the conjecture that $\det M(d)=(−1)^d$, via induction on $d$. It's trivial to check for $d=1,2$. We'll assume true for $d=k$ and prove true for $d=k+1$. We'll use Laplace expansion (by cofactors) along column $2d−1$ to prove that the $\det M(d)=(−1)^d$. Notice that this column has only one nonzero entry in the last row, namely 1; therefore, $\det M(d)=(−1)^{2d+(2d−1)}M_{2d,2d−1}=−M_{2d,2d−1}$. Suppose $d$ is odd. Then to evaluate $M_{2d,2d−1}$, we use Laplace expansion along row $2d−1$. Then $M_{2d,2d−1}=(−1)^{(2d−1)+(2d−1)}\det M(d−1)=\det M(d−1)$, since the only nonzero entry is in column $2d$, which is 1. Hence, $\det M(d)=−M_{2d,2d−1}=−\det M(d−1)=(−1)^d$, by the induction hypothesis. Suppose $d$ is even. Then to evaluate $M_{2d,2d−1}$, again we use Laplace expansion along row $2d−1$. There are two nonzero entries in this row, namely the one in column $2d$, which is 1, and the one in column $d+1$, which is -1. Then we get $M_{2d,2d−1}=(−1)^{(2d−1)+(2d−1)}M(d−1)+(−1)^{(2d−1)+(d+1)}\cdot (−1)\cdot 0=\det M(d−1)$. The last term is zero, because row $d+2$ has all zeroes when column $d+1$ is deleted. Hence, $\det M(d)=−M_{2d,2d−1}=−\det M(d−1)=(−1)^d$, by the induction hypothesis. QED What am I missing here? This was way too easy.","Doron Zeilberger posted a question here: http://arxiv.org/abs/1401.1532 of evaluating the determinant of a very sparse matrix. The $2d \times 2d$ matrix $M(d)$ has ones in the pattern $1,0,1,0,1,0,\dots$ in both the subdiagonals and the superdiagonals. And $M_{b-1,2b}=M_{3b+1,2b-1}=-1$, for all $b$. The entries are zero everywhere else. This matrix happens to be related to the Collatz Conjecture, in that the matrix is nonsingular for all $d$ iff there are no nontrivial cycles of the Collatz function iteration. See http://www.math.rutgers.edu/~zeilberg/mamarim/mamarimhtml/detconj.html If one swaps all odd and even columns, $2b-1$ and $2b$, one gets all ones on the diagonal of this matrix. Then if one performs Gaussian elimination, one gets an upper-triangular matrix with all ones on the diagonal (for any $d$); therefore, the matrix is nonsingular. I verified this on a spreadsheet. If this doesn't convince you, here is a formal proof: (Invalid formal proof) We'll prove the conjecture that $\det M(d)=(−1)^d$, via induction on $d$. It's trivial to check for $d=1,2$. We'll assume true for $d=k$ and prove true for $d=k+1$. We'll use Laplace expansion (by cofactors) along column $2d−1$ to prove that the $\det M(d)=(−1)^d$. Notice that this column has only one nonzero entry in the last row, namely 1; therefore, $\det M(d)=(−1)^{2d+(2d−1)}M_{2d,2d−1}=−M_{2d,2d−1}$. Suppose $d$ is odd. Then to evaluate $M_{2d,2d−1}$, we use Laplace expansion along row $2d−1$. Then $M_{2d,2d−1}=(−1)^{(2d−1)+(2d−1)}\det M(d−1)=\det M(d−1)$, since the only nonzero entry is in column $2d$, which is 1. Hence, $\det M(d)=−M_{2d,2d−1}=−\det M(d−1)=(−1)^d$, by the induction hypothesis. Suppose $d$ is even. Then to evaluate $M_{2d,2d−1}$, again we use Laplace expansion along row $2d−1$. There are two nonzero entries in this row, namely the one in column $2d$, which is 1, and the one in column $d+1$, which is -1. Then we get $M_{2d,2d−1}=(−1)^{(2d−1)+(2d−1)}M(d−1)+(−1)^{(2d−1)+(d+1)}\cdot (−1)\cdot 0=\det M(d−1)$. The last term is zero, because row $d+2$ has all zeroes when column $d+1$ is deleted. Hence, $\det M(d)=−M_{2d,2d−1}=−\det M(d−1)=(−1)^d$, by the induction hypothesis. QED What am I missing here? This was way too easy.",,['matrices']
32,Logarithm of singular matrix,Logarithm of singular matrix,,"How do we define logarithm of a singular matrix(Say it is real square symmetric and has distinct eigen values). I tried searching online but could not find much information(Something that someone as dumb as me could understand). MATLAB logm help says something about principal and non principal logarithm and MATLAB does give log of singular matrix. Can someone explain? So, if I am to calculate it by hand using either decoposition into eigen value matrix and eigen vector matrices or jordan form, how do I do it?","How do we define logarithm of a singular matrix(Say it is real square symmetric and has distinct eigen values). I tried searching online but could not find much information(Something that someone as dumb as me could understand). MATLAB logm help says something about principal and non principal logarithm and MATLAB does give log of singular matrix. Can someone explain? So, if I am to calculate it by hand using either decoposition into eigen value matrix and eigen vector matrices or jordan form, how do I do it?",,"['matrices', 'logarithms']"
33,Is always possible to find a generalized eigenvector for the Jordan basis M?,Is always possible to find a generalized eigenvector for the Jordan basis M?,,"$A$ is a defective matrix, meaning that there are fewer linearly independent eigenvectors than eigenvalues; the algebraic multiplicity of $\lambda_1$ is $v_i = 2$ while the geometric multiplicity is $\mu_1 = 1$: $$ A = \begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix}, \lambda_1 = 1, e_1 = \begin{bmatrix}1 \\0\end{bmatrix} $$ The block diagonal matrix $J$ (Jordan form) would be: $$ J = \begin{bmatrix}\lambda_1 & 1 \\ 0 & \lambda_1\end{bmatrix} $$ Correct me if I'm wrong: The number of blocks $J(\lambda_i)$ in $J$ equals the number of distinct eigenvalues ($1$ in this example). Each block is a square matrix of order $v_i$ (the algebraic multiplicity of $\lambda_i$). In this case there is only one block $J(\lambda_1)$ of order $2$. Each block $J(\lambda_i)$ has many miniblocks as the geometric multiplicity $\mu_i$ (in this case $2$), that is $J(\lambda_i, 1)...J(\lambda_i, \mu_i)$. Each miniblock has this strcture: $$ \begin{bmatrix}\lambda_i & 1 \\ 0 & \lambda_i\end{bmatrix} $$ Right now $J = \begin{bmatrix}1 & 1\\0 & 1\end{bmatrix}$. I need to find out the basis (the matrix $M$). Assume that the columns of $M$ are $x_1$ and $x_2$. We know that (Jordan form) $AM = MJ$. Computing the product a column at time we have: $$ Ax_1 = \lambda_1x_1\\ x_1 = e_1 = \begin{bmatrix}1\\0\end{bmatrix}\\ Ax_2 = x_1 + \lambda_1x_2\\ (A - I\lambda_1)x_2 = x_1 $$ And $x_2$ is the generalized eigenvector ($\begin{bmatrix}c\\1\end{bmatrix}$). Question 1 : the choice of $c$ is up to me? I imagine that, because $M$ should be not singulare, I have to choose the right value by myself. Question 2 is: is always possible to solve $(A - I\lambda_1)x_2 = x_1$ and why? In other words, the generalized eigenvector always exists?","$A$ is a defective matrix, meaning that there are fewer linearly independent eigenvectors than eigenvalues; the algebraic multiplicity of $\lambda_1$ is $v_i = 2$ while the geometric multiplicity is $\mu_1 = 1$: $$ A = \begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix}, \lambda_1 = 1, e_1 = \begin{bmatrix}1 \\0\end{bmatrix} $$ The block diagonal matrix $J$ (Jordan form) would be: $$ J = \begin{bmatrix}\lambda_1 & 1 \\ 0 & \lambda_1\end{bmatrix} $$ Correct me if I'm wrong: The number of blocks $J(\lambda_i)$ in $J$ equals the number of distinct eigenvalues ($1$ in this example). Each block is a square matrix of order $v_i$ (the algebraic multiplicity of $\lambda_i$). In this case there is only one block $J(\lambda_1)$ of order $2$. Each block $J(\lambda_i)$ has many miniblocks as the geometric multiplicity $\mu_i$ (in this case $2$), that is $J(\lambda_i, 1)...J(\lambda_i, \mu_i)$. Each miniblock has this strcture: $$ \begin{bmatrix}\lambda_i & 1 \\ 0 & \lambda_i\end{bmatrix} $$ Right now $J = \begin{bmatrix}1 & 1\\0 & 1\end{bmatrix}$. I need to find out the basis (the matrix $M$). Assume that the columns of $M$ are $x_1$ and $x_2$. We know that (Jordan form) $AM = MJ$. Computing the product a column at time we have: $$ Ax_1 = \lambda_1x_1\\ x_1 = e_1 = \begin{bmatrix}1\\0\end{bmatrix}\\ Ax_2 = x_1 + \lambda_1x_2\\ (A - I\lambda_1)x_2 = x_1 $$ And $x_2$ is the generalized eigenvector ($\begin{bmatrix}c\\1\end{bmatrix}$). Question 1 : the choice of $c$ is up to me? I imagine that, because $M$ should be not singulare, I have to choose the right value by myself. Question 2 is: is always possible to solve $(A - I\lambda_1)x_2 = x_1$ and why? In other words, the generalized eigenvector always exists?",,"['matrices', 'block-matrices']"
34,How do you write / represent the 'all ones matrix'?,How do you write / represent the 'all ones matrix'?,,Is there a convention to write the all ones matrix in formulas? I'm going to write about the following formular: $$ A = B + XD + DX + N $$ Where D is a diagonal matrix and X the all ones matrix: $$ X = \begin{pmatrix}   1 & 1 & \cdots & 1 \\   1 & 1 & \cdots & 1 \\   \vdots & \vdots & \ddots & \vdots \\   1 & 1 & \cdots & 1  \end{pmatrix} $$ Is there a greek letter or other convention?,Is there a convention to write the all ones matrix in formulas? I'm going to write about the following formular: $$ A = B + XD + DX + N $$ Where D is a diagonal matrix and X the all ones matrix: $$ X = \begin{pmatrix}   1 & 1 & \cdots & 1 \\   1 & 1 & \cdots & 1 \\   \vdots & \vdots & \ddots & \vdots \\   1 & 1 & \cdots & 1  \end{pmatrix} $$ Is there a greek letter or other convention?,,"['matrices', 'notation', 'convention']"
35,Prove that this matrix equation has no roots if a matrix meets certain conditions,Prove that this matrix equation has no roots if a matrix meets certain conditions,,"Could you explain to me how to solve matrix equations? Here is an example: Prove that: $$2X^2 + X = \begin{bmatrix} -1&5&3\\-2&1&2\\0&-4&-3\end{bmatrix}$$ has no solutions in $M(3,3;\mathbb{R})$, where $M(3,3;\mathbb{R})$ is the space of all $3\times3$ matrices with real entries.","Could you explain to me how to solve matrix equations? Here is an example: Prove that: $$2X^2 + X = \begin{bmatrix} -1&5&3\\-2&1&2\\0&-4&-3\end{bmatrix}$$ has no solutions in $M(3,3;\mathbb{R})$, where $M(3,3;\mathbb{R})$ is the space of all $3\times3$ matrices with real entries.",,['matrices']
36,Power of a block matrix with eigenvalues on the unit circle,Power of a block matrix with eigenvalues on the unit circle,,"In the expression $$\begin{bmatrix}A & C \\ 0 & B\end{bmatrix}^n = \begin{bmatrix}A^n & * \\ 0 & B^n\end{bmatrix},$$ I wonder whether the term denoted by * can be expressed in a simple form when we assume the following: (1) $A$ has its eigenvalues on or inside the unit circle. Those on the         unit circle are simple;  (2)  $B$ has its eigenvalues strictly inside the unit circle; (3)  $A$ and $B$ may have different dimensions. In fact, I am interested for the value of * as $n \rightarrow \infty$. It would be $C(I-B)^{-1}$ when $A=I$ but in a general case, $A^n$, though bounded, may not converge as $n \rightarrow \infty$. So, I wonder whether * can have a simple expression.","In the expression $$\begin{bmatrix}A & C \\ 0 & B\end{bmatrix}^n = \begin{bmatrix}A^n & * \\ 0 & B^n\end{bmatrix},$$ I wonder whether the term denoted by * can be expressed in a simple form when we assume the following: (1) $A$ has its eigenvalues on or inside the unit circle. Those on the         unit circle are simple;  (2)  $B$ has its eigenvalues strictly inside the unit circle; (3)  $A$ and $B$ may have different dimensions. In fact, I am interested for the value of * as $n \rightarrow \infty$. It would be $C(I-B)^{-1}$ when $A=I$ but in a general case, $A^n$, though bounded, may not converge as $n \rightarrow \infty$. So, I wonder whether * can have a simple expression.",,"['matrices', 'convergence-divergence', 'exponentiation']"
37,Eigenvalues of $\left[\begin{matrix}1&\epsilon&2\epsilon\\\epsilon&1&2\epsilon\\-2\epsilon&-2\epsilon&2\end{matrix} \right]$ as a series in $\epsilon$,Eigenvalues of  as a series in,\left[\begin{matrix}1&\epsilon&2\epsilon\\\epsilon&1&2\epsilon\\-2\epsilon&-2\epsilon&2\end{matrix} \right] \epsilon,"Consider the matrix $$ M := \left[ \begin{matrix} 1 & \epsilon & 2\epsilon \\ \epsilon & 1 & 2\epsilon \\ -2\epsilon & - 2\epsilon & 2 \end{matrix} \right] $$ for some small parameter $0 < \epsilon \ll 1$ . My question is how does one solve for the eigenvalues $\lambda$ of this matrix as a series in $\epsilon$ , given that the zeroth-order (with $\epsilon = 0$ ) eigenvalues have degeneracy ? I am more interested in the procedure than anything else here. The eigenvalues are exactly given by $$ \lambda \in \left\{ 1 - \epsilon, \frac{3 + \epsilon - \sqrt{ 1 - 2 \epsilon - 31 \epsilon^2}}{2} , \frac{3 + \epsilon + \sqrt{ 1 - 2 \epsilon - 31 \epsilon^2}}{2} \right\} \ , $$ which when expanded for $0  < \epsilon \ll 1$ gives respectively $$ \lambda \in \left\{ 1 - \epsilon, 1 + \epsilon + 8 \epsilon^2 + \mathcal{O}(\epsilon^3)  , 2 -  8\epsilon^2 + \mathcal{O}(\epsilon^3) \right\} \ . $$ How would one solve for these eigenvalues as a series if it was not possible to solve for the exact eigenvalues? My attempt: Normally what I would do would be to assume I have a series $$ \lambda = \lambda_0 + \epsilon \lambda_1 + \epsilon^2 \lambda_2 + \ldots $$ and insert this series into the characteristic equation for the matrix which is here $$ \lambda^3 - 4 \lambda^2 + (5 + 7 \epsilon^2) \lambda -2 - 6 \epsilon^2 + 8 \epsilon^3 = 0 \ . $$ However this method yields nonsense because of the degeneracy of the first two eigenvalues (both being 1) in the limit that $\epsilon = 0$ . To describe what happens: the characteristic equation tells me that $\lambda_0^3 - 4 \lambda_0^2 + 5 \lambda_0 - 2 = 0$ for $\epsilon =0$ , which gives me that $\lambda_0 \in \{1,1,2\}$ . However, at next order in $\epsilon$ in the characteristic equation tells me $5 \lambda_1 - 8 \lambda_0\lambda_1 + 3 \lambda_0^2 \lambda_1 = 0$ , which when evaluated for the degenerate $\lambda_0 = 1$ just says that $0=0$ and so gives no information at all, and so I cannot proceed to find $\lambda_1$ .","Consider the matrix for some small parameter . My question is how does one solve for the eigenvalues of this matrix as a series in , given that the zeroth-order (with ) eigenvalues have degeneracy ? I am more interested in the procedure than anything else here. The eigenvalues are exactly given by which when expanded for gives respectively How would one solve for these eigenvalues as a series if it was not possible to solve for the exact eigenvalues? My attempt: Normally what I would do would be to assume I have a series and insert this series into the characteristic equation for the matrix which is here However this method yields nonsense because of the degeneracy of the first two eigenvalues (both being 1) in the limit that . To describe what happens: the characteristic equation tells me that for , which gives me that . However, at next order in in the characteristic equation tells me , which when evaluated for the degenerate just says that and so gives no information at all, and so I cannot proceed to find .","
M := \left[ \begin{matrix} 1 & \epsilon & 2\epsilon \\ \epsilon & 1 & 2\epsilon \\ -2\epsilon & - 2\epsilon & 2 \end{matrix} \right]
 0 < \epsilon \ll 1 \lambda \epsilon \epsilon = 0 
\lambda \in \left\{ 1 - \epsilon, \frac{3 + \epsilon - \sqrt{ 1 - 2 \epsilon - 31 \epsilon^2}}{2} , \frac{3 + \epsilon + \sqrt{ 1 - 2 \epsilon - 31 \epsilon^2}}{2} \right\} \ ,
 0  < \epsilon \ll 1 
\lambda \in \left\{ 1 - \epsilon, 1 + \epsilon + 8 \epsilon^2 + \mathcal{O}(\epsilon^3)  , 2 -  8\epsilon^2 + \mathcal{O}(\epsilon^3) \right\} \ .
 
\lambda = \lambda_0 + \epsilon \lambda_1 + \epsilon^2 \lambda_2 + \ldots
 
\lambda^3 - 4 \lambda^2 + (5 + 7 \epsilon^2) \lambda -2 - 6 \epsilon^2 + 8 \epsilon^3 = 0 \ .
 \epsilon = 0 \lambda_0^3 - 4 \lambda_0^2 + 5 \lambda_0 - 2 = 0 \epsilon =0 \lambda_0 \in \{1,1,2\} \epsilon 5 \lambda_1 - 8 \lambda_0\lambda_1 + 3 \lambda_0^2 \lambda_1 = 0 \lambda_0 = 1 0=0 \lambda_1","['matrices', 'eigenvalues-eigenvectors', 'power-series', 'perturbation-theory']"
38,Inverting Triangular Matrices,Inverting Triangular Matrices,,"A $d × d$ triangular matrix $L$ with non-zero diagonal entries can be expressed in the form $(\Delta + A)$ , where $Δ$ is an invertible diagonal matrix and $A$ is a strictly triangular matrix. Show how to compute the inverse of $L$ using only diagonal matrix inversions and matrix multiplicatons/additions. Note that strictly triangular matrices of size $d × d$ are always nilpotent and satisfy $A^d = 0$ . It is mentioned a page before that $(I+A)^{-1}=I-A+A^2-A^3+A^4+\dots$ which I see is obtained via Taylor Expansions, using this hint, my attempt: $(\Delta+A)^{-1}=\\\Delta^{-1}\Delta(A+\Delta)^{-1}=\\\Delta^{-1}[(A+\Delta)\Delta^{-1}]^{-1}=\\\Delta^{-1}(I+A\Delta^{-1})^{-1}$ Expanding: $\Delta^{-1}[I-A\Delta^{-1}+(A\Delta^{-1})^2-(A\Delta^{-1})^3+\cdots]$ And since $A$ is a nilpotent matrix, any $A^d=0$ , therefore: $(\Delta+A)^{-1}=\Delta^{-1}(I-A\Delta^{-1})$ Although something in my mathematical gut tells me this is ridiculous. SOURCE : Linear Algebra and Optimization for Machine Learning: A Textbook (Charu C. Aggarwal); page 18 problem 1.2.11","A triangular matrix with non-zero diagonal entries can be expressed in the form , where is an invertible diagonal matrix and is a strictly triangular matrix. Show how to compute the inverse of using only diagonal matrix inversions and matrix multiplicatons/additions. Note that strictly triangular matrices of size are always nilpotent and satisfy . It is mentioned a page before that which I see is obtained via Taylor Expansions, using this hint, my attempt: Expanding: And since is a nilpotent matrix, any , therefore: Although something in my mathematical gut tells me this is ridiculous. SOURCE : Linear Algebra and Optimization for Machine Learning: A Textbook (Charu C. Aggarwal); page 18 problem 1.2.11",d × d L (\Delta + A) Δ A L d × d A^d = 0 (I+A)^{-1}=I-A+A^2-A^3+A^4+\dots (\Delta+A)^{-1}=\\\Delta^{-1}\Delta(A+\Delta)^{-1}=\\\Delta^{-1}[(A+\Delta)\Delta^{-1}]^{-1}=\\\Delta^{-1}(I+A\Delta^{-1})^{-1} \Delta^{-1}[I-A\Delta^{-1}+(A\Delta^{-1})^2-(A\Delta^{-1})^3+\cdots] A A^d=0 (\Delta+A)^{-1}=\Delta^{-1}(I-A\Delta^{-1}),"['matrices', 'inverse']"
39,Finding a symmetric adjacency matrix closest to a given (non-symmetric) adjacency matrix,Finding a symmetric adjacency matrix closest to a given (non-symmetric) adjacency matrix,,"I am trying to solve a problem on graphs, which I have reduced to the following optimization problem in matrix $X \in \{0,1\}^{n \times n}$ $$\begin{array}{ll} \text{minimize} & \| X - A \|_F^2\\ \text{subject to} & X 1_n = m 1_n\\ & X=X^\top\end{array}$$ where matrix $A \in \{0,1\}^{n \times n}$ is given. Matrix $X$ is the adjacency matrix of a non-directed $m$ -regular graph, while matrix $A$ is the adjacency matrix of a directed graph. I am quite clueless on how to go on solving this problem and would be happy to get a direction.","I am trying to solve a problem on graphs, which I have reduced to the following optimization problem in matrix where matrix is given. Matrix is the adjacency matrix of a non-directed -regular graph, while matrix is the adjacency matrix of a directed graph. I am quite clueless on how to go on solving this problem and would be happy to get a direction.","X \in \{0,1\}^{n \times n} \begin{array}{ll} \text{minimize} & \| X - A \|_F^2\\ \text{subject to} & X 1_n = m 1_n\\ & X=X^\top\end{array} A \in \{0,1\}^{n \times n} X m A","['matrices', 'graph-theory', 'discrete-optimization', 'binary-programming', 'adjacency-matrix']"
40,Filling an array(Putnam),Filling an array(Putnam),,"Alan and Barbara play a game in which they take turns filling entries of an initially empty $ 2008\times 2008$ array. Alan plays first. At each turn, a player chooses a real number and places it in a vacant entry. The game ends when all entries are filled. Alan wins if the determinant of the resulting matrix is nonzero; Barbara wins if it is zero. Which player has a winning strategy? This question has been posted before. I saw a solution recently this way: When Alan places a number in any spot in the array, Barbara places the same number in the column, but one row up or down. That was she forces the rows to be linearly dependent, and the resulting determinate is zero. This method works fine and I tried with arbitrary values in matrices of smaller rank. But I need help in proving that this indeed forms linearly dependent rows which leads the determinant of the matrix to be zero?","Alan and Barbara play a game in which they take turns filling entries of an initially empty $ 2008\times 2008$ array. Alan plays first. At each turn, a player chooses a real number and places it in a vacant entry. The game ends when all entries are filled. Alan wins if the determinant of the resulting matrix is nonzero; Barbara wins if it is zero. Which player has a winning strategy? This question has been posted before. I saw a solution recently this way: When Alan places a number in any spot in the array, Barbara places the same number in the column, but one row up or down. That was she forces the rows to be linearly dependent, and the resulting determinate is zero. This method works fine and I tried with arbitrary values in matrices of smaller rank. But I need help in proving that this indeed forms linearly dependent rows which leads the determinant of the matrix to be zero?",,"['matrices', 'contest-math', 'determinant', 'combinatorial-game-theory']"
41,Decomposition to rotation around arbitrary axis,Decomposition to rotation around arbitrary axis,,"In 3d, I have a $4\times4$ matrix $M$, which has only a rotation part and a translation part. In other words, I can compute $X'=RX+T$ ( with $R$ a $3\times3$ rotation matrix, $T$ a vector for the translation, $X$ a point and $X'$ its image). Can I write the $4\times4$ matrix $M$ as a rotation around an arbitrary axis (not necessarily through origin)? I would write it as $X'=R(X-X_a)+X_a$  with $X_a$ a point on the axis, and $R$ the same rotation matrix. If yes, I would like to get one point $X_a$, i.e. a point on the axis. My first thought was to solve the system $RX_a+T=X_a$. It's a singular system because we look for an axis. So I set $z=0$ and solve for $x$ and $y$ to get one point. Is there a mistake somewhere? I thought about another way to get this axis, but I don't understand why it does not work. I take two random points, $X_1$ and $X_2$ and compute their image $X_1'$ and $X_2'$ by $M$. I assume the axis is on the plane perpendicular to $X_1X_1'$ through the middle of $X_1X_1'$. Indeed, $X_1$ would be at the same distance to the axis than $X_1'$. I assume this also for $X_2X_2'$. So I get the axis with the intersection between both planes. What is wrong in this approach? Thanks for helping","In 3d, I have a $4\times4$ matrix $M$, which has only a rotation part and a translation part. In other words, I can compute $X'=RX+T$ ( with $R$ a $3\times3$ rotation matrix, $T$ a vector for the translation, $X$ a point and $X'$ its image). Can I write the $4\times4$ matrix $M$ as a rotation around an arbitrary axis (not necessarily through origin)? I would write it as $X'=R(X-X_a)+X_a$  with $X_a$ a point on the axis, and $R$ the same rotation matrix. If yes, I would like to get one point $X_a$, i.e. a point on the axis. My first thought was to solve the system $RX_a+T=X_a$. It's a singular system because we look for an axis. So I set $z=0$ and solve for $x$ and $y$ to get one point. Is there a mistake somewhere? I thought about another way to get this axis, but I don't understand why it does not work. I take two random points, $X_1$ and $X_2$ and compute their image $X_1'$ and $X_2'$ by $M$. I assume the axis is on the plane perpendicular to $X_1X_1'$ through the middle of $X_1X_1'$. Indeed, $X_1$ would be at the same distance to the axis than $X_1'$. I assume this also for $X_2X_2'$. So I get the axis with the intersection between both planes. What is wrong in this approach? Thanks for helping",,"['matrices', 'geometry', 'rotations']"
42,"If $e^{xA}$ and $e^{yB}$ commute $\forall x,y\in \mathbb{R}$, do A and B commute?","If  and  commute , do A and B commute?","e^{xA} e^{yB} \forall x,y\in \mathbb{R}","We know that the matrices $A$ and $B$ don't commute even if $e^{A}$ and $e^{B}$ commute. However, if the problem now is that $e^{xA}$ and $e^{yB}$ commute $\forall x,y\in \mathbb{R}$, then do A and B commute? The following is my proof: $\frac{\partial }{\partial y}\frac{\partial }{\partial x}(e^{xA}e^{yB})=(\frac{\mathrm{d} }{\mathrm{d} x}e^{xA})(\frac{\mathrm{d} }{\mathrm{d} y}e^{yB})=Ae^{xA}Be^{yB}$ $\frac{\partial }{\partial y}\frac{\partial }{\partial x}(e^{yB}e^{xA})=(\frac{\mathrm{d} }{\mathrm{d} y}e^{yB})(\frac{\mathrm{d} }{\mathrm{d} x}e^{xA})=Be^{yB}Ae^{xA}$ $\because0=\frac{\partial }{\partial y}\frac{\partial }{\partial x}[e^{xA},e^{yB}]=Ae^{xA}Be^{yB}-Be^{yB}Ae^{xA}$ Then set $x=y=0$, we have $[A,B]=0$. Is my proof correct?","We know that the matrices $A$ and $B$ don't commute even if $e^{A}$ and $e^{B}$ commute. However, if the problem now is that $e^{xA}$ and $e^{yB}$ commute $\forall x,y\in \mathbb{R}$, then do A and B commute? The following is my proof: $\frac{\partial }{\partial y}\frac{\partial }{\partial x}(e^{xA}e^{yB})=(\frac{\mathrm{d} }{\mathrm{d} x}e^{xA})(\frac{\mathrm{d} }{\mathrm{d} y}e^{yB})=Ae^{xA}Be^{yB}$ $\frac{\partial }{\partial y}\frac{\partial }{\partial x}(e^{yB}e^{xA})=(\frac{\mathrm{d} }{\mathrm{d} y}e^{yB})(\frac{\mathrm{d} }{\mathrm{d} x}e^{xA})=Be^{yB}Ae^{xA}$ $\because0=\frac{\partial }{\partial y}\frac{\partial }{\partial x}[e^{xA},e^{yB}]=Ae^{xA}Be^{yB}-Be^{yB}Ae^{xA}$ Then set $x=y=0$, we have $[A,B]=0$. Is my proof correct?",,['matrices']
43,The set of all matrix with rank $n-1$ is a hypersurface.,The set of all matrix with rank  is a hypersurface.,n-1,"Prove that the set $M$ of $n\times n$ matrices with rank $n-1$ is a hypersurface in $\mathbb{R}^{n²}$ and find the tangent space at $A=(a_{ij})$ where $a_{ij}=\begin{cases}  \delta_{ij} \ \text{if} \ (i,j)\neq(n,n) \\   0 \ \ \ \text{if} \  (i,j)=(n,n) \end{cases}$ The teacher left this problem along with others to study for the test (a graduate course in multivariable calculus).  If I find a function $f:\mathbb{R}^{n²} \rightarrow \mathbb{R}\in C^1$ such that $M=f^{-1}(c)$ and $grad \ f(x)\neq 0 \  (\forall x;f(x)=c$) do the problem becomes a merely computation but I can't find such function. Can anybody help me? Thanks in advance.","Prove that the set $M$ of $n\times n$ matrices with rank $n-1$ is a hypersurface in $\mathbb{R}^{n²}$ and find the tangent space at $A=(a_{ij})$ where $a_{ij}=\begin{cases}  \delta_{ij} \ \text{if} \ (i,j)\neq(n,n) \\   0 \ \ \ \text{if} \  (i,j)=(n,n) \end{cases}$ The teacher left this problem along with others to study for the test (a graduate course in multivariable calculus).  If I find a function $f:\mathbb{R}^{n²} \rightarrow \mathbb{R}\in C^1$ such that $M=f^{-1}(c)$ and $grad \ f(x)\neq 0 \  (\forall x;f(x)=c$) do the problem becomes a merely computation but I can't find such function. Can anybody help me? Thanks in advance.",,"['matrices', 'multivariable-calculus', 'manifolds', 'smooth-manifolds']"
44,"""Averaging"" transformation matrices?","""Averaging"" transformation matrices?",,"I have a question on how best to ""average"" transformation matrices. Say that I have n number of 4x4 transformation matrices, and I wanted to find a matrix that approximated each one of the n 4x4 transformation matrices (an average of sorts). Are there any methods that would work?","I have a question on how best to ""average"" transformation matrices. Say that I have n number of 4x4 transformation matrices, and I wanted to find a matrix that approximated each one of the n 4x4 transformation matrices (an average of sorts). Are there any methods that would work?",,"['matrices', 'affine-geometry', 'least-squares', 'rigid-transformation']"
45,Determinant of block matrix with commuting blocks,Determinant of block matrix with commuting blocks,,"I know that given a $2N \times 2N$ block matrix with $N \times N$ blocks like $$\mathbf{S} = \begin{pmatrix} A & B\\  C & D \end{pmatrix}$$ we can calculate $$\det(\mathbf{S})=\det(AD-BD^{-1}CD)$$ and so clearly if $D$ and $C$ commute this reduces to $\det(AD-BC)$ , which is a very nice property. My question is, for a general $nN\times nN$ matrix with $N\times N$ blocks where all of the blocks commute with each other, can we find the determinant in a similar way? That is, by first finding the determinant treating the blocks like scalars and then taking the determinant of the resulting $N\times N$ matrix. Thanks in advance!","I know that given a block matrix with blocks like we can calculate and so clearly if and commute this reduces to , which is a very nice property. My question is, for a general matrix with blocks where all of the blocks commute with each other, can we find the determinant in a similar way? That is, by first finding the determinant treating the blocks like scalars and then taking the determinant of the resulting matrix. Thanks in advance!","2N \times 2N N \times N \mathbf{S} = \begin{pmatrix}
A & B\\ 
C & D
\end{pmatrix} \det(\mathbf{S})=\det(AD-BD^{-1}CD) D C \det(AD-BC) nN\times nN N\times N N\times N","['matrices', 'determinant', 'block-matrices']"
46,"If $f(AB) =f(A)f(B)$, then $A$ is inversible iff $f(A)\neq 0$","If , then  is inversible iff",f(AB) =f(A)f(B) A f(A)\neq 0,"Let $f:\mathscr M_n(\mathbb K) \to \mathbb K$ be a non constant function such as $f(AB) = f(A)f(B)$ for all $A,B$ in $\mathscr M_n(\mathbb K)$. The question is to show that $M\in GL_n(\mathbb K)$ iff $f(M)\neq 0$. I've already shown that $f(Id) = 1$ and $M\in GL_n(\mathbb K) \Rightarrow f(M)\neq 0$. I'm stuck on the converse.","Let $f:\mathscr M_n(\mathbb K) \to \mathbb K$ be a non constant function such as $f(AB) = f(A)f(B)$ for all $A,B$ in $\mathscr M_n(\mathbb K)$. The question is to show that $M\in GL_n(\mathbb K)$ iff $f(M)\neq 0$. I've already shown that $f(Id) = 1$ and $M\in GL_n(\mathbb K) \Rightarrow f(M)\neq 0$. I'm stuck on the converse.",,"['matrices', 'inverse']"
47,Maximum dimension of a nilpotent vector space,Maximum dimension of a nilpotent vector space,,"What is the maximum dimension of a vector space of $\mathcal{M}_n(\mathbb{R})$ containing only nilpotent matrices ? ($\mathcal{M}_n(\mathbb{R})$ : matrices $n\times n$ with coefficients in $\mathbb{R}$) I don't really know how to solve this problem.There must be a way to give some good upper bound to the dimension of the vector space, which would seem to be $(n^2-n)/2$, but I can't manage to get a good result ...","What is the maximum dimension of a vector space of $\mathcal{M}_n(\mathbb{R})$ containing only nilpotent matrices ? ($\mathcal{M}_n(\mathbb{R})$ : matrices $n\times n$ with coefficients in $\mathbb{R}$) I don't really know how to solve this problem.There must be a way to give some good upper bound to the dimension of the vector space, which would seem to be $(n^2-n)/2$, but I can't manage to get a good result ...",,"['matrices', 'nilpotence']"
48,How to reverse matrix vector multiplication?,How to reverse matrix vector multiplication?,,"I'm using the simple matrix x vector multiplication below to calculate result . And now I wonder how can I calculate vector if I know matrix and result ? So when I multiply matrix and vector afterwards again I get result . Sorry I don't know how you call this multiplication. I was never deep in those math topics. I have a program that does my calculation. I hope you can understand and classify the multiplication: // float[] matrix = [4x4], float[] vector = [4] column vector  float[] result = new float[vector.Length]; for (int column = 0; column < 4; column++) {     for (int row = 0; row < 4; row++)         result[column] += matrix[column + row * 4] * vector[row]; } return result; Update: I found this for inverted matrices and I now remember mathematicians don't care for complexity of things but we programmers do. Is there no way to avoid matrix inversion (to lessen complexity) ? Solution: I implemented the raw 4x4 matrix inversion and (alternatively) I inverted the matrix generation . In the end I got the very same matrices and the very same valid results for my vector. I choose the 2nd path because that reduces the complexity to around the same as the calculation done in my first sentence above. Thank you for the help!","I'm using the simple matrix x vector multiplication below to calculate result . And now I wonder how can I calculate vector if I know matrix and result ? So when I multiply matrix and vector afterwards again I get result . Sorry I don't know how you call this multiplication. I was never deep in those math topics. I have a program that does my calculation. I hope you can understand and classify the multiplication: // float[] matrix = [4x4], float[] vector = [4] column vector  float[] result = new float[vector.Length]; for (int column = 0; column < 4; column++) {     for (int row = 0; row < 4; row++)         result[column] += matrix[column + row * 4] * vector[row]; } return result; Update: I found this for inverted matrices and I now remember mathematicians don't care for complexity of things but we programmers do. Is there no way to avoid matrix inversion (to lessen complexity) ? Solution: I implemented the raw 4x4 matrix inversion and (alternatively) I inverted the matrix generation . In the end I got the very same matrices and the very same valid results for my vector. I choose the 2nd path because that reduces the complexity to around the same as the calculation done in my first sentence above. Thank you for the help!",,"['matrices', 'numerical-linear-algebra', 'vectors']"
49,The importance of the full-row-rank assumption for the simplex method,The importance of the full-row-rank assumption for the simplex method,,"Consider a linear programming model in the usual form ready for applying the simplex method. I understand that having the constraint equations' coefficient matrix $A$ be of full row rank means not having any redundancy in the constraints. However, will $A$ not being of full row rank cause any problems in the application of the simplex algorithm?  For example, if there is a degenerate basic feasible point that has not been preprocessed away, then the simplex algorithm (if unmodified) may go into an infinite loop and never terminate.  How about when $A$ is not of full row rank?","Consider a linear programming model in the usual form ready for applying the simplex method. I understand that having the constraint equations' coefficient matrix $A$ be of full row rank means not having any redundancy in the constraints. However, will $A$ not being of full row rank cause any problems in the application of the simplex algorithm?  For example, if there is a degenerate basic feasible point that has not been preprocessed away, then the simplex algorithm (if unmodified) may go into an infinite loop and never terminate.  How about when $A$ is not of full row rank?",,"['matrices', 'optimization', 'convex-optimization', 'linear-programming', 'simplex-method']"
50,Calculate the pseudo inverse of the matrix,Calculate the pseudo inverse of the matrix,,"The subject is to calculate the pseudo inverse if matrix $\begin{equation*}   \mathbf{A} = \left(     \begin{array}{ccc}       1 & 0  \\       2 & 1  \\       0 & 1  \\     \end{array}   \right) \end{equation*}$ My answer is as follows: (SVD decomposition) First, $\begin{equation*}   \mathbf{A^TA} = \left(     \begin{array}{ccc}       5 & 2  \\       2 & 2  \\     \end{array}   \right) \end{equation*}$, with eigenvalues $\lambda_1 = 6, \lambda_2 = 1$, and eigenvectors $\begin{equation*}   \mathbf{x_1} = \frac{1}{\sqrt{5}}\left(      \begin{array}{ccc}       2   \\       1   \\     \end{array}   \right) \end{equation*}$, $\begin{equation*}   \mathbf{x_2} = \frac{1}{\sqrt{5}}\left(      \begin{array}{ccc}       -1   \\       2   \\     \end{array}   \right) \end{equation*}$, so the matrix $\begin{equation*}   \mathbf{V} = \frac{1}{\sqrt{5}}\left(      \begin{array}{ccc}       2 & -1  \\       1 & 2  \\     \end{array}   \right) \end{equation*}$. Second, $\begin{equation*}   \mathbf{AA^T} = \left(     \begin{array}{ccc}       1 & 2 & 0 \\       2 & 5 & 1 \\       0 & 1 & 1 \\     \end{array}   \right) \end{equation*}$, with eigenvalues $\lambda_1 = 6, \lambda_2 = 1,\lambda_3 = 0$, and eigenvectors $\begin{equation*}   \mathbf{x_1} = \frac{1}{\sqrt{30}}\left(      \begin{array}{ccc}       2   \\       5   \\       1   \\     \end{array}   \right) \end{equation*}$, $\begin{equation*}   \mathbf{x_2} = \frac{1}{\sqrt{5}}\left(      \begin{array}{ccc}       -1   \\       0    \\       2   \\     \end{array}   \right) \end{equation*}$, $\begin{equation*}   \mathbf{x_3} = \frac{1}{\sqrt{6}}\left(      \begin{array}{ccc}       2   \\       -1    \\       1  \\     \end{array}   \right) \end{equation*}$ , so the matrix $\begin{equation*}   \mathbf{U} = \left(      \begin{array}{ccc}       \frac{2}{\sqrt{30}} & - \frac{1}{\sqrt{5}} & \frac{2}{\sqrt{6}} \\       \frac{5}{\sqrt{30}} & 0 & - \frac{1}{\sqrt{6}} \\       \frac{1}{\sqrt{30}} &  \frac{2}{\sqrt{5}} & \frac{1}{\sqrt{6}} \\     \end{array}   \right) \end{equation*}$, and  $\begin{equation*}   \mathbf{\Sigma} = \left(      \begin{array}{ccc}       6 & 0  \\       0 & 1  \\       0 & 0  \\     \end{array}   \right) \end{equation*}$. Then, the pseudo inverse becomes: $A^+ = V \Sigma^+U^T$. The problem comes to: when I was checking the SVD decomposition, I found $A\ne U\Sigma V^T$. However, I find nothing odds in the calculation. Please help me to point out the error.","The subject is to calculate the pseudo inverse if matrix $\begin{equation*}   \mathbf{A} = \left(     \begin{array}{ccc}       1 & 0  \\       2 & 1  \\       0 & 1  \\     \end{array}   \right) \end{equation*}$ My answer is as follows: (SVD decomposition) First, $\begin{equation*}   \mathbf{A^TA} = \left(     \begin{array}{ccc}       5 & 2  \\       2 & 2  \\     \end{array}   \right) \end{equation*}$, with eigenvalues $\lambda_1 = 6, \lambda_2 = 1$, and eigenvectors $\begin{equation*}   \mathbf{x_1} = \frac{1}{\sqrt{5}}\left(      \begin{array}{ccc}       2   \\       1   \\     \end{array}   \right) \end{equation*}$, $\begin{equation*}   \mathbf{x_2} = \frac{1}{\sqrt{5}}\left(      \begin{array}{ccc}       -1   \\       2   \\     \end{array}   \right) \end{equation*}$, so the matrix $\begin{equation*}   \mathbf{V} = \frac{1}{\sqrt{5}}\left(      \begin{array}{ccc}       2 & -1  \\       1 & 2  \\     \end{array}   \right) \end{equation*}$. Second, $\begin{equation*}   \mathbf{AA^T} = \left(     \begin{array}{ccc}       1 & 2 & 0 \\       2 & 5 & 1 \\       0 & 1 & 1 \\     \end{array}   \right) \end{equation*}$, with eigenvalues $\lambda_1 = 6, \lambda_2 = 1,\lambda_3 = 0$, and eigenvectors $\begin{equation*}   \mathbf{x_1} = \frac{1}{\sqrt{30}}\left(      \begin{array}{ccc}       2   \\       5   \\       1   \\     \end{array}   \right) \end{equation*}$, $\begin{equation*}   \mathbf{x_2} = \frac{1}{\sqrt{5}}\left(      \begin{array}{ccc}       -1   \\       0    \\       2   \\     \end{array}   \right) \end{equation*}$, $\begin{equation*}   \mathbf{x_3} = \frac{1}{\sqrt{6}}\left(      \begin{array}{ccc}       2   \\       -1    \\       1  \\     \end{array}   \right) \end{equation*}$ , so the matrix $\begin{equation*}   \mathbf{U} = \left(      \begin{array}{ccc}       \frac{2}{\sqrt{30}} & - \frac{1}{\sqrt{5}} & \frac{2}{\sqrt{6}} \\       \frac{5}{\sqrt{30}} & 0 & - \frac{1}{\sqrt{6}} \\       \frac{1}{\sqrt{30}} &  \frac{2}{\sqrt{5}} & \frac{1}{\sqrt{6}} \\     \end{array}   \right) \end{equation*}$, and  $\begin{equation*}   \mathbf{\Sigma} = \left(      \begin{array}{ccc}       6 & 0  \\       0 & 1  \\       0 & 0  \\     \end{array}   \right) \end{equation*}$. Then, the pseudo inverse becomes: $A^+ = V \Sigma^+U^T$. The problem comes to: when I was checking the SVD decomposition, I found $A\ne U\Sigma V^T$. However, I find nothing odds in the calculation. Please help me to point out the error.",,"['matrices', 'svd']"
51,A particular subgroup of the general linear group,A particular subgroup of the general linear group,,"Say a $n \times n$ matrix with real coefficients $a_{ij}$ has property P if $\sum_j a_{ij} >0, \forall i$. Say a group (or subgroup) of matrices has property P if every element has property P. What would be the biggest subgroup of the general linear group $\text{GL}_ n(\mathbb{R})$ having property P ? I'm alread having some problems for the $2 \times 2$ case. I found that matrices of the form $\begin{bmatrix} a & 0 \\ 0 & b\end{bmatrix}$ or $\begin{bmatrix} 0 & a \\ b & 0\end{bmatrix}$ with strictly positive coefficients work, obviously, as well as matrices of the form $\begin{bmatrix} 1 & a \\ 0 & 1+a\end{bmatrix}$ ($a > 0$), but I don't see a general way of doing it.","Say a $n \times n$ matrix with real coefficients $a_{ij}$ has property P if $\sum_j a_{ij} >0, \forall i$. Say a group (or subgroup) of matrices has property P if every element has property P. What would be the biggest subgroup of the general linear group $\text{GL}_ n(\mathbb{R})$ having property P ? I'm alread having some problems for the $2 \times 2$ case. I found that matrices of the form $\begin{bmatrix} a & 0 \\ 0 & b\end{bmatrix}$ or $\begin{bmatrix} 0 & a \\ b & 0\end{bmatrix}$ with strictly positive coefficients work, obviously, as well as matrices of the form $\begin{bmatrix} 1 & a \\ 0 & 1+a\end{bmatrix}$ ($a > 0$), but I don't see a general way of doing it.",,"['group-theory', 'matrices']"
52,The Matrix Equation $X^{2}=C$,The Matrix Equation,X^{2}=C,"Let matrix $C_{n\times‎n}$ and equation $X^{2}=C$ be given, i want to find matrix $X$. For $n=2$, $X$ is obtained by solving a system of equations; $$\left\{ \begin{array}{l} x_{11}^2 + {x_{12}}{x_{21}} = {c_{11}}\\ {x_{11}}{x_{12}} + {x_{12}}{x_{22}} = {c_{12}}\\ {x_{21}}{x_{11}} + {x_{22}}{x_{21}} = {c_{21}}\\ {x_{21}}{x_{12}} + x_{22}^2 = {c_{22}} \end{array} \right. $$ but does analytical method (without solve a system of equations) for solving such equations exist? Thanks.","Let matrix $C_{n\times‎n}$ and equation $X^{2}=C$ be given, i want to find matrix $X$. For $n=2$, $X$ is obtained by solving a system of equations; $$\left\{ \begin{array}{l} x_{11}^2 + {x_{12}}{x_{21}} = {c_{11}}\\ {x_{11}}{x_{12}} + {x_{12}}{x_{22}} = {c_{12}}\\ {x_{21}}{x_{11}} + {x_{22}}{x_{21}} = {c_{21}}\\ {x_{21}}{x_{12}} + x_{22}^2 = {c_{22}} \end{array} \right. $$ but does analytical method (without solve a system of equations) for solving such equations exist? Thanks.",,['matrices']
53,Differential of transposed matrices,Differential of transposed matrices,,"I'm puzzling about how to deal with the differential of a transposed matrix. I was wondering if there is some rule such that $d(X^{T}) = (dX)^{T}$. In general I work with derivation on the trace of a matrix and I get sometimes the following situation: $$ tr(d(X^{T})AX + Bd(X^{T})CX + DdX) $$ where X can be a rectangular matrix. I'm quite sure that such expression can be rearranged as follows: $$ tr((AX + CXB)d(X^{T})) + tr(DdX) $$ Clearly I would like to obtain something like $tr(J(X)dX)$ for derivative, but I'm not able to go on. Some suggestion?","I'm puzzling about how to deal with the differential of a transposed matrix. I was wondering if there is some rule such that $d(X^{T}) = (dX)^{T}$. In general I work with derivation on the trace of a matrix and I get sometimes the following situation: $$ tr(d(X^{T})AX + Bd(X^{T})CX + DdX) $$ where X can be a rectangular matrix. I'm quite sure that such expression can be rearranged as follows: $$ tr((AX + CXB)d(X^{T})) + tr(DdX) $$ Clearly I would like to obtain something like $tr(J(X)dX)$ for derivative, but I'm not able to go on. Some suggestion?",,"['matrices', 'derivatives', 'trace']"
54,How do I prove that in every commuting family there is a common eigenvector?,How do I prove that in every commuting family there is a common eigenvector?,,"The proof given by my textbook is highly non-satisfying. The author adopted some magic-like ""reductio ad absurdum"" and the proof (although is correct) didn't reveal the nature of this problem. I made my own effort into it and tried a different approach. Yet I can't finish it. Let $\mathscr{F}$ be a commuting family in $M_n(\mathbb{C}^n)$, and $A\in\mathscr{F}$, then $A$ has $n$ eigenvalues. We pick one, say $\lambda$. Let $x$ be one of its eigenvector. We can easily prove that, if $A$ has no other eigenvector with eigenvalue $\lambda$ that linearly independent with $x$, which means that $\{cx|c\in\mathbb{C}\}$ are the only vectors satisfying $Ax=\lambda x$, then $x$ is a common eigenvector. Because $\forall B\in\mathscr{F}$ and $\forall y\in \{cx|c\in\mathbb{C}\}$, $$A(Bx)=ABx=BAx=B(Ax)=B(\lambda x)=\lambda (Bx)$$, so that $Bx$ has to be in $\{cx|c\in\mathbb{C}\}$, that is, $Bx=c_0x$ for some $c_0\in\mathbb{C}$, which means $x$ is a eigenvector of $B$ too. But what if there are vectors satisfying $Ax=\lambda x$ that's not in  $\{cx|c\in\mathbb{C}\}$? Well, then we should have a set of linearly independent eigenvectors $\{x_1,x_2,...,x_k\}$, that $\{c_1x_1+c_2x_2+...+c_kx_k|c_i\in\mathbb{C}\}$ are the only vectors satisfying $Ax=\lambda x$. Now, I have a reasonable hypothesis that there exists some $x=c_1x_1+c_2x_2+...+c_kx_k$, that can be proven to be a common eigenvector of $\mathscr{F}$. I've tried some approaches to prove it but all failed. Do you guys believe it's true? And if it is true then how do I prove it?","The proof given by my textbook is highly non-satisfying. The author adopted some magic-like ""reductio ad absurdum"" and the proof (although is correct) didn't reveal the nature of this problem. I made my own effort into it and tried a different approach. Yet I can't finish it. Let $\mathscr{F}$ be a commuting family in $M_n(\mathbb{C}^n)$, and $A\in\mathscr{F}$, then $A$ has $n$ eigenvalues. We pick one, say $\lambda$. Let $x$ be one of its eigenvector. We can easily prove that, if $A$ has no other eigenvector with eigenvalue $\lambda$ that linearly independent with $x$, which means that $\{cx|c\in\mathbb{C}\}$ are the only vectors satisfying $Ax=\lambda x$, then $x$ is a common eigenvector. Because $\forall B\in\mathscr{F}$ and $\forall y\in \{cx|c\in\mathbb{C}\}$, $$A(Bx)=ABx=BAx=B(Ax)=B(\lambda x)=\lambda (Bx)$$, so that $Bx$ has to be in $\{cx|c\in\mathbb{C}\}$, that is, $Bx=c_0x$ for some $c_0\in\mathbb{C}$, which means $x$ is a eigenvector of $B$ too. But what if there are vectors satisfying $Ax=\lambda x$ that's not in  $\{cx|c\in\mathbb{C}\}$? Well, then we should have a set of linearly independent eigenvectors $\{x_1,x_2,...,x_k\}$, that $\{c_1x_1+c_2x_2+...+c_kx_k|c_i\in\mathbb{C}\}$ are the only vectors satisfying $Ax=\lambda x$. Now, I have a reasonable hypothesis that there exists some $x=c_1x_1+c_2x_2+...+c_kx_k$, that can be proven to be a common eigenvector of $\mathscr{F}$. I've tried some approaches to prove it but all failed. Do you guys believe it's true? And if it is true then how do I prove it?",,['matrices']
55,Why use a matrix?,Why use a matrix?,,Why would I want to use a matrix?  It's good for organizing a few numbers but I can't find too much use for them.  Could someone explain?,Why would I want to use a matrix?  It's good for organizing a few numbers but I can't find too much use for them.  Could someone explain?,,['matrices']
56,Efficient algorithm of rank-one update of the Cholesky decomposition,Efficient algorithm of rank-one update of the Cholesky decomposition,,"Suppose that I have a symmetric positive definite matrix $X$ and that I Cholesky-decompose it $$ X = L L^T $$ Now, given a vector $v$ , suppose we want to decompose the following matrix using the Cholesky decomposition: $$ X + v v^T = M M^T $$ I know the complexity of the Cholesky decomposition is $O(n^3)$ . Is there a algorithm to compute $M$ given $X$ , $L$ , $v$ , that has a complexity less than $O(n^3)$ ?","Suppose that I have a symmetric positive definite matrix and that I Cholesky-decompose it Now, given a vector , suppose we want to decompose the following matrix using the Cholesky decomposition: I know the complexity of the Cholesky decomposition is . Is there a algorithm to compute given , , , that has a complexity less than ?",X  X = L L^T  v  X + v v^T = M M^T  O(n^3) M X L v O(n^3),"['matrices', 'numerical-linear-algebra', 'matrix-decomposition', 'positive-definite', 'cholesky-decomposition']"
57,Hadamard Product Matrix Norm Inequality,Hadamard Product Matrix Norm Inequality,,"Let $A \odot B$ denote the Hadamard or entry-wise product of two matrices with equivalent dimensions.  In this post ( Hadamard product: Optimal bound on operator norm ) it is claimed without proof that if $A$ is positive-definite, then $$\|A \odot B\|_2 \leq \max_{i, j}|A_{i,j}| \|B\|_2$$ where $\| \cdot \|_2$ is the matrix operator norm induced from the Euclidean norm.  Does anyone know how to prove this or have a reference?  To me it does not seem so simple.","Let denote the Hadamard or entry-wise product of two matrices with equivalent dimensions.  In this post ( Hadamard product: Optimal bound on operator norm ) it is claimed without proof that if is positive-definite, then where is the matrix operator norm induced from the Euclidean norm.  Does anyone know how to prove this or have a reference?  To me it does not seem so simple.","A \odot B A \|A \odot B\|_2 \leq \max_{i, j}|A_{i,j}| \|B\|_2 \| \cdot \|_2",['matrices']
58,Faithful action of stabiliser on group of automorphisms of a hypersurface,Faithful action of stabiliser on group of automorphisms of a hypersurface,,"Let $f\in\mathbb{C}[x_{1},\ldots,x_{n}]$ , and $G_{f}\le\mathrm{GL}_{n}(\mathbb{C})$ be the group of all matrices under which $f$ is invariant, i.e. the stabiliser of $f$ in $\mathrm{GL}_{n}(\mathbb{C})$ consisting of all $\pi\in\mathrm{GL}_{n}(\mathbb{C})$ such that $f(\pi(x_{1},\ldots,x_{n}))=f$ . Then each matrix in $G_{f}$ induces an automorphism on the hypersurface $$V=\mathcal{V}(f)=\{p\in\mathbb{C}^{n}:f(p)=0\}.$$ Thus we get a group homomorphism $\phi:G_{f}\to\mathrm{Aut}(V)$ , that assigns each matrix an automorphism of $V$ . My question is; under what conditions is the action of $G_{f}$ on $V$ faithful? That is, when is the mapping, $\phi$ injective? There is a small discussion of this here , and it is mentioned that this question deserved its own thread.","Let , and be the group of all matrices under which is invariant, i.e. the stabiliser of in consisting of all such that . Then each matrix in induces an automorphism on the hypersurface Thus we get a group homomorphism , that assigns each matrix an automorphism of . My question is; under what conditions is the action of on faithful? That is, when is the mapping, injective? There is a small discussion of this here , and it is mentioned that this question deserved its own thread.","f\in\mathbb{C}[x_{1},\ldots,x_{n}] G_{f}\le\mathrm{GL}_{n}(\mathbb{C}) f f \mathrm{GL}_{n}(\mathbb{C}) \pi\in\mathrm{GL}_{n}(\mathbb{C}) f(\pi(x_{1},\ldots,x_{n}))=f G_{f} V=\mathcal{V}(f)=\{p\in\mathbb{C}^{n}:f(p)=0\}. \phi:G_{f}\to\mathrm{Aut}(V) V G_{f} V \phi","['matrices', 'algebraic-geometry', 'polynomials']"
59,Matrix decomposition into 2x2 elementary transforms,Matrix decomposition into 2x2 elementary transforms,,"Rotation matrices can be decomposed into a product of $\frac{n(n-1)}{2}$ elementary rotations operating on only two coordinates. Similarly, can any square matrix be decomposed into a product of $\frac{n(n-1)}{2}$ linear transforms operating on two coordinates only each? Note: $ n(n-1) $ can be done by reversing Gaussian elimination.","Rotation matrices can be decomposed into a product of elementary rotations operating on only two coordinates. Similarly, can any square matrix be decomposed into a product of linear transforms operating on two coordinates only each? Note: can be done by reversing Gaussian elimination.",\frac{n(n-1)}{2} \frac{n(n-1)}{2}  n(n-1) ,"['matrices', 'matrix-decomposition']"
60,Square roots of integral matrices,Square roots of integral matrices,,"Define $n\times n$ matrices $L , M$  as follows: $\,$ L$_{ij}$= $1$ if $\,$i + j $\geq$n + 1 and is zero otherwise; $\,$ $\qquad$ $\qquad$ $\quad$M$_{ij}$= $\min(i,j)$ for $1 \leq i , j \leq n$ .  It is straightforward to check that $M = L^2$ .  For example, when $n=3$ we have $\qquad$ $\qquad$ $\qquad$$\begin{bmatrix}1 & 1 & 1\\1 & 2 & 2\\1 & 2 & 3\\ \end{bmatrix}$ =  $\begin{bmatrix}0 & 0 & 1\\0 & 1 & 1\\1 & 1 & 1\\ \end{bmatrix}^2$ . [Remarks: $\det(M) = 1$ ; $M$ is positive definite but $L$ is not  ; the eigenvalues of $M$ are distinct ( to see this, it's easier to work with $M^{-1}$ which is tridiagonal ). ] General theory then predicts that $L$ can be written as a polynomial in $M$. A priori , this polynomial might be expected to have algebraic numbers as coefficients, but the examples below suggest that the situation may be nicer than that. First, when $n=2$ we find by inspection that  $L = M - I.$ Next, for $n=3$, a calculation shows that $L = M^2 - 5M + 2I$. And yet again for $n=4$, we  find $L = 2M^3 - 19M^2 + 21M - 5I.$ In each case (so far) the polynomial turned out to have integral coefficients. Questions: (1) Is it always true, for arbitrary $n$ , that $L$ can be expressed as a polynomial in $M$ with integral coefficients? (2) Presumably, matrices like $M$ above which have an integral square root are the exception rather than the rule.  Is it easy to show that almost all ( in any suitable sense) matrices in $\text{SL}(n,Z)$ do not possess a square root in $\text{GL}(n,Z)$ whenever $n \geq 2$ ?","Define $n\times n$ matrices $L , M$  as follows: $\,$ L$_{ij}$= $1$ if $\,$i + j $\geq$n + 1 and is zero otherwise; $\,$ $\qquad$ $\qquad$ $\quad$M$_{ij}$= $\min(i,j)$ for $1 \leq i , j \leq n$ .  It is straightforward to check that $M = L^2$ .  For example, when $n=3$ we have $\qquad$ $\qquad$ $\qquad$$\begin{bmatrix}1 & 1 & 1\\1 & 2 & 2\\1 & 2 & 3\\ \end{bmatrix}$ =  $\begin{bmatrix}0 & 0 & 1\\0 & 1 & 1\\1 & 1 & 1\\ \end{bmatrix}^2$ . [Remarks: $\det(M) = 1$ ; $M$ is positive definite but $L$ is not  ; the eigenvalues of $M$ are distinct ( to see this, it's easier to work with $M^{-1}$ which is tridiagonal ). ] General theory then predicts that $L$ can be written as a polynomial in $M$. A priori , this polynomial might be expected to have algebraic numbers as coefficients, but the examples below suggest that the situation may be nicer than that. First, when $n=2$ we find by inspection that  $L = M - I.$ Next, for $n=3$, a calculation shows that $L = M^2 - 5M + 2I$. And yet again for $n=4$, we  find $L = 2M^3 - 19M^2 + 21M - 5I.$ In each case (so far) the polynomial turned out to have integral coefficients. Questions: (1) Is it always true, for arbitrary $n$ , that $L$ can be expressed as a polynomial in $M$ with integral coefficients? (2) Presumably, matrices like $M$ above which have an integral square root are the exception rather than the rule.  Is it easy to show that almost all ( in any suitable sense) matrices in $\text{SL}(n,Z)$ do not possess a square root in $\text{GL}(n,Z)$ whenever $n \geq 2$ ?",,"['matrices', 'number-theory']"
61,Why is the inverse matrix of $A^TA$ is guaranteed to exists?,Why is the inverse matrix of  is guaranteed to exists?,A^TA,"For a matrix $A$ of an arbitrary size $n{\times}m$ where $n>m$ and $rank\left(A\right)=m$, there is no guarantee that the inverse matrix $A^{-1}$ will exist. But for the multiplication of the matrix with its transpose $A^T{\cdot}A$ the inverse $\left(A^T{\cdot}A\right)^{-1}$ is guaranteed to exist. Why is it so?","For a matrix $A$ of an arbitrary size $n{\times}m$ where $n>m$ and $rank\left(A\right)=m$, there is no guarantee that the inverse matrix $A^{-1}$ will exist. But for the multiplication of the matrix with its transpose $A^T{\cdot}A$ the inverse $\left(A^T{\cdot}A\right)^{-1}$ is guaranteed to exist. Why is it so?",,['matrices']
62,"Tangent Space of SL(n,R) at arbitrary point, e.g. not at $\mathbb{1}$","Tangent Space of SL(n,R) at arbitrary point, e.g. not at",\mathbb{1},"I am looking for the tangent space of $SL(n,\mathbb{R}) = \{A\in\mathbb{R}^{n\times n} | \det{A}=1 \}$ (actually $n=3$) at an arbitrary $M\in SL(n,\mathbb{R})$. From now on I will omit the $\mathbb{R}$ for convenience. It is well known and easy to prove that the tangent space at the identity matrix $\mathbb{1}$, $T_1SL(n)$, is the set of all traceless matrices of the same size. However I am not able to generalize this result. These are my thoughts: Let $\gamma:\mathbb{R}\to SL(n)$ be a curve with known $\gamma(0) = M$ and unknown $\gamma^\prime(0)=X$. Let's differentiate the $SL$-characteristic equation: $$\frac{d}{dt}|_{t=0}\det{\gamma}(t) = D(\det)|_{\gamma(0)}\,\cdot\,\gamma^\prime(0) = \frac{d}{dt}1 = 0$$ Here $A\cdot B = A_{ij}B_{ij}$. The trick for $M=\mathbb{1}$ is $$D(\det)|_1 = \mathbb{1},$$ so the above differentiation equation reads ${\rm tr}(\gamma^\prime(0))=0$. However in the general case $$D(\det)|_M = {\rm Cof}(M)$$ the cofactor matrix. As far as I see ${\rm Cof}(M) \cdot X = 0$ is of no use. Any ideas out there? Thanks.","I am looking for the tangent space of $SL(n,\mathbb{R}) = \{A\in\mathbb{R}^{n\times n} | \det{A}=1 \}$ (actually $n=3$) at an arbitrary $M\in SL(n,\mathbb{R})$. From now on I will omit the $\mathbb{R}$ for convenience. It is well known and easy to prove that the tangent space at the identity matrix $\mathbb{1}$, $T_1SL(n)$, is the set of all traceless matrices of the same size. However I am not able to generalize this result. These are my thoughts: Let $\gamma:\mathbb{R}\to SL(n)$ be a curve with known $\gamma(0) = M$ and unknown $\gamma^\prime(0)=X$. Let's differentiate the $SL$-characteristic equation: $$\frac{d}{dt}|_{t=0}\det{\gamma}(t) = D(\det)|_{\gamma(0)}\,\cdot\,\gamma^\prime(0) = \frac{d}{dt}1 = 0$$ Here $A\cdot B = A_{ij}B_{ij}$. The trick for $M=\mathbb{1}$ is $$D(\det)|_1 = \mathbb{1},$$ so the above differentiation equation reads ${\rm tr}(\gamma^\prime(0))=0$. However in the general case $$D(\det)|_M = {\rm Cof}(M)$$ the cofactor matrix. As far as I see ${\rm Cof}(M) \cdot X = 0$ is of no use. Any ideas out there? Thanks.",,"['matrices', 'differential-geometry', 'lie-groups', 'matrix-equations', 'matrix-calculus']"
63,How the quotient group of following matrix group looks like?,How the quotient group of following matrix group looks like?,,"Consider the following subsets of the group of $2\times 2$ non-singular matrices over $\mathbb{R}$:    $G=\left\{ \begin{bmatrix}a&b\\ 0&d\end{bmatrix}: a,b,d\in \mathbb{R}, ad=1\right\}$ $H=\left\{ \begin{bmatrix}1&b\\ 0&1\end{bmatrix}: b\in \mathbb{R}\right\}$ Which of the following statements are correct? $G$ forms a group under matrix multiplication. $H$ is a normal subgroup of $G$. The quotient group $G/H$ is well defined and is abelian. The quotient group $G/H$ is well defined and is isomorphic to the group of 2x2 diagonal matrices (over reals) with determinant 1. So I checked that option 1 is true. Also for all $g\in G$, $g^{-1}Hg=H$ so $H$ is a normal subgroup of $G$. Option 2 is also correct. But I am stuck with option 3 and 4. Since $H$ is normal, we can talk about the quotient group $G/H$, but I don't know how this quotient group is look like ? and also the well define part. Also I need help with the 4th option. How can I do this? Any help would be great. thanks.","Consider the following subsets of the group of $2\times 2$ non-singular matrices over $\mathbb{R}$:    $G=\left\{ \begin{bmatrix}a&b\\ 0&d\end{bmatrix}: a,b,d\in \mathbb{R}, ad=1\right\}$ $H=\left\{ \begin{bmatrix}1&b\\ 0&1\end{bmatrix}: b\in \mathbb{R}\right\}$ Which of the following statements are correct? $G$ forms a group under matrix multiplication. $H$ is a normal subgroup of $G$. The quotient group $G/H$ is well defined and is abelian. The quotient group $G/H$ is well defined and is isomorphic to the group of 2x2 diagonal matrices (over reals) with determinant 1. So I checked that option 1 is true. Also for all $g\in G$, $g^{-1}Hg=H$ so $H$ is a normal subgroup of $G$. Option 2 is also correct. But I am stuck with option 3 and 4. Since $H$ is normal, we can talk about the quotient group $G/H$, but I don't know how this quotient group is look like ? and also the well define part. Also I need help with the 4th option. How can I do this? Any help would be great. thanks.",,"['matrices', 'group-theory', 'normal-subgroups', 'quotient-group']"
64,What is the bandwidth of a matrix?,What is the bandwidth of a matrix?,,"I was solving a problem which involved finding the bandwidth of a matrix. I interpreted the bandwidth as a non-negative number which is closest to the diagonal. And this interpretation of mine does work on some examples. However, the condition is: Bandwidth of a matrix A is defined as the smallest non-negative integer K such that A(i, j) = 0 for |i - j| > K. Now, I am confused as the condition says that a[i,j] should be 0 and |i-j| of the same should be > K. I am not able to figure out why a[i,j] should be 0. Please help me clearing what K is and what does this condition evaluate to.","I was solving a problem which involved finding the bandwidth of a matrix. I interpreted the bandwidth as a non-negative number which is closest to the diagonal. And this interpretation of mine does work on some examples. However, the condition is: Bandwidth of a matrix A is defined as the smallest non-negative integer K such that A(i, j) = 0 for |i - j| > K. Now, I am confused as the condition says that a[i,j] should be 0 and |i-j| of the same should be > K. I am not able to figure out why a[i,j] should be 0. Please help me clearing what K is and what does this condition evaluate to.",,['matrices']
65,About a refined Burnside's theorem in prime characteristic.,About a refined Burnside's theorem in prime characteristic.,,"The following result is due to W. Burnside: Theorem. Let $G$ be a subgroup of $\textrm{GL}_n(\mathbb{C})$. If $G$ has finite exponent, then $G$ is finite. The proof relies on the following: Lemma. Let $A\in\mathcal{M}_n(\mathbb{C})$ such that for all $k\in\{1,\ldots,n\}$, $\textrm{tr}(A^k)=0$, then $A$ is nilpotent. It is not hard to see that the theorem still holds over fields of characteristic zero. Indeed, to establish the lemma it suffices to consider a splitting field of the characteristic polynomial of $A$. However, the theorem fails to be true over infinite field of prime characteristic; consider the following subgroup of $\textrm{GL}_2(\mathbb{F}_p(t))$: $$G:=\left\{\begin{pmatrix}1&f\\0&1\end{pmatrix};f\in\mathbb{F}_p(t)\right\}.$$ Notice that $G$ is infinite albeit having exponent $p$. My conjecture is that the following refinement is true: Conjecture. Let $k$ be an infinite field of prime characteristic $p$ and let $G$ be a subgroup of $\textrm{GL}_n(k)$. If the exponent of $G$ is finite and prime with $p$, then $G$ is finite. I already proved the conjecture for $n<p$, in that precise case it is not hard to see that the lemma still holds. However, for $n\geqslant p$, the lemma is false. For example, consider $p=2$, $n=2$ and $A=I_2$. Any enlightenment and/or references will be greatly appreciated!","The following result is due to W. Burnside: Theorem. Let $G$ be a subgroup of $\textrm{GL}_n(\mathbb{C})$. If $G$ has finite exponent, then $G$ is finite. The proof relies on the following: Lemma. Let $A\in\mathcal{M}_n(\mathbb{C})$ such that for all $k\in\{1,\ldots,n\}$, $\textrm{tr}(A^k)=0$, then $A$ is nilpotent. It is not hard to see that the theorem still holds over fields of characteristic zero. Indeed, to establish the lemma it suffices to consider a splitting field of the characteristic polynomial of $A$. However, the theorem fails to be true over infinite field of prime characteristic; consider the following subgroup of $\textrm{GL}_2(\mathbb{F}_p(t))$: $$G:=\left\{\begin{pmatrix}1&f\\0&1\end{pmatrix};f\in\mathbb{F}_p(t)\right\}.$$ Notice that $G$ is infinite albeit having exponent $p$. My conjecture is that the following refinement is true: Conjecture. Let $k$ be an infinite field of prime characteristic $p$ and let $G$ be a subgroup of $\textrm{GL}_n(k)$. If the exponent of $G$ is finite and prime with $p$, then $G$ is finite. I already proved the conjecture for $n<p$, in that precise case it is not hard to see that the lemma still holds. However, for $n\geqslant p$, the lemma is false. For example, consider $p=2$, $n=2$ and $A=I_2$. Any enlightenment and/or references will be greatly appreciated!",,"['matrices', 'group-theory', 'positive-characteristic']"
66,Eigenvectors of a tridiagonal stochastic matrix,Eigenvectors of a tridiagonal stochastic matrix,,"I'm looking for the eigenvectors of this matrix: \begin{equation} \nonumber M = \frac{1}{N} \left( \begin{array}{ccccccccc}  0 & 1 &&&&&&&\\  N &  0 & 2&&&&&&& \\ &N-1 &  0 & 3&&&&&& \\ &&N-2 &  0& \ldots&&&&& \\ &&&\vdots & \ddots &&&&& \\ &&&&& 0 & N-2 && \\ &&&&&3 & 0 &N-1 & \\ &&&&&&2 & 0&N  \\ &&&&&&& 1 &0 \\ \end{array} \right)  \end{equation} where only nonzero entries are shown. This matrix has size $(N+1) \times (N+1)$. Its superdiagonal entries grow linearly from $1$ to $N$, and the subdiagonal entries decrease linearly from $N$ to $1$.","I'm looking for the eigenvectors of this matrix: \begin{equation} \nonumber M = \frac{1}{N} \left( \begin{array}{ccccccccc}  0 & 1 &&&&&&&\\  N &  0 & 2&&&&&&& \\ &N-1 &  0 & 3&&&&&& \\ &&N-2 &  0& \ldots&&&&& \\ &&&\vdots & \ddots &&&&& \\ &&&&& 0 & N-2 && \\ &&&&&3 & 0 &N-1 & \\ &&&&&&2 & 0&N  \\ &&&&&&& 1 &0 \\ \end{array} \right)  \end{equation} where only nonzero entries are shown. This matrix has size $(N+1) \times (N+1)$. Its superdiagonal entries grow linearly from $1$ to $N$, and the subdiagonal entries decrease linearly from $N$ to $1$.",,"['matrices', 'eigenvalues-eigenvectors', 'tridiagonal-matrices']"
67,Farkas Lemma Question with strict inequality,Farkas Lemma Question with strict inequality,,"I have a question which I thought that can be solved by Farkas Lemma, but I could not manage it. Prove that only one of the systems has a feasible solution, where $A$ is an $m \times n$ matrix, $C$ is a $r \times n$ matrix: System 1. \begin{align*} A\mathbf{x} &\leq \mathbf{b} \\ C\mathbf{x} & > \mathbf{d} \\ \mathbf{x} &\geq 0 \end{align*} System 2. \begin{align*} \mathbf{y}^T A &\geq \mathbf{s}^T C \\ \mathbf{y}^T \mathbf{b} &= \mathbf{s}^T\mathbf{d} \\ \sum_{k=1}^r s_k &= 1  \\  \mathbf{y}, \mathbf{s} &\geq 0 \end{align*}","I have a question which I thought that can be solved by Farkas Lemma, but I could not manage it. Prove that only one of the systems has a feasible solution, where $A$ is an $m \times n$ matrix, $C$ is a $r \times n$ matrix: System 1. \begin{align*} A\mathbf{x} &\leq \mathbf{b} \\ C\mathbf{x} & > \mathbf{d} \\ \mathbf{x} &\geq 0 \end{align*} System 2. \begin{align*} \mathbf{y}^T A &\geq \mathbf{s}^T C \\ \mathbf{y}^T \mathbf{b} &= \mathbf{s}^T\mathbf{d} \\ \sum_{k=1}^r s_k &= 1  \\  \mathbf{y}, \mathbf{s} &\geq 0 \end{align*}",,"['matrices', 'linear-programming', 'proof-explanation']"
68,Showing two norms on $\mathbb{R}^n$ are dual,Showing two norms on  are dual,\mathbb{R}^n,"I am having trouble showing the following result. If $A$ is a positive definite matrix, then the norms (on $\mathbb{R}^n$) $\|x\|_A:= \sqrt{x^\top A x}$ and $\|y\|_{A^{-1}}:= \sqrt{y^\top A^{-1} y}$ are dual. [Here, the dual $\|\cdot\|^*$ of a norm $\|\cdot \|$ on $\mathbb{R}^n$ is defined to be $\|y\|^* := \sup_{x \in \mathbb{R}^n} \frac{y^\top x}{\|x\|}$.] For $A$ being the identity matrix, the result follows from the Cauchy-Schwarz inequality: $y^\top x \le \|y\| \|x\|$ with equality attained when $x=y$, so $$\|y\|^* = \sup_{x \in \mathbb{R}^n} \frac{y^\top x}{\|x\|} = \|y\|.$$ However I am having trouble with general $A$. I tried using Cauchy-Schwarz on the inner product $\langle x,y \rangle = x^\top A y$, but got stuck. Any hints would be appreciated! So I have some progress: going backwards from the stated result, I've shown that $$\frac{y^\top y}{\|y\|_A} = \frac{y^\top y}{\sqrt{y^\top A y}} = \sqrt{y^\top A^{-1} y} = \|y\|_{A^{-1}}$$ (I omitted my work), so that assuming $\|\cdot\|_{A^{-1}}$ is indeed the dual of $\|\cdot\|_A$, an $x$ that maximizes $\sup_{x \in \mathbb{R}^n} \frac{y^\top x}{\|x\|_A}$ is $x=y$. However, I am still stuck on showing that $x=y$ indeed maximizes $\frac{y^\top x}{\|x\|_A}$. Cauchy-Swcharz (for the usual Euclidean norm) only gives $\frac{y^\top x}{\|x\|_A} \le \frac{\|y\|_2 \|x\|_2}{\|x\|_A}$ with equality if and only if $x$ and $y$ differ by a scalar, but the $\|x\|_A$ will mess things up.","I am having trouble showing the following result. If $A$ is a positive definite matrix, then the norms (on $\mathbb{R}^n$) $\|x\|_A:= \sqrt{x^\top A x}$ and $\|y\|_{A^{-1}}:= \sqrt{y^\top A^{-1} y}$ are dual. [Here, the dual $\|\cdot\|^*$ of a norm $\|\cdot \|$ on $\mathbb{R}^n$ is defined to be $\|y\|^* := \sup_{x \in \mathbb{R}^n} \frac{y^\top x}{\|x\|}$.] For $A$ being the identity matrix, the result follows from the Cauchy-Schwarz inequality: $y^\top x \le \|y\| \|x\|$ with equality attained when $x=y$, so $$\|y\|^* = \sup_{x \in \mathbb{R}^n} \frac{y^\top x}{\|x\|} = \|y\|.$$ However I am having trouble with general $A$. I tried using Cauchy-Schwarz on the inner product $\langle x,y \rangle = x^\top A y$, but got stuck. Any hints would be appreciated! So I have some progress: going backwards from the stated result, I've shown that $$\frac{y^\top y}{\|y\|_A} = \frac{y^\top y}{\sqrt{y^\top A y}} = \sqrt{y^\top A^{-1} y} = \|y\|_{A^{-1}}$$ (I omitted my work), so that assuming $\|\cdot\|_{A^{-1}}$ is indeed the dual of $\|\cdot\|_A$, an $x$ that maximizes $\sup_{x \in \mathbb{R}^n} \frac{y^\top x}{\|x\|_A}$ is $x=y$. However, I am still stuck on showing that $x=y$ indeed maximizes $\frac{y^\top x}{\|x\|_A}$. Cauchy-Swcharz (for the usual Euclidean norm) only gives $\frac{y^\top x}{\|x\|_A} \le \frac{\|y\|_2 \|x\|_2}{\|x\|_A}$ with equality if and only if $x$ and $y$ differ by a scalar, but the $\|x\|_A$ will mess things up.",,"['matrices', 'normed-spaces', 'inner-products', 'duality-theorems']"
69,Evalute big determinant,Evalute big determinant,,"Today in exam I tried to evaluate this determinant but failed, only somehow ""guessed"" the answer I got here. Now in home I've managed to find something intuitive, just want to know whether the approach is correct, and is there more faster way exist. Given determinant $$\det\begin{vmatrix} 1 & 2 & 3 & ... & n-2 & n-1 & n\\  2 & 3 & 4 & ... & n-1 & n & n\\  3 & 4 & 5 & ... & n & n & n \\  . & . & . & . & . & . &. \\ n & n & n & ... & n & n & n \end{vmatrix}$$ First thing I did, was rearranging rows. I remember from another problem, where I used to evaluate determinant of matrix of this kind $\det\begin{vmatrix} 0 & 0 ... & 0 & 1\\  0 & 0 ... & 1 & 0\\  0 & 0 ... & 0 & 0\\  . & . & . & . \\ 1 & 0 ... & 0 & 0 \end{vmatrix}$ is $(-1)^{\frac{n(n-1)}{2}}*\det\begin{vmatrix} 1 & 0 ... & 0 & 0\\  0 & 1 ... & 0 & 0\\  0 & 0 ... & 0 & 0\\  . & . & . & . \\ 0 & 0 ... & 0 & 1 \end{vmatrix}=(-1)^{\frac{n(n-1)}{2}}$. So it becomes $$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix} n & n & n & ... & n & n & n \\ . & . & . & . & . & . &. \\ 3 & 4 & 5 & ... & n & n & n \\  2 & 3 & 4 & ... & n-1 & n & n\\  1 & 2 & 3 & ... & n-2 & n-1 & n\\  \end{vmatrix}$$ And then I transposed it $$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix} n & n-1 & n-2 & ... & 3 & 2 & 1 \\ . & . & . & . & . & . &. \\ n & n & n & ... & n & n-1 & n-2 \\  n & n & n & ... & n & n & n-1\\  n & n & n & ... & n & n & n\\  \end{vmatrix}$$ and tried to subtract first row from all. $$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix} n & n-1 & n-2 & ... & 3 & 2 & 1 \\ 0 & 1 & 1 & ... & 1 & 1 & 1 \\  0 & 1 & 2 & ... & 2 & 2 & 2\\  . & . & . & . & . & . &. \\ 0 & 1 & 2 & ... & n-3 & n-2 & n-1\\  \end{vmatrix}$$ next step is subtracting second row from others below. $$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix} n & n-1 & n-2 & ... & 3 & 2 & 1 \\ 0 & 1 & 1 & ... & 1 & 1 & 1 \\  0 & 0 & 1 & ... & 1 & 1 & 1\\  . & . & . & . & . & . &. \\ 0 & 0 & 1 & ... & n-4 & n-3 & n-2\\  \end{vmatrix}$$ doing this for finite n we'll get $$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix} n & n-1 & n-2 & ... & 3 & 2 & 1 \\ 0 & 1 & 1 & ... & 1 & 1 & 1 \\  0 & 0 & 1 & ... & 1 & 1 & 1\\  . & . & . & . & . & . &. \\ 0 & 0 & 0 & ... & 0 & 0 & 1\\  \end{vmatrix}$$ So my final solution is $n*(-1)^{\frac{n(n-1)}{2}}$. Do you see any mistakes? And maybe there's more easier approach? thanks.","Today in exam I tried to evaluate this determinant but failed, only somehow ""guessed"" the answer I got here. Now in home I've managed to find something intuitive, just want to know whether the approach is correct, and is there more faster way exist. Given determinant $$\det\begin{vmatrix} 1 & 2 & 3 & ... & n-2 & n-1 & n\\  2 & 3 & 4 & ... & n-1 & n & n\\  3 & 4 & 5 & ... & n & n & n \\  . & . & . & . & . & . &. \\ n & n & n & ... & n & n & n \end{vmatrix}$$ First thing I did, was rearranging rows. I remember from another problem, where I used to evaluate determinant of matrix of this kind $\det\begin{vmatrix} 0 & 0 ... & 0 & 1\\  0 & 0 ... & 1 & 0\\  0 & 0 ... & 0 & 0\\  . & . & . & . \\ 1 & 0 ... & 0 & 0 \end{vmatrix}$ is $(-1)^{\frac{n(n-1)}{2}}*\det\begin{vmatrix} 1 & 0 ... & 0 & 0\\  0 & 1 ... & 0 & 0\\  0 & 0 ... & 0 & 0\\  . & . & . & . \\ 0 & 0 ... & 0 & 1 \end{vmatrix}=(-1)^{\frac{n(n-1)}{2}}$. So it becomes $$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix} n & n & n & ... & n & n & n \\ . & . & . & . & . & . &. \\ 3 & 4 & 5 & ... & n & n & n \\  2 & 3 & 4 & ... & n-1 & n & n\\  1 & 2 & 3 & ... & n-2 & n-1 & n\\  \end{vmatrix}$$ And then I transposed it $$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix} n & n-1 & n-2 & ... & 3 & 2 & 1 \\ . & . & . & . & . & . &. \\ n & n & n & ... & n & n-1 & n-2 \\  n & n & n & ... & n & n & n-1\\  n & n & n & ... & n & n & n\\  \end{vmatrix}$$ and tried to subtract first row from all. $$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix} n & n-1 & n-2 & ... & 3 & 2 & 1 \\ 0 & 1 & 1 & ... & 1 & 1 & 1 \\  0 & 1 & 2 & ... & 2 & 2 & 2\\  . & . & . & . & . & . &. \\ 0 & 1 & 2 & ... & n-3 & n-2 & n-1\\  \end{vmatrix}$$ next step is subtracting second row from others below. $$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix} n & n-1 & n-2 & ... & 3 & 2 & 1 \\ 0 & 1 & 1 & ... & 1 & 1 & 1 \\  0 & 0 & 1 & ... & 1 & 1 & 1\\  . & . & . & . & . & . &. \\ 0 & 0 & 1 & ... & n-4 & n-3 & n-2\\  \end{vmatrix}$$ doing this for finite n we'll get $$(-1)^{\frac{n(n-1)}{2}}\det\begin{vmatrix} n & n-1 & n-2 & ... & 3 & 2 & 1 \\ 0 & 1 & 1 & ... & 1 & 1 & 1 \\  0 & 0 & 1 & ... & 1 & 1 & 1\\  . & . & . & . & . & . &. \\ 0 & 0 & 0 & ... & 0 & 0 & 1\\  \end{vmatrix}$$ So my final solution is $n*(-1)^{\frac{n(n-1)}{2}}$. Do you see any mistakes? And maybe there's more easier approach? thanks.",,"['matrices', 'determinant']"
70,Is there any geometrical interpretation as to why matrix product is not commutative?,Is there any geometrical interpretation as to why matrix product is not commutative?,,"Is there any geometrical interpretation as to why matrix product is not commutative? Similarly, is there any geometrical interpretation of matrix product when you have  matrices $A$, $B$ such that $AB=BA$?","Is there any geometrical interpretation as to why matrix product is not commutative? Similarly, is there any geometrical interpretation of matrix product when you have  matrices $A$, $B$ such that $AB=BA$?",,"['matrices', 'geometry']"
71,Calculation of determinant of an arrowhead matrix,Calculation of determinant of an arrowhead matrix,,"Is there any easier way to make sure the determinant of the following $n \times n$ matrix is $n$ ? $$\begin{vmatrix}   1 & -1 & -1 & -1 & \cdots & -1 \\   1 &  1 &  0 &  0 & \cdots &  0 \\   1 &  0 &  1 &  0 & \cdots &  0 \\   1 &  0 &  0 &  1 & \cdots &  0 \\   \vdots & \vdots &  \vdots & \vdots  & \ddots &  \vdots \\   1 & 0 & 0 & 0 &\cdots  & 1  \end{vmatrix} = n$$ I figured it with a smaller dimension and it indeed produces the determinant that is the size of dimension. I tried to do a cofactor expansion with the first row, and each term produces the determinant of $1$ and if you sum them up, then the total determinant will be $n$ . But the sign change for each cofactor is confusing, and it is not easily seen that each cofactor term is actually positive $1$ .","Is there any easier way to make sure the determinant of the following matrix is ? I figured it with a smaller dimension and it indeed produces the determinant that is the size of dimension. I tried to do a cofactor expansion with the first row, and each term produces the determinant of and if you sum them up, then the total determinant will be . But the sign change for each cofactor is confusing, and it is not easily seen that each cofactor term is actually positive .","n \times n n \begin{vmatrix}
  1 & -1 & -1 & -1 & \cdots & -1 \\
  1 &  1 &  0 &  0 & \cdots &  0 \\
  1 &  0 &  1 &  0 & \cdots &  0 \\
  1 &  0 &  0 &  1 & \cdots &  0 \\
  \vdots & \vdots &  \vdots & \vdots  & \ddots &  \vdots \\
  1 & 0 & 0 & 0 &\cdots  & 1
 \end{vmatrix} = n 1 n 1","['matrices', 'determinant']"
72,Is this matrix positive-semidefinite in general?,Is this matrix positive-semidefinite in general?,,"for the matrix written below I was wondering if one can show that it is positive-semidefinite for $n>3$ and $0< \alpha<1$. (Or not. For $n=2, 3$ it works by showing that all principal minors are non-negative.) $$ C_{n,n} =  \begin{pmatrix}   1 & \alpha^1& \alpha^2 & \cdots & \alpha^{n-1} \\   \alpha^1 & 1 &  \alpha^1&\cdots & \alpha^{n-2} \\   \vdots  & \vdots & \vdots   & \ddots & \vdots  \\    \alpha^{n-1} & \alpha^{n-2} & \alpha^{n-3}& \cdots & 1 \end{pmatrix}  =\begin{pmatrix}   \alpha^0 & \alpha^1& \alpha^2 & \cdots & \alpha^{n-1} \\   \alpha^1 & \alpha^0 &  \alpha^1&\cdots & \alpha^{n-2} \\   \vdots  & \vdots & \vdots   & \ddots & \vdots  \\    \alpha^{n-1} & \alpha^{n-2} & \alpha^{n-3}& \cdots & \alpha^0  \end{pmatrix} $$","for the matrix written below I was wondering if one can show that it is positive-semidefinite for $n>3$ and $0< \alpha<1$. (Or not. For $n=2, 3$ it works by showing that all principal minors are non-negative.) $$ C_{n,n} =  \begin{pmatrix}   1 & \alpha^1& \alpha^2 & \cdots & \alpha^{n-1} \\   \alpha^1 & 1 &  \alpha^1&\cdots & \alpha^{n-2} \\   \vdots  & \vdots & \vdots   & \ddots & \vdots  \\    \alpha^{n-1} & \alpha^{n-2} & \alpha^{n-3}& \cdots & 1 \end{pmatrix}  =\begin{pmatrix}   \alpha^0 & \alpha^1& \alpha^2 & \cdots & \alpha^{n-1} \\   \alpha^1 & \alpha^0 &  \alpha^1&\cdots & \alpha^{n-2} \\   \vdots  & \vdots & \vdots   & \ddots & \vdots  \\    \alpha^{n-1} & \alpha^{n-2} & \alpha^{n-3}& \cdots & \alpha^0  \end{pmatrix} $$",,"['matrices', 'laplace-expansion']"
73,Determinant of a $2 \times 2$ block matrix,Determinant of a  block matrix,2 \times 2,"$\textbf{Problem}$: Let a $2n \times 2n$ matrix be given in the form $M=\left[ {\begin{array}{cc}               A & B \\              C & D \\                 \end{array} } \right]$, where each block is an $n \times n$ matrix. Suppose that $A$ is invertible and that $AC=CA$. Use block multiplication to prove that $\det M= \det(AD-CB)$. Give an example to show that this formula need not hold if $AC \neq CA$ $\textbf{Proof}$: Let $A,B,C,D,X \in \textbf{M}_n(K)$ such that $A+BX$ is invertible. For all $Y \in \textbf{M}_n(K)$, we have: $$\left[ {\begin{array}{cc}              I_n & 0 \\              Y & I_n \\                 \end{array} } \right] \left[ {\begin{array}{cc}              A & B \\              C & D \\                 \end{array} } \right] \left[ {\begin{array}{cc}              I_n & 0 \\              X & I_n \\                 \end{array} } \right]= \left[ {\begin{array}{cc}              A+BX & B \\              YA+C+(YB+D)X & YB+D \\                 \end{array} } \right].$$ Let $Y=-(C+DX)(A+BX)^{-1}$. Hence: $$YA+C+(YB+D)X=Y(A+BX)+(C+DX)=0.$$ Since $\det\left[ {\begin{array}{cc}              I_n & 0 \\              Y & I_n \\                 \end{array} } \right]= \det\left[ {\begin{array}{cc}              I_n & 0 \\              X & I_n \\                 \end{array} } \right]= (\det(I_n))^2=1$, we can conclude that: \begin{align*} \det\left[ {\begin{array}{cc}              A & B \\              C & D \\                 \end{array} } \right]&=\det\left[ {\begin{array}{cc}              A+BX & B \\              0 & YB+D \\                 \end{array} } \right]\\ &= \det(A+BX)\det(-(C+DX)(A+BX)^{-1}B+D). \end{align*} In particular for $X=0$, we have: \begin{align*} \det\left[ {\begin{array}{cc}              A & B \\              C & D \\                 \end{array} } \right]&=\det(A)\det(-CA^{-1}B+D)=\det(-ACA^{-1}B+AD) \\ &=\det(-CAA^{-1}B+AD)=\det(AD-CB). \end{align*} I just wanted someone to verify my proof and help me with the second part of this question. Thank you in advance","$\textbf{Problem}$: Let a $2n \times 2n$ matrix be given in the form $M=\left[ {\begin{array}{cc}               A & B \\              C & D \\                 \end{array} } \right]$, where each block is an $n \times n$ matrix. Suppose that $A$ is invertible and that $AC=CA$. Use block multiplication to prove that $\det M= \det(AD-CB)$. Give an example to show that this formula need not hold if $AC \neq CA$ $\textbf{Proof}$: Let $A,B,C,D,X \in \textbf{M}_n(K)$ such that $A+BX$ is invertible. For all $Y \in \textbf{M}_n(K)$, we have: $$\left[ {\begin{array}{cc}              I_n & 0 \\              Y & I_n \\                 \end{array} } \right] \left[ {\begin{array}{cc}              A & B \\              C & D \\                 \end{array} } \right] \left[ {\begin{array}{cc}              I_n & 0 \\              X & I_n \\                 \end{array} } \right]= \left[ {\begin{array}{cc}              A+BX & B \\              YA+C+(YB+D)X & YB+D \\                 \end{array} } \right].$$ Let $Y=-(C+DX)(A+BX)^{-1}$. Hence: $$YA+C+(YB+D)X=Y(A+BX)+(C+DX)=0.$$ Since $\det\left[ {\begin{array}{cc}              I_n & 0 \\              Y & I_n \\                 \end{array} } \right]= \det\left[ {\begin{array}{cc}              I_n & 0 \\              X & I_n \\                 \end{array} } \right]= (\det(I_n))^2=1$, we can conclude that: \begin{align*} \det\left[ {\begin{array}{cc}              A & B \\              C & D \\                 \end{array} } \right]&=\det\left[ {\begin{array}{cc}              A+BX & B \\              0 & YB+D \\                 \end{array} } \right]\\ &= \det(A+BX)\det(-(C+DX)(A+BX)^{-1}B+D). \end{align*} In particular for $X=0$, we have: \begin{align*} \det\left[ {\begin{array}{cc}              A & B \\              C & D \\                 \end{array} } \right]&=\det(A)\det(-CA^{-1}B+D)=\det(-ACA^{-1}B+AD) \\ &=\det(-CAA^{-1}B+AD)=\det(AD-CB). \end{align*} I just wanted someone to verify my proof and help me with the second part of this question. Thank you in advance",,"['matrices', 'solution-verification', 'block-matrices']"
74,Centralizers of Matrices,Centralizers of Matrices,,"let $A$ be a complex matrix. Denote by $J(A)$ the Jordan Canonical Form of $A$. Let $C[J(A)]$ be the centralizer of $J(A)$ in $M_n(\mathbb C)$. Can we construct a real matrix $B$, that is, $B$ has only real entries,  verifying the equality $C[J(A)]=C(B)$, in $M_n(\mathbb C)$?","let $A$ be a complex matrix. Denote by $J(A)$ the Jordan Canonical Form of $A$. Let $C[J(A)]$ be the centralizer of $J(A)$ in $M_n(\mathbb C)$. Can we construct a real matrix $B$, that is, $B$ has only real entries,  verifying the equality $C[J(A)]=C(B)$, in $M_n(\mathbb C)$?",,['matrices']
75,What is the last index of a third-order tensor called?,What is the last index of a third-order tensor called?,,In a third-order tensor I guess the first and second index would be called row and column respectively but is there a name for the third index?,In a third-order tensor I guess the first and second index would be called row and column respectively but is there a name for the third index?,,"['matrices', 'terminology', 'tensors']"
76,How to convert quadratic programming problem to matrix form,How to convert quadratic programming problem to matrix form,,"I am new to this topic and am looking at an example I can't figure out.  Can someone please help explain how this example creates the matrices used in the solver?  Thanks! This is the PROBLEM Minimize $2x_1^2 + x_2^2 + x_1x_2 + x_1 + x_2,\quad$ subject to: $x_1 \geq 0$ $x_2 \geq 0$ $x_1 + x_2 = 1$ ANSWER: Matrix Form for solver: Q = [4.0, 1.0] [1.0, 2.0] p = [1.0] [1.0] G = [-1.0, 0.0] [0.0, -1.0] h = [0.0] [0.0] A = [1.0, 1.0] b = [1.0] result = solve.qp(Q, p, G, h, A, b)","I am new to this topic and am looking at an example I can't figure out.  Can someone please help explain how this example creates the matrices used in the solver?  Thanks! This is the PROBLEM Minimize subject to: ANSWER: Matrix Form for solver: Q = [4.0, 1.0] [1.0, 2.0] p = [1.0] [1.0] G = [-1.0, 0.0] [0.0, -1.0] h = [0.0] [0.0] A = [1.0, 1.0] b = [1.0] result = solve.qp(Q, p, G, h, A, b)","2x_1^2 + x_2^2 + x_1x_2 + x_1 + x_2,\quad x_1 \geq 0 x_2 \geq 0 x_1 + x_2 = 1","['matrices', 'optimization', 'quadratic-programming']"
77,Convert from fixed axis $XYZ$ rotations to Euler $ZXZ$ rotations,Convert from fixed axis  rotations to Euler  rotations,XYZ ZXZ,"Because I'm a new user, I can't post images or hyperlinks, there is a complete version with images here: https://stackoverflow.com/questions/9774958/converting-from-a-euler-zxz-rotation-to-fixed-axis-xyz-rotations The problem I have, is that I need to convert from $XYZ$ fixed axis rotations, to Euler rotations about $Z$, then $X'$, then $Z''$. Here are the relevant matricies: $$X = \left(\begin{matrix} 1 & 0 & 0 \\ 0 & \cos(\theta) & -\sin(\theta) \\ 0 & \sin(\theta) & \cos(\theta)\end{matrix}\right)$$ $$Y = \left(\begin{matrix} \cos(\phi) & 0 & \sin(\phi) \\ 0 & 1 & 0 \\ -\sin(\phi) & 0 & \cos(\phi)\end{matrix}\right)$$ $$Z = \left(\begin{matrix} \cos(\psi) & -\sin(\psi) & 0 \\ \sin(\psi) & \cos(\psi) & 0 \\ 0 & 0 & 1\end{matrix}\right)$$ Combined, as $R_z(\psi) R_y(\phi) R_x(\theta) = R_{xyz}(\theta,\phi,\psi)$; they give: $R_{xyz}$: i.imgur.com/8UQM6.jpg And the Rotation matrix for the Specific convention of Euler angles I want; is this: Euler: So my initial plan, was to compare matrix elements, and extract the angles I wanted that way; I came up with this (actual current code at the end): But this doesn't work under several circumstances. The most obvious being when $\cos(\theta)\cos(\phi) = 1$; since then $\cos(\beta) = 1$, and so $\sin(\beta) = 0$. Where $\sin(\beta)$ is s2 in the code. This happens only when $\cos(\theta)$ and $\cos(\phi) = \pm 1$. So right away I can just rule out the possible situations; When $\theta$ or $\phi = 0, 180, 360, 540, \ldots$, then $\cos(\theta)$, and $\cos(\phi)$ are $\pm 1$; so I only need to do it differently for these cases; And I ended up with this code: public static double[] ZXZtoEuler(double θ, double φ, double ψ){      θ *= Math.PI/180.0;     φ *= Math.PI/180.0;     ψ *= Math.PI/180.0;      double α = -1;     double β = -1;     double γ = -1;      double c2 = Math.cos(θ) * Math.cos(φ);      β = Math.acos(r(c2));      if(eq(c2,1) || eq(c2,-1)){         if(eq(Math.cos(θ),1)){             if(eq(Math.cos(φ),1)){                 α = 0.0;                 γ = ψ;             }else if(eq(Math.cos(φ),-1)){                 α = 0.0;                 γ = Math.PI - ψ;             }         }else if(eq(Math.cos(θ),-1)){             if(eq(Math.cos(φ),1)){                 α = 0.0;                 γ = -ψ;             }else if(eq(Math.cos(φ),-1)){                 α = 0.0;                 γ = ψ + Math.PI;             }         }     }else{          //original way          double s2 = Math.sin(β);          double c3 = ( Math.sin(θ) * Math.cos(φ) )/ s2;         double s1 = ( Math.sin(θ) * Math.sin(ψ) + Math.cos(θ) * Math.sin(φ) * Math.cos(ψ) )/s2;          γ = Math.acos(r(c3));         α = Math.asin(r(s1));      }      α *= 180/Math.PI;     β *= 180/Math.PI;     γ *= 180/Math.PI;      return new double[] {r(α), r(β), r(γ)}; } Where r and eq are just two simple functions; public static double r(double a){     double prec = 1000000000.0;     return Math.round(a*prec)/prec; }  static double thresh = 1E-4; public static boolean eq(double a, double b){     return (Math.abs(a-b) < thresh); } eq is just to compare the numbers for tests, and r is to prevent floating point errors pushing numbers outside the range of Math.acos / Math.asin and giving me NaN results; (i.e. every now and then I'd end up with Math.acos(1.000000000000000004) or something.) Which takes into account the 4 cases of having rotations around $x$ and $y$ which leave c2==1 . But now is where the problem occurs; Everything I have done above, makes sense to me, but it does not give the correct angles; Here is some output, in each pair, the first are the $\theta, \phi, \psi$ angles, and the second of each pair is the corresponding $\alpha, \beta, \gamma$ lines. Ignoring the rounding errors, it seems to be getting some of the angles off by about [0.0, 0.0, 0.0] - correct! [0.0, 0.0, 0.0] [0.0, 0.0, 45.0] - correct! [0.0, 0.0, 45.0] [0.0, 0.0, 90.0] - correct! [0.0, 0.0, 90.0] [0.0, 0.0, 135.0] - correct! [0.0, 0.0, 135.0] [0.0, 0.0, 180.0] - correct [0.0, 0.0, 180.0] [0.0, 0.0, 225.0] - correct [0.0, 0.0, 225.0] [0.0, 0.0, 270.0] - correct [0.0, 0.0, 270.0] [0.0, 0.0, 315.0] - correct [0.0, 0.0, 315.0] [0.0, 45.0, 0.0] - incorrect: should be [90, 45, -90] [90.0, 44.999982, 90.0] [0.0, 45.0, 45.0] [45.000018, 44.999982, 90.0] [0.0, 45.0, 90.0] [0.0, 44.999982, 90.0] [0.0, 45.0, 135.0] [-45.000018, 44.999982, 90.0] [0.0, 45.0, 180.0] [-90.0, 44.999982, 90.0] [0.0, 45.0, 225.0] [-45.000018, 44.999982, 90.0] [0.0, 45.0, 270.0] [0.0, 44.999982, 90.0] [0.0, 45.0, 315.0] [45.000018, 44.999982, 90.0] [0.0, 90.0, 0.0] [90.0, 90.0, 90.0] [0.0, 90.0, 45.0] [45.000018, 90.0, 90.0] [0.0, 90.0, 90.0] [0.0, 90.0, 90.0] [0.0, 90.0, 135.0] [-45.000018, 90.0, 90.0] [0.0, 90.0, 180.0] [-90.0, 90.0, 90.0] [0.0, 90.0, 225.0] [-45.000018, 90.0, 90.0] Can anyone think of a solution? EDIT: I don't know if this helps, but there are a few other ways at looking at these rotations: A way to rotate about an arbitrary axis, is to reorient that axis as the Z Axis, and then do the inverse of the reorientation; you can apply this over and over, to obtain the Euler rotations in terms of the original fixed axis rotations; $$R_{Z^{\prime\prime} X^{\prime} Z}(\alpha,\beta,\gamma) = R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha)$$ $$R_{X^{\prime}}(\beta) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(-\alpha)$$ so $$R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})$$ $$R_{Z^{\prime\prime}} = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)R_Y(\frac{\pi}{2})R_Z(-\beta)R_Y(-\frac{\pi}{2})R_Z(-\alpha)$$ And so the whole thing; $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)R_Y(\frac{\pi}{2})R_Z(-\beta)R_Y(-\frac{\pi}{2})R_Z(-\alpha)R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})$$ Which cancels down; $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)R_Y(\frac{\pi}{2})R_Z(-\beta)\mathbf{R_Y(-\frac{\pi}{2})R_Y(\frac{\pi}{2})}R_Z(\beta)R_Y(-\frac{\pi}{2})$$ $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)R_Y(\frac{\pi}{2})\mathbf{R_Z(-\beta)R_Z(\beta)}R_Y(-\frac{\pi}{2})$$ $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)\mathbf{R_Y(\frac{\pi}{2})R_Y(-\frac{\pi}{2})}$$ $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)$$ And note that  $$R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2}) = R_X(\beta)$$ so we have  $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_X(\beta)R_Z(\gamma)$$ Now i'm not sure how much this might help... but it's something i guess...","Because I'm a new user, I can't post images or hyperlinks, there is a complete version with images here: https://stackoverflow.com/questions/9774958/converting-from-a-euler-zxz-rotation-to-fixed-axis-xyz-rotations The problem I have, is that I need to convert from $XYZ$ fixed axis rotations, to Euler rotations about $Z$, then $X'$, then $Z''$. Here are the relevant matricies: $$X = \left(\begin{matrix} 1 & 0 & 0 \\ 0 & \cos(\theta) & -\sin(\theta) \\ 0 & \sin(\theta) & \cos(\theta)\end{matrix}\right)$$ $$Y = \left(\begin{matrix} \cos(\phi) & 0 & \sin(\phi) \\ 0 & 1 & 0 \\ -\sin(\phi) & 0 & \cos(\phi)\end{matrix}\right)$$ $$Z = \left(\begin{matrix} \cos(\psi) & -\sin(\psi) & 0 \\ \sin(\psi) & \cos(\psi) & 0 \\ 0 & 0 & 1\end{matrix}\right)$$ Combined, as $R_z(\psi) R_y(\phi) R_x(\theta) = R_{xyz}(\theta,\phi,\psi)$; they give: $R_{xyz}$: i.imgur.com/8UQM6.jpg And the Rotation matrix for the Specific convention of Euler angles I want; is this: Euler: So my initial plan, was to compare matrix elements, and extract the angles I wanted that way; I came up with this (actual current code at the end): But this doesn't work under several circumstances. The most obvious being when $\cos(\theta)\cos(\phi) = 1$; since then $\cos(\beta) = 1$, and so $\sin(\beta) = 0$. Where $\sin(\beta)$ is s2 in the code. This happens only when $\cos(\theta)$ and $\cos(\phi) = \pm 1$. So right away I can just rule out the possible situations; When $\theta$ or $\phi = 0, 180, 360, 540, \ldots$, then $\cos(\theta)$, and $\cos(\phi)$ are $\pm 1$; so I only need to do it differently for these cases; And I ended up with this code: public static double[] ZXZtoEuler(double θ, double φ, double ψ){      θ *= Math.PI/180.0;     φ *= Math.PI/180.0;     ψ *= Math.PI/180.0;      double α = -1;     double β = -1;     double γ = -1;      double c2 = Math.cos(θ) * Math.cos(φ);      β = Math.acos(r(c2));      if(eq(c2,1) || eq(c2,-1)){         if(eq(Math.cos(θ),1)){             if(eq(Math.cos(φ),1)){                 α = 0.0;                 γ = ψ;             }else if(eq(Math.cos(φ),-1)){                 α = 0.0;                 γ = Math.PI - ψ;             }         }else if(eq(Math.cos(θ),-1)){             if(eq(Math.cos(φ),1)){                 α = 0.0;                 γ = -ψ;             }else if(eq(Math.cos(φ),-1)){                 α = 0.0;                 γ = ψ + Math.PI;             }         }     }else{          //original way          double s2 = Math.sin(β);          double c3 = ( Math.sin(θ) * Math.cos(φ) )/ s2;         double s1 = ( Math.sin(θ) * Math.sin(ψ) + Math.cos(θ) * Math.sin(φ) * Math.cos(ψ) )/s2;          γ = Math.acos(r(c3));         α = Math.asin(r(s1));      }      α *= 180/Math.PI;     β *= 180/Math.PI;     γ *= 180/Math.PI;      return new double[] {r(α), r(β), r(γ)}; } Where r and eq are just two simple functions; public static double r(double a){     double prec = 1000000000.0;     return Math.round(a*prec)/prec; }  static double thresh = 1E-4; public static boolean eq(double a, double b){     return (Math.abs(a-b) < thresh); } eq is just to compare the numbers for tests, and r is to prevent floating point errors pushing numbers outside the range of Math.acos / Math.asin and giving me NaN results; (i.e. every now and then I'd end up with Math.acos(1.000000000000000004) or something.) Which takes into account the 4 cases of having rotations around $x$ and $y$ which leave c2==1 . But now is where the problem occurs; Everything I have done above, makes sense to me, but it does not give the correct angles; Here is some output, in each pair, the first are the $\theta, \phi, \psi$ angles, and the second of each pair is the corresponding $\alpha, \beta, \gamma$ lines. Ignoring the rounding errors, it seems to be getting some of the angles off by about [0.0, 0.0, 0.0] - correct! [0.0, 0.0, 0.0] [0.0, 0.0, 45.0] - correct! [0.0, 0.0, 45.0] [0.0, 0.0, 90.0] - correct! [0.0, 0.0, 90.0] [0.0, 0.0, 135.0] - correct! [0.0, 0.0, 135.0] [0.0, 0.0, 180.0] - correct [0.0, 0.0, 180.0] [0.0, 0.0, 225.0] - correct [0.0, 0.0, 225.0] [0.0, 0.0, 270.0] - correct [0.0, 0.0, 270.0] [0.0, 0.0, 315.0] - correct [0.0, 0.0, 315.0] [0.0, 45.0, 0.0] - incorrect: should be [90, 45, -90] [90.0, 44.999982, 90.0] [0.0, 45.0, 45.0] [45.000018, 44.999982, 90.0] [0.0, 45.0, 90.0] [0.0, 44.999982, 90.0] [0.0, 45.0, 135.0] [-45.000018, 44.999982, 90.0] [0.0, 45.0, 180.0] [-90.0, 44.999982, 90.0] [0.0, 45.0, 225.0] [-45.000018, 44.999982, 90.0] [0.0, 45.0, 270.0] [0.0, 44.999982, 90.0] [0.0, 45.0, 315.0] [45.000018, 44.999982, 90.0] [0.0, 90.0, 0.0] [90.0, 90.0, 90.0] [0.0, 90.0, 45.0] [45.000018, 90.0, 90.0] [0.0, 90.0, 90.0] [0.0, 90.0, 90.0] [0.0, 90.0, 135.0] [-45.000018, 90.0, 90.0] [0.0, 90.0, 180.0] [-90.0, 90.0, 90.0] [0.0, 90.0, 225.0] [-45.000018, 90.0, 90.0] Can anyone think of a solution? EDIT: I don't know if this helps, but there are a few other ways at looking at these rotations: A way to rotate about an arbitrary axis, is to reorient that axis as the Z Axis, and then do the inverse of the reorientation; you can apply this over and over, to obtain the Euler rotations in terms of the original fixed axis rotations; $$R_{Z^{\prime\prime} X^{\prime} Z}(\alpha,\beta,\gamma) = R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha)$$ $$R_{X^{\prime}}(\beta) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(-\alpha)$$ so $$R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})$$ $$R_{Z^{\prime\prime}} = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)R_Y(\frac{\pi}{2})R_Z(-\beta)R_Y(-\frac{\pi}{2})R_Z(-\alpha)$$ And so the whole thing; $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)R_Y(\frac{\pi}{2})R_Z(-\beta)R_Y(-\frac{\pi}{2})R_Z(-\alpha)R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})$$ Which cancels down; $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)R_Y(\frac{\pi}{2})R_Z(-\beta)\mathbf{R_Y(-\frac{\pi}{2})R_Y(\frac{\pi}{2})}R_Z(\beta)R_Y(-\frac{\pi}{2})$$ $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)R_Y(\frac{\pi}{2})\mathbf{R_Z(-\beta)R_Z(\beta)}R_Y(-\frac{\pi}{2})$$ $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)\mathbf{R_Y(\frac{\pi}{2})R_Y(-\frac{\pi}{2})}$$ $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2})R_Z(\gamma)$$ And note that  $$R_Y(\frac{\pi}{2})R_Z(\beta)R_Y(-\frac{\pi}{2}) = R_X(\beta)$$ so we have  $$R_{Z^{\prime\prime}}(\gamma)R_{X^{\prime}}(\beta)R_Z(\alpha) = R_Z(\alpha)R_X(\beta)R_Z(\gamma)$$ Now i'm not sure how much this might help... but it's something i guess...",,"['matrices', 'rotations']"
78,"$(A-λI)X=0$ works, but$ (λI-A)X=0$ does not when finding eigenvectors. Why?","works, but does not when finding eigenvectors. Why?",(A-λI)X=0  (λI-A)X=0,"Either works when trying to find the eigenvalues, but only the former works when trying to find corresponding eigenvectors. I can understand how it makes a difference, but what I don't understand how one is supposed to ""know"" the former is the ""correct"" form, since it starts from here: $AX = λX$ And from there, you can end up with either: $AX - λX = 0$ or  $λX - AX = 0$ And finally: $(A-λI)X = 0$ or $(λI-A)X = 0$ And so should they not both be correct? Furthermore, once it's in that form, can you not multiply both sides by $-1$ to flip them? I guess I'm overlooking some sort of rule of algebra when dealing with matrices. Let me know. Thanks. EDIT Here is an example: $\pmatrix{5&3\\6&2}$ has an eigenvalue $-4$ If we do $(A-λI)X = 0$: $\pmatrix{9&3\\6&2}*X = 0$ This solves to $X = \pmatrix{-1\\3}$ (and multiples of it) If we do $(λI-A)X = 0$: $\pmatrix{-9&3\\6&-2}*X = 0$ This solves to $X = \pmatrix{1\\3}$ (and multiples of it)","Either works when trying to find the eigenvalues, but only the former works when trying to find corresponding eigenvectors. I can understand how it makes a difference, but what I don't understand how one is supposed to ""know"" the former is the ""correct"" form, since it starts from here: $AX = λX$ And from there, you can end up with either: $AX - λX = 0$ or  $λX - AX = 0$ And finally: $(A-λI)X = 0$ or $(λI-A)X = 0$ And so should they not both be correct? Furthermore, once it's in that form, can you not multiply both sides by $-1$ to flip them? I guess I'm overlooking some sort of rule of algebra when dealing with matrices. Let me know. Thanks. EDIT Here is an example: $\pmatrix{5&3\\6&2}$ has an eigenvalue $-4$ If we do $(A-λI)X = 0$: $\pmatrix{9&3\\6&2}*X = 0$ This solves to $X = \pmatrix{-1\\3}$ (and multiples of it) If we do $(λI-A)X = 0$: $\pmatrix{-9&3\\6&-2}*X = 0$ This solves to $X = \pmatrix{1\\3}$ (and multiples of it)",,['matrices']
79,Invariance of a symmetric sum,Invariance of a symmetric sum,,"Consider two matrices $M$ and $N$ of same dimension, such that $L=MN+NM$ is the symmetric sum in the sense that interchanging $M$ and $N$ does not change $L$ . Is there any transformation $T[M]=M^\prime$ and $T[N]=N^\prime$ such that $M^\prime N^\prime+N^\prime M^\prime=MN+NM$ ? In other words, the symmetric sum is invariant under the transformation $T$ .","Consider two matrices and of same dimension, such that is the symmetric sum in the sense that interchanging and does not change . Is there any transformation and such that ? In other words, the symmetric sum is invariant under the transformation .",M N L=MN+NM M N L T[M]=M^\prime T[N]=N^\prime M^\prime N^\prime+N^\prime M^\prime=MN+NM T,"['matrices', 'linear-transformations']"
80,What does a linear transformation of $\mathbf{C}^2$ look like?,What does a linear transformation of  look like?,\mathbf{C}^2,"This is a math question, but is based on a physics topic I'm trying to develop an intuition for. Let me know if this question would be better placed in physics. What I know: The ""characteristic matrix"" in thin film optics relates the E and H field amplitudes before and after passing through a thin film of material. For a single layer on a substrate, the equation is: $$     \begin{pmatrix}      E_b \\ H_b      \end{pmatrix} =     \begin{pmatrix}     \cos(\delta) & i(1/y_1)\sin(\delta) \\     iy_1 \sin(\delta) &  \cos(\delta) \\     \end{pmatrix}       \begin{pmatrix}       E_a \\ H_a       \end{pmatrix} $$ where $E_a,H_a$ are the field amplitudes at the film/substrate interface, $E_b,H_b$ are the field amplitudes at the air/film interface, $\delta$ = the phase thickness of the material, $y_1$ = characteristic admittance of the thin film. It can also be written in terms of the substrate admittance by normalizing to $E_a$ : $$     \begin{pmatrix}      B \\ C      \end{pmatrix} =     \begin{pmatrix}     \cos(\delta) & i(1/y_1)\sin(\delta) \\     iy_1 \sin(\delta) &  \cos(\delta) \\     \end{pmatrix}       \begin{pmatrix}       1 \\ y_{sub}       \end{pmatrix} $$ I was playing around with the matrix, when I realized: if you view $M$ as a linear transformation of $\mathbf{C}$ , it corresponds to a pure rotation and scaling of the basis vectors (1,0) and (0, $ i$ ). In other words, for vector $\mathbf{v}$ in the complex plane, $M\mathbf{v}$ would rotate $\mathbf{v}$ by $\delta$ and then it's components would both be scaled by $y_1$ and $(1/y_1)$ . See below for how the basis vectors rotate while staying normal to one another for $\delta = \frac{\pi}{2}$ : $$     \begin{pmatrix}      0 \\ iy_1      \end{pmatrix} =     \begin{pmatrix}     0 & i(1/y_1) \\     iy_1 &  0 \\     \end{pmatrix}       \begin{pmatrix}       1 \\ 0       \end{pmatrix} $$ $$     \begin{pmatrix}      -(1/y_1) \\ 0     \end{pmatrix} =     \begin{pmatrix}     0 & i(1/y_1) \\     iy_1 &  0 \\     \end{pmatrix}       \begin{pmatrix}       0 \\ i       \end{pmatrix} $$ I thought this was a really interesting geometric interpretation of what this matrix was doing, until I realized that the vectors $(E_a, H_a)$ or $(1, Y_{sub})$ have complex components themselves and therefore don't exist in $\mathbf{C}$ , but actually in $\mathbf{C}^2$ or $\mathbf{R}^4$ . What I don't know: So my questions now are, what does a linear transformation look like in $\mathbf{C}^2$ ? Does my pure rotation + scaling idea still make sense for this matrix? I'm not sure I even understand what $\mathbf{C}^2$ looks like much less how it would be transformed by a complex matrix...","This is a math question, but is based on a physics topic I'm trying to develop an intuition for. Let me know if this question would be better placed in physics. What I know: The ""characteristic matrix"" in thin film optics relates the E and H field amplitudes before and after passing through a thin film of material. For a single layer on a substrate, the equation is: where are the field amplitudes at the film/substrate interface, are the field amplitudes at the air/film interface, = the phase thickness of the material, = characteristic admittance of the thin film. It can also be written in terms of the substrate admittance by normalizing to : I was playing around with the matrix, when I realized: if you view as a linear transformation of , it corresponds to a pure rotation and scaling of the basis vectors (1,0) and (0, ). In other words, for vector in the complex plane, would rotate by and then it's components would both be scaled by and . See below for how the basis vectors rotate while staying normal to one another for : I thought this was a really interesting geometric interpretation of what this matrix was doing, until I realized that the vectors or have complex components themselves and therefore don't exist in , but actually in or . What I don't know: So my questions now are, what does a linear transformation look like in ? Does my pure rotation + scaling idea still make sense for this matrix? I'm not sure I even understand what looks like much less how it would be transformed by a complex matrix...","
    \begin{pmatrix} 
    E_b \\ H_b 
    \end{pmatrix} =
    \begin{pmatrix}
    \cos(\delta) & i(1/y_1)\sin(\delta) \\
    iy_1 \sin(\delta) &  \cos(\delta) \\
    \end{pmatrix} 
     \begin{pmatrix} 
     E_a \\ H_a 
     \end{pmatrix}
 E_a,H_a E_b,H_b \delta y_1 E_a 
    \begin{pmatrix} 
    B \\ C 
    \end{pmatrix} =
    \begin{pmatrix}
    \cos(\delta) & i(1/y_1)\sin(\delta) \\
    iy_1 \sin(\delta) &  \cos(\delta) \\
    \end{pmatrix} 
     \begin{pmatrix} 
     1 \\ y_{sub} 
     \end{pmatrix}
 M \mathbf{C}  i \mathbf{v} M\mathbf{v} \mathbf{v} \delta y_1 (1/y_1) \delta = \frac{\pi}{2} 
    \begin{pmatrix} 
    0 \\ iy_1 
    \end{pmatrix} =
    \begin{pmatrix}
    0 & i(1/y_1) \\
    iy_1 &  0 \\
    \end{pmatrix} 
     \begin{pmatrix} 
     1 \\ 0 
     \end{pmatrix}
 
    \begin{pmatrix} 
    -(1/y_1) \\ 0
    \end{pmatrix} =
    \begin{pmatrix}
    0 & i(1/y_1) \\
    iy_1 &  0 \\
    \end{pmatrix} 
     \begin{pmatrix} 
     0 \\ i 
     \end{pmatrix}
 (E_a, H_a) (1, Y_{sub}) \mathbf{C} \mathbf{C}^2 \mathbf{R}^4 \mathbf{C}^2 \mathbf{C}^2","['matrices', 'complex-numbers', 'vector-spaces', 'linear-transformations', 'physics']"
81,Fast way to Invert ADA' when D is a diagonal matrix that changes each iteration?,Fast way to Invert ADA' when D is a diagonal matrix that changes each iteration?,,"So I have a statistical learning algorithm in which D is a diagonal matrix that changes each iteration while A stays the same. I'm looking for a fast way to invert ADA' each iteration which ends up being a .9 million by .9 million sized matrix. A is m by n with m < n. My thoughts have been drawn to doing an economic SVD on A to get A=SVU' (and V ends up being a square diagonal matrix) at which point I only need to worry about inverting the inner U'DU term and U'*U=I, I feel like there should be something possible but I can't figure it out. Any ideas? some additional notes: A is fairly sparse, preprocessing things like the SVD of A can take as long as necessary...etc MATLAB code that i've tested to show that the suggested approach doesn't work (unsure how to format this): rows=90; cols=120; G=rand(rows,cols); [X,Y,Z]=svd(G,'econ'); A=Z'; %the above was just to generate A s.t. A'A=I, as in second paragraph above d=rand(1,cols); D=diag(d); M=A*D*A'; Minv=M^(-1) %something to compare with %[U,E,V]=svd(A); %also tried this, it didn't work either [U,E,V]=svd(A,'econ'); Einv=E'; Einv(1:rows,1:rows)=diag(1./diag(E)); %calculate inverse of E Ap=V*Einv*U'; Minv2=Ap'*D^(-1)*Ap; max(max((Minv-Minv2).^2))  %did it work? (no)","So I have a statistical learning algorithm in which D is a diagonal matrix that changes each iteration while A stays the same. I'm looking for a fast way to invert ADA' each iteration which ends up being a .9 million by .9 million sized matrix. A is m by n with m < n. My thoughts have been drawn to doing an economic SVD on A to get A=SVU' (and V ends up being a square diagonal matrix) at which point I only need to worry about inverting the inner U'DU term and U'*U=I, I feel like there should be something possible but I can't figure it out. Any ideas? some additional notes: A is fairly sparse, preprocessing things like the SVD of A can take as long as necessary...etc MATLAB code that i've tested to show that the suggested approach doesn't work (unsure how to format this): rows=90; cols=120; G=rand(rows,cols); [X,Y,Z]=svd(G,'econ'); A=Z'; %the above was just to generate A s.t. A'A=I, as in second paragraph above d=rand(1,cols); D=diag(d); M=A*D*A'; Minv=M^(-1) %something to compare with %[U,E,V]=svd(A); %also tried this, it didn't work either [U,E,V]=svd(A,'econ'); Einv=E'; Einv(1:rows,1:rows)=diag(1./diag(E)); %calculate inverse of E Ap=V*Einv*U'; Minv2=Ap'*D^(-1)*Ap; max(max((Minv-Minv2).^2))  %did it work? (no)",,"['matrices', 'inverse', 'sparse-matrices']"
82,Can every orthogonal matrix be written as a product of Givens rotations?,Can every orthogonal matrix be written as a product of Givens rotations?,,"I'd like to know whether every orthogonal matrix $$ A \in \mathcal{O}_n(\mathbb{R})$$ can be written as a product of givens-rotations. I know that when we do QR-decomposition of matrix $A$ we get $$ A = Q R $$ So my idea was to prove that $R$ must be the identity $I_n$ , however I'm stuck at that. Can somebody give me a hint on how I could prove this?","I'd like to know whether every orthogonal matrix can be written as a product of givens-rotations. I know that when we do QR-decomposition of matrix we get So my idea was to prove that must be the identity , however I'm stuck at that. Can somebody give me a hint on how I could prove this?", A \in \mathcal{O}_n(\mathbb{R}) A  A = Q R  R I_n,"['matrices', 'matrix-decomposition', 'orthogonal-matrices']"
83,Determinant of an $n \times n$ matrix [duplicate],Determinant of an  matrix [duplicate],n \times n,"This question already has answers here : Circular determinant problem (2 answers) Solve $n$th order determinant (3 answers) Closed 6 years ago . In my linear algebra class, my professor gave us this determinant for practice: $$det\pmatrix     {1  & 2 & 3 & \dots & n \\ 2  & 3 & 4 & \dots & 1 \\ \vdots & \vdots & \vdots & \dots & \vdots \\ n   & 1 & 2 & \dots & n-1}$$ Where the $i$-th row or column is the set $$\{1,2,\dots,n\}$$ shifted $i-1$ times to the left. He recommended we add rows $2$ through $n$ to row $1$. And then factor out the constant. This yields: $$\frac{n(n+1)}{2}det\pmatrix     {1  & 1 & 1 & \dots & 1 \\ 2  & 3 & 4 & \dots & 1 \\ \vdots & \vdots & \vdots & \dots & \vdots \\ n   & 1 & 2 & \dots & n-1}$$ However, I cannot for the life of me find a useful way to simplify this further. I have tried adding the first column to each subsequent column, subtracting the first row from each subsequent row, but nothing seems to simplify the matrix. Perhaps, there is an inductive solution, but I haven't been able to find one simply from the $2\times2$ and $3\times3$ cases. Any help would be appreciated.","This question already has answers here : Circular determinant problem (2 answers) Solve $n$th order determinant (3 answers) Closed 6 years ago . In my linear algebra class, my professor gave us this determinant for practice: $$det\pmatrix     {1  & 2 & 3 & \dots & n \\ 2  & 3 & 4 & \dots & 1 \\ \vdots & \vdots & \vdots & \dots & \vdots \\ n   & 1 & 2 & \dots & n-1}$$ Where the $i$-th row or column is the set $$\{1,2,\dots,n\}$$ shifted $i-1$ times to the left. He recommended we add rows $2$ through $n$ to row $1$. And then factor out the constant. This yields: $$\frac{n(n+1)}{2}det\pmatrix     {1  & 1 & 1 & \dots & 1 \\ 2  & 3 & 4 & \dots & 1 \\ \vdots & \vdots & \vdots & \dots & \vdots \\ n   & 1 & 2 & \dots & n-1}$$ However, I cannot for the life of me find a useful way to simplify this further. I have tried adding the first column to each subsequent column, subtracting the first row from each subsequent row, but nothing seems to simplify the matrix. Perhaps, there is an inductive solution, but I haven't been able to find one simply from the $2\times2$ and $3\times3$ cases. Any help would be appreciated.",,"['matrices', 'determinant']"
84,Primitive roots of unity occuring as eigenvalues of a product,Primitive roots of unity occuring as eigenvalues of a product,,"I am currently trying to understand the proof of Benson's Lemma (1.9.1) in Generalized Quadrangles by Payne and Thas. Background We have two $k × k$ matrices $Q$ and $M$ . We want to determine some formula for $\operatorname{tr}(QM)$ . The following information has been proven thus far: The matrix $Q$ is a permutation matrix with order $n$ . Hence, the eigenvalues $\xi$ of $Q$ are all $n$ -th roots of unity. and The matrix $M$ is a real symmetric matrix. It has eigenvalues $\lambda_0, \lambda_1,$ and $\lambda_2$ with multiplicity $m_0, m_1,$ and $m_2$ respectively. We know that each $\lambda_j$ is an integer, and furthermore that $\lambda_0 = 0$ and $m_1 = 1$ . We also know that $QM = MQ$ , and that $\operatorname{tr}(QM)$ is an integer. Lastly, we know that there is some eigenvalue $\theta_1$ of $QM$ such that $\theta_1 = \lambda_1$ . Deductions Since both $Q$ and $M$ are normal matrices, they are both diagonalizable. Moreover, since they commute, they are simultaneously diagonalizable (by $S$ , say). Then $$ S(QM)S^{-1} = (SQS^{-1})(SMS^{-1}),$$ and so the eigenvalues $\theta$ of $QM$ have the form $\theta = \xi\lambda$ where $\xi$ is an eigenvalue of $Q$ and $\lambda$ is an eigenvalue of $M$ . Since the eigenvalue $\theta_1$ agrees with $\lambda_1$ (which has a multiplicity of $1$ as an eigenvalue of $M$ ), it follows that $\theta_1$ has a multiplicity of $1$ as an eigenvalue of $QM$ . Moreover, since $\lambda_0 = 0$ has a multiplicity of $m_0$ in $M$ , it follows that $0$ is an eigenvalue of $QM$ also with multiplicity of $m_0$ . Therefore, all other eigenvalues of $QM$ have the form $\xi\lambda_2$ . We know that there are exactly $m_2$ such eigenvalues. The Problem Payne and Thas claim the following fact: For each divisor $d$ of $n$ , let $U_d$ denote the sum of all primitive $d$ -th roots of unity. Then $U_d$ is an integer. For each divisor $d$ of $n$ , the primitive $d$ -th roots of unity all contribute the same number of times to eigenvalues $\theta$ of $QM$ with $|\theta| = \lambda_2$ . Let $a_d$ denote the multiplicity of $\xi_d\lambda_2$ as an eigenvalue of $QM$ , with $d \mid n$ and $\xi_d$ a primitive $d$ -th root of unity, then we have $$\operatorname{tr}(QM) = \lambda_1 + \sum_{d \mid n}(a_dU_d)\lambda_2.$$ This bolded statement is what I am having trouble understanding. If $\xi_1$ and $\xi_2$ are two different primitive $d$ -th roots of unity, why should the multiplicity of $\xi_1\lambda_2$ and $\xi_2\lambda_2$ be equal? If $\xi$ is a $d$ -th root of unity, and there exists an eigenvalue $\xi\lambda_2$ of $QM$ , why do all other $d$ -th roots of unity appear as well?","I am currently trying to understand the proof of Benson's Lemma (1.9.1) in Generalized Quadrangles by Payne and Thas. Background We have two matrices and . We want to determine some formula for . The following information has been proven thus far: The matrix is a permutation matrix with order . Hence, the eigenvalues of are all -th roots of unity. and The matrix is a real symmetric matrix. It has eigenvalues and with multiplicity and respectively. We know that each is an integer, and furthermore that and . We also know that , and that is an integer. Lastly, we know that there is some eigenvalue of such that . Deductions Since both and are normal matrices, they are both diagonalizable. Moreover, since they commute, they are simultaneously diagonalizable (by , say). Then and so the eigenvalues of have the form where is an eigenvalue of and is an eigenvalue of . Since the eigenvalue agrees with (which has a multiplicity of as an eigenvalue of ), it follows that has a multiplicity of as an eigenvalue of . Moreover, since has a multiplicity of in , it follows that is an eigenvalue of also with multiplicity of . Therefore, all other eigenvalues of have the form . We know that there are exactly such eigenvalues. The Problem Payne and Thas claim the following fact: For each divisor of , let denote the sum of all primitive -th roots of unity. Then is an integer. For each divisor of , the primitive -th roots of unity all contribute the same number of times to eigenvalues of with . Let denote the multiplicity of as an eigenvalue of , with and a primitive -th root of unity, then we have This bolded statement is what I am having trouble understanding. If and are two different primitive -th roots of unity, why should the multiplicity of and be equal? If is a -th root of unity, and there exists an eigenvalue of , why do all other -th roots of unity appear as well?","k × k Q M \operatorname{tr}(QM) Q n \xi Q n M \lambda_0, \lambda_1, \lambda_2 m_0, m_1, m_2 \lambda_j \lambda_0 = 0 m_1 = 1 QM = MQ \operatorname{tr}(QM) \theta_1 QM \theta_1 = \lambda_1 Q M S  S(QM)S^{-1} = (SQS^{-1})(SMS^{-1}), \theta QM \theta = \xi\lambda \xi Q \lambda M \theta_1 \lambda_1 1 M \theta_1 1 QM \lambda_0 = 0 m_0 M 0 QM m_0 QM \xi\lambda_2 m_2 d n U_d d U_d d n d \theta QM |\theta| = \lambda_2 a_d \xi_d\lambda_2 QM d \mid n \xi_d d \operatorname{tr}(QM) = \lambda_1 + \sum_{d \mid n}(a_dU_d)\lambda_2. \xi_1 \xi_2 d \xi_1\lambda_2 \xi_2\lambda_2 \xi d \xi\lambda_2 QM d","['matrices', 'eigenvalues-eigenvectors', 'roots-of-unity', 'finite-geometry']"
85,Characterizing duals of cones that are linear images of the positive semidefinite cone,Characterizing duals of cones that are linear images of the positive semidefinite cone,,"Let $M_n$ denote the space of $n\times n$ matrices over complex numbers. The space of self-adjoint matrices is denoted  $$ M_n^{sa} = \{A\in M_n\, :\, A^*=A \}, $$ where $A^*$ denotes the conjugate transpose of $A$,  and the cone of positive semidefinite matrices will be denoted  $$  M_n^+ = \{A\in M_n^{sa}\, :\, A\geq0\}, $$ where $A\geq0$ denotes that $A$ is positive semidefinite. The adjoint of a linear map of matrices $\phi:M_n\rightarrow M_m$ is the unique map $\phi^*:M_m\rightarrow M_n$ that satisfies  $$ \mathrm{Tr}(\phi(A)B) = \mathrm{Tr}(A\phi^*(B)) $$ for all $A\in M_n$ and $B\in M_m$. The dual cone of a  subset $S\subseteq M_{n}^{sa}$ is defined by $$ S^* = \{B\in M_{n}^{sa}\, : \, \forall A\in S, \, \mathrm{Tr}(AB)\geq 0\} $$ Note that $S^*$ is always a closed and convex cone. Let $\phi$ be a linear map of matrices and consider the cones of self-adjoint matrices defined by $$ K = \{A\in M_n^{sa} \, :\, \phi(A)\geq 0\} $$ and  $$ L= \{\phi^*(B) \, :\, B\geq 0\}. $$ It is evident that both $K$ and $L$ are both convex cones and that $L\subseteq K^*$ and $K\subseteq L^*$. Question: Is it necessarily the case that $K^*=L$ or $L^*=K$? If not, under what circumstances would it hold that $K^*=L$ or $L^*=K$? For the linear maps $\phi$ that I've played around with so far this seems to be the case and I'm wondering if there is a counterexample.","Let $M_n$ denote the space of $n\times n$ matrices over complex numbers. The space of self-adjoint matrices is denoted  $$ M_n^{sa} = \{A\in M_n\, :\, A^*=A \}, $$ where $A^*$ denotes the conjugate transpose of $A$,  and the cone of positive semidefinite matrices will be denoted  $$  M_n^+ = \{A\in M_n^{sa}\, :\, A\geq0\}, $$ where $A\geq0$ denotes that $A$ is positive semidefinite. The adjoint of a linear map of matrices $\phi:M_n\rightarrow M_m$ is the unique map $\phi^*:M_m\rightarrow M_n$ that satisfies  $$ \mathrm{Tr}(\phi(A)B) = \mathrm{Tr}(A\phi^*(B)) $$ for all $A\in M_n$ and $B\in M_m$. The dual cone of a  subset $S\subseteq M_{n}^{sa}$ is defined by $$ S^* = \{B\in M_{n}^{sa}\, : \, \forall A\in S, \, \mathrm{Tr}(AB)\geq 0\} $$ Note that $S^*$ is always a closed and convex cone. Let $\phi$ be a linear map of matrices and consider the cones of self-adjoint matrices defined by $$ K = \{A\in M_n^{sa} \, :\, \phi(A)\geq 0\} $$ and  $$ L= \{\phi^*(B) \, :\, B\geq 0\}. $$ It is evident that both $K$ and $L$ are both convex cones and that $L\subseteq K^*$ and $K\subseteq L^*$. Question: Is it necessarily the case that $K^*=L$ or $L^*=K$? If not, under what circumstances would it hold that $K^*=L$ or $L^*=K$? For the linear maps $\phi$ that I've played around with so far this seems to be the case and I'm wondering if there is a counterexample.",,"['matrices', 'positive-semidefinite', 'semidefinite-programming', 'convex-cone', 'dual-cone']"
86,Conversion from differential to derivative for trace of a matrix,Conversion from differential to derivative for trace of a matrix,,"I am studying the matrix calculus page written in wikipedia and I have a question. In the table ' $\text{Identities: scalar-by-matrix}\frac{\partial y}{\partial \mathbf{X}}$ ' , it is shown : (1) $\frac{\partial \operatorname{tr}(\mathbf{AX})}{\partial \mathbf{X}} = \frac{\partial \operatorname{tr}(\mathbf{XA})}{\partial \mathbf{X}} =\mathbf{A}^\top$ This can be easily proved by showing: $\frac{\partial}{\partial \mathbf{X_{ij}}}   \operatorname{tr}(\mathbf{AX}) = \frac{\partial}{\partial \mathbf{X_{ij}}} (\sum_r \sum_s a_{rs}x_{sr}) =a_{ji}  $ (2)In another section 'Conversion from differential to derivative form' it is shown the canonical form $dy = \operatorname{tr}(\mathbf{A}\,d\mathbf{X}) $ is equivalent to differential form$\frac{dy}{d\mathbf{X}} = \mathbf{A}$. Can anyone help please to understand the this conversion? How this conversion is related to to formula (1)? I thought if they are related then the $\frac{dy}{d\mathbf{X}} = \mathbf{A}^\top$. I found this proof from the following note . but I don't understand what happen in the last part ( It is shown inside the red box). I assumed it will be $a_{ji}$. Why the derivative is written with respect to $x_{ji}$. I would appreciate any insights on this. Thank you.","I am studying the matrix calculus page written in wikipedia and I have a question. In the table ' $\text{Identities: scalar-by-matrix}\frac{\partial y}{\partial \mathbf{X}}$ ' , it is shown : (1) $\frac{\partial \operatorname{tr}(\mathbf{AX})}{\partial \mathbf{X}} = \frac{\partial \operatorname{tr}(\mathbf{XA})}{\partial \mathbf{X}} =\mathbf{A}^\top$ This can be easily proved by showing: $\frac{\partial}{\partial \mathbf{X_{ij}}}   \operatorname{tr}(\mathbf{AX}) = \frac{\partial}{\partial \mathbf{X_{ij}}} (\sum_r \sum_s a_{rs}x_{sr}) =a_{ji}  $ (2)In another section 'Conversion from differential to derivative form' it is shown the canonical form $dy = \operatorname{tr}(\mathbf{A}\,d\mathbf{X}) $ is equivalent to differential form$\frac{dy}{d\mathbf{X}} = \mathbf{A}$. Can anyone help please to understand the this conversion? How this conversion is related to to formula (1)? I thought if they are related then the $\frac{dy}{d\mathbf{X}} = \mathbf{A}^\top$. I found this proof from the following note . but I don't understand what happen in the last part ( It is shown inside the red box). I assumed it will be $a_{ji}$. Why the derivative is written with respect to $x_{ji}$. I would appreciate any insights on this. Thank you.",,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'canonical-transformation']"
87,Derivative of transpose of a matrix,Derivative of transpose of a matrix,,"If there is some function that takes the transpose of a matrix such as $g(x) = x^t$ where $x$ is some square matrix. What would then be the derivative of the function, $\frac{dg}{dx}$?","If there is some function that takes the transpose of a matrix such as $g(x) = x^t$ where $x$ is some square matrix. What would then be the derivative of the function, $\frac{dg}{dx}$?",,"['matrices', 'derivatives', 'matrix-calculus']"
88,Prove that if A is M-matrix then A is also a P-matrix,Prove that if A is M-matrix then A is also a P-matrix,,"$A \in \mathbb{R}^{n \times n}$ is a $P$ -matrix if all its principal minors are positive. Let $I$ be the identity matrix of rank $n$ . $A \in \mathbb{R}^{n \times n}$ is a non-singular $M$ -matrix if $A=I-B$ where $B \in \mathbb{R}^{n \times n}$ has only non negative entries and the largest eigenvalue of $B$ (or maximum of moduli) is strictly smaller than one. Claim $1$ : Nonsingular $M$ -matrices are a subclass of $P$ -matrices. I am trying to prove claim $1$ . If we call $\mathbb{M}$ the set of non-singular $M$ -matrices and $\mathbb{P}$ the set of $P$ -matrices, then proving the above claim comes down to taking one (any) element of the set $\mathbb{M}$ and showing that this element also belongs to $\mathbb{P}$ . It seems long but doable for $n=2$ . Suppose $A=I-B \in \mathbb{M}$ . Take a matrix $B$ with non-negative entries $b_{11}=a$ , $b_{12}=b$ , $b_{21}=c$ , $b_{22}=d$ . We compute the largest eigenvalue of $B$ (case where $\Delta$ is zero or strictly positive gives 6 different cases to consider). For example for the case $\Delta =0$ with $a=d$ and $b=c=0$ we get $$\lambda_1=\lambda_2 = \frac{a+d}{2}$$ Since $A$ is in $\mathbb{M}$ we have the condition: $$ a < 1$$ For A to also be in $\mathbb{P}$ the following must be satisfied: $$ (1 - a)(1 -d) - bc \geq 0$$ $$\Leftrightarrow (1-a)^2 \geq 0$$ which is always true. Other cases get longer and messier. Then to suppose the hypothesis holds for n and show it for n+1 seems also long. any suggestions of a straight forward way to prove the claim ? Also for $n=2$ we can take $A \in \mathbb{P}$ and suppose that $A \not \in \mathbb{M}$ . We get a contradiction directly for the case $a=d$ and $b=c=0$ (and with more effort we check it for the other cases)... Thank you","is a -matrix if all its principal minors are positive. Let be the identity matrix of rank . is a non-singular -matrix if where has only non negative entries and the largest eigenvalue of (or maximum of moduli) is strictly smaller than one. Claim : Nonsingular -matrices are a subclass of -matrices. I am trying to prove claim . If we call the set of non-singular -matrices and the set of -matrices, then proving the above claim comes down to taking one (any) element of the set and showing that this element also belongs to . It seems long but doable for . Suppose . Take a matrix with non-negative entries , , , . We compute the largest eigenvalue of (case where is zero or strictly positive gives 6 different cases to consider). For example for the case with and we get Since is in we have the condition: For A to also be in the following must be satisfied: which is always true. Other cases get longer and messier. Then to suppose the hypothesis holds for n and show it for n+1 seems also long. any suggestions of a straight forward way to prove the claim ? Also for we can take and suppose that . We get a contradiction directly for the case and (and with more effort we check it for the other cases)... Thank you",A \in \mathbb{R}^{n \times n} P I n A \in \mathbb{R}^{n \times n} M A=I-B B \in \mathbb{R}^{n \times n} B 1 M P 1 \mathbb{M} M \mathbb{P} P \mathbb{M} \mathbb{P} n=2 A=I-B \in \mathbb{M} B b_{11}=a b_{12}=b b_{21}=c b_{22}=d B \Delta \Delta =0 a=d b=c=0 \lambda_1=\lambda_2 = \frac{a+d}{2} A \mathbb{M}  a < 1 \mathbb{P}  (1 - a)(1 -d) - bc \geq 0 \Leftrightarrow (1-a)^2 \geq 0 n=2 A \in \mathbb{P} A \not \in \mathbb{M} a=d b=c=0,"['matrices', 'linear-programming']"
89,Product of a Finite Number of Matrices Related to Roots of Unity,Product of a Finite Number of Matrices Related to Roots of Unity,,"Does anyone have an idea how to prove the following identity? $$ \mathop{\mathrm{Tr}}\left(\prod_{j=0}^{n-1}\begin{pmatrix}   x^{-2j} & -x^{2j+1} \\ 1 & 0 \end{pmatrix}\right)= \begin{cases} 2 & \text{if } n=0\pmod{6}\\ 1 & \text{if } n=1,5\pmod{6}\\ -1 & \text{if } n=2,4\pmod{6}\\ 4 & \text{if } n=3\pmod{6} \end{cases}, $$ where $x=e^{\frac{\pi i}{n}}$ and the product sign means usual matrix multiplication. I have tried induction but there are too many terms in all of four entries as $n$ grows. I think maybe using generating functions is the way?","Does anyone have an idea how to prove the following identity? $$ \mathop{\mathrm{Tr}}\left(\prod_{j=0}^{n-1}\begin{pmatrix}   x^{-2j} & -x^{2j+1} \\ 1 & 0 \end{pmatrix}\right)= \begin{cases} 2 & \text{if } n=0\pmod{6}\\ 1 & \text{if } n=1,5\pmod{6}\\ -1 & \text{if } n=2,4\pmod{6}\\ 4 & \text{if } n=3\pmod{6} \end{cases}, $$ where $x=e^{\frac{\pi i}{n}}$ and the product sign means usual matrix multiplication. I have tried induction but there are too many terms in all of four entries as $n$ grows. I think maybe using generating functions is the way?",,"['matrices', 'number-theory', 'polynomials', 'recurrence-relations']"
90,Jacobian of composite functions with different number of variables,Jacobian of composite functions with different number of variables,,"It is said that it is possible to calculate the Jacobian of a composed function by multiplying the Jacobians of each function, that is $$ J_f = J_{f_1} \cdot J_{f_2} \cdots J_{f_nx} $$ where $$ f = f_1 \circ f_2 \circ \cdots\circ f_n $$ I'm trying to do this on very simple examples that are not well-behaved (where the jacobians are not square matrices), and I can't get this done. I'm hoping someone can help me solve this (Any external lectures or material is also welcome. I could't find any related question). let $$ f(x) = x^2 + x $$ This could be written as $$ f_1(x, y) = x + y $$ $$ f_2(x) = x^2 $$ $$ f(x) = f_1(f_2(x), x) $$ We now know that $$ Jf_1 = [1, 1] $$ $$ Jf_2 = [2x] $$ And then $$ Jf = Jf_1 \cdot Jf_2 $$ Which is not a valid product. What to do in this case? Forcing both matrices as 2x2 seems to lead to an invalid solution: Jf1 = [1, 1]       [0, 0]  Jf2 = [2x 0]       [0  0]  Jf1 * Jf2 = [2x 0]             [0  0] (Would mean that df/dx = 2x). What am I missing?","It is said that it is possible to calculate the Jacobian of a composed function by multiplying the Jacobians of each function, that is $$ J_f = J_{f_1} \cdot J_{f_2} \cdots J_{f_nx} $$ where $$ f = f_1 \circ f_2 \circ \cdots\circ f_n $$ I'm trying to do this on very simple examples that are not well-behaved (where the jacobians are not square matrices), and I can't get this done. I'm hoping someone can help me solve this (Any external lectures or material is also welcome. I could't find any related question). let $$ f(x) = x^2 + x $$ This could be written as $$ f_1(x, y) = x + y $$ $$ f_2(x) = x^2 $$ $$ f(x) = f_1(f_2(x), x) $$ We now know that $$ Jf_1 = [1, 1] $$ $$ Jf_2 = [2x] $$ And then $$ Jf = Jf_1 \cdot Jf_2 $$ Which is not a valid product. What to do in this case? Forcing both matrices as 2x2 seems to lead to an invalid solution: Jf1 = [1, 1]       [0, 0]  Jf2 = [2x 0]       [0  0]  Jf1 * Jf2 = [2x 0]             [0  0] (Would mean that df/dx = 2x). What am I missing?",,"['matrices', 'multivariable-calculus', 'function-and-relation-composition']"
91,Inverse of a sum of positive definite matrices,Inverse of a sum of positive definite matrices,,"Let $A,B$ be symmetric positive definite matrices. Let $A^{-1} = LL^T$ (Cholesky decomposition, $L$ is lower-triangular). I think the following identities are true, but I haven't found them online: $$ (A+B)^{-1} = L(I+L^TBL)^{-1}L^T \\ |A+B| = |A| \, |I+L^T B L| $$ Are they correct? And if so, how do you show they're true? I suppose they can be derived from the Woodbury formula and the matrix determinant lemma, but I tried and I couldn't prove it...","Let $A,B$ be symmetric positive definite matrices. Let $A^{-1} = LL^T$ (Cholesky decomposition, $L$ is lower-triangular). I think the following identities are true, but I haven't found them online: $$ (A+B)^{-1} = L(I+L^TBL)^{-1}L^T \\ |A+B| = |A| \, |I+L^T B L| $$ Are they correct? And if so, how do you show they're true? I suppose they can be derived from the Woodbury formula and the matrix determinant lemma, but I tried and I couldn't prove it...",,['matrices']
92,Equivalence of Frobenius norm and trace norm,Equivalence of Frobenius norm and trace norm,,"According to [1] , [2] and other related publications, the following holds for any matrix $X$: $$\| X\|_\Sigma=\min_{X=UV'}\|U\|_\mathrm{Fro}\|V\|_\mathrm{Fro}=\min_{X=UV'}\frac{1}{2}(\|U\|_\mathrm{Fro}^{2}+\|V\|_\mathrm{Fro}^2)$$ where $\|\cdot\|_\Sigma$ is the trace (nuclear/Ky-Fan) norm and $\|\cdot\|_\mathrm{Fro}$ is the Frobenius norm. Can anyone show why this equality is true? In the publications, it is filed under ""Preliminaries"" and one of the few Lemmas without proof. I find this relationship very fundamental and interesting, but could not find it anywhere else, let alone proof. Thank you for any help! What I already have: If I understand the 'hint' in ref. 1 (above) correctly, $\min_{X=UV'}\|U\|_\mathrm{Fro}\|V\|_\mathrm{Fro}$ is minimized by $U=\hat{U}\sqrt{\Lambda}$ and $V=\hat{V}\sqrt{\Lambda}$, where $X=\hat{U}\Lambda \hat{V'}$ is the singlular value decomposition of $X$ (Page 75, Lemma 8 in ref. 1 (above)).","According to [1] , [2] and other related publications, the following holds for any matrix $X$: $$\| X\|_\Sigma=\min_{X=UV'}\|U\|_\mathrm{Fro}\|V\|_\mathrm{Fro}=\min_{X=UV'}\frac{1}{2}(\|U\|_\mathrm{Fro}^{2}+\|V\|_\mathrm{Fro}^2)$$ where $\|\cdot\|_\Sigma$ is the trace (nuclear/Ky-Fan) norm and $\|\cdot\|_\mathrm{Fro}$ is the Frobenius norm. Can anyone show why this equality is true? In the publications, it is filed under ""Preliminaries"" and one of the few Lemmas without proof. I find this relationship very fundamental and interesting, but could not find it anywhere else, let alone proof. Thank you for any help! What I already have: If I understand the 'hint' in ref. 1 (above) correctly, $\min_{X=UV'}\|U\|_\mathrm{Fro}\|V\|_\mathrm{Fro}$ is minimized by $U=\hat{U}\sqrt{\Lambda}$ and $V=\hat{V}\sqrt{\Lambda}$, where $X=\hat{U}\Lambda \hat{V'}$ is the singlular value decomposition of $X$ (Page 75, Lemma 8 in ref. 1 (above)).",,"['matrices', 'normed-spaces', 'trace', 'nuclear-norm']"
93,Is there an $O(n^2)$ test to determine if an $n \times n$ Boolean matrix $B$ has an inverse?,Is there an  test to determine if an  Boolean matrix  has an inverse?,O(n^2) n \times n B,"D.E. Rutherford shows that if a Boolean matrix $B$ has an inverse, then $B^{-1}= B^T$, or $BB^T=B^TB=I$. I have two related questions: The only invertible Boolean matrices I can find are permutation matrices. Are there others? Is there an $O(n^2)$ test to determine if an $n \times n$ Boolean         matrix $B$ has an inverse? Note : The $O(n^2)$ Matlab function I gave here is wrong. UPDATE : I have posted a new $O(n^2)$ Matlab invertibility test here .","D.E. Rutherford shows that if a Boolean matrix $B$ has an inverse, then $B^{-1}= B^T$, or $BB^T=B^TB=I$. I have two related questions: The only invertible Boolean matrices I can find are permutation matrices. Are there others? Is there an $O(n^2)$ test to determine if an $n \times n$ Boolean         matrix $B$ has an inverse? Note : The $O(n^2)$ Matlab function I gave here is wrong. UPDATE : I have posted a new $O(n^2)$ Matlab invertibility test here .",,"['matrices', 'boolean-algebra']"
94,Proving that a set generates $\Gamma_0(4)$,Proving that a set generates,\Gamma_0(4),"I want to show that $$\Gamma_0(4) = \biggl\{\gamma = \pmatrix{a&b\cr c&d}\in {\rm SL}_2({\bf Z}) :  c\equiv 0\pmod 4\biggr\}$$ is generated by the three matrices $$\pmatrix{1&1\cr 0&1},\quad\pmatrix{1&0\cr 4&1},\quad\hbox{and}\quad\pmatrix{-1&0\cr 0&-1}.$$ I tried to do this by showing that note that for any $\gamma = \bigl( {a\atop c}{b\atop d}\bigr) \in \Gamma_0(4)$ , we have $$\pmatrix{a&b\cr c&d}\pmatrix{1&-n\cr 0&1} = \pmatrix{a & b-na\cr c&d-nc}\qquad\hbox{and}\qquad \pmatrix{a&b\cr c&d}\pmatrix{1&0\cr -4n&1} = \pmatrix{a-4nb & b\cr c-4nd&d}.$$ If $c$ is $0$ we are done, since $\gamma$ is in $\Gamma_0(4)$ in this case. Otherwise, if $|c|<|d|$ , we can apply the division algorithm to get $q$ and $d'$ such that $|d| = |c|q + d'$ with $|d'| < |c|/2$ , and the first transformation applied $q$ times produces a matrix whose bottom-left entry is $c$ and whose bottom-right entry is $d'$ . On the other hand, if $d\ne 0$ and $|c|>4|d|$ , then we can find $q$ and $c'$ such that $|c| = 4|d|q + c'$ where $|c'| < 2|d|$ and we apply the second transformation $q$ times to get a matrix with bottom-left entry $c'$ . In each case, we have strictly reduced the quantity $\min\bigl\{|c|, 2|d|\bigr\}$ , so the process must terminate with $|c| = 0$ or $|d|=0$ . In the first case we have found an element of $\Gamma_0(4)$ , and the second case cannot happen, since it would imply that $c = \pm 1$ . I think this is the right idea except that we're missing the case where $|d|\le |c|\le 4|d|$ (correct me if there are any other holes in the proof other than this). I'm just not sure what to do in this case.","I want to show that is generated by the three matrices I tried to do this by showing that note that for any , we have If is we are done, since is in in this case. Otherwise, if , we can apply the division algorithm to get and such that with , and the first transformation applied times produces a matrix whose bottom-left entry is and whose bottom-right entry is . On the other hand, if and , then we can find and such that where and we apply the second transformation times to get a matrix with bottom-left entry . In each case, we have strictly reduced the quantity , so the process must terminate with or . In the first case we have found an element of , and the second case cannot happen, since it would imply that . I think this is the right idea except that we're missing the case where (correct me if there are any other holes in the proof other than this). I'm just not sure what to do in this case.","\Gamma_0(4) = \biggl\{\gamma = \pmatrix{a&b\cr c&d}\in {\rm SL}_2({\bf Z}) : 
c\equiv 0\pmod 4\biggr\} \pmatrix{1&1\cr 0&1},\quad\pmatrix{1&0\cr 4&1},\quad\hbox{and}\quad\pmatrix{-1&0\cr 0&-1}. \gamma = \bigl( {a\atop c}{b\atop d}\bigr) \in \Gamma_0(4) \pmatrix{a&b\cr c&d}\pmatrix{1&-n\cr 0&1} = \pmatrix{a & b-na\cr c&d-nc}\qquad\hbox{and}\qquad
\pmatrix{a&b\cr c&d}\pmatrix{1&0\cr -4n&1} = \pmatrix{a-4nb & b\cr c-4nd&d}. c 0 \gamma \Gamma_0(4) |c|<|d| q d' |d| = |c|q + d' |d'| < |c|/2 q c d' d\ne 0 |c|>4|d| q c' |c| = 4|d|q + c' |c'| < 2|d| q c' \min\bigl\{|c|, 2|d|\bigr\} |c| = 0 |d|=0 \Gamma_0(4) c = \pm 1 |d|\le |c|\le 4|d|","['matrices', 'group-theory', 'modular-forms']"
95,Perturbation of the spectrum of a matrix by adding small decaying coefficients.,Perturbation of the spectrum of a matrix by adding small decaying coefficients.,,"BACKGROUND & MOTIVATION: We consider in a complex infinite-dimensional Hilbert space a bounded operator $T$ . We pick a Hilbert basis $(e_n)_{n\in\mathbb{Z}}$ and project $T$ onto it: for all $N\in\mathbb{N}^*$ , we let \begin{align*} T_N&:=(\langle e_m,Te_n\rangle)_{|m|,|n|\leq N} \end{align*} where $\langle\cdot,\cdot\rangle$ is the inner product of the Hilbert space. The main motivation is to determine whether $0$ is in the spectrum of $T$ or not. We assume that $T$ is of the form $\mathrm{1}+K$ where $K$ is trace-class, so that $T$ is Fredholm of index $0$ : in particular, its spectrum consists in eigenvalues and $0$ is an eigenvalue if and only if the Fredholm determinant $\det(\mathrm{1}+K)$ cancels. Since $T_N\to T$ as $N\to+\infty$ in the operator norm topology, we have: \begin{align*} \lim_{N\to+\infty}\det((\langle e_m,Te_n\rangle)_{|m|,|n|\leq N})&=\det(T). \end{align*} Since $K$ is compact, we do know that the non-zero eigenvalues of $K_N:=(\langle e_m,Ke_n\rangle)_{|m|,|n|\leq N}$ converge to non-zero eigenvalues of $K$ . Now the goal is to give an estimate on the rate of convergence of the eigenvalues of $K_N$ with, say, modulus greater than $1/4$ , to their limits in the spectrum of $K$ . In the literature, such an estimate relies on bounding the resolvent $(K-\lambda)^{-1}$ for $\lambda$ on a small contour enclosing an eigenvalue of $K$ , which is not feasible in practice. Fix $N\in\mathbb{N}^*$ and $N'>N$ . To simplify notations and put the problem into a general form, let $A_N\equiv A\in\mathcal{M}_{2N+1}(\mathbb{C})$ (so $A$ plays the role of $K_N$ above) and \begin{align*} A'&:=\begin{pmatrix}E_1&E_2&E_3\\E_4&A&E_5\\E_6&E_7&E_8\end{pmatrix}\in\mathcal{M}_{2N'+1}(\mathbb{C}) \end{align*} where $E_1,E_3,E_6,E_8\in\mathcal{M}_{N'-N}(\mathbb{C})$ , $E_2,E_7\in\mathcal{M}_{N'-N,2N+1}(\mathbb{C})$ and $E_4,E_5\in\mathcal{M}_{2N+1,N'-N}(\mathbb{C})$ have $\|\cdot\|_1$ and $\|\cdot\|_\infty$ norms $\leq\varepsilon$ ; we also have that $\sum_{k}|E_j|_{kk}\leq\varepsilon$ by the trace-class property of $K$ . Typically, the coefficients of $A'$ decay as ""we move to the exterior"" of $A$ ', that is: \begin{align*} &|(E_2)_{j,k}|\lesssim\frac{1}{(N'+j)^2};&&|(E_7)_{j,k}|\lesssim\frac{1}{(N+j)^2};\\ &|(E_4)_{j,k}|\lesssim\frac{1}{(N'+k)^2};&&|(E_5)_{j,k}|\lesssim\frac{1}{(N+k)^2};\\ &|(E_1)_{j,k}|\lesssim\frac{1}{(N'+j)^2},\frac{1}{(N'+k)^2};&&|(E_6)_{j,k}|\lesssim\frac{1}{(N'+k)^2},\frac{1}{(N+j)^2};\\ &|(E_3)_{j,k}|\lesssim\frac{1}{(N'+j)^2},\frac{1}{(N+k)^2};&&|(E_8)_{j,k}|\lesssim\frac{1}{(N+k)^2},\frac{1}{(N+j)^2}. \end{align*} Using the formula \begin{align*} \lambda\mathrm{1}_{2N+1}&=\frac{1}{2\pi\mathrm{i}}\oint(A-\mu)^{-1}\mu\mathrm{d}\mu \end{align*} for all eigenvalue $\lambda$ of $A$ , we know that there exists $\delta(\varepsilon)>0$ such that \begin{align*} \mathrm{Spec}(A')\setminus\overline{D(0,1/4)}\subset\mathrm{Spec}(A)\setminus\overline{D(0,1/4)}+D(0,\delta(\varepsilon))\tag{$\star$} \end{align*} where $\mathrm{Spec}$ denotes the spectrum of the corresponding matrix; here we removed the closed discs $\setminus\overline{D(0,1/4)}$ as we do not interest ourselves in the spectrum that accumulate at 0 (by compactness of $K$ ). I would like to get an explicit estimate of $\delta(\varepsilon)$ . The matrix $A$ is not normal . One way I see to prove the statement for the largest (in modulus) eigenvalues is to use the definition of the spectral radius as $\lim_{n\to+\infty}\|A^n\|^{\frac{1}{n}}$ ; but this implies to compute the coefficients of $A^n$ : we can see that we obtain a matrix whose each line consists in a sum of $3^n$ products of the matrices $E_j$ and $A$ , and indeed the term $A^n$ is obtained on the line $k$ for $N'+1\leq k\leq N'+2N+1$ . I do not see a clean way to proceed then as terms containing $A^\ell$ with $\ell\leq n-1$ do not vanish at the limit $n\to+\infty$ (and could blow in norm as $n\to+\infty$ ). Another thing I tried without success is the following: let $u\in\mathbb{C}^{2N+1}$ and $\lambda\in\mathbb{C}$ such that $Au=\lambda u$ with $|\lambda|>1/4$ . We look for $u'\in\mathbb{C}^{2N'+1}$ and $\lambda'\in\mathbb{C}$ such that $A'u'=\lambda'u'$ of the form \begin{align*} u'&=\sum_{k=0}^{+\infty}\varepsilon^ku_k,\qquad\qquad\lambda'=\sum_{k=0}^{+\infty}\varepsilon^k\lambda_k \end{align*} where $u_0=(0,u,0)$ and $\lambda_0=\lambda$ . The idea was then to eliminate terms of order $\varepsilon^k$ with an appropriate choice of $u_k$ and $\lambda_k$ -- perhaps by setting $\varepsilon^{2k}\lambda_k$ in the series defining $\lambda'$ instead of $\varepsilon^k\lambda_k$ . I could not manage to do it so far.","BACKGROUND & MOTIVATION: We consider in a complex infinite-dimensional Hilbert space a bounded operator . We pick a Hilbert basis and project onto it: for all , we let where is the inner product of the Hilbert space. The main motivation is to determine whether is in the spectrum of or not. We assume that is of the form where is trace-class, so that is Fredholm of index : in particular, its spectrum consists in eigenvalues and is an eigenvalue if and only if the Fredholm determinant cancels. Since as in the operator norm topology, we have: Since is compact, we do know that the non-zero eigenvalues of converge to non-zero eigenvalues of . Now the goal is to give an estimate on the rate of convergence of the eigenvalues of with, say, modulus greater than , to their limits in the spectrum of . In the literature, such an estimate relies on bounding the resolvent for on a small contour enclosing an eigenvalue of , which is not feasible in practice. Fix and . To simplify notations and put the problem into a general form, let (so plays the role of above) and where , and have and norms ; we also have that by the trace-class property of . Typically, the coefficients of decay as ""we move to the exterior"" of ', that is: Using the formula for all eigenvalue of , we know that there exists such that where denotes the spectrum of the corresponding matrix; here we removed the closed discs as we do not interest ourselves in the spectrum that accumulate at 0 (by compactness of ). I would like to get an explicit estimate of . The matrix is not normal . One way I see to prove the statement for the largest (in modulus) eigenvalues is to use the definition of the spectral radius as ; but this implies to compute the coefficients of : we can see that we obtain a matrix whose each line consists in a sum of products of the matrices and , and indeed the term is obtained on the line for . I do not see a clean way to proceed then as terms containing with do not vanish at the limit (and could blow in norm as ). Another thing I tried without success is the following: let and such that with . We look for and such that of the form where and . The idea was then to eliminate terms of order with an appropriate choice of and -- perhaps by setting in the series defining instead of . I could not manage to do it so far.","T (e_n)_{n\in\mathbb{Z}} T N\in\mathbb{N}^* \begin{align*}
T_N&:=(\langle e_m,Te_n\rangle)_{|m|,|n|\leq N}
\end{align*} \langle\cdot,\cdot\rangle 0 T T \mathrm{1}+K K T 0 0 \det(\mathrm{1}+K) T_N\to T N\to+\infty \begin{align*}
\lim_{N\to+\infty}\det((\langle e_m,Te_n\rangle)_{|m|,|n|\leq N})&=\det(T).
\end{align*} K K_N:=(\langle e_m,Ke_n\rangle)_{|m|,|n|\leq N} K K_N 1/4 K (K-\lambda)^{-1} \lambda K N\in\mathbb{N}^* N'>N A_N\equiv A\in\mathcal{M}_{2N+1}(\mathbb{C}) A K_N \begin{align*}
A'&:=\begin{pmatrix}E_1&E_2&E_3\\E_4&A&E_5\\E_6&E_7&E_8\end{pmatrix}\in\mathcal{M}_{2N'+1}(\mathbb{C})
\end{align*} E_1,E_3,E_6,E_8\in\mathcal{M}_{N'-N}(\mathbb{C}) E_2,E_7\in\mathcal{M}_{N'-N,2N+1}(\mathbb{C}) E_4,E_5\in\mathcal{M}_{2N+1,N'-N}(\mathbb{C}) \|\cdot\|_1 \|\cdot\|_\infty \leq\varepsilon \sum_{k}|E_j|_{kk}\leq\varepsilon K A' A \begin{align*}
&|(E_2)_{j,k}|\lesssim\frac{1}{(N'+j)^2};&&|(E_7)_{j,k}|\lesssim\frac{1}{(N+j)^2};\\
&|(E_4)_{j,k}|\lesssim\frac{1}{(N'+k)^2};&&|(E_5)_{j,k}|\lesssim\frac{1}{(N+k)^2};\\
&|(E_1)_{j,k}|\lesssim\frac{1}{(N'+j)^2},\frac{1}{(N'+k)^2};&&|(E_6)_{j,k}|\lesssim\frac{1}{(N'+k)^2},\frac{1}{(N+j)^2};\\
&|(E_3)_{j,k}|\lesssim\frac{1}{(N'+j)^2},\frac{1}{(N+k)^2};&&|(E_8)_{j,k}|\lesssim\frac{1}{(N+k)^2},\frac{1}{(N+j)^2}.
\end{align*} \begin{align*}
\lambda\mathrm{1}_{2N+1}&=\frac{1}{2\pi\mathrm{i}}\oint(A-\mu)^{-1}\mu\mathrm{d}\mu
\end{align*} \lambda A \delta(\varepsilon)>0 \begin{align*}
\mathrm{Spec}(A')\setminus\overline{D(0,1/4)}\subset\mathrm{Spec}(A)\setminus\overline{D(0,1/4)}+D(0,\delta(\varepsilon))\tag{\star}
\end{align*} \mathrm{Spec} \setminus\overline{D(0,1/4)} K \delta(\varepsilon) A \lim_{n\to+\infty}\|A^n\|^{\frac{1}{n}} A^n 3^n E_j A A^n k N'+1\leq k\leq N'+2N+1 A^\ell \ell\leq n-1 n\to+\infty n\to+\infty u\in\mathbb{C}^{2N+1} \lambda\in\mathbb{C} Au=\lambda u |\lambda|>1/4 u'\in\mathbb{C}^{2N'+1} \lambda'\in\mathbb{C} A'u'=\lambda'u' \begin{align*}
u'&=\sum_{k=0}^{+\infty}\varepsilon^ku_k,\qquad\qquad\lambda'=\sum_{k=0}^{+\infty}\varepsilon^k\lambda_k
\end{align*} u_0=(0,u,0) \lambda_0=\lambda \varepsilon^k u_k \lambda_k \varepsilon^{2k}\lambda_k \lambda' \varepsilon^k\lambda_k","['matrices', 'eigenvalues-eigenvectors']"
96,How to multiply certain analytic matrices so they behave like the identity near $0$?,How to multiply certain analytic matrices so they behave like the identity near ?,0,"I have the following parametrized matrix: $$ A(u,t) = \begin{bmatrix} \cos(t(u+s)) & \frac{\sin(t(u+s))}{u+s} \\ -(u+s)\sin(t(u+s)) & \cos(t(u+s))\end{bmatrix} \quad u \in \Bbb R\setminus \{0\} \text{ and } t > 0 $$ which is clearly entire in $s$ . For any given $N$ , my goal is to find, or at least prove the existence of, a sequence $\{(u_i,t_i)\}_{i=1}^M$ s.t. $$\prod_{i=1}^M A(u_i,t_i) =I+O(s^N)$$ I have been playing around with this problem and I have found numerical solutions up to $N=8$ . My hope is that there is some pattern I can recursively exploit to generate new solutions from old ones. It is easy to get rid of odd powers of $s$ because of the parity of their coefficients, but the even powers are more troublesome. By Taylor expanding, I can tell that this problem would be equivalent to solving a system of equations involving powers and trig functions, but I do not know enough about such systems to find my required sequence of $(u_i,t_i)$ . I looked online and it seems there is a link to the Matrix Membership problem, which is in general undecidable, but perhaps in this case, with such an explicit form for $A$ , something more can be said. Are there any references that I might find useful? Edit For example, for $N=3$ , note that, to second order, $$A(-1,t_1)A(1,4\pi)A(-1,4\pi-t_1) = I + \begin{bmatrix}-8\pi\sin\left(2t_1\right)&-8\pi\cos\left(2t_1\right)\\-8\pi\cos\left(2t_1\right)&8\pi\sin\left(2t_1\right) \end{bmatrix}s^2=I+B(t_1)s^2$$ and $B(t_1+\pi/2)=-B(t_1)$ , so by choosing $t_1=\pi$ , we have $$A(-1,\pi)A(1,4\pi)A(-1,3\pi)A(-1,3\pi/2)A(1,4\pi)A(-1,5\pi/2)=I+O(s^3)$$ I believe that in general, using $u_i$ 's with fixed absolute value should be possible, but I cannot prove it. By choosing $|u_i|=1$ and looking at the zeroth order term of the general product of $A(u_i,t_i)$ 's, it is easy to see that $\sum_i t_i \in 2\pi \Bbb N$ . My guess is that similar constraints apply to higher order terms, but getting explicit expressions for those is harder.","I have the following parametrized matrix: which is clearly entire in . For any given , my goal is to find, or at least prove the existence of, a sequence s.t. I have been playing around with this problem and I have found numerical solutions up to . My hope is that there is some pattern I can recursively exploit to generate new solutions from old ones. It is easy to get rid of odd powers of because of the parity of their coefficients, but the even powers are more troublesome. By Taylor expanding, I can tell that this problem would be equivalent to solving a system of equations involving powers and trig functions, but I do not know enough about such systems to find my required sequence of . I looked online and it seems there is a link to the Matrix Membership problem, which is in general undecidable, but perhaps in this case, with such an explicit form for , something more can be said. Are there any references that I might find useful? Edit For example, for , note that, to second order, and , so by choosing , we have I believe that in general, using 's with fixed absolute value should be possible, but I cannot prove it. By choosing and looking at the zeroth order term of the general product of 's, it is easy to see that . My guess is that similar constraints apply to higher order terms, but getting explicit expressions for those is harder.","
A(u,t) = \begin{bmatrix} \cos(t(u+s)) & \frac{\sin(t(u+s))}{u+s} \\ -(u+s)\sin(t(u+s)) & \cos(t(u+s))\end{bmatrix} \quad u \in \Bbb R\setminus \{0\} \text{ and } t > 0
 s N \{(u_i,t_i)\}_{i=1}^M \prod_{i=1}^M A(u_i,t_i) =I+O(s^N) N=8 s (u_i,t_i) A N=3 A(-1,t_1)A(1,4\pi)A(-1,4\pi-t_1) = I + \begin{bmatrix}-8\pi\sin\left(2t_1\right)&-8\pi\cos\left(2t_1\right)\\-8\pi\cos\left(2t_1\right)&8\pi\sin\left(2t_1\right) \end{bmatrix}s^2=I+B(t_1)s^2 B(t_1+\pi/2)=-B(t_1) t_1=\pi A(-1,\pi)A(1,4\pi)A(-1,3\pi)A(-1,3\pi/2)A(1,4\pi)A(-1,5\pi/2)=I+O(s^3) u_i |u_i|=1 A(u_i,t_i) \sum_i t_i \in 2\pi \Bbb N","['matrices', 'trigonometry', 'matrix-equations']"
97,Dimensions of Orthogonal Group Representations,Dimensions of Orthogonal Group Representations,,"I'm aware that the irreducible representations of the orthogonal group $O(n;\mathbb{C})$ are labeled by partitions $\lambda$ such that the sum of the first two columns of $\lambda$ is at most $n$ . Is there a way to determine the dimension of the $\lambda$ representation? Perhaps by counting some kind of tableau, similar to how we do for the symmetric group $S_n$ ?","I'm aware that the irreducible representations of the orthogonal group are labeled by partitions such that the sum of the first two columns of is at most . Is there a way to determine the dimension of the representation? Perhaps by counting some kind of tableau, similar to how we do for the symmetric group ?",O(n;\mathbb{C}) \lambda \lambda n \lambda S_n,"['matrices', 'representation-theory', 'integer-partitions', 'orthogonal-matrices']"
98,Is the spectral norm submultiplicative? [duplicate],Is the spectral norm submultiplicative? [duplicate],,"This question already has answers here : Show that the operator norm is submultiplicative (2 answers) Closed 3 years ago . I wonder if the $2$-norm or spectral norm is also submultiplicative for non-square matrices, i.e., $$\|  A B \|_2 \leq \| A \|_2 \cdot \| B \|_2$$ if the number of columns of $A$ coincides with the number of rows of $B$. In the literature I can only find a statement about square matrices. Thanks a lot for any remarks.","This question already has answers here : Show that the operator norm is submultiplicative (2 answers) Closed 3 years ago . I wonder if the $2$-norm or spectral norm is also submultiplicative for non-square matrices, i.e., $$\|  A B \|_2 \leq \| A \|_2 \cdot \| B \|_2$$ if the number of columns of $A$ coincides with the number of rows of $B$. In the literature I can only find a statement about square matrices. Thanks a lot for any remarks.",,"['matrices', 'normed-spaces', 'matrix-norms', 'spectral-norm']"
99,Singularity positive semidefinite,Singularity positive semidefinite,,The determinant of a matrix equals the product of its eigenvalues. A positive semidefinite matrix is a symmetric matrix with only nonnegative eigenvalues. A positive definite matrix is a symmetric matrix with only positive eigenvalues. Combining (1) and (3) yields that a positive definite matrix is always nonsingular since its determinant never becomes zero. Is is true that for a positive semidefinite matrix at least one of its eigenvalues equals zero and thus its determinant always equals zero => a positive semidefinite matrix is always singular? You would say that specifically having a positive semidefinite matrix instead of a positive definite matrix implies that at least one of the eigenvalues equals zero. Is this correct?,The determinant of a matrix equals the product of its eigenvalues. A positive semidefinite matrix is a symmetric matrix with only nonnegative eigenvalues. A positive definite matrix is a symmetric matrix with only positive eigenvalues. Combining (1) and (3) yields that a positive definite matrix is always nonsingular since its determinant never becomes zero. Is is true that for a positive semidefinite matrix at least one of its eigenvalues equals zero and thus its determinant always equals zero => a positive semidefinite matrix is always singular? You would say that specifically having a positive semidefinite matrix instead of a positive definite matrix implies that at least one of the eigenvalues equals zero. Is this correct?,,"['matrices', 'determinant', 'positive-definite', 'positive-semidefinite']"
