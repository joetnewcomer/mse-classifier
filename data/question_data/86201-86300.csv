,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Baby Rudin theorem 10.33 ( STOKES' THEOREM),Baby Rudin theorem 10.33 ( STOKES' THEOREM),,"The definitions which we need for the proof of the theorem. We define the standard simplex $Q^k$ to be the set of all $u$ $\in$ $R^k$ of the form $u$ = $\sum_{i=1}^k$ $\alpha_i$ $e_i$ . Assume now that $p_0$ , $p_1$ ,... $p_k$ are points of $R^n$ . The oriented affine $k$ -simplex $\sigma$ $=$ [ $p_0$ , $p_1$ ,... $p_k$ ] is defined to be the $k$ -surface in $R^n$ with parameter domain $Q^k$ which is given by the affine mapping $\sigma$ ( $\sum_{i=1}^k$ $\alpha_i$ $e_i$ ) $=$ $\sigma(u)$ $=p_0$ + $\sum_{i=1}^k$ $\alpha_i$ ( $p_i$ - $p_0$ ). Note that $\sigma$ is characterized by $\sigma(0)$ = $p_0$ , $\sigma(e_i)$ = $p_i$ (for $1$ $\leq$ $i$ $\leq$ $k$ ). For $k$ $\geq$ $1$ , the boundary of the oriented affine $k$ -simplex $\sigma$ $=$ [ $p_0$ , $p_1$ ,... $p_k$ ] is defined to be the affine ( $k-1$ )-chain $\partial$$\sigma$ = $\sum_{j=0}^k$ $(-1)^j$ [ $p_0$ ,..., $p_{j-1}$ , $p_{j+1}$ ,.., $p_k$ ]. For $1$ $\leq$ $j$ $\leq$ $k$ , observe that the simplex $\sigma_j$ = [ $p_0$ ,..., $p_{j-1}$ , $p_{j+1}$ ,.., $p_k$ ] has $Q^{k-1}$ as its parameter domain and that is defined by $\sigma_j(u)$ = $p_0$ + $Bu$ ( $u$ $\in$ $Q^{k-1}$ ) The class $\mathscr C'$ means the class of  continuously differentiable functions and etc. $x_j$ = ${ \begin{cases} {u_j (1 \leq j \lt r),} \\ {1 - (u_1 + ... + u_{k-1})  (j=r),} \\ {u_{j-1}  (r \lt j  \leq k). }  \end{cases} } $ ( this is $(98)$ ). $x_j$ = ${ \begin{cases} {u_j (1 \leq j \lt i),} \\ { 0  (j=i),} \\ {u_{j-1}  (i \lt j  \leq k). }  \end{cases} } $ . ( this is $(99)$ ). I dont'understand how do we get the $(98)$ and $(99)$ . Any help would be appreciated.","The definitions which we need for the proof of the theorem. We define the standard simplex to be the set of all of the form = . Assume now that , ,... are points of . The oriented affine -simplex [ , ,... ] is defined to be the -surface in with parameter domain which is given by the affine mapping ( ) + ( - ). Note that is characterized by = , = (for ). For , the boundary of the oriented affine -simplex [ , ,... ] is defined to be the affine ( )-chain = [ ,..., , ,.., ]. For , observe that the simplex = [ ,..., , ,.., ] has as its parameter domain and that is defined by = + ( ) The class means the class of  continuously differentiable functions and etc. = ( this is ). = . ( this is ). I dont'understand how do we get the and . Any help would be appreciated.","Q^k u \in R^k u \sum_{i=1}^k \alpha_i e_i p_0 p_1 p_k R^n k \sigma = p_0 p_1 p_k k R^n Q^k \sigma \sum_{i=1}^k \alpha_i e_i = \sigma(u) =p_0 \sum_{i=1}^k \alpha_i p_i p_0 \sigma \sigma(0) p_0 \sigma(e_i) p_i 1 \leq i \leq k k \geq 1 k \sigma = p_0 p_1 p_k k-1 \partial\sigma \sum_{j=0}^k (-1)^j p_0 p_{j-1} p_{j+1} p_k 1 \leq j \leq k \sigma_j p_0 p_{j-1} p_{j+1} p_k Q^{k-1} \sigma_j(u) p_0 Bu u \in Q^{k-1} \mathscr C' x_j { \begin{cases} {u_j (1 \leq j \lt r),} \\ {1 - (u_1 + ... + u_{k-1})  (j=r),} \\ {u_{j-1}  (r \lt j  \leq k). }  \end{cases} }  (98) x_j { \begin{cases} {u_j (1 \leq j \lt i),} \\ { 0  (j=i),} \\ {u_{j-1}  (i \lt j  \leq k). }  \end{cases} }  (99) (98) (99)","['real-analysis', 'calculus', 'functional-analysis', 'analysis', 'multivariable-calculus']"
1,Rademacher's integral with $sgn$ function,Rademacher's integral with  function,sgn,"Let $(r_n)_{n=1}^\infty$ be the sequence of Rademacher functions. Prove that for all positive integers $n_1<n_2<\cdots<n_k$ and $p_1,\dots,p_k$ , it holds that \begin{equation*}      \int_{0}^1r_{n_1}^{p_1}(t)\cdots r_{n_k}^{p_k}(t)dt=\left\{\begin{aligned}      &1, \text{ if each }p_j\text{ is even},\\      &0, \text{ otherwise.}     \end{aligned}\right.  \end{equation*} My attempt was to look at each $n$ th Rademacher's function: $$r_n(t)=sgn(\sin(2^n\pi t)),$$ for all $t\in[0,1]$ and \begin{equation*}     sgn:\mathbb{R}\to\mathbb{R},\text{ }\text{ }sgn(x)=\left\{\begin{aligned}         1&, &\text{ if }x>0,\\         0&, &\text{ if } x=0,\\         -1&, &\text{ if } x<0.     \end{aligned}\right. \end{equation*} So $$\int_0^1\displaystyle\prod_{j=1}^k[sgn(\sin(2^{n_j}\pi t))]^{p_j}dt$$ will be zero if at least for a $j_0$ , we have: $$\sin(2^{n_{j_0}}\pi t)=0.$$ It happens when $2^{n_{j_0}}\pi t=\pi l$ , for all $l\in\mathbb{Z}$ . Then $2^{n_{j_0}} t=l$ . Hence $2^{n_{j_0}}t$ is a integer number. I can't get anything done for the $p_j$ . Is there any other approach to this exercise?","Let be the sequence of Rademacher functions. Prove that for all positive integers and , it holds that My attempt was to look at each th Rademacher's function: for all and So will be zero if at least for a , we have: It happens when , for all . Then . Hence is a integer number. I can't get anything done for the . Is there any other approach to this exercise?","(r_n)_{n=1}^\infty n_1<n_2<\cdots<n_k p_1,\dots,p_k \begin{equation*}
     \int_{0}^1r_{n_1}^{p_1}(t)\cdots r_{n_k}^{p_k}(t)dt=\left\{\begin{aligned}
     &1, \text{ if each }p_j\text{ is even},\\
     &0, \text{ otherwise.}
    \end{aligned}\right. 
\end{equation*} n r_n(t)=sgn(\sin(2^n\pi t)), t\in[0,1] \begin{equation*}
    sgn:\mathbb{R}\to\mathbb{R},\text{ }\text{ }sgn(x)=\left\{\begin{aligned}
        1&, &\text{ if }x>0,\\
        0&, &\text{ if } x=0,\\
        -1&, &\text{ if } x<0.
    \end{aligned}\right.
\end{equation*} \int_0^1\displaystyle\prod_{j=1}^k[sgn(\sin(2^{n_j}\pi t))]^{p_j}dt j_0 \sin(2^{n_{j_0}}\pi t)=0. 2^{n_{j_0}}\pi t=\pi l l\in\mathbb{Z} 2^{n_{j_0}} t=l 2^{n_{j_0}}t p_j","['real-analysis', 'functional-analysis', 'analysis', 'rademacher-distribution']"
2,Adjoint system associated to Schauder basis,Adjoint system associated to Schauder basis,,"Let $X$ be reflexive Banach space or if you need one could assume that $X=H$ where $H$ is a usual separable Hilbert space and $\{e_n\}_{n = 1}^\infty$ be a Schauder basis in it which means for any x $\in X$ you have a unique representation $x = \sum_{n = 1} ^{\infty} x_ne_n$ . By saying that system $\{f_n\}_{n=1}^{\infty}$ is adjoint to $\{e_n\}_{n = 1}^{\infty}$ I mean a biorthogonal system associated with $\{e_n\}_{n=1}^{\infty}$ i.e. $(f_i, e_j) = \delta_{ij}$ for $\forall$ i, j. My question is would adjoint system $\{f_n\}_{n=1}^{\infty}$ associated to arbitrary Schauder basis $\{e_n\}_{n=1}^{\infty}$ be Schauder basis in $X^*$ (or in $H$ respectively) itself or not? I'll just add that answer is positive if you'd ask the same question for complete, minimal system or Riesz basis. Thank you for any advices and remarks.","Let be reflexive Banach space or if you need one could assume that where is a usual separable Hilbert space and be a Schauder basis in it which means for any x you have a unique representation . By saying that system is adjoint to I mean a biorthogonal system associated with i.e. for i, j. My question is would adjoint system associated to arbitrary Schauder basis be Schauder basis in (or in respectively) itself or not? I'll just add that answer is positive if you'd ask the same question for complete, minimal system or Riesz basis. Thank you for any advices and remarks.","X X=H H \{e_n\}_{n = 1}^\infty \in X x = \sum_{n = 1} ^{\infty} x_ne_n \{f_n\}_{n=1}^{\infty} \{e_n\}_{n = 1}^{\infty} \{e_n\}_{n=1}^{\infty} (f_i, e_j) = \delta_{ij} \forall \{f_n\}_{n=1}^{\infty} \{e_n\}_{n=1}^{\infty} X^* H","['functional-analysis', 'banach-spaces', 'schauder-basis']"
3,Prove these two projections are equivalent,Prove these two projections are equivalent,,"Problem: Let $e_1, e_2, f_1$ and $f_2$ be projections of $M$ which is a Von Neumann algebra, such that $e_1e_2=f_1f_2=0$ . If $e_1+e_2=f_1+f_2$ , $e_1 \sim e_2$ and $f_1 \sim f_2$ , prove that $e_1 \sim f_1$ . My attempt: Suppose $e_1$ and $f_1$ are not equivalent then by the comparison lemma there is a central projection $p$ such that either $pe_1\prec pf_1$ or $pf_1\prec pe_1$ (In this notation, for example $pe_1\prec pf_1$ means $pe_1$ is equivalent to a subprojection of $pf_1$ but $pe_1$ and $pf_1$ are not equivalent). Let's assume $pe_1\prec pf_1$ . Then we have: $$pe_2 \sim pe_1\prec pf_1\sim pf_2.$$ Thus $pe_2\prec pf_2$ . Moreover, since $p$ is central and $e_1e_2=f_1f_2=0$ , we infer that $pe_1+pe_2\preceq pf_1+pf_2$ . I wonder if we are allowed to replace $\preceq$ with $\prec$ in the last relation. At this stage, I don't know how to arrive at a contradiction while we already have $pe_1+pe_2=pf_1+pf_2$ . Any help would be highly appreciated.","Problem: Let and be projections of which is a Von Neumann algebra, such that . If , and , prove that . My attempt: Suppose and are not equivalent then by the comparison lemma there is a central projection such that either or (In this notation, for example means is equivalent to a subprojection of but and are not equivalent). Let's assume . Then we have: Thus . Moreover, since is central and , we infer that . I wonder if we are allowed to replace with in the last relation. At this stage, I don't know how to arrive at a contradiction while we already have . Any help would be highly appreciated.","e_1, e_2, f_1 f_2 M e_1e_2=f_1f_2=0 e_1+e_2=f_1+f_2 e_1 \sim e_2 f_1 \sim f_2 e_1 \sim f_1 e_1 f_1 p pe_1\prec pf_1 pf_1\prec pe_1 pe_1\prec pf_1 pe_1 pf_1 pe_1 pf_1 pe_1\prec pf_1 pe_2 \sim pe_1\prec pf_1\sim pf_2. pe_2\prec pf_2 p e_1e_2=f_1f_2=0 pe_1+pe_2\preceq pf_1+pf_2 \preceq \prec pe_1+pe_2=pf_1+pf_2","['functional-analysis', 'operator-theory', 'c-star-algebras', 'banach-algebras', 'von-neumann-algebras']"
4,A question about the proof of Bernstein-Type Lemmas,A question about the proof of Bernstein-Type Lemmas,,"In the book Bahouri, Hajer, Jean-Yves Chemin, and Raphaël Danchin. Fourier analysis and nonlinear partial differential equations. Vol. 343. Springer Science & Business Media, 2011. , In the proof of Lemma 2.1. there are the following inequalities $$ \begin{split} \left \| \partial^\alpha g  \right \|_{L^r} &\leq \left \| \partial^\alpha g  \right \|_{L^\infty} + \left \| \partial^\alpha g  \right \|_{L^1} \quad \color{Red}{\mathbf{(1)}}\\ &\leq C\left \| (1 + |\cdot|^2)^d \partial^\alpha g  \right \|_{L^\infty} \quad \color{Red}{\mathbf{(2)}}\\ &\leq C\left \| (\mathrm{Id} + \Delta)^d ((\cdot)^\alpha \phi)  \right \|_{L^1} \quad \color{Red}{\mathbf{(3)}}\\ &\leq C^{k+1} \quad \color{Red}{\mathbf{(4)}} \end{split} $$ where, $g = \mathcal{F}^{-1} \phi$ , $\phi$ is a function of $\mathcal{D}(\mathbb{R}^d)$ with value $1$ near $B = \{ξ ∈ \mathbb{R}^d : |ξ| ≤ R\}$ with $R > 0$ , $\mathcal{D}(\mathbb{R}^d)$ is the space of smooth compactly supported functions on $\mathbb{R}^d$ , $C$ is a constant, $1/r = 1-1/p+1/q$ with $p,q \in [1, \infty]$ . My questions are: for $\color{Red}{\mathbf{(1)}}$ , is it also true for the inequality $\left \| \partial^\alpha g  \right \|_{L^r} \leq \left \| \partial^\alpha g  \right \|_{L^1}$ ? I can understand from $\color{Red}{\mathbf{(1)}}$ to $\color{Red}{\mathbf{(2)}}$ . But I'm not clear from $\color{Red}{\mathbf{(2)}}$ to $\color{Red}{\mathbf{(3)}}$ . Why is it the same constant $C$ from $\color{Red}{\mathbf{(3)}}$ to $\color{Red}{\mathbf{(4)}}$ ?","In the book Bahouri, Hajer, Jean-Yves Chemin, and Raphaël Danchin. Fourier analysis and nonlinear partial differential equations. Vol. 343. Springer Science & Business Media, 2011. , In the proof of Lemma 2.1. there are the following inequalities where, , is a function of with value near with , is the space of smooth compactly supported functions on , is a constant, with . My questions are: for , is it also true for the inequality ? I can understand from to . But I'm not clear from to . Why is it the same constant from to ?","
\begin{split}
\left \| \partial^\alpha g  \right \|_{L^r} &\leq \left \| \partial^\alpha g  \right \|_{L^\infty} + \left \| \partial^\alpha g  \right \|_{L^1} \quad \color{Red}{\mathbf{(1)}}\\
&\leq C\left \| (1 + |\cdot|^2)^d \partial^\alpha g  \right \|_{L^\infty} \quad \color{Red}{\mathbf{(2)}}\\
&\leq C\left \| (\mathrm{Id} + \Delta)^d ((\cdot)^\alpha \phi)  \right \|_{L^1} \quad \color{Red}{\mathbf{(3)}}\\
&\leq C^{k+1} \quad \color{Red}{\mathbf{(4)}}
\end{split}
 g = \mathcal{F}^{-1} \phi \phi \mathcal{D}(\mathbb{R}^d) 1 B = \{ξ ∈ \mathbb{R}^d : |ξ| ≤ R\} R > 0 \mathcal{D}(\mathbb{R}^d) \mathbb{R}^d C 1/r = 1-1/p+1/q p,q \in [1, \infty] \color{Red}{\mathbf{(1)}} \left \| \partial^\alpha g  \right \|_{L^r} \leq \left \| \partial^\alpha g  \right \|_{L^1} \color{Red}{\mathbf{(1)}} \color{Red}{\mathbf{(2)}} \color{Red}{\mathbf{(2)}} \color{Red}{\mathbf{(3)}} C \color{Red}{\mathbf{(3)}} \color{Red}{\mathbf{(4)}}","['functional-analysis', 'fourier-analysis', 'normed-spaces']"
5,"Exercise 2.3.5 in Grafakos, Classical Fourier Analysis","Exercise 2.3.5 in Grafakos, Classical Fourier Analysis",,"I got stuck in Exercise $2.3.5$ (c) of Grafakos, Classical Fourier Analysis for a few days. Notation here $\mathcal{S}$ is the Schwartz Class. $\mathrm{C}_0^\infty$ means smooth functions with compact support. $\tau^y$ means translation by $y$ , that is, $(\tau^yf)(x):=f(x-y).$ $\partial_j$ means the partial derivative on the $j$ -th component. For multi-index $\beta=(\beta_1,\beta_2,\cdots,\beta_n)$ and $x=(x_1,\cdots,x_n)\in\mathbb{R}^n$ , $|\beta|:=\beta_1+\beta_2+\cdots+\beta_n,$ and $$\partial^\beta f(x):=\frac{\partial^{|\beta|}}{\partial^{\beta_1}_{x_1}\cdots\partial^{\beta_n}_{x_n}}f(x).$$ $e_j$ is  the elementary vector in $\mathbb{R}^n$ : $1$ on the $j$ -th component and $0$ on the others. And $e_j$ can also be viewed as a multi-index. $\hat{f}$ is the Fourier transform of $f$ . Problem here $2.3.5.$ Let $f\in\mathcal{S}(\mathbb{R}^n)$ and $\varphi\in\mathrm{C}_0^\infty(\mathbb{R}^n)$ be identically equal to 1 in a neighbourhood of the origin. Define $\varphi_k(x)=\varphi(x/k)$ as in the proof of Proposition $2.3.23$ . (a)Prove that $(\tau^{-he_j}f-f)/h\to\partial_jf$ in $\mathcal{S}$ as $h\to 0.$ (b)Prove that $\varphi_k f\to f$ in $\mathcal{S}$ as $k\to\infty.$ (c)Prove that the sequence $\varphi_k\widehat{\varphi_kf}$ converges to $\hat{f}$ in $\mathcal{S}$ as $k\to\infty.$ (It doesn't matter if you know nothing about that proposition. Grafakos left some details in that proposition as exercise here. And if you need further explanations for anything above, please comment to let me know.) The following is what I have done. To prove (a), for any multi-indices $\alpha,\beta$ , $x\in\mathbb{R}^n$ , $$|x^\alpha\partial^\beta(\frac{\tau^{-he_j}f(x)-f(x)}{h}-\partial_jf(x))|=|x^\alpha\cdot\frac{\partial^\beta f(x+he_j)-\partial^\beta f(x)-\partial^{\beta+e_j}f(x)h}{h}|$$ Use the mean value theorem and this semi-norm of $f$ in $\mathcal{S}$ : $\sup\limits_{x\in\mathbb{R}^n}|x^\alpha\partial^{\beta+2e_j}f(x)|$ to prove the result. $~$ To prove (b), suppose $\varphi$ equals to $1$ on $B(0,R):=\{x\in\mathbb{R}^n:|x|<R\}$ . Then for multi-indices $\alpha,\beta$ , we split the semi-norm into two parts after using Leibniz's rule: $$|x^\alpha\partial^\beta(\varphi_kf-f)|=|x^\alpha(\sum\limits_{0<\gamma\leq\beta}\tbinom{\beta}{\gamma} \partial^\gamma\varphi_k ~ \partial^{\beta-\gamma}f)+x^\alpha(\varphi_k-1)\partial^\beta f|.$$ The sum is taken over multi-index $\gamma$ , where (the partial order on multi-indices) $\gamma\leq\beta$ means $\gamma_j\leq\beta_j$ for every $j\in\{1,2,\cdots,n\}$ , and $\gamma>0$ means $\gamma\geq0$ as multi-index but $\gamma\not=0.$ Use the fact that $\varphi_k-1$ and $\partial^\gamma\varphi_k(\gamma>0)$ vanish on $B(0,kR)$ to help prove the result. $~$ Now prove (c). It is really tempting to prove (c) with (b), because (b) says that the sequence $\{\varphi_k\}_{k\geq1}$ approximates the identity map. So, it seems that $\widehat{\varphi_kf}$ is almost $\hat{f}$ . Then $\varphi_k\widehat{\varphi_kf}$ is almost $\varphi_k\hat{f}$ , and thus almost $\hat{f}.$ But actually it doesn't make sense! To be rigorous, consider the sequence with two indices: $\{\varphi_j(\widehat{\varphi_kf})\}_{k,j\geq1}$ . Fix any one of $k,j$ and let the other $\to\infty$ , for example fix $k$ and let $j\to\infty$ , we get the limit $\widehat{\varphi_kf}$ , then we let $k\to\infty$ to get $\hat{f}.$ The same result if we first let $k\to\infty$ then let $j\to\infty$ . With this intepretation, what we care about is the diagonal subsequence where $k=j$ . Let's simplify this problem. Consider an infinite diagonal matrix $(a_{j,k})_{j,k\geq1}$ , where all entries on the diagonal equal $1$ . Then the limit of every row and column regarded as a sequence is $0$ , and the limits of these two sequences of row/column limits are both $0$ . But the limit of the diagonal sequence is $1$ . They are not equal... $~$ Also, I have tried to split $x^\alpha\partial^\beta(\varphi_k\widehat{\varphi_kf})$ into two parts as I did in (b), but it points to the same ""diagonal sequence"" problem. So I want to know how to prove (c), or how to deal with this ""diagonal"" problem. Is there anything I've missed or any tool I need? Thanks in advance.","I got stuck in Exercise (c) of Grafakos, Classical Fourier Analysis for a few days. Notation here is the Schwartz Class. means smooth functions with compact support. means translation by , that is, means the partial derivative on the -th component. For multi-index and , and is  the elementary vector in : on the -th component and on the others. And can also be viewed as a multi-index. is the Fourier transform of . Problem here Let and be identically equal to 1 in a neighbourhood of the origin. Define as in the proof of Proposition . (a)Prove that in as (b)Prove that in as (c)Prove that the sequence converges to in as (It doesn't matter if you know nothing about that proposition. Grafakos left some details in that proposition as exercise here. And if you need further explanations for anything above, please comment to let me know.) The following is what I have done. To prove (a), for any multi-indices , , Use the mean value theorem and this semi-norm of in : to prove the result. To prove (b), suppose equals to on . Then for multi-indices , we split the semi-norm into two parts after using Leibniz's rule: The sum is taken over multi-index , where (the partial order on multi-indices) means for every , and means as multi-index but Use the fact that and vanish on to help prove the result. Now prove (c). It is really tempting to prove (c) with (b), because (b) says that the sequence approximates the identity map. So, it seems that is almost . Then is almost , and thus almost But actually it doesn't make sense! To be rigorous, consider the sequence with two indices: . Fix any one of and let the other , for example fix and let , we get the limit , then we let to get The same result if we first let then let . With this intepretation, what we care about is the diagonal subsequence where . Let's simplify this problem. Consider an infinite diagonal matrix , where all entries on the diagonal equal . Then the limit of every row and column regarded as a sequence is , and the limits of these two sequences of row/column limits are both . But the limit of the diagonal sequence is . They are not equal... Also, I have tried to split into two parts as I did in (b), but it points to the same ""diagonal sequence"" problem. So I want to know how to prove (c), or how to deal with this ""diagonal"" problem. Is there anything I've missed or any tool I need? Thanks in advance.","2.3.5 \mathcal{S} \mathrm{C}_0^\infty \tau^y y (\tau^yf)(x):=f(x-y). \partial_j j \beta=(\beta_1,\beta_2,\cdots,\beta_n) x=(x_1,\cdots,x_n)\in\mathbb{R}^n |\beta|:=\beta_1+\beta_2+\cdots+\beta_n, \partial^\beta f(x):=\frac{\partial^{|\beta|}}{\partial^{\beta_1}_{x_1}\cdots\partial^{\beta_n}_{x_n}}f(x). e_j \mathbb{R}^n 1 j 0 e_j \hat{f} f 2.3.5. f\in\mathcal{S}(\mathbb{R}^n) \varphi\in\mathrm{C}_0^\infty(\mathbb{R}^n) \varphi_k(x)=\varphi(x/k) 2.3.23 (\tau^{-he_j}f-f)/h\to\partial_jf \mathcal{S} h\to 0. \varphi_k f\to f \mathcal{S} k\to\infty. \varphi_k\widehat{\varphi_kf} \hat{f} \mathcal{S} k\to\infty. \alpha,\beta x\in\mathbb{R}^n |x^\alpha\partial^\beta(\frac{\tau^{-he_j}f(x)-f(x)}{h}-\partial_jf(x))|=|x^\alpha\cdot\frac{\partial^\beta f(x+he_j)-\partial^\beta f(x)-\partial^{\beta+e_j}f(x)h}{h}| f \mathcal{S} \sup\limits_{x\in\mathbb{R}^n}|x^\alpha\partial^{\beta+2e_j}f(x)| ~ \varphi 1 B(0,R):=\{x\in\mathbb{R}^n:|x|<R\} \alpha,\beta |x^\alpha\partial^\beta(\varphi_kf-f)|=|x^\alpha(\sum\limits_{0<\gamma\leq\beta}\tbinom{\beta}{\gamma} \partial^\gamma\varphi_k ~ \partial^{\beta-\gamma}f)+x^\alpha(\varphi_k-1)\partial^\beta f|. \gamma \gamma\leq\beta \gamma_j\leq\beta_j j\in\{1,2,\cdots,n\} \gamma>0 \gamma\geq0 \gamma\not=0. \varphi_k-1 \partial^\gamma\varphi_k(\gamma>0) B(0,kR) ~ \{\varphi_k\}_{k\geq1} \widehat{\varphi_kf} \hat{f} \varphi_k\widehat{\varphi_kf} \varphi_k\hat{f} \hat{f}. \{\varphi_j(\widehat{\varphi_kf})\}_{k,j\geq1} k,j \to\infty k j\to\infty \widehat{\varphi_kf} k\to\infty \hat{f}. k\to\infty j\to\infty k=j (a_{j,k})_{j,k\geq1} 1 0 0 1 ~ x^\alpha\partial^\beta(\varphi_k\widehat{\varphi_kf})","['real-analysis', 'functional-analysis', 'fourier-analysis', 'harmonic-analysis']"
6,$A$ has property of Baire if and only if $A=B \sqcup Q$ where $B$ is a $G_{\delta}$ set and $Q$ is of first category,has property of Baire if and only if  where  is a  set and  is of first category,A A=B \sqcup Q B G_{\delta} Q,"Over $\Bbb{R}$ , A set $A$ with a property of Baire is defined in our notes as the symmetric difference $A=G\triangle Q$ where $G$ is open and $Q$ is of first category. I am asked to show that a set $A$ has property of Baire if and only if $A=B \sqcup Q$ where $B$ is a $G_{\delta}$ set and $Q$ is of first category. I can show that for $A=G\triangle Q$ with open $G$ and first-category $Q$ , one gets $N=\overline{G}\setminus G$ is closed (and nowhere dense), and $N\triangle Q$ is of first category, hence $A={\overline{G}\triangle N \triangle Q}=\overline{G}\triangle (N \triangle Q)$ which means it is also a symmetric difference of closed set and a first category set. But for the countable intersection of open sets $B=\bigcap B_i$ , I am not sure what to do similar to the above method. Is $B$ nowhere dense? If so, then $A $ is a first category set, but is it still the symmetric difference between an open set and a first-category set? I am not sure what to say about such $B$ .","Over , A set with a property of Baire is defined in our notes as the symmetric difference where is open and is of first category. I am asked to show that a set has property of Baire if and only if where is a set and is of first category. I can show that for with open and first-category , one gets is closed (and nowhere dense), and is of first category, hence which means it is also a symmetric difference of closed set and a first category set. But for the countable intersection of open sets , I am not sure what to do similar to the above method. Is nowhere dense? If so, then is a first category set, but is it still the symmetric difference between an open set and a first-category set? I am not sure what to say about such .",\Bbb{R} A A=G\triangle Q G Q A A=B \sqcup Q B G_{\delta} Q A=G\triangle Q G Q N=\overline{G}\setminus G N\triangle Q A={\overline{G}\triangle N \triangle Q}=\overline{G}\triangle (N \triangle Q) B=\bigcap B_i B A  B,"['real-analysis', 'functional-analysis', 'solution-verification', 'baire-category']"
7,Monotone approximation of elements in AF-algebras and $C^*$-algebras,Monotone approximation of elements in AF-algebras and -algebras,C^*,"Suppose that we are given an AF-algebra $A$ and a sequence of finite-dimensional subalgebras $\mathbb{C}=A_0\subset A_1\subset A_2\subset\ldots$ such that $A=\overline{\bigcup\limits_{n\geq 0}A_n}$ . Let me denote this dense subalgebra of $A$ by $A^{LS}$ , i.e. $A^{LS}= \bigcup\limits_{n\geq 0}A_n$ . Next, we define the positive elements as $$A^+=\left\{h^2\ \middle|\ h\in A,\ h^*=h \right\}$$ and $$(A^{LS})^+=\left\{h^2\ \middle|\ h\in A^{LS},\ h^*=h \right\}.$$ Then for any $y\in A^+$ we can find a sequence $\{y_n\}_{n\geq 1}\subset (A^{LS})^+$ such that $\lim\limits_{n\to\infty}y_n=y$ . Questions: Is it true that we can pick this $\{y_n\}$ in such a way that $y_n\leq y_{n+1}$ for any $n$ with respect to the partial order $\leq$ on $A^{LS}$ defined by $(A^{LS})^+$ ? More generally. Suppose we are given a unital separable $C^*$ -algebra $A$ with a unital dense $*$ -subalgebra $\widetilde{A}\subset A$ . Is it true that any $y\in A^+$ can be approximated by a monotone (in the sence of the partial order defined by positive elements) sequence of elements from $\left(\widetilde{A}\right)^+$ ? Is it possible at least for any $y\in A^+$ and any open neighbourhood of $y$ find an element $z\in \left(\widetilde{A}\right)^+$ such that $z$ belongs to that neighbourhood and $z\leq y$ ? Thanks in advance!","Suppose that we are given an AF-algebra and a sequence of finite-dimensional subalgebras such that . Let me denote this dense subalgebra of by , i.e. . Next, we define the positive elements as and Then for any we can find a sequence such that . Questions: Is it true that we can pick this in such a way that for any with respect to the partial order on defined by ? More generally. Suppose we are given a unital separable -algebra with a unital dense -subalgebra . Is it true that any can be approximated by a monotone (in the sence of the partial order defined by positive elements) sequence of elements from ? Is it possible at least for any and any open neighbourhood of find an element such that belongs to that neighbourhood and ? Thanks in advance!","A \mathbb{C}=A_0\subset A_1\subset A_2\subset\ldots A=\overline{\bigcup\limits_{n\geq 0}A_n} A A^{LS} A^{LS}= \bigcup\limits_{n\geq 0}A_n A^+=\left\{h^2\ \middle|\ h\in A,\ h^*=h \right\} (A^{LS})^+=\left\{h^2\ \middle|\ h\in A^{LS},\ h^*=h \right\}. y\in A^+ \{y_n\}_{n\geq 1}\subset (A^{LS})^+ \lim\limits_{n\to\infty}y_n=y \{y_n\} y_n\leq y_{n+1} n \leq A^{LS} (A^{LS})^+ C^* A * \widetilde{A}\subset A y\in A^+ \left(\widetilde{A}\right)^+ y\in A^+ y z\in \left(\widetilde{A}\right)^+ z z\leq y","['functional-analysis', 'operator-algebras', 'c-star-algebras']"
8,Questions about infinite dimensional normed spaces,Questions about infinite dimensional normed spaces,,"Problem Show that if $X$ is an infinite-dimensional normed vector space, then there is a sequence $\{x_n\}$ such that $\|x_n\|=1,\|x_n-x_m\|=1\ (n\neq m)$ ． My view I have confirmed that the following two theorems hold. Theorem1 :If $M$ be a close subspace of a normed vector space $X$ which is $X\neq M$ , then for all $\varepsilon>0$ there is $u_{\varepsilon}\in X$ such that $\|u_{\varepsilon}\|=1,d(u,M)>1-\varepsilon$ ． Theorem2 :By Theorem 1, if $M$ is finite dimensional, we can get $u_0\in X$ such that $\|u_0\|=1,d(u_0,M)\geq1$ . I think I will use these theorems, but I can't think of a specific way to do it. Also, from a geometric point of view, if $X$ is one-dimensional (line), we can get $x_1\in X$ where $\|x_1\|=1$ . If $X$ is two-dimensional (plane), then by drawing an equilateral triangle with one point at the origin, we can obtain $x_1,x_2\in X$ such that $\|x_i\|=1\ (i=1,2),\|x_1-x_2\|=1$ . Furthermore, if $X$ is three-dimensional (space), by considering a regular square pyramid with a single point at the origin, we can obtain $x_1,x_2,x_3\in X$ such that $\|x_i\|=1\ (i=1,2,3),\|x_i-x_j\|=1\ (i\neq j)$ . I was wondering if I could do the same thing in higher dimensions, so that I could do the same thing in infinite dimensional space, but it didn't work. Postscript (Nov. 11, 2021) I still haven't solved this problem yet, so I'll describe another method that I've newly thought of. My view (New) Take $x_1\in X$ such that $\|x_1\|=1$ , and let $M_1$ be a finite-dimensional subspace of $X$ spaned by $x_1$ . From Theorem 2 , we obtain $x_2\in X$ such that $\|x_2\|=1$ and $d(x_2,M_1)\geq 1$ . Next, let $M_2$ be a finite dimensional subspace of $X$ spaned by $x_1$ and $x_2$ . From Theorem 2 , we obtain $x_3\in X$ such that $\|x_3\|=1$ and $d(x_3,M_2)\geq 1$ . Continuing this work, let $M_n$ be a finite dimensional subspace of $X$ spaned by $x_1,\cdots,x_n$ . From Theorem 2 , we obtain $X_{n+1}\in X$ such that $\|x_{n+1}\|=1$ and $d(x_{n+1},M_n)\geq 1$ . Assume that this task is completed in a finite number of times. In this case, $X$ is a finite dimensional space, which is a contradiction. Therefore, this work can be done infinitely many times. In this case, $\{x_n\}_{n=1}^{\infty}$ satisfies $\|x_n\|=1\ (n=1,2,\cdots)$ , and from $d(x_{n+1},M_n)\geq 1$ , we can say $1\leq\|x_n-x_m\|$ for $n\neq m$ . My Question Now if I can show that $\|x_n-x_m\|\leq 1\ (n\neq m)$ , I think the proof is done. Do you have any better ideas?","Problem Show that if is an infinite-dimensional normed vector space, then there is a sequence such that ． My view I have confirmed that the following two theorems hold. Theorem1 :If be a close subspace of a normed vector space which is , then for all there is such that ． Theorem2 :By Theorem 1, if is finite dimensional, we can get such that . I think I will use these theorems, but I can't think of a specific way to do it. Also, from a geometric point of view, if is one-dimensional (line), we can get where . If is two-dimensional (plane), then by drawing an equilateral triangle with one point at the origin, we can obtain such that . Furthermore, if is three-dimensional (space), by considering a regular square pyramid with a single point at the origin, we can obtain such that . I was wondering if I could do the same thing in higher dimensions, so that I could do the same thing in infinite dimensional space, but it didn't work. Postscript (Nov. 11, 2021) I still haven't solved this problem yet, so I'll describe another method that I've newly thought of. My view (New) Take such that , and let be a finite-dimensional subspace of spaned by . From Theorem 2 , we obtain such that and . Next, let be a finite dimensional subspace of spaned by and . From Theorem 2 , we obtain such that and . Continuing this work, let be a finite dimensional subspace of spaned by . From Theorem 2 , we obtain such that and . Assume that this task is completed in a finite number of times. In this case, is a finite dimensional space, which is a contradiction. Therefore, this work can be done infinitely many times. In this case, satisfies , and from , we can say for . My Question Now if I can show that , I think the proof is done. Do you have any better ideas?","X \{x_n\} \|x_n\|=1,\|x_n-x_m\|=1\ (n\neq m) M X X\neq M \varepsilon>0 u_{\varepsilon}\in X \|u_{\varepsilon}\|=1,d(u,M)>1-\varepsilon M u_0\in X \|u_0\|=1,d(u_0,M)\geq1 X x_1\in X \|x_1\|=1 X x_1,x_2\in X \|x_i\|=1\ (i=1,2),\|x_1-x_2\|=1 X x_1,x_2,x_3\in X \|x_i\|=1\ (i=1,2,3),\|x_i-x_j\|=1\ (i\neq j) x_1\in X \|x_1\|=1 M_1 X x_1 x_2\in X \|x_2\|=1 d(x_2,M_1)\geq 1 M_2 X x_1 x_2 x_3\in X \|x_3\|=1 d(x_3,M_2)\geq 1 M_n X x_1,\cdots,x_n X_{n+1}\in X \|x_{n+1}\|=1 d(x_{n+1},M_n)\geq 1 X \{x_n\}_{n=1}^{\infty} \|x_n\|=1\ (n=1,2,\cdots) d(x_{n+1},M_n)\geq 1 1\leq\|x_n-x_m\| n\neq m \|x_n-x_m\|\leq 1\ (n\neq m)","['functional-analysis', 'vector-spaces', 'normed-spaces']"
9,Checking the Folner condition only on a set of generators,Checking the Folner condition only on a set of generators,,"Let $G$ be a countable, discrete group with a set of generators $S$ . Let $F_n\subset G$ be a sequence of finite sets that satisfies $$\frac{|sF_n\triangle F_n|}{|F_n|}\to0$$ for all $s\in S$ . Is it true then that $F_n$ is a Folner sequence? In other words, is it true that $$\frac{|gF_n\triangle F_n|}{|F_n|}\to0$$ for all $g\in G$ ? In particular, does it suffice to check the Folner condition on the generators instead of the entire group? Let's set $\alpha_n(g)=\frac{|gF_n\triangle F_n|}{|F_n|}$ . I believe that this question boils down to showing that (1) if $\alpha_n(g)\to0$ then $\alpha_n(g^{-1})\to0$ and that (2) if $\alpha_n(g)\to0$ and $\alpha_n(h)\to0$ then $\alpha_n(gh)\to0$ . For (2) I observed that $$\alpha_n(gh)=\frac{|ghF_n\triangle F_n|}{|F_n|}=\frac{|(ghF_n\triangle hF_n)\triangle(hF_n\triangle F_n)|}{|F_n|}\leq\frac{|ghF_n\triangle hF_n|}{|F_n|}+\alpha_n(h)=$$ $$=\frac{|g(hF_n)\triangle hF_n|}{|hF_n|}+\alpha_n(h)$$ and I would be done if I could get some estimate of the form $|ghF_n\triangle hF_n|\leq C|gF_n\triangle F_n|$ , but this seems impossible to me. Such an estimate would also be enough to show (1). Any hint or reference is appreciated.","Let be a countable, discrete group with a set of generators . Let be a sequence of finite sets that satisfies for all . Is it true then that is a Folner sequence? In other words, is it true that for all ? In particular, does it suffice to check the Folner condition on the generators instead of the entire group? Let's set . I believe that this question boils down to showing that (1) if then and that (2) if and then . For (2) I observed that and I would be done if I could get some estimate of the form , but this seems impossible to me. Such an estimate would also be enough to show (1). Any hint or reference is appreciated.",G S F_n\subset G \frac{|sF_n\triangle F_n|}{|F_n|}\to0 s\in S F_n \frac{|gF_n\triangle F_n|}{|F_n|}\to0 g\in G \alpha_n(g)=\frac{|gF_n\triangle F_n|}{|F_n|} \alpha_n(g)\to0 \alpha_n(g^{-1})\to0 \alpha_n(g)\to0 \alpha_n(h)\to0 \alpha_n(gh)\to0 \alpha_n(gh)=\frac{|ghF_n\triangle F_n|}{|F_n|}=\frac{|(ghF_n\triangle hF_n)\triangle(hF_n\triangle F_n)|}{|F_n|}\leq\frac{|ghF_n\triangle hF_n|}{|F_n|}+\alpha_n(h)= =\frac{|g(hF_n)\triangle hF_n|}{|hF_n|}+\alpha_n(h) |ghF_n\triangle hF_n|\leq C|gF_n\triangle F_n|,"['functional-analysis', 'group-theory', 'amenability']"
10,Elementary tensors of pure states separate the points of the spatial tensor product,Elementary tensors of pure states separate the points of the spatial tensor product,,"Let $A,B$ be two $C^*$ -algebras. Given two states $\phi\in S(A),\psi\in S(B)$ one defines the elementary tensor state $\phi\odot\psi:A\odot B\to\mathbb{C}$ as the unique linear map satisfying $\phi\odot\psi(a\otimes b)=\phi(a)\psi(b)$ on elementary tensors. One proves with elementary arguments that all such functionals are algebraically positive and continuous with respect to the spatial norm (and by Takesaki's theorem w.r.t any $C^*$ -norm on $A\odot B$ ). Denote the extension of $\phi\odot\psi$ on a state on $A\otimes B$ (the spatial TP) by $\phi\otimes\psi\in S(A\otimes B)$ . Let $0\neq x\in A\otimes B$ . It is easily proven that there exist states $\phi\in S(A)$ and $\psi\in S(B)$ so that $\phi\otimes\psi(x^*x)>0$ . Here is a simple argument: As seen in Murphy, we have that $$\|x^*x\|_{\min}=\sup_{\tau\in S(A),\rho\in S(B)}\|\pi_\tau\otimes\pi_\rho(x^*x)\|_{B(H_\tau\otimes H_\rho)}$$ where $(H_\tau,\pi_\tau,\xi_\tau)$ is the GNS triplet. So we find a pair of states $\tau,\rho$ such that $\|\pi_\tau\otimes\pi_\rho((x^*x)^{1/2})\|\neq0$ , i.e. $\pi_\tau\otimes\pi_\rho((x^*x)^{1/2})\neq0$ . This thing here is an operator on $H_\tau\otimes H_\rho$ , i.e. $\pi_\tau\otimes\pi_\rho((x^*x)^{1/2})\in B(H_\tau\otimes H_\rho)$ , and as it is non-zero, we can find unit vectors $\xi_1\in H_\tau,\xi_2\in H_\rho$ such that $\pi_\tau\otimes\pi_\rho((x^*x)^{1/2})(\xi_1\otimes\xi_2)\neq0$ . Simply set $\phi(a)=\langle\pi_\tau(a)\xi_1,\xi_1\rangle$ and $\psi(b)=\langle\pi_\rho(b)\xi_2,\xi_2\rangle$ . These are obviously states on $A$ and $B$ , and $\phi\otimes\psi(x^*x)\neq0$ . Here is my question : apparently it is also true that the elementary tensors of pure states separate the positive elements, i.e. given $0\neq x\in (A\otimes B)_+$ , we can find pure states $\phi\in\text{PS}(A),\psi\in\text{PS}(B)$ such that $\phi\otimes\psi(x)\neq0$ . How can this be proved, or where can I find a proof? Here are the things I tried With elementary arguments as those presented in Murphy's book, I was able to show that we also have $$\|x\|_{\min}=\sup_{\tau\in\text{PS}(A),\rho\in\text{PS}(B)}\|\pi_\tau\otimes\pi_\rho(x)\|_{B(H_\tau\otimes H_\rho)}$$ the proof is the same but one has to observe first that the direct sum of the GNS representations for pure states $\bigoplus_{\tau\in\text{PS}(A)}\pi_\tau:A\to\mathbb{B}(\bigoplus_{\tau\in\text{PS}(A)}H_\tau)$ are faithful. The thing is that, if I try to mimic the above proof, the rest of the proof gets messed up: If I knew that the vectors $\xi_1,\xi_2$ found earlier are the canonical cyclic vectors, then everything would work out like a charm since the resulting states $\phi,\psi$ would actually be $\tau$ and $\rho$ which would be pure this time. But of course this is too much to expect I guess. Also, one can show (similarly to the other equations) that $$\|x^*x\|_{\min}=\sup\bigg\{\frac{\tau\otimes\rho(y^*x^*xy)}{\tau\otimes\rho(y^*y)}:\tau\in\text{PS}(A),\rho\in\text{PS}(B),y\in A\odot B\text{ s.t. }\tau\otimes\rho(y^*y)\neq0\bigg\}.$$ So we could start like this: $x^*x\neq0$ , so we get pure states $\tau,\rho$ and some $y\in A\odot B$ such that $\tau\otimes\rho(y^*x^*xy)\neq0$ . And everything would be perfect if there wasn't this $y$ appearing here. Another approach would be this product states on the tensor product *-algebra using the so-called slice map, but I want to consider elements of the entire $C^*$ -tensor product and not just the algebraic TP. That means that (among many other things) I'd have to extend the slice map to the spatial TP and I don't see this happening at the moment. edit: of course slice maps extend to the spatial tp, this is a simple corollary of the fact that the TP of two completely positive maps is again c.p.","Let be two -algebras. Given two states one defines the elementary tensor state as the unique linear map satisfying on elementary tensors. One proves with elementary arguments that all such functionals are algebraically positive and continuous with respect to the spatial norm (and by Takesaki's theorem w.r.t any -norm on ). Denote the extension of on a state on (the spatial TP) by . Let . It is easily proven that there exist states and so that . Here is a simple argument: As seen in Murphy, we have that where is the GNS triplet. So we find a pair of states such that , i.e. . This thing here is an operator on , i.e. , and as it is non-zero, we can find unit vectors such that . Simply set and . These are obviously states on and , and . Here is my question : apparently it is also true that the elementary tensors of pure states separate the positive elements, i.e. given , we can find pure states such that . How can this be proved, or where can I find a proof? Here are the things I tried With elementary arguments as those presented in Murphy's book, I was able to show that we also have the proof is the same but one has to observe first that the direct sum of the GNS representations for pure states are faithful. The thing is that, if I try to mimic the above proof, the rest of the proof gets messed up: If I knew that the vectors found earlier are the canonical cyclic vectors, then everything would work out like a charm since the resulting states would actually be and which would be pure this time. But of course this is too much to expect I guess. Also, one can show (similarly to the other equations) that So we could start like this: , so we get pure states and some such that . And everything would be perfect if there wasn't this appearing here. Another approach would be this product states on the tensor product *-algebra using the so-called slice map, but I want to consider elements of the entire -tensor product and not just the algebraic TP. That means that (among many other things) I'd have to extend the slice map to the spatial TP and I don't see this happening at the moment. edit: of course slice maps extend to the spatial tp, this is a simple corollary of the fact that the TP of two completely positive maps is again c.p.","A,B C^* \phi\in S(A),\psi\in S(B) \phi\odot\psi:A\odot B\to\mathbb{C} \phi\odot\psi(a\otimes b)=\phi(a)\psi(b) C^* A\odot B \phi\odot\psi A\otimes B \phi\otimes\psi\in S(A\otimes B) 0\neq x\in A\otimes B \phi\in S(A) \psi\in S(B) \phi\otimes\psi(x^*x)>0 \|x^*x\|_{\min}=\sup_{\tau\in S(A),\rho\in S(B)}\|\pi_\tau\otimes\pi_\rho(x^*x)\|_{B(H_\tau\otimes H_\rho)} (H_\tau,\pi_\tau,\xi_\tau) \tau,\rho \|\pi_\tau\otimes\pi_\rho((x^*x)^{1/2})\|\neq0 \pi_\tau\otimes\pi_\rho((x^*x)^{1/2})\neq0 H_\tau\otimes H_\rho \pi_\tau\otimes\pi_\rho((x^*x)^{1/2})\in B(H_\tau\otimes H_\rho) \xi_1\in H_\tau,\xi_2\in H_\rho \pi_\tau\otimes\pi_\rho((x^*x)^{1/2})(\xi_1\otimes\xi_2)\neq0 \phi(a)=\langle\pi_\tau(a)\xi_1,\xi_1\rangle \psi(b)=\langle\pi_\rho(b)\xi_2,\xi_2\rangle A B \phi\otimes\psi(x^*x)\neq0 0\neq x\in (A\otimes B)_+ \phi\in\text{PS}(A),\psi\in\text{PS}(B) \phi\otimes\psi(x)\neq0 \|x\|_{\min}=\sup_{\tau\in\text{PS}(A),\rho\in\text{PS}(B)}\|\pi_\tau\otimes\pi_\rho(x)\|_{B(H_\tau\otimes H_\rho)} \bigoplus_{\tau\in\text{PS}(A)}\pi_\tau:A\to\mathbb{B}(\bigoplus_{\tau\in\text{PS}(A)}H_\tau) \xi_1,\xi_2 \phi,\psi \tau \rho \|x^*x\|_{\min}=\sup\bigg\{\frac{\tau\otimes\rho(y^*x^*xy)}{\tau\otimes\rho(y^*y)}:\tau\in\text{PS}(A),\rho\in\text{PS}(B),y\in A\odot B\text{ s.t. }\tau\otimes\rho(y^*y)\neq0\bigg\}. x^*x\neq0 \tau,\rho y\in A\odot B \tau\otimes\rho(y^*x^*xy)\neq0 y C^*","['functional-analysis', 'operator-algebras', 'c-star-algebras']"
11,Gluing two functions from Sobolev spaces.,Gluing two functions from Sobolev spaces.,,"I am studying Galdi's Introduction to the mathematical theory of Navier-Stokes equations and somewhere in a proof, he ""glued"" two functions in $W^{1, 2}(\Omega_1)$ and $W^{1, 2}(\Omega_2)$ to obtain a function in $W^{1, 2}(\Omega_1 \cup \Omega_2)$ and I don't really understand why it is true. Let me explain it in more details. Let $B_1$ and $B_2$ be the open balls in $\mathbb R^n$ of radius $1$ and $2$ respectively. Let $\Omega_1 = B_1$ and $\Omega_2 = B_2 \cap (\overline{B_1}^c)$ . We consider $\boldsymbol{u}_1 \in W^{1, 2}(\Omega_1)$ and $\boldsymbol{u}_2 \in W^{1, 2}(\Omega_2)$ (where $\boldsymbol{u}= (u^1, \ldots, u^m)$ and $\boldsymbol{u}_i \in W^{1, 2}(\Omega_i)$ actually means $\boldsymbol{u}_i \in [W^{1, 2}(\Omega_i)]^m$ ) verifying $$\nabla \cdot \boldsymbol{u}_i = 0 ~~\text{in }\Omega_i \quad \text{and} \quad \boldsymbol{u}_1 = \boldsymbol{u}_2 ~~~\text{at }\partial \Omega_1,$$ in the trace sens. My question is, if we consider $$\boldsymbol{u}: x \in \overline{\Omega_1} \cup \Omega_2 = \Omega \mapsto  \begin{cases} \boldsymbol{u}_1(x) & \text{if } x \in \Omega_1\\ \boldsymbol{u}_2(x) & \text{if } x \in \Omega_2\\  \end{cases}, $$ is that true that $\boldsymbol{u} \in W^{1, 2}(\overline{\Omega_1} \cup \Omega_2)$ and that $\nabla \cdot \boldsymbol{u} = 0$ in $\overline{\Omega_1} \cup \Omega_2 $ ? I don't really see why this should be true because if we take $\boldsymbol{\psi} \in C^\infty_0(\overline{\Omega_1} \cup \Omega_2)$ , we should be able to show that $$\int_{\overline{\Omega_1} \cup \Omega_2} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi} = - \int_{\overline{\Omega_1} \cup \Omega_2} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi}$$ but $$\int_{\overline{\Omega_1} \cup \Omega_2} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi} = \int_{\Omega_1} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi}  + \int_{\Omega_2} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi} \stackrel{?}{=}-\int_{\Omega_1} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi}  - \int_{\Omega_2} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi} = -\int_{\overline{\Omega_1} \cup \Omega_2} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi} $$ where I don't see why the second equality could be satisfied as $\boldsymbol{\psi}|_{\Omega_i} \notin C^\infty_0(\Omega_i)$ . Could someone help me with this ?","I am studying Galdi's Introduction to the mathematical theory of Navier-Stokes equations and somewhere in a proof, he ""glued"" two functions in and to obtain a function in and I don't really understand why it is true. Let me explain it in more details. Let and be the open balls in of radius and respectively. Let and . We consider and (where and actually means ) verifying in the trace sens. My question is, if we consider is that true that and that in ? I don't really see why this should be true because if we take , we should be able to show that but where I don't see why the second equality could be satisfied as . Could someone help me with this ?","W^{1, 2}(\Omega_1) W^{1, 2}(\Omega_2) W^{1, 2}(\Omega_1 \cup \Omega_2) B_1 B_2 \mathbb R^n 1 2 \Omega_1 = B_1 \Omega_2 = B_2 \cap (\overline{B_1}^c) \boldsymbol{u}_1 \in W^{1, 2}(\Omega_1) \boldsymbol{u}_2 \in W^{1, 2}(\Omega_2) \boldsymbol{u}= (u^1, \ldots, u^m) \boldsymbol{u}_i \in W^{1, 2}(\Omega_i) \boldsymbol{u}_i \in [W^{1, 2}(\Omega_i)]^m \nabla \cdot \boldsymbol{u}_i = 0 ~~\text{in }\Omega_i \quad \text{and} \quad \boldsymbol{u}_1 = \boldsymbol{u}_2 ~~~\text{at }\partial \Omega_1, \boldsymbol{u}: x \in \overline{\Omega_1} \cup \Omega_2 = \Omega \mapsto 
\begin{cases}
\boldsymbol{u}_1(x) & \text{if } x \in \Omega_1\\
\boldsymbol{u}_2(x) & \text{if } x \in \Omega_2\\ 
\end{cases},
 \boldsymbol{u} \in W^{1, 2}(\overline{\Omega_1} \cup \Omega_2) \nabla \cdot \boldsymbol{u} = 0 \overline{\Omega_1} \cup \Omega_2  \boldsymbol{\psi} \in C^\infty_0(\overline{\Omega_1} \cup \Omega_2) \int_{\overline{\Omega_1} \cup \Omega_2} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi} = - \int_{\overline{\Omega_1} \cup \Omega_2} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi} \int_{\overline{\Omega_1} \cup \Omega_2} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi} = \int_{\Omega_1} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi}  + \int_{\Omega_2} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi} \stackrel{?}{=}-\int_{\Omega_1} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi}  - \int_{\Omega_2} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi} = -\int_{\overline{\Omega_1} \cup \Omega_2} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi}  \boldsymbol{\psi}|_{\Omega_i} \notin C^\infty_0(\Omega_i)","['functional-analysis', 'sobolev-spaces', 'weak-derivatives']"
12,Abstract Wiener Space for $\ell^2(\mathbb{R})$,Abstract Wiener Space for,\ell^2(\mathbb{R}),"Let $\ell^2(\mathbb{R})$ denote the space of square-summable real-valued sequences equipped with the inner product $$ \langle x, y \rangle = \sum_{n = 1}^{\infty} x_n y_n $$ Let $\nu$ denote its canonical cylinder measure i.e. a cylinder measure defined on the weak sigma algebra whose Fourier transform has the form $$ \exp( - \frac{1}{2} \langle x, x \rangle_{\ell^2}) ~~.$$ Does there exist a measurable norm associated to $\ell^2$ and $\nu$ ? If so, is there a nice description of its associated abstract Wiener space? If not, is there a proof that there is not? I do know that $\ell^2$ is the Cameron-Martin space of $\mathbb{R}^{\infty}$ with the product topology and the distribution of an iid sequence of random variables $(X_n)_{n \in \mathbb{N}}$ with $X_1 \sim \mathcal{N}(0,1)$ , and also that $\ell^2$ lies dense in that space w.r.t. the topology of point-wise convergence. So it ""should be"" $\mathbb{R}^{\infty}$ , but it is not since $\mathbb{R}^{\infty}$ is only separable Frechet and not Banach.","Let denote the space of square-summable real-valued sequences equipped with the inner product Let denote its canonical cylinder measure i.e. a cylinder measure defined on the weak sigma algebra whose Fourier transform has the form Does there exist a measurable norm associated to and ? If so, is there a nice description of its associated abstract Wiener space? If not, is there a proof that there is not? I do know that is the Cameron-Martin space of with the product topology and the distribution of an iid sequence of random variables with , and also that lies dense in that space w.r.t. the topology of point-wise convergence. So it ""should be"" , but it is not since is only separable Frechet and not Banach.","\ell^2(\mathbb{R})  \langle x, y \rangle = \sum_{n = 1}^{\infty} x_n y_n  \nu  \exp( - \frac{1}{2} \langle x, x \rangle_{\ell^2}) ~~. \ell^2 \nu \ell^2 \mathbb{R}^{\infty} (X_n)_{n \in \mathbb{N}} X_1 \sim \mathcal{N}(0,1) \ell^2 \mathbb{R}^{\infty} \mathbb{R}^{\infty}","['functional-analysis', 'measure-theory', 'gaussian-measure']"
13,Show that the quotient norms fail to be a norm in the quotient space $\ell^{\infty}/c_{00}$.,Show that the quotient norms fail to be a norm in the quotient space .,\ell^{\infty}/c_{00},"Let $(V, \|\ \cdot\ \|)$ be a normed space over $\mathbb{R}$ or $\mathbb{C}$ and let $W\subset V$ be a linear subspace. We define the following equivalence relation on $W$ : $$x\sim y\iff x-y\in W.$$ Then, we can define the quotient space $$V/W=V/\sim=\{[x]:x\in V\},\ \ \text{where}\ \ [x]\ \ \text{is the equivalence class}.$$ And we define the quotient ""norm"" to be $$\|[x]\|_{V/W}=\inf_{y\in W}\|x-y\|.$$ It is known that $\|\ \cdot\ \|_{V/W}$ is a semi-norm, and when $W$ is not closed, it may fail to be a norm since it may fail the property that $$\|[x]\|_{V/W}=0\iff [x]=[0].$$ One counterexample has been given here: Concrete counterexample for norms on quotient spaces , but it seems that the definition of the quotient in this post is a little bit different than mine. Therefore, I tried to come up with an example by considering $c_{00}$ as a subspace of $\ell^{\infty}$ (as long as the elements in $c_{00}$ comes from $\ell^{\infty}$ , the subspace property is clear). Firstly note that $c_{00}$ is not closed in $\ell^{\infty}$ . Indeed, for each $n\in\mathbb{N}$ , set $$x_{n}:=\Bigg(1, \dfrac{1}{2},\dfrac{1}{3},\cdots,\dfrac{1}{n},0,0,\cdots\Bigg)\ \ \text{and}\ \ x=\Bigg(1, \dfrac{1}{2}, \dfrac{1}{3},\cdots\Bigg).$$ Then $(x_{n})_{n=1}^{\infty}\subset c_{00}$ and $\|x-x_{n}\|_{\infty}=\frac{1}{n+1}\longrightarrow 0$ , and thus $x_{n}\longrightarrow x$ in $\ell^{\infty}$ , but $x\notin c_{00}$ . However, I cannot come up with a sequence $x\in c_{00}$ (so that $[x]=0$ ) but $\|[x]\|_{\ell^{\infty}/c_{00}}\neq 0.$ Any idea? Thank you!","Let be a normed space over or and let be a linear subspace. We define the following equivalence relation on : Then, we can define the quotient space And we define the quotient ""norm"" to be It is known that is a semi-norm, and when is not closed, it may fail to be a norm since it may fail the property that One counterexample has been given here: Concrete counterexample for norms on quotient spaces , but it seems that the definition of the quotient in this post is a little bit different than mine. Therefore, I tried to come up with an example by considering as a subspace of (as long as the elements in comes from , the subspace property is clear). Firstly note that is not closed in . Indeed, for each , set Then and , and thus in , but . However, I cannot come up with a sequence (so that ) but Any idea? Thank you!","(V, \|\ \cdot\ \|) \mathbb{R} \mathbb{C} W\subset V W x\sim y\iff x-y\in W. V/W=V/\sim=\{[x]:x\in V\},\ \ \text{where}\ \ [x]\ \ \text{is the equivalence class}. \|[x]\|_{V/W}=\inf_{y\in W}\|x-y\|. \|\ \cdot\ \|_{V/W} W \|[x]\|_{V/W}=0\iff [x]=[0]. c_{00} \ell^{\infty} c_{00} \ell^{\infty} c_{00} \ell^{\infty} n\in\mathbb{N} x_{n}:=\Bigg(1, \dfrac{1}{2},\dfrac{1}{3},\cdots,\dfrac{1}{n},0,0,\cdots\Bigg)\ \ \text{and}\ \ x=\Bigg(1, \dfrac{1}{2}, \dfrac{1}{3},\cdots\Bigg). (x_{n})_{n=1}^{\infty}\subset c_{00} \|x-x_{n}\|_{\infty}=\frac{1}{n+1}\longrightarrow 0 x_{n}\longrightarrow x \ell^{\infty} x\notin c_{00} x\in c_{00} [x]=0 \|[x]\|_{\ell^{\infty}/c_{00}}\neq 0.","['real-analysis', 'functional-analysis', 'analysis', 'normed-spaces', 'quotient-spaces']"
14,What is regularity of solution of Dirichlet problem with Dirac distribution as boundary data?,What is regularity of solution of Dirichlet problem with Dirac distribution as boundary data?,,"I was thinking if we have Dirac distribution as boundary condition, then what will be regularity of solution. Problem is following, $$ \left\{\begin{matrix} \nabla_x(\gamma(x)\nabla_x u(x,y))=0 & in~\Omega\\  u(x,y)=\delta_y(x) & on ~\partial \Omega  \end{matrix}\right. $$ where $x,y\in \mathbb R^n , \gamma(x)>c>0 $ . $\Omega$ is with smooth boundary. Any help or  reference is greatly appreciated.","I was thinking if we have Dirac distribution as boundary condition, then what will be regularity of solution. Problem is following, where . is with smooth boundary. Any help or  reference is greatly appreciated.","
\left\{\begin{matrix}
\nabla_x(\gamma(x)\nabla_x u(x,y))=0 & in~\Omega\\ 
u(x,y)=\delta_y(x) & on ~\partial \Omega 
\end{matrix}\right.
 x,y\in \mathbb R^n , \gamma(x)>c>0  \Omega","['functional-analysis', 'partial-differential-equations', 'distribution-theory', 'regularity-theory-of-pdes']"
15,Chain rule for functional derivates with the same variables,Chain rule for functional derivates with the same variables,,"I am following an online course where we should sometimes compute functional derivatives. The scope of the course is density functional theory, so we have the density $n(r)$ and we have functionals of the form $F[n(r)]$ . $$n(r) = \sum_i \Psi_i^*(r)\Psi_i(r)$$ In the course we need to perform derivatives with respect to $\Psi_i^*(r)$ not with respect to $n(r)$ . One quick result is that: $$\frac{\delta n(r)}{\delta \Psi_i^*(r)} = \frac{\delta \Psi_i^*(r)}{\delta \Psi_i^*(r)} \Psi_i(r) = \Psi_i(r)$$ One of the students used the chain rules for functional derivatives: $$\frac{\delta F[n(r)]}{\delta \Psi_i^*(r')} = \int \frac{\delta F[n(r)]}{\delta n(r)}\frac{\delta n(r)}{\delta \Psi_i^*(r')}dr = \int \frac{\delta F[n(r)]}{\delta n(r)}\delta(r - r')\Psi_i(r) dr = \frac{\delta F[n(r)]}{\delta n(r')}\Psi_i(r')$$ Which is a really helpful result because it allows to compute derivatives directly with respect to $n(r)$ and still get what we want by just multiplying by $\Psi_i(r')$ . Now the problem I have, is that, in this line he introduced a new variable $r'$ while it shouldn't, the derivative should be with one variable $r$ everywhere, and I believe that this is not valid when the variables are the same (the integral doesn't vanish.). What I did is that I used the functional derivative definition of composed function $F[g(f(r))]$ and adapted to each functional I needed to derive which lead to the same results with more work While I acknowledge that the first solution is faster, I wouldn't be able to find it by myself because it doesn't make so much sense to me. Any thoughts?","I am following an online course where we should sometimes compute functional derivatives. The scope of the course is density functional theory, so we have the density and we have functionals of the form . In the course we need to perform derivatives with respect to not with respect to . One quick result is that: One of the students used the chain rules for functional derivatives: Which is a really helpful result because it allows to compute derivatives directly with respect to and still get what we want by just multiplying by . Now the problem I have, is that, in this line he introduced a new variable while it shouldn't, the derivative should be with one variable everywhere, and I believe that this is not valid when the variables are the same (the integral doesn't vanish.). What I did is that I used the functional derivative definition of composed function and adapted to each functional I needed to derive which lead to the same results with more work While I acknowledge that the first solution is faster, I wouldn't be able to find it by myself because it doesn't make so much sense to me. Any thoughts?",n(r) F[n(r)] n(r) = \sum_i \Psi_i^*(r)\Psi_i(r) \Psi_i^*(r) n(r) \frac{\delta n(r)}{\delta \Psi_i^*(r)} = \frac{\delta \Psi_i^*(r)}{\delta \Psi_i^*(r)} \Psi_i(r) = \Psi_i(r) \frac{\delta F[n(r)]}{\delta \Psi_i^*(r')} = \int \frac{\delta F[n(r)]}{\delta n(r)}\frac{\delta n(r)}{\delta \Psi_i^*(r')}dr = \int \frac{\delta F[n(r)]}{\delta n(r)}\delta(r - r')\Psi_i(r) dr = \frac{\delta F[n(r)]}{\delta n(r')}\Psi_i(r') n(r) \Psi_i(r') r' r F[g(f(r))],"['functional-analysis', 'functional-equations', 'chain-rule', 'functional-calculus']"
16,How to complete this proof process of Arzelà-Ascoli theorem?,How to complete this proof process of Arzelà-Ascoli theorem?,,"Theorem. Let $X$ be a compact space and $Y$ be a metric space. A set $\mathscr F \subseteq C(X,Y)$ is precompact if and only if it is pointwise precompact and equi-continuous. Definitions. A subset $\mathscr F \subseteq C(X,Y)$ is called: equi-continuous if for every $x ∈ X$ and every $ε > 0$ , there exists an open neighborhood $U \subseteq X$ of $x$ such that $d_Y(f(x), f(x')) < ε$ for all $ x'∈ U$ and all $f ∈ \mathscr F$ . precompact if the closure of $\mathscr F$ is compact. pointwise precompact if for each $x∈X$ , $\mathscr F(x) = \{f(x): f∈\mathscr F \}$ is precompact. Lemma. Let $(X,d)$ be a metric space and let $K \subseteq X$ . Then every sequence in $K$ has a Cauchy subsequence if and only if $K $ is totally bounded. I know how to prove it from left, the way is similar with $X$ is compact metric space. But from right: auth give me some hint. The key is is to prove every sequence has Cauchy subsequence. By the lemma, we only need to prove $\mathscr F$ is totally bounded. He gives me two hint: The set $F = \{f(x) :  x ∈ X, f ∈ \mathscr F\} \subseteq Y$ is precompact so  is totally bounded. Let $ε > 0$ . Cover $F$ by finitely many open balls $V_1,\dots,V_n$ of radius $ε/3$ and cover $X$ by finitely many open sets $U_1,\dots,U_m$ such that $$\sup_{x,x' ∈ U_i} \sup_{f ∈ \mathscr F} d_Y(f(x),f(x'))＜\frac ε3.$$ Let $\alpha $ be a any map from $\{1,\dots,m\}$ to $\{1,\dots,n\}$ . Define $$\mathscr{F}_α = \{f ∈ \mathscr F : f(U_i) ∩ V_{\alpha(i)} ≠ \varnothing \} .$$ Let $A$ be the set of all $α$ such that $\mathscr F_α ≠ \varnothing$ . Prove that $\mathscr F = \bigcup_{α∈A} \mathscr F_α$ . I can following the hint 2 to achieve it, but I don't how to prove hint 1. My try: Originally, I choose $f_n(x_n)$ be sequence of $F$ , I use diagonal method to find a subsequence $g_n$ of $f_n$ such that $f_{n_k}$ converges at each $x_i$ , and I want to prove that $f_{n_k}(x_{n_k})$ converges to subsequence of $f_n(x_n)$ . However I think this way is not ture now.","Theorem. Let be a compact space and be a metric space. A set is precompact if and only if it is pointwise precompact and equi-continuous. Definitions. A subset is called: equi-continuous if for every and every , there exists an open neighborhood of such that for all and all . precompact if the closure of is compact. pointwise precompact if for each , is precompact. Lemma. Let be a metric space and let . Then every sequence in has a Cauchy subsequence if and only if is totally bounded. I know how to prove it from left, the way is similar with is compact metric space. But from right: auth give me some hint. The key is is to prove every sequence has Cauchy subsequence. By the lemma, we only need to prove is totally bounded. He gives me two hint: The set is precompact so  is totally bounded. Let . Cover by finitely many open balls of radius and cover by finitely many open sets such that Let be a any map from to . Define Let be the set of all such that . Prove that . I can following the hint 2 to achieve it, but I don't how to prove hint 1. My try: Originally, I choose be sequence of , I use diagonal method to find a subsequence of such that converges at each , and I want to prove that converges to subsequence of . However I think this way is not ture now.","X Y \mathscr F \subseteq C(X,Y) \mathscr F \subseteq C(X,Y) x ∈ X ε > 0 U \subseteq X x d_Y(f(x), f(x')) < ε  x'∈ U f ∈ \mathscr F \mathscr F x∈X \mathscr F(x) = \{f(x): f∈\mathscr F \} (X,d) K \subseteq X K K  X \mathscr F F = \{f(x) :  x ∈ X, f ∈ \mathscr F\} \subseteq Y ε > 0 F V_1,\dots,V_n ε/3 X U_1,\dots,U_m \sup_{x,x' ∈ U_i} \sup_{f ∈ \mathscr F} d_Y(f(x),f(x'))＜\frac ε3. \alpha  \{1,\dots,m\} \{1,\dots,n\} \mathscr{F}_α = \{f ∈ \mathscr F : f(U_i) ∩ V_{\alpha(i)} ≠ \varnothing \} . A α \mathscr F_α ≠ \varnothing \mathscr F = \bigcup_{α∈A} \mathscr F_α f_n(x_n) F g_n f_n f_{n_k} x_i f_{n_k}(x_{n_k}) f_n(x_n)","['real-analysis', 'functional-analysis']"
17,Generalisation of an inequality to infinite dimensional normed linear spaces,Generalisation of an inequality to infinite dimensional normed linear spaces,,"Lemma 6.3 in Rudin's Real and Complex Analysis states that: For every $n$ and for every $n$ -tuple of complex number $z_1,...,z_n$ , there exists $S\subseteq \{1,...,n\}$ such that $$\left| \sum_{k \in S} z_k \right| \geqslant \frac1\pi\sum_{i=1}^{n} | z_i |$$ This can be generalised to $\mathbb R^d$ for all $d$ (see, for example, here ) and, exploiting the fact that all norms on a finite dimensional vector space are equivalent, we arrive at the following generalisation Let $(X,||\cdot||)$ be a finite dimensional normed space. There exists a constant $c>0$ , that depends only on $X$ and $||\cdot||$ , such that the following holds: for any $n$ and any $n$ -tuple $(v_1,v_2,…,v_n)$ of elements of $X$ , there exists a subset $J⊆\{1,2,…,n\}$ such that $$\left\| \sum_{j \in J} v_j \right\| \geqslant c \sum_{i=1}^{n} \| v_i \|.$$ This is not always the case for infinite dimensional normed spaces. For example, if $(X,\langle\cdot,\cdot\rangle)$ is an infinite dimensional Hilbert space, pick a countable orthonormal subset $\{e_n\}_{n\in\mathbb N}$ . Then for all $n$ and for all subsets $J\subseteq\{0,...,n\}$ , we have that $\sum_{i=0}^{n} \| e_i \|=n$ and $\left\| \sum_{j \in J} e_j \right\|=\sqrt{|J|+1}\leqslant\sqrt{n+1}$ , but there is no constant $c>0$ such that $\forall n\in\mathbb N:\sqrt{n+1}\geqslant cn$ . A similar argument also works for $\ell^p$ for all $p\in(1,\infty]$ , however, I wasn't able to apply the same argument for $\ell^1.$ My question is: does there exist an infinite dimensional normed space for which the theorem above holds? I couldn't come up with an example, so i tried to prove that is is never the case. My idea was to use Riesz's lemma , but I got stuck.","Lemma 6.3 in Rudin's Real and Complex Analysis states that: For every and for every -tuple of complex number , there exists such that This can be generalised to for all (see, for example, here ) and, exploiting the fact that all norms on a finite dimensional vector space are equivalent, we arrive at the following generalisation Let be a finite dimensional normed space. There exists a constant , that depends only on and , such that the following holds: for any and any -tuple of elements of , there exists a subset such that This is not always the case for infinite dimensional normed spaces. For example, if is an infinite dimensional Hilbert space, pick a countable orthonormal subset . Then for all and for all subsets , we have that and , but there is no constant such that . A similar argument also works for for all , however, I wasn't able to apply the same argument for My question is: does there exist an infinite dimensional normed space for which the theorem above holds? I couldn't come up with an example, so i tried to prove that is is never the case. My idea was to use Riesz's lemma , but I got stuck.","n n z_1,...,z_n S\subseteq \{1,...,n\} \left| \sum_{k \in S} z_k \right| \geqslant \frac1\pi\sum_{i=1}^{n} | z_i | \mathbb R^d d (X,||\cdot||) c>0 X ||\cdot|| n n (v_1,v_2,…,v_n) X J⊆\{1,2,…,n\} \left\| \sum_{j \in J} v_j \right\| \geqslant c \sum_{i=1}^{n} \| v_i \|. (X,\langle\cdot,\cdot\rangle) \{e_n\}_{n\in\mathbb N} n J\subseteq\{0,...,n\} \sum_{i=0}^{n} \| e_i \|=n \left\| \sum_{j \in J} e_j \right\|=\sqrt{|J|+1}\leqslant\sqrt{n+1} c>0 \forall n\in\mathbb N:\sqrt{n+1}\geqslant cn \ell^p p\in(1,\infty] \ell^1.","['functional-analysis', 'inequality', 'normed-spaces']"
18,Compact embedding of $l^p$ to $c_0$,Compact embedding of  to,l^p c_0,if $c_0$ is a space of real sequences that converges to zero with sup norm. I can show that $l^{p} \hookrightarrow c_{0}$ embedding is not compact with the sup norm. But I want something more intresting than that. I want to find norm on $c_0$ such that this emdedding be compact! But until now I can't find such norm. So if one have any idea about this ....,if is a space of real sequences that converges to zero with sup norm. I can show that embedding is not compact with the sup norm. But I want something more intresting than that. I want to find norm on such that this emdedding be compact! But until now I can't find such norm. So if one have any idea about this ....,c_0 l^{p} \hookrightarrow c_{0} c_0,['functional-analysis']
19,Finding the “root” of a monotone function (in the sense of composition),Finding the “root” of a monotone function (in the sense of composition),,"Let $f:[0,\infty)\rightarrow [0,\infty) $ be a smooth and monotone function s.t $f(0)=0$ . Let $N\in\mathbb{N}$ . Can we find a function $g: [0,\infty) \rightarrow [0,\infty) $ s.t $g\circ\cdots\circ g$ ( $g$ composed with itself $N$ times) equals $f$ ? Can we say something about $g$ ‘s monotonicity? Its smoothness? I cannot come up with any basic answers. Thanks in advance to the helpers.",Let be a smooth and monotone function s.t . Let . Can we find a function s.t ( composed with itself times) equals ? Can we say something about ‘s monotonicity? Its smoothness? I cannot come up with any basic answers. Thanks in advance to the helpers.,"f:[0,\infty)\rightarrow [0,\infty)  f(0)=0 N\in\mathbb{N} g: [0,\infty) \rightarrow [0,\infty)  g\circ\cdots\circ g g N f g","['real-analysis', 'calculus', 'functional-analysis']"
20,Does a bounded from below linear operator with target set equal to the domain always have closed image?,Does a bounded from below linear operator with target set equal to the domain always have closed image?,,"I'm using the definition of a bounded from below linear operator as a linear operator $X\overset{T}{{\to}}Y$ , where $X$ and $Y$ are Banach spaces and the operator satisfies: for all $x \in X$ , $\lVert T(x) \rVert \geq C \lVert x \rVert$ , for some constant $C \gt 0$ . In this question i will refer only to such an operator in a Banach space $B$ , $ T \colon B{{\to}}B$ . Note that I don't assume it to be continuous. With only these conditions, can we guarantee that the image of $T$ is closed in $B$ ? (Which is equivalent to asking if these assumptions imply continuity of the operator, by the open mapping theorem applied to the inverse of the operator, which exists since $\lVert T(x) \rVert = 0 \Rightarrow \lVert x \rVert = 0$ ). I have tried to prove that it is closed with only these conditions and checked out other posts that have similar questions, but they don't seem to answer this one, at least in an immediate way. At this point, I believe that it is not generally true, but I also haven't been able to write a counter-example. I appreciate any help and thank you in advance!","I'm using the definition of a bounded from below linear operator as a linear operator , where and are Banach spaces and the operator satisfies: for all , , for some constant . In this question i will refer only to such an operator in a Banach space , . Note that I don't assume it to be continuous. With only these conditions, can we guarantee that the image of is closed in ? (Which is equivalent to asking if these assumptions imply continuity of the operator, by the open mapping theorem applied to the inverse of the operator, which exists since ). I have tried to prove that it is closed with only these conditions and checked out other posts that have similar questions, but they don't seem to answer this one, at least in an immediate way. At this point, I believe that it is not generally true, but I also haven't been able to write a counter-example. I appreciate any help and thank you in advance!",X\overset{T}{{\to}}Y X Y x \in X \lVert T(x) \rVert \geq C \lVert x \rVert C \gt 0 B  T \colon B{{\to}}B T B \lVert T(x) \rVert = 0 \Rightarrow \lVert x \rVert = 0,"['functional-analysis', 'linear-transformations', 'banach-spaces']"
21,Weak convergence implies almost everywhere?,Weak convergence implies almost everywhere?,,"Suppose that $L^2(R^n;dx) \ni f_n\geq 0$ and $\int f_ng\to 0$ for all continuous function $g$ with compact  support. Then, does $f_n$ convergence to $0$ almost everywhere? Intuitively, it convergence to $0$ by taking function $g$ s.t., $g=1$ around $x$ for almost every $x$ . However I cannot prove.","Suppose that and for all continuous function with compact  support. Then, does convergence to almost everywhere? Intuitively, it convergence to by taking function s.t., around for almost every . However I cannot prove.",L^2(R^n;dx) \ni f_n\geq 0 \int f_ng\to 0 g f_n 0 0 g g=1 x x,"['real-analysis', 'functional-analysis', 'convergence-divergence', 'lebesgue-integral']"
22,Show that the characteristic function of a finite signed measure on a normed vector space is uniformly continuous,Show that the characteristic function of a finite signed measure on a normed vector space is uniformly continuous,,"Let $E$ be a normed $\mathbb R$ -vector space, $\mu$ be a finite signed measure on $(E,\mathcal B(E))$ and $$\hat\mu:E'\to\mathbb C\;,\;\;\;\varphi\mapsto\int\mu({\rm d}x)e^{{\rm i}\varphi}$$ denote the characteristic function of $\mu$ . Replying to a previous formulation of this question, Kavi Rama Murthy has shown that if $E$ is complete and separable and $\mu$ is nonnegative, then $\hat\mu$ is uniformly continuous. It is easy to see that his proof still works in the general case as long as we are assuming that $\mu$ is tight $^1$ , i.e. $$\forall\varepsilon>0:\exists K\subseteq E\text{ compact}:|\mu|(K^c)<\varepsilon\tag1.$$ Taking a closer look at the proof, I've observed the following: Let $\langle\;\cdot\;,\;\cdot\;\rangle$ denote the duality pairing between $E$ and $E'$ and $$p_x(\varphi):=|\langle x,\varphi\rangle|\;\;\;\text{for }\varphi\in E'$$ for $x\in E$ . By definition, the weak* topology $\sigma(E',E)$ on $E'$ is the topology generated by the seminorm family $(p_x)_{x\in E}$ . Now, if $K\subseteq E$ is compact, $$p_K(\varphi):=\sup_{x\in K}p_x(\varphi)\;\;\;\text{for }\varphi\in E'$$ should be a seminorm on $E'$ as well. And if I'm not missing something, the topology generated by $(p_K:K\subseteq E\text{ is compact})$ is precisely the topology $\sigma_c(E',E)$ of compact convergence on $E'$ . What Kavi Rama Murthy has shown is that, since $\mu$ is tight, for all $\varepsilon>0$ , there is a compact $K\subseteq E$ and a $\delta>0$ with $$|\hat\mu(\varphi_1)-\hat\mu(\varphi_2)|<\varepsilon\;\;\;\text{for all }\varphi_1,\varphi_2\in E'\text{ with }p_K(\varphi_1-\varphi_2)<\delta\tag2.$$ Question : Are we able to conclude that $\hat\mu$ is $\sigma_c(E',E)$ -continuous? EDIT : In order to conclude that $\hat\mu$ is (uniformly) $\sigma_c(E',E)$ -continuous, we need to that $(2)$ holds for $K$ replaced by an arbitrary compact $\tilde K\subseteq E$ . Given $\varepsilon>0$ , we can show $(2)$ by choosing the compact subset $K\subseteq E$ such that $$|\mu|(K^c)<\varepsilon\tag3.$$ We may then write \begin{equation}\begin{split}\left|\hat\mu(\varphi_1)-\hat\mu(\varphi_2)\right|&\le\underbrace{\int_{K\cap\tilde K}\left|e^{{\rm i}\varphi_1}-e^{{\rm i}\varphi_2}\right|{\rm d}\left|\mu\right|}_{<\:\varepsilon}\\&\;\;\;\;\;\;\;\;\;\;\;\;+\int_{K\cap\tilde K^c}\left|e^{{\rm i}\varphi_1}-e^{{\rm i}\varphi_2}\right|{\rm d}\left|\mu\right|\\&\;\;\;\;\;\;\;\;\;\;\;\;+\underbrace{\int_{K\cap\tilde K}\left|e^{{\rm i}\varphi_1}-e^{{\rm i}\varphi_2}\right|{\rm d}\left|\mu\right|}_{<\:2\varepsilon}\end{split}\tag4\end{equation} for all $\varphi_1,\varphi_2\in E'$ with $p_{\tilde K}(\varphi_1-\varphi_2)<\delta$ , where $$\delta:=\frac\varepsilon{\left\|\mu\right\|},$$ but I have no idea how we can control the second integral. EDIT 2 A ""proof"" of this claim can be (found in Linde's Probability in Banach Spaces ), but I have no idea why this proof is correct, since he is concluding the continuity immediately from $(2)$ (for a single $K$ ): Maybe we need to assume that $\mu$ is even Radon, i.e. that for all $B\in\mathcal (E)$ , there is a compact $C\subseteq E$ with $C\subseteq B$ and $|\mu|(B\setminus C)<\varepsilon$ . The author is actually imposing this assumption, but he obviously doesn't make use of it in his proof (he would need to consider an arbitrary compact $\tilde K\subseteq E$ , as I did above). $^1$ On a complete separable metric space, every finite signed measure is tight.","Let be a normed -vector space, be a finite signed measure on and denote the characteristic function of . Replying to a previous formulation of this question, Kavi Rama Murthy has shown that if is complete and separable and is nonnegative, then is uniformly continuous. It is easy to see that his proof still works in the general case as long as we are assuming that is tight , i.e. Taking a closer look at the proof, I've observed the following: Let denote the duality pairing between and and for . By definition, the weak* topology on is the topology generated by the seminorm family . Now, if is compact, should be a seminorm on as well. And if I'm not missing something, the topology generated by is precisely the topology of compact convergence on . What Kavi Rama Murthy has shown is that, since is tight, for all , there is a compact and a with Question : Are we able to conclude that is -continuous? EDIT : In order to conclude that is (uniformly) -continuous, we need to that holds for replaced by an arbitrary compact . Given , we can show by choosing the compact subset such that We may then write for all with , where but I have no idea how we can control the second integral. EDIT 2 A ""proof"" of this claim can be (found in Linde's Probability in Banach Spaces ), but I have no idea why this proof is correct, since he is concluding the continuity immediately from (for a single ): Maybe we need to assume that is even Radon, i.e. that for all , there is a compact with and . The author is actually imposing this assumption, but he obviously doesn't make use of it in his proof (he would need to consider an arbitrary compact , as I did above). On a complete separable metric space, every finite signed measure is tight.","E \mathbb R \mu (E,\mathcal B(E)) \hat\mu:E'\to\mathbb C\;,\;\;\;\varphi\mapsto\int\mu({\rm d}x)e^{{\rm i}\varphi} \mu E \mu \hat\mu \mu ^1 \forall\varepsilon>0:\exists K\subseteq E\text{ compact}:|\mu|(K^c)<\varepsilon\tag1. \langle\;\cdot\;,\;\cdot\;\rangle E E' p_x(\varphi):=|\langle x,\varphi\rangle|\;\;\;\text{for }\varphi\in E' x\in E \sigma(E',E) E' (p_x)_{x\in E} K\subseteq E p_K(\varphi):=\sup_{x\in K}p_x(\varphi)\;\;\;\text{for }\varphi\in E' E' (p_K:K\subseteq E\text{ is compact}) \sigma_c(E',E) E' \mu \varepsilon>0 K\subseteq E \delta>0 |\hat\mu(\varphi_1)-\hat\mu(\varphi_2)|<\varepsilon\;\;\;\text{for all }\varphi_1,\varphi_2\in E'\text{ with }p_K(\varphi_1-\varphi_2)<\delta\tag2. \hat\mu \sigma_c(E',E) \hat\mu \sigma_c(E',E) (2) K \tilde K\subseteq E \varepsilon>0 (2) K\subseteq E |\mu|(K^c)<\varepsilon\tag3. \begin{equation}\begin{split}\left|\hat\mu(\varphi_1)-\hat\mu(\varphi_2)\right|&\le\underbrace{\int_{K\cap\tilde K}\left|e^{{\rm i}\varphi_1}-e^{{\rm i}\varphi_2}\right|{\rm d}\left|\mu\right|}_{<\:\varepsilon}\\&\;\;\;\;\;\;\;\;\;\;\;\;+\int_{K\cap\tilde K^c}\left|e^{{\rm i}\varphi_1}-e^{{\rm i}\varphi_2}\right|{\rm d}\left|\mu\right|\\&\;\;\;\;\;\;\;\;\;\;\;\;+\underbrace{\int_{K\cap\tilde K}\left|e^{{\rm i}\varphi_1}-e^{{\rm i}\varphi_2}\right|{\rm d}\left|\mu\right|}_{<\:2\varepsilon}\end{split}\tag4\end{equation} \varphi_1,\varphi_2\in E' p_{\tilde K}(\varphi_1-\varphi_2)<\delta \delta:=\frac\varepsilon{\left\|\mu\right\|}, (2) K \mu B\in\mathcal (E) C\subseteq E C\subseteq B |\mu|(B\setminus C)<\varepsilon \tilde K\subseteq E ^1","['functional-analysis', 'probability-theory', 'measure-theory', 'characteristic-functions', 'weak-topology']"
23,"$\langle S\alpha,\alpha\rangle = \langle T\alpha,\alpha\rangle \Longrightarrow S=T$ for unbounded operators",for unbounded operators,"\langle S\alpha,\alpha\rangle = \langle T\alpha,\alpha\rangle \Longrightarrow S=T","Like showed for example here , here or here , it is well-known that on a complex Hilbert space $H$ (or inner product space), basically by polarisation, for any bounded linear operator $T : H \to H$ , we have \begin{equation} \forall v \in H : \quad \langle Tv,v\rangle = 0 \quad\Longrightarrow\quad T = 0. \end{equation} Applying this equation to the difference, an easy consequence this is that if $S : H \to H$ is another bounded linear operator on $H$ , then \begin{equation} \forall v \in H : \quad \langle Sv,v\rangle = \langle Tv,v\rangle \quad\Longrightarrow\quad S = T. \end{equation} Does the result also hold true if the dimension of $H$ is not finite? for possibly unbounded operators or do we must assume that the operator is self-adjoint, normal? What happens if the operator is not self-adjoint nor even symmetric? For example consider the difference of two covariant derivatives, which is not symmetric in general as $T$ , and the Hilbert space of equivalence classes of $L^2$ -Borel $p$ -forms on a Riemannian manifold.","Like showed for example here , here or here , it is well-known that on a complex Hilbert space (or inner product space), basically by polarisation, for any bounded linear operator , we have Applying this equation to the difference, an easy consequence this is that if is another bounded linear operator on , then Does the result also hold true if the dimension of is not finite? for possibly unbounded operators or do we must assume that the operator is self-adjoint, normal? What happens if the operator is not self-adjoint nor even symmetric? For example consider the difference of two covariant derivatives, which is not symmetric in general as , and the Hilbert space of equivalence classes of -Borel -forms on a Riemannian manifold.","H T : H \to H \begin{equation}
\forall v \in H : \quad \langle Tv,v\rangle = 0 \quad\Longrightarrow\quad T = 0.
\end{equation} S : H \to H H \begin{equation}
\forall v \in H : \quad \langle Sv,v\rangle = \langle Tv,v\rangle \quad\Longrightarrow\quad S = T.
\end{equation} H T L^2 p","['functional-analysis', 'differential-geometry', 'differential-forms']"
24,Weak convergence of product of sequences that are weakly convergent in $H^1$ and weak-* convergent in $L^\infty$,Weak convergence of product of sequences that are weakly convergent in  and weak-* convergent in,H^1 L^\infty,"I'm currently trying to understand a step of a proof in a paper. Suppose that we have two sequences $(n_k), (V_k)\subset H^1(\Omega)\cap L^\infty(\Omega)$ that converge weakly in $H^1(\Omega)$ to some limit elements $n,V\in H^1(\Omega)$ , where $\Omega\subset\mathbb{R}^d$ is a bounded Lipschitz domain $(d\in\{1,2,3\})$ . Further, assume that the sequences are uniformly bounded in $H^1(\Omega)$ and $L^\infty(\Omega)$ as well as weak-* convergent in $L^\infty(\Omega)$ to $n,V$ . The authors now argue that this is more than sufficient to show $$ \lim_{k\to\infty}\int_\Omega n_k\nabla V_k\cdot\nabla \varphi\;\text{d}x=\int_\Omega n\nabla V\cdot\nabla\varphi\;\text{d}x\quad\forall\varphi\in H^1(\Omega) $$ without giving more details. Here, $\nabla u$ denotes the weak derivative of $u$ . As I'm quite new to the topic, I tried to understand why this holds true. My approach was to consider the following: $$ \int_\Omega (n_k\nabla V_k-n\nabla V)\cdot\nabla\varphi\;\text{d}x=\int_\Omega n_k(\nabla V_k-\nabla V)\cdot\nabla\varphi\;\text{d}x+\int_\Omega (n_k-n)\nabla V\cdot\nabla\varphi\;\text{d}x. $$ We have that $\nabla V\cdot\nabla\varphi\in L^1(\Omega)$ and therefore the second integral tends to $0$ by the weak-* convergence of $(n_k)$ . However, the first integral is tricky for me. If there wasn't the term $n_k$ , one could see that the integral tends to $0$ because of the weak convergence of $V_k$ in $H^1(\Omega)$ . I could apply the Hoelder inequality to estimate $$ \left\vert\int_\Omega n_k(\nabla V_k-\nabla V)\cdot\nabla\varphi\;\text{d}x\right\vert\leq\Vert n_k\Vert_{L^\infty(\Omega)}\Vert \nabla V_k-\nabla V\Vert_{L^2(\Omega)}\Vert\nabla\varphi\Vert_{L^2(\Omega)} $$ but I don't know if $(\nabla V_k)$ strongly converges in $L^2(\Omega)$ . Could someone provide me with a hint on how to tackle the problem? Thanks. To give a little more context: I'm given a weakly convergent sequence $(n_k,V_k)$ that is a weak solution of a semilinear elliptic PDE system. I want to show that I can pass to the limit in the weak formulation where the term above appears. For the other terms in the weak formulation it was easy to verify that this is possible as they were linear.","I'm currently trying to understand a step of a proof in a paper. Suppose that we have two sequences that converge weakly in to some limit elements , where is a bounded Lipschitz domain . Further, assume that the sequences are uniformly bounded in and as well as weak-* convergent in to . The authors now argue that this is more than sufficient to show without giving more details. Here, denotes the weak derivative of . As I'm quite new to the topic, I tried to understand why this holds true. My approach was to consider the following: We have that and therefore the second integral tends to by the weak-* convergence of . However, the first integral is tricky for me. If there wasn't the term , one could see that the integral tends to because of the weak convergence of in . I could apply the Hoelder inequality to estimate but I don't know if strongly converges in . Could someone provide me with a hint on how to tackle the problem? Thanks. To give a little more context: I'm given a weakly convergent sequence that is a weak solution of a semilinear elliptic PDE system. I want to show that I can pass to the limit in the weak formulation where the term above appears. For the other terms in the weak formulation it was easy to verify that this is possible as they were linear.","(n_k), (V_k)\subset H^1(\Omega)\cap L^\infty(\Omega) H^1(\Omega) n,V\in H^1(\Omega) \Omega\subset\mathbb{R}^d (d\in\{1,2,3\}) H^1(\Omega) L^\infty(\Omega) L^\infty(\Omega) n,V 
\lim_{k\to\infty}\int_\Omega n_k\nabla V_k\cdot\nabla \varphi\;\text{d}x=\int_\Omega n\nabla V\cdot\nabla\varphi\;\text{d}x\quad\forall\varphi\in H^1(\Omega)
 \nabla u u 
\int_\Omega (n_k\nabla V_k-n\nabla V)\cdot\nabla\varphi\;\text{d}x=\int_\Omega n_k(\nabla V_k-\nabla V)\cdot\nabla\varphi\;\text{d}x+\int_\Omega (n_k-n)\nabla V\cdot\nabla\varphi\;\text{d}x.
 \nabla V\cdot\nabla\varphi\in L^1(\Omega) 0 (n_k) n_k 0 V_k H^1(\Omega) 
\left\vert\int_\Omega n_k(\nabla V_k-\nabla V)\cdot\nabla\varphi\;\text{d}x\right\vert\leq\Vert n_k\Vert_{L^\infty(\Omega)}\Vert \nabla V_k-\nabla V\Vert_{L^2(\Omega)}\Vert\nabla\varphi\Vert_{L^2(\Omega)}
 (\nabla V_k) L^2(\Omega) (n_k,V_k)","['functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'weak-convergence']"
25,A Sobolev space on a perforated domain with extension by a solid vector field,A Sobolev space on a perforated domain with extension by a solid vector field,,"Presentation : Let $\Omega \subset \mathbb{R}^3$ an open bounded regular set and $B=B(a,r)$ a ball such as $\bar{B} \subset \Omega$ . I'm studying the following space : $$V=\{v|_{\Omega \setminus B} \in (H^1(\Omega \setminus B))^3 \ | \ \operatorname{div}(v) = 0 \text{ in } \Omega \setminus B \ | \ \gamma_0(v)=0 \text{ a.e on }  \partial \Omega \ | \ \exists (b,c) \in \mathbb{R}^3 \times \mathbb{R}^3 \text{ such as } v|_{B}(x)=b + c \wedge (x-a) \}$$ where $\gamma_0 : (H^1(\Omega \setminus B))^3 \rightarrow (L^2(\partial \Omega \cup \partial B))^3$ is the sobolev trace application on $\Omega$ and the divergence condition should be understand in the distribution sense : $$\int\limits_{\Omega \setminus B} v \cdot \nabla \varphi=0, \ \forall \varphi \in C^\infty_c(\Omega \setminus B).$$ Concretely, the function $v \in V$ is a divergence free element in $(H^1(\Omega \setminus B))^3$ , which is worth $0$ on $\partial \Omega$ , that has been extended in the ball $B$ by a solid vector field $x \mapsto b + c \wedge (x-a)$ so as to get a function $v$ which is at least in $L^2(\Omega)$ . I want to study the property of this extension. My question I was wondering if it were possible to show that an element $v \in V$ is  in $(H^1_0(\Omega))^3$ and is divergence free in $\Omega$ . My attempt so far I want to show that the weak derivatives of $v$ are elements of $L^2(\Omega)$ . However, since I have a jump at the interface $\partial B$ , i'm not sure I will be able to have this kind of regularity. For the divergence free condition, I know that the divergence of a solid vector field $x \mapsto b + c \wedge (x-a)$ is $0$ pointwise, so the weak divergence is equal to $0$ both in $\Omega \setminus B$ and $B$ , but again I have an issue with the jump at $\partial B$ . Any help or references wich deal with such extension are welcomed, as always :). Feel free to ask me questions.","Presentation : Let an open bounded regular set and a ball such as . I'm studying the following space : where is the sobolev trace application on and the divergence condition should be understand in the distribution sense : Concretely, the function is a divergence free element in , which is worth on , that has been extended in the ball by a solid vector field so as to get a function which is at least in . I want to study the property of this extension. My question I was wondering if it were possible to show that an element is  in and is divergence free in . My attempt so far I want to show that the weak derivatives of are elements of . However, since I have a jump at the interface , i'm not sure I will be able to have this kind of regularity. For the divergence free condition, I know that the divergence of a solid vector field is pointwise, so the weak divergence is equal to both in and , but again I have an issue with the jump at . Any help or references wich deal with such extension are welcomed, as always :). Feel free to ask me questions.","\Omega \subset \mathbb{R}^3 B=B(a,r) \bar{B} \subset \Omega V=\{v|_{\Omega \setminus B} \in (H^1(\Omega \setminus B))^3 \ | \ \operatorname{div}(v) = 0 \text{ in } \Omega \setminus B \ | \ \gamma_0(v)=0 \text{ a.e on }  \partial \Omega \ | \ \exists (b,c) \in \mathbb{R}^3 \times \mathbb{R}^3 \text{ such as } v|_{B}(x)=b + c \wedge (x-a) \} \gamma_0 : (H^1(\Omega \setminus B))^3 \rightarrow (L^2(\partial \Omega \cup \partial B))^3 \Omega \int\limits_{\Omega \setminus B} v \cdot \nabla \varphi=0, \ \forall \varphi \in C^\infty_c(\Omega \setminus B). v \in V (H^1(\Omega \setminus B))^3 0 \partial \Omega B x \mapsto b + c \wedge (x-a) v L^2(\Omega) v \in V (H^1_0(\Omega))^3 \Omega v L^2(\Omega) \partial B x \mapsto b + c \wedge (x-a) 0 0 \Omega \setminus B B \partial B","['functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'distribution-theory', 'weak-derivatives']"
26,"Let $X,Y$ be banach spaces $T,T_n: X\to Y$ and let $T_n \to T$ pointwise, show $T_n \to T$ uniformly on all compact sets","Let  be banach spaces  and let  pointwise, show  uniformly on all compact sets","X,Y T,T_n: X\to Y T_n \to T T_n \to T","Let $X,Y$ be banach spaces $T,T_n: X\to Y$ and let $T_n \to T$ pointwise (weak*), show $T_n \to T$ uniformly on all compact sets. I reason like this: I claim that $T_n$ are equicontinuous. That is true as by uniform boundedness principle $\|T_n\|\leq M$ for all $n$ . Thus $T_n$ are all lipshitz of constant less than $M$ , which means they are equicontinuous. Now Pointwise+equicontinuity imply uniform on a compact sets, and so the result follows. Is this correct? IS there another solution to this problem?","Let be banach spaces and let pointwise (weak*), show uniformly on all compact sets. I reason like this: I claim that are equicontinuous. That is true as by uniform boundedness principle for all . Thus are all lipshitz of constant less than , which means they are equicontinuous. Now Pointwise+equicontinuity imply uniform on a compact sets, and so the result follows. Is this correct? IS there another solution to this problem?","X,Y T,T_n: X\to Y T_n \to T T_n \to T T_n \|T_n\|\leq M n T_n M","['functional-analysis', 'solution-verification', 'banach-spaces', 'uniform-convergence', 'pointwise-convergence']"
27,"Does the convergence to 0 in $L^2(0,T;L^2(B))$ where $|\mathbb{R}^{d}-B|<\infty$ imply the convergence in $L^2(0,T;L^2(\mathbb{R}^{d}))$?",Does the convergence to 0 in  where  imply the convergence in ?,"L^2(0,T;L^2(B)) |\mathbb{R}^{d}-B|<\infty L^2(0,T;L^2(\mathbb{R}^{d}))","Definition: One denotes $L^p(a,b;X)$ , $1\leq p<+\infty$ , the set of (classes of) measurable functions $u:(a,b)\rightarrow X$ such that $$\|u \|_{L^p(a,b;X)}:= \left(\int_{a}^{b}\|u(t)\|^p_X dt\right)^{\frac{1}{p}}<\infty .$$ Let $(f_n)$ be a sequence in $L^2(0,T;L^2(\mathbb{R}^{d}))$ such that: $\|f_n\|_{L^2(0,T;L^2(\mathbb{R}^{d}))} \leq C_T$ for all $n \in \mathbb{N}$ ; (or $\|f_n(t)\|_{L^2(\mathbb{R}^{d})}\leq C_T$ for all $t \in (0,T))$ $f_n \rightarrow 0$ a.e. in $(0,T)\times \mathbb{R}^{d}$ ; $f_n \rightarrow 0$ in $L^2(0,T;L^2(K))$ for all compact $K \subset \mathbb{R}^{d}$ . $f_n \rightarrow 0$ in $L^2(0,T;L^2(B))$ where $B \subset \mathbb{R}^{d}$ and $|\mathbb{R}^{d}-B|<\infty;$ My question: Is it true that $f_n \rightarrow 0$ in $L^2(0,T;L^2(\mathbb{R}^{d}))$ or that there is a subsequence of $f_n$ that converges to $0$ in $L^2(0,T;L^2(\mathbb{R}^{d}))$ ? Or what additional hypothesis do I need for this convergence to be true? Note that $$\int_{0}^{T}\int_{\mathbb{R}^{d}} |f_n(x,t)|^2\,dxdt=\int_{0}^{T}\int_{B} |f_n(x,t)|^2\,dxdt+\int_{0}^{T}\int_{\mathbb{R}^{d}-B} |f_n(x,t)|^2\,dxdt=I_1+I_2.$$ Observe that $I_1 \rightarrow 0$ by virtue of the fourth hypothesis above. It remains to prove that $I_2 \rightarrow 0$ . From Egorov's Theorem, given $\varepsilon>0$ there exists a closed subset $F \subset (0,T)\times (\mathbb{R}^{d}-B)$ such that $|((0,T)\times (\mathbb{R}^{d}-B))-F|<\varepsilon|$ and $f_n\rightarrow 0$ uniformly in $F$ , which implies that $f_n \rightarrow 0$ in $L^2(F)$ . Thus, we obtain \begin{eqnarray} I_2&=&\int_{0}^{T}\int_{\mathbb{R}^{d}-B} |f_n(x,t)|^2\,dxdt\\ &=& \int_{F} |f_n(x,t)|^2\,dxdt+\int_{((0,T)\times(\mathbb{R}^{d}-B))-F} |f_n(x,t)|^2\,dxdt. \end{eqnarray} Is there any way to prove that the $\int_{((0,T)\times(\mathbb{R}^{d}-B))-F} |f_n(x,t)|^2\,dxdt$ integral converges to 0?","Definition: One denotes , , the set of (classes of) measurable functions such that Let be a sequence in such that: for all ; (or for all a.e. in ; in for all compact . in where and My question: Is it true that in or that there is a subsequence of that converges to in ? Or what additional hypothesis do I need for this convergence to be true? Note that Observe that by virtue of the fourth hypothesis above. It remains to prove that . From Egorov's Theorem, given there exists a closed subset such that and uniformly in , which implies that in . Thus, we obtain Is there any way to prove that the integral converges to 0?","L^p(a,b;X) 1\leq p<+\infty u:(a,b)\rightarrow X \|u \|_{L^p(a,b;X)}:= \left(\int_{a}^{b}\|u(t)\|^p_X dt\right)^{\frac{1}{p}}<\infty . (f_n) L^2(0,T;L^2(\mathbb{R}^{d})) \|f_n\|_{L^2(0,T;L^2(\mathbb{R}^{d}))} \leq C_T n \in \mathbb{N} \|f_n(t)\|_{L^2(\mathbb{R}^{d})}\leq C_T t \in (0,T)) f_n \rightarrow 0 (0,T)\times \mathbb{R}^{d} f_n \rightarrow 0 L^2(0,T;L^2(K)) K \subset \mathbb{R}^{d} f_n \rightarrow 0 L^2(0,T;L^2(B)) B \subset \mathbb{R}^{d} |\mathbb{R}^{d}-B|<\infty; f_n \rightarrow 0 L^2(0,T;L^2(\mathbb{R}^{d})) f_n 0 L^2(0,T;L^2(\mathbb{R}^{d})) \int_{0}^{T}\int_{\mathbb{R}^{d}} |f_n(x,t)|^2\,dxdt=\int_{0}^{T}\int_{B} |f_n(x,t)|^2\,dxdt+\int_{0}^{T}\int_{\mathbb{R}^{d}-B} |f_n(x,t)|^2\,dxdt=I_1+I_2. I_1 \rightarrow 0 I_2 \rightarrow 0 \varepsilon>0 F \subset (0,T)\times (\mathbb{R}^{d}-B) |((0,T)\times (\mathbb{R}^{d}-B))-F|<\varepsilon| f_n\rightarrow 0 F f_n \rightarrow 0 L^2(F) \begin{eqnarray}
I_2&=&\int_{0}^{T}\int_{\mathbb{R}^{d}-B} |f_n(x,t)|^2\,dxdt\\
&=& \int_{F} |f_n(x,t)|^2\,dxdt+\int_{((0,T)\times(\mathbb{R}^{d}-B))-F} |f_n(x,t)|^2\,dxdt.
\end{eqnarray} \int_{((0,T)\times(\mathbb{R}^{d}-B))-F} |f_n(x,t)|^2\,dxdt","['functional-analysis', 'lebesgue-integral']"
28,Proving a subset of $H^1(\mathbb{R}^d)$ is compactly embedded in $L^2(\mathbb{R}^d)$.,Proving a subset of  is compactly embedded in .,H^1(\mathbb{R}^d) L^2(\mathbb{R}^d),"I was recenty reading about the weighted Lebesgue spaces and came accross an exercise that asks  to prove that $H^1(\mathbb{R}^d) \cap L^2(\mathbb{R}^d,|x|^2\,dx)$ is compactly embedded in $L^2(\mathbb{R}^d)$ . Where $H^1(\mathbb{R}^d)$ is the usual Sobolev space and $L^2(\mathbb{R}^d,|x|^2\,dx)$ is the weighted Lebesgue space containing functions $f$ for which $\int_{\mathbb{R}^d} |f|^2\,|x|^2\,dx< \infty$ The compact embedding theorems I know work on a bounded domain, not sure how to prove this one.","I was recenty reading about the weighted Lebesgue spaces and came accross an exercise that asks  to prove that is compactly embedded in . Where is the usual Sobolev space and is the weighted Lebesgue space containing functions for which The compact embedding theorems I know work on a bounded domain, not sure how to prove this one.","H^1(\mathbb{R}^d) \cap L^2(\mathbb{R}^d,|x|^2\,dx) L^2(\mathbb{R}^d) H^1(\mathbb{R}^d) L^2(\mathbb{R}^d,|x|^2\,dx) f \int_{\mathbb{R}^d} |f|^2\,|x|^2\,dx< \infty","['functional-analysis', 'lebesgue-integral', 'sobolev-spaces']"
29,On a result of Mazur about convergence in locally convex spaces,On a result of Mazur about convergence in locally convex spaces,,"My question is about the following result from Simon (2011) (Theorem 5.3). Let $X$ be a locally convex space and $Y$ its space of continuous [linear] functionals. Let $\{x_n\}$ be a sequence in $X$ with $x_n \to x_\infty$ in the $\sigma(X,Y)$ -topology. Then, $$ x_\infty = \bigcap_n \mathrm{cch}(\{x_m\}_{m\ge n}). $$ Here, $\sigma(X,Y)$ is the weak topology on $X$ with respect to $Y$ . For a set $A$ , $\mathrm{cch}(A)$ is the closed convex hull of $A$ . Note that $X$ may be endowed with a topology stronger than the weak topology (subject to the requirement that this topology gives $Y$ as its dual). The proof establishes fairly quickly that $x_\infty \in \cap_n \mathrm{cch}(\{x_m\}_{m\ge n})=:A$ . It is, however, silent on the (non-)existence of other points in $A$ . Thus, my question . How does one show that $x_\infty$ is the only member of $A$ ? For reference, here is the proof from the text. Let $C_n=\mathrm{cch}(\{x_m\}_{m\ge n})$ . If $x_\infty \notin C_n$ , there exists $y\in Y$ such that $ \langle y,x_\infty \rangle > \sup_{x\in C_n} \langle y,x\rangle \ge \sup_{m \ge n} \langle y, x_m \rangle$ . This is incompatible with $\langle y, x_n \rangle \to \langle y, x_\infty \rangle$ . If $X$ were a Fréchet space, then the result might follow from a version of Cantor's Intersection Theorem. However, $X$ may not be metrisable. Moreover, showing that the sequence $\{C_n\}$ has vanishing diameter might require that $x_n \to x_\infty$ in the original topology, and this need not be the case.","My question is about the following result from Simon (2011) (Theorem 5.3). Let be a locally convex space and its space of continuous [linear] functionals. Let be a sequence in with in the -topology. Then, Here, is the weak topology on with respect to . For a set , is the closed convex hull of . Note that may be endowed with a topology stronger than the weak topology (subject to the requirement that this topology gives as its dual). The proof establishes fairly quickly that . It is, however, silent on the (non-)existence of other points in . Thus, my question . How does one show that is the only member of ? For reference, here is the proof from the text. Let . If , there exists such that . This is incompatible with . If were a Fréchet space, then the result might follow from a version of Cantor's Intersection Theorem. However, may not be metrisable. Moreover, showing that the sequence has vanishing diameter might require that in the original topology, and this need not be the case.","X Y \{x_n\} X x_n \to x_\infty \sigma(X,Y)  x_\infty = \bigcap_n \mathrm{cch}(\{x_m\}_{m\ge n}).  \sigma(X,Y) X Y A \mathrm{cch}(A) A X Y x_\infty \in \cap_n \mathrm{cch}(\{x_m\}_{m\ge n})=:A A x_\infty A C_n=\mathrm{cch}(\{x_m\}_{m\ge n}) x_\infty \notin C_n y\in Y  \langle y,x_\infty \rangle > \sup_{x\in C_n} \langle y,x\rangle \ge \sup_{m \ge n} \langle y, x_m \rangle \langle y, x_n \rangle \to \langle y, x_\infty \rangle X X \{C_n\} x_n \to x_\infty","['functional-analysis', 'convex-analysis', 'weak-convergence', 'topological-vector-spaces', 'locally-convex-spaces']"
30,Limits on locally convex spaces,Limits on locally convex spaces,,"A curve on a locally convex space is a function $\gamma : I \to F$ where $F$ is a locally convex space and $I \subseteq \mathbb{R}$ is an interval. The curve is differentiable if the following limit exists: $$ \gamma'(x) := \lim_{t \to 0}\frac{\gamma(x+t)-\gamma(x)}{t} $$ but what does this limit mean? I mean...elements $\gamma(x+t)$ and $\gamma(x)$ are in a lcs and this is not (necessarily) a normed space. I'm really stuck at this definition. If $F$ is locally convex, then it is a topological vector space (with, say, a topology given by a family of seminorms). The notion of a limit is replaced by the following. Definition: Let $f: I \subseteq \mathbb{R} \to F$ . We write $\lim_{x \to a}f(x) = L$ if for every neighborhood $V$ of the origin there exists $\delta > 0$ such that $0 \lt |x-a| \lt \delta$ implies $f(x) - L \in V$ . Is this the right definition?","A curve on a locally convex space is a function where is a locally convex space and is an interval. The curve is differentiable if the following limit exists: but what does this limit mean? I mean...elements and are in a lcs and this is not (necessarily) a normed space. I'm really stuck at this definition. If is locally convex, then it is a topological vector space (with, say, a topology given by a family of seminorms). The notion of a limit is replaced by the following. Definition: Let . We write if for every neighborhood of the origin there exists such that implies . Is this the right definition?",\gamma : I \to F F I \subseteq \mathbb{R}  \gamma'(x) := \lim_{t \to 0}\frac{\gamma(x+t)-\gamma(x)}{t}  \gamma(x+t) \gamma(x) F f: I \subseteq \mathbb{R} \to F \lim_{x \to a}f(x) = L V \delta > 0 0 \lt |x-a| \lt \delta f(x) - L \in V,"['functional-analysis', 'analysis', 'locally-convex-spaces']"
31,Help understanding Reproducing Kernel Hilbert spaces?,Help understanding Reproducing Kernel Hilbert spaces?,,"I am trying to wrap my head around some concepts of Reproducing Kernel Hilbert Spaces (RKHS) without having a formal background in functional analysis. Since I am trying to form an intuition about what this space is and how it does what it does, I would appreciate it if you could double-check my reasoning. A RKHS belonging to a kernel $k(x,x')$ (evaluated at $x$ , centered on $x'$ ), $x,x' \in \mathcal{X}$ contains functions of the form $$f(x)=\sum_{i=1}^{m}\alpha_ik(x,x_i)$$ where $\alpha_i \in \mathbb{R}$ are some coefficients and $m \in \mathbb{N}$ is some number to which $i$ is counting the indices. Now RKHSs have inner products. The rules for inner products state that if I have two vectors $\textbf{a}=[a_1,a_2,...,a_m]$ and $\textbf{b}=[b_1,b_2,...,b_m]$ in some $m$ -dimensional vector space, then their inner product $\langle\textbf{a},\textbf{b}\rangle$ is: $$\langle\textbf{a},\textbf{b}\rangle=\sum_{i=1}^{m}a_ib_i$$ One can see the similarity between the right-hand side of the first and second equation. So if we define $f(\cdot)=[\alpha_1,\alpha_2,...,\alpha_m]$ and $k(x,\cdot)=[k(x,x_1),k(x,x_2),...,k(x,x_m)]$ we could express $f(x)$ as an inner product in a RKHS $\mathcal{H}$ : $$f(x)=\langle f(\cdot),k(x,\cdot)\rangle_\mathcal{H}$$ That's the so-called reproducing property, if I understand correctly. Am I correct so far? If yes, I have a few questions: If the RKHS is a space, it should have a dimensionality (in our case $m$ ) and orthonormal bases. If we take a parameter space, for example, each of its dimension has a fixed interpretation (say, a 2-D space with dimensions $x=weight$ and $y=age$ allows for an inner product of two vectors, but both vectors will contain elements of the type $(weight,age)$ ). Now I can see how the expression in Equation 1 can be interpreted as an inner product, but the two vectors (albeit of equal length) contain different elements: $f(\cdot)$ is a vector of scalar coefficients, and $k(x,\cdot)$ is a vector of functions. They do not seem to share their bases in the same sense that the weight-age-example above would. Does this mean that the RKHS $\mathcal{H}$ is sort of a general-purpose space with no fixed definition as to what its dimensions represent, or is there a different interpretation I am missing? My second question relates to the dimensionality $m$ (which I adopted from the Wikipedia article). The way I understand it, this dimensionality $m$ relates to the number of elements in the set $\mathcal{X}$ . Strictly speaking, a function $f(x)$ defined according to Equation 1 could theoretically contain kernels centered on every element $x' \in \mathcal{X}$ , in which case the dimensionality of $\mathcal{H}$ would be as large as the set itself ( $m=|\mathcal{X}|$ ) and possibly infinite if the set $\mathcal{X}$ is infinitely large (e.g., $\mathcal{X}$ is the continuous real line). Wouldn't specifying $|\mathcal{X}|$ as the upper limit of the sum be more general than $m$ ? If we are only interested in $f(x)$ which are based only on a subset of $\mathcal{X}$ we could still sum over all theoretically possible dimensions and throw out the irrelevant kernels by setting their corresponding entries in $f(\cdot)$ to zero. Is this right or am I missing something?","I am trying to wrap my head around some concepts of Reproducing Kernel Hilbert Spaces (RKHS) without having a formal background in functional analysis. Since I am trying to form an intuition about what this space is and how it does what it does, I would appreciate it if you could double-check my reasoning. A RKHS belonging to a kernel (evaluated at , centered on ), contains functions of the form where are some coefficients and is some number to which is counting the indices. Now RKHSs have inner products. The rules for inner products state that if I have two vectors and in some -dimensional vector space, then their inner product is: One can see the similarity between the right-hand side of the first and second equation. So if we define and we could express as an inner product in a RKHS : That's the so-called reproducing property, if I understand correctly. Am I correct so far? If yes, I have a few questions: If the RKHS is a space, it should have a dimensionality (in our case ) and orthonormal bases. If we take a parameter space, for example, each of its dimension has a fixed interpretation (say, a 2-D space with dimensions and allows for an inner product of two vectors, but both vectors will contain elements of the type ). Now I can see how the expression in Equation 1 can be interpreted as an inner product, but the two vectors (albeit of equal length) contain different elements: is a vector of scalar coefficients, and is a vector of functions. They do not seem to share their bases in the same sense that the weight-age-example above would. Does this mean that the RKHS is sort of a general-purpose space with no fixed definition as to what its dimensions represent, or is there a different interpretation I am missing? My second question relates to the dimensionality (which I adopted from the Wikipedia article). The way I understand it, this dimensionality relates to the number of elements in the set . Strictly speaking, a function defined according to Equation 1 could theoretically contain kernels centered on every element , in which case the dimensionality of would be as large as the set itself ( ) and possibly infinite if the set is infinitely large (e.g., is the continuous real line). Wouldn't specifying as the upper limit of the sum be more general than ? If we are only interested in which are based only on a subset of we could still sum over all theoretically possible dimensions and throw out the irrelevant kernels by setting their corresponding entries in to zero. Is this right or am I missing something?","k(x,x') x x' x,x' \in \mathcal{X} f(x)=\sum_{i=1}^{m}\alpha_ik(x,x_i) \alpha_i \in \mathbb{R} m \in \mathbb{N} i \textbf{a}=[a_1,a_2,...,a_m] \textbf{b}=[b_1,b_2,...,b_m] m \langle\textbf{a},\textbf{b}\rangle \langle\textbf{a},\textbf{b}\rangle=\sum_{i=1}^{m}a_ib_i f(\cdot)=[\alpha_1,\alpha_2,...,\alpha_m] k(x,\cdot)=[k(x,x_1),k(x,x_2),...,k(x,x_m)] f(x) \mathcal{H} f(x)=\langle f(\cdot),k(x,\cdot)\rangle_\mathcal{H} m x=weight y=age (weight,age) f(\cdot) k(x,\cdot) \mathcal{H} m m \mathcal{X} f(x) x' \in \mathcal{X} \mathcal{H} m=|\mathcal{X}| \mathcal{X} \mathcal{X} |\mathcal{X}| m f(x) \mathcal{X} f(\cdot)","['functional-analysis', 'hilbert-spaces', 'reproducing-kernel-hilbert-spaces', 'function-spaces']"
32,Spectrum of the product of positive elements of a $C^\ast$-algebra.,Spectrum of the product of positive elements of a -algebra.,C^\ast,"I am working on problem 2.a in Murphy's $\textit{$C^\ast$-Algebras and Operator Theory}$ , which asks to show that for positive elements $a, b$ of a unital $C^\ast$ -algebra $A$ , $\sigma(ab) \subset [0, \infty)$ . By the definition given in this textbook, $a \in A$ is positive if $a$ is hermitian and $\sigma(a) \subset [0, \infty)$ . It's true that, if $a$ and $b$ commute, then $ab$ is positive, as: $$ab = (a^{1/2}b^{1/2})^\ast (a^{1/2}b^{1/2}),$$ from which it follows that $\sigma(ab) \subset [0, \infty)$ . Then, to solve the given problem, I invoke the following argument: $$\sigma(ab) \cup \{0\} = \sigma((a^{1/2}b^{1/2})^\ast (a^{1/2}b^{1/2})) \cup \{0\} \subset [0, \infty),$$ from which it follows that $\sigma(ab) \subset [0, \infty)$ . My question is this: we have that, for arbitrary positive $a, b \in A$ , $a$ and $b$ are hermitian, from which it follows that $ab$ is hermitian. Furthermore, by the above argument, it follows that $\sigma(ab) \subset [0, \infty)$ . Does it not follow from this that $ab$ is positive for arbitrary positive $a,b$ ?","I am working on problem 2.a in Murphy's , which asks to show that for positive elements of a unital -algebra , . By the definition given in this textbook, is positive if is hermitian and . It's true that, if and commute, then is positive, as: from which it follows that . Then, to solve the given problem, I invoke the following argument: from which it follows that . My question is this: we have that, for arbitrary positive , and are hermitian, from which it follows that is hermitian. Furthermore, by the above argument, it follows that . Does it not follow from this that is positive for arbitrary positive ?","\textit{C^\ast-Algebras and Operator Theory} a, b C^\ast A \sigma(ab) \subset [0, \infty) a \in A a \sigma(a) \subset [0, \infty) a b ab ab = (a^{1/2}b^{1/2})^\ast (a^{1/2}b^{1/2}), \sigma(ab) \subset [0, \infty) \sigma(ab) \cup \{0\} = \sigma((a^{1/2}b^{1/2})^\ast (a^{1/2}b^{1/2})) \cup \{0\} \subset [0, \infty), \sigma(ab) \subset [0, \infty) a, b \in A a b ab \sigma(ab) \subset [0, \infty) ab a,b","['functional-analysis', 'operator-theory', 'spectral-theory', 'c-star-algebras']"
33,A simple $C^*$ algebra whose all idempotents are contained in a given disc,A simple  algebra whose all idempotents are contained in a given disc,C^*,Assume that $A$ is a simple $C^*$ algebra and $K>0$ is a fixed number. Assume that all idempotents $e\in A$ satisfy $|e|<K$ . Does this imply that $A$ has no non trivial idempotent?,Assume that is a simple algebra and is a fixed number. Assume that all idempotents satisfy . Does this imply that has no non trivial idempotent?,A C^* K>0 e\in A |e|<K A,"['functional-analysis', 'operator-algebras', 'c-star-algebras']"
34,Functions near any Lipschitz,Functions near any Lipschitz,,"Let $\operatorname{Lip}\subseteq C([0,1]^d,\mathbb{R}^d)$ be the set of all Lipschitz functions from $[0,1]^d$ to $\mathbb{R}^d$ . Which non-affine functions $f \in C(\mathbb{R}^d,\mathbb{R}^d)$ satisfy: There exists some $\epsilon\geq \delta\triangleq K\epsilon>0$ such that, for every ${g} \in Lip$ and every $g_1,g_2 \in Ball_{\epsilon}({g})$ $$ \inf_{          \underset{B,A\in Mat_{d\times d}(\mathbb{R})}{a,b \in \mathbb{R}^d} } \sup_{x \in [0,1]^d} \big\|(a + A\, f (B\, g_1(x)+b)) - g_2(x)\big\|_2 < \delta. $$ Intuitively, I would expect that it's necessary for $f$ to be Lipschitz, maybe contractive. Note: $Ball_{\epsilon}(g)\triangleq \left\{h \in C([0,1]^d;\mathbb{R}^d):\,  \sup_{x \in [0,1]^d} \big\|h(x)- g(x)\big\|_2<\epsilon \right\}$ (for $\epsilon >0$ and $g \in C([0,1]^d;\mathbb{R}^d)$ ).","Let be the set of all Lipschitz functions from to . Which non-affine functions satisfy: There exists some such that, for every and every Intuitively, I would expect that it's necessary for to be Lipschitz, maybe contractive. Note: (for and ).","\operatorname{Lip}\subseteq C([0,1]^d,\mathbb{R}^d) [0,1]^d \mathbb{R}^d f \in C(\mathbb{R}^d,\mathbb{R}^d) \epsilon\geq \delta\triangleq K\epsilon>0 {g} \in Lip g_1,g_2 \in Ball_{\epsilon}({g}) 
\inf_{
         \underset{B,A\in Mat_{d\times d}(\mathbb{R})}{a,b \in \mathbb{R}^d}
}
\sup_{x \in [0,1]^d} \big\|(a + A\, f (B\, g_1(x)+b)) - g_2(x)\big\|_2 < \delta.
 f Ball_{\epsilon}(g)\triangleq \left\{h \in C([0,1]^d;\mathbb{R}^d):\, 
\sup_{x \in [0,1]^d} \big\|h(x)- g(x)\big\|_2<\epsilon
\right\} \epsilon >0 g \in C([0,1]^d;\mathbb{R}^d)","['real-analysis', 'functional-analysis']"
35,Dual space of quotient of C$^*$-algebras,Dual space of quotient of C-algebras,^*,I'm interested in understanding what the dual space of a quotient of a $C^*$ -algebra $A$ looks like. Let $A$ denote a $C^*$ -algebra and $I$ a closed ideal therein. Denote the dual space of $A$ by $B$ . I think one can say something like: The dual space $B_I$ of the quotient $A/I$ is canonically isometrically isomorphic to a weak $^*$ closed subset of B. Is this true? Thank you very much!,I'm interested in understanding what the dual space of a quotient of a -algebra looks like. Let denote a -algebra and a closed ideal therein. Denote the dual space of by . I think one can say something like: The dual space of the quotient is canonically isometrically isomorphic to a weak closed subset of B. Is this true? Thank you very much!,C^* A A C^* I A B B_I A/I ^*,"['functional-analysis', 'operator-algebras']"
36,When does $e^{hD}$ give a well defined operator on analytic functions?,When does  give a well defined operator on analytic functions?,e^{hD},"Let $D$ denote the differentiation operator. It is a classic result that $e^{cD}f(a)=f(a+c)$ if $f(x)$ is an analytic function and the radius of convergence of the Taylor series of $f(x)$ at $x=a$ is bigger than $c$ .  More generally, if $T$ is a linear operator on an algebra satisfying $T(ab)=T(a)b+aT(b)$ and $S=e^T$ is well defined (e.g., if $T$ is locally nilpotent or the algebra has a norm and $T$ is bounded), then $S(ab)=S(a)S(b)$ .  This implies that if $f$ is an analytic function, and if everything actually makes sense, then $S(f(a))=f(S(a))$ . In particular, if $T=h(x)D$ and $S=e^{h(x)D}$ , then defining $g(x)=S(x)$ (the left hand side is a function of $x$ , the right hand side is an operator applied to the identity function) satisfies $S(f(x))=f(g(x))$ , at least when things work out and everything is well defined. Question: What conditions on $h$ or $f$ make things work out, i.e., turn this formal argument into a rigorous argument? Some observations.  If $T(f)=\lambda f$ , then $e^T(f)=e^{\lambda}f$ , and if $h(x)f'(x)=\lambda f(x)$ , then $f(x)=e^{\lambda \int 1/h(x) dx}$ .  If we set $H(x)=\int 1/h(x) dx$ , then this implies that $H(g(x))=1+H(x)$ .  If this functional equation has no solution, or if the solution was not analytic, that would show that $e^{hD}$ was not a well defined operator, but short of that, I don't see a good approach to the problem.  Trying to get an explicit formula just seems too messy.","Let denote the differentiation operator. It is a classic result that if is an analytic function and the radius of convergence of the Taylor series of at is bigger than .  More generally, if is a linear operator on an algebra satisfying and is well defined (e.g., if is locally nilpotent or the algebra has a norm and is bounded), then .  This implies that if is an analytic function, and if everything actually makes sense, then . In particular, if and , then defining (the left hand side is a function of , the right hand side is an operator applied to the identity function) satisfies , at least when things work out and everything is well defined. Question: What conditions on or make things work out, i.e., turn this formal argument into a rigorous argument? Some observations.  If , then , and if , then .  If we set , then this implies that .  If this functional equation has no solution, or if the solution was not analytic, that would show that was not a well defined operator, but short of that, I don't see a good approach to the problem.  Trying to get an explicit formula just seems too messy.",D e^{cD}f(a)=f(a+c) f(x) f(x) x=a c T T(ab)=T(a)b+aT(b) S=e^T T T S(ab)=S(a)S(b) f S(f(a))=f(S(a)) T=h(x)D S=e^{h(x)D} g(x)=S(x) x S(f(x))=f(g(x)) h f T(f)=\lambda f e^T(f)=e^{\lambda}f h(x)f'(x)=\lambda f(x) f(x)=e^{\lambda \int 1/h(x) dx} H(x)=\int 1/h(x) dx H(g(x))=1+H(x) e^{hD},"['real-analysis', 'complex-analysis', 'functional-analysis', 'operator-theory']"
37,Fourier transform of function defined on finite interval,Fourier transform of function defined on finite interval,,"Let $f(t)$ be a function defined on the finite interval $[t_1,\, t_2]$ . Is the Fourier transform of such a function uniquely defined? In the sense that there exists only one function $\hat{f}(\omega)$ giving the Fourier coefficients that sum up to $f(t)$ in the interval $[t_1,\,t_2]$ ? I am aware that one can find the Fourier transform of a function $g_1(t)$ defined on $[-\infty, \,+\infty]$ as $$g_1(t) = \begin{cases} f(t) & \text{for}\;t \in [t_1, t_2]\\ 0 & \text{for}\;t \lt t_1 \;\text{and}\;t \gt t_2 \end{cases} $$ The Fourier transform of this function would yield the coefficients that sum up to $f(t)$ on the interval $[t_1,\,t_2]$ , I think. However, the same could be said about functions $g_2(t)$ and $g_3(t)$ defined via $$g_2(t) = \begin{cases} f(t) & \text{for}\;t \in [t_1, t_2]\\ 4 & \text{for}\;t \lt t_1 \;\text{and}\;t \gt t_2 \end{cases} $$ $$g_3(t) = \begin{cases} f(t) & \text{for}\;t \in [t_1, t_2]\\ t & \text{for}\;t \lt t_1 \;\text{and}\;t \gt t_2 \end{cases} $$ or any other function that matches the definition of $f(t)$ on $[t_1,\,t_2]$ and somehow continues this to be defined on $[-\infty,\,+\infty]$ . In particular, by continuing $f(t)$ periodically on $[-\infty,\,+\infty]$ , one would get a Fourier series that would also be a valid representation of $f(t)$ on $[t_1,\,t_2]$ . Or is this reasoning incorrect? If not, in what way does the concept of a Fourier transform apply to functions that are only defined on a finite interval?","Let be a function defined on the finite interval . Is the Fourier transform of such a function uniquely defined? In the sense that there exists only one function giving the Fourier coefficients that sum up to in the interval ? I am aware that one can find the Fourier transform of a function defined on as The Fourier transform of this function would yield the coefficients that sum up to on the interval , I think. However, the same could be said about functions and defined via or any other function that matches the definition of on and somehow continues this to be defined on . In particular, by continuing periodically on , one would get a Fourier series that would also be a valid representation of on . Or is this reasoning incorrect? If not, in what way does the concept of a Fourier transform apply to functions that are only defined on a finite interval?","f(t) [t_1,\, t_2] \hat{f}(\omega) f(t) [t_1,\,t_2] g_1(t) [-\infty, \,+\infty] g_1(t) =
\begin{cases}
f(t) & \text{for}\;t \in [t_1, t_2]\\
0 & \text{for}\;t \lt t_1 \;\text{and}\;t \gt t_2
\end{cases}
 f(t) [t_1,\,t_2] g_2(t) g_3(t) g_2(t) =
\begin{cases}
f(t) & \text{for}\;t \in [t_1, t_2]\\
4 & \text{for}\;t \lt t_1 \;\text{and}\;t \gt t_2
\end{cases}
 g_3(t) =
\begin{cases}
f(t) & \text{for}\;t \in [t_1, t_2]\\
t & \text{for}\;t \lt t_1 \;\text{and}\;t \gt t_2
\end{cases}
 f(t) [t_1,\,t_2] [-\infty,\,+\infty] f(t) [-\infty,\,+\infty] f(t) [t_1,\,t_2]","['functional-analysis', 'fourier-analysis', 'fourier-transform']"
38,A proper subspace of a normed vector space has empty interior clarification,A proper subspace of a normed vector space has empty interior clarification,,"So every proper subspace of a normed vector space has empty interior. I'm not asking for the proof, my problem is that this seems to me very strange.  So if I have a normed vector space, in any proper subspace I can't take any ball inside the subspace? For example suppose we work on a set with finite measure, $[a,b]$ for example. Let's take $L^{P}$ spaces over $[a,b]$ . We know that now $L^{\infty}$ is included in $L^{1}$ . So $L^{\infty}$ is a proper subspace of $L^{1}$ . Now this means that $L^{\infty}$ is nowhere dense?","So every proper subspace of a normed vector space has empty interior. I'm not asking for the proof, my problem is that this seems to me very strange.  So if I have a normed vector space, in any proper subspace I can't take any ball inside the subspace? For example suppose we work on a set with finite measure, for example. Let's take spaces over . We know that now is included in . So is a proper subspace of . Now this means that is nowhere dense?","[a,b] L^{P} [a,b] L^{\infty} L^{1} L^{\infty} L^{1} L^{\infty}","['functional-analysis', 'lp-spaces']"
39,Multipliers and corners of $C^*$-algebras,Multipliers and corners of -algebras,C^*,"Let $A$ and $B$ be $C^*$ -algebras. Suppose that there exists a projection $p$ in $\mathcal{M}(B)$ , the multiplier algebra of $B$ , such that $A=pBp$ . That is, $A$ is a corner of $B$ . Question: Is it true that the corner $p(\mathcal{M}(B))p$ contains the multiplier algebra of $A$ (which we view as a subalgebra of $B$ )? In other words, does every multiplier of the corner come from the corner of the multiplier algebra?","Let and be -algebras. Suppose that there exists a projection in , the multiplier algebra of , such that . That is, is a corner of . Question: Is it true that the corner contains the multiplier algebra of (which we view as a subalgebra of )? In other words, does every multiplier of the corner come from the corner of the multiplier algebra?",A B C^* p \mathcal{M}(B) B A=pBp A B p(\mathcal{M}(B))p A B,"['functional-analysis', 'operator-theory', 'operator-algebras', 'c-star-algebras']"
40,Is the invariant subspace problem open for invertible maps?,Is the invariant subspace problem open for invertible maps?,,"Let $T: H \rightarrow H$ be a bounded linear operator with bounded inverse on the separable complex Hilbert space. Does $T$ preserve a closed proper non-trival invariant subspace? I'm aware the question is (famously) open for bounded linear maps, and of partial results, but no survey (or Tao's blog, etc) seem to address the invertible case. If it is open, does a positive or negative answer imply the answer in the non-invertible case?","Let be a bounded linear operator with bounded inverse on the separable complex Hilbert space. Does preserve a closed proper non-trival invariant subspace? I'm aware the question is (famously) open for bounded linear maps, and of partial results, but no survey (or Tao's blog, etc) seem to address the invertible case. If it is open, does a positive or negative answer imply the answer in the non-invertible case?",T: H \rightarrow H T,"['real-analysis', 'functional-analysis']"
41,The uniqueness of the continuation of the functional.,The uniqueness of the continuation of the functional.,,"I have a continuous linear functional $f_0 \in c_0^*$ where $$c_0 = \lbrace{x: (x_1, \dots, x_n, \dots)|\lim_{n\rightarrow\infty}x_n = 0\rbrace} \subset m = \lbrace{x: (x_1, \dots, x_n, \dots)|\sup_{n\in\mathbb{N}}|x_n| < \infty\rbrace}$$ My question: Prove that $f_0$ have a uniqueness continuation $f$ on space $m$ with respecting the norm, i.e $$\forall x \in c_0,\ f(x) = f_0(x)\quad \text{and}\quad ||f_0|| = ||f||.$$ The Hahn-Banach theorem guarantees the existence of such a continuation, but not its uniqueness. I tried to figure out some properties of the space conjugate to $c_0$ . I proved that $c_0^* \cong l^1$ . Having proved this, I saw how functionals on $c_0$ looks like. Then I proved that $(l^1)^* \cong m$ and saw how functionals on $l^1$ looks like. This is good, because in these spaces I cannot use the Riesz theorem, but, as I said, in the process of the proof I obtained the general form of functionals on these spaces. But further, I do not know what to do. How to use this information? Or maybe there are some tricky lemmas or theorems that give information about the uniqueness of the continuation? Could you tell me what to do next? Thanks in advance!","I have a continuous linear functional where My question: Prove that have a uniqueness continuation on space with respecting the norm, i.e The Hahn-Banach theorem guarantees the existence of such a continuation, but not its uniqueness. I tried to figure out some properties of the space conjugate to . I proved that . Having proved this, I saw how functionals on looks like. Then I proved that and saw how functionals on looks like. This is good, because in these spaces I cannot use the Riesz theorem, but, as I said, in the process of the proof I obtained the general form of functionals on these spaces. But further, I do not know what to do. How to use this information? Or maybe there are some tricky lemmas or theorems that give information about the uniqueness of the continuation? Could you tell me what to do next? Thanks in advance!","f_0 \in c_0^* c_0 = \lbrace{x: (x_1, \dots, x_n, \dots)|\lim_{n\rightarrow\infty}x_n = 0\rbrace} \subset m = \lbrace{x: (x_1, \dots, x_n, \dots)|\sup_{n\in\mathbb{N}}|x_n| < \infty\rbrace} f_0 f m \forall x \in c_0,\ f(x) = f_0(x)\quad \text{and}\quad ||f_0|| = ||f||. c_0 c_0^* \cong l^1 c_0 (l^1)^* \cong m l^1","['functional-analysis', 'lp-spaces']"
42,Finite rank operators on Hilbert spaces,Finite rank operators on Hilbert spaces,,"Let $H$ be a Hilbert space. Question 1: Are all rank one operators from $H$ to $H$ is of the form $$T:H\rightarrow H, x \mapsto \langle x,u\rangle v $$ For some $u,v \in H$ . Question 2: Suppose $I \subseteq L(H)$ is an ideal and contains all the rank one operators, how do we show it contains all the finite rank operators? These two statements seem to be true, but I could not  find any reference. After some thought: Let us fix orthonormal basis $\{u_i\}$ of $H$ . We have two observations: Operator $T^*$ exists. So $$ Tx = \sum \langle Tx , u_i \rangle u_i = \sum \langle x, T^*u_i \rangle u_i $$ $$x \mapsto \langle x,v \rangle w$$ are rank one if $v \not=0, w \not= 0$ . Combining the above two, $T$ is rank one if and only if it is of the form $x \mapsto \langle x,v \rangle w $ . Any finite rank operator, must again be of the form $\sum_j \langle x, v_j \rangle w_j$ (finite sum). These are generated by the rank one operators. I would be happy if anyone point some possible pitfalls / mistake I made in my proof.","Let be a Hilbert space. Question 1: Are all rank one operators from to is of the form For some . Question 2: Suppose is an ideal and contains all the rank one operators, how do we show it contains all the finite rank operators? These two statements seem to be true, but I could not  find any reference. After some thought: Let us fix orthonormal basis of . We have two observations: Operator exists. So are rank one if . Combining the above two, is rank one if and only if it is of the form . Any finite rank operator, must again be of the form (finite sum). These are generated by the rank one operators. I would be happy if anyone point some possible pitfalls / mistake I made in my proof.","H H H T:H\rightarrow H, x \mapsto \langle x,u\rangle v  u,v \in H I \subseteq L(H) \{u_i\} H T^*  Tx = \sum \langle Tx , u_i \rangle u_i = \sum \langle x, T^*u_i \rangle u_i  x \mapsto \langle x,v \rangle w v \not=0, w \not= 0 T x \mapsto \langle x,v \rangle w  \sum_j \langle x, v_j \rangle w_j","['functional-analysis', 'operator-theory', 'hilbert-spaces', 'operator-algebras', 'compact-operators']"
43,the spectrum of a bounded linear operator on $X\times X$,the spectrum of a bounded linear operator on,X\times X,"If we consider $X\neq\{0\}$ to be a complex Banach space then the product $X\times X$ is a Banach space with the norm $\|(x,y)\|=\|x\|+\|y\|$ . $T(x,y)=(x + y,x - y)$ is then a bounded linear operator on $X\times X$ . Find $\sigma(T)$ and the subsets $\sigma_p(T),\sigma_c(T), \sigma_r(T)$ , where $\sigma(T)$ is the spectrum of operator $T$ , and the subsets are the point spectrum, the continuous spectrum, and the residual spectrum, respectively. I am stuck at this problem. I know that the spectrum is supposed to be the set of eigenvalues, which I am supposed to find with matrices I believe. So I'd have to compute $T-\lambda I$ but I am not sure how the matrix of $T$ looks in this space? Then of course I'd have to compute the subsets which seems even harder.","If we consider to be a complex Banach space then the product is a Banach space with the norm . is then a bounded linear operator on . Find and the subsets , where is the spectrum of operator , and the subsets are the point spectrum, the continuous spectrum, and the residual spectrum, respectively. I am stuck at this problem. I know that the spectrum is supposed to be the set of eigenvalues, which I am supposed to find with matrices I believe. So I'd have to compute but I am not sure how the matrix of looks in this space? Then of course I'd have to compute the subsets which seems even harder.","X\neq\{0\} X\times X \|(x,y)\|=\|x\|+\|y\| T(x,y)=(x + y,x - y) X\times X \sigma(T) \sigma_p(T),\sigma_c(T), \sigma_r(T) \sigma(T) T T-\lambda I T","['functional-analysis', 'banach-spaces', 'spectral-theory']"
44,Show that $(y_n)\in l^q$,Show that,(y_n)\in l^q,"The problem I'm trying to solve is: Let $1\leq p, q\leq\infty$ such that $\frac{1}{q}+\frac{1}{p}=1$ . If $$\sum_{n=1}^{\infty} |x_n||y_n|<\infty$$ for all $(x_n)\in l^p$ , then $(y_n)\in l^q$ . $\bullet$ If $p=\infty$ , then picking $(x_n)=(1,1,1,...)\in l^\infty$ , we have $$\sum_{n=1}^{\infty} |y_n|<\infty$$ i.e. $(y_n)\in l^1$ . but how can I prove for $1\leq p<\infty$ ?","The problem I'm trying to solve is: Let such that . If for all , then . If , then picking , we have i.e. . but how can I prove for ?","1\leq p, q\leq\infty \frac{1}{q}+\frac{1}{p}=1 \sum_{n=1}^{\infty} |x_n||y_n|<\infty (x_n)\in l^p (y_n)\in l^q \bullet p=\infty (x_n)=(1,1,1,...)\in l^\infty \sum_{n=1}^{\infty} |y_n|<\infty (y_n)\in l^1 1\leq p<\infty","['functional-analysis', 'lp-spaces']"
45,"$[T,S]:=TS-ST=I$ cannot holds",cannot holds,"[T,S]:=TS-ST=I","Let $\mathcal{B}(\mathcal{H})$ the algebra of all bounded linear operators on an infinite dimensional complex Hilbert space $\mathcal{H}$ . Let $T,S\in \mathcal{B}(\mathcal{H})$ . I want to prove that the equality \begin{equation}\label{commz} [T,S]:=TS-ST=I \tag{1}, \end{equation} cannot hold. To see this, assume that $(1)$ holds. We shall prove by induction that \begin{equation}\label{tag2} [T, S^n] = nS^{n - 1},\;n\in \mathbb{N}^*. \end{equation} By assumption, $[T,S]=S^{0}=I$ . Suppose $[T,S^{n}]= nS^{n-1}$ for some $n\in \mathbb{N}^*$ . Then \begin{align*}    [T,S^{n+1}]   & = TS^{n+1}-S^{n+1}T\\     & =(TS^{n}-S^{n}T)S+S^{n}TS-S^{n+1}T \\      & = [T,S^{n}]S+S^{n}[T,S] \\      & = nS^{n-1}S+S^{n}=(n+1) S^{n}. \end{align*} So, \begin{equation*}\label{tag11} TS^n - S^nT = n S^{n=1}, \end{equation*} holds for all $n\in \mathbb{N}^*$ . Hence, \begin{align*} n\|S^{n-1}\| & = \|TS^n - S^nT\|\\  &\leq 2 \|T\|\cdot\|S^{n} \|\\  &\leq 2 \|T\|\cdot\|S\|\cdot\|S^{n-1} \|. \end{align*} If $\|S^{n-1} \|\ne 0$ , we get $n \le 2 \|T\|\|S\|$ , for all $n\in \mathbb{N}^*$ . This leads to a contradiction. So, $(1)$ cannot hold for $T,S\in \mathcal{B}(\mathcal{H})$ . Why $S^{n-1}\ne 0$ for all $n\in \mathbb{N}^*$ ?","Let the algebra of all bounded linear operators on an infinite dimensional complex Hilbert space . Let . I want to prove that the equality cannot hold. To see this, assume that holds. We shall prove by induction that By assumption, . Suppose for some . Then So, holds for all . Hence, If , we get , for all . This leads to a contradiction. So, cannot hold for . Why for all ?","\mathcal{B}(\mathcal{H}) \mathcal{H} T,S\in \mathcal{B}(\mathcal{H}) \begin{equation}\label{commz}
[T,S]:=TS-ST=I \tag{1},
\end{equation} (1) \begin{equation}\label{tag2}
[T, S^n] = nS^{n - 1},\;n\in \mathbb{N}^*.
\end{equation} [T,S]=S^{0}=I [T,S^{n}]= nS^{n-1} n\in \mathbb{N}^* \begin{align*}
   [T,S^{n+1}]
  & = TS^{n+1}-S^{n+1}T\\
    & =(TS^{n}-S^{n}T)S+S^{n}TS-S^{n+1}T \\
     & = [T,S^{n}]S+S^{n}[T,S] \\
     & = nS^{n-1}S+S^{n}=(n+1) S^{n}.
\end{align*} \begin{equation*}\label{tag11}
TS^n - S^nT = n S^{n=1},
\end{equation*} n\in \mathbb{N}^* \begin{align*}
n\|S^{n-1}\|
& = \|TS^n - S^nT\|\\
 &\leq 2 \|T\|\cdot\|S^{n} \|\\
 &\leq 2 \|T\|\cdot\|S\|\cdot\|S^{n-1} \|.
\end{align*} \|S^{n-1} \|\ne 0 n \le 2 \|T\|\|S\| n\in \mathbb{N}^* (1) T,S\in \mathcal{B}(\mathcal{H}) S^{n-1}\ne 0 n\in \mathbb{N}^*","['functional-analysis', 'proof-verification', 'operator-theory']"
46,Prove that $T$ is a bounded operator on a disk algebra and prove the existence of a Borel measure on a boundary of an open unit disk.,Prove that  is a bounded operator on a disk algebra and prove the existence of a Borel measure on a boundary of an open unit disk.,T,"Let $A(D)$ be the space of holomorphic functions on the open unit disk $D$ and continuous on the closed disk $\bar{D}$ . Then $A(D)$ is a Banach space if we set $\|f\|=\sup\{|f(z)|:z\in\bar{D}\}$ . For $f\in A(D)$ write $$ f(z)=\sum_{n=0}^{\infty}a_nz^n. $$ a) Let $\{c_n\}_{n=0}^{\infty}$ be a sequence of complex numbers with the property that if $f\in A(D)$ represented as above, and $$ f^*(z)=\sum_{n=0}^{\infty}c_na_nz^n, $$ then $f^*\in A(D)$ . Prove that there exists $C>0$ such that $\|f^*\|\leq C\|f\|$ for all $f\in A(D)$ . I am thinking that I need to prove $T:A(D)\to A(D)$ with $T(f)=f^*$ is bounded (or continuous). Do you have any other ideas? b) Let $\omega\in D$ . Prove that there exists a Borel measure $\mu_{\omega}$ on $\partial D=\{z:|z|=1\}$ such that $$ f(\omega)=\int_{\partial D}f(t)d\mu_{\omega}(t),\;f\in A. $$ I am thinking that we can use the Maximum Modulus principle, but is the measure $\mu_{\omega}$ unique? Could you please give me some ideas? Thank you so much.","Let be the space of holomorphic functions on the open unit disk and continuous on the closed disk . Then is a Banach space if we set . For write a) Let be a sequence of complex numbers with the property that if represented as above, and then . Prove that there exists such that for all . I am thinking that I need to prove with is bounded (or continuous). Do you have any other ideas? b) Let . Prove that there exists a Borel measure on such that I am thinking that we can use the Maximum Modulus principle, but is the measure unique? Could you please give me some ideas? Thank you so much.","A(D) D \bar{D} A(D) \|f\|=\sup\{|f(z)|:z\in\bar{D}\} f\in A(D) 
f(z)=\sum_{n=0}^{\infty}a_nz^n.
 \{c_n\}_{n=0}^{\infty} f\in A(D) 
f^*(z)=\sum_{n=0}^{\infty}c_na_nz^n,
 f^*\in A(D) C>0 \|f^*\|\leq C\|f\| f\in A(D) T:A(D)\to A(D) T(f)=f^* \omega\in D \mu_{\omega} \partial D=\{z:|z|=1\} 
f(\omega)=\int_{\partial D}f(t)d\mu_{\omega}(t),\;f\in A.
 \mu_{\omega}","['functional-analysis', 'analysis', 'banach-spaces', 'holomorphic-functions', 'maximum-principle']"
47,Operator norm $ ( \ell_2 \to \ell_1)$,Operator norm, ( \ell_2 \to \ell_1),"Let $X$ be a ﬁnite dimensional normed vector space and $Y$ an arbitrary normed vector space. $ T:X→Y$ . I want to  calculate $\|T\|$ for where $X = K^n$ , equipped with the Euclidean norm $\|\cdot\|_2$ , $Y := \ell_1(\mathbb{N})$ and $Tx := (x_1,\ldots,x_n,0,0,\ldots) \in \ell_1(\mathbb{N})$ , for all $x = (x_1,\ldots,x_n) \in K^n$ . I do not know how to continue $$ ||T∥_2 = \sup \limits_{x \neq 0} \frac{∥Tx∥_1}{∥x∥_2} =  \sup \limits_{x \neq 0} \frac{∥(x_1,…,x_n,0,0,…)∥_1}{∥(x_1,…,x_n)∥_2} = \sup \limits_{x \neq 0} \frac{|x_1|+…+|x_n|}{(|x_1|^2+…+|x_n|^2)^{\frac{1}{2}}}= ? $$","Let be a ﬁnite dimensional normed vector space and an arbitrary normed vector space. . I want to  calculate for where , equipped with the Euclidean norm , and , for all . I do not know how to continue","X Y  T:X→Y \|T\| X = K^n \|\cdot\|_2 Y := \ell_1(\mathbb{N}) Tx := (x_1,\ldots,x_n,0,0,\ldots) \in \ell_1(\mathbb{N}) x = (x_1,\ldots,x_n) \in K^n  ||T∥_2 = \sup \limits_{x \neq 0} \frac{∥Tx∥_1}{∥x∥_2} =  \sup \limits_{x \neq 0} \frac{∥(x_1,…,x_n,0,0,…)∥_1}{∥(x_1,…,x_n)∥_2} = \sup \limits_{x \neq 0} \frac{|x_1|+…+|x_n|}{(|x_1|^2+…+|x_n|^2)^{\frac{1}{2}}}= ? ","['functional-analysis', 'operator-theory', 'normed-spaces', 'lp-spaces']"
48,Integral equations and the Fredholm alternative / theory,Integral equations and the Fredholm alternative / theory,,"The Fredholm alternative states that either: $$ 0 = \lambda \phi(x) - \int_a^b K(x,y) \phi(y) dy $$ has a non-trivial solution, or: $$ f(x) = \lambda \phi(x) - \int_a^b K(x,y) \phi(y) dy $$ always has a unique solution for any f(x) A sufficient condition is for the kernel K to be square-integrable, but depending on sources there is some confusion whether $\lambda$ must be a non-zero complex number.  For example, Wikipedia says so but also refers to this page which doesn't... (Note that $\lambda=0$ the integral equation is called Fredholm of the first kind, and $\lambda \neq 0$ is of second kind.) Adding to the confusion, some authors prefer to have $\lambda$ scaling the integral rather than $\phi(x)$ , in which case it makes sense to require it to be nonzero. Could anyone clarify whether the Fredholm alternative is only true of second kind equations (i.e. $\lambda \neq 0$ )?  And if so, why...? Thanks!","The Fredholm alternative states that either: has a non-trivial solution, or: always has a unique solution for any f(x) A sufficient condition is for the kernel K to be square-integrable, but depending on sources there is some confusion whether must be a non-zero complex number.  For example, Wikipedia says so but also refers to this page which doesn't... (Note that the integral equation is called Fredholm of the first kind, and is of second kind.) Adding to the confusion, some authors prefer to have scaling the integral rather than , in which case it makes sense to require it to be nonzero. Could anyone clarify whether the Fredholm alternative is only true of second kind equations (i.e. )?  And if so, why...? Thanks!"," 0 = \lambda \phi(x) - \int_a^b K(x,y) \phi(y) dy   f(x) = \lambda \phi(x) - \int_a^b K(x,y) \phi(y) dy  \lambda \lambda=0 \lambda \neq 0 \lambda \phi(x) \lambda \neq 0","['functional-analysis', 'hilbert-spaces', 'integral-equations']"
49,Action of a projection on a pairwise orthogonal projections.,Action of a projection on a pairwise orthogonal projections.,,Let us consider two projections $e_1$ and $e_2$ on the Hilbert space $H$ with $e_1e_2=0$. Does there exists any projection $p$ with $$\overline{pe_1(H)}=\overline{pe_2(H)}=\overline{p(e_1+e_2)(H)}\neq0$$,Let us consider two projections $e_1$ and $e_2$ on the Hilbert space $H$ with $e_1e_2=0$. Does there exists any projection $p$ with $$\overline{pe_1(H)}=\overline{pe_2(H)}=\overline{p(e_1+e_2)(H)}\neq0$$,,"['functional-analysis', 'operator-theory', 'operator-algebras']"
50,"Linear function, that is not continuous?","Linear function, that is not continuous?",,"Let $V:=\{f:[0,1]\to\mathbb{R}|f\quad\text{continuous}\}$ provided with the norm $\|f\|_1:=\int_0^1 |f(x)|\, dx$ for $f\in V$ . Show that $\varphi : V\to\mathbb{R}$ , $\varphi(f):=f(0)$ is linear but not continuous. It is easy to show, that $\varphi$ is linear. Let $a\in\mathbb{R}$ and $f,g\in V$ . Then $\varphi(af+g)=(af+g)(0)=af(0)+g(0)=a\varphi(f)+\varphi(g)$ . Now I want to show, that it is not continuous. I know some equivalent statements for a linear function $f:V\to W$ between two normed spaces, which I tried to disprove. First, that there exists a $c>0$ such that $\|f(v)\|\leq c\cdot \|v\|$ for every $v\in V$ I tried several sequences of functions, but they did not work out. What I want to do is to give a sequence such that $f_n(0)=\text{constant}>0$ but $\|f_n\|_1\to 0$ or that $f_n(0)\to \infty$ while $\|f_n\|_1$ stays bounded. But nothing really worked. I tried a lot with $\cos(nx)$ but the absolute value makes it hard to inegrate. $\cos$ or $\sin$ should be involved I guess. Hints are appreciated. Thanks in advance.","Let provided with the norm for . Show that , is linear but not continuous. It is easy to show, that is linear. Let and . Then . Now I want to show, that it is not continuous. I know some equivalent statements for a linear function between two normed spaces, which I tried to disprove. First, that there exists a such that for every I tried several sequences of functions, but they did not work out. What I want to do is to give a sequence such that but or that while stays bounded. But nothing really worked. I tried a lot with but the absolute value makes it hard to inegrate. or should be involved I guess. Hints are appreciated. Thanks in advance.","V:=\{f:[0,1]\to\mathbb{R}|f\quad\text{continuous}\} \|f\|_1:=\int_0^1 |f(x)|\, dx f\in V \varphi : V\to\mathbb{R} \varphi(f):=f(0) \varphi a\in\mathbb{R} f,g\in V \varphi(af+g)=(af+g)(0)=af(0)+g(0)=a\varphi(f)+\varphi(g) f:V\to W c>0 \|f(v)\|\leq c\cdot \|v\| v\in V f_n(0)=\text{constant}>0 \|f_n\|_1\to 0 f_n(0)\to \infty \|f_n\|_1 \cos(nx) \cos \sin","['functional-analysis', 'continuity', 'normed-spaces']"
51,"Theorem 1.14 (b) Rudin functional analysis, few clarifications.","Theorem 1.14 (b) Rudin functional analysis, few clarifications.",,"Going through such theorem which states: In a topological vector space $X$ : (a) every neighborhood of $0$ contains a balanced neighborhood of $0$ and (b) every convex neighborhood of $0$ contains a balanced convex neighborhood of $0$ Proof of (a): We pick a neighborhood $U$ of $0$ because of continuity of multiplication there's a $\delta > 0$ and a neighborhood $V$ such that $\alpha V \subset U$ when $|\alpha| < \delta$. Now we pick $$ W = \bigcup_{|\alpha| <\delta} \alpha V $$ And $W$ is a neighborhood of $0$ (this is because is obtained as arbitrary union of open sets that are neighborhoods of $0$ right?). It is also balanced because if we pick $0 \leq |\beta| \leq 1$ we have $$ \beta W = \beta \bigcup _{|\alpha| < \delta} \alpha V = \bigcup _{|\alpha| < \delta} \beta \alpha V \subset W $$ is this right? Proof of (b): This is a bit confusing to me, all over the proof, I'll just try to highlight what I don't understand. Is $W$ chosen as balanced neighborhood subset of $U$ (convex neighborhood of $0$), is it because of (a)? Why $\alpha^{-1}W = W$? This should be consequence of the fact that $W$ is balanced, but I really don't see why. Why $A^o \subset U$? Given $A$ is convex so is $A^o$, why? I guess this because of theorem 1.13 (d), right? It might be me but I really get confused with the proof that $A$ is balanced, can you clarify? Proof of (b) : Suppose $U$ is a convex neighborhood of $0$ in $X$. Let $A = \bigcap \alpha U$ where $| \alpha | = 1$, choose $W$ as in part (a). Since $W$ is balanced, $\alpha^{-1} W = W$ when $|\alpha| = 1$; hence $W \subset \alpha U$. Thus $W \subset A$, which implies that the interior $A^o$ of $A$ is a neighborhood of $0$. Clearly $A^o \subset U$. Being an intersection of convex sets, $A$ is convex; hence so is $A^{o}$. To prove that $A^o$ is a neighborhood with the desired properties we have to show that $A^o$ is a neighborhood with the desired properties, we have to show that $A^o$ is balanced; for this it suffeces to prove that $A$ is balanced. Choose $r$ and $\beta$ so that $0 \leq r \leq 1, |\beta| = 1$. Then   $$ r\beta A = \bigcap_{|\alpha| = 1} r\beta \alpha U = \bigcap_{|\alpha| = 1} r\alpha U, $$   Since $\alpha U$ is a convex set that contains $0$, we have $r \alpha U \subset \alpha U$. Thus $r\beta A \subset A$ which completes the proof.","Going through such theorem which states: In a topological vector space $X$ : (a) every neighborhood of $0$ contains a balanced neighborhood of $0$ and (b) every convex neighborhood of $0$ contains a balanced convex neighborhood of $0$ Proof of (a): We pick a neighborhood $U$ of $0$ because of continuity of multiplication there's a $\delta > 0$ and a neighborhood $V$ such that $\alpha V \subset U$ when $|\alpha| < \delta$. Now we pick $$ W = \bigcup_{|\alpha| <\delta} \alpha V $$ And $W$ is a neighborhood of $0$ (this is because is obtained as arbitrary union of open sets that are neighborhoods of $0$ right?). It is also balanced because if we pick $0 \leq |\beta| \leq 1$ we have $$ \beta W = \beta \bigcup _{|\alpha| < \delta} \alpha V = \bigcup _{|\alpha| < \delta} \beta \alpha V \subset W $$ is this right? Proof of (b): This is a bit confusing to me, all over the proof, I'll just try to highlight what I don't understand. Is $W$ chosen as balanced neighborhood subset of $U$ (convex neighborhood of $0$), is it because of (a)? Why $\alpha^{-1}W = W$? This should be consequence of the fact that $W$ is balanced, but I really don't see why. Why $A^o \subset U$? Given $A$ is convex so is $A^o$, why? I guess this because of theorem 1.13 (d), right? It might be me but I really get confused with the proof that $A$ is balanced, can you clarify? Proof of (b) : Suppose $U$ is a convex neighborhood of $0$ in $X$. Let $A = \bigcap \alpha U$ where $| \alpha | = 1$, choose $W$ as in part (a). Since $W$ is balanced, $\alpha^{-1} W = W$ when $|\alpha| = 1$; hence $W \subset \alpha U$. Thus $W \subset A$, which implies that the interior $A^o$ of $A$ is a neighborhood of $0$. Clearly $A^o \subset U$. Being an intersection of convex sets, $A$ is convex; hence so is $A^{o}$. To prove that $A^o$ is a neighborhood with the desired properties we have to show that $A^o$ is a neighborhood with the desired properties, we have to show that $A^o$ is balanced; for this it suffeces to prove that $A$ is balanced. Choose $r$ and $\beta$ so that $0 \leq r \leq 1, |\beta| = 1$. Then   $$ r\beta A = \bigcap_{|\alpha| = 1} r\beta \alpha U = \bigcap_{|\alpha| = 1} r\alpha U, $$   Since $\alpha U$ is a convex set that contains $0$, we have $r \alpha U \subset \alpha U$. Thus $r\beta A \subset A$ which completes the proof.",,"['functional-analysis', 'proof-explanation', 'topological-vector-spaces']"
52,Arzela-Ascoli theorem exercise,Arzela-Ascoli theorem exercise,,"The question : Define a metric space  $C(K)=\left \{ f: K\rightarrow \mathbb{R}  > \right\} $ , where $f$ is continuous function on $K$. Let  $K\in  \mathbb{R}$ be compact and let $B\subset C(K)$ be compact. Prove that   $B$ is equicontinuousas follows: Prove that the map $F:C(K)\times K\rightarrow \mathbb{R}$ defined   by $F(f,x)=f(x)$ is continuous. Use uniform continuity of $F$ restricted to $B\times K$ to deduce   the result. My attempt : For $F(f,x)=f(x),F(g,x)=g(x)$, $\left | F(f,x)-F(g,x) \right |=\left | f(x)-g(x) \right |\leq \sup\left | f(x)-g(x) \right |=d(f,g)$ Hence, $F$ is continuous on $C(K)\times K $. If I show $B$ is closed and pointwise bounded ,then $B$ is equicontinuous by Arzela-Ascoli theorem. Since $B$ and $K$are compact, cartesian product $B\times K$ is also a compact set. So, $F$ is uniformly continuous on $B\times K$ . $B$ is compact. By Heine-Borel theorem, $B$ is closed and bounded. It suffices to show that $B$ is pointwise bounded. But, I don't know this part using the uniform continuity of F on $B\times K$.","The question : Define a metric space  $C(K)=\left \{ f: K\rightarrow \mathbb{R}  > \right\} $ , where $f$ is continuous function on $K$. Let  $K\in  \mathbb{R}$ be compact and let $B\subset C(K)$ be compact. Prove that   $B$ is equicontinuousas follows: Prove that the map $F:C(K)\times K\rightarrow \mathbb{R}$ defined   by $F(f,x)=f(x)$ is continuous. Use uniform continuity of $F$ restricted to $B\times K$ to deduce   the result. My attempt : For $F(f,x)=f(x),F(g,x)=g(x)$, $\left | F(f,x)-F(g,x) \right |=\left | f(x)-g(x) \right |\leq \sup\left | f(x)-g(x) \right |=d(f,g)$ Hence, $F$ is continuous on $C(K)\times K $. If I show $B$ is closed and pointwise bounded ,then $B$ is equicontinuous by Arzela-Ascoli theorem. Since $B$ and $K$are compact, cartesian product $B\times K$ is also a compact set. So, $F$ is uniformly continuous on $B\times K$ . $B$ is compact. By Heine-Borel theorem, $B$ is closed and bounded. It suffices to show that $B$ is pointwise bounded. But, I don't know this part using the uniform continuity of F on $B\times K$.",,"['real-analysis', 'functional-analysis', 'analysis', 'equicontinuity', 'arzela-ascoli']"
53,Abuse of notation with distributions,Abuse of notation with distributions,,"A distribution is an element of the continuous dual space of some function space. Let us take the Schwartz space $\mathcal{S} := \mathcal{S}(\mathbb{R}^n)$ just as an example. A distribution $\phi \in \mathcal{S}'$ is then a map $$ \phi: \mathcal{S} \rightarrow \mathbb{C}.$$ My question is this: how do I interpret $\phi(x)$? I see this written a lot, but I don't understand how to work with it. What does for example $\phi(x) = \phi(-x)$ mean? The only thing I can think of is that $\phi(f) = \phi(\hat{f})$, where $\hat{f}(x) = f(-x)$. And more specifically for the problem I'm working on: I have a distribution $$ \mathcal{W} : \mathcal{S}(\underbrace{\mathbb{R}^4 \times \dots \times \mathbb{R}^4}_{n \text{ times}}) \rightarrow \mathbb{C} $$ and then they say that $\mathcal{W}$ is translation invariant, i.e. $$\mathcal{W}(x_1 +a,\dots, x_n + a) = \mathcal{W}(x_1,\dots,x_n)$$ so it can be writthen as a distribution $\mathfrak{W}$ that only depends on the differences $x_1-x_2,\dots,x_{n-1} - x_n$: $$\mathcal{W}(x_1,\dots,x_n) = \mathfrak{W}(x_1-x_2,\dots,x_{n-1}-x_n).$$ How do I interpret this last line?","A distribution is an element of the continuous dual space of some function space. Let us take the Schwartz space $\mathcal{S} := \mathcal{S}(\mathbb{R}^n)$ just as an example. A distribution $\phi \in \mathcal{S}'$ is then a map $$ \phi: \mathcal{S} \rightarrow \mathbb{C}.$$ My question is this: how do I interpret $\phi(x)$? I see this written a lot, but I don't understand how to work with it. What does for example $\phi(x) = \phi(-x)$ mean? The only thing I can think of is that $\phi(f) = \phi(\hat{f})$, where $\hat{f}(x) = f(-x)$. And more specifically for the problem I'm working on: I have a distribution $$ \mathcal{W} : \mathcal{S}(\underbrace{\mathbb{R}^4 \times \dots \times \mathbb{R}^4}_{n \text{ times}}) \rightarrow \mathbb{C} $$ and then they say that $\mathcal{W}$ is translation invariant, i.e. $$\mathcal{W}(x_1 +a,\dots, x_n + a) = \mathcal{W}(x_1,\dots,x_n)$$ so it can be writthen as a distribution $\mathfrak{W}$ that only depends on the differences $x_1-x_2,\dots,x_{n-1} - x_n$: $$\mathcal{W}(x_1,\dots,x_n) = \mathfrak{W}(x_1-x_2,\dots,x_{n-1}-x_n).$$ How do I interpret this last line?",,"['functional-analysis', 'notation', 'distribution-theory', 'schwartz-space']"
54,Approximating $L^1$ function with convolutions,Approximating  function with convolutions,L^1,"Let $f\in L^1([0,1])$, and suppose that $\int_0^1f\varphi^{(n)} = 0$ for every $\varphi\in C_c^\infty(0,1)$, where $\varphi^{(n)}$ is the nth derivative.  Show that $f$ is a polynomial of degree at most $n-1$ (Given hint: Approximate $f$ by smooth functions using convolutions) Thoughts: Going off the hint, we choose $\psi\in C_c^\infty$ such that $\int_0^1\psi =1$ and define $\psi_\epsilon(x) = \frac{1}{\epsilon}\psi(x/\epsilon)$ so that $(f*\psi_\epsilon)\to f$ in $L^1$ After this, I'm a bit unsure. If we somehow show $\int_{[0,1]} (f*\psi_\epsilon)^{(n)}\varphi = 0$ (which I haven't), and letting $\epsilon \to 0$, how would that show $f$ is even differentiable, and a polynomial.","Let $f\in L^1([0,1])$, and suppose that $\int_0^1f\varphi^{(n)} = 0$ for every $\varphi\in C_c^\infty(0,1)$, where $\varphi^{(n)}$ is the nth derivative.  Show that $f$ is a polynomial of degree at most $n-1$ (Given hint: Approximate $f$ by smooth functions using convolutions) Thoughts: Going off the hint, we choose $\psi\in C_c^\infty$ such that $\int_0^1\psi =1$ and define $\psi_\epsilon(x) = \frac{1}{\epsilon}\psi(x/\epsilon)$ so that $(f*\psi_\epsilon)\to f$ in $L^1$ After this, I'm a bit unsure. If we somehow show $\int_{[0,1]} (f*\psi_\epsilon)^{(n)}\varphi = 0$ (which I haven't), and letting $\epsilon \to 0$, how would that show $f$ is even differentiable, and a polynomial.",,"['real-analysis', 'functional-analysis', 'fourier-analysis', 'convolution']"
55,Norms are equivalent if one hand side of inequality holds,Norms are equivalent if one hand side of inequality holds,,"Let $(X,\|.\|_1)$ and $(X,\|.\|_2)$ are Banach spaces and $\forall x \in X$. Show that if  $\|.\|_1 \le k \|.\|_2 $ for $\exists k \gt 0$ then $\|.\|_1$ and $ \|.\|_2 $ are equivalent. I could think only belows $(x_n)$ is an arbitrary Cauchy Sequence in $X$ and since it is Banach wrt $ \|.\|_2 $ thus $\exists x \in X$ such that $\|x_n-x\|_2 \lt \varepsilon /k$ From inequeality we have $\|x_n-x\|_1 \le k\|x_n-x\|_2 \lt \varepsilon$  hence $x_n \to x$ wrt $\|.\|_1$ norm. I cannot continue and I think there are some mistakes and deficiencies my writtens above. I will be apreciated for any help I have used a version of Banach Isomorphism Theorem thanks to your comments : Let $I:(X,\|.\|_2) \to (X,\|.\|_1)$ identity map. It is (1-1) and onto. $\|I(x)\|_1 = \|x\|_1 \le k\|x\|_1 $ thus $I$ is bounded and equivalently continuous. By Banach Isomorphism Theorem it is a homeomorphism hence $I^{-1}$ is continuous and equivalently bounded. We can find $\exists c \gt 0$ such that $\|I^{-1}(x)\|_2 = \|x\|_2 \le c\|x\|_1 $ Finally we get the norms are equivalent Could you please check it?","Let $(X,\|.\|_1)$ and $(X,\|.\|_2)$ are Banach spaces and $\forall x \in X$. Show that if  $\|.\|_1 \le k \|.\|_2 $ for $\exists k \gt 0$ then $\|.\|_1$ and $ \|.\|_2 $ are equivalent. I could think only belows $(x_n)$ is an arbitrary Cauchy Sequence in $X$ and since it is Banach wrt $ \|.\|_2 $ thus $\exists x \in X$ such that $\|x_n-x\|_2 \lt \varepsilon /k$ From inequeality we have $\|x_n-x\|_1 \le k\|x_n-x\|_2 \lt \varepsilon$  hence $x_n \to x$ wrt $\|.\|_1$ norm. I cannot continue and I think there are some mistakes and deficiencies my writtens above. I will be apreciated for any help I have used a version of Banach Isomorphism Theorem thanks to your comments : Let $I:(X,\|.\|_2) \to (X,\|.\|_1)$ identity map. It is (1-1) and onto. $\|I(x)\|_1 = \|x\|_1 \le k\|x\|_1 $ thus $I$ is bounded and equivalently continuous. By Banach Isomorphism Theorem it is a homeomorphism hence $I^{-1}$ is continuous and equivalently bounded. We can find $\exists c \gt 0$ such that $\|I^{-1}(x)\|_2 = \|x\|_2 \le c\|x\|_1 $ Finally we get the norms are equivalent Could you please check it?",,"['functional-analysis', 'banach-spaces', 'normed-spaces']"
56,$E$ a Banach Space and $(\varphi_n)\subset E^{\ast}$ such that $x_n\to0\Rightarrow \sum \varphi_n(x_n)<\infty$,a Banach Space and  such that,E (\varphi_n)\subset E^{\ast} x_n\to0\Rightarrow \sum \varphi_n(x_n)<\infty,"Let $E$ be a Banach vector space and for every $n\in \mathbb{N}$ let $\varphi_n$ be an element of $E^{\ast}$ . Prove that the following assertions are equivalent: a) For every sequence $(x_n)$ which tends to $0$ , $\sum_{n=1}^{\infty}\varphi_n(x_n)$ is convergent. b) $\sum_{n=1}^{\infty}\varphi_n$ is absolutely convergent. For $(b)\Rightarrow (a)$ I think the following argument is valid: If $x_n\to 0$ , in particular $(x_n)$ is a bounded sequence. Let $M$ be an uper bound. Therefore $\sum_{n}\left |\varphi_n(x_n)\right |\leq \sum_{n}\left \|\varphi_n\right \|\left \|x_n\right \|\leq M\sum_{n}\left \|\varphi_n\right \|$ , which is finite by the assumption. Now, what about $(a)\Rightarrow (b)$ ? One thing we can say is that the convergence of $\sum_{n=1}^{\infty}\varphi_n(x_n)$ is an absolute convergence: if $x_n\to 0$ then taking $y_n=\text{sgn}(\varphi(x_n))x_n$ (where $\text{sgn}(x)=1$ if $x\geq 0$ and $\text{sgn}(x)=-1$ if $x<0$ ), we have that $y_n\to 0$ and $\sum_{n=1}^{\infty}\varphi_n(y_n)=\sum_{n=1}^{\infty}|\varphi_n(x_n)|$ . (If the field is $\mathbb{C}$ we can take a rotation). I do not know how to continue. How would you proceed?","Let be a Banach vector space and for every let be an element of . Prove that the following assertions are equivalent: a) For every sequence which tends to , is convergent. b) is absolutely convergent. For I think the following argument is valid: If , in particular is a bounded sequence. Let be an uper bound. Therefore , which is finite by the assumption. Now, what about ? One thing we can say is that the convergence of is an absolute convergence: if then taking (where if and if ), we have that and . (If the field is we can take a rotation). I do not know how to continue. How would you proceed?",E n\in \mathbb{N} \varphi_n E^{\ast} (x_n) 0 \sum_{n=1}^{\infty}\varphi_n(x_n) \sum_{n=1}^{\infty}\varphi_n (b)\Rightarrow (a) x_n\to 0 (x_n) M \sum_{n}\left |\varphi_n(x_n)\right |\leq \sum_{n}\left \|\varphi_n\right \|\left \|x_n\right \|\leq M\sum_{n}\left \|\varphi_n\right \| (a)\Rightarrow (b) \sum_{n=1}^{\infty}\varphi_n(x_n) x_n\to 0 y_n=\text{sgn}(\varphi(x_n))x_n \text{sgn}(x)=1 x\geq 0 \text{sgn}(x)=-1 x<0 y_n\to 0 \sum_{n=1}^{\infty}\varphi_n(y_n)=\sum_{n=1}^{\infty}|\varphi_n(x_n)| \mathbb{C},"['functional-analysis', 'continuity', 'banach-spaces', 'normed-spaces']"
57,"Showing a function is in Holder space for some $a \in (0,1] $",Showing a function is in Holder space for some,"a \in (0,1] ","Hi Im stuck on this exercise :  for which $a \in (0,1]$ is $f(x)=x^{2}\sin(\frac{1}{x^{3}})$  in $C^{a}((0,1])$ This is my attempt so far : $|f(x)| \leq x^{2} $ $|f'(x)| = 2x\sin(1/x^{3})-\frac{3}{x^{2}}\cos(1/x^{3}) \leq 2x+ \frac{3}{x^{2}} $ Then for $ 0<y<x \leq 1$ we have $|f(x)-f(y)| \leq x^{2} +y^{2} \leq 2x^{2}$ I would like now to get this in terms of $|x-y|$ and then use the mean value theorem to find $a$ for which $f(x)$ is in $C^{a}((0,1])$.","Hi Im stuck on this exercise :  for which $a \in (0,1]$ is $f(x)=x^{2}\sin(\frac{1}{x^{3}})$  in $C^{a}((0,1])$ This is my attempt so far : $|f(x)| \leq x^{2} $ $|f'(x)| = 2x\sin(1/x^{3})-\frac{3}{x^{2}}\cos(1/x^{3}) \leq 2x+ \frac{3}{x^{2}} $ Then for $ 0<y<x \leq 1$ we have $|f(x)-f(y)| \leq x^{2} +y^{2} \leq 2x^{2}$ I would like now to get this in terms of $|x-y|$ and then use the mean value theorem to find $a$ for which $f(x)$ is in $C^{a}((0,1])$.",,"['functional-analysis', 'functional-inequalities', 'holder-spaces']"
58,How is extension by analytic continuation done?,How is extension by analytic continuation done?,,"I understand  that by defining an expression for a function to be analytic, we can extend the range of the expression beyond it's usual range. One case is that a series which is asymptotic to a function defined abstractly within certain domain could in fact diverage outside of that domain while the function is still defined. Can analytic continuation enable us to extend the range of an expression from just a little block of known domain and range to infinite range and domain? If so, how is it done? Could it be only done by computer simulation? Or algebraic manipulation is enough? What are the ways to do this? Could it be done by hand in special case? In general, how is it done?","I understand  that by defining an expression for a function to be analytic, we can extend the range of the expression beyond it's usual range. One case is that a series which is asymptotic to a function defined abstractly within certain domain could in fact diverage outside of that domain while the function is still defined. Can analytic continuation enable us to extend the range of an expression from just a little block of known domain and range to infinite range and domain? If so, how is it done? Could it be only done by computer simulation? Or algebraic manipulation is enough? What are the ways to do this? Could it be done by hand in special case? In general, how is it done?",,"['complex-analysis', 'functional-analysis', 'analytic-continuation']"
59,Question on Proof in Evans' PDEs: Sobolev Inequality,Question on Proof in Evans' PDEs: Sobolev Inequality,,"I've currently started to study about Sobolev inequalities and here is Gagliardo-Nirenberg-Sobolev inequality from Evans' book: I understand the whole proof except for some details I'm about to ask: In $\;(12)\;$ the last inequality results from general Holder inequality for $\;(\int_{\mathbb R} \vert Du(x) \vert\;dy_i)^{\frac{1}{n-1}}\;$. However this inequality states that in generall $\;\int_{\mathbb R} \vert u_{x_1}\dots u_{x_n}\vert \;dx \le (\int_{\mathbb R} {\vert u_{x_1} \vert}^{p_1}\;dx)^{1/{p_1}} \dots (\int_{\mathbb R} {\vert u_{x_n} \vert}^{p_n}\;dx)^{1/{p_n}}\;$. Which are the appropriate $\;p_i\;$ here and why I can't see them? In the first two lines, I don't fully understand why $\;\vert u(x) \vert \le\int_{\mathbb R} \vert Du(x_1,\dots,y_1,\dots,x_n)\;dy_i\;$ Any help would be valuable. Thanks in advance!","I've currently started to study about Sobolev inequalities and here is Gagliardo-Nirenberg-Sobolev inequality from Evans' book: I understand the whole proof except for some details I'm about to ask: In $\;(12)\;$ the last inequality results from general Holder inequality for $\;(\int_{\mathbb R} \vert Du(x) \vert\;dy_i)^{\frac{1}{n-1}}\;$. However this inequality states that in generall $\;\int_{\mathbb R} \vert u_{x_1}\dots u_{x_n}\vert \;dx \le (\int_{\mathbb R} {\vert u_{x_1} \vert}^{p_1}\;dx)^{1/{p_1}} \dots (\int_{\mathbb R} {\vert u_{x_n} \vert}^{p_n}\;dx)^{1/{p_n}}\;$. Which are the appropriate $\;p_i\;$ here and why I can't see them? In the first two lines, I don't fully understand why $\;\vert u(x) \vert \le\int_{\mathbb R} \vert Du(x_1,\dots,y_1,\dots,x_n)\;dy_i\;$ Any help would be valuable. Thanks in advance!",,"['functional-analysis', 'inequality', 'partial-differential-equations', 'proof-explanation', 'sobolev-spaces']"
60,Image of a closed set is closed under bounded linear transformation between Banach spaces?,Image of a closed set is closed under bounded linear transformation between Banach spaces?,,"Let $A,B$ be Banach spaces and let $T:A \to B$ be a surjective, bounded, linear operator. Let $A_1$ be a non-empty subset of $A$, then: $T(A_1)$ is closed if and only if $A_1+ \textrm{ker}(T)$ is closed. I have shown that if $A_1+ \textrm{ker} (T)$ is closed, then $T(A_1)$ is closed, but am unsure of how to proceed in the other direction. Note that we know that $T(A \setminus [A_1 + \textrm{ker}(T)]) = B \setminus T(A_1)$. Figured the reverse direction could be proved using some modification of the Open Mapping Theorem or Closed Graph theorem, but not sure how to tackle this. Thanks.","Let $A,B$ be Banach spaces and let $T:A \to B$ be a surjective, bounded, linear operator. Let $A_1$ be a non-empty subset of $A$, then: $T(A_1)$ is closed if and only if $A_1+ \textrm{ker}(T)$ is closed. I have shown that if $A_1+ \textrm{ker} (T)$ is closed, then $T(A_1)$ is closed, but am unsure of how to proceed in the other direction. Note that we know that $T(A \setminus [A_1 + \textrm{ker}(T)]) = B \setminus T(A_1)$. Figured the reverse direction could be proved using some modification of the Open Mapping Theorem or Closed Graph theorem, but not sure how to tackle this. Thanks.",,"['functional-analysis', 'banach-spaces', 'open-map']"
61,Understanding weak convergence of probability measures,Understanding weak convergence of probability measures,,"Weak convergence of a sequence probability measures, denoted $P_n \Rightarrow P$, is typically defined as, $$ \int\limits_{\mathbb{R}}fdP_n \xrightarrow{n\rightarrow\infty} \int\limits_{\mathbb{R}}fdP \;\;\;\; (*)$$ for all $f \in C_b(\mathbb{R})$, the space of continuous and bounded functions. However I read here that if $(*)$ holds for all $f \in C_c^\infty(\mathbb{R})$, the space of smooth compactly supported functions, then weak convergence also holds. The proof in the link is as follows: Fix $\epsilon > 0$ and choose a smooth compactly supported function $g$ with $0 \leq g \leq 1$ and $\int gdP \geq 1−ϵ$. Let $K$ be the support of $g$. Then by assumption, $$\int gdP_n\rightarrow\int gdP \geq 1 - \epsilon$$    so we see that, $P_n(K)\geq 1−\epsilon$. In other words, $\{P_n\}$ is tight. Hence after passing to a subsequence, $\{P_{n_k}\}$ converges weakly to some measure $P^*$. Now we notice that $\int fdP =\int fdP^*$ for all $f\in C_c^\infty(\mathbb{R})$, and it follows from a monotone class type argument that $P = P^*$. Finally use the ""double subsequence"" trick to conclude that the original sequence $\{P_n\}$ also converges weakly to $P$. I think I am able to follow the above until the last sentence. How we can conclude weak convergence of the original sequence? All we've shown is that for all $f \in C_b(\mathbb{R})$, $$ \int f dP_{n_k} \xrightarrow{k\rightarrow\infty} \int f dP $$ for this particular subsequence $\{P_{n_k}\}$. I'm not sure what the ""double subsequence"" trick is. Is it some sort of diagonal argument? How can we use it to conclude that, $$  \int\limits_{\mathbb{R}}fdP_n \xrightarrow{n\rightarrow\infty} \int\limits_{\mathbb{R}}fdP $$ for all $f \in C_b(\mathbb{R})$?","Weak convergence of a sequence probability measures, denoted $P_n \Rightarrow P$, is typically defined as, $$ \int\limits_{\mathbb{R}}fdP_n \xrightarrow{n\rightarrow\infty} \int\limits_{\mathbb{R}}fdP \;\;\;\; (*)$$ for all $f \in C_b(\mathbb{R})$, the space of continuous and bounded functions. However I read here that if $(*)$ holds for all $f \in C_c^\infty(\mathbb{R})$, the space of smooth compactly supported functions, then weak convergence also holds. The proof in the link is as follows: Fix $\epsilon > 0$ and choose a smooth compactly supported function $g$ with $0 \leq g \leq 1$ and $\int gdP \geq 1−ϵ$. Let $K$ be the support of $g$. Then by assumption, $$\int gdP_n\rightarrow\int gdP \geq 1 - \epsilon$$    so we see that, $P_n(K)\geq 1−\epsilon$. In other words, $\{P_n\}$ is tight. Hence after passing to a subsequence, $\{P_{n_k}\}$ converges weakly to some measure $P^*$. Now we notice that $\int fdP =\int fdP^*$ for all $f\in C_c^\infty(\mathbb{R})$, and it follows from a monotone class type argument that $P = P^*$. Finally use the ""double subsequence"" trick to conclude that the original sequence $\{P_n\}$ also converges weakly to $P$. I think I am able to follow the above until the last sentence. How we can conclude weak convergence of the original sequence? All we've shown is that for all $f \in C_b(\mathbb{R})$, $$ \int f dP_{n_k} \xrightarrow{k\rightarrow\infty} \int f dP $$ for this particular subsequence $\{P_{n_k}\}$. I'm not sure what the ""double subsequence"" trick is. Is it some sort of diagonal argument? How can we use it to conclude that, $$  \int\limits_{\mathbb{R}}fdP_n \xrightarrow{n\rightarrow\infty} \int\limits_{\mathbb{R}}fdP $$ for all $f \in C_b(\mathbb{R})$?",,['functional-analysis']
62,Examples of unbounded approximate units in $C^*$-algebras,Examples of unbounded approximate units in -algebras,C^*,"Sometimes I see the requirement that approximate units $(u_\lambda)_\lambda$ in $C^*$-algebras must be eventually bounded, that means for some $C>0$ there is a $\lambda_0$ such that $\|u_\lambda\|<C$ for all $\lambda\ge\lambda_0$. Yet I can not come up with an example of an approximate unit that is not eventually bounded, so I'm asking for an example here. Bonus points when the approximate unit is a sequence and not just a net. For this question I define an approximate unit of a $C^*$-algebra $A$ as a net $(u_\lambda)_\lambda$ in $A$ such that $u_\lambda a\to a$ and $a u_\lambda\to a$ for all $a\in A$. I have looked at a few $C^*$-algebras I know: $C_0(\mathbb R)$: I'm quite positive that all approximate units are bounded here, but I have not worked out the details yet. $C_0(X)$ (and therefore all commutative $C^*$-algebras): I guess only something considerably larger than $X=\mathbb R$ can work, maybe $X=\ell^2(\mathbb N)$. group algebras $C^*(G)$: With the multiplication being a form of convolution my guess is that all approximate units converge to some ""dirac"" function with the weight being centered at the unit element of $G$. My gut feeling tells me again that these converge to $1$ in norm, but I could very well be wrong. compact operators on a Hilbert space $\mathcal K(H)$. The projections on finite subspaces are an approximate unit (bounded by 1). General case: Some $C^*$-subalgebra $A\subset\mathcal B(H)$.","Sometimes I see the requirement that approximate units $(u_\lambda)_\lambda$ in $C^*$-algebras must be eventually bounded, that means for some $C>0$ there is a $\lambda_0$ such that $\|u_\lambda\|<C$ for all $\lambda\ge\lambda_0$. Yet I can not come up with an example of an approximate unit that is not eventually bounded, so I'm asking for an example here. Bonus points when the approximate unit is a sequence and not just a net. For this question I define an approximate unit of a $C^*$-algebra $A$ as a net $(u_\lambda)_\lambda$ in $A$ such that $u_\lambda a\to a$ and $a u_\lambda\to a$ for all $a\in A$. I have looked at a few $C^*$-algebras I know: $C_0(\mathbb R)$: I'm quite positive that all approximate units are bounded here, but I have not worked out the details yet. $C_0(X)$ (and therefore all commutative $C^*$-algebras): I guess only something considerably larger than $X=\mathbb R$ can work, maybe $X=\ell^2(\mathbb N)$. group algebras $C^*(G)$: With the multiplication being a form of convolution my guess is that all approximate units converge to some ""dirac"" function with the weight being centered at the unit element of $G$. My gut feeling tells me again that these converge to $1$ in norm, but I could very well be wrong. compact operators on a Hilbert space $\mathcal K(H)$. The projections on finite subspaces are an approximate unit (bounded by 1). General case: Some $C^*$-subalgebra $A\subset\mathcal B(H)$.",,"['functional-analysis', 'operator-algebras', 'c-star-algebras']"
63,Explain a proof about the norm of derivation of operators,Explain a proof about the norm of derivation of operators,,"Let $E$ be a complex Hilbert space. Let $A\in \mathcal{L}(E)$. Consider \begin{eqnarray*} W_{0}(A) &=&\{\alpha\in \mathbb{C}:\;\exists\,(z_n)\subset E\;\;\hbox{such that}\;\|z_n\|=1,\displaystyle\lim_{n\rightarrow+\infty}\langle A z_n,z_n\rangle=\alpha,\\ &&\phantom{++++++++++}\;\hbox{and}\;\displaystyle\lim_{n\rightarrow+\infty}\|Az_n\|= \|A\| \}. \end{eqnarray*} If $0\notin W_{0}(A)$,  by the transformation $Ae^{i\alpha}$ of $A$, why we can suppose that $\Re e( W_{0}(A))\geq\tau>0$. Notice that this result figures in the proof of THEOREM $2$. ( 1 )","Let $E$ be a complex Hilbert space. Let $A\in \mathcal{L}(E)$. Consider \begin{eqnarray*} W_{0}(A) &=&\{\alpha\in \mathbb{C}:\;\exists\,(z_n)\subset E\;\;\hbox{such that}\;\|z_n\|=1,\displaystyle\lim_{n\rightarrow+\infty}\langle A z_n,z_n\rangle=\alpha,\\ &&\phantom{++++++++++}\;\hbox{and}\;\displaystyle\lim_{n\rightarrow+\infty}\|Az_n\|= \|A\| \}. \end{eqnarray*} If $0\notin W_{0}(A)$,  by the transformation $Ae^{i\alpha}$ of $A$, why we can suppose that $\Re e( W_{0}(A))\geq\tau>0$. Notice that this result figures in the proof of THEOREM $2$. ( 1 )",,"['functional-analysis', 'operator-theory', 'hilbert-spaces']"
64,Locally convex subspace,Locally convex subspace,,"While studying functional analysis the following question came up. Let $(E, P)$ be a locally convex space where $P$ is a family of seminorms. Also, let $F \subseteq E$ be a linear subspace endowed with the family of seminorms $P_F := \{p_{\vert F} \; : \; p \in P\}$. Show that a subset $V \subseteq F$ is open with regard to $P_F$ if and only if $V = U \cap F$ where $U \subseteq E$ and $U$ is open with regard to $P$. What I tried so far is the following. First, since $F$ is a linear subspace we know that $0 \in F$ and $E$ is locally convex, the intersection of convex sets is convex which makes $(F, P_F)$ a locally convex space as well, i.e. there is a balanced absorbing convex local base at zero. Also, $V$ being open with regard to the family of seminorms in the space $(F,P_F)$ is to say that $$\forall x \in V, \ \exists \ \epsilon_x > 0, p_1^x, \ldots, p_{n_x}^x \in P_F \ : V(x, p_1^x, \ldots, p_{n_x}^x, \epsilon_x) = \bigcap_{i=1}^{n_x} \{y \in F : p_i^x(x - y) < \epsilon\} \subseteq V$$ For the ""$\Rightarrow$"" direction I now have to somehow show that, based on this notion of $V$ being open, we can infer that there has to be another set $U \subseteq E$ which is open to the entire seminorm family and for which it holds that $V = U \cap F$. But why is this the case? The other ""$\Leftarrow$"" direction seems to be clearer. Since $U$ is open to the entire family of seminorms, we can restrict it to those sets being open to $P_F$ by intersecting it with $F$. But this seems heuristic at best, how would one use the notions of local convexity and openess regarding a seminorm system correctly to write it down?","While studying functional analysis the following question came up. Let $(E, P)$ be a locally convex space where $P$ is a family of seminorms. Also, let $F \subseteq E$ be a linear subspace endowed with the family of seminorms $P_F := \{p_{\vert F} \; : \; p \in P\}$. Show that a subset $V \subseteq F$ is open with regard to $P_F$ if and only if $V = U \cap F$ where $U \subseteq E$ and $U$ is open with regard to $P$. What I tried so far is the following. First, since $F$ is a linear subspace we know that $0 \in F$ and $E$ is locally convex, the intersection of convex sets is convex which makes $(F, P_F)$ a locally convex space as well, i.e. there is a balanced absorbing convex local base at zero. Also, $V$ being open with regard to the family of seminorms in the space $(F,P_F)$ is to say that $$\forall x \in V, \ \exists \ \epsilon_x > 0, p_1^x, \ldots, p_{n_x}^x \in P_F \ : V(x, p_1^x, \ldots, p_{n_x}^x, \epsilon_x) = \bigcap_{i=1}^{n_x} \{y \in F : p_i^x(x - y) < \epsilon\} \subseteq V$$ For the ""$\Rightarrow$"" direction I now have to somehow show that, based on this notion of $V$ being open, we can infer that there has to be another set $U \subseteq E$ which is open to the entire seminorm family and for which it holds that $V = U \cap F$. But why is this the case? The other ""$\Leftarrow$"" direction seems to be clearer. Since $U$ is open to the entire family of seminorms, we can restrict it to those sets being open to $P_F$ by intersecting it with $F$. But this seems heuristic at best, how would one use the notions of local convexity and openess regarding a seminorm system correctly to write it down?",,"['functional-analysis', 'locally-convex-spaces']"
65,When the linear operator attains its maximum on a convex set,When the linear operator attains its maximum on a convex set,,Let $f$ be a non zero continuous linear functional on a Banach space $X$. i.e. $$f:X\rightarrow\mathbb{R}$$ is linear and bounded. Let $E$ be any non empty closed convex set of $X$ such that $$sup_{x\in E} |f(x)|$$ is attained. Then show that the supremum is attained at some extreme point of E. Now if the assumption includes that the set $E$ is also compact then the supremum should be attaind (by the extreme value theorem) further by Krein- Milman theorem we can guarantee that it's attained at some extreme point. But what about my question ?  Is there any resource or book to find the proof ?,Let $f$ be a non zero continuous linear functional on a Banach space $X$. i.e. $$f:X\rightarrow\mathbb{R}$$ is linear and bounded. Let $E$ be any non empty closed convex set of $X$ such that $$sup_{x\in E} |f(x)|$$ is attained. Then show that the supremum is attained at some extreme point of E. Now if the assumption includes that the set $E$ is also compact then the supremum should be attaind (by the extreme value theorem) further by Krein- Milman theorem we can guarantee that it's attained at some extreme point. But what about my question ?  Is there any resource or book to find the proof ?,,"['functional-analysis', 'convex-analysis', 'convex-optimization']"
66,Poincare-type inequality on ball.,Poincare-type inequality on ball.,,"Let $B_r$ be an open ball in $\Bbb R^d$ with radius $r$. Let $u\in W^{1,p}(B_r)$ and define $ \bar u=\frac 1{|B_r|}\int u\ dx, $ so that by Poincare inequality we have  $$ ||u-\bar u||_{L^p(B_r)} \le C||Du ||_{L^p(B_r)}\quad;\quad C=C(r,p). $$ Actually, $C(r,p)=C(p)\cdot r$ by dilating $B_r$ to $B_1$ and change of variables. Now, by Sobolev embedding we have $W^{1,p}(\Omega) \hookrightarrow L^q(\Omega)$ for $q\in [1,\frac{dp}{d-p}]$, i.e. $$ ||u||_{L^q} \le C'||u||_{W^{1,p}}\quad;\quad C'=C'(\Omega,p,q,d). $$ Combining these we can easily get $$ ||u-\bar u||_{L^q(B_r)} \le C'' ||Du||_{L^p(B_r)} $$ for some constant $C''$ depending on $r,p,q,d$. However, I've seen a version that says   $$ \left(\frac 1{|B_r|}\int |u-\bar u|^q \right)^{1/q} \le Cr \left(\frac 1{|B_r|}\int |Du|^p \right)^{1/p} $$   where $C=C(d,p,q)$ does not depend on $r$. Is that an easy consequence of the above or does it require another approach entirely? I tried scaling argument but the term $||u||_p$ and $||Du||_p$ in $||u||_{W^{1,p}}$ scale differently. I seem to somewhat stuck, could anyone please help?","Let $B_r$ be an open ball in $\Bbb R^d$ with radius $r$. Let $u\in W^{1,p}(B_r)$ and define $ \bar u=\frac 1{|B_r|}\int u\ dx, $ so that by Poincare inequality we have  $$ ||u-\bar u||_{L^p(B_r)} \le C||Du ||_{L^p(B_r)}\quad;\quad C=C(r,p). $$ Actually, $C(r,p)=C(p)\cdot r$ by dilating $B_r$ to $B_1$ and change of variables. Now, by Sobolev embedding we have $W^{1,p}(\Omega) \hookrightarrow L^q(\Omega)$ for $q\in [1,\frac{dp}{d-p}]$, i.e. $$ ||u||_{L^q} \le C'||u||_{W^{1,p}}\quad;\quad C'=C'(\Omega,p,q,d). $$ Combining these we can easily get $$ ||u-\bar u||_{L^q(B_r)} \le C'' ||Du||_{L^p(B_r)} $$ for some constant $C''$ depending on $r,p,q,d$. However, I've seen a version that says   $$ \left(\frac 1{|B_r|}\int |u-\bar u|^q \right)^{1/q} \le Cr \left(\frac 1{|B_r|}\int |Du|^p \right)^{1/p} $$   where $C=C(d,p,q)$ does not depend on $r$. Is that an easy consequence of the above or does it require another approach entirely? I tried scaling argument but the term $||u||_p$ and $||Du||_p$ in $||u||_{W^{1,p}}$ scale differently. I seem to somewhat stuck, could anyone please help?",,"['functional-analysis', 'ordinary-differential-equations', 'measure-theory', 'partial-differential-equations', 'sobolev-spaces']"
67,Finding a $\text C^\infty$ function which is zero just on a given closed set [duplicate],Finding a  function which is zero just on a given closed set [duplicate],\text C^\infty,"This question already has an answer here : Every closed subset $E\subseteq \mathbb{R}^n$ is the zero point set of a smooth function (1 answer) Closed 6 years ago . If $\text E$ is an arbitrary closed set in $\mathbf R^n$, show that there is an $\text f\in \text C^\infty(\mathbf R^n)$ such that $\text f(x)=0~$for every $x \in \text E$ and $\text f(x)>0$ for every other $x\in \mathbf R^n$. My approach is as follows : Since $\text E$ is given to be closed hence $\text E^c=\text V$ will be an open set in $\mathbf R^n$ and hence $\text V$ can be written as countable union of open sets $\{\text V_i\}_{i=1}^{\infty}$ and hence one can have a partition of unity $\{\psi_i\}_{i=1}^{\infty}\subset \text C^\infty (\text V)$ such that $\text {supp}(\psi_i)\subset \text V_i$. Then if i take $f=\Sigma_{i=1}^{\infty} \psi_i$ then my this $\text f$ is in $\text C^\infty (\text V)$ and $f(x)>0$ in $\text E^c$ and $f(x)=0$ in $\text E$. Now i am just left with to show that $f\in \text C^\infty (\mathbf R^n)$ or how can i generalize my $f$ to get the desired result. Any type of help will be appreciated. Thanks in advance.","This question already has an answer here : Every closed subset $E\subseteq \mathbb{R}^n$ is the zero point set of a smooth function (1 answer) Closed 6 years ago . If $\text E$ is an arbitrary closed set in $\mathbf R^n$, show that there is an $\text f\in \text C^\infty(\mathbf R^n)$ such that $\text f(x)=0~$for every $x \in \text E$ and $\text f(x)>0$ for every other $x\in \mathbf R^n$. My approach is as follows : Since $\text E$ is given to be closed hence $\text E^c=\text V$ will be an open set in $\mathbf R^n$ and hence $\text V$ can be written as countable union of open sets $\{\text V_i\}_{i=1}^{\infty}$ and hence one can have a partition of unity $\{\psi_i\}_{i=1}^{\infty}\subset \text C^\infty (\text V)$ such that $\text {supp}(\psi_i)\subset \text V_i$. Then if i take $f=\Sigma_{i=1}^{\infty} \psi_i$ then my this $\text f$ is in $\text C^\infty (\text V)$ and $f(x)>0$ in $\text E^c$ and $f(x)=0$ in $\text E$. Now i am just left with to show that $f\in \text C^\infty (\mathbf R^n)$ or how can i generalize my $f$ to get the desired result. Any type of help will be appreciated. Thanks in advance.",,"['functional-analysis', 'partial-differential-equations', 'distribution-theory']"
68,Show that the canonical inclusion $ L^\infty → L^1 $ is continuous but not compact. (a problem in real analysis),Show that the canonical inclusion  is continuous but not compact. (a problem in real analysis), L^\infty → L^1 ,"Would you mind giving some hints (not the whole solution) to me? Problem : Consider the space $ L^p = L^p(0, 1),$ with $ p ∈ [1, ∞].$ Show that the canonical inclusion   $ L^\infty → L^1 $    is continuous but not compact. Is it possible to find $ p, q ∈ [1, ∞] $ with $p < q$ such that the inclusion $L^q \to L^p$ is compact?","Would you mind giving some hints (not the whole solution) to me? Problem : Consider the space $ L^p = L^p(0, 1),$ with $ p ∈ [1, ∞].$ Show that the canonical inclusion   $ L^\infty → L^1 $    is continuous but not compact. Is it possible to find $ p, q ∈ [1, ∞] $ with $p < q$ such that the inclusion $L^q \to L^p$ is compact?",,"['real-analysis', 'functional-analysis', 'analysis', 'lp-spaces']"
69,"Inverse spectral theorem, clarification from a video.","Inverse spectral theorem, clarification from a video.",,"I'm following some of the lectures of professor Schuller. I'm watching this video specifically At about minute 23 he defines the PVM (Projected Valued Measure) as a map $$ P_A : \sigma(\Theta_\mathbb{R}) \rightarrow \mathcal{L}(\mathcal{H}) $$ with the specific expression $$ \langle\psi,P_A(\Omega)\phi\rangle=\int \chi_\Omega d\mu_{\psi,\phi}^A \;\;\forall\psi, \phi \in \mathcal{H} $$ Here $\mathcal{H}$ is an hilbert space, $A$ is a self-adjoint operator in $\mathcal{H}$, while $\mu_{\psi,\phi}^A$ is a complex valued measure constructed by polarization of a real valued measure $\mu_{\phi}^A$ (these concepts are expressed a bit earlier in the same lecture, but all the constructions are given later). My question is why the PVM defined above does represent an operator when evaluated in a measurable set? suppose indeed a self-adjoint operator $A$ is given, and we also fix a measurable set $\Omega$, the PVM seems to me to take two arguments and not just one. Schuller also says that the map is somehow given by that expression, so there's probably something I'm missing. Can you help me in interpreting that expression? Update: I've watched the whole video, and I guess my question may be explained as follows. Suppose we are in a vector space of dimension $n$ embedded with a dot product. The dot product is characterized by a matrix $A \in \mathbb{R}^{n\times n}$. We can recover such matrix by knowing how the dot product acts on a basis. By taking the canonical base $V = \left\{e_1,\ldots,e_n \right\}$ (columns vectors), we can recover the element $a_{ij}$ as $$ \langle e_i,e_j \rangle = e_i^T  A  e_j = a_{ij} $$ If the base is not the canonical one we can perform a transformation and still retrieving the matrix (which again is actually an operator). Hence for finite dimension the dot product is fully characterized once we now how it acts on a basis. (I haven't been very precise, but you've got my point I hope). The expression $$ \langle\psi,P_A(\Omega)\phi\rangle=\int \chi_\Omega d\mu_{\psi,\phi}^A \;\;\forall\psi, \phi \in \mathcal{H} $$ it seems to me be somehow related, therefore I was wondering if there's any theorem that characterize self-adjoints operator in a similar fashion as matrices and dot products in finite dimensions. Name of a theorem, any reference, or if you could prove it for me would be really useful.","I'm following some of the lectures of professor Schuller. I'm watching this video specifically At about minute 23 he defines the PVM (Projected Valued Measure) as a map $$ P_A : \sigma(\Theta_\mathbb{R}) \rightarrow \mathcal{L}(\mathcal{H}) $$ with the specific expression $$ \langle\psi,P_A(\Omega)\phi\rangle=\int \chi_\Omega d\mu_{\psi,\phi}^A \;\;\forall\psi, \phi \in \mathcal{H} $$ Here $\mathcal{H}$ is an hilbert space, $A$ is a self-adjoint operator in $\mathcal{H}$, while $\mu_{\psi,\phi}^A$ is a complex valued measure constructed by polarization of a real valued measure $\mu_{\phi}^A$ (these concepts are expressed a bit earlier in the same lecture, but all the constructions are given later). My question is why the PVM defined above does represent an operator when evaluated in a measurable set? suppose indeed a self-adjoint operator $A$ is given, and we also fix a measurable set $\Omega$, the PVM seems to me to take two arguments and not just one. Schuller also says that the map is somehow given by that expression, so there's probably something I'm missing. Can you help me in interpreting that expression? Update: I've watched the whole video, and I guess my question may be explained as follows. Suppose we are in a vector space of dimension $n$ embedded with a dot product. The dot product is characterized by a matrix $A \in \mathbb{R}^{n\times n}$. We can recover such matrix by knowing how the dot product acts on a basis. By taking the canonical base $V = \left\{e_1,\ldots,e_n \right\}$ (columns vectors), we can recover the element $a_{ij}$ as $$ \langle e_i,e_j \rangle = e_i^T  A  e_j = a_{ij} $$ If the base is not the canonical one we can perform a transformation and still retrieving the matrix (which again is actually an operator). Hence for finite dimension the dot product is fully characterized once we now how it acts on a basis. (I haven't been very precise, but you've got my point I hope). The expression $$ \langle\psi,P_A(\Omega)\phi\rangle=\int \chi_\Omega d\mu_{\psi,\phi}^A \;\;\forall\psi, \phi \in \mathcal{H} $$ it seems to me be somehow related, therefore I was wondering if there's any theorem that characterize self-adjoints operator in a similar fashion as matrices and dot products in finite dimensions. Name of a theorem, any reference, or if you could prove it for me would be really useful.",,"['functional-analysis', 'measure-theory', 'operator-theory', 'spectral-theory']"
70,On Kadec norms definition.,On Kadec norms definition.,,"So I have this definition of Kadec norm $\textbf{Definition:}$ Let $(X,\|\|)$ be a Banach space. The norm $\|\|$ is said to be a Kadec norm if $x_n \xrightarrow{w} \bar{x}$ and $\|x_n\|\to \|\bar{x}\|$ implies $x_n\to \bar{x}.$ I was wondering why is it necessary in this definition that $X$ to be a Banach space. I though that it is not at all, and that then the condition would imply that $X$ is Banach. But I was not able to prove that statement. Any thoughts?","So I have this definition of Kadec norm $\textbf{Definition:}$ Let $(X,\|\|)$ be a Banach space. The norm $\|\|$ is said to be a Kadec norm if $x_n \xrightarrow{w} \bar{x}$ and $\|x_n\|\to \|\bar{x}\|$ implies $x_n\to \bar{x}.$ I was wondering why is it necessary in this definition that $X$ to be a Banach space. I though that it is not at all, and that then the condition would imply that $X$ is Banach. But I was not able to prove that statement. Any thoughts?",,"['functional-analysis', 'normed-spaces', 'weak-convergence']"
71,$H^1_0$ on half space,on half space,H^1_0,Is it true that for any $u \in H^1_0(\mathbb R^n_+)$ there holds $u = 0$ on $\partial \mathbb R^n_+$? How can I prove it?,Is it true that for any $u \in H^1_0(\mathbb R^n_+)$ there holds $u = 0$ on $\partial \mathbb R^n_+$? How can I prove it?,,"['functional-analysis', 'sobolev-spaces']"
72,"On the definition of spectral integrals in Conways ""A course in functional analysis""","On the definition of spectral integrals in Conways ""A course in functional analysis""",,"I am trying to make sense of the spectral integral defined in Conways ""A course in functional analysis"" but I cant really settle on how to think about it. He does the following, He just proved that $<E(\Delta)g,h>=E(\Delta)_{g,h}$ is a measure of bounded variation on the spectra or any $X$ with a sigma algebra if one prefers. Now, the left hand side of the inequlities at the end seems to contain some kind of Riemann sum(which I cant define a integral for w.r.t a arbitrary measure) rather then simple functions as one would have in the case of a Lebgue integral. So what integral concept is the sum< $\sum \phi(x_{k})E(\Delta_{k})g,h>$ associated to? Is he inventing his own concept here? Edit! It might be possible to consider $\phi(x_{k})$ a step function and do Lebague all the way, I am not sure tho.","I am trying to make sense of the spectral integral defined in Conways ""A course in functional analysis"" but I cant really settle on how to think about it. He does the following, He just proved that $<E(\Delta)g,h>=E(\Delta)_{g,h}$ is a measure of bounded variation on the spectra or any $X$ with a sigma algebra if one prefers. Now, the left hand side of the inequlities at the end seems to contain some kind of Riemann sum(which I cant define a integral for w.r.t a arbitrary measure) rather then simple functions as one would have in the case of a Lebgue integral. So what integral concept is the sum< $\sum \phi(x_{k})E(\Delta_{k})g,h>$ associated to? Is he inventing his own concept here? Edit! It might be possible to consider $\phi(x_{k})$ a step function and do Lebague all the way, I am not sure tho.",,"['functional-analysis', 'lebesgue-integral', 'riemann-integration']"
73,Neumann problem has unique twice continuously differentiable solution,Neumann problem has unique twice continuously differentiable solution,,"I want to show that for each $f\in\mathcal{C}([0,\pi])$, there exists a unique twice continuously differentiable solution to the Neumann problem on an interval $$ -\frac{d^2u}{dx^2}+u=f$$ on $(0,\pi)$ with boundary conditions $\frac{du}{dx}(0)=\frac{du}{dx}(\pi)=0$. I also want to show that the map that sends $f\mapsto u$ extends to a compact self-adjoint operator on $L^2(0,\pi)$. It was suggested that I use the fact that $c_k\cos(kx)$ is an orthonormal basis of $L^2(0,\pi)$ for normalizing coefficients $c_k$ but I'm unsure how to proceed. Ideally, I would use Hilbert space theory to solve this. Thanks!","I want to show that for each $f\in\mathcal{C}([0,\pi])$, there exists a unique twice continuously differentiable solution to the Neumann problem on an interval $$ -\frac{d^2u}{dx^2}+u=f$$ on $(0,\pi)$ with boundary conditions $\frac{du}{dx}(0)=\frac{du}{dx}(\pi)=0$. I also want to show that the map that sends $f\mapsto u$ extends to a compact self-adjoint operator on $L^2(0,\pi)$. It was suggested that I use the fact that $c_k\cos(kx)$ is an orthonormal basis of $L^2(0,\pi)$ for normalizing coefficients $c_k$ but I'm unsure how to proceed. Ideally, I would use Hilbert space theory to solve this. Thanks!",,"['functional-analysis', 'hilbert-spaces', 'lp-spaces']"
74,When are integral operators trace class?,When are integral operators trace class?,,"Define an integral operator $T$ on $L^2([0,1])$ by $$Tf(x) = \int_0^1 K(x, y) f(y) \, dy. $$ Such an operator is Hilbert-Schmidt when $K$ is in $L^2([0,1]\times [0,1])$. I heard that if $K$ is smooth, then $T$ is in fact trace-class. Why is this? When is such an operator trace-class?","Define an integral operator $T$ on $L^2([0,1])$ by $$Tf(x) = \int_0^1 K(x, y) f(y) \, dy. $$ Such an operator is Hilbert-Schmidt when $K$ is in $L^2([0,1]\times [0,1])$. I heard that if $K$ is smooth, then $T$ is in fact trace-class. Why is this? When is such an operator trace-class?",,"['real-analysis', 'functional-analysis', 'operator-theory', 'integral-transforms']"
75,How to use open mapping theorem in this case?,How to use open mapping theorem in this case?,,"If I have $X,Y$ Banach spaces and $T : X \to Y$ a linear and surjective map then $T$ is a open map. I want to prove that if $(y_n)$ is a bounded sequence on $Y$ then there exists a bounded sequence $(x_n)$ on $X$ such that $T(x_n) = y_n$. I want this because I am trying to prove that if $y_n \to 0$ then there exists a sequence $(x_n)$ in $X$ that converges to $0$ and $T(x_n) = y_n.$ Since $T$ is surjective, clearly there exists $(x_n) \in X$ such that $T(x_n) = y_n$. This is easy. Once $(y_n)$ is bounded it lies on a ball for a radius big enough. So, how can I find a ball in $X$ such that $(x_n)$ lies in a ball? The radius of such ball must be related with the radius of the ball containing $(y_n)$ somehow. How can I do this? It is obviously open map theorem, but how?","If I have $X,Y$ Banach spaces and $T : X \to Y$ a linear and surjective map then $T$ is a open map. I want to prove that if $(y_n)$ is a bounded sequence on $Y$ then there exists a bounded sequence $(x_n)$ on $X$ such that $T(x_n) = y_n$. I want this because I am trying to prove that if $y_n \to 0$ then there exists a sequence $(x_n)$ in $X$ that converges to $0$ and $T(x_n) = y_n.$ Since $T$ is surjective, clearly there exists $(x_n) \in X$ such that $T(x_n) = y_n$. This is easy. Once $(y_n)$ is bounded it lies on a ball for a radius big enough. So, how can I find a ball in $X$ such that $(x_n)$ lies in a ball? The radius of such ball must be related with the radius of the ball containing $(y_n)$ somehow. How can I do this? It is obviously open map theorem, but how?",,['functional-analysis']
76,"If $\int f \bar g \, d \mu \leq \|g\|$ for all square integrable $g$, does it follow that $f$ is square integrable?","If  for all square integrable , does it follow that  is square integrable?","\int f \bar g \, d \mu \leq \|g\| g f","Let $(X, \mu)$ be a measurable space. The Cauchy-Schwarz inequality tells us that if $f \in L^2(X)$, then $|\int_X f \bar g\, d\mu|$ is bounded as $g$ ranges over the elements of $L^2(X)$ with norm $1$. I was wondering if the following partial converse holds Let $f$ be a complex-valued measurable function on $X$. Suppose that for all $g \in L^2(X)$ with norm $1$, the inequality $|\int_X f \bar g \, d\mu| \leq 1$ holds. Then $f$ is square integrable. (i.e. $f \in L^2(X)$)","Let $(X, \mu)$ be a measurable space. The Cauchy-Schwarz inequality tells us that if $f \in L^2(X)$, then $|\int_X f \bar g\, d\mu|$ is bounded as $g$ ranges over the elements of $L^2(X)$ with norm $1$. I was wondering if the following partial converse holds Let $f$ be a complex-valued measurable function on $X$. Suppose that for all $g \in L^2(X)$ with norm $1$, the inequality $|\int_X f \bar g \, d\mu| \leq 1$ holds. Then $f$ is square integrable. (i.e. $f \in L^2(X)$)",,"['real-analysis', 'functional-analysis', 'lebesgue-integral', 'lp-spaces']"
77,"Is this Set of Bessel Functions a Basis for All $C^{1}[0,a]$ Functions?",Is this Set of Bessel Functions a Basis for All  Functions?,"C^{1}[0,a]","Consider the following set of Bessel functions $$\{J_1(\alpha_ir)\}, \qquad J_0(\alpha_ia)=0 \tag{1}$$ I want to show that this set of functions form a basis for the space of $C^{1}[0,a]$ functions. So I should prove that They are linearly independent and that they span the space so they form a basis for that space. My Work My first thought was to find the corresponding Sturm-Liouville problem for this set of functions. However, I failed to find proper boundary conditions. For example, the following Sturm-Liouville system \begin{align} \,&\frac{d}{dr}\left[r\frac{dR}{dr}\right]+\left[\lambda r + \frac{1}{r}\right]R=0\\ &R(0)<\infty\\ &R(a)=0 \tag{2} \end{align} leads to the following basis $$\{J_1(\alpha_ir)\}, \qquad J_1(\alpha_ia)=0 \tag{3}$$ for the aforementioned space (note the order of the Bessel functions). Anyway, I could show that the set of functions mentioned in Eq.$(1)$ are orthogonal and consequently linearly independent by just computing the following integral $$\int_{0}^{a} r J_1(\alpha r) J_1(\beta r)dr = \frac{a}{\alpha^2-\beta^2}\Big(\beta J_0(\beta a)J_1(\alpha a)- \alpha J_0(\alpha a)J_1(\beta a)\Big)$$","Consider the following set of Bessel functions $$\{J_1(\alpha_ir)\}, \qquad J_0(\alpha_ia)=0 \tag{1}$$ I want to show that this set of functions form a basis for the space of $C^{1}[0,a]$ functions. So I should prove that They are linearly independent and that they span the space so they form a basis for that space. My Work My first thought was to find the corresponding Sturm-Liouville problem for this set of functions. However, I failed to find proper boundary conditions. For example, the following Sturm-Liouville system \begin{align} \,&\frac{d}{dr}\left[r\frac{dR}{dr}\right]+\left[\lambda r + \frac{1}{r}\right]R=0\\ &R(0)<\infty\\ &R(a)=0 \tag{2} \end{align} leads to the following basis $$\{J_1(\alpha_ir)\}, \qquad J_1(\alpha_ia)=0 \tag{3}$$ for the aforementioned space (note the order of the Bessel functions). Anyway, I could show that the set of functions mentioned in Eq.$(1)$ are orthogonal and consequently linearly independent by just computing the following integral $$\int_{0}^{a} r J_1(\alpha r) J_1(\beta r)dr = \frac{a}{\alpha^2-\beta^2}\Big(\beta J_0(\beta a)J_1(\alpha a)- \alpha J_0(\alpha a)J_1(\beta a)\Big)$$",,"['calculus', 'functional-analysis', 'ordinary-differential-equations', 'bessel-functions', 'sturm-liouville']"
78,Division of Distributions by Polynomials,Division of Distributions by Polynomials,,"Let $P(z)$ be a non-null complex polynomial in $n$ variables $z=(z_1,\dots,z_n)$: \begin{equation} P(z)=\sum_{|\alpha| \leq N} c_{\alpha} z^{\alpha}, \end{equation} where as usual for every $\alpha=(\alpha_1,\dots,\alpha_n) \in \mathbb{N}^{n}$ we set $|\alpha|=\alpha_1+\dots+\alpha_n$, and $z^{\alpha}=z_1^{\alpha_1}\dots z_n^{\alpha_n}$. Consider $P$ as a polynomial function from $\mathbb{R}^n$ into $\mathbb{C}$: \begin{equation} P(x)=\sum_{|\alpha| \leq N} c_{\alpha} x^{\alpha} \quad (x \in \mathbb{R}^n). \end{equation} Define the linear subspace $\mathcal{M}_{\mathcal{S}}$ of the Schwartz space $\mathcal{S}=\mathcal{S}(\mathbb{R}^n)$: \begin{equation} \mathcal{M}_{\mathcal{S}}= \{ \psi \in \mathcal{S}: \psi=P\phi, \phi \in \mathcal{S} \}, \end{equation} and the linear continuous multiplication map $M_{P}:\mathcal{S} \rightarrow \mathcal{M}_{\mathcal{S}}$ \begin{equation} M_{P}(\phi)=P\phi \quad (\phi \in \mathcal{S}). \end{equation} In his work On the Division of Distributions by Polynomials , Hörmander proved the following remarkable result (whose proof is unexpectedly very complicated). Theorem (1). The map $M_P$ has a linear continuous inverse $M_{P}^{-1}:\mathcal{M}_{\mathcal{S}} \rightarrow \mathcal{S}$. From this result we can easily deduce the following Theorem (2). Let $T \in \mathcal{S}'$. Then there exists $S \in \mathcal{S}'$ such that $P \cdot S=T$. Proof. The map $T \circ M_{P}^{-1}: \mathcal{M}_{\mathcal{S}} \rightarrow \mathbb{C}$ is a linear continuous functional, so by the Hahn-Banach Theorem it can be extened to a continuous linear functional $S$ on $\mathcal{S}$. $S$ satifies $S(P\phi)=T(\phi)$ for each $\phi \in \mathcal{S}$, so $P \cdot S = T$. QED Hörmander says that an exactly analogous argument proves the following result. Theorem (3). Let $\Omega$ be an open set of $\mathbb{R}^n$, and $T \in \mathcal{D'}(\Omega)$. Then there exists $S \in \mathcal{D'}(\Omega)$ such that $P \cdot S=T$. Could you see some way of proving this theorem by using Theorem (1)? The fact is that if we define the subpspace of $\mathcal{D}(\Omega)$ \begin{equation} \mathcal{M}_{\mathcal{D}}= \{ \psi \in \mathcal{D}(\Omega): \psi=P\phi, \phi \in \mathcal{D}(\Omega) \}, \end{equation} and the linear continuous multiplication map $N_{P}:\mathcal{D}(\Omega) \rightarrow \mathcal{M}_{\mathcal{D}}$ \begin{equation} N_{P}(\phi)=P\phi \quad (\phi \in \mathcal{D}(\Omega)), \end{equation} I see no way of deducing from Theorem (1) that $N_P$ has a linear continuous inverse. If we could do this, then of course we could prove Theorem (3) by using the same argument we used to prove Theorem (2). Any help is welcome. Thank you very much in advance for your attention. NOTE. Let me notice that there is instead a way of proving Theorem (3) by using Theorem (2) (but of course this was not what Hörmander had in mind). Let $\Gamma$ be the collection of all open rectangles $\omega$, such that the closure of $\omega$ is a compact set contained in $\Omega$. Clearly $\Gamma$ is an open covering of $\Omega$. Let $\omega \in \Gamma$ and choose $\xi \in \mathcal{D}(\Omega)$ such that $\xi=1$ on $\omega$. Since $\xi \cdot T$ is a distribution with compact support, it defines a tempered distribution, so that by Theorem (2) there exists $V \in \mathcal{S}'$ such that  \begin{equation} V(P \phi)= T(\xi \phi) \quad (\phi \in \mathcal{S}). \end{equation} In particular, we have \begin{equation} V(P\phi)=T(\xi \phi)=T(\phi) \quad (\phi \in \mathcal{D}(\omega)). \end{equation} Let us denote with $S_{\omega}$ the restriction of $V$ to $\mathcal{D}(\omega)$. We have $S_{\omega} \in \mathcal{D}(\omega)$. Moreover, if  $T_{\omega}$ is the restriction of $T$ to $\mathcal{D}(\omega)$, then we have $ H \cdot S_{\omega} = T_{\omega}$. In other terms, the equation $P \cdot S = T$ has a solution on $\omega$. Now, we know that there exists  a locally finite partition of unity $(\psi_j)_{j=1}^{\infty}$ in $\Omega$ subordinate to the open cover $\Gamma$ (see Rudin, Functional Analysis, Second Edition, Theorem (6.20)). This means that $(\psi_j)_{j=1}^{\infty}$ is a sequence in $\mathcal{D}(\Omega)$, with $\psi_j \geq 0$, such that: (i) each $\psi_j$ has its support in some member of $\Gamma$, (ii) $\sum_{j=1}^{\infty} \psi_j(x)=1$ for every $x \in \Omega$, (iii) to every compact $K \subset \Omega$ correspond an integer $m$ and an open set $W \supset K$ such that \begin{equation} \psi_1(x)+\dots+\psi_m(x)=1, \end{equation} for all $x \in W$. Let $\omega_j$ be the element of $\Gamma$ which contains the support of $\psi_j$ according to (i). Then define \begin{equation} S(\phi)= \sum_{j=1}^{\infty} S_{\omega_j}(\psi_j \phi) \quad (\phi \in \mathcal{D}(\Omega)). \end{equation} Since for each $\phi \in \mathcal{D}(\Omega)$ only finitely many of the functions $\psi_j \phi$ are different from zero, it is easy to see that $S$ is well defined, that $S \in \mathcal{D'}(\Omega)$ and that $P \cdot S = T$. QED","Let $P(z)$ be a non-null complex polynomial in $n$ variables $z=(z_1,\dots,z_n)$: \begin{equation} P(z)=\sum_{|\alpha| \leq N} c_{\alpha} z^{\alpha}, \end{equation} where as usual for every $\alpha=(\alpha_1,\dots,\alpha_n) \in \mathbb{N}^{n}$ we set $|\alpha|=\alpha_1+\dots+\alpha_n$, and $z^{\alpha}=z_1^{\alpha_1}\dots z_n^{\alpha_n}$. Consider $P$ as a polynomial function from $\mathbb{R}^n$ into $\mathbb{C}$: \begin{equation} P(x)=\sum_{|\alpha| \leq N} c_{\alpha} x^{\alpha} \quad (x \in \mathbb{R}^n). \end{equation} Define the linear subspace $\mathcal{M}_{\mathcal{S}}$ of the Schwartz space $\mathcal{S}=\mathcal{S}(\mathbb{R}^n)$: \begin{equation} \mathcal{M}_{\mathcal{S}}= \{ \psi \in \mathcal{S}: \psi=P\phi, \phi \in \mathcal{S} \}, \end{equation} and the linear continuous multiplication map $M_{P}:\mathcal{S} \rightarrow \mathcal{M}_{\mathcal{S}}$ \begin{equation} M_{P}(\phi)=P\phi \quad (\phi \in \mathcal{S}). \end{equation} In his work On the Division of Distributions by Polynomials , Hörmander proved the following remarkable result (whose proof is unexpectedly very complicated). Theorem (1). The map $M_P$ has a linear continuous inverse $M_{P}^{-1}:\mathcal{M}_{\mathcal{S}} \rightarrow \mathcal{S}$. From this result we can easily deduce the following Theorem (2). Let $T \in \mathcal{S}'$. Then there exists $S \in \mathcal{S}'$ such that $P \cdot S=T$. Proof. The map $T \circ M_{P}^{-1}: \mathcal{M}_{\mathcal{S}} \rightarrow \mathbb{C}$ is a linear continuous functional, so by the Hahn-Banach Theorem it can be extened to a continuous linear functional $S$ on $\mathcal{S}$. $S$ satifies $S(P\phi)=T(\phi)$ for each $\phi \in \mathcal{S}$, so $P \cdot S = T$. QED Hörmander says that an exactly analogous argument proves the following result. Theorem (3). Let $\Omega$ be an open set of $\mathbb{R}^n$, and $T \in \mathcal{D'}(\Omega)$. Then there exists $S \in \mathcal{D'}(\Omega)$ such that $P \cdot S=T$. Could you see some way of proving this theorem by using Theorem (1)? The fact is that if we define the subpspace of $\mathcal{D}(\Omega)$ \begin{equation} \mathcal{M}_{\mathcal{D}}= \{ \psi \in \mathcal{D}(\Omega): \psi=P\phi, \phi \in \mathcal{D}(\Omega) \}, \end{equation} and the linear continuous multiplication map $N_{P}:\mathcal{D}(\Omega) \rightarrow \mathcal{M}_{\mathcal{D}}$ \begin{equation} N_{P}(\phi)=P\phi \quad (\phi \in \mathcal{D}(\Omega)), \end{equation} I see no way of deducing from Theorem (1) that $N_P$ has a linear continuous inverse. If we could do this, then of course we could prove Theorem (3) by using the same argument we used to prove Theorem (2). Any help is welcome. Thank you very much in advance for your attention. NOTE. Let me notice that there is instead a way of proving Theorem (3) by using Theorem (2) (but of course this was not what Hörmander had in mind). Let $\Gamma$ be the collection of all open rectangles $\omega$, such that the closure of $\omega$ is a compact set contained in $\Omega$. Clearly $\Gamma$ is an open covering of $\Omega$. Let $\omega \in \Gamma$ and choose $\xi \in \mathcal{D}(\Omega)$ such that $\xi=1$ on $\omega$. Since $\xi \cdot T$ is a distribution with compact support, it defines a tempered distribution, so that by Theorem (2) there exists $V \in \mathcal{S}'$ such that  \begin{equation} V(P \phi)= T(\xi \phi) \quad (\phi \in \mathcal{S}). \end{equation} In particular, we have \begin{equation} V(P\phi)=T(\xi \phi)=T(\phi) \quad (\phi \in \mathcal{D}(\omega)). \end{equation} Let us denote with $S_{\omega}$ the restriction of $V$ to $\mathcal{D}(\omega)$. We have $S_{\omega} \in \mathcal{D}(\omega)$. Moreover, if  $T_{\omega}$ is the restriction of $T$ to $\mathcal{D}(\omega)$, then we have $ H \cdot S_{\omega} = T_{\omega}$. In other terms, the equation $P \cdot S = T$ has a solution on $\omega$. Now, we know that there exists  a locally finite partition of unity $(\psi_j)_{j=1}^{\infty}$ in $\Omega$ subordinate to the open cover $\Gamma$ (see Rudin, Functional Analysis, Second Edition, Theorem (6.20)). This means that $(\psi_j)_{j=1}^{\infty}$ is a sequence in $\mathcal{D}(\Omega)$, with $\psi_j \geq 0$, such that: (i) each $\psi_j$ has its support in some member of $\Gamma$, (ii) $\sum_{j=1}^{\infty} \psi_j(x)=1$ for every $x \in \Omega$, (iii) to every compact $K \subset \Omega$ correspond an integer $m$ and an open set $W \supset K$ such that \begin{equation} \psi_1(x)+\dots+\psi_m(x)=1, \end{equation} for all $x \in W$. Let $\omega_j$ be the element of $\Gamma$ which contains the support of $\psi_j$ according to (i). Then define \begin{equation} S(\phi)= \sum_{j=1}^{\infty} S_{\omega_j}(\psi_j \phi) \quad (\phi \in \mathcal{D}(\Omega)). \end{equation} Since for each $\phi \in \mathcal{D}(\Omega)$ only finitely many of the functions $\psi_j \phi$ are different from zero, it is easy to see that $S$ is well defined, that $S \in \mathcal{D'}(\Omega)$ and that $P \cdot S = T$. QED",,"['functional-analysis', 'distribution-theory']"
79,Linear map dense trajectory,Linear map dense trajectory,,"Is there a finite-dimensional $\mathbb{R}$-normed space $V$ with $x_0 \in V$ and linear map $T \colon V \to V$ such that $\{T^nx_0\}_{n=1}^{\infty}$ is dense in $V$? I got this question of Bollobas' Introductory Linear Analysis. My intuition leans towards no, but I can't come up with a convincing argument without assuming things. There is a hint given and it is to first prove the result over a $\mathbb{C}$-normed space.","Is there a finite-dimensional $\mathbb{R}$-normed space $V$ with $x_0 \in V$ and linear map $T \colon V \to V$ such that $\{T^nx_0\}_{n=1}^{\infty}$ is dense in $V$? I got this question of Bollobas' Introductory Linear Analysis. My intuition leans towards no, but I can't come up with a convincing argument without assuming things. There is a hint given and it is to first prove the result over a $\mathbb{C}$-normed space.",,"['functional-analysis', 'dynamical-systems', 'linear-transformations']"
80,"Is the set {$x^n$} a closed and bounded subset of C $[0,1]$ that is NOT compact?",Is the set {} a closed and bounded subset of C  that is NOT compact?,"x^n [0,1]","Consider the space of continuous, real-valued functions on $[0,1]$ under the sup-norm metric. I believe that the set of functions {$x^n$} for all natural n is a closed and bounded subset of our metric space that is not compact. It is bounded as the difference between any two functions in the set is at most $1$ and I believe it is not compact as it does not contain a convergent subsequence (no subsequence converges uniformly to a continuous function, let alone a continuous function in the set). I don't have a rigorous proof of closure but do believe it is closed. Hence, I believe that it works as an example of a subset of a metric space that is closed and bounded but not compact. Is my reasoning valid?","Consider the space of continuous, real-valued functions on $[0,1]$ under the sup-norm metric. I believe that the set of functions {$x^n$} for all natural n is a closed and bounded subset of our metric space that is not compact. It is bounded as the difference between any two functions in the set is at most $1$ and I believe it is not compact as it does not contain a convergent subsequence (no subsequence converges uniformly to a continuous function, let alone a continuous function in the set). I don't have a rigorous proof of closure but do believe it is closed. Hence, I believe that it works as an example of a subset of a metric space that is closed and bounded but not compact. Is my reasoning valid?",,"['real-analysis', 'functional-analysis']"
81,Inner product in dual Hilbert space,Inner product in dual Hilbert space,,"Let $H^*$ be a dual space of a Hilbert space $H$. Then inner product is defined as $$(f,g)=(J^{-1}f,J^{-1}g),$$ where $f,g\in H^*$ and $J\colon H\to H^*$ is the canonical isomorphism. I want to prove that  $$\|f\|=\|J^{-1}f\|=\sqrt{(J^{-1}f,J^{-1}f)}=\sqrt{(f,f)}.$$ Any ideas on how to approach this proof?","Let $H^*$ be a dual space of a Hilbert space $H$. Then inner product is defined as $$(f,g)=(J^{-1}f,J^{-1}g),$$ where $f,g\in H^*$ and $J\colon H\to H^*$ is the canonical isomorphism. I want to prove that  $$\|f\|=\|J^{-1}f\|=\sqrt{(J^{-1}f,J^{-1}f)}=\sqrt{(f,f)}.$$ Any ideas on how to approach this proof?",,"['functional-analysis', 'hilbert-spaces']"
82,Exponential of Operators,Exponential of Operators,,Let $H$ be an Hilbert Space $\exp(T)$ the exponential for an operator $T \in L(H)$. I know that $\exp(A)^{*} \exp(A)=\exp(A) \exp(A)^{*}=id$. Can I conclude that $A^{*}A=AA^{*}$? Cannot find an counter example neither succeeded my attempts at rearranging. Any hint is appreciated. EDIT: For my task i just need that i can conclude $$ \exp(A)\exp(A)^{*} = \exp(A)^{*}\exp(A) = \exp(A+A^*) $$,Let $H$ be an Hilbert Space $\exp(T)$ the exponential for an operator $T \in L(H)$. I know that $\exp(A)^{*} \exp(A)=\exp(A) \exp(A)^{*}=id$. Can I conclude that $A^{*}A=AA^{*}$? Cannot find an counter example neither succeeded my attempts at rearranging. Any hint is appreciated. EDIT: For my task i just need that i can conclude $$ \exp(A)\exp(A)^{*} = \exp(A)^{*}\exp(A) = \exp(A+A^*) $$,,['functional-analysis']
83,Non-self Adjoint Operator Algebra References,Non-self Adjoint Operator Algebra References,,"The problem I am working on has led me to define a norm closed sub-algebra $\mathscr{A}$ of $\mathscr{B}(\mathscr{H})$.  The algebra is generated by some mild relations, and in general, will not be closed under taking adjoints. Every operator algebra I have played with up until this point has been a C*-algebra, and so I am currently a bit uncomfortable with the object I am working with. In seeking out references for some basic theory of non-self adjoint operator algebras, I have come across papers over particular results but nothing I would label as a collection of essentials.  Thus my question: Is there a book or paper that goes over the basics of non-self adjoint operator algebras? If not, what are the fundamental results that I should seek out? Thanks in advance.","The problem I am working on has led me to define a norm closed sub-algebra $\mathscr{A}$ of $\mathscr{B}(\mathscr{H})$.  The algebra is generated by some mild relations, and in general, will not be closed under taking adjoints. Every operator algebra I have played with up until this point has been a C*-algebra, and so I am currently a bit uncomfortable with the object I am working with. In seeking out references for some basic theory of non-self adjoint operator algebras, I have come across papers over particular results but nothing I would label as a collection of essentials.  Thus my question: Is there a book or paper that goes over the basics of non-self adjoint operator algebras? If not, what are the fundamental results that I should seek out? Thanks in advance.",,"['functional-analysis', 'operator-theory', 'operator-algebras']"
84,Convex sets in infinite dimensional Banach spaces,Convex sets in infinite dimensional Banach spaces,,"I am reviewing  functional analysis and getting stuck in this question. Let $X$ be an infinite dimensional Banach space. Show that there exist convex sets $K_1, K_2$ such that $K_1\cap K_2=\emptyset, K_1\cup K_2=X, cl(K_1)=cl(K_2)=X$ where $cl(E)$ is the closure of $E$. I have no idea about where to start. Could anyone give me some hints ? Thank you very much.","I am reviewing  functional analysis and getting stuck in this question. Let $X$ be an infinite dimensional Banach space. Show that there exist convex sets $K_1, K_2$ such that $K_1\cap K_2=\emptyset, K_1\cup K_2=X, cl(K_1)=cl(K_2)=X$ where $cl(E)$ is the closure of $E$. I have no idea about where to start. Could anyone give me some hints ? Thank you very much.",,"['real-analysis', 'functional-analysis', 'convex-analysis']"
85,square root commutes with multiplication for positive elements in a $C^*$ algebra?,square root commutes with multiplication for positive elements in a  algebra?,C^*,"Let $A$ be a unital $C^*$ algebra.  If $z\in A$ is invertible, then so is $z^*$ and $z^*z$ and, furthermore, $z^*z$ is positive, so we can define using the functional calculus $|z|=\sqrt{z^*z}$.  My book then claims that $|z|$ is invertible with inverse $\sqrt{(z^*z)^{-1}}$.  Why is $|z|*|z|^{-1}=1$?  To me this looks like trying to say that $\sqrt{z^*z}*\sqrt{(z^*z)^{-1}}=\sqrt{(z^*z)(z^*z)^{-1}}=\sqrt1=1$, but I don't see why you can pull the product inside the square root like you can for reals (I don't know if this is actually how to prove the claim). I don't see how some of the basic properties about continuous functions pass through the functional calculus and still hold inside of $A$.","Let $A$ be a unital $C^*$ algebra.  If $z\in A$ is invertible, then so is $z^*$ and $z^*z$ and, furthermore, $z^*z$ is positive, so we can define using the functional calculus $|z|=\sqrt{z^*z}$.  My book then claims that $|z|$ is invertible with inverse $\sqrt{(z^*z)^{-1}}$.  Why is $|z|*|z|^{-1}=1$?  To me this looks like trying to say that $\sqrt{z^*z}*\sqrt{(z^*z)^{-1}}=\sqrt{(z^*z)(z^*z)^{-1}}=\sqrt1=1$, but I don't see why you can pull the product inside the square root like you can for reals (I don't know if this is actually how to prove the claim). I don't see how some of the basic properties about continuous functions pass through the functional calculus and still hold inside of $A$.",,['functional-analysis']
86,How do linear operators acting on paths of Gaussian processes influence the covariance function?,How do linear operators acting on paths of Gaussian processes influence the covariance function?,,"It is well-known that applying a linear transformation $A$ on an $n$-dimensional centered Gaussian distribution with covariance matrix $\Sigma$ results in another centered Gaussian distribution with covariance matrix $A\Sigma A^T$. I am interested in generalisations of this property for Gaussian processes. So, let's say we have a centered Gaussian process $X$ over an intervall $[0,t]$ with paths a.s. in a certain function space $V$ and a linear operator $T:V\to W$, where $W$ is another function space (if necessary, over the same intervall).  Intuitively, I would guess that $T(X)$ is again a centered Gaussian process, which could be proved by looking at the finite-dimensional distributions of the process, if I'm not mistaken. The question is now, what can be said about the covariance function of $T(X)$ given the covariance function of $X$, e.g. something similar to the ""$A\Sigma A^T$""-expression above? Is there any general statement (even with some additional assumptions like continuity or compactness of $T$) and if so, where can I find it?","It is well-known that applying a linear transformation $A$ on an $n$-dimensional centered Gaussian distribution with covariance matrix $\Sigma$ results in another centered Gaussian distribution with covariance matrix $A\Sigma A^T$. I am interested in generalisations of this property for Gaussian processes. So, let's say we have a centered Gaussian process $X$ over an intervall $[0,t]$ with paths a.s. in a certain function space $V$ and a linear operator $T:V\to W$, where $W$ is another function space (if necessary, over the same intervall).  Intuitively, I would guess that $T(X)$ is again a centered Gaussian process, which could be proved by looking at the finite-dimensional distributions of the process, if I'm not mistaken. The question is now, what can be said about the covariance function of $T(X)$ given the covariance function of $X$, e.g. something similar to the ""$A\Sigma A^T$""-expression above? Is there any general statement (even with some additional assumptions like continuity or compactness of $T$) and if so, where can I find it?",,"['functional-analysis', 'probability-theory', 'reference-request', 'stochastic-processes']"
87,Expanding a norm over a given space,Expanding a norm over a given space,,"Let  $1 \leq p<q \leq \infty$ (p an q are not related) Let $\Phi$ be the vector space of all sequences with at most finitely many nonzero elements, meaning $\Phi=\{\{x_n\}_{n=1}^\infty|$ there is $n_o$ such that $x_n=0$ whenever $n\leq n_0\}$ Prove that $\|x\|_q \leq \|x\|_p$ for all $x \in \Phi$ and there is no constant $C$ such that $\|x\|_p \leq C\|x\|_q$ for all $x \in \Phi$ How can I show that $\sup_{0\neq x\in \Psi} \frac{\|x\|_p}{\|x\|_q}=\infty$? this proves both inequations but I cant manage to epand and solve the sup function. What is the system here? is it at all a good direction?","Let  $1 \leq p<q \leq \infty$ (p an q are not related) Let $\Phi$ be the vector space of all sequences with at most finitely many nonzero elements, meaning $\Phi=\{\{x_n\}_{n=1}^\infty|$ there is $n_o$ such that $x_n=0$ whenever $n\leq n_0\}$ Prove that $\|x\|_q \leq \|x\|_p$ for all $x \in \Phi$ and there is no constant $C$ such that $\|x\|_p \leq C\|x\|_q$ for all $x \in \Phi$ How can I show that $\sup_{0\neq x\in \Psi} \frac{\|x\|_p}{\|x\|_q}=\infty$? this proves both inequations but I cant manage to epand and solve the sup function. What is the system here? is it at all a good direction?",,"['functional-analysis', 'supremum-and-infimum']"
88,"Continuity of Banach limit and existence of $\Lambda_0\in(\ell^\infty/c_0)^*$ such that $\Lambda=\Lambda_0\circ q_0$, with $q_0$ the quotient map","Continuity of Banach limit and existence of  such that , with  the quotient map",\Lambda_0\in(\ell^\infty/c_0)^* \Lambda=\Lambda_0\circ q_0 q_0,"Let $\Lambda$ be any Banach limit on $\ell^\infty$ , where $\ell^\infty$ denotes the space of bounded real sequences. A Banach limit is defined as a linear functional $\Lambda$ such that $$ \Lambda(\tau x)=\Lambda(x), \forall x\in\ell^\infty$$ $$ \liminf_{n\rightarrow\infty}x_n\leq\Lambda(x)\leq\limsup_{n\rightarrow\infty}x_n$$ where we write $x=(x_n)_{n\in\mathbb{N}}$ for a sequence $x\in\ell^\infty$ and we define left translation on $\ell^\infty$ by $(\tau x)_n=x_{n+1},n=1,2,\dots$ . I would like to show that $\Lambda\in(\ell^\infty)^*$ , which means that $\Lambda$ is a continuous, linear functional on $\ell^\infty$ . Thus I need to show that $\Lambda$ is continuous. How do I do this? Furthermore I wish to show that there exists a continuous, linear functional $\Lambda_0\in(\ell^\infty/c_0)^*$ such that $\Lambda=\Lambda_0\circ q_0$ , where $$q_0:\ell^\infty\rightarrow\ell^\infty/c_0$$ is the quotient map and $$c_0=\{(x_n)\in\ell^\infty\mid \lim_{n\rightarrow\infty}x_n=0\}$$ I can't seem to get anywhere with these questions. Any help is greatly appreciated.","Let be any Banach limit on , where denotes the space of bounded real sequences. A Banach limit is defined as a linear functional such that where we write for a sequence and we define left translation on by . I would like to show that , which means that is a continuous, linear functional on . Thus I need to show that is continuous. How do I do this? Furthermore I wish to show that there exists a continuous, linear functional such that , where is the quotient map and I can't seem to get anywhere with these questions. Any help is greatly appreciated.","\Lambda \ell^\infty \ell^\infty \Lambda  \Lambda(\tau x)=\Lambda(x), \forall x\in\ell^\infty  \liminf_{n\rightarrow\infty}x_n\leq\Lambda(x)\leq\limsup_{n\rightarrow\infty}x_n x=(x_n)_{n\in\mathbb{N}} x\in\ell^\infty \ell^\infty (\tau x)_n=x_{n+1},n=1,2,\dots \Lambda\in(\ell^\infty)^* \Lambda \ell^\infty \Lambda \Lambda_0\in(\ell^\infty/c_0)^* \Lambda=\Lambda_0\circ q_0 q_0:\ell^\infty\rightarrow\ell^\infty/c_0 c_0=\{(x_n)\in\ell^\infty\mid \lim_{n\rightarrow\infty}x_n=0\}","['real-analysis', 'functional-analysis', 'analysis']"
89,Linear functional: continuous at $x_0=0 \iff$ continuous at all $x$,Linear functional: continuous at  continuous at all,x_0=0 \iff x,"Let $f$ be a linear functional  on a normed space $(X, ||\cdot ||)$. Prove that $f$ is continuous $\iff$ it is continuous at all $x \in X$. Backward direction is trivial: Since $f$ is continuous at all $x$, then it is continuous at $0$ in particular. Forward direction: Assume $f$ is continuous at $x_0=0$, then $$\forall \varepsilon >0, \, \, \exists \delta >0: \, \, ||f(y)-f(0)||=||f(y-0)||=||f(y)||< \varepsilon \, \, \text{whenever} \, \, ||y-0||<\delta, \, \, y \in X$$ Fix $x_0 \in X$. Fix $\varepsilon _0 >0$. Then there exists $\delta_0>0$ such that $$||f(x)-f(x_0)||=||f(x-x_0)||=||f(y)||<\varepsilon_0$$ We let $y=x-x_0$ and the last ""$<$"" holds because we know $||f(y)||< \varepsilon$ for all $\varepsilon>0$ so in particular, it holds for $\varepsilon_0$. Is this OK?","Let $f$ be a linear functional  on a normed space $(X, ||\cdot ||)$. Prove that $f$ is continuous $\iff$ it is continuous at all $x \in X$. Backward direction is trivial: Since $f$ is continuous at all $x$, then it is continuous at $0$ in particular. Forward direction: Assume $f$ is continuous at $x_0=0$, then $$\forall \varepsilon >0, \, \, \exists \delta >0: \, \, ||f(y)-f(0)||=||f(y-0)||=||f(y)||< \varepsilon \, \, \text{whenever} \, \, ||y-0||<\delta, \, \, y \in X$$ Fix $x_0 \in X$. Fix $\varepsilon _0 >0$. Then there exists $\delta_0>0$ such that $$||f(x)-f(x_0)||=||f(x-x_0)||=||f(y)||<\varepsilon_0$$ We let $y=x-x_0$ and the last ""$<$"" holds because we know $||f(y)||< \varepsilon$ for all $\varepsilon>0$ so in particular, it holds for $\varepsilon_0$. Is this OK?",,"['functional-analysis', 'normed-spaces']"
90,"If $0\leq a \leq b$ and $a$ is invertible, then $b$ is invertible","If  and  is invertible, then  is invertible",0\leq a \leq b a b,"Let $\mathscr A$ be a unital C*-algebra and let $a,b\in \mathscr A$ such that $0\leq a \leq b$ and $a$ is invertible. How to show that $b$ is invertible? ($0\leq a \leq b$ means that $a,b$ is positive and that $b-a$ is positive. Moreover, a positive element is a hermitian element with a spectrum which is a subset of $[0,\infty)$.) Since $a$ is invertible, we have that $0 \not \in \sigma(a)$. I guess my question is why this implies that $0 \not \in \sigma(b)$. Maybe one should use functional calculus in some way(?).","Let $\mathscr A$ be a unital C*-algebra and let $a,b\in \mathscr A$ such that $0\leq a \leq b$ and $a$ is invertible. How to show that $b$ is invertible? ($0\leq a \leq b$ means that $a,b$ is positive and that $b-a$ is positive. Moreover, a positive element is a hermitian element with a spectrum which is a subset of $[0,\infty)$.) Since $a$ is invertible, we have that $0 \not \in \sigma(a)$. I guess my question is why this implies that $0 \not \in \sigma(b)$. Maybe one should use functional calculus in some way(?).",,"['functional-analysis', 'operator-algebras', 'spectral-theory', 'c-star-algebras']"
91,Rearrangement of Schauder basis,Rearrangement of Schauder basis,,"My question is a part of an exercise in Banach space theory. In the space $c_0$, for $n \in \mathbb{N}$, let $s_n=\sum_{j=1}^n e_k=(1,1,\cdots,1,0,\cdots,0)$. It's easy to see that $(s_n)_{n\ge 1}$ is a Schauder basis. I want to find an rearrangement $\sigma:\mathbb N \to \mathbb N$(a bijection, of course) of $(s_n)_{n\ge 1}$ such that $(s_{\sigma(n)})_{n\ge 1}$ is no longer a Schauder basis. There are some properties for Schauder basis that might be helpful. For instance, if some coordinate functional is not bounded then the rearrangement must not be a Schauder basis. But I have no idea how to find such an rearrangement. Thanks in advance!","My question is a part of an exercise in Banach space theory. In the space $c_0$, for $n \in \mathbb{N}$, let $s_n=\sum_{j=1}^n e_k=(1,1,\cdots,1,0,\cdots,0)$. It's easy to see that $(s_n)_{n\ge 1}$ is a Schauder basis. I want to find an rearrangement $\sigma:\mathbb N \to \mathbb N$(a bijection, of course) of $(s_n)_{n\ge 1}$ such that $(s_{\sigma(n)})_{n\ge 1}$ is no longer a Schauder basis. There are some properties for Schauder basis that might be helpful. For instance, if some coordinate functional is not bounded then the rearrangement must not be a Schauder basis. But I have no idea how to find such an rearrangement. Thanks in advance!",,"['functional-analysis', 'banach-spaces']"
92,What does the symbol $H^1_0(\Omega)$ mean?,What does the symbol  mean?,H^1_0(\Omega),"Here $\Omega \subset \mathbb{R}^n$ is a closed disc centred at $0$ with radius $r$.The book I am reading is assuming the Dirichlet boundary condition on $\Omega$ and claiming that the dual of  $H^1_0(\Omega)$ is $H^{-1}(\Omega)$. I understand the concept of Sobolev spaces on $\mathbb{R}^n$ or on compact manifolds without boundary. I just need a little help with this notation $H^1_0(\Omega)$. Thanks! PS: Basically, it is the zero on the subscript that is confusing me.","Here $\Omega \subset \mathbb{R}^n$ is a closed disc centred at $0$ with radius $r$.The book I am reading is assuming the Dirichlet boundary condition on $\Omega$ and claiming that the dual of  $H^1_0(\Omega)$ is $H^{-1}(\Omega)$. I understand the concept of Sobolev spaces on $\mathbb{R}^n$ or on compact manifolds without boundary. I just need a little help with this notation $H^1_0(\Omega)$. Thanks! PS: Basically, it is the zero on the subscript that is confusing me.",,"['real-analysis', 'functional-analysis', 'reference-request', 'partial-differential-equations']"
93,On the space $L^0$ and $\lim_{p \to 0} \|f\|_p$,On the space  and,L^0 \lim_{p \to 0} \|f\|_p,"For $0 < p < \infty$, the definitions of the spaces $L^p$ are very natural. Then, we of course want $L^\infty$ and $L^0$ to be some kind of limits of $L^p$ spaces. What does the parameter $p$ tell about a function $f \in L^p$? If $f \in L^p$ for very large $p$, it means that $f$ behaves very well near its singularities. If $f \in L^p$ for very small $p$, it means that $f$ decreases fast at infinity. The limiting case $p \to \infty$ then of course means that $f \in L^\infty$ if $f$ has no singularities in some sense (essentially bounded). Then we define $\|f\|_\infty = \operatorname{ess sup}_{x\in X} |f(x)|$, which is a norm in $L^\infty$. In some cases it even holds that $\|f\|_\infty = \lim_{p \to \infty} \|f\|_p$. How about $p \to 0$? I've never really seen anything said about this space, so I guess it's not very interesting. But I still started to think about it. Now, $f \in L^0$ should mean that $f$ is extremely bounded at infinity, meaning that $f$ is compactly supported. Of course because we are talking about measure and integration theory, the notion of compact support has to be modified accordingly. I guess we want to define that $f \in L^0$ if the support of $f$ is compact modulo sets of measure $0$. Is this how we should think about $L^0$? If it is, then we probably want to define $\|f\|_0 = \mu(\operatorname{spt} f)$, or something like that. So, what is the number $\lim_{p \to 0} \|f\|_p$? For measurable compactly supported bounded functions it is easy to see that  \begin{align} \lim_{p \to 0} \int_X |f|^p = \mu(\operatorname{spt} f)\,. \end{align} But if we want to take the limit of $\|f\|_p$, the integral should be raised to the power $1/p$, which tends to infinity, which complicates things. So my questions are: what should the space $L^0$ be, what should be its ""norm"" $\|f\|_0$ and when is $\|f\|_0$ the limit of $\|f\|_p$? Also, why does the wikipedia page on $L^p$ spaces refer to $L^0$ as the space of measurable functions? (In the case $\mu(X) < \infty$ it would make sense since $L^0$ restricts the behavior of its elements at infinity, and in the $\mu(X)< \infty$ case there is no ""behavior at infinity"".)","For $0 < p < \infty$, the definitions of the spaces $L^p$ are very natural. Then, we of course want $L^\infty$ and $L^0$ to be some kind of limits of $L^p$ spaces. What does the parameter $p$ tell about a function $f \in L^p$? If $f \in L^p$ for very large $p$, it means that $f$ behaves very well near its singularities. If $f \in L^p$ for very small $p$, it means that $f$ decreases fast at infinity. The limiting case $p \to \infty$ then of course means that $f \in L^\infty$ if $f$ has no singularities in some sense (essentially bounded). Then we define $\|f\|_\infty = \operatorname{ess sup}_{x\in X} |f(x)|$, which is a norm in $L^\infty$. In some cases it even holds that $\|f\|_\infty = \lim_{p \to \infty} \|f\|_p$. How about $p \to 0$? I've never really seen anything said about this space, so I guess it's not very interesting. But I still started to think about it. Now, $f \in L^0$ should mean that $f$ is extremely bounded at infinity, meaning that $f$ is compactly supported. Of course because we are talking about measure and integration theory, the notion of compact support has to be modified accordingly. I guess we want to define that $f \in L^0$ if the support of $f$ is compact modulo sets of measure $0$. Is this how we should think about $L^0$? If it is, then we probably want to define $\|f\|_0 = \mu(\operatorname{spt} f)$, or something like that. So, what is the number $\lim_{p \to 0} \|f\|_p$? For measurable compactly supported bounded functions it is easy to see that  \begin{align} \lim_{p \to 0} \int_X |f|^p = \mu(\operatorname{spt} f)\,. \end{align} But if we want to take the limit of $\|f\|_p$, the integral should be raised to the power $1/p$, which tends to infinity, which complicates things. So my questions are: what should the space $L^0$ be, what should be its ""norm"" $\|f\|_0$ and when is $\|f\|_0$ the limit of $\|f\|_p$? Also, why does the wikipedia page on $L^p$ spaces refer to $L^0$ as the space of measurable functions? (In the case $\mu(X) < \infty$ it would make sense since $L^0$ restricts the behavior of its elements at infinity, and in the $\mu(X)< \infty$ case there is no ""behavior at infinity"".)",,"['real-analysis', 'functional-analysis', 'measure-theory', 'intuition', 'lp-spaces']"
94,Help understanding Rudin's proof showing that $C_c(X)$ is dense in $L^p(\mu)$,Help understanding Rudin's proof showing that  is dense in,C_c(X) L^p(\mu),"The proof is from Rudin's ""Real and Complex Analysis."" It states For $1\leq p<\infty$, $C_c(X)$ is dense in $L^p(\mu)$ The proof is Let $S$ be the class of all complex, measurable, simple functions on $X$ such that $\mu(\{x:s(x)\neq 0\})<\infty$. If $s\in S$ and $\epsilon<0$, there exists a $g\in C_c(X)$ such that $g(x)=s(x)$ except on a set of measure $<\epsilon$ and $\vert g\vert\leq\,\parallel s\,\parallel_{\infty}$ (Lusin's Theorem). Hence $$\parallel g-s\parallel_p\leq 2\epsilon^{1/p}\parallel s\,\parallel_{\infty}$$   Since $S$ is dense in $L^p(\mu)$, this completes the proof. I have trouble understanding the inequalities $$\vert g\vert\leq\,\parallel s\,\parallel_{\infty}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(1)$$$$\parallel g-s\parallel_p\leq 2\epsilon^{1/p}\parallel s\,\parallel_{\infty}\,\,\,\,\,\,\,\,\,\,\,(2)$$ I know that the first inequality is supposed to follow from Lusin's Theorem, which in this case would result in $$\sup_{x\in X}\vert g(x)\vert\leq\sup_{x\in X}\vert s(x)\vert$$ However, I am not sure how to get to $(1)$ from here. Lastly, I don't understand how to get inequality $(2)$. Added: I forgot to mention that $X$ is supposed to be a locally compact Hausdorff space and that $\mu$ is a measure on a $\sigma$-algebra $\mathfrak M$ in $X$ with the properties of the measure of Riesz's Representation Theorem.","The proof is from Rudin's ""Real and Complex Analysis."" It states For $1\leq p<\infty$, $C_c(X)$ is dense in $L^p(\mu)$ The proof is Let $S$ be the class of all complex, measurable, simple functions on $X$ such that $\mu(\{x:s(x)\neq 0\})<\infty$. If $s\in S$ and $\epsilon<0$, there exists a $g\in C_c(X)$ such that $g(x)=s(x)$ except on a set of measure $<\epsilon$ and $\vert g\vert\leq\,\parallel s\,\parallel_{\infty}$ (Lusin's Theorem). Hence $$\parallel g-s\parallel_p\leq 2\epsilon^{1/p}\parallel s\,\parallel_{\infty}$$   Since $S$ is dense in $L^p(\mu)$, this completes the proof. I have trouble understanding the inequalities $$\vert g\vert\leq\,\parallel s\,\parallel_{\infty}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(1)$$$$\parallel g-s\parallel_p\leq 2\epsilon^{1/p}\parallel s\,\parallel_{\infty}\,\,\,\,\,\,\,\,\,\,\,(2)$$ I know that the first inequality is supposed to follow from Lusin's Theorem, which in this case would result in $$\sup_{x\in X}\vert g(x)\vert\leq\sup_{x\in X}\vert s(x)\vert$$ However, I am not sure how to get to $(1)$ from here. Lastly, I don't understand how to get inequality $(2)$. Added: I forgot to mention that $X$ is supposed to be a locally compact Hausdorff space and that $\mu$ is a measure on a $\sigma$-algebra $\mathfrak M$ in $X$ with the properties of the measure of Riesz's Representation Theorem.",,"['real-analysis', 'functional-analysis']"
95,Uniformly continuous unitary representations.,Uniformly continuous unitary representations.,,"Let $U$ be a unitary rep. of $\mathbb{R}^d$ on a separable Hilbert space $H$, and $H\cong\oplus L^2_{\mu_v}(\mathbb{R}^d)$ be the spectral decomposition (according to the spectral theorem for these representations). Then it seems that $t\mapsto U(t)$ is continuous (in the operator norm) iff there is a bounded set $K$ in $\mathbb{R}^d$ such that the suppport of all the spectral measures $\mu_v$ is contained in $K$. Is this true, and how could one prove it? I would be happy if someone gives me a sketch of proof and hints, containing the main idea.","Let $U$ be a unitary rep. of $\mathbb{R}^d$ on a separable Hilbert space $H$, and $H\cong\oplus L^2_{\mu_v}(\mathbb{R}^d)$ be the spectral decomposition (according to the spectral theorem for these representations). Then it seems that $t\mapsto U(t)$ is continuous (in the operator norm) iff there is a bounded set $K$ in $\mathbb{R}^d$ such that the suppport of all the spectral measures $\mu_v$ is contained in $K$. Is this true, and how could one prove it? I would be happy if someone gives me a sketch of proof and hints, containing the main idea.",,"['functional-analysis', 'measure-theory', 'representation-theory']"
96,"Let $f_1,f_2,\ldots, f_n$ be linear functionals on $X$. Show $f=\sum_{i=1}^n\lambda_i f_i$ iff $\bigcap \ker f_i \subset \ker f$",Let  be linear functionals on . Show  iff,"f_1,f_2,\ldots, f_n X f=\sum_{i=1}^n\lambda_i f_i \bigcap \ker f_i \subset \ker f","Problem Let $f_1,f_2,\ldots, f_n$ be linear functionals on a vector space $X$. Show that there exist constants $\lambda_1,\ldots,\lambda_n$ satisfying $$f=\sum_{i=1}^n\lambda_i f_i$$ if and only if  $\bigcap_{i=1}^n \ker f_i \subset \ker f$. Attempt If there exist constants $\lambda_1,\ldots,\lambda_n$ satisfying $$f=\sum_{i=1}^n\lambda_i f_i,$$ then $x\in \bigcap_{i=1}^n \ker f_i$ gives $\sum_{i=1}^n\lambda_i f_i=0$. On the other hand, define $$V=\{y\in\mathbb{R}^n:\exists x\in X \textrm{ such that } y=\left(f_1(x),\ldots,f_n(x)\right)\},$$ and  $g:V \rightarrow \mathbb{R}$ by $$g\left(\left(f_1(x),\ldots,f_n(x)\right)\right)=f(x).$$ $V$ is seen to be a vector subspace of $\mathbb{R}^n$. If $\left(f_1(x),\ldots,f_n(x)\right)=\left(f_1(y),\ldots,f_n(y)\right)$ Then  $f_i(x-y)=0$ so $(x-y) \in \ker f_i$ for all $i$. By assumption, $(x-y)\in \ker f$ so $f(x)=f(y)$. Hence $g$ is well defined. Clearly $g$ is linear. Denote by $g^*$ a linear extention of $g$ to all of $\mathbb{R}^n$. Then there exist $\lambda_1,\ldots,\lambda_n$ such that $g^*(z_1,\ldots,z_n)=\sum_{i=1}^n\lambda_i z_i$. $\ ^{(1)}$ In particular, $(z_1,\ldots,z_n)=(f_1(x),\ldots,f_n(x))$ give the desired result. Question I am having trouble justifying $(1)$. That is, why can we say $g^*(z_1,\ldots z_n)=\sum_{i=1}^n \lambda_i z_i$?","Problem Let $f_1,f_2,\ldots, f_n$ be linear functionals on a vector space $X$. Show that there exist constants $\lambda_1,\ldots,\lambda_n$ satisfying $$f=\sum_{i=1}^n\lambda_i f_i$$ if and only if  $\bigcap_{i=1}^n \ker f_i \subset \ker f$. Attempt If there exist constants $\lambda_1,\ldots,\lambda_n$ satisfying $$f=\sum_{i=1}^n\lambda_i f_i,$$ then $x\in \bigcap_{i=1}^n \ker f_i$ gives $\sum_{i=1}^n\lambda_i f_i=0$. On the other hand, define $$V=\{y\in\mathbb{R}^n:\exists x\in X \textrm{ such that } y=\left(f_1(x),\ldots,f_n(x)\right)\},$$ and  $g:V \rightarrow \mathbb{R}$ by $$g\left(\left(f_1(x),\ldots,f_n(x)\right)\right)=f(x).$$ $V$ is seen to be a vector subspace of $\mathbb{R}^n$. If $\left(f_1(x),\ldots,f_n(x)\right)=\left(f_1(y),\ldots,f_n(y)\right)$ Then  $f_i(x-y)=0$ so $(x-y) \in \ker f_i$ for all $i$. By assumption, $(x-y)\in \ker f$ so $f(x)=f(y)$. Hence $g$ is well defined. Clearly $g$ is linear. Denote by $g^*$ a linear extention of $g$ to all of $\mathbb{R}^n$. Then there exist $\lambda_1,\ldots,\lambda_n$ such that $g^*(z_1,\ldots,z_n)=\sum_{i=1}^n\lambda_i z_i$. $\ ^{(1)}$ In particular, $(z_1,\ldots,z_n)=(f_1(x),\ldots,f_n(x))$ give the desired result. Question I am having trouble justifying $(1)$. That is, why can we say $g^*(z_1,\ldots z_n)=\sum_{i=1}^n \lambda_i z_i$?",,['functional-analysis']
97,Adjoint operator on Banach space,Adjoint operator on Banach space,,"Suppose $X$ and $Y$ are Banach spaces and $T:X\to Y$ is a bounded linear operator. Show that $T$ is an isometric isomorphism if and only if its adjoint $T^*$ is also an isometric isomorphism. Given an example where $T$ is isometric while $T^*$ is not. I manage to prove that if $T$ is an isometric isomorphism, then $T^*$ is an isomorphism. However, I don't know how to show $T^*$ is isometric. Can someone please help me? For the other direction, can I just show $T^{**}=T$ (reflexive) and conclude; or do I have to use something else to prove? Also, what would be the example? Can I use the shift in $l^2$? Thank you.","Suppose $X$ and $Y$ are Banach spaces and $T:X\to Y$ is a bounded linear operator. Show that $T$ is an isometric isomorphism if and only if its adjoint $T^*$ is also an isometric isomorphism. Given an example where $T$ is isometric while $T^*$ is not. I manage to prove that if $T$ is an isometric isomorphism, then $T^*$ is an isomorphism. However, I don't know how to show $T^*$ is isometric. Can someone please help me? For the other direction, can I just show $T^{**}=T$ (reflexive) and conclude; or do I have to use something else to prove? Also, what would be the example? Can I use the shift in $l^2$? Thank you.",,"['functional-analysis', 'banach-spaces', 'adjoint-operators', 'isometry']"
98,Isomorphism on dense subset,Isomorphism on dense subset,,"I am wondering if the following could be done. I want to show two Banach spaces $X$ and $Y$ are isomorphic. If $A$ is dense in $X$, and $B$ is dense in $Y$, is it sufficient to show there is an isomorphism $S : A\to B$ to deduce (extending in the obvious way) that $X$ and $Y$ are isomorphic?","I am wondering if the following could be done. I want to show two Banach spaces $X$ and $Y$ are isomorphic. If $A$ is dense in $X$, and $B$ is dense in $Y$, is it sufficient to show there is an isomorphism $S : A\to B$ to deduce (extending in the obvious way) that $X$ and $Y$ are isomorphic?",,"['functional-analysis', 'banach-spaces']"
99,Is this proof that $c_0$ is a closed subspace of $\ell^\infty$ correct? Also need some help finish it.,Is this proof that  is a closed subspace of  correct? Also need some help finish it.,c_0 \ell^\infty,"$c_0 \subset \ell^\infty$ is the subspace of all sequences of scalars converging to zero. Let $x \in \bar{c_0}$. Then there exists a Cauchy sequence of sequences $(x^k)_{k \in \mathbb{N}}$ in $c_0$, where $x^k = (a_1^k, a_2^k,a_3^k \dots )$ such that $x^k \to x = (a_1, a_2, a_3, \dots) $ under the $\ell^\infty$ norm. We will show $x \in c_0$. By definition of convergence, for any $\epsilon > 0$, there exists $N_1 \in \mathbb{N}$ such that $$\|x^k - x\|_{\ell^\infty} < \epsilon /3 \quad \forall \, \, k \ge N_1$$ This implies $|a_i^k - a_i| \le \sup_{j \ge 1} \{|a_j^k - a_j|\} < \epsilon/3 \quad \forall \,\, i \ge 1$, Since $x$ is convergent in $\ell^\infty$, it follows that for all $k \in \mathbb N$ and $\epsilon > 0$, there exists $N_2(\epsilon) \in \mathbb N$ such that $$|a_i^k - a_j^k| < \epsilon/3 \quad \forall \, \, i, j \ge N_2$$ Then, $$|a_i - a_j| \le |a_i - a_i^k| + |a_i^k - a_j^k| + |a_j^k - a_j| < \epsilon \quad \forall \, \, i, j \ge N_2$$ This implies $x = (a_1, a_2, \dots )$ is a Cauchy sequence of numbers in $\ell^\infty$ and so it's convergent.To show $x \in c_0$, we must show $a_i \to 0$. $|a_i|\le |a_i - a_i^k| + |a_i^k|$ This is where I get stuck. Can I claim $|a_i^k| < 2\epsilon /3$?","$c_0 \subset \ell^\infty$ is the subspace of all sequences of scalars converging to zero. Let $x \in \bar{c_0}$. Then there exists a Cauchy sequence of sequences $(x^k)_{k \in \mathbb{N}}$ in $c_0$, where $x^k = (a_1^k, a_2^k,a_3^k \dots )$ such that $x^k \to x = (a_1, a_2, a_3, \dots) $ under the $\ell^\infty$ norm. We will show $x \in c_0$. By definition of convergence, for any $\epsilon > 0$, there exists $N_1 \in \mathbb{N}$ such that $$\|x^k - x\|_{\ell^\infty} < \epsilon /3 \quad \forall \, \, k \ge N_1$$ This implies $|a_i^k - a_i| \le \sup_{j \ge 1} \{|a_j^k - a_j|\} < \epsilon/3 \quad \forall \,\, i \ge 1$, Since $x$ is convergent in $\ell^\infty$, it follows that for all $k \in \mathbb N$ and $\epsilon > 0$, there exists $N_2(\epsilon) \in \mathbb N$ such that $$|a_i^k - a_j^k| < \epsilon/3 \quad \forall \, \, i, j \ge N_2$$ Then, $$|a_i - a_j| \le |a_i - a_i^k| + |a_i^k - a_j^k| + |a_j^k - a_j| < \epsilon \quad \forall \, \, i, j \ge N_2$$ This implies $x = (a_1, a_2, \dots )$ is a Cauchy sequence of numbers in $\ell^\infty$ and so it's convergent.To show $x \in c_0$, we must show $a_i \to 0$. $|a_i|\le |a_i - a_i^k| + |a_i^k|$ This is where I get stuck. Can I claim $|a_i^k| < 2\epsilon /3$?",,['functional-analysis']
