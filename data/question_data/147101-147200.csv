,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Find all functions $f(x+y)=f(x^{2}+y^{2})$ for positive $x,y$",Find all functions  for positive,"f(x+y)=f(x^{2}+y^{2}) x,y","Find all functions $f:\mathbb{R}^{+}\to \mathbb{R}$ such that for any $x,y\in \mathbb{R}^{+}$ the following holds: $$f(x+y)=f(x^{2}+y^{2}).$$","Find all functions $f:\mathbb{R}^{+}\to \mathbb{R}$ such that for any $x,y\in \mathbb{R}^{+}$ the following holds: $$f(x+y)=f(x^{2}+y^{2}).$$",,"['analysis', 'functions', 'contest-math', 'functional-equations']"
1,Solving a quadratic equation with precision when using floating point variables,Solving a quadratic equation with precision when using floating point variables,,"I know how to solve a basic quadratic equation with the formula $$t_{1,2}=\dfrac{-b\pm\sqrt{b^2-4ac}}{2a}$$ but I learned that if $b \approx \sqrt{b^2-4ac}$ floating point precision may give slightly wrong results and this approach is better. It works, indeed. But why? Is there a simple explanation on why this works?","I know how to solve a basic quadratic equation with the formula $$t_{1,2}=\dfrac{-b\pm\sqrt{b^2-4ac}}{2a}$$ but I learned that if $b \approx \sqrt{b^2-4ac}$ floating point precision may give slightly wrong results and this approach is better. It works, indeed. But why? Is there a simple explanation on why this works?",,"['analysis', 'quadratics', 'floating-point']"
2,Uses for esoteric integral symbols,Uses for esoteric integral symbols,,"A while ago, I was searching for a TeX package which would provide a double integral symbol with a circle which I could use to typeset some lecture notes on surface integrals.  I happened upon the esint package, which includes several integral symbols I had never seen before.  In particular, integral symbols with a square ( \sqint and \sqiint ), a sloped dash ( \fint ) and the very curious \landupint and \landdownint .  Is anyone familiar with any uses for these symbols? Added (TB) Here's an image of the integral signs in question:","A while ago, I was searching for a TeX package which would provide a double integral symbol with a circle which I could use to typeset some lecture notes on surface integrals.  I happened upon the esint package, which includes several integral symbols I had never seen before.  In particular, integral symbols with a square ( \sqint and \sqiint ), a sloped dash ( \fint ) and the very curious \landupint and \landdownint .  Is anyone familiar with any uses for these symbols? Added (TB) Here's an image of the integral signs in question:",,"['analysis', 'notation']"
3,The Notorious Triangle Problem,The Notorious Triangle Problem,,"I was told this question by a friend, who said that their friend had thought about it on and off for six months without any luck.  I have then had it for a while without any luck either.  It is in the style of a contest question, but it is unlike any other, as really nothing seems to gain any ground. Here's the question: Let $f:[0,1]\rightarrow \mathbb{R}$ be a continuous function satisfying the following property:  If $ABC$ is the equilateral triangle with side lengths 1, we have for any point $P$ inside $ABC$, $f(\overline{AP})+f(\overline{BP})+f(\overline{CP})=0$ where $\overline{AP}$ is the distance from point P to vertex $A$.  (Example: by taking $P=A$ we see $f(0)=-2f(1)$.) Prove or disprove: $f$ must be identically zero on its domain Notice that continuity is critical since otherwise I could just take the function $f(x)=0$ whenever $x\in (0,1)$ and $f(0)=1$, $f(1)=\frac{-1}{2}$.","I was told this question by a friend, who said that their friend had thought about it on and off for six months without any luck.  I have then had it for a while without any luck either.  It is in the style of a contest question, but it is unlike any other, as really nothing seems to gain any ground. Here's the question: Let $f:[0,1]\rightarrow \mathbb{R}$ be a continuous function satisfying the following property:  If $ABC$ is the equilateral triangle with side lengths 1, we have for any point $P$ inside $ABC$, $f(\overline{AP})+f(\overline{BP})+f(\overline{CP})=0$ where $\overline{AP}$ is the distance from point P to vertex $A$.  (Example: by taking $P=A$ we see $f(0)=-2f(1)$.) Prove or disprove: $f$ must be identically zero on its domain Notice that continuity is critical since otherwise I could just take the function $f(x)=0$ whenever $x\in (0,1)$ and $f(0)=1$, $f(1)=\frac{-1}{2}$.",,"['analysis', 'contest-math', 'triangles', 'functional-equations']"
4,What is a good book for math students to learn machine learning in depth?,What is a good book for math students to learn machine learning in depth?,,"I am a math master student and have done fundamental math courses like probability theory, measure theory, linear algebra and know a little bit about functional analysis. What is good way for me to learn machine learning in depth? I have read the classical text Pattern-Recognition and Machine Learning last summer; my impression was that it was very ineffective to read the book chapter by chapter like a mathematical text. The book does not go deep enough for many algorithms and skip too many steps considered too technical by engineers. Is there a machine learning book that maybe does not cover too many topics, but treat each one in depth and takes advantage of math when necessary? It will be great to be able connect fundamental mathematical objects with machine learning (I am thinking about Lp spaces, hilbert space etc).","I am a math master student and have done fundamental math courses like probability theory, measure theory, linear algebra and know a little bit about functional analysis. What is good way for me to learn machine learning in depth? I have read the classical text Pattern-Recognition and Machine Learning last summer; my impression was that it was very ineffective to read the book chapter by chapter like a mathematical text. The book does not go deep enough for many algorithms and skip too many steps considered too technical by engineers. Is there a machine learning book that maybe does not cover too many topics, but treat each one in depth and takes advantage of math when necessary? It will be great to be able connect fundamental mathematical objects with machine learning (I am thinking about Lp spaces, hilbert space etc).",,"['analysis', 'reference-request', 'book-recommendation', 'machine-learning']"
5,Transforming a distance function to a kernel,Transforming a distance function to a kernel,,"Fix a domain $X$: Let $d : X \times X \rightarrow \mathbb{R}$ be a distance function on $X$, with the properties $d(x,y) = 0 \iff x = y$ for all $x,y$ $d(x,y) = d(y,x)$ for all $x,y$ Optionally, $d$ might satisfy the triangle inequality (making it a metric), but this is not necessarily the case. In machine learning, a kernel is a function $K : X \times X \rightarrow \mathbb{R}$ with the properties $K(x,y) = 1 \iff x = y$ $K(x,y) = K(y,x)$ for all $n$ and all sequences $x_1, \ldots, x_n$, the Gram matrix $\mathbf{K}$ with $\mathbf{K}_{ij} = K(x_i, x_j)$ is positive definite. A space that admits a kernel is handy in ML, and often if all you have is a distance function, you can compute a kernel-like object in the form $$ \kappa_d(x, y) = \exp(- \gamma d^2(x,y)) $$ The question: Under what conditions on $d$ is the resulting function $\kappa_d$ a   kernel ?","Fix a domain $X$: Let $d : X \times X \rightarrow \mathbb{R}$ be a distance function on $X$, with the properties $d(x,y) = 0 \iff x = y$ for all $x,y$ $d(x,y) = d(y,x)$ for all $x,y$ Optionally, $d$ might satisfy the triangle inequality (making it a metric), but this is not necessarily the case. In machine learning, a kernel is a function $K : X \times X \rightarrow \mathbb{R}$ with the properties $K(x,y) = 1 \iff x = y$ $K(x,y) = K(y,x)$ for all $n$ and all sequences $x_1, \ldots, x_n$, the Gram matrix $\mathbf{K}$ with $\mathbf{K}_{ij} = K(x_i, x_j)$ is positive definite. A space that admits a kernel is handy in ML, and often if all you have is a distance function, you can compute a kernel-like object in the form $$ \kappa_d(x, y) = \exp(- \gamma d^2(x,y)) $$ The question: Under what conditions on $d$ is the resulting function $\kappa_d$ a   kernel ?",,"['analysis', 'hilbert-spaces', 'machine-learning']"
6,How does topology enter Number theory and how we can grasp its essence?,How does topology enter Number theory and how we can grasp its essence?,,"In infinite Galois theory,main theorem failed and we get a ""Krull topology"" to mend the main theorem, we even generalized that to make definition for profinite group. In local class field theory, the multiplicative group modulo the norm group is isomorphic to the Galois group both algebraically and topologically. So is the topology we just defined on infinite Galois group is ""forced to define"" because of that isomorphism? If you teach such a course in Galois theory, how would you convince your students that this topology comes out in ""natural""? There is another way topology arise: by study prime $p$, we deduce the $p$-adic topology, we complete the space we get local field and also a topological field. Can we realize that all the topologies that arise in number theory are just a mimic of that? Since I am not a math major, I get into math since my teacher teach me quadratic reciprocity when I was a high schooler, I led all the way myself to class field theory and get lost when I encounter the ""modern language"" such as Galois cohomology and schemes. I saw a remark that in order to gain arithmetic information we need a ground base: the direct product of all $\mathbb{Q}_p$ ($p$-adic number field), but since it is not locally compact, we can not apply analysis on it, so we make restricted product and get idèles and adèles. I know little about analysis and I get puzzled. Hecke proved quadratic reciprocity for number field using Fourier analysis and Tate has a famous ""Thesis"", both of which confused me. Can someone explain that to me, what led these guys to do that,and what were they doing? Finally I summarize my odd words: Why we introduce topology to various group such as a Galois group, and how do we realize that topology? Why, in doing arithmetic, do we need a locally compact group or field to apply ""analysis""? what is ""apply analysis"" all about? (If it is not that easy to explain for a outsider, give me a link to some elementary resource as well; thanks!)","In infinite Galois theory,main theorem failed and we get a ""Krull topology"" to mend the main theorem, we even generalized that to make definition for profinite group. In local class field theory, the multiplicative group modulo the norm group is isomorphic to the Galois group both algebraically and topologically. So is the topology we just defined on infinite Galois group is ""forced to define"" because of that isomorphism? If you teach such a course in Galois theory, how would you convince your students that this topology comes out in ""natural""? There is another way topology arise: by study prime $p$, we deduce the $p$-adic topology, we complete the space we get local field and also a topological field. Can we realize that all the topologies that arise in number theory are just a mimic of that? Since I am not a math major, I get into math since my teacher teach me quadratic reciprocity when I was a high schooler, I led all the way myself to class field theory and get lost when I encounter the ""modern language"" such as Galois cohomology and schemes. I saw a remark that in order to gain arithmetic information we need a ground base: the direct product of all $\mathbb{Q}_p$ ($p$-adic number field), but since it is not locally compact, we can not apply analysis on it, so we make restricted product and get idèles and adèles. I know little about analysis and I get puzzled. Hecke proved quadratic reciprocity for number field using Fourier analysis and Tate has a famous ""Thesis"", both of which confused me. Can someone explain that to me, what led these guys to do that,and what were they doing? Finally I summarize my odd words: Why we introduce topology to various group such as a Galois group, and how do we realize that topology? Why, in doing arithmetic, do we need a locally compact group or field to apply ""analysis""? what is ""apply analysis"" all about? (If it is not that easy to explain for a outsider, give me a link to some elementary resource as well; thanks!)",,"['analysis', 'algebraic-geometry', 'galois-theory', 'algebraic-number-theory', 'class-field-theory']"
7,Ideals in $C(X)$,Ideals in,C(X),"Let $X$ be a Hausdorf Compact topological space.  Please help me to show, for the purpose of understanding an example in some of my lecture notes, that the closed ideals in $C(X)$ are of the following form: $I_C=\{f|f(C)={0\}}$ and $C$ is a closed set. What I have done so far is to show that $I_C$ is a closed ideal, and that if $I$ is an ideal, then I should define $C$ to be the vanishing set of $I$, which is closed.  I can then see that $I$ is contained in $I_C$.  I then try to do a sort of urysohn/partition of unity argument to show that if $f$ is in $I_C$ i.e. it vanishes on C that I can write it in terms of things in I.  That's where I struggle. Thanks.","Let $X$ be a Hausdorf Compact topological space.  Please help me to show, for the purpose of understanding an example in some of my lecture notes, that the closed ideals in $C(X)$ are of the following form: $I_C=\{f|f(C)={0\}}$ and $C$ is a closed set. What I have done so far is to show that $I_C$ is a closed ideal, and that if $I$ is an ideal, then I should define $C$ to be the vanishing set of $I$, which is closed.  I can then see that $I$ is contained in $I_C$.  I then try to do a sort of urysohn/partition of unity argument to show that if $f$ is in $I_C$ i.e. it vanishes on C that I can write it in terms of things in I.  That's where I struggle. Thanks.",,"['analysis', 'operator-algebras']"
8,An asymptotic expression of sum of powers of binomial coefficients.,An asymptotic expression of sum of powers of binomial coefficients.,,"Let $k$ be a fixed positive number and $n$ an integer increasing to infinity. Then  $$\sum_{\nu =0}^n \binom{n}{\nu}^k \sim \frac{2^{kn}}{\sqrt{k}} \left( \frac{2}{\pi n} \right)^{\frac{k-1}{2}}.$$  This is from Polya's Problems and Theorems in Analysis , Vol. 1, Part II, Problem 40. The proof provided in the book is too simple. It says details can be found in Jordan's Cours d'Analyse , Vol.2, 3rd Ed, pp. 218-221. However, I cannot find this edition online, and what's worse, there is not any English translations. Can anyone give a proof in detail?","Let $k$ be a fixed positive number and $n$ an integer increasing to infinity. Then  $$\sum_{\nu =0}^n \binom{n}{\nu}^k \sim \frac{2^{kn}}{\sqrt{k}} \left( \frac{2}{\pi n} \right)^{\frac{k-1}{2}}.$$  This is from Polya's Problems and Theorems in Analysis , Vol. 1, Part II, Problem 40. The proof provided in the book is too simple. It says details can be found in Jordan's Cours d'Analyse , Vol.2, 3rd Ed, pp. 218-221. However, I cannot find this edition online, and what's worse, there is not any English translations. Can anyone give a proof in detail?",,"['analysis', 'asymptotics', 'binomial-coefficients']"
9,How do I construct a function $\operatorname{sog}$ such that $\operatorname{sog}\circ\operatorname{sog} = \log$?,How do I construct a function  such that ?,\operatorname{sog} \operatorname{sog}\circ\operatorname{sog} = \log,Imagine a real-valued semilog function $\DeclareMathOperator{\sog}{sog}\sog$ with the property that $$\sog(\sog(x)) = \log(x)$$   for all real $x>0$. My questions: Does such a function exist? How do I compute it?,Imagine a real-valued semilog function $\DeclareMathOperator{\sog}{sog}\sog$ with the property that $$\sog(\sog(x)) = \log(x)$$   for all real $x>0$. My questions: Does such a function exist? How do I compute it?,,"['analysis', 'functions', 'logarithms']"
10,If $\lim_{x\to\infty}(f(x)+f'(x))=L$ show that $\lim_{x\to\infty} f(x) = L$ and $\lim_{x\to\infty} f'(x) = 0$ [duplicate],If  show that  and  [duplicate],\lim_{x\to\infty}(f(x)+f'(x))=L \lim_{x\to\infty} f(x) = L \lim_{x\to\infty} f'(x) = 0,"This question already has answers here : If $\lim\limits_{x\rightarrow\infty} (f'(x)+f(x)) =L<\infty$, does $\lim\limits_{x\rightarrow\infty} f(x) $ exist? (2 answers) Closed 5 months ago . Let $f:(0,\infty) \to R$ be differentiable. Suppose that $\lim_{x\to\infty}(f(x)+f'(x))=L$. Show that $\lim_{x\to\infty} f(x) = L$ and $\lim_{x\to\infty} f'(x) = 0$. (Hint: Write $f(x) = e^xf(x)/e^x$ and use l’Hopital’s Rule.) My working for $\lim_{x\to\infty}f'(x)=0$: For $\lim_{x\to\infty}f'(x) = 0$, I let $f(x) = e^xf(x)/e^x$ and applied quotient rule which then cancels off $e^{2x}$ and I'm left with $\lim_{x\to\infty}(f(x)+f'(x)-f(x))$. Can I then equate this with $\lim_{x\to\infty} \left(f(x)+f'(x)\right) - \lim_{x\to\infty}f(x)$ which then gives $L-L=0$? Is this step correct?","This question already has answers here : If $\lim\limits_{x\rightarrow\infty} (f'(x)+f(x)) =L<\infty$, does $\lim\limits_{x\rightarrow\infty} f(x) $ exist? (2 answers) Closed 5 months ago . Let $f:(0,\infty) \to R$ be differentiable. Suppose that $\lim_{x\to\infty}(f(x)+f'(x))=L$. Show that $\lim_{x\to\infty} f(x) = L$ and $\lim_{x\to\infty} f'(x) = 0$. (Hint: Write $f(x) = e^xf(x)/e^x$ and use l’Hopital’s Rule.) My working for $\lim_{x\to\infty}f'(x)=0$: For $\lim_{x\to\infty}f'(x) = 0$, I let $f(x) = e^xf(x)/e^x$ and applied quotient rule which then cancels off $e^{2x}$ and I'm left with $\lim_{x\to\infty}(f(x)+f'(x)-f(x))$. Can I then equate this with $\lim_{x\to\infty} \left(f(x)+f'(x)\right) - \lim_{x\to\infty}f(x)$ which then gives $L-L=0$? Is this step correct?",,['analysis']
11,A curious infinite product of factorials,A curious infinite product of factorials,,"I found the following infinite product of factorials without proof: $$\small\prod_{n=1}^\infty\frac{{(2 n)!}^{20}\,{(8 n)!}^{32}\,{(32 n)!}^2}{{n!}^4\,{(4 n)!}^{37}\,{(16 n)!}^{13}}=\\\frac{\sin ^{14}\!\frac{\pi }{8}\cdot\sin \frac{\pi }{32} \cdot\sin \frac{3 \pi    }{32} \cdot\sin \frac{5 \pi }{32} \cdot\sin \frac{7 \pi }{32}}{\sin ^6\!\frac{\pi }{16} \cdot\sin ^6\!\frac{3\pi }{16}}\cdot \frac{2^{1283/64}\,\pi^{14}\,\Gamma^{10} \!\left(\frac{1}{8}\right) \Gamma^2\! \left(\frac{5}{32}\right) \Gamma^2 \!\left(\frac{7}{32}\right)}{\Gamma^{18}    \!\left(\frac{5}{8}\right) \Gamma^{10} \!\left(\frac{1}{16}\right) \Gamma^{10} \!\left(\frac{3}{16}\right) \Gamma^2 \!\left(\frac{17}{32}\right)    \Gamma^2 \!\left(\frac{19}{32}\right)}.$$ We can verify that $$\small\frac{{(2 n)!}^{20}\,{(8 n)!}^{32}\,{(32 n)!}^2}{{n!}^4\,{(4 n)!}^{37}\,{(16 n)!}^{13}}=1+\mathcal O\left(n^{-3}\right),$$ so the product indeed converges. Can you suggest how to prove its closed form on the right? Is it possible to further simplify it? Is it possible to find a simpler convergent infinite product of this form (involving only integer powers of factorials of integer multiples of $n$ )?",I found the following infinite product of factorials without proof: We can verify that so the product indeed converges. Can you suggest how to prove its closed form on the right? Is it possible to further simplify it? Is it possible to find a simpler convergent infinite product of this form (involving only integer powers of factorials of integer multiples of )?,"\small\prod_{n=1}^\infty\frac{{(2 n)!}^{20}\,{(8 n)!}^{32}\,{(32 n)!}^2}{{n!}^4\,{(4 n)!}^{37}\,{(16 n)!}^{13}}=\\\frac{\sin ^{14}\!\frac{\pi }{8}\cdot\sin \frac{\pi }{32} \cdot\sin \frac{3 \pi
   }{32} \cdot\sin \frac{5 \pi }{32} \cdot\sin \frac{7 \pi }{32}}{\sin ^6\!\frac{\pi }{16} \cdot\sin ^6\!\frac{3\pi }{16}}\cdot \frac{2^{1283/64}\,\pi^{14}\,\Gamma^{10} \!\left(\frac{1}{8}\right) \Gamma^2\! \left(\frac{5}{32}\right) \Gamma^2 \!\left(\frac{7}{32}\right)}{\Gamma^{18}
   \!\left(\frac{5}{8}\right) \Gamma^{10} \!\left(\frac{1}{16}\right) \Gamma^{10} \!\left(\frac{3}{16}\right) \Gamma^2 \!\left(\frac{17}{32}\right)
   \Gamma^2 \!\left(\frac{19}{32}\right)}. \small\frac{{(2 n)!}^{20}\,{(8 n)!}^{32}\,{(32 n)!}^2}{{n!}^4\,{(4 n)!}^{37}\,{(16 n)!}^{13}}=1+\mathcal O\left(n^{-3}\right), n","['analysis', 'factorial', 'gamma-function', 'infinite-product', 'conjectures']"
12,Is it necessary for one to understand analysis?,Is it necessary for one to understand analysis?,,"Is it necessary for one to understand analysis in order to pursue a career in mathematics? Basically, I am very weak at analysis. But the problem is that most of the topics listed in the syllabus include analysis: Real analysis Complex analysis Topology Functional analysis Measure theory Manifold theory All require a good understanding of analysis. Is there any scope for me to survive in the mathematical world? I am only interested in Group Theory and Ring Theory (i.e, Abstract Algebra). Is it possible for me to just carry on, or will I also have to develop a liking for the topics listed above? Is it possible to pursue a career without them? Sorry if the question is very personal, but I don’t have other options.","Is it necessary for one to understand analysis in order to pursue a career in mathematics? Basically, I am very weak at analysis. But the problem is that most of the topics listed in the syllabus include analysis: Real analysis Complex analysis Topology Functional analysis Measure theory Manifold theory All require a good understanding of analysis. Is there any scope for me to survive in the mathematical world? I am only interested in Group Theory and Ring Theory (i.e, Abstract Algebra). Is it possible for me to just carry on, or will I also have to develop a liking for the topics listed above? Is it possible to pursue a career without them? Sorry if the question is very personal, but I don’t have other options.",,"['analysis', 'soft-question', 'learning']"
13,Differentiable at a point with positive derivative implies increasing in neighborhood of point?,Differentiable at a point with positive derivative implies increasing in neighborhood of point?,,"Let $\,f: \mathbb{R} \rightarrow\mathbb{R}$ be some function st $f(0)=0$ and $f'(0) > 0$. Is it the case that $f$ must be increasing in some neightborhood of zero? If $f$ is differentiable in some neighborhood of $0$ then the answer is trivial with the MVT, however all we have is differentiability at a point. I don't think the premise holds, take for example $f(x) = \begin{cases} \sin(x) & \text{, $x\in\mathbb{Q}$} \\ x & \text{, $x \notin \mathbb{Q}$} \end{cases} $ The function seems to be differentiable near $0$ with derivative $1$ but is neither increasing nor decreasing near $0$. Is this correct? Would you have anymore counterexamples?","Let $\,f: \mathbb{R} \rightarrow\mathbb{R}$ be some function st $f(0)=0$ and $f'(0) > 0$. Is it the case that $f$ must be increasing in some neightborhood of zero? If $f$ is differentiable in some neighborhood of $0$ then the answer is trivial with the MVT, however all we have is differentiability at a point. I don't think the premise holds, take for example $f(x) = \begin{cases} \sin(x) & \text{, $x\in\mathbb{Q}$} \\ x & \text{, $x \notin \mathbb{Q}$} \end{cases} $ The function seems to be differentiable near $0$ with derivative $1$ but is neither increasing nor decreasing near $0$. Is this correct? Would you have anymore counterexamples?",,"['analysis', 'derivatives']"
14,Sequence converging to different limits,Sequence converging to different limits,,"In a metric space, is it possible to find a sequence which converges to two different limits with respect to two different metrics? Obviously the metrics can't be equivalent.","In a metric space, is it possible to find a sequence which converges to two different limits with respect to two different metrics? Obviously the metrics can't be equivalent.",,"['analysis', 'metric-spaces']"
15,What are the consequences if Axiom of Infinity is negated?,What are the consequences if Axiom of Infinity is negated?,,"What mathematics can be built with standard ZFC with Axiom of Infinity replaced with its negation? Can the analysis be built?  Is there special name for ""ZFC without Infinity"" set theory? I also assume that using symbols like $+\infty$, $-\infty$ when dealing with properties of functions and their limits would still be possible even with Axim of Infinity negated (correct me if I am wrong). UPDATE In light of the answer by Andres Caicedo which suggested Peano arithmetic, I want to point out that Wikipedia says about Peano arithmetic ""Axioms 1, 6, 7 and 8 imply that the set of natural numbers is infinite, because it contains at least the infinite subset"". I do not know how this can be interpreted as having axiom of infinity but I am interested what if Peano arithmetic modified the following way: Added an axiom 5a: There is a natural number $\infty$ which has no successor, for any natural number n $S(\infty)=n$ is false. Axiom 6 modified: For every natural number n except $\infty$, S(n) is a natural number","What mathematics can be built with standard ZFC with Axiom of Infinity replaced with its negation? Can the analysis be built?  Is there special name for ""ZFC without Infinity"" set theory? I also assume that using symbols like $+\infty$, $-\infty$ when dealing with properties of functions and their limits would still be possible even with Axim of Infinity negated (correct me if I am wrong). UPDATE In light of the answer by Andres Caicedo which suggested Peano arithmetic, I want to point out that Wikipedia says about Peano arithmetic ""Axioms 1, 6, 7 and 8 imply that the set of natural numbers is infinite, because it contains at least the infinite subset"". I do not know how this can be interpreted as having axiom of infinity but I am interested what if Peano arithmetic modified the following way: Added an axiom 5a: There is a natural number $\infty$ which has no successor, for any natural number n $S(\infty)=n$ is false. Axiom 6 modified: For every natural number n except $\infty$, S(n) is a natural number",,"['analysis', 'set-theory', 'axioms']"
16,Indefinite Integral for $\cos x/(1+x^2)$,Indefinite Integral for,\cos x/(1+x^2),I have been working on the indefinite integral of $\cos x/(1+x^2)$. $$ \int\frac{\cos x}{1+x^2}\;dx\text{ or } \int\frac{\sin x}{1+x^2}\;dx $$ are they unsolvable(impossible to solve) or is there a way to solve them even by approximation? Thank you very much.,I have been working on the indefinite integral of $\cos x/(1+x^2)$. $$ \int\frac{\cos x}{1+x^2}\;dx\text{ or } \int\frac{\sin x}{1+x^2}\;dx $$ are they unsolvable(impossible to solve) or is there a way to solve them even by approximation? Thank you very much.,,"['analysis', 'indefinite-integrals']"
17,"Show that integral involving $\frac {x^{a}-x^{b}}{(1+x^{a})(1+x^{b})}$ is actually zero for every $(a,b)$",Show that integral involving  is actually zero for every,"\frac {x^{a}-x^{b}}{(1+x^{a})(1+x^{b})} (a,b)","Can one show that, for every $a$ and $b$ in $\mathbb R$, $$ \int_{0}^{\infty} \! \frac {1}{1+x^{2}} \frac {x^{a}-x^{b}}{(1+x^{a})(1+x^{b})}~\mathrm{d}x=0\ ? $$ Any hints?","Can one show that, for every $a$ and $b$ in $\mathbb R$, $$ \int_{0}^{\infty} \! \frac {1}{1+x^{2}} \frac {x^{a}-x^{b}}{(1+x^{a})(1+x^{b})}~\mathrm{d}x=0\ ? $$ Any hints?",,"['analysis', 'definite-integrals']"
18,Which Mathematical Analysis I Book or Textbook Is The Best?,Which Mathematical Analysis I Book or Textbook Is The Best?,,"I'm in search of a mathematical analysis text that covers at least the same material as Walter Rudin's Principles of ... but does so in much more detail, without relegating the important results to the exercises, contrary to what Rudin does. Which one is it, if any? Do the mathematics students at places like the MIT, Harvard, or UC Berkeley, where Rudin is used, cover this textbook fully, solving each and every problem? If not, then how much of it is taught and in what detail? Is there any university where this book is covered fully in their analysis courses? Can I access any video lectures based on Rudin? Is there any TV channel dedicated to higher level mathematics?","I'm in search of a mathematical analysis text that covers at least the same material as Walter Rudin's Principles of ... but does so in much more detail, without relegating the important results to the exercises, contrary to what Rudin does. Which one is it, if any? Do the mathematics students at places like the MIT, Harvard, or UC Berkeley, where Rudin is used, cover this textbook fully, solving each and every problem? If not, then how much of it is taught and in what detail? Is there any university where this book is covered fully in their analysis courses? Can I access any video lectures based on Rudin? Is there any TV channel dedicated to higher level mathematics?",,"['analysis', 'reference-request', 'education']"
19,What is the difference between totally bounded and uniformly bounded?,What is the difference between totally bounded and uniformly bounded?,,Can somebody please explain me what the difference is between totally bounded and uniformly bounded functions?,Can somebody please explain me what the difference is between totally bounded and uniformly bounded functions?,,['analysis']
20,Is there a 'far' irrational number?,Is there a 'far' irrational number?,,"I recently learned of so-called 'far' numbers at a talk. In the talk, it was proven that there is a dense subset of the interval $[0,1]$ of far numbers (however, far numbers were only a minor point of the talk, so we quickly went on to bigger and better things). A number $x$ might be called far if there is a positive constant $c$ s.t. $\displaystyle \left\| \;x - \frac{k}{2^n} \right \| \geq \frac{c}{2^n}$ for all $k, n \in \mathbb{N}$. It is not so bad to see that for any odd prime $p$, $1/p$ is a far number. For that matter, for most rationals, it's not so hard to see that they are far. It happens to be that the set of far numbers has measure $0$. But I began to wonder: Is there a 'far' irrational number?","I recently learned of so-called 'far' numbers at a talk. In the talk, it was proven that there is a dense subset of the interval $[0,1]$ of far numbers (however, far numbers were only a minor point of the talk, so we quickly went on to bigger and better things). A number $x$ might be called far if there is a positive constant $c$ s.t. $\displaystyle \left\| \;x - \frac{k}{2^n} \right \| \geq \frac{c}{2^n}$ for all $k, n \in \mathbb{N}$. It is not so bad to see that for any odd prime $p$, $1/p$ is a far number. For that matter, for most rationals, it's not so hard to see that they are far. It happens to be that the set of far numbers has measure $0$. But I began to wonder: Is there a 'far' irrational number?",,"['number-theory', 'analysis', 'diophantine-approximation']"
21,When is the difference of two convex functions convex?,When is the difference of two convex functions convex?,,"Assume that $X$ is a finite-dimensional Banach space. I know that, in general, if two functions $f, g : X \to \mathbb{R}$ are convex, then the function $(f-g) : X \to \mathbb{R}$ given by $x \mapsto f(x) - g(x)$ is not necessarily convex. Are there conditions we can impose on $f$ and $g$ so that the difference is still convex, e.g., if $f (x) \geq g (x)$ for every $x$ then can we say it's convex? Are there any results about the convexity of the difference of convex functions?","Assume that is a finite-dimensional Banach space. I know that, in general, if two functions are convex, then the function given by is not necessarily convex. Are there conditions we can impose on and so that the difference is still convex, e.g., if for every then can we say it's convex? Are there any results about the convexity of the difference of convex functions?","X f, g : X \to \mathbb{R} (f-g) : X \to \mathbb{R} x \mapsto f(x) - g(x) f g f (x) \geq g (x) x","['analysis', 'convex-analysis']"
22,"Is there an English translation of Jordan's ""Cours D'analyse""","Is there an English translation of Jordan's ""Cours D'analyse""",,"I am trying to find an English translation of Camille Jordan's work ""Cours D'analyse"". Only the French edition is on Amazon, so since this is a somewhat specialized topic, I thought perhaps someone in this forum might know. TIA, Matt","I am trying to find an English translation of Camille Jordan's work ""Cours D'analyse"". Only the French edition is on Amazon, so since this is a somewhat specialized topic, I thought perhaps someone in this forum might know. TIA, Matt",,"['analysis', 'reference-request']"
23,"Convolution intuition: clarifying Terence Tao's ""blurring""/""fuzz"" interpretation","Convolution intuition: clarifying Terence Tao's ""blurring""/""fuzz"" interpretation",,"On this math.MO post, ""What is convolution intuitively?"" , Terence Tao's answer (in the case where one function is a bump function) involves ""blurring"" and ""fuzz."" Could someone clarify his interpretation more explicitly? The intuition still escapes me. Thanks!","On this math.MO post, ""What is convolution intuitively?"" , Terence Tao's answer (in the case where one function is a bump function) involves ""blurring"" and ""fuzz."" Could someone clarify his interpretation more explicitly? The intuition still escapes me. Thanks!",,"['analysis', 'intuition', 'convolution']"
24,Does the set of differences of a Lebesgue measurable set contains elements of at most a certain length?,Does the set of differences of a Lebesgue measurable set contains elements of at most a certain length?,,"I want to show that if $E\subset \mathbb{R}^n$ is a Lebesgue measurable set where $\lambda(E)>0$, then $E-E=\{x-y:x,y\in E\}\supseteq\{z\in\mathbb{R}^n:|z|<\delta\}$ for some $\delta>0$, where $|z|=\sqrt{\sum_{i=1}^n z_i^2}$. My approach is this. Take some $J$, a box in $\mathbb{R}^n$ with equal side lengths such that $\lambda(E\cap J)>3\lambda(J)/4$. Setting $\epsilon=3\lambda(J)/2$, take $x\in\mathbb{R}^n$ such that $|x|\leq\epsilon$. Then $E\cap J\subseteq J$ and  $$((E\cap J)+x)\cup(E\cap J)\subseteq J\cup(J+x).$$ Since Lebesgue measure is translation invariant, it follows that $\lambda((E\cap J)+x)=\lambda(E\cap J)$, and so $((E\cap J)+x)\cap(E\cap J)\neq\emptyset$. If it were empty, then $$2\lambda(E\cap J)=\lambda(((E\cap J)+x)\cup(E\cap J))\leq\lambda(J\cup(J+x))\leq 3\lambda(J)/2,$$ thus $\lambda(E\cap J)\leq 3\lambda(J)/4$, a contradiction. Then $((E\cap J)+x)\cap (E\cap J)\neq\emptyset$, and so $x\in (E\cap J)-(E\cap J)\subseteq E-E$. Thus $E-E$ contains the box of $x$ such that $|x|\leq \epsilon$. Is this valid? If not, can it be fixed? Many thanks.","I want to show that if $E\subset \mathbb{R}^n$ is a Lebesgue measurable set where $\lambda(E)>0$, then $E-E=\{x-y:x,y\in E\}\supseteq\{z\in\mathbb{R}^n:|z|<\delta\}$ for some $\delta>0$, where $|z|=\sqrt{\sum_{i=1}^n z_i^2}$. My approach is this. Take some $J$, a box in $\mathbb{R}^n$ with equal side lengths such that $\lambda(E\cap J)>3\lambda(J)/4$. Setting $\epsilon=3\lambda(J)/2$, take $x\in\mathbb{R}^n$ such that $|x|\leq\epsilon$. Then $E\cap J\subseteq J$ and  $$((E\cap J)+x)\cup(E\cap J)\subseteq J\cup(J+x).$$ Since Lebesgue measure is translation invariant, it follows that $\lambda((E\cap J)+x)=\lambda(E\cap J)$, and so $((E\cap J)+x)\cap(E\cap J)\neq\emptyset$. If it were empty, then $$2\lambda(E\cap J)=\lambda(((E\cap J)+x)\cup(E\cap J))\leq\lambda(J\cup(J+x))\leq 3\lambda(J)/2,$$ thus $\lambda(E\cap J)\leq 3\lambda(J)/4$, a contradiction. Then $((E\cap J)+x)\cap (E\cap J)\neq\emptyset$, and so $x\in (E\cap J)-(E\cap J)\subseteq E-E$. Thus $E-E$ contains the box of $x$ such that $|x|\leq \epsilon$. Is this valid? If not, can it be fixed? Many thanks.",,"['analysis', 'measure-theory']"
25,A function and its Fourier transform cannot both be compactly supported,A function and its Fourier transform cannot both be compactly supported,,"I am stuck on the following problem from Stein and Shakarchi's third book. I can't figure out how to use the hint productively. Once I know $f$ is a trigonometric polynomial, I see how to finish the problem, but I don't know how to conclude that $f$ is a trigonometric polynomial. I tried substituting in the Fourier transform in the formula for Fourier coefficients and switching the order of integration, but I couldn't get that to work. I can't think of any more ideas. Problem: Suppose $f$ is continuous on $\mathbb{R}$. Prove $f$ and $\hat f$ cannot both be compactly supported unless $f=0$. Hint: Suppose $f$ is supported in $[0, 1/2]$ and expand it in a Fourier series in $[0,1]$. Show $f$ must be a trigonometric polynomial. This question was asked before, but with different hypotheses and in the context of complex analysis. Please do not close as a duplicate.","I am stuck on the following problem from Stein and Shakarchi's third book. I can't figure out how to use the hint productively. Once I know $f$ is a trigonometric polynomial, I see how to finish the problem, but I don't know how to conclude that $f$ is a trigonometric polynomial. I tried substituting in the Fourier transform in the formula for Fourier coefficients and switching the order of integration, but I couldn't get that to work. I can't think of any more ideas. Problem: Suppose $f$ is continuous on $\mathbb{R}$. Prove $f$ and $\hat f$ cannot both be compactly supported unless $f=0$. Hint: Suppose $f$ is supported in $[0, 1/2]$ and expand it in a Fourier series in $[0,1]$. Show $f$ must be a trigonometric polynomial. This question was asked before, but with different hypotheses and in the context of complex analysis. Please do not close as a duplicate.",,"['analysis', 'fourier-analysis', 'fourier-series']"
26,Asymptotics of sum of binomials,Asymptotics of sum of binomials,,"How can you compute the asymptotics of $$S=n  + m - \sum_{k=1}^{n} k^{k-1} \binom{n}{k} \frac{(n-k)^{n+m-k}}{n^{n+m-1}}\;?$$ We have that $n \geq m$ and $n,m \geq 1$. A simple application of Stirling's approximation gives $$S \approx T =  n + m - \frac{n^{3/2-m}}{\sqrt{2\pi}} \sum_{k=1}^n \frac{(n-k)^{m-1/2}}{k^{3/2}}$$ A more accurate approximation is given by $$n+m- \frac{\left(1+\frac{1}{12 n}\right) n }{\sqrt{2 \pi }} \sum _{k=1}^{n-1} \frac{ (1-\frac{k}{n})^{m-\frac{1}{2}}}{\left(1+\frac{1}{12 k}\right) k^{3/2} \left(1+\frac{1}{12 (n-k)}\right) }$$ Via an indirect and handy wavy argument, my guess is guess for constant $m$ is that the answer is $$S \sim \sqrt{2n} \frac{\Gamma(m+\frac{1}{2})}{(m-1)!}$$ Update. When $m$ grows almost as quickly as $n$ I think my guess is an underestimate.  For example when $n=m$ it seems numerically that $S \sim 1.841 n$ and in fact if $n=m$ then it is suggested that $S \sim n\left(2-\left( -W\left(-\frac{1}{\mathrm{e}^2}\right)\right)\right)$ (see Is $\sum_{k=1}^{n} k^{k-1} (n-k)^{2n-k} \binom{n}{k} \sim\frac{n^{2n}}{2\pi} $? ). Update 2. When $m=1$ then $S$ is precisely the average number of people required to find a pair with the same birthday.  This is solved at the wikipedia entry for the Birthday Problem and so $S \sim \sqrt{\frac{\pi n}{2}}$ (which equals  my guess above).  I would however ideally like to find the asymptotics in terms of $m$ and $n$ without assuming that $m$ is fixed. Update 3. For the $m=1$ case we can prove that the correct asymptotics is $\sqrt{\frac{\pi n}{2}}$ in two ways. We will first show the result using the following identity. $$n  + 1 - \sum_{k=1}^{n} k^{k-1} \binom{n}{k} \frac{(n-k)^{n+1-k}}{n^{n+1-1}}=1 + \sum_{k=1}^n \frac{n!}{(n-k)!n^k} = 1+Q(n)$$ The numerator of the sum on the left is $n^{n+1}-Q(n)n^n$ (Q(n) is called Ramanujan's function by Knuth) according to A219706 and A063169 .  This immediately gives the identity. We also know that $Q(n) \sim \sqrt{\frac{\pi n}{2}}$ from the wikipedia (is there a better reference?). The second proof follows from the amazing answer of GEdgar where he shows that $$\sum_{k=1}^n \binom{n}{k} k^{k-1}(n-k)^{n-k+1} = 	n^n\Bigg( n 	-\frac{\sqrt{2\pi}}{2} n^{1/2} + \frac{1}{3} 	-\frac{\sqrt{2\pi}}{24} n^{-1/2} 	+\frac{4}{135}n^{-1} 	-\frac{\sqrt{2\pi}}{576}n^{-3/2} 	+O\left(n^{-2}\right)\Bigg) $$","How can you compute the asymptotics of $$S=n  + m - \sum_{k=1}^{n} k^{k-1} \binom{n}{k} \frac{(n-k)^{n+m-k}}{n^{n+m-1}}\;?$$ We have that $n \geq m$ and $n,m \geq 1$. A simple application of Stirling's approximation gives $$S \approx T =  n + m - \frac{n^{3/2-m}}{\sqrt{2\pi}} \sum_{k=1}^n \frac{(n-k)^{m-1/2}}{k^{3/2}}$$ A more accurate approximation is given by $$n+m- \frac{\left(1+\frac{1}{12 n}\right) n }{\sqrt{2 \pi }} \sum _{k=1}^{n-1} \frac{ (1-\frac{k}{n})^{m-\frac{1}{2}}}{\left(1+\frac{1}{12 k}\right) k^{3/2} \left(1+\frac{1}{12 (n-k)}\right) }$$ Via an indirect and handy wavy argument, my guess is guess for constant $m$ is that the answer is $$S \sim \sqrt{2n} \frac{\Gamma(m+\frac{1}{2})}{(m-1)!}$$ Update. When $m$ grows almost as quickly as $n$ I think my guess is an underestimate.  For example when $n=m$ it seems numerically that $S \sim 1.841 n$ and in fact if $n=m$ then it is suggested that $S \sim n\left(2-\left( -W\left(-\frac{1}{\mathrm{e}^2}\right)\right)\right)$ (see Is $\sum_{k=1}^{n} k^{k-1} (n-k)^{2n-k} \binom{n}{k} \sim\frac{n^{2n}}{2\pi} $? ). Update 2. When $m=1$ then $S$ is precisely the average number of people required to find a pair with the same birthday.  This is solved at the wikipedia entry for the Birthday Problem and so $S \sim \sqrt{\frac{\pi n}{2}}$ (which equals  my guess above).  I would however ideally like to find the asymptotics in terms of $m$ and $n$ without assuming that $m$ is fixed. Update 3. For the $m=1$ case we can prove that the correct asymptotics is $\sqrt{\frac{\pi n}{2}}$ in two ways. We will first show the result using the following identity. $$n  + 1 - \sum_{k=1}^{n} k^{k-1} \binom{n}{k} \frac{(n-k)^{n+1-k}}{n^{n+1-1}}=1 + \sum_{k=1}^n \frac{n!}{(n-k)!n^k} = 1+Q(n)$$ The numerator of the sum on the left is $n^{n+1}-Q(n)n^n$ (Q(n) is called Ramanujan's function by Knuth) according to A219706 and A063169 .  This immediately gives the identity. We also know that $Q(n) \sim \sqrt{\frac{\pi n}{2}}$ from the wikipedia (is there a better reference?). The second proof follows from the amazing answer of GEdgar where he shows that $$\sum_{k=1}^n \binom{n}{k} k^{k-1}(n-k)^{n-k+1} = 	n^n\Bigg( n 	-\frac{\sqrt{2\pi}}{2} n^{1/2} + \frac{1}{3} 	-\frac{\sqrt{2\pi}}{24} n^{-1/2} 	+\frac{4}{135}n^{-1} 	-\frac{\sqrt{2\pi}}{576}n^{-3/2} 	+O\left(n^{-2}\right)\Bigg) $$",,['analysis']
27,Weakly Harmonic Functions (Weak Solutions to Laplace's Equation $\Delta u=0$) and Logic of Test Function Techniques.,Weakly Harmonic Functions (Weak Solutions to Laplace's Equation ) and Logic of Test Function Techniques.,\Delta u=0,"In analysis we often use test functions $\phi\in C_{0}^{\infty}(U)$ in order to make some kind of deduction about another function $u:U\mapsto\mathbb{C}$. For example, if one can obtain the conclusion $\int_{U}u\phi\;dx=0$ for every $\phi$, then we can conclude $u\equiv0$ in $U$. I have a couple of questions though.  Integration by parts very often comes up in situations involving test functions, and the fact that they are compactly supported allows you to throw out the boundary integrals.  I can see why this is justified when the set $U$ is open, but if $U$ is closed or compact, can we use test functions in the same way?  Or do we really need to restrict ourselves to open domains? My other question has to do with an exercise I'm working on.  It asks us to define a weakly harmonic function as one which satisfies $$\int_{U}u\Delta\phi\;dx=0,$$ and asks us to prove that this condition is equivalent to being harmonic: $\Delta u=0$.  Actually, we are allowed to assume $u$ is continuous in $U$, even though the result is true for more general $u\in L^{1}_{loc}(U)$.  But after proving this, it struck me that the above logic that I've used in various other instances seems contradictory to the present situation.  Applying that logic to the weakly-harmonic condition should imply that $u=0$, since $\Delta\phi$ is more or less an arbitrary function anyway, right? How do you resolve this? With the above questions resolved by Jose's answer (see below), I figured I would go ahead and post a proof to the claim that weak solutions to Laplace's equation are indeed classical solutions, at least under the hypothesis that the weak solution is $\mathscr{C}^{2}(\bar{U})$, and later merely $\mathscr{C}(U)$.  Of course, much of what is presented in the proof generalizes.  Indeed, the first part of the proof is to show that the ''standard'' mollification of a weak harmonic solution is in fact harmonic; by using the concept of an adjoint, it is not difficult to extend my argument to general linear PDE, provided certain conditions are satisfied.  Of course, what follows from there is unique to Laplace's equation $\Delta u=0$; in particular due to the standard mollifier $$\eta_{\epsilon}=\frac{C}{\epsilon^{n}}e^{\frac{1}{|x|^{2}-1}}$$ being radial symmetric and harmonic functions satisfying the important mean-value properties.  Going one step further, one can easily deduce the theorem for arbitrary weak solutions in $L^{1}(\mathbb{R}^{n},\mathscr{L},\mu)$, at least after redefining such a function on a set of measure zero. (Note that any arcane reference's to ""appendix,"" ""text,"" etc. refers to Evan's PDE text, though this particular theorem does not appear in the text; not even as an exercise.) Proof. Let's first assume $u\in\mathscr{C}^{2}(\bar{U}).$  Then integration by parts (or Green's identity) shows \begin{align*} 0 &=\int\limits_{U}u\Delta\phi\;dx\\ &=\int\limits_{\partial U}u\frac{\partial\phi}{\partial\nu}\;dS-\int\limits_{U}\nabla u\cdot\nabla\phi\;dx\\ &=-\int\limits_{\partial U}\phi\frac{\partial u}{\partial\nu}\;dS+\int\limits_{U}\phi\Delta u\;dx\\ &=\int\limits_{U}\phi\Delta u\;dx. \end{align*} Conversely, it is easy to see from the above that if $u$ is harmonic in $U$, then $u$ is also weakly harmonic.  Thus, being a weak solution to $\Delta u=0$ is equivalent to being a classical solution, at least when $u\in\mathscr{C}^{2}(\bar{U}).$  We now strengthen the assertion by assuming only $u\in\mathscr{C}(U)$ and mollifying $u$ (with the standard radial mollifier $\eta_{\epsilon}$ as defined in the appendix) in order to work again with a sufficiently smooth function.  To that end, let $\epsilon>0$ be given and define the set $$U_{\epsilon}:=\{x\in U\;:\;\text{dist}(x,\partial U)>\epsilon\}.$$ (The set $U_{\epsilon}$ is obtained by cutting out a wedge of width $\epsilon$ along the perimeter of $U$).  Since the function $\eta_{\epsilon}(y)$ is supported on $B(0,\epsilon)$, we find that for any $x\in U_{\epsilon}$ fixed, $\eta_{\epsilon}(x-y)$ is compactly supported in $U$ (in particular, on $B(x,\epsilon)$) and as such (as a function of $y$), $$\psi(y):=\eta_{\epsilon}(x-y)\in\mathscr{C}^{\infty}_{0}(U).$$  For fixed $x\in U_{\epsilon}$, we then compute \begin{align} \Delta u_{\epsilon}(x) &=\Delta_{x}(\eta_{\epsilon}\star u)\\ &=\Delta_{x}\int\limits_{\mathbb{R}^{n}}\eta_{\epsilon}(x-y)u(y)\;dy\\ &=\Delta_{x}\int\limits_{U}\eta_{\epsilon}(x-y)u(y)\;dy\\ &=\int\limits_{U}\Delta_{x}\Big[\eta_{\epsilon}(x-y)\Big]u(y)\;dy\\ &=\int\limits_{U}(-1)^{2}\Delta_{y}\Big[\eta_{\epsilon}(x-y)\Big]u(y)\;dy\\ &=\int\limits_{U}u\Delta\psi\;dy\\ &=0. \end{align} Line 3 follows from $\text{supp}\{\eta_{\epsilon}(x-y)\}\subset U$ as discussed above.  Differentiation under the integral sign in line 4 is justified by continuity of both $u$ and continuous differentiability of $\eta_{\epsilon}(x-y)$ as a function of $x$.  Line 5 follows from the chain rule, and finally line 7 follows from $u$ being a weak solution to $\Delta u=0$ and the previous remarks.  We have just proved that the mollified version of the weak solution to $\Delta w=0$ is a classical solution in the restricted domain $U_{\epsilon}$: $$\int\limits_{U}u\Delta\phi\;dx=0\;\forall\;\phi\in\mathscr{C}^{\infty}_{0}(U)\Longrightarrow \Delta u_{\epsilon}=0\;\text{in}\;U_{\epsilon}$$ for all $\epsilon>0$.  We now show that this fact actually implies $\Delta u=0$ in $U$. Now choose $\epsilon'>0$ and put $$u_{\epsilon\epsilon'}:=\eta_{\epsilon'}\star u_{\epsilon}.$$  Note that $\eta_{\epsilon'}(x-y)$ is supported on $U_{\epsilon}$ whenever $x\in U_{\epsilon+\epsilon'}$; in particular, for a fixed $x\in U_{\epsilon+\epsilon'}$, $\eta_{\epsilon'}(x-y)$ is supported on $B(x,\epsilon')$.  Since $\eta_{\epsilon'}$ is radial and $u_{\epsilon}$ is harmonic in $U_{\epsilon+\epsilon'}$ (and thus satisfies the mean-value properties there), for such $x$ fixed we compute in polar coordinates (as in Theorem 6 in the text) \begin{align*} u_{\epsilon\epsilon'}(x) &=\int\limits_{U_{\epsilon}}\eta_{\epsilon'}(x-y)u_{\epsilon}(y)\;dy\\ &=\frac{1}{\epsilon'^{n}}\int\limits_{B(x,\epsilon')}\eta\left(\frac{|x-y|}{\epsilon'}\right)u_{\epsilon}(y)\;dy\\ &=\frac{1}{\epsilon'^{n}}\int\limits_{0}^{\epsilon'}\eta\left(\frac{r}{\epsilon'}\right)\left(\int\limits_{\partial B(x,r)}u_{\epsilon}(y)\;dS(y)\right)dr\\ &=\frac{u_{\epsilon}(x)n\alpha(n)}{\epsilon'^{n}}\int\limits_{0}^{\epsilon'}r^{n-1}\eta\left(\frac{r}{\epsilon'}\right)\;dr\\ &=\frac{u_{\epsilon}(x)}{\epsilon'^{n}}\int\limits_{B(x,\epsilon)}\eta\left(\frac{x-y}{\epsilon'}\right)\;dy\\ &=u_{\epsilon}(x)\int\limits_{B(0,\epsilon)}\eta_{\epsilon}(y)\;dy\\ &=u_{\epsilon}(x). \end{align*} Since convolution is associative, we find that for $x\in U_{\epsilon+\epsilon'}$, $$u_{\epsilon'\epsilon}(x)=u_{\epsilon\epsilon'}(x)=u_{\epsilon}(x)\to u(x)\;\text{uniformly as}\;\epsilon\to0,$$ or $$u(x)=u_{\epsilon'}(x).$$  From what was proven above, $u_{\epsilon'}$ is harmonic in $U_{\epsilon'}$, and therefore so is $u(x)$.  Sending $\epsilon'\to0$, we obtain the result for all $x\in U$, and conclude that weak solutions to Laplace's equation are in fact smooth solutions in the classical sense. QED (Here is an alternative argument to finish off the proof that $\Delta u=0$ in $U$, though it is not quite rigorous and may actually be incorrect): Now, $\Delta u_{\epsilon}\to0$ uniformly as $\epsilon\to0$ in any compact subset of $U$, and therefore, $\nabla u_{\epsilon}$ and $\nabla^{2}u_{\epsilon}$ must both converge uniformly to some limit in the same compact subset.  Consequently, since $u_{\epsilon}\to u$ pointwise for at least one $x\in U$ (in fact, uniformly for all $x$ in any $V\subset\subset U$), we find $\Delta u$ exists, and $$\Delta u=\lim_{\epsilon\to0^{+}}\Delta u_{\epsilon}=0$$ as required.","In analysis we often use test functions $\phi\in C_{0}^{\infty}(U)$ in order to make some kind of deduction about another function $u:U\mapsto\mathbb{C}$. For example, if one can obtain the conclusion $\int_{U}u\phi\;dx=0$ for every $\phi$, then we can conclude $u\equiv0$ in $U$. I have a couple of questions though.  Integration by parts very often comes up in situations involving test functions, and the fact that they are compactly supported allows you to throw out the boundary integrals.  I can see why this is justified when the set $U$ is open, but if $U$ is closed or compact, can we use test functions in the same way?  Or do we really need to restrict ourselves to open domains? My other question has to do with an exercise I'm working on.  It asks us to define a weakly harmonic function as one which satisfies $$\int_{U}u\Delta\phi\;dx=0,$$ and asks us to prove that this condition is equivalent to being harmonic: $\Delta u=0$.  Actually, we are allowed to assume $u$ is continuous in $U$, even though the result is true for more general $u\in L^{1}_{loc}(U)$.  But after proving this, it struck me that the above logic that I've used in various other instances seems contradictory to the present situation.  Applying that logic to the weakly-harmonic condition should imply that $u=0$, since $\Delta\phi$ is more or less an arbitrary function anyway, right? How do you resolve this? With the above questions resolved by Jose's answer (see below), I figured I would go ahead and post a proof to the claim that weak solutions to Laplace's equation are indeed classical solutions, at least under the hypothesis that the weak solution is $\mathscr{C}^{2}(\bar{U})$, and later merely $\mathscr{C}(U)$.  Of course, much of what is presented in the proof generalizes.  Indeed, the first part of the proof is to show that the ''standard'' mollification of a weak harmonic solution is in fact harmonic; by using the concept of an adjoint, it is not difficult to extend my argument to general linear PDE, provided certain conditions are satisfied.  Of course, what follows from there is unique to Laplace's equation $\Delta u=0$; in particular due to the standard mollifier $$\eta_{\epsilon}=\frac{C}{\epsilon^{n}}e^{\frac{1}{|x|^{2}-1}}$$ being radial symmetric and harmonic functions satisfying the important mean-value properties.  Going one step further, one can easily deduce the theorem for arbitrary weak solutions in $L^{1}(\mathbb{R}^{n},\mathscr{L},\mu)$, at least after redefining such a function on a set of measure zero. (Note that any arcane reference's to ""appendix,"" ""text,"" etc. refers to Evan's PDE text, though this particular theorem does not appear in the text; not even as an exercise.) Proof. Let's first assume $u\in\mathscr{C}^{2}(\bar{U}).$  Then integration by parts (or Green's identity) shows \begin{align*} 0 &=\int\limits_{U}u\Delta\phi\;dx\\ &=\int\limits_{\partial U}u\frac{\partial\phi}{\partial\nu}\;dS-\int\limits_{U}\nabla u\cdot\nabla\phi\;dx\\ &=-\int\limits_{\partial U}\phi\frac{\partial u}{\partial\nu}\;dS+\int\limits_{U}\phi\Delta u\;dx\\ &=\int\limits_{U}\phi\Delta u\;dx. \end{align*} Conversely, it is easy to see from the above that if $u$ is harmonic in $U$, then $u$ is also weakly harmonic.  Thus, being a weak solution to $\Delta u=0$ is equivalent to being a classical solution, at least when $u\in\mathscr{C}^{2}(\bar{U}).$  We now strengthen the assertion by assuming only $u\in\mathscr{C}(U)$ and mollifying $u$ (with the standard radial mollifier $\eta_{\epsilon}$ as defined in the appendix) in order to work again with a sufficiently smooth function.  To that end, let $\epsilon>0$ be given and define the set $$U_{\epsilon}:=\{x\in U\;:\;\text{dist}(x,\partial U)>\epsilon\}.$$ (The set $U_{\epsilon}$ is obtained by cutting out a wedge of width $\epsilon$ along the perimeter of $U$).  Since the function $\eta_{\epsilon}(y)$ is supported on $B(0,\epsilon)$, we find that for any $x\in U_{\epsilon}$ fixed, $\eta_{\epsilon}(x-y)$ is compactly supported in $U$ (in particular, on $B(x,\epsilon)$) and as such (as a function of $y$), $$\psi(y):=\eta_{\epsilon}(x-y)\in\mathscr{C}^{\infty}_{0}(U).$$  For fixed $x\in U_{\epsilon}$, we then compute \begin{align} \Delta u_{\epsilon}(x) &=\Delta_{x}(\eta_{\epsilon}\star u)\\ &=\Delta_{x}\int\limits_{\mathbb{R}^{n}}\eta_{\epsilon}(x-y)u(y)\;dy\\ &=\Delta_{x}\int\limits_{U}\eta_{\epsilon}(x-y)u(y)\;dy\\ &=\int\limits_{U}\Delta_{x}\Big[\eta_{\epsilon}(x-y)\Big]u(y)\;dy\\ &=\int\limits_{U}(-1)^{2}\Delta_{y}\Big[\eta_{\epsilon}(x-y)\Big]u(y)\;dy\\ &=\int\limits_{U}u\Delta\psi\;dy\\ &=0. \end{align} Line 3 follows from $\text{supp}\{\eta_{\epsilon}(x-y)\}\subset U$ as discussed above.  Differentiation under the integral sign in line 4 is justified by continuity of both $u$ and continuous differentiability of $\eta_{\epsilon}(x-y)$ as a function of $x$.  Line 5 follows from the chain rule, and finally line 7 follows from $u$ being a weak solution to $\Delta u=0$ and the previous remarks.  We have just proved that the mollified version of the weak solution to $\Delta w=0$ is a classical solution in the restricted domain $U_{\epsilon}$: $$\int\limits_{U}u\Delta\phi\;dx=0\;\forall\;\phi\in\mathscr{C}^{\infty}_{0}(U)\Longrightarrow \Delta u_{\epsilon}=0\;\text{in}\;U_{\epsilon}$$ for all $\epsilon>0$.  We now show that this fact actually implies $\Delta u=0$ in $U$. Now choose $\epsilon'>0$ and put $$u_{\epsilon\epsilon'}:=\eta_{\epsilon'}\star u_{\epsilon}.$$  Note that $\eta_{\epsilon'}(x-y)$ is supported on $U_{\epsilon}$ whenever $x\in U_{\epsilon+\epsilon'}$; in particular, for a fixed $x\in U_{\epsilon+\epsilon'}$, $\eta_{\epsilon'}(x-y)$ is supported on $B(x,\epsilon')$.  Since $\eta_{\epsilon'}$ is radial and $u_{\epsilon}$ is harmonic in $U_{\epsilon+\epsilon'}$ (and thus satisfies the mean-value properties there), for such $x$ fixed we compute in polar coordinates (as in Theorem 6 in the text) \begin{align*} u_{\epsilon\epsilon'}(x) &=\int\limits_{U_{\epsilon}}\eta_{\epsilon'}(x-y)u_{\epsilon}(y)\;dy\\ &=\frac{1}{\epsilon'^{n}}\int\limits_{B(x,\epsilon')}\eta\left(\frac{|x-y|}{\epsilon'}\right)u_{\epsilon}(y)\;dy\\ &=\frac{1}{\epsilon'^{n}}\int\limits_{0}^{\epsilon'}\eta\left(\frac{r}{\epsilon'}\right)\left(\int\limits_{\partial B(x,r)}u_{\epsilon}(y)\;dS(y)\right)dr\\ &=\frac{u_{\epsilon}(x)n\alpha(n)}{\epsilon'^{n}}\int\limits_{0}^{\epsilon'}r^{n-1}\eta\left(\frac{r}{\epsilon'}\right)\;dr\\ &=\frac{u_{\epsilon}(x)}{\epsilon'^{n}}\int\limits_{B(x,\epsilon)}\eta\left(\frac{x-y}{\epsilon'}\right)\;dy\\ &=u_{\epsilon}(x)\int\limits_{B(0,\epsilon)}\eta_{\epsilon}(y)\;dy\\ &=u_{\epsilon}(x). \end{align*} Since convolution is associative, we find that for $x\in U_{\epsilon+\epsilon'}$, $$u_{\epsilon'\epsilon}(x)=u_{\epsilon\epsilon'}(x)=u_{\epsilon}(x)\to u(x)\;\text{uniformly as}\;\epsilon\to0,$$ or $$u(x)=u_{\epsilon'}(x).$$  From what was proven above, $u_{\epsilon'}$ is harmonic in $U_{\epsilon'}$, and therefore so is $u(x)$.  Sending $\epsilon'\to0$, we obtain the result for all $x\in U$, and conclude that weak solutions to Laplace's equation are in fact smooth solutions in the classical sense. QED (Here is an alternative argument to finish off the proof that $\Delta u=0$ in $U$, though it is not quite rigorous and may actually be incorrect): Now, $\Delta u_{\epsilon}\to0$ uniformly as $\epsilon\to0$ in any compact subset of $U$, and therefore, $\nabla u_{\epsilon}$ and $\nabla^{2}u_{\epsilon}$ must both converge uniformly to some limit in the same compact subset.  Consequently, since $u_{\epsilon}\to u$ pointwise for at least one $x\in U$ (in fact, uniformly for all $x$ in any $V\subset\subset U$), we find $\Delta u$ exists, and $$\Delta u=\lim_{\epsilon\to0^{+}}\Delta u_{\epsilon}=0$$ as required.",,"['analysis', 'partial-differential-equations', 'harmonic-functions']"
28,Intersection between orthogonal complement of a subspace and a set,Intersection between orthogonal complement of a subspace and a set,,"Consider the normed vector space $E=\mathbb{R}^n$. Define $ P=\{x \in \mathbb{R}^n: x_i \geq 0, \forall i \}$. Let $M$ be a subspace such that $M \cap P = \{0\}$. I want to see that $M^\perp \cap {\rm Int}(P) \neq \emptyset $. This seems obvious geometrically, any idea how a short proof would look like?","Consider the normed vector space $E=\mathbb{R}^n$. Define $ P=\{x \in \mathbb{R}^n: x_i \geq 0, \forall i \}$. Let $M$ be a subspace such that $M \cap P = \{0\}$. I want to see that $M^\perp \cap {\rm Int}(P) \neq \emptyset $. This seems obvious geometrically, any idea how a short proof would look like?",,"['analysis', 'hilbert-spaces', 'normed-spaces']"
29,Can the set of computable numbers be used as a theoretical basis for calculus?,Can the set of computable numbers be used as a theoretical basis for calculus?,,"I recall from my Real Analysis course that the rational numbers $\mathbb{Q}$ are not suitable for doing calculus, and I believe the reason was that $\mathbb{Q}$ does not possess the least-upper-bound property, which creates problems when defining continuity and computing limits of Riemann sums.  (This is discussed in more detail here: Importance of Least Upper Bound Property of $\mathbb{R}$ ) So, $\mathbb{R}$ has some desirable properties.  On the other hand, I think it also has some undesirable properties.  For example: $\mathbb{R}$ is uncountably infinite, which means we could never enumerate all real numbers, even given infinite time and space (as demonstrated by Cantor's diagonal argument) In contrast, the set of computable numbers is countably infinite.  This means that the vast majority of real numbers cannot be described in any algorithmic way.   As a consequence, it is impossible to represent the vast majority of real numbers in a computer algebra system, even given an unlimited amount of working memory. This seems like a philosophical problem for calculus, and I'd be interested to know whether anyone has found a viable alternative to use in place of $\mathbb{R}$. In particular, if we use the set of computable numbers instead of $\mathbb{R}$, can we define differential and integral calculus sensibly and avoid the limitations of $\mathbb{R}$ mentioned above? (After all, the ""important"" irrational numbers, including $e$ and $\pi$ and the algebraic irrationals, are all included in the computable numbers, so I doubt we'd miss out on any number we've ever heard of.) Note: The Wikipedia article on computable numbers (cited above) discusses this somewhat, but the details (as of this writing) are a bit sparse.  It mentions ""computable analysis"", a ""constructivist"" branch of mathematics that attempts to use computable numbers instead of $\mathbb{R}$. The article says: The computable numbers form a real closed field and can be used in the place of real numbers for many, but not all, mathematical purposes. but it does not say precisely what the computable numbers can or cannot be used for, especially regarding calculus. Also, I realize that this question is entirely moot in practice, since finite computer memory means that we can only use computers to manipulate a finite set of numbers (ditto regarding our finite brains).  But it still seems interesting from a theoretical perspective.","I recall from my Real Analysis course that the rational numbers $\mathbb{Q}$ are not suitable for doing calculus, and I believe the reason was that $\mathbb{Q}$ does not possess the least-upper-bound property, which creates problems when defining continuity and computing limits of Riemann sums.  (This is discussed in more detail here: Importance of Least Upper Bound Property of $\mathbb{R}$ ) So, $\mathbb{R}$ has some desirable properties.  On the other hand, I think it also has some undesirable properties.  For example: $\mathbb{R}$ is uncountably infinite, which means we could never enumerate all real numbers, even given infinite time and space (as demonstrated by Cantor's diagonal argument) In contrast, the set of computable numbers is countably infinite.  This means that the vast majority of real numbers cannot be described in any algorithmic way.   As a consequence, it is impossible to represent the vast majority of real numbers in a computer algebra system, even given an unlimited amount of working memory. This seems like a philosophical problem for calculus, and I'd be interested to know whether anyone has found a viable alternative to use in place of $\mathbb{R}$. In particular, if we use the set of computable numbers instead of $\mathbb{R}$, can we define differential and integral calculus sensibly and avoid the limitations of $\mathbb{R}$ mentioned above? (After all, the ""important"" irrational numbers, including $e$ and $\pi$ and the algebraic irrationals, are all included in the computable numbers, so I doubt we'd miss out on any number we've ever heard of.) Note: The Wikipedia article on computable numbers (cited above) discusses this somewhat, but the details (as of this writing) are a bit sparse.  It mentions ""computable analysis"", a ""constructivist"" branch of mathematics that attempts to use computable numbers instead of $\mathbb{R}$. The article says: The computable numbers form a real closed field and can be used in the place of real numbers for many, but not all, mathematical purposes. but it does not say precisely what the computable numbers can or cannot be used for, especially regarding calculus. Also, I realize that this question is entirely moot in practice, since finite computer memory means that we can only use computers to manipulate a finite set of numbers (ditto regarding our finite brains).  But it still seems interesting from a theoretical perspective.",,"['analysis', 'philosophy']"
30,Application de Stone-Weierstrass,Application de Stone-Weierstrass,,"Bonjour, J'ai rencontré le problème suivant dans le livre ""Real and Functional Analysis"" de Lang, au chapitre $3$. J'explique d'abord le contexte, puis j'en viendrai à la question précise. Il faut démontrer que les fonctions de la forme $e^{-x}p(x)$, où $p$ est un polynôme, sont denses dans l'ensemble des fonctions continues sur $[0, +\infty[$ qui tendent vers zéro en $+\infty$, muni de la norme sup. Pour cela, on applique Stone-Weierstrass au complété $[0,+\infty]$, en considérant l'algèbre des fonctions de la forme $\sum_{n=1}^{N}{e^{-nx}p_n(x)}$ définies sur le complété (auxquelles on doit rajouter aussi des constantes). Ensuite, il faut montrer qu'on peut approcher uniformément les fonctions $e^{-nx}p(x)$ par des fonctions de la forme $e^{-x}q(x)$. Lang suggère d'approcher d'abord $e^{-2x}$ par des fonctions $e^{-x}q(x)$ à l'aide de la formule de Taylor avec reste, puis $e^{-nx}p(x)$ en général. Je crois avoir réussi la première partie, et la deuxième partie en découle assez facilement pour $n \geq 3$. Par contre, je n'arrive pas à traiter le cas $n = 2$. Alors ma question est la suivante: Pour $m \in \mathbf{N}$, peut-on approcher uniformément $x^m e^{-2x}$ sur $[0,+\infty[$ par des fonctions de la forme $e^{-x}q(x)$, où $q$ est un polynôme? J'apprécierais beaucoup une référence ou une démonstration (ou les deux). (Vous pouvez répondre dans une autre langue.) [Mod: attempt at translation below] Hello, I've encountered the following problem in the book ""Real and Functional Analysis"" by Lang, in chapter 3. I'll first explain the context, after which I'll pose my precise question. We want to prove that the functions of the form $e^{-x}p(x)$, where $p$ is a polynomial, are dense in the set of continuous functions defined on $[0,\infty)$ which tends to 0 at $+\infty$, equipped with the sup norm. For this, we apply Stone-Weierstrass on the completion $[0,+\infty]$, and consider the algebra generated from the functions of the form $\sum_{n=1}^N e^{-nx}p_n(x)$ (to which we add also the constant functions). Next, we need to show that we can uniformly approximate the functions $e^{-nx}p(x)$ by functions of the form $e^{-x}q(x)$. Lang suggests to first approximate $e^{-2x}$ by functions of the form $e^{-x}q(x)$ using Taylor's theorem with remainders. Then consider $e^{-nx}p(x)$ in general. I think I know how to do the first step. For the second step, the $n\geq 3$ cases are easy. On the other hand, I don't know how to treat the case $n=2$. So here's my question: can we approximate a function of the form $x^me^{-2x}$, where $m\in\mathbb{N}$, uniformly over $[0,\infty)$, by functions of the form $e^{-x}q(x)$, where $q$ is a polynomial? I would appreciate a reference or a proof (or both). (You can post your answer in another language.)","Bonjour, J'ai rencontré le problème suivant dans le livre ""Real and Functional Analysis"" de Lang, au chapitre $3$. J'explique d'abord le contexte, puis j'en viendrai à la question précise. Il faut démontrer que les fonctions de la forme $e^{-x}p(x)$, où $p$ est un polynôme, sont denses dans l'ensemble des fonctions continues sur $[0, +\infty[$ qui tendent vers zéro en $+\infty$, muni de la norme sup. Pour cela, on applique Stone-Weierstrass au complété $[0,+\infty]$, en considérant l'algèbre des fonctions de la forme $\sum_{n=1}^{N}{e^{-nx}p_n(x)}$ définies sur le complété (auxquelles on doit rajouter aussi des constantes). Ensuite, il faut montrer qu'on peut approcher uniformément les fonctions $e^{-nx}p(x)$ par des fonctions de la forme $e^{-x}q(x)$. Lang suggère d'approcher d'abord $e^{-2x}$ par des fonctions $e^{-x}q(x)$ à l'aide de la formule de Taylor avec reste, puis $e^{-nx}p(x)$ en général. Je crois avoir réussi la première partie, et la deuxième partie en découle assez facilement pour $n \geq 3$. Par contre, je n'arrive pas à traiter le cas $n = 2$. Alors ma question est la suivante: Pour $m \in \mathbf{N}$, peut-on approcher uniformément $x^m e^{-2x}$ sur $[0,+\infty[$ par des fonctions de la forme $e^{-x}q(x)$, où $q$ est un polynôme? J'apprécierais beaucoup une référence ou une démonstration (ou les deux). (Vous pouvez répondre dans une autre langue.) [Mod: attempt at translation below] Hello, I've encountered the following problem in the book ""Real and Functional Analysis"" by Lang, in chapter 3. I'll first explain the context, after which I'll pose my precise question. We want to prove that the functions of the form $e^{-x}p(x)$, where $p$ is a polynomial, are dense in the set of continuous functions defined on $[0,\infty)$ which tends to 0 at $+\infty$, equipped with the sup norm. For this, we apply Stone-Weierstrass on the completion $[0,+\infty]$, and consider the algebra generated from the functions of the form $\sum_{n=1}^N e^{-nx}p_n(x)$ (to which we add also the constant functions). Next, we need to show that we can uniformly approximate the functions $e^{-nx}p(x)$ by functions of the form $e^{-x}q(x)$. Lang suggests to first approximate $e^{-2x}$ by functions of the form $e^{-x}q(x)$ using Taylor's theorem with remainders. Then consider $e^{-nx}p(x)$ in general. I think I know how to do the first step. For the second step, the $n\geq 3$ cases are easy. On the other hand, I don't know how to treat the case $n=2$. So here's my question: can we approximate a function of the form $x^me^{-2x}$, where $m\in\mathbb{N}$, uniformly over $[0,\infty)$, by functions of the form $e^{-x}q(x)$, where $q$ is a polynomial? I would appreciate a reference or a proof (or both). (You can post your answer in another language.)",,"['analysis', 'approximation']"
31,Prove that $f:\mathbb R^n\to\mathbb R$ is affine if and only if it is convex and concave,Prove that  is affine if and only if it is convex and concave,f:\mathbb R^n\to\mathbb R,"Suppose $f:\mathbb{R}^n\to \mathbb{R}$ is both convex and concave, how to prove that $f$ is linear? or exactly speaking, $f$ is affine? I thought for the whole day, but I cannot figure it out. When I was working on this problem, I met another problem, are all the convex function continuous? If not, is there any counter example? Actually, I can prove for one dimensional case, in which $f:\mathbb{R}\to \mathbb{R}$. However, I cannot generalize it into n dimensional cases. By the way, I use definition for convex(concave) like this: $$f(t\vec{x}+(1-t)\vec{y})\leq(or \geq) tf(\vec{x})+(1-t)f(\vec{y}), \forall t\in[0,1].$$ Thank you so much!","Suppose $f:\mathbb{R}^n\to \mathbb{R}$ is both convex and concave, how to prove that $f$ is linear? or exactly speaking, $f$ is affine? I thought for the whole day, but I cannot figure it out. When I was working on this problem, I met another problem, are all the convex function continuous? If not, is there any counter example? Actually, I can prove for one dimensional case, in which $f:\mathbb{R}\to \mathbb{R}$. However, I cannot generalize it into n dimensional cases. By the way, I use definition for convex(concave) like this: $$f(t\vec{x}+(1-t)\vec{y})\leq(or \geq) tf(\vec{x})+(1-t)f(\vec{y}), \forall t\in[0,1].$$ Thank you so much!",,"['analysis', 'convex-analysis', 'convex-geometry']"
32,Fundamental solution for Helmholtz equation in higher dimensions,Fundamental solution for Helmholtz equation in higher dimensions,,The fundamental solution for Helmholtz equation $(\Delta + k^2) u = -\delta$ is $e^{i k r}/r$ in 3d and $H_0^1(kr)$ in 2d (up to normalization constants). Is there an explicit expression (eventually in terms of special functions) for the fundamental solution in dimension $\geq 4$? How can one derive it?,The fundamental solution for Helmholtz equation $(\Delta + k^2) u = -\delta$ is $e^{i k r}/r$ in 3d and $H_0^1(kr)$ in 2d (up to normalization constants). Is there an explicit expression (eventually in terms of special functions) for the fundamental solution in dimension $\geq 4$? How can one derive it?,,"['analysis', 'partial-differential-equations', 'special-functions', 'greens-function', 'fundamental-solution']"
33,Rigorous definition of positive orientation of curve in $\mathbb{R}^n$,Rigorous definition of positive orientation of curve in,\mathbb{R}^n,"When one formulates the Green Theorem the phrase ""curve positively oriented"" comes up. After a thorough google search the only description of the above phrase seems to be: ""A curve has positive orientation if a region $R$ is on the left when traveling around the outside of $R$, or on the right when traveling around the inside of $R$.  (taken from http://mathworld.wolfram.com/CurveOrientation.html ) This is of course a not rigorous definition as the terms ""left,right,outside,inside"" are not (to my knowledge) properly defined in the language of mathematics. My question is, what is the rigorous definition of orientation of a curve in $\mathbb{R}^n$?","When one formulates the Green Theorem the phrase ""curve positively oriented"" comes up. After a thorough google search the only description of the above phrase seems to be: ""A curve has positive orientation if a region $R$ is on the left when traveling around the outside of $R$, or on the right when traveling around the inside of $R$.  (taken from http://mathworld.wolfram.com/CurveOrientation.html ) This is of course a not rigorous definition as the terms ""left,right,outside,inside"" are not (to my knowledge) properly defined in the language of mathematics. My question is, what is the rigorous definition of orientation of a curve in $\mathbb{R}^n$?",,"['analysis', 'multivariable-calculus']"
34,What are the differences between differential and gradient?,What are the differences between differential and gradient?,,"As far as i know, both differential and gradient are vectors where their dot product with a unit vector give directional derivative with the direction of the unit vector. So what are the differences?","As far as i know, both differential and gradient are vectors where their dot product with a unit vector give directional derivative with the direction of the unit vector. So what are the differences?",,"['analysis', 'derivatives']"
35,Pushforward measure integral property,Pushforward measure integral property,,"Let $\mu$ be a Borel measure on $X$ and $T: X \rightarrow Y$ a Borel map ($Y$ topological space). I define $$ T _* \mu(E) = \mu(T^{-1}(E))  \qquad \forall E \subset Y \quad \text{Borel} $$ It is easy to prove that $T_* \mu$ is a measure on $Y$. But why, if $f : Y \rightarrow \mathbb{R}$ is $L^1(T_* \mu)$, $$ \int_Y f \ d T_* \mu = \int_X f \circ T \ d \mu$$","Let $\mu$ be a Borel measure on $X$ and $T: X \rightarrow Y$ a Borel map ($Y$ topological space). I define $$ T _* \mu(E) = \mu(T^{-1}(E))  \qquad \forall E \subset Y \quad \text{Borel} $$ It is easy to prove that $T_* \mu$ is a measure on $Y$. But why, if $f : Y \rightarrow \mathbb{R}$ is $L^1(T_* \mu)$, $$ \int_Y f \ d T_* \mu = \int_X f \circ T \ d \mu$$",,['analysis']
36,Why is it important to study the eigenvalues of the Laplacian?,Why is it important to study the eigenvalues of the Laplacian?,,"Why is it important to study the eigenvalues of the Laplacian acting on regions in $\mathbb R^n$? What information does this give us? What problems does this information help us solve? In particular, if $0$ is in the spectrum, does this tell us anything about the solvability of the Dirichlet or Poisson problem for the region? This second question is motivated by the following theorem that I found while flipping through Davies' Spectral Theory and Differential Operators . From the emphasis he puts on it and the amount of work he did to prove it, I feel it must be important, but I can't say why. Let $\Omega\subset \mathbb R^2$ be regular, and let $H$ be the   Friedrichs extension of $\small -\triangle$ initially defined on   $C^\infty_c(\Omega)$. Then $0\in \operatorname{Spec} H$ if and only if   the inradius of $\Omega$ is infinite.","Why is it important to study the eigenvalues of the Laplacian acting on regions in $\mathbb R^n$? What information does this give us? What problems does this information help us solve? In particular, if $0$ is in the spectrum, does this tell us anything about the solvability of the Dirichlet or Poisson problem for the region? This second question is motivated by the following theorem that I found while flipping through Davies' Spectral Theory and Differential Operators . From the emphasis he puts on it and the amount of work he did to prove it, I feel it must be important, but I can't say why. Let $\Omega\subset \mathbb R^2$ be regular, and let $H$ be the   Friedrichs extension of $\small -\triangle$ initially defined on   $C^\infty_c(\Omega)$. Then $0\in \operatorname{Spec} H$ if and only if   the inradius of $\Omega$ is infinite.",,"['analysis', 'partial-differential-equations', 'spectral-theory', 'harmonic-functions']"
37,Concrete example of the Lie derivative of a one-form,Concrete example of the Lie derivative of a one-form,,"Let $\alpha$ be a one-form and $X$ a vector field. For example take: \begin{align*} \alpha &= y^2 dx + x^2 dy\\ &\\ X &= \frac{\partial}{\partial x}+xy \frac{\partial}{\partial y}. \end{align*} I'm trying to understand how the Lie derivative works on concrete examples. In particular I want to apply the formula: $$\mathcal{L}_X\alpha = \left(X^j\frac{\partial \alpha_i}{\partial \phi^j} + \alpha_j \frac{\partial X^j}{\partial \phi^i}\right)d\phi^i.$$ If I understand the definitions correctly, it should be: \begin{align*} \mathcal{L}_X\alpha =&\ \left(1\frac{\partial y^2}{\partial x} + y^2 \frac{\partial 1}{\partial x}\right)dx +\left(xy\frac{\partial y^2}{\partial y} + x^2 \frac{\partial xy}{\partial x} \right)dx\\  &\ +\left(1\frac{\partial x^2}{\partial x} + y^2 \frac{\partial 1}{\partial y}\right)dy + \left(xy\frac{\partial x^2}{\partial y} + x^2 \frac{\partial xy}{\partial y}\right)dy\\ =&\ (0+0)dx+(2xy^2+x^2y)dx+(2x+0)dy+(0+x^3)dy\\ =&\ (2xy^2+x^2y)dx+(2x+x^3)dy. \end{align*} Is this correct at all ? If not, I'd appreaciate any tip or hint as to what I'm doing wrong.","Let $\alpha$ be a one-form and $X$ a vector field. For example take: \begin{align*} \alpha &= y^2 dx + x^2 dy\\ &\\ X &= \frac{\partial}{\partial x}+xy \frac{\partial}{\partial y}. \end{align*} I'm trying to understand how the Lie derivative works on concrete examples. In particular I want to apply the formula: $$\mathcal{L}_X\alpha = \left(X^j\frac{\partial \alpha_i}{\partial \phi^j} + \alpha_j \frac{\partial X^j}{\partial \phi^i}\right)d\phi^i.$$ If I understand the definitions correctly, it should be: \begin{align*} \mathcal{L}_X\alpha =&\ \left(1\frac{\partial y^2}{\partial x} + y^2 \frac{\partial 1}{\partial x}\right)dx +\left(xy\frac{\partial y^2}{\partial y} + x^2 \frac{\partial xy}{\partial x} \right)dx\\  &\ +\left(1\frac{\partial x^2}{\partial x} + y^2 \frac{\partial 1}{\partial y}\right)dy + \left(xy\frac{\partial x^2}{\partial y} + x^2 \frac{\partial xy}{\partial y}\right)dy\\ =&\ (0+0)dx+(2xy^2+x^2y)dx+(2x+0)dy+(0+x^3)dy\\ =&\ (2xy^2+x^2y)dx+(2x+x^3)dy. \end{align*} Is this correct at all ? If not, I'd appreaciate any tip or hint as to what I'm doing wrong.",,"['analysis', 'differential-geometry', 'differential-forms', 'lie-derivative']"
38,Asymptotic estimate for Riemann-Lebesgue Lemma,Asymptotic estimate for Riemann-Lebesgue Lemma,,"Let $f$ be a real-valued, $L^1$ integrable function on the interval $[a,b]$.  Then the Riemann-Lebesgue Lemma tells us that: $$\int_a^bf(x)\sin(2\pi nx)dx\rightarrow0  \text{  as  } n\rightarrow\infty.$$ Does this have any asymptotic estimate attached to it?  i.e. for sufficiently nice $f$ (say continuously differentiable), do we have the estimate that, say: $$\int_a^bf(x)\sin(2\pi nx)dx= O(1/n)$$ or something similar? Any kind of reference would also be appreciated!","Let $f$ be a real-valued, $L^1$ integrable function on the interval $[a,b]$.  Then the Riemann-Lebesgue Lemma tells us that: $$\int_a^bf(x)\sin(2\pi nx)dx\rightarrow0  \text{  as  } n\rightarrow\infty.$$ Does this have any asymptotic estimate attached to it?  i.e. for sufficiently nice $f$ (say continuously differentiable), do we have the estimate that, say: $$\int_a^bf(x)\sin(2\pi nx)dx= O(1/n)$$ or something similar? Any kind of reference would also be appreciated!",,"['analysis', 'asymptotics']"
39,Continuity of a function that maps a point to the closest point on a compact convex set,Continuity of a function that maps a point to the closest point on a compact convex set,,"Let $K$ be a nonempty compact convex subset of $\mathbb R^n$ and let $f$ be the function that maps $x \in \mathbb R^n$ to the unique closest point $y \in K$ with respect to the $\ell_2$ norm. I want to prove that $f$ is continuous, but I can't seem to figure out how. My thoughts: Suppose $x_n \to x$ in $\mathbb R^n$. Let $y_n = f(x_n)$ and let $y = f(x)$. By the compactness of $K$, there is a convergent subsequence $(y_{k_n})$ that converges to some $y' \in K$. If $y \ne y'$, then $\|x-y\| < \|x-y'\|$. Furthermore, any point $z \ne y$ on the line segment joining $y,y'$ also satisfies $\|x-y\|<\|x-z\|$. I don't know where to go from here. Any tips?","Let $K$ be a nonempty compact convex subset of $\mathbb R^n$ and let $f$ be the function that maps $x \in \mathbb R^n$ to the unique closest point $y \in K$ with respect to the $\ell_2$ norm. I want to prove that $f$ is continuous, but I can't seem to figure out how. My thoughts: Suppose $x_n \to x$ in $\mathbb R^n$. Let $y_n = f(x_n)$ and let $y = f(x)$. By the compactness of $K$, there is a convergent subsequence $(y_{k_n})$ that converges to some $y' \in K$. If $y \ne y'$, then $\|x-y\| < \|x-y'\|$. Furthermore, any point $z \ne y$ on the line segment joining $y,y'$ also satisfies $\|x-y\|<\|x-z\|$. I don't know where to go from here. Any tips?",,['analysis']
40,Relationship beween Ricci curvature and sectional curvature,Relationship beween Ricci curvature and sectional curvature,,"Let $(M,g)$ be a Riemannian manifold and assume that for all orthonormal $v,z$ the sectional curvatures is bounded from below i.e. $K(v,z) \geq C$, where $C > 0$. Is it in this case possible for the Ricci curvature to vanish? Or is this condition, on the sectional curvature, very strong? Sorry if the question is too trivial :). Gunam","Let $(M,g)$ be a Riemannian manifold and assume that for all orthonormal $v,z$ the sectional curvatures is bounded from below i.e. $K(v,z) \geq C$, where $C > 0$. Is it in this case possible for the Ricci curvature to vanish? Or is this condition, on the sectional curvature, very strong? Sorry if the question is too trivial :). Gunam",,"['analysis', 'differential-geometry', 'riemannian-geometry']"
41,Constant functions,Constant functions,,"Let $f$ , $g$ , $h$ be three functions from the set of positive real numbers to itself satisfying $$f(x)g(y) = h\left((x^2+y^2)^{\frac{1}{2}}\right)$$ for all positive real numbers $x$ , $y$ . Show that $\dfrac{f(x)}{g(x)}$ , $\dfrac{g(x)}{h(x)}$ and $\dfrac{h(x)}{f(x)}$ are all constant functions . I have proved that $\dfrac{f(x)}{g(x)}$ is constant and can see that proving either of the last two will prove the final one , but I am not able to prove any of the last two . Thanks for any help .","Let $f$ , $g$ , $h$ be three functions from the set of positive real numbers to itself satisfying $$f(x)g(y) = h\left((x^2+y^2)^{\frac{1}{2}}\right)$$ for all positive real numbers $x$ , $y$ . Show that $\dfrac{f(x)}{g(x)}$ , $\dfrac{g(x)}{h(x)}$ and $\dfrac{h(x)}{f(x)}$ are all constant functions . I have proved that $\dfrac{f(x)}{g(x)}$ is constant and can see that proving either of the last two will prove the final one , but I am not able to prove any of the last two . Thanks for any help .",,['analysis']
42,Study Tips and Techniques for Self-Oriented Students,Study Tips and Techniques for Self-Oriented Students,,"I am asking for what, if any, the preferred study skills of approaching classes are like in higher math education are like in general, and I am not asking necessarily for personal anecdotes for this question (though they are welcome if it's all you have to share). My question is what are the better studying methods for learning higher mathematics(i.e., subjects involving more proofs instead of calculations like Analysis, Topology, Axiomatic Set Theory, Abstract Algebra, etc.)? Let me explain what I mean by giving you an example. As an undergrad at first, courses like the Calculus sequence, Differential Equations could be approached by learning how to solve problems even if you weren't able to understand the proofs(which were usually skipped over in the textbook by the professor). But now, that I have taken Linear Algebra, and I am about to take Analysis I usually take a much different approach: $ $$\cdot$$ $ I start by rewriting all the definitions and theorems and proofs of a chapter and memorizing them. Then I will work through the examples, and ideally get to the exercises and finish off the chapter. And also spend time reflecting on the topics to get a more intuitive understanding of the concepts involved. I skip very little, anything if at all from the books I'm working with even if the topics are skipped in class. This approach is very effective, especially when I am studying a topic for my own interests, and I am really able to understand things at a level that my peers usually have trouble following. The drawback is that I move at a much slower pace than my peers and I end up struggling in a class towards the end of a semester because I am behind on the syllabus. I end up risking a low grade though all but once I have managed an A- or above. Also am able to work out Jech's Set Theory which others have told me is out of my league but I actually find it the right amount of challenging using this approach. The method followed by the math department at my school feels very superficial and not as the effective long-term. Usually, professors do not focus on the reasoning or intuition behind concepts. Tests are oriented so that we memorize by rote the proofs of the major theorems and regurgitate them on exams. Let me give more concrete questions: I asked a math professor once, and he said that real mathematics is usually done where you understand 2 or 3 pages a day of a text on your first reading of it. Is this true for graduate level work for the average student? Do students who follow my immersive way of studying tend to have an advantage over those who don't when we get to grad school? In terms of learning theory and doing exercises, how much importance is recommended to place on each? 4. Are there study techniques used in more advanced math courses (like engaging in discourse with peers, focusing more on memorization before attempting to do problem sets, taking notes in a particular way) that are more fruitful than others?","I am asking for what, if any, the preferred study skills of approaching classes are like in higher math education are like in general, and I am not asking necessarily for personal anecdotes for this question (though they are welcome if it's all you have to share). My question is what are the better studying methods for learning higher mathematics(i.e., subjects involving more proofs instead of calculations like Analysis, Topology, Axiomatic Set Theory, Abstract Algebra, etc.)? Let me explain what I mean by giving you an example. As an undergrad at first, courses like the Calculus sequence, Differential Equations could be approached by learning how to solve problems even if you weren't able to understand the proofs(which were usually skipped over in the textbook by the professor). But now, that I have taken Linear Algebra, and I am about to take Analysis I usually take a much different approach: I start by rewriting all the definitions and theorems and proofs of a chapter and memorizing them. Then I will work through the examples, and ideally get to the exercises and finish off the chapter. And also spend time reflecting on the topics to get a more intuitive understanding of the concepts involved. I skip very little, anything if at all from the books I'm working with even if the topics are skipped in class. This approach is very effective, especially when I am studying a topic for my own interests, and I am really able to understand things at a level that my peers usually have trouble following. The drawback is that I move at a much slower pace than my peers and I end up struggling in a class towards the end of a semester because I am behind on the syllabus. I end up risking a low grade though all but once I have managed an A- or above. Also am able to work out Jech's Set Theory which others have told me is out of my league but I actually find it the right amount of challenging using this approach. The method followed by the math department at my school feels very superficial and not as the effective long-term. Usually, professors do not focus on the reasoning or intuition behind concepts. Tests are oriented so that we memorize by rote the proofs of the major theorems and regurgitate them on exams. Let me give more concrete questions: I asked a math professor once, and he said that real mathematics is usually done where you understand 2 or 3 pages a day of a text on your first reading of it. Is this true for graduate level work for the average student? Do students who follow my immersive way of studying tend to have an advantage over those who don't when we get to grad school? In terms of learning theory and doing exercises, how much importance is recommended to place on each? 4. Are there study techniques used in more advanced math courses (like engaging in discourse with peers, focusing more on memorization before attempting to do problem sets, taking notes in a particular way) that are more fruitful than others?", \cdot ,"['analysis', 'soft-question', 'self-learning', 'education', 'advice']"
43,2012 Putnam Exam A3,2012 Putnam Exam A3,,"This question mostly eluded me during the exam itself. Problem : Suppose that $f: [-1, 1] \rightarrow \mathbb{R}$ continuously, and that $$\begin{align} \text{(i)}\qquad   &f(x) = \frac{2 - x^2}{2} f\left(\frac{x^2}{2-x^2}\right)\\ \text{(ii)}\qquad  &f(0) = 1\\[6pt] \text{(iii)}\qquad &\lim_{x\to 1^-}\frac{f(x)}{\sqrt{1-x}}\ \text{exists} \end{align}$$ Determine the closed form of $f$, and prove that it is unique. --- All that I managed to do was - using the eventual heuristic of ""powers of $1 - x^2$ will cancel neatly in ( iii ) and work in ( ii ), so let's see if they work in ( i )"" - discover that $$ f(x) = \sqrt{1 - x^2}$$ Now, this $f$ is an involution , so it suffices to prove that if $g$ is a function satisfying ( i ) to ( iii ), then $g(f(x)) = x$. This, however, was more than I could do. How do you prove uniqueness? (I ran across one solution I did not understand; please, be gentle.)","This question mostly eluded me during the exam itself. Problem : Suppose that $f: [-1, 1] \rightarrow \mathbb{R}$ continuously, and that $$\begin{align} \text{(i)}\qquad   &f(x) = \frac{2 - x^2}{2} f\left(\frac{x^2}{2-x^2}\right)\\ \text{(ii)}\qquad  &f(0) = 1\\[6pt] \text{(iii)}\qquad &\lim_{x\to 1^-}\frac{f(x)}{\sqrt{1-x}}\ \text{exists} \end{align}$$ Determine the closed form of $f$, and prove that it is unique. --- All that I managed to do was - using the eventual heuristic of ""powers of $1 - x^2$ will cancel neatly in ( iii ) and work in ( ii ), so let's see if they work in ( i )"" - discover that $$ f(x) = \sqrt{1 - x^2}$$ Now, this $f$ is an involution , so it suffices to prove that if $g$ is a function satisfying ( i ) to ( iii ), then $g(f(x)) = x$. This, however, was more than I could do. How do you prove uniqueness? (I ran across one solution I did not understand; please, be gentle.)",,"['analysis', 'contest-math']"
44,interval sequences which contains infinite items of an arithmetic progression,interval sequences which contains infinite items of an arithmetic progression,,"Here's a problem in real analysis which has bothered me and my friends for several days: For an arbitrary sequence of intervals $(a_i,b_i)$, $a_i$ and $b_i$ tend to infinity and the intersection of any two intervals is empty, must there be an arithmetic progression such that there are infinite items of the progression lying in the interval sequence? thank you","Here's a problem in real analysis which has bothered me and my friends for several days: For an arbitrary sequence of intervals $(a_i,b_i)$, $a_i$ and $b_i$ tend to infinity and the intersection of any two intervals is empty, must there be an arithmetic progression such that there are infinite items of the progression lying in the interval sequence? thank you",,['analysis']
45,Elliptic regularity in Sobolev spaces of negative order,Elliptic regularity in Sobolev spaces of negative order,,"I am having some trouble with Sobolev spaces of negative order. More precisely I am considering the space $W^{-1,p}(\mathbb{R}^2),$ considered as 'the' dual space of $W^{1,q}(\mathbb{R}^2).$ Question 1: Is there a nice reference for Sobolev spaces of negative order, for $1<p<\infty.$ Question 2: Suppose $f\in W^{-1,p}(\mathbb{R}^2,\mathbb{C})$ is a weak solution to the inhomogeneous Cauchy-Riemann equation, i.e. $\left\langle f,\overline{\partial} g+Sg \right\rangle$ for all smooth and compactly supported $g,$ where $\overline{\partial}$ is the Cauchy-Riemann operator and $S$ is smooth. Is it then true that $f$ is itself smooth? I know the case $S=0$ is sometimes called Weyl's Lemma.","I am having some trouble with Sobolev spaces of negative order. More precisely I am considering the space $W^{-1,p}(\mathbb{R}^2),$ considered as 'the' dual space of $W^{1,q}(\mathbb{R}^2).$ Question 1: Is there a nice reference for Sobolev spaces of negative order, for $1<p<\infty.$ Question 2: Suppose $f\in W^{-1,p}(\mathbb{R}^2,\mathbb{C})$ is a weak solution to the inhomogeneous Cauchy-Riemann equation, i.e. $\left\langle f,\overline{\partial} g+Sg \right\rangle$ for all smooth and compactly supported $g,$ where $\overline{\partial}$ is the Cauchy-Riemann operator and $S$ is smooth. Is it then true that $f$ is itself smooth? I know the case $S=0$ is sometimes called Weyl's Lemma.",,"['analysis', 'reference-request', 'partial-differential-equations', 'sobolev-spaces']"
46,Dual and completion of metric spaces,Dual and completion of metric spaces,,"Say we have a metric space $(M,d)$, and we want to complete it in the following sense: Definition : A completion of $(M,d)$ is a complete metric space $(\widetilde{M},d')$ together with a Lipschitz funcion $i:M\rightarrow\widetilde{M}$ such that for every other complete metric space $(N,\rho)$ together with a Lipschitz function $f:M\rightarrow N$, there exists an unique Lipschitz function $F:\widetilde{M}\rightarrow N$ such that $F\circ i=f$. This definition is adapted from the definition for uniform spaces ( wikipedia ). I changed the condition that the functions are uniformly continuous to Lipschitz so any two completions of a metric space would be ""equivalent"" as metric spaces, and not just be ""uniformly equivalent"". One could also suppose the functions are isometries, for example. (I think the better ""morfisms"" in the category of metric spaces are Lipschitz functions.) The usual completion of $M$ is defined to be (a quotient of) the set $\widetilde{M}$ of Cauchy sequences on $M$ with the (pseudo)metric $d'\left((x_n)_n,(y_n)_n\right)=\lim d(x_n,y_n)$ and the inclusion $i:x\in M\mapsto (x)_n\in\widetilde{M}$ (it's the same as the one given in wikipedia). However, suppose we are working with a normed (vector) space, let's say $(X,\Vert\cdot\Vert)$. The completion of $X$ (making the proper adaptations in the definition: that the functions are linear, etc...) can be very easily defined as the closure of $ev(X)$ as a subspace of $X''$, where $Y'$ denotes the dual of a normed space $Y$ with the operator norm, and $ev:X\rightarrow X''$ is the evaluation function: $ev(x)(f)=f(x)$ for every $x\in X$ and $f\in X'$. My question is: would we be able to make a similar construction for general metric spaces, that is, to find a nice definition of dual of a metric space for which the dual of it would be a complete metric space? If so, we could try to just define the completion of $M$ as the closure of $ev(M)$ in $dual(dual(M))$. In other words, I would like to find a kind of (nice) (algebraic, analytic, etc..) structure so the category of sets with that structure is dual to the category of metric spaces. The first possible ""dual"" of $(M,d)$ that comes to my mind is $C_b(M)$, the set of continuous bounded functions from $M$ to $\mathbb{R}$ with the $\infty$-norm. Problem is that this is a commutative, unital $C^*$-Algebra, and we know the natural dual of a commutative $C^*$-Algebra is a compact hausdorff topological space, not a metric space. (also, I guess a dual notion of metric space couls have many applications other than just making completions)","Say we have a metric space $(M,d)$, and we want to complete it in the following sense: Definition : A completion of $(M,d)$ is a complete metric space $(\widetilde{M},d')$ together with a Lipschitz funcion $i:M\rightarrow\widetilde{M}$ such that for every other complete metric space $(N,\rho)$ together with a Lipschitz function $f:M\rightarrow N$, there exists an unique Lipschitz function $F:\widetilde{M}\rightarrow N$ such that $F\circ i=f$. This definition is adapted from the definition for uniform spaces ( wikipedia ). I changed the condition that the functions are uniformly continuous to Lipschitz so any two completions of a metric space would be ""equivalent"" as metric spaces, and not just be ""uniformly equivalent"". One could also suppose the functions are isometries, for example. (I think the better ""morfisms"" in the category of metric spaces are Lipschitz functions.) The usual completion of $M$ is defined to be (a quotient of) the set $\widetilde{M}$ of Cauchy sequences on $M$ with the (pseudo)metric $d'\left((x_n)_n,(y_n)_n\right)=\lim d(x_n,y_n)$ and the inclusion $i:x\in M\mapsto (x)_n\in\widetilde{M}$ (it's the same as the one given in wikipedia). However, suppose we are working with a normed (vector) space, let's say $(X,\Vert\cdot\Vert)$. The completion of $X$ (making the proper adaptations in the definition: that the functions are linear, etc...) can be very easily defined as the closure of $ev(X)$ as a subspace of $X''$, where $Y'$ denotes the dual of a normed space $Y$ with the operator norm, and $ev:X\rightarrow X''$ is the evaluation function: $ev(x)(f)=f(x)$ for every $x\in X$ and $f\in X'$. My question is: would we be able to make a similar construction for general metric spaces, that is, to find a nice definition of dual of a metric space for which the dual of it would be a complete metric space? If so, we could try to just define the completion of $M$ as the closure of $ev(M)$ in $dual(dual(M))$. In other words, I would like to find a kind of (nice) (algebraic, analytic, etc..) structure so the category of sets with that structure is dual to the category of metric spaces. The first possible ""dual"" of $(M,d)$ that comes to my mind is $C_b(M)$, the set of continuous bounded functions from $M$ to $\mathbb{R}$ with the $\infty$-norm. Problem is that this is a commutative, unital $C^*$-Algebra, and we know the natural dual of a commutative $C^*$-Algebra is a compact hausdorff topological space, not a metric space. (also, I guess a dual notion of metric space couls have many applications other than just making completions)",,"['analysis', 'metric-spaces', 'category-theory', 'duality-theorems']"
47,2-dimensional Lebesgue measure of certain sets in $R^3$,2-dimensional Lebesgue measure of certain sets in,R^3,"Let $\theta >0$ and $E \subseteq \mathbb{R}^3$ be a closed set (I've added closedness as a new requirement) which satisfies the following condition: For any $x \in E$ , there exist at least two lines $L_1,L_2 \subseteq E$ passing through $x$ with $\angle(L_1,L_2) \in (\theta, \frac{\pi}{2})$ . I am trying to figure out whether $\mathscr{L}^2(E)>0$ (excluding non-measurable sets). In fact, I can't even come up with 2-dimensional (Hausdorff) sets satisfying the above condition other than 2-planes or a union of them. Would appreciate some suggestions.","Let and be a closed set (I've added closedness as a new requirement) which satisfies the following condition: For any , there exist at least two lines passing through with . I am trying to figure out whether (excluding non-measurable sets). In fact, I can't even come up with 2-dimensional (Hausdorff) sets satisfying the above condition other than 2-planes or a union of them. Would appreciate some suggestions.","\theta >0 E \subseteq \mathbb{R}^3 x \in E L_1,L_2 \subseteq E x \angle(L_1,L_2) \in (\theta, \frac{\pi}{2}) \mathscr{L}^2(E)>0","['analysis', 'measure-theory', 'lebesgue-measure', 'geometric-measure-theory']"
48,Motivation of Splines,Motivation of Splines,,"What is the motivation of splines, in particular cubic splines. For example, why does it matter that they have any type of smoothness at the knots.","What is the motivation of splines, in particular cubic splines. For example, why does it matter that they have any type of smoothness at the knots.",,"['analysis', 'numerical-methods', 'spline', 'motivation']"
49,Expressing $\sin(2x)$ as a polynomial of $\sin{x}$,Expressing  as a polynomial of,\sin(2x) \sin{x},"Using trignometric identities ( double angle forumlas) one can see that $\cos{2x} = 2 \: \cos^{2}{x} - 1$ can be expressed as a polynomial of $\cos{x}$, where $p(\cos{x})=2 \: \cos^{2}{x}-1$. Then its natural to ask the same question for the sine function. Can we express $\sin{2x}$ as a polynomial of $\sin{x}$?","Using trignometric identities ( double angle forumlas) one can see that $\cos{2x} = 2 \: \cos^{2}{x} - 1$ can be expressed as a polynomial of $\cos{x}$, where $p(\cos{x})=2 \: \cos^{2}{x}-1$. Then its natural to ask the same question for the sine function. Can we express $\sin{2x}$ as a polynomial of $\sin{x}$?",,['analysis']
50,The Dirac delta does not belong in L2,The Dirac delta does not belong in L2,,"I need to prove that Dirac's delta does not belong in $L^2(\mathbb{R})$. First, I found the next definition of Dirac's delta $$\delta :D(\mathbb R)\to \mathbb R$$ is defined by: $$\langle \delta,\varphi \rangle=\int_{-\infty}^{+\infty}\varphi(x)\delta(x)\,\mathrm{d}x = \varphi(0),$$ and $$\delta(x)= \begin{cases} 1,& x= 0\\ 0 ,& x\ne 0. \end{cases} \\$$ The space $L^2(\mathbb{R})=\{f:f \text{ is measurable and } \|f\|_{2}<+\infty \}$. I'm thinking suppose otherwise, i.e, Dirac's delta in $L^2$, but I have problems to prove that Dirac's delta is measurable, but I suspect that in calculating of $\|f\|_2$ I'll find the contradiction. Could you give me any suggestions??","I need to prove that Dirac's delta does not belong in $L^2(\mathbb{R})$. First, I found the next definition of Dirac's delta $$\delta :D(\mathbb R)\to \mathbb R$$ is defined by: $$\langle \delta,\varphi \rangle=\int_{-\infty}^{+\infty}\varphi(x)\delta(x)\,\mathrm{d}x = \varphi(0),$$ and $$\delta(x)= \begin{cases} 1,& x= 0\\ 0 ,& x\ne 0. \end{cases} \\$$ The space $L^2(\mathbb{R})=\{f:f \text{ is measurable and } \|f\|_{2}<+\infty \}$. I'm thinking suppose otherwise, i.e, Dirac's delta in $L^2$, but I have problems to prove that Dirac's delta is measurable, but I suspect that in calculating of $\|f\|_2$ I'll find the contradiction. Could you give me any suggestions??",,"['analysis', 'measure-theory']"
51,Difficulties with Chapter 2 in Rudin,Difficulties with Chapter 2 in Rudin,,"I have been reading Rudin (Principles of Mathematical Analysis)  on my own now for around a month or so. While I was able to complete the first chapter without any difficulty, I am having problems trying to get the second chapter right. I have been able to get the definitions and work out some problems, but I am still not sure if I understand the thing and it is certainly not internalized. I am wondering whether I should take this shaky structure with me to the next chapters, hoping that the application there improves my understanding, or to stop and complete this chapter really well? What do you think? As for my background, I am quiet close to completing Linear Algebra by Lang (having done a course in Linear Algebra from Strang). I have completed Spivak's Calculus. I come from an engineering background and so I have done multivariable calculus, fourier analysis, numerical analysis, basic probability and random variables as required for engineering. One of the professors advised that I may be better off studying Part I from Topology and Modern Analysis by GF Simmons, but I am finding that completing that book itself may take a semester and I would prefer not to wait that long to start with analysis. Thank You EDIT : If it makes any difference, I am studying on my own. EDIT :  So, I have accepted the answer by Samuel Reid. I too have found the limit point definition as illustrated by Rudin and the large set of definitions listed there somewhat dry and without any motivation or examples. This is one of the places in the book which makes it a little difficult for self-study. What I found working in this case is, taking some drill problems from other books and working through them. I will advise anyone to go real slow over the sections 2.18 to 2.32 . There are too many definitions and new concepts in that sections and to miss even one means you cannot move forward. To tell the truth, I found Simmons's 50 pages (from chapter 2 section 10 to the end of chapter 3) to be more useful than the corresponding 4.5 pages in Rudin.","I have been reading Rudin (Principles of Mathematical Analysis)  on my own now for around a month or so. While I was able to complete the first chapter without any difficulty, I am having problems trying to get the second chapter right. I have been able to get the definitions and work out some problems, but I am still not sure if I understand the thing and it is certainly not internalized. I am wondering whether I should take this shaky structure with me to the next chapters, hoping that the application there improves my understanding, or to stop and complete this chapter really well? What do you think? As for my background, I am quiet close to completing Linear Algebra by Lang (having done a course in Linear Algebra from Strang). I have completed Spivak's Calculus. I come from an engineering background and so I have done multivariable calculus, fourier analysis, numerical analysis, basic probability and random variables as required for engineering. One of the professors advised that I may be better off studying Part I from Topology and Modern Analysis by GF Simmons, but I am finding that completing that book itself may take a semester and I would prefer not to wait that long to start with analysis. Thank You EDIT : If it makes any difference, I am studying on my own. EDIT :  So, I have accepted the answer by Samuel Reid. I too have found the limit point definition as illustrated by Rudin and the large set of definitions listed there somewhat dry and without any motivation or examples. This is one of the places in the book which makes it a little difficult for self-study. What I found working in this case is, taking some drill problems from other books and working through them. I will advise anyone to go real slow over the sections 2.18 to 2.32 . There are too many definitions and new concepts in that sections and to miss even one means you cannot move forward. To tell the truth, I found Simmons's 50 pages (from chapter 2 section 10 to the end of chapter 3) to be more useful than the corresponding 4.5 pages in Rudin.",,['analysis']
52,A surjective map which is not a submersion,A surjective map which is not a submersion,,"Is there an example of a smooth map between smooth manifolds which is surjective, but not a submersion? I feel there can't be one, but don't know of a proof. Nor do I know of a counter-example. Kindly help!","Is there an example of a smooth map between smooth manifolds which is surjective, but not a submersion? I feel there can't be one, but don't know of a proof. Nor do I know of a counter-example. Kindly help!",,"['analysis', 'differential-topology']"
53,Dini's continuity vs Holder continuity,Dini's continuity vs Holder continuity,,"(listed items are just the definitions, you can skip to ""Clearly"" if you are familiar with them) Let $E \subset \mathbb{R}^N$ and let $f \colon E \to \mathbb{R}.$ The modulus of continuity of $f$ is the increasing function $\omega_f \colon [0,\infty) \to [0,\infty)$ defined by $$\omega_f(t) := \sup\{|f(x) - f(y)| : x,y \in E, \|x - y\| \le t\}.$$The function $f$ is Dini's continuous if $$\int_0^1\frac{\omega_f(t)}{t}\ dt < \infty$$ Let $E \subset \mathbb{R}^N$ and let $f \colon E \to \mathbb{R}.$ The function $f$ is Holder continuous with exponent $\alpha \in (0,1)$ if $$|f|_{C^{0,\alpha}(E)} := \sup\Big\{\frac{|f(x) - f(y)|}{\|x - y\|^{\alpha}} : x,y \in E, x \neq y\Big\} < \infty.$$ Clearly if $f$ is Holder continuous with exponent $\alpha$, then $\omega_f(t) \le Lt^{\alpha}$ for some $L > 0$ and hence $f$ is Dini's continuous. Do you know a counterexample to the other inclusion? (namely, I am looking for a Dini's continuous function such that it is not Holder continuous for any $\alpha$). [Motivation for the question: I know that the Fourier series of a Holder continuous function converges uniformly to the function. I also know that the Fourier series of a continuous function can be very bad (it can diverge on a dense subset). Today I read that the Fourier series of a Dini's continuous function converges uniformly, so, as it is natural, I want to know what functions I was missing before.]","(listed items are just the definitions, you can skip to ""Clearly"" if you are familiar with them) Let $E \subset \mathbb{R}^N$ and let $f \colon E \to \mathbb{R}.$ The modulus of continuity of $f$ is the increasing function $\omega_f \colon [0,\infty) \to [0,\infty)$ defined by $$\omega_f(t) := \sup\{|f(x) - f(y)| : x,y \in E, \|x - y\| \le t\}.$$The function $f$ is Dini's continuous if $$\int_0^1\frac{\omega_f(t)}{t}\ dt < \infty$$ Let $E \subset \mathbb{R}^N$ and let $f \colon E \to \mathbb{R}.$ The function $f$ is Holder continuous with exponent $\alpha \in (0,1)$ if $$|f|_{C^{0,\alpha}(E)} := \sup\Big\{\frac{|f(x) - f(y)|}{\|x - y\|^{\alpha}} : x,y \in E, x \neq y\Big\} < \infty.$$ Clearly if $f$ is Holder continuous with exponent $\alpha$, then $\omega_f(t) \le Lt^{\alpha}$ for some $L > 0$ and hence $f$ is Dini's continuous. Do you know a counterexample to the other inclusion? (namely, I am looking for a Dini's continuous function such that it is not Holder continuous for any $\alpha$). [Motivation for the question: I know that the Fourier series of a Holder continuous function converges uniformly to the function. I also know that the Fourier series of a continuous function can be very bad (it can diverge on a dense subset). Today I read that the Fourier series of a Dini's continuous function converges uniformly, so, as it is natural, I want to know what functions I was missing before.]",,"['analysis', 'continuity']"
54,Riemann's thinking on symmetrizing the zeta functional equation,Riemann's thinking on symmetrizing the zeta functional equation,,"In the translated version of Riemann's classic On the Number of Prime Numbers less than a Given Quantity , he quickly derives the zeta functional equation through contour integration essentially as $$\zeta(s)=2(2\pi)^{s-1}\Gamma(1-s)\sin(\tfrac12\pi s)\zeta(1-s).$$ and says three lines later that it may be expressed as $$\xi(s) = \pi^{-s/2}\ \Gamma\left(\frac{s}{2}\right)\ \zeta(s)=\xi(1-s).$$ After reading MO-Q7656 , I started wondering whether, as his ideas evolved before he wrote the paper , he first constructed $\xi(s)$ by noticing that multiplying $\zeta(s)$ by $\Gamma(\frac{s}{2})$ introduces a simple pole at $s=0$ thereby reflecting the pole of $\zeta(s)$ at $s=1$ through the line $s=1/2$ and that the other simple poles of $\Gamma(\frac{s}{2})$ are removed by the zeros on the real line of the zeta function. The $\pi^{-s/2}$ can easily be determined as a normalization by an entire function $c^s$ where $c$ is a constant, using the complex conjugate symmetry of the gamma and zeta fct. about the real axis. Anyone familiar with how his ideas (thinking) evolved? (Update 5/13/2012) Riemann in his fourth equality in his paper, before he writes down the functional eqn., has $$2sin(\pi s)(s-1)!\zeta(s)=i\int_{+\infty}^{+\infty}\frac{(-x)^{s-1}}{e^x-1}dx$$ where the contour sandwiches the positive real axis and surrounds the origin in the positive sense. For $m=0,1,2, ...,$ this gives $$\zeta(-m)=\frac{(-1)^{m}}{2\pi i}\oint_{|z|=1}\frac{m!}{z^{m+1}}\frac{1}{e^z-1}dz=\frac{(-1)^{m}}{m+1}\frac{1}{2\pi i}\oint_{|z|=1}\frac{(m+1)!}{z^{m+2}}\frac{z}{e^z-1}dz$$ from whence you can see, if you are familiar with the exponential generating fct. for the Bernoulli numbers, that the integral vanishes for even $m$. Riemann certainly was familiar with these numbers and states that the integral relation he gives implies the vanishing of $\zeta(s)$ for $m$ even (but gives no explicit proof). Edwards in Riemann's Zeta Function (pg. 12, Dover ed.) even speculates that "".. it may well have been this problem of deriving (2) [Euler's formula for $\zeta(2n)$ for positive $n$] anew which led Riemann to the discovery of the functional equation ...."" Riemann gives two proofs of the fct. eqn.--the first based on contour integration and the singularities of $\frac{1}{e^z-1}$, the second, on the theta function. Edwards wonders: ""Since the second proof renders the first proof wholly unnecessary, one may ask why Riemann included the first proof at all. Perhaps the first proof shows the argument by which he originally discovered the functional equation or perhaps it exhibits some properties which were important in his understanding of it."" In fact, Riemann states on page 3 of his paper, ""This property of the function [$\xi(s)=\xi(1-s)$] induced me to introduce, in place of $(s-1)!$, the integral $(s/2-1)!$ into the general term of the series"" for zeta. And then he proceeds to introduce the Jacobi theta function. Edit Dec 18, 2014 In the early 1820s, both Abel and Plana separately published what is now called the Abel-Plana formula (see Wikipedia). In the title of Plana's, he mentions the Bernoulliens. I wonder how much Riemann was influenced by these papers.","In the translated version of Riemann's classic On the Number of Prime Numbers less than a Given Quantity , he quickly derives the zeta functional equation through contour integration essentially as $$\zeta(s)=2(2\pi)^{s-1}\Gamma(1-s)\sin(\tfrac12\pi s)\zeta(1-s).$$ and says three lines later that it may be expressed as $$\xi(s) = \pi^{-s/2}\ \Gamma\left(\frac{s}{2}\right)\ \zeta(s)=\xi(1-s).$$ After reading MO-Q7656 , I started wondering whether, as his ideas evolved before he wrote the paper , he first constructed $\xi(s)$ by noticing that multiplying $\zeta(s)$ by $\Gamma(\frac{s}{2})$ introduces a simple pole at $s=0$ thereby reflecting the pole of $\zeta(s)$ at $s=1$ through the line $s=1/2$ and that the other simple poles of $\Gamma(\frac{s}{2})$ are removed by the zeros on the real line of the zeta function. The $\pi^{-s/2}$ can easily be determined as a normalization by an entire function $c^s$ where $c$ is a constant, using the complex conjugate symmetry of the gamma and zeta fct. about the real axis. Anyone familiar with how his ideas (thinking) evolved? (Update 5/13/2012) Riemann in his fourth equality in his paper, before he writes down the functional eqn., has $$2sin(\pi s)(s-1)!\zeta(s)=i\int_{+\infty}^{+\infty}\frac{(-x)^{s-1}}{e^x-1}dx$$ where the contour sandwiches the positive real axis and surrounds the origin in the positive sense. For $m=0,1,2, ...,$ this gives $$\zeta(-m)=\frac{(-1)^{m}}{2\pi i}\oint_{|z|=1}\frac{m!}{z^{m+1}}\frac{1}{e^z-1}dz=\frac{(-1)^{m}}{m+1}\frac{1}{2\pi i}\oint_{|z|=1}\frac{(m+1)!}{z^{m+2}}\frac{z}{e^z-1}dz$$ from whence you can see, if you are familiar with the exponential generating fct. for the Bernoulli numbers, that the integral vanishes for even $m$. Riemann certainly was familiar with these numbers and states that the integral relation he gives implies the vanishing of $\zeta(s)$ for $m$ even (but gives no explicit proof). Edwards in Riemann's Zeta Function (pg. 12, Dover ed.) even speculates that "".. it may well have been this problem of deriving (2) [Euler's formula for $\zeta(2n)$ for positive $n$] anew which led Riemann to the discovery of the functional equation ...."" Riemann gives two proofs of the fct. eqn.--the first based on contour integration and the singularities of $\frac{1}{e^z-1}$, the second, on the theta function. Edwards wonders: ""Since the second proof renders the first proof wholly unnecessary, one may ask why Riemann included the first proof at all. Perhaps the first proof shows the argument by which he originally discovered the functional equation or perhaps it exhibits some properties which were important in his understanding of it."" In fact, Riemann states on page 3 of his paper, ""This property of the function [$\xi(s)=\xi(1-s)$] induced me to introduce, in place of $(s-1)!$, the integral $(s/2-1)!$ into the general term of the series"" for zeta. And then he proceeds to introduce the Jacobi theta function. Edit Dec 18, 2014 In the early 1820s, both Abel and Plana separately published what is now called the Abel-Plana formula (see Wikipedia). In the title of Plana's, he mentions the Bernoulliens. I wonder how much Riemann was influenced by these papers.",,"['analysis', 'math-history', 'functional-equations', 'riemann-zeta']"
55,Does a nonlinear additive function on R imply a Hamel basis of R?,Does a nonlinear additive function on R imply a Hamel basis of R?,,"A function is additive if $f(x+y) = f(x) + f(y)$.  Intuitively, it might seem that an additive function from R to R must be linear, specifically of the form $f(x) = kx$.  But assuming the axiom of choice, that is wrong, and the proof is rather simple: you just take a Hamel basis of $\mathbb{R}$ as a vector space over $\mathbb{Q}$, and then you define your function f to be different in at least two distinct elements of the basis. But my question is this: if there is no Hamel basis of $\mathbb{R}$, then must $f$ be linear?  To put it another way, does ZF + the existence of a nonlinear additive function imply the existence of Hamel basis of $\mathbb{R}$? I checked the Consequences of the Axiom of Choice Project, a database of choice axioms and their relationships here , and it said that it didn't know.","A function is additive if $f(x+y) = f(x) + f(y)$.  Intuitively, it might seem that an additive function from R to R must be linear, specifically of the form $f(x) = kx$.  But assuming the axiom of choice, that is wrong, and the proof is rather simple: you just take a Hamel basis of $\mathbb{R}$ as a vector space over $\mathbb{Q}$, and then you define your function f to be different in at least two distinct elements of the basis. But my question is this: if there is no Hamel basis of $\mathbb{R}$, then must $f$ be linear?  To put it another way, does ZF + the existence of a nonlinear additive function imply the existence of Hamel basis of $\mathbb{R}$? I checked the Consequences of the Axiom of Choice Project, a database of choice axioms and their relationships here , and it said that it didn't know.",,"['analysis', 'logic', 'set-theory', 'axiom-of-choice']"
56,Can someone clarify Example I.I.2 from Hardy's Course of Pure Mathematics?,Can someone clarify Example I.I.2 from Hardy's Course of Pure Mathematics?,,"""If $\lambda, m,$ and $n$ are positive rational numbers, and $m > n$, then $\lambda(m^2 − n^2), 2\lambda mn$, and $\lambda(m^2 + n^2)$ are positive rational numbers. Hence show how to determine any number of right-angled triangles the lengths of all of whose sides are rational."" What does he mean by ""how to determine any number of right angled triangles""?","""If $\lambda, m,$ and $n$ are positive rational numbers, and $m > n$, then $\lambda(m^2 − n^2), 2\lambda mn$, and $\lambda(m^2 + n^2)$ are positive rational numbers. Hence show how to determine any number of right-angled triangles the lengths of all of whose sides are rational."" What does he mean by ""how to determine any number of right angled triangles""?",,"['analysis', 'rational-numbers']"
57,Isoperimetric inequality implies Wirtinger's inequality,Isoperimetric inequality implies Wirtinger's inequality,,"Let $C: x=x(t), y=y(t), a\le t\le b$ be a $C^1$ closed curve (not necessarily simple).The isoperimetric inequality says that  $$ A\le \frac{\ell^2}{4\pi},$$ where $$A=\left|\int_C y(t)x'(t) dt\right|$$ is the area enclosed by $C$, and  $\ell=\int_a^b \sqrt{(x'(t))^2+(y'(t))^2} dt$ is the arc length of $C$.  My question is how to use this theorem to prove  Wirtinger theorem: If $f(t)$ is a $T$-periodic $C^1$ real-valued function such that $\int_0^T f(t) dt=0,$ then $$\int_0^T |f(t)|^2 dt\le \frac{T^2}{4\pi^2}\int_0^T |f'(t)|^2 dt.$$","Let $C: x=x(t), y=y(t), a\le t\le b$ be a $C^1$ closed curve (not necessarily simple).The isoperimetric inequality says that  $$ A\le \frac{\ell^2}{4\pi},$$ where $$A=\left|\int_C y(t)x'(t) dt\right|$$ is the area enclosed by $C$, and  $\ell=\int_a^b \sqrt{(x'(t))^2+(y'(t))^2} dt$ is the arc length of $C$.  My question is how to use this theorem to prove  Wirtinger theorem: If $f(t)$ is a $T$-periodic $C^1$ real-valued function such that $\int_0^T f(t) dt=0,$ then $$\int_0^T |f(t)|^2 dt\le \frac{T^2}{4\pi^2}\int_0^T |f'(t)|^2 dt.$$",,"['analysis', 'fourier-analysis', 'inequality', 'fourier-series']"
58,Bounding the integral $\int_{2}^{x} \frac{\mathrm dt}{\log^{n}{t}}$,Bounding the integral,\int_{2}^{x} \frac{\mathrm dt}{\log^{n}{t}},"If $x \geq 2$, then how do we prove that $$\int_{2}^{x} \frac{\mathrm dt}{\log^{n}{t}} = O\Bigl(\frac{x}{\log^{n}{x}}\Bigr)?$$","If $x \geq 2$, then how do we prove that $$\int_{2}^{x} \frac{\mathrm dt}{\log^{n}{t}} = O\Bigl(\frac{x}{\log^{n}{x}}\Bigr)?$$",,['analysis']
59,Why don't we include $\pm\infty$ in $\mathbb R$?,Why don't we include  in ?,\pm\infty \mathbb R,"Why don't we include $\pm\infty$ in $\mathbb R$? If we do so, many equations will got real solution (e.g. $2^x=0$), and $\mathbb R$ will be much more complete. Why don't we do so? Thank you.","Why don't we include $\pm\infty$ in $\mathbb R$? If we do so, many equations will got real solution (e.g. $2^x=0$), and $\mathbb R$ will be much more complete. Why don't we do so? Thank you.",,['analysis']
60,"$\liminf, \limsup$ and continuous functions",and continuous functions,"\liminf, \limsup","If a function $f: \mathbb R \to \mathbb R$ is continuous in x, $f$ is sequentially continuous in x: $$ \lim_{n\rightarrow +\infty} x_n = x \Rightarrow \lim_{n\rightarrow +\infty} f(x_n)=f(x)$$ I'm wondering whether the same  is true with $\liminf$ or $\limsup$ instead of $\lim$ $$ \liminf_{n\rightarrow +\infty} x_n = x \Rightarrow \liminf_{n\rightarrow +\infty} f(x_n)=f(x)$$ I can't find a counterexample to prove that it is false.","If a function $f: \mathbb R \to \mathbb R$ is continuous in x, $f$ is sequentially continuous in x: $$ \lim_{n\rightarrow +\infty} x_n = x \Rightarrow \lim_{n\rightarrow +\infty} f(x_n)=f(x)$$ I'm wondering whether the same  is true with $\liminf$ or $\limsup$ instead of $\lim$ $$ \liminf_{n\rightarrow +\infty} x_n = x \Rightarrow \liminf_{n\rightarrow +\infty} f(x_n)=f(x)$$ I can't find a counterexample to prove that it is false.",,"['analysis', 'limsup-and-liminf']"
61,"If $E$ is Lebesgue measurable, show that there exists a closed set $F$ with $F \subset E$ and $m(E\setminus F)<\epsilon$","If  is Lebesgue measurable, show that there exists a closed set  with  and",E F F \subset E m(E\setminus F)<\epsilon,"Just having trouble with this problem. First, it says to prove that if $E$ is Lebesgue Measurable, and $\epsilon>0$ is arbitrary, then there is an open $O$ such that $E \subset O$ and $m(O\setminus E)<\epsilon$. Now for this part, since $E$ is Lebesgue measurable, I was able to take the definition of being Lebesgue outer measurable (the infimum of open coverings of $E$) and easily construct $O$ from that, and it works out. But now I want to show that there is an $F$ closed with $F \subset E$ and $M(E\setminus F)<\epsilon$. The problem is, I can't figure out how to construct this $F$! The idea seems obvious intuitively, but I feel like I am given no tools for constructing a closed subset of an arbitrary set that satisfies these conditions. Am I missing something obvious here? Thanks!!","Just having trouble with this problem. First, it says to prove that if $E$ is Lebesgue Measurable, and $\epsilon>0$ is arbitrary, then there is an open $O$ such that $E \subset O$ and $m(O\setminus E)<\epsilon$. Now for this part, since $E$ is Lebesgue measurable, I was able to take the definition of being Lebesgue outer measurable (the infimum of open coverings of $E$) and easily construct $O$ from that, and it works out. But now I want to show that there is an $F$ closed with $F \subset E$ and $M(E\setminus F)<\epsilon$. The problem is, I can't figure out how to construct this $F$! The idea seems obvious intuitively, but I feel like I am given no tools for constructing a closed subset of an arbitrary set that satisfies these conditions. Am I missing something obvious here? Thanks!!",,"['analysis', 'measure-theory']"
62,Eigenfunctions of the Laplace-Beltrami operator of a torus,Eigenfunctions of the Laplace-Beltrami operator of a torus,,"The eigenfunctions of the Laplace-Beltrami operator of the flat torus $\mathbb{T}^2=\mathbb{R}^2/\mathbb{Z}^2$ and their multiplicity are well-known. What happens if we change the sides of the torus and we consider two sides that are not commensurable, for example $\mathbb{T}_{1,\sqrt{2}}^2=\mathbb{R}^2/(\mathbb{Z}\times\sqrt{2}\mathbb{Z}$)? What are the eigenfunctions of the Laplace-Beltrami operator of $\mathbb{T}_{1,\sqrt{2}}^2$? and their multiplicity? Thank you in advance","The eigenfunctions of the Laplace-Beltrami operator of the flat torus $\mathbb{T}^2=\mathbb{R}^2/\mathbb{Z}^2$ and their multiplicity are well-known. What happens if we change the sides of the torus and we consider two sides that are not commensurable, for example $\mathbb{T}_{1,\sqrt{2}}^2=\mathbb{R}^2/(\mathbb{Z}\times\sqrt{2}\mathbb{Z}$)? What are the eigenfunctions of the Laplace-Beltrami operator of $\mathbb{T}_{1,\sqrt{2}}^2$? and their multiplicity? Thank you in advance",,"['analysis', 'differential-geometry', 'fourier-analysis']"
63,How does Lambert's W behave near ∞?,How does Lambert's W behave near ∞?,,"How does $W$ behave near $+\infty$ compared to $\log$?  In particular, I'm interested in the asymptotic expansion of $$\frac{W(x)}{\ln(x)}$$ near $\infty$ (but along the positive real line, if that matters).  Clearly $W(x)\sim\ln(x)$ as $x$ increases, and the next term looks hyperbolic.","How does $W$ behave near $+\infty$ compared to $\log$?  In particular, I'm interested in the asymptotic expansion of $$\frac{W(x)}{\ln(x)}$$ near $\infty$ (but along the positive real line, if that matters).  Clearly $W(x)\sim\ln(x)$ as $x$ increases, and the next term looks hyperbolic.",,"['analysis', 'asymptotics', 'special-functions']"
64,Can you string together inequalities and equalities into a single statement?,Can you string together inequalities and equalities into a single statement?,,"If I write a statement like $$|f(x) - f(y)| \leq K|x - y| < K \delta = K \cdot \frac{\varepsilon}{K} = \varepsilon,$$ is that appropriate? Context: I have a classmate who insists that ""mixing"" the equalities on the end of a statement that begins with inequalities somehow makes the statement inappropriate. I'm not sure if he thinks the above statement is literally wrong or (more likely) thinks it is somehow just stylistically unpleasant to the point where it should be avoided. My contention is that the statement is read from left to right and if it consists of a string of individually true statements, then the statement is true; and there isn't any other convenient way to write the statement, so it must be stylistically acceptable as well. Any rules, thoughts, or opinions?","If I write a statement like is that appropriate? Context: I have a classmate who insists that ""mixing"" the equalities on the end of a statement that begins with inequalities somehow makes the statement inappropriate. I'm not sure if he thinks the above statement is literally wrong or (more likely) thinks it is somehow just stylistically unpleasant to the point where it should be avoided. My contention is that the statement is read from left to right and if it consists of a string of individually true statements, then the statement is true; and there isn't any other convenient way to write the statement, so it must be stylistically acceptable as well. Any rules, thoughts, or opinions?","|f(x) - f(y)| \leq K|x - y| < K \delta = K \cdot \frac{\varepsilon}{K} = \varepsilon,","['analysis', 'inequality', 'proof-writing']"
65,Why is this number so close to $1$?,Why is this number so close to ?,1,"The only positive solution of the equation $\sin (\tan x) = x$ is at a number $a = 0.999906...$. Is it a coincidence that the number $a$ is so close to $1$, or is there a conceptual explanation? It was obvious $a$ was going to have to be less than $1$, but not this close. The answer seems to be related to $\tan 1 \approx \pi/2$, but that merely shifts the problem to explaining why that is the case. (I became interested in this question after reading this other question: Prove: $\sin (\tan x) \geq {x}$ ) Edit: I have thought about the question some more, and added some ideas in the form of an answer partially addressing why we have $\tan 1 \approx \pi/2$.","The only positive solution of the equation $\sin (\tan x) = x$ is at a number $a = 0.999906...$. Is it a coincidence that the number $a$ is so close to $1$, or is there a conceptual explanation? It was obvious $a$ was going to have to be less than $1$, but not this close. The answer seems to be related to $\tan 1 \approx \pi/2$, but that merely shifts the problem to explaining why that is the case. (I became interested in this question after reading this other question: Prove: $\sin (\tan x) \geq {x}$ ) Edit: I have thought about the question some more, and added some ideas in the form of an answer partially addressing why we have $\tan 1 \approx \pi/2$.",,"['analysis', 'approximation']"
66,Orders of Growth between Polynomial and Exponential,Orders of Growth between Polynomial and Exponential,,"What is known in contemporary mathematics about orders of growth for functions that exceed any degree polynomial, but fall short of exponential?   This is a subject for which I've found little literature in the past. An example: $Ae^{a\sqrt x}$ clearly will outrun any finite degree polynomial, but will be outrun by $Be^{bx}$. If we replace $x$ with $y^2$ then that example doesn't seem so deep. Are there functions that exceed polynomial growth yet fall short of $Ae^{ax^p}$ for any power $0<p<1$?     What classes of functions can we distinguish with different kinds of in-between orders of growth?   What can we know about their power series expansions, or behavior in the complex plane?   Those are examples of the kinds of questions I have, and would like to find literature on. Have any definitions or terminology been established concerning this?  The right jargon will facilitate searching.","What is known in contemporary mathematics about orders of growth for functions that exceed any degree polynomial, but fall short of exponential?   This is a subject for which I've found little literature in the past. An example: $Ae^{a\sqrt x}$ clearly will outrun any finite degree polynomial, but will be outrun by $Be^{bx}$. If we replace $x$ with $y^2$ then that example doesn't seem so deep. Are there functions that exceed polynomial growth yet fall short of $Ae^{ax^p}$ for any power $0<p<1$?     What classes of functions can we distinguish with different kinds of in-between orders of growth?   What can we know about their power series expansions, or behavior in the complex plane?   Those are examples of the kinds of questions I have, and would like to find literature on. Have any definitions or terminology been established concerning this?  The right jargon will facilitate searching.",,"['analysis', 'asymptotics', 'computational-complexity']"
67,Application of Dominated Convergence Theorem?,Application of Dominated Convergence Theorem?,,Find with proof the following limit: $$\lim_{n \to \infty} \int_{-\infty}^{\infty} \frac{(\sin(x))^n}{x^2}dx$$ I want to use the DCT but I cannot seem to dominate $f_{n}(x)=\frac{(\sin(x))^n}{x^2}$ by an integrable function. Any help would be appreciated.,Find with proof the following limit: $$\lim_{n \to \infty} \int_{-\infty}^{\infty} \frac{(\sin(x))^n}{x^2}dx$$ I want to use the DCT but I cannot seem to dominate $f_{n}(x)=\frac{(\sin(x))^n}{x^2}$ by an integrable function. Any help would be appreciated.,,"['analysis', 'measure-theory']"
68,The measure of the image of a set of measure zero,The measure of the image of a set of measure zero,,"Let $f$ be an absolutely continuously monotone function on $[0,1]$. Suppose $E$ has measure zero. How do I go about showing that the measure of $f(E)$ is zero? Thanks. Edit: Would this work? Since $f$ is absolutely continuous, for every $\varepsilon \gt 0$, there is a $\delta \gt 0$ such that for a family of non-overlapping intervals $\{[x_i,y_i]\}_i$ of $[0,1]$, we have $$ \sum_i(y_i - x_i)\lt \delta ~\implies ~ \sum_i |f(y_i)-f(x_i)|\lt \varepsilon.$$ Let $(x_k,y_k)\subset [x_i,y_i]$ cover $E$. then $E\subset \bigcup (x_k,y_k)$(disjoint) and $\sum(y_k-x_k)\lt \delta.$ Also, $$f(E)\subset f\left(\bigcup (x_i,y_i)\right)=\bigcup\left(f(x_k),f(y_k)\right)~,$$ and  $$ \mu(f(E))\leq \mu \left(\bigcup\left(f(x_k),f(y_k)\right)\right)=\sum_k \left|f(y_k)-f(x_k)\right|\lt \varepsilon.$$  Thus , $\mu(f(E))=0$.","Let $f$ be an absolutely continuously monotone function on $[0,1]$. Suppose $E$ has measure zero. How do I go about showing that the measure of $f(E)$ is zero? Thanks. Edit: Would this work? Since $f$ is absolutely continuous, for every $\varepsilon \gt 0$, there is a $\delta \gt 0$ such that for a family of non-overlapping intervals $\{[x_i,y_i]\}_i$ of $[0,1]$, we have $$ \sum_i(y_i - x_i)\lt \delta ~\implies ~ \sum_i |f(y_i)-f(x_i)|\lt \varepsilon.$$ Let $(x_k,y_k)\subset [x_i,y_i]$ cover $E$. then $E\subset \bigcup (x_k,y_k)$(disjoint) and $\sum(y_k-x_k)\lt \delta.$ Also, $$f(E)\subset f\left(\bigcup (x_i,y_i)\right)=\bigcup\left(f(x_k),f(y_k)\right)~,$$ and  $$ \mu(f(E))\leq \mu \left(\bigcup\left(f(x_k),f(y_k)\right)\right)=\sum_k \left|f(y_k)-f(x_k)\right|\lt \varepsilon.$$  Thus , $\mu(f(E))=0$.",,"['analysis', 'measure-theory']"
69,Equality in Minkowski's inequality proof(no integrals),Equality in Minkowski's inequality proof(no integrals),,"So what I'm looking for is a proof for when does the equality hold in Minkowski's inequality? I'm talking about this form of inequality: $$\left( \sum_{K=1}^n |x_k + y_k|^p \right)^{\frac{1}{p}} \leq \left( \sum_{k=1}^n |x_k|^p\right)^{\frac{1}{p}} + \left( \sum_{k=1}^n |y_k|^p\right)^{\frac{1}{p}}$$ Not the one using integrals. I know when does the equality hold, I just can't figure out how to prove it. It holds when (X1, ..., Xn) = c*(Y1, ..., Yn), where c is a real constant (this is similar to Holder's inequality which I used to prove Minkowski's inequality). So can I just point out that it similarly holds in Minkowski's case, as I suppose it does? Thanks. EDIT: I proved that if (X1, ..., Xn) = c*(Y1, ..., Yn) , then the equality holds. Now, how to do in reverse - if equality holds, then (X1, ..., Xn) = c*(Y1, ..., Yn) ? Because these are equivalent.","So what I'm looking for is a proof for when does the equality hold in Minkowski's inequality? I'm talking about this form of inequality: Not the one using integrals. I know when does the equality hold, I just can't figure out how to prove it. It holds when (X1, ..., Xn) = c*(Y1, ..., Yn), where c is a real constant (this is similar to Holder's inequality which I used to prove Minkowski's inequality). So can I just point out that it similarly holds in Minkowski's case, as I suppose it does? Thanks. EDIT: I proved that if (X1, ..., Xn) = c*(Y1, ..., Yn) , then the equality holds. Now, how to do in reverse - if equality holds, then (X1, ..., Xn) = c*(Y1, ..., Yn) ? Because these are equivalent.",\left( \sum_{K=1}^n |x_k + y_k|^p \right)^{\frac{1}{p}} \leq \left( \sum_{k=1}^n |x_k|^p\right)^{\frac{1}{p}} + \left( \sum_{k=1}^n |y_k|^p\right)^{\frac{1}{p}},"['analysis', 'inequality']"
70,Complete ordered field is an Archimedean field that cannot be extended to an Archimedean field,Complete ordered field is an Archimedean field that cannot be extended to an Archimedean field,,"As a bonus problem, our professor of real analysis asked us to prove that the real numbers (a complete ordered field) cannot be extended into an Archimedean field, with no definition of what he meant by extending. I have tried using proof by contradiction to show that if we have some set, $\mathbb{R}^{*}$ such that $\mathbb{R}$ is a proper subset of $\mathbb{R}^{*}$, and that $\mathbb{R}^{*}$ forms an Archimedean field, then because for there to be new elements in $\mathbb{R}^{*}$ as opposed to $\mathbb{R}$, they would have to be larger (or smaller) than all the elements of R. But then we would have reached a contradiction with the Archimedean property. Professor returned this solution and said it's not the right solution (without any further comment). Can anyone offer some enlightenment on what I have done wrong or what I could try now? All I have found on Google are mentions in textbooks that go along the lines of ""every Archimedean field is isomorphic to a subfield of real numbers"". That would imply that we cannot extend reals into an Archimedean field, but how can one go about proving that? I can only use basic definition of an ordered (Archimedean) field and other ""basics"", we have not yet covered sequences, etc.","As a bonus problem, our professor of real analysis asked us to prove that the real numbers (a complete ordered field) cannot be extended into an Archimedean field, with no definition of what he meant by extending. I have tried using proof by contradiction to show that if we have some set, $\mathbb{R}^{*}$ such that $\mathbb{R}$ is a proper subset of $\mathbb{R}^{*}$, and that $\mathbb{R}^{*}$ forms an Archimedean field, then because for there to be new elements in $\mathbb{R}^{*}$ as opposed to $\mathbb{R}$, they would have to be larger (or smaller) than all the elements of R. But then we would have reached a contradiction with the Archimedean property. Professor returned this solution and said it's not the right solution (without any further comment). Can anyone offer some enlightenment on what I have done wrong or what I could try now? All I have found on Google are mentions in textbooks that go along the lines of ""every Archimedean field is isomorphic to a subfield of real numbers"". That would imply that we cannot extend reals into an Archimedean field, but how can one go about proving that? I can only use basic definition of an ordered (Archimedean) field and other ""basics"", we have not yet covered sequences, etc.",,"['analysis', 'field-theory']"
71,Prove that $|k(x)|\le C|x|^{-n}$ under suitable hypothesis on $k\in\mathcal{C}^1(\Bbb R^n\setminus\{0\})$,Prove that  under suitable hypothesis on,|k(x)|\le C|x|^{-n} k\in\mathcal{C}^1(\Bbb R^n\setminus\{0\}),"DON'T BE AFRAID FROM THE +500 BOUNTY: it doesn't matter that I KNOW this problem is really hard, I put it only because I need to solve the problem really URGENTLY! Let $n\ge2$; given a kernel $k\in\mathcal{C}^1(\Bbb R^n\setminus\{0\})$ such that $$|\nabla k(x)|\le|x|^{-n-1}\;\;\; \mbox{for} \;\;x\neq0  \tag{1}$$   $$\int_{|x|=r}k(x)\,d\sigma_n(x)=0\;\;\;\;\;\forall r>0  \tag{2}$$ then I must prove that $$ |k(x)|\le C|x|^{-n}\;\;\;\;\mbox{for}\;\;x\neq 0. $$ Is this still true if the gradient condition $(1)$ is replaced with $$ |k(x+h)-k(x)|\le\frac{|h|^{\alpha}}{|x|^{n+\alpha}}\;\;\;\;\mbox{for}\;\;|h|<\frac12|x| \tag{3} $$ where $0<\alpha\le 1$? I tried to work with the convolution operator $A_Kf:=K*f$ and taking its Fourier transform to get some information, but I'm totally lost, I don't really know where to start","DON'T BE AFRAID FROM THE +500 BOUNTY: it doesn't matter that I KNOW this problem is really hard, I put it only because I need to solve the problem really URGENTLY! Let $n\ge2$; given a kernel $k\in\mathcal{C}^1(\Bbb R^n\setminus\{0\})$ such that $$|\nabla k(x)|\le|x|^{-n-1}\;\;\; \mbox{for} \;\;x\neq0  \tag{1}$$   $$\int_{|x|=r}k(x)\,d\sigma_n(x)=0\;\;\;\;\;\forall r>0  \tag{2}$$ then I must prove that $$ |k(x)|\le C|x|^{-n}\;\;\;\;\mbox{for}\;\;x\neq 0. $$ Is this still true if the gradient condition $(1)$ is replaced with $$ |k(x+h)-k(x)|\le\frac{|h|^{\alpha}}{|x|^{n+\alpha}}\;\;\;\;\mbox{for}\;\;|h|<\frac12|x| \tag{3} $$ where $0<\alpha\le 1$? I tried to work with the convolution operator $A_Kf:=K*f$ and taking its Fourier transform to get some information, but I'm totally lost, I don't really know where to start",,"['analysis', 'harmonic-analysis', 'singular-integrals']"
72,Integer parts of multiples of irrationals,Integer parts of multiples of irrationals,,"Let $\alpha>0$ and define  $$S(\alpha)=\big\{\lfloor n \alpha \rfloor: n\in\Bbb Z^+ \big\}.$$ Here $\lfloor x\rfloor$ is the integer part of $x$ and $\mathbb Z^+$ the set of positive integers. Question. Is it possible to find $\alpha,\beta,\gamma>0$, such that $$ S(\alpha)\cap S(\beta)= S(\beta) \cap S(\gamma) =S(\alpha)\cap S(\gamma) = \varnothing\text{?} $$ It is well-known that the following holds $$ S(\alpha)\cap S(\beta)=\varnothing\quad\,\text{and}\,\quad S(\alpha)\cup S(\beta)=\mathbb Z^+ $$ if and only if $\alpha$ and $\beta$ are irrational (and positive) and $\,\,\dfrac{1}{\alpha}+\dfrac{1}{\beta}=1$. Update 1. According to a 1995 Putnam examination question, there are no $\alpha$, $\beta$ and $\gamma$ for which $S(\alpha)$, $S(\beta)$ and $S(\gamma)$ form a partition of $\mathbb Z^+$. Update 2. The following result holds: $S(\alpha)\cap S(\beta)=\varnothing$ if and only $\alpha,\beta$ are irrational and if there exist positive integers $m,n$, such that $$ \frac{m}{\alpha}+\frac{n}{\beta}=1. $$ This is Theorem 8 in: Th. Skolem, On certain distributions of integers in pairs with given differences , Math. Scand., 5 (1957) 57-68.","Let $\alpha>0$ and define  $$S(\alpha)=\big\{\lfloor n \alpha \rfloor: n\in\Bbb Z^+ \big\}.$$ Here $\lfloor x\rfloor$ is the integer part of $x$ and $\mathbb Z^+$ the set of positive integers. Question. Is it possible to find $\alpha,\beta,\gamma>0$, such that $$ S(\alpha)\cap S(\beta)= S(\beta) \cap S(\gamma) =S(\alpha)\cap S(\gamma) = \varnothing\text{?} $$ It is well-known that the following holds $$ S(\alpha)\cap S(\beta)=\varnothing\quad\,\text{and}\,\quad S(\alpha)\cup S(\beta)=\mathbb Z^+ $$ if and only if $\alpha$ and $\beta$ are irrational (and positive) and $\,\,\dfrac{1}{\alpha}+\dfrac{1}{\beta}=1$. Update 1. According to a 1995 Putnam examination question, there are no $\alpha$, $\beta$ and $\gamma$ for which $S(\alpha)$, $S(\beta)$ and $S(\gamma)$ form a partition of $\mathbb Z^+$. Update 2. The following result holds: $S(\alpha)\cap S(\beta)=\varnothing$ if and only $\alpha,\beta$ are irrational and if there exist positive integers $m,n$, such that $$ \frac{m}{\alpha}+\frac{n}{\beta}=1. $$ This is Theorem 8 in: Th. Skolem, On certain distributions of integers in pairs with given differences , Math. Scand., 5 (1957) 57-68.",,"['number-theory', 'analysis', 'irrational-numbers', 'ceiling-and-floor-functions', 'set-partition']"
73,Prove the set of which sin(nx) converges has Lebesgue measure zero (from Baby Rudin Chapter 11),Prove the set of which sin(nx) converges has Lebesgue measure zero (from Baby Rudin Chapter 11),,"I am trying to work through Rudin. This is a question from chapter 11: Suppose that $\{n_k\}$ is an increasing sequence of positive integers and $E$ is the set of all $x$ in $(-\pi,\pi)$ at which $\sin(n_kx)$ converges. Prove that $E$ has Lebesgue measure zero. Hint: For every subset $A$ of $E$, $\int_A \sin (n_kx) dx$ tends to zero, and $2\int_{A} \sin^2 (n_k x)dx$ tends to the measure of $A$. So, if I can use the hint, I'm pretty sure I can get the question. The thing is, I have no idea how to prove the hint. Using the hint, here's how I think you do the question: Define $f$ on $E$ as  \begin{align*}f(x)= \lim_{k\rightarrow \infty} \sin(n_kx) \end{align*} Notice that $\sin(n_kx)\leq 1$, and $1\in L$ on $A\subset E$ for every measurable $A$ (since $E$ has finite measure), so Theorem $11.32$ in Rudin (the Lebesgue dominated convergence theorem) says \begin{align*} \int_A f(x) dx = \int_A \lim_{k\rightarrow \infty} \sin(n_kx) =  \lim_{k\rightarrow \infty} \int_A \sin(n_kx) = 0 \end{align*} (by the first hint). But this was true for all $A\subseteq E$, so by one of the questions on the last assignment, $f(x)=0$ almost everywhere on $E$ and so $f^2(x)=0$ almost everywhere on $E$. Using Theorem $11.32$ again, we get \begin{align*} \mu(A) = \lim_{k \rightarrow \infty} \int_A 2\sin^2(n_kx) = 2\int_A \lim_{k\rightarrow \infty}  \sin^2(n_kx)= 2 \int_A f^2(x)=0 \end{align*} Therefore $\mu(E)=0$. Does anybody know how to prove the hint? Thanks!","I am trying to work through Rudin. This is a question from chapter 11: Suppose that $\{n_k\}$ is an increasing sequence of positive integers and $E$ is the set of all $x$ in $(-\pi,\pi)$ at which $\sin(n_kx)$ converges. Prove that $E$ has Lebesgue measure zero. Hint: For every subset $A$ of $E$, $\int_A \sin (n_kx) dx$ tends to zero, and $2\int_{A} \sin^2 (n_k x)dx$ tends to the measure of $A$. So, if I can use the hint, I'm pretty sure I can get the question. The thing is, I have no idea how to prove the hint. Using the hint, here's how I think you do the question: Define $f$ on $E$ as  \begin{align*}f(x)= \lim_{k\rightarrow \infty} \sin(n_kx) \end{align*} Notice that $\sin(n_kx)\leq 1$, and $1\in L$ on $A\subset E$ for every measurable $A$ (since $E$ has finite measure), so Theorem $11.32$ in Rudin (the Lebesgue dominated convergence theorem) says \begin{align*} \int_A f(x) dx = \int_A \lim_{k\rightarrow \infty} \sin(n_kx) =  \lim_{k\rightarrow \infty} \int_A \sin(n_kx) = 0 \end{align*} (by the first hint). But this was true for all $A\subseteq E$, so by one of the questions on the last assignment, $f(x)=0$ almost everywhere on $E$ and so $f^2(x)=0$ almost everywhere on $E$. Using Theorem $11.32$ again, we get \begin{align*} \mu(A) = \lim_{k \rightarrow \infty} \int_A 2\sin^2(n_kx) = 2\int_A \lim_{k\rightarrow \infty}  \sin^2(n_kx)= 2 \int_A f^2(x)=0 \end{align*} Therefore $\mu(E)=0$. Does anybody know how to prove the hint? Thanks!",,"['analysis', 'measure-theory']"
74,"What does ""arg inf"" mean?","What does ""arg inf"" mean?",,I noticed this term on this post . But the term arg inf is not clearly defined.,I noticed this term on this post . But the term arg inf is not clearly defined.,,"['analysis', 'notation']"
75,Prerequisites for Baby Rudin,Prerequisites for Baby Rudin,,What are the prerequisites for going through Baby Rudin? Does one need calculus? I was wondering if I will be able to read it soon; I know basic precalc/calc.,What are the prerequisites for going through Baby Rudin? Does one need calculus? I was wondering if I will be able to read it soon; I know basic precalc/calc.,,['analysis']
76,"Polynomials $f$ with integer coefficients such that $f(x) \geq 0$ on $[-2,2]$ and $f(x) \leq \frac{1}{1+x}$ on $(-1,2]$",Polynomials  with integer coefficients such that  on  and  on,"f f(x) \geq 0 [-2,2] f(x) \leq \frac{1}{1+x} (-1,2]","Find polynomials with integer coefficients $f\in\mathbb{Z}[x]$ such that $f(x)\ge 0$ on $x\in[-2,2]$ and $\frac{1}{1+x}\ge f(x)$ on $x\in(-1,2]$. I guess only such polynomial is just $0$, but it seems hard to prove. I found an example $f(x)=x^2(x+1)(x-1)^2(x-2)^2$ satisfying $0\le f(x)\le \frac{1}{1+x}$ on $x\in(-1,2]$, but it satisfies $f(x)<0$ on $x\in [-2,-1)$. Are there nontrivial (i.e., other than the zero function) examples?","Find polynomials with integer coefficients $f\in\mathbb{Z}[x]$ such that $f(x)\ge 0$ on $x\in[-2,2]$ and $\frac{1}{1+x}\ge f(x)$ on $x\in(-1,2]$. I guess only such polynomial is just $0$, but it seems hard to prove. I found an example $f(x)=x^2(x+1)(x-1)^2(x-2)^2$ satisfying $0\le f(x)\le \frac{1}{1+x}$ on $x\in(-1,2]$, but it satisfies $f(x)<0$ on $x\in [-2,-1)$. Are there nontrivial (i.e., other than the zero function) examples?",,"['analysis', 'polynomials']"
77,Prove that $f$ is convex in an interval given an inequality with determinant,Prove that  is convex in an interval given an inequality with determinant,f,"Today,I found a interesting problem: if    $$\begin{vmatrix} \cos{x}&\sin{x}&f(x)\\ \cos{y}&\sin{y}&f(y)\\ \cos{z}&\sin{z}&f(z) \end{vmatrix}\ge 0$$   for all $x,y,z$ of an open interval $I$ for which $x<y<z<x+\pi$. show that:   $f(x)$ is convex on $I$","Today,I found a interesting problem: if    $$\begin{vmatrix} \cos{x}&\sin{x}&f(x)\\ \cos{y}&\sin{y}&f(y)\\ \cos{z}&\sin{z}&f(z) \end{vmatrix}\ge 0$$   for all $x,y,z$ of an open interval $I$ for which $x<y<z<x+\pi$. show that:   $f(x)$ is convex on $I$",,['analysis']
78,Minimal definition of the derivative,Minimal definition of the derivative,,"The definition of the Fréchet derivative according to Wikipedia is: Let $V$ and $W$ be Banach spaces, and $U\subset V$ be an open subset of $V$. A function $f : U \to W$  is called Fréchet differentiable at $x \in U$ if there exists a bounded linear operator $(Df)_x : V\to W$ such that $$\lim_{h \to 0} \frac{ \| f(x + h) - f(x) - (Df)_x(h) \|_{W} }{ \|h\|_{V} } = 0.$$ I am wondering if perhaps there is some more ""axiomatic"" interpretation of the derivative. For instance, could we define the derivative as the unique operator on differentiable functions such that The derivative operator is linear. That is, $D(cf) \equiv cD(f)$ and $D(f+g) \equiv D(f)+D(g)$. The chain rule holds. That is, $D(f \circ g)_x \equiv D(f)_{g(x)} \circ D(g)_x $. The product rule holds. That is, if $B : X \times Y \to Z$ is a continuous bilinear operator, then $(DB)_{(x,y)} (u,v) = B(u,y) + B(x,v)$. The derivative operator is the identity on bounded linear operators. That is, if $f: U \to V$ is defined by $f(x) = A(x)$ where $A$ is a bounded linear operator, then for all $x \in U$, $(Df)_x \equiv A$. Are these rules enough to precisely specify the derivative? Are there some additional rules of this sort that would provide an equivalent definition of the derivative? Are any of these rules redundant?","The definition of the Fréchet derivative according to Wikipedia is: Let $V$ and $W$ be Banach spaces, and $U\subset V$ be an open subset of $V$. A function $f : U \to W$  is called Fréchet differentiable at $x \in U$ if there exists a bounded linear operator $(Df)_x : V\to W$ such that $$\lim_{h \to 0} \frac{ \| f(x + h) - f(x) - (Df)_x(h) \|_{W} }{ \|h\|_{V} } = 0.$$ I am wondering if perhaps there is some more ""axiomatic"" interpretation of the derivative. For instance, could we define the derivative as the unique operator on differentiable functions such that The derivative operator is linear. That is, $D(cf) \equiv cD(f)$ and $D(f+g) \equiv D(f)+D(g)$. The chain rule holds. That is, $D(f \circ g)_x \equiv D(f)_{g(x)} \circ D(g)_x $. The product rule holds. That is, if $B : X \times Y \to Z$ is a continuous bilinear operator, then $(DB)_{(x,y)} (u,v) = B(u,y) + B(x,v)$. The derivative operator is the identity on bounded linear operators. That is, if $f: U \to V$ is defined by $f(x) = A(x)$ where $A$ is a bounded linear operator, then for all $x \in U$, $(Df)_x \equiv A$. Are these rules enough to precisely specify the derivative? Are there some additional rules of this sort that would provide an equivalent definition of the derivative? Are any of these rules redundant?",,"['analysis', 'derivatives']"
79,"If the generating function summation and zeta regularized sum of a divergent series exist, do they always coincide?","If the generating function summation and zeta regularized sum of a divergent series exist, do they always coincide?",,"One could assign a value to divergent series by means of several summation methods. One summation method we could consider is the generating function method. Let's sum, for example, the fibonacci series  by means of this method. We consider the generating function of the fibonacci series: $$ g(x) = \sum_{n=1}^{\infty} f_{n} x^{n},  $$ in which the $n$ 'th Fibonacci number is defined as: $f_{n} = f_{n-1} + f_{n-2} $ and $f_{0} = 0 $ and $f_{1} = 1$ . In the following wikipedia article it is explained (amongst many other things) that it is possible to find a closed-form for $g$ : $$g(x) = \frac{x}{1-x-x^2} . $$ Therefore, we could state that the divergent series summation by means of the generating function summation method $G$ amounts to taking the following limit: $$ G \Big{(} \sum_{n=1}^{\infty}f_{n} \Big{)} = \lim_{x \to 1} g(x) = -1 .  $$ We could, also try to assign a value to the sum of all Fibonacci numbers by means of zeta function regularisation. A closed form of the function $$ z(x) = \sum_{n=1}^{\infty} f_{n}^{-x} $$ can be found in equation $(5)$ of the following paper by Navas. (Who mistakenly asserts that he is finding the analytic continuation of the Fibonacci Dirichlet series. He is actually doing zeta function regularization of the Fibonacci series. The two methods are confused quite often, though.) He finds that $$ z(x) = 5^{x/2} \sum_{k=1}^{\infty} \binom{-x}{k} \frac{1}{\phi^{x+2k} + (-1)^{k+1} } . $$ The zeta regularized sum $R$ of the Fibonacci series is therefore $$ R \Big{(} \sum_{n=1}^{\infty} f_{n} \Big{)} = \lim_{x \to -1} z(x) = \frac{1}{\sqrt{5}} \Big{(} \frac{1}{\phi^{-1} -1} + \frac{1}{\phi + 1} \Big{)} = -1 .$$ We have thus found that the generating function summation and the zeta regularized sum of the Fibonacci series coincide (define $F := \sum_{n=1}^{\infty} f_{n} )$ : $$ G(F) = -1 = R(F) .$$ This does not always happen, though. If we define the sum of natural numbers $ N = \sum_{n =1}^{\infty} n $ , then $G (N) $ does not exist. This is because the corresponding generating function amounts to $$ p(x) = \sum_{n=1}^{\infty} n x^{n} = \frac{1}{ (1-x)^{2} } , $$ for which $\lim_{x \to 1 } p(x) $ does not exist. The zeta regularized sum does exist, however. We have the notorious equation $R(N) = - \frac{1}{12} $ (see this page). Question (1) is now: Do the $G$ and $R$ summation methods of a divergent series always coincide, if both methods lead to a finite number that can be assigned to the divergent series at hand? Question (2) : Is there a closed form of the actual analytic continuation of the Fibonacci dirichlet series $ d(x) = \sum_{n=1}^{\infty} \frac{ f_{n} }{ n^{x} } $ ? Bonus question: are there any references for collections of generating function expansion, zeta function analytic continuations and analytic continuations of dirichlet series? For the first group of functions there is the book Generatingfunctionology by Wilf, but I can't find any big overview papers/books/articles on the last two groups of functions.","One could assign a value to divergent series by means of several summation methods. One summation method we could consider is the generating function method. Let's sum, for example, the fibonacci series  by means of this method. We consider the generating function of the fibonacci series: in which the 'th Fibonacci number is defined as: and and . In the following wikipedia article it is explained (amongst many other things) that it is possible to find a closed-form for : Therefore, we could state that the divergent series summation by means of the generating function summation method amounts to taking the following limit: We could, also try to assign a value to the sum of all Fibonacci numbers by means of zeta function regularisation. A closed form of the function can be found in equation of the following paper by Navas. (Who mistakenly asserts that he is finding the analytic continuation of the Fibonacci Dirichlet series. He is actually doing zeta function regularization of the Fibonacci series. The two methods are confused quite often, though.) He finds that The zeta regularized sum of the Fibonacci series is therefore We have thus found that the generating function summation and the zeta regularized sum of the Fibonacci series coincide (define : This does not always happen, though. If we define the sum of natural numbers , then does not exist. This is because the corresponding generating function amounts to for which does not exist. The zeta regularized sum does exist, however. We have the notorious equation (see this page). Question (1) is now: Do the and summation methods of a divergent series always coincide, if both methods lead to a finite number that can be assigned to the divergent series at hand? Question (2) : Is there a closed form of the actual analytic continuation of the Fibonacci dirichlet series ? Bonus question: are there any references for collections of generating function expansion, zeta function analytic continuations and analytic continuations of dirichlet series? For the first group of functions there is the book Generatingfunctionology by Wilf, but I can't find any big overview papers/books/articles on the last two groups of functions."," g(x) = \sum_{n=1}^{\infty} f_{n} x^{n},   n f_{n} = f_{n-1} + f_{n-2}  f_{0} = 0  f_{1} = 1 g g(x) = \frac{x}{1-x-x^2} .  G  G \Big{(} \sum_{n=1}^{\infty}f_{n} \Big{)} = \lim_{x \to 1} g(x) = -1 .    z(x) = \sum_{n=1}^{\infty} f_{n}^{-x}  (5)  z(x) = 5^{x/2} \sum_{k=1}^{\infty} \binom{-x}{k} \frac{1}{\phi^{x+2k} + (-1)^{k+1} } .  R  R \Big{(} \sum_{n=1}^{\infty} f_{n} \Big{)} = \lim_{x \to -1} z(x) = \frac{1}{\sqrt{5}} \Big{(} \frac{1}{\phi^{-1} -1} + \frac{1}{\phi + 1} \Big{)} = -1 . F := \sum_{n=1}^{\infty} f_{n} )  G(F) = -1 = R(F) .  N = \sum_{n =1}^{\infty} n  G (N)   p(x) = \sum_{n=1}^{\infty} n x^{n} = \frac{1}{ (1-x)^{2} } ,  \lim_{x \to 1 } p(x)  R(N) = - \frac{1}{12}  G R  d(x) = \sum_{n=1}^{\infty} \frac{ f_{n} }{ n^{x} } ","['analysis', 'fibonacci-numbers', 'divergent-series']"
80,characteristic curves for second-order equations,characteristic curves for second-order equations,,"Reading about characteristic curves for second-order equations, in particular semi-linear equations of second order with two independent variables: $a(x,y)u_{xx}+2b(x,y)u_{xy}+c(x,y)u_{y,y}=f(x,y,u,u_{x},u_{y})$     $   $  $  $    $(1)$ My book reference, define characteristic curve to $(1)$, as plane curves along which the PDE can be written in a form containing only total derivatives of $u_{x}$ and $u_{y}$. I do not understand this definition ( only total derivatives of $u_{x}$ and $u_{y}$??? ), I do not know how to see it this, for example, in equation $xu_{xx}+2xu_{xy}+xu_{yy}=u_{x}+u_{y}$. I read some questions about characteristic curves here, but did not help. Can anyone help me? Thank you.","Reading about characteristic curves for second-order equations, in particular semi-linear equations of second order with two independent variables: $a(x,y)u_{xx}+2b(x,y)u_{xy}+c(x,y)u_{y,y}=f(x,y,u,u_{x},u_{y})$     $   $  $  $    $(1)$ My book reference, define characteristic curve to $(1)$, as plane curves along which the PDE can be written in a form containing only total derivatives of $u_{x}$ and $u_{y}$. I do not understand this definition ( only total derivatives of $u_{x}$ and $u_{y}$??? ), I do not know how to see it this, for example, in equation $xu_{xx}+2xu_{xy}+xu_{yy}=u_{x}+u_{y}$. I read some questions about characteristic curves here, but did not help. Can anyone help me? Thank you.",,"['analysis', 'partial-differential-equations']"
81,Is my proof of differentiability correct?,Is my proof of differentiability correct?,,"Exercise : Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be defined by $$f(x) =\begin{cases}x^2 & x \in \mathbb{Q} \\x^3 & x \in \mathbb{R - Q}   \end{cases}$$ for $x \in \mathbb{R}$. Using only the limit definition of derivatives, determine whether or not $f$ is differentiable at $0$. I think it is. Consider the difference quotient function $\phi:(-1, 1) - \{0\} \rightarrow \mathbb{R}$ defined by $$\phi(x) = \frac{f(x) - f(0)}{x - 0}$$ for $x \in (-1, 1) - \{0\}$. On this domain, we can say that $$\phi(x) = \begin{cases}x & x \in \mathbb{Q} \\x^2 & x \in \mathbb{R - Q}   \end{cases}$$ and $0 <|\phi(x)| \leq |x|$ on $(-1, 1)$ so $\lim_{x \to 0}\phi(x) = 0$ since $\lim_{x \to 0} |x| = 0$ implies $\lim_{x \to 0}|\phi(x)| = 0$. Hence, $f$ is differentiable at $0$ and $f'(0) = 0$.","Exercise : Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be defined by $$f(x) =\begin{cases}x^2 & x \in \mathbb{Q} \\x^3 & x \in \mathbb{R - Q}   \end{cases}$$ for $x \in \mathbb{R}$. Using only the limit definition of derivatives, determine whether or not $f$ is differentiable at $0$. I think it is. Consider the difference quotient function $\phi:(-1, 1) - \{0\} \rightarrow \mathbb{R}$ defined by $$\phi(x) = \frac{f(x) - f(0)}{x - 0}$$ for $x \in (-1, 1) - \{0\}$. On this domain, we can say that $$\phi(x) = \begin{cases}x & x \in \mathbb{Q} \\x^2 & x \in \mathbb{R - Q}   \end{cases}$$ and $0 <|\phi(x)| \leq |x|$ on $(-1, 1)$ so $\lim_{x \to 0}\phi(x) = 0$ since $\lim_{x \to 0} |x| = 0$ implies $\lim_{x \to 0}|\phi(x)| = 0$. Hence, $f$ is differentiable at $0$ and $f'(0) = 0$.",,"['analysis', 'derivatives']"
82,Defining division by zero [duplicate],Defining division by zero [duplicate],,"This question already has answers here : Why not to extend the set of natural numbers to make it closed under division by zero? (8 answers) Closed 10 years ago . I have looked through some of the previous questions posted on this topic, and I think mine is different. Is there a flaw in defining division by zero? For example, define $\frac{a}{0} = \infty_a$ it would seem like things work now, for example, $\frac{a/0}{b/0}=\frac{\infty_a}{\infty_b}=a/b$. What could go wrong with this idea, or more specifically, is it defined in some branch of mathematics?","This question already has answers here : Why not to extend the set of natural numbers to make it closed under division by zero? (8 answers) Closed 10 years ago . I have looked through some of the previous questions posted on this topic, and I think mine is different. Is there a flaw in defining division by zero? For example, define $\frac{a}{0} = \infty_a$ it would seem like things work now, for example, $\frac{a/0}{b/0}=\frac{\infty_a}{\infty_b}=a/b$. What could go wrong with this idea, or more specifically, is it defined in some branch of mathematics?",,"['analysis', 'arithmetic']"
83,"A compact set, which is not closed.","A compact set, which is not closed.",,"I'm looking for a compact set, which is not closed. I read somewhere that $Z^+$ are compact and not closed, but I don't understand why. Are there any other examples of compact sets that are not closed and could you please explain? I know that we can't look in the reals because every compact set in the reals is closed and bounded correct?","I'm looking for a compact set, which is not closed. I read somewhere that $Z^+$ are compact and not closed, but I don't understand why. Are there any other examples of compact sets that are not closed and could you please explain? I know that we can't look in the reals because every compact set in the reals is closed and bounded correct?",,"['analysis', 'compactness']"
84,Why is interchanging the order of limits in this situation equivalent to asking for continuity?,Why is interchanging the order of limits in this situation equivalent to asking for continuity?,,"The following is an excerpt from Rudin's book in mathematical analysis. Here he states: The part highlighted in red is the one I can't seem to wrap my head around. I thought that if we wanted to know whether the limit, say $f$, of a sequence of functions, say $f_n$, is continuous or not then we would just need: $$\lim_{t\to x} (\lim_{n \to \infty}f_n(t)) = f(x)$$ I.e. just that the limit of functions $f_n$, assumed to be $f$, is continuous by definition. So I don't understand the right hand side of the equation in red. Can somebody explain this?","The following is an excerpt from Rudin's book in mathematical analysis. Here he states: The part highlighted in red is the one I can't seem to wrap my head around. I thought that if we wanted to know whether the limit, say $f$, of a sequence of functions, say $f_n$, is continuous or not then we would just need: $$\lim_{t\to x} (\lim_{n \to \infty}f_n(t)) = f(x)$$ I.e. just that the limit of functions $f_n$, assumed to be $f$, is continuous by definition. So I don't understand the right hand side of the equation in red. Can somebody explain this?",,['analysis']
85,$\pi + e$ is rational or $\pi-e$ is rational,is rational or  is rational,\pi + e \pi-e,"I was asked to find the truth value of the statement: $$ \pi + e \; \text{ is rational or } \pi - e\; \text{ is rational } $$ I am only allowed to use the fact that $\pi, e $ are irrational numbers and cannot use the theory of transcendental numbers. Cannot proceed. any help would be appreciated.","I was asked to find the truth value of the statement: $$ \pi + e \; \text{ is rational or } \pi - e\; \text{ is rational } $$ I am only allowed to use the fact that $\pi, e $ are irrational numbers and cannot use the theory of transcendental numbers. Cannot proceed. any help would be appreciated.",,"['analysis', 'pi']"
86,Definition of tangent cone in continuous optimization .,Definition of tangent cone in continuous optimization .,,"Looking at the definition of tangent cone in continuous optimization : If $M$ is a open subset of $\mathbb R^n$ $x \in M$, The tangent cone of $M$ at $x$ is defined by  $$\mathbb T (M, x) = \big\{d \in \mathbb R^n |   \exists x_k \subset M , \eta_k \subset \mathbb R   : \eta_k \to 0  , x_k \to x \text{ and } \frac{x_k-x}{\eta_k} \to d\big\} $$ I don't understand the definition , what is actually happening here ? Can anyone explain me elaborately . I kind of make sense if i consider $M=\{y : y=x^2\} \subset \mathbb R^2$ . But for a general set $M$ i don't get the idea.","Looking at the definition of tangent cone in continuous optimization : If $M$ is a open subset of $\mathbb R^n$ $x \in M$, The tangent cone of $M$ at $x$ is defined by  $$\mathbb T (M, x) = \big\{d \in \mathbb R^n |   \exists x_k \subset M , \eta_k \subset \mathbb R   : \eta_k \to 0  , x_k \to x \text{ and } \frac{x_k-x}{\eta_k} \to d\big\} $$ I don't understand the definition , what is actually happening here ? Can anyone explain me elaborately . I kind of make sense if i consider $M=\{y : y=x^2\} \subset \mathbb R^2$ . But for a general set $M$ i don't get the idea.",,"['analysis', 'optimization', 'nonlinear-optimization']"
87,Show that $\sqrt{4+2\sqrt{3}}-\sqrt{3}$ is rational using the rational zeros theorem,Show that  is rational using the rational zeros theorem,\sqrt{4+2\sqrt{3}}-\sqrt{3},"What I've done so far: Let $$r = \sqrt{4+2\sqrt{3}}-\sqrt{3}.$$ Thus, $$r^2 = 2\sqrt{3}-2\sqrt{3}\sqrt{4+2\sqrt{3}}+7$$ and $$r^4=52\sqrt{3}-28\sqrt{3}\sqrt{4+2\sqrt{3}}-24\sqrt{4+2\sqrt{3}}+109.$$ I did this because in a similar example in class, we related $r^2$ and $r^4$ to find a polynomial such that $mr^4+nr^2 = 0$ for some integers $m,n$. However, I cannot find such relation here. Am I doing this right or is there another approach to these type of problems.","What I've done so far: Let $$r = \sqrt{4+2\sqrt{3}}-\sqrt{3}.$$ Thus, $$r^2 = 2\sqrt{3}-2\sqrt{3}\sqrt{4+2\sqrt{3}}+7$$ and $$r^4=52\sqrt{3}-28\sqrt{3}\sqrt{4+2\sqrt{3}}-24\sqrt{4+2\sqrt{3}}+109.$$ I did this because in a similar example in class, we related $r^2$ and $r^4$ to find a polynomial such that $mr^4+nr^2 = 0$ for some integers $m,n$. However, I cannot find such relation here. Am I doing this right or is there another approach to these type of problems.",,"['analysis', 'proof-verification']"
88,Why does $L^2$ convergence not imply almost sure convergence,Why does  convergence not imply almost sure convergence,L^2,"What's wrong with this argument? Let $f_n$ be a sequence of functions such that $f_n \to f$ in $L^2(\Omega)$. This means $$\lVert f_n - f \rVert_{L^2(\Omega)} \to 0,$$ i.e., $$\int_\Omega(f_n - f)^2 \to 0.$$ Since the integrand is positive, this must mean that $f_n \to f$ a.e. Why is this not true? Apparently this only true for a subsequence $f_n$ (and in all $L^p$ spaces).","What's wrong with this argument? Let $f_n$ be a sequence of functions such that $f_n \to f$ in $L^2(\Omega)$. This means $$\lVert f_n - f \rVert_{L^2(\Omega)} \to 0,$$ i.e., $$\int_\Omega(f_n - f)^2 \to 0.$$ Since the integrand is positive, this must mean that $f_n \to f$ a.e. Why is this not true? Apparently this only true for a subsequence $f_n$ (and in all $L^p$ spaces).",,['analysis']
89,"Proof of the following inequality $ \frac{x - y}{\log x - \log y} > \sqrt{xy} $, $x>y$.","Proof of the following inequality , .", \frac{x - y}{\log x - \log y} > \sqrt{xy}  x>y,"I have seen the following inequality  $$ \frac{x - y}{\log x - \log y} > \sqrt{xy} \ , \quad \forall x>y $$ be stated as a near ""obvious"" fact in another question , on the site.  The inequality is very cute, but so far I have not been able to prove it.  It reminds me of the Lipschitz inequality, but has some minor differences. Also Jensens inequality comes to mind. Is there something obvious I am missing, or is this ineqality not as easy to prove as it looks?","I have seen the following inequality  $$ \frac{x - y}{\log x - \log y} > \sqrt{xy} \ , \quad \forall x>y $$ be stated as a near ""obvious"" fact in another question , on the site.  The inequality is very cute, but so far I have not been able to prove it.  It reminds me of the Lipschitz inequality, but has some minor differences. Also Jensens inequality comes to mind. Is there something obvious I am missing, or is this ineqality not as easy to prove as it looks?",,"['analysis', 'inequality']"
90,Equivalence to the prime number theorem,Equivalence to the prime number theorem,,"I was just reading this question and answer: How will this equation imply PNT and it raised a whole new question: Given that  $\sum_{n\le x} \Lambda(n)=x+o(x)$, prove that $$\sum_{n\le x} \frac{\Lambda(n)}{n}=\log x-\gamma +o(1),$$ where gamma is the Euler constant. The above question is for the bounty.  The full version above has been bothering me.  Gerry Myerson pointed out we can prove $\sum_{n\leq x } \frac{\Lambda(n)}{n} =\log x +O(1)$ from only chebyshev estimate.  But this is not my question. My Attempts: I tried partial summation I end up with something like $\sum_{n} \frac{\psi(n)-n}{n^2}$.  for that to converge $o(n)$ isn't strong enough.  To do it my way, I would need to assume $$\psi(x)-x = O\left(\frac{x}{(\log x)^{1+\epsilon}}\right)$$ so that it converges.  (Otherwise it could be as big as $\log x$ which is no good) Can we prove the above estimate using only the basic prime number theorem $\psi(x)-x=o(x)$?  Why or why not?  Thank you! Please note that my question is about a subtlety with converge, and blunt partial summation doesn't seem to work.","I was just reading this question and answer: How will this equation imply PNT and it raised a whole new question: Given that  $\sum_{n\le x} \Lambda(n)=x+o(x)$, prove that $$\sum_{n\le x} \frac{\Lambda(n)}{n}=\log x-\gamma +o(1),$$ where gamma is the Euler constant. The above question is for the bounty.  The full version above has been bothering me.  Gerry Myerson pointed out we can prove $\sum_{n\leq x } \frac{\Lambda(n)}{n} =\log x +O(1)$ from only chebyshev estimate.  But this is not my question. My Attempts: I tried partial summation I end up with something like $\sum_{n} \frac{\psi(n)-n}{n^2}$.  for that to converge $o(n)$ isn't strong enough.  To do it my way, I would need to assume $$\psi(x)-x = O\left(\frac{x}{(\log x)^{1+\epsilon}}\right)$$ so that it converges.  (Otherwise it could be as big as $\log x$ which is no good) Can we prove the above estimate using only the basic prime number theorem $\psi(x)-x=o(x)$?  Why or why not?  Thank you! Please note that my question is about a subtlety with converge, and blunt partial summation doesn't seem to work.",,"['number-theory', 'analysis', 'prime-numbers', 'asymptotics', 'analytic-number-theory']"
91,Every collection of disjoint non-empty open subsets of $\mathbb{R}$ is countable?,Every collection of disjoint non-empty open subsets of  is countable?,\mathbb{R},"This is my first question on the forum. I'm wondering if the following proof is valid. Proof: Let $\{A_\lambda\}_{\lambda \in L}$ be an arbitrary collection of disjoint non-empty open subsets of $\mathbb{R}$. Since every non-empty open subset of $\mathbb{R}$ can be written uniquely as a countable union of disjoint open intervals, we can take the union $A = \bigcup\limits_{\lambda \in L}A_\lambda$ and decompose it $A = \bigcup\limits_{n \in \mathbb{N}} I_n$ in disjoint open intervals which forms a countable collection. We can also decompose each $A_\lambda$ as $\bigcup\limits_{m \in \mathbb{N}}J_{\lambda,m}$. For $\lambda \neq \mu \in L$, $A_\lambda \cap A_\mu = \emptyset$ and this is a new representation of $A$: $$A = \bigcup_{n \in \mathbb{N}} I_n = \bigcup_{\substack{\lambda \in L \\ m \in \mathbb{N}}} J_{\lambda,m}$$ No matter how complicated the union over $L$ is, the $J_{\lambda,m}$ are disjoint open intervals. Thus, by the uniqueness, the two collections are exactly the same. As the final argument, we produce an injection $\varphi:\{A_\lambda\} \mapsto \{J_{\lambda,m}\}$ picking for each $A_\lambda$ some $J_{\lambda,m}$. I can't see any fault, but the result seems incredibly strong to me. P.S.: stack exchange has some bug related to \bigcup and \bigcap symbols?","This is my first question on the forum. I'm wondering if the following proof is valid. Proof: Let $\{A_\lambda\}_{\lambda \in L}$ be an arbitrary collection of disjoint non-empty open subsets of $\mathbb{R}$. Since every non-empty open subset of $\mathbb{R}$ can be written uniquely as a countable union of disjoint open intervals, we can take the union $A = \bigcup\limits_{\lambda \in L}A_\lambda$ and decompose it $A = \bigcup\limits_{n \in \mathbb{N}} I_n$ in disjoint open intervals which forms a countable collection. We can also decompose each $A_\lambda$ as $\bigcup\limits_{m \in \mathbb{N}}J_{\lambda,m}$. For $\lambda \neq \mu \in L$, $A_\lambda \cap A_\mu = \emptyset$ and this is a new representation of $A$: $$A = \bigcup_{n \in \mathbb{N}} I_n = \bigcup_{\substack{\lambda \in L \\ m \in \mathbb{N}}} J_{\lambda,m}$$ No matter how complicated the union over $L$ is, the $J_{\lambda,m}$ are disjoint open intervals. Thus, by the uniqueness, the two collections are exactly the same. As the final argument, we produce an injection $\varphi:\{A_\lambda\} \mapsto \{J_{\lambda,m}\}$ picking for each $A_\lambda$ some $J_{\lambda,m}$. I can't see any fault, but the result seems incredibly strong to me. P.S.: stack exchange has some bug related to \bigcup and \bigcap symbols?",,"['analysis', 'elementary-set-theory']"
92,Proof of the product rule for the divergence,Proof of the product rule for the divergence,,"How can I prove that $\nabla \cdot (fv) = \nabla f \cdot v + f\nabla \cdot v,$ where $v$ is a vector field and $f$ a scalar valued function? Many thanks for the help!","How can I prove that $\nabla \cdot (fv) = \nabla f \cdot v + f\nabla \cdot v,$ where $v$ is a vector field and $f$ a scalar valued function? Many thanks for the help!",,"['analysis', 'operator-theory']"
93,Showing that a totally bounded set is relatively compact (closure is compact),Showing that a totally bounded set is relatively compact (closure is compact),,"I have been tasked with showing that for a metric space $(X,d)$, a subset $E \subseteq X$ is relatively compact $\iff$ $E$ is totally bounded. I believe I have shown the forward implication $(\Rightarrow)$. However, I'm struggling to show the backward implication. For clarity, a set $E$ is said to be relatively compact if its closure $\overline{E}$ is compact. Attempt at a proof: Assume $E$ is totally bounded. Then by a corollary, we know that every sequence in $E$ has a Cauchy subsequence. Since $\overline{E}$ contains all limits points of $E$, we know that every Cauchy subsequence converges to some point in $\overline{E}$. Then $\overline{E}$ is sequentially compact (every sequence has a convergent subsequence). This implies that $\overline{E}$ is compact, and therefore $E$ is relatively compact. My issue arises from the fact that I'm not considering what happens to sequences living inside of $\overline{E}\backslash E = \partial E$. I don't know if those converge or have Cauchy subsequences or anything. I only know things about sequences in $E$. Maybe I'm missing something obvious, or perhaps this entire proof is offbase. Any help would be appreciated.","I have been tasked with showing that for a metric space $(X,d)$, a subset $E \subseteq X$ is relatively compact $\iff$ $E$ is totally bounded. I believe I have shown the forward implication $(\Rightarrow)$. However, I'm struggling to show the backward implication. For clarity, a set $E$ is said to be relatively compact if its closure $\overline{E}$ is compact. Attempt at a proof: Assume $E$ is totally bounded. Then by a corollary, we know that every sequence in $E$ has a Cauchy subsequence. Since $\overline{E}$ contains all limits points of $E$, we know that every Cauchy subsequence converges to some point in $\overline{E}$. Then $\overline{E}$ is sequentially compact (every sequence has a convergent subsequence). This implies that $\overline{E}$ is compact, and therefore $E$ is relatively compact. My issue arises from the fact that I'm not considering what happens to sequences living inside of $\overline{E}\backslash E = \partial E$. I don't know if those converge or have Cauchy subsequences or anything. I only know things about sequences in $E$. Maybe I'm missing something obvious, or perhaps this entire proof is offbase. Any help would be appreciated.",,"['analysis', 'metric-spaces', 'compactness', 'cauchy-sequences']"
94,Prove that the equation: $c_0+c_1x+\ldots+c_nx^n=0$ has a real solution between 0 and 1. [duplicate],Prove that the equation:  has a real solution between 0 and 1. [duplicate],c_0+c_1x+\ldots+c_nx^n=0,"This question already has an answer here : Prove existence of a real root. (1 answer) Closed 4 years ago . Let $c_0,c_1,c_2,\ldots ,c_n$ be constants such that : $$c_0+\frac{c_1}{2}+\ldots+\frac{c_{n-1}}{n}+\frac{c_n}{n+1}=0$$ I have to prove that the equation: $$c_0+c_1x+\ldots+c_nx^n=0$$ Has a real solution between 0 and 1. Didn't know how to start...I thought that maybe I could use something about derivative...But I'm lost... Any help,much appreciated!!!","This question already has an answer here : Prove existence of a real root. (1 answer) Closed 4 years ago . Let $c_0,c_1,c_2,\ldots ,c_n$ be constants such that : $$c_0+\frac{c_1}{2}+\ldots+\frac{c_{n-1}}{n}+\frac{c_n}{n+1}=0$$ I have to prove that the equation: $$c_0+c_1x+\ldots+c_nx^n=0$$ Has a real solution between 0 and 1. Didn't know how to start...I thought that maybe I could use something about derivative...But I'm lost... Any help,much appreciated!!!",,['analysis']
95,"Proving that if $f'$ has at most $n-1$ zeros, then $f$ has at most $n$ zeros","Proving that if  has at most  zeros, then  has at most  zeros",f' n-1 f n,"Is this proof correct? The problem is the following. Let $n$ be a natural number. Suppose that the function $f:\mathbb{R}\to\mathbb{R}$ is differentiable and that the following equation has at most $n-1$ solutions:   $$f'(x)=0, \quad x \in \mathbb{R}.$$   Prove that the following equation has at most $n$ solutions:   $$f(x)=0,\quad x \in \mathbb{R}.$$ My proof is: Let $f'(x)=0$ have solutions $x_1$, $x_2,\ldots ,x_{n-1}$. since  $$f'(x_1)=\lim\limits_{{x}\to{x_1}}\frac{f(x)-f(x_1)}{x-x_1} = 0$$ and doing so, $f(x)=f(x_{n-1})$ Therefore $f(x)=f(x_1)=f(x_2)=\cdots=f(x_{n-1})$. Let $x_n$ be one of solutions of $f(x)=0$. $$0=f(x_n)=f(x_1)=f(x_2)=\cdots=f(x_{n-1}),$$  so $f(x)=0$ has solutions like $x_1$, $x_2,\ldots, x_n$. If there is wrong part, please let me know.","Is this proof correct? The problem is the following. Let $n$ be a natural number. Suppose that the function $f:\mathbb{R}\to\mathbb{R}$ is differentiable and that the following equation has at most $n-1$ solutions:   $$f'(x)=0, \quad x \in \mathbb{R}.$$   Prove that the following equation has at most $n$ solutions:   $$f(x)=0,\quad x \in \mathbb{R}.$$ My proof is: Let $f'(x)=0$ have solutions $x_1$, $x_2,\ldots ,x_{n-1}$. since  $$f'(x_1)=\lim\limits_{{x}\to{x_1}}\frac{f(x)-f(x_1)}{x-x_1} = 0$$ and doing so, $f(x)=f(x_{n-1})$ Therefore $f(x)=f(x_1)=f(x_2)=\cdots=f(x_{n-1})$. Let $x_n$ be one of solutions of $f(x)=0$. $$0=f(x_n)=f(x_1)=f(x_2)=\cdots=f(x_{n-1}),$$  so $f(x)=0$ has solutions like $x_1$, $x_2,\ldots, x_n$. If there is wrong part, please let me know.",,['analysis']
96,How to show the intersection of arbitrary compact sets is compact in a general metric space?,How to show the intersection of arbitrary compact sets is compact in a general metric space?,,"I understand that if you are working in $\mathbb{R}^n$, then the intersection of an arbitrary collection of compact sets is compact because it is closed and bounded. But what if you are not in $\mathbb{R}^n$? How can you show that the intersection of arbitrary compact sets in a general metric space is compact? This is homework so I only need a hint. Thank you!","I understand that if you are working in $\mathbb{R}^n$, then the intersection of an arbitrary collection of compact sets is compact because it is closed and bounded. But what if you are not in $\mathbb{R}^n$? How can you show that the intersection of arbitrary compact sets in a general metric space is compact? This is homework so I only need a hint. Thank you!",,['analysis']
97,Differentiability of the distance function,Differentiability of the distance function,,"Suppose that $d:X \times X \to \mathbb{R}$ is a geodesic distance function on a smooth Riemannian manifold $X$ ($d$ is determined by metric tensor) and $x \in X$ is fixed. What can be said about the points in which $f:=d(x,\cdot)$ is differentiable? In particular: -is it possible that $f$ is everywhere differentiable? -how big the set of points in which $f$ isn't differentiable can be? -what can be said about the structure of the set of point in which $f$ isn't differentiable (can it be a submanifold?) I would be grateful if anybody could shed some light on these issues.","Suppose that $d:X \times X \to \mathbb{R}$ is a geodesic distance function on a smooth Riemannian manifold $X$ ($d$ is determined by metric tensor) and $x \in X$ is fixed. What can be said about the points in which $f:=d(x,\cdot)$ is differentiable? In particular: -is it possible that $f$ is everywhere differentiable? -how big the set of points in which $f$ isn't differentiable can be? -what can be said about the structure of the set of point in which $f$ isn't differentiable (can it be a submanifold?) I would be grateful if anybody could shed some light on these issues.",,"['analysis', 'differential-geometry', 'metric-spaces', 'riemannian-geometry']"
98,Wrong Wolfram alpha result?,Wrong Wolfram alpha result?,,"I have this function  $$  f(x,y) = \left\{ 	\begin{array}{ll} 		\frac{x^3}{x^2 + y^2}  & \mbox{if } (x,y) \neq (0,0) \\ 		0 & \mbox{if } (x,y) = (0,0) 	\end{array} \right. $$ And I want to find the directional derivative in the $(1,1)$ direction at $(0,0)$, so using the limit definition this gives me $$ \begin{align*} 	D_vf(0,0) = \lim_{t\to 0}\frac{f((0,0) + t(1,1)) - f(0,0)}{t} = \lim_{t\to 0}\frac{t^3}{2t^2 t} = \frac{1}{2} \end{align*} $$ But wolfram alpha is giving me $0$ as a result Wolfram alpha result Is there something wrong with my procedure? Any help appreciated :)","I have this function  $$  f(x,y) = \left\{ 	\begin{array}{ll} 		\frac{x^3}{x^2 + y^2}  & \mbox{if } (x,y) \neq (0,0) \\ 		0 & \mbox{if } (x,y) = (0,0) 	\end{array} \right. $$ And I want to find the directional derivative in the $(1,1)$ direction at $(0,0)$, so using the limit definition this gives me $$ \begin{align*} 	D_vf(0,0) = \lim_{t\to 0}\frac{f((0,0) + t(1,1)) - f(0,0)}{t} = \lim_{t\to 0}\frac{t^3}{2t^2 t} = \frac{1}{2} \end{align*} $$ But wolfram alpha is giving me $0$ as a result Wolfram alpha result Is there something wrong with my procedure? Any help appreciated :)",,"['analysis', 'derivatives', 'wolfram-alpha']"
99,Defining Measures on a Manifold: How To,Defining Measures on a Manifold: How To,,"Everyone: Given smooth manifolds $M,N$ ($m$- and $n$- manifolds respectively) Sard's  theorem says that for $f:M \to N$ in $C^k$ ; $k \geq 1$, the image of the set of  critical points of $f$ in $M$ --points  in $M$ where the Jacobian has rank $< m$ --has measure zero in N: http://en.wikipedia.org/wiki/Sard%27s_theorem Now, I know what it means for a set to have measure zero in the case of $M, N$ being Euclidean spaces, but I am not clear on how we define measures on manifolds; I know we may pull back different types of objects from $\mathbb R^n$ into a manifold, like $k$-forms, etc., but I don't know if/how one pullsback a measure from $\mathbb R^n$, since I am not even clear on what kind of object a measure is. Is it just a  $0$-form, i.e., just a function? If so, can we define a measure globally on a manifold, or do we just pullback separately for each chart?","Everyone: Given smooth manifolds $M,N$ ($m$- and $n$- manifolds respectively) Sard's  theorem says that for $f:M \to N$ in $C^k$ ; $k \geq 1$, the image of the set of  critical points of $f$ in $M$ --points  in $M$ where the Jacobian has rank $< m$ --has measure zero in N: http://en.wikipedia.org/wiki/Sard%27s_theorem Now, I know what it means for a set to have measure zero in the case of $M, N$ being Euclidean spaces, but I am not clear on how we define measures on manifolds; I know we may pull back different types of objects from $\mathbb R^n$ into a manifold, like $k$-forms, etc., but I don't know if/how one pullsback a measure from $\mathbb R^n$, since I am not even clear on what kind of object a measure is. Is it just a  $0$-form, i.e., just a function? If so, can we define a measure globally on a manifold, or do we just pullback separately for each chart?",,"['analysis', 'differential-geometry']"
