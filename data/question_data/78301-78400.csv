,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Eigenvectors of complex matrix,Eigenvectors of complex matrix,,"I'm working on a problem where I am trying to find the eigenvectors of a pretty complicated matrix, and I am in need of some assistance. The matrix in question is: $$A =\begin{bmatrix}         \sin(x) & \cos(x)\cos(y) - i\cos(x)\sin(y)\\         \cos(x)\cos(y) + i\cos(x)\sin(y) & -\sin(x)\\         \end{bmatrix}$$ I know that the matrix is Hermitian, so that it is equal to its own conjugate transpose. Moreover, the eigenvalues are $\lambda = \pm 1$, as $A^2 = I$. However, I'm not sure how to use these properties to find the possible eigenvectors (if that would even help), and I would like to avoid doing it by brute force if possible, as it seem unruly. Thus far, I have tried to separate the matrix into real and imaginary parts, but that didn't seem to help. I also had the thought to assume diagonalization in an attempt to find the diagonalizing unitary matrix (and, in turn, the eigenvectors), but I don't see that making things much nicer either. Any help would be greatly appreciated.","I'm working on a problem where I am trying to find the eigenvectors of a pretty complicated matrix, and I am in need of some assistance. The matrix in question is: $$A =\begin{bmatrix}         \sin(x) & \cos(x)\cos(y) - i\cos(x)\sin(y)\\         \cos(x)\cos(y) + i\cos(x)\sin(y) & -\sin(x)\\         \end{bmatrix}$$ I know that the matrix is Hermitian, so that it is equal to its own conjugate transpose. Moreover, the eigenvalues are $\lambda = \pm 1$, as $A^2 = I$. However, I'm not sure how to use these properties to find the possible eigenvectors (if that would even help), and I would like to avoid doing it by brute force if possible, as it seem unruly. Thus far, I have tried to separate the matrix into real and imaginary parts, but that didn't seem to help. I also had the thought to assume diagonalization in an attempt to find the diagonalizing unitary matrix (and, in turn, the eigenvectors), but I don't see that making things much nicer either. Any help would be greatly appreciated.",,"['matrices', 'eigenvalues-eigenvectors']"
1,What is $\mbox{Tr}^2(A)-\mbox{Tr}(A^2)$ in terms of the eigenvalues of $A$?,What is  in terms of the eigenvalues of ?,\mbox{Tr}^2(A)-\mbox{Tr}(A^2) A,"I am looking for a way to relate the terms of the characteristic polynomial of a $3 \times 3$ matrix to its eigenvalues. The definition I start with (taken from Wolfram MathWorld ) is $\\P_{3}(A)=x^{3} + \mbox{Tr}(A)x^{2} + (\mbox{Tr}^{2}(A) - \mbox{Tr}(A^{2}))x^1 +(\mbox{Tr}^{3}(A) + 2 \mbox{Tr} (A^{3}) -3 \mbox{Tr} (A) \mbox{Tr}(A^{2}))x^0$ Which can also be written as $\\P_{3}(A)=x^{3} + \mbox{Tr}(A)x^{2} + (\mbox{Tr}^{2}(A) - \mbox{Tr}(A^{2}))x^1 + \det(A)x^0$, which again can be rewritten using the properties of eigenvalues as $\\P_{3}(A)=x^{3} + x^{2} \sum_{i=1}^{i=3} \lambda_{i} + x^1(\mbox{Tr}^{2}(A) - \mbox{Tr}(A^{2})) + x^0 \prod_{i=1}^{i=3} \lambda_{i}$ . I am wondering is there a way to relate the term in $x$ also to the eigenvalues in a similar way that can be done in with the terms in $x^{2}$ and $x^{0}$? Thanks!","I am looking for a way to relate the terms of the characteristic polynomial of a $3 \times 3$ matrix to its eigenvalues. The definition I start with (taken from Wolfram MathWorld ) is $\\P_{3}(A)=x^{3} + \mbox{Tr}(A)x^{2} + (\mbox{Tr}^{2}(A) - \mbox{Tr}(A^{2}))x^1 +(\mbox{Tr}^{3}(A) + 2 \mbox{Tr} (A^{3}) -3 \mbox{Tr} (A) \mbox{Tr}(A^{2}))x^0$ Which can also be written as $\\P_{3}(A)=x^{3} + \mbox{Tr}(A)x^{2} + (\mbox{Tr}^{2}(A) - \mbox{Tr}(A^{2}))x^1 + \det(A)x^0$, which again can be rewritten using the properties of eigenvalues as $\\P_{3}(A)=x^{3} + x^{2} \sum_{i=1}^{i=3} \lambda_{i} + x^1(\mbox{Tr}^{2}(A) - \mbox{Tr}(A^{2})) + x^0 \prod_{i=1}^{i=3} \lambda_{i}$ . I am wondering is there a way to relate the term in $x$ also to the eigenvalues in a similar way that can be done in with the terms in $x^{2}$ and $x^{0}$? Thanks!",,"['matrices', 'polynomials', 'eigenvalues-eigenvectors']"
2,"If $BA = I$, prove that $AB = I$ (using determinants)","If , prove that  (using determinants)",BA = I AB = I,"I've seen this problem around here, but I wanted to check if this particular solution is right. So, if $BA = I$, then $det(B)det(A) = 1$, meaning neither $det(B)$ or $det(A)$ are equal to $0$. Because $det(B) \neq 0$, $B$ must be invertible, which means $CB = I$ for some matrix $C$. Next, consider $CBA$. $BA = I$, so $CBA = C(I) = C$. $CB = I$, so $CBA = (I)A = A$ $CBA = C = A$. Now knowing that $C = A$, I can substitute $A$ in for $C$ to get $CB = AB = I$, which is what I wanted to prove.","I've seen this problem around here, but I wanted to check if this particular solution is right. So, if $BA = I$, then $det(B)det(A) = 1$, meaning neither $det(B)$ or $det(A)$ are equal to $0$. Because $det(B) \neq 0$, $B$ must be invertible, which means $CB = I$ for some matrix $C$. Next, consider $CBA$. $BA = I$, so $CBA = C(I) = C$. $CB = I$, so $CBA = (I)A = A$ $CBA = C = A$. Now knowing that $C = A$, I can substitute $A$ in for $C$ to get $CB = AB = I$, which is what I wanted to prove.",,"['matrices', 'proof-verification', 'determinant']"
3,Explain Similarity Transformation,Explain Similarity Transformation,,"For two n-by-n matrices $X$ and $Y$, we know that they are similar if the following is true for some invertible n-by-n matrix $M$: $X = M^{-1}YM$. Can anyone explain what the $M$ and $M^{-1}$ are doing? In other words, I'm finding plenty of information that contains this equation, however nobody seems to explain why we need to use both M and it's inverse. I know for a vector transformation we simply front-multiply by $M^{-1}$, but for matrices we must multiply on both sides. Why? Thanks!","For two n-by-n matrices $X$ and $Y$, we know that they are similar if the following is true for some invertible n-by-n matrix $M$: $X = M^{-1}YM$. Can anyone explain what the $M$ and $M^{-1}$ are doing? In other words, I'm finding plenty of information that contains this equation, however nobody seems to explain why we need to use both M and it's inverse. I know for a vector transformation we simply front-multiply by $M^{-1}$, but for matrices we must multiply on both sides. Why? Thanks!",,"['matrices', 'group-theory', 'transformation']"
4,Is a real logarithm of a special orthogonal matrix necessarily skew-symmetric?,Is a real logarithm of a special orthogonal matrix necessarily skew-symmetric?,,"The exponential map from the Lie algebra of skew-symmetric matrices $\mathfrak{so}(n)$ to the Lie group $\operatorname{SO}(n)$ is surjective and so I know that given any special orthogonal matrix there exists a skew-symmetric real logarithm. However, must all real logarithms of a special orthogonal matrix be skew-symmetric?","The exponential map from the Lie algebra of skew-symmetric matrices $\mathfrak{so}(n)$ to the Lie group $\operatorname{SO}(n)$ is surjective and so I know that given any special orthogonal matrix there exists a skew-symmetric real logarithm. However, must all real logarithms of a special orthogonal matrix be skew-symmetric?",,"['matrices', 'lie-groups', 'lie-algebras', 'exponentiation']"
5,L2 Matrix Norm Upper Bound in terms of Bounds of its Column,L2 Matrix Norm Upper Bound in terms of Bounds of its Column,,"I need to find an upper bound for a matrix norm in terms of bounds of its columns. I have a vector $\varepsilon_i(x) \in R^{n\times1} $ such that  $||\varepsilon_i(x)||_2\le\gamma_0$. I also have a matrix $Z=[\varepsilon_1, \varepsilon_2, \varepsilon_3, ... ,\varepsilon_N] \in R^{n\times N}$. Using the information $||\varepsilon_i(x)||_2\le\gamma_0$, can I find an upper bound for $||Z||_2$? If this were to be a frobenius norm question, it would be quite easy to show. However, I couldn't find an inequality for L2 norm case. Thank you in advance for your help.","I need to find an upper bound for a matrix norm in terms of bounds of its columns. I have a vector $\varepsilon_i(x) \in R^{n\times1} $ such that  $||\varepsilon_i(x)||_2\le\gamma_0$. I also have a matrix $Z=[\varepsilon_1, \varepsilon_2, \varepsilon_3, ... ,\varepsilon_N] \in R^{n\times N}$. Using the information $||\varepsilon_i(x)||_2\le\gamma_0$, can I find an upper bound for $||Z||_2$? If this were to be a frobenius norm question, it would be quite easy to show. However, I couldn't find an inequality for L2 norm case. Thank you in advance for your help.",,"['matrices', 'inequality', 'normed-spaces']"
6,Derivative of Schatten $p$-norm,Derivative of Schatten -norm,p,"The nuclear norm is a special case of the Schatten $p$-norm. I know how to find out the derivative of nuclear norm, but what is the derivative expression of the matrix Schatten $p$-norm?","The nuclear norm is a special case of the Schatten $p$-norm. I know how to find out the derivative of nuclear norm, but what is the derivative expression of the matrix Schatten $p$-norm?",,"['matrices', 'derivatives', 'matrix-calculus', 'matrix-norms', 'matrix-analysis']"
7,Covariance Matrix of mean-centered Random Variables,Covariance Matrix of mean-centered Random Variables,,"I read here that for n x d data matrix X, where X is mean-centered, V = $X^{T}*X$ is its covariance matrix. Why is that? As I understand the element $V_{i,j}$ of the covariance matrix is defined by $E[(X_i - \mu_i)(X_j-\mu_j)]$ and here, because of the mean-centering we would have $V_{i,j} = E[(X_i)(X_j)]$ - but this is not equivalent to just multiplying X with its transpose - or am I missing something?","I read here that for n x d data matrix X, where X is mean-centered, V = $X^{T}*X$ is its covariance matrix. Why is that? As I understand the element $V_{i,j}$ of the covariance matrix is defined by $E[(X_i - \mu_i)(X_j-\mu_j)]$ and here, because of the mean-centering we would have $V_{i,j} = E[(X_i)(X_j)]$ - but this is not equivalent to just multiplying X with its transpose - or am I missing something?",,['matrices']
8,Is the max matrix norm induced?,Is the max matrix norm induced?,,"Let $\|A \| = \max_{1 \le i,j \le n} |a_{ij}|$, where $A$ is a square matrix. I can prove that this is a matrix norm, but is it an induced norm?","Let $\|A \| = \max_{1 \le i,j \le n} |a_{ij}|$, where $A$ is a square matrix. I can prove that this is a matrix norm, but is it an induced norm?",,"['matrices', 'normed-spaces']"
9,Interesting Array of Integers with Strange Pattern,Interesting Array of Integers with Strange Pattern,,"I was experimenting and I found this pattern: Start with an (infinite) array with top row with all ones, and leftmost two columns also all ones. $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Then, to find the columns, starting from the top row, add numbers on the diagonal to the left: $$         \begin{matrix}         1 & \color{blue}{1} & \color{blue}{1} & 1 & 1 & 1 & 1 & 1 & \cdots \\         \color{blue}{1} & 1 & \color{blue}{1+1} & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ After this step, replace the numbers below the last one like this: $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Repeat: $$         \begin{matrix}         1 & 1 & \color{red}{1} & \color{red}{1} & 1 & 1 & 1 & 1 & \cdots \\         1 & \color{red}{1} & 2 & \color{red}{1+1} & ? & ? & ? & ? & \cdots \\         \color{red}{1} & 1 & 2 & \color{red}{1+1+1} & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Replace: $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & 2 & 2 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Repeat: $$         \begin{matrix}         1 & 1 & 1 & \color{blue}{1} & \color{blue}{1} & 1 & 1 & 1 & \cdots \\         1 & 1 & \color{blue}{2} & 2 & \color{blue}{1+2} & ? & ? & ? & \cdots \\         1 & \color{blue}{1} & 2 & 3 & \color{blue}{1+2+1} & ? & ? & ? & \cdots \\         \color{blue}{1} & 1 & 2 & 3 & \color{blue}{1+2+1+1} & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Replace: $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & 2 & 2 & 3 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 4 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ I guess you get the idea now. So after filling in the array, I got: $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & 2 & 2 & 3 & 3 & 4 & ? & \cdots \\         1 & 1 & \color{red}{2} & 3 & 4 & 5 & 7 & ? & \cdots \\         1 & 1 & 2 & \color{red}{3} & 5 & 6 & 9 & ? & \cdots \\         1 & 1 & 2 & 3 & \color{red}{5} & 7 & 10 & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & \color{red}{7} & 11 & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & 7 & \color{red}{11} & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & 7 & 11 & \color{red}{?} & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ So it looks like the numbers on the diagonal might be the prime numbers. But computing the next column destroys this hope: $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & 2 & 2 & 3 & 3 & 4 & 4 & \cdots \\         1 & 1 & \color{red}{2} & 3 & 4 & 5 & 7 & 8 & \cdots \\         1 & 1 & 2 & \color{red}{3} & 5 & 6 & 9 & 11 & \cdots \\         1 & 1 & 2 & 3 & \color{red}{5} & 7 & 10 & 13 & \cdots \\         1 & 1 & 2 & 3 & 5 & \color{red}{7} & 11 & 14 & \cdots \\         1 & 1 & 2 & 3 & 5 & 7 & \color{red}{11} & 15 & \cdots \\         1 & 1 & 2 & 3 & 5 & 7 & 11 & \color{red}{15} & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Writing out this sequence: $$2, 3, 5, 7, 11, 15, \cdots $$ So my question is, what is this sequence, that mirrored the primes but then suddenly diverged?","I was experimenting and I found this pattern: Start with an (infinite) array with top row with all ones, and leftmost two columns also all ones. $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Then, to find the columns, starting from the top row, add numbers on the diagonal to the left: $$         \begin{matrix}         1 & \color{blue}{1} & \color{blue}{1} & 1 & 1 & 1 & 1 & 1 & \cdots \\         \color{blue}{1} & 1 & \color{blue}{1+1} & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & ? & ? & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ After this step, replace the numbers below the last one like this: $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Repeat: $$         \begin{matrix}         1 & 1 & \color{red}{1} & \color{red}{1} & 1 & 1 & 1 & 1 & \cdots \\         1 & \color{red}{1} & 2 & \color{red}{1+1} & ? & ? & ? & ? & \cdots \\         \color{red}{1} & 1 & 2 & \color{red}{1+1+1} & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & ? & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Replace: $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & 2 & 2 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Repeat: $$         \begin{matrix}         1 & 1 & 1 & \color{blue}{1} & \color{blue}{1} & 1 & 1 & 1 & \cdots \\         1 & 1 & \color{blue}{2} & 2 & \color{blue}{1+2} & ? & ? & ? & \cdots \\         1 & \color{blue}{1} & 2 & 3 & \color{blue}{1+2+1} & ? & ? & ? & \cdots \\         \color{blue}{1} & 1 & 2 & 3 & \color{blue}{1+2+1+1} & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & ? & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Replace: $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & 2 & 2 & 3 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 4 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & ? & ? & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & ? & ? & ? & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ I guess you get the idea now. So after filling in the array, I got: $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & 2 & 2 & 3 & 3 & 4 & ? & \cdots \\         1 & 1 & \color{red}{2} & 3 & 4 & 5 & 7 & ? & \cdots \\         1 & 1 & 2 & \color{red}{3} & 5 & 6 & 9 & ? & \cdots \\         1 & 1 & 2 & 3 & \color{red}{5} & 7 & 10 & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & \color{red}{7} & 11 & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & 7 & \color{red}{11} & ? & \cdots \\         1 & 1 & 2 & 3 & 5 & 7 & 11 & \color{red}{?} & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ So it looks like the numbers on the diagonal might be the prime numbers. But computing the next column destroys this hope: $$         \begin{matrix}         1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\         1 & 1 & 2 & 2 & 3 & 3 & 4 & 4 & \cdots \\         1 & 1 & \color{red}{2} & 3 & 4 & 5 & 7 & 8 & \cdots \\         1 & 1 & 2 & \color{red}{3} & 5 & 6 & 9 & 11 & \cdots \\         1 & 1 & 2 & 3 & \color{red}{5} & 7 & 10 & 13 & \cdots \\         1 & 1 & 2 & 3 & 5 & \color{red}{7} & 11 & 14 & \cdots \\         1 & 1 & 2 & 3 & 5 & 7 & \color{red}{11} & 15 & \cdots \\         1 & 1 & 2 & 3 & 5 & 7 & 11 & \color{red}{15} & \cdots \\         \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\         \end{matrix} $$ Writing out this sequence: $$2, 3, 5, 7, 11, 15, \cdots $$ So my question is, what is this sequence, that mirrored the primes but then suddenly diverged?",,"['matrices', 'arithmetic']"
10,"Calculating the rank of a matrix , reduced row echelon or row echelon?","Calculating the rank of a matrix , reduced row echelon or row echelon?",,"I am trying to calculate the rank of a matrix and everytime I search for the steps required to calculate the rank of a matrix, the answer always uses the terms row echelon form and reduced row echelon form interchangeably when calculating the rank which is really confusing. I have read this question row echelon vs reduced row echelon form in which the given answer says that we use row echelon form instead of reduced row echelon form (as it's a tedious process) to calculate the rank however one the answers from a paper my university gave me had this written on it: Alternatively the rank is obtained by counting number of non-all-zero rows in reduced matrix form above. As: rank of A + nullity of A = dimension of A It seems to me now that it doesn't really matter which form I use? Is there a difference between using row echelon form to calculate the rank and reduced row echelon form to calculate the rank of a matrix? I'm really confused.","I am trying to calculate the rank of a matrix and everytime I search for the steps required to calculate the rank of a matrix, the answer always uses the terms row echelon form and reduced row echelon form interchangeably when calculating the rank which is really confusing. I have read this question row echelon vs reduced row echelon form in which the given answer says that we use row echelon form instead of reduced row echelon form (as it's a tedious process) to calculate the rank however one the answers from a paper my university gave me had this written on it: Alternatively the rank is obtained by counting number of non-all-zero rows in reduced matrix form above. As: rank of A + nullity of A = dimension of A It seems to me now that it doesn't really matter which form I use? Is there a difference between using row echelon form to calculate the rank and reduced row echelon form to calculate the rank of a matrix? I'm really confused.",,['matrices']
11,Proving a function of matrix is convex,Proving a function of matrix is convex,,"I have a function of a matrix and a vector $f(A,b)=y^\top (I-A)^{-1} b$ and I want to know the conditions under which it is convex. For functions of a vector, the positive definiteness of the Hessian is sufficient to claim convexity. How do we extend this to functions of matrices? I know the first derivative $\frac{\partial f}{\partial A}=(I-A)^{-\top}y\ b^\top(I-A)^{-\top}$, but how to extend this for finding convexity?","I have a function of a matrix and a vector $f(A,b)=y^\top (I-A)^{-1} b$ and I want to know the conditions under which it is convex. For functions of a vector, the positive definiteness of the Hessian is sufficient to claim convexity. How do we extend this to functions of matrices? I know the first derivative $\frac{\partial f}{\partial A}=(I-A)^{-\top}y\ b^\top(I-A)^{-\top}$, but how to extend this for finding convexity?",,"['matrices', 'convex-analysis', 'convex-optimization', 'nonlinear-optimization']"
12,What is the difference between coordinates transformation and change of coordinates?,What is the difference between coordinates transformation and change of coordinates?,,"In the context on 3D computer graphics, what is the difference between coordinates transformation and change of coordinates ? It can just be a matter of notation, but my book makes a clear distinction between the 2 terms (that I do not understand completely). As far as I understand, change of coordinates means to change the reference frame of all points expressed in a given frame and coordinates transformation means, given a set of points (in some reference frame), use one of them as the new origin (of a new reference frame) and express all the others in terms of that one. It seems to me that the coordinates transformation concept is similar to global and object coordinates in 3D computer graphics applications (like openGL), but then again they seem highly similar. Can you make some examples to point out the difference (if any)?","In the context on 3D computer graphics, what is the difference between coordinates transformation and change of coordinates ? It can just be a matter of notation, but my book makes a clear distinction between the 2 terms (that I do not understand completely). As far as I understand, change of coordinates means to change the reference frame of all points expressed in a given frame and coordinates transformation means, given a set of points (in some reference frame), use one of them as the new origin (of a new reference frame) and express all the others in terms of that one. It seems to me that the coordinates transformation concept is similar to global and object coordinates in 3D computer graphics applications (like openGL), but then again they seem highly similar. Can you make some examples to point out the difference (if any)?",,"['matrices', 'terminology', 'transformation', 'coordinate-systems', '3d']"
13,Moscow puzzle. Number lattice and number rearrangement. Quicker solution?,Moscow puzzle. Number lattice and number rearrangement. Quicker solution?,,"I have already considered chains of numbers like $4-19, 19-9, 9-22$, to solve the problem and got the answer. However just out of curiosity, can anyone think of a better/quicker solution? (answer is 19 btw)","I have already considered chains of numbers like $4-19, 19-9, 9-22$, to solve the problem and got the answer. However just out of curiosity, can anyone think of a better/quicker solution? (answer is 19 btw)",,"['matrices', 'discrete-mathematics', 'graph-theory', 'algorithms']"
14,Assume that $A $ is an $n \times n$ symmetric positive-definite matrix.,Assume that  is an  symmetric positive-definite matrix.,A  n \times n,Assume that $A$ is an $n\times n$ symmetric positive-definite matrix. Prove that: the element of $A$ with maximum magnitude must lie on the diagonal.,Assume that $A$ is an $n\times n$ symmetric positive-definite matrix. Prove that: the element of $A$ with maximum magnitude must lie on the diagonal.,,['matrices']
15,Determine cycle from adjacency matrix,Determine cycle from adjacency matrix,,Is there a way/algorithm to determine if there is a cycle in a graph if I only have the adjacency matrix and can not visualize the graph?,Is there a way/algorithm to determine if there is a cycle in a graph if I only have the adjacency matrix and can not visualize the graph?,,"['matrices', 'graph-theory']"
16,Prove that $\nabla_A \mbox{Tr} \left( A A^T \right) = 2A$,Prove that,\nabla_A \mbox{Tr} \left( A A^T \right) = 2A,"Prove that $$\nabla_A \mbox{Tr} \left( A A^T \right) = 2A$$ where $A$ is any square matrix. I did a simple derivative with product rule, but I don't know where i messed up. I started with $$ \nabla_A \mbox{Tr} \left( A A^T \right) = \mbox{Tr} \left( \frac {\partial A}{\partial A}A^T+A\frac {\partial A^T}{\partial A} \right) $$","Prove that where is any square matrix. I did a simple derivative with product rule, but I don't know where i messed up. I started with",\nabla_A \mbox{Tr} \left( A A^T \right) = 2A A  \nabla_A \mbox{Tr} \left( A A^T \right) = \mbox{Tr} \left( \frac {\partial A}{\partial A}A^T+A\frac {\partial A^T}{\partial A} \right) ,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
17,Abelian group generators and relations,Abelian group generators and relations,,"(a) Define what it means for an abelian group to be finitely generated .  Explain the terms elementary divisors and rank of $G$ and describe the structure theorem for finitely generated abelian groups. (b) Consider the integral matrix $R:=\left[\begin{array}{cccc}2&2&2&2\\4&4&8&5\\6&12&12&8\\4&10&8&6\end{array}\right]$ . Determine the structure of the abelian group given by generators and relations $$A_r:=\left\langle a_1,a_2,a_3,a_4\mid R\circ a = 0\right\rangle.$$ I know this is a long question, But I am really struggling to find a method online that is clear and can be applied again to another example. I put part a in to give some context of the question but I am pretty happy with the definitions, part b is the problem. A method and answer for part b would be amazing, thanks.","(a) Define what it means for an abelian group to be finitely generated .  Explain the terms elementary divisors and rank of and describe the structure theorem for finitely generated abelian groups. (b) Consider the integral matrix . Determine the structure of the abelian group given by generators and relations I know this is a long question, But I am really struggling to find a method online that is clear and can be applied again to another example. I put part a in to give some context of the question but I am pretty happy with the definitions, part b is the problem. A method and answer for part b would be amazing, thanks.","G R:=\left[\begin{array}{cccc}2&2&2&2\\4&4&8&5\\6&12&12&8\\4&10&8&6\end{array}\right] A_r:=\left\langle a_1,a_2,a_3,a_4\mid R\circ a = 0\right\rangle.","['group-theory', 'matrices', 'finite-groups', 'abelian-groups']"
18,How to combine Unitary Matrices in a clever way?,How to combine Unitary Matrices in a clever way?,,"I am trying to implement genetic-type algorithms on unitary matrices. Hopefully I should be able to use this question for the mutation part. But I am having an issue with the cross-over step. So here is my question: Given two unitary matrices $A$ and $B$(both square matrices with the same size, and their elements belong to $\mathbb C$), what is a clever way of combining them so the result would still be unitary. Where, by clever I mean if the parents $A$ and $B$ are close to each other, the child would also be close to them. I can give a precise notion of what I mean by being close(if necessary), but I think most of the reasonable notions would satisfy me. Besides the trivial choice of $A^iB^j$, I actually haven't been able to construct any unitary operators from $A$ and $B$ yet; and $A^iB^j$ is not necessarily close to $A$ or $B$. I don't know if this is asking too much, but since the unitary matrices I'm dealing with would scale like $2^n\times2^n$(where $n>10$), I guess another aspect of being clever, should be the efficiency of the algorithm. Edit: So I thought about a way of generating off-springs. I think it satisfies the first requirement of being clever, however I'm not sure how efficient it could be implemented. It is built upon the fact that if $H$ is Hermitian, then $e^{iH}$ would be unitary. So this is how it works: $$A'=e^{i\alpha(B-A+B^\dagger-A^\dagger)}A \\ B'=e^{i\alpha(A-B+A^\dagger-B^\dagger)}B$$ where $\alpha$ is a real number. I don't think this method is unique or perfect, and I would appreciate any improvements to it.","I am trying to implement genetic-type algorithms on unitary matrices. Hopefully I should be able to use this question for the mutation part. But I am having an issue with the cross-over step. So here is my question: Given two unitary matrices $A$ and $B$(both square matrices with the same size, and their elements belong to $\mathbb C$), what is a clever way of combining them so the result would still be unitary. Where, by clever I mean if the parents $A$ and $B$ are close to each other, the child would also be close to them. I can give a precise notion of what I mean by being close(if necessary), but I think most of the reasonable notions would satisfy me. Besides the trivial choice of $A^iB^j$, I actually haven't been able to construct any unitary operators from $A$ and $B$ yet; and $A^iB^j$ is not necessarily close to $A$ or $B$. I don't know if this is asking too much, but since the unitary matrices I'm dealing with would scale like $2^n\times2^n$(where $n>10$), I guess another aspect of being clever, should be the efficiency of the algorithm. Edit: So I thought about a way of generating off-springs. I think it satisfies the first requirement of being clever, however I'm not sure how efficient it could be implemented. It is built upon the fact that if $H$ is Hermitian, then $e^{iH}$ would be unitary. So this is how it works: $$A'=e^{i\alpha(B-A+B^\dagger-A^\dagger)}A \\ B'=e^{i\alpha(A-B+A^\dagger-B^\dagger)}B$$ where $\alpha$ is a real number. I don't think this method is unique or perfect, and I would appreciate any improvements to it.",,"['matrices', 'algorithms', 'optimization']"
19,Can this transformation be expressed as a matrix equation?,Can this transformation be expressed as a matrix equation?,,"I have a matrix $A$ of dimension $m \times n$, and I need to get a matrix $B$ which is also $m \times n$, that has the following specifications: Element $B_{ij}$ of matrix $B$ is the product of the sum of all elements in row $i$, and the sum of all elements in row $j$, divided by the sum of all elements in $A$: $$B_{ij} = \frac{\sum{A_i}\sum{A_j}}{\sum{A_{ij}}}$$ Is there a matrix equation of basic operations (addition, multiplication, trace, transpose, etc.) that can be used to express this transformation?","I have a matrix $A$ of dimension $m \times n$, and I need to get a matrix $B$ which is also $m \times n$, that has the following specifications: Element $B_{ij}$ of matrix $B$ is the product of the sum of all elements in row $i$, and the sum of all elements in row $j$, divided by the sum of all elements in $A$: $$B_{ij} = \frac{\sum{A_i}\sum{A_j}}{\sum{A_{ij}}}$$ Is there a matrix equation of basic operations (addition, multiplication, trace, transpose, etc.) that can be used to express this transformation?",,['matrices']
20,On the ambiguity of the definition of Lie algebras of real matrix groups,On the ambiguity of the definition of Lie algebras of real matrix groups,,"I have been studying Rossmann's Lie Groups . In the context of this book, a linear group $G$ is a group of invertible real or complex matrices, and its Lie algebra $\mathfrak{g}$ consists of those matrices $X$ for which the exponential $e^{tX}$ belongs to $G$ for every $t\in\mathbb{R}$. As far as I understand, the matrices $X$ which are candidates to be an element of $\mathfrak{g}$ are taken a priori to be real if $G$ consists of real matrices. However, we can of course regard any such $G$ as a group of complex matrices as well; in any case, one should get the same $\mathfrak{g}$. This is what I want to prove. It is clearly enough to show this for $G=\mathrm{GL}(n,\mathbb{R})$. Therefore, I want to show that any complex matrix $X$ for which $e^{tX}$ is real for every $t\in\mathbb{R}$ is itself real. This is easy to see if $X$ is diagonal, but what about general $X$? Any reference or sketch of proof is welcome.","I have been studying Rossmann's Lie Groups . In the context of this book, a linear group $G$ is a group of invertible real or complex matrices, and its Lie algebra $\mathfrak{g}$ consists of those matrices $X$ for which the exponential $e^{tX}$ belongs to $G$ for every $t\in\mathbb{R}$. As far as I understand, the matrices $X$ which are candidates to be an element of $\mathfrak{g}$ are taken a priori to be real if $G$ consists of real matrices. However, we can of course regard any such $G$ as a group of complex matrices as well; in any case, one should get the same $\mathfrak{g}$. This is what I want to prove. It is clearly enough to show this for $G=\mathrm{GL}(n,\mathbb{R})$. Therefore, I want to show that any complex matrix $X$ for which $e^{tX}$ is real for every $t\in\mathbb{R}$ is itself real. This is easy to see if $X$ is diagonal, but what about general $X$? Any reference or sketch of proof is welcome.",,"['matrices', 'lie-groups', 'lie-algebras']"
21,Linear optimization problem: Minimizing a linear function over an affine set.,Linear optimization problem: Minimizing a linear function over an affine set.,,"The problem is as follows: Give an explicit solution of the linear optimization problem below. $$     \text{minimize}\ c^Tx \\    \text{subject to}\ Ax\ =\ b $$ No other information is given. My solution is  basically as follows: $$ p^*\ = \left\{    \begin{array}{l l}     \infty & \quad \text{if $Ax=b$ has no solution}\\     \lambda^\top b & \quad c=A^\top \lambda \text{ for some } \lambda\\     \ -\infty & \quad \text{if $Ax=b$ has infinitely many solutions (underdetermined)}   \end{array} \right. $$ When $Ax=b$ has no solution the problem is infeasible. Therefore the optimal point is $\infty$. When $Ax=b$ has infinitely many solutions, the system in unbounded below and therefore the optimal solution in $-\infty$. Now the actual solution of this problem is shown here (See problem 4.8 part a): http://www.docin.com/p-347099771.html I would like to understand how they got the second solution ($\lambda^Tb$) when A is non-singular and Ax=b has a unique solution. Thanks in advance.","The problem is as follows: Give an explicit solution of the linear optimization problem below. $$     \text{minimize}\ c^Tx \\    \text{subject to}\ Ax\ =\ b $$ No other information is given. My solution is  basically as follows: $$ p^*\ = \left\{    \begin{array}{l l}     \infty & \quad \text{if $Ax=b$ has no solution}\\     \lambda^\top b & \quad c=A^\top \lambda \text{ for some } \lambda\\     \ -\infty & \quad \text{if $Ax=b$ has infinitely many solutions (underdetermined)}   \end{array} \right. $$ When $Ax=b$ has no solution the problem is infeasible. Therefore the optimal point is $\infty$. When $Ax=b$ has infinitely many solutions, the system in unbounded below and therefore the optimal solution in $-\infty$. Now the actual solution of this problem is shown here (See problem 4.8 part a): http://www.docin.com/p-347099771.html I would like to understand how they got the second solution ($\lambda^Tb$) when A is non-singular and Ax=b has a unique solution. Thanks in advance.",,"['matrices', 'optimization', 'convex-optimization']"
22,Eigenvalues of symmetric matrix in real inner product space,Eigenvalues of symmetric matrix in real inner product space,,"I got the following exercise to solve: Let $A\in\mathbb{R}^{n\times n}$   be a symmetric matrix and let $\lambda_{1}\geq\lambda_{2}\geq\cdots\geq\lambda_{n}$   be its eigenvalues sorted in a desecnding order. Show that: $$\lambda_{k}=\max_{V,\,\dim\left(V\right)=k}\left\{ \min\frac{\left<x,Ax\right>}{\left<x,x\right>}\,:\, x\in V\,,\, x\neq0\right\} $$  There is no mention of some specific inner product being taken here so I assume it needs to be proven for a general inner product on an $n$   dimensional real inner-product space. I'm completely stumped with this, help would be greatly appreciated!","I got the following exercise to solve: Let $A\in\mathbb{R}^{n\times n}$   be a symmetric matrix and let $\lambda_{1}\geq\lambda_{2}\geq\cdots\geq\lambda_{n}$   be its eigenvalues sorted in a desecnding order. Show that: $$\lambda_{k}=\max_{V,\,\dim\left(V\right)=k}\left\{ \min\frac{\left<x,Ax\right>}{\left<x,x\right>}\,:\, x\in V\,,\, x\neq0\right\} $$  There is no mention of some specific inner product being taken here so I assume it needs to be proven for a general inner product on an $n$   dimensional real inner-product space. I'm completely stumped with this, help would be greatly appreciated!",,"['matrices', 'eigenvalues-eigenvectors', 'inner-products']"
23,Parabolic subgroups of $\mathrm{Sl}_n$ are the ones that stabilize some flag,Parabolic subgroups of  are the ones that stabilize some flag,\mathrm{Sl}_n,I am looking for a reference for the above statement that every parabolic subgroup of $\mathrm{Sl}_n(\Bbbk)$ stabilizes some flag in $\Bbbk^n$. I have gone through a large pile of books and can't seem to find one. Thanks a bunch in advance! Edit: I understand $G=\mathrm{Sl}_n(\Bbbk)$ as a connected algebraic group and define a parabolic subgroup $P\subseteq G$ to be one that contains a maximal connected solvable subgroup. I know how this is equivalent to $G/P$ being complete (or projective).,I am looking for a reference for the above statement that every parabolic subgroup of $\mathrm{Sl}_n(\Bbbk)$ stabilizes some flag in $\Bbbk^n$. I have gone through a large pile of books and can't seem to find one. Thanks a bunch in advance! Edit: I understand $G=\mathrm{Sl}_n(\Bbbk)$ as a connected algebraic group and define a parabolic subgroup $P\subseteq G$ to be one that contains a maximal connected solvable subgroup. I know how this is equivalent to $G/P$ being complete (or projective).,,"['matrices', 'reference-request', 'lie-groups', 'algebraic-groups']"
24,"""Fully correlated"" definition","""Fully correlated"" definition",,"Really sorry to be a noob, but I'm a programmer, not a mathematician, and all of my knowledge about statistics come from this book ""Schaum's Outline of Theory and Problems of Probability, Random Variables, and Random Processes"". I'm implementing an UKF for target tracking using C++. Everything went well until an error about covariance matrix of state is not positive definite happened. After a little research, I found this link Under what circumstance will a covariance matrix be positive semi-definite rather than positive definite? which  almost answer everything I need. Only one thing I don't understand: The answer says "" This happens if and only if some linear combination of X is ‘ fully correlated ’ "". Can anyone explain for me what does "" fully correlated "" mean? And example would be great. I have search Google about its definition but there is no luck at all.","Really sorry to be a noob, but I'm a programmer, not a mathematician, and all of my knowledge about statistics come from this book ""Schaum's Outline of Theory and Problems of Probability, Random Variables, and Random Processes"". I'm implementing an UKF for target tracking using C++. Everything went well until an error about covariance matrix of state is not positive definite happened. After a little research, I found this link Under what circumstance will a covariance matrix be positive semi-definite rather than positive definite? which  almost answer everything I need. Only one thing I don't understand: The answer says "" This happens if and only if some linear combination of X is ‘ fully correlated ’ "". Can anyone explain for me what does "" fully correlated "" mean? And example would be great. I have search Google about its definition but there is no luck at all.",,"['matrices', 'statistics']"
25,"Given unimodular matrices $A, B$, solve the matrix equation $T^\top A T = B$","Given unimodular matrices , solve the matrix equation","A, B T^\top A T = B","Given two symmetric integer unimodular matrices $A$ and $B$ with $\det A = \det B = \pm 1$ . How do we find any integer unimodular matrices $T$ such that $$ T^\top A T = B? $$ Here $T^\top$ , denotes the transpose of $T$ . As an example, here are the data from the Mathematica Table: A = ({{2, -1, 0, 0, 0, 0, 0, 0, 0, 0},       {-1, 2, -1, 0, 0, 0, 0, 0, 0, 0},       {0, -1, 2, -1, 0, 0, 0, -1, 0, 0},       {0, 0, -1, 2, -1, 0, 0, 0, 0, 0},       {0, 0, 0, -1, 2, -1, 0, 0, 0, 0},       {0, 0, 0, 0, -1, 2, -1, 0, 0, 0},       {0, 0, 0, 0, 0, -1, 2, 0, 0, 0},       {0, 0, -1, 0, 0, 0, 0, 2, 0, 0},       {0, 0, 0, 0, 0, 0, 0, 0, 1, 0},       {0, 0, 0, 0, 0, 0, 0, 0, 0, -1}    });   B = ({{1, 0, 0, 0, 0, 0, 0, 0, 0, 0},       {0, 1, 0, 0, 0, 0, 0, 0, 0, 0},       {0, 0, 1, 0, 0, 0, 0, 0, 0, 0},       {0, 0, 0, 1, 0, 0, 0, 0, 0, 0},       {0, 0, 0, 0, 1, 0, 0, 0, 0, 0},       {0, 0, 0, 0, 0, 1, 0, 0, 0, 0},       {0, 0, 0, 0, 0, 0, 1, 0, 0, 0},       {0, 0, 0, 0, 0, 0, 0, 1, 0, 0},       {0, 0, 0, 0, 0, 0, 0, 0, 1, 0},       {0, 0, 0, 0, 0, 0, 0, 0, 0, -1}    }); In matrix form, $$A= \left( \begin{array}{cccccccccc}  2 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  -1 & 2 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & -1 & 2 & -1 & 0 & 0 & 0 & -1 & 0 & 0 \\  0 & 0 & -1 & 2 & -1 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & -1 & 2 & -1 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & -1 & 2 & -1 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & -1 & 2 & 0 & 0 & 0 \\  0 & 0 & -1 & 0 & 0 & 0 & 0 & 2 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 \\ \end{array} \right) $$ $$ B=\left( \begin{array}{cccccccccc}  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 \\ \end{array} \right) $$ such that $\det A=\det B=-1$ here. Can you solve integer matrix $T$ with $T^\top A T = B$ ?","Given two symmetric integer unimodular matrices and with . How do we find any integer unimodular matrices such that Here , denotes the transpose of . As an example, here are the data from the Mathematica Table: A = ({{2, -1, 0, 0, 0, 0, 0, 0, 0, 0},       {-1, 2, -1, 0, 0, 0, 0, 0, 0, 0},       {0, -1, 2, -1, 0, 0, 0, -1, 0, 0},       {0, 0, -1, 2, -1, 0, 0, 0, 0, 0},       {0, 0, 0, -1, 2, -1, 0, 0, 0, 0},       {0, 0, 0, 0, -1, 2, -1, 0, 0, 0},       {0, 0, 0, 0, 0, -1, 2, 0, 0, 0},       {0, 0, -1, 0, 0, 0, 0, 2, 0, 0},       {0, 0, 0, 0, 0, 0, 0, 0, 1, 0},       {0, 0, 0, 0, 0, 0, 0, 0, 0, -1}    });   B = ({{1, 0, 0, 0, 0, 0, 0, 0, 0, 0},       {0, 1, 0, 0, 0, 0, 0, 0, 0, 0},       {0, 0, 1, 0, 0, 0, 0, 0, 0, 0},       {0, 0, 0, 1, 0, 0, 0, 0, 0, 0},       {0, 0, 0, 0, 1, 0, 0, 0, 0, 0},       {0, 0, 0, 0, 0, 1, 0, 0, 0, 0},       {0, 0, 0, 0, 0, 0, 1, 0, 0, 0},       {0, 0, 0, 0, 0, 0, 0, 1, 0, 0},       {0, 0, 0, 0, 0, 0, 0, 0, 1, 0},       {0, 0, 0, 0, 0, 0, 0, 0, 0, -1}    }); In matrix form, such that here. Can you solve integer matrix with ?","A B \det A = \det B = \pm 1 T 
T^\top A T = B?
 T^\top T A=
\left(
\begin{array}{cccccccccc}
 2 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 -1 & 2 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & -1 & 2 & -1 & 0 & 0 & 0 & -1 & 0 & 0 \\
 0 & 0 & -1 & 2 & -1 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & -1 & 2 & -1 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & -1 & 2 & -1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & -1 & 2 & 0 & 0 & 0 \\
 0 & 0 & -1 & 0 & 0 & 0 & 0 & 2 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 \\
\end{array}
\right)
 
B=\left(
\begin{array}{cccccccccc}
 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 \\
\end{array}
\right)
 \det A=\det B=-1 T T^\top A T = B","['matrices', 'quadratics', 'matrix-equations', 'integer-lattices', 'unimodular-matrices']"
26,Intuition of $D\leftarrow XC^{T}\text{diag}(C1_n)^{-1}$ update rule in matrix factorization,Intuition of  update rule in matrix factorization,D\leftarrow XC^{T}\text{diag}(C1_n)^{-1},"I am reading this paper where they use Matrix Factorization over Attention mechanism in their Hamburger model. In section 2.2.2 they say, Vector Quantization (VQ) (Gray & Neuhoff, 1998), a classic data compression algorithm, can be formulated as an optimization problem in term of matrix decomposition: $$ \min _{\boldsymbol{D}, \boldsymbol{C}}\|\boldsymbol{X}-\boldsymbol{D} \boldsymbol{C}\|_F \quad \text { s.t. } \mathbf{c}_i \in\left\{\mathbf{e}_1, \mathbf{e}_2, \cdots, \mathbf{e}_r\right\}\tag1 $$ where $e_i$ is the canonical basis vector, $\mathbf{e}_i=[0, \cdots, 1, \cdots, 0]^{\top}$ . The solution to minimize the objective in Eq. (1) is K-means (Gray & Neuhoff, 1998). However, to ensure that VQ is differentiable, we replace the hard arg min and Euclidean distance with softmax and cosine similarity, leading to Alg. 1, where $\operatorname{cosine}(\boldsymbol{D}, \boldsymbol{X})$ is a similarity matrix whose entries satisfy $\operatorname{cosine}(\boldsymbol{D}, \boldsymbol{X})_{i j}=\frac{\mathbf{d}_i^{\top} \mathbf{x}_j}{\|\mathbf{d}\|\|\mathbf{x}\|}$ , and softmax is applied column-wise and $T$ is the temperature. Further, we can obtain a hard assignment by a one-hot vector when $T \rightarrow 0$ . I didn't understand the last line, ""...and softmax is applied column-wise and $T$ is the temperature. Further, we can obtain a hard assignment by a one-hot vector when $T \rightarrow 0$ ."" What they mean by $T$ (Temperature) here? In fact, I couldn't get the justification for the replacement of $\arg \min$ with softmax here. And what is their update rule doing for $D\leftarrow XC^{T}\text{diag}(C1_n)^{-1}$ And it seems like their VQ is similar to traditional non-negative matrix factorization with regularization on $C$ . Or am I confused between these two? Thanks in advance. update I remove some questions which might be not going with M.SE rules. And convert the thread with a single math-based question only. Hope it will get better reach now.","I am reading this paper where they use Matrix Factorization over Attention mechanism in their Hamburger model. In section 2.2.2 they say, Vector Quantization (VQ) (Gray & Neuhoff, 1998), a classic data compression algorithm, can be formulated as an optimization problem in term of matrix decomposition: where is the canonical basis vector, . The solution to minimize the objective in Eq. (1) is K-means (Gray & Neuhoff, 1998). However, to ensure that VQ is differentiable, we replace the hard arg min and Euclidean distance with softmax and cosine similarity, leading to Alg. 1, where is a similarity matrix whose entries satisfy , and softmax is applied column-wise and is the temperature. Further, we can obtain a hard assignment by a one-hot vector when . I didn't understand the last line, ""...and softmax is applied column-wise and is the temperature. Further, we can obtain a hard assignment by a one-hot vector when ."" What they mean by (Temperature) here? In fact, I couldn't get the justification for the replacement of with softmax here. And what is their update rule doing for And it seems like their VQ is similar to traditional non-negative matrix factorization with regularization on . Or am I confused between these two? Thanks in advance. update I remove some questions which might be not going with M.SE rules. And convert the thread with a single math-based question only. Hope it will get better reach now.","
\min _{\boldsymbol{D}, \boldsymbol{C}}\|\boldsymbol{X}-\boldsymbol{D} \boldsymbol{C}\|_F \quad \text { s.t. } \mathbf{c}_i \in\left\{\mathbf{e}_1, \mathbf{e}_2, \cdots, \mathbf{e}_r\right\}\tag1
 e_i \mathbf{e}_i=[0, \cdots, 1, \cdots, 0]^{\top} \operatorname{cosine}(\boldsymbol{D}, \boldsymbol{X}) \operatorname{cosine}(\boldsymbol{D}, \boldsymbol{X})_{i j}=\frac{\mathbf{d}_i^{\top} \mathbf{x}_j}{\|\mathbf{d}\|\|\mathbf{x}\|} T T \rightarrow 0 T T \rightarrow 0 T \arg \min D\leftarrow XC^{T}\text{diag}(C1_n)^{-1} C","['matrices', 'optimization', 'matrix-decomposition', 'nonnegative-matrices']"
27,Non-negative integer matrix representations of finite groups,Non-negative integer matrix representations of finite groups,,"I wanted to know all the non-negative integer matrix (NIM) irreducible representations (irrep) of finite groups i.e. the homomorphisms $\varphi : G \to GL(n,\mathbb{Z}_{\geq 0})$ . By irreducible NIM rep (or NIM irrep), I mean that we can't write it into a direct sum of smaller NIM reps. Edit: This proof shows that $GL(n,\mathbb{Z}_{\geq 0}) = S_n$ which simplifies things a lot. For context: I am a physics grad student and these representations are called NIM reps in the physics literature. These NIM reps are useful when we study the action of topological defect lines on boundary states of 2D CFTs. I state the things I am able to prove in chronological order: Every matrix $M \in GL(n,\mathbb{Z}_{\geq 0})$ such that $M^m = I$ for some $m\in\mathbb{Z}_{+}$ is a permutation matrix. ( Proof ) This implies that matrices in NIM reps of finite groups are permutation matrices. For the cyclic group of order prime $\mathbb{Z}_p$ , the only non-trivial NIM irrep is the regular representation of $\mathbb{Z}_p$ . (Here, irreducible = not a direct sum of smaller NIM reps) For a group $\mathbb{Z}_n$ , for every factor $k$ of $n$ , there is a NIM irrep of $\mathbb{Z}_n$ which is the regular representation of $\mathbb{Z}_k$ . Also, these are the only NIM irreps of $\mathbb{Z}_n$ . I would show my work, but it is too long to write here (there may be a shorter version out there) although the steps are elementary. If there is some obvious mistake in the three claims above, I  would be more than happy to correct myself. For non-abelian finite groups, I don't know how to proceed and how hard the problem is. My question: Is the classification of NIM irreps of abelian and non-abelian finite groups known already? Could you state the results if possible or could you refer me to some literature on this? Thanks.","I wanted to know all the non-negative integer matrix (NIM) irreducible representations (irrep) of finite groups i.e. the homomorphisms . By irreducible NIM rep (or NIM irrep), I mean that we can't write it into a direct sum of smaller NIM reps. Edit: This proof shows that which simplifies things a lot. For context: I am a physics grad student and these representations are called NIM reps in the physics literature. These NIM reps are useful when we study the action of topological defect lines on boundary states of 2D CFTs. I state the things I am able to prove in chronological order: Every matrix such that for some is a permutation matrix. ( Proof ) This implies that matrices in NIM reps of finite groups are permutation matrices. For the cyclic group of order prime , the only non-trivial NIM irrep is the regular representation of . (Here, irreducible = not a direct sum of smaller NIM reps) For a group , for every factor of , there is a NIM irrep of which is the regular representation of . Also, these are the only NIM irreps of . I would show my work, but it is too long to write here (there may be a shorter version out there) although the steps are elementary. If there is some obvious mistake in the three claims above, I  would be more than happy to correct myself. For non-abelian finite groups, I don't know how to proceed and how hard the problem is. My question: Is the classification of NIM irreps of abelian and non-abelian finite groups known already? Could you state the results if possible or could you refer me to some literature on this? Thanks.","\varphi : G \to GL(n,\mathbb{Z}_{\geq 0}) GL(n,\mathbb{Z}_{\geq 0}) = S_n M \in GL(n,\mathbb{Z}_{\geq 0}) M^m = I m\in\mathbb{Z}_{+} \mathbb{Z}_p \mathbb{Z}_p \mathbb{Z}_n k n \mathbb{Z}_n \mathbb{Z}_k \mathbb{Z}_n","['matrices', 'group-theory', 'finite-groups', 'representation-theory', 'permutation-matrices']"
28,Which of the following properties must be true.,Which of the following properties must be true.,,"Consider a matrix $A = (a_{ij})_{n×n}$ with integer entries such that $a_{ij} = 0$ for $i>j$ and $a_{ii} = 1$ for $i = 1,2...,n$ . Which of the following properties must be true ? $A^{-1}$ exists and it has integer entries. $A^{-1}$ exists and it has some entries that are not integers. $A^{-1}$ is a polynomial function of $A$ with integer coefficients. $A^{-1}$ is not a power of $A$ unless $A$ is the identity matrix. My Attempt: Here we see that $|A|=1$ and $A^{-1} = \frac{adj(A)}{|A|} = adj(A)$ . So option 1 is true and option 2 is false. Also $ch_A(x) = (x-1)^n$ and we know that every square matrix satisfy its characteristic polynomial. So $(A-I)^n = O \implies n_{C_0}A^n(-I)^0+n_{C_1}A^{n-1}(-I)^1+n_{C_2}A^{n-2}(-I)^2+n_{C_3}A^{n-3}(-I)^3+...+n_{C_{n-1}}A(-I)^{n-1}+n_{C_n}A^0(-I)^n = 0$ There will be two cases. Case 1: $n = 2m , m \in \Bbb N$ . Therefore $n_{C_0}A^n-n_{C_1}A^{n-1}+n_{C_2}A^{n-2}-n_{C_3}A^{n-3}+...-n_{C_{n-1}}A+n_{C_n}I = 0$ Now multiplying by $A^{-1}$ on both sides, we get $n_{C_0}A^{n-1}-n_{C_1}A^{n-2}+n_{C_2}A^{n-3}-n_{C_3}A^{n-4}+...-n_{C_{n-1}}I+n_{C_n}A^{-1} = 0 \implies n_{C_0}A^{n-1}+n_{C_1}A^{n-2}-n_{C_2}A^{n-3}+n_{C_3}A^{n-4}+...+n_{C_{n-1}}I = A^{-1}$ Case 2: $n = 2m+1 , m \in \Bbb N$ . Therefore $n_{C_0}A^{n-1}+n_{C_1}A^{n-2}-n_{C_2}A^{n-3}+n_{C_3}A^{n-4}+...-n_{C_{n-1}}I = A^{-1}$ We know that $n_{C_r} \in \Bbb Z$ . Hence option 3 is correct. If I take $A = I$ then above cases can be written as Case 1: $n = 2m , m \in \Bbb N$ . $n_{C_0}I^{n-1}+n_{C_1}I^{n-2}-n_{C_2}I^{n-3}+n_{C_3}I^{n-4}+...+n_{C_{n-1}}I = A^{-1}$ . So $I(n_{C_0}+n_{C_1}-n_{C_2}+n_{C_3}+...+n_{C_{n-1}}) = A^{-1} \implies kI = A^{-1}$ , where $k =(n_{C_0}+n_{C_1}-n_{C_2}+n_{C_3}+...+n_{C_{n-1}} \in \Bbb Z$ We see clearly that $A^{-1}$ is powers of $A$ and $A$ is not powers of $A$ when $A = I$ . So option 4 is true. Please correct me if I'm wrong anywhere. Thanks.","Consider a matrix with integer entries such that for and for . Which of the following properties must be true ? exists and it has integer entries. exists and it has some entries that are not integers. is a polynomial function of with integer coefficients. is not a power of unless is the identity matrix. My Attempt: Here we see that and . So option 1 is true and option 2 is false. Also and we know that every square matrix satisfy its characteristic polynomial. So There will be two cases. Case 1: . Therefore Now multiplying by on both sides, we get Case 2: . Therefore We know that . Hence option 3 is correct. If I take then above cases can be written as Case 1: . . So , where We see clearly that is powers of and is not powers of when . So option 4 is true. Please correct me if I'm wrong anywhere. Thanks.","A = (a_{ij})_{n×n} a_{ij} = 0 i>j a_{ii} = 1 i = 1,2...,n A^{-1} A^{-1} A^{-1} A A^{-1} A A |A|=1 A^{-1} = \frac{adj(A)}{|A|} = adj(A) ch_A(x) = (x-1)^n (A-I)^n = O \implies n_{C_0}A^n(-I)^0+n_{C_1}A^{n-1}(-I)^1+n_{C_2}A^{n-2}(-I)^2+n_{C_3}A^{n-3}(-I)^3+...+n_{C_{n-1}}A(-I)^{n-1}+n_{C_n}A^0(-I)^n = 0 n = 2m , m \in \Bbb N n_{C_0}A^n-n_{C_1}A^{n-1}+n_{C_2}A^{n-2}-n_{C_3}A^{n-3}+...-n_{C_{n-1}}A+n_{C_n}I = 0 A^{-1} n_{C_0}A^{n-1}-n_{C_1}A^{n-2}+n_{C_2}A^{n-3}-n_{C_3}A^{n-4}+...-n_{C_{n-1}}I+n_{C_n}A^{-1} = 0 \implies n_{C_0}A^{n-1}+n_{C_1}A^{n-2}-n_{C_2}A^{n-3}+n_{C_3}A^{n-4}+...+n_{C_{n-1}}I = A^{-1} n = 2m+1 , m \in \Bbb N n_{C_0}A^{n-1}+n_{C_1}A^{n-2}-n_{C_2}A^{n-3}+n_{C_3}A^{n-4}+...-n_{C_{n-1}}I = A^{-1} n_{C_r} \in \Bbb Z A = I n = 2m , m \in \Bbb N n_{C_0}I^{n-1}+n_{C_1}I^{n-2}-n_{C_2}I^{n-3}+n_{C_3}I^{n-4}+...+n_{C_{n-1}}I = A^{-1} I(n_{C_0}+n_{C_1}-n_{C_2}+n_{C_3}+...+n_{C_{n-1}}) = A^{-1} \implies kI = A^{-1} k =(n_{C_0}+n_{C_1}-n_{C_2}+n_{C_3}+...+n_{C_{n-1}} \in \Bbb Z A^{-1} A A A A = I","['matrices', 'solution-verification', 'inverse']"
29,Can we determine higher powers of a matrix in terms of lower powered matrices?,Can we determine higher powers of a matrix in terms of lower powered matrices?,,"Consider a n-ordered square matrix A. Using Cayley-Hamilton Theorem, I can represent the matrix $A^n$ as a matrix polynomial P(A) of degree n-1. Further any matrix $A^k$ where $k>n$ can also be represented as follows: $$A^k= a_{k,n-1} A^{n-1} + a_{k,n-2} A^{n-2}+a_{k,n-3} A^{n-3} ... + a_{k,2} A^{2}+a_{k,1} A^{1} + a_{k,0}I$$ What I want to know that is there any way to determine the coefficients as a function of k?","Consider a n-ordered square matrix A. Using Cayley-Hamilton Theorem, I can represent the matrix as a matrix polynomial P(A) of degree n-1. Further any matrix where can also be represented as follows: What I want to know that is there any way to determine the coefficients as a function of k?","A^n A^k k>n A^k= a_{k,n-1} A^{n-1} + a_{k,n-2} A^{n-2}+a_{k,n-3} A^{n-3} ... + a_{k,2} A^{2}+a_{k,1} A^{1} + a_{k,0}I","['matrices', 'functions', 'cayley-hamilton']"
30,Fill in a $5 \times 5$ square to make equal products in rows and columns,Fill in a  square to make equal products in rows and columns,5 \times 5,"Let's say you fill in a $5 \times 5$ square with $1, 2,\dots, 25$ . Is there a way to fill it so that the product of the first row is equal to the product of the first column, the product of the second row is equal to the product of the second column, etc? What about generalization for a $n \times n$ square filled with $1, 2,\ldots, n^2$ ? Edit: Here are some updates on progress. For $n=3$ , it is a possibility: $$\begin{matrix} 5 & 1 & 6\\                  2 & 7 & 4\\                  3 & 8 & 9\end{matrix}$$ I'm not sure how to do $n=4, 5$ . I know that if there are $>n$ prime number between $n^2/2$ and $n^2$ then $n$ doesn't work. I do not know how to proceed.","Let's say you fill in a square with . Is there a way to fill it so that the product of the first row is equal to the product of the first column, the product of the second row is equal to the product of the second column, etc? What about generalization for a square filled with ? Edit: Here are some updates on progress. For , it is a possibility: I'm not sure how to do . I know that if there are prime number between and then doesn't work. I do not know how to proceed.","5 \times 5 1, 2,\dots, 25 n \times n 1, 2,\ldots, n^2 n=3 \begin{matrix} 5 & 1 & 6\\
                 2 & 7 & 4\\
                 3 & 8 & 9\end{matrix} n=4, 5 >n n^2/2 n^2 n","['matrices', 'elementary-number-theory']"
31,The matrix function $\frac{e^x - 1}{x}$ for non-invertible matrices,The matrix function  for non-invertible matrices,\frac{e^x - 1}{x},"I am trying to evaluate the matrix function $$ f(X) = \frac{e^X - I}{X}, \tag{1} $$ where $e^X$ is the usual matrix exponential, defined by $$ e^X = \sum_{n=0}^{\infty} \frac{1}{n!} X^n. $$ The problem is that all matrices I am working with are singular (if it is of relevance, $X \in \mathfrak{so}(3)$ ). According to this Wikipedia article , one may consider the function $f$ to be a real function, find its Maclaurin series, and use this to evaluate the function for a matrix. This seems reasonable enough, and it also seems to be how the matrix exponential is defined. The Maclaurin series for $(1)$ is given by $$ f(x) = \sum_{n=0}^{\infty} \frac{X^n}{(n+1)!} = I + \frac{1}{2!}X + \frac{1}{3!}X^2 + \ldots, $$ indicating that the invertibility of $X$ shouldn't matter when it comes to finding a meaningful value for $(1)$ . Now for my question: Is there a closed form of $(1)$ that do not involve the inverse? To be clear, my goal is to implement the function in Python, and I would rather do it without computing a large partial sum and use that as the approximation. I know a closed form of the matrix exponential for matrices in $\mathfrak{so}(3)$ .","I am trying to evaluate the matrix function where is the usual matrix exponential, defined by The problem is that all matrices I am working with are singular (if it is of relevance, ). According to this Wikipedia article , one may consider the function to be a real function, find its Maclaurin series, and use this to evaluate the function for a matrix. This seems reasonable enough, and it also seems to be how the matrix exponential is defined. The Maclaurin series for is given by indicating that the invertibility of shouldn't matter when it comes to finding a meaningful value for . Now for my question: Is there a closed form of that do not involve the inverse? To be clear, my goal is to implement the function in Python, and I would rather do it without computing a large partial sum and use that as the approximation. I know a closed form of the matrix exponential for matrices in .","
f(X) = \frac{e^X - I}{X}, \tag{1}
 e^X 
e^X = \sum_{n=0}^{\infty} \frac{1}{n!} X^n.
 X \in \mathfrak{so}(3) f (1) 
f(x) = \sum_{n=0}^{\infty} \frac{X^n}{(n+1)!} = I + \frac{1}{2!}X + \frac{1}{3!}X^2 + \ldots,
 X (1) (1) \mathfrak{so}(3)","['matrices', 'lie-algebras', 'matrix-exponential']"
32,How prove this determinant is $0?$,How prove this determinant is,0?,find the value $$A_{n}=\begin{vmatrix} 1-\dfrac{1}{(n+1)^2}&\dfrac{1}{2}&\dfrac{1}{3}&\cdots&\dfrac{1}{n+1}\\ \dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}&\cdots&\dfrac{1}{n+2}\\ \dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5}&\cdots&\dfrac{1}{n+3}\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ \dfrac{1}{n+1}&\dfrac{1}{n+2}&\dfrac{1}{n+3}&\cdots&\dfrac{1}{2n+1} \end{vmatrix}$$ show that $\det(A_{n})=0$ I have prove $$\det(A_{1})=\begin{vmatrix} \dfrac{3}{4}&\dfrac{1}{2}\\ \dfrac{1}{2}&\dfrac{1}{3} \end{vmatrix}=0$$ and $$\det(A_{2})=\begin{vmatrix} \dfrac{8}{9}&\dfrac{1}{2}&\dfrac{1}{3}\\ \dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}\\ \dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5} \end{vmatrix}=0$$ $$\det(A_{3})=\begin{vmatrix} \dfrac{15}{16}&\dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}\\ \dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5}\\ \dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5}&\dfrac{1}{6}\\ \dfrac{1}{4}&\dfrac{1}{5}&\dfrac{1}{6}&\dfrac{!}{7} \end{vmatrix}=0$$,find the value show that I have prove and,"A_{n}=\begin{vmatrix}
1-\dfrac{1}{(n+1)^2}&\dfrac{1}{2}&\dfrac{1}{3}&\cdots&\dfrac{1}{n+1}\\
\dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}&\cdots&\dfrac{1}{n+2}\\
\dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5}&\cdots&\dfrac{1}{n+3}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
\dfrac{1}{n+1}&\dfrac{1}{n+2}&\dfrac{1}{n+3}&\cdots&\dfrac{1}{2n+1}
\end{vmatrix} \det(A_{n})=0 \det(A_{1})=\begin{vmatrix}
\dfrac{3}{4}&\dfrac{1}{2}\\
\dfrac{1}{2}&\dfrac{1}{3}
\end{vmatrix}=0 \det(A_{2})=\begin{vmatrix}
\dfrac{8}{9}&\dfrac{1}{2}&\dfrac{1}{3}\\
\dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}\\
\dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5}
\end{vmatrix}=0 \det(A_{3})=\begin{vmatrix}
\dfrac{15}{16}&\dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}\\
\dfrac{1}{2}&\dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5}\\
\dfrac{1}{3}&\dfrac{1}{4}&\dfrac{1}{5}&\dfrac{1}{6}\\
\dfrac{1}{4}&\dfrac{1}{5}&\dfrac{1}{6}&\dfrac{!}{7}
\end{vmatrix}=0",['matrices']
33,Matrices over noncommutative rings?,Matrices over noncommutative rings?,,"In chapter 1, section 2 of Categories for the Working Mathematician , Mac Lane says: For each commutative ring $K$, the set $\mathbf{Matr_K}$ of all rectangular matrices with entries in $K$ is a category; the objects are all positive integers $m,n,...$, and each $m\times n$ matrix $A$ is regarded as an arrow $A:n\rightarrow m$, with composition the usual matrix product. This is undoubtedly true. But why restrict the statement to commutative rings? Surely, matrices over noncommutative rings also form a category. Or am I missing something? The answer to this question suggests the noncommutative rings are just not that well studied, so perhaps Perhaps Mac Lane was going for familiarly over generality. Another question addresses the same passage in the book but does not raise the question of commutativity. To be specific about my questions Is my claim ""For each noncommutative ring $K$, the set of all rectangular matrices with entries in $K$ is a category"" true? Is the category of rectangular matrices over a commutative ring somehow nicer than that over all (commutative or otherwise) rings? EDIT: Strikeout the claim that noncommutative rings are not well studied, given the consensus in the comments that they are. In that case, why stipulate commutative rings? Were they less well studied back in the 1970s?","In chapter 1, section 2 of Categories for the Working Mathematician , Mac Lane says: For each commutative ring $K$, the set $\mathbf{Matr_K}$ of all rectangular matrices with entries in $K$ is a category; the objects are all positive integers $m,n,...$, and each $m\times n$ matrix $A$ is regarded as an arrow $A:n\rightarrow m$, with composition the usual matrix product. This is undoubtedly true. But why restrict the statement to commutative rings? Surely, matrices over noncommutative rings also form a category. Or am I missing something? The answer to this question suggests the noncommutative rings are just not that well studied, so perhaps Perhaps Mac Lane was going for familiarly over generality. Another question addresses the same passage in the book but does not raise the question of commutativity. To be specific about my questions Is my claim ""For each noncommutative ring $K$, the set of all rectangular matrices with entries in $K$ is a category"" true? Is the category of rectangular matrices over a commutative ring somehow nicer than that over all (commutative or otherwise) rings? EDIT: Strikeout the claim that noncommutative rings are not well studied, given the consensus in the comments that they are. In that case, why stipulate commutative rings? Were they less well studied back in the 1970s?",,"['matrices', 'category-theory', 'noncommutative-algebra']"
34,Proof of spectral radius bound $\min_i \sum_j a_{ij} \le \rho(A) \le \max_i \sum_j a_{ij}$,Proof of spectral radius bound,\min_i \sum_j a_{ij} \le \rho(A) \le \max_i \sum_j a_{ij},I was reading one of the theorem in Roger A. Horn's Matrix Analysis and yet failed to understand how to prove it. Let $A=[a_{ij}] \in M_n$ be nonnegative and $\rho(A)$ is spectral radius of $A$.  Then $$\min_{1 \le i \le n} \sum_{j=1}^n a_{ij} \le \rho(A) \le \max_{1 \le i \le n} \sum_{j=1}^n a_{ij}$$ and $$\min_{1 \le j \le n} \sum_{i=1}^n a_{ij} \le \rho(A) \le \max_{1 \le j \le n} \sum_{i=1}^n a_{ij}$$ Can anyone help me to give me brief explaination and detailed proof of this theorem?,I was reading one of the theorem in Roger A. Horn's Matrix Analysis and yet failed to understand how to prove it. Let $A=[a_{ij}] \in M_n$ be nonnegative and $\rho(A)$ is spectral radius of $A$.  Then $$\min_{1 \le i \le n} \sum_{j=1}^n a_{ij} \le \rho(A) \le \max_{1 \le i \le n} \sum_{j=1}^n a_{ij}$$ and $$\min_{1 \le j \le n} \sum_{i=1}^n a_{ij} \le \rho(A) \le \max_{1 \le j \le n} \sum_{i=1}^n a_{ij}$$ Can anyone help me to give me brief explaination and detailed proof of this theorem?,,"['matrices', 'eigenvalues-eigenvectors', 'spectral-radius']"
35,Computing flops for matrix multiplication,Computing flops for matrix multiplication,,"I'm trying to compute the flops necessary to multiply $A(BC)$ and $(AB)C$, given $A \in \Bbb{R}^{m\times p}, B \in \Bbb{R}^{p\times n}, C \in \Bbb{R}^{n\times q}$,  using the definition above. I've determined: $A(BC)$ = $(2n-1)pq + (2p-1)mq$ flops and $(AB)C$ = $(2p-1)mn + (2n-1)mq$ flops. Seeing as only one term, $-mq$, appears after distribution, I'm not sure which choice would have less flops.","I'm trying to compute the flops necessary to multiply $A(BC)$ and $(AB)C$, given $A \in \Bbb{R}^{m\times p}, B \in \Bbb{R}^{p\times n}, C \in \Bbb{R}^{n\times q}$,  using the definition above. I've determined: $A(BC)$ = $(2n-1)pq + (2p-1)mq$ flops and $(AB)C$ = $(2p-1)mn + (2n-1)mq$ flops. Seeing as only one term, $-mq$, appears after distribution, I'm not sure which choice would have less flops.",,"['matrices', 'numerical-methods', 'computational-complexity', 'numerical-linear-algebra', 'computational-mathematics']"
36,Problems in finding an integral basis in a ring of algebraic ntegers,Problems in finding an integral basis in a ring of algebraic ntegers,,"I have some problems with this paper . Firstly, with theorem 3.4: I don't know if this is the original formulation in the literature, but the statement seems rather trivial to me if one considers $0$ to be an algebraic integer. Moreover there are cases where a non trivial integer doesn't even exist as in the classical example of the ring of integers of  $\Bbb Q(\sqrt{d})$ where $d \in  \Bbb Z$ and $d \neq 1 \mod 4$. Secondly, in the algorithm that is based on this theorem. As an example I consider the polynomial $x^4+5x+5$. The Galois group of this polynomial over $\Bbb Q$ is the cyclic group of order $4$ abd its splitting field can be represented by its companion matrix $M = \left(\begin{smallmatrix}0 & 0 & 0 & -5\\1 & 0 & 0 & -5\\0 & 1 & 0 & 0\\0 & 0 & 1 & 0\end{smallmatrix}\right)$. Since $M$ is an algebraic integer one can take as a basis of integers for the splitting field the set $B = \{M^0,M,M^2,M^3\}$.The discriminant of this basis using $\Delta = \det(\operatorname{Tr}(B_iB_j))$ equals $5^3\dot 11^2$ (as does the discriminant of the polynomial). If I apply the algorithm I find an algebraic integer with coefficients $\frac{1}{11}\left [ 1, 3, 6,1\right ]$ (the minimal polynomial is $x^4+x^3+6x^2-4x+1$). If I replace in B the second element by this element I obtain a basis whose discriminant now is $5^33^2$, so we are nowhere nearer to an integral basis, on the contrary, one bastard goes out and another comes in. Conclusion: We know that the discriminant of an intergral basis divides that of a basis of integers so in out case it divides both discriminants so also their $\gcd, 5^3$. If only there was a way to calculate the $\gcd$ of two bases of integers.","I have some problems with this paper . Firstly, with theorem 3.4: I don't know if this is the original formulation in the literature, but the statement seems rather trivial to me if one considers $0$ to be an algebraic integer. Moreover there are cases where a non trivial integer doesn't even exist as in the classical example of the ring of integers of  $\Bbb Q(\sqrt{d})$ where $d \in  \Bbb Z$ and $d \neq 1 \mod 4$. Secondly, in the algorithm that is based on this theorem. As an example I consider the polynomial $x^4+5x+5$. The Galois group of this polynomial over $\Bbb Q$ is the cyclic group of order $4$ abd its splitting field can be represented by its companion matrix $M = \left(\begin{smallmatrix}0 & 0 & 0 & -5\\1 & 0 & 0 & -5\\0 & 1 & 0 & 0\\0 & 0 & 1 & 0\end{smallmatrix}\right)$. Since $M$ is an algebraic integer one can take as a basis of integers for the splitting field the set $B = \{M^0,M,M^2,M^3\}$.The discriminant of this basis using $\Delta = \det(\operatorname{Tr}(B_iB_j))$ equals $5^3\dot 11^2$ (as does the discriminant of the polynomial). If I apply the algorithm I find an algebraic integer with coefficients $\frac{1}{11}\left [ 1, 3, 6,1\right ]$ (the minimal polynomial is $x^4+x^3+6x^2-4x+1$). If I replace in B the second element by this element I obtain a basis whose discriminant now is $5^33^2$, so we are nowhere nearer to an integral basis, on the contrary, one bastard goes out and another comes in. Conclusion: We know that the discriminant of an intergral basis divides that of a basis of integers so in out case it divides both discriminants so also their $\gcd, 5^3$. If only there was a way to calculate the $\gcd$ of two bases of integers.",,"['matrices', 'number-theory', 'algebraic-number-theory']"
37,"If $P$ is positive definite, is$X^T P X$ also positive definite?","If  is positive definite, is also positive definite?",P X^T P X,"Let $P\in\Re^{r\times r}$ be a symmetric positive definite matrix; i.e. $P=P^T\succ 0$ . Also, let $X\in\Re^{r\times m}$ be a matrix such that $\mathrm{rank}(X)=r$ with $r\leq m$ . My question is about the positive definiteness (or positive semi-definiteness) of the following product: $X^TPX$ Can we say anything about the positive (semi-)definiteness of this product? As far as I tried with several numerical examples, we should be able to say that it is indeed semidefinite.","Let be a symmetric positive definite matrix; i.e. . Also, let be a matrix such that with . My question is about the positive definiteness (or positive semi-definiteness) of the following product: Can we say anything about the positive (semi-)definiteness of this product? As far as I tried with several numerical examples, we should be able to say that it is indeed semidefinite.",P\in\Re^{r\times r} P=P^T\succ 0 X\in\Re^{r\times m} \mathrm{rank}(X)=r r\leq m X^TPX,"['matrices', 'positive-definite', 'positive-semidefinite']"
38,"When does a real, positive definite matrix have positive entries?","When does a real, positive definite matrix have positive entries?",,"Let $A = (a_{ij})_{i,j=1,\dots,n}$ be a real symmetric square matrix. Suppose $A$ is positive definite.  Are there sufficient conditions that guarantee $a_{ij} > 0$ for all $i,j = 1,\dots, n$? I know that $a_{ii} > 0$ for all $i=1,\dots,n.$ One thing I found is that $a_{ij} > 0$ for all $i,j = 1, \dots, n$ if and only if $A^{-1}$ is monotone , i.e. $A^{-1}x \ge 0$ implies $x \ge 0$ for all $x \in \mathbb{R}^n.$ Is there a nice way to connect this with the fact that $A$ and $A^{-1}$ are positive definite?","Let $A = (a_{ij})_{i,j=1,\dots,n}$ be a real symmetric square matrix. Suppose $A$ is positive definite.  Are there sufficient conditions that guarantee $a_{ij} > 0$ for all $i,j = 1,\dots, n$? I know that $a_{ii} > 0$ for all $i=1,\dots,n.$ One thing I found is that $a_{ij} > 0$ for all $i,j = 1, \dots, n$ if and only if $A^{-1}$ is monotone , i.e. $A^{-1}x \ge 0$ implies $x \ge 0$ for all $x \in \mathbb{R}^n.$ Is there a nice way to connect this with the fact that $A$ and $A^{-1}$ are positive definite?",,"['matrices', 'positive-definite', 'positive-semidefinite']"
39,Cholesky decompostion: upper triangular or lower triangular?,Cholesky decompostion: upper triangular or lower triangular?,,"Can I find and use $U$ such that $$A = U U^{T}$$ where $U$ is an upper triangular matrix, to find a solution instead of finding $L$ such that $ A = L L^{T}$ (where $L$ is a lower triangular matrix) to solve $Ax=b$ using Cholesky factorization? If not, what is the correct way of using Cholesky factorization?","Can I find and use $U$ such that $$A = U U^{T}$$ where $U$ is an upper triangular matrix, to find a solution instead of finding $L$ such that $ A = L L^{T}$ (where $L$ is a lower triangular matrix) to solve $Ax=b$ using Cholesky factorization? If not, what is the correct way of using Cholesky factorization?",,"['matrices', 'numerical-linear-algebra', 'matrix-decomposition']"
40,How to do a unitary diagonalization of a normal matrix?,How to do a unitary diagonalization of a normal matrix?,,"It is easy to diagonalize a normal matrix such that $D = P^{-1} A P$ by simply putting all the orthogonal eigenvectors as columns for $P$. But I spent hours trying a unitary diagonalization of the following Hermitian (and therefore Normal) matrix: $$  A =   \begin{bmatrix}     0 & i & 1 \\     -i & 0 & 0 \\     1 & 0 & 0   \end{bmatrix} $$ such that $ D = U^*AU $. I know that by definition every normal matrix is unitarily diagonalizable. The eigenvalues of this matrix are $ \{ 0, -\sqrt{2}, \sqrt{2} \} $. What did not work but was my most promising try, was to scale down the eigenvectors by their norm so the matrix $ P $ became orthonormal. The result does not give me the diagonal matrix with the desired eigenvalues though. Also, Google search did not yield a single nicely explained way to do a unitary transform of a normal matrix. The only document that I believe to try to explain it is here , although it does not show clearly how to construct $ U $.","It is easy to diagonalize a normal matrix such that $D = P^{-1} A P$ by simply putting all the orthogonal eigenvectors as columns for $P$. But I spent hours trying a unitary diagonalization of the following Hermitian (and therefore Normal) matrix: $$  A =   \begin{bmatrix}     0 & i & 1 \\     -i & 0 & 0 \\     1 & 0 & 0   \end{bmatrix} $$ such that $ D = U^*AU $. I know that by definition every normal matrix is unitarily diagonalizable. The eigenvalues of this matrix are $ \{ 0, -\sqrt{2}, \sqrt{2} \} $. What did not work but was my most promising try, was to scale down the eigenvectors by their norm so the matrix $ P $ became orthonormal. The result does not give me the diagonal matrix with the desired eigenvalues though. Also, Google search did not yield a single nicely explained way to do a unitary transform of a normal matrix. The only document that I believe to try to explain it is here , although it does not show clearly how to construct $ U $.",,"['matrices', 'diagonalization']"
41,"Semisimple, connected Lie groups generated by unipotent elements.","Semisimple, connected Lie groups generated by unipotent elements.",,"Let $G$ be a linear, semisimple Lie group with no compact factors. The unipotent elements of $G$ are those that have only eigenvalue 1. I've seen it asserted that $G$ is generated by its unipotent elements: see Exercise #2 $\S 4.5$ in Dave Witte Morris' book on Arithmetic Groups . The hint in the book is that you consider the simple factors. But even considering $SL(2, \mathbb{R})$, it is unclear to me why it is generated by its unipotent elements. I am aware that any matrix in $G$ can be written as a product of commuting hyperbolic, elliptic, and unipotent element, but I am unsure of how you might generally express the hyperbolic and elliptic elements as unipotent elements.","Let $G$ be a linear, semisimple Lie group with no compact factors. The unipotent elements of $G$ are those that have only eigenvalue 1. I've seen it asserted that $G$ is generated by its unipotent elements: see Exercise #2 $\S 4.5$ in Dave Witte Morris' book on Arithmetic Groups . The hint in the book is that you consider the simple factors. But even considering $SL(2, \mathbb{R})$, it is unclear to me why it is generated by its unipotent elements. I am aware that any matrix in $G$ can be written as a product of commuting hyperbolic, elliptic, and unipotent element, but I am unsure of how you might generally express the hyperbolic and elliptic elements as unipotent elements.",,"['matrices', 'lie-groups']"
42,Matrix on complex field.,Matrix on complex field.,,"An $n\times n$ complex matrix $A$ satisfies $A^k=I_n$, where $I_n$ is $n\times n$ identity matrix and $k$ is positive integer $\gt1$. Suppose that 1 is not an eigen value of $A$. Then which of the fallowing is necessarily true? 1) $A$ is diagonalizble 2) $A+A^2+...+A^{k-1}=0$ 3) $tr(A)+tr(A^2)+...+tr(A^{k-1})=-n$ 4) $A^{-1}+A^{-2}+...+A^{-(k-1)}=-I_n$ I think 1) and 2) are ture.","An $n\times n$ complex matrix $A$ satisfies $A^k=I_n$, where $I_n$ is $n\times n$ identity matrix and $k$ is positive integer $\gt1$. Suppose that 1 is not an eigen value of $A$. Then which of the fallowing is necessarily true? 1) $A$ is diagonalizble 2) $A+A^2+...+A^{k-1}=0$ 3) $tr(A)+tr(A^2)+...+tr(A^{k-1})=-n$ 4) $A^{-1}+A^{-2}+...+A^{-(k-1)}=-I_n$ I think 1) and 2) are ture.",,['matrices']
43,Counterexamples to the Matrix norm AM-GM inequality?,Counterexamples to the Matrix norm AM-GM inequality?,,"I am new here and this my first question, I hope I am being as clear as possible and apologize in advance for any misunderstandings. I am researching the Arithmetic-Geometric Mean (AM-GM) inequality for matrices. Which can be seen as an ""extension"" of the obvious AM-GM inequality for positive numbers $a,b$ : $ \sqrt{ab} \leq \frac{1}{2}(a+b) $. One of such possible extensions is the following: $|||A^{1/2}B^{1/2}|||\leq \frac{1}{2}|||A+B|||$   As shown in ""Positive Definite Matrices"" by R.Bhatia. Where A, B are positive, hermitian matrices, and $||| \cdot |||$ is a unitarily invariant norm. This is precisely my problem/question: all proofs and attempts to investigate the AM-GM matrix-norm-inequality use unitarily invariant norms. Does this mean that for non-unitarily invariant norms this inequality does not hold? Is there a counterexample for this? perhaps for p-norms such as  $\|\cdot\|_\infty$ , $\|\cdot\|_1$? Or an explanation why a proof for non-unitarily invariant norms could not work? I am just lost since all the texts I read regarding the AM-GM inequality, immediately jump to using unitarily invariant norms. Thank you!!","I am new here and this my first question, I hope I am being as clear as possible and apologize in advance for any misunderstandings. I am researching the Arithmetic-Geometric Mean (AM-GM) inequality for matrices. Which can be seen as an ""extension"" of the obvious AM-GM inequality for positive numbers $a,b$ : $ \sqrt{ab} \leq \frac{1}{2}(a+b) $. One of such possible extensions is the following: $|||A^{1/2}B^{1/2}|||\leq \frac{1}{2}|||A+B|||$   As shown in ""Positive Definite Matrices"" by R.Bhatia. Where A, B are positive, hermitian matrices, and $||| \cdot |||$ is a unitarily invariant norm. This is precisely my problem/question: all proofs and attempts to investigate the AM-GM matrix-norm-inequality use unitarily invariant norms. Does this mean that for non-unitarily invariant norms this inequality does not hold? Is there a counterexample for this? perhaps for p-norms such as  $\|\cdot\|_\infty$ , $\|\cdot\|_1$? Or an explanation why a proof for non-unitarily invariant norms could not work? I am just lost since all the texts I read regarding the AM-GM inequality, immediately jump to using unitarily invariant norms. Thank you!!",,"['matrices', 'inequality', 'examples-counterexamples', 'normed-spaces', 'means']"
44,"Is the set of symmetric matrices in $M_k(n,\mathbb{R})$ a smooth submanifold of $M_k(n,\mathbb{R})$?",Is the set of symmetric matrices in  a smooth submanifold of ?,"M_k(n,\mathbb{R}) M_k(n,\mathbb{R})","Let $M_k(n,\mathbb{R})$ be the set of all $n \times n$ real matrices of rank $k$. This forms a smooth manifold (see Proposition 1.14 on page 133 of Optimization and Dynamical Systems by Helmke and Moore). Is the set $S$ of symmetric matrices in $M_k(n,\mathbb{R})$ a smoothly embedded submanifold of $M_k(n,\mathbb{R})$?","Let $M_k(n,\mathbb{R})$ be the set of all $n \times n$ real matrices of rank $k$. This forms a smooth manifold (see Proposition 1.14 on page 133 of Optimization and Dynamical Systems by Helmke and Moore). Is the set $S$ of symmetric matrices in $M_k(n,\mathbb{R})$ a smoothly embedded submanifold of $M_k(n,\mathbb{R})$?",,"['matrices', 'differential-geometry', 'smooth-manifolds']"
45,Compact notation for block diagonal matrices?,Compact notation for block diagonal matrices?,,Is there a more compact notation for representing a block-diagonal matrix than $\left[\begin{array}{cccc} \mathbf{W}_1 & 0 & ... & 0\\ 0 & \mathbf{W}_2 & ... & 0\\ 0 & 0 & \ddots & 0\\ 0 & 0 & ... & \mathbf{W}_n \end{array}\right] $ Something as simple as replacing $\left[\begin{array}{c} \mathbf{m}_1\\ \mathbf{m}_2\\ \vdots\\ \mathbf{m}_n \end{array}\right] $ with $\left[\mathbf{m}_1^T~\mathbf{m}_2^T~...~\mathbf{m}_n^T\right]^T$ for vectors?  I'm just trying to reduce the space required for a manuscript.  Is $ diag\left\{\mathbf{W}_1~ \mathbf{W}_2~...~\mathbf{W}_n \right\} $ acceptable notation?,Is there a more compact notation for representing a block-diagonal matrix than $\left[\begin{array}{cccc} \mathbf{W}_1 & 0 & ... & 0\\ 0 & \mathbf{W}_2 & ... & 0\\ 0 & 0 & \ddots & 0\\ 0 & 0 & ... & \mathbf{W}_n \end{array}\right] $ Something as simple as replacing $\left[\begin{array}{c} \mathbf{m}_1\\ \mathbf{m}_2\\ \vdots\\ \mathbf{m}_n \end{array}\right] $ with $\left[\mathbf{m}_1^T~\mathbf{m}_2^T~...~\mathbf{m}_n^T\right]^T$ for vectors?  I'm just trying to reduce the space required for a manuscript.  Is $ diag\left\{\mathbf{W}_1~ \mathbf{W}_2~...~\mathbf{W}_n \right\} $ acceptable notation?,,"['matrices', 'notation']"
46,One more question about mapping quaternionic matrices into real matrices,One more question about mapping quaternionic matrices into real matrices,,"Real matrices that lie in the image of the inclusion homomorphism $\rho_n: M_n(\mathbb C) \to M_{2n}(\mathbb R)$ are called complex linear real matrices. It is easy to see that a real matrix is complex linear if and only if it commutes with $I = \rho_n(iI)$. In analogy to this I am now studying the quaternionic inclusion $M_n(\mathbb H) \to M_{4n}(\mathbb R)$ using the inclusion $\psi_n: M_n(\mathbb H) \to M_{2n}(\mathbb C)$. If $i,j,k$ denote the unit quaternions then I want to find matrices $I$ and $J$ such that a real matrix is quaternionic linear if and only if it commutes with $I$ and $J$. In $1$ dimension I tried $I=\rho_{2n} \circ \psi_n (iI)$ and $J=\rho_{2n} \circ \psi_n (jI)$ but the problem then is that $I^2 \neq -1$ and $J^2 \neq -1$. Why does $I=\rho_{2n} \circ \psi_n (iI), J=\rho_{2n} \circ \psi_n (jI)$ not work in the quaternionic case? Is there an insightful geometric (or other) explanation? For the inclusion of complex matrices into real matrices setting $J=\rho_{2n} (iI)$ worked. Edit For a definition of $\rho_n$: define $\rho_n : M^n(\mathbb C) \to M^{2n}(\mathbb R)$ as $A_{ij}\mapsto \begin{array}{cc} a_{ij} & b_{ij} \\ -b_{ij} & a_{ij} \end{array}$ if $A_{ij}=(a_{ij} + i b_{ij})$ and $\color{blue}{\psi_n}: M^n(\mathbb H) \to M^{2n}(\mathbb C)$ as $A_{ij}\mapsto \begin{array}{cc} a_{ij} & b_{ij} \\ -\overline{b_{ij}} & \overline{a_{ij}} \end{array}$ if $A_{ij}=(a_{ij} +  b_{ij}j)$ Edit 2 (in response to the anwer) Let $$I = \rho(\color{blue}{\psi(i)})= \rho\left ( \begin{array}{cc} \color{blue}{i} & \color{blue}{0} \\ \color{blue}{0} & \color{blue}{-i} \end{array}\right)$$  Then  $$ I = \left ( \begin{array}{cccc}  0 & 1 & 0 & 0 \\  -1 & 0 & 0 & 0\\ 0 & 0 & 0 & -1 \\ 0 & 0 & 1 & 0 \end{array} \right )$$ so that $$  I^2 = -1$$ Edit 3 After the discussion with Incnis Mrsi I calculated $J^2=I^2 =-1$ for $J=\rho_{2}(\psi_1(j))$ and $I=\rho_{2}(\psi_1(i))$. I am confused that this seems to work. The reason why I asked this quetion is the following passage in Tapp's matrix groups for undergraduates : In particular, why is $I\neq\rho_{2n}(\psi_n(i))$ and $J\neq\rho_{2n}(\psi_n(j))$ for   $n>1$? (for $n=1$, apparently it works as I just verified).","Real matrices that lie in the image of the inclusion homomorphism $\rho_n: M_n(\mathbb C) \to M_{2n}(\mathbb R)$ are called complex linear real matrices. It is easy to see that a real matrix is complex linear if and only if it commutes with $I = \rho_n(iI)$. In analogy to this I am now studying the quaternionic inclusion $M_n(\mathbb H) \to M_{4n}(\mathbb R)$ using the inclusion $\psi_n: M_n(\mathbb H) \to M_{2n}(\mathbb C)$. If $i,j,k$ denote the unit quaternions then I want to find matrices $I$ and $J$ such that a real matrix is quaternionic linear if and only if it commutes with $I$ and $J$. In $1$ dimension I tried $I=\rho_{2n} \circ \psi_n (iI)$ and $J=\rho_{2n} \circ \psi_n (jI)$ but the problem then is that $I^2 \neq -1$ and $J^2 \neq -1$. Why does $I=\rho_{2n} \circ \psi_n (iI), J=\rho_{2n} \circ \psi_n (jI)$ not work in the quaternionic case? Is there an insightful geometric (or other) explanation? For the inclusion of complex matrices into real matrices setting $J=\rho_{2n} (iI)$ worked. Edit For a definition of $\rho_n$: define $\rho_n : M^n(\mathbb C) \to M^{2n}(\mathbb R)$ as $A_{ij}\mapsto \begin{array}{cc} a_{ij} & b_{ij} \\ -b_{ij} & a_{ij} \end{array}$ if $A_{ij}=(a_{ij} + i b_{ij})$ and $\color{blue}{\psi_n}: M^n(\mathbb H) \to M^{2n}(\mathbb C)$ as $A_{ij}\mapsto \begin{array}{cc} a_{ij} & b_{ij} \\ -\overline{b_{ij}} & \overline{a_{ij}} \end{array}$ if $A_{ij}=(a_{ij} +  b_{ij}j)$ Edit 2 (in response to the anwer) Let $$I = \rho(\color{blue}{\psi(i)})= \rho\left ( \begin{array}{cc} \color{blue}{i} & \color{blue}{0} \\ \color{blue}{0} & \color{blue}{-i} \end{array}\right)$$  Then  $$ I = \left ( \begin{array}{cccc}  0 & 1 & 0 & 0 \\  -1 & 0 & 0 & 0\\ 0 & 0 & 0 & -1 \\ 0 & 0 & 1 & 0 \end{array} \right )$$ so that $$  I^2 = -1$$ Edit 3 After the discussion with Incnis Mrsi I calculated $J^2=I^2 =-1$ for $J=\rho_{2}(\psi_1(j))$ and $I=\rho_{2}(\psi_1(i))$. I am confused that this seems to work. The reason why I asked this quetion is the following passage in Tapp's matrix groups for undergraduates : In particular, why is $I\neq\rho_{2n}(\psi_n(i))$ and $J\neq\rho_{2n}(\psi_n(j))$ for   $n>1$? (for $n=1$, apparently it works as I just verified).",,"['matrices', 'lie-groups', 'quaternions']"
47,Jacobian Matrix in dynamical systems,Jacobian Matrix in dynamical systems,,Can someone explain what exactly the Jacobian matrix is (specifically in its application to dynamical systems) and maybe give an example of how to compute it? It really confuses me...and I haven't been able to find any good resources online.,Can someone explain what exactly the Jacobian matrix is (specifically in its application to dynamical systems) and maybe give an example of how to compute it? It really confuses me...and I haven't been able to find any good resources online.,,"['matrices', 'dynamical-systems']"
48,efficient way to invert a Matrix plus a diagonal one,efficient way to invert a Matrix plus a diagonal one,,"Let $\Sigma$ be a $n \times n$ matrix, $V$ a $2 \times 2$ matrix and $I_{2 n}$ the identity matrix on dimension $2n \times 2n$. Both $\Sigma$ and $V$ are covariance matrices, thus real, symmetric and positive definite. I need to calculate $(\Sigma\otimes V+\phi I_{2 n})^{-1}$ where $\phi$ is a positive scalar and $\otimes$ is the Kronecker product. How can I use the property of the Kronecker product to compute the inversion efficiently?","Let $\Sigma$ be a $n \times n$ matrix, $V$ a $2 \times 2$ matrix and $I_{2 n}$ the identity matrix on dimension $2n \times 2n$. Both $\Sigma$ and $V$ are covariance matrices, thus real, symmetric and positive definite. I need to calculate $(\Sigma\otimes V+\phi I_{2 n})^{-1}$ where $\phi$ is a positive scalar and $\otimes$ is the Kronecker product. How can I use the property of the Kronecker product to compute the inversion efficiently?",,"['matrices', 'covariance']"
49,Minimizing Frobenius norm for two variables,Minimizing Frobenius norm for two variables,,"I need to minimize squared Frobenius norm: $\|\mathbf{A} - \mathbf{x}\mathbf{y}^T\|_F^2$. Namely I need to prove that for this norm to reach minimum $\mathbf{x}$ should be eigenvector of $\mathbf{A}\mathbf{A}^T$ corresponding to the largest eigenvalue, and  $\mathbf{y}$ should be the same for $\mathbf{A}^T\mathbf{A}$. So I tried to represent norm in the form of $\sum\limits_i\sum\limits_j(a_{ij} - x_iy_j)^2$ and take partial derivatives both for $x_i$ and $y_j$. But I'm unable to find analytical solution for this. And I believe it would be difficult to prove  relation to eigenvector of $\mathbf{A}\mathbf{A}^T$ in this kind of solution. I've tried to expand $(a_{ij} - x_iy_j)^2$ with same result. Also this looks like something from SVD, because in SVD $\mathbf{U}$ diagonalizes $\mathbf{A}\mathbf{A}^T$ and $\mathbf{V}$ diagonalizes $\mathbf{A}^T\mathbf{A}$. But again I have no idea how to solve initial task with this knowledge. Would be grateful for any suggestions.","I need to minimize squared Frobenius norm: $\|\mathbf{A} - \mathbf{x}\mathbf{y}^T\|_F^2$. Namely I need to prove that for this norm to reach minimum $\mathbf{x}$ should be eigenvector of $\mathbf{A}\mathbf{A}^T$ corresponding to the largest eigenvalue, and  $\mathbf{y}$ should be the same for $\mathbf{A}^T\mathbf{A}$. So I tried to represent norm in the form of $\sum\limits_i\sum\limits_j(a_{ij} - x_iy_j)^2$ and take partial derivatives both for $x_i$ and $y_j$. But I'm unable to find analytical solution for this. And I believe it would be difficult to prove  relation to eigenvector of $\mathbf{A}\mathbf{A}^T$ in this kind of solution. I've tried to expand $(a_{ij} - x_iy_j)^2$ with same result. Also this looks like something from SVD, because in SVD $\mathbf{U}$ diagonalizes $\mathbf{A}\mathbf{A}^T$ and $\mathbf{V}$ diagonalizes $\mathbf{A}^T\mathbf{A}$. But again I have no idea how to solve initial task with this knowledge. Would be grateful for any suggestions.",,"['matrices', 'optimization']"
50,Eigenvalues less than or equal to 1,Eigenvalues less than or equal to 1,,"What proprieties does a square $n\times n$ real matrix $\mathbf M$ need to have in order to have all it's eigenvalues be less than or equal to one in absolute value? I'm looking for proprieties such as ""Have all it's elements be less than one"" or ""The sum of the squares of it's columns have to add up to one or less"" or similar proprieties. [Note: I am not claiming these examples to be true, I am using them merely as demonstrations of the kind proprieties I am looking for] Much appreciated.","What proprieties does a square $n\times n$ real matrix $\mathbf M$ need to have in order to have all it's eigenvalues be less than or equal to one in absolute value? I'm looking for proprieties such as ""Have all it's elements be less than one"" or ""The sum of the squares of it's columns have to add up to one or less"" or similar proprieties. [Note: I am not claiming these examples to be true, I am using them merely as demonstrations of the kind proprieties I am looking for] Much appreciated.",,"['matrices', 'eigenvalues-eigenvectors']"
51,All Possible Jordan Canonical Forms Given Characteristic Polynomial,All Possible Jordan Canonical Forms Given Characteristic Polynomial,,"I am given the characteristic polynomial $x^2(x^2-1)$ and am asked to find all possible jordan canonical forms.  What I have so far is: Possible elementary divisors are: 1) $x,x,(x+1),(x-1)$, 2) $x,x,(x+1)(x-1)$, 3) $x^2,(x+1)(x-1)$, and 4) $x^2(x+1)(x-1)$.  I therefore got the possible Jordan forms as: 1)=2) \begin{matrix}         0 & 0 & 0 & 0 \\         0 & 0 & 0 & 0 \\         0 & 0 & 1 & 0 \\         0 & 0 & 0 & -1         \end{matrix} and 3)=4) \begin{matrix}         0 & 1 & 0 & 0 \\         0 & 0 & 0 & 0 \\         0 & 0 & 1 & 0 \\         0 & 0 & 0 & -1         \end{matrix} I'm really unsure if these are correct, so any insight would be greatly appreciated!","I am given the characteristic polynomial $x^2(x^2-1)$ and am asked to find all possible jordan canonical forms.  What I have so far is: Possible elementary divisors are: 1) $x,x,(x+1),(x-1)$, 2) $x,x,(x+1)(x-1)$, 3) $x^2,(x+1)(x-1)$, and 4) $x^2(x+1)(x-1)$.  I therefore got the possible Jordan forms as: 1)=2) \begin{matrix}         0 & 0 & 0 & 0 \\         0 & 0 & 0 & 0 \\         0 & 0 & 1 & 0 \\         0 & 0 & 0 & -1         \end{matrix} and 3)=4) \begin{matrix}         0 & 1 & 0 & 0 \\         0 & 0 & 0 & 0 \\         0 & 0 & 1 & 0 \\         0 & 0 & 0 & -1         \end{matrix} I'm really unsure if these are correct, so any insight would be greatly appreciated!",,"['matrices', 'jordan-normal-form']"
52,Visualizing Sylvester's law,Visualizing Sylvester's law,,"According to Sylvester's law, every $2 \times 2$ real symmetric matrix is congruent to exactly one of six standard types. List them. I know that the symmetric matrix is congruent to the diagonal matrix, but what do they want me to list. What are the ""standard types"" ? If we consider the operation of $GL_2$ on $2 \times 2$ matrices by $P \star A= PAP^t$m then Sylvester's Law asserts that the symmetric matrices form six orbits. We may view the symmetric matrices as points in $\mathbb{R}^3$, letting $(x,y,z)$ correspond to the matrix $\begin{pmatrix} x& y\\ y& z \end{pmatrix}$. Describe the decomposition of $\mathbb{R}^3$ into orbits geometrically, and make a clear drawing depicting it. Please help, I am really having difficulty","According to Sylvester's law, every $2 \times 2$ real symmetric matrix is congruent to exactly one of six standard types. List them. I know that the symmetric matrix is congruent to the diagonal matrix, but what do they want me to list. What are the ""standard types"" ? If we consider the operation of $GL_2$ on $2 \times 2$ matrices by $P \star A= PAP^t$m then Sylvester's Law asserts that the symmetric matrices form six orbits. We may view the symmetric matrices as points in $\mathbb{R}^3$, letting $(x,y,z)$ correspond to the matrix $\begin{pmatrix} x& y\\ y& z \end{pmatrix}$. Describe the decomposition of $\mathbb{R}^3$ into orbits geometrically, and make a clear drawing depicting it. Please help, I am really having difficulty",,[]
53,Chain rule for matrix exponentials,Chain rule for matrix exponentials,,"I need help in proving the following theorem: If $M(t)$ is an $n \times n$ matrix of differentiable functions, then $$ \frac{d}{dt}\left( \exp(M(t))\right) = \frac{d}{dt}M(t) \exp(M(t)) = \exp(M(t)) M'(t) $$    if and only if $M(t)$ and $\frac{d}{dt} M(t)$ commute. Please this is not homework. I am reading about the chain rule for matrix exponentials and I came across it.","I need help in proving the following theorem: If $M(t)$ is an $n \times n$ matrix of differentiable functions, then $$ \frac{d}{dt}\left( \exp(M(t))\right) = \frac{d}{dt}M(t) \exp(M(t)) = \exp(M(t)) M'(t) $$    if and only if $M(t)$ and $\frac{d}{dt} M(t)$ commute. Please this is not homework. I am reading about the chain rule for matrix exponentials and I came across it.",,"['matrices', 'ordinary-differential-equations']"
54,$\|A-B\|^2 = ?$,,\|A-B\|^2 = ?,"We know that if $x,y \in \mathbb{R}$ \begin{equation} (x-y)^2 = x^2 -2xy + y^2 \end{equation} If $x,y$ are vectors in $\mathbb{R}^n$ we have \begin{equation} |x-y|^2=|x|^2 - 2 \ x \cdot y +|y|^2. \end{equation} where $x\cdot y$ is the usual scalar product. We know that there are several ways to define $\|A\|$ when $A$ is a matrix, for example \begin{equation} \|A\| = \sup  \{ |Ax|: |x|=1\} \end{equation} Is there a similar formula as above? This is a formula to $\|A-B\|^2?$","We know that if $x,y \in \mathbb{R}$ \begin{equation} (x-y)^2 = x^2 -2xy + y^2 \end{equation} If $x,y$ are vectors in $\mathbb{R}^n$ we have \begin{equation} |x-y|^2=|x|^2 - 2 \ x \cdot y +|y|^2. \end{equation} where $x\cdot y$ is the usual scalar product. We know that there are several ways to define $\|A\|$ when $A$ is a matrix, for example \begin{equation} \|A\| = \sup  \{ |Ax|: |x|=1\} \end{equation} Is there a similar formula as above? This is a formula to $\|A-B\|^2?$",,['matrices']
55,"Let $A$ be a matrix sized $p\times p$, where $2\le p$. Using recurrence relations, describe $A^k$.","Let  be a matrix sized , where . Using recurrence relations, describe .",A p\times p 2\le p A^k,"Let $A$ be a matrix sized $p\times p$, Where $2\le p$. The matrix values in the main diagonal are $0$ and the rest are $1$'s. Example for $A$ where $p=5$: $$\begin{bmatrix} 0 & 1 & 1 & 1 & 1 \\ 1 & 0 & 1 & 1 & 1 \\ 1 & 1 & 0 & 1 & 1 \\ 1 & 1 & 1 & 0 & 1 \\ 1 & 1 & 1 & 1 & 0 \\ \end{bmatrix} $$ Using reccurences relations, describe $A^k$.","Let $A$ be a matrix sized $p\times p$, Where $2\le p$. The matrix values in the main diagonal are $0$ and the rest are $1$'s. Example for $A$ where $p=5$: $$\begin{bmatrix} 0 & 1 & 1 & 1 & 1 \\ 1 & 0 & 1 & 1 & 1 \\ 1 & 1 & 0 & 1 & 1 \\ 1 & 1 & 1 & 0 & 1 \\ 1 & 1 & 1 & 1 & 0 \\ \end{bmatrix} $$ Using reccurences relations, describe $A^k$.",,"['matrices', 'recurrence-relations']"
56,"Every p-group is isomorphic to some subgroup of U(n,p)","Every p-group is isomorphic to some subgroup of U(n,p)",,"Let $U(n,p)$ be the group of upper diagonal matrices with elements from $\mathbb{F}_p$ and determinant $1$. Then prove/disprove that every $p$-group is isomorphic to some subgroup of $U(n,p)$. My ideas: We can go about by induction, since if $G = A \times B$, then we can append the matrices along the diagonal, to get a corresponding matrix for G. Another idea is that we can consider the Jordan decomposition of the permutation matrices.","Let $U(n,p)$ be the group of upper diagonal matrices with elements from $\mathbb{F}_p$ and determinant $1$. Then prove/disprove that every $p$-group is isomorphic to some subgroup of $U(n,p)$. My ideas: We can go about by induction, since if $G = A \times B$, then we can append the matrices along the diagonal, to get a corresponding matrix for G. Another idea is that we can consider the Jordan decomposition of the permutation matrices.",,"['group-theory', 'matrices']"
57,What is the fastest numeric method for determinant calculation?,What is the fastest numeric method for determinant calculation?,,"I have a C++ matrix class which can do the following operations on a square matrix related to determinant calculation: LU Decomposition Calculation of eigenvalues Calculation of determinant by adjoint method I want to calculate determinant. Which these there methods is faster? I can say that the answer is not ""3"", but at least can you compare the first two methods for me? Is there any other numeric method which is better (faster and/or more reliable) than the ones in the list? My matrix class can also do QR decomposition and can solve linear matrix equations in Ax=b format; can these features be used in determinant calculation?","I have a C++ matrix class which can do the following operations on a square matrix related to determinant calculation: LU Decomposition Calculation of eigenvalues Calculation of determinant by adjoint method I want to calculate determinant. Which these there methods is faster? I can say that the answer is not ""3"", but at least can you compare the first two methods for me? Is there any other numeric method which is better (faster and/or more reliable) than the ones in the list? My matrix class can also do QR decomposition and can solve linear matrix equations in Ax=b format; can these features be used in determinant calculation?",,"['matrices', 'numerical-methods', 'determinant']"
58,"Symmetric, upper triangular, diagonal and null-trace matrix spaces: are they manifolds?","Symmetric, upper triangular, diagonal and null-trace matrix spaces: are they manifolds?",,"I have to prove that to each of following classes of matrices can be given a manifold structure: symmetric (denoted with $\mathcal{S}$) upper triangular diagonal null trace. I am interested in rather simply proofs that do not allow methods from the theory of Lie groups, instead tools like implicit functions theorem, basic results from basic topology, very basic results from manifold theory are allowed. About (1) I believe that since $\mathcal{S}$ is isomorphic to the upper right triangular matrices, we can consider the bijection $$ M \in \mathcal{S} \mapsto (a_{11}\dots a_{1n},a_{22},\dots,a_{2n},a_{33},\dots a_{nn}) \in \mathbb{R}^{\frac{n(n+1)}{2}}$$ which defines a bijection. So we can trivially induce a topology on $\mathcal{S}$ from the standard topology over $\mathbb{R}^{\frac{n(n+1)}{2}}$, obtaining a single chart $C^\infty$ atlas (please correct if I'm wrong!!!). For (2) and (3) the reasoning is almost the same as in (1). About (4) I'd like to try with implicit function theorem, taking the space of $n \times n$ matrices ($n^2$-manifold) as starting point, considering the defining equation $f = \sum_i a_{ii} = 0$ and observing that the gradient of $f$ is not $0$ for each null trace matrix. So we can conclude that the space of null trace matrices is a closed submanifold of $M(n,\mathbb{R})$ of dimension $n^2-1$. Is the preceding reasoning correct? Thanks.","I have to prove that to each of following classes of matrices can be given a manifold structure: symmetric (denoted with $\mathcal{S}$) upper triangular diagonal null trace. I am interested in rather simply proofs that do not allow methods from the theory of Lie groups, instead tools like implicit functions theorem, basic results from basic topology, very basic results from manifold theory are allowed. About (1) I believe that since $\mathcal{S}$ is isomorphic to the upper right triangular matrices, we can consider the bijection $$ M \in \mathcal{S} \mapsto (a_{11}\dots a_{1n},a_{22},\dots,a_{2n},a_{33},\dots a_{nn}) \in \mathbb{R}^{\frac{n(n+1)}{2}}$$ which defines a bijection. So we can trivially induce a topology on $\mathcal{S}$ from the standard topology over $\mathbb{R}^{\frac{n(n+1)}{2}}$, obtaining a single chart $C^\infty$ atlas (please correct if I'm wrong!!!). For (2) and (3) the reasoning is almost the same as in (1). About (4) I'd like to try with implicit function theorem, taking the space of $n \times n$ matrices ($n^2$-manifold) as starting point, considering the defining equation $f = \sum_i a_{ii} = 0$ and observing that the gradient of $f$ is not $0$ for each null trace matrix. So we can conclude that the space of null trace matrices is a closed submanifold of $M(n,\mathbb{R})$ of dimension $n^2-1$. Is the preceding reasoning correct? Thanks.",,['matrices']
59,"If a matrix is written with a double bar instead of square brackets, is there any significance?","If a matrix is written with a double bar instead of square brackets, is there any significance?",,"Shilov's Linear Algebra writes a matrix with two bars on each side rather than square brackets. I didn't find any mention of it with a quick Google search, and I can't see any other examples this double bar notation. Does it mean something other than a matrix? Is it an older notation? Or is it something else? Thank you","Shilov's Linear Algebra writes a matrix with two bars on each side rather than square brackets. I didn't find any mention of it with a quick Google search, and I can't see any other examples this double bar notation. Does it mean something other than a matrix? Is it an older notation? Or is it something else? Thank you",,"['matrices', 'notation']"
60,Orthogonal matrix over cross product,Orthogonal matrix over cross product,,"Let $a$ and $b$ be two unitary vectors in $\mathbb E^3$ , and let $Q$ be an orthogonal matrix. Does the following hold? $$Qa \wedge Qb = \pm Q(a \wedge b)$$","Let and be two unitary vectors in , and let be an orthogonal matrix. Does the following hold?",a b \mathbb E^3 Q Qa \wedge Qb = \pm Q(a \wedge b),"['matrices', 'vectors']"
61,Invertible $N \times N$ matrix over ${\rm GF}(2)$ having on each row and column $N/2$ ones,Invertible  matrix over  having on each row and column  ones,N \times N {\rm GF}(2) N/2,"As per the title, I'm looking for the name and for a way to construct a ${\rm GF}(2)$ square matrix of size $N$ with the following properties: All rows/columns should be linearly independent On each row and each column there should be $N/2$ ones and zeros Can anybody provide some pointers? It doesn't have to be a general solution (i.e. for all $N$) as long as it works for some $N$ it will be ok. edit: It has been pointed out that if N is a multiple of 4 the two conditions above are incompatible. Let me relax number 2) by allowing, when N is a multiple of 4, the rows and columns to be (minimally) unbalanced. edit2: Bonus points for pointing out a way to build matrices as the ones above, with the added constraint that the conditions above should be valid also for their inverse (obviously number 1 is implied).","As per the title, I'm looking for the name and for a way to construct a ${\rm GF}(2)$ square matrix of size $N$ with the following properties: All rows/columns should be linearly independent On each row and each column there should be $N/2$ ones and zeros Can anybody provide some pointers? It doesn't have to be a general solution (i.e. for all $N$) as long as it works for some $N$ it will be ok. edit: It has been pointed out that if N is a multiple of 4 the two conditions above are incompatible. Let me relax number 2) by allowing, when N is a multiple of 4, the rows and columns to be (minimally) unbalanced. edit2: Bonus points for pointing out a way to build matrices as the ones above, with the added constraint that the conditions above should be valid also for their inverse (obviously number 1 is implied).",,"['matrices', 'random']"
62,Postive definiteness of block matrix,Postive definiteness of block matrix,,"I'm going to write a lower-size version of my question. The Solution or hint for this one might be sufficient as well. Let $G=\begin{bmatrix} A & B\\ C&D \end{bmatrix} $ where $$A_{11}=2C_{1}^{\frac{1}{2}}(\langle\eta_1,\eta_1\rangle)=2$$ $$A_{12}=2C_{1}^{\frac{1}{2}}(\langle\eta_1,\eta_2\rangle) +i(\eta_{11}\eta_{22}-\eta_{12}\eta_{21})C_{0}^{\frac{3}{2}}(\langle\eta_1,\eta_2\rangle)$$ $$A_{21}=\overline{A_{12}}$$ $$A_{22}=2C_{1}^{\frac{1}{2}}(\langle\eta_2,\eta_2\rangle)=2$$ So $A$ is a hermitian matrix. $$B_{11}=0$$ $$B_{12}=(\eta_{11}\eta_{23}-\eta_{13}\eta_{21})C_{0}^{\frac{3}{2}}(\langle\eta_1,\eta_2\rangle) +i(\eta_{12}\eta_{23}-\eta_{13}\eta_{22})C_{0}^{\frac{3}{2}}(\langle\eta_1,\eta_2\rangle)$$ $$B_{21}=-B_{12}$$ $$B_{22}=0$$ $$C=-\overline{B}$$ i.e., $C_{11}=C_{22}=0, $ $C_{12}=-\overline{B_{12}}, $ and $C_{21}=\overline{B_{12}}$ , and $$D=\overline{A}$$ i.e., $D_{11}=D_{22}=2, $ $D_{12}=\overline{A_{12}}, $ and $D_{21}=A_{12}$ . where $\eta_{i}=(\eta_{i1},\eta_{i2},\eta_{i3})$ (we have only two points) are the points on the unit sphere. And $\langle\cdot,\cdot\rangle$ is the inner product. Prove that $G$ is positive definite for almost every random choice of points. Edit after a comment \begin{align*} 	\det(G)&=16+\vert A_{12}\vert^{4}-\vert B_{12}\vert^{4}-8(\vert A_{12}\vert^{2}+\vert B_{12}\vert^{2})\\            &\;\;\; +2\vert A_{12}\vert^{2}\vert B_{12}\vert^{2}+(\vert A_{12}\vert^{2}S+\vert B_{12}\vert^{2}T)i-TS \end{align*} where \begin{align*} 	\vert A_{12}\vert^{2}&=4 \big(C_{1}^{\frac{1}{2}}(\langle \eta_{1},\eta_{2} \rangle)\big)^{2}+(\eta_{11}\eta_{22}-\eta_{12}\eta_{21})^{2} \big( C_{0}^{\frac{3}{2}}(\langle \eta_{1},\eta_{2} \rangle)\big)^{2}\\ 	\vert B_{12}\vert^{2}&=(\eta_{11}\eta_{23}-\eta_{13}\eta_{21})^{2} \big( C_{0}^{\frac{3}{2}}(\langle\eta_1,\eta_2\rangle)\big)^{2} +(\eta_{12}\eta_{23}-\eta_{13}\eta_{22})^{2} \big( C_{0}^{\frac{3}{2}}(\langle\eta_1,\eta_2\rangle)\big)^{2}\\ 	T&=4\, C_{1}^{\frac{1}{2}}(\langle \eta_{1},\eta_{2} \rangle)\, C_{0}^{\frac{3}{2}}(\langle \eta_{1},\eta_{2} \rangle)\, (\eta_{11}\eta_{22}-\eta_{12}\eta_{21})\\ 	S&=2\,\big(C_{0}^{\frac{3}{2}}(\langle \eta_{1},\eta_{2} \rangle)\big)^{2}\, (\eta_{11}\eta_{23}-\eta_{13}\eta_{21})\, (\eta_{12}\eta_{23}-\eta_{13}\eta_{22})\\ \end{align*} I understand that $C_{1}^{\frac{1}{2}}(\langle \eta_{1},\eta_{2} \rangle)=\langle \eta_{1},\eta_{2} \rangle=\cos(\theta)$ and $C_{0}^{\frac{3}{2}}(\langle \eta_{1},\eta_{2} \rangle)=1$ . But even after this it's not obvious that the determinant is nonzero.","I'm going to write a lower-size version of my question. The Solution or hint for this one might be sufficient as well. Let where So is a hermitian matrix. i.e., and , and i.e., and . where (we have only two points) are the points on the unit sphere. And is the inner product. Prove that is positive definite for almost every random choice of points. Edit after a comment where I understand that and . But even after this it's not obvious that the determinant is nonzero.","G=\begin{bmatrix} A & B\\
C&D
\end{bmatrix}
 A_{11}=2C_{1}^{\frac{1}{2}}(\langle\eta_1,\eta_1\rangle)=2 A_{12}=2C_{1}^{\frac{1}{2}}(\langle\eta_1,\eta_2\rangle) +i(\eta_{11}\eta_{22}-\eta_{12}\eta_{21})C_{0}^{\frac{3}{2}}(\langle\eta_1,\eta_2\rangle) A_{21}=\overline{A_{12}} A_{22}=2C_{1}^{\frac{1}{2}}(\langle\eta_2,\eta_2\rangle)=2 A B_{11}=0 B_{12}=(\eta_{11}\eta_{23}-\eta_{13}\eta_{21})C_{0}^{\frac{3}{2}}(\langle\eta_1,\eta_2\rangle) +i(\eta_{12}\eta_{23}-\eta_{13}\eta_{22})C_{0}^{\frac{3}{2}}(\langle\eta_1,\eta_2\rangle) B_{21}=-B_{12} B_{22}=0 C=-\overline{B} C_{11}=C_{22}=0,  C_{12}=-\overline{B_{12}},  C_{21}=\overline{B_{12}} D=\overline{A} D_{11}=D_{22}=2,  D_{12}=\overline{A_{12}},  D_{21}=A_{12} \eta_{i}=(\eta_{i1},\eta_{i2},\eta_{i3}) \langle\cdot,\cdot\rangle G \begin{align*}
	\det(G)&=16+\vert A_{12}\vert^{4}-\vert B_{12}\vert^{4}-8(\vert A_{12}\vert^{2}+\vert B_{12}\vert^{2})\\
           &\;\;\; +2\vert A_{12}\vert^{2}\vert B_{12}\vert^{2}+(\vert A_{12}\vert^{2}S+\vert B_{12}\vert^{2}T)i-TS
\end{align*} \begin{align*}
	\vert A_{12}\vert^{2}&=4 \big(C_{1}^{\frac{1}{2}}(\langle \eta_{1},\eta_{2} \rangle)\big)^{2}+(\eta_{11}\eta_{22}-\eta_{12}\eta_{21})^{2} \big( C_{0}^{\frac{3}{2}}(\langle \eta_{1},\eta_{2} \rangle)\big)^{2}\\
	\vert B_{12}\vert^{2}&=(\eta_{11}\eta_{23}-\eta_{13}\eta_{21})^{2} \big( C_{0}^{\frac{3}{2}}(\langle\eta_1,\eta_2\rangle)\big)^{2} +(\eta_{12}\eta_{23}-\eta_{13}\eta_{22})^{2} \big( C_{0}^{\frac{3}{2}}(\langle\eta_1,\eta_2\rangle)\big)^{2}\\
	T&=4\, C_{1}^{\frac{1}{2}}(\langle \eta_{1},\eta_{2} \rangle)\, C_{0}^{\frac{3}{2}}(\langle \eta_{1},\eta_{2} \rangle)\, (\eta_{11}\eta_{22}-\eta_{12}\eta_{21})\\
	S&=2\,\big(C_{0}^{\frac{3}{2}}(\langle \eta_{1},\eta_{2} \rangle)\big)^{2}\, (\eta_{11}\eta_{23}-\eta_{13}\eta_{21})\, (\eta_{12}\eta_{23}-\eta_{13}\eta_{22})\\
\end{align*} C_{1}^{\frac{1}{2}}(\langle \eta_{1},\eta_{2} \rangle)=\langle \eta_{1},\eta_{2} \rangle=\cos(\theta) C_{0}^{\frac{3}{2}}(\langle \eta_{1},\eta_{2} \rangle)=1","['matrices', 'positive-definite', 'block-matrices']"
63,Can $\operatorname{Tr} f(M)$ for matrices $M$ be upper bounded by $\sum_{i=1}^n f(M_{ii})$?,Can  for matrices  be upper bounded by ?,\operatorname{Tr} f(M) M \sum_{i=1}^n f(M_{ii}),"For a convex, analytic function $f: \mathbb{R} \to \mathbb{R}$ and a real-valued, symmetric matrix $M \in \mathbb{R}^{n \times n}$ we have the bound $$\operatorname{Tr} f(M) \geq \sum_{i = 1}^n f (M_{ii}).$$ Here $f(M)$ denotes the matrix function of $f$ applied to $M$ , not the element-wise application of $f$ to $M$ . Can the trace of $f(M)$ be similarly upper bounded by the trace of the element-wise application of $f$ to $M$ ? In particular I am interested in a bound for $f(x) = x^{-1}$ applied to positive-definite matrices.","For a convex, analytic function and a real-valued, symmetric matrix we have the bound Here denotes the matrix function of applied to , not the element-wise application of to . Can the trace of be similarly upper bounded by the trace of the element-wise application of to ? In particular I am interested in a bound for applied to positive-definite matrices.",f: \mathbb{R} \to \mathbb{R} M \in \mathbb{R}^{n \times n} \operatorname{Tr} f(M) \geq \sum_{i = 1}^n f (M_{ii}). f(M) f M f M f(M) f M f(x) = x^{-1},"['matrices', 'convex-analysis', 'trace']"
64,Optimization problem involving the inverse matrix,Optimization problem involving the inverse matrix,,"I have a question related to optimization. Given natural numbers $n$ and $\ell$ , matrices ${\bf K}_1, \dots, {\bf K}_\ell \in \Bbb R^{n \times n}$ and a vector ${\bf y} \in \Bbb R^n$ , define $${\bf K} := s_1 {\bf K}_1 + \dots + s_\ell {\bf K}_\ell + \lambda {\bf I}_n$$ where $s_1, \dots, s_\ell, \lambda \in \Bbb R$ , $\lambda > 0$ and ${\bf I}_n$ is the identity matrix. It is also given that the ${\bf K}_1, \dots, {\bf K}_\ell$ are all symmetric and semidefinite so ${\bf K}$ is invertible. The problem is to minimize ${\bf y}^\top {\bf K}^{-1} {\bf y}$ under the constraints $s_1, \dots s_\ell \ge 0$ and $s_1 + \dots + s_\ell = 1$ . I have searched that this problem is related to semidefinite programming (SDP) but since I have no background on optimization theory, I cannot proceed any further. How can I solve this problem?","I have a question related to optimization. Given natural numbers and , matrices and a vector , define where , and is the identity matrix. It is also given that the are all symmetric and semidefinite so is invertible. The problem is to minimize under the constraints and . I have searched that this problem is related to semidefinite programming (SDP) but since I have no background on optimization theory, I cannot proceed any further. How can I solve this problem?","n \ell {\bf K}_1, \dots, {\bf K}_\ell \in \Bbb R^{n \times n} {\bf y} \in \Bbb R^n {\bf K} := s_1 {\bf K}_1 + \dots + s_\ell {\bf K}_\ell + \lambda {\bf I}_n s_1, \dots, s_\ell, \lambda \in \Bbb R \lambda > 0 {\bf I}_n {\bf K}_1, \dots, {\bf K}_\ell {\bf K} {\bf y}^\top {\bf K}^{-1} {\bf y} s_1, \dots s_\ell \ge 0 s_1 + \dots + s_\ell = 1","['matrices', 'optimization', 'convex-optimization', 'semidefinite-programming']"
65,Show that $XY=0$ or $YX=0$,Show that  or,XY=0 YX=0,"We have $X,Y$ $(2×2)$ matrices with complex entries and $X=A^{2}-B^{2}$ and $Y=AB-BA$ . We know that $\det(X)=\det(Y)=0$ . Show that $XY=0$ or $YX=0$ . I see that Trace of $Y$ is $0$ and $\det(Y)$ is also $0$ so by $C-H$ , $Y^{2}=0$ . I also saw how we can write $(X+Y)$ as $(A-B)(A+B)$ but nothing more.","We have matrices with complex entries and and . We know that . Show that or . I see that Trace of is and is also so by , . I also saw how we can write as but nothing more.","X,Y (2×2) X=A^{2}-B^{2} Y=AB-BA \det(X)=\det(Y)=0 XY=0 YX=0 Y 0 \det(Y) 0 C-H Y^{2}=0 (X+Y) (A-B)(A+B)","['matrices', 'determinant', 'cayley-hamilton']"
66,"Can any unitary matrix be written as a product of ""2D"" unitary matrices?","Can any unitary matrix be written as a product of ""2D"" unitary matrices?",,"Given a $n\times n$ unitary matrix $\mathbf{U}$ , can this be rewritten as a product $$ \mathbf{U}=\prod_{i=1}^n\mathbf{U}_i $$ where $\mathbf{U}_i$ are unitary matrices themselves, but they only really `affect' two dimensions? For $n=3$ , they would have this shape (in analogy to the rotation matrices): $$ \mathbf{U}_1= \pmatrix{ a_{11}&a_{12}&0\\ a_{21}&a_{22}&0\\ 0&0&1}, \quad \mathbf{U}_2= \pmatrix{ b_{11}&0&b_{12}\\ 0&1&0\\ b_{21}&0&b_{22}}, \mathbf{U}_3= \pmatrix{ 1&0&0\\ 0&c_{11}&c_{12}\\ 0&c_{21}&c_{22}} $$ With the 2x2 unitary matrices $\mathbf{A}$ , $\mathbf{B}$ , and $\mathbf{C}$ . (Maybe, the ones have to be replaced with a phase factor $e^{i\varphi}$ to have enough flexibility?) The point is, that I have a property which is preserved under multiplication with such a ""2D"" unitary matrix. And I suspect that it generalizes to any unitary matrix (ideally even a unitary transformation in $L^2$ ), but I am not sure.","Given a unitary matrix , can this be rewritten as a product where are unitary matrices themselves, but they only really `affect' two dimensions? For , they would have this shape (in analogy to the rotation matrices): With the 2x2 unitary matrices , , and . (Maybe, the ones have to be replaced with a phase factor to have enough flexibility?) The point is, that I have a property which is preserved under multiplication with such a ""2D"" unitary matrix. And I suspect that it generalizes to any unitary matrix (ideally even a unitary transformation in ), but I am not sure.","n\times n \mathbf{U} 
\mathbf{U}=\prod_{i=1}^n\mathbf{U}_i
 \mathbf{U}_i n=3 
\mathbf{U}_1=
\pmatrix{
a_{11}&a_{12}&0\\
a_{21}&a_{22}&0\\
0&0&1},
\quad
\mathbf{U}_2=
\pmatrix{
b_{11}&0&b_{12}\\
0&1&0\\
b_{21}&0&b_{22}},
\mathbf{U}_3=
\pmatrix{
1&0&0\\
0&c_{11}&c_{12}\\
0&c_{21}&c_{22}}
 \mathbf{A} \mathbf{B} \mathbf{C} e^{i\varphi} L^2","['matrices', 'matrix-decomposition', 'unitary-matrices']"
67,What is this vector notation called?,What is this vector notation called?,,"I am asking here because I believe this is an issue with my understanding of mathematical notation, and not an issue with a physics misunderstanding although there is heavy physics context here. I am writing a physics simulator, and right now I am working on simulating elastic collisions between spheres of equal mass (mass of 1 unit makes much of the math work out nicely). I found this fantastic writeup https://physics.stackexchange.com/questions/107648/what-are-the-general-solutions-to-a-hard-sphere-collision which has gotten me most of the way there. If the spheres collide head on my simulation works but when they collide at an angle, the result shows energy was not conserved (energy lost). I feel I must be misunderstanding the notation. The portion in question is the calculation of the impulse vector. Specifically this (copied from the link): $$ J=2 \hat{j}\frac{\hat{j} \bullet R'}{\frac1{M_a} + \frac1{M_b}} $$ Where R' is $$ R'=V_{a1}-V_{b1} $$ and j is the direction of the impulse (a unit vector through the COM of each sphere) Considering that I have equal masses of 1 this equation should reduce to: $$ J = \hat{j}\left( {\hat{j} \bullet R'}\right) $$ I assume the quantity is a dot product of R' and the unit vector j. If I have two spheres traveling toward each other such that they will collide at a 45 degree angle there is clearly something wrong here. If spheres A / B have velocities (say A is centered on the X-axis and B starts a bit above it, such that they collide at an angle) $$ V_a = \begin{bmatrix} 5 \\ 0 \\ 0 \end{bmatrix} V_b = \begin{bmatrix} -5 \\ 0 \\ 0 \end{bmatrix} $$ My calculation for the direction of impulse is fine but I find $$ \hat{j} = \begin{bmatrix} -\frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} \\ 0 \end{bmatrix} $$ $$ R' = \begin{bmatrix} 10 \\ 0 \\ 0 \end{bmatrix} $$ Which (here must be my misunderstanding) would mean $$ J = \begin{bmatrix} -\frac{10}{\sqrt{2}} \\ 0 \\ 0 \end{bmatrix} $$ This is clearly wrong, there needs to be a vertical component to the impulse vector. I assume the write-up is correct and I'm misunderstanding the notation, what is the correct way to interpret this? edit: The correct vector for reference is $$ J = \begin{bmatrix} -5 \\ -5 \\ 0 \end{bmatrix} $$ Which causes the spheres to change direction by 90 deg and continue on as expected. Final edit for posterity: In my final implementation I actually had to use the absolute value of the dot product. If it's negative, it flips the direction of the impulse and yields incorrect results. I am not sure if this is a mistake in the formula, or an artifact of my implementation. Either way it is working correctly now.","I am asking here because I believe this is an issue with my understanding of mathematical notation, and not an issue with a physics misunderstanding although there is heavy physics context here. I am writing a physics simulator, and right now I am working on simulating elastic collisions between spheres of equal mass (mass of 1 unit makes much of the math work out nicely). I found this fantastic writeup https://physics.stackexchange.com/questions/107648/what-are-the-general-solutions-to-a-hard-sphere-collision which has gotten me most of the way there. If the spheres collide head on my simulation works but when they collide at an angle, the result shows energy was not conserved (energy lost). I feel I must be misunderstanding the notation. The portion in question is the calculation of the impulse vector. Specifically this (copied from the link): Where R' is and j is the direction of the impulse (a unit vector through the COM of each sphere) Considering that I have equal masses of 1 this equation should reduce to: I assume the quantity is a dot product of R' and the unit vector j. If I have two spheres traveling toward each other such that they will collide at a 45 degree angle there is clearly something wrong here. If spheres A / B have velocities (say A is centered on the X-axis and B starts a bit above it, such that they collide at an angle) My calculation for the direction of impulse is fine but I find Which (here must be my misunderstanding) would mean This is clearly wrong, there needs to be a vertical component to the impulse vector. I assume the write-up is correct and I'm misunderstanding the notation, what is the correct way to interpret this? edit: The correct vector for reference is Which causes the spheres to change direction by 90 deg and continue on as expected. Final edit for posterity: In my final implementation I actually had to use the absolute value of the dot product. If it's negative, it flips the direction of the impulse and yields incorrect results. I am not sure if this is a mistake in the formula, or an artifact of my implementation. Either way it is working correctly now.","
J=2 \hat{j}\frac{\hat{j} \bullet R'}{\frac1{M_a} + \frac1{M_b}}
 
R'=V_{a1}-V_{b1}
 
J = \hat{j}\left( {\hat{j} \bullet R'}\right)
 
V_a = \begin{bmatrix} 5 \\ 0 \\ 0 \end{bmatrix}
V_b = \begin{bmatrix} -5 \\ 0 \\ 0 \end{bmatrix}
 
\hat{j} = \begin{bmatrix} -\frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} \\ 0 \end{bmatrix}
 
R' = \begin{bmatrix} 10 \\ 0 \\ 0 \end{bmatrix}
 
J = \begin{bmatrix} -\frac{10}{\sqrt{2}} \\ 0 \\ 0 \end{bmatrix}
 
J = \begin{bmatrix} -5 \\ -5 \\ 0 \end{bmatrix}
","['matrices', 'notation', 'vectors']"
68,What is the Hessian of $X \mapsto f \left( X^T X \right)$ in terms of the Hessian of $f$?,What is the Hessian of  in terms of the Hessian of ?,X \mapsto f \left( X^T X \right) f,"Given $f : \mbox{Sym} (n) \to \mathbb{R}$ , let function $g: \mathbb{R}^{k ,n} \to \mathbb{R}$ be such that $$g(X) := f \left(X^T X \right)$$ What is the Hessian of $g$ in terms of the Hessian of $f$ ? Vectorizing $X$ and directly computing for the Hessian definitely works, but the notation is quite messy and I spent a lot of time on it but still can't get a clear solution. For now, I want to use the gradient which is $X(\nabla f(X^TX)^T + \nabla f(X^TX))$ to take its derivative and get the linear map representation of the hessian. But I'm having trouble taking the derivative of $\nabla f(X^TX)$ . Suppose $Hf : Sym(n) \to Sym(n)$ is the linear map representation of the Hessian of $f$ , then is $D(\nabla f(X^TX))$ just the map $V \to Hf(X^TX)V^TX+Hf(X^TX)X^TV$ ? If so, then the hessian of $g$ at $X$ would be the linear map on $\mathbb{R}^{k,n}$ that $$V \to X(D(\nabla f(X^TX))(V)^T + D(\nabla f(X^TX))(V))+V(\nabla f(X^TX)^T+\nabla f(X^TX))$$ But I'm not really sure of this. Any remarks will be much appreciated.","Given , let function be such that What is the Hessian of in terms of the Hessian of ? Vectorizing and directly computing for the Hessian definitely works, but the notation is quite messy and I spent a lot of time on it but still can't get a clear solution. For now, I want to use the gradient which is to take its derivative and get the linear map representation of the hessian. But I'm having trouble taking the derivative of . Suppose is the linear map representation of the Hessian of , then is just the map ? If so, then the hessian of at would be the linear map on that But I'm not really sure of this. Any remarks will be much appreciated.","f : \mbox{Sym} (n) \to \mathbb{R} g: \mathbb{R}^{k ,n} \to \mathbb{R} g(X) := f \left(X^T X \right) g f X X(\nabla f(X^TX)^T + \nabla f(X^TX)) \nabla f(X^TX) Hf : Sym(n) \to Sym(n) f D(\nabla f(X^TX)) V \to Hf(X^TX)V^TX+Hf(X^TX)X^TV g X \mathbb{R}^{k,n} V \to X(D(\nabla f(X^TX))(V)^T + D(\nabla f(X^TX))(V))+V(\nabla f(X^TX)^T+\nabla f(X^TX))","['matrices', 'multivariable-calculus', 'optimization', 'matrix-calculus', 'hessian-matrix']"
69,What is a non binary adjacency Matrix?,What is a non binary adjacency Matrix?,,I am going through the implementation of a graph convolutional neural network. I came across a non-binary adjacency matrix in the case of a directed graph. The particular issue is discussed here in the following https://github.com/tkipf/pygcn/issues/3 Can someone explain what a non-binary adjacency matrix looks like? How can an adjacency matrix have something other than 0 or 1?,I am going through the implementation of a graph convolutional neural network. I came across a non-binary adjacency matrix in the case of a directed graph. The particular issue is discussed here in the following https://github.com/tkipf/pygcn/issues/3 Can someone explain what a non-binary adjacency matrix looks like? How can an adjacency matrix have something other than 0 or 1?,,"['matrices', 'graph-theory', 'symmetric-matrices', 'adjacency-matrix']"
70,Determinant of a certain Toeplitz matrix,Determinant of a certain Toeplitz matrix,,"Compute the following determinant $$\begin{vmatrix} x & 1 & 2 & 3 & \cdots & n-1 & n\\ 1 & x & 1 & 2 & \cdots & n-2 & n-1\\ 2 & 1 & x & 1 & \cdots & n-3 & n-2\\ 3 & 2 & 1 & x & \cdots & n-4 & n-3\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ n-1 & n-2 & n-3 & n-4 & \cdots & x & 1\\ n & n-1 & n-2 & n-3 & \cdots & 1 &x \end{vmatrix}$$ I tried the following. I subtracted the second row from the first, the third from the second, the fourth from the third, and so on. I got: \begin{vmatrix} x-1 & 1-x & 1 & 1 & \cdots & 1 & 1\\ -1 & x-1 & 1-x & 1 & \cdots & 1 & 1\\ -1 & -1 & x-1 & 1-x & \cdots & 1 & 1\\ 3 & 2 & 1 & x & \cdots & n-4 & n-3\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ -1 & -1 & -1 & -1 & \cdots & x-1 & 1-x\\ n & n-1 & n-2 & n-3 & \cdots & 1 &x \end{vmatrix} I did the same thing with the columns. I subtracted the second row from the first, the third from the second, the fourth from the third, and so on. And I got: \begin{vmatrix} 2x-2 & -x & 0 & 1 & \cdots & 0 & 1\\ -x & 2x-2 & -x & 1 & \cdots & 0 & 1\\ -2 & -x & 2x-2 & 1-x & \cdots & 0 & 1\\ 1 & 1 & 1-x & x & \cdots & -1 & n-3\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ -2 & -2 & -2 & -1 & \cdots & 2x-2 & 1-x\\ 1 & 1 & 1 & n-3 & \cdots & 1-x &x \end{vmatrix} I hope I didn’t make a mistake somewhere. With this part I don't know what to do next. I don't know if I'm doing it right. Thank you in advance !","Compute the following determinant I tried the following. I subtracted the second row from the first, the third from the second, the fourth from the third, and so on. I got: I did the same thing with the columns. I subtracted the second row from the first, the third from the second, the fourth from the third, and so on. And I got: I hope I didn’t make a mistake somewhere. With this part I don't know what to do next. I don't know if I'm doing it right. Thank you in advance !",\begin{vmatrix} x & 1 & 2 & 3 & \cdots & n-1 & n\\ 1 & x & 1 & 2 & \cdots & n-2 & n-1\\ 2 & 1 & x & 1 & \cdots & n-3 & n-2\\ 3 & 2 & 1 & x & \cdots & n-4 & n-3\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ n-1 & n-2 & n-3 & n-4 & \cdots & x & 1\\ n & n-1 & n-2 & n-3 & \cdots & 1 &x \end{vmatrix} \begin{vmatrix} x-1 & 1-x & 1 & 1 & \cdots & 1 & 1\\ -1 & x-1 & 1-x & 1 & \cdots & 1 & 1\\ -1 & -1 & x-1 & 1-x & \cdots & 1 & 1\\ 3 & 2 & 1 & x & \cdots & n-4 & n-3\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ -1 & -1 & -1 & -1 & \cdots & x-1 & 1-x\\ n & n-1 & n-2 & n-3 & \cdots & 1 &x \end{vmatrix} \begin{vmatrix} 2x-2 & -x & 0 & 1 & \cdots & 0 & 1\\ -x & 2x-2 & -x & 1 & \cdots & 0 & 1\\ -2 & -x & 2x-2 & 1-x & \cdots & 0 & 1\\ 1 & 1 & 1-x & x & \cdots & -1 & n-3\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ -2 & -2 & -2 & -1 & \cdots & 2x-2 & 1-x\\ 1 & 1 & 1 & n-3 & \cdots & 1-x &x \end{vmatrix},"['matrices', 'determinant', 'toeplitz-matrices']"
71,Prove that $\det ((A + B + C) (A^3 + B^3 + C^3-3ABC))\geq 0 $,Prove that,\det ((A + B + C) (A^3 + B^3 + C^3-3ABC))\geq 0 ,"Suppose that A, B and C are 2x2 matrices that switch between each other. Prove that $$\det ((A + B + C) (A^3 + B^3 + C^3-3ABC))\geq 0. $$ I did $$A^3+B^3+C^3-3ABC=\frac12(A+B+C)((A-B)^2+(A-C)^2+(B-C)^2)$$ So, this determinant is equivalent to $$\frac14[\det(A+B+C)]^2\det((A-B)^2+(A-C)^2+(B-C)^2)$$ But how can I prove that $$\det((A-B)^2+(A-C)^2+(B-C)^2)\geq0?$$ Can someone help me? Thanks for attention.","Suppose that A, B and C are 2x2 matrices that switch between each other. Prove that I did So, this determinant is equivalent to But how can I prove that Can someone help me? Thanks for attention.",\det ((A + B + C) (A^3 + B^3 + C^3-3ABC))\geq 0.  A^3+B^3+C^3-3ABC=\frac12(A+B+C)((A-B)^2+(A-C)^2+(B-C)^2) \frac14[\det(A+B+C)]^2\det((A-B)^2+(A-C)^2+(B-C)^2) \det((A-B)^2+(A-C)^2+(B-C)^2)\geq0?,"['matrices', 'determinant', 'factoring']"
72,Tensor product of two $n\times n$ orthogonal matrices with determinant $+1$,Tensor product of two  orthogonal matrices with determinant,n\times n +1,"If two square matrices A and B are two $n\times n$ orthogonal matrices with determinant unity i.e., $\det A=\det B=+1$ and $A^TA=B^TB=I$ , will the tensor product $C\equiv A\otimes B$ also be orthogonal and have determinant $+1$ ? Can we understand this without restricting to special choices for $n$ such as $n=2,3$ etc","If two square matrices A and B are two orthogonal matrices with determinant unity i.e., and , will the tensor product also be orthogonal and have determinant ? Can we understand this without restricting to special choices for such as etc","n\times n \det A=\det B=+1 A^TA=B^TB=I C\equiv A\otimes B +1 n n=2,3","['matrices', 'tensor-products', 'rotations', 'orthogonal-matrices']"
73,True/false: $\det(A^2+I)\ge 0$ for every $3 \times 3$ matrix with real entries and rank $>0$,True/false:  for every  matrix with real entries and rank,\det(A^2+I)\ge 0 3 \times 3 >0,"I have the following proposition about which I have to say whether it is true or false. $\det(A^2+I)\ge 0$ for every $3 \times 3$ matrix with real entries and rank $>0$ . $I$ is the identity matrix. I tried brutal ways (taking a generic matrix, evaluating its square and adding $I$ ), but there are too many calculations and I feel that manner will not lead me to anything of interesting. I also tried to construct a counterexample, but nothing. I am not able to prove nor confute this assertion.","I have the following proposition about which I have to say whether it is true or false. for every matrix with real entries and rank . is the identity matrix. I tried brutal ways (taking a generic matrix, evaluating its square and adding ), but there are too many calculations and I feel that manner will not lead me to anything of interesting. I also tried to construct a counterexample, but nothing. I am not able to prove nor confute this assertion.",\det(A^2+I)\ge 0 3 \times 3 >0 I I,"['matrices', 'determinant', 'matrix-rank']"
74,Derivative of matrices product AXA^T with respect to A. (Plus result when A is a vector.),Derivative of matrices product AXA^T with respect to A. (Plus result when A is a vector.),,"I want to know how to find an expression for $$\frac{\partial (AXA^T)}{\partial A}$$ where no information is given a priori on the dimensions of $A$ and $X$ . The question is related to machine learning but I am not given any additional details on the nature of the matrices; I am only given the result: $$\frac{\partial (AXA^T)}{\partial A}=A(X+X^T)$$ (in Andrew's answer below it is shown this is only the result if A is size $(1\times k)$ , i.e. a row vector) I have seen similar questions on the forum and was trying to approach this by differentiating the given product: \begin{align}\mathrm{d}(AXA^T)&=\mathrm{d}(AX)A^T+AX\mathrm{d}(A^T)= \left[ \mathrm{d}AX+A\mathrm{d}X \right]A^T+AX(\mathrm{d}A)^T=\\ &=\mathrm{d}AXA^T+AX(\mathrm{d}A)^T+ A\mathrm{d}XA^T\end{align} Then setting $\mathrm{d}X$ to zero (since we are derivating with $X$ constant): \begin{align} \partial(AXA^T)= \partial AXA^T+AX(\partial A)^T \end{align} Here I get stuck because I am unable to express it in a way I can premultiply by $(\partial A)^{-1}$ and obtain my derivative. I have tried by attempting to transpose twice the second term on the right hand side to get \begin{align}\partial(AXA^T)= \partial AXA^T + \left((\partial A)X^TA^T\right)^T \end{align} and thought maybe there are symmetry assumptions in the solution I was given, to finally lead to it. I have also seen quite similar results in the Matrix Cookbook (e.g. formulae 79 and 80), but they are not the same and  are given in index notation which is confusing me a little bit more; also I would like to actually learn how to calculate them since I have never come up with this kind of derivatives (with respect to matrices) and do not even know how exactly they are defined. I have also tried to proceed with the calculus rules (product rule of derivatives) but felt I was probably missing things and am not sure if they hold in their usual form here. I would appreciate your help in any of those questions. EDIT: The clarification given by the authors of this exercise is to just use the simple product rule (I am unsure if this is actually possible with matrices, at least without introducing any special products): \begin{align} \frac{\partial (AXA^T)}{\partial A} = \frac{\partial A}{\partial A}XA^T+A\frac{\partial XA^T}{\partial A} = (XA^T)^T+AX=AX^T+AX=A(X+X^T) \end{align} saying on the side that they have applied the property: $\frac{ \partial A}{\partial A}B= B^T$ , which according to them follows from $\left[ \frac{\partial A}{\partial A}B\right]_i=\frac{\partial \sum_{k=1}^n A_k B_k}{\partial A_i}=B_i$ , ""for the i-th element"". (I cannot see how this property follows from there either, and how these operations are performed that way with a just single index and with different-dimension matrices.)","I want to know how to find an expression for where no information is given a priori on the dimensions of and . The question is related to machine learning but I am not given any additional details on the nature of the matrices; I am only given the result: (in Andrew's answer below it is shown this is only the result if A is size , i.e. a row vector) I have seen similar questions on the forum and was trying to approach this by differentiating the given product: Then setting to zero (since we are derivating with constant): Here I get stuck because I am unable to express it in a way I can premultiply by and obtain my derivative. I have tried by attempting to transpose twice the second term on the right hand side to get and thought maybe there are symmetry assumptions in the solution I was given, to finally lead to it. I have also seen quite similar results in the Matrix Cookbook (e.g. formulae 79 and 80), but they are not the same and  are given in index notation which is confusing me a little bit more; also I would like to actually learn how to calculate them since I have never come up with this kind of derivatives (with respect to matrices) and do not even know how exactly they are defined. I have also tried to proceed with the calculus rules (product rule of derivatives) but felt I was probably missing things and am not sure if they hold in their usual form here. I would appreciate your help in any of those questions. EDIT: The clarification given by the authors of this exercise is to just use the simple product rule (I am unsure if this is actually possible with matrices, at least without introducing any special products): saying on the side that they have applied the property: , which according to them follows from , ""for the i-th element"". (I cannot see how this property follows from there either, and how these operations are performed that way with a just single index and with different-dimension matrices.)","\frac{\partial (AXA^T)}{\partial A} A X \frac{\partial (AXA^T)}{\partial A}=A(X+X^T) (1\times k) \begin{align}\mathrm{d}(AXA^T)&=\mathrm{d}(AX)A^T+AX\mathrm{d}(A^T)= \left[ \mathrm{d}AX+A\mathrm{d}X \right]A^T+AX(\mathrm{d}A)^T=\\
&=\mathrm{d}AXA^T+AX(\mathrm{d}A)^T+
A\mathrm{d}XA^T\end{align} \mathrm{d}X X \begin{align}
\partial(AXA^T)= \partial AXA^T+AX(\partial A)^T
\end{align} (\partial A)^{-1} \begin{align}\partial(AXA^T)= \partial AXA^T + \left((\partial A)X^TA^T\right)^T
\end{align} \begin{align}
\frac{\partial (AXA^T)}{\partial A} = \frac{\partial A}{\partial A}XA^T+A\frac{\partial XA^T}{\partial A} = (XA^T)^T+AX=AX^T+AX=A(X+X^T)
\end{align} \frac{ \partial A}{\partial A}B= B^T \left[ \frac{\partial A}{\partial A}B\right]_i=\frac{\partial \sum_{k=1}^n A_k B_k}{\partial A_i}=B_i","['matrices', 'derivatives', 'partial-derivative', 'matrix-calculus', 'machine-learning']"
75,"If three nonzero real matrices mutually anticommute, then at least one of them has a negative off-diagonal element","If three nonzero real matrices mutually anticommute, then at least one of them has a negative off-diagonal element",,"The following is a statement which I believe to be correct but unable to prove (I have been trying to find a counterexample for a long time but never succeeded in doing so): Let $X_1,X_2,X_3$ be $n\times n$ nonzero real matrices satisfying $X_{i}X_j=-X_jX_i\neq 0$ for $1\leq i<j\leq 3$ [as an example, $\{X_1,X_2,X_3\}=\{\sigma^x,i\sigma^y,\sigma^z\}$ , the Pauli-matrices] . Prove that at least one of $X_1,X_2,X_3$ has a negative off-diagonal element. The case $n=2$ is easy and straightforward: just expand $X_1,X_2,X_3$ in the basis of Pauli matrices and solve the equation $\{X_i,X_j\}=0$ . But when $n$ becomes large, the coupled quadratic equations quickly becomes formidable, which I have no way to handle. Notice also that the $\neq 0$ condition is important, otherwise we got a simple counter-example: $X_1=X_2=X_3=\sigma^+$ , which mutually anti-commute but are all non-negative.","The following is a statement which I believe to be correct but unable to prove (I have been trying to find a counterexample for a long time but never succeeded in doing so): Let be nonzero real matrices satisfying for [as an example, , the Pauli-matrices] . Prove that at least one of has a negative off-diagonal element. The case is easy and straightforward: just expand in the basis of Pauli matrices and solve the equation . But when becomes large, the coupled quadratic equations quickly becomes formidable, which I have no way to handle. Notice also that the condition is important, otherwise we got a simple counter-example: , which mutually anti-commute but are all non-negative.","X_1,X_2,X_3 n\times n X_{i}X_j=-X_jX_i\neq 0 1\leq i<j\leq 3 \{X_1,X_2,X_3\}=\{\sigma^x,i\sigma^y,\sigma^z\} X_1,X_2,X_3 n=2 X_1,X_2,X_3 \{X_i,X_j\}=0 n \neq 0 X_1=X_2=X_3=\sigma^+","['matrices', 'clifford-algebras']"
76,Convexity of $|A^TA|$,Convexity of,|A^TA|,"Assume $A\in\mathbb{R}^{m\times n}$ is a $m$ by $n$ matrix. Let $|\cdot|$ denote the Frobenius norm of matrix. Define function $f:\mathbb{R}^{m\times n}\to\mathbb{R}$ as $f(A):=|A^TA|$ . Is $f$ a convex function? Intuitively, I think this function is a composition of a norm and something with quadratic structure, and thus should be convex. To prove this, $f(tA+(1-t)B)=|t^2A^TA+t(1-t)(A^TB+B^TA)+(1-t)^2B^TB|\le t^2|A^TA|+(1-t)^2|B^TB|+t(1-t)|A^TB+B^TA|$ . If $|A^TB+B^TA|\le|A^TA|+|B^TB|$ , then we get the convexity. Is this inequality true?","Assume is a by matrix. Let denote the Frobenius norm of matrix. Define function as . Is a convex function? Intuitively, I think this function is a composition of a norm and something with quadratic structure, and thus should be convex. To prove this, . If , then we get the convexity. Is this inequality true?",A\in\mathbb{R}^{m\times n} m n |\cdot| f:\mathbb{R}^{m\times n}\to\mathbb{R} f(A):=|A^TA| f f(tA+(1-t)B)=|t^2A^TA+t(1-t)(A^TB+B^TA)+(1-t)^2B^TB|\le t^2|A^TA|+(1-t)^2|B^TB|+t(1-t)|A^TB+B^TA| |A^TB+B^TA|\le|A^TA|+|B^TB|,"['matrices', 'convex-analysis', 'matrix-norms']"
77,Matrix equation with symmetric and positive definite matrices [duplicate],Matrix equation with symmetric and positive definite matrices [duplicate],,"This question already has answers here : Find $C$, if $A=CBC$, where $A$,$B$,$C$ are symmetric matrices. (3 answers) Closed last year . Let matrices $P_0$ and $P_1$ be symmetric and positive definite. Is it possible to find a symmetric matrix $S$ such that $SP_0S=P_1$? If $P_0=I$, this is always possible: $S=\sqrt{P_1}$.","This question already has answers here : Find $C$, if $A=CBC$, where $A$,$B$,$C$ are symmetric matrices. (3 answers) Closed last year . Let matrices $P_0$ and $P_1$ be symmetric and positive definite. Is it possible to find a symmetric matrix $S$ such that $SP_0S=P_1$? If $P_0=I$, this is always possible: $S=\sqrt{P_1}$.",,"['matrices', 'matrix-equations', 'positive-definite', 'symmetric-matrices']"
78,Norm of difference in exponential of matrices,Norm of difference in exponential of matrices,,"I would like to prove that $\|e^A-e^B\| \leq \|A-B\|e^{max\{\|A\|,\|B\|\}}$, where $A,B \in \mathbb{R}^{n \times n}$. So far I was able to create the first difference term, but I have no idea how to incorporate the max norm. I've read this post, where the Fréchet calculus was mentioned, but I'm still stuck. Any help would be appreciated. Thank you in advance!","I would like to prove that $\|e^A-e^B\| \leq \|A-B\|e^{max\{\|A\|,\|B\|\}}$, where $A,B \in \mathbb{R}^{n \times n}$. So far I was able to create the first difference term, but I have no idea how to incorporate the max norm. I've read this post, where the Fréchet calculus was mentioned, but I'm still stuck. Any help would be appreciated. Thank you in advance!",,"['matrices', 'inequality', 'normed-spaces', 'matrix-exponential']"
79,Is $SO(n)$ a normal subgroup of $SO(n+1)$?,Is  a normal subgroup of ?,SO(n) SO(n+1),"I am trying to learn about homogeneous and symmetric spaces. I know that the quotient $SO(n+1)/SO(n)$ should look like the sphere $S^n$. But how so (i.e, what is the equivalence criterion)? Is $SO(n)$ a normal subgroup and the quotient has a group isomorphism with $S^n$, or we simply have a diffeomorphism between the coset space $SO(n+1)/SO(n)$ and the sphere $S^n$? Also, it would be really helpful if somebody can please suggest some survey/introductory literature on homogeneous spaces. Thank you very much for your help, comments are welcome!","I am trying to learn about homogeneous and symmetric spaces. I know that the quotient $SO(n+1)/SO(n)$ should look like the sphere $S^n$. But how so (i.e, what is the equivalence criterion)? Is $SO(n)$ a normal subgroup and the quotient has a group isomorphism with $S^n$, or we simply have a diffeomorphism between the coset space $SO(n+1)/SO(n)$ and the sphere $S^n$? Also, it would be really helpful if somebody can please suggest some survey/introductory literature on homogeneous spaces. Thank you very much for your help, comments are welcome!",,"['matrices', 'differential-geometry', 'manifolds', 'riemannian-geometry']"
80,Higher powers of matrix,Higher powers of matrix,,"$$     A = \begin{bmatrix}     1 & 1 & 13 \\     5 & 2 & 6 \\     -2 & -1 & -3 \\     \end{bmatrix} $$ Find $A^{14}+3A-2I$. One way is to find $A^2$, then $A^4$, then $A^8$, then $A^{14}$. Another way is using eigen values and diagonal matrix concept. In our university exam, this question is given only for 5 marks. So I am wondering if there is any simple way to do it. Appreciate any hint.","$$     A = \begin{bmatrix}     1 & 1 & 13 \\     5 & 2 & 6 \\     -2 & -1 & -3 \\     \end{bmatrix} $$ Find $A^{14}+3A-2I$. One way is to find $A^2$, then $A^4$, then $A^8$, then $A^{14}$. Another way is using eigen values and diagonal matrix concept. In our university exam, this question is given only for 5 marks. So I am wondering if there is any simple way to do it. Appreciate any hint.",,['matrices']
81,A regular connected graph has k loops?,A regular connected graph has k loops?,,"Suppose $A$ is an order-$n$ matrix of zeroes and ones such that $A^2 = J$, the matrix of all ones. Show that $A$ has exactly $k$ ones along the diagonal, where $k^2 = n$. I can show that $AJ=JA=kJ$, so in graph theoretic terms, $A$ is a regular graph of order $k$. And $A$ is connected, or else $A^2$ would have zeroes in it. I wonder if there's a theorem that can establish that the graph has exactly $k$ loops? Or perhaps it's more obvious from a matrix perspective. I know that regularity and connectivity by itself isn't sufficient, since for example the matrix $$A_\varnothing=\begin{bmatrix}1& & &1\\ &1&1& \\1& &1& \\&1&&1\end{bmatrix}$$ is $k$-regular and connected, but fails to satisfy $A_\varnothing^2 = J$ and fails to have $k$ ones along the diagonal. An example that does exhibit this property is $$A_\checkmark = \begin{bmatrix}1&1&&\\&&1&1\\1&1&&\\&&1&1\end{bmatrix}$$","Suppose $A$ is an order-$n$ matrix of zeroes and ones such that $A^2 = J$, the matrix of all ones. Show that $A$ has exactly $k$ ones along the diagonal, where $k^2 = n$. I can show that $AJ=JA=kJ$, so in graph theoretic terms, $A$ is a regular graph of order $k$. And $A$ is connected, or else $A^2$ would have zeroes in it. I wonder if there's a theorem that can establish that the graph has exactly $k$ loops? Or perhaps it's more obvious from a matrix perspective. I know that regularity and connectivity by itself isn't sufficient, since for example the matrix $$A_\varnothing=\begin{bmatrix}1& & &1\\ &1&1& \\1& &1& \\&1&&1\end{bmatrix}$$ is $k$-regular and connected, but fails to satisfy $A_\varnothing^2 = J$ and fails to have $k$ ones along the diagonal. An example that does exhibit this property is $$A_\checkmark = \begin{bmatrix}1&1&&\\&&1&1\\1&1&&\\&&1&1\end{bmatrix}$$",,"['matrices', 'graph-theory', 'algebraic-graph-theory']"
82,Elevation rotation of a matrix in polar-coordinates,Elevation rotation of a matrix in polar-coordinates,,"Disclaimer: I don't seem to be able to wrap my mind around this, because MAYBE the program I'm using to visualize 3D Data performs some kind of transformation I'm not aware of, therefore produces results that are not conform to what I would expect. I have a 3D antenna pattern, which is described by a 2D matrix of 181 x 360 points (one every 1° for each dimension). Let phi be the plane with 360 points (0° to 359°) associated to the azimuth angle, and theta the one with 181 (0° to 180°) points associated to the elevation angle. R would instead be the value of the matrix by the coordinates phi and theta. Since we move on a spherical surface, the point phi = x and theta = 180° + y is equal to phi = x + 180° and theta = 360° - (180° + y) . Now, suppose I have to rotate the 3D pattern by an elevation of delta degrees. Strictly reasoning on the matrix, which is the transformation I would have to perform? I tried: Building the 360 x 360 matrix (with the ""rule"" mentioned above), then selecting a 181 x 360 matrix, whose first element is the delta-th element of the original matrix. Splitting the 181 x 360 matrix in four 181 x 90 matrices, which are shifted in different directions, and whose missing elements (all the data which would be shifted before the first index or after the 181th index) are taken from the ""displaced"" elements from other matrices (e.g.: Matrix1 shifts 20 positions up, therefore 20 positions by the end indexes become nulls. Matrix2 shifts 20 positions down, therefore Matrix1 inherits the ""downward-displaced"" data from Matrix2 and Matrix2 the ""upward-displaced"" one from Matrix1. What is the correct transformation? EDIT: I don't seem to have found a flaw yet in splitting the matrix in 4 columns and rotate them. Even if I do that, though, the 3D picture doesn't look good. Am I not considering something here?","Disclaimer: I don't seem to be able to wrap my mind around this, because MAYBE the program I'm using to visualize 3D Data performs some kind of transformation I'm not aware of, therefore produces results that are not conform to what I would expect. I have a 3D antenna pattern, which is described by a 2D matrix of 181 x 360 points (one every 1° for each dimension). Let phi be the plane with 360 points (0° to 359°) associated to the azimuth angle, and theta the one with 181 (0° to 180°) points associated to the elevation angle. R would instead be the value of the matrix by the coordinates phi and theta. Since we move on a spherical surface, the point phi = x and theta = 180° + y is equal to phi = x + 180° and theta = 360° - (180° + y) . Now, suppose I have to rotate the 3D pattern by an elevation of delta degrees. Strictly reasoning on the matrix, which is the transformation I would have to perform? I tried: Building the 360 x 360 matrix (with the ""rule"" mentioned above), then selecting a 181 x 360 matrix, whose first element is the delta-th element of the original matrix. Splitting the 181 x 360 matrix in four 181 x 90 matrices, which are shifted in different directions, and whose missing elements (all the data which would be shifted before the first index or after the 181th index) are taken from the ""displaced"" elements from other matrices (e.g.: Matrix1 shifts 20 positions up, therefore 20 positions by the end indexes become nulls. Matrix2 shifts 20 positions down, therefore Matrix1 inherits the ""downward-displaced"" data from Matrix2 and Matrix2 the ""upward-displaced"" one from Matrix1. What is the correct transformation? EDIT: I don't seem to have found a flaw yet in splitting the matrix in 4 columns and rotate them. Even if I do that, though, the 3D picture doesn't look good. Am I not considering something here?",,"['matrices', 'polar-coordinates', 'rotations']"
83,Condition on a matrix so that its determinant is $2$,Condition on a matrix so that its determinant is,2,"Is the following statement true or false? There exists $A \in M_{3,3}(\mathbb{Z})$ with determinant $2$ such that $$A\begin{pmatrix}2\\1\\4\end{pmatrix} = \begin{pmatrix}4\\-8\\16\end{pmatrix}$$ I first thought of eigenvalues but it doesn't look like the second vector is a multiple of the first. What are the conditions on $A$ so that it's ""true""? Or is it always false?","Is the following statement true or false? There exists $A \in M_{3,3}(\mathbb{Z})$ with determinant $2$ such that $$A\begin{pmatrix}2\\1\\4\end{pmatrix} = \begin{pmatrix}4\\-8\\16\end{pmatrix}$$ I first thought of eigenvalues but it doesn't look like the second vector is a multiple of the first. What are the conditions on $A$ so that it's ""true""? Or is it always false?",,"['matrices', 'diophantine-equations']"
84,"Revisit ""Matrix exponential of a skew symmetric matrix"" and rotation matrices","Revisit ""Matrix exponential of a skew symmetric matrix"" and rotation matrices",,"Recall the following: Matrix exponential of a skew symmetric matrix The conclusion is: $$e^C = I + \dfrac{\sin x}{x}C + \dfrac{1-\cos x}{x^2}C^2$$ $x = \sqrt{a_1^2+a_2^2+a_3^2}$ $C=\left( \begin{array}{ccc}  0 & -a_3 & a_2 \\ a_3 & 0 & -a_1 \\ -a_2 & a_1 & 0 \\ \end{array} \right).$ And by the fact that $$e^C\in SO(3),$$ i.e., the exponential of skew-symmetric matrix is an element of $SO(3)$. (Please see the reference (p.4): http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5766283 ) Also The differential equation for the rotation matrix:  $$\dot{C} = -\omega^{\times}C$$  where $\omega^{\times} = \left( \begin{array}{ccc}  0 & -\omega_3 & \omega_2 \\ \omega_3 & 0 & -\omega_1 \\ -\omega_2 & \omega_1 & 0 \\ \end{array} \right)$ And we can think that the rotation axis with angular velocity in this dynamic is $\hat{\omega} = \left( \begin{array}{ccc}  \omega_1 \\ \omega_2 \\ \omega_3\end{array} \right)$ My question is suppose (rotation about the $z$-axis) $\hat{\omega} = \left( \begin{array}{ccc}  0 \\ 0 \\ \omega_3\end{array} \right)$    i.e., $\omega^{\times} = \left( \begin{array}{ccc}  0 & -\omega_3 & 0 \\ \omega_3 & 0 & 0 \\ 0 & 0 & 0 \\ \end{array} \right)$ How to derive the corresponding rotation matrix is: $R=\left( \begin{array}{ccc}  \cos\omega t  & -\sin\omega t  & 0 \\ \sin\omega t   & \cos\omega t & 0 \\ 0 & 0 & 1 \\ \end{array} \right).$ There is an identity matrix $I$ in the equation; how to deal with that?","Recall the following: Matrix exponential of a skew symmetric matrix The conclusion is: $$e^C = I + \dfrac{\sin x}{x}C + \dfrac{1-\cos x}{x^2}C^2$$ $x = \sqrt{a_1^2+a_2^2+a_3^2}$ $C=\left( \begin{array}{ccc}  0 & -a_3 & a_2 \\ a_3 & 0 & -a_1 \\ -a_2 & a_1 & 0 \\ \end{array} \right).$ And by the fact that $$e^C\in SO(3),$$ i.e., the exponential of skew-symmetric matrix is an element of $SO(3)$. (Please see the reference (p.4): http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5766283 ) Also The differential equation for the rotation matrix:  $$\dot{C} = -\omega^{\times}C$$  where $\omega^{\times} = \left( \begin{array}{ccc}  0 & -\omega_3 & \omega_2 \\ \omega_3 & 0 & -\omega_1 \\ -\omega_2 & \omega_1 & 0 \\ \end{array} \right)$ And we can think that the rotation axis with angular velocity in this dynamic is $\hat{\omega} = \left( \begin{array}{ccc}  \omega_1 \\ \omega_2 \\ \omega_3\end{array} \right)$ My question is suppose (rotation about the $z$-axis) $\hat{\omega} = \left( \begin{array}{ccc}  0 \\ 0 \\ \omega_3\end{array} \right)$    i.e., $\omega^{\times} = \left( \begin{array}{ccc}  0 & -\omega_3 & 0 \\ \omega_3 & 0 & 0 \\ 0 & 0 & 0 \\ \end{array} \right)$ How to derive the corresponding rotation matrix is: $R=\left( \begin{array}{ccc}  \cos\omega t  & -\sin\omega t  & 0 \\ \sin\omega t   & \cos\omega t & 0 \\ 0 & 0 & 1 \\ \end{array} \right).$ There is an identity matrix $I$ in the equation; how to deal with that?",,"['matrices', 'ordinary-differential-equations', 'matrix-equations', 'matrix-calculus']"
85,"Show that in Lyapunov equation $A^TQ+QA=-I$, the matrix $Q$ is positive definite.","Show that in Lyapunov equation , the matrix  is positive definite.",A^TQ+QA=-I Q,Let $A$ be matrix whose eigenvalues all have negative real parts. Define $Q=\int^{\infty}_0 B(t)dt$ where $B(t)=e^{A^Tt}e^{At}$ . Prove that $Q$ is symmetric and positive definite. This question is related to the corresponding Lyapunov equation $A^TQ+QA=-I$ . By the above we know that $B(t)^T=B(t)$ and $\forall x \neq 0. x^TB(t)x>0$ . Therefore: \begin{align} -I &=\lim_{\tau \to \infty} B(\tau) -I\\ &=\lim_{\tau \to \infty} \int^{\tau}_0\frac{d B(t)}{dt} \\ &= \lim_{\tau \to \infty} \Big( A^T\int^{\tau}_0B(t)dt+\int^{\tau}_0B(t)dt\ A \Big)\\ &=A^TQ+QA\\ \end{align} However I am confused on how to use these facts to show that $Q$ is symmetric and positive definite.,Let be matrix whose eigenvalues all have negative real parts. Define where . Prove that is symmetric and positive definite. This question is related to the corresponding Lyapunov equation . By the above we know that and . Therefore: However I am confused on how to use these facts to show that is symmetric and positive definite.,"A Q=\int^{\infty}_0 B(t)dt B(t)=e^{A^Tt}e^{At} Q A^TQ+QA=-I B(t)^T=B(t) \forall x \neq 0. x^TB(t)x>0 \begin{align}
-I
&=\lim_{\tau \to \infty} B(\tau) -I\\
&=\lim_{\tau \to \infty} \int^{\tau}_0\frac{d B(t)}{dt} \\
&= \lim_{\tau \to \infty} \Big( A^T\int^{\tau}_0B(t)dt+\int^{\tau}_0B(t)dt\ A \Big)\\
&=A^TQ+QA\\
\end{align} Q","['matrices', 'ordinary-differential-equations', 'dynamical-systems', 'matrix-equations', 'linear-control']"
86,Exercise 8.18 in Algebraic Graph Theory by Godsil and Royle,Exercise 8.18 in Algebraic Graph Theory by Godsil and Royle,,"This is at page 190 of Algebraic Graph Theory by  Chris Godsil and Gordon Royle. Let $\mathbf B$ be the submatrix of the symmetric matrix $\mathbf A$ obtained by deleting the $i$th row and column of $\mathbf A.$ Problem 1: Show that if $\mathbf x$ is an eigenvector for $\mathbf A$ such that $x_i=0$, then the vector $\mathbf y$ we get by deleting the $i$th coordinate from $\mathbf x$ is an eigenvector for $\mathbf B$. We call $\mathbf y$ the restriction of $\mathbf x$, and $\mathbf x$ the extension of $\mathbf y$. Now, suppose that $\theta$ is a common eigenvalue of $\mathbf A$ and $\mathbf B$, and that its multiplicity as an eigenvalue of $\mathbf A$ is $m$. Problem 2: If the multiplicity of $\theta$ as an eigenvalue of $\mathbf B$ is $m-1$, show that each $\theta$-eigenvectors of $\mathbf B$ extends to an eigenvector for $\mathbf A$. (It also has the third question, but I didn't copy it.) The first problem is a direct consequence of the definition of the eigenvector that $\mathbf A \mathbf x = \theta \mathbf x$. But I have no idea on how to prove the second problem. How can I prove it?","This is at page 190 of Algebraic Graph Theory by  Chris Godsil and Gordon Royle. Let $\mathbf B$ be the submatrix of the symmetric matrix $\mathbf A$ obtained by deleting the $i$th row and column of $\mathbf A.$ Problem 1: Show that if $\mathbf x$ is an eigenvector for $\mathbf A$ such that $x_i=0$, then the vector $\mathbf y$ we get by deleting the $i$th coordinate from $\mathbf x$ is an eigenvector for $\mathbf B$. We call $\mathbf y$ the restriction of $\mathbf x$, and $\mathbf x$ the extension of $\mathbf y$. Now, suppose that $\theta$ is a common eigenvalue of $\mathbf A$ and $\mathbf B$, and that its multiplicity as an eigenvalue of $\mathbf A$ is $m$. Problem 2: If the multiplicity of $\theta$ as an eigenvalue of $\mathbf B$ is $m-1$, show that each $\theta$-eigenvectors of $\mathbf B$ extends to an eigenvector for $\mathbf A$. (It also has the third question, but I didn't copy it.) The first problem is a direct consequence of the definition of the eigenvector that $\mathbf A \mathbf x = \theta \mathbf x$. But I have no idea on how to prove the second problem. How can I prove it?",,"['matrices', 'graph-theory', 'eigenvalues-eigenvectors', 'spectral-graph-theory', 'adjacency-matrix']"
87,Prove that if A invertible and $\left\lVert A-B\right\rVert <\left\lVert A^{-1}\right\rVert ^{-1}$ then the following inequality holds,Prove that if A invertible and  then the following inequality holds,\left\lVert A-B\right\rVert <\left\lVert A^{-1}\right\rVert ^{-1},Prove that if A invertible and $\left\lVert A-B\right\rVert <\left\lVert A^{-1}\right\rVert ^{-1}$ then  $$\left\lVert A^{-1}-B^{-1}\right\rVert \leq \left\lVert A^{-1}\right\rVert \frac{\lVert I-A^{-1}B\rVert}{1-\left\lVert I-A^{-1}B\right\rVert }$$ I am trying and trying but can't find the right solution. What I get is: $$ \left\lVert A^{-1}-B^{-1}\right\rVert = \left\lVert A^{-1}-\sum\limits_{k=0}^\infty (I-A^{-1}B)^k A^{-1}\right\rVert \leq \left\lVert A^{-1}\right\rVert + \left\lVert \sum\limits_{k=0}^\infty (I-A^{-1}B)^k A^{-1}\right\rVert \leq \\ \left\lVert A^{-1}\right\rVert + \left\lVert \sum\limits_{k=0}^\infty (I-A^{-1}B)^k \right\rVert \cdot \left\lVert A^{-1}\right\rVert \leq \left\lVert A^{-1}\right\rVert \left[ 1 + \frac{1}{1-\left\lVert I-A^{-1}B \right\rVert} \right] = \left\lVert A^{-1}\right\rVert \left[ \frac{2-\left\lVert I-A^{-1}B \right\rVert}{1-\left\lVert I-A^{-1}B \right\rVert} \right] \stackrel{?}{\le}  \left\lVert A^{-1}\right\rVert \frac{\lVert I-A^{-1}B\rVert}{1-\left\lVert I-A^{-1}B\right\rVert } $$,Prove that if A invertible and $\left\lVert A-B\right\rVert <\left\lVert A^{-1}\right\rVert ^{-1}$ then  $$\left\lVert A^{-1}-B^{-1}\right\rVert \leq \left\lVert A^{-1}\right\rVert \frac{\lVert I-A^{-1}B\rVert}{1-\left\lVert I-A^{-1}B\right\rVert }$$ I am trying and trying but can't find the right solution. What I get is: $$ \left\lVert A^{-1}-B^{-1}\right\rVert = \left\lVert A^{-1}-\sum\limits_{k=0}^\infty (I-A^{-1}B)^k A^{-1}\right\rVert \leq \left\lVert A^{-1}\right\rVert + \left\lVert \sum\limits_{k=0}^\infty (I-A^{-1}B)^k A^{-1}\right\rVert \leq \\ \left\lVert A^{-1}\right\rVert + \left\lVert \sum\limits_{k=0}^\infty (I-A^{-1}B)^k \right\rVert \cdot \left\lVert A^{-1}\right\rVert \leq \left\lVert A^{-1}\right\rVert \left[ 1 + \frac{1}{1-\left\lVert I-A^{-1}B \right\rVert} \right] = \left\lVert A^{-1}\right\rVert \left[ \frac{2-\left\lVert I-A^{-1}B \right\rVert}{1-\left\lVert I-A^{-1}B \right\rVert} \right] \stackrel{?}{\le}  \left\lVert A^{-1}\right\rVert \frac{\lVert I-A^{-1}B\rVert}{1-\left\lVert I-A^{-1}B\right\rVert } $$,,"['matrices', 'inequality', 'normed-spaces', 'matrix-calculus']"
88,Optimizing a matrix multiplication,Optimizing a matrix multiplication,,"Given the following operation: $$ \left(\begin{array}{c} f(1) \\ f(2) \\ \ldots \\ f(n)\end{array}\right) \begin{pmatrix} 1^{-1} & 1^0 & \cdots & 1^{n-2}\\ 2^{-1} & 2^0 & \cdots & 2^{n-2}\\ \vdots & \vdots & & \vdots \\ n^{-1} & n^0 & \cdots & n^{n-2}\end{pmatrix}$$ So a $n\times 1$ vector multiplied by a $n\times n$ matrix. As an example the $n=4$ matrix would be $$\begin{pmatrix} 1 & 1 & 1 & 1\\ 1/2 & 1 & 2 & 4 \\ 1/3 & 1 & 3 & 9 \\ 1/4 & 1 & 4 & 16\end{pmatrix}$$ I'm looking for anything I can take advantage of to reduce the complexity of this operation from $$O(n^2)$$ down to something better. Performing a Discrete Fourier Transform (DFT) for example uses the cyclic property of the exponential in a Fourier transform to reduce the operation. That doesn't apply here, since I am not using a sinusoid, but I'm wondering if there is anything that can achieve the same effect for this matrix.","Given the following operation: $$ \left(\begin{array}{c} f(1) \\ f(2) \\ \ldots \\ f(n)\end{array}\right) \begin{pmatrix} 1^{-1} & 1^0 & \cdots & 1^{n-2}\\ 2^{-1} & 2^0 & \cdots & 2^{n-2}\\ \vdots & \vdots & & \vdots \\ n^{-1} & n^0 & \cdots & n^{n-2}\end{pmatrix}$$ So a $n\times 1$ vector multiplied by a $n\times n$ matrix. As an example the $n=4$ matrix would be $$\begin{pmatrix} 1 & 1 & 1 & 1\\ 1/2 & 1 & 2 & 4 \\ 1/3 & 1 & 3 & 9 \\ 1/4 & 1 & 4 & 16\end{pmatrix}$$ I'm looking for anything I can take advantage of to reduce the complexity of this operation from $$O(n^2)$$ down to something better. Performing a Discrete Fourier Transform (DFT) for example uses the cyclic property of the exponential in a Fourier transform to reduce the operation. That doesn't apply here, since I am not using a sinusoid, but I'm wondering if there is anything that can achieve the same effect for this matrix.",,"['matrices', 'numerical-linear-algebra', 'fourier-transform']"
89,"Is there a mathematical property which could help ""sum up"" information from certain matrix areas?","Is there a mathematical property which could help ""sum up"" information from certain matrix areas?",,"I have a matrix $$A= \begin{pmatrix} 2 & -1 & 4 \\ -3 & 8 & -5\\ 12 & -7 & 16 \end{pmatrix} $$ and I would like to create the matrix $$B= \begin{pmatrix} 6 & 5 & 6 \\ 11 & 26 & 15\\ 10 & 21 & 12 \end{pmatrix} $$ where each entry of B is the sum of its surrounding cells in $A$. So the first entry of $B$ is $2-1-3+8=6$. The $3\times 3$ matrix and the summing ""radius"" are a simplification, the question aims at $m \times n$ matrices along with arbitrary rectangular areas to be measured. Is there some mathematical property which could help avoid having to implement something along the lines of $$b_{kl}=\sum_{l-a}^{l+b}\sum_{k-c}^{k+d}a_{kl} ~~~\text{given that the entries exist}$$ ? Special case: would things be easier if the entries only consisted of a fixed amount of $0$ and $1$?","I have a matrix $$A= \begin{pmatrix} 2 & -1 & 4 \\ -3 & 8 & -5\\ 12 & -7 & 16 \end{pmatrix} $$ and I would like to create the matrix $$B= \begin{pmatrix} 6 & 5 & 6 \\ 11 & 26 & 15\\ 10 & 21 & 12 \end{pmatrix} $$ where each entry of B is the sum of its surrounding cells in $A$. So the first entry of $B$ is $2-1-3+8=6$. The $3\times 3$ matrix and the summing ""radius"" are a simplification, the question aims at $m \times n$ matrices along with arbitrary rectangular areas to be measured. Is there some mathematical property which could help avoid having to implement something along the lines of $$b_{kl}=\sum_{l-a}^{l+b}\sum_{k-c}^{k+d}a_{kl} ~~~\text{given that the entries exist}$$ ? Special case: would things be easier if the entries only consisted of a fixed amount of $0$ and $1$?",,"['matrices', 'programming']"
90,Let $M=\big( \begin{smallmatrix} A & B \\ C & D \end{smallmatrix} \big)$. Prove $\det(M)=\det(A)\cdot \det(D-C·A^{-1}·B)$.,Let . Prove .,M=\big( \begin{smallmatrix} A & B \\ C & D \end{smallmatrix} \big) \det(M)=\det(A)\cdot \det(D-C·A^{-1}·B),"Let  (shown in matrix blocks) $M=\big( \begin{smallmatrix} A & B \\ C & D \end{smallmatrix} \big)$ a square matrix such that $A$ is invertible and $D$ is a square matrix. I have to prove that $$\det(M)=\det(A)\cdot \det(D-C·A^{-1}·B)$$ I also have an indication to use it: Consider previously these cases: \begin{align*} A_1 &= \begin{pmatrix} A & 0 \\ 0 & I \end{pmatrix},\\ A_2 &= \begin{pmatrix} I & 0 \\ 0 & D \end{pmatrix},\\ A_3 &= A_1·A_2,\\ A_4 &= \begin{pmatrix} I & A^{-1}·B \\ 0 & I \end{pmatrix},\text{ and}\\ A_5 &= A_3\cdot A_4. \end{align*} The problem is difficult in general for me and I don't even know how to use the indication.","Let  (shown in matrix blocks) $M=\big( \begin{smallmatrix} A & B \\ C & D \end{smallmatrix} \big)$ a square matrix such that $A$ is invertible and $D$ is a square matrix. I have to prove that $$\det(M)=\det(A)\cdot \det(D-C·A^{-1}·B)$$ I also have an indication to use it: Consider previously these cases: \begin{align*} A_1 &= \begin{pmatrix} A & 0 \\ 0 & I \end{pmatrix},\\ A_2 &= \begin{pmatrix} I & 0 \\ 0 & D \end{pmatrix},\\ A_3 &= A_1·A_2,\\ A_4 &= \begin{pmatrix} I & A^{-1}·B \\ 0 & I \end{pmatrix},\text{ and}\\ A_5 &= A_3\cdot A_4. \end{align*} The problem is difficult in general for me and I don't even know how to use the indication.",,"['matrices', 'determinant']"
91,Can $A^{T}(AA^{T})^{-1}A$ be simplified?,Can  be simplified?,A^{T}(AA^{T})^{-1}A,Let $A$ is an $m\times n$ ($m<n$) real matrix with full positive entries and $\text{Rank}(A)=m$. Thus $(AA^{T})^{-1}$ is an $m\times m$ symmetric $M$-matrix since $AA^{T}$ is nonnegtive and positive definited. I think $A^{T}(AA^{T})^{-1}A$ can be simplified to a simple form due to its special structure but I don't know how to do it. I tried using a pseudo inverse and the Cholesky decomposition but they made it more complex. Thanks.,Let $A$ is an $m\times n$ ($m<n$) real matrix with full positive entries and $\text{Rank}(A)=m$. Thus $(AA^{T})^{-1}$ is an $m\times m$ symmetric $M$-matrix since $AA^{T}$ is nonnegtive and positive definited. I think $A^{T}(AA^{T})^{-1}A$ can be simplified to a simple form due to its special structure but I don't know how to do it. I tried using a pseudo inverse and the Cholesky decomposition but they made it more complex. Thanks.,,"['matrices', 'inverse', 'matrix-decomposition', 'positive-definite']"
92,Eigenvalues of block matrix of order $m+1$,Eigenvalues of block matrix of order,m+1,"How to find eigenvalues of following matrix? $\begin{bmatrix} mkI-A & -A & -A & \cdots & -A\\ -A & kI-A & O & \cdots & O\\ -A & O & kI-A & \cdots & O\\ \vdots & \vdots & \vdots & \ddots & \vdots\\ -A & O & O & \cdots & kI-A\\ \end{bmatrix}_{m+1}$ Where, $A$ is any square matrix of order $n$, $O$ is zero matrix of order $n$ $I$ is identity matrix of order $n$, $k \in \mathbb{N}$ As well as one eigenvalue of above matrix is zero. I think if we can convert above matrix in terms of either Kronecker product or Kronecker sum of two matrices then we can find eigenvalues of above matrix by taking multiplication or addition of two matrices respectively. The other way is might be if we can convert above matrix in block diagonal matrix then we can find eigenvalues easily.","How to find eigenvalues of following matrix? $\begin{bmatrix} mkI-A & -A & -A & \cdots & -A\\ -A & kI-A & O & \cdots & O\\ -A & O & kI-A & \cdots & O\\ \vdots & \vdots & \vdots & \ddots & \vdots\\ -A & O & O & \cdots & kI-A\\ \end{bmatrix}_{m+1}$ Where, $A$ is any square matrix of order $n$, $O$ is zero matrix of order $n$ $I$ is identity matrix of order $n$, $k \in \mathbb{N}$ As well as one eigenvalue of above matrix is zero. I think if we can convert above matrix in terms of either Kronecker product or Kronecker sum of two matrices then we can find eigenvalues of above matrix by taking multiplication or addition of two matrices respectively. The other way is might be if we can convert above matrix in block diagonal matrix then we can find eigenvalues easily.",,"['matrices', 'eigenvalues-eigenvectors']"
93,Eigenvalues of block matrix related,Eigenvalues of block matrix related,,"What are the eigenvalues of following block matrix? $$\begin{bmatrix} A & B \\  B^T & O \end{bmatrix}$$ Here, $A$ and $B$ are any square matrices of order $n$ , $O$ is zero matrix of order $n$ .","What are the eigenvalues of following block matrix? Here, and are any square matrices of order , is zero matrix of order .","\begin{bmatrix}
A & B \\ 
B^T & O
\end{bmatrix} A B n O n","['matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
94,Prove the following result for Hermitian and Skew-Hermitian matrix,Prove the following result for Hermitian and Skew-Hermitian matrix,,"If $H$ be a Hermitian matrix, prove that $\det H$ is real number. If $S$ be a skew Hermitian matrix of order $n$, prove that (i). if $n$ be even, then $\det S$ is real number; (ii). if $n$ be odd, then $\det S$ is a purely imaginary number or zero. Attempt: 1. Let $H=P+iQ$ be a Hermitian matrix, where $P,Q$ are real matrices. Then $\bar{H}^t=H\implies P^t-iQ^t=P+iQ\implies P^t=P$ and $Q^t-Q$.  How can I show that $\det H$ is real?","If $H$ be a Hermitian matrix, prove that $\det H$ is real number. If $S$ be a skew Hermitian matrix of order $n$, prove that (i). if $n$ be even, then $\det S$ is real number; (ii). if $n$ be odd, then $\det S$ is a purely imaginary number or zero. Attempt: 1. Let $H=P+iQ$ be a Hermitian matrix, where $P,Q$ are real matrices. Then $\bar{H}^t=H\implies P^t-iQ^t=P+iQ\implies P^t=P$ and $Q^t-Q$.  How can I show that $\det H$ is real?",,"['matrices', 'hermitian-matrices']"
95,How can I determine B-inverse from an optimal tableau of a LP?,How can I determine B-inverse from an optimal tableau of a LP?,,"(This is NOT a homework question, I am reviewing for my upcoming exam) Given this linear program: and this optimal tableau: I am attempting to determine $B$ inverse using the table above. From the table I know that my basic variables are $x_1$, $x_2$ and $e_2$. I previously believed that $B$ inverse could simply be read from the optimal tableau as the values for your non basic variables. Therefore I originally thought $B$ inverse was the columns (excluding the first row) of $s_3$, $a_1$ and $a_2$ in the table above. The columns of my BVs from my original equation (and therefore the value for B) is: \begin{bmatrix}1&2&0\\1&-1&-1\\2&1&0\end{bmatrix} and if I find the inverse of $B$ myself I get: \begin{bmatrix}-1/3&0&2/3\\2/3&0&-1/3\\-1&-1&1\end{bmatrix} However this does not match the entries for my non basic variables in the table. Not only are the last two columns flipped, but the last row also has the wrong signs. I presume the problem is that $x_1$, $x_2$ and $e_2$ in the final solution do not form the identity matrix, and possibly because $e_2$ is an excess variable and not a slack variable, however I can't be certain as my textbook only has an example where in the optimal solution, the BVs columns form the identity matrix and there are only slack variables. Is it possible to read $B$ inverse from the optimal table in such a question?","(This is NOT a homework question, I am reviewing for my upcoming exam) Given this linear program: and this optimal tableau: I am attempting to determine $B$ inverse using the table above. From the table I know that my basic variables are $x_1$, $x_2$ and $e_2$. I previously believed that $B$ inverse could simply be read from the optimal tableau as the values for your non basic variables. Therefore I originally thought $B$ inverse was the columns (excluding the first row) of $s_3$, $a_1$ and $a_2$ in the table above. The columns of my BVs from my original equation (and therefore the value for B) is: \begin{bmatrix}1&2&0\\1&-1&-1\\2&1&0\end{bmatrix} and if I find the inverse of $B$ myself I get: \begin{bmatrix}-1/3&0&2/3\\2/3&0&-1/3\\-1&-1&1\end{bmatrix} However this does not match the entries for my non basic variables in the table. Not only are the last two columns flipped, but the last row also has the wrong signs. I presume the problem is that $x_1$, $x_2$ and $e_2$ in the final solution do not form the identity matrix, and possibly because $e_2$ is an excess variable and not a slack variable, however I can't be certain as my textbook only has an example where in the optimal solution, the BVs columns form the identity matrix and there are only slack variables. Is it possible to read $B$ inverse from the optimal table in such a question?",,"['matrices', 'inverse', 'linear-programming', 'simplex']"
96,Does anyone know the Burnside Matrices?,Does anyone know the Burnside Matrices?,,"For $G$ a fine group with conjugacy classes $C_1,\dots C_k$ we introduced the Burnside Matrices $A_r$ where $1<r<k$ with entries: $$A_r := \Big(\sqrt{\frac{|C_t|}{|C_s|}}a_{rst}\Big)_{1\leq s,t\leq k} $$ where $$a_{rst} = \frac{1}{|C_t|} |\{(x,y) \in C_r\times C_s\ |\ xy\in C_t\}|$$ Has anyone ever heard of them? I've been trying to find more Information on these matrices. I even checked then collected works of William Burnside. They have very interesting properties, mainly they are simulatiously diaganalizable by a unitary matrix $V$. Calling the column vectors $v_s$ and defining: $$ \chi_s (g) = \sqrt{|G| } \sum_{j=1}^k\frac{v_{sj}}{\sqrt{|C_j|}} \delta_{C_j}(g)\ 1\leq s\leq k $$ where g is an element of $G$ and the delta is $1$ for $g\in C_j$ and else zero. then the matrix: $$\big(\chi_s(C_t)\big)_{1\leq s,t\leq k}$$ is exactly the character table of all irreducible representations of $G$.","For $G$ a fine group with conjugacy classes $C_1,\dots C_k$ we introduced the Burnside Matrices $A_r$ where $1<r<k$ with entries: $$A_r := \Big(\sqrt{\frac{|C_t|}{|C_s|}}a_{rst}\Big)_{1\leq s,t\leq k} $$ where $$a_{rst} = \frac{1}{|C_t|} |\{(x,y) \in C_r\times C_s\ |\ xy\in C_t\}|$$ Has anyone ever heard of them? I've been trying to find more Information on these matrices. I even checked then collected works of William Burnside. They have very interesting properties, mainly they are simulatiously diaganalizable by a unitary matrix $V$. Calling the column vectors $v_s$ and defining: $$ \chi_s (g) = \sqrt{|G| } \sum_{j=1}^k\frac{v_{sj}}{\sqrt{|C_j|}} \delta_{C_j}(g)\ 1\leq s\leq k $$ where g is an element of $G$ and the delta is $1$ for $g\in C_j$ and else zero. then the matrix: $$\big(\chi_s(C_t)\big)_{1\leq s,t\leq k}$$ is exactly the character table of all irreducible representations of $G$.",,"['matrices', 'group-theory', 'finite-groups', 'representation-theory', 'characters']"
97,"If a matrix as well as its Hermitian part both have determinant one, must the matrix be Hermitian?","If a matrix as well as its Hermitian part both have determinant one, must the matrix be Hermitian?",,"If $x\in\mathrm{M}_2(\mathbb{C})$, $y=\dfrac{x+x^{\dagger}}{2}$, and $z=\dfrac{z-z^{\dagger}}{2}$, then $x=y+z$. Also, $y$ and $z$ respectively are Hermitian and anti-Hermitian, i.e. $y^{\dagger}=y$ and $z^{\dagger}=-z$, where $^\dagger$ denotes the conjugate transpose. Now suppose $\det(x)=\det(y)=1$. Does this force $x^{\dagger}=x$? I cannot find a counterexample but I can't prove it either. Some thoughts: Let $H$ and $A$ respectively denote the sets of $2\times2$ complex Hermitian and anti-Hermitian matrices, and consider the map $\phi:\mathrm{M}_2(\mathbb{C})\rightarrow H\times A$ which takes $x$ as above to $(y,z)$. Then $x\in H\iff\phi(x)=(x,0)$. Also, $\phi$ is a vector space isomorphism if we think of $\mathrm{M}_2(\mathbb{\mathbb{C}})$ as an 8 dimensional real vector space, so taking the first coordinate of its image gives a projection onto $H$ as a 4 dimensional real subspace.  I want to say that this projection cannot fix the determinant of $x$ unless $x$ is already in its image, but it's difficult to say anything in particular about the determinant here because it maps into $\mathbb{C}$ and the vector spaces I'm thinking about are real.","If $x\in\mathrm{M}_2(\mathbb{C})$, $y=\dfrac{x+x^{\dagger}}{2}$, and $z=\dfrac{z-z^{\dagger}}{2}$, then $x=y+z$. Also, $y$ and $z$ respectively are Hermitian and anti-Hermitian, i.e. $y^{\dagger}=y$ and $z^{\dagger}=-z$, where $^\dagger$ denotes the conjugate transpose. Now suppose $\det(x)=\det(y)=1$. Does this force $x^{\dagger}=x$? I cannot find a counterexample but I can't prove it either. Some thoughts: Let $H$ and $A$ respectively denote the sets of $2\times2$ complex Hermitian and anti-Hermitian matrices, and consider the map $\phi:\mathrm{M}_2(\mathbb{C})\rightarrow H\times A$ which takes $x$ as above to $(y,z)$. Then $x\in H\iff\phi(x)=(x,0)$. Also, $\phi$ is a vector space isomorphism if we think of $\mathrm{M}_2(\mathbb{\mathbb{C}})$ as an 8 dimensional real vector space, so taking the first coordinate of its image gives a projection onto $H$ as a 4 dimensional real subspace.  I want to say that this projection cannot fix the determinant of $x$ unless $x$ is already in its image, but it's difficult to say anything in particular about the determinant here because it maps into $\mathbb{C}$ and the vector spaces I'm thinking about are real.",,['matrices']
98,Matrix norm question,Matrix norm question,,"Let $A^*$ denote the complex conjugate transpose of a matrix $A$. In the Euclidean norm, if $$||A^*A+AA^*||=||A^*A||$$ does it imply that $AA^*=0$. If not, could you give a counter-example?","Let $A^*$ denote the complex conjugate transpose of a matrix $A$. In the Euclidean norm, if $$||A^*A+AA^*||=||A^*A||$$ does it imply that $AA^*=0$. If not, could you give a counter-example?",,['matrices']
99,"Given a finite metric space, are the matrices of triangle inequality errors invertible?","Given a finite metric space, are the matrices of triangle inequality errors invertible?",,"I have been working on some problems regarding finite metric spaces and have already proven/positively answered the following statement/question if the underlying metric has additional properties. Now I'm wondering if the statement is true in general. Suppose we have a finite metric space $(X,d)$ and we fix $y \in X$. Now consider the matrix $$M(y) := ( d (x, y) + d (y, z) - d (x, z))_{x, z \in X\setminus \{y\}}$$ of all possible errors that arise in the triangle inequality where the 'middle point' is fixed. Is $M(y)$ invertible? Any ideas are greatly appreciated.","I have been working on some problems regarding finite metric spaces and have already proven/positively answered the following statement/question if the underlying metric has additional properties. Now I'm wondering if the statement is true in general. Suppose we have a finite metric space $(X,d)$ and we fix $y \in X$. Now consider the matrix $$M(y) := ( d (x, y) + d (y, z) - d (x, z))_{x, z \in X\setminus \{y\}}$$ of all possible errors that arise in the triangle inequality where the 'middle point' is fixed. Is $M(y)$ invertible? Any ideas are greatly appreciated.",,"['matrices', 'metric-spaces', 'inverse']"
