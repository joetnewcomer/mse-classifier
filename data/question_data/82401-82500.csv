,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How do I determine if the vectors lie on a plane in an $N\times N$ matrix?,How do I determine if the vectors lie on a plane in an  matrix?,N\times N,"In a $2\times2$ matrix, it is quite easy to see if the vectors lie on a plane or not. By vector, I mean the columns of the matrix. I usually determine if the numbers are of a certain multiple. From there, I can judge that the columns of the matrix lie on a plane and that means its determinant is zero and therefore not invertible. So for instance:  $$ \left |\begin{bmatrix} 2 & 4\\  4 & 8 \end{bmatrix}  \right | = 0 $$ Both vectors $\begin{bmatrix} 2\\ 4 \end{bmatrix}$ and $\begin{bmatrix} 4\\ 8 \end{bmatrix}$ lie on the same plane because the second vector is just 2 times of the first vector. But in a $3\times3$ or $N\times N$ matrix, how can I get to know if a matrix is invertible or not by observing it without going through to calculate its determinant? For example in a matrix say $\begin{bmatrix} 1 & 4 & 7\\  2 & 5 & 8\\  3 & 6 & 9 \end{bmatrix}$, the numbers are not of a common multiple of each other. But still, the determinant of this matrix gives zero, which means the matrix is not invertible. But I wouldn't have known this without going through the trouble to calculate its determinant value. Since the numbers are not of a multiple, I am also not sure if the columns of the matrix lie on the same plane or not. Thanks for any help.","In a $2\times2$ matrix, it is quite easy to see if the vectors lie on a plane or not. By vector, I mean the columns of the matrix. I usually determine if the numbers are of a certain multiple. From there, I can judge that the columns of the matrix lie on a plane and that means its determinant is zero and therefore not invertible. So for instance:  $$ \left |\begin{bmatrix} 2 & 4\\  4 & 8 \end{bmatrix}  \right | = 0 $$ Both vectors $\begin{bmatrix} 2\\ 4 \end{bmatrix}$ and $\begin{bmatrix} 4\\ 8 \end{bmatrix}$ lie on the same plane because the second vector is just 2 times of the first vector. But in a $3\times3$ or $N\times N$ matrix, how can I get to know if a matrix is invertible or not by observing it without going through to calculate its determinant? For example in a matrix say $\begin{bmatrix} 1 & 4 & 7\\  2 & 5 & 8\\  3 & 6 & 9 \end{bmatrix}$, the numbers are not of a common multiple of each other. But still, the determinant of this matrix gives zero, which means the matrix is not invertible. But I wouldn't have known this without going through the trouble to calculate its determinant value. Since the numbers are not of a multiple, I am also not sure if the columns of the matrix lie on the same plane or not. Thanks for any help.",,"['linear-algebra', 'matrices', 'vector-spaces']"
1,Least squares approximation (matrices),Least squares approximation (matrices),,I’m quite stuck on the question below. It keeps coming up in my exam and I don’t know how to do it! Please help me! Thanks! Show that the matrix $P = A (A^tA)^{-1} A^t$ represents an orthogonal projection onto $\mathcal{R}(A)$. Hence or otherwise explain why $x^\star = (A^t A)^{-1} A^t b$ represents the least squares solution of the matrix equation $Ax=b$.,I’m quite stuck on the question below. It keeps coming up in my exam and I don’t know how to do it! Please help me! Thanks! Show that the matrix $P = A (A^tA)^{-1} A^t$ represents an orthogonal projection onto $\mathcal{R}(A)$. Hence or otherwise explain why $x^\star = (A^t A)^{-1} A^t b$ represents the least squares solution of the matrix equation $Ax=b$.,,"['linear-algebra', 'matrices']"
2,Linear Regression with 3x3 Matrices,Linear Regression with 3x3 Matrices,,"Here's my Homework Problem: We can generalize the least squares method to other polynomial curves. To find the quadratic equation $y=a x^2+b x+c$ that best fits the points $(-1, −3)$, $(0, 0)$, $(1, −1)$, and $(2, 1)$, we first write the matrix equation $AX=B$ that would result if a quadratic equation satisfied by all four points did indeed exist. (The third equation in this system would correspond to $x=1$ and $y= −1$: $a+b+c = −1$.) We proceed by writing the normal system $A^T A X=A^T B$. Use elementary row operations to find the equation of the quadratic equation that best fits the given four points.    Enter the (exact) value of y(1) on the quadratic regression curve. So far I have a solvable matrix using $A^T A = A^T B$ $$ \left( \begin{array}{rrr} 18 & 8 & 6 \\  8 & 6 & 2 \\ 6 & 2 & 4 \end{array} \right)  \left( \begin{matrix} a \\ b \\ c \end{matrix} \right)  =  \left( \begin{array}{r} 6 \\ 4 \\ -3 \end{array} \right)  $$ Normally, $$ \left( \begin{matrix} a & b \\ c & d \end{matrix} \right)^{-1}  =  \frac{1}{a d - b c} \left( \begin{array}{rr} d & -b \\ -c & a \end{array} \right) \> . $$ How does this law translate from $2 \times 2$ matrices to $3 \times 3$?","Here's my Homework Problem: We can generalize the least squares method to other polynomial curves. To find the quadratic equation $y=a x^2+b x+c$ that best fits the points $(-1, −3)$, $(0, 0)$, $(1, −1)$, and $(2, 1)$, we first write the matrix equation $AX=B$ that would result if a quadratic equation satisfied by all four points did indeed exist. (The third equation in this system would correspond to $x=1$ and $y= −1$: $a+b+c = −1$.) We proceed by writing the normal system $A^T A X=A^T B$. Use elementary row operations to find the equation of the quadratic equation that best fits the given four points.    Enter the (exact) value of y(1) on the quadratic regression curve. So far I have a solvable matrix using $A^T A = A^T B$ $$ \left( \begin{array}{rrr} 18 & 8 & 6 \\  8 & 6 & 2 \\ 6 & 2 & 4 \end{array} \right)  \left( \begin{matrix} a \\ b \\ c \end{matrix} \right)  =  \left( \begin{array}{r} 6 \\ 4 \\ -3 \end{array} \right)  $$ Normally, $$ \left( \begin{matrix} a & b \\ c & d \end{matrix} \right)^{-1}  =  \frac{1}{a d - b c} \left( \begin{array}{rr} d & -b \\ -c & a \end{array} \right) \> . $$ How does this law translate from $2 \times 2$ matrices to $3 \times 3$?",,"['matrices', 'regression']"
3,Is this a positive semi- definite matrix,Is this a positive semi- definite matrix,,"I have a matrix $A$, which satisfies : $A$ is symmetric; all the diagonal entries of $A$ are equal to $1$; other entries of $A$ is between $0$ and $1$. My question is, whether $A$ is a positive semi-definite matrix?","I have a matrix $A$, which satisfies : $A$ is symmetric; all the diagonal entries of $A$ are equal to $1$; other entries of $A$ is between $0$ and $1$. My question is, whether $A$ is a positive semi-definite matrix?",,['linear-algebra']
4,Proof of multiplicativity,Proof of multiplicativity,,"I define the function $D(\mathbf M)$ for square matrices by taking a maximal set of all triples  $(\mathbf v_i,k_i,n_i)_{i \in I}$ that satisfy $(\mathbf{M}-k\mathbf{I})^n\mathbf{v}_i = \mathbf{0}$ with the $\mathbf{v}_i$ linearly independent, and letting $$D(\mathbf M) = \prod_{i \in I} k_i.$$ For example the matrix $$\mathbf X=\begin{pmatrix}0&1&1\\1&0&1\\1&1&0\end{pmatrix}$$ has the set: $((-1\quad 0\quad 1)^T,-1,1)$ $((-1\quad 1\quad 0)^T,-1,1)$ $((1\quad 1\quad 1)^T,2,1)$ so $D(\mathbf X) = -1 \cdot -1 \cdot 2 = 2$. It is equal to the determinant but I don't want to use that. How can it be shown that $D(\mathbf A\mathbf B) = D(\mathbf A)D(\mathbf B)$?","I define the function $D(\mathbf M)$ for square matrices by taking a maximal set of all triples  $(\mathbf v_i,k_i,n_i)_{i \in I}$ that satisfy $(\mathbf{M}-k\mathbf{I})^n\mathbf{v}_i = \mathbf{0}$ with the $\mathbf{v}_i$ linearly independent, and letting $$D(\mathbf M) = \prod_{i \in I} k_i.$$ For example the matrix $$\mathbf X=\begin{pmatrix}0&1&1\\1&0&1\\1&1&0\end{pmatrix}$$ has the set: $((-1\quad 0\quad 1)^T,-1,1)$ $((-1\quad 1\quad 0)^T,-1,1)$ $((1\quad 1\quad 1)^T,2,1)$ so $D(\mathbf X) = -1 \cdot -1 \cdot 2 = 2$. It is equal to the determinant but I don't want to use that. How can it be shown that $D(\mathbf A\mathbf B) = D(\mathbf A)D(\mathbf B)$?",,"['linear-algebra', 'matrices']"
5,How does $norm(A) \leq p(A) + e$ imply $norm(A) < 1$ if $p(A) < 1$?,How does  imply  if ?,norm(A) \leq p(A) + e norm(A) < 1 p(A) < 1,"I am given a lemma and it states: Lemma: Let $A$ be a real $n\times n$ matrix.  Then given any $e > 0$, there is a $norm$ such that $norm(A) \leq p(A) + e$,  where $p(A)$ is the spectral radius of $A$. They then go on to state that based on this lemma, if $p(A) < 1$, then $norm(A) < 1$ for the correct choice of the norm. What I'm looking for help on is understanding how they came to that conclusion based only on that lemma.  For example, given $e =0.001$, then the lemma states there is a norm such that $norm(A)\leq p(A) + .001$. Since it is a ""or equal"" then this means there is the possibility that $norm(A) = p(A) + 0.001$, and in such a case then $norm(A) > p(A)$.  So it seems that based on the lemma, then $p(A) < 1$ does not necessarily always imply that $norm(A) < 1$. I even tried to do a proof of this but still came up with the same result: $$\begin{align} p(A)<1&\implies p(A)+e<1+e\\ &\implies norm(A)\leq p(A)+e<1+e\\ &\implies norm(A)-e\leq p(A)<1. \end{align} $$ This pretty much says the same thing, if $norm(A) - e \leq p(A)$, then $p(A) < 1$, but if $p(A)$ close to $1$ by a difference less than $e$, then $norm(A) > 1$ in the ""equal to"" case where $norm(A) - e = p(A)$. Now of course you could pick a smaller e, but the way the lemma reads it says that for a given $e$, you can find a norm s.t. $norm(A) \leq p(A) + e$, so the ""or equal"" part will still get you using a counter example I gave similar to the $e = .001$ above Now I have seen a different proof on wikipedia that shows if $p(A) < 1$ then $norm(A) < 1$, so I am not disputing that fact, but that proof gets into looking at entries of the Jordan Normal Form.  So my point is, yes $p(A) < 1$ then $norm(A) < 1$, BUT I don't understand how you can conclude that based only on the Lemma I gave at the beginning.  It's these big jumps in rational that confuse me.","I am given a lemma and it states: Lemma: Let $A$ be a real $n\times n$ matrix.  Then given any $e > 0$, there is a $norm$ such that $norm(A) \leq p(A) + e$,  where $p(A)$ is the spectral radius of $A$. They then go on to state that based on this lemma, if $p(A) < 1$, then $norm(A) < 1$ for the correct choice of the norm. What I'm looking for help on is understanding how they came to that conclusion based only on that lemma.  For example, given $e =0.001$, then the lemma states there is a norm such that $norm(A)\leq p(A) + .001$. Since it is a ""or equal"" then this means there is the possibility that $norm(A) = p(A) + 0.001$, and in such a case then $norm(A) > p(A)$.  So it seems that based on the lemma, then $p(A) < 1$ does not necessarily always imply that $norm(A) < 1$. I even tried to do a proof of this but still came up with the same result: $$\begin{align} p(A)<1&\implies p(A)+e<1+e\\ &\implies norm(A)\leq p(A)+e<1+e\\ &\implies norm(A)-e\leq p(A)<1. \end{align} $$ This pretty much says the same thing, if $norm(A) - e \leq p(A)$, then $p(A) < 1$, but if $p(A)$ close to $1$ by a difference less than $e$, then $norm(A) > 1$ in the ""equal to"" case where $norm(A) - e = p(A)$. Now of course you could pick a smaller e, but the way the lemma reads it says that for a given $e$, you can find a norm s.t. $norm(A) \leq p(A) + e$, so the ""or equal"" part will still get you using a counter example I gave similar to the $e = .001$ above Now I have seen a different proof on wikipedia that shows if $p(A) < 1$ then $norm(A) < 1$, so I am not disputing that fact, but that proof gets into looking at entries of the Jordan Normal Form.  So my point is, yes $p(A) < 1$ then $norm(A) < 1$, BUT I don't understand how you can conclude that based only on the Lemma I gave at the beginning.  It's these big jumps in rational that confuse me.",,['matrices']
6,What $3 \times 3$ matrix gives the determinant $a^{2}+b^{2}+c^{2}$?,What  matrix gives the determinant ?,3 \times 3 a^{2}+b^{2}+c^{2},"The $2D$ rotation matrix is given by a $2\times{2}$ matrix: $$\begin{bmatrix} a & -b\\ b & a \end{bmatrix}$$ The determinant of this matrix is $a^{2}+b^{2}$ . Can one construct a $3\times{3}$ matrix for $3D$ rotations where the determinant of the matrix is $a^{2}+b^{2}+c^{2}$ , and the elements of the matrix are $a,b,c\in\mathbb{R}$ . If there is not, is there a $3\times{3}$ or $2\times{2}$ matrix that can be constructed with the same determinant, $a^{2}+b^{2}+c^{2}$ given complex numbers?","The rotation matrix is given by a matrix: The determinant of this matrix is . Can one construct a matrix for rotations where the determinant of the matrix is , and the elements of the matrix are . If there is not, is there a or matrix that can be constructed with the same determinant, given complex numbers?","2D 2\times{2} \begin{bmatrix}
a & -b\\
b & a
\end{bmatrix} a^{2}+b^{2} 3\times{3} 3D a^{2}+b^{2}+c^{2} a,b,c\in\mathbb{R} 3\times{3} 2\times{2} a^{2}+b^{2}+c^{2}","['linear-algebra', 'matrices', 'linear-transformations', 'determinant', 'rotations']"
7,"If $A$ is normal with $\sigma(A)\subseteq \mathbb{R}\cup\mathbb{T}$, does $\text{dim ker}(AB-BA)=\text{dim ker}(A^*B-BA^*)$?","If  is normal with , does ?",A \sigma(A)\subseteq \mathbb{R}\cup\mathbb{T} \text{dim ker}(AB-BA)=\text{dim ker}(A^*B-BA^*),"This clearly holds if $A$ is self-adjoint, and also if $A$ is unitary, because then $A(\text{ker}(AB-BA))=\text{ker}(A^*B-BA^*)$ . To prove this, if $w\in\text{ker}(AB-BA)$ , then $A^*B(Aw)=A^*ABw=Bw=BA^*(Aw)$ , so $Aw\in\text{ker}(A^*B-BA^*)$ and if $v\in\text{ker}(A^*B-BA^*)$ , then $v=AA^*v$ and with $w=A^*v$ we have $ABw=ABA^*v=AA^*Bv=Bv=BAA^*v=BAw$ , so $w\in\text{ker}(AB-BA)$ . If $A$ is normal with spectrum contained in the union of the real line and the unit circle, then there several things one can try. On the one hand, if we diagonalize $A=UDU^*$ , then $D$ can be split into the sum of two diagonal matrices, one with the real entries and $0$ 's else, and one with the entries on the unit circle and $0$ 's else, $D=D_1+D_2$ . Moreover, let $J$ be the diagonal matrix which has $1$ 's where $D$ has real entries and $0$ 's else, then $D=(D_1-J)+(D_2+J)$ , so $A=U(D_1-J)U^*+U(D_2+J)U^*=:A_1+A_2$ , where $A_1$ is self-adjoint and $A_2$ is unitary. In this case, $A^*=A_1+A_2^*$ Another possibility is to split up $D$ as the product of two diagonal matrices, one with the real entries and $1$ 's where the entries from the unit circle were, and one with the entries from the unit circle and $1$ 's where the real entries were, $D=D_1D_2$ , which leads to $A=(UD_1U^*)(UD_2U^*)=:A_1A_2$ where $A_1$ is self-adjoint and $A_2$ is unitary again. In this case $A^*=A_1A_2^*$ . I couldn't prove the statement in both cases. It would be good to express $\text{dim ker}(A_1B-BA_1)$ and $\text{dim ker}(A_2B-BA_2)$ in terms of $\text{dim ker}(AB-BA)$ .","This clearly holds if is self-adjoint, and also if is unitary, because then . To prove this, if , then , so and if , then and with we have , so . If is normal with spectrum contained in the union of the real line and the unit circle, then there several things one can try. On the one hand, if we diagonalize , then can be split into the sum of two diagonal matrices, one with the real entries and 's else, and one with the entries on the unit circle and 's else, . Moreover, let be the diagonal matrix which has 's where has real entries and 's else, then , so , where is self-adjoint and is unitary. In this case, Another possibility is to split up as the product of two diagonal matrices, one with the real entries and 's where the entries from the unit circle were, and one with the entries from the unit circle and 's where the real entries were, , which leads to where is self-adjoint and is unitary again. In this case . I couldn't prove the statement in both cases. It would be good to express and in terms of .",A A A(\text{ker}(AB-BA))=\text{ker}(A^*B-BA^*) w\in\text{ker}(AB-BA) A^*B(Aw)=A^*ABw=Bw=BA^*(Aw) Aw\in\text{ker}(A^*B-BA^*) v\in\text{ker}(A^*B-BA^*) v=AA^*v w=A^*v ABw=ABA^*v=AA^*Bv=Bv=BAA^*v=BAw w\in\text{ker}(AB-BA) A A=UDU^* D 0 0 D=D_1+D_2 J 1 D 0 D=(D_1-J)+(D_2+J) A=U(D_1-J)U^*+U(D_2+J)U^*=:A_1+A_2 A_1 A_2 A^*=A_1+A_2^* D 1 1 D=D_1D_2 A=(UD_1U^*)(UD_2U^*)=:A_1A_2 A_1 A_2 A^*=A_1A_2^* \text{dim ker}(A_1B-BA_1) \text{dim ker}(A_2B-BA_2) \text{dim ker}(AB-BA),"['linear-algebra', 'matrices', 'matrix-rank']"
8,Solve $\| X A - B \|$ subject to $X C = C X$,Solve  subject to,\| X A - B \| X C = C X,"Given $A, B \in \mathbb{R}^{n \times k}$ and S.P.D. $C \in \mathbb{R}^{n \times n}$ , I would like to find an analytical solution for the matrix $X \in \mathbb{R}^{n \times n}$ that minimizes \begin{align} \lVert X A - B  \rVert^2_{F} \end{align} subject to the hard constraint $$ C X = X C. $$ Given the eigendecomposition of $C$ with $C = R \Lambda R^T$ , from Nearest commuting matrix it appears that a reasonable projection operator onto the space of matrices commuting with $C$ is $P_C(X) = R P_\Lambda(R^T X R) R^T$ with $$ [P_\Lambda(X)]_{ij} = \begin{cases} X_{ij}, & \lambda_i = \lambda_j \\ 0, & \textrm{otherwise}  \end{cases} $$ . With this in mind, my guess at solving this would be to apply this projection operator to the unconstrained minimal solution to the least squares system, e.g. $$ X = P_C(B A^{\dagger}) $$ with $A^{\dagger}$ denoting the M.P. pseudoinverse of $A$ . Alternatively, it seems like the solution $$ X = P_C(B A^T) $$ is also reasonable (similar to the orthogonal procrustes solution). That said, I don't know which one of these (if either) is the minimal solution that commutes with $C$ . EDIT: Building on top of user1551's excellent answer below, the solution to the case where $X$ is also constrained to be orthogonal can be expressed concisely as $$ X = R U V^T R^T $$ where $U, V^T$ are recovered from the SVD $$ U \Sigma  V^T = P_{\Lambda}(R^T B A^T R). $$ Pretty nifty.","Given and S.P.D. , I would like to find an analytical solution for the matrix that minimizes subject to the hard constraint Given the eigendecomposition of with , from Nearest commuting matrix it appears that a reasonable projection operator onto the space of matrices commuting with is with . With this in mind, my guess at solving this would be to apply this projection operator to the unconstrained minimal solution to the least squares system, e.g. with denoting the M.P. pseudoinverse of . Alternatively, it seems like the solution is also reasonable (similar to the orthogonal procrustes solution). That said, I don't know which one of these (if either) is the minimal solution that commutes with . EDIT: Building on top of user1551's excellent answer below, the solution to the case where is also constrained to be orthogonal can be expressed concisely as where are recovered from the SVD Pretty nifty.","A, B \in \mathbb{R}^{n \times k} C \in \mathbb{R}^{n \times n} X \in \mathbb{R}^{n \times n} \begin{align}
\lVert X A - B  \rVert^2_{F}
\end{align} 
C X = X C.
 C C = R \Lambda R^T C P_C(X) = R P_\Lambda(R^T X R) R^T 
[P_\Lambda(X)]_{ij} = \begin{cases} X_{ij}, & \lambda_i = \lambda_j \\ 0, & \textrm{otherwise} 
\end{cases}
 
X = P_C(B A^{\dagger})
 A^{\dagger} A 
X = P_C(B A^T)
 C X 
X = R U V^T R^T
 U, V^T 
U \Sigma  V^T = P_{\Lambda}(R^T B A^T R).
","['matrices', 'matrix-equations', 'matrix-decomposition', 'least-squares', 'symmetric-matrices']"
9,Understanding subspace-restricted isomorphisms and global isomorphisms,Understanding subspace-restricted isomorphisms and global isomorphisms,,"(It may be noted that the author was not aware of Endomorphisms & Automorphisms at the time of writing this question) I'm trying to better understand the concept of a linear transformation acting as an isomorphism on a specific subspace of the vector space, even though it may not be an isomorphism on the entire vector space. To elaborate, I am referring to a situation where a linear transformation acts like an isomorphism but only when we look at a smaller, specific part of the space (a subspace). Consider a linear transformation, $T: \mathbb{R}^3 \to \mathbb{R}^3$ defined by a rank 2 matrix: $$ A = \begin{pmatrix} 1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 0 \end{pmatrix} $$ $T$ is not an isomorphism on $\mathbb{R}^3$ , but it acts as an isomorphism on the subspace $W = \text{span}{((1, 0, 0), (0, 1, 0))}$ . I have a few related questions: What is the proper terminology for a linear transformation that acts as an isomorphism on a specific subspace but not necessarily on the entire space? I've tentatively called this a ""subspace-restricted isomorphism"" or ""restricted isomorphism."" How does the matrix representation of a subspace-restricted isomorphism differ from the matrix representation of the linear transformation on the entire space? Specifically, if we choose a basis that includes a basis for the invariant subspace, what structure does the matrix have? In contrast to a subspace-restricted isomorphism, what would be an appropriate name for a linear transformation that acts as an isomorphism on the entire vector space? I've considered terms like ""global isomorphism"" or ""space isomorphism."" Are there any standard names or conventions for such local (not global) properties in abstract algebra literature? I'd appreciate any insights, clarifications, or references to relevant literature that could help me better understand these concepts and their proper terminology. Thank you!","(It may be noted that the author was not aware of Endomorphisms & Automorphisms at the time of writing this question) I'm trying to better understand the concept of a linear transformation acting as an isomorphism on a specific subspace of the vector space, even though it may not be an isomorphism on the entire vector space. To elaborate, I am referring to a situation where a linear transformation acts like an isomorphism but only when we look at a smaller, specific part of the space (a subspace). Consider a linear transformation, defined by a rank 2 matrix: is not an isomorphism on , but it acts as an isomorphism on the subspace . I have a few related questions: What is the proper terminology for a linear transformation that acts as an isomorphism on a specific subspace but not necessarily on the entire space? I've tentatively called this a ""subspace-restricted isomorphism"" or ""restricted isomorphism."" How does the matrix representation of a subspace-restricted isomorphism differ from the matrix representation of the linear transformation on the entire space? Specifically, if we choose a basis that includes a basis for the invariant subspace, what structure does the matrix have? In contrast to a subspace-restricted isomorphism, what would be an appropriate name for a linear transformation that acts as an isomorphism on the entire vector space? I've considered terms like ""global isomorphism"" or ""space isomorphism."" Are there any standard names or conventions for such local (not global) properties in abstract algebra literature? I'd appreciate any insights, clarifications, or references to relevant literature that could help me better understand these concepts and their proper terminology. Thank you!","T: \mathbb{R}^3 \to \mathbb{R}^3 
A = \begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 0
\end{pmatrix}
 T \mathbb{R}^3 W = \text{span}{((1, 0, 0), (0, 1, 0))}","['linear-algebra', 'abstract-algebra', 'matrices', 'linear-transformations', 'vector-space-isomorphism']"
10,The existence of a matrix $B$ with $AB=BA^c$ implies nilpotence of $A^r-I$.,The existence of a matrix  with  implies nilpotence of .,B AB=BA^c A^r-I,"My friend and me were studying some group theory, and we thought of the following problem. Let $n$ be a positive integer, and let $A\in \text{GL}_n(\mathbb{Z})$ be a matrix with integer entries that is invertible over $\mathbb{Z}$ (i.e. $\text{det}(A)=\pm 1$ ). Question: Does the existence of a matrix $B\in \text{GL}_n(\mathbb{Q})$ and an integer $c>1$ such that $$AB = BA^c$$ imply that there exists some pair of integers $r,s>0$ such that $(A^r-I)^s=0$ ? Or equivalently that $A^r-I$ is nilpotent? This implication seems a bit too random to be true, does anyone know a counterexample (or a proof)? What we have thought of so far: The matrix $B$ needs to be invertible, as else $B=0$ and for example the matrix $$A=\left(\begin{matrix} 2 & 5 \\            1 & 3\end{matrix}\right)$$ has eigenvalues that are not roots of unity, but it is actually diagonalizable, showing that $A^r-I=S(D^r-I)S^{-1}$ will never be nilpotent (its determinant is nonzero). We are not really interested in the case when $A$ is not invertible, but when $A$ has determinant $|\text{det}(A)|>1$ the equation $AB=BA^c$ will fail for any $c>1$ by taking the determinant on both sides. We will need $c>1$ , as else we may take the commuting matrices $$A = \left(\begin{matrix} 1 & 5 \\ 1 & 6 \end{matrix}\right); B= \left(\begin{matrix} 2 & 5 \\ 1 & 7 \end{matrix}\right).$$ That is, we have $AB=BA$ , but $A$ has eigenvalues that are not roots of unity, showing that $A^r-I$ will never be nilpotent for $r>0$ with the same argument as before. We have only tried $2\times 2$ matrices, as it seems incredibly difficult to check if such a matrix $B$ exists in any case. To show that there is some support for the question, we may take $$A = \left(\begin{matrix} 1 & -3 \\ 1 & -2 \end{matrix}\right)$$ and $$B= \left(\begin{matrix} -6 & -3 \\ -7 & 6 \end{matrix}\right).$$ This example has $AB=BA^2$ , the matrix $A$ is diagonalizable and the eigenvalues are roots of unity (implying that $A$ is of finite order, so $A^r-I=0$ for some $r>0$ , in this case $r=3$ ). These were the final thought we had before submitting this question: only assuming the relation $AB=BA^c$ does not imply nilpotence of $A^r-I$ for some $r>0$ as we have seen above. So we do need the invertibility of $A$ and $B$ . Moreover, somehow $A$ having determinant $\pm 1$ makes it more plausible for this to be true; the ""excess"" of the relation $AB=BA^c$ is something along the lines of $A^{c-1}$ . This is a positive power, so we think this might somehow imply that the eigenvalues of $A$ must be roots of unity. Can this imply the statement?","My friend and me were studying some group theory, and we thought of the following problem. Let be a positive integer, and let be a matrix with integer entries that is invertible over (i.e. ). Question: Does the existence of a matrix and an integer such that imply that there exists some pair of integers such that ? Or equivalently that is nilpotent? This implication seems a bit too random to be true, does anyone know a counterexample (or a proof)? What we have thought of so far: The matrix needs to be invertible, as else and for example the matrix has eigenvalues that are not roots of unity, but it is actually diagonalizable, showing that will never be nilpotent (its determinant is nonzero). We are not really interested in the case when is not invertible, but when has determinant the equation will fail for any by taking the determinant on both sides. We will need , as else we may take the commuting matrices That is, we have , but has eigenvalues that are not roots of unity, showing that will never be nilpotent for with the same argument as before. We have only tried matrices, as it seems incredibly difficult to check if such a matrix exists in any case. To show that there is some support for the question, we may take and This example has , the matrix is diagonalizable and the eigenvalues are roots of unity (implying that is of finite order, so for some , in this case ). These were the final thought we had before submitting this question: only assuming the relation does not imply nilpotence of for some as we have seen above. So we do need the invertibility of and . Moreover, somehow having determinant makes it more plausible for this to be true; the ""excess"" of the relation is something along the lines of . This is a positive power, so we think this might somehow imply that the eigenvalues of must be roots of unity. Can this imply the statement?","n A\in \text{GL}_n(\mathbb{Z}) \mathbb{Z} \text{det}(A)=\pm 1 B\in \text{GL}_n(\mathbb{Q}) c>1 AB = BA^c r,s>0 (A^r-I)^s=0 A^r-I B B=0 A=\left(\begin{matrix} 2 & 5 \\ 
          1 & 3\end{matrix}\right) A^r-I=S(D^r-I)S^{-1} A A |\text{det}(A)|>1 AB=BA^c c>1 c>1 A = \left(\begin{matrix} 1 & 5 \\ 1 & 6 \end{matrix}\right); B= \left(\begin{matrix} 2 & 5 \\ 1 & 7 \end{matrix}\right). AB=BA A A^r-I r>0 2\times 2 B A = \left(\begin{matrix} 1 & -3 \\ 1 & -2 \end{matrix}\right) B= \left(\begin{matrix} -6 & -3 \\ -7 & 6 \end{matrix}\right). AB=BA^2 A A A^r-I=0 r>0 r=3 AB=BA^c A^r-I r>0 A B A \pm 1 AB=BA^c A^{c-1} A","['linear-algebra', 'matrices', 'group-theory', 'nilpotent-groups']"
11,Finding determinant and rank of matrix $A$,Finding determinant and rank of matrix,A,"Finding rank of this matrix : $$A_{n\times n}= \begin{bmatrix} a+1&a&\cdots&a\\ a&a+1&\cdots&a\\ \vdots&\vdots&\ddots&\vdots\\ a&a&\cdots&a+1 \end{bmatrix} $$ $(a\geq 2, a\in \mathbb{N})$ I'm trying to prove $det(A) \ne 0$ so that $rank(A)=n$ but I don't know how to prove it quickly. Can anyone help me ?",Finding rank of this matrix : I'm trying to prove so that but I don't know how to prove it quickly. Can anyone help me ?,"A_{n\times n}=
\begin{bmatrix}
a+1&a&\cdots&a\\
a&a+1&\cdots&a\\
\vdots&\vdots&\ddots&\vdots\\
a&a&\cdots&a+1
\end{bmatrix}
 (a\geq 2, a\in \mathbb{N}) det(A) \ne 0 rank(A)=n","['linear-algebra', 'matrices', 'matrix-rank']"
12,Which matrix operation is occuring?,Which matrix operation is occuring?,,"I am currently going through a financial book that covers some matrix multiplication but I am not certain which operation is occurring to get this result. The book provides spreadsheets so I can see the result and therefore deduce what is happening but I don't know whether the math is correct or something else. The book is taking a [1xn] vector and multiplying this by an [nxn] matrix and then multiplying it by an [nx1] vector (whether first vector is [nx1] or [1xn] is not indicated by the book, same for the last vector). The literal formula provided by the book is $\Sigma = \sigma.\rho.\sigma^T$ where $\Sigma$ is the covariance matrix, $\sigma$ is the vector of standard deviations, $\rho$ is the correlation matrix. The following are the inputs and outputs, assuming $\sigma$ is a row vector: $\Sigma = \begin{bmatrix}\sigma_1 & \sigma_2\end{bmatrix} . \begin{bmatrix}\rho_{1,1} & \rho_{1,2}\\\rho_{2,1} & \rho_{2,2}\end{bmatrix} .\begin{bmatrix}\sigma_1\\\sigma_2\end{bmatrix} = \begin{bmatrix}\rho_{1,1}\sigma_1\sigma_1 & \rho_{1,2}\sigma_1\sigma_2\\\rho_{2,1}\sigma_2\sigma_1 & \rho_{2,2}\sigma_2\sigma_2\end{bmatrix}$ With a dot product (to my understanding), the dimensions of the matrix and vectors would result in a single scalar, not a matrix. If the dimensions of the vectors were flipped (column vector then row vector), the dot product would be impossible due to the different number of columns in the vector as rows in the matrix. Any help is greatly appreciate! I apologize if this is a simple problem but this has been puzzling me for awhile now.","I am currently going through a financial book that covers some matrix multiplication but I am not certain which operation is occurring to get this result. The book provides spreadsheets so I can see the result and therefore deduce what is happening but I don't know whether the math is correct or something else. The book is taking a [1xn] vector and multiplying this by an [nxn] matrix and then multiplying it by an [nx1] vector (whether first vector is [nx1] or [1xn] is not indicated by the book, same for the last vector). The literal formula provided by the book is where is the covariance matrix, is the vector of standard deviations, is the correlation matrix. The following are the inputs and outputs, assuming is a row vector: With a dot product (to my understanding), the dimensions of the matrix and vectors would result in a single scalar, not a matrix. If the dimensions of the vectors were flipped (column vector then row vector), the dot product would be impossible due to the different number of columns in the vector as rows in the matrix. Any help is greatly appreciate! I apologize if this is a simple problem but this has been puzzling me for awhile now.","\Sigma = \sigma.\rho.\sigma^T \Sigma \sigma \rho \sigma \Sigma = \begin{bmatrix}\sigma_1 & \sigma_2\end{bmatrix} . \begin{bmatrix}\rho_{1,1} & \rho_{1,2}\\\rho_{2,1} & \rho_{2,2}\end{bmatrix} .\begin{bmatrix}\sigma_1\\\sigma_2\end{bmatrix} = \begin{bmatrix}\rho_{1,1}\sigma_1\sigma_1 & \rho_{1,2}\sigma_1\sigma_2\\\rho_{2,1}\sigma_2\sigma_1 & \rho_{2,2}\sigma_2\sigma_2\end{bmatrix}","['matrices', 'matrix-equations']"
13,Why is $GSp(4)$ an interesting group?,Why is  an interesting group?,GSp(4),"In various settings (geometry, automorphic forms) the symplectic group does appear. In my mind, it may be realized formally as the group of matrices $$GSp(4) = \left\{ g \in GL(4) \ : \ g^T J g = \lambda_g J, \lambda_g \in GL(1) \right\}$$ where $J$ is the block matrix $$J =  \begin{pmatrix} 0 & -I_2 \\ I_2 & 0 \end{pmatrix}$$ I understand this is a group and this may be interesting in this respect. But why does it seem so important? I understand it is one of the main types of reductive groups, but is there an intuition behind it? Can we understand geometrically why it is so important? why it is also so genuinely different from $GL(n)$ ? Any insight, reference, examples, etc. will help.","In various settings (geometry, automorphic forms) the symplectic group does appear. In my mind, it may be realized formally as the group of matrices where is the block matrix I understand this is a group and this may be interesting in this respect. But why does it seem so important? I understand it is one of the main types of reductive groups, but is there an intuition behind it? Can we understand geometrically why it is so important? why it is also so genuinely different from ? Any insight, reference, examples, etc. will help.","GSp(4) = \left\{ g \in GL(4) \ : \ g^T J g = \lambda_g J, \lambda_g \in GL(1) \right\} J J = 
\begin{pmatrix}
0 & -I_2 \\
I_2 & 0
\end{pmatrix} GL(n)","['matrices', 'group-theory', 'number-theory', 'symplectic-geometry', 'automorphic-forms']"
14,Can a matrix be orthogonal without being orthonormal? [duplicate],Can a matrix be orthogonal without being orthonormal? [duplicate],,"This question already has answers here : orthogonal vs orthonormal matrices - what are simplest possible definitions and examples of each ?? (1 answer) Orthogonal and Orthonormal Matrix (1 answer) Difference between orthogonal and orthonormal matrices (2 answers) Closed 8 months ago . I know a set of orthogonal vectors is a set where vectors have a null dot product pairwise, whatever their norms, and a set of orthonormal vectors is a set of orthogonal vectors where all vectors have a norm equal to 1. Now I'm trying to dig into SVD decomposition $\small X=U \Sigma V^T$ introduced by San José University and read about U and V: there exist two orthogonal matrices So I'm trying to understand what are orthogonal matrices. Wikipedia says: In linear algebra, an orthogonal matrix, or orthonormal matrix, is a real square matrix whose columns and rows are orthonormal vectors So Wikipedia implies orthogonal matrix is synonymous for orthonormal matrix , and a matrix cannot be orthogonal if a row or a column is a vector with a norm different of 1. This site says: For matrices, an orthogonal matrix has orthogonal rows and columns. This means that the dot product of any two rows or columns is zero An orthonormal matrix, on the other hand, not only has orthogonal rows and columns but also has orthonormal rows and columns. This means that each row and column is a unit vector On the other hand this answer here states: There is no thing as an ""orthonormal"" matrix I'm confused about the difference between orthogonal and orthonormal matrices. Can this be clarified: Is there a definition for orthogonal matrix and/or orthonormal matrix ? Can a matrix be orthogonal without being orthonormal? Does that depend on whether the matrix is square or not? Please focus on these three questions, not adding further confusion. Related, but not helpful: Difference between orthogonal and orthonormal matrices","This question already has answers here : orthogonal vs orthonormal matrices - what are simplest possible definitions and examples of each ?? (1 answer) Orthogonal and Orthonormal Matrix (1 answer) Difference between orthogonal and orthonormal matrices (2 answers) Closed 8 months ago . I know a set of orthogonal vectors is a set where vectors have a null dot product pairwise, whatever their norms, and a set of orthonormal vectors is a set of orthogonal vectors where all vectors have a norm equal to 1. Now I'm trying to dig into SVD decomposition introduced by San José University and read about U and V: there exist two orthogonal matrices So I'm trying to understand what are orthogonal matrices. Wikipedia says: In linear algebra, an orthogonal matrix, or orthonormal matrix, is a real square matrix whose columns and rows are orthonormal vectors So Wikipedia implies orthogonal matrix is synonymous for orthonormal matrix , and a matrix cannot be orthogonal if a row or a column is a vector with a norm different of 1. This site says: For matrices, an orthogonal matrix has orthogonal rows and columns. This means that the dot product of any two rows or columns is zero An orthonormal matrix, on the other hand, not only has orthogonal rows and columns but also has orthonormal rows and columns. This means that each row and column is a unit vector On the other hand this answer here states: There is no thing as an ""orthonormal"" matrix I'm confused about the difference between orthogonal and orthonormal matrices. Can this be clarified: Is there a definition for orthogonal matrix and/or orthonormal matrix ? Can a matrix be orthogonal without being orthonormal? Does that depend on whether the matrix is square or not? Please focus on these three questions, not adding further confusion. Related, but not helpful: Difference between orthogonal and orthonormal matrices",\small X=U \Sigma V^T,"['linear-algebra', 'matrices', 'definition']"
15,Connected components of real matrices such that $M^2=I$,Connected components of real matrices such that,M^2=I,"Suppose that $S$ is the set of all real $n\times n$ matrices $A$ such that $A^2=I_n$ . Since these matrices are diagonalizable with $\pm 1$ as eigenvalues, we get the partition $S=\cup_{0\leq k \leq n} S_k$ where $S_k$ is the set of all matrices of the form $PJ_k P^{-1}$ where $P$ is a real invertible matrix and $J_k$ the diagonal $(1,\cdots,1,-1,\cdots,-1)$ (the first $k$ components are equal to $1$ , and the other $n-k$ components are equal to $-1$ ). My question is : are the $S_k$ the connected components of $S$ ? (and if yes, why ?) If not, what are the connected components of $S$ ?","Suppose that is the set of all real matrices such that . Since these matrices are diagonalizable with as eigenvalues, we get the partition where is the set of all matrices of the form where is a real invertible matrix and the diagonal (the first components are equal to , and the other components are equal to ). My question is : are the the connected components of ? (and if yes, why ?) If not, what are the connected components of ?","S n\times n A A^2=I_n \pm 1 S=\cup_{0\leq k \leq n} S_k S_k PJ_k P^{-1} P J_k (1,\cdots,1,-1,\cdots,-1) k 1 n-k -1 S_k S S","['linear-algebra', 'general-topology', 'matrices']"
16,How can I calculate $\mathbf{X}\mathbf{X}^{\rm T} \mathbf{v}$ in the most effective way without storing $\mathbf{X}\mathbf{X}^{\rm T}$,How can I calculate  in the most effective way without storing,\mathbf{X}\mathbf{X}^{\rm T} \mathbf{v} \mathbf{X}\mathbf{X}^{\rm T},"I have a matrix $\mathbf{X} \in \mathbb{R}^{n \times m}$ and a vector $\mathbf{v} \in \mathbb{R}^n$ with $n \gg m$ . I would like to compute $\mathbf{X}\mathbf{X}^{\rm T} \mathbf{v}$ in the most effective way, but I cannot store $\mathbf{X} \mathbf{X}^{\rm T}$ . Do you have an idea how to perform these multiplications as fast as possible? Of course, some partial results (like some rows of $\mathbf{X} \mathbf{X}^{\rm T}$ ) can be temporarily stored.","I have a matrix and a vector with . I would like to compute in the most effective way, but I cannot store . Do you have an idea how to perform these multiplications as fast as possible? Of course, some partial results (like some rows of ) can be temporarily stored.",\mathbf{X} \in \mathbb{R}^{n \times m} \mathbf{v} \in \mathbb{R}^n n \gg m \mathbf{X}\mathbf{X}^{\rm T} \mathbf{v} \mathbf{X} \mathbf{X}^{\rm T} \mathbf{X} \mathbf{X}^{\rm T},"['linear-algebra', 'matrices', 'linear-transformations']"
17,Eigenvectors of any arbitrary matrix is same as its adjoint.,Eigenvectors of any arbitrary matrix is same as its adjoint.,,"Recently I came across Normal matrices and their properties, one of which states that their eigenvectors are the same as their adjoint and are orthogonal. I've gone across some proofs and I understand it but when I tried to prove the same using the inner product, It somehow states that the above is true for any arbitrary matrix. Can someone possibly help me point out where I'm going wrong? Let's say A is a matrix and B is its adjoint. If x is an eigenvector of A with eigenvalue k, then, < x | A | x > = < x | k x > = k < x | x > also < x | A | x > = < Bx | x > hence < B x | x > = k < x | x > =  < kx | x > So, x is also an eigenvector of adjoint of A.","Recently I came across Normal matrices and their properties, one of which states that their eigenvectors are the same as their adjoint and are orthogonal. I've gone across some proofs and I understand it but when I tried to prove the same using the inner product, It somehow states that the above is true for any arbitrary matrix. Can someone possibly help me point out where I'm going wrong? Let's say A is a matrix and B is its adjoint. If x is an eigenvector of A with eigenvalue k, then, < x | A | x > = < x | k x > = k < x | x > also < x | A | x > = < Bx | x > hence < B x | x > = k < x | x > =  < kx | x > So, x is also an eigenvector of adjoint of A.",,"['matrices', 'eigenvalues-eigenvectors', 'inner-products', 'adjoint-operators']"
18,Standard Matrix,Standard Matrix,,"During an exam, we were asked to determine the standard matrix of the linear image, $P: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ which projects a vector onto the $y=-x$ axis. What I did is, I took the unit vector in the direction of the $y=-x$ axis, which is $\begin{bmatrix} 1 \\ -1 \end{bmatrix}$ . Then I used the general formula for vector projection, using an arbitrary vector $\vec{x}$ , with entries $x_1$ and $x_2$ . In the answers model, they give that: $ \begin{bmatrix} 1 & 0 \\ 0 & -1  \end{bmatrix}$ A standard matrix is for projection. So is what I did unnecessarily difficult, or just wrong? In fact, I came up with: $ \begin{bmatrix} \frac{1}{2} & -\frac{1}{2} \\ -\frac{1}{2} & \frac{1}{2}  \end{bmatrix}$ After all, I now don't see how my answer matches theirs, if it were right, which I would think? What else is wrong with my approach?","During an exam, we were asked to determine the standard matrix of the linear image, which projects a vector onto the axis. What I did is, I took the unit vector in the direction of the axis, which is . Then I used the general formula for vector projection, using an arbitrary vector , with entries and . In the answers model, they give that: A standard matrix is for projection. So is what I did unnecessarily difficult, or just wrong? In fact, I came up with: After all, I now don't see how my answer matches theirs, if it were right, which I would think? What else is wrong with my approach?","P: \mathbb{R}^2 \rightarrow \mathbb{R}^2 y=-x y=-x \begin{bmatrix} 1 \\ -1 \end{bmatrix} \vec{x} x_1 x_2  \begin{bmatrix}
1 & 0 \\
0 & -1 
\end{bmatrix}  \begin{bmatrix}
\frac{1}{2} & -\frac{1}{2} \\
-\frac{1}{2} & \frac{1}{2} 
\end{bmatrix}","['linear-algebra', 'matrices', 'linear-transformations', 'projection', 'projection-matrices']"
19,Extinction of non-dominant species in generalized competitive Lotka-Volterra systems,Extinction of non-dominant species in generalized competitive Lotka-Volterra systems,,"I am studying the generalized $n$ -species competitive Lotka-Volterra system where populations of species $i$ are defined by the standard differential equation: $$ \dot x_i = f_i(\mathbf{x}) := x_i \left( 1 - \sum_j a_{ij}x_j \right) $$ where all $a_{ij} \geq 0$ and $a_{ii}=1$ . I know that any asymptotic behavior can be observed in general. However, I am wondering if there exist some constraints on the competition (or interaction) matrix $\mathbf{A}$ — with elements $a_{ij}$ — such that only a single species survives. More formally, do there exist characteristics of $\mathbf{A}$ such that there is only one fixed point $\mathbf{x}^\star_k$ wherein $x_k = 1$ and $\forall j\neq k, x_j=0$ ? I have already observed that if $\mathbf{A}$ is a lower triangular matrix with all elements $1$ we obtain that only the dominant species $x_1$ will survive. We can also always reorder/relabel any system such that the species with index $1$ is the dominant one. It will the the only one to survive as we have in the fixed point $\mathbf{x}^\star$ : $$ f_1(\mathbf{x}^\star) = x_1^\star(1-x_1^\star) = 0, \;\text{thus}\; x_1^\star = 1, $$ as we are interested in systems with $x_i(0) > 0$ . This is just logistic growth of species $1$ . For subsequent species $x_2$ we obtain $$ f_2(\mathbf{x}^\star) = x_2^\star(1-x_2^\star-x_1^\star) = 0, \;\text{thus}\; x_2^\star = 0, $$ and the same holds for any other $i>2$ . Is this the only way that we can make only a single (dominant) species survive if all initial abundances are positive, $x_i>0$ ? Or are there perhaps more constraints we can place on the competition matrix $\mathbf{A}$ (or its elements $a_{ij}$ ) that ensure only a single species survives in the end?","I am studying the generalized -species competitive Lotka-Volterra system where populations of species are defined by the standard differential equation: where all and . I know that any asymptotic behavior can be observed in general. However, I am wondering if there exist some constraints on the competition (or interaction) matrix — with elements — such that only a single species survives. More formally, do there exist characteristics of such that there is only one fixed point wherein and ? I have already observed that if is a lower triangular matrix with all elements we obtain that only the dominant species will survive. We can also always reorder/relabel any system such that the species with index is the dominant one. It will the the only one to survive as we have in the fixed point : as we are interested in systems with . This is just logistic growth of species . For subsequent species we obtain and the same holds for any other . Is this the only way that we can make only a single (dominant) species survive if all initial abundances are positive, ? Or are there perhaps more constraints we can place on the competition matrix (or its elements ) that ensure only a single species survives in the end?","n i  \dot x_i = f_i(\mathbf{x}) := x_i \left( 1 - \sum_j a_{ij}x_j \right)  a_{ij} \geq 0 a_{ii}=1 \mathbf{A} a_{ij} \mathbf{A} \mathbf{x}^\star_k x_k = 1 \forall j\neq k, x_j=0 \mathbf{A} 1 x_1 1 \mathbf{x}^\star 
f_1(\mathbf{x}^\star) = x_1^\star(1-x_1^\star) = 0, \;\text{thus}\; x_1^\star = 1,
 x_i(0) > 0 1 x_2 
f_2(\mathbf{x}^\star) = x_2^\star(1-x_2^\star-x_1^\star) = 0, \;\text{thus}\; x_2^\star = 0,
 i>2 x_i>0 \mathbf{A} a_{ij}","['matrices', 'ordinary-differential-equations', 'constraints', 'population-dynamics']"
20,Is $ \left\{ A P + P A^{T} \mid P \succ 0 \right\} $ open?,Is  open?, \left\{ A P + P A^{T} \mid P \succ 0 \right\} ,"Consider an $n$ by $n$ matrix $A$ , and the set $$ \left\{ A P + P A^{T} \mid P \succ 0 \right\} $$ Is this set open under the standard metric (the one induced by Frobenius norm) in the space of all symmetric matrices $S^n$ ? I think this is true if and only if $A$ is invertible. I think if we assume $A$ is invertible, very roughly speaking, a small perturbation $\epsilon$ in the value of $AP +PA^{T}$ can be obtained by $AP$ and $PA^{T}$ each perturbate at most $\epsilon$ amount. Since $A$ is invertible, $P$ can also perturbate small enough with the choice of $\epsilon$ . Since the eigenvalues change continuously, we can change a tiny amount so that all eigenvalues of $P$ remain positive. I am not sure, however, if $A$ being invertible is required for the set to be open, and I'm also having a little trouble making my argument precise. Also, in general, for $$\left\{\begin{bmatrix} A_{1}P+PA_{1}^{T} \\ \vdots \\ A_{m}P + P A_{m}^{T} \end{bmatrix} : P \succ 0\right\}$$ Under the standard metric, i.e., sum the difference between each entry square root. When is this open in the space of vectors of m symmetric matrices? I think if each $A_{i}P+PA_{i}^{T}$ is open, in the product space equipped with this standard product metric, the set is open as well. However, I'm not sure about this or whether there exists open sets such that not every A is invertible.","Consider an by matrix , and the set Is this set open under the standard metric (the one induced by Frobenius norm) in the space of all symmetric matrices ? I think this is true if and only if is invertible. I think if we assume is invertible, very roughly speaking, a small perturbation in the value of can be obtained by and each perturbate at most amount. Since is invertible, can also perturbate small enough with the choice of . Since the eigenvalues change continuously, we can change a tiny amount so that all eigenvalues of remain positive. I am not sure, however, if being invertible is required for the set to be open, and I'm also having a little trouble making my argument precise. Also, in general, for Under the standard metric, i.e., sum the difference between each entry square root. When is this open in the space of vectors of m symmetric matrices? I think if each is open, in the product space equipped with this standard product metric, the set is open as well. However, I'm not sure about this or whether there exists open sets such that not every A is invertible.","n n A  \left\{ A P + P A^{T} \mid P \succ 0 \right\}  S^n A A \epsilon AP +PA^{T} AP PA^{T} \epsilon A P \epsilon P A \left\{\begin{bmatrix}
A_{1}P+PA_{1}^{T} \\
\vdots \\
A_{m}P + P A_{m}^{T}
\end{bmatrix} : P \succ 0\right\} A_{i}P+PA_{i}^{T}","['linear-algebra', 'matrices', 'control-theory', 'positive-semidefinite', 'semidefinite-programming']"
21,Show group defined via quotient of matrix $A$'s columns has order $|\det A|$.,Show group defined via quotient of matrix 's columns has order .,A |\det A|,"The problem: Let $M \leq \mathbb{Z}^n$ be the subgroup generated by the rows of an $n \times n$ matrix $A$ with entries in $\mathbb{Z}$ . Show that $G=\mathbb{Z}^n/M$ is finite if and only if $\det A \neq 0$ , and in that case the order of $G$ is $|\det A|$ . Let's label the rows of $A$ by $v_1, ..., v_n \in \mathbb{Z}^n$ . The if and only if (solution verification) Firstly, I think I understand how to show the if and only if. Here's what I did, if there are any mistakes please correct me. If $\det A = 0$ , then there will be some $v \in \mathbb{Z}$ such that $v \notin \text{span} \left(v_1, ..., v_n \right)$ . Then $v+M, 2v+M, 3v+M, ...$ are all distinct elements of $G$ . Conversely, if $\det A \neq 0$ , we can always write any $v$ as some linear combination $v=\sum_{i=1}^n q_i v_i$ where $q_i \in \mathbb{Q}$ . $q_i$ will have denominator at most the lowest common multiple of the integers in $v_i$ , say $b_i$ , otherwise we'll not get an integer. Then we can write any element of $G$ as $v+M=\sum_{i=1}^n \{q_i\} v_i$ where $q_i \in \mathbb{Q}$ where $\{ \cdot \}$ denotes the fractional part. Thus there's at most $\prod_{i=1}^{n} b_i < \infty$ elements in $G$ . $|G|=|\det A|$ (help) I'm stuck on this. I don't really get how I can relate anything I've done in the first part of this to the determinant, the use of the lowest common multiple or the product $\prod_{i=1}^{n} b_i$ doesn't seem to lend too easily into thinking about determinants, but perhaps I'm missing something. I'm thinking we could perhaps try to show that the group is isomorphic to the same group with $A$ in Smith Normal Form, which would then lead nicely into thinking of the determinant, since it'll be a diagonal matrix, but I'm not really sure how to approach that.","The problem: Let be the subgroup generated by the rows of an matrix with entries in . Show that is finite if and only if , and in that case the order of is . Let's label the rows of by . The if and only if (solution verification) Firstly, I think I understand how to show the if and only if. Here's what I did, if there are any mistakes please correct me. If , then there will be some such that . Then are all distinct elements of . Conversely, if , we can always write any as some linear combination where . will have denominator at most the lowest common multiple of the integers in , say , otherwise we'll not get an integer. Then we can write any element of as where where denotes the fractional part. Thus there's at most elements in . (help) I'm stuck on this. I don't really get how I can relate anything I've done in the first part of this to the determinant, the use of the lowest common multiple or the product doesn't seem to lend too easily into thinking about determinants, but perhaps I'm missing something. I'm thinking we could perhaps try to show that the group is isomorphic to the same group with in Smith Normal Form, which would then lead nicely into thinking of the determinant, since it'll be a diagonal matrix, but I'm not really sure how to approach that.","M \leq \mathbb{Z}^n n \times n A \mathbb{Z} G=\mathbb{Z}^n/M \det A \neq 0 G |\det A| A v_1, ..., v_n \in \mathbb{Z}^n \det A = 0 v \in \mathbb{Z} v \notin \text{span} \left(v_1, ..., v_n \right) v+M, 2v+M, 3v+M, ... G \det A \neq 0 v v=\sum_{i=1}^n q_i v_i q_i \in \mathbb{Q} q_i v_i b_i G v+M=\sum_{i=1}^n \{q_i\} v_i q_i \in \mathbb{Q} \{ \cdot \} \prod_{i=1}^{n} b_i < \infty G |G|=|\det A| \prod_{i=1}^{n} b_i A","['linear-algebra', 'matrices', 'group-theory', 'determinant', 'quotient-group']"
22,Why can we use the identity matrix when defining the characteristic polynomial?,Why can we use the identity matrix when defining the characteristic polynomial?,,"We begin with $$\lambda v = Tv$$ where $\lambda$ is an eigenvalue, $v$ is an eigenvector, and $T$ is the transformation in question. We state $$\lambda v - Tv = 0$$ We then must state $$(\lambda I - T)v = 0$$ What allows us to bring in $I$ , the identity matrix? I understand that subtraction of a linear transformation (or matrix) from a scalar is ill-defined, but what permits us to multiply $\lambda$ by $I$ ? Would a more rigorous evaluation be \begin{align*} \lambda v - Tv &= \lambda Iv - Tv \\ &= (\lambda I - T)v \end{align*} where the identity matrix provides a multiplicative identity of some sorts? I understand that $I \in M_{n \times n}(\mathbb{F})$ , so I am wondering how it can be the identity for $v \in M_{n \times 1}(\mathbb{F})$ ? The reason I ask is that I am proving Cayley-Hamilton for a diagonal matrix, and I have come upon the following: \begin{align*} \chi(t)  	&= det(D-tI) \\  	&= \prod_{i=1}^{n}d_{ii}-t \\ 	&= (d_{11} - t)(d_{22} - t) \cdots (d_{nn} - t) \end{align*} Consider \begin{align*} \chi([D]) &= (d_{11}I - D)(d_{22}I - D) \cdots (d_{nn}I - D) \end{align*} Where does the $I$ come from? The underlying question is really this: I understand that $I \in M_{n \times n}(\mathbb{F})$ , so I am wondering how it can be the identity for $v \in M_{n \times 1}(\mathbb{F})$ ?","We begin with where is an eigenvalue, is an eigenvector, and is the transformation in question. We state We then must state What allows us to bring in , the identity matrix? I understand that subtraction of a linear transformation (or matrix) from a scalar is ill-defined, but what permits us to multiply by ? Would a more rigorous evaluation be where the identity matrix provides a multiplicative identity of some sorts? I understand that , so I am wondering how it can be the identity for ? The reason I ask is that I am proving Cayley-Hamilton for a diagonal matrix, and I have come upon the following: Consider Where does the come from? The underlying question is really this: I understand that , so I am wondering how it can be the identity for ?","\lambda v = Tv \lambda v T \lambda v - Tv = 0 (\lambda I - T)v = 0 I \lambda I \begin{align*}
\lambda v - Tv &= \lambda Iv - Tv \\ &= (\lambda I - T)v
\end{align*} I \in M_{n \times n}(\mathbb{F}) v \in M_{n \times 1}(\mathbb{F}) \begin{align*}
\chi(t) 
	&= det(D-tI) \\ 
	&= \prod_{i=1}^{n}d_{ii}-t \\
	&= (d_{11} - t)(d_{22} - t) \cdots (d_{nn} - t)
\end{align*} \begin{align*}
\chi([D]) &= (d_{11}I - D)(d_{22}I - D) \cdots (d_{nn}I - D)
\end{align*} I I \in M_{n \times n}(\mathbb{F}) v \in M_{n \times 1}(\mathbb{F})","['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors', 'linear-transformations']"
23,Let $T\colon V\to V$ over the IPS $V$ and $B$ be an orthogonal basis for $V$. Find the simplest connexion between $[T^*]_B$ and $([T]_B)^*$,Let  over the IPS  and  be an orthogonal basis for . Find the simplest connexion between  and,T\colon V\to V V B V [T^*]_B ([T]_B)^*,"Let $T\colon V\to V$ over an Inner Product Space $V$ . Let $B$ be an orthogonal basis for $V$ . Find the simplest connexion between $[T^*]_B$ and $([T]_B)^*$ . So I know that if $B$ was an orthonormal basis for $V$ we would have received: $$[T^*]_B=\langle T^*{b_i},b_j\rangle=\operatorname{adj}\langle T{b_i},b_j\rangle=\operatorname{adj}([T]_B)_{ji}$$ and therefore $\,[T^*]_B=([T]_B)^*$ . I am not sure how to find this relation using $B$ as an orthogonal basis. Do I just normalise the vectors?",Let over an Inner Product Space . Let be an orthogonal basis for . Find the simplest connexion between and . So I know that if was an orthonormal basis for we would have received: and therefore . I am not sure how to find this relation using as an orthogonal basis. Do I just normalise the vectors?,"T\colon V\to V V B V [T^*]_B ([T]_B)^* B V [T^*]_B=\langle T^*{b_i},b_j\rangle=\operatorname{adj}\langle T{b_i},b_j\rangle=\operatorname{adj}([T]_B)_{ji} \,[T^*]_B=([T]_B)^* B","['linear-algebra', 'matrices', 'inner-products', 'orthogonality', 'orthonormal']"
24,Unitary matrix corruption,Unitary matrix corruption,,Suppose I have got a unitary matrix and I want to introduce random noise to simulate data corruption. How to introduce the noise in a proper way such that the corrupted matrix is also unitary?,Suppose I have got a unitary matrix and I want to introduce random noise to simulate data corruption. How to introduce the noise in a proper way such that the corrupted matrix is also unitary?,,"['matrices', 'unitary-matrices', 'noise']"
25,Element-wise product of a matrix by a vector,Element-wise product of a matrix by a vector,,"Suppose I have an $m \times n$ matrix $\mathbf{A}$ , and a column $m$ -vector $\mathbf{v}$ . Let $\mathbf{B}$ represents the matrix resulted from the element-wise multiplication of each column of $\mathbf{A}$ with the vector $\mathbf{v}$ . For example, $$\mathbf{A} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32} \\ \end{bmatrix}, \qquad \mathbf{v} = \begin{bmatrix} v_{1} \\ v_{2} \\ v_{3} \\ \end{bmatrix}, \qquad \mathbf{B} = \begin{bmatrix} a_{11} v_{1} & a_{12} v_{1} \\ a_{21} v_{2} & a_{22} v_{2} \\ a_{31} v_{3} & a_{32} v_{3}  \end{bmatrix}$$ What do we call this kind of multiplication? Is it a Hadamard product?","Suppose I have an matrix , and a column -vector . Let represents the matrix resulted from the element-wise multiplication of each column of with the vector . For example, What do we call this kind of multiplication? Is it a Hadamard product?","m \times n \mathbf{A} m \mathbf{v} \mathbf{B} \mathbf{A} \mathbf{v} \mathbf{A} = \begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
a_{31} & a_{32} \\
\end{bmatrix}, \qquad
\mathbf{v} = \begin{bmatrix}
v_{1} \\
v_{2} \\
v_{3} \\
\end{bmatrix}, \qquad
\mathbf{B} = \begin{bmatrix}
a_{11} v_{1} & a_{12} v_{1} \\
a_{21} v_{2} & a_{22} v_{2} \\
a_{31} v_{3} & a_{32} v_{3} 
\end{bmatrix}","['matrices', 'terminology']"
26,Spectral radius of symmetric matrix with negative entries multiplied by a diagonal matrix,Spectral radius of symmetric matrix with negative entries multiplied by a diagonal matrix,,"Let $M$ be a irreducible, symmetric matrix with some negative entries such that $M^k>0$ for some $k>k_o$ and $\sum_j m_{ij}=1$ with $m_{ij} \in \Re$ , and spectral radius $\rho(M)=1$ . After multiplying it with a diagonal matrix $D={\rm diag}(d_{ii})$ where $0<d_{ii}\leq 1$ with at least one $d_{ii}<1$ . Is there an easy way to show $\rho(DM)<1$ ? I know that this result is true as stated. Similar questions have been asked for matrices $M$ but only for nonnegative entries, e.g., Substochastic matrix spectral radius .","Let be a irreducible, symmetric matrix with some negative entries such that for some and with , and spectral radius . After multiplying it with a diagonal matrix where with at least one . Is there an easy way to show ? I know that this result is true as stated. Similar questions have been asked for matrices but only for nonnegative entries, e.g., Substochastic matrix spectral radius .",M M^k>0 k>k_o \sum_j m_{ij}=1 m_{ij} \in \Re \rho(M)=1 D={\rm diag}(d_{ii}) 0<d_{ii}\leq 1 d_{ii}<1 \rho(DM)<1 M,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'spectral-radius']"
27,Bounding error of approximating $(I-A)^t$ with $\exp(-At)$ for large $t$,Bounding error of approximating  with  for large,(I-A)^t \exp(-At) t,"Suppose $A$ is a diagonal matrix with diagonal entries $\in (0,1]$ I'm interested in bounding the following quantity in terms of $t$ : $$f_A(t)=\operatorname{Tr}\exp(-At)-\operatorname{Tr}(I-A)^t$$ Suppose $\operatorname{Tr}(A^2)>1$ and $f_A(t)$ is a decreasing function of $t$ , what is a good bound in this regime? Example, plotting $f_A(t)$ for diagonal $A$ with diagonal values $1,\frac{1}{2},\frac{1}{3},\ldots,\frac{1}{100}$ Notebook Motivation: see previous question solved with a bound which is tight for small values of $\operatorname{Tr}(A^2)$","Suppose is a diagonal matrix with diagonal entries I'm interested in bounding the following quantity in terms of : Suppose and is a decreasing function of , what is a good bound in this regime? Example, plotting for diagonal with diagonal values Notebook Motivation: see previous question solved with a bound which is tight for small values of","A \in (0,1] t f_A(t)=\operatorname{Tr}\exp(-At)-\operatorname{Tr}(I-A)^t \operatorname{Tr}(A^2)>1 f_A(t) t f_A(t) A 1,\frac{1}{2},\frac{1}{3},\ldots,\frac{1}{100} \operatorname{Tr}(A^2)","['linear-algebra', 'matrices', 'upper-lower-bounds', 'matrix-exponential']"
28,$A$ is a square matrix of order $2$ with real entries and ${\rm Tr}(A)+|A|=2$. Show that $|A^2+|A|\cdot A+{\rm Tr}(A)I_2|\geq 4$,is a square matrix of order  with real entries and . Show that,A 2 {\rm Tr}(A)+|A|=2 |A^2+|A|\cdot A+{\rm Tr}(A)I_2|\geq 4,$A$ is a square matrix of order $2$ with real entries and ${\rm Tr}(A)+|A|=2$ . Show that $$|A^2+|A|\cdot A+{\rm Tr}(A)I_2|\geq 4$$ My Attempt I could observe that ${\rm Tr}(A)+|A|=2\Rightarrow 1+a+d+ad-bc=3 \Rightarrow|A+I|=3$ By Cayley-Hamilton theorem for a $2\times 2$ matrix we have $A^2-{\rm Tr}(A)\cdot A+|A|\cdot I_2=0$ . $A^2={\rm Tr}(A)\cdot A-|A|\cdot I_2$ So $A^2+|A|\cdot A+{\rm Tr}(A)I_2={\rm Tr}(A)\cdot A-|A|\cdot I_2+|A|\cdot A+{\rm Tr}(A)I_2$ $=({\rm Tr}(A)+|A|)A+({\rm Tr}(A)-|A|)I_2$ $=2A+({\rm Tr}(A)-|A|)I_2=2A+2(1-|A|)I_2$ $|A^2+|A|\cdot A+{\rm Tr}(A)I_2|=4|A+(1-|A|)I_2|$ Not able to proceed from here. I am wondering whether Cayley Hamilton was the right approach or not.,is a square matrix of order with real entries and . Show that My Attempt I could observe that By Cayley-Hamilton theorem for a matrix we have . So Not able to proceed from here. I am wondering whether Cayley Hamilton was the right approach or not.,A 2 {\rm Tr}(A)+|A|=2 |A^2+|A|\cdot A+{\rm Tr}(A)I_2|\geq 4 {\rm Tr}(A)+|A|=2\Rightarrow 1+a+d+ad-bc=3 \Rightarrow|A+I|=3 2\times 2 A^2-{\rm Tr}(A)\cdot A+|A|\cdot I_2=0 A^2={\rm Tr}(A)\cdot A-|A|\cdot I_2 A^2+|A|\cdot A+{\rm Tr}(A)I_2={\rm Tr}(A)\cdot A-|A|\cdot I_2+|A|\cdot A+{\rm Tr}(A)I_2 =({\rm Tr}(A)+|A|)A+({\rm Tr}(A)-|A|)I_2 =2A+({\rm Tr}(A)-|A|)I_2=2A+2(1-|A|)I_2 |A^2+|A|\cdot A+{\rm Tr}(A)I_2|=4|A+(1-|A|)I_2|,"['linear-algebra', 'matrices', 'determinant', 'trace', 'cayley-hamilton']"
29,Upper bound for the norm of a matrix by using an upper bound on the entries,Upper bound for the norm of a matrix by using an upper bound on the entries,,"Let $F:\mathbb{R}^3\setminus\{0\}\to\mathbb{R}^3$ be a function of class $C^1$ such that $$(x_1, x_2, x_3)\in \mathbb{R}^3\setminus\{0\}\mapsto F_i(x_1, x_2, x_3)\in\mathbb{R}, \quad \forall i\in\{1, 2, 3\}.$$ As an exercise, I have to find an upper bound (better if as tight as possible) for $$\left\Vert\left(\frac{\partial F_i}{\partial x_j}(y)\right)_{i, j=1, 2, 3}\right\Vert  $$ where the above notation refers to the norm of the matrix $\left(\frac{\partial F_i}{\partial x_j}(y)\right)_{i, j=1, 2, 3}$ as it is defined in https://en.wikipedia.org/wiki/Matrix_norm (the norm defined through the supremum). $\textbf{EDIT:}$ By following the idea given in the answer by @kieransquared, I checked again all the computations. The only information I have is that $$ \frac{\partial F_i}{\partial x_j}(y) \le\begin{cases} \displaystyle\frac{1}{\alpha^{\beta +1}} -\frac{y_i^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i=j\\[10pt] \displaystyle -\frac{y_j^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i\neq j, \end{cases}$$ where $a, \beta\ge 1$ , $\alpha,\gamma >0$ constants and $|x|$ denotes the euclidean norm of the vector $(x_1, x_2, x_3)$ . Hence, by using the triangle inequality, it follows that $$ \left\vert\frac{\partial F_i}{\partial x_j}(y)\right\vert \le\begin{cases} \displaystyle\frac{1}{\alpha^{\beta +1}} +\frac{y_i^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i=j\\[10pt] \displaystyle \frac{y_j^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i\neq j, \end{cases}$$ Anyway, I can not deduce the desired upper bound from that information. I hope someone could help. Thank you.","Let be a function of class such that As an exercise, I have to find an upper bound (better if as tight as possible) for where the above notation refers to the norm of the matrix as it is defined in https://en.wikipedia.org/wiki/Matrix_norm (the norm defined through the supremum). By following the idea given in the answer by @kieransquared, I checked again all the computations. The only information I have is that where , constants and denotes the euclidean norm of the vector . Hence, by using the triangle inequality, it follows that Anyway, I can not deduce the desired upper bound from that information. I hope someone could help. Thank you.","F:\mathbb{R}^3\setminus\{0\}\to\mathbb{R}^3 C^1 (x_1, x_2, x_3)\in \mathbb{R}^3\setminus\{0\}\mapsto F_i(x_1, x_2, x_3)\in\mathbb{R}, \quad \forall i\in\{1, 2, 3\}. \left\Vert\left(\frac{\partial F_i}{\partial x_j}(y)\right)_{i, j=1, 2, 3}\right\Vert   \left(\frac{\partial F_i}{\partial x_j}(y)\right)_{i, j=1, 2, 3} \textbf{EDIT:}  \frac{\partial F_i}{\partial x_j}(y) \le\begin{cases}
\displaystyle\frac{1}{\alpha^{\beta +1}} -\frac{y_i^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i=j\\[10pt]
\displaystyle -\frac{y_j^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i\neq j,
\end{cases} a, \beta\ge 1 \alpha,\gamma >0 |x| (x_1, x_2, x_3)  \left\vert\frac{\partial F_i}{\partial x_j}(y)\right\vert \le\begin{cases}
\displaystyle\frac{1}{\alpha^{\beta +1}} +\frac{y_i^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i=j\\[10pt]
\displaystyle \frac{y_j^2 (\beta +1)}{\gamma^{\beta +3}} &\hbox{ if } i\neq j,
\end{cases}","['real-analysis', 'calculus', 'matrices', 'upper-lower-bounds', 'matrix-norms']"
30,System of linear equations with a free variable.,System of linear equations with a free variable.,,"The following matrix $A_n$ is an $(n-1) \times n$ matrix. I need to solve $A_nX = 0$ , where $X = (x_1, \cdots, x_n)$ . \begin{equation} A_n = \begin{bmatrix} b_2 & a_2 & b_3 & 0 & 0 & 0 & 0 & \cdots & 0 & 0 \\ 0 & b_3 & a_3 & b_4 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & b_4 & a_4 & b_5 & 0 & 0 & \cdots  & 0 & 0\\ 0 & 0 & 0 & b_5 & a_5 & b_6 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & b_6 & a_6 & b_7 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \ddots & \ddots & \ddots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \ddots & \ddots & \ddots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \ddots & \ddots & b_{n-1} & a_{n-1} & b_n\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & b_n & a_n\\ \end{bmatrix} \end{equation} Since $$ -\dfrac{b_n}{a_n}x_{n-1} = x_n, $$ so $x_n$ is a free variable and thus there is always a solution to this system of equations. Let us denote $x_m \sim x_j$ if we can write $x_j$ in terms of $x_m$ . Therefore $x_{n-1} \sim x_n$ . By substituting $x_n$ by $x_{n-1}$ in $(n-1)$ th row, it implies $x_{n-2} \sim x_{n-1}$ . Therefore we will end up with $x_1 \sim x_2$ . My problem is to find the formula, say $\phi$ , so that $x_2 = \phi x_1$ . Now I need a hint to propose a method that can help to solve this problem. Cramer's rule seems doesn't work because the right hand side is zero vector.","The following matrix is an matrix. I need to solve , where . Since so is a free variable and thus there is always a solution to this system of equations. Let us denote if we can write in terms of . Therefore . By substituting by in th row, it implies . Therefore we will end up with . My problem is to find the formula, say , so that . Now I need a hint to propose a method that can help to solve this problem. Cramer's rule seems doesn't work because the right hand side is zero vector.","A_n (n-1) \times n A_nX = 0 X = (x_1, \cdots, x_n) \begin{equation}
A_n = \begin{bmatrix}
b_2 & a_2 & b_3 & 0 & 0 & 0 & 0 & \cdots & 0 & 0 \\
0 & b_3 & a_3 & b_4 & 0 & 0 & 0 & \cdots & 0 & 0\\
0 & 0 & b_4 & a_4 & b_5 & 0 & 0 & \cdots  & 0 & 0\\
0 & 0 & 0 & b_5 & a_5 & b_6 & 0 & \cdots & 0 & 0\\
0 & 0 & 0 & 0 & b_6 & a_6 & b_7 & \cdots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \ddots & \ddots & \ddots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \ddots & \ddots & \ddots & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \ddots & \ddots & b_{n-1} & a_{n-1} & b_n\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & b_n & a_n\\
\end{bmatrix}
\end{equation} 
-\dfrac{b_n}{a_n}x_{n-1} = x_n,
 x_n x_m \sim x_j x_j x_m x_{n-1} \sim x_n x_n x_{n-1} (n-1) x_{n-2} \sim x_{n-1} x_1 \sim x_2 \phi x_2 = \phi x_1","['linear-algebra', 'matrices', 'systems-of-equations']"
31,Show $\nabla f=A\nabla g$ by chain rule,Show  by chain rule,\nabla f=A\nabla g,"Let $A$ be a $2\times 2$ an invertible matrix, $f:\mathbb R^2\to\mathbb R$ be smooth, and define $g:\mathbb R^2\to \mathbb R$ by $$g(Ax)=f(x),$$ i.e., if I set the variable of $g$ as $y$ , $g(y)=f(A^{-1}y).$ Then, show $\nabla f=A\nabla g$ using chain rule. Where do I mistake ? Let $D$ represent the derivative. \begin{align} D_y g(y) &=D_y(f(A^{-1}y))\\ &=(D_x f)(A^{-1}y)\cdot D_y(A^{-1}y)\\ &=(D_x f)(x)\cdot A^{-1} , \end{align} and since $D_y g(y)=\nabla g(y)$ , $(D_x f)(x)=\nabla f(x)$ , I get $$\nabla g=\nabla f\cdot A^{-1}$$ and then $$\nabla f=(\nabla g)A.$$","Let be a an invertible matrix, be smooth, and define by i.e., if I set the variable of as , Then, show using chain rule. Where do I mistake ? Let represent the derivative. and since , , I get and then","A 2\times 2 f:\mathbb R^2\to\mathbb R g:\mathbb R^2\to \mathbb R g(Ax)=f(x), g y g(y)=f(A^{-1}y). \nabla f=A\nabla g D \begin{align}
D_y g(y)
&=D_y(f(A^{-1}y))\\
&=(D_x f)(A^{-1}y)\cdot D_y(A^{-1}y)\\
&=(D_x f)(x)\cdot A^{-1} ,
\end{align} D_y g(y)=\nabla g(y) (D_x f)(x)=\nabla f(x) \nabla g=\nabla f\cdot A^{-1} \nabla f=(\nabla g)A.","['calculus', 'matrices', 'derivatives', 'chain-rule', 'scalar-fields']"
32,For which values of $\alpha$ is the solution stable?,For which values of  is the solution stable?,\alpha,If we have the following differential equation $X'=\begin{bmatrix} 1 & 1 \\ -1 & \alpha  \end{bmatrix}X$ for which values of $\alpha$ is the zero solution stable? My attempt : For stability we need the real part of all eigenvalues to be negative and geometric and algebraic multiplicity to be the same.I tried computing the eigenvalues and see the different cases but it is an incredible amount of work and I got stuck. Would there be any simpler and nicer way to do this(with knowledge of a first course in ODE)? Thanks,If we have the following differential equation for which values of is the zero solution stable? My attempt : For stability we need the real part of all eigenvalues to be negative and geometric and algebraic multiplicity to be the same.I tried computing the eigenvalues and see the different cases but it is an incredible amount of work and I got stuck. Would there be any simpler and nicer way to do this(with knowledge of a first course in ODE)? Thanks,X'=\begin{bmatrix} 1 & 1 \\ -1 & \alpha  \end{bmatrix}X \alpha,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
33,How to differentiate the largest eigenvalue of a matrix?,How to differentiate the largest eigenvalue of a matrix?,,"Currently, I am facing the following problem. Let $t \mapsto H (t)$ be a continuously differentiable, symmetric matrix-valued function. I would like to calculate the following derivative. Is there any general method to calculate it? $$\frac{{\rm d}}{{\rm d} t} \lambda_{\max}(H(t))$$","Currently, I am facing the following problem. Let be a continuously differentiable, symmetric matrix-valued function. I would like to calculate the following derivative. Is there any general method to calculate it?",t \mapsto H (t) \frac{{\rm d}}{{\rm d} t} \lambda_{\max}(H(t)),"['linear-algebra', 'matrices', 'derivatives', 'eigenvalues-eigenvectors', 'matrix-calculus']"
34,How to interpret the square matrix M'M?,How to interpret the square matrix M'M?,,"Say, I have a matrix M of shape n x d . M'M results in a square matrix of shape d x d . How can I interpret M'M ? import numpy as np M = np.array([[1, 2], [2, 1], [3, 4], [4, 3]]) prod = M.T.dot(M)","Say, I have a matrix M of shape n x d . M'M results in a square matrix of shape d x d . How can I interpret M'M ? import numpy as np M = np.array([[1, 2], [2, 1], [3, 4], [4, 3]]) prod = M.T.dot(M)",,"['matrices', 'inner-products', 'intuition', 'matrix-decomposition']"
35,"If the product of $n$ positive definite matrices is symmetric, is it also positive definite?","If the product of  positive definite matrices is symmetric, is it also positive definite?",n,"A well-known result for the product of two PD (or PSD) matrices is: Proposition: Let $A$ and $B$ be positive definite (respectively positive semidefinite) Hermitian matrices of the same size. If $D:=AB$ is Hermitian, then $D$ is also positive definite (respectively positive semidefinite). This can be extended to the product of three PD (or PSD) matrices: Is the product of three positive semidefinite matrices positive semidefinite Proposition: Let $A$ , $B$ , and $C$ be positive definite (respectively positive semidefinite) Hermitian matrices of the same size. If $D:=ABC$ is Hermitian, then $D$ is also positive definite (respectively positive semidefinite). Is there any extension of this to the product of $n$ PD (or PSD) matrices?","A well-known result for the product of two PD (or PSD) matrices is: Proposition: Let and be positive definite (respectively positive semidefinite) Hermitian matrices of the same size. If is Hermitian, then is also positive definite (respectively positive semidefinite). This can be extended to the product of three PD (or PSD) matrices: Is the product of three positive semidefinite matrices positive semidefinite Proposition: Let , , and be positive definite (respectively positive semidefinite) Hermitian matrices of the same size. If is Hermitian, then is also positive definite (respectively positive semidefinite). Is there any extension of this to the product of PD (or PSD) matrices?",A B D:=AB D A B C D:=ABC D n,"['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-definite', 'positive-semidefinite']"
36,Show that $\det(A)=2^{p}$,Show that,\det(A)=2^{p},"We have a $(n×n)$ -matrix $A$ with complex entries such that $\,A^{2}=3A-2I$ . $~$ Show that there exists $p\in\{0,1,2,...,n\}$ such that $\det(A)=2^{p}$ . I don't know if my proof is good. I took the polynomial $G(x)=x^{2}-3x+2=(x-1)(x-2)$ . So $A$ is a solution for $G(x)$ . The $A$ 's minimal polynomial divides $G(x)$ so the minimal polynomial can be $(x-1)$ or $(x-2)$ . So we will get that $\det(A)=2^{0}$ or $\det(A)=2^{n}$ .",We have a -matrix with complex entries such that . Show that there exists such that . I don't know if my proof is good. I took the polynomial . So is a solution for . The 's minimal polynomial divides so the minimal polynomial can be or . So we will get that or .,"(n×n) A \,A^{2}=3A-2I ~ p\in\{0,1,2,...,n\} \det(A)=2^{p} G(x)=x^{2}-3x+2=(x-1)(x-2) A G(x) A G(x) (x-1) (x-2) \det(A)=2^{0} \det(A)=2^{n}","['matrices', 'eigenvalues-eigenvectors', 'minimal-polynomials', 'characteristic-polynomial']"
37,Form of Q in extended QR decomposition calculated with Householder reflections,Form of Q in extended QR decomposition calculated with Householder reflections,,"Let $A = QR$ be the extended QR decomposition of matrix $A \in \mathbb{R}^{m \times n}$ which is calculated by using $n$ Householder reflections. Prove by construction that there exist an upper triangular matrix $U \in \mathbb{R}^{n \times n}$ and such a matrix $W \in \mathbb{R}^{m \times n}$ that $Q = I - WUW^T$ . I've tried solving this problem by rearranging equations algebraically but this didn't give me any ideas. I assume that I have to use the fact that Q = $\prod_{i = 1}^n{(I - \frac{2}{w_i^Tw_i}w_iw_i^T)}$ for some vectors $w_i$ but I really don't know how use it. I know $w_i$ aren't just ""some"" vectors but I couldn't think of any property they have which would help me solve the problem. Any help is much appreciated.","Let be the extended QR decomposition of matrix which is calculated by using Householder reflections. Prove by construction that there exist an upper triangular matrix and such a matrix that . I've tried solving this problem by rearranging equations algebraically but this didn't give me any ideas. I assume that I have to use the fact that Q = for some vectors but I really don't know how use it. I know aren't just ""some"" vectors but I couldn't think of any property they have which would help me solve the problem. Any help is much appreciated.",A = QR A \in \mathbb{R}^{m \times n} n U \in \mathbb{R}^{n \times n} W \in \mathbb{R}^{m \times n} Q = I - WUW^T \prod_{i = 1}^n{(I - \frac{2}{w_i^Tw_i}w_iw_i^T)} w_i w_i,"['linear-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra', 'matrix-decomposition']"
38,Prove $tr_B(vec(A)vec(B)^T)=AB^T$,Prove,tr_B(vec(A)vec(B)^T)=AB^T,"Let $A,B\in \mathbb{R}^{n\times n}$ . And let $vec$ be a map : $\mathbb{R}^{n\times n}\to \mathbb{R}^{n^2\times1}$ : $vec(e_a e_b^T)=e_a\otimes e_b$ , here $e_a$ denotes the vector with   with an entry $1$ on index $a$ and other entries $0$ . Intuitively the operation $vec$ puts every row of a matrix into a new vector. Partial trace is defined as $tr_B(A\otimes B)=tr(B)A$ . Then how to prove $tr_B(vec(A)vec(B)^T)=AB^T$ ? Here $\otimes$ denotes the Kronecker product. Attempts: I try to decomposite $vec(A)vec(B)^T$ into the form $\sum_{ij} X_i\otimes Y_j$ so $tr_B(vec(A)vec(B)^T)=\sum_{ij}(X_i tr(Y_j))$ , but how to achieve that? I have one new idea: $vec(A)=\sum_{ij}a_{ij} e_i\otimes e_j$ and $vec(B)=\sum_{kl}b_{kl}e_k\otimes e_l$ , I will have a try along this way.","Let . And let be a map : : , here denotes the vector with   with an entry on index and other entries . Intuitively the operation puts every row of a matrix into a new vector. Partial trace is defined as . Then how to prove ? Here denotes the Kronecker product. Attempts: I try to decomposite into the form so , but how to achieve that? I have one new idea: and , I will have a try along this way.","A,B\in \mathbb{R}^{n\times n} vec \mathbb{R}^{n\times n}\to \mathbb{R}^{n^2\times1} vec(e_a e_b^T)=e_a\otimes e_b e_a 1 a 0 vec tr_B(A\otimes B)=tr(B)A tr_B(vec(A)vec(B)^T)=AB^T \otimes vec(A)vec(B)^T \sum_{ij} X_i\otimes Y_j tr_B(vec(A)vec(B)^T)=\sum_{ij}(X_i tr(Y_j)) vec(A)=\sum_{ij}a_{ij} e_i\otimes e_j vec(B)=\sum_{kl}b_{kl}e_k\otimes e_l","['linear-algebra', 'matrices', 'kronecker-product', 'quantum-information']"
39,Hadamard product operator norm,Hadamard product operator norm,,"suppose that $A$ and $B$ be $n×n$ nonnegative matrices. Consider the operator (spectral) norm on $A$ $\lVert A \rVert_{op} = sup_{x \ne 0} \dfrac{\lVert Ax \rVert}{\lVert x \rVert}= sup_{\lVert x \rVert=1} \lVert Ax \rVert.$ Then $ \lVert A \circ B \rVert _{op} ≤ \rho (A^{T} B)$ ? where $\rho (A)$ spectral radius and $A \circ B=( a_{ij}  b_{ij})$ is a the Hadamard product. I know that $\rho (A \circ B ) ≤ \rho (AB),$ and $(A \circ B)(B \circ A) ≤  AB \circ BA$ .",suppose that and be nonnegative matrices. Consider the operator (spectral) norm on Then ? where spectral radius and is a the Hadamard product. I know that and .,"A B n×n A \lVert A \rVert_{op} = sup_{x \ne 0} \dfrac{\lVert Ax \rVert}{\lVert x \rVert}= sup_{\lVert x \rVert=1} \lVert Ax \rVert.  \lVert A \circ B \rVert _{op} ≤ \rho (A^{T} B) \rho (A) A \circ B=( a_{ij}  b_{ij}) \rho (A \circ B ) ≤ \rho (AB), (A \circ B)(B \circ A) ≤  AB \circ BA","['linear-algebra', 'matrices']"
40,Intuition behind formula for matrix inverse in terms of partitions,Intuition behind formula for matrix inverse in terms of partitions,,"Suppose we have a square matrix $A$ that can be partitioned into $A=\pmatrix{A_{11}&A_{12}\\ A_{21}&A_{22}}$ . Then its inverse is given by the formula $$A^{-1}=\pmatrix{(A_{11} - A_{12}A_{22}A_{21})^{-1}&-A_{11}^{-1}A_{12}(A_{22} - A_{21}A_{11}A_{12})^{-1}\\ -A_{22}^{-1}A_{21}(A_{11} - A_{12}A_{22}A_{21})^{-1}&(A_{22} - A_{21}A_{11}A_{12})^{-1}}$$ Now, we can simply multiply $A^{-1}$ and $A$ to check that this indeed is the correct formula, given $A_{11}$ and $A_{22}$ are not singular. But is there any intuition (probably geometric) behind why this is so?: The formula does have a pattern in it!","Suppose we have a square matrix that can be partitioned into . Then its inverse is given by the formula Now, we can simply multiply and to check that this indeed is the correct formula, given and are not singular. But is there any intuition (probably geometric) behind why this is so?: The formula does have a pattern in it!",A A=\pmatrix{A_{11}&A_{12}\\ A_{21}&A_{22}} A^{-1}=\pmatrix{(A_{11} - A_{12}A_{22}A_{21})^{-1}&-A_{11}^{-1}A_{12}(A_{22} - A_{21}A_{11}A_{12})^{-1}\\ -A_{22}^{-1}A_{21}(A_{11} - A_{12}A_{22}A_{21})^{-1}&(A_{22} - A_{21}A_{11}A_{12})^{-1}} A^{-1} A A_{11} A_{22},"['linear-algebra', 'matrices', 'inverse', 'intuition']"
41,"How can one determine $e^{tA}$, where $A =\begin{bmatrix} 1 & 0 \\ 1 & 2 \end{bmatrix}$? [closed]","How can one determine , where ? [closed]",e^{tA} A =\begin{bmatrix} 1 & 0 \\ 1 & 2 \end{bmatrix},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Let $A =\begin{bmatrix} 1 & 0 \\ 1 & 2  \end{bmatrix}$ be given. How can one compute the matrix function $e^{tA}$ ? Do you have to decompose $A$ , if so how?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Let be given. How can one compute the matrix function ? Do you have to decompose , if so how?","A =\begin{bmatrix}
1 & 0 \\
1 & 2 
\end{bmatrix} e^{tA} A","['matrices', 'matrix-decomposition', 'matrix-exponential']"
42,Writing $e^A$ as an expression of matrix A,Writing  as an expression of matrix A,e^A,"Let $A\in\mathscr{L}(x)$ and satisfies the condition $A^2=A$ . Write the function $e^A$ as expression of A. My attempt: Let's start with $e^x$ , which we  re-write as a series: $$f(x) = \sum\limits_{j=0}^\infty\bigg(\frac{1}{n!}\bigg)x^n$$ . Here $$\bigg(\frac{1}{n!}\bigg)=a_{n}$$ , so we get $$f(x) = \sum\limits_{j=0}^\infty\bigg(a_n\bigg)x^n$$ Since we have a that $A$ is a diagonizable matrix, we have the following.  Suppose $P^{-1}AP=D=\mathrm{diag}(\lambda_1,\dots,\lambda_n)$ . The given function $f(x)=e^x$ , we can define in terms of A as $$f(A) = \begin{pmatrix} e^{(\lambda_1)} & 0 & \cdots & 0 \\ 0 & e^{(\lambda_2)} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & e^{(\lambda_n)} \end{pmatrix}$$ . But to me, this seems odd, since $e=\sum\frac{1}{n!}$ , so shouldn't the right answer be: $$f(A) = \begin{pmatrix} \frac{1}{1!}^{x} & 0 & \cdots & 0 \\ 0 & \frac{1}{2!}^{x} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \frac{1}{n!}^{x} \end{pmatrix}$$ . Maybe I am confusing the matrix with the variable x here, but the first matrix doesn't seem to give the full answer. Any hints? Thanks","Let and satisfies the condition . Write the function as expression of A. My attempt: Let's start with , which we  re-write as a series: . Here , so we get Since we have a that is a diagonizable matrix, we have the following.  Suppose . The given function , we can define in terms of A as . But to me, this seems odd, since , so shouldn't the right answer be: . Maybe I am confusing the matrix with the variable x here, but the first matrix doesn't seem to give the full answer. Any hints? Thanks","A\in\mathscr{L}(x) A^2=A e^A e^x f(x) = \sum\limits_{j=0}^\infty\bigg(\frac{1}{n!}\bigg)x^n \bigg(\frac{1}{n!}\bigg)=a_{n} f(x) = \sum\limits_{j=0}^\infty\bigg(a_n\bigg)x^n A P^{-1}AP=D=\mathrm{diag}(\lambda_1,\dots,\lambda_n) f(x)=e^x f(A) = \begin{pmatrix} e^{(\lambda_1)} & 0 & \cdots & 0 \\ 0 & e^{(\lambda_2)} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & e^{(\lambda_n)} \end{pmatrix} e=\sum\frac{1}{n!} f(A) = \begin{pmatrix} \frac{1}{1!}^{x} & 0 & \cdots & 0 \\ 0 & \frac{1}{2!}^{x} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \frac{1}{n!}^{x} \end{pmatrix}","['linear-algebra', 'matrices', 'exponential-function']"
43,Solving Kronecker product equation,Solving Kronecker product equation,,I am trying to solve for $\mathbf{X}$ the following equation $\mathbf{A}\otimes \mathbf{X}=\mathbf{B}$ Is there a closed form solution to this? Thank you very much in advance!,I am trying to solve for the following equation Is there a closed form solution to this? Thank you very much in advance!,\mathbf{X} \mathbf{A}\otimes \mathbf{X}=\mathbf{B},"['matrices', 'systems-of-equations', 'kronecker-product']"
44,How many real square roots can a real invertible matrix have?,How many real square roots can a real invertible matrix have?,,"Let A be a real square matrix nxn. The real matrix B with the size nxn is considered to be a square root of a matrix A if A=B*B. So I wonder, how many different real square roots can an invertible real matrix have. I know there can be 0 square roots. If a matrix has a negative determinant, it cannot have square roots. I also know that a matrix can have infinitely many square roots. The example is an identity matrix of a size 2x2. Also a matrix can have exactly 2^n square roots. The example is any positive definite matrix. Generally, if a matrix has a finite amount of square roots, it has to be even, since for every square root matrix B, the matrix -B is also a square root. So the question is, are there any other options? I need the answer to prove another theorem, the link is here Prove that there is no isomorphism between groups of matrices of a size n with determinant 1 and matrices of a size n with determinant +1/-1 If it turns out these are all the options, the theorem is proven. Otherwise, I'll need to find at least a single example of a matrix with the smallest non-zero amount of square roots. The link to wikipedia article on square roots is here, it might also help: https://en.wikipedia.org/wiki/Square_root_of_a_matrix","Let A be a real square matrix nxn. The real matrix B with the size nxn is considered to be a square root of a matrix A if A=B*B. So I wonder, how many different real square roots can an invertible real matrix have. I know there can be 0 square roots. If a matrix has a negative determinant, it cannot have square roots. I also know that a matrix can have infinitely many square roots. The example is an identity matrix of a size 2x2. Also a matrix can have exactly 2^n square roots. The example is any positive definite matrix. Generally, if a matrix has a finite amount of square roots, it has to be even, since for every square root matrix B, the matrix -B is also a square root. So the question is, are there any other options? I need the answer to prove another theorem, the link is here Prove that there is no isomorphism between groups of matrices of a size n with determinant 1 and matrices of a size n with determinant +1/-1 If it turns out these are all the options, the theorem is proven. Otherwise, I'll need to find at least a single example of a matrix with the smallest non-zero amount of square roots. The link to wikipedia article on square roots is here, it might also help: https://en.wikipedia.org/wiki/Square_root_of_a_matrix",,"['linear-algebra', 'matrices', 'determinant']"
45,How to find matrix multiplications like AB = 10A+B? [closed],How to find matrix multiplications like AB = 10A+B? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question saw this one. How do we find others (not necessarily 2 by 2)? How do we generalize it?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question saw this one. How do we find others (not necessarily 2 by 2)? How do we generalize it?",,['matrices']
46,Derivative of square root of unitary matrix,Derivative of square root of unitary matrix,,"Given a (special) unitary matrix $U(x) \in SU(2)$ which is a function of $x\in\mathbb{R}$ , along with its unitary square root $u(x) = \sqrt{U(x)}$ such that $u(x)^2 = U(x)$ and $u(x)^\dagger u(x) = \mathbb{1}_{2\times2}$ , what is the derivative $$\frac{d}{dx}u(x)=\frac{d}{dx}\sqrt{U(x)}?$$ Can it be expressed purely in terms of products and sums of $u(x)$ , $u(x)^\dagger$ , $U(x)$ , $U(x)^\dagger$ , $\frac{d}{dx}U(x)$ and $\frac{d}{dx}U(x)^\dagger$ ? Naively, it seems like it should be something like (suppressing arguments) $$\frac{du}{dx} = \frac{1}{2}\left(\frac{1}{2} u^\dagger \frac{dU}{dx} + \frac{1}{2}\frac{dU}{dx}u^\dagger \right),$$ following from a ""symmetrized"" form of the regular chain rule $\tfrac{d}{dx}\left[f(x)\right]^{1/2}=\frac{1}{2} \left[f(x)\right]^{-1/2}f'(x)$ with the identification of $u\leftrightarrow \left[f(x)\right]^{1/2}$ and $u^\dagger = u^{-1} \leftrightarrow \left[ f(x)\right]^{-1/2}$ . Note: Here $U^\dagger$ is the conjugate transpose. Edit: I am actually interested in the quantity $$ u^\dagger \frac{du}{dx} + u \frac{du^\dagger}{dx}, $$ so even if there is not a way to write $\tfrac{du}{dx}$ simply, an expression for this would be sufficient.","Given a (special) unitary matrix which is a function of , along with its unitary square root such that and , what is the derivative Can it be expressed purely in terms of products and sums of , , , , and ? Naively, it seems like it should be something like (suppressing arguments) following from a ""symmetrized"" form of the regular chain rule with the identification of and . Note: Here is the conjugate transpose. Edit: I am actually interested in the quantity so even if there is not a way to write simply, an expression for this would be sufficient.","U(x) \in SU(2) x\in\mathbb{R} u(x) = \sqrt{U(x)} u(x)^2 = U(x) u(x)^\dagger u(x) = \mathbb{1}_{2\times2} \frac{d}{dx}u(x)=\frac{d}{dx}\sqrt{U(x)}? u(x) u(x)^\dagger U(x) U(x)^\dagger \frac{d}{dx}U(x) \frac{d}{dx}U(x)^\dagger \frac{du}{dx} = \frac{1}{2}\left(\frac{1}{2} u^\dagger \frac{dU}{dx} + \frac{1}{2}\frac{dU}{dx}u^\dagger \right), \tfrac{d}{dx}\left[f(x)\right]^{1/2}=\frac{1}{2} \left[f(x)\right]^{-1/2}f'(x) u\leftrightarrow \left[f(x)\right]^{1/2} u^\dagger = u^{-1} \leftrightarrow \left[ f(x)\right]^{-1/2} U^\dagger  u^\dagger \frac{du}{dx} + u \frac{du^\dagger}{dx},  \tfrac{du}{dx}","['matrices', 'derivatives', 'lie-groups', 'matrix-calculus', 'unitary-matrices']"
47,Show that there exists a $v$ s.t. $A^TAv=\|A\|_2^2v$,Show that there exists a  s.t.,v A^TAv=\|A\|_2^2v,"Given $A \in \mathbb{R}^{m \times n}$ , show that there exists a $v$ , s.t. $\|v\| = 1$ and $A^T A v = \|A\|_2^2 v$ , where $$\|A\|_2 :=\max_{\|v\|=1}\|Av\|$$ is the spectral norm. My attempt: By the spectral theorem there exists $O \in O(\mathbb{R}^m): A^TA = O^TDO$ , with $D$ diagonal. Now: $$||A^TA||_2= \max_{||v||=1}||A^TAv||= \max_{||v||=1}\sqrt{v^TO^TDOO^TDOv}= \max_{||v||=1}||DOv||= \max_{||v||=1}||Dv||=|\lambda_{\max}|$$ With $\lambda_{\max}$ the largest eigenvalue by absolute value. So if I new that $\lambda_{\max}\geq 0$ I would know $\exists v: A^TAv=||A^TA||_2v$ . Now I would only have to show that $||A||_2^2= ||A^TA||_2$ , but somehow I can’t find a way to show that. I‘d be really thankful for some help:)","Given , show that there exists a , s.t. and , where is the spectral norm. My attempt: By the spectral theorem there exists , with diagonal. Now: With the largest eigenvalue by absolute value. So if I new that I would know . Now I would only have to show that , but somehow I can’t find a way to show that. I‘d be really thankful for some help:)",A \in \mathbb{R}^{m \times n} v \|v\| = 1 A^T A v = \|A\|_2^2 v \|A\|_2 :=\max_{\|v\|=1}\|Av\| O \in O(\mathbb{R}^m): A^TA = O^TDO D ||A^TA||_2= \max_{||v||=1}||A^TAv||= \max_{||v||=1}\sqrt{v^TO^TDOO^TDOv}= \max_{||v||=1}||DOv||= \max_{||v||=1}||Dv||=|\lambda_{\max}| \lambda_{\max} \lambda_{\max}\geq 0 \exists v: A^TAv=||A^TA||_2v ||A||_2^2= ||A^TA||_2,"['linear-algebra', 'matrices', 'matrix-norms', 'spectral-norm']"
48,Operator norm of a matrix with elements normalized by geometric mean of row and column sum,Operator norm of a matrix with elements normalized by geometric mean of row and column sum,,"Suppose $M \in \mathbb{R}^{n \times n}$ is a square matrix with nonnegative entries, and suppose all its row and column sums are positive. Normalize it to form an equivalently sized matrix $A \in \mathbb{R}^{n \times n}$ by dividing each element by the square root of both its row and column sum: $$A_{i j} = \frac{M_{i j}}{\sqrt{\left(\sum_{k} M_{i k}\right)\left(\sum_{k} M_{k j}\right)}}$$ Can we show that the $\ell_2$ operator norm of A is $\| A \|_2 = 1$ ? I have been able to show that $\|A\|_2 \geq 1$ as follows. Let $c = \sum_{i j} M_{i j}$ . Take $v \in \mathbb{R}^n$ with $v_i = \sqrt{\frac{1}{c} \sum_{k} M_{k i}}$ . Clearly $\|v\|_2 = \sqrt{\frac{1}{c}\sum_i \sum_{k} M_{k i}} = 1$ . Also, $$\|A v\|_2 = \sqrt{\sum_i \left(\sum_j A_{i j} v_j \right)^2}$$ $$= \sqrt{\sum_i \left(\sum_j \sqrt{\frac{\sum_{k} M_{k j}}{c}} \frac{M_{i j}}{\sqrt{\left(\sum_{k} M_{i k}\right)\left(\sum_{k} M_{k j}\right)}} \right)^2}$$ $$= \sqrt{\sum_i \left(\frac{\sum_j M_{i j}}{\sqrt{c \left(\sum_{k} M_{i k}\right)}} \right)^2}$$ $$= \sqrt{\frac{1}{c} \sum_i \sum_j M_{i j}} = 1.$$ However, I can't seem to show that $\| A \|_2 \leq 1$ .","Suppose is a square matrix with nonnegative entries, and suppose all its row and column sums are positive. Normalize it to form an equivalently sized matrix by dividing each element by the square root of both its row and column sum: Can we show that the operator norm of A is ? I have been able to show that as follows. Let . Take with . Clearly . Also, However, I can't seem to show that .",M \in \mathbb{R}^{n \times n} A \in \mathbb{R}^{n \times n} A_{i j} = \frac{M_{i j}}{\sqrt{\left(\sum_{k} M_{i k}\right)\left(\sum_{k} M_{k j}\right)}} \ell_2 \| A \|_2 = 1 \|A\|_2 \geq 1 c = \sum_{i j} M_{i j} v \in \mathbb{R}^n v_i = \sqrt{\frac{1}{c} \sum_{k} M_{k i}} \|v\|_2 = \sqrt{\frac{1}{c}\sum_i \sum_{k} M_{k i}} = 1 \|A v\|_2 = \sqrt{\sum_i \left(\sum_j A_{i j} v_j \right)^2} = \sqrt{\sum_i \left(\sum_j \sqrt{\frac{\sum_{k} M_{k j}}{c}} \frac{M_{i j}}{\sqrt{\left(\sum_{k} M_{i k}\right)\left(\sum_{k} M_{k j}\right)}} \right)^2} = \sqrt{\sum_i \left(\frac{\sum_j M_{i j}}{\sqrt{c \left(\sum_{k} M_{i k}\right)}} \right)^2} = \sqrt{\frac{1}{c} \sum_i \sum_j M_{i j}} = 1. \| A \|_2 \leq 1,"['linear-algebra', 'matrices', 'matrix-norms']"
49,"centraliser of an element in $\mathrm{GL}(n,\mathbf Z)$",centraliser of an element in,"\mathrm{GL}(n,\mathbf Z)","Given a matrix $M \in \mathrm{GL}(n,\mathbf Z)$ , is there a description of what the centraliser of $M$ looks like? Here are some related facts: We know that two matrices commute implying they have at least one common eigenvector. The power of $M$ is an element in the centraliser of $M$ , but the converse is not true, e.g. take $M$ as the identity matrix. This post gives us a description of all rational    matrices that commute with $M$ , but not all of such matrices are in $\mathrm{GL}(n,\mathbf Z)$ . Any reference/ideas would be really appreciated.","Given a matrix , is there a description of what the centraliser of looks like? Here are some related facts: We know that two matrices commute implying they have at least one common eigenvector. The power of is an element in the centraliser of , but the converse is not true, e.g. take as the identity matrix. This post gives us a description of all rational    matrices that commute with , but not all of such matrices are in . Any reference/ideas would be really appreciated.","M \in \mathrm{GL}(n,\mathbf Z) M M M M M \mathrm{GL}(n,\mathbf Z)","['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory']"
50,Determining an integral positive-definite symmetric matrix is diagonalizable over $\Bbb Z$,Determining an integral positive-definite symmetric matrix is diagonalizable over,\Bbb Z,"According to this question: Diagonalization of a symmetric bilinear form over the integers , not all definite symmetric matrices with integer entries are diagonalizable over $\Bbb Z$ . (Here we are considering diagonalization of a bilinear form, so this means that given a definite symmetric matrix $H \in M_{n\times n}(\Bbb Z)$ , we cannot always find an invertible matrix $P\in GL(n,\Bbb Z)$ such that $P^t HP$ is diagonal.) I am curious about: if we are given an explicit definite symmetric integral matrix, then is there a way (or algorithm) to determine whether it is diagonalizable over $\Bbb Z$ ? Actually I want to determine whether the negative-definite matrix $$A= \begin{bmatrix} -1 & 1 & 1 & 1 & & 1 &  \\ 1 & -2 & \\ 1 & & -5 & \\1 & & & -4 & 1 \\ & & & 1 & -2 &  \\1 & & & & & -71 & 1 \\ & & & & & 1 & -2 \end{bmatrix}$$ is diagonalizable over $\Bbb Z$ . Using elementary row&column operations (corresponding to $E^tAE$ for some integral elementary matrix $E$ ), I've got $$ A'=\begin{bmatrix} -1   \\  & -1 & \\  & & -3 & 2 & & 2 \\ & & 2 & -2 & 1 & 2 \\ & & & 1 & -2 &  \\ & & 2 & 2 & & -69 & 1 \\ & & & & & 1 & -2 \end{bmatrix} ,$$ and it seems $A'$ is not diagonalziable because the $5\times 5$ submatrix of $A'$ doesn't seem diagonalizable. Is there a way to prove that $A$ is not diagonalziable?","According to this question: Diagonalization of a symmetric bilinear form over the integers , not all definite symmetric matrices with integer entries are diagonalizable over . (Here we are considering diagonalization of a bilinear form, so this means that given a definite symmetric matrix , we cannot always find an invertible matrix such that is diagonal.) I am curious about: if we are given an explicit definite symmetric integral matrix, then is there a way (or algorithm) to determine whether it is diagonalizable over ? Actually I want to determine whether the negative-definite matrix is diagonalizable over . Using elementary row&column operations (corresponding to for some integral elementary matrix ), I've got and it seems is not diagonalziable because the submatrix of doesn't seem diagonalizable. Is there a way to prove that is not diagonalziable?","\Bbb Z H \in M_{n\times n}(\Bbb Z) P\in GL(n,\Bbb Z) P^t HP \Bbb Z A=
\begin{bmatrix} -1 & 1 & 1 & 1 & & 1 &  \\ 1 & -2 & \\ 1 & & -5 & \\1 & & & -4 & 1 \\ & & & 1 & -2 &  \\1 & & & & & -71 & 1 \\ & & & & & 1 & -2 \end{bmatrix} \Bbb Z E^tAE E  A'=\begin{bmatrix} -1   \\  & -1 & \\  & & -3 & 2 & & 2 \\ & & 2 & -2 & 1 & 2 \\ & & & 1 & -2 &  \\ & & 2 & 2 & & -69 & 1 \\ & & & & & 1 & -2 \end{bmatrix} , A' 5\times 5 A' A","['linear-algebra', 'matrices', 'diagonalization', 'positive-definite', 'bilinear-form']"
51,Why should the rows of $AB^\top$ sum to one if the rows of $A$ and column of $B$ sum to one,Why should the rows of  sum to one if the rows of  and column of  sum to one,AB^\top A B,"For $A \in \mathbb{R}^{a \times n}$ and $B \in \mathbb{R}^{b\times n}$ $$ \sum_j AB^\top = \mathbb{1}^a \quad \textit{iff} \quad \sum_jA = \mathbb{1}^a \quad \text{and} \quad \sum_i B = (\mathbb{1}^n)^\top $$ I saw this immediately before section 3.4 of this paper (they are using different notation). I tested this with random matrices and it is true, but I cannot see why this should always be true. Is there some theorem or some easy reason to see why this is true?","For and I saw this immediately before section 3.4 of this paper (they are using different notation). I tested this with random matrices and it is true, but I cannot see why this should always be true. Is there some theorem or some easy reason to see why this is true?","A \in \mathbb{R}^{a \times n} B \in \mathbb{R}^{b\times n} 
\sum_j AB^\top = \mathbb{1}^a \quad \textit{iff} \quad \sum_jA = \mathbb{1}^a \quad \text{and} \quad \sum_i B = (\mathbb{1}^n)^\top
","['linear-algebra', 'matrices']"
52,Matrix problems over a finite field,Matrix problems over a finite field,,"Assume $F_p$ is a finite field, $|F_p|=p$ and $M_n(F_p)$ is all the $n\times n$ matrices over $F_p$ . A relation is defined over $M_n(F_p)$ : $$A,B\in M_n(F_p),A\sim B \Leftrightarrow \exists~~u,v\in \mathbb{Z^+}~~A^u=B^v.$$ It is easy to verify that this is an equivalence relation.. So for any $A\in M_n(F_p)$ , we define $$S_A=\left\{B\in M_n(F_p)|B\sim A\right\}.$$ So we have $$S_0=\left\{ B\in M_n(F_p)|B\sim 0 \right\} $$ and $$S_I=\left\{ B\in M_n(F_p)|B\sim I \right\},$$ where $0$ is the $0$ matrix and $I$ is the identity matrix. The question is how to calculate the cardinality of $S_0$ and $S_I$ . If we define $$S=\left\{S_A|A\in M_n(F_p)\right\},$$ the question is how to calculate the cardinality of $S$ . By reviewing the article, I have known that $S_0$ is the $\mathrm{Fine}-\mathrm{Herstein}$ theorem, so $|S_0|=q^{n^2-n}.$ I would like to ask about the remaining two questions. I would like to get some articles or links to articles. Thanks.","Assume is a finite field, and is all the matrices over . A relation is defined over : It is easy to verify that this is an equivalence relation.. So for any , we define So we have and where is the matrix and is the identity matrix. The question is how to calculate the cardinality of and . If we define the question is how to calculate the cardinality of . By reviewing the article, I have known that is the theorem, so I would like to ask about the remaining two questions. I would like to get some articles or links to articles. Thanks.","F_p |F_p|=p M_n(F_p) n\times n F_p M_n(F_p) A,B\in M_n(F_p),A\sim B \Leftrightarrow \exists~~u,v\in \mathbb{Z^+}~~A^u=B^v. A\in M_n(F_p) S_A=\left\{B\in M_n(F_p)|B\sim A\right\}. S_0=\left\{ B\in M_n(F_p)|B\sim 0 \right\}  S_I=\left\{ B\in M_n(F_p)|B\sim I \right\}, 0 0 I S_0 S_I S=\left\{S_A|A\in M_n(F_p)\right\}, S S_0 \mathrm{Fine}-\mathrm{Herstein} |S_0|=q^{n^2-n}.","['linear-algebra', 'matrices', 'reference-request', 'finite-fields', 'equivalence-relations']"
53,"Exercise 4, Section 3.4 of Hoffman’s Linear Algebra","Exercise 4, Section 3.4 of Hoffman’s Linear Algebra",,"Let $V$ be a two-dimensional vector space over the field $F$ , and let $B$ be an ordered basis for $V$ . If $T$ is a linear operator on $V$ and $[T]_B= \begin{bmatrix} a & b \\c & d\\ \end{bmatrix}$ . Prove that $T^2-(a+d)T+(ad-bc)I=0$ . My attempt: It’s easy to check $T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V\in L(V,V)$ . Let $B=\{\alpha_1,\alpha_2\}$ be basis of $V$ . Then $T(\alpha_1)=a\cdot \alpha_1+c\cdot \alpha_2$ and $T(\alpha_2)=b\cdot \alpha_1 +d\cdot \alpha_2$ . So \begin{align} T^2(\alpha_1) &=T(T(\alpha_1)) \\ &=T(a\cdot \alpha_1 +c\cdot \alpha_2) \\ &=a\cdot T(\alpha_1)+c\cdot T(\alpha_2) \\ &=a\cdot (a\cdot \alpha_1+c\cdot \alpha_2)+c\cdot (b\cdot \alpha_1+d\cdot \alpha_2)\\ &=(a^2+cb)\cdot \alpha_1 +(ac+cd)\alpha_2. \end{align} Then $(T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V)(\alpha_1)$ $=T^2(\alpha_1)-(a+d)\cdot T(\alpha_1)+(ad-bc)\cdot \text{id}_V(\alpha_1)$ $= [(a^2+cb)\cdot \alpha_1 +(ac+cd)\alpha_2]-(a+d)\cdot [a\cdot \alpha_1+c\cdot \alpha_2] +(ad-bc)\cdot \alpha_1$ $=[(a^2+cb)\cdot \alpha_1 +(ac+cd)\alpha_2]$$-$$[(a^2+ad)\cdot \alpha_1+(ac+cd)\cdot \alpha_2]$$+$$[(ad)\cdot \alpha_1-(bc)\cdot \alpha_1]$ $=0_V$ . Similarly $(T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V)(\alpha_2)=0_V$ . By theorem 1 section 3.1, $T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V=0_{L(V)}$ . Is my proof correct? Once can also show $(T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V)(\alpha)=0_V$ , $\forall \alpha \in V$ . But that is not efficient. Observation: See $\text{tr}([T]_{B})=a+d$ and $\text{det}([T]_B)=ad-bc$ . So $T^2-\text{tr}([T]_{B})\cdot T+ \text{det}([T]_B) \cdot \text{id}_V=0_{L(V)}$ . Can we generalize this problem?","Let be a two-dimensional vector space over the field , and let be an ordered basis for . If is a linear operator on and . Prove that . My attempt: It’s easy to check . Let be basis of . Then and . So Then . Similarly . By theorem 1 section 3.1, . Is my proof correct? Once can also show , . But that is not efficient. Observation: See and . So . Can we generalize this problem?","V F B V T V [T]_B=
\begin{bmatrix} a & b \\c & d\\ \end{bmatrix} T^2-(a+d)T+(ad-bc)I=0 T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V\in L(V,V) B=\{\alpha_1,\alpha_2\} V T(\alpha_1)=a\cdot \alpha_1+c\cdot \alpha_2 T(\alpha_2)=b\cdot \alpha_1 +d\cdot \alpha_2 \begin{align}
T^2(\alpha_1) &=T(T(\alpha_1)) \\
&=T(a\cdot \alpha_1 +c\cdot \alpha_2) \\
&=a\cdot T(\alpha_1)+c\cdot T(\alpha_2) \\
&=a\cdot (a\cdot \alpha_1+c\cdot \alpha_2)+c\cdot (b\cdot \alpha_1+d\cdot \alpha_2)\\
&=(a^2+cb)\cdot \alpha_1 +(ac+cd)\alpha_2.
\end{align} (T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V)(\alpha_1) =T^2(\alpha_1)-(a+d)\cdot T(\alpha_1)+(ad-bc)\cdot \text{id}_V(\alpha_1) = [(a^2+cb)\cdot \alpha_1 +(ac+cd)\alpha_2]-(a+d)\cdot [a\cdot \alpha_1+c\cdot \alpha_2] +(ad-bc)\cdot \alpha_1 =[(a^2+cb)\cdot \alpha_1 +(ac+cd)\alpha_2]-[(a^2+ad)\cdot \alpha_1+(ac+cd)\cdot \alpha_2]+[(ad)\cdot \alpha_1-(bc)\cdot \alpha_1] =0_V (T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V)(\alpha_2)=0_V T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V=0_{L(V)} (T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V)(\alpha)=0_V \forall \alpha \in V \text{tr}([T]_{B})=a+d \text{det}([T]_B)=ad-bc T^2-\text{tr}([T]_{B})\cdot T+ \text{det}([T]_B) \cdot \text{id}_V=0_{L(V)}","['linear-algebra', 'matrices', 'solution-verification', 'proof-writing', 'linear-transformations']"
54,Could the product of a skew-symmetric matrix and an invertible matrix be nilpotent?,Could the product of a skew-symmetric matrix and an invertible matrix be nilpotent?,,"Suppose that $A$ is a $d\times d$ skew-symmetric matrix, $B$ is a $d\times d$ invertible matrix and $AB$ is a nilpotent matrix. The unique example I could find is $A=O$ , the null matrix. My question: Does there exist another pair of matrices $A,B$ such that $A$ is skew-symmetric, $B$ is invertible and $AB$ is nilpotent? I guess $A=O$ is the only possibility. Since $AB$ is nilpotent, its Jordan canonical form is \begin{bmatrix}     S_1 & 0 & \ldots & 0 \\     0 & S_2 & \ldots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \ldots & S_r  \end{bmatrix} where each of the blocks $ S_{1},S_{2},\dots ,S_{r}$ is a shift matrix (possibly of different sizes). And the column transformations can not change this matrix to be skew-symmetric.","Suppose that is a skew-symmetric matrix, is a invertible matrix and is a nilpotent matrix. The unique example I could find is , the null matrix. My question: Does there exist another pair of matrices such that is skew-symmetric, is invertible and is nilpotent? I guess is the only possibility. Since is nilpotent, its Jordan canonical form is where each of the blocks is a shift matrix (possibly of different sizes). And the column transformations can not change this matrix to be skew-symmetric.","A d\times d B d\times d AB A=O A,B A B AB A=O AB \begin{bmatrix} 
   S_1 & 0 & \ldots & 0 \\ 
   0 & S_2 & \ldots & 0 \\
   \vdots & \vdots & \ddots & \vdots \\
   0 & 0 & \ldots & S_r 
\end{bmatrix}  S_{1},S_{2},\dots ,S_{r}","['linear-algebra', 'matrices', 'linear-transformations', 'matrix-decomposition']"
55,$\operatorname{tr}(A^k)=0 \space \forall k\in \Bbb{Z}^+$ implies $A$ is nilpotent. Does this imply $\operatorname {char}(K) =0$?,implies  is nilpotent. Does this imply ?,\operatorname{tr}(A^k)=0 \space \forall k\in \Bbb{Z}^+ A \operatorname {char}(K) =0,"$\mathcal{M}_{n}(K) $ : Set of all $n×n$ matrices over the field $K$ . $A\in \mathcal{M}_{n}(K) $ is called nilpotent if $A^k=\textbf{0}$ for some $k\in \Bbb{Z}^+$ It is clear that if $A$ is nilpotent then $\operatorname{tr}(A^k) =0$ for all $k\in \Bbb{Z}^+$ The converse is also true if $\operatorname{char}(K) =0$ My question: Suppose for a matrix $A\in\mathcal{M}_n(K)$ , $\operatorname{tr}(A^k)=0 \space  \forall k\in \Bbb{Z}^+ $ implies $A$ is nilpotent. Can we conclude that $\operatorname{char}(K) =0$ ?",": Set of all matrices over the field . is called nilpotent if for some It is clear that if is nilpotent then for all The converse is also true if My question: Suppose for a matrix , implies is nilpotent. Can we conclude that ?",\mathcal{M}_{n}(K)  n×n K A\in \mathcal{M}_{n}(K)  A^k=\textbf{0} k\in \Bbb{Z}^+ A \operatorname{tr}(A^k) =0 k\in \Bbb{Z}^+ \operatorname{char}(K) =0 A\in\mathcal{M}_n(K) \operatorname{tr}(A^k)=0 \space  \forall k\in \Bbb{Z}^+  A \operatorname{char}(K) =0,"['linear-algebra', 'abstract-algebra', 'matrices', 'soft-question', 'trace']"
56,Sub-blocks of matrix quadratic equations,Sub-blocks of matrix quadratic equations,,"Suppose the following matrix quadratic equation has at least one real solution for $X$ : $$ \begin{bmatrix}  	A & a \\ 	0 & 1\\ \end{bmatrix} X^2 + \begin{bmatrix}  	B & b \\ 	0 & 0\\ \end{bmatrix} X + \begin{bmatrix}  	C & c \\ 	0 & 0\\ \end{bmatrix} = 0, $$ where $A$ , $B$ and $C$ are real square matrices, $a$ , $b$ and $c$ are real vectors, and $X$ is a square matrix the same size as $\begin{bmatrix}  	A & a \\ 	0 & 1\\ \end{bmatrix}$ . Now consider the following matrix quadratic equation for $Y$ : $$A Y^2 + B Y + C = 0,$$ where $Y$ is a square matrix the same size as $A$ . Must this have a real solution for $Y$ ? If not, must this at least hold generically over all $A,B,C,a,b,c$ for which the first equation has a real solution for $X$ ? Note: In the case in which $A,B,C,a,b,c$ are all scalars, the solutions to the original equation are as follows: Either: $X_{1,1}$ is a solution to $A X_{1,1}^2+B X_{1,1}+C=0$ , $X_{2,1}=X_{2,2}=0$ , $X_{1,2}=-\frac{c}{AX_{1,1}+B}$ . OR $X_{1,1}=-\frac{C c}{B c-b C}$ , $X_{1,2}=-\frac{c^2}{B c-b C}$ , $X_{2,1}=-\frac{C^2}{B c-b C}$ , $X_{2,2}=\frac{C c}{B c-b C}$ . In the former case, my claim holds. In the latter case, there is no guarantee that it does (it seems).","Suppose the following matrix quadratic equation has at least one real solution for : where , and are real square matrices, , and are real vectors, and is a square matrix the same size as . Now consider the following matrix quadratic equation for : where is a square matrix the same size as . Must this have a real solution for ? If not, must this at least hold generically over all for which the first equation has a real solution for ? Note: In the case in which are all scalars, the solutions to the original equation are as follows: Either: is a solution to , , . OR , , , . In the former case, my claim holds. In the latter case, there is no guarantee that it does (it seems).","X 
\begin{bmatrix} 
	A & a \\
	0 & 1\\
\end{bmatrix} X^2 +
\begin{bmatrix} 
	B & b \\
	0 & 0\\
\end{bmatrix} X +
\begin{bmatrix} 
	C & c \\
	0 & 0\\
\end{bmatrix} = 0,
 A B C a b c X \begin{bmatrix} 
	A & a \\
	0 & 1\\
\end{bmatrix} Y A Y^2 + B Y + C = 0, Y A Y A,B,C,a,b,c X A,B,C,a,b,c X_{1,1} A X_{1,1}^2+B X_{1,1}+C=0 X_{2,1}=X_{2,2}=0 X_{1,2}=-\frac{c}{AX_{1,1}+B} X_{1,1}=-\frac{C c}{B c-b C} X_{1,2}=-\frac{c^2}{B c-b C} X_{2,1}=-\frac{C^2}{B c-b C} X_{2,2}=\frac{C c}{B c-b C}","['linear-algebra', 'matrices', 'quadratics', 'matrix-equations', 'quadratic-forms']"
57,Rank of limit points of a conjugacy class,Rank of limit points of a conjugacy class,,"Problem: Let $t \to P_t$ be a one parameter subgroup $\mathbb{C}^* \to \text{Gl}_{n}(\mathbb{C})$ . Let $X$ be a $n \times n$ nilpotent matrix. I want to show that if $lim_{t \to 0} P_t^{-1}XP_t = Y$ then $\text{rank}(X) \geq \text{rank}(Y)$ . I have checked that this is true if $X$ is in the Jordan canonical form and the one parameter subgroups are in diagonal form. Firstly in this case  we can assume that $X$ is indecomposable(hence rank is n-1) and so only one jordan block(since the actions are blockwise due to the nature of $P_t$ ), then if $P_t(i, i) = t^{i}$ , then the $Y$ as defined above has $t^{i_k +i_{k+1}}$ on the super diagonal entries and $0$ elsewhere. So, the rank is at most $n-1$ . But for an arbitrary nilpotent matrix I don't know how to show. We can make one simplification: if $Z$ is a nilpotent matrix then $Z  = A^{-1}XA$ where $X$ is in Jordan canonical form, but then we cannot assume that $P_t$ are diagonal and so on... So, either we prove that given $X$ in Jordan Canonical form and $P_t$ in arbitrary form.. or $X$ in arbitrary form and $P_t$ in special form that the rank cannot increase on taking limits. I am interested in this question since combined with fact that any point in the Zariski closure of a conjugacy class  in the affine variety of these nilpotent matrices is captured by a one parameter subgroup If $Y$ is in closure of $X$ then, $Y^2$ is in closure of $X^2$ and so on.... Partition(there is one partition corresponding ot every class) of $X$ majorizes partition of $Y$ iff $rank^{i}(X) \geq rank^{i}(Y)$ we would have shown that if $Y$ is in the closure of the conjugacy class of $X$ then the partition corresponding to $X$ majorizes that of $Y$ . I have shown the converse using one parameter subgroups and block matrix identitities here","Problem: Let be a one parameter subgroup . Let be a nilpotent matrix. I want to show that if then . I have checked that this is true if is in the Jordan canonical form and the one parameter subgroups are in diagonal form. Firstly in this case  we can assume that is indecomposable(hence rank is n-1) and so only one jordan block(since the actions are blockwise due to the nature of ), then if , then the as defined above has on the super diagonal entries and elsewhere. So, the rank is at most . But for an arbitrary nilpotent matrix I don't know how to show. We can make one simplification: if is a nilpotent matrix then where is in Jordan canonical form, but then we cannot assume that are diagonal and so on... So, either we prove that given in Jordan Canonical form and in arbitrary form.. or in arbitrary form and in special form that the rank cannot increase on taking limits. I am interested in this question since combined with fact that any point in the Zariski closure of a conjugacy class  in the affine variety of these nilpotent matrices is captured by a one parameter subgroup If is in closure of then, is in closure of and so on.... Partition(there is one partition corresponding ot every class) of majorizes partition of iff we would have shown that if is in the closure of the conjugacy class of then the partition corresponding to majorizes that of . I have shown the converse using one parameter subgroups and block matrix identitities here","t \to P_t \mathbb{C}^* \to \text{Gl}_{n}(\mathbb{C}) X n \times n lim_{t \to 0} P_t^{-1}XP_t = Y \text{rank}(X) \geq \text{rank}(Y) X X P_t P_t(i, i) = t^{i} Y t^{i_k +i_{k+1}} 0 n-1 Z Z  = A^{-1}XA X P_t X P_t X P_t Y X Y^2 X^2 X Y rank^{i}(X) \geq rank^{i}(Y) Y X X Y","['matrices', 'algebraic-groups', 'integer-partitions', 'nilpotence', 'affine-varieties']"
58,Book/Note recommendation for topology on spaces of matrices,Book/Note recommendation for topology on spaces of matrices,,"Recently I have faced two questions, which are solved in this website. One is to show that the space of orthogonal matrices is compact, another one is to show that the space of nilpotent matrices is connected. I want to study and solve problems like these on topological aspects on the set of matrices, but unable to find any book or notes specially on this topic. It will be beneficial if someone can suggest some materials. Thanks in advance!!","Recently I have faced two questions, which are solved in this website. One is to show that the space of orthogonal matrices is compact, another one is to show that the space of nilpotent matrices is connected. I want to study and solve problems like these on topological aspects on the set of matrices, but unable to find any book or notes specially on this topic. It will be beneficial if someone can suggest some materials. Thanks in advance!!",,"['general-topology', 'matrices', 'reference-request', 'book-recommendation', 'lecture-notes']"
59,Using diagonalisation of a symmetric on quadratic form,Using diagonalisation of a symmetric on quadratic form,,"Let $f (x, y) = 7x_2 + 4y_2 + 4xy$ . Use the diagonalisation of a symmetric matrix to write this quadratic form in the form $λ_1u_2^2 +λ_2v_2^2$ , with u and v linear combinations of x and y. Here's what I have tried: step 1. get the symmetric matrix $$f (x, y) = 7x_2 + 4y_2 + 4xy=\left(\begin{matrix}7&2\\2&4\\\end{matrix}\right)$$ step 2. find the eigenvalues $$det(A-\lambda I) \\ \left(\begin{matrix}7-\lambda&2\\2&4-\lambda\\\end{matrix}\right)=(7-\lambda)(4-\lambda)-4=(\lambda-3)(\lambda-8)$$ step 3. find the eigenvectors case for $\lambda =3$ $$\left(\begin{matrix}7-3&2\\2&4-3\\\end{matrix}\right)=\left(\begin{matrix}4&2\\2&1\\\end{matrix}\right)  $$ Row reduced $$\left(\begin{matrix}4&2\\2&1\\\end{matrix}\right)=\left(\begin{matrix}1&\frac{1}{2}\\0&0\\\end{matrix}\right) \implies \left(\begin{matrix}x\\y\\\end{matrix}\right)=y_1\left(\begin{matrix}-\frac{1}{2}\\1\\\end{matrix}\right)$$ Doing the same for $\lambda =8$ (i'll skip the calculation), I get $\left(\begin{matrix}x\\y\\\end{matrix}\right)=y_2\left(\begin{matrix}2\\1\\\end{matrix}\right)$ Finding the orthogonal eigenvectors $$u_1 = \frac{y_1}{||y_1||} = \frac{2}{\sqrt{5}}\left(\begin{matrix}-\frac{1}{2}\\1\\\end{matrix}\right) \\ u_2 = \frac{y_1}{||y_1||} = \frac{1}{\sqrt{5}}\left(\begin{matrix}2\\1\\\end{matrix}\right) \\ P=(u_1,u_2)=\left(\begin{matrix}-\frac{1}{\sqrt{5}}&\frac{2}{\sqrt{5}}\\\frac{2}{\sqrt{5}}&\frac{1}{\sqrt{5}}\\\end{matrix}\right) \\ D = P^TAP = \left(\begin{matrix}-\frac{1}{\sqrt{5}}&\frac{2}{\sqrt{5}}\\\frac{2}{\sqrt{5}}&\frac{1}{\sqrt{5}}\\\end{matrix}\right)\left(\begin{matrix}7&2\\2&4\\\end{matrix}\right)\left(\begin{matrix}-\frac{1}{\sqrt{5}}&\frac{2}{\sqrt{5}}\\\frac{2}{\sqrt{5}}&\frac{1}{\sqrt{5}}\\\end{matrix}\right) = \left(\begin{matrix}3&0\\0&8\\\end{matrix}\right) \\ x^TAx = \lambda^TD\lambda =  \left(\begin{matrix}\lambda_1&\lambda_2\\\end{matrix}\right)\left(\begin{matrix}3&0\\0&8\\\end{matrix}\right)\left(\begin{matrix}\lambda_1\\\lambda_2\\\end{matrix}\right) = 3\lambda_1^2+8\lambda_2^2$$ However, my result does not look like it's in the same form as the question: $λ_1u_2^2 +λ_2v_2^2 \ne  3\lambda_1^2+8\lambda_2^2$ , how do I proceed from here?","Let . Use the diagonalisation of a symmetric matrix to write this quadratic form in the form , with u and v linear combinations of x and y. Here's what I have tried: step 1. get the symmetric matrix step 2. find the eigenvalues step 3. find the eigenvectors case for Row reduced Doing the same for (i'll skip the calculation), I get Finding the orthogonal eigenvectors However, my result does not look like it's in the same form as the question: , how do I proceed from here?","f (x, y) = 7x_2 + 4y_2 + 4xy λ_1u_2^2 +λ_2v_2^2 f (x, y) = 7x_2 + 4y_2 + 4xy=\left(\begin{matrix}7&2\\2&4\\\end{matrix}\right) det(A-\lambda I) \\
\left(\begin{matrix}7-\lambda&2\\2&4-\lambda\\\end{matrix}\right)=(7-\lambda)(4-\lambda)-4=(\lambda-3)(\lambda-8) \lambda =3 \left(\begin{matrix}7-3&2\\2&4-3\\\end{matrix}\right)=\left(\begin{matrix}4&2\\2&1\\\end{matrix}\right) 
 \left(\begin{matrix}4&2\\2&1\\\end{matrix}\right)=\left(\begin{matrix}1&\frac{1}{2}\\0&0\\\end{matrix}\right) \implies \left(\begin{matrix}x\\y\\\end{matrix}\right)=y_1\left(\begin{matrix}-\frac{1}{2}\\1\\\end{matrix}\right) \lambda =8 \left(\begin{matrix}x\\y\\\end{matrix}\right)=y_2\left(\begin{matrix}2\\1\\\end{matrix}\right) u_1 = \frac{y_1}{||y_1||} = \frac{2}{\sqrt{5}}\left(\begin{matrix}-\frac{1}{2}\\1\\\end{matrix}\right) \\
u_2 = \frac{y_1}{||y_1||} = \frac{1}{\sqrt{5}}\left(\begin{matrix}2\\1\\\end{matrix}\right) \\
P=(u_1,u_2)=\left(\begin{matrix}-\frac{1}{\sqrt{5}}&\frac{2}{\sqrt{5}}\\\frac{2}{\sqrt{5}}&\frac{1}{\sqrt{5}}\\\end{matrix}\right) \\
D = P^TAP = \left(\begin{matrix}-\frac{1}{\sqrt{5}}&\frac{2}{\sqrt{5}}\\\frac{2}{\sqrt{5}}&\frac{1}{\sqrt{5}}\\\end{matrix}\right)\left(\begin{matrix}7&2\\2&4\\\end{matrix}\right)\left(\begin{matrix}-\frac{1}{\sqrt{5}}&\frac{2}{\sqrt{5}}\\\frac{2}{\sqrt{5}}&\frac{1}{\sqrt{5}}\\\end{matrix}\right) = \left(\begin{matrix}3&0\\0&8\\\end{matrix}\right) \\
x^TAx = \lambda^TD\lambda =  \left(\begin{matrix}\lambda_1&\lambda_2\\\end{matrix}\right)\left(\begin{matrix}3&0\\0&8\\\end{matrix}\right)\left(\begin{matrix}\lambda_1\\\lambda_2\\\end{matrix}\right) = 3\lambda_1^2+8\lambda_2^2 λ_1u_2^2 +λ_2v_2^2 \ne  3\lambda_1^2+8\lambda_2^2","['linear-algebra', 'matrices', 'diagonalization', 'quadratic-forms']"
60,"For Ax=B, where all variables are matrixes, If A is constant, and B'=1.05B, does x'=1.05x for Ax'=B'?","For Ax=B, where all variables are matrixes, If A is constant, and B'=1.05B, does x'=1.05x for Ax'=B'?",,"I feel like the answer to this is is quite a simple yes, but I was asked to prove it on a MATLAB script by solving for x on matrices of increasing sizes, and to my surprise, it was never exactly a 5% difference, and it seems the discrepancy grows as the number of elements in the array increases. What's going on here? Is it some sort of computing estimation quirk or is my understanding incorrect? Taking the question further, I also tested if all elements in B were given a maximum of a 5% error, multiplying them by random values between 0.95 and 1.05. My thought was that the error in x would be bounded by 5% as well, because a sort of weighted average of every error in B would be bounded between 1.05 and 0.95, but the error in this test grew exponentially with the size of the matrix, much faster than the previous test. Any explanation about what's going on here would be very appreciated.","I feel like the answer to this is is quite a simple yes, but I was asked to prove it on a MATLAB script by solving for x on matrices of increasing sizes, and to my surprise, it was never exactly a 5% difference, and it seems the discrepancy grows as the number of elements in the array increases. What's going on here? Is it some sort of computing estimation quirk or is my understanding incorrect? Taking the question further, I also tested if all elements in B were given a maximum of a 5% error, multiplying them by random values between 0.95 and 1.05. My thought was that the error in x would be bounded by 5% as well, because a sort of weighted average of every error in B would be bounded between 1.05 and 0.95, but the error in this test grew exponentially with the size of the matrix, much faster than the previous test. Any explanation about what's going on here would be very appreciated.",,"['linear-algebra', 'matrices', 'systems-of-equations', 'matlab', 'error-propagation']"
61,The Triple Pendulum,The Triple Pendulum,,"Triple Pendulum Consider the triple pendulum made out of three masses $m_1 = 2m$ and $m_2 = m_3 = m$ and attached on rods of negligible mass and the same length $\ell$ as depicted in the figure below and parameterized with the angles $\theta$ , $\phi$ and $\chi$ . For small angles $\mid \theta \mid, \mid \phi \mid, \mid \chi \mid\ll 1$ the Lagrangian is given (up to an irrelevant constant) by, $$L=\frac12 m \ell^2\left(2{\dot{\theta}}^2+\left({\dot{\theta}}+{\dot{\phi}}\right)^2+\left({\dot{\theta}}+{\dot{\chi}}\right)^2\right)-\frac12gm\ell\left(4\theta^2+\phi^2+\chi^2\right)\tag{1}$$ Show that we can perform a change of coordinates $(\theta, \phi, \chi)\to(q_1,q_2,q_3)$ so that the kinetic term is diagonalized and normalized, $T=({\dot q_1}^2+{\dot q_2}^2+{\dot q_3}^2)$ . In terms of the vector ${\bf {q}}=(q_1, q_2, q_3)$ , show that the Lagrangian can be expressed as follows, $$L=\frac12{\bf{\dot q}\cdot\bf{\dot q}}-\frac12{\bf qkq}\tag{2}$$ and find an expression for the matrix $\bf k$ . So in $(1)$ , I will let ${\dot{q_1}}^2=2m{\ell}^2{\dot{\theta}}^2$ , ${\dot{q_2}}^2=m{\ell}^2(\dot{\theta}+{\dot{\phi}})^2$ & $\,{\dot{q_3}}^2=m{\ell}^2(\dot{\theta}+{\dot{\chi}})^2$ . Inverting these in favour of the angles gives $$\dot \theta=\frac{\dot{q_1}}{\sqrt{2m}{\ell}}\tag{a}$$ $$\dot \theta +\dot \phi=\frac{\dot{q_2}}{\sqrt m\ell}\tag{b}$$ $$\dot \theta +\dot \chi=\frac{\dot{q_3}}{\sqrt m\ell}\tag{c}$$ Insertion of $(\mathrm{a})$ , $(\mathrm{b})$ & $(\mathrm{c})$ into the first (kinetic) term of eqn $(1)$ gives $$T=\frac12m\ell^2\left[2\frac{{\dot{q_1}}^2}{2m\ell^2}+\frac{{\dot{q_2}}^2}{m\ell^2}+\frac{{\dot{q_3}}^2}{m\ell^2}\right]=\frac12\left[{\dot{q_1}}^2+{\dot{q_2}}^2+{\dot{q_3}}^2\right]=\frac12{\bf{\dot q}\cdot\bf{\dot q}}\tag{3}$$ So I can correctly get the kinetic, ( $T$ ) part of eqn $(2)$ , now using the same method to get the potential part ( $V$ ), from $(\mathrm{a})$ , $(\mathrm{b})$ & $(\mathrm{c})$ it follows that $$\theta=\frac{{q_1}}{\sqrt{2m}{\ell}}\tag{d}$$ $$\phi=\frac{{q_2}}{\sqrt{m}{\ell}}-\frac{{q_1}}{\sqrt{2m}{\ell}}=\frac{1}{\sqrt{m}\ell}\left(q_2-\frac{q_1}{\sqrt2}\right)\tag{e}$$ $$\chi=\frac{{q_3}}{\sqrt{m}{\ell}}-\frac{{q_1}}{\sqrt{2m}{\ell}}=\frac{1}{\sqrt{m}\ell}\left(q_3-\frac{q_1}{\sqrt2}\right)\tag{f}$$ From $(\mathrm{d})$ , $(\mathrm{e})$ & $(\mathrm{f})$ it follows that $$\theta^2=\frac{{{q_1}^2}}{2m{\ell^2}}\tag{g}$$ $$\phi^2=\frac{1}{m{\ell^2}}\left(q_2-\frac{q_1}{\sqrt2}\right)\left(q_2-\frac{q_1}{\sqrt2}\right)=\frac{1}{m\ell^2}\left({q_2}^2-\sqrt 2q_1q_2+\frac12{q_1}^2\right)\tag{h}$$ $$\chi^2=\frac{1}{m{\ell^2}}\left(q_3-\frac{q_1}{\sqrt2}\right)\left(q_3-\frac{q_1}{\sqrt2}\right)=\frac{1}{m\ell^2}\left({q_3}^2-\sqrt 2q_1q_3+\frac12{q_1}^2\right)\tag{i}$$ Now, insertion of $(\mathrm{g})$ , $(\mathrm{h})$ & $(\mathrm{i})$ into the second term of $(1)$ , $V=\frac12gm\ell\left(4\theta^2+\phi^2+\chi^2\right)$ , transforms to $$\begin{align}V&=\frac{gm\ell}{2m{\ell}^2}\left(4\frac{{q_1}^2}{2}+{q_2}^2-\sqrt 2q_1q_2+\frac12{q_1}^2+{q_3}^2-\sqrt 2q_1q_3+\frac12{q_1}^2\right)\\&=\frac12\frac{g}{\ell}\left(2{q_1}^2+{q_1}^2+{q_2}^2+{q_3}^2-\sqrt 2q_1q_2-\sqrt 2q_1q_3\right)\\&=\frac{g}{2\ell}\left(3{q_1}^2+{q_2}^2+{q_3}^2-\sqrt 2q_1\left(q_2+q_3\right)\right)\tag{4}\end{align}$$ I have really laboured the point with the algebra here, but I did so on purpose so it would be easy to identify errors. This is as far as I can get, and it doesn't look anything like the second term of eqn $(2)$ - $\frac12{\bf qkq}$ . But I can tell you that the last expression (for $V$ ) is correct as I have the author's solution below: I have some questions regarding this solution. Why is the authors eqn $(29)$ different from my eqn $(3)$ and what is meant by ""the kinetic term is diagonal""? My equation $(4)$ matches with the author's equation $(33)$ which is good. But how on earth do you deduce the kinetic matrix (sometimes known as 'mass matrix'), $\bf k$ from this? Taking the matrix $\bf k$ as given for a moment, if ${\bf {q}}=(q_1, q_2, q_3)$ is a row vector, then the only way to compute $\bf qkq$ as matrix multiplication is to write it as ${\bf q}{\bf k}{\bf q}^T$ because $${\bf q}{\bf k}{\bf q}^T=\frac{g}{\ell}\begin{pmatrix}q_1 & q_2 &  q_3\end{pmatrix}\begin{pmatrix} 3 & -1/\sqrt2 & -1/\sqrt2 \\ -1/\sqrt2 & 1 & 0 \\ -1/\sqrt2 & 0 & 1 \\ \end{pmatrix}\begin{pmatrix}q_1 \\ q_2 \\  q_3\end{pmatrix} $$ Which is in the format required for matrix multiplication (row times column) as from left to right we have matrices with (1x3) times (3x3) times (3x1) with the result being a (1x1) matrix (scalar) as required by equation $(33)$ or my $(4)$ . So why is $(2)$ not being written as ${\bf q}{\bf k}{\bf q}^T$ ?","Triple Pendulum Consider the triple pendulum made out of three masses and and attached on rods of negligible mass and the same length as depicted in the figure below and parameterized with the angles , and . For small angles the Lagrangian is given (up to an irrelevant constant) by, Show that we can perform a change of coordinates so that the kinetic term is diagonalized and normalized, . In terms of the vector , show that the Lagrangian can be expressed as follows, and find an expression for the matrix . So in , I will let , & . Inverting these in favour of the angles gives Insertion of , & into the first (kinetic) term of eqn gives So I can correctly get the kinetic, ( ) part of eqn , now using the same method to get the potential part ( ), from , & it follows that From , & it follows that Now, insertion of , & into the second term of , , transforms to I have really laboured the point with the algebra here, but I did so on purpose so it would be easy to identify errors. This is as far as I can get, and it doesn't look anything like the second term of eqn - . But I can tell you that the last expression (for ) is correct as I have the author's solution below: I have some questions regarding this solution. Why is the authors eqn different from my eqn and what is meant by ""the kinetic term is diagonal""? My equation matches with the author's equation which is good. But how on earth do you deduce the kinetic matrix (sometimes known as 'mass matrix'), from this? Taking the matrix as given for a moment, if is a row vector, then the only way to compute as matrix multiplication is to write it as because Which is in the format required for matrix multiplication (row times column) as from left to right we have matrices with (1x3) times (3x3) times (3x1) with the result being a (1x1) matrix (scalar) as required by equation or my . So why is not being written as ?","m_1 = 2m m_2 = m_3 = m \ell \theta \phi \chi \mid \theta \mid, \mid \phi \mid, \mid \chi \mid\ll 1 L=\frac12 m \ell^2\left(2{\dot{\theta}}^2+\left({\dot{\theta}}+{\dot{\phi}}\right)^2+\left({\dot{\theta}}+{\dot{\chi}}\right)^2\right)-\frac12gm\ell\left(4\theta^2+\phi^2+\chi^2\right)\tag{1} (\theta, \phi, \chi)\to(q_1,q_2,q_3) T=({\dot q_1}^2+{\dot q_2}^2+{\dot q_3}^2) {\bf {q}}=(q_1, q_2, q_3) L=\frac12{\bf{\dot q}\cdot\bf{\dot q}}-\frac12{\bf qkq}\tag{2} \bf k (1) {\dot{q_1}}^2=2m{\ell}^2{\dot{\theta}}^2 {\dot{q_2}}^2=m{\ell}^2(\dot{\theta}+{\dot{\phi}})^2 \,{\dot{q_3}}^2=m{\ell}^2(\dot{\theta}+{\dot{\chi}})^2 \dot \theta=\frac{\dot{q_1}}{\sqrt{2m}{\ell}}\tag{a} \dot \theta +\dot \phi=\frac{\dot{q_2}}{\sqrt m\ell}\tag{b} \dot \theta +\dot \chi=\frac{\dot{q_3}}{\sqrt m\ell}\tag{c} (\mathrm{a}) (\mathrm{b}) (\mathrm{c}) (1) T=\frac12m\ell^2\left[2\frac{{\dot{q_1}}^2}{2m\ell^2}+\frac{{\dot{q_2}}^2}{m\ell^2}+\frac{{\dot{q_3}}^2}{m\ell^2}\right]=\frac12\left[{\dot{q_1}}^2+{\dot{q_2}}^2+{\dot{q_3}}^2\right]=\frac12{\bf{\dot q}\cdot\bf{\dot q}}\tag{3} T (2) V (\mathrm{a}) (\mathrm{b}) (\mathrm{c}) \theta=\frac{{q_1}}{\sqrt{2m}{\ell}}\tag{d} \phi=\frac{{q_2}}{\sqrt{m}{\ell}}-\frac{{q_1}}{\sqrt{2m}{\ell}}=\frac{1}{\sqrt{m}\ell}\left(q_2-\frac{q_1}{\sqrt2}\right)\tag{e} \chi=\frac{{q_3}}{\sqrt{m}{\ell}}-\frac{{q_1}}{\sqrt{2m}{\ell}}=\frac{1}{\sqrt{m}\ell}\left(q_3-\frac{q_1}{\sqrt2}\right)\tag{f} (\mathrm{d}) (\mathrm{e}) (\mathrm{f}) \theta^2=\frac{{{q_1}^2}}{2m{\ell^2}}\tag{g} \phi^2=\frac{1}{m{\ell^2}}\left(q_2-\frac{q_1}{\sqrt2}\right)\left(q_2-\frac{q_1}{\sqrt2}\right)=\frac{1}{m\ell^2}\left({q_2}^2-\sqrt 2q_1q_2+\frac12{q_1}^2\right)\tag{h} \chi^2=\frac{1}{m{\ell^2}}\left(q_3-\frac{q_1}{\sqrt2}\right)\left(q_3-\frac{q_1}{\sqrt2}\right)=\frac{1}{m\ell^2}\left({q_3}^2-\sqrt 2q_1q_3+\frac12{q_1}^2\right)\tag{i} (\mathrm{g}) (\mathrm{h}) (\mathrm{i}) (1) V=\frac12gm\ell\left(4\theta^2+\phi^2+\chi^2\right) \begin{align}V&=\frac{gm\ell}{2m{\ell}^2}\left(4\frac{{q_1}^2}{2}+{q_2}^2-\sqrt 2q_1q_2+\frac12{q_1}^2+{q_3}^2-\sqrt 2q_1q_3+\frac12{q_1}^2\right)\\&=\frac12\frac{g}{\ell}\left(2{q_1}^2+{q_1}^2+{q_2}^2+{q_3}^2-\sqrt 2q_1q_2-\sqrt 2q_1q_3\right)\\&=\frac{g}{2\ell}\left(3{q_1}^2+{q_2}^2+{q_3}^2-\sqrt 2q_1\left(q_2+q_3\right)\right)\tag{4}\end{align} (2) \frac12{\bf qkq} V (29) (3) (4) (33) \bf k \bf k {\bf {q}}=(q_1, q_2, q_3) \bf qkq {\bf q}{\bf k}{\bf q}^T {\bf q}{\bf k}{\bf q}^T=\frac{g}{\ell}\begin{pmatrix}q_1 & q_2 &  q_3\end{pmatrix}\begin{pmatrix}
3 & -1/\sqrt2 & -1/\sqrt2 \\
-1/\sqrt2 & 1 & 0 \\
-1/\sqrt2 & 0 & 1 \\
\end{pmatrix}\begin{pmatrix}q_1 \\ q_2 \\  q_3\end{pmatrix}
 (33) (4) (2) {\bf q}{\bf k}{\bf q}^T","['matrices', 'mathematical-physics', 'classical-mechanics']"
62,Tower of Hanoi sequence via eigendecomposition,Tower of Hanoi sequence via eigendecomposition,,"The following sequence comes from the Tower of Hanoi . $$ 1, 3, 7, 15, \dots $$ Find the $64$ -th term using eigendecomposition. By general pattern, I know that the answer is obviously $2^{64} -1$ . However, I am asked to solve using eigendecomposition. $$ T(n) = 2^n - 1 $$ Let $F_k$ be the terms of the sequence. $$F_{k+1} = 2 F_k+1$$ In matrix form.. $$\begin{bmatrix} F_{k+1} \\F_k\end{bmatrix} = \begin{bmatrix} 2&1 \\1&0\end{bmatrix}\begin{bmatrix} F_{k} \\1\end{bmatrix}$$ Please help me continue.","The following sequence comes from the Tower of Hanoi . Find the -th term using eigendecomposition. By general pattern, I know that the answer is obviously . However, I am asked to solve using eigendecomposition. Let be the terms of the sequence. In matrix form.. Please help me continue."," 1, 3, 7, 15, \dots  64 2^{64} -1  T(n) = 2^n - 1  F_k F_{k+1} = 2 F_k+1 \begin{bmatrix} F_{k+1} \\F_k\end{bmatrix} = \begin{bmatrix} 2&1 \\1&0\end{bmatrix}\begin{bmatrix} F_{k} \\1\end{bmatrix}","['linear-algebra', 'sequences-and-series', 'matrices', 'eigenvalues-eigenvectors', 'recurrence-relations']"
63,Symmetric matrix with positive entries being invertible?,Symmetric matrix with positive entries being invertible?,,"Let $u_1,\dots,u_n\in S^{n-1}$ be $n$ linearly independent unit vectors (thus a basis for $\mathbb{R}^n$ ). Let $a_{ij}:=u_i^\top u_j$ . Then $a_{ii}=1$ and $a_{ij}=a_{ji}$ . My question is: Is the following matrix always invertible $$A=(a_{ij}^2)_{1\leq i,j\leq n}$$ For $n=2$ it is, but for general $n$ I think there are counterexamples. Since the entrywise squareroot of $A$ is invertible, I guess we can start with some invertible(maybe unitary?) matrix $P$ with $P^2$ (entrywise) not invertible, and then modify it to find the corresponding vectors. But I failed to modify it to find such unit vectors...","Let be linearly independent unit vectors (thus a basis for ). Let . Then and . My question is: Is the following matrix always invertible For it is, but for general I think there are counterexamples. Since the entrywise squareroot of is invertible, I guess we can start with some invertible(maybe unitary?) matrix with (entrywise) not invertible, and then modify it to find the corresponding vectors. But I failed to modify it to find such unit vectors...","u_1,\dots,u_n\in S^{n-1} n \mathbb{R}^n a_{ij}:=u_i^\top u_j a_{ii}=1 a_{ij}=a_{ji} A=(a_{ij}^2)_{1\leq i,j\leq n} n=2 n A P P^2","['linear-algebra', 'matrices']"
64,Rotated coordinates on a unit sphere,Rotated coordinates on a unit sphere,,"Given three points on a unit sphere, their coordinates are $p_1 = [\phi_1, \theta_1]$ , $p_2 = [\phi_2, \theta_2]$ , and $p_3 = [\phi_3, \theta_3]$ , where $\phi$ and $\theta$ are azimuthal angle and polar angle, respectively. Rotate about the sphere center so that $p_1$ locates at $[0, 0]$ and $p_2$ at $[0, l]$ (obviously, $l$ equals to the great-arc distance between $p_1$ and $p_2$ ). On the rotated sphere, what is the new coordinates $[\phi, \theta]$ for $p_3$ ?","Given three points on a unit sphere, their coordinates are , , and , where and are azimuthal angle and polar angle, respectively. Rotate about the sphere center so that locates at and at (obviously, equals to the great-arc distance between and ). On the rotated sphere, what is the new coordinates for ?","p_1 = [\phi_1, \theta_1] p_2 = [\phi_2, \theta_2] p_3 = [\phi_3, \theta_3] \phi \theta p_1 [0, 0] p_2 [0, l] l p_1 p_2 [\phi, \theta] p_3","['matrices', 'geometry', 'linear-transformations', 'spherical-coordinates', 'spherical-geometry']"
65,How do you deduce that matrices are equal,How do you deduce that matrices are equal,,"i wanted to ask for a clarification. I was looking around in my linear algebra text when i reached this justification: Considered two coloumn vectors $X$ and $Y$ , and assume $$X ^tC Y = X^t C^t Y$$ for every $X,Y \in V$ , with $V$ an $n$ -dimensional vectorial space, with $X^t, Y^t$ being the transposed of $ X, Y$ . My book says that because of this is valid for every $X,Y$ , we can deduce: $$ C^t = C$$ Now, it is intuitively true, but i was wondering if ,maybe the general sum (it's a bilinear form) could equals without needing $C^t = C$ . I've seen this type of justification also in other theorems, but i want to know if there is a way to prove it formally, beacuse i'm not satisfied. Thanks in advance.","i wanted to ask for a clarification. I was looking around in my linear algebra text when i reached this justification: Considered two coloumn vectors and , and assume for every , with an -dimensional vectorial space, with being the transposed of . My book says that because of this is valid for every , we can deduce: Now, it is intuitively true, but i was wondering if ,maybe the general sum (it's a bilinear form) could equals without needing . I've seen this type of justification also in other theorems, but i want to know if there is a way to prove it formally, beacuse i'm not satisfied. Thanks in advance.","X Y X ^tC Y = X^t C^t Y X,Y \in V V n X^t, Y^t  X, Y X,Y  C^t = C C^t = C","['linear-algebra', 'matrices']"
66,Show that the matrix is a generator matrix of a MDS code,Show that the matrix is a generator matrix of a MDS code,,"Let $a_1,\dots,a_n$ be pairwise distinct elements of $\mathbb{F}_q$ and $k ≤ n$ . I have to show that the matrix $\begin{bmatrix}1&\dots&1&0\\a_1&\dots&a_n&0\\a_1^2&\dots&a_n^2&0\\\vdots&&\vdots&0\\a_1^{k-1}&\dots&a_n^{k-1}&1\end{bmatrix}$ is a generator matrix of an $[n+1,k,n−k+2]$ MDS code. if q is a power of 2, then the matrix $\begin{bmatrix}1&\dots&1&0&0\\a_1&\dots&a_q&1&0\\a_1^2&\dots&a_q^2&0&1\end{bmatrix}$ is a generator matrix of an $[q + 2, 3, q]$ MDS code. I don't even know where to start, the things we covered in the lecture are not so many, but what I thought could be useful, but don't know how to apply are: Theorem: Let $C$ be an $[n,k]$ code with generator matrix $G$ and parity check matrix $H$ . The following are equivalent: $C$ is MDS All $(n-k)\times(n-k)$ full size minors of $H$ are invertible All $k\times k$ full size minors og G are invertible Maybe I could use this theorem, part (3), but I'm not sure how to show that all the minors og G are invertible. Should I maybe, from definition of MDS, show that the distance is indeed $d(C)=n-k+2$ , and then conclude that the code is MDS? But aren't the elements arbitrary? Should I maybe look for a parity check matrix $H$ and do something? Please any hint is appreciated.","Let be pairwise distinct elements of and . I have to show that the matrix is a generator matrix of an MDS code. if q is a power of 2, then the matrix is a generator matrix of an MDS code. I don't even know where to start, the things we covered in the lecture are not so many, but what I thought could be useful, but don't know how to apply are: Theorem: Let be an code with generator matrix and parity check matrix . The following are equivalent: is MDS All full size minors of are invertible All full size minors og G are invertible Maybe I could use this theorem, part (3), but I'm not sure how to show that all the minors og G are invertible. Should I maybe, from definition of MDS, show that the distance is indeed , and then conclude that the code is MDS? But aren't the elements arbitrary? Should I maybe look for a parity check matrix and do something? Please any hint is appreciated.","a_1,\dots,a_n \mathbb{F}_q k ≤ n \begin{bmatrix}1&\dots&1&0\\a_1&\dots&a_n&0\\a_1^2&\dots&a_n^2&0\\\vdots&&\vdots&0\\a_1^{k-1}&\dots&a_n^{k-1}&1\end{bmatrix} [n+1,k,n−k+2] \begin{bmatrix}1&\dots&1&0&0\\a_1&\dots&a_q&1&0\\a_1^2&\dots&a_q^2&0&1\end{bmatrix} [q + 2, 3, q] C [n,k] G H C (n-k)\times(n-k) H k\times k d(C)=n-k+2 H","['abstract-algebra', 'matrices', 'discrete-mathematics', 'finite-fields', 'coding-theory']"
67,What is the motivation and intuition behind the symplectic form?,What is the motivation and intuition behind the symplectic form?,,"That's the definition we got for the symplectic form: Let $$\omega : \: \mathbb{C}^n \times \mathbb{C}^n \rightarrow \mathbb{C}$$ be a bilinear, anti-symmetric and non-degenerate ( $\forall_{y \in \mathbb{C}^n} \: \omega(x,y)=0 \: \Rightarrow \: x=0$ ) map. Let $e_1, ..., e_n$ be the base of $\mathbb{C}^n$ which means, that for $x, y \in \mathbb{C}^n$ we got $x = \sum x_i e_i$ , $y = \sum y_i e_i$ Then, our symplectic form is equal to $$\omega(x,y) = x^T \Omega y$$ My question here is: What does this symplectic form intuitively tell us? How can I understand it in an intuitive way (and not abstract like here)?","That's the definition we got for the symplectic form: Let be a bilinear, anti-symmetric and non-degenerate ( ) map. Let be the base of which means, that for we got , Then, our symplectic form is equal to My question here is: What does this symplectic form intuitively tell us? How can I understand it in an intuitive way (and not abstract like here)?","\omega : \: \mathbb{C}^n \times \mathbb{C}^n \rightarrow \mathbb{C} \forall_{y \in \mathbb{C}^n} \: \omega(x,y)=0 \: \Rightarrow \: x=0 e_1, ..., e_n \mathbb{C}^n x, y \in \mathbb{C}^n x = \sum x_i e_i y = \sum y_i e_i \omega(x,y) = x^T \Omega y","['matrices', 'geometry', 'differential-geometry', 'symplectic-geometry', 'bilinear-form']"
68,How many triples of mutually conjugate linear maps are there?,How many triples of mutually conjugate linear maps are there?,,"I’m looking for finite-dimensional complex vector spaces $V$ with three invertible linear maps $r,s,t:V\to V$ , satisfying the following “mutual-conjugation” condition: $rsr^{-1}=t$ $sts^{-1}=r$ $trt^{-1}=s$ For lack of a better term, I’m calling them conj-representations . In order to avoid redundancies, say: Two conj-reps $V, V’$ are isomorphic if there is a linear isomorphism $f$ between them respecting the actions of the three maps (i.e. $fr=r’f$ ). A conj-rep is irreducible if it has no subconj-rep (i.e. the three maps do not restrict to a proper subspace). Question: how many isomorphism classes of irreducible conj-reps are there? Example: take $V=\mathrm{Span}(e_1,e_2,e_3)$ and: $r$ permutes $e_1 \leftrightarrow e_2$ and fixes $e_3$ . $s$ permutes $e_2 \leftrightarrow e_3$ and fixes $e_1$ . $t$ permutes $e_3 \leftrightarrow e_1$ and fixes $e_2$ . Note that this is not irreducible, as it has subconj-reps $\mathrm{Span}(e_1+e_2+e_3)$ and $\mathrm{Span}(e_1-e_2,e_2-e_3)$ . Another $1$ -dimensional example: $r, s, t$ all act as $-1$ . I can prove that if all three maps are involutions, then these are all the conj-reps. I’m looking for examples where the maps are not involutions. Also, it’s not hard to show all $1$ -dimensional ones must be of the form $r=s=t=\lambda \mathrm{Id}$ . In general, you can always scale a conj-rep, so I mean up to scalars.","I’m looking for finite-dimensional complex vector spaces with three invertible linear maps , satisfying the following “mutual-conjugation” condition: For lack of a better term, I’m calling them conj-representations . In order to avoid redundancies, say: Two conj-reps are isomorphic if there is a linear isomorphism between them respecting the actions of the three maps (i.e. ). A conj-rep is irreducible if it has no subconj-rep (i.e. the three maps do not restrict to a proper subspace). Question: how many isomorphism classes of irreducible conj-reps are there? Example: take and: permutes and fixes . permutes and fixes . permutes and fixes . Note that this is not irreducible, as it has subconj-reps and . Another -dimensional example: all act as . I can prove that if all three maps are involutions, then these are all the conj-reps. I’m looking for examples where the maps are not involutions. Also, it’s not hard to show all -dimensional ones must be of the form . In general, you can always scale a conj-rep, so I mean up to scalars.","V r,s,t:V\to V rsr^{-1}=t sts^{-1}=r trt^{-1}=s V, V’ f fr=r’f V=\mathrm{Span}(e_1,e_2,e_3) r e_1 \leftrightarrow e_2 e_3 s e_2 \leftrightarrow e_3 e_1 t e_3 \leftrightarrow e_1 e_2 \mathrm{Span}(e_1+e_2+e_3) \mathrm{Span}(e_1-e_2,e_2-e_3) 1 r, s, t -1 1 r=s=t=\lambda \mathrm{Id}","['linear-algebra', 'abstract-algebra', 'matrices', 'representation-theory', 'nonassociative-algebras']"
69,A question about skew-symmetric matrix.,A question about skew-symmetric matrix.,,"I have no idea how to prove/disprove this Statement. Statement: Let $K$ be a field of arbitrary characteristics. Let $X$ be a skew-symmetric $(n \times n)-$ matrix over $K$ and $H=(h_{ij})$ be skew-symmetric with $$h_{ij} = \begin{cases} 0,  & i=j \\[2ex] 1, & i >j \\[2ex] -1 & i<j \\ \end{cases}$$ Then there is $Q$ such that $X=QHQ^T.$ Any help is appreciated. Thank you!",I have no idea how to prove/disprove this Statement. Statement: Let be a field of arbitrary characteristics. Let be a skew-symmetric matrix over and be skew-symmetric with Then there is such that Any help is appreciated. Thank you!,"K X (n \times n)- K H=(h_{ij}) h_{ij} =
\begin{cases}
0,  & i=j \\[2ex]
1, & i >j \\[2ex]
-1 & i<j \\
\end{cases} Q X=QHQ^T.","['linear-algebra', 'matrices', 'skew-symmetric-matrices']"
70,Solving a quadratic equation problem with two variables,Solving a quadratic equation problem with two variables,,"This is a post of two three problems regarding the method to solve bivariate quadratic equations. In brief, How does the elimination happen here . Or, how is the elimination used? ( Update, I know how this is done now. See update below ) Is the method below viable? If not, how to solve this problem in matrix format? (Update) Nope, this method below is not viable because rank 2 matrices cannot be factorized. And an alterative matrix method can be seen in chapter 11 of this book . (Update) I also post a method. Is it valid and complete? In order to be complete, some measurements should be done to avoid the case where there are no solutions (i.e., conics don't intersect) The very source of the problem is: $\begin{cases} a_1x^2 + b_1y^2 + c_1xy + d_1x + e_1y + f_1 = 0 \\ a_2x^2 + b_2y^2 + c_2xy + d_2x + e_2y + f_2 = 0  \end{cases}$ (Problem 1) I find a method here , but it does not tell me how the elimination work. How did the elimination work? (Backgrounds of problem 2) I heard that this problem also can be solved by matrices: Say we convert the equations into $\begin{cases} X^TAX = 0 \\ X^TBX = 0 \\ X = \begin{bmatrix}x &y &1\end{bmatrix}^T \end{cases}$ where $A$ and $B$ are symmetric and are obtained from the two equations respectively. And if $rank(A) = 3$ and $rank(B) =3$ (which happens most of the cases for randomly generated real symmetric matrices), to find the solution, one should find the eigenvalue of $A^{-1}B$ , i.e., $[V, D] = eigs(A^{-1}B)$ , where $V$ is the eigenvector matrix and $D$ is the eigenvalue matrix. Then we can always find $ X^T(\lambda{A} - B)X = 0$ , where $\lambda$ is the diagonal value of matrix $D$ . ( $\lambda{A} - B$ ) is guaranteed to have a rank of lower than 3 since its determinant is $0$ , and either it can be rank $1$ or rank $2$ . Let's call it $C$ . Then I do another diagonalization, such that $X^TCX = X^TP^{-1} D_C PX = X^TP^T D_C PX = (PX)^TD_C(PX) = PX \cdot D_CPX$ , where $D_C$ is the diagonal matrix for $C$ , and $P$ is its corresponding  eigenvector matrix. Assume $D_C = \begin{bmatrix} \lambda_{C1} & 0 & 0 \\ 0 & \lambda_{C2} & 0 \\ 0 & 0 & \lambda_{C3} \end{bmatrix}$ , $P= \begin{bmatrix} p_1 & p_4 & p_7 \\ p_2 & p_5 & p_8 \\ p_3 & p_6  & p_9 \end{bmatrix}$ , $\lambda_{C3} = 0$ Then the final result is something like: $\lambda_{C1} (p_1 x + p_2 y + p_3)^2 + \lambda_{C2} (p_4 x + p_5 y + p_6)^2 = 0$ (Problem 2) is the following true? If yes, prove it and express the equation in a factorized format (You can use any notation if you want). If not, how to solve in matrix format? The rank of matrix $C$ , as I tested with Matlab scripts, can be either $1$ or $2$ . In that post (I don't know where it is now), he says each eigenvalue corresponds to a $C$ , and $X^TCX=0$ corresponds to a factorized quadratic equation so that you can find $3$ equations that has the format of: $\begin{cases} Eq_1 = (m_{11}x+m_{12}y+m_{13})(m_{14}x+m_{15}y+m_{16}) = 0 \\ Eq_2 = (m_{21}x+m_{22}y+m_{23})(m_{24}x+m_{25}y+m_{26}) = 0 \\ Eq_3 = (m_{31}x+m_{32}y+m_{33})(m_{34}x+m_{35}y+m_{36}) = 0 \\  \end{cases}$ And $Eq_3$ can be removed because ""it is not telling anything new"" and can be represented by $Eq_1$ and $Eq_2$ . There are $4$ solutions in total by solving the systems of equations below: Solve $(x_1, y_1)\begin{cases} m_{11}x+m_{12}y+m_{13} = 0 \\ m_{21}x+m_{22}y+m_{23} = 0 \end{cases}$ Solve $(x_2, y_2) \begin{cases} m_{11}x+m_{12}y+m_{13} = 0 \\ m_{24}x+m_{25}y+m_{26} = 0 \end{cases}$ Solve $(x_3, y_3) \begin{cases} m_{14}x+m_{15}y+m_{16} = 0 \\ m_{21}x+m_{22}y+m_{23} = 0 \end{cases}$ Solve $(x_4, y_4) \begin{cases} m_{14}x+m_{15}y+m_{16} = 0 \\ m_{24}x+m_{25}y+m_{26} = 0 \end{cases}$ (Edited for some grammatical problem) Any $(m_{1}x+m_{2}y+m_{3})(m_{4}x+m_{5}y+m_{6})$ is not different from $ =  \begin{bmatrix} x &y & 1\end{bmatrix} \begin{bmatrix} m_1 \\ m_2 \\ m_3\end{bmatrix} \begin{bmatrix} m_4 & m_5 & m_6\end{bmatrix} \begin{bmatrix} x\\y\\1\end{bmatrix} =  \begin{bmatrix} x &y & 1\end{bmatrix} \begin{bmatrix}  m_1 m_4 & m_1 m_5 & m_1 m_6 \\  m_2 m_4 & m_2 m_5 & m_2 m_6 \\  m_3 m_4 & m_3 m_5 & m_3 m_6 \\  \end{bmatrix} \begin{bmatrix} x\\y\\1\end{bmatrix} $ The method is proved to be invalid. But can someone solve it with matrix form? (Edited for proof) I find a solution in Perspectives on Projective Geometry by Jurgen Richter-Gebert. The process is seen in chapter 11 of this book . (Edited for finding one complete solution) Okay, I am still not satisfied. So I want to make a method as well. It seems like we can always do the following step because eigenvalues of symmetric matrices are always real. $C_{12} = \sqrt{\lambda_{C1}} (p_1 x + p_2 y + p_3) = \pm\sqrt{-\lambda_{C2}} (p_4 x + p_5 y + p_6)$ The $_{12}$ notation above means there are two equations ( $C1$ and $C2$ , plus and minus respectively) written into one. And with three eigenvalues we will have three groups of equations that look like this below (Shown two, but actually three). After some work, both equations can always be written as $\begin{bmatrix}c_1 & c_2 & c_3\end{bmatrix}$ $\begin{bmatrix}x \\ y \\ 1\end{bmatrix}$ = 0, which represents a line. $C_1$ and $C_2$ represent two branches of solution. Then, for a different eigenvalue, we will have $C_{34}$ ( $C_3$ and $C_4$ ). Similarly, $C_3$ and $C_4$ represent two branches of solution. A set of solution can be obtained from, similar to the last method, $\begin{cases} C_1 = 0 \\ C_3 = 0 \end{cases}\rightarrow (x_1, y_1)$ $\begin{cases} C_1 = 0 \\ C_4 = 0 \end{cases}\rightarrow (x_2, y_2)$ $\begin{cases} C_2 = 0 \\ C_3 = 0 \end{cases}\rightarrow (x_3, y_3)$ $\begin{cases} C_2 = 0 \\ C_4 = 0 \end{cases}\rightarrow (x_4, y_4)$ For combination of different ranks of matrices, the strategy is about finding two degenerate combinations ( $\det(\lambda_1 A + \mu B)$ ) such that we have two groups of lines. If both full rank (i.e., the combinations of $(\lambda , \mu)$ should not have a zero in it), we need to use two different eigenvalues of one matrix and establish two new degenerate cases. If one is full rank and one is singular (i.e., one of the combination of $(\lambda , \mu)$ has a zero in it), then we only need one more degenerate case. If both are singular, then we already have two degenerate cases. (In the combination, two of them should be $(something_1, 0)$ and $(0, something_2)$ ). Question 3: Why can we just use two of the three will be fine for both full rank cases? I did not prove this yet, because I am new to this question (saw it 3 days ago). But with some educated guess, just imagine the solutions are the lines that intersect on the intersection of conics (four points). You can draw three ""kinds"" of lines. Each two lines are parallel. The ""three"" here corresponds to three eigenvalues. The two parallel lines corresponds to the $\pm$ cases (two branches). If you pick any two combination of groups you can always find the four points and it is just up to you to use which two of them. (Edited for posting an alternative method) Allow me to make some stupid jokes, here's a not so detailed procedure on how to find the quartic equation. First, treat $x$ as a constant in the function, then we can find y as a function of $x$ , where there are two parts, one with square roots and one without. I'd like to write this as: $y = notsoshit_{1}(x) \pm shit(x)$ , And let's consider $notsoshit_{2}(x) = shit(x)^2$ because function without square roots are good elements in the bigger function. I want to avoid derive this myself and it is really... long. The $notsoshit$ functions contain elements that are not independent to what we have in the original functions, and then they can disappear (although it can be lengthy). Thus, substitute this into the original two equations, you will get something like $\begin{cases} a_{3}x^2 + b_{3}shit(x)x  + c_{3}x + d_{3}shit(x) + e_{3} = 0 \\ a_{4}x^2 + b_{4}shit(x)x  + c_{4}x + d_{4}shit(x) + e_{4} = 0 \end{cases}$ Then we should eliminate the shit(x) element (Choose either one function to eliminate $d$ ). The next step is putting $shit(x)x$ to R.H.S., so that no square roots on L.H.S., and then make do a square for this function, so that no square roots on both sides. It is something like: $(a_{5}x^2+b_{5}x + c_{5})^2 = d_{5}(shit(x) x)^2 = d_{5}notsoshit_{2}(x)x^2$ After some work it will be in the beautiful format: $a_{6}x^4+b_{6}x^3 + c_{6}x^2 + d_{6}x + e = 0$ I am not sure how wolfram did this process (Link in the very front), but then the mission is to eliminate $shit(x)$ by adding the two equations with some ratios. (Edited for posting the method for elimination /Problem 1)","This is a post of two three problems regarding the method to solve bivariate quadratic equations. In brief, How does the elimination happen here . Or, how is the elimination used? ( Update, I know how this is done now. See update below ) Is the method below viable? If not, how to solve this problem in matrix format? (Update) Nope, this method below is not viable because rank 2 matrices cannot be factorized. And an alterative matrix method can be seen in chapter 11 of this book . (Update) I also post a method. Is it valid and complete? In order to be complete, some measurements should be done to avoid the case where there are no solutions (i.e., conics don't intersect) The very source of the problem is: (Problem 1) I find a method here , but it does not tell me how the elimination work. How did the elimination work? (Backgrounds of problem 2) I heard that this problem also can be solved by matrices: Say we convert the equations into where and are symmetric and are obtained from the two equations respectively. And if and (which happens most of the cases for randomly generated real symmetric matrices), to find the solution, one should find the eigenvalue of , i.e., , where is the eigenvector matrix and is the eigenvalue matrix. Then we can always find , where is the diagonal value of matrix . ( ) is guaranteed to have a rank of lower than 3 since its determinant is , and either it can be rank or rank . Let's call it . Then I do another diagonalization, such that , where is the diagonal matrix for , and is its corresponding  eigenvector matrix. Assume , , Then the final result is something like: (Problem 2) is the following true? If yes, prove it and express the equation in a factorized format (You can use any notation if you want). If not, how to solve in matrix format? The rank of matrix , as I tested with Matlab scripts, can be either or . In that post (I don't know where it is now), he says each eigenvalue corresponds to a , and corresponds to a factorized quadratic equation so that you can find equations that has the format of: And can be removed because ""it is not telling anything new"" and can be represented by and . There are solutions in total by solving the systems of equations below: Solve Solve Solve Solve (Edited for some grammatical problem) Any is not different from The method is proved to be invalid. But can someone solve it with matrix form? (Edited for proof) I find a solution in Perspectives on Projective Geometry by Jurgen Richter-Gebert. The process is seen in chapter 11 of this book . (Edited for finding one complete solution) Okay, I am still not satisfied. So I want to make a method as well. It seems like we can always do the following step because eigenvalues of symmetric matrices are always real. The notation above means there are two equations ( and , plus and minus respectively) written into one. And with three eigenvalues we will have three groups of equations that look like this below (Shown two, but actually three). After some work, both equations can always be written as = 0, which represents a line. and represent two branches of solution. Then, for a different eigenvalue, we will have ( and ). Similarly, and represent two branches of solution. A set of solution can be obtained from, similar to the last method, For combination of different ranks of matrices, the strategy is about finding two degenerate combinations ( ) such that we have two groups of lines. If both full rank (i.e., the combinations of should not have a zero in it), we need to use two different eigenvalues of one matrix and establish two new degenerate cases. If one is full rank and one is singular (i.e., one of the combination of has a zero in it), then we only need one more degenerate case. If both are singular, then we already have two degenerate cases. (In the combination, two of them should be and ). Question 3: Why can we just use two of the three will be fine for both full rank cases? I did not prove this yet, because I am new to this question (saw it 3 days ago). But with some educated guess, just imagine the solutions are the lines that intersect on the intersection of conics (four points). You can draw three ""kinds"" of lines. Each two lines are parallel. The ""three"" here corresponds to three eigenvalues. The two parallel lines corresponds to the cases (two branches). If you pick any two combination of groups you can always find the four points and it is just up to you to use which two of them. (Edited for posting an alternative method) Allow me to make some stupid jokes, here's a not so detailed procedure on how to find the quartic equation. First, treat as a constant in the function, then we can find y as a function of , where there are two parts, one with square roots and one without. I'd like to write this as: , And let's consider because function without square roots are good elements in the bigger function. I want to avoid derive this myself and it is really... long. The functions contain elements that are not independent to what we have in the original functions, and then they can disappear (although it can be lengthy). Thus, substitute this into the original two equations, you will get something like Then we should eliminate the shit(x) element (Choose either one function to eliminate ). The next step is putting to R.H.S., so that no square roots on L.H.S., and then make do a square for this function, so that no square roots on both sides. It is something like: After some work it will be in the beautiful format: I am not sure how wolfram did this process (Link in the very front), but then the mission is to eliminate by adding the two equations with some ratios. (Edited for posting the method for elimination /Problem 1)","\begin{cases}
a_1x^2 + b_1y^2 + c_1xy + d_1x + e_1y + f_1 = 0 \\
a_2x^2 + b_2y^2 + c_2xy + d_2x + e_2y + f_2 = 0 
\end{cases} \begin{cases}
X^TAX = 0 \\
X^TBX = 0 \\
X = \begin{bmatrix}x &y &1\end{bmatrix}^T
\end{cases} A B rank(A) = 3 rank(B) =3 A^{-1}B [V, D] = eigs(A^{-1}B) V D  X^T(\lambda{A} - B)X = 0 \lambda D \lambda{A} - B 0 1 2 C X^TCX = X^TP^{-1} D_C PX = X^TP^T D_C PX = (PX)^TD_C(PX) = PX \cdot D_CPX D_C C P D_C = \begin{bmatrix} \lambda_{C1} & 0 & 0 \\ 0 & \lambda_{C2} & 0 \\ 0 & 0 & \lambda_{C3} \end{bmatrix} P= \begin{bmatrix} p_1 & p_4 & p_7 \\ p_2 & p_5 & p_8 \\ p_3 & p_6  & p_9 \end{bmatrix} \lambda_{C3} = 0 \lambda_{C1} (p_1 x + p_2 y + p_3)^2 + \lambda_{C2} (p_4 x + p_5 y + p_6)^2 = 0 C 1 2 C X^TCX=0 3 \begin{cases}
Eq_1 = (m_{11}x+m_{12}y+m_{13})(m_{14}x+m_{15}y+m_{16}) = 0 \\
Eq_2 = (m_{21}x+m_{22}y+m_{23})(m_{24}x+m_{25}y+m_{26}) = 0 \\
Eq_3 = (m_{31}x+m_{32}y+m_{33})(m_{34}x+m_{35}y+m_{36}) = 0 \\ 
\end{cases} Eq_3 Eq_1 Eq_2 4 (x_1, y_1)\begin{cases}
m_{11}x+m_{12}y+m_{13} = 0 \\
m_{21}x+m_{22}y+m_{23} = 0
\end{cases} (x_2, y_2)
\begin{cases}
m_{11}x+m_{12}y+m_{13} = 0 \\
m_{24}x+m_{25}y+m_{26} = 0
\end{cases} (x_3, y_3)
\begin{cases}
m_{14}x+m_{15}y+m_{16} = 0 \\
m_{21}x+m_{22}y+m_{23} = 0
\end{cases} (x_4, y_4)
\begin{cases}
m_{14}x+m_{15}y+m_{16} = 0 \\
m_{24}x+m_{25}y+m_{26} = 0
\end{cases} (m_{1}x+m_{2}y+m_{3})(m_{4}x+m_{5}y+m_{6})  = 
\begin{bmatrix} x &y & 1\end{bmatrix}
\begin{bmatrix} m_1 \\ m_2 \\ m_3\end{bmatrix}
\begin{bmatrix} m_4 & m_5 & m_6\end{bmatrix}
\begin{bmatrix} x\\y\\1\end{bmatrix} = 
\begin{bmatrix} x &y & 1\end{bmatrix}
\begin{bmatrix} 
m_1 m_4 & m_1 m_5 & m_1 m_6 \\ 
m_2 m_4 & m_2 m_5 & m_2 m_6 \\ 
m_3 m_4 & m_3 m_5 & m_3 m_6 \\ 
\end{bmatrix}
\begin{bmatrix} x\\y\\1\end{bmatrix}
 C_{12} = \sqrt{\lambda_{C1}} (p_1 x + p_2 y + p_3) = \pm\sqrt{-\lambda_{C2}} (p_4 x + p_5 y + p_6) _{12} C1 C2 \begin{bmatrix}c_1 & c_2 & c_3\end{bmatrix} \begin{bmatrix}x \\ y \\ 1\end{bmatrix} C_1 C_2 C_{34} C_3 C_4 C_3 C_4 \begin{cases}
C_1 = 0 \\
C_3 = 0
\end{cases}\rightarrow (x_1, y_1) \begin{cases}
C_1 = 0 \\
C_4 = 0
\end{cases}\rightarrow (x_2, y_2) \begin{cases}
C_2 = 0 \\
C_3 = 0
\end{cases}\rightarrow (x_3, y_3) \begin{cases}
C_2 = 0 \\
C_4 = 0
\end{cases}\rightarrow (x_4, y_4) \det(\lambda_1 A + \mu B) (\lambda , \mu) (\lambda , \mu) (something_1, 0) (0, something_2) \pm x x y = notsoshit_{1}(x) \pm shit(x) notsoshit_{2}(x) = shit(x)^2 notsoshit \begin{cases}
a_{3}x^2 + b_{3}shit(x)x  + c_{3}x + d_{3}shit(x) + e_{3} = 0 \\
a_{4}x^2 + b_{4}shit(x)x  + c_{4}x + d_{4}shit(x) + e_{4} = 0
\end{cases} d shit(x)x (a_{5}x^2+b_{5}x + c_{5})^2 = d_{5}(shit(x) x)^2 = d_{5}notsoshit_{2}(x)x^2 a_{6}x^4+b_{6}x^3 + c_{6}x^2 + d_{6}x + e = 0 shit(x)","['matrices', 'systems-of-equations', 'quadratics', 'conic-sections', 'quartics']"
71,Help in understanding a proof used in numerical linear algebra,Help in understanding a proof used in numerical linear algebra,,"I am studying numerical linear algebra with the help of Trefethen and Bau's book. I have come across a proof I do not fully understand. I will attempt to bring the statement and proof as they appear in the book. I apologize in advance for the long post, but I wish to bring the reasoning as it appears in the book. Let $A$ be an $m \times m$ real, symmetric matrix. Suppose we have $n$ linearly independent vectors $v_1^{(0)},v_2^{(0)},\dots,v_n^{(0)}$ , and define $V^{(0)}$ to be the $m \times n$ matrix whose columns are $v_1^{(0)},v_2^{(0)},\dots,v_n^{(0)}$ in this order. Now we define the matrix $V^{(k)}=A^kV^{(0)}$ (the result after $k$ applications of $A$ ) and we extract a reduced QR factorization of this matrix $\hat{Q}^{(k)}\hat{R}^{(k)}=V^{(k)}$ where $\hat{Q}^{(k)}$ is an $m \times n$ matrix whose columns form an orthonormal basis for the column space of $V^{(k)}$ . We make the following assumption on the eigenvalues of $A$ : $|\lambda_1|>|\lambda_2|>\dots>|\lambda_n|>|\lambda_{n+1}| \geq |\lambda_{n+2}| \geq \dots \geq |\lambda_{m}|$ . Next, we define the matrix $Q$ whose columns are the normalized eigenvectors of $A$ in the order corresponding to the ordering of the eigenvalues in the assumption, and we take $\hat{Q}$ to be the $m \times n$ matrix whose columns are the first $n$ eigenvectors of $A$ , $q_1,\dots,q_n$ . We now note that $\hat{Q}$ and $\hat{Q}^{(k)}$ are entirely different matrices despite the similar notation. Now, we assume that all the leading principal minors of $\hat{Q}^TV^{(0)}$ are nonsingular. Now, we have the following theorem Assume the notation and assumptions used above. Then as $k \to \infty$ , the columns of $\hat{Q}^{(k)}$ converge linearly to the eigenvectors of $A$ , so that $$|q_j^{(k)}- \pm q_j| = O(C^k)$$ for each $j$ with $1 \leq j \leq  n$ where $C < 1$ is the constant $\max_{1 \leq k \leq n} |\lambda_{k+1}/|\lambda_k|$ . The $\pm$ sign means that at each step $k$ one or the other choice of sign is to be taken. Proof: Extend $\hat{Q}$ to a full $m \times m$ orthogonal matrix $Q$ of eigenvectors of $A$ (in the order matching the ordering of the eigenvalues above), and let $\Lambda$ be the corresponding diagonal matrix of eigenvalues so that $A=Q\Lambda Q^T$ . Just as $\hat{Q}$ is the leading $m \times n$ section of $Q$ , define $\tilde{\Lambda}$ (still diagonal) to be the leading $n \times n$ section of $\Lambda$ . Then we have $$V^{(k)}=A^kV^{(0)}=Q\Lambda^k Q^TV^{(0)}=\hat{Q}\hat{\Lambda}^k\hat{Q}^TV^{(0)}+O(|\lambda_{n+1}|^k)$$ as $k \to \infty$ . Due to the assumption on $\hat{Q}^TV^{(0)}$ , then this matrix itself is nonsingular in particular. Thus, we can multiply the term $O(|\lambda_{n+1}|^k)$ on the right by $\left(\hat{Q}^TV^{(0)}\right)^{-1}\hat{Q}^TV^{(0)}$ to transform our equation to $$V^{(k)}=(\hat{Q}\hat{\Lambda}^k+O(|\lambda_{n+1}|^k))Q^TV^{(0)}.$$ Since $\hat{Q}^TV^{(0)}$ is nonsingular, the column space of this matrix is the same as the column space of $$\hat{Q}\hat{\Lambda}^k+O(|\lambda_{n+1}|^k).$$ From the form of $\hat{Q}\hat{\Lambda}^k$ and the assumption on the ordering of the eigenvalues, it is clear that this column space converges linearly to that of $\hat{Q}$ . This convergence can be quantified, for example, by defining angles between subspaces; we omit the details. Now, in fact, we have assumed that not only is $\hat{Q}^TV^{(0)}$ nonsingular but so are all of its leading principal minors. It follows that the argument above also applies to leading subsets of the columns of $V^{(k)}$ and $\hat{Q}$ : the first column, the first and second columns, the first second and third columns, and so on. In each case, we conclude that the space spanned by indicated columns of $V^{(k)}$ converges linearly to the space spanned by the corresponding columns of $\hat{Q}$ . From the convergence of the successive column spaces, together with the definition of the $QR$ factorization, the convergence result follows. Here is where I get confused: How did the authors obtain $$V^{(k)}=A^kV^{(0)}=Q\Lambda^k Q^TV^{(0)}=\hat{Q}\hat{\Lambda}^k\hat{Q}^TV^{(0)}+O(|\lambda_{n+1}|^k)$$ , and what is the exact meaning of the big O notation here for matrices? I do not understand the statement in boldface. How does the form of $\hat{Q}\hat{\Lambda}^k$ and the assumption on the ordering of the eigenvalues lead to linear convergence? I would appreciate any and all help in understanding and clarifying these points. I really want to know how the authors conclude linear convergence in the statement in boldface. I thank all helpers.","I am studying numerical linear algebra with the help of Trefethen and Bau's book. I have come across a proof I do not fully understand. I will attempt to bring the statement and proof as they appear in the book. I apologize in advance for the long post, but I wish to bring the reasoning as it appears in the book. Let be an real, symmetric matrix. Suppose we have linearly independent vectors , and define to be the matrix whose columns are in this order. Now we define the matrix (the result after applications of ) and we extract a reduced QR factorization of this matrix where is an matrix whose columns form an orthonormal basis for the column space of . We make the following assumption on the eigenvalues of : . Next, we define the matrix whose columns are the normalized eigenvectors of in the order corresponding to the ordering of the eigenvalues in the assumption, and we take to be the matrix whose columns are the first eigenvectors of , . We now note that and are entirely different matrices despite the similar notation. Now, we assume that all the leading principal minors of are nonsingular. Now, we have the following theorem Assume the notation and assumptions used above. Then as , the columns of converge linearly to the eigenvectors of , so that for each with where is the constant . The sign means that at each step one or the other choice of sign is to be taken. Proof: Extend to a full orthogonal matrix of eigenvectors of (in the order matching the ordering of the eigenvalues above), and let be the corresponding diagonal matrix of eigenvalues so that . Just as is the leading section of , define (still diagonal) to be the leading section of . Then we have as . Due to the assumption on , then this matrix itself is nonsingular in particular. Thus, we can multiply the term on the right by to transform our equation to Since is nonsingular, the column space of this matrix is the same as the column space of From the form of and the assumption on the ordering of the eigenvalues, it is clear that this column space converges linearly to that of . This convergence can be quantified, for example, by defining angles between subspaces; we omit the details. Now, in fact, we have assumed that not only is nonsingular but so are all of its leading principal minors. It follows that the argument above also applies to leading subsets of the columns of and : the first column, the first and second columns, the first second and third columns, and so on. In each case, we conclude that the space spanned by indicated columns of converges linearly to the space spanned by the corresponding columns of . From the convergence of the successive column spaces, together with the definition of the factorization, the convergence result follows. Here is where I get confused: How did the authors obtain , and what is the exact meaning of the big O notation here for matrices? I do not understand the statement in boldface. How does the form of and the assumption on the ordering of the eigenvalues lead to linear convergence? I would appreciate any and all help in understanding and clarifying these points. I really want to know how the authors conclude linear convergence in the statement in boldface. I thank all helpers.","A m \times m n v_1^{(0)},v_2^{(0)},\dots,v_n^{(0)} V^{(0)} m \times n v_1^{(0)},v_2^{(0)},\dots,v_n^{(0)} V^{(k)}=A^kV^{(0)} k A \hat{Q}^{(k)}\hat{R}^{(k)}=V^{(k)} \hat{Q}^{(k)} m \times n V^{(k)} A |\lambda_1|>|\lambda_2|>\dots>|\lambda_n|>|\lambda_{n+1}| \geq |\lambda_{n+2}| \geq \dots \geq |\lambda_{m}| Q A \hat{Q} m \times n n A q_1,\dots,q_n \hat{Q} \hat{Q}^{(k)} \hat{Q}^TV^{(0)} k \to \infty \hat{Q}^{(k)} A |q_j^{(k)}- \pm q_j| = O(C^k) j 1 \leq j \leq  n C < 1 \max_{1 \leq k \leq n} |\lambda_{k+1}/|\lambda_k| \pm k \hat{Q} m \times m Q A \Lambda A=Q\Lambda Q^T \hat{Q} m \times n Q \tilde{\Lambda} n \times n \Lambda V^{(k)}=A^kV^{(0)}=Q\Lambda^k Q^TV^{(0)}=\hat{Q}\hat{\Lambda}^k\hat{Q}^TV^{(0)}+O(|\lambda_{n+1}|^k) k \to \infty \hat{Q}^TV^{(0)} O(|\lambda_{n+1}|^k) \left(\hat{Q}^TV^{(0)}\right)^{-1}\hat{Q}^TV^{(0)} V^{(k)}=(\hat{Q}\hat{\Lambda}^k+O(|\lambda_{n+1}|^k))Q^TV^{(0)}. \hat{Q}^TV^{(0)} \hat{Q}\hat{\Lambda}^k+O(|\lambda_{n+1}|^k). \hat{Q}\hat{\Lambda}^k \hat{Q} \hat{Q}^TV^{(0)} V^{(k)} \hat{Q} V^{(k)} \hat{Q} QR V^{(k)}=A^kV^{(0)}=Q\Lambda^k Q^TV^{(0)}=\hat{Q}\hat{\Lambda}^k\hat{Q}^TV^{(0)}+O(|\lambda_{n+1}|^k) \hat{Q}\hat{\Lambda}^k","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'diagonalization']"
72,Number of invertible matrix of order 3 using only 0 and 1,Number of invertible matrix of order 3 using only 0 and 1,,"Let A be a 3 × 3 matrix whose each entry is either 0 or 1. If the probability that A is invertible is $\frac{3n}{64}$ , then is equal to_______ My approach is as follow $A = \left[ {\begin{array}{*{20}{c}} {{a_{11}}}&{{a_{12}}}&{{a_{13}}}\\ {{a_{21}}}&{{a_{22}}}&{{a_{23}}}\\ {{a_{31}}}&{{a_{32}}}&{{a_{33}}} \end{array}} \right] \Rightarrow \left| A \right| = {a_{11}}\left( {{a_{22}}.{a_{33}} - {a_{23}}.{a_{32}}} \right) - {a_{12}}\left( {{a_{21}}.{a_{33}} - {a_{23}}.{a_{31}}} \right) + {a_{13}}\left( {{a_{21}}.{a_{32}} - {a_{22}}.{a_{31}}} \right)$ The total number of Matrices are $2^9$ and the number of favourable cases is $X$ Hence the probability is $\frac{X}{2^9}$ , using $0$ and $1$ how do I find that the matrix is invertible","Let A be a 3 × 3 matrix whose each entry is either 0 or 1. If the probability that A is invertible is , then is equal to_______ My approach is as follow The total number of Matrices are and the number of favourable cases is Hence the probability is , using and how do I find that the matrix is invertible","\frac{3n}{64} A = \left[ {\begin{array}{*{20}{c}}
{{a_{11}}}&{{a_{12}}}&{{a_{13}}}\\
{{a_{21}}}&{{a_{22}}}&{{a_{23}}}\\
{{a_{31}}}&{{a_{32}}}&{{a_{33}}}
\end{array}} \right] \Rightarrow \left| A \right| = {a_{11}}\left( {{a_{22}}.{a_{33}} - {a_{23}}.{a_{32}}} \right) - {a_{12}}\left( {{a_{21}}.{a_{33}} - {a_{23}}.{a_{31}}} \right) + {a_{13}}\left( {{a_{21}}.{a_{32}} - {a_{22}}.{a_{31}}} \right) 2^9 X \frac{X}{2^9} 0 1","['linear-algebra', 'matrices']"
73,"Lower central series of the Unitriangular group $UT(n, \mathbb{Z}_p)$",Lower central series of the Unitriangular group,"UT(n, \mathbb{Z}_p)","This is Exercise 5.44 from Rotman's book "" An Introduction to the theory of Groups (4th Ed) "". Specificaly, the exercise asks us to prove that the $i$ -eth term in the lower central series is the set of all unitriangular matrices with "" $0$ "" in the $i$ super-diagonals. In particular, this means the group is nilpotent of class $n-1$ . While I believe I could do it in a rather straightforward way by calculating the commutators explicitly (by hand), Rotman provides a hint: ""Given $A \in UT(n, \mathbb{Z}_p)$ , consider the powers of the matrix $A -I_n$ "". What I don't get is how to use this hint... I get that $A - I_n$ is nilpotent of class $n - z$ , where "" $z$ "" denotes the number of super-diagonals with only $0$ . So, for instance, if $n = 2$ and $$A = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}$$ then $$A - I_2 = \begin{bmatrix} 0 & 2 \\ 0 & 0 \end{bmatrix}$$ which is nilpotent of class $2$ ( $(A-I_2)^2 = 0$ ). However, how can this be used to solve the problem at hand, in a way to (hopefully) avoid too much explicit calculation? Please provide only a hint (if possible). Thanks in advance!","This is Exercise 5.44 from Rotman's book "" An Introduction to the theory of Groups (4th Ed) "". Specificaly, the exercise asks us to prove that the -eth term in the lower central series is the set of all unitriangular matrices with "" "" in the super-diagonals. In particular, this means the group is nilpotent of class . While I believe I could do it in a rather straightforward way by calculating the commutators explicitly (by hand), Rotman provides a hint: ""Given , consider the powers of the matrix "". What I don't get is how to use this hint... I get that is nilpotent of class , where "" "" denotes the number of super-diagonals with only . So, for instance, if and then which is nilpotent of class ( ). However, how can this be used to solve the problem at hand, in a way to (hopefully) avoid too much explicit calculation? Please provide only a hint (if possible). Thanks in advance!","i 0 i n-1 A \in UT(n, \mathbb{Z}_p) A -I_n A - I_n n - z z 0 n = 2 A = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix} A - I_2 = \begin{bmatrix} 0 & 2 \\ 0 & 0 \end{bmatrix} 2 (A-I_2)^2 = 0","['matrices', 'group-theory', 'nilpotence', 'nilpotent-groups']"
74,Projection matrices with constant diagonal,Projection matrices with constant diagonal,,"Given $k<n$ , does there always exist an $n\times n$ projection matrix of rank $k$ with a constant diagonal, i.e., all diagonal entries equal to $k/n$ ? Here, when I say projection matrix, I mean specifically orthogonal projection, so $P=P^2=P^*$ It is easy to construct examples when $k=1$ , or when $k=2$ and $n$ is even, and given a Hadamard matrix of order $n$ , one can use it to construct examples of projection matrices of rank k and size n for all $k<n$ .  However, trying to find an example with $n=3, k=2$ seems difficult enough (the way I approached it required using a computer to solve a system of 6 quadratic equations in 6 unknowns). Is there a construction that works for general $n,k$ ? Is there a non-constructive proof that such a projection exists for every $n$ and $k$ ?","Given , does there always exist an projection matrix of rank with a constant diagonal, i.e., all diagonal entries equal to ? Here, when I say projection matrix, I mean specifically orthogonal projection, so It is easy to construct examples when , or when and is even, and given a Hadamard matrix of order , one can use it to construct examples of projection matrices of rank k and size n for all .  However, trying to find an example with seems difficult enough (the way I approached it required using a computer to solve a system of 6 quadratic equations in 6 unknowns). Is there a construction that works for general ? Is there a non-constructive proof that such a projection exists for every and ?","k<n n\times n k k/n P=P^2=P^* k=1 k=2 n n k<n n=3, k=2 n,k n k","['linear-algebra', 'matrices', 'projection-matrices']"
75,Applications of matrix differentiation,Applications of matrix differentiation,,"I know that ordinary differentiation has many real world applications, from quantum physics to economics, but I cannot think of any real world applications of matrix differentiation. So, do any real world application exist?","I know that ordinary differentiation has many real world applications, from quantum physics to economics, but I cannot think of any real world applications of matrix differentiation. So, do any real world application exist?",,"['calculus', 'matrices', 'derivatives', 'matrix-calculus', 'applications']"
76,"Matrix spaces A,B with the same dimension implying B=QAP?","Matrix spaces A,B with the same dimension implying B=QAP?",,"I have a conjecture: If two finite-dimensional matrix spaces $\mathcal{A},\mathcal{B}\subseteq \mathrm{M}(n,\mathbb{F})$ have the same dimension, then there would exist $P,Q\in\operatorname{GL}(n,\mathbb{F})$ such that $\mathcal{B}=Q^{-1}\mathcal{A}P$ . ( Reminder: Matrix spaces refer to linear spaces spanned by matrices . ) Is it true or false? It is well-known that two finite-dimensional linear spaces are isomorphic if and only if they have the same dimension. Also, this means there is a bijection from a basis of one space to a basis of the other. Does it complete the proof of my conjecture? It seems that if the bases are vectors instead of matrices, then the statement would be true. So I'm very confused if the two cases can be the same. Thank you!","I have a conjecture: If two finite-dimensional matrix spaces have the same dimension, then there would exist such that . ( Reminder: Matrix spaces refer to linear spaces spanned by matrices . ) Is it true or false? It is well-known that two finite-dimensional linear spaces are isomorphic if and only if they have the same dimension. Also, this means there is a bijection from a basis of one space to a basis of the other. Does it complete the proof of my conjecture? It seems that if the bases are vectors instead of matrices, then the statement would be true. So I'm very confused if the two cases can be the same. Thank you!","\mathcal{A},\mathcal{B}\subseteq \mathrm{M}(n,\mathbb{F}) P,Q\in\operatorname{GL}(n,\mathbb{F}) \mathcal{B}=Q^{-1}\mathcal{A}P","['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces', 'vector-space-isomorphism']"
77,Minimum EigenValue,Minimum EigenValue,,"From Rayleigh Equation, we know for symmetric $n\times n$ matrix $A$ $$\min_{x\in \mathbb{R}^n\setminus \{0\}}\frac{x^TAx}{x^Tx} = \lambda_{\min}(A)$$ My question is does the same equation hold for the following scenario $$\inf_{x\in \mathbb{S}^{n-1}\setminus\{x^*\}}\frac{(x-x^*)^TA(x-x^*)}{(x-x^*)^T(x-x^*)} = \lambda_{\min}(A)$$ where $\mathbb{S}^{n-1}$ is the unit sphere in $n$ dimensions and $x^*$ is a fixed element in $\mathbb{S}^{n-1}$ . Further, it seems that the minimum can be achieved if $x^*$ is orthogonal to the eigenvector corresponding to the minimum eigen value. Can this also be proved formally/","From Rayleigh Equation, we know for symmetric matrix My question is does the same equation hold for the following scenario where is the unit sphere in dimensions and is a fixed element in . Further, it seems that the minimum can be achieved if is orthogonal to the eigenvector corresponding to the minimum eigen value. Can this also be proved formally/",n\times n A \min_{x\in \mathbb{R}^n\setminus \{0\}}\frac{x^TAx}{x^Tx} = \lambda_{\min}(A) \inf_{x\in \mathbb{S}^{n-1}\setminus\{x^*\}}\frac{(x-x^*)^TA(x-x^*)}{(x-x^*)^T(x-x^*)} = \lambda_{\min}(A) \mathbb{S}^{n-1} n x^* \mathbb{S}^{n-1} x^*,"['real-analysis', 'linear-algebra', 'matrices', 'convex-optimization']"
78,Lower-bounding minimal eigenvalue via the Schur complement,Lower-bounding minimal eigenvalue via the Schur complement,,"Suppose that $$M=\left( \begin{array}{cc} A & B\\ B^\top & C  \end{array} \right)$$ for some symmetric matrices $A$ and $C$ , and $C$ is invertible. Is it true that: $$\lambda_{\min}(M) \ge \min\left\{\lambda_{\min}(C)~,~\lambda_{\min}(A-BC^{-1}B^\top)\right\}~?$$ Here, $\lambda_{\min}$ refers to the minimum eigenvalue of a matrix. Even if the above inequality is incorrect, is there a way to lower bound $\lambda_{\min}(A)$ in terms of $\lambda_{\min}(C)$ and $\lambda_{\min}(A-BC^{-1}B^\top)$ ? You may even assume that $M$ is non-negative definite, and $$A=u^\top u~, ~B = u^\top X~\textrm{and}~C= X^\top X$$ for some vector $u$ and some matrix $X$ , if that helps! Any help will be greatly appreciated!","Suppose that for some symmetric matrices and , and is invertible. Is it true that: Here, refers to the minimum eigenvalue of a matrix. Even if the above inequality is incorrect, is there a way to lower bound in terms of and ? You may even assume that is non-negative definite, and for some vector and some matrix , if that helps! Any help will be greatly appreciated!","M=\left(
\begin{array}{cc}
A & B\\
B^\top & C 
\end{array}
\right) A C C \lambda_{\min}(M) \ge \min\left\{\lambda_{\min}(C)~,~\lambda_{\min}(A-BC^{-1}B^\top)\right\}~? \lambda_{\min} \lambda_{\min}(A) \lambda_{\min}(C) \lambda_{\min}(A-BC^{-1}B^\top) M A=u^\top u~, ~B = u^\top X~\textrm{and}~C= X^\top X u X","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'upper-lower-bounds', 'schur-complement']"
79,Partial derivative of a diagonal matrix w.r.t a vector,Partial derivative of a diagonal matrix w.r.t a vector,,"I am trying to find the second partial derivative of the function $Y=diag\boldsymbol(S)\mathbb P diag \boldsymbol(\beta)diag^{-1}(\mathbb P^{T}\boldsymbol S +\mathbb P^{T}\boldsymbol E + \mathbb P^{T}\boldsymbol I +\mathbb P^{T}\boldsymbol R )\mathbb P^{T}\boldsymbol I$ where $\mathbb P=[(p_{ij})]_{n\times n},\boldsymbol S, \boldsymbol E, \boldsymbol I$ and $\boldsymbol R $ are $n\times 1$ vectors with respect to $\boldsymbol S$ . I am not sure of how to approach this. I was thinking of using the product rule whereby I will take $$ X=\underbrace{diag\boldsymbol(S)\mathbb P diag \boldsymbol(\beta)}_{Y}\underbrace{diag^{-1}(\mathbb P^{T}\boldsymbol S +\mathbb P^{T}\boldsymbol E + \mathbb P^{T}\boldsymbol I +\mathbb P^{T}\boldsymbol R )\mathbb P^{T}\boldsymbol I}_{Z}$$ so that $$ \frac{\partial X}{\partial \boldsymbol S}=\frac{\partial Y}{\partial \boldsymbol S}Z+Y\frac{\partial Z}{\partial\boldsymbol S}$$ This is proving difficult to achieve as I am not sure if what I am thinking of doing is correct. For example, I was thinking of taking $D=diag(\mathbb P^{T}\boldsymbol S +\mathbb P^{T}\boldsymbol E + \mathbb P^{T}\boldsymbol I +\mathbb P^{T}\boldsymbol R )$ so that $$\frac{\partial Z}{\partial S}=-D\frac{\partial D}{\partial S}D^{-1}\mathbb P^{T}\boldsymbol I$$ where $$\frac{\partial D}{\partial S}=\mathbb P^{T}$$ Is this process correct? Because I am imagigning that this method (for the derivative of the inverse of a matrix) can only be applied when one is finding the derivative w.r.t a matrix and not w.r.t a vector. Could somebody help me obtain $\frac{\partial X}{\partial\boldsymbol S}$ and subsequently $\frac{\partial^{2} X}{\partial\boldsymbol S^{2}}$ , $\frac{\partial^{2} X}{\partial\boldsymbol I\partial \boldsymbol E}$ . I will appreciate it very much.","I am trying to find the second partial derivative of the function where and are vectors with respect to . I am not sure of how to approach this. I was thinking of using the product rule whereby I will take so that This is proving difficult to achieve as I am not sure if what I am thinking of doing is correct. For example, I was thinking of taking so that where Is this process correct? Because I am imagigning that this method (for the derivative of the inverse of a matrix) can only be applied when one is finding the derivative w.r.t a matrix and not w.r.t a vector. Could somebody help me obtain and subsequently , . I will appreciate it very much.","Y=diag\boldsymbol(S)\mathbb P diag \boldsymbol(\beta)diag^{-1}(\mathbb P^{T}\boldsymbol S +\mathbb P^{T}\boldsymbol E + \mathbb P^{T}\boldsymbol I +\mathbb P^{T}\boldsymbol R )\mathbb P^{T}\boldsymbol I \mathbb P=[(p_{ij})]_{n\times n},\boldsymbol S, \boldsymbol E, \boldsymbol I \boldsymbol R  n\times 1 \boldsymbol S  X=\underbrace{diag\boldsymbol(S)\mathbb P diag \boldsymbol(\beta)}_{Y}\underbrace{diag^{-1}(\mathbb P^{T}\boldsymbol S +\mathbb P^{T}\boldsymbol E + \mathbb P^{T}\boldsymbol I +\mathbb P^{T}\boldsymbol R )\mathbb P^{T}\boldsymbol I}_{Z}  \frac{\partial X}{\partial \boldsymbol S}=\frac{\partial Y}{\partial \boldsymbol S}Z+Y\frac{\partial Z}{\partial\boldsymbol S} D=diag(\mathbb P^{T}\boldsymbol S +\mathbb P^{T}\boldsymbol E + \mathbb P^{T}\boldsymbol I +\mathbb P^{T}\boldsymbol R ) \frac{\partial Z}{\partial S}=-D\frac{\partial D}{\partial S}D^{-1}\mathbb P^{T}\boldsymbol I \frac{\partial D}{\partial S}=\mathbb P^{T} \frac{\partial X}{\partial\boldsymbol S} \frac{\partial^{2} X}{\partial\boldsymbol S^{2}} \frac{\partial^{2} X}{\partial\boldsymbol I\partial \boldsymbol E}","['matrices', 'partial-derivative', 'matrix-calculus', 'tridiagonal-matrices']"
80,Positive semidefinite matrix ordering and nuclear norm of products,Positive semidefinite matrix ordering and nuclear norm of products,,"Let $A, A', B, B'$ be finite-dimensional, complex-valued, Hermitian, positive-semidefinite matrices. Moreover, let $(A-A')$ and $(B-B')$ also be positive-semidefinite. The 1-norm is defined as $\|X\|_1 = Tr(\sqrt{X^*X})$ where $X^*$ is the transpose conjugate of $X$ . Can one claim anything about the relationship between $\|AB\|_1$ and $\|A'B'\|_1$ ? In particular is $$\|AB\|_1\geq \|A'B'\|_1?$$ If not, can someone show a counterexample?","Let be finite-dimensional, complex-valued, Hermitian, positive-semidefinite matrices. Moreover, let and also be positive-semidefinite. The 1-norm is defined as where is the transpose conjugate of . Can one claim anything about the relationship between and ? In particular is If not, can someone show a counterexample?","A, A', B, B' (A-A') (B-B') \|X\|_1 = Tr(\sqrt{X^*X}) X^* X \|AB\|_1 \|A'B'\|_1 \|AB\|_1\geq \|A'B'\|_1?","['linear-algebra', 'matrices', 'positive-semidefinite', 'nuclear-norm']"
81,How to understand transformations on operators,How to understand transformations on operators,,"This question is related, or maybe is better to say inspired , to/by this other one about quantum mechanics . From what I currently understand the action of a transformation $T$ on another matrix, or to be more general on another operator $A$ , is obtained by applying the following formula: $$TAT^{-1}$$ Just to put some reference this formula can be found on this wikipedia article about change of basis . The result of this calculation should be another operator $A'$ $$A'=TAT^{-1} \tag{1}$$ that does exactly what $A$ was doing but on the transformed space (right?). Through the course of my studies I was always struggling to remember this formula; I was always making confusion between $TAT^{-1}$ and $T^{-1}AT$ , and I was always forced to double check. But then I found some nice physical/mathematical interpretation of this formula that made remembering it a lot easier: of course the formula must be $TAT^{-1}$ , because we want the action of $A'$ to be the same of $A$ but in the transformed space, so what we do to accomplish this is Using $T^{-1}$ to turn $\vec{v}'$ into $v$ Now that we are dealing with a vector in the non-transformed space we can apply $A$ to get the desired action Then we put back the result in the transformed space by applying $T$ This interpretation allows to clearly understand and remember why $T^{-1}$ must come before $T$ . I was really convinced by my own reasoning, but then I realized that there is a huge problem with my line of though: lets take $T$ to be a rotation , and lets also take $A$ to be some operator involving rotations . Problem now is that rotations do not commute , rotating in one way and then in another way is not the same as doing the opposite, and so my interpretation on why (1) works breaks apart badly! So is my interpretation of (1) wrong? Or does (1) not work with rotations? Seems strange.. In the case of my explanation of (1) being wrong: is there some other useful interpretation of (1)? This topic has also huge physical implications: for example in quantum mechanics we apply the parity operator $P$ to other hermitian operators all the time, and it would be nice to understand why the formula should be $PAP^{-1}$ and not $P^{-1}AP$ . (Maybe using parity as an example is not a great idea since $P=P^{-1}$ , but come on you get what I am saying here!)","This question is related, or maybe is better to say inspired , to/by this other one about quantum mechanics . From what I currently understand the action of a transformation on another matrix, or to be more general on another operator , is obtained by applying the following formula: Just to put some reference this formula can be found on this wikipedia article about change of basis . The result of this calculation should be another operator that does exactly what was doing but on the transformed space (right?). Through the course of my studies I was always struggling to remember this formula; I was always making confusion between and , and I was always forced to double check. But then I found some nice physical/mathematical interpretation of this formula that made remembering it a lot easier: of course the formula must be , because we want the action of to be the same of but in the transformed space, so what we do to accomplish this is Using to turn into Now that we are dealing with a vector in the non-transformed space we can apply to get the desired action Then we put back the result in the transformed space by applying This interpretation allows to clearly understand and remember why must come before . I was really convinced by my own reasoning, but then I realized that there is a huge problem with my line of though: lets take to be a rotation , and lets also take to be some operator involving rotations . Problem now is that rotations do not commute , rotating in one way and then in another way is not the same as doing the opposite, and so my interpretation on why (1) works breaks apart badly! So is my interpretation of (1) wrong? Or does (1) not work with rotations? Seems strange.. In the case of my explanation of (1) being wrong: is there some other useful interpretation of (1)? This topic has also huge physical implications: for example in quantum mechanics we apply the parity operator to other hermitian operators all the time, and it would be nice to understand why the formula should be and not . (Maybe using parity as an example is not a great idea since , but come on you get what I am saying here!)",T A TAT^{-1} A' A'=TAT^{-1} \tag{1} A TAT^{-1} T^{-1}AT TAT^{-1} A' A T^{-1} \vec{v}' v A T T^{-1} T T A P PAP^{-1} P^{-1}AP P=P^{-1},"['matrices', 'operator-algebras', 'transformation', 'quantum-mechanics', 'change-of-basis']"
82,"If $S$ is symmetric positive definite and $SA$ symmetric, is then $A$ symmetric?","If  is symmetric positive definite and  symmetric, is then  symmetric?",S SA A,"We are given real matrices $S$ and $A$ . We know that $S$ is symmetric positive definite and that $SA$ is symmetric. Is A necessarily symmetric then? I've figured out that if $A$ is symmetric, then $S$ and $A$ must commute. I've tried finding a $2 \times 2$ and a $3 \times 3$ counterexample, as it seemed to me that this is not generally true, but I couldn't find any.","We are given real matrices and . We know that is symmetric positive definite and that is symmetric. Is A necessarily symmetric then? I've figured out that if is symmetric, then and must commute. I've tried finding a and a counterexample, as it seemed to me that this is not generally true, but I couldn't find any.",S A S SA A S A 2 \times 2 3 \times 3,"['linear-algebra', 'matrices', 'linear-transformations', 'positive-definite', 'symmetric-matrices']"
83,Units of ${\rm Mat}_{n\times n}(\mathbb{Z})$,Units of,{\rm Mat}_{n\times n}(\mathbb{Z}),"I am trying to calculate the units of the ring ${\rm Mat}_{n\times n}(\mathbb{Z})$ , but I am a little bit stuck. Here is my try: Let's take $A\in U({\rm Mat}_{n\times n}(\mathbb{Z}))$ ; then, there exists $B\in {\rm Mat}_{n\times n}(\mathbb{Z})$ such that $AB=BA=Id$ ; taking determinants, we conclude that this implies that $\det(A)\det(B)=\det(Id)=1$ ; so, taking on account that the determinant of a matrix with integer coeficients is also an integer, we have two options from the last conclusion: or $\det(A)=\det(B)=1$ or $\det(A)=\det(B)=-1$ ; so by all of this, I would be able to conclude that $$U({\rm Mat}_{n\times n}(\mathbb{Z}))\subseteq\{A\in{\rm Mat}_{n\times n}(\mathbb{Z}): \det(A)=\pm 1\}$$ How should I proceed for finishing and giving the concrete units of this ring? Thank you in advance for your help!","I am trying to calculate the units of the ring , but I am a little bit stuck. Here is my try: Let's take ; then, there exists such that ; taking determinants, we conclude that this implies that ; so, taking on account that the determinant of a matrix with integer coeficients is also an integer, we have two options from the last conclusion: or or ; so by all of this, I would be able to conclude that How should I proceed for finishing and giving the concrete units of this ring? Thank you in advance for your help!",{\rm Mat}_{n\times n}(\mathbb{Z}) A\in U({\rm Mat}_{n\times n}(\mathbb{Z})) B\in {\rm Mat}_{n\times n}(\mathbb{Z}) AB=BA=Id \det(A)\det(B)=\det(Id)=1 \det(A)=\det(B)=1 \det(A)=\det(B)=-1 U({\rm Mat}_{n\times n}(\mathbb{Z}))\subseteq\{A\in{\rm Mat}_{n\times n}(\mathbb{Z}): \det(A)=\pm 1\},"['matrices', 'group-theory', 'ring-theory']"
84,Is $\textbf{A}^m=\textbf{I}$ diagonalizable? [duplicate],Is  diagonalizable? [duplicate],\textbf{A}^m=\textbf{I},"This question already has answers here : Let $A$ be a complex matrix such that $A^n = I$, show that $A$ is diagonalisable. (2 answers) Closed 3 years ago . Let $\textbf{A} \in M_{n\times n}(\mathbb{C})$ be a Matrix and $\textbf{A}^m=\textbf{I}$ where $m\neq \infty$ . Show (with Jordan normal form) that $\textbf{A}$ is diagonalizable. I have no clue how to do that...","This question already has answers here : Let $A$ be a complex matrix such that $A^n = I$, show that $A$ is diagonalisable. (2 answers) Closed 3 years ago . Let be a Matrix and where . Show (with Jordan normal form) that is diagonalizable. I have no clue how to do that...",\textbf{A} \in M_{n\times n}(\mathbb{C}) \textbf{A}^m=\textbf{I} m\neq \infty \textbf{A},"['linear-algebra', 'matrices', 'diagonalization']"
85,Eigenvalues of a matrix close to a tridiagonal Toeplitz matrix,Eigenvalues of a matrix close to a tridiagonal Toeplitz matrix,,"I am trying to find all the eigenvalues of $P$ defined below: $$P=\begin{bmatrix} 0.5&0.5&0&0&\cdots 0&0 \\ 0.25&0.5&0.25&0&\cdots 0&0\\ 0&0.25&0.5&0.25&\cdots 0&0\\ \vdots \\ 0&0&0&0&\cdots 0.5&0.5 \end{bmatrix}_{n\times n}$$ So $P$ has $0.5$ along the main diagonal. It has $0.25$ on diagonals above and below the main diagonal except for the first and last row. Hence $P$ is not exactly a Toeplitz matrix. My attempt: A paper I'm looking at, gives eigenvalues of the following Toeplitz matrix $$ Q=\begin{bmatrix} b&a \\ c&b&a \\ &\ddots&\ddots&\ddots \\ &&c&b&a \\ &&&c&b \end{bmatrix} $$ as $\lambda_j = b+2\sqrt{ca}\cos\left(\frac{j\pi}{n+1} \right)$ So I'm wondering if I will be able to find eigenvalues of $P$ even though its not a Toeplitz matrix precisely?","I am trying to find all the eigenvalues of defined below: So has along the main diagonal. It has on diagonals above and below the main diagonal except for the first and last row. Hence is not exactly a Toeplitz matrix. My attempt: A paper I'm looking at, gives eigenvalues of the following Toeplitz matrix as So I'm wondering if I will be able to find eigenvalues of even though its not a Toeplitz matrix precisely?","P P=\begin{bmatrix}
0.5&0.5&0&0&\cdots 0&0 \\
0.25&0.5&0.25&0&\cdots 0&0\\
0&0.25&0.5&0.25&\cdots 0&0\\
\vdots \\
0&0&0&0&\cdots 0.5&0.5
\end{bmatrix}_{n\times n} P 0.5 0.25 P 
Q=\begin{bmatrix}
b&a \\
c&b&a \\
&\ddots&\ddots&\ddots \\
&&c&b&a \\
&&&c&b
\end{bmatrix}
 \lambda_j = b+2\sqrt{ca}\cos\left(\frac{j\pi}{n+1} \right) P",['linear-algebra']
86,"If $B = R^TR$ and $B$ is symmetric positive definite, then $R$ is invertible","If  and  is symmetric positive definite, then  is invertible",B = R^TR B R,"I have a problem in my textbook: Assume B is symmetric positive semi-definite. Show that $B=R^TR$ for some square matrix $R$ that is not necessarily symmetric, and that if $B$ is symmetric positive definite, then $R$ is invertible. My approach: Since $B$ is PSD and symmetric, it is diagonalizable, so we can write $B=V\Lambda V^T$ . So, $B=V\Lambda V^T = B=V\Lambda^{1/2}\Lambda^{1/2} V^T = (\Lambda^{1/2}V^T)^T\Lambda^{1/2}V^T=R^TR$ for $R=\Lambda^{1/2}V^T$ Now, for the second part, i'm not quite sure if i'm doing it right: If $B$ is PD, then all eigenvalues are positive. Hence, $R= \begin{bmatrix} v_{11}\sqrt{\lambda_1} & 0 &...&0 \\ 0 & v_{22}\sqrt{\lambda_2} & ... & 0\\0 &...&...&v_{nn}\sqrt{\lambda_n}  \end{bmatrix}$ And it has an inverse: $R^{-1}= \begin{bmatrix} 1/v_{11}\sqrt{\lambda_1} & 0 &...&0 \\ 0 & 1/v_{22}\sqrt{\lambda_2} & ... & 0\\0 &...&...&1/v_{nn}\sqrt{\lambda_n}  \end{bmatrix}$ With which $R^{-1}R=I_n$ . Hence, $R$ is invertible. I am not sure whether the second part is rigorous, or if it's even correct. Any help would be appreciated.","I have a problem in my textbook: Assume B is symmetric positive semi-definite. Show that for some square matrix that is not necessarily symmetric, and that if is symmetric positive definite, then is invertible. My approach: Since is PSD and symmetric, it is diagonalizable, so we can write . So, for Now, for the second part, i'm not quite sure if i'm doing it right: If is PD, then all eigenvalues are positive. Hence, And it has an inverse: With which . Hence, is invertible. I am not sure whether the second part is rigorous, or if it's even correct. Any help would be appreciated.",B=R^TR R B R B B=V\Lambda V^T B=V\Lambda V^T = B=V\Lambda^{1/2}\Lambda^{1/2} V^T = (\Lambda^{1/2}V^T)^T\Lambda^{1/2}V^T=R^TR R=\Lambda^{1/2}V^T B R= \begin{bmatrix} v_{11}\sqrt{\lambda_1} & 0 &...&0 \\ 0 & v_{22}\sqrt{\lambda_2} & ... & 0\\0 &...&...&v_{nn}\sqrt{\lambda_n}  \end{bmatrix} R^{-1}= \begin{bmatrix} 1/v_{11}\sqrt{\lambda_1} & 0 &...&0 \\ 0 & 1/v_{22}\sqrt{\lambda_2} & ... & 0\\0 &...&...&1/v_{nn}\sqrt{\lambda_n}  \end{bmatrix} R^{-1}R=I_n R,['matrices']
87,Proving invertibility of $A^{T}A$,Proving invertibility of,A^{T}A,"Take the matrix $A$ , which is $m\times n$ and of rank $n$ . Hence, a full column-rank matrix. I need to show that $N(A^{T}A) = N(A)$ , and deduce that $A^{T}A$ is invertible. Since A is a full column-rank matrix, I see that $dim \space N(A) = n-r = n-n =0$ , since $r=n$ . Howver, I'm not sure how to characterize the null-space of $N(A^{T}A)$ in order to arrive at $N(A^{T}A) = N(A)$ . Once this is done, I see how $N(A) = \vec{0}$ implies the invertibility of $A^{T}A$ .","Take the matrix , which is and of rank . Hence, a full column-rank matrix. I need to show that , and deduce that is invertible. Since A is a full column-rank matrix, I see that , since . Howver, I'm not sure how to characterize the null-space of in order to arrive at . Once this is done, I see how implies the invertibility of .",A m\times n n N(A^{T}A) = N(A) A^{T}A dim \space N(A) = n-r = n-n =0 r=n N(A^{T}A) N(A^{T}A) = N(A) N(A) = \vec{0} A^{T}A,"['linear-algebra', 'matrices', 'analysis', 'inverse', 'projection-matrices']"
88,"Let $n\geq 1$ and let $A$ be a $n\times n$ matrix with real entries such that $A^k=O$, then find value of $\det(I+A)$","Let  and let  be a  matrix with real entries such that , then find value of",n\geq 1 A n\times n A^k=O \det(I+A),"Let $n\geq 1$ and let $A$ be a $n\times n$ matrix with real entries such that $A^k=O$ , for some $k\geq 1$ . Let $I$ be the $n\times n$ identity matrix. Then find value of $\det(I+A)$ . My Attempt Since $A^k=O$ we have $\det(A)=0$ and also $A^k=A^{k+1}=A^{k+2}=...=O$ Let $p\in \Bbb N$ where $p\geq k$ $(I+A)^p=I+\binom{p}{1}A+\binom{p}{2}A^2+...+\binom{p}{k-1}A^{k-1}+O$ (since $A^k=A^{k+1}=A^{k+2}=...=O$ ) $A^{k-1}(I+A)^p=A^{k-1}$ $\det(A^{k-1}(I+A)^p)=\det(A^{k-1})=0$ I wonder if I am doing right because I cannot justify further steps","Let and let be a matrix with real entries such that , for some . Let be the identity matrix. Then find value of . My Attempt Since we have and also Let where (since ) I wonder if I am doing right because I cannot justify further steps",n\geq 1 A n\times n A^k=O k\geq 1 I n\times n \det(I+A) A^k=O \det(A)=0 A^k=A^{k+1}=A^{k+2}=...=O p\in \Bbb N p\geq k (I+A)^p=I+\binom{p}{1}A+\binom{p}{2}A^2+...+\binom{p}{k-1}A^{k-1}+O A^k=A^{k+1}=A^{k+2}=...=O A^{k-1}(I+A)^p=A^{k-1} \det(A^{k-1}(I+A)^p)=\det(A^{k-1})=0,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
89,Prove $\text{null}(A) \subseteq \text{null}(BA)$?,Prove ?,\text{null}(A) \subseteq \text{null}(BA),"The problem How does one show that $\text{null}(A) \subseteq \text{null}(BA)$ ? is it even true for all $A$ and $B$ ? My understandings Please correct me if I'm wrong. By definition, null space of any matrix $M$ , also $\text{null}(M)$ , is the set of all of the answers for $Ax=0$ . This equation has one obvious answer where $x=0$ , that's also the same thing in saying that linear transformations keep/do not move the origin . So, I can know that null space of matrices is at least zero. Now, for arbitrary $m \times n$ matrices $A$ and $B$ , we can say $\text{null}(A) > 0$ and $\text{null}(B) > 0$ , so $\text{null}(BA) > 0$ . Where I need help I suspect that I can say $BA$ is a linear transformation by $B$ on $A$ . Now, since $B$ might or might not nullify things that $A$ spanned, and since $A$ itself is can be thought of as a linear transformation on the identity matrix, and $A$ might or might not have nulled the space spanned by those unit vectors, $\text{null}(BA) \ge \text{null}(A)$ , because at least everything that $A$ has nullified is still nullified in $BA$ because of multiplication by zero, and $B$ might or might not have nulled more stuff.","The problem How does one show that ? is it even true for all and ? My understandings Please correct me if I'm wrong. By definition, null space of any matrix , also , is the set of all of the answers for . This equation has one obvious answer where , that's also the same thing in saying that linear transformations keep/do not move the origin . So, I can know that null space of matrices is at least zero. Now, for arbitrary matrices and , we can say and , so . Where I need help I suspect that I can say is a linear transformation by on . Now, since might or might not nullify things that spanned, and since itself is can be thought of as a linear transformation on the identity matrix, and might or might not have nulled the space spanned by those unit vectors, , because at least everything that has nullified is still nullified in because of multiplication by zero, and might or might not have nulled more stuff.",\text{null}(A) \subseteq \text{null}(BA) A B M \text{null}(M) Ax=0 x=0 m \times n A B \text{null}(A) > 0 \text{null}(B) > 0 \text{null}(BA) > 0 BA B A B A A A \text{null}(BA) \ge \text{null}(A) A BA B,"['linear-algebra', 'matrices', 'solution-verification', 'intuition', 'fake-proofs']"
90,Is there a shortcut to invert this 3x3 matrix?,Is there a shortcut to invert this 3x3 matrix?,,"Is there an easy way to invert the matrix on right-hand-side? Like is there some quick formula or observation we can make or do I have to do it manually all the way? Please, dont be harsh on me since I am not good at linear algebra. EDIT: Result","Is there an easy way to invert the matrix on right-hand-side? Like is there some quick formula or observation we can make or do I have to do it manually all the way? Please, dont be harsh on me since I am not good at linear algebra. EDIT: Result",,"['linear-algebra', 'matrices', 'matrix-equations']"
91,"What's a ""rank-two update"" to a matrix?","What's a ""rank-two update"" to a matrix?",,"I'm reading a book, and on its numerical optimization chapter, more precisely in the BFGS algorithm section, the authors state that it's a ""rank-two update to the matrix"". I've searched for the definition, but I only found the definition for the rank-one update here . So my question is what is a rank-2 update, and why is the BFGS an example of such an update. Thanks in advance.","I'm reading a book, and on its numerical optimization chapter, more precisely in the BFGS algorithm section, the authors state that it's a ""rank-two update to the matrix"". I've searched for the definition, but I only found the definition for the rank-one update here . So my question is what is a rank-2 update, and why is the BFGS an example of such an update. Thanks in advance.",,"['linear-algebra', 'matrices', 'numerical-methods', 'numerical-optimization']"
92,Show that a matrix has full column rank,Show that a matrix has full column rank,,"Consider a matrix $A$ of size $T\times 2K$ . Consider the collection of numbers $k_1,k_2,...,k_T$ , where each $k_t\in \{1,...,K\}$ . Each $t$ -th row of the matrix $A$ is structured as follows: its $k_t$ -th element is equal to $1$ its $(k_t+K)$ -th element is equal to some scalar $\delta_t>0$ all other elements are zero. Further, $A$ does not contain zero columns . I believe that the following holds: $A$ has full column rank $2K$ if and only if there exists $t\in \{1,...,T\}$ and $\tau\in \{1,...,T\}$ with $t\neq \tau$ such that $$ k_t=k_{\tau} \quad \delta_t\neq \delta_{\tau} $$ Question: I wrote a proof for this claim (below), which looks to me a bit ""naive"". Is there anything more formal and, perhaps, simpler that you can suggest? My proof: $A$ has full column rank $2K$ if and only if the following system admits as unique solution $x=0_{2K\times 1}$ : $$ (*)\hspace{1cm} x_{k_t} + \delta_{t} x_{k_t+K}=0 \quad \text{ for each $t\in \{1,...,T\}$}. $$ First, I prove sufficiency. If there exist $t,\tau$ such that $k_t=k_{\tau}\equiv k$ with $\delta_{t}\neq\delta_{\tau}$ , then system $(*)$ contains the equations $$ \begin{aligned} & x_{k}+\delta_t x_{k+K}=0,\\ & x_{k}+\delta_{\tau} x_{k+K}=0,\\ \end{aligned} $$ from which it follows that $x_{k}=x_{k+K}=0$ . If the above is true for every $k\in \{1,...,K\}$ , then $x_{k}=x_{k+K}=0$ for each $k\in \{1,...,K\}$ . Hence, the unique solution of system $(*)$ is $x=0_{2K\times 1}$ . Now, I prove necessity. Suppose there exist only a $t$ such that $k_t\equiv k$ . Then, system $(*)$ contains only one equation involving  any of $x_k,x_{k+K}$ , which is $$ x_{k}+\delta_t x_{k+K}=0, $$ which is satisfied for any values of $x_k, x_{k+K}$ such that $x_k=-\delta_t x_{k+K}$ . Therefore, system $(*)$ has not a unique solution.","Consider a matrix of size . Consider the collection of numbers , where each . Each -th row of the matrix is structured as follows: its -th element is equal to its -th element is equal to some scalar all other elements are zero. Further, does not contain zero columns . I believe that the following holds: has full column rank if and only if there exists and with such that Question: I wrote a proof for this claim (below), which looks to me a bit ""naive"". Is there anything more formal and, perhaps, simpler that you can suggest? My proof: has full column rank if and only if the following system admits as unique solution : First, I prove sufficiency. If there exist such that with , then system contains the equations from which it follows that . If the above is true for every , then for each . Hence, the unique solution of system is . Now, I prove necessity. Suppose there exist only a such that . Then, system contains only one equation involving  any of , which is which is satisfied for any values of such that . Therefore, system has not a unique solution.","A T\times 2K k_1,k_2,...,k_T k_t\in \{1,...,K\} t A k_t 1 (k_t+K) \delta_t>0 A A 2K t\in \{1,...,T\} \tau\in \{1,...,T\} t\neq \tau 
k_t=k_{\tau} \quad \delta_t\neq \delta_{\tau}
 A 2K x=0_{2K\times 1} 
(*)\hspace{1cm} x_{k_t} + \delta_{t} x_{k_t+K}=0 \quad \text{ for each t\in \{1,...,T\}}.
 t,\tau k_t=k_{\tau}\equiv k \delta_{t}\neq\delta_{\tau} (*) 
\begin{aligned}
& x_{k}+\delta_t x_{k+K}=0,\\
& x_{k}+\delta_{\tau} x_{k+K}=0,\\
\end{aligned}
 x_{k}=x_{k+K}=0 k\in \{1,...,K\} x_{k}=x_{k+K}=0 k\in \{1,...,K\} (*) x=0_{2K\times 1} t k_t\equiv k (*) x_k,x_{k+K} 
x_{k}+\delta_t x_{k+K}=0,
 x_k, x_{k+K} x_k=-\delta_t x_{k+K} (*)","['linear-algebra', 'matrices', 'matrix-rank']"
93,"Proof that a Markov matrix, $M$, has$ |\det(M)|=1$ iff $M$ is a permutation matrix","Proof that a Markov matrix, , has iff  is a permutation matrix",M  |\det(M)|=1 M,"I've read in a number of places (eg here ) that if a matrix $M$ is Markov (aka stochastic), then $|\det(M)|\leq 1$ , with equality $|\det(M)|=1\iff$ $M$ is a permutation matrix. It is fairly easy to show the first part of this result, based on the eigenvalues of $M$ being of magnitude 1 or less , but I'm struggling with the second part. If anyone could provide a proof (or reference) of this result, I would be much obliged.","I've read in a number of places (eg here ) that if a matrix is Markov (aka stochastic), then , with equality is a permutation matrix. It is fairly easy to show the first part of this result, based on the eigenvalues of being of magnitude 1 or less , but I'm struggling with the second part. If anyone could provide a proof (or reference) of this result, I would be much obliged.",M |\det(M)|\leq 1 |\det(M)|=1\iff M M,"['linear-algebra', 'matrices', 'determinant']"
94,Identity involving matrices of binomial coefficients,Identity involving matrices of binomial coefficients,,"Let $P = (p_{i,j})$ and $Q = (q_{i,j})$ be the $n \times n$ lower triangular matrices defined by $$p_{i,j} =  \begin{cases} \dbinom{i}{j-1} & \text{for}\ 1\le j\le i \le n\\ \hfill 0 \hfill & \text{elsewhere} \end{cases}$$ and $$q_{i,j} = \begin{cases} \dbinom{i}{j}   & \text{for}\ 1 \le j \le i \le n\\ \hfill 0 \hfill & \text{elsewhere} \end{cases}$$ For any square matrix $M=(m_{i,j})$ , define $M^{alt} = (m_{i,j}^{alt})$ with entries $m_{i,j}^{alt} = (-1)^{i+j} m_{i,j}$ . The following appears to be true per Mathematica: $$P(P^{-1})^{alt} = Q.$$ Any ideas how to go about proving it? Thank you.","Let and be the lower triangular matrices defined by and For any square matrix , define with entries . The following appears to be true per Mathematica: Any ideas how to go about proving it? Thank you.","P = (p_{i,j}) Q = (q_{i,j}) n \times n p_{i,j} = 
\begin{cases}
\dbinom{i}{j-1} & \text{for}\ 1\le j\le i \le n\\
\hfill 0 \hfill & \text{elsewhere}
\end{cases} q_{i,j} =
\begin{cases}
\dbinom{i}{j}   & \text{for}\ 1 \le j \le i \le n\\
\hfill 0 \hfill & \text{elsewhere}
\end{cases} M=(m_{i,j}) M^{alt} = (m_{i,j}^{alt}) m_{i,j}^{alt} = (-1)^{i+j} m_{i,j} P(P^{-1})^{alt} = Q.","['combinatorics', 'matrices', 'binomial-coefficients']"
95,Determinant of $n\times n$ Matrix Linear Algebra,Determinant of  Matrix Linear Algebra,n\times n,"So, I have a matrix $$   A = \begin{pmatrix}    0 & 1 & 1 & ... & 1 \\   1 & 0 & x & ... & x \\   1 & x & 0 & ... & x \\   \vdots & \vdots & \vdots & \ddots & \vdots \\   1 & x & x & ... & 0   \end{pmatrix} $$ I need to evaluate it's determinant. At first I calculated $\det A$ for $n=2,3,4$ . And I got a pattern $\det A=(-1)^{n-1}(n-1)x^{n-2}$ for every $n\ge2$ . But I need to solve it differently. I added all rows to the first, then multiplied every row (except 1) by $(1+x(n-2))$ and subtracted first row multiplied by $x$ . This is what I got: $$   \begin{pmatrix}    n-1 & 1+x(n-2) & 1+x(n-2) & ... & 1+x(n-2) \\   1-x & -x(1+x(n-2)) & 0 & ... & 0 \\   1-x & 0 & -x(1+x(n-2)) & ... & 0 \\   \vdots & \vdots & \vdots & \ddots & \vdots \\   1-x & 0 & 0 & ... & -x(1+x(n-2))   \end{pmatrix} $$ Now I can multiply diagonal elements, but I don't know what can I do with the rest of it. Any hints will be helful.","So, I have a matrix I need to evaluate it's determinant. At first I calculated for . And I got a pattern for every . But I need to solve it differently. I added all rows to the first, then multiplied every row (except 1) by and subtracted first row multiplied by . This is what I got: Now I can multiply diagonal elements, but I don't know what can I do with the rest of it. Any hints will be helful.","
  A = \begin{pmatrix} 
  0 & 1 & 1 & ... & 1 \\
  1 & 0 & x & ... & x \\
  1 & x & 0 & ... & x \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  1 & x & x & ... & 0
  \end{pmatrix}
 \det A n=2,3,4 \det A=(-1)^{n-1}(n-1)x^{n-2} n\ge2 (1+x(n-2)) x 
  \begin{pmatrix} 
  n-1 & 1+x(n-2) & 1+x(n-2) & ... & 1+x(n-2) \\
  1-x & -x(1+x(n-2)) & 0 & ... & 0 \\
  1-x & 0 & -x(1+x(n-2)) & ... & 0 \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  1-x & 0 & 0 & ... & -x(1+x(n-2))
  \end{pmatrix}
","['linear-algebra', 'matrices', 'determinant']"
96,"If $a$ is an eigenvalue for $A$ and $b$ is an eigenvalue for $B$, what is an eigenvalue for $AB$?","If  is an eigenvalue for  and  is an eigenvalue for , what is an eigenvalue for ?",a A b B AB,Let both $A$ and $B$ be two $n \times n$ matrices such that $a$ is an eigenvalue for $A$ and $b$ is an eigenvalue for $B$ . How can I show that $ab$ is an eigenvalue for $AB$ ? I began my solution by stating: $$Av = av$$ $$Bw = bw$$ $$\implies ABvw = abvw$$ $$\implies ABvw = abvw$$ But I'm not sure what my last step should be to complete the proof. Any guidance is greatly appreciated!,Let both and be two matrices such that is an eigenvalue for and is an eigenvalue for . How can I show that is an eigenvalue for ? I began my solution by stating: But I'm not sure what my last step should be to complete the proof. Any guidance is greatly appreciated!,A B n \times n a A b B ab AB Av = av Bw = bw \implies ABvw = abvw \implies ABvw = abvw,['linear-algebra']
97,Show that controllability and observability are not affected by replacing A with (A+αI ),Show that controllability and observability are not affected by replacing A with (A+αI ),,"I have been asked to show that controllability and observability are not affected by replacing $A$ with $(A+αI)$ . And also to show that this is not necessarily true for stabilizability. I have thought about different approaches: Popov-Belevitch-Hautus test If the system is controllable must be true that $$Rank [(A - \lambda I) B] = n$$ If we replace $A$ with $(A+αI)$ then $$Rank [((A +αI) - \lambda I) B] $$ $$Rank [(A  - (\lambda -α)I) B] $$ $$Rank [(A  - gI) B]  $$ where $g = (\lambda -α) \in C$ $(A  - gI)$ has already $rank = n$ for all $g \in C$ except for those which are eigenvalues for $A$ . For those values, we need to prove that the concatenation of $B$ will guarantee the rank to be n. The issue is that since B is the same we can't guarantee it. Controllability Matrix Given an LTI if and only if the control matrix $C$ has full column rank, then the system is controllable. $$ C = [B ,AB, A²B ... A^{n-1} B]$$ but then altering the diagonal of $A$ can affect the rank of C, or if not, how come? Controllability Gramian I tried to plug $(A+αI)$ in the integral, but doing that then I don't know how to prove that the matrix $W$ is still nonsingular for any t > 0.","I have been asked to show that controllability and observability are not affected by replacing with . And also to show that this is not necessarily true for stabilizability. I have thought about different approaches: Popov-Belevitch-Hautus test If the system is controllable must be true that If we replace with then where has already for all except for those which are eigenvalues for . For those values, we need to prove that the concatenation of will guarantee the rank to be n. The issue is that since B is the same we can't guarantee it. Controllability Matrix Given an LTI if and only if the control matrix has full column rank, then the system is controllable. but then altering the diagonal of can affect the rank of C, or if not, how come? Controllability Gramian I tried to plug in the integral, but doing that then I don't know how to prove that the matrix is still nonsingular for any t > 0.","A (A+αI) Rank [(A - \lambda I) B] = n A (A+αI) Rank [((A +αI) - \lambda I) B]  Rank [(A  - (\lambda -α)I) B]  Rank [(A  - gI) B]   g = (\lambda -α) \in C (A  - gI) rank = n g \in C A B C  C = [B ,AB, A²B ... A^{n-1} B] A (A+αI) W","['linear-algebra', 'matrices', 'control-theory', 'linear-control']"
98,How to get the integral of $\log(\det(A + Bt))$ w.r.t variable t?,How to get the integral of  w.r.t variable t?,\log(\det(A + Bt)),"Suppose we have two positive definite matrices $A$ and $B$ , now I want to get the integral of: \begin{align} \int_{a}^{b} \log(\det(A + Bt)) dt ~~~~~~~~~~~~\text{for } a, b > 0  \end{align} Analytic solution is better, but good approximation is also acceptable here. I tried to approximate this integral via interpolation, but as $A$ , $B$ are of large size (i.e, 100 × 100), that maybe too slow to calculate $\det(A + Bt)$ every time. When I seek the analytics solution, I met the problem of finding: $\frac{d(\det(A + Bt))}{dt}$ . If you have any ideas or inference on this question, welcome to your answer and suggestions, thank so much!!","Suppose we have two positive definite matrices and , now I want to get the integral of: Analytic solution is better, but good approximation is also acceptable here. I tried to approximate this integral via interpolation, but as , are of large size (i.e, 100 × 100), that maybe too slow to calculate every time. When I seek the analytics solution, I met the problem of finding: . If you have any ideas or inference on this question, welcome to your answer and suggestions, thank so much!!","A B \begin{align}
\int_{a}^{b} \log(\det(A + Bt)) dt ~~~~~~~~~~~~\text{for } a, b > 0 
\end{align} A B \det(A + Bt) \frac{d(\det(A + Bt))}{dt}","['integration', 'matrices', 'definite-integrals', 'applications']"
99,Finding the determinant of cofactor matrix,Finding the determinant of cofactor matrix,,"Let \begin{align} \Delta &=  \begin{vmatrix} x_1 & x_2 & x_3 \\  x_4 & x_5 & x_6 \\  x_7 & x_8 & x_9 \end{vmatrix} \end{align} and let $C_i$ represent the cofactor of $x_i$ . Find \begin{align}  \begin{vmatrix} C_1 & C_2 & C_3 \\  C_4 & C_5 & C_6 \\  C_7 & C_8 & C_9 \end{vmatrix} \end{align} in terms of $\Delta$ . Generalize that if $\Delta$ represents the determinant of a $n\times n$ matrix, then the determinant of the cofactor matrix is $\Delta^{n-1}$ . Looking at the generalization, I was tempted to multiply both determinants. $$ \begin{vmatrix} x_1 & x_2 & x_3 \\  x_4 & x_5 & x_6 \\  x_7 & x_8 & x_9 \end{vmatrix} \times  \begin{vmatrix} C_1 & C_2 & C_3 \\  C_4 & C_5 & C_6 \\  C_7 & C_8 & C_9 \end{vmatrix} = \begin{vmatrix} x_1C_1+x_2C_2+x_3C_3 & x_1C_4+x_2C_5+x_3C_6 & x_1C_7+x_2C_8+x_3C_9 \\  x_4C_1+x_5C_2+x_6C_3 & x_4C_4+x_5C_5+x_6C_6 & x_4C_7+x_5C_8+x_6C_9 \\  x_7C_1+x_8C_2+x_9C_3 & x_7C_4+x_8C_5+x_9C_6 & x_7C_7+x_8C_8+x_9C_9 \end{vmatrix} $$ $$ \implies \Delta \times  \begin{vmatrix} C_1 & C_2 & C_3 \\  C_4 & C_5 & C_6 \\  C_7 & C_8 & C_9 \end{vmatrix} = \begin{vmatrix} \Delta & x_1C_4+x_2C_5+x_3C_6 & x_1C_7+x_2C_8+x_3C_9 \\  x_4C_1+x_5C_2+x_6C_3 & \Delta & x_4C_7+x_5C_8+x_6C_9 \\  x_7C_1+x_8C_2+x_9C_3 & x_7C_4+x_8C_5+x_9C_6 & \Delta  \end{vmatrix} $$ Can someone help me figure out the next step? (This huge determinant must be equal to $\Delta^3$ .)","Let and let represent the cofactor of . Find in terms of . Generalize that if represents the determinant of a matrix, then the determinant of the cofactor matrix is . Looking at the generalization, I was tempted to multiply both determinants. Can someone help me figure out the next step? (This huge determinant must be equal to .)","\begin{align}
\Delta &= 
\begin{vmatrix}
x_1 & x_2 & x_3 \\ 
x_4 & x_5 & x_6 \\ 
x_7 & x_8 & x_9
\end{vmatrix}
\end{align} C_i x_i \begin{align} 
\begin{vmatrix}
C_1 & C_2 & C_3 \\ 
C_4 & C_5 & C_6 \\ 
C_7 & C_8 & C_9
\end{vmatrix}
\end{align} \Delta \Delta n\times n \Delta^{n-1} 
\begin{vmatrix}
x_1 & x_2 & x_3 \\ 
x_4 & x_5 & x_6 \\ 
x_7 & x_8 & x_9
\end{vmatrix}
\times 
\begin{vmatrix}
C_1 & C_2 & C_3 \\ 
C_4 & C_5 & C_6 \\ 
C_7 & C_8 & C_9
\end{vmatrix}
=
\begin{vmatrix}
x_1C_1+x_2C_2+x_3C_3 & x_1C_4+x_2C_5+x_3C_6 & x_1C_7+x_2C_8+x_3C_9 \\ 
x_4C_1+x_5C_2+x_6C_3 & x_4C_4+x_5C_5+x_6C_6 & x_4C_7+x_5C_8+x_6C_9 \\ 
x_7C_1+x_8C_2+x_9C_3 & x_7C_4+x_8C_5+x_9C_6 & x_7C_7+x_8C_8+x_9C_9
\end{vmatrix}
 
\implies
\Delta
\times 
\begin{vmatrix}
C_1 & C_2 & C_3 \\ 
C_4 & C_5 & C_6 \\ 
C_7 & C_8 & C_9
\end{vmatrix}
=
\begin{vmatrix}
\Delta & x_1C_4+x_2C_5+x_3C_6 & x_1C_7+x_2C_8+x_3C_9 \\ 
x_4C_1+x_5C_2+x_6C_3 & \Delta & x_4C_7+x_5C_8+x_6C_9 \\ 
x_7C_1+x_8C_2+x_9C_3 & x_7C_4+x_8C_5+x_9C_6 & \Delta 
\end{vmatrix}
 \Delta^3","['matrices', 'determinant']"
