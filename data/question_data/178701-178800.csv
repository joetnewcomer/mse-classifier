,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Cross product of operators,Cross product of operators,,How to show that: $ (-i\nabla-eA)\times(-i\nabla-eA) = (ie\nabla \times A) $ i and e are constants A is a vector field $\nabla$ = vector differential operator,How to show that: $ (-i\nabla-eA)\times(-i\nabla-eA) = (ie\nabla \times A) $ i and e are constants A is a vector field $\nabla$ = vector differential operator,,"['differential-geometry', 'multivariable-calculus', 'operator-theory']"
1,"Maximum value of $f(x, y, z) = yz + xz + xy − 2xyz$ for $x, y, z \ge 1$",Maximum value of  for,"f(x, y, z) = yz + xz + xy − 2xyz x, y, z \ge 1","I know I should try to explore a limit such as $\lim_{(x,y,z)\rightarrow(\infty,a, b)} f(x,y,z)$ where $a,b$ are some constants. Is it true that I can directly replace $y$ and $z$ in the limit by way of continuity? Then the limit would become $\lim_{x\rightarrow\infty}(a+b)x + ab - 2abx = \lim_{x\rightarrow\infty} (a-2ab+b)x + ab = \pm\infty$ depending on $a,b$ therefore the function cannot have a maximum value. I'm fairly certain my thinking is wrong but I don't know how to proceed with this problem. Please help. EDIT: Calculating the 1st partial derivatives like so: $\begin{cases}f'_x = z + y - 2zy = 0\\f'_y = x + z - 2xz = 0\\f'_z = x + y - 2xy = 0\end{cases}$ $f'_y - f'_z = (z-y) - 2x(z-y) = (1-2x)(z-y)$ where the only solution is $z=y$ since $x \ge 1 \implies x\neq 1/2$ . Then substituting into the first two equations I get: $\begin{cases}x+y-2xy=0\\2y-2y^2=0\iff y(1-y)(1+y) = 0\end{cases}$ The only solution is $y=1$ since the other fall outside of the possible values for $y$ . Then going back to $f'_y$ I get $x + 1 - 2x = 0 \iff x = 1$ . Therefore there is a critical point at $(1,1,1)$ . However $f''_{xx} = 0$ which means you can't tell anything about what kind of point it is solely off of the derivative test. How can I proceed from here?",I know I should try to explore a limit such as where are some constants. Is it true that I can directly replace and in the limit by way of continuity? Then the limit would become depending on therefore the function cannot have a maximum value. I'm fairly certain my thinking is wrong but I don't know how to proceed with this problem. Please help. EDIT: Calculating the 1st partial derivatives like so: where the only solution is since . Then substituting into the first two equations I get: The only solution is since the other fall outside of the possible values for . Then going back to I get . Therefore there is a critical point at . However which means you can't tell anything about what kind of point it is solely off of the derivative test. How can I proceed from here?,"\lim_{(x,y,z)\rightarrow(\infty,a, b)} f(x,y,z) a,b y z \lim_{x\rightarrow\infty}(a+b)x + ab - 2abx = \lim_{x\rightarrow\infty} (a-2ab+b)x + ab = \pm\infty a,b \begin{cases}f'_x = z + y - 2zy = 0\\f'_y = x + z - 2xz = 0\\f'_z = x + y - 2xy = 0\end{cases} f'_y - f'_z = (z-y) - 2x(z-y) = (1-2x)(z-y) z=y x \ge 1 \implies x\neq 1/2 \begin{cases}x+y-2xy=0\\2y-2y^2=0\iff y(1-y)(1+y) = 0\end{cases} y=1 y f'_y x + 1 - 2x = 0 \iff x = 1 (1,1,1) f''_{xx} = 0","['real-analysis', 'multivariable-calculus']"
2,Evaluation of the given line integral,Evaluation of the given line integral,,"Question: Evaluate $\int_{C}$ B .d r along the curve $x^{2}$ + $y^{2}$ =1, $z$ = 1 in the positive direction from (0,1,2) to (1,0,2);given B = (xz²+y) i +(z-y) j +(xy-z) k The question itself is easy,but I don't know how to handle z=1 Here's my attempt:- $\int_{C}$ B .d r = $\int_{C}$ (xz²+y) i +(z-y) j +(xy-z) k .(dx i +dy j +dz k ) = $\int_{C}$ (xz²+y)dx+ $\int_{C}$ (z-y)dy+ $\int_{C}$ (xy-z)dz Should I put z=1 in the above integral?after this I will integrate all the three integrals and put up the values given in the question.","Question: Evaluate B .d r along the curve + =1, = 1 in the positive direction from (0,1,2) to (1,0,2);given B = (xz²+y) i +(z-y) j +(xy-z) k The question itself is easy,but I don't know how to handle z=1 Here's my attempt:- B .d r = (xz²+y) i +(z-y) j +(xy-z) k .(dx i +dy j +dz k ) = (xz²+y)dx+ (z-y)dy+ (xy-z)dz Should I put z=1 in the above integral?after this I will integrate all the three integrals and put up the values given in the question.",\int_{C} x^{2} y^{2} z \int_{C} \int_{C} \int_{C} \int_{C} \int_{C},"['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral', 'line-integrals']"
3,Why does $\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\vec{a}\frac{4\pi}{3}\delta\left(\vec{r}\right)$?,Why does ?,\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\vec{a}\frac{4\pi}{3}\delta\left(\vec{r}\right),"I got this identity from a physics textbook , but figure the Math Stack Exchange could help. Allegedly: $$ \left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\vec{a}\frac{4\pi}{3}\delta\left(\vec{r}\right) $$ Where $r=\left(x^2+y^2+z^2\right)^{1/2}$ ans $\vec{a}$ is a constant vector. And I must have misunderstanding since when I carry out the computation, $$ \nabla\frac{1}{r}=-\frac{\vec{r}}{r^3}=-\hat{x}\frac{x}{r^3}-\hat{y}\frac{y}{r^3}-\hat{z}\frac{z}{r^3} $$ $$ \left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\left( a_x\partial_x+a_y\partial_y+a_z\partial_z \right)\left( -\hat{x}\frac{x}{r^3}-\hat{y}\frac{y}{r^3}-\hat{z}\frac{z}{r^3} \right) $$ Using: $$ \partial_x\frac{1}{r^3}=-3\frac{x}{r^5} $$ I get, for the x-component of $\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}$ : \begin{equation} \tag{1}\label{xComponent} a_x\frac{-1}{r^3}+3a_x\frac{x^2}{r^5}+3a_y\frac{xy}{r^5}+3a_z\frac{xz}{r^5} \end{equation} I see how the x-component goes to $\infty$ when $\vec{r}=0$ , but do not see how it is zero for arbitrary $\vec{r}\neq 0$ and the factors of $a_y$ and $a_z$ make it seem that the x-component of $\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}$ is not proportional to $a_x$ . Which brings me to my question: Why does $\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\vec{a}\frac{4\pi}{3}\delta\left(\vec{r}\right)$ ? By the way, the identity can be found in 2.8 of appendix F in Blundell's Magnetism in Condensed Matter . Thanks all in advance","I got this identity from a physics textbook , but figure the Math Stack Exchange could help. Allegedly: Where ans is a constant vector. And I must have misunderstanding since when I carry out the computation, Using: I get, for the x-component of : I see how the x-component goes to when , but do not see how it is zero for arbitrary and the factors of and make it seem that the x-component of is not proportional to . Which brings me to my question: Why does ? By the way, the identity can be found in 2.8 of appendix F in Blundell's Magnetism in Condensed Matter . Thanks all in advance","
\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\vec{a}\frac{4\pi}{3}\delta\left(\vec{r}\right)
 r=\left(x^2+y^2+z^2\right)^{1/2} \vec{a} 
\nabla\frac{1}{r}=-\frac{\vec{r}}{r^3}=-\hat{x}\frac{x}{r^3}-\hat{y}\frac{y}{r^3}-\hat{z}\frac{z}{r^3}
 
\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\left(
a_x\partial_x+a_y\partial_y+a_z\partial_z
\right)\left(
-\hat{x}\frac{x}{r^3}-\hat{y}\frac{y}{r^3}-\hat{z}\frac{z}{r^3}
\right)
 
\partial_x\frac{1}{r^3}=-3\frac{x}{r^5}
 \left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r} \begin{equation}
\tag{1}\label{xComponent}
a_x\frac{-1}{r^3}+3a_x\frac{x^2}{r^5}+3a_y\frac{xy}{r^5}+3a_z\frac{xz}{r^5}
\end{equation} \infty \vec{r}=0 \vec{r}\neq 0 a_y a_z \left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r} a_x \left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\vec{a}\frac{4\pi}{3}\delta\left(\vec{r}\right)",['multivariable-calculus']
4,"Unambiguous derivative notation in Spivak's ""Calculus on Manifolds""","Unambiguous derivative notation in Spivak's ""Calculus on Manifolds""",,"I don't understand Spivak's comment at the end that $f$ means something different on the two sides of the equation. Don't they both refer to the same function? Also, the expression $D_1(f \circ (g, h))$ isn't clear about which variable should be first. The first var of $f$ is $u$ , but the first variable of $g, h$ is $x$ . So I'm wondering what that statement means since this notation purports to remove ambiguities. I'm self-studying to prepare for grad school after a long gap, so I don't have a professor to consult. Thank you for any advice.","I don't understand Spivak's comment at the end that means something different on the two sides of the equation. Don't they both refer to the same function? Also, the expression isn't clear about which variable should be first. The first var of is , but the first variable of is . So I'm wondering what that statement means since this notation purports to remove ambiguities. I'm self-studying to prepare for grad school after a long gap, so I don't have a professor to consult. Thank you for any advice.","f D_1(f \circ (g, h)) f u g, h x","['calculus', 'multivariable-calculus', 'notation', 'manifolds']"
5,"Figuring out if $\lim_{(x,y)\to(0,0)}\frac{-x^6y^1(x^2+1)}{(x^6+y^2)\sqrt{x^2+y^2}}$ exists",Figuring out if  exists,"\lim_{(x,y)\to(0,0)}\frac{-x^6y^1(x^2+1)}{(x^6+y^2)\sqrt{x^2+y^2}}","I need to find out if the limit exists. $$\lim_{(x,y)\to(0,0)}\frac{-x^6y^1(x^2+1)}{(x^6+y^2)\sqrt{x^2+y^2}}$$ First, I approached the limit from $y=0$ , and the result was $\frac{0}{x^7}$ . Then, I approached from $x = 0$ , and the result was $\frac{0}{y^3}$ . This made me assume that the limit does not exist. However, Wolfram Alpha calculated the limit as zero. What is the solution to this question?","I need to find out if the limit exists. First, I approached the limit from , and the result was . Then, I approached from , and the result was . This made me assume that the limit does not exist. However, Wolfram Alpha calculated the limit as zero. What is the solution to this question?","\lim_{(x,y)\to(0,0)}\frac{-x^6y^1(x^2+1)}{(x^6+y^2)\sqrt{x^2+y^2}} y=0 \frac{0}{x^7} x = 0 \frac{0}{y^3}","['limits', 'multivariable-calculus']"
6,"How do you find the multivariable limit $\lim_{(x,y)\to(0,0)}\frac{xy}{\sqrt x +\sqrt y }$",How do you find the multivariable limit,"\lim_{(x,y)\to(0,0)}\frac{xy}{\sqrt x +\sqrt y }","$\lim_{(x,y)\to(0,0)}\frac{xy}{\sqrt x+\sqrt y}$ considering domain $\{(x,y) \in \mathbb{R}^2 : x,y \ge 0, (x,y) \ne 0\}$ I tried using polar coordinates, but the theta function is unbounded. I also tried using the sandwich theorem, but could not find appropriate bounds. How do I approach this question","considering domain I tried using polar coordinates, but the theta function is unbounded. I also tried using the sandwich theorem, but could not find appropriate bounds. How do I approach this question","\lim_{(x,y)\to(0,0)}\frac{xy}{\sqrt x+\sqrt y} \{(x,y) \in \mathbb{R}^2 : x,y \ge 0, (x,y) \ne 0\}","['limits', 'multivariable-calculus']"
7,Solve nonlinear system of equations and show it has infinite solutions,Solve nonlinear system of equations and show it has infinite solutions,,"So we have this system of nonlinear equations \begin{align*} \sin(x+u) - e^y + 1 = 0\\ x^2 + y + e^u = 1 \end{align*} and we want to show that it has infinitely many solutions $(x,y,u)$ . I tried starting by finding the Jacobian matrix (so I can use the theorem of implicit functions) $$ J_{(x,y,u)} = \left[\begin{array}{ccc}    \cos(x+u) & -e^y & \cos(x+u) \\    2x & 1 & e^u     \end{array}\right] $$ but obviously this is not a square matrix so the determinant is not defined. I was thinking we could add another equation, I was told "" $H(x,y,u) = u$ (or $x$ or $y$ )"" could work but how is that ""allowed""?","So we have this system of nonlinear equations and we want to show that it has infinitely many solutions . I tried starting by finding the Jacobian matrix (so I can use the theorem of implicit functions) but obviously this is not a square matrix so the determinant is not defined. I was thinking we could add another equation, I was told "" (or or )"" could work but how is that ""allowed""?","\begin{align*}
\sin(x+u) - e^y + 1 = 0\\
x^2 + y + e^u = 1
\end{align*} (x,y,u) 
J_{(x,y,u)} = \left[\begin{array}{ccc}
   \cos(x+u) & -e^y & \cos(x+u) \\
   2x & 1 & e^u
    \end{array}\right]
 H(x,y,u) = u x y","['calculus', 'multivariable-calculus', 'jacobian', 'implicit-function-theorem', 'inverse-function-theorem']"
8,Fundamental stress and pressure tensors of the Stokes system in $\mathbb{R}^3$.,Fundamental stress and pressure tensors of the Stokes system in .,\mathbb{R}^3,"Let $\mathbf{\mathcal{G}}$ denote the Oseen-Burgers tensor and $\mathbf{\Pi}$ denote the fundamental pressure vector in $\mathbb{R}^3$ , i.e. on components we have $$\mathbf{\mathcal{G}}_{jk}(\mathbf{x}-\mathbf{x_0})=\frac{1}{8\pi}\left\{\frac{\delta_{jk}}{|\mathbf{x}-\mathbf{x_0}|}+\frac{\hat{x}_j\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^3}\right\}, \mathbf{\Pi}_k(\mathbf{x}-\mathbf{x_0})=\frac{1}{8\pi}\frac{2\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^3},$$ where $$\hat{\mathbf{x}}:=\mathbf{x}-\mathbf{x_0}=(\hat{x}_1,..., \hat{x}_n).$$ Then, the pair $(\mathbf{\mathcal{G}}, \mathbf{\Pi})$ satisfies the following equations: $$\Delta_{\mathbf{x}}\mathbf{\mathcal{G}}(\mathbf{x}-\mathbf{x_0})-\nabla_{\mathbf{x}}\mathbf{\Pi}(\mathbf{x}-\mathbf{x_0})=-\delta_{\mathbf{x_0}}(\mathbf{x})\mathbb{I}, \operatorname{div}_{\mathbf{x}}\mathbf{\mathcal{G}}(\mathbf{x}-\mathbf{x_0})=0, \forall \mathbf{x}\in \mathbb{R}^3.$$ This part I understand and it is all clear, but the following result I do not understand. Let $\mathbf{S}(S_{ijk})$ denote the fundamental stress tensor of the Stokes system in $\mathbb{R}^3$ and let $\mathbf{\Lambda}(\Lambda_{ik})$ denote the fundamental pressure tensor of the Stokes system in $\mathbb{R}^3$ , i.e. $$S_{ijk}(\mathbf{x}-\mathbf{x_0}):=-\mathbf{\Pi}_j(\mathbf{x}-\mathbf{x_0})\delta_{ik}+\frac{\partial \mathbf{\mathcal{G}}_{ij}}{\partial x_k}(\mathbf{x}-\mathbf{x_0})+\frac{\partial \mathbf{\mathcal{G}}_{kj}}{\partial x_i}(\mathbf{x}-\mathbf{x_0})=-\frac{3}{4\pi}\frac{\hat{x}_i\hat{x}_j\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^5},$$ $$\Lambda_{ik}(\mathbf{x}-\mathbf{x_0}):=-2\frac{\partial \Pi_i(\mathbf{x}-\mathbf{x_0})}{\partial x_k}=-2\frac{\Pi_k(\mathbf{x}-\mathbf{x_0})}{\partial x_i}=\frac{1}{2\pi}\left(-\frac{\delta_{ik}}{|\mathbf{x}-\mathbf{x_0}|^3}+3\frac{\hat{x}_i\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^5}\right).$$ Then, I am told that the pair $(S_{ijk}, \Lambda_{ik})$ satisfies the Stokes system in $\mathbb{R}^3$ whenever $\mathbf{x}\ne\mathbf{x_0}$ , i.e. $$\begin{cases}  -\Delta_{\mathbf{x_0}}S_{ijk}(\mathbf{x}-\mathbf{x_0})+\frac{\partial \Lambda_{ik}(\mathbf{x}-\mathbf{x_0})}{\partial x_{0;j}}=0, \\ \frac{\partial S_{ijk}(\mathbf{x}-\mathbf{x_0})}{\partial x_{0;j}}=0, \end{cases}$$ where $\mathbf{x_0}=(x_{0;1}, ..., x_{0:n})$ . Of course, the Einstein summation convention is used. This claim is supposedly obvious using the relations I know about $\mathbf{\mathcal{G}}$ and $\mathbf{\Pi}$ , but I cannot see how to obtain it. I tried writing on components the fact that $(\mathbf{\mathcal{G}}, \mathbf{\Pi})$ is a fundamental solution of the Stokes system and using the symmetry of these tensors, but I got nowhere.","Let denote the Oseen-Burgers tensor and denote the fundamental pressure vector in , i.e. on components we have where Then, the pair satisfies the following equations: This part I understand and it is all clear, but the following result I do not understand. Let denote the fundamental stress tensor of the Stokes system in and let denote the fundamental pressure tensor of the Stokes system in , i.e. Then, I am told that the pair satisfies the Stokes system in whenever , i.e. where . Of course, the Einstein summation convention is used. This claim is supposedly obvious using the relations I know about and , but I cannot see how to obtain it. I tried writing on components the fact that is a fundamental solution of the Stokes system and using the symmetry of these tensors, but I got nowhere.","\mathbf{\mathcal{G}} \mathbf{\Pi} \mathbb{R}^3 \mathbf{\mathcal{G}}_{jk}(\mathbf{x}-\mathbf{x_0})=\frac{1}{8\pi}\left\{\frac{\delta_{jk}}{|\mathbf{x}-\mathbf{x_0}|}+\frac{\hat{x}_j\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^3}\right\}, \mathbf{\Pi}_k(\mathbf{x}-\mathbf{x_0})=\frac{1}{8\pi}\frac{2\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^3}, \hat{\mathbf{x}}:=\mathbf{x}-\mathbf{x_0}=(\hat{x}_1,..., \hat{x}_n). (\mathbf{\mathcal{G}}, \mathbf{\Pi}) \Delta_{\mathbf{x}}\mathbf{\mathcal{G}}(\mathbf{x}-\mathbf{x_0})-\nabla_{\mathbf{x}}\mathbf{\Pi}(\mathbf{x}-\mathbf{x_0})=-\delta_{\mathbf{x_0}}(\mathbf{x})\mathbb{I}, \operatorname{div}_{\mathbf{x}}\mathbf{\mathcal{G}}(\mathbf{x}-\mathbf{x_0})=0, \forall \mathbf{x}\in \mathbb{R}^3. \mathbf{S}(S_{ijk}) \mathbb{R}^3 \mathbf{\Lambda}(\Lambda_{ik}) \mathbb{R}^3 S_{ijk}(\mathbf{x}-\mathbf{x_0}):=-\mathbf{\Pi}_j(\mathbf{x}-\mathbf{x_0})\delta_{ik}+\frac{\partial \mathbf{\mathcal{G}}_{ij}}{\partial x_k}(\mathbf{x}-\mathbf{x_0})+\frac{\partial \mathbf{\mathcal{G}}_{kj}}{\partial x_i}(\mathbf{x}-\mathbf{x_0})=-\frac{3}{4\pi}\frac{\hat{x}_i\hat{x}_j\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^5}, \Lambda_{ik}(\mathbf{x}-\mathbf{x_0}):=-2\frac{\partial \Pi_i(\mathbf{x}-\mathbf{x_0})}{\partial x_k}=-2\frac{\Pi_k(\mathbf{x}-\mathbf{x_0})}{\partial x_i}=\frac{1}{2\pi}\left(-\frac{\delta_{ik}}{|\mathbf{x}-\mathbf{x_0}|^3}+3\frac{\hat{x}_i\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^5}\right). (S_{ijk}, \Lambda_{ik}) \mathbb{R}^3 \mathbf{x}\ne\mathbf{x_0} \begin{cases} 
-\Delta_{\mathbf{x_0}}S_{ijk}(\mathbf{x}-\mathbf{x_0})+\frac{\partial \Lambda_{ik}(\mathbf{x}-\mathbf{x_0})}{\partial x_{0;j}}=0, \\
\frac{\partial S_{ijk}(\mathbf{x}-\mathbf{x_0})}{\partial x_{0;j}}=0,
\end{cases} \mathbf{x_0}=(x_{0;1}, ..., x_{0:n}) \mathbf{\mathcal{G}} \mathbf{\Pi} (\mathbf{\mathcal{G}}, \mathbf{\Pi})","['multivariable-calculus', 'partial-differential-equations', 'mathematical-physics', 'fluid-dynamics']"
9,"Without using differential forms, can we unify or organize all the various multivariable integrals: multiple, line, flux, etc.?","Without using differential forms, can we unify or organize all the various multivariable integrals: multiple, line, flux, etc.?",,"In single dimension calculus, there is one and only one obvious way of integrating: over an interval.  Of course, there are multiple ways of defining the integral (Riemann sum, Darboux integral, Lebesgue integral), but, where defined, these definitions agree. Once we move from $\mathbb R$ to $\mathbb R^n$ , there seems to be a plethora of types of integrals, with most sources making no attempt to unify or organize them: In this section we will continue looking at line integrals and define the second kind of line integral we’ll be looking at... In this section we will define the third type of line integrals we’ll be looking at: line integrals of vector fields. The Area under a Curve and its Many Generalizations ... The method of doing this used is generalized to define a wide variety of integrals ... Is there a way to unify or at least organize all these types of integrals? My attempt to do so is below.  Is it correct? What revisions does it need? Update: My goal is to do this in a simple way, without invoking the complexity and abstraction of differential forms. For $f: \mathbb R \to \mathbb R^m$ , the one obvious way to integrate is over an interval of $\mathbb R$ .  When instead $f$ takes an input in $\mathbb R^n$ , we gain one, and in some cases two, new ways to integrate: Region (""Multiple"") Integrals For multiple dimensions, with $f: \mathbb R^n \to \mathbb R^m$ , we can take the integral over any ""nice"" $k \leq n$ dimensional region of $\mathbb R^n$ .  We can do this for regions of any dimension $k \leq n$ by partitioning it into very small subregions.  However, for $k > 1$ , these subregions are not defined by two endpoints such that we can define $\Delta x_i$ .  Rather, we need to take the size or region measure (hypervolume, volume, area, etc.) appropriate for $k$ (irrespective of $n$ ) of each subregion. This partition is, unlike intervals, unordered and unoriented, and the size of each subregion is always positive: $$\sum_{r_{i, j,...} \in R}f(\xi_{i,j,...})\cdot \|r_{i,j,...}\|.$$ The integral is the limit of such as $\max_{i,j,...} \|r_{i,j,...}\| \to 0$ (provided it exists).  (And the fact that the partition unordered and unoriented, with each subregion having positive size, is the reason why the Change of Variables Theorem uses the absolute value of the determinant and not the determinant itself.) By Fubini's theorem, this limit is (under reasonable conditions) equal to an iterated integral, and so these are often called multiple integrals .  However, it is important to emphasize that there is nothing ""multiple"" in the definition of multiple integrals: They are limits of single Riemann sums over partitions of regions with dimension $> 1$ . This definition therefore includes volume integrals , area integrals , line integrals over scalar fields ; all are simply region integrals, defined the same way, with the region having different dimensions. Vector Field Integrals: Line (dim = 1) and Flux (codim = 1) Furthermore, if $n = m$ , then $f$ defines a vector field , and we gain the ability to take the dot product $f(x) \cdot x$ .  This gives us two new ways to take integrals: line integrals and flux integrals . If the region is a curve (i.e. $k = 1$ ), we can approximate each subregion $r_i$ by a vector $s_i \in \mathbb R^n$ and take the Riemann sum of $$\sum f(\xi_i) \cdot s_i.$$ And if the region is a surface (i.e. $k = n - 1$ ), we can construct a flux integral by approximating each $r_i$ by an $k$ dimensional hyperplane $p_i$ and letting $n_i$ be the unique vector that is normal to $p_i$ , has magnitude equal to the ""size"" (volume, area, etc.) of $p_i$ , and has a sign determined by convention, giving the Riemann sum of $$\sum f(\xi_i) \cdot n_i.$$","In single dimension calculus, there is one and only one obvious way of integrating: over an interval.  Of course, there are multiple ways of defining the integral (Riemann sum, Darboux integral, Lebesgue integral), but, where defined, these definitions agree. Once we move from to , there seems to be a plethora of types of integrals, with most sources making no attempt to unify or organize them: In this section we will continue looking at line integrals and define the second kind of line integral we’ll be looking at... In this section we will define the third type of line integrals we’ll be looking at: line integrals of vector fields. The Area under a Curve and its Many Generalizations ... The method of doing this used is generalized to define a wide variety of integrals ... Is there a way to unify or at least organize all these types of integrals? My attempt to do so is below.  Is it correct? What revisions does it need? Update: My goal is to do this in a simple way, without invoking the complexity and abstraction of differential forms. For , the one obvious way to integrate is over an interval of .  When instead takes an input in , we gain one, and in some cases two, new ways to integrate: Region (""Multiple"") Integrals For multiple dimensions, with , we can take the integral over any ""nice"" dimensional region of .  We can do this for regions of any dimension by partitioning it into very small subregions.  However, for , these subregions are not defined by two endpoints such that we can define .  Rather, we need to take the size or region measure (hypervolume, volume, area, etc.) appropriate for (irrespective of ) of each subregion. This partition is, unlike intervals, unordered and unoriented, and the size of each subregion is always positive: The integral is the limit of such as (provided it exists).  (And the fact that the partition unordered and unoriented, with each subregion having positive size, is the reason why the Change of Variables Theorem uses the absolute value of the determinant and not the determinant itself.) By Fubini's theorem, this limit is (under reasonable conditions) equal to an iterated integral, and so these are often called multiple integrals .  However, it is important to emphasize that there is nothing ""multiple"" in the definition of multiple integrals: They are limits of single Riemann sums over partitions of regions with dimension . This definition therefore includes volume integrals , area integrals , line integrals over scalar fields ; all are simply region integrals, defined the same way, with the region having different dimensions. Vector Field Integrals: Line (dim = 1) and Flux (codim = 1) Furthermore, if , then defines a vector field , and we gain the ability to take the dot product .  This gives us two new ways to take integrals: line integrals and flux integrals . If the region is a curve (i.e. ), we can approximate each subregion by a vector and take the Riemann sum of And if the region is a surface (i.e. ), we can construct a flux integral by approximating each by an dimensional hyperplane and letting be the unique vector that is normal to , has magnitude equal to the ""size"" (volume, area, etc.) of , and has a sign determined by convention, giving the Riemann sum of","\mathbb R \mathbb R^n f: \mathbb R \to \mathbb R^m \mathbb R f \mathbb R^n f: \mathbb R^n \to \mathbb R^m k \leq n \mathbb R^n k \leq n k > 1 \Delta x_i k n \sum_{r_{i, j,...} \in R}f(\xi_{i,j,...})\cdot \|r_{i,j,...}\|. \max_{i,j,...} \|r_{i,j,...}\| \to 0 > 1 n = m f f(x) \cdot x k = 1 r_i s_i \in \mathbb R^n \sum f(\xi_i) \cdot s_i. k = n - 1 r_i k p_i n_i p_i p_i \sum f(\xi_i) \cdot n_i.","['real-analysis', 'calculus', 'integration', 'multivariable-calculus', 'definition']"
10,Proving that mixed partial derivatives can be taken in any order for for smooth functions on smooth manifolds,Proving that mixed partial derivatives can be taken in any order for for smooth functions on smooth manifolds,,"The following is a description of the partial derivative on smooth manifolds from John Lee's Introduction to Smooth Manifolds. Suppose $M$ is a smooth manifold and let $(U,\phi)$ be a smooth coordinate chart on $M$ . Then $\phi$ is a diffeomorphism from $U$ to an open subset $\hat{U}\subset \mathbb{R}^n$ . So $d\phi_p : T_p M \to T_{\phi(p)} \mathbb{R}^n$ is an isomorphism. Let the standard coordinate frame be $\partial/\partial x^1 |_{\phi(p)},\dots \partial / \partial x^n|_{\phi(p)}$ for $T_{\phi(p)} \mathbb{R}^n$ . Then we use the notation $\partial/ \partial x^i|_p$ as the preimages of these vectors under the isomorphism $d\phi_p$ that form a basis for $T_p M$ characterized by either of the following expressions: $$\frac{\partial}{\partial x^i}|_p = (d\phi_p)^{-1}(\frac{\partial}{\partial x^i}|_{\phi(p)}) = d(\phi^{-1})_{\phi(p)}(\frac{\partial}{\partial x^i}|_{\phi(p)}).$$ Unwinding the definitions, we see that $\partial/ \partial x^i|_p$ acts on a function $f\in C^\infty(U)$ by $$\frac{\partial}{\partial x^i}|_p f = \frac{\partial}{\partial x^i}|_{\phi(p)} (f\circ \phi^{-1}) = \frac{\partial \hat{f}}{\partial x^i}(\hat{p}),$$ where $\hat{f}=f\circ \phi^{-1}$ is the coordinate representation of $f$ , and $\hat{p}=(p^1, \dots, p^n)= \phi(p)$ is the coordinate representation of $p$ .  In other words, $\partial / \partial x^i|_p$ is just the derivation that takes the $i$ th partial derivative of the coordinate representation of $f$ at the coordinate representation of $p$ . In the special case of standard coordinates on $\mathbb{R}^n$ , the vectors $\partial/ \partial x^i|_p$ are literally the partial derivative operators. Now, for the general smooth manifold $M$ how do we guarantee that that mixed partial derivatives of a smooth function can be taken in any order, i.e. $\frac{\partial^2 f}{\partial x^i \partial x^j}=\frac{\partial^2 f}{\partial x^j \partial x^i}$ ? I think this follows from the fact that this is true for smooth functions on the Euclidean space, but how do we compute the second order derivative for smooth functions on manifolds as in the single derivative case above? I cannot figure out how to define $\frac{\partial}{\partial x^i}|_p \frac{\partial f}{\partial x^i}= \frac{\partial}{\partial x^j}|_p \frac{\partial \hat{f}}{\partial x^j}(\hat{p})$ using the definition above. Can we put the second order derivative of $f\in C^\infty(M)$ in terms of second order derivative of the coordinate representation of $f$ in $C^\infty(\mathbb{R}^n)$ so we can use the result in $\mathbb{R}^n$ to conclude the result? Or do we rely on a different result to conclude that mixed partial derivatives can take any order? I would greatly appreciate some help as I have been puzzled about the precise form of mixture of partial derivatives for smooth manifolds and why the order does not matter as for smooth functions on $\mathbb{R}^n$ .","The following is a description of the partial derivative on smooth manifolds from John Lee's Introduction to Smooth Manifolds. Suppose is a smooth manifold and let be a smooth coordinate chart on . Then is a diffeomorphism from to an open subset . So is an isomorphism. Let the standard coordinate frame be for . Then we use the notation as the preimages of these vectors under the isomorphism that form a basis for characterized by either of the following expressions: Unwinding the definitions, we see that acts on a function by where is the coordinate representation of , and is the coordinate representation of .  In other words, is just the derivation that takes the th partial derivative of the coordinate representation of at the coordinate representation of . In the special case of standard coordinates on , the vectors are literally the partial derivative operators. Now, for the general smooth manifold how do we guarantee that that mixed partial derivatives of a smooth function can be taken in any order, i.e. ? I think this follows from the fact that this is true for smooth functions on the Euclidean space, but how do we compute the second order derivative for smooth functions on manifolds as in the single derivative case above? I cannot figure out how to define using the definition above. Can we put the second order derivative of in terms of second order derivative of the coordinate representation of in so we can use the result in to conclude the result? Or do we rely on a different result to conclude that mixed partial derivatives can take any order? I would greatly appreciate some help as I have been puzzled about the precise form of mixture of partial derivatives for smooth manifolds and why the order does not matter as for smooth functions on .","M (U,\phi) M \phi U \hat{U}\subset \mathbb{R}^n d\phi_p : T_p M \to T_{\phi(p)} \mathbb{R}^n \partial/\partial x^1 |_{\phi(p)},\dots \partial / \partial x^n|_{\phi(p)} T_{\phi(p)} \mathbb{R}^n \partial/ \partial x^i|_p d\phi_p T_p M \frac{\partial}{\partial x^i}|_p = (d\phi_p)^{-1}(\frac{\partial}{\partial x^i}|_{\phi(p)}) = d(\phi^{-1})_{\phi(p)}(\frac{\partial}{\partial x^i}|_{\phi(p)}). \partial/ \partial x^i|_p f\in C^\infty(U) \frac{\partial}{\partial x^i}|_p f = \frac{\partial}{\partial x^i}|_{\phi(p)} (f\circ \phi^{-1}) = \frac{\partial \hat{f}}{\partial x^i}(\hat{p}), \hat{f}=f\circ \phi^{-1} f \hat{p}=(p^1, \dots, p^n)= \phi(p) p \partial / \partial x^i|_p i f p \mathbb{R}^n \partial/ \partial x^i|_p M \frac{\partial^2 f}{\partial x^i \partial x^j}=\frac{\partial^2 f}{\partial x^j \partial x^i} \frac{\partial}{\partial x^i}|_p \frac{\partial f}{\partial x^i}= \frac{\partial}{\partial x^j}|_p \frac{\partial \hat{f}}{\partial x^j}(\hat{p}) f\in C^\infty(M) f C^\infty(\mathbb{R}^n) \mathbb{R}^n \mathbb{R}^n","['multivariable-calculus', 'differential-geometry', 'manifolds', 'partial-derivative', 'smooth-manifolds']"
11,Are the elements of a vector field real functions?,Are the elements of a vector field real functions?,,"I am currently trying to understand the definition of differentiation of scalar fields. In doing so, I have been considering the question in the title. I have the following definition in my lecture notes: A vector field, $\underline{f}$ , is a map $\underline{f}:\mathbb{R}^n \to \mathbb{R}^n$ such that $\underline{x} \mapsto \underline{f}(\underline{x})$ . Since $\underline{f}$ is a function on $\mathbb{R}^n$ , I assume it must have $n$ components such that $\underline{f} = \begin{pmatrix} f_1 \\ f_2 \\ \vdots \\ f_n \end{pmatrix}$ . So does this mean that $\underline{f}(\underline{x}) = \begin{pmatrix} f_1(x_1) \\ f_2(x_2) \\ \vdots \\ f_n(x_n) \end{pmatrix}$ , when the $f_i$ are real functions $f_i: \mathbb{R} \to \mathbb{R}$ ?","I am currently trying to understand the definition of differentiation of scalar fields. In doing so, I have been considering the question in the title. I have the following definition in my lecture notes: A vector field, , is a map such that . Since is a function on , I assume it must have components such that . So does this mean that , when the are real functions ?",\underline{f} \underline{f}:\mathbb{R}^n \to \mathbb{R}^n \underline{x} \mapsto \underline{f}(\underline{x}) \underline{f} \mathbb{R}^n n \underline{f} = \begin{pmatrix} f_1 \\ f_2 \\ \vdots \\ f_n \end{pmatrix} \underline{f}(\underline{x}) = \begin{pmatrix} f_1(x_1) \\ f_2(x_2) \\ \vdots \\ f_n(x_n) \end{pmatrix} f_i f_i: \mathbb{R} \to \mathbb{R},"['multivariable-calculus', 'vector-analysis']"
12,Evaluation or simplification of $\displaystyle{\int_0^{\frac{\pi}{4}}}\dfrac{dx}{\sqrt{A-\cos x-Bx}}$,Evaluation or simplification of,\displaystyle{\int_0^{\frac{\pi}{4}}}\dfrac{dx}{\sqrt{A-\cos x-Bx}},"The period for an inverted pendulum on which some external forces act is expressed in terms of its initial angle, $\Phi_o\in[0º,70º]$ (higher angles are unstable), as $$\mathcal{T}(\Phi_o)=2\sqrt 2\sqrt{\dfrac{\ell}{g}}\int_0^{\Phi_o}\dfrac{d\Phi}{\sqrt{(\cos\Phi_o-\cos\Phi)+\zeta(\Phi_o-\Phi)}},$$ where $g$ is gravity, and $\ell$ and $\zeta$ are pendulum-related constants. In brief, we're interested in $$\int_0^{\Phi_o}\dfrac{d\Phi}{\sqrt{A-\cos\Phi-B\Phi}}.$$ Is there any series expansion for this integral or any elliptical type function describing it maybe? Using Desmos to visualize the period in terms of the initial angle, looks like there shouldn't be any problem with convergence of this integral within the initial angle's interval and zeta's interval, $\zeta\in[0,2]$ . If it were too hard to find an expression in terms of $\Phi_o$ , at least it'd want to try it for $\Phi_o=\frac{\pi}{4}$ , which is the default inital value I use the most.","The period for an inverted pendulum on which some external forces act is expressed in terms of its initial angle, (higher angles are unstable), as where is gravity, and and are pendulum-related constants. In brief, we're interested in Is there any series expansion for this integral or any elliptical type function describing it maybe? Using Desmos to visualize the period in terms of the initial angle, looks like there shouldn't be any problem with convergence of this integral within the initial angle's interval and zeta's interval, . If it were too hard to find an expression in terms of , at least it'd want to try it for , which is the default inital value I use the most.","\Phi_o\in[0º,70º] \mathcal{T}(\Phi_o)=2\sqrt 2\sqrt{\dfrac{\ell}{g}}\int_0^{\Phi_o}\dfrac{d\Phi}{\sqrt{(\cos\Phi_o-\cos\Phi)+\zeta(\Phi_o-\Phi)}}, g \ell \zeta \int_0^{\Phi_o}\dfrac{d\Phi}{\sqrt{A-\cos\Phi-B\Phi}}. \zeta\in[0,2] \Phi_o \Phi_o=\frac{\pi}{4}","['integration', 'multivariable-calculus', 'definite-integrals', 'power-series', 'elliptic-integrals']"
13,Relationship between gradient and position vector,Relationship between gradient and position vector,,"When we take a derivative of the function f(x) with respect to x we find out how much an infinitely small change in x will change f(x) . When we take the gradient of a multivariable function say f(x,y,z) we define it as $$ \nabla.f(x,y,z) = \frac{\partial f(x,y,z)}{\partial x} \hat{i} + \frac{\partial f(x,y,z)}{\partial y} \hat{j} + \frac{\partial f(x,y,z)}{\partial z} \hat{k} $$ and if the position vector is given by $ \vec{r}(x,y,z) = x\hat{i} + y\hat{j} + z\hat{k} $ Then is it safe to correlate that the gradient is the directional derivative of the function with respect to the position vector $\vec{r}$ If I am wrong can you explain in detail why ?","When we take a derivative of the function f(x) with respect to x we find out how much an infinitely small change in x will change f(x) . When we take the gradient of a multivariable function say f(x,y,z) we define it as and if the position vector is given by Then is it safe to correlate that the gradient is the directional derivative of the function with respect to the position vector If I am wrong can you explain in detail why ?"," \nabla.f(x,y,z) = \frac{\partial f(x,y,z)}{\partial x} \hat{i} + \frac{\partial f(x,y,z)}{\partial y} \hat{j} + \frac{\partial f(x,y,z)}{\partial z} \hat{k}
  \vec{r}(x,y,z) = x\hat{i} + y\hat{j} + z\hat{k}  \vec{r}","['multivariable-calculus', 'vectors', 'vector-analysis']"
14,Proving that a quadratic form has a single eigenvalue in an open interval,Proving that a quadratic form has a single eigenvalue in an open interval,,"I need help with this exercise: Show that the matrix B = \begin{pmatrix} 3 & 2 & 1 \\ 2 & 4 & 1 \\ 1 & 1 & 1 \end{pmatrix} has exactly one eigenvalue in the open interval (1,2) by studying the signatures of the quadratic forms with matrices $B-I$ and $B-2I$ . The only thing I have managed to do is compute the signatures of $B-I$ and $B-2I$ . They're (2,1) and (1,2) respectively. I don't know how the open interval comes into play.","I need help with this exercise: Show that the matrix B = has exactly one eigenvalue in the open interval (1,2) by studying the signatures of the quadratic forms with matrices and . The only thing I have managed to do is compute the signatures of and . They're (2,1) and (1,2) respectively. I don't know how the open interval comes into play.","\begin{pmatrix}
3 & 2 & 1 \\
2 & 4 & 1 \\
1 & 1 & 1
\end{pmatrix} B-I B-2I B-I B-2I","['linear-algebra', 'multivariable-calculus', 'quadratic-forms']"
15,Flux through a tetrahedron,Flux through a tetrahedron,,"Im solving a problem and have stumbled can not wrap my head around the solution I get. The problem is formulated: Calculate the flux of $\vec{F}=(x,z,0)$ out from the tetrahedron $x+2y+3z=6$ in the first octant. My solution is the following: Parametrization: $r(x,y)=(x,y,\frac{6-2y-x}{3})$ with $0\leq y\leq 3-\frac{x}{2}, 0\leq x\leq 6$ , which implies $\vec{N}=(\frac{1}{3},\frac{2}{3},1)$ . Then i just evaluate the double integral: $$\int^{6}_{0}\int^{3-x/2}_{0}\vec{F}\cdot \vec{N}dydx=\int^{6}_{0}\int^{3-x/2}_{0}(x,\frac{6-2y-x}{3},0)\cdot(\frac{1}{3},\frac{2}{3},1)dydx=10.$$ I've tried recalculating several times over and can not get the correct answer of 6. Which step am i doing wrong here?","Im solving a problem and have stumbled can not wrap my head around the solution I get. The problem is formulated: Calculate the flux of out from the tetrahedron in the first octant. My solution is the following: Parametrization: with , which implies . Then i just evaluate the double integral: I've tried recalculating several times over and can not get the correct answer of 6. Which step am i doing wrong here?","\vec{F}=(x,z,0) x+2y+3z=6 r(x,y)=(x,y,\frac{6-2y-x}{3}) 0\leq y\leq 3-\frac{x}{2}, 0\leq x\leq 6 \vec{N}=(\frac{1}{3},\frac{2}{3},1) \int^{6}_{0}\int^{3-x/2}_{0}\vec{F}\cdot \vec{N}dydx=\int^{6}_{0}\int^{3-x/2}_{0}(x,\frac{6-2y-x}{3},0)\cdot(\frac{1}{3},\frac{2}{3},1)dydx=10.","['calculus', 'integration', 'multivariable-calculus']"
16,Wedge product defined on not alternating tensors?,Wedge product defined on not alternating tensors?,,"I am currently reading Calculus on Manifolds by Spivak. In there, it defined wedge product as follows To determine the dimensions of $\Lambda^k(V)$ , we would like a theorem analogous to Theorem 4-1. Of course, if $\omega \in \Lambda^k(V)$ and $\eta \in \Lambda^l(V)$ , then $\omega \otimes \eta$ is usually not in $\Lambda^{k+l}(V)$ . We will therefore define a new product, the wedge product $\omega \wedge \eta \in \Lambda^{k+l}(V)$ by $$ \omega \wedge \eta=\frac{(k+l) !}{k ! l !} \operatorname{Alt}(\omega \otimes \eta) $$ I am not very sure if here Spivak assumed $\omega \in \Lambda^k(V)$ and $\eta \in \Lambda^l(V)$ , i.e. $\omega$ and $\eta$ are the alternating tensors. In the rest of the text, should I always assume whenever Spivak used the $\wedge$ symbol, he means that both operands are alternating tensors? Or in other words, is there any reason to use this definition on tensors that are not alternating? Later on Spivak defined differential forms with alternating tensors, and I don't think I have experienced the case where $\omega$ and $\eta$ are not alternating. I'm aware of other definitions of wedge products as the product in exterior algebra. However, I don't know anything about exterior algebra, and it seems to be very time consuming for me to learn about it just for this simple question.","I am currently reading Calculus on Manifolds by Spivak. In there, it defined wedge product as follows To determine the dimensions of , we would like a theorem analogous to Theorem 4-1. Of course, if and , then is usually not in . We will therefore define a new product, the wedge product by I am not very sure if here Spivak assumed and , i.e. and are the alternating tensors. In the rest of the text, should I always assume whenever Spivak used the symbol, he means that both operands are alternating tensors? Or in other words, is there any reason to use this definition on tensors that are not alternating? Later on Spivak defined differential forms with alternating tensors, and I don't think I have experienced the case where and are not alternating. I'm aware of other definitions of wedge products as the product in exterior algebra. However, I don't know anything about exterior algebra, and it seems to be very time consuming for me to learn about it just for this simple question.","\Lambda^k(V) \omega \in \Lambda^k(V) \eta \in \Lambda^l(V) \omega \otimes \eta \Lambda^{k+l}(V) \omega \wedge \eta \in \Lambda^{k+l}(V) 
\omega \wedge \eta=\frac{(k+l) !}{k ! l !} \operatorname{Alt}(\omega \otimes \eta)
 \omega \in \Lambda^k(V) \eta \in \Lambda^l(V) \omega \eta \wedge \omega \eta","['multivariable-calculus', 'differential-geometry', 'differential-forms', 'exterior-algebra']"
17,Function where order of integration cannot be reversed,Function where order of integration cannot be reversed,,"Our professor gave us the following problem to give us an example where we cannot switch the order of integration. However, I am stuck on the last part, so maybe someone can give me an answer. Let $i = 1,2,\dots$ and $\phi_i: \mathbb{R} \rightarrow \mathbb{R}$ be continuous functions with $\mathrm{supp}(\phi_i) \subset (2^{-i}, 2^{1-i})$ and $\int_{\mathbb{R}}\phi_i = 1$ . Then we define $$f(x,y) := \sum_{i = 1}^{\infty}(\phi_i(x) - \phi_{i+1}(x))\phi_i(y).$$ I already managed to prove that $f$ is continuous on $\mathbb{R}^2 - \{(0,0)\}$ . The main problem is the following, however; we are meant to show that $$ \int_{\mathbb{R}}{\left(\int_{\mathbb{R}}f(x,y)\mathrm{d}x\right)\mathrm{d}y} = 0,$$ but $$ \int_{\mathbb{R}}{\left(\int_{\mathbb{R}}f(x,y)\mathrm{d}y\right)\mathrm{d}x} = 1.$$ The first result is what I also obtain, but I seem to be doing something wrong, as I obtain 0 for the second integral as well.","Our professor gave us the following problem to give us an example where we cannot switch the order of integration. However, I am stuck on the last part, so maybe someone can give me an answer. Let and be continuous functions with and . Then we define I already managed to prove that is continuous on . The main problem is the following, however; we are meant to show that but The first result is what I also obtain, but I seem to be doing something wrong, as I obtain 0 for the second integral as well.","i = 1,2,\dots \phi_i: \mathbb{R} \rightarrow \mathbb{R} \mathrm{supp}(\phi_i) \subset (2^{-i}, 2^{1-i}) \int_{\mathbb{R}}\phi_i = 1 f(x,y) := \sum_{i = 1}^{\infty}(\phi_i(x) - \phi_{i+1}(x))\phi_i(y). f \mathbb{R}^2 - \{(0,0)\}  \int_{\mathbb{R}}{\left(\int_{\mathbb{R}}f(x,y)\mathrm{d}x\right)\mathrm{d}y} = 0,  \int_{\mathbb{R}}{\left(\int_{\mathbb{R}}f(x,y)\mathrm{d}y\right)\mathrm{d}x} = 1.","['real-analysis', 'integration', 'multivariable-calculus']"
18,If $F$ is closed then $A = \{a\in\mathbb{R}^{n}: \text{there exists}\quad x \in F\quad\text{with}\quad ||x - a|| = \delta\}$ is closed.,If  is closed then  is closed.,F A = \{a\in\mathbb{R}^{n}: \text{there exists}\quad x \in F\quad\text{with}\quad ||x - a|| = \delta\},"I'm trying to prove the next: Let $F\subset\mathbb{R}^{n}$ be a closed set and $\delta>0.$ Show that A is closed in $\mathbb{R}^{n}$ if $$A = \{a\in\mathbb{R}^{n}: \text{there exists}\quad x \in F\quad\text{with}\quad ||x - a|| = \delta\}.$$ I was attempting to prove that each convergent sequence on $A$ must have its limit in $A$ : If $(a_{m})_{m\in\mathbb{N}}\subset A$ such that $a_{m}\rightarrow a,$ then for each $m$ we have there exists $x_{m}\in F$ with property $||a_{m} - x_{m}|| = \delta.$ So I was trying to prove that the sequence $(x_{m})_{m\in\mathbb{N}}$ must be convergent in $F$ to some $x$ to utilize closedness of $F$ and have the desired property but I do not know how to do this. Other attempt was writting A as a union of circumferences of closed balls with center in each point of $F$ and trying to prove the limit of a sequence must be in such union but seems not a good way... Any kind of help is thanked in advanced.",I'm trying to prove the next: Let be a closed set and Show that A is closed in if I was attempting to prove that each convergent sequence on must have its limit in : If such that then for each we have there exists with property So I was trying to prove that the sequence must be convergent in to some to utilize closedness of and have the desired property but I do not know how to do this. Other attempt was writting A as a union of circumferences of closed balls with center in each point of and trying to prove the limit of a sequence must be in such union but seems not a good way... Any kind of help is thanked in advanced.,"F\subset\mathbb{R}^{n} \delta>0. \mathbb{R}^{n} A = \{a\in\mathbb{R}^{n}: \text{there exists}\quad x \in F\quad\text{with}\quad ||x - a|| = \delta\}. A A (a_{m})_{m\in\mathbb{N}}\subset A a_{m}\rightarrow a, m x_{m}\in F ||a_{m} - x_{m}|| = \delta. (x_{m})_{m\in\mathbb{N}} F x F F","['real-analysis', 'multivariable-calculus']"
19,Integration by Parts: Product of Many Functions [duplicate],Integration by Parts: Product of Many Functions [duplicate],,"This question already has answers here : Generalising integration by parts for the product of more than two functions (2 answers) Closed 12 months ago . ""Classic"" integration by parts has two functions ( $u$ and $dv$ ; $\int udv = uv - \int vdu$ ). What if there is a product of $n$ functions? In other words, what's the solution to the following? \begin{equation}\label{eq:tosolve}     F(x)=\int_{a}^{b}\prod_{i=1}^{n}u_{i}(x)\:dx \end{equation} Wikipedia's integration by parts article mentions this problem and offers an equation, but I think a few extra steps are necessary for a full solution. Wikipedia leaves us with the following product rule for $n$ functions: \begin{equation} \bigg(\prod_{i=1}^{n}u_{i}(x)\bigg)'=\sum_{j=1}^{n}u_{j}'(x)\prod_{i\neq j}^{n}u_{i}(x). \end{equation} Integrating, this leads to \begin{equation}\label{eq:onwiki}     \bigg[\prod_{i=1}^{n}u_{i}(x)\bigg]_{a}^{b}=\sum_{j=1}^{n}\int_{a}^{b}u_{j}'(x)\prod_{i\neq j}^{n}u_{i}(x)\:dx \end{equation} The object we want to solve for (the RHS of the first equation at the top) does not appear in the expression immediately above. How does one recover it?","This question already has answers here : Generalising integration by parts for the product of more than two functions (2 answers) Closed 12 months ago . ""Classic"" integration by parts has two functions ( and ; ). What if there is a product of functions? In other words, what's the solution to the following? Wikipedia's integration by parts article mentions this problem and offers an equation, but I think a few extra steps are necessary for a full solution. Wikipedia leaves us with the following product rule for functions: Integrating, this leads to The object we want to solve for (the RHS of the first equation at the top) does not appear in the expression immediately above. How does one recover it?","u dv \int udv = uv - \int vdu n \begin{equation}\label{eq:tosolve}
    F(x)=\int_{a}^{b}\prod_{i=1}^{n}u_{i}(x)\:dx
\end{equation} n \begin{equation}
\bigg(\prod_{i=1}^{n}u_{i}(x)\bigg)'=\sum_{j=1}^{n}u_{j}'(x)\prod_{i\neq j}^{n}u_{i}(x).
\end{equation} \begin{equation}\label{eq:onwiki}
    \bigg[\prod_{i=1}^{n}u_{i}(x)\bigg]_{a}^{b}=\sum_{j=1}^{n}\int_{a}^{b}u_{j}'(x)\prod_{i\neq j}^{n}u_{i}(x)\:dx
\end{equation}","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
20,Line integral equals zero because the vector field and the curve are perpendicular.,Line integral equals zero because the vector field and the curve are perpendicular.,,"Let $F: \Omega \subseteq \mathbb R^2 \to \mathbb R^2$ defined as $F(x,y) = (-y,x)$ and $\gamma:[0,2] \to \mathbb R^2$ defined as $\gamma(t) = (R(1-t), 0)$ for some $R>0$ . We have $$  \int_{\gamma}\vec F \cdot \vec {d\ell}= \int_{0}^{2}\langle \begin{bmatrix} 0 \\ R(1-t) \end{bmatrix}  , \begin{bmatrix} -R \\ 0 \end{bmatrix}  \rangle dt = 0 $$ Supposedly this is because in this case $F$ and $\gamma$ are perpindicular but I'm having trouble seeing why. If I try to visualize the image of $F$ I think it sort of looks like a spiral rotating anti-clockwise (?), and the image of $\gamma$ simply as the segment $[-R, R]$ . How are they perpindicular?","Let defined as and defined as for some . We have Supposedly this is because in this case and are perpindicular but I'm having trouble seeing why. If I try to visualize the image of I think it sort of looks like a spiral rotating anti-clockwise (?), and the image of simply as the segment . How are they perpindicular?","F: \Omega \subseteq \mathbb R^2 \to \mathbb R^2 F(x,y) = (-y,x) \gamma:[0,2] \to \mathbb R^2 \gamma(t) = (R(1-t), 0) R>0  
\int_{\gamma}\vec F \cdot \vec {d\ell}= \int_{0}^{2}\langle \begin{bmatrix}
0 \\
R(1-t)
\end{bmatrix} 
,
\begin{bmatrix}
-R \\
0
\end{bmatrix} 
\rangle
dt = 0
 F \gamma F \gamma [-R, R]","['real-analysis', 'integration', 'multivariable-calculus']"
21,How to simplify $\int_{-t}^t \int_{-t}^t \int_{-t}^t (xyz) dx dy dz$?,How to simplify ?,\int_{-t}^t \int_{-t}^t \int_{-t}^t (xyz) dx dy dz,"During the integration of a highly symmetric integrand $$ \int_{-t}^t \int_{-t}^t \int_{-t}^t (xyz) dx dy dz$$ What manipulations can be done to reduce the above triple integral to just a single integral of for instance some function $f(w)$ of some variable $w$ such that $$\int_{-t}^t \int_{-t}^t \int_{-t}^t (xyz) dx dy dz = \int_{-t}^t f(w) dw $$ ? Attempt: $ \int_{-t}^t x^3 (dx)^3$ misses many terms when $x,y,z$ have different values.",During the integration of a highly symmetric integrand What manipulations can be done to reduce the above triple integral to just a single integral of for instance some function of some variable such that ? Attempt: misses many terms when have different values.," \int_{-t}^t \int_{-t}^t \int_{-t}^t (xyz) dx dy dz f(w) w \int_{-t}^t \int_{-t}^t \int_{-t}^t (xyz) dx dy dz = \int_{-t}^t f(w) dw   \int_{-t}^t x^3 (dx)^3 x,y,z","['integration', 'multivariable-calculus']"
22,Prove or disprove that a piecewise complex function has a derivative at $0$,Prove or disprove that a piecewise complex function has a derivative at,0,"Prove or disprove that the function: $$ f(x+iy)=\left\{\begin{matrix} \sqrt{xy}\: \: \: \: \text{ if } \space xy\geq 0 \\ i\sqrt{-xy}\: \: \: \: \text{ if } \space xy<  0 \end{matrix}\right. $$ has a derivative at $0.$ I'm pretty sure that the function doesn't have derivative at $0.$ I tried showing that by using Cauchy–Riemann equations: for $\\xy \geq 0:$ $$\\u_{x}(x,y)=\frac{\sqrt{y}}{2\sqrt{x}} \\u_{y}(x,y)=\frac{\sqrt{x}}{2\sqrt{y}} \\v_{x}(x,y)=0 \\v_{y}(x,y)=0$$ for $\\xy \geq 0:$ $$ \\u_{x}(x,y)=0 \\u_{y}(x,y)=0 \\v_{x}(x,y)=-\frac{\sqrt{y}}{2\sqrt{-xy}} \\v_{y}(x,y)=-\frac{\sqrt{x}}{2\sqrt{-xy}} $$ so: $$ \\0=\frac{\sqrt{y}}{2\sqrt{x}} \\0=\frac{\sqrt{x}}{2\sqrt{y}} $$ and: $$ \\0=-\frac{\sqrt{y}}{2\sqrt{-xy}} \\0=-\frac{\sqrt{x}}{2\sqrt{-xy}} $$ so for $(x,y)=(0,0)$ the equations are true. Furthermore, I tried applying the limit definition of the derivative $\lim_{(x,y) \to (0,0)}\frac{f(x+iy)-f(0)}{x+iy-0}$ but I encountered difficulties dealing with the piecewise function. How should I approach to solving this kind of piecewise function problem over the complex plane?","Prove or disprove that the function: has a derivative at I'm pretty sure that the function doesn't have derivative at I tried showing that by using Cauchy–Riemann equations: for for so: and: so for the equations are true. Furthermore, I tried applying the limit definition of the derivative but I encountered difficulties dealing with the piecewise function. How should I approach to solving this kind of piecewise function problem over the complex plane?"," f(x+iy)=\left\{\begin{matrix} \sqrt{xy}\: \: \: \: \text{ if } \space xy\geq 0
\\ i\sqrt{-xy}\: \: \: \: \text{ if } \space xy<  0
\end{matrix}\right.  0. 0. \\xy \geq 0: \\u_{x}(x,y)=\frac{\sqrt{y}}{2\sqrt{x}}
\\u_{y}(x,y)=\frac{\sqrt{x}}{2\sqrt{y}}
\\v_{x}(x,y)=0
\\v_{y}(x,y)=0 \\xy \geq 0: 
\\u_{x}(x,y)=0
\\u_{y}(x,y)=0
\\v_{x}(x,y)=-\frac{\sqrt{y}}{2\sqrt{-xy}}
\\v_{y}(x,y)=-\frac{\sqrt{x}}{2\sqrt{-xy}}
 
\\0=\frac{\sqrt{y}}{2\sqrt{x}}
\\0=\frac{\sqrt{x}}{2\sqrt{y}}
 
\\0=-\frac{\sqrt{y}}{2\sqrt{-xy}}
\\0=-\frac{\sqrt{x}}{2\sqrt{-xy}}
 (x,y)=(0,0) \lim_{(x,y) \to (0,0)}\frac{f(x+iy)-f(0)}{x+iy-0}","['calculus', 'multivariable-calculus']"
23,What is the surface integral of hemisphere?,What is the surface integral of hemisphere?,,"Calculate the integral $∫ ∫_{Y}z dS$ if Y is the part of the conic surface $z = \sqrt{x^2 + y^2}$ where z is between 0 and 1. So this is what I did: $z = \sqrt{4-x^2-y^2}$ and then I used spehere's coordinates and got z to $z=2\sqrt{1-\sin^2(\theta)}$ and my area elemnent or whatever it is called is given by $r^2\sin(\theta)$ and then I multiplied this by z and parted the integrals. I for some reasons think that I don't need to take r as the variable(I saw a similar quation where someone just took r as a constant, don't understand why and if I can do that here). This means my variables are theta and phi and the bounds are $0\leq \theta \leq \frac{\pi}2$ and $0\leq \phi \leq 2\pi$ . First of all I don't know if I have done right so far so correct me if I'm wrong but otherwise how in the hell am I supposed integrate $\int 2\sqrt{1-\sin^2(\theta)} = 2 \int \sqrt{\frac12 + \cos(2\theta)}$ EDIT: I accidentally posted the wrong question, this was the question my ""solution"" was for: Evaluate the surface integral $\int\int_{Y}zdS$ when Y is the upper half of the sphere $x^2+y^2+z^2=4$ , i.e. the part where z ≥ 0.","Calculate the integral if Y is the part of the conic surface where z is between 0 and 1. So this is what I did: and then I used spehere's coordinates and got z to and my area elemnent or whatever it is called is given by and then I multiplied this by z and parted the integrals. I for some reasons think that I don't need to take r as the variable(I saw a similar quation where someone just took r as a constant, don't understand why and if I can do that here). This means my variables are theta and phi and the bounds are and . First of all I don't know if I have done right so far so correct me if I'm wrong but otherwise how in the hell am I supposed integrate EDIT: I accidentally posted the wrong question, this was the question my ""solution"" was for: Evaluate the surface integral when Y is the upper half of the sphere , i.e. the part where z ≥ 0.",∫ ∫_{Y}z dS z = \sqrt{x^2 + y^2} z = \sqrt{4-x^2-y^2} z=2\sqrt{1-\sin^2(\theta)} r^2\sin(\theta) 0\leq \theta \leq \frac{\pi}2 0\leq \phi \leq 2\pi \int 2\sqrt{1-\sin^2(\theta)} = 2 \int \sqrt{\frac12 + \cos(2\theta)} \int\int_{Y}zdS x^2+y^2+z^2=4,"['calculus', 'multivariable-calculus', 'surface-integrals']"
24,Non continuous function,Non continuous function,,"I'm struggling to answer this question : Show that this function admits partial derivatives in every direction without being continuous in $(0,0)$ : $f(x,y)=\begin{cases}  y^2\log \left|x\right|& \text{ if } x \neq 0  \\  0 &  \text{ if } x=0 \end{cases}$ I have no problem showing that f admits partial derivatives in every directions, but I'm struggling to show that it is not continuous.","I'm struggling to answer this question : Show that this function admits partial derivatives in every direction without being continuous in : I have no problem showing that f admits partial derivatives in every directions, but I'm struggling to show that it is not continuous.","(0,0) f(x,y)=\begin{cases}
 y^2\log \left|x\right|& \text{ if } x \neq 0 
\\
 0 &  \text{ if } x=0
\end{cases}","['multivariable-calculus', 'continuity', 'partial-derivative']"
25,How to compute the following definite integral?,How to compute the following definite integral?,,"$$\int_{-\infty}^{+\infty} e^{-(ap - ibx)^2} dp$$ where $a$ , $p$ , $b$ , $x$ are real numbers and $a$ is greater than zero. I think that I should declare a new variable $m = ap - ibx$ , then $dm = adp$ and the integral will just become: $$\frac{1}{a}\int_{}^{} e^{-m^2} dm$$ However, it is not quite a Gaussian since $m$ is complex and the borders of integration will be something like $(-\infty - ibx)$ and $(+\infty - ibx)$ . Therefore I do not know how to prove that the given integral should be equal to $\sqrt{\pi}/a$ .","where , , , are real numbers and is greater than zero. I think that I should declare a new variable , then and the integral will just become: However, it is not quite a Gaussian since is complex and the borders of integration will be something like and . Therefore I do not know how to prove that the given integral should be equal to .",\int_{-\infty}^{+\infty} e^{-(ap - ibx)^2} dp a p b x a m = ap - ibx dm = adp \frac{1}{a}\int_{}^{} e^{-m^2} dm m (-\infty - ibx) (+\infty - ibx) \sqrt{\pi}/a,"['calculus', 'integration', 'complex-analysis', 'multivariable-calculus']"
26,Is the Gauss Divergence theorem applicable when the Divergence of a vector field is a dirac delta function?,Is the Gauss Divergence theorem applicable when the Divergence of a vector field is a dirac delta function?,,"(This question is motivated by electrostatics) In electrostatics one comes across discontinuous vector fields all the time, for example, the vector field $ \vec E=\frac{\vec r}{r^3} $ which has a missing point discontinuity at $\vec r=0$ , whose divergence $\nabla\cdot \vec E$ is the dirac delta distribution (to be precise, $\vec \nabla \cdot \vec E=4\pi\delta(\vec r)$ , using test functions and so on). It is not clear to me how one can assert that the volume integral of this dirac delta distribution, $\vec \nabla \cdot \vec E$ , is equal to the surface integral of the field, which is discontinuous. Yet, physics textbooks still apply the Gauss Divergence theorem, to say that $\iiint_{V}\nabla\cdot\vec E d\tau=\iint_{S}\vec E\cdot\hat ndS$ , where V contains the singularity $\vec r =0$ , but don't explain why one is allowed to do this, when the vector field is clearly discontinuous in the domain in which the integral is being done. Alas, the integral $\iint_{S}\vec E \cdot \hat n dS= \iint_S\frac{\hat r \cdot \hat n dS}{r^2}$ comes out to be $4\pi$ , but my question still holds. Now, I am aware that the result for this case comes out to be exactly the same, $4\pi$ , through different calculations, but is this a coincicende? More precicely, given a vector field $\vec E$ which is defined , continuous and differentiable everywhere except at $\vec r=0$ , whose divergence $\vec \nabla \cdot \vec E$ comes out to be the dirac delta distribution, can we apply gauss divergence theorem in such a case (case where we include the origin)?","(This question is motivated by electrostatics) In electrostatics one comes across discontinuous vector fields all the time, for example, the vector field which has a missing point discontinuity at , whose divergence is the dirac delta distribution (to be precise, , using test functions and so on). It is not clear to me how one can assert that the volume integral of this dirac delta distribution, , is equal to the surface integral of the field, which is discontinuous. Yet, physics textbooks still apply the Gauss Divergence theorem, to say that , where V contains the singularity , but don't explain why one is allowed to do this, when the vector field is clearly discontinuous in the domain in which the integral is being done. Alas, the integral comes out to be , but my question still holds. Now, I am aware that the result for this case comes out to be exactly the same, , through different calculations, but is this a coincicende? More precicely, given a vector field which is defined , continuous and differentiable everywhere except at , whose divergence comes out to be the dirac delta distribution, can we apply gauss divergence theorem in such a case (case where we include the origin)?", \vec E=\frac{\vec r}{r^3}  \vec r=0 \nabla\cdot \vec E \vec \nabla \cdot \vec E=4\pi\delta(\vec r) \vec \nabla \cdot \vec E \iiint_{V}\nabla\cdot\vec E d\tau=\iint_{S}\vec E\cdot\hat ndS \vec r =0 \iint_{S}\vec E \cdot \hat n dS= \iint_S\frac{\hat r \cdot \hat n dS}{r^2} 4\pi 4\pi \vec E \vec r=0 \vec \nabla \cdot \vec E,"['multivariable-calculus', 'physics', 'dirac-delta', 'divergence-theorem']"
27,Leibniz integral rule for differentiation under the integral sign - is this condition needed?,Leibniz integral rule for differentiation under the integral sign - is this condition needed?,,See the general rule (the theorem) here: Leibniz rule - general form My question is: do we need to require the derivatives $a'(x)$ and $b'(x)$ to be continuous? The Wikipedia page requires that condition. But I have a calculus book which requires only that these derivatives exist i.e. it requires only that the functions $a(x)$ and $b(x)$ are differentiable. Which one is correct?,See the general rule (the theorem) here: Leibniz rule - general form My question is: do we need to require the derivatives and to be continuous? The Wikipedia page requires that condition. But I have a calculus book which requires only that these derivatives exist i.e. it requires only that the functions and are differentiable. Which one is correct?,a'(x) b'(x) a(x) b(x),"['real-analysis', 'calculus', 'integration', 'multivariable-calculus']"
28,How to make sense of partial derivatives w.r.t to polar coordinates?,How to make sense of partial derivatives w.r.t to polar coordinates?,,"Ok, so I'm trying to learn multivariable calculus. As I come from a math background, I want to understand it as rigurously as possible. I've seen that differential geometry does the job, but I don't want to go that far yet. So my question is how to make sense of partial derivatives w.r.t to other coordinate systems besides the linear ones? For example, polar coordinates do not behave nicely under linear transformations. Even worse, the vector basis is local, like in physics, so it seems a nightmare to keep track of how to write things like gradients in a vector basis that is always changing with the point. I want a mathematical explanation to keep me sane, physical analogies will not do.","Ok, so I'm trying to learn multivariable calculus. As I come from a math background, I want to understand it as rigurously as possible. I've seen that differential geometry does the job, but I don't want to go that far yet. So my question is how to make sense of partial derivatives w.r.t to other coordinate systems besides the linear ones? For example, polar coordinates do not behave nicely under linear transformations. Even worse, the vector basis is local, like in physics, so it seems a nightmare to keep track of how to write things like gradients in a vector basis that is always changing with the point. I want a mathematical explanation to keep me sane, physical analogies will not do.",,"['multivariable-calculus', 'partial-derivative', 'polar-coordinates', 'differential']"
29,"Given a function $F$, how to evaluate $\max_{t\in (0,1)}\left\vert \frac{\partial^2 F}{\partial x_i \partial x_j}(ty)\right\vert$?","Given a function , how to evaluate ?","F \max_{t\in (0,1)}\left\vert \frac{\partial^2 F}{\partial x_i \partial x_j}(ty)\right\vert","As an ""extra problem"" for my calculus 2 test class we got the following. Since it is an extra problem, I expected it to be more difficult than the other exercises, but actually but not that much. This is the problem: Let $F:\mathbb{R}^3\setminus\{0\}\to\mathbb{R}$ such that $$F:(x_1, x_2, x_3)\mapsto -\frac{1}{|x|^2} +\frac{1}{|x|}\in\mathbb{R}.$$ For a fixed $y\in\mathbb{R}^3\setminus\{0\}$ , find $$\max_{t\in (0,1)}\left\vert \frac{\partial^2 F}{\partial x_i \partial x_j}(ty)\right\vert.$$ To start I computed $\nabla F(x)$ which is $$\nabla F(x) = \left(\frac{2}{|x|^4}-\frac{1}{|x|^3}\right) x,$$ where $x$ denotes the vector $x=(x_1, x_2, x_3)$ . I went ahead by computing the second partial derivatives. I had $$\frac{\partial^2 F}{\partial x_i^2} = \frac{2}{|x|^4}-\frac{1}{|x|^3} +\frac{3x_i^2}{|x|^5}-\frac{8x_i^2}{|x|^6}, \quad i\in\{1, 2, 3\}$$ and $$\frac{\partial^2 F}{\partial x_i\partial x_j} = \frac{3x_i x_j}{|x|^5}-\frac{8x_i x_j}{|x|^6}, \quad i\in\{1, 2, 3\}, i\neq j.$$ I got stuck at this point since I have no idea how to establish the maximum. I’ve been thinking about it unsuccessfully for a couple of days, but I am really interested in the solution. Could someone please help in finding that maximum? Any hint will be very helpful. Thank you in advance.","As an ""extra problem"" for my calculus 2 test class we got the following. Since it is an extra problem, I expected it to be more difficult than the other exercises, but actually but not that much. This is the problem: Let such that For a fixed , find To start I computed which is where denotes the vector . I went ahead by computing the second partial derivatives. I had and I got stuck at this point since I have no idea how to establish the maximum. I’ve been thinking about it unsuccessfully for a couple of days, but I am really interested in the solution. Could someone please help in finding that maximum? Any hint will be very helpful. Thank you in advance.","F:\mathbb{R}^3\setminus\{0\}\to\mathbb{R} F:(x_1, x_2, x_3)\mapsto -\frac{1}{|x|^2} +\frac{1}{|x|}\in\mathbb{R}. y\in\mathbb{R}^3\setminus\{0\} \max_{t\in (0,1)}\left\vert \frac{\partial^2 F}{\partial x_i \partial x_j}(ty)\right\vert. \nabla F(x) \nabla F(x) = \left(\frac{2}{|x|^4}-\frac{1}{|x|^3}\right) x, x x=(x_1, x_2, x_3) \frac{\partial^2 F}{\partial x_i^2} = \frac{2}{|x|^4}-\frac{1}{|x|^3} +\frac{3x_i^2}{|x|^5}-\frac{8x_i^2}{|x|^6}, \quad i\in\{1, 2, 3\} \frac{\partial^2 F}{\partial x_i\partial x_j} = \frac{3x_i x_j}{|x|^5}-\frac{8x_i x_j}{|x|^6}, \quad i\in\{1, 2, 3\}, i\neq j.","['real-analysis', 'calculus', 'functional-analysis', 'multivariable-calculus', 'derivatives']"
30,"Does additivity of $ f_1(x)g_1(y) + f_2(x)g_2(y)$ tells us something about $f_1, f_2$?",Does additivity of  tells us something about ?," f_1(x)g_1(y) + f_2(x)g_2(y) f_1, f_2","Let $f_1, f_2, g_1, g_2$ be non-constant continuous real functions, such that $ f_1(x)g_1(y) + f_2(x)g_2(y) $ is additive in $x,y$ , that is, there exist functions $f,g$ such that $$ f_1(x)g_1(y) + f_2(x)g_2(y) = f(x) + g(y), \forall x,y\in\mathbb{R}. $$ What does it imply for $f_1, \dots, g_2$ ? Is it true that $f_1(x) = af_2(x)+b$ for some $a,b\in\mathbb{R}$ ? What about the case when we have $n$ pairs of functions, i.e. when $ f_1(x)g_1(y) + \dots + f_n(x)g_n(y) $ is additive?","Let be non-constant continuous real functions, such that is additive in , that is, there exist functions such that What does it imply for ? Is it true that for some ? What about the case when we have pairs of functions, i.e. when is additive?","f_1, f_2, g_1, g_2 
f_1(x)g_1(y) + f_2(x)g_2(y)
 x,y f,g 
f_1(x)g_1(y) + f_2(x)g_2(y) = f(x) + g(y), \forall x,y\in\mathbb{R}.
 f_1, \dots, g_2 f_1(x) = af_2(x)+b a,b\in\mathbb{R} n 
f_1(x)g_1(y) + \dots + f_n(x)g_n(y)
","['functional-analysis', 'multivariable-calculus', 'functions']"
31,"Coordinate system $(\phi, U, x_1, x_2)$ around $p \in S^1$ such that $S^1 \cap U = \{(x_1, x_2): x_2 = 0\}$.",Coordinate system  around  such that .,"(\phi, U, x_1, x_2) p \in S^1 S^1 \cap U = \{(x_1, x_2): x_2 = 0\}","Consider $S^1$ as a subspace $\Phi^{-1}(1) \subseteq \mathbb{R}^2$ , where $\Phi(x_1, x_2) = x_1^2 + x_2^2$ . Using the Implicit Function Theorem, show that for all points $p \in S^1$ , there is a coordinate system $(\phi, U, x_1, x_2)$ around $p$ such that $S^1 \cap U = \{(x_1, x_2): x_2 = 0\}$ . I think that I have to show the existence of a chart $(\phi, U)$ around $p\in U \subseteq \mathbb{R}^2$ such that $\pi_2\circ \phi(u, v) = x_2(u, v) = 0$ precisely when $(u, v) \in S^1 \cap U$ , i.e. $\Phi(u, v) = 1$ . However, I am not sure how to involve the Implicit Function Theorem here.","Consider as a subspace , where . Using the Implicit Function Theorem, show that for all points , there is a coordinate system around such that . I think that I have to show the existence of a chart around such that precisely when , i.e. . However, I am not sure how to involve the Implicit Function Theorem here.","S^1 \Phi^{-1}(1) \subseteq \mathbb{R}^2 \Phi(x_1, x_2) = x_1^2 + x_2^2 p \in S^1 (\phi, U, x_1, x_2) p S^1 \cap U = \{(x_1, x_2): x_2 = 0\} (\phi, U) p\in U \subseteq \mathbb{R}^2 \pi_2\circ \phi(u, v) = x_2(u, v) = 0 (u, v) \in S^1 \cap U \Phi(u, v) = 1","['multivariable-calculus', 'differential-geometry', 'manifolds', 'smooth-manifolds', 'implicit-function-theorem']"
32,"Nature of critical points of $f(x,y,z)$",Nature of critical points of,"f(x,y,z)","How can I study the nature of the critical points of $f(x,y,z)=3xy^2+6y^2+x^2z+3z^2$ ? Since $$ \begin{cases} f'_x=3y^2+2xz\\ f'_y=6xy+12y\\ f'_z=x^2+6z \end{cases} $$ the only critical point is $(0,0,0)$ . The Hessian matrix is $$ H_f(0,0,0)=\left( \begin{matrix} 0 & 0 & 0\\ 0 & 12 & 0\\ 0 & 0 & 6 \end{matrix}\right), $$ whose determinant is zero. Now I'm stuck, because I've tried to study $f$ along many restriction, and every time I realized that $O$ is a minimum, but I am not able to prove (or disprove) that $O$ is indeed a minimum. What can I do now?","How can I study the nature of the critical points of ? Since the only critical point is . The Hessian matrix is whose determinant is zero. Now I'm stuck, because I've tried to study along many restriction, and every time I realized that is a minimum, but I am not able to prove (or disprove) that is indeed a minimum. What can I do now?","f(x,y,z)=3xy^2+6y^2+x^2z+3z^2 
\begin{cases}
f'_x=3y^2+2xz\\
f'_y=6xy+12y\\
f'_z=x^2+6z
\end{cases}
 (0,0,0) 
H_f(0,0,0)=\left(
\begin{matrix}
0 & 0 & 0\\
0 & 12 & 0\\
0 & 0 & 6
\end{matrix}\right),
 f O O","['multivariable-calculus', 'optimization']"
33,Why is a multivariable function satisfying the following conditions exact?,Why is a multivariable function satisfying the following conditions exact?,,"I read this section in the book ""Mathematics Methods for Physics and Engineering"" Determining whether a differential containing many variable $x_1,x_2,...,x_n$ exact is a simple extension of the above. A differential containing many variables can be written in general as $$df=\Sigma^n_{i=1}g_i(x_1,x_2,...,x_n)dx_i$$ and the function will be exact if $\frac{\partial g_i}{\partial x_j}=\frac{\partial g_j}{\partial x_i}$ for all pairs of i and n. I can understand the situation when there are only two variables in the function, but when there are many variables, I am a bit confused about why this condition is already necessary  to prove that the function is exact, instead of requiring all partial derivatives of the function to be equal.","I read this section in the book ""Mathematics Methods for Physics and Engineering"" Determining whether a differential containing many variable exact is a simple extension of the above. A differential containing many variables can be written in general as and the function will be exact if for all pairs of i and n. I can understand the situation when there are only two variables in the function, but when there are many variables, I am a bit confused about why this condition is already necessary  to prove that the function is exact, instead of requiring all partial derivatives of the function to be equal.","x_1,x_2,...,x_n df=\Sigma^n_{i=1}g_i(x_1,x_2,...,x_n)dx_i \frac{\partial g_i}{\partial x_j}=\frac{\partial g_j}{\partial x_i}","['multivariable-calculus', 'partial-differential-equations']"
34,Find the directional derivative of function,Find the directional derivative of function,,"$$f(x,y) =      \begin{cases}     \displaystyle     \frac{x^{3}-3xy^{2}}{x^{2}+y^{2}},      & (x,y) \neq (0,0) \\     0 , & (x,y) = (0,0)     \end{cases}$$ Find the directional derivative in $(0,0)$ such that makes an angle $135°$ with $x$ positive direction. My attempt: $\lim_{h\to 0}f_x(0+h,0)=\frac{\frac{(0+h)^3-3(0+h)0^2}{(0+h)^2+0^2}-f(0,0)}{h}=1$ $\lim_{h\to 0}f_y(0,0+h)=\frac{\frac{(0)^3-3(0)(0+h)^2}{(0)^2+(0+h)^2}-f(0,0)}{h}=0$ The directional derivative is $(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$ , i have to normalize the vector , so $(-1,1)$ . The directional derivative is $(1,0)(-1,1)=-1$ My solution isn't correct , I can't get why , hope for some help , thanks !","Find the directional derivative in such that makes an angle with positive direction. My attempt: The directional derivative is , i have to normalize the vector , so . The directional derivative is My solution isn't correct , I can't get why , hope for some help , thanks !","f(x,y) = 
    \begin{cases}
    \displaystyle
    \frac{x^{3}-3xy^{2}}{x^{2}+y^{2}}, 
    & (x,y) \neq (0,0) \\
    0 , & (x,y) = (0,0)
    \end{cases} (0,0) 135° x \lim_{h\to 0}f_x(0+h,0)=\frac{\frac{(0+h)^3-3(0+h)0^2}{(0+h)^2+0^2}-f(0,0)}{h}=1 \lim_{h\to 0}f_y(0,0+h)=\frac{\frac{(0)^3-3(0)(0+h)^2}{(0)^2+(0+h)^2}-f(0,0)}{h}=0 (-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}) (-1,1) (1,0)(-1,1)=-1","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
35,Meaning of curl defined as in differential geometry,Meaning of curl defined as in differential geometry,,"there is a short remark in the textbook about GR I have been reading, which has been bugging me for the last few hours: My questions are: (1): In multivariable calculus, curl can be intuitively described as the rotation vector of a vector field. Why is it here a (0,2)-tensor field? (2): How can one derive the second equation from the first equation? Best regards. EDIT: The second equation only holds for vector fields $Y,Z \perp X$ .","there is a short remark in the textbook about GR I have been reading, which has been bugging me for the last few hours: My questions are: (1): In multivariable calculus, curl can be intuitively described as the rotation vector of a vector field. Why is it here a (0,2)-tensor field? (2): How can one derive the second equation from the first equation? Best regards. EDIT: The second equation only holds for vector fields .","Y,Z \perp X","['multivariable-calculus', 'differential-geometry', 'curl']"
36,"Finding the total derivative of $g(x)=(x,f(x))$ step by step.",Finding the total derivative of  step by step.,"g(x)=(x,f(x))","Suppose that $f:\mathbb R^n\to \mathbb R^k$ is differentiable. Define $g:\mathbb R^n\to \mathbb R^{n+k}$ as $g(x)=(x, f(x))$ . Then, the total derivative of $f$ at some $x\in \mathbb R^n$ is to be determined. I think the question above needs a little explanation. $x\in \mathbb R^n$ is an $n-$ tuple and therefore looks like $(x_1,x_2,...,x_n)$ and similarly for $f(x)$ so $g(x)=((x_1,...,x_n), (f_1(x),...,f_k(x))\in \mathbb R^n\times \mathbb R^k.$ But here in order that the question makes sense, let's identify $\mathbb R^n\times \mathbb R^k$ by $\mathbb R^{n+k}$ (I actually don't understand why it can be done. I do understand that $\mathbb R^{n+k}$ is isomorphic to $\mathbb R^n\times \mathbb R^k$ as vector spaces but that doesn't seem the reason for such replacement. One possible reason could be: norms on a finite dimensional vector spaces are equivalent. But here we have two isomorphic vector spaces so I don't understand how to see the validity of this step. So let me take such replacement for granted for now.). Let $f(x)=(f_1(x),f_2(x),...,f_k(x))$ , where $f_i:\mathbb R^n\to \mathbb R$ . $g$ is differentiable as each of its coordinate map is differentiable. So $g(x)=(x_1,x_2,...,x_n, f_1(x),f_2(x),...f_k(x))$ . Writing Jacobian matrix for $g$ at $x$ (it will be an $(n+k)\times n$ matrix) gives: $\begin{bmatrix}1 &0&0&\cdots &0\\ 0&1&0&\cdots&0\\ \cdots&\cdots&\cdots & &\cdots\\ \cdots&\cdots&\cdots & &\cdots\\  0&0&0&\cdots &1\\ \frac{\partial f_1(x)}{\partial x_1}&\frac{\partial f_1(x)}{\partial x_2}&\cdots&\cdots& \frac{\partial f_1(x)}{\partial x_n}\\ \frac{\partial f_2(x)}{\partial x_1}&\frac{\partial f_2(x)}{\partial x_2}&\cdots&\cdots& \frac{\partial f_2(x)}{\partial x_n}\\ \cdots&\cdots&\cdots & &\cdots\\ \cdots&\cdots&\cdots & &\cdots\\ \frac{\partial f_k(x)}{\partial x_1}&\frac{\partial f_k(x)}{\partial x_2}&\cdots&\cdots& \frac{\partial f_k(x)}{\partial x_n}\end{bmatrix}$ But this does not look correct as it has $f_i$ 's, which I created for convenience and they were not given in the original problem. So how do I get the correct total derivative? I'm having difficulty understanding this subject.","Suppose that is differentiable. Define as . Then, the total derivative of at some is to be determined. I think the question above needs a little explanation. is an tuple and therefore looks like and similarly for so But here in order that the question makes sense, let's identify by (I actually don't understand why it can be done. I do understand that is isomorphic to as vector spaces but that doesn't seem the reason for such replacement. One possible reason could be: norms on a finite dimensional vector spaces are equivalent. But here we have two isomorphic vector spaces so I don't understand how to see the validity of this step. So let me take such replacement for granted for now.). Let , where . is differentiable as each of its coordinate map is differentiable. So . Writing Jacobian matrix for at (it will be an matrix) gives: But this does not look correct as it has 's, which I created for convenience and they were not given in the original problem. So how do I get the correct total derivative? I'm having difficulty understanding this subject.","f:\mathbb R^n\to \mathbb R^k g:\mathbb R^n\to \mathbb R^{n+k} g(x)=(x, f(x)) f x\in \mathbb R^n x\in \mathbb R^n n- (x_1,x_2,...,x_n) f(x) g(x)=((x_1,...,x_n), (f_1(x),...,f_k(x))\in \mathbb R^n\times \mathbb R^k. \mathbb R^n\times \mathbb R^k \mathbb R^{n+k} \mathbb R^{n+k} \mathbb R^n\times \mathbb R^k f(x)=(f_1(x),f_2(x),...,f_k(x)) f_i:\mathbb R^n\to \mathbb R g g(x)=(x_1,x_2,...,x_n, f_1(x),f_2(x),...f_k(x)) g x (n+k)\times n \begin{bmatrix}1 &0&0&\cdots &0\\
0&1&0&\cdots&0\\
\cdots&\cdots&\cdots & &\cdots\\
\cdots&\cdots&\cdots & &\cdots\\
 0&0&0&\cdots &1\\
\frac{\partial f_1(x)}{\partial x_1}&\frac{\partial f_1(x)}{\partial x_2}&\cdots&\cdots& \frac{\partial f_1(x)}{\partial x_n}\\
\frac{\partial f_2(x)}{\partial x_1}&\frac{\partial f_2(x)}{\partial x_2}&\cdots&\cdots& \frac{\partial f_2(x)}{\partial x_n}\\
\cdots&\cdots&\cdots & &\cdots\\
\cdots&\cdots&\cdots & &\cdots\\
\frac{\partial f_k(x)}{\partial x_1}&\frac{\partial f_k(x)}{\partial x_2}&\cdots&\cdots& \frac{\partial f_k(x)}{\partial x_n}\end{bmatrix} f_i","['real-analysis', 'calculus', 'multivariable-calculus', 'differential-topology']"
37,Vector Calculus and the Navier Stokes Equation,Vector Calculus and the Navier Stokes Equation,,"I am reading through a paper that derives the thin-film equation from the Navier Stokes equation; however, am having some trouble understanding the vector calculus notation used. For example, the material derivative is defined as: $$ \frac{D\mathbf{v}}{dt} = \frac{\partial\mathbf{v}}{\partial t}+\mathbf{v}\cdot\nabla\mathbf{v}$$ where in $2D$ we have $\mathbf{v}=(u,w)$ and $\nabla=(\partial_x,\partial_z)$ . Hence, the gradient of $\mathbf{v}$ is the transpose of the Jacobian given by: $$ \nabla\mathbf{v} = \begin{bmatrix} u_x & w_x \\ u_z & w_z \end{bmatrix} $$ Now my question is what to make of: $$ \mathbf{v}\cdot\nabla\mathbf{v}=\begin{bmatrix} u \\ w \end{bmatrix} \cdot \begin{bmatrix} u_x & w_x \\ u_z & w_z \end{bmatrix} $$ As far as I am aware the dot product can be defined for two vectors and can be extended to two square matrices but I don't quite know what to make of this when you have a vector and a square matrix. Specifically, for this case I already know from past experience that: $$ \frac{D\mathbf{v}}{dt} = \frac{\partial \mathbf{v}}{\partial t}+\mathbf{v}\cdot\nabla\mathbf{v} = (u_t+uu_x+wu_z,w_t+uw_x+ww_z) $$ but upon coming back to this I am a little confused as to how to show this given the notation above.","I am reading through a paper that derives the thin-film equation from the Navier Stokes equation; however, am having some trouble understanding the vector calculus notation used. For example, the material derivative is defined as: where in we have and . Hence, the gradient of is the transpose of the Jacobian given by: Now my question is what to make of: As far as I am aware the dot product can be defined for two vectors and can be extended to two square matrices but I don't quite know what to make of this when you have a vector and a square matrix. Specifically, for this case I already know from past experience that: but upon coming back to this I am a little confused as to how to show this given the notation above."," \frac{D\mathbf{v}}{dt} = \frac{\partial\mathbf{v}}{\partial t}+\mathbf{v}\cdot\nabla\mathbf{v} 2D \mathbf{v}=(u,w) \nabla=(\partial_x,\partial_z) \mathbf{v}  \nabla\mathbf{v} = \begin{bmatrix} u_x & w_x \\ u_z & w_z \end{bmatrix}   \mathbf{v}\cdot\nabla\mathbf{v}=\begin{bmatrix} u \\ w \end{bmatrix} \cdot \begin{bmatrix} u_x & w_x \\ u_z & w_z \end{bmatrix}   \frac{D\mathbf{v}}{dt} = \frac{\partial \mathbf{v}}{\partial t}+\mathbf{v}\cdot\nabla\mathbf{v} = (u_t+uu_x+wu_z,w_t+uw_x+ww_z) ","['multivariable-calculus', 'partial-differential-equations', 'vector-analysis', 'fluid-dynamics']"
38,Centroid coordinate for region,Centroid coordinate for region,,"This is the region from which I want to calculate the centroid: $\{(x, y) \in \mathbb{R}^2 : 0 < 2x < y < 3-x^2\}$ . I calculated the area for the region and it is $A = \frac{5}{3}$ . Now, for the $x$ coordinate of the centroid I'm trying to calculate: $$\bar{x} = \frac{S_x}{A} = \frac{\int_0^1\int_{2x}^{3-x^2}ydydx}{\int_0^2\int_0^{\frac{1}{2}y}dxdy + \int_2^3\int_0^\sqrt{3-y}dxdy} = \frac{\frac{1}{2}\int_0^1\frac{y^2}{2}\biggr\rvert_{2x}^{3-x^2}dx}{\frac{5}{3}} = \frac{21/6}{5/3} = \frac{21}{15}$$ But the solutions give $\frac{7}{20}$ . I'm not understanding what am I doing wrong. Is it in my logic by constructing the $\bar{x}$ integrals? Or is it my integral calculations? I'm being around this problem for a while now.","This is the region from which I want to calculate the centroid: . I calculated the area for the region and it is . Now, for the coordinate of the centroid I'm trying to calculate: But the solutions give . I'm not understanding what am I doing wrong. Is it in my logic by constructing the integrals? Or is it my integral calculations? I'm being around this problem for a while now.","\{(x, y) \in \mathbb{R}^2 : 0 < 2x < y < 3-x^2\} A = \frac{5}{3} x \bar{x} = \frac{S_x}{A} = \frac{\int_0^1\int_{2x}^{3-x^2}ydydx}{\int_0^2\int_0^{\frac{1}{2}y}dxdy + \int_2^3\int_0^\sqrt{3-y}dxdy} = \frac{\frac{1}{2}\int_0^1\frac{y^2}{2}\biggr\rvert_{2x}^{3-x^2}dx}{\frac{5}{3}} = \frac{21/6}{5/3} = \frac{21}{15} \frac{7}{20} \bar{x}","['calculus', 'multivariable-calculus', 'functions', 'area', 'centroid']"
39,Not following the derivation of $\frac{dy}{dx}=-\frac{F_x}{F_y}$,Not following the derivation of,\frac{dy}{dx}=-\frac{F_x}{F_y},"I've seen similar questions to mine asked on the forum, but I haven't seen answers that address the part I'm confused about. My calculus textbook (Thomas from Pearson) derives the following formula to ""take some of the algebra out of implicit differentiation"": Suppose the function $F(x,y)$ is differentiable and the equation $F(x,y)=0$ defines $y$ implicitly as a differentiable function of $x$ . Then at any point where $F_y\neq 0$ , we have $$\frac{dy}{dx}=-\frac{F_x}{F_y}$$ . (The formula itself is pretty intuitive to me, except for the negative sign.) I feel like I am misinterpreting the derivation given, as it seems to be using $F(x,y)$ to denote two different functions and treating them as if they are the same. The derivation goes like this: Suppose that (1) the function $F(x,y)$ is differentiable and that (2) the equation $F(x,y)=0$ defines $y$ implicitly as a differentiable function of $x$ . Since $w=F(x,y)=0$ , the derivative $\frac{dw}{dx}$ must be zero. As I understand this, they are defining a new function $w:\{(x,y):F(x,y)=0\}\rightarrow\{0\}$ , a level curve of the original $F(x,y)$ , which is zero everywhere on its domain, and we're to suppose that its domain defines $y$ implicitly in terms of $x$ . But then they continue: ... Computing the derivative [of the equation $w=F(x,y)=0$ ] from the chain rule, we find $$0=\frac{dw}{dx}=F_x\frac{dx}{dx}+F_y\frac{dy}{dx}=F_x+F_y\frac{dy}{dx}.$$ Therefore, we have $$\frac{dy}{dx}=-\frac{F_x}{F_y}.$$ This is where I get confused. In the example questions, it is clear that $F_x$ and $F_y$ denote the partial derivatives of the original function $F(x,y)$ of which $w$ is a level curve. But this use of the chain rule seems to assume that those are also the partials of w (which is a constant function, and should have zero derivatives, no?). I'm interpreting this as a special case of $$\frac{dw}{dt}=\frac{\partial w}{\partial x}\frac{dx}{dt}+\frac{\partial w}{\partial y}\frac{dy}{dt}$$ where $t=x$ , and where $\frac{\partial w}{\partial x}$ and $\frac{\partial w}{\partial y}$ are written as $F_x$ and $F_y$ . But I'm not seeing how the former and the latter partials are equivalent. Why can we assume both that $\frac{dw}{dx}=0$ and that $F_x=\frac{\partial w}{\partial x}$ , when $F_x$ is not zero in general? Or is that assumption not actually being made by using the chain rule this way? What am I missing or getting wrong here? I'd really appreciate if someone would set me on the right track so that I can get some intuition for why this theorem works. Thanks!","I've seen similar questions to mine asked on the forum, but I haven't seen answers that address the part I'm confused about. My calculus textbook (Thomas from Pearson) derives the following formula to ""take some of the algebra out of implicit differentiation"": Suppose the function is differentiable and the equation defines implicitly as a differentiable function of . Then at any point where , we have . (The formula itself is pretty intuitive to me, except for the negative sign.) I feel like I am misinterpreting the derivation given, as it seems to be using to denote two different functions and treating them as if they are the same. The derivation goes like this: Suppose that (1) the function is differentiable and that (2) the equation defines implicitly as a differentiable function of . Since , the derivative must be zero. As I understand this, they are defining a new function , a level curve of the original , which is zero everywhere on its domain, and we're to suppose that its domain defines implicitly in terms of . But then they continue: ... Computing the derivative [of the equation ] from the chain rule, we find Therefore, we have This is where I get confused. In the example questions, it is clear that and denote the partial derivatives of the original function of which is a level curve. But this use of the chain rule seems to assume that those are also the partials of w (which is a constant function, and should have zero derivatives, no?). I'm interpreting this as a special case of where , and where and are written as and . But I'm not seeing how the former and the latter partials are equivalent. Why can we assume both that and that , when is not zero in general? Or is that assumption not actually being made by using the chain rule this way? What am I missing or getting wrong here? I'd really appreciate if someone would set me on the right track so that I can get some intuition for why this theorem works. Thanks!","F(x,y) F(x,y)=0 y x F_y\neq 0 \frac{dy}{dx}=-\frac{F_x}{F_y} F(x,y) F(x,y) F(x,y)=0 y x w=F(x,y)=0 \frac{dw}{dx} w:\{(x,y):F(x,y)=0\}\rightarrow\{0\} F(x,y) y x w=F(x,y)=0 0=\frac{dw}{dx}=F_x\frac{dx}{dx}+F_y\frac{dy}{dx}=F_x+F_y\frac{dy}{dx}. \frac{dy}{dx}=-\frac{F_x}{F_y}. F_x F_y F(x,y) w \frac{dw}{dt}=\frac{\partial w}{\partial x}\frac{dx}{dt}+\frac{\partial w}{\partial y}\frac{dy}{dt} t=x \frac{\partial w}{\partial x} \frac{\partial w}{\partial y} F_x F_y \frac{dw}{dx}=0 F_x=\frac{\partial w}{\partial x} F_x","['multivariable-calculus', 'partial-derivative', 'chain-rule', 'implicit-differentiation']"
40,"Help understanding how change of order of integration results in $\int_x^{1} \int_0^{t} f(s)ds dt = \int_{0}^{1} (1- \max(x,t)) f(t) dt$",Help understanding how change of order of integration results in,"\int_x^{1} \int_0^{t} f(s)ds dt = \int_{0}^{1} (1- \max(x,t)) f(t) dt","In this question, it mentions that after changing the order of integration, we obtain: $$\int_x^{1} \int_0^{t} f(s)ds dt = \int_{0}^{1} (1- \max(x,t)) f(t) dt$$ (assume $f\in C[0,1]$ and $x \in [0,1]$ ). I am trying to understand why. EDIT Thanks to dan_fulea and Brian Moehring I identified my mistake and the solution. Below is the correct form: $$\int_x^{1} \int_0^{t} f(s)ds dt = \int_0^{1} \int_{\max\{x,s\}}^{1} f(s)dt ds = \int_0^{1} f(s)\int_{\max\{x,s\}}^{1} dt ds=\int_0^{1} f(s)\big(1-\max\{x,s\}\big) ds$$","In this question, it mentions that after changing the order of integration, we obtain: (assume and ). I am trying to understand why. EDIT Thanks to dan_fulea and Brian Moehring I identified my mistake and the solution. Below is the correct form:","\int_x^{1} \int_0^{t} f(s)ds dt = \int_{0}^{1} (1- \max(x,t)) f(t) dt f\in C[0,1] x \in [0,1] \int_x^{1} \int_0^{t} f(s)ds dt = \int_0^{1} \int_{\max\{x,s\}}^{1} f(s)dt ds = \int_0^{1} f(s)\int_{\max\{x,s\}}^{1} dt ds=\int_0^{1} f(s)\big(1-\max\{x,s\}\big) ds","['calculus', 'integration', 'multivariable-calculus']"
41,"My teacher said that this is not necessary in the line integral, but why?","My teacher said that this is not necessary in the line integral, but why?",,"Question: Calculate the Scalar line integral: $$\int_C \left(𝑥\,𝑑𝑥 − 𝑦\,𝑑𝑦\right)$$ C is the segment traveled in the direction: $$(1,1)\,to\,(2,3)$$ I started by solving this question by parameterizing the points. Parameterization formula: $$(x,y)=B\,t+(1-t)\,A$$ $$(x,y)=(2,3)\,t+(1-t)\,(1,1)$$ $$(x,y)=(2t,3t)+(1-t,1-t)$$ $$(x,y)=(1+t,1+2t)$$ Our parameterization determines that $$ 0 ≤ t≤1$$ So our function $$r(t)=(1+t)\hat{i}+(1+2t)\hat{j}$$ $$r'(t)=(1,2)$$ $$ |r'(t) |=\sqrt{1^{2}+2^{2}}=\sqrt{5}$$ So our scalar line integral will be $$\int_C 𝑥\,𝑑𝑥 − 𝑦\,𝑑𝑦 =\int_0^1 [(1+t)\,1 - (1+2t)\,2 ]\,|r'(t) | dt$$ $$=\int_0^1 [1+t - 2-4t ]\,\sqrt{5}dt $$ $$=\sqrt{5} \int_0^1 -3t - 1 dt$$ $$\therefore\sqrt{5}\,(\frac{-5}{2})$$ The thing is, my professor said that it is not necessary to have put the $$|r'(t) |$$ in the scalar integral formula, because that was another case, but he didn't explain... was he right?","Question: Calculate the Scalar line integral: C is the segment traveled in the direction: I started by solving this question by parameterizing the points. Parameterization formula: Our parameterization determines that So our function So our scalar line integral will be The thing is, my professor said that it is not necessary to have put the in the scalar integral formula, because that was another case, but he didn't explain... was he right?","\int_C \left(𝑥\,𝑑𝑥 − 𝑦\,𝑑𝑦\right) (1,1)\,to\,(2,3) (x,y)=B\,t+(1-t)\,A (x,y)=(2,3)\,t+(1-t)\,(1,1) (x,y)=(2t,3t)+(1-t,1-t) (x,y)=(1+t,1+2t)  0 ≤ t≤1 r(t)=(1+t)\hat{i}+(1+2t)\hat{j} r'(t)=(1,2)  |r'(t) |=\sqrt{1^{2}+2^{2}}=\sqrt{5} \int_C 𝑥\,𝑑𝑥 − 𝑦\,𝑑𝑦 =\int_0^1 [(1+t)\,1 - (1+2t)\,2 ]\,|r'(t) | dt =\int_0^1 [1+t - 2-4t ]\,\sqrt{5}dt  =\sqrt{5} \int_0^1 -3t - 1 dt \therefore\sqrt{5}\,(\frac{-5}{2}) |r'(t) |","['calculus', 'integration', 'multivariable-calculus', 'line-integrals']"
42,"Computing $\int_V(x^2+y^2+z^2)dxdydz,$ where $V$ is the intersection of two spheres in $\Bbb R^3.$",Computing  where  is the intersection of two spheres in,"\int_V(x^2+y^2+z^2)dxdydz, V \Bbb R^3.","Compute $$\int_V(x^2+y^2+z^2)dxdydz,$$ where $V$ is the intersection of the spheres $x^2+^2+z^2\le1$ and $x^2+y^2+z^2\le 2z.$ My attempt: $$\begin{aligned}\begin{cases}x^2+y^2+z^2&\le 1\\x^2+y^2+(z-1)^2&\le 1\end{cases}\\ \\\text{for the orthogonal projection of the intersection}\\\\1-z^2&=1-(z-1)^2\\2z-1&=0\\z&=\frac12\\\implies \boxed{x^2+y^2\le\frac14}\\\psi(r,\varphi,z)&=(r\cos\varphi,r\sin\varphi,z),\\J_\psi(r,\varphi,z)&=r\\S=\{(r,\varphi,z):0\le r\le\frac12,0\le\varphi\le2\pi,1-\sqrt{1-r^2}\le z\le\sqrt{1-r^2}\}\\\int_Vf(x,y,z)dxdydz&=\int_{\psi(S)}f(x,y,z)\\&=\int_0^{1/2}\int_0^{2\pi}\int_{1-\sqrt{1-r^2}}^{\sqrt{1-r^2}}(r^2+z^2)rdzd\varphi dr\\&=2\pi\int_0^{1/2}r\left(r^2z+\frac{z^3}3\right)\Big|_{1-\sqrt{1-r^2}}^{\sqrt{1-r^2}}\\&=2\pi\int_0^{1/2}r\left(2r^2\sqrt{1-r^2}-r^2+\frac23(1-r^2)^{3/2}-\frac43+\sqrt{1-r^2}+r^2\right)dr\\&=2\pi\int_0^{1/2}\left(2r^3\sqrt{1-r^2}+\frac23r(1-r^2)^{2/3}-\frac43r+r\sqrt{1-r^2}\right)dr\end{aligned}$$ Side computations: $$\begin{aligned}\frac{\sqrt{1-r^2}^3-(1-\sqrt{1-r^2})^3}3&=\frac{(1-r^2)^{3/2}-1+3\sqrt{1-r^2}-3(1-r^2)+(1-r^2)^{3/2}}3\\&=\frac23(1-r^2)^{3/2}-\frac43+\sqrt{1-r^2}+r^2\\\end{aligned}$$ $$\begin{aligned}\int_0^{1/2} 2r^3\sqrt{1-r^2}&=\begin{bmatrix}du=-\frac13r\sqrt{1-r^2}dr\implies  u=(1-r^2)^{3/2}\\v=-6r^2\implies dv=-12rdr \end{bmatrix}\\&=-6r^2(1-r^2)^{3/2}\Big|_0^{1/2}-\int_0^{1/2} -12r(1-r^2)^{3/2}dr\\&=-6r^2(1-r^2)^{1/2}\Big|_0^{1/2}-\frac{12}5(1-r^2)^{5/2}\Big|_0^{1/2}\\\int_0^{1/2}\frac23r(1-r^2)^{1/2}&=-\frac13\int_0^{1/2}-2r(1-r^2)^{1/2}dr\\&=-\frac29(1-r^2)^{3/2}\Big|_0^{1/2}\\\int_0^{1/2}r(1-r^2)^{1/2}dr&=-\frac12\int_0^{1/2}-2r(1-r^2)^{1/2}dr\\&=-\frac13(1-r^2)^{3/2}\Big|_0^{1/2}\end{aligned}$$ I stopped here because this already got too long and it wouldn't work on exam. Is there any better method?",Compute where is the intersection of the spheres and My attempt: Side computations: I stopped here because this already got too long and it wouldn't work on exam. Is there any better method?,"\int_V(x^2+y^2+z^2)dxdydz, V x^2+^2+z^2\le1 x^2+y^2+z^2\le 2z. \begin{aligned}\begin{cases}x^2+y^2+z^2&\le 1\\x^2+y^2+(z-1)^2&\le 1\end{cases}\\ \\\text{for the orthogonal projection of the intersection}\\\\1-z^2&=1-(z-1)^2\\2z-1&=0\\z&=\frac12\\\implies \boxed{x^2+y^2\le\frac14}\\\psi(r,\varphi,z)&=(r\cos\varphi,r\sin\varphi,z),\\J_\psi(r,\varphi,z)&=r\\S=\{(r,\varphi,z):0\le r\le\frac12,0\le\varphi\le2\pi,1-\sqrt{1-r^2}\le z\le\sqrt{1-r^2}\}\\\int_Vf(x,y,z)dxdydz&=\int_{\psi(S)}f(x,y,z)\\&=\int_0^{1/2}\int_0^{2\pi}\int_{1-\sqrt{1-r^2}}^{\sqrt{1-r^2}}(r^2+z^2)rdzd\varphi dr\\&=2\pi\int_0^{1/2}r\left(r^2z+\frac{z^3}3\right)\Big|_{1-\sqrt{1-r^2}}^{\sqrt{1-r^2}}\\&=2\pi\int_0^{1/2}r\left(2r^2\sqrt{1-r^2}-r^2+\frac23(1-r^2)^{3/2}-\frac43+\sqrt{1-r^2}+r^2\right)dr\\&=2\pi\int_0^{1/2}\left(2r^3\sqrt{1-r^2}+\frac23r(1-r^2)^{2/3}-\frac43r+r\sqrt{1-r^2}\right)dr\end{aligned} \begin{aligned}\frac{\sqrt{1-r^2}^3-(1-\sqrt{1-r^2})^3}3&=\frac{(1-r^2)^{3/2}-1+3\sqrt{1-r^2}-3(1-r^2)+(1-r^2)^{3/2}}3\\&=\frac23(1-r^2)^{3/2}-\frac43+\sqrt{1-r^2}+r^2\\\end{aligned} \begin{aligned}\int_0^{1/2} 2r^3\sqrt{1-r^2}&=\begin{bmatrix}du=-\frac13r\sqrt{1-r^2}dr\implies  u=(1-r^2)^{3/2}\\v=-6r^2\implies dv=-12rdr \end{bmatrix}\\&=-6r^2(1-r^2)^{3/2}\Big|_0^{1/2}-\int_0^{1/2} -12r(1-r^2)^{3/2}dr\\&=-6r^2(1-r^2)^{1/2}\Big|_0^{1/2}-\frac{12}5(1-r^2)^{5/2}\Big|_0^{1/2}\\\int_0^{1/2}\frac23r(1-r^2)^{1/2}&=-\frac13\int_0^{1/2}-2r(1-r^2)^{1/2}dr\\&=-\frac29(1-r^2)^{3/2}\Big|_0^{1/2}\\\int_0^{1/2}r(1-r^2)^{1/2}dr&=-\frac12\int_0^{1/2}-2r(1-r^2)^{1/2}dr\\&=-\frac13(1-r^2)^{3/2}\Big|_0^{1/2}\end{aligned}","['integration', 'multivariable-calculus']"
43,Asking About Best Upper Bound And lowerBound,Asking About Best Upper Bound And lowerBound,,"Find the best lower and upper bounds for $$\left(\cos A-\sin A\right)\left(\cos B-\sin B\right)\left(\cos C-\sin C\right),$$ $1)~~$ overall acute-angled $\Delta ABC.$ first of all we know that $\cos(A)-\sin(A) = -\sqrt{2}\sin(A - \frac{\pi}{4})$ $\Pi { -\sqrt{2}\sin(A - \frac{\pi}{4})}=-2\sqrt{2}*\Pi{(\sin(A-\frac{\pi}{4}))}$ $-\frac{1}{2}*\sqrt{2}<\sin(A-\frac{\pi}{4})<\frac{1}{2}*\sqrt{2}$ $-\frac{1}{2}*\sqrt{2}<\sin(B-\frac{\pi}{4})<\frac{1}{2}*\sqrt{2}$ $-\frac{1}{2}*\sqrt{2}<\sin(C-\frac{\pi}{4})<\frac{1}{2}*\sqrt{2}$ Then Lagrange? or there is any other method you all could suggest Edit: For Everyone who wanna post or finish using every possible method just write it down here",Find the best lower and upper bounds for overall acute-angled first of all we know that Then Lagrange? or there is any other method you all could suggest Edit: For Everyone who wanna post or finish using every possible method just write it down here,"\left(\cos A-\sin A\right)\left(\cos B-\sin B\right)\left(\cos C-\sin C\right), 1)~~ \Delta ABC. \cos(A)-\sin(A) = -\sqrt{2}\sin(A - \frac{\pi}{4}) \Pi { -\sqrt{2}\sin(A - \frac{\pi}{4})}=-2\sqrt{2}*\Pi{(\sin(A-\frac{\pi}{4}))} -\frac{1}{2}*\sqrt{2}<\sin(A-\frac{\pi}{4})<\frac{1}{2}*\sqrt{2} -\frac{1}{2}*\sqrt{2}<\sin(B-\frac{\pi}{4})<\frac{1}{2}*\sqrt{2} -\frac{1}{2}*\sqrt{2}<\sin(C-\frac{\pi}{4})<\frac{1}{2}*\sqrt{2}","['calculus', 'multivariable-calculus', 'inequality', 'lagrange-multiplier']"
44,Chain rule multivariable calculus.,Chain rule multivariable calculus.,,"I have problem understanding the chain rule. For example consider a function $w = f(x,y)$ and $y=x^2$ . By the chain rule: \begin{equation} \frac{\partial w}{\partial x} = \frac{\partial w}{\partial x}\frac{\partial x}{\partial x}+\frac{\partial w}{\partial y}\frac{\partial y}{\partial x} = \frac{\partial w}{\partial x}+2x\frac{\partial w}{\partial y} \end{equation} so $\frac{\partial w}{\partial y} = 0$ . What is wrong with this reasoning? Can you give some example to show that this is not true?",I have problem understanding the chain rule. For example consider a function and . By the chain rule: so . What is wrong with this reasoning? Can you give some example to show that this is not true?,"w = f(x,y) y=x^2 \begin{equation}
\frac{\partial w}{\partial x} = \frac{\partial w}{\partial x}\frac{\partial x}{\partial x}+\frac{\partial w}{\partial y}\frac{\partial y}{\partial x} = \frac{\partial w}{\partial x}+2x\frac{\partial w}{\partial y}
\end{equation} \frac{\partial w}{\partial y} = 0","['calculus', 'multivariable-calculus', 'chain-rule']"
45,Limits of $u$ and $v$,Limits of  and,u v,"I am trying to evaluate a double integral $$I=\int_{0}^2\int_{0}^{2-x}(x+y)^2e^{\frac{2y}{x+y}}dydx$$ I used the transformation $$x+y=v, y=uv$$ That is $$x=v(1-u), y=uv$$ We get the Jacobian as: $$J=\left|\begin{array}{ll} \frac{\partial x}{\partial u} & \frac{\partial y}{\partial u} \\ \frac{\partial x}{\partial v} & \frac{\partial y}{\partial v} \end{array}\right|=\left|\begin{array}{cc} -v & 1-u \\ v & u \end{array}\right|=-v$$ So we get: $$I=\iint_{D(u,v)}v^3e^{2u}dudv$$ I am unable to figure out the limits of $u$ and $v$",I am trying to evaluate a double integral I used the transformation That is We get the Jacobian as: So we get: I am unable to figure out the limits of and,"I=\int_{0}^2\int_{0}^{2-x}(x+y)^2e^{\frac{2y}{x+y}}dydx x+y=v, y=uv x=v(1-u), y=uv J=\left|\begin{array}{ll}
\frac{\partial x}{\partial u} & \frac{\partial y}{\partial u} \\
\frac{\partial x}{\partial v} & \frac{\partial y}{\partial v}
\end{array}\right|=\left|\begin{array}{cc}
-v & 1-u \\
v & u
\end{array}\right|=-v I=\iint_{D(u,v)}v^3e^{2u}dudv u v","['multivariable-calculus', 'multiple-integral', 'jacobian', 'change-of-variable']"
46,Finding level curves to function,Finding level curves to function,,"I am supposed to find and draw a few level curves for the function $g(x,y) = e^{\sqrt{x^2-y^2}}$ . I have already calculated the domain of the function: $Df=\lbrace(x,y) : y ≤ ±|x|\rbrace$ In order to find a few level curves, I began by calculating the following for a constant c: $e^{\sqrt{x^2-y^2}}=c$ , This gives $\sqrt{x^2-y^2}=\ln(c)$ and $c>0$ . The first level curve, when $c=1$ : $$g(x,y)=e^{\sqrt{x^2-y^2}}=1\implies x^2-y^2=0\implies y=±x,$$ which I can I can easily draw, but I am having trouble finding more level curves. It feels like they get very complicated e.g. when $c=1$ . How do I find values of $c$ that result in level curves that aren't too hard to draw?","I am supposed to find and draw a few level curves for the function . I have already calculated the domain of the function: In order to find a few level curves, I began by calculating the following for a constant c: , This gives and . The first level curve, when : which I can I can easily draw, but I am having trouble finding more level curves. It feels like they get very complicated e.g. when . How do I find values of that result in level curves that aren't too hard to draw?","g(x,y) = e^{\sqrt{x^2-y^2}} Df=\lbrace(x,y) : y ≤ ±|x|\rbrace e^{\sqrt{x^2-y^2}}=c \sqrt{x^2-y^2}=\ln(c) c>0 c=1 g(x,y)=e^{\sqrt{x^2-y^2}}=1\implies x^2-y^2=0\implies y=±x, c=1 c",['multivariable-calculus']
47,Proving a two variable limit using $\epsilon- \delta$ approach.,Proving a two variable limit using  approach.,\epsilon- \delta,"I came across a question in which we had to prove the following limit by $\epsilon-\delta$ approach: $\lim_{(x,y)\to(-1,-1)}xy-2x^2=-1$ I had to prove that: For every $\epsilon>0$ in $\sqrt{(x+1)^2+(y+1)^2}<\epsilon$ , there exists a corresponding $\delta$ such that $|xy-2x^2+1|< \delta$ . So, I proceeded by putting $x+1=x_1$ and $y+1=y_1$ so that it would simplify the former expression inside the square root and would also help me to convert it to polar. However, neither of the methods worked and I ran out of ideas. I am new to multivariable calculus and am using $\epsilon-\delta$ approach in two variables for the first time. Would someone please help me with this?","I came across a question in which we had to prove the following limit by approach: I had to prove that: For every in , there exists a corresponding such that . So, I proceeded by putting and so that it would simplify the former expression inside the square root and would also help me to convert it to polar. However, neither of the methods worked and I ran out of ideas. I am new to multivariable calculus and am using approach in two variables for the first time. Would someone please help me with this?","\epsilon-\delta \lim_{(x,y)\to(-1,-1)}xy-2x^2=-1 \epsilon>0 \sqrt{(x+1)^2+(y+1)^2}<\epsilon \delta |xy-2x^2+1|< \delta x+1=x_1 y+1=y_1 \epsilon-\delta","['limits', 'multivariable-calculus', 'epsilon-delta']"
48,"If $F=\{(x,y,z)\in\mathbb R^3:f(x,y,z)=0\}$ is non-empty and $\frac{\partial f}{\partial x}\neq0$ on $F$, can $F$ be finite?","If  is non-empty and  on , can  be finite?","F=\{(x,y,z)\in\mathbb R^3:f(x,y,z)=0\} \frac{\partial f}{\partial x}\neq0 F F","In our analysis exam, we were given the question Let $f:\mathbb R^3\to \mathbb R$ is a $\mathcal C^\infty$ -smooth function. Let $F=\{(x,y,z)\in \mathbb R^3: f(x,y,z)=0\}$ is non-empty and $\frac{\partial f}{\partial x}\neq 0$ on $F$ . Prove or give a counterexample- $F$ can never be a collection of finitely many points in $\mathbb R^3$ . On one hand, I tried to figure out a lot of counterexamples none of which worked. And, on the other hand, I couldn't figure out any way to show that one point in $F$ forces more of them. I can't show much of my work except some counterexamples, which I thought were quite close $f(x,y,z)=x^2+y^2+z^2$ which gives only one point in $F$ which is unfortunately the point where $\frac{\partial f}{\partial x}=0$ . $f(x,y,z)=x^2+y^2+z^2+1$ which satisfies all other properties except that $F$ is not non-empty. I couldn't figure out any more.","In our analysis exam, we were given the question Let is a -smooth function. Let is non-empty and on . Prove or give a counterexample- can never be a collection of finitely many points in . On one hand, I tried to figure out a lot of counterexamples none of which worked. And, on the other hand, I couldn't figure out any way to show that one point in forces more of them. I can't show much of my work except some counterexamples, which I thought were quite close which gives only one point in which is unfortunately the point where . which satisfies all other properties except that is not non-empty. I couldn't figure out any more.","f:\mathbb R^3\to \mathbb R \mathcal C^\infty F=\{(x,y,z)\in \mathbb R^3: f(x,y,z)=0\} \frac{\partial f}{\partial x}\neq 0 F F \mathbb R^3 F f(x,y,z)=x^2+y^2+z^2 F \frac{\partial f}{\partial x}=0 f(x,y,z)=x^2+y^2+z^2+1 F","['calculus', 'analysis', 'multivariable-calculus']"
49,Do I have the right bounds and function for this integral?,Do I have the right bounds and function for this integral?,,"Find the integral of function $f(x,y,z)=(x^2+y^2+z^2)^{3/2}$ inside the sphere $(z-2)^2+x^2+y^2=4$ My approach: by changing it to spherical coordinates, we have $0\le\rho\le2\cos\varphi$ $(0\le\theta\le2\pi,\ 0\le\varphi\le\frac\pi2)$ , and the function becomes $f(\rho,\theta,\varphi)=ρ^3$ , which, when multplied by the Jacobian, $ρ^2\sin\varphi$ , becomes $ρ^5\sin\varphi$ . In other words, we are integrating this function in the above $\rho,\theta,\varphi$ bounds. Is this correct？Why would Wolfram alpha's calculator give a much larger value when integrated in Cartesian coordinates?","Find the integral of function inside the sphere My approach: by changing it to spherical coordinates, we have , and the function becomes , which, when multplied by the Jacobian, , becomes . In other words, we are integrating this function in the above bounds. Is this correct？Why would Wolfram alpha's calculator give a much larger value when integrated in Cartesian coordinates?","f(x,y,z)=(x^2+y^2+z^2)^{3/2} (z-2)^2+x^2+y^2=4 0\le\rho\le2\cos\varphi (0\le\theta\le2\pi,\ 0\le\varphi\le\frac\pi2) f(\rho,\theta,\varphi)=ρ^3 ρ^2\sin\varphi ρ^5\sin\varphi \rho,\theta,\varphi","['multivariable-calculus', 'multiple-integral', 'spherical-coordinates']"
50,Set of points A that are differentiable using the complex function $f(z) = |z|^{2}+e^{-5z}$,Set of points A that are differentiable using the complex function,f(z) = |z|^{2}+e^{-5z},"I have to check if this function is differentiable one of the following conditions: $ A=\{(0,0)\} $ or $ A=\{\text{z } \in \text{ C: }|z|=1 \} $ . To the first condition, I have verified it's differentiable using Cauchy-Riemann theorem to $f(z) = x^{2} + y^{2} + e^{-5x}(\cos(5y)-i*\sin(5y))$ , following: $\qquad \frac{\partial u}{\partial x}= 2x - 5 e^{-5x}\cos(5y) \qquad \frac{\partial v}{\partial y}= - 5 e^{-5x}\cos(5y)$ $\qquad \frac{\partial u}{\partial y}= 2y - 5 e^{-5x}\sin(5y) \qquad -\frac{\partial v}{\partial x}= - 5 e^{-5x}\sin(5y)$ Throught this I can conclude that $x=0$ and $y=0$ satisfy the theorem. However, to the second condition, I supposed that $x^{2}$ and $y^{2}$ are any constants that respect $|z|^{2} = 1$ . Thus, I have analyzed the Cauchy-Riemann to $f(z) = 1+e^{-5z}$ and it also respected the differentiability to $f(z) = 1 + e^{-5x}(\cos(5y)-i*\sin(5y))$ : $\qquad \frac{\partial u}{\partial x}= - 5 e^{-5x}\cos(5y) \qquad \frac{\partial v}{\partial y}= - 5 e^{-5x}\cos(5y)$ $\qquad \frac{\partial u}{\partial y}= - 5 e^{-5x}\sin(5y) \qquad -\frac{\partial v}{\partial x}= - 5 e^{-5x}\sin(5y)$ It looks like is not correct to me that this second conditions is differentiable because by the first condition we confirmed that it's only differentiable at $\{0,0\}$ . However, I can't understand why my assumption to the second condition is wrong and how I can show that is not differentiable at $|z|$ equal to any constant. I appreciate very much any help since I couldn't find errors or explanations myself.","I have to check if this function is differentiable one of the following conditions: or . To the first condition, I have verified it's differentiable using Cauchy-Riemann theorem to , following: Throught this I can conclude that and satisfy the theorem. However, to the second condition, I supposed that and are any constants that respect . Thus, I have analyzed the Cauchy-Riemann to and it also respected the differentiability to : It looks like is not correct to me that this second conditions is differentiable because by the first condition we confirmed that it's only differentiable at . However, I can't understand why my assumption to the second condition is wrong and how I can show that is not differentiable at equal to any constant. I appreciate very much any help since I couldn't find errors or explanations myself."," A=\{(0,0)\}   A=\{\text{z } \in \text{ C: }|z|=1 \}  f(z) = x^{2} + y^{2} + e^{-5x}(\cos(5y)-i*\sin(5y)) \qquad \frac{\partial u}{\partial x}= 2x - 5 e^{-5x}\cos(5y) \qquad \frac{\partial v}{\partial y}= - 5 e^{-5x}\cos(5y) \qquad \frac{\partial u}{\partial y}= 2y - 5 e^{-5x}\sin(5y) \qquad -\frac{\partial v}{\partial x}= - 5 e^{-5x}\sin(5y) x=0 y=0 x^{2} y^{2} |z|^{2} = 1 f(z) = 1+e^{-5z} f(z) = 1 + e^{-5x}(\cos(5y)-i*\sin(5y)) \qquad \frac{\partial u}{\partial x}= - 5 e^{-5x}\cos(5y) \qquad \frac{\partial v}{\partial y}= - 5 e^{-5x}\cos(5y) \qquad \frac{\partial u}{\partial y}= - 5 e^{-5x}\sin(5y) \qquad -\frac{\partial v}{\partial x}= - 5 e^{-5x}\sin(5y) \{0,0\} |z|","['complex-analysis', 'multivariable-calculus']"
51,"Problem $2.1.1$, Berkeley Problems in Mathematics ($3$rd Edition)","Problem , Berkeley Problems in Mathematics (rd Edition)",2.1.1 3,"Berkeley Problem in Mathematics $2.1.1$ is the following: Let $f$ be a function from $\mathbb{R}^n$ to $\mathbb{R}^n$ such that: (a) If $K$ is a compact subset of $\mathbb{R}^n$ , then $f(K)$ is a compact subset of $\mathbb{R}^n$ . (b) If $K_1$ , $K_2$ ,... is a decreasing sequence of compact subset of $\mathbb{R}^n$ , then $f(\cap_{i=1}^{\infty}K_i) = \cap_{i=1}^{\infty}f(K_i)$ . Show that $f$ is continious. The solution can be written as follows: We want to show that $f$ is continious on $\mathbb{R}^n$ . Thus, fix $x_0 \in \mathbb{R}^n$ . We need to show that $f$ is continious at $x_0$ , i.e for all $\epsilon>0$ , there exists a $\delta>0$ such that $|x-x_0| < \delta$ implies that $|f(x) - f(x_0)| < \epsilon$ . Given $\epsilon >0$ , let $B$ be the ball centered at $f(x_0)$ with radius $\epsilon$ . Define $K_i$ be the ball centered at $x_0$ with radius $\frac{1}{i}$ for $i= 1,2,...$ . Note that, each $K_i$ is a compact subset of $\mathbb{R}^n$ . Also, $\cap_{i=0}^{\infty} K_i = x_0$ .(Nested Interval Property). By, (b) we can conclude that $\cap_{i=0}^{\infty}f(K_i) = f(x_0)$ . Now, consider the sets $A_i:= (\mathbb{R}^n - B) \cap f(K_i)$ . Clearly, $A_i$ s are compact subsets of $\mathbb{R}^n$ by (a) and $A_i$ s are decreasing. Moreover, $\cap_{i=1}^{\infty} A_i = \emptyset$ .This means that, there exist a natural number $m$ such that $(\mathbb{R}^n - B) \cap f(K_m) = \emptyset$ . In other words, if $|x-x_0| < \frac{1}{m}$ , then $|f(x) - f(x_0)| < \epsilon$ . As a result, $f$ is continious at $x_0$ , since $x_0$ is arbitrary, $f$ is continious at $\mathbb{R}^n$ . We did use both assumptions (a) and (b). However, I could not find a counter-example when either assumption (a) or assumption (b) is removed. Is there an easy counter-example? Thanks in advance.","Berkeley Problem in Mathematics is the following: Let be a function from to such that: (a) If is a compact subset of , then is a compact subset of . (b) If , ,... is a decreasing sequence of compact subset of , then . Show that is continious. The solution can be written as follows: We want to show that is continious on . Thus, fix . We need to show that is continious at , i.e for all , there exists a such that implies that . Given , let be the ball centered at with radius . Define be the ball centered at with radius for . Note that, each is a compact subset of . Also, .(Nested Interval Property). By, (b) we can conclude that . Now, consider the sets . Clearly, s are compact subsets of by (a) and s are decreasing. Moreover, .This means that, there exist a natural number such that . In other words, if , then . As a result, is continious at , since is arbitrary, is continious at . We did use both assumptions (a) and (b). However, I could not find a counter-example when either assumption (a) or assumption (b) is removed. Is there an easy counter-example? Thanks in advance.","2.1.1 f \mathbb{R}^n \mathbb{R}^n K \mathbb{R}^n f(K) \mathbb{R}^n K_1 K_2 \mathbb{R}^n f(\cap_{i=1}^{\infty}K_i) = \cap_{i=1}^{\infty}f(K_i) f f \mathbb{R}^n x_0 \in \mathbb{R}^n f x_0 \epsilon>0 \delta>0 |x-x_0| < \delta |f(x) - f(x_0)| < \epsilon \epsilon >0 B f(x_0) \epsilon K_i x_0 \frac{1}{i} i= 1,2,... K_i \mathbb{R}^n \cap_{i=0}^{\infty} K_i = x_0 \cap_{i=0}^{\infty}f(K_i) = f(x_0) A_i:= (\mathbb{R}^n - B) \cap f(K_i) A_i \mathbb{R}^n A_i \cap_{i=1}^{\infty} A_i = \emptyset m (\mathbb{R}^n - B) \cap f(K_m) = \emptyset |x-x_0| < \frac{1}{m} |f(x) - f(x_0)| < \epsilon f x_0 x_0 f \mathbb{R}^n","['real-analysis', 'multivariable-calculus', 'continuity', 'compactness']"
52,Triple Integral - My answer seems too large,Triple Integral - My answer seems too large,,"Consider $R=[(x,y,z)\in \mathbb{R}^3 | 1 \leq x^2 + y^2 \leq 16, 0 \leq z \leq y+4]$ . Then calculate the integral $$I=\int_R(x-y)dV$$ I thought about the region and found that as $€ \leq x^2 + y^2 \leq 16,$ we have $x \in [1,4], y \in [1,4]$ . Then we can have $z \in [0,8]$ I then changed the integral to $\int_1^4 \int_1^{16-x^2}\int_0^{y+4}(x-y)dzdydx$ , which I found to be $\frac{93461}{70} \approx 1335.16$ However, if I was to make the region larger, say taking the  circle $x^2+y^2=16 \ (a=4)$ and making it into a spheroid with $z=8 \ (c=4)$ , then I would get volume $\frac{4}{3} \pi 4^24=\frac{256}{3}\pi \approx 268.08$ Hence I know that my answer is far too large, so I have probably found the incorrect limits of integration?","Consider . Then calculate the integral I thought about the region and found that as we have . Then we can have I then changed the integral to , which I found to be However, if I was to make the region larger, say taking the  circle and making it into a spheroid with , then I would get volume Hence I know that my answer is far too large, so I have probably found the incorrect limits of integration?","R=[(x,y,z)\in \mathbb{R}^3 | 1 \leq x^2 + y^2 \leq 16, 0 \leq z \leq y+4] I=\int_R(x-y)dV € \leq x^2 + y^2 \leq 16, x \in [1,4], y \in [1,4] z \in [0,8] \int_1^4 \int_1^{16-x^2}\int_0^{y+4}(x-y)dzdydx \frac{93461}{70} \approx 1335.16 x^2+y^2=16 \ (a=4) z=8 \ (c=4) \frac{4}{3} \pi 4^24=\frac{256}{3}\pi \approx 268.08","['geometry', 'multivariable-calculus', 'definite-integrals', '3d', 'multiple-integral']"
53,Integrating mass element of a spherical disc,Integrating mass element of a spherical disc,,"Suppose we have a solid disc of uniform surface density $\sigma$ , and radius $a$ .  I want to find its mass by integration. Normally we do this by the following method : We set the origin of a polar coordinate system at the center of the disc. Then we take a small area element, at a distance $r$ from this origin $O$ . Let $dm$ be the mass of this element. In polar coordinates, we know $dm=\sigma dxdy=\sigma rdrd\psi$ . Hence $$M=\int dm=\int_{0}^{2\pi}\int_{0}^{a}\sigma rdrd\psi = \sigma\pi a^2$$ However, what I want to do, is to make my origin at a point on the circumference of the disc, as follows : Now I want to carry out the same integration. What would be the new mass element, and the limits of integration, that would get me the exact same answer ? My guess is that the mass element would still be $dm=\sigma rdrd\psi$ however I'm not sure. The limits of integration would be very different though. Can anyone show me what these would be and how to find it, so that my final answer is exactly the same. Another guess is that the limits of $d\psi$ are symmetric, between some $[-c,c]$ where $c$ is some constant or maybe a function of $r$ such that $c=c(r)$ . However I think that it is symmetric even though I not know the form or the values. (note : I need $d\psi$ and $dr$ to be the variables of integration, not $dx$ , $dy$ or anything else. ) Any help would be highly appreciated.","Suppose we have a solid disc of uniform surface density , and radius .  I want to find its mass by integration. Normally we do this by the following method : We set the origin of a polar coordinate system at the center of the disc. Then we take a small area element, at a distance from this origin . Let be the mass of this element. In polar coordinates, we know . Hence However, what I want to do, is to make my origin at a point on the circumference of the disc, as follows : Now I want to carry out the same integration. What would be the new mass element, and the limits of integration, that would get me the exact same answer ? My guess is that the mass element would still be however I'm not sure. The limits of integration would be very different though. Can anyone show me what these would be and how to find it, so that my final answer is exactly the same. Another guess is that the limits of are symmetric, between some where is some constant or maybe a function of such that . However I think that it is symmetric even though I not know the form or the values. (note : I need and to be the variables of integration, not , or anything else. ) Any help would be highly appreciated.","\sigma a r O dm dm=\sigma dxdy=\sigma rdrd\psi M=\int dm=\int_{0}^{2\pi}\int_{0}^{a}\sigma rdrd\psi = \sigma\pi a^2 dm=\sigma rdrd\psi d\psi [-c,c] c r c=c(r) d\psi dr dx dy","['integration', 'multivariable-calculus', 'definite-integrals', 'polar-coordinates', 'multiple-integral']"
54,"Triple integral evaluation, I'm having difficulty understanding the change of the order of integration.","Triple integral evaluation, I'm having difficulty understanding the change of the order of integration.",,$$\int_{0}^1\int_{0}^z\int_{z}^1e^{y^3}dydxdz + \int_{0}^1\int_{z}^1\int_{x}^1e^{y^3}dydxdz$$ is the problem. The solution states that the result of the sum of the two is $$\int_{0}^1\int_{0}^y\int_{0}^ye^{y^3}dxdzdy$$ which I can't wrap my head around. Exactly how is the 'conversion' from dydxdz to dxdzdy achieved?,is the problem. The solution states that the result of the sum of the two is which I can't wrap my head around. Exactly how is the 'conversion' from dydxdz to dxdzdy achieved?,\int_{0}^1\int_{0}^z\int_{z}^1e^{y^3}dydxdz + \int_{0}^1\int_{z}^1\int_{x}^1e^{y^3}dydxdz \int_{0}^1\int_{0}^y\int_{0}^ye^{y^3}dxdzdy,"['integration', 'multivariable-calculus', 'multiple-integral']"
55,Why $\left(\frac{\partial x}{\partial y}\right)_z=-\left(\frac{\partial x}{\partial z}\right)_y \left(\frac{\partial z}{\partial y}\right)_x$?,Why ?,\left(\frac{\partial x}{\partial y}\right)_z=-\left(\frac{\partial x}{\partial z}\right)_y \left(\frac{\partial z}{\partial y}\right)_x,"I was reading a statistical mechanics book, the author use: $\left(\frac{\partial x}{\partial y}\right)_z=-\left(\frac{\partial x}{\partial z}\right)_y \left(\frac{\partial z}{\partial y}\right)_x$ so I'm trying to prove this, but I'm stuck. I'm using the theorem: $dF=\frac{\partial F}{\partial x}dx+\frac{\partial F}{\partial y}dy+ \frac{\partial F}{\partial z}dz=0$ with $dz=\left(\frac{\partial z}{\partial x}\right)_y dx +\left(\frac{\partial z}{\partial y}\right)_xdy$ if $z$ is constant then $dz=0$ then $\left(\frac{\partial z}{\partial x}\right)_y dx =-\left(\frac{\partial z}{\partial y}\right)_xdy$ so I get: $\frac{\partial F}{\partial x}dx+\frac{\partial F}{\partial y}dy=0$ but this result doesn't lead me to anything","I was reading a statistical mechanics book, the author use: so I'm trying to prove this, but I'm stuck. I'm using the theorem: with if is constant then then so I get: but this result doesn't lead me to anything",\left(\frac{\partial x}{\partial y}\right)_z=-\left(\frac{\partial x}{\partial z}\right)_y \left(\frac{\partial z}{\partial y}\right)_x dF=\frac{\partial F}{\partial x}dx+\frac{\partial F}{\partial y}dy+ \frac{\partial F}{\partial z}dz=0 dz=\left(\frac{\partial z}{\partial x}\right)_y dx +\left(\frac{\partial z}{\partial y}\right)_xdy z dz=0 \left(\frac{\partial z}{\partial x}\right)_y dx =-\left(\frac{\partial z}{\partial y}\right)_xdy \frac{\partial F}{\partial x}dx+\frac{\partial F}{\partial y}dy=0,"['calculus', 'multivariable-calculus', 'partial-derivative', 'statistical-mechanics']"
56,Limit in two variables using epsilon-delta,Limit in two variables using epsilon-delta,,"In a class assignment, I was asked to find the following limit: $$\underset{(x,y)\to (0,0)}{\lim}\frac{1-\cos(x^2\cdot y)}{x^6+ y^4}$$ I computed it to be $0$ numerically and as such, I wanted to prove it using the $\varepsilon-\delta$ definition, but I have not been able to find a $\delta$ such that $\left\vert{\sqrt{x^2+y^2}}\right\vert < \delta$ implies $\left\vert{\frac{1-\cos(x^2\cdot y)}{x^6+ y^4}}\right\vert < \varepsilon$ . I also tried using the squeeze theorem, but I have not been able to find an upper-bound for $\frac{1-\cos(x^2\cdot y)}{x^6+ y^4}$ that does not go to infinity as $(x,y)$ goes to $(0,0)$ . Any help or clue would be greatly appreciated. Thank you in advance!","In a class assignment, I was asked to find the following limit: I computed it to be numerically and as such, I wanted to prove it using the definition, but I have not been able to find a such that implies . I also tried using the squeeze theorem, but I have not been able to find an upper-bound for that does not go to infinity as goes to . Any help or clue would be greatly appreciated. Thank you in advance!","\underset{(x,y)\to (0,0)}{\lim}\frac{1-\cos(x^2\cdot y)}{x^6+ y^4} 0 \varepsilon-\delta \delta \left\vert{\sqrt{x^2+y^2}}\right\vert < \delta \left\vert{\frac{1-\cos(x^2\cdot y)}{x^6+ y^4}}\right\vert < \varepsilon \frac{1-\cos(x^2\cdot y)}{x^6+ y^4} (x,y) (0,0)","['calculus', 'limits', 'multivariable-calculus', 'epsilon-delta']"
57,"Prove that $||A||\leq\sqrt{\sum_{i,j}a_{ij}^2}\leq\sqrt{n}\,||A||$. I cannot prove the second inequality.",Prove that . I cannot prove the second inequality.,"||A||\leq\sqrt{\sum_{i,j}a_{ij}^2}\leq\sqrt{n}\,||A||","I am reading ""Multivariable Mathematics"" by Theodore Shifrin. The following exercise is in this book (Exercise 5.1.5 on p.201): Suppose $A$ is an $m\times n$ matrix. Prove that $||A||\leq\sqrt{\sum_{i,j}a_{ij}^2}\leq\sqrt{n}\,||A||$ . In this book, the definition of $||A||$ is as follows: $||A||:=\max_{||\mathbf{x}||=1} ||A\mathbf{x}||$ . I proved the first inequality but I cannot prove the second inequality. My partial solution is here: Let $||\mathbf{x}||=1$ . $$||A\mathbf{x}||=\sqrt{(a_{11}x_1+\dots+a_{1n}x_n)^2+\dots +(a_{m1}x_1+\dots+a_{mn}x_n)^2}.$$ By the Cauchy–Schwarz inequality, $$(a_{i1}x_1+\dots+a_{in}x_n)^2\leq(a_{i1}^2+\dots+a_{in}^2)\cdot(x_1^2+\dots+x_n^2)$$ for any $i$ such that $1\leq i \leq m$ . So, $$||A\mathbf{x}||=\sqrt{(a_{11}x_1+\dots+a_{1n}x_n)^2+\dots +(a_{m1}x_1+\dots+a_{mn}x_n)^2}\leq\sqrt{(a_{11}^2+\dots+a_{1n}^2+\dots+a_{m1}^2+\dots+a_{mn}^2)\cdot(x_1^2+\dots+x_n^2)}=\sqrt{a_{11}^2+\dots+a_{1n}^2+\dots+a_{m1}^2+\dots+a_{mn}^2}=\sqrt{\sum_{i,j}a_{ij}^2}.$$ $$||A||\geq \left\lVert A\begin{bmatrix} \frac{1}{\sqrt{n}} \\ \vdots \\ \frac{1}{\sqrt{n}} \\ \end{bmatrix}\right\rVert = \frac{1}{\sqrt{n}} \left\lVert A\begin{bmatrix} 1 \\ \vdots \\ 1 \\ \end{bmatrix}\right\rVert = \frac{1}{\sqrt{n}} \left\lVert \begin{bmatrix} a_{11}+\dots+a_{1n} \\ \vdots \\ a_{m1}+\dots+a_{mn} \\ \end{bmatrix}\right\rVert = \frac{1}{\sqrt{n}}\sqrt{(a_{11}^2+\dots+a_{1n}^2+\dots+a_{m1}^2+\dots+a_{mn}^2)+2(a_{11}a_{12}+\dots+a_{1{n-1}}a_{1n})+\dots+2(a_{m1}a_{m2}+\dots+a_{m{n-1}}a_{mn})}.$$ But we cannot say $$2(a_{11}a_{12}+\dots+a_{1{n-1}}a_{1n})+\dots+2(a_{m1}a_{m2}+\dots+a_{m{n-1}}a_{mn})\geq 0.$$","I am reading ""Multivariable Mathematics"" by Theodore Shifrin. The following exercise is in this book (Exercise 5.1.5 on p.201): Suppose is an matrix. Prove that . In this book, the definition of is as follows: . I proved the first inequality but I cannot prove the second inequality. My partial solution is here: Let . By the Cauchy–Schwarz inequality, for any such that . So, But we cannot say","A m\times n ||A||\leq\sqrt{\sum_{i,j}a_{ij}^2}\leq\sqrt{n}\,||A|| ||A|| ||A||:=\max_{||\mathbf{x}||=1} ||A\mathbf{x}|| ||\mathbf{x}||=1 ||A\mathbf{x}||=\sqrt{(a_{11}x_1+\dots+a_{1n}x_n)^2+\dots +(a_{m1}x_1+\dots+a_{mn}x_n)^2}. (a_{i1}x_1+\dots+a_{in}x_n)^2\leq(a_{i1}^2+\dots+a_{in}^2)\cdot(x_1^2+\dots+x_n^2) i 1\leq i \leq m ||A\mathbf{x}||=\sqrt{(a_{11}x_1+\dots+a_{1n}x_n)^2+\dots +(a_{m1}x_1+\dots+a_{mn}x_n)^2}\leq\sqrt{(a_{11}^2+\dots+a_{1n}^2+\dots+a_{m1}^2+\dots+a_{mn}^2)\cdot(x_1^2+\dots+x_n^2)}=\sqrt{a_{11}^2+\dots+a_{1n}^2+\dots+a_{m1}^2+\dots+a_{mn}^2}=\sqrt{\sum_{i,j}a_{ij}^2}. ||A||\geq \left\lVert A\begin{bmatrix}
\frac{1}{\sqrt{n}} \\
\vdots \\
\frac{1}{\sqrt{n}} \\
\end{bmatrix}\right\rVert
=
\frac{1}{\sqrt{n}} \left\lVert A\begin{bmatrix}
1 \\
\vdots \\
1 \\
\end{bmatrix}\right\rVert
=
\frac{1}{\sqrt{n}} \left\lVert \begin{bmatrix}
a_{11}+\dots+a_{1n} \\
\vdots \\
a_{m1}+\dots+a_{mn} \\
\end{bmatrix}\right\rVert
=
\frac{1}{\sqrt{n}}\sqrt{(a_{11}^2+\dots+a_{1n}^2+\dots+a_{m1}^2+\dots+a_{mn}^2)+2(a_{11}a_{12}+\dots+a_{1{n-1}}a_{1n})+\dots+2(a_{m1}a_{m2}+\dots+a_{m{n-1}}a_{mn})}. 2(a_{11}a_{12}+\dots+a_{1{n-1}}a_{1n})+\dots+2(a_{m1}a_{m2}+\dots+a_{m{n-1}}a_{mn})\geq 0.","['linear-algebra', 'multivariable-calculus', 'inequality', 'normed-spaces']"
58,upper bound on the integral of a product of non-negative functions [closed],upper bound on the integral of a product of non-negative functions [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question i have two functions $G_1(\theta,\phi)$ , $G_2(\theta,\phi)$ , $G_{1,2}: [0,\pi] \times [0,2\pi] \to \mathbb{R}^+_0$ i know that both functions satisfy that: \begin{equation*} \int_0^\pi\int_0^{2\pi}G_{1,2}(\theta,\phi)\,d\phi\, d\theta = 4\pi \end{equation*} and i'm trying to find an upper bound B, for the integral of the product of both functions over their domain: \begin{equation*} \int_0^\pi\int_0^{2\pi}G_1(\theta,\phi)G_2(\theta,\phi) \, d\phi \, d\theta \leq B \end{equation*}","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question i have two functions , , i know that both functions satisfy that: and i'm trying to find an upper bound B, for the integral of the product of both functions over their domain:","G_1(\theta,\phi) G_2(\theta,\phi) G_{1,2}: [0,\pi] \times [0,2\pi] \to \mathbb{R}^+_0 \begin{equation*}
\int_0^\pi\int_0^{2\pi}G_{1,2}(\theta,\phi)\,d\phi\, d\theta = 4\pi
\end{equation*} \begin{equation*}
\int_0^\pi\int_0^{2\pi}G_1(\theta,\phi)G_2(\theta,\phi) \, d\phi \, d\theta \leq B
\end{equation*}","['integration', 'multivariable-calculus', 'definite-integrals', 'upper-lower-bounds']"
59,"Min and max of $f(x,y)=e^{-xy}$ where $x^2+4y^2 \leq 5$",Min and max of  where,"f(x,y)=e^{-xy} x^2+4y^2 \leq 5","I am trying to use Lagrange multipliers to find the maximum and minimum values of the function $$f(x,y)=e^{-xy}$$ constrained as $$x^2+4y^2=5$$ I began this problem by setting up the Lagrangian: $$f(x,y) = e^{-xy}$$ $$g(x,y) = x^2+4y^2-5$$ $$L(x,y) = f(x,y) - \lambda g(x,y) = e^{-xy} - \lambda (x^2+4y^2-5)$$ So our equations are: $$-ye^{-xy} - 2\lambda x = 0$$ $$-xe^{xy} -8\lambda y = 0$$ $$x^2+4y^2-5 = 0$$ Now from the first equation, $e^{-xy} = 2\lambda x/y$ . Substituting into the second equation yields: $2x^2 \lambda / y = 8 \lambda y$ or $x^2+4y^2 = 0$ . This clearly violates the third equation, meaning this system has no solution. Doe this mean there are no local maxima or minima? Any guidance is greatly appreciated!","I am trying to use Lagrange multipliers to find the maximum and minimum values of the function constrained as I began this problem by setting up the Lagrangian: So our equations are: Now from the first equation, . Substituting into the second equation yields: or . This clearly violates the third equation, meaning this system has no solution. Doe this mean there are no local maxima or minima? Any guidance is greatly appreciated!","f(x,y)=e^{-xy} x^2+4y^2=5 f(x,y) = e^{-xy} g(x,y) = x^2+4y^2-5 L(x,y) = f(x,y) - \lambda g(x,y) = e^{-xy} - \lambda (x^2+4y^2-5) -ye^{-xy} - 2\lambda x = 0 -xe^{xy} -8\lambda y = 0 x^2+4y^2-5 = 0 e^{-xy} = 2\lambda x/y 2x^2 \lambda / y = 8 \lambda y x^2+4y^2 = 0","['analysis', 'multivariable-calculus']"
60,"Given a general region, find the double integral bounded between $y = x$ and $y=3x-x^2 $","Given a general region, find the double integral bounded between  and",y = x y=3x-x^2 ,"$$ J  = \iint_R (x^2-xy)\,dx \,dy, $$ Suppose region R is bounded between $y = x$ and $y=3x-x^2 $ My attempt using vertical integration: $$ \int^{x=2}_{x=0} \int^{y=3x-x^2}_{y=x} \left({x^2-xy}\right)dy\ dx$$ $$\int^2_0 \left[x^2y-x\frac{y^2}{2}\right]^{3x-x^2}_{x}\, dx$$ $$\int^2_0 \frac{-x^5+4x^4-4x^3}{2} \,dx $$ $$\boxed{J = -\frac{8}{15}}$$ My attempt using horizontal integration : $$ \int^{y=2}_{y=0} \int^{x=y}_{x=3\,\pm \sqrt{9-y}} \left({x^2-xy}\right)dx\ dy$$ For $ x = 3+\sqrt{9-y}$ $$  \int^2_0 \left[\frac{x^3}{3}-\frac{x^2}{2}y\right]^y_{3+\sqrt{9-y} }\,dy$$ For $ x = 3-\sqrt{9-y}$ $$  \int^2_0 \left[\frac{x^3}{3}-\frac{x^2}{2}y\right]^y_{3-\sqrt{9-y} }\,dy$$ My doubts : 1.) How do I set my limit of integration for horizontal integration, if there is $\pm$ to be considered ? 2.) the answer as negative what does that imply in questions related to double integrals? Could you guys please help","Suppose region R is bounded between and My attempt using vertical integration: My attempt using horizontal integration : For For My doubts : 1.) How do I set my limit of integration for horizontal integration, if there is to be considered ? 2.) the answer as negative what does that imply in questions related to double integrals? Could you guys please help"," J  = \iint_R (x^2-xy)\,dx \,dy,  y = x y=3x-x^2   \int^{x=2}_{x=0} \int^{y=3x-x^2}_{y=x} \left({x^2-xy}\right)dy\ dx \int^2_0 \left[x^2y-x\frac{y^2}{2}\right]^{3x-x^2}_{x}\, dx \int^2_0 \frac{-x^5+4x^4-4x^3}{2} \,dx  \boxed{J = -\frac{8}{15}}  \int^{y=2}_{y=0} \int^{x=y}_{x=3\,\pm \sqrt{9-y}} \left({x^2-xy}\right)dx\ dy  x = 3+\sqrt{9-y}   \int^2_0 \left[\frac{x^3}{3}-\frac{x^2}{2}y\right]^y_{3+\sqrt{9-y} }\,dy  x = 3-\sqrt{9-y}   \int^2_0 \left[\frac{x^3}{3}-\frac{x^2}{2}y\right]^y_{3-\sqrt{9-y} }\,dy \pm","['multivariable-calculus', 'multiple-integral', 'iterated-integrals']"
61,"Find minimum value of the function $f(x,y,z)=x+y+z$ on the ball $x^2+y^2+z^2=a^2$. (Where did I make a mistake?)",Find minimum value of the function  on the ball . (Where did I make a mistake?),"f(x,y,z)=x+y+z x^2+y^2+z^2=a^2","Find minimum value of the function $f(x,y,z)=x+y+z$ on the ball $x^2+y^2+z^2=a^2$ , where $x,y,z \ge 0$ and $a>0.$ What I did: Lagrange function: $L(x,y,z)=x+y+z+\lambda(x^2+y^2+z^2-a^2)$ . $L_x=1+2\lambda x = 0.$ $L_y=1+2\lambda y = 0.$ $L_z=1+2\lambda z=0.$ $L_{\lambda}=x^2+y^2+z^2=a^2$ . From the first three equations: $x=y=z=-\frac{1}{2\lambda}$ From the last equation: $\frac{3}{4\lambda^2}=a^2 \Longrightarrow\lambda=\pm \frac{\sqrt{3}}{2a}$ . Since we want $x,y,z\ge0.$ $x=y=z=\frac{a}{\sqrt{3}}$ (Took the negative $\lambda$ ). And substituting back into $f(x,y,z)=\frac{3a}{\sqrt{3}}=\sqrt{3}a$ . But the answer is $a$ . Would appreciate any help, I can't find where did I make a mistake, and I don't want to assume that the final answer given is false without being sure about it, because I know I make alot of mistakes. Thanks in advance!","Find minimum value of the function on the ball , where and What I did: Lagrange function: . . From the first three equations: From the last equation: . Since we want (Took the negative ). And substituting back into . But the answer is . Would appreciate any help, I can't find where did I make a mistake, and I don't want to assume that the final answer given is false without being sure about it, because I know I make alot of mistakes. Thanks in advance!","f(x,y,z)=x+y+z x^2+y^2+z^2=a^2 x,y,z \ge 0 a>0. L(x,y,z)=x+y+z+\lambda(x^2+y^2+z^2-a^2) L_x=1+2\lambda x = 0. L_y=1+2\lambda y = 0. L_z=1+2\lambda z=0. L_{\lambda}=x^2+y^2+z^2=a^2 x=y=z=-\frac{1}{2\lambda} \frac{3}{4\lambda^2}=a^2 \Longrightarrow\lambda=\pm \frac{\sqrt{3}}{2a} x,y,z\ge0. x=y=z=\frac{a}{\sqrt{3}} \lambda f(x,y,z)=\frac{3a}{\sqrt{3}}=\sqrt{3}a a","['multivariable-calculus', 'lagrange-multiplier']"
62,Behaviour of density function when flattening the hemisphere,Behaviour of density function when flattening the hemisphere,,"Suppose I keep my hollow sphere centred at the origin and I define a uniform scalar value (say mass density) at each point of it, call it $\phi$ . If I were to smoothly flatten the hemisphere into the $z=0$ plane, I would get a disc. Now, since the mass must remain constant under compression, the new flattened disc has some density function $\rho$ such that: $$ \int_{disc} \rho dA = \int_{sphere} \phi dA$$ That is masses are conserved. Given the functional form of $\phi$ can determine what $\rho$ should be? If not, what about the special case where $\phi$ is uniform? How does the result generalize? Context: I was reading this truly beautiful question on physics stack exchange and after some thought on what went wrong in OP's calculation , I think the question in OP's post will be solved if the answer to the above questions I've asked is answered.","Suppose I keep my hollow sphere centred at the origin and I define a uniform scalar value (say mass density) at each point of it, call it . If I were to smoothly flatten the hemisphere into the plane, I would get a disc. Now, since the mass must remain constant under compression, the new flattened disc has some density function such that: That is masses are conserved. Given the functional form of can determine what should be? If not, what about the special case where is uniform? How does the result generalize? Context: I was reading this truly beautiful question on physics stack exchange and after some thought on what went wrong in OP's calculation , I think the question in OP's post will be solved if the answer to the above questions I've asked is answered.",\phi z=0 \rho  \int_{disc} \rho dA = \int_{sphere} \phi dA \phi \rho \phi,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
63,"If a matrix $A$ is orthogonal, show that $\Delta(f \circ A) = (\Delta f)\circ A$","If a matrix  is orthogonal, show that",A \Delta(f \circ A) = (\Delta f)\circ A,"Let $O(d)$ be the space of orthogonal $d \times d$ matrices over $\mathbb{R}$ . These matrices act on $C_c^2(\mathbb{R}^d)$ , the set of twice continuously differentiable functions $f : \mathbb{R}^d \to \mathbb{R}$ with compact support, via $(f \circ A)(x) := f(Ax)$ , where $A \in O(d)$ . I am trying to show that for all $f \in C_c^2(\mathbb{R}^d) $ and for all $A \in O(d)$ , we have $\Delta(f \circ A) = (\Delta f)\circ A$ . Here, $\Delta$ denotes the Laplace operator. I am aware of the hint in the answer given to this related question here , however I am unable to complete the proof. Further hints on how exactly to apply the chain rule, as well as a full proof would be very much appreciated. Thanks! Edit : Until now, I have the following: Let $A := (a_{ij})$ , $x := (x_1,...,x_d)$ . Since $AA^\top = I$ , we have $$ \sum_{j=1}^d a_{ij}a_{kj} = \delta_{ik}, $$ where $\delta_{ik}$ is the Kronecker Delta. We thus have $$(f \circ A)(x) = f(Ax) = f \left( \sum_{i=1}^da_{1i}x_i,..., \sum_{i=1}^da_{di}x_i\right).$$ We denote $z_i = g_i(x_1,...,x_d) = \sum_{k=1}^da_{ik}x_k$ . With the chain rule $$ \frac{\partial f}{\partial x_j} = \sum_{i=1}^d  \frac{\partial f}{\partial z_i} \frac{\partial z_i}{\partial x_j},$$ we get that \begin{align}  \frac{\partial}{\partial x_j}(f \circ A)(x) &= \frac{\partial}{\partial x_j}f(z_1,...,z_d)\\ &= \sum_{i=1}^d \frac{\partial f(z_1,...,z_d)}{\partial z_i} \frac{\partial g_i(x_1,...,x_d)}{\partial x_j}\\ &= \sum_{i=1}^d a_{ij} \frac{\partial f(z_1,...,z_d)}{\partial z_i}. \end{align} In the next step we get \begin{align} \frac{\partial^2}{\partial x_j^2}(f \circ A)(x) = \frac{\partial}{\partial x_j} \left( \sum_{i=1}^d a_{ij} \frac{\partial f(z_1,...,z_d)}{\partial z_i} \right) = \sum_{i=1}^d a_{ij} \frac{\partial^2 f(z_1,...,z_d)}{\partial x_j \partial z_i}, \end{align} but I'm not sure if this last step is valid. Now, if we compute $(\Delta f) \circ A$ , we get \begin{align} ((\Delta f)\circ A)(x) &= (\Delta f)(Ax)\\  &= \left( \sum_{j=1}^d \frac{\partial^2f}{\partial x_j^2} \right) (z_1,....z_d)\\ &= \sum_{j=1}^d \frac{\partial^2f(z_1,...,z_d)}{\partial x_j^2}, \end{align} and since we already computed the first and second partial derivative of $f$ , we get that $\Delta(f \circ A) = (\Delta f) \circ A$ . I have two problems: I haven't used the fact that $A$ is orthogonal. I'm unsure whether or not I'm using the definition of the chain rule correctly. Any help is appreciated!","Let be the space of orthogonal matrices over . These matrices act on , the set of twice continuously differentiable functions with compact support, via , where . I am trying to show that for all and for all , we have . Here, denotes the Laplace operator. I am aware of the hint in the answer given to this related question here , however I am unable to complete the proof. Further hints on how exactly to apply the chain rule, as well as a full proof would be very much appreciated. Thanks! Edit : Until now, I have the following: Let , . Since , we have where is the Kronecker Delta. We thus have We denote . With the chain rule we get that In the next step we get but I'm not sure if this last step is valid. Now, if we compute , we get and since we already computed the first and second partial derivative of , we get that . I have two problems: I haven't used the fact that is orthogonal. I'm unsure whether or not I'm using the definition of the chain rule correctly. Any help is appreciated!","O(d) d \times d \mathbb{R} C_c^2(\mathbb{R}^d) f : \mathbb{R}^d \to \mathbb{R} (f \circ A)(x) := f(Ax) A \in O(d) f \in C_c^2(\mathbb{R}^d)  A \in O(d) \Delta(f \circ A) = (\Delta f)\circ A \Delta A := (a_{ij}) x := (x_1,...,x_d) AA^\top = I  \sum_{j=1}^d a_{ij}a_{kj} = \delta_{ik},  \delta_{ik} (f \circ A)(x) = f(Ax) = f \left( \sum_{i=1}^da_{1i}x_i,..., \sum_{i=1}^da_{di}x_i\right). z_i = g_i(x_1,...,x_d) = \sum_{k=1}^da_{ik}x_k  \frac{\partial f}{\partial x_j} = \sum_{i=1}^d  \frac{\partial f}{\partial z_i} \frac{\partial z_i}{\partial x_j}, \begin{align} 
\frac{\partial}{\partial x_j}(f \circ A)(x) &= \frac{\partial}{\partial x_j}f(z_1,...,z_d)\\
&= \sum_{i=1}^d \frac{\partial f(z_1,...,z_d)}{\partial z_i} \frac{\partial g_i(x_1,...,x_d)}{\partial x_j}\\
&= \sum_{i=1}^d a_{ij} \frac{\partial f(z_1,...,z_d)}{\partial z_i}.
\end{align} \begin{align}
\frac{\partial^2}{\partial x_j^2}(f \circ A)(x) = \frac{\partial}{\partial x_j} \left( \sum_{i=1}^d a_{ij} \frac{\partial f(z_1,...,z_d)}{\partial z_i} \right) = \sum_{i=1}^d a_{ij} \frac{\partial^2 f(z_1,...,z_d)}{\partial x_j \partial z_i},
\end{align} (\Delta f) \circ A \begin{align}
((\Delta f)\circ A)(x) &= (\Delta f)(Ax)\\ 
&= \left( \sum_{j=1}^d \frac{\partial^2f}{\partial x_j^2} \right) (z_1,....z_d)\\
&= \sum_{j=1}^d \frac{\partial^2f(z_1,...,z_d)}{\partial x_j^2},
\end{align} f \Delta(f \circ A) = (\Delta f) \circ A A","['real-analysis', 'linear-algebra', 'multivariable-calculus', 'orthogonality', 'laplacian']"
64,Find $f\in C^{1}$ such that $x\partial_{x}f+y\partial_{y}f=(x^2+y^2)^{1/2}$,Find  such that,f\in C^{1} x\partial_{x}f+y\partial_{y}f=(x^2+y^2)^{1/2},"In a problem I am looking to find a function $C^{1}$ , $f:\mathbb{R}^{2}\to\mathbb{R}$ such that \begin{equation*}     x\frac{\partial f}{\partial x}(x,y)+y\frac{\partial f}{\partial y}(x,y)=(x^2+y^2)^{1/2} \end{equation*} I tried doing the change of variables \begin{eqnarray*}     x=u\text{ and }y=uv \end{eqnarray*} getting \begin{eqnarray*}     u\frac{\partial f}{\partial u}\frac{\partial u}{\partial x}+uv\frac{\partial f}{\partial u}\frac{\partial u}{\partial x} & = & \sqrt{u^{2}+u^{2}v^{2}}\\     \Rightarrow u\frac{\partial f}{\partial u} + u\frac{\partial f}{\partial u} & = & \sqrt{u^{2}+u^{2}v^{2}}\\     \Rightarrow 2u\frac{\partial f}{\partial u} & = & |u|\sqrt{1+v^{2}}\\ \end{eqnarray*} But from this point I don't know how to move forward, I tried to integrate u which led me to a resolution by trigonometric substitution (giving something of the form $f(x,y)=\frac{1}{2}\sec(\theta(x,y))+C$ ) I would appreciate any ideas or indications on how to proceed.","In a problem I am looking to find a function , such that I tried doing the change of variables getting But from this point I don't know how to move forward, I tried to integrate u which led me to a resolution by trigonometric substitution (giving something of the form ) I would appreciate any ideas or indications on how to proceed.","C^{1} f:\mathbb{R}^{2}\to\mathbb{R} \begin{equation*}
    x\frac{\partial f}{\partial x}(x,y)+y\frac{\partial f}{\partial y}(x,y)=(x^2+y^2)^{1/2}
\end{equation*} \begin{eqnarray*}
    x=u\text{ and }y=uv
\end{eqnarray*} \begin{eqnarray*}
    u\frac{\partial f}{\partial u}\frac{\partial u}{\partial x}+uv\frac{\partial f}{\partial u}\frac{\partial u}{\partial x} & = & \sqrt{u^{2}+u^{2}v^{2}}\\
    \Rightarrow u\frac{\partial f}{\partial u} + u\frac{\partial f}{\partial u} & = & \sqrt{u^{2}+u^{2}v^{2}}\\
    \Rightarrow 2u\frac{\partial f}{\partial u} & = & |u|\sqrt{1+v^{2}}\\
\end{eqnarray*} f(x,y)=\frac{1}{2}\sec(\theta(x,y))+C","['real-analysis', 'analysis', 'multivariable-calculus', 'partial-differential-equations', 'continuity']"
65,Prove that there exists no scalar field which has derivative $> 0$ at a fixed point wrt every direction,Prove that there exists no scalar field which has derivative  at a fixed point wrt every direction,> 0,The problem statement : Prove that there is no scalar field $f$ such that $\nabla_\vec{y}f(\vec{a})>0$ for a fixed vector $\vec{a}$ and every non-zero vector $\vec{y}$ . Here $\nabla_\vec{y}f(\vec{a})$ represents the derivative of $f$ at $\vec{a}$ with respect to $\vec{y}$ . $$ \nabla_\vec{y}f(\vec{a})=\lim_{h\rightarrow 0}\frac{f(\vec{a}+h\vec{y})-f(\vec{a})}{h}$$ I proved it using that if $\vec{y}\neq0$ then $$\nabla_\vec{-y}f(\vec{a})=-\nabla_\vec{y}f(\vec{a})<0$$ This gives a contradiction. Is there any other way to prove this ?,The problem statement : Prove that there is no scalar field such that for a fixed vector and every non-zero vector . Here represents the derivative of at with respect to . I proved it using that if then This gives a contradiction. Is there any other way to prove this ?,f \nabla_\vec{y}f(\vec{a})>0 \vec{a} \vec{y} \nabla_\vec{y}f(\vec{a}) f \vec{a} \vec{y}  \nabla_\vec{y}f(\vec{a})=\lim_{h\rightarrow 0}\frac{f(\vec{a}+h\vec{y})-f(\vec{a})}{h} \vec{y}\neq0 \nabla_\vec{-y}f(\vec{a})=-\nabla_\vec{y}f(\vec{a})<0,"['real-analysis', 'multivariable-calculus', 'derivatives', 'scalar-fields']"
66,Is $\mathbb{Q} \times \mathbb{Q}$ a $G_\delta$ set?,Is  a  set?,\mathbb{Q} \times \mathbb{Q} G_\delta,"I can prove that $\mathbb{Q}$ is not a $G_\delta$ set in $\mathbb{R}$ . I was applying the same Baire space argument to show that $\mathbb{Q} \times \mathbb{Q}$ is not a $G_\delta$ set. I was thinking like this: We can write $(\mathbb{Q} \times \mathbb{Q})^c= \displaystyle{\bigcap_{(p,q)\in \mathbb{Q} \times \mathbb{Q}}\{(p,q)\}^c}= \displaystyle{\bigcap_{n=1}^{\infty}V_n}$ where each $V_n$ is open in $\mathbb{R}^2$ . If we can write $\mathbb{Q} \times \mathbb{Q}= \displaystyle{\bigcap_{n=1}^{\infty}U_n}$ where each $U_n$ is open in $\mathbb{R}^2$ . Then $\displaystyle{\bigcap_{n=1}^{\infty}(U_n \cap V_n)}= \phi$ is dense in $\mathbb{R}^2$ -----which is a contradiction.! Am I thinking correctly ? Please help. Thanks in advance.",I can prove that is not a set in . I was applying the same Baire space argument to show that is not a set. I was thinking like this: We can write where each is open in . If we can write where each is open in . Then is dense in -----which is a contradiction.! Am I thinking correctly ? Please help. Thanks in advance.,"\mathbb{Q} G_\delta \mathbb{R} \mathbb{Q} \times \mathbb{Q} G_\delta (\mathbb{Q} \times \mathbb{Q})^c= \displaystyle{\bigcap_{(p,q)\in \mathbb{Q} \times \mathbb{Q}}\{(p,q)\}^c}= \displaystyle{\bigcap_{n=1}^{\infty}V_n} V_n \mathbb{R}^2 \mathbb{Q} \times \mathbb{Q}= \displaystyle{\bigcap_{n=1}^{\infty}U_n} U_n \mathbb{R}^2 \displaystyle{\bigcap_{n=1}^{\infty}(U_n \cap V_n)}= \phi \mathbb{R}^2","['general-topology', 'functional-analysis', 'multivariable-calculus', 'the-baire-space']"
67,What is $\mathbb{R}^{n+1}-\mathbb{R}^n$? [duplicate],What is ? [duplicate],\mathbb{R}^{n+1}-\mathbb{R}^n,"This question already has answers here : Why is $\mathbb{R}^2$ not a subset and /or a subspace of $\mathbb{R}^3$? (2 answers) Are the reals $\mathbb{R}$ a subset of $\mathbb{R}^2$? (2 answers) Are the reals genuinely a subset of the complex numbers? [duplicate] (8 answers) Closed 3 years ago . In C.H. Edwards's Advanced Calculus of Several Variables he defines the ordinate set $\mathcal{O}_f$ of a function $f:\mathbb{R}^n\to\mathbb{R}$ as the set of points between $\mathbb{R}^n$ and the graph of $f,$ including the points of evaluation, $\mathbf{x}\in\mathbb{R}^n$ and the points in the graph $\left\{\mathbf{x},f\left(\mathbf{x}\right)\right\}\in\mathbb{R}^{n+1}$ .  Later on he defines a set $\hat{\mathcal{G}}=\partial\mathcal{O}_f-\mathbb{R}^n,$ where $\partial\mathcal{O}_f$ is the boundary of $\mathcal{O}_f.$ The intent seems clear.  First $$\mathbb{R}^{n+1}-\mathbb{R}^n=\mathbb{R}^n\times\left(\mathbb{R}-\left\{0\right\}\right)$$ where $\times$ means Cartesian product.  Then $$\hat{\mathcal{G}}=\left(\mathbb{R}^{n+1}-\mathbb{R}^n\right)\cap\partial\mathcal{O}_f.$$ But long ago I learned that $\mathbb{R}^n$ is the set of all real number n-tuples, and $\mathbb{R}^{n+1}$ is the set of all (n+1)-tuples, so elements of $\mathbb{R}^{n}$ are not elements of $\mathbb{R}^{n+1}$ and $\mathbb{R}^{n}$ is not a subset of $\mathbb{R}^{n+1}.$ So am I correct in concluding that $\mathbb{R}^{n+1}-\mathbb{R}^n$ is not really the relative complement of the two sets?","This question already has answers here : Why is $\mathbb{R}^2$ not a subset and /or a subspace of $\mathbb{R}^3$? (2 answers) Are the reals $\mathbb{R}$ a subset of $\mathbb{R}^2$? (2 answers) Are the reals genuinely a subset of the complex numbers? [duplicate] (8 answers) Closed 3 years ago . In C.H. Edwards's Advanced Calculus of Several Variables he defines the ordinate set of a function as the set of points between and the graph of including the points of evaluation, and the points in the graph .  Later on he defines a set where is the boundary of The intent seems clear.  First where means Cartesian product.  Then But long ago I learned that is the set of all real number n-tuples, and is the set of all (n+1)-tuples, so elements of are not elements of and is not a subset of So am I correct in concluding that is not really the relative complement of the two sets?","\mathcal{O}_f f:\mathbb{R}^n\to\mathbb{R} \mathbb{R}^n f, \mathbf{x}\in\mathbb{R}^n \left\{\mathbf{x},f\left(\mathbf{x}\right)\right\}\in\mathbb{R}^{n+1} \hat{\mathcal{G}}=\partial\mathcal{O}_f-\mathbb{R}^n, \partial\mathcal{O}_f \mathcal{O}_f. \mathbb{R}^{n+1}-\mathbb{R}^n=\mathbb{R}^n\times\left(\mathbb{R}-\left\{0\right\}\right) \times \hat{\mathcal{G}}=\left(\mathbb{R}^{n+1}-\mathbb{R}^n\right)\cap\partial\mathcal{O}_f. \mathbb{R}^n \mathbb{R}^{n+1} \mathbb{R}^{n} \mathbb{R}^{n+1} \mathbb{R}^{n} \mathbb{R}^{n+1}. \mathbb{R}^{n+1}-\mathbb{R}^n","['multivariable-calculus', 'elementary-set-theory', 'vector-spaces']"
68,"Find the flux for the vector field $F =(x^2,y^2,z^2)$",Find the flux for the vector field,"F =(x^2,y^2,z^2)","Find the flux for the vector field $\vec{F}=(x^2,y^2,z^2)$ across the boundary to the ball given as $$(x-1)^2 +(y+1)^2 +(z-2)^2 \le 4$$ Edit: How much I've done: The center of the sphere is $(1,−1,2)$ (I've sketched it), thus $r(θ,ϕ)=(1+cosθsinϕ, −1+sinθsinϕ, 2+cosϕ)$ . Then to find the normal vector $\vec{N}$ , I've calculated $\frac{∂r(θ,ϕ)}{∂ϕ}=(cosθcosϕ,sinθcosϕ,−sinϕ)$ and $\frac {∂r(θ,ϕ)}{∂θ}=(−sinθsinϕ,cosθsinϕ,0)$ then $\frac{∂r(θ,ϕ)}{∂ϕ} \times \frac{∂r(θ,ϕ)}{∂θ}$ , and finally got $(sin2ϕ,sinθsin2ϕ,sinϕcosϕ)$ . Am I on the right track?","Find the flux for the vector field across the boundary to the ball given as Edit: How much I've done: The center of the sphere is (I've sketched it), thus . Then to find the normal vector , I've calculated and then , and finally got . Am I on the right track?","\vec{F}=(x^2,y^2,z^2) (x-1)^2 +(y+1)^2 +(z-2)^2 \le 4 (1,−1,2) r(θ,ϕ)=(1+cosθsinϕ, −1+sinθsinϕ, 2+cosϕ) \vec{N} \frac{∂r(θ,ϕ)}{∂ϕ}=(cosθcosϕ,sinθcosϕ,−sinϕ) \frac {∂r(θ,ϕ)}{∂θ}=(−sinθsinϕ,cosθsinϕ,0) \frac{∂r(θ,ϕ)}{∂ϕ} \times \frac{∂r(θ,ϕ)}{∂θ} (sin2ϕ,sinθsin2ϕ,sinϕcosϕ)","['multivariable-calculus', 'vector-analysis', 'vector-fields', 'spheres']"
69,How would I use Green's Theorem to evaluate this Line Integral?,How would I use Green's Theorem to evaluate this Line Integral?,,"If I have the closed loop $C$ ( https://ibb.co/hK7xC5V ) which is the union of $C_1,C_2,C_3$ and $C_4$ where $$C_1=\sin(x)-2\pi,\text{where $x$ goes from 0 to $2\pi$}$$ $$C_2=-\sin(y)+2\pi,\text{where $y$ goes from $-2\pi$ to 0}$$ $$C_3=\sin(x),\text{where $x$ goes from $2\pi$ to 0}$$ $$C_4=-\sin(y),\text{where $y$ goes from 0 to $-2\pi$}$$ How would I evaluate $$\oint\limits_C\vec{F}\cdot d\vec{r}$$ using Green's Theorem? (An example would also be appreciated with any field $\vec{F}$ which has nonzero $x$ and $y$ components)",If I have the closed loop ( https://ibb.co/hK7xC5V ) which is the union of and where How would I evaluate using Green's Theorem? (An example would also be appreciated with any field which has nonzero and components),"C C_1,C_2,C_3 C_4 C_1=\sin(x)-2\pi,\text{where x goes from 0 to 2\pi} C_2=-\sin(y)+2\pi,\text{where y goes from -2\pi to 0} C_3=\sin(x),\text{where x goes from 2\pi to 0} C_4=-\sin(y),\text{where y goes from 0 to -2\pi} \oint\limits_C\vec{F}\cdot d\vec{r} \vec{F} x y","['multivariable-calculus', 'vector-analysis', 'vector-fields', 'line-integrals', 'greens-theorem']"
70,"Is the set made up of all points that satisfy $g_{1}(x, y, z)=x^2 + 2y^2=1 $, $\:g_{2}(x, y, z)= xy + xz=2$ compact?","Is the set made up of all points that satisfy ,  compact?","g_{1}(x, y, z)=x^2 + 2y^2=1  \:g_{2}(x, y, z)= xy + xz=2","I want to find the least value of a function whose domain is made up of all points that satisfy the following: $g_1(x, y, z)= x^2 + 2y^2=1$ $g_2(x, y, z)= xy + xz=2$ In my book it is written that ""a minimum exists since the domain is a closed set"". However, previously there is a theorem that one can only be certain that function obtains a max and min value if the domain is compact, i.e. both closed and bounded. From what I see, the domain is not bounded, how can we then be certain that ""a minimum exists""? Any help would be appreciated. $\textbf{Edit:}$ : I want to find the least distance form the origin, so my function is $f(x, y,z)=𝑥^2+𝑦^2+𝑧^2$","I want to find the least value of a function whose domain is made up of all points that satisfy the following: In my book it is written that ""a minimum exists since the domain is a closed set"". However, previously there is a theorem that one can only be certain that function obtains a max and min value if the domain is compact, i.e. both closed and bounded. From what I see, the domain is not bounded, how can we then be certain that ""a minimum exists""? Any help would be appreciated. : I want to find the least distance form the origin, so my function is","g_1(x, y, z)= x^2 + 2y^2=1 g_2(x, y, z)= xy + xz=2 \textbf{Edit:} f(x, y,z)=𝑥^2+𝑦^2+𝑧^2","['multivariable-calculus', 'optimization', 'compactness', 'maxima-minima']"
71,Find the gradient and hessian of $g(x)=f(Ax)$,Find the gradient and hessian of,g(x)=f(Ax),"Let $f(z):\mathbb R^m\rightarrow \mathbb R$ be a real-valued function from $\mathbb R^m$ to $\mathbb R$ . Let $A^{m\times n}, x^{n\times 1}$ , and let $g(x)=f(Ax)$ . Find the gradient and hessian of g(x) in terms of A, $\nabla f(x)$ , and the hessian $H(f(x))$ I tried using the chain rule $\underbrace{\nabla g(x)}_{n\times 1}=\underbrace{A}_{m\times n} \underbrace{\nabla f(Ax)}_{n\times 1}$ But it seems as if the dimensions don't work out. Is the dimension of $\nabla g(x)$ actually mx1? I thought it should be nx1 because there are n  elements of x. If that is the case, my application of chain rule is probably wrong.","Let be a real-valued function from to . Let , and let . Find the gradient and hessian of g(x) in terms of A, , and the hessian I tried using the chain rule But it seems as if the dimensions don't work out. Is the dimension of actually mx1? I thought it should be nx1 because there are n  elements of x. If that is the case, my application of chain rule is probably wrong.","f(z):\mathbb R^m\rightarrow \mathbb R \mathbb R^m \mathbb R A^{m\times n}, x^{n\times 1} g(x)=f(Ax) \nabla f(x) H(f(x)) \underbrace{\nabla g(x)}_{n\times 1}=\underbrace{A}_{m\times n} \underbrace{\nabla f(Ax)}_{n\times 1} \nabla g(x)","['multivariable-calculus', 'chain-rule']"
72,"How do we apply chain rule to a function of two variables, which are also functions.","How do we apply chain rule to a function of two variables, which are also functions.",,"Let $f$ and $g$ be differentiable functions. $$ \frac{\partial}{\partial z}(g(f(x+y),f(z)) $$ How do we apply the chain rule here correctly? Do we just treat $f(x+y)$ and $f(z)$ as some variables $u$ and $v$ ?",Let and be differentiable functions. How do we apply the chain rule here correctly? Do we just treat and as some variables and ?,"f g 
\frac{\partial}{\partial z}(g(f(x+y),f(z))
 f(x+y) f(z) u v","['multivariable-calculus', 'derivatives']"
73,"Calculating $\lim_{(x,y)\rightarrow (0,0)}\frac{1 - \cos(\pi x y ) + \sin (\pi(x^2 + y^2))}{x^2 + y^2}$ if it exists",Calculating  if it exists,"\lim_{(x,y)\rightarrow (0,0)}\frac{1 - \cos(\pi x y ) + \sin (\pi(x^2 + y^2))}{x^2 + y^2}","I'm trying to calculate the above limit. I ran a few paths and found out that the limit is $\pi$ (which I also confirmed through WolframAlpha), but to prove it I use polar coordinates to get an expression of the form $F(r) \cdot G(\theta)$ where $F(r)\rightarrow$ when $r\rightarrow 0$ and $G$ is blocked. So far I have: $$\lim_{r\rightarrow 0} \left|f(r\cos \theta,r \sin \theta)\right| = \lim \left|\frac{1-\cos(\pi r^2 \cos\theta \sin \theta ) + \sin (\pi r^2 (\sin^2\theta  + \cos ^2 \theta ))}{r^2\cos ^2 \theta + r^2 \sin^2 \theta}\right| \\ \underset{\sin ^2 \theta + \cos ^2 \theta = 1}{=} \lim \left|\frac{1 - \cos(\pi r^2 \cos\theta \sin\theta ) + \sin (\pi r^2 )}{r^2}\right|$$ However, I don't know how to separate $r$ from the rest of the expression at this stage. But it occurred to me that this only works if the limit of the function is zero. So, how do I confirm this is the correct limit in a case such as this?","I'm trying to calculate the above limit. I ran a few paths and found out that the limit is (which I also confirmed through WolframAlpha), but to prove it I use polar coordinates to get an expression of the form where when and is blocked. So far I have: However, I don't know how to separate from the rest of the expression at this stage. But it occurred to me that this only works if the limit of the function is zero. So, how do I confirm this is the correct limit in a case such as this?","\pi F(r) \cdot G(\theta) F(r)\rightarrow r\rightarrow 0 G \lim_{r\rightarrow 0} \left|f(r\cos \theta,r \sin \theta)\right| = \lim \left|\frac{1-\cos(\pi r^2 \cos\theta \sin \theta ) + \sin (\pi r^2 (\sin^2\theta  + \cos ^2 \theta ))}{r^2\cos ^2 \theta + r^2 \sin^2 \theta}\right|
\\ \underset{\sin ^2 \theta + \cos ^2 \theta = 1}{=} \lim \left|\frac{1 - \cos(\pi r^2 \cos\theta \sin\theta ) + \sin (\pi r^2 )}{r^2}\right| r","['real-analysis', 'limits', 'multivariable-calculus', 'polar-coordinates']"
74,Why velocity and accelleration must be linearly independent?,Why velocity and accelleration must be linearly independent?,,"I am struggling to understand the definition of biregular curve. Definition A curve $I \ni t \mapsto P(t) \in \mathbb{R}^3$ is said to be biregular when the velocity and accelleration vectors are linearly independent, that is when their cross product is different from zero $$dP/dt \land d^2P/d^2t \neq 0$$ Q. I read that this is equivalent to the fact that $$d \underline{t}/ds \neq 0$$ and this is what I am having trouble understanding. Why is so? Recall that $$s= \int_0^t \|dP/dt\| \, dt,$$ is the curvilinean coordinate, and $$\underline{t}(s)=dP/ds$$ so that $$dP/dt= dP/ds \  \dot{s}= \underline{t} \ \dot{s}$$ where $$\dot{s}=ds/dt.$$ Moreover $$d^2P/d^2t= d\underline{t}/ds \ \dot{s}^2 + \underline{t} \ \ddot{s}.$$ Hence we have $$dP/dt \land d^2P/d^2t = \underline{t} \ \dot{s} \land (d\underline{t}/ds \ \dot{s}^2 + \underline{t} \ \ddot{s})$$ $$ = \dot{s}^3(\underline{t}\land d\underline{t}/ds)+\ddot{s}(\underline{t}\land \underline{t})=\dot{s}^3(\underline{t}\land d\underline{t}/ds).$$ So, the curve is biregular $\iff$ $\dot{s}=\|dP/dt\|\neq 0$ and $\underline{t}\land d\underline{t}/ds \neq 0.$ How is this equivalent to the fact that $d \underline{t}/ds \neq 0$ ? Why is it not possible that $d\underline{t}/ds$ and $\underline{t}$ are both different from zero and yet linearly dependent?","I am struggling to understand the definition of biregular curve. Definition A curve is said to be biregular when the velocity and accelleration vectors are linearly independent, that is when their cross product is different from zero Q. I read that this is equivalent to the fact that and this is what I am having trouble understanding. Why is so? Recall that is the curvilinean coordinate, and so that where Moreover Hence we have So, the curve is biregular and How is this equivalent to the fact that ? Why is it not possible that and are both different from zero and yet linearly dependent?","I \ni t \mapsto P(t) \in \mathbb{R}^3 dP/dt \land d^2P/d^2t \neq 0 d \underline{t}/ds \neq 0 s= \int_0^t \|dP/dt\| \, dt, \underline{t}(s)=dP/ds dP/dt= dP/ds \  \dot{s}= \underline{t} \ \dot{s} \dot{s}=ds/dt. d^2P/d^2t= d\underline{t}/ds \ \dot{s}^2 + \underline{t} \ \ddot{s}. dP/dt \land d^2P/d^2t = \underline{t} \ \dot{s} \land (d\underline{t}/ds \ \dot{s}^2 + \underline{t} \ \ddot{s})  = \dot{s}^3(\underline{t}\land d\underline{t}/ds)+\ddot{s}(\underline{t}\land \underline{t})=\dot{s}^3(\underline{t}\land d\underline{t}/ds). \iff \dot{s}=\|dP/dt\|\neq 0 \underline{t}\land d\underline{t}/ds \neq 0. d \underline{t}/ds \neq 0 d\underline{t}/ds \underline{t}","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'differential-topology', 'curves']"
75,Derivative of a linear vector function by matrix - 3d-Jacobian or a vector?,Derivative of a linear vector function by matrix - 3d-Jacobian or a vector?,,"If I differentiate $$ Y =  \pmatrix{a & b \\ c & d} \pmatrix {x \\ y} $$ with respect to the matrix on the left side, what do I get? On one hand, I think I should get $\pmatrix{x \\ y}$ because it's a linear function. On the other hand, I think I should get a 3-dimensional Jacobian matrix (if it's a real thing) where the each 4 entries are 2-d vectors $$ {\partial Y \over \partial a} = \pmatrix{x \\ 0}, {\partial Y \over \partial b} = \pmatrix{y \\ 0} \\ {\partial Y \over \partial c} = \pmatrix{0 \\ x}, {\partial Y \over \partial d} = \pmatrix{0 \\ y} \\ $$ Which one is right?","If I differentiate with respect to the matrix on the left side, what do I get? On one hand, I think I should get because it's a linear function. On the other hand, I think I should get a 3-dimensional Jacobian matrix (if it's a real thing) where the each 4 entries are 2-d vectors Which one is right?","
Y = 
\pmatrix{a & b \\ c & d}
\pmatrix {x \\ y}
 \pmatrix{x \\ y} 
{\partial Y \over \partial a} = \pmatrix{x \\ 0},
{\partial Y \over \partial b} = \pmatrix{y \\ 0} \\
{\partial Y \over \partial c} = \pmatrix{0 \\ x},
{\partial Y \over \partial d} = \pmatrix{0 \\ y} \\
","['calculus', 'multivariable-calculus', 'jacobian']"
76,"Computing the flows $\theta, \Psi$ of $X$ and $Y,$ and verifying that the flows do not commute.",Computing the flows  of  and  and verifying that the flows do not commute.,"\theta, \Psi X Y,","This is Problem 9-18 from Professor Lee's Intro to Smooth Manifolds (2nd edition). I believe I have most of the proof done, however, I am unsure how to finish it from where I currently am at. Can someone please help me? Thank you so much! Define vector fields $X$ and $Y$ on the plane by $$X = x\frac{\partial}{\partial x} - y\frac{\partial}{\partial y}, \hspace{20 pt} Y = x\frac{\partial}{\partial y} + y \frac{\partial}{\partial x}.$$ Compute the flows $\theta, \Psi$ of $X$ and $Y,$ and verify that the flows do not commute by finding explicit open intervals $J$ and $K$ containing $0$ such that $\theta_s \circ \Psi_t$ and $\Psi_t \circ \theta_s$ are both defined for all $(s,t) \in J\times K,$ but they are unequal for some such $(s,t).$ $\textit{Proof.}$ Recall that the flow is a family of integral curves for a specific vector field. Let $\gamma(t) = (x(t),y(t))$ be the candidate for the flow. We must solve $$X(\gamma(t)) = \gamma'(t).$$ We need to delineate $\gamma'(t).$ It must be an element of the tangent space at $\gamma(t).$ So, now we verify that. $$\gamma'(t) = x'(t)\partial_{x(t)} + y'(t)\partial_{y(t)}.$$ On the other side we have $X(\gamma(t)) = x(t)\partial_{x(t)} -y(t)\partial_{y(t)}.$ Once $X(\gamma(t)) = \gamma'(t)$ it assigns us $$x'(t) = x(t), y'(t) = -y(t).$$ These equations have the solutions $$x(t) = a\cos t = b\sin t, \hspace{20 pt} y(t) = a\sin t + b \cos t,$$ where $a,b$ are arbitrary constants.","This is Problem 9-18 from Professor Lee's Intro to Smooth Manifolds (2nd edition). I believe I have most of the proof done, however, I am unsure how to finish it from where I currently am at. Can someone please help me? Thank you so much! Define vector fields and on the plane by Compute the flows of and and verify that the flows do not commute by finding explicit open intervals and containing such that and are both defined for all but they are unequal for some such Recall that the flow is a family of integral curves for a specific vector field. Let be the candidate for the flow. We must solve We need to delineate It must be an element of the tangent space at So, now we verify that. On the other side we have Once it assigns us These equations have the solutions where are arbitrary constants.","X Y X = x\frac{\partial}{\partial x} - y\frac{\partial}{\partial y}, \hspace{20 pt} Y = x\frac{\partial}{\partial y} + y \frac{\partial}{\partial x}. \theta, \Psi X Y, J K 0 \theta_s \circ \Psi_t \Psi_t \circ \theta_s (s,t) \in J\times K, (s,t). \textit{Proof.} \gamma(t) = (x(t),y(t)) X(\gamma(t)) = \gamma'(t). \gamma'(t). \gamma(t). \gamma'(t) = x'(t)\partial_{x(t)} + y'(t)\partial_{y(t)}. X(\gamma(t)) = x(t)\partial_{x(t)} -y(t)\partial_{y(t)}. X(\gamma(t)) = \gamma'(t) x'(t) = x(t), y'(t) = -y(t). x(t) = a\cos t = b\sin t, \hspace{20 pt} y(t) = a\sin t + b \cos t, a,b","['multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'vector-fields']"
77,Limit by polar coordinates,Limit by polar coordinates,,"Hi I need some help with this limit: $$ \lim_{(x,y) \to (0,0)}  \frac{x^4y}{x^2+(x+y)^2}$$ I used polar coordinates and got that $$ \frac{x^4y}{x^2+(x+y)^2}= \frac{r^3\cdot(\cos φ)^4  \sin φ } {1+ (\cosφ)^2+\sin(2φ)} $$ I didn't know how to proceed from this.",Hi I need some help with this limit: I used polar coordinates and got that I didn't know how to proceed from this.,"
\lim_{(x,y) \to (0,0)}  \frac{x^4y}{x^2+(x+y)^2} 
\frac{x^4y}{x^2+(x+y)^2}= \frac{r^3\cdot(\cos φ)^4  \sin φ } {1+ (\cosφ)^2+\sin(2φ)}
","['limits', 'multivariable-calculus']"
78,Find $\int_0^2 \int_0^{\sqrt{3}x} f(\sqrt{x^2+y^2})dydx$ in polar coordinates.,Find  in polar coordinates.,\int_0^2 \int_0^{\sqrt{3}x} f(\sqrt{x^2+y^2})dydx,"I need to find: $$\int_0^2 \int_0^{\sqrt{3}x} f(\sqrt{x^2+y^2})dydx$$ in polar coordinates. Since $x=r\cos(\theta)$ and $y=r\sin(\theta)$ , I got: $y=\sqrt{3}x \iff r\sin(\theta)=\sqrt{3}r\cos(\theta)\iff \tan(\theta)=\sqrt{3}\iff \theta=\arctan(\sqrt{3})$ From this I conclude that $0 \leq \theta \leq \arctan(\sqrt{3})$ . My problem is I'm not sure where the radius is, I thought about from $0$ to $2\sqrt{3}$ (from graphing $y=\sqrt{3}$ ) but I think it is wrong. Is there something I'm missing in order to get the $r$ interval?","I need to find: in polar coordinates. Since and , I got: From this I conclude that . My problem is I'm not sure where the radius is, I thought about from to (from graphing ) but I think it is wrong. Is there something I'm missing in order to get the interval?",\int_0^2 \int_0^{\sqrt{3}x} f(\sqrt{x^2+y^2})dydx x=r\cos(\theta) y=r\sin(\theta) y=\sqrt{3}x \iff r\sin(\theta)=\sqrt{3}r\cos(\theta)\iff \tan(\theta)=\sqrt{3}\iff \theta=\arctan(\sqrt{3}) 0 \leq \theta \leq \arctan(\sqrt{3}) 0 2\sqrt{3} y=\sqrt{3} r,"['multivariable-calculus', 'multiple-integral']"
79,Frenet-Serret formula: why is $T$'s magnitude unitary?,Frenet-Serret formula: why is 's magnitude unitary?,T,Why is $T$ 's tangent vector magnitude unitary? $$T=\frac{dr}{ds}$$,Why is 's tangent vector magnitude unitary?,T T=\frac{dr}{ds},"['calculus', 'multivariable-calculus', 'differential-geometry', 'frenet-frame']"
80,On derivatives of the inverse of a real function,On derivatives of the inverse of a real function,,"Take two open intervals $I,J\subseteq \mathbb{R}$ and a bijection $f:I\rightarrow J$ with inverse $g:J\rightarrow I$ . For every $k \in \mathbb{N}, k \geq 1$ , I know that  if $f \in C^k(I,J)$ and for all $t \in I$ , $f'(t) \neq 0 $ then $g \in C^k(J,I)$ , but I don't know how to prove it. The way I thought I would go about this is: I find the general form of the $k$ -th derivative of $g$ I realize that $g^{(k)}$ is the composition of continuous functions and is therefore continuous The trouble is, the general form of $g^{(k)}$ seems to get very complicated, so I was wondering if there is any other way to show this in a simpler way (most likely using induction). My final aim is to prove that $f$ is smooth if and only if $g$ is. On a related sidenote, I seem to recall that the result is still true if we take $I,J\subseteq \mathbb{R}^n$ open and simply connected (with the jacobian of $f$ being invertible for all $x \in I$ ). I would just like to know if this is true out of curiousity. Edit: I forgot an ipotesis on the first derivative being not $0$ on all of $I$ , added it.","Take two open intervals and a bijection with inverse . For every , I know that  if and for all , then , but I don't know how to prove it. The way I thought I would go about this is: I find the general form of the -th derivative of I realize that is the composition of continuous functions and is therefore continuous The trouble is, the general form of seems to get very complicated, so I was wondering if there is any other way to show this in a simpler way (most likely using induction). My final aim is to prove that is smooth if and only if is. On a related sidenote, I seem to recall that the result is still true if we take open and simply connected (with the jacobian of being invertible for all ). I would just like to know if this is true out of curiousity. Edit: I forgot an ipotesis on the first derivative being not on all of , added it.","I,J\subseteq \mathbb{R} f:I\rightarrow J g:J\rightarrow I k \in \mathbb{N}, k \geq 1 f \in C^k(I,J) t \in I f'(t) \neq 0  g \in C^k(J,I) k g g^{(k)} g^{(k)} f g I,J\subseteq \mathbb{R}^n f x \in I 0 I","['real-analysis', 'multivariable-calculus']"
81,Doubt about the existence of the gradient.,Doubt about the existence of the gradient.,,"Consider $$ f(x,y) = \begin{cases} \frac{xy}{x^2+y^2} & , \text{if } \ (x,y)\neq(0,0) \\ \hspace{0.5cm}0 &, \text{if } \ (x,y)=(0,0) \end{cases} $$ Computing the limit at zero $$ \left\{  \begin{array}{c} x=rcos\theta \\  y=rsin\theta \\  \end{array} \right. $$ $$ (x,y) \to (0,0) \Longleftrightarrow r \to 0 $$ $$ \lim_{ r \to \ 0 }  \frac{ r^2cos\theta sin\theta }{ r^2( cos^2 \theta + sin^2 \theta) } = cos \theta sin \theta   $$ Then the limit does not exist. Hence this function is not continuos at zero. $$ \nabla f(x,y) = \begin{cases} \left(  \frac{y(y^2-x^2)}{(x^2+y^2)^2} , \frac{x(x^2-y^2)}{(x^2+y^2)^2} \right) & , \text{if } \ (x,y)\neq(0,0) \\ \hspace{0.5cm} (0,0) &, \text{if } \ (x,y)=(0,0) \end{cases} $$ So it doesn't matter that the function is not continuous at zero? or this function does not differentiable at zero?",Consider Computing the limit at zero Then the limit does not exist. Hence this function is not continuos at zero. So it doesn't matter that the function is not continuous at zero? or this function does not differentiable at zero?," f(x,y) = \begin{cases} \frac{xy}{x^2+y^2} & , \text{if } \ (x,y)\neq(0,0) \\
\hspace{0.5cm}0 &, \text{if } \ (x,y)=(0,0) \end{cases}  
\left\{ 
\begin{array}{c}
x=rcos\theta \\ 
y=rsin\theta \\ 
\end{array}
\right.
  (x,y) \to (0,0) \Longleftrightarrow r \to 0   \lim_{ r \to \ 0 }  \frac{ r^2cos\theta sin\theta }{ r^2( cos^2 \theta + sin^2 \theta) } = cos \theta sin \theta     \nabla f(x,y) = \begin{cases} \left(  \frac{y(y^2-x^2)}{(x^2+y^2)^2} , \frac{x(x^2-y^2)}{(x^2+y^2)^2} \right) & , \text{if } \ (x,y)\neq(0,0) \\
\hspace{0.5cm} (0,0) &, \text{if } \ (x,y)=(0,0) \end{cases} ","['multivariable-calculus', 'derivatives', 'continuity', 'vector-analysis']"
82,Problem 6 chapter 3 from Evans PDE 2nd edition,Problem 6 chapter 3 from Evans PDE 2nd edition,,"I am working on the following problem I have solved (a) but I'm struggling with (b). This is what I've done so far: First I modified the given equation: $$u_t +div(u\mathbb{b})=u_t+Du\cdot\mathbb{b}+u\,div(\mathbb{b})=0.$$ Then I tried to solve it with the method of characteristics and got the following ODEs $$\dot{\mathbb{x}}=\mathbb{b},\quad \dot z=-div (\mathbb{b})\, z,\quad \dot t=1$$ with initial conditions $$\mathbb{x}(0)=a,\quad z(0)=g(a),\quad t(0)=0.$$ From these I then solve $z$ and get $$z=g(a)e^{-div(\mathbb{b})}.$$ I don't know how to proceed (or if I've made any mistakes) and I have no idea how to use the hint or part (a). Any help with this one? EDIT: I think my solution $z$ is incorrect, I didn't think about the fact that $\mathbb{b}$ depends on $\mathbb{x}$ . I haven't yet figured out the correct one.","I am working on the following problem I have solved (a) but I'm struggling with (b). This is what I've done so far: First I modified the given equation: Then I tried to solve it with the method of characteristics and got the following ODEs with initial conditions From these I then solve and get I don't know how to proceed (or if I've made any mistakes) and I have no idea how to use the hint or part (a). Any help with this one? EDIT: I think my solution is incorrect, I didn't think about the fact that depends on . I haven't yet figured out the correct one.","u_t +div(u\mathbb{b})=u_t+Du\cdot\mathbb{b}+u\,div(\mathbb{b})=0. \dot{\mathbb{x}}=\mathbb{b},\quad \dot z=-div (\mathbb{b})\, z,\quad \dot t=1 \mathbb{x}(0)=a,\quad z(0)=g(a),\quad t(0)=0. z z=g(a)e^{-div(\mathbb{b})}. z \mathbb{b} \mathbb{x}","['multivariable-calculus', 'partial-differential-equations']"
83,"Differential of $\langle f,g \rangle $, if $f,g: E\subset \mathbb{R}\to \mathbb{R}^{m}$ are differentiable functions.","Differential of , if  are differentiable functions.","\langle f,g \rangle  f,g: E\subset \mathbb{R}\to \mathbb{R}^{m}","If $f,g: E\subset \mathbb{R}\to \mathbb{R}^{m}$ are differentiable real functions. Prove that $$\frac{d}{dt}\left\langle f(t),g(t) \right\rangle=\left\langle f(t), \frac{d}{dt}g(t) \right\rangle +\left\langle \frac{d}{dt}f(t), g(t) \right\rangle$$ How can I prove that? I know that if $f,g: U\subset \mathbb{R}^{n}\to \mathbb{R}$ are differentiable functions, so $$d(fg)=fd(g)+gd(f)$$ Can I use it?","If are differentiable real functions. Prove that How can I prove that? I know that if are differentiable functions, so Can I use it?","f,g: E\subset \mathbb{R}\to \mathbb{R}^{m} \frac{d}{dt}\left\langle f(t),g(t) \right\rangle=\left\langle f(t), \frac{d}{dt}g(t) \right\rangle +\left\langle \frac{d}{dt}f(t), g(t) \right\rangle f,g: U\subset \mathbb{R}^{n}\to \mathbb{R} d(fg)=fd(g)+gd(f)",['real-analysis']
84,"Evaluating the surface integral $\iint_S {({x^2} + {y^2})} \,dS$ using spherical coordinates",Evaluating the surface integral  using spherical coordinates,"\iint_S {({x^2} + {y^2})} \,dS","For the integral $$\iint\limits_S {({x^2} + {y^2})} \,dS\quad,\,S:{x^2} + {y^2} + {z^2} = 2z$$ The correct answer is $${{8\pi } \over 3}$$ I used Spherical coordinate system, it turns to $$\int_0^{2\pi } {d\theta \int_0^{{\pi  \over 2}} {({r^2}{{\sin }^2}\varphi } )({r^2}\sin \varphi )\,d\varphi } ,r = 2\cos \varphi $$ Then use $r = 2\cos \varphi$ , it turns to $$32\pi \int_0^{{\pi  \over 2}} {{{\sin }^3}\varphi {{\cos }^4}\varphi \,d\varphi }  = {{64} \over {35}}\pi $$ Doesn't match the answer, I wonder where am I wrong.","For the integral The correct answer is I used Spherical coordinate system, it turns to Then use , it turns to Doesn't match the answer, I wonder where am I wrong.","\iint\limits_S {({x^2} + {y^2})} \,dS\quad,\,S:{x^2} + {y^2} + {z^2} = 2z {{8\pi } \over 3} \int_0^{2\pi } {d\theta \int_0^{{\pi  \over 2}} {({r^2}{{\sin }^2}\varphi } )({r^2}\sin \varphi )\,d\varphi } ,r = 2\cos \varphi  r = 2\cos \varphi 32\pi \int_0^{{\pi  \over 2}} {{{\sin }^3}\varphi {{\cos }^4}\varphi \,d\varphi }  = {{64} \over {35}}\pi ","['integration', 'multivariable-calculus', 'spherical-coordinates', 'surface-integrals', 'multiple-integral']"
85,Find surface area of part of cylinder.,Find surface area of part of cylinder.,,"I ran into trouble when I'm trying to find a surface area of parts of the cylinder $x^2+z^2=4$ bounded by another cylinder $x^2+y^2=4$ , I simply used a traditional way of double integral, change into polar coordinate calculate $$ \iint\limits_{x^2+y^2=4}   \sqrt{\left(\frac{\partial z}{\partial x}\right)^2+         \left(\frac{\partial z}{\partial y}\right)^2+1} \,dx\,dy  =  \int_0^{2\pi}\int_{0}^{2} \frac{2r}{\sqrt{4-(r\cos\theta)^2}} \,dr\,d\theta $$ and eventually this integral diverges. Could anyone tell me where I was wrong ?  thanks a lot.","I ran into trouble when I'm trying to find a surface area of parts of the cylinder bounded by another cylinder , I simply used a traditional way of double integral, change into polar coordinate calculate and eventually this integral diverges. Could anyone tell me where I was wrong ?  thanks a lot.","x^2+z^2=4 x^2+y^2=4 
\iint\limits_{x^2+y^2=4}
  \sqrt{\left(\frac{\partial z}{\partial x}\right)^2+
        \left(\frac{\partial z}{\partial y}\right)^2+1} \,dx\,dy
 = 
\int_0^{2\pi}\int_{0}^{2} \frac{2r}{\sqrt{4-(r\cos\theta)^2}} \,dr\,d\theta
","['multivariable-calculus', 'surface-integrals']"
86,For which $k$ does $(a+b+c)\left(\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\right)+k-3\ge \left(2+\frac k3\right)\cdot \frac{a+b+c}{\sqrt[3]{abc}}$ hold?,For which  does  hold?,k (a+b+c)\left(\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\right)+k-3\ge \left(2+\frac k3\right)\cdot \frac{a+b+c}{\sqrt[3]{abc}},"By generalizing this (1) and this (2) questions and performing some research $$(a+b+c)\left(\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\right)+k-3\ge \left(2+\frac k3\right)\cdot \frac{a+b+c}{\sqrt[3]{abc}},\hbox{ for }a,b,c>0$$ for all $0\le k<k_0\approx 11.108$ . The main goal was to prove the original inequality from (2), however, letting $a=x^3,\,b=y^3,\,c=z^3$ and clearing the denominator, the inequality becomes $$3 k x^3 y^3 z^3 + 3 \sum\limits_{sym}x^6 y^3 z^0 - \left(3+\frac k2\right)\sum\limits_{sym} x^5 y^2 z^2\ge 0\tag{1}$$ and I'm failing to apply Muirhead's inequality . The method from this answer works only for $k\le 3$ , and even with calculus I don't think that solving system of $3$ equations like $\frac{\partial}{\partial x}$ LHS(1) $=0$ : $$5 k x^3 y^2 z^2 - 9 k x y^3 z^3 + 2 k y^5 z^2 + 2 k y^2 z^5 - 18 x^4 y^3 - 18 x^4 z^3 + 30 x^3 y^2 z^2 - 9 x y^6 - 9 x z^6 + 12 y^5 z^2 + 12 y^2 z^5=0$$ may lead to something neat.) Any help is appreciated. Thanks. The question : what is $k_0$ .","By generalizing this (1) and this (2) questions and performing some research for all . The main goal was to prove the original inequality from (2), however, letting and clearing the denominator, the inequality becomes and I'm failing to apply Muirhead's inequality . The method from this answer works only for , and even with calculus I don't think that solving system of equations like LHS(1) : may lead to something neat.) Any help is appreciated. Thanks. The question : what is .","(a+b+c)\left(\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\right)+k-3\ge \left(2+\frac k3\right)\cdot \frac{a+b+c}{\sqrt[3]{abc}},\hbox{ for }a,b,c>0 0\le k<k_0\approx 11.108 a=x^3,\,b=y^3,\,c=z^3 3 k x^3 y^3 z^3 + 3 \sum\limits_{sym}x^6 y^3 z^0 - \left(3+\frac k2\right)\sum\limits_{sym} x^5 y^2 z^2\ge 0\tag{1} k\le 3 3 \frac{\partial}{\partial x} =0 5 k x^3 y^2 z^2 - 9 k x y^3 z^3 + 2 k y^5 z^2 + 2 k y^2 z^5 - 18 x^4 y^3 - 18 x^4 z^3 + 30 x^3 y^2 z^2 - 9 x y^6 - 9 x z^6 + 12 y^5 z^2 + 12 y^2 z^5=0 k_0","['multivariable-calculus', 'inequality', 'optimization', 'symmetric-polynomials', 'uvw']"
87,"(RESOLVED) Given $z = f (x, y)$ and $x = r \cos \theta $, $ y = r \sin \theta$ prove the following","(RESOLVED) Given  and ,  prove the following","z = f (x, y) x = r \cos \theta   y = r \sin \theta","Question: Let $z = f (x, y)$ and $x = r \cos \theta $ , $ y = r \sin \theta$ Show that $$\frac{\partial^2z}{\partial x^2} + \frac{\partial^2z}{\partial y^2} = \frac{\partial^2z}{\partial r^2} + \frac{1}{r^2} \frac{\partial^2z}{\partial \theta^2} + \frac{1}{r} \frac{\partial z}{\partial r} $$ My attempt is to show that LS=RS, but I am stuck on how to eliminate the $\frac{1}{r} \frac{\partial z}{\partial r} $ term . See below $$LS: \frac{\partial^2z}{\partial x^2} + \frac{\partial^2z}{\partial y^2}$$ For the RS, first find its given partial derivatives/expressions First term: $$===>\frac{\partial^2z}{\partial r^2} = \frac{\partial^2z}{\partial x^2}cos^2\theta + \frac{\partial^2z}{\partial y^2}sin^2\theta + 2(\frac{\partial^2z}{\partial x\partial y}sin\theta cos\theta)$$ This is the part where I get stuck $$ \frac{1}{r^2} \frac{\partial^2z}{\partial \theta^2}=\frac{1}{r^2}[\frac{\partial}{\partial \theta}(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)]$$ Now treat $(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)$ as a function of $z$ hence $(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)$ can be seen as $\frac{\partial z}{\partial \theta}$ $$= \frac{1}{r^2}[\frac{\partial}{\partial x}(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)\frac{\partial x}{\partial \theta} + \frac{\partial}{\partial y}(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)\frac{\partial y}{\partial \theta}]$$ We know $\frac{\partial x}{\partial \theta} = -rsin\theta$ and $\frac{\partial y}{\partial \theta}=rcos\theta$ , so substitute them in $$= \frac{1}{r^2}[\frac{\partial}{\partial x}(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)(-rsin\theta) + \frac{\partial}{\partial y}(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)(rcos\theta)]$$ $$= \frac{1}{r^2}[(\frac{\partial ^2z}{\partial x^2}r^2sin^2\theta-\frac{\partial ^2z}{\partial x\partial y}r^2cos\theta sin\theta) + (-\frac{\partial ^2z}{\partial x\partial y}r^2cos\theta sin\theta+\frac{\partial ^2z}{\partial y^2}r^2cos^2\theta)]$$ Second term: $$===>\frac{1}{r^2} \frac{\partial^2z}{\partial \theta^2} = \frac{\partial ^2z}{\partial x^2}sin^2\theta-2(\frac{\partial ^2z}{\partial x\partial y}cos\theta sin\theta) +\frac{\partial ^2z}{\partial y^2}cos^2\theta$$ Now for the last term on the right side, Third term: $$===>\frac{1}{r} \frac{\partial z}{\partial r}= \frac{1}{r}[\frac{\partial z}{\partial x}cos\theta + \frac{\partial z}{\partial y}sin\theta]$$ Combining all terms of the right side, we have the following $$\frac{\partial^2z}{\partial x^2}cos^2\theta + \frac{\partial^2z}{\partial y^2}sin^2\theta + \frac{\partial^2z}{\partial x^2}sin^2\theta + \frac{\partial^2z}{\partial y^2}cos^2\theta + \frac{1}{r}[\frac{\partial z}{\partial x}cos\theta + \frac{\partial z}{\partial y}sin\theta]$$ $$===>RS: \frac{\partial^2z}{\partial x^2} + \frac{\partial^2z}{\partial y^2} + \frac{1}{r}[\frac{\partial z}{\partial x}cos\theta + \frac{\partial z}{\partial y}sin\theta]$$ which, $$≠LS=\frac{\partial^2z}{\partial x^2} + \frac{\partial^2z}{\partial y^2}$$ My QUESTION: There is supposed to be an extra term for the second term of the right side that cancels the third term, but I can't see it. Anyways, what did I do wrong? I believe my error is evaluating the second term of the right side, but it seems correct to me.","Question: Let and , Show that My attempt is to show that LS=RS, but I am stuck on how to eliminate the term . See below For the RS, first find its given partial derivatives/expressions First term: This is the part where I get stuck Now treat as a function of hence can be seen as We know and , so substitute them in Second term: Now for the last term on the right side, Third term: Combining all terms of the right side, we have the following which, My QUESTION: There is supposed to be an extra term for the second term of the right side that cancels the third term, but I can't see it. Anyways, what did I do wrong? I believe my error is evaluating the second term of the right side, but it seems correct to me.","z = f (x, y) x = r \cos \theta   y = r \sin \theta \frac{\partial^2z}{\partial x^2} + \frac{\partial^2z}{\partial y^2} = \frac{\partial^2z}{\partial r^2} + \frac{1}{r^2} \frac{\partial^2z}{\partial \theta^2} + \frac{1}{r} \frac{\partial z}{\partial r}  \frac{1}{r} \frac{\partial z}{\partial r}  LS: \frac{\partial^2z}{\partial x^2} + \frac{\partial^2z}{\partial y^2} ===>\frac{\partial^2z}{\partial r^2} = \frac{\partial^2z}{\partial x^2}cos^2\theta + \frac{\partial^2z}{\partial y^2}sin^2\theta + 2(\frac{\partial^2z}{\partial x\partial y}sin\theta cos\theta)  \frac{1}{r^2} \frac{\partial^2z}{\partial \theta^2}=\frac{1}{r^2}[\frac{\partial}{\partial \theta}(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)] (-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta) z (-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta) \frac{\partial z}{\partial \theta} = \frac{1}{r^2}[\frac{\partial}{\partial x}(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)\frac{\partial x}{\partial \theta} + \frac{\partial}{\partial y}(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)\frac{\partial y}{\partial \theta}] \frac{\partial x}{\partial \theta} = -rsin\theta \frac{\partial y}{\partial \theta}=rcos\theta = \frac{1}{r^2}[\frac{\partial}{\partial x}(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)(-rsin\theta) + \frac{\partial}{\partial y}(-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta)(rcos\theta)] = \frac{1}{r^2}[(\frac{\partial ^2z}{\partial x^2}r^2sin^2\theta-\frac{\partial ^2z}{\partial x\partial y}r^2cos\theta sin\theta) + (-\frac{\partial ^2z}{\partial x\partial y}r^2cos\theta sin\theta+\frac{\partial ^2z}{\partial y^2}r^2cos^2\theta)] ===>\frac{1}{r^2} \frac{\partial^2z}{\partial \theta^2} = \frac{\partial ^2z}{\partial x^2}sin^2\theta-2(\frac{\partial ^2z}{\partial x\partial y}cos\theta sin\theta) +\frac{\partial ^2z}{\partial y^2}cos^2\theta ===>\frac{1}{r} \frac{\partial z}{\partial r}= \frac{1}{r}[\frac{\partial z}{\partial x}cos\theta + \frac{\partial z}{\partial y}sin\theta] \frac{\partial^2z}{\partial x^2}cos^2\theta + \frac{\partial^2z}{\partial y^2}sin^2\theta + \frac{\partial^2z}{\partial x^2}sin^2\theta + \frac{\partial^2z}{\partial y^2}cos^2\theta + \frac{1}{r}[\frac{\partial z}{\partial x}cos\theta + \frac{\partial z}{\partial y}sin\theta] ===>RS: \frac{\partial^2z}{\partial x^2} + \frac{\partial^2z}{\partial y^2} + \frac{1}{r}[\frac{\partial z}{\partial x}cos\theta + \frac{\partial z}{\partial y}sin\theta] ≠LS=\frac{\partial^2z}{\partial x^2} + \frac{\partial^2z}{\partial y^2}","['calculus', 'multivariable-calculus']"
88,"Absolute minimum and maximum of $f(x,y,z)=x^4+y^4+z^4-4xyz$",Absolute minimum and maximum of,"f(x,y,z)=x^4+y^4+z^4-4xyz","I have to find the absolute maximum and minimum of the function $f(x,y,z)=x^4+y^4+z^4-4xyz$ over $x^2+y^2+z^2\leq 9$ , $x,y,z\geq 0$ . I'm having problems to find the constrained extremas in $x^2+y^2+z^2=9$ . I have tried by using the Lagrange's Multipliers theorem, by parametrizing the sphere and by algebraic manipulating the function but I haven't been able to come up with the solution. Could anybody help me? Thank you in advance.","I have to find the absolute maximum and minimum of the function over , . I'm having problems to find the constrained extremas in . I have tried by using the Lagrange's Multipliers theorem, by parametrizing the sphere and by algebraic manipulating the function but I haven't been able to come up with the solution. Could anybody help me? Thank you in advance.","f(x,y,z)=x^4+y^4+z^4-4xyz x^2+y^2+z^2\leq 9 x,y,z\geq 0 x^2+y^2+z^2=9","['multivariable-calculus', 'optimization', 'maxima-minima', 'lagrange-multiplier']"
89,Proving that the derivative is unique in higher dimensions,Proving that the derivative is unique in higher dimensions,,"Can someone help me prove this please? I am thinking of using triangle inequality. However, I feel as if I would be doing to much and there is a better way to prove the following. $\def\h{{\mathbf h}} \def\x{{\mathbf x}} \def\f{{\mathbf f}} \def\0{{\mathbf 0}} \def\R{{\mathbb R}}   \def\L{{\mathcal L}}$ Let $\f\colon D\to \R^m$ where $D\subseteq\R^n$ is open.  Let $\x_0\in D$ and suppose that $\f$ is differentiable at $\x_0$ .  Prove $T\in\L(\R^n,\R^m)$ satisfies the definition of derivative, such that $$\lim_{\h\to\0}\frac{\|\f(\x_0+\h)-\f(\x_0)-T(\h)\|}{\|\h\|} = 0$$ is unique. Essentially, I want to conclude $\|T-S\|<\epsilon$ . $\textit{Proof.}$ Suppose $T$ and $S$ are two linear transformations which satisfy our definition. By $\epsilon-\delta$ definition of limit, for any given $\epsilon >0$ , there exists $\delta >0$ such that $0< \|\h\| < \delta$ then $\displaystyle{\frac{\|\f(\x_0+\h)-\f(\x_0)-T(\h)\|}{\|\h\|}<\frac{\epsilon}{2}}.$ Now, $0<\|\h\|<\delta$ implies $\displaystyle{\|\f(\x_0+\h)-\f(\x_0)-T(\h)\| < \frac{\epsilon}{2}\|\h\|}$","Can someone help me prove this please? I am thinking of using triangle inequality. However, I feel as if I would be doing to much and there is a better way to prove the following. Let where is open.  Let and suppose that is differentiable at .  Prove satisfies the definition of derivative, such that is unique. Essentially, I want to conclude . Suppose and are two linear transformations which satisfy our definition. By definition of limit, for any given , there exists such that then Now, implies","\def\h{{\mathbf h}}
\def\x{{\mathbf x}}
\def\f{{\mathbf f}}
\def\0{{\mathbf 0}}
\def\R{{\mathbb R}}  
\def\L{{\mathcal L}} \f\colon D\to \R^m D\subseteq\R^n \x_0\in D \f \x_0 T\in\L(\R^n,\R^m) \lim_{\h\to\0}\frac{\|\f(\x_0+\h)-\f(\x_0)-T(\h)\|}{\|\h\|} = 0 \|T-S\|<\epsilon \textit{Proof.} T S \epsilon-\delta \epsilon >0 \delta >0 0< \|\h\| < \delta \displaystyle{\frac{\|\f(\x_0+\h)-\f(\x_0)-T(\h)\|}{\|\h\|}<\frac{\epsilon}{2}}. 0<\|\h\|<\delta \displaystyle{\|\f(\x_0+\h)-\f(\x_0)-T(\h)\| < \frac{\epsilon}{2}\|\h\|}","['real-analysis', 'linear-algebra', 'multivariable-calculus', 'normed-spaces', 'epsilon-delta']"
90,$Df = 0$ on open and connected set $\implies f$ is a constant function,on open and connected set  is a constant function,Df = 0 \implies f,"Suppose $(E, \parallel \parallel),(F, \parallel \parallel)$ are Banach spaces, $U \subset E$ is an open, connected set, and $f : U \to F$ such that $Df = 0$ .  Prove that $f$ is a constant function. Using the MVT, then for $y \in [x,y] :=  \{ (1-t)x+ty \mid 0 \leq t \leq 1\}$ , $$ \| f(x)-f(y) \| \leq \sup_{c \in [x,y]} \|Df(c)(y-x) \| = 0. $$ So $f(x) = f(y), \ \forall y \in [x,y]$ . How do I show that $f(x)=f(y)$ for all $y \in U$ ? (I know it has to do with the connectedness of $U$ , but I don't know how.)","Suppose are Banach spaces, is an open, connected set, and such that .  Prove that is a constant function. Using the MVT, then for , So . How do I show that for all ? (I know it has to do with the connectedness of , but I don't know how.)","(E, \parallel \parallel),(F, \parallel \parallel) U \subset E f : U \to F Df = 0 f y \in [x,y] :=  \{ (1-t)x+ty \mid 0 \leq t \leq 1\}  \| f(x)-f(y) \| \leq \sup_{c \in [x,y]} \|Df(c)(y-x) \| = 0.  f(x) = f(y), \ \forall y \in [x,y] f(x)=f(y) y \in U U","['real-analysis', 'functional-analysis', 'multivariable-calculus', 'frechet-derivative']"
91,how to find directional derivative,how to find directional derivative,,"I am trying to find the directional derivative of the following problem $F(x,y,z) = 4x^2+ 3y−3xz+ 2z^2$ at the point $(2,1,2)$ in the direction $i−k$ ; I worked out the derivatives of $F(x,y,z)$ as $f_x = 8x -3z$ $f_y = 3$ $f_z = -3x+4z$ But I don't know to do next; can someone explain how to find the directional derivative here please? Thank you",I am trying to find the directional derivative of the following problem at the point in the direction ; I worked out the derivatives of as But I don't know to do next; can someone explain how to find the directional derivative here please? Thank you,"F(x,y,z) = 4x^2+ 3y−3xz+ 2z^2 (2,1,2) i−k F(x,y,z) f_x = 8x -3z f_y = 3 f_z = -3x+4z","['algebra-precalculus', 'multivariable-calculus']"
92,Why isn't torsion the magnitude of the derivative of the binormal vector?,Why isn't torsion the magnitude of the derivative of the binormal vector?,,"I am learning about the $TNB$ , curvature and torsion and I found this weird derivation about torsion: $$\tau=-B'(s)\cdot \hat{N}(s).$$ And the explaination I got basically is that since $$|\hat{B}(s)|=1,$$ then $$B'(s)\perp \hat{B}(s).$$ Also using some algebraic manipulation you get that $$\hat{T}(s)\perp B'(s).$$ Therefore $B'(s)$ must be parallel to the normal vector $\hat{N}(s)$ since it's perpendicular to both $B$ and $T$ . So you could say that $B'(s)$ is some constant $\tau$ multiplied by $\hat{N}$ , and that's how you get to the first formula. So the question is, why is it done this way? Isn't it posible to say ""torsion is the speed of change of the binormal vector"", write it down as $\tau=|B'(t)|$ ? Am I missing something? Is there a historic or practical reason why it's defined this way? It just seemed very counterintuitive.","I am learning about the , curvature and torsion and I found this weird derivation about torsion: And the explaination I got basically is that since then Also using some algebraic manipulation you get that Therefore must be parallel to the normal vector since it's perpendicular to both and . So you could say that is some constant multiplied by , and that's how you get to the first formula. So the question is, why is it done this way? Isn't it posible to say ""torsion is the speed of change of the binormal vector"", write it down as ? Am I missing something? Is there a historic or practical reason why it's defined this way? It just seemed very counterintuitive.","TNB \tau=-B'(s)\cdot \hat{N}(s). |\hat{B}(s)|=1, B'(s)\perp \hat{B}(s). \hat{T}(s)\perp B'(s). B'(s) \hat{N}(s) B T B'(s) \tau \hat{N} \tau=|B'(t)|","['multivariable-calculus', 'differential-geometry', 'vectors']"
93,Finding limit points and isolated points of a set,Finding limit points and isolated points of a set,,"I'm trying to solve the following problem: find limit points and isolated points of the following set: $$A=\left\{(x,y)\in\mathbb{E}^2: x=(-1)^n\frac{n}{n+1}, y=3, n\in \mathbb{N}\right\}$$ How should I proceed? I know that limit point is a point, that where if I put a sphere of arbitrary length, I should always get some point that belongs to the set. However, I fail to draw that function. (Still learning functions of two variables). How should I proceed without drawing the function? Thanks","I'm trying to solve the following problem: find limit points and isolated points of the following set: How should I proceed? I know that limit point is a point, that where if I put a sphere of arbitrary length, I should always get some point that belongs to the set. However, I fail to draw that function. (Still learning functions of two variables). How should I proceed without drawing the function? Thanks","A=\left\{(x,y)\in\mathbb{E}^2: x=(-1)^n\frac{n}{n+1}, y=3, n\in \mathbb{N}\right\}","['calculus', 'multivariable-calculus']"
94,"For improper multiple integrals, can divergence be concluded if one of the inner integrals diverges?","For improper multiple integrals, can divergence be concluded if one of the inner integrals diverges?",,"While solving $$ \iint\limits_{(-\infty, +\infty)} \dfrac{\cos(x^2 + y^2)}{e^{x^2+ y^2}} \, \, dx dy$$ I reached the following stage after passing to polar coordinates: $$  \iint\limits_{(-\infty, +\infty)} \dfrac{\cos(r^2)}{e^{r^2}} \, \, rdr \, d\theta = \dfrac12\iint\limits_{(-\infty, +\infty)} \dfrac{\cos(u)}{e^{u}} \, \, du \,d\theta $$ Now, since there is no $\theta$ variable in the integrand, if we choose to integrate first with respect to $\theta$ then our partial integral is automatically divergent.  Thus my question is, Is the divergence of at least one of the inner integrals in a multiple   integral enough to conclude that the whole multiple integral diverges?","While solving I reached the following stage after passing to polar coordinates: Now, since there is no variable in the integrand, if we choose to integrate first with respect to then our partial integral is automatically divergent.  Thus my question is, Is the divergence of at least one of the inner integrals in a multiple   integral enough to conclude that the whole multiple integral diverges?"," \iint\limits_{(-\infty, +\infty)} \dfrac{\cos(x^2 + y^2)}{e^{x^2+ y^2}} \, \, dx dy   \iint\limits_{(-\infty, +\infty)} \dfrac{\cos(r^2)}{e^{r^2}} \, \, rdr \, d\theta = \dfrac12\iint\limits_{(-\infty, +\infty)} \dfrac{\cos(u)}{e^{u}} \, \, du \,d\theta
 \theta \theta","['real-analysis', 'integration', 'multivariable-calculus', 'improper-integrals', 'multiple-integral']"
95,proving inverse of gradient operator,proving inverse of gradient operator,,"I'm trying to prove the inverse of the gradient operation ( $\nabla V$ ): $\vec{E} = \nabla V$ is the line integral operation: $\int\vec{E}\bullet d\vec{\mathcal{l}} = V$ Here's my work: $\vec{E} = \nabla V$ take the line integral of both sides: $\int\vec{E}\bullet d\vec{\mathcal{l}} = \int \nabla V\bullet d\vec{\mathcal{l}}$ then for some reason i don't fully understand, right hand side becomes equal to V: $\int\vec{E}\bullet d\vec{\mathcal{l}} = V$ I guess my question is this.  why does: $\int \nabla V\bullet d\vec{\mathcal{l}} = V$ ?","I'm trying to prove the inverse of the gradient operation ( ): is the line integral operation: Here's my work: take the line integral of both sides: then for some reason i don't fully understand, right hand side becomes equal to V: I guess my question is this.  why does: ?",\nabla V \vec{E} = \nabla V \int\vec{E}\bullet d\vec{\mathcal{l}} = V \vec{E} = \nabla V \int\vec{E}\bullet d\vec{\mathcal{l}} = \int \nabla V\bullet d\vec{\mathcal{l}} \int\vec{E}\bullet d\vec{\mathcal{l}} = V \int \nabla V\bullet d\vec{\mathcal{l}} = V,"['multivariable-calculus', 'vector-analysis']"
96,What is the geometric difference between partial derivative and ordinary derivative?,What is the geometric difference between partial derivative and ordinary derivative?,,"Ordinary derivative of a function let $~y=f(x)~$ represents the slope of the tangent drawn at the point $~x~$ to the curve $~f(x)~$ . But, what about partial derivative ? Please, attach images if possible for understanding.","Ordinary derivative of a function let represents the slope of the tangent drawn at the point to the curve . But, what about partial derivative ? Please, attach images if possible for understanding.",~y=f(x)~ ~x~ ~f(x)~,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
97,How do I define $\theta$?,How do I define ?,\theta,"I need to calculate $\iint f(x,y) \,dx \,dy$ with the given region $D$ where $$f(x,y) = xy$$ and $$D = \{(x,y)\in \mathbb{R^2} \space | \space 0 \leq y \leq x, x^2+y^2 \leq 4 \}$$ I have used cylindrical coordinates trying to calculate this. So $f(x,y) = r \cos(\theta) r \sin(\theta)$ and because $x^2+y^2 \leq 2^2$ is a circle, I know that $ 0 \leq r \leq 2$ . Now, how do I know the values for $\theta$ ? I can't get anything out of $0 \leq y \leq x$ .","I need to calculate with the given region where and I have used cylindrical coordinates trying to calculate this. So and because is a circle, I know that . Now, how do I know the values for ? I can't get anything out of .","\iint f(x,y) \,dx \,dy D f(x,y) = xy D = \{(x,y)\in \mathbb{R^2} \space | \space 0 \leq y \leq x, x^2+y^2 \leq 4 \} f(x,y) = r \cos(\theta) r \sin(\theta) x^2+y^2 \leq 2^2  0 \leq r \leq 2 \theta 0 \leq y \leq x","['integration', 'multivariable-calculus', 'definite-integrals', 'surfaces']"
98,How to integrate $\int_{0}^{\frac{\pi}{2}}\frac{-2x\cos^{2}\theta}{1-x^{2}\cos^{2}\theta}d\theta$,How to integrate,\int_{0}^{\frac{\pi}{2}}\frac{-2x\cos^{2}\theta}{1-x^{2}\cos^{2}\theta}d\theta,How to integrate $$\int_{0}^{\frac{\pi}{2}}\frac{-2x\cos^{2}\theta}{1-x^{2}\cos^{2}\theta}d\theta$$ THis question arises from an earlier question I was working on which I posted here: Find $f'(x)$ given that $f(x) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)d\theta$ I tried a few different things all to no avail...Which Trig trick am I missing to apply to this question?,How to integrate THis question arises from an earlier question I was working on which I posted here: Find $f'(x)$ given that $f(x) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)d\theta$ I tried a few different things all to no avail...Which Trig trick am I missing to apply to this question?,\int_{0}^{\frac{\pi}{2}}\frac{-2x\cos^{2}\theta}{1-x^{2}\cos^{2}\theta}d\theta,"['integration', 'multivariable-calculus', 'derivatives']"
99,"Find $\lim_{(x,y) \to (0,0)} \frac{xy-\sin(x)\sin(y)}{x^2+y^2}$",Find,"\lim_{(x,y) \to (0,0)} \frac{xy-\sin(x)\sin(y)}{x^2+y^2}","Find $$\lim_{(x,y) \to (0,0)} \frac{xy-\sin(x)\sin(y)}{x^2+y^2}$$ What I did was: Find the second order Taylor expansion for $xy-\sin(x)\sin(y)$ at $(0,0)$ : $P_2 (0,0)=f(0,0)+df_{(0,0)}+d^2f_{(0,0)}+r_2(0,0)$ $f(0,0)=0$ $df_{(0,0)}=\frac{\partial f}{\partial x} (0,0) \Delta x + \frac{\partial f}{\partial y} (0,0) \Delta y = 0$ $d^2f_{(0,0)}=\frac{\partial f}{\partial x} (0,0) \Delta x^2 + \frac{\partial f}{\partial x \partial y} (0,0) \Delta x \Delta y + \frac{\partial f}{\partial y} (0,0) \Delta y^2 = 0$ $\implies$ $P_2(0,0)=r_2(0,0)$ Now I have $\lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{x^2+y^2}$ $\implies  \lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{x^2+y^2} = \lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{x^2+y^2} \cdot \frac{\sqrt{x^2+y^2}}{\sqrt{x^2+y^2}} = \lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{\sqrt{x^2+y^2}} \cdot \frac{\sqrt{x^2+y^2}}{x^2+y^2}= 0$ However, wolframalpha is telling me the limit does not exist...","Find What I did was: Find the second order Taylor expansion for at : Now I have However, wolframalpha is telling me the limit does not exist...","\lim_{(x,y) \to (0,0)} \frac{xy-\sin(x)\sin(y)}{x^2+y^2} xy-\sin(x)\sin(y) (0,0) P_2 (0,0)=f(0,0)+df_{(0,0)}+d^2f_{(0,0)}+r_2(0,0) f(0,0)=0 df_{(0,0)}=\frac{\partial f}{\partial x} (0,0) \Delta x + \frac{\partial f}{\partial y} (0,0) \Delta y = 0 d^2f_{(0,0)}=\frac{\partial f}{\partial x} (0,0) \Delta x^2 + \frac{\partial f}{\partial x \partial y} (0,0) \Delta x \Delta y + \frac{\partial f}{\partial y} (0,0) \Delta y^2 = 0 \implies P_2(0,0)=r_2(0,0) \lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{x^2+y^2} \implies  \lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{x^2+y^2} = \lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{x^2+y^2} \cdot \frac{\sqrt{x^2+y^2}}{\sqrt{x^2+y^2}} = \lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{\sqrt{x^2+y^2}} \cdot \frac{\sqrt{x^2+y^2}}{x^2+y^2}= 0","['calculus', 'limits', 'multivariable-calculus', 'taylor-expansion']"
