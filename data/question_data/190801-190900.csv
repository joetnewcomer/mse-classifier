,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Mean value theorem (maybe),Mean value theorem (maybe),,"Here is one problem in my midterm test in introduction to real analysis Let $f$ be continuous function in $[a,b]$, $a>0$ and differentiable in $(a,b)$ show that there exist $c$ in $(a,b)$ such that $$\frac{af(b)−bf(a)}{a−b}=f(c)−cf′(c)$$ Thanks. I'm sorry for my lack in English.","Here is one problem in my midterm test in introduction to real analysis Let $f$ be continuous function in $[a,b]$, $a>0$ and differentiable in $(a,b)$ show that there exist $c$ in $(a,b)$ such that $$\frac{af(b)−bf(a)}{a−b}=f(c)−cf′(c)$$ Thanks. I'm sorry for my lack in English.",,"['real-analysis', 'derivatives']"
1,"$f:(0,1)\cup (1,2)\to \Bbb R$ and $f(x)=0$ if $x\in (0,1)$ otherwise $1$.",and  if  otherwise .,"f:(0,1)\cup (1,2)\to \Bbb R f(x)=0 x\in (0,1) 1","Till today, I always thought that if the derivative of a function is $0$ at every point in the  domain, then the only functions possible for which this is true are constant functions. But, my teacher gave me the example $$f:(0,1)\cup (1,2)\to \Bbb R$$ with $$f(x) = \begin{cases}0&\mathrm{\ if\ } 0<x<1 \\ 1 &\mathrm{\ if\ }1<x<2\end{cases}$$ This function is not constant but has derivative $0$ everywhere in domain.I know it is pretty much possible because of the domain he chose, but what is special there in this domain which makes this happen. Can anybody give some other examples not of this form (which I gave above) having derivative $0$ without function being constant.","Till today, I always thought that if the derivative of a function is $0$ at every point in the  domain, then the only functions possible for which this is true are constant functions. But, my teacher gave me the example $$f:(0,1)\cup (1,2)\to \Bbb R$$ with $$f(x) = \begin{cases}0&\mathrm{\ if\ } 0<x<1 \\ 1 &\mathrm{\ if\ }1<x<2\end{cases}$$ This function is not constant but has derivative $0$ everywhere in domain.I know it is pretty much possible because of the domain he chose, but what is special there in this domain which makes this happen. Can anybody give some other examples not of this form (which I gave above) having derivative $0$ without function being constant.",,"['calculus', 'derivatives']"
2,How do I find the derivative of this equation using the fundamental theorem of calculus? [closed],How do I find the derivative of this equation using the fundamental theorem of calculus? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Can someone explain how to find the derivative using the fundamental theorem of calculus on the following equation? $$g(x) = \int_{2}^{x} t^2 \sin(t) dt  $$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Can someone explain how to find the derivative using the fundamental theorem of calculus on the following equation? $$g(x) = \int_{2}^{x} t^2 \sin(t) dt  $$",,"['calculus', 'derivatives']"
3,find the minimum value of $a+b+c$,find the minimum value of,a+b+c,"There are natural numbers: $a$, $b$, $c$. $$\begin{cases} ab+bc+ca+\frac32(a+b+c)=5015,\\ 2abc-a-b-c=6366 \end{cases} $$ I need to find the minimum value of $a+b+c$. To my mind there's something connected with the derivative. I've already tried to find an equation for $a+b+c$, but I stuck right here. If we do some math in the second equation, we can get this one $a+b+c=3183/abc$ Everything's great, but if we consider $a+b+c = l$ (for example), then we need to find a minimum of this function. But as far as I'm concerned this is a Hyperbolic function, which doesn't have extreme values. So there's my first confusion. Moving on I decided to do something with a first equation. But, unfortunately, no matter how I rearrange my $a$, $b$ and $c$ I get some equations and can't get anything useful out of it. And the last thing. If I get an equation for $a+b+c$ from the first equation then find a derivative and its zero value, I get $c = -b$. Well, that perfectly concludes the problem. Could anyone help me get this one done? Sorry for image confusion.","There are natural numbers: $a$, $b$, $c$. $$\begin{cases} ab+bc+ca+\frac32(a+b+c)=5015,\\ 2abc-a-b-c=6366 \end{cases} $$ I need to find the minimum value of $a+b+c$. To my mind there's something connected with the derivative. I've already tried to find an equation for $a+b+c$, but I stuck right here. If we do some math in the second equation, we can get this one $a+b+c=3183/abc$ Everything's great, but if we consider $a+b+c = l$ (for example), then we need to find a minimum of this function. But as far as I'm concerned this is a Hyperbolic function, which doesn't have extreme values. So there's my first confusion. Moving on I decided to do something with a first equation. But, unfortunately, no matter how I rearrange my $a$, $b$ and $c$ I get some equations and can't get anything useful out of it. And the last thing. If I get an equation for $a+b+c$ from the first equation then find a derivative and its zero value, I get $c = -b$. Well, that perfectly concludes the problem. Could anyone help me get this one done? Sorry for image confusion.",,"['linear-algebra', 'derivatives', 'natural-numbers']"
4,"If $x_n \to c$ and and $f(x_i)=f(c)$, then all the derivatives at $a$ vanish","If  and and , then all the derivatives at  vanish",x_n \to c f(x_i)=f(c) a,"My book claims the following Let $x_n \in (a,b)\setminus\{c\}$ be a sequence that converges to $c \in (a,b)$ . If $f: (a,b) \to \mathbb{R}$ satisfies that $f(x_i)=f(c)$ for all $i$ , then all the derivatives at $c$ that exist vanish. This seems right, but I haven't been able to prove it, the book just says that it follows from Taylor's formula. What's a proof of this, preferably using Taylor's formula? What I tried is use the formula on the $f(x_i)$ , so we get that we have a sequence $h_n=x_n - c$ with $h_n \to 0$ and $$f(x_k)=f(c+h_k)=\sum^n_{i=0}\frac{f^{(i)}(c)}{i!}h^i_k+r(h_k)=f(c)$$ Making $k$ go to infinity doesn't seem to accomplish anything, yet there doesn't seem like there's more to do with the data available.","My book claims the following Let be a sequence that converges to . If satisfies that for all , then all the derivatives at that exist vanish. This seems right, but I haven't been able to prove it, the book just says that it follows from Taylor's formula. What's a proof of this, preferably using Taylor's formula? What I tried is use the formula on the , so we get that we have a sequence with and Making go to infinity doesn't seem to accomplish anything, yet there doesn't seem like there's more to do with the data available.","x_n \in (a,b)\setminus\{c\} c \in (a,b) f: (a,b) \to \mathbb{R} f(x_i)=f(c) i c f(x_i) h_n=x_n - c h_n \to 0 f(x_k)=f(c+h_k)=\sum^n_{i=0}\frac{f^{(i)}(c)}{i!}h^i_k+r(h_k)=f(c) k","['real-analysis', 'derivatives', 'taylor-expansion']"
5,Derivative of a single valued function as a limit of a function in $\mathbb{R}^2$,Derivative of a single valued function as a limit of a function in,\mathbb{R}^2,"I am very familiar to the usual definition of derivative of a function $f(x) : \mathbb{R} \rightarrow \mathbb{R}^n$ as $\lim_{x \rightarrow a}\frac{f(x)-f(a)}{(x-a)}$ . My question is when can I relax the condition of ""fixing a $a$ ""? That is, when the following also holds: $$\lim_{(x,y)\rightarrow(a,a)} \dfrac{f(x)-f(y)}{x-y} = f'(a) \;\text{?}$$ I am convinced that this will work most of the time but I'm having trouble proving it.","I am very familiar to the usual definition of derivative of a function as . My question is when can I relax the condition of ""fixing a ""? That is, when the following also holds: I am convinced that this will work most of the time but I'm having trouble proving it.","f(x) : \mathbb{R} \rightarrow \mathbb{R}^n \lim_{x \rightarrow a}\frac{f(x)-f(a)}{(x-a)} a \lim_{(x,y)\rightarrow(a,a)} \dfrac{f(x)-f(y)}{x-y} = f'(a) \;\text{?}","['real-analysis', 'derivatives']"
6,"Is the remainder term in a Taylor polynomial approximation for $\log{(1+x)}$ correct in Spivak's Calculus, Ch. 20?","Is the remainder term in a Taylor polynomial approximation for  correct in Spivak's Calculus, Ch. 20?",\log{(1+x)},"In Chapter 20 of Spivak's Calculus 4th edition page 427 is written From the calculations on page 413, we see that for $x\geq 0$ we have $$\log{(1+x)}=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+...+\frac{(-1)^{n-1}x^n}{n}+\frac{(-1)^n}{n+1}t^{n+1}\tag{1}$$ where $$\left | \frac{(-1)^n}{n+1}t^{n+1}\right |\leq \frac{x^{n+1}}{n+1}$$ and there is a slightly more complicated estimate when $-1<x<0$ (Problem 16). As far as I can tell the last term in $(1)$ $$\frac{(-1)^n}{n+1}t^{n+1}\tag{2}$$ is the remainder term when we use a Taylor polynomial approximation. However, isn't this remainder supposed to be $$R_{n,0}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-0)^{n+1}, t\in (0,x)$$ $$=\frac{(-1)^n \cdot n!}{(1+t)^{n+1}}\cdot \frac{1}{(n+1)!}\cdot x^{n+1}, t\in (0,x)$$ $$=\frac{(-1)^n x^{n+1}}{(1+t)^{n+1}(n+1)}\tag{3}$$ ? I used $$f^{(k)}(x)=(-1)^{k-1} \frac{(k-1)!}{(1+x)^k}$$ thus $$f^{(k+1)}(t)=(-1)^{k} \frac{k!}{(1+t)^{k+1}}$$ Am I making some silly mistake here? Is the remainder term $(2)$ or $(3)$ ?","In Chapter 20 of Spivak's Calculus 4th edition page 427 is written From the calculations on page 413, we see that for we have where and there is a slightly more complicated estimate when (Problem 16). As far as I can tell the last term in is the remainder term when we use a Taylor polynomial approximation. However, isn't this remainder supposed to be ? I used thus Am I making some silly mistake here? Is the remainder term or ?","x\geq 0 \log{(1+x)}=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+...+\frac{(-1)^{n-1}x^n}{n}+\frac{(-1)^n}{n+1}t^{n+1}\tag{1} \left | \frac{(-1)^n}{n+1}t^{n+1}\right |\leq \frac{x^{n+1}}{n+1} -1<x<0 (1) \frac{(-1)^n}{n+1}t^{n+1}\tag{2} R_{n,0}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-0)^{n+1}, t\in (0,x) =\frac{(-1)^n \cdot n!}{(1+t)^{n+1}}\cdot \frac{1}{(n+1)!}\cdot x^{n+1}, t\in (0,x) =\frac{(-1)^n x^{n+1}}{(1+t)^{n+1}(n+1)}\tag{3} f^{(k)}(x)=(-1)^{k-1} \frac{(k-1)!}{(1+x)^k} f^{(k+1)}(t)=(-1)^{k} \frac{k!}{(1+t)^{k+1}} (2) (3)","['calculus', 'integration', 'derivatives', 'polynomials', 'taylor-expansion']"
7,"Spivak, Ch. 18, Problem 36: Understanding solution manual ""easily"" showing $\frac{1}{b-a}\int_a^b \log{f}\leq\log(\frac{1}{b-a}\int_a^b f )$","Spivak, Ch. 18, Problem 36: Understanding solution manual ""easily"" showing",\frac{1}{b-a}\int_a^b \log{f}\leq\log(\frac{1}{b-a}\int_a^b f ),"The following problem is from Chapter 18 of Spivak's Calculus (a) Let $f$ be a positive function on $[a,b]$ and let $P_n$ be a partition of $[a,b]$ into $n$ equal subintervals. Use Problem 2-22 to show that $$\frac{1}{b-a}L(\log{f},P_n)\leq\log\left(\frac{1}{b-a}L(f,P_n)\right )$$ (b) Use the Appendix to Chapter 13 to conclude that for all integrable $f>0$ we have $$\frac{1}{b-a}\int_a^b \log{f}\leq \log\left (\frac{1}{b-a}\int_a^b  f\right )$$ The Appendix to Chapter 13 contains one theorem which says Suppose $f$ is integrable on $[a,b]$ . Then for every $\epsilon>0$ there is a $\delta>0$ such that if $P=\{t_0,...,t_n\}$ is a partition of $[a,b]$ with $t_i-t_{i-1}<\delta$ for all $i$ then $$\left | \sum\limits_{i=1}^n f(x_i)\Delta t_i-\int_a^b f(x)dx\right |<\epsilon$$ which basically says that any Riemann sum can be made arbitrarily close to the integral (which is defined in terms of lower and upper sums being equal). The solution manual solution to part $(b)$ Theorem 1 shows that if $f$ is integrable then for every $\epsilon>0$ there is a $\delta>0$ such that $$\left | \sum\limits_{i=1}^n f(x_i)\Delta t_i-\int_a^b f(x)dx\right  |<\epsilon/2$$ for any partition $P=\{t_0,...,t_n\}$ of $[a,b]$ , and choices $x_i$ in $[t_{i-1},t_i]$ , for which all $t_i-t_{i-1}<\delta$ . It is easy to conclude that we then have $$\left |L(f,P)-\int_a^b f(x)dx \right |<\epsilon$$ for such partitions (we need to increase $\epsilon/2$ to $\epsilon$ since $m_i$ may not actually be $f(x_i)$ for any $x_i$ in $[t_{i-1},t_i]$ ). I'm not sure how the relationship between $\epsilon/2$ and $\epsilon$ was reached. Why do we increase $\epsilon/2$ to $\epsilon$ ? All I came up with was that since $$L(f,P)\leq \int_a^b f\leq U(f,P)$$ for all partitions $P$ , and by definition of integrability of $f$ $$\forall\epsilon, \exists\text{ partition } P, 0<U(f,P)-L(f,P)<\epsilon$$ then $$0\leq \int_a^b f - L(f,P)\leq U(f,P)-L(f,P)<\epsilon$$ $$|L(f,P)-\int_a^b f|<\epsilon$$ What does this have to do with Theorem 1? In particular $$\left |L(\log{f},P_n)-\int_a^b \log{f} \right |<\epsilon$$ $$\left |L(f,P_n)-\int_a^b f \right |<\epsilon$$ For $n$ sufficiently large. The desired result then follows easily from part $(a)$ . I don't think I've ever been so frustrated with the use of the word ""easily"". How does this last step follow from part $(a)$ ?","The following problem is from Chapter 18 of Spivak's Calculus (a) Let be a positive function on and let be a partition of into equal subintervals. Use Problem 2-22 to show that (b) Use the Appendix to Chapter 13 to conclude that for all integrable we have The Appendix to Chapter 13 contains one theorem which says Suppose is integrable on . Then for every there is a such that if is a partition of with for all then which basically says that any Riemann sum can be made arbitrarily close to the integral (which is defined in terms of lower and upper sums being equal). The solution manual solution to part Theorem 1 shows that if is integrable then for every there is a such that for any partition of , and choices in , for which all . It is easy to conclude that we then have for such partitions (we need to increase to since may not actually be for any in ). I'm not sure how the relationship between and was reached. Why do we increase to ? All I came up with was that since for all partitions , and by definition of integrability of then What does this have to do with Theorem 1? In particular For sufficiently large. The desired result then follows easily from part . I don't think I've ever been so frustrated with the use of the word ""easily"". How does this last step follow from part ?","f [a,b] P_n [a,b] n \frac{1}{b-a}L(\log{f},P_n)\leq\log\left(\frac{1}{b-a}L(f,P_n)\right ) f>0 \frac{1}{b-a}\int_a^b \log{f}\leq \log\left (\frac{1}{b-a}\int_a^b
 f\right ) f [a,b] \epsilon>0 \delta>0 P=\{t_0,...,t_n\} [a,b] t_i-t_{i-1}<\delta i \left | \sum\limits_{i=1}^n f(x_i)\Delta t_i-\int_a^b f(x)dx\right |<\epsilon (b) f \epsilon>0 \delta>0 \left | \sum\limits_{i=1}^n f(x_i)\Delta t_i-\int_a^b f(x)dx\right
 |<\epsilon/2 P=\{t_0,...,t_n\} [a,b] x_i [t_{i-1},t_i] t_i-t_{i-1}<\delta \left |L(f,P)-\int_a^b f(x)dx \right |<\epsilon \epsilon/2 \epsilon m_i f(x_i) x_i [t_{i-1},t_i] \epsilon/2 \epsilon \epsilon/2 \epsilon L(f,P)\leq \int_a^b f\leq U(f,P) P f \forall\epsilon, \exists\text{ partition } P, 0<U(f,P)-L(f,P)<\epsilon 0\leq \int_a^b f - L(f,P)\leq U(f,P)-L(f,P)<\epsilon |L(f,P)-\int_a^b f|<\epsilon \left |L(\log{f},P_n)-\int_a^b \log{f} \right |<\epsilon \left |L(f,P_n)-\int_a^b f \right |<\epsilon n (a) (a)","['calculus', 'integration', 'derivatives', 'proof-explanation', 'logarithms']"
8,Can every symmetric Jacobian matrix be a Hessian matrix?,Can every symmetric Jacobian matrix be a Hessian matrix?,,"Say we have a function $\mathbf{f}(\mathbf{x})$ where $\mathbf{x}\in\mathbb{R}^n$ and $\mathbf{f}:\mathbb{R}^n\rightarrow\mathbb{R}^n$ with a Jacobian matrix $\mathbf{J} = \partial \mathbf{f}/\partial \mathbf{x}\in\mathbb{R}^{n\times n}$ . If the Jacobian matrix $\mathbf{J}$ is real and symmetric everywhere, does a function $y:\mathbb{R}^n\rightarrow\mathbb{R}$ that satisfies $\mathbf{J}=\frac{\partial^2 y}{\partial \mathbf{x}\partial \mathbf{x}}$ exist?","Say we have a function where and with a Jacobian matrix . If the Jacobian matrix is real and symmetric everywhere, does a function that satisfies exist?",\mathbf{f}(\mathbf{x}) \mathbf{x}\in\mathbb{R}^n \mathbf{f}:\mathbb{R}^n\rightarrow\mathbb{R}^n \mathbf{J} = \partial \mathbf{f}/\partial \mathbf{x}\in\mathbb{R}^{n\times n} \mathbf{J} y:\mathbb{R}^n\rightarrow\mathbb{R} \mathbf{J}=\frac{\partial^2 y}{\partial \mathbf{x}\partial \mathbf{x}},"['derivatives', 'differential-forms', 'differential', 'jacobian', 'hessian-matrix']"
9,Partial derivatives where implicit equations are involved,Partial derivatives where implicit equations are involved,,"I have a function: $$f\bigl(P,R\bigr)=h\bigl(P,R,N(P,R)\bigr)$$ where $P$ and $R$ are independent variables and $N$ is the solution of the equation: $$P-g(R,N)=0$$ I am trying to find $\frac{\partial f}{\partial P}$ and $\frac{\partial f}{\partial R}$ .",I have a function: where and are independent variables and is the solution of the equation: I am trying to find and .,"f\bigl(P,R\bigr)=h\bigl(P,R,N(P,R)\bigr) P R N P-g(R,N)=0 \frac{\partial f}{\partial P} \frac{\partial f}{\partial R}","['calculus', 'derivatives', 'partial-derivative', 'chain-rule', 'implicit-differentiation']"
10,Is WolframAlpha wrong on piecewise differentiation at $x=0$?,Is WolframAlpha wrong on piecewise differentiation at ?,x=0,"I'm differentiating the function: $${ f\left(x\right) = \begin{cases}{x \sin ( \frac{1}{x}) } && {x\>\ne\>0} \\ {0} && {x\>=\>0}\end{cases} }$$ In my understanding it is not differentiable at $x= 0$ , as it is not a constant function so we must use the definition of a limit to differentiate at $x=0$ . Wolfram says its derivative is $0$ at $x=0$ . Where am I wrong?","I'm differentiating the function: In my understanding it is not differentiable at , as it is not a constant function so we must use the definition of a limit to differentiate at . Wolfram says its derivative is at . Where am I wrong?",{ f\left(x\right) = \begin{cases}{x \sin ( \frac{1}{x}) } && {x\>\ne\>0} \\ {0} && {x\>=\>0}\end{cases} } x= 0 x=0 0 x=0,"['derivatives', 'wolfram-alpha']"
11,Application of Mean Value/Rolle's Theorem?,Application of Mean Value/Rolle's Theorem?,,"Question Let $f$ be a differentiable function such that $f'$ is continuous on $[0, 1]$ and $M$ the maximum value of $|f'(x)|$ on $[0, 1]\ $ . Prove that, if $f(0) = f(1) = 0$ , then $$\int^{1}_{0} |f(x)| \mathrm {d}x \leq \frac M 4\ .$$ I have stared long and hard at this question for quite some time, but I am not sure where to even begin. Since I see that the endpoints are given, I think of MVT, but since they are also equal, I think of Rolle's Theorem too. These are just my intuitions - they could be wrong. Moreover, the moduli of $f'(x)$ and $f(x)$ are throwing me off. Any hints/suggestions on how to approach this question would be greatly appreciated :) P.S. I am only taking an introductory calculus module in college, so the tools at my disposal are, roughly, IVT, MVT and Rolle's Theorem.","Question Let be a differentiable function such that is continuous on and the maximum value of on . Prove that, if , then I have stared long and hard at this question for quite some time, but I am not sure where to even begin. Since I see that the endpoints are given, I think of MVT, but since they are also equal, I think of Rolle's Theorem too. These are just my intuitions - they could be wrong. Moreover, the moduli of and are throwing me off. Any hints/suggestions on how to approach this question would be greatly appreciated :) P.S. I am only taking an introductory calculus module in college, so the tools at my disposal are, roughly, IVT, MVT and Rolle's Theorem.","f f' [0, 1] M |f'(x)| [0, 1]\  f(0) = f(1) = 0 \int^{1}_{0} |f(x)| \mathrm {d}x \leq \frac M 4\ . f'(x) f(x)","['real-analysis', 'calculus', 'integration', 'derivatives', 'proof-writing']"
12,What is the second derivative of the absolute function $\left|\frac{x+1}{x+2}\right|$?,What is the second derivative of the absolute function ?,\left|\frac{x+1}{x+2}\right|,"I calculated the derivative of $\left|\frac{x+1}{x+2}\right|$ in the same way that I would do with $ \frac{x+1}{x+2}$ in order to study the function. But when I verified on wolfram, I noticed it is all wrong. Wolfram uses the chain rule as you can see here . I don't get it. The only rule I've been taught as far as absolute function derivatives are concerned, is $|x|' = \frac{x}{|x|}$ . Does a similar rule apply for $f(x)$ ? And why does wolfram uses chain rule? Edit I calculated the derivatives as there is no absolute and then, at the result, I applied the absolute. My answers are $|(\frac{x+1}{x+2})|' = |(\frac{x+1}{x+2})'| = \frac{1}{\left(x+2\right)^2}$ and $|(\frac{x+1}{x+2})|'' = |(\frac{x+1}{x+2})''| = \frac{2}{\left(x+2\right)^3}$ Wolfram's answer is $\left(\left|\frac{x+1}{x+2}\right|\right)'\:=\frac{\left|x+2\right|\left(x+1\right)}{\left|x+1\right|\left(x+2\right)^3}$","I calculated the derivative of in the same way that I would do with in order to study the function. But when I verified on wolfram, I noticed it is all wrong. Wolfram uses the chain rule as you can see here . I don't get it. The only rule I've been taught as far as absolute function derivatives are concerned, is . Does a similar rule apply for ? And why does wolfram uses chain rule? Edit I calculated the derivatives as there is no absolute and then, at the result, I applied the absolute. My answers are and Wolfram's answer is",\left|\frac{x+1}{x+2}\right|  \frac{x+1}{x+2} |x|' = \frac{x}{|x|} f(x) |(\frac{x+1}{x+2})|' = |(\frac{x+1}{x+2})'| = \frac{1}{\left(x+2\right)^2} |(\frac{x+1}{x+2})|'' = |(\frac{x+1}{x+2})''| = \frac{2}{\left(x+2\right)^3} \left(\left|\frac{x+1}{x+2}\right|\right)'\:=\frac{\left|x+2\right|\left(x+1\right)}{\left|x+1\right|\left(x+2\right)^3},"['calculus', 'derivatives', 'soft-question']"
13,What is the difference between $dy$ and $Δy$ and why is $dx$ is same as $Δx$ when $x$ is an independent variable and $y$ is dependent one?,What is the difference between  and  and why is  is same as  when  is an independent variable and  is dependent one?,dy Δy dx Δx x y,"I have two questions, say we have a function $y=f(x)$ $Q1.$ $x$ is the independent variable here, then how is $dx$ = $Δx$ ? Here, $dx$ is an infinitesimal while $\Delta x$ is just the finite change in x values, then how can infinitesimal change $=$ finite change. Shouldn't $dx=\lim_{\Delta x\rightarrow 0}{\Delta x}$ ? $Q2.$ What is the difference between $dy$ and $Δy$ ? If we look at $dy$ and $Δy$ . $dy = \lim_{\Delta x\rightarrow 0}{f(x+\Delta x) - f(x))}$ $Δy$ = ${f(x+\Delta x) - f(x)}$ So $dy$ is the limiting case of $\Delta y$ , Am I right about this? I was totally confused by this thing that even after seeing several explanations about this, I still can't wrap my head around it.","I have two questions, say we have a function is the independent variable here, then how is = ? Here, is an infinitesimal while is just the finite change in x values, then how can infinitesimal change finite change. Shouldn't ? What is the difference between and ? If we look at and . = So is the limiting case of , Am I right about this? I was totally confused by this thing that even after seeing several explanations about this, I still can't wrap my head around it.",y=f(x) Q1. x dx Δx dx \Delta x = dx=\lim_{\Delta x\rightarrow 0}{\Delta x} Q2. dy Δy dy Δy dy = \lim_{\Delta x\rightarrow 0}{f(x+\Delta x) - f(x))} Δy {f(x+\Delta x) - f(x)} dy \Delta y,"['calculus', 'derivatives', 'infinitesimals']"
14,Prove that $\ln(1+\frac{1}{x}) < \frac{1}{({x^2 + x})^{1/2}}$,Prove that,\ln(1+\frac{1}{x}) < \frac{1}{({x^2 + x})^{1/2}},"Prove that $$\ln\left(1+\frac{1}{x}\right) < \frac{1}{({x^2 + x})^{1/2}}.$$ I assumed $$g(x) =  \frac{1}{({x^2 + x})^{1/2}} - \ln\left(1+\frac{1}{x}\right)$$ and got it's derivative which is negative, which means, it is a decreasing function. But, it is a function where $\ln(1+\frac{1}{x})$ never exceeds $\frac{1}{({x^2 + x})^{1/2}}$ . Graph for the function Also, $g(x)$ don't have any extremum points in $x > 0$ . Both tends to $0$ as $x$ tends to $\infty$ . How do I prove the inequality?","Prove that I assumed and got it's derivative which is negative, which means, it is a decreasing function. But, it is a function where never exceeds . Graph for the function Also, don't have any extremum points in . Both tends to as tends to . How do I prove the inequality?",\ln\left(1+\frac{1}{x}\right) < \frac{1}{({x^2 + x})^{1/2}}. g(x) =  \frac{1}{({x^2 + x})^{1/2}} - \ln\left(1+\frac{1}{x}\right) \ln(1+\frac{1}{x}) \frac{1}{({x^2 + x})^{1/2}} g(x) x > 0 0 x \infty,"['derivatives', 'inequality']"
15,Constant function composition,Constant function composition,,"I have the following Problem: Let $g \in C^1(\mathbb{R^2};\mathbb{R})$ . Show that an injective $f \in C^1 ((-1,1);\mathbb{R^2})$ exists, so $g \circ f$ is constant. The hint asks if there is a $(x,y) \in \mathbb{R^2}$ with ${\rm grad}(F(x,y)) \ne 0$ . However I have no  idea how to tackle this problem, I saw some examples here: Composition of functions is constant . But these functions are partially constant and therefore not injective, has anyone ideas?","I have the following Problem: Let . Show that an injective exists, so is constant. The hint asks if there is a with . However I have no  idea how to tackle this problem, I saw some examples here: Composition of functions is constant . But these functions are partially constant and therefore not injective, has anyone ideas?","g \in C^1(\mathbb{R^2};\mathbb{R}) f \in C^1 ((-1,1);\mathbb{R^2}) g \circ f (x,y) \in \mathbb{R^2} {\rm grad}(F(x,y)) \ne 0","['real-analysis', 'derivatives']"
16,Gradient of Gaussian Smoothing,Gradient of Gaussian Smoothing,,"In Nesterov's "" Random Gradient-Free Minimization of Convex Functions "", a Gaussian smoothing of a continuous convex function with respect to parameter $\mu$ and covariance matrix $B^{-1} \succ 0$ is defined as: $$ f_{\mu} (x) = \frac{1}{\kappa} \int_{\mathbb{R}^n} f(x + \mu u) e^{- \frac{1}{2} u^\top B u} \,\mathrm{d}u \tag{9} $$ where $$ \kappa = \int_{\mathbb{R}^n} e^{- \frac{1}{2} u^\top B u} \,\mathrm{d}u \tag{10}.$$ The paper goes on to derive a gradient of this expectation, $$ \nabla f_{\mu} (x) = \frac{1}{\mu \kappa} \int_{\mathbb{R}^n} f(x + \mu u) e^{- \frac{1}{2} u^\top B u} B u \,\mathrm{d} u \tag{21}, $$ which should mean that $$ \nabla f_{\mu} (x) = \frac{1}{\mu} \mathbb{E}_{u \sim \mathcal{N}(0, B^{-1})} [ f(x + \mu u) B u]. $$ However this seems to conflict with the earlier claim (just below (5)) that $$ \nabla f_{\mu} (x) = \frac{1}{\mu} \mathbb{E}_{u} [ f(x + \mu u) u ]. $$ I am wondering if I am missing something here. Are there some implicit assumptions that I'm not picking up on? Note: a similar question ( Partial derivative for Gaussian Smoothing ) is almost relevant, but assumes $\mathcal{N} (0, I)$ instead of $\mathcal{N} (0, B^{-1})$ .","In Nesterov's "" Random Gradient-Free Minimization of Convex Functions "", a Gaussian smoothing of a continuous convex function with respect to parameter and covariance matrix is defined as: where The paper goes on to derive a gradient of this expectation, which should mean that However this seems to conflict with the earlier claim (just below (5)) that I am wondering if I am missing something here. Are there some implicit assumptions that I'm not picking up on? Note: a similar question ( Partial derivative for Gaussian Smoothing ) is almost relevant, but assumes instead of .","\mu B^{-1} \succ 0  f_{\mu} (x) = \frac{1}{\kappa} \int_{\mathbb{R}^n} f(x + \mu u) e^{- \frac{1}{2} u^\top B u} \,\mathrm{d}u \tag{9}   \kappa = \int_{\mathbb{R}^n} e^{- \frac{1}{2} u^\top B u} \,\mathrm{d}u \tag{10}.  \nabla f_{\mu} (x) = \frac{1}{\mu \kappa} \int_{\mathbb{R}^n} f(x + \mu u) e^{- \frac{1}{2} u^\top B u} B u \,\mathrm{d} u \tag{21},   \nabla f_{\mu} (x) = \frac{1}{\mu} \mathbb{E}_{u \sim \mathcal{N}(0, B^{-1})} [ f(x + \mu u) B u].   \nabla f_{\mu} (x) = \frac{1}{\mu} \mathbb{E}_{u} [ f(x + \mu u) u ].  \mathcal{N} (0, I) \mathcal{N} (0, B^{-1})","['derivatives', 'probability-distributions', 'expected-value']"
17,Find the Minimum Value of $x^2+y^2$,Find the Minimum Value of,x^2+y^2,"Given: $x+y=2\sin{a}-\cos{b}\\ xy=2\cos{a}+\sin{b}$ Find the minimum value of $x^2+y^2$ . Attempt: $\begin{aligned} x^2+y^2&=(x+y)^2-2xy\\ &=(2\sin{a}-\cos{b})^2-2(2\cos{a}+\sin{b})\\ &=4\sin^2{a}-4\sin{a}\cos{b}+\cos^2{b}-4\cos{a}-2\sin{b} \end{aligned}$ $\begin{aligned} f_{a}(a,b)&=0\\ 8\sin{a}\cos{a}-4\cos{a}\cos{b}+4\sin{a}&=0\\ \cos{a}\cos{b}&=2\sin{a}\sin{b}+\sin{a}\\ \end{aligned}$ $\begin{aligned} f_{b}(a,b)&=0\\ 4\sin{a}\sin{b}-2\sin{b}\cos{b}-2\cos{b}&=0\\ 2\sin{a}\sin{b}&=\sin{b}\cos{b}+\cos{b}\\ \end{aligned}$ Tried to plot it on a graph, the answer should be -6.","Given: Find the minimum value of . Attempt: Tried to plot it on a graph, the answer should be -6.","x+y=2\sin{a}-\cos{b}\\
xy=2\cos{a}+\sin{b} x^2+y^2 \begin{aligned}
x^2+y^2&=(x+y)^2-2xy\\
&=(2\sin{a}-\cos{b})^2-2(2\cos{a}+\sin{b})\\
&=4\sin^2{a}-4\sin{a}\cos{b}+\cos^2{b}-4\cos{a}-2\sin{b}
\end{aligned} \begin{aligned}
f_{a}(a,b)&=0\\
8\sin{a}\cos{a}-4\cos{a}\cos{b}+4\sin{a}&=0\\
\cos{a}\cos{b}&=2\sin{a}\sin{b}+\sin{a}\\
\end{aligned} \begin{aligned}
f_{b}(a,b)&=0\\
4\sin{a}\sin{b}-2\sin{b}\cos{b}-2\cos{b}&=0\\
2\sin{a}\sin{b}&=\sin{b}\cos{b}+\cos{b}\\
\end{aligned}","['derivatives', 'trigonometry']"
18,Differentiate $11x^5 + x^4y + xy^5=18$,Differentiate,11x^5 + x^4y + xy^5=18,"I am not sure how to differentiate $11x^5 + x^4y + xy^5=18$ . I have a little bit of experience with implicit differentiation, but I'm not sure how to handle terms where both variables are multiplied together. I have tried $$\frac{d}{dx}(11x^5 + x^4y+xy^5) = \frac{d}{dx}(18)$$ $$\frac{d}{dx}(11x^5)+\frac{d}{dx}(x^4y) + \frac{d}{dx}(xy^5)=0$$ differentiating each term $$\frac{d}{dx} (11x^5)=11\frac{d}{dy}(5x^4) = 55x^4$$ $$\frac{d}{dx} (x^4y) = [4x^3 \cdot y] + [1 \cdot x^4] = 4yx^3+x^4$$ $$\frac{d}{dx}(xy^5) = [1 \cdot y^5] + [x \cdot 5y^4] = y^5 + 5xy^4$$ finding $\frac{dy}{dx}$ $$\frac{dy}{dx}=\frac{-x}{y} = \frac{-[4yx^3+x^4] + [y^5+5xy^4]}{55x^4}$$ According to the website I'm using, ""WebWork"", this is wrong.","I am not sure how to differentiate . I have a little bit of experience with implicit differentiation, but I'm not sure how to handle terms where both variables are multiplied together. I have tried differentiating each term finding According to the website I'm using, ""WebWork"", this is wrong.",11x^5 + x^4y + xy^5=18 \frac{d}{dx}(11x^5 + x^4y+xy^5) = \frac{d}{dx}(18) \frac{d}{dx}(11x^5)+\frac{d}{dx}(x^4y) + \frac{d}{dx}(xy^5)=0 \frac{d}{dx} (11x^5)=11\frac{d}{dy}(5x^4) = 55x^4 \frac{d}{dx} (x^4y) = [4x^3 \cdot y] + [1 \cdot x^4] = 4yx^3+x^4 \frac{d}{dx}(xy^5) = [1 \cdot y^5] + [x \cdot 5y^4] = y^5 + 5xy^4 \frac{dy}{dx} \frac{dy}{dx}=\frac{-x}{y} = \frac{-[4yx^3+x^4] + [y^5+5xy^4]}{55x^4},"['calculus', 'derivatives', 'implicit-differentiation']"
19,Solving non linear pde $z_x z_y = \frac{x^2 z^3}{y}$,Solving non linear pde,z_x z_y = \frac{x^2 z^3}{y},"solve non linear pde $$z_x z_y = \frac{x^2 z^3}{y}$$ By trial I assumed $$z = kx^p y^q$$ , which on putting in equation and comparing powers will give : $k = pq$ , $p = -3$ and $q = 0$ , this is trivial solution $z = 0$ , but this is not what was sought Is there any way to proceed in such case of non linear pde?","solve non linear pde By trial I assumed , which on putting in equation and comparing powers will give : , and , this is trivial solution , but this is not what was sought Is there any way to proceed in such case of non linear pde?",z_x z_y = \frac{x^2 z^3}{y} z = kx^p y^q k = pq p = -3 q = 0 z = 0,"['calculus', 'derivatives', 'partial-differential-equations']"
20,Confusing Lagrange multipliers question,Confusing Lagrange multipliers question,,"Let $a_1,a_2, \dots, a_n$ be reals, we define a function $f: \mathbb R^n \to \mathbb R$ by $f(x) = \sum_{i=1}^{n}a_ix_i-\sum_{i=1}^{n}x_i\ln(x_i)$ , in addition, we are also given that $0 \cdot \ln(0)$ is defined to be $0$ . Find the supremum of $f$ on the domain $\Omega = \{x \in \mathbb R^n: x_i \geq 0, \sum_{i=1}^{n}x_i = 1\}$ . What I did: $\Omega$ is closed and bounded and hence compact, and $f$ is continuous. It follows that $f$ admits a minimum and maximum (which confuses me as to why they asked for supremum and not maximum). Let's look for the maximum when $x_i > 0$ , by solving the system $\begin{cases}a_i-\ln(x_i)-1 = \lambda \\ \sum_{i=1}^{n}x_i = 1 \end{cases}$ . From the first $n$ equations we get $\ln(x_i) = a_i-1-\lambda$ or in other words $x_i = e^{a_i -1 -\lambda} > 0$ Plugging this to the constraint to find $\lambda$ , we get $\lambda = \ln(\sum_{i=1}^{n}e^{a_i}) - 1$ , so $x_i = e^{a_i-\ln(\sum_{i=1}^{n}e^{a_i})}$ is a potential maximum, or minimum, remember $f$ also attains a minimum. Now we need to check what happens when $x_i$ is zero. This gives us an identical problem, but one dimension smaller. After that we need to check when $(x_i,x_j)$ are zero over all pairs. Then over all triples and so forth. Surely this isn't the intended way. What's going on here?","Let be reals, we define a function by , in addition, we are also given that is defined to be . Find the supremum of on the domain . What I did: is closed and bounded and hence compact, and is continuous. It follows that admits a minimum and maximum (which confuses me as to why they asked for supremum and not maximum). Let's look for the maximum when , by solving the system . From the first equations we get or in other words Plugging this to the constraint to find , we get , so is a potential maximum, or minimum, remember also attains a minimum. Now we need to check what happens when is zero. This gives us an identical problem, but one dimension smaller. After that we need to check when are zero over all pairs. Then over all triples and so forth. Surely this isn't the intended way. What's going on here?","a_1,a_2, \dots, a_n f: \mathbb R^n \to \mathbb R f(x) = \sum_{i=1}^{n}a_ix_i-\sum_{i=1}^{n}x_i\ln(x_i) 0 \cdot \ln(0) 0 f \Omega = \{x \in \mathbb R^n: x_i \geq 0, \sum_{i=1}^{n}x_i = 1\} \Omega f f x_i > 0 \begin{cases}a_i-\ln(x_i)-1 = \lambda \\ \sum_{i=1}^{n}x_i = 1 \end{cases} n \ln(x_i) = a_i-1-\lambda x_i = e^{a_i -1 -\lambda} > 0 \lambda \lambda = \ln(\sum_{i=1}^{n}e^{a_i}) - 1 x_i = e^{a_i-\ln(\sum_{i=1}^{n}e^{a_i})} f x_i (x_i,x_j)","['real-analysis', 'calculus', 'derivatives', 'lagrange-multiplier', 'maxima-minima']"
21,Integral of a function with compact support,Integral of a function with compact support,,I am reading a book with the following statement Since $g$ has compact support $$\sum_{i=1}^d \int_{\mathbb{R}^d} \frac{\partial (fF_i g)}{x_i} dx = 0$$ where $\frac{dx}{dt} = F(x)$ with $x\in \mathbb{R}^d$ $g: \mathbb{R}^d \rightarrow \mathbb{R}$ is continuously differentiable with compact support $f: \mathbb{R}^d \rightarrow \mathbb{R}$ The compact support means that if it is zero outside of a compact set. How can we say the sum of integral is zero?,I am reading a book with the following statement Since has compact support where with is continuously differentiable with compact support The compact support means that if it is zero outside of a compact set. How can we say the sum of integral is zero?,g \sum_{i=1}^d \int_{\mathbb{R}^d} \frac{\partial (fF_i g)}{x_i} dx = 0 \frac{dx}{dt} = F(x) x\in \mathbb{R}^d g: \mathbb{R}^d \rightarrow \mathbb{R} f: \mathbb{R}^d \rightarrow \mathbb{R},"['integration', 'derivatives', 'partial-derivative']"
22,How to prove $\dfrac{d}{dx}e^x=e^x$? [duplicate],How to prove ? [duplicate],\dfrac{d}{dx}e^x=e^x,This question already has answers here : Proving that $\lim\limits_{x \to 0}\frac{e^x-1}{x} = 1$ (8 answers) Closed 5 years ago . I have a problem : How to prove $$\dfrac{d}{dx}e^x=e^x\;?$$ My answer is \begin{eqnarray} \dfrac{d}{dx}e^x&=&\lim\limits_{h\to 0}\dfrac{e^{x+h}-e^x}{h}\\ &=&\lim\limits_{h\to 0}\dfrac{e^{x}e^h-e^x}{h}\\ &=&e^{x}\lim\limits_{h\to 0}\dfrac{e^h-1}{h}\\ \end{eqnarray} But I don't know to find $\lim\limits_{h\to 0}\dfrac{e^h-1}{h}$ . So how to find it? Note:  I cannot use L'Hospital rule.,This question already has answers here : Proving that $\lim\limits_{x \to 0}\frac{e^x-1}{x} = 1$ (8 answers) Closed 5 years ago . I have a problem : How to prove My answer is But I don't know to find . So how to find it? Note:  I cannot use L'Hospital rule.,"\dfrac{d}{dx}e^x=e^x\;? \begin{eqnarray}
\dfrac{d}{dx}e^x&=&\lim\limits_{h\to 0}\dfrac{e^{x+h}-e^x}{h}\\
&=&\lim\limits_{h\to 0}\dfrac{e^{x}e^h-e^x}{h}\\
&=&e^{x}\lim\limits_{h\to 0}\dfrac{e^h-1}{h}\\
\end{eqnarray} \lim\limits_{h\to 0}\dfrac{e^h-1}{h}","['calculus', 'derivatives', 'exponential-function', 'limits-without-lhopital']"
23,Proof Verification: Derivative theorem,Proof Verification: Derivative theorem,,"Suppose that $f: \mathbb{R} \to \mathbb{R}$ is differentiable at $c$ and that $f(c)=0$. Show that $g(x)=|f(x)|$is differentiable at $c$ iff $f'(c)=0$ $\implies$ Suppose $g'(c)$ exists Then we have that  for $x<c$, $\lim_{x \to c}\frac{g(x)-g(c)}{x-c}$ =$\lim_{x \to c}\frac{|f(x)|}{x-c} \le 0$ and for $x>c$, $\lim_{x \to c}\frac{g(x)-g(c)}{x-c}$ =$\lim_{x \to c}\frac{|f(x)|}{x-c} \ge 0$ Then, by squeeze theorem, we have that $g'(c)=0$ Can anyone please tell me how to use the aforementioned result to get to the point where I can show that $f'(c)=0$? $\impliedby$ $f'(c) = 0 \implies \lim_{x \to c}\frac{f(x)}{x-c} =0$ $\implies |f'(c)|=0 = g'(c)$ Can anyone please verify this proof and also help arrive at the result that is emboldened? I'd also appreciate if you could lend some tips/hints to approach such questions and if there are easier ways to proof this. Thank you.","Suppose that $f: \mathbb{R} \to \mathbb{R}$ is differentiable at $c$ and that $f(c)=0$. Show that $g(x)=|f(x)|$is differentiable at $c$ iff $f'(c)=0$ $\implies$ Suppose $g'(c)$ exists Then we have that  for $x<c$, $\lim_{x \to c}\frac{g(x)-g(c)}{x-c}$ =$\lim_{x \to c}\frac{|f(x)|}{x-c} \le 0$ and for $x>c$, $\lim_{x \to c}\frac{g(x)-g(c)}{x-c}$ =$\lim_{x \to c}\frac{|f(x)|}{x-c} \ge 0$ Then, by squeeze theorem, we have that $g'(c)=0$ Can anyone please tell me how to use the aforementioned result to get to the point where I can show that $f'(c)=0$? $\impliedby$ $f'(c) = 0 \implies \lim_{x \to c}\frac{f(x)}{x-c} =0$ $\implies |f'(c)|=0 = g'(c)$ Can anyone please verify this proof and also help arrive at the result that is emboldened? I'd also appreciate if you could lend some tips/hints to approach such questions and if there are easier ways to proof this. Thank you.",,"['real-analysis', 'calculus', 'derivatives', 'proof-verification', 'proof-explanation']"
24,What is the solution set of the system $x^x + y^y = 31$ and $x + y = 5$?,What is the solution set of the system  and ?,x^x + y^y = 31 x + y = 5,"It may be easily seen (heuristically) that the only solution (in $\mathbb R$) of the system $x^x + y^y = 31$ and $ x + y = 5$ is the set containing $ (2, 3)$ and $(3, 2)$. My question is, how may one go about demonstrating that those are indeed the only intersection points of the two equations? As I said above, I can see why this is so, intuitively, but can't seem to put it down properly. Thank you.","It may be easily seen (heuristically) that the only solution (in $\mathbb R$) of the system $x^x + y^y = 31$ and $ x + y = 5$ is the set containing $ (2, 3)$ and $(3, 2)$. My question is, how may one go about demonstrating that those are indeed the only intersection points of the two equations? As I said above, I can see why this is so, intuitively, but can't seem to put it down properly. Thank you.",,"['calculus', 'real-analysis', 'derivatives', 'exponential-function', 'systems-of-equations']"
25,A positive function which even derivatives are positive and odd derivatives negative,A positive function which even derivatives are positive and odd derivatives negative,,"Is there any function $f(x)$ which satisfies the following criterion for positive values of $x$ : being positive ($f(x)>0$) having negative odd derivatives ($f^{2k+1}(x)<0$) having positive even derivatives ($f^{2k}(x)\gt0$) The exponential function $f:x\rightarrow e^{-x}$ does satisfy such conditions but I would like to have a function that could be used to fit some experimental data. Therefore, I want $f(x)$ to have at least 3 parameters that can be adjusted and which optimal values over the experimental data will ensure the previously mentioned inequalities. For example, $f:x\rightarrow x^{c_1}\exp \left[c_2 (1-x^{c_3})\right]$ has 3 parameters $c_i$ but does not satisfy the constraints previously mentioned for all values of $c_i$ and therefore is not good for me. Any idea ?","Is there any function $f(x)$ which satisfies the following criterion for positive values of $x$ : being positive ($f(x)>0$) having negative odd derivatives ($f^{2k+1}(x)<0$) having positive even derivatives ($f^{2k}(x)\gt0$) The exponential function $f:x\rightarrow e^{-x}$ does satisfy such conditions but I would like to have a function that could be used to fit some experimental data. Therefore, I want $f(x)$ to have at least 3 parameters that can be adjusted and which optimal values over the experimental data will ensure the previously mentioned inequalities. For example, $f:x\rightarrow x^{c_1}\exp \left[c_2 (1-x^{c_3})\right]$ has 3 parameters $c_i$ but does not satisfy the constraints previously mentioned for all values of $c_i$ and therefore is not good for me. Any idea ?",,"['derivatives', 'parametric', 'constraints']"
26,Meaning of functional differentiability,Meaning of functional differentiability,,"I just began studying variational calculus, and I'm having some issues getting a conceptual grasp on functional differentiability. Let $J[y]$ be a functional defined on some normed linear space, and let   $$\Delta J [h] = J [y+h]-J [y] $$   Suppose that   $$\Delta J[h] = \phi [h] + \epsilon ||h||$$   Where $\phi [h] $ is a linear functional and $\epsilon \to 0$ as $||h|| \to 0$. Then $J [h]$ is said to be differentiable . From the definition of $\Delta J [h] $, it is clear that we're concerned with finding how $J $ varies as its argument changes (very analogous to traditional differentiation). But I don't think we could graph this result, as infinitely many functions will have the same norm (graphing the tangent line is generally the easiest way to understand a derivative in Real Analysis), so I'm struggling to see the full meaning of this definition. For our norm, we are using $||h|| = \max_{a \le x \le b} |h(x)|$. If $||h|| \to 0$, then $h$ approaches the zero function, so that $y+h \to y$, which is consistent with ordinary derivatives. But why would the fact that we can write $\Delta J[h] = \phi [h] + \epsilon  ||h||$ correspond to differentiability (perhaps I'm trying to relate it too much to real analysis here)? It seems to say that the if a (generally) nonlinear functional is differentiable, we can relate 'small' changes in $J$ to some linear functional  (sort of similar to a linear approximation, I suppose), as: $$\frac {\Delta J [h]-\phi [h]}{||h||} = \epsilon $$ If $||h|| \to 0$ and $\epsilon \to 0$, it necessarily demands that $\Delta J [h] - \phi [h] \to 0$ What is the purpose of having this class of functionals and how does it relate to how a general functional changes with respect to its variable?","I just began studying variational calculus, and I'm having some issues getting a conceptual grasp on functional differentiability. Let $J[y]$ be a functional defined on some normed linear space, and let   $$\Delta J [h] = J [y+h]-J [y] $$   Suppose that   $$\Delta J[h] = \phi [h] + \epsilon ||h||$$   Where $\phi [h] $ is a linear functional and $\epsilon \to 0$ as $||h|| \to 0$. Then $J [h]$ is said to be differentiable . From the definition of $\Delta J [h] $, it is clear that we're concerned with finding how $J $ varies as its argument changes (very analogous to traditional differentiation). But I don't think we could graph this result, as infinitely many functions will have the same norm (graphing the tangent line is generally the easiest way to understand a derivative in Real Analysis), so I'm struggling to see the full meaning of this definition. For our norm, we are using $||h|| = \max_{a \le x \le b} |h(x)|$. If $||h|| \to 0$, then $h$ approaches the zero function, so that $y+h \to y$, which is consistent with ordinary derivatives. But why would the fact that we can write $\Delta J[h] = \phi [h] + \epsilon  ||h||$ correspond to differentiability (perhaps I'm trying to relate it too much to real analysis here)? It seems to say that the if a (generally) nonlinear functional is differentiable, we can relate 'small' changes in $J$ to some linear functional  (sort of similar to a linear approximation, I suppose), as: $$\frac {\Delta J [h]-\phi [h]}{||h||} = \epsilon $$ If $||h|| \to 0$ and $\epsilon \to 0$, it necessarily demands that $\Delta J [h] - \phi [h] \to 0$ What is the purpose of having this class of functionals and how does it relate to how a general functional changes with respect to its variable?",,"['derivatives', 'calculus-of-variations', 'frechet-derivative', 'gateaux-derivative']"
27,Studying the derivative of the integral function $\frac{1}{x}\int_0^x\arctan(e^t)\mathrm{d}t$,Studying the derivative of the integral function,\frac{1}{x}\int_0^x\arctan(e^t)\mathrm{d}t,"I was trying to calculate the derivative of the function $$ F(x) =\frac{1}{x}\int_0^x\arctan(e^t)\mathrm{d}t $$ I thought the fastest way was to use the Leibniz's rule for the derivative of a product, $$ (f\cdot g)' = f'g + g'f $$ and, choosing as $f(x) = \frac{1}{x}$ and as $g(x) = \int_0^x\arctan(e^t)\mathrm{d}t$, applying for the second one's derivative the fundamental theorem of calculus, I obtained $$ -\frac{1}{x^2}\int_0^x\arctan(e^t)\mathrm{d}t + \frac{1}{x}\left[\arctan(e^t)\right]\Bigg|_{t = 0}^{t = x} = -\frac{1}{x^2}\int_0^x\arctan(e^t)\mathrm{d}t + \frac{1}{x}\left[\arctan(e^x)-\frac{\pi}{4}\right] $$ Now there come the problems, since I don't know how to evaluate the limit as $x\rightarrow0$ for the first term of the expression, while the second one, as $x\rightarrow0$, $g'(x)f(x)\rightarrow\frac{1}{2}$. So I plotted the whole thing and I saw something very strange: The blue one is the function (which is right), the red one it's the derivative as calculated before. As you can notice looks like the derivative have a discontinuity in the point 0, while looking at the graph of the function $F(x)$ one would say that there's not such a discontinuity. I tried to evaluate the whole thing with Mathematica, but I did not solve the problem: there are strange things happening at the origin. Now, there are two possibilities: The derivative is wrong, but I wonder where, as it's so simple and linear Grapher app from Mac OS X cannot handle with such functions in a proper way Can you find out the bug?","I was trying to calculate the derivative of the function $$ F(x) =\frac{1}{x}\int_0^x\arctan(e^t)\mathrm{d}t $$ I thought the fastest way was to use the Leibniz's rule for the derivative of a product, $$ (f\cdot g)' = f'g + g'f $$ and, choosing as $f(x) = \frac{1}{x}$ and as $g(x) = \int_0^x\arctan(e^t)\mathrm{d}t$, applying for the second one's derivative the fundamental theorem of calculus, I obtained $$ -\frac{1}{x^2}\int_0^x\arctan(e^t)\mathrm{d}t + \frac{1}{x}\left[\arctan(e^t)\right]\Bigg|_{t = 0}^{t = x} = -\frac{1}{x^2}\int_0^x\arctan(e^t)\mathrm{d}t + \frac{1}{x}\left[\arctan(e^x)-\frac{\pi}{4}\right] $$ Now there come the problems, since I don't know how to evaluate the limit as $x\rightarrow0$ for the first term of the expression, while the second one, as $x\rightarrow0$, $g'(x)f(x)\rightarrow\frac{1}{2}$. So I plotted the whole thing and I saw something very strange: The blue one is the function (which is right), the red one it's the derivative as calculated before. As you can notice looks like the derivative have a discontinuity in the point 0, while looking at the graph of the function $F(x)$ one would say that there's not such a discontinuity. I tried to evaluate the whole thing with Mathematica, but I did not solve the problem: there are strange things happening at the origin. Now, there are two possibilities: The derivative is wrong, but I wonder where, as it's so simple and linear Grapher app from Mac OS X cannot handle with such functions in a proper way Can you find out the bug?",,"['calculus', 'real-analysis', 'integration', 'derivatives']"
28,Prove there exists $t>0$ such that $\cos(t) < 0$.,Prove there exists  such that .,t>0 \cos(t) < 0,"I want to prove that there exists a positive real number $t$ such that $\cos(t)$ is negative. Here's what I know $$\cos(x) := \sum_{n=0}^\infty{x^{2n}(-1)^n\over(2n)!}, \;\;(x\in\mathbb R)$$ $${d\over dx}\cos(x) = -\sin(x)$$ $$\cos\left({\pi\over2}\right) = 0, \;\; \cos(0) = 1,$$ $$\sin\left({\pi\over2}\right) = 1, \;\; \sin(0) = 0.$$ I should also specify : The only thing I know about ${\pi\over2}$ is that it is the smallest positive number such that $\cos(\cdot)$ vanishes. Here's what I tried so far: Since ${d\over dx}\cos(x) = -\sin(x)$ for all $x\in\mathbb R$, we use the fact that $\sin(\pi/2) = 1$ to give us $$\left.{d\over dx}\cos\left(x\right)\right|_{x = {\pi\over2}} = -\sin\left({\pi\over2}\right) = -1.$$ So $\cos(x)$ is decreasing at $x={\pi\over2}$. Using the fact that $\sin(\cdot)$ and $\cos(\cdot)$ are continuous (since differentiable $\implies$ continuous), we know that ( and here's where I'm not sure ) there exists $\epsilon > 0 $ such that $\cos\left({\pi\over2} + \epsilon\right) < 0$. Call $t = {\pi\over2} + \epsilon > 0$. This completes the proof. Is there enough justification to make this claim? Since $\sin(\cdot)$ is continuous on $\mathbb R$, for some $\epsilon > 0$ we have $\left.{d\over dx}\cos(x + \epsilon)\right|_{x={\pi\over2}} = -\sin({\pi\over2}+\epsilon) < 0$. Hence $\cos(\cdot)$ is still decreasing at ${\pi\over2} + \epsilon$, and since $\cos({\pi\over2})=0$, it must be that $\cos(t) = \cos({\pi\over2}+\epsilon) < 0 $.","I want to prove that there exists a positive real number $t$ such that $\cos(t)$ is negative. Here's what I know $$\cos(x) := \sum_{n=0}^\infty{x^{2n}(-1)^n\over(2n)!}, \;\;(x\in\mathbb R)$$ $${d\over dx}\cos(x) = -\sin(x)$$ $$\cos\left({\pi\over2}\right) = 0, \;\; \cos(0) = 1,$$ $$\sin\left({\pi\over2}\right) = 1, \;\; \sin(0) = 0.$$ I should also specify : The only thing I know about ${\pi\over2}$ is that it is the smallest positive number such that $\cos(\cdot)$ vanishes. Here's what I tried so far: Since ${d\over dx}\cos(x) = -\sin(x)$ for all $x\in\mathbb R$, we use the fact that $\sin(\pi/2) = 1$ to give us $$\left.{d\over dx}\cos\left(x\right)\right|_{x = {\pi\over2}} = -\sin\left({\pi\over2}\right) = -1.$$ So $\cos(x)$ is decreasing at $x={\pi\over2}$. Using the fact that $\sin(\cdot)$ and $\cos(\cdot)$ are continuous (since differentiable $\implies$ continuous), we know that ( and here's where I'm not sure ) there exists $\epsilon > 0 $ such that $\cos\left({\pi\over2} + \epsilon\right) < 0$. Call $t = {\pi\over2} + \epsilon > 0$. This completes the proof. Is there enough justification to make this claim? Since $\sin(\cdot)$ is continuous on $\mathbb R$, for some $\epsilon > 0$ we have $\left.{d\over dx}\cos(x + \epsilon)\right|_{x={\pi\over2}} = -\sin({\pi\over2}+\epsilon) < 0$. Hence $\cos(\cdot)$ is still decreasing at ${\pi\over2} + \epsilon$, and since $\cos({\pi\over2})=0$, it must be that $\cos(t) = \cos({\pi\over2}+\epsilon) < 0 $.",,"['real-analysis', 'derivatives', 'proof-verification', 'trigonometric-series']"
29,Does the second derivative of a convex function exist at all but countably many points?,Does the second derivative of a convex function exist at all but countably many points?,,"I know that the second derivative of a convex function exists almost everywhere ( Alexandrov's theorem ). And I know that the first derivative exists everywhere except countably many points ( This question ). So, does the second derivative exist everywhere except countably many points? I'd appreciate a source I can cite, or a counterexample.","I know that the second derivative of a convex function exists almost everywhere ( Alexandrov's theorem ). And I know that the first derivative exists everywhere except countably many points ( This question ). So, does the second derivative exist everywhere except countably many points? I'd appreciate a source I can cite, or a counterexample.",,"['real-analysis', 'derivatives', 'convex-analysis']"
30,Second derivative of the cost function of logistic function,Second derivative of the cost function of logistic function,,Do I have the correct solution for the second derivative of the cost function of a logistic function? Cost Function $$J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{i}\log(h_\theta(x^{i}))+(1-y^{i})\log(1-h_\theta(x^{i}))$$ where $h_{\theta}(x)$ is defined as follows $$h_{\theta}(x)=g(\theta^{T}x)$$ $$g(z)=\frac{1}{1+e^{-z}}$$ First Derivative $$ \frac{\partial}{\partial\theta_{j}}J(\theta) =\sum_{i=1}^{m}(h_\theta(x^{i})-y^i)x_j^i$$ Second Derivative  $$ \begin{align*} \frac{\partial}{\partial^2\theta_{j}}J'(\theta) &= \frac{\partial}{\partial\theta}\sum_{i=1}^{m}(h_\theta(x^{i})x_j^i -y^ix^i_j) \\ &= \frac{\partial}{\partial\theta}\sum_{i=1}^{m}(h_\theta(x^{i})x_j^i) \\ &= \frac{\partial}{\partial\theta}\sum_{i=1}^{m}\frac{x^{i}}{1+e^{-z}} \\ &= x^2 h_\theta(x) ^2 \end{align*}  $$,Do I have the correct solution for the second derivative of the cost function of a logistic function? Cost Function $$J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{i}\log(h_\theta(x^{i}))+(1-y^{i})\log(1-h_\theta(x^{i}))$$ where $h_{\theta}(x)$ is defined as follows $$h_{\theta}(x)=g(\theta^{T}x)$$ $$g(z)=\frac{1}{1+e^{-z}}$$ First Derivative $$ \frac{\partial}{\partial\theta_{j}}J(\theta) =\sum_{i=1}^{m}(h_\theta(x^{i})-y^i)x_j^i$$ Second Derivative  $$ \begin{align*} \frac{\partial}{\partial^2\theta_{j}}J'(\theta) &= \frac{\partial}{\partial\theta}\sum_{i=1}^{m}(h_\theta(x^{i})x_j^i -y^ix^i_j) \\ &= \frac{\partial}{\partial\theta}\sum_{i=1}^{m}(h_\theta(x^{i})x_j^i) \\ &= \frac{\partial}{\partial\theta}\sum_{i=1}^{m}\frac{x^{i}}{1+e^{-z}} \\ &= x^2 h_\theta(x) ^2 \end{align*}  $$,,"['derivatives', 'partial-derivative', 'regression', 'machine-learning']"
31,Differentiation of the euclidean norm of $\Vert Ax+b\Vert^{2}$,Differentiation of the euclidean norm of,\Vert Ax+b\Vert^{2},"Let $A$ be an $m\times n$ real matrix, $x$ an $n\times 1$ vector and $b$ an $m\times 1$ vector. I want to compute \begin{equation} \dfrac{\partial }{\partial x} \Vert Ax+b\Vert^{2}. \end{equation} First, I expanded \begin{equation} \Vert Ax+b\Vert^{2}=(Ax+b)^{T}(Ax+b)=x^{T}A^{T}Ax+2x^{T}A^{T}b+b^{T}b \end{equation} then I computed \begin{eqnarray} \dfrac{\partial }{\partial x}(x^{T}A^{T}Ax+2x^{T}A^{T}b+b^{T}b)=A^{T}Ax+x^{T}A^{T}A+2A^{T}b \end{eqnarray} but I know the above is wrong since $A^{T}Ax$ and $x^{T}A^{T}A$ does not have the same dimention. Thanks for the help.","Let $A$ be an $m\times n$ real matrix, $x$ an $n\times 1$ vector and $b$ an $m\times 1$ vector. I want to compute \begin{equation} \dfrac{\partial }{\partial x} \Vert Ax+b\Vert^{2}. \end{equation} First, I expanded \begin{equation} \Vert Ax+b\Vert^{2}=(Ax+b)^{T}(Ax+b)=x^{T}A^{T}Ax+2x^{T}A^{T}b+b^{T}b \end{equation} then I computed \begin{eqnarray} \dfrac{\partial }{\partial x}(x^{T}A^{T}Ax+2x^{T}A^{T}b+b^{T}b)=A^{T}Ax+x^{T}A^{T}A+2A^{T}b \end{eqnarray} but I know the above is wrong since $A^{T}Ax$ and $x^{T}A^{T}A$ does not have the same dimention. Thanks for the help.",,"['calculus', 'linear-algebra', 'derivatives', 'normed-spaces']"
32,How to calculate set of subgradients [duplicate],How to calculate set of subgradients [duplicate],,"This question already has an answer here : What is the subgradient of $f(x)=\max_i a_i^T x+b_i$? (1 answer) Closed last month . I am trying to understand question 1(a) here . They calculate a subgradient at a given $x$ for the following convex function: $$f(x) =\max_{i=1, ..., m} (a_i^Tx + b_i)$$ The solution is to find $k \in \{1,...m\}$ for which $f(x) = (a_k^T x + b_k)$, then the subgradient is $a_k$. I understand the solution. However, it is only one solution. The size of the set of subgradients is infinite. Therefore, I wonder, what are all the other subgradients? How can I calculate other subgradients from the set? Thanks.","This question already has an answer here : What is the subgradient of $f(x)=\max_i a_i^T x+b_i$? (1 answer) Closed last month . I am trying to understand question 1(a) here . They calculate a subgradient at a given $x$ for the following convex function: $$f(x) =\max_{i=1, ..., m} (a_i^Tx + b_i)$$ The solution is to find $k \in \{1,...m\}$ for which $f(x) = (a_k^T x + b_k)$, then the subgradient is $a_k$. I understand the solution. However, it is only one solution. The size of the set of subgradients is infinite. Therefore, I wonder, what are all the other subgradients? How can I calculate other subgradients from the set? Thanks.",,"['calculus', 'derivatives', 'optimization', 'convex-analysis', 'subgradient']"
33,Differentiability at 0 for modified Dirichlet Function,Differentiability at 0 for modified Dirichlet Function,,"Consider the modified Dirichlet function $f$ where $f(x) = x$ if $x$ is rational and $f(x) = 0$ otherwise. I have managed to prove that $f$ is continuous at and only at $0$. My question is, then, ""Is $f$ differentiable at $0$?"" I have had a hard time proving this one way or the other using the following definition of the derivative at a point $c$: $\lim\limits_{x \to c} \frac{f(x)-f(c)}{x-c}$. I'd appreciate any help.","Consider the modified Dirichlet function $f$ where $f(x) = x$ if $x$ is rational and $f(x) = 0$ otherwise. I have managed to prove that $f$ is continuous at and only at $0$. My question is, then, ""Is $f$ differentiable at $0$?"" I have had a hard time proving this one way or the other using the following definition of the derivative at a point $c$: $\lim\limits_{x \to c} \frac{f(x)-f(c)}{x-c}$. I'd appreciate any help.",,"['calculus', 'derivatives']"
34,"Can the second derivative of a function be interpreted as the slope of its ""concavity lines""?","Can the second derivative of a function be interpreted as the slope of its ""concavity lines""?",,"Can the second derivative of a function be interpreted as the slope of its ""concavity lines""? For example consider the following picture: Does $f''$ for each point $x$ that corresponds to an arrow being drawn (for which there are $9$ in the picature) itself correspond to the slope of the line that would be obtained by extending its concavity arrow into a line?","Can the second derivative of a function be interpreted as the slope of its ""concavity lines""? For example consider the following picture: Does $f''$ for each point $x$ that corresponds to an arrow being drawn (for which there are $9$ in the picature) itself correspond to the slope of the line that would be obtained by extending its concavity arrow into a line?",,['derivatives']
35,On an injective ring homomorphism from the ring of continuous functions to the ring of differentiable functions,On an injective ring homomorphism from the ring of continuous functions to the ring of differentiable functions,,"Let $\phi : C \to D$ be an injective ring homomorphism such that $\phi(1)=1$, where $1$ denotes the constant function $1$ and $C$, $D$ are the rings of continuous, respectively differentiable functions on $\mathbb{R}$. If $f \in C ,g \in D$ are such that $\phi(f)=g$, and $t_0\in \mathbb R$, is it true that $\phi (f-g(t_0))=g-g(t_0)$ ? This is used on page 944, Problem 11617 . And why if the image of $\phi $ consists of only constant functions, then $\phi$ is not injective ? Please help. Thanks in advance.","Let $\phi : C \to D$ be an injective ring homomorphism such that $\phi(1)=1$, where $1$ denotes the constant function $1$ and $C$, $D$ are the rings of continuous, respectively differentiable functions on $\mathbb{R}$. If $f \in C ,g \in D$ are such that $\phi(f)=g$, and $t_0\in \mathbb R$, is it true that $\phi (f-g(t_0))=g-g(t_0)$ ? This is used on page 944, Problem 11617 . And why if the image of $\phi $ consists of only constant functions, then $\phi$ is not injective ? Please help. Thanks in advance.",,"['real-analysis', 'derivatives']"
36,Matrix derivative quadratic form - product rule,Matrix derivative quadratic form - product rule,,I am trying to find the derivative of the expression below using product rule but I am unable to do so. Below is my solution. $ \frac{d}{dx}(x^TAx) \\= \frac{d}{dx}x^T(Ax) + (x^T)\frac{d}{dx}Ax \\  = Ax +x^TA$ Im unable to get $x^T(A+A^T)$. Help thanks.,I am trying to find the derivative of the expression below using product rule but I am unable to do so. Below is my solution. $ \frac{d}{dx}(x^TAx) \\= \frac{d}{dx}x^T(Ax) + (x^T)\frac{d}{dx}Ax \\  = Ax +x^TA$ Im unable to get $x^T(A+A^T)$. Help thanks.,,"['derivatives', 'matrix-calculus', 'quadratic-forms']"
37,Differentiability of function for $\Bbb{Q}$ and $\Bbb{R}\setminus \Bbb{Q}$,Differentiability of function for  and,\Bbb{Q} \Bbb{R}\setminus \Bbb{Q},"A function $f:\Bbb{R}\to\Bbb{R}$ is defined by $f(x)=x$, if $x$ is rational; $\sin(x)$ if $x$ is irrational. Show that $f$ is differentiable at $0$ and $f'(0)=1$. Here I'm thinking to apply definition separately for rational and irrational.. and I'm getting same limit... But can we conclude the differentiability with this result??","A function $f:\Bbb{R}\to\Bbb{R}$ is defined by $f(x)=x$, if $x$ is rational; $\sin(x)$ if $x$ is irrational. Show that $f$ is differentiable at $0$ and $f'(0)=1$. Here I'm thinking to apply definition separately for rational and irrational.. and I'm getting same limit... But can we conclude the differentiability with this result??",,"['real-analysis', 'derivatives']"
38,Discontinuous derivative *not* by oscillation,Discontinuous derivative *not* by oscillation,,"All the differentiable functions that I have ever seen whose derivative is discontinuous, achieve this discontinuity by oscillating: See, e.g., this question . Is it possible to construct differentiable functions where the discontinuity of the derivative is achieved by some other way? (The basic idea achieving discontinuous behavior by osciallation is: Let it oscillate around the point $x_0$ where we want our derivative to be discontinuous, but decrease the amplitude as we approach $x_0$ so that we can achieve differentiability. Then the derivative will oscillate as well, but will not descrease in amplitude, so will be discontinuous at $x_0$.)","All the differentiable functions that I have ever seen whose derivative is discontinuous, achieve this discontinuity by oscillating: See, e.g., this question . Is it possible to construct differentiable functions where the discontinuity of the derivative is achieved by some other way? (The basic idea achieving discontinuous behavior by osciallation is: Let it oscillate around the point $x_0$ where we want our derivative to be discontinuous, but decrease the amplitude as we approach $x_0$ so that we can achieve differentiability. Then the derivative will oscillate as well, but will not descrease in amplitude, so will be discontinuous at $x_0$.)",,['derivatives']
39,Sum of partial derivatives,Sum of partial derivatives,,"Suppose that $$\mu_i(x)=x_i \int_0^1 t^{n-1} \rho(tx) dt$$ where $\rho$ is a function on $\mathbb R^n$ and $tx=(tx_1,\dots,tx_n)\in \mathbb R^n$. Show that $$\sum_{i=1}^n \frac{\partial\mu_i}{\partial x_i}=\rho .$$ This problem looks simple, but I am having difficulty in showing the result. I guess that the first step is to find $\frac{\partial\mu_i}{\partial x_i}$ using the product rule.","Suppose that $$\mu_i(x)=x_i \int_0^1 t^{n-1} \rho(tx) dt$$ where $\rho$ is a function on $\mathbb R^n$ and $tx=(tx_1,\dots,tx_n)\in \mathbb R^n$. Show that $$\sum_{i=1}^n \frac{\partial\mu_i}{\partial x_i}=\rho .$$ This problem looks simple, but I am having difficulty in showing the result. I guess that the first step is to find $\frac{\partial\mu_i}{\partial x_i}$ using the product rule.",,"['calculus', 'derivatives']"
40,"Given a function $f(x)$ defined for all real $x$,and is such that $f(x+h)-f(x)<6h^2$ for all real $h$ and $x.$Show that $f(x)$ is constant","Given a function  defined for all real ,and is such that  for all real  and Show that  is constant",f(x) x f(x+h)-f(x)<6h^2 h x. f(x),"Given a function $f(x)$ defined for all real $x$,and is such that $$f(x+h)-f(x)<6h^2$$ for all real $h$ and $x.$Show that $f(x)$ is constant. To prove $f(x)$ as constant i need to prove that $f'(x)=0.$ $f'(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}<\lim_{h\to 0} 6h$ $f'(x)<0$ I am not able to prove $f'(x)$ equal to zero.Please help me.Thanks.","Given a function $f(x)$ defined for all real $x$,and is such that $$f(x+h)-f(x)<6h^2$$ for all real $h$ and $x.$Show that $f(x)$ is constant. To prove $f(x)$ as constant i need to prove that $f'(x)=0.$ $f'(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}<\lim_{h\to 0} 6h$ $f'(x)<0$ I am not able to prove $f'(x)$ equal to zero.Please help me.Thanks.",,"['calculus', 'real-analysis', 'derivatives']"
41,"$f:\mathbb R \to \mathbb R$ be differentiable such that $f(0)=0$ and $f'(x)>f(x),\forall x \in \mathbb R$ ; then is $f(x)>0,\forall x>0$?",be differentiable such that  and  ; then is ?,"f:\mathbb R \to \mathbb R f(0)=0 f'(x)>f(x),\forall x \in \mathbb R f(x)>0,\forall x>0","Let $f:\mathbb R \to \mathbb R$ be a differentiable function such that $f(0)=0$ and $f'(x)>f(x),\forall x \in \mathbb R$ ; then is it true that $f(x)>0,\forall x>0$ ?","Let $f:\mathbb R \to \mathbb R$ be a differentiable function such that $f(0)=0$ and $f'(x)>f(x),\forall x \in \mathbb R$ ; then is it true that $f(x)>0,\forall x>0$ ?",,['real-analysis']
42,"If $f:[0,\infty) \to \mathbb{R}$ be a differentiable function with $f(0)=1$ . . . Show that $f'(x)\geq e^x$ for all $x>0$.",If  be a differentiable function with  . . . Show that  for all .,"f:[0,\infty) \to \mathbb{R} f(0)=1 f'(x)\geq e^x x>0","If $f:[0,\infty) \to \mathbb{R}$ be a differentiable function with $f(0)=1$ and $f'(x)\geq f(x)$ for all $x>0$. Show that $f'(x)\geq e^x$ for all $x>0$. Hint: Consider $g(x)=e^{-x}f(x)$. My main concern is what does $g(x)$ have to do with anything. I feel if I understood its relevance, then I could solve the problem. I'm just not sure how I can declare it's relevance to the problem at hand. Edit: charlestoncrabb is correct on the proof, I'm just not certain of the relevance of $g(x)$","If $f:[0,\infty) \to \mathbb{R}$ be a differentiable function with $f(0)=1$ and $f'(x)\geq f(x)$ for all $x>0$. Show that $f'(x)\geq e^x$ for all $x>0$. Hint: Consider $g(x)=e^{-x}f(x)$. My main concern is what does $g(x)$ have to do with anything. I feel if I understood its relevance, then I could solve the problem. I'm just not sure how I can declare it's relevance to the problem at hand. Edit: charlestoncrabb is correct on the proof, I'm just not certain of the relevance of $g(x)$",,"['real-analysis', 'derivatives']"
43,Limitations of fractional derivative approximation with Taylor series,Limitations of fractional derivative approximation with Taylor series,,"I was playing around with the concept of fraction derivatives, and came across some base functions for which it is defined, namely power and exponential functions $$ \left(\frac{d}{dt}\right)^\alpha t^k = \frac{\Gamma(k+1)}{\Gamma(k-\alpha+1)} t^{k-\alpha}, \quad k\geq0 \tag{1} $$ $$ \left(\frac{d}{dt}\right)^\alpha e^{kt} = k^\alpha e^{kt} = e^{kt + \alpha \log(k)}. \tag{2} $$ I was wondering what the fractional derivative would be of $\sin(\omega t)$ and initially though that if I would calculate its Taylor series at $t=0$, I then could use (1) to find its fraction derivative. But later I came across (2) and realized that in this case it can be found much easier. Namely $\sin(\omega t)$ can be written as $$ \sin(\omega t) = \frac{1}{2i} \left(e^{i\omega t} - e^{-i\omega t}\right), \tag{3} $$ thus the fractional derivative can be found with (2) $$ \left(\frac{d}{dt}\right)^\alpha \sin(\omega t) = \frac{1}{2i} \left(e^{i\omega t + \alpha \log(i\omega)} - e^{-i\omega t + \alpha \log(-i\omega)}\right) = \omega^\alpha \sin\left(\omega t + \alpha \frac{\pi}{2}\right). \tag{4} $$ But if I compare this with the result I get when using the Taylor series I get very wrong results for $t<0$ and for $t$ slightly larger than $0$ I get a transition towards the correct result. For example here are the results I get when $\alpha=\frac 12$, $\omega=1$ and the Taylor series is approximated with 50 terms: Could it be that (1) is only true for $t>0$ and that taking the factional derivative of a Taylor series might not have a convergence near the point at which the Taylor series is constructed, in my case $t=0$?","I was playing around with the concept of fraction derivatives, and came across some base functions for which it is defined, namely power and exponential functions $$ \left(\frac{d}{dt}\right)^\alpha t^k = \frac{\Gamma(k+1)}{\Gamma(k-\alpha+1)} t^{k-\alpha}, \quad k\geq0 \tag{1} $$ $$ \left(\frac{d}{dt}\right)^\alpha e^{kt} = k^\alpha e^{kt} = e^{kt + \alpha \log(k)}. \tag{2} $$ I was wondering what the fractional derivative would be of $\sin(\omega t)$ and initially though that if I would calculate its Taylor series at $t=0$, I then could use (1) to find its fraction derivative. But later I came across (2) and realized that in this case it can be found much easier. Namely $\sin(\omega t)$ can be written as $$ \sin(\omega t) = \frac{1}{2i} \left(e^{i\omega t} - e^{-i\omega t}\right), \tag{3} $$ thus the fractional derivative can be found with (2) $$ \left(\frac{d}{dt}\right)^\alpha \sin(\omega t) = \frac{1}{2i} \left(e^{i\omega t + \alpha \log(i\omega)} - e^{-i\omega t + \alpha \log(-i\omega)}\right) = \omega^\alpha \sin\left(\omega t + \alpha \frac{\pi}{2}\right). \tag{4} $$ But if I compare this with the result I get when using the Taylor series I get very wrong results for $t<0$ and for $t$ slightly larger than $0$ I get a transition towards the correct result. For example here are the results I get when $\alpha=\frac 12$, $\omega=1$ and the Taylor series is approximated with 50 terms: Could it be that (1) is only true for $t>0$ and that taking the factional derivative of a Taylor series might not have a convergence near the point at which the Taylor series is constructed, in my case $t=0$?",,"['derivatives', 'taylor-expansion']"
44,Finding the $n$th derivative of trigonometric function..,Finding the th derivative of trigonometric function..,n,"My maths teacher has asked me to find the $n$th derivative of $\cos^9(x)$. He gave us a hint which are as follows: if $t=\cos x + i\sin x$,    $1/t=\cos x - i\sin x$,    then $2\cos x=(t+1/t)$. How am I supposed to solve this? Please help me with explanations because I am not good at this. And yes he's taught us Leibniz Theorem. Thanks.","My maths teacher has asked me to find the $n$th derivative of $\cos^9(x)$. He gave us a hint which are as follows: if $t=\cos x + i\sin x$,    $1/t=\cos x - i\sin x$,    then $2\cos x=(t+1/t)$. How am I supposed to solve this? Please help me with explanations because I am not good at this. And yes he's taught us Leibniz Theorem. Thanks.",,"['calculus', 'trigonometry', 'derivatives']"
45,The least value of the function $f(x)=|x-a|+|x-b|+|x-c|+|x-d|$ [duplicate],The least value of the function  [duplicate],f(x)=|x-a|+|x-b|+|x-c|+|x-d|,"This question already has answers here : The median minimizes the sum of absolute deviations (the $ {\ell}_{1} $ norm) (11 answers) Closed 8 years ago . If $a<b<c<d$ and $x\in\mathbb R$ then what is the least value of the function $$f(x)=|x-a|+|x-b|+|x-c|+|x-d|\ ?$$ $f(x)= \begin{cases}        a-x+b-x+c-x+d-x & x\leq a \\       x-a+b-x+c-x+d-x & a< x\leq b \\       x-a+x-b+c-x+d-x & b< x\leq c \\       x-a+x-b+x-c+d-x & c< x\leq d \\       x-a+x-b+x-c+x-d & x> d    \end{cases}$ then $f'(x)=\begin{cases}        -4 & x\leq a \\       -2 & a< x\leq b \\       0 & b< x\leq c \\       2 & c< x\leq d \\       4 & x> d    \end{cases}$ From here on,i am stuck.Help me out","This question already has answers here : The median minimizes the sum of absolute deviations (the $ {\ell}_{1} $ norm) (11 answers) Closed 8 years ago . If $a<b<c<d$ and $x\in\mathbb R$ then what is the least value of the function $$f(x)=|x-a|+|x-b|+|x-c|+|x-d|\ ?$$ $f(x)= \begin{cases}        a-x+b-x+c-x+d-x & x\leq a \\       x-a+b-x+c-x+d-x & a< x\leq b \\       x-a+x-b+c-x+d-x & b< x\leq c \\       x-a+x-b+x-c+d-x & c< x\leq d \\       x-a+x-b+x-c+x-d & x> d    \end{cases}$ then $f'(x)=\begin{cases}        -4 & x\leq a \\       -2 & a< x\leq b \\       0 & b< x\leq c \\       2 & c< x\leq d \\       4 & x> d    \end{cases}$ From here on,i am stuck.Help me out",,"['derivatives', 'supremum-and-infimum']"
46,Find the derivative of the function. $y = \cot^2(\sin θ)$,Find the derivative of the function.,y = \cot^2(\sin θ),My work is as follows. Criticism welcomed. $$y = \cot^2(\sin\theta) = (\cot(\sin\theta))^2$$ Power Rule combined with the Chain Rule: $$\begin{align} y' & = 2(\cot(\sin \theta)) \cdot \frac d{dx}(\cot(\sin\theta)) \cdot \frac d{dx}(\sin \theta) \\ \\ &= 2(\cot(\sin \theta)) \cdot (- \csc^2(\sin \theta)) \cdot \cos \theta \end{align}$$,My work is as follows. Criticism welcomed. $$y = \cot^2(\sin\theta) = (\cot(\sin\theta))^2$$ Power Rule combined with the Chain Rule: $$\begin{align} y' & = 2(\cot(\sin \theta)) \cdot \frac d{dx}(\cot(\sin\theta)) \cdot \frac d{dx}(\sin \theta) \\ \\ &= 2(\cot(\sin \theta)) \cdot (- \csc^2(\sin \theta)) \cdot \cos \theta \end{align}$$,,"['calculus', 'derivatives']"
47,"""Symmetric"" numerical computation of second derivative","""Symmetric"" numerical computation of second derivative",,"When numerically computing a first derivative, it is better to use $$f'(x) \approx \frac{f(x + \Delta x / 2) - f(x - \Delta x / 2)}{\Delta x}$$ than to use $$f'(x) \approx \frac{f(x + \Delta x) - f(x)}{\Delta x}$$ since it's more symmetric, and hence typically more accurate. However, what is the equivalent of this phenomenon in the second derivative case? In other words, the obvious candidate formula is $$f''(x) \approx \frac{f'(x + \Delta x / 2) - f'(x - \Delta x / 2)}{\Delta x}$$ but is there a more accurate (""symmetric""?) approximation for the second derivative as is the case for the first derivative?","When numerically computing a first derivative, it is better to use $$f'(x) \approx \frac{f(x + \Delta x / 2) - f(x - \Delta x / 2)}{\Delta x}$$ than to use $$f'(x) \approx \frac{f(x + \Delta x) - f(x)}{\Delta x}$$ since it's more symmetric, and hence typically more accurate. However, what is the equivalent of this phenomenon in the second derivative case? In other words, the obvious candidate formula is $$f''(x) \approx \frac{f'(x + \Delta x / 2) - f'(x - \Delta x / 2)}{\Delta x}$$ but is there a more accurate (""symmetric""?) approximation for the second derivative as is the case for the first derivative?",,"['derivatives', 'numerical-methods']"
48,"$\frac{d}{dx} \int_{a}^{x} f(x,t) \ dt$",,"\frac{d}{dx} \int_{a}^{x} f(x,t) \ dt","Does  $$\frac{d}{dx} \int_{a}^{x} f(x,t) \ dt$$ equal to $f(x,x)$ by Fundamental Theorem of Calculus?","Does  $$\frac{d}{dx} \int_{a}^{x} f(x,t) \ dt$$ equal to $f(x,x)$ by Fundamental Theorem of Calculus?",,"['calculus', 'integration', 'derivatives']"
49,"Evaluating derivative of $\int^{3x}_{2x} \sin(t^3 + 1) \,\mathrm dt$",Evaluating derivative of,"\int^{3x}_{2x} \sin(t^3 + 1) \,\mathrm dt","Maybe I'm not very good at my trig rules but I'm having a tough time finding derivative of $$\int^{3x}_{2x} \sin(t^3 + 1) \,\mathrm dt$$ I believe that $u = t^3 + 1$ and $du = 3t^2$, but I'm not sure how to get that to fit back into the integral? I can't just do $-\cos(3t^2)$ can I?","Maybe I'm not very good at my trig rules but I'm having a tough time finding derivative of $$\int^{3x}_{2x} \sin(t^3 + 1) \,\mathrm dt$$ I believe that $u = t^3 + 1$ and $du = 3t^2$, but I'm not sure how to get that to fit back into the integral? I can't just do $-\cos(3t^2)$ can I?",,"['calculus', 'integration', 'derivatives', 'definite-integrals']"
50,Finding the infinite sum of a rational function using integrals,Finding the infinite sum of a rational function using integrals,,"let's take the sum: $$\displaystyle \sum_{n=1}^{\infty}\frac{1}{9n^2 + 3n - 2}\\\implies 9n^2 + 3n - 2 = 9n^2 + 6n - 3n - 2 = 3n(3n + 2) - (3n + 2) = (3n - 1)(3n + 2)$$ The simplest way would be to use partial fractions, and then convert this into a telescoping series. Which makes the sum extremely simple,  but I am looking for a way in, which I could use integrals, perhaps a definite integral in, which I could derive this sum. Any ideas? The $f(n)$ is $f(n) = \frac{1}{(3n-1)(3n+2)}$ So can we use integrals?","let's take the sum: $$\displaystyle \sum_{n=1}^{\infty}\frac{1}{9n^2 + 3n - 2}\\\implies 9n^2 + 3n - 2 = 9n^2 + 6n - 3n - 2 = 3n(3n + 2) - (3n + 2) = (3n - 1)(3n + 2)$$ The simplest way would be to use partial fractions, and then convert this into a telescoping series. Which makes the sum extremely simple,  but I am looking for a way in, which I could use integrals, perhaps a definite integral in, which I could derive this sum. Any ideas? The $f(n)$ is $f(n) = \frac{1}{(3n-1)(3n+2)}$ So can we use integrals?",,"['calculus', 'real-analysis', 'integration', 'derivatives', 'definite-integrals']"
51,"If $f $ is differentiable at $(x,y)$ then $ f_{xy}$ exists at $(x,y)$?",If  is differentiable at  then  exists at ?,"f  (x,y)  f_{xy} (x,y)",Suppose $f:{\bf R}^2 \rightarrow {\bf R}$ is once differentiable at a point $p$. Does it follow that $f_{xy}$ (the derivative of $f $ w.r.t to $x$ and then w.r.t to $y$) exist at $p$?,Suppose $f:{\bf R}^2 \rightarrow {\bf R}$ is once differentiable at a point $p$. Does it follow that $f_{xy}$ (the derivative of $f $ w.r.t to $x$ and then w.r.t to $y$) exist at $p$?,,['derivatives']
52,Differentiability in $f:\mathbb{R}^2 \to \mathbb{R}^2$ v/s $g:\mathbb{C} \to \mathbb{C}$,Differentiability in  v/s,f:\mathbb{R}^2 \to \mathbb{R}^2 g:\mathbb{C} \to \mathbb{C},"I was reading around stuff on differentiability in $\mathbb{C}$ and wondered whether it is same as differentiability in $\mathbb{R}^2$. I approached a professor and gave me an example and asked me to think over it. $f:\mathbb{R}^2 \to \mathbb{R}^2$,   $ f(x,y)=(x,-y)$ And $g:\mathbb{C} \to \mathbb{C}$,   $ g(z)=\bar z$ ,       the conjugate of z If we could plot both the graphs, both would be exactly the same. Clearly $g$ is not differentiable anywhere in $\mathbb{C}$ Also $f$ is a Linear Transformation (as we can find a Matrix of transformation for the given function) and hence differentiable. Why is there such a difference. I read Complex differentiability vs differentiability in $\mathbb{R}^2$ and couldn't understand much as I'm just a beginner in Linear Algebra. Please, try to explain in the simplest way possible. Thanks.","I was reading around stuff on differentiability in $\mathbb{C}$ and wondered whether it is same as differentiability in $\mathbb{R}^2$. I approached a professor and gave me an example and asked me to think over it. $f:\mathbb{R}^2 \to \mathbb{R}^2$,   $ f(x,y)=(x,-y)$ And $g:\mathbb{C} \to \mathbb{C}$,   $ g(z)=\bar z$ ,       the conjugate of z If we could plot both the graphs, both would be exactly the same. Clearly $g$ is not differentiable anywhere in $\mathbb{C}$ Also $f$ is a Linear Transformation (as we can find a Matrix of transformation for the given function) and hence differentiable. Why is there such a difference. I read Complex differentiability vs differentiability in $\mathbb{R}^2$ and couldn't understand much as I'm just a beginner in Linear Algebra. Please, try to explain in the simplest way possible. Thanks.",,['derivatives']
53,About matrix derivative,About matrix derivative,,Suppose $A$ is a matrix with order n*n. we have the following equity but I don't know why. $f(x)=\frac{1}{2}x^TAx-b^Tx$. then $f'(x)=\frac{1}{2}A^Tx+\frac{1}{2}Ax-b$ Is there any rule like scalar function's derivative? thanks.,Suppose $A$ is a matrix with order n*n. we have the following equity but I don't know why. $f(x)=\frac{1}{2}x^TAx-b^Tx$. then $f'(x)=\frac{1}{2}A^Tx+\frac{1}{2}Ax-b$ Is there any rule like scalar function's derivative? thanks.,,"['calculus', 'linear-algebra', 'derivatives']"
54,Prove that the graph $y=x^{3/5}$ has a vertical tangent at the origin.,Prove that the graph  has a vertical tangent at the origin.,y=x^{3/5},"My first attempt was to just differentiate and get $y'=\frac{3}{5x^{2/5}}$ and the at the origin, the gradient is $\infty$, but I'm not sure if this is sufficient enough. Then I differentiated using the definition of the derivative as a limit to try and make it more formal but I'm still not sure if that suffices. Any ideas on how to 'prove' this properly?","My first attempt was to just differentiate and get $y'=\frac{3}{5x^{2/5}}$ and the at the origin, the gradient is $\infty$, but I'm not sure if this is sufficient enough. Then I differentiated using the definition of the derivative as a limit to try and make it more formal but I'm still not sure if that suffices. Any ideas on how to 'prove' this properly?",,"['calculus', 'derivatives']"
55,Derive recursion formula for an integral,Derive recursion formula for an integral,,Im having trouble understanding questions involving deriving a recursion formula. I need to derive the recursion formula for $I_n$ where $n>=2$ $$I_n = \int(x^2-1)^n dx$$ The other questions ive done so far I used the integration by parts and derived a formula where n is decreasing in the integral. I am using the same method and I got this answer: $$x(x^2-1)^n-2n\int x^2(x^2-1)^{n-1}$$ However the textbook is giving me a different answer.,Im having trouble understanding questions involving deriving a recursion formula. I need to derive the recursion formula for $I_n$ where $n>=2$ $$I_n = \int(x^2-1)^n dx$$ The other questions ive done so far I used the integration by parts and derived a formula where n is decreasing in the integral. I am using the same method and I got this answer: $$x(x^2-1)^n-2n\int x^2(x^2-1)^{n-1}$$ However the textbook is giving me a different answer.,,"['calculus', 'integration', 'derivatives', 'recursion']"
56,Show something using the Mean Value Theorem,Show something using the Mean Value Theorem,,"I've got an exercise about differentiability, mean value theorem and suprema. To be honest I don't understand the structure of this question. Maybe you guys are so kind to help me out :) Let $f: [0,1] \rightarrow \mathbb{R}$ be differentiable with $f(0) = 0$ , and satisfying $$|f'(x)|\le M|f(x)|, x\in[0,1]  $$ for some $M>0$ a.) Use the Mean Value Theorem to show that for all $x  \le x_0 \in[0,1], y\in[0,x_0]:$ $$ |f(x)|\le x_0 \text{ sup} |f'(y)|\le M x_0 \text{ sup} |f(y)|$$ b.) Use the previous part to show that f is the zero-function on [0,1]. ( Hint : What happens if we choose $x_0$ such that M $x_0<1$ ?) Known definitions, theorems: Mean Value Theorem (There is a c for which f'(c) equals f(b)-f(a)/(b-a) if [a,b] is the domain, and f is continuous on [a,b], differentiable on (a,b) Differentiable function means that that for all points c $\in$ A, the limit of f(x)-f(c)/(x-c) exists. Interior Extremum Theorem (Intermediate Value theorem). States that if f attains a maximum or minimum value on a open interval, then at some point c $\in$ (a,b), f'(c)=0 Darboux Theorem : If f is differentiable on an interval [a,b] and if $\alpha$ satisfies $f'(a) < \alpha < f'(b)$ , then there exists a point c $\in$ (a,b) where $ f'(c) = \alpha $ .","I've got an exercise about differentiability, mean value theorem and suprema. To be honest I don't understand the structure of this question. Maybe you guys are so kind to help me out :) Let be differentiable with , and satisfying for some a.) Use the Mean Value Theorem to show that for all b.) Use the previous part to show that f is the zero-function on [0,1]. ( Hint : What happens if we choose such that M ?) Known definitions, theorems: Mean Value Theorem (There is a c for which f'(c) equals f(b)-f(a)/(b-a) if [a,b] is the domain, and f is continuous on [a,b], differentiable on (a,b) Differentiable function means that that for all points c A, the limit of f(x)-f(c)/(x-c) exists. Interior Extremum Theorem (Intermediate Value theorem). States that if f attains a maximum or minimum value on a open interval, then at some point c (a,b), f'(c)=0 Darboux Theorem : If f is differentiable on an interval [a,b] and if satisfies , then there exists a point c (a,b) where .","f: [0,1] \rightarrow \mathbb{R} f(0) = 0 |f'(x)|\le M|f(x)|, x\in[0,1]   M>0 x  \le x_0 \in[0,1], y\in[0,x_0]:  |f(x)|\le x_0 \text{ sup} |f'(y)|\le M x_0 \text{ sup} |f(y)| x_0 x_0<1 \in \in \alpha f'(a) < \alpha < f'(b) \in  f'(c) = \alpha ","['real-analysis', 'derivatives']"
57,Continuity and differentiability of function series,Continuity and differentiability of function series,,"Examine the continuity and differentiability of functions: a) $\displaystyle f(x)=\sum_{n=1}^{+\infty}\frac{\sin(nx)}{n^3}$ b) $\displaystyle f(x)=\sum_{n=1}^{+\infty}\arctan\left(\frac{x}{n^2} \right)$ in the case of differentiability explore the sign of $f '(0)$ . So, we are dealing with function series I think. I tried a): Let $f_n(x)=\frac{\sin(nx)}{n^3}$ . With Weierstrass M-test $|f_n|\le \frac{1}{n^3}$ so the series $\sum_{n=1}^{+\infty}f_n$ converges  uniformly. The same applies for series of derivatives that is: $\sum_{n=1}^{+\infty}\frac{\cos(nx)}{n^2}$ so function $f$ is differentiable and so it is continuous. $f'(0)>0$ Is this argumentation ok? I don't know how to approach b).","Examine the continuity and differentiability of functions: a) b) in the case of differentiability explore the sign of . So, we are dealing with function series I think. I tried a): Let . With Weierstrass M-test so the series converges  uniformly. The same applies for series of derivatives that is: so function is differentiable and so it is continuous. Is this argumentation ok? I don't know how to approach b).",\displaystyle f(x)=\sum_{n=1}^{+\infty}\frac{\sin(nx)}{n^3} \displaystyle f(x)=\sum_{n=1}^{+\infty}\arctan\left(\frac{x}{n^2} \right) f '(0) f_n(x)=\frac{\sin(nx)}{n^3} |f_n|\le \frac{1}{n^3} \sum_{n=1}^{+\infty}f_n \sum_{n=1}^{+\infty}\frac{\cos(nx)}{n^2} f f'(0)>0,"['real-analysis', 'derivatives', 'continuity']"
58,Why do derivatives of certain equations relating to circles yield other similar equations? [duplicate],Why do derivatives of certain equations relating to circles yield other similar equations? [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Why is the derivative of a circle's area its perimeter (and similarly for spheres)? We all know that the volume of a sphere is: $V = \frac{4}{3}\pi r^{3}$ and its surface area is $S = 4 \pi r^2$ Now we see that $\frac{dV}{dr} = S$ As well, the area of a circle is $A = \pi r^2$ The circumference is $C = 2 \pi r$ Now we see again that $\frac{dA}{dr}=C$ There may be more that I have not noticed.  Why does this relationship occur?","This question already has answers here : Closed 12 years ago . Possible Duplicate: Why is the derivative of a circle's area its perimeter (and similarly for spheres)? We all know that the volume of a sphere is: $V = \frac{4}{3}\pi r^{3}$ and its surface area is $S = 4 \pi r^2$ Now we see that $\frac{dV}{dr} = S$ As well, the area of a circle is $A = \pi r^2$ The circumference is $C = 2 \pi r$ Now we see again that $\frac{dA}{dr}=C$ There may be more that I have not noticed.  Why does this relationship occur?",,"['calculus', 'euclidean-geometry', 'circles', 'derivatives', 'spherical-geometry']"
59,"How to show that $\left[D_\mu, F^{\mu\nu}\right]=\left(\partial_\mu \delta_{ae}-gf^{bae}A_\mu^b\right)F^{\mu\nu a}t^e$?",How to show that ?,"\left[D_\mu, F^{\mu\nu}\right]=\left(\partial_\mu \delta_{ae}-gf^{bae}A_\mu^b\right)F^{\mu\nu a}t^e","I'm trying to show that $$\left[D_\mu, F^{\mu\nu}\right]=\left(\partial_\mu \delta_{ae}-gf^{bae}A_\mu^b\right)F^{\mu\nu a}t^e\tag{1}$$ where the covariant derivative, $D_\mu=\partial_\mu+ig A_\mu$ , the gauge field $A_\mu=A_\mu^b t^b$ with $t^b$ being the matrix generators of the fundamental representation. $g$ is a constant and $F^{\mu\nu}=F^{\mu\nu a}t^a$ is the Maxwell field strength tensor. Using the commutator relation $[A+B, C]=[A,C]+[B,C]$ , and the Lie algebra $[t^a, t^b]=if^{abc}t^c$ , where $f^{abc}$ is a structure constant. I find that, $$\left[D_\mu, F^{\mu\nu}\right]=\left[\partial_\mu+ig A_\mu, F^{\mu\nu}\right]=[\partial_\mu, F^{\mu\nu a}t^a]+ig[A_\mu^b t^b,F^{\mu\nu a}t^a]$$ $$=\partial_\mu F^{\mu\nu e}t^e-F^{\mu\nu e}t^e\partial_\mu+igA_\mu^bF^{\mu\nu a}[t^b,t^a]$$ $$=\partial_\mu F^{\mu\nu e}t^e-{F^{\mu\nu e}t^e\partial_\mu}-gA_\mu^bF^{\mu\nu a}f^{bae}t^e$$ $$=\partial_\mu \delta_{ae}F^{\mu\nu a}t^e-{F^{\mu\nu e}t^e\partial_\mu}-gA_\mu^bF^{\mu\nu a}f^{bae}t^e$$ $$=\left(\partial_\mu \delta_{ae}-gf^{bae}A_\mu^b\right)F^{\mu\nu a}t^e-\color{red}{F^{\mu\nu e}t^e\partial_\mu}\tag{2}$$ The final equality of $(2)$ is identical to $(1)$ except for the presence of the term in red which I cannot seem to get rid of. Why should the term marked red vanish?","I'm trying to show that where the covariant derivative, , the gauge field with being the matrix generators of the fundamental representation. is a constant and is the Maxwell field strength tensor. Using the commutator relation , and the Lie algebra , where is a structure constant. I find that, The final equality of is identical to except for the presence of the term in red which I cannot seem to get rid of. Why should the term marked red vanish?","\left[D_\mu, F^{\mu\nu}\right]=\left(\partial_\mu \delta_{ae}-gf^{bae}A_\mu^b\right)F^{\mu\nu a}t^e\tag{1} D_\mu=\partial_\mu+ig A_\mu A_\mu=A_\mu^b t^b t^b g F^{\mu\nu}=F^{\mu\nu a}t^a [A+B, C]=[A,C]+[B,C] [t^a, t^b]=if^{abc}t^c f^{abc} \left[D_\mu, F^{\mu\nu}\right]=\left[\partial_\mu+ig A_\mu, F^{\mu\nu}\right]=[\partial_\mu, F^{\mu\nu a}t^a]+ig[A_\mu^b t^b,F^{\mu\nu a}t^a] =\partial_\mu F^{\mu\nu e}t^e-F^{\mu\nu e}t^e\partial_\mu+igA_\mu^bF^{\mu\nu a}[t^b,t^a] =\partial_\mu F^{\mu\nu e}t^e-{F^{\mu\nu e}t^e\partial_\mu}-gA_\mu^bF^{\mu\nu a}f^{bae}t^e =\partial_\mu \delta_{ae}F^{\mu\nu a}t^e-{F^{\mu\nu e}t^e\partial_\mu}-gA_\mu^bF^{\mu\nu a}f^{bae}t^e =\left(\partial_\mu \delta_{ae}-gf^{bae}A_\mu^b\right)F^{\mu\nu a}t^e-\color{red}{F^{\mu\nu e}t^e\partial_\mu}\tag{2} (2) (1)","['calculus', 'derivatives', 'representation-theory', 'partial-derivative', 'mathematical-physics']"
60,Implicit differentiation gives different answer,Implicit differentiation gives different answer,,"I'm trying to implicitly differentiate the following equation: $\frac{x+f(x)}{c+f(x)}=4$ where $c$ is a constant. Here's what I get: $$ \frac{\mathrm d}{dx}\left(\frac{x+f(x)}{c+f(x)}\right)=\frac{\mathrm d}{\mathrm dx}(4) $$ $$ \frac{(1+f'(x))(c+f(x))-(x+f(x))(f'(x))}{(c+f(x))^2}=0 $$ $$ \frac{c+f(x)+cf'(x)+f(x)f'(x)-xf'(x)-f(x)f'(x)}{(c+f(x))^2}=0 $$ $$ c+f(x)+cf'(x)-xf'(x)=0 $$ $$ c+f(x)=xf'(x)-cf'(x) $$ $$ f'(x)=\frac{c+f(x)}{x-c} $$ However, when I isolate $f(x)$ first... $$ \frac{x+f(x)}{c+f(x)}=4 $$ $$ x+f(x)=4c+4f(x) $$ $$ x-4c=3f(x) $$ $$ \frac{x}{3}-\frac{4}{3}c=f(x) $$ ... I get a different answer: $$ f'(x)=\frac{\mathrm d}{\mathrm dx}\left(\frac{x}{3}-\frac{4}{3}c\right) $$ $$ f'(x)=\frac{1}{3} $$ Hopefully someone can shed some light on where I'm going wrong. Thanks!","I'm trying to implicitly differentiate the following equation: where is a constant. Here's what I get: However, when I isolate first... ... I get a different answer: Hopefully someone can shed some light on where I'm going wrong. Thanks!","\frac{x+f(x)}{c+f(x)}=4 c 
\frac{\mathrm d}{dx}\left(\frac{x+f(x)}{c+f(x)}\right)=\frac{\mathrm d}{\mathrm dx}(4)
 
\frac{(1+f'(x))(c+f(x))-(x+f(x))(f'(x))}{(c+f(x))^2}=0
 
\frac{c+f(x)+cf'(x)+f(x)f'(x)-xf'(x)-f(x)f'(x)}{(c+f(x))^2}=0
 
c+f(x)+cf'(x)-xf'(x)=0
 
c+f(x)=xf'(x)-cf'(x)
 
f'(x)=\frac{c+f(x)}{x-c}
 f(x) 
\frac{x+f(x)}{c+f(x)}=4
 
x+f(x)=4c+4f(x)
 
x-4c=3f(x)
 
\frac{x}{3}-\frac{4}{3}c=f(x)
 
f'(x)=\frac{\mathrm d}{\mathrm dx}\left(\frac{x}{3}-\frac{4}{3}c\right)
 
f'(x)=\frac{1}{3}
","['calculus', 'derivatives', 'implicit-differentiation']"
61,Can we find the exact value of a double sum with cosine without differentiation?,Can we find the exact value of a double sum with cosine without differentiation?,,"After finding an interesting double sum $$\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{(-1)^{m+n}}{(m+n)^2}  = \frac{\pi^2}{12}-\ln 2 ,$$ I started to investigate a harder one $$\displaystyle \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} \cos (m+n)}{(m+n)^2} .$$ Noting that $\displaystyle \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} \cos (m+n)}{(m+n)^2}=\Re f(1)\tag*{}  $ where $\displaystyle f(x)=\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} e^{(m+n) x i}}{(m+n)^2}\tag*{}  $ Differentiating $f(x)$ once and twice yields $\displaystyle f^{\prime}(x)=\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} i e^{(m+n) x i}}{m+n}\tag*{} $ and $$ \displaystyle \begin{aligned}f^{\prime \prime}(x) =\sum_{m=1}^{\infty}(-1)^m\left[\sum_{n=1}^{\infty}(-1)^{n-1} e^{(m+n) x i}\right] =\sum_{m=1}^{\infty} \frac{(-1)^m e^{(m+1) x i}}{1+e^{x i}}=-\frac{e^{2 xi }}{\left(1+e^{xi}\right)^2}\end{aligned}\tag*{} $$ Integrating back gives $$f^{\prime}(x)-f^{\prime}(0)=\int_0^x f^{\prime \prime}(t) d t  =-\int_0^x \frac{e^{2 i t}}{\left(1+e^{i t}\right)^2} d t =i \ln \left(1+e^{i x}\right)+\frac{1}{2} \tan \frac{x}{2}-i \ln 2$$ Rearranging gives us $ \displaystyle f^{\prime}(x)=i f(0)+i \ln \left(1+e^{i x}\right)+\frac{1}{2} \tan \frac{x}{2}-i \ln 2\tag*{} $ Integrating once more gives $$\begin{aligned} f(1)-f(0)&= \int_0^1\left[i f(0)+i \ln \left(1+e^{i t}\right)+\frac{1}{2} \tan \frac{t}{2}-i \ln 2\right] d t \end{aligned}$$ $$= i f(0)+i \int_0^1 \ln \left(1+e^{i t}\right) d t+\frac{1}{2} \int_0^1 \tan \frac{t}{2} d t-i \ln 2 $$ $$=  i f(0)+i\left[-i \operatorname{Li_2}\left(-e^{-i x}\right)\right]_0^1+\ln \left(\sec \frac{1}{2}\right)-i \ln 2$$ Rearranging gives $\displaystyle \begin{aligned}f(1)&=f(0)+i f(0)-\operatorname{Li_2}\left(-e^{-i}\right)-\frac{\pi^2}{12}+\ln \left(\sec \frac 12\right) -i \ln 2\end{aligned}\tag*{} $ $\displaystyle \boxed{\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} \cos (m+n)}{(m+n)^2}=\Re f(1)=-\ln 2-\Re\left(\operatorname{Li}_2\left(-e^{-i}\right)\right)+\ln \left(\sec \frac{1}{2}\right) \approx{0.0099041}} \tag*{} $ My question: Can we find the exact value of  the double sum with cosine without differentiation? Your comments and alternative solution are highly appreciated. Footnote: $$f(0) =\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n}}{(m+n)^2}  =\sum_{m=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{m+n-1} \int_0^1 x^{m+n-1} \ln x d x \\ =\int_0^1\left[\sum_{m=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{m+n-1} x^{m+n-1}\right] \ln x d x \\ =\int_0^1 \ln x \sum_{m=1}^{\infty} \frac{(-1)^m x^m}{1+x} d x=\int_0^1 \ln x \frac{-x}{(1+x)^2} d x \\  =-\int_0^1 \frac{x^2 \ln x}{(1+x)^2} d x=\frac{\pi^2}{12}-\ln 2   $$",After finding an interesting double sum I started to investigate a harder one Noting that where Differentiating once and twice yields and Integrating back gives Rearranging gives us Integrating once more gives Rearranging gives My question: Can we find the exact value of  the double sum with cosine without differentiation? Your comments and alternative solution are highly appreciated. Footnote:,"\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{(-1)^{m+n}}{(m+n)^2}  = \frac{\pi^2}{12}-\ln 2 , \displaystyle \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} \cos (m+n)}{(m+n)^2} . \displaystyle \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} \cos (m+n)}{(m+n)^2}=\Re f(1)\tag*{}   \displaystyle f(x)=\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} e^{(m+n) x i}}{(m+n)^2}\tag*{}   f(x) \displaystyle f^{\prime}(x)=\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} i e^{(m+n) x i}}{m+n}\tag*{}   \displaystyle \begin{aligned}f^{\prime \prime}(x) =\sum_{m=1}^{\infty}(-1)^m\left[\sum_{n=1}^{\infty}(-1)^{n-1} e^{(m+n) x i}\right] =\sum_{m=1}^{\infty} \frac{(-1)^m e^{(m+1) x i}}{1+e^{x i}}=-\frac{e^{2 xi }}{\left(1+e^{xi}\right)^2}\end{aligned}\tag*{}  f^{\prime}(x)-f^{\prime}(0)=\int_0^x f^{\prime \prime}(t) d t  =-\int_0^x \frac{e^{2 i t}}{\left(1+e^{i t}\right)^2} d t =i \ln \left(1+e^{i x}\right)+\frac{1}{2} \tan \frac{x}{2}-i \ln 2  \displaystyle f^{\prime}(x)=i f(0)+i \ln \left(1+e^{i x}\right)+\frac{1}{2} \tan \frac{x}{2}-i \ln 2\tag*{}  \begin{aligned} f(1)-f(0)&= \int_0^1\left[i f(0)+i \ln \left(1+e^{i t}\right)+\frac{1}{2} \tan \frac{t}{2}-i \ln 2\right] d t \end{aligned} = i f(0)+i \int_0^1 \ln \left(1+e^{i t}\right) d t+\frac{1}{2} \int_0^1 \tan \frac{t}{2} d t-i \ln 2  =  i f(0)+i\left[-i \operatorname{Li_2}\left(-e^{-i x}\right)\right]_0^1+\ln \left(\sec \frac{1}{2}\right)-i \ln 2 \displaystyle \begin{aligned}f(1)&=f(0)+i f(0)-\operatorname{Li_2}\left(-e^{-i}\right)-\frac{\pi^2}{12}+\ln \left(\sec \frac 12\right) -i \ln 2\end{aligned}\tag*{}  \displaystyle \boxed{\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} \cos (m+n)}{(m+n)^2}=\Re f(1)=-\ln 2-\Re\left(\operatorname{Li}_2\left(-e^{-i}\right)\right)+\ln \left(\sec \frac{1}{2}\right) \approx{0.0099041}} \tag*{}  f(0) =\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n}}{(m+n)^2}  =\sum_{m=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{m+n-1} \int_0^1 x^{m+n-1} \ln x d x \\ =\int_0^1\left[\sum_{m=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{m+n-1} x^{m+n-1}\right] \ln x d x \\ =\int_0^1 \ln x \sum_{m=1}^{\infty} \frac{(-1)^m x^m}{1+x} d x=\int_0^1 \ln x \frac{-x}{(1+x)^2} d x \\  =-\int_0^1 \frac{x^2 \ln x}{(1+x)^2} d x=\frac{\pi^2}{12}-\ln 2   ","['integration', 'derivatives', 'summation', 'pi', 'trigonometric-series']"
62,Product Rule for differential forms [duplicate],Product Rule for differential forms [duplicate],,"This question already has an answer here : Why is the exterior product an antiderivation intuitively? (1 answer) Closed 8 months ago . For a $p$ -form, $\alpha$ , and a $q$ -form, $\beta$ , why is the exterior derivative product rule given by $$d(\alpha\wedge\beta)=d\alpha\wedge\beta+(-1)^p\alpha\wedge d\beta,$$ with $(-1)^p$ ? Wikipedia says that it comes from the graded product (Leibniz) rule, but those Wikipedia articles give this relation as a definition that applies to differential forms. Both resources I am using to learn about differential forms give this relation without proof. How can I mathematically prove it and/or physically/geometrically see it?","This question already has an answer here : Why is the exterior product an antiderivation intuitively? (1 answer) Closed 8 months ago . For a -form, , and a -form, , why is the exterior derivative product rule given by with ? Wikipedia says that it comes from the graded product (Leibniz) rule, but those Wikipedia articles give this relation as a definition that applies to differential forms. Both resources I am using to learn about differential forms give this relation without proof. How can I mathematically prove it and/or physically/geometrically see it?","p \alpha q \beta d(\alpha\wedge\beta)=d\alpha\wedge\beta+(-1)^p\alpha\wedge d\beta, (-1)^p","['differential-geometry', 'derivatives']"
63,Differentiable function with differentiable inverse must be continuously differentiable?,Differentiable function with differentiable inverse must be continuously differentiable?,,"I was wondering, if there is a function $f$ that is differentiable everywhere in $\mathbb{R}$ (but not continuously differentiable ), with a differentiable inverse . Since it needs to be bijective and it is continuous, it is w.l.o.g. strictly increasing and thus has nonnegative derivative everywhere. I have tried looking at the standard pathological functions that I know of, but I couldn‘t find any example. If the restriction to invertible functions with differentiable inverse wasn’t there, then there are some good examples of such pathological functions.","I was wondering, if there is a function that is differentiable everywhere in (but not continuously differentiable ), with a differentiable inverse . Since it needs to be bijective and it is continuous, it is w.l.o.g. strictly increasing and thus has nonnegative derivative everywhere. I have tried looking at the standard pathological functions that I know of, but I couldn‘t find any example. If the restriction to invertible functions with differentiable inverse wasn’t there, then there are some good examples of such pathological functions.",f \mathbb{R},"['real-analysis', 'derivatives', 'continuity', 'examples-counterexamples']"
64,Find $\min \int_0^{x\arctan x}\frac{e^{t-\cos t}}{1+t^{2023}}\mathrm dt$,Find,\min \int_0^{x\arctan x}\frac{e^{t-\cos t}}{1+t^{2023}}\mathrm dt,"Find the minimum value of : $$f(x) = \int_0^{x\arctan x}\frac{e^{t-\cos t}}{1+t^{2023}}\mathrm dt$$ Using FTC, we have ; $$f'(x) = \frac{e^{x\arctan x - \cos (x\arctan x)}}{1+x\arctan x}.(\arctan x + \frac{x}{x^2 + 1})$$ For $f$ to be minimum, $f' = 0\implies\arctan x = -\frac{x}{x^2 + 1}$ However, I can't proceed further and don't find it much useful. This question appeared today in my test (JEE) for which I had an average of 3.5 minutes to solve it. But in such short time, I am failing to find a short and elegant method.","Find the minimum value of : Using FTC, we have ; For to be minimum, However, I can't proceed further and don't find it much useful. This question appeared today in my test (JEE) for which I had an average of 3.5 minutes to solve it. But in such short time, I am failing to find a short and elegant method.",f(x) = \int_0^{x\arctan x}\frac{e^{t-\cos t}}{1+t^{2023}}\mathrm dt f'(x) = \frac{e^{x\arctan x - \cos (x\arctan x)}}{1+x\arctan x}.(\arctan x + \frac{x}{x^2 + 1}) f f' = 0\implies\arctan x = -\frac{x}{x^2 + 1},"['real-analysis', 'calculus', 'derivatives', 'definite-integrals', 'maxima-minima']"
65,Is my proof for the single variable chain rule correct?,Is my proof for the single variable chain rule correct?,,"I have studied the proof for the Chain Rule given by Spivak in Calculus but I found the argumentation convoluted, so I tried proving the Chain Rule on my own. I feel like what I've written is watertight but I figured I'd ask here to see if some people stronger than me in math could find any issues with it. Begin with the statement $\lim_{h \to 0} \frac{f(g(a+h))-f(g(a))}{h}$ . Add and subtract $g(a)$ so that we get: $=\lim_{h \to 0} \frac{f(g(a)+g(a+h)-g(a))-f(g(a))}{h}$ Assume that the function $g$ is not locally constant. If it is locally constant then by default, $\frac{d}{dx}f(g(x)) = 0$ . Then, we multiply by $\frac{h}{h}$ inside the function to get: $=\lim_{h \to 0} \frac{f(g(a)+h\frac{g(a+h)-g(a)}{h})-f(g(a))}{h}$ Then multiply the whole limit expression by $1$ again with $\frac{\frac{g(a+h)-g(a)}{h}}{\frac{g(a+h)-g(a)}{h}}$ : $=\lim_{h \to 0} \frac{f(g(a)+h\frac{g(a+h)-g(a)}{h})-f(g(a))}{h\frac{g(a+h)-g(a)}{h}}\frac{g(a+h)-g(a)}{h}$ Since the limit of a product of functions is the product of the limits of each function, $=\lim_{h \to 0} \frac{f(g(a)+h\frac{g(a+h)-g(a)}{h})-f(g(a))}{h\frac{g(a+h)-g(a)}{h}}\lim_{h \to 0}\frac{g(a+h)-g(a)}{h}$ . The right hand expression is just $g'(a)$ . The left hand expression remains to be evaluated. Here I introduce the secant slope function as well as the derivative: $\phi(a, h) = \begin{cases} \frac{g(a+h)-g(a)}{h} & h \neq 0 \\ g'(a) & h = 0\end{cases}$ Substituting, $\frac{d}{dx}f(g(x)) = [\lim_{h \to 0} \frac{f(g(a)+h\phi(a, h)) - f(g(a))}{h\phi(a,h)}]g'(a)$ Proceed by evaluating the remaining limit expression. Call it $L$ , and write a $\delta-\epsilon$ statement of the existence of the limit: $\forall\epsilon>0, \exists\delta>0: 0<|h|<\delta \implies |\frac{f(g(a)+h\phi(a, h)) - f(g(a))}{h\phi(a,h)} - L|<\epsilon$ We massage the $\delta$ expression by multiplying it by $|\phi(a, h)|$ : $\forall\epsilon>0, \exists\delta>0: 0<|h\phi(a, h)|<\delta|\phi(a, h)| \implies |\frac{f(g(a)+h\phi(a, h)) - f(g(a))}{h\phi(a,h)} - L|<\epsilon$ The point of this was to construct a new $\delta^*$ and allow a substitution to do some cleanup work: $k = h\phi(a, h)$ , so that we construct a derivative expression: $\forall\epsilon>0, \exists\delta^*>0: 0<|k|<\delta^* \implies |\frac{f(g(a)+k) - f(g(a))}{k} - L|<\epsilon$ Clearly, $L = f'(g(a))$ . So now I can write the chain rule: $\lim_{h \to 0} \frac{f(g(a+h))-f(g(a))}{h} = f'(g(a))g'(a)$ . Or, $\frac{d}{dx}f(g(x)) = f'(g(x))g'(x)$ I will greatly appreciate any feedback on this!","I have studied the proof for the Chain Rule given by Spivak in Calculus but I found the argumentation convoluted, so I tried proving the Chain Rule on my own. I feel like what I've written is watertight but I figured I'd ask here to see if some people stronger than me in math could find any issues with it. Begin with the statement . Add and subtract so that we get: Assume that the function is not locally constant. If it is locally constant then by default, . Then, we multiply by inside the function to get: Then multiply the whole limit expression by again with : Since the limit of a product of functions is the product of the limits of each function, . The right hand expression is just . The left hand expression remains to be evaluated. Here I introduce the secant slope function as well as the derivative: Substituting, Proceed by evaluating the remaining limit expression. Call it , and write a statement of the existence of the limit: We massage the expression by multiplying it by : The point of this was to construct a new and allow a substitution to do some cleanup work: , so that we construct a derivative expression: Clearly, . So now I can write the chain rule: . Or, I will greatly appreciate any feedback on this!","\lim_{h \to 0} \frac{f(g(a+h))-f(g(a))}{h} g(a) =\lim_{h \to 0} \frac{f(g(a)+g(a+h)-g(a))-f(g(a))}{h} g \frac{d}{dx}f(g(x)) = 0 \frac{h}{h} =\lim_{h \to 0} \frac{f(g(a)+h\frac{g(a+h)-g(a)}{h})-f(g(a))}{h} 1 \frac{\frac{g(a+h)-g(a)}{h}}{\frac{g(a+h)-g(a)}{h}} =\lim_{h \to 0} \frac{f(g(a)+h\frac{g(a+h)-g(a)}{h})-f(g(a))}{h\frac{g(a+h)-g(a)}{h}}\frac{g(a+h)-g(a)}{h} =\lim_{h \to 0} \frac{f(g(a)+h\frac{g(a+h)-g(a)}{h})-f(g(a))}{h\frac{g(a+h)-g(a)}{h}}\lim_{h \to 0}\frac{g(a+h)-g(a)}{h} g'(a) \phi(a, h) = \begin{cases} \frac{g(a+h)-g(a)}{h} & h \neq 0 \\ g'(a) & h = 0\end{cases} \frac{d}{dx}f(g(x)) = [\lim_{h \to 0} \frac{f(g(a)+h\phi(a, h)) - f(g(a))}{h\phi(a,h)}]g'(a) L \delta-\epsilon \forall\epsilon>0, \exists\delta>0: 0<|h|<\delta \implies |\frac{f(g(a)+h\phi(a, h)) - f(g(a))}{h\phi(a,h)} - L|<\epsilon \delta |\phi(a, h)| \forall\epsilon>0, \exists\delta>0: 0<|h\phi(a, h)|<\delta|\phi(a, h)| \implies |\frac{f(g(a)+h\phi(a, h)) - f(g(a))}{h\phi(a,h)} - L|<\epsilon \delta^* k = h\phi(a, h) \forall\epsilon>0, \exists\delta^*>0: 0<|k|<\delta^* \implies |\frac{f(g(a)+k) - f(g(a))}{k} - L|<\epsilon L = f'(g(a)) \lim_{h \to 0} \frac{f(g(a+h))-f(g(a))}{h} = f'(g(a))g'(a) \frac{d}{dx}f(g(x)) = f'(g(x))g'(x)","['calculus', 'derivatives', 'solution-verification']"
66,Number of Critical Point of $|x-1|+|x|$,Number of Critical Point of,|x-1|+|x|,"Let $f(x) =|x-1|+|x|$ . Find the number of critical points of the function in its domain. According to me the definition of critical point is the point on the function where either $f'(x) =0$ or $f'(x) $ does not exist. In the interval $(-\infty, 0) $ $$f(x) =1-2x$$ $$f'(x) =-2 \neq 0$$ Similarly in interval $(1, \infty) $ $$f(x) =2x-1$$ $$f'(x) =2 \neq 0$$ The problem comes in the interval $[0, 1]$ . Firstly is it trivial the the derivative of the function does not exist at points $\{0, 1\}$ In the interval (0, 1) $$f(x) =1$$ $$f'(x) =0 \forall x \in (0, 1) $$ So the critical points to this function all reals in the interval $[0, 1]$ . Hence the function has an infinite number of critical points. But the answer to this problem is given as $2$ . Why is this happening? Is my definition  of critical points incomplete or wrong? Or maybe the answer to the problem is given wrong.","Let . Find the number of critical points of the function in its domain. According to me the definition of critical point is the point on the function where either or does not exist. In the interval Similarly in interval The problem comes in the interval . Firstly is it trivial the the derivative of the function does not exist at points In the interval (0, 1) So the critical points to this function all reals in the interval . Hence the function has an infinite number of critical points. But the answer to this problem is given as . Why is this happening? Is my definition  of critical points incomplete or wrong? Or maybe the answer to the problem is given wrong.","f(x) =|x-1|+|x| f'(x) =0 f'(x)  (-\infty, 0)  f(x) =1-2x f'(x) =-2 \neq 0 (1, \infty)  f(x) =2x-1 f'(x) =2 \neq 0 [0, 1] \{0, 1\} f(x) =1 f'(x) =0 \forall x \in (0, 1)  [0, 1] 2","['real-analysis', 'calculus', 'derivatives']"
67,How do you extract useful information from this calculus question?,How do you extract useful information from this calculus question?,,"Let $f$ be a twice differentiable function defined in $[-3,3]$ such that $f(0)=-4, f’(3)=0, f’(-3)=12$ , and $f’’(x)\geq-2, \forall x \in [-3,3]$ . If $g(x)=\int_0^x {f(t)dt}$ then find the maximum value of g(x). The thing is that I’m trying to get some info out of this about the function $f$ , by basically having a rough graph, but the criterion $f’’(x)\geq-2$ doesn’t really give me anything because $f$ could be either convex-up(like $x^2$ ) or convex-down(like $-x^2$ ) or even have a point of inflection. It just can’t be too-much-concave-down, if you know what I mean. The function starts off as increasing, but IDK what happens after that. Also if it were just $\int_0^x {f’(t)dt}$ I might have been more confident, but $\int_0^x {f(t)dt}$ just throws me off.","Let be a twice differentiable function defined in such that , and . If then find the maximum value of g(x). The thing is that I’m trying to get some info out of this about the function , by basically having a rough graph, but the criterion doesn’t really give me anything because could be either convex-up(like ) or convex-down(like ) or even have a point of inflection. It just can’t be too-much-concave-down, if you know what I mean. The function starts off as increasing, but IDK what happens after that. Also if it were just I might have been more confident, but just throws me off.","f [-3,3] f(0)=-4, f’(3)=0, f’(-3)=12 f’’(x)\geq-2, \forall x \in [-3,3] g(x)=\int_0^x {f(t)dt} f f’’(x)\geq-2 f x^2 -x^2 \int_0^x {f’(t)dt} \int_0^x {f(t)dt}","['calculus', 'derivatives', 'definite-integrals']"
68,Find the derivative of $\frac{d}{dx}\left(\tan \left(\sqrt{x}\right)\right)$ - no chain rule,Find the derivative of  - no chain rule,\frac{d}{dx}\left(\tan \left(\sqrt{x}\right)\right),"I'm trying to find the derivative of: $$ \frac{d}{dx}\left(\tan \left(\sqrt{x}\right)\right) $$ As per the chain rule I have to find the derivative of $tan()$ and then $(\sqrt{x})$ which at the end is equal to $\frac{\sec ^2\left(\sqrt{x}\right)}{2\sqrt{x}}$ However, in my exercise, I don't have to use the chain rule nor L'Hopital rule. I'm looking to re-write $\tan(\sqrt{x})$ in a way to solve the exercise. I'm thinking of $$ \lim _{h\to 0}\left(\frac{\tan\left(\sqrt{x+h}\right)-\tan\left(\sqrt{x}\right)}{h}\right) $$ but I'm kind of lost on how to solve it","I'm trying to find the derivative of: As per the chain rule I have to find the derivative of and then which at the end is equal to However, in my exercise, I don't have to use the chain rule nor L'Hopital rule. I'm looking to re-write in a way to solve the exercise. I'm thinking of but I'm kind of lost on how to solve it","
\frac{d}{dx}\left(\tan \left(\sqrt{x}\right)\right)
 tan() (\sqrt{x}) \frac{\sec ^2\left(\sqrt{x}\right)}{2\sqrt{x}} \tan(\sqrt{x}) 
\lim _{h\to 0}\left(\frac{\tan\left(\sqrt{x+h}\right)-\tan\left(\sqrt{x}\right)}{h}\right)
","['calculus', 'derivatives']"
69,Proof that eigenvalues of the Coulomb hamiltonian are not nonnegative,Proof that eigenvalues of the Coulomb hamiltonian are not nonnegative,,"Let's consider the Coulomb Hamiltonian $$ -\Delta - \frac{1}{|x|}$$ in $\mathbb{R}^3$ . It is known that eigenvalues of the Coulomb Hamiltonian are negative. I know it has negative eigenvalues, but I can't prove it hasn't nonnegative eigenvalues. That is, I want to prove the following. If smooth function $u$ satisfies the following $$\int_{\mathbb{R}^3 } |u(x)|^2 dx + \int_{\mathbb{R}^3 } |\nabla u(x)|^2 dx ＜ \infty$$ and $$-\Delta u - \frac{u}{|x|} = \lambda u$$ for some $\lambda \geq 0$ , then $u=0$ . Any advice would be appreciated.","Let's consider the Coulomb Hamiltonian in . It is known that eigenvalues of the Coulomb Hamiltonian are negative. I know it has negative eigenvalues, but I can't prove it hasn't nonnegative eigenvalues. That is, I want to prove the following. If smooth function satisfies the following and for some , then . Any advice would be appreciated.","
-\Delta - \frac{1}{|x|} \mathbb{R}^3 u \int_{\mathbb{R}^3
} |u(x)|^2 dx + \int_{\mathbb{R}^3
} |\nabla u(x)|^2 dx ＜ \infty -\Delta u - \frac{u}{|x|} = \lambda u \lambda \geq 0 u=0","['integration', 'derivatives', 'eigenvalues-eigenvectors', 'operator-theory']"
70,Does there exist bounded function with a bounded first derivative but unbounded second derivative?,Does there exist bounded function with a bounded first derivative but unbounded second derivative?,,I know that a bounded function with a bounded second derivative also has a bounded first derivative. But consider a bounded function $f(x)$ such that $f'(x)$ is also bounded. Can we claim that $f''(x)$ will also be bounded. Intuition says NO ! But I am unable to find an example. Please help.,I know that a bounded function with a bounded second derivative also has a bounded first derivative. But consider a bounded function such that is also bounded. Can we claim that will also be bounded. Intuition says NO ! But I am unable to find an example. Please help.,f(x) f'(x) f''(x),"['real-analysis', 'derivatives']"
71,How the right term of the derivative is gained?,How the right term of the derivative is gained?,,This deduction is one of the typical ones I think. What I want to deduce is the right term from the left term of the below equation. $$\frac{d}{dx}\left(\log\left(\frac{a+\sqrt{a^{2}+x^{2}}}{x}\right)\right)=\frac{-a}{x\sqrt{a^{2}+x^{2}}}$$ $\frac{d}{dx}\left(\log\left(\frac{a+\sqrt{a^{2}+x^{2}}}{x}\right)\right)$ $=\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\frac{d}{dx}\left(\frac{a+\sqrt{a^{2}+x^{2}}}{x}\right)$ $=\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\frac{d}{dx}\left(x^{-1}\left(a+\sqrt{a^{2}+x^{2}}\right)\right)$ $=\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\left((x^{-1})'\left(a+\sqrt{a^{2}+x^{2}}\right)\right)\left(x^{-1}\frac{d}{dx}\left(a+\sqrt{a^{2}+x^{2}}\right)\right)$ $=\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\left((-1\cdot x^{-2})\left(a+\sqrt{a^{2}+x^{2}}\right)\right)\left(x^{-1}\frac{d}{dx}\left(\sqrt{a^{2}+x^{2}}\right)\right)$ $=\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\left(-x^{-2}\left(a+\sqrt{a^{2}+x^{2}}\right)\right)\left(x^{-1}\frac{d}{dx}\left(\left(a^{2}+x^{2}\right)^{\frac{1}{2}}\right)\right)$ Anyone deduced it in someday? ps. I have to go to work. Back after about 7hours.,This deduction is one of the typical ones I think. What I want to deduce is the right term from the left term of the below equation. Anyone deduced it in someday? ps. I have to go to work. Back after about 7hours.,\frac{d}{dx}\left(\log\left(\frac{a+\sqrt{a^{2}+x^{2}}}{x}\right)\right)=\frac{-a}{x\sqrt{a^{2}+x^{2}}} \frac{d}{dx}\left(\log\left(\frac{a+\sqrt{a^{2}+x^{2}}}{x}\right)\right) =\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\frac{d}{dx}\left(\frac{a+\sqrt{a^{2}+x^{2}}}{x}\right) =\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\frac{d}{dx}\left(x^{-1}\left(a+\sqrt{a^{2}+x^{2}}\right)\right) =\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\left((x^{-1})'\left(a+\sqrt{a^{2}+x^{2}}\right)\right)\left(x^{-1}\frac{d}{dx}\left(a+\sqrt{a^{2}+x^{2}}\right)\right) =\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\left((-1\cdot x^{-2})\left(a+\sqrt{a^{2}+x^{2}}\right)\right)\left(x^{-1}\frac{d}{dx}\left(\sqrt{a^{2}+x^{2}}\right)\right) =\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\left(-x^{-2}\left(a+\sqrt{a^{2}+x^{2}}\right)\right)\left(x^{-1}\frac{d}{dx}\left(\left(a^{2}+x^{2}\right)^{\frac{1}{2}}\right)\right),"['derivatives', 'systems-of-equations']"
72,Can the tangent line be defined independently of the derivative?,Can the tangent line be defined independently of the derivative?,,"The graph of the function $f:x \mapsto x^{1/3}$ has a 'vertical tangent' at $x=0$ : Although this idea is certainly geometrically sound, from what I understand the tangent line is defined by the derivative, not vice versa. In other words, the tangent line to a function at the point $(a,f(a))$ is simply the line given by the equation $$ y - f(a) = f'(a)(x-a) \, , $$ where $f'(a)$ is of course defined as a limit. Since $f'(0)$ does not exist in this case, I'm unsure if we can truly say that the graph has a vertical tangent. The intuitive idea of a tangent 'just touching' the curve breaks down when we consider, for instance, the graph of a linear function, where the tangent touches the graph of the function itself at infinitely many points. Nevertheless, I have heard people say that the tangent line is fundamentally a geometric concept. Although the slope of the tangent line 'agrees' with the derivative if the derivative exists, there are instances where the tangent line is a meaningful concept even when the derivative doesn't exist. If this be the case, then what is the formal definition of a tangent?","The graph of the function has a 'vertical tangent' at : Although this idea is certainly geometrically sound, from what I understand the tangent line is defined by the derivative, not vice versa. In other words, the tangent line to a function at the point is simply the line given by the equation where is of course defined as a limit. Since does not exist in this case, I'm unsure if we can truly say that the graph has a vertical tangent. The intuitive idea of a tangent 'just touching' the curve breaks down when we consider, for instance, the graph of a linear function, where the tangent touches the graph of the function itself at infinitely many points. Nevertheless, I have heard people say that the tangent line is fundamentally a geometric concept. Although the slope of the tangent line 'agrees' with the derivative if the derivative exists, there are instances where the tangent line is a meaningful concept even when the derivative doesn't exist. If this be the case, then what is the formal definition of a tangent?","f:x \mapsto x^{1/3} x=0 (a,f(a)) 
y - f(a) = f'(a)(x-a) \, ,
 f'(a) f'(0)","['calculus', 'derivatives', 'tangent-line']"
73,Prove that the sum of derivatives of a non-negative function is non-negative [duplicate],Prove that the sum of derivatives of a non-negative function is non-negative [duplicate],,This question already has answers here : Sum of derivatives of a polynomial (3 answers) Closed 3 years ago . Let $f(x)$ be a polynomial of degree $n$ with real coefficients such that $f(x)$ is non-negative for all real $x$ . Let $g(x)=f(x)+f'(x)+f''(x)+\dots$ be the sum of $f(x)$ and the first $n$ derivatives of $f(x)$ . Show that $g(x)$ is non-negative for all real $x$ . Thanks for any comments/answers in advance!,This question already has answers here : Sum of derivatives of a polynomial (3 answers) Closed 3 years ago . Let be a polynomial of degree with real coefficients such that is non-negative for all real . Let be the sum of and the first derivatives of . Show that is non-negative for all real . Thanks for any comments/answers in advance!,f(x) n f(x) x g(x)=f(x)+f'(x)+f''(x)+\dots f(x) n f(x) g(x) x,"['derivatives', 'polynomials']"
74,Directional Derivatives on a Manifold.,Directional Derivatives on a Manifold.,,Let $M$ be a manifold (if its easier then say a Riemannian Manifold). My aim is to understand directional derivatives of some function $\Phi: M\to \mathbb{R}$ . $\underline{\textit{My Understanding So Far :}}$ For a manifold $M$ the tangent space at a point $p$ (denoted $T_p M$ ) can be seen as the set of equivalence classes of curves on $M$ passing through $p$ . More specifically : https://en.wikipedia.org/wiki/Tangent_space#Definition_via_tangent_curves . A Riemannian Manifold is a manifold $M$ equipped with an inner product on $T_p M \times T_p M$ (which will let us talk about angles and lengths of curves on $M$ ). The classical directional derivative (in direction $v$ and at point $p$ ) of a function acting on $\mathbb{R}^d$ tells you how much the function changes at point $p$ and in direction $v$ . $\underline{\textit{Help : }}$ Im finding it hard to go from the above to the definition of directional derivatives given here https://en.wikipedia.org/wiki/Tangent_space#Tangent_vectors_as_directional_derivatives . Any help ? :),Let be a manifold (if its easier then say a Riemannian Manifold). My aim is to understand directional derivatives of some function . For a manifold the tangent space at a point (denoted ) can be seen as the set of equivalence classes of curves on passing through . More specifically : https://en.wikipedia.org/wiki/Tangent_space#Definition_via_tangent_curves . A Riemannian Manifold is a manifold equipped with an inner product on (which will let us talk about angles and lengths of curves on ). The classical directional derivative (in direction and at point ) of a function acting on tells you how much the function changes at point and in direction . Im finding it hard to go from the above to the definition of directional derivatives given here https://en.wikipedia.org/wiki/Tangent_space#Tangent_vectors_as_directional_derivatives . Any help ? :),M \Phi: M\to \mathbb{R} \underline{\textit{My Understanding So Far :}} M p T_p M M p M T_p M \times T_p M M v p \mathbb{R}^d p v \underline{\textit{Help : }},"['derivatives', 'differential-geometry', 'smooth-manifolds', 'tangent-spaces']"
75,What does $\lim_{x \to a} \frac{f(x)-f(a)-f'(a)(x-a)}{(x-a)^{2}}$ evaluate to?,What does  evaluate to?,\lim_{x \to a} \frac{f(x)-f(a)-f'(a)(x-a)}{(x-a)^{2}},"Problem: Suppose that $f$ is twice continuously differentiable within a neighborhood of the point $a$ and $f''(a) = 4$ . Compute \begin{align*}  \lim_{x \to a} \frac{f(x)-f(a)-f'(a)(x-a)}{(x-a)^{2}} \end{align*} I have done this work \begin{align*} \lim_{x \to a} \frac{f(x)-f(a)-f'(a)(x-a)}{(x-a)^{2}} &= \lim_{x \to a} \frac{\frac{f(x)-f(a)}{x-a} - f'(a)}{x-a} \\ &= \lim_{x\to a}\frac{f'(a)-f'(a)}{x-a} \\ &= 0 \end{align*} This makes sense, but I am not confident in my answer because I didn't use the given that $f''(a) = 4$ . Any help would be greatly appreciated.","Problem: Suppose that is twice continuously differentiable within a neighborhood of the point and . Compute I have done this work This makes sense, but I am not confident in my answer because I didn't use the given that . Any help would be greatly appreciated.","f a f''(a) = 4 \begin{align*}
 \lim_{x \to a} \frac{f(x)-f(a)-f'(a)(x-a)}{(x-a)^{2}} \end{align*} \begin{align*}
\lim_{x \to a} \frac{f(x)-f(a)-f'(a)(x-a)}{(x-a)^{2}} &= \lim_{x \to a} \frac{\frac{f(x)-f(a)}{x-a} - f'(a)}{x-a} \\
&= \lim_{x\to a}\frac{f'(a)-f'(a)}{x-a} \\
&= 0
\end{align*} f''(a) = 4","['real-analysis', 'calculus', 'derivatives']"
76,"Continuity/Differentiability of ""Cosine Mixture Function""","Continuity/Differentiability of ""Cosine Mixture Function""",,"In the paper ""A literature survey of benchmark functions for global optimization problems"" at this arxiv link https://arxiv.org/pdf/1308.4008.pdf , it describes the Cosine Mixture Function as ""Discontinuous, Non-Differentiable, Separable, Scalable, Multimodal"". The n-dimensional function is as follows: $$f(x)=-0.1\sum_{i=1}^n \cos{5\pi x_i}-\sum_{i=1}^n x_i^2$$ I'm not great at maths so it isn't immediately apparent where this function is discontinuous, and I can't see any areas where the function is undefined. Any help would be greatly appreciated as I'm pretty sure I am missing something.","In the paper ""A literature survey of benchmark functions for global optimization problems"" at this arxiv link https://arxiv.org/pdf/1308.4008.pdf , it describes the Cosine Mixture Function as ""Discontinuous, Non-Differentiable, Separable, Scalable, Multimodal"". The n-dimensional function is as follows: I'm not great at maths so it isn't immediately apparent where this function is discontinuous, and I can't see any areas where the function is undefined. Any help would be greatly appreciated as I'm pretty sure I am missing something.",f(x)=-0.1\sum_{i=1}^n \cos{5\pi x_i}-\sum_{i=1}^n x_i^2,"['calculus', 'derivatives', 'optimization', 'continuity']"
77,Does the existence of the second derivative implies the existence of the first?,Does the existence of the second derivative implies the existence of the first?,,"It can be a Spivak's ""Calculus"" question, but I have not found anywhere so far the answer. The second derivative seems to be defined in terms of the first, but is there any way to formally prove that in general the existence of the second derivative implies the existence of the first?.","It can be a Spivak's ""Calculus"" question, but I have not found anywhere so far the answer. The second derivative seems to be defined in terms of the first, but is there any way to formally prove that in general the existence of the second derivative implies the existence of the first?.",,"['calculus', 'integration', 'derivatives']"
78,"Can we make sense of $\frac{\partial f(x_1, x_2, \dots, x_n)}{\partial g(x_1, x_2, \dots, x_n)}$?",Can we make sense of ?,"\frac{\partial f(x_1, x_2, \dots, x_n)}{\partial g(x_1, x_2, \dots, x_n)}","Let $f, g \in \mathbb R^n \rightarrow \mathbb R$ . Is there some way to make sense of the expression: $$\frac{\partial f(x_1, x_2, \dots, x_n)}{\partial g(x_1, x_2, \dots, x_n)}$$ I want some way to measure ""how much $f$ changes along $g$ "" --- I'm not sure what a reasonable definition of this. Here is one that came to mind. Let us define $\frac{\partial f}{\partial g}\big(t \big)$ : At each point $t \in \mathbb R^n$ , we first compute $g'(t) \in \mathbb R^n$ . Now, we compute the directional derivative of $f$ along $g'(t)$ : $$ \frac{\partial f(x_1, x_2, \dots x_n)}{g(x_1, x_2, \dots x_n)} (t) : \mathbb R^n \rightarrow \mathbb R\equiv (\nabla_{g'(t)} f)(t) = \lim_{h \rightarrow 0} \frac{f(t + hg'(t)) - f(t)}{h}$$ Is this a well-known idea? If so, what is it called? My problem with this is that it returns a scalar at each point: what I am actually interested in is to find a new function which tells me ""how to move $f$ infinitesimally so we can make it closer to $g$ . The given definition above clearly generalizes to any manifold: all I need is a directional derivative, which I do possess on a manifold: Can we say something interesting about this in a larger setting?","Let . Is there some way to make sense of the expression: I want some way to measure ""how much changes along "" --- I'm not sure what a reasonable definition of this. Here is one that came to mind. Let us define : At each point , we first compute . Now, we compute the directional derivative of along : Is this a well-known idea? If so, what is it called? My problem with this is that it returns a scalar at each point: what I am actually interested in is to find a new function which tells me ""how to move infinitesimally so we can make it closer to . The given definition above clearly generalizes to any manifold: all I need is a directional derivative, which I do possess on a manifold: Can we say something interesting about this in a larger setting?","f, g \in \mathbb R^n \rightarrow \mathbb R \frac{\partial f(x_1, x_2, \dots, x_n)}{\partial g(x_1, x_2, \dots, x_n)} f g \frac{\partial f}{\partial g}\big(t \big) t \in \mathbb R^n g'(t) \in \mathbb R^n f g'(t) 
\frac{\partial f(x_1, x_2, \dots x_n)}{g(x_1, x_2, \dots x_n)} (t) : \mathbb R^n \rightarrow \mathbb R\equiv (\nabla_{g'(t)} f)(t) = \lim_{h \rightarrow 0} \frac{f(t + hg'(t)) - f(t)}{h} f g","['calculus', 'derivatives', 'differential-geometry', 'partial-derivative']"
79,Matrix Differentiation of Generative Neural Network,Matrix Differentiation of Generative Neural Network,,"I wish to construct a neural network (restricted Boltzmann Machine) with network parameter set $\Omega$ that will minimise the cost function, \begin{equation} \mathcal{C}(\Omega) = -\text{Tr}\big[A\log(B_\Omega)\big]  \end{equation} where $B_\Omega$ is a Hermitian, postive semi-definite matrix generated by the neural network, and $A$ is a fixed Hermitian, postive semi-definite matrix. Due to the $\log(\cdot)$ , this appears very tricky. How do I take derivates of this cost function with respect to an arbitrary parameter $\Omega_i$ of the network? Is this out of reach analytically and perhaps automatic differentiation is a better approach?","I wish to construct a neural network (restricted Boltzmann Machine) with network parameter set that will minimise the cost function, where is a Hermitian, postive semi-definite matrix generated by the neural network, and is a fixed Hermitian, postive semi-definite matrix. Due to the , this appears very tricky. How do I take derivates of this cost function with respect to an arbitrary parameter of the network? Is this out of reach analytically and perhaps automatic differentiation is a better approach?","\Omega \begin{equation}
\mathcal{C}(\Omega) = -\text{Tr}\big[A\log(B_\Omega)\big] 
\end{equation} B_\Omega A \log(\cdot) \Omega_i","['derivatives', 'optimization', 'matrix-calculus', 'neural-networks']"
80,Why do we involve the left-hand derivative?,Why do we involve the left-hand derivative?,,"I've always thought of slope as ""change in y as x increases by 1,"" so when I think of the slope at a point on a curve, I think of ""how would y change as x increases by 1 from this point on."" The left-hand derivative kind of seems backwards to me, like ""at what rate did y change at in order to get to this point."" Why do we force a double-sided limit to exist for differentiability? Why do we need both derivatives to be equal? Please help me understand this.","I've always thought of slope as ""change in y as x increases by 1,"" so when I think of the slope at a point on a curve, I think of ""how would y change as x increases by 1 from this point on."" The left-hand derivative kind of seems backwards to me, like ""at what rate did y change at in order to get to this point."" Why do we force a double-sided limit to exist for differentiability? Why do we need both derivatives to be equal? Please help me understand this.",,"['calculus', 'derivatives']"
81,Anti-symmetric derivative implies differentiability,Anti-symmetric derivative implies differentiability,,"Wikipedia defines Symmetric derivative as a limit $$\lim_{h\to 0} \frac{f(x+h)-f(x-h)} {2h}$$ which doesn't imply differentiability. An obvious example is $f(x)=|x|$ or $f=\chi_{\mathbb Q}$ I think a slightly different, unbalanced form of the limit guarantees differentiability.  Especially the following limit $$\lim_{h\to 0} \frac{f(a+2h)-f(a-h)} {3h}=\gamma$$ and the continuity of $f$ at $a$ implies $f'(a)=\gamma$ . My question is, Is the following proof valid? Is it possible to prove an analogous extension with the condition $$\lim_{h\to 0} \frac{f(x+ph)-f(x-qh)} {(p+q)h} = \gamma$$ while $p,q$ are different positive real values? (Proof) Define $$G(h)=\begin{cases}  {\frac{f(a+2h)-f(a-h)} {3h}-\gamma}\quad h\neq 0 \\ 0 \quad\quad\quad \quad \quad\quad\quad\quad h=0\\ \end{cases} $$ and substitute $h$ with $\frac 12 (-\frac 12)^kh = x_k h$ . $$G(x_kh)=\frac{f(a+ (-\frac 12)^kh)-f(a+(-\frac 12)^{k+1}h)} {3x_k h}-\gamma$$ By multiplying $3x_k$ both side and adding up from $k=0$ to $n-1$ , $$3\sum_{k=0}^{n-1}{x_k G(x_k h)}=\sum_{k=0}^{n-1}(\frac{f(a+ (-\frac 12)^kh)-f(a+(-\frac 12)^{k+1}h)} h)- (1-(-\frac 12)^n)\gamma $$ RHS become $$\frac{f(a+h)-f(a-(1/2)^n)} h - (1-(-\frac 12)^n)\gamma\quad \to \quad\frac{f(a+h)-f(a)} h -\gamma$$ by continuity. So we have to show that the left side is zero. Since $|G(x)|<M$ for some $0$ neighborhood, Weierstrass M-test guarantees that $\sum_k ^\infty{x_k G(x_kh)}$ is continuous at $h=0$ and obviously its value is zero.","Wikipedia defines Symmetric derivative as a limit which doesn't imply differentiability. An obvious example is or I think a slightly different, unbalanced form of the limit guarantees differentiability.  Especially the following limit and the continuity of at implies . My question is, Is the following proof valid? Is it possible to prove an analogous extension with the condition while are different positive real values? (Proof) Define and substitute with . By multiplying both side and adding up from to , RHS become by continuity. So we have to show that the left side is zero. Since for some neighborhood, Weierstrass M-test guarantees that is continuous at and obviously its value is zero.","\lim_{h\to 0} \frac{f(x+h)-f(x-h)} {2h} f(x)=|x| f=\chi_{\mathbb Q} \lim_{h\to 0} \frac{f(a+2h)-f(a-h)} {3h}=\gamma f a f'(a)=\gamma \lim_{h\to 0} \frac{f(x+ph)-f(x-qh)} {(p+q)h} = \gamma p,q G(h)=\begin{cases} 
{\frac{f(a+2h)-f(a-h)} {3h}-\gamma}\quad h\neq 0 \\
0 \quad\quad\quad \quad \quad\quad\quad\quad h=0\\
\end{cases}
 h \frac 12 (-\frac 12)^kh = x_k h G(x_kh)=\frac{f(a+ (-\frac 12)^kh)-f(a+(-\frac 12)^{k+1}h)} {3x_k h}-\gamma 3x_k k=0 n-1 3\sum_{k=0}^{n-1}{x_k G(x_k h)}=\sum_{k=0}^{n-1}(\frac{f(a+ (-\frac 12)^kh)-f(a+(-\frac 12)^{k+1}h)} h)- (1-(-\frac 12)^n)\gamma
 \frac{f(a+h)-f(a-(1/2)^n)} h - (1-(-\frac 12)^n)\gamma\quad \to \quad\frac{f(a+h)-f(a)} h -\gamma |G(x)|<M 0 \sum_k ^\infty{x_k G(x_kh)} h=0","['real-analysis', 'calculus', 'proof-verification', 'derivatives', 'weak-derivatives']"
82,"Filling a gap in triangle inequality proof for $d(x,y)= \left\lvert \frac{|x-y|}{1+|x-y|} \right\lvert $.",Filling a gap in triangle inequality proof for .,"d(x,y)= \left\lvert \frac{|x-y|}{1+|x-y|} \right\lvert ","For the function $d:\mathbb{R} \times \mathbb{R} \to \mathbb{R}$ defined as $$d(x,y)= \left\lvert \frac{|x-y|}{1+|x-y|} \right\lvert. $$ I want to prove this is a metric but Im having issues proving the triangle inequality. I know this problem has been discused before and the one idea im trying to prove this is using the fact that $f(x)=\frac{x}{1+x}$ is increasing and also satisfies the subadditivity property. This is $f(a+b) \leq f(a) + f(b)$ for every $a,b \geq 0$ . Im stuck proving the subadditivity, someone mention this can be done with intermediate value theorem but I dont see how. Thanks for reading and helping.","For the function defined as I want to prove this is a metric but Im having issues proving the triangle inequality. I know this problem has been discused before and the one idea im trying to prove this is using the fact that is increasing and also satisfies the subadditivity property. This is for every . Im stuck proving the subadditivity, someone mention this can be done with intermediate value theorem but I dont see how. Thanks for reading and helping.","d:\mathbb{R} \times \mathbb{R} \to \mathbb{R} d(x,y)= \left\lvert \frac{|x-y|}{1+|x-y|} \right\lvert.  f(x)=\frac{x}{1+x} f(a+b) \leq f(a) + f(b) a,b \geq 0","['real-analysis', 'calculus', 'derivatives', 'metric-spaces']"
83,Prove the inequality $\ln {(1+\frac{1}{x})}> \frac{2}{2x+1}$,Prove the inequality,\ln {(1+\frac{1}{x})}> \frac{2}{2x+1},"Prove the inequality $$\ln {(1+\frac{1}{x})}> \frac{2}{2x+1}$$ for $x>0$ . My attempt: Let $$f(x)=\ln {(1+\frac{1}{x})}-\frac{2}{2x+1}$$ Then $$f'(x)=-\frac{1}{x(x+1)}+\frac{4}{(2x+1)^2}$$ $$f''(x)=\frac{1}{x^2}-\frac{1}{(x+1)^2}-\frac{8}{(2x+1)^3}>0$$ Then the function $f$ is convex. There exists a minimal point $x_0$ such that $f(x)\geq f(x_0)$ . However, there's no critical point $x_0$ such that $f'(x_0)=0$ , and $\lim_{x \rightarrow \infty} \sup {f'(x)}=0$ . Then I want to show that $f(x)>0$ , how do I continue my proof? I have been trying another approach using Cauchy's MVT by letting $$f(x)=\ln {x}$$ $$g(x)=\frac{1}{2x+1}$$ such that $$\frac{f(x+1)-f(x)}{g(x+1)-g(x)}=\frac{f'(c)}{g'(c)}$$ where $c \in (x,x+1)$ but failed.  As what I did is $$\ln {(1+\frac{1}{x})}=\frac{1}{c} \cdot \frac{(2c+1)^2}{2} \cdot \frac {2}{(2x+1)(2x+3)}$$ I can't simply do the inequality $$\frac{1}{c} \cdot \frac{(2c+1)^2}{2}>\frac{1}{x} \cdot \frac{(2x+1)^2}{2}$$ as $c>x$ because $\frac{1}{c} < \frac{1}{x}$ but $\frac{(2c+1)^2}{2} > \frac{(2x+1)^2}{2}$ . Edited: Of course, I know that $$\ln {(1+ \frac{1}{x})}>\frac{x}{1+x}$$ for $x>-1$ . I just need to prove that $$\frac{x}{x+1}>\frac{2}{2x+1}$$ But I hope to find out another approach using calculus method.","Prove the inequality for . My attempt: Let Then Then the function is convex. There exists a minimal point such that . However, there's no critical point such that , and . Then I want to show that , how do I continue my proof? I have been trying another approach using Cauchy's MVT by letting such that where but failed.  As what I did is I can't simply do the inequality as because but . Edited: Of course, I know that for . I just need to prove that But I hope to find out another approach using calculus method.","\ln {(1+\frac{1}{x})}> \frac{2}{2x+1} x>0 f(x)=\ln {(1+\frac{1}{x})}-\frac{2}{2x+1} f'(x)=-\frac{1}{x(x+1)}+\frac{4}{(2x+1)^2} f''(x)=\frac{1}{x^2}-\frac{1}{(x+1)^2}-\frac{8}{(2x+1)^3}>0 f x_0 f(x)\geq f(x_0) x_0 f'(x_0)=0 \lim_{x \rightarrow \infty} \sup {f'(x)}=0 f(x)>0 f(x)=\ln {x} g(x)=\frac{1}{2x+1} \frac{f(x+1)-f(x)}{g(x+1)-g(x)}=\frac{f'(c)}{g'(c)} c \in (x,x+1) \ln {(1+\frac{1}{x})}=\frac{1}{c} \cdot \frac{(2c+1)^2}{2} \cdot \frac {2}{(2x+1)(2x+3)} \frac{1}{c} \cdot \frac{(2c+1)^2}{2}>\frac{1}{x} \cdot \frac{(2x+1)^2}{2} c>x \frac{1}{c} < \frac{1}{x} \frac{(2c+1)^2}{2} > \frac{(2x+1)^2}{2} \ln {(1+ \frac{1}{x})}>\frac{x}{1+x} x>-1 \frac{x}{x+1}>\frac{2}{2x+1}","['derivatives', 'proof-verification']"
84,Proof that $\cos(2\theta) = \cos^2\theta-\sin^2\theta$ by showing equivalence of derivatives,Proof that  by showing equivalence of derivatives,\cos(2\theta) = \cos^2\theta-\sin^2\theta,"It can be shown using trig identities that $\cos(2\theta) = \cos^2\theta-\sin^2\theta$ . But if we let $f(x) = \sin(2x)$ , we can differentiate two ways: 1) $$f(x) = \sin(2x) \rightarrow f(x) = 2\sin(x)\cos(x)$$ Differentiating using the product rule we see that : $$f^{'}(x) = 2[\cos(x)\cos(x)-\sin(x)\sin(x)] = 2[\cos^2(x)-\sin^2(x)]$$ 2) If we differentiate $f(x)$ as is, then : $$\frac{d}{dx}f(x) = 2\cos(2x) $$ Therefore: $2\cos(2x) = 2[\cos^2(x) - \sin^2(x)] \rightarrow \cos(2x) = \cos^2(x) -\sin^2(x)$ Is this a viable proof? Thanks in Advance!","It can be shown using trig identities that . But if we let , we can differentiate two ways: 1) Differentiating using the product rule we see that : 2) If we differentiate as is, then : Therefore: Is this a viable proof? Thanks in Advance!",\cos(2\theta) = \cos^2\theta-\sin^2\theta f(x) = \sin(2x) f(x) = \sin(2x) \rightarrow f(x) = 2\sin(x)\cos(x) f^{'}(x) = 2[\cos(x)\cos(x)-\sin(x)\sin(x)] = 2[\cos^2(x)-\sin^2(x)] f(x) \frac{d}{dx}f(x) = 2\cos(2x)  2\cos(2x) = 2[\cos^2(x) - \sin^2(x)] \rightarrow \cos(2x) = \cos^2(x) -\sin^2(x),"['derivatives', 'trigonometry']"
85,Is there a smarter way to differentiate the function $f(x) = \sin^{-1} \frac{2x}{1+x^2}$?,Is there a smarter way to differentiate the function ?,f(x) = \sin^{-1} \frac{2x}{1+x^2},"Given $f(x) = \sin^{-1} \frac{2x}{1+x^2}$ , Prove that $$f'(x) = \begin{cases}\phantom{-}\frac{2}{1+x^2},\,|x|<1 \\\\ -\frac{2}{1+x^2},\,|x|>1 \end{cases}$$ Obviously the standard approach would be to use the chain rule and simplify from there. But I noticed that some of these expressions are familiar, specifically, from the tangent half-angle formulae: If $x = \tan \frac \theta 2$ , then $\sin \theta = \frac{2x}{1+x^2}$ and $\frac{d\theta}{dx} = \frac{2}{1+x^2}$ . So my question is: can this observation be used to construct a more elegant proof?","Given , Prove that Obviously the standard approach would be to use the chain rule and simplify from there. But I noticed that some of these expressions are familiar, specifically, from the tangent half-angle formulae: If , then and . So my question is: can this observation be used to construct a more elegant proof?","f(x) = \sin^{-1} \frac{2x}{1+x^2} f'(x) = \begin{cases}\phantom{-}\frac{2}{1+x^2},\,|x|<1 \\\\ -\frac{2}{1+x^2},\,|x|>1 \end{cases} x = \tan \frac \theta 2 \sin \theta = \frac{2x}{1+x^2} \frac{d\theta}{dx} = \frac{2}{1+x^2}","['calculus', 'derivatives']"
86,Find the $n$-th derivative of $f(x)=\frac{x}{\sqrt{1-x}}$,Find the -th derivative of,n f(x)=\frac{x}{\sqrt{1-x}},"Find the $n$-th derivative of $$f(x)=\frac{x}{\sqrt{1-x}}$$ First I just calculated the first, second and 3-th, 4-th derivatives and now I want to summarize the general formula. But it seems too complicated. Then I want to use binomial theorem or Taylor expansion... Also got no more clues.","Find the $n$-th derivative of $$f(x)=\frac{x}{\sqrt{1-x}}$$ First I just calculated the first, second and 3-th, 4-th derivatives and now I want to summarize the general formula. But it seems too complicated. Then I want to use binomial theorem or Taylor expansion... Also got no more clues.",,"['calculus', 'derivatives']"
87,How is this definition of differentiability for an arbitrary subset $S$ of $\mathbb{R}^n$ common and used?,How is this definition of differentiability for an arbitrary subset  of  common and used?,S \mathbb{R}^n,"In the book of Analysis On Manifold by Munkres, at page 144, in the exercise 3.a, the author defines differentiability for an arbitrary subset $S$ of $\mathbb{R}^n$ as Let $S$ be an arbitrary subset of $\mathbb{R}^n$; let $x_0 \in S$. We   say that $f:S \to \mathbb{R}$ is diff'able at $x_0$, of class $C^r$,   provided that there is a a $C^r$ function $g: U\to \mathbb{R}$ defined   in a neighbourhood of $U$ of $x_0$ in $\mathbb{R}^n$, such that $g$   agrees with $f$ on the set $U \cap S$. However, considering the answers given to this question, how is the above definition common and used ?","In the book of Analysis On Manifold by Munkres, at page 144, in the exercise 3.a, the author defines differentiability for an arbitrary subset $S$ of $\mathbb{R}^n$ as Let $S$ be an arbitrary subset of $\mathbb{R}^n$; let $x_0 \in S$. We   say that $f:S \to \mathbb{R}$ is diff'able at $x_0$, of class $C^r$,   provided that there is a a $C^r$ function $g: U\to \mathbb{R}$ defined   in a neighbourhood of $U$ of $x_0$ in $\mathbb{R}^n$, such that $g$   agrees with $f$ on the set $U \cap S$. However, considering the answers given to this question, how is the above definition common and used ?",,"['calculus', 'real-analysis', 'derivatives']"
88,Maxima and minima of $\operatorname{sinc}$ function,Maxima and minima of  function,\operatorname{sinc},"The function $\operatorname{sinc}{\pi x}$ has maxima and minima given by the function's intersections with $\cos \pi x$, or alternatively by $\frac {d}{dx}\operatorname{sinc}{\pi x}=0$. Mathematica tells me that $$\frac {d}{dx}\operatorname{sinc}{\pi x}=\pi \Bigl(\frac {\cos \pi x}{\pi x}-\frac {\sin \pi x}{\pi^2 x^2}\Bigr)$$ So question 1, how do I prove this? And question 2, how do I derive an equation for all maxima and minima?","The function $\operatorname{sinc}{\pi x}$ has maxima and minima given by the function's intersections with $\cos \pi x$, or alternatively by $\frac {d}{dx}\operatorname{sinc}{\pi x}=0$. Mathematica tells me that $$\frac {d}{dx}\operatorname{sinc}{\pi x}=\pi \Bigl(\frac {\cos \pi x}{\pi x}-\frac {\sin \pi x}{\pi^2 x^2}\Bigr)$$ So question 1, how do I prove this? And question 2, how do I derive an equation for all maxima and minima?",,"['trigonometry', 'derivatives']"
89,Differentiability of the Riemann function,Differentiability of the Riemann function,,"I have defined the function $\zeta : (1, +\infty) \longrightarrow \mathbb{R}$ defined as: $$\zeta(x) = \sum_{n=1}^{+\infty} \frac{1}{n^x}$$ I have to study the differentiability of this function. I have tried to use the theorem that allows you to swap summation and derivative, but I don't know how to prove that $\sum_{n=1}^{+\infty} g'(x)$ converges uniformly, necessary to use the theorem, where $g(x) := \frac{1}{n^x}$","I have defined the function defined as: I have to study the differentiability of this function. I have tried to use the theorem that allows you to swap summation and derivative, but I don't know how to prove that converges uniformly, necessary to use the theorem, where","\zeta : (1, +\infty) \longrightarrow \mathbb{R} \zeta(x) = \sum_{n=1}^{+\infty} \frac{1}{n^x} \sum_{n=1}^{+\infty} g'(x) g(x) := \frac{1}{n^x}","['real-analysis', 'derivatives', 'riemann-zeta', 'riemann-integration']"
90,Is there a well defined difference between $\nabla$ and $D$?,Is there a well defined difference between  and ?,\nabla D,"When we apply $\nabla$ or $D$ to a function $f:\mathbb R^n\to \mathbb R$, then they in principle do the same operation. However, in textbooks $\nabla$ is often written as a column vector $(\partial_1,...,\partial_n)^T$, whereas $D$ is written as a row vector $(\partial_1,...,\partial_n)$. Is there a well defined difference between these two operators when they are applied to a single-valued real function?","When we apply $\nabla$ or $D$ to a function $f:\mathbb R^n\to \mathbb R$, then they in principle do the same operation. However, in textbooks $\nabla$ is often written as a column vector $(\partial_1,...,\partial_n)^T$, whereas $D$ is written as a row vector $(\partial_1,...,\partial_n)$. Is there a well defined difference between these two operators when they are applied to a single-valued real function?",,['derivatives']
91,Proving area under curve,Proving area under curve,,"Hey guys I've been stuck on this problem for a while and I've still got no headway into how I'm supposed to do this. Any tips or guiding points would be helpful. Let $f$ be a differentiable function such that $f′$ is continuous on $[0,1]$, and $M$ the maximum value of $|f′(x)|$ on $[0,1]$. Prove that if $f(0) = f(1) = 0$, then $$\int_0^1 |f(x)| dx\leq\frac{M}{4}$$ I've come to a conclusion via Rolle's theorem that there exists a c between 0 and 1 where f'(c) = 0 and that would be a local extrema. But nothing past that other than the fact that the max value of M is 1.","Hey guys I've been stuck on this problem for a while and I've still got no headway into how I'm supposed to do this. Any tips or guiding points would be helpful. Let $f$ be a differentiable function such that $f′$ is continuous on $[0,1]$, and $M$ the maximum value of $|f′(x)|$ on $[0,1]$. Prove that if $f(0) = f(1) = 0$, then $$\int_0^1 |f(x)| dx\leq\frac{M}{4}$$ I've come to a conclusion via Rolle's theorem that there exists a c between 0 and 1 where f'(c) = 0 and that would be a local extrema. But nothing past that other than the fact that the max value of M is 1.",,"['calculus', 'real-analysis', 'derivatives']"
92,Intermediate Value Theorem and Fundamental Theorem of Calculus question,Intermediate Value Theorem and Fundamental Theorem of Calculus question,,"I received a question on a previous exam, but I had no clue how to go about doing it.  I know I'm supposed to use the MVT, IVT and FTC, but I'm not sure where.  The question is Suppose $f(x)$ is integrable on $[a,b]$, with $f(x)\geq0$ on $[a,b]$, and that $g(x)$ is continuous on $[a,b]$.  Assuming that $f(x)g(x)$ is integrable on $[a,b]$, show that $\exists c\in[a,b]$ so that $$\int^b_af(x)g(x)dx=g(c)\int^b_af(x)dx.$$ Thank you in advance.","I received a question on a previous exam, but I had no clue how to go about doing it.  I know I'm supposed to use the MVT, IVT and FTC, but I'm not sure where.  The question is Suppose $f(x)$ is integrable on $[a,b]$, with $f(x)\geq0$ on $[a,b]$, and that $g(x)$ is continuous on $[a,b]$.  Assuming that $f(x)g(x)$ is integrable on $[a,b]$, show that $\exists c\in[a,b]$ so that $$\int^b_af(x)g(x)dx=g(c)\int^b_af(x)dx.$$ Thank you in advance.",,"['integration', 'derivatives', 'continuity', 'convex-analysis']"
93,What does Gradient actually mean?,What does Gradient actually mean?,,"One day I was reading my physics book and suddenly I came across the word gradient . The following mathematical method is used in my book to determine gradient: $$\Omega(x,y,z)=3x\cdot(y^2)\cdot(z^3)-4xy$$ Now they determined the gradient in point (2,1,1) and it was $$ \nabla \Omega = 7i-20j+18k$$ Now what does it actually mean? I mean if we know that 20 N of net force is applied on a mass of 2 kg than it will have an acceleration of 10 $\rm m/s^2$. In my example, the gradient in point (2,1,1) is determined. What information can I get from that? The same is also for divergence and curl.","One day I was reading my physics book and suddenly I came across the word gradient . The following mathematical method is used in my book to determine gradient: $$\Omega(x,y,z)=3x\cdot(y^2)\cdot(z^3)-4xy$$ Now they determined the gradient in point (2,1,1) and it was $$ \nabla \Omega = 7i-20j+18k$$ Now what does it actually mean? I mean if we know that 20 N of net force is applied on a mass of 2 kg than it will have an acceleration of 10 $\rm m/s^2$. In my example, the gradient in point (2,1,1) is determined. What information can I get from that? The same is also for divergence and curl.",,"['vectors', 'derivatives', 'vector-fields']"
94,Study the continuity domain and derivability domain,Study the continuity domain and derivability domain,,"Let $(u_n)_{n\in\mathbb{N}}$ with the general term $u_n=\frac {1+x^{n}}{1+x+x^{2}+...+x^{n+p-1}}$, where $x\ge0$ and $p \in \mathbb{N}$. Let $f(x)= \lim_{n\to\infty}u_n$. Find the differentiability and continuity domain. First I tried to simplify a little $u_n$ using the sum of the geometric progression and I got this $$u_n=\frac{(1+x^{n})(x-1)}{x^{n+p}-1}.$$ So if $x \in (0,1)$, then $f(x)=1-x$. What should I do when $x = 1$ and $x>1$?","Let $(u_n)_{n\in\mathbb{N}}$ with the general term $u_n=\frac {1+x^{n}}{1+x+x^{2}+...+x^{n+p-1}}$, where $x\ge0$ and $p \in \mathbb{N}$. Let $f(x)= \lim_{n\to\infty}u_n$. Find the differentiability and continuity domain. First I tried to simplify a little $u_n$ using the sum of the geometric progression and I got this $$u_n=\frac{(1+x^{n})(x-1)}{x^{n+p}-1}.$$ So if $x \in (0,1)$, then $f(x)=1-x$. What should I do when $x = 1$ and $x>1$?",,"['calculus', 'real-analysis', 'derivatives']"
95,"Is Differentiation of total variation function $V(f, [0,x])$ possible in this condition?",Is Differentiation of total variation function  possible in this condition?,"V(f, [0,x])","I am having difficulty of determining whether differentiation of total variation function $V(f, [0,x])$ is possible on $x = 0$ when $f:[0,1]→R$ is defined as: $ f(x) = 0$ for $x = 0$ and $f(x) = x^2sin(1/x)$ for $x≠0$ I think it would not be possible intuitively, but I cannot come up with precise proof.","I am having difficulty of determining whether differentiation of total variation function $V(f, [0,x])$ is possible on $x = 0$ when $f:[0,1]→R$ is defined as: $ f(x) = 0$ for $x = 0$ and $f(x) = x^2sin(1/x)$ for $x≠0$ I think it would not be possible intuitively, but I cannot come up with precise proof.",,"['real-analysis', 'derivatives', 'bounded-variation']"
96,Prove that function $f$ is injective if its Jacobian matrix is positive definite.,Prove that function  is injective if its Jacobian matrix is positive definite.,f,"Assume that $\Omega\in\mathbb{R}^m$ is an open convex set and the vector-valued function $f:\Omega\rightarrow\mathbb{R}^m$ is differentiable. If Jacobian matrix $J_f(x)$ is positive definite for all $x\in\Omega$, prove that $f$ is an injective function on $\Omega$. I have no way of dealing with it. Is there a theorem to do with it?","Assume that $\Omega\in\mathbb{R}^m$ is an open convex set and the vector-valued function $f:\Omega\rightarrow\mathbb{R}^m$ is differentiable. If Jacobian matrix $J_f(x)$ is positive definite for all $x\in\Omega$, prove that $f$ is an injective function on $\Omega$. I have no way of dealing with it. Is there a theorem to do with it?",,"['calculus', 'real-analysis', 'linear-algebra', 'derivatives']"
97,The $n$-th derivative of the reciprocal of a function and a binomial identity,The -th derivative of the reciprocal of a function and a binomial identity,n,"While I was looking for an answer to this MSE post in order to prove \begin{align*} \frac{d^n}{dx^n}\left(\frac{1}{1-e^{-x}}\right)=(-1)^n\sum_{j=1}^n{n\brace j}j!\frac{e^{-jx}}{\left(1-e^{-jx}\right)^{j+1}}\tag{1} \end{align*} with the  numbers ${n\brace j}$ denoting the Stirling numbers of the second kind I considered the following formula of the reciprocal of the $n$-th derivative of a function which might be interesting by itself. With $D_x:=\frac{d}{dx}$ the following relationship is valid according to (3.63) in H.W. Goulds Binomial Identities, vol. I \begin{align*} D_x^n\left(\frac{1}{f(x)}\right)=\sum_{j=0}^n(-1)^j\binom{n+1}{j+1}\frac{1}{\left(f(x)\right)^{j+1}}D_x^n\left(\left(f(x)\right)^j\right) \end{align*} $$ $$ Applying this formula to the function $f(x)=\frac{1}{1-e^{-x}}$ we obtain   \begin{align*} D_x^n&\left(\frac{1}{1-e^{-x}}\right)\\ &=\sum_{j=0}^n(-1)^j\binom{n+1}{j+1}\frac{1}{\left(1-e^{-x}\right)^{j+1}}D_x^n\left(\left(1-e^{-x}\right)^j\right)\\ &=\sum_{j=0}^n\frac{(-1)^j}{\left(1-e^{-x}\right)^{j+1}}\binom{n+1}{j+1}D_x^n\left(\sum_{k=0}^j\binom{j}{k}(-1)^ke^{-kx}\right)\\ &=(-1)^n\sum_{j=0}^n\frac{(-1)^j}{\left(1-e^{-x}\right)^{j+1}}\binom{n+1}{j+1}\sum_{k=0}^j\binom{j}{k}(-1)^kk^ne^{-kx}\\ &=\frac{(-1)^n}{(1-e^{-x})^{n+1}}\sum_{j=1}^n(-1)^j\left(1-e^{-x}\right)^{n-j}\binom{n+1}{j+1}\sum_{k=1}^j\binom{j}{k}(-1)^kk^ne^{-kx}\tag{2} \end{align*} On the other hand with the identity ${n\brace j}=\frac{1}{j!}\sum_{k=0}^j(-1)^{j-k}\binom{j}{k}k^n$ we obtain from (1)   \begin{align*} D_x^n&\left(\frac{1}{1-e^{-x}}\right)\\ &=(-1)^n\sum_{j=1}^n{n\brace j}j!\frac{e^{-jx}}{\left(1-e^{-jx}\right)^{j+1}}\\ &=(-1)^n\sum_{j=1}^n\frac{e^{-jx}}{\left(1-e^{-jx}\right)^{j+1}}\sum_{k=0}^j(-1)^{j-k}\binom{j}{k}k^n\\ &=\frac{(-1)^n}{(1-e^{-x})^{n+1}}\sum_{j=1}^n(-1)^je^{-jx}(1-e^{-x})^{n-j}\sum_{k=1}^j\binom{j}{k}(-1)^{k}k^n\tag{3} \end{align*} I have difficulties to prove the equality of (2) with (3). So putting $y=e^{-x}$ I would like to ask for a prove of the following relationship Claim: The following is valid for $n\geq 1$ and $y\geq 0$.   \begin{align*} \sum_{j=1}^n&(-1)^j\left(1-y\right)^{n-j}\binom{n+1}{j+1}\sum_{k=1}^j\binom{j}{k}(-1)^kk^ny^k\\ &=\sum_{j=1}^n(-1)^jy^j(1-y)^{n-j}\sum_{k=1}^j\binom{j}{k}(-1)^{k}k^n \end{align*} Please note I'm not interested in a proof by induction. I would like to see how to transform one side into the other, maybe with the help of generating functions. Many thanks in advance.","While I was looking for an answer to this MSE post in order to prove \begin{align*} \frac{d^n}{dx^n}\left(\frac{1}{1-e^{-x}}\right)=(-1)^n\sum_{j=1}^n{n\brace j}j!\frac{e^{-jx}}{\left(1-e^{-jx}\right)^{j+1}}\tag{1} \end{align*} with the  numbers ${n\brace j}$ denoting the Stirling numbers of the second kind I considered the following formula of the reciprocal of the $n$-th derivative of a function which might be interesting by itself. With $D_x:=\frac{d}{dx}$ the following relationship is valid according to (3.63) in H.W. Goulds Binomial Identities, vol. I \begin{align*} D_x^n\left(\frac{1}{f(x)}\right)=\sum_{j=0}^n(-1)^j\binom{n+1}{j+1}\frac{1}{\left(f(x)\right)^{j+1}}D_x^n\left(\left(f(x)\right)^j\right) \end{align*} $$ $$ Applying this formula to the function $f(x)=\frac{1}{1-e^{-x}}$ we obtain   \begin{align*} D_x^n&\left(\frac{1}{1-e^{-x}}\right)\\ &=\sum_{j=0}^n(-1)^j\binom{n+1}{j+1}\frac{1}{\left(1-e^{-x}\right)^{j+1}}D_x^n\left(\left(1-e^{-x}\right)^j\right)\\ &=\sum_{j=0}^n\frac{(-1)^j}{\left(1-e^{-x}\right)^{j+1}}\binom{n+1}{j+1}D_x^n\left(\sum_{k=0}^j\binom{j}{k}(-1)^ke^{-kx}\right)\\ &=(-1)^n\sum_{j=0}^n\frac{(-1)^j}{\left(1-e^{-x}\right)^{j+1}}\binom{n+1}{j+1}\sum_{k=0}^j\binom{j}{k}(-1)^kk^ne^{-kx}\\ &=\frac{(-1)^n}{(1-e^{-x})^{n+1}}\sum_{j=1}^n(-1)^j\left(1-e^{-x}\right)^{n-j}\binom{n+1}{j+1}\sum_{k=1}^j\binom{j}{k}(-1)^kk^ne^{-kx}\tag{2} \end{align*} On the other hand with the identity ${n\brace j}=\frac{1}{j!}\sum_{k=0}^j(-1)^{j-k}\binom{j}{k}k^n$ we obtain from (1)   \begin{align*} D_x^n&\left(\frac{1}{1-e^{-x}}\right)\\ &=(-1)^n\sum_{j=1}^n{n\brace j}j!\frac{e^{-jx}}{\left(1-e^{-jx}\right)^{j+1}}\\ &=(-1)^n\sum_{j=1}^n\frac{e^{-jx}}{\left(1-e^{-jx}\right)^{j+1}}\sum_{k=0}^j(-1)^{j-k}\binom{j}{k}k^n\\ &=\frac{(-1)^n}{(1-e^{-x})^{n+1}}\sum_{j=1}^n(-1)^je^{-jx}(1-e^{-x})^{n-j}\sum_{k=1}^j\binom{j}{k}(-1)^{k}k^n\tag{3} \end{align*} I have difficulties to prove the equality of (2) with (3). So putting $y=e^{-x}$ I would like to ask for a prove of the following relationship Claim: The following is valid for $n\geq 1$ and $y\geq 0$.   \begin{align*} \sum_{j=1}^n&(-1)^j\left(1-y\right)^{n-j}\binom{n+1}{j+1}\sum_{k=1}^j\binom{j}{k}(-1)^kk^ny^k\\ &=\sum_{j=1}^n(-1)^jy^j(1-y)^{n-j}\sum_{k=1}^j\binom{j}{k}(-1)^{k}k^n \end{align*} Please note I'm not interested in a proof by induction. I would like to see how to transform one side into the other, maybe with the help of generating functions. Many thanks in advance.",,"['derivatives', 'summation', 'binomial-coefficients', 'generating-functions', 'stirling-numbers']"
98,Prove $f(c) = c$ under a condition.,Prove  under a condition.,f(c) = c,"Suppose $f(x)$ is continuous on $[0,1]$ and $f(0) = 1, f(1) = 0$. Prove that there is a point $c$ in $(0, 1)$ such that $f(c) = c$. let $l(x) = x$ and $d(x) = f(x) - l(x)$. We have $d(1) = f(1) - l(1) = -1$ and $d(0) = 1$. We divide $[-1, 1]$ in $H$ equal parts , where $H$ is a infinite hyperinterger. $$-1, -1 + \delta, -1 + 2\delta, \ ... \  , -1 + H\delta = 1$$ Let $-1 + K\delta$ be the last partition point such that $d^*(-1 + K\delta) < 0$ $$\therefore d^*(-1 + K\delta) < 0 < d^*(-1 + (K+1)\delta)$$ $$\therefore f^*(-1 + K\delta) < -1 + K\delta  < f^*(-1 + (K+1)\delta) - \delta$$ Since $f^*(-1 + K\delta) \approx  f^*(-1 + (K+1)\delta) - \delta$, therefore $f^*(-1 + K\delta) \approx -1 + K\delta$ Let $c = st(-1 + K\delta)$ By taking standard part of the $f^*(-1 + K\delta) \approx -1 + K\delta$ , we get $f(c) = c$. I think this is probably correct but to be on the safe side please check my proof.","Suppose $f(x)$ is continuous on $[0,1]$ and $f(0) = 1, f(1) = 0$. Prove that there is a point $c$ in $(0, 1)$ such that $f(c) = c$. let $l(x) = x$ and $d(x) = f(x) - l(x)$. We have $d(1) = f(1) - l(1) = -1$ and $d(0) = 1$. We divide $[-1, 1]$ in $H$ equal parts , where $H$ is a infinite hyperinterger. $$-1, -1 + \delta, -1 + 2\delta, \ ... \  , -1 + H\delta = 1$$ Let $-1 + K\delta$ be the last partition point such that $d^*(-1 + K\delta) < 0$ $$\therefore d^*(-1 + K\delta) < 0 < d^*(-1 + (K+1)\delta)$$ $$\therefore f^*(-1 + K\delta) < -1 + K\delta  < f^*(-1 + (K+1)\delta) - \delta$$ Since $f^*(-1 + K\delta) \approx  f^*(-1 + (K+1)\delta) - \delta$, therefore $f^*(-1 + K\delta) \approx -1 + K\delta$ Let $c = st(-1 + K\delta)$ By taking standard part of the $f^*(-1 + K\delta) \approx -1 + K\delta$ , we get $f(c) = c$. I think this is probably correct but to be on the safe side please check my proof.",,"['calculus', 'real-analysis']"
99,"Find the function $f$, given that $ f'(x) = f(x)(1-f(x))$ and that $f(0) = \frac12$","Find the function , given that  and that",f  f'(x) = f(x)(1-f(x)) f(0) = \frac12,"Find the function $f$, given that $$f'(x) = f(x)(1-f(x))$$ and that $f(0) = \frac12$ The answer is $$y = \frac{1}{1+e^{-x}}$$ What I tried doing is changing $f'(x)$ as \frac{dy}{dx} and all the $f(x)$ as $y$ to make it easier to read for myself. Manipulating the equation. I got $$\left(y + \frac1y\right)dy = 1dx$$ but when I integrate I'm not getting the correct answer, I'm pretty sure I'm doing this incorrectly. Any advice?","Find the function $f$, given that $$f'(x) = f(x)(1-f(x))$$ and that $f(0) = \frac12$ The answer is $$y = \frac{1}{1+e^{-x}}$$ What I tried doing is changing $f'(x)$ as \frac{dy}{dx} and all the $f(x)$ as $y$ to make it easier to read for myself. Manipulating the equation. I got $$\left(y + \frac1y\right)dy = 1dx$$ but when I integrate I'm not getting the correct answer, I'm pretty sure I'm doing this incorrectly. Any advice?",,"['calculus', 'integration', 'derivatives']"
