,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Does $\lim_{(x,y) \to (0,0)} \frac{x y \sin^2 y}{x^2 y}$ exists?",Does  exists?,"\lim_{(x,y) \to (0,0)} \frac{x y \sin^2 y}{x^2 y}","$\displaystyle\lim_{(x,y) \to (0,0)} \frac{x y \sin^2 y}{x^2 y}$ Along $x =y$ , it becomes \begin{equation} \lim_{(x,y) \to (0,0)}\frac{y^2 \sin^2 y}{y^3} = 0. \end{equation} But along $x = y^3$ , \begin{equation} \lim_{(x,y) \to (0,0)}\frac{y^4 \sin^2 y}{y^7} = \lim_{(x,y) \to (0,0)}\frac{ \sin^2 y}{y^3}. \end{equation} The limit is undefined. This is what I thought, but using wolframalpha, it say the limit is $0$ . Where did I go wrong?","Along , it becomes But along , The limit is undefined. This is what I thought, but using wolframalpha, it say the limit is . Where did I go wrong?","\displaystyle\lim_{(x,y) \to (0,0)} \frac{x y \sin^2 y}{x^2 y} x =y \begin{equation}
\lim_{(x,y) \to (0,0)}\frac{y^2 \sin^2 y}{y^3} = 0.
\end{equation} x = y^3 \begin{equation}
\lim_{(x,y) \to (0,0)}\frac{y^4 \sin^2 y}{y^7} = \lim_{(x,y) \to (0,0)}\frac{ \sin^2 y}{y^3}.
\end{equation} 0","['calculus', 'multivariable-calculus']"
1,Question about discontinuous function with directional derivatives at a points,Question about discontinuous function with directional derivatives at a points,,"For a function, if at a point $a$ , the function has directional derivatives along some lines, but the function is discontinuous at $a$ , does that mean along those lines, the function is continuous, but along some other directions the function is not? What does the graph of such a function look like? Continuous in some direction but discontinuous in others?","For a function, if at a point , the function has directional derivatives along some lines, but the function is discontinuous at , does that mean along those lines, the function is continuous, but along some other directions the function is not? What does the graph of such a function look like? Continuous in some direction but discontinuous in others?",a a,['calculus']
2,Divergence theorem: Simple solid region OR compact region and piecewise smooth boundary,Divergence theorem: Simple solid region OR compact region and piecewise smooth boundary,,"While searching for the statements of divergence theorem, I came across two different conditions: $(1)$ In wikipedia and proofwiki , it is said that the volume should be compact and boundary should be piecewise smooth. $(2)$ In Stewart's calculus book and this website and a number of websites, it is said that the volume must be simple solid and nothing is said about boundary. So why are there two statements? Which one is more general? Also what exactly is ""piecewise smooth boundary""? I guess it means that the surface $S$ can be broken up into finite number of surfaces $S_1,S_2,S_3,...$ or $S_n$ all of which (considered separately) have a unique tangent plane at every point on each of the surfaces $S_n$ .","While searching for the statements of divergence theorem, I came across two different conditions: In wikipedia and proofwiki , it is said that the volume should be compact and boundary should be piecewise smooth. In Stewart's calculus book and this website and a number of websites, it is said that the volume must be simple solid and nothing is said about boundary. So why are there two statements? Which one is more general? Also what exactly is ""piecewise smooth boundary""? I guess it means that the surface can be broken up into finite number of surfaces or all of which (considered separately) have a unique tangent plane at every point on each of the surfaces .","(1) (2) S S_1,S_2,S_3,... S_n S_n","['calculus', 'multivariable-calculus', 'definite-integrals', 'vector-fields', 'divergence-operator']"
3,Strategies for proving that a critical point is neither maximum nor minimum,Strategies for proving that a critical point is neither maximum nor minimum,,"Let $f(x,y) = x^3 + y^3 -3x$ . Since it's everywhere differentiable, we know that its only critical points are $(1,0),(-1,0)$ given that the gradient $\nabla f(x,y) = (3x^2-3,3y^2)$ is zero if and only $x=\pm1$ and $y=0$ . However, $\det Hf|_{(\pm1,0)} = 0$ , so we can't apply the Second partial derivative test. Since $f$ is odd, we might suspect these are saddle points. One way to proceed is to find curves: $$f \circ \alpha (t) = f\circ (t^2+1, (3^{\frac{1}{3}}t^{\frac{2}{3}})) = (t^2+1)^3 -3$$ $$f \circ \beta (t) = f\circ (-t^2+1, (-3^{\frac{1}{3}}t^{\frac{2}{3}})) = (-t^2+1)^3 -3$$ And it's clear that $f \circ \alpha (t)$ has a minimum at $t=0$ and that $f \circ \beta (t)$ has a maximum at $t=0$ , which implies $(1,0)$ is a saddle point of $f$ . But this method depends on one's ingeniousness and it's very time-consuming. I've lost half an hour trying to find those curves! Are there any other strategies to tackle these kinds of problems?","Let . Since it's everywhere differentiable, we know that its only critical points are given that the gradient is zero if and only and . However, , so we can't apply the Second partial derivative test. Since is odd, we might suspect these are saddle points. One way to proceed is to find curves: And it's clear that has a minimum at and that has a maximum at , which implies is a saddle point of . But this method depends on one's ingeniousness and it's very time-consuming. I've lost half an hour trying to find those curves! Are there any other strategies to tackle these kinds of problems?","f(x,y) = x^3 + y^3 -3x (1,0),(-1,0) \nabla f(x,y) = (3x^2-3,3y^2) x=\pm1 y=0 \det Hf|_{(\pm1,0)} = 0 f f \circ \alpha (t) = f\circ (t^2+1, (3^{\frac{1}{3}}t^{\frac{2}{3}})) = (t^2+1)^3 -3 f \circ \beta (t) = f\circ (-t^2+1, (-3^{\frac{1}{3}}t^{\frac{2}{3}})) = (-t^2+1)^3 -3 f \circ \alpha (t) t=0 f \circ \beta (t) t=0 (1,0) f","['multivariable-calculus', 'maxima-minima', 'hessian-matrix']"
4,$f$ is a smooth map $R^n \rightarrow R^n$ such that $f\circ{f}=f$. Proving that $f(R^n)$ is a smooth surface in $R^n$,is a smooth map  such that . Proving that  is a smooth surface in,f R^n \rightarrow R^n f\circ{f}=f f(R^n) R^n,"Given that $f$ is a smooth map $R^n \rightarrow R^n$ such that $f\circ{f}=f$ , how can I prove that $f(R^n)$ is a smooth surface in $R^n$ ? So, I need to prove that each point of $f(R^n)$ has a neighborhood diffeomorphic to an open subset of $R^k$ , $k \leq n$ . All I managed to understand is that $f$ is an identity function on a subset of $R^n$ and sends the complement of that subset to that same subset. Any hints?","Given that is a smooth map such that , how can I prove that is a smooth surface in ? So, I need to prove that each point of has a neighborhood diffeomorphic to an open subset of , . All I managed to understand is that is an identity function on a subset of and sends the complement of that subset to that same subset. Any hints?",f R^n \rightarrow R^n f\circ{f}=f f(R^n) R^n f(R^n) R^k k \leq n f R^n,"['multivariable-calculus', 'differential-geometry']"
5,What is a neat way to solve $\nabla\mathbf{u}+\nabla\mathbf{u}^T=\mathbf{\mathbf{C}}$?,What is a neat way to solve ?,\nabla\mathbf{u}+\nabla\mathbf{u}^T=\mathbf{\mathbf{C}},"Let $\mathbf{u}:\mathbb{R}^3\to\mathbb{R}^3$ be a smooth enough vector field that satisfies the following equation $$\nabla\mathbf{u}+\nabla\mathbf{u}^T=\mathbf{C},\tag{1}$$ where $\nabla\mathbf{u}$ is gradient of $\mathbf{u}$ and $^T$ denotes transpose of a second order tensor and $\mathbf{C}$ is a constant symmetric second order tensor. What is a neat way to solve $(1)$ ? I mean I am not really inclined to write down the components and solve each scalar equation. I want a basis-free approach. The final answer is $$\mathbf{u}(\mathbf{x})=\mathbf{c}_1+\mathbf{c}_2\times\mathbf{x}+\mathbf{C}\cdot\mathbf{x}\tag{2}$$ where $\mathbf{c}_1$ and $\mathbf{c}_2$ are two constant vectors, $\cdot$ is simple contraction (or generalized dot product) and $\times$ denotes the usual cross product.","Let be a smooth enough vector field that satisfies the following equation where is gradient of and denotes transpose of a second order tensor and is a constant symmetric second order tensor. What is a neat way to solve ? I mean I am not really inclined to write down the components and solve each scalar equation. I want a basis-free approach. The final answer is where and are two constant vectors, is simple contraction (or generalized dot product) and denotes the usual cross product.","\mathbf{u}:\mathbb{R}^3\to\mathbb{R}^3 \nabla\mathbf{u}+\nabla\mathbf{u}^T=\mathbf{C},\tag{1} \nabla\mathbf{u} \mathbf{u} ^T \mathbf{C} (1) \mathbf{u}(\mathbf{x})=\mathbf{c}_1+\mathbf{c}_2\times\mathbf{x}+\mathbf{C}\cdot\mathbf{x}\tag{2} \mathbf{c}_1 \mathbf{c}_2 \cdot \times","['multivariable-calculus', 'partial-differential-equations', 'vector-analysis', 'tensors', 'vector-fields']"
6,"Need help in solving an equation involving volume, single and double layer potentials","Need help in solving an equation involving volume, single and double layer potentials",,"Let be $V \subset \mathbb{R}^n $ , $ 3\leq n$ an open set, where you can apply Gauß's Theorem. To show is, that for all $ U \in C^{(1)} ( \bar{V} ) \cap C^{(2)} (V) $ with  bounded 2nd derivatives the following equation for $y \in V $ : $$ (n-2) \omega_{n-1} U(y) = \int_{ \partial V} \left[\frac{1}{|x-y|^{n-2}} \frac{ \partial U}{ \partial \nu} (x)-U(x) \frac{ \partial }{\partial \nu_x} \frac{1}{|x-y|^{n-2}}\right] d\sigma(x)- \int_V \frac{ \Delta U(x)}{ |x-y|^{n-2}} dx $$ where $ \nu_x $ is the outer normal unit vector on $x\in \partial V$ and $w_{n-1} := \frac{n \pi^{n/2}}{ \Gamma( \frac{n}{2} +1) }$ Well, I know that $W(x):= |x-y|^{-(n-2)} $ is not defined in $x=y$ . Therefore, instead of integrating over $V$ , first integrate over $V_{\epsilon}:= V$ \ $ K_{\epsilon}(y) $ and use the limes $\epsilon \rightarrow 0+ $ Thats pretty much it. Do you guys maybe know what that is for an Equation? I didn't know a proper title for the question, sorry about that. I find it quite hard to solve. Any help is therefore very appreciated !!","Let be , an open set, where you can apply Gauß's Theorem. To show is, that for all with  bounded 2nd derivatives the following equation for : where is the outer normal unit vector on and Well, I know that is not defined in . Therefore, instead of integrating over , first integrate over \ and use the limes Thats pretty much it. Do you guys maybe know what that is for an Equation? I didn't know a proper title for the question, sorry about that. I find it quite hard to solve. Any help is therefore very appreciated !!",V \subset \mathbb{R}^n   3\leq n  U \in C^{(1)} ( \bar{V} ) \cap C^{(2)} (V)  y \in V   (n-2) \omega_{n-1} U(y) = \int_{ \partial V} \left[\frac{1}{|x-y|^{n-2}} \frac{ \partial U}{ \partial \nu} (x)-U(x) \frac{ \partial }{\partial \nu_x} \frac{1}{|x-y|^{n-2}}\right] d\sigma(x)- \int_V \frac{ \Delta U(x)}{ |x-y|^{n-2}} dx   \nu_x  x\in \partial V w_{n-1} := \frac{n \pi^{n/2}}{ \Gamma( \frac{n}{2} +1) } W(x):= |x-y|^{-(n-2)}  x=y V V_{\epsilon}:= V  K_{\epsilon}(y)  \epsilon \rightarrow 0+ ,"['real-analysis', 'multivariable-calculus', 'multiple-integral', 'gaussian-integral', 'potential-theory']"
7,Is there a systematic way to construct functions with prescribed local extrema?,Is there a systematic way to construct functions with prescribed local extrema?,,"I'm teaching multivariable calculus and having a hard time coming up with optimization problems. Suppose I have three lists of points $\{a_1, \dotsc, a_r\}$ , $\{b_1, \dotsc, b_s\}$ , and $\{c_1, \dotsc, c_t\}$ in $\Bbb R^n$ . Is it possible to construct a function $f:\Bbb R^n\to\Bbb R$ that has a local max at every $a_i$ , a local min at every $b_i$ , a saddle point at every $c_i$ , and no other local extrema? Furthermore, is it possible to accomplish this with $f\in\Bbb R[x_1, \dotsc, x_n]$ ? A weaker version of this question is: given points $\{p_1,\dotsc,p_k\}$ in $\Bbb R^n$ , is it possible to construct a function $f:\Bbb R^n\to \Bbb R$ whose set of critical points is exactly $\{p_1,\dotsc,p_k\}$ ? So, for instance, is there a systematic way to construct $f\in\Bbb R[x, y, z]$ with a local maximum at $(1, -3, 2)$ and a saddle point at $(2, -8, 1)$ ?","I'm teaching multivariable calculus and having a hard time coming up with optimization problems. Suppose I have three lists of points , , and in . Is it possible to construct a function that has a local max at every , a local min at every , a saddle point at every , and no other local extrema? Furthermore, is it possible to accomplish this with ? A weaker version of this question is: given points in , is it possible to construct a function whose set of critical points is exactly ? So, for instance, is there a systematic way to construct with a local maximum at and a saddle point at ?","\{a_1, \dotsc, a_r\} \{b_1, \dotsc, b_s\} \{c_1, \dotsc, c_t\} \Bbb R^n f:\Bbb R^n\to\Bbb R a_i b_i c_i f\in\Bbb R[x_1, \dotsc, x_n] \{p_1,\dotsc,p_k\} \Bbb R^n f:\Bbb R^n\to \Bbb R \{p_1,\dotsc,p_k\} f\in\Bbb R[x, y, z] (1, -3, 2) (2, -8, 1)","['multivariable-calculus', 'optimization', 'hessian-matrix']"
8,A function $f:\mathbb{R}^2\rightarrow\mathbb{R}^2$ that maps a circle to a circle,A function  that maps a circle to a circle,f:\mathbb{R}^2\rightarrow\mathbb{R}^2,Suppose f is a continuous function from $\mathbb{R}^2$ to $\mathbb{R}^2$ that maps a circle to a circle. How do I prove that f is differentiable? Will the function still be differentiable if continuity is not assumed?,Suppose f is a continuous function from to that maps a circle to a circle. How do I prove that f is differentiable? Will the function still be differentiable if continuity is not assumed?,\mathbb{R}^2 \mathbb{R}^2,"['real-analysis', 'multivariable-calculus']"
9,"Prove that $\int_0^\pi \ln(1+\alpha \cos x)\, dx= \pi \ln\left(\frac{1+\sqrt{1-\alpha^2}}{2}\right)$",Prove that,"\int_0^\pi \ln(1+\alpha \cos x)\, dx= \pi \ln\left(\frac{1+\sqrt{1-\alpha^2}}{2}\right)","Prove that $$\int_0^\pi \ln(1+\alpha \cos x)\, dx= \pi \ln\left(\frac{1+\sqrt{1-\alpha^2}}{2}\right).$$ This question is under the Leibnitz's Rule section of my book, but I'm not sure if you're supposed to use it here. Using it simply gives the derivative of $\ln(1+\alpha \cos x)$ with respect to $\alpha$, which isn't really useful. Can anyone help? Edit: So I've taken some suggestions from the comments and worked out the question. I was able to get pretty close to the answer, but I'm unable to get the answer exactly. I've skipped a couple of steps here, but do let me know if I did something wrong. $$I(a)=\int_0^\pi \ln(1+\alpha \cos x)\, dx$$ $$I'(a)=\int^\pi_0\frac{cosx}{1+acosx}dx$$ $$I'(a)=\frac{1}{a}\int^\pi_0\frac{acosx+1-1}{1+acosx}dx$$ $$I'(a)=\frac{1}{a}[\pi-\int^\pi_0\frac{1}{1+acosx}dx]$$ By the Tangent Half-Angle Substitution, $$I'(a)=\frac{1}{a}[\pi-\int^\pi_0\frac{1}{1+acosx}dx]$$ $$I'(a)=\pi[\frac{1}{a}-\frac{1}{\sqrt {1-a^2}}]$$ $$I(a)=\int\pi[\frac{1}{a}-\frac{1}{\sqrt {1-a^2}}]da$$ Using normal integrating techniques, $$I(a)=\pi[ln\sqrt{1-a^2}+1]$$ Note that I can't get the last term, $\pi ln2$. If I can get this last term, I can get the answer. Now, I'm wondering if it is because it is a constant term, and thus when I have differentiated by $a$, in the beginning, the information in that term is lost. Can someone help me continue or point out where I went wrong?","Prove that $$\int_0^\pi \ln(1+\alpha \cos x)\, dx= \pi \ln\left(\frac{1+\sqrt{1-\alpha^2}}{2}\right).$$ This question is under the Leibnitz's Rule section of my book, but I'm not sure if you're supposed to use it here. Using it simply gives the derivative of $\ln(1+\alpha \cos x)$ with respect to $\alpha$, which isn't really useful. Can anyone help? Edit: So I've taken some suggestions from the comments and worked out the question. I was able to get pretty close to the answer, but I'm unable to get the answer exactly. I've skipped a couple of steps here, but do let me know if I did something wrong. $$I(a)=\int_0^\pi \ln(1+\alpha \cos x)\, dx$$ $$I'(a)=\int^\pi_0\frac{cosx}{1+acosx}dx$$ $$I'(a)=\frac{1}{a}\int^\pi_0\frac{acosx+1-1}{1+acosx}dx$$ $$I'(a)=\frac{1}{a}[\pi-\int^\pi_0\frac{1}{1+acosx}dx]$$ By the Tangent Half-Angle Substitution, $$I'(a)=\frac{1}{a}[\pi-\int^\pi_0\frac{1}{1+acosx}dx]$$ $$I'(a)=\pi[\frac{1}{a}-\frac{1}{\sqrt {1-a^2}}]$$ $$I(a)=\int\pi[\frac{1}{a}-\frac{1}{\sqrt {1-a^2}}]da$$ Using normal integrating techniques, $$I(a)=\pi[ln\sqrt{1-a^2}+1]$$ Note that I can't get the last term, $\pi ln2$. If I can get this last term, I can get the answer. Now, I'm wondering if it is because it is a constant term, and thus when I have differentiated by $a$, in the beginning, the information in that term is lost. Can someone help me continue or point out where I went wrong?",,"['calculus', 'multivariable-calculus']"
10,What sort of answer does Spivak expect to Problem 2-13(d) from Calculus on Manifolds?,What sort of answer does Spivak expect to Problem 2-13(d) from Calculus on Manifolds?,,"Problem 2-13 from Spivak's Calculus on Manifolds asks the following: Problem 2-13. Define $IP : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ by $IP(x,y) = \langle x, y \rangle$. Find $D(IP)(a,b)$ and $(IP)'(a,b)$. If $f,g : \mathbb{R} \to \mathbb{R}^n$ are differentiable and $h : \mathbb{R} \to \mathbb{R}$ is defined by $h(t) = \langle f(t), g(t) \rangle$, show that $$h'(a) = \langle f'(a)^T, g(a) \rangle + \langle f(a), g'(a)^T \rangle.$$ If $f : \mathbb{R} \to \mathbb{R}$ is differentiable and $|f(t)| = 1$ for all $t$, show that $\langle f'(t)^T, f(t) \rangle = 0$. Exhibit a differentiable function $f : \mathbb{R} \to \mathbb{R}$ such that the function $|f|$ defined by $|f|(t) = |f(t)|$ is not differentiable. What I find weird is that part (4) of the problem seems quite unrelated to the previous three parts of the question. The first three parts are specifically about the inner product as a multilinear function and the properties it and its derivative have, whereas for part (4) I can just take $f(x) = x$ and be done with it. Typically, Spivak's problems are set up in a way to motivate some deeper idea. It is possible that he has slipped up here (after all, Calculus on Manifolds has numerous typos as well). But, I'd still like to ask, Is there a way to look at part (4) of this problem in such a way that it connects to the inner product (or more precisely, to the previous three parts of the problem)?","Problem 2-13 from Spivak's Calculus on Manifolds asks the following: Problem 2-13. Define $IP : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ by $IP(x,y) = \langle x, y \rangle$. Find $D(IP)(a,b)$ and $(IP)'(a,b)$. If $f,g : \mathbb{R} \to \mathbb{R}^n$ are differentiable and $h : \mathbb{R} \to \mathbb{R}$ is defined by $h(t) = \langle f(t), g(t) \rangle$, show that $$h'(a) = \langle f'(a)^T, g(a) \rangle + \langle f(a), g'(a)^T \rangle.$$ If $f : \mathbb{R} \to \mathbb{R}$ is differentiable and $|f(t)| = 1$ for all $t$, show that $\langle f'(t)^T, f(t) \rangle = 0$. Exhibit a differentiable function $f : \mathbb{R} \to \mathbb{R}$ such that the function $|f|$ defined by $|f|(t) = |f(t)|$ is not differentiable. What I find weird is that part (4) of the problem seems quite unrelated to the previous three parts of the question. The first three parts are specifically about the inner product as a multilinear function and the properties it and its derivative have, whereas for part (4) I can just take $f(x) = x$ and be done with it. Typically, Spivak's problems are set up in a way to motivate some deeper idea. It is possible that he has slipped up here (after all, Calculus on Manifolds has numerous typos as well). But, I'd still like to ask, Is there a way to look at part (4) of this problem in such a way that it connects to the inner product (or more precisely, to the previous three parts of the problem)?",,['real-analysis']
11,Prove that second partial derivatives does not depend on the order of differentiation [duplicate],Prove that second partial derivatives does not depend on the order of differentiation [duplicate],,"This question already has an answer here : Symmetry of second derivative - Sufficiency of twice-differentiability (1 answer) Closed 5 years ago . I'm trying to prove that if $$\dfrac{\partial^2 f}{\partial x \partial y} \quad \text{and} \quad  \dfrac{\partial^2 f}{\partial y \partial x}$$ are continuous in an open set containing $a \in \mathbb{R}^2$, then they are equal using the definition of derivative: First, I say $$ \dfrac{\partial^2f}{\partial x \partial y} = \dfrac{\partial}{\partial x} \left( \dfrac{\partial f}{\partial y} \right) = \dfrac{\partial}{\partial x} \left( \lim_{h \to 0} \dfrac{f(x,y+h)-f(x,y)}{h} \right) = \lim_{k \to 0} \dfrac{1}{k} \left( \lim_{h \to 0} \dfrac{f(x+k,y+h)-f(x+k,y)}{h} - \lim_{h \to 0} \dfrac{f(x,y+h)-f(x,y)}{h} \right) = \\ \lim_{k \to 0} \left( \lim_{h \to 0} \dfrac{f(x+k,y+h)-f(x+k,y)-f(x,y+h)+f(x,y)}{kh} \right) $$ Then I do the same thing for $ \dfrac{\partial^2 f}{\partial y \partial x} $, and I get $$ \lim_{h \to 0} \left( \lim_{k \to 0} \dfrac{f(x+k,y+h)-f(x+k,y)-f(x,y+h)+f(x,y)}{kh} \right)$$ My question is: am I allow to say that those limits are equal because both $$\dfrac{\partial^2 f}{\partial x \partial y} \quad \text{and} \quad  \dfrac{\partial^2 f}{\partial y \partial x}$$  are continuous on $a$, or do I need something extra? Please let me know if the question is clear enough, or if I made some silly mistakes :) Thanks in advance!","This question already has an answer here : Symmetry of second derivative - Sufficiency of twice-differentiability (1 answer) Closed 5 years ago . I'm trying to prove that if $$\dfrac{\partial^2 f}{\partial x \partial y} \quad \text{and} \quad  \dfrac{\partial^2 f}{\partial y \partial x}$$ are continuous in an open set containing $a \in \mathbb{R}^2$, then they are equal using the definition of derivative: First, I say $$ \dfrac{\partial^2f}{\partial x \partial y} = \dfrac{\partial}{\partial x} \left( \dfrac{\partial f}{\partial y} \right) = \dfrac{\partial}{\partial x} \left( \lim_{h \to 0} \dfrac{f(x,y+h)-f(x,y)}{h} \right) = \lim_{k \to 0} \dfrac{1}{k} \left( \lim_{h \to 0} \dfrac{f(x+k,y+h)-f(x+k,y)}{h} - \lim_{h \to 0} \dfrac{f(x,y+h)-f(x,y)}{h} \right) = \\ \lim_{k \to 0} \left( \lim_{h \to 0} \dfrac{f(x+k,y+h)-f(x+k,y)-f(x,y+h)+f(x,y)}{kh} \right) $$ Then I do the same thing for $ \dfrac{\partial^2 f}{\partial y \partial x} $, and I get $$ \lim_{h \to 0} \left( \lim_{k \to 0} \dfrac{f(x+k,y+h)-f(x+k,y)-f(x,y+h)+f(x,y)}{kh} \right)$$ My question is: am I allow to say that those limits are equal because both $$\dfrac{\partial^2 f}{\partial x \partial y} \quad \text{and} \quad  \dfrac{\partial^2 f}{\partial y \partial x}$$  are continuous on $a$, or do I need something extra? Please let me know if the question is clear enough, or if I made some silly mistakes :) Thanks in advance!",,"['calculus', 'multivariable-calculus', 'derivatives']"
12,Find the volume common to the surfaces $y^2+z^2=4ax$ and $x^2+y^2=2ax$,Find the volume common to the surfaces  and,y^2+z^2=4ax x^2+y^2=2ax,"Find the volume common to the surfaces $y^2+z^2=4ax$ and $x^2+y^2=2ax$ I set up the following integral : $$\iiint_{z=-\sqrt{4ax-y^2}}^{\sqrt{4ax-y^2}}\,dz\,dy\,dx = 2\cdot \int_{x=0}^{2a} \int_{y=-\sqrt{2ax-x^2}}^{\sqrt{2ax-x^2}} \sqrt{4ax-y^2}\,dy\,dx$$ Which is not easy to integrate. If I use polar coordinates the integration becomes $$2\cdot \int_{\theta=-\pi/2}^{\pi/2} \int_{r=0}^{2a\cos(\theta)} \sqrt{4ar\cos(\theta)-r^2\sin^2(\theta)} \cdot r\,dr\,d\theta$$ Which is again difficult","Find the volume common to the surfaces $y^2+z^2=4ax$ and $x^2+y^2=2ax$ I set up the following integral : $$\iiint_{z=-\sqrt{4ax-y^2}}^{\sqrt{4ax-y^2}}\,dz\,dy\,dx = 2\cdot \int_{x=0}^{2a} \int_{y=-\sqrt{2ax-x^2}}^{\sqrt{2ax-x^2}} \sqrt{4ax-y^2}\,dy\,dx$$ Which is not easy to integrate. If I use polar coordinates the integration becomes $$2\cdot \int_{\theta=-\pi/2}^{\pi/2} \int_{r=0}^{2a\cos(\theta)} \sqrt{4ar\cos(\theta)-r^2\sin^2(\theta)} \cdot r\,dr\,d\theta$$ Which is again difficult",,"['calculus', 'multivariable-calculus', 'multiple-integral']"
13,Showing curl curl of symmetric gradient is 0,Showing curl curl of symmetric gradient is 0,,"I am trying to show that $curl\,curl\,(\mathcal E)=0$, where $\mathcal E$ represents the symmetric gradient, i.e., for vector-valued function $u(x_1,x_2)=(u_1(x_1,x_2),u_2(x_1,x_2))$ $$ \mathcal E(u)= \begin{bmatrix} \partial_1 u_1 & \frac12(\partial_2 u_1+\partial_1 u_2)\\ \frac12(\partial_2 u_1+\partial_1 u_2) & \partial_2 u_2 \end{bmatrix} $$ I know there is a formula sayinng that  $$ curl\,curl\,= \nabla div - \Delta $$ and I am tryinng to apply this formula. However, I got confused that, say $\mathcal E(u)$ is a 2 by 2 matrix, so $\Delta \mathcal E(u)$ should be a 2 by 1 vector. But $div(\mathcal E(u))$ is a 2 by 1 vector and $\nabla div \mathcal E(u)$ is then a 2 by 2 matrix... so their dimension does not match... where I am wrong? Also, I tried to explicitly write down the computation but they did not cancel to 0... update: for a reference I found, they write $\nabla div - \Delta$ as a 2 by 1 vector, but no explicit formula can be found there... Any help is really welcome!","I am trying to show that $curl\,curl\,(\mathcal E)=0$, where $\mathcal E$ represents the symmetric gradient, i.e., for vector-valued function $u(x_1,x_2)=(u_1(x_1,x_2),u_2(x_1,x_2))$ $$ \mathcal E(u)= \begin{bmatrix} \partial_1 u_1 & \frac12(\partial_2 u_1+\partial_1 u_2)\\ \frac12(\partial_2 u_1+\partial_1 u_2) & \partial_2 u_2 \end{bmatrix} $$ I know there is a formula sayinng that  $$ curl\,curl\,= \nabla div - \Delta $$ and I am tryinng to apply this formula. However, I got confused that, say $\mathcal E(u)$ is a 2 by 2 matrix, so $\Delta \mathcal E(u)$ should be a 2 by 1 vector. But $div(\mathcal E(u))$ is a 2 by 1 vector and $\nabla div \mathcal E(u)$ is then a 2 by 2 matrix... so their dimension does not match... where I am wrong? Also, I tried to explicitly write down the computation but they did not cancel to 0... update: for a reference I found, they write $\nabla div - \Delta$ as a 2 by 1 vector, but no explicit formula can be found there... Any help is really welcome!",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'differential-geometry', 'tensors']"
14,Weak derivative of $|x|^\alpha$ in $\mathbb{R}^N \setminus \{0\}$,Weak derivative of  in,|x|^\alpha \mathbb{R}^N \setminus \{0\},"I want to generalize the fact that the weak derivative of $|x|$ is $sgn(x)$. The expected result should be $\frac{d|x|^{\alpha}}{dx_i} = \alpha x_i |x|^{\alpha - 2}$. A way to do this is found in page 50 of these notes . However, I don't have the tools to believe that: $|x|^{\alpha}$ is weakly differentiable provided that the pointwise derivative,   which is defined almost everywhere, is locally integrable So I would like to do it without using this fact. The domain I think weak derivatives have a dependence on the domain so I specify that I want the derivative of $|x|^{\alpha}$ in $\mathbb{R}^N \setminus \{0\}$ The computation $\int |x|^{\alpha} \frac{d\phi}{dx_i} d(x_1,\ldots,x_N)$, $\phi \in \mathcal{C}_0^{\infty}$, by Fubini's theorem (?) I can write: $\int_{\mathbb{R}^{N-1} \setminus \{0\}} (\int_{\mathbb{R} \setminus \{0\}} |x|^{\alpha} \frac{d\phi}{dx_i} dx_i) d(x_1,\ldots,x_{i-1},x_{i+1},x_N)$ integrating by parts $\int_{\mathbb{R} \setminus \{0\}} |x|^{\alpha} \frac{d\phi}{dx_i} dx_i$ and using that $\phi \in \mathcal{C}_0^{\infty}$: $\int_{-\infty}^0 |x|^{\alpha} \frac{d\phi}{dx_i} dx_i =  -\int_{-\infty}^0 \alpha x_i |x|^{\alpha-2} \phi dx_i$, analogously, $\int_{0}^\infty |x|^{\alpha} \frac{d\phi}{dx_i} dx_i =  -\int_{0}^\infty \alpha x_i |x|^{\alpha-2} \phi dx_i$ so that I get the result equal to: $-\int_{\mathbb{R}^N \setminus \{0\}} \alpha x_i|x|^{\alpha}\phi dx_i$ The only thing to finish the proof would be to see where I missed the - sign.","I want to generalize the fact that the weak derivative of $|x|$ is $sgn(x)$. The expected result should be $\frac{d|x|^{\alpha}}{dx_i} = \alpha x_i |x|^{\alpha - 2}$. A way to do this is found in page 50 of these notes . However, I don't have the tools to believe that: $|x|^{\alpha}$ is weakly differentiable provided that the pointwise derivative,   which is defined almost everywhere, is locally integrable So I would like to do it without using this fact. The domain I think weak derivatives have a dependence on the domain so I specify that I want the derivative of $|x|^{\alpha}$ in $\mathbb{R}^N \setminus \{0\}$ The computation $\int |x|^{\alpha} \frac{d\phi}{dx_i} d(x_1,\ldots,x_N)$, $\phi \in \mathcal{C}_0^{\infty}$, by Fubini's theorem (?) I can write: $\int_{\mathbb{R}^{N-1} \setminus \{0\}} (\int_{\mathbb{R} \setminus \{0\}} |x|^{\alpha} \frac{d\phi}{dx_i} dx_i) d(x_1,\ldots,x_{i-1},x_{i+1},x_N)$ integrating by parts $\int_{\mathbb{R} \setminus \{0\}} |x|^{\alpha} \frac{d\phi}{dx_i} dx_i$ and using that $\phi \in \mathcal{C}_0^{\infty}$: $\int_{-\infty}^0 |x|^{\alpha} \frac{d\phi}{dx_i} dx_i =  -\int_{-\infty}^0 \alpha x_i |x|^{\alpha-2} \phi dx_i$, analogously, $\int_{0}^\infty |x|^{\alpha} \frac{d\phi}{dx_i} dx_i =  -\int_{0}^\infty \alpha x_i |x|^{\alpha-2} \phi dx_i$ so that I get the result equal to: $-\int_{\mathbb{R}^N \setminus \{0\}} \alpha x_i|x|^{\alpha}\phi dx_i$ The only thing to finish the proof would be to see where I missed the - sign.",,"['real-analysis', 'multivariable-calculus', 'proof-verification', 'sobolev-spaces', 'weak-derivatives']"
15,"Understanding Duhamel's principle, PDE","Understanding Duhamel's principle, PDE",,"so i'm really having a problem with Duhamel's principle, i'll explain what i know and if possible could someone help me fix my lack of understanding. Thanks! so from my understanding of Duhamel's principle, if we have an inhomgenous PDE (for example the heat equation) such as (Current case is finite domain $(0,a)$ ) $$u_t = Du_{xx}+f(x,t),~~~ u(x,0)=\phi(x) \\ u(0,t)=h(t), ~~~~~~ u(a,t)=g(t)$$ my first issue, in the above we have inhomogenous boundry conditions, if we had homogenous boundry conditions can i still use Duhamel's principle?  (Despite the fact that i could solve it using Seperation of variables) or is the inhomogenous BC's the ""signal"" to use Duhamels principle? First we find $v(x,t)$ which satisfies an auxiliary question $$v_t=Dv_{xx},\\~~v(x,0) = 0 \\ v(0,t)= g(t), ~~~~~~~~~~~~~~~~ v(a,t) = h(t)$$ which basically comes down to finding an interpolation of h and g. (so long as its matchs the boundary conditions its all good). Second: we solve the problem $$w_t = Dw_{xx}+f(x,t), \\w(x,0)= \phi(x) \\ w(0,t)= w(a,t) = 0$$ with solution $$w(x,t) = \sum w_n(t) \sin{\frac{\pi n x}{a}}$$ $$\text{ where } w_n(t) = w_n(0)e^{\frac{-D \pi^2 n^2 t}{a^2}} + e^{\frac{-D \pi^2 n^2 t}{a^2}} \int_{0}^{t} e^{\frac{-D \pi^2 n^2 s}{a^2}} f_n(s)ds$$ $$w_n(0) = \frac{2}{a}\int_0^{a} \phi(x) \sin{\frac{\pi n x}{a}} dx$$ and $$f_n(x) = \frac{2}{a}\int_0^a f(x,t)\sin{\frac{\pi n x}{a}}$$ then the solution to our original problem is $$u(x,t) = v(x,t) + w(x,t)$$ step 1: Solve a shifted problem by finding $v(x,t)$ which satisfies the Boundary conditions from the general problem. step 2: solve the inhomogenous problem with homogeneous boundary conditions, whilst keeping the initial condition step 3: Sum the two. i believe this is the method to use the principle. In this instance (in terms of the heat equation) we have an external source which is adding values to the PDE. so the problem with solution v(x,t) is akin to shifting the initial amount of energy in the system by $\phi(x)$ and eliminating the external source. (this makes sense because if we dont have the external source then we should have zero initial energy also extending the principle to neumann conditions. are all we doing again defining v to solve the boundary conditions in terms of $v$ and $v_t$ ? in the next scenario we have 1 dimensional infinite domains so our problem is $$u_t = Du_{xx}+f(x,t) \\ u(x,0) = \phi(x)$$ in this instance we dont have boundary conditions so we solve the following $$v_t=Dv_{xx}, \\v(x,0) = \phi(x)$$ ie we eliminate the source again and have a solution of $$v(x,t) = \int_{\mathbb{R}}\Phi(x-y)\phi(y)~dy$$ next we solve $$w_t = Dw_{xx}+f(x,t) \\ w(x,0) = 0$$ which again gives $$w(x,t) = \int_0^t \int_{\mathbb{R}}\Phi(x-y,t-s)f(y,s)~dyds$$ which are true due to the translational and diliational invariance of the heat equation. finally $u = v +w$ so to me it seems that practically we eliminate our initial condition and source term from our problem. solve the equation, then solve a new equation with the source term but no initial condition, and sum the two. intuitively i have no idea why. or whether my understanding of the procedure is correct (in fairness ive been going over this for the last few days, tried researching it and at this point just frazzled) any help guys and girls? thanks for the help. Update: after having let my brain deglaze a bit and then redoing some research on the subject i've built up a little bit more of an intuitive rationale why, again i would need people to help confirm. from my current understanding the arguement is that a non-homogenous system is akin to a homogenous one where at some time period s < t a source term starts adding in energy to the system, in that case what we're really doing is just kind of ""Psuedo"" scaling back time and since v and w both solve our ""basic"" version of the heat equation they can be considered particular solutions that correspond to the different set ups of the system, Ie one solves for the Boundary conditions and one solves for the source itself. am i on the right lines? update 2: ive found a pdf on the physics exchange which explains the procedure more accurately. A better explination of procedure than mine","so i'm really having a problem with Duhamel's principle, i'll explain what i know and if possible could someone help me fix my lack of understanding. Thanks! so from my understanding of Duhamel's principle, if we have an inhomgenous PDE (for example the heat equation) such as (Current case is finite domain ) my first issue, in the above we have inhomogenous boundry conditions, if we had homogenous boundry conditions can i still use Duhamel's principle?  (Despite the fact that i could solve it using Seperation of variables) or is the inhomogenous BC's the ""signal"" to use Duhamels principle? First we find which satisfies an auxiliary question which basically comes down to finding an interpolation of h and g. (so long as its matchs the boundary conditions its all good). Second: we solve the problem with solution and then the solution to our original problem is step 1: Solve a shifted problem by finding which satisfies the Boundary conditions from the general problem. step 2: solve the inhomogenous problem with homogeneous boundary conditions, whilst keeping the initial condition step 3: Sum the two. i believe this is the method to use the principle. In this instance (in terms of the heat equation) we have an external source which is adding values to the PDE. so the problem with solution v(x,t) is akin to shifting the initial amount of energy in the system by and eliminating the external source. (this makes sense because if we dont have the external source then we should have zero initial energy also extending the principle to neumann conditions. are all we doing again defining v to solve the boundary conditions in terms of and ? in the next scenario we have 1 dimensional infinite domains so our problem is in this instance we dont have boundary conditions so we solve the following ie we eliminate the source again and have a solution of next we solve which again gives which are true due to the translational and diliational invariance of the heat equation. finally so to me it seems that practically we eliminate our initial condition and source term from our problem. solve the equation, then solve a new equation with the source term but no initial condition, and sum the two. intuitively i have no idea why. or whether my understanding of the procedure is correct (in fairness ive been going over this for the last few days, tried researching it and at this point just frazzled) any help guys and girls? thanks for the help. Update: after having let my brain deglaze a bit and then redoing some research on the subject i've built up a little bit more of an intuitive rationale why, again i would need people to help confirm. from my current understanding the arguement is that a non-homogenous system is akin to a homogenous one where at some time period s < t a source term starts adding in energy to the system, in that case what we're really doing is just kind of ""Psuedo"" scaling back time and since v and w both solve our ""basic"" version of the heat equation they can be considered particular solutions that correspond to the different set ups of the system, Ie one solves for the Boundary conditions and one solves for the source itself. am i on the right lines? update 2: ive found a pdf on the physics exchange which explains the procedure more accurately. A better explination of procedure than mine","(0,a) u_t = Du_{xx}+f(x,t),~~~ u(x,0)=\phi(x) \\ u(0,t)=h(t), ~~~~~~ u(a,t)=g(t) v(x,t) v_t=Dv_{xx},\\~~v(x,0) = 0 \\ v(0,t)= g(t), ~~~~~~~~~~~~~~~~ v(a,t) = h(t) w_t = Dw_{xx}+f(x,t), \\w(x,0)= \phi(x) \\ w(0,t)= w(a,t) = 0 w(x,t) = \sum w_n(t) \sin{\frac{\pi n x}{a}} \text{ where } w_n(t) = w_n(0)e^{\frac{-D \pi^2 n^2 t}{a^2}} + e^{\frac{-D \pi^2 n^2 t}{a^2}} \int_{0}^{t} e^{\frac{-D \pi^2 n^2 s}{a^2}} f_n(s)ds w_n(0) = \frac{2}{a}\int_0^{a} \phi(x) \sin{\frac{\pi n x}{a}} dx f_n(x) = \frac{2}{a}\int_0^a f(x,t)\sin{\frac{\pi n x}{a}} u(x,t) = v(x,t) + w(x,t) v(x,t) \phi(x) v v_t u_t = Du_{xx}+f(x,t) \\ u(x,0) = \phi(x) v_t=Dv_{xx}, \\v(x,0) = \phi(x) v(x,t) = \int_{\mathbb{R}}\Phi(x-y)\phi(y)~dy w_t = Dw_{xx}+f(x,t) \\ w(x,0) = 0 w(x,t) = \int_0^t \int_{\mathbb{R}}\Phi(x-y,t-s)f(y,s)~dyds u = v +w","['multivariable-calculus', 'partial-differential-equations', 'heat-equation']"
16,Find the largest value and the smallest value of expression: $P=a+2b+2c.$,Find the largest value and the smallest value of expression:,P=a+2b+2c.,"Let real numbers $ a, b, c$ satisfy $\left\{ \begin{align}   & {{a}^{2}}+{{b}^{2}}+{{c}^{2}}\le 25 \\   & 2a+b-2c\ge 12 \\  \end{align} \right.$ . Find the largest value and the smallest value of expression: $$P=a+2b+2c.$$ I solved the problem as below: $$\left\{ \begin{align}   & {{a}^{2}}+{{b}^{2}}+{{c}^{2}}\le 25 \\   & 2a+b-2c\ge 12 \\  \end{align} \right.\Rightarrow {{a}^{2}}+{{b}^{2}}+{{c}^{2}}-2a-b+2c-13\le 0  (1)$$  $$P=a+2b+2c\Leftrightarrow a+2b+2c-P=0 (2) $$   $(1)$ corresponding sphere with center $I\left( 1;\frac{1}{2};-1 \right)$ and radius $$R=\frac{\sqrt{61}}{2}$$ $(2)$ corresponds to the plane $$\left( \alpha  \right):a+2b+2c-P=0$$ The problem of finding conditions of plane and sphere with common intersection. This happens when: $$d\left( I,\left( \alpha  \right) \right)=R\Leftrightarrow \frac{\left| -P \right|}{3}\le \frac{\sqrt{61}}{2}\Leftrightarrow -\frac{\sqrt{61}}{2}\le P\le \frac{\sqrt{61}}{2}$$ So $$\min P=-\frac{\sqrt{61}}{2},\max P=\frac{\sqrt{61}}{2}$$ Please comment and give better solution.","Let real numbers $ a, b, c$ satisfy $\left\{ \begin{align}   & {{a}^{2}}+{{b}^{2}}+{{c}^{2}}\le 25 \\   & 2a+b-2c\ge 12 \\  \end{align} \right.$ . Find the largest value and the smallest value of expression: $$P=a+2b+2c.$$ I solved the problem as below: $$\left\{ \begin{align}   & {{a}^{2}}+{{b}^{2}}+{{c}^{2}}\le 25 \\   & 2a+b-2c\ge 12 \\  \end{align} \right.\Rightarrow {{a}^{2}}+{{b}^{2}}+{{c}^{2}}-2a-b+2c-13\le 0  (1)$$  $$P=a+2b+2c\Leftrightarrow a+2b+2c-P=0 (2) $$   $(1)$ corresponding sphere with center $I\left( 1;\frac{1}{2};-1 \right)$ and radius $$R=\frac{\sqrt{61}}{2}$$ $(2)$ corresponds to the plane $$\left( \alpha  \right):a+2b+2c-P=0$$ The problem of finding conditions of plane and sphere with common intersection. This happens when: $$d\left( I,\left( \alpha  \right) \right)=R\Leftrightarrow \frac{\left| -P \right|}{3}\le \frac{\sqrt{61}}{2}\Leftrightarrow -\frac{\sqrt{61}}{2}\le P\le \frac{\sqrt{61}}{2}$$ So $$\min P=-\frac{\sqrt{61}}{2},\max P=\frac{\sqrt{61}}{2}$$ Please comment and give better solution.",,"['multivariable-calculus', 'inequality', 'optimization']"
17,Self-convolution of $f(\vec{r}) = e^{-x^2-y^2}/r^2$,Self-convolution of,f(\vec{r}) = e^{-x^2-y^2}/r^2,"I wonder if the self-convolution of $$ f(\vec{r}) = \frac{e^{-(x^2+y^2)}}{r^2}, $$ (where $\vec{r} = (x,y,z)$, and $r^2 = x^2+y^2+z^2$), $$ (f*f)(\vec{r}') = \iiint_{-\infty}^\infty d^3\vec{r}\, f(\vec{r}) f(\vec{r}'-\vec{r}), $$ can be evaluated or simplified in any way? Thanks. Edit: Using the identity $$ \frac{1}{A} = \int_0^\infty d\nu\, e^{-A\nu} \text{ for } A>0, $$ the integration dimension can be decreased by 1: $$ \begin{align*} \iiint_{-\infty}^\infty d^3\vec{r}\, f(\vec{r}) f(\vec{r}'-\vec{r})  &= \iiint_{-\infty}^\infty d^3\vec{r}\, \frac{e^{-x^2-y^2}}{r^2} \frac{e^{-(x'-x)^2-(y'-y)^2}}{|\vec{r}' - \vec{r}|^2} \\ &= \iint_0^\infty d^2\vec{\nu}\iiint_{-\infty}^\infty d^3\vec{r}\, e^{-x^2-y^2-(x'-x)^2-(y'-y)^2- r^2\nu_1 - |\vec{r}' - \vec{r}|^2 \nu_2} \\ &= \pi^{3/2} \iint_0^\infty d^2\vec{\nu} \frac{\exp \left(-\frac{(\nu_1+1) (\nu_2+1) (x'^2+y'^2)}{\nu_1+\nu_2+2}-\frac{\nu_1 \nu_2 z'^2}{\nu_1+\nu_2}\right)}{(\nu_1+\nu_2+2)\sqrt{\nu_1+\nu_2}}. \end{align*} $$ Can this integral be further reduced?","I wonder if the self-convolution of $$ f(\vec{r}) = \frac{e^{-(x^2+y^2)}}{r^2}, $$ (where $\vec{r} = (x,y,z)$, and $r^2 = x^2+y^2+z^2$), $$ (f*f)(\vec{r}') = \iiint_{-\infty}^\infty d^3\vec{r}\, f(\vec{r}) f(\vec{r}'-\vec{r}), $$ can be evaluated or simplified in any way? Thanks. Edit: Using the identity $$ \frac{1}{A} = \int_0^\infty d\nu\, e^{-A\nu} \text{ for } A>0, $$ the integration dimension can be decreased by 1: $$ \begin{align*} \iiint_{-\infty}^\infty d^3\vec{r}\, f(\vec{r}) f(\vec{r}'-\vec{r})  &= \iiint_{-\infty}^\infty d^3\vec{r}\, \frac{e^{-x^2-y^2}}{r^2} \frac{e^{-(x'-x)^2-(y'-y)^2}}{|\vec{r}' - \vec{r}|^2} \\ &= \iint_0^\infty d^2\vec{\nu}\iiint_{-\infty}^\infty d^3\vec{r}\, e^{-x^2-y^2-(x'-x)^2-(y'-y)^2- r^2\nu_1 - |\vec{r}' - \vec{r}|^2 \nu_2} \\ &= \pi^{3/2} \iint_0^\infty d^2\vec{\nu} \frac{\exp \left(-\frac{(\nu_1+1) (\nu_2+1) (x'^2+y'^2)}{\nu_1+\nu_2+2}-\frac{\nu_1 \nu_2 z'^2}{\nu_1+\nu_2}\right)}{(\nu_1+\nu_2+2)\sqrt{\nu_1+\nu_2}}. \end{align*} $$ Can this integral be further reduced?",,"['multivariable-calculus', 'fourier-analysis', 'improper-integrals', 'convolution']"
18,Proving $\lim_{\vec{h}\to 0}\frac{\|f(\vec{u}+\vec{h})-f(\vec{u})\|}{\|\vec{h}\|} = \|J_f(\vec{u})\|_{\text{op}}$?,Proving ?,\lim_{\vec{h}\to 0}\frac{\|f(\vec{u}+\vec{h})-f(\vec{u})\|}{\|\vec{h}\|} = \|J_f(\vec{u})\|_{\text{op}},"I'm looking to prove the limit in the title statement for $f:\mathbb{R}^n\to\mathbb{R}^m$ differentiable in the necessary open set, where $\|A\|_{\text{op}}$ is the operator norm. I proceeded as follows: $$ \begin{aligned} \left|\frac{\|f(\vec u+\vec h) - f(\vec u)\|}{\|\vec h\|} - \|J_f(\vec u)\|_{\text{op}}\right| & = \left|\frac{\|f(\vec u+\vec h)-f(\vec u)\|-\|J_f(\vec u)\|_{\text{op}}\|\vec  h\|}{\|\vec h\|}\right| \\ & \le \left|\frac{\|f(\vec u+\vec h) - f(\vec u)\|-\|J_f(\vec u)\vec h\|}{\|\vec  h\|}\right| \\ & \le \frac{\|f(\vec u+\vec h)-f(\vec u)-J_f(\vec u)\vec h\|}{\|\vec h\|} \end{aligned} $$ where we use $\|Av\|\le\|A\|_{\text{op}}\|v\|$ to go from the first line to the second line, and the reverse triangle inequality to go from the second line to the third line. Then by the squeeze theorem we have $$ \begin{aligned} 0 & \le\lim_{\vec h\to 0}\left|\frac{\|f(\vec u+\vec h)-f(\vec u)\|}{\|\vec h\|} - \|J_f(\vec u)\|_{\text{op}}\right| \\ & \le\lim_{\vec h\to 0}\frac{\|f(\vec u+\vec h)-f(\vec u)-J_f(\vec u)\vec h\|}{\|\vec h\|} \\ & = 0 \end{aligned} $$ Looks alright, right? Well I realized there's a tiny detail: Suppose $z<y$. Then clearly, $x-y<x-z$. However , it is not necessarily true that $|x-y|<|x-z|$. This means we can't use the fact that $\|Av\|\le\|A\|_{\text{op}}\|v\|$ to go from the first line to the second line above. Is this ""proof"" salvageable? Is what I'm trying to prove even true? If not, what does the limit actually evaluate to?","I'm looking to prove the limit in the title statement for $f:\mathbb{R}^n\to\mathbb{R}^m$ differentiable in the necessary open set, where $\|A\|_{\text{op}}$ is the operator norm. I proceeded as follows: $$ \begin{aligned} \left|\frac{\|f(\vec u+\vec h) - f(\vec u)\|}{\|\vec h\|} - \|J_f(\vec u)\|_{\text{op}}\right| & = \left|\frac{\|f(\vec u+\vec h)-f(\vec u)\|-\|J_f(\vec u)\|_{\text{op}}\|\vec  h\|}{\|\vec h\|}\right| \\ & \le \left|\frac{\|f(\vec u+\vec h) - f(\vec u)\|-\|J_f(\vec u)\vec h\|}{\|\vec  h\|}\right| \\ & \le \frac{\|f(\vec u+\vec h)-f(\vec u)-J_f(\vec u)\vec h\|}{\|\vec h\|} \end{aligned} $$ where we use $\|Av\|\le\|A\|_{\text{op}}\|v\|$ to go from the first line to the second line, and the reverse triangle inequality to go from the second line to the third line. Then by the squeeze theorem we have $$ \begin{aligned} 0 & \le\lim_{\vec h\to 0}\left|\frac{\|f(\vec u+\vec h)-f(\vec u)\|}{\|\vec h\|} - \|J_f(\vec u)\|_{\text{op}}\right| \\ & \le\lim_{\vec h\to 0}\frac{\|f(\vec u+\vec h)-f(\vec u)-J_f(\vec u)\vec h\|}{\|\vec h\|} \\ & = 0 \end{aligned} $$ Looks alright, right? Well I realized there's a tiny detail: Suppose $z<y$. Then clearly, $x-y<x-z$. However , it is not necessarily true that $|x-y|<|x-z|$. This means we can't use the fact that $\|Av\|\le\|A\|_{\text{op}}\|v\|$ to go from the first line to the second line above. Is this ""proof"" salvageable? Is what I'm trying to prove even true? If not, what does the limit actually evaluate to?",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
19,"Is there a generalization of Stokes theorem for forms with poles and distributions, as seen in physics?","Is there a generalization of Stokes theorem for forms with poles and distributions, as seen in physics?",,"Stokes theorem for smooth differential forms is well-known. If $\alpha$ is a smooth differential $n$-form defined on an $(n+1)$-dimensional compact oriented manifold with boundary, then we have $$\int_Md\alpha=\int_{\partial M}\alpha.$$ In physics, one considers a generalized form of this equation, where the manifold need not be compact, and the differential form need not be smooth everywhere; it may have poles. For example, in electrostatics one sees a 2-form field generated by a point charge $Q$ $$E=\dfrac{Q}{4\pi\epsilon_0r^3}\left(x\,dy\wedge dz+y\,dz\wedge dx+z\,dx\wedge dy\right)=\dfrac{Q}{4\pi\epsilon_0r^2}d\Omega_{S^2},$$ where $d\Omega_{S^2}$ is the volume form of the sphere.  I suppose this must be understood as a distribution-valued form, because the physicist computes $$dE=\frac{Q}{4\pi\epsilon_0}\delta^3(r)\,d\text{vol}_{\mathbb{R}^3},$$ where $\delta^3$ is the Dirac delta function in $\mathbb{R}^3$. Perhaps this is a weak exterior derivative ? The physics discussions I have seen justify this computation by saying it is necessary to make the Gauss's law hold. But where I come from, we don't conspire to ensure theorems hold because we like them, but rather prove from the definitions that theorems follow logically. Could you say how this theory of distribution valued differential forms looks, and state a version of Stokes' theorem for it? I would also accept a reference.","Stokes theorem for smooth differential forms is well-known. If $\alpha$ is a smooth differential $n$-form defined on an $(n+1)$-dimensional compact oriented manifold with boundary, then we have $$\int_Md\alpha=\int_{\partial M}\alpha.$$ In physics, one considers a generalized form of this equation, where the manifold need not be compact, and the differential form need not be smooth everywhere; it may have poles. For example, in electrostatics one sees a 2-form field generated by a point charge $Q$ $$E=\dfrac{Q}{4\pi\epsilon_0r^3}\left(x\,dy\wedge dz+y\,dz\wedge dx+z\,dx\wedge dy\right)=\dfrac{Q}{4\pi\epsilon_0r^2}d\Omega_{S^2},$$ where $d\Omega_{S^2}$ is the volume form of the sphere.  I suppose this must be understood as a distribution-valued form, because the physicist computes $$dE=\frac{Q}{4\pi\epsilon_0}\delta^3(r)\,d\text{vol}_{\mathbb{R}^3},$$ where $\delta^3$ is the Dirac delta function in $\mathbb{R}^3$. Perhaps this is a weak exterior derivative ? The physics discussions I have seen justify this computation by saying it is necessary to make the Gauss's law hold. But where I come from, we don't conspire to ensure theorems hold because we like them, but rather prove from the definitions that theorems follow logically. Could you say how this theory of distribution valued differential forms looks, and state a version of Stokes' theorem for it? I would also accept a reference.",,"['multivariable-calculus', 'distribution-theory', 'differential-forms', 'weak-derivatives', 'stokes-theorem']"
20,"Spivak Calculus on manifolds, Theorem 4-8.","Spivak Calculus on manifolds, Theorem 4-8.",,"In Michael Spivak's book, Calculus on Manifolds, in 4th Chapter theorem 8 (4-8), I am having a difficulty in making sense of the expression, $f^{*}\omega \wedge f^{*}\eta$. Here $f:\mathbb{R}^{n}\to \mathbb{R}^{m}$ is a differentiable function. $Df(p): \mathbb{R}^{n}\to \mathbb{R}^{m}$ is the derivative at $p$ and a linear transformation induced by this is $f_{*}: \mathbb{R}^{m}_{f(p)} \to \mathbb{R}^{n}_{p}$ such that $f_{*}(v_{p}) = (Df(p)(v))_{f(p)}$. $f^{*}\omega(p)((v_{1})_{p},\ldots,(v_{k})_{p}) = w(f(p))(f_{*}(v_{1})_{p}),\ldots,f_{*}((v_{k})_{p}))$ $\omega$ and $\eta $ are $k$-forms on $\mathbb{R}^{m}$. $\omega\wedge\eta(p) = \sum_{1\leq i_{1}<\ldots i_{k}\leq n}  \sum_{1\leq j_{1}<j_{2}<\ldots < j_{l}\leq n} \omega_{i_{1}i_{2},\ldots,i_{k}}(p)\eta_{j_{1}j_{2},\ldots,j_{l}}\varphi_{i_{1}}(p)\wedge\ldots\wedge\varphi_{i_{k}}(p)\wedge\varphi_{j_{1}} (p)\wedge\ldots\wedge\varphi_{j_{l}}(p)$. From the above two notions, I am unable to make sense of $f^{*}\omega\wedge f^{*}\eta$. Please tell me if I need to give any more clarifications about the question.","In Michael Spivak's book, Calculus on Manifolds, in 4th Chapter theorem 8 (4-8), I am having a difficulty in making sense of the expression, $f^{*}\omega \wedge f^{*}\eta$. Here $f:\mathbb{R}^{n}\to \mathbb{R}^{m}$ is a differentiable function. $Df(p): \mathbb{R}^{n}\to \mathbb{R}^{m}$ is the derivative at $p$ and a linear transformation induced by this is $f_{*}: \mathbb{R}^{m}_{f(p)} \to \mathbb{R}^{n}_{p}$ such that $f_{*}(v_{p}) = (Df(p)(v))_{f(p)}$. $f^{*}\omega(p)((v_{1})_{p},\ldots,(v_{k})_{p}) = w(f(p))(f_{*}(v_{1})_{p}),\ldots,f_{*}((v_{k})_{p}))$ $\omega$ and $\eta $ are $k$-forms on $\mathbb{R}^{m}$. $\omega\wedge\eta(p) = \sum_{1\leq i_{1}<\ldots i_{k}\leq n}  \sum_{1\leq j_{1}<j_{2}<\ldots < j_{l}\leq n} \omega_{i_{1}i_{2},\ldots,i_{k}}(p)\eta_{j_{1}j_{2},\ldots,j_{l}}\varphi_{i_{1}}(p)\wedge\ldots\wedge\varphi_{i_{k}}(p)\wedge\varphi_{j_{1}} (p)\wedge\ldots\wedge\varphi_{j_{l}}(p)$. From the above two notions, I am unable to make sense of $f^{*}\omega\wedge f^{*}\eta$. Please tell me if I need to give any more clarifications about the question.",,"['multivariable-calculus', 'definition']"
21,Where may I find information regarding quantitative aspects of diffeomorphism?,Where may I find information regarding quantitative aspects of diffeomorphism?,,"If anyone bothered to check on my question history, they would see that I have already been asking several questions related to this theme. Unfortunately I haven't had much success on the specifics, so I might as well as ask this super general question as a last resort. But fundamentally, I just want to know that if $f \in C^\infty(R^n)$ and all of its derivatives vanish in some sense rapidly enough as $|x| \to \infty$, and $\phi: U \to R^n$ is a diffeomorphism, $U$ an open bounded connected subset of $R^n$, then can we assert that $f\circ \phi: U \to R^n$ has bounded derivatives of all orders? The issue is of course that since $U$ is a bounded set, $D\phi$ must diverge, and in fact every derivative diverges hence they must do so rapidly (?). $D(f\circ \phi) = (Df)\circ \phi D\phi$, so it comes down to comparing the rate of vanishing of $D^n f$ and the rate of divergence of $D^m \phi$. This question will probably defy any sort of generalization, but can we at least assert that if $f$ and $D^n f$ vanish say $O(e^{-|x|^2})$ as $|x| \to \infty$, then given $U$, we may find $\phi$ such that $f \circ \phi$ has bounded derivatives of all orders?","If anyone bothered to check on my question history, they would see that I have already been asking several questions related to this theme. Unfortunately I haven't had much success on the specifics, so I might as well as ask this super general question as a last resort. But fundamentally, I just want to know that if $f \in C^\infty(R^n)$ and all of its derivatives vanish in some sense rapidly enough as $|x| \to \infty$, and $\phi: U \to R^n$ is a diffeomorphism, $U$ an open bounded connected subset of $R^n$, then can we assert that $f\circ \phi: U \to R^n$ has bounded derivatives of all orders? The issue is of course that since $U$ is a bounded set, $D\phi$ must diverge, and in fact every derivative diverges hence they must do so rapidly (?). $D(f\circ \phi) = (Df)\circ \phi D\phi$, so it comes down to comparing the rate of vanishing of $D^n f$ and the rate of divergence of $D^m \phi$. This question will probably defy any sort of generalization, but can we at least assert that if $f$ and $D^n f$ vanish say $O(e^{-|x|^2})$ as $|x| \to \infty$, then given $U$, we may find $\phi$ such that $f \circ \phi$ has bounded derivatives of all orders?",,"['multivariable-calculus', 'differential-geometry', 'differential-topology']"
22,Multivariate Calculus: Differentiating the following problem,Multivariate Calculus: Differentiating the following problem,,"Suppose $\mu$ is $m \times 1 $, $A$ is $m \times m$, $B$ is always $m \times n$ and $\Sigma$ is $n \times n$. Note that $\Sigma$ is symmetric. I need to differentiate the follow form: $$\ell = -\log( \det[B \Sigma B^T]) - \operatorname{tr}([B \Sigma B^T]^{-1} [\mu\mu^T - \mu\mu^T A^T - A(\mu\mu^T)^T + A \mu\mu^T A^T])$$ Now I would like to know how can I obtain the following: $$\frac{\partial \ell }{\partial A} = \text{?}$$ $$\frac{\partial \ell }{\partial \Sigma} = \text{?}$$ $$\frac{\partial \ell }{\partial B} = \text{?}$$ And What would the optimal $A$ , $\Sigma$ and $B$ be after differentiating and rearranging the terms to one side ? Update: Through its differential, I have taken an attempt and obtained the following:  Let $Z = [\mu\mu^T - \mu\mu^T A^T - A(\mu\mu^T)^T + A \mu\mu^T A^T]$ $$d \ell = -tr\Big(\big[2 B^T \Sigma(B\Sigma B^T)^{-1})\big]dB + \big[ B^T(B\Sigma B^T)^{-1}B\big] d\Sigma + \big[ (B\Sigma B^T)^{-1}Z(B\Sigma B^T)^{-1} B\Sigma +\big((B\Sigma B^T)^{-1}Z(B \Sigma B^T)^{-1}B \Sigma\big)^T \big]dB + \big[ B^T(B\Sigma B^T)^{-1}Z^T (B\Sigma B^T)^{-1}B\big]d \Sigma\Big) - tr\Big(\Big(\big[B\Sigma B^T\big]^{-1}\big[ 2\mu^T\mu - 2\mu\mu^TA^T\big]\Big)dA\Big)$$ Please kindly verify if it is correct. The problem that remains is how to rearrange the terms such that the optimal $A, B, \Sigma$ will be on one side.","Suppose $\mu$ is $m \times 1 $, $A$ is $m \times m$, $B$ is always $m \times n$ and $\Sigma$ is $n \times n$. Note that $\Sigma$ is symmetric. I need to differentiate the follow form: $$\ell = -\log( \det[B \Sigma B^T]) - \operatorname{tr}([B \Sigma B^T]^{-1} [\mu\mu^T - \mu\mu^T A^T - A(\mu\mu^T)^T + A \mu\mu^T A^T])$$ Now I would like to know how can I obtain the following: $$\frac{\partial \ell }{\partial A} = \text{?}$$ $$\frac{\partial \ell }{\partial \Sigma} = \text{?}$$ $$\frac{\partial \ell }{\partial B} = \text{?}$$ And What would the optimal $A$ , $\Sigma$ and $B$ be after differentiating and rearranging the terms to one side ? Update: Through its differential, I have taken an attempt and obtained the following:  Let $Z = [\mu\mu^T - \mu\mu^T A^T - A(\mu\mu^T)^T + A \mu\mu^T A^T]$ $$d \ell = -tr\Big(\big[2 B^T \Sigma(B\Sigma B^T)^{-1})\big]dB + \big[ B^T(B\Sigma B^T)^{-1}B\big] d\Sigma + \big[ (B\Sigma B^T)^{-1}Z(B\Sigma B^T)^{-1} B\Sigma +\big((B\Sigma B^T)^{-1}Z(B \Sigma B^T)^{-1}B \Sigma\big)^T \big]dB + \big[ B^T(B\Sigma B^T)^{-1}Z^T (B\Sigma B^T)^{-1}B\big]d \Sigma\Big) - tr\Big(\Big(\big[B\Sigma B^T\big]^{-1}\big[ 2\mu^T\mu - 2\mu\mu^TA^T\big]\Big)dA\Big)$$ Please kindly verify if it is correct. The problem that remains is how to rearrange the terms such that the optimal $A, B, \Sigma$ will be on one side.",,['multivariable-calculus']
23,(Crazy idea) Line integral and the generalization of the $x$-axis,(Crazy idea) Line integral and the generalization of the -axis,x,"I've been thinking about the mathematical motivation of the definition of line integrals and a crazy idea came to my mind. First of all, let's begin with the definition of integral line of a function: If $f$ is defined on a smooth curve $\gamma:[a,b]\to \mathbb R$ , then the line integral of $f$ along $\gamma$ is $$\int_{\gamma}f(x,y)ds=\int_a^bf(\gamma(t))|\gamma'(t)|dt$$ For me this definition is simply saying that the line integration is the one where the function is being integrate over a curve instead of a $x$ -axis: Note the $|\gamma'(t)|$ is the correction of the lengths of the partitions of the curves (the partitions are in the curve instead of the $x$ -axis). Am I right?","I've been thinking about the mathematical motivation of the definition of line integrals and a crazy idea came to my mind. First of all, let's begin with the definition of integral line of a function: If is defined on a smooth curve , then the line integral of along is For me this definition is simply saying that the line integration is the one where the function is being integrate over a curve instead of a -axis: Note the is the correction of the lengths of the partitions of the curves (the partitions are in the curve instead of the -axis). Am I right?","f \gamma:[a,b]\to \mathbb R f \gamma \int_{\gamma}f(x,y)ds=\int_a^bf(\gamma(t))|\gamma'(t)|dt x |\gamma'(t)| x","['multivariable-calculus', 'line-integrals']"
24,Find the linear tangent map $T_{I_n} \ f$,Find the linear tangent map,T_{I_n} \ f,"I want to find $T_{I_n} \ f$ where the map is $\ f : X \mapsto X^2$ with $X\in SL_n(\mathbb{R})$. By definition of the linear tangent map : $T_{I_n}\ f : T_{I_n}SL_n (\mathbb{R})\to T_{f(I_n)}SL_n(\mathbb{R})$. But here notice that $f(I_n)=I_n^2=I_n$ and $\ f : SL_n(\mathbb{R})\to SL_n (\mathbb{R})$ because if $\det (X)=1$ then $\det(X^2)=(\det(X))^2=1$. Now I want to determine $T_{I_n}SL_n (\mathbb{R})$. So I consider the $\mathcal{C}^{\infty}$-map $g: X \mapsto \det(X)-1$ for $X\in SL_n(\mathbb{R})$. Using the basis of elementary matrix and differentiability theory I find : for $H \in \mathcal{M}_n(\mathbb{R})$ $Dg(X).H=Tr(X^{-1}H).$ Notice that it's a submanifold of $\mathcal{M}_n(\mathbb{R})$ where the dimension is $n^2-1$ (the codim is $1$ by definition of $g$). Indeed the derivative is surjective (we can chose for instance $H=X\neq 0$ and the derivative does not vanish) so $g$ is a submersion. Now we link this argument to the tangent space. For $X$ the tangent space is : $\ker Dg(X).H=Tr(X^{-1}H)=0$. So for $I_n$ it is $T_{I_n}SL_n(\mathbb{R})=\{H \in \mathcal{M}_n(\mathbb{R})\ / \ Tr(H)=0\}=\mathfrak{sl}(\mathbb{R})$. So I have to find : $T_{I_n}\ f : \mathfrak{sl}(\mathbb{R}) \to \mathfrak{sl}(\mathbb{R})$. Now I use the argument of drawing a curve on the manifold. I have to build $\gamma : 0\in L\subset \mathbb{R} \to SL_n (\mathbb{R})$ where $\gamma(0)=I_n \in SL_n(\mathbb{R})$. Then if I chose $\gamma(t)=I_n+tX$, $t\in L$, I get $\gamma'(0)=X\in SL_n(\mathbb{R})$. But is $\gamma(t)\in SL_n(\mathbb{R})$ for all $t\in L$ ? Or maybe I have to use the set $\mathfrak{sl}(\mathbb{R})$ ? I think I'm confused with the domains of maps. To conclude I have to use the equivalence class of curves such that : $T_{I_n} \ f (\overline{\gamma(t)})=\overline{f(\gamma(t))}$. Thanks in advance !","I want to find $T_{I_n} \ f$ where the map is $\ f : X \mapsto X^2$ with $X\in SL_n(\mathbb{R})$. By definition of the linear tangent map : $T_{I_n}\ f : T_{I_n}SL_n (\mathbb{R})\to T_{f(I_n)}SL_n(\mathbb{R})$. But here notice that $f(I_n)=I_n^2=I_n$ and $\ f : SL_n(\mathbb{R})\to SL_n (\mathbb{R})$ because if $\det (X)=1$ then $\det(X^2)=(\det(X))^2=1$. Now I want to determine $T_{I_n}SL_n (\mathbb{R})$. So I consider the $\mathcal{C}^{\infty}$-map $g: X \mapsto \det(X)-1$ for $X\in SL_n(\mathbb{R})$. Using the basis of elementary matrix and differentiability theory I find : for $H \in \mathcal{M}_n(\mathbb{R})$ $Dg(X).H=Tr(X^{-1}H).$ Notice that it's a submanifold of $\mathcal{M}_n(\mathbb{R})$ where the dimension is $n^2-1$ (the codim is $1$ by definition of $g$). Indeed the derivative is surjective (we can chose for instance $H=X\neq 0$ and the derivative does not vanish) so $g$ is a submersion. Now we link this argument to the tangent space. For $X$ the tangent space is : $\ker Dg(X).H=Tr(X^{-1}H)=0$. So for $I_n$ it is $T_{I_n}SL_n(\mathbb{R})=\{H \in \mathcal{M}_n(\mathbb{R})\ / \ Tr(H)=0\}=\mathfrak{sl}(\mathbb{R})$. So I have to find : $T_{I_n}\ f : \mathfrak{sl}(\mathbb{R}) \to \mathfrak{sl}(\mathbb{R})$. Now I use the argument of drawing a curve on the manifold. I have to build $\gamma : 0\in L\subset \mathbb{R} \to SL_n (\mathbb{R})$ where $\gamma(0)=I_n \in SL_n(\mathbb{R})$. Then if I chose $\gamma(t)=I_n+tX$, $t\in L$, I get $\gamma'(0)=X\in SL_n(\mathbb{R})$. But is $\gamma(t)\in SL_n(\mathbb{R})$ for all $t\in L$ ? Or maybe I have to use the set $\mathfrak{sl}(\mathbb{R})$ ? I think I'm confused with the domains of maps. To conclude I have to use the equivalence class of curves such that : $T_{I_n} \ f (\overline{\gamma(t)})=\overline{f(\gamma(t))}$. Thanks in advance !",,"['multivariable-calculus', 'differential-geometry', 'manifolds', 'smooth-manifolds', 'tangent-bundle']"
25,Integration of differential forms - how to extend (and fix?) this intuition?,Integration of differential forms - how to extend (and fix?) this intuition?,,"For context, I'm a first-year undergrad in a linear algebra / multivariable calculus course. I've developed some intuition about $k$-vectors and $k$-forms, and I want to know: Are there any problems with this intuition (mistakes or areas in which it is counterproductive)? How can this be extended to things like closed/exact forms, the exterior derivative, and integration of differential forms? $\newcommand{\reals}{\mathbb{R}}$ Oh, and I've heard that all of this can be extended to ""fields"" instead of just $\reals$, and the complex numbers are an example of such a field. I haven't worked with them, though, so I'm just going to use $\reals$ for now. Okay, so I'm familiar with vectors (both as elements of an abstract vector space and as elements of $\reals^n$ - in my class, we're mainly working with $\reals^n$, though). From what I understand, a bivector (in Euclidean space) can be thought of as an ""oriented parallelogram"" similarly to how vectors can be thought of as oriented line segments. Similarly, a trivector can be thought of as an oriented parallelepiped, and this is extended to higher dimensions that are harder to visualize. We can construct a $k$-vector from $k$ regular vectors by taking the wedge product of those vectors. The wedge product is multilinear and alternating. (This defines an ""orientation"" - the equivalent of forward/back or clockwise/counterclockwise - for each $k$-vector that switches if you switch two vectors in the wedge product.) Geometrically, if two oriented parallelepipeds have the same orientation, they ""span the same subspace of $\reals^n$"", and one's ""component vectors"" can be rotated and scaled to ""meet"" the other's without leaving that subspace or using reflections. (If they ""share the same subspace"" but cannot be rotated/scaled to ""meet"" each other, then one is a negative scalar multiple of the other.) (I'd never state it formally like this, but this is just about getting across my intuition, so hopefully it makes sense.) A covector is an element of the dual space of a vector space, $V^*$, and is a linear transformation from $V$ to $\reals$. $V^*$ is also a vector space. Covectors are also called $1$-forms. They can also be wedged together to create $k$-forms, which are alternating multilinear transformations from $V^n$ to $\reals$. (We write the set of all $k$-forms as $\Lambda^k(\reals^n)$.) (I'm vaguely familiar with this visualization of $k$-forms, but the exterior derivative part loses me.) A smooth vector field is a $\mathcal C^\infty$ function that maps every point in $\reals^n$ to a vector. (Though I've never heard the terms, I assume there are also ""bivector fields"", ""trivector fields"", and so on.) A differential $k$-form is a smooth $k$-covector field, which is the same as a smooth vector field but replacing ""vector"" with ""$k$-covector"" or ""$k$-form"". Is this intuition ""correct""? Can it be extended to describe closed/exact $k$-forms, the exterior derivative, and/or integration of differential forms?","For context, I'm a first-year undergrad in a linear algebra / multivariable calculus course. I've developed some intuition about $k$-vectors and $k$-forms, and I want to know: Are there any problems with this intuition (mistakes or areas in which it is counterproductive)? How can this be extended to things like closed/exact forms, the exterior derivative, and integration of differential forms? $\newcommand{\reals}{\mathbb{R}}$ Oh, and I've heard that all of this can be extended to ""fields"" instead of just $\reals$, and the complex numbers are an example of such a field. I haven't worked with them, though, so I'm just going to use $\reals$ for now. Okay, so I'm familiar with vectors (both as elements of an abstract vector space and as elements of $\reals^n$ - in my class, we're mainly working with $\reals^n$, though). From what I understand, a bivector (in Euclidean space) can be thought of as an ""oriented parallelogram"" similarly to how vectors can be thought of as oriented line segments. Similarly, a trivector can be thought of as an oriented parallelepiped, and this is extended to higher dimensions that are harder to visualize. We can construct a $k$-vector from $k$ regular vectors by taking the wedge product of those vectors. The wedge product is multilinear and alternating. (This defines an ""orientation"" - the equivalent of forward/back or clockwise/counterclockwise - for each $k$-vector that switches if you switch two vectors in the wedge product.) Geometrically, if two oriented parallelepipeds have the same orientation, they ""span the same subspace of $\reals^n$"", and one's ""component vectors"" can be rotated and scaled to ""meet"" the other's without leaving that subspace or using reflections. (If they ""share the same subspace"" but cannot be rotated/scaled to ""meet"" each other, then one is a negative scalar multiple of the other.) (I'd never state it formally like this, but this is just about getting across my intuition, so hopefully it makes sense.) A covector is an element of the dual space of a vector space, $V^*$, and is a linear transformation from $V$ to $\reals$. $V^*$ is also a vector space. Covectors are also called $1$-forms. They can also be wedged together to create $k$-forms, which are alternating multilinear transformations from $V^n$ to $\reals$. (We write the set of all $k$-forms as $\Lambda^k(\reals^n)$.) (I'm vaguely familiar with this visualization of $k$-forms, but the exterior derivative part loses me.) A smooth vector field is a $\mathcal C^\infty$ function that maps every point in $\reals^n$ to a vector. (Though I've never heard the terms, I assume there are also ""bivector fields"", ""trivector fields"", and so on.) A differential $k$-form is a smooth $k$-covector field, which is the same as a smooth vector field but replacing ""vector"" with ""$k$-covector"" or ""$k$-form"". Is this intuition ""correct""? Can it be extended to describe closed/exact $k$-forms, the exterior derivative, and/or integration of differential forms?",,"['multivariable-calculus', 'differential-geometry', 'intuition', 'exterior-algebra']"
26,Convergence of Taylor series in several variables,Convergence of Taylor series in several variables,,"Given the function  $$ f(x,y,z) = \frac{1}{x^2+y^2+z^2} \qquad x,y,z\in \mathbb{R} \;, $$ I want to calculate the domain of convergence of the corresponding Taylor expansion at point $P_0 = (x_0, y_0, z_0).$ How can that be accomplished? If the function depends only on a single variable that is done by the Cauchy-Hadamard theorem. But how do you do that if it depends on multiple variables? I found this journal article about convergence of power series in several complex variables and ""Reinhard domains"" https://arxiv.org/abs/1601.00274 helpful for understanding but I am still not able to calculate the domain of convergence. The only thing I was able to calculate up to now by a numerical approximation is the convergence radius of the Taylor expansion of $f(x,y,z)$ at $P_0$ in the direction pointing towards the singularity. The result (as expected) is $\sqrt{x_0^2+y_0^2+z_0^2}$ for that path. But what is the convergence radius if I choose a path that is passing by the singularity at a certain distance and not passing through the singularity? Is it infinite?","Given the function  $$ f(x,y,z) = \frac{1}{x^2+y^2+z^2} \qquad x,y,z\in \mathbb{R} \;, $$ I want to calculate the domain of convergence of the corresponding Taylor expansion at point $P_0 = (x_0, y_0, z_0).$ How can that be accomplished? If the function depends only on a single variable that is done by the Cauchy-Hadamard theorem. But how do you do that if it depends on multiple variables? I found this journal article about convergence of power series in several complex variables and ""Reinhard domains"" https://arxiv.org/abs/1601.00274 helpful for understanding but I am still not able to calculate the domain of convergence. The only thing I was able to calculate up to now by a numerical approximation is the convergence radius of the Taylor expansion of $f(x,y,z)$ at $P_0$ in the direction pointing towards the singularity. The result (as expected) is $\sqrt{x_0^2+y_0^2+z_0^2}$ for that path. But what is the convergence radius if I choose a path that is passing by the singularity at a certain distance and not passing through the singularity? Is it infinite?",,"['multivariable-calculus', 'convergence-divergence', 'numerical-methods', 'taylor-expansion']"
27,Another Hessian determinant question,Another Hessian determinant question,,"The Kulkarni–Nomizu product of two symmetric matrices $h_{ij}$ and $k_{kl}$ is $$ (h\wedge \!\!\!\!\!\!\bigcirc k)_{ijkl} = h_{ik}k_{jl} + h_{jl}k_{ik} - h_{il}k_{jk} - h_{jk}k_{il}. $$ 'Squaring' a matrix with this product, we get \begin{align} (h\wedge \!\!\!\!\!\!\bigcirc h)_{ijkl} &= h_{ik}h_{jl} + h_{jl}h_{ik} - h_{il}h_{jk} - h_{jk}h_{il} \\ &= 2(h_{ik}h_{jl} - h_{il}h_{jk}), \end{align} (which I recognize as twice the determinant of $h_{ij}$ in 2D). In particular, if $h_{ij}$ is the Hessian of a scalar function $f$, $$ h_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j} \equiv f_{,ij}, $$ then half the product is $$ \tfrac{1}{2}(h\wedge \!\!\!\!\!\!\bigcirc h)_{ijkl} = f_{,ik}f_{,jl} - f_{,il}f_{,jk}. $$ Again, in 2D, this is the determinant of the Hessian, which I understand to be the discriminant at a critical point (only in 2D). Finally, if we trace this over the indices $(i,k)$, and again over $(j,l)$, we get \begin{align} \sum_i \sum_j \tfrac{1}{2}(h\wedge \!\!\!\!\!\!\bigcirc h)_{ijij} &= \sum_i h_{ii} \sum_j h_{jj} - \sum_i \sum_j h_{ij} h_{ji} \\ &= (h_{11}+h_{22}+\ldots)^2 - (h_{11}^2+h_{22}^2+\ldots+h_{12}h_{21}+\ldots) \\ &= 2(h_{11}h_{22}-h_{21}h_{21})+2(h_{11}h_{33}-h_{31}h_{31})+\ldots \\ &= 2\sum (\text{determinants of $2\times2$ minors of $h$.}) \\ &= 2\sum (\text{products of eigenvalues of $2\times2$ minors of $h$?}) \end{align} If $h$ is the Hessian of $f$, this is $$ 2(f_{,11}f_{,22}-f_{,21}f_{,21})+2(f_{,11}f_{,33}-f_{,31}f_{,31})+\ldots  $$ or (I think) $$ 2(\lambda_1\lambda_2 + \lambda_1\lambda_3 + \ldots + \lambda_2\lambda_3 + \ldots) $$ Questions What would possess me to take the Kulkarni–Nomizu product of a Hessian? Has anyone seen anything like this? What does the product tell us about $f$ geometrically? (Not necessarily at a critical point). What does the double trace tell us about $f$? What does the sum of pairs of eigenvalues mean?? EDIT: I just found out this is called the 2nd symmetric function of eigenvalues. What does it mean?","The Kulkarni–Nomizu product of two symmetric matrices $h_{ij}$ and $k_{kl}$ is $$ (h\wedge \!\!\!\!\!\!\bigcirc k)_{ijkl} = h_{ik}k_{jl} + h_{jl}k_{ik} - h_{il}k_{jk} - h_{jk}k_{il}. $$ 'Squaring' a matrix with this product, we get \begin{align} (h\wedge \!\!\!\!\!\!\bigcirc h)_{ijkl} &= h_{ik}h_{jl} + h_{jl}h_{ik} - h_{il}h_{jk} - h_{jk}h_{il} \\ &= 2(h_{ik}h_{jl} - h_{il}h_{jk}), \end{align} (which I recognize as twice the determinant of $h_{ij}$ in 2D). In particular, if $h_{ij}$ is the Hessian of a scalar function $f$, $$ h_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j} \equiv f_{,ij}, $$ then half the product is $$ \tfrac{1}{2}(h\wedge \!\!\!\!\!\!\bigcirc h)_{ijkl} = f_{,ik}f_{,jl} - f_{,il}f_{,jk}. $$ Again, in 2D, this is the determinant of the Hessian, which I understand to be the discriminant at a critical point (only in 2D). Finally, if we trace this over the indices $(i,k)$, and again over $(j,l)$, we get \begin{align} \sum_i \sum_j \tfrac{1}{2}(h\wedge \!\!\!\!\!\!\bigcirc h)_{ijij} &= \sum_i h_{ii} \sum_j h_{jj} - \sum_i \sum_j h_{ij} h_{ji} \\ &= (h_{11}+h_{22}+\ldots)^2 - (h_{11}^2+h_{22}^2+\ldots+h_{12}h_{21}+\ldots) \\ &= 2(h_{11}h_{22}-h_{21}h_{21})+2(h_{11}h_{33}-h_{31}h_{31})+\ldots \\ &= 2\sum (\text{determinants of $2\times2$ minors of $h$.}) \\ &= 2\sum (\text{products of eigenvalues of $2\times2$ minors of $h$?}) \end{align} If $h$ is the Hessian of $f$, this is $$ 2(f_{,11}f_{,22}-f_{,21}f_{,21})+2(f_{,11}f_{,33}-f_{,31}f_{,31})+\ldots  $$ or (I think) $$ 2(\lambda_1\lambda_2 + \lambda_1\lambda_3 + \ldots + \lambda_2\lambda_3 + \ldots) $$ Questions What would possess me to take the Kulkarni–Nomizu product of a Hessian? Has anyone seen anything like this? What does the product tell us about $f$ geometrically? (Not necessarily at a critical point). What does the double trace tell us about $f$? What does the sum of pairs of eigenvalues mean?? EDIT: I just found out this is called the 2nd symmetric function of eigenvalues. What does it mean?",,"['linear-algebra', 'multivariable-calculus', 'differential-geometry', 'matrix-calculus', 'hessian-matrix']"
28,Show directional derivative equals $0$ for the local maximum or minimum along the same direction,Show directional derivative equals  for the local maximum or minimum along the same direction,0,"Let $(A,V,\|\cdot\|_V)$ be a $n$ -dimensional normed affine space where $A$ is the point set and $V$ is a $F$ -vector space, and $F$ is equipped with a modulus $|\cdot|$ . Let $(B,W,\|\cdot\|_W)$ be a one-dimensional normed affine space in which $W$ is a one dimensional $F$ -vector space. If we choose an origin $b\in B$ and a non-zero vector ${\bf w}\in W$ then every point in $B$ can be written as $b+c{\bf w}$ for some $c \in F$ . Here all norms and modulus are real-valued. Again, note $B$ and $W$ are ONE-DIMENSIONAL. 1. Directional derivative A function $f:A \to B$ , we say $f$ is differentiable at $a \in A$ along direction ${\bf{v}}\in V$ if there exists some ${\bf u} \in W$ s.t. $\lim_{h→0}⁡\frac{‖f({ a}+h{\bf v})-f({ a})-h{\bf u}‖_W}{|h|}=0$ where $h \to 0$ is a notation for $|h| \to 0$ . We say $\frac{\partial f({ a})}{\partial {\bf v}}={\bf u}$ is the derivative of $f$ along direction $\bf {v}$ at $ a$ . 2. Local maximum and minimum along a direction When an origin $b\in B$ and a base ${\bf w} \in W$ are chosen, a $f:A \to B$ map is equivalent to a corresponding $f_F:A\to F$ map since every point in $B$ is uniquely mapped to a coordinate in $F$ . A function $f:A\to B$ achieves a local maximum at point $a^* \in A$ along direction ${\bf v} \in V$ if there exists $\epsilon > 0$ s.t. $|f_F(a^*+h{\bf v})|\le|f_F(a^*)|$ for all $|h|<\epsilon$ . Similarly $f$ achieves a local minimum at $a^* \in A$ along direction $\bf v$ if there exists $\epsilon > 0$ s.t. $|f_F(a^*+h{\bf v})|\ge|f_F(a^*)|$ for all $|h|<\epsilon$ . Problem Now the problem is to prove if $f$ is differentiable along direction $\bf v$ near and at a local maximum or minimum $a^*$ , then $\frac{\partial f({ a^*})}{\partial {\bf v}}={\bf 0}$","Let be a -dimensional normed affine space where is the point set and is a -vector space, and is equipped with a modulus . Let be a one-dimensional normed affine space in which is a one dimensional -vector space. If we choose an origin and a non-zero vector then every point in can be written as for some . Here all norms and modulus are real-valued. Again, note and are ONE-DIMENSIONAL. 1. Directional derivative A function , we say is differentiable at along direction if there exists some s.t. where is a notation for . We say is the derivative of along direction at . 2. Local maximum and minimum along a direction When an origin and a base are chosen, a map is equivalent to a corresponding map since every point in is uniquely mapped to a coordinate in . A function achieves a local maximum at point along direction if there exists s.t. for all . Similarly achieves a local minimum at along direction if there exists s.t. for all . Problem Now the problem is to prove if is differentiable along direction near and at a local maximum or minimum , then","(A,V,\|\cdot\|_V) n A V F F |\cdot| (B,W,\|\cdot\|_W) W F b\in B {\bf w}\in W B b+c{\bf w} c \in F B W f:A \to B f a \in A {\bf{v}}\in V {\bf u} \in W \lim_{h→0}⁡\frac{‖f({ a}+h{\bf v})-f({ a})-h{\bf u}‖_W}{|h|}=0 h \to 0 |h| \to 0 \frac{\partial f({ a})}{\partial {\bf v}}={\bf u} f \bf {v}  a b\in B {\bf w} \in W f:A \to B f_F:A\to F B F f:A\to B a^* \in A {\bf v} \in V \epsilon > 0 |f_F(a^*+h{\bf v})|\le|f_F(a^*)| |h|<\epsilon f a^* \in A \bf v \epsilon > 0 |f_F(a^*+h{\bf v})|\ge|f_F(a^*)| |h|<\epsilon f \bf v a^* \frac{\partial f({ a^*})}{\partial {\bf v}}={\bf 0}","['real-analysis', 'linear-algebra', 'multivariable-calculus', 'differential-geometry']"
29,"In the context of curl, what does the difference between two partial derivatives tell me about rotation in a plane?","In the context of curl, what does the difference between two partial derivatives tell me about rotation in a plane?",,"Let $\mathbf{A}(x,y,z)$ be a vector field. The curl of this vector field is defined as $$\nabla \times \mathbf{A} = \left(\frac{\partial A_z}{\partial y} - \frac{\partial A_y}{\partial z}\right) \mathbf{i} + \left(\frac{\partial A_x}{\partial z} - \frac{\partial A_z}{\partial x}\right) \mathbf{j} + \left(\frac{\partial A_y}{\partial x} - \frac{\partial A_x}{\partial y}\right) \mathbf{k}$$ Let's consider a simple example from Wikipedia. $$\mathbf{A}(x,y,z) = y\mathbf{\hat{x}}-x\mathbf{\hat{y}}+0\mathbf{\hat{z}}$$ This corresponds to the following source: https://commons.wikimedia.org/wiki/File:Uniform_curl.svg The curl is $$\nabla \times \mathbf{A} =0\boldsymbol{\hat{x}}+0\mathbf{\hat{y}}+ \left({\frac{\partial}{\partial x}}(-x) -{\frac{\partial}{\partial y}} y\right)\mathbf{\hat{z}}=-2\mathbf{\hat{z}}$$ But there is where I am confused. I don't get what $\left({\frac{\partial}{\partial x}}(-x) -{\frac{\partial}{\partial y}} y\right)$ has to do with the rotation and size of the vectors in the picture shown above. My Question How do these derivatives tell me anything about the rotation? Any why must they be opposites? Why is it not $\frac{\partial A_x}{\partial x}$ ? I understand of course why, when you compute the cross product, it comes out that way, but I want to understand the intuition of why that is important for rotation.","Let be a vector field. The curl of this vector field is defined as Let's consider a simple example from Wikipedia. This corresponds to the following source: https://commons.wikimedia.org/wiki/File:Uniform_curl.svg The curl is But there is where I am confused. I don't get what has to do with the rotation and size of the vectors in the picture shown above. My Question How do these derivatives tell me anything about the rotation? Any why must they be opposites? Why is it not ? I understand of course why, when you compute the cross product, it comes out that way, but I want to understand the intuition of why that is important for rotation.","\mathbf{A}(x,y,z) \nabla \times \mathbf{A} = \left(\frac{\partial A_z}{\partial y} - \frac{\partial A_y}{\partial z}\right) \mathbf{i} + \left(\frac{\partial A_x}{\partial z} - \frac{\partial A_z}{\partial x}\right) \mathbf{j} + \left(\frac{\partial A_y}{\partial x} - \frac{\partial A_x}{\partial y}\right) \mathbf{k} \mathbf{A}(x,y,z) = y\mathbf{\hat{x}}-x\mathbf{\hat{y}}+0\mathbf{\hat{z}} \nabla \times \mathbf{A} =0\boldsymbol{\hat{x}}+0\mathbf{\hat{y}}+ \left({\frac{\partial}{\partial x}}(-x) -{\frac{\partial}{\partial y}} y\right)\mathbf{\hat{z}}=-2\mathbf{\hat{z}} \left({\frac{\partial}{\partial x}}(-x) -{\frac{\partial}{\partial y}} y\right) \frac{\partial A_x}{\partial x}","['multivariable-calculus', 'vector-analysis']"
30,Change the order of integration if the inner integral is to a power,Change the order of integration if the inner integral is to a power,,"Is there anything I can do with a double integral of the following form? $$ \int_0^1 \left(\int_0^x f(j) dj\right)^\alpha g(x) dx$$ For $\alpha = 1$, I can do $$ \int_0^1 \int_0^x f(j) dj g(x) dx = \int_0^x f(j) \int_0^1    g(x) dx dj$$, where the inner integral has a well defined solution. For general $\alpha$, I know that there is a difference between $x_1^a + x_2^a$ and $(x_1 + x_2)^a$, but is there perhaps a trick I can use to still change the order of integration?","Is there anything I can do with a double integral of the following form? $$ \int_0^1 \left(\int_0^x f(j) dj\right)^\alpha g(x) dx$$ For $\alpha = 1$, I can do $$ \int_0^1 \int_0^x f(j) dj g(x) dx = \int_0^x f(j) \int_0^1    g(x) dx dj$$, where the inner integral has a well defined solution. For general $\alpha$, I know that there is a difference between $x_1^a + x_2^a$ and $(x_1 + x_2)^a$, but is there perhaps a trick I can use to still change the order of integration?",,"['integration', 'multivariable-calculus', 'definite-integrals']"
31,Why don't we use partial notation for double integrals?,Why don't we use partial notation for double integrals?,,"Now I realize there are a few questions like this on here, but none of them really get to the heart of what I'm asking... In single variable calculus we learn that the following can describe the relationship between a derivative and an integral... $$ \int \frac{dy}{dx} dx = \int dy \frac{dx}{dx} = \int dy = y  $$ In multivariable calculus we learn to take partial derivatives using the  $\partial$ symbol but then unintuitively learn to take double integrals with this notation... $$ \int\int f(x,y)dxdy $$ But using the relationships from single variable calculus (of a derivative to an integral), this would intuitively seem to be the way to notate a double integral... $$\int\int \frac{\partial z}{\partial x \partial y} \partial x \partial y = \int\int \partial z \frac{\partial x}{\partial x } \frac{\partial y}{\partial y } = \int\int \partial z = z $$ As opposed to the common notation of... $$ f(x,y)=\frac{\partial z}{\partial x \partial y} \rightarrow \int\int f(x, y) dx dy = z $$ Wouldn't this, however, imply... $$\int\int \frac{\partial z}{\partial x \partial y} dx dy = \int\int \partial z \frac{dx}{\partial x } \frac{dy}{\partial y }  $$ For what reason do we use this notation when it doesn't seem algebraically consistent like single variable notation? Does $ \frac{dx}{\partial x} = 1 $ in the same way that $ \frac{dx}{dx} = 1 $ ? Am I wildly overthinking things?","Now I realize there are a few questions like this on here, but none of them really get to the heart of what I'm asking... In single variable calculus we learn that the following can describe the relationship between a derivative and an integral... $$ \int \frac{dy}{dx} dx = \int dy \frac{dx}{dx} = \int dy = y  $$ In multivariable calculus we learn to take partial derivatives using the  $\partial$ symbol but then unintuitively learn to take double integrals with this notation... $$ \int\int f(x,y)dxdy $$ But using the relationships from single variable calculus (of a derivative to an integral), this would intuitively seem to be the way to notate a double integral... $$\int\int \frac{\partial z}{\partial x \partial y} \partial x \partial y = \int\int \partial z \frac{\partial x}{\partial x } \frac{\partial y}{\partial y } = \int\int \partial z = z $$ As opposed to the common notation of... $$ f(x,y)=\frac{\partial z}{\partial x \partial y} \rightarrow \int\int f(x, y) dx dy = z $$ Wouldn't this, however, imply... $$\int\int \frac{\partial z}{\partial x \partial y} dx dy = \int\int \partial z \frac{dx}{\partial x } \frac{dy}{\partial y }  $$ For what reason do we use this notation when it doesn't seem algebraically consistent like single variable notation? Does $ \frac{dx}{\partial x} = 1 $ in the same way that $ \frac{dx}{dx} = 1 $ ? Am I wildly overthinking things?",,"['integration', 'multivariable-calculus', 'notation']"
32,Closed surfaces and Multivariable Calculus,Closed surfaces and Multivariable Calculus,,"Is it possible to know if a surface is closed by its equation(for example, the surface: $x^{2/3} + y^{2/3} + z^2 = 1$) only using Multivariable Calculus ? If so, how ? I'd like references(books) that explain this.","Is it possible to know if a surface is closed by its equation(for example, the surface: $x^{2/3} + y^{2/3} + z^2 = 1$) only using Multivariable Calculus ? If so, how ? I'd like references(books) that explain this.",,"['calculus', 'multivariable-calculus']"
33,Mixed derivatives in multivariable calculus [closed],Mixed derivatives in multivariable calculus [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose $f$ is a function having variables $x$ and $y$. Can $f_{xy}$ at a point be equal to $f_{yx}$ without any of the two being continuous there? If they are equal but discontinuous functions?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose $f$ is a function having variables $x$ and $y$. Can $f_{xy}$ at a point be equal to $f_{yx}$ without any of the two being continuous there? If they are equal but discontinuous functions?",,"['calculus', 'multivariable-calculus', 'derivatives']"
34,The divergence theorem (Gauss) in 1 dimension: For me it is not clear how to compute the boundary integral!,The divergence theorem (Gauss) in 1 dimension: For me it is not clear how to compute the boundary integral!,,"Gauss tells us that     $$ \int_V \text{div}\vec{F}\, d^{(n)}V=\int_{S}\vec{F}\cdot\vec{\nu}\, d^{(n-1)}S, $$     where $S=\partial V$ and $\vec{\nu}$ is the outer normal. For $n=1$, $V=[a,b]$ and $f\colon V\to\mathbb{R}$, this gives $$ \int_a^b\frac{df}{dx}dx = \int_{S}f\cdot \nu d^{(0)}S, $$ $S=\partial V=\left\{a,b\right\}$ and $\nu(a)=-1, \nu(b)=1$. In nearly all literature it is said without any reasoning that $$ \int_{S}f\cdot \nu d^{(0)}S=f(b)-f(a), $$ i.e. we get the fundamental theorem of calculus. I really wonder how we really can compute this integral, that is, how can we parametrize $S$ and what the meaning of the $0$-dimensional Lebesgue-measure could be. To be more concrete: Assume $S=\gamma(A)$ for some parametrization $\gamma$ (here $A$ has to be $0$-dim.), then we compute the integral over S by $$ \int_A (f\cdot\nu)(\gamma(x))\sqrt{\text{det}(D\gamma(x)^T D\gamma(x))}\lambda^0 x $$ But ... (1) What is a good parametrization $\gamma$? Maybe just the identity map $\text{id}\colon S\to S$? (2) What is the 0-dimensional Lebesgue-measure $\lambda^0$?!","Gauss tells us that     $$ \int_V \text{div}\vec{F}\, d^{(n)}V=\int_{S}\vec{F}\cdot\vec{\nu}\, d^{(n-1)}S, $$     where $S=\partial V$ and $\vec{\nu}$ is the outer normal. For $n=1$, $V=[a,b]$ and $f\colon V\to\mathbb{R}$, this gives $$ \int_a^b\frac{df}{dx}dx = \int_{S}f\cdot \nu d^{(0)}S, $$ $S=\partial V=\left\{a,b\right\}$ and $\nu(a)=-1, \nu(b)=1$. In nearly all literature it is said without any reasoning that $$ \int_{S}f\cdot \nu d^{(0)}S=f(b)-f(a), $$ i.e. we get the fundamental theorem of calculus. I really wonder how we really can compute this integral, that is, how can we parametrize $S$ and what the meaning of the $0$-dimensional Lebesgue-measure could be. To be more concrete: Assume $S=\gamma(A)$ for some parametrization $\gamma$ (here $A$ has to be $0$-dim.), then we compute the integral over S by $$ \int_A (f\cdot\nu)(\gamma(x))\sqrt{\text{det}(D\gamma(x)^T D\gamma(x))}\lambda^0 x $$ But ... (1) What is a good parametrization $\gamma$? Maybe just the identity map $\text{id}\colon S\to S$? (2) What is the 0-dimensional Lebesgue-measure $\lambda^0$?!",,"['real-analysis', 'integration', 'multivariable-calculus', 'differential-geometry', 'differential-topology']"
35,derivative of cosine similarity,derivative of cosine similarity,,"What is derivative of cosine similarity between two vectors? I found a paper that refers the derivative of cosine similarity  ($\cos=\frac{v_i\cdot{v_j}}{|v_i||v_j|}$) is as below. $$ \frac{\partial{\cos}}{\partial{v_i}}=\frac{\cos\cdot{v_i}}{\left|v_i\right|^2}+\frac{v_j}{\left|v_i\right|\left|v_j\right|}. $$ There's no detailed explanation in the paper and I couldn't find any derivation of the derivative in internet. Is the derivation correct?  If it's correct, how the derivative is found?","What is derivative of cosine similarity between two vectors? I found a paper that refers the derivative of cosine similarity  ($\cos=\frac{v_i\cdot{v_j}}{|v_i||v_j|}$) is as below. $$ \frac{\partial{\cos}}{\partial{v_i}}=\frac{\cos\cdot{v_i}}{\left|v_i\right|^2}+\frac{v_j}{\left|v_i\right|\left|v_j\right|}. $$ There's no detailed explanation in the paper and I couldn't find any derivation of the derivative in internet. Is the derivation correct?  If it's correct, how the derivative is found?",,"['multivariable-calculus', 'partial-derivative']"
36,Uniform Continuity,Uniform Continuity,,"This question has three parts. a) Difference between continuity and uniform continuity b) Geometrical meaning of uniform continuity c) Correct the example Definition of Continuity of a function in an interval $I$ is quite similar to uniform continuity. I know that continuity is a pointwise property of a function and that when a function is continuous in an interval it means that it is continuous in every point in that interval. It is also easy to unterstant what this geometrically means (i think). The first one is clearly a continuous function while the second one is not. The same logic applies to functions with more variables. However i don't understand what uniform continuity means geometrically and how do we show in which interval  a function is uniformly continuous. For Example:  $$ f(x,y) = x^2 + xy $$ The definition of uniform continuity says: We have two points $P=(x,y)$ and $P_0 = (x_0,y_o) $ $$ \forall ε>0 \exists d>0 / \forall P,P_0 \in I (|x-x_0|<δ, |y-y_0| < δ \Rightarrow |f(P) - f(P_0)|<ε )$$ So we take $ |x-x_0|<δ $ and $ |y-y_0|<δ $ $$|x^2 +xy -x_0^2 -x_0y_0|= | (x-x_0)^2 + (x-x_0)(y-y_0) + (2x_0 + y_0)(x-x_0) + x_0(y-y_0)|<δ^2+δ^2+|2x_0+y_0|δ + |x_0| δ = 2δ^2 + (|2x_0 +y_0|+|x_0|)δ<2δ^2+(3|x_0|+|y_0|)δ $$ Now if we choose $δ<1$ $$|f(P)-f(P_0)|< (3|x_0|+|y_0|+2)δ=ε \Rightarrow δ=\frac{ε}{3|x_0|+|y_0|+2} $$ Now is the condition for the function to be uniformly continuous, δ to be positive? In this case  $$ δ>0 \forall P_0 \in \mathbb R^2 $$ So the function is uniformly continuous in $ \mathbb R^2 $ . However i previously chose $ δ <1 $ so $|PP_0|<1 $. Doesn't that affect the outcome?","This question has three parts. a) Difference between continuity and uniform continuity b) Geometrical meaning of uniform continuity c) Correct the example Definition of Continuity of a function in an interval $I$ is quite similar to uniform continuity. I know that continuity is a pointwise property of a function and that when a function is continuous in an interval it means that it is continuous in every point in that interval. It is also easy to unterstant what this geometrically means (i think). The first one is clearly a continuous function while the second one is not. The same logic applies to functions with more variables. However i don't understand what uniform continuity means geometrically and how do we show in which interval  a function is uniformly continuous. For Example:  $$ f(x,y) = x^2 + xy $$ The definition of uniform continuity says: We have two points $P=(x,y)$ and $P_0 = (x_0,y_o) $ $$ \forall ε>0 \exists d>0 / \forall P,P_0 \in I (|x-x_0|<δ, |y-y_0| < δ \Rightarrow |f(P) - f(P_0)|<ε )$$ So we take $ |x-x_0|<δ $ and $ |y-y_0|<δ $ $$|x^2 +xy -x_0^2 -x_0y_0|= | (x-x_0)^2 + (x-x_0)(y-y_0) + (2x_0 + y_0)(x-x_0) + x_0(y-y_0)|<δ^2+δ^2+|2x_0+y_0|δ + |x_0| δ = 2δ^2 + (|2x_0 +y_0|+|x_0|)δ<2δ^2+(3|x_0|+|y_0|)δ $$ Now if we choose $δ<1$ $$|f(P)-f(P_0)|< (3|x_0|+|y_0|+2)δ=ε \Rightarrow δ=\frac{ε}{3|x_0|+|y_0|+2} $$ Now is the condition for the function to be uniformly continuous, δ to be positive? In this case  $$ δ>0 \forall P_0 \in \mathbb R^2 $$ So the function is uniformly continuous in $ \mathbb R^2 $ . However i previously chose $ δ <1 $ so $|PP_0|<1 $. Doesn't that affect the outcome?",,"['real-analysis', 'multivariable-calculus', 'continuity', 'uniform-continuity']"
37,"Find the gradient of $f(x,y,x)$ given its maximum value at a point $P_0$ in the direction $(1,1,-1)$",Find the gradient of  given its maximum value at a point  in the direction,"f(x,y,x) P_0 (1,1,-1)","I am asked to solve the following problem: The directional derivative of the function $f(x,y,z)$ has maximum value of $2\sqrt{3}$ at the point $P_0$ in the direction $(1,1,-1)$. What is the gradient of $f$ at the point? What is the derivative of $f(x,y,z)$ on the direction $(1,1,0)$? My solution: Let's say the gradient is $(a,b,c)$ and given that the unit vector for the direction $(1,1,-1)$ is $\frac{1}{\sqrt{3}} \left( 1,1,-1 \right)$ $$ 2 \sqrt{3} = \frac{1}{\sqrt{3}} (a,b,c) \cdot (1,1,-1)\\ 6 = (a,b,c) \cdot (1,1,-1)\\ a+b-c = 6 $$ We also know that he maximum value for the directional derivative occurs on the direction of the gradient: $$ (a,b,c) = k (1,1,-1)\\ a = k\\ b = k\\ c = -k $$ Substituting the equations found, we have that $k=2$ and $$(a,b,c) = (2,2,-2)$$ For the second question, the directional derivative is $$ \frac{1}{\sqrt{2}} (2,2,-2) \cdot (1,1,0) = 2 \sqrt{2} $$ Did I make a mistake somewhere? Thank you.","I am asked to solve the following problem: The directional derivative of the function $f(x,y,z)$ has maximum value of $2\sqrt{3}$ at the point $P_0$ in the direction $(1,1,-1)$. What is the gradient of $f$ at the point? What is the derivative of $f(x,y,z)$ on the direction $(1,1,0)$? My solution: Let's say the gradient is $(a,b,c)$ and given that the unit vector for the direction $(1,1,-1)$ is $\frac{1}{\sqrt{3}} \left( 1,1,-1 \right)$ $$ 2 \sqrt{3} = \frac{1}{\sqrt{3}} (a,b,c) \cdot (1,1,-1)\\ 6 = (a,b,c) \cdot (1,1,-1)\\ a+b-c = 6 $$ We also know that he maximum value for the directional derivative occurs on the direction of the gradient: $$ (a,b,c) = k (1,1,-1)\\ a = k\\ b = k\\ c = -k $$ Substituting the equations found, we have that $k=2$ and $$(a,b,c) = (2,2,-2)$$ For the second question, the directional derivative is $$ \frac{1}{\sqrt{2}} (2,2,-2) \cdot (1,1,0) = 2 \sqrt{2} $$ Did I make a mistake somewhere? Thank you.",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
38,How to take the second derivative using multi variable chain rule?,How to take the second derivative using multi variable chain rule?,,I am working on this question: Now I have the first part and found $$\frac{\partial F}{\partial x} = \frac{\partial f}{\partial u}+\frac{\partial f}{\partial v}\\ \frac{\partial F}{\partial y} = \frac{\partial f}{\partial u}-\frac{\partial f}{\partial v}\\ \frac{\partial F}{\partial x}\frac{\partial F}{\partial y} = \left(\frac{\partial f}{\partial u}\right)^2-\left(\frac{\partial f}{\partial v}\right)^2$$ as required. Yet I am stuck on the second part I know $$\frac{\partial ^2F}{\partial x \partial y}=\frac{\partial}{\partial x}\left(\frac{\partial F}{\partial y}\right)$$ but here is where I am not sure is this: $$\frac{\partial}{\partial x}\left(\frac{\partial F}{\partial u}-\frac{\partial F}{\partial v}\right)=\frac{\partial F}{\partial u}\frac{\partial u}{\partial x}-\frac{\partial F}{\partial v}\frac{\partial v}{\partial x}=\frac{\partial F}{\partial y}$$ However I think this might be wrong? Any help?,I am working on this question: Now I have the first part and found $$\frac{\partial F}{\partial x} = \frac{\partial f}{\partial u}+\frac{\partial f}{\partial v}\\ \frac{\partial F}{\partial y} = \frac{\partial f}{\partial u}-\frac{\partial f}{\partial v}\\ \frac{\partial F}{\partial x}\frac{\partial F}{\partial y} = \left(\frac{\partial f}{\partial u}\right)^2-\left(\frac{\partial f}{\partial v}\right)^2$$ as required. Yet I am stuck on the second part I know $$\frac{\partial ^2F}{\partial x \partial y}=\frac{\partial}{\partial x}\left(\frac{\partial F}{\partial y}\right)$$ but here is where I am not sure is this: $$\frac{\partial}{\partial x}\left(\frac{\partial F}{\partial u}-\frac{\partial F}{\partial v}\right)=\frac{\partial F}{\partial u}\frac{\partial u}{\partial x}-\frac{\partial F}{\partial v}\frac{\partial v}{\partial x}=\frac{\partial F}{\partial y}$$ However I think this might be wrong? Any help?,,['multivariable-calculus']
39,Integral of a multivariate Gaussian distribution over quadratically separated partions,Integral of a multivariate Gaussian distribution over quadratically separated partions,,"Imagine in the space of $\Re^n$, the quadratic curve $c: f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^TW\mathbf{x} + \mathbf{w}^T\mathbf{x} + w_0$ (with $W$ being a symmetric positive definite matrix, $\mathbf{w}$ a vector and $w_0$ a constant) partitions the space into two regions $c_1 = \lbrace \mathbf{x} \in \Re^n : f(\mathbf{x}) \geq 0 \rbrace$ and $c_2 = \bar{c_1}$. In this space, the random vector $\mathbf{Y}$ is distributed with a white (spherical) multivariate normal distribution ($\mathbf{Y} \sim \mathcal{N}(\mathbf{y};\mathbf{y}_0,\sigma^2 I_n)$). What is the probability that $\mathbf{y}$ is in $c_1$ or $c_2$? In other words, is there any easy trick to calculate the following integral? \begin{equation} Pr[\mathbf{Y} \in c_i] = \int_{\mathbf{y} \in c_i} \frac{1}{(2\pi \sigma^2)^{n/2}} exp{[-\frac{1}{2\sigma}(\mathbf{y} - \mathbf{y}_0)^T (\mathbf{y} - \mathbf{y}_0)]} d\mathbf{y} \\i = 1,2 \end{equation}","Imagine in the space of $\Re^n$, the quadratic curve $c: f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^TW\mathbf{x} + \mathbf{w}^T\mathbf{x} + w_0$ (with $W$ being a symmetric positive definite matrix, $\mathbf{w}$ a vector and $w_0$ a constant) partitions the space into two regions $c_1 = \lbrace \mathbf{x} \in \Re^n : f(\mathbf{x}) \geq 0 \rbrace$ and $c_2 = \bar{c_1}$. In this space, the random vector $\mathbf{Y}$ is distributed with a white (spherical) multivariate normal distribution ($\mathbf{Y} \sim \mathcal{N}(\mathbf{y};\mathbf{y}_0,\sigma^2 I_n)$). What is the probability that $\mathbf{y}$ is in $c_1$ or $c_2$? In other words, is there any easy trick to calculate the following integral? \begin{equation} Pr[\mathbf{Y} \in c_i] = \int_{\mathbf{y} \in c_i} \frac{1}{(2\pi \sigma^2)^{n/2}} exp{[-\frac{1}{2\sigma}(\mathbf{y} - \mathbf{y}_0)^T (\mathbf{y} - \mathbf{y}_0)]} d\mathbf{y} \\i = 1,2 \end{equation}",,"['multivariable-calculus', 'probability-distributions', 'quadratic-forms', 'gaussian-integral']"
40,Evaluate the integral using spherical coordinates,Evaluate the integral using spherical coordinates,,Given the integral  $\int^{1}_{0}\int^{\sqrt{1-x^{2}}}_{0}\int^{\sqrt{1-x^{2}-y^{2}}}_{0} \dfrac{1}{x^{2}+y^{2}+z^{2}}dzdxdy$ I need to evaluate this using spherical coordinates. So far I have that $0\leq r \leq 1$ and I understand that $\theta$ is the angle made in the xy plane and has to be less than or equal to $2\pi$ and $\varphi$ is the angle made revolving around the z-axis and is less than or equal to $\pi$ however I am not sure on how to workout the limits of $\theta$ and $\varphi$ for this question.,Given the integral  $\int^{1}_{0}\int^{\sqrt{1-x^{2}}}_{0}\int^{\sqrt{1-x^{2}-y^{2}}}_{0} \dfrac{1}{x^{2}+y^{2}+z^{2}}dzdxdy$ I need to evaluate this using spherical coordinates. So far I have that $0\leq r \leq 1$ and I understand that $\theta$ is the angle made in the xy plane and has to be less than or equal to $2\pi$ and $\varphi$ is the angle made revolving around the z-axis and is less than or equal to $\pi$ however I am not sure on how to workout the limits of $\theta$ and $\varphi$ for this question.,,"['integration', 'multivariable-calculus', 'spherical-coordinates']"
41,Abstract appoach to multivariable calculus,Abstract appoach to multivariable calculus,,"(I have posted a similar question hours ago, but I deleted that and slightly modified the question and reposting it here.) So far, I have studied all undergraduate courses only except Green's theorem and Stoke's theorem. As far as I know, to prove the most general form of Stoke's theorem, one must know some sophisticated algebras. (Tensor algebra, Exterior algebra and etc) I like this approach but it seems like I have to learn so many things to achieve this. Anyway, what are the rigorous texts introducing this general form of Stoke's theorem ? Moreover, while the general form of Stoke's theorem requires lots of prerequisites, Green's theorem seems much easier to prove than Stoke's theorem in abstract context. I'm not asking for a very top generalized setting of Green's theorem. Just like one learns Cauchy's Integral formula with relation of homotopy theory in graduate school while one learns it for a simple closed curve (without knowledge of homotopy theory), I'm curious whether there is a text treating Green's theorem and related multivariable calculus in graduate-level abstract setting . Say $\alpha$ is a differentiable simple closed curve one is applying Green's theorem on. And consider a rectifiable simple closed curve $\beta$ which is homotopic to $\alpha$ rel $\{0,1\}$. Such $\beta$ can be chosen very close to $\alpha$ so I think the area difference of interiors of $\alpha$ and $\beta$ can be made arbitrarily small or zero. I think to make this assertion precise, one needs lebesgue measure to control areas. Even though a text does not cover what I just described, what is an abstract text treating multivariable calculus that you know? Thank you in advance.","(I have posted a similar question hours ago, but I deleted that and slightly modified the question and reposting it here.) So far, I have studied all undergraduate courses only except Green's theorem and Stoke's theorem. As far as I know, to prove the most general form of Stoke's theorem, one must know some sophisticated algebras. (Tensor algebra, Exterior algebra and etc) I like this approach but it seems like I have to learn so many things to achieve this. Anyway, what are the rigorous texts introducing this general form of Stoke's theorem ? Moreover, while the general form of Stoke's theorem requires lots of prerequisites, Green's theorem seems much easier to prove than Stoke's theorem in abstract context. I'm not asking for a very top generalized setting of Green's theorem. Just like one learns Cauchy's Integral formula with relation of homotopy theory in graduate school while one learns it for a simple closed curve (without knowledge of homotopy theory), I'm curious whether there is a text treating Green's theorem and related multivariable calculus in graduate-level abstract setting . Say $\alpha$ is a differentiable simple closed curve one is applying Green's theorem on. And consider a rectifiable simple closed curve $\beta$ which is homotopic to $\alpha$ rel $\{0,1\}$. Such $\beta$ can be chosen very close to $\alpha$ so I think the area difference of interiors of $\alpha$ and $\beta$ can be made arbitrarily small or zero. I think to make this assertion precise, one needs lebesgue measure to control areas. Even though a text does not cover what I just described, what is an abstract text treating multivariable calculus that you know? Thank you in advance.",,"['real-analysis', 'multivariable-calculus', 'reference-request', 'book-recommendation']"
42,Integral in terms of hypergeometric,Integral in terms of hypergeometric,,"I am considering the following integral $$\int_{-t}^{\infty} \frac{dy}{y-s} \frac{1}{y^2} \frac{1}{y^{\epsilon}} {}_2F_1(1,1,2+\epsilon, -t/y)$$ Rewriting the hypergeometric using its integral representation and making a change of variables $y=-t/u$ I obtain the integral, up to some numerical factors, $$\int_0^1 \int_0^1 dz\, du (1-uz)^{-1}u^{1+\epsilon} (1-z)^{\epsilon} (1+\frac{us}{t})^{-1}$$ My question is how to make progress with this? I have attempted partial fractions on the term $$\frac{1}{(1-uz)} \frac{1}{(1+\frac{us}{t})} = \frac{z}{\frac{s}{t}+z} \frac{1}{(1-uz)} + \frac{\frac{s}{t}}{\frac{s}{t}+z}\frac{1}{(1+\frac{us}{t})}$$ but I am not sure if this has helped me at all. Thanks for any comments!","I am considering the following integral $$\int_{-t}^{\infty} \frac{dy}{y-s} \frac{1}{y^2} \frac{1}{y^{\epsilon}} {}_2F_1(1,1,2+\epsilon, -t/y)$$ Rewriting the hypergeometric using its integral representation and making a change of variables $y=-t/u$ I obtain the integral, up to some numerical factors, $$\int_0^1 \int_0^1 dz\, du (1-uz)^{-1}u^{1+\epsilon} (1-z)^{\epsilon} (1+\frac{us}{t})^{-1}$$ My question is how to make progress with this? I have attempted partial fractions on the term $$\frac{1}{(1-uz)} \frac{1}{(1+\frac{us}{t})} = \frac{z}{\frac{s}{t}+z} \frac{1}{(1-uz)} + \frac{\frac{s}{t}}{\frac{s}{t}+z}\frac{1}{(1+\frac{us}{t})}$$ but I am not sure if this has helped me at all. Thanks for any comments!",,"['integration', 'multivariable-calculus', 'definite-integrals', 'hypergeometric-function', 'substitution']"
43,Find absolute maximum and minimum of a function with two variables,Find absolute maximum and minimum of a function with two variables,,"I have the function$ f(x,y)=(x+y-2)^2$ I have the constraints $0\leq x \leq 3$ and $x \leq y \leq 3$ The partial derivatives are $f_x =2(x+y-2)$ and $f_y =2(x+y-2)$ So the stationary points in the domain are (0,2) and (1,1) Now if I look at the second partial derivative it isn't helpful at $f_{xx} =2 f_{yy}=2$ and $f_{xy} =2$ How can I find the absolute maximum and minimum thanks","I have the function$ f(x,y)=(x+y-2)^2$ I have the constraints $0\leq x \leq 3$ and $x \leq y \leq 3$ The partial derivatives are $f_x =2(x+y-2)$ and $f_y =2(x+y-2)$ So the stationary points in the domain are (0,2) and (1,1) Now if I look at the second partial derivative it isn't helpful at $f_{xx} =2 f_{yy}=2$ and $f_{xy} =2$ How can I find the absolute maximum and minimum thanks",,"['calculus', 'multivariable-calculus', 'functions', 'optimization']"
44,Polar coordinates for double integral for $\theta$,Polar coordinates for double integral for,\theta,"To evaluate the integral $$\iint_D \sqrt{x^2+y^2}dA$$ $$D=\{(x,y)\mid0\leq(x-1)^2+y^2\leq1 \}$$ it should be best to change variables into polar coordinates to get $$\int_{\frac{-\pi}{2}}^{\frac{\pi}{2}}\int_0^{2cos\theta} r^2drd\theta$$ Every problem like the above I have seen are done by integrating with respect to $r$ then $\theta$. Most often these integrals are done over a circle centered at the origin so the limits are constants and the order of integration is changeable through Fubini's theorem. I have not yet seen one where the limits are functions of r and the integral is first done with respect to $\theta$. If I decided to switch the the order of integration of the previous integral would I end up with the following integral? $$\int_{0}^{2}\int_{-cos^{-1}\frac{r}{2}}^{cos^{-1}\frac{r}{2}} r^2 d\theta dr$$ This integral seems far worse than the previous, so I also ask how often would switching the polar bounds be beneficial? I would assume this should only be done if $\theta=f(r)$. Is that correct? Any other information or resources on integrating with respect to $\theta$ would also be appreciated.","To evaluate the integral $$\iint_D \sqrt{x^2+y^2}dA$$ $$D=\{(x,y)\mid0\leq(x-1)^2+y^2\leq1 \}$$ it should be best to change variables into polar coordinates to get $$\int_{\frac{-\pi}{2}}^{\frac{\pi}{2}}\int_0^{2cos\theta} r^2drd\theta$$ Every problem like the above I have seen are done by integrating with respect to $r$ then $\theta$. Most often these integrals are done over a circle centered at the origin so the limits are constants and the order of integration is changeable through Fubini's theorem. I have not yet seen one where the limits are functions of r and the integral is first done with respect to $\theta$. If I decided to switch the the order of integration of the previous integral would I end up with the following integral? $$\int_{0}^{2}\int_{-cos^{-1}\frac{r}{2}}^{cos^{-1}\frac{r}{2}} r^2 d\theta dr$$ This integral seems far worse than the previous, so I also ask how often would switching the polar bounds be beneficial? I would assume this should only be done if $\theta=f(r)$. Is that correct? Any other information or resources on integrating with respect to $\theta$ would also be appreciated.",,"['integration', 'multivariable-calculus', 'definite-integrals', 'polar-coordinates']"
45,The definition of strong continuity via joint continuity,The definition of strong continuity via joint continuity,,"A semigroup $S(t)$ on a Banach space $E$ is a family of bounded linear operators $\{S(t)\}_{t\ge 0}$ with the property that $S(t)S(s)=S(t+s)$ for any $s,t\ge 0$ and that $S(0)=I$. A semigroup is furthermore called strongly continuous if the map $(x,t)\mapsto S(t)x$ is continuous. I was told that this is equivalent of saying $t\to S(t)x$ is continuous for every $x$. How can I see the equivalence of two ways of defining strong continuity? Could anyone expand what $(x,t)\mapsto S(t)x$ is continuous really mean? Can one show this via the usual strategy of 2-sided continuity? How could this be the same as saying  $t\to S(t)x$ is continuous for every $x$? Appreciate for any helps.","A semigroup $S(t)$ on a Banach space $E$ is a family of bounded linear operators $\{S(t)\}_{t\ge 0}$ with the property that $S(t)S(s)=S(t+s)$ for any $s,t\ge 0$ and that $S(0)=I$. A semigroup is furthermore called strongly continuous if the map $(x,t)\mapsto S(t)x$ is continuous. I was told that this is equivalent of saying $t\to S(t)x$ is continuous for every $x$. How can I see the equivalence of two ways of defining strong continuity? Could anyone expand what $(x,t)\mapsto S(t)x$ is continuous really mean? Can one show this via the usual strategy of 2-sided continuity? How could this be the same as saying  $t\to S(t)x$ is continuous for every $x$? Appreciate for any helps.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'semigroup-of-operators']"
46,Sufficient conditions for integration by parts in higher dimensions,Sufficient conditions for integration by parts in higher dimensions,,"If $\Omega\subset {\mathbb R}^n$ is a bounded open set with $C^1$ boundary and $\nu$ denotes the outward unit normal to $\partial \Omega$, then the following formula holds for every pair of $C^1$ functions $u$ and $v$: $$\int_\Omega v \frac{\partial u}{\partial x_i} = \int_{\partial \Omega}  uv\,  \nu_i - \int_\Omega u \frac{\partial v}{\partial x_i}  $$ ($\nu_i$ denotes the $i$-th component of the vector $\nu$; moreover the functions $u$, $v$ and their partial derivatives are assumed to have continuous extensions up to the boundary). I am hoping it would also be true if $v$ were instead only bounded and $C^1$ on $\Omega$. References are appreciated. I should mentioned that this has been ask before here: Find a rigorous reference that prove the following integration by parts formula in higher dimension? .  I cannot access the reference provided so I'm asking the question again.","If $\Omega\subset {\mathbb R}^n$ is a bounded open set with $C^1$ boundary and $\nu$ denotes the outward unit normal to $\partial \Omega$, then the following formula holds for every pair of $C^1$ functions $u$ and $v$: $$\int_\Omega v \frac{\partial u}{\partial x_i} = \int_{\partial \Omega}  uv\,  \nu_i - \int_\Omega u \frac{\partial v}{\partial x_i}  $$ ($\nu_i$ denotes the $i$-th component of the vector $\nu$; moreover the functions $u$, $v$ and their partial derivatives are assumed to have continuous extensions up to the boundary). I am hoping it would also be true if $v$ were instead only bounded and $C^1$ on $\Omega$. References are appreciated. I should mentioned that this has been ask before here: Find a rigorous reference that prove the following integration by parts formula in higher dimension? .  I cannot access the reference provided so I'm asking the question again.",,"['integration', 'multivariable-calculus', 'reference-request']"
47,Solving non-linear equations in a chosen subspace,Solving non-linear equations in a chosen subspace,,"I'm trying to find the root $\mathbf{f(x)=0}$ to the following sets of equations $$ f_1(x,y,z) = x^\prime -  \frac{x}{\sqrt{x^2+y^2+z^2}} = 0 \\ f_2(x,y,z) = y^\prime - \frac{y}{\sqrt{x^2+y^2+z^2}} = 0 \\ f_3(x,y,z) = z^\prime - \frac{z}{\sqrt{x^2+y^2+z^2}} = 0 $$ where $x^\prime,y^\prime,z^\prime$ are known. When I try to use the Newton method, i.e $\mathbf{x}^k = \mathbf{x}^{k-1} - J^{-1} \mathbf{f(x^{k-1})}$ it fails since the jacobian $J$ is not invertible (no unique solution). The root $(x,y,z)$ I want to find lies in a plane $ax+by+cz=d$. How do I limit the search to this subspace of $\mathbf{R}^3$?","I'm trying to find the root $\mathbf{f(x)=0}$ to the following sets of equations $$ f_1(x,y,z) = x^\prime -  \frac{x}{\sqrt{x^2+y^2+z^2}} = 0 \\ f_2(x,y,z) = y^\prime - \frac{y}{\sqrt{x^2+y^2+z^2}} = 0 \\ f_3(x,y,z) = z^\prime - \frac{z}{\sqrt{x^2+y^2+z^2}} = 0 $$ where $x^\prime,y^\prime,z^\prime$ are known. When I try to use the Newton method, i.e $\mathbf{x}^k = \mathbf{x}^{k-1} - J^{-1} \mathbf{f(x^{k-1})}$ it fails since the jacobian $J$ is not invertible (no unique solution). The root $(x,y,z)$ I want to find lies in a plane $ax+by+cz=d$. How do I limit the search to this subspace of $\mathbf{R}^3$?",,"['multivariable-calculus', 'differential-geometry', 'nonlinear-optimization', 'nonlinear-system', 'newton-raphson']"
48,Axysimmetric Poisson equation solution,Axysimmetric Poisson equation solution,,"I'm struggling to find the solution to the follow Poisson problem in spherical coordinates: $$ \Delta\, f\left(r,\theta\right) = \sum_{l=1}^{4}k_{l} \left(r\right) P_{l}^{1}\left( \theta \right)$$ In the above equation $P_{l}^{1}\left( \theta \right)$ are the Associated Legendre polynomials of the first kind and the functions $k_{l}\left(r\right)$ are polynomials decreasing as $\frac{1}{r^{n}}$ with $n>7$. Note that the problem is axysimmetric since the function $f$ and the right hand side do not depend the angle $\phi$, nevertheless the problem is $3$D. The boundary conditions to the equations are: $$ \begin{cases}f\left(1,\theta \right)&= -\sin\left( \theta \right) \\ f\left(\infty,\theta \right) & = 0 \end{cases} $$ I'm quite clueless on how to proceed, do you have any suggestion? maybe a particular form of $\ f\left(r,\theta\right)\;$ is helpful to find the solution? (EDIT: this problem arise from the solution of the stokes equation with a body force where a sphere is free to move under the effect of the above mentioned body force field; if you feel that more details are required to the question I will gladly add them.)","I'm struggling to find the solution to the follow Poisson problem in spherical coordinates: $$ \Delta\, f\left(r,\theta\right) = \sum_{l=1}^{4}k_{l} \left(r\right) P_{l}^{1}\left( \theta \right)$$ In the above equation $P_{l}^{1}\left( \theta \right)$ are the Associated Legendre polynomials of the first kind and the functions $k_{l}\left(r\right)$ are polynomials decreasing as $\frac{1}{r^{n}}$ with $n>7$. Note that the problem is axysimmetric since the function $f$ and the right hand side do not depend the angle $\phi$, nevertheless the problem is $3$D. The boundary conditions to the equations are: $$ \begin{cases}f\left(1,\theta \right)&= -\sin\left( \theta \right) \\ f\left(\infty,\theta \right) & = 0 \end{cases} $$ I'm quite clueless on how to proceed, do you have any suggestion? maybe a particular form of $\ f\left(r,\theta\right)\;$ is helpful to find the solution? (EDIT: this problem arise from the solution of the stokes equation with a body force where a sphere is free to move under the effect of the above mentioned body force field; if you feel that more details are required to the question I will gladly add them.)",,"['multivariable-calculus', 'legendre-polynomials', 'poissons-equation']"
49,Volume of Region Paraboloids,Volume of Region Paraboloids,,"How do I find the volume of the solid region which is bounded by $z=2x^2+2y^2$ and $z=3-x^2-y^2$? So I first realized that these two functions are paraboloids and I have to find the volume of their intersection. But, I'm not quite sure how to do so. Can someone please help me out through each step explaining me so that I understand? UPDATE: So I was trying it out by myself and I set up the volume integral as: $$\int^{2\pi}_0\int^1_0\int^{3-r^2}_{2r^2}{rdzdrd\theta}$$ Am I right?","How do I find the volume of the solid region which is bounded by $z=2x^2+2y^2$ and $z=3-x^2-y^2$? So I first realized that these two functions are paraboloids and I have to find the volume of their intersection. But, I'm not quite sure how to do so. Can someone please help me out through each step explaining me so that I understand? UPDATE: So I was trying it out by myself and I set up the volume integral as: $$\int^{2\pi}_0\int^1_0\int^{3-r^2}_{2r^2}{rdzdrd\theta}$$ Am I right?",,['multivariable-calculus']
50,Condition for Continuity (two variable),Condition for Continuity (two variable),,"I came across the following question while studying for quals.  This one is from a previous qualifier.  I have a few ideas (which I'll mention below), but am stuck on how to complete the problem.  Any help or guidance you may be able to offer would be very much appreciated.  Here is the question as it appears on the qual: Define:   $$   f(x,y) =    \begin{cases}     {\Large\frac{|x|^m|y|^n}{|x|^p+|y|^q}},\; (x,y) \neq (0,0) \\      \quad\quad 0,\;\quad\quad (x,y) = (0,0)   \end{cases}   \quad\text{where } m, n, p, q >0 $$ Find some necessary conditions on $m,n,p,q$ such that $f(x,y)$ is continuous at $(0,0)$; find some sufficient conditions on $m,n,p,q$ such that $f(x,y)$ is continuous at $(x,y)=(0,0)$. Find a necessary and sufficient condition on $m,n,p,q$ such that $f(x,y)$ is continuous at $(0,0)$. My thoughts on this problem were to break it up into cases.  At first, I played around with $m,n,p,q$ only being in $\mathbb{N}$, hoping this would provide me with some insight. I think I can say that if $m,n,p,q$ are all equal, that should work, as it should if either $m\geq p$ or $n\geq q$, or both.  This seems to work even for fractional values.  I feel like there is much more I am missing. On the other hand, if I can find conditions on $m,n,p,q$ such that $f$ becomes differentiable, then that should also force $f$ to be continuous. Without forcing differentiability, I am guessing there will be many cases to consider (i.e., when $x,y$ are positive, negative, etc.). I am just a bit lost on how to proceed.  I am having trouble ironing out the details.  Thank you very much for any help you can offer.","I came across the following question while studying for quals.  This one is from a previous qualifier.  I have a few ideas (which I'll mention below), but am stuck on how to complete the problem.  Any help or guidance you may be able to offer would be very much appreciated.  Here is the question as it appears on the qual: Define:   $$   f(x,y) =    \begin{cases}     {\Large\frac{|x|^m|y|^n}{|x|^p+|y|^q}},\; (x,y) \neq (0,0) \\      \quad\quad 0,\;\quad\quad (x,y) = (0,0)   \end{cases}   \quad\text{where } m, n, p, q >0 $$ Find some necessary conditions on $m,n,p,q$ such that $f(x,y)$ is continuous at $(0,0)$; find some sufficient conditions on $m,n,p,q$ such that $f(x,y)$ is continuous at $(x,y)=(0,0)$. Find a necessary and sufficient condition on $m,n,p,q$ such that $f(x,y)$ is continuous at $(0,0)$. My thoughts on this problem were to break it up into cases.  At first, I played around with $m,n,p,q$ only being in $\mathbb{N}$, hoping this would provide me with some insight. I think I can say that if $m,n,p,q$ are all equal, that should work, as it should if either $m\geq p$ or $n\geq q$, or both.  This seems to work even for fractional values.  I feel like there is much more I am missing. On the other hand, if I can find conditions on $m,n,p,q$ such that $f$ becomes differentiable, then that should also force $f$ to be continuous. Without forcing differentiability, I am guessing there will be many cases to consider (i.e., when $x,y$ are positive, negative, etc.). I am just a bit lost on how to proceed.  I am having trouble ironing out the details.  Thank you very much for any help you can offer.",,['real-analysis']
51,"Construct Curvilinear Coordinates from ""Tangent Basis Field""","Construct Curvilinear Coordinates from ""Tangent Basis Field""",,"Let say that you are on a open, simple connected subset of the Euclidian plane (2 dimensional) and you have a second order tensor field defined over it. At every point we have a second order tensor which is real, positive, and symmetric, thus it has at every point two eigenvectors which are orthogonal. So in some sense this defines at every point a tangent basis. If the tensor field is smooth, then the variation of this basis from point to point is also smooth. Let us suppose that is as smooth as we may need. My question is the following: Can I construct some curvilinear coordinates that has these eigenvectors as basis in their tangent space? If it can be done, under what other assumptions or conditions? If it can only be done locally, then it is good enough.","Let say that you are on a open, simple connected subset of the Euclidian plane (2 dimensional) and you have a second order tensor field defined over it. At every point we have a second order tensor which is real, positive, and symmetric, thus it has at every point two eigenvectors which are orthogonal. So in some sense this defines at every point a tangent basis. If the tensor field is smooth, then the variation of this basis from point to point is also smooth. Let us suppose that is as smooth as we may need. My question is the following: Can I construct some curvilinear coordinates that has these eigenvectors as basis in their tangent space? If it can be done, under what other assumptions or conditions? If it can only be done locally, then it is good enough.",,"['multivariable-calculus', 'differential-geometry']"
52,Exercise from Pugh's Real Analysis Regarding Zero Derivative on Open Subsets of $\mathbb{R}^m$,Exercise from Pugh's Real Analysis Regarding Zero Derivative on Open Subsets of,\mathbb{R}^m,"Assume that $U$ is a connected open subset of $\mathbb{R}^n$ and $f:U\to\mathbb{R}^m$ is differentiable everywhere on $U$. If $(Df)_p=0$ for all $p\in U$, show that $f$ is constant. I immediately thought that this would be equal to showing that if $\text{grad}(f)$ is $0$ everywhere on an open subset of $\mathbb{R}^m$, then $f$ is constant on that subset, thus enabling us to use the generalized MVT: for any $[a,b]$ contained in $U$ we have $|f(p)-f(q)|\le\max\{\text{grad}(f)\}|p-q|$, and since $\text{grad}(f)=0$ everywhere on $U$, we get $f(p)=f(q)$ for all $p,q\in U$. Is this sufficient? Pugh remarks that ""the enjoyable open and closed argument is left to you as Exercise 20"", suggesting that more is required. :\ Any help is appreciated","Assume that $U$ is a connected open subset of $\mathbb{R}^n$ and $f:U\to\mathbb{R}^m$ is differentiable everywhere on $U$. If $(Df)_p=0$ for all $p\in U$, show that $f$ is constant. I immediately thought that this would be equal to showing that if $\text{grad}(f)$ is $0$ everywhere on an open subset of $\mathbb{R}^m$, then $f$ is constant on that subset, thus enabling us to use the generalized MVT: for any $[a,b]$ contained in $U$ we have $|f(p)-f(q)|\le\max\{\text{grad}(f)\}|p-q|$, and since $\text{grad}(f)=0$ everywhere on $U$, we get $f(p)=f(q)$ for all $p,q\in U$. Is this sufficient? Pugh remarks that ""the enjoyable open and closed argument is left to you as Exercise 20"", suggesting that more is required. :\ Any help is appreciated",,"['real-analysis', 'multivariable-calculus']"
53,"Angle form, 1-form, proof verification.","Angle form, 1-form, proof verification.",,"Check that the $1$-form $d\,\text{arg}$ in $\mathbb{R}^2 - \{0\}$ is just the form$${{-y}\over{x^2 + y^2}}\,dx + {{x}\over{x^2 + y^2}}\,dy.$$ My solution is as follows. Observe that we can define $\text{arg}\,z= \tan^{-1}(y/x)$ locally. This definition only works if $\text{arg}\,z$ is congruent to $\theta$ modulo $2\pi$ for some $\theta \in (-\pi/2,\pi/2)$. If $\text{arg}\,z$ is outside this range, we just add $\pi$ to it, leaving the exterior derivative unchanged. Taking this to be a $0$-form, we calculate its exterior derivative, as follows.$$d\left(\tan^{-1}\left({y\over{x}}\right)\right) = {{\partial f}\over{\partial x}}dx + {{\partial f}\over{\partial y}}dy = -{y\over{x^2 + y^2}}dx + {x\over{x^2 + y^2}}dy.$$In neighborhoods of $\pm\pi/2$, we instead define $\text{arg}\,z = \cot^{-1}(x/y)$, seeing that it is not clear that our earlier definition of $\text{arg}\,z$ is even continuous near $\pm\pi/2$. $\cot^{-1}(x/y)$ gets us the same exterior derivative as $\tan^{-1}(y/x)$, and it is well-defined, so we are done. My question is, is what I have valid? And is there a cleaner/simpler way of doing the problem/alternate perspective I'm missing? Much thanks in advance.","Check that the $1$-form $d\,\text{arg}$ in $\mathbb{R}^2 - \{0\}$ is just the form$${{-y}\over{x^2 + y^2}}\,dx + {{x}\over{x^2 + y^2}}\,dy.$$ My solution is as follows. Observe that we can define $\text{arg}\,z= \tan^{-1}(y/x)$ locally. This definition only works if $\text{arg}\,z$ is congruent to $\theta$ modulo $2\pi$ for some $\theta \in (-\pi/2,\pi/2)$. If $\text{arg}\,z$ is outside this range, we just add $\pi$ to it, leaving the exterior derivative unchanged. Taking this to be a $0$-form, we calculate its exterior derivative, as follows.$$d\left(\tan^{-1}\left({y\over{x}}\right)\right) = {{\partial f}\over{\partial x}}dx + {{\partial f}\over{\partial y}}dy = -{y\over{x^2 + y^2}}dx + {x\over{x^2 + y^2}}dy.$$In neighborhoods of $\pm\pi/2$, we instead define $\text{arg}\,z = \cot^{-1}(x/y)$, seeing that it is not clear that our earlier definition of $\text{arg}\,z$ is even continuous near $\pm\pi/2$. $\cot^{-1}(x/y)$ gets us the same exterior derivative as $\tan^{-1}(y/x)$, and it is well-defined, so we are done. My question is, is what I have valid? And is there a cleaner/simpler way of doing the problem/alternate perspective I'm missing? Much thanks in advance.",,"['real-analysis', 'general-topology']"
54,How can I show that this function is smooth?,How can I show that this function is smooth?,,"I got an assignment which I just can't find the right way to solve and I hope that someone could help me out here. It goes like this: Let $\Omega\in R^n$ be a domain and $b_1,...,b_n:\Omega\to R^n$ smooth mappings (or function, don't know the correct translation into english), so that for every $x\in \Omega$ the vectors $b_1(x),...,b_n(x)$ are linearly independent. Let $c_1,...,c_n:\Omega\to R$ be mappings (or functions). Show that the function $F(x):=c_1b_1(x)+...+c_n(x)b_n(x)$ is smooth when $c_1,...,c_n$ are smooth.","I got an assignment which I just can't find the right way to solve and I hope that someone could help me out here. It goes like this: Let $\Omega\in R^n$ be a domain and $b_1,...,b_n:\Omega\to R^n$ smooth mappings (or function, don't know the correct translation into english), so that for every $x\in \Omega$ the vectors $b_1(x),...,b_n(x)$ are linearly independent. Let $c_1,...,c_n:\Omega\to R$ be mappings (or functions). Show that the function $F(x):=c_1b_1(x)+...+c_n(x)b_n(x)$ is smooth when $c_1,...,c_n$ are smooth.",,"['multivariable-calculus', 'derivatives']"
55,Can anyone tell me if this is correct?,Can anyone tell me if this is correct?,,"Suppose that the temperature of a metal plate is given by $T(x; y) = x^2 +2x+y^2$, for points $(x, y)$ on the elliptical plate defined by $x^2 + 4y^2 <= 24$. Find the maximum and minimum temperatures on the plate. This is what i have done so far. Finding critical point: $T(x)=2x+2$, $T(y)=2y$. Equating to $0$, $x=-1$, $y=0$. Critical point is $(-1,0)$ and is a minimum. On the boundary,   $  x^2 + 4y^2 = 24$ $g(x,y)=x^2 + 4y^2$ $g(x)=2x, g(y)=8y$ $2x+2=A2x$---------(1) $2y  =A8y$---------(2) $x^2+4y^2=24$------(3) When solving from equation 1 and 3 im getting $ x=-1,y=|(23/4)^{0.5}|,$ and $x=|24^{0.5}|, y=0$ and when from eqn 2 and 3 im getting $x=|24^{0.5}|,y=0, A= 0.25 , x=-4/3, y=|(50/9)^{0.5}|$. Is this correct? am getting different values when using equation $1$ and $2$.","Suppose that the temperature of a metal plate is given by $T(x; y) = x^2 +2x+y^2$, for points $(x, y)$ on the elliptical plate defined by $x^2 + 4y^2 <= 24$. Find the maximum and minimum temperatures on the plate. This is what i have done so far. Finding critical point: $T(x)=2x+2$, $T(y)=2y$. Equating to $0$, $x=-1$, $y=0$. Critical point is $(-1,0)$ and is a minimum. On the boundary,   $  x^2 + 4y^2 = 24$ $g(x,y)=x^2 + 4y^2$ $g(x)=2x, g(y)=8y$ $2x+2=A2x$---------(1) $2y  =A8y$---------(2) $x^2+4y^2=24$------(3) When solving from equation 1 and 3 im getting $ x=-1,y=|(23/4)^{0.5}|,$ and $x=|24^{0.5}|, y=0$ and when from eqn 2 and 3 im getting $x=|24^{0.5}|,y=0, A= 0.25 , x=-4/3, y=|(50/9)^{0.5}|$. Is this correct? am getting different values when using equation $1$ and $2$.",,"['multivariable-calculus', 'lagrange-multiplier']"
56,Application of Cauchy-Schwarz with Sobolev norms,Application of Cauchy-Schwarz with Sobolev norms,,"I'm working through the problems in the initial value formulation chapter in Wald's General Relativity . A short summary of the problem. I have to show that $$\sup_{x\in A}|f(x)|\le C||f||_{A,k}$$ where $A\subset \mathbb{R}^n$ satisfies the uniform interior cone condition, $C$ is a constant, $k>n/2$ is an integer and $||.||_{A,k}$ is the Sobolev norm. Let $Q$ denote the solid closed cone in $\mathbb{R}^n$ of height $H$ and solid angle $\Omega$, with vertex at the origin. Let $\psi:\mathbb{R}\longrightarrow\mathbb{R}$ be a $C^\infty$ function with $\psi(r)=1$ for $r<H/3$ and $\psi(r)=0$ for $r >2H/3$. I have shown that $$\tag{1}f(0)=C_1\int_{Q\subset\mathbb{R}^n} r^{k-n}\frac{d^k}{dr^k}(\psi f)\,dx$$ The hint says to show $$\tag{2}|f(0)|\le C||f||_{Q,k}$$ using Cauchy-Schwarz. From this I can obtain the final result using the uniform interior cone condition. Any help would be greatly appreciated. EDIT: I should be using Cauchy-Schwarz with the $L^2$ inner product, not the Sobolev one. I can write $$f(0)=C_1\langle r^{k-n},\partial_r^k(\psi f)\rangle_{L^2}$$ and thus $$\tag{3}|f(0)|\le C_1 ||r^{k-n}||_{L^2}\cdot ||\partial^k_r(\psi f)||_{L^2}$$ For the first factor I get $$||r^{k-n}||_{L^2}=\int_Q |r^{k-n}|^2\,dx=\int_Q r^{2k-2n}\,dx=\int_Q r^{2k-2n}r^{n-1}\,drd\Omega=C_2$$ a constant, iff $k>n/2$. Putting this into (3), I get $$\tag{4}|f(0)|\le C_3||\partial^k_r(\psi f)||_{L^2}$$ I can obtain (2) by showing that $$||\partial^k_r(\psi f)||_{L^2}\le C_4 ||f||_{Q,k}$$ Intuitively, this is clear, because $||f||_{Q,k}$ contains ""more terms"" which are all positive. But my problem then lies with the equals in $\le$. I don't see how it could be anything other than strictly larger, but since I'm using my gut here, I could very well be wrong. EDIT 2: I guess the equality could hold based on the precise form of $\psi$ perhaps, but I'm still not sure.","I'm working through the problems in the initial value formulation chapter in Wald's General Relativity . A short summary of the problem. I have to show that $$\sup_{x\in A}|f(x)|\le C||f||_{A,k}$$ where $A\subset \mathbb{R}^n$ satisfies the uniform interior cone condition, $C$ is a constant, $k>n/2$ is an integer and $||.||_{A,k}$ is the Sobolev norm. Let $Q$ denote the solid closed cone in $\mathbb{R}^n$ of height $H$ and solid angle $\Omega$, with vertex at the origin. Let $\psi:\mathbb{R}\longrightarrow\mathbb{R}$ be a $C^\infty$ function with $\psi(r)=1$ for $r<H/3$ and $\psi(r)=0$ for $r >2H/3$. I have shown that $$\tag{1}f(0)=C_1\int_{Q\subset\mathbb{R}^n} r^{k-n}\frac{d^k}{dr^k}(\psi f)\,dx$$ The hint says to show $$\tag{2}|f(0)|\le C||f||_{Q,k}$$ using Cauchy-Schwarz. From this I can obtain the final result using the uniform interior cone condition. Any help would be greatly appreciated. EDIT: I should be using Cauchy-Schwarz with the $L^2$ inner product, not the Sobolev one. I can write $$f(0)=C_1\langle r^{k-n},\partial_r^k(\psi f)\rangle_{L^2}$$ and thus $$\tag{3}|f(0)|\le C_1 ||r^{k-n}||_{L^2}\cdot ||\partial^k_r(\psi f)||_{L^2}$$ For the first factor I get $$||r^{k-n}||_{L^2}=\int_Q |r^{k-n}|^2\,dx=\int_Q r^{2k-2n}\,dx=\int_Q r^{2k-2n}r^{n-1}\,drd\Omega=C_2$$ a constant, iff $k>n/2$. Putting this into (3), I get $$\tag{4}|f(0)|\le C_3||\partial^k_r(\psi f)||_{L^2}$$ I can obtain (2) by showing that $$||\partial^k_r(\psi f)||_{L^2}\le C_4 ||f||_{Q,k}$$ Intuitively, this is clear, because $||f||_{Q,k}$ contains ""more terms"" which are all positive. But my problem then lies with the equals in $\le$. I don't see how it could be anything other than strictly larger, but since I'm using my gut here, I could very well be wrong. EDIT 2: I guess the equality could hold based on the precise form of $\psi$ perhaps, but I'm still not sure.",,"['real-analysis', 'multivariable-calculus', 'inequality', 'partial-differential-equations', 'inner-products']"
57,Maximum / Minimum Question with 3 Variables?,Maximum / Minimum Question with 3 Variables?,,"I seem to be stuck in this problem, would need your help! Question: Assume I have : 147 of x, 174 of y, 238 of z A different amount of x, y and z are being used to produce 3 different products A, B and C, satisfying the 3 equations below. A = 10x + 5y + 3z, B = 3x + 10y + 5z, C = 5x + 3y + 10z The question is how do I find out which combination of A, B and C I should produce in order to maximize the usage of the current resources and produce the maximum amount of A, B and C combined. (A + B + C will be maximum.) Thanks for the help!","I seem to be stuck in this problem, would need your help! Question: Assume I have : 147 of x, 174 of y, 238 of z A different amount of x, y and z are being used to produce 3 different products A, B and C, satisfying the 3 equations below. A = 10x + 5y + 3z, B = 3x + 10y + 5z, C = 5x + 3y + 10z The question is how do I find out which combination of A, B and C I should produce in order to maximize the usage of the current resources and produce the maximum amount of A, B and C combined. (A + B + C will be maximum.) Thanks for the help!",,"['multivariable-calculus', 'linear-programming']"
58,Multivariable Calculus or Differential Geometry (Analysis on Manifolds) after single variable calculus,Multivariable Calculus or Differential Geometry (Analysis on Manifolds) after single variable calculus,,"Background: Applied Mathematics program, finished with single variable calculus, and in parallel with basic analysis. (Not knowledge of multivariable calculus yet) Please feel free to recommend multivariable version in the favor of Peter Lax's ? It is good taste of Peter Lax's new Calculus text, Calculus with Application ,for single variable, which balances between theory and application, and more or less a more modern writing with the same spirit to Courant's Introduction to Calculus and Analysis . Note that I plan to read Zorich's Mathematical Analysis as second reading for Calculus, which contains rigorous development of multivariable calculus. So here the first reading is welcomed, or some comment to convince the level of Zorich is enough for first reading just after single variable calculus. (Since in some multivariable calculus/analysis textbooks, calculation training is not enough, for example, Duistermaat's Multidimensional Real Analysis does not provide training such as how to compute double integrals, volumes, intuition of directional derivatives etc...) Or, Is it ok to directly go to Differential Geometry or analysis on manifolds ? (Although I am able to compute some double integrals, extreme values, tangent planes according to formula tables, but let's still assume skills more or less zero level, will it provide these computational training with direct entering of differential geometry, e.g. by do Carmo, or analysis of manifolds, e.g. by Munkres or Spivak) ?","Background: Applied Mathematics program, finished with single variable calculus, and in parallel with basic analysis. (Not knowledge of multivariable calculus yet) Please feel free to recommend multivariable version in the favor of Peter Lax's ? It is good taste of Peter Lax's new Calculus text, Calculus with Application ,for single variable, which balances between theory and application, and more or less a more modern writing with the same spirit to Courant's Introduction to Calculus and Analysis . Note that I plan to read Zorich's Mathematical Analysis as second reading for Calculus, which contains rigorous development of multivariable calculus. So here the first reading is welcomed, or some comment to convince the level of Zorich is enough for first reading just after single variable calculus. (Since in some multivariable calculus/analysis textbooks, calculation training is not enough, for example, Duistermaat's Multidimensional Real Analysis does not provide training such as how to compute double integrals, volumes, intuition of directional derivatives etc...) Or, Is it ok to directly go to Differential Geometry or analysis on manifolds ? (Although I am able to compute some double integrals, extreme values, tangent planes according to formula tables, but let's still assume skills more or less zero level, will it provide these computational training with direct entering of differential geometry, e.g. by do Carmo, or analysis of manifolds, e.g. by Munkres or Spivak) ?",,"['multivariable-calculus', 'reference-request', 'differential-geometry', 'book-recommendation']"
59,Uniqueness of solution to $u_{t} - \Delta u + |\nabla u|^{2} = 0$,Uniqueness of solution to,u_{t} - \Delta u + |\nabla u|^{2} = 0,"The problem I am working on is as follows: Let $\Omega$ be a connected bounded domain in $\mathbb{R}^{n}$ with smooth boundary and let $f, g: \mathbb{R}^{n} \rightarrow R$ be smooth. Show that there is at most one smooth solution $u(x, t)$ such that \begin{align*} \begin{cases} u_{t} - \Delta u + |\nabla u|^{2} = 0 & \text{ in } \Omega \times (0, \infty)\\ u(x, t) = g(x) & \text{ on } \partial \Omega \times (0, \infty)\\ u(x, 0) = f(x) & \text{ in } \Omega. \end{cases} \end{align*} This seems like an energy method type problem. Suppose $u_{1}, u_{2}$ are 2 smooth solutions of the above equation. Let $w := u_{1} - u_{2}$. Then \begin{align*} \begin{cases} w_{t} - \Delta w + \nabla w \cdot \nabla(u_{1} + u_{2})= 0 & \text{ in } \Omega \times (0, \infty)\\ w(x, t) = 0 & \text{ on } \partial \Omega \times (0, \infty)\\ w(x, 0) = 0 & \text{ in } \Omega. \end{cases} \end{align*} My question is: how can I define an $E(t)$ such that $E(t) \leq 0$ for all time $t \geq 0$? As a first attempt, I tried defining $E(t) := \frac{1}{2}\int_{\Omega} w^{2}\, dx$, but I get $$E'(t) = \int_{\Omega}w(\Delta w - \nabla w \cdot \nabla (u_{1} + u_{2}))\, dx.$$ However, I am not sure how to work with the $\nabla (u_{1} + u_{2})$ term. Any hints on how to get around this?","The problem I am working on is as follows: Let $\Omega$ be a connected bounded domain in $\mathbb{R}^{n}$ with smooth boundary and let $f, g: \mathbb{R}^{n} \rightarrow R$ be smooth. Show that there is at most one smooth solution $u(x, t)$ such that \begin{align*} \begin{cases} u_{t} - \Delta u + |\nabla u|^{2} = 0 & \text{ in } \Omega \times (0, \infty)\\ u(x, t) = g(x) & \text{ on } \partial \Omega \times (0, \infty)\\ u(x, 0) = f(x) & \text{ in } \Omega. \end{cases} \end{align*} This seems like an energy method type problem. Suppose $u_{1}, u_{2}$ are 2 smooth solutions of the above equation. Let $w := u_{1} - u_{2}$. Then \begin{align*} \begin{cases} w_{t} - \Delta w + \nabla w \cdot \nabla(u_{1} + u_{2})= 0 & \text{ in } \Omega \times (0, \infty)\\ w(x, t) = 0 & \text{ on } \partial \Omega \times (0, \infty)\\ w(x, 0) = 0 & \text{ in } \Omega. \end{cases} \end{align*} My question is: how can I define an $E(t)$ such that $E(t) \leq 0$ for all time $t \geq 0$? As a first attempt, I tried defining $E(t) := \frac{1}{2}\int_{\Omega} w^{2}\, dx$, but I get $$E'(t) = \int_{\Omega}w(\Delta w - \nabla w \cdot \nabla (u_{1} + u_{2}))\, dx.$$ However, I am not sure how to work with the $\nabla (u_{1} + u_{2})$ term. Any hints on how to get around this?",,"['multivariable-calculus', 'partial-differential-equations']"
60,Solving system of first-order PDEs with Frobenius theorem,Solving system of first-order PDEs with Frobenius theorem,,"I've been stuck trying to solve this system: $$\ \frac{\partial u}{\partial x} = \frac{-2xy^2}{u} + 3y $$ $$\ \frac{\partial u}{\partial y} = \frac{-2x^2y}{u} + 3x $$ Which must satisfy $ \ u(0,0) = 1$ and the solution lies on $u(x,y) = z$ I've started by choosing vector fields $ \ X = \ \left(1,0,\dfrac{\partial u}{\partial x}\right)$  and $ \ Y = \ \left(0,1,\dfrac{\partial u}{\partial y}\right)$ which by the Frobenius theorem must satisfy $\left[X,Y\right] =0$ where the brackets indicate the jacobi brackets . Then to work out the flow of the X vector field: $ \dfrac{dx}{dt} = 1$ =>  $ x = t $ $ \dfrac{dy}{dt} = 0$ =>  $ y = 0 $ $ \dfrac{dz}{dt} = \dfrac{-2xy^2}{z} + 3y$ where at $t=0, z= 1$ Now I am stuck integrating this last equation. I believe there is a simple trick such as making a substitution. Don't worry, this isn't homework","I've been stuck trying to solve this system: $$\ \frac{\partial u}{\partial x} = \frac{-2xy^2}{u} + 3y $$ $$\ \frac{\partial u}{\partial y} = \frac{-2x^2y}{u} + 3x $$ Which must satisfy $ \ u(0,0) = 1$ and the solution lies on $u(x,y) = z$ I've started by choosing vector fields $ \ X = \ \left(1,0,\dfrac{\partial u}{\partial x}\right)$  and $ \ Y = \ \left(0,1,\dfrac{\partial u}{\partial y}\right)$ which by the Frobenius theorem must satisfy $\left[X,Y\right] =0$ where the brackets indicate the jacobi brackets . Then to work out the flow of the X vector field: $ \dfrac{dx}{dt} = 1$ =>  $ x = t $ $ \dfrac{dy}{dt} = 0$ =>  $ y = 0 $ $ \dfrac{dz}{dt} = \dfrac{-2xy^2}{z} + 3y$ where at $t=0, z= 1$ Now I am stuck integrating this last equation. I believe there is a simple trick such as making a substitution. Don't worry, this isn't homework",,"['integration', 'multivariable-calculus', 'partial-differential-equations', 'manifolds']"
61,Unable to evaluate this surface integral,Unable to evaluate this surface integral,,"Question is to find the surface integral $$\iint p(x^4 + y^4 + z^4) \, \mathbb dS.$$ The given surface is $$ \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1,$$ and $p$ is the length of the perpendicular from the origin to the tangent plane at $(x, y, z)$. I have attempted solving the question as follows: Parametrizing the surface: $$ x = (a\cos x)\cos y,$$ $$ y = (b \cos x) \sin y ,$$ $$z = c \sin x $$ I am not able to calculate $p$ correctly. This is an example problem and the value of $p$ mentioned in the book is $$p = \frac{1}{ \sqrt{\frac{\cos^2x + \cos^2y}{ a^2} + \frac{\cos^2x + \cos^2y}{ b^2} + \frac{\sin^2x}{c^2}}}.$$ I know that the normal vector to the tangent plane at a point is given by gradient of the Surface vector. I am getting $$p = 2 \sqrt{2} \sqrt{\frac{\cos^2x + \cos^2y}{ a^2} + \frac{\cos^2x + \cos^2y}{ b^2} + \frac{\sin^2x}{c^2}}.$$ I am not able to understand what I am doing wrong. Please help. Thanks a lot.","Question is to find the surface integral $$\iint p(x^4 + y^4 + z^4) \, \mathbb dS.$$ The given surface is $$ \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1,$$ and $p$ is the length of the perpendicular from the origin to the tangent plane at $(x, y, z)$. I have attempted solving the question as follows: Parametrizing the surface: $$ x = (a\cos x)\cos y,$$ $$ y = (b \cos x) \sin y ,$$ $$z = c \sin x $$ I am not able to calculate $p$ correctly. This is an example problem and the value of $p$ mentioned in the book is $$p = \frac{1}{ \sqrt{\frac{\cos^2x + \cos^2y}{ a^2} + \frac{\cos^2x + \cos^2y}{ b^2} + \frac{\sin^2x}{c^2}}}.$$ I know that the normal vector to the tangent plane at a point is given by gradient of the Surface vector. I am getting $$p = 2 \sqrt{2} \sqrt{\frac{\cos^2x + \cos^2y}{ a^2} + \frac{\cos^2x + \cos^2y}{ b^2} + \frac{\sin^2x}{c^2}}.$$ I am not able to understand what I am doing wrong. Please help. Thanks a lot.",,"['multivariable-calculus', 'vector-analysis', 'surface-integrals']"
62,Seperating single integral into an double integral.,Seperating single integral into an double integral.,,"Please refer to : How to prove that $\int_{0}^{\infty}\sin{x}\arctan{\frac{1}{x}}\,\mathrm dx=\frac{\pi }{2} \big(\frac{e-1}e\big)$ The answer by @Venus. What is the procedure in converting that single integral, dividing it into parts, and making it a double integral? And also, how Venus took $\sin(x)$ and brought it inside the first integral, and interchanging the integrals? What is the criterion? I am very interested in this. Any links advice or comment is very helpful. Thanks!","Please refer to : How to prove that $\int_{0}^{\infty}\sin{x}\arctan{\frac{1}{x}}\,\mathrm dx=\frac{\pi }{2} \big(\frac{e-1}e\big)$ The answer by @Venus. What is the procedure in converting that single integral, dividing it into parts, and making it a double integral? And also, how Venus took $\sin(x)$ and brought it inside the first integral, and interchanging the integrals? What is the criterion? I am very interested in this. Any links advice or comment is very helpful. Thanks!",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'multivariable-calculus']"
63,Does It Make Sense to Take the Partial Derivative of a Directional Vector?,Does It Make Sense to Take the Partial Derivative of a Directional Vector?,,"Let $\Omega$ be a domain in $\mathbb R^2$ and $C$ be a smooth curve wholly contained in $\Omega$.  Moreover, $C$ is parametrized by $x(t), y(t)$.  If $Q:  \Omega \rightarrow \mathbb R$ has continuous first-order partials, the directional derivative $D_n$ (in the direction of the unit normal) can be written as: $$D_nu = u_{x} y' - u_{y} x'$$ I don't believe that it makes sense to have $\frac{\partial}{\partial x} D_nu$ or $\frac{\partial}{\partial y} D_nu$.  But, if you look at $D_nu$ as a function of $t$, our parameter, then $\frac{\partial}{\partial x} D_nu = \frac{\partial}{\partial y} D_nu = 0$. This question came up when I was doing the proof to show that a harmonic $u$ is infinitely differentiable.  Suppose that $D$ is a Jordan domain inside $\Omega$.  Via Green's identity: $$u(x, y) = \frac{-1}{2\pi} \int_{\partial D} \text{ln}|(x, y) - \sigma(t)| D_nu - u(\sigma(t)) D_n \text{ln}|(x, y) -\sigma(t)| \space dt$$ Since the integrand is continuous, when we differentiate $u$ with respect to $x$, we can take the derivative operation inside the integral to yield ....  Well, what do you have in $\frac{\partial}{\partial x} D_nu$?","Let $\Omega$ be a domain in $\mathbb R^2$ and $C$ be a smooth curve wholly contained in $\Omega$.  Moreover, $C$ is parametrized by $x(t), y(t)$.  If $Q:  \Omega \rightarrow \mathbb R$ has continuous first-order partials, the directional derivative $D_n$ (in the direction of the unit normal) can be written as: $$D_nu = u_{x} y' - u_{y} x'$$ I don't believe that it makes sense to have $\frac{\partial}{\partial x} D_nu$ or $\frac{\partial}{\partial y} D_nu$.  But, if you look at $D_nu$ as a function of $t$, our parameter, then $\frac{\partial}{\partial x} D_nu = \frac{\partial}{\partial y} D_nu = 0$. This question came up when I was doing the proof to show that a harmonic $u$ is infinitely differentiable.  Suppose that $D$ is a Jordan domain inside $\Omega$.  Via Green's identity: $$u(x, y) = \frac{-1}{2\pi} \int_{\partial D} \text{ln}|(x, y) - \sigma(t)| D_nu - u(\sigma(t)) D_n \text{ln}|(x, y) -\sigma(t)| \space dt$$ Since the integrand is continuous, when we differentiate $u$ with respect to $x$, we can take the derivative operation inside the integral to yield ....  Well, what do you have in $\frac{\partial}{\partial x} D_nu$?",,['multivariable-calculus']
64,The missing vector derivative operation,The missing vector derivative operation,,"Generally, the differential operator $d$ takes differential $k$-forms to $(k+1)$-forms.  In $\mathbb{R}^n$ (or more generally on an oriented Riemannian manifold), we can identify vector fields with $1$-forms and $(n-1)$-forms, and functions with $0$-forms and $n$-forms.  If we write $\phi$ for the map between vector fields and 1-forms, and $\psi$ for the map between vector fields and $(n-1)$-forms, then the composite function $=$ 0-form $\xrightarrow{d}$ 1-form $\xrightarrow{\phi}$ vector field is the gradient, and the composite vector field $\xrightarrow{\psi}$ $(n-1)$-form $\xrightarrow{d}$ $n$-form $=$ function is the divergence.  When $n=3$, we also have the composite vector field $\xrightarrow{\phi}$ 1-form $\xrightarrow{d}$ 2-form $\xrightarrow{\psi}$ vector field which is the curl.  And when $n=2$, we have the composite vector field $\xrightarrow{\phi}$ 1-form $\xrightarrow{d}$ 2-form $=$ function which is the ""scalar curl"" that appears in Green's theorem.  Note that it differs from the $n=2$ case of the divergence only in that it involves $\phi$ rather than $\psi$.  When $n=2$ both $\phi$ and $\psi$ relate vector fields to 1-forms, but they do it differently; we have $\phi(\langle F,G\rangle) = F\,dx + G\,dy$, while $\psi(\langle F,G\rangle) = -G\,dx + F\,dy$. This suggests that there is another way to put these together when $n=2$: function $=$ 0-form $\xrightarrow{d}$ 1-form $\xrightarrow{\psi}$ vector field This looks like the $n=2$ case of the gradient, but involves $\psi$ rather than $\phi$.  In coordinates, it takes a function $f$ to the vector field $\langle \frac{\partial f}{\partial y} , -\frac{\partial f}{\partial x}\rangle$. I've never seen this operation before; does it have a name?  It seems potentially interesting, e.g. when we specialize the generalized Stokes' theorem to it we get an FTC for its flux line integrals along curves, analogous to the usual FTC for the flow line integral of a gradient field.","Generally, the differential operator $d$ takes differential $k$-forms to $(k+1)$-forms.  In $\mathbb{R}^n$ (or more generally on an oriented Riemannian manifold), we can identify vector fields with $1$-forms and $(n-1)$-forms, and functions with $0$-forms and $n$-forms.  If we write $\phi$ for the map between vector fields and 1-forms, and $\psi$ for the map between vector fields and $(n-1)$-forms, then the composite function $=$ 0-form $\xrightarrow{d}$ 1-form $\xrightarrow{\phi}$ vector field is the gradient, and the composite vector field $\xrightarrow{\psi}$ $(n-1)$-form $\xrightarrow{d}$ $n$-form $=$ function is the divergence.  When $n=3$, we also have the composite vector field $\xrightarrow{\phi}$ 1-form $\xrightarrow{d}$ 2-form $\xrightarrow{\psi}$ vector field which is the curl.  And when $n=2$, we have the composite vector field $\xrightarrow{\phi}$ 1-form $\xrightarrow{d}$ 2-form $=$ function which is the ""scalar curl"" that appears in Green's theorem.  Note that it differs from the $n=2$ case of the divergence only in that it involves $\phi$ rather than $\psi$.  When $n=2$ both $\phi$ and $\psi$ relate vector fields to 1-forms, but they do it differently; we have $\phi(\langle F,G\rangle) = F\,dx + G\,dy$, while $\psi(\langle F,G\rangle) = -G\,dx + F\,dy$. This suggests that there is another way to put these together when $n=2$: function $=$ 0-form $\xrightarrow{d}$ 1-form $\xrightarrow{\psi}$ vector field This looks like the $n=2$ case of the gradient, but involves $\psi$ rather than $\phi$.  In coordinates, it takes a function $f$ to the vector field $\langle \frac{\partial f}{\partial y} , -\frac{\partial f}{\partial x}\rangle$. I've never seen this operation before; does it have a name?  It seems potentially interesting, e.g. when we specialize the generalized Stokes' theorem to it we get an FTC for its flux line integrals along curves, analogous to the usual FTC for the flow line integral of a gradient field.",,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
65,How to partial differentiate a total differential and be rigorous on all the notion?,How to partial differentiate a total differential and be rigorous on all the notion?,,"Start with $$dS=\left(\frac{\partial S}{\partial T}\right)_VdT+\left(\frac{\partial S}{\partial V}\right)_TdV$$ Using the notes shown here Method 1: i) Divide both sides by dV $$\frac{dS}{dV}=\left(\frac{\partial S}{\partial T}\right)_V\frac{dT}{dV}+\left(\frac{\partial S}{\partial V}\right)_T\frac{dV}{dV}$$ ii) and at const. P $$\left(\frac{dS}{dV}\right)_P=\left(\frac{\partial S}{\partial T}\right)_V\left(\frac{dT}{dV}\right)_P+\left(\frac{\partial S}{\partial V}\right)_T\left(\frac{dV}{dV}\right)_P$$ $$\left(\frac{dS}{dV}\right)_P=\left(\frac{\partial S}{\partial T}\right)_V\left(\frac{dT}{dV}\right)_P+\left(\frac{\partial S}{\partial V}\right)_T$$ Question 1: but how does $$\left(\frac{dS}{dV}\right)_P=\left(\frac{\partial S}{\partial T}\right)_V\left(\frac{dT}{dV}\right)_P+\left(\frac{\partial S}{\partial V}\right)_T$$ becomes $$\left(\frac{\partial S}{\partial V}\right)_P=\left(\frac{\partial S}{\partial T}\right)_V\left(\frac{\partial T}{\partial V}\right)_P+\left(\frac{\partial S}{\partial V}\right)_T???$$ Using the notes shown here Method 2: Differentiate both side wrt V, holding P const. and use product rule $$\frac{\partial}{\partial V}\left(dS\right)_P=\frac{\partial}{\partial V}\left(\left(\frac{\partial S}{\partial T}\right)_VdT\right)_P+\frac{\partial}{\partial V}\left(\left(\frac{\partial S}{\partial V}\right)_TdV\right)_P$$ $$\left(\frac{\partial dS}{\partial V}\right)_P=\left(\left(\frac{\partial^2 S}{\partial V \partial T}\right)_V\right)_PdT+\left(\frac{\partial S}{\partial T}\right)_V\left(\frac{\partial dT}{\partial V}\right)_P+\left(\left(\frac{\partial^2 S}{\partial V^2}\right)_T\right)_PdV+\left(\frac{\partial S}{\partial V}\right)_T\left(\frac{\partial dV}{\partial V}\right)_P$$ Question 2: I got so many extra terms, and how to deal with these $$\left(\frac{\partial \text{ d blah}_1}{\partial \text{ blah}_2}\right)_{\text{blah}_3}$$ terms? Tl:dr How to partial differentiate a total differential rigorously?","Start with Using the notes shown here Method 1: i) Divide both sides by dV ii) and at const. P Question 1: but how does becomes Using the notes shown here Method 2: Differentiate both side wrt V, holding P const. and use product rule Question 2: I got so many extra terms, and how to deal with these terms? Tl:dr How to partial differentiate a total differential rigorously?",dS=\left(\frac{\partial S}{\partial T}\right)_VdT+\left(\frac{\partial S}{\partial V}\right)_TdV \frac{dS}{dV}=\left(\frac{\partial S}{\partial T}\right)_V\frac{dT}{dV}+\left(\frac{\partial S}{\partial V}\right)_T\frac{dV}{dV} \left(\frac{dS}{dV}\right)_P=\left(\frac{\partial S}{\partial T}\right)_V\left(\frac{dT}{dV}\right)_P+\left(\frac{\partial S}{\partial V}\right)_T\left(\frac{dV}{dV}\right)_P \left(\frac{dS}{dV}\right)_P=\left(\frac{\partial S}{\partial T}\right)_V\left(\frac{dT}{dV}\right)_P+\left(\frac{\partial S}{\partial V}\right)_T \left(\frac{dS}{dV}\right)_P=\left(\frac{\partial S}{\partial T}\right)_V\left(\frac{dT}{dV}\right)_P+\left(\frac{\partial S}{\partial V}\right)_T \left(\frac{\partial S}{\partial V}\right)_P=\left(\frac{\partial S}{\partial T}\right)_V\left(\frac{\partial T}{\partial V}\right)_P+\left(\frac{\partial S}{\partial V}\right)_T??? \frac{\partial}{\partial V}\left(dS\right)_P=\frac{\partial}{\partial V}\left(\left(\frac{\partial S}{\partial T}\right)_VdT\right)_P+\frac{\partial}{\partial V}\left(\left(\frac{\partial S}{\partial V}\right)_TdV\right)_P \left(\frac{\partial dS}{\partial V}\right)_P=\left(\left(\frac{\partial^2 S}{\partial V \partial T}\right)_V\right)_PdT+\left(\frac{\partial S}{\partial T}\right)_V\left(\frac{\partial dT}{\partial V}\right)_P+\left(\left(\frac{\partial^2 S}{\partial V^2}\right)_T\right)_PdV+\left(\frac{\partial S}{\partial V}\right)_T\left(\frac{\partial dV}{\partial V}\right)_P \left(\frac{\partial \text{ d blah}_1}{\partial \text{ blah}_2}\right)_{\text{blah}_3},"['multivariable-calculus', 'homogeneous-equation']"
66,Improper multivariable integrals,Improper multivariable integrals,,"I'm having trouble with the integral $$\iiint_{1\le x^2+y^2+z^2 }\frac{\mathrm{d}x~\mathrm{d}y~\mathrm{d}z}{xyz}$$ this is what I've done so far: $$\lim_{b\to +\infty}\int_1^b \int_0^{2\pi} \int_0^{\pi}\frac {r^2\sin(\psi)~\mathrm{d}\psi~\mathrm{d}\theta ~\mathrm{d}r}{r^3\cos(\theta)\sin(\theta)\cos(\psi)\sin²(\psi)}$$ but I'm lost. Besides, $\cos(x)$ and $\sin(x)$ are zero in many points of the domain, do I have to consider those too? Any hints please?","I'm having trouble with the integral $$\iiint_{1\le x^2+y^2+z^2 }\frac{\mathrm{d}x~\mathrm{d}y~\mathrm{d}z}{xyz}$$ this is what I've done so far: $$\lim_{b\to +\infty}\int_1^b \int_0^{2\pi} \int_0^{\pi}\frac {r^2\sin(\psi)~\mathrm{d}\psi~\mathrm{d}\theta ~\mathrm{d}r}{r^3\cos(\theta)\sin(\theta)\cos(\psi)\sin²(\psi)}$$ but I'm lost. Besides, $\cos(x)$ and $\sin(x)$ are zero in many points of the domain, do I have to consider those too? Any hints please?",,"['real-analysis', 'multivariable-calculus', 'improper-integrals']"
67,Does chain rule require continuously differentiability?,Does chain rule require continuously differentiability?,,"Recently I read the book Advanced Calculus written by Fitzpatrick. The Theorem 15.34 tells that  If $F:\mathbb R^n\to\mathbb R^m$ is CONTINUOUSLY differentiable (all partial derivatives exist and continuous) and $g:\mathbb R^m\to\mathbb R$ is also CONTINUOUSLY differentiable, then $\frac{\partial}{\partial x_i}(g\circ F)(x)=\sum^m_{j=1}D_jg(F(x))\frac{\partial F_j}{\partial x_i}(x)$ and $g\circ F$ is also continuously differentiable. I know that continuously differentiable (all partial derivatives exist and continuous) implies differentiable. But a function is differentiable may not indicate that its partial derivatives are continuous. I wonder that whether the chain rule can be applied if it is differentiable but do not have continuous derivatives. p.s. The proof in the book contains $\lim_{h\to0}\nabla g(x+h)=\nabla g(x)$ which needs continuity of $\nabla g$","Recently I read the book Advanced Calculus written by Fitzpatrick. The Theorem 15.34 tells that  If $F:\mathbb R^n\to\mathbb R^m$ is CONTINUOUSLY differentiable (all partial derivatives exist and continuous) and $g:\mathbb R^m\to\mathbb R$ is also CONTINUOUSLY differentiable, then $\frac{\partial}{\partial x_i}(g\circ F)(x)=\sum^m_{j=1}D_jg(F(x))\frac{\partial F_j}{\partial x_i}(x)$ and $g\circ F$ is also continuously differentiable. I know that continuously differentiable (all partial derivatives exist and continuous) implies differentiable. But a function is differentiable may not indicate that its partial derivatives are continuous. I wonder that whether the chain rule can be applied if it is differentiable but do not have continuous derivatives. p.s. The proof in the book contains $\lim_{h\to0}\nabla g(x+h)=\nabla g(x)$ which needs continuity of $\nabla g$",,['multivariable-calculus']
68,Second degree partial differential equation with variable-change,Second degree partial differential equation with variable-change,,"Edit: @Etienne mentioned that I did a typo, writing $u_y' = -xye^{-y}$ instead of $u_y' = -xe^{-y}$. I've corrected that in the calculations and now it's closer to being correct! Though I still miss $e^{-y}$ multiplicated with $f_{uv}''$ to get the correct answer. Problem: Transform the differential equation: $$ x\frac{\partial ^2f}{\partial x^2} + \frac{\partial ^2f}{\partial x \,\partial y} + \frac{\partial f}{\partial x} =  xe^{-2y} $$ by introducing: $$  \begin{cases} u = &xe^{-y} \\ v = &y \end{cases} $$ How I am solving it. Rewrite to: $$ xf_{xy}'' + f_{yx}'' + f_{x} = xe^{-2y} $$ solve $x$ and $y$ in $u$ and $v$: $$ \begin{cases} x = &\frac{u}{e^{-v}} \\ y = &v \end{cases} $$ And calculate: $$ \begin{cases} u_x' = & e^{-y} \\ u_y' = & -xe^{-y} \end{cases}  $$ $$ \begin{cases} v_x' = & 0 \\ v_y' = & 1 \end{cases}  $$ Find $f_x'$ by using the chainrule: $$ f_x' = f_u' \cdot u_x' + f_v' \cdot v_x' = f_u' \cdot e^{-y}+f_v'\cdot0=f_u'e^{-y} $$ set $g=f_u'$ and $h=f_v'$ and calculate $f_{xx}''$ $$ f_x' = f_u'e^{-y} \underbrace{= }_{g=f_u'} ge^{-y} $$ $$ f_{xx}'' = g_x' \cdot e^{-y} $$ Find $g_x'$ (using chainrule): $$ g_x' = g_u' \cdot u_x' + g_v' \cdot v_x' = g_u' \cdot e^{-y} + g_v' \cdot 0 =  g_u' \cdot e^{-y} \underbrace{= }_{g=f_u'} (f_u')_u' \cdot e^{-y} = f_{uu}'' \cdot e^{-y} $$ insert $g_x'$ in $f_{xx}''$ gives: $$ f_{xx}'' \underbrace{=}_{g_x' = f_{uu}'' \cdot e^{-y}} (f_{uu}'' \cdot e^{-y}) \cdot e^{-y} = f_{uu}'' \cdot e^{-2y} \underbrace{=}_{y=v} f_{uu}'' \cdot e^{-2v} $$ Now I have $f_x'$ and  $f_{xx}''$ and I only need $f_{yx}''$ to insert the differentials ito the original equation. But $f_{yx}''$ can be found using $(f_x')_y'$ since $f_{xy}'' = f_{yx}''$. $$ f_{xy}'' = -ge^{-y} + g_y'e^{-y} $$ Find $g_y'$ (using chainrule): $$ g_y' = g_u' \cdot u_y' + g_v' \cdot v_y' = g_u' \cdot (-xe^{-y}) + g_v' \cdot 1 \underbrace{=}_{g=f_u'} (f_u')_u' \cdot (-xe^{-y}) + (f_u')_v' \cdot 1 = $$ $$ - f_{uu}'' \cdot xe^{-y} + f_{uv}'' = - f_{uu}'' \cdot \frac{u}{e^{-v}}e^{-v} + f_{uv}'' = - f_{uu}'' \cdot u + f_{uv}'' $$ replace $g$ and $g_y'$ and translate to $u$ and $v$ instead of $x$ and $y$ $$ f_{xy}'' = -f_u' e^{-v} + (- f_{uu}''u + f_{uv}'')e^{-v} = -f_u' e^{-v} - f_{uu}''ue^{-v} + f_{uv}''e^{-v} $$ Insert $f_{xx}''$, $f_{yx}''$ and $f_v'$ in the original equation: $$ \frac{u}{e^{-v}} (f_{uu}'' e^{-2v}) + -f_u' e^{-v} - f_{uu}''ue^{-v} + f_{uv}''e^{-v} + f_u'e^{-v} = xe^{-2y} $$ $f_u' e^{-v}$ and $uf_{uu}''e^{-v}$ cancels and I get $$ f_{uv}'' = ue^{-v} = \frac{\partial ^2f}{\partial u \,\partial v} $$ Obviously I've missed something. The terms with $f_{uu}''$ is gonna cancel out each other, but I can't figure out whats wrong. I've recalculated this assignment 3 times and it must be something Im not doing correctly, since I probably wouldnt do the same error three times in a row.. The answer is supposed to be: $$ \frac{\partial ^2f}{\partial u \,\partial v} = u $$ So I'm missing a $v$ in the first term and $e^{-v}$ multiplied with $f_{uv}''$","Edit: @Etienne mentioned that I did a typo, writing $u_y' = -xye^{-y}$ instead of $u_y' = -xe^{-y}$. I've corrected that in the calculations and now it's closer to being correct! Though I still miss $e^{-y}$ multiplicated with $f_{uv}''$ to get the correct answer. Problem: Transform the differential equation: $$ x\frac{\partial ^2f}{\partial x^2} + \frac{\partial ^2f}{\partial x \,\partial y} + \frac{\partial f}{\partial x} =  xe^{-2y} $$ by introducing: $$  \begin{cases} u = &xe^{-y} \\ v = &y \end{cases} $$ How I am solving it. Rewrite to: $$ xf_{xy}'' + f_{yx}'' + f_{x} = xe^{-2y} $$ solve $x$ and $y$ in $u$ and $v$: $$ \begin{cases} x = &\frac{u}{e^{-v}} \\ y = &v \end{cases} $$ And calculate: $$ \begin{cases} u_x' = & e^{-y} \\ u_y' = & -xe^{-y} \end{cases}  $$ $$ \begin{cases} v_x' = & 0 \\ v_y' = & 1 \end{cases}  $$ Find $f_x'$ by using the chainrule: $$ f_x' = f_u' \cdot u_x' + f_v' \cdot v_x' = f_u' \cdot e^{-y}+f_v'\cdot0=f_u'e^{-y} $$ set $g=f_u'$ and $h=f_v'$ and calculate $f_{xx}''$ $$ f_x' = f_u'e^{-y} \underbrace{= }_{g=f_u'} ge^{-y} $$ $$ f_{xx}'' = g_x' \cdot e^{-y} $$ Find $g_x'$ (using chainrule): $$ g_x' = g_u' \cdot u_x' + g_v' \cdot v_x' = g_u' \cdot e^{-y} + g_v' \cdot 0 =  g_u' \cdot e^{-y} \underbrace{= }_{g=f_u'} (f_u')_u' \cdot e^{-y} = f_{uu}'' \cdot e^{-y} $$ insert $g_x'$ in $f_{xx}''$ gives: $$ f_{xx}'' \underbrace{=}_{g_x' = f_{uu}'' \cdot e^{-y}} (f_{uu}'' \cdot e^{-y}) \cdot e^{-y} = f_{uu}'' \cdot e^{-2y} \underbrace{=}_{y=v} f_{uu}'' \cdot e^{-2v} $$ Now I have $f_x'$ and  $f_{xx}''$ and I only need $f_{yx}''$ to insert the differentials ito the original equation. But $f_{yx}''$ can be found using $(f_x')_y'$ since $f_{xy}'' = f_{yx}''$. $$ f_{xy}'' = -ge^{-y} + g_y'e^{-y} $$ Find $g_y'$ (using chainrule): $$ g_y' = g_u' \cdot u_y' + g_v' \cdot v_y' = g_u' \cdot (-xe^{-y}) + g_v' \cdot 1 \underbrace{=}_{g=f_u'} (f_u')_u' \cdot (-xe^{-y}) + (f_u')_v' \cdot 1 = $$ $$ - f_{uu}'' \cdot xe^{-y} + f_{uv}'' = - f_{uu}'' \cdot \frac{u}{e^{-v}}e^{-v} + f_{uv}'' = - f_{uu}'' \cdot u + f_{uv}'' $$ replace $g$ and $g_y'$ and translate to $u$ and $v$ instead of $x$ and $y$ $$ f_{xy}'' = -f_u' e^{-v} + (- f_{uu}''u + f_{uv}'')e^{-v} = -f_u' e^{-v} - f_{uu}''ue^{-v} + f_{uv}''e^{-v} $$ Insert $f_{xx}''$, $f_{yx}''$ and $f_v'$ in the original equation: $$ \frac{u}{e^{-v}} (f_{uu}'' e^{-2v}) + -f_u' e^{-v} - f_{uu}''ue^{-v} + f_{uv}''e^{-v} + f_u'e^{-v} = xe^{-2y} $$ $f_u' e^{-v}$ and $uf_{uu}''e^{-v}$ cancels and I get $$ f_{uv}'' = ue^{-v} = \frac{\partial ^2f}{\partial u \,\partial v} $$ Obviously I've missed something. The terms with $f_{uu}''$ is gonna cancel out each other, but I can't figure out whats wrong. I've recalculated this assignment 3 times and it must be something Im not doing correctly, since I probably wouldnt do the same error three times in a row.. The answer is supposed to be: $$ \frac{\partial ^2f}{\partial u \,\partial v} = u $$ So I'm missing a $v$ in the first term and $e^{-v}$ multiplied with $f_{uv}''$",,"['multivariable-calculus', 'partial-differential-equations']"
69,Computation of a certain flux integral,Computation of a certain flux integral,,"Let $$\Omega = \{(x_1, x_2, x_3) \in \mathbb{R}^3 : \max(|x|_1, |x|_2, |x|_3) \leq 1\}$$ $$F_i(x) = \frac{x_i}{\|x\|^3}$$ and suppose $\varphi(y)$ be a continuously differentiable function of $y_i = x_i/\|x\|$, with $\varphi$ having average value $1$ over the unit sphere. Calculate $$\int_{\partial \Omega} \varphi F \cdot n \; dS$$ The solution I get is zero by the divergence theorem. But this is not what the solution manual says. Am I wrong, or is the solution manual wrong? I never use the assumption on the average value of $\varphi$.","Let $$\Omega = \{(x_1, x_2, x_3) \in \mathbb{R}^3 : \max(|x|_1, |x|_2, |x|_3) \leq 1\}$$ $$F_i(x) = \frac{x_i}{\|x\|^3}$$ and suppose $\varphi(y)$ be a continuously differentiable function of $y_i = x_i/\|x\|$, with $\varphi$ having average value $1$ over the unit sphere. Calculate $$\int_{\partial \Omega} \varphi F \cdot n \; dS$$ The solution I get is zero by the divergence theorem. But this is not what the solution manual says. Am I wrong, or is the solution manual wrong? I never use the assumption on the average value of $\varphi$.",,['multivariable-calculus']
70,"Best book for learning multiple integrals, line integrals, Green's theorem, etc.","Best book for learning multiple integrals, line integrals, Green's theorem, etc.",,"I've been searching for a book that teaches multiple integrals and such in a way that I can understand. I need to learn it quickly, so I don't need too much of the intuition, I just need to be able to do the mechanics of it in an exam. I also learn well with lots of worked examples. Can someone recommend a few books? Thanks.","I've been searching for a book that teaches multiple integrals and such in a way that I can understand. I need to learn it quickly, so I don't need too much of the intuition, I just need to be able to do the mechanics of it in an exam. I also learn well with lots of worked examples. Can someone recommend a few books? Thanks.",,"['multivariable-calculus', 'contour-integration', 'book-recommendation', 'volume', 'surfaces']"
71,Can Lagrange multipliers be used to give a good bound on the number of critical points?,Can Lagrange multipliers be used to give a good bound on the number of critical points?,,"I will explain my problem by illustrating a simple case. Easy question: Let $f(x,y)$ be a ""generic"" polynomial in two variables, of total degree $\le D$. What's a good upper bound for how many critical points (minimum/maximum) it has in $\mathbb R^2$? Solution: The critical points satisfy $\nabla f = 0$, so we can try to count the combined zeroes of the two polynomials $\frac {\partial f}{\partial x}$ and $\frac {\partial f}{\partial y}$, both of degree $\le D$. We assume they're both irreducible ($f$ is ""generic""), and Bezout's theorem says there are at most $D^2$ solutions. Harder question: What's a good upper bound for how many critical points $f$ has on a given straight line $Ax + By = C$? (i.e. minimum/maximum subject to the constraint $Ax + By = C$.) Approach #1: Lagrange multipliers. Each critical point satisfies $\nabla f = (\lambda A, \lambda B)$ for some $\lambda \in \mathbb R$. So we have a system of 3 equations $$\cases{\frac {\partial f}{\partial x}=\lambda A \\ \frac{\partial f}{\partial y}=\lambda B \\ Ax+By=C}$$ in 3 variables $x,y,\lambda$. The first two equations are polynomial of degree at most $D$ and the last one is of degree $1$. By Bezout, there are at most $D^2$ solutions. Approach #2: Parameterize the line. Suppose the line is given by $y=Px+Q$. Define a function $g(t)=f(t,Pt+Q)$. This is a ""generic"" polynomial in one variable of degree $\le D$. By ""Bezout"" (or just elementary algebra) there are at most $D$ solutions to $g'(t)=0$. My question: Clearly Approach #2 gave a better result. However, I want to apply this in a harder scenario where there are more variables, more constraints, and some of the constraints are non-linear. In this scenario it seems ""ugly"" to parameterize the constraints like I did in approach #2, and very nice and clean to use Lagrange multipliers. But I want a better result. How do I modify the Lagrange multiplier approach to get the better bound, while avoiding parameterizing the curve (line)?","I will explain my problem by illustrating a simple case. Easy question: Let $f(x,y)$ be a ""generic"" polynomial in two variables, of total degree $\le D$. What's a good upper bound for how many critical points (minimum/maximum) it has in $\mathbb R^2$? Solution: The critical points satisfy $\nabla f = 0$, so we can try to count the combined zeroes of the two polynomials $\frac {\partial f}{\partial x}$ and $\frac {\partial f}{\partial y}$, both of degree $\le D$. We assume they're both irreducible ($f$ is ""generic""), and Bezout's theorem says there are at most $D^2$ solutions. Harder question: What's a good upper bound for how many critical points $f$ has on a given straight line $Ax + By = C$? (i.e. minimum/maximum subject to the constraint $Ax + By = C$.) Approach #1: Lagrange multipliers. Each critical point satisfies $\nabla f = (\lambda A, \lambda B)$ for some $\lambda \in \mathbb R$. So we have a system of 3 equations $$\cases{\frac {\partial f}{\partial x}=\lambda A \\ \frac{\partial f}{\partial y}=\lambda B \\ Ax+By=C}$$ in 3 variables $x,y,\lambda$. The first two equations are polynomial of degree at most $D$ and the last one is of degree $1$. By Bezout, there are at most $D^2$ solutions. Approach #2: Parameterize the line. Suppose the line is given by $y=Px+Q$. Define a function $g(t)=f(t,Pt+Q)$. This is a ""generic"" polynomial in one variable of degree $\le D$. By ""Bezout"" (or just elementary algebra) there are at most $D$ solutions to $g'(t)=0$. My question: Clearly Approach #2 gave a better result. However, I want to apply this in a harder scenario where there are more variables, more constraints, and some of the constraints are non-linear. In this scenario it seems ""ugly"" to parameterize the constraints like I did in approach #2, and very nice and clean to use Lagrange multipliers. But I want a better result. How do I modify the Lagrange multiplier approach to get the better bound, while avoiding parameterizing the curve (line)?",,"['calculus', 'multivariable-calculus', 'algebraic-geometry', 'lagrange-multiplier']"
72,"Scalar line integral, surface integral, change of variables","Scalar line integral, surface integral, change of variables",,"When we do a scalar line integral, we parameterize the curve, then integrate, but we introduce the magnitude of the derivative of our parameterization into the integral. When we do a surface integral, we integrate over the domain of the parameterization of the surface and introduce the Jacobian. When we do a change of variables, we also introduce the Jacobian. Are these all similar in that we integrate over a new domain and now have to ""adjust"" to make the integral correct? Is the some sort of Jacobian used in the scalar line integral? Is this all because we are now integrating over a new domain?","When we do a scalar line integral, we parameterize the curve, then integrate, but we introduce the magnitude of the derivative of our parameterization into the integral. When we do a surface integral, we integrate over the domain of the parameterization of the surface and introduce the Jacobian. When we do a change of variables, we also introduce the Jacobian. Are these all similar in that we integrate over a new domain and now have to ""adjust"" to make the integral correct? Is the some sort of Jacobian used in the scalar line integral? Is this all because we are now integrating over a new domain?",,"['calculus', 'multivariable-calculus']"
73,Proof that Curl and Divergence Uniquely Define Vector Field,Proof that Curl and Divergence Uniquely Define Vector Field,,Why/how do curl and divergence yield a unique vector field? Can anyone give me a proof? Also can you construct a vector field from any curl/divergence?,Why/how do curl and divergence yield a unique vector field? Can anyone give me a proof? Also can you construct a vector field from any curl/divergence?,,['multivariable-calculus']
74,Product of Elements in SU(2),Product of Elements in SU(2),,"Let $$ V := \frac{x_4+i\vec{x}\cdot{\vec{\sigma}}}{\left|x\right|}$$ where $\left(x_1,x_2,x_3,x_4\right)\in\mathbb{R}^4$, $|x|$ is the Euclidean norm, and $\sigma^j$ are the Pauli matrices. Let $\theta_1,\,\theta_2,\,\theta_3$ be some parametrization of $S^3$. How do you ""see"" that \begin{align} \sum_{i=1}^3\sum_{j=1}^3\sum_{k=1}^3\varepsilon^{i j k} tr\left[V\left(\frac{\partial}{\partial \theta_i}V^{-1}\right)V\left(\frac{\partial}{\partial \theta_j}V^{-1}\right)V\left(\frac{\partial}{\partial \theta_k}V^{-1}\right)\right]\end{align} is proportional to $\left[\sin\left(\theta_1\right)\right]^2\sin\left(\theta_2\right)$, where $\varepsilon^{i j k}$ is the totally anti-symmetric tensor? I have tried explicitly calculating this expression but after half a page of tediousness it seems like the wrong way to go. Using Mathematica I get the right result ($-12\times \left[\sin\left(\theta_1\right)\right]^2\sin\left(\theta_2\right)$), but I would like to know how to just ""see"" this result and compute the proportionality constant conveniently.","Let $$ V := \frac{x_4+i\vec{x}\cdot{\vec{\sigma}}}{\left|x\right|}$$ where $\left(x_1,x_2,x_3,x_4\right)\in\mathbb{R}^4$, $|x|$ is the Euclidean norm, and $\sigma^j$ are the Pauli matrices. Let $\theta_1,\,\theta_2,\,\theta_3$ be some parametrization of $S^3$. How do you ""see"" that \begin{align} \sum_{i=1}^3\sum_{j=1}^3\sum_{k=1}^3\varepsilon^{i j k} tr\left[V\left(\frac{\partial}{\partial \theta_i}V^{-1}\right)V\left(\frac{\partial}{\partial \theta_j}V^{-1}\right)V\left(\frac{\partial}{\partial \theta_k}V^{-1}\right)\right]\end{align} is proportional to $\left[\sin\left(\theta_1\right)\right]^2\sin\left(\theta_2\right)$, where $\varepsilon^{i j k}$ is the totally anti-symmetric tensor? I have tried explicitly calculating this expression but after half a page of tediousness it seems like the wrong way to go. Using Mathematica I get the right result ($-12\times \left[\sin\left(\theta_1\right)\right]^2\sin\left(\theta_2\right)$), but I would like to know how to just ""see"" this result and compute the proportionality constant conveniently.",,"['calculus', 'linear-algebra', 'multivariable-calculus']"
75,Is there a generalization of integration by parts?,Is there a generalization of integration by parts?,,"In the original integration by part formula there are two functions $u(x)$ and $v(x)$. What if the integral involves another function $w(x)$ as well? Second of all, I know that there is a several variable version of integration by substitution, is there any several variables version of integration by parts? I read the wikipedia page on the higher dimensional case , and I found it really complicated.  Could anyone show me an example on how to do the higher dimensional case?","In the original integration by part formula there are two functions $u(x)$ and $v(x)$. What if the integral involves another function $w(x)$ as well? Second of all, I know that there is a several variable version of integration by substitution, is there any several variables version of integration by parts? I read the wikipedia page on the higher dimensional case , and I found it really complicated.  Could anyone show me an example on how to do the higher dimensional case?",,"['calculus', 'integration', 'multivariable-calculus']"
76,double integral with substitution,double integral with substitution,,"1. Find the area integral of $ \ f(x,y)=x^2 + y^2 \ $ in $ \ D=\{(x,y)\in\Bbb{R}|0\le x-2y\le2, |3x-y|\le1\} \ $ when we do a substitution $ \ u=x-2y \ $ and $ \ v=3x-y \ $ I did the calculations like this. Does it seem right? $x=\frac 15(2v-u)$ and $y=\frac 15(v-3u)$ $dxdy=\frac{1}{\begin{vmatrix}1&-2\\3&-1\end{vmatrix}}dudv=\frac 15dudv$ $$\frac 15 \int_{-1}^{1}\int_0^2\frac{1}{25}(2v-u)^2+\frac{1}{25}(v-3u)^2dudv=\frac {12}{25} $$ 2. Also similar. $$\int_{-1}^0 \int_{-\sqrt{1-x^2}}^0 \frac{2}{1+\sqrt{x^2+y^2}}dxdy$$ I think I should go to polar coordinates and $x=-\sqrt{1-x^2}$ so $x=-\frac{1}{\sqrt2}$ and $x=0$. Then $\phi\in[5\pi/4, 3\pi/2]$ or because symmetry $\phi\in[0, \pi/4]$ $$\int_{0}^{\pi/4} \int_0^1 \frac{2r}{1+r}drd\phi$$ And now just integrate?","1. Find the area integral of $ \ f(x,y)=x^2 + y^2 \ $ in $ \ D=\{(x,y)\in\Bbb{R}|0\le x-2y\le2, |3x-y|\le1\} \ $ when we do a substitution $ \ u=x-2y \ $ and $ \ v=3x-y \ $ I did the calculations like this. Does it seem right? $x=\frac 15(2v-u)$ and $y=\frac 15(v-3u)$ $dxdy=\frac{1}{\begin{vmatrix}1&-2\\3&-1\end{vmatrix}}dudv=\frac 15dudv$ $$\frac 15 \int_{-1}^{1}\int_0^2\frac{1}{25}(2v-u)^2+\frac{1}{25}(v-3u)^2dudv=\frac {12}{25} $$ 2. Also similar. $$\int_{-1}^0 \int_{-\sqrt{1-x^2}}^0 \frac{2}{1+\sqrt{x^2+y^2}}dxdy$$ I think I should go to polar coordinates and $x=-\sqrt{1-x^2}$ so $x=-\frac{1}{\sqrt2}$ and $x=0$. Then $\phi\in[5\pi/4, 3\pi/2]$ or because symmetry $\phi\in[0, \pi/4]$ $$\int_{0}^{\pi/4} \int_0^1 \frac{2r}{1+r}drd\phi$$ And now just integrate?",,['multivariable-calculus']
77,Differential calculus question.,Differential calculus question.,,"Find a constant c so that at any point of intersection of the two spheres $(x-c)^2+y^2+z^2=3$ and $x^2+(y-1)^2+z^2=1$ , the corresponding tangent planes are perpendicular to each other.","Find a constant c so that at any point of intersection of the two spheres $(x-c)^2+y^2+z^2=3$ and $x^2+(y-1)^2+z^2=1$ , the corresponding tangent planes are perpendicular to each other.",,"['calculus', 'multivariable-calculus', 'derivatives']"
78,Cone is a submanifold iff it is a vector subspace,Cone is a submanifold iff it is a vector subspace,,"Could you tell me how to prove the following? Let $\emptyset \neq M \subset \mathbb{R}^n $ be a cone ($\forall x \in M, t \in \mathbb{R} : tx \in M $ ), $0 \le d \le n.$ Prove that $M$ is a $d$ -dimensional submanifold class $\mathcal{C}^1 \iff M$ is a $d$ -dimensional subspace of $\mathbb{R}^n $. I will be grateful for all your  help. Thank you.","Could you tell me how to prove the following? Let $\emptyset \neq M \subset \mathbb{R}^n $ be a cone ($\forall x \in M, t \in \mathbb{R} : tx \in M $ ), $0 \le d \le n.$ Prove that $M$ is a $d$ -dimensional submanifold class $\mathcal{C}^1 \iff M$ is a $d$ -dimensional subspace of $\mathbb{R}^n $. I will be grateful for all your  help. Thank you.",,"['real-analysis', 'multivariable-calculus', 'manifolds']"
79,Fubini theorem for improper Riemann integral,Fubini theorem for improper Riemann integral,,"Is there a version of Fubini's theorem for improper Riemann integrals? Here's an example of what such a version might look like. If $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is bounded and non-negative over the set $E\subseteq\mathbb{R}^n$ and if the improper Riemann integral $I:=\int_E f\left(\mathbf{x}\right)\ \mathrm{d}\mathbf{x}$ exists and is finite, then the following iterated integral is well defined and evaluates recursively to $I$: $$ \int_{-\infty}^\infty\int_{-\infty}^\infty\cdots\int_{-\infty}^\infty f\left(x_1,x_2,\dots,x_n\right)\mathbb{1}_E\left(x_1,x_2,\dots,x_n\right)\  \mathrm{d}x_n\cdots\mathrm{d}x_2\mathrm{d}x_1 $$ So, for almost every $\left(x_1,\dots,x_{n-1}\right)\in\mathbb{R}^{n-1}$, the function $$ x_n \mapsto f\left(x_1,\dots,x_{n-1},x_n\right)\mathbb{1}_E\left(x_1,\dots,x_{n-1},x_n\right) $$ is non-negative, bounded and Riemann integrable on every closed interval $\left[a,b\right]\subseteq\mathbb{R}$ ($a<b$) and the function $g:\mathbb{R}^{n-1}\rightarrow\mathbb{R}$ $$ g\left(x_1,\dots,x_{n-1}\right):=\lim_{r\rightarrow\infty}\int_{-r}^r f\left(x_1,\dots,x_{n-1},x_n\right)\mathbb{1}_E\left(x_1,\dots,x_{n-1},x_n\right)\ \mathrm{d}x_n $$ is again bounded and non-negative over $E'$ = the projection of $E$ on the first $n-1$ coordinates, and the improper integral $\int_{E'}f\left(\mathbf{x}'\right)\ \mathrm{d}\mathbf{x}'$ exists and equals $I$.","Is there a version of Fubini's theorem for improper Riemann integrals? Here's an example of what such a version might look like. If $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is bounded and non-negative over the set $E\subseteq\mathbb{R}^n$ and if the improper Riemann integral $I:=\int_E f\left(\mathbf{x}\right)\ \mathrm{d}\mathbf{x}$ exists and is finite, then the following iterated integral is well defined and evaluates recursively to $I$: $$ \int_{-\infty}^\infty\int_{-\infty}^\infty\cdots\int_{-\infty}^\infty f\left(x_1,x_2,\dots,x_n\right)\mathbb{1}_E\left(x_1,x_2,\dots,x_n\right)\  \mathrm{d}x_n\cdots\mathrm{d}x_2\mathrm{d}x_1 $$ So, for almost every $\left(x_1,\dots,x_{n-1}\right)\in\mathbb{R}^{n-1}$, the function $$ x_n \mapsto f\left(x_1,\dots,x_{n-1},x_n\right)\mathbb{1}_E\left(x_1,\dots,x_{n-1},x_n\right) $$ is non-negative, bounded and Riemann integrable on every closed interval $\left[a,b\right]\subseteq\mathbb{R}$ ($a<b$) and the function $g:\mathbb{R}^{n-1}\rightarrow\mathbb{R}$ $$ g\left(x_1,\dots,x_{n-1}\right):=\lim_{r\rightarrow\infty}\int_{-r}^r f\left(x_1,\dots,x_{n-1},x_n\right)\mathbb{1}_E\left(x_1,\dots,x_{n-1},x_n\right)\ \mathrm{d}x_n $$ is again bounded and non-negative over $E'$ = the projection of $E$ on the first $n-1$ coordinates, and the improper integral $\int_{E'}f\left(\mathbf{x}'\right)\ \mathrm{d}\mathbf{x}'$ exists and equals $I$.",,"['real-analysis', 'multivariable-calculus', 'improper-integrals']"
80,Is zero vector potential for Helmholtz decomposition of curl and divergence free vector fields necessary?,Is zero vector potential for Helmholtz decomposition of curl and divergence free vector fields necessary?,,"Helmholtz's theorem tells us that a sufficiently smooth vector field $\mathbf{A}$ can be decomposed into curl and gradient free parts as the gradient of a scalar potential plus the curl of a vector potential  $$ \mathbf{A} = \nabla \phi + \nabla \times \mathbf{B}$$ Given the constraint that the vector field must be divergence and curl free  $$ \nabla \cdot \mathbf{A} = 0$$  $$ \nabla \times \mathbf{A} = 0$$ this leads to $$\nabla^2 \phi = 0 $$ $$\nabla \times (\nabla \times \mathbf{B}) = 0$$  In application, such as potential flow theroy, it is often taken as a given that $\mathbf{B}=\mathbf{0}$ and $\mathbf{A} = \nabla \phi$.  I can see no reason that this is absolutely required.  Other than simplification, is there a justification for $\mathbf{B}=\mathbf{0}$?  For potential flow theory the choice leads to useful solutions, but is it ignoring a subset of possible solutions? Edit : There are certainly solutions that have non-zero $\mathbf{B}$ that satify the divergence free and curl free constraints.  For instance $$\phi = 0$$ $$\mathbf{B}=\alpha y \hat{i}+\beta x\hat{j}$$  which gives $$\mathbf{A} = \beta-\alpha$$ for constants $\alpha$ and $\beta$.  Any constant vector field satisfies the divergence and curl free constraint.  My intuition is that while $\mathbf{B}=0$ is not necessary, it simply does not add any new unique solution that can't be found by simply satisfying the constraints with $\phi$.","Helmholtz's theorem tells us that a sufficiently smooth vector field $\mathbf{A}$ can be decomposed into curl and gradient free parts as the gradient of a scalar potential plus the curl of a vector potential  $$ \mathbf{A} = \nabla \phi + \nabla \times \mathbf{B}$$ Given the constraint that the vector field must be divergence and curl free  $$ \nabla \cdot \mathbf{A} = 0$$  $$ \nabla \times \mathbf{A} = 0$$ this leads to $$\nabla^2 \phi = 0 $$ $$\nabla \times (\nabla \times \mathbf{B}) = 0$$  In application, such as potential flow theroy, it is often taken as a given that $\mathbf{B}=\mathbf{0}$ and $\mathbf{A} = \nabla \phi$.  I can see no reason that this is absolutely required.  Other than simplification, is there a justification for $\mathbf{B}=\mathbf{0}$?  For potential flow theory the choice leads to useful solutions, but is it ignoring a subset of possible solutions? Edit : There are certainly solutions that have non-zero $\mathbf{B}$ that satify the divergence free and curl free constraints.  For instance $$\phi = 0$$ $$\mathbf{B}=\alpha y \hat{i}+\beta x\hat{j}$$  which gives $$\mathbf{A} = \beta-\alpha$$ for constants $\alpha$ and $\beta$.  Any constant vector field satisfies the divergence and curl free constraint.  My intuition is that while $\mathbf{B}=0$ is not necessary, it simply does not add any new unique solution that can't be found by simply satisfying the constraints with $\phi$.",,"['multivariable-calculus', 'partial-differential-equations']"
81,Far-field Poynting vector for time varying source [check my math please],Far-field Poynting vector for time varying source [check my math please],,"This is a question from classical electrodynamics, but it's the maths of it I need some help in. I have fields $\rho\left(\vec{r},t\right)$, $\vec{J}\left(\vec{r},t\right)$, $\vec{E}\left(\vec{r},t\right)$ and $\vec{B}\left(\vec{r},t\right)$ in $\mathbb{R}^3$. The fields $\rho$ and $\vec{J}$ are related by the Continuity Equation : $$ -\dfrac {\partial \rho (\vec{r}, t)} {\partial t} = \nabla \cdot \vec{J} (\vec{r}, t) $$ Let us say $\mathbb{V}_s \subset \mathbb{R}^3$ is the smallest spherical volume such that: $$ \forall t, \qquad \forall \vec{r}\notin \mathbb{V}_s: \qquad\qquad \rho\left(\vec{r},t\right) = \vec{J}\left(\vec{r},t\right) = 0 $$ $$ \forall t, \qquad \forall \vec{r}\in \partial\mathbb{V}_s: \qquad\qquad \rho\left(\vec{r},t\right) = \vec{J}\left(\vec{r},t\right) = 0  $$ Since $\mathbb{V}_s$ is a sphere, I say that its center is at $\vec{r}_0$, and its diameter id $\mathcal{D}$. Now given the definitions: $$ \vec{r}_s \in \mathbb{V}_s,\qquad \vec{R} = \vec{r} - \vec{r}_s,\qquad R=\left|\vec{R}\right|, \qquad \hat{R}=\frac {\vec{R}} {R}, \qquad t_r = t - \frac {R} {c} $$ The fields $\vec{E}$ and $\vec{B}$ are as follows: $$ \vec{E}\left(\vec{r},t\right) = \frac {1} {4 \pi \epsilon_0} \iiint_{\mathbb{V}_s} {\left[ \frac {\rho (\vec{r}_s, t_r)} {R^2} \hat{R} + \frac {1} {R c} \frac {\partial \rho (\vec{r}_s, t_r) } {\partial t} \hat{R} - \frac {1} {R c^2} \frac {\partial \vec{J} (\vec{r}_s, t_r) } {\partial t} \right]} \space dV\left(\vec{r}_s\right) $$ $$ \vec{B}\left(\vec{r},t\right) = \frac {\mu_0} {4 \pi} \iiint_{\mathbb{V}_s} {\left[ \frac {\vec{J}  (\vec{r}_s, t_r)} {R^2} \times \hat{R} + \frac {1} {R c} \frac {\partial \vec{J}  (\vec{r}_s, t_r) } {\partial t} \times \hat{R} \right]} \space dV\left(\vec{r}_s\right) $$ (Physically, $\vec{E}$, $\vec{B}$. $\vec{J}$ & $\rho$ have the standard meanings from Electrodynamics. The equations above are the Jefimenko's equations ) Now the paper ""The Relation Between Expressions for Time-Dependent Electromagnetic Fields Given by Jefimenko and by Panofsky and Phillips"" by Kirk T. McDonald shows that the expression for $\vec{E}$ can be transformed into: $$ \scriptsize{ \vec{E}\left(\vec{r},t\right) = \frac {1} {4 \pi \epsilon_0} \iiint_{\mathbb{V}_s} {\left[ \frac {\rho (\vec{r}_s, t_r)} {R^2} \hat{R}  + \frac        { \left(\vec{J} (\vec{r}_s, t_r) \cdot \hat{R}\right)\hat{R} + \left(\vec{J} (\vec{r}_s, t_r) \times\hat{R}\right) \times \hat{R} }        {R^2 c}  + \frac {1} {R c^2} \left( \frac {\partial \vec{J} (\vec{r}_s, t_r) } {\partial t} \times \hat{R} \right) \times \hat{R} \right]} \space dV\left(\vec{r}_s\right) } $$ Now I want to compute the quantity $\vec{S}\left(\vec{r},t\right) =  \dfrac {\vec{E}\left(\vec{r},t\right) \times \vec{B}\left(\vec{r},t\right)} {\mu_0}$ for $\vec{r}$ for which $R \gg \mathcal{D}$. I figured that for such values of $\vec{r}$, the maximum possible difference between the values of $\dfrac 1 R$ for any two points $\vec{r}_s \in \mathbb{V}_s$ is: $$\frac{1}{R} - \frac{1}{R+\mathcal{D}} = \frac{\mathcal{D}}{R\left(R+\mathcal{D}\right)} = \frac{1}{R}\cfrac{ \color{red}{\frac{\mathcal{D}}{R}}}{1+\color{red}{\frac{\mathcal{D}}{R}}}$$ Similarly, the maximum possible angle between the values of $\hat{R}$ for any two points $\vec{r}_s \in \mathbb{V}_s$  is: $$ 2\;{\tan}^{-1}\left(\frac12 \color{red} {\frac{\mathcal{D}}{R}}\right) $$ When $R \gg \mathcal{D}$, $\dfrac {\mathcal{D}} R \approx 0$, and each of the two expressions above are $\approx 0$ as well. So $\dfrac 1 R$ and $\hat{R}$ are essentially independent of $\vec{r}_s$. We can, therefore, bring out all $\dfrac 1 R$ and $\hat{R}$ factors outside the integral sign. So if I define the following: $$ W\left(\vec{r},t\right) =  \iiint_{\mathbb{V}_s}  \rho \left( \vec{r}_s, t_r \right) \space dV\left(\vec{r}_s\right) $$ $$ \vec{Y}\left(\vec{r},t\right) =  \iiint_{\mathbb{V}_s}  \vec{J} \left( \vec{r}_s, t_r \right) \space dV\left(\vec{r}_s\right) $$ $$ \vec{Z}\left(\vec{r},t\right) =  \iiint_{\mathbb{V}_s}   \frac {\partial \vec{J} \left( \vec{r}_s, t_r \right) } {\partial t} \space dV\left(\vec{r}_s\right)  = \frac {\partial \vec{Y}\left(\vec{r},t\right)} {\partial t} $$ I can then say: $$ \vec{E}\left(\vec{r},t\right) = \frac {1} {4 \pi \epsilon_0} \left[    \frac {W (\vec{r}, t)} {R^2} \hat{R}     + \frac {\vec{Y} (\vec{r}, t) \cdot \hat{R}} {R^2 c} \hat{R}     + \frac {\left(\vec{Y}(\vec{r}, t) \times \hat{R}\right) \times \hat{R}} {R^2 c}     + \frac {\left(\vec{Z}(\vec{r}, t) \times \hat{R}\right) \times \hat{R}} {R c^2}   \right] $$ $$ \vec{B}\left(\vec{r},t\right) = \frac {\mu_0} {4 \pi} \left[     \frac {\vec{Y}  (\vec{r}, t) \times \hat{R}} {R^2}     + \frac {\vec{Z}(\vec{r}, t) \times \hat{R}} {R c}  \right] $$ Applying $\color{blue}{\left(\vec{a} \times \hat{u}\right) \times \hat{u} \equiv \left(\vec{a}\cdot\hat{u}\right)\hat{u} - \vec{a}}$, we can then say: $$ \vec{E}\left(\vec{r},t\right) = \frac {1} {4 \pi \epsilon_0} \left[  \left(     \frac {W (\vec{r}, t)} {R^2}     + 2\frac        {\vec{Y} (\vec{r}, t) \cdot \hat{R}}        {R^2 c}     + \frac       {\vec{Z}\left( \vec{r}, t \right)\cdot\hat{R}}       {R c^2}    \right) \hat{R}  - \frac 1 {Rc} \left(      \frac {\vec{Y} (\vec{r}, t) } {R}    + \frac {\vec{Z}(\vec{r}, t)} {c}    \right) \right] $$ $$ \vec{B}\left(\vec{r},t\right) = \frac {\mu_0} {4 \pi} \left[ \frac 1 R {\left( \frac {\vec{Y}  (\vec{r}, t)} {R} + \frac {\vec{Z}(\vec{r}, t)} {c} \right)} \times \hat{R} \right] $$ Thus, if I define: $$ \mathcal{U}\left(\vec{r},t\right) =   \color{blue}{   \vec{W} \left(\vec{r},t\right)    + \frac {\vec{Y} \left(\vec{r},t\right) \cdot \hat{R}} {c}  }  = \iiint_{\mathbb{V}_s} \left[    \rho \left( \vec{r}_s, t_r \right)    + \frac { \vec{J} \left( \vec{r}_s, t_r \right) \cdot \hat{R} } {c} \right] \space dV\left(\vec{r}_s\right) $$ $$ \vec{\mathcal{X}} \left(\vec{r},t\right) =   \color{blue}{     \frac {\vec{Y} \left(\vec{r},t\right)} {R}     + \frac {\vec{Z} \left(\vec{r},t\right)}  {c}   } = \iiint_{\mathbb{V}_s} \left[    \frac { \vec{J} \left( \vec{r}_s, t_r \right) } {R}    + \frac 1 c \frac {\partial \vec{J} \left( \vec{r}_s, t_r \right) } {\partial t} \right] \space dV\left(\vec{r}_s\right) $$ I get: $$ \vec{E}\left(\vec{r},t\right) = \frac {1} {4 \pi \epsilon_0}  \left[   \frac {\mathcal{U}\left(\vec{r},t\right)} {R^2} \hat{R}   + \frac {\left( \vec{\mathcal{X}}\left(\vec{r},t\right) \times \hat{R}\right) \times \hat{R}} {Rc}  \right] $$ $$ \vec{B}\left(\vec{r},t\right) = \frac {\mu_0} {4 \pi}  \left[   \frac {\vec{\mathcal{X}} \left(\vec{r},t\right) \times \hat{R}} {R}  \right] $$ Finally giving me: $$ \vec{S}\left(\vec{r},t\right) =   \mathcal{U} \left(\vec{r},t\right) \frac {\hat{R} \times \left( \vec{\mathcal{X}} \left(\vec{r},t\right) \times \hat{R} \right) } {16 \pi^2 \epsilon_0 R^3}   + \frac {\left| \vec{\mathcal{X}} \left(\vec{r},t\right) \times \hat{R} \right|^2}  {16 \pi^2 \epsilon_0 R^2 c} \hat{R} $$ Now my first question is: Is my math correct so far? Now, I define a sphere $\mathbb{V}$ with its center at $\vec{r}_0$ (the center of the source volume $\mathbb{V}_s$.) such that point $\vec{r}$ is on its surface. Then the unit vector $\hat{R}$ will actually be the unit normal $\hat{n}\left(\vec{r}\right)$ at $\vec{r}$. I want to compute the power passing the surface of $\mathbb{V}$, which should be: $$ \oint_{\partial\mathbb{V}}   \vec{S}\left(\vec{r},t\right) \cdot \hat{R} \;ds\left(\vec{r}\right) = \frac 1 {16 \pi^2 \epsilon_0 R^2 c} \oint_{\partial\mathbb{V}} \left| \frac {\vec{Y}\left(\vec{r},t\right) \times \hat{R}} {R} + \frac {\vec{Z}\left(\vec{r},t\right) \times \hat{R}} {c} \right|^2 \;ds\left(\vec{r}\right)  $$ My second question is: How do I parameterize and compute this surface integral? I have got to the following expression (it seems to me that when I am integrating over $\vec{r}$, I can still take $\dfrac 1 R$ out from under the integral, but not $\hat{R}$), but I'm stuck after this: $$ \color{blue}{\dfrac {\oint_{\partial\mathbb{V}} \left|\vec{Y}\left(\vec{r},t\right) \times \hat{R}\right|^2 \;ds\left(\vec{r}\right)} {16 \pi^2 \epsilon_0 R^4 c} + \dfrac {\oint_{\partial\mathbb{V}} \left(\vec{Y}\left(\vec{r},t\right) \times \hat{R}\right)\cdot\left(\vec{Z}\left(\vec{r},t\right) \times \hat{R}\right) \;ds\left(\vec{r}\right)} {8 \pi^2 \epsilon_0 R^3 c^2}} \color{red}{+ \dfrac {\oint_{\partial\mathbb{V}} \left|\vec{Z}\left(\vec{r},t\right) \times \hat{R}\right|^2 \;ds\left(\vec{r}\right)} {16 \pi^2 \epsilon_0 R^2 c^3}} $$ I expected to prove that the term in $\color{red}{red}$ is independent of $R$, and the term in $\color{blue}{blue}$ is 'reversible', by which I mean if there are two time instants $t_1$ and $t_2$ such that: $$\forall \vec{r}_s \in \mathbb{V}_s:\qquad \rho\left(t_1\right) = \rho\left(t_2\right), \qquad\vec{J}\left(t_1\right) = \vec{J}\left(t_2\right)$$ ... then I hoped to prove that: $$ \int_{t_1}^{t_2} \dfrac {\oint_{\partial\mathbb{V}} \left|\vec{Y}\left(\vec{r},t\right) \times \hat{R}\right|^2 \;ds\left(\vec{r}\right)} {16 \pi^2 \epsilon_0 R^4 c} + \dfrac {\oint_{\partial\mathbb{V}} \left(\vec{Y}\left(\vec{r},t\right) \times \hat{R}\right)\cdot\left(\vec{Z}\left(\vec{r},t\right) \times \hat{R}\right) \;ds\left(\vec{r}\right)} {8 \pi^2 \epsilon_0 R^3 c^2}\;dt = 0$$ ... but so far I have not been able to make any progress. Would appreciate any help...","This is a question from classical electrodynamics, but it's the maths of it I need some help in. I have fields $\rho\left(\vec{r},t\right)$, $\vec{J}\left(\vec{r},t\right)$, $\vec{E}\left(\vec{r},t\right)$ and $\vec{B}\left(\vec{r},t\right)$ in $\mathbb{R}^3$. The fields $\rho$ and $\vec{J}$ are related by the Continuity Equation : $$ -\dfrac {\partial \rho (\vec{r}, t)} {\partial t} = \nabla \cdot \vec{J} (\vec{r}, t) $$ Let us say $\mathbb{V}_s \subset \mathbb{R}^3$ is the smallest spherical volume such that: $$ \forall t, \qquad \forall \vec{r}\notin \mathbb{V}_s: \qquad\qquad \rho\left(\vec{r},t\right) = \vec{J}\left(\vec{r},t\right) = 0 $$ $$ \forall t, \qquad \forall \vec{r}\in \partial\mathbb{V}_s: \qquad\qquad \rho\left(\vec{r},t\right) = \vec{J}\left(\vec{r},t\right) = 0  $$ Since $\mathbb{V}_s$ is a sphere, I say that its center is at $\vec{r}_0$, and its diameter id $\mathcal{D}$. Now given the definitions: $$ \vec{r}_s \in \mathbb{V}_s,\qquad \vec{R} = \vec{r} - \vec{r}_s,\qquad R=\left|\vec{R}\right|, \qquad \hat{R}=\frac {\vec{R}} {R}, \qquad t_r = t - \frac {R} {c} $$ The fields $\vec{E}$ and $\vec{B}$ are as follows: $$ \vec{E}\left(\vec{r},t\right) = \frac {1} {4 \pi \epsilon_0} \iiint_{\mathbb{V}_s} {\left[ \frac {\rho (\vec{r}_s, t_r)} {R^2} \hat{R} + \frac {1} {R c} \frac {\partial \rho (\vec{r}_s, t_r) } {\partial t} \hat{R} - \frac {1} {R c^2} \frac {\partial \vec{J} (\vec{r}_s, t_r) } {\partial t} \right]} \space dV\left(\vec{r}_s\right) $$ $$ \vec{B}\left(\vec{r},t\right) = \frac {\mu_0} {4 \pi} \iiint_{\mathbb{V}_s} {\left[ \frac {\vec{J}  (\vec{r}_s, t_r)} {R^2} \times \hat{R} + \frac {1} {R c} \frac {\partial \vec{J}  (\vec{r}_s, t_r) } {\partial t} \times \hat{R} \right]} \space dV\left(\vec{r}_s\right) $$ (Physically, $\vec{E}$, $\vec{B}$. $\vec{J}$ & $\rho$ have the standard meanings from Electrodynamics. The equations above are the Jefimenko's equations ) Now the paper ""The Relation Between Expressions for Time-Dependent Electromagnetic Fields Given by Jefimenko and by Panofsky and Phillips"" by Kirk T. McDonald shows that the expression for $\vec{E}$ can be transformed into: $$ \scriptsize{ \vec{E}\left(\vec{r},t\right) = \frac {1} {4 \pi \epsilon_0} \iiint_{\mathbb{V}_s} {\left[ \frac {\rho (\vec{r}_s, t_r)} {R^2} \hat{R}  + \frac        { \left(\vec{J} (\vec{r}_s, t_r) \cdot \hat{R}\right)\hat{R} + \left(\vec{J} (\vec{r}_s, t_r) \times\hat{R}\right) \times \hat{R} }        {R^2 c}  + \frac {1} {R c^2} \left( \frac {\partial \vec{J} (\vec{r}_s, t_r) } {\partial t} \times \hat{R} \right) \times \hat{R} \right]} \space dV\left(\vec{r}_s\right) } $$ Now I want to compute the quantity $\vec{S}\left(\vec{r},t\right) =  \dfrac {\vec{E}\left(\vec{r},t\right) \times \vec{B}\left(\vec{r},t\right)} {\mu_0}$ for $\vec{r}$ for which $R \gg \mathcal{D}$. I figured that for such values of $\vec{r}$, the maximum possible difference between the values of $\dfrac 1 R$ for any two points $\vec{r}_s \in \mathbb{V}_s$ is: $$\frac{1}{R} - \frac{1}{R+\mathcal{D}} = \frac{\mathcal{D}}{R\left(R+\mathcal{D}\right)} = \frac{1}{R}\cfrac{ \color{red}{\frac{\mathcal{D}}{R}}}{1+\color{red}{\frac{\mathcal{D}}{R}}}$$ Similarly, the maximum possible angle between the values of $\hat{R}$ for any two points $\vec{r}_s \in \mathbb{V}_s$  is: $$ 2\;{\tan}^{-1}\left(\frac12 \color{red} {\frac{\mathcal{D}}{R}}\right) $$ When $R \gg \mathcal{D}$, $\dfrac {\mathcal{D}} R \approx 0$, and each of the two expressions above are $\approx 0$ as well. So $\dfrac 1 R$ and $\hat{R}$ are essentially independent of $\vec{r}_s$. We can, therefore, bring out all $\dfrac 1 R$ and $\hat{R}$ factors outside the integral sign. So if I define the following: $$ W\left(\vec{r},t\right) =  \iiint_{\mathbb{V}_s}  \rho \left( \vec{r}_s, t_r \right) \space dV\left(\vec{r}_s\right) $$ $$ \vec{Y}\left(\vec{r},t\right) =  \iiint_{\mathbb{V}_s}  \vec{J} \left( \vec{r}_s, t_r \right) \space dV\left(\vec{r}_s\right) $$ $$ \vec{Z}\left(\vec{r},t\right) =  \iiint_{\mathbb{V}_s}   \frac {\partial \vec{J} \left( \vec{r}_s, t_r \right) } {\partial t} \space dV\left(\vec{r}_s\right)  = \frac {\partial \vec{Y}\left(\vec{r},t\right)} {\partial t} $$ I can then say: $$ \vec{E}\left(\vec{r},t\right) = \frac {1} {4 \pi \epsilon_0} \left[    \frac {W (\vec{r}, t)} {R^2} \hat{R}     + \frac {\vec{Y} (\vec{r}, t) \cdot \hat{R}} {R^2 c} \hat{R}     + \frac {\left(\vec{Y}(\vec{r}, t) \times \hat{R}\right) \times \hat{R}} {R^2 c}     + \frac {\left(\vec{Z}(\vec{r}, t) \times \hat{R}\right) \times \hat{R}} {R c^2}   \right] $$ $$ \vec{B}\left(\vec{r},t\right) = \frac {\mu_0} {4 \pi} \left[     \frac {\vec{Y}  (\vec{r}, t) \times \hat{R}} {R^2}     + \frac {\vec{Z}(\vec{r}, t) \times \hat{R}} {R c}  \right] $$ Applying $\color{blue}{\left(\vec{a} \times \hat{u}\right) \times \hat{u} \equiv \left(\vec{a}\cdot\hat{u}\right)\hat{u} - \vec{a}}$, we can then say: $$ \vec{E}\left(\vec{r},t\right) = \frac {1} {4 \pi \epsilon_0} \left[  \left(     \frac {W (\vec{r}, t)} {R^2}     + 2\frac        {\vec{Y} (\vec{r}, t) \cdot \hat{R}}        {R^2 c}     + \frac       {\vec{Z}\left( \vec{r}, t \right)\cdot\hat{R}}       {R c^2}    \right) \hat{R}  - \frac 1 {Rc} \left(      \frac {\vec{Y} (\vec{r}, t) } {R}    + \frac {\vec{Z}(\vec{r}, t)} {c}    \right) \right] $$ $$ \vec{B}\left(\vec{r},t\right) = \frac {\mu_0} {4 \pi} \left[ \frac 1 R {\left( \frac {\vec{Y}  (\vec{r}, t)} {R} + \frac {\vec{Z}(\vec{r}, t)} {c} \right)} \times \hat{R} \right] $$ Thus, if I define: $$ \mathcal{U}\left(\vec{r},t\right) =   \color{blue}{   \vec{W} \left(\vec{r},t\right)    + \frac {\vec{Y} \left(\vec{r},t\right) \cdot \hat{R}} {c}  }  = \iiint_{\mathbb{V}_s} \left[    \rho \left( \vec{r}_s, t_r \right)    + \frac { \vec{J} \left( \vec{r}_s, t_r \right) \cdot \hat{R} } {c} \right] \space dV\left(\vec{r}_s\right) $$ $$ \vec{\mathcal{X}} \left(\vec{r},t\right) =   \color{blue}{     \frac {\vec{Y} \left(\vec{r},t\right)} {R}     + \frac {\vec{Z} \left(\vec{r},t\right)}  {c}   } = \iiint_{\mathbb{V}_s} \left[    \frac { \vec{J} \left( \vec{r}_s, t_r \right) } {R}    + \frac 1 c \frac {\partial \vec{J} \left( \vec{r}_s, t_r \right) } {\partial t} \right] \space dV\left(\vec{r}_s\right) $$ I get: $$ \vec{E}\left(\vec{r},t\right) = \frac {1} {4 \pi \epsilon_0}  \left[   \frac {\mathcal{U}\left(\vec{r},t\right)} {R^2} \hat{R}   + \frac {\left( \vec{\mathcal{X}}\left(\vec{r},t\right) \times \hat{R}\right) \times \hat{R}} {Rc}  \right] $$ $$ \vec{B}\left(\vec{r},t\right) = \frac {\mu_0} {4 \pi}  \left[   \frac {\vec{\mathcal{X}} \left(\vec{r},t\right) \times \hat{R}} {R}  \right] $$ Finally giving me: $$ \vec{S}\left(\vec{r},t\right) =   \mathcal{U} \left(\vec{r},t\right) \frac {\hat{R} \times \left( \vec{\mathcal{X}} \left(\vec{r},t\right) \times \hat{R} \right) } {16 \pi^2 \epsilon_0 R^3}   + \frac {\left| \vec{\mathcal{X}} \left(\vec{r},t\right) \times \hat{R} \right|^2}  {16 \pi^2 \epsilon_0 R^2 c} \hat{R} $$ Now my first question is: Is my math correct so far? Now, I define a sphere $\mathbb{V}$ with its center at $\vec{r}_0$ (the center of the source volume $\mathbb{V}_s$.) such that point $\vec{r}$ is on its surface. Then the unit vector $\hat{R}$ will actually be the unit normal $\hat{n}\left(\vec{r}\right)$ at $\vec{r}$. I want to compute the power passing the surface of $\mathbb{V}$, which should be: $$ \oint_{\partial\mathbb{V}}   \vec{S}\left(\vec{r},t\right) \cdot \hat{R} \;ds\left(\vec{r}\right) = \frac 1 {16 \pi^2 \epsilon_0 R^2 c} \oint_{\partial\mathbb{V}} \left| \frac {\vec{Y}\left(\vec{r},t\right) \times \hat{R}} {R} + \frac {\vec{Z}\left(\vec{r},t\right) \times \hat{R}} {c} \right|^2 \;ds\left(\vec{r}\right)  $$ My second question is: How do I parameterize and compute this surface integral? I have got to the following expression (it seems to me that when I am integrating over $\vec{r}$, I can still take $\dfrac 1 R$ out from under the integral, but not $\hat{R}$), but I'm stuck after this: $$ \color{blue}{\dfrac {\oint_{\partial\mathbb{V}} \left|\vec{Y}\left(\vec{r},t\right) \times \hat{R}\right|^2 \;ds\left(\vec{r}\right)} {16 \pi^2 \epsilon_0 R^4 c} + \dfrac {\oint_{\partial\mathbb{V}} \left(\vec{Y}\left(\vec{r},t\right) \times \hat{R}\right)\cdot\left(\vec{Z}\left(\vec{r},t\right) \times \hat{R}\right) \;ds\left(\vec{r}\right)} {8 \pi^2 \epsilon_0 R^3 c^2}} \color{red}{+ \dfrac {\oint_{\partial\mathbb{V}} \left|\vec{Z}\left(\vec{r},t\right) \times \hat{R}\right|^2 \;ds\left(\vec{r}\right)} {16 \pi^2 \epsilon_0 R^2 c^3}} $$ I expected to prove that the term in $\color{red}{red}$ is independent of $R$, and the term in $\color{blue}{blue}$ is 'reversible', by which I mean if there are two time instants $t_1$ and $t_2$ such that: $$\forall \vec{r}_s \in \mathbb{V}_s:\qquad \rho\left(t_1\right) = \rho\left(t_2\right), \qquad\vec{J}\left(t_1\right) = \vec{J}\left(t_2\right)$$ ... then I hoped to prove that: $$ \int_{t_1}^{t_2} \dfrac {\oint_{\partial\mathbb{V}} \left|\vec{Y}\left(\vec{r},t\right) \times \hat{R}\right|^2 \;ds\left(\vec{r}\right)} {16 \pi^2 \epsilon_0 R^4 c} + \dfrac {\oint_{\partial\mathbb{V}} \left(\vec{Y}\left(\vec{r},t\right) \times \hat{R}\right)\cdot\left(\vec{Z}\left(\vec{r},t\right) \times \hat{R}\right) \;ds\left(\vec{r}\right)} {8 \pi^2 \epsilon_0 R^3 c^2}\;dt = 0$$ ... but so far I have not been able to make any progress. Would appreciate any help...",,"['multivariable-calculus', 'vector-analysis']"
82,Taylor's Theorem for Multivariate Functions,Taylor's Theorem for Multivariate Functions,,"Please look at this theorem in Wiki regarding Taylor's theorem generalized to multivariate functions: Multivariate version of Taylor's Theorem The version stated there is one that I'm not familiar with - most versions require that the $k+1$-th partials exist (and/or continuous) and the remainder term is different (due to the additional conditions). For the 1-dimensional case, the $h_\alpha$ is easy to obtain. But for the multivariate case, how does $h_{\alpha}$ look like? The wiki article does not have any citations for that theorem so I'm wondering if anyone can point me to a source.","Please look at this theorem in Wiki regarding Taylor's theorem generalized to multivariate functions: Multivariate version of Taylor's Theorem The version stated there is one that I'm not familiar with - most versions require that the $k+1$-th partials exist (and/or continuous) and the remainder term is different (due to the additional conditions). For the 1-dimensional case, the $h_\alpha$ is easy to obtain. But for the multivariate case, how does $h_{\alpha}$ look like? The wiki article does not have any citations for that theorem so I'm wondering if anyone can point me to a source.",,"['multivariable-calculus', 'taylor-expansion']"
83,How to solve this special form of multivariate quadratic equations?,How to solve this special form of multivariate quadratic equations?,,"Suppose there are $M$ known real vectors $$ x^{i}=[x_{1}^{i},x_{2}^{i},\cdots,x_{N}^{i}],i=1,2\cdots,M $$ in N-dimensional vector space and $M\geq 2N$. I want to know whether they span two orthogonal hyperplanes, that is some of these vectors span one hyperplane while the rest span another and these two hyperplanes are mutually orthogonal (I mean their normal vectors are orthogonal, maybe 'vertical' is a better word). So I write the unknown normal vectors to the two hyperplanes as $$ n=[n_{1},n_{2},\cdots,n_{N}]\\ m=[m_{1},m_{2},\cdots,m_{N}] $$ and they are orthogonal $$ m\cdot n=0 $$ Now if $x$ are all on these two hyperplanes then the following equations must have a solution $$ (m\cdot x^{i})(n\cdot x^{i})=0 $$ for all $x^{i}$. Expanding the above equations, $$ (n_{1}x_{1}^{i}+n_{2}x_{2}^{i}+\cdots +n_{N}x_{N}^{i})(m_{1}x_{1}^{i}+m_{2}x_{2}^{i}+\cdots +m_{N}x_{N}^{i})=0\\ \sum_{i=1}^{N}n_{i}m_{i}=0 $$ are multivariate quadratic equations of $n_{i}$ and $m_{i}$. Now my question is: How do I know whether these equations are solvable or not given certain $x^{i}$s? Are there any constraints that can be placed on $x^{i}$s so that these equations have at least one real solution? Solving multivariate quadratic equations is a NP-hard problem in general. But I think this special form maybe not that hard. English is not my mother language so I've tried to explain my question as clear as possible. If there are any confusing points in my description, please leave a comment and I'll answer you as soon as I can. Thank you! Supplement: Maybe an example will help. Consider one vector $x^{1}=[1,1]$ in 2-dimensional real vector space. The above equations should be $$ (n_{1}+n_{2})(m_{1}+m_{2})=0\\ n_{1}\cdot m_{1}+n_{2}\cdot m_{2}=0 $$ Some simple calculation gives $$ \frac{n_{1}}{n_{2}}=-\frac{m_{1}}{m_{2}}=-\frac{m_{2}}{m_{1}}=\pm 1 $$ Because only the orientations of $n$ and $m$ are cared about, the ratios between entries are enough to specify the two vectors. Now if we add another vector $x^{2}=[0,1]$ to the original one-vector set, the equations should be $$ (n_{1}+n_{2})(m_{1}+m_{2})=0\\ n_{2}\cdot m_{2}=0\\ n_{1}\cdot m_{1}+n_{2}\cdot m_{2}=0 $$ This time there is no real solution. It is harder to tell whether there is a real solution for these equations in higher dimensional space.","Suppose there are $M$ known real vectors $$ x^{i}=[x_{1}^{i},x_{2}^{i},\cdots,x_{N}^{i}],i=1,2\cdots,M $$ in N-dimensional vector space and $M\geq 2N$. I want to know whether they span two orthogonal hyperplanes, that is some of these vectors span one hyperplane while the rest span another and these two hyperplanes are mutually orthogonal (I mean their normal vectors are orthogonal, maybe 'vertical' is a better word). So I write the unknown normal vectors to the two hyperplanes as $$ n=[n_{1},n_{2},\cdots,n_{N}]\\ m=[m_{1},m_{2},\cdots,m_{N}] $$ and they are orthogonal $$ m\cdot n=0 $$ Now if $x$ are all on these two hyperplanes then the following equations must have a solution $$ (m\cdot x^{i})(n\cdot x^{i})=0 $$ for all $x^{i}$. Expanding the above equations, $$ (n_{1}x_{1}^{i}+n_{2}x_{2}^{i}+\cdots +n_{N}x_{N}^{i})(m_{1}x_{1}^{i}+m_{2}x_{2}^{i}+\cdots +m_{N}x_{N}^{i})=0\\ \sum_{i=1}^{N}n_{i}m_{i}=0 $$ are multivariate quadratic equations of $n_{i}$ and $m_{i}$. Now my question is: How do I know whether these equations are solvable or not given certain $x^{i}$s? Are there any constraints that can be placed on $x^{i}$s so that these equations have at least one real solution? Solving multivariate quadratic equations is a NP-hard problem in general. But I think this special form maybe not that hard. English is not my mother language so I've tried to explain my question as clear as possible. If there are any confusing points in my description, please leave a comment and I'll answer you as soon as I can. Thank you! Supplement: Maybe an example will help. Consider one vector $x^{1}=[1,1]$ in 2-dimensional real vector space. The above equations should be $$ (n_{1}+n_{2})(m_{1}+m_{2})=0\\ n_{1}\cdot m_{1}+n_{2}\cdot m_{2}=0 $$ Some simple calculation gives $$ \frac{n_{1}}{n_{2}}=-\frac{m_{1}}{m_{2}}=-\frac{m_{2}}{m_{1}}=\pm 1 $$ Because only the orientations of $n$ and $m$ are cared about, the ratios between entries are enough to specify the two vectors. Now if we add another vector $x^{2}=[0,1]$ to the original one-vector set, the equations should be $$ (n_{1}+n_{2})(m_{1}+m_{2})=0\\ n_{2}\cdot m_{2}=0\\ n_{1}\cdot m_{1}+n_{2}\cdot m_{2}=0 $$ This time there is no real solution. It is harder to tell whether there is a real solution for these equations in higher dimensional space.",,['multivariable-calculus']
84,How to show that the Hessian matrix of $G$ is positive definite?,How to show that the Hessian matrix of  is positive definite?,G,"Let $\{g_i:X\subset\Bbb R\to\Bbb R;\;i=1,\ldots,m\}$ be a linearly independent set of real functions. Given $n$ points $(x_1,y_1),\ldots,(x_n,y_n)\in X,$ consider the following function $$G(\beta_1,\ldots,\beta_n)=\sum_{k=1}^n\left(\sum_{l=1}^m\left[ \beta_lg_l(x_k)-y_k\right]\right)^2$$ I need to prove that $G$ attains a local minimum. For this, I need to show that the Hessian matrix of $G$ is positive definite, but I'm not able to do. I'm trying to show it, but I'm able to prove that it's just positive semidefinite (for this, I'm doing things like this and this ). Can someone help me show that the Hessian matrix of $G$ is positive definite? Notice that $$\frac{\partial G}{\partial \beta_i}=\sum_{k=1}^n\left(2g_i(x_k)\sum_{l=1}^m\left[ \beta_lg_l(x_k)-y_k\right]\right)$$ and $$\frac{\partial^2 G}{\partial \beta_j\beta_i}=2\sum_{k=1}^ng_i(x_k)g_j(x_k)$$ Hence, the Hessian matrix of $G$ is $$H=2 \begin{bmatrix} \sum_{k=1}^ng_1(x_k)^2 & \sum_{k=1}^ng_1(x_k)g_2(x_k) & \cdots & \sum_{k=1}^ng_1(x_k)g_m(x_k)\\  \sum_{k=1}^ng_2(x_k)g_1(x_k) & \sum_{k=1}^ng_2(x_k)^2 & \cdots & \sum_{k=1}^ng_2(x_k)g_m(x_k)\\   &  & \vdots & \\  \sum_{k=1}^ng_m(x_k)g_1(x_k) & \sum_{k=1}^ng_m(x_k)g_2(x_k) & \cdots & \sum_{k=1}^ng_m(x_k)^2 \end{bmatrix}$$ Thanks.","Let be a linearly independent set of real functions. Given points consider the following function I need to prove that attains a local minimum. For this, I need to show that the Hessian matrix of is positive definite, but I'm not able to do. I'm trying to show it, but I'm able to prove that it's just positive semidefinite (for this, I'm doing things like this and this ). Can someone help me show that the Hessian matrix of is positive definite? Notice that and Hence, the Hessian matrix of is Thanks.","\{g_i:X\subset\Bbb R\to\Bbb R;\;i=1,\ldots,m\} n (x_1,y_1),\ldots,(x_n,y_n)\in X, G(\beta_1,\ldots,\beta_n)=\sum_{k=1}^n\left(\sum_{l=1}^m\left[ \beta_lg_l(x_k)-y_k\right]\right)^2 G G G \frac{\partial G}{\partial \beta_i}=\sum_{k=1}^n\left(2g_i(x_k)\sum_{l=1}^m\left[ \beta_lg_l(x_k)-y_k\right]\right) \frac{\partial^2 G}{\partial \beta_j\beta_i}=2\sum_{k=1}^ng_i(x_k)g_j(x_k) G H=2
\begin{bmatrix}
\sum_{k=1}^ng_1(x_k)^2 & \sum_{k=1}^ng_1(x_k)g_2(x_k) & \cdots & \sum_{k=1}^ng_1(x_k)g_m(x_k)\\ 
\sum_{k=1}^ng_2(x_k)g_1(x_k) & \sum_{k=1}^ng_2(x_k)^2 & \cdots & \sum_{k=1}^ng_2(x_k)g_m(x_k)\\ 
 &  & \vdots & \\ 
\sum_{k=1}^ng_m(x_k)g_1(x_k) & \sum_{k=1}^ng_m(x_k)g_2(x_k) & \cdots & \sum_{k=1}^ng_m(x_k)^2
\end{bmatrix}","['multivariable-calculus', 'optimization', 'numerical-methods', 'numerical-linear-algebra']"
85,"Mean value properties of $f(t)=(\cos t,\sin t)$ [duplicate]",Mean value properties of  [duplicate],"f(t)=(\cos t,\sin t)","This question already has answers here : continuous function from $[\pi,2\pi]\to \mathbb{R}^2$ (2 answers) Closed 6 years ago . I am stuck on the following question which I came across in a recent exam paper: Let $f \colon [\pi,2 \pi] \to \Bbb R^2$ be the function $f(t)=(\cos t,\sin t)$ .Then which of the following are necessarily correct? The options are: There exists $t_0 \in [\pi,2\pi]$ such that $f'(t_0)=\frac{f(2 \pi)-f(\pi)}{\pi}$ There exists no $t_0 \in [\pi,2\pi]$ such  that $f'(t_0)=\frac{f(2 \pi)-f(\pi)}{\pi}$ There exists $t_0 \in [\pi,2\pi]$ such that $||f(2 \pi)-f(\pi)|| \le \pi ||f'(t_0)||$ $f'(t)=(-\sin t,\cos t) \,\,\forall t \in [\pi,2\pi]$ . MY ATTEMPT: Clearly option 4 is correct but I am not sure how to prove/disprove for other options. Looking at options 1,2,3 I think mean value theorem has a role to play.  Can someone explain me with some details. My edit:  Option 3 appears to be true since $||f(2 \pi)-f(\pi)||=||(1,0)-(-1,0)||=||(2,0)||=2$ and $\pi ||f'(t_0)||=\pi \,\,\text{since}\, ||f'(t)||=||(- \sin t,\cos t)||=1$ and so $||f(2 \pi)-f(\pi)|| \le \pi ||f'(t_0)||$ is true. Option 1 makes no sense after putting the values of $\pi, 2\pi$ in $f$ . So option 2 appears to be correct. Am I going in the right direction? Is there any other elegant way of tackling this problem?","This question already has answers here : continuous function from $[\pi,2\pi]\to \mathbb{R}^2$ (2 answers) Closed 6 years ago . I am stuck on the following question which I came across in a recent exam paper: Let be the function .Then which of the following are necessarily correct? The options are: There exists such that There exists no such  that There exists such that . MY ATTEMPT: Clearly option 4 is correct but I am not sure how to prove/disprove for other options. Looking at options 1,2,3 I think mean value theorem has a role to play.  Can someone explain me with some details. My edit:  Option 3 appears to be true since and and so is true. Option 1 makes no sense after putting the values of in . So option 2 appears to be correct. Am I going in the right direction? Is there any other elegant way of tackling this problem?","f \colon [\pi,2 \pi] \to \Bbb R^2 f(t)=(\cos t,\sin t) t_0 \in [\pi,2\pi] f'(t_0)=\frac{f(2 \pi)-f(\pi)}{\pi} t_0 \in [\pi,2\pi] f'(t_0)=\frac{f(2 \pi)-f(\pi)}{\pi} t_0 \in [\pi,2\pi] ||f(2 \pi)-f(\pi)|| \le \pi ||f'(t_0)|| f'(t)=(-\sin t,\cos t) \,\,\forall t \in [\pi,2\pi] ||f(2 \pi)-f(\pi)||=||(1,0)-(-1,0)||=||(2,0)||=2 \pi ||f'(t_0)||=\pi \,\,\text{since}\, ||f'(t)||=||(- \sin t,\cos t)||=1 ||f(2 \pi)-f(\pi)|| \le \pi ||f'(t_0)|| \pi, 2\pi f","['calculus', 'real-analysis', 'multivariable-calculus']"
86,Divergence theorem in volume integral,Divergence theorem in volume integral,,"We have a partial differential equation  \begin{equation} \nabla \cdot (p_1^2\nabla\alpha)=0\,. \end{equation} Question: from this equation how can I write the following condition? \begin{equation} \int_\Omega\alpha\nabla \cdot(p_1^2\nabla\alpha)= \int_{\partial\Omega}\alpha p_1^2 n\cdot\nabla\alpha -\int_\Omega p_1^2(\nabla\alpha)^2=0 \,. \end{equation} $p_1$ and $\alpha$ are position dependent variable.","We have a partial differential equation  \begin{equation} \nabla \cdot (p_1^2\nabla\alpha)=0\,. \end{equation} Question: from this equation how can I write the following condition? \begin{equation} \int_\Omega\alpha\nabla \cdot(p_1^2\nabla\alpha)= \int_{\partial\Omega}\alpha p_1^2 n\cdot\nabla\alpha -\int_\Omega p_1^2(\nabla\alpha)^2=0 \,. \end{equation} $p_1$ and $\alpha$ are position dependent variable.",,['multivariable-calculus']
87,Proof of the arc length parametrization is $1$,Proof of the arc length parametrization is,1,"Let $\gamma : [a,b] \to \mathbb R^n$ be a regular curve. Let $p:[a,b] \to [0, p(b)]$ be the map $p(t) = \int_{a}^t \|\gamma' (s) \|ds$. Then $p^{-1}: [0,p(b)] \to [a,b]$. I tried to show that $\|\gamma'(p^{-1}(s))\| = 1$ (for all $s\in[a,b]$). Can you check my work please? Proof is: By definitions, if $s=p(t)$ then $p'(t) = \|\gamma' (t) \|$ for all $t \in [a,b]$ and $\gamma'(p^{-1}(s)) = {d \over ds}\gamma(p^{-1}(s)) = {d p^{-1} (s) \over ds } {d \gamma(p^{-1}) \over d p^{-1}} = {1 \over p' (p^{-1} (s))} {d \gamma (p^{-1}) \over dp^{-1}}$ therefore  $$\|\gamma'(p^{-1}(s))\| =  {1 \over \| p' (p^{-1} (s))\|} \left \| {d \gamma (p^{-1})  \over  dp^{-1} } \right \|= {1 \over  \| p' (p^{-1} (p(t)))\| } \|\gamma' (t)\|= {\|\gamma'(t)\| \over \|p'(t)\|}= {\|\gamma' (t)\| \over \|\|\gamma' (t)\|\|}1$$.","Let $\gamma : [a,b] \to \mathbb R^n$ be a regular curve. Let $p:[a,b] \to [0, p(b)]$ be the map $p(t) = \int_{a}^t \|\gamma' (s) \|ds$. Then $p^{-1}: [0,p(b)] \to [a,b]$. I tried to show that $\|\gamma'(p^{-1}(s))\| = 1$ (for all $s\in[a,b]$). Can you check my work please? Proof is: By definitions, if $s=p(t)$ then $p'(t) = \|\gamma' (t) \|$ for all $t \in [a,b]$ and $\gamma'(p^{-1}(s)) = {d \over ds}\gamma(p^{-1}(s)) = {d p^{-1} (s) \over ds } {d \gamma(p^{-1}) \over d p^{-1}} = {1 \over p' (p^{-1} (s))} {d \gamma (p^{-1}) \over dp^{-1}}$ therefore  $$\|\gamma'(p^{-1}(s))\| =  {1 \over \| p' (p^{-1} (s))\|} \left \| {d \gamma (p^{-1})  \over  dp^{-1} } \right \|= {1 \over  \| p' (p^{-1} (p(t)))\| } \|\gamma' (t)\|= {\|\gamma'(t)\| \over \|p'(t)\|}= {\|\gamma' (t)\| \over \|\|\gamma' (t)\|\|}1$$.",,"['differential-geometry', 'multivariable-calculus']"
88,Integration over a surface,Integration over a surface,,"Let $S$ be given by $$S= \left[(x,y,z) \in \Bbb{R}\;|\; x^2+y^2+z^2+xy+xz+yz=\frac12 \right]$$ and  $$\omega = xdy \wedge dz\, -\, \frac {2z}{y^3} \, dx\wedge dy \,+\, \frac1{y^2}dz\wedge dx $$ Explain how to give an orientation for $S$ and compute $\int_S \omega$ with respect to that orientation. I know it involves Stokes's theorem and that $d\omega = dx \wedge dy \wedge dz$ but I'm kind lost about the orientation and actually computing it.","Let $S$ be given by $$S= \left[(x,y,z) \in \Bbb{R}\;|\; x^2+y^2+z^2+xy+xz+yz=\frac12 \right]$$ and  $$\omega = xdy \wedge dz\, -\, \frac {2z}{y^3} \, dx\wedge dy \,+\, \frac1{y^2}dz\wedge dx $$ Explain how to give an orientation for $S$ and compute $\int_S \omega$ with respect to that orientation. I know it involves Stokes's theorem and that $d\omega = dx \wedge dy \wedge dz$ but I'm kind lost about the orientation and actually computing it.",,"['multivariable-calculus', 'differential-forms']"
89,Spherical coordinates grad and div.,Spherical coordinates grad and div.,,"Struggling with the following: Prove the identity $$ \nabla = e_{r}(e_{r} \cdot \nabla) + e_{\theta}(e_{\theta} \cdot \nabla) + e_{\phi}(e_{\phi} \cdot \nabla).$$ Given the vector fields $F=F_{r}e_{r}+F_{\theta}e_{\theta}+ F_{\phi}e_{\phi}$ show that $$ \nabla \cdot F=\frac{1}{r^{2}}\frac \partial {{\partial r}}(r^{2}F_r)+\frac{1}{r\sin\theta}\frac \partial {\partial \theta}(\sin\theta F_\theta)+\frac{1}{r\sin\theta}\frac \partial {\partial\phi} $$ Any help will be most appreciated, many thanks.","Struggling with the following: Prove the identity $$ \nabla = e_{r}(e_{r} \cdot \nabla) + e_{\theta}(e_{\theta} \cdot \nabla) + e_{\phi}(e_{\phi} \cdot \nabla).$$ Given the vector fields $F=F_{r}e_{r}+F_{\theta}e_{\theta}+ F_{\phi}e_{\phi}$ show that $$ \nabla \cdot F=\frac{1}{r^{2}}\frac \partial {{\partial r}}(r^{2}F_r)+\frac{1}{r\sin\theta}\frac \partial {\partial \theta}(\sin\theta F_\theta)+\frac{1}{r\sin\theta}\frac \partial {\partial\phi} $$ Any help will be most appreciated, many thanks.",,"['calculus', 'multivariable-calculus', 'spherical-coordinates']"
90,Integral when variable of integration is a multivariable function,Integral when variable of integration is a multivariable function,,"I recently ran into a kind of integral that I've never encountered before. How should the following integral be expressed as a ""normal"" double integral? $\iint \mathrm d f(u,v)$ where $f:\mathbb{R}^2\rightarrow \mathbb{R}$ The only thing I can think of that might be correct is $\iint \frac{\partial f}{\partial u} \frac{\partial f}{\partial v} \mathrm du \ \mathrm dv$ but I cannot motivate this and I've had no luck with my reference books nor with Google. [Edit] The following comes from http://dx.doi.org/10.1006/jmva.1994.1031 $$\hat V(t) = n(1-\hat F_n(t))^2 \int_0^t\int_0^t \frac{d\hat H_n(u,v)}{Y_n(u) Y_n(v)}$$ where $\hat F_n(t) \in \mathbb{R}$ is a distribution function, $Y_n(t) \in \mathbb{N}$ is a count and $\hat H_n$ is defined as $$\hat H_n(s,t) = \frac{1}{n}\sum_{i=1}^m\sum_{j=1}^{K_i}\sum_{l=1}^{K_i} \left\{ I_{(Z_{ij}\le s,\Delta_{ij}=1)} - \int_0^s I_{(Z_{ij}\ge u)} d\hat\Lambda_n(u) \right\} $$ $$ \times \left\{ I_{(Z_{il}\le t,\Delta_{il}=1)} - \int_0^t I_{(Z_{il}\ge u)} d\hat\Lambda_n(u) \right\}$$ where $I\in \{0,1\}$ is the indicator function and $\hat\Lambda_n \in \mathbb{R}$ is the so-called Nelson estimator. $Z_{ij} \in \mathbb{R}$ is a time and $\Delta_{ij} \in \{0,1\}$ is a censoring indicator.","I recently ran into a kind of integral that I've never encountered before. How should the following integral be expressed as a ""normal"" double integral? $\iint \mathrm d f(u,v)$ where $f:\mathbb{R}^2\rightarrow \mathbb{R}$ The only thing I can think of that might be correct is $\iint \frac{\partial f}{\partial u} \frac{\partial f}{\partial v} \mathrm du \ \mathrm dv$ but I cannot motivate this and I've had no luck with my reference books nor with Google. [Edit] The following comes from http://dx.doi.org/10.1006/jmva.1994.1031 $$\hat V(t) = n(1-\hat F_n(t))^2 \int_0^t\int_0^t \frac{d\hat H_n(u,v)}{Y_n(u) Y_n(v)}$$ where $\hat F_n(t) \in \mathbb{R}$ is a distribution function, $Y_n(t) \in \mathbb{N}$ is a count and $\hat H_n$ is defined as $$\hat H_n(s,t) = \frac{1}{n}\sum_{i=1}^m\sum_{j=1}^{K_i}\sum_{l=1}^{K_i} \left\{ I_{(Z_{ij}\le s,\Delta_{ij}=1)} - \int_0^s I_{(Z_{ij}\ge u)} d\hat\Lambda_n(u) \right\} $$ $$ \times \left\{ I_{(Z_{il}\le t,\Delta_{il}=1)} - \int_0^t I_{(Z_{il}\ge u)} d\hat\Lambda_n(u) \right\}$$ where $I\in \{0,1\}$ is the indicator function and $\hat\Lambda_n \in \mathbb{R}$ is the so-called Nelson estimator. $Z_{ij} \in \mathbb{R}$ is a time and $\Delta_{ij} \in \{0,1\}$ is a censoring indicator.",,"['real-analysis', 'integration', 'multivariable-calculus']"
91,Extremal curve passing through a set of points,Extremal curve passing through a set of points,,"I'm having trouble recasting the following question in a form amenable to the calculus of variations. Question: Given a set of $n$ points $P=\{(x_1,y_1),..(x_n,y_n)\}$ what is the curve passing through these points such that: (C1) its length is minimal (C2) the maximum curvature it takes on, as it interpolates the above points, is as small as possible? (Note: this is related to my earlier MO question here , I wasn't sure if this was good enough for MO.) My partial answer: Suppose we express the curve in the form $y=f(x)$; suppose further that $L(y)$ and $\kappa(y)$ denote length and curvature as a function of $y$. Then, since we need to reduce both length and curvature, we need to minimize $L(y) + \lambda \kappa(y)$ over the interval $[x_1,x_n]$. This reduces to minimizing $F(y) = \int_{x1}^{x_n} \sqrt{1+y'^2} dx  + \lambda \frac{ y''} { (1+y'^2)^{3/2}}$. Doubts: (1) Is $F(y)$ the correct functional? It seems not. (2) How do I alter $F(y)$ to ensure that it passes through all points in $P$?","I'm having trouble recasting the following question in a form amenable to the calculus of variations. Question: Given a set of $n$ points $P=\{(x_1,y_1),..(x_n,y_n)\}$ what is the curve passing through these points such that: (C1) its length is minimal (C2) the maximum curvature it takes on, as it interpolates the above points, is as small as possible? (Note: this is related to my earlier MO question here , I wasn't sure if this was good enough for MO.) My partial answer: Suppose we express the curve in the form $y=f(x)$; suppose further that $L(y)$ and $\kappa(y)$ denote length and curvature as a function of $y$. Then, since we need to reduce both length and curvature, we need to minimize $L(y) + \lambda \kappa(y)$ over the interval $[x_1,x_n]$. This reduces to minimizing $F(y) = \int_{x1}^{x_n} \sqrt{1+y'^2} dx  + \lambda \frac{ y''} { (1+y'^2)^{3/2}}$. Doubts: (1) Is $F(y)$ the correct functional? It seems not. (2) How do I alter $F(y)$ to ensure that it passes through all points in $P$?",,"['optimization', 'multivariable-calculus']"
92,Making the rigorous link between the conceptual interpretation of curl and the formula,Making the rigorous link between the conceptual interpretation of curl and the formula,,"EDIT: I would like to clarify for potential new readers what the gist of this question is, and how it differs from other questions about curl on this site. Namely, I'm confused about: The specifics of the physical interpretation of curl (is it infinitesimal torque, average infinitesimal torque, average infinitesimal angular acceleration , etc.) How this specific physical interpretation translates into a formula (ideally, the integral below as seen on Wikipedia) How this formula becomes the usual definition of curl. I'm not confused about what curl is on a general level, I'm confused about why the formula for curl coincides with the physical interpretation , on an intuitive and rigorous basis. I'm having a bit of difficulty making the connection between the intuitive idea of the curl of a vector field (as the torque experienced on an infinitesimally small paddle wheel placed at a point in the vector field), and the formula for it (I'll stick to 2-D first off, which is how the MIT OpenCourseware lectures define it at first): $$ \nabla\times F = N_x-M_y $$ where $F(x,y)=(M(x,y),N(x,y))$. (Yes this is a scalar whereas curl is normally a vector, but I interpret this definition of curl to be the signed magnitude of the torque, since the direction is perpendicular to the $xy$ plane for all 2-D fields). I can accept that this formula aligns with the idea of ""infinitesimal torque"", but I'd like to see it in action in a rigorous setting. So, I asked myself, what would the torque be at a point in a vector field? If we were to place a non-infinitesimal paddle wheel (a circle of radius $r$) centered at $(x_1, y_1)$, it would be experiencing forces at each point in its area. If $\vec{r}$ is the position vector from the center of the wheel to where the force is exerted, then the torque is $\vec{r}\cdot\vec{F}$. The net torque is just the sum of all the torques, so the net torque experienced by the paddle wheel should be: $$ \tau=\mathop{\iint}_{(x-x_1)^2+(y-y_1)^2\le r^2}(x-x_1,y-y_1)\cdot F(x,y)\ \text{d}A $$ If we want to find the infinitesimal torque, we would supposedly take the limit as $r\to 0$. However, this doesn't make sense, as this would imply that the infinitesimal torque is zero (since the integral goes to zero). What's missing? I assume we needed some sort of limiting factor to divide by in the limit. I know from physics that $\tau = \alpha I$ where $I$ is the moment of inertia (for our 2-D paddle wheel with unit density, $I=\frac\pi2r^4$). So maybe curl is actually the infinitesimal angular acceleration? If so, then we have $$ \nabla\times F = \lim_{r\to0}\frac{2}{\pi r^4}\mathop{\iint}_{(x-x_1)^2+(y-y_1)^2\le r^2}(x-x_1,y-y_1)\cdot F(x,y)\ \text{d}A $$ This looks similar but not quite what the formula on Wikipedia is: $$ (\nabla\times F)\cdot\mathbf{\vec{n}} = \|\nabla\times F\| = \lim{A\to 0}\frac1{|A|}\oint_C\mathbf{F}\cdot d\mathbf{r} $$ Why, here, is the area $|A|$ the limiting factor? That doesn't make much sense to me. Is curl then the average infinitesimal torque, averaged over an infinitesimal area (since $|A|\to 0$)? Wikipedia also says that the curl is the ""circulation density "", rather than simply the torque. So is my initial interpretation of the curl wrong? Furthermore, what change would I have to make to my interpretation to go from my integral to the integral on wikipedia (since the integrands are different and I'm not sure what to do about that), and then how do you go from the integral to the initial formula for curl? Any and all help would be appreciated :)","EDIT: I would like to clarify for potential new readers what the gist of this question is, and how it differs from other questions about curl on this site. Namely, I'm confused about: The specifics of the physical interpretation of curl (is it infinitesimal torque, average infinitesimal torque, average infinitesimal angular acceleration , etc.) How this specific physical interpretation translates into a formula (ideally, the integral below as seen on Wikipedia) How this formula becomes the usual definition of curl. I'm not confused about what curl is on a general level, I'm confused about why the formula for curl coincides with the physical interpretation , on an intuitive and rigorous basis. I'm having a bit of difficulty making the connection between the intuitive idea of the curl of a vector field (as the torque experienced on an infinitesimally small paddle wheel placed at a point in the vector field), and the formula for it (I'll stick to 2-D first off, which is how the MIT OpenCourseware lectures define it at first): $$ \nabla\times F = N_x-M_y $$ where $F(x,y)=(M(x,y),N(x,y))$. (Yes this is a scalar whereas curl is normally a vector, but I interpret this definition of curl to be the signed magnitude of the torque, since the direction is perpendicular to the $xy$ plane for all 2-D fields). I can accept that this formula aligns with the idea of ""infinitesimal torque"", but I'd like to see it in action in a rigorous setting. So, I asked myself, what would the torque be at a point in a vector field? If we were to place a non-infinitesimal paddle wheel (a circle of radius $r$) centered at $(x_1, y_1)$, it would be experiencing forces at each point in its area. If $\vec{r}$ is the position vector from the center of the wheel to where the force is exerted, then the torque is $\vec{r}\cdot\vec{F}$. The net torque is just the sum of all the torques, so the net torque experienced by the paddle wheel should be: $$ \tau=\mathop{\iint}_{(x-x_1)^2+(y-y_1)^2\le r^2}(x-x_1,y-y_1)\cdot F(x,y)\ \text{d}A $$ If we want to find the infinitesimal torque, we would supposedly take the limit as $r\to 0$. However, this doesn't make sense, as this would imply that the infinitesimal torque is zero (since the integral goes to zero). What's missing? I assume we needed some sort of limiting factor to divide by in the limit. I know from physics that $\tau = \alpha I$ where $I$ is the moment of inertia (for our 2-D paddle wheel with unit density, $I=\frac\pi2r^4$). So maybe curl is actually the infinitesimal angular acceleration? If so, then we have $$ \nabla\times F = \lim_{r\to0}\frac{2}{\pi r^4}\mathop{\iint}_{(x-x_1)^2+(y-y_1)^2\le r^2}(x-x_1,y-y_1)\cdot F(x,y)\ \text{d}A $$ This looks similar but not quite what the formula on Wikipedia is: $$ (\nabla\times F)\cdot\mathbf{\vec{n}} = \|\nabla\times F\| = \lim{A\to 0}\frac1{|A|}\oint_C\mathbf{F}\cdot d\mathbf{r} $$ Why, here, is the area $|A|$ the limiting factor? That doesn't make much sense to me. Is curl then the average infinitesimal torque, averaged over an infinitesimal area (since $|A|\to 0$)? Wikipedia also says that the curl is the ""circulation density "", rather than simply the torque. So is my initial interpretation of the curl wrong? Furthermore, what change would I have to make to my interpretation to go from my integral to the integral on wikipedia (since the integrands are different and I'm not sure what to do about that), and then how do you go from the integral to the initial formula for curl? Any and all help would be appreciated :)",,"['multivariable-calculus', 'vector-fields']"
93,Dimensions for the cheapest possible rectangular box,Dimensions for the cheapest possible rectangular box,,"What are the dimensions for the cheapest possible rectangular box with a volume of 504 cm3  if the material for the bottom costs \$20/cm2, material for the sides costs \$6/cm2, and material for the top costs \$8/cm2 ? I not sure how to use the information that I was given to solve this question. I know that I should use more 6 dollar pieces but I don't know how many of each I should use without doing guess and check.","What are the dimensions for the cheapest possible rectangular box with a volume of 504 cm3  if the material for the bottom costs \$20/cm2, material for the sides costs \$6/cm2, and material for the top costs \$8/cm2 ? I not sure how to use the information that I was given to solve this question. I know that I should use more 6 dollar pieces but I don't know how many of each I should use without doing guess and check.",,"['calculus', 'multivariable-calculus']"
94,Why does $\frac{\partial^2f}{\partial x \partial y} = \frac{\partial^2f}{\partial y \partial x}$,Why does,\frac{\partial^2f}{\partial x \partial y} = \frac{\partial^2f}{\partial y \partial x},"I'm studying multivariable calculus and I came across the following property of partial derivatives: $$\frac{\partial^2f}{\partial x \partial y} = \frac{\partial^2f}{\partial y \partial x}$$ where $f = f(x,y)$. Somehow this seems intuitive, but if I would need to prove this to a 10-year-old what should I do? So I'm looking for the proof :) Is this true in the general case? For example if $f = f(x_1, ..., x_n)$ and you would take the partials in random order. Would the rule still hold? Thank you for any help :)","I'm studying multivariable calculus and I came across the following property of partial derivatives: $$\frac{\partial^2f}{\partial x \partial y} = \frac{\partial^2f}{\partial y \partial x}$$ where $f = f(x,y)$. Somehow this seems intuitive, but if I would need to prove this to a 10-year-old what should I do? So I'm looking for the proof :) Is this true in the general case? For example if $f = f(x_1, ..., x_n)$ and you would take the partials in random order. Would the rule still hold? Thank you for any help :)",,"['multivariable-calculus', 'partial-derivative']"
95,How do I show that this Limit of 2 variables is zero?,How do I show that this Limit of 2 variables is zero?,,"How do I show that :$$\lim_{(x,y)\to(0,0)}xy\frac{x^2-y^2}{x^2+y^2}=0?$$ I'm stumped...","How do I show that :$$\lim_{(x,y)\to(0,0)}xy\frac{x^2-y^2}{x^2+y^2}=0?$$ I'm stumped...",,"['calculus', 'multivariable-calculus']"
96,How to find this integral...,How to find this integral...,,$$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-\left(3x^2+2\sqrt2xy+3y^2\right)}dxdy$$ I have no idea how to integrate this function. If the middle $xy$ term would not have been present it would have been easy. But the $xy$ term is causing a problem.,$$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-\left(3x^2+2\sqrt2xy+3y^2\right)}dxdy$$ I have no idea how to integrate this function. If the middle $xy$ term would not have been present it would have been easy. But the $xy$ term is causing a problem.,,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'improper-integrals']"
97,maximum value of $a+b+c$ given $a^2+b^2+c^2=48$?,maximum value of  given ?,a+b+c a^2+b^2+c^2=48,"How can i get maximum value of this  $a+b+c$ given $a^2+b^2+c^2=48$ by not  using AM,GM and lagrange multipliers .","How can i get maximum value of this  $a+b+c$ given $a^2+b^2+c^2=48$ by not  using AM,GM and lagrange multipliers .",,"['calculus', 'multivariable-calculus']"
98,Optimization of $2x+3y+z$ under the constraint $x^2+ y^2+ z^2= 1$,Optimization of  under the constraint,2x+3y+z x^2+ y^2+ z^2= 1,"Let $x, y$ and $z$ be real numbers such that $x^2+ y^2+ z^2= 1$. Find the   maximum and minimum values of $2x + 3y + z$ . How can I able to solve the problem? Thanks for your help.","Let $x, y$ and $z$ be real numbers such that $x^2+ y^2+ z^2= 1$. Find the   maximum and minimum values of $2x + 3y + z$ . How can I able to solve the problem? Thanks for your help.",,"['real-analysis', 'multivariable-calculus', 'optimization', 'maxima-minima']"
99,"double integral $\int_0^t \int_0^s \frac{\min(u,v)}{uv} \, dv \, du$",double integral,"\int_0^t \int_0^s \frac{\min(u,v)}{uv} \, dv \, du","I want to calculate the double integral:  $$\int_0^t \int_0^s \frac{\min(u,v)}{uv} \, dv \, du$$ I don't know how to o that even if it seems simple. Thanks in advance for your help","I want to calculate the double integral:  $$\int_0^t \int_0^s \frac{\min(u,v)}{uv} \, dv \, du$$ I don't know how to o that even if it seems simple. Thanks in advance for your help",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
