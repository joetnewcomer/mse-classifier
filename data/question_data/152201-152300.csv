,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Fourier transform of $L^1$ function square summable?,Fourier transform of  function square summable?,L^1,It is known that for a $L^1$ function $f: \mathbb{R} \rightarrow \mathbb{C}$ the Fourier transform vanishes at infinity and is continuous. Does this even mean that $(\hat{f}(n))_{n \in \mathbb{Z}}$ is square-summable?,It is known that for a $L^1$ function $f: \mathbb{R} \rightarrow \mathbb{C}$ the Fourier transform vanishes at infinity and is continuous. Does this even mean that $(\hat{f}(n))_{n \in \mathbb{Z}}$ is square-summable?,,"['calculus', 'real-analysis', 'analysis', 'functional-analysis', 'fourier-analysis']"
1,Inclusion of $L^p$ spaces,Inclusion of  spaces,L^p,Let $X \subset L^1(\mathbb{R})$ a closed linear subspace satisfying \begin{align} X\subset \bigcup_{p>1} L^p(\mathbb{R})\end{align}    Show that $X\subset L^{p_0}(\mathbb{R})$ for some $p_0>1.$ I guess the problem is that in infinite measure spaces the inclusion $L^p\subset L^q$ only holds for $p=q$. Is it maybe possbile to apply Baire's Theorem in some way?,Let $X \subset L^1(\mathbb{R})$ a closed linear subspace satisfying \begin{align} X\subset \bigcup_{p>1} L^p(\mathbb{R})\end{align}    Show that $X\subset L^{p_0}(\mathbb{R})$ for some $p_0>1.$ I guess the problem is that in infinite measure spaces the inclusion $L^p\subset L^q$ only holds for $p=q$. Is it maybe possbile to apply Baire's Theorem in some way?,,"['real-analysis', 'functional-analysis', 'measure-theory', 'banach-spaces']"
2,Technical issue - how does author substitute in these integrals?,Technical issue - how does author substitute in these integrals?,,"A little prerequisite knowledge first, the author defines a 'mollifying' function $\rho_k: \mathbb{R}^n \rightarrow \mathbb{R}$ to have several properties, but that aren't relevant to my question (from what I understand, it exists to smooth solutions to PDEs, and enables us to show that the solution itself is smooth by definition). In our case, $\rho_k(x) = \rho_k(|x|)$ is radial. Anyway, the author defines a harmonic function $u:\Omega \subset \mathbb{R}^n \rightarrow \mathbb{R}$ and then takes the 'convolution' of $u$ and $\rho_k$ to be: $$u_k(x) := (u \star \rho_k)(x) = \int_{\mathbb{R}^n}u(y)\rho_k(x-y)dy$$ After justifying that $u_k(x)$ is $C^{\infty}$, he goes on to show that $u_k = u$, and thus to deduce that $u$ is also $C^{\infty}$ and this is where I get lost; first he writes: $$u_k(x) = \int_{\mathbb{R}^n}u(y)\rho_k(x-y)dy = \int_{\mathbb{R}^n}u(x-z)\rho_k(z)dz = \int_{\mathbb{R}^n}u(x-z)\rho_k(|z|)dz$$ Which I understand; however then, he states that we need to transform $z$ into polar coordinates, from which he subsequently writes: $$\begin{align}\int_{\mathbb{R}^n}u(x-z)\rho_k(|z|)dz = \int_{0}^{\infty}\int_{S^{n-1}}u(x + z(r,\theta))\rho_k(r)r^{n-1}d\theta dr &= \int_{S^{n-1}}\int_{0}^{\infty}u(x)\rho_k(r)r^{n-1}d\theta dr\end{align}$$ First, what substitution exactly is made here? And what is $S^{n-1}$? I presume it is the unit sphere in $\mathbb{R}^{n-1}$,  but then why doesn't he just write the limits as $(0,2\pi)$? Also, why is an initial integral over $\mathbb{R}^n$ equivalent to taking this kind of product integral? Also I think he uses the mean value property to get from the second to third bit, but I'm not exactly sure how. It's all confusing me a little and I just want a bit of clarification for, what the author would probably tell me are, 'obvious' points...","A little prerequisite knowledge first, the author defines a 'mollifying' function $\rho_k: \mathbb{R}^n \rightarrow \mathbb{R}$ to have several properties, but that aren't relevant to my question (from what I understand, it exists to smooth solutions to PDEs, and enables us to show that the solution itself is smooth by definition). In our case, $\rho_k(x) = \rho_k(|x|)$ is radial. Anyway, the author defines a harmonic function $u:\Omega \subset \mathbb{R}^n \rightarrow \mathbb{R}$ and then takes the 'convolution' of $u$ and $\rho_k$ to be: $$u_k(x) := (u \star \rho_k)(x) = \int_{\mathbb{R}^n}u(y)\rho_k(x-y)dy$$ After justifying that $u_k(x)$ is $C^{\infty}$, he goes on to show that $u_k = u$, and thus to deduce that $u$ is also $C^{\infty}$ and this is where I get lost; first he writes: $$u_k(x) = \int_{\mathbb{R}^n}u(y)\rho_k(x-y)dy = \int_{\mathbb{R}^n}u(x-z)\rho_k(z)dz = \int_{\mathbb{R}^n}u(x-z)\rho_k(|z|)dz$$ Which I understand; however then, he states that we need to transform $z$ into polar coordinates, from which he subsequently writes: $$\begin{align}\int_{\mathbb{R}^n}u(x-z)\rho_k(|z|)dz = \int_{0}^{\infty}\int_{S^{n-1}}u(x + z(r,\theta))\rho_k(r)r^{n-1}d\theta dr &= \int_{S^{n-1}}\int_{0}^{\infty}u(x)\rho_k(r)r^{n-1}d\theta dr\end{align}$$ First, what substitution exactly is made here? And what is $S^{n-1}$? I presume it is the unit sphere in $\mathbb{R}^{n-1}$,  but then why doesn't he just write the limits as $(0,2\pi)$? Also, why is an initial integral over $\mathbb{R}^n$ equivalent to taking this kind of product integral? Also I think he uses the mean value property to get from the second to third bit, but I'm not exactly sure how. It's all confusing me a little and I just want a bit of clarification for, what the author would probably tell me are, 'obvious' points...",,['analysis']
3,Are not all neighborhoods of $0$ in a locally convex space absorbent?,Are not all neighborhoods of  in a locally convex space absorbent?,0,"A locally convex space (LCS) can be defined as a topological vector space (i.e. scalar product and sum are continuous) whose topology is generated by translation of  a family of balanced and absorbent convex sets. Now let's imagine that I want to trim this definition as much as possible considering that we are in a topological vector space. For example it would be enough to say that a base of open neighborhoods of $0$ is given by a family $\{U_\alpha\}_\alpha$ of balaced absorbent convex sets since then a base of neighborhoods for $x$ would be $\{x+U_\alpha\}_\alpha$. A set $U$ in $X$ is absorbent if given $x\in X$ there is $r\geq 0$ such that $x\in rU$. My question is Isn't every neighborhood of $0$ in a topological vector space absorbent?. This is because given $x\in X$ the function from $\mathbb{R}$ or $\mathbb{C}$ that does $\lambda\mapsto \lambda x$ is continuous and so for $\lambda>0$ sufficiently small we have $$ \lambda x \in U \Rightarrow x\in \frac{1}{\lambda} U. $$ Hence it is enough to say a that a LCS is a top. vector space where a base of open neighborhoods of $0$ is given by a family of balanced convex sets, and being absorbent would be but a necessary condition for the neighborhoods (I always thought before that the absorbent condition was made so that the seminorms could be defined). Extra question: Is every family closed under intersection of absorbent sets containing zero sufficient to define by translation a base for a topology in a vector space where sum and scalar product are continuous? Or in other words, is any of the other conditions (being balaced, convex or any other) also necessary given that a LCS is a topological vector space? EDIT: Okay by looking at possible basis of open neighborhood of $0$ in $\mathbb{R}$ it becomes clear that they need not be formed by convex or balanced sets, but my question still remains as if there is any other condition that is necessary if we want them to generate a topological vector space.","A locally convex space (LCS) can be defined as a topological vector space (i.e. scalar product and sum are continuous) whose topology is generated by translation of  a family of balanced and absorbent convex sets. Now let's imagine that I want to trim this definition as much as possible considering that we are in a topological vector space. For example it would be enough to say that a base of open neighborhoods of $0$ is given by a family $\{U_\alpha\}_\alpha$ of balaced absorbent convex sets since then a base of neighborhoods for $x$ would be $\{x+U_\alpha\}_\alpha$. A set $U$ in $X$ is absorbent if given $x\in X$ there is $r\geq 0$ such that $x\in rU$. My question is Isn't every neighborhood of $0$ in a topological vector space absorbent?. This is because given $x\in X$ the function from $\mathbb{R}$ or $\mathbb{C}$ that does $\lambda\mapsto \lambda x$ is continuous and so for $\lambda>0$ sufficiently small we have $$ \lambda x \in U \Rightarrow x\in \frac{1}{\lambda} U. $$ Hence it is enough to say a that a LCS is a top. vector space where a base of open neighborhoods of $0$ is given by a family of balanced convex sets, and being absorbent would be but a necessary condition for the neighborhoods (I always thought before that the absorbent condition was made so that the seminorms could be defined). Extra question: Is every family closed under intersection of absorbent sets containing zero sufficient to define by translation a base for a topology in a vector space where sum and scalar product are continuous? Or in other words, is any of the other conditions (being balaced, convex or any other) also necessary given that a LCS is a topological vector space? EDIT: Okay by looking at possible basis of open neighborhood of $0$ in $\mathbb{R}$ it becomes clear that they need not be formed by convex or balanced sets, but my question still remains as if there is any other condition that is necessary if we want them to generate a topological vector space.",,"['analysis', 'functional-analysis', 'convex-analysis', 'locally-convex-spaces']"
4,Prove that two functionals with identical differentials differ by a constant.,Prove that two functionals with identical differentials differ by a constant.,,"I am self-studying Calculus of Variations and am struggling to prove results about the variation of a functional that are analogous to results in elementary analysis about differentials/derivatives. Prove that if two differentiable functionals defined on the same normed linear space have the same differential (first variation) at every point of the space, then they differ by a constant. ( Gelfand & Fomin, ""Calculus of Variations"" , Problem 1.13) So far, I have reasoned as follows: Suppose $J_1[y]$ and $J_2[y]$ are functionals on the normed linear space $V$, and that $\delta J_1[y,h] = \delta J_2[y,h]$. Consider the functional $J[y] = J_2[y] - J_1[y]$.  It follows directly from the definition that $\delta J[y,h] = \delta J_2[y,h] - \delta J_1[y,h]$ is the principal linear part of the variation $\Delta J$, and hence is the first variation of $J[y]$. By hypothesis, $\delta J[y,h] \equiv 0 \;\forall y\in V$. Thus, I have reduced the original problem to proving that a functional whose variation vanishes everywhere is everywhere constant, but I can't figure out how to proceed.  The common proof of the analogous result in standard calculus relies on the Mean Value Theorem, which I don't think will work here. Pointers?","I am self-studying Calculus of Variations and am struggling to prove results about the variation of a functional that are analogous to results in elementary analysis about differentials/derivatives. Prove that if two differentiable functionals defined on the same normed linear space have the same differential (first variation) at every point of the space, then they differ by a constant. ( Gelfand & Fomin, ""Calculus of Variations"" , Problem 1.13) So far, I have reasoned as follows: Suppose $J_1[y]$ and $J_2[y]$ are functionals on the normed linear space $V$, and that $\delta J_1[y,h] = \delta J_2[y,h]$. Consider the functional $J[y] = J_2[y] - J_1[y]$.  It follows directly from the definition that $\delta J[y,h] = \delta J_2[y,h] - \delta J_1[y,h]$ is the principal linear part of the variation $\Delta J$, and hence is the first variation of $J[y]$. By hypothesis, $\delta J[y,h] \equiv 0 \;\forall y\in V$. Thus, I have reduced the original problem to proving that a functional whose variation vanishes everywhere is everywhere constant, but I can't figure out how to proceed.  The common proof of the analogous result in standard calculus relies on the Mean Value Theorem, which I don't think will work here. Pointers?",,"['analysis', 'calculus-of-variations']"
5,"What does $x_e$ mean in $I(x_e,y_0)$?",What does  mean in ?,"x_e I(x_e,y_0)","A book I am using has a problem which includes two points on the graph of $y=\ln x$, $M_1(x_1, y_1)$ and $M_2(x_2, y_2)$ and identifies the middle of the chord $M_1 M_2$ between them as $I(x_e, y_0)$. What do you suppose is meant by $x_e$ in this case? Is it $x=e$ or $x$ such that $y=e$ or ?  I could put the whole problem here, but I hope to solve it myself once I understand the notation. Update: the whole problem (please don't solve it yet or I won't get to. I would like to understand the problem better. I've done some thinking and computing and have some good ideas, but I don't understand the problem fully. Thus, hints would be more appreciated than an answer.): Klein & Reeb, Problem I.1.10 Soient $M_1(x_1,x_y)\,M_2(x_2,y_2)$ deux points du graphe de $y=\ln x$. Soit $I(x_e,y_0)$ le milieu de la corde $M_1 M_2$. D'aprés la concavité du graphe $y_0 < \ln x_0$; en déduire: $2 \sqrt{x_1 x_2} < x_1 + x_2$. My translation: $M_1(x_1,x_y)\,M_2(x_2,y_2)$ are two points on the graph of $y=\ln x$. $I(x_e,y_0)$ is the middle of the chord $M_1 M_2$. According to the concavity of the graph  $y_0 < \ln x_0$; deduce $2 \sqrt{x_1 x_2} < x_1 + x_2$. Update: given GPerez's comment below which agrees with a thought I had too about $x_e$, I'd say it's possible (but not yet conclusive in my mind) the book has a typo and $x_0$ was meant instead. Update: my thought process on solving the problem was approximately thus: since $y=\ln x$ is concave, if $M_1$ is negative, then $0<x_1<1$ since $\ln x < 0$ for $0 < x < 1$. If $x_e$ wasn't a typo, I had supposed that, since it was the x-coordinate of the midpoint of $M_1 M_2$ it might mean that $M_2$ had to be located to the right of either where $y$ attains the value $e$ or where $x=e$, and knowing this would indicate that $M_2$ is to the right of that point since it is the right endpoint. From these facts, we would then deduce that $0<x_1<1$ and (for the case where we suppose $x_e$ means $x=e$:) $e<x_2<\cdot$ where $\cdot$ here is some value to the right of $x_2$ or (for the case where we suppose that $x_e$ means $y=e$:) $1<x_2<\cdot$ where $\cdot$ here is some value to the right of $x_2$. Then using the rules of logarithmic manipulation we could expand the LHS and RHS of the inequality given and compare term by term to prove it based on our knowledge of the range of possible values for $x_1$ and $x_2$ supposed above. That was my plan, but given the uncertainty of the notation and the incompletion, as of yet, of my computations, I've not concluded the solution.","A book I am using has a problem which includes two points on the graph of $y=\ln x$, $M_1(x_1, y_1)$ and $M_2(x_2, y_2)$ and identifies the middle of the chord $M_1 M_2$ between them as $I(x_e, y_0)$. What do you suppose is meant by $x_e$ in this case? Is it $x=e$ or $x$ such that $y=e$ or ?  I could put the whole problem here, but I hope to solve it myself once I understand the notation. Update: the whole problem (please don't solve it yet or I won't get to. I would like to understand the problem better. I've done some thinking and computing and have some good ideas, but I don't understand the problem fully. Thus, hints would be more appreciated than an answer.): Klein & Reeb, Problem I.1.10 Soient $M_1(x_1,x_y)\,M_2(x_2,y_2)$ deux points du graphe de $y=\ln x$. Soit $I(x_e,y_0)$ le milieu de la corde $M_1 M_2$. D'aprés la concavité du graphe $y_0 < \ln x_0$; en déduire: $2 \sqrt{x_1 x_2} < x_1 + x_2$. My translation: $M_1(x_1,x_y)\,M_2(x_2,y_2)$ are two points on the graph of $y=\ln x$. $I(x_e,y_0)$ is the middle of the chord $M_1 M_2$. According to the concavity of the graph  $y_0 < \ln x_0$; deduce $2 \sqrt{x_1 x_2} < x_1 + x_2$. Update: given GPerez's comment below which agrees with a thought I had too about $x_e$, I'd say it's possible (but not yet conclusive in my mind) the book has a typo and $x_0$ was meant instead. Update: my thought process on solving the problem was approximately thus: since $y=\ln x$ is concave, if $M_1$ is negative, then $0<x_1<1$ since $\ln x < 0$ for $0 < x < 1$. If $x_e$ wasn't a typo, I had supposed that, since it was the x-coordinate of the midpoint of $M_1 M_2$ it might mean that $M_2$ had to be located to the right of either where $y$ attains the value $e$ or where $x=e$, and knowing this would indicate that $M_2$ is to the right of that point since it is the right endpoint. From these facts, we would then deduce that $0<x_1<1$ and (for the case where we suppose $x_e$ means $x=e$:) $e<x_2<\cdot$ where $\cdot$ here is some value to the right of $x_2$ or (for the case where we suppose that $x_e$ means $y=e$:) $1<x_2<\cdot$ where $\cdot$ here is some value to the right of $x_2$. Then using the rules of logarithmic manipulation we could expand the LHS and RHS of the inequality given and compare term by term to prove it based on our knowledge of the range of possible values for $x_1$ and $x_2$ supposed above. That was my plan, but given the uncertainty of the notation and the incompletion, as of yet, of my computations, I've not concluded the solution.",,"['analysis', 'geometry', 'notation']"
6,"Stieltjes Integral - If $f, f^2, g, g^2\in R(\alpha)$ for an arbitrary integrator $\alpha$, then is $fg\in R(\alpha)$","Stieltjes Integral - If  for an arbitrary integrator , then is","f, f^2, g, g^2\in R(\alpha) \alpha fg\in R(\alpha)","My question is if $f, f^2, g, g^2\in R(\alpha)$ on $[a,b]$ for an arbitrary integrator $\alpha$, then is $fg\in R(\alpha)$ as well? This question stemmed from a problem in Apostol's Analysis, in which $f, f^2, g, g^2\in R(\alpha)$ on $[a,b]$ is assumed, and I have to prove an identity that involves the term $\int_a^b f(x)g(x)d{\alpha(x)}$, so I was wondering how the existence of this integral is guaranteed when there is no assumption that the integrator $\alpha$ is of bounded variation (in which case only $f,g\in R(\alpha)$ would be enough. This is the original problem. If $f, f^2, g, g^2\in R(\alpha)$ on $[a,b]$, prove that  $$\frac{1}{2}\int_a^b[\int_a^b \begin{vmatrix}         f(x) & g(x) \\         f(y) & g(y) \\         \end{vmatrix} d{\alpha(y)}]d{\alpha(x)} =(\int_a^b f(x)^2d{\alpha(x)})(\int_a^b g(x)^2d{\alpha(x)})-(\int_a^b f(x)g(x)d{\alpha(x)})^2$$ When $\alpha$ is increasing on $[a,b]$, deduce the Cauchy-Schwarz inequality. Is this just a mistake from the author or a fact that follows from the above assumptions? I would appreciate if anyone could explain this to me.","My question is if $f, f^2, g, g^2\in R(\alpha)$ on $[a,b]$ for an arbitrary integrator $\alpha$, then is $fg\in R(\alpha)$ as well? This question stemmed from a problem in Apostol's Analysis, in which $f, f^2, g, g^2\in R(\alpha)$ on $[a,b]$ is assumed, and I have to prove an identity that involves the term $\int_a^b f(x)g(x)d{\alpha(x)}$, so I was wondering how the existence of this integral is guaranteed when there is no assumption that the integrator $\alpha$ is of bounded variation (in which case only $f,g\in R(\alpha)$ would be enough. This is the original problem. If $f, f^2, g, g^2\in R(\alpha)$ on $[a,b]$, prove that  $$\frac{1}{2}\int_a^b[\int_a^b \begin{vmatrix}         f(x) & g(x) \\         f(y) & g(y) \\         \end{vmatrix} d{\alpha(y)}]d{\alpha(x)} =(\int_a^b f(x)^2d{\alpha(x)})(\int_a^b g(x)^2d{\alpha(x)})-(\int_a^b f(x)g(x)d{\alpha(x)})^2$$ When $\alpha$ is increasing on $[a,b]$, deduce the Cauchy-Schwarz inequality. Is this just a mistake from the author or a fact that follows from the above assumptions? I would appreciate if anyone could explain this to me.",,"['real-analysis', 'integration', 'analysis']"
7,Existence of a differentiable function given a unit gradient field,Existence of a differentiable function given a unit gradient field,,"I'm trying to prove that ""Given a unit vector field $V$, it can always be uniquely determined a differentiable function $f$ that satisfies $\nabla f = V$."" To provide you more information, the unit vector field $V$ is actually determined from $V=\frac{\nabla \phi}{\| \nabla \phi \|}$, where $\phi$ is a level-set function. That is, I want to prove the existence and uniqueness of a function $f$ satisfying $\nabla f = \frac{\nabla \phi}{\| \nabla \phi \|}$. Can any of you give me a proof or at least some advice? Any kind of help will be greatly appreciated. Thank you all in advance. :)","I'm trying to prove that ""Given a unit vector field $V$, it can always be uniquely determined a differentiable function $f$ that satisfies $\nabla f = V$."" To provide you more information, the unit vector field $V$ is actually determined from $V=\frac{\nabla \phi}{\| \nabla \phi \|}$, where $\phi$ is a level-set function. That is, I want to prove the existence and uniqueness of a function $f$ satisfying $\nabla f = \frac{\nabla \phi}{\| \nabla \phi \|}$. Can any of you give me a proof or at least some advice? Any kind of help will be greatly appreciated. Thank you all in advance. :)",,"['calculus', 'analysis', 'differential-geometry', 'vector-fields', 'poissons-equation']"
8,"Dense subset of $L^p[a,b]$",Dense subset of,"L^p[a,b]","It is true that $C_{0}^{\infty}[a,b]$,  (the space of all smooth functions f with the property that f and all its derivatives vanish at a and b) is dense in $L^p[a,b]$ with $1\leq p< \infty$ ? How do I prove it? I know that $C_{0}^{\infty}(\mathbb{R})$ is dense in $L_{p}(\mathbb{R})$ with $1\leq p< \infty$, but I don't know if this can help me.","It is true that $C_{0}^{\infty}[a,b]$,  (the space of all smooth functions f with the property that f and all its derivatives vanish at a and b) is dense in $L^p[a,b]$ with $1\leq p< \infty$ ? How do I prove it? I know that $C_{0}^{\infty}(\mathbb{R})$ is dense in $L_{p}(\mathbb{R})$ with $1\leq p< \infty$, but I don't know if this can help me.",,"['real-analysis', 'analysis', 'functional-analysis', 'lebesgue-integral', 'lp-spaces']"
9,Generalising Riemann integral to functions with values in a Banach space,Generalising Riemann integral to functions with values in a Banach space,,"I found the following idea for generalising the Riemann integral to functions with values in Banach spaces, but I get stuck when discontinuous functions come into play: We fix an interval $ [a,b] $ on which we'll be integrating and a Banach space X. Define $ S[a,b] $ as the set of all step functions on [a,b] with values in X. We have a very obvious definition of Riemann integral for such a function, where we take values of the function on subintervals, multiplied by the length of the corresponding subinterval and sum all of those. The function $ I $ which to a step function $ f $ assings a value $ I(f) $ as described above is linear and continuous on $ S[a,b] $ as a subspace of the Banach space of bounded function from $ [a,b] $ to $ X $, so we can uniquely extend $ I $ to a linear operator on the closure of $ S[a,b] $. So $ I $ gives us a notion of integrability. Continuous functions are clearly integrable according to the construction above, so all's fine up to this point. But for this to be a proper generalisation, we would need functions continuous almost-everywhere to be integrable(at least when $ X = \mathbb{R} $), so possible to approximate uniformly by step functions, which is not the case. I thought about using topology of pointwise convergence instead of topology of uniform convergence induced by the typical norm on the space of bounded functions. But from what I've found, we can approximate almost-everywhere continuous functions with step functions almost-everywhere, but not everywhere, so even with this topology these functions might not be integrable. Is there a simple way to fix this construction to make almost-everywhere continuous real-valued functions integrable? Or is there a different way to define the integral which will work better?","I found the following idea for generalising the Riemann integral to functions with values in Banach spaces, but I get stuck when discontinuous functions come into play: We fix an interval $ [a,b] $ on which we'll be integrating and a Banach space X. Define $ S[a,b] $ as the set of all step functions on [a,b] with values in X. We have a very obvious definition of Riemann integral for such a function, where we take values of the function on subintervals, multiplied by the length of the corresponding subinterval and sum all of those. The function $ I $ which to a step function $ f $ assings a value $ I(f) $ as described above is linear and continuous on $ S[a,b] $ as a subspace of the Banach space of bounded function from $ [a,b] $ to $ X $, so we can uniquely extend $ I $ to a linear operator on the closure of $ S[a,b] $. So $ I $ gives us a notion of integrability. Continuous functions are clearly integrable according to the construction above, so all's fine up to this point. But for this to be a proper generalisation, we would need functions continuous almost-everywhere to be integrable(at least when $ X = \mathbb{R} $), so possible to approximate uniformly by step functions, which is not the case. I thought about using topology of pointwise convergence instead of topology of uniform convergence induced by the typical norm on the space of bounded functions. But from what I've found, we can approximate almost-everywhere continuous functions with step functions almost-everywhere, but not everywhere, so even with this topology these functions might not be integrable. Is there a simple way to fix this construction to make almost-everywhere continuous real-valued functions integrable? Or is there a different way to define the integral which will work better?",,"['integration', 'analysis', 'functional-analysis']"
10,"for a given $f$, $f$ is measurable iff $f^{-1} (${$-\infty$}$) \in \mathcal{M}$ , $f^{-1} (${$\infty$}$) \in \mathcal{M}$ and f is measurable on $Y$.","for a given ,  is measurable iff {} , {} and f is measurable on .",f f f^{-1} ( -\infty ) \in \mathcal{M} f^{-1} ( \infty ) \in \mathcal{M} Y,"Let $f : X \rightarrow \bar{\mathbb{R}}$ and $Y = f^{-1}(\mathbb{R})$ then f is measurable iff $f^{-1} (${$-\infty$}$) \in \mathcal{M}$ , $f^{-1} (${$\infty$}$) \in \mathcal{M}$ and f is measurable on $Y$. My approach : $\Rightarrow$  $f$ is measurable thus we have $f^{-1}(E) \in \mathcal{M} $  $\forall E \in \mathcal{B}_{\bar{\mathbb{R}}} $  now we know that $\mathcal{B}_{\bar{\mathbb{R}}} =\mathcal{B}_{\mathbb{R}} \cup ${$-\infty$} $ \cup ${$\infty$} Thus now $f^{-1} (${$-\infty$}$) \in \mathcal{M}$ , $f^{-1} (${$\infty$}$) \in \mathcal{M}$ and also $f^{-1}(E) \in \mathcal{M} $  $\forall E \in \mathcal{B}_{\mathbb{R}} $ this is nothing but f is measurable on $Y$ ( As $f : Y \rightarrow \mathbb{R}$. Hence proved $\Leftarrow$ NO clue ! Is my approach and first part of proof correct ? Someone please help.","Let $f : X \rightarrow \bar{\mathbb{R}}$ and $Y = f^{-1}(\mathbb{R})$ then f is measurable iff $f^{-1} (${$-\infty$}$) \in \mathcal{M}$ , $f^{-1} (${$\infty$}$) \in \mathcal{M}$ and f is measurable on $Y$. My approach : $\Rightarrow$  $f$ is measurable thus we have $f^{-1}(E) \in \mathcal{M} $  $\forall E \in \mathcal{B}_{\bar{\mathbb{R}}} $  now we know that $\mathcal{B}_{\bar{\mathbb{R}}} =\mathcal{B}_{\mathbb{R}} \cup ${$-\infty$} $ \cup ${$\infty$} Thus now $f^{-1} (${$-\infty$}$) \in \mathcal{M}$ , $f^{-1} (${$\infty$}$) \in \mathcal{M}$ and also $f^{-1}(E) \in \mathcal{M} $  $\forall E \in \mathcal{B}_{\mathbb{R}} $ this is nothing but f is measurable on $Y$ ( As $f : Y \rightarrow \mathbb{R}$. Hence proved $\Leftarrow$ NO clue ! Is my approach and first part of proof correct ? Someone please help.",,"['analysis', 'measure-theory', 'proof-verification']"
11,"Given a pair of continuous functions from a topological space to an ordered set, how to prove that this set is closed? [duplicate]","Given a pair of continuous functions from a topological space to an ordered set, how to prove that this set is closed? [duplicate]",,"This question already has answers here : How to prove that this set is closed? (2 answers) Closed 7 years ago . Given that $X$ is an arbitrary topological space, $Y$ is a totally ordered set in the order topology, and $f$, $g \colon X \to Y$ are continuous functions, how to show that the subset $A$ of $X$ given by  $$ A \colon= \left\{ \ x \in X \  \colon \  f(x) \leq g(x) \ \right\} $$  is closed in $X$? Edit based on the answer by Hagen von Eitzen: Let $U \colon= \{ u \times v \in Y \times Y \ \colon \ u < v \ \}$. We show that $U$ is open in $Y \times Y$. Let $u \times v \in U$. Then $u < v$. Case I. If there is some $y \in Y$ such that $u < y < v$, then $u \times v \in (-\infty, y) \times (y, +\infty) \subset U$. Case 2. If $(u,v)$ is empty, then $u \in (-\infty, v)$ and $v \in (u, +\infty)$, and so $u \times v \in (-\infty, v) \times (u, +\infty)$. Moreover, if $a \times b \in (-\infty, v) \times (u, +\infty)$, then we must have $a < v$ and $b> u$. So $a \leq u < v \leq b$, which implies that $a < b$ and so $a \times b \in U$. Thus, $u \times v \in (-\infty, v) \times (u, +\infty) \subset U$. So $U$ is open in $Y \times Y$. Similarly, we can show that the set  $$V \colon= \left\{ \ u \times v \in Y \times Y \ \colon \ u > v \ \right\}$$ is open in $Y \times Y$. Thus, it follows that the set $A \colon= \{ \ u \times v \in Y \times Y \ \colon \ u \leq v \ \}$ is closed in $Y \times Y$. Now since the maps $f \colon X \to Y$ and $g \colon X \to Y$ are continuous, so is the map $f \times g \colon X \to Y \times Y$ defined as $$(f \times g)(x) \colon= f(x) \times g(x) \ \mbox{ for all } \ x \in X.$$ Thus, the inverse image under $f \times g$ of the set $A$ is closed in $X$. But  $$  \begin{align}  (f \times g)^{-1} (A) &= \left\{ \ x \in X \ \colon \ (f \times g)(x) \in A \ \right\} \\ &= \left\{ \ x \in X \ \colon \ f(x) \times g(x) \in A \ \right\} \\ &=  \left\{ \ x \in X \ \colon \ f(x) \leq g(x) \ \right\}. \end{align}  $$","This question already has answers here : How to prove that this set is closed? (2 answers) Closed 7 years ago . Given that $X$ is an arbitrary topological space, $Y$ is a totally ordered set in the order topology, and $f$, $g \colon X \to Y$ are continuous functions, how to show that the subset $A$ of $X$ given by  $$ A \colon= \left\{ \ x \in X \  \colon \  f(x) \leq g(x) \ \right\} $$  is closed in $X$? Edit based on the answer by Hagen von Eitzen: Let $U \colon= \{ u \times v \in Y \times Y \ \colon \ u < v \ \}$. We show that $U$ is open in $Y \times Y$. Let $u \times v \in U$. Then $u < v$. Case I. If there is some $y \in Y$ such that $u < y < v$, then $u \times v \in (-\infty, y) \times (y, +\infty) \subset U$. Case 2. If $(u,v)$ is empty, then $u \in (-\infty, v)$ and $v \in (u, +\infty)$, and so $u \times v \in (-\infty, v) \times (u, +\infty)$. Moreover, if $a \times b \in (-\infty, v) \times (u, +\infty)$, then we must have $a < v$ and $b> u$. So $a \leq u < v \leq b$, which implies that $a < b$ and so $a \times b \in U$. Thus, $u \times v \in (-\infty, v) \times (u, +\infty) \subset U$. So $U$ is open in $Y \times Y$. Similarly, we can show that the set  $$V \colon= \left\{ \ u \times v \in Y \times Y \ \colon \ u > v \ \right\}$$ is open in $Y \times Y$. Thus, it follows that the set $A \colon= \{ \ u \times v \in Y \times Y \ \colon \ u \leq v \ \}$ is closed in $Y \times Y$. Now since the maps $f \colon X \to Y$ and $g \colon X \to Y$ are continuous, so is the map $f \times g \colon X \to Y \times Y$ defined as $$(f \times g)(x) \colon= f(x) \times g(x) \ \mbox{ for all } \ x \in X.$$ Thus, the inverse image under $f \times g$ of the set $A$ is closed in $X$. But  $$  \begin{align}  (f \times g)^{-1} (A) &= \left\{ \ x \in X \ \colon \ (f \times g)(x) \in A \ \right\} \\ &= \left\{ \ x \in X \ \colon \ f(x) \times g(x) \in A \ \right\} \\ &=  \left\{ \ x \in X \ \colon \ f(x) \leq g(x) \ \right\}. \end{align}  $$",,"['general-topology', 'continuity']"
12,"Book Suggestion, Analysis (Multivariable Calculus)","Book Suggestion, Analysis (Multivariable Calculus)",,"I am doing my 4th Analysis course. Now we are doing functions from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$ And we are talking about differentiabiity . I have a lot of problems understanding the course because geometrically it is not very intuitive. I mean obviously for higher dimensions it's normal that it's not really intuitive, because we can't actually imagine it. But the idea is the same with three dimensions. So can anyone suggest a book that explains the subject and gives geometric intuition to what is going on? Maybe a book that you have studies during your analysis course that helped you with geometric intuition. Looking forward for replies. Thanks!","I am doing my 4th Analysis course. Now we are doing functions from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$ And we are talking about differentiabiity . I have a lot of problems understanding the course because geometrically it is not very intuitive. I mean obviously for higher dimensions it's normal that it's not really intuitive, because we can't actually imagine it. But the idea is the same with three dimensions. So can anyone suggest a book that explains the subject and gives geometric intuition to what is going on? Maybe a book that you have studies during your analysis course that helped you with geometric intuition. Looking forward for replies. Thanks!",,"['analysis', 'multivariable-calculus', 'book-recommendation', 'advice']"
13,Compute $\int_{1}^{\infty} \frac{J^2_{n}(k)}{k^m} dk$,Compute,\int_{1}^{\infty} \frac{J^2_{n}(k)}{k^m} dk,"Question as the title showed,in which J means Bessel functions, n and m are positive integers. How to get the analytic result? Any comment is much appreciated. Many thanks in advance.How to simplify  the following formula when $n>2m+1$ and $n$ is odd.","Question as the title showed,in which J means Bessel functions, n and m are positive integers. How to get the analytic result? Any comment is much appreciated. Many thanks in advance.How to simplify  the following formula when $n>2m+1$ and $n$ is odd.",,"['calculus', 'analysis', 'special-functions']"
14,Uniform convergence and composition,Uniform convergence and composition,,"Suppose we have a sequence of analytic functions $f_j:(1,M) \to (1,M)$ for some $M \in \mathbb{R}^+$. We are guaranteed that there is an $n \in \mathbb{N}$ such that its $n$'th iterate $$g_j(x) =f_j( f_j(...(n\,times)...(f_j(x)) = (f_j \circ f_j \circ ...(n\,times).. \circ f_j)(x)$$ converges uniformly, $g_j \to g$. Does this guarantee that $f_j \to f$ uniformly? If so how would one go about showing this? I think a proof by contradiction would work, if $f_j \not\to f$ uniformly then $g_j \not\to g$ uniformly. Is this sufficient enough of an argument? Any help or suggestions would be greatly appreciated. Thanks.","Suppose we have a sequence of analytic functions $f_j:(1,M) \to (1,M)$ for some $M \in \mathbb{R}^+$. We are guaranteed that there is an $n \in \mathbb{N}$ such that its $n$'th iterate $$g_j(x) =f_j( f_j(...(n\,times)...(f_j(x)) = (f_j \circ f_j \circ ...(n\,times).. \circ f_j)(x)$$ converges uniformly, $g_j \to g$. Does this guarantee that $f_j \to f$ uniformly? If so how would one go about showing this? I think a proof by contradiction would work, if $f_j \not\to f$ uniformly then $g_j \not\to g$ uniformly. Is this sufficient enough of an argument? Any help or suggestions would be greatly appreciated. Thanks.",,['analysis']
15,Schwarz 's lemma and sharp upper bound,Schwarz 's lemma and sharp upper bound,,"Let $f$ be a holomorphic function on $|z|<1$ with $|f(z)|<1$ for all $|z|<1$. (1) Find necessary and sufficient conditions for equality of $$\frac{|f'(z)|}{1-|f(z)|^2} \leq \frac{1}{1-|z|^2}$$ for all $|z| < 1.$ (2) If $f(\frac{1}{2}) = \frac{1}{3}$, find a sharp upper bound for $|f'(\frac{1}{2})|.$ I know that the inequality is Schwarz-Pick Lemma, and I think that it has something to do with Mobius Transformation (Linear fractional transformation) which I supposed that it will be introduced later in my textbook (Complex Analysis in Spirit of Lipman Bers, second edition). This problem is in chapter 6, and the Mobius should be introduced in chapter 8. So, actually, I do not know much about Mobius Transformation. So I do not have any clear idea for the condition concerning equality of the inequality above. Also, I do not know what is a sharp upper bound. I found the definition on http://en.wikipedia.org/wiki/Upper_and_lower_bounds ,but I do not fully understand what it means, and, according to the question, what I have to do with $|f'(\frac{1}{2})|$","Let $f$ be a holomorphic function on $|z|<1$ with $|f(z)|<1$ for all $|z|<1$. (1) Find necessary and sufficient conditions for equality of $$\frac{|f'(z)|}{1-|f(z)|^2} \leq \frac{1}{1-|z|^2}$$ for all $|z| < 1.$ (2) If $f(\frac{1}{2}) = \frac{1}{3}$, find a sharp upper bound for $|f'(\frac{1}{2})|.$ I know that the inequality is Schwarz-Pick Lemma, and I think that it has something to do with Mobius Transformation (Linear fractional transformation) which I supposed that it will be introduced later in my textbook (Complex Analysis in Spirit of Lipman Bers, second edition). This problem is in chapter 6, and the Mobius should be introduced in chapter 8. So, actually, I do not know much about Mobius Transformation. So I do not have any clear idea for the condition concerning equality of the inequality above. Also, I do not know what is a sharp upper bound. I found the definition on http://en.wikipedia.org/wiki/Upper_and_lower_bounds ,but I do not fully understand what it means, and, according to the question, what I have to do with $|f'(\frac{1}{2})|$",,"['complex-analysis', 'analysis']"
16,"Show that if $U \subseteq C$ and $V \subseteq C$ are both open and convex sets, then the set $U \cap V \subseteq C$ is open and convex as well.","Show that if  and  are both open and convex sets, then the set  is open and convex as well.",U \subseteq C V \subseteq C U \cap V \subseteq C,I think you have to prove that as it is the intersection then both are in open and convex sets seeing as they are on their own. Don't really know how to put this down in notation though.,I think you have to prove that as it is the intersection then both are in open and convex sets seeing as they are on their own. Don't really know how to put this down in notation though.,,['analysis']
17,Pointwise convergence imply uniform convergence,Pointwise convergence imply uniform convergence,,I am trying to find a condition under which a sequence of continuous functions on a metric space (or more generally in a topological space) which point wise converge to some function f should imply uniform convergence. Basically I am searching for a theorem which proves uniform convergence provided that we know pointwise convergence is true. Is there any such theorem? Thanks.,I am trying to find a condition under which a sequence of continuous functions on a metric space (or more generally in a topological space) which point wise converge to some function f should imply uniform convergence. Basically I am searching for a theorem which proves uniform convergence provided that we know pointwise convergence is true. Is there any such theorem? Thanks.,,"['real-analysis', 'analysis', 'functions', 'metric-spaces']"
18,"Finding any $\delta$ that $||(y,s)-(x,t)||<\delta$ implies $s<||y||$",Finding any  that  implies,"\delta ||(y,s)-(x,t)||<\delta s<||y||","I want to show that the set $$S=\{(x,t)\in\mathbb{R}^n\times\mathbb{R}\;|\;\;t<||x||\}$$  is an open set. Let $(x,t)\in S$, so we have $t<||x||$. So we have to show that  $$B((x,t);\delta)=\{(y,s)\in\mathbb{R}^n\times\mathbb{R}\;|\;||(y,s)-(x,t)||<\delta\}\subset S$$ So we need to show that if $||(y,s)-(x,t)||<\delta$, then $s<||y||$ and so $B((x,t);\delta)\subset S$, I tried to find such $\delta$ and couldn't find it. Please help me.","I want to show that the set $$S=\{(x,t)\in\mathbb{R}^n\times\mathbb{R}\;|\;\;t<||x||\}$$  is an open set. Let $(x,t)\in S$, so we have $t<||x||$. So we have to show that  $$B((x,t);\delta)=\{(y,s)\in\mathbb{R}^n\times\mathbb{R}\;|\;||(y,s)-(x,t)||<\delta\}\subset S$$ So we need to show that if $||(y,s)-(x,t)||<\delta$, then $s<||y||$ and so $B((x,t);\delta)\subset S$, I tried to find such $\delta$ and couldn't find it. Please help me.",,"['real-analysis', 'analysis']"
19,Small question about condition in the Bouchala paper's 2005,Small question about condition in the Bouchala paper's 2005,,"I have this condition from this paper: http://ejde.math.txstate.edu/Volumes/2005/08/bouchala.pdf (Strong resonance problems for the one-dimensional $p$-Laplacian. Bouchala, Jiri. Electronic Journal of Differential Equations (EJDE), Volume: 2005, page Paper No. 08, 10 p., electronic only.) $$ \forall v\in \ker(-\Delta_p-\lambda)\setminus\{0\}:\\(p-1)\int_0^{\pi} h(x) v(x)  dx< \underline{F(+\infty)}\int_0^{\pi} v^+(x) dx+\overline{  F(-\infty)}\int_0^{\pi} v^-(x) dx$$ where  $h\in L^{p'}(0,\pi),  p'=\frac{p}{p-1}, p>1$ and $$F(x)=\begin{cases}\frac{p}{x}\int_0^x  g(s) ds-g(x), x\neq 0\\ (p-1) g(0), x=0,\end{cases}$$ $g:\mathbb{R}\rightarrow \mathbb{R}$ is a continuous function, $v^+=\max\{0,v\}$ and $v^-=\min\{0,v\},$ and $$\underline{F(+\infty)}=\liminf_{x\rightarrow +\infty} F(x) \qquad  \overline{ F(-\infty)} =\limsup_{x\rightarrow -\infty} F(x) .$$ where in the paper prove that: $$\lim_{\|u_n\|\rightarrow+\infty}\int_0^{\pi} F(u_n)\frac{u_n}{\|u_n\|} dx=(p-1)\int_0^{\pi} h(x)v(x) dx$$ such that $\frac{u_n}{\|u_n\|}=v_n\rightharpoonup v$ (weakly) in $W^{1,p}_0$ and strongly in $C^0$ My question: How can we conclude from this that $$\underline{F(+\infty)}>-\infty ~\text{and}~ \overline{F(-\infty)}<+\infty $$ Thank you.","I have this condition from this paper: http://ejde.math.txstate.edu/Volumes/2005/08/bouchala.pdf (Strong resonance problems for the one-dimensional $p$-Laplacian. Bouchala, Jiri. Electronic Journal of Differential Equations (EJDE), Volume: 2005, page Paper No. 08, 10 p., electronic only.) $$ \forall v\in \ker(-\Delta_p-\lambda)\setminus\{0\}:\\(p-1)\int_0^{\pi} h(x) v(x)  dx< \underline{F(+\infty)}\int_0^{\pi} v^+(x) dx+\overline{  F(-\infty)}\int_0^{\pi} v^-(x) dx$$ where  $h\in L^{p'}(0,\pi),  p'=\frac{p}{p-1}, p>1$ and $$F(x)=\begin{cases}\frac{p}{x}\int_0^x  g(s) ds-g(x), x\neq 0\\ (p-1) g(0), x=0,\end{cases}$$ $g:\mathbb{R}\rightarrow \mathbb{R}$ is a continuous function, $v^+=\max\{0,v\}$ and $v^-=\min\{0,v\},$ and $$\underline{F(+\infty)}=\liminf_{x\rightarrow +\infty} F(x) \qquad  \overline{ F(-\infty)} =\limsup_{x\rightarrow -\infty} F(x) .$$ where in the paper prove that: $$\lim_{\|u_n\|\rightarrow+\infty}\int_0^{\pi} F(u_n)\frac{u_n}{\|u_n\|} dx=(p-1)\int_0^{\pi} h(x)v(x) dx$$ such that $\frac{u_n}{\|u_n\|}=v_n\rightharpoonup v$ (weakly) in $W^{1,p}_0$ and strongly in $C^0$ My question: How can we conclude from this that $$\underline{F(+\infty)}>-\infty ~\text{and}~ \overline{F(-\infty)}<+\infty $$ Thank you.",,"['analysis', 'functional-analysis', 'partial-differential-equations']"
20,Smooth manifolds and cartesian products,Smooth manifolds and cartesian products,,"So assume you have two smooth maps $f:M\rightarrow N$ and $g:P\rightarrow Q$. If we define the map $f\times g:M\times P\rightarrow N\times Q$ to be such that $(f\times g)(x,y)=(f(x),g(y))$. Does it follow that such a map is smooth?","So assume you have two smooth maps $f:M\rightarrow N$ and $g:P\rightarrow Q$. If we define the map $f\times g:M\times P\rightarrow N\times Q$ to be such that $(f\times g)(x,y)=(f(x),g(y))$. Does it follow that such a map is smooth?",,"['real-analysis', 'analysis', 'differential-topology', 'smooth-manifolds']"
21,Prove that function with positive second derivative grows faster than a linear function,Prove that function with positive second derivative grows faster than a linear function,,"The statement of the problem is as follows, Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be differentiable, and $f''(0)$ exists and is greater than $0$. We also have $f(0) = 0$. Prove that there exists an $x > 0$ such that $f(2x) > 2f(x)$. I have tried doing this based on the sequential definition of limits and the definition of the derivative but I have only been able to show that there exists $x > 0$ such that $f(2x) > f(x)$. EDIT: There was an additional condition I forgot to include, $f(0) = 0$","The statement of the problem is as follows, Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be differentiable, and $f''(0)$ exists and is greater than $0$. We also have $f(0) = 0$. Prove that there exists an $x > 0$ such that $f(2x) > 2f(x)$. I have tried doing this based on the sequential definition of limits and the definition of the derivative but I have only been able to show that there exists $x > 0$ such that $f(2x) > f(x)$. EDIT: There was an additional condition I forgot to include, $f(0) = 0$",,['analysis']
22,"Usual and ""unusual"" indeterminate forms","Usual and ""unusual"" indeterminate forms",,"I've always read there are seven ""main"" indeterminate forms, 0/0, ∞/∞,∞−∞,0 ∞ ,0 0 ,1 ∞ ,∞ 0 . I've recently started working more keenly on analysis, and the seven ""main"" forms are never expanded to include ""unusual"" forms. Does anyone know what are they ? I've read somewhere that things likes lim [sin x + sin x] as x goes to ∞ could be one such indetermination. What do you think ? All the best, G.","I've always read there are seven ""main"" indeterminate forms, 0/0, ∞/∞,∞−∞,0 ∞ ,0 0 ,1 ∞ ,∞ 0 . I've recently started working more keenly on analysis, and the seven ""main"" forms are never expanded to include ""unusual"" forms. Does anyone know what are they ? I've read somewhere that things likes lim [sin x + sin x] as x goes to ∞ could be one such indetermination. What do you think ? All the best, G.",,"['analysis', 'indeterminate-forms']"
23,Convergence of integers by transformations,Convergence of integers by transformations,,"Let $x=(a,b)$, where $a,b$ are in $N$ Now we have the transformations: $$T_1(x) = (ka, b+1)$$  $$T_2(x) = (b,a)$$ where $k$ is in $N$. Where the order of choosing a transformation is not fixed.  (E.g. you can first apply 3 times $$T_1(x)$$, then $$T_2(x)$$ Will we for all $(a,b)$ produce $a=b$ by only being allowed to use these two transformations? If so, is this true for every $k$? I'm specifically interested in the case $k=2$ though.","Let $x=(a,b)$, where $a,b$ are in $N$ Now we have the transformations: $$T_1(x) = (ka, b+1)$$  $$T_2(x) = (b,a)$$ where $k$ is in $N$. Where the order of choosing a transformation is not fixed.  (E.g. you can first apply 3 times $$T_1(x)$$, then $$T_2(x)$$ Will we for all $(a,b)$ produce $a=b$ by only being allowed to use these two transformations? If so, is this true for every $k$? I'm specifically interested in the case $k=2$ though.",,"['linear-algebra', 'combinatorics', 'analysis', 'linear-transformations', 'arithmetic-combinatorics']"
24,Center of Mass of objects with infinite length?,Center of Mass of objects with infinite length?,,"Suppose you have $f(x) = \frac{\sin(x)}{x}$ And you have that shape, find the center of mass of $f(x)$ in $x \in (-\infty, \infty)$ Is it possible considering $f(x)$ is an even function?","Suppose you have $f(x) = \frac{\sin(x)}{x}$ And you have that shape, find the center of mass of $f(x)$ in $x \in (-\infty, \infty)$ Is it possible considering $f(x)$ is an even function?",,"['calculus', 'real-analysis', 'integration', 'analysis']"
25,Separate form for $f'(x)$ [duplicate],Separate form for  [duplicate],f'(x),"This question already has an answer here : Proof for alternative definition of the derivative (1 answer) Closed 9 years ago . $\qquad^{\star\star}(b)$ Prove, more generally, that   $$f'(x) = \lim_{h, k \to 0^+}\frac{f(x + h) - f(x - k)}{h + k}$$ ONLY HINTS PLEASE. The denominator is the issue. I thought of $u = h + k$ but that created an issue  for the limit bounds. I tried adding and subtraction $f(x)$ in the numerator but the denominator causes issues. ONLY HINTS PLEASE! Attempts: $$f'(x) = \lim_{h, k \to 0} \frac{f(x + h) - f(x + h + k) + f(x + h + k) - f(x - k)}{h+k} $$ $$ = \lim_{h, k \to 0} -\frac{f(x + h + k) - f(x + h)}{h+k} + \frac{f(x + h + k) - f(x - k)}{h+k}$$ let $u = x + k$. As $k , h\to 0,$ $u \to x$ and $x - k =  u - 2k$ Which makes things weird.","This question already has an answer here : Proof for alternative definition of the derivative (1 answer) Closed 9 years ago . $\qquad^{\star\star}(b)$ Prove, more generally, that   $$f'(x) = \lim_{h, k \to 0^+}\frac{f(x + h) - f(x - k)}{h + k}$$ ONLY HINTS PLEASE. The denominator is the issue. I thought of $u = h + k$ but that created an issue  for the limit bounds. I tried adding and subtraction $f(x)$ in the numerator but the denominator causes issues. ONLY HINTS PLEASE! Attempts: $$f'(x) = \lim_{h, k \to 0} \frac{f(x + h) - f(x + h + k) + f(x + h + k) - f(x - k)}{h+k} $$ $$ = \lim_{h, k \to 0} -\frac{f(x + h + k) - f(x + h)}{h+k} + \frac{f(x + h + k) - f(x - k)}{h+k}$$ let $u = x + k$. As $k , h\to 0,$ $u \to x$ and $x - k =  u - 2k$ Which makes things weird.",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'proof-writing']"
26,Characterization of Sobolev space $H^m(\mathbb R^n)$ with $m\in\mathbb N_0$?,Characterization of Sobolev space  with ?,H^m(\mathbb R^n) m\in\mathbb N_0,"I want to show that if $m\in\mathbb N_0:=\mathbb N\cup\{0\}$ then $$H^m(\mathbb R^n):=\{u\in\mathscr{S}^\prime(\mathbb R^n): \exists f\in L^2(\mathbb R^n); \partial^\alpha f\in L^2(\mathbb R^n)\ \forall\ |\alpha|\leq m\ \textrm{and}\ u=T_f\}.$$ Above $T_f$ stands for the tempered distribution $$\phi\longmapsto \int_{\mathbb R^n} f(x) \phi(x)\ dx,\ \forall \phi\in\mathscr{S}(\mathbb R^n).$$ I don't like simply writing $u\in L^2(\mathbb R^n)$. I can prove the inclusion $\supseteq$ easily, however I'm stuck on the other direction, can anyone help me? Obs: Indeed, which statement is corret: $$H^m(\mathbb R^n):=\{u\in\mathscr{S}^\prime(\mathbb R^n): \exists f\in L^2(\mathbb R^n); \partial^\alpha f\in L^2(\mathbb R^n)\ \forall\ |\alpha|\leq m\ \textrm{and}\ u=T_f\},$$ or: $$H^m(\mathbb R^n):=\{u\in \mathscr{S}^\prime(\mathbb R^n): \forall |\alpha|\leq m\ \exists f_\alpha\in L^2(\mathbb R^n); \partial^\alpha u=T_{f_\alpha}\}?$$ The strangest thing is that the inclusion $\supseteq$ holds in the first case and the inclusion $\subseteq$ holds in the second case, however I can't prove either converse. Recall: For $s\in\mathbb R$ we define $$H^s(\mathbb R^n):=\{u\in \mathscr{S}^\prime(\mathbb R^n): \exists f\in L^1_{\textrm{loc}}(\mathbb R^n); \widehat{u}=T_f\ \textrm{and}\ (1+|\cdot|^2)^{s/2} f\in L^2(\mathbb R^n)\}$$ In the definition above I also avoided any abuse of language (that bothers me indeed).","I want to show that if $m\in\mathbb N_0:=\mathbb N\cup\{0\}$ then $$H^m(\mathbb R^n):=\{u\in\mathscr{S}^\prime(\mathbb R^n): \exists f\in L^2(\mathbb R^n); \partial^\alpha f\in L^2(\mathbb R^n)\ \forall\ |\alpha|\leq m\ \textrm{and}\ u=T_f\}.$$ Above $T_f$ stands for the tempered distribution $$\phi\longmapsto \int_{\mathbb R^n} f(x) \phi(x)\ dx,\ \forall \phi\in\mathscr{S}(\mathbb R^n).$$ I don't like simply writing $u\in L^2(\mathbb R^n)$. I can prove the inclusion $\supseteq$ easily, however I'm stuck on the other direction, can anyone help me? Obs: Indeed, which statement is corret: $$H^m(\mathbb R^n):=\{u\in\mathscr{S}^\prime(\mathbb R^n): \exists f\in L^2(\mathbb R^n); \partial^\alpha f\in L^2(\mathbb R^n)\ \forall\ |\alpha|\leq m\ \textrm{and}\ u=T_f\},$$ or: $$H^m(\mathbb R^n):=\{u\in \mathscr{S}^\prime(\mathbb R^n): \forall |\alpha|\leq m\ \exists f_\alpha\in L^2(\mathbb R^n); \partial^\alpha u=T_{f_\alpha}\}?$$ The strangest thing is that the inclusion $\supseteq$ holds in the first case and the inclusion $\subseteq$ holds in the second case, however I can't prove either converse. Recall: For $s\in\mathbb R$ we define $$H^s(\mathbb R^n):=\{u\in \mathscr{S}^\prime(\mathbb R^n): \exists f\in L^1_{\textrm{loc}}(\mathbb R^n); \widehat{u}=T_f\ \textrm{and}\ (1+|\cdot|^2)^{s/2} f\in L^2(\mathbb R^n)\}$$ In the definition above I also avoided any abuse of language (that bothers me indeed).",,"['analysis', 'partial-differential-equations', 'sobolev-spaces']"
27,Perturbation of Laplacian,Perturbation of Laplacian,,Let's consider a potential $V(x)\in L^3(\mathbb{R}^3)$. I want to know if the following Hamiltonian  $$-\Delta+V(x)$$ is self-adjoint on $H^2(\mathbb{R}^3)$.  My idea is to use Kato-Rellich theorem; so for $f\in D(-\Delta)=H^2(\mathbb{R}^3)$ we have $$\Vert Vf\Vert_{L^2(\mathbb{R}^3)}\leq\Vert V\Vert_{L^3(\mathbb{R}^3)}\Vert f\Vert_{L^6(\mathbb{R}^3)}$$ Now I can use Gagliardo-Niremberg-Sobolev inequality to get $$\Vert f\Vert_{L^6(\mathbb{R}^3)}\leq C\Vert\Delta f\Vert_{L^2(\mathbb{R}^3)}^{\frac{1}{2}}\Vert f\Vert_{L^2(\mathbb{R}^3)}^{\frac{1}{2}}$$ Now I can use Young inequality $$\Vert f\Vert_{L^6(\mathbb{R}^3)}\leq C(\epsilon\Vert\Delta f\Vert_{L^2(\mathbb{R}^3)}+\frac{1}{\epsilon}\Vert f\Vert_{L^2(\mathbb{R}^3)})$$ So we have that $V$ is bounded with respect to the Laplacian and so Kato-Rellich theorem gives the thesis. Is there something worng in this reasoning?,Let's consider a potential $V(x)\in L^3(\mathbb{R}^3)$. I want to know if the following Hamiltonian  $$-\Delta+V(x)$$ is self-adjoint on $H^2(\mathbb{R}^3)$.  My idea is to use Kato-Rellich theorem; so for $f\in D(-\Delta)=H^2(\mathbb{R}^3)$ we have $$\Vert Vf\Vert_{L^2(\mathbb{R}^3)}\leq\Vert V\Vert_{L^3(\mathbb{R}^3)}\Vert f\Vert_{L^6(\mathbb{R}^3)}$$ Now I can use Gagliardo-Niremberg-Sobolev inequality to get $$\Vert f\Vert_{L^6(\mathbb{R}^3)}\leq C\Vert\Delta f\Vert_{L^2(\mathbb{R}^3)}^{\frac{1}{2}}\Vert f\Vert_{L^2(\mathbb{R}^3)}^{\frac{1}{2}}$$ Now I can use Young inequality $$\Vert f\Vert_{L^6(\mathbb{R}^3)}\leq C(\epsilon\Vert\Delta f\Vert_{L^2(\mathbb{R}^3)}+\frac{1}{\epsilon}\Vert f\Vert_{L^2(\mathbb{R}^3)})$$ So we have that $V$ is bounded with respect to the Laplacian and so Kato-Rellich theorem gives the thesis. Is there something worng in this reasoning?,,"['real-analysis', 'analysis', 'functional-analysis', 'operator-theory', 'mathematical-physics']"
28,Issues proving a basis via wedge product,Issues proving a basis via wedge product,,"On a quiz I was given the problem"" a series that is a basis for $[-1,1]$ is $ \sum_0^{\infty} c_n P_n $, where $ P_n $ is a polynomial and each polynomial $P_n$ is orthonormal to the others. Using the dot product definition used in Fourier series, what is the formula for computing $c_5$?"" I really have no idea.i believe we are supposed to use the wedge product? But I'm not sure how to use that. Maybe  $\int_{-1}^1  \sum_0^{5} c_n P_n dx $? That is really the only thing I can think of but I don't know what to do with it or how to apply it.  I believe the polynomial is called a legendre polynomial?","On a quiz I was given the problem"" a series that is a basis for $[-1,1]$ is $ \sum_0^{\infty} c_n P_n $, where $ P_n $ is a polynomial and each polynomial $P_n$ is orthonormal to the others. Using the dot product definition used in Fourier series, what is the formula for computing $c_5$?"" I really have no idea.i believe we are supposed to use the wedge product? But I'm not sure how to use that. Maybe  $\int_{-1}^1  \sum_0^{5} c_n P_n dx $? That is really the only thing I can think of but I don't know what to do with it or how to apply it.  I believe the polynomial is called a legendre polynomial?",,"['integration', 'analysis', 'fourier-series']"
29,"Prove $\int_{0}^{1} \left|\frac {f^{''}(x)}{f(x)}\right|\, dx \geq 4$ [closed]",Prove  [closed],"\int_{0}^{1} \left|\frac {f^{''}(x)}{f(x)}\right|\, dx \geq 4","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I find an interesting theorem,but have no idea to prove it. $f(x) \in C^2[0,1]$ and $f(0)=f(1)=0$ , $f(x) \not = 0 \     \ ,  x\in (0,1) $ Prove that  if $\displaystyle\int_{0}^{1} \left|\frac {f^{''}(x)}{f(x)}\right| dx$ exists, $$\int_{0}^{1} \left|\frac {f^{''}(x)}{f(x)}\right| dx \geq 4$$ It is a problem for fun. If anyone knows how to do, plz tell me, not just vote "" It's useless"". Edit: I submit a proof in the same question . And there are more two excellent answers.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I find an interesting theorem,but have no idea to prove it. and , Prove that  if exists, It is a problem for fun. If anyone knows how to do, plz tell me, not just vote "" It's useless"". Edit: I submit a proof in the same question . And there are more two excellent answers.","f(x) \in C^2[0,1] f(0)=f(1)=0 f(x) \not = 0 \     \ ,  x\in (0,1)  \displaystyle\int_{0}^{1} \left|\frac {f^{''}(x)}{f(x)}\right| dx \int_{0}^{1} \left|\frac {f^{''}(x)}{f(x)}\right| dx \geq 4","['calculus', 'integration', 'analysis', 'inequality', 'integral-inequality']"
30,Evans PDE derivation of solution for Poisson's equation - integration by parts,Evans PDE derivation of solution for Poisson's equation - integration by parts,,"In Evans' PDE text, when proving the solution to Poisson's equation, when he integrates by parts he takes the inward pointing normal, whereas in the integration by parts formula in the appendix, it uses the outward pointing normal. This is probably a very elementary question but what is going on here? The fact that it is the inward pointing normal is important and comes in to play in the next few lines. So I'm hoping to get over this minor obstacle!","In Evans' PDE text, when proving the solution to Poisson's equation, when he integrates by parts he takes the inward pointing normal, whereas in the integration by parts formula in the appendix, it uses the outward pointing normal. This is probably a very elementary question but what is going on here? The fact that it is the inward pointing normal is important and comes in to play in the next few lines. So I'm hoping to get over this minor obstacle!",,"['calculus', 'real-analysis', 'analysis', 'partial-differential-equations', 'vector-analysis']"
31,Differentation uder the integral sign,Differentation uder the integral sign,,"Let $F(x)=\int_{\sin x}^{\cos x} e^{x\sqrt{1-y^2}} \, dy $. My task is to calculate $F'(x)$. My idea is to use http://en.wikipedia.org/wiki/Differentiation_under_the_integral_sign and I get: $$F'(x)=\int_{\sin x}^{\cos x}\sqrt{1-y^2} e^{x\sqrt{1-y^2}}\,dy -\sin x \cdot e^{x\sqrt{1-\cos^2x}}-\cos x \cdot e^{x\sqrt{1-\sin^2x}},$$ but now it is not any easier. The integral is still not solved. Have you got any ideas?","Let $F(x)=\int_{\sin x}^{\cos x} e^{x\sqrt{1-y^2}} \, dy $. My task is to calculate $F'(x)$. My idea is to use http://en.wikipedia.org/wiki/Differentiation_under_the_integral_sign and I get: $$F'(x)=\int_{\sin x}^{\cos x}\sqrt{1-y^2} e^{x\sqrt{1-y^2}}\,dy -\sin x \cdot e^{x\sqrt{1-\cos^2x}}-\cos x \cdot e^{x\sqrt{1-\sin^2x}},$$ but now it is not any easier. The integral is still not solved. Have you got any ideas?",,"['calculus', 'real-analysis', 'integration', 'analysis', 'multivariable-calculus']"
32,Show that $d+1$-dimensional Lebesgue measure of set $G$ equals $0$,Show that -dimensional Lebesgue measure of set  equals,d+1 G 0,"Let $D \subset \mathbb{R}^d$ and let $f:D \rightarrow \mathbb{R} $ be measurable function. Let $G=\{(x_1,x_2,\ldots,x_d,f(x_1,x_2,\ldots,x_d))\in \mathbb{R}^{d+1}:(x_1,x_2,\ldots,x_d)\in D \} $ be the graph of $f$. Show that $d+1$-dimensional Lebesgue measure of $G$ equals $0$. I honestly don't know how to bite that. I thought of using an integral and applying Fubini's theorem, but I don't even know how to start. I would appreciate any hints.","Let $D \subset \mathbb{R}^d$ and let $f:D \rightarrow \mathbb{R} $ be measurable function. Let $G=\{(x_1,x_2,\ldots,x_d,f(x_1,x_2,\ldots,x_d))\in \mathbb{R}^{d+1}:(x_1,x_2,\ldots,x_d)\in D \} $ be the graph of $f$. Show that $d+1$-dimensional Lebesgue measure of $G$ equals $0$. I honestly don't know how to bite that. I thought of using an integral and applying Fubini's theorem, but I don't even know how to start. I would appreciate any hints.",,"['analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
33,Two-leg games in Elo rating for football teams,Two-leg games in Elo rating for football teams,,"Do you know Elo rating for association football? It is a numerical estimation of strength of football clubs using simple mathematical formula based  past results allowing predictions for the future. If two teams play each other, the resulting change in points of one team is calculated as follows:   $$ \Delta P = W \times \max (\sqrt{| G - G_{O}|}, 1) \times (R - \frac{1}{10^{\frac{-(P - P_{O} + V)}{400}} + 1})$$ where $\Delta P$ : The point that the team gets after the match. $W$ : Weight(significance) of the game. 0 for friendly matches, 20 for official matches, 40 for ""big"" games (such as World cup, ...) $G$ : Number of goals scored in the game by our team. $G_{O}$ : Number of goals scored in the game by the opponent. $R$ : Result of the game. 1 for win, 0.5 for draw, 0 for defeat. $P$ : Current point of our team, before the game. $P_{O}$ : Current point of the opponent, before the game. $V$ : Home advantage. 0 for neutral venue, 90 for domestic home game, -90 for domestic away game, 120 for international home game, -120 for international away game. For example: Real Sociedad (Spain) beat Real Madrid (Spain) 4-2 at their home. Before the game, the points of two teams were Real Sociedad 1752, Real Madrid 2081.  After the game, Real Sociedad get $\displaystyle 20 \times \sqrt{2} \times (1 - \frac{1}{10^{\frac{-(1752+90-2081)}{400}} + 1}) = 22.5798$ points, Real Madrid lose $22.5798$ points. (As you notice, the rating is always zero-sum game) Chelsea (England, 1950 pts) beats Schalke (Germany, 1773 pts) 5-0 at the away game.  Chelsea get $18.7248$ points, Schalke lose $18.7248$ points. Real Madrid (Spain, 2101 pts) beats Razgrad (Bulgaria, 1542 pts) 4-0 at their home. Real Madrid get $0.7869$ points, Razgrad lose $0.7869$ points. The formula is generally considered reasonable when it comes to single-leg games. However, the problem arises on two-leg games (such as UEFA Champions League knockout tournament): A team winning its first leg by a high margin can afford to lose the second leg by a narrow margin. Consider this real example: In 2013/14 UEFA Champions League quarterfinal game, Real Madrid (pts around 2050) beat Dortmund(pts around 1870) 3-0 at first-leg home game, and lost to Dortmund 0-2 at second-leg away game, but anyway qualified through semifinal with aggregate score 3-2. If we apply the formula to each two of games individually, then Real Madrid get 5.18 points in the first leg, and lost 17.15 points in the second leg, resulting total loss of 12.03 points. This is NOT PLAUSIBLE, because the eventual winner of the two-leg game was Real Madrid anyway. So, we have to apply the formula in different manner for 2nd-leg games in double-leg tournament matches. What I think that the new revised formula must satisfy is: What really matter is to qualify for the next round. Hence, if you qualify, the total change in the points throughout 2 games must be positive. If you fail to qualify, the total change must be negative. Winning/drawing/losing the second leg game also matters. If you win the first game 4-0 and lose the second game 3-0, then anyway you must lose some points in second game, but the amount of losing points should not exceed the amount of point you've gain in the first game (because we must satisfy the condition 1). The first leg should be considered as a standard game. I thought several cases: 1) First Win, Second Win : Just apply the formula as usual. I see no reason in changing that. 2) First Win, Second Draw (Eventual win) : First you get point, and second you get or lose point. If your team are considered as very strong, then your team would lost some points in second game, but the amount of point loss must be less than the amount of point gain. In terms of mathematics, $\Delta P_1 > 0$ and $\Delta P_1 + \Delta P_2 > 0$. 3) First Win, Second Lose (Eventual Win, like 3-0 and 0-2, or away goal win) : $\Delta P_1 > 0$, $\Delta P_2 < 0$, $\Delta P_1 + \Delta P_2 > 0$. 4) First Win, Second Lose (Eventual Lose, like 3-0 and 0-4, or away goal lose) : $\Delta P_1 > 0$, $\Delta P_2 < 0$, $\Delta P_1 + \Delta P_2 < 0$. 5) First Draw, Second Win (Eventual Win) : $\Delta P_2 > 0$, $\Delta P_1 + \Delta P_2 > 0$. 6) First Draw, Second Draw (Eventual Win, like away goal or penalty shootouts after extra time) : $\Delta P_1 + \Delta P_2 > 0$. Since the point system is zero-sum, these cases are all we need to consider. I'm struggling on making ideas about how to revise the formula for case 2)~6). Any talk would be appreciated. Thanks.","Do you know Elo rating for association football? It is a numerical estimation of strength of football clubs using simple mathematical formula based  past results allowing predictions for the future. If two teams play each other, the resulting change in points of one team is calculated as follows:   $$ \Delta P = W \times \max (\sqrt{| G - G_{O}|}, 1) \times (R - \frac{1}{10^{\frac{-(P - P_{O} + V)}{400}} + 1})$$ where $\Delta P$ : The point that the team gets after the match. $W$ : Weight(significance) of the game. 0 for friendly matches, 20 for official matches, 40 for ""big"" games (such as World cup, ...) $G$ : Number of goals scored in the game by our team. $G_{O}$ : Number of goals scored in the game by the opponent. $R$ : Result of the game. 1 for win, 0.5 for draw, 0 for defeat. $P$ : Current point of our team, before the game. $P_{O}$ : Current point of the opponent, before the game. $V$ : Home advantage. 0 for neutral venue, 90 for domestic home game, -90 for domestic away game, 120 for international home game, -120 for international away game. For example: Real Sociedad (Spain) beat Real Madrid (Spain) 4-2 at their home. Before the game, the points of two teams were Real Sociedad 1752, Real Madrid 2081.  After the game, Real Sociedad get $\displaystyle 20 \times \sqrt{2} \times (1 - \frac{1}{10^{\frac{-(1752+90-2081)}{400}} + 1}) = 22.5798$ points, Real Madrid lose $22.5798$ points. (As you notice, the rating is always zero-sum game) Chelsea (England, 1950 pts) beats Schalke (Germany, 1773 pts) 5-0 at the away game.  Chelsea get $18.7248$ points, Schalke lose $18.7248$ points. Real Madrid (Spain, 2101 pts) beats Razgrad (Bulgaria, 1542 pts) 4-0 at their home. Real Madrid get $0.7869$ points, Razgrad lose $0.7869$ points. The formula is generally considered reasonable when it comes to single-leg games. However, the problem arises on two-leg games (such as UEFA Champions League knockout tournament): A team winning its first leg by a high margin can afford to lose the second leg by a narrow margin. Consider this real example: In 2013/14 UEFA Champions League quarterfinal game, Real Madrid (pts around 2050) beat Dortmund(pts around 1870) 3-0 at first-leg home game, and lost to Dortmund 0-2 at second-leg away game, but anyway qualified through semifinal with aggregate score 3-2. If we apply the formula to each two of games individually, then Real Madrid get 5.18 points in the first leg, and lost 17.15 points in the second leg, resulting total loss of 12.03 points. This is NOT PLAUSIBLE, because the eventual winner of the two-leg game was Real Madrid anyway. So, we have to apply the formula in different manner for 2nd-leg games in double-leg tournament matches. What I think that the new revised formula must satisfy is: What really matter is to qualify for the next round. Hence, if you qualify, the total change in the points throughout 2 games must be positive. If you fail to qualify, the total change must be negative. Winning/drawing/losing the second leg game also matters. If you win the first game 4-0 and lose the second game 3-0, then anyway you must lose some points in second game, but the amount of losing points should not exceed the amount of point you've gain in the first game (because we must satisfy the condition 1). The first leg should be considered as a standard game. I thought several cases: 1) First Win, Second Win : Just apply the formula as usual. I see no reason in changing that. 2) First Win, Second Draw (Eventual win) : First you get point, and second you get or lose point. If your team are considered as very strong, then your team would lost some points in second game, but the amount of point loss must be less than the amount of point gain. In terms of mathematics, $\Delta P_1 > 0$ and $\Delta P_1 + \Delta P_2 > 0$. 3) First Win, Second Lose (Eventual Win, like 3-0 and 0-2, or away goal win) : $\Delta P_1 > 0$, $\Delta P_2 < 0$, $\Delta P_1 + \Delta P_2 > 0$. 4) First Win, Second Lose (Eventual Lose, like 3-0 and 0-4, or away goal lose) : $\Delta P_1 > 0$, $\Delta P_2 < 0$, $\Delta P_1 + \Delta P_2 < 0$. 5) First Draw, Second Win (Eventual Win) : $\Delta P_2 > 0$, $\Delta P_1 + \Delta P_2 > 0$. 6) First Draw, Second Draw (Eventual Win, like away goal or penalty shootouts after extra time) : $\Delta P_1 + \Delta P_2 > 0$. Since the point system is zero-sum, these cases are all we need to consider. I'm struggling on making ideas about how to revise the formula for case 2)~6). Any talk would be appreciated. Thanks.",,"['analysis', 'functions', 'inequality']"
34,Prove $\int\limits_{0}^{\pi/2}\frac{dx}{1+\sin^2{(\tan{x})}}=\frac{\pi}{2\sqrt{2}}\bigl(\frac{e^2+3-2\sqrt{2}}{e^2-3+2\sqrt{2}}\bigr)$,Prove,\int\limits_{0}^{\pi/2}\frac{dx}{1+\sin^2{(\tan{x})}}=\frac{\pi}{2\sqrt{2}}\bigl(\frac{e^2+3-2\sqrt{2}}{e^2-3+2\sqrt{2}}\bigr),Prove the following integral   $$I=\int\limits_{0}^{\frac{\pi}{2}}\dfrac{dx}{1+\sin^2{(\tan{x})}}=\dfrac{\pi}{2\sqrt{2}}\left(\dfrac{e^2+3-2\sqrt{2}}{e^2-3+2\sqrt{2}}\right)$$ This integral result was calculated using Mathematica and  I like this integral. But I can't solve it. My idea: Let $$\tan{x}=t\Longrightarrow dx=\dfrac{1}{1+t^2}dt$$ so $$I=\int\limits_{0}^{\infty}\dfrac{dt}{1+\sin^2{t}}\cdot \dfrac{1}{1+t^2}$$ then I can't proceed. Can you help me? Thank you.,Prove the following integral   $$I=\int\limits_{0}^{\frac{\pi}{2}}\dfrac{dx}{1+\sin^2{(\tan{x})}}=\dfrac{\pi}{2\sqrt{2}}\left(\dfrac{e^2+3-2\sqrt{2}}{e^2-3+2\sqrt{2}}\right)$$ This integral result was calculated using Mathematica and  I like this integral. But I can't solve it. My idea: Let $$\tan{x}=t\Longrightarrow dx=\dfrac{1}{1+t^2}dt$$ so $$I=\int\limits_{0}^{\infty}\dfrac{dt}{1+\sin^2{t}}\cdot \dfrac{1}{1+t^2}$$ then I can't proceed. Can you help me? Thank you.,,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
35,Relation between norm and determinant of a linear operator [closed],Relation between norm and determinant of a linear operator [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question Let $A$ be a $n\times n$ matrix and define $T:\mathbb R^n\to \mathbb R^n$ by $T(X)=AX$. Is there a formula that can present the norm $\|T\|$ as the determinant $\det(A)$?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question Let $A$ be a $n\times n$ matrix and define $T:\mathbb R^n\to \mathbb R^n$ by $T(X)=AX$. Is there a formula that can present the norm $\|T\|$ as the determinant $\det(A)$?,,"['real-analysis', 'linear-algebra', 'analysis', 'determinant']"
36,"multivariable calculus, existence of partials","multivariable calculus, existence of partials",,"Let $f(x,y)$ be a continuous funciton on $I=(a,b)\times (c,d)$ such that (i) $f_x$ exists and continuous on $I$. (ii) For some $x_0\in (a,b)$, $f'(x_0,y)$ exist. (iii) $f_{xy}(x,y)$ exist and continuous on $I$. Show that $f_y$, $f_{xy}$ exists and $f_{xy}=f_{yx}$. I dont know how to approach this. Any hints? Thanks","Let $f(x,y)$ be a continuous funciton on $I=(a,b)\times (c,d)$ such that (i) $f_x$ exists and continuous on $I$. (ii) For some $x_0\in (a,b)$, $f'(x_0,y)$ exist. (iii) $f_{xy}(x,y)$ exist and continuous on $I$. Show that $f_y$, $f_{xy}$ exists and $f_{xy}=f_{yx}$. I dont know how to approach this. Any hints? Thanks",,"['real-analysis', 'analysis', 'multivariable-calculus']"
37,Mathematical Analysis: What is the Velocity of the Falling object through each point in its path?,Mathematical Analysis: What is the Velocity of the Falling object through each point in its path?,,"I have been working through the book called ""Mathematics"" written by A.D. Aleksandrov, A.n. Kolmogorov and M.A. Lavrent'ev recently and have had some difficulty with understanding Examples given by the authors regarding Mathematical Analysis which is the main topic in Chapter 2. I understand the process of working out the increment  $\Delta s=s_2-s_1=\dfrac{g}{2}(2t\Delta t+\Delta t^2)$  which represents the distance covered in the time from $t$ to $t + tΔ$. The authors now describe the process of finding the average velocity over the section of path  Δs by dividing  Δs (which we know from the equation above) by  Δt. This is then shown in the equation below. $v_{av}=\dfrac{\Delta s}{\Delta t}=gt+\dfrac{g}{2}\Delta t$ This is the part of the Example which I don't understand. Firstly what is the value of  Δt and how does one know what to divide by? And how to the authors get a velocity of gt + g/2 x Δt? Is there a 'value' for Δt and if yes how does one know it? Have I missed anything? I have a link for the ebook version 'Mathematics'. The example which is involved in my question can be found on page 66 and 67 (Example 1) This shows the full example and can be read for further understanding when answering my question..... See here . I hope this helps and Thank you for any answers in advance.","I have been working through the book called ""Mathematics"" written by A.D. Aleksandrov, A.n. Kolmogorov and M.A. Lavrent'ev recently and have had some difficulty with understanding Examples given by the authors regarding Mathematical Analysis which is the main topic in Chapter 2. I understand the process of working out the increment  $\Delta s=s_2-s_1=\dfrac{g}{2}(2t\Delta t+\Delta t^2)$  which represents the distance covered in the time from $t$ to $t + tΔ$. The authors now describe the process of finding the average velocity over the section of path  Δs by dividing  Δs (which we know from the equation above) by  Δt. This is then shown in the equation below. $v_{av}=\dfrac{\Delta s}{\Delta t}=gt+\dfrac{g}{2}\Delta t$ This is the part of the Example which I don't understand. Firstly what is the value of  Δt and how does one know what to divide by? And how to the authors get a velocity of gt + g/2 x Δt? Is there a 'value' for Δt and if yes how does one know it? Have I missed anything? I have a link for the ebook version 'Mathematics'. The example which is involved in my question can be found on page 66 and 67 (Example 1) This shows the full example and can be read for further understanding when answering my question..... See here . I hope this helps and Thank you for any answers in advance.",,['analysis']
38,Lebesgue measure is separable?,Lebesgue measure is separable?,,"I would like to better understand the following definition: $(M, \mathcal {A}, \mu) $ a probability space is separable if there exists a countable family $ \mathcal {E} \subset \mathcal {A} $ such that for all $ A \in \mathcal{A} $ and $ \varepsilon> $ 0, there is $ B \in \mathcal {E} $ such that $ \mu (A \triangle B )<\varepsilon$. I wonder if the Lebesgue probability space ($M=[0,1],$ $\mathcal {A}=$Lebesgue measurable sets in [0,1], $\mu=m$) is separable, as finding  $\mathcal {E} $? Thanks for any suggestions","I would like to better understand the following definition: $(M, \mathcal {A}, \mu) $ a probability space is separable if there exists a countable family $ \mathcal {E} \subset \mathcal {A} $ such that for all $ A \in \mathcal{A} $ and $ \varepsilon> $ 0, there is $ B \in \mathcal {E} $ such that $ \mu (A \triangle B )<\varepsilon$. I wonder if the Lebesgue probability space ($M=[0,1],$ $\mathcal {A}=$Lebesgue measurable sets in [0,1], $\mu=m$) is separable, as finding  $\mathcal {E} $? Thanks for any suggestions",,"['analysis', 'measure-theory', 'lebesgue-measure']"
39,Show g is unbounded above if g and g' are increasing,Show g is unbounded above if g and g' are increasing,,"Suppose $g$ is a function defined on the set of real numbers where $g(y)$, $g'(y)$, and $g''(y)$ are all greater than $0$ for all $y \in \mathbb R$. Show that $g$ is unbounded above as $y$ approaches ∞. I know that $g$ and $g'$ are strictly increasing since $g'$ and $g''$ are greater than $0$, but I am having trouble with the rest of the reasoning.","Suppose $g$ is a function defined on the set of real numbers where $g(y)$, $g'(y)$, and $g''(y)$ are all greater than $0$ for all $y \in \mathbb R$. Show that $g$ is unbounded above as $y$ approaches ∞. I know that $g$ and $g'$ are strictly increasing since $g'$ and $g''$ are greater than $0$, but I am having trouble with the rest of the reasoning.",,['real-analysis']
40,Poincare map trouble,Poincare map trouble,,"Consider $ X' = F(X)$, $F \in C^1(\mathbb{R}^2)$. Suppose that the system has an orbit $\mathcal{O}_p$ and $\Sigma$ an transversal section in $P$. Show that if $$\pi^{n+1}(\Sigma) \subset \pi^{n}(\Sigma)$$ and $$\bigcap_{n \geq 1}\pi^{n}(\Sigma) = \mathcal{O} $$ then $\mathcal{O}$ is Lyapunov asymptotically stable.","Consider $ X' = F(X)$, $F \in C^1(\mathbb{R}^2)$. Suppose that the system has an orbit $\mathcal{O}_p$ and $\Sigma$ an transversal section in $P$. Show that if $$\pi^{n+1}(\Sigma) \subset \pi^{n}(\Sigma)$$ and $$\bigcap_{n \geq 1}\pi^{n}(\Sigma) = \mathcal{O} $$ then $\mathcal{O}$ is Lyapunov asymptotically stable.",,"['analysis', 'dynamical-systems']"
41,"""Greatest lower bound function""","""Greatest lower bound function""",,"If $f $ is a function continuous at $c, h $ is positive and $m$ is a function defined as $ m(h)=\inf \{ f(x):  x \in [c,c+h] \}$ ,   how can I prove that the limit of $ m $ as $ h $ approaches $ 0 $ is $ f(c)$ ? Thanks","If $f $ is a function continuous at $c, h $ is positive and $m$ is a function defined as $ m(h)=\inf \{ f(x):  x \in [c,c+h] \}$ ,   how can I prove that the limit of $ m $ as $ h $ approaches $ 0 $ is $ f(c)$ ? Thanks",,"['calculus', 'analysis', 'functions', 'supremum-and-infimum']"
42,Particular $L^p$ space,Particular  space,L^p,"I am confusing some definitions. Suppose we have a Cauchy sequence $(f_n) \subset L^2(\Omega,C^0([0,1],\mathbb{R}))$, where $\Omega$ is a measurable space with measure $\mu$ and $C^0([0,1],\mathbb{R})$ is equipped with the uniform norm (that is a Banach space..). What can we conclude and how ? 1) $(f_n)\,\,$converges to a function $f \in L^2(\Omega,C^0([0,1],\mathbb{R}))$ because it is a complete space ? (But I do not know why is it.. I know for $L^p([a,b], \mathbb{R})$). 2) For each $\omega \in \Omega$, $f(\omega)$ is a continuous function on $[0,1]$, because $C^0([0,1],\mathbb{R})$ is a complete space (with the uniform norm) ? If these two conclusions are true, could someone write explicitly what we need to prove to get these two conclusions ? I think it is $\int_{\Omega} \sup\limits_{t\in[0,1]} \{(f_n(\omega)(t) - f_m(\omega)(t))^2\} d\mu(\omega) \stackrel{n,m\rightarrow\infty}{\longrightarrow}0$, but I am not sure and even if it is true, I do not understand why..  Could someone help me to clarify these few points, I am really confused for a few days ? Thank you so much. Marcus","I am confusing some definitions. Suppose we have a Cauchy sequence $(f_n) \subset L^2(\Omega,C^0([0,1],\mathbb{R}))$, where $\Omega$ is a measurable space with measure $\mu$ and $C^0([0,1],\mathbb{R})$ is equipped with the uniform norm (that is a Banach space..). What can we conclude and how ? 1) $(f_n)\,\,$converges to a function $f \in L^2(\Omega,C^0([0,1],\mathbb{R}))$ because it is a complete space ? (But I do not know why is it.. I know for $L^p([a,b], \mathbb{R})$). 2) For each $\omega \in \Omega$, $f(\omega)$ is a continuous function on $[0,1]$, because $C^0([0,1],\mathbb{R})$ is a complete space (with the uniform norm) ? If these two conclusions are true, could someone write explicitly what we need to prove to get these two conclusions ? I think it is $\int_{\Omega} \sup\limits_{t\in[0,1]} \{(f_n(\omega)(t) - f_m(\omega)(t))^2\} d\mu(\omega) \stackrel{n,m\rightarrow\infty}{\longrightarrow}0$, but I am not sure and even if it is true, I do not understand why..  Could someone help me to clarify these few points, I am really confused for a few days ? Thank you so much. Marcus",,"['analysis', 'functional-analysis', 'measure-theory', 'lp-spaces']"
43,Determine whether this map is an isomorphism,Determine whether this map is an isomorphism,,"Assume all the normed spaces are over $\mathbb{F}=\mathbb{C}$ or $\mathbb{R}$. Let $c$ be the space of all convergent sequences equipped with the supremum norm. For $g\in\ell^1$, define the map $$T_g:c\to \mathbb{F},\quad T_g(f)=g(1)\lim_{k\to\infty}f(k)+\sum_{k=1}^{\infty}g(k+1)f(k).$$ It is easy to see that $T_g\in c^*$. Now let $$\phi:\ell^1\to c^*,\quad \phi(g)=T_g.$$ Show that $\phi$ is an isometry and determine whether $\phi$ is an isomorphism. First I showed that $\phi$ is an isometry. It is trivial that $\phi$ is linear, and it is bounded since $$\Vert T_g\Vert\leq\Vert g\Vert_1.$$ On the other hand, since $g\in\ell^1$, $\lim_{N\to\infty}\sum_{k=N}^{\infty}|g(k)|=0$. So for each $N\in\mathbb{N}$, let $$f_N(k)=\begin{cases}\text{sgn}(g(k+1)),&k\leq N\\ \text{sgn}(g(1)),&k>N\end{cases}$$ So $f_N\in c$ and $\Vert f_N\Vert_\infty=1$ for all $N$. It follows by triangle inequality that $$\Vert T_g(f_N)\Vert\geq\sum_{k=1}^{N+1}|g(k)|-\sum_{k=N+2}^\infty|g(k)|$$ Thus $$\Vert T_g\Vert\geq\lim_{N\to\infty}\sum_{k=1}^{N+1}|g(k)|=\Vert g\Vert_1$$ which means $\phi$ is an isometry. To determine whether $\phi$ is an isomorphism, it suffices to check whether $T_g$ is surjective because an isometry is already injective. At this point I cannot continue and get stuck. Can someone help me with this? Thanks.","Assume all the normed spaces are over $\mathbb{F}=\mathbb{C}$ or $\mathbb{R}$. Let $c$ be the space of all convergent sequences equipped with the supremum norm. For $g\in\ell^1$, define the map $$T_g:c\to \mathbb{F},\quad T_g(f)=g(1)\lim_{k\to\infty}f(k)+\sum_{k=1}^{\infty}g(k+1)f(k).$$ It is easy to see that $T_g\in c^*$. Now let $$\phi:\ell^1\to c^*,\quad \phi(g)=T_g.$$ Show that $\phi$ is an isometry and determine whether $\phi$ is an isomorphism. First I showed that $\phi$ is an isometry. It is trivial that $\phi$ is linear, and it is bounded since $$\Vert T_g\Vert\leq\Vert g\Vert_1.$$ On the other hand, since $g\in\ell^1$, $\lim_{N\to\infty}\sum_{k=N}^{\infty}|g(k)|=0$. So for each $N\in\mathbb{N}$, let $$f_N(k)=\begin{cases}\text{sgn}(g(k+1)),&k\leq N\\ \text{sgn}(g(1)),&k>N\end{cases}$$ So $f_N\in c$ and $\Vert f_N\Vert_\infty=1$ for all $N$. It follows by triangle inequality that $$\Vert T_g(f_N)\Vert\geq\sum_{k=1}^{N+1}|g(k)|-\sum_{k=N+2}^\infty|g(k)|$$ Thus $$\Vert T_g\Vert\geq\lim_{N\to\infty}\sum_{k=1}^{N+1}|g(k)|=\Vert g\Vert_1$$ which means $\phi$ is an isometry. To determine whether $\phi$ is an isomorphism, it suffices to check whether $T_g$ is surjective because an isometry is already injective. At this point I cannot continue and get stuck. Can someone help me with this? Thanks.",,"['analysis', 'functional-analysis']"
44,"If a function $f$ is is differentiable and oscillatory, then is its derivative $f'$ also oscillatory?","If a function  is is differentiable and oscillatory, then is its derivative  also oscillatory?",f f',I know this is true but I'm trying to figure out a proof for it. To clarify what oscillatory means: Oscillatory means that a function $f$ has an unbounded set of roots.,I know this is true but I'm trying to figure out a proof for it. To clarify what oscillatory means: Oscillatory means that a function $f$ has an unbounded set of roots.,,"['real-analysis', 'analysis']"
45,Compactness of a sequence,Compactness of a sequence,,"Let $\theta_n(x,t)$ a sequence such that $$\theta_n\rightarrow\theta\;\;\mbox{in}\;\;C((0,T],H^s)\;\;\mbox{where}\;\;s>1.$$ Consider $\phi \in C^{\infty}$ and $$g_n(t,\phi)=\langle\theta_n(t),\phi\rangle=\int_{\mathbb{T}^2}\theta_n(x,t)\phi(x)dx$$ I can show that $g_n(\cdot,\phi)\in C([0,\tau])$ where $\tau=\frac{T}{2}$ and $$||\dot{g}_n(\cdot,\phi)||_{L^{1+\delta}[0,\tau]}$$ for $\delta>0$. My question is: I can conclude that the sequence $g_n(t,\phi)$ is compact in $C([0,\tau])$?","Let $\theta_n(x,t)$ a sequence such that $$\theta_n\rightarrow\theta\;\;\mbox{in}\;\;C((0,T],H^s)\;\;\mbox{where}\;\;s>1.$$ Consider $\phi \in C^{\infty}$ and $$g_n(t,\phi)=\langle\theta_n(t),\phi\rangle=\int_{\mathbb{T}^2}\theta_n(x,t)\phi(x)dx$$ I can show that $g_n(\cdot,\phi)\in C([0,\tau])$ where $\tau=\frac{T}{2}$ and $$||\dot{g}_n(\cdot,\phi)||_{L^{1+\delta}[0,\tau]}$$ for $\delta>0$. My question is: I can conclude that the sequence $g_n(t,\phi)$ is compact in $C([0,\tau])$?",,"['real-analysis', 'analysis', 'functional-analysis', 'partial-differential-equations']"
46,sawtooth function.,sawtooth function.,,"Let $x \in\mathbb{R}$  $f(x)=\{ x\}$ be  fractional part function or sawtooth function.    and $F_x=\{f(n\ x);\ n\in\mathbb{N}^*\}$ Show that  $x\in\mathbb{Q}\Longleftrightarrow \ \exists\ q \in \mathbb{N}^* \mid\ f(q\ x)=0 $ Let $x=\dfrac{p}{q};\ (p,q)\in\mathbb{Z}\times\mathbb{N}^{*},\ $Show that  : $\forall\ n\in\mathbb{N}^*\ : \ f(n\ x)=f(r\ x)\ $ and deduce that  $F_x\ $ is finite (the number $r$ is to define) all i can say that we've $\forall x \in Z \quad f(x)=0 \iff x$ is in the $Z$  therefore the elements $x$ for which there are $q\in N*$ such that $f(qx)=0$ are the elements of $Q$. any help would be appreciated","Let $x \in\mathbb{R}$  $f(x)=\{ x\}$ be  fractional part function or sawtooth function.    and $F_x=\{f(n\ x);\ n\in\mathbb{N}^*\}$ Show that  $x\in\mathbb{Q}\Longleftrightarrow \ \exists\ q \in \mathbb{N}^* \mid\ f(q\ x)=0 $ Let $x=\dfrac{p}{q};\ (p,q)\in\mathbb{Z}\times\mathbb{N}^{*},\ $Show that  : $\forall\ n\in\mathbb{N}^*\ : \ f(n\ x)=f(r\ x)\ $ and deduce that  $F_x\ $ is finite (the number $r$ is to define) all i can say that we've $\forall x \in Z \quad f(x)=0 \iff x$ is in the $Z$  therefore the elements $x$ for which there are $q\in N*$ such that $f(qx)=0$ are the elements of $Q$. any help would be appreciated",,"['calculus', 'real-analysis', 'algebra-precalculus', 'analysis']"
47,Help with an analysis problem involving isomorphisms and volumes.,Help with an analysis problem involving isomorphisms and volumes.,,"I would like some help solving the following problem: Let $f: U \subseteq \mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ be a class $C^{1}$ function. Suppose that for some $a \in U$ the derivative of $f$ in $a$ is an isomorphism. Prove that $$ \lim_{r \rightarrow 0} \frac{Vol \left( f \left( B\left[ a; r\right] \right) \right)}{Vol \left( B\left[ a; r\right] \right)} = \left| \det f^{\prime} \left( a \right) \right|,$$ where $Vol$ means volume and $B\left[ a; r\right] = \{ u \in \mathbb{R}^{m}: \| u-a \| \leq r \}$ is the closed ball. So far, I only know that the right side of the equation is never zero, and was thinking that the hypothesis in this problem are the same as the Inverse Function Theorem. However I don't know if it is really of use here since it gives a result about $f \left(U\right)$ and the problem involves $f \left( B\left[ a; r\right] \right).$ I need some hints to get moving; I do not want the full solution of the problem,  since this is an exam-type exercise and I need to learn how to solve it by myself.","I would like some help solving the following problem: Let $f: U \subseteq \mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ be a class $C^{1}$ function. Suppose that for some $a \in U$ the derivative of $f$ in $a$ is an isomorphism. Prove that $$ \lim_{r \rightarrow 0} \frac{Vol \left( f \left( B\left[ a; r\right] \right) \right)}{Vol \left( B\left[ a; r\right] \right)} = \left| \det f^{\prime} \left( a \right) \right|,$$ where $Vol$ means volume and $B\left[ a; r\right] = \{ u \in \mathbb{R}^{m}: \| u-a \| \leq r \}$ is the closed ball. So far, I only know that the right side of the equation is never zero, and was thinking that the hypothesis in this problem are the same as the Inverse Function Theorem. However I don't know if it is really of use here since it gives a result about $f \left(U\right)$ and the problem involves $f \left( B\left[ a; r\right] \right).$ I need some hints to get moving; I do not want the full solution of the problem,  since this is an exam-type exercise and I need to learn how to solve it by myself.",,"['real-analysis', 'linear-algebra', 'analysis', 'geometry']"
48,Fubini's theorem application proof check,Fubini's theorem application proof check,,"I have proven a problem but I am unsure whether it is correct because the proof seems so simple that I think I might be mistaken. Please be kind to comment on my proof and tell me whats wrong with it. The problem: Let $A$ be a rectangle in $\Bbb R^k$ and B a rectangle in $\Bbb R^n$. Let $Q=A\times B$. Let $f:Q\to R$ be bounded and Riemann integrable. Show that if $\int_Qf$ exists, then $\int_yf(x,y)$ exists for $x\in A-D$  where D is a set of measure zero in $\Bbb R^k$. My idea: It just seems like a direct application of Fubini theorem. I wrote that if $\int_Qf $ exists, then by fubini theorem, $\int_Qf=\int_x\int_y f$ and it implies that $\int_yf(x,y)$ exists. Fubini's theorem version that I use: Let $A$ be a rectangle in $\Bbb R^k$ and B a rectangle in $\Bbb R^n$. Let $Q=A\times B$. Let $f:Q\to R$ be bounded. $f$ is in the form $f(x,y)$ for $x\in A, y\in B$. If f is integrable over Q, then $$\int_Q f=\int_x \int_yf=\int_y\int_xf$$ Is this right? Thank you very much for your help","I have proven a problem but I am unsure whether it is correct because the proof seems so simple that I think I might be mistaken. Please be kind to comment on my proof and tell me whats wrong with it. The problem: Let $A$ be a rectangle in $\Bbb R^k$ and B a rectangle in $\Bbb R^n$. Let $Q=A\times B$. Let $f:Q\to R$ be bounded and Riemann integrable. Show that if $\int_Qf$ exists, then $\int_yf(x,y)$ exists for $x\in A-D$  where D is a set of measure zero in $\Bbb R^k$. My idea: It just seems like a direct application of Fubini theorem. I wrote that if $\int_Qf $ exists, then by fubini theorem, $\int_Qf=\int_x\int_y f$ and it implies that $\int_yf(x,y)$ exists. Fubini's theorem version that I use: Let $A$ be a rectangle in $\Bbb R^k$ and B a rectangle in $\Bbb R^n$. Let $Q=A\times B$. Let $f:Q\to R$ be bounded. $f$ is in the form $f(x,y)$ for $x\in A, y\in B$. If f is integrable over Q, then $$\int_Q f=\int_x \int_yf=\int_y\int_xf$$ Is this right? Thank you very much for your help",,"['real-analysis', 'integration', 'analysis', 'functions', 'proof-verification']"
49,Continuous functions and IVT,Continuous functions and IVT,,"Suppose that $f$ is continuous on $[0,1]$ and $f(0) < 0$ and $f(1) > 1$. Show that there is some $x \in (0,1)$ such that $f(x) = x^{2014}$. I am guessing that here we should use the Intermediate Value Theorem. It states: Suppose that $f$ is continuous on $[a,b]$ and $k$ is between $f(a)$ and $f(b)$, then there exists $c \in (a,b)$ such that $f(c) = k$. But from there I am not sure how to prove that $f(x) = x^{2014}$. Could anyone help me out please? Thanks.","Suppose that $f$ is continuous on $[0,1]$ and $f(0) < 0$ and $f(1) > 1$. Show that there is some $x \in (0,1)$ such that $f(x) = x^{2014}$. I am guessing that here we should use the Intermediate Value Theorem. It states: Suppose that $f$ is continuous on $[a,b]$ and $k$ is between $f(a)$ and $f(b)$, then there exists $c \in (a,b)$ such that $f(c) = k$. But from there I am not sure how to prove that $f(x) = x^{2014}$. Could anyone help me out please? Thanks.",,"['real-analysis', 'analysis']"
50,"Prove that the normed vector space $(S_F,\|\cdot\|_1)$ is not Banach.",Prove that the normed vector space  is not Banach.,"(S_F,\|\cdot\|_1)",$S_F$ is the space of real sequences $\mathbf a=(a_n)_{n=1}^{\infty}$ such that every sequence $\mathbf a\in S_F$ is eventually zero. $\|\cdot\|_1$ is the norm defined as $\|\mathbf a\|_1=\sum_{n=1}^{\infty}\lvert a_n\rvert$. I know that a Banach space is one where every Cauchy sequence in $S_F$ converges to an element of $S_F$ but I don't know how to prove that a normed vector space is not Banach. Can anyone help?,$S_F$ is the space of real sequences $\mathbf a=(a_n)_{n=1}^{\infty}$ such that every sequence $\mathbf a\in S_F$ is eventually zero. $\|\cdot\|_1$ is the norm defined as $\|\mathbf a\|_1=\sum_{n=1}^{\infty}\lvert a_n\rvert$. I know that a Banach space is one where every Cauchy sequence in $S_F$ converges to an element of $S_F$ but I don't know how to prove that a normed vector space is not Banach. Can anyone help?,,"['real-analysis', 'analysis']"
51,Requesting information on constructed discontinuous functions (from any perspective),Requesting information on constructed discontinuous functions (from any perspective),,"Suppose $f:\mathbb{R}\rightarrow \mathbb{R}$ is a continuous function. Define the function $F:\mathbb{R}\rightarrow \mathbb{R}$ as $$F(x)=f(x)\prod_{n=1}^\infty\frac{x-\frac{1}{n}}{x-\frac{1}{n}}$$ I have several questions regarding the above, and similarly constructed functions. Intuitively, the function $F(x)$ is almost the same function as $f(x)$. But, there are countably many singularities which have an accumulation point at $f(0)$. The function $F$ also retains the limit structure of $f$ (i.e. $\lim\limits_{x\rightarrow y}F(x)=\lim\limits_{x\rightarrow y}f(x) = f(y)$). I have not had an in depth study of integration. Although, since the set of $x$ such that $F(x)$ is not defined has measure $0$, I would expect some form of integration to be consistent for all intervals of the real line. So the singularities of $F$ seem trivial. However, I would argue the function $F$ is strictly different than the function $f$. Since the construction of $F$ required the knowledge of the continuous function $f$, it seems clear that we can, in some manner, ""patch"" the holes, and treat this function as $f$. Question 1 : Are there certain properties of $F$ which would intuitively be the same as $f$, but are not the same under comparison? (I understand that, looking at $F$ and $f$ as a 1-manifold, there are certainly many topological properties which are not consistent, but this is attributed to $F$ being discontinuous. I'm looking more for an analysts perspective here). Question(s) 2 : If we looked instead at functions of a complex variable, does the $F$ lose any of the information contained in $f$? I mean to say, if $f$ is a continuous and analytic function, then would integrating around the closed contour $|z|=2$ be the same as for $F$? What must be true to about the singularities for this to change? I apologize if I am vague. I'm not looking for answers to each question. I've just been thinking about this and wondering if there is any information regarding functions like the one above. If I am misunderstanding anything, I would appreciate being straightened out and have see my ideas written formally. To the same extent, don't go easy on me if you can explain the topic!","Suppose $f:\mathbb{R}\rightarrow \mathbb{R}$ is a continuous function. Define the function $F:\mathbb{R}\rightarrow \mathbb{R}$ as $$F(x)=f(x)\prod_{n=1}^\infty\frac{x-\frac{1}{n}}{x-\frac{1}{n}}$$ I have several questions regarding the above, and similarly constructed functions. Intuitively, the function $F(x)$ is almost the same function as $f(x)$. But, there are countably many singularities which have an accumulation point at $f(0)$. The function $F$ also retains the limit structure of $f$ (i.e. $\lim\limits_{x\rightarrow y}F(x)=\lim\limits_{x\rightarrow y}f(x) = f(y)$). I have not had an in depth study of integration. Although, since the set of $x$ such that $F(x)$ is not defined has measure $0$, I would expect some form of integration to be consistent for all intervals of the real line. So the singularities of $F$ seem trivial. However, I would argue the function $F$ is strictly different than the function $f$. Since the construction of $F$ required the knowledge of the continuous function $f$, it seems clear that we can, in some manner, ""patch"" the holes, and treat this function as $f$. Question 1 : Are there certain properties of $F$ which would intuitively be the same as $f$, but are not the same under comparison? (I understand that, looking at $F$ and $f$ as a 1-manifold, there are certainly many topological properties which are not consistent, but this is attributed to $F$ being discontinuous. I'm looking more for an analysts perspective here). Question(s) 2 : If we looked instead at functions of a complex variable, does the $F$ lose any of the information contained in $f$? I mean to say, if $f$ is a continuous and analytic function, then would integrating around the closed contour $|z|=2$ be the same as for $F$? What must be true to about the singularities for this to change? I apologize if I am vague. I'm not looking for answers to each question. I've just been thinking about this and wondering if there is any information regarding functions like the one above. If I am misunderstanding anything, I would appreciate being straightened out and have see my ideas written formally. To the same extent, don't go easy on me if you can explain the topic!",,"['analysis', 'functions', 'soft-question', 'singularity-theory']"
52,Changes in the unit ball,Changes in the unit ball,,"Given the open unit ball in $\mathbb{R}^2$ (or for any other $\mathbb{R}^n$ as well), if I use the function $f:B^2 \rightarrow \mathbb{R}^2$ that does the following: $f(x,y)= (\frac{x}{2}, \frac{y}{2} )$, will the result on the entire ball, i.e. $f(B^2)$, be the ball with a radius of $\frac{1}{2}$ (with same center)? Or perhaps with a radius of $\frac{1}{4}$? Edit: And another question is how can I move the ball to another ball with a different center point? should I add to just one ( $x$ or $y$ ) coordinate or to both? What I mean is, if I have a ball with radius $r$ and center point $x_0$ and I want to transfer this ball with to a ball with same radius $r$, just with a center $x_1$, what type of function should I need? (the same questions applies in general to $\mathbb{R}^n$, just thought $\mathbb{R}^2$ would be easier to see)","Given the open unit ball in $\mathbb{R}^2$ (or for any other $\mathbb{R}^n$ as well), if I use the function $f:B^2 \rightarrow \mathbb{R}^2$ that does the following: $f(x,y)= (\frac{x}{2}, \frac{y}{2} )$, will the result on the entire ball, i.e. $f(B^2)$, be the ball with a radius of $\frac{1}{2}$ (with same center)? Or perhaps with a radius of $\frac{1}{4}$? Edit: And another question is how can I move the ball to another ball with a different center point? should I add to just one ( $x$ or $y$ ) coordinate or to both? What I mean is, if I have a ball with radius $r$ and center point $x_0$ and I want to transfer this ball with to a ball with same radius $r$, just with a center $x_1$, what type of function should I need? (the same questions applies in general to $\mathbb{R}^n$, just thought $\mathbb{R}^2$ would be easier to see)",,['analysis']
53,Can this summation be expressed differently?,Can this summation be expressed differently?,,"Lets say I have a sum that states the following $$ \sum_{j=0}^{k-c} {k-c \choose j}\ln(a)^{k-c-j} \frac{d^j}{dx^j}[(x)_c] $$ where $(x)_c$ is the falling factorial such that $$ (x)_c = x(x-1)(x-2)\cdots(x-c+1) $$ How can I evaluate this summation? Viewing the summations partially, I know that $$ \sum_{j=0}^{k-c}{k-c \choose j}\ln(a)^{k-c-j} = \left(\frac{1}{\ln(a)}+1\right)^{k-c} \ln(a)^{k-c} $$ But it is possible that $k-c > c$ and therefore having the summation above be incomplete due to differentiating a constant, resulting in an incomplete gamma function result seen here: http://www.wolframalpha.com/input/?i=sum+%28%28k-c%29%21%2F%28%28k-c-r%29%21%29%29ln%28y%29%5E%28k-c-r%29%2C+r%3D0..k-c%2Bn An interesting thing to note is that taking multiple derivatives of the falling factorial x results in the sum of the permutations that the falling factorial can be presented in, multiplied by the factorial of the amount of times differentiated. an example would be: $$ \frac{d}{dx} [x(x-1)(x-2)(x-3)] = 1![x(x-1)(x-2)+x(x-1)(x-3)+x(x-2)(x-3)+(x-1)(x-2)(x-3)] $$ $$ \frac{d^2}{dx^2} [x(x-1)(x-2)(x-3)] = 2![x(x-1)+x(x-2)+x(x-3)+(x-1)(x-2)+(x-1)(x-3)+(x-2)(x-3)] $$ Note that the number of terms produced by taking $n$ derivatives of $(x)_c$ is directly proportional to ${n \choose c}$ Is there no defined way to express this summation? Thank you for any information in advance.","Lets say I have a sum that states the following $$ \sum_{j=0}^{k-c} {k-c \choose j}\ln(a)^{k-c-j} \frac{d^j}{dx^j}[(x)_c] $$ where $(x)_c$ is the falling factorial such that $$ (x)_c = x(x-1)(x-2)\cdots(x-c+1) $$ How can I evaluate this summation? Viewing the summations partially, I know that $$ \sum_{j=0}^{k-c}{k-c \choose j}\ln(a)^{k-c-j} = \left(\frac{1}{\ln(a)}+1\right)^{k-c} \ln(a)^{k-c} $$ But it is possible that $k-c > c$ and therefore having the summation above be incomplete due to differentiating a constant, resulting in an incomplete gamma function result seen here: http://www.wolframalpha.com/input/?i=sum+%28%28k-c%29%21%2F%28%28k-c-r%29%21%29%29ln%28y%29%5E%28k-c-r%29%2C+r%3D0..k-c%2Bn An interesting thing to note is that taking multiple derivatives of the falling factorial x results in the sum of the permutations that the falling factorial can be presented in, multiplied by the factorial of the amount of times differentiated. an example would be: $$ \frac{d}{dx} [x(x-1)(x-2)(x-3)] = 1![x(x-1)(x-2)+x(x-1)(x-3)+x(x-2)(x-3)+(x-1)(x-2)(x-3)] $$ $$ \frac{d^2}{dx^2} [x(x-1)(x-2)(x-3)] = 2![x(x-1)+x(x-2)+x(x-3)+(x-1)(x-2)+(x-1)(x-3)+(x-2)(x-3)] $$ Note that the number of terms produced by taking $n$ derivatives of $(x)_c$ is directly proportional to ${n \choose c}$ Is there no defined way to express this summation? Thank you for any information in advance.",,"['calculus', 'combinatorics', 'analysis', 'derivatives', 'summation']"
54,Weak convergence in $l_p$ implies pointwise convergence? [duplicate],Weak convergence in  implies pointwise convergence? [duplicate],l_p,"This question already has an answer here : weak convergence in $l^p$ implies bounded and pointwise convergence (1 answer) Closed 7 years ago . Could someone please share their thoughts on this one: Consider at $l_p(Y)$, for $1<p<\infty$ with the counting measure on $Y$. Show that if a sequence weakly converges in $l_p(Y)$ then it would converge pointwise in Y. Show that the converse holds only when Y is finite. Thanks!","This question already has an answer here : weak convergence in $l^p$ implies bounded and pointwise convergence (1 answer) Closed 7 years ago . Could someone please share their thoughts on this one: Consider at $l_p(Y)$, for $1<p<\infty$ with the counting measure on $Y$. Show that if a sequence weakly converges in $l_p(Y)$ then it would converge pointwise in Y. Show that the converse holds only when Y is finite. Thanks!",,"['real-analysis', 'analysis', 'measure-theory', 'convergence-divergence', 'lp-spaces']"
55,How to apply Fubini's theorem in proof of Osgood's lemma,How to apply Fubini's theorem in proof of Osgood's lemma,,"In the proof of Osgood's lemma for seperate holomorphicity, at one step we get that $$f(z)=\frac{1}{(2\pi \iota)^n}\int_{|w_i-\zeta_i|=r_i}\sum_{v_1,v_2,\ldots,v_n}\frac{f(\zeta)z_1^{v_1}\ldots z_n^{v_n}}{\zeta_1^{v_1+1}\ldots \zeta_n^{v_n+1}}d\zeta_1\ldots d\zeta_n,$$  (call the integrand $g$) for $z\in{\triangle (w,r)}.$ After this step i am supposed to change the summation and integration. But i am not able to justify this change.\ MY ATTEMPT: I am taking space $X=\mathbb{C}^n$ with product measure=$\mu$ and $Y=\mathbb{N}^n$ with counting measure=$\nu$. For Fubini's theorem i need to show that $g$ is integrable i product measure $\mu \times \nu.$ Can anyone help me in proving this. Or is there any other method to see this.","In the proof of Osgood's lemma for seperate holomorphicity, at one step we get that $$f(z)=\frac{1}{(2\pi \iota)^n}\int_{|w_i-\zeta_i|=r_i}\sum_{v_1,v_2,\ldots,v_n}\frac{f(\zeta)z_1^{v_1}\ldots z_n^{v_n}}{\zeta_1^{v_1+1}\ldots \zeta_n^{v_n+1}}d\zeta_1\ldots d\zeta_n,$$  (call the integrand $g$) for $z\in{\triangle (w,r)}.$ After this step i am supposed to change the summation and integration. But i am not able to justify this change.\ MY ATTEMPT: I am taking space $X=\mathbb{C}^n$ with product measure=$\mu$ and $Y=\mathbb{N}^n$ with counting measure=$\nu$. For Fubini's theorem i need to show that $g$ is integrable i product measure $\mu \times \nu.$ Can anyone help me in proving this. Or is there any other method to see this.",,"['complex-analysis', 'analysis', 'measure-theory', 'proof-writing', 'several-complex-variables']"
56,Coordinate wise convergence of bounded sequences,Coordinate wise convergence of bounded sequences,,"Show that if $(x^{(n)})$ is a bounded sequence in $l^\infty$, then there exist a subsequence $x^{(n_k)}$ that converge coordinate wise. Is this some generalization of Bolzano-Weierstrass Theorem?","Show that if $(x^{(n)})$ is a bounded sequence in $l^\infty$, then there exist a subsequence $x^{(n_k)}$ that converge coordinate wise. Is this some generalization of Bolzano-Weierstrass Theorem?",,"['real-analysis', 'analysis']"
57,Dual Radon transform: different conventions?,Dual Radon transform: different conventions?,,"I am having a hard time trying to understand apparently two different definitions of the dual Radon Transform. I am reading simultaneously the book ""Mathematics of computerized tomography"", by Frank Natterer, and ""Radon transform on homogeneous spaces"", by Sigurdur Helgason (by now, only the first chapter of each book). I will try to be thorough in the exposition of my doubt(s). To this end, let me explain a little bit the treatment of this subject in each author, to save you from the trouble of having to look at the books mentioned. (Note: in the sequel, $\omega_{n}$ is the surface area of the unit sphere in $\mathbb{R}^{n},$ $\mathcal{P}$ is the set of all hyperplanes, $R(f)(\pi)=\int_{\pi}f(y)dm(y)$ is the Radon transform, $R(f)(\theta,s)=\int_{\{ \langle x,\theta \rangle =0 \}}f(x+s\theta)dm(x),$ is also the Radon transform, seen as a symmetric function over the cylinder ) Let me start with Natterer. He defines the dual Radon transform as follows:  first, a continuous function $g(\pi)$ in the set of hyperplanes can be identified with a symmetric continuous function in $S^{n-1} \times \mathbb{R}$ of the form $g(\theta, s),$ where $\theta$ is the normal vector to the hyperplane and $s$ is the distance from the plane to the origin.  Then Natterer defines the dual Radon transform as: $ R^{\#}(g)(x):=\displaystyle \int _{S^{n-1}} g(\theta ,\langle x,\theta \rangle ) dS(\theta).$ He strongly motivates this definition, so I take it is not a misprint. Is with this definition that he derives an important formula (in particular, to derive the inversion theorem): $$R^{\#}R(f)(x)=\omega_{n-1}\int_{\mathbb{R}^{n}} \vert x-y \vert ^{-1}f(y) dy.$$ Now, let me very briefly explain what Helgason does: The dual transform is now defined as $R^{\#}(g)(x)=\displaystyle \int_{\{ \pi \in \mathcal{P} \hspace{1mm}:\hspace{1mm} x \in \pi \} }g(\pi) d\mu,$ where $\mu$ is the unique normalized measure invariant under rotations. Helgason readily says that this must be $\displaystyle c\int_{S^{n-1}} g(\theta,\langle x,\theta \rangle) dS(\theta)$ for certain constant $c,$ which seems very logical but I don't seem to be able to prove rigorously . Of course, for Natterer $c=1,$ but the point is that this is not so for Helgason.  For Helgason, is trivial that his definition implies that $R^{\#}(g)(x)=\displaystyle \int_{O(n)} g(x+k \pi_{0}) dk,$ where $dk$ is the Haar measure on the orthogonal group and $\pi_{0}$ is any fixed hyperplane containing the origin (so, another doubt , how do we prove this?). Now, an argument based upon the uniqueness of normalized rotationally invariant measures shows that $c=\frac{1}{\omega_{n}}.$ Due to this, Helgason derives the formula: $$R^{\#}R(f)(x)=\frac{\omega_{n-1}}{\omega_{n}}\int_{\mathbb{R}^{n}} \vert x-y \vert ^{-1}f(y) dy.$$ My feeling is that Helgason's point of view is more correct, but I cannot completely grasp his arguments, and he seems a little thick in his exposition at some points. I am worried, because I do not know what's going on with Natterer's book. So, to sum up, I want to clarify: 1º: The arguments used by Helgason, which I have marked as doubts. 2º: The convention used by Natterer, which seems less logical than Helgason's. Also, I'd like to find out which convention or criterion is more widely employed. As a last point, if you have any suggestions about other books which I could use in my study of the Radon Transform, I will appreciate them! Thank you. EDIT:  From a quick overview of several lecture notes available online, I take that Natterer's convention is pretty common. This seems weird to me, for that convention is incompatible with the natural definition of Helgason $R^{\#}(g)(x)=\displaystyle \int_{\{ \pi \in \mathcal{P} \hspace{1mm}:\hspace{1mm} x \in \pi \} }g(\pi) d\mu.$","I am having a hard time trying to understand apparently two different definitions of the dual Radon Transform. I am reading simultaneously the book ""Mathematics of computerized tomography"", by Frank Natterer, and ""Radon transform on homogeneous spaces"", by Sigurdur Helgason (by now, only the first chapter of each book). I will try to be thorough in the exposition of my doubt(s). To this end, let me explain a little bit the treatment of this subject in each author, to save you from the trouble of having to look at the books mentioned. (Note: in the sequel, $\omega_{n}$ is the surface area of the unit sphere in $\mathbb{R}^{n},$ $\mathcal{P}$ is the set of all hyperplanes, $R(f)(\pi)=\int_{\pi}f(y)dm(y)$ is the Radon transform, $R(f)(\theta,s)=\int_{\{ \langle x,\theta \rangle =0 \}}f(x+s\theta)dm(x),$ is also the Radon transform, seen as a symmetric function over the cylinder ) Let me start with Natterer. He defines the dual Radon transform as follows:  first, a continuous function $g(\pi)$ in the set of hyperplanes can be identified with a symmetric continuous function in $S^{n-1} \times \mathbb{R}$ of the form $g(\theta, s),$ where $\theta$ is the normal vector to the hyperplane and $s$ is the distance from the plane to the origin.  Then Natterer defines the dual Radon transform as: $ R^{\#}(g)(x):=\displaystyle \int _{S^{n-1}} g(\theta ,\langle x,\theta \rangle ) dS(\theta).$ He strongly motivates this definition, so I take it is not a misprint. Is with this definition that he derives an important formula (in particular, to derive the inversion theorem): $$R^{\#}R(f)(x)=\omega_{n-1}\int_{\mathbb{R}^{n}} \vert x-y \vert ^{-1}f(y) dy.$$ Now, let me very briefly explain what Helgason does: The dual transform is now defined as $R^{\#}(g)(x)=\displaystyle \int_{\{ \pi \in \mathcal{P} \hspace{1mm}:\hspace{1mm} x \in \pi \} }g(\pi) d\mu,$ where $\mu$ is the unique normalized measure invariant under rotations. Helgason readily says that this must be $\displaystyle c\int_{S^{n-1}} g(\theta,\langle x,\theta \rangle) dS(\theta)$ for certain constant $c,$ which seems very logical but I don't seem to be able to prove rigorously . Of course, for Natterer $c=1,$ but the point is that this is not so for Helgason.  For Helgason, is trivial that his definition implies that $R^{\#}(g)(x)=\displaystyle \int_{O(n)} g(x+k \pi_{0}) dk,$ where $dk$ is the Haar measure on the orthogonal group and $\pi_{0}$ is any fixed hyperplane containing the origin (so, another doubt , how do we prove this?). Now, an argument based upon the uniqueness of normalized rotationally invariant measures shows that $c=\frac{1}{\omega_{n}}.$ Due to this, Helgason derives the formula: $$R^{\#}R(f)(x)=\frac{\omega_{n-1}}{\omega_{n}}\int_{\mathbb{R}^{n}} \vert x-y \vert ^{-1}f(y) dy.$$ My feeling is that Helgason's point of view is more correct, but I cannot completely grasp his arguments, and he seems a little thick in his exposition at some points. I am worried, because I do not know what's going on with Natterer's book. So, to sum up, I want to clarify: 1º: The arguments used by Helgason, which I have marked as doubts. 2º: The convention used by Natterer, which seems less logical than Helgason's. Also, I'd like to find out which convention or criterion is more widely employed. As a last point, if you have any suggestions about other books which I could use in my study of the Radon Transform, I will appreciate them! Thank you. EDIT:  From a quick overview of several lecture notes available online, I take that Natterer's convention is pretty common. This seems weird to me, for that convention is incompatible with the natural definition of Helgason $R^{\#}(g)(x)=\displaystyle \int_{\{ \pi \in \mathcal{P} \hspace{1mm}:\hspace{1mm} x \in \pi \} }g(\pi) d\mu.$",,['analysis']
58,An interesting proof using Green's representation formula?,An interesting proof using Green's representation formula?,,"Let $B_R(0)$ be a ball in $\mathbb R^3 $ and define $$u(x)=\int_{B_R(0)}\frac{1}{|y-x|}dy$$ Prove that $$ u(x) =   \begin{cases}     \frac{2}{3}\pi(3R^2-|x|^2) & \quad \text{for $0 \le|x|\le R$ }\\[8pt]     \frac{4}{3}\pi \frac{R^3}{|x|} & \quad \text{for $|x|>R$}   \end{cases}$$ My attempt: For $|x|>R$, $u$ is harmonic and it goes to zero as $|x|$ goes to infinity, so we can use the fundamental solution $\frac{c}{|x|}$, c is some constant. By the continuity of $u(x)$, we get $u(x)=\frac{4}{3}\pi \frac{R^3}{|x|}$. For $|x| \le R$, I found that the function $w(x)=\frac{2}{3}\pi(3R^2-|x|^2)$ solves $\Delta w=-4\pi$, and $w(x)=\frac{4}{3}\pi R^3$ on $\partial B_R(0)$. Therefore I can use the Green's representatio formula in a ball. So I get something like this, when $n=3$: $$u(x)=\frac{R^2-|x|^2}{R^2}\int _{\partial B_R(0)}\frac{1}{|x-y|^3}ds(y)+\frac{3}{R^3}\int_{B_R(0)}\frac{1}{|x-y|}-\frac{R|y|}{|y|^2x-R^2y}\,dy,$$ t hen I don't know what to do next to get the desired equation $u(x)=\int_{B_R(0)}\frac{1}{|y-x|}dy$. This is really bothering me, your help will be appreciated!:)","Let $B_R(0)$ be a ball in $\mathbb R^3 $ and define $$u(x)=\int_{B_R(0)}\frac{1}{|y-x|}dy$$ Prove that $$ u(x) =   \begin{cases}     \frac{2}{3}\pi(3R^2-|x|^2) & \quad \text{for $0 \le|x|\le R$ }\\[8pt]     \frac{4}{3}\pi \frac{R^3}{|x|} & \quad \text{for $|x|>R$}   \end{cases}$$ My attempt: For $|x|>R$, $u$ is harmonic and it goes to zero as $|x|$ goes to infinity, so we can use the fundamental solution $\frac{c}{|x|}$, c is some constant. By the continuity of $u(x)$, we get $u(x)=\frac{4}{3}\pi \frac{R^3}{|x|}$. For $|x| \le R$, I found that the function $w(x)=\frac{2}{3}\pi(3R^2-|x|^2)$ solves $\Delta w=-4\pi$, and $w(x)=\frac{4}{3}\pi R^3$ on $\partial B_R(0)$. Therefore I can use the Green's representatio formula in a ball. So I get something like this, when $n=3$: $$u(x)=\frac{R^2-|x|^2}{R^2}\int _{\partial B_R(0)}\frac{1}{|x-y|^3}ds(y)+\frac{3}{R^3}\int_{B_R(0)}\frac{1}{|x-y|}-\frac{R|y|}{|y|^2x-R^2y}\,dy,$$ t hen I don't know what to do next to get the desired equation $u(x)=\int_{B_R(0)}\frac{1}{|y-x|}dy$. This is really bothering me, your help will be appreciated!:)",,"['analysis', 'partial-differential-equations', 'harmonic-analysis']"
59,Calculate difficult Fourier Transform,Calculate difficult Fourier Transform,,I have to calculate a quite difficult Fourier Transform for my class $$\int_{-\infty}^\infty dw\frac{(\varGamma-iw)w^3}{(\varGamma-iw)^2+1}\frac{J_{1}(|wr|)}{|wr|}e^{-iwt}$$ $J_1$ is the normal Bessel function of first order. Please help me.,I have to calculate a quite difficult Fourier Transform for my class $$\int_{-\infty}^\infty dw\frac{(\varGamma-iw)w^3}{(\varGamma-iw)^2+1}\frac{J_{1}(|wr|)}{|wr|}e^{-iwt}$$ $J_1$ is the normal Bessel function of first order. Please help me.,,"['analysis', 'fourier-analysis']"
60,Integrals of compactly supported functions of positive type,Integrals of compactly supported functions of positive type,,"Consider a continuous function $f: \mathbb{R} \rightarrow \mathbb{R}$, supported on $[-1,1]$, of positive type. Assume $f(0) = 1$; what is the ""largest"" area $\int f\,dx$ that can be achieved? To be more precise, let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a continuous function satisfying: $f$ is supported on $[-1,1]$, for all $x$, $0 \leq f(x) \leq 1 = f(0)$, and $f$ has positive type: for any finite family of points $x_1 < \cdots < x_n$ in $\mathbb{R}$, the matrix $(f(x_i - x_j))_{ij}$ is positive semi-definite. How large can $\int f(x)\,dx$ be? Remarks One example of such a function is the ""triangle"" $t(x) = \max(1 - |x|,0)$. This achieves $\int t(x)\,dx = 1$. Is that the best one can do? There are functions $g$ of positive type (and satisfying the other requirements above) for which $g(x) > t(x)$ for some $x$. (However, I do not know of any for which $\int g(x)\,dx > 1$.)","Consider a continuous function $f: \mathbb{R} \rightarrow \mathbb{R}$, supported on $[-1,1]$, of positive type. Assume $f(0) = 1$; what is the ""largest"" area $\int f\,dx$ that can be achieved? To be more precise, let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a continuous function satisfying: $f$ is supported on $[-1,1]$, for all $x$, $0 \leq f(x) \leq 1 = f(0)$, and $f$ has positive type: for any finite family of points $x_1 < \cdots < x_n$ in $\mathbb{R}$, the matrix $(f(x_i - x_j))_{ij}$ is positive semi-definite. How large can $\int f(x)\,dx$ be? Remarks One example of such a function is the ""triangle"" $t(x) = \max(1 - |x|,0)$. This achieves $\int t(x)\,dx = 1$. Is that the best one can do? There are functions $g$ of positive type (and satisfying the other requirements above) for which $g(x) > t(x)$ for some $x$. (However, I do not know of any for which $\int g(x)\,dx > 1$.)",,"['analysis', 'fourier-analysis', 'harmonic-analysis', 'duality-theorems']"
61,Calculate the flux of $\underline{v}$ across the boundary of the sector.,Calculate the flux of  across the boundary of the sector.,\underline{v},"For $a\in(0,1)$ , calculate without use of the divergence theorem the flux of $\underline{v}(x,y) = g(y/x)(-1/x,1/y)$ across the boundary of the sector $ S_a := \{(x,y)\in \Omega : 1\leqslant x^2+y^2 \leqslant 4, a\leqslant y/x\leqslant 1/a \} $ where $\Omega:=\{(x,y): x>0, y>0\} $ . Initial progress: I've drawn out what I think $S_a$ looks like and it's a sector of an annulus in the upper right quadrant of the $x,y$ plane. I've worked out the flux across each of the straight sides of the boundary and got $(a+1/a)g(1/a)ln(2)$ and $-(a+1/a)g(a)ln(2)$ for the other. The issue I'm having is with the two curved sides of the boundary because I'm unsure how to parametrise them. Is the work I've done so far correct? And how would I finish it off? New progress: So I used that parametrisation and worked out the flux across both of the curved sides of the boundary to be $0$ . Is this correct? Does this mean the answer is $Flux= (a+1/a)ln(2)(g(1/a)-g(a))$ ?","For , calculate without use of the divergence theorem the flux of across the boundary of the sector where . Initial progress: I've drawn out what I think looks like and it's a sector of an annulus in the upper right quadrant of the plane. I've worked out the flux across each of the straight sides of the boundary and got and for the other. The issue I'm having is with the two curved sides of the boundary because I'm unsure how to parametrise them. Is the work I've done so far correct? And how would I finish it off? New progress: So I used that parametrisation and worked out the flux across both of the curved sides of the boundary to be . Is this correct? Does this mean the answer is ?","a\in(0,1) \underline{v}(x,y) = g(y/x)(-1/x,1/y)  S_a := \{(x,y)\in \Omega : 1\leqslant x^2+y^2 \leqslant 4, a\leqslant y/x\leqslant 1/a \}  \Omega:=\{(x,y): x>0, y>0\}  S_a x,y (a+1/a)g(1/a)ln(2) -(a+1/a)g(a)ln(2) 0 Flux= (a+1/a)ln(2)(g(1/a)-g(a))",['analysis']
62,Do these limits commute?,Do these limits commute?,,"Given a sequence of functions $f_{n,m}:\mathbb{R}^{n} \to \mathbb{R}$, suppose that $$\displaystyle\lim_{m} f_{n,m}(x)$$ exists almost everywhere (for any fixed n) and also suppose that $$\displaystyle\lim_{n} f_{n,m}$$ exists in norm $L^p$(for any fixed $m$). I would like to know if the iterative limit commute, that is, if $$\displaystyle\lim_{n}\lim_{m} f_{n,m}(x) = \displaystyle\lim_{m}\lim_{n} f_{n,m}(x)$$ almost everywhere (where the limit in the index $n$ is again understood as a $L^p$ limit and the limit in the index $m$ is understood as a limit almost everywhere). If not, which are the sufficient conditions for this to happen? Thanks in advance.","Given a sequence of functions $f_{n,m}:\mathbb{R}^{n} \to \mathbb{R}$, suppose that $$\displaystyle\lim_{m} f_{n,m}(x)$$ exists almost everywhere (for any fixed n) and also suppose that $$\displaystyle\lim_{n} f_{n,m}$$ exists in norm $L^p$(for any fixed $m$). I would like to know if the iterative limit commute, that is, if $$\displaystyle\lim_{n}\lim_{m} f_{n,m}(x) = \displaystyle\lim_{m}\lim_{n} f_{n,m}(x)$$ almost everywhere (where the limit in the index $n$ is again understood as a $L^p$ limit and the limit in the index $m$ is understood as a limit almost everywhere). If not, which are the sufficient conditions for this to happen? Thanks in advance.",,"['real-analysis', 'analysis', 'limits', 'normed-spaces', 'lp-spaces']"
63,"How prove $f(x)\le f(b)$. if $f(x)$ is continuous everywhere in [a,b], differentiable except at a countable number of points in [a,b]","How prove . if  is continuous everywhere in [a,b], differentiable except at a countable number of points in [a,b]",f(x)\le f(b) f(x),"QUestion: let $f(x)$  is continuous everywhere in [a,b], differentiable except at a countable number of points in [a,b].and  $f'(x)\ge 0$ show that   $$f(x)\le f(b)$$ This problem is from this: [china BBS] http://www.duodaa.com/?qa=4999 /一个证明题 My idea: if $f'(x)$ is integrable,Assmue that $f(x)$ is not derivative on $x_{i},x_{i}>x_{i-1},i=1,2,3,\cdots,n$,then we have $$0 \le \int_{x_{i-1}}^{x_{i}}f'(x)dx=f(x_{i})-f(x_{i-1}),i=1,2,\cdots, x_{0}=a,x_{n}=b$$ then have $$f(b)\ge f(x)$$ But I know this methods is not true,so How prove it? Thank you","QUestion: let $f(x)$  is continuous everywhere in [a,b], differentiable except at a countable number of points in [a,b].and  $f'(x)\ge 0$ show that   $$f(x)\le f(b)$$ This problem is from this: [china BBS] http://www.duodaa.com/?qa=4999 /一个证明题 My idea: if $f'(x)$ is integrable,Assmue that $f(x)$ is not derivative on $x_{i},x_{i}>x_{i-1},i=1,2,3,\cdots,n$,then we have $$0 \le \int_{x_{i-1}}^{x_{i}}f'(x)dx=f(x_{i})-f(x_{i-1}),i=1,2,\cdots, x_{0}=a,x_{n}=b$$ then have $$f(b)\ge f(x)$$ But I know this methods is not true,so How prove it? Thank you",,"['real-analysis', 'analysis', 'inequality']"
64,Why is it enough to prove the sentence?,Why is it enough to prove the sentence?,,I am looking at the proof of the theorem that for any rectangle the outer measure is equal to the volume. At the beginning of the proof there is the following sentence: It is enough to look at the case where the rectangle R is closed and bounded. Why does it stand?,I am looking at the proof of the theorem that for any rectangle the outer measure is equal to the volume. At the beginning of the proof there is the following sentence: It is enough to look at the case where the rectangle R is closed and bounded. Why does it stand?,,"['real-analysis', 'analysis', 'measure-theory', 'volume']"
65,I would like prove a result in integration,I would like prove a result in integration,,I would like prove this result $$\int_0^1 \frac{\left(\log (1+x)\right)^2}{x}\mathrm dx=\frac{\zeta(3)}{4}$$,I would like prove this result $$\int_0^1 \frac{\left(\log (1+x)\right)^2}{x}\mathrm dx=\frac{\zeta(3)}{4}$$,,"['integration', 'analysis']"
66,Finding strictly function that satisfies this limit,Finding strictly function that satisfies this limit,,"I am trying to find a strictly increasing function $\varphi: [a,b] \rightarrow \mathbb{R}$ such that $ \lim_{y \to x} \frac{2x - a - b}{\varphi(y) - \varphi(x)} = 0 $ and $ \lim_{y \to x} \frac{a + b -2y}{\varphi(y) - \varphi(x)} = 0 $ Background: Let $f,F:[a,b] \to \mathbb{R}$ then f is an MC-derivative (monotonically controlled derivative) of F if $\exists \varphi : [a,b] \to \mathbb{R}$ strictly increasing s.t $ \lim_{y \to x} \frac{F(y) - F(x) -f(x)(y-x)}{\varphi(y) - \varphi(x)} = 0 $ with $x,y \in [a,b]$. I encountered the problem above when attempting to show this holds for step functions, in particular: $f(x) = \left\{  \begin{array}{l l}     -1 & \quad \text{$x \in [a, \frac{a+b}{2})$}\\     1 & \quad \text{$x \in [\frac{a+b}{2},b]$}   \end{array} \right. $ $F(x) = \left\{  \begin{array}{l l}     a-x & \quad \text{$x \in [a, \frac{a+b}{2})$}\\     x-b & \quad \text{$x \in [\frac{a+b}{2},b]$}   \end{array} \right. $ Thanks.","I am trying to find a strictly increasing function $\varphi: [a,b] \rightarrow \mathbb{R}$ such that $ \lim_{y \to x} \frac{2x - a - b}{\varphi(y) - \varphi(x)} = 0 $ and $ \lim_{y \to x} \frac{a + b -2y}{\varphi(y) - \varphi(x)} = 0 $ Background: Let $f,F:[a,b] \to \mathbb{R}$ then f is an MC-derivative (monotonically controlled derivative) of F if $\exists \varphi : [a,b] \to \mathbb{R}$ strictly increasing s.t $ \lim_{y \to x} \frac{F(y) - F(x) -f(x)(y-x)}{\varphi(y) - \varphi(x)} = 0 $ with $x,y \in [a,b]$. I encountered the problem above when attempting to show this holds for step functions, in particular: $f(x) = \left\{  \begin{array}{l l}     -1 & \quad \text{$x \in [a, \frac{a+b}{2})$}\\     1 & \quad \text{$x \in [\frac{a+b}{2},b]$}   \end{array} \right. $ $F(x) = \left\{  \begin{array}{l l}     a-x & \quad \text{$x \in [a, \frac{a+b}{2})$}\\     x-b & \quad \text{$x \in [\frac{a+b}{2},b]$}   \end{array} \right. $ Thanks.",,"['real-analysis', 'analysis']"
67,Convergence of a series with alternating denominator - Real Analysis [duplicate],Convergence of a series with alternating denominator - Real Analysis [duplicate],,"This question already has answers here : How can I show whether the series $\sum\limits_{n=1}^\infty \frac{(-1)^n}{n(2+(-1)^n)} $ converges or diverges? (4 answers) Closed 3 years ago . Decide if the series converges absolutely, conditionally, or not at all. \begin{equation} \sum_{n=1}^{\infty}\frac{(-1)^n}{(2+(-1)^n)n} \end{equation} I'm having a lot of trouble with this one. I know the even terms will be of the form $\frac{1}{3n}$ and the odd terms of the form $-\frac{1}{n}$ but I'm not really sure how I can use that to prove convergence. Any hints would be greatly appreciated.","This question already has answers here : How can I show whether the series $\sum\limits_{n=1}^\infty \frac{(-1)^n}{n(2+(-1)^n)} $ converges or diverges? (4 answers) Closed 3 years ago . Decide if the series converges absolutely, conditionally, or not at all. \begin{equation} \sum_{n=1}^{\infty}\frac{(-1)^n}{(2+(-1)^n)n} \end{equation} I'm having a lot of trouble with this one. I know the even terms will be of the form $\frac{1}{3n}$ and the odd terms of the form $-\frac{1}{n}$ but I'm not really sure how I can use that to prove convergence. Any hints would be greatly appreciated.",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
68,Least sum of power of distances,Least sum of power of distances,,Let $n$ points in a $3$-dimensional space. Find the point $X$ that   minimizes the sum of distances $\|A_1X\|^q+ \|A_2X\|^q + ...  +\|A_nX\|^q $ (where $q \in \mathbb{Q^+}$). Are there any general intuitive methods to tackle this problem?,Let $n$ points in a $3$-dimensional space. Find the point $X$ that   minimizes the sum of distances $\|A_1X\|^q+ \|A_2X\|^q + ...  +\|A_nX\|^q $ (where $q \in \mathbb{Q^+}$). Are there any general intuitive methods to tackle this problem?,,"['calculus', 'analysis', 'geometry', 'multivariable-calculus', 'problem-solving']"
69,First order PDE with discontinuous coefficients,First order PDE with discontinuous coefficients,,"I want to consider the following equation $$u_t+\mathrm{sgn}(x)u_x=0,\,\,u(0,x)=u_0(x)$$ Now if $x>0$ or $x<0$ I can use the method of characteristics to obtain $u(t,x)=u_0(x-t)$ if $x>t$ and $u(t,x)=u_0(x+t)$ if $x<-t$. What about the region $-t<x<t$?","I want to consider the following equation $$u_t+\mathrm{sgn}(x)u_x=0,\,\,u(0,x)=u_0(x)$$ Now if $x>0$ or $x<0$ I can use the method of characteristics to obtain $u(t,x)=u_0(x-t)$ if $x>t$ and $u(t,x)=u_0(x+t)$ if $x<-t$. What about the region $-t<x<t$?",,"['real-analysis', 'analysis', 'partial-differential-equations']"
70,How to prove that $b^{x+y} = b^x b^y$ using this approach?,How to prove that  using this approach?,b^{x+y} = b^x b^y,"Fix $b>1$. If $m$, $n$, $p$, $q$ are integers, $n > 0$, $q > 0$, $r = m/n = p/q$, then I can prove that $(b^m)^{1/n} = (b^p)^{1/q}$. Hence it makes sense to define $b^r = (b^m)^{1/n}$. I can also show that $b^{r+s} = b^r b^s$ if $r$ and $s$ are rational. If $x$ is real, let us define $B(x)$ to be the set of all numbers $b^t$, where $t$ is rational and $t \leq x$. Then we can also show that $b^r = \sup B(r)$, where $r$ is rational. Hence it makes sense to define $b^x = \sup B(x)$ for every real $x$. Now here is my question: How to prove, using the above scheme, that $b^{x+y} = b^x b^y$ for all real $x$ and $y$? My effort: It can be shown that if $A$ and $B$ are two non-empty sets of positive real numbers, if $A$ and $B$ are both bounded above in $\mathbb{R}$, and if the set $C$ is defined as $$C \colon= \{ab \colon a \in A, b \in B \}, $$ then we have $$\sup C = \sup A \cdot \sup B.$$ So using this result, we obtain $$ b^x b^y = \sup B(x) \cdot \sup B(y) = \sup \{ b^r \colon r \in \mathbb{Q}, r \leq x \} \cdot \sup \{ b^s \colon s \in \mathbb{Q}, s \leq y \} = \sup \{ b^{r+s} \colon r \in \mathbb{Q}, s \in \mathbb{Q}, r \leq x, s \leq y \} \leq \sup \{ b^t \colon t\in \mathbb{Q}, t \leq x+y \} = \sup B(x+y) = b^{x+y}. $$ Now how to prove the reverse inequality?","Fix $b>1$. If $m$, $n$, $p$, $q$ are integers, $n > 0$, $q > 0$, $r = m/n = p/q$, then I can prove that $(b^m)^{1/n} = (b^p)^{1/q}$. Hence it makes sense to define $b^r = (b^m)^{1/n}$. I can also show that $b^{r+s} = b^r b^s$ if $r$ and $s$ are rational. If $x$ is real, let us define $B(x)$ to be the set of all numbers $b^t$, where $t$ is rational and $t \leq x$. Then we can also show that $b^r = \sup B(r)$, where $r$ is rational. Hence it makes sense to define $b^x = \sup B(x)$ for every real $x$. Now here is my question: How to prove, using the above scheme, that $b^{x+y} = b^x b^y$ for all real $x$ and $y$? My effort: It can be shown that if $A$ and $B$ are two non-empty sets of positive real numbers, if $A$ and $B$ are both bounded above in $\mathbb{R}$, and if the set $C$ is defined as $$C \colon= \{ab \colon a \in A, b \in B \}, $$ then we have $$\sup C = \sup A \cdot \sup B.$$ So using this result, we obtain $$ b^x b^y = \sup B(x) \cdot \sup B(y) = \sup \{ b^r \colon r \in \mathbb{Q}, r \leq x \} \cdot \sup \{ b^s \colon s \in \mathbb{Q}, s \leq y \} = \sup \{ b^{r+s} \colon r \in \mathbb{Q}, s \in \mathbb{Q}, r \leq x, s \leq y \} \leq \sup \{ b^t \colon t\in \mathbb{Q}, t \leq x+y \} = \sup B(x+y) = b^{x+y}. $$ Now how to prove the reverse inequality?",,"['calculus', 'real-analysis', 'analysis', 'exponentiation', 'foundations']"
71,Weird conformal map problem,Weird conformal map problem,,"Construct a conformal map from the region $\omega$ = open disk of radius 1 centered at 0 minus the closed disk of radius 0.5 centered at 0.5 to $\mathbb{D}$ = disk radius 1 centered at 0. I really have no clue where to begin.  I am alright at standard conformal map examples, but this one is hard.  Some help would be awesome.","Construct a conformal map from the region $\omega$ = open disk of radius 1 centered at 0 minus the closed disk of radius 0.5 centered at 0.5 to $\mathbb{D}$ = disk radius 1 centered at 0. I really have no clue where to begin.  I am alright at standard conformal map examples, but this one is hard.  Some help would be awesome.",,"['complex-analysis', 'analysis', 'complex-numbers', 'conformal-geometry']"
72,"Let $f(z) = z + z^2$ and let $V = \displaystyle \{z \in \mathbb{C} : |z| < \frac{1}{2}, \frac{3\pi}{4} < arg\{z\} < \frac{5\pi}{4}\}$.",Let  and let .,"f(z) = z + z^2 V = \displaystyle \{z \in \mathbb{C} : |z| < \frac{1}{2}, \frac{3\pi}{4} < arg\{z\} < \frac{5\pi}{4}\}","Let $f(z) = z + z^2$ and let $V = \displaystyle \{z \in \mathbb{C} : |z| < \frac{1}{2}, \frac{3\pi}{4} < arg\{z\} < \frac{5\pi}{4}\}$. $(a)$  Show that $f(V) \subset V.$ $(b)$ Let $f_n$ be the nth iterate of $f$.  Thus $f_1(z) = f(z)$ and $f_{n +1}(z) = f(f_n(z))$ for $n = 1, 2, ...$ For each point $z \in V$, show that $f_n(z) \rightarrow 0$ as $n \rightarrow \infty$. I tried just doing directly.  I really didn't make in progress (I am having trouble with even a).  It doesn't seem hard, but I think I am just not seeing it.  I got that all points in $f(V)$ belong to the right half circle but that is all.  Some help would be great.  Thanks.","Let $f(z) = z + z^2$ and let $V = \displaystyle \{z \in \mathbb{C} : |z| < \frac{1}{2}, \frac{3\pi}{4} < arg\{z\} < \frac{5\pi}{4}\}$. $(a)$  Show that $f(V) \subset V.$ $(b)$ Let $f_n$ be the nth iterate of $f$.  Thus $f_1(z) = f(z)$ and $f_{n +1}(z) = f(f_n(z))$ for $n = 1, 2, ...$ For each point $z \in V$, show that $f_n(z) \rightarrow 0$ as $n \rightarrow \infty$. I tried just doing directly.  I really didn't make in progress (I am having trouble with even a).  It doesn't seem hard, but I think I am just not seeing it.  I got that all points in $f(V)$ belong to the right half circle but that is all.  Some help would be great.  Thanks.",,"['complex-analysis', 'analysis', 'complex-numbers', 'conformal-geometry']"
73,Measure theory problem from Stein real analysis,Measure theory problem from Stein real analysis,,"Let $\mu$ be a Borel measure on the sphere $S^{d-1} = \{x \in \mathbb{R}^d:|x|=1\}$ which is rotation-invariant in the sense that $\mu(r(E)) = \mu(E),$ for every rotation $r$ of $\mathbb{R}^d$ and each Borel subset $E$ of $S^{d-1}.$ If $\mu(S^{d-1}) <  \infty,$ then $\mu$ is a constant multiple of measure $\sigma$ arising in the polar coordinate integration formula. There is a hint in this problem: Show that $\int_{S^{d-1}} Y_{k}(x) d \mu(x) = 0$ for every surface spherical harmonic of degree $k \ge 1.$ This would imply that there is a constant $c$ such that $$\int_{S^{d-1}}fd\mu = c\int_{S^{d-1}}fd\sigma$$ for every continuous functions $f$ on $S^{d-1}.$ I am thinking to show the hint we need to use stieltjes integral properties and Ergodic theorems but not having much luck so far.","Let $\mu$ be a Borel measure on the sphere $S^{d-1} = \{x \in \mathbb{R}^d:|x|=1\}$ which is rotation-invariant in the sense that $\mu(r(E)) = \mu(E),$ for every rotation $r$ of $\mathbb{R}^d$ and each Borel subset $E$ of $S^{d-1}.$ If $\mu(S^{d-1}) <  \infty,$ then $\mu$ is a constant multiple of measure $\sigma$ arising in the polar coordinate integration formula. There is a hint in this problem: Show that $\int_{S^{d-1}} Y_{k}(x) d \mu(x) = 0$ for every surface spherical harmonic of degree $k \ge 1.$ This would imply that there is a constant $c$ such that $$\int_{S^{d-1}}fd\mu = c\int_{S^{d-1}}fd\sigma$$ for every continuous functions $f$ on $S^{d-1}.$ I am thinking to show the hint we need to use stieltjes integral properties and Ergodic theorems but not having much luck so far.",,"['real-analysis', 'analysis', 'measure-theory']"
74,"Condition on Equality of closure of open ball and closed ball, suppose I have a counterexample","Condition on Equality of closure of open ball and closed ball, suppose I have a counterexample",,"Let $(X, d)$ be a metric space. Also for $x \in X$ and $r \ge 0$ define: $$  B(x,r) = \{ y \in X : d(x,y) < r \} \quad \mbox{ and } \quad  K(x,r) = \{ y \in X : d(x,y) \le r \}. $$ Denote by $\mbox{cl}(U)$ the closure of some set in the topology induced by $d : X \times X \to \mathbb R$. Then we have i) $\mbox{cl}(B(x,r)) \subseteq K(x,r)$ ii) if $X$ is normed, then we have $\mbox{cl}(B(x,r)) = K(x,r)$. This fact is from an authoritative source, guess must people know it? (if not, I can give proof and reference too). I think I have found a counterexample, so I am a little bit confused. Counterexample 1: Let $2^{\mathbb N}$ be the set of all one-sided infinite $0$-$1$-sequences. For $\xi = (\xi_i) \in 2^{\mathbb N}$ define $$  || \xi || := \sum_{i=1}^{\infty} \frac{1}{2^i} \xi_i. $$ Then $(2^{\mathbb N}, ||\cdot ||)$ is a normed space over $\mathbb F_2 = \{ 0, 1 \}$ (the finite field with two elements). Therefore it is also a metric space, with metric $$  d(\xi,\eta) = ||\xi - \eta|| = \sum_{i=1}^{\infty} (\xi_i - \eta_i) =\sum_{i=1}^{\infty} \frac{1}{2^i} d'(\xi_i, \eta_i) $$ where $d'(0,0) = d(1,1) = 0, d(0,1) = d(1,0) = 1$ (by the way, this is the discrete metric on $\{0,1\}$), but this inclusion is proper, so both sets are not equal, despite the fact that the space is normed? Let $\xi = 00000000\ldots$ we have \begin{align*}  B(\xi, 1/4) & = \{ \eta  : d(\xi, \eta) < 1/4 \} \\              & = \{ \eta = (\eta_i) : \eta_1 = 0, \eta_2 = 0 \} \setminus \{ 001111\ldots \} \end{align*} and \begin{align*}  K(\xi, 1/4) & = \{ \eta : d(\xi, \eta) \le 1/4 \} \\              & = \{ \eta = (\eta_i) : \eta_1 = \eta_2 = 0 \} \cup \{ 010000\ldots \} \end{align*} We have $$  \mbox{cl}(B(\xi,1/4)) = \{ \eta = (\eta_i) : \eta_1 = \eta_2 = 0 \} $$ and so $\mbox{cl}(B(\xi,1/4)) \subseteq K(\xi, 1/4)$, but $\mbox{cl}((B(\xi,1/4)) \ne K(\xi, 1/4)$. What went wrong here?","Let $(X, d)$ be a metric space. Also for $x \in X$ and $r \ge 0$ define: $$  B(x,r) = \{ y \in X : d(x,y) < r \} \quad \mbox{ and } \quad  K(x,r) = \{ y \in X : d(x,y) \le r \}. $$ Denote by $\mbox{cl}(U)$ the closure of some set in the topology induced by $d : X \times X \to \mathbb R$. Then we have i) $\mbox{cl}(B(x,r)) \subseteq K(x,r)$ ii) if $X$ is normed, then we have $\mbox{cl}(B(x,r)) = K(x,r)$. This fact is from an authoritative source, guess must people know it? (if not, I can give proof and reference too). I think I have found a counterexample, so I am a little bit confused. Counterexample 1: Let $2^{\mathbb N}$ be the set of all one-sided infinite $0$-$1$-sequences. For $\xi = (\xi_i) \in 2^{\mathbb N}$ define $$  || \xi || := \sum_{i=1}^{\infty} \frac{1}{2^i} \xi_i. $$ Then $(2^{\mathbb N}, ||\cdot ||)$ is a normed space over $\mathbb F_2 = \{ 0, 1 \}$ (the finite field with two elements). Therefore it is also a metric space, with metric $$  d(\xi,\eta) = ||\xi - \eta|| = \sum_{i=1}^{\infty} (\xi_i - \eta_i) =\sum_{i=1}^{\infty} \frac{1}{2^i} d'(\xi_i, \eta_i) $$ where $d'(0,0) = d(1,1) = 0, d(0,1) = d(1,0) = 1$ (by the way, this is the discrete metric on $\{0,1\}$), but this inclusion is proper, so both sets are not equal, despite the fact that the space is normed? Let $\xi = 00000000\ldots$ we have \begin{align*}  B(\xi, 1/4) & = \{ \eta  : d(\xi, \eta) < 1/4 \} \\              & = \{ \eta = (\eta_i) : \eta_1 = 0, \eta_2 = 0 \} \setminus \{ 001111\ldots \} \end{align*} and \begin{align*}  K(\xi, 1/4) & = \{ \eta : d(\xi, \eta) \le 1/4 \} \\              & = \{ \eta = (\eta_i) : \eta_1 = \eta_2 = 0 \} \cup \{ 010000\ldots \} \end{align*} We have $$  \mbox{cl}(B(\xi,1/4)) = \{ \eta = (\eta_i) : \eta_1 = \eta_2 = 0 \} $$ and so $\mbox{cl}(B(\xi,1/4)) \subseteq K(\xi, 1/4)$, but $\mbox{cl}((B(\xi,1/4)) \ne K(\xi, 1/4)$. What went wrong here?",,"['general-topology', 'analysis', 'metric-spaces', 'normed-spaces']"
75,"Can we expect, $h\ast \mu \in L^{2}(\mathbb R, (1+|x|^{2})^{s})$ for $h\in \mathcal{S}(\mathbb R), \mu\in M(\mathbb R)$ and $s>1/2$?","Can we expect,  for  and ?","h\ast \mu \in L^{2}(\mathbb R, (1+|x|^{2})^{s}) h\in \mathcal{S}(\mathbb R), \mu\in M(\mathbb R) s>1/2","We put, $M(\mathbb R)=$ The space of complex bounded Borel measure on $\mathbb R$ [With each complex Borel measure $\mu$ on $\mathbb R$ there is associated a set function $|\mu|,$ the total variation of $\mu,$ defined by, $$|\mu|(E)= \sup \sum |\mu(E_{i})|,$$ the supreme being taken over all finite collections of pairwise disjoint  Borel sets $E_{i}$ whose union is $E.$ Then $|\mu|$ is also a measure on $\mathbb R.$ If $\|\mu\|= |\mu|(\mathbb R)< \infty,$ we say $\mu$ is a bounded complex Borel measure on $\mathbb R.$] Fact. If $f\in L^{p}(\mathbb R), (1\leq p \leq \infty)$  and $\mu \in M(\mathbb R),$ then the integral $$f\ast \mu (x)= \int_{\mathbb R} f(x-y) d\mu(y)$$ exists for all most all $x,$ $f\ast \mu \in L^{p},$ and $\|f\ast \mu\|_{L^{p}}\leq \|f\|_{L^{p}} \|\mu\|.$ Let $h\in \mathcal{S}(\mathbb R)$( Schwartz space ) and $\mu \in M(\mathbb R).$ My Question is : Let $s>\frac{1}{2}.$ Can we show that $ \int_{\mathbb R} |(h\ast \mu)(x)|^{2}(1+|x|^{2})^{s} dx< \infty$ ?  (that is, can we expect, $h\ast \mu \in L^{2}(\mathbb R, (1+|x|^{2})^{s})$; in other words, can we expect, $h\ast \mu $ in $L^{2}$ with respect to weight $(1+|x|^{2})^{s}$ ?) My attempt : $ (\int_{\mathbb R} |(h\ast \mu)(x)|^{2}(1+|x|^{2})^{s} dx)^{1/2}= (\int_{\mathbb R} |\int_{\mathbb R} h(x-y) d\mu(y)|^{2}(1+|x|^{2})^{s} dx )^{1/2};$ now by Minkowski inequality for the integrals, gives us, $$\|h\ast\mu\|_{L^{2}(\mathbb R, (1+|x|^{2})^{s})}\leq \int_{\mathbb R} (\int_{\mathbb R} |h(x-y)|^{2} (1+|x|^{2})^{2s}dx)^{1/2} d\mu(y);$$ from here, I don't know how to proceed.. Proof of the above fact . If $f$ and $\mu$ are non-negative, then $f\ast \mu (x)$ exists(possibly being equal to $\infty$) for every $x,$ and by Minkowski's inequality for integrals, $$\|f\ast \mu\|_{L^{p}}\leq \int_{\mathbb R}\|f(\cdot-y)\|_{L^{p}}d\mu(y)=\|f\|_{p}\|\mu\|.$$ In particular, $f\ast \mu(x)<\infty$ for all most all $x.$ In the general case, this argument applies to $|f|$ and $|\mu|,$ and the result follows.","We put, $M(\mathbb R)=$ The space of complex bounded Borel measure on $\mathbb R$ [With each complex Borel measure $\mu$ on $\mathbb R$ there is associated a set function $|\mu|,$ the total variation of $\mu,$ defined by, $$|\mu|(E)= \sup \sum |\mu(E_{i})|,$$ the supreme being taken over all finite collections of pairwise disjoint  Borel sets $E_{i}$ whose union is $E.$ Then $|\mu|$ is also a measure on $\mathbb R.$ If $\|\mu\|= |\mu|(\mathbb R)< \infty,$ we say $\mu$ is a bounded complex Borel measure on $\mathbb R.$] Fact. If $f\in L^{p}(\mathbb R), (1\leq p \leq \infty)$  and $\mu \in M(\mathbb R),$ then the integral $$f\ast \mu (x)= \int_{\mathbb R} f(x-y) d\mu(y)$$ exists for all most all $x,$ $f\ast \mu \in L^{p},$ and $\|f\ast \mu\|_{L^{p}}\leq \|f\|_{L^{p}} \|\mu\|.$ Let $h\in \mathcal{S}(\mathbb R)$( Schwartz space ) and $\mu \in M(\mathbb R).$ My Question is : Let $s>\frac{1}{2}.$ Can we show that $ \int_{\mathbb R} |(h\ast \mu)(x)|^{2}(1+|x|^{2})^{s} dx< \infty$ ?  (that is, can we expect, $h\ast \mu \in L^{2}(\mathbb R, (1+|x|^{2})^{s})$; in other words, can we expect, $h\ast \mu $ in $L^{2}$ with respect to weight $(1+|x|^{2})^{s}$ ?) My attempt : $ (\int_{\mathbb R} |(h\ast \mu)(x)|^{2}(1+|x|^{2})^{s} dx)^{1/2}= (\int_{\mathbb R} |\int_{\mathbb R} h(x-y) d\mu(y)|^{2}(1+|x|^{2})^{s} dx )^{1/2};$ now by Minkowski inequality for the integrals, gives us, $$\|h\ast\mu\|_{L^{2}(\mathbb R, (1+|x|^{2})^{s})}\leq \int_{\mathbb R} (\int_{\mathbb R} |h(x-y)|^{2} (1+|x|^{2})^{2s}dx)^{1/2} d\mu(y);$$ from here, I don't know how to proceed.. Proof of the above fact . If $f$ and $\mu$ are non-negative, then $f\ast \mu (x)$ exists(possibly being equal to $\infty$) for every $x,$ and by Minkowski's inequality for integrals, $$\|f\ast \mu\|_{L^{p}}\leq \int_{\mathbb R}\|f(\cdot-y)\|_{L^{p}}d\mu(y)=\|f\|_{p}\|\mu\|.$$ In particular, $f\ast \mu(x)<\infty$ for all most all $x.$ In the general case, this argument applies to $|f|$ and $|\mu|,$ and the result follows.",,"['analysis', 'measure-theory', 'fourier-analysis', 'sobolev-spaces', 'lp-spaces']"
76,(Strictly) concave/convex function: Increasing / decreasing slope triangle?,(Strictly) concave/convex function: Increasing / decreasing slope triangle?,,"I have the following (possibly quick) question. In a paper I am working with, the following conclusions are drawn which I have a hard time to understand. Since they are given without proof, I assume that my mind has not woken up properly yet and I will greatly appreciate help! First conclusion: Let U be a strictly concave function on $\mathbb{R}^+$ and let $w,z,y>x\geq 0$. Then, the equality $U(y)-U(x) = U(w) - U(z)$ implies due to strict concavity of $U$ that $w-z > y-x$. Graphically, I agree with the conclusion but I'd like to have a technical proof. Second conclusion (converse): For a function $U$ and for all $w,z,y>x\geq 0$ such that $w-z > y-x$ it holds that $U(y)-U(x) = U(w) - U(z)$. Hence, $U$ is concave on $\mathbb{R}^+$. In the paper they just write concave. Do they mean strictly concave? Again, can anyone show me where the conclusion comes from? I can see that both conclusions arise from an if and only if statement about (strictly) concave functions, however for some reason I am not able to derive this statement. If someone knows of a result for convex functions that would obviously help as well. Thank you for any input, I am just stuck at the moment :(. Best,  Martin","I have the following (possibly quick) question. In a paper I am working with, the following conclusions are drawn which I have a hard time to understand. Since they are given without proof, I assume that my mind has not woken up properly yet and I will greatly appreciate help! First conclusion: Let U be a strictly concave function on $\mathbb{R}^+$ and let $w,z,y>x\geq 0$. Then, the equality $U(y)-U(x) = U(w) - U(z)$ implies due to strict concavity of $U$ that $w-z > y-x$. Graphically, I agree with the conclusion but I'd like to have a technical proof. Second conclusion (converse): For a function $U$ and for all $w,z,y>x\geq 0$ such that $w-z > y-x$ it holds that $U(y)-U(x) = U(w) - U(z)$. Hence, $U$ is concave on $\mathbb{R}^+$. In the paper they just write concave. Do they mean strictly concave? Again, can anyone show me where the conclusion comes from? I can see that both conclusions arise from an if and only if statement about (strictly) concave functions, however for some reason I am not able to derive this statement. If someone knows of a result for convex functions that would obviously help as well. Thank you for any input, I am just stuck at the moment :(. Best,  Martin",,"['analysis', 'convex-analysis']"
77,Deck transformations,Deck transformations,,"We have a theorem that says that if a group $G$ acts on a path-connected space $Y$ properly discontinuously, then $\pi: Y \rightarrow Y/G$ is a covering map. Especially, $G$ is isomorphic to the group of deck transformations. Now, I would like to understand this: 1.) Does the group of deck transformations to a given covering map always act properly and discontinuously on the covering space? 2.) If I have a covering map $p:X \rightarrow Y$ and I look at the group of deck transforms $G(X,p)$. Does it then follow that $Y$ is isomorphic to $X/G(x,p)$?","We have a theorem that says that if a group $G$ acts on a path-connected space $Y$ properly discontinuously, then $\pi: Y \rightarrow Y/G$ is a covering map. Especially, $G$ is isomorphic to the group of deck transformations. Now, I would like to understand this: 1.) Does the group of deck transformations to a given covering map always act properly and discontinuously on the covering space? 2.) If I have a covering map $p:X \rightarrow Y$ and I look at the group of deck transforms $G(X,p)$. Does it then follow that $Y$ is isomorphic to $X/G(x,p)$?",,"['calculus', 'real-analysis']"
78,when we can say that $\limsup_{n\to\infty}f(a_n)=f(\limsup_{n\to\infty}a_n)$?,when we can say that ?,\limsup_{n\to\infty}f(a_n)=f(\limsup_{n\to\infty}a_n),when we can say that $\limsup_{n\to\infty}f(a_n)=f(\limsup_{n\to\infty}a_n)$? (What conditions must have the function $f$?),when we can say that $\limsup_{n\to\infty}f(a_n)=f(\limsup_{n\to\infty}a_n)$? (What conditions must have the function $f$?),,"['analysis', 'limsup-and-liminf']"
79,"For $1 \leq r < p < \infty$ prove the continuous injection of $L^p([0, 1])$ into $L^r([0, 1])$.",For  prove the continuous injection of  into .,"1 \leq r < p < \infty L^p([0, 1]) L^r([0, 1])","For $1 \leq r < p < \infty$ prove the continuous injection of $L^p([0, 1])$ into $L^r([0, 1])$. I am having a hard time starting.  Any suggestions.  I tried a straight forward approach.  That is, given $\epsilon > 0$, I tried to find a $\delta >0$ such that $||f - g||_p < \delta$ implies that $||f - g||_r < \epsilon.$ Thanks for any help.","For $1 \leq r < p < \infty$ prove the continuous injection of $L^p([0, 1])$ into $L^r([0, 1])$. I am having a hard time starting.  Any suggestions.  I tried a straight forward approach.  That is, given $\epsilon > 0$, I tried to find a $\delta >0$ such that $||f - g||_p < \delta$ implies that $||f - g||_r < \epsilon.$ Thanks for any help.",,"['analysis', 'measure-theory', 'lebesgue-integral', 'lp-spaces', 'lebesgue-measure']"
80,$\|\phi_{\lambda}- \phi_{\lambda} \ast f \|_{L^{2}(\mathbb R)}\to 0$ as $\lambda \to \infty$? ($\phi_{\lambda}(x)=\lambda^{-1} \phi(x/\lambda).$),as ? (),\|\phi_{\lambda}- \phi_{\lambda} \ast f \|_{L^{2}(\mathbb R)}\to 0 \lambda \to \infty \phi_{\lambda}(x)=\lambda^{-1} \phi(x/\lambda).,"For $f\in L^{1}(\mathbb R),$ we define its Fourier transform as follows: $\hat{f}(t)=\int_{\mathbb R} f(x) e^{-ix\cdot t} dx ,(t\in \mathbb R).$ Suppose that $f\in L^{1}(\mathbb R)$ with $\hat{f}(0)=1.$ Let $\phi\in \mathcal{S}(\mathbb R),$ (Schwartz space). For $\lambda >0,$ we define, $\phi_{\lambda}(x)=\lambda^{-1} \phi(x/\lambda).$ Then $\|\phi_{\lambda}- \phi_{\lambda} \ast f \|_{L^{1}(\mathbb R)}\to 0$ as $\lambda \to \infty.$ [Since $\hat{f}(0)=1,$ for $x\in \mathbb R,$ we have, $\phi_{\lambda}(x)-\phi_{\lambda}\ast f(x) = \int_{\mathbb R} f(y) (\phi_{\lambda}(x)-\phi_{\lambda}(x-y)) dy;$ so $\|\phi_{\lambda}-\phi_{\lambda}\ast f\|_{L^{1}(\mathbb R)} \leq \int_{\mathbb R} |f(y)|(\int_{\mathbb R}\lambda^{-1}|(\phi(x/\lambda)- \phi ((x-y)/\lambda)| dx) dy= \int_{\mathbb R}|f(y)|( \int_{\mathbb R} (\phi(z)-\phi(z-\lambda^{-1}z)| dz)dy$ ;and the inner integral is at most $2\|\phi\|_{L^{1}},$ and it tends to zero for every $y\in \mathbb R,$ as $\lambda \to \infty.$ Hence by dominated convergence theorem, we get, $\|\phi_{\lambda}- \phi_{\lambda} \ast f \|_{L^{1}(\mathbb R)}\to 0$ as $\lambda \to \infty.$] My Question is : (1) Suppose $f\in L^{2}(\mathbb R)$ with $\hat{f}(0)=1.$ Then can we expect,    $\|\phi_{\lambda}- \phi_{\lambda} \ast f \|_{L^{2}(\mathbb R)}\to 0$ as $\lambda \to \infty  ?$   (2) If answer is yes, can we expect similar result for other $L^{p}$ ? Edit : It is well-known that, for $f\in L^{p}, (1\leq p <\infty),$ we have $\|f-\phi_{\lambda}\ast f\|_{L^{p}} \to 0$ as $\lambda \to 0.$ (Bit roughly speaking, approximate identity )","For $f\in L^{1}(\mathbb R),$ we define its Fourier transform as follows: $\hat{f}(t)=\int_{\mathbb R} f(x) e^{-ix\cdot t} dx ,(t\in \mathbb R).$ Suppose that $f\in L^{1}(\mathbb R)$ with $\hat{f}(0)=1.$ Let $\phi\in \mathcal{S}(\mathbb R),$ (Schwartz space). For $\lambda >0,$ we define, $\phi_{\lambda}(x)=\lambda^{-1} \phi(x/\lambda).$ Then $\|\phi_{\lambda}- \phi_{\lambda} \ast f \|_{L^{1}(\mathbb R)}\to 0$ as $\lambda \to \infty.$ [Since $\hat{f}(0)=1,$ for $x\in \mathbb R,$ we have, $\phi_{\lambda}(x)-\phi_{\lambda}\ast f(x) = \int_{\mathbb R} f(y) (\phi_{\lambda}(x)-\phi_{\lambda}(x-y)) dy;$ so $\|\phi_{\lambda}-\phi_{\lambda}\ast f\|_{L^{1}(\mathbb R)} \leq \int_{\mathbb R} |f(y)|(\int_{\mathbb R}\lambda^{-1}|(\phi(x/\lambda)- \phi ((x-y)/\lambda)| dx) dy= \int_{\mathbb R}|f(y)|( \int_{\mathbb R} (\phi(z)-\phi(z-\lambda^{-1}z)| dz)dy$ ;and the inner integral is at most $2\|\phi\|_{L^{1}},$ and it tends to zero for every $y\in \mathbb R,$ as $\lambda \to \infty.$ Hence by dominated convergence theorem, we get, $\|\phi_{\lambda}- \phi_{\lambda} \ast f \|_{L^{1}(\mathbb R)}\to 0$ as $\lambda \to \infty.$] My Question is : (1) Suppose $f\in L^{2}(\mathbb R)$ with $\hat{f}(0)=1.$ Then can we expect,    $\|\phi_{\lambda}- \phi_{\lambda} \ast f \|_{L^{2}(\mathbb R)}\to 0$ as $\lambda \to \infty  ?$   (2) If answer is yes, can we expect similar result for other $L^{p}$ ? Edit : It is well-known that, for $f\in L^{p}, (1\leq p <\infty),$ we have $\|f-\phi_{\lambda}\ast f\|_{L^{p}} \to 0$ as $\lambda \to 0.$ (Bit roughly speaking, approximate identity )",,"['real-analysis', 'analysis', 'fourier-analysis', 'lp-spaces']"
81,Image of Cantor set under Cantor-Lebesgue function,Image of Cantor set under Cantor-Lebesgue function,,"Let $m^{\ast}$ be the Lebesgue outer measure and $m$ the Lebesgue measure. Let $\phi$ be the Cantor Lebesgue function and let $\psi(x) := x + \phi(x)$. Let $C$ be the standard Cantor set, why does $m^{\ast}(\psi(C^{c})) = 1$? I know that: If $I = (a, b)$ is an open interval removed in the construction of the Cantor set, $\psi(I) = I + \phi(a)$ and hence $m^{\ast}(\psi(I)) = m^{\ast}(I) = m(I)$. Since $m(C^{c}) = 1$, $$1 = m\left(\bigcup_{I \in C^{c}}I\right) = \sum_{I \in C^{c}}m(I) = \sum_{I \in C^{c}}m^{\ast}(\psi(I))$$ but the expression on the right hand side is $\geq m^{\ast}(\psi(C^{c}))$ by countable subadditivity of $m^{\ast}$.","Let $m^{\ast}$ be the Lebesgue outer measure and $m$ the Lebesgue measure. Let $\phi$ be the Cantor Lebesgue function and let $\psi(x) := x + \phi(x)$. Let $C$ be the standard Cantor set, why does $m^{\ast}(\psi(C^{c})) = 1$? I know that: If $I = (a, b)$ is an open interval removed in the construction of the Cantor set, $\psi(I) = I + \phi(a)$ and hence $m^{\ast}(\psi(I)) = m^{\ast}(I) = m(I)$. Since $m(C^{c}) = 1$, $$1 = m\left(\bigcup_{I \in C^{c}}I\right) = \sum_{I \in C^{c}}m(I) = \sum_{I \in C^{c}}m^{\ast}(\psi(I))$$ but the expression on the right hand side is $\geq m^{\ast}(\psi(C^{c}))$ by countable subadditivity of $m^{\ast}$.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
82,How to calculate $\int_{\partial B_2(0)}\frac{2z^2+7z+11}{z^3+4z^2-z-4}\;dz$?,How to calculate ?,\int_{\partial B_2(0)}\frac{2z^2+7z+11}{z^3+4z^2-z-4}\;dz,"I want to calculate $$\displaystyle\int_{\partial B_2(0)}\underbrace{\frac{2z^2+7z+11}{z^3+4z^2-z-4}}_{=:f(z)}\;dz\tag{0}$$ Partial fraction decomposition yields $$f(z)=\underbrace{\frac{1}{z+4}}_{=:f_1(z)}-\underbrace{\frac{1}{z+1}}_{=:f_2(z)}+\underbrace{\frac{2}{z-1}}_{=:f_3(z)}\tag{1}$$ From this representation of $f$, it's easy to see that $-4$ and $\pm 1$ are poles of $f$. That means, that we can't take benefit from Cauchy's integral theorem , since $f$ is unbounded in a neighborhood of one of these poles. However, since $f$ is holomorphic on $\mathbb{C}\setminus\left\{-4,\pm 1\right\}$ we can apply the residue theorem which states here $$\int_{\partial B_2(0)}f(z)\;dz=2\pi i\sum_{z_0\in\left\{-4,\pm 1\right\}}\text{res}(f,z_0)\;\text{ind}_{\partial B_2(0)}\text{ }z_0$$ The winding number of $-4$ is obvious equal to $0$ while that ones of $\pm 1$ are equal to $1$. So, what would be smart to do now? Either we consider $f$ as a whole or as the sum of $f_1$, $f_2$ and $f_3$: In the first case, we would need to calculate the integrals $$\int_{\partial B_{\delta_\pm}(\pm 1)}f(z)\;dz$$ with $B_{\delta_\pm}(\pm 1)\subset B_2(0)$ In the second case, we would need to determine the Laurent series expansion of $f_1$, $f_2$ and $f_3$ at $\pm 1$. We can take advantage of the fact, that $f_1$, $f_2$ and $f_3$ in $(1)$ are in their Laurent series form at $-4$, $-1$ and $1$, respectively. What would be the easier way? Is there some rule of thumb in general? It seems like in this case, both options are too complicated and it would be easier to calculate $(0)$ from the definition without the residue theorem. Or is there something what prevents me from doing this? Notes: $B_r(z_0):=\left\{z\in\mathbb{C}:|z-z_0|<r\right\}$ $A_{r,R}(z_0):=\left\{z\in\mathbb{C}:r<|z-z_0|<R\right\}$ $\text{ind}_{\gamma}\text{ }z_0$ is the winding number of $z_0$ wrt $\gamma$","I want to calculate $$\displaystyle\int_{\partial B_2(0)}\underbrace{\frac{2z^2+7z+11}{z^3+4z^2-z-4}}_{=:f(z)}\;dz\tag{0}$$ Partial fraction decomposition yields $$f(z)=\underbrace{\frac{1}{z+4}}_{=:f_1(z)}-\underbrace{\frac{1}{z+1}}_{=:f_2(z)}+\underbrace{\frac{2}{z-1}}_{=:f_3(z)}\tag{1}$$ From this representation of $f$, it's easy to see that $-4$ and $\pm 1$ are poles of $f$. That means, that we can't take benefit from Cauchy's integral theorem , since $f$ is unbounded in a neighborhood of one of these poles. However, since $f$ is holomorphic on $\mathbb{C}\setminus\left\{-4,\pm 1\right\}$ we can apply the residue theorem which states here $$\int_{\partial B_2(0)}f(z)\;dz=2\pi i\sum_{z_0\in\left\{-4,\pm 1\right\}}\text{res}(f,z_0)\;\text{ind}_{\partial B_2(0)}\text{ }z_0$$ The winding number of $-4$ is obvious equal to $0$ while that ones of $\pm 1$ are equal to $1$. So, what would be smart to do now? Either we consider $f$ as a whole or as the sum of $f_1$, $f_2$ and $f_3$: In the first case, we would need to calculate the integrals $$\int_{\partial B_{\delta_\pm}(\pm 1)}f(z)\;dz$$ with $B_{\delta_\pm}(\pm 1)\subset B_2(0)$ In the second case, we would need to determine the Laurent series expansion of $f_1$, $f_2$ and $f_3$ at $\pm 1$. We can take advantage of the fact, that $f_1$, $f_2$ and $f_3$ in $(1)$ are in their Laurent series form at $-4$, $-1$ and $1$, respectively. What would be the easier way? Is there some rule of thumb in general? It seems like in this case, both options are too complicated and it would be easier to calculate $(0)$ from the definition without the residue theorem. Or is there something what prevents me from doing this? Notes: $B_r(z_0):=\left\{z\in\mathbb{C}:|z-z_0|<r\right\}$ $A_{r,R}(z_0):=\left\{z\in\mathbb{C}:r<|z-z_0|<R\right\}$ $\text{ind}_{\gamma}\text{ }z_0$ is the winding number of $z_0$ wrt $\gamma$",,"['integration', 'complex-analysis', 'analysis', 'complex-integration']"
83,"Existence of increasing, smooth modulus of continuity","Existence of increasing, smooth modulus of continuity",,"First, recall the definition: Given a function $f:M\to N$, where $M$ and $N$ are metric spaces, a modulus of continuity for $f$ is a function $\omega:[0,\infty)\to[0,\infty)$ such that $\omega(0)=\lim_{t\to 0^+}\omega(t)=0$; For every $x,x'\in M$,  $d(f(x),f(x'))\leq\omega(d(x,x'))$. According to Wikipedia , the existence of a modulus of continuity $\omega$ for a function $f$ implies the existence of a increasing and smooth (in $(0,\infty)$) modulus of continuity for $f$. I don't see how we can obtain this. The way suggested is the following: Given a modulus of identity $\omega$, let $\omega_1(t)=\sup_{s\leq t}\omega(s)$. Then $\omega_1$ is also a modulus of identity and is increasing (hence measurable, and locally integrable). Then, let $\omega_2(t)=\frac{1}{t}\int_t^{2t}\omega_1(s)ds$. $\omega_2$ is also a modulus of identity and it is continuous. I haven't checked, but I believe that with Lebesgue's Differentiation Theorem or something of the kind, we can show that $\omega_2$ is non-decreasing (alternatively, simply use $\omega_3(t)=\sup_{s\leq t}\omega_2(t)$). Then, it is stated that a suitable adaptation in the definition of $\omega_2$ will give us a smooth function. By this ""suitable adaptation"" I believe we should use some kind of molification: Notice that the definition of $\omega_2$ is pretty much molification by the pulse function $p(x)=\begin{cases}1&\text{, if }|x|\leq 1/2\\0&\text{, otherwise}\end{cases}$. This is what I was thinking: Let $\rho:\mathbb{R}\to\mathbb{R}$ be smooth, non-negative, $\operatorname{supp}\rho\subseteq[-1,1]$, and $\int_{-1}^1\rho(x)dx=1$. Making an abuse of notation, let's define, for $t>0$, $\rho(x,t)=\frac{2}{t}\rho(\frac{2x}{t}-3)$, so that $\operatorname{supp}\rho(\cdot,t)\subseteq[t,2t]$ and $\int_t^{2t}\rho(x,t)dx=1$. Finally, we define $\widetilde{w}(t)=\int_t^{2t}\rho(x,t)w(x)dx$. Using the usual arguments about molifications/Dominated Convergence, it is easy to show that $\widetilde{w}$ is smooth in $(0,\infty)$, and Dominated Convergence also implies that $\lim_{t\to 0}\widetilde{w}(t)=0$. The problem is showing that $\widetilde{w}$ is increasing. Alternative approachs are appreciated.","First, recall the definition: Given a function $f:M\to N$, where $M$ and $N$ are metric spaces, a modulus of continuity for $f$ is a function $\omega:[0,\infty)\to[0,\infty)$ such that $\omega(0)=\lim_{t\to 0^+}\omega(t)=0$; For every $x,x'\in M$,  $d(f(x),f(x'))\leq\omega(d(x,x'))$. According to Wikipedia , the existence of a modulus of continuity $\omega$ for a function $f$ implies the existence of a increasing and smooth (in $(0,\infty)$) modulus of continuity for $f$. I don't see how we can obtain this. The way suggested is the following: Given a modulus of identity $\omega$, let $\omega_1(t)=\sup_{s\leq t}\omega(s)$. Then $\omega_1$ is also a modulus of identity and is increasing (hence measurable, and locally integrable). Then, let $\omega_2(t)=\frac{1}{t}\int_t^{2t}\omega_1(s)ds$. $\omega_2$ is also a modulus of identity and it is continuous. I haven't checked, but I believe that with Lebesgue's Differentiation Theorem or something of the kind, we can show that $\omega_2$ is non-decreasing (alternatively, simply use $\omega_3(t)=\sup_{s\leq t}\omega_2(t)$). Then, it is stated that a suitable adaptation in the definition of $\omega_2$ will give us a smooth function. By this ""suitable adaptation"" I believe we should use some kind of molification: Notice that the definition of $\omega_2$ is pretty much molification by the pulse function $p(x)=\begin{cases}1&\text{, if }|x|\leq 1/2\\0&\text{, otherwise}\end{cases}$. This is what I was thinking: Let $\rho:\mathbb{R}\to\mathbb{R}$ be smooth, non-negative, $\operatorname{supp}\rho\subseteq[-1,1]$, and $\int_{-1}^1\rho(x)dx=1$. Making an abuse of notation, let's define, for $t>0$, $\rho(x,t)=\frac{2}{t}\rho(\frac{2x}{t}-3)$, so that $\operatorname{supp}\rho(\cdot,t)\subseteq[t,2t]$ and $\int_t^{2t}\rho(x,t)dx=1$. Finally, we define $\widetilde{w}(t)=\int_t^{2t}\rho(x,t)w(x)dx$. Using the usual arguments about molifications/Dominated Convergence, it is easy to show that $\widetilde{w}$ is smooth in $(0,\infty)$, and Dominated Convergence also implies that $\lim_{t\to 0}\widetilde{w}(t)=0$. The problem is showing that $\widetilde{w}$ is increasing. Alternative approachs are appreciated.",,['real-analysis']
84,"Show that there is no function $\phi\in C^2(\mathbb R^3,\mathbb R)$ such that $\nabla \phi=(-y,x,0)^t$",Show that there is no function  such that,"\phi\in C^2(\mathbb R^3,\mathbb R) \nabla \phi=(-y,x,0)^t","I want to show that there exists no function $\phi\in C^2(\mathbb R^3,\mathbb R)$ such that $\nabla \phi=(-y,x,0)^t$. I did it this way: I know $\nabla \phi=(\phi_x,\phi_y,\phi_z)^t$. Integrating each component: $\int -y dx=-yx+c_1$ and $\int x dy=xy+c_2$ and $\int 0dz=c_3$ with constants $c_i$. So we get $-yx+c_1=xy+c_2 \Leftrightarrow 2xy=c_1-c_2$ which is a contradiction. Now my tutor said this is wrong because I didn't used the fact $\phi\in C^2(\mathbb R^3,\mathbb R)$ but he couldn't explain me why my calculation is wrong. So is he right or can you do it like this? What's the problem about integrating each component? I will be happy about any hints or comments. Edit: I am not interested in a solution. It's easy to see the rotations of the left and right side aren't equal and it follows the statement above.","I want to show that there exists no function $\phi\in C^2(\mathbb R^3,\mathbb R)$ such that $\nabla \phi=(-y,x,0)^t$. I did it this way: I know $\nabla \phi=(\phi_x,\phi_y,\phi_z)^t$. Integrating each component: $\int -y dx=-yx+c_1$ and $\int x dy=xy+c_2$ and $\int 0dz=c_3$ with constants $c_i$. So we get $-yx+c_1=xy+c_2 \Leftrightarrow 2xy=c_1-c_2$ which is a contradiction. Now my tutor said this is wrong because I didn't used the fact $\phi\in C^2(\mathbb R^3,\mathbb R)$ but he couldn't explain me why my calculation is wrong. So is he right or can you do it like this? What's the problem about integrating each component? I will be happy about any hints or comments. Edit: I am not interested in a solution. It's easy to see the rotations of the left and right side aren't equal and it follows the statement above.",,['calculus']
85,Winding number and homotopy,Winding number and homotopy,,"Given two maps $f,g : S^1 \rightarrow S^1$, I want to show that if they have the same winding number, then there is a homotopy between them. Well, we know that we can write them as $f(\exp(2 \pi i t))= f(1) \exp(2 \pi i \phi_f(t))$ and $g(\exp(2 \pi i t))= g(1) \exp(2 \pi i \phi_g(t))$. Where $\phi_f(0) = \phi_g(0) = 0$ and $\phi_f(1) = \phi_g(1) = n$, where $n$ is the winding number. Now my first idea was that with $z(t):=\phi_f(t)-\phi_g(t)$ and $g(1)/f(1) = \exp(2 \pi i k)$ for some constant $k$, that $H(t,s) = f(\exp(2 \pi i t)) \cdot \exp(2 \pi i k s) \cdot \exp(2 \pi i s z(t))$ should do it, but then I was wondering: Where exactly did I use that they have the same winding number? So my proof must be wrong. Can andybody here correct it?","Given two maps $f,g : S^1 \rightarrow S^1$, I want to show that if they have the same winding number, then there is a homotopy between them. Well, we know that we can write them as $f(\exp(2 \pi i t))= f(1) \exp(2 \pi i \phi_f(t))$ and $g(\exp(2 \pi i t))= g(1) \exp(2 \pi i \phi_g(t))$. Where $\phi_f(0) = \phi_g(0) = 0$ and $\phi_f(1) = \phi_g(1) = n$, where $n$ is the winding number. Now my first idea was that with $z(t):=\phi_f(t)-\phi_g(t)$ and $g(1)/f(1) = \exp(2 \pi i k)$ for some constant $k$, that $H(t,s) = f(\exp(2 \pi i t)) \cdot \exp(2 \pi i k s) \cdot \exp(2 \pi i s z(t))$ should do it, but then I was wondering: Where exactly did I use that they have the same winding number? So my proof must be wrong. Can andybody here correct it?",,['real-analysis']
86,Interchanging index of summation in $d$ dimensions,Interchanging index of summation in  dimensions,d,"Let $\alpha = (\alpha_{1}, \ldots, \alpha_{d}) \in \mathbb{Z}_{\geq 0}^{d}$ and let $|\alpha| = \alpha_{1} + \cdots + \alpha_{d}$. I have the following question about interchanging summations: Is $$\sum_{j = 0}^{k}\sum_{\alpha: |\alpha| = j}\sum_{\substack{0 \leq \gamma_{i} \leq \alpha_{i}\\i = 1, 2, \ldots, d}} = \sum_{j = 0}^{k}\sum_{\gamma: |\gamma| = j}\sum_{\substack{\gamma_{i} \leq \alpha_{i} \leq k\\i = 1, 2, \ldots, d}}?$$ If not, how would I interchange the $\sum_{j = 0}^{k}\sum_{\alpha: |\alpha| = j}$ and $\sum_{\substack{0 \leq \gamma_{i} \leq \alpha_{i}\\i = 1, 2, \ldots, d}}$? In the case of one dimension, above centered expression boils down to $$\sum_{\alpha = 0}^{k}\sum_{\gamma = 0}^{\alpha} = \sum_{\gamma= 0}^{k}\sum_{\alpha = \gamma}^{k}$$ which can easily be seen by looking at $\sum_{\alpha = 0}^{k}\sum_{\gamma = 0}^{k}1_{\gamma \leq \alpha}.$","Let $\alpha = (\alpha_{1}, \ldots, \alpha_{d}) \in \mathbb{Z}_{\geq 0}^{d}$ and let $|\alpha| = \alpha_{1} + \cdots + \alpha_{d}$. I have the following question about interchanging summations: Is $$\sum_{j = 0}^{k}\sum_{\alpha: |\alpha| = j}\sum_{\substack{0 \leq \gamma_{i} \leq \alpha_{i}\\i = 1, 2, \ldots, d}} = \sum_{j = 0}^{k}\sum_{\gamma: |\gamma| = j}\sum_{\substack{\gamma_{i} \leq \alpha_{i} \leq k\\i = 1, 2, \ldots, d}}?$$ If not, how would I interchange the $\sum_{j = 0}^{k}\sum_{\alpha: |\alpha| = j}$ and $\sum_{\substack{0 \leq \gamma_{i} \leq \alpha_{i}\\i = 1, 2, \ldots, d}}$? In the case of one dimension, above centered expression boils down to $$\sum_{\alpha = 0}^{k}\sum_{\gamma = 0}^{\alpha} = \sum_{\gamma= 0}^{k}\sum_{\alpha = \gamma}^{k}$$ which can easily be seen by looking at $\sum_{\alpha = 0}^{k}\sum_{\gamma = 0}^{k}1_{\gamma \leq \alpha}.$",,"['real-analysis', 'analysis', 'summation']"
87,limit of the integrations of a sequence of integrable functions,limit of the integrations of a sequence of integrable functions,,"Let $(f_n)^\infty_{n=1}$ be a sequence of Lebesgue integrable functions on $[0,1]$ such that $f_n$ converges to $f$ almost everywhere in $[0,1]$. Suppose further (a). $\sup_n\int_0^1|f_n|d\mu<\infty$; (b). for any $\epsilon>0$, there exists $\delta>0$ such that for any measurable subset $E$ of $[0,1]$ with $\mu(E)<\delta$, $\sup_n\int_E|f_n|d\mu<\epsilon$. Prove that $f$ is integrable on $[0,1]$ and $\int_0^1fd\mu=\lim_{n\to\infty}\int_0^1f_nd\mu$. How to prove this? I confronted with some difficulties. My idea: Let  \begin{eqnarray*} f_n^N=\max \{\min\{f_n,N\},-N\},\\ f^N=\max \{\min\{f,N\},-N\}. \end{eqnarray*} Then for any $x\in[0,1]$, $|f_n(x)|\leq N$, $|f(x)|\leq N$. By Lebesgue Dominated Convergence Theorem, for each $N\in\mathbb{N}$,  \begin{eqnarray*} \lim_{n\to\infty}\int f_n^Nd\mu=\int\lim_{n\to\infty}f_n^Nd\mu =\int f^Nd\mu. \end{eqnarray*} Since $|f_n^N|\leq |f|$, by Lebesgue Dominated Convergence Theorem, for each $n\in\mathbb{N}$, \begin{eqnarray*} \lim_{N\to\infty}\int f_n^Nd\mu=\int\lim_{N\to\infty}f_n^Nd\mu=\int f_nd\mu. \end{eqnarray*} By Levi's monotone convergence theorem, \begin{eqnarray*} \int fd\mu&=&\int f^+d\mu-\int f^-d\mu\\ &=&\int\lim_{N\to\infty}f^{N+}d\mu-\int\lim_{N\to\infty}f^{N-}d\mu\\ &=&\lim_{N\to\infty}\int f^{N+}d\mu-\lim_{N\to\infty}\int f^{N-}d\mu\\ &=&\lim_{N\to\infty}\int f^Nd\mu. \end{eqnarray*} Letting $N\to\infty$ in the first equality, \begin{eqnarray*} \int fd\mu&=&\lim_{N\to\infty}\int f^N d\mu\\ &=&\lim_{N\to\infty}\lim_{n\to\infty}\int f^N_n d\mu\\ &=&\lim_{n\to\infty}\lim_{N\to\infty}\int f^N_n d\mu \\ &  &\text{ (need to prove the order of two limits can be exchanged)}\\ &=& \lim_{n\to\infty}\int f_nd\mu.  \end{eqnarray*} To prove that the order of the two limits can be reversed, we only need to prove: (1). $\lim_{n\to\infty,N\to\infty}f^N_nd\mu$ exists; (2). for any $n\in\mathbb{N}$, $\lim \int f^N_nd\mu$ exists; (3). for any $N\in\mathbb{N}$, $\lim_{n\to\infty}\int f^N_n d\mu$ exists. (2) is obtained by Lebesgue dominated convergence theorem. (3) is obtained by Egoroff theorem: for any $\delta>0$, there exists $E\subseteq [0,1]$, $\mu(E)<\delta$ such that $f_n^N\to^n f^N$ uniformly on $[0,1]\setminus E$. Hence for any $\epsilon >0$, there exists $n_0$ such that for any $n\geq n_0$,  $|f^N_n-f^N|<\epsilon$. Thus  \begin{eqnarray*} |\int_{[0,1]\setminus E} f^N_n d\mu-\int_{[0,1]\setminus E} f^N d\mu|<\epsilon. \end{eqnarray*} Let $N$ be fixed. Choose $\epsilon=2N\delta$. Then \begin{eqnarray*} |\int_{E} f^N_n d\mu-\int_{E} f^N d\mu|\leq \int_E |f^N_n-f^N|d\mu\leq 2N\mu(E)<\epsilon. \end{eqnarray*} Hence for all $n\geq n_0$,  \begin{eqnarray*} |\int f^N_n d\mu-\int f^N d\mu| &\leq&|\int_{E} f^N_n d\mu-\int_{E} f^N d\mu|+|\int_{[0,1]\setminus E} f^N_n d\mu-\int_{[0,1]\setminus E} f^N d\mu|\\ &<& 2\epsilon. \end{eqnarray*} Since $\delta>0$ is arbitrary, $\epsilon=2N\delta$ is also arbitrary. Hence we obtain (3). But I do not know how to prove (1). Until now, I have not used the given conditions (a), (b). How to prove the double limit exists by applying (a) and (b)? Without (a) or (b), the double limit in (1) may not exist?","Let $(f_n)^\infty_{n=1}$ be a sequence of Lebesgue integrable functions on $[0,1]$ such that $f_n$ converges to $f$ almost everywhere in $[0,1]$. Suppose further (a). $\sup_n\int_0^1|f_n|d\mu<\infty$; (b). for any $\epsilon>0$, there exists $\delta>0$ such that for any measurable subset $E$ of $[0,1]$ with $\mu(E)<\delta$, $\sup_n\int_E|f_n|d\mu<\epsilon$. Prove that $f$ is integrable on $[0,1]$ and $\int_0^1fd\mu=\lim_{n\to\infty}\int_0^1f_nd\mu$. How to prove this? I confronted with some difficulties. My idea: Let  \begin{eqnarray*} f_n^N=\max \{\min\{f_n,N\},-N\},\\ f^N=\max \{\min\{f,N\},-N\}. \end{eqnarray*} Then for any $x\in[0,1]$, $|f_n(x)|\leq N$, $|f(x)|\leq N$. By Lebesgue Dominated Convergence Theorem, for each $N\in\mathbb{N}$,  \begin{eqnarray*} \lim_{n\to\infty}\int f_n^Nd\mu=\int\lim_{n\to\infty}f_n^Nd\mu =\int f^Nd\mu. \end{eqnarray*} Since $|f_n^N|\leq |f|$, by Lebesgue Dominated Convergence Theorem, for each $n\in\mathbb{N}$, \begin{eqnarray*} \lim_{N\to\infty}\int f_n^Nd\mu=\int\lim_{N\to\infty}f_n^Nd\mu=\int f_nd\mu. \end{eqnarray*} By Levi's monotone convergence theorem, \begin{eqnarray*} \int fd\mu&=&\int f^+d\mu-\int f^-d\mu\\ &=&\int\lim_{N\to\infty}f^{N+}d\mu-\int\lim_{N\to\infty}f^{N-}d\mu\\ &=&\lim_{N\to\infty}\int f^{N+}d\mu-\lim_{N\to\infty}\int f^{N-}d\mu\\ &=&\lim_{N\to\infty}\int f^Nd\mu. \end{eqnarray*} Letting $N\to\infty$ in the first equality, \begin{eqnarray*} \int fd\mu&=&\lim_{N\to\infty}\int f^N d\mu\\ &=&\lim_{N\to\infty}\lim_{n\to\infty}\int f^N_n d\mu\\ &=&\lim_{n\to\infty}\lim_{N\to\infty}\int f^N_n d\mu \\ &  &\text{ (need to prove the order of two limits can be exchanged)}\\ &=& \lim_{n\to\infty}\int f_nd\mu.  \end{eqnarray*} To prove that the order of the two limits can be reversed, we only need to prove: (1). $\lim_{n\to\infty,N\to\infty}f^N_nd\mu$ exists; (2). for any $n\in\mathbb{N}$, $\lim \int f^N_nd\mu$ exists; (3). for any $N\in\mathbb{N}$, $\lim_{n\to\infty}\int f^N_n d\mu$ exists. (2) is obtained by Lebesgue dominated convergence theorem. (3) is obtained by Egoroff theorem: for any $\delta>0$, there exists $E\subseteq [0,1]$, $\mu(E)<\delta$ such that $f_n^N\to^n f^N$ uniformly on $[0,1]\setminus E$. Hence for any $\epsilon >0$, there exists $n_0$ such that for any $n\geq n_0$,  $|f^N_n-f^N|<\epsilon$. Thus  \begin{eqnarray*} |\int_{[0,1]\setminus E} f^N_n d\mu-\int_{[0,1]\setminus E} f^N d\mu|<\epsilon. \end{eqnarray*} Let $N$ be fixed. Choose $\epsilon=2N\delta$. Then \begin{eqnarray*} |\int_{E} f^N_n d\mu-\int_{E} f^N d\mu|\leq \int_E |f^N_n-f^N|d\mu\leq 2N\mu(E)<\epsilon. \end{eqnarray*} Hence for all $n\geq n_0$,  \begin{eqnarray*} |\int f^N_n d\mu-\int f^N d\mu| &\leq&|\int_{E} f^N_n d\mu-\int_{E} f^N d\mu|+|\int_{[0,1]\setminus E} f^N_n d\mu-\int_{[0,1]\setminus E} f^N d\mu|\\ &<& 2\epsilon. \end{eqnarray*} Since $\delta>0$ is arbitrary, $\epsilon=2N\delta$ is also arbitrary. Hence we obtain (3). But I do not know how to prove (1). Until now, I have not used the given conditions (a), (b). How to prove the double limit exists by applying (a) and (b)? Without (a) or (b), the double limit in (1) may not exist?",,"['real-analysis', 'integration', 'analysis', 'measure-theory', 'convergence-divergence']"
88,Prove that $\prod\limits_i(1+2\alpha_{i})\prod\limits_j(1-2\beta_{j})<\prod\limits_i(1+2x_{i})\prod\limits_j(1-2y_{j})$,Prove that,\prod\limits_i(1+2\alpha_{i})\prod\limits_j(1-2\beta_{j})<\prod\limits_i(1+2x_{i})\prod\limits_j(1-2y_{j}),"Let $m,n\in N^{+}$ and $i=1,2,\ldots,n,\;j=1,2,\ldots,m\,$ and $\,x_{i},\alpha_{i},y_{j},\beta_{j}$ be real numbers such that $$0\le x_{i}<\alpha_{i}<\dfrac{1}{2},\qquad0\le y_{j}<\beta_{j}<\dfrac{1}{2}$$ Assume that $$\prod_{i=1}^{n}(1+x_{i})\prod_{j=1}^{m}(1-y_{j})=\prod_{i=1}^{n}(1+\alpha_{i})\prod_{j=1}^{m}(1-\beta_{j})$$ Show that $$\prod_{i=1}^{n}(1+2\alpha_{i})\prod_{j=1}^{m}(1-2\beta_{j})<\prod_{i=1}^{n}(1+2x_{i})\prod_{j=1}^{m}(1-2y_{j})$$ this problem seems it's nice. my idea: the condition can $$\sum_{i=1}^{n}\ln{(1+x_{i})}+\sum_{j=1}^{m}\ln{(1-y_{j})}=\sum_{i=1}^{n}\ln{(1+\alpha_{i})}+\sum_{j=1}^{m}\ln{(1-\beta_{j})}$$ and prove $$\sum_{i=1}^{n}\ln{(1+2\alpha_{i})}+\sum_{j=1}^{m}\ln{(1-2\beta_{j})}<\sum_{i=1}^{n}\ln{(1+2x_{i})}+\sum_{j=1}^{m}\ln{(1-2y_{j})}$$ then I can't.Thank you",Let and and be real numbers such that Assume that Show that this problem seems it's nice. my idea: the condition can and prove then I can't.Thank you,"m,n\in N^{+} i=1,2,\ldots,n,\;j=1,2,\ldots,m\, \,x_{i},\alpha_{i},y_{j},\beta_{j} 0\le x_{i}<\alpha_{i}<\dfrac{1}{2},\qquad0\le y_{j}<\beta_{j}<\dfrac{1}{2} \prod_{i=1}^{n}(1+x_{i})\prod_{j=1}^{m}(1-y_{j})=\prod_{i=1}^{n}(1+\alpha_{i})\prod_{j=1}^{m}(1-\beta_{j}) \prod_{i=1}^{n}(1+2\alpha_{i})\prod_{j=1}^{m}(1-2\beta_{j})<\prod_{i=1}^{n}(1+2x_{i})\prod_{j=1}^{m}(1-2y_{j}) \sum_{i=1}^{n}\ln{(1+x_{i})}+\sum_{j=1}^{m}\ln{(1-y_{j})}=\sum_{i=1}^{n}\ln{(1+\alpha_{i})}+\sum_{j=1}^{m}\ln{(1-\beta_{j})} \sum_{i=1}^{n}\ln{(1+2\alpha_{i})}+\sum_{j=1}^{m}\ln{(1-2\beta_{j})}<\sum_{i=1}^{n}\ln{(1+2x_{i})}+\sum_{j=1}^{m}\ln{(1-2y_{j})}","['analysis', 'inequality']"
89,How prove the constant term of $\left(1+x+\frac{1}{x}\right)^p\equiv1\pmod {p^2}$,How prove the constant term of,\left(1+x+\frac{1}{x}\right)^p\equiv1\pmod {p^2},"if $p>3$ is odd prime number,show that: the constant term of $$\left(1+x+\dfrac{1}{x}\right)^p\equiv1\pmod {p^2}$$ My try: since $$(1+x+\dfrac{1}{x})^p=\sum_{k=0}^{p}\binom{p}{k}\left(x+\dfrac{1}{x}\right)^k=\sum_{k=0}^{p}\binom{p}{k}\sum_{j=0}^{k}\binom{k}{j}x^{k-2j}$$ so when $k=2j$,then the term is constant. But how prove this constant $\equiv 1\pmod {p^2}$?","if $p>3$ is odd prime number,show that: the constant term of $$\left(1+x+\dfrac{1}{x}\right)^p\equiv1\pmod {p^2}$$ My try: since $$(1+x+\dfrac{1}{x})^p=\sum_{k=0}^{p}\binom{p}{k}\left(x+\dfrac{1}{x}\right)^k=\sum_{k=0}^{p}\binom{p}{k}\sum_{j=0}^{k}\binom{k}{j}x^{k-2j}$$ so when $k=2j$,then the term is constant. But how prove this constant $\equiv 1\pmod {p^2}$?",,"['analysis', 'number-theory']"
90,Proof of uniform continuity on compact sets,Proof of uniform continuity on compact sets,,"Show that a function $f:\mathbb{R} \rightarrow \mathbb{R}$ that is continuous on a compact set $K$ is uniformly continuous on $K$. Is the proof below correct? Proof: Let $\epsilon > 0$ and let $x \in K$. Because $f$ is continuous on $K$ there exists a $\delta(x)$ such that whenever $|y - x| < \delta(x)$ it follows that $|f(y) - f(x)| < \epsilon / 3$. Now consider the collection of sets, $$\mathcal{C}_K = \{ V_{\delta(x)}(x) : x \in K \} $$ where $V_{\delta(x)}(x)$ is the open neighborhood $(x - \delta(x), x + \delta(x))$. The collection $\mathcal{C}_k$ form an open cover of $K$ and because we are given that $K$ is compact there exists a finite subcover of $K$: $$\mathcal{C}'_K = \{ V_{\delta(x_n)}(x_n) : n \in \{1, 2, \ldots, N\}\}$$ Before we can find a suitable choice of $\delta$ for the given $\epsilon$ it might be that $K$ is disconnected which can lead to a situation in which there exist $x, y \in K$ such that there exists no open interval contained entirely within $K$ which also contains $x$ and $y$. This will allow the possibility that $x$ and $y$ can be arbitrarily close to each other and yet also allow $|f(x) - f(y)|$ to be larger than our desired maximum value of $\epsilon$. To prevent this situation from occurring define, $$ D = \{ |x_m - x_n| - \delta(x_m) - \delta(x_n) : \forall m, n \in \{1, 2, \ldots, N \} \mathrm{\ such\ that\ } |x_m - x_n| > \delta(x_m) + \delta(x_n) \} $$ Now take, $$ \delta_{\epsilon} = \min \{ \{ \delta(x_n) : n \in \{1, \ldots, N \}\} \cup D \}$$ Now whenever $|x - y| < \delta_\epsilon$ it must be that $x \in V_{\delta(x_n)}$ and $y \in V_{\delta(x_m)}$ for some $m, n \in \{1, \ldots, N\}$ and there must exist a $l \in \{1, \ldots, N\}$ such that, $$ A = V_{\delta(x_n)}(x_n) \cap V_{\delta(x_l)}(x_l) \neq \emptyset$$ and, $$ B = V_{\delta(x_m)}(x_m) \cap V_{\delta(x_l)}(x_l) \neq \emptyset$$ Let $a$ be a point in $A$ and $b$ be a point in $B$. Then it follows from the way that we have constructed the open cover $\mathcal{C}'_K$ that, $$ \begin{align} |f(x) - f(y)| &= |f(x) - f(a) + f(a) - f(b) + f(b) - f(y)| \\               &\leq |f(x) - f(a)| + |f(a) - f(b)| + |f(b) - f(y)| \\               &< \epsilon / 3 + \epsilon / 3 + \epsilon / 3 \\               &< \epsilon \end{align} $$ Since $\delta_\epsilon$ is independent of $x$ and $y$ we can conclude that $f$ is uniformly continuous on $K$.","Show that a function $f:\mathbb{R} \rightarrow \mathbb{R}$ that is continuous on a compact set $K$ is uniformly continuous on $K$. Is the proof below correct? Proof: Let $\epsilon > 0$ and let $x \in K$. Because $f$ is continuous on $K$ there exists a $\delta(x)$ such that whenever $|y - x| < \delta(x)$ it follows that $|f(y) - f(x)| < \epsilon / 3$. Now consider the collection of sets, $$\mathcal{C}_K = \{ V_{\delta(x)}(x) : x \in K \} $$ where $V_{\delta(x)}(x)$ is the open neighborhood $(x - \delta(x), x + \delta(x))$. The collection $\mathcal{C}_k$ form an open cover of $K$ and because we are given that $K$ is compact there exists a finite subcover of $K$: $$\mathcal{C}'_K = \{ V_{\delta(x_n)}(x_n) : n \in \{1, 2, \ldots, N\}\}$$ Before we can find a suitable choice of $\delta$ for the given $\epsilon$ it might be that $K$ is disconnected which can lead to a situation in which there exist $x, y \in K$ such that there exists no open interval contained entirely within $K$ which also contains $x$ and $y$. This will allow the possibility that $x$ and $y$ can be arbitrarily close to each other and yet also allow $|f(x) - f(y)|$ to be larger than our desired maximum value of $\epsilon$. To prevent this situation from occurring define, $$ D = \{ |x_m - x_n| - \delta(x_m) - \delta(x_n) : \forall m, n \in \{1, 2, \ldots, N \} \mathrm{\ such\ that\ } |x_m - x_n| > \delta(x_m) + \delta(x_n) \} $$ Now take, $$ \delta_{\epsilon} = \min \{ \{ \delta(x_n) : n \in \{1, \ldots, N \}\} \cup D \}$$ Now whenever $|x - y| < \delta_\epsilon$ it must be that $x \in V_{\delta(x_n)}$ and $y \in V_{\delta(x_m)}$ for some $m, n \in \{1, \ldots, N\}$ and there must exist a $l \in \{1, \ldots, N\}$ such that, $$ A = V_{\delta(x_n)}(x_n) \cap V_{\delta(x_l)}(x_l) \neq \emptyset$$ and, $$ B = V_{\delta(x_m)}(x_m) \cap V_{\delta(x_l)}(x_l) \neq \emptyset$$ Let $a$ be a point in $A$ and $b$ be a point in $B$. Then it follows from the way that we have constructed the open cover $\mathcal{C}'_K$ that, $$ \begin{align} |f(x) - f(y)| &= |f(x) - f(a) + f(a) - f(b) + f(b) - f(y)| \\               &\leq |f(x) - f(a)| + |f(a) - f(b)| + |f(b) - f(y)| \\               &< \epsilon / 3 + \epsilon / 3 + \epsilon / 3 \\               &< \epsilon \end{align} $$ Since $\delta_\epsilon$ is independent of $x$ and $y$ we can conclude that $f$ is uniformly continuous on $K$.",,"['analysis', 'proof-writing', 'proof-verification', 'compactness', 'uniform-continuity']"
91,Finite Trigonometric Sum,Finite Trigonometric Sum,,"I have a dynamical system model whose equilibria depend on the solution of the following finite sum: \begin{align} \sum_{j\neq i}^n\frac{\sin(\theta_j-\theta_i)}{\left(1-\cos(\theta_j-\theta_i)\right)^{3/2}}=0 \end{align} I think I'm ok to suppose $\theta_i=0$ and run the sum from $j=1$ to $j=n-1$. Is this alright? Also, can I jettison the denominator altogether and focus on the sine terms? The solutions should be confined to the domain $(\theta_j-\theta_i)\in[0,2\pi]\:\forall\: j$. Im sure they are related to the roots of unity but haven't worked that out. Maybe there is some clever way to solve this equation once and for all? \begin{align} \sum_{j=1}^{n-1}\sin\left(\frac{2\pi j}{n}\right) = 0 \end{align} Does this exhaust the solutions? Assume that it does for a moment. This constraint on the angles in the system then leads finally to an additional trig sum that has some interesting properties. It appears that if the $l|k$ then the $k^{th}$ term contains the $l^{th}$ term with an additional factor. viz: \begin{align} \frac{1}{2\sqrt{2}}\sum_{j=1}^{n-1}\frac{1}{\sqrt{1-\cos(\frac{2\pi j}{n})}} = \frac{1}{4}\sum_{j=1}^{n-1}\csc\left(\frac{\pi j}{n}\right) \end{align} The first 12 terms: \begin{align} &0,\:\frac{1}{4}\:,\frac{1}{\sqrt{3}}\:,\frac{1}{4}+\frac{1}{\sqrt{2}}\:,\sqrt{1+\frac{2}{\sqrt{5}}}\:,\frac{5}{4}+\frac{1}{\sqrt{3}},\\[2mm] &\frac{1}{\sqrt{2 \left(1+\sin \left(\frac{\pi}{14}\right)\right)}}+\frac{1}{\sqrt{2-2 \sin \left(\frac{3 \pi }{14}\right)}}+\frac{1}{\sqrt{2\left(1+\cos \left(\frac{\pi}{7}\right)\right)}},\\[2mm] &\frac{1}{4}+\frac{1}{\sqrt{2}}+\sqrt{2+\sqrt{2}},\\[2mm] &\frac{1}{\sqrt{3}}+\frac{1}{\sqrt{2-2 \sin \left(\frac{\pi }{18}\right)}}+\frac{1}{\sqrt{2 \left(1+\cos \left(\frac{\pi}{9}\right)\right)}}+\frac{1}{2} \csc \left(\frac{\pi}{9}\right),\frac{1}{4}+\sqrt{5}+\sqrt{1+\frac{2}{\sqrt{5}}},\\[2mm] &\frac{1}{\sqrt{2 \left(1+\sin\left(\frac{\pi }{22}\right)\right)}}+\frac{1}{\sqrt{2-2 \sin \left(\frac{3 \pi}{22}\right)}}+\frac{1}{\sqrt{2 \left(1+\sin \left(\frac{5 \pi}{22}\right)\right)}}+\frac{1}{\sqrt{2 \left(1+\cos \left(\frac{\pi}{11}\right)\right)}}+\frac{1}{2} \csc \left(\frac{\pi}{11}\right),\\[2mm] &\frac{5}{4}+\sqrt{6}+\sqrt{\frac{5}{6}+\sqrt{\frac{2}{3}}} \end{align} You see for example 6 is divisible by 2 and 3 and the 6th term is equal to the 2nd term plus the 3rd term  plus 1. This sequence must be related the the symmetry point group $C_{nh}$ and its subgroups but how?","I have a dynamical system model whose equilibria depend on the solution of the following finite sum: \begin{align} \sum_{j\neq i}^n\frac{\sin(\theta_j-\theta_i)}{\left(1-\cos(\theta_j-\theta_i)\right)^{3/2}}=0 \end{align} I think I'm ok to suppose $\theta_i=0$ and run the sum from $j=1$ to $j=n-1$. Is this alright? Also, can I jettison the denominator altogether and focus on the sine terms? The solutions should be confined to the domain $(\theta_j-\theta_i)\in[0,2\pi]\:\forall\: j$. Im sure they are related to the roots of unity but haven't worked that out. Maybe there is some clever way to solve this equation once and for all? \begin{align} \sum_{j=1}^{n-1}\sin\left(\frac{2\pi j}{n}\right) = 0 \end{align} Does this exhaust the solutions? Assume that it does for a moment. This constraint on the angles in the system then leads finally to an additional trig sum that has some interesting properties. It appears that if the $l|k$ then the $k^{th}$ term contains the $l^{th}$ term with an additional factor. viz: \begin{align} \frac{1}{2\sqrt{2}}\sum_{j=1}^{n-1}\frac{1}{\sqrt{1-\cos(\frac{2\pi j}{n})}} = \frac{1}{4}\sum_{j=1}^{n-1}\csc\left(\frac{\pi j}{n}\right) \end{align} The first 12 terms: \begin{align} &0,\:\frac{1}{4}\:,\frac{1}{\sqrt{3}}\:,\frac{1}{4}+\frac{1}{\sqrt{2}}\:,\sqrt{1+\frac{2}{\sqrt{5}}}\:,\frac{5}{4}+\frac{1}{\sqrt{3}},\\[2mm] &\frac{1}{\sqrt{2 \left(1+\sin \left(\frac{\pi}{14}\right)\right)}}+\frac{1}{\sqrt{2-2 \sin \left(\frac{3 \pi }{14}\right)}}+\frac{1}{\sqrt{2\left(1+\cos \left(\frac{\pi}{7}\right)\right)}},\\[2mm] &\frac{1}{4}+\frac{1}{\sqrt{2}}+\sqrt{2+\sqrt{2}},\\[2mm] &\frac{1}{\sqrt{3}}+\frac{1}{\sqrt{2-2 \sin \left(\frac{\pi }{18}\right)}}+\frac{1}{\sqrt{2 \left(1+\cos \left(\frac{\pi}{9}\right)\right)}}+\frac{1}{2} \csc \left(\frac{\pi}{9}\right),\frac{1}{4}+\sqrt{5}+\sqrt{1+\frac{2}{\sqrt{5}}},\\[2mm] &\frac{1}{\sqrt{2 \left(1+\sin\left(\frac{\pi }{22}\right)\right)}}+\frac{1}{\sqrt{2-2 \sin \left(\frac{3 \pi}{22}\right)}}+\frac{1}{\sqrt{2 \left(1+\sin \left(\frac{5 \pi}{22}\right)\right)}}+\frac{1}{\sqrt{2 \left(1+\cos \left(\frac{\pi}{11}\right)\right)}}+\frac{1}{2} \csc \left(\frac{\pi}{11}\right),\\[2mm] &\frac{5}{4}+\sqrt{6}+\sqrt{\frac{5}{6}+\sqrt{\frac{2}{3}}} \end{align} You see for example 6 is divisible by 2 and 3 and the 6th term is equal to the 2nd term plus the 3rd term  plus 1. This sequence must be related the the symmetry point group $C_{nh}$ and its subgroups but how?",,"['analysis', 'finite-groups', 'summation', 'roots-of-unity', 'trigonometric-series']"
92,Problem involving the algebraic orders of complex functions,Problem involving the algebraic orders of complex functions,,"Problem: If $f(z)$ and $g(z)$ have the algebraic orders $h$ and $k$ at $z=a$, show that (a) $fg$ has the order $h+k$ (b) $f/g$ the order $h-k$ (c) $f+g$ an order which does not exceed $\max\ (h,k)$. Question: I'm not sure how to approach (c), and feel that my proof for (b) has a dubious step in it that I highlighted below. Attempt for (a): To say that $f(z)$ and $g(z)$ have algebraic orders $h$ and $k$ respectively at $z = a$ is to say that $$ \lim_{n \to \infty} |(z-a)|^{\alpha} |f(z)| = 0 \text{ for all } \alpha > r $$ and $$ \lim_{n \to \infty} |(z-a)|^{\beta} |g(z)| = 0 \text{ for all } \beta > k $$ where $h$ and $k$ are the minimal integers satisfying these properties. Then we have that \begin{align*} & \lim_{n \to \infty} |(z-a)|^{\alpha+\beta} |f(z) \cdot g(z)| = 0 \\ \iff & \lim_{n \to \infty} |(z-a)|^{\alpha} |(z-a)|^{\beta} |f(z) \cdot g(z)| = 0\\  \iff & \alpha + \beta > h + k \\ \end{align*} as desired. Attempt for (b): First, we observe that if $g(z) \ne 0$, then \begin{align*} & \lim_{n \to \infty}  |(z-a)|^{\beta} \left|g(z)\right| = 0 \\ \iff & \underbrace{\lim_{n \to \infty} |(z-a)|^{-\beta} \left|{1 \over g(z)}\right| = 0 }_{\text{this feels dubious to me}}\\  \iff & - \beta > - k \\ \end{align*} Then if we continue to assume that $g(z) \ne 0$, we have that \begin{align*} & \lim_{n \to \infty} |(z-a)|^{\alpha-\beta} \left|{f(z) \over g(z)}\right| = 0 \\ \iff & \lim_{n \to \infty} |(z-a)|^{\alpha} |(z-a)|^{-\beta} \left|{f(z) \over g(z)}\right| = 0\\  \iff & \alpha - \beta > h - k \\ \end{align*} as desired.","Problem: If $f(z)$ and $g(z)$ have the algebraic orders $h$ and $k$ at $z=a$, show that (a) $fg$ has the order $h+k$ (b) $f/g$ the order $h-k$ (c) $f+g$ an order which does not exceed $\max\ (h,k)$. Question: I'm not sure how to approach (c), and feel that my proof for (b) has a dubious step in it that I highlighted below. Attempt for (a): To say that $f(z)$ and $g(z)$ have algebraic orders $h$ and $k$ respectively at $z = a$ is to say that $$ \lim_{n \to \infty} |(z-a)|^{\alpha} |f(z)| = 0 \text{ for all } \alpha > r $$ and $$ \lim_{n \to \infty} |(z-a)|^{\beta} |g(z)| = 0 \text{ for all } \beta > k $$ where $h$ and $k$ are the minimal integers satisfying these properties. Then we have that \begin{align*} & \lim_{n \to \infty} |(z-a)|^{\alpha+\beta} |f(z) \cdot g(z)| = 0 \\ \iff & \lim_{n \to \infty} |(z-a)|^{\alpha} |(z-a)|^{\beta} |f(z) \cdot g(z)| = 0\\  \iff & \alpha + \beta > h + k \\ \end{align*} as desired. Attempt for (b): First, we observe that if $g(z) \ne 0$, then \begin{align*} & \lim_{n \to \infty}  |(z-a)|^{\beta} \left|g(z)\right| = 0 \\ \iff & \underbrace{\lim_{n \to \infty} |(z-a)|^{-\beta} \left|{1 \over g(z)}\right| = 0 }_{\text{this feels dubious to me}}\\  \iff & - \beta > - k \\ \end{align*} Then if we continue to assume that $g(z) \ne 0$, we have that \begin{align*} & \lim_{n \to \infty} |(z-a)|^{\alpha-\beta} \left|{f(z) \over g(z)}\right| = 0 \\ \iff & \lim_{n \to \infty} |(z-a)|^{\alpha} |(z-a)|^{-\beta} \left|{f(z) \over g(z)}\right| = 0\\  \iff & \alpha - \beta > h - k \\ \end{align*} as desired.",,"['complex-analysis', 'analysis']"
93,Limit of an integral of gradient,Limit of an integral of gradient,,"Let $U$ be an open connected set with smooth boundary in $\mathbb{R}^{d}$ and $U_{n} = \{x \in U: d(x, \partial U) > 1/n\}$. Let $f_{n} \in C_{c}^{\infty}(\mathbb{R}^{d})$ such that $f_{n}(x) = 1$ on $U_{n}$ and $0$ outside $U$ (we can find such an $f_{n}$ by Urysohn's Lemma). Is there a way to rigorously justify that $$\int_{\mathbb{R}^{d}}|\nabla f_{n}|\, dx \rightarrow m(\partial U)$$ as $n \rightarrow \infty$? Here $m$ denotes $d - 1$ dimensional Lebesgue measure.","Let $U$ be an open connected set with smooth boundary in $\mathbb{R}^{d}$ and $U_{n} = \{x \in U: d(x, \partial U) > 1/n\}$. Let $f_{n} \in C_{c}^{\infty}(\mathbb{R}^{d})$ such that $f_{n}(x) = 1$ on $U_{n}$ and $0$ outside $U$ (we can find such an $f_{n}$ by Urysohn's Lemma). Is there a way to rigorously justify that $$\int_{\mathbb{R}^{d}}|\nabla f_{n}|\, dx \rightarrow m(\partial U)$$ as $n \rightarrow \infty$? Here $m$ denotes $d - 1$ dimensional Lebesgue measure.",,"['real-analysis', 'analysis']"
94,An inequality for some series,An inequality for some series,,"Consider real positive numbers $t_1,t_2,\ldots, t_n$ for some $n\in\Bbb N$ , with $\sum_{i=1}^nt_i^2=n$ , such that if $0<t_i<1$ then $$\frac{t_i}{\sin\left(\dfrac{t_i\pi}{1+t_i}\right)}<1$$ and if $t_i>1$ then $$\frac{t_i}{\sin\left(\dfrac{t_i\pi}{1+t_i}\right)}<t_i^2.$$ Now can we say that the following inequality hold? $$\sum_{i=1}^n\frac{t_i}{\sin\left(\dfrac{t_i\pi}{1+t_i}\right)}\le n$$","Consider real positive numbers for some , with , such that if then and if then Now can we say that the following inequality hold?","t_1,t_2,\ldots, t_n n\in\Bbb N \sum_{i=1}^nt_i^2=n 0<t_i<1 \frac{t_i}{\sin\left(\dfrac{t_i\pi}{1+t_i}\right)}<1 t_i>1 \frac{t_i}{\sin\left(\dfrac{t_i\pi}{1+t_i}\right)}<t_i^2. \sum_{i=1}^n\frac{t_i}{\sin\left(\dfrac{t_i\pi}{1+t_i}\right)}\le n","['real-analysis', 'sequences-and-series', 'analysis', 'inequality']"
95,Problem with infinite product measures,Problem with infinite product measures,,"Given some measurable space $\left(X,\mathcal{F}\right)$ and two probability measures $\mu$ and $\nu$ on this space one can define $$H_{\theta}(\mu,\nu)=\int\left(\frac{d\mu}{d\lambda}\right)^{\theta}\left(\frac{d\nu}{d\lambda}\right)^{1-\theta}d\lambda  $$  where $\theta\in[0,1]  $ and $\lambda$ is any positive measure such that $\mu,\nu<<\lambda$ (it is in fact independent of the choice of this measure, and we can in fact assume it is also a probability measure). Now let $\mathbb{X}$ be a polish space, $\mathcal{F}=\mathbb{B}(\mathbb{X})  $, and consider the countable product of measurable spaces $\left(\prod_{n}\mathbb{X},\otimes_{n}\mathcal{F}\right)  $ , and the following measures $ \mu=\prod_{n}\mu_{n},\:\nu=\prod_{n}\nu_{n} $, on this space. Where $\mu_{n}$ and $\nu_{n}$ are probability measures on $\left(\mathbb{X},\mathcal{F}\right)  $. I wish to show that $H_{\theta}(\mu,\nu)=\prod_{n=1}^{\infty}H_{\theta}(\mu_{n},\nu_{n}).$ It is trivial for the case $\theta=0,1$, so assume $\theta\in(0,1)$. I can show that $H_{\theta}(\mu_{1}\times\mu_{2},\nu_{1}\times\nu_{2})=H_{\theta}(\mu_{1},\nu_{1})H_{\theta}(\mu_{2},\nu_{2})$ From which it follows that, $$H_{\theta}(\mu,\nu)=\prod_{n=1}^{N}H_{\theta}(\mu_{n},\nu_{n})H_{\theta}(\prod_{n=N+1}^{\infty}\mu_{n},\prod_{n=N+1}^{\infty}\nu_{n}).  $$ And since (by Hölder's inequality), $$H_{\theta}(\prod_{n=N+1}^{\infty}\mu_{n},\prod_{n=N+1}^{\infty}\nu_{n})\leq1,$$ it only remains to show that $$H_{\theta}(\prod_{n=N+1}^{\infty}\mu_{n},\prod_{n=N+1}^{\infty}\nu_{n})\geq1.  $$ Since then $$H_{\theta}(\mu,\nu)=\lim_{N}\prod_{n=1}^{N}H_{\theta}(\mu_{n},\nu_{n})\lim_{N}H_{\theta}(\prod_{n=N+1}^{\infty}\mu_{n},\prod_{n=N+1}^{\infty}\nu_{n})=\prod_{n=1}^{\infty}H_{\theta}(\mu_{n},\nu_{n}).  $$ I am kind of stuck here. Maybe this idea is going nowhere. I am not very familiar with infinite product measures and their integrals. Any pointers would be appreciated.","Given some measurable space $\left(X,\mathcal{F}\right)$ and two probability measures $\mu$ and $\nu$ on this space one can define $$H_{\theta}(\mu,\nu)=\int\left(\frac{d\mu}{d\lambda}\right)^{\theta}\left(\frac{d\nu}{d\lambda}\right)^{1-\theta}d\lambda  $$  where $\theta\in[0,1]  $ and $\lambda$ is any positive measure such that $\mu,\nu<<\lambda$ (it is in fact independent of the choice of this measure, and we can in fact assume it is also a probability measure). Now let $\mathbb{X}$ be a polish space, $\mathcal{F}=\mathbb{B}(\mathbb{X})  $, and consider the countable product of measurable spaces $\left(\prod_{n}\mathbb{X},\otimes_{n}\mathcal{F}\right)  $ , and the following measures $ \mu=\prod_{n}\mu_{n},\:\nu=\prod_{n}\nu_{n} $, on this space. Where $\mu_{n}$ and $\nu_{n}$ are probability measures on $\left(\mathbb{X},\mathcal{F}\right)  $. I wish to show that $H_{\theta}(\mu,\nu)=\prod_{n=1}^{\infty}H_{\theta}(\mu_{n},\nu_{n}).$ It is trivial for the case $\theta=0,1$, so assume $\theta\in(0,1)$. I can show that $H_{\theta}(\mu_{1}\times\mu_{2},\nu_{1}\times\nu_{2})=H_{\theta}(\mu_{1},\nu_{1})H_{\theta}(\mu_{2},\nu_{2})$ From which it follows that, $$H_{\theta}(\mu,\nu)=\prod_{n=1}^{N}H_{\theta}(\mu_{n},\nu_{n})H_{\theta}(\prod_{n=N+1}^{\infty}\mu_{n},\prod_{n=N+1}^{\infty}\nu_{n}).  $$ And since (by Hölder's inequality), $$H_{\theta}(\prod_{n=N+1}^{\infty}\mu_{n},\prod_{n=N+1}^{\infty}\nu_{n})\leq1,$$ it only remains to show that $$H_{\theta}(\prod_{n=N+1}^{\infty}\mu_{n},\prod_{n=N+1}^{\infty}\nu_{n})\geq1.  $$ Since then $$H_{\theta}(\mu,\nu)=\lim_{N}\prod_{n=1}^{N}H_{\theta}(\mu_{n},\nu_{n})\lim_{N}H_{\theta}(\prod_{n=N+1}^{\infty}\mu_{n},\prod_{n=N+1}^{\infty}\nu_{n})=\prod_{n=1}^{\infty}H_{\theta}(\mu_{n},\nu_{n}).  $$ I am kind of stuck here. Maybe this idea is going nowhere. I am not very familiar with infinite product measures and their integrals. Any pointers would be appreciated.",,"['probability', 'analysis', 'measure-theory', 'stochastic-processes']"
96,Estimate $\displaystyle\left|\int_{\frac{\pi}{k}}^{\frac{\pi}{2}} (\sin \theta)^{-1+\frac{i(n+1)y}{2}} d\theta \right|$,Estimate,\displaystyle\left|\int_{\frac{\pi}{k}}^{\frac{\pi}{2}} (\sin \theta)^{-1+\frac{i(n+1)y}{2}} d\theta \right|,"I have to estimate the following integral  $$\left|\int_{\frac{\pi}{k}}^{\frac{\pi}{2}} (\sin \theta)^{-1+\frac{i(n+1)y}{2}} d\theta \right|,\quad \forall k,n\geq 2 $$ According to Sogge (Oscillatory Integrals and Spherical Harmonics. Duke Mathematical Journal 53 (1986), no. 1, 43-65) the integral is $\leq C \max(1,|y|^2)$ where $C$ is independent of $y,k$, and this can be proved by a routine integration by parts argument but I cannot figure it out. Thanks in advance!","I have to estimate the following integral  $$\left|\int_{\frac{\pi}{k}}^{\frac{\pi}{2}} (\sin \theta)^{-1+\frac{i(n+1)y}{2}} d\theta \right|,\quad \forall k,n\geq 2 $$ According to Sogge (Oscillatory Integrals and Spherical Harmonics. Duke Mathematical Journal 53 (1986), no. 1, 43-65) the integral is $\leq C \max(1,|y|^2)$ where $C$ is independent of $y,k$, and this can be proved by a routine integration by parts argument but I cannot figure it out. Thanks in advance!",,"['calculus', 'analysis']"
97,Fundamental Theorem of Calculus and inverse..,Fundamental Theorem of Calculus and inverse..,,"If $F(x)$ is defined as $$F(x)= \int_{a}^{x} f(t) dt$$   calculate $(F^{-1})'(y)$ in terms of $f$. I have been working on this for a while now, does the aanswer to this incorporate the Inverse Function Theorem? $$(f^{-1})'(y_{0}))= \frac{1}{f'(x_{0})}$$ where $y_{0}=f(x_{0})$. But I seem to have trouble adapting this theorem to integrals,  would this be correct $$(F^{-1})'(y) = \frac{1}{F'(x)}$$ where $y=F(x)$?? and so by fundamental theorem of calculus $$\frac{1}{F'(x)} = \frac{1}{f(x)}$$ any help would be greatly appreciated..","If $F(x)$ is defined as $$F(x)= \int_{a}^{x} f(t) dt$$   calculate $(F^{-1})'(y)$ in terms of $f$. I have been working on this for a while now, does the aanswer to this incorporate the Inverse Function Theorem? $$(f^{-1})'(y_{0}))= \frac{1}{f'(x_{0})}$$ where $y_{0}=f(x_{0})$. But I seem to have trouble adapting this theorem to integrals,  would this be correct $$(F^{-1})'(y) = \frac{1}{F'(x)}$$ where $y=F(x)$?? and so by fundamental theorem of calculus $$\frac{1}{F'(x)} = \frac{1}{f(x)}$$ any help would be greatly appreciated..",,"['real-analysis', 'integration', 'analysis', 'definite-integrals']"
98,Is there a simple and fast way of computing the residue at an essential singularity?,Is there a simple and fast way of computing the residue at an essential singularity?,,"Is there a simple and fast way of computing the residue at an essential singularity ? I mean if we have a pole of order $n$ at $c$ we can use the formula : $$\mathrm{Res}(f,c) = \frac{1}{(n-1)!} \lim_{z \to c} \frac{d^{n-1}}{dz^{n-1}}\left( (z-c)^{n}f(z) \right) $$ Is there a similar formula for an essential singularity (without using Laurent series) ?","Is there a simple and fast way of computing the residue at an essential singularity ? I mean if we have a pole of order $n$ at $c$ we can use the formula : $$\mathrm{Res}(f,c) = \frac{1}{(n-1)!} \lim_{z \to c} \frac{d^{n-1}}{dz^{n-1}}\left( (z-c)^{n}f(z) \right) $$ Is there a similar formula for an essential singularity (without using Laurent series) ?",,"['complex-analysis', 'analysis', 'residue-calculus']"
99,The dual of the Annihilator,The dual of the Annihilator,,"Let $X$ be a Banach space, and $I$ be a closed subspace. Then it's known that $(X/I)^*=I^{\perp}$. My question is what is the second dual of $X/I$? or what is the dual of $I^{\perp}$ ? If we know that $(X/I)$ is reflexive, does this give any additional information about $I$,  and about $(I^{\perp})^*$ rather than being reflexive of course! Thank you in advance.","Let $X$ be a Banach space, and $I$ be a closed subspace. Then it's known that $(X/I)^*=I^{\perp}$. My question is what is the second dual of $X/I$? or what is the dual of $I^{\perp}$ ? If we know that $(X/I)$ is reflexive, does this give any additional information about $I$,  and about $(I^{\perp})^*$ rather than being reflexive of course! Thank you in advance.",,"['analysis', 'functional-analysis', 'banach-spaces']"
