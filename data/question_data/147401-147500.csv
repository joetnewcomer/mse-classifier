,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"What is $\int_0^{\infty}\!e^{-x^2}e^{-ae^{bx^2}}\,dx$?",What is ?,"\int_0^{\infty}\!e^{-x^2}e^{-ae^{bx^2}}\,dx","I've been trying without success to evaluate $$ {\Large\int_0^{\infty}\!e^{-x^2}e^{\Large \,-ae^{\,bx^2}}\,dx}. $$ It's not in my integral tables. Wolfram online integrator won't do it. It doesn't seem to be amenable to a contour integral method, and the method of integrating $e^{‚àíx^2}$ alone doesn't work either. I'm trying to solve a PDE that shows up in a financial mathematics context and have made a lot of progress. If I could do this integral, I would have a closed form. Any halep would be appreciated.","I've been trying without success to evaluate $$ {\Large\int_0^{\infty}\!e^{-x^2}e^{\Large \,-ae^{\,bx^2}}\,dx}. $$ It's not in my integral tables. Wolfram online integrator won't do it. It doesn't seem to be amenable to a contour integral method, and the method of integrating $e^{‚àíx^2}$ alone doesn't work either. I'm trying to solve a PDE that shows up in a financial mathematics context and have made a lot of progress. If I could do this integral, I would have a closed form. Any halep would be appreciated.",,"['analysis', 'definite-integrals']"
1,limit connected with a periodic function,limit connected with a periodic function,,"Let $f$ be a $1$-periodic function, i.e., $f(x+1)=f(x)$, defined on the interval $(0, 1)$ by the formula $$ f(x)=2x-1. $$ For a real number $x$ consider the series $$ \sum_{n=1}^\infty\frac{f(nx)}{n}. $$ It is easily seen that the series converges if $x$ is rational and if $f$ is assumed to vanish at all integers. My question is: does the convergence hold for irrational $x$? I would also be grateful for references to any results about kindred objects.","Let $f$ be a $1$-periodic function, i.e., $f(x+1)=f(x)$, defined on the interval $(0, 1)$ by the formula $$ f(x)=2x-1. $$ For a real number $x$ consider the series $$ \sum_{n=1}^\infty\frac{f(nx)}{n}. $$ It is easily seen that the series converges if $x$ is rational and if $f$ is assumed to vanish at all integers. My question is: does the convergence hold for irrational $x$? I would also be grateful for references to any results about kindred objects.",,"['number-theory', 'analysis', 'reference-request']"
2,Generalization for L'H√¥pital's rule and Stolz-Ces√†ro theorem?,Generalization for L'H√¥pital's rule and Stolz-Ces√†ro theorem?,,"Due to the similarity of L'H√¥pital's rule and Stolz-Ces√†ro theorem I guess that it must exists a generalized theorem where each of these two theorems are special cases. Note that in the setting of finite calculus the $\Delta $ operator is the inverse of the $\sum$ operator, that is, if $f:\Bbb N \to \Bbb R $ then $\Delta  f$ can be seen as the finite calculus derivative of $f$ and $\sum_0^n f$ as a finite calculus primitive of $f$ . Indeed in a measure-theoretic context one can see $f$ as the Radon-Nikodym derivative of $f \,\mathrm d \mu $ where $\mu $ is the counting measure in $\Bbb N $ (and $\Delta f$ can be seen as some Radon-Nikodym derivative of the measure $\Delta f \,\mathrm d \mu $ respect to the counting measure $\mu $ ). So suppose we have an operator $T$ , what condition we will need on $T$ (or the space where it is defined) to have a version of the Stolz-Ces√†ro theorem? I mean, when it holds that $$ \lim_{n\to \infty }\frac{Tf_n}{Tg_n}=h \implies \lim_{n\to \infty }\frac{f_n}{g_n}=h $$ assuming that $(g_n)$ is eventually not zero or that it diverges to infinity, and that the RHS of above is an indeterminate form of the kind $0/0$ or $\infty/\infty$ ? There is some theorem about that, or some paper or bibliography?","Due to the similarity of L'H√¥pital's rule and Stolz-Ces√†ro theorem I guess that it must exists a generalized theorem where each of these two theorems are special cases. Note that in the setting of finite calculus the operator is the inverse of the operator, that is, if then can be seen as the finite calculus derivative of and as a finite calculus primitive of . Indeed in a measure-theoretic context one can see as the Radon-Nikodym derivative of where is the counting measure in (and can be seen as some Radon-Nikodym derivative of the measure respect to the counting measure ). So suppose we have an operator , what condition we will need on (or the space where it is defined) to have a version of the Stolz-Ces√†ro theorem? I mean, when it holds that assuming that is eventually not zero or that it diverges to infinity, and that the RHS of above is an indeterminate form of the kind or ? There is some theorem about that, or some paper or bibliography?","\Delta  \sum f:\Bbb N \to \Bbb R  \Delta  f f \sum_0^n f f f f \,\mathrm d \mu  \mu  \Bbb N  \Delta f \Delta f \,\mathrm d \mu  \mu  T T 
\lim_{n\to \infty }\frac{Tf_n}{Tg_n}=h \implies \lim_{n\to \infty }\frac{f_n}{g_n}=h
 (g_n) 0/0 \infty/\infty","['analysis', 'reference-request', 'soft-question', 'operator-theory']"
3,Approximating intervals and squares by increasingly dense disjoint finite sets with special properties,Approximating intervals and squares by increasingly dense disjoint finite sets with special properties,,"Apologies for the length of the question. Consider interval $I=[0,1]$. For any $n \in \mathbb{N}$ we can always find two finite sets $S_{1n} \subset I$ and $S_{2n} \subset I$ such that: a) $S_{1n}\cap S_{2n} = \emptyset$ b) There is at most one point from $S_{1n} $ between any two consecutive points of  $S_{2n}$. Similarly, there is at most one point from $S_{2n} $ between any two consecutive points of  $S_{1n}$. c) $H(S_{1n}, I) \rightarrow 0$ and $H(S_{2n}, I) \rightarrow 0$ as $n \rightarrow \infty$, where $H$ stands for the Hausdorff distance. E.g., we can always choose:  $$S_{1n}=\left\{ \frac{k}{2^n}: \, k - \text{ odd }, \; 0 \leq k \leq 2^n\right\},$$ $$S_{2n}=\left\{ \frac{k}{2^n}: \, k - \text{ even }, \; 0 \leq k \leq 2^n\right\}.$$ Here is why property b) is interesting to me. For any given $n$, it holds that from any point $a \in (0,1)$ there is always a direction (right or left) to go where the first encountered point is going to be a point from  $S_{1n} $. Similarly, there is always a direction (right or left) to go where the first encountered point is going to be a point from  $S_{2n}$. A construction like this is not possible with three disjoint sets on $I$. $\star$ $\star$ $\star$ However, I wonder if something analogous is possible with three sets when we consider the square $I_2=[0,1]^2$. In a nutshell, I wonder if for $n \in \mathbb{N}$ we can always find three finite lattices $S_{1n} \subset I_2$, $S_{2n} \subset I_2$ and $S_{3n} \subset I_2$  such that: A*) Their projections $S^x_{1n}$, $S^x_{2n}$, $S^x_{3n}$ on the first coordinate are pairwise disjoint. Their projections $S^y_{1n}$, $S^y_{2n}$, $S^y_{3n}$ on the second coordinate are pairwise disjoint. B*) With any point $b=(b_1,b_2)\in \bigcup_{j=1}^3 S_{jn}$ associate a ‚Äúcross‚Äù $$\hat{b} = (b_1\times [0,1])\cup ([0,1]\times b_2).$$ Then for any $n$ construct the net from these ‚Äúcrosses‚Äù:  $$W_n= \bigcup_{b \in S_{1n}  \cup S_{2n}  \cup S_{3n}}  \hat{b} $$ Here is a description of the property analogous to b). For any given $n$ and for any $j=1,2,3$, it holds that from any point $a \in Int(I_2)$ there is  always a direction (either horizontal or vertical ‚Äì so we have four possible directions now) to go where the first encountered point on $ W_n $ is going to be from a ‚Äúcross‚Äù associated with a point from $S_{jn}$. C*) $H(S_{jn}, I_2) \rightarrow 0$as $n \rightarrow \infty$, $j=1,2,3$.","Apologies for the length of the question. Consider interval $I=[0,1]$. For any $n \in \mathbb{N}$ we can always find two finite sets $S_{1n} \subset I$ and $S_{2n} \subset I$ such that: a) $S_{1n}\cap S_{2n} = \emptyset$ b) There is at most one point from $S_{1n} $ between any two consecutive points of  $S_{2n}$. Similarly, there is at most one point from $S_{2n} $ between any two consecutive points of  $S_{1n}$. c) $H(S_{1n}, I) \rightarrow 0$ and $H(S_{2n}, I) \rightarrow 0$ as $n \rightarrow \infty$, where $H$ stands for the Hausdorff distance. E.g., we can always choose:  $$S_{1n}=\left\{ \frac{k}{2^n}: \, k - \text{ odd }, \; 0 \leq k \leq 2^n\right\},$$ $$S_{2n}=\left\{ \frac{k}{2^n}: \, k - \text{ even }, \; 0 \leq k \leq 2^n\right\}.$$ Here is why property b) is interesting to me. For any given $n$, it holds that from any point $a \in (0,1)$ there is always a direction (right or left) to go where the first encountered point is going to be a point from  $S_{1n} $. Similarly, there is always a direction (right or left) to go where the first encountered point is going to be a point from  $S_{2n}$. A construction like this is not possible with three disjoint sets on $I$. $\star$ $\star$ $\star$ However, I wonder if something analogous is possible with three sets when we consider the square $I_2=[0,1]^2$. In a nutshell, I wonder if for $n \in \mathbb{N}$ we can always find three finite lattices $S_{1n} \subset I_2$, $S_{2n} \subset I_2$ and $S_{3n} \subset I_2$  such that: A*) Their projections $S^x_{1n}$, $S^x_{2n}$, $S^x_{3n}$ on the first coordinate are pairwise disjoint. Their projections $S^y_{1n}$, $S^y_{2n}$, $S^y_{3n}$ on the second coordinate are pairwise disjoint. B*) With any point $b=(b_1,b_2)\in \bigcup_{j=1}^3 S_{jn}$ associate a ‚Äúcross‚Äù $$\hat{b} = (b_1\times [0,1])\cup ([0,1]\times b_2).$$ Then for any $n$ construct the net from these ‚Äúcrosses‚Äù:  $$W_n= \bigcup_{b \in S_{1n}  \cup S_{2n}  \cup S_{3n}}  \hat{b} $$ Here is a description of the property analogous to b). For any given $n$ and for any $j=1,2,3$, it holds that from any point $a \in Int(I_2)$ there is  always a direction (either horizontal or vertical ‚Äì so we have four possible directions now) to go where the first encountered point on $ W_n $ is going to be from a ‚Äúcross‚Äù associated with a point from $S_{jn}$. C*) $H(S_{jn}, I_2) \rightarrow 0$as $n \rightarrow \infty$, $j=1,2,3$.",,"['analysis', 'discrete-mathematics', 'approximation']"
4,Choice of $q$ in Baby Rudin's Example 1.1,Choice of  in Baby Rudin's Example 1.1,q,"First, my apologies if this has already been asked/answered.  I wasn't able to find this question via search. My question comes from Rudin's ""Principles of Mathematical Analysis,"" or ""Baby Rudin,"" Ch 1, Example 1.1 on p. 2.  In the second version of the proof, showing that sets A and B do not have greatest or lowest elements respectively, he presents a seemingly arbitrary assignment of a number $q$ that satisfies equations (3) and (4), plus other conditions needed to show that $q$ is the right number for the proof.  As an exercise, I tried to derive his choice of $q$ so that I may learn more about the problem. If we write equations (3) as $q = p - (p^2 - 2)x$ , we can write (4) as $$ q^2 - 2 = (p^2 - 2)[1 - 2px + (p^2 - 2)x^2]. $$ Here, we need a rational $x > 0$ , chosen such that the expression in $[...]$ is positive.  Using the quadratic formula and the sign of $(p^2 - 2)$ , it can be shown that we need $$ x \in \left(0, \frac{1}{p + \sqrt{2}}\right) \mbox{ for } p \in A, $$ or, for $p \in B$ , $x < 1/\left(p + \sqrt{2}\right)$ or $x > 1/\left(p - \sqrt{2}\right)$ . Notice that there are MANY solutions to these equations! The easiest to see, perhaps, is letting $x = 1/(p + n)$ for $n \geq 2$ .  Notice that Rudin chooses $n = 2$ for his answer, but it checks out easily for other $n$ . The Question: Why does Rudin choose $x = 1/(p + 2)$ specifically?  Is it just to make the expressions work out clearly algebraically?  Why doesn't he comment on his particular choice or the nature of the set of solutions that will work for the proof?  Is there a simpler derivation for the number $q$ that I am missing?","First, my apologies if this has already been asked/answered.  I wasn't able to find this question via search. My question comes from Rudin's ""Principles of Mathematical Analysis,"" or ""Baby Rudin,"" Ch 1, Example 1.1 on p. 2.  In the second version of the proof, showing that sets A and B do not have greatest or lowest elements respectively, he presents a seemingly arbitrary assignment of a number that satisfies equations (3) and (4), plus other conditions needed to show that is the right number for the proof.  As an exercise, I tried to derive his choice of so that I may learn more about the problem. If we write equations (3) as , we can write (4) as Here, we need a rational , chosen such that the expression in is positive.  Using the quadratic formula and the sign of , it can be shown that we need or, for , or . Notice that there are MANY solutions to these equations! The easiest to see, perhaps, is letting for .  Notice that Rudin chooses for his answer, but it checks out easily for other . The Question: Why does Rudin choose specifically?  Is it just to make the expressions work out clearly algebraically?  Why doesn't he comment on his particular choice or the nature of the set of solutions that will work for the proof?  Is there a simpler derivation for the number that I am missing?","q q q q = p - (p^2 - 2)x 
q^2 - 2 = (p^2 - 2)[1 - 2px + (p^2 - 2)x^2].
 x > 0 [...] (p^2 - 2) 
x \in \left(0, \frac{1}{p + \sqrt{2}}\right) \mbox{ for } p \in A,
 p \in B x < 1/\left(p + \sqrt{2}\right) x > 1/\left(p - \sqrt{2}\right) x = 1/(p + n) n \geq 2 n = 2 n x = 1/(p + 2) q","['real-analysis', 'analysis', 'self-learning']"
5,"Lebesgue point of density on $[0,1]$ and Dynkin's theorem",Lebesgue point of density on  and Dynkin's theorem,"[0,1]","The problem defines a density point $x\in[0,1]$ for a Borel set $A\subset [0,1]$ if $$ \lim_{\varepsilon \rightarrow 0^+} \frac{\mu([x-\varepsilon,x+\varepsilon]\cap A)}{2\varepsilon}=1.$$Denote all the density point of $A$ to be $A^*$. The problem asks  to show $$A^*\text{ is Borel}, \mu(A\,\Delta\, A^*)=0, \forall \text{Borel } A\subset[0,1]$$ where $\mu$ is the Lebesgue measure and $\Delta$ means symmetric difference. The point here is that the professor asks to use Dynkin's $\pi-\lambda$ theorem to prove and he note that interval $I$ would have $\mu(I\,\Delta\, I^*)=0$. I manage to show $A^*$ is Borel but do not know how to show the second part.  I let $\pi$ system be all the intervals in $[0,1]$   and the $\lambda$ system to be $S=\{A\text{ is Borel}:\mu(A\,\Delta\, A^*)=0\}$. I got trouble in checking that if $A\in S$ then $A^c\in S$ and if $A_1,\ldots,A_n,\ldots\in S$, $\bigcup A_n\in S$. Indeed, I am not sure that whether Dynkin's theorem could be applied here or make the problem easier. Or we still need to go through the proof using Vitali covering lemma.","The problem defines a density point $x\in[0,1]$ for a Borel set $A\subset [0,1]$ if $$ \lim_{\varepsilon \rightarrow 0^+} \frac{\mu([x-\varepsilon,x+\varepsilon]\cap A)}{2\varepsilon}=1.$$Denote all the density point of $A$ to be $A^*$. The problem asks  to show $$A^*\text{ is Borel}, \mu(A\,\Delta\, A^*)=0, \forall \text{Borel } A\subset[0,1]$$ where $\mu$ is the Lebesgue measure and $\Delta$ means symmetric difference. The point here is that the professor asks to use Dynkin's $\pi-\lambda$ theorem to prove and he note that interval $I$ would have $\mu(I\,\Delta\, I^*)=0$. I manage to show $A^*$ is Borel but do not know how to show the second part.  I let $\pi$ system be all the intervals in $[0,1]$   and the $\lambda$ system to be $S=\{A\text{ is Borel}:\mu(A\,\Delta\, A^*)=0\}$. I got trouble in checking that if $A\in S$ then $A^c\in S$ and if $A_1,\ldots,A_n,\ldots\in S$, $\bigcup A_n\in S$. Indeed, I am not sure that whether Dynkin's theorem could be applied here or make the problem easier. Or we still need to go through the proof using Vitali covering lemma.",,"['analysis', 'measure-theory', 'lebesgue-measure']"
6,What does it mean to not be able to take the derivative of a function multiple times? [duplicate],What does it mean to not be able to take the derivative of a function multiple times? [duplicate],,"This question already has answers here : Example of function that is differentiable, but the second derivative is not defined (3 answers) Closed 5 years ago . I'm taking a intro to mathematical analysis course and I'm having trouble understanding this definition. They are talking about how it can be interesting to see what happens if you take the derivative of a function multiple times (this is discussed in the intro to Taylor polynomials). They throw this definition at us, which they call $C^kfunctions$ : A function $f: I \rightarrow \mathbb{R} $ belongs to $C^k(I)$ if it is possible to take the derivative of the function k times on $I$ and if $f^{(k)}(x)$ is continuous on $I$ I'm a little confused by this. Aren't all functions $C^\infty$ then? Can't you just keep taking the derivative of a function even if it becomes $0$ ? Specifically I'm on a chapter now where they are discussing curve integrals and they keep mentioning that it is a $C^2$ function. What does this mean? Can you only take the derivative 2 times? I'm confused and can't wrap my mind around it. I would love if anyone was able to explain it in such a way that I could understand and apply it. Thanks in advance.","This question already has answers here : Example of function that is differentiable, but the second derivative is not defined (3 answers) Closed 5 years ago . I'm taking a intro to mathematical analysis course and I'm having trouble understanding this definition. They are talking about how it can be interesting to see what happens if you take the derivative of a function multiple times (this is discussed in the intro to Taylor polynomials). They throw this definition at us, which they call : A function belongs to if it is possible to take the derivative of the function k times on and if is continuous on I'm a little confused by this. Aren't all functions then? Can't you just keep taking the derivative of a function even if it becomes ? Specifically I'm on a chapter now where they are discussing curve integrals and they keep mentioning that it is a function. What does this mean? Can you only take the derivative 2 times? I'm confused and can't wrap my mind around it. I would love if anyone was able to explain it in such a way that I could understand and apply it. Thanks in advance.",C^kfunctions f: I \rightarrow \mathbb{R}  C^k(I) I f^{(k)}(x) I C^\infty 0 C^2,"['analysis', 'derivatives']"
7,Prove that $e^x\ge x+1$ for all real $x$ [duplicate],Prove that  for all real  [duplicate],e^x\ge x+1 x,"This question already has answers here : Simplest or nicest proof that $1+x \le e^x$ (26 answers) Closed 9 years ago . Without using differentiation, logarithmic function, rigorously, prove that $$e^x\ge x+1$$ for all real values of $x$.","This question already has answers here : Simplest or nicest proof that $1+x \le e^x$ (26 answers) Closed 9 years ago . Without using differentiation, logarithmic function, rigorously, prove that $$e^x\ge x+1$$ for all real values of $x$.",,"['analysis', 'inequality', 'exponential-function']"
8,How can I obtain this division's limit without using derivatives?,How can I obtain this division's limit without using derivatives?,,"$$\lim_{y\to 0} \frac{y}{\cos\left(\frac{\pi}{2}(1+y)\right)}$$ Can anybody help me? I can use basic properties of limits, and some of those basic known limits. I know it would be easier with derivatives, but I was just wondering if it's possible without  L-Hospital's rule, derivatives, Taylor series. Thank you in advance! My ideas for now: changing cosine into sine. Maybe that. I have no other clue.","$$\lim_{y\to 0} \frac{y}{\cos\left(\frac{\pi}{2}(1+y)\right)}$$ Can anybody help me? I can use basic properties of limits, and some of those basic known limits. I know it would be easier with derivatives, but I was just wondering if it's possible without  L-Hospital's rule, derivatives, Taylor series. Thank you in advance! My ideas for now: changing cosine into sine. Maybe that. I have no other clue.",,"['analysis', 'limits-without-lhopital']"
9,Taylor series for different points... how do they look?,Taylor series for different points... how do they look?,,I can't understand what it means to do the Taylor series at the point $a$. The best way would be showing me how it looks for different $a$ on a graph. Do I find those graphs on the Internet?,I can't understand what it means to do the Taylor series at the point $a$. The best way would be showing me how it looks for different $a$ on a graph. Do I find those graphs on the Internet?,,"['analysis', 'power-series', 'taylor-expansion']"
10,If $f(x + y) = f(x) + f(y)$ showing that $f(cx) = cf(x)$ holds for rational $c$,If  showing that  holds for rational,f(x + y) = f(x) + f(y) f(cx) = cf(x) c,"For $f:\mathbb{R}^n \to \mathbb{R}^m$, if $f(x + y) = f(x) + f(y)$ for then for rational $c$, how would you show that $f(cx) = cf(x)$ holds? I tried that for $c = \frac{a}{b}$, $a,b \in \mathbb{Z}$ clearly  $$ f\left(\frac{a}{b}x\right) = f\left(\frac{x}{b}+\dots+\frac{x}{b}\right) = af\left(\frac{x}{b}\right) $$ but I can't seem to finish it, any help?","For $f:\mathbb{R}^n \to \mathbb{R}^m$, if $f(x + y) = f(x) + f(y)$ for then for rational $c$, how would you show that $f(cx) = cf(x)$ holds? I tried that for $c = \frac{a}{b}$, $a,b \in \mathbb{Z}$ clearly  $$ f\left(\frac{a}{b}x\right) = f\left(\frac{x}{b}+\dots+\frac{x}{b}\right) = af\left(\frac{x}{b}\right) $$ but I can't seem to finish it, any help?",,['analysis']
11,A difficult inequality involving complex numbers,A difficult inequality involving complex numbers,,"Suppose that $z_1,\ldots,z_n$ are complex numbers with the property that there is some constant $C$ such that $$\big|z_1^r+\cdots+z_n^r\big|\leqslant C$$ for all integers $r\geqslant0$. Show that for all $i$ we have $\left|z_i\right|\leqslant1$. Pretty sure need to perfume some sort of analytic arguments, this 'for all r in non-negative integers' statement is very strong, otherwise this inequality will hardly hold.","Suppose that $z_1,\ldots,z_n$ are complex numbers with the property that there is some constant $C$ such that $$\big|z_1^r+\cdots+z_n^r\big|\leqslant C$$ for all integers $r\geqslant0$. Show that for all $i$ we have $\left|z_i\right|\leqslant1$. Pretty sure need to perfume some sort of analytic arguments, this 'for all r in non-negative integers' statement is very strong, otherwise this inequality will hardly hold.",,"['analysis', 'inequality', 'complex-numbers']"
12,Orthogonal polynomials and Gram Schmidt,Orthogonal polynomials and Gram Schmidt,,"How can we use the Gram Schmidt procedure to calculate $L_0,L_1, L_2, L_3$, where ${L_0(x), L_1(x), L_2(x), L_3(x)}$ is an orthogonal set of polynomials on $(0, \infty)$ w.r.t. the weight function $w(x) = e^{-x}$ and $L_0(x)=1$. Can someone illustrate how to calculate $L_2$ and $L_3$? So I get the idea? I get some integrals but they seem to lead me not far. Also, now with $L_0,L_1, L_2, L_3$, how can we compute the least squares polynomial of degree 1, 2, and 3 on the interval $(0,\infty)$ for the weight function $w(x) = e^{-x}$ for $f(x) = e^{-2x}$? Thanks.","How can we use the Gram Schmidt procedure to calculate $L_0,L_1, L_2, L_3$, where ${L_0(x), L_1(x), L_2(x), L_3(x)}$ is an orthogonal set of polynomials on $(0, \infty)$ w.r.t. the weight function $w(x) = e^{-x}$ and $L_0(x)=1$. Can someone illustrate how to calculate $L_2$ and $L_3$? So I get the idea? I get some integrals but they seem to lead me not far. Also, now with $L_0,L_1, L_2, L_3$, how can we compute the least squares polynomial of degree 1, 2, and 3 on the interval $(0,\infty)$ for the weight function $w(x) = e^{-x}$ for $f(x) = e^{-2x}$? Thanks.",,"['analysis', 'numerical-methods']"
13,Examples of perfect sets.,Examples of perfect sets.,,"Let $0\lt a\lt 1$. Can I get examples of of subsets of $[0,1]$ that are perfect sets, contains no intervals and has measure $1-a$. Well, I know by construction the Cantor set is perfect, contains no intervals. However, it has measure $0$. So it's no good.","Let $0\lt a\lt 1$. Can I get examples of of subsets of $[0,1]$ that are perfect sets, contains no intervals and has measure $1-a$. Well, I know by construction the Cantor set is perfect, contains no intervals. However, it has measure $0$. So it's no good.",,"['analysis', 'measure-theory', 'examples-counterexamples']"
14,Visualizing Balls in Ultrametric Spaces,Visualizing Balls in Ultrametric Spaces,,"I've been reading about ultrametric spaces, and I find that a lot of the results about balls in ultrametric spaces are very counter-intuitive. For example: if two balls share a common point, then one of the balls is contained in the other. The reason I find these results so counter-intuitive is that I can easily picture ""counter-examples,"" the problem being that these ""counter-examples"" are balls in Euclidean space. My issue is not that I cannot prove these results. My issue is that I don't know how to think about/picture balls in ultrametric spaces, which makes it more difficult for me to actually come up with the proofs. Hence, does anyone have any hints as to how to think about/picture balls in ultrametric spaces?","I've been reading about ultrametric spaces, and I find that a lot of the results about balls in ultrametric spaces are very counter-intuitive. For example: if two balls share a common point, then one of the balls is contained in the other. The reason I find these results so counter-intuitive is that I can easily picture ""counter-examples,"" the problem being that these ""counter-examples"" are balls in Euclidean space. My issue is not that I cannot prove these results. My issue is that I don't know how to think about/picture balls in ultrametric spaces, which makes it more difficult for me to actually come up with the proofs. Hence, does anyone have any hints as to how to think about/picture balls in ultrametric spaces?",,"['analysis', 'p-adic-number-theory']"
15,What are the explanations for certain steps in these proofs for the irrationality/rationality of certain numbers?,What are the explanations for certain steps in these proofs for the irrationality/rationality of certain numbers?,,"From Stephen Abbott's Understanding Analysis : Theorem: There is no rational number whose square is 2. Proof: Assume for contradiction, that there exist integers $p$ and $q$ satisfying (1) $\left(\frac{p}{q}\right)^2 = 2$. We may also assume that $p$ and $q$ have no common factor, because, if they had one, we could simply cancel it out and rewrite the fraction in lowest terms. Now, equation (1) implies (2) $p^2 = 2q^2$. From this we can see that the integer $p^2$ is an even number (it is divisible by 2), and hence $p$ must be even as well because the square of an odd number is odd. This allows us to write $p=2r$, where $r$ is also an integer. If we substitute $2r$ for $p$ in equation (2), then a little algebra yields the relationship $2r^2=q^2$. This last equation implies that $q^2$ is even, and hence $q$ must also be even. Thus, we have shown that $p$ and $q$ are both even (i.e., divisible by $2$) when they were originally assumed to have no common factor. From this logical impasse, we can only conclude that equation (1) cannot hold for any integers $p$ and $q$, and thus the theorem is proved. Exercise: (a) Prove that $\sqrt{3}$ is irrational. Does a similar argument work to show that $\sqrt{6}$ is irrational? (b) Where does the proof of the irrationality of $\sqrt{2}$ break down if we try to use it to prove $\sqrt{4}$ is irrational? Solution: (a) Assume, for contradiction, that there exist integers $p$ and $q$ satisfying (1) $\left(\frac{p}{q}\right)^2 = 3$. Let us also assume that $p$ and $q$ have no common factor. Now, equation (1) implies (2) $p^2 = 3q^2$. From this, we can see that $p^2$ is a multiple of $3$ and hence $p$ must also be a multiple of $3$. This allows us to write $p = 3r$, where $r$ is an integer. After substituting $3r$ for $p$ in equation (2), we get $(3r)^2 = 3q^2$, which can be simplified to $3r^2=q^2$. This implies $q^2$ is a multiple of $3$ and hence $q$ is also a multiple of $3$. Thus we have shown $p$ and $q$ have a common factor, namely $3$, when they were originally assumed to have no common factor.   A similar argument will work for $\sqrt{6}$ as well because we get $p^2 = 6q^2$ which implies $p$ is a multiple of $2$ and $3$. After making the necessary substitutions, we can conclude $q$ is a multiple of $6$, and therefore $\sqrt{6}$ must be irrational. (b) In this case, the fact that $p^2$ is a multiple of $4$ does not imply $p$ is also a multiple of $4$. Thus, our proof breaks down at this point. Question(s): Could somebody please help and spell out for me some of the theorems which are being used along the way in the arguments above? Since the theorem quoted above is the first theorem in the book, I am not exactly sure what is assumed to be common knowledge or whatever... For $\sqrt{3}$, what exactly is being used to justify ""$p^2$ is a multiple of $3$ and hence $p$ must also be a multiple of $3$""? I've seen some other proofs for this which have to do with even and odd numbers, but the one above has to do specifically with $3$, right? For $\sqrt{6}$, why is it important to say ""$p^2 = 6q^2$ implies $p$ is a multiple of $2$ and $3$"" instead of ""is a multiple of $6$""? For $\sqrt{4}$, why is it so clear that ""the fact that $p^2$ is a multiple of $4$ does not imply $p$ is also a multiple of $4$""? I know I can think of a counterexample like $p=2 \Rightarrow p^2=4$ and $4$ is divisible by $4$, but $2$ is not. But what is the pattern to notice? Any help is appreciated. Also, as if it weren't obvious, I think that answers which are not too advanced will be most helpful for me, thanks!","From Stephen Abbott's Understanding Analysis : Theorem: There is no rational number whose square is 2. Proof: Assume for contradiction, that there exist integers $p$ and $q$ satisfying (1) $\left(\frac{p}{q}\right)^2 = 2$. We may also assume that $p$ and $q$ have no common factor, because, if they had one, we could simply cancel it out and rewrite the fraction in lowest terms. Now, equation (1) implies (2) $p^2 = 2q^2$. From this we can see that the integer $p^2$ is an even number (it is divisible by 2), and hence $p$ must be even as well because the square of an odd number is odd. This allows us to write $p=2r$, where $r$ is also an integer. If we substitute $2r$ for $p$ in equation (2), then a little algebra yields the relationship $2r^2=q^2$. This last equation implies that $q^2$ is even, and hence $q$ must also be even. Thus, we have shown that $p$ and $q$ are both even (i.e., divisible by $2$) when they were originally assumed to have no common factor. From this logical impasse, we can only conclude that equation (1) cannot hold for any integers $p$ and $q$, and thus the theorem is proved. Exercise: (a) Prove that $\sqrt{3}$ is irrational. Does a similar argument work to show that $\sqrt{6}$ is irrational? (b) Where does the proof of the irrationality of $\sqrt{2}$ break down if we try to use it to prove $\sqrt{4}$ is irrational? Solution: (a) Assume, for contradiction, that there exist integers $p$ and $q$ satisfying (1) $\left(\frac{p}{q}\right)^2 = 3$. Let us also assume that $p$ and $q$ have no common factor. Now, equation (1) implies (2) $p^2 = 3q^2$. From this, we can see that $p^2$ is a multiple of $3$ and hence $p$ must also be a multiple of $3$. This allows us to write $p = 3r$, where $r$ is an integer. After substituting $3r$ for $p$ in equation (2), we get $(3r)^2 = 3q^2$, which can be simplified to $3r^2=q^2$. This implies $q^2$ is a multiple of $3$ and hence $q$ is also a multiple of $3$. Thus we have shown $p$ and $q$ have a common factor, namely $3$, when they were originally assumed to have no common factor.   A similar argument will work for $\sqrt{6}$ as well because we get $p^2 = 6q^2$ which implies $p$ is a multiple of $2$ and $3$. After making the necessary substitutions, we can conclude $q$ is a multiple of $6$, and therefore $\sqrt{6}$ must be irrational. (b) In this case, the fact that $p^2$ is a multiple of $4$ does not imply $p$ is also a multiple of $4$. Thus, our proof breaks down at this point. Question(s): Could somebody please help and spell out for me some of the theorems which are being used along the way in the arguments above? Since the theorem quoted above is the first theorem in the book, I am not exactly sure what is assumed to be common knowledge or whatever... For $\sqrt{3}$, what exactly is being used to justify ""$p^2$ is a multiple of $3$ and hence $p$ must also be a multiple of $3$""? I've seen some other proofs for this which have to do with even and odd numbers, but the one above has to do specifically with $3$, right? For $\sqrt{6}$, why is it important to say ""$p^2 = 6q^2$ implies $p$ is a multiple of $2$ and $3$"" instead of ""is a multiple of $6$""? For $\sqrt{4}$, why is it so clear that ""the fact that $p^2$ is a multiple of $4$ does not imply $p$ is also a multiple of $4$""? I know I can think of a counterexample like $p=2 \Rightarrow p^2=4$ and $4$ is divisible by $4$, but $2$ is not. But what is the pattern to notice? Any help is appreciated. Also, as if it weren't obvious, I think that answers which are not too advanced will be most helpful for me, thanks!",,"['analysis', 'rationality-testing']"
16,$f(x) + f(1-x) = f(1)$,,f(x) + f(1-x) = f(1),"I was discussing with a friend of mine about her research and I came across this problem. The problem essentially boils down to this. $f(x)$ is a function defined in $[0,1]$ such that $f(x) + f(1-x) = f(1)$. I want to find a condition on $f(x)$ so that I can conclude $f(x) = f(1)x$. Clearly, $f \in C^{0}[0,1]$ alone is insufficient to conclude $f(x) = f(1)x$. My hunch is if $f(x) \in C^{\infty}[0,1]$, then $f(x) = f(1)x$. However, I am unable to prove it. Further, is there a weaker condition with which I can conclude $f(x) = f(1)x$? This problem closely resembles another problem: If $f(x+y) = f(x) + f(y)$, $\forall x,y \in \mathbb{R}$ and if $f(x)$ is continuous at atleast one point, then $f(x) = f(1)x$. I know how to prove this statement, but I am unable to see whether this will help me with the original problem. Though these details might not be of much importance to her, I am curious to know. EDIT: As Qiaochu pointed out, I need stronger conditions on $f$ to come up with some reasonable answer. Here is something which I know that $f$ has to satisfy the following: $\forall n \in \mathbb{Z}^{+}\backslash \{1\}$, $f(x_1) + f(x_2) + \cdots + f(x_n) = f(1)$, where $\displaystyle \sum_{k=1}^{n} x_k = 1$, $x_i \geq 0$. Note that $n=2$ boils down to what I had written earlier.","I was discussing with a friend of mine about her research and I came across this problem. The problem essentially boils down to this. $f(x)$ is a function defined in $[0,1]$ such that $f(x) + f(1-x) = f(1)$. I want to find a condition on $f(x)$ so that I can conclude $f(x) = f(1)x$. Clearly, $f \in C^{0}[0,1]$ alone is insufficient to conclude $f(x) = f(1)x$. My hunch is if $f(x) \in C^{\infty}[0,1]$, then $f(x) = f(1)x$. However, I am unable to prove it. Further, is there a weaker condition with which I can conclude $f(x) = f(1)x$? This problem closely resembles another problem: If $f(x+y) = f(x) + f(y)$, $\forall x,y \in \mathbb{R}$ and if $f(x)$ is continuous at atleast one point, then $f(x) = f(1)x$. I know how to prove this statement, but I am unable to see whether this will help me with the original problem. Though these details might not be of much importance to her, I am curious to know. EDIT: As Qiaochu pointed out, I need stronger conditions on $f$ to come up with some reasonable answer. Here is something which I know that $f$ has to satisfy the following: $\forall n \in \mathbb{Z}^{+}\backslash \{1\}$, $f(x_1) + f(x_2) + \cdots + f(x_n) = f(1)$, where $\displaystyle \sum_{k=1}^{n} x_k = 1$, $x_i \geq 0$. Note that $n=2$ boils down to what I had written earlier.",,['analysis']
17,Set Theoretic Definition of Complex Numbers: How to Distinguish $\mathbb{C}$ from $\mathbb{R}^2$?,Set Theoretic Definition of Complex Numbers: How to Distinguish  from ?,\mathbb{C} \mathbb{R}^2,"I have spent some time looking for a rigorous, set-theoretic definition of the complex numbers. I have read the book Elements of Set Theory by Herbert Enderton (1977) which does an excellent job of constructing numbers from sets including the natural numbers, integers, and rational numbers, but stops at the real numbers. So far, I have only found two comparable constructions of complex numbers The set of all $2 \times 2$ matrices taking real-valued components The set of all ordered pairs taking real-valued components I favor the second construction better, because I feel it has a stronger geometric interpretation because of its similarities to Euclidean vector spaces. That is, define \begin{equation*} \mathbb{C}=\{(x,y):x,y \in \mathbb{R}\}, \end{equation*} which also is exactly how the Euclidean plane, $\mathbb{R}^2$, is defined. This leads me to my question. With $\mathbb{C}$ defined exactly the same as how one defines $\mathbb{R}^2$, how does one distinguish the elements of these two sets? For example, how does one distinguish the ordinary vector $(0,1) \in \mathbb{R}^2$ from what we define to be $i$, namely the number $i=(0,1) \in \mathbb{C}$, when they are set-theoretically identical? In set theory, these two very different ""numbers"" -- the vector $(0,1)$ and the number $i$ -- are exactly the same set! Thanks for your thoughts!","I have spent some time looking for a rigorous, set-theoretic definition of the complex numbers. I have read the book Elements of Set Theory by Herbert Enderton (1977) which does an excellent job of constructing numbers from sets including the natural numbers, integers, and rational numbers, but stops at the real numbers. So far, I have only found two comparable constructions of complex numbers The set of all $2 \times 2$ matrices taking real-valued components The set of all ordered pairs taking real-valued components I favor the second construction better, because I feel it has a stronger geometric interpretation because of its similarities to Euclidean vector spaces. That is, define \begin{equation*} \mathbb{C}=\{(x,y):x,y \in \mathbb{R}\}, \end{equation*} which also is exactly how the Euclidean plane, $\mathbb{R}^2$, is defined. This leads me to my question. With $\mathbb{C}$ defined exactly the same as how one defines $\mathbb{R}^2$, how does one distinguish the elements of these two sets? For example, how does one distinguish the ordinary vector $(0,1) \in \mathbb{R}^2$ from what we define to be $i$, namely the number $i=(0,1) \in \mathbb{C}$, when they are set-theoretically identical? In set theory, these two very different ""numbers"" -- the vector $(0,1)$ and the number $i$ -- are exactly the same set! Thanks for your thoughts!",,"['analysis', 'elementary-set-theory', 'complex-numbers']"
18,2013th derivative of rational function,2013th derivative of rational function,,"I am struggling to find $f^{(2013)}(0)$ for $$f(x) = \frac{1}{1 + x + x^3 + x^4}$$ I know that I should use power series, and following a hint I rewrote the equation as the following: $$1 = (1 + x + x^3 + x^4)(\sum_{n = 0}^{\infty} a_n x^n) = \sum_{n = 0}^{\infty} a_n x^n + \sum_{n = 0}^{\infty} a_n x^{n + 1} + \sum_{n = 0}^{\infty} a_n x^{n + 3} + \sum_{n = 0}^{\infty} a_n x^{n + 4}$$ I can work out the value of $a_0$ easily, but I'm not really sure how to solve for the remaining coefficients and hence find the derivatives... I know that the pattern is supposed to be, but I'm completely at a loss. Any hints as to how to proceed?","I am struggling to find $f^{(2013)}(0)$ for $$f(x) = \frac{1}{1 + x + x^3 + x^4}$$ I know that I should use power series, and following a hint I rewrote the equation as the following: $$1 = (1 + x + x^3 + x^4)(\sum_{n = 0}^{\infty} a_n x^n) = \sum_{n = 0}^{\infty} a_n x^n + \sum_{n = 0}^{\infty} a_n x^{n + 1} + \sum_{n = 0}^{\infty} a_n x^{n + 3} + \sum_{n = 0}^{\infty} a_n x^{n + 4}$$ I can work out the value of $a_0$ easily, but I'm not really sure how to solve for the remaining coefficients and hence find the derivatives... I know that the pattern is supposed to be, but I'm completely at a loss. Any hints as to how to proceed?",,"['analysis', 'power-series']"
19,A bounded sequence whose sequence of averages does not converge,A bounded sequence whose sequence of averages does not converge,,"Can we find a bounded sequence $\{a_n\}$ such that the sequence of its averages, say, sequence $\{b_n\}$, where  $$b_n=\frac{1}{n}\sum_{i=1}^n a_i,$$ does not converge?","Can we find a bounded sequence $\{a_n\}$ such that the sequence of its averages, say, sequence $\{b_n\}$, where  $$b_n=\frac{1}{n}\sum_{i=1}^n a_i,$$ does not converge?",,['analysis']
20,Convergence of a Fourier series,Convergence of a Fourier series,,"Let $f$ be the $2\pi$ periodic function which is the even extension of $$x^{1/n}, 0 \le x \le \pi,$$ where $n \ge 2$. I am looking for a general theorem that implies that the Fourier series of $f$ converges to $f$, pointwise, uniformly or absolutely.","Let $f$ be the $2\pi$ periodic function which is the even extension of $$x^{1/n}, 0 \le x \le \pi,$$ where $n \ge 2$. I am looking for a general theorem that implies that the Fourier series of $f$ converges to $f$, pointwise, uniformly or absolutely.",,"['analysis', 'fourier-analysis', 'fourier-series']"
21,"What does ""dx"" mean in ‚à´ùëì(ùë•)ùëëùë•? [duplicate]","What does ""dx"" mean in ‚à´ùëì(ùë•)ùëëùë•? [duplicate]",,"This question already has answers here : What does multiplying the integrand by $dx$ mean in an indefinite integral? (3 answers) Closed 6 months ago . The community reviewed whether to reopen this question 6 months ago and left it closed: Original close reason(s) were not resolved I have this confusion while studying indefinite integrals. Is dx a derivative of x or a notation? If it is just a notation,how can we explain that the transformation from dx to du satisfies the operational law of differentiation?","This question already has answers here : What does multiplying the integrand by $dx$ mean in an indefinite integral? (3 answers) Closed 6 months ago . The community reviewed whether to reopen this question 6 months ago and left it closed: Original close reason(s) were not resolved I have this confusion while studying indefinite integrals. Is dx a derivative of x or a notation? If it is just a notation,how can we explain that the transformation from dx to du satisfies the operational law of differentiation?",,"['analysis', 'indefinite-integrals']"
22,Can we define a derivative on the $p$-adic numbers?,Can we define a derivative on the -adic numbers?,p,"Or on any complete valuated field, really. This question came to me when I was thinking about real and complex differentiability, which we can define in a very similar manner: Let $\mathbb F\in\{\mathbb R,\mathbb C\}$ , and $V,W$ normed, finite dimensional vector spaces over $\mathbb F$ . Also let $f:V\to W$ . We call $f$ $\mathbb F$ -differentiable at $x_0\in V$ if there is an $\mathbb F$ -linear map $\mathrm Df_{x_0}:V\to W$ such that $$\lim_{x\to x_0}\frac{\Vert f(x)-f(x_0)-\mathrm Df_{x_0}(x-x_0)\Vert}{\Vert x-x_0\Vert}=0.$$ This gives us a very natural way to talk about the real or complex differentiability of a function $\mathbb C\to\mathbb C$ , for instance. Just consider $\mathbb C$ a two-dimensional vector space over $\mathbb R$ or a one-dimensional vector space over $\mathbb C$ . I'm wondering wether this can be generalized in an interesting way. This is my idea: We would need a generalization of a norm to general fields, and of course we would still need to work in complete spaces. Norms can be generalized using a valuation: Let $\mathbb F$ be a field. A valuation is a map $\vert\cdot\vert:\mathbb F\to\mathbb R$ satisfying the following for all $a,b\in\mathbb F$ : $\vert a\vert\geq0$ and $\vert a\vert=0$ iff $a=0$ . $\vert ab\vert=\vert a\vert\vert b\vert$ . $\vert a+b\vert\leq\vert a\vert+\vert b\vert$ . Now let $V$ be a vector space over $\mathbb F$ . A norm on $V$ is a map $\Vert \cdot\Vert:V\to\mathbb R$ satisfying the following for all $v,w\in V,~a\in \mathbb F$ : $\Vert v\Vert\geq0$ and $\Vert v\Vert=0$ iff $v=0$ . $\Vert av\Vert=\vert a\vert\Vert v\Vert$ . $\Vert v+w\Vert\leq\Vert v\Vert+\Vert w\Vert$ . A norm on $V$ induces a metric $\Vert x-y\Vert$ . We can now consider finite-dimensional, normed vector spaces $V,W$ over $F$ which are complete with respect to the induced metric, and are free to define: A function $f:V\to W$ is $\mathbb F$ -differentiable in $x_0\in V$ if there exists an $\mathbb F$ -linear map $\mathrm Df_{x_0}:V\to W$ such that $$\lim_{x\to x_0}\frac{\Vert f(x)-f(x_0)-\mathrm Df_{x_0}(x-x_0)\Vert}{\Vert x-x_0\Vert}=0.$$ Now the question is wether such a definition is actually interesting. For instance, finite fields and finite-dimensional vector spaces over them will be discrete, so while they are complete, evaluating the above limit is still not possible. But I imagine infinite fields, like the $p$ -adic numbers, complex $p$ -adic numbers, or function fields, could make the definition a sensible one. Has such a thing been done before? Are there any obstacles to the well-definedness? Or something that would make the theory of such derivatives uninteresting?","Or on any complete valuated field, really. This question came to me when I was thinking about real and complex differentiability, which we can define in a very similar manner: Let , and normed, finite dimensional vector spaces over . Also let . We call -differentiable at if there is an -linear map such that This gives us a very natural way to talk about the real or complex differentiability of a function , for instance. Just consider a two-dimensional vector space over or a one-dimensional vector space over . I'm wondering wether this can be generalized in an interesting way. This is my idea: We would need a generalization of a norm to general fields, and of course we would still need to work in complete spaces. Norms can be generalized using a valuation: Let be a field. A valuation is a map satisfying the following for all : and iff . . . Now let be a vector space over . A norm on is a map satisfying the following for all : and iff . . . A norm on induces a metric . We can now consider finite-dimensional, normed vector spaces over which are complete with respect to the induced metric, and are free to define: A function is -differentiable in if there exists an -linear map such that Now the question is wether such a definition is actually interesting. For instance, finite fields and finite-dimensional vector spaces over them will be discrete, so while they are complete, evaluating the above limit is still not possible. But I imagine infinite fields, like the -adic numbers, complex -adic numbers, or function fields, could make the definition a sensible one. Has such a thing been done before? Are there any obstacles to the well-definedness? Or something that would make the theory of such derivatives uninteresting?","\mathbb F\in\{\mathbb R,\mathbb C\} V,W \mathbb F f:V\to W f \mathbb F x_0\in V \mathbb F \mathrm Df_{x_0}:V\to W \lim_{x\to x_0}\frac{\Vert f(x)-f(x_0)-\mathrm Df_{x_0}(x-x_0)\Vert}{\Vert x-x_0\Vert}=0. \mathbb C\to\mathbb C \mathbb C \mathbb R \mathbb C \mathbb F \vert\cdot\vert:\mathbb F\to\mathbb R a,b\in\mathbb F \vert a\vert\geq0 \vert a\vert=0 a=0 \vert ab\vert=\vert a\vert\vert b\vert \vert a+b\vert\leq\vert a\vert+\vert b\vert V \mathbb F V \Vert \cdot\Vert:V\to\mathbb R v,w\in V,~a\in \mathbb F \Vert v\Vert\geq0 \Vert v\Vert=0 v=0 \Vert av\Vert=\vert a\vert\Vert v\Vert \Vert v+w\Vert\leq\Vert v\Vert+\Vert w\Vert V \Vert x-y\Vert V,W F f:V\to W \mathbb F x_0\in V \mathbb F \mathrm Df_{x_0}:V\to W \lim_{x\to x_0}\frac{\Vert f(x)-f(x_0)-\mathrm Df_{x_0}(x-x_0)\Vert}{\Vert x-x_0\Vert}=0. p p","['analysis', 'p-adic-number-theory', 'differential', 'valuation-theory']"
23,How to use the Lagrange's remainder to prove that log(1+x) = sum(...)?,How to use the Lagrange's remainder to prove that log(1+x) = sum(...)?,,"Using Lagrange's remainder, I have to prove that: $\log(1+x) = \sum\limits_{n=1}^\infty (-1)^{n+1} \cdot \frac{x^n}{n}, \; \forall |x| < 1$ I am not quite sure how to do this. I started with the Taylor series for $x_0 = 0$: $f(x_0) = \sum\limits_{n=0}^\infty \frac{f^{(n)}(x_0)}{n!} \cdot x^n + r_n$, where $r_n$ is the remainder. Then, I used induction to prove that the n-th derivative of $\log(1+x)$ can be written as: $f^{(n)} = (-1)^{n+1} \cdot \frac{(n-1)!}{(1+x)^n}, \forall n \in \mathbb{N}$ I plugged this formula into the Taylor series for $\log(1+x)$ and ended up with: $f(x_0) = \sum\limits_{n=1}^\infty (-1)^{n+1} \cdot \frac{x^n}{n} + r_n$, which already looked quite promising. As the formula which I have to prove doesn't have that remainder $r_n$, I tried to show that $\lim_{n \to \infty} r_n = 0$, using Lagrange's remainder formula (for $x_0 = 0$ and $|x| < 1$). So now I basically showed that the formula was valid for $x \to x_0 = 0$. I also showed that the radius of convergence of this power series is $r = 1$, that is to say the power series converges $\forall |x| < 1$. What is bugging me, is the fact, that to my opinion, the formula is only valid for $x \to 0$. I mean sure, the radius of convergence is 1, but does this actually tell me that the formula is valid within $(-1,1)$? I've never done something like this before, thus the insecurity. I'd be delighted, if someone could help me out and tell me, whether the things I've shown are already sufficient or whether I still need to prove something.","Using Lagrange's remainder, I have to prove that: $\log(1+x) = \sum\limits_{n=1}^\infty (-1)^{n+1} \cdot \frac{x^n}{n}, \; \forall |x| < 1$ I am not quite sure how to do this. I started with the Taylor series for $x_0 = 0$: $f(x_0) = \sum\limits_{n=0}^\infty \frac{f^{(n)}(x_0)}{n!} \cdot x^n + r_n$, where $r_n$ is the remainder. Then, I used induction to prove that the n-th derivative of $\log(1+x)$ can be written as: $f^{(n)} = (-1)^{n+1} \cdot \frac{(n-1)!}{(1+x)^n}, \forall n \in \mathbb{N}$ I plugged this formula into the Taylor series for $\log(1+x)$ and ended up with: $f(x_0) = \sum\limits_{n=1}^\infty (-1)^{n+1} \cdot \frac{x^n}{n} + r_n$, which already looked quite promising. As the formula which I have to prove doesn't have that remainder $r_n$, I tried to show that $\lim_{n \to \infty} r_n = 0$, using Lagrange's remainder formula (for $x_0 = 0$ and $|x| < 1$). So now I basically showed that the formula was valid for $x \to x_0 = 0$. I also showed that the radius of convergence of this power series is $r = 1$, that is to say the power series converges $\forall |x| < 1$. What is bugging me, is the fact, that to my opinion, the formula is only valid for $x \to 0$. I mean sure, the radius of convergence is 1, but does this actually tell me that the formula is valid within $(-1,1)$? I've never done something like this before, thus the insecurity. I'd be delighted, if someone could help me out and tell me, whether the things I've shown are already sufficient or whether I still need to prove something.",,"['analysis', 'taylor-expansion', 'power-series']"
24,Bernoulli's inequality for rational exponents,Bernoulli's inequality for rational exponents,,"We've proved that $$(1+x)^n \geq 1+nx \quad \forall n \in \mathbb{N} \land x \geq -1$$ with induction, and next excercises are to prove $1$ and $2$: $$(1+x)^p \leq 1+px \quad \forall p \in \mathbb{Q}\cap[0,1] \land x \geq -1 \tag{1}$$ $$(1+x)^p \geq 1+px \quad \forall p \in \mathbb{Q}\cap[1,\infty) \land x \geq -1 \tag{2}$$ I was told that $(1)$ will be useful in proving $(2)$, so it's suggested to prove the $(1)$ first. My work : For the first one, i was able to prove that $$(1+x)^{1/n} \leq 1+\frac{x}{n} \quad \forall n \in \mathbb{N}_{+} \land x \geq -1$$ Like this: $$(1+x)^{1/n} \overset{?}{\leq} 1+\frac{x}{n}$$ $$(1+x)\overset{?}{\leq}  \left(1+\frac{x}{n}\right)^n$$ $$ 1+nx \leq (1+x)^n$$ Which is true, but I could not go further. But I was able to prove $(2)$ from $(1)$: for $q \in \mathbb{Q}\cap (0,1]$, we have that $$(1+x)^q \leq 1+qx$$ Letting $pq=1$: $$1+x \leq \left( 1+\frac{x}{p}\right)^p$$ $$1+px \leq (1+x)^p$$ Could you give me a hint to the first one?","We've proved that $$(1+x)^n \geq 1+nx \quad \forall n \in \mathbb{N} \land x \geq -1$$ with induction, and next excercises are to prove $1$ and $2$: $$(1+x)^p \leq 1+px \quad \forall p \in \mathbb{Q}\cap[0,1] \land x \geq -1 \tag{1}$$ $$(1+x)^p \geq 1+px \quad \forall p \in \mathbb{Q}\cap[1,\infty) \land x \geq -1 \tag{2}$$ I was told that $(1)$ will be useful in proving $(2)$, so it's suggested to prove the $(1)$ first. My work : For the first one, i was able to prove that $$(1+x)^{1/n} \leq 1+\frac{x}{n} \quad \forall n \in \mathbb{N}_{+} \land x \geq -1$$ Like this: $$(1+x)^{1/n} \overset{?}{\leq} 1+\frac{x}{n}$$ $$(1+x)\overset{?}{\leq}  \left(1+\frac{x}{n}\right)^n$$ $$ 1+nx \leq (1+x)^n$$ Which is true, but I could not go further. But I was able to prove $(2)$ from $(1)$: for $q \in \mathbb{Q}\cap (0,1]$, we have that $$(1+x)^q \leq 1+qx$$ Letting $pq=1$: $$1+x \leq \left( 1+\frac{x}{p}\right)^p$$ $$1+px \leq (1+x)^p$$ Could you give me a hint to the first one?",,"['analysis', 'inequality']"
25,Fourier transform of a Schwartz space function and norm,Fourier transform of a Schwartz space function and norm,,"The Schwartz space (of rapidly decreasing functions) is the set of all $C^{\infty}$ functions $f\colon\mathbb{R}\to\mathbb{C}$ such that  $$ x^jf^{(k)}(x) \to 0 $$ for all integers $j,k\geq0$ as $x \to {\pm\infty}$. Let $f$ be a rapidly decreasing function in Schwartz space and let $\mathcal{F}(f)$ its Fourier transform. If  $$ \| f \|_2 = \| \mathcal{F}(f) \|_2 \quad, $$  can we deduce that $\mathcal{F}(f)$ is a Schwartz function?","The Schwartz space (of rapidly decreasing functions) is the set of all $C^{\infty}$ functions $f\colon\mathbb{R}\to\mathbb{C}$ such that  $$ x^jf^{(k)}(x) \to 0 $$ for all integers $j,k\geq0$ as $x \to {\pm\infty}$. Let $f$ be a rapidly decreasing function in Schwartz space and let $\mathcal{F}(f)$ its Fourier transform. If  $$ \| f \|_2 = \| \mathcal{F}(f) \|_2 \quad, $$  can we deduce that $\mathcal{F}(f)$ is a Schwartz function?",,"['analysis', 'fourier-analysis']"
26,Is there a pair of open sets whose Minkowski sum is not open?,Is there a pair of open sets whose Minkowski sum is not open?,,"Is it possible to find an example where the Minkowski sum of two open sets is not open? (If someone could think of one, could they possibly also suggest how they came up with the example? Perhaps there is a ""common counterexamples"" list that people usually use to approach this sort of question?)","Is it possible to find an example where the Minkowski sum of two open sets is not open? (If someone could think of one, could they possibly also suggest how they came up with the example? Perhaps there is a ""common counterexamples"" list that people usually use to approach this sort of question?)",,['analysis']
27,Compute the gradient of mean square error,Compute the gradient of mean square error,,Let $Y = \begin{pmatrix} y_1  \\ \cdots \\ y_N\end{pmatrix}$ and $X = \begin{pmatrix} x_{11} & \cdots & x_{1D}  \\ \cdots & \cdots & \cdots \\ x_{N1} & \cdots &x_{ND}\end{pmatrix}$. Let also $e = y - Xw$ and let's write the mean square error as $L(w) = \frac{1}{2N} \sum_{i=1}^{N} (y_n - x_n^Tw)^2 = \frac{1}{2N} e^T e$. I want to prove that the gradient of $L(w)$ is $-\frac{1}{N} X^T e$. What would be a way of proving this?,Let $Y = \begin{pmatrix} y_1  \\ \cdots \\ y_N\end{pmatrix}$ and $X = \begin{pmatrix} x_{11} & \cdots & x_{1D}  \\ \cdots & \cdots & \cdots \\ x_{N1} & \cdots &x_{ND}\end{pmatrix}$. Let also $e = y - Xw$ and let's write the mean square error as $L(w) = \frac{1}{2N} \sum_{i=1}^{N} (y_n - x_n^Tw)^2 = \frac{1}{2N} e^T e$. I want to prove that the gradient of $L(w)$ is $-\frac{1}{N} X^T e$. What would be a way of proving this?,,"['analysis', 'statistics', 'mean-square-error', 'gradient-descent']"
28,How to prove this relation between the laplacian of the logarithm and the dirac delta function?,How to prove this relation between the laplacian of the logarithm and the dirac delta function?,,"Why is this true in two dimensions? $$\nabla^2\bigg(\ln(r)\bigg)=2\pi\delta^{(2)}(\mathbf{r}),$$ where $\delta^{(2)}$ denotes the two-dimensional $\delta$-function and $r=\sqrt{x^2+y^2}$ in Cartesian coordinates. I understand that both function will look the same. But I do not know how to prove this rigorously.","Why is this true in two dimensions? $$\nabla^2\bigg(\ln(r)\bigg)=2\pi\delta^{(2)}(\mathbf{r}),$$ where $\delta^{(2)}$ denotes the two-dimensional $\delta$-function and $r=\sqrt{x^2+y^2}$ in Cartesian coordinates. I understand that both function will look the same. But I do not know how to prove this rigorously.",,"['analysis', 'dirac-delta', 'laplacian']"
29,"Exercise 9, Chapter 2 of Stein's Fourier Analysis. Showing that a fourier series does not converge absolutely but converges conditionally.","Exercise 9, Chapter 2 of Stein's Fourier Analysis. Showing that a fourier series does not converge absolutely but converges conditionally.",,"Let $f(x)=\chi_{[a,b]}(x)$ be the characteristic function of the interval $[a,b]\subset [-\pi,\pi]$. Show that if $a\neq -\pi$, or $b\neq \pi$ and $a\neq b$, then the Fourier series does not converge absolutely for any $x$. [Hint: It suffices to prove that for many values of $n$ one has $|\sin n\theta_0|\ge c \gt 0$ where $\theta_0=(b-a)/2.$] However, prove that the Fourier series converges at every point $x$. I've computed the Fourier series and got $\frac{b-a}{2\pi}+\sum_{n\neq 0}\frac{e^{-ina}-e^{-inb}}{2\pi in}e^{inx}.$ Also, $|e^{-ina}-e^{-inb}|=2|\sin n\theta_0|$, and $\theta_0\in (0,\pi)$, so I can see that for infinitely many values of $n$, we have $|\sin n\theta_0|\ge c \gt 0$. But this does not guarantee $\sum_{n\neq 0}|\frac{e^{-ina}-e^{-inb}}{2\pi in}e^{inx}|\ge \sum \frac{c}{n}$, and in fact we might have this inequality only for the squares of integers, in which case the right hand side converges. So how does the hint solve the problem? Moreover, for the second problem, to show that the Fourier series converges at every point, I think I need to use Dirichlet's test, using $1/n$ as the decreasing sequence to $0$, but how can I show that $\frac{e^{-ina}-e^{-inb}}{2\pi in}e^{inx}$ has bounded partial sums? I would greatly appreciate any help.","Let $f(x)=\chi_{[a,b]}(x)$ be the characteristic function of the interval $[a,b]\subset [-\pi,\pi]$. Show that if $a\neq -\pi$, or $b\neq \pi$ and $a\neq b$, then the Fourier series does not converge absolutely for any $x$. [Hint: It suffices to prove that for many values of $n$ one has $|\sin n\theta_0|\ge c \gt 0$ where $\theta_0=(b-a)/2.$] However, prove that the Fourier series converges at every point $x$. I've computed the Fourier series and got $\frac{b-a}{2\pi}+\sum_{n\neq 0}\frac{e^{-ina}-e^{-inb}}{2\pi in}e^{inx}.$ Also, $|e^{-ina}-e^{-inb}|=2|\sin n\theta_0|$, and $\theta_0\in (0,\pi)$, so I can see that for infinitely many values of $n$, we have $|\sin n\theta_0|\ge c \gt 0$. But this does not guarantee $\sum_{n\neq 0}|\frac{e^{-ina}-e^{-inb}}{2\pi in}e^{inx}|\ge \sum \frac{c}{n}$, and in fact we might have this inequality only for the squares of integers, in which case the right hand side converges. So how does the hint solve the problem? Moreover, for the second problem, to show that the Fourier series converges at every point, I think I need to use Dirichlet's test, using $1/n$ as the decreasing sequence to $0$, but how can I show that $\frac{e^{-ina}-e^{-inb}}{2\pi in}e^{inx}$ has bounded partial sums? I would greatly appreciate any help.",,"['analysis', 'convergence-divergence', 'fourier-analysis', 'fourier-series', 'conditional-convergence']"
30,How find the example such $\left(\sup_{x\in R}|f'(x)|\right)^2=2\sup_{x\in R}|f(x)|\cdot\sup_{x\in R}|f''(x)|$,How find the example such,\left(\sup_{x\in R}|f'(x)|\right)^2=2\sup_{x\in R}|f(x)|\cdot\sup_{x\in R}|f''(x)|,"Question: Find a example function $f$,such $f\in C^2(R)$,and such $$\left(\sup_{x\in R}|f'(x)|\right)^2=2\sup_{x\in R}|f(x)|\cdot\sup_{x\in R}|f''(x)|$$ This problem is from when I prove this inequality $$\left(\sup_{x\in R}|f'(x)|\right)^2\le 2\sup_{x\in R}|f(x)|\cdot\sup_{x\in R}|f''(x)|$$ poof: this inequality I have$$f(x+y)=f(x)+f'(x)y+f''(\xi)\dfrac{y^2}{2}$$ $$f(x-y)=f(x)-f'(x)y+f''(\eta)\dfrac{y^2}{2}$$ then  $$f(x+y)-f(x-y)=2f'(x)y+\left(f''(\xi)-f''(\eta)\right)\dfrac{y^2}{2}$$ then  $$2y|f'(x)|=|f(x+y)-f(x-y)-(f''(\xi)-f''(\eta))\dfrac{y^2}{2}|\le 2a+by^2$$ so  $$\sup_{x\in R}|f'(x)|\le\dfrac{a}{y}+\dfrac{by}{2}$$ By Done But this problem ask the constant $2$ is best.so I think we must find a example,I can't find the function such $f\in C^2(R)$","Question: Find a example function $f$,such $f\in C^2(R)$,and such $$\left(\sup_{x\in R}|f'(x)|\right)^2=2\sup_{x\in R}|f(x)|\cdot\sup_{x\in R}|f''(x)|$$ This problem is from when I prove this inequality $$\left(\sup_{x\in R}|f'(x)|\right)^2\le 2\sup_{x\in R}|f(x)|\cdot\sup_{x\in R}|f''(x)|$$ poof: this inequality I have$$f(x+y)=f(x)+f'(x)y+f''(\xi)\dfrac{y^2}{2}$$ $$f(x-y)=f(x)-f'(x)y+f''(\eta)\dfrac{y^2}{2}$$ then  $$f(x+y)-f(x-y)=2f'(x)y+\left(f''(\xi)-f''(\eta)\right)\dfrac{y^2}{2}$$ then  $$2y|f'(x)|=|f(x+y)-f(x-y)-(f''(\xi)-f''(\eta))\dfrac{y^2}{2}|\le 2a+by^2$$ so  $$\sup_{x\in R}|f'(x)|\le\dfrac{a}{y}+\dfrac{by}{2}$$ By Done But this problem ask the constant $2$ is best.so I think we must find a example,I can't find the function such $f\in C^2(R)$",,['analysis']
31,"How common is the use of the term ""primitive"" to mean ""antiderivative""?","How common is the use of the term ""primitive"" to mean ""antiderivative""?",,"I don't know if this should actually be asked on the English stackexchange.  It seemed like I would find better answers here. I have all but finished an undergraduate degree in mathematics in the United States, but I have never once heard the term ""primitive"" to mean ""antiderivative"" until recently, when someone from Europe pointed it out to me.  According to him, it's a common term there.  So I was wondering if people could give me an idea of how common this term is, and where.  I know for sure that if someone says ""primitive"" to a math student in the US, that the student won't know what he is talking about. Does the reverse hold for ""antiderivative"" (or the also common ""integral"") elsewhere?","I don't know if this should actually be asked on the English stackexchange.  It seemed like I would find better answers here. I have all but finished an undergraduate degree in mathematics in the United States, but I have never once heard the term ""primitive"" to mean ""antiderivative"" until recently, when someone from Europe pointed it out to me.  According to him, it's a common term there.  So I was wondering if people could give me an idea of how common this term is, and where.  I know for sure that if someone says ""primitive"" to a math student in the US, that the student won't know what he is talking about. Does the reverse hold for ""antiderivative"" (or the also common ""integral"") elsewhere?",,"['analysis', 'terminology']"
32,On distributions over $\mathbb R$ whose derivatives vanishes,On distributions over  whose derivatives vanishes,\mathbb R,"Let $I \subset \mathbb R$ be open, $u \in \mathcal D'(I)$ be a distribution whose distributional derivatives vanishes (i.e. is zero for all test functions, which we may assume to be complex valued ). We show $\forall c \in \mathbb C: \forall \phi \in \mathcal D(I) : u(\phi) = \int c\cdot\phi dx$. (EDIT: Correctly, $c$ should be quantified with $\exists$. My question has been why the following proof doesn't allow for arbitrary complex $c$, which explains the preceding statement.) Proof: Let $\Psi \in D(I)$. $\Psi$ is the derivative of a test function iff $\int \phi dx = 0$. In that case $u(\Psi) = 0$. Let $h \in D(I)$ be arbitrary with $\int h dx = 1$. Now for every test function $\phi \in D(I)$ we see: $\phi - \int \phi dx \cdot h \in D(I)$ and $\int ( \phi - \int \phi dx h ) dx = 0$. therefore $u( \phi - \int \phi dx h ) = 0$, i.e. $u(\phi) = u(h) \int \phi dx$ $\square$ If this is not wrong, how can I interpret the fact that $h$ has been arbitrary? Note: This is part of a larger proof, which shows the same for non-one-dimensional domains.","Let $I \subset \mathbb R$ be open, $u \in \mathcal D'(I)$ be a distribution whose distributional derivatives vanishes (i.e. is zero for all test functions, which we may assume to be complex valued ). We show $\forall c \in \mathbb C: \forall \phi \in \mathcal D(I) : u(\phi) = \int c\cdot\phi dx$. (EDIT: Correctly, $c$ should be quantified with $\exists$. My question has been why the following proof doesn't allow for arbitrary complex $c$, which explains the preceding statement.) Proof: Let $\Psi \in D(I)$. $\Psi$ is the derivative of a test function iff $\int \phi dx = 0$. In that case $u(\Psi) = 0$. Let $h \in D(I)$ be arbitrary with $\int h dx = 1$. Now for every test function $\phi \in D(I)$ we see: $\phi - \int \phi dx \cdot h \in D(I)$ and $\int ( \phi - \int \phi dx h ) dx = 0$. therefore $u( \phi - \int \phi dx h ) = 0$, i.e. $u(\phi) = u(h) \int \phi dx$ $\square$ If this is not wrong, how can I interpret the fact that $h$ has been arbitrary? Note: This is part of a larger proof, which shows the same for non-one-dimensional domains.",,"['analysis', 'distribution-theory']"
33,Definition of $C^k$ boundary,Definition of  boundary,C^k,"Can someone give me a resonable definition of $C^k$ boundary, e.g., to define and after give a brief explain about the definition. I need this 'cause I'm not understanding what the Evan's book said. Thanks!","Can someone give me a resonable definition of $C^k$ boundary, e.g., to define and after give a brief explain about the definition. I need this 'cause I'm not understanding what the Evan's book said. Thanks!",,"['analysis', 'partial-differential-equations', 'definition']"
34,Intermediate value-like theorem for $\mathbb{C}$?,Intermediate value-like theorem for ?,\mathbb{C},"Is there an intermediate value like theorem for $\mathbb{C}$? I know $\mathbb {C}$ isn't ordered, but if we have a function $f:\mathbb{C}\to\mathbb{C}$ that's continuous, what can we conclude about it? Also, if  we have a function, $g:\mathbb{C}\to\mathbb{R}$ ,continuous with $g(x)>0>g(y)$ does that imply a z ""between"" them satisfying $g(z)=0$. Edit: I apologize if the question is vague and confusing. I really want to ask for which definition of between, (for example maybe the  part of the plane dividing the points), and with relaxed definition on intermediateness for the first part, can we prove any such results?","Is there an intermediate value like theorem for $\mathbb{C}$? I know $\mathbb {C}$ isn't ordered, but if we have a function $f:\mathbb{C}\to\mathbb{C}$ that's continuous, what can we conclude about it? Also, if  we have a function, $g:\mathbb{C}\to\mathbb{R}$ ,continuous with $g(x)>0>g(y)$ does that imply a z ""between"" them satisfying $g(z)=0$. Edit: I apologize if the question is vague and confusing. I really want to ask for which definition of between, (for example maybe the  part of the plane dividing the points), and with relaxed definition on intermediateness for the first part, can we prove any such results?",,"['analysis', 'continuity']"
35,Taylor series of product of two functions,Taylor series of product of two functions,,let $f$ and $g$ be infinitley differentiable functions and $a_k = \frac{f^{(k)}(a)}{k!}$ and $b_e = \frac{g^{(e)}(a)}{e!}$ be cofficients of Taylor Polynomial at $a$ then what would be the coefficients of $fg$ . rather than asking my specific question I asked this general question so other can benefit too So I think we would need to multiply the two polynomials but that's just my intuition and I don't know how to justify it and I don't think it would be as simple.,let and be infinitley differentiable functions and and be cofficients of Taylor Polynomial at then what would be the coefficients of . rather than asking my specific question I asked this general question so other can benefit too So I think we would need to multiply the two polynomials but that's just my intuition and I don't know how to justify it and I don't think it would be as simple.,f g a_k = \frac{f^{(k)}(a)}{k!} b_e = \frac{g^{(e)}(a)}{e!} a fg,"['analysis', 'taylor-expansion']"
36,"$f, \hat{f} \in L^{p}(\mathbb R) \cap C(\mathbb R) \implies |f(x)| \to 0$ as $|x| \to \infty$?",as ?,"f, \hat{f} \in L^{p}(\mathbb R) \cap C(\mathbb R) \implies |f(x)| \to 0 |x| \to \infty","Suppose $f, \hat{f} \in L^{p}(\mathbb R) \cap C(\mathbb R)\cap L^{\infty}(\mathbb R), (1<p<\infty).$ My Question : Can we expect $\lim_{|x|\to \infty} |f(x)|=0$ ? (In other words, If $f$ and its Fourier transform $\hat{f}$ both are in  $L^{p} $ space and continuous , does it mean that $f$ vanishes at infinity) [We note that for $p=1,$ the result follows from Riemann Lebsgue lemma, and inversion formula] Edit: If needed, additionally, we assume $f$ is  in $A(\mathbb T),$  that is, if we restrict to $f$ to finite interval, say $[0, 2\pi] \subset \mathbb R,$  then $\hat{f} \in \ell^{1}(\mathbb Z),$ that is, $\sum_{n\in \mathbb Z} |\hat{f}(n)| < \infty.$ Edit again: We assume that  $f\in \mathcal{F}L^{1}(\mathbb R)$ means that $\phi f \in A(\mathbb T)$ for all $\phi \in C^{\infty}(\mathbb R)$ with $\phi $ is compactly supported in $[0, 2\pi)$ (or in general case any interval I of length $2\pi$ ).  With this assumption can we expect $|f(x)|\to 0 $ as $|x|\to \infty$?","Suppose $f, \hat{f} \in L^{p}(\mathbb R) \cap C(\mathbb R)\cap L^{\infty}(\mathbb R), (1<p<\infty).$ My Question : Can we expect $\lim_{|x|\to \infty} |f(x)|=0$ ? (In other words, If $f$ and its Fourier transform $\hat{f}$ both are in  $L^{p} $ space and continuous , does it mean that $f$ vanishes at infinity) [We note that for $p=1,$ the result follows from Riemann Lebsgue lemma, and inversion formula] Edit: If needed, additionally, we assume $f$ is  in $A(\mathbb T),$  that is, if we restrict to $f$ to finite interval, say $[0, 2\pi] \subset \mathbb R,$  then $\hat{f} \in \ell^{1}(\mathbb Z),$ that is, $\sum_{n\in \mathbb Z} |\hat{f}(n)| < \infty.$ Edit again: We assume that  $f\in \mathcal{F}L^{1}(\mathbb R)$ means that $\phi f \in A(\mathbb T)$ for all $\phi \in C^{\infty}(\mathbb R)$ with $\phi $ is compactly supported in $[0, 2\pi)$ (or in general case any interval I of length $2\pi$ ).  With this assumption can we expect $|f(x)|\to 0 $ as $|x|\to \infty$?",,"['analysis', 'fourier-analysis', 'distribution-theory']"
37,Constructing the natural numbers without set theory.,Constructing the natural numbers without set theory.,,As I understand it the idea of defining everything as sets is a relatively new idea in mathematics.  Does that mean there's a non-set theoretic definition of the natural numbers?  Could there be?,As I understand it the idea of defining everything as sets is a relatively new idea in mathematics.  Does that mean there's a non-set theoretic definition of the natural numbers?  Could there be?,,"['analysis', 'definition', 'number-systems']"
38,What does 'finite-valued' mean?,What does 'finite-valued' mean?,,"In Rudin, while defining the concept of 'pointwise bounded', it says: if there exists a finite-valuded function $\phi$  defined on $E$ such that $|f_{n}(x)|<\phi (x)$. Here, I am quite puzzled by the definition of finite-valued. Is $f(x)=1/x$ an example of finite-valued function, since when $x$ tends to zero, it tends to infinity, how could it be classified as finite-valued? And any example of a function which is not finite-valuded?  Thanks so much for your help!","In Rudin, while defining the concept of 'pointwise bounded', it says: if there exists a finite-valuded function $\phi$  defined on $E$ such that $|f_{n}(x)|<\phi (x)$. Here, I am quite puzzled by the definition of finite-valued. Is $f(x)=1/x$ an example of finite-valued function, since when $x$ tends to zero, it tends to infinity, how could it be classified as finite-valued? And any example of a function which is not finite-valuded?  Thanks so much for your help!",,['analysis']
39,What is the infinity norm on a continuous function space?,What is the infinity norm on a continuous function space?,,"As I understand it the norm $\|f\|_2$ on the set of continuous functions on $[0,1]$ is defined by $$\|f\|_2 = \sqrt{\int_0^1|f(t)|^2dt}$$ but what is the infinity norm $\|f\|_\infty$? Is it $$\int_0^1 \max|f(t)|dt$$ so in other words just $\max|f(t)|$?","As I understand it the norm $\|f\|_2$ on the set of continuous functions on $[0,1]$ is defined by $$\|f\|_2 = \sqrt{\int_0^1|f(t)|^2dt}$$ but what is the infinity norm $\|f\|_\infty$? Is it $$\int_0^1 \max|f(t)|dt$$ so in other words just $\max|f(t)|$?",,[]
40,Does analysis do anything other than estimating things and discussing convergence? [closed],Does analysis do anything other than estimating things and discussing convergence? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 years ago . Improve this question Let me first of all declare that till a few months ago, Analysis was the subject I liked most, as I was pretty good at it and the idea was simple: you essentially bound things. But then, I had these spurts of realizations: has Analysis nothing else to offer? Let's take a quick look at what analysis does. Fourier analysis: Hell lot of PDE's, heat equations, etc. and hell lot of estimating things. The concept of generalized function I admit, is really nice, but that's essentially all to it. From the beginning to the end, estimate integrals or show certain functions belong to a class. Complex analysis: This is a beautiful subject, although estimations crop up here as well, quite often. However, I do like things like Cauchy's Theorem, Morera's Theorem, etc. simply because they are NOT estimating things! Functional analysis: Convergence on arbitrary spaces and some fairly complicated existential theorems. Due to lack of concrete integration, there is lack of estimating things but then, there's always the concept of showing convergence. Analytic Number Theory: I got bored to death trying to study this. From the first page to the last (probably the book I chose wasn't friendly) I saw integrals being estimated. I have no grievance towards analysis in particular, and as I have mentioned, I am actually good in it. I can grasp analysis concepts really well and my background is quite strong. However, after a point, you really begin to wonder whether a subject has anything else to offer other than estimating integrals/series and checking convergence. I, unfortunately, haven't been exposed to prospective fields of Analysis which go beyond these. So, at times, the journey has been immensely boring. I would like to ask the community of mathematicians here: what is your opinion? I would love to know if there are topics in analysis beyond these estimations and computations, so if you know of them please do tell me. Yes, something I missed is: why did I like analysis? Because it reduced a lot of computations I used to do as a high school student. Look at the Riemann-Lebesgue Lemma. Look at the power of Stone-Weierstrass. These are theorems that really boost my interest. But then, what about the rest?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 years ago . Improve this question Let me first of all declare that till a few months ago, Analysis was the subject I liked most, as I was pretty good at it and the idea was simple: you essentially bound things. But then, I had these spurts of realizations: has Analysis nothing else to offer? Let's take a quick look at what analysis does. Fourier analysis: Hell lot of PDE's, heat equations, etc. and hell lot of estimating things. The concept of generalized function I admit, is really nice, but that's essentially all to it. From the beginning to the end, estimate integrals or show certain functions belong to a class. Complex analysis: This is a beautiful subject, although estimations crop up here as well, quite often. However, I do like things like Cauchy's Theorem, Morera's Theorem, etc. simply because they are NOT estimating things! Functional analysis: Convergence on arbitrary spaces and some fairly complicated existential theorems. Due to lack of concrete integration, there is lack of estimating things but then, there's always the concept of showing convergence. Analytic Number Theory: I got bored to death trying to study this. From the first page to the last (probably the book I chose wasn't friendly) I saw integrals being estimated. I have no grievance towards analysis in particular, and as I have mentioned, I am actually good in it. I can grasp analysis concepts really well and my background is quite strong. However, after a point, you really begin to wonder whether a subject has anything else to offer other than estimating integrals/series and checking convergence. I, unfortunately, haven't been exposed to prospective fields of Analysis which go beyond these. So, at times, the journey has been immensely boring. I would like to ask the community of mathematicians here: what is your opinion? I would love to know if there are topics in analysis beyond these estimations and computations, so if you know of them please do tell me. Yes, something I missed is: why did I like analysis? Because it reduced a lot of computations I used to do as a high school student. Look at the Riemann-Lebesgue Lemma. Look at the power of Stone-Weierstrass. These are theorems that really boost my interest. But then, what about the rest?",,"['analysis', 'soft-question']"
41,Asymptotics for incomplete $\Gamma$ with equal arguments,Asymptotics for incomplete  with equal arguments,\Gamma,"On the digital library of mathematical functions, there is a uniform asymptotic expansion of the incomplete gamma function of equal arguments $\Gamma(z,z)$ for large $z$ ( http://dlmf.nist.gov/8.11#v -- formula 8.11.12). The lowest order terms are $$\Gamma(z,z) \sim z^{z-1}e^{-z} \left(\sqrt{\frac{\pi}{2}} z^{\frac{1}{2}} - \frac{1}{3} + \frac{\sqrt{2 \pi} }{24 z^{\frac{1}{2}}} + \ldots \right)$$ The digital library unfortunately cites no sources. Anybody has any idea how this was derived? I have tried to derive it with Laplace's approximation. In the integral that defines the incomplete $\Gamma$ , the maximum of the integrand is at $t=z-1$ while the lower limit of integration is $z$ . So the maximum lies outside the integration range. You must therefore approximate the integrand for small positive values of $t -z$ , and this gives something completely different from the digital-library formula. Moreover: the function I am really interested in is $\Gamma(z,z-a)$ , calculated for large $z$ . Here $a >1$ is a fixed real number, so the maximum is outside the integration range. But the Laplace method may not be sufficient because I need corrections from the $t=z-a$ too. Does anybody have any hint on how to calculate this?","On the digital library of mathematical functions, there is a uniform asymptotic expansion of the incomplete gamma function of equal arguments for large ( http://dlmf.nist.gov/8.11#v -- formula 8.11.12). The lowest order terms are The digital library unfortunately cites no sources. Anybody has any idea how this was derived? I have tried to derive it with Laplace's approximation. In the integral that defines the incomplete , the maximum of the integrand is at while the lower limit of integration is . So the maximum lies outside the integration range. You must therefore approximate the integrand for small positive values of , and this gives something completely different from the digital-library formula. Moreover: the function I am really interested in is , calculated for large . Here is a fixed real number, so the maximum is outside the integration range. But the Laplace method may not be sufficient because I need corrections from the too. Does anybody have any hint on how to calculate this?","\Gamma(z,z) z \Gamma(z,z) \sim z^{z-1}e^{-z} \left(\sqrt{\frac{\pi}{2}} z^{\frac{1}{2}} - \frac{1}{3} + \frac{\sqrt{2 \pi} }{24 z^{\frac{1}{2}}} + \ldots \right) \Gamma t=z-1 z t -z \Gamma(z,z-a) z a >1 t=z-a","['analysis', 'asymptotics']"
42,What is $\inf\emptyset$ and $\sup\emptyset$? [duplicate],What is  and ? [duplicate],\inf\emptyset \sup\emptyset,"This question already has answers here : Infimum and supremum of the empty set (6 answers) Closed 9 years ago . I was told that $\sup\emptyset=-\infty$ and $\inf\emptyset=\infty$, where $\emptyset$ is the empty set. This seems paradoxical to me as to how supremum can be less than infimum. Is there any proof for this or is it taken for granted ?","This question already has answers here : Infimum and supremum of the empty set (6 answers) Closed 9 years ago . I was told that $\sup\emptyset=-\infty$ and $\inf\emptyset=\infty$, where $\emptyset$ is the empty set. This seems paradoxical to me as to how supremum can be less than infimum. Is there any proof for this or is it taken for granted ?",,"['analysis', 'order-theory', 'supremum-and-infimum']"
43,Convergence of Fourier Series,Convergence of Fourier Series,,Is there an $f\in L^1(\mathbb{T})$ whose Fourier series converges a.e. on $\mathbb{T}$ but not a.e. to $f$?,Is there an $f\in L^1(\mathbb{T})$ whose Fourier series converges a.e. on $\mathbb{T}$ but not a.e. to $f$?,,"['analysis', 'fourier-analysis', 'fourier-series']"
44,Why is non-standard analysis not fully or at all integrated in our current school systems (around the world) [duplicate],Why is non-standard analysis not fully or at all integrated in our current school systems (around the world) [duplicate],,"This question already has answers here : What are the disadvantages of non-standard analysis? (4 answers) Closed 7 years ago . Before you read my text down below. I would want you to know that I am down below talking about education on all levels, but mostly on the levels that is below university level. But on university level, at least not in all parts of the world, non-standard analysis is not very widespread. Non-standard analysis gives a more in-depth explanation and rigorous ground for understanding limits. While limits most often is taught implicitly and gives an implicit understanding - which makes it a hard subject to understand, for many new to the concepts of limits - non-standard analysis is a lot better method to use instead of limits, as it gives an explicit understanding. I would really believe that at least some areas of non-standard analysis should be given more place in regular education; the standard part function is a perfect example. Non-standard analysis are also a good and firm ground to have if you want to learn limits. There exists an elementary book written on the subject, which unforunately did not make it all the way into our schools: https://www.math.wisc.edu/~keisler/calc.html . I think non-standard analysis to be a good thing to educate students in, at an early level, if you want to give them understanding about certain parts of math. But traditional methods are closely held to. Why are traditional methods so hard held to? Why not integrate non-standard analysis into current school systems?","This question already has answers here : What are the disadvantages of non-standard analysis? (4 answers) Closed 7 years ago . Before you read my text down below. I would want you to know that I am down below talking about education on all levels, but mostly on the levels that is below university level. But on university level, at least not in all parts of the world, non-standard analysis is not very widespread. Non-standard analysis gives a more in-depth explanation and rigorous ground for understanding limits. While limits most often is taught implicitly and gives an implicit understanding - which makes it a hard subject to understand, for many new to the concepts of limits - non-standard analysis is a lot better method to use instead of limits, as it gives an explicit understanding. I would really believe that at least some areas of non-standard analysis should be given more place in regular education; the standard part function is a perfect example. Non-standard analysis are also a good and firm ground to have if you want to learn limits. There exists an elementary book written on the subject, which unforunately did not make it all the way into our schools: https://www.math.wisc.edu/~keisler/calc.html . I think non-standard analysis to be a good thing to educate students in, at an early level, if you want to give them understanding about certain parts of math. But traditional methods are closely held to. Why are traditional methods so hard held to? Why not integrate non-standard analysis into current school systems?",,"['analysis', 'soft-question', 'nonstandard-analysis', 'infinitesimals']"
45,Roots of the Chebyshev polynomials of the second kind.,Roots of the Chebyshev polynomials of the second kind.,,"It is known that the roots of the Chebyshev polynomials of the second kind , denote it by $U_n(x)$, are in the interval $(-1,1)$ and they are simple (of multiplicity one). I have noticed that the roots of $U_n{(x)}+U_{n-1}(x)$ (by looking at the law ranks of $n$) also lies in $(-1,1)$, I also noticed that  for $(1-x)U_n{(x)}+U_{n-1}(x)$ the roots lie in  $(-2,2)$. But I don't have any idea how to prove that in general, I wonder, first, if these claims are true? and how can I start proving them?","It is known that the roots of the Chebyshev polynomials of the second kind , denote it by $U_n(x)$, are in the interval $(-1,1)$ and they are simple (of multiplicity one). I have noticed that the roots of $U_n{(x)}+U_{n-1}(x)$ (by looking at the law ranks of $n$) also lies in $(-1,1)$, I also noticed that  for $(1-x)U_n{(x)}+U_{n-1}(x)$ the roots lie in  $(-2,2)$. But I don't have any idea how to prove that in general, I wonder, first, if these claims are true? and how can I start proving them?",,"['analysis', 'roots', 'orthogonal-polynomials', 'chebyshev-polynomials']"
46,Finding solutions to $ x^x = 2x$,Finding solutions to, x^x = 2x,"A friend claims it isn't possible to find a closed form for the smaller positive real solution of $x^x = 2x$. Numerically we have seen that $0.346...$ and $2$ are solutions, but are failing to do anything but approximate the first solution. The first attempt to find a closed form was using the same trick as solving $$x^{x^{x^x...}} = 2$$ Where raising $x$ to the power of both sides gives: $$x^{x^{x^x...}} = x^2$$ $$\Rightarrow 2 = x^2 $$ $$\Rightarrow x = \sqrt{2}$$ But without an infinite power tower we got nowhere. We know $f(z) = z^z$ is analytic with inverse $f^{-1}(z) = e^{W(ln(z))}$ where $W(z)$ is the Lambert-W function, But $x = e^{W(ln(2x))}$ doesn't seem any better. I am just wondering how to approach such a problem.","A friend claims it isn't possible to find a closed form for the smaller positive real solution of $x^x = 2x$. Numerically we have seen that $0.346...$ and $2$ are solutions, but are failing to do anything but approximate the first solution. The first attempt to find a closed form was using the same trick as solving $$x^{x^{x^x...}} = 2$$ Where raising $x$ to the power of both sides gives: $$x^{x^{x^x...}} = x^2$$ $$\Rightarrow 2 = x^2 $$ $$\Rightarrow x = \sqrt{2}$$ But without an infinite power tower we got nowhere. We know $f(z) = z^z$ is analytic with inverse $f^{-1}(z) = e^{W(ln(z))}$ where $W(z)$ is the Lambert-W function, But $x = e^{W(ln(2x))}$ doesn't seem any better. I am just wondering how to approach such a problem.",,"['analysis', 'tetration', 'lambert-w']"
47,Remainder form of Taylor polynomial at $x_0$: $ \frac{1}{n!}f^{(n)}(x_0 +\theta(x-x_0))(x-x_0)^n$ with $\theta \to \frac{1}{n+1}$ as $ x \to x_0$,Remainder form of Taylor polynomial at :  with  as,x_0  \frac{1}{n!}f^{(n)}(x_0 +\theta(x-x_0))(x-x_0)^n \theta \to \frac{1}{n+1}  x \to x_0,"If the function $f: \mathbb R \to \mathbb R$ is $n+1$ times differentiable at $x_0$ and $f^{(n+1)}(x_0) \neq 0$, then a form of the remainder in Taylor's Formula is supposedly $$r_n(x_0;x) = \frac{f^{(n)}\big(x_0 +\theta(x-x_0)\big)}{n!}(x-x_0)^n,$$ where $ 0< \theta < 1$ and $\theta = \theta(x)$ approaches $\frac{1}{n+1}$ as $x$ approaches $x_0.$ How would one derive this? This seems quite strange to me. For example, taking $f(x) =e^x$, the normal Taylor expansion for $n=2$ at $0$ would have the form  (Lagrange Remainder): $$e^x = 1 + x + \frac12x^2 +\frac{1}{3!}e^{\xi}x^3,$$ but in this case it is $$e^x = 1 + x + \frac12x^2 +\frac{1}{2!}e^{\theta(x)}x^2$$. The properties of $\theta$ do not make much sense. Am I missing something?","If the function $f: \mathbb R \to \mathbb R$ is $n+1$ times differentiable at $x_0$ and $f^{(n+1)}(x_0) \neq 0$, then a form of the remainder in Taylor's Formula is supposedly $$r_n(x_0;x) = \frac{f^{(n)}\big(x_0 +\theta(x-x_0)\big)}{n!}(x-x_0)^n,$$ where $ 0< \theta < 1$ and $\theta = \theta(x)$ approaches $\frac{1}{n+1}$ as $x$ approaches $x_0.$ How would one derive this? This seems quite strange to me. For example, taking $f(x) =e^x$, the normal Taylor expansion for $n=2$ at $0$ would have the form  (Lagrange Remainder): $$e^x = 1 + x + \frac12x^2 +\frac{1}{3!}e^{\xi}x^3,$$ but in this case it is $$e^x = 1 + x + \frac12x^2 +\frac{1}{2!}e^{\theta(x)}x^2$$. The properties of $\theta$ do not make much sense. Am I missing something?",,"['analysis', 'taylor-expansion']"
48,Rudin Theorem 2.41 - Heine-Borel Theorem,Rudin Theorem 2.41 - Heine-Borel Theorem,,"When proving Theorem 2.41 in Principles of Mathematical Analysis : Let $E \subset \mathbb{R}^k$. If every infinite subset of $E$ has a limit point in $E$, then $E$ is closed and bounded. Rudin says, ""If $E$ is not bounded then $E$ contains points $x_n$ with $$\vert x_n \vert > n \hspace{2cm} (n = 1,2,3,...).$$ The set $S$ consisting of these points $x_n$ is infinite and clearly has no limit point in $\mathbb{R}^k$, hence has none in $E$."" My first reaction to this was that it is indeed obvious as $S$ is composed of discrete points so I could find a ball around any given point not containing a point of $S$. So, if $p \in \mathbb{R}^k$ is the point I'm constructing a ball around, taking the radius of the ball as $r = \min_{x_n \neq p \in S} \{ \vert x_n - p \vert \}$ should work and as $p$ was arbitrary $S$ doesn't have a limit point. However, I'm not sure if I can take the minimum over an infinite set? I honestly don't know why I shouldn't be able to, but nonetheless I decided to try another proof: Suppose that $S$ has a limit point $p \in \mathbb{R}^k$. Then for any $r > 0$ $B_r(p)$ has infinitely many points of $S$. So in particular, $B_{\frac{1}{2}}(p)$ contains infinitely many points of $S$. Therefore, there exists a point $y_m \in B_{\frac{1}{2}}(p)$ with $\vert y_m \vert > m > \vert p \vert + 1$ (as otherwise, there would be only finitely many points of $S$ in $B_{\frac{1}{2}}(p)$). Therefore, we have $$\vert y_m - p \vert \geq \big\vert \vert y_m \vert - \vert p \vert \big\vert > 1 $$ However, this is a contradition as we also have $\vert y_m - p \vert < \frac{1}{2}$. Therefore, as $p$ was arbitrary, $S$ has no limit point in $\mathbb{R}^k$. My questions : Can I take the min over an infinite set as in my first argument? Is my second argument okay? I appreciate any answers. Thanks.","When proving Theorem 2.41 in Principles of Mathematical Analysis : Let $E \subset \mathbb{R}^k$. If every infinite subset of $E$ has a limit point in $E$, then $E$ is closed and bounded. Rudin says, ""If $E$ is not bounded then $E$ contains points $x_n$ with $$\vert x_n \vert > n \hspace{2cm} (n = 1,2,3,...).$$ The set $S$ consisting of these points $x_n$ is infinite and clearly has no limit point in $\mathbb{R}^k$, hence has none in $E$."" My first reaction to this was that it is indeed obvious as $S$ is composed of discrete points so I could find a ball around any given point not containing a point of $S$. So, if $p \in \mathbb{R}^k$ is the point I'm constructing a ball around, taking the radius of the ball as $r = \min_{x_n \neq p \in S} \{ \vert x_n - p \vert \}$ should work and as $p$ was arbitrary $S$ doesn't have a limit point. However, I'm not sure if I can take the minimum over an infinite set? I honestly don't know why I shouldn't be able to, but nonetheless I decided to try another proof: Suppose that $S$ has a limit point $p \in \mathbb{R}^k$. Then for any $r > 0$ $B_r(p)$ has infinitely many points of $S$. So in particular, $B_{\frac{1}{2}}(p)$ contains infinitely many points of $S$. Therefore, there exists a point $y_m \in B_{\frac{1}{2}}(p)$ with $\vert y_m \vert > m > \vert p \vert + 1$ (as otherwise, there would be only finitely many points of $S$ in $B_{\frac{1}{2}}(p)$). Therefore, we have $$\vert y_m - p \vert \geq \big\vert \vert y_m \vert - \vert p \vert \big\vert > 1 $$ However, this is a contradition as we also have $\vert y_m - p \vert < \frac{1}{2}$. Therefore, as $p$ was arbitrary, $S$ has no limit point in $\mathbb{R}^k$. My questions : Can I take the min over an infinite set as in my first argument? Is my second argument okay? I appreciate any answers. Thanks.",,[]
49,differential inverse matrix,differential inverse matrix,,"How to show that the application $$f:U=GL(\mathbb{R}^{2})\subset\mathbb{R}^{n^{2}}\longrightarrow \mathbb{R}^{n^{2}},$$ defined by $$f(A)=A^{-1}$$ is differentiable and and its derivative at point $A\in U$ is the linear transformation $$f'(A):\mathbb{R}^{n^{2}}\longrightarrow \mathbb{R}^{n^{2}},$$ defined by $f'(A)\cdot V=-A\cdot V\cdot A^{-1}$.","How to show that the application $$f:U=GL(\mathbb{R}^{2})\subset\mathbb{R}^{n^{2}}\longrightarrow \mathbb{R}^{n^{2}},$$ defined by $$f(A)=A^{-1}$$ is differentiable and and its derivative at point $A\in U$ is the linear transformation $$f'(A):\mathbb{R}^{n^{2}}\longrightarrow \mathbb{R}^{n^{2}},$$ defined by $f'(A)\cdot V=-A\cdot V\cdot A^{-1}$.",,"['analysis', 'multivariable-calculus']"
50,What's a measure valued solution of a PDE?,What's a measure valued solution of a PDE?,,"What's a measure valued solution of a PDE? For instance the Fokker-Planck equation \begin{align} \partial_t\mu_t+\sum_i\partial_i(b_i\mu_t)-\frac{1}{2}\sum_{ij}\partial_{ij}(a_{ij}\mu_t)=0 \end{align} it says that for a measure $\mu=\mu(t,x)=\mu_t(x)$ being a solution of the above equation means \begin{align} \frac{d}{dt}\int_{\mathbb{R}^N}\phi(x)d\mu_t(x)=\int_{\mathbb{R}^N}\left(\sum_ib_i(t,x)\partial_i\phi(x)+\frac{1}{2}\sum_{ij}a_{ij}(t,x)\partial_{ij}\phi(x)\right)d\mu_t(x) \end{align} How do we get this? Also if compare this with the weak formulation, What's the connection between a measure valued solution and a distributional solution?","What's a measure valued solution of a PDE? For instance the Fokker-Planck equation it says that for a measure being a solution of the above equation means How do we get this? Also if compare this with the weak formulation, What's the connection between a measure valued solution and a distributional solution?","\begin{align}
\partial_t\mu_t+\sum_i\partial_i(b_i\mu_t)-\frac{1}{2}\sum_{ij}\partial_{ij}(a_{ij}\mu_t)=0
\end{align} \mu=\mu(t,x)=\mu_t(x) \begin{align}
\frac{d}{dt}\int_{\mathbb{R}^N}\phi(x)d\mu_t(x)=\int_{\mathbb{R}^N}\left(\sum_ib_i(t,x)\partial_i\phi(x)+\frac{1}{2}\sum_{ij}a_{ij}(t,x)\partial_{ij}\phi(x)\right)d\mu_t(x)
\end{align}","['analysis', 'measure-theory', 'partial-differential-equations']"
51,Recurrence relations on a continuous domain,Recurrence relations on a continuous domain,,"While attempting to read Shannon's paper I came across the following (p. 3): suppose $N\colon \mathbb{R} \to \mathbb{R}$ is a function, which for some fixed (given) set of values $t_1, t_2, \dots, t_n$ satisfies $$ N(t) = N(t-t_1) + N(t-t_2) + \dots + N(t-t_n). \quad \quad \quad (1)$$ Then, he says, ""according to a well-known result in finite differences"", $N(t)$  is asymptotic for large $t$ to $X_0^t$ where $X_0$ is the largest real solution of the characteristic equation $$ X^{-t_1} + X^{-t_2} + \dots + X^{-t_n} = 1. \quad \quad \quad (2)$$ My question: what is this well-known theory, and where can I read about it? (If the proof is short enough to outline here in the space of a math.SE answer that would be great too, of course!) What sort of an equation is $(2)$, anyway? [Some fuzzy attempts follow.] I could not prove this, but I can non-rigorously find the result plausible: for instance, if we guess that $N(t)$ is of the form $X^t$ (or even $c X^t$) for some $X$, then plugging in $N(t) = c X^t$ into the equation $(1)$ gives  $$ cX^{t} = cX^{t-t_1} + cX^{t-t_2} + \dots + cX^{t-t_n}$$ so dividing by $cX^t$ we get equation $(2)$, $$ 1 = X^{-t_1} + X^{-t_2} + \dots + X^{-t_n}$$ And of course for any solution $X$ of the above equation and for any constant $c$, we can take $N(t) = cX^t$. Also linear combinations of solutions to $(1)$ are also solutions, so $N(t)$ could even be of the form  $$N(t) = c_0X_0^t + c_1X_1^t + \dots$$ in which asymptotically only the largest solution $X_0$ matters. That is, $$\lim_{t\to\infty}\frac{N(t)}{X_0^t} = \lim_{t\to\infty}\left(c_0 + \frac{c_1X_1^t}{X_0^t} + \dots \right)= c_0.$$ Now if were working with recurrence relations over the integers ‚Äî¬†$N\colon \mathbb{Z}\to\mathbb{R}$, say, with all the $t_i$ being integers, especially if they are integers $1$ to $n$ ‚Äî¬†then I guess we could further say that we have found an $n$-dimensional space of solutions (here $n$ being the number of solutions to equation $(2)$), and the solution space also is of dimension $n$ (it has $n$ ""degrees of freedom"" because the first $n$ values determine the sequence ‚Äî¬†all this needs to be made more rigorous and to consider more general $t_i$), and therefore these must be all the solutions. That would complete the ""proof"". But in this real-number case I'm completely clueless how one would prove a thing like this, and everything I wrote above may be entirely the wrong tack to pursue.","While attempting to read Shannon's paper I came across the following (p. 3): suppose $N\colon \mathbb{R} \to \mathbb{R}$ is a function, which for some fixed (given) set of values $t_1, t_2, \dots, t_n$ satisfies $$ N(t) = N(t-t_1) + N(t-t_2) + \dots + N(t-t_n). \quad \quad \quad (1)$$ Then, he says, ""according to a well-known result in finite differences"", $N(t)$  is asymptotic for large $t$ to $X_0^t$ where $X_0$ is the largest real solution of the characteristic equation $$ X^{-t_1} + X^{-t_2} + \dots + X^{-t_n} = 1. \quad \quad \quad (2)$$ My question: what is this well-known theory, and where can I read about it? (If the proof is short enough to outline here in the space of a math.SE answer that would be great too, of course!) What sort of an equation is $(2)$, anyway? [Some fuzzy attempts follow.] I could not prove this, but I can non-rigorously find the result plausible: for instance, if we guess that $N(t)$ is of the form $X^t$ (or even $c X^t$) for some $X$, then plugging in $N(t) = c X^t$ into the equation $(1)$ gives  $$ cX^{t} = cX^{t-t_1} + cX^{t-t_2} + \dots + cX^{t-t_n}$$ so dividing by $cX^t$ we get equation $(2)$, $$ 1 = X^{-t_1} + X^{-t_2} + \dots + X^{-t_n}$$ And of course for any solution $X$ of the above equation and for any constant $c$, we can take $N(t) = cX^t$. Also linear combinations of solutions to $(1)$ are also solutions, so $N(t)$ could even be of the form  $$N(t) = c_0X_0^t + c_1X_1^t + \dots$$ in which asymptotically only the largest solution $X_0$ matters. That is, $$\lim_{t\to\infty}\frac{N(t)}{X_0^t} = \lim_{t\to\infty}\left(c_0 + \frac{c_1X_1^t}{X_0^t} + \dots \right)= c_0.$$ Now if were working with recurrence relations over the integers ‚Äî¬†$N\colon \mathbb{Z}\to\mathbb{R}$, say, with all the $t_i$ being integers, especially if they are integers $1$ to $n$ ‚Äî¬†then I guess we could further say that we have found an $n$-dimensional space of solutions (here $n$ being the number of solutions to equation $(2)$), and the solution space also is of dimension $n$ (it has $n$ ""degrees of freedom"" because the first $n$ values determine the sequence ‚Äî¬†all this needs to be made more rigorous and to consider more general $t_i$), and therefore these must be all the solutions. That would complete the ""proof"". But in this real-number case I'm completely clueless how one would prove a thing like this, and everything I wrote above may be entirely the wrong tack to pursue.",,"['analysis', 'reference-request', 'recurrence-relations', 'functional-equations', 'finite-differences']"
52,Motivation behind Borel $\sigma$ Algebras,Motivation behind Borel  Algebras,\sigma,"I understand what a Borel sigma algebra is, I am just not sure why we have the motivation to find such a sigma algebra. It seems clear to me that the power set would satisfy everything I have learned about thus far. At the moment I try to gather some intuition from topology. If we look at the box topology and the product topology on an infinite set. It turns out that the box topology simply has too many open sets in it for it to describe anything in a useful way. I assume that somehow the power set simply has too many open sets in it for it be useful. To borrow a term from topology the Borel sigma algebra is defined in such a way that it is the coarsest sigma algebra that contains the things we are interested in studying my question is WHY do we need it.","I understand what a Borel sigma algebra is, I am just not sure why we have the motivation to find such a sigma algebra. It seems clear to me that the power set would satisfy everything I have learned about thus far. At the moment I try to gather some intuition from topology. If we look at the box topology and the product topology on an infinite set. It turns out that the box topology simply has too many open sets in it for it to describe anything in a useful way. I assume that somehow the power set simply has too many open sets in it for it be useful. To borrow a term from topology the Borel sigma algebra is defined in such a way that it is the coarsest sigma algebra that contains the things we are interested in studying my question is WHY do we need it.",,"['analysis', 'measure-theory']"
53,Minimisation of Energy Functional,Minimisation of Energy Functional,,"Suppose I have the energy functional given by $$\Pi(u) : = \frac{1}{2} \int_{\Omega} k\nabla u \cdot \nabla u dx - \int_{\Omega} fu dx + \int_{\partial \Omega} \sigma_0 u^2 ds,$$ where $f \in L^2(\Omega)$ and $\sigma_0 \in C^{\infty}(\partial \Omega)$ and $\partial \Omega$ denotes the boundary of $\Omega$. Also note that $\sigma_0 > 0$. How do I show this energy functional attains a minimum in $H^1(U)$? According to Evan's, we need to show that $\Pi(u)$ is coercive and convex, but I haven't had any luck.","Suppose I have the energy functional given by $$\Pi(u) : = \frac{1}{2} \int_{\Omega} k\nabla u \cdot \nabla u dx - \int_{\Omega} fu dx + \int_{\partial \Omega} \sigma_0 u^2 ds,$$ where $f \in L^2(\Omega)$ and $\sigma_0 \in C^{\infty}(\partial \Omega)$ and $\partial \Omega$ denotes the boundary of $\Omega$. Also note that $\sigma_0 > 0$. How do I show this energy functional attains a minimum in $H^1(U)$? According to Evan's, we need to show that $\Pi(u)$ is coercive and convex, but I haven't had any luck.",,['analysis']
54,Lipschitz continuous Gradient and bounded Hessian,Lipschitz continuous Gradient and bounded Hessian,,"Is the following proposition true? Is there a simple proof? Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a twice continuously differentiable function. If there exists $M$ such that $||\nabla^2 f(x)||_2 \leq M$ for all $x$ then  $$ \forall x, y \in \mathbb{R}^n: \quad ||\nabla f(x) - \nabla f(y)||_2 \leq M ||x - y||_2 $$ Update I believe I was able to prove it. Please tell me if I am wrong here, and where. First, we remember that any real symmetric matrix $H$ is diagonalizable $H = P^T \Lambda P$, and therefore: $$ u^T H u = (P u)^T \Lambda (P u) \leq \lambda_H ||P u||^2 = \lambda_H ||u||^2 $$ where  $$\lambda_H = \max_{i = 1, \dots, n} \{ |\lambda_1(H)|, \dots, |\lambda_n(H)| \}$$ Also, since $||H||_2^2 = \lambda_{max}(H^T H)$ and we know that $H^T H = P^T \Lambda^2 P$, then we also know that $||H||_2 = \lambda_H$ and therefore: $$ u^T H u \leq ||H||_2 ||u||^2 $$ Now, using the multivariate first order Taylor approximation, we have: $$ \begin{aligned} f(x) - f(y) = \nabla f(y)^T (x - y) + 0.5 (y - z)^T \nabla^2 f(z) (y - z) \\ f(y) - f(x) = -\nabla f(x)^T (x - y) + 0.5 (x - w)^T \nabla^2 f(w) (x - w) \end{aligned} $$ where $z \in [x, y]$ and $w \in [x, y]$. Adding both equations, and re-arranging, we have: $$ \begin{aligned} (\nabla f(y) - \nabla f(x))^T(x - y)   &=  -0.5 (y - z)^T \nabla^2 f(z) (y - z) - 0.5 (x - w)^T \nabla^2 f(w) (x - w) \\  &\geq -0.5 ||\nabla^2 f(z)||_2 ||y - z||^2 - 0.5 ||\nabla^2 f(w)||_2 ||x - w||^2 \\  &\geq -0.5 M ||y - z||^2 - 0.5 M ||x - w||^2 \\  &\geq -M ||x - y||^2 \end{aligned} $$ Using Cauchy-Schwartz we also obtain $$ - ||\nabla f(y) - \nabla f(x)|| \cdot ||y - x|| \geq (\nabla f(y) - \nabla f(x))^T(x - y) \geq -M ||x - y||^2 $$ Dividing both sides by $-||y - x||$ gives the desired result.","Is the following proposition true? Is there a simple proof? Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a twice continuously differentiable function. If there exists $M$ such that $||\nabla^2 f(x)||_2 \leq M$ for all $x$ then  $$ \forall x, y \in \mathbb{R}^n: \quad ||\nabla f(x) - \nabla f(y)||_2 \leq M ||x - y||_2 $$ Update I believe I was able to prove it. Please tell me if I am wrong here, and where. First, we remember that any real symmetric matrix $H$ is diagonalizable $H = P^T \Lambda P$, and therefore: $$ u^T H u = (P u)^T \Lambda (P u) \leq \lambda_H ||P u||^2 = \lambda_H ||u||^2 $$ where  $$\lambda_H = \max_{i = 1, \dots, n} \{ |\lambda_1(H)|, \dots, |\lambda_n(H)| \}$$ Also, since $||H||_2^2 = \lambda_{max}(H^T H)$ and we know that $H^T H = P^T \Lambda^2 P$, then we also know that $||H||_2 = \lambda_H$ and therefore: $$ u^T H u \leq ||H||_2 ||u||^2 $$ Now, using the multivariate first order Taylor approximation, we have: $$ \begin{aligned} f(x) - f(y) = \nabla f(y)^T (x - y) + 0.5 (y - z)^T \nabla^2 f(z) (y - z) \\ f(y) - f(x) = -\nabla f(x)^T (x - y) + 0.5 (x - w)^T \nabla^2 f(w) (x - w) \end{aligned} $$ where $z \in [x, y]$ and $w \in [x, y]$. Adding both equations, and re-arranging, we have: $$ \begin{aligned} (\nabla f(y) - \nabla f(x))^T(x - y)   &=  -0.5 (y - z)^T \nabla^2 f(z) (y - z) - 0.5 (x - w)^T \nabla^2 f(w) (x - w) \\  &\geq -0.5 ||\nabla^2 f(z)||_2 ||y - z||^2 - 0.5 ||\nabla^2 f(w)||_2 ||x - w||^2 \\  &\geq -0.5 M ||y - z||^2 - 0.5 M ||x - w||^2 \\  &\geq -M ||x - y||^2 \end{aligned} $$ Using Cauchy-Schwartz we also obtain $$ - ||\nabla f(y) - \nabla f(x)|| \cdot ||y - x|| \geq (\nabla f(y) - \nabla f(x))^T(x - y) \geq -M ||x - y||^2 $$ Dividing both sides by $-||y - x||$ gives the desired result.",,"['analysis', 'lipschitz-functions']"
55,Nonstandard complex numbers and categoricity,Nonstandard complex numbers and categoricity,,"Let ${}^*\mathbb{C}$ be a nonstandard complex number field (given, for instance, as a countable ultrapower.) By the transfer principle ${}^*\mathbb{C}$ is algebraically closed of characteristic zero, and by the construction as a quotient of $\mathbb{C}^\mathbb{N}$ we see it's of cardinality $\mathfrak{c}$. The theory of algebraically closed fields of a fixed characteristic is categorical, so this shows ${}^*\mathbb{C}$ is isomorphic to $\mathbb{C}$ in the category of fields. I'm trying to understand how to interpret this fact in terms of the nonstandardness of ${}^*\mathbb{C}$, namely that $\exists x\in {}^*\mathbb{C} \forall r\in\mathbb{R} x \bar x<r$. Question: Am I reading the above correctly to imply that there exists a hyperreal-valued ""absolute value"" on $\mathbb{C}$ which takes on infinitesimal, standard, and infinite values? This seems impossible, because the absolute value would have to be infinite, finite, or infinitesimal on real lines in $\mathbb{C}$, and then the triangle inequality would close, for instance, the infinitesimal part under sums. Would we just get a strange decomposition of the plane into three unions of lines, according to which piece of ${}^*\mathbb{R}$ our absolute value fell into? It remains unclear to me that such a decomposition is possible.","Let ${}^*\mathbb{C}$ be a nonstandard complex number field (given, for instance, as a countable ultrapower.) By the transfer principle ${}^*\mathbb{C}$ is algebraically closed of characteristic zero, and by the construction as a quotient of $\mathbb{C}^\mathbb{N}$ we see it's of cardinality $\mathfrak{c}$. The theory of algebraically closed fields of a fixed characteristic is categorical, so this shows ${}^*\mathbb{C}$ is isomorphic to $\mathbb{C}$ in the category of fields. I'm trying to understand how to interpret this fact in terms of the nonstandardness of ${}^*\mathbb{C}$, namely that $\exists x\in {}^*\mathbb{C} \forall r\in\mathbb{R} x \bar x<r$. Question: Am I reading the above correctly to imply that there exists a hyperreal-valued ""absolute value"" on $\mathbb{C}$ which takes on infinitesimal, standard, and infinite values? This seems impossible, because the absolute value would have to be infinite, finite, or infinitesimal on real lines in $\mathbb{C}$, and then the triangle inequality would close, for instance, the infinitesimal part under sums. Would we just get a strange decomposition of the plane into three unions of lines, according to which piece of ${}^*\mathbb{R}$ our absolute value fell into? It remains unclear to me that such a decomposition is possible.",,"['analysis', 'model-theory', 'nonstandard-analysis']"
56,Is Euler's Introductio in analysin infinitorum suitable for studying analysis today?,Is Euler's Introductio in analysin infinitorum suitable for studying analysis today?,,"I've read the following quote on Wanner's Analysis by Its History : ... our students of mathematics would profit much more from a study of Euler's Introductio in analysin infinitorum , rather than of the available modern textbooks. (Andr√© Weil, 1979; quoted by J.D. Blanton, 1988, p. xii) I got the mentioned book (there is a translated version published by Springer ) and it seems a nice read. The translator mentions in the preface that the standard analysis courses puts low emphasis in the ordinary treatise of the elements of algebra and also that he fixes  this defect. My concern at the moment is that the book may be dated but Andr√© Weil said it's a worthy read, I'd like to know if someone already read Euler's book and some modern introduction to analysis to make a fair comparison. It's important to notice that although the book is a translation, the translator made some edits in several parts of the book, I guess that with the intention of making it a readable piece for today's needs.","I've read the following quote on Wanner's Analysis by Its History : ... our students of mathematics would profit much more from a study of Euler's Introductio in analysin infinitorum , rather than of the available modern textbooks. (Andr√© Weil, 1979; quoted by J.D. Blanton, 1988, p. xii) I got the mentioned book (there is a translated version published by Springer ) and it seems a nice read. The translator mentions in the preface that the standard analysis courses puts low emphasis in the ordinary treatise of the elements of algebra and also that he fixes  this defect. My concern at the moment is that the book may be dated but Andr√© Weil said it's a worthy read, I'd like to know if someone already read Euler's book and some modern introduction to analysis to make a fair comparison. It's important to notice that although the book is a translation, the translator made some edits in several parts of the book, I guess that with the intention of making it a readable piece for today's needs.",,"['analysis', 'reference-request', 'soft-question', 'math-history', 'infinitesimals']"
57,Application of Weierstrass theorem,Application of Weierstrass theorem,,"Let f be a continuously differentiable function on $[a.b]$. Show that there is a sequence of polynomials $\{P_n\}$ such that $P_n(x) \to f$ and $P'_n(x) \to f' (x)$ uniformly on $[a,b]$ My approach has been as follows.  Since f is continuously differentiable, we have $Q_n(x) \ to f'(x)$ on$ [a,b]$ uniformly (Weierstrass) . I'm not sure how to find a polynomial that converges to f uniformly who's derivative is $Q(x)$. My first thought was $P(x)=\int_{a}^{x} Q(t)dt$. There is a relationship between uniform continuity and differentiation, but that requires convergence at a point. I'm having problems building a good argument for this part of the proof.","Let f be a continuously differentiable function on $[a.b]$. Show that there is a sequence of polynomials $\{P_n\}$ such that $P_n(x) \to f$ and $P'_n(x) \to f' (x)$ uniformly on $[a,b]$ My approach has been as follows.  Since f is continuously differentiable, we have $Q_n(x) \ to f'(x)$ on$ [a,b]$ uniformly (Weierstrass) . I'm not sure how to find a polynomial that converges to f uniformly who's derivative is $Q(x)$. My first thought was $P(x)=\int_{a}^{x} Q(t)dt$. There is a relationship between uniform continuity and differentiation, but that requires convergence at a point. I'm having problems building a good argument for this part of the proof.",,['analysis']
58,Finding $a$ such that $x^a \cdot \sin(1/x)$ is uniformly continuous.,Finding  such that  is uniformly continuous.,a x^a \cdot \sin(1/x),"Assuming that $\sin x$ is continuous on $\mathbb R$, find all real $\alpha$ such that $x^\alpha\sin (1/x)$ is uniformly continuous on the open interval (0,1). I'm guessing that I need to show that $x^\alpha\sin x$ is continuously extendable to [0,1]. Doing that for $x=1$ is pretty trivial, but I am having trouble doing that for $x=0$. I believe that the $\lim_{x\to 0}x^\alpha\sin (1/x)=0$, but how can I find what $f(0)$ equals? I would appreciate any guidance! Thanks for your help in advance.","Assuming that $\sin x$ is continuous on $\mathbb R$, find all real $\alpha$ such that $x^\alpha\sin (1/x)$ is uniformly continuous on the open interval (0,1). I'm guessing that I need to show that $x^\alpha\sin x$ is continuously extendable to [0,1]. Doing that for $x=1$ is pretty trivial, but I am having trouble doing that for $x=0$. I believe that the $\lim_{x\to 0}x^\alpha\sin (1/x)=0$, but how can I find what $f(0)$ equals? I would appreciate any guidance! Thanks for your help in advance.",,"['analysis', 'continuity']"
59,On a double sum involving prime numbers,On a double sum involving prime numbers,,"$$\sum_{i,j=1}^{\infty}\left[\frac{x}{p_ip_j}\right]=x\sum_{p_ip_j\leq x}\frac{1}{p_ip_j}+O(x),p_i< p_j$$   where $p_i$ is the $i$th prime, and ""[ ]"" represents the largest integer not exceeding... I don't know how to deal with it. Could you give me a proof?","$$\sum_{i,j=1}^{\infty}\left[\frac{x}{p_ip_j}\right]=x\sum_{p_ip_j\leq x}\frac{1}{p_ip_j}+O(x),p_i< p_j$$   where $p_i$ is the $i$th prime, and ""[ ]"" represents the largest integer not exceeding... I don't know how to deal with it. Could you give me a proof?",,"['number-theory', 'analysis', 'analytic-number-theory']"
60,How do I approach mathematical proofs involving nested if-then statements?,How do I approach mathematical proofs involving nested if-then statements?,,"I'm new to proofs and trying to prove the following statement. I already have trouble writing the ""automatic parts"" of the proof at the beginning because I'm not sure what assumptions I can and should make given the nested if-then statements. I am finding that particularly confusing. Is there an algorithmic strategic approach to generally proving nested if-then-statements so that it's easier to see what should be defined and assumed in the beginning? Here is the statement: Let $A,B$ be sets and let $f: A \rightarrow B$ be a function. If, for all $W,X \subseteq A$ , if $f(W) \subseteq f(X)$ , then $W \subseteq X$ , then $f$ is injective. I try breaking this down to the general form $(P \Rightarrow Q)\Rightarrow R$ such that P: ""for all $W,X \subseteq A$ , $f(W) \subseteq f(X)$ "" Q: "" $W \subseteq X$ "" R: "" $f$ is injective"" My proof is then as follows (line-for-line): (1) Suppose for all $W,X \subseteq A$ , if $f(W) \subseteq f(X)$ , then $W \subseteq X$ . (2) Suppose $f(W) \subseteq f(X)$ . (3) Let $y \in f(W)$ . (4) Then, for all $y \in f(W)$ , there exists $w \in W$ such that $f(w) = y$ . (5) Since $f(W) \subseteq f(X)$ , it follows that $y \in f(X)$ as well. (6) So, there exists $x \in X$ such that $f(x) = y$ . (7) Hence, $f(w)=f(x)$ . (8) Based on the initial assumptions, from $f(W) \subseteq f(X)$ follows that $W \subseteq X$ . (9) So, $w \in X$ . So, I have $f(w)=f(x)$ and $x,w \in X$ . Since I'm trying to prove that $f$ is injective, I need to show that $x=w$ but I'm not seeing how to explicitly show this. My suspicion is that I might've already messed up at the very beginning because the nested if-then statements tend to give me trouble. Any help would be greatly appreciated. EDIT Thanks to everyone for suggestions and corrections. I believe that I now have a decent proof. Here again the statement: Let $A,B$ be sets and let $f: A \rightarrow B$ be a function. If, for all $W,X \subseteq A$ , if $f(W) \subseteq f(X)$ , then $W \subseteq X$ , then $f$ is injective. And the proof: (1) Suppose for all $W, X \subseteq A$ , if $f(W) \subseteq f(X)$ , then $W \subseteq X$ . (2) Let $x, w \in A$ and suppose $f(x) = f(w)$ . (3) Let $W = \{w\}$ and $X = \{x\}$ . (4) Note that $f(W)=f(\{w\})=\{f(w)\}$ and $f(X)=f(\{x\})=\{f(x)\}$ . (5) Since $f(w) = f(x)$ , it follows that $\{f(w)\} \subseteq \{f(x)\}$ , i.e., $f(W) \subseteq f(X)$ . (6) So, by the initial assumption, $W \subseteq X$ , i.e., $\{w\} \subseteq \{x\}$ . (7) Hence, $w = x$ and so $f$ is injective. $\square$","I'm new to proofs and trying to prove the following statement. I already have trouble writing the ""automatic parts"" of the proof at the beginning because I'm not sure what assumptions I can and should make given the nested if-then statements. I am finding that particularly confusing. Is there an algorithmic strategic approach to generally proving nested if-then-statements so that it's easier to see what should be defined and assumed in the beginning? Here is the statement: Let be sets and let be a function. If, for all , if , then , then is injective. I try breaking this down to the general form such that P: ""for all , "" Q: "" "" R: "" is injective"" My proof is then as follows (line-for-line): (1) Suppose for all , if , then . (2) Suppose . (3) Let . (4) Then, for all , there exists such that . (5) Since , it follows that as well. (6) So, there exists such that . (7) Hence, . (8) Based on the initial assumptions, from follows that . (9) So, . So, I have and . Since I'm trying to prove that is injective, I need to show that but I'm not seeing how to explicitly show this. My suspicion is that I might've already messed up at the very beginning because the nested if-then statements tend to give me trouble. Any help would be greatly appreciated. EDIT Thanks to everyone for suggestions and corrections. I believe that I now have a decent proof. Here again the statement: Let be sets and let be a function. If, for all , if , then , then is injective. And the proof: (1) Suppose for all , if , then . (2) Let and suppose . (3) Let and . (4) Note that and . (5) Since , it follows that , i.e., . (6) So, by the initial assumption, , i.e., . (7) Hence, and so is injective.","A,B f: A \rightarrow B W,X \subseteq A f(W) \subseteq f(X) W \subseteq X f (P \Rightarrow Q)\Rightarrow R W,X \subseteq A f(W) \subseteq f(X) W \subseteq X f W,X \subseteq A f(W) \subseteq f(X) W \subseteq X f(W) \subseteq f(X) y \in f(W) y \in f(W) w \in W f(w) = y f(W) \subseteq f(X) y \in f(X) x \in X f(x) = y f(w)=f(x) f(W) \subseteq f(X) W \subseteq X w \in X f(w)=f(x) x,w \in X f x=w A,B f: A \rightarrow B W,X \subseteq A f(W) \subseteq f(X) W \subseteq X f W, X \subseteq A f(W) \subseteq f(X) W \subseteq X x, w \in A f(x) = f(w) W = \{w\} X = \{x\} f(W)=f(\{w\})=\{f(w)\} f(X)=f(\{x\})=\{f(x)\} f(w) = f(x) \{f(w)\} \subseteq \{f(x)\} f(W) \subseteq f(X) W \subseteq X \{w\} \subseteq \{x\} w = x f \square","['analysis', 'logic', 'proof-writing']"
61,Uncountable unions of nested sets in a sigma field,Uncountable unions of nested sets in a sigma field,,"I know that in a $\sigma$-algebra, uncountable unions may not exist. However suppose I have a directed system $\{A_i,i\in I\}$ where for each $i\in I$, $A_i\in\mathcal A$ (the $\sigma$-algebra) with the property that for any $i,j\in I$, we have $A_i\subseteq A_j$ or $A_j\subseteq A_i$. Then I believe $\cup_{i\in I}A_i\in\mathcal A$ but I am having difficulty in writing a proof. Notice here that $I$ may be uncountable.","I know that in a $\sigma$-algebra, uncountable unions may not exist. However suppose I have a directed system $\{A_i,i\in I\}$ where for each $i\in I$, $A_i\in\mathcal A$ (the $\sigma$-algebra) with the property that for any $i,j\in I$, we have $A_i\subseteq A_j$ or $A_j\subseteq A_i$. Then I believe $\cup_{i\in I}A_i\in\mathcal A$ but I am having difficulty in writing a proof. Notice here that $I$ may be uncountable.",,"['analysis', 'measure-theory', 'elementary-set-theory']"
62,$\|D_{f}(x) v\|=\|v\|$ $\implies$ $f$ is an isometry,is an isometry,\|D_{f}(x) v\|=\|v\| \implies f,"Let $f\colon \mathbb{R}^m \to \mathbb{R}^m$ be a $C^2$ map such that $\|D_f(x)v\|=\|v\|$ for all $v\in\mathbb{R}^m$, where $D_f(x)$ is the derivative of $f$ at $x$. Then I am asked to prove that $\|f(x)-f(y)\|=\|x-y\|$ for all $x,y\in\mathbb{R}^m$. There's a hint that says to use the Schwarz theorem. I just can prove that $\|f(x)-f(y)\| \le \|x-y\|$, using the image of $\alpha \colon [0,1] \to \mathbb{R}^m$, $\alpha(t)=x+(y-x)t$. Consider $\gamma\colon [0,1] \to \mathbb{R}^m$, $\gamma(t)=f(\alpha(t))$, we have that $\gamma '(t)=D_{f}(x +(y-x)t).(y-x)$. Since $\gamma$ connect $f(x)$ and $f(y)$ and it is $C^{1}$ we have: $\|f(x)-f(y)\| \leq L(\gamma) = \displaystyle\int_{[0,1]} \|\gamma '(t)\|dt=\displaystyle\int_{[0,1]}\|D_{f}(x +(y-x)t).(y-x)\|dt$. By assumtion this is equals to $\displaystyle\int_{[0,1]} \|x-y\| \leq \|\displaystyle\int_{[0,1]} x-y\space dt\|= \|x-y\|$. I have no idea how use schwarz theorem in this case. Any help is welcome.","Let $f\colon \mathbb{R}^m \to \mathbb{R}^m$ be a $C^2$ map such that $\|D_f(x)v\|=\|v\|$ for all $v\in\mathbb{R}^m$, where $D_f(x)$ is the derivative of $f$ at $x$. Then I am asked to prove that $\|f(x)-f(y)\|=\|x-y\|$ for all $x,y\in\mathbb{R}^m$. There's a hint that says to use the Schwarz theorem. I just can prove that $\|f(x)-f(y)\| \le \|x-y\|$, using the image of $\alpha \colon [0,1] \to \mathbb{R}^m$, $\alpha(t)=x+(y-x)t$. Consider $\gamma\colon [0,1] \to \mathbb{R}^m$, $\gamma(t)=f(\alpha(t))$, we have that $\gamma '(t)=D_{f}(x +(y-x)t).(y-x)$. Since $\gamma$ connect $f(x)$ and $f(y)$ and it is $C^{1}$ we have: $\|f(x)-f(y)\| \leq L(\gamma) = \displaystyle\int_{[0,1]} \|\gamma '(t)\|dt=\displaystyle\int_{[0,1]}\|D_{f}(x +(y-x)t).(y-x)\|dt$. By assumtion this is equals to $\displaystyle\int_{[0,1]} \|x-y\| \leq \|\displaystyle\int_{[0,1]} x-y\space dt\|= \|x-y\|$. I have no idea how use schwarz theorem in this case. Any help is welcome.",,['analysis']
63,What is duality argument for the operator on $L^p-$ spaces?,What is duality argument for the operator on  spaces?,L^p-,"Suppose that the operator  $T: L^{p}(\mathbb R) \to  L^{p}(\mathbb R)$ (say for instance, some nice convolution operator)  is bounded for $1\leq p \leq  2.$ At various, places we see that (for instance: in the proof of Hilbert transform is bounded on $1<p<\infty$ ), if the operator is bounded on for the range $1<p<2,$ then the operator  is bounded on $L^{p}$  for $p>2$ by duality argument. My Vague Question is: What is the standard duality argument in these kind of situations?   Can  you illustrate some specific example?","Suppose that the operator  $T: L^{p}(\mathbb R) \to  L^{p}(\mathbb R)$ (say for instance, some nice convolution operator)  is bounded for $1\leq p \leq  2.$ At various, places we see that (for instance: in the proof of Hilbert transform is bounded on $1<p<\infty$ ), if the operator is bounded on for the range $1<p<2,$ then the operator  is bounded on $L^{p}$  for $p>2$ by duality argument. My Vague Question is: What is the standard duality argument in these kind of situations?   Can  you illustrate some specific example?",,"['analysis', 'fourier-analysis', 'lp-spaces', 'harmonic-analysis']"
64,An explicit construction for an everywhere discontinuous real function with $F((a+b)/2)\leq(F(a)+F(b))/2$?,An explicit construction for an everywhere discontinuous real function with ?,F((a+b)/2)\leq(F(a)+F(b))/2,"I would like to know an explicit method on constructing an everywhere discontinuous real function $F$ with the property: $$F((a+b)/2)\leq(F(a)+F(b))/2.$$ There is a non-constructive example (with the inequality been trivial): Take a Hamel basis $S$ of the $\mathbb{Q}$-linear space $\mathbb{R}$, take in $S$ a countably-infinite subset $X=\{x_1, x_2, ...\}$, then by multiplying a rational number $c_n$ to $x_n$ for each $n$ we can produce a set $Y=\{y_1, y_2, ...\}$ with $0< y_n\leq1/n$. Now we replace the $X$ in $S$ with $Y$ and obtain a new Hamel basis $T$. Take a $t_0\in T$; let $F(t_0)=0$ and let $F(t)=1$ for any other $t\in T$, then $F$ extends to a function on $\mathbb{R}$ linearly, and it is clear this is a required function. By the answer of Conifold below, such an explicit method does not exist. But it would still be nice to know how to give such a non-constructive function with the inequality been strict.","I would like to know an explicit method on constructing an everywhere discontinuous real function $F$ with the property: $$F((a+b)/2)\leq(F(a)+F(b))/2.$$ There is a non-constructive example (with the inequality been trivial): Take a Hamel basis $S$ of the $\mathbb{Q}$-linear space $\mathbb{R}$, take in $S$ a countably-infinite subset $X=\{x_1, x_2, ...\}$, then by multiplying a rational number $c_n$ to $x_n$ for each $n$ we can produce a set $Y=\{y_1, y_2, ...\}$ with $0< y_n\leq1/n$. Now we replace the $X$ in $S$ with $Y$ and obtain a new Hamel basis $T$. Take a $t_0\in T$; let $F(t_0)=0$ and let $F(t)=1$ for any other $t\in T$, then $F$ extends to a function on $\mathbb{R}$ linearly, and it is clear this is a required function. By the answer of Conifold below, such an explicit method does not exist. But it would still be nice to know how to give such a non-constructive function with the inequality been strict.",,"['analysis', 'convex-analysis', 'axiom-of-choice', 'constructive-mathematics']"
65,"Proving this function $f:[0,1]\to \mathbb{R}$ is continuous on $[0,1]$",Proving this function  is continuous on,"f:[0,1]\to \mathbb{R} [0,1]","I am looking for a hint or feedback on what I've already done, not a full solution. $f=t\sin{\left(\frac{1}{t}\right)}$ for $t\ne 0$, $f(0)=0$, My idea is that I only have to worry about the steep parts. My approach: Proof sketch: Let $\epsilon > 0 $ be arbitrary, we seek to find $\delta$ such that $\forall x,y \in [0,1]$, $|x-y|<\delta \Longrightarrow |x\sin{\left(\frac{1}{x}\right)}- y\sin{\left(\frac{1}{y}\right)}|<\epsilon$ SCRATCHWORK \begin{align*} & t\sin{\left(\frac{1}{t}\right)} = \frac{2}{\pi(4k+1)} \text{    is true for   } t =\frac{2}{\pi(4k+1)}  \\ & t\sin{\left(\frac{1}{t}\right)} = - \frac{2}{\pi(4k+3)}  \text{    is true for   } t =\frac{2}{\pi(4k+3)}  \end{align*} and  \begin{align*} &\text{distance}\left(f_1\left(\frac{2}{\pi(4k+1)}\right), f_1\left(\frac{2}{\pi(4k+3)}\right)\right) = \\ & \frac{2}{\pi(4k+1)} - \left(- \frac{2}{\pi(4k+3)} \right) =\\ & \frac{16k+8}{\pi (4k+1)(4k+3)}=\\ & \left(\frac{4}{\pi}\right)\frac{4k+2}{(4k+1)(4k+3)} & \end{align*} Define $\epsilon(k) = $ such $k$ so that  $$ \left(\frac{4}{\pi}\right)\frac{4k+2}{(4k+1)(4k+3)} < \epsilon  $$ So as long as $\delta = \frac{2}{\pi(4\epsilon(k)+1)} - \frac{2}{\pi(4\epsilon(k)+3)}= \frac{4}{\pi(4\epsilon(k)+1)(4\epsilon(k)+3)}$ we should be good?  I am trying to focus on the pre image of the steepest parts of the function and then looking at the width of that. I am pretty brain dead right now... I feel like this delta works, but I need more/better justification...","I am looking for a hint or feedback on what I've already done, not a full solution. $f=t\sin{\left(\frac{1}{t}\right)}$ for $t\ne 0$, $f(0)=0$, My idea is that I only have to worry about the steep parts. My approach: Proof sketch: Let $\epsilon > 0 $ be arbitrary, we seek to find $\delta$ such that $\forall x,y \in [0,1]$, $|x-y|<\delta \Longrightarrow |x\sin{\left(\frac{1}{x}\right)}- y\sin{\left(\frac{1}{y}\right)}|<\epsilon$ SCRATCHWORK \begin{align*} & t\sin{\left(\frac{1}{t}\right)} = \frac{2}{\pi(4k+1)} \text{    is true for   } t =\frac{2}{\pi(4k+1)}  \\ & t\sin{\left(\frac{1}{t}\right)} = - \frac{2}{\pi(4k+3)}  \text{    is true for   } t =\frac{2}{\pi(4k+3)}  \end{align*} and  \begin{align*} &\text{distance}\left(f_1\left(\frac{2}{\pi(4k+1)}\right), f_1\left(\frac{2}{\pi(4k+3)}\right)\right) = \\ & \frac{2}{\pi(4k+1)} - \left(- \frac{2}{\pi(4k+3)} \right) =\\ & \frac{16k+8}{\pi (4k+1)(4k+3)}=\\ & \left(\frac{4}{\pi}\right)\frac{4k+2}{(4k+1)(4k+3)} & \end{align*} Define $\epsilon(k) = $ such $k$ so that  $$ \left(\frac{4}{\pi}\right)\frac{4k+2}{(4k+1)(4k+3)} < \epsilon  $$ So as long as $\delta = \frac{2}{\pi(4\epsilon(k)+1)} - \frac{2}{\pi(4\epsilon(k)+3)}= \frac{4}{\pi(4\epsilon(k)+1)(4\epsilon(k)+3)}$ we should be good?  I am trying to focus on the pre image of the steepest parts of the function and then looking at the width of that. I am pretty brain dead right now... I feel like this delta works, but I need more/better justification...",,['analysis']
66,Proof to show function f satisfies Lipschitz condition when derivatives f' exist and are continuous,Proof to show function f satisfies Lipschitz condition when derivatives f' exist and are continuous,,"The question is as follows: Given a function $f$, 2 known information: (1) $f'(x)$ exist (2) $f'(x)$ are continuous Goal: function $f$ satisfies Lipschitz condition on any bounded interval $[a,b]$ Here is my attempt: 1/ Recall Lipschitz condition:  a function $f$ satisfies Lipschitz if there is a real number $N$ such that $|f(x) - f(y)| \leq N  |x - y|$ 2/ First, I plan to show this : (*) Knowing $f'(x)$ is continuous at any point $x \implies f'(x)$ is   bounded at some neighborhood about $x$ Using definition of continuity on $f'(x)$, I say that for any $\epsilon > 0$, there is $\delta > 0$ such that for any $y$, we have $|x - y| < \delta => |f'(x) - f'(y)| < \epsilon$. After some works, I realize that $f'(y)$ is in the neighborhood of $(f'(x) - \epsilon, f'(x) + \epsilon)$. So if I let my $N = max${$f'(x) + \epsilon, -f'(x) + \epsilon$}, I reach the conclusion that $f'(x)$ is bounded (at least below) 3/ Then, I plan to use (*) to say this : If function $f$ has a derivative $f'$ such that $f'$ is bounded by   some number K $ \implies f$ satisfies Lipschitz condition on any interval   [a,b] I plan to use the Mean Value Theorem, provided that by (*), there is a derivative $f'(x) < K$ where $x$ is in between some $x_1$ and $x_2$ in $[a,b]$ Would someone please check if my ideas are correct? I feel very shaky about part 2 of my work.  If the derivative is bounded, then I think the proof will be way easier.  But to conclude that continuous $\implies $ bounded, I'm not sure if I can claim such thing . Thank you in advance.","The question is as follows: Given a function $f$, 2 known information: (1) $f'(x)$ exist (2) $f'(x)$ are continuous Goal: function $f$ satisfies Lipschitz condition on any bounded interval $[a,b]$ Here is my attempt: 1/ Recall Lipschitz condition:  a function $f$ satisfies Lipschitz if there is a real number $N$ such that $|f(x) - f(y)| \leq N  |x - y|$ 2/ First, I plan to show this : (*) Knowing $f'(x)$ is continuous at any point $x \implies f'(x)$ is   bounded at some neighborhood about $x$ Using definition of continuity on $f'(x)$, I say that for any $\epsilon > 0$, there is $\delta > 0$ such that for any $y$, we have $|x - y| < \delta => |f'(x) - f'(y)| < \epsilon$. After some works, I realize that $f'(y)$ is in the neighborhood of $(f'(x) - \epsilon, f'(x) + \epsilon)$. So if I let my $N = max${$f'(x) + \epsilon, -f'(x) + \epsilon$}, I reach the conclusion that $f'(x)$ is bounded (at least below) 3/ Then, I plan to use (*) to say this : If function $f$ has a derivative $f'$ such that $f'$ is bounded by   some number K $ \implies f$ satisfies Lipschitz condition on any interval   [a,b] I plan to use the Mean Value Theorem, provided that by (*), there is a derivative $f'(x) < K$ where $x$ is in between some $x_1$ and $x_2$ in $[a,b]$ Would someone please check if my ideas are correct? I feel very shaky about part 2 of my work.  If the derivative is bounded, then I think the proof will be way easier.  But to conclude that continuous $\implies $ bounded, I'm not sure if I can claim such thing . Thank you in advance.",,['analysis']
67,Locally Lipschitz implies Lipschitz under equivalent metrics?,Locally Lipschitz implies Lipschitz under equivalent metrics?,,"Bonjour. For $i=1,2$ let $X_i$ be a non-empty set and $d_i$ a metric $X_i^2 \to \mathbb{R}$. Suppose $f$ is a locally Lipschitz (*) function $(X_1, d_1) \to (X_2, d_2)$. Question. Do there exist metrics $\delta_1: X_1^2 \to \mathbb{R}$ and $\delta_2: X_2^2 \to \mathbb{R}$ such that i) $\delta_i$ is (topologically) equivalent to $d_i$ (for $i=1,2$) and ii) $f$ is a Lipschitz function $(X_1, \delta_1) \to (X_2, \delta_2)$? (*) For each $x \in X_1$, there exists a neighborhood $U_x$ of $x$ in $(X_1, d_1)$ such that the restriction of $f$ to $U_x$ is a Lipschitz function $(U_x, d_1) \to (X_2, d_2)$.","Bonjour. For $i=1,2$ let $X_i$ be a non-empty set and $d_i$ a metric $X_i^2 \to \mathbb{R}$. Suppose $f$ is a locally Lipschitz (*) function $(X_1, d_1) \to (X_2, d_2)$. Question. Do there exist metrics $\delta_1: X_1^2 \to \mathbb{R}$ and $\delta_2: X_2^2 \to \mathbb{R}$ such that i) $\delta_i$ is (topologically) equivalent to $d_i$ (for $i=1,2$) and ii) $f$ is a Lipschitz function $(X_1, \delta_1) \to (X_2, \delta_2)$? (*) For each $x \in X_1$, there exists a neighborhood $U_x$ of $x$ in $(X_1, d_1)$ such that the restriction of $f$ to $U_x$ is a Lipschitz function $(U_x, d_1) \to (X_2, d_2)$.",,"['analysis', 'metric-spaces']"
68,Closed expression for a continued fraction,Closed expression for a continued fraction,,"Does anyone know a closed expression for the following continued fraction? $$G(x) = \cfrac{1}{x+1+\cfrac{1}{x+3+\cfrac{4}{x+5+\cfrac{9}{x+7+\cfrac{16}{x+9+\cdots}}}}}$$ All I know is that $G(0) = \frac{\pi}{4}$ and $G(x)$ converges for $x \geq 0$ , while also $G(x) \sim \frac{1}{x} \ (x \to \infty)$ . The equation $G(0) = \frac{\pi}{4}$ follows from a well-known continued fraction expansion for $\arctan$ .  However, the expansion above is different.  It came up in my research, and I can't find it in any continued fraction tables.","Does anyone know a closed expression for the following continued fraction? All I know is that and converges for , while also . The equation follows from a well-known continued fraction expansion for .  However, the expansion above is different.  It came up in my research, and I can't find it in any continued fraction tables.",G(x) = \cfrac{1}{x+1+\cfrac{1}{x+3+\cfrac{4}{x+5+\cfrac{9}{x+7+\cfrac{16}{x+9+\cdots}}}}} G(0) = \frac{\pi}{4} G(x) x \geq 0 G(x) \sim \frac{1}{x} \ (x \to \infty) G(0) = \frac{\pi}{4} \arctan,"['analysis', 'special-functions', 'continued-fractions']"
69,Can we deduce strictly increasing on a sub-interval for such a function?,Can we deduce strictly increasing on a sub-interval for such a function?,,"Assume that we have a function $f : [0, 1] \rightarrow [0, 1]$ which is differentiable, $f(0) = 0$ and $f(1)=1$ . Can we deduce that there exists an open sub-interval in $[0, 1]$ in which $f$ is strictly increasing over that? Note that here, continuity of $f^{\prime}$ is not mentioned.","Assume that we have a function which is differentiable, and . Can we deduce that there exists an open sub-interval in in which is strictly increasing over that? Note that here, continuity of is not mentioned.","f : [0, 1] \rightarrow [0, 1] f(0) = 0 f(1)=1 [0, 1] f f^{\prime}",['analysis']
70,Divergence of normal vector field,Divergence of normal vector field,,"The unit sphere in $\mathbb R^3$ can be written as  $$ (\sin\theta\cos\varphi,\cos\theta\sin\varphi, \cos\theta) $$ And the normal vector is same with position vector i.e. $$ \nu=(\sin\theta\cos\varphi,\cos\theta\sin\varphi, \cos\theta) $$ And mean curvature is  $$ H=\text{div}_{\mathbb R^3} \nu $$ But I am really fuzzy with $\frac{\partial }{\partial x}\sin\theta\cos\varphi$. At a point of unit sphere , there is normal vector , but moving a little along $x$-axis , there is not normal vector of unit sphere. Then , how to define the $\text{div}_{\mathbb R^3} \nu$ ? Question origin: I want to calculate a example to verify the mean curvature formula, so I do the above, then got stuck in the divergence.","The unit sphere in $\mathbb R^3$ can be written as  $$ (\sin\theta\cos\varphi,\cos\theta\sin\varphi, \cos\theta) $$ And the normal vector is same with position vector i.e. $$ \nu=(\sin\theta\cos\varphi,\cos\theta\sin\varphi, \cos\theta) $$ And mean curvature is  $$ H=\text{div}_{\mathbb R^3} \nu $$ But I am really fuzzy with $\frac{\partial }{\partial x}\sin\theta\cos\varphi$. At a point of unit sphere , there is normal vector , but moving a little along $x$-axis , there is not normal vector of unit sphere. Then , how to define the $\text{div}_{\mathbb R^3} \nu$ ? Question origin: I want to calculate a example to verify the mean curvature formula, so I do the above, then got stuck in the divergence.",,"['analysis', 'riemannian-geometry', 'analytic-geometry', 'curvature']"
71,If the derivative of $f:R^m\to R^m$ is isometric then $f$ is isometric,If the derivative of  is isometric then  is isometric,f:R^m\to R^m f,"I can't finish this problem Let $f:R^m\to R^m$ be a $C^1$ function such that for all $x\in R^m$, $f'(x):R^m\to R^m$ is an isometry i.e. $|f'(x)v|=|v|$ for all $v\in R^m$. Prove that $f$ is an isometry, i.e. that $|f(x)-f(y)|=|x-y|$ for all $x,y\in R^m$. Conclude that there is a linear isometry $T:R^m\to R^m$ and $a\in R^m$ such that $f(x)=Tx+a$ for all $x\in R^m$. This is what I've done: Since $|f'(x)v|=|v|$ for all $x,v$ we have that $|f'(x)|=1$. The mean value inequality then says that $|f(x)-f(y)|\le 1\cdot |x-y|=|x-y|$. Ideally one would like to apply the previous inequality with $f^{-1}$ in place of $f$ but we don't know that $f$ is bijective. I tried to use the inverse function theorem but it only assures the inverse of $f$ when $f$ is restricted to some open sets. So how would one prove that $f$ is bijective?. After that I think i can finish the problem.","I can't finish this problem Let $f:R^m\to R^m$ be a $C^1$ function such that for all $x\in R^m$, $f'(x):R^m\to R^m$ is an isometry i.e. $|f'(x)v|=|v|$ for all $v\in R^m$. Prove that $f$ is an isometry, i.e. that $|f(x)-f(y)|=|x-y|$ for all $x,y\in R^m$. Conclude that there is a linear isometry $T:R^m\to R^m$ and $a\in R^m$ such that $f(x)=Tx+a$ for all $x\in R^m$. This is what I've done: Since $|f'(x)v|=|v|$ for all $x,v$ we have that $|f'(x)|=1$. The mean value inequality then says that $|f(x)-f(y)|\le 1\cdot |x-y|=|x-y|$. Ideally one would like to apply the previous inequality with $f^{-1}$ in place of $f$ but we don't know that $f$ is bijective. I tried to use the inverse function theorem but it only assures the inverse of $f$ when $f$ is restricted to some open sets. So how would one prove that $f$ is bijective?. After that I think i can finish the problem.",,"['analysis', 'derivatives']"
72,Proving the Well-Ordering Principle for Natural Numbers,Proving the Well-Ordering Principle for Natural Numbers,,"I know the WOP is treated like axiom of a natural number, but I was curious if I can prove WOP defined for the set of natural numbers N by following: Suppose A is a subset of N, which then obeys all axioms from Peano postulates, and let us assume that A does not have a least element.  Then, 1 in A must be a successor to another natural number, which is 0.  However, 0 is neither in A nor N.  (If 0 is in N, then we argue that 0 must be a successor to -1, which is not part of N).  Also, 1 in A being a successor contradicts with the Peano axiom (1 cannot be a successor).  Hence, our assumption that A having no least element is not true.  Therefore, A has a least element.  Without out loss o generality, every subset of the N has a least element. I understand this is silly proof, but I wanted to make sure that I have reasoning for WOP in N before moving on.","I know the WOP is treated like axiom of a natural number, but I was curious if I can prove WOP defined for the set of natural numbers N by following: Suppose A is a subset of N, which then obeys all axioms from Peano postulates, and let us assume that A does not have a least element.  Then, 1 in A must be a successor to another natural number, which is 0.  However, 0 is neither in A nor N.  (If 0 is in N, then we argue that 0 must be a successor to -1, which is not part of N).  Also, 1 in A being a successor contradicts with the Peano axiom (1 cannot be a successor).  Hence, our assumption that A having no least element is not true.  Therefore, A has a least element.  Without out loss o generality, every subset of the N has a least element. I understand this is silly proof, but I wanted to make sure that I have reasoning for WOP in N before moving on.",,['analysis']
73,Fourier transform of the Heaviside function,Fourier transform of the Heaviside function,,"As you can see from the title I want to calculate the Fourier transform of the Heaviside function $u(t)$. Proven the the Heaviside function is a tempered distribution I must evaluate: $$ \langle F(u(t)), \varphi \rangle \qquad \varphi \in S_{\xi} $$ Then I use the following property of the Fourier transform: $$ F(T^{(n)}) = (2 \pi i)^n \xi^n F(T)  $$ In my case, as we have that $u' = \delta$: $$ F(\delta) =  2 \pi i \xi F(u) $$ In this way I proved that $F(u)$ it's a solution of the following division problem for tempered distribution: $$ \begin{cases} \xi T = \frac{1}{2 \pi i} \\ T \in S' \end{cases} $$ If I find another solution of the problem, then the two solution will differ of $c \delta \ , c \in \mathbb{C}$. Let's prove that $p.v. \frac{1}{2 \pi i \xi}$ it's a solution for the problem. $$ \langle p.v. \frac{1}{2\pi i \xi}, \varphi\rangle = \frac{1}{2\pi i}\  p.v. \int_{\mathbb{R}} \frac{\xi \varphi(\xi)}{\xi} d\xi = \frac{1}{2 \pi i} \int_{\mathbb{R}} \varphi(\xi) d\xi = \langle \frac{1}{2 \pi i} , \varphi \rangle $$ Then we conclude that: $$ F(u) = p.v.\ \frac{1}{2\pi i \xi} + c \delta \qquad c \in \mathbb{C} $$ Now, there is the problem. How can I set the value of c ? Thanks in advance.","As you can see from the title I want to calculate the Fourier transform of the Heaviside function $u(t)$. Proven the the Heaviside function is a tempered distribution I must evaluate: $$ \langle F(u(t)), \varphi \rangle \qquad \varphi \in S_{\xi} $$ Then I use the following property of the Fourier transform: $$ F(T^{(n)}) = (2 \pi i)^n \xi^n F(T)  $$ In my case, as we have that $u' = \delta$: $$ F(\delta) =  2 \pi i \xi F(u) $$ In this way I proved that $F(u)$ it's a solution of the following division problem for tempered distribution: $$ \begin{cases} \xi T = \frac{1}{2 \pi i} \\ T \in S' \end{cases} $$ If I find another solution of the problem, then the two solution will differ of $c \delta \ , c \in \mathbb{C}$. Let's prove that $p.v. \frac{1}{2 \pi i \xi}$ it's a solution for the problem. $$ \langle p.v. \frac{1}{2\pi i \xi}, \varphi\rangle = \frac{1}{2\pi i}\  p.v. \int_{\mathbb{R}} \frac{\xi \varphi(\xi)}{\xi} d\xi = \frac{1}{2 \pi i} \int_{\mathbb{R}} \varphi(\xi) d\xi = \langle \frac{1}{2 \pi i} , \varphi \rangle $$ Then we conclude that: $$ F(u) = p.v.\ \frac{1}{2\pi i \xi} + c \delta \qquad c \in \mathbb{C} $$ Now, there is the problem. How can I set the value of c ? Thanks in advance.",,"['analysis', 'fourier-analysis', 'distribution-theory']"
74,vector space of continuously differentiable functions is complete regarding a specific norm [duplicate],vector space of continuously differentiable functions is complete regarding a specific norm [duplicate],,"This question already has an answer here : Prove that $C^1([a,b])$ with the $C^1$- norm is a Banach Space. (1 answer) Closed 4 years ago . Consider $C^1[a, b] = \{f: [a, b] \to \mathbb{C}\mid f\text{ continuously differentiable}\}$. Consider the following norm: $$\|f\|_{C^1} = \|f\|_\infty + \|f'\|_\infty$$ Now, it needs to be shown that $C^1[a, b]$ is complete with regard to the $\|\cdot\|_{C^1}$-Norm (using the definition for complete that a metric space $X$ is complete $\Longleftrightarrow$ every Cauchy-series in $X$ also converges in $X$). I know that $C^1[a, b]$ is not complete in respect to the $\infty$-norm. I guess these special Cauchy-sequences of functions that do not converge in $C^1[a, b]$ using the $\infty$-norm simply aren't Cauchy-sequences in respect to the $C^1$-norm. But how can this be proven? Thanks in advance. Edit: The linked thread only asks for why $||f||_{C^1}$ is a norm, and doesn't deal with whether or not $C^1[a, b]$ is complete regarding this norm at all, which is the specific topic of this thread.","This question already has an answer here : Prove that $C^1([a,b])$ with the $C^1$- norm is a Banach Space. (1 answer) Closed 4 years ago . Consider $C^1[a, b] = \{f: [a, b] \to \mathbb{C}\mid f\text{ continuously differentiable}\}$. Consider the following norm: $$\|f\|_{C^1} = \|f\|_\infty + \|f'\|_\infty$$ Now, it needs to be shown that $C^1[a, b]$ is complete with regard to the $\|\cdot\|_{C^1}$-Norm (using the definition for complete that a metric space $X$ is complete $\Longleftrightarrow$ every Cauchy-series in $X$ also converges in $X$). I know that $C^1[a, b]$ is not complete in respect to the $\infty$-norm. I guess these special Cauchy-sequences of functions that do not converge in $C^1[a, b]$ using the $\infty$-norm simply aren't Cauchy-sequences in respect to the $C^1$-norm. But how can this be proven? Thanks in advance. Edit: The linked thread only asks for why $||f||_{C^1}$ is a norm, and doesn't deal with whether or not $C^1[a, b]$ is complete regarding this norm at all, which is the specific topic of this thread.",,"['analysis', 'normed-spaces']"
75,A problem related to the submarine puzzle,A problem related to the submarine puzzle,,"Submarine puzzle: A submarine is located at an integer somewhere along the number line.   It is moving at some integral velocity (an integral number of units   per second). Every second you may drop a bomb which will destroy the   submarine if the submarine is at that location. Can you be guaranteed of destroying the submarine? If so, what   strategy would you use? (from http://math-fail.com/2011/01/submarine-puzzle.html ) And the answer is to enumerate all $(a,b)$ pairs, where $a, b$ are integers and the location of the submarine at time $t$ equals to $at+b$. My question is, if now $a, b$ can be any real numbers, and the bomb can now destroy the submarine in a region of length 1, i.e. if at time $t$ the bomb is dropped at $x_t$ and $x_t - 0.5 \le at+b \le x_t+0.5$, the submarine will be destroyed. Then can we be guaranteed to destroy the submarine? Some thinking: When $b$ is given, the mission can be done by enumerating $a$ in all rational number (since rational number is dense). When $a$ is restricted to an integer, the mission can be done by the same method as above. For general case, I think it is not possible to destroy the submarine, but I cannot prove it.","Submarine puzzle: A submarine is located at an integer somewhere along the number line.   It is moving at some integral velocity (an integral number of units   per second). Every second you may drop a bomb which will destroy the   submarine if the submarine is at that location. Can you be guaranteed of destroying the submarine? If so, what   strategy would you use? (from http://math-fail.com/2011/01/submarine-puzzle.html ) And the answer is to enumerate all $(a,b)$ pairs, where $a, b$ are integers and the location of the submarine at time $t$ equals to $at+b$. My question is, if now $a, b$ can be any real numbers, and the bomb can now destroy the submarine in a region of length 1, i.e. if at time $t$ the bomb is dropped at $x_t$ and $x_t - 0.5 \le at+b \le x_t+0.5$, the submarine will be destroyed. Then can we be guaranteed to destroy the submarine? Some thinking: When $b$ is given, the mission can be done by enumerating $a$ in all rational number (since rational number is dense). When $a$ is restricted to an integer, the mission can be done by the same method as above. For general case, I think it is not possible to destroy the submarine, but I cannot prove it.",,['analysis']
76,"$f''(x)+e^xf(x)=0$ , prove $f(x)$ is bounded",", prove  is bounded",f''(x)+e^xf(x)=0 f(x),"Differentiable function in $\mathbb{R}$ for which $f''(x) + e^x f(x)=0$ for every $x$. Prove that $f(x)$ is bounded as $x \rightarrow +\infty$ I have tried a few stuff but they didnt work out, for example i noticed that the function has infinite max and min as $x \rightarrow +\infty$ but thats still not enough to prove it, any ideas?","Differentiable function in $\mathbb{R}$ for which $f''(x) + e^x f(x)=0$ for every $x$. Prove that $f(x)$ is bounded as $x \rightarrow +\infty$ I have tried a few stuff but they didnt work out, for example i noticed that the function has infinite max and min as $x \rightarrow +\infty$ but thats still not enough to prove it, any ideas?",,['analysis']
77,Homework: Smooth mapping $f$ satisfying $f\circ f=f$,Homework: Smooth mapping  satisfying,f f\circ f=f,"This is an exercise in Mathematical Analysis by Zorich, in the subsection 12.1. Let $f:\mathbb{R}^n\rightarrow\mathbb{R}^n$ be a smooth mapping satisfying condition $f\circ f=f$ . $\quad$ a) Show that the set $f(\mathbb{R}^n)$ is a smooth surface in $\mathbb{R}^n$ . $\quad$ b) By what property of the mapping $f$ is the dimension of the surface determined? According to Zorich, the 'smooth surface' here has the same meaning as 'manifold' in Euclidean Space. So to prove this it's necessary to give it an atlas. But how to obtain the local charts? The only idea in my mind is to perform the differentiation to get the matrix equality: $$f'(f(x))\cdot f'(x)=f'(x)$$ without any progress. Thank you indeed if you can give me some help!","This is an exercise in Mathematical Analysis by Zorich, in the subsection 12.1. Let be a smooth mapping satisfying condition . a) Show that the set is a smooth surface in . b) By what property of the mapping is the dimension of the surface determined? According to Zorich, the 'smooth surface' here has the same meaning as 'manifold' in Euclidean Space. So to prove this it's necessary to give it an atlas. But how to obtain the local charts? The only idea in my mind is to perform the differentiation to get the matrix equality: without any progress. Thank you indeed if you can give me some help!",f:\mathbb{R}^n\rightarrow\mathbb{R}^n f\circ f=f \quad f(\mathbb{R}^n) \mathbb{R}^n \quad f f'(f(x))\cdot f'(x)=f'(x),['analysis']
78,Why are square functions important in analysis?,Why are square functions important in analysis?,,"I have been reading through chapter 1 of E.M. Stein's textbook Harmonic Analysis: Real Variable Methods, Orthogonality, and Oscillatory Integrals. In chapter 1, Stein discusses the relationship between singular integrals, maximal functions, and square functions.  I have taken an introductory course on harmonic analysis and therefore understand the importance of singular integrals and maximal functions.  However, I haven't encountered square functions yet, and I don't have any intuition about them or any context to understand why they are important.  Can someone explain to me why square functions are important so that I can understand them a bit better?","I have been reading through chapter 1 of E.M. Stein's textbook Harmonic Analysis: Real Variable Methods, Orthogonality, and Oscillatory Integrals. In chapter 1, Stein discusses the relationship between singular integrals, maximal functions, and square functions.  I have taken an introductory course on harmonic analysis and therefore understand the importance of singular integrals and maximal functions.  However, I haven't encountered square functions yet, and I don't have any intuition about them or any context to understand why they are important.  Can someone explain to me why square functions are important so that I can understand them a bit better?",,"['analysis', 'soft-question', 'fourier-analysis', 'harmonic-analysis', 'singular-integrals']"
79,Rectifiable functions,Rectifiable functions,,"A continuous function $\alpha: [a,b] \to \mathbb{R}^k$ is called a curve. For each partition $P = \{t_0<t_1<....<t_n=b\}$, define $l(\alpha, P) = \sum_{i=1}^n \left|\alpha(t_i) - \alpha(t_{i-1})\right|$. Let $l(\alpha) = \sup\{l(\alpha, P): P \text{ partitions } [a,b]\}$ If $l(\alpha) < \infty$, then $\alpha$ is called rectifiable. a) Let $c < a < b < d.$ Suppose $\alpha'$ is continuous on $(c,d)$ for some curve $\alpha: [c,d] \to \mathbb{R}^k$. Show that the restriction $\alpha|_{[a,b]}$ of $\alpha$ to $[a,b]$ is rectifiable with $l(\alpha|_{[a,b]}) = \int_a^b \left|\alpha'(t)\right|\, dt$. b) Let $c < a < b < d.$ Suppose $f: (c,d) \to \mathbb{R}$ is continuously differentiable. Let $\alpha: [a,b] \to \mathbb{R}^2$ be given by $\alpha(t) = (t,f(t))$. Find $l(\alpha)$ in terms of $f$. Here's what I have so far. a) we can use the Cauchy-Schwarz inequality, unsure how to implement it, and not sure what else we can invoke b) If $f$ is continuously differentiable, can I talk about uniform convergence, will it help? I am stuck on this one.","A continuous function $\alpha: [a,b] \to \mathbb{R}^k$ is called a curve. For each partition $P = \{t_0<t_1<....<t_n=b\}$, define $l(\alpha, P) = \sum_{i=1}^n \left|\alpha(t_i) - \alpha(t_{i-1})\right|$. Let $l(\alpha) = \sup\{l(\alpha, P): P \text{ partitions } [a,b]\}$ If $l(\alpha) < \infty$, then $\alpha$ is called rectifiable. a) Let $c < a < b < d.$ Suppose $\alpha'$ is continuous on $(c,d)$ for some curve $\alpha: [c,d] \to \mathbb{R}^k$. Show that the restriction $\alpha|_{[a,b]}$ of $\alpha$ to $[a,b]$ is rectifiable with $l(\alpha|_{[a,b]}) = \int_a^b \left|\alpha'(t)\right|\, dt$. b) Let $c < a < b < d.$ Suppose $f: (c,d) \to \mathbb{R}$ is continuously differentiable. Let $\alpha: [a,b] \to \mathbb{R}^2$ be given by $\alpha(t) = (t,f(t))$. Find $l(\alpha)$ in terms of $f$. Here's what I have so far. a) we can use the Cauchy-Schwarz inequality, unsure how to implement it, and not sure what else we can invoke b) If $f$ is continuously differentiable, can I talk about uniform convergence, will it help? I am stuck on this one.",,['analysis']
80,Number of Pythagorean Triples under a given Quantity,Number of Pythagorean Triples under a given Quantity,,"Consider the function $Pt(n)$. It tells us how many primitive Pythagorean Triples there are (below $n$) when any argument $n \in \mathbb{N}$ is plugged in. Is there an 'exact formula'; i.e. an elementary function of even a combination of known special functions like the Gamma and Error Function, that describes $Pt(n)$ ? Max Edit: I'm also interested in the exact value of the limit of $Pt(n)/n$ when $n$ tends to infinity.","Consider the function $Pt(n)$. It tells us how many primitive Pythagorean Triples there are (below $n$) when any argument $n \in \mathbb{N}$ is plugged in. Is there an 'exact formula'; i.e. an elementary function of even a combination of known special functions like the Gamma and Error Function, that describes $Pt(n)$ ? Max Edit: I'm also interested in the exact value of the limit of $Pt(n)/n$ when $n$ tends to infinity.",,"['number-theory', 'analysis', 'diophantine-equations']"
81,Uniqueness and continuous dependence on the data of Heat equation.,Uniqueness and continuous dependence on the data of Heat equation.,,"Let two smooth $v_1$ and $v_2$ both satisfy the system $$\partial_t{v}-\Delta v=f \quad \text{in} \quad U \times (0,\infty), $$ $$v = g \quad \text{on} \quad \partial U \times (0,\infty),$$ for some fixed given smooth $f: \bar{U}\times (0,\infty) \rightarrow \mathbb{R}$ and $g: \partial U \times (0,\infty).$ $U$ is open, bounded and $U \subset \mathbb{R}^n.$ Show that $$\sup_{x \in U}  |v_1(t, x) ‚àí v_2(t, x)| \rightarrow 0,$$ as $t \rightarrow \infty.$ This is my work: Let $ u =v_1 -v_2,$ it is sufficient to prove $\sup_{x \in U}  |u(x,t)| \rightarrow 0,$ as $t \rightarrow \infty. (1)$ $u$ obeys the system $$\partial_t{u}-\Delta u=0 \quad \text{in} \quad U \times (0,\infty), $$ $$u = 0 \quad \text{on} \quad \partial U \times (0,\infty).$$ Multiply both sides by $u.|u|^{2(m-1)},$ note that $\partial_t(|u|^{2m})=2m\partial_tu.u.|u|^{2(m-1)}$ then $$\dfrac{1}{2m}\partial_t\int_{U}|u|^{2m}dx=\int_{U}\Delta u.u.|u|^{2(m-1)}dx$$ Apply integration by part for the RHS, we get $$\dfrac{1}{2m}\partial_t\int_{U}|u|^{2m}dx=-(2m-1)\int_{U}|\nabla u|^2|u|^{2(m-1)}dx.$$ By the generalize Poincare's inequality, we obtain $$\partial_t\int_{U}|u|^{2m}dx \leq -2C\left(2-\dfrac{1}{m}\right)\int_{U}|u|^{2m}dx$$ or $$\partial_t\Vert u(t,\cdot)\Vert^{2m}_{L^{2m}(U)} \leq -2C\left(2-\dfrac{1}{m}\right)\Vert u(t,\cdot)\Vert^{2m}_{L^{2m}(U)}$$ $\Rightarrow 2m \Vert u(t,\cdot)\Vert^{2m-1}.\partial_t\Vert u(t,\cdot)\Vert_{L^{2m}(U)} \leq  -2C\left(2-\dfrac{1}{m}\right)\Vert u(t,\cdot)\Vert^{2m}_{L^{2m}(U)}$ $\Rightarrow \partial_t\Vert u(t,\cdot)\Vert_{L^{2m}(U)} \leq -2\dfrac{C}{m}\left(2-\dfrac{1}{m}\right)\Vert u(t,\cdot)\Vert_{L^{2m}(U)}$ Applying Gronwall's inequality, we get $\Vert u(t,\cdot)\Vert_{L^{2m}(U)} \leq e^{-2\frac{C}{m}\left(2-\frac{1}{m}\right)t}\Vert u(0,\cdot)\Vert_{L^{2m}(U)}.$ I am planing to let $m \rightarrow \infty$ to obtain $\Vert u(t,\cdot)\Vert_{L^{\infty}(U)}$ and let $t \rightarrow \infty$ then $e^{-2\frac{C}{m}\left(2-\frac{1}{m}\right)t} \rightarrow 0$ to obtain (1). But the problem is as $m \rightarrow \infty,$ $e^{-2\frac{C}{m}\left(2-\frac{1}{m}\right)t} \rightarrow 1 \neq 0.$ Am I on the right track or did I make some wrong steps? Could you provide any ideas to improve my work?","Let two smooth and both satisfy the system for some fixed given smooth and is open, bounded and Show that as This is my work: Let it is sufficient to prove as obeys the system Multiply both sides by note that then Apply integration by part for the RHS, we get By the generalize Poincare's inequality, we obtain or Applying Gronwall's inequality, we get I am planing to let to obtain and let then to obtain (1). But the problem is as Am I on the right track or did I make some wrong steps? Could you provide any ideas to improve my work?","v_1 v_2 \partial_t{v}-\Delta v=f \quad \text{in} \quad U \times (0,\infty),  v = g \quad \text{on} \quad \partial U \times (0,\infty), f: \bar{U}\times (0,\infty) \rightarrow \mathbb{R} g: \partial U \times (0,\infty). U U \subset \mathbb{R}^n. \sup_{x \in U}  |v_1(t, x) ‚àí v_2(t, x)| \rightarrow 0, t \rightarrow \infty.  u =v_1 -v_2, \sup_{x \in U}  |u(x,t)| \rightarrow 0, t \rightarrow \infty. (1) u \partial_t{u}-\Delta u=0 \quad \text{in} \quad U \times (0,\infty),  u = 0 \quad \text{on} \quad \partial U \times (0,\infty). u.|u|^{2(m-1)}, \partial_t(|u|^{2m})=2m\partial_tu.u.|u|^{2(m-1)} \dfrac{1}{2m}\partial_t\int_{U}|u|^{2m}dx=\int_{U}\Delta u.u.|u|^{2(m-1)}dx \dfrac{1}{2m}\partial_t\int_{U}|u|^{2m}dx=-(2m-1)\int_{U}|\nabla u|^2|u|^{2(m-1)}dx. \partial_t\int_{U}|u|^{2m}dx \leq -2C\left(2-\dfrac{1}{m}\right)\int_{U}|u|^{2m}dx \partial_t\Vert u(t,\cdot)\Vert^{2m}_{L^{2m}(U)} \leq -2C\left(2-\dfrac{1}{m}\right)\Vert u(t,\cdot)\Vert^{2m}_{L^{2m}(U)} \Rightarrow 2m \Vert u(t,\cdot)\Vert^{2m-1}.\partial_t\Vert u(t,\cdot)\Vert_{L^{2m}(U)} \leq  -2C\left(2-\dfrac{1}{m}\right)\Vert u(t,\cdot)\Vert^{2m}_{L^{2m}(U)} \Rightarrow \partial_t\Vert u(t,\cdot)\Vert_{L^{2m}(U)} \leq -2\dfrac{C}{m}\left(2-\dfrac{1}{m}\right)\Vert u(t,\cdot)\Vert_{L^{2m}(U)} \Vert u(t,\cdot)\Vert_{L^{2m}(U)} \leq e^{-2\frac{C}{m}\left(2-\frac{1}{m}\right)t}\Vert u(0,\cdot)\Vert_{L^{2m}(U)}. m \rightarrow \infty \Vert u(t,\cdot)\Vert_{L^{\infty}(U)} t \rightarrow \infty e^{-2\frac{C}{m}\left(2-\frac{1}{m}\right)t} \rightarrow 0 m \rightarrow \infty, e^{-2\frac{C}{m}\left(2-\frac{1}{m}\right)t} \rightarrow 1 \neq 0.","['analysis', 'partial-differential-equations', 'heat-equation', 'gronwall-type-inequality']"
82,Show that there is no continuously differentiable function on closed disks,Show that there is no continuously differentiable function on closed disks,,"In Cartesian coordinates, the closed unit  disk  is given by $$ \bar{D}=\left\{(x, y) \mid x^2+y^2 \leq 1\right\}, $$ and its boundary is given by $$ \partial D=\left\{(x, y) \mid x^2+y^2=1\right\}. $$ Prove that  there is no continuously differentiable function $f: \bar{D} \rightarrow \mathbb{R}^2 $ satisfying: $$ f\left(\bar{D}\right) \subseteq  \partial D \;\;\text{and}\;\; f(x,y)=f\left(x y, x^2-y^2\right). $$ Here, I have no idea why the question emphasize $f$ need to be differentiable. If someone can help me understand how to think about it would be great.","In Cartesian coordinates, the closed unit  disk  is given by and its boundary is given by Prove that  there is no continuously differentiable function satisfying: Here, I have no idea why the question emphasize need to be differentiable. If someone can help me understand how to think about it would be great.","
\bar{D}=\left\{(x, y) \mid x^2+y^2 \leq 1\right\},
 
\partial D=\left\{(x, y) \mid x^2+y^2=1\right\}.
 f: \bar{D} \rightarrow \mathbb{R}^2  
f\left(\bar{D}\right) \subseteq  \partial D \;\;\text{and}\;\; f(x,y)=f\left(x y, x^2-y^2\right).
 f",['analysis']
83,Example of a sequence on an infinite-dimensional vector space with respect to different norms [closed],Example of a sequence on an infinite-dimensional vector space with respect to different norms [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am stuck with the following problem: Give example of an infinite dimensional vector space V and two norms $\theta$ and $\rho$ on V and the sequence $\{x_{n}\}_{n\geq 1}$ of V such that: The sequence $\{x_{n}\}_{n\geq 1}$ is cauchy in $(V, \rho)$ The sequence $\{x_{n}\}_{n\geq 1}$ is not cauchy in $(V, \theta)$ It is easy to find a vector space, a norm, and a sequence that meet one of the items but I do not know how to proceed. Any help is appreciated.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am stuck with the following problem: Give example of an infinite dimensional vector space V and two norms and on V and the sequence of V such that: The sequence is cauchy in The sequence is not cauchy in It is easy to find a vector space, a norm, and a sequence that meet one of the items but I do not know how to proceed. Any help is appreciated.","\theta \rho \{x_{n}\}_{n\geq 1} \{x_{n}\}_{n\geq 1} (V, \rho) \{x_{n}\}_{n\geq 1} (V, \theta)","['analysis', 'cauchy-sequences']"
84,A function that verifies the property $f(ab) = f(a) + f(b)$,A function that verifies the property,f(ab) = f(a) + f(b),"Is there a function $f: (0,+\infty) \to \mathbb{R}$ that satisfies the property : $$  f(ab) = f(a) + f(b), \forall (a,b) \in\mathbb{R}^2 $$ Other than the logarithmic functions. If $f$ is differentiable at $1$ , then the answer is no, but if $f$ is not differentiable at $1$ , I can only show that it verifies the basic properties of logarithmic functions, not that it must be one. Thank you.","Is there a function that satisfies the property : Other than the logarithmic functions. If is differentiable at , then the answer is no, but if is not differentiable at , I can only show that it verifies the basic properties of logarithmic functions, not that it must be one. Thank you.","f: (0,+\infty) \to \mathbb{R}   f(ab) = f(a) + f(b), \forall (a,b) \in\mathbb{R}^2  f 1 f 1","['analysis', 'functions', 'functional-equations']"
85,Derivative of Shannon entropy for discrete distrubtions,Derivative of Shannon entropy for discrete distrubtions,,"I'm trying to find the derivate of the Shannon entropy for discrete distributions, i.e. the derivative of: $H(P)=-\sum_{i=0}^n p_i * log(p_i)$ I didn't have much trouble finding the solution for the binary case, using  $p_1 = 1-p_0$ such that: $H(p_0) = -p_0 * log(p_0) - (1-p_0) * log(1 - p_0)$ $H'(p_0) = log(1-p_0) - log(p_0))$ However, I'm not sure how to deal with the constraint that $\sum_{i=0}^n p_i=1 $ in the general case. Obviously, computing a partial derivative under the assumption that $p_i$ and $p_j$ are independent if $i\ne j$ leads to a meaningless result. I'd appreciate any tips on how this should be approached. Edit: Derivative under the assumption that all probabilities are independent from each other: $H(p_i)=-(log(p_i)-1)/ln(2)$","I'm trying to find the derivate of the Shannon entropy for discrete distributions, i.e. the derivative of: $H(P)=-\sum_{i=0}^n p_i * log(p_i)$ I didn't have much trouble finding the solution for the binary case, using  $p_1 = 1-p_0$ such that: $H(p_0) = -p_0 * log(p_0) - (1-p_0) * log(1 - p_0)$ $H'(p_0) = log(1-p_0) - log(p_0))$ However, I'm not sure how to deal with the constraint that $\sum_{i=0}^n p_i=1 $ in the general case. Obviously, computing a partial derivative under the assumption that $p_i$ and $p_j$ are independent if $i\ne j$ leads to a meaningless result. I'd appreciate any tips on how this should be approached. Edit: Derivative under the assumption that all probabilities are independent from each other: $H(p_i)=-(log(p_i)-1)/ln(2)$",,"['analysis', 'entropy']"
86,"Proof of $\sup AB=\max\{\sup A\sup B,\sup A\inf B,\inf A\sup B,\inf A\inf B\}$",Proof of,"\sup AB=\max\{\sup A\sup B,\sup A\inf B,\inf A\sup B,\inf A\inf B\}","I want to prove the following result: Theorem $\quad $ If $A$ and $B$ are bounded sets of real numbers, then     \begin{gather*}\tag{$\star$} 		\sup(A\cdot B)=\max\{\sup A\cdot\sup B, \sup A\cdot\inf B, \inf A\cdot\sup B, \inf A\cdot\inf B\}. 	\end{gather*} Although in the thread Show that  sup(A‚ãÖB)=max{supA‚ãÖsupB,supA‚ãÖinfB,infA‚ãÖsupB,infA‚ãÖinfB} , some suggestions are given, but I have trouble in understanding them. So I have tried to prove as follows. Please check whether it is right or wrong. Note that $AB$ is defined by $$AB=\{z\in\mathbb{R}\mid \exists x\in A, y\in B: z=xy\}.$$ In order to prove this theorem, we need the following lemma. Lemma $\quad $ Let $A$ and $B$ be nonempty sets of nonnegative real numbers. Suppose that $A$ and $B$  are bounded  above. Then     \begin{gather*} 		\sup AB=\sup A\cdot\sup B. 	\end{gather*} Proof. Let $A\subset [0,+\infty)$ and $B\subset [0,+\infty),$ and $A,B$ nonempty, bounded above.  Put $a=\sup A, b=\sup B, c=\sup AB.$ Since $A\subset [0,+\infty)$ and $B\subset [0,+\infty),$   we see that $0$ is a lower bound of both $A$ and $B,$ so $a, b, c$ are nonnegative.  Let $z\in AB.$ Then there are $x\in A, y\in B$ such that $z=xy.$  Since $0\leq x\leq a$ and $0\leq y\leq b,$ we have $xy\leq ab.$  So $z\leq ab.$ By arbitrariness of $z,$ we deduce that $ab$ is an upper bound  of $AB.$  Hence $c\leq ab.$   Note that $c\geq0.$  If $c<ab,$ then we have $a>0$ and $b>0.$ It follows that $\frac{c}{b}<a=\sup A.$  Hence there exists $x_1\in A$ such that $\frac{c}{b}<x_1.$  So $x_1>0.$ Then we deduce that $\frac{c}{x_1}<b=\sup B.$ So there exists $y_1\in B$ such that $\frac{c}{x_1}<y_1.$ From $x_1>0$ it follows that $c<x_1y_1.$  But $x_1y_1\in AB,$ so we have $\sup AB=c<x_1y_1\in AB,$ which is absurd.  Hence we have $c\geq ab.$ And thus we conclude that $\sup AB=c=ab=\sup A\cdot \sup B.$  $\qquad\Box$ Proof of the theorem. let $A$ and $B$ be bounded sets of real numbers. Put   \begin{gather*}  	a=\sup A, \quad a'=\inf A,\quad b=\sup B,\quad b'=\inf B, \quad c=\sup AB.  \end{gather*}   We shall prove that $c=\max\{ab,a'b,ab',a'b'\}.$   And we plan to prove this statement by cases. (i)   Case $a'\geq 0, b'\geq 0.$   Thus, for $x\in A$ and $y\in B,$ we have $a\geq x\geq a'\geq 0, b\geq y\geq b'\geq 0,$ so $A\subset [0,+\infty), B\subset [0,+\infty),$ and  $a'b\leq ab,  ab'\leq ab, a' b'\leq ab,$ which implies  that $\max\{ab,a'b,ab',a'b'\}=ab.$  By the above Lemma , we have $\sup AB=\sup A\cdot \sup B=ab.$ Hence in this case the desired equality $(\star)$ holds. (ii) $a'\geq 0, b'<0.$   In this case we have for all $x\in A,$  $x\geq a'\geq 0,$ and so $x\geq 0.$ We consider two sub-cases. (ii.1) $b>0.$  In this case $0\leq a'\leq a,  b'<0<b,$ so $a'b\leq ab, ab'\leq a'b'\leq 0\leq ab,$ which gives that   $\max\{ab,a'b,ab',a'b'\}=ab.$   Let $z\in AB,$ then there exists $x\in A, y\in B$ such that $z=xy.$ Since $x\geq 0,$  $xy\leq xb\leq ab.$ And so $c\leq ab.$  If $a=0,$ then we deduce that $a'=0,$ and so $A=\{0\}=AB,$ which shows the desired result is obvious.  If $a>0,$ then, for every $0<\epsilon<\min\{a,b\},$ we have $a-\epsilon>0, b-\epsilon>0.$ Hence there exist $x_1\in A, y_1\in B$ such that $x_1>a-\epsilon >0, y_1>b-\epsilon>0,$ which implies that $x_1y_1>(a-\epsilon)(b-\epsilon)=ab-\epsilon(a+b-\epsilon).$ Since $\lim_{\epsilon\to 0+}\epsilon(a+b-\epsilon)=0,$ we have, by the order preserving property of limit, $x_1y_1\geq ab,$ and so $c\geq ab.$   It follows  that $c=ab.$  Hence we have proved, in this case, that $\sup AB=c=ab=\max\{ab,a'b,ab',a'b'\}.$ (ii.2) $b\leq 0.$ Then $a'b'\leq a'b$ and $ab'\leq ab\leq a'b.$  So $\max\{ab,a'b,ab',a'b'\}=a'b.$  Let $z\in AB,$ then  there are $x\in A, y\in B$ such that $z=xy\leq xb\leq a'b,$ which implies that $c\leq a'b.$  On the other hand, for every $\epsilon >0,$   there exists $x_2\in A, y_2\in B,$ such that $0\leq x_2<a'+\epsilon, 0\geq b\geq y_2>b-\epsilon,$ it follows that $x_2y_2\geq (a'+\epsilon)y_2>(a'+\epsilon)(b-\epsilon)=a'b+\epsilon(b-a'-\epsilon),$ which implies that $x_2y_2\geq a'b.$  Thus $c\geq a'b.$  As a result, $c=a'b=\max\{ab,a'b,ab',a'b'\}.$ (iii) $a'<0, b'\geq 0.$  Swapping $A$ and $B,$ and   using Case (ii), the desired result follows. (iv) $a'<0, b'<0.$ There are four sub-cases. (iv.1)  $a> 0, b>0.$  That is, $a'<0<a, b'<0<b.$ Because in this case $a'b<0<a'b'\leq \max\{ab,a'b'\}$ and $ab'<0<ab\leq \max\{ab,a'b'\},$  we deduce that $\max\{ab,a'b,ab',a'b'\}=\max\{ab,a'b'\}.$   Then, for any $x\in A$ and $y\in B,$ we have $xy\leq \max\{ab, a'b'\}.$  Assume first that $\max\{ab, a'b'\}=a'b'.$  For sufficiently small $\epsilon>0,$ such that $a'+\epsilon<0$ and $b'+\epsilon<0,$  there exist $x^*\in A$ and $y^*\in B$ for which $x^*<a'+\epsilon<0$ and $y^*<b '+\epsilon<0.$ This gives $x^*y^*>(a'+\epsilon)(b'+\epsilon)=a'b'+\epsilon(a'+b'+\epsilon).$  Hence $\sup AB=c\geq x^*y^*\geq a'b'.$  Therefore $a'b'$ is the least upper bound  of $A\cdot B.$  In the case where $\max\{ab, a'b'\}=ab,$   for sufficiently small $\epsilon>0,$  such that   $a-\epsilon>0$ and $b-\epsilon>0,$  there exists $x_1\in A$ and $y_1\in B$ such that  $x_1>a-\epsilon>0,  y_1>b-\epsilon>0.$ This gives $x_1y_1>(a-\epsilon)(b-\epsilon)=ab-a\epsilon-\epsilon(b-\epsilon)=ab-\epsilon\big(a+b-\epsilon\big).$ Since   $\lim_{\epsilon\to 0+}\epsilon\big(  a+b-\epsilon\big)=0,$ the order preserving property of limit gives that $x_1y_1\geq ab,$ and hence $c\geq ab.$  Therefore $\sup (AB)=c=ab=\max\{ab,a'b,ab',a'b'\}.$ Therefore we have proved that, in this sub-case, $\sup AB=\max\{ab,ab',a'b,a'b'\}.$ (iv.2) $a>0, b\leq 0.$  Then $ab\leq 0\leq a'b\leq a'b',  ab'\leq 0\leq a'b',$ which gives that $\max\{ab,a'b,ab',a'b'\}=a'b'.$   In this case, for every $y\in B,$  $y\leq 0.$  Let $z\in AB,$ then there exist $x\in A, y\in B$ such that $z=xy.$  If $x\geq 0,$ then  since $a'< x\leq a,$ so $z=xy\leq a'y\leq a'b';$ while if $x<0,$ then $z=xy\leq a'y\leq a'b'.$  Hence $a'b'$ is an upper bound of $AB.$  Since $a'<0$ and $b'<0,$ for sufficiently small $\epsilon>0,$   such that $a'+\epsilon<0$ and $b'+\epsilon<0,$   there exist $x_1\in A, y_1\in B$ such that $x_1<a'+\epsilon<0, y_1<b'+\epsilon<0,$ which implies that   $x_1y_1>(a'+\epsilon)(b'+\epsilon)=a'b'+\epsilon(a'+b'+\epsilon)\to a'b', $ as $\epsilon\to 0+0.$ Hence $x_1y_1\geq a'b'.$  Thus $c=\sup AB\geq x_1y_1\geq a'b'.$  Therefore we have $\sup AB=c=a'b'=\max\{ab,a'b,ab',a'b'\}.$ (iv.3)  $a\leq 0, b>0.$  Similar to sub-case (iv.2), or you can argue by simply swapping  $A$ and $B.$ (iv.4) $a\leq 0, b\leq 0.$  Because in this case $a,a',b,b'$ are non-positive, $ab\leq a'b\leq a'b',$ and $ab'\leq a'b',$ we deduce that $\{ab,a'b,ab',a'b'\}=a'b'.$    For every $x\in A, y\in B,$ we have $a'\leq x\leq a\leq 0, b'\leq y\leq b\leq 0,$ so   $xy\leq a'b'.$  From this it follows that $a'b'$ is an upper bound of $AB.$   Let $\epsilon>0$ be sufficiently small such that  $a'+\epsilon<0$ and $b'+\epsilon<0.$   Then there exist $x_1\in A, y_1\in B$ such that $x_1<a'+\epsilon<0, y_1<b'+\epsilon<0.$ So $x_1y_1>(a'+\epsilon)(b'+\epsilon)=a'b'+\epsilon(a'+b'+\epsilon). $  It follows  that $c\geq a'b'.$  As a consequence, we have $c=a'b'=\max\{ab,a'b,ab',a'b'\}.$ $\qquad\Box$","I want to prove the following result: Theorem $\quad $ If $A$ and $B$ are bounded sets of real numbers, then     \begin{gather*}\tag{$\star$} 		\sup(A\cdot B)=\max\{\sup A\cdot\sup B, \sup A\cdot\inf B, \inf A\cdot\sup B, \inf A\cdot\inf B\}. 	\end{gather*} Although in the thread Show that  sup(A‚ãÖB)=max{supA‚ãÖsupB,supA‚ãÖinfB,infA‚ãÖsupB,infA‚ãÖinfB} , some suggestions are given, but I have trouble in understanding them. So I have tried to prove as follows. Please check whether it is right or wrong. Note that $AB$ is defined by $$AB=\{z\in\mathbb{R}\mid \exists x\in A, y\in B: z=xy\}.$$ In order to prove this theorem, we need the following lemma. Lemma $\quad $ Let $A$ and $B$ be nonempty sets of nonnegative real numbers. Suppose that $A$ and $B$  are bounded  above. Then     \begin{gather*} 		\sup AB=\sup A\cdot\sup B. 	\end{gather*} Proof. Let $A\subset [0,+\infty)$ and $B\subset [0,+\infty),$ and $A,B$ nonempty, bounded above.  Put $a=\sup A, b=\sup B, c=\sup AB.$ Since $A\subset [0,+\infty)$ and $B\subset [0,+\infty),$   we see that $0$ is a lower bound of both $A$ and $B,$ so $a, b, c$ are nonnegative.  Let $z\in AB.$ Then there are $x\in A, y\in B$ such that $z=xy.$  Since $0\leq x\leq a$ and $0\leq y\leq b,$ we have $xy\leq ab.$  So $z\leq ab.$ By arbitrariness of $z,$ we deduce that $ab$ is an upper bound  of $AB.$  Hence $c\leq ab.$   Note that $c\geq0.$  If $c<ab,$ then we have $a>0$ and $b>0.$ It follows that $\frac{c}{b}<a=\sup A.$  Hence there exists $x_1\in A$ such that $\frac{c}{b}<x_1.$  So $x_1>0.$ Then we deduce that $\frac{c}{x_1}<b=\sup B.$ So there exists $y_1\in B$ such that $\frac{c}{x_1}<y_1.$ From $x_1>0$ it follows that $c<x_1y_1.$  But $x_1y_1\in AB,$ so we have $\sup AB=c<x_1y_1\in AB,$ which is absurd.  Hence we have $c\geq ab.$ And thus we conclude that $\sup AB=c=ab=\sup A\cdot \sup B.$  $\qquad\Box$ Proof of the theorem. let $A$ and $B$ be bounded sets of real numbers. Put   \begin{gather*}  	a=\sup A, \quad a'=\inf A,\quad b=\sup B,\quad b'=\inf B, \quad c=\sup AB.  \end{gather*}   We shall prove that $c=\max\{ab,a'b,ab',a'b'\}.$   And we plan to prove this statement by cases. (i)   Case $a'\geq 0, b'\geq 0.$   Thus, for $x\in A$ and $y\in B,$ we have $a\geq x\geq a'\geq 0, b\geq y\geq b'\geq 0,$ so $A\subset [0,+\infty), B\subset [0,+\infty),$ and  $a'b\leq ab,  ab'\leq ab, a' b'\leq ab,$ which implies  that $\max\{ab,a'b,ab',a'b'\}=ab.$  By the above Lemma , we have $\sup AB=\sup A\cdot \sup B=ab.$ Hence in this case the desired equality $(\star)$ holds. (ii) $a'\geq 0, b'<0.$   In this case we have for all $x\in A,$  $x\geq a'\geq 0,$ and so $x\geq 0.$ We consider two sub-cases. (ii.1) $b>0.$  In this case $0\leq a'\leq a,  b'<0<b,$ so $a'b\leq ab, ab'\leq a'b'\leq 0\leq ab,$ which gives that   $\max\{ab,a'b,ab',a'b'\}=ab.$   Let $z\in AB,$ then there exists $x\in A, y\in B$ such that $z=xy.$ Since $x\geq 0,$  $xy\leq xb\leq ab.$ And so $c\leq ab.$  If $a=0,$ then we deduce that $a'=0,$ and so $A=\{0\}=AB,$ which shows the desired result is obvious.  If $a>0,$ then, for every $0<\epsilon<\min\{a,b\},$ we have $a-\epsilon>0, b-\epsilon>0.$ Hence there exist $x_1\in A, y_1\in B$ such that $x_1>a-\epsilon >0, y_1>b-\epsilon>0,$ which implies that $x_1y_1>(a-\epsilon)(b-\epsilon)=ab-\epsilon(a+b-\epsilon).$ Since $\lim_{\epsilon\to 0+}\epsilon(a+b-\epsilon)=0,$ we have, by the order preserving property of limit, $x_1y_1\geq ab,$ and so $c\geq ab.$   It follows  that $c=ab.$  Hence we have proved, in this case, that $\sup AB=c=ab=\max\{ab,a'b,ab',a'b'\}.$ (ii.2) $b\leq 0.$ Then $a'b'\leq a'b$ and $ab'\leq ab\leq a'b.$  So $\max\{ab,a'b,ab',a'b'\}=a'b.$  Let $z\in AB,$ then  there are $x\in A, y\in B$ such that $z=xy\leq xb\leq a'b,$ which implies that $c\leq a'b.$  On the other hand, for every $\epsilon >0,$   there exists $x_2\in A, y_2\in B,$ such that $0\leq x_2<a'+\epsilon, 0\geq b\geq y_2>b-\epsilon,$ it follows that $x_2y_2\geq (a'+\epsilon)y_2>(a'+\epsilon)(b-\epsilon)=a'b+\epsilon(b-a'-\epsilon),$ which implies that $x_2y_2\geq a'b.$  Thus $c\geq a'b.$  As a result, $c=a'b=\max\{ab,a'b,ab',a'b'\}.$ (iii) $a'<0, b'\geq 0.$  Swapping $A$ and $B,$ and   using Case (ii), the desired result follows. (iv) $a'<0, b'<0.$ There are four sub-cases. (iv.1)  $a> 0, b>0.$  That is, $a'<0<a, b'<0<b.$ Because in this case $a'b<0<a'b'\leq \max\{ab,a'b'\}$ and $ab'<0<ab\leq \max\{ab,a'b'\},$  we deduce that $\max\{ab,a'b,ab',a'b'\}=\max\{ab,a'b'\}.$   Then, for any $x\in A$ and $y\in B,$ we have $xy\leq \max\{ab, a'b'\}.$  Assume first that $\max\{ab, a'b'\}=a'b'.$  For sufficiently small $\epsilon>0,$ such that $a'+\epsilon<0$ and $b'+\epsilon<0,$  there exist $x^*\in A$ and $y^*\in B$ for which $x^*<a'+\epsilon<0$ and $y^*<b '+\epsilon<0.$ This gives $x^*y^*>(a'+\epsilon)(b'+\epsilon)=a'b'+\epsilon(a'+b'+\epsilon).$  Hence $\sup AB=c\geq x^*y^*\geq a'b'.$  Therefore $a'b'$ is the least upper bound  of $A\cdot B.$  In the case where $\max\{ab, a'b'\}=ab,$   for sufficiently small $\epsilon>0,$  such that   $a-\epsilon>0$ and $b-\epsilon>0,$  there exists $x_1\in A$ and $y_1\in B$ such that  $x_1>a-\epsilon>0,  y_1>b-\epsilon>0.$ This gives $x_1y_1>(a-\epsilon)(b-\epsilon)=ab-a\epsilon-\epsilon(b-\epsilon)=ab-\epsilon\big(a+b-\epsilon\big).$ Since   $\lim_{\epsilon\to 0+}\epsilon\big(  a+b-\epsilon\big)=0,$ the order preserving property of limit gives that $x_1y_1\geq ab,$ and hence $c\geq ab.$  Therefore $\sup (AB)=c=ab=\max\{ab,a'b,ab',a'b'\}.$ Therefore we have proved that, in this sub-case, $\sup AB=\max\{ab,ab',a'b,a'b'\}.$ (iv.2) $a>0, b\leq 0.$  Then $ab\leq 0\leq a'b\leq a'b',  ab'\leq 0\leq a'b',$ which gives that $\max\{ab,a'b,ab',a'b'\}=a'b'.$   In this case, for every $y\in B,$  $y\leq 0.$  Let $z\in AB,$ then there exist $x\in A, y\in B$ such that $z=xy.$  If $x\geq 0,$ then  since $a'< x\leq a,$ so $z=xy\leq a'y\leq a'b';$ while if $x<0,$ then $z=xy\leq a'y\leq a'b'.$  Hence $a'b'$ is an upper bound of $AB.$  Since $a'<0$ and $b'<0,$ for sufficiently small $\epsilon>0,$   such that $a'+\epsilon<0$ and $b'+\epsilon<0,$   there exist $x_1\in A, y_1\in B$ such that $x_1<a'+\epsilon<0, y_1<b'+\epsilon<0,$ which implies that   $x_1y_1>(a'+\epsilon)(b'+\epsilon)=a'b'+\epsilon(a'+b'+\epsilon)\to a'b', $ as $\epsilon\to 0+0.$ Hence $x_1y_1\geq a'b'.$  Thus $c=\sup AB\geq x_1y_1\geq a'b'.$  Therefore we have $\sup AB=c=a'b'=\max\{ab,a'b,ab',a'b'\}.$ (iv.3)  $a\leq 0, b>0.$  Similar to sub-case (iv.2), or you can argue by simply swapping  $A$ and $B.$ (iv.4) $a\leq 0, b\leq 0.$  Because in this case $a,a',b,b'$ are non-positive, $ab\leq a'b\leq a'b',$ and $ab'\leq a'b',$ we deduce that $\{ab,a'b,ab',a'b'\}=a'b'.$    For every $x\in A, y\in B,$ we have $a'\leq x\leq a\leq 0, b'\leq y\leq b\leq 0,$ so   $xy\leq a'b'.$  From this it follows that $a'b'$ is an upper bound of $AB.$   Let $\epsilon>0$ be sufficiently small such that  $a'+\epsilon<0$ and $b'+\epsilon<0.$   Then there exist $x_1\in A, y_1\in B$ such that $x_1<a'+\epsilon<0, y_1<b'+\epsilon<0.$ So $x_1y_1>(a'+\epsilon)(b'+\epsilon)=a'b'+\epsilon(a'+b'+\epsilon). $  It follows  that $c\geq a'b'.$  As a consequence, we have $c=a'b'=\max\{ab,a'b,ab',a'b'\}.$ $\qquad\Box$",,"['analysis', 'proof-verification', 'supremum-and-infimum']"
87,under what conditions can orthogonal vector fields make curvilinear coordinate system?,under what conditions can orthogonal vector fields make curvilinear coordinate system?,,"I am considering n-dimensional Euclidean space $\mathbb{R}^n$. For any $x\in\mathbb{R}^n$, $v_1(x), \cdots, v_n(x)$ are orthogonal vectors. As functions of $x$, $v_i$'s are differentiable and non-zero everywhere. For $i=1,\cdots,n$, let $\gamma_i(t_i)$ be integral curves driven by $v_i(x)$, i.e. $$\frac{d\gamma_i(t_i)}{dt_i}=v_i(\gamma_i(t_i))).$$ The question is, can $v_i$ always be ""properly scaled"" such that $\gamma_i$'s can define a curvilinear coordinate system, i.e., any $x\in\mathbb{R}^n$ can be expressed as $x(t_1,\cdots, t_n)$. If not, then under what conditions for $v_i$'s can this be realized? An example I have in mind for $v_i$'s is the eigenvectors of the Hessian of a smooth function $f:\mathbb{R}^n\rightarrow \mathbb{R}$. How can one decide the magnitude of the eigenvectors to make a curvilinear coordinate system? The question asked above is more general than this example.","I am considering n-dimensional Euclidean space $\mathbb{R}^n$. For any $x\in\mathbb{R}^n$, $v_1(x), \cdots, v_n(x)$ are orthogonal vectors. As functions of $x$, $v_i$'s are differentiable and non-zero everywhere. For $i=1,\cdots,n$, let $\gamma_i(t_i)$ be integral curves driven by $v_i(x)$, i.e. $$\frac{d\gamma_i(t_i)}{dt_i}=v_i(\gamma_i(t_i))).$$ The question is, can $v_i$ always be ""properly scaled"" such that $\gamma_i$'s can define a curvilinear coordinate system, i.e., any $x\in\mathbb{R}^n$ can be expressed as $x(t_1,\cdots, t_n)$. If not, then under what conditions for $v_i$'s can this be realized? An example I have in mind for $v_i$'s is the eigenvectors of the Hessian of a smooth function $f:\mathbb{R}^n\rightarrow \mathbb{R}$. How can one decide the magnitude of the eigenvectors to make a curvilinear coordinate system? The question asked above is more general than this example.",,"['analysis', 'differential-geometry', 'coordinate-systems']"
88,Prove that $f(x) = x^{1/5}$ is continuous everywhere,Prove that  is continuous everywhere,f(x) = x^{1/5},"Need to prove that $f(x) = x^{1/5}$ is continuous everywhere, where $f: \mathbb{R} \to \mathbb{R}$: from definition we need to show that given $ \epsilon > 0 $ $\exists \delta > 0 $ s.t. $|x-x_0|<\delta \Rightarrow \left|x^{\frac{1}{5}} - x_0^{\frac{1}{5}}\right| < \epsilon$ for any point $x_0 \in \mathbb{R}$ I have a proof but it's somewhat unjustified: consider  $\left|x^\frac15 - x_0^\frac15\right| \geq \left|x^\frac15\right| - \left|x_0^\frac15\right| $ from the triangle inequality since $\left|x^\frac15\right| < |x|$ and $\left|x_0^\frac15\right| < |x_0|$ then $\left|x^\frac15 - x_0^\frac15\right| \geq \left|x^\frac15\right| - \left|x_0^\frac15\right|  < |x| - \left|x_0\right| = \left|x-x_0\right| = \delta$ so we can choose $\delta = \epsilon$? Overall I'm not happy with the proof, in the last inequality I don't think I can just state that delta = epsilon and be done, but I have no idea what else to do. I also am not sure about this step  $|x| - |x_0| = |x-x_0|$and $\left|x^\frac15\right| < |x|$ and $\left|x_0^\frac15\right| < |x_0|$ that step also... if anyone could help me out.. thank you","Need to prove that $f(x) = x^{1/5}$ is continuous everywhere, where $f: \mathbb{R} \to \mathbb{R}$: from definition we need to show that given $ \epsilon > 0 $ $\exists \delta > 0 $ s.t. $|x-x_0|<\delta \Rightarrow \left|x^{\frac{1}{5}} - x_0^{\frac{1}{5}}\right| < \epsilon$ for any point $x_0 \in \mathbb{R}$ I have a proof but it's somewhat unjustified: consider  $\left|x^\frac15 - x_0^\frac15\right| \geq \left|x^\frac15\right| - \left|x_0^\frac15\right| $ from the triangle inequality since $\left|x^\frac15\right| < |x|$ and $\left|x_0^\frac15\right| < |x_0|$ then $\left|x^\frac15 - x_0^\frac15\right| \geq \left|x^\frac15\right| - \left|x_0^\frac15\right|  < |x| - \left|x_0\right| = \left|x-x_0\right| = \delta$ so we can choose $\delta = \epsilon$? Overall I'm not happy with the proof, in the last inequality I don't think I can just state that delta = epsilon and be done, but I have no idea what else to do. I also am not sure about this step  $|x| - |x_0| = |x-x_0|$and $\left|x^\frac15\right| < |x|$ and $\left|x_0^\frac15\right| < |x_0|$ that step also... if anyone could help me out.. thank you",,['analysis']
89,Formula for Sum of Logarithms $\ln(n)^m$,Formula for Sum of Logarithms,\ln(n)^m,As you know $\sum_{n=1}^k \ln(n) =\ln(k!)$ is there a formula for $\sum_{n=1}^k \ln(n)^m$?,As you know $\sum_{n=1}^k \ln(n) =\ln(k!)$ is there a formula for $\sum_{n=1}^k \ln(n)^m$?,,"['analysis', 'summation', 'logarithms']"
90,Why isn't Volterra's function Riemann integrable?,Why isn't Volterra's function Riemann integrable?,,"My construction of Volterra's function is as follows. Let $F(x)=\begin{cases} x^2\sin\left(\frac{1}{x}\right) &\text{ if } x \neq 0\\ 0 &\text { if } x =0 \end{cases}$ On the interval $\left[0,\frac{1}{8}\right]$ we find the most extreme value of $F'(x)=0$.  Call that value $x_0$.  Then we we get $F(x)$ for all $x \in [0, x_0]$ and $F(x_0)$ for all $x \in[x_0, \frac{1}{8}]$.  Then we mirror the function from $\left[\frac{1}{8}, \frac{1}{4}\right]$.  Outside of $\frac{1}{4}$, we say the function is 0.  We call this function $V_1(x)$.  Then we translate the graph of $V_1(x)$ to the first deleted interval of the Smith-Cantor-Volterra set $\left[\frac{3}{8},\frac{5}{8}\right]$. We repeat this for each interval $\left[0,\frac{1}{2\cdot4^n}\right]$, reflect from $\left[\frac{1}{2\cdot4^n},\frac{1}{4^n}\right]$.  Call it $V_n(x)$.  Then we translate to the next removed pieces of the Smith-Cantor-Volterra set. I see that the function $V(x)$ is differentiable everywhere and that $V'(x)$ is bounded, but I'm not sure why its not Riemann integrable (without using Lebesgue's criteria).","My construction of Volterra's function is as follows. Let $F(x)=\begin{cases} x^2\sin\left(\frac{1}{x}\right) &\text{ if } x \neq 0\\ 0 &\text { if } x =0 \end{cases}$ On the interval $\left[0,\frac{1}{8}\right]$ we find the most extreme value of $F'(x)=0$.  Call that value $x_0$.  Then we we get $F(x)$ for all $x \in [0, x_0]$ and $F(x_0)$ for all $x \in[x_0, \frac{1}{8}]$.  Then we mirror the function from $\left[\frac{1}{8}, \frac{1}{4}\right]$.  Outside of $\frac{1}{4}$, we say the function is 0.  We call this function $V_1(x)$.  Then we translate the graph of $V_1(x)$ to the first deleted interval of the Smith-Cantor-Volterra set $\left[\frac{3}{8},\frac{5}{8}\right]$. We repeat this for each interval $\left[0,\frac{1}{2\cdot4^n}\right]$, reflect from $\left[\frac{1}{2\cdot4^n},\frac{1}{4^n}\right]$.  Call it $V_n(x)$.  Then we translate to the next removed pieces of the Smith-Cantor-Volterra set. I see that the function $V(x)$ is differentiable everywhere and that $V'(x)$ is bounded, but I'm not sure why its not Riemann integrable (without using Lebesgue's criteria).",,['analysis']
91,How smooth is a smooth function?,How smooth is a smooth function?,,"Let's say a smooth function is a $\mathcal{C}^\infty$ function on $\mathbb{R}$. Some smooth functions are not analytic, the most notorious example being the bump functions. A non-analytic $\mathcal{C}^\infty$ function seems (formally at least) ""less smooth"" that an analytic function, but I am wondering how one can quantify this. Looking at derivability does not give us anything (since a $\mathcal{C}^\infty$ is, well, $\mathcal{C}^\infty$), and looking at the radius of convergence of the Taylor series does not give us anything we do not already know. So, what if we look at the Fourier transform? This is a classical strategy: to quantify the smoothness of a function, quantify the integrability or the decay of its Fourier transform. Let's assume that we work in Scwhartz space $\mathcal{S} (\mathbb{R})$. Then: For any $f$ in $\mathcal{S} (\mathbb{R})$, if $\hat{f} (x) =_{\pm \infty} O (e^{-|x|^{1+\varepsilon}})$ for some $\varepsilon > 0$, then $f$ is analytic (I am not one hundred percent sure - I have not proved with utter rigour - but it seems rather safe). For all $\varepsilon > 0$, there exists a bump function $f$ such that $\hat{f} (x)$ has a magnitude of roughly(*) $e^{-|x|^{1-\varepsilon}}$ for large enough $|x|$ (again, I am not totally sure, but I think this is what is said in http://math.mit.edu/~stevenj/bump-saddle.pdf ). So I have two questions: Are there analytic functions $f$ in $\mathcal{S} (\mathbb{R})$ such that $\hat{f} (x)$ has a magnitude of roughly $e^{-|x|^{1-\varepsilon}}$ for some $\varepsilon > 0$ ? If it where not the case, and provided what I said before is true, then we would know that functions whose Fourier transform is of order $e^{-|x|^{1-\varepsilon}}$ are $\mathcal{C}^\infty$ but not analytic, and that functions whose Fourier transform is of order $e^{-|x|^{1+\varepsilon}}$ are analytic. What about intermediate growth rates, for instance when $\hat{f} (x)$ has a magnitude of roughly $e^{-C|x|}$? Of course, any more precise criterion or any related reference is welcome. (*) To be more precise : $\limsup_{x \to \pm \infty} \frac{-\ln |\hat{f} (x)|}{|x|^{1-\varepsilon}} = 1$.","Let's say a smooth function is a $\mathcal{C}^\infty$ function on $\mathbb{R}$. Some smooth functions are not analytic, the most notorious example being the bump functions. A non-analytic $\mathcal{C}^\infty$ function seems (formally at least) ""less smooth"" that an analytic function, but I am wondering how one can quantify this. Looking at derivability does not give us anything (since a $\mathcal{C}^\infty$ is, well, $\mathcal{C}^\infty$), and looking at the radius of convergence of the Taylor series does not give us anything we do not already know. So, what if we look at the Fourier transform? This is a classical strategy: to quantify the smoothness of a function, quantify the integrability or the decay of its Fourier transform. Let's assume that we work in Scwhartz space $\mathcal{S} (\mathbb{R})$. Then: For any $f$ in $\mathcal{S} (\mathbb{R})$, if $\hat{f} (x) =_{\pm \infty} O (e^{-|x|^{1+\varepsilon}})$ for some $\varepsilon > 0$, then $f$ is analytic (I am not one hundred percent sure - I have not proved with utter rigour - but it seems rather safe). For all $\varepsilon > 0$, there exists a bump function $f$ such that $\hat{f} (x)$ has a magnitude of roughly(*) $e^{-|x|^{1-\varepsilon}}$ for large enough $|x|$ (again, I am not totally sure, but I think this is what is said in http://math.mit.edu/~stevenj/bump-saddle.pdf ). So I have two questions: Are there analytic functions $f$ in $\mathcal{S} (\mathbb{R})$ such that $\hat{f} (x)$ has a magnitude of roughly $e^{-|x|^{1-\varepsilon}}$ for some $\varepsilon > 0$ ? If it where not the case, and provided what I said before is true, then we would know that functions whose Fourier transform is of order $e^{-|x|^{1-\varepsilon}}$ are $\mathcal{C}^\infty$ but not analytic, and that functions whose Fourier transform is of order $e^{-|x|^{1+\varepsilon}}$ are analytic. What about intermediate growth rates, for instance when $\hat{f} (x)$ has a magnitude of roughly $e^{-C|x|}$? Of course, any more precise criterion or any related reference is welcome. (*) To be more precise : $\limsup_{x \to \pm \infty} \frac{-\ln |\hat{f} (x)|}{|x|^{1-\varepsilon}} = 1$.",,"['analysis', 'fourier-analysis']"
92,"If a sequence of boundaries converges, do the spectrums of the enclosed regions also converge?","If a sequence of boundaries converges, do the spectrums of the enclosed regions also converge?",,"A planar region will have associated to it a spectrum consisting of Dirichlet eigenvalues, or parameters $\lambda$ for which it is possible to solve the Dirichlet problem for the Laplacian operator, $$ \begin{cases} \Delta u + \lambda u = 0 \\ u|_{\partial R} = 0 \end{cases}$$ I'm wondering, if we have a sequence of boundaries $\partial R_n$ converging pointwise towards $\partial R$, then will the spectrums also converge? (I make the notion of convergence formal in the following manner: $\cap_{N=1}^\infty l(\cup_{n=N}^\infty\partial R_n)=\partial R$; $\cap_{N=1}^\infty l(\cup_{n=N}^\infty\mathrm{spec}(R_n))=\mathrm{spec}( R)$, where $ l(\cdot)$ denotes the set of accumulation points of a set and $\mathrm{spec}(\cdot)$ denotes the spectrum of a region.) One motivating pathological example is the sequence of boundaries, indexed by $n$, defined by the polar equations $r=1+\frac{1}{n}\sin(n^2\theta)$. The boundaries converge to the unit circle. However, since the gradient of any eigenfunction must be orthogonal to the region boundary (as it is a level set), the eigenfunctions can't possibly converge to anything (under any meaningful notion) and so it makes me question if it's even possible for the eigenvalues to do so. If the answer is ""no, the spectrum doesn't necessarily converge,"" a much broader question arises: what are necessary and sufficient conditions for it to converge? Intuitively, I imagine a necessary condition is that the curvature of the boundaries also converge appropriately, but I have no idea if that's sufficient. EDIT: Another interesting question is if the principal eigenvalue (the smallest nonzero one) can grow arbitrarily large.","A planar region will have associated to it a spectrum consisting of Dirichlet eigenvalues, or parameters $\lambda$ for which it is possible to solve the Dirichlet problem for the Laplacian operator, $$ \begin{cases} \Delta u + \lambda u = 0 \\ u|_{\partial R} = 0 \end{cases}$$ I'm wondering, if we have a sequence of boundaries $\partial R_n$ converging pointwise towards $\partial R$, then will the spectrums also converge? (I make the notion of convergence formal in the following manner: $\cap_{N=1}^\infty l(\cup_{n=N}^\infty\partial R_n)=\partial R$; $\cap_{N=1}^\infty l(\cup_{n=N}^\infty\mathrm{spec}(R_n))=\mathrm{spec}( R)$, where $ l(\cdot)$ denotes the set of accumulation points of a set and $\mathrm{spec}(\cdot)$ denotes the spectrum of a region.) One motivating pathological example is the sequence of boundaries, indexed by $n$, defined by the polar equations $r=1+\frac{1}{n}\sin(n^2\theta)$. The boundaries converge to the unit circle. However, since the gradient of any eigenfunction must be orthogonal to the region boundary (as it is a level set), the eigenfunctions can't possibly converge to anything (under any meaningful notion) and so it makes me question if it's even possible for the eigenvalues to do so. If the answer is ""no, the spectrum doesn't necessarily converge,"" a much broader question arises: what are necessary and sufficient conditions for it to converge? Intuitively, I imagine a necessary condition is that the curvature of the boundaries also converge appropriately, but I have no idea if that's sufficient. EDIT: Another interesting question is if the principal eigenvalue (the smallest nonzero one) can grow arbitrarily large.",,"['analysis', 'differential-geometry']"
93,"Applications of the ""soft maximum""","Applications of the ""soft maximum""",,"There is a little triviality that has been referred to as the ""soft maximum"" over on John Cook's Blog that I find to be fun, at the very least. The idea is this:  given a list of values, say $x_1,x_2,\ldots,x_n$ , the function $g(x_1,x_2,\ldots,x_n) = \log(\exp(x_1) + \exp(x_2) + \cdots + \exp(x_n))$ returns a value very near the maximum in the list. This happens because that exponentiation exaggerates the differences between the $x_i$ values.  For the largest $x_i$, $\exp(x_i)$ will be $really$ large.  This largest exponential will significantly outweigh all of the others combined. Taking the logarithm, i.e. undoing the exponentiation,  we essentially recover the largest of the $x_i$'s.  (Of course, if two of the values were very near one another, we aren't guaranteed to get the true maximum, but it won't be far off!) About this, John Cook says: ""The soft maximum approximates the hard maximum but it also rounds off the corners.""  This couldn't really be said any better. I recall trying to cleverly construct sequences for proofs in advanced calculus where not-everywhere-differentiable operations would have been great to use if they didn't have that pesky non-differentiable trait.  I can't recall a specific incidence where I was tempted to use $max(x_i)$, but this seems at least plausible that it would have come up. Has anyone used this before or have a scenario off hand where it would be useful?","There is a little triviality that has been referred to as the ""soft maximum"" over on John Cook's Blog that I find to be fun, at the very least. The idea is this:  given a list of values, say $x_1,x_2,\ldots,x_n$ , the function $g(x_1,x_2,\ldots,x_n) = \log(\exp(x_1) + \exp(x_2) + \cdots + \exp(x_n))$ returns a value very near the maximum in the list. This happens because that exponentiation exaggerates the differences between the $x_i$ values.  For the largest $x_i$, $\exp(x_i)$ will be $really$ large.  This largest exponential will significantly outweigh all of the others combined. Taking the logarithm, i.e. undoing the exponentiation,  we essentially recover the largest of the $x_i$'s.  (Of course, if two of the values were very near one another, we aren't guaranteed to get the true maximum, but it won't be far off!) About this, John Cook says: ""The soft maximum approximates the hard maximum but it also rounds off the corners.""  This couldn't really be said any better. I recall trying to cleverly construct sequences for proofs in advanced calculus where not-everywhere-differentiable operations would have been great to use if they didn't have that pesky non-differentiable trait.  I can't recall a specific incidence where I was tempted to use $max(x_i)$, but this seems at least plausible that it would have come up. Has anyone used this before or have a scenario off hand where it would be useful?",,"['numerical-methods', 'analysis']"
94,Why can we pass limit under integral sign in proof of solving Poisson's equation? (Evans PDE),Why can we pass limit under integral sign in proof of solving Poisson's equation? (Evans PDE),,"On page 23 of Lawrence Evans' Partial Differential Equations text (2nd edition) he claims that $$\frac{ f( x + he_i - y) - f( x-y)}{h} \to \frac{ \partial f}{ \partial x_i} ( x-y)$$ uniformly on $\mathbb{R}^n$ as $h \to 0$ . So $$\frac{ \partial u}{ \partial x_i } (x) = \int_{\mathbb{R}^n }\Phi(y) \frac{ \partial f}{ \partial x_i} ( x- y) dy$$ Why does this hold true? I have proved uniform convergence as follows: Proof: It suffices to show that $$\underbrace{ \frac{ f( x + h e_i - y ) - f( x-y) }{ h} - \frac{ \partial f}{ \partial x_i } ( x-y)}_{g_h(y)}  \to 0 $$ uniformly as $h \to 0$ . But by the Mean Value Theorem, for each $h$ there exists a $\theta_h$ such that $0 < \theta_h < 1$ and $\frac{ f( x + h e_i - y ) - f( x-y) }{ h}  = f_{x_i} ( x+ \theta_h h e_i - y) $ . So \begin{align*} g_h(y) &=  \frac{ f( x + h e_i - y ) - f( x-y) }{ h} - \frac{ \partial f}{ \partial x_i } ( x-y) \\ &=  f_{x_i} ( x+ \theta_h h e_i - y)  -  f_{x_i} ( x-y)  \end{align*} but $f_{x_i}$ is uniformly continuous ( $f \in C_c^2$ so $f_{ x_i} \in C_c^1$ and therefore continuous on a compact set, so uniformly continuous). Therefore, regardless of $y$ , for small enough $h$ this difference can be made arbitrarily small. So $g_h(y) \to 0$ uniformly, as we intended to show. QED. But how does uniform convergence allow us to move the derivative through the integral sign?","On page 23 of Lawrence Evans' Partial Differential Equations text (2nd edition) he claims that uniformly on as . So Why does this hold true? I have proved uniform convergence as follows: Proof: It suffices to show that uniformly as . But by the Mean Value Theorem, for each there exists a such that and . So but is uniformly continuous ( so and therefore continuous on a compact set, so uniformly continuous). Therefore, regardless of , for small enough this difference can be made arbitrarily small. So uniformly, as we intended to show. QED. But how does uniform convergence allow us to move the derivative through the integral sign?","\frac{ f( x + he_i - y) - f( x-y)}{h} \to \frac{ \partial f}{ \partial x_i} ( x-y) \mathbb{R}^n h \to 0 \frac{ \partial u}{ \partial x_i } (x) = \int_{\mathbb{R}^n }\Phi(y) \frac{ \partial f}{ \partial x_i} ( x- y) dy \underbrace{ \frac{ f( x + h e_i - y ) - f( x-y) }{ h} - \frac{ \partial f}{ \partial x_i } ( x-y)}_{g_h(y)}  \to 0  h \to 0 h \theta_h 0 < \theta_h < 1 \frac{ f( x + h e_i - y ) - f( x-y) }{ h}  = f_{x_i} ( x+ \theta_h h e_i - y)  \begin{align*} g_h(y) &=  \frac{ f( x + h e_i - y ) - f( x-y) }{ h} - \frac{ \partial f}{ \partial x_i } ( x-y) \\
&=  f_{x_i} ( x+ \theta_h h e_i - y)  -  f_{x_i} ( x-y) 
\end{align*} f_{x_i} f \in C_c^2 f_{ x_i} \in C_c^1 y h g_h(y) \to 0","['analysis', 'partial-differential-equations', 'linear-pde', 'poissons-equation']"
95,"In a compact metric space, if we keep adding closed balls centered on boundary, do we always cover the entire space?","In a compact metric space, if we keep adding closed balls centered on boundary, do we always cover the entire space?",,"Let $X$ be a compact connected metric space, and let $W_1=B(x,r)$ denote the closed metric ball centered at $x\in X$ with radius $r$ . We recursively define $W_{k+1}=W_k \cup B(y,r)$ , where $y$ denote a point on the boundary of $W_k$ and $B(y,r)$ is not contained in $W_k$ . Is it true that there always exist $n>0$ such that $W_n=X$ for some $n$ ? If not, is there any additional requirement that can make this true?","Let be a compact connected metric space, and let denote the closed metric ball centered at with radius . We recursively define , where denote a point on the boundary of and is not contained in . Is it true that there always exist such that for some ? If not, is there any additional requirement that can make this true?","X W_1=B(x,r) x\in X r W_{k+1}=W_k \cup B(y,r) y W_k B(y,r) W_k n>0 W_n=X n","['analysis', 'metric-spaces', 'compactness']"
96,Integral form of the conservation law $u_t+f(u)_x=0$,Integral form of the conservation law,u_t+f(u)_x=0,"Consider the  conservation law given by $$u_t+f(u)_x=0$$ We know that in general weak solutions are not smooth but are bounded in $L^{\infty}$ norm (they do not belong to Sobolev spaces). However while deriving the numerical schemes most of the books say, integrating the conservation law over $(a,b) \times (t_1,t_2)$ and applying fundamental theorem of calculus we get $$\int_a^b u(x,t_1)dx - \int_a^b u(x,t_2)dx= -\int_{t_1}^{t_2} f\big(u(b,t)\big)dt+ \int_{t_1}^{t_2} f\big(u(a,t)\big)dt$$ I have the following doubts: How can we perform integration by parts as the solution does not possess any regularity? If a function satisfies the above integral formulation, can we say that it is a weak solution? Conversely, if $u$ is a weak solution, will it satisfy the above integral formulation? If so how to prove it? Thank you.","Consider the  conservation law given by We know that in general weak solutions are not smooth but are bounded in norm (they do not belong to Sobolev spaces). However while deriving the numerical schemes most of the books say, integrating the conservation law over and applying fundamental theorem of calculus we get I have the following doubts: How can we perform integration by parts as the solution does not possess any regularity? If a function satisfies the above integral formulation, can we say that it is a weak solution? Conversely, if is a weak solution, will it satisfy the above integral formulation? If so how to prove it? Thank you.","u_t+f(u)_x=0 L^{\infty} (a,b) \times (t_1,t_2) \int_a^b u(x,t_1)dx - \int_a^b u(x,t_2)dx= -\int_{t_1}^{t_2} f\big(u(b,t)\big)dt+ \int_{t_1}^{t_2} f\big(u(a,t)\big)dt u","['analysis', 'partial-differential-equations', 'numerical-methods', 'hyperbolic-equations']"
97,Regularity of the heat kernel,Regularity of the heat kernel,,"Let $(M,g)$ be a compact Riemannian manifold.  Let $H:M\times M\times\mathbb{R}_{>0}\to\mathbb{R}$ be the heat kernel. i.e. $H\in C^0(M\times M\times\mathbb{R}_{>0})$ is the unique continuous function such that for all $y\in M$, (A) $H^y\in C^{2,1}(M\times\mathbb{R}_{>0})$ (B) $\left(\Delta^g-\dfrac{\partial}{\partial t}\right)H^y=0$ (C) $\displaystyle\lim_{t\to0}H^y_t=\delta_y$ , where $H^y(x,t):=H(x,y,t)$, $H^y_t(x):=H^y(x,t)$, $C^{2,1}(M\times\mathbb{R}_{>0}):=\{\varphi:M\times\mathbb{R}_{>0}\to\mathbb{R}|\text{ For each chart }(U;x^1,\cdots,x^m)\subset M, \dfrac{\partial\varphi}{\partial t}, \dfrac{\partial\varphi}{\partial x^i},\text{ and }\dfrac{\partial^2\varphi}{\partial x^i\partial x^j}:U\times\mathbb{R}_{>0}\to\mathbb{R} \text{ are well defined and continuous.}\}$. ${\bf [Question 1]}$ From (A) and (B) above it is derived that $H^y\in C^\infty(M\times\mathbb{R}_{>0})$ for all $y\in M$. How about the regularity of H as a function on $M\times M\times\mathbb{R}_{>0}$? Does it hold that $H\in C^\infty(M\times M\times \mathbb{R}_{>0})$? If not, aren't there any regularity result of $H:M\times M\times\mathbb{R}_{>0}\to\mathbb{R}$ which is useful to exchange integrals and differentiation? ${\bf [Question 2]}$ Suppose that $F:M\times[0,T]\to \mathbb{R}$ is a continuous function. Then is it true that the function \begin{eqnarray} u(x,t):=-\int_0^t\int_M H(x,y,t-\tau)F(y,\tau)\mu_g(dy)d\tau \end{eqnarray} belongs to $C^{1,0}(M\times[0,T])\cap C^{2,1}(M\times(0,T))$? ${\bf [Question 3]}$ Suppose that $f:M\to\mathbb{R}$ be a $C^1$ function. Does the function \begin{eqnarray} v(x,t):=\int_M H(x,y,t)f(y)\mu_g dy \end{eqnarray} belong to $C^{1,0}(M\times[0,\infty))\cap C^{2,1}(M\times\mathbb{R}_{>0})$? Please tell me also references. Thank you.","Let $(M,g)$ be a compact Riemannian manifold.  Let $H:M\times M\times\mathbb{R}_{>0}\to\mathbb{R}$ be the heat kernel. i.e. $H\in C^0(M\times M\times\mathbb{R}_{>0})$ is the unique continuous function such that for all $y\in M$, (A) $H^y\in C^{2,1}(M\times\mathbb{R}_{>0})$ (B) $\left(\Delta^g-\dfrac{\partial}{\partial t}\right)H^y=0$ (C) $\displaystyle\lim_{t\to0}H^y_t=\delta_y$ , where $H^y(x,t):=H(x,y,t)$, $H^y_t(x):=H^y(x,t)$, $C^{2,1}(M\times\mathbb{R}_{>0}):=\{\varphi:M\times\mathbb{R}_{>0}\to\mathbb{R}|\text{ For each chart }(U;x^1,\cdots,x^m)\subset M, \dfrac{\partial\varphi}{\partial t}, \dfrac{\partial\varphi}{\partial x^i},\text{ and }\dfrac{\partial^2\varphi}{\partial x^i\partial x^j}:U\times\mathbb{R}_{>0}\to\mathbb{R} \text{ are well defined and continuous.}\}$. ${\bf [Question 1]}$ From (A) and (B) above it is derived that $H^y\in C^\infty(M\times\mathbb{R}_{>0})$ for all $y\in M$. How about the regularity of H as a function on $M\times M\times\mathbb{R}_{>0}$? Does it hold that $H\in C^\infty(M\times M\times \mathbb{R}_{>0})$? If not, aren't there any regularity result of $H:M\times M\times\mathbb{R}_{>0}\to\mathbb{R}$ which is useful to exchange integrals and differentiation? ${\bf [Question 2]}$ Suppose that $F:M\times[0,T]\to \mathbb{R}$ is a continuous function. Then is it true that the function \begin{eqnarray} u(x,t):=-\int_0^t\int_M H(x,y,t-\tau)F(y,\tau)\mu_g(dy)d\tau \end{eqnarray} belongs to $C^{1,0}(M\times[0,T])\cap C^{2,1}(M\times(0,T))$? ${\bf [Question 3]}$ Suppose that $f:M\to\mathbb{R}$ be a $C^1$ function. Does the function \begin{eqnarray} v(x,t):=\int_M H(x,y,t)f(y)\mu_g dy \end{eqnarray} belong to $C^{1,0}(M\times[0,\infty))\cap C^{2,1}(M\times\mathbb{R}_{>0})$? Please tell me also references. Thank you.",,"['analysis', 'differential-geometry', 'partial-differential-equations', 'heat-equation', 'regularity-theory-of-pdes']"
98,Sum of a certain series related to the primes,Sum of a certain series related to the primes,,"It is well known that $$\sum_{n > 0}\frac{1}{n}$$ diverges, but $$\sum_{n > 0}\frac{1}{n^2} = \frac{\pi^2}{6}$$ converges.  Similarly, $$\sum_{p}\frac{1}{p}$$ diverges, but $$\sum_{p} \frac{1}{p^2}$$ clearly converges. Is any simple closed form known for this sum, like the one for $\zeta(2)$?","It is well known that $$\sum_{n > 0}\frac{1}{n}$$ diverges, but $$\sum_{n > 0}\frac{1}{n^2} = \frac{\pi^2}{6}$$ converges.  Similarly, $$\sum_{p}\frac{1}{p}$$ diverges, but $$\sum_{p} \frac{1}{p^2}$$ clearly converges. Is any simple closed form known for this sum, like the one for $\zeta(2)$?",,"['analysis', 'elementary-number-theory', 'prime-numbers', 'analytic-number-theory']"
99,A curious problem about Lebesgue measure.,A curious problem about Lebesgue measure.,,"The Problem: Let $(B(x_{m},0.5))_{m}$ be a sequence of disjoint open discs in $\mathbb{R}^{2}$ centered in $x_{m}$ and with radius 0.5. Let $\psi(n)$ be the number of these discs contained in the open disc $B(0,n)$ (that is, the disc centered in (0,0) and with radius $n$). Prove that if $\lim \inf \frac{\psi(n)}{n^{2}} = k > 0$, then there exists a ray starting from (0,0) that crosses an infinite number of the discs $(B(x_{m},0.5))_{m}$. My Thoughts: I find this problem particularly curious. There are several hints below the problem: Use that if $A \subset \mathbb{R}^{2}$ is Lebesgue-measurable and $k \geq 0$ then $kA= \{ kx:x \in A \} $ is Lebesgue-measurable too and $\lambda(kA)=k^{2}\lambda(A)$. Use that $\mu( \cup _{n} A_{n}) < +\infty$ implies $\mu( \lim \sup A_{n}) \geq \lim \inf _{n} \mu (A_{n})$ for any measure $\mu$. I have thought about calling $R_{\alpha}$ to the ray with angle $\alpha$ and $A_{n} = \{ \alpha : R_{\alpha}$ crosses $B(x_{n},0.5) \}$. Then it would be enough to prove that $\lim \sup A_{n} \neq \emptyset$. Using the second hint, it is enought to prove that $\lim \inf \mu(A_{n}) >0$ for certain measure $\mu$. It would be done if I could find a measure such that $\mu(A_{n})=\frac{\psi(n)}{n^{2}}$. I feel it is almost done but I'm stuck for nearly a week. Thanks in advance!","The Problem: Let $(B(x_{m},0.5))_{m}$ be a sequence of disjoint open discs in $\mathbb{R}^{2}$ centered in $x_{m}$ and with radius 0.5. Let $\psi(n)$ be the number of these discs contained in the open disc $B(0,n)$ (that is, the disc centered in (0,0) and with radius $n$). Prove that if $\lim \inf \frac{\psi(n)}{n^{2}} = k > 0$, then there exists a ray starting from (0,0) that crosses an infinite number of the discs $(B(x_{m},0.5))_{m}$. My Thoughts: I find this problem particularly curious. There are several hints below the problem: Use that if $A \subset \mathbb{R}^{2}$ is Lebesgue-measurable and $k \geq 0$ then $kA= \{ kx:x \in A \} $ is Lebesgue-measurable too and $\lambda(kA)=k^{2}\lambda(A)$. Use that $\mu( \cup _{n} A_{n}) < +\infty$ implies $\mu( \lim \sup A_{n}) \geq \lim \inf _{n} \mu (A_{n})$ for any measure $\mu$. I have thought about calling $R_{\alpha}$ to the ray with angle $\alpha$ and $A_{n} = \{ \alpha : R_{\alpha}$ crosses $B(x_{n},0.5) \}$. Then it would be enough to prove that $\lim \sup A_{n} \neq \emptyset$. Using the second hint, it is enought to prove that $\lim \inf \mu(A_{n}) >0$ for certain measure $\mu$. It would be done if I could find a measure such that $\mu(A_{n})=\frac{\psi(n)}{n^{2}}$. I feel it is almost done but I'm stuck for nearly a week. Thanks in advance!",,"['analysis', 'measure-theory', 'geometric-measure-theory', 'lebesgue-measure']"
