,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Condition number for matrix of eigenvectors of a diagonally dominant matrix,Condition number for matrix of eigenvectors of a diagonally dominant matrix,,"Let $A$ be a diagonalizable matrix, i.e., $A=X D X^{-1}$. Recall that columns of $X$ correspond to eigenvectors of $A$, and the diagonal entries of the diagonal matrix $D$ correspond to its eigenvalues. Suppose that $A$ is strictly row/column dominant. In particular assume that $A_{ii}\geq \sum_{j\neq i} |A_{ij}| + c$ for some $c>0$. Similarly for the column sums. Is it possible to choose $X$ such that  the condition number of $X$ is bounded, i.e., can we choose $X$ such that  $\kappa(X)= \|X\|_2 \times \|X^{-1}\|_2$ is bounded (e.g., in terms of $c$)?","Let $A$ be a diagonalizable matrix, i.e., $A=X D X^{-1}$. Recall that columns of $X$ correspond to eigenvectors of $A$, and the diagonal entries of the diagonal matrix $D$ correspond to its eigenvalues. Suppose that $A$ is strictly row/column dominant. In particular assume that $A_{ii}\geq \sum_{j\neq i} |A_{ij}| + c$ for some $c>0$. Similarly for the column sums. Is it possible to choose $X$ such that  the condition number of $X$ is bounded, i.e., can we choose $X$ such that  $\kappa(X)= \|X\|_2 \times \|X^{-1}\|_2$ is bounded (e.g., in terms of $c$)?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition']"
1,"Does $〈Ax,y⟩=〈x,A^*y⟩$ hold for any inner product?",Does  hold for any inner product?,"〈Ax,y⟩=〈x,A^*y⟩","Consider $\Bbb C^n$. Let $A^*$ be the conjugate transpose of matrix $A$, I see in many materials $A^*$ is also called the adjoint of $A$. I am now confused by that does $〈Ax,y⟩=〈x,A^*y⟩$ hold just for the standard inner product (whose proof is simple), or any inner product? Thanks!","Consider $\Bbb C^n$. Let $A^*$ be the conjugate transpose of matrix $A$, I see in many materials $A^*$ is also called the adjoint of $A$. I am now confused by that does $〈Ax,y⟩=〈x,A^*y⟩$ hold just for the standard inner product (whose proof is simple), or any inner product? Thanks!",,"['linear-algebra', 'matrices']"
2,Trace of the multiplication of two matrix,Trace of the multiplication of two matrix,,"1) Let $F$ be a field and $f$ be a linear functional on $M_{n}(F)$. Then there is a nonzero matrix $T$ in $M_{n}(F)$ such that $f(M)=\mathrm{tr}(TM)$ for all $M\subset M_{n}(F)$. Every one help me. Why? 2) Let $F$ be a  field. Consider the a mapping on the algebra of all linear transformations on $F^n$ as follows: $$(T,A)=trace(TA).$$ Clearly, this is an inner product. Then this is non-singular. Why? What's the ralation between two  parts of questions?","1) Let $F$ be a field and $f$ be a linear functional on $M_{n}(F)$. Then there is a nonzero matrix $T$ in $M_{n}(F)$ such that $f(M)=\mathrm{tr}(TM)$ for all $M\subset M_{n}(F)$. Every one help me. Why? 2) Let $F$ be a  field. Consider the a mapping on the algebra of all linear transformations on $F^n$ as follows: $$(T,A)=trace(TA).$$ Clearly, this is an inner product. Then this is non-singular. Why? What's the ralation between two  parts of questions?",,"['linear-algebra', 'matrices', 'functional-analysis', 'linear-transformations', 'matrix-equations']"
3,The algorithm to find an eigenvector of a symmetric tridiagonal matrix associated with a known eigenvalue.,The algorithm to find an eigenvector of a symmetric tridiagonal matrix associated with a known eigenvalue.,,"This question is related to my previous question The algorithm to find the largest eigenvalue and one of its eigenvector of a symmetric tridiagonal matrix? The matrix in question is a symmetric tridiagonal matrix in the form of $$\left( {\begin{array}{*{20}{c}}   {{x_1}}&{{y_1}}&{}&{} \\    {{y_1}}&{{x_2}}& \ddots &{} \\    {}& \ddots & \ddots &{{y_{n - 1}}} \\    {}&{}&{{y_{n - 1}}}&{{x_n}}  \end{array}} \right)$$ where $y_1,...,y_{n-1}$ are all positive numbers. The question is Suppose now we already know an eigenvalue $\lambda$ of it, what is the right way to compute an eigenvector associated with  $\lambda$? There seems to be an naive method, but I am very not sure if it is correct. Suppose the eigenvector is $a=(a_1,...,a_n)$, Set $a_1$ to arbitrary non-zero number if $y_1 \neq \lambda$, and set $a_1 = 0$ otherwise, then solve $a_2$ by $x_1a_1+y_1a_2=\lambda (a_1+a_2)$, then solve $a_3$ by $y_1a_1+x_2a_2+y_2a_3=\lambda (a_1+a_2+a_3)$, and so on so forth. Will it really be so simple?","This question is related to my previous question The algorithm to find the largest eigenvalue and one of its eigenvector of a symmetric tridiagonal matrix? The matrix in question is a symmetric tridiagonal matrix in the form of $$\left( {\begin{array}{*{20}{c}}   {{x_1}}&{{y_1}}&{}&{} \\    {{y_1}}&{{x_2}}& \ddots &{} \\    {}& \ddots & \ddots &{{y_{n - 1}}} \\    {}&{}&{{y_{n - 1}}}&{{x_n}}  \end{array}} \right)$$ where $y_1,...,y_{n-1}$ are all positive numbers. The question is Suppose now we already know an eigenvalue $\lambda$ of it, what is the right way to compute an eigenvector associated with  $\lambda$? There seems to be an naive method, but I am very not sure if it is correct. Suppose the eigenvector is $a=(a_1,...,a_n)$, Set $a_1$ to arbitrary non-zero number if $y_1 \neq \lambda$, and set $a_1 = 0$ otherwise, then solve $a_2$ by $x_1a_1+y_1a_2=\lambda (a_1+a_2)$, then solve $a_3$ by $y_1a_1+x_2a_2+y_2a_3=\lambda (a_1+a_2+a_3)$, and so on so forth. Will it really be so simple?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'tridiagonal-matrices']"
4,Solving a first order linear system matrix,Solving a first order linear system matrix,,"I am attempting to solve a system of equations using a method from a textbook. The problem comes in when I find multiple copies of the same eigenvalue, and I am struggling to find the 3 eigenvectors. Here is the question: Solve $x'(t) = Bx(t)$ where $B$ is \begin{bmatrix} -1 & 2 & -1 \\ 0 & -1 & 2  \\ 0 & 0 & -1\end{bmatrix} The answer is stated to be: $x = (a + (2b - c)t + 2ct^2)e^{-t}$ $y = (b + 2ct)e^{-t}$ $z = ce^{-t}$ $\textbf{Attempt at solution:}$ Since $B$ is an upper triangular matrix it's clear that the determinant is $(-1 -  \lambda)^3 $ which gives us $\lambda_{1,2,3} = -1$ as eigenvalues. After plugging in $\lambda = -1$ for $(B - \lambda I)$ The resulting matrix is \begin{bmatrix} 0 & 2 & -1 \\ 0 & 0 & 2  \\ 0 & 0 & 0\end{bmatrix} Therefore, after multiplying this matrix by-  \begin{bmatrix} x_1  \\ x_2  \\ x_3\end{bmatrix} and setting this equal to 0, I get that: $2x_2 - x_3 = 0$  and  $2x_3 = 0$ I then get the eigenvector $v_1 = (1,0,0)$ (I believe this is correct) I am not sure how to proceed from this point, in terms of finding 2 more eigenvectors to reach that solution.  If someone could provide a walkthrough from this point and show how the answer is reached, that would be incredibly helpful! This is for self-study.","I am attempting to solve a system of equations using a method from a textbook. The problem comes in when I find multiple copies of the same eigenvalue, and I am struggling to find the 3 eigenvectors. Here is the question: Solve $x'(t) = Bx(t)$ where $B$ is \begin{bmatrix} -1 & 2 & -1 \\ 0 & -1 & 2  \\ 0 & 0 & -1\end{bmatrix} The answer is stated to be: $x = (a + (2b - c)t + 2ct^2)e^{-t}$ $y = (b + 2ct)e^{-t}$ $z = ce^{-t}$ $\textbf{Attempt at solution:}$ Since $B$ is an upper triangular matrix it's clear that the determinant is $(-1 -  \lambda)^3 $ which gives us $\lambda_{1,2,3} = -1$ as eigenvalues. After plugging in $\lambda = -1$ for $(B - \lambda I)$ The resulting matrix is \begin{bmatrix} 0 & 2 & -1 \\ 0 & 0 & 2  \\ 0 & 0 & 0\end{bmatrix} Therefore, after multiplying this matrix by-  \begin{bmatrix} x_1  \\ x_2  \\ x_3\end{bmatrix} and setting this equal to 0, I get that: $2x_2 - x_3 = 0$  and  $2x_3 = 0$ I then get the eigenvector $v_1 = (1,0,0)$ (I believe this is correct) I am not sure how to proceed from this point, in terms of finding 2 more eigenvectors to reach that solution.  If someone could provide a walkthrough from this point and show how the answer is reached, that would be incredibly helpful! This is for self-study.",,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
5,Singular value decomposition with zero eigenvalue.,Singular value decomposition with zero eigenvalue.,,I want to calculate the SVD ($A = U\Sigma V^*$)of $$A =  \begin{bmatrix}     0       & 2  \\     0       & 0  \\     0       & 0   \end{bmatrix}$$ but $$A^TA =  \begin{bmatrix}     0       & 0  \\     0       & 4     \end{bmatrix}$$ which has a zero eigenvalue. The problem with this is that the columns of $U$ are given by $$u_i = \frac{Av_i}{\sigma_i}$$ where $\sigma_i = \sqrt{\lambda_i}$.,I want to calculate the SVD ($A = U\Sigma V^*$)of $$A =  \begin{bmatrix}     0       & 2  \\     0       & 0  \\     0       & 0   \end{bmatrix}$$ but $$A^TA =  \begin{bmatrix}     0       & 0  \\     0       & 4     \end{bmatrix}$$ which has a zero eigenvalue. The problem with this is that the columns of $U$ are given by $$u_i = \frac{Av_i}{\sigma_i}$$ where $\sigma_i = \sqrt{\lambda_i}$.,,[]
6,Matrix exponential using the Schur decomposition,Matrix exponential using the Schur decomposition,,"I have a Hermitian $m\times m$ matrix, say $A$. I can use Schur decomposition and transform the matrix in to $A=QTQ^{\dagger}$. Is it then possible to calculate straightforward the matrix exponential using $\exp[A]=Q\cdot\exp[-a T]\cdot Q^{\dagger}$, where $a>1$ is a scalar and $\dagger$ denotes the conjugate transpose of $Q$. Thanks for any suggestion.","I have a Hermitian $m\times m$ matrix, say $A$. I can use Schur decomposition and transform the matrix in to $A=QTQ^{\dagger}$. Is it then possible to calculate straightforward the matrix exponential using $\exp[A]=Q\cdot\exp[-a T]\cdot Q^{\dagger}$, where $a>1$ is a scalar and $\dagger$ denotes the conjugate transpose of $Q$. Thanks for any suggestion.",,"['linear-algebra', 'matrices', 'matrix-exponential', 'schur-decomposition']"
7,Symmetric Part of the inverse of a matrix,Symmetric Part of the inverse of a matrix,,"I've this question that i hope it has an elegant solution. Consider the symmetric part of a matrix $\mathcal{H}\{A\} = \dfrac{A+A^{T}}{2}$ where $A\in\mathbb{R}^{n\times n}$. Furthermore, consider that $\mathcal{H}\{A\}<0$, i.e. is negative-definite. Provided that $A$ is invertible: Is it possible to say that $\mathcal{H}\{A^{-1}\}$ will be also negative-definite? Thanks in advance!","I've this question that i hope it has an elegant solution. Consider the symmetric part of a matrix $\mathcal{H}\{A\} = \dfrac{A+A^{T}}{2}$ where $A\in\mathbb{R}^{n\times n}$. Furthermore, consider that $\mathcal{H}\{A\}<0$, i.e. is negative-definite. Provided that $A$ is invertible: Is it possible to say that $\mathcal{H}\{A^{-1}\}$ will be also negative-definite? Thanks in advance!",,"['linear-algebra', 'matrices', 'linear-transformations', 'matrix-decomposition', 'symmetric-matrices']"
8,Product of upper triangular matrix and lower triangular matrix,Product of upper triangular matrix and lower triangular matrix,,"I need to show the following: Let $A,B \in GL(n)$ so that both of them are upper triangular matrices and  $A*B^t$ or $A^t*B$ is a diagonal matrix. Show that $A$ and $B$ are diagonal matrices as well. I tried it for small $n$ and it is kinda clear but i don't know how to prove it. I tried contraposition but it did not work well. Some help would be appreciated!","I need to show the following: Let $A,B \in GL(n)$ so that both of them are upper triangular matrices and  $A*B^t$ or $A^t*B$ is a diagonal matrix. Show that $A$ and $B$ are diagonal matrices as well. I tried it for small $n$ and it is kinda clear but i don't know how to prove it. I tried contraposition but it did not work well. Some help would be appreciated!",,"['linear-algebra', 'matrices', 'matrix-calculus']"
9,Significance of adding zero rows and columns to a matrix.,Significance of adding zero rows and columns to a matrix.,,"Suppose I have a $n\times n$ matrix to which I add a row of zeros and a column of zeros, somewhere in the matrix, to make it $(n+1)\times (n+1)$. When I multiply the matrix by itself, or multiply it with other matrices where I have inserted a row and column of zeros in the same way, it seems to behave as though the extra row/column were not there. Is there a way to think about the added row/column, and why it does not affect the product? What have I done to this matrix? I think (but not sure at all) that it is like I have put my matrix in a higher matrix space but it only spans the subset where it previously existed?","Suppose I have a $n\times n$ matrix to which I add a row of zeros and a column of zeros, somewhere in the matrix, to make it $(n+1)\times (n+1)$. When I multiply the matrix by itself, or multiply it with other matrices where I have inserted a row and column of zeros in the same way, it seems to behave as though the extra row/column were not there. Is there a way to think about the added row/column, and why it does not affect the product? What have I done to this matrix? I think (but not sure at all) that it is like I have put my matrix in a higher matrix space but it only spans the subset where it previously existed?",,"['linear-algebra', 'matrices']"
10,Why we have elementary operations on matrix?,Why we have elementary operations on matrix?,,"I just started with matrices, so this might be the stupidest question I ever asked. When we had to find the inverse of a matrix $A$, we write $A$ as $A = IA$ then we use elementary operations to convert $A$ on LHS to $I$. The elementary operations I have been taught is $R_i \to kR_j$, $R_i \to R_j$ and $R_i \to R_i + kR_j$. I want to know why we have only these operations and not something like $R_i \to R_i + 10$ ? How did somebody came up this 3 operations ? It would be nice if the answers are a little bit beginner friendly because I don't know much about linear algebra.","I just started with matrices, so this might be the stupidest question I ever asked. When we had to find the inverse of a matrix $A$, we write $A$ as $A = IA$ then we use elementary operations to convert $A$ on LHS to $I$. The elementary operations I have been taught is $R_i \to kR_j$, $R_i \to R_j$ and $R_i \to R_i + kR_j$. I want to know why we have only these operations and not something like $R_i \to R_i + 10$ ? How did somebody came up this 3 operations ? It would be nice if the answers are a little bit beginner friendly because I don't know much about linear algebra.",,['linear-algebra']
11,Find all $3 \times 3$ magic square matrices $M$ such that $M^2$ is also magic,Find all  magic square matrices  such that  is also magic,3 \times 3 M M^2,"A magic square matrix $M$ is a square matrix with real entries such that the sum of the entries in each column, each row, and each main diagonal is the same. The problem is to characterize all $3 \times 3$ magic square matrices $M$ such that $M^2$ is also a magic square matrix. Supposedly, there is an elegant way to do this besides writing out variables and bashing out the multiplication, but I haven't yet found such a solution.","A magic square matrix is a square matrix with real entries such that the sum of the entries in each column, each row, and each main diagonal is the same. The problem is to characterize all magic square matrices such that is also a magic square matrix. Supposedly, there is an elegant way to do this besides writing out variables and bashing out the multiplication, but I haven't yet found such a solution.",M 3 \times 3 M M^2,"['linear-algebra', 'matrices', 'magic-square']"
12,A multiple of the identity?,A multiple of the identity?,,"Suppose that $f:V\to V$ is a linear transformation such that $f$ is represented by the same matrix with respect to every bases for $V$ (i.e., If $\lbrace e_i \rbrace$ and $\lbrace \bar{e_i}  \rbrace$ are different bases for $V$, then the matrices representing $f$ with respect to these different bases are the same). Does this imply that $f:V\to V$ must be a multiple of the identity transformation ?! I appreciate any help. Thanks in advance.","Suppose that $f:V\to V$ is a linear transformation such that $f$ is represented by the same matrix with respect to every bases for $V$ (i.e., If $\lbrace e_i \rbrace$ and $\lbrace \bar{e_i}  \rbrace$ are different bases for $V$, then the matrices representing $f$ with respect to these different bases are the same). Does this imply that $f:V\to V$ must be a multiple of the identity transformation ?! I appreciate any help. Thanks in advance.",,"['linear-algebra', 'matrices', 'linear-transformations']"
13,Interpolation in $SO(3)$ : different approaches,Interpolation in  : different approaches,SO(3),"I am studying rotations and in particular interpolation between 2 matrices $R_1,R_2 \in SO(3)$ which is: find a smooth path between the 2 matrices. I found some slides about it but not yet a good book, I asked the author of the slides and he told me he does not know about a good book about it. The slides are really nice but I need more details. My doubt is rising when he is talking about the interpolation between 2matrices $R_1,R_2 \in SO(3)$. He is trying to do that with different approaches: Approach 1 : Euler angles Approach 2 : use the geodesics of $SO(3)$ Regarding the Approach 1 he says that the interpolation will be not an intuitive motion and the topology will NOT be preserved. What does it mean exactly? Regarding the Approach 2 it does not say it explicitly but I think that you get a really intuitive motion because you are basically moving on a sphere (because you are using the geodesic between the matrix $R_1$ and $R_2$). You can look at the following image to get an idea: Is that correct? Thanks in advance for your help.","I am studying rotations and in particular interpolation between 2 matrices $R_1,R_2 \in SO(3)$ which is: find a smooth path between the 2 matrices. I found some slides about it but not yet a good book, I asked the author of the slides and he told me he does not know about a good book about it. The slides are really nice but I need more details. My doubt is rising when he is talking about the interpolation between 2matrices $R_1,R_2 \in SO(3)$. He is trying to do that with different approaches: Approach 1 : Euler angles Approach 2 : use the geodesics of $SO(3)$ Regarding the Approach 1 he says that the interpolation will be not an intuitive motion and the topology will NOT be preserved. What does it mean exactly? Regarding the Approach 2 it does not say it explicitly but I think that you get a really intuitive motion because you are basically moving on a sphere (because you are using the geodesic between the matrix $R_1$ and $R_2$). You can look at the following image to get an idea: Is that correct? Thanks in advance for your help.",,"['linear-algebra', 'matrices', 'rotations', 'orthogonal-matrices']"
14,Exact norm of matrix exponential,Exact norm of matrix exponential,,"It seems that there is no way of computing $$ \lVert e^{tA}\lVert $$ for an arbitrary real square matrix $A$ and any matrix norm $\lVert\cdot\lVert$, in terms of $t \in \mathbb{R}$ and $\lVert A \lVert$, i.e. $$f:(t,\lVert A \lVert ) \mapsto \lVert e^{tA}\lVert$$ without explicitly performing the matrix exponential (exactly or approximately). Is that right? Because so far I could only find upper-bound estimates. If this problem in its full generality is not solved, can anyone say something about its solvability? For my particular case it would suffice to restrict to the case $$A=A^\mathrm{T}, \quad \mathrm{tr}(A) = 0$$ meaning $\left(e^A\right)^\mathrm{T} = e^A$ and $\det(e^A) = 1$. Thanks for any suggestions. edit: since it is very unlikely that such an $f$ exists, I think the requirement should be eased to $$f:(t, A  ) \mapsto  \lVert e^{tA} \lVert$$ just somehow circumventing exponentiation.","It seems that there is no way of computing $$ \lVert e^{tA}\lVert $$ for an arbitrary real square matrix $A$ and any matrix norm $\lVert\cdot\lVert$, in terms of $t \in \mathbb{R}$ and $\lVert A \lVert$, i.e. $$f:(t,\lVert A \lVert ) \mapsto \lVert e^{tA}\lVert$$ without explicitly performing the matrix exponential (exactly or approximately). Is that right? Because so far I could only find upper-bound estimates. If this problem in its full generality is not solved, can anyone say something about its solvability? For my particular case it would suffice to restrict to the case $$A=A^\mathrm{T}, \quad \mathrm{tr}(A) = 0$$ meaning $\left(e^A\right)^\mathrm{T} = e^A$ and $\det(e^A) = 1$. Thanks for any suggestions. edit: since it is very unlikely that such an $f$ exists, I think the requirement should be eased to $$f:(t, A  ) \mapsto  \lVert e^{tA} \lVert$$ just somehow circumventing exponentiation.",,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-calculus', 'matrix-exponential']"
15,Can a perturbation of a matrix product always be represented as product of perturbations of its factor matrices?,Can a perturbation of a matrix product always be represented as product of perturbations of its factor matrices?,,"Given $A\in\mathbb{R}^{m\times n}$ with $A=BC$ for some $B\in\mathbb{R}^{m\times k}, C\in\mathbb{R}^{k\times n}$. Assume that $k\geq\min(m,n)$ so that this decomposition always exists for every $A\in\mathbb{R}^{m\times n}.$ Can we prove that for any perturbation $A'$ of $A$, there always exist some perturbations $B',C'$ of $B,C$ such that $A'=B'C'?$  More precisely, let $\epsilon>0$ and $A'$ satisfying $||A-A'||<\epsilon$ for some suitable norm. Show that there always exist two matrices $B',C'$ which satisfy the following conditions: a) $A'=B'C'$ b) $||B'-B||$ and $||C'-C||$ can be upper-bounded by $\epsilon$ and other constant terms. The second condition implies that $B'$ and $C'$ can be chosen to be sufficiently close to $B$ and $C$ as long as $A'$ is sufficiently close to $A.$ Moreover, note that there is no constraint on $B$ and $C$, except that $k\geq\min(m,n).$ Intuitively i think it should be possible but i cannot prove/disprove it. If this is not possible then what conditions are needed for such existence of $B',C'$? Thanks,","Given $A\in\mathbb{R}^{m\times n}$ with $A=BC$ for some $B\in\mathbb{R}^{m\times k}, C\in\mathbb{R}^{k\times n}$. Assume that $k\geq\min(m,n)$ so that this decomposition always exists for every $A\in\mathbb{R}^{m\times n}.$ Can we prove that for any perturbation $A'$ of $A$, there always exist some perturbations $B',C'$ of $B,C$ such that $A'=B'C'?$  More precisely, let $\epsilon>0$ and $A'$ satisfying $||A-A'||<\epsilon$ for some suitable norm. Show that there always exist two matrices $B',C'$ which satisfy the following conditions: a) $A'=B'C'$ b) $||B'-B||$ and $||C'-C||$ can be upper-bounded by $\epsilon$ and other constant terms. The second condition implies that $B'$ and $C'$ can be chosen to be sufficiently close to $B$ and $C$ as long as $A'$ is sufficiently close to $A.$ Moreover, note that there is no constraint on $B$ and $C$, except that $k\geq\min(m,n).$ Intuitively i think it should be possible but i cannot prove/disprove it. If this is not possible then what conditions are needed for such existence of $B',C'$? Thanks,",,"['linear-algebra', 'matrices', 'spectral-theory', 'perturbation-theory', 'random-matrices']"
16,Diagonalizable Matrices vs Hermitian matrices,Diagonalizable Matrices vs Hermitian matrices,,Currently all i know is: All hermitian matrices are diagonlizable but not vice versa. The eigenvectors of a diagonlizable matrix can form a Basis for the vector space it operates on. The eigenvectors for a Hermitian matrix can form an orthogonal or orthonormal Basis for the vector space it operates on. And its eigenvalues have to be real. Is what I have said correct? Is there anything im missing?  Thank you in advance.,Currently all i know is: All hermitian matrices are diagonlizable but not vice versa. The eigenvectors of a diagonlizable matrix can form a Basis for the vector space it operates on. The eigenvectors for a Hermitian matrix can form an orthogonal or orthonormal Basis for the vector space it operates on. And its eigenvalues have to be real. Is what I have said correct? Is there anything im missing?  Thank you in advance.,,"['matrices', 'eigenvalues-eigenvectors']"
17,Pseudoinverse of $2\times 2$ matrix,Pseudoinverse of  matrix,2\times 2,"How can I find the Moore-Penrose pseudoinverse of the $2 \times 2$ complex matrix $$A=\begin{pmatrix}0&a\\0&b\end{pmatrix}$$ for $a \neq 0$ and $b \neq 0$? Here I want to use the limit formula $$A^+=\lim_{\epsilon \to 0}  (\epsilon I+A^*A)^{-1}A^*$$ since $\mbox{rank}(A)=1$, which is not full rank. Any help, please?","How can I find the Moore-Penrose pseudoinverse of the $2 \times 2$ complex matrix $$A=\begin{pmatrix}0&a\\0&b\end{pmatrix}$$ for $a \neq 0$ and $b \neq 0$? Here I want to use the limit formula $$A^+=\lim_{\epsilon \to 0}  (\epsilon I+A^*A)^{-1}A^*$$ since $\mbox{rank}(A)=1$, which is not full rank. Any help, please?",,"['linear-algebra', 'matrices', 'pseudoinverse']"
18,"Prove that if A and B are doubly-stochastic matrices of order n, then AB is also a doubly-stochastic matrix","Prove that if A and B are doubly-stochastic matrices of order n, then AB is also a doubly-stochastic matrix",,"Prove that if A and B are doubly-stochastic matrices of order n, then AB is also a doubly-stochastic matrix. My attempt: Since the size of A is nxn and the size of B is nxn, then the size of AB is also nxn. Let A = ($a_{ij}$) and B = ($b_{ij}$). Consider the sum of the entries in the $i$th row of AB: $a_{i1}b_{11} + a_{i2}b_{21} + a_{i3}b_{31} + ... + a_{in}b_{n1}$ + $a_{i1}b_{12} + a_{i2}b_{22} + a_{i3}b_{32} + ... + a_{in}b_{n2}$ + $...a_{in}b_{nn}$ = $a_{i1}(b_{11} + b_{12} + ... + b_{1n}) + a_{i2}(b_{21} + b_{22} + ... + b_{2n}) + ...+ a_{in}(b_{n1} + b_{n2} + ... + b_{nn})$ = $a_{i1} + a_{i2} + ... + a_{in}$ = $1$ Next, I would consider the sum of the entries in the jth column of AB. However, at this point, I'm stuck. So, I'm wondering if my method is possible. Is there also a better way to prove this?","Prove that if A and B are doubly-stochastic matrices of order n, then AB is also a doubly-stochastic matrix. My attempt: Since the size of A is nxn and the size of B is nxn, then the size of AB is also nxn. Let A = ($a_{ij}$) and B = ($b_{ij}$). Consider the sum of the entries in the $i$th row of AB: $a_{i1}b_{11} + a_{i2}b_{21} + a_{i3}b_{31} + ... + a_{in}b_{n1}$ + $a_{i1}b_{12} + a_{i2}b_{22} + a_{i3}b_{32} + ... + a_{in}b_{n2}$ + $...a_{in}b_{nn}$ = $a_{i1}(b_{11} + b_{12} + ... + b_{1n}) + a_{i2}(b_{21} + b_{22} + ... + b_{2n}) + ...+ a_{in}(b_{n1} + b_{n2} + ... + b_{nn})$ = $a_{i1} + a_{i2} + ... + a_{in}$ = $1$ Next, I would consider the sum of the entries in the jth column of AB. However, at this point, I'm stuck. So, I'm wondering if my method is possible. Is there also a better way to prove this?",,"['linear-algebra', 'matrices']"
19,What would be an algorithm to shut down all lamps?,What would be an algorithm to shut down all lamps?,,"I found an algorithm problem that I was unable to solve from http://www.ohjelmointiputka.net/postit/tehtava.php?tunnus=muslam . I guess as it requires linear algebra, this might a correct place to ask an algorithm. We have $n$ lamp to form a circular loop with $n$ on/off switches. At the beginning, some lamps are on, denoted by the bit 1 and some are off, denoted by the bit 0. We have been given an integer $m$. Pressing each switch changes the state of the lamp that correspond the switch and its $m$ nearest lamps in each direction. Find out how many switches one has to press to shut down all the lamps and give a list of the pressed switches. I tried to think that I need to solve a matrix equation over $\mathbb Z/2\mathbb Z$ but found out that the coefficient matrix I made has no inverse. I thought that I need a coefficient matrix such that $A_{i,j}=1$ if pressing switch $i$ changes the lamp state $j$ and $A_{i,j}=0$ if it does not change. Here is my code for the case there are six lamps with initial states 101101 and every switch affects to the given lamp and one of its nearest lamps in both directions. # Generate the zero matrix def generate_matrix(w, h):     return [[0 for x in range(w)] for y in range(h)]  # Print matrix def print_matrix(A):     for row in range(0,len(A[0])):         line = """"         for col in range(0,len(A[0])):             line += str(A[row][col])             if col == len(A[0]):                 line += ""\n""         print(line)  # Swap two rows def swap_rows(A,i,j):     for k in range(0,len(A[0])):         temp = A[k][i]         A[k][i] = A[k][j]         A[k][j] = temp     return A  # Add one row to another def mult_rows(A,i,j):     for k in range(0,len(A[0])):         A[k][j] += A[k][i]     return A  # Distance between two indices def dist(i,j,n):     d1 = max(i-j,j-i)     dist_beg = min(i,j)     dist_end = n-max(i,j)     d2 = dist_beg + dist_end     return min(d1,d2)  # Initialize the coefficient matrix def gen_coeff_matrix(n,effect_width):     A = generate_matrix(n,n)     for i in range(0,n):         for j in range(0,n):             if dist(i,j,n) <= effect_width:                 A[i][j] = 1     return A  # Initialize the unit matrix def init_unit_matrix(n):     A = generate_matrix(n,n)     for i in range(0,n):         A[i][i] = 1     return A  # Inverse of the matrix def inv(M):     n = len(M[0])     U = init_unit_matrix(n)     for column in range(0,n):         if M[column][column] == 0:             i = 1             while M[column+i][column+i] != 0:                 ++i             swap_rows(A,column,column+i)             swap_rows(U,column,column+i)         for c in range(0,n):             if M[c][column] == 1:                 if c != column:                     mult_rows(M,c,column)                     mult_rows(U,c,column)     return U  A = init_unit_matrix(5) B = gen_coeff_matrix(6,1) print_matrix(B) It gives the matrix 110001 111000 011100 001110 000111 100011 but it is not invertible according to Sage: sage: F = FiniteField(2) sage: C = Matrix(F,[[1,1,0,0,0,1],[1,1,1,0,0,0],[0,1,1,1,0,0],[0,0,1,1,1,0],[0,0,0,1,1,1],[1,0,0,0,1,1]]) sage: C [1 1 0 0 0 1] [1 1 1 0 0 0] [0 1 1 1 0 0] [0 0 1 1 1 0] [0 0 0 1 1 1] [1 0 0 0 1 1] sage: C^(-1) --------------------------------------------------------------------------- ZeroDivisionError                         Traceback (most recent call last) <ipython-input-4-48e78ab0a5de> in <module>() ----> 1 C**(-Integer(1))  sage/matrix/matrix0.pyx in sage.matrix.matrix0.Matrix.__pow__ (/usr/lib/sagemath//src/build/cythonized/sage/matrix/matrix0.c:36015)()  sage/structure/element.pyx in sage.structure.element.generic_power_c (/usr/lib/sagemath//src/build/cythonized/sage/structure/element.c:29266)()  sage/matrix/matrix_mod2_dense.pyx in sage.matrix.matrix_mod2_dense.Matrix_mod2_dense.__invert__ (/usr/lib/sagemath//src/build/cythonized/sage/matrix/matrix_mod2_dense.c:7868)()  ZeroDivisionError: Matrix does not have full rank. I also tried with the following command: sage: V=Matrix(F,[[1,0,1,1,0,1]]) sage: C\V  ValueError: number of rows of self must equal number of rows of B Questions: Is this the right way to think about the problem? Is there a mistake in the reasoning how to make the coefficient matrix? Is the just a silly bug in my code?","I found an algorithm problem that I was unable to solve from http://www.ohjelmointiputka.net/postit/tehtava.php?tunnus=muslam . I guess as it requires linear algebra, this might a correct place to ask an algorithm. We have $n$ lamp to form a circular loop with $n$ on/off switches. At the beginning, some lamps are on, denoted by the bit 1 and some are off, denoted by the bit 0. We have been given an integer $m$. Pressing each switch changes the state of the lamp that correspond the switch and its $m$ nearest lamps in each direction. Find out how many switches one has to press to shut down all the lamps and give a list of the pressed switches. I tried to think that I need to solve a matrix equation over $\mathbb Z/2\mathbb Z$ but found out that the coefficient matrix I made has no inverse. I thought that I need a coefficient matrix such that $A_{i,j}=1$ if pressing switch $i$ changes the lamp state $j$ and $A_{i,j}=0$ if it does not change. Here is my code for the case there are six lamps with initial states 101101 and every switch affects to the given lamp and one of its nearest lamps in both directions. # Generate the zero matrix def generate_matrix(w, h):     return [[0 for x in range(w)] for y in range(h)]  # Print matrix def print_matrix(A):     for row in range(0,len(A[0])):         line = """"         for col in range(0,len(A[0])):             line += str(A[row][col])             if col == len(A[0]):                 line += ""\n""         print(line)  # Swap two rows def swap_rows(A,i,j):     for k in range(0,len(A[0])):         temp = A[k][i]         A[k][i] = A[k][j]         A[k][j] = temp     return A  # Add one row to another def mult_rows(A,i,j):     for k in range(0,len(A[0])):         A[k][j] += A[k][i]     return A  # Distance between two indices def dist(i,j,n):     d1 = max(i-j,j-i)     dist_beg = min(i,j)     dist_end = n-max(i,j)     d2 = dist_beg + dist_end     return min(d1,d2)  # Initialize the coefficient matrix def gen_coeff_matrix(n,effect_width):     A = generate_matrix(n,n)     for i in range(0,n):         for j in range(0,n):             if dist(i,j,n) <= effect_width:                 A[i][j] = 1     return A  # Initialize the unit matrix def init_unit_matrix(n):     A = generate_matrix(n,n)     for i in range(0,n):         A[i][i] = 1     return A  # Inverse of the matrix def inv(M):     n = len(M[0])     U = init_unit_matrix(n)     for column in range(0,n):         if M[column][column] == 0:             i = 1             while M[column+i][column+i] != 0:                 ++i             swap_rows(A,column,column+i)             swap_rows(U,column,column+i)         for c in range(0,n):             if M[c][column] == 1:                 if c != column:                     mult_rows(M,c,column)                     mult_rows(U,c,column)     return U  A = init_unit_matrix(5) B = gen_coeff_matrix(6,1) print_matrix(B) It gives the matrix 110001 111000 011100 001110 000111 100011 but it is not invertible according to Sage: sage: F = FiniteField(2) sage: C = Matrix(F,[[1,1,0,0,0,1],[1,1,1,0,0,0],[0,1,1,1,0,0],[0,0,1,1,1,0],[0,0,0,1,1,1],[1,0,0,0,1,1]]) sage: C [1 1 0 0 0 1] [1 1 1 0 0 0] [0 1 1 1 0 0] [0 0 1 1 1 0] [0 0 0 1 1 1] [1 0 0 0 1 1] sage: C^(-1) --------------------------------------------------------------------------- ZeroDivisionError                         Traceback (most recent call last) <ipython-input-4-48e78ab0a5de> in <module>() ----> 1 C**(-Integer(1))  sage/matrix/matrix0.pyx in sage.matrix.matrix0.Matrix.__pow__ (/usr/lib/sagemath//src/build/cythonized/sage/matrix/matrix0.c:36015)()  sage/structure/element.pyx in sage.structure.element.generic_power_c (/usr/lib/sagemath//src/build/cythonized/sage/structure/element.c:29266)()  sage/matrix/matrix_mod2_dense.pyx in sage.matrix.matrix_mod2_dense.Matrix_mod2_dense.__invert__ (/usr/lib/sagemath//src/build/cythonized/sage/matrix/matrix_mod2_dense.c:7868)()  ZeroDivisionError: Matrix does not have full rank. I also tried with the following command: sage: V=Matrix(F,[[1,0,1,1,0,1]]) sage: C\V  ValueError: number of rows of self must equal number of rows of B Questions: Is this the right way to think about the problem? Is there a mistake in the reasoning how to make the coefficient matrix? Is the just a silly bug in my code?",,"['linear-algebra', 'matrices', 'algorithms', 'sagemath']"
20,How to prove an equality,How to prove an equality,,"The matrices $A,B,C$ all $2\times 2$ dimensions so: $$A^2 +B^2 +C^2 =AB+BC+CA.$$  Prove that $$(A^2 +B^2 +C^2 -BA-CB-AC)^2=O_2$$ Can someone help me with this? Thank you.","The matrices $A,B,C$ all $2\times 2$ dimensions so: $$A^2 +B^2 +C^2 =AB+BC+CA.$$  Prove that $$(A^2 +B^2 +C^2 -BA-CB-AC)^2=O_2$$ Can someone help me with this? Thank you.",,['matrices']
21,Finding the minimal polynomial of a matrix in the real vector space of continuous real-valued functions.,Finding the minimal polynomial of a matrix in the real vector space of continuous real-valued functions.,,"I'm studying for a linear algebra final and I'm working through some old test problems: In the real vector space of continuous real-valued functions defined on $\mathbb{R}$ consider the following functions $p_i, i =0,1,2$ and $\exp$ defined as follows: $p_i(x)=x^i$ and $exp(x)=e^x$ for all $x\in \mathbb{R}$. Set $V=span_\mathbb{R}\{p_0,p_1,p_2,\exp\}$ and consider the endomorphism $\sigma: V\to V$ defined as $$(\sigma f)(x):= f(x-1) \text{ for all } x\in \mathbb{R}$$    Determine the matrix representation, characteristic polynomial, eigenspaces, and the minimal polynomial of $\sigma$. Is $\sigma$ diagonalizable? I think I've got everything but the minimal polynomial (please correct me if I'm wrong). I've determined that the matrix representation is $$A:=[\sigma]_\beta^\beta = \begin{pmatrix} 1 & -1 & 1 & 0\\ 0 & 1 & -2 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & e^{-1}  \end{pmatrix} $$ Also, the characteristic polynomial $\chi_A = (x-1)^3(x-e^{-1})$ and I found the eigenspaces to be $$\left\langle \begin{pmatrix} p_0\\ 0\\ 0\\ 0 \end{pmatrix} \right\rangle \text{ and } \left\langle \begin{pmatrix} 0\\ 0\\ 0\\ \exp \end{pmatrix} \right\rangle$$ Thus since the eigenvalue $1$ has algebraic multiplicity $3$ but geometric multiplicity $1$, $\sigma$ is not diagonalizable. I know that the minimal polynomial $M_A$ divides $\chi_A$, so $M_A$ is either $(x-1)(x-e^{-1})$, $(x-1)^2(x-e^{-1})$ or $(x-1)^3(x-e^{-1})$. In theory, I know that if the polynomial with smallest degree with $A$ as a root is the minimal polynomial, but to check this seems computationally cumbersome for a paper and pencil exam. Is there some better way?","I'm studying for a linear algebra final and I'm working through some old test problems: In the real vector space of continuous real-valued functions defined on $\mathbb{R}$ consider the following functions $p_i, i =0,1,2$ and $\exp$ defined as follows: $p_i(x)=x^i$ and $exp(x)=e^x$ for all $x\in \mathbb{R}$. Set $V=span_\mathbb{R}\{p_0,p_1,p_2,\exp\}$ and consider the endomorphism $\sigma: V\to V$ defined as $$(\sigma f)(x):= f(x-1) \text{ for all } x\in \mathbb{R}$$    Determine the matrix representation, characteristic polynomial, eigenspaces, and the minimal polynomial of $\sigma$. Is $\sigma$ diagonalizable? I think I've got everything but the minimal polynomial (please correct me if I'm wrong). I've determined that the matrix representation is $$A:=[\sigma]_\beta^\beta = \begin{pmatrix} 1 & -1 & 1 & 0\\ 0 & 1 & -2 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & e^{-1}  \end{pmatrix} $$ Also, the characteristic polynomial $\chi_A = (x-1)^3(x-e^{-1})$ and I found the eigenspaces to be $$\left\langle \begin{pmatrix} p_0\\ 0\\ 0\\ 0 \end{pmatrix} \right\rangle \text{ and } \left\langle \begin{pmatrix} 0\\ 0\\ 0\\ \exp \end{pmatrix} \right\rangle$$ Thus since the eigenvalue $1$ has algebraic multiplicity $3$ but geometric multiplicity $1$, $\sigma$ is not diagonalizable. I know that the minimal polynomial $M_A$ divides $\chi_A$, so $M_A$ is either $(x-1)(x-e^{-1})$, $(x-1)^2(x-e^{-1})$ or $(x-1)^3(x-e^{-1})$. In theory, I know that if the polynomial with smallest degree with $A$ as a root is the minimal polynomial, but to check this seems computationally cumbersome for a paper and pencil exam. Is there some better way?",,"['linear-algebra', 'matrices', 'minimal-polynomials']"
22,How to find Generator Matrix from a given Parity Check Matrix?,How to find Generator Matrix from a given Parity Check Matrix?,,I'm given a Parity Check Matrix \begin{bmatrix}0&1&1&1&1&0&0\\1&0&1&1&0&1&0\\ 1&1&0&1&0&0&1\end{bmatrix} and I have to find the Generator Matrix of it.I spent many days try to solve it but I can't,I'm given a Parity Check Matrix \begin{bmatrix}0&1&1&1&1&0&0\\1&0&1&1&0&1&0\\ 1&1&0&1&0&0&1\end{bmatrix} and I have to find the Generator Matrix of it.I spent many days try to solve it but I can't,,"['linear-algebra', 'matrices']"
23,why is E(XX') the covariance matrix,why is E(XX') the covariance matrix,,"In regression people often refer to the $X'X$ term as the estimate of $E(xx')$ which makes sense by LLN. However, people also refer to $E(xx')$ as the covariance matrix of $x$ which only seems to make sense if $E(x)=0$. Why does it makes sense if the $x$'s are not demeaned?","In regression people often refer to the $X'X$ term as the estimate of $E(xx')$ which makes sense by LLN. However, people also refer to $E(xx')$ as the covariance matrix of $x$ which only seems to make sense if $E(x)=0$. Why does it makes sense if the $x$'s are not demeaned?",,"['matrices', 'regression', 'covariance']"
24,Linear nilpotent functions,Linear nilpotent functions,,"Let $A \in M_{n}(K)$ be a nilpotent matrix, where $M_n(K)$ is the space of $n \times n$ matrices over the field $K$. Prove that the following statements are equivalent : (i) $\mu _A = \chi _A (= \lambda^n).$ (The minimal and characteristic polynomials are equal.) (ii) $A^{n-1}\neq 0. $ (iii) $\operatorname{rank} A =n{-}1.$ (iv) There exists a vector $x \in K^n$ such that $A^i x$, $i = 0, \dots, n-1$ is a $K$-basis of $K^n.$ I could prove the implications (i) $\iff$ (ii). To prove that (ii) $\iff$ (iii), I have with me the following example: Let $v_1, v_2, \dots, v_n$ be a basis of $K^n$. Define a $K$-linear map $f:K^n \to K^n$ where $f(v_i):=v_{i+1},\ i=1,...,n{-}1$ and $f(v_n)=0$. This is a nilpotent function with $f^{n-1}\neq 0$ and $\operatorname{rank} f=n{-}1$. Now the question is: Can any nilpotent matrix with rank $n-1$ be expressed as the above function by change of basis? How can I prove that such a basis exists? If I can do this, the proof follows. Or else please suggest other ways to prove the equivalence of the statements.","Let $A \in M_{n}(K)$ be a nilpotent matrix, where $M_n(K)$ is the space of $n \times n$ matrices over the field $K$. Prove that the following statements are equivalent : (i) $\mu _A = \chi _A (= \lambda^n).$ (The minimal and characteristic polynomials are equal.) (ii) $A^{n-1}\neq 0. $ (iii) $\operatorname{rank} A =n{-}1.$ (iv) There exists a vector $x \in K^n$ such that $A^i x$, $i = 0, \dots, n-1$ is a $K$-basis of $K^n.$ I could prove the implications (i) $\iff$ (ii). To prove that (ii) $\iff$ (iii), I have with me the following example: Let $v_1, v_2, \dots, v_n$ be a basis of $K^n$. Define a $K$-linear map $f:K^n \to K^n$ where $f(v_i):=v_{i+1},\ i=1,...,n{-}1$ and $f(v_n)=0$. This is a nilpotent function with $f^{n-1}\neq 0$ and $\operatorname{rank} f=n{-}1$. Now the question is: Can any nilpotent matrix with rank $n-1$ be expressed as the above function by change of basis? How can I prove that such a basis exists? If I can do this, the proof follows. Or else please suggest other ways to prove the equivalence of the statements.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'nilpotence', 'cayley-hamilton']"
25,$e^Ae^B$ when $A$ and $B$ anticommute,when  and  anticommute,e^Ae^B A B,"I know that if $A,B$ ($n\times n$ real matrices) commute, then $e^{A}e^B = e^{A+B}$.  Is there a similar identity when $A,B$ anti commute?","I know that if $A,B$ ($n\times n$ real matrices) commute, then $e^{A}e^B = e^{A+B}$.  Is there a similar identity when $A,B$ anti commute?",,"['linear-algebra', 'matrices', 'exponentiation']"
26,Inverse of $ I_{T}+AA^{\prime }$,Inverse of, I_{T}+AA^{\prime },"Suppose $I_{T}$ is a $T\times T$ identity matrix, and $A$ is a $T\times m$ matrix where $m<T.$ Is there any simple way to calculate the inverse of $% I_{T}+AA^{\prime },$ $\left( I_{T}+AA^{\prime }\right) ^{-1},$ if $% I_{T}+AA^{\prime }$ is invertible$?$ When $a$ is a vector, there is a simple way to calculate the inverse of $I_{T}+aa^{\prime }$. So, except for the Woodbury matrix identity, is there any simple way?","Suppose $I_{T}$ is a $T\times T$ identity matrix, and $A$ is a $T\times m$ matrix where $m<T.$ Is there any simple way to calculate the inverse of $% I_{T}+AA^{\prime },$ $\left( I_{T}+AA^{\prime }\right) ^{-1},$ if $% I_{T}+AA^{\prime }$ is invertible$?$ When $a$ is a vector, there is a simple way to calculate the inverse of $I_{T}+aa^{\prime }$. So, except for the Woodbury matrix identity, is there any simple way?",,"['matrices', 'inverse']"
27,Geometric interpretation of non-square matrices,Geometric interpretation of non-square matrices,,I realize that a $n\times{n}$ matrix can be interpreted as linear transformation of a vector in n-dimensional coordinate system. But I am not able to interpret any $m\times{n}$ matrix same way since $m\times{n}$ doesn't mention all the coordinates. How  can this type of transformation be visualized?,I realize that a $n\times{n}$ matrix can be interpreted as linear transformation of a vector in n-dimensional coordinate system. But I am not able to interpret any $m\times{n}$ matrix same way since $m\times{n}$ doesn't mention all the coordinates. How  can this type of transformation be visualized?,,"['linear-algebra', 'matrices', 'linear-transformations']"
28,Why is the matrix $(I - A)$ theoretically singular?,Why is the matrix  theoretically singular?,(I - A),"I've the following Matlab code to compute the eigenvector using the inverse iteration (or power) method: A = p * G * D + delta; x = (I − A) \ e; x = x / sum(x); taken from the 4th page of this chapter about the pagerank algorithm by Cleve Moler. First, I don't see how this code relates to the ""typical"" inverse iteration method. If you know how, I would appreciate an explanation. But my main question in this post is: why $(I - A)$ is theoretically singular? A should be sparse. Would this help? The author says: Because I − A is theoretically singular, with exact computation some diagonal element of the upper triangular factor of $I − A$ should be zero and this computation should fail. Why some of the diagonals should be $0$?","I've the following Matlab code to compute the eigenvector using the inverse iteration (or power) method: A = p * G * D + delta; x = (I − A) \ e; x = x / sum(x); taken from the 4th page of this chapter about the pagerank algorithm by Cleve Moler. First, I don't see how this code relates to the ""typical"" inverse iteration method. If you know how, I would appreciate an explanation. But my main question in this post is: why $(I - A)$ is theoretically singular? A should be sparse. Would this help? The author says: Because I − A is theoretically singular, with exact computation some diagonal element of the upper triangular factor of $I − A$ should be zero and this computation should fail. Why some of the diagonals should be $0$?",,['matrices']
29,Left Inverses of a Matrix,Left Inverses of a Matrix,,"I am confused about left inverses after examining two particular problems. From what I understand in order to find the left inverse of an $m\times n$ matrix I must first transpose it then augment it with the identity matrix that would result after multiplying $A$ by an unknown $X$. After this I should perform RREF as far as I can and whatever I'm left with (barring any inconsistent rows) will be the columns of $X$ (the inverse of $A$). At that point it should be true that $XA = I$, however I do not get that for the first problem. I checked to make sure the first matrix does not have a right inverse so I don't know why $AX$ produces the identity matrix when it was the left inverse that was being solved for. $A = \begin{bmatrix}1 & 2 \\ 2 & 5 \\ 3 & 7\end{bmatrix}$ $\begin{bmatrix}1 & 2 & 3 & | & 1 & 0\\ 2 & 5 & 7 & | & 0 & 1\\\end{bmatrix} => RREF => \begin{bmatrix}1 & 0 & 1 & | & 5 & | & -2\\0 & 1 & 1 & | & -2 & | & 1\end{bmatrix}$ Therefore, $X = \begin{bmatrix}5-r & -2-t \\ -2-r & 1-t \\ r & t\end{bmatrix}$ (See correction) However $XA$ does not produce the identity matrix while $AX$ does. Is this because I transposed $A$ in order to augment it and find $X$? As far as I know, $X$ should be the left inverse, so $XA$ should work. What really confuses me is that for this second matrix, $B$, I attempt the exact same thing and find that $XB = I$ as expected. $B = \begin{bmatrix}2 & -1 \\ 4 & -1 \\ 2 & 2\end{bmatrix}$ $\begin{bmatrix}2 & 4 & 2 & | & 1 & 0\\ -1 & -1 & 2 & | & 0 & 1\end{bmatrix} => RREF => \begin{bmatrix}2 & 4 & 2 &| & 1 & 0 \\ 0 & 1 & 3 & | & 1/2 & 1\end{bmatrix}$ Therefore, $X = \begin{bmatrix}-1/2+5s & 1/2-3s & s \\ -2+5t & 1-3t & t\end{bmatrix}$ Could someone please explain what is happening? I strongly suspect it has to do with taking the transpose, but why then does it work for one and not the other? The problems are both ""cooked"" so I know there are definitely left inverses for both. **** Correction / Solved **** $X = \begin{bmatrix}5-r & -2-r & r\\ -2-t & 1-t & t\\\end{bmatrix}$","I am confused about left inverses after examining two particular problems. From what I understand in order to find the left inverse of an $m\times n$ matrix I must first transpose it then augment it with the identity matrix that would result after multiplying $A$ by an unknown $X$. After this I should perform RREF as far as I can and whatever I'm left with (barring any inconsistent rows) will be the columns of $X$ (the inverse of $A$). At that point it should be true that $XA = I$, however I do not get that for the first problem. I checked to make sure the first matrix does not have a right inverse so I don't know why $AX$ produces the identity matrix when it was the left inverse that was being solved for. $A = \begin{bmatrix}1 & 2 \\ 2 & 5 \\ 3 & 7\end{bmatrix}$ $\begin{bmatrix}1 & 2 & 3 & | & 1 & 0\\ 2 & 5 & 7 & | & 0 & 1\\\end{bmatrix} => RREF => \begin{bmatrix}1 & 0 & 1 & | & 5 & | & -2\\0 & 1 & 1 & | & -2 & | & 1\end{bmatrix}$ Therefore, $X = \begin{bmatrix}5-r & -2-t \\ -2-r & 1-t \\ r & t\end{bmatrix}$ (See correction) However $XA$ does not produce the identity matrix while $AX$ does. Is this because I transposed $A$ in order to augment it and find $X$? As far as I know, $X$ should be the left inverse, so $XA$ should work. What really confuses me is that for this second matrix, $B$, I attempt the exact same thing and find that $XB = I$ as expected. $B = \begin{bmatrix}2 & -1 \\ 4 & -1 \\ 2 & 2\end{bmatrix}$ $\begin{bmatrix}2 & 4 & 2 & | & 1 & 0\\ -1 & -1 & 2 & | & 0 & 1\end{bmatrix} => RREF => \begin{bmatrix}2 & 4 & 2 &| & 1 & 0 \\ 0 & 1 & 3 & | & 1/2 & 1\end{bmatrix}$ Therefore, $X = \begin{bmatrix}-1/2+5s & 1/2-3s & s \\ -2+5t & 1-3t & t\end{bmatrix}$ Could someone please explain what is happening? I strongly suspect it has to do with taking the transpose, but why then does it work for one and not the other? The problems are both ""cooked"" so I know there are definitely left inverses for both. **** Correction / Solved **** $X = \begin{bmatrix}5-r & -2-r & r\\ -2-t & 1-t & t\\\end{bmatrix}$",,"['linear-algebra', 'matrices', 'inverse', 'transpose']"
30,Lev Landau's hard question about matrices,Lev Landau's hard question about matrices,,"I heard that the famous Russian physicist Lev Landau once asked one student who wanted to work in his group to find the optimal solution to the following problem: Given a matrix $A \in \mathbb{C}^{n \times n}$ with $A^*=-A$ and zero trace, assume $\mathbb{C}^{n \times n}$ carries the Frobenius inner-product. Assume that $A$ is normalized, then we can find (by Gram-Schmidt) an ONB of all skew-Hermitian matrices with zero trace including $A$ . So $A,M_1,...,M_{n^2-2}$ form an ONB of the vector space of skew-Hermitian matrices with zero trace. Question: Is there now a matrix $B^*=-B$ that does not commute with $A$ but with all other matrices of the ONB? Or if that is impossible, is there at least a matrix that is somehow good, in the sense that it does not commute with $A$ but with many other matrices of the ONB? If anything is unclear, please let me know.","I heard that the famous Russian physicist Lev Landau once asked one student who wanted to work in his group to find the optimal solution to the following problem: Given a matrix with and zero trace, assume carries the Frobenius inner-product. Assume that is normalized, then we can find (by Gram-Schmidt) an ONB of all skew-Hermitian matrices with zero trace including . So form an ONB of the vector space of skew-Hermitian matrices with zero trace. Question: Is there now a matrix that does not commute with but with all other matrices of the ONB? Or if that is impossible, is there at least a matrix that is somehow good, in the sense that it does not commute with but with many other matrices of the ONB? If anything is unclear, please let me know.","A \in \mathbb{C}^{n \times n} A^*=-A \mathbb{C}^{n \times n} A A A,M_1,...,M_{n^2-2} B^*=-B A A","['calculus', 'linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
31,How to solve the optimization problem $\mathbf{F}=\arg\min_{\mathbf{F}} tr((\mathbf{G}^H\mathbf{F}^H\mathbf{F}\mathbf{G}+\alpha\mathbf{I})^{-1})$,How to solve the optimization problem,\mathbf{F}=\arg\min_{\mathbf{F}} tr((\mathbf{G}^H\mathbf{F}^H\mathbf{F}\mathbf{G}+\alpha\mathbf{I})^{-1}),"I have an optimization problem, which tries to minimize the trace $\mathrm{trace}((\mathbf{G}^H\mathbf{F}^H\mathbf{F}\mathbf{G}+\alpha\mathbf{I})^{-1})$ with respect to $\mathbf{F}$, where $\mathbf{G}$ is a given complex matrix of size $N\times K$ ($N>K$), and $\mathbf{F}$ is of size $L\times N$ ($L<N$) which has the power constraint $||\mathbf{F}||^2_{\text{F}}\leq K$ (frobenius norm), $\alpha$ is a positive constant, $\mathbf{I}$ is an identity matrix of size $K\times K$. That is, $$ \begin{array} \,\min_{ F\in\mathbb{C}^{L\times N} }  &  \mathrm{trace}((\mathbf{G}^H\mathbf{F}^H\mathbf{F}\mathbf{G}+\alpha\mathbf{I})^{-1}) \\ \mathrm{such \;\;that} %&  %G\in \mathbb{C}^{N\times K} %\\ & ||\mathbf{F}||^2_{\text{F}}\leq K %& \end{array} $$ Are there any experts aware of this optimization problem? Thanks in advance!","I have an optimization problem, which tries to minimize the trace $\mathrm{trace}((\mathbf{G}^H\mathbf{F}^H\mathbf{F}\mathbf{G}+\alpha\mathbf{I})^{-1})$ with respect to $\mathbf{F}$, where $\mathbf{G}$ is a given complex matrix of size $N\times K$ ($N>K$), and $\mathbf{F}$ is of size $L\times N$ ($L<N$) which has the power constraint $||\mathbf{F}||^2_{\text{F}}\leq K$ (frobenius norm), $\alpha$ is a positive constant, $\mathbf{I}$ is an identity matrix of size $K\times K$. That is, $$ \begin{array} \,\min_{ F\in\mathbb{C}^{L\times N} }  &  \mathrm{trace}((\mathbf{G}^H\mathbf{F}^H\mathbf{F}\mathbf{G}+\alpha\mathbf{I})^{-1}) \\ \mathrm{such \;\;that} %&  %G\in \mathbb{C}^{N\times K} %\\ & ||\mathbf{F}||^2_{\text{F}}\leq K %& \end{array} $$ Are there any experts aware of this optimization problem? Thanks in advance!",,"['matrices', 'optimization', 'calculus-of-variations', 'matrix-calculus']"
32,Inequality of product of matrices,Inequality of product of matrices,,"Is this inequality true? $X^T P Y \ge \lambda_{min}(P)X^TY $  if   $P$ is a positive definite matrix and $Y=sgn(X)$ where $X$ is a vector, $sgn(X)$ is a vector which its elements are the sign of the elements of the vector $X$, $\lambda_{min}(P)$ is the minimum eigenvalue of $P$. Generally, can you give my an inequality that relates $X^T P Y$ to $X^T Y$?","Is this inequality true? $X^T P Y \ge \lambda_{min}(P)X^TY $  if   $P$ is a positive definite matrix and $Y=sgn(X)$ where $X$ is a vector, $sgn(X)$ is a vector which its elements are the sign of the elements of the vector $X$, $\lambda_{min}(P)$ is the minimum eigenvalue of $P$. Generally, can you give my an inequality that relates $X^T P Y$ to $X^T Y$?",,"['matrices', 'inequality']"
33,Matrix decomposition into square positive integer matrices,Matrix decomposition into square positive integer matrices,,"This is an attempt at an analogy with prime numbers. Let's consider only square matrices with positive integer entries. Which of them are 'prime' and how to decompose such a matrix in general? To illustrate, there is a product of two general $2 \times 2$ matrices: $$AB=\left[ \begin{matrix} a_{11} &  a_{12} \\  a_{21} &  a_{22} \end{matrix} \right] \left[ \begin{matrix} b_{11} &  b_{12} \\  b_{21} &  b_{22} \end{matrix} \right]=\left[ \begin{matrix} a_{11} b_{11}+a_{12} b_{21} &  a_{11} b_{12}+a_{12} b_{22} \\  a_{21} b_{11}+a_{22} b_{21} &  a_{21} b_{12}+a_{22} b_{22} \end{matrix} \right]$$ Exchanging $a$ and $b$ we obtain the expression for the other product $BA$. Now, if we allow zero, negative and/or rational entries we can probably decompose any matrix in an infinite number of ways. However, if we restrict ourselves: $$a_{jk},~b_{jk} \in \mathbb{N}$$ The problem becomes well defined. Is there an algorithm to decompose an arbitrary square positive integer matrix into a product of several positive integer matrices of the same dimensions? There is a set of matrices which can'be be decomposed, just like the prime numbers (or irreducible polynomials, for example). The most trivial one is (remember, zero entries are not allowed): $$\left[ \begin{matrix} 1 & 1 \\  1 &  1 \end{matrix} \right]$$ There are no natural numbers $a_{11},b_{11},a_{12},b_{21}$, such that: $$a_{11} b_{11}+a_{12} b_{21}=1$$ The same extends to any dimension $d$. Any 'composite' $d \times d$ matrix will have all entries $ \geq d$. Thus, for square matrices we can name several more 'primes': $$\left[ \begin{matrix} 2 & 1 \\  1 &  1 \end{matrix} \right],~~~\left[ \begin{matrix} 1 & 2 \\  1 &  1 \end{matrix} \right],~~~\left[ \begin{matrix} 1 & 1 \\  2 &  1 \end{matrix} \right],~~~\left[ \begin{matrix} 1 & 1 \\  1 &  2 \end{matrix} \right],~~~\left[ \begin{matrix} 2 & 2 \\  1 &  1 \end{matrix} \right],~~~\left[ \begin{matrix} 1 & 1 \\  2 &  2 \end{matrix} \right], \dots$$ And in general, any matrix which has at least one entry equal to $1$. It makes sense, that most entries in 'composite' matrices will be large, since we are multiplying and adding natural numbers. For example: $$\left[ \begin{matrix} 1 &  2 &  4 \\  3 &  3 &  1 \\  3 &  4 &  4 \end{matrix} \right] \left[ \begin{matrix} 2 &  5 &  5 \\  4 &  5 &  5 \\  5 &  1 &  4 \end{matrix} \right]=\left[ \begin{matrix} 30 &  19 &  31 \\  23 &  31 &  34 \\  42 &  39 &  51 \end{matrix} \right]$$ $$\left[ \begin{matrix} 2 &  5 &  5 \\  4 &  5 &  5 \\  5 &  1 &  4 \end{matrix} \right] \left[ \begin{matrix} 1 &  2 &  4 \\  3 &  3 &  1 \\  3 &  4 &  4 \end{matrix} \right] =\left[ \begin{matrix} 32 &  39 &  33 \\  34 &  43 &  41 \\  20 &  29 &  37 \end{matrix} \right]$$ If no decomposition algorithm for this case exists, is it at least possible to recognize a matrix that can't be decomposed according to the above rules?","This is an attempt at an analogy with prime numbers. Let's consider only square matrices with positive integer entries. Which of them are 'prime' and how to decompose such a matrix in general? To illustrate, there is a product of two general $2 \times 2$ matrices: $$AB=\left[ \begin{matrix} a_{11} &  a_{12} \\  a_{21} &  a_{22} \end{matrix} \right] \left[ \begin{matrix} b_{11} &  b_{12} \\  b_{21} &  b_{22} \end{matrix} \right]=\left[ \begin{matrix} a_{11} b_{11}+a_{12} b_{21} &  a_{11} b_{12}+a_{12} b_{22} \\  a_{21} b_{11}+a_{22} b_{21} &  a_{21} b_{12}+a_{22} b_{22} \end{matrix} \right]$$ Exchanging $a$ and $b$ we obtain the expression for the other product $BA$. Now, if we allow zero, negative and/or rational entries we can probably decompose any matrix in an infinite number of ways. However, if we restrict ourselves: $$a_{jk},~b_{jk} \in \mathbb{N}$$ The problem becomes well defined. Is there an algorithm to decompose an arbitrary square positive integer matrix into a product of several positive integer matrices of the same dimensions? There is a set of matrices which can'be be decomposed, just like the prime numbers (or irreducible polynomials, for example). The most trivial one is (remember, zero entries are not allowed): $$\left[ \begin{matrix} 1 & 1 \\  1 &  1 \end{matrix} \right]$$ There are no natural numbers $a_{11},b_{11},a_{12},b_{21}$, such that: $$a_{11} b_{11}+a_{12} b_{21}=1$$ The same extends to any dimension $d$. Any 'composite' $d \times d$ matrix will have all entries $ \geq d$. Thus, for square matrices we can name several more 'primes': $$\left[ \begin{matrix} 2 & 1 \\  1 &  1 \end{matrix} \right],~~~\left[ \begin{matrix} 1 & 2 \\  1 &  1 \end{matrix} \right],~~~\left[ \begin{matrix} 1 & 1 \\  2 &  1 \end{matrix} \right],~~~\left[ \begin{matrix} 1 & 1 \\  1 &  2 \end{matrix} \right],~~~\left[ \begin{matrix} 2 & 2 \\  1 &  1 \end{matrix} \right],~~~\left[ \begin{matrix} 1 & 1 \\  2 &  2 \end{matrix} \right], \dots$$ And in general, any matrix which has at least one entry equal to $1$. It makes sense, that most entries in 'composite' matrices will be large, since we are multiplying and adding natural numbers. For example: $$\left[ \begin{matrix} 1 &  2 &  4 \\  3 &  3 &  1 \\  3 &  4 &  4 \end{matrix} \right] \left[ \begin{matrix} 2 &  5 &  5 \\  4 &  5 &  5 \\  5 &  1 &  4 \end{matrix} \right]=\left[ \begin{matrix} 30 &  19 &  31 \\  23 &  31 &  34 \\  42 &  39 &  51 \end{matrix} \right]$$ $$\left[ \begin{matrix} 2 &  5 &  5 \\  4 &  5 &  5 \\  5 &  1 &  4 \end{matrix} \right] \left[ \begin{matrix} 1 &  2 &  4 \\  3 &  3 &  1 \\  3 &  4 &  4 \end{matrix} \right] =\left[ \begin{matrix} 32 &  39 &  33 \\  34 &  43 &  41 \\  20 &  29 &  37 \end{matrix} \right]$$ If no decomposition algorithm for this case exists, is it at least possible to recognize a matrix that can't be decomposed according to the above rules?",,"['linear-algebra', 'matrices', 'diophantine-equations', 'prime-factorization', 'matrix-decomposition']"
34,"Two diagonal matrices each other's entries rearranged (same eigenvalues and multiplicities), are they similar?","Two diagonal matrices each other's entries rearranged (same eigenvalues and multiplicities), are they similar?",,"Two diagonal matrices each other's entries rearranged (same eigenvalues and multiplicities), are they similar? This seems like such a simple question, but I can't quite see a connection. I want to say that the two diagonal matrices can commute with each other, and this gives us the algebraic edge to show their similarity, but I'm generally bad with abstract algebra proofs.","Two diagonal matrices each other's entries rearranged (same eigenvalues and multiplicities), are they similar? This seems like such a simple question, but I can't quite see a connection. I want to say that the two diagonal matrices can commute with each other, and this gives us the algebraic edge to show their similarity, but I'm generally bad with abstract algebra proofs.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
35,"Given block matrix $M$, show determinant relationship between $M$ and the block elements of $M.$","Given block matrix , show determinant relationship between  and the block elements of",M M M.,"Given that $M = \begin{pmatrix} A & B \\ C &D \end{pmatrix}$ and $M^{-1} = \begin{pmatrix} P & Q \\ R & S \end{pmatrix},$ where $A, B,\dots$ are $k \times k$ matrices, show that $\det(M) \cdot \det(S) = \det(A).$ Gone through sheets of paper on this one... I feel like there is a trick and a quick answer.","Given that $M = \begin{pmatrix} A & B \\ C &D \end{pmatrix}$ and $M^{-1} = \begin{pmatrix} P & Q \\ R & S \end{pmatrix},$ where $A, B,\dots$ are $k \times k$ matrices, show that $\det(M) \cdot \det(S) = \det(A).$ Gone through sheets of paper on this one... I feel like there is a trick and a quick answer.",,"['linear-algebra', 'matrices', 'determinant']"
36,Multiplication of unitary matrices to make symmetric off-diagonal elements zero,Multiplication of unitary matrices to make symmetric off-diagonal elements zero,,"Context Starting with a unitary matrix $U$ of size $m \times m$, I have read of a way to obtain a diagonal matrix by sequentially multiplying $U$ from the right by unitary matrices $V$ of a certain form. Each of these matrices is the identity matrix, except that the elements $V_{ab}$, $V_{ba}$, $V_{aa}$, $V_{bb}$ are replaced by elements of a $2\times2$ unitary matrix, chosen such that the target element $U^{\mathrm{new}}_{ab}$ of the output matrix $U^{\mathrm{new}}=U^{\mathrm{old}}V$ becomes zero. For the diagonalization, the sequential multiplications are performed to make all off-diagonal elements of each row zero, starting with the last. When a row has been completed in this way, the off-diagonal elements of the corresponding column have automatically become zero as well. Problem My question is why the off-diagonal elements of the corresponding column become zero if the off-diagonal elements of a row are made zero in this way? Example As a $3\times 3$ example, starting with this unitary matrix $$U=\left(         \begin{matrix}         0.5000+0.0000i & 0.5000+0.5000i & -0.5000+0.0000i \\         0.0000-0.5774i & 0.0000+0.5774i & 0.5774+0.0000i \\         0.0000+0.6455i & 0.3873+0.1291i & 0.5164+0.3873i \\         \end{matrix}\right) $$ the first steps that lead to a diagonalized last row and column would be achieved with the following matrices $$V1=\left(         \begin{matrix}         1.0000+0.0000i & 0.0000+0.0000i & 0.0000+0.0000i \\         0.0000+0.0000i & -0.8452+0.0000i & 0.5345+0.0000i \\         0.0000+0.0000i & 0.5071-0.1690i & 0.8018-0.2673i \\         \end{matrix}\right) $$ $$V2=\left(         \begin{matrix}         -0.7638+0.0000i & 0.0000+0.0000i & 0.6455+0.0000i \\         0.0000+0.0000i & 1.0000+0.0000i & 0.0000+0.0000i \\         0.2041+0.6124i & 0.0000+0.0000i & 0.2415+0.7246i \\         \end{matrix}\right). $$ The results of the first two multiplication steps are: $$U\times V1=\left(         \begin{matrix}         0.5000+0.0000i & -0.6761-0.3381i & -0.1336+0.4009i \\         0.0000-0.5774i & 0.2928-0.5855i & 0.4629+0.1543i \\         0.0000+0.6455i & 0.0000+0.0000i & 0.7246+0.2415i \\         \end{matrix}\right) $$ $$U\times V1\times V2=\left(         \begin{matrix}         -0.6547+0.0000i & -0.6761-0.3381i & 0.0000+0.0000i \\         0.0000+0.7559i & 0.2928-0.5855i & 0.0000+0.0000i \\         0.0000+0.0000i & 0.0000+0.0000i & 0.0000+1.0000i \\         \end{matrix}\right), $$ where as expected the last row and column have zeros as off-diagonal entries. Known property Since $U^{\mathrm{old}}$ and $V$ are both unitary, so is their product $U^{\mathrm{new}}$. However, based on that alone I do not see why the effect on the columns happens. If anyone knows an explanation, it would be a great help!","Context Starting with a unitary matrix $U$ of size $m \times m$, I have read of a way to obtain a diagonal matrix by sequentially multiplying $U$ from the right by unitary matrices $V$ of a certain form. Each of these matrices is the identity matrix, except that the elements $V_{ab}$, $V_{ba}$, $V_{aa}$, $V_{bb}$ are replaced by elements of a $2\times2$ unitary matrix, chosen such that the target element $U^{\mathrm{new}}_{ab}$ of the output matrix $U^{\mathrm{new}}=U^{\mathrm{old}}V$ becomes zero. For the diagonalization, the sequential multiplications are performed to make all off-diagonal elements of each row zero, starting with the last. When a row has been completed in this way, the off-diagonal elements of the corresponding column have automatically become zero as well. Problem My question is why the off-diagonal elements of the corresponding column become zero if the off-diagonal elements of a row are made zero in this way? Example As a $3\times 3$ example, starting with this unitary matrix $$U=\left(         \begin{matrix}         0.5000+0.0000i & 0.5000+0.5000i & -0.5000+0.0000i \\         0.0000-0.5774i & 0.0000+0.5774i & 0.5774+0.0000i \\         0.0000+0.6455i & 0.3873+0.1291i & 0.5164+0.3873i \\         \end{matrix}\right) $$ the first steps that lead to a diagonalized last row and column would be achieved with the following matrices $$V1=\left(         \begin{matrix}         1.0000+0.0000i & 0.0000+0.0000i & 0.0000+0.0000i \\         0.0000+0.0000i & -0.8452+0.0000i & 0.5345+0.0000i \\         0.0000+0.0000i & 0.5071-0.1690i & 0.8018-0.2673i \\         \end{matrix}\right) $$ $$V2=\left(         \begin{matrix}         -0.7638+0.0000i & 0.0000+0.0000i & 0.6455+0.0000i \\         0.0000+0.0000i & 1.0000+0.0000i & 0.0000+0.0000i \\         0.2041+0.6124i & 0.0000+0.0000i & 0.2415+0.7246i \\         \end{matrix}\right). $$ The results of the first two multiplication steps are: $$U\times V1=\left(         \begin{matrix}         0.5000+0.0000i & -0.6761-0.3381i & -0.1336+0.4009i \\         0.0000-0.5774i & 0.2928-0.5855i & 0.4629+0.1543i \\         0.0000+0.6455i & 0.0000+0.0000i & 0.7246+0.2415i \\         \end{matrix}\right) $$ $$U\times V1\times V2=\left(         \begin{matrix}         -0.6547+0.0000i & -0.6761-0.3381i & 0.0000+0.0000i \\         0.0000+0.7559i & 0.2928-0.5855i & 0.0000+0.0000i \\         0.0000+0.0000i & 0.0000+0.0000i & 0.0000+1.0000i \\         \end{matrix}\right), $$ where as expected the last row and column have zeros as off-diagonal entries. Known property Since $U^{\mathrm{old}}$ and $V$ are both unitary, so is their product $U^{\mathrm{new}}$. However, based on that alone I do not see why the effect on the columns happens. If anyone knows an explanation, it would be a great help!",,"['linear-algebra', 'matrices', 'complex-numbers']"
37,Explicit solution to a Rayleigh quotient equation,Explicit solution to a Rayleigh quotient equation,,"For 5 months! I have been struggling to solve the following equations analytically without numeric method (i,e, Newton method): Main equation: $$  \biggl(M^2-\cfrac{\mathbf{x^{\text{T}}}M^2\mathbf{x}}{\mathbf{x^{\text{T}}}\mathbf{x}}E\biggr)\mathbf{x}=\mathbf{1} $$ Constraint equations: $$ \begin{cases}  \mathbf{x^{\text{T}}1}=0 \\ \\ \mathbf{x^{\text{T}}x}=u  \end{cases} $$ where $\{M,E\}\in\mathbf{R}^{n \times n}$ and $\{\mathbf{1},\mathbf{x}\}\in\mathbf{R}^n$ are defined, then $M$ is an arbitrary symmetric matrix, $E$   is an identical matrix,   $\mathbf{1}$ is all one vector, $\mathbf{x}$ is a   variable vector and $u\in\mathbf{R}$ is a scalar.   Furthermore, as a knowledge, the below equation form is called Rayleigh   quotient $R(M^2,\mathbf{x})$: $$R(M^2,\mathbf{x}):=\cfrac{\mathbf{x^{\text{T}}}M^2\mathbf{x}}{\mathbf{x^{\text{T}}}\mathbf{x}}$$ Now, we attempt to estimate the $\mathbf{x}$. Does the analytic solution or method exist? My ability is shortage but, I guess that this problem has a beautiful solution. Also, main equation is a simultaneous cubic equation. Theoretically, this is solvable. Just, this is my theme question. Furthermore, same question is already asked on math overflow . Then answerers provided worthful information which may be solution to clue.","For 5 months! I have been struggling to solve the following equations analytically without numeric method (i,e, Newton method): Main equation: $$  \biggl(M^2-\cfrac{\mathbf{x^{\text{T}}}M^2\mathbf{x}}{\mathbf{x^{\text{T}}}\mathbf{x}}E\biggr)\mathbf{x}=\mathbf{1} $$ Constraint equations: $$ \begin{cases}  \mathbf{x^{\text{T}}1}=0 \\ \\ \mathbf{x^{\text{T}}x}=u  \end{cases} $$ where $\{M,E\}\in\mathbf{R}^{n \times n}$ and $\{\mathbf{1},\mathbf{x}\}\in\mathbf{R}^n$ are defined, then $M$ is an arbitrary symmetric matrix, $E$   is an identical matrix,   $\mathbf{1}$ is all one vector, $\mathbf{x}$ is a   variable vector and $u\in\mathbf{R}$ is a scalar.   Furthermore, as a knowledge, the below equation form is called Rayleigh   quotient $R(M^2,\mathbf{x})$: $$R(M^2,\mathbf{x}):=\cfrac{\mathbf{x^{\text{T}}}M^2\mathbf{x}}{\mathbf{x^{\text{T}}}\mathbf{x}}$$ Now, we attempt to estimate the $\mathbf{x}$. Does the analytic solution or method exist? My ability is shortage but, I guess that this problem has a beautiful solution. Also, main equation is a simultaneous cubic equation. Theoretically, this is solvable. Just, this is my theme question. Furthermore, same question is already asked on math overflow . Then answerers provided worthful information which may be solution to clue.",,"['linear-algebra', 'matrices', 'vectors', 'systems-of-equations']"
38,Determinant of $N \times\ N$ matrix,Determinant of  matrix,N \times\ N,"So the question asks: For $n \geq 2$, compute the determinant of the following matrix:   $$  B =  \begin{bmatrix}   -X     &  1     & 0      & \cdots &  0      & 0      \\    0     & -X     & 1      & \ddots & \vdots  & \vdots \\   \vdots & \ddots & \ddots & \ddots &  0      & \vdots \\   \vdots &        & \ddots & \ddots &  1      & 0      \\   0      & \cdots & \cdots & 0      & -X      & 1      \\   a_0    & a_1    & \cdots & \cdots & a_{n-2} & (a_{n-1} - X)  \end{bmatrix} $$ Looking at the $2 \times 2$ and $3 \times 3$ forms of this matrix: $\det \begin{bmatrix} -X & 0 \\ 0 & (a_1-X) \end{bmatrix} = -X(a_1-X) - 0 = X^2 - a_1X $ by expansion along the first row: $\det \begin{bmatrix} -X & 1 & 0  \\ 0 & -X & 0 \\ 0 & 0 & (a_2-X) \end{bmatrix} = (-X) \times\det \begin{bmatrix} -X & 0 \\ 0 & a_2-X \end{bmatrix} - 1  \det\begin{bmatrix} 0 & 0 \\ 0 & a_2-X \end{bmatrix}$ $= (-X)[(-X)(a_2-X) -0] - 0 = X^3 - a_2X^2  $ So it looks like: $\det  \begin{bmatrix}   -X     &  1     & 0      & \cdots &  0      & 0      \\    0     & -X     & 1      & \ddots & \vdots  & \vdots \\   \vdots & \ddots & \ddots & \ddots &  0      & \vdots \\   \vdots &        & \ddots & \ddots &  1      & 0      \\   0      & \cdots & \cdots & 0      & -X      & 1      \\   a_0    & a_1    & \cdots & \cdots & a_{n-2} & (a_{n-1} - X)  \end{bmatrix} = X^{n} - a_{n-1}X^{n-1} - a_{n-2}X^{n-2}  ... - a_1X$ Does this look right? Is ""prove by induction"" valid to use here?","So the question asks: For $n \geq 2$, compute the determinant of the following matrix:   $$  B =  \begin{bmatrix}   -X     &  1     & 0      & \cdots &  0      & 0      \\    0     & -X     & 1      & \ddots & \vdots  & \vdots \\   \vdots & \ddots & \ddots & \ddots &  0      & \vdots \\   \vdots &        & \ddots & \ddots &  1      & 0      \\   0      & \cdots & \cdots & 0      & -X      & 1      \\   a_0    & a_1    & \cdots & \cdots & a_{n-2} & (a_{n-1} - X)  \end{bmatrix} $$ Looking at the $2 \times 2$ and $3 \times 3$ forms of this matrix: $\det \begin{bmatrix} -X & 0 \\ 0 & (a_1-X) \end{bmatrix} = -X(a_1-X) - 0 = X^2 - a_1X $ by expansion along the first row: $\det \begin{bmatrix} -X & 1 & 0  \\ 0 & -X & 0 \\ 0 & 0 & (a_2-X) \end{bmatrix} = (-X) \times\det \begin{bmatrix} -X & 0 \\ 0 & a_2-X \end{bmatrix} - 1  \det\begin{bmatrix} 0 & 0 \\ 0 & a_2-X \end{bmatrix}$ $= (-X)[(-X)(a_2-X) -0] - 0 = X^3 - a_2X^2  $ So it looks like: $\det  \begin{bmatrix}   -X     &  1     & 0      & \cdots &  0      & 0      \\    0     & -X     & 1      & \ddots & \vdots  & \vdots \\   \vdots & \ddots & \ddots & \ddots &  0      & \vdots \\   \vdots &        & \ddots & \ddots &  1      & 0      \\   0      & \cdots & \cdots & 0      & -X      & 1      \\   a_0    & a_1    & \cdots & \cdots & a_{n-2} & (a_{n-1} - X)  \end{bmatrix} = X^{n} - a_{n-1}X^{n-1} - a_{n-2}X^{n-2}  ... - a_1X$ Does this look right? Is ""prove by induction"" valid to use here?",,"['linear-algebra', 'matrices', 'polynomials', 'proof-verification', 'determinant']"
39,higher dimensional Pauli matrices?,higher dimensional Pauli matrices?,,"I'm trying to find $3 \times 3$ matrices with some similarity to Pauli matrices. I have some candidates, but they are not perfect. I'm not sure if perfect versions exist, and that is my question. Recall for Pauli matrices $p_k$: $\{p_i,p_j\} = p_ip_j + p_jp_i = 2 \delta_{ij}$ I'm looking for matrices that satisfy: $\{a_i,a_j,a_k\} = 3! \delta_{ijk}$ where $\{A,B,C\}$ is the sum over all permutations of three symbols: $\{A,B,C\} = ABC + BCA + CAB + ACB + CBA + BAC$ My candidate matrices are: $a_1 = \begin{bmatrix} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0\\ \end{bmatrix}$ $a_2 = \begin{bmatrix} 0 & 0 & a\\ b & 0 & 0\\ 0 & c & 0\\ \end{bmatrix}$ $a_3 = \begin{bmatrix} 0 & a & 0\\ 0 & 0 & b\\ c & 0 & 0\\ \end{bmatrix}$ $a_4 = \begin{bmatrix} a & 0 & 0\\ 0 & b & 0\\ 0 & 0 & c\\ \end{bmatrix}$ where $a,b,c$ are the three third roots of unity (and have the property $a + b + c = 0$). And of course, unlike Pauli matrices, the eigenvalues of these matrices are not real (they are $a,b,c$), which is expected since they are permutation matrices with $a^3 = I$. Now, they have the desired property of: $\{a_1,a_1,a_1\} = 3\\ \{a_1,a_1,a_2\} = 0\\ \{a_1,a_2,a_3\} = 0\\ \{a_1,a_3,a_4\} = 0\\ \cdots$ However:  $\{a_2,a_3,a_4\} = -3I$ So my question is, are there a set of 4 matrices that satisfy $\{a_i,a_j,a_k\} = 3! \delta_{ijk}$ We have 3 such matrices $a_1,a_2,a_3$, but do there exist 4? I'm 90% sure the answer is no, but I'd like to be 100%. And we can extend the problem to higher dimensions: For example, the $d = 4$ case. Define: $\{A,B,C,D\}$ as the sum over all permutations of four symbols. How many matrices can we find that satisfy: $\{a_i,a_j,a_k,a_l\} = 4! \delta_{ijkl}$ It suspect it might be 3 for all higher d. Now, where did this problem come from? Consider: $M_1 = u*a_1 + v*a_2 + w*a_3$ such that $M_1^3 = (u^3 + v^3 + w^3)I$ And $M_2 = u*a_1 + v*a_2 + x*a_3 + y*a_4$ such that $M_2^3 = (u^3 + v^3 + x^3 + y^3)I$ And their determinants: $\det(M_1) = u^3 + v^3 + w^3\\ \det(M_2) = u^3 + v^3 + x^3 + y^3 -3vxy$ So an alternative definition of the problem, do there exist matrices, not equal to the identity matrix, $a_1,a_2,a_3,a_4$ such that $\det(M_2)$ doesn't have the cross term? Similarly for higher $d$, where the determinant gets even messier! eg: $d = 4, M = u*b_1 + v*b_2 + x*b_3 + y*b_4$ and $\det(M) = u^4 + v^4 + x^4 + y^4 + mess$","I'm trying to find $3 \times 3$ matrices with some similarity to Pauli matrices. I have some candidates, but they are not perfect. I'm not sure if perfect versions exist, and that is my question. Recall for Pauli matrices $p_k$: $\{p_i,p_j\} = p_ip_j + p_jp_i = 2 \delta_{ij}$ I'm looking for matrices that satisfy: $\{a_i,a_j,a_k\} = 3! \delta_{ijk}$ where $\{A,B,C\}$ is the sum over all permutations of three symbols: $\{A,B,C\} = ABC + BCA + CAB + ACB + CBA + BAC$ My candidate matrices are: $a_1 = \begin{bmatrix} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0\\ \end{bmatrix}$ $a_2 = \begin{bmatrix} 0 & 0 & a\\ b & 0 & 0\\ 0 & c & 0\\ \end{bmatrix}$ $a_3 = \begin{bmatrix} 0 & a & 0\\ 0 & 0 & b\\ c & 0 & 0\\ \end{bmatrix}$ $a_4 = \begin{bmatrix} a & 0 & 0\\ 0 & b & 0\\ 0 & 0 & c\\ \end{bmatrix}$ where $a,b,c$ are the three third roots of unity (and have the property $a + b + c = 0$). And of course, unlike Pauli matrices, the eigenvalues of these matrices are not real (they are $a,b,c$), which is expected since they are permutation matrices with $a^3 = I$. Now, they have the desired property of: $\{a_1,a_1,a_1\} = 3\\ \{a_1,a_1,a_2\} = 0\\ \{a_1,a_2,a_3\} = 0\\ \{a_1,a_3,a_4\} = 0\\ \cdots$ However:  $\{a_2,a_3,a_4\} = -3I$ So my question is, are there a set of 4 matrices that satisfy $\{a_i,a_j,a_k\} = 3! \delta_{ijk}$ We have 3 such matrices $a_1,a_2,a_3$, but do there exist 4? I'm 90% sure the answer is no, but I'd like to be 100%. And we can extend the problem to higher dimensions: For example, the $d = 4$ case. Define: $\{A,B,C,D\}$ as the sum over all permutations of four symbols. How many matrices can we find that satisfy: $\{a_i,a_j,a_k,a_l\} = 4! \delta_{ijkl}$ It suspect it might be 3 for all higher d. Now, where did this problem come from? Consider: $M_1 = u*a_1 + v*a_2 + w*a_3$ such that $M_1^3 = (u^3 + v^3 + w^3)I$ And $M_2 = u*a_1 + v*a_2 + x*a_3 + y*a_4$ such that $M_2^3 = (u^3 + v^3 + x^3 + y^3)I$ And their determinants: $\det(M_1) = u^3 + v^3 + w^3\\ \det(M_2) = u^3 + v^3 + x^3 + y^3 -3vxy$ So an alternative definition of the problem, do there exist matrices, not equal to the identity matrix, $a_1,a_2,a_3,a_4$ such that $\det(M_2)$ doesn't have the cross term? Similarly for higher $d$, where the determinant gets even messier! eg: $d = 4, M = u*b_1 + v*b_2 + x*b_3 + y*b_4$ and $\det(M) = u^4 + v^4 + x^4 + y^4 + mess$",,['matrices']
40,Eigenvalues of the Sum of a Positive Definite Diagonal Matrix and a Rank $2$ Skew Symmetric Matrix,Eigenvalues of the Sum of a Positive Definite Diagonal Matrix and a Rank  Skew Symmetric Matrix,2,"Consider a $N \times N$ matrix $A=B_1+B_2$, where $B_1$ is a diagonal matrix with all the diagonal entries between $0$ and $1$ and $B_2$ is a skew symmetric matrix which can be written as $$B_2= \begin{bmatrix} 0_{N-1} & f \\ -f^T & 0 \end{bmatrix}$$ where $0_{N-1}$ is the $N-1 \times N-1$ zero matrix, $f$ is a $N-1$ by $1$ vector. So $B_2=-B_2^T$. My question is how many complex eigenvalues (non-zero imaginary part) $A$ has? I have run some numerical simulations. It seems that $A$ can at most have one pair of complex eigenvalues. And it is plausible to me because the skew symmetric $B_2$ is only rank $2$. How can we prove this mathematically or come up with a counter example? If needed, we can put an upper bound on $|f|_2$. But I don't know that if it is necessary, as indicated from my simulations.","Consider a $N \times N$ matrix $A=B_1+B_2$, where $B_1$ is a diagonal matrix with all the diagonal entries between $0$ and $1$ and $B_2$ is a skew symmetric matrix which can be written as $$B_2= \begin{bmatrix} 0_{N-1} & f \\ -f^T & 0 \end{bmatrix}$$ where $0_{N-1}$ is the $N-1 \times N-1$ zero matrix, $f$ is a $N-1$ by $1$ vector. So $B_2=-B_2^T$. My question is how many complex eigenvalues (non-zero imaginary part) $A$ has? I have run some numerical simulations. It seems that $A$ can at most have one pair of complex eigenvalues. And it is plausible to me because the skew symmetric $B_2$ is only rank $2$. How can we prove this mathematically or come up with a counter example? If needed, we can put an upper bound on $|f|_2$. But I don't know that if it is necessary, as indicated from my simulations.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'positive-definite']"
41,A property about positive definite matrix,A property about positive definite matrix,,"For any two positive definite same-sized matrices $A$ and $B$, I guessed a property about them and I have verified it via numerical experiment in matlab. The property is stated below: Assume $A$ and $B$ are both $n\times n$ positive definite matrices, then we define a vector $\vec{V}$. $$V(i)=A_{ii}B_{ii}$$ that is, the $i$th component of $\vec{V}$ is $A_{ii}B_{ii}$. The minimal component of $\vec{V}$ is: $$V_{min}=min\{V_{i}:i=1,2,3...,n\}$$ Then we can find a bound for the eigenvalue of their product $AB$, denote the smallest eigenvalue of $AB$ $\lambda_{min}$. The claim is that: $$V_{min}\geq\lambda_{min}$$ Can someone prove it?","For any two positive definite same-sized matrices $A$ and $B$, I guessed a property about them and I have verified it via numerical experiment in matlab. The property is stated below: Assume $A$ and $B$ are both $n\times n$ positive definite matrices, then we define a vector $\vec{V}$. $$V(i)=A_{ii}B_{ii}$$ that is, the $i$th component of $\vec{V}$ is $A_{ii}B_{ii}$. The minimal component of $\vec{V}$ is: $$V_{min}=min\{V_{i}:i=1,2,3...,n\}$$ Then we can find a bound for the eigenvalue of their product $AB$, denote the smallest eigenvalue of $AB$ $\lambda_{min}$. The claim is that: $$V_{min}\geq\lambda_{min}$$ Can someone prove it?",,"['linear-algebra', 'matrices']"
42,Why are matrix norms defined the way they are?,Why are matrix norms defined the way they are?,,"Given $A$ a square matrix Define: $\|A\|_1$ as the max absolute column sum $\|A\|_2$ as the sum of the squares of each element $\|A\|_\infty$ as the max absolute row sum Pray tell, why are matrix norms defined this way? Is this a property inherited or derived from the vector norms? (Source doesn't say: http://www.personal.soton.ac.uk/jav/soton/HELM/workbooks/workbook_30/30_4_matrx_norms.pdf )","Given $A$ a square matrix Define: $\|A\|_1$ as the max absolute column sum $\|A\|_2$ as the sum of the squares of each element $\|A\|_\infty$ as the max absolute row sum Pray tell, why are matrix norms defined this way? Is this a property inherited or derived from the vector norms? (Source doesn't say: http://www.personal.soton.ac.uk/jav/soton/HELM/workbooks/workbook_30/30_4_matrx_norms.pdf )",,"['matrices', 'definition', 'normed-spaces', 'matrix-calculus', 'banach-algebras']"
43,Best algorithm to compute the first eigenvector of symmetric matrix,Best algorithm to compute the first eigenvector of symmetric matrix,,"Assume that we have a real symmetric matrix $\mathbf{A}\in\mathbb{R}^{n\times n}$ obtained as following : $$\mathbf{A}=\mathbf{N}-\mathbf{P},$$ with $\mathbf{N}\in\mathbb{R}^{n\times n}$ and $\mathbf{P}\in\mathbb{R}^{n\times n}$ two real symmetric positive-definite matrices. What is the best algorithm (in terms of computation time) to compute the eigenvector corresponding to the largest (algebraic) eigenvalues of $\mathbf{A}$ matrix ? For the requested information, the dimension of the matrices is typically in the following order of magnitude: $10^{2}\leq n \leq 10^{5}$. The matrices are dense and as regards their the spectrum of eigenvalue, I do not know how it is distributed. However, I can describe the computation method of $\mathbf{N}$ and $\mathbf{P}$ : $$ \mathbf{N} = \sum_{i=1}^{d_n} v_i \mathbf{n}_i\mathbf{n}_i^\top $$ $$ \mathbf{P} = \sum_{j=1}^{d_p} u_j \mathbf{p}_j\mathbf{p}_j^\top $$ with $d_n \geq n$, $d_p \geq n$, $\mathbf{n}_i \in \mathbb{R}^{n}$, $\mathbf{p}_j \in \mathbb{R}^{n}$, $||\mathbf{n}_i||_2 \leq 2$, $||\mathbf{p}_j||_2 \leq 2$, $v_i \in \mathbb{R}^+$, $u_j \in \mathbb{R}^+$, $\sum_i v_i = 1$ and $\sum_j u_j = 1$. I have access to a lot of computational power and I can use a parallel algorithm on CPU or GPU.","Assume that we have a real symmetric matrix $\mathbf{A}\in\mathbb{R}^{n\times n}$ obtained as following : $$\mathbf{A}=\mathbf{N}-\mathbf{P},$$ with $\mathbf{N}\in\mathbb{R}^{n\times n}$ and $\mathbf{P}\in\mathbb{R}^{n\times n}$ two real symmetric positive-definite matrices. What is the best algorithm (in terms of computation time) to compute the eigenvector corresponding to the largest (algebraic) eigenvalues of $\mathbf{A}$ matrix ? For the requested information, the dimension of the matrices is typically in the following order of magnitude: $10^{2}\leq n \leq 10^{5}$. The matrices are dense and as regards their the spectrum of eigenvalue, I do not know how it is distributed. However, I can describe the computation method of $\mathbf{N}$ and $\mathbf{P}$ : $$ \mathbf{N} = \sum_{i=1}^{d_n} v_i \mathbf{n}_i\mathbf{n}_i^\top $$ $$ \mathbf{P} = \sum_{j=1}^{d_p} u_j \mathbf{p}_j\mathbf{p}_j^\top $$ with $d_n \geq n$, $d_p \geq n$, $\mathbf{n}_i \in \mathbb{R}^{n}$, $\mathbf{p}_j \in \mathbb{R}^{n}$, $||\mathbf{n}_i||_2 \leq 2$, $||\mathbf{p}_j||_2 \leq 2$, $v_i \in \mathbb{R}^+$, $u_j \in \mathbb{R}^+$, $\sum_i v_i = 1$ and $\sum_j u_j = 1$. I have access to a lot of computational power and I can use a parallel algorithm on CPU or GPU.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'symmetric-matrices']"
44,How to calculate the derivative of logarithm of a matrix?,How to calculate the derivative of logarithm of a matrix?,,"Given a square matrix $M$, we know the exponential of $M$ is $$\exp(M)=\sum_{n=0}^\infty{\frac{M^n}{n!}}$$ and the logarithm is $$\log(M)=-\sum_{k=1}^\infty\frac{(I-M)^k}{k}$$ The derivative of $\exp(M)$ should be itself. It is easy to prove if $\frac{dM}{M}=I$. But how to calculate the derivative of $\log(M)$? By the same way of calculation of the derivative of $\exp(M)$, the derivative of $\log(M)$ cannot converge. So what is the derivative of $\log(M)$?","Given a square matrix $M$, we know the exponential of $M$ is $$\exp(M)=\sum_{n=0}^\infty{\frac{M^n}{n!}}$$ and the logarithm is $$\log(M)=-\sum_{k=1}^\infty\frac{(I-M)^k}{k}$$ The derivative of $\exp(M)$ should be itself. It is easy to prove if $\frac{dM}{M}=I$. But how to calculate the derivative of $\log(M)$? By the same way of calculation of the derivative of $\exp(M)$, the derivative of $\log(M)$ cannot converge. So what is the derivative of $\log(M)$?",,"['matrices', 'differential-operators']"
45,Non-singular matrix,Non-singular matrix,,"Assuming $B$, $I+B$, $I+B^{-1}$ are all non-singular, show that $$(I+B)^{-1}+(I+B^{-1})^{-1}=I$$ All I know is that determinants not equal to $0$ and that the inverse of $B$ exists.","Assuming $B$, $I+B$, $I+B^{-1}$ are all non-singular, show that $$(I+B)^{-1}+(I+B^{-1})^{-1}=I$$ All I know is that determinants not equal to $0$ and that the inverse of $B$ exists.",,['matrices']
46,How is it the matrix-vector multiplication with SVD is $O(m+n)$?,How is it the matrix-vector multiplication with SVD is ?,O(m+n),"Assuming the singular value decomposition is known, how is it the matrix-vector product of $\mathbf{A}$ ($m \times n$) and vector $\mathbf{x}$ ($n \times 1$) has $O(m+n)$ complexity? Somewhat related: Efficient low rank matrix-vector multiplication and this post on Using SVD to approximate matrix-vector multiplication? The way I break it down, for $\mathbf{Ax} = \mathbf{U \Sigma V}^T\mathbf{x}$: the product $\mathbf{L} = \mathbf{V}^T\mathbf{x}$ has $O(n^2)$ complexity the product $\mathbf{H} = \mathbf{\Sigma}\mathbf{L}$ has $O(n)$ complexity the product $\mathbf{U}\mathbf{H}$ has $O(mn)$ complexity. Why is the matrix-product complexity of order $O(m+n)$ instead of $O(n^2 + n + mn)$?","Assuming the singular value decomposition is known, how is it the matrix-vector product of $\mathbf{A}$ ($m \times n$) and vector $\mathbf{x}$ ($n \times 1$) has $O(m+n)$ complexity? Somewhat related: Efficient low rank matrix-vector multiplication and this post on Using SVD to approximate matrix-vector multiplication? The way I break it down, for $\mathbf{Ax} = \mathbf{U \Sigma V}^T\mathbf{x}$: the product $\mathbf{L} = \mathbf{V}^T\mathbf{x}$ has $O(n^2)$ complexity the product $\mathbf{H} = \mathbf{\Sigma}\mathbf{L}$ has $O(n)$ complexity the product $\mathbf{U}\mathbf{H}$ has $O(mn)$ complexity. Why is the matrix-product complexity of order $O(m+n)$ instead of $O(n^2 + n + mn)$?",,"['matrices', 'matrix-equations', 'matrix-decomposition', 'matrix-rank', 'svd']"
47,Derivative of trace of matrix,Derivative of trace of matrix,,"I'm new to matrix calculus and I have a problem with my assignment. Following is a function of trace of matrices: $$  f =  \mathrm{tr}[\mathbf{X} \mathbf{X^T}]  - \mathrm{tr}[\mathbf{X} \mathbf{H^T} \mathbf{W}^T]  - \mathrm{tr}[\mathbf{W} \mathbf{H} \mathbf{X}^T]  + \mathrm{tr}[\mathbf{W} \mathbf{H} \mathbf{H}^T \mathbf{W}^T]. $$ And I have to prove that: $$  \frac  {\partial \mathrm{tr}[   (\mathbf{X}-\mathbf{W}\mathbf{H})   (\mathbf{X}-\mathbf{W}\mathbf{H})^T   ]  }  {\partial W}  = -2 \mathbf{X} \mathbf{H}^T    + 2 \mathbf{W} \mathbf{H} \mathbf{H}^T. $$ In the second equation, the numerator is f in the fist equation. Can you help me solve this? Thank you.","I'm new to matrix calculus and I have a problem with my assignment. Following is a function of trace of matrices: $$  f =  \mathrm{tr}[\mathbf{X} \mathbf{X^T}]  - \mathrm{tr}[\mathbf{X} \mathbf{H^T} \mathbf{W}^T]  - \mathrm{tr}[\mathbf{W} \mathbf{H} \mathbf{X}^T]  + \mathrm{tr}[\mathbf{W} \mathbf{H} \mathbf{H}^T \mathbf{W}^T]. $$ And I have to prove that: $$  \frac  {\partial \mathrm{tr}[   (\mathbf{X}-\mathbf{W}\mathbf{H})   (\mathbf{X}-\mathbf{W}\mathbf{H})^T   ]  }  {\partial W}  = -2 \mathbf{X} \mathbf{H}^T    + 2 \mathbf{W} \mathbf{H} \mathbf{H}^T. $$ In the second equation, the numerator is f in the fist equation. Can you help me solve this? Thank you.",,"['matrices', 'matrix-calculus']"
48,Determinant of nth order,Determinant of nth order,,"I want to solve the following determinant: $D_n= \begin{vmatrix}         a_n & a_{n-1} & \cdots & a_2 & x\\         a_n & a_{n-1} & \cdots & x & a_1\\         a_n & a_{n-1} & \cdots & a_2 & a_1\\         \vdots & \vdots & \ddots & \vdots & \vdots\\         a_n & x & \cdots & a_2 & a_1\\         a_n & a_{n-1} & \cdots & a_2 & a_1\\         \end{vmatrix}$ The idea I had was to 1) get the $a_n$ in front of the determinant, which gets me to: $D_n= a_n \begin{vmatrix}         1 & a_{n-1} & \cdots & a_2 & x\\         1 & a_{n-1} & \cdots & x & a_1\\         1 & a_{n-1} & \cdots & a_2 & a_1\\         \vdots & \vdots & \ddots & \vdots & \vdots\\         1 & x & \cdots & a_2 & a_1\\         1 & a_{n-1} & \cdots & a_2 & a_1\\         \end{vmatrix}$ Then I multiplied the nth row with $-1$ and added it to all the other ones, which gives me: $D_n= a_n \begin{vmatrix}         0 & 0 & \cdots & 0 & x-a_1\\         0 & 0 & \cdots & x-a_2 & 0\\         0 & 0 & \cdots & 0 & 0\\         \vdots & \vdots & \ddots & \vdots & \vdots\\         0 & x-a_{n-1} & \cdots & 0 & 0\\         1 & a_{n-1} & \cdots & a_2 & a_1\\         \end{vmatrix}$ So, I got a triangular determinant, but with the opposite diagonal, not the main one. How do I transform it into a real triangular determinant? Obviously, my idea to replace every adjacent row (1st and nth, (n-1)th and 2nd...) is wrong? Any ideas?","I want to solve the following determinant: $D_n= \begin{vmatrix}         a_n & a_{n-1} & \cdots & a_2 & x\\         a_n & a_{n-1} & \cdots & x & a_1\\         a_n & a_{n-1} & \cdots & a_2 & a_1\\         \vdots & \vdots & \ddots & \vdots & \vdots\\         a_n & x & \cdots & a_2 & a_1\\         a_n & a_{n-1} & \cdots & a_2 & a_1\\         \end{vmatrix}$ The idea I had was to 1) get the $a_n$ in front of the determinant, which gets me to: $D_n= a_n \begin{vmatrix}         1 & a_{n-1} & \cdots & a_2 & x\\         1 & a_{n-1} & \cdots & x & a_1\\         1 & a_{n-1} & \cdots & a_2 & a_1\\         \vdots & \vdots & \ddots & \vdots & \vdots\\         1 & x & \cdots & a_2 & a_1\\         1 & a_{n-1} & \cdots & a_2 & a_1\\         \end{vmatrix}$ Then I multiplied the nth row with $-1$ and added it to all the other ones, which gives me: $D_n= a_n \begin{vmatrix}         0 & 0 & \cdots & 0 & x-a_1\\         0 & 0 & \cdots & x-a_2 & 0\\         0 & 0 & \cdots & 0 & 0\\         \vdots & \vdots & \ddots & \vdots & \vdots\\         0 & x-a_{n-1} & \cdots & 0 & 0\\         1 & a_{n-1} & \cdots & a_2 & a_1\\         \end{vmatrix}$ So, I got a triangular determinant, but with the opposite diagonal, not the main one. How do I transform it into a real triangular determinant? Obviously, my idea to replace every adjacent row (1st and nth, (n-1)th and 2nd...) is wrong? Any ideas?",,"['linear-algebra', 'matrices', 'determinant']"
49,Are real antisymmetric matrices orthogonally similar to their transposes?,Are real antisymmetric matrices orthogonally similar to their transposes?,,"Let $A$ be a real antisymmetric matrix. Is it true that $A$ must be orthogonally similar to its transpose (i.e to $-A$)? Note: It's known that every matrix is similar to its transpose . It's also known, that in general, not every real matrix is orthogonally similar to its transpose . However, in the example given in the reference above, $A$ is not antisymmetric.","Let $A$ be a real antisymmetric matrix. Is it true that $A$ must be orthogonally similar to its transpose (i.e to $-A$)? Note: It's known that every matrix is similar to its transpose . It's also known, that in general, not every real matrix is orthogonally similar to its transpose . However, in the example given in the reference above, $A$ is not antisymmetric.",,"['linear-algebra', 'matrices']"
50,"Evaluate $\int{\mathrm{tr}}\left( {AB(I-xB)^{-1}}\right) {\,dx}$ for $A$ and $B$ square real matrices",Evaluate  for  and  square real matrices,"\int{\mathrm{tr}}\left( {AB(I-xB)^{-1}}\right) {\,dx} A B","Let $A$ and $B$ be two real $n\times n$ matrices, and let $C(x)=B(I-xB)^{-1}$, where $I$ is the identity matrix of order $n$, for any real scalar $x$ such that $I-xB$ is invertible. Denote by $\mathrm{tr}$ the trace operator. Is it possible to get a closed form solution for $$\int{\mathrm{tr}}\left(  {AC(x)}\right)   {\,dx},$$ at least when $B$ is diagonalizable? What I've done so far: Assuming $B$ is diagonalizable, $C$ is too, and hence it admits the spectral decomposition $$ C(x)=\sum_{\lambda\in\mathrm{Sp}(B)}\frac{\lambda}{1-x\lambda}Q_{\lambda}, $$ where $\mathrm{Sp}(B)$ denotes the set of distinct eigenvalues of $B$, and $Q_{\lambda}$ is the projector onto $\mathrm{null}(B-\lambda I)$ along $\mathrm{col}(B-\lambda I)$ ($\mathrm{null}$ and $\mathrm{col}$ stand for the null and the column spaces). Hence $$ \int{\mathrm{tr}}\left(  {AC(x)}\right)  {\,dx}=\sum_{\lambda\in \mathrm{Sp}(B)}\mathrm{tr}(AQ_{\lambda})\int\frac{\lambda}{1-x\lambda}\,dx $$ If all the eigenvalues of $B$ are real we obtain $$ \int{\mathrm{tr}}\left(  {AC(x)}\right)  {\,dx}=\sum_{\lambda\in \mathrm{Sp}(B)}\mathrm{tr}(AQ_{\lambda})\int\frac{\lambda}{1-x\lambda }\,dx=\sum_{\lambda\in\mathrm{Sp}(B)}\mathrm{ln}(\left\vert 1-x\lambda \right\vert )\mathrm{tr}(AQ_{\lambda}) $$ But what about the case in which not all eigenvalues of $B$ are real? Do we get any simplification from the fact that the eigenvalues of real matrices come in complex conjugate pairs, and the eigenvectors (and hence the projectors $Q_{\lambda}$) associated to complex conjugate eigenvalues are complex conjugates? Can we still use the formula in the last display above for $\int{\mathrm{tr}}\left(  {AC(x)}\right)  {\,dx}$?","Let $A$ and $B$ be two real $n\times n$ matrices, and let $C(x)=B(I-xB)^{-1}$, where $I$ is the identity matrix of order $n$, for any real scalar $x$ such that $I-xB$ is invertible. Denote by $\mathrm{tr}$ the trace operator. Is it possible to get a closed form solution for $$\int{\mathrm{tr}}\left(  {AC(x)}\right)   {\,dx},$$ at least when $B$ is diagonalizable? What I've done so far: Assuming $B$ is diagonalizable, $C$ is too, and hence it admits the spectral decomposition $$ C(x)=\sum_{\lambda\in\mathrm{Sp}(B)}\frac{\lambda}{1-x\lambda}Q_{\lambda}, $$ where $\mathrm{Sp}(B)$ denotes the set of distinct eigenvalues of $B$, and $Q_{\lambda}$ is the projector onto $\mathrm{null}(B-\lambda I)$ along $\mathrm{col}(B-\lambda I)$ ($\mathrm{null}$ and $\mathrm{col}$ stand for the null and the column spaces). Hence $$ \int{\mathrm{tr}}\left(  {AC(x)}\right)  {\,dx}=\sum_{\lambda\in \mathrm{Sp}(B)}\mathrm{tr}(AQ_{\lambda})\int\frac{\lambda}{1-x\lambda}\,dx $$ If all the eigenvalues of $B$ are real we obtain $$ \int{\mathrm{tr}}\left(  {AC(x)}\right)  {\,dx}=\sum_{\lambda\in \mathrm{Sp}(B)}\mathrm{tr}(AQ_{\lambda})\int\frac{\lambda}{1-x\lambda }\,dx=\sum_{\lambda\in\mathrm{Sp}(B)}\mathrm{ln}(\left\vert 1-x\lambda \right\vert )\mathrm{tr}(AQ_{\lambda}) $$ But what about the case in which not all eigenvalues of $B$ are real? Do we get any simplification from the fact that the eigenvalues of real matrices come in complex conjugate pairs, and the eigenvectors (and hence the projectors $Q_{\lambda}$) associated to complex conjugate eigenvalues are complex conjugates? Can we still use the formula in the last display above for $\int{\mathrm{tr}}\left(  {AC(x)}\right)  {\,dx}$?",,"['linear-algebra', 'integration', 'matrices', 'eigenvalues-eigenvectors', 'trace']"
51,"similarity transform mapping diagonal matrix of complex conjugates, to real matrix","similarity transform mapping diagonal matrix of complex conjugates, to real matrix",,"I know there is some unitary matrix $T$ that maps a diagonal matrix $A = \begin{bmatrix}a+bj & 0 \cr 0 & a-bj \end{bmatrix}$ to a real matrix, namely $$T^{-1}AT = \begin{bmatrix}a & b \cr -b & a \end{bmatrix}$$ but I am having trouble figuring how to construct $T$. At first I thought it was just a Givens rotation, that can't be right because Givens rotations have real entries and that wouldn't eliminate the non-real components. How can I compute $T$?","I know there is some unitary matrix $T$ that maps a diagonal matrix $A = \begin{bmatrix}a+bj & 0 \cr 0 & a-bj \end{bmatrix}$ to a real matrix, namely $$T^{-1}AT = \begin{bmatrix}a & b \cr -b & a \end{bmatrix}$$ but I am having trouble figuring how to construct $T$. At first I thought it was just a Givens rotation, that can't be right because Givens rotations have real entries and that wouldn't eliminate the non-real components. How can I compute $T$?",,['matrices']
52,How can I solve this triangular linear system?,How can I solve this triangular linear system?,,"I am trying to create a model for simulating the dynamics of a rope by treating it like several connected pendulums. The goal is to solve the following system for $\vec\alpha$, which is a vector of all the angular accelerations $\alpha$ of all the rope segments. The system is: $$C\vec\alpha = S\vec\omega - \frac{g}{L}\vec s$$ Where $\vec\alpha$ is the vector of unknowns, $\vec\omega$ is a vector of known angular velocities and $\frac{g}{L}$ is a constant. $C$ and $S$ are $m \times m$ matrices and $\vec s$ is a vector. They're defined as follows: $$ C = \begin{pmatrix} 1& 0& 0& \cdots& 0\\ \cos(\theta_1-\theta_2)& 1& 0& \cdots& 0\\ \cos(\theta_1-\theta_3)& \cos(\theta_2-\theta_3)& 1& \cdots& 0\\ \vdots& \vdots& \vdots& \ddots& \vdots&\\ \cos(\theta_1-\theta_m)& \cos(\theta_2-\theta_m)& cos(\theta_3-\theta_m)& \cdots& 1 \end{pmatrix}$$ $S$ is the same as $C$, except every instance of $\cos$ is replaced with $\sin$. $$ \vec s = \begin{pmatrix} \sin(\theta_1)\\ \sin(\theta_2)\\ \vdots\\ \sin(\theta_m)\\ \end{pmatrix}$$ Additionally, $\theta_n$ is a known angle for all integers $n$ in $[1,m]$. The objective is to solve the given linear system for $\vec\alpha$. The obvious way to do so is to left-multiply by the inverse of $C$. Am I right in assuming this would yield the following? $$ \vec \alpha = C^{-1}S\vec\omega-\frac{g}{L}C^{-1}S$$ I'm quite confident this is correct, but if it breaks some rule of matrix multiplication I've overlooked please tell me. Either way, I would like help with finding the inverse of $C$. Additionally, if possible, I would like help finding $C^{-1}S$ and $C^{-1}\vec s$. Any help is appreciated.","I am trying to create a model for simulating the dynamics of a rope by treating it like several connected pendulums. The goal is to solve the following system for $\vec\alpha$, which is a vector of all the angular accelerations $\alpha$ of all the rope segments. The system is: $$C\vec\alpha = S\vec\omega - \frac{g}{L}\vec s$$ Where $\vec\alpha$ is the vector of unknowns, $\vec\omega$ is a vector of known angular velocities and $\frac{g}{L}$ is a constant. $C$ and $S$ are $m \times m$ matrices and $\vec s$ is a vector. They're defined as follows: $$ C = \begin{pmatrix} 1& 0& 0& \cdots& 0\\ \cos(\theta_1-\theta_2)& 1& 0& \cdots& 0\\ \cos(\theta_1-\theta_3)& \cos(\theta_2-\theta_3)& 1& \cdots& 0\\ \vdots& \vdots& \vdots& \ddots& \vdots&\\ \cos(\theta_1-\theta_m)& \cos(\theta_2-\theta_m)& cos(\theta_3-\theta_m)& \cdots& 1 \end{pmatrix}$$ $S$ is the same as $C$, except every instance of $\cos$ is replaced with $\sin$. $$ \vec s = \begin{pmatrix} \sin(\theta_1)\\ \sin(\theta_2)\\ \vdots\\ \sin(\theta_m)\\ \end{pmatrix}$$ Additionally, $\theta_n$ is a known angle for all integers $n$ in $[1,m]$. The objective is to solve the given linear system for $\vec\alpha$. The obvious way to do so is to left-multiply by the inverse of $C$. Am I right in assuming this would yield the following? $$ \vec \alpha = C^{-1}S\vec\omega-\frac{g}{L}C^{-1}S$$ I'm quite confident this is correct, but if it breaks some rule of matrix multiplication I've overlooked please tell me. Either way, I would like help with finding the inverse of $C$. Additionally, if possible, I would like help finding $C^{-1}S$ and $C^{-1}\vec s$. Any help is appreciated.",,"['linear-algebra', 'matrices', 'trigonometry', 'matrix-equations']"
53,norm with respect to matrix,norm with respect to matrix,,"In Short: I am studying robotics. I have an equation that asks me to take the norm of a vector with respect to a matrix. I do not know what that means. In long: When I read norm, and I am working with vectors, I think of the Euclidean Norm or the magnitude: $||\textbf{x}|| = \sqrt{x_1^2 + x_2^2 + ... x_n^2}$. I am uncertain on how the matrix that I am taking that value with respect to changes it. The exact equation I am considering here is:  $$f(q_{t+1}) = ||q_{t+1} - q_t||^2_w + ||y^*_{t+1} -\phi({q_t})||^2_C$$ Which, when minimized gives the optimal joint command $q_{t+1}$ for a robotic arm moving to the coordinates $y^*$. $W$ and $C$ are both matrices of the size $|q| \times |q|$. I give the robotic information for context only. I have a very good understanding of that, I am just deeply confused by the ""with respect to"" phrase. What does it mean? How do these matrices change the norm? Does it regulate it in some way? Is it like normalizing a random variable (dividing an r.v. by its highest possible value). Any light would help.","In Short: I am studying robotics. I have an equation that asks me to take the norm of a vector with respect to a matrix. I do not know what that means. In long: When I read norm, and I am working with vectors, I think of the Euclidean Norm or the magnitude: $||\textbf{x}|| = \sqrt{x_1^2 + x_2^2 + ... x_n^2}$. I am uncertain on how the matrix that I am taking that value with respect to changes it. The exact equation I am considering here is:  $$f(q_{t+1}) = ||q_{t+1} - q_t||^2_w + ||y^*_{t+1} -\phi({q_t})||^2_C$$ Which, when minimized gives the optimal joint command $q_{t+1}$ for a robotic arm moving to the coordinates $y^*$. $W$ and $C$ are both matrices of the size $|q| \times |q|$. I give the robotic information for context only. I have a very good understanding of that, I am just deeply confused by the ""with respect to"" phrase. What does it mean? How do these matrices change the norm? Does it regulate it in some way? Is it like normalizing a random variable (dividing an r.v. by its highest possible value). Any light would help.",,"['linear-algebra', 'matrices']"
54,Finding the determinant of anti-diagonal matrix,Finding the determinant of anti-diagonal matrix,,"How would one find the determinant of an anti-diagonal matrix ($n \times n$), without using eigenvalues and/or traces (those I haven't learned yet): My initial idea was to swap the first and n-th row, then the second and $n-1$-th row and so on, until I get a diagonal determinant, however how many swaps do I have to perform for that to happen? (Note: I do know the sign changes so I'll have $-1$ to some power times the now diagonal determinant, the problem is to find to what power). For example: $\left| \begin{matrix} 0 & 0 & 0 & \dots & 0 & n-1 \\ 0 & 0 & 0 & \dots & n-1 & 0 \\ \vdots \\ 0 & n-1 & 0 & \dots & 0 & 0 \\ n-1 & 0 & 0 & \dots & 0 & 0 \end{matrix} \right|$.","How would one find the determinant of an anti-diagonal matrix ($n \times n$), without using eigenvalues and/or traces (those I haven't learned yet): My initial idea was to swap the first and n-th row, then the second and $n-1$-th row and so on, until I get a diagonal determinant, however how many swaps do I have to perform for that to happen? (Note: I do know the sign changes so I'll have $-1$ to some power times the now diagonal determinant, the problem is to find to what power). For example: $\left| \begin{matrix} 0 & 0 & 0 & \dots & 0 & n-1 \\ 0 & 0 & 0 & \dots & n-1 & 0 \\ \vdots \\ 0 & n-1 & 0 & \dots & 0 & 0 \\ n-1 & 0 & 0 & \dots & 0 & 0 \end{matrix} \right|$.",,['matrices']
55,Noncommuting complex matrices: Existence of a simultaneous eigenvector,Noncommuting complex matrices: Existence of a simultaneous eigenvector,,Let $A$ and $B$ be $n\times n$ matrices with complex entries such that $AB - BA$ is a linear combination of $A$ and $B$ . I'd like to prove that there exists a non-zero vector $v$ that is an eigenvector of both $A$ and $B$ .,Let and be matrices with complex entries such that is a linear combination of and . I'd like to prove that there exists a non-zero vector that is an eigenvector of both and .,A B n\times n AB - BA A B v A B,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'lie-algebras']"
56,Second order derivative of log of vector,Second order derivative of log of vector,,I have a vector of size $n$ x $1$ named $\alpha$. Let $f(\alpha) = u\cdot\mathbf 1^{\!\top}ln(\alpha)$ where $u$ is scalar. What is the $f'(\alpha)$ and $f''(\alpha)$ and equivalent Matlab code? According to me the first derivative is $$f'(\alpha) = u/\alpha$$ and equivalent MATLAB code is -- f_a_1 = u ./ a and for the second derivative $$f''(\alpha) = u\cdot(Diag(\alpha)*Diag(\alpha))^{-1}$$ Equivalent MATLAB code is f_a_2 = u*inv(diag(a)*diag(a)) Is my inference correct?,I have a vector of size $n$ x $1$ named $\alpha$. Let $f(\alpha) = u\cdot\mathbf 1^{\!\top}ln(\alpha)$ where $u$ is scalar. What is the $f'(\alpha)$ and $f''(\alpha)$ and equivalent Matlab code? According to me the first derivative is $$f'(\alpha) = u/\alpha$$ and equivalent MATLAB code is -- f_a_1 = u ./ a and for the second derivative $$f''(\alpha) = u\cdot(Diag(\alpha)*Diag(\alpha))^{-1}$$ Equivalent MATLAB code is f_a_2 = u*inv(diag(a)*diag(a)) Is my inference correct?,,"['calculus', 'matrices', 'matlab', 'matrix-calculus']"
57,If a matrix is row equivalent to some invertible matrix then it is invertible,If a matrix is row equivalent to some invertible matrix then it is invertible,,"I'm not really sure where to go with this question, any help would be appreciated. Question Let $A$ be an $n\times n$ matrix. Prove that if $A$ is row equivalent to some invertible $n\times n$ matrix $B$ then A is invertible. Attempt I'm not sure where a starting point would be. I know that an $n\times n$ matrix B is invertible if there is a matrix A such that B is both the left and the right inverse of A: AB = In and BA = In, but I'm not sure if this would be useful.","I'm not really sure where to go with this question, any help would be appreciated. Question Let $A$ be an $n\times n$ matrix. Prove that if $A$ is row equivalent to some invertible $n\times n$ matrix $B$ then A is invertible. Attempt I'm not sure where a starting point would be. I know that an $n\times n$ matrix B is invertible if there is a matrix A such that B is both the left and the right inverse of A: AB = In and BA = In, but I'm not sure if this would be useful.",,"['linear-algebra', 'matrices']"
58,$3\times3\times3$ hypermatrix multiplication,hypermatrix multiplication,3\times3\times3,"Here's an image showing what I am trying to do: The two hypermatrices are multiplied together by taking appropriate slices from each hypermatrix, and realised into a vector by an associated vector, given by ( a , b , c ) in this case. But I can't figure out how to do the simple matrix multiplication. For example red times green times vector equals blue, but there doesn't seem to be an obvious orientation for the matrices.","Here's an image showing what I am trying to do: The two hypermatrices are multiplied together by taking appropriate slices from each hypermatrix, and realised into a vector by an associated vector, given by ( a , b , c ) in this case. But I can't figure out how to do the simple matrix multiplication. For example red times green times vector equals blue, but there doesn't seem to be an obvious orientation for the matrices.",,['matrices']
59,Does $|x^*|=|x|$ in a star ring with an absolute value?,Does  in a star ring with an absolute value?,|x^*|=|x|,"Let $R$ be a star ring with an absolute value . Is it true that $|x^*|=|x|$ for all $x\in R$? Here a star ring is a ring with a function $*:R\to R$ called conjugation such that $(x+y)^*=x^*+y^*$ $(xy)^*=y^*x^*$ $x^{**}=x,$ and an absolute value is a function $|\cdot|:R\to\Bbb R$ such that $|x|=0\iff x=0$ $|x-y|\le|x|+|y|$ $|xy|=|x||y|.$ Obviously it is true for the trivial conjugation $x^*=x$, and it is also true for $\Bbb C$ and matrix rings over $\Bbb R$ and $\Bbb C$ with transposition and any of the various common matrix norms, so I wonder if it is true in general.","Let $R$ be a star ring with an absolute value . Is it true that $|x^*|=|x|$ for all $x\in R$? Here a star ring is a ring with a function $*:R\to R$ called conjugation such that $(x+y)^*=x^*+y^*$ $(xy)^*=y^*x^*$ $x^{**}=x,$ and an absolute value is a function $|\cdot|:R\to\Bbb R$ such that $|x|=0\iff x=0$ $|x-y|\le|x|+|y|$ $|xy|=|x||y|.$ Obviously it is true for the trivial conjugation $x^*=x$, and it is also true for $\Bbb C$ and matrix rings over $\Bbb R$ and $\Bbb C$ with transposition and any of the various common matrix norms, so I wonder if it is true in general.",,"['abstract-algebra', 'matrices', 'ring-theory', 'absolute-value', 'noncommutative-algebra']"
60,"How would I solve the following equation, which is similar to an algebraic Riccati equation or a nonlinear sylvester equation?","How would I solve the following equation, which is similar to an algebraic Riccati equation or a nonlinear sylvester equation?",,"I have the following matrix equation that I would like to solve for $X$: $0 = AX + XB + XCX + D$ In general, $X$ will be rectangular, with $(m\times n)$ dimensions. So if I write the equation out with indices, it is: $0 = A_{mm}X_{mn} + X_{mn}B_{nn} + X_{mn}C_{nm}X_{mn} + D_{mn}$ I assume everything to be real, and $m,n$ are dimensions small enough that a diagonalization of an $m\times n$ matrix is computationally feasible. I see that if $C=0$, then it is just a linear Sylvester equation, and if $A=B$ then it seems to be an Algebraic Riccati equation, but neither of these assertions can be made. I appreciate any guidance towards a solution. Perhaps this is an equation with well known properties (I'm not a mathematician)? Thanks in advance!","I have the following matrix equation that I would like to solve for $X$: $0 = AX + XB + XCX + D$ In general, $X$ will be rectangular, with $(m\times n)$ dimensions. So if I write the equation out with indices, it is: $0 = A_{mm}X_{mn} + X_{mn}B_{nn} + X_{mn}C_{nm}X_{mn} + D_{mn}$ I assume everything to be real, and $m,n$ are dimensions small enough that a diagonalization of an $m\times n$ matrix is computationally feasible. I see that if $C=0$, then it is just a linear Sylvester equation, and if $A=B$ then it seems to be an Algebraic Riccati equation, but neither of these assertions can be made. I appreciate any guidance towards a solution. Perhaps this is an equation with well known properties (I'm not a mathematician)? Thanks in advance!",,"['linear-algebra', 'matrices', 'algorithms', 'control-theory']"
61,"Geometric series of matrices, left and right multiplying","Geometric series of matrices, left and right multiplying",,"Let A and M be square matrices of the same size. Assuming that the following serries converges, what is its sum? $M+AMA^T + A^2M{A^T}^2+\ ... $ I suspect that the sum can be expressed by a closed formula similar to the classical formula for geometric series with scalars. Edit: I am not sure if that helps but: $M$ is symetric positive definite, $A$ is invertible, $A^iM(A^T)^i$ is symetric positive definite and the sum is also symetric positive definite. I don't mind if you use some additional assumptions. But I would realy appretiate solutiuon using matrix algebra.","Let A and M be square matrices of the same size. Assuming that the following serries converges, what is its sum? $M+AMA^T + A^2M{A^T}^2+\ ... $ I suspect that the sum can be expressed by a closed formula similar to the classical formula for geometric series with scalars. Edit: I am not sure if that helps but: $M$ is symetric positive definite, $A$ is invertible, $A^iM(A^T)^i$ is symetric positive definite and the sum is also symetric positive definite. I don't mind if you use some additional assumptions. But I would realy appretiate solutiuon using matrix algebra.",,"['sequences-and-series', 'matrices']"
62,"Solving a complex, sparse, linear system using the Schur complement","Solving a complex, sparse, linear system using the Schur complement",,"Solution method I am repetitively solving sparse linear systems (for the need of ARNOLDI iterations) of the type: $$\underbrace{\begin{bmatrix} J_1 & J_2 \\ J_3 & J_4 \end{bmatrix}}_J \underbrace{\begin{bmatrix} x \\ y \end{bmatrix}}_z= \underbrace{\begin{bmatrix} b_x \\ b_y \end{bmatrix}}_b$$ I build the Schur-complement ($J_4$ is non-singular): $$J_D=J_1-J_2J_4^{-1}J_3$$ Then, I solve for $x$: $$J_Dx=b_x-J_2J_4^{-1}b_y$$ and then for $y$: $$J_4y=b_y-J_3x$$ At this point I have a full solution of $z$. Intermediate calculations To calculate $J_2J_4^{-1}J_3$, I first compute $\tilde{J_2}=J_2J_4^{-1}$ as follows: $$\tilde{J_2}^T=(J_2J_4^{-1})^T=(J_4^{-1})^TJ_2^T$$ Thus, I get $\tilde{J_2}$ by solving the system: $$J_4^T\tilde{J_2}^T=J_2^T$$ and tacking the transpose. Edit: The above is implemented in Matlab and I use the conjugate transpose ' (mathworks.com/help/matlab/ref/ctranspose.html) for both the real and the complex. Also I use dot (mathworks.com/help/matlab/ref/dot.html) for the dot products. Problem This way of solving works perfectly when $J$ is real (even if $b$ is complex). That is, solving $Jz=b$ directly or by building the Schur-complement gives the same $z$ as expected. However, when $J_4$ becomes complex, then I get different results... I suspect that I did something really wrong when taking the transpose for $\tilde{J_2}$ or doing the dot product for $J_2J_4^{-1}b_y$. Yet, I fail to detect my error. Note: The reason I solve using the Schur-complement is that matrices $J_2,J_3,J_4$ have some particular structures that allow for good parallelization of the operations involved. E.g. $J_4$ is block diagonal and non-singular.","Solution method I am repetitively solving sparse linear systems (for the need of ARNOLDI iterations) of the type: $$\underbrace{\begin{bmatrix} J_1 & J_2 \\ J_3 & J_4 \end{bmatrix}}_J \underbrace{\begin{bmatrix} x \\ y \end{bmatrix}}_z= \underbrace{\begin{bmatrix} b_x \\ b_y \end{bmatrix}}_b$$ I build the Schur-complement ($J_4$ is non-singular): $$J_D=J_1-J_2J_4^{-1}J_3$$ Then, I solve for $x$: $$J_Dx=b_x-J_2J_4^{-1}b_y$$ and then for $y$: $$J_4y=b_y-J_3x$$ At this point I have a full solution of $z$. Intermediate calculations To calculate $J_2J_4^{-1}J_3$, I first compute $\tilde{J_2}=J_2J_4^{-1}$ as follows: $$\tilde{J_2}^T=(J_2J_4^{-1})^T=(J_4^{-1})^TJ_2^T$$ Thus, I get $\tilde{J_2}$ by solving the system: $$J_4^T\tilde{J_2}^T=J_2^T$$ and tacking the transpose. Edit: The above is implemented in Matlab and I use the conjugate transpose ' (mathworks.com/help/matlab/ref/ctranspose.html) for both the real and the complex. Also I use dot (mathworks.com/help/matlab/ref/dot.html) for the dot products. Problem This way of solving works perfectly when $J$ is real (even if $b$ is complex). That is, solving $Jz=b$ directly or by building the Schur-complement gives the same $z$ as expected. However, when $J_4$ becomes complex, then I get different results... I suspect that I did something really wrong when taking the transpose for $\tilde{J_2}$ or doing the dot product for $J_2J_4^{-1}b_y$. Yet, I fail to detect my error. Note: The reason I solve using the Schur-complement is that matrices $J_2,J_3,J_4$ have some particular structures that allow for good parallelization of the operations involved. E.g. $J_4$ is block diagonal and non-singular.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'sparse-matrices', 'schur-complement']"
63,Matrices with Parallel Column Differences,Matrices with Parallel Column Differences,,"I have two $p\times q$ matrices $A$ and $B$ that belong to the set: $$ \{X\in R^{p\times q}: \sum_{i,j} X_{i,j}^2=1 ; \sum_{i,j} X_{i,j}=0 \} $$ Also, showing the $i$'th column of $A$ by $a_i$, I know that $$ (a_i - a_j) = c_{ij} (b_i-b_j) ; \qquad c_{ij}>0 $$  which means that $(a_i - a_j)$ and $(b_i - b_j)$ are parallel and in the same direction for all $i$ and $j$. Can I conclude $A=B$ ?","I have two $p\times q$ matrices $A$ and $B$ that belong to the set: $$ \{X\in R^{p\times q}: \sum_{i,j} X_{i,j}^2=1 ; \sum_{i,j} X_{i,j}=0 \} $$ Also, showing the $i$'th column of $A$ by $a_i$, I know that $$ (a_i - a_j) = c_{ij} (b_i-b_j) ; \qquad c_{ij}>0 $$  which means that $(a_i - a_j)$ and $(b_i - b_j)$ are parallel and in the same direction for all $i$ and $j$. Can I conclude $A=B$ ?",,"['linear-algebra', 'matrices', 'algebra-precalculus', 'matrix-calculus']"
64,The column space of a positive semi-definite matrix A is contained in the column space of A+B?,The column space of a positive semi-definite matrix A is contained in the column space of A+B?,,"In this problem I'm given that A and B are both positive semi-definite matrices. I have to show that A+B is also a positive semi-definite matrix, but that part is simple. I'm also responsible for showing that the column space of A must be contained in the column space of the matrix A+B, and this is the part that evades me... This looks as if it may be possible to solve by contradiction (if an element from the column space of A is not in the column space of A+B then perhaps somehow this violates the positive semi-definiteness of A, B, or A+B?), but all of my attempts to show this (by contradiction or otherwise) have gone nowhere. Any hints would be greatly appreciated!","In this problem I'm given that A and B are both positive semi-definite matrices. I have to show that A+B is also a positive semi-definite matrix, but that part is simple. I'm also responsible for showing that the column space of A must be contained in the column space of the matrix A+B, and this is the part that evades me... This looks as if it may be possible to solve by contradiction (if an element from the column space of A is not in the column space of A+B then perhaps somehow this violates the positive semi-definiteness of A, B, or A+B?), but all of my attempts to show this (by contradiction or otherwise) have gone nowhere. Any hints would be greatly appreciated!",,"['linear-algebra', 'matrices', 'statistics', 'elementary-set-theory', 'vector-spaces']"
65,"$S = \left\{ x^* Ax\mid x \in C^n ,\ x^*x = 1 \right\} \implies S\;$ is compact and convex",is compact and convex,"S = \left\{ x^* Ax\mid x \in C^n ,\ x^*x = 1 \right\} \implies S\;","Let $\,A \in {\mathbb{C}^{n \times n}}\,$ and $\,S = \left\{ {{x^*}Ax \mid x \in \mathbb C^n,\ {x^*}x = 1} \right\}.\,$ Why is $A$ compact and convex ?","Let $\,A \in {\mathbb{C}^{n \times n}}\,$ and $\,S = \left\{ {{x^*}Ax \mid x \in \mathbb C^n,\ {x^*}x = 1} \right\}.\,$ Why is $A$ compact and convex ?",,"['linear-algebra', 'matrices', 'functional-analysis']"
66,Equality of two matrices,Equality of two matrices,,"If we have a diagonal matrix D which verify $D = A^*MA = B^*MB$ where $^*$ denotes the conjugate transpose, with A, B and M being unitary matrices Plus, B is symetric, M is real, and none of these matrices are equal to I. Do we have $A = B$ ?","If we have a diagonal matrix D which verify $D = A^*MA = B^*MB$ where $^*$ denotes the conjugate transpose, with A, B and M being unitary matrices Plus, B is symetric, M is real, and none of these matrices are equal to I. Do we have $A = B$ ?",,"['matrices', 'orthonormal']"
67,Proving that $L_{22}L_{22}^T=S$ is the Schur complement of a Cholesky factorization,Proving that  is the Schur complement of a Cholesky factorization,L_{22}L_{22}^T=S,"Let $A$ be an $(n+m) \times (n+m)$ symmetric positive definite matrix $$A=\begin{bmatrix}A_{11} & A_{12}\\ A_{12}^T & A_{22}\end{bmatrix}$$ where $A_{11}$ is an $n \times n$ matrix, $A_{12}$ is an $n \times m$ matrix, and $A_{22}$ is an $m \times m$ matrix. We can factor $A$ into $LL^T$ , as follows: $$A=\begin{bmatrix}L_{11} & 0\\L_{21} & L_{22} \end{bmatrix}\begin{bmatrix}L_{11} & 0\\L_{21} & L_{22} \end{bmatrix}^T \text{,}$$ where $L_{11}$ is an $n \times n$ , $L_{21}$ is $n \times m$ , and $L_{22}$ is $m \times m$ . I want to show that $S=L_{22}L_{22}^T$ where $S=A_{22}-A_{12}^TA_{11}^{-1}A_{12}$ is the Schur complement of $A_{11}$ . I'm stuck trying to unravel how to apply this to block matrices.  I'm having a bit of a conceptual hurdle translating my definitions to this problem.","Let be an symmetric positive definite matrix where is an matrix, is an matrix, and is an matrix. We can factor into , as follows: where is an , is , and is . I want to show that where is the Schur complement of . I'm stuck trying to unravel how to apply this to block matrices.  I'm having a bit of a conceptual hurdle translating my definitions to this problem.","A (n+m) \times (n+m) A=\begin{bmatrix}A_{11} & A_{12}\\ A_{12}^T & A_{22}\end{bmatrix} A_{11} n \times n A_{12} n \times m A_{22} m \times m A LL^T A=\begin{bmatrix}L_{11} & 0\\L_{21} & L_{22} \end{bmatrix}\begin{bmatrix}L_{11} & 0\\L_{21} & L_{22} \end{bmatrix}^T \text{,} L_{11} n \times n L_{21} n \times m L_{22} m \times m S=L_{22}L_{22}^T S=A_{22}-A_{12}^TA_{11}^{-1}A_{12} A_{11}","['linear-algebra', 'matrices', 'matrix-decomposition', 'schur-complement', 'cholesky-decomposition']"
68,Show that any 2D vectors can be expressed in the form...,Show that any 2D vectors can be expressed in the form...,,"(a) Show that any 2D vector can be expressed in the form   $s \begin{pmatrix} 3 \\ -1 \end{pmatrix} + t \begin{pmatrix} 2 \\ 7 \end{pmatrix},$   where $s$ and $t$ are real numbers. (b) Let $u$ and $v$ be non-zero vectors. Show that any 2D vector can be expressed in the form   $s u + t v$   where $s$ and $t$ are real numbers, if and only if of the vectors $u$ and $v$, one vector is not a scalar multiple of the other vector. No idea what to do to start these problems. Hints appreciated.","(a) Show that any 2D vector can be expressed in the form   $s \begin{pmatrix} 3 \\ -1 \end{pmatrix} + t \begin{pmatrix} 2 \\ 7 \end{pmatrix},$   where $s$ and $t$ are real numbers. (b) Let $u$ and $v$ be non-zero vectors. Show that any 2D vector can be expressed in the form   $s u + t v$   where $s$ and $t$ are real numbers, if and only if of the vectors $u$ and $v$, one vector is not a scalar multiple of the other vector. No idea what to do to start these problems. Hints appreciated.",,"['calculus', 'matrices', 'algebra-precalculus', 'trigonometry', 'vectors']"
69,Does the spectral radius of a matrix define a norm?,Does the spectral radius of a matrix define a norm?,,"Does the spectral radius of a matrix define a norm? Does it satisfy the properties of norm, i.e., $\| x \| \ge 0$ $\| x \| = 0 \implies x=0$ $\| kx \| = |k| \cdot \|x \|$ $\| x+y \| \le \|x\| + \|y\|$","Does the spectral radius of a matrix define a norm? Does it satisfy the properties of norm, i.e., $\| x \| \ge 0$ $\| x \| = 0 \implies x=0$ $\| kx \| = |k| \cdot \|x \|$ $\| x+y \| \le \|x\| + \|y\|$",,"['matrices', 'normed-spaces', 'spectral-radius']"
70,Comparing $\text{tr}(A^{-1})$ and $\text{tr}(A(B+A)^{-2})$ for pd $A$ and psd $B$,Comparing  and  for pd  and psd,\text{tr}(A^{-1}) \text{tr}(A(B+A)^{-2}) A B,"Suppose that $A$ is positive definite and $B$ positive semidefinite, both with dimension $n\times n$. Is there some inequality between $$ \text{tr}(A^{-1})\quad\text{and}\quad\text{tr}(A(B+A)^{-2})? $$ Progress so far : When $n=1$ with $A=a>0$, $B=b\geq 0$, we have $$ \text{tr}(A(B+A)^{-2})=\frac{a}{(a+b)^2}\leq\frac{a}{a^2}=\frac{1}{a}=\text{tr}(A^{-1}). $$ So it is suggestive that $\text{tr}(A(B+A)^{-2})\leq \text{tr}(A^{-1})$. I have tried \begin{align*} \text{tr}(A(B+A)^{-2})&=\text{tr}[A[B+A]^{-1}[B+A]^{-1}]\\ &=\text{tr}[A[A^{1/2}(A^{-1/2}BA^{-1/2}+I)A^{1/2}]^{-1}[A^{1/2}(A^{-1/2}BA^{-1/2}+I)A^{1/2}]^{-1}]\\ &=\text{tr}[AA^{-1/2}(A^{-1/2}BA^{-1/2}+I)^{-1}A^{-1}(A^{-1/2}BA^{-1/2}+I)^{-1}A^{-1/2}]\\ &=\text{tr}[A^{-1}(A^{-1/2}BA^{-1/2}+I)^{-2}]\\ &\leq\text{tr}[A^{-1}]\text{tr}[(A^{-1/2}BA^{-1/2}+I)^{-2}]. \end{align*} If $e_1,\ldots,e_n\geq 0$ are the eigenvalues of $A^{-1/2}BA^{-1/2}$, then $$ \text{tr}[(A^{-1/2}BA^{-1/2}+I)^{-2}]=\sum_{i=1}^n\frac{1}{(e_i+1)^2}\leq \sum_{i=1}^n\frac{1}{(0+1)^2}=n. $$ Thus, we have proved $$ \text{tr}(A(B+A)^{-2})\leq n\text{tr}[A^{-1}]. $$ But this isn't really what we seek. Plus, when $B$ is the zero matrix, $\text{tr}(A(B+A)^{-2})=\text{tr}[A^{-1}]$ so it seems the inequality can be made tigher.","Suppose that $A$ is positive definite and $B$ positive semidefinite, both with dimension $n\times n$. Is there some inequality between $$ \text{tr}(A^{-1})\quad\text{and}\quad\text{tr}(A(B+A)^{-2})? $$ Progress so far : When $n=1$ with $A=a>0$, $B=b\geq 0$, we have $$ \text{tr}(A(B+A)^{-2})=\frac{a}{(a+b)^2}\leq\frac{a}{a^2}=\frac{1}{a}=\text{tr}(A^{-1}). $$ So it is suggestive that $\text{tr}(A(B+A)^{-2})\leq \text{tr}(A^{-1})$. I have tried \begin{align*} \text{tr}(A(B+A)^{-2})&=\text{tr}[A[B+A]^{-1}[B+A]^{-1}]\\ &=\text{tr}[A[A^{1/2}(A^{-1/2}BA^{-1/2}+I)A^{1/2}]^{-1}[A^{1/2}(A^{-1/2}BA^{-1/2}+I)A^{1/2}]^{-1}]\\ &=\text{tr}[AA^{-1/2}(A^{-1/2}BA^{-1/2}+I)^{-1}A^{-1}(A^{-1/2}BA^{-1/2}+I)^{-1}A^{-1/2}]\\ &=\text{tr}[A^{-1}(A^{-1/2}BA^{-1/2}+I)^{-2}]\\ &\leq\text{tr}[A^{-1}]\text{tr}[(A^{-1/2}BA^{-1/2}+I)^{-2}]. \end{align*} If $e_1,\ldots,e_n\geq 0$ are the eigenvalues of $A^{-1/2}BA^{-1/2}$, then $$ \text{tr}[(A^{-1/2}BA^{-1/2}+I)^{-2}]=\sum_{i=1}^n\frac{1}{(e_i+1)^2}\leq \sum_{i=1}^n\frac{1}{(0+1)^2}=n. $$ Thus, we have proved $$ \text{tr}(A(B+A)^{-2})\leq n\text{tr}[A^{-1}]. $$ But this isn't really what we seek. Plus, when $B$ is the zero matrix, $\text{tr}(A(B+A)^{-2})=\text{tr}[A^{-1}]$ so it seems the inequality can be made tigher.",,"['calculus', 'linear-algebra', 'matrices', 'inequality']"
71,$A^tA-AA^t$ in Mathematical Physics,in Mathematical Physics,A^tA-AA^t,"In very different contexts of mathematical physics (rigid body mechanics, fluidodynamics, general relativity, quantum field theory,...) I have come across the following expression: $$ A^tA-AA^t, $$ where $A$ is a square matrix and $A^t$ its transpose, or the vector analogue: $$ vv^t-\Bbb{1}v^tv, $$ where $\Bbb{1}$ is the identity and $v$ is a vector (or also the differential operator $\nabla$). What do the quantities above really measure? Is there an intuitive, geometric or algebraic meaning? Any explanation which works only in one context (say, fluidodynamics) would be also welcome. Thanks! Addendum : Thanks to Autolatry's answer, here is an example of what I mean by ""intuitive meaning"": The first term alone, namely $A^tA$, has the significance of a magnitude (at least in the Euclidean case). It gives the metric tensor for embedded submanifolds, it gives the length of a vector, its determinant gives the volume form... I'm asking if the difference of the two terms above also has a geometrical meaning.","In very different contexts of mathematical physics (rigid body mechanics, fluidodynamics, general relativity, quantum field theory,...) I have come across the following expression: $$ A^tA-AA^t, $$ where $A$ is a square matrix and $A^t$ its transpose, or the vector analogue: $$ vv^t-\Bbb{1}v^tv, $$ where $\Bbb{1}$ is the identity and $v$ is a vector (or also the differential operator $\nabla$). What do the quantities above really measure? Is there an intuitive, geometric or algebraic meaning? Any explanation which works only in one context (say, fluidodynamics) would be also welcome. Thanks! Addendum : Thanks to Autolatry's answer, here is an example of what I mean by ""intuitive meaning"": The first term alone, namely $A^tA$, has the significance of a magnitude (at least in the Euclidean case). It gives the metric tensor for embedded submanifolds, it gives the length of a vector, its determinant gives the volume form... I'm asking if the difference of the two terms above also has a geometrical meaning.",,"['linear-algebra', 'matrices', 'intuition', 'mathematical-physics', 'geometric-interpretation']"
72,Show determinant of $\left[\begin{matrix} A & 0 \\ C & D\end{matrix}\right] = \det{A}\cdot \det{D}$ [duplicate],Show determinant of  [duplicate],\left[\begin{matrix} A & 0 \\ C & D\end{matrix}\right] = \det{A}\cdot \det{D},"This question already has answers here : The determinant of block triangular matrix as product of determinants of diagonal blocks (2 answers) Closed 6 years ago . Let $A \in \mathbb{R}^{n, n}$ , $B \in \mathbb{R}^{n, m}$ , $C \in \mathbb{R}^{m, n}$ and $D \in \mathbb{R}^{m, m}$ be matrices. Now, I have seen on Wikipedia the explanation of why determinant of $\left[\begin{matrix} A & 0 \\ C & D\end{matrix}\right] = \det{A}\cdot \det{D}$ , but I still did not get it. Specifically, the explanation is: This can be seen ... from a decomposition like: $\left[\begin{matrix} A & 0 \\ C & D\end{matrix}\right] = \left[\begin{matrix} A & 0 \\ C & I_{m}\end{matrix}\right]\left[\begin{matrix} I_n & 0 \\ 0 & D\end{matrix}\right]$ I understood that the equation is true from the standard rules of matrix-matrix multiplication, but it is still not too clear why this should prove what we want to prove or show. If $A$ , $B$ , $C$ and $D$ were regular reals (and $I_{i}$ was $1$ ), then the equation and the explanation would be obvious, because of the standard rules of calculating determinants... But in this case, I cannot understand why the equation shows that the final determinant is $$\det{A} \cdot \det{D}$$ Those 2 matrices $\left[\begin{matrix} A & 0 \\ C & I_{m}\end{matrix}\right]$ and $\left[\begin{matrix} I_n & 0 \\ 0 & D\end{matrix}\right]$ basically could not be triangular or diagonal matrices, from my understanding...","This question already has answers here : The determinant of block triangular matrix as product of determinants of diagonal blocks (2 answers) Closed 6 years ago . Let , , and be matrices. Now, I have seen on Wikipedia the explanation of why determinant of , but I still did not get it. Specifically, the explanation is: This can be seen ... from a decomposition like: I understood that the equation is true from the standard rules of matrix-matrix multiplication, but it is still not too clear why this should prove what we want to prove or show. If , , and were regular reals (and was ), then the equation and the explanation would be obvious, because of the standard rules of calculating determinants... But in this case, I cannot understand why the equation shows that the final determinant is Those 2 matrices and basically could not be triangular or diagonal matrices, from my understanding...","A \in \mathbb{R}^{n, n} B \in \mathbb{R}^{n, m} C \in \mathbb{R}^{m, n} D \in \mathbb{R}^{m, m} \left[\begin{matrix} A & 0 \\ C & D\end{matrix}\right] = \det{A}\cdot \det{D} \left[\begin{matrix} A & 0 \\ C & D\end{matrix}\right] = \left[\begin{matrix} A & 0 \\ C & I_{m}\end{matrix}\right]\left[\begin{matrix} I_n & 0 \\ 0 & D\end{matrix}\right] A B C D I_{i} 1 \det{A} \cdot \det{D} \left[\begin{matrix} A & 0 \\ C & I_{m}\end{matrix}\right] \left[\begin{matrix} I_n & 0 \\ 0 & D\end{matrix}\right]","['matrices', 'solution-verification']"
73,Matrix with prime entries and largest possible determinant,Matrix with prime entries and largest possible determinant,,"Let $n\ge 1$ be a natural number. Arrange the first $n^2$ primes in a $n\times n$-matrix, such that the determinant becomes as large as possible. What is the largest possible determinant and which matrix has it ? For $n=2$, we have $\pmatrix{7&3\\2&5}$ with determinant $29$, which is optimal. For $n=3$, we have $\pmatrix{23&7&3\\5&19&13\\11&2&17}$ with determinant $6640$, which is optimal. For $n=4$, the best value I found is $\pmatrix{37&7&47&23\\17&19&11&53\\41&31&2&5\\3&43&29&13}$ with deteminant $4673460$.","Let $n\ge 1$ be a natural number. Arrange the first $n^2$ primes in a $n\times n$-matrix, such that the determinant becomes as large as possible. What is the largest possible determinant and which matrix has it ? For $n=2$, we have $\pmatrix{7&3\\2&5}$ with determinant $29$, which is optimal. For $n=3$, we have $\pmatrix{23&7&3\\5&19&13\\11&2&17}$ with determinant $6640$, which is optimal. For $n=4$, the best value I found is $\pmatrix{37&7&47&23\\17&19&11&53\\41&31&2&5\\3&43&29&13}$ with deteminant $4673460$.",,"['linear-algebra', 'matrices', 'prime-numbers', 'determinant']"
74,How is the perturbation in one column of a symmetric matrix reflected in its eigenvalues?,How is the perturbation in one column of a symmetric matrix reflected in its eigenvalues?,,"Suppose we have a 0-1 square symmetric matrix. Then it's eigenvalues are real. But I have observed that by multiplying any of its column by a positive constant, even if the matrix is not symmetric still its eigenvalues are real. Further, the inertia (the number of positive, negative, zero eigenvalues) remains the same. Now how to prove these observations.  Also if possible, suggest me - 'what is the relationship between the spectra of the two matrices?'.","Suppose we have a 0-1 square symmetric matrix. Then it's eigenvalues are real. But I have observed that by multiplying any of its column by a positive constant, even if the matrix is not symmetric still its eigenvalues are real. Further, the inertia (the number of positive, negative, zero eigenvalues) remains the same. Now how to prove these observations.  Also if possible, suggest me - 'what is the relationship between the spectra of the two matrices?'.",,"['linear-algebra', 'matrices']"
75,A matrix representation for the inverse matrix.,A matrix representation for the inverse matrix.,,"I have a problem from ""Methods of Algebriac Geometry in Control Theory by Peter Falb"" textbook: Show that if $A$ is $\,n\times n\,$ matrix, then $\displaystyle\,(zI-A)^{-1} = \sum_{j=1}^n \phi_j\left(z\right) A^{n-j} \big/ \det\left[zI-A\right]$. Compute $\phi_j\left(z\right)$. I am using the following formula: $$ {\left(zI-A\right)}^{-1} =  \frac{1}{\det \left(zI-A\right)} \sum_{s=0}^{n-1}\left(zI-A\right)^{s} \sum_{k_1,k_2,\ldots ,k_{n-1}} \prod_{l=1}^{n-1} \frac{\left(-1\right)^{k_l+1}}{l^{k_l}\,k_{l}!}\, \left(\operatorname{tr}\left(zI-A\right)^l\right)^{k_l} $$  where the second summation is over $\displaystyle\,j+\sum_{l=1}^{n-1}lk_l=n-1$. But I don't see how to find $\,\phi_j\left(z\right)$. Can anyone help me on this? Thanks in advance. P.S In the book it's stated that $\phi_j(z)$ is a polynomial of degree $n-j$.","I have a problem from ""Methods of Algebriac Geometry in Control Theory by Peter Falb"" textbook: Show that if $A$ is $\,n\times n\,$ matrix, then $\displaystyle\,(zI-A)^{-1} = \sum_{j=1}^n \phi_j\left(z\right) A^{n-j} \big/ \det\left[zI-A\right]$. Compute $\phi_j\left(z\right)$. I am using the following formula: $$ {\left(zI-A\right)}^{-1} =  \frac{1}{\det \left(zI-A\right)} \sum_{s=0}^{n-1}\left(zI-A\right)^{s} \sum_{k_1,k_2,\ldots ,k_{n-1}} \prod_{l=1}^{n-1} \frac{\left(-1\right)^{k_l+1}}{l^{k_l}\,k_{l}!}\, \left(\operatorname{tr}\left(zI-A\right)^l\right)^{k_l} $$  where the second summation is over $\displaystyle\,j+\sum_{l=1}^{n-1}lk_l=n-1$. But I don't see how to find $\,\phi_j\left(z\right)$. Can anyone help me on this? Thanks in advance. P.S In the book it's stated that $\phi_j(z)$ is a polynomial of degree $n-j$.",,"['linear-algebra', 'matrices']"
76,Linear decomposition of positive semi-definite matrices,Linear decomposition of positive semi-definite matrices,,"It is true that the vector space of $n\times n$ Hermitian matrices is an $n^2-$dimensional real vector space and that one can find a basis for this space consisting exclusively of positive semi-definite matrices (see basis for hermitian matrices ). My question is, if we have the linear combination  $$ M=\sum_{k=1}^{n^2}x_kB_k $$ where $B_k$ is the aforementioned positive semi definite basis matrix, what are the restrictions on $x_k$ so that $M$ remains positive semi-definite? A sufficient condition is (I think) that all $x_k\geq 0$, but I don't think it is necessary. If $B_k$ were real numbers, the solution to that problem would be one of the two halfspaces into which $\mathbb{R}^{n^2}$ is divided into by the hyperplane $\sum_{k=1}^{n^2}x_kB_k=0$. Could there be an analogy with this case, when $B_k$ belong to the vector space of Hermitian matrices?","It is true that the vector space of $n\times n$ Hermitian matrices is an $n^2-$dimensional real vector space and that one can find a basis for this space consisting exclusively of positive semi-definite matrices (see basis for hermitian matrices ). My question is, if we have the linear combination  $$ M=\sum_{k=1}^{n^2}x_kB_k $$ where $B_k$ is the aforementioned positive semi definite basis matrix, what are the restrictions on $x_k$ so that $M$ remains positive semi-definite? A sufficient condition is (I think) that all $x_k\geq 0$, but I don't think it is necessary. If $B_k$ were real numbers, the solution to that problem would be one of the two halfspaces into which $\mathbb{R}^{n^2}$ is divided into by the hyperplane $\sum_{k=1}^{n^2}x_kB_k=0$. Could there be an analogy with this case, when $B_k$ belong to the vector space of Hermitian matrices?",,"['linear-algebra', 'matrices']"
77,Let $A$ is nonsingular and each eigenvalue of $A$ is either $+1$ or $-1$.Why $A$ is similar to ${A^{ - 1}}$?,Let  is nonsingular and each eigenvalue of  is either  or .Why  is similar to ?,A A +1 -1 A {A^{ - 1}},Let $A \in {M_n}$ is nonsingular and each eigenvalue of $A$ is either $+1$ or $-1$.Why $A$ is similar to ${A^{ - 1}}$?,Let $A \in {M_n}$ is nonsingular and each eigenvalue of $A$ is either $+1$ or $-1$.Why $A$ is similar to ${A^{ - 1}}$?,,"['linear-algebra', 'matrices']"
78,Markov chain ergodicity,Markov chain ergodicity,,"$(X_n)_n$ is a discrete-time, time-homogenous Markov chain. I have have the following transition matrix and  want to show whether the chain is ergodic. $$P = \begin{pmatrix} \frac{1}{2} & 0 & 0 & \frac{1}{2}\\ 0 & \frac{1}{2} & \frac{1}{2} & 0\\ 0 & \frac{1}{2} & \frac{1}{4} & \frac{1}{4}\\ \frac{1}{2} & 0 & 0 & \frac{1}{2}\end{pmatrix}$$ I tried to compute $P^2 P^3 $.... I don't have all coefficients $>0$. There is a unique stationary distribution [$\frac{1}{2},0,0,\frac{1}{2}$] so I can't prove this is not ergodic. More over, I can't prove that the distribution is periodic for example $P=P^5$ . I tried to show that the matrix is irreducible with $A=(A+ \, \text{Id})^n$ ( find a n with all coefficients >0). I am quite stuck now, thanks for your help","$(X_n)_n$ is a discrete-time, time-homogenous Markov chain. I have have the following transition matrix and  want to show whether the chain is ergodic. $$P = \begin{pmatrix} \frac{1}{2} & 0 & 0 & \frac{1}{2}\\ 0 & \frac{1}{2} & \frac{1}{2} & 0\\ 0 & \frac{1}{2} & \frac{1}{4} & \frac{1}{4}\\ \frac{1}{2} & 0 & 0 & \frac{1}{2}\end{pmatrix}$$ I tried to compute $P^2 P^3 $.... I don't have all coefficients $>0$. There is a unique stationary distribution [$\frac{1}{2},0,0,\frac{1}{2}$] so I can't prove this is not ergodic. More over, I can't prove that the distribution is periodic for example $P=P^5$ . I tried to show that the matrix is irreducible with $A=(A+ \, \text{Id})^n$ ( find a n with all coefficients >0). I am quite stuck now, thanks for your help",,"['matrices', 'probability-theory', 'markov-chains']"
79,How to find if this huge vector is in the column space of this huge matrix?,How to find if this huge vector is in the column space of this huge matrix?,,"I newbie to linear algebra, so I hope you are patient with me. I have to say if a vector $\vec{u} = \left[ \begin{matrix} 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1\end{matrix} \right]$ is in the column space of a matrix $A = \left[ \begin{matrix} 1 & 1 & 0 & 0 & 1 & 0 & 0 \\ 1&0&0&-1&0&1&1 \\ 0&1&0&0&-1&0&1 \\ 0&0&0&0&0&0&1\\0&0&-1&0&1&0&1 \\ 0&0&0&0&0&0&0 \\ 0&1&1&1&0&1&-1\end{matrix} \right]$. My intuition is to say no, because, if I am not wrong, what we need to find is a vector $\vec{v}$ such that $$A\vec{v} = \vec{u}$$ But there's no vector such that multiplied by a row of all $0$s (the matrix row before its last one), which will give $1$, the component before the last of $\vec{u}$. Am I right or what? Should I need to find the $RREF$ anyway?","I newbie to linear algebra, so I hope you are patient with me. I have to say if a vector $\vec{u} = \left[ \begin{matrix} 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1\end{matrix} \right]$ is in the column space of a matrix $A = \left[ \begin{matrix} 1 & 1 & 0 & 0 & 1 & 0 & 0 \\ 1&0&0&-1&0&1&1 \\ 0&1&0&0&-1&0&1 \\ 0&0&0&0&0&0&1\\0&0&-1&0&1&0&1 \\ 0&0&0&0&0&0&0 \\ 0&1&1&1&0&1&-1\end{matrix} \right]$. My intuition is to say no, because, if I am not wrong, what we need to find is a vector $\vec{v}$ such that $$A\vec{v} = \vec{u}$$ But there's no vector such that multiplied by a row of all $0$s (the matrix row before its last one), which will give $1$, the component before the last of $\vec{u}$. Am I right or what? Should I need to find the $RREF$ anyway?",,"['linear-algebra', 'matrices']"
80,Maximize trace of matrix equation given two constraints,Maximize trace of matrix equation given two constraints,,"Let $\mathbf{Q}$ be a rotation matrix and $\mathbf{A}$ and $\mathbf{B}$ be two real-valued matrices of the same size. I want to maximize the function $$ f(\mathbf{Q})=tr\;\mathbf{QA} \qquad \text{s.t.} \; \mathbf{Q}'\mathbf{Q} = \mathbf{I} \quad \text{and} \quad  tr\; \mathbf{QB} \geq 0 $$ If only the orthogonality constraint is imposed, the solution is  $$ \mathbf{Q} = \mathbf{VU}'  $$ (see e.g. Cliff, 1966 ) with $\mathbf{U}$ and $\mathbf{V}$ from the singular value decomposition of $\mathbf{A} = \mathbf{U \Lambda V}'$. Question : How can I find a solution that also includes the second constraint, i.e. $tr\; \mathbf{QB} \geq 0$, given that it exists? Footnote: I am not a mathematician, so please be verbose in your answer, so I have a chance to follow :) Literature Cliff, N. (1966). Orthogonal rotation to congruence. Psychometrika, 31(1), 33–42. http://doi.org/10.1007/BF02289455","Let $\mathbf{Q}$ be a rotation matrix and $\mathbf{A}$ and $\mathbf{B}$ be two real-valued matrices of the same size. I want to maximize the function $$ f(\mathbf{Q})=tr\;\mathbf{QA} \qquad \text{s.t.} \; \mathbf{Q}'\mathbf{Q} = \mathbf{I} \quad \text{and} \quad  tr\; \mathbf{QB} \geq 0 $$ If only the orthogonality constraint is imposed, the solution is  $$ \mathbf{Q} = \mathbf{VU}'  $$ (see e.g. Cliff, 1966 ) with $\mathbf{U}$ and $\mathbf{V}$ from the singular value decomposition of $\mathbf{A} = \mathbf{U \Lambda V}'$. Question : How can I find a solution that also includes the second constraint, i.e. $tr\; \mathbf{QB} \geq 0$, given that it exists? Footnote: I am not a mathematician, so please be verbose in your answer, so I have a chance to follow :) Literature Cliff, N. (1966). Orthogonal rotation to congruence. Psychometrika, 31(1), 33–42. http://doi.org/10.1007/BF02289455",,"['linear-algebra', 'matrices', 'matrix-equations', 'trace']"
81,LU Factorization - Linear Algebra,LU Factorization - Linear Algebra,,LU-factorization My solution: Am I on the right path? Or am I completely off?,LU-factorization My solution: Am I on the right path? Or am I completely off?,,"['linear-algebra', 'matrices', 'factoring']"
82,Relation between rank of matrix over $K$ and over $L\supset K$,Relation between rank of matrix over  and over,K L\supset K,"Let $K$ be a subfield of a larger field $L$ (e.g. $\mathbb{Q}$ over $\mathbb{R}$). Let $A$ be an $m\times n$ matrix with entries in $K$. Then $A$ can be regarded as either a matrix in $K(m,n)$ or as a matrix in $L(m,n)$. Show that the rank of $A$ is the same in either case. (I'm thinking that the rank of $A$ is the number of non-zero rows in its row-reduced form, but I don't know how to construct a formal proof.) Thanks!","Let $K$ be a subfield of a larger field $L$ (e.g. $\mathbb{Q}$ over $\mathbb{R}$). Let $A$ be an $m\times n$ matrix with entries in $K$. Then $A$ can be regarded as either a matrix in $K(m,n)$ or as a matrix in $L(m,n)$. Show that the rank of $A$ is the same in either case. (I'm thinking that the rank of $A$ is the number of non-zero rows in its row-reduced form, but I don't know how to construct a formal proof.) Thanks!",,"['matrices', 'extension-field', 'matrix-rank']"
83,Multiplication order of rotation matrices,Multiplication order of rotation matrices,,"I have three 3D coordinate frames: O, A and B, as shown below. I want to know the rotation matrix R AB between A and B, that is the rotation that is required, with respect to the frame A, to move from A to B. Let us imagine that all I know, is the rotation matrix R AO between A and O, and the rotation matrix R OB between O and B. By inspecting the above diagram: $$ R_{AO} = \left [\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \\ \end{array} \right ]$$ $$ R_{OB} = \left [\begin{array}{ccc} 0 & 0 & 1 \\ -1 & 0 & 0 \\ 0 & -1 & 0 \\ \end{array} \right ]$$ So, what is the correct way to determine R AB ? There are two suggestions that come to mind: (1) R AB = R AO R OB (2) R AB = R OB R AO Now, my intuition is that (1) is correct, i.e. post-multiplication. This is because I am multiplying everything with respect to the local coordinate frame (as discussed in http://web.cse.ohio-state.edu/~whmin/courses/cse5542-2013-spring/6-Transformation_II.pdf ). However, when I compute this, I get: $$ R_{AB} = \left [\begin{array}{ccc} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \\ \end{array} \right ]$$ Whereas by inspection of the diagram, it should be: $$ R_{AB} = \left [\begin{array}{ccc} 0 & 1 & 0 \\ -1 & 0 & 0 \\ 0 & 0 & 1 \\ \end{array} \right ]$$ Which, I have noticed, is equal to the pre-multiplication solution of (2). Please can somebody explain to me why (2) seems to be correct, rather than (1)? I was under the impression that if all your transformations are with respect to the current local frame, then post-multiplication should be done, i.e. multiply the matrices from left to right as you move between each frame. However, when doing the maths, pre-multiplication gives the expected answer.","I have three 3D coordinate frames: O, A and B, as shown below. I want to know the rotation matrix R AB between A and B, that is the rotation that is required, with respect to the frame A, to move from A to B. Let us imagine that all I know, is the rotation matrix R AO between A and O, and the rotation matrix R OB between O and B. By inspecting the above diagram: $$ R_{AO} = \left [\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \\ \end{array} \right ]$$ $$ R_{OB} = \left [\begin{array}{ccc} 0 & 0 & 1 \\ -1 & 0 & 0 \\ 0 & -1 & 0 \\ \end{array} \right ]$$ So, what is the correct way to determine R AB ? There are two suggestions that come to mind: (1) R AB = R AO R OB (2) R AB = R OB R AO Now, my intuition is that (1) is correct, i.e. post-multiplication. This is because I am multiplying everything with respect to the local coordinate frame (as discussed in http://web.cse.ohio-state.edu/~whmin/courses/cse5542-2013-spring/6-Transformation_II.pdf ). However, when I compute this, I get: $$ R_{AB} = \left [\begin{array}{ccc} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \\ \end{array} \right ]$$ Whereas by inspection of the diagram, it should be: $$ R_{AB} = \left [\begin{array}{ccc} 0 & 1 & 0 \\ -1 & 0 & 0 \\ 0 & 0 & 1 \\ \end{array} \right ]$$ Which, I have noticed, is equal to the pre-multiplication solution of (2). Please can somebody explain to me why (2) seems to be correct, rather than (1)? I was under the impression that if all your transformations are with respect to the current local frame, then post-multiplication should be done, i.e. multiply the matrices from left to right as you move between each frame. However, when doing the maths, pre-multiplication gives the expected answer.",,"['linear-algebra', 'matrices', 'geometry', 'transformation', 'coordinate-systems']"
84,Singular value decomposition for matrices that are not square?,Singular value decomposition for matrices that are not square?,,"I understand that the Singular Value Decomposition is defined as SVD = $U\Sigma V^T$ , but I am slightly confused about the calculations when the matrix is not square.  For example, I have the matrix: $$ \begin{bmatrix} 1 & -1 \\ -2 & 2 \\ 2 & -2 \end{bmatrix} $$ When I am solving for $V$, however, I am missing the last component. Have I done something wrong when calculating for matrices that are not square matrices? $$\det(A^T A - \lambda I) =          \begin{bmatrix}         2 - \lambda & -4 & 4 \\         -4 & 8 - \lambda & -8\\         4 & -8 & 8 - \lambda         \end{bmatrix} $$ $\lambda = 0, 2, 16$ Eigenvectors respectively are:  \begin{bmatrix}                      1  \\                    1/2 \\         0  \end{bmatrix} \begin{bmatrix}                      1  \\                    2/7 \\        -2/7  \end{bmatrix} \begin{bmatrix}                      0  \\                    1 \\        -1  \end{bmatrix} Therefore $$\Sigma =  \begin{bmatrix}                      \sqrt 2  & 0 & 0  \\                    0 & \sqrt 16 & 0 \\         0 & 0 & 0  \end{bmatrix}$$ Also, $$V =  \begin{bmatrix}        7/\sqrt 57 & 0 & 2/\sqrt 5 \\        2/\sqrt 57 & 1/\sqrt 2 & 1/\sqrt 5 \\        2/\sqrt 57 & -1/\sqrt 2 & 0 \end{bmatrix}$$ This is the portion I am confused about. Is $U = AV / \sqrt\lambda $ ? What if I have am missing a vector so that I can only get the first two columns of $U$?","I understand that the Singular Value Decomposition is defined as SVD = $U\Sigma V^T$ , but I am slightly confused about the calculations when the matrix is not square.  For example, I have the matrix: $$ \begin{bmatrix} 1 & -1 \\ -2 & 2 \\ 2 & -2 \end{bmatrix} $$ When I am solving for $V$, however, I am missing the last component. Have I done something wrong when calculating for matrices that are not square matrices? $$\det(A^T A - \lambda I) =          \begin{bmatrix}         2 - \lambda & -4 & 4 \\         -4 & 8 - \lambda & -8\\         4 & -8 & 8 - \lambda         \end{bmatrix} $$ $\lambda = 0, 2, 16$ Eigenvectors respectively are:  \begin{bmatrix}                      1  \\                    1/2 \\         0  \end{bmatrix} \begin{bmatrix}                      1  \\                    2/7 \\        -2/7  \end{bmatrix} \begin{bmatrix}                      0  \\                    1 \\        -1  \end{bmatrix} Therefore $$\Sigma =  \begin{bmatrix}                      \sqrt 2  & 0 & 0  \\                    0 & \sqrt 16 & 0 \\         0 & 0 & 0  \end{bmatrix}$$ Also, $$V =  \begin{bmatrix}        7/\sqrt 57 & 0 & 2/\sqrt 5 \\        2/\sqrt 57 & 1/\sqrt 2 & 1/\sqrt 5 \\        2/\sqrt 57 & -1/\sqrt 2 & 0 \end{bmatrix}$$ This is the portion I am confused about. Is $U = AV / \sqrt\lambda $ ? What if I have am missing a vector so that I can only get the first two columns of $U$?",,"['linear-algebra', 'matrices', 'svd', 'matrix-decomposition']"
85,Problem from Biggs graph theory,Problem from Biggs graph theory,,"From Norman Biggs, Algebraic Graph Theory , 2nd edition 1993, p. 13, exercise 2i: 2i. An upper bound for the largest eigenvalue. Suppose that the eigenvalues of $\Gamma$ are $\lambda_0 \geq \lambda_1 \geq \ldots \geq \lambda_{n-1}$, where $\Gamma$ has $n$ vertices and $m$ edges. From 2h we obtain $\sum \lambda_i = 0$ and $\sum \lambda_i^2 = 2m$. It follows that $$\lambda_0 \leq \left(\dfrac{2m\left(n-1\right)}{n}\right)^{\frac{1}{2}}.$$ I have tried to solve this problem but I just can't.  The matrix of a graph on $n$ vertices is $n\times n$ with all entries $0$ or $1$, and diagonal $0$. My attempt: We know that for such a matrix $|\lambda_0|\leq n-1$. \begin{eqnarray*} \lambda_{0}^{2} & = & \frac{\lambda_{0}^{2}}{n}+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\  & = & \frac{\left|\lambda_{0}\right|^{2}}{n}+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\  & \leq & \left|\sum_{1\leq i}\lambda_{i}\right|\left(\frac{n-1}{n}\right)+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\  & \leq & \sum_{1\leq i}\left|\lambda_{i}\right|\left(\frac{n-1}{n}\right)+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\  & \leq & \sum_{1\leq i}\lambda_{i}^{2}\left(\frac{n-1}{n}\right)+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\  & = & \sum\lambda_{i}^{2}\left(\frac{n-1}{n}\right)\\  & = & \frac{2m(n-1)}{n} \end{eqnarray*} I think everything is ok until the last $\leq$. This would be true if all the eigenvalues were integers, but as someone pointed out, the eigenvalues of these matrices don't have to be integers.","From Norman Biggs, Algebraic Graph Theory , 2nd edition 1993, p. 13, exercise 2i: 2i. An upper bound for the largest eigenvalue. Suppose that the eigenvalues of $\Gamma$ are $\lambda_0 \geq \lambda_1 \geq \ldots \geq \lambda_{n-1}$, where $\Gamma$ has $n$ vertices and $m$ edges. From 2h we obtain $\sum \lambda_i = 0$ and $\sum \lambda_i^2 = 2m$. It follows that $$\lambda_0 \leq \left(\dfrac{2m\left(n-1\right)}{n}\right)^{\frac{1}{2}}.$$ I have tried to solve this problem but I just can't.  The matrix of a graph on $n$ vertices is $n\times n$ with all entries $0$ or $1$, and diagonal $0$. My attempt: We know that for such a matrix $|\lambda_0|\leq n-1$. \begin{eqnarray*} \lambda_{0}^{2} & = & \frac{\lambda_{0}^{2}}{n}+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\  & = & \frac{\left|\lambda_{0}\right|^{2}}{n}+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\  & \leq & \left|\sum_{1\leq i}\lambda_{i}\right|\left(\frac{n-1}{n}\right)+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\  & \leq & \sum_{1\leq i}\left|\lambda_{i}\right|\left(\frac{n-1}{n}\right)+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\  & \leq & \sum_{1\leq i}\lambda_{i}^{2}\left(\frac{n-1}{n}\right)+\lambda_{0}^{2}\left(\frac{n-1}{n}\right)\\  & = & \sum\lambda_{i}^{2}\left(\frac{n-1}{n}\right)\\  & = & \frac{2m(n-1)}{n} \end{eqnarray*} I think everything is ok until the last $\leq$. This would be true if all the eigenvalues were integers, but as someone pointed out, the eigenvalues of these matrices don't have to be integers.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
86,"Matrix norm question, normal matrices","Matrix norm question, normal matrices",,"Let $A^*$ denote the complex conjugate transpose of a matrix $A$. In the Euclidean norm (operator norm), if $$\|A^*A+AA^*\|=2\,\|A^*A\|$$ prove/disprove that $A$ is normal.","Let $A^*$ denote the complex conjugate transpose of a matrix $A$. In the Euclidean norm (operator norm), if $$\|A^*A+AA^*\|=2\,\|A^*A\|$$ prove/disprove that $A$ is normal.",,['matrices']
87,Invertible Matrices and Linear independence,Invertible Matrices and Linear independence,,"If a matrix is invertible, what does this tell us application wise? I am familiar with what it implies in regards to the properties of the matrix, i.e: the determinant is non-zero, and for a matrix $A$, $Ax=0$ implies $x=0$. However, during discussions in my lectures for an optimization class, a class mate always brings up non-invertible and invertible matrices and what they imply. I can't be too specific as I tend to get lost in discussion whenever this happens, but could anyone clue me in on what the significance of invertibility and linear independence (of which the definitions I am aware of) is? Kind regards.","If a matrix is invertible, what does this tell us application wise? I am familiar with what it implies in regards to the properties of the matrix, i.e: the determinant is non-zero, and for a matrix $A$, $Ax=0$ implies $x=0$. However, during discussions in my lectures for an optimization class, a class mate always brings up non-invertible and invertible matrices and what they imply. I can't be too specific as I tend to get lost in discussion whenever this happens, but could anyone clue me in on what the significance of invertibility and linear independence (of which the definitions I am aware of) is? Kind regards.",,"['linear-algebra', 'matrices']"
88,"Questions about matrix rank, trace, and invertibility","Questions about matrix rank, trace, and invertibility",,"(a) Prove that a square matrix $T$ of rank one has $\text{tr}(T)=0$ if and only if $T^2=0$. (b) Consider a matrix $A$ of the form $A=aI+T$, where $a\ne0$, $I$ is the identity matrix,   and $T$ has rank one and zero trace. Find the inverse and the determinant of $A$. (c) Find the inverse of $A$ as above when $T$ has rank one but nonzero trace $\text{tr}(T)=b$. For which value of $b$ is $A$ not invertible? I'm still stuck on part (a), but campus buildings are closing soon, so I'll be working from home but would love to get some hints / comments on this question.  I'll have limited access to this site - on my phone. For part (a), I've been trying to look at the SVD of matrix $A$, since one can read off the rank very easily - by looking at the number of non-zero singular values of $A$.  Then I am trying some block matrix multiplication to see whether $T^2 = 0$, from assuming that $\text{tr}(T) =0$.  So far, no luck.  Do you think I should stick with this SVD approach, or is it better to play around with the definition and properties of nilpotent operators? Any other hints for the other parts of the question would be greatly appreciated. Thanks!","(a) Prove that a square matrix $T$ of rank one has $\text{tr}(T)=0$ if and only if $T^2=0$. (b) Consider a matrix $A$ of the form $A=aI+T$, where $a\ne0$, $I$ is the identity matrix,   and $T$ has rank one and zero trace. Find the inverse and the determinant of $A$. (c) Find the inverse of $A$ as above when $T$ has rank one but nonzero trace $\text{tr}(T)=b$. For which value of $b$ is $A$ not invertible? I'm still stuck on part (a), but campus buildings are closing soon, so I'll be working from home but would love to get some hints / comments on this question.  I'll have limited access to this site - on my phone. For part (a), I've been trying to look at the SVD of matrix $A$, since one can read off the rank very easily - by looking at the number of non-zero singular values of $A$.  Then I am trying some block matrix multiplication to see whether $T^2 = 0$, from assuming that $\text{tr}(T) =0$.  So far, no luck.  Do you think I should stick with this SVD approach, or is it better to play around with the definition and properties of nilpotent operators? Any other hints for the other parts of the question would be greatly appreciated. Thanks!",,"['linear-algebra', 'matrices', 'inverse', 'trace', 'matrix-rank']"
89,Two different matrix representations of complex numbers,Two different matrix representations of complex numbers,,"There are two different ways to represent a complex number with $2 \times 2$ real matrices: $$ \rho: \mathbb{C} \rightarrow M_2(\mathbb{R}) \qquad  \rho(z)=\rho(a+ib)=  \left[   \begin{array}{ccccc} a&b  \\  -b &a \end{array} \right] $$ and $$ \bar\rho: \mathbb{C} \rightarrow M_2(\mathbb{R}) \qquad  \bar\rho(z)=\bar\rho(a+ib)=  \left[   \begin{array}{ccccc} a&-b  \\  b &a \end{array} \right] $$ We can find one or the other in many sources, and somewhere both, as in History of the matrix representation of complex numbers Clearly this two representations generate the same subring of $M_2(\mathbb{R})$ but ve have: $\bar \rho(z)=\rho(\bar z)$. The existence of this double representation  has some (hidden for me) significance or is purely fortuitous ?","There are two different ways to represent a complex number with $2 \times 2$ real matrices: $$ \rho: \mathbb{C} \rightarrow M_2(\mathbb{R}) \qquad  \rho(z)=\rho(a+ib)=  \left[   \begin{array}{ccccc} a&b  \\  -b &a \end{array} \right] $$ and $$ \bar\rho: \mathbb{C} \rightarrow M_2(\mathbb{R}) \qquad  \bar\rho(z)=\bar\rho(a+ib)=  \left[   \begin{array}{ccccc} a&-b  \\  b &a \end{array} \right] $$ We can find one or the other in many sources, and somewhere both, as in History of the matrix representation of complex numbers Clearly this two representations generate the same subring of $M_2(\mathbb{R})$ but ve have: $\bar \rho(z)=\rho(\bar z)$. The existence of this double representation  has some (hidden for me) significance or is purely fortuitous ?",,"['matrices', 'soft-question', 'complex-numbers', 'field-theory']"
90,A matrix version of L'Hopital's Rule?,A matrix version of L'Hopital's Rule?,,"Is there a version of L'Hopital's Rule for matrix calculus? For example: let $A$ be a symmetric $n\times n$ positive definite matrix and $b$ be an $n\times 1$ vector. As $b$ converges to $0_{n\times 1}$, $$b'Ab \to 0, $$ and $$Abb'A \to 0_{n\times n}.$$ Find $$\lim_{b\to 0} \frac{Abb'A}{b'Ab}$$or prove that it does not exist. Thanks. Note: $b'$ denotes the transpose of $b$: $b' \equiv b^{T} $.","Is there a version of L'Hopital's Rule for matrix calculus? For example: let $A$ be a symmetric $n\times n$ positive definite matrix and $b$ be an $n\times 1$ vector. As $b$ converges to $0_{n\times 1}$, $$b'Ab \to 0, $$ and $$Abb'A \to 0_{n\times n}.$$ Find $$\lim_{b\to 0} \frac{Abb'A}{b'Ab}$$or prove that it does not exist. Thanks. Note: $b'$ denotes the transpose of $b$: $b' \equiv b^{T} $.",,"['calculus', 'matrices']"
91,Projection matrices identity: $P_{M_ZX}=P_{M_Zx}+P_{M_{[x\; Z]}X_2}$,Projection matrices identity:,P_{M_ZX}=P_{M_Zx}+P_{M_{[x\; Z]}X_2},"For any real matrix $Y$ with $n$ rows and full column rank, we define the orthogonal projection matrices $$ \underbrace{P_Y}_{n\times n}=Y(Y'Y)^{-1}Y',\quad \underbrace{M_Y}_{n\times n}=I_n-P_Y. $$ Among many other properties, $P_Y$ fixes the column space of $Y$ whereas $M_Y$ sends each vector in that space to $0$. A paper (reference available if you are interested) I'm reading cites the following identity   $$ P_{M_ZX}=P_{M_Zx}+P_{M_{[x\; Z]}X_2}.\tag{$\star$} $$   (The subscript of the first term on the RHS is $M_Z\cdot x$.) The dimensions of various things are   $$ Z:n\times l;\quad x:n\times1;\quad X_2:n\times k;\quad X=[x\; X_2]:n\times(1+k). $$ I've spent some time trying to prove ($\star$) suspecting it is probably clever algebraic manipulation but I can't figure it out. A better approach probably involves looking at the RHS of ($\star$) and recognize some relationship between $M_Zx$ and $M_{[x\;Z]}X_2$. I don't have enough knowledge at the moment for this latter approach but I'm interested in learning more about projection matrices. This book will probably aid me but I don't have a copy yet. Can someone help please? Thank you very much.","For any real matrix $Y$ with $n$ rows and full column rank, we define the orthogonal projection matrices $$ \underbrace{P_Y}_{n\times n}=Y(Y'Y)^{-1}Y',\quad \underbrace{M_Y}_{n\times n}=I_n-P_Y. $$ Among many other properties, $P_Y$ fixes the column space of $Y$ whereas $M_Y$ sends each vector in that space to $0$. A paper (reference available if you are interested) I'm reading cites the following identity   $$ P_{M_ZX}=P_{M_Zx}+P_{M_{[x\; Z]}X_2}.\tag{$\star$} $$   (The subscript of the first term on the RHS is $M_Z\cdot x$.) The dimensions of various things are   $$ Z:n\times l;\quad x:n\times1;\quad X_2:n\times k;\quad X=[x\; X_2]:n\times(1+k). $$ I've spent some time trying to prove ($\star$) suspecting it is probably clever algebraic manipulation but I can't figure it out. A better approach probably involves looking at the RHS of ($\star$) and recognize some relationship between $M_Zx$ and $M_{[x\;Z]}X_2$. I don't have enough knowledge at the moment for this latter approach but I'm interested in learning more about projection matrices. This book will probably aid me but I don't have a copy yet. Can someone help please? Thank you very much.",,"['linear-algebra', 'matrices']"
92,"How to put a matrix in Jordan canonical form, when it has a multiple eigenvalue?","How to put a matrix in Jordan canonical form, when it has a multiple eigenvalue?",,"Put the matrix $$\begin{bmatrix} 3 & -4\\ 1 & -1\end{bmatrix}$$ in Jordan Canonical Form.  Moreover, find the appropriate transition matrix to the basis in which the original matrix assumes its Jordan form. I'm having a lot of trouble with this.  I know that the eigenvalue has multiplicity two and is $\lambda = 1$.  I can find the first eigenvector, which is: \begin{bmatrix}         2 \\         1  \\         \end{bmatrix} I'm having trouble finding the second since both eigenvalues tell us the same thing.  But I'm not nearly as concerned about the eigenvectors as I am about what to do after. If anyone could explain thoroughly the next steps involved (not necessarily the answer but how to obtain it), I would be forever grateful.  This homework is in 2 days and it may determine my grade letter.","Put the matrix $$\begin{bmatrix} 3 & -4\\ 1 & -1\end{bmatrix}$$ in Jordan Canonical Form.  Moreover, find the appropriate transition matrix to the basis in which the original matrix assumes its Jordan form. I'm having a lot of trouble with this.  I know that the eigenvalue has multiplicity two and is $\lambda = 1$.  I can find the first eigenvector, which is: \begin{bmatrix}         2 \\         1  \\         \end{bmatrix} I'm having trouble finding the second since both eigenvalues tell us the same thing.  But I'm not nearly as concerned about the eigenvectors as I am about what to do after. If anyone could explain thoroughly the next steps involved (not necessarily the answer but how to obtain it), I would be forever grateful.  This homework is in 2 days and it may determine my grade letter.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
93,"how to prove the relationship about spectral radius, numerical radius and matrix two norm?","how to prove the relationship about spectral radius, numerical radius and matrix two norm?",,"When I read page 24 in Iterative Methods for Sparse Linear Systems, Second Edition , I can not understand the following statement: (My major is not math) Let $A \in \mathbb{C}^{n \times n}$ have eigenvalues $\lambda_{1},\dots,\lambda_{n}$ , $\rho(A)$ its spectral radius and $\gamma(A)$ its numerical radius.   I want to prove the relationships $$ \rho(A) \le \gamma(A) \le \| A \|_2 \qquad \text{and} \qquad  \frac{\| A \|_2}{2} \le \gamma(A) \le \| A \|_2. $$","When I read page 24 in Iterative Methods for Sparse Linear Systems, Second Edition , I can not understand the following statement: (My major is not math) Let have eigenvalues , its spectral radius and its numerical radius.   I want to prove the relationships","A \in \mathbb{C}^{n \times n} \lambda_{1},\dots,\lambda_{n} \rho(A) \gamma(A) 
\rho(A) \le \gamma(A) \le \| A \|_2
\qquad \text{and} \qquad
 \frac{\| A \|_2}{2} \le \gamma(A) \le \| A \|_2.
","['matrices', 'numerical-linear-algebra', 'spectral-radius']"
94,Differentiating matrices with respect to a vector,Differentiating matrices with respect to a vector,,"Given a matrix $X$ (which doesn't need to be square) and a vector $b$, how can I get the following equality? $$\frac{b^t X^t X b}{\partial b} = (X^t X ) b $$ Why is this wrong? $$\frac{b^t X^t X b}{\partial b} = \frac{(X b )^t X b}{\partial b} = \frac{( X b )^t}{\partial b} X b + (X b)^t\frac{X b}{\partial b} = X^t X b + (X b)^t X $$ Also, how can I directly calculate this without multiplying the terms in the numerator?  $$ \frac{(y-X b)^t (y-X b) }{\partial b}$$","Given a matrix $X$ (which doesn't need to be square) and a vector $b$, how can I get the following equality? $$\frac{b^t X^t X b}{\partial b} = (X^t X ) b $$ Why is this wrong? $$\frac{b^t X^t X b}{\partial b} = \frac{(X b )^t X b}{\partial b} = \frac{( X b )^t}{\partial b} X b + (X b)^t\frac{X b}{\partial b} = X^t X b + (X b)^t X $$ Also, how can I directly calculate this without multiplying the terms in the numerator?  $$ \frac{(y-X b)^t (y-X b) }{\partial b}$$",,"['matrices', 'derivatives', 'matrix-calculus']"
95,Cramers Rule. The why and how.,Cramers Rule. The why and how.,,"Can someone explain how Cramer's rule works. I understand the mechanics of it, and it's fairly straightforward to show algebraically that it's equivalent to GJ and substitution, but what's happening under the hood? I'm guessing it has to do with properties of determinants but...","Can someone explain how Cramer's rule works. I understand the mechanics of it, and it's fairly straightforward to show algebraically that it's equivalent to GJ and substitution, but what's happening under the hood? I'm guessing it has to do with properties of determinants but...",,"['linear-algebra', 'abstract-algebra', 'matrices', 'determinant']"
96,How to prove that a nilpotent operator has a basis representation that is strictly upper triangular?,How to prove that a nilpotent operator has a basis representation that is strictly upper triangular?,,"I wish to prove the following:  if $T:X\to X$ is a nilpotent (and the     linear space $X$ is finite-dimensional), then there exists a basis     of $X$ such that the matrix representation of $T$ is upper     triangular with zero diagonal elements. I was trying to prove $\{x_1,...,x_{k+1},v, Tv, ..., T^{n-k-2}v\}$ is a basis for $X$ where $x_i$ are basis for $N(T)$ and $v,..., T^{n-k-2}v$ are basis for $R(T)$ where $T^{n-k}=0$ and $n-k$ is minimal. However the matrix representation w.r.t this basis is diagonal. Is there an easy way to prove this ? I know that for all $T:X\rightarrow X$, there exists a basis that the matrix representation is  upper triangular. How is this useful?","I wish to prove the following:  if $T:X\to X$ is a nilpotent (and the     linear space $X$ is finite-dimensional), then there exists a basis     of $X$ such that the matrix representation of $T$ is upper     triangular with zero diagonal elements. I was trying to prove $\{x_1,...,x_{k+1},v, Tv, ..., T^{n-k-2}v\}$ is a basis for $X$ where $x_i$ are basis for $N(T)$ and $v,..., T^{n-k-2}v$ are basis for $R(T)$ where $T^{n-k}=0$ and $n-k$ is minimal. However the matrix representation w.r.t this basis is diagonal. Is there an easy way to prove this ? I know that for all $T:X\rightarrow X$, there exists a basis that the matrix representation is  upper triangular. How is this useful?",,"['linear-algebra', 'matrices']"
97,Adjacency matrix and connectivity proof,Adjacency matrix and connectivity proof,,"Let $G$ be graph on $n$ vertices, $A$ its adjacency matrix, and $I_{n}$ the $n\times n$ identity matrix. Prove that $G$ is connected iff the matrix $(I_{n} + A)^{n-1}$ has no 0s. My proof: If the graph is connected the matrix $(I_{n} + A)^{n-1}$ has no 0s. The identity matrix  takes care of the non-zero values for the diagonal (otherwise the diagonals would be 0 since self-loops are disallowed in simple graphs). If the graph is connected there must exist a path between any two vertices $u,v \in G$. $A^{n-1}_{i,j}$ represents the number of paths of length $n-1$ between any two vertices. If any $A^{n-1}_{i,j}$ would be 0, that would mean that there would not be any path connecting those vertices. Thus, the graph would be disconnected. But we assumed that it was connected If the matrix $(I_{n} + A)^{n-1}$ has no 0s then G is connected If every $(I_{n} + A)^{n-1}$ is non-zero this means that there exists at least one path of length $n - 1$ between any two vertices. If this is the case, the graph must be connected. Is this correct?","Let $G$ be graph on $n$ vertices, $A$ its adjacency matrix, and $I_{n}$ the $n\times n$ identity matrix. Prove that $G$ is connected iff the matrix $(I_{n} + A)^{n-1}$ has no 0s. My proof: If the graph is connected the matrix $(I_{n} + A)^{n-1}$ has no 0s. The identity matrix  takes care of the non-zero values for the diagonal (otherwise the diagonals would be 0 since self-loops are disallowed in simple graphs). If the graph is connected there must exist a path between any two vertices $u,v \in G$. $A^{n-1}_{i,j}$ represents the number of paths of length $n-1$ between any two vertices. If any $A^{n-1}_{i,j}$ would be 0, that would mean that there would not be any path connecting those vertices. Thus, the graph would be disconnected. But we assumed that it was connected If the matrix $(I_{n} + A)^{n-1}$ has no 0s then G is connected If every $(I_{n} + A)^{n-1}$ is non-zero this means that there exists at least one path of length $n - 1$ between any two vertices. If this is the case, the graph must be connected. Is this correct?",,"['matrices', 'graph-theory', 'proof-verification']"
98,Find a polynomial $f(Z)$ of degree less than 2 such that $e^{tA}=f(A)$,Find a polynomial  of degree less than 2 such that,f(Z) e^{tA}=f(A),Let $A=\begin{pmatrix}3&-2\\2&-2\end{pmatrix}$. As the question says I need a polynomial $f(Z)$ of degree less than 2 such that $e^{tA}=f(A)$. Should my polynomial just be the first 2 terms of the exponential summation? i.e. $I_2+tA$ or does it mean something else? I've already worked out that $e^{tA}=-\frac{1}{3}\begin{pmatrix}e^{-t}-4e^{2t}&-2e^{-t}+2e^{2t}\\2e^{-t}-2e^{2t}&-4e^{-t}+e^{2t}\end{pmatrix}$ I just don't know what polynomial it wants. Edit: Diagonalising $A$ gives $\begin{pmatrix}-1&0\\0&2\end{pmatrix}$ doesn't it? But what do I do with that to get the polynomial it asks for? Edit 2: So I've worked out $c_0(t)=\frac{1}{3}(e^{2t}+2e^{-t})$ and $c_1(t)=\frac{1}{3}(e^{2t}-e^{-t})$. Are they like the coefficients for the polynomial? Or is there something else?,Let $A=\begin{pmatrix}3&-2\\2&-2\end{pmatrix}$. As the question says I need a polynomial $f(Z)$ of degree less than 2 such that $e^{tA}=f(A)$. Should my polynomial just be the first 2 terms of the exponential summation? i.e. $I_2+tA$ or does it mean something else? I've already worked out that $e^{tA}=-\frac{1}{3}\begin{pmatrix}e^{-t}-4e^{2t}&-2e^{-t}+2e^{2t}\\2e^{-t}-2e^{2t}&-4e^{-t}+e^{2t}\end{pmatrix}$ I just don't know what polynomial it wants. Edit: Diagonalising $A$ gives $\begin{pmatrix}-1&0\\0&2\end{pmatrix}$ doesn't it? But what do I do with that to get the polynomial it asks for? Edit 2: So I've worked out $c_0(t)=\frac{1}{3}(e^{2t}+2e^{-t})$ and $c_1(t)=\frac{1}{3}(e^{2t}-e^{-t})$. Are they like the coefficients for the polynomial? Or is there something else?,,['linear-algebra']
99,How to find nonnegative solutions of a linear system?,How to find nonnegative solutions of a linear system?,,I have the following system of $M$ linear equations in $N$ unknowns. $$ \begin{bmatrix} 3 & 0 & 1 & 0 & -1 & -3  & 2\\ 1 & 2 & 0 & 4 & 0 & 0  & -1\\ 1 & 1 & 0 & 0 & -1 & -1  & -2\\ 0 & 0 & 1 & 0 & -3 & -1  & 1 \\ \end{bmatrix} \begin{bmatrix} x_{1}\\ x_{2}\\ x_{3}\\ x_{4} \\ x_{5} \\ x_{6} \\ x_{7} \\ \end{bmatrix} = \begin{bmatrix} 1\\ 0\\ 0\\ -1\\ \end{bmatrix}$$ Is there any algorithm for finding answers of this equations that ${x_{i} \ge 0}$ ? Comment : I just want that $x_i \ge 0$ . It can change to $$ \begin{bmatrix} 1 & 0 & 0 & 0 & 2/3 & -2/3 & 1/3 & 2/3\\ 0 & 1 & 0 & 0 & -5/3 & -1/3 & -7/3 & -2/3 \\ 0 & 0 & 1 & 0 & -3 & -1 & 1 & -1 \\ 0 & 0 & 0 & 1 & 2/3 & 1/3 & 5/6 & 1/6 \\ \end{bmatrix} $$,I have the following system of linear equations in unknowns. Is there any algorithm for finding answers of this equations that ? Comment : I just want that . It can change to,"M N 
\begin{bmatrix}
3 & 0 & 1 & 0 & -1 & -3  & 2\\
1 & 2 & 0 & 4 & 0 & 0  & -1\\
1 & 1 & 0 & 0 & -1 & -1  & -2\\
0 & 0 & 1 & 0 & -3 & -1  & 1 \\
\end{bmatrix}
\begin{bmatrix}
x_{1}\\
x_{2}\\
x_{3}\\
x_{4} \\
x_{5} \\
x_{6} \\
x_{7} \\
\end{bmatrix} =
\begin{bmatrix}
1\\
0\\
0\\
-1\\
\end{bmatrix} {x_{i} \ge 0} x_i \ge 0 
\begin{bmatrix}
1 & 0 & 0 & 0 & 2/3 & -2/3 & 1/3 & 2/3\\
0 & 1 & 0 & 0 & -5/3 & -1/3 & -7/3 & -2/3 \\
0 & 0 & 1 & 0 & -3 & -1 & 1 & -1 \\
0 & 0 & 0 & 1 & 2/3 & 1/3 & 5/6 & 1/6 \\
\end{bmatrix}
","['linear-algebra', 'matrices', 'matlab', 'integer-programming']"
