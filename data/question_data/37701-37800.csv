,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Amazing isomorphisms [closed],Amazing isomorphisms [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question Just as a recreational topic, what group/ring/other algebraic structure isomorphisms you know that seem unusual, or downright unintuitive? Are there such structures which we don't yet know whether they are isomorphic or not?","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question Just as a recreational topic, what group/ring/other algebraic structure isomorphisms you know that seem unusual, or downright unintuitive? Are there such structures which we don't yet know whether they are isomorphic or not?",,"['abstract-algebra', 'group-theory', 'soft-question', 'group-isomorphism', 'exceptional-isomorphisms']"
1,Why are modular lattices important?,Why are modular lattices important?,,"A lattice $(L,\leq)$ is said to be modular when $$ (\forall x,a,b\in L)\quad x \leq b \implies x \vee (a \wedge b) = (x \vee a) \wedge b, $$ where $\vee$ is the join operation, and $\wedge$ is the meet operation. ( Join and meet .) The ideals of a ring form a modular lattice. So do submodules of a module. These facts are easy to prove, but I have never seen any striking examples of their utility. Actually, in a seminar I took part in, the speaker said the modularity condition wasn't very natural and that there was an ongoing search for better ones (this was in the context of the Gabriel dimension and its generalization to lattices -- unfortunately, I didn't understand much of that). I would like to see some motivation for this notion. That is, I would like to know when it is useful, and if it is natural. At the moment, it doesn't look any more natural to me than any random condition in the language of lattices. If you could shed some light on the opinion I quote in the previous paragraph, it would be very helpful as well. I would be especially interested in algebraic motivation, as I know very little about other areas if mathematics.","A lattice $(L,\leq)$ is said to be modular when $$ (\forall x,a,b\in L)\quad x \leq b \implies x \vee (a \wedge b) = (x \vee a) \wedge b, $$ where $\vee$ is the join operation, and $\wedge$ is the meet operation. ( Join and meet .) The ideals of a ring form a modular lattice. So do submodules of a module. These facts are easy to prove, but I have never seen any striking examples of their utility. Actually, in a seminar I took part in, the speaker said the modularity condition wasn't very natural and that there was an ongoing search for better ones (this was in the context of the Gabriel dimension and its generalization to lattices -- unfortunately, I didn't understand much of that). I would like to see some motivation for this notion. That is, I would like to know when it is useful, and if it is natural. At the moment, it doesn't look any more natural to me than any random condition in the language of lattices. If you could shed some light on the opinion I quote in the previous paragraph, it would be very helpful as well. I would be especially interested in algebraic motivation, as I know very little about other areas if mathematics.",,"['abstract-algebra', 'soft-question']"
2,Surjective endomorphisms of finitely generated modules are isomorphisms,Surjective endomorphisms of finitely generated modules are isomorphisms,,"My Problem: Let $M$ be a finitely generated $A$-module and $T$ an endomorphism. I want to show that if $T$ is surjective then it is invertible. My attempt: Let $m_1,...,m_n$ be the generators of $M$ over $A$. For every $b = b_1 m_1 + ... + b_n m_n$ with $b_i \in A$ there is $a = a_1 m_1 + ... + a_n m_n$ with $a_i \in A$ such that  $$ T(a)=b $$ or in matrix-vector notation $$ T \vec{a} = \vec{b} $$ where $\vec{x}$ is the column vector of $x_1,...,x_n$ where $x = x_1 m_1 + ... + x_n m_n$. I multiply by the adjugate matrix to get $$ \mathrm{adj}(T) \vec{b} = \mathrm{adj}(T) T \vec{a} = \det(T) I_n \vec{a} = \det(T) \vec{a} \ . $$ Now take $\vec{b}=0$. Then $\vec{0} = \det(T) \vec{a}$ and thus $T$ is injective if and only if $\det(T)$ is not a zero divisor. If I prove that $T$ is injective, then I'll get it is invertible. For that, I think the way is to prove that $\det(T)$ is not a zero divisor. The importance of finitely generated condition: Let $M = A^{\aleph_0} =\{ ( a_1 , a_2 , ... ) \mid a_i \in A \}$ be a not finitely generated $A$-module. Let $T : M \to M$ defined by $$ T(a_1, a_2, a_3, ... ) = (a_2, a_3, ... ) \ . $$ Then clearly $T$ is surjective but not injective ($\ker T = \{ ( a , 0 , 0 , ... ) \mid a \in A \}$), and thus not invertible. The importance of surjective and not injective condition: Need to find a counter-example.","My Problem: Let $M$ be a finitely generated $A$-module and $T$ an endomorphism. I want to show that if $T$ is surjective then it is invertible. My attempt: Let $m_1,...,m_n$ be the generators of $M$ over $A$. For every $b = b_1 m_1 + ... + b_n m_n$ with $b_i \in A$ there is $a = a_1 m_1 + ... + a_n m_n$ with $a_i \in A$ such that  $$ T(a)=b $$ or in matrix-vector notation $$ T \vec{a} = \vec{b} $$ where $\vec{x}$ is the column vector of $x_1,...,x_n$ where $x = x_1 m_1 + ... + x_n m_n$. I multiply by the adjugate matrix to get $$ \mathrm{adj}(T) \vec{b} = \mathrm{adj}(T) T \vec{a} = \det(T) I_n \vec{a} = \det(T) \vec{a} \ . $$ Now take $\vec{b}=0$. Then $\vec{0} = \det(T) \vec{a}$ and thus $T$ is injective if and only if $\det(T)$ is not a zero divisor. If I prove that $T$ is injective, then I'll get it is invertible. For that, I think the way is to prove that $\det(T)$ is not a zero divisor. The importance of finitely generated condition: Let $M = A^{\aleph_0} =\{ ( a_1 , a_2 , ... ) \mid a_i \in A \}$ be a not finitely generated $A$-module. Let $T : M \to M$ defined by $$ T(a_1, a_2, a_3, ... ) = (a_2, a_3, ... ) \ . $$ Then clearly $T$ is surjective but not injective ($\ker T = \{ ( a , 0 , 0 , ... ) \mid a \in A \}$), and thus not invertible. The importance of surjective and not injective condition: Need to find a counter-example.",,"['abstract-algebra', 'commutative-algebra', 'modules']"
3,"Why $\gcd(b,qb+r)=\gcd(b,r),\,$ so $\,\gcd(b,a) = \gcd(b,a\bmod b)$",Why  so,"\gcd(b,qb+r)=\gcd(b,r),\, \,\gcd(b,a) = \gcd(b,a\bmod b)","Given: $a = qb + r$ . Then it holds that $\gcd(a,b)=\gcd(b,r)$ . That doesn't sound logical to me. Why is this so? Addendum by LePressentiment on 11/29/2013: (in the interest of http://meta.math.stackexchange.com/a/4110/53259 and averting a duplicate) What's the intuition behind this result? I only recognise the proof and examples solely due to algebraic properties and formal definitions; I'd like to apprehend the result naturally.",Given: . Then it holds that . That doesn't sound logical to me. Why is this so? Addendum by LePressentiment on 11/29/2013: (in the interest of http://meta.math.stackexchange.com/a/4110/53259 and averting a duplicate) What's the intuition behind this result? I only recognise the proof and examples solely due to algebraic properties and formal definitions; I'd like to apprehend the result naturally.,"a = qb + r \gcd(a,b)=\gcd(b,r)","['abstract-algebra', 'elementary-number-theory', 'divisibility', 'ideals', 'gcd-and-lcm']"
4,Show that every group of prime order is cyclic,Show that every group of prime order is cyclic,,"Show that every group of prime order is cyclic. I was given this problem for homework and I am not sure where to start. I know a solution using Lagrange's theorem, but we have not proven Lagrange's theorem yet, actually our teacher hasn't even mentioned it, so I am guessing there must be another solution. The only thing I could think of was showing that a group of prime order $p$ is isomorphic to $\mathbb{Z}/p\mathbb{Z}$. Would this work? Any guidance would be appreciated.","Show that every group of prime order is cyclic. I was given this problem for homework and I am not sure where to start. I know a solution using Lagrange's theorem, but we have not proven Lagrange's theorem yet, actually our teacher hasn't even mentioned it, so I am guessing there must be another solution. The only thing I could think of was showing that a group of prime order $p$ is isomorphic to $\mathbb{Z}/p\mathbb{Z}$. Would this work? Any guidance would be appreciated.",,['abstract-algebra']
5,Inner Product Spaces over Finite Fields,Inner Product Spaces over Finite Fields,,"Inner product spaces are defined over a field $\mathbb{F}$ which is either $\mathbb{R}$ or $\mathbb{C}$. I want to know what happens if we try to define them over some finite field. Here's an example: Let $\mathbb{F} = \{0,1,a,b\}$ be a finite field with + and * defined by the following Cayley tables: Now, define a very simple vector space $\mathcal{V} = \{O, V\}$ over $\mathbb{F}$ as follows: $\mathcal{V}$ is an Abelian group over addition, with identity $O$. Therefore, $O+O = V+V = O$, and $O+V = V+O =V$. The scalar multiplication is governed with these rules: For any $e \in \mathbb{F}$, we have $eO = O$. Define $0V = O$, and $1V=aV=bV=V$. One can easily check that $\mathcal{V}$ is a vector space over $\mathbb{F}$. Now, we define an inner product for $\mathcal{V}$: $\langle O,O \rangle = 0$ and $\langle V,V \rangle = 1$; $\langle V,O \rangle = \langle O,V \rangle = 0$. It seems that the above example demonstrates an inner product space over a finite field. Is the above notion ever studied? Does it have any applications? We avoided ""conjugate symmetry"" in the definition above, by assuming the conjugate of each member of $\mathbb{F}$ is itself. Can we define conjugation for fields other than $\mathbb{C}$? (Well, I heard the name C*-algebra , but I don't know whether it relates to my question.) For instance, let $\mathbb{Q}[\sqrt 3] = \{a+b\sqrt 3 \mid a,b \in \mathbb{Q} \}$ be $\mathbb{Q}$ adjoined with $\sqrt 3$. For any $e = a+b\sqrt 3$ in $\mathbb{Q}[\sqrt 3]$, can we define the conjugate of $e$ as $e^* = a-b\sqrt 3$? This satisfies the condition below: Both addition and multiplication of $e$ and $e^*$ are members of the underlying subfield.","Inner product spaces are defined over a field $\mathbb{F}$ which is either $\mathbb{R}$ or $\mathbb{C}$. I want to know what happens if we try to define them over some finite field. Here's an example: Let $\mathbb{F} = \{0,1,a,b\}$ be a finite field with + and * defined by the following Cayley tables: Now, define a very simple vector space $\mathcal{V} = \{O, V\}$ over $\mathbb{F}$ as follows: $\mathcal{V}$ is an Abelian group over addition, with identity $O$. Therefore, $O+O = V+V = O$, and $O+V = V+O =V$. The scalar multiplication is governed with these rules: For any $e \in \mathbb{F}$, we have $eO = O$. Define $0V = O$, and $1V=aV=bV=V$. One can easily check that $\mathcal{V}$ is a vector space over $\mathbb{F}$. Now, we define an inner product for $\mathcal{V}$: $\langle O,O \rangle = 0$ and $\langle V,V \rangle = 1$; $\langle V,O \rangle = \langle O,V \rangle = 0$. It seems that the above example demonstrates an inner product space over a finite field. Is the above notion ever studied? Does it have any applications? We avoided ""conjugate symmetry"" in the definition above, by assuming the conjugate of each member of $\mathbb{F}$ is itself. Can we define conjugation for fields other than $\mathbb{C}$? (Well, I heard the name C*-algebra , but I don't know whether it relates to my question.) For instance, let $\mathbb{Q}[\sqrt 3] = \{a+b\sqrt 3 \mid a,b \in \mathbb{Q} \}$ be $\mathbb{Q}$ adjoined with $\sqrt 3$. For any $e = a+b\sqrt 3$ in $\mathbb{Q}[\sqrt 3]$, can we define the conjugate of $e$ as $e^* = a-b\sqrt 3$? This satisfies the condition below: Both addition and multiplication of $e$ and $e^*$ are members of the underlying subfield.",,"['abstract-algebra', 'vector-spaces', 'finite-fields', 'inner-products']"
6,Zero divisor in $R[x]$,Zero divisor in,R[x],"Let $R$ be commutative ring with no (nonzero) nilpotents. If $f(x) = a_0+a_1x+\cdots+a_nx^n$ is a zero divisor in $R[x]$, how do I show there's an element $b \ne 0$ in $R$ such that $ba_0=ba_1=\cdots=ba_n=0$?","Let $R$ be commutative ring with no (nonzero) nilpotents. If $f(x) = a_0+a_1x+\cdots+a_nx^n$ is a zero divisor in $R[x]$, how do I show there's an element $b \ne 0$ in $R$ such that $ba_0=ba_1=\cdots=ba_n=0$?",,"['abstract-algebra', 'polynomials', 'ring-theory', 'commutative-algebra']"
7,Why is a finite integral domain always field?,Why is a finite integral domain always field?,,"This is how I'm approaching it: let $R$ be a finite integral domain and I'm trying to show every element in $R$ has an inverse: let $R-\{0\}=\{x_1,x_2,\ldots,x_k\}$, then as $R$ is closed under multiplication $\prod_{n=1}^k\ x_i=x_j$, therefore by canceling $x_j$ we get $x_1x_2\cdots x_{j-1}x_{j+1}\cdots x_k=1 $, by commuting any of these elements to the front we find an inverse for first term, e.g. for $x_m$ we have $x_m(x_1\cdots x_{m-1}x_{m+1}\cdots x_{j-1}x_{j+1}\cdots x_k)=1$, where $(x_m)^{-1}=x_1\cdots x_{m-1}x_{m+1}\cdots x_{j-1}x_{j+1}\cdots x_k$. As far as I can see this is correct, so we have found inverses for all $x_i\in R$ apart from $x_j$ if I am right so far. How would we find $(x_{j})^{-1}$?","This is how I'm approaching it: let $R$ be a finite integral domain and I'm trying to show every element in $R$ has an inverse: let $R-\{0\}=\{x_1,x_2,\ldots,x_k\}$, then as $R$ is closed under multiplication $\prod_{n=1}^k\ x_i=x_j$, therefore by canceling $x_j$ we get $x_1x_2\cdots x_{j-1}x_{j+1}\cdots x_k=1 $, by commuting any of these elements to the front we find an inverse for first term, e.g. for $x_m$ we have $x_m(x_1\cdots x_{m-1}x_{m+1}\cdots x_{j-1}x_{j+1}\cdots x_k)=1$, where $(x_m)^{-1}=x_1\cdots x_{m-1}x_{m+1}\cdots x_{j-1}x_{j+1}\cdots x_k$. As far as I can see this is correct, so we have found inverses for all $x_i\in R$ apart from $x_j$ if I am right so far. How would we find $(x_{j})^{-1}$?",,"['abstract-algebra', 'ring-theory', 'field-theory', 'integral-domain', 'finite-rings']"
8,Structure Theorem for abelian torsion groups that are not finitely generated?,Structure Theorem for abelian torsion groups that are not finitely generated?,,"I know about the structure theorem for finitely generated abelian groups . I'm wondering whether there exists a similar structure theorem for abelian groups that are not finitely generated. In particular, I'm interested in torsion groups . Maybe having a finite exponent helps?","I know about the structure theorem for finitely generated abelian groups . I'm wondering whether there exists a similar structure theorem for abelian groups that are not finitely generated. In particular, I'm interested in torsion groups . Maybe having a finite exponent helps?",,"['abstract-algebra', 'group-theory', 'modules', 'abelian-groups']"
9,"Why is the exterior algebra called the ""exterior algebra?"" What makes it ""exterior?""","Why is the exterior algebra called the ""exterior algebra?"" What makes it ""exterior?""",,"Why is the exterior algebra called the ""exterior algebra?"" What makes it ""exterior?"" Is it just because a module can be universally embedded into its exterior algebra, so one could view the exterior algebra as surrounding the module? Why is it not just called the ""alternating algebra?""","Why is the exterior algebra called the ""exterior algebra?"" What makes it ""exterior?"" Is it just because a module can be universally embedded into its exterior algebra, so one could view the exterior algebra as surrounding the module? Why is it not just called the ""alternating algebra?""",,"['abstract-algebra', 'differential-geometry', 'terminology', 'math-history']"
10,A subgroup of a cyclic group is cyclic - Understanding Proof,A subgroup of a cyclic group is cyclic - Understanding Proof,,"I'm having some trouble understanding the proof of the following theorem A subgroup of a cyclic group is cyclic I will list each step of the proof in my textbook and indicate the places that I'm confused and hopefully somewhere out there can clear some things up for me. Proof Let $G$ be a cyclic group generated by "" $a$ "" and let $H$ be a subgroup of $G$ . If $H = {\{e\}}$ , then $H = \langle e\rangle $ is cyclic. If $H \neq \space {\{e\}}$ , then $a^n \in H$ for some $n \in \mathbb{Z}^{+}$ . Let $m$ be the smallest integer in $\mathbb{Z}^{+}$ such that $a^m \in H$ . We claim that $c = a^m$ generates $H$ ; that is, $$H = \langle a^m\rangle =  \langle c\rangle.$$ We must show that every $b \in H$ is a power of $c$ . Since $b \in H$ and $H \leq G$ , we have $b = a^n$ for some $n$ . Find $q$ and $r$ such that $$ n = mq + r \space \space \space \space for \space \space \space 0 \leq r < m,$$ Alright this is the first part in the proof where I start to get confused. Where does the division algorithm come from and why are we using it? The proof continues as follows: $$a^n = a^{mq + r} = (a^m)^q \cdot a^r,$$ so $$ a^r = (a^m)^{-q} \cdot a^n.$$ Now since $a^n \in H$ , $a^m \in H$ , and $H$ is a group, both $(a^m)^{-q}$ and $a^n$ are in $H$ . Thus $$(a^m)^{-q} \cdot a^n \in H; \space \space \space \text{that is,} \space \space a^r \in H.$$ This is another point at which I’m a little confused. What exactly about $a^n$ and $a^m$ being elements of $H$ allows us to accept that $(a^m)^{-q}$ and $a^{n}$ are in $H$ ? The proof continues: Since $m$ was the smallest positive integer such that $a^m \in H$ and $0$ $\leq r$ $< m$ , we must have $r = 0$ . Thus $n = qm$ and $$b \space = \space a^n \space = \space (a^m)^q \space = \space c^q,$$ so $b$ is a power of $c.$ This final step is confusing as well, but I think its just because of the previous parts I was confused about. Any help in understanding this proof would be greatly appreciated","I'm having some trouble understanding the proof of the following theorem A subgroup of a cyclic group is cyclic I will list each step of the proof in my textbook and indicate the places that I'm confused and hopefully somewhere out there can clear some things up for me. Proof Let be a cyclic group generated by "" "" and let be a subgroup of . If , then is cyclic. If , then for some . Let be the smallest integer in such that . We claim that generates ; that is, We must show that every is a power of . Since and , we have for some . Find and such that Alright this is the first part in the proof where I start to get confused. Where does the division algorithm come from and why are we using it? The proof continues as follows: so Now since , , and is a group, both and are in . Thus This is another point at which I’m a little confused. What exactly about and being elements of allows us to accept that and are in ? The proof continues: Since was the smallest positive integer such that and , we must have . Thus and so is a power of This final step is confusing as well, but I think its just because of the previous parts I was confused about. Any help in understanding this proof would be greatly appreciated","G a H G H = {\{e\}} H = \langle e\rangle  H \neq \space {\{e\}} a^n \in H n \in \mathbb{Z}^{+} m \mathbb{Z}^{+} a^m \in H c = a^m H H = \langle a^m\rangle =  \langle c\rangle. b \in H c b \in H H \leq G b = a^n n q r  n = mq + r \space \space \space \space for \space \space \space 0 \leq r < m, a^n = a^{mq + r} = (a^m)^q \cdot a^r,  a^r = (a^m)^{-q} \cdot a^n. a^n \in H a^m \in H H (a^m)^{-q} a^n H (a^m)^{-q} \cdot a^n \in H; \space \space \space \text{that is,} \space \space a^r \in H. a^n a^m H (a^m)^{-q} a^{n} H m a^m \in H 0 \leq r < m r = 0 n = qm b \space = \space a^n \space = \space (a^m)^q \space = \space c^q, b c.","['abstract-algebra', 'group-theory', 'proof-explanation', 'cyclic-groups']"
11,Order of automorphism group of cyclic group [closed],Order of automorphism group of cyclic group [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $G$ be a cyclic group of order $m$. What is the order of $\text{Aut}(G)$? I want to know the proof as well (elementary if possible). I would still accept the proof if one answers with $m = p$, a prime. Or on top of that, I would accept the answer with extra assumption: $q \equiv 1$ mod $p$ with another prime $p$.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $G$ be a cyclic group of order $m$. What is the order of $\text{Aut}(G)$? I want to know the proof as well (elementary if possible). I would still accept the proof if one answers with $m = p$, a prime. Or on top of that, I would accept the answer with extra assumption: $q \equiv 1$ mod $p$ with another prime $p$.",,"['abstract-algebra', 'group-theory', 'cyclic-groups', 'automorphism-group']"
12,Understanding isomorphic equivalences of tensor product,Understanding isomorphic equivalences of tensor product,,"I get some big picture of tensor and tensor product by reading their Wikipedia articles, and several questions and answers posted before by others. But I cannot figure out how to show the following isomorphic equivalences: from Zach Conn : For finite-dimensional spaces V,W, the tensor product $V^* \otimes W$ is   isomorphic to the space of homomorphisms $\textrm{Hom}(V, W)$. So in other   words   every linear map $V \to W$ has a tensor expansion, i.e., a   representation as    a tensor in $V^* \otimes W$. I wonder why ""the tensor product $V^* \otimes W$ is isomorphic to the space of homomorphisms $\textrm{Hom}(V,W)$""? from Hans Lundmark : a bilinear map $B:V \times U \to \mathbb{R}$ can be canonically identified with an   element   of the space $V^* \otimes U^*$. How is a bilinear map $B:V \times U \to \mathbb{R}$ canonically identified with an element of the space $V^* \otimes U^*$? from Qiaochu Yuan : A linear transformation $V \to V$, An element of $V^* \otimes V$, A linear map $V \otimes V^* \to \mathbb{R} $. The identification (of the first) with the second picture comes   from the fact that   dual distributes over tensor product (which again comes down to   tensor   contraction) and the fact that $V^{**} \cong V$. Alternately, again by   tensor   contraction, there is a natural bilinear map $V \times (V^* \otimes V) \to V$ which   identifies an element of $V^* \otimes V$ with a linear transformation $V \to V$. How does that dual distributes over tensor product come from tensor contraction, and how does this lead to the identification? How is the natural bilinear map $V \times (V^* \otimes V) \to V$ defined? Thanks and regards!","I get some big picture of tensor and tensor product by reading their Wikipedia articles, and several questions and answers posted before by others. But I cannot figure out how to show the following isomorphic equivalences: from Zach Conn : For finite-dimensional spaces V,W, the tensor product $V^* \otimes W$ is   isomorphic to the space of homomorphisms $\textrm{Hom}(V, W)$. So in other   words   every linear map $V \to W$ has a tensor expansion, i.e., a   representation as    a tensor in $V^* \otimes W$. I wonder why ""the tensor product $V^* \otimes W$ is isomorphic to the space of homomorphisms $\textrm{Hom}(V,W)$""? from Hans Lundmark : a bilinear map $B:V \times U \to \mathbb{R}$ can be canonically identified with an   element   of the space $V^* \otimes U^*$. How is a bilinear map $B:V \times U \to \mathbb{R}$ canonically identified with an element of the space $V^* \otimes U^*$? from Qiaochu Yuan : A linear transformation $V \to V$, An element of $V^* \otimes V$, A linear map $V \otimes V^* \to \mathbb{R} $. The identification (of the first) with the second picture comes   from the fact that   dual distributes over tensor product (which again comes down to   tensor   contraction) and the fact that $V^{**} \cong V$. Alternately, again by   tensor   contraction, there is a natural bilinear map $V \times (V^* \otimes V) \to V$ which   identifies an element of $V^* \otimes V$ with a linear transformation $V \to V$. How does that dual distributes over tensor product come from tensor contraction, and how does this lead to the identification? How is the natural bilinear map $V \times (V^* \otimes V) \to V$ defined? Thanks and regards!",,"['abstract-algebra', 'vector-spaces', 'tensor-products']"
13,Why are algebraic structures preserved under intersection but not union?,Why are algebraic structures preserved under intersection but not union?,,"In general, the intersection of subgroups/subrings/subfields/sub(vector)spaces will still be subgroups/subrings/subfields/sub(vector)spaces. However, the union will (generally) not be. Is there a ""deep"" reason for this?","In general, the intersection of subgroups/subrings/subfields/sub(vector)spaces will still be subgroups/subrings/subfields/sub(vector)spaces. However, the union will (generally) not be. Is there a ""deep"" reason for this?",,"['abstract-algebra', 'category-theory', 'big-picture']"
14,Why is $\mathbb{Z}[X]$ not a euclidean domain? What goes wrong with the degree function?,Why is  not a euclidean domain? What goes wrong with the degree function?,\mathbb{Z}[X],"I know that $K[X]$ is a Euclidean domain but $\mathbb{Z}[X]$ is not. I understand this, if I consider the ideal $\langle X,2 \rangle$ which is a principal ideal of $K[X]$ but not of $\mathbb{Z}[X]$. So $\mathbb{Z}[X]$ isn't a principal ideal domain and therefore not an Euclidean domain. But I don't understand this, if I consider the definition of Euclidean domains. Basically, a Euclidean domain is a ring where I can do division with remainders. For polynomial rings, the Euclidean function should be the degree of the polynomials. What's the crucial difference between $K[X]$ and $\mathbb{Z}[X]$ with respect to this? I already did exercises involving polynomial division in $\mathbb{Z}[X]$, so clearly I must be missing something here.","I know that $K[X]$ is a Euclidean domain but $\mathbb{Z}[X]$ is not. I understand this, if I consider the ideal $\langle X,2 \rangle$ which is a principal ideal of $K[X]$ but not of $\mathbb{Z}[X]$. So $\mathbb{Z}[X]$ isn't a principal ideal domain and therefore not an Euclidean domain. But I don't understand this, if I consider the definition of Euclidean domains. Basically, a Euclidean domain is a ring where I can do division with remainders. For polynomial rings, the Euclidean function should be the degree of the polynomials. What's the crucial difference between $K[X]$ and $\mathbb{Z}[X]$ with respect to this? I already did exercises involving polynomial division in $\mathbb{Z}[X]$, so clearly I must be missing something here.",,"['abstract-algebra', 'ring-theory']"
15,Subgroups of a cyclic group and their order.,Subgroups of a cyclic group and their order.,,"Lemma $1.92$ in Rotman's textbook (Advanced Modern Algebra, second edition) states, Let $G = \langle a \rangle$ be a cyclic group. (i) Every subgroup $S$ of $G$ is cyclic. (ii) If $|G|=n$ , then $G$ has a unique subgroup of order $d$ for each divisor $d$ of $n$ . I understand how every subgroup must be cyclic and that there must be a subgroup for each divisor of $d$ . But how is that subgroup unique? I'm having trouble understanding this intuitively. For example, if we look at the cyclic subgroup $\Bbb{7}$ , we know that there are $6$ elements of order $7$ . So we have six different cyclic subgroups of order $7$ , right? Thanks in advance.","Lemma in Rotman's textbook (Advanced Modern Algebra, second edition) states, Let be a cyclic group. (i) Every subgroup of is cyclic. (ii) If , then has a unique subgroup of order for each divisor of . I understand how every subgroup must be cyclic and that there must be a subgroup for each divisor of . But how is that subgroup unique? I'm having trouble understanding this intuitively. For example, if we look at the cyclic subgroup , we know that there are elements of order . So we have six different cyclic subgroups of order , right? Thanks in advance.",1.92 G = \langle a \rangle S G |G|=n G d d n d \Bbb{7} 6 7 7,['abstract-algebra']
16,What's the difference between isomorphism and homeomorphism?,What's the difference between isomorphism and homeomorphism?,,"I think that they are similar (or same), but I am not sure. Can anyone explain the difference between isomorphism and homeomorphism ?","I think that they are similar (or same), but I am not sure. Can anyone explain the difference between isomorphism and homeomorphism ?",,"['abstract-algebra', 'differential-topology']"
17,"If $\lvert\operatorname{Hom}(H,G_1)\rvert = \lvert\operatorname{Hom}(H,G_2)\rvert$ for any $H$ then $G_1 \cong G_2$",If  for any  then,"\lvert\operatorname{Hom}(H,G_1)\rvert = \lvert\operatorname{Hom}(H,G_2)\rvert H G_1 \cong G_2","Let $G_1$ and $G_2$ be two finite groups such that for any finite group $H$, $\lvert\operatorname{Hom}(H,G_1)\rvert = \lvert\operatorname{Hom}(H,G_2)\rvert$. How can I show that $G_1 \cong G_2$ ?","Let $G_1$ and $G_2$ be two finite groups such that for any finite group $H$, $\lvert\operatorname{Hom}(H,G_1)\rvert = \lvert\operatorname{Hom}(H,G_2)\rvert$. How can I show that $G_1 \cong G_2$ ?",,"['abstract-algebra', 'group-theory', 'finite-groups', 'group-isomorphism']"
18,Finitely generated module with a submodule that is not finitely generated [duplicate],Finitely generated module with a submodule that is not finitely generated [duplicate],,"This question already has answers here : Example of a finitely generated module with submodules that are not finitely generated (4 answers) Closed 8 years ago . Can someone give an example of a ring $R$, a left $R$-module $M$ and a submodule $N$ of $M$ such that $M$ is finitely generated, but $N$ is not finitely generated? I tried a couple of examples of modules I know and got nothing...","This question already has answers here : Example of a finitely generated module with submodules that are not finitely generated (4 answers) Closed 8 years ago . Can someone give an example of a ring $R$, a left $R$-module $M$ and a submodule $N$ of $M$ such that $M$ is finitely generated, but $N$ is not finitely generated? I tried a couple of examples of modules I know and got nothing...",,"['abstract-algebra', 'modules', 'finitely-generated']"
19,A $p$-group of order $p^n$ has a normal subgroup of order $p^k$ for each $0\le k \le n$,A -group of order  has a normal subgroup of order  for each,p p^n p^k 0\le k \le n,"This is problem 3 from Hungerford's section about the Sylow theorems. I have already read hints saying to use induction and that $p$-groups always have non-trivial centres, but I'm still confused. This is what I have so far: Suppose $|G| = p^n$. For $k = 0$, $\{e\}$ is a normal subgroup of order $p^0$. Suppose $N_1, ..., N_k$ are normal subgroups of $G$ with orders as described (ie. $|N_i| = p^i$) and $k < n$. $G/N_k$ is a $p$-group, so it has a non-trivial centre $C(G/N_k)$. $\pi^{-1}(C(G/N_k))$ is a subgroup of $G$, where $\pi : G \to G/N_k$ is the quotient map. It is normal in $G$ because $\pi$ is a homomorphism and $C(G/N_k)$ is a normal subgroup of $G/N_k$. I know it contains $N_k$ and some stuff not in $N_k$, so it has to have order at least $p^{k+1}$, but I do not know how to argue it must be equal, or indeed if it even is equal.","This is problem 3 from Hungerford's section about the Sylow theorems. I have already read hints saying to use induction and that $p$-groups always have non-trivial centres, but I'm still confused. This is what I have so far: Suppose $|G| = p^n$. For $k = 0$, $\{e\}$ is a normal subgroup of order $p^0$. Suppose $N_1, ..., N_k$ are normal subgroups of $G$ with orders as described (ie. $|N_i| = p^i$) and $k < n$. $G/N_k$ is a $p$-group, so it has a non-trivial centre $C(G/N_k)$. $\pi^{-1}(C(G/N_k))$ is a subgroup of $G$, where $\pi : G \to G/N_k$ is the quotient map. It is normal in $G$ because $\pi$ is a homomorphism and $C(G/N_k)$ is a normal subgroup of $G/N_k$. I know it contains $N_k$ and some stuff not in $N_k$, so it has to have order at least $p^{k+1}$, but I do not know how to argue it must be equal, or indeed if it even is equal.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'p-groups']"
20,What is Spec of the Adeles?,What is Spec of the Adeles?,,"Let $K$ be a global field and $A_K$ the ring of adeles. What are the prime ideals of $A_K$? I have been told that a full proof of this is quite subtle, but have been unable to find a reference for this result.","Let $K$ be a global field and $A_K$ the ring of adeles. What are the prime ideals of $A_K$? I have been told that a full proof of this is quite subtle, but have been unable to find a reference for this result.",,"['abstract-algebra', 'number-theory', 'algebraic-geometry', 'arithmetic-geometry']"
21,What is $\tau(A_n)$?,What is ?,\tau(A_n),"Suppose G is a finite group. Define $\tau(G)$ as the minimal number, such that $\forall X \subset G$ if $|X| > \tau(G)$, then $XXX = \langle X \rangle$. What is $\tau(A_n)$? Similar problems for some  different classes of groups are already answered: 1)  $\tau(\mathbb{Z}_n) = \lceil \frac{n}{3} \rceil + 1$ (this is a number-theoretic fact proved via arithmetic progressions) 2) Gowers, Nikolov and Pyber proved the fact that $\tau(SL_n(\mathbb{Z}_p)) = 2|SL_n(\mathbb{Z}_p)|^{1-\frac{1}{3(n+1)}}$ (this fact is proved with linear algebra) However, I have never seen anything like that for $A_n$. It will be interesting to know if there is something...","Suppose G is a finite group. Define $\tau(G)$ as the minimal number, such that $\forall X \subset G$ if $|X| > \tau(G)$, then $XXX = \langle X \rangle$. What is $\tau(A_n)$? Similar problems for some  different classes of groups are already answered: 1)  $\tau(\mathbb{Z}_n) = \lceil \frac{n}{3} \rceil + 1$ (this is a number-theoretic fact proved via arithmetic progressions) 2) Gowers, Nikolov and Pyber proved the fact that $\tau(SL_n(\mathbb{Z}_p)) = 2|SL_n(\mathbb{Z}_p)|^{1-\frac{1}{3(n+1)}}$ (this fact is proved with linear algebra) However, I have never seen anything like that for $A_n$. It will be interesting to know if there is something...",,"['abstract-algebra', 'group-theory', 'finite-groups', 'permutations', 'additive-combinatorics']"
22,Tensor product algebra $\mathbb{C}\otimes_\mathbb{R} \mathbb{C}$,Tensor product algebra,\mathbb{C}\otimes_\mathbb{R} \mathbb{C},I want to understand the tensor product $\mathbb C$-algebra $\mathbb{C}\otimes_\mathbb{R} \mathbb{C}$. Of course it must be isomorphic to $\mathbb{C}\times\mathbb{C}.$ How can one construct an explicit isomorphism?,I want to understand the tensor product $\mathbb C$-algebra $\mathbb{C}\otimes_\mathbb{R} \mathbb{C}$. Of course it must be isomorphic to $\mathbb{C}\times\mathbb{C}.$ How can one construct an explicit isomorphism?,,"['abstract-algebra', 'commutative-algebra', 'tensor-products']"
23,"Can real numbers be considered ""even"" or ""odd""?","Can real numbers be considered ""even"" or ""odd""?",,Is the concept of even/odd numbers applicable to decimal numbers? For e.g. - 4.222 is a even number?,Is the concept of even/odd numbers applicable to decimal numbers? For e.g. - 4.222 is a even number?,,"['abstract-algebra', 'elementary-number-theory', 'parity']"
24,what are the different applications of group theory in CS? [closed],what are the different applications of group theory in CS? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question What are some applications of abstract algebra in computer science an undergraduate could begin exploring after a first course? Gallian's text goes into Hamming distance, coding theory, etc., I vaguely recall seeing discussions of abstract algebra in theory of computation / automata theory but what else? I'm not familiar with any applications past this. Bonus: What are some textbooks / resources that one could learn about said applications?","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question What are some applications of abstract algebra in computer science an undergraduate could begin exploring after a first course? Gallian's text goes into Hamming distance, coding theory, etc., I vaguely recall seeing discussions of abstract algebra in theory of computation / automata theory but what else? I'm not familiar with any applications past this. Bonus: What are some textbooks / resources that one could learn about said applications?",,"['abstract-algebra', 'computer-science']"
25,How to prove that a polynomial of degree $n$ has at most $n$ roots?,How to prove that a polynomial of degree  has at most  roots?,n n,"How can I prove, that a polynomial function $$f(x) = \sum_{0\le k \le n}a_k x^k\qquad n\in\mathbb N,\ a_k\in\mathbb C$$ is zero for at most $n$ different values of $x$ , unless all $a_0,a_1,\ldots,a_n$ are zero?","How can I prove, that a polynomial function is zero for at most different values of , unless all are zero?","f(x) = \sum_{0\le k \le n}a_k x^k\qquad n\in\mathbb N,\ a_k\in\mathbb C n x a_0,a_1,\ldots,a_n","['abstract-algebra', 'polynomials', 'roots']"
26,Why is the commutator defined differently for groups and rings?,Why is the commutator defined differently for groups and rings?,,"The commutator of two elements in a group is defined as $[g, h] = g^{−1}h^{−1}gh.$ In a ring, the commutator of two elements is $[a, b] = ab - ba.$ I'm asking because a ring is a (abelian) group under addition, so I would have expected it to be $[g, h] = -g-h+g+h.$","The commutator of two elements in a group is defined as $[g, h] = g^{−1}h^{−1}gh.$ In a ring, the commutator of two elements is $[a, b] = ab - ba.$ I'm asking because a ring is a (abelian) group under addition, so I would have expected it to be $[g, h] = -g-h+g+h.$",,"['abstract-algebra', 'group-theory', 'ring-theory']"
27,Requesting abstract algebra book recommendations [duplicate],Requesting abstract algebra book recommendations [duplicate],,This question already has answers here : Good abstract algebra books for self study (13 answers) Closed 1 year ago . I've taken up self-study of math. (How smart can that be?) I've just about finished a course in real analysis which spent a lot of time on metric spaces and some time revisiting calculus. I was thinking of trying abstract algebra. I would appreciate any book recommendations. Thanks in advance. Andrew,This question already has answers here : Good abstract algebra books for self study (13 answers) Closed 1 year ago . I've taken up self-study of math. (How smart can that be?) I've just about finished a course in real analysis which spent a lot of time on metric spaces and some time revisiting calculus. I was thinking of trying abstract algebra. I would appreciate any book recommendations. Thanks in advance. Andrew,,['abstract-algebra']
28,Prove that the Gaussian Integer's ring is a Euclidean domain,Prove that the Gaussian Integer's ring is a Euclidean domain,,"I'm having some trouble proving that the Gaussian Integer's ring ($\mathbb{Z}[ i ]$) is an Euclidean domain. Here is what i've got so far. To be a Euclidean domain means that there is a defined application (often called norm) that verifies this two conditions: $\forall a, b \in \mathbb{Z}[i] \backslash {0} \hspace{2 mm} a \mid b \hspace{2 mm} \rightarrow N(a) \leq N (b)$ $\forall a, b \in \mathbb{Z}[i] \hspace{2 mm} b \neq 0 \rightarrow \exists c,r \in \mathbb{Z}[i] \hspace {2 mm}$ so that $\hspace{2 mm} a = bc + r \hspace{2 mm} \text{and} \hspace{2 mm} (r = 0 \hspace{2 mm} \text{or} \hspace{2 mm} r \neq 0 \hspace{2 mm} N(r) \lt N (b) )$ I have that the application meant to be the ""norm"" goes: $N(a +b i) = a^2 + b^2$, and I've managed to prove the first condition, given that N is a multiplicative function, but I can not find a way to prove the second condition. I've search for a similar question but I have not found any so far, please redirect me if there's already a question about this and forgive me for my poor use of latex.","I'm having some trouble proving that the Gaussian Integer's ring ($\mathbb{Z}[ i ]$) is an Euclidean domain. Here is what i've got so far. To be a Euclidean domain means that there is a defined application (often called norm) that verifies this two conditions: $\forall a, b \in \mathbb{Z}[i] \backslash {0} \hspace{2 mm} a \mid b \hspace{2 mm} \rightarrow N(a) \leq N (b)$ $\forall a, b \in \mathbb{Z}[i] \hspace{2 mm} b \neq 0 \rightarrow \exists c,r \in \mathbb{Z}[i] \hspace {2 mm}$ so that $\hspace{2 mm} a = bc + r \hspace{2 mm} \text{and} \hspace{2 mm} (r = 0 \hspace{2 mm} \text{or} \hspace{2 mm} r \neq 0 \hspace{2 mm} N(r) \lt N (b) )$ I have that the application meant to be the ""norm"" goes: $N(a +b i) = a^2 + b^2$, and I've managed to prove the first condition, given that N is a multiplicative function, but I can not find a way to prove the second condition. I've search for a similar question but I have not found any so far, please redirect me if there's already a question about this and forgive me for my poor use of latex.",,"['abstract-algebra', 'ring-theory', 'gaussian-integers', 'euclidean-domain']"
29,Do odd imaginary numbers exist? [parity for Gaussian integers],Do odd imaginary numbers exist? [parity for Gaussian integers],,Is the concept of an odd imaginary number defined/well-defined/used in mathematics? I searched around but couldn't find anything. Thanks!,Is the concept of an odd imaginary number defined/well-defined/used in mathematics? I searched around but couldn't find anything. Thanks!,,"['abstract-algebra', 'elementary-number-theory', 'complex-numbers', 'parity']"
30,"""Cayley's theorem"" for Lie algebras?","""Cayley's theorem"" for Lie algebras?",,"Groups can be defined abstractly as sets with a binary operation satisfying certain identities, or concretely as a collection of permutations of a set.  Cayley's theorem ensures that these two definitions are equivalent: any abstract group acts as a collection of permutations of its underlying set, and this action is faithful. Similarly, rings can be defined abstractly as sets with a pair of binary operations satisfying certain identities, or concretely as a collection of endomorphisms of an abelian group.  There is a ""Cayley's theorem"" here as well: any abstract ring acts as a collection of endomorphisms of its underlying abelian group, and this action is faithful. The situation for Lie algebras seems much less clear to me.  The adjoint representation is not generally faithful, and Ado's theorem comes with qualifications and doesn't have the simplicity of the two theorems above.  For me, the problem is that I don't have a good sense of what the concrete definition of a Lie algebra is supposed to be. I suspect that a good concrete definition of a Lie algebra is as a space of derivations on some algebra closed under commutator.  In that case, is it correct to say that a Lie algebra acts faithfully as derivations on its universal enveloping algebra?  Is this a good analogue of Cayley's theorem? (Motivation: in the books on Lie algebras I have read, the authors verify that Lie algebras which occur in nature satisfy alternativity and the Jacobi identity, but I have never seen any simple justification that these axioms are ""enough"" in the same way that Cayley's theorem tells you that the axioms for a group or a ring are ""enough.""  There is just Ado's theorem which, again, comes with qualifications and is hard.)","Groups can be defined abstractly as sets with a binary operation satisfying certain identities, or concretely as a collection of permutations of a set.  Cayley's theorem ensures that these two definitions are equivalent: any abstract group acts as a collection of permutations of its underlying set, and this action is faithful. Similarly, rings can be defined abstractly as sets with a pair of binary operations satisfying certain identities, or concretely as a collection of endomorphisms of an abelian group.  There is a ""Cayley's theorem"" here as well: any abstract ring acts as a collection of endomorphisms of its underlying abelian group, and this action is faithful. The situation for Lie algebras seems much less clear to me.  The adjoint representation is not generally faithful, and Ado's theorem comes with qualifications and doesn't have the simplicity of the two theorems above.  For me, the problem is that I don't have a good sense of what the concrete definition of a Lie algebra is supposed to be. I suspect that a good concrete definition of a Lie algebra is as a space of derivations on some algebra closed under commutator.  In that case, is it correct to say that a Lie algebra acts faithfully as derivations on its universal enveloping algebra?  Is this a good analogue of Cayley's theorem? (Motivation: in the books on Lie algebras I have read, the authors verify that Lie algebras which occur in nature satisfy alternativity and the Jacobi identity, but I have never seen any simple justification that these axioms are ""enough"" in the same way that Cayley's theorem tells you that the axioms for a group or a ring are ""enough.""  There is just Ado's theorem which, again, comes with qualifications and is hard.)",,"['abstract-algebra', 'lie-algebras']"
31,When are two semidirect products isomorphic?,When are two semidirect products isomorphic?,,"Let $N$, $H$ be groups, and $\varphi : H \to \operatorname{Aut}(N)$ a group homomorphism. Then we can form $N \rtimes_{\varphi} H$, the semidirect product of $N$ and $H$ with respect to $\varphi$. Is there a way to determine if $\varphi_1, \varphi_2 : H \to \operatorname{Aut}(N)$ lead to isomorphic semidirect products by just comparing $\varphi_1$ and $\varphi_2$?","Let $N$, $H$ be groups, and $\varphi : H \to \operatorname{Aut}(N)$ a group homomorphism. Then we can form $N \rtimes_{\varphi} H$, the semidirect product of $N$ and $H$ with respect to $\varphi$. Is there a way to determine if $\varphi_1, \varphi_2 : H \to \operatorname{Aut}(N)$ lead to isomorphic semidirect products by just comparing $\varphi_1$ and $\varphi_2$?",,"['abstract-algebra', 'group-theory', 'reference-request']"
32,Shortest irreducible polynomials over $\Bbb F_p$ of degree $n$,Shortest irreducible polynomials over  of degree,\Bbb F_p n,"For any prime $p$ , one can realize any finite field $\Bbb F_{p^n}$ as the quotient of the ring $\Bbb F_p[X]$ by the maximal ideal generated by an irreducible polynomial $f$ of degree $n$ . By dividing by the leading coefficient, we may as well assume $f$ is monic, in which case we can write it as $$f(X) = X^n + a_{n - 1} X^n + \cdots + a_1 X + a_0. \def\co{\color{#00bf00}{1}} \def\ct{\color{#0000ff}{2}} \def\ch{\color{#bf00bf}{3}} \def\cf{\color{#ff0000}{4}} \def\ci{\color{#ff7f00}{5}} $$ If we let $\zeta$ denote a root of $f$ , then $\Bbb F_{p^n} \cong \Bbb F_p[X] / \langle f \rangle \cong \Bbb F_p[\zeta]$ , and so when computing multiplication in this field and write elements as polynomials in $\zeta$ of degree $< n$ , one way or another we use iteratively the identity $$\zeta^n = -a_{n - 1} \zeta^{n - 1} - \cdots - a_1 \zeta - a_0.$$ Manually multiplying elements in this field is naively more efficient, then, when one chooses a polynomial $f$ with fewer nonzero coefficients. So, naturally, we can ask just how efficient we can be: For any prime $p$ and any positive integer $n > 1$ , what is the least number $\lambda(p, n)$ of nonzero coefficients an irreducible polynomial of degree $n$ over $\Bbb F_p$ can have? Some general observations: The only polynomial of degree $n$ with exactly one nonzero coefficient is $X^n$ , $\lambda(p, n) > 1$ . Jim Belk's answer shows that there is an irreducible polynomial of the form $X^n + a$ , that is, $\lambda(p, n) = 2$ , if $p \not\mid n$ and $p$ has order $n$ modulo $n (p - 1)$ . Thus, if these criteria do not hold for $(p, n)$ , we have $\lambda(p, n) \geq 3$ . Case $p = 2$ . Several behaviors are peculiar to the case $p = 2$ . First, if $f(X) \in \Bbb F_p[X]$ has an even number of terms with coefficient $1$ , then $f(1) = 0$ and so $f$ is divisible by $x - 1$ , hence (if $\deg f > 1$ ) not irreducible. Thus, for $n > 1$ , $\lambda(p, n)$ must be odd. Some additional facts about this case: Swan has given several sufficient conditions for the reducibility of a trinomial $x^n + x^k + 1$ in $\Bbb F_2[x]$ (see citation below). One of these conditions in particular implies that all such trinomials are reducible when $n \equiv 0 \bmod 8$ , and hence $\lambda(2, 8m) > 3$ . More details can be found in $\S$ 40.9 of Jörg Arndt's Matters Computational (pdf warning, $>5$ MB). Ciet, Quiscater, and Siet showed similarly that $\lambda(2, n) > 3$ if $n \equiv 13 \bmod 24$ or $n \equiv 19 \bmod 24$ . Case $p \neq 2$ . If $n = 2$ and we write $p = 2 q + 1$ , then $p^2 = (2 q + 1)^2 = 4q(q + 1) + 1 \equiv 1 \pmod {4 q} = 1 \pmod {2(p - 1)}$ , so by Jim Belk's characterization, $\lambda(p, 2)$ = 2. Harry Altman gives a proof (generalizing an observation) below that for $p > 3$ we have $\lambda(p, 3) = 2$ for $p \equiv 1 \bmod 3$ and $\lambda(p, 3) = 3$ for $p \equiv 2 \bmod 3$ . These facts together give us: A characterization of $(p, n)$ such that $\lambda(p, n) = 2$ . Knowledge of all $\lambda(p, n)$ , $n \leq 3$ . It thus remains to determine which $(p, n)$ have $\lambda(p, n) > 3$ and $\lambda(p, n)$ for those values. Some naive experimentation suggests that it is rare for $\lambda(2, n) > 5$ and for $\lambda(p, n) > 3$ for $p > 2$ . A naive Maple script gives that the only values of $\lambda(n, p)$ that occur for $p < 2^5, n \leq 2^8$ are $2, 3, 4, 5$ . Apparently minimal examples are: \begin{array}{crrr} \hline \lambda(p, n) & p & n & f(X) \\ \hline 2 & 3 &  2 & X^2 + 1 \\ 3 & 2 &  2 & X^2 + X + 1 \\ 4 & 5 & 35 & X^{35} + X^4 + 4 X + 1 \\ 5 & 2 &  5 & X^8 + X^4 + X^3 + X + 1 \\ \hline \end{array} For $p = 2$ , Table of Low-Weight Binary Irreducible Polynomials gives minimal polynomials (and hence values $\lambda(2, n)$ ) for all $n \leq 10^5$ . In all cases, $\lambda(2, n) \in \{3, 5\}$ . See also OEIS A057486 , ""Degrees of absolutely reducible trinomials, i.e. numbers $n$ such that $x^n + x^m + 1$ is factorable [modulo $2$ ] for all $m$ between $1$ and $n$ ."" What is the smallest degree $n$ , if any, such that $\lambda(2, n) > 5$ , i.e., for which there are no irreducible trinomials or pentanomials over $\Bbb F_2$ ? If there is such a degree, what is the maximum value of $\lambda(2, n)$ , if any? Among $2 < p < 2^5$ and $n \leq 2^8$ , the only values $\lambda(p, n) > 3$ are the following, and in each case $\lambda(p, n) = 4$ : \begin{array}{rl} \hline p & n \\ \hline 3 & 49, 57, 65, 68, 75, 98, 105, 123, 129, 130, 132, 149, 161, 175, 189, \\ & \quad 197, 207, 212, 213, 221, 223, 231, 233 \\ 5 & 35, 70, 123, 125, 140, 181, 191, 209, 213, 219, 237, 249, 250, 253 \\ 7 & 124, 163 \\ 11 & 219 \\ 17 & 231 \\ \hline \end{array} Searching $2 < p \leq 8161$ (the $2^{10}$ th prime) and $n \leq 2^4$ yields no cases where $\lambda(p, n) > 3$ . What is the smallest $n$ such that $\lambda(p, n) \leq 3$ for all $p > 2$ ? (We know from the above that $4 \leq n \leq 35$ .) Is $\lambda(p, n) > 4$ for some $n$ and $p > 2$ ? If so, what is a minimal example, and what is the maximum value of $\lambda(p, n)$ , $p > 2$ ? References Jörg Arndt, Matters Computational (pdf warning, $>5$ MB) Mathieu Ciet, Jean-Jacques Quisquater, Francesco Sica, ""A Short Note on Irreducible Trinomials in Binary Fields"", (2002). Gadiel Seroussi, ""Table of Low-Weight Binary Irreducible Polynomials,"" Computer Systems Laboratory HPL-98-135. Richard G. Swan, ""Factorization of polynomials over finite fields"", Pacific Journal of Mathematics , (12) 3, pp. 1099-1106, (1962).","For any prime , one can realize any finite field as the quotient of the ring by the maximal ideal generated by an irreducible polynomial of degree . By dividing by the leading coefficient, we may as well assume is monic, in which case we can write it as If we let denote a root of , then , and so when computing multiplication in this field and write elements as polynomials in of degree , one way or another we use iteratively the identity Manually multiplying elements in this field is naively more efficient, then, when one chooses a polynomial with fewer nonzero coefficients. So, naturally, we can ask just how efficient we can be: For any prime and any positive integer , what is the least number of nonzero coefficients an irreducible polynomial of degree over can have? Some general observations: The only polynomial of degree with exactly one nonzero coefficient is , . Jim Belk's answer shows that there is an irreducible polynomial of the form , that is, , if and has order modulo . Thus, if these criteria do not hold for , we have . Case . Several behaviors are peculiar to the case . First, if has an even number of terms with coefficient , then and so is divisible by , hence (if ) not irreducible. Thus, for , must be odd. Some additional facts about this case: Swan has given several sufficient conditions for the reducibility of a trinomial in (see citation below). One of these conditions in particular implies that all such trinomials are reducible when , and hence . More details can be found in 40.9 of Jörg Arndt's Matters Computational (pdf warning, MB). Ciet, Quiscater, and Siet showed similarly that if or . Case . If and we write , then , so by Jim Belk's characterization, = 2. Harry Altman gives a proof (generalizing an observation) below that for we have for and for . These facts together give us: A characterization of such that . Knowledge of all , . It thus remains to determine which have and for those values. Some naive experimentation suggests that it is rare for and for for . A naive Maple script gives that the only values of that occur for are . Apparently minimal examples are: For , Table of Low-Weight Binary Irreducible Polynomials gives minimal polynomials (and hence values ) for all . In all cases, . See also OEIS A057486 , ""Degrees of absolutely reducible trinomials, i.e. numbers such that is factorable [modulo ] for all between and ."" What is the smallest degree , if any, such that , i.e., for which there are no irreducible trinomials or pentanomials over ? If there is such a degree, what is the maximum value of , if any? Among and , the only values are the following, and in each case : Searching (the th prime) and yields no cases where . What is the smallest such that for all ? (We know from the above that .) Is for some and ? If so, what is a minimal example, and what is the maximum value of , ? References Jörg Arndt, Matters Computational (pdf warning, MB) Mathieu Ciet, Jean-Jacques Quisquater, Francesco Sica, ""A Short Note on Irreducible Trinomials in Binary Fields"", (2002). Gadiel Seroussi, ""Table of Low-Weight Binary Irreducible Polynomials,"" Computer Systems Laboratory HPL-98-135. Richard G. Swan, ""Factorization of polynomials over finite fields"", Pacific Journal of Mathematics , (12) 3, pp. 1099-1106, (1962).","p \Bbb F_{p^n} \Bbb F_p[X] f n f f(X) = X^n + a_{n - 1} X^n + \cdots + a_1 X + a_0.
\def\co{\color{#00bf00}{1}}
\def\ct{\color{#0000ff}{2}}
\def\ch{\color{#bf00bf}{3}}
\def\cf{\color{#ff0000}{4}}
\def\ci{\color{#ff7f00}{5}}
 \zeta f \Bbb F_{p^n} \cong \Bbb F_p[X] / \langle f \rangle \cong \Bbb F_p[\zeta] \zeta < n \zeta^n = -a_{n - 1} \zeta^{n - 1} - \cdots - a_1 \zeta - a_0. f p n > 1 \lambda(p, n) n \Bbb F_p n X^n \lambda(p, n) > 1 X^n + a \lambda(p, n) = 2 p \not\mid n p n n (p - 1) (p, n) \lambda(p, n) \geq 3 p = 2 p = 2 f(X) \in \Bbb F_p[X] 1 f(1) = 0 f x - 1 \deg f > 1 n > 1 \lambda(p, n) x^n + x^k + 1 \Bbb F_2[x] n \equiv 0 \bmod 8 \lambda(2, 8m) > 3 \S >5 \lambda(2, n) > 3 n \equiv 13 \bmod 24 n \equiv 19 \bmod 24 p \neq 2 n = 2 p = 2 q + 1 p^2 = (2 q + 1)^2 = 4q(q + 1) + 1 \equiv 1 \pmod {4 q} = 1 \pmod {2(p - 1)} \lambda(p, 2) p > 3 \lambda(p, 3) = 2 p \equiv 1 \bmod 3 \lambda(p, 3) = 3 p \equiv 2 \bmod 3 (p, n) \lambda(p, n) = 2 \lambda(p, n) n \leq 3 (p, n) \lambda(p, n) > 3 \lambda(p, n) \lambda(2, n) > 5 \lambda(p, n) > 3 p > 2 \lambda(n, p) p < 2^5, n \leq 2^8 2, 3, 4, 5 \begin{array}{crrr}
\hline
\lambda(p, n) & p & n & f(X) \\
\hline
2 & 3 &  2 & X^2 + 1 \\
3 & 2 &  2 & X^2 + X + 1 \\
4 & 5 & 35 & X^{35} + X^4 + 4 X + 1 \\
5 & 2 &  5 & X^8 + X^4 + X^3 + X + 1 \\
\hline
\end{array} p = 2 \lambda(2, n) n \leq 10^5 \lambda(2, n) \in \{3, 5\} n x^n + x^m + 1 2 m 1 n n \lambda(2, n) > 5 \Bbb F_2 \lambda(2, n) 2 < p < 2^5 n \leq 2^8 \lambda(p, n) > 3 \lambda(p, n) = 4 \begin{array}{rl}
\hline
p & n \\
\hline
3 & 49, 57, 65, 68, 75, 98, 105, 123, 129, 130, 132, 149, 161, 175, 189, \\ & \quad 197, 207, 212, 213, 221, 223, 231, 233 \\
5 & 35, 70, 123, 125, 140, 181, 191, 209, 213, 219, 237, 249, 250, 253 \\
7 & 124, 163 \\
11 & 219 \\
17 & 231 \\
\hline
\end{array} 2 < p \leq 8161 2^{10} n \leq 2^4 \lambda(p, n) > 3 n \lambda(p, n) \leq 3 p > 2 4 \leq n \leq 35 \lambda(p, n) > 4 n p > 2 \lambda(p, n) p > 2 >5","['abstract-algebra', 'polynomials', 'field-theory', 'finite-fields']"
33,Prove that every group of order $4$ is abelian,Prove that every group of order  is abelian,4,"How can I show that every group of order $4$ is abelian? Let's denote $e,a,b,c$ as the four elements of the group. Since $e$ is identity, we have $e*x=x*e$ for every $x$: $$\begin{array}{|c|cccc|} \hline     & e & a & b & c \\\hline   e & e & a & b & c \\   a & a &   &   &   \\   b & b &   &   &   \\   c & c &   &   &   \\\hline \end{array}$$ Now $a*a=$? We have several possibilities. If I choose $a*a=b$ I can show $a*b=a*(a*a)=(a*a)*a=b*a$. But so far I have shown only that $a$ and $b$ commute, there are also other pairs. And I have only discussed the choice $a*a=b$, there are also other possibilities. Is there a simpler way to do this?","How can I show that every group of order $4$ is abelian? Let's denote $e,a,b,c$ as the four elements of the group. Since $e$ is identity, we have $e*x=x*e$ for every $x$: $$\begin{array}{|c|cccc|} \hline     & e & a & b & c \\\hline   e & e & a & b & c \\   a & a &   &   &   \\   b & b &   &   &   \\   c & c &   &   &   \\\hline \end{array}$$ Now $a*a=$? We have several possibilities. If I choose $a*a=b$ I can show $a*b=a*(a*a)=(a*a)*a=b*a$. But so far I have shown only that $a$ and $b$ commute, there are also other pairs. And I have only discussed the choice $a*a=b$, there are also other possibilities. Is there a simpler way to do this?",,"['abstract-algebra', 'group-theory', 'abelian-groups']"
34,What does it mean when two Groups are isomorphic?,What does it mean when two Groups are isomorphic?,,"I'm not asking for the formal definition I know it. An isomorphism is a bijective homomorphism. In my book it's indicated many times when two groups are isomorphic, and I don't understand what's the reason for that. What can we ""do"" when we know that 2 groups are isomorphic? What does it really mean when 2 groups are isomorphic?","I'm not asking for the formal definition I know it. An isomorphism is a bijective homomorphism. In my book it's indicated many times when two groups are isomorphic, and I don't understand what's the reason for that. What can we ""do"" when we know that 2 groups are isomorphic? What does it really mean when 2 groups are isomorphic?",,"['abstract-algebra', 'group-theory', 'soft-question', 'group-isomorphism']"
35,Commutative non Noetherian rings in which all maximal ideals are finitely generated,Commutative non Noetherian rings in which all maximal ideals are finitely generated,,"In commutative rings we have the following Theorem. $R$ is Noetherian if and only if each prime ideal of $R$ is finitely generated. From this Theorem I am looking for commutative rings $R$ in which every maximal ideal is finitely generated but $R$ is non Noetherian. Question : Is there a straightforward example of a commutative ring $R$ so that each maximal ideal is finitely generated, but $R$ is non Noetherian? Thank You","In commutative rings we have the following Theorem. $R$ is Noetherian if and only if each prime ideal of $R$ is finitely generated. From this Theorem I am looking for commutative rings $R$ in which every maximal ideal is finitely generated but $R$ is non Noetherian. Question : Is there a straightforward example of a commutative ring $R$ so that each maximal ideal is finitely generated, but $R$ is non Noetherian? Thank You",,"['abstract-algebra', 'commutative-algebra', 'noetherian']"
36,Why do we care about two subgroups being conjugate?,Why do we care about two subgroups being conjugate?,,"In classifications of the subgroups of a given group, results are often stated up to conjugacy.  I would like to know why this is. More generally, I don't understand why ""conjugacy"" is an equivalence relation we care about, beyond the fact that it is stronger than ""abstractly isomorphic."" My vague understanding is that while ""abstractly isomorphic"" is the correct ""intrinsic"" notion of isomorphism, so ""conjugate"" is the correct ""extrinsic"" notion.  But why have we designated this notion of equivalence, and not some other one? To receive a satisfactory answer, let me be slightly more precise: Question: Given two subgroups $H_1, H_2$ of a given group $G$, what properties are preserved under conjugacy that may break under general abstract isomorphism? For example, is it true that $G/H_1 \cong G/H_2$ iff $H_1$ is conjugate to $H_2$?  Or, is it true that two subgroups $H_1, H_2 \leq \text{GL}(V)$ are conjugate iff their representations are isomorphic?  I'm sure these are easy questions to answer -- admittedly, I haven't thought fully about either -- but I raise them by way of example. What are other such equivalent characterizations?","In classifications of the subgroups of a given group, results are often stated up to conjugacy.  I would like to know why this is. More generally, I don't understand why ""conjugacy"" is an equivalence relation we care about, beyond the fact that it is stronger than ""abstractly isomorphic."" My vague understanding is that while ""abstractly isomorphic"" is the correct ""intrinsic"" notion of isomorphism, so ""conjugate"" is the correct ""extrinsic"" notion.  But why have we designated this notion of equivalence, and not some other one? To receive a satisfactory answer, let me be slightly more precise: Question: Given two subgroups $H_1, H_2$ of a given group $G$, what properties are preserved under conjugacy that may break under general abstract isomorphism? For example, is it true that $G/H_1 \cong G/H_2$ iff $H_1$ is conjugate to $H_2$?  Or, is it true that two subgroups $H_1, H_2 \leq \text{GL}(V)$ are conjugate iff their representations are isomorphic?  I'm sure these are easy questions to answer -- admittedly, I haven't thought fully about either -- but I raise them by way of example. What are other such equivalent characterizations?",,"['abstract-algebra', 'group-theory', 'representation-theory', 'lie-groups']"
37,"Why are polynomials defined to be ""formal"" (vs. functions)?","Why are polynomials defined to be ""formal"" (vs. functions)?",,"Despite the fact that $\forall n, n^3 + 2n \equiv 0 \pmod 3$, I understand that $n^3 + 2n$ (considered as a polynomial with coefficients in $\mathbb Z/3\mathbb Z$) is not equal to the zero polynomial. What is the value of defining polynomials in this (strange) way? What situations does it make things simpler? I ask this because it seemed natural to me to define polynomials as a subset of functions, so I was surprised by this.","Despite the fact that $\forall n, n^3 + 2n \equiv 0 \pmod 3$, I understand that $n^3 + 2n$ (considered as a polynomial with coefficients in $\mathbb Z/3\mathbb Z$) is not equal to the zero polynomial. What is the value of defining polynomials in this (strange) way? What situations does it make things simpler? I ask this because it seemed natural to me to define polynomials as a subset of functions, so I was surprised by this.",,['abstract-algebra']
38,"What does it mean for something to hold ""up to isomorphism""?","What does it mean for something to hold ""up to isomorphism""?",,"For example, to say that there are 2 such groups up to isomorphism such that the order of the group $G$ is equal to $p^2$ ?","For example, to say that there are 2 such groups up to isomorphism such that the order of the group is equal to ?",G p^2,"['abstract-algebra', 'group-theory', 'group-isomorphism', 'group-homomorphism']"
39,How can we show that $\mathbb Q$ is not a free $\mathbb Z$-module?,How can we show that  is not a free -module?,\mathbb Q \mathbb Z,"I am really confused from the definition. How do we know that $\mathbb Q$ is not a free $\mathbb Z$-module? In class people use it as a trivial fact, but I don't seem to understand.","I am really confused from the definition. How do we know that $\mathbb Q$ is not a free $\mathbb Z$-module? In class people use it as a trivial fact, but I don't seem to understand.",,"['abstract-algebra', 'modules']"
40,Why are ideals more important than subrings?,Why are ideals more important than subrings?,,"I have read that subgroups, subrings, submodules, etc. are substructures . But if you look at the definition of the Noetherian rings and Noetherian modules, Noetherian rings are defined with ideals and Noetherian modules are defined with submodules. Isn't it awkward? Why does submodule correspond to ideal, not subring? Is there any definition of Noetherian with subrings? As I'm studying commutative algebra, it looks like ideals are more important than subrings. But why is it ideal, not subring (which seems to correspond to all other substructures)? Though I am not very familiar with pseudo-rings , is it true that ideal is a sub-pseudo-ring (or sub-rng) and thus we can view ideal as a kind of substructure?","I have read that subgroups, subrings, submodules, etc. are substructures . But if you look at the definition of the Noetherian rings and Noetherian modules, Noetherian rings are defined with ideals and Noetherian modules are defined with submodules. Isn't it awkward? Why does submodule correspond to ideal, not subring? Is there any definition of Noetherian with subrings? As I'm studying commutative algebra, it looks like ideals are more important than subrings. But why is it ideal, not subring (which seems to correspond to all other substructures)? Though I am not very familiar with pseudo-rings , is it true that ideal is a sub-pseudo-ring (or sub-rng) and thus we can view ideal as a kind of substructure?",,"['abstract-algebra', 'commutative-algebra', 'ideals', 'rngs']"
41,"How to prove that $\mathbb{Q}[\sqrt{p_1}, \sqrt{p_2}, \ldots,\sqrt{p_n} ] = \mathbb{Q}[\sqrt{p_1}+ \sqrt{p_2}+\cdots + \sqrt{p_n}]$, for $p_i$ prime?","How to prove that , for  prime?","\mathbb{Q}[\sqrt{p_1}, \sqrt{p_2}, \ldots,\sqrt{p_n} ] = \mathbb{Q}[\sqrt{p_1}+ \sqrt{p_2}+\cdots + \sqrt{p_n}] p_i","This is Exercise 18.14 from Algebra, Isaacs. $p_{1}\ ,\ p_{2}\ ,\ ... p_{n}$ are different prime numbers. How to show that $$\mathbb{Q}[\sqrt{p_{1}}, \sqrt{p_{2}}, \ldots, \sqrt{p_{n}} ] = \mathbb{Q}[\sqrt{p_{1}}+ \sqrt{p_{2}}+\cdots + \sqrt{p_{n}}] \quad ?$$  First we can note that the Galois group of the first extension over $\mathbb{Q}$ ( which I call $E$ ) is elementary abelian of order $2^{n}$, so we can prove that the orbit of $\sqrt{p_{1}}+\sqrt{p_{2}}+ ... + \sqrt{p_{n}}$ under $\operatorname{Gal}(E/\mathbb{Q})$ contains $2^{n}$ elements, but how to do so?","This is Exercise 18.14 from Algebra, Isaacs. $p_{1}\ ,\ p_{2}\ ,\ ... p_{n}$ are different prime numbers. How to show that $$\mathbb{Q}[\sqrt{p_{1}}, \sqrt{p_{2}}, \ldots, \sqrt{p_{n}} ] = \mathbb{Q}[\sqrt{p_{1}}+ \sqrt{p_{2}}+\cdots + \sqrt{p_{n}}] \quad ?$$  First we can note that the Galois group of the first extension over $\mathbb{Q}$ ( which I call $E$ ) is elementary abelian of order $2^{n}$, so we can prove that the orbit of $\sqrt{p_{1}}+\sqrt{p_{2}}+ ... + \sqrt{p_{n}}$ under $\operatorname{Gal}(E/\mathbb{Q})$ contains $2^{n}$ elements, but how to do so?",,"['abstract-algebra', 'field-theory', 'galois-theory']"
42,Can we think of a chain homotopy as a homotopy?,Can we think of a chain homotopy as a homotopy?,,"I'm taking a course in algebraic topology, which includes an introduction to (simplicial) homology, and I'm looking for a bit of intuition regarding chain homotopies. The definitions I'm using are: Let $f,g : X \to Y$ be continuous functions between topological spaces. A homotopy from $f$ to $g$ is a continuous map $H : X \times [0,1] \to Y$ such that $H(\, \cdot\, , 0) = f$ and $H(\, \cdot\, , 1) = g$ . Let $f_{\bullet}, g_{\bullet} : A_{\bullet} \to B_{\bullet}$ be chain maps between chain complexes $(A, d_A)$ and $(B,d_B)$ . A chain homotopy from $f$ to $g$ is a sequence of maps $h_n : A_n \to B_{n+1}$ such that $f_n-g_n=d_Bh_n+h_{n-1}d_A$ . I'm aware of the properties of a chain homotopy and how they are similar to those of a homotopy, but I still find the definition quite opaque and the notion quite hard to picture $-$ it would help me a lot if I could think of a chain homotopy in a similar way to how I think of a homotopy. Or, to make my question a bit less vague, I'd like to know: What is the rationale behind the definition of a chain homotopy? Is there a fundamental similarity between a chain homotopy and a homotopy, beyond their further consequences? (General waffle would also be appreciated; I'd really like to develop a good understanding.)","I'm taking a course in algebraic topology, which includes an introduction to (simplicial) homology, and I'm looking for a bit of intuition regarding chain homotopies. The definitions I'm using are: Let be continuous functions between topological spaces. A homotopy from to is a continuous map such that and . Let be chain maps between chain complexes and . A chain homotopy from to is a sequence of maps such that . I'm aware of the properties of a chain homotopy and how they are similar to those of a homotopy, but I still find the definition quite opaque and the notion quite hard to picture it would help me a lot if I could think of a chain homotopy in a similar way to how I think of a homotopy. Or, to make my question a bit less vague, I'd like to know: What is the rationale behind the definition of a chain homotopy? Is there a fundamental similarity between a chain homotopy and a homotopy, beyond their further consequences? (General waffle would also be appreciated; I'd really like to develop a good understanding.)","f,g : X \to Y f g H : X \times [0,1] \to Y H(\, \cdot\, , 0) = f H(\, \cdot\, , 1) = g f_{\bullet}, g_{\bullet} : A_{\bullet} \to B_{\bullet} (A, d_A) (B,d_B) f g h_n : A_n \to B_{n+1} f_n-g_n=d_Bh_n+h_{n-1}d_A -","['abstract-algebra', 'algebraic-topology', 'intuition', 'homological-algebra']"
43,Show that there are infinitely many powers of two starting with the digit 7 [duplicate],Show that there are infinitely many powers of two starting with the digit 7 [duplicate],,This question already has answers here : Starting digits of $2^n$. (1 answer) Is $2^k = 2013...$ for some $k$? [duplicate] (1 answer) Closed 7 years ago . This is a contest math problem which I was not able to solve. A hint toward the solution would be helpful as well. Problem: Show that there are infinitely many powers of 2 starting with the digit 7. Thanks in advance.,This question already has answers here : Starting digits of $2^n$. (1 answer) Is $2^k = 2013...$ for some $k$? [duplicate] (1 answer) Closed 7 years ago . This is a contest math problem which I was not able to solve. A hint toward the solution would be helpful as well. Problem: Show that there are infinitely many powers of 2 starting with the digit 7. Thanks in advance.,,"['abstract-algebra', 'number-theory']"
44,Is $\mathbb{Z}[x]$ a principal ideal domain?,Is  a principal ideal domain?,\mathbb{Z}[x],Is $ \mathbb{Z}[x] $ a principal ideal domain? Since the standard definition of principal ideal domain is quite difficult to use. Could you give me some equivalent conditions on whether a ring is a principal ideal domain?,Is $ \mathbb{Z}[x] $ a principal ideal domain? Since the standard definition of principal ideal domain is quite difficult to use. Could you give me some equivalent conditions on whether a ring is a principal ideal domain?,,"['abstract-algebra', 'ring-theory', 'ideals', 'principal-ideal-domains']"
45,Is there a commutative operation for which the inverse of the operation is also commutative?,Is there a commutative operation for which the inverse of the operation is also commutative?,,"For instance addition is commutative, but the inverse, subtraction, is not. $$ 5+2 = 2+5\\ 5-2 \neq 2-5 $$ Same for multiplication/division: $$ 5\times4 = 4\times5\\ 5/4 \neq 4/5 $$ So is there a group operation $\circ$ with the inverse $\circ^{-1}$ such that $$ a\circ b = b\circ a\\ a\circ^{-1}b = b\circ^{-1}a $$","For instance addition is commutative, but the inverse, subtraction, is not. $$ 5+2 = 2+5\\ 5-2 \neq 2-5 $$ Same for multiplication/division: $$ 5\times4 = 4\times5\\ 5/4 \neq 4/5 $$ So is there a group operation $\circ$ with the inverse $\circ^{-1}$ such that $$ a\circ b = b\circ a\\ a\circ^{-1}b = b\circ^{-1}a $$",,['abstract-algebra']
46,How can you show there are only 2 nonabelian groups of order 8?,How can you show there are only 2 nonabelian groups of order 8?,,"It's often said that there are only two nonabelian groups of order 8 up to isomorphism, one is the quaternion group, the other given by the relations $a^4=1$, $b^2=1$ and $bab^{-1}=a^3$. I've never understood why these are the only two. Is there a reference or proof walkthrough on how to show any nonabelian group of order 8 is isomorphic to one of these?","It's often said that there are only two nonabelian groups of order 8 up to isomorphism, one is the quaternion group, the other given by the relations $a^4=1$, $b^2=1$ and $bab^{-1}=a^3$. I've never understood why these are the only two. Is there a reference or proof walkthrough on how to show any nonabelian group of order 8 is isomorphic to one of these?",,"['abstract-algebra', 'group-theory']"
47,Subgroup of $\mathbb{Q}$ with finite index,Subgroup of  with finite index,\mathbb{Q},"Consider the group $\mathbb{Q}$ under addition of rational numbers. If $H$ is a subgroup of $\mathbb{Q}$ with finite index, then $H = \mathbb{Q}$. I just saw this on our exam earlier and was stumped on how to show this. Any ideas?","Consider the group $\mathbb{Q}$ under addition of rational numbers. If $H$ is a subgroup of $\mathbb{Q}$ with finite index, then $H = \mathbb{Q}$. I just saw this on our exam earlier and was stumped on how to show this. Any ideas?",,"['abstract-algebra', 'group-theory']"
48,Is $\mathbb{Q}(\sqrt{2}) \cong \mathbb{Q}(\sqrt{3})$?,Is ?,\mathbb{Q}(\sqrt{2}) \cong \mathbb{Q}(\sqrt{3}),"In this post we saw isomorphism of vector spaces over $\mathbb{Q}$ . Just came across this question: Is $\mathbb{Q}(\sqrt{2}) \cong \mathbb{Q}(\sqrt{3})$ ? I know these as $\mathbb{Q}$ -Vector spaces, are isomorphic from the  linked post. But as fields are they isomorphic? I neither know how to prove it nor how to disprove it.","In this post we saw isomorphism of vector spaces over . Just came across this question: Is ? I know these as -Vector spaces, are isomorphic from the  linked post. But as fields are they isomorphic? I neither know how to prove it nor how to disprove it.",\mathbb{Q} \mathbb{Q}(\sqrt{2}) \cong \mathbb{Q}(\sqrt{3}) \mathbb{Q},['abstract-algebra']
49,Alternative proof that the parity of permutation is well defined?,Alternative proof that the parity of permutation is well defined?,,"I learned the following theorem about the properties of permutation from Gallian's Contemporary Abstract Algebra . Theorem 5.4 $\;$ Always Even or Always Odd If a permutation $\alpha$ can be expressed as a product of an even number of $2$-cycles, then every decomposition of $\alpha$ into a product of $2$-cycles must have an even number of $2$-cycles. In symbols, if   $$   \alpha = \beta_1 \beta_2 \dotsm \beta_r   \quad\text{and}\quad   \alpha = \gamma_1 \gamma_2 \dotsm \gamma_s $$   where the $\beta$'s and the $\gamma$'s are $2$-cycles, then $r$ and $s$ are both even or both odd. When I tried to reconstruct the proof myself, I found that it suffices to prove the following lemma: If $\epsilon=\beta_1\beta_2\cdots\beta_r$ where $\beta$'s are $2$-cycles, then $r$ is even. The original proof for this lemma uses the following key property of the product of $\beta_1\beta_2$: The product can always be expressed in one of the following forms on the left:   $$\begin{align}(ab)(ab)&=\epsilon\\ (ab)(ac)&=(bc)(ab)\\ (ab)(cd)&=(cd)(ab)\\ (ab)(bc)&=(bc)(ac)\end{align}$$ The proof for the lemma is based on such property and mathematical induction. I found it really hard to remind of such property, so I set it as an exercise to give another proof. However, I have no idea how to actually do it. So here is my question : Does anybody know an alternative proof of the lemma or the theorem?","I learned the following theorem about the properties of permutation from Gallian's Contemporary Abstract Algebra . Theorem 5.4 $\;$ Always Even or Always Odd If a permutation $\alpha$ can be expressed as a product of an even number of $2$-cycles, then every decomposition of $\alpha$ into a product of $2$-cycles must have an even number of $2$-cycles. In symbols, if   $$   \alpha = \beta_1 \beta_2 \dotsm \beta_r   \quad\text{and}\quad   \alpha = \gamma_1 \gamma_2 \dotsm \gamma_s $$   where the $\beta$'s and the $\gamma$'s are $2$-cycles, then $r$ and $s$ are both even or both odd. When I tried to reconstruct the proof myself, I found that it suffices to prove the following lemma: If $\epsilon=\beta_1\beta_2\cdots\beta_r$ where $\beta$'s are $2$-cycles, then $r$ is even. The original proof for this lemma uses the following key property of the product of $\beta_1\beta_2$: The product can always be expressed in one of the following forms on the left:   $$\begin{align}(ab)(ab)&=\epsilon\\ (ab)(ac)&=(bc)(ab)\\ (ab)(cd)&=(cd)(ab)\\ (ab)(bc)&=(bc)(ac)\end{align}$$ The proof for the lemma is based on such property and mathematical induction. I found it really hard to remind of such property, so I set it as an exercise to give another proof. However, I have no idea how to actually do it. So here is my question : Does anybody know an alternative proof of the lemma or the theorem?",,['abstract-algebra']
50,"What does ""homomorphism"" require that ""morphism"" doesn't?","What does ""homomorphism"" require that ""morphism"" doesn't?",,"I'm starting to learn category theory, but there's one thing I don't get: all morphisms seem to be homomorphisms; the definition seems to be the same. What's the difference between these two? Can you give me an example of a non-homomorphic morphism? Thank you for your patience,","I'm starting to learn category theory, but there's one thing I don't get: all morphisms seem to be homomorphisms; the definition seems to be the same. What's the difference between these two? Can you give me an example of a non-homomorphic morphism? Thank you for your patience,",,"['category-theory', 'definition', 'abstract-algebra']"
51,Is there a simple explanation why degree 5 polynomials (and up) are unsolvable?,Is there a simple explanation why degree 5 polynomials (and up) are unsolvable?,,"We can solve (get some kind of answer) equations like: $$ ax^2 + bx + c=0$$ $$ax^3 + bx^2 + cx + d=0$$ $$ax^4 + bx^3 + cx^2 + dx + e=0$$ But why is there no formula for an equation like $$ax^5 + bx^4 + cx^3 + dx^2 + ex + f=0$$ I'm not sure if this has anything to do with the Galois theory , but is there a dumbed-down simple explanation as to why degree 5 polynomials (and up) are unsolvable?","We can solve (get some kind of answer) equations like: $$ ax^2 + bx + c=0$$ $$ax^3 + bx^2 + cx + d=0$$ $$ax^4 + bx^3 + cx^2 + dx + e=0$$ But why is there no formula for an equation like $$ax^5 + bx^4 + cx^3 + dx^2 + ex + f=0$$ I'm not sure if this has anything to do with the Galois theory , but is there a dumbed-down simple explanation as to why degree 5 polynomials (and up) are unsolvable?",,"['abstract-algebra', 'polynomials', 'roots', 'factoring']"
52,Intuition on the Orbit-Stabilizer Theorem,Intuition on the Orbit-Stabilizer Theorem,,"The Orbit-Stabilizer says that, given a group $G$ which acts on a set $X$, then there exists a bijection between the orbit of an element $x\in X$ and the set of left cosets of the stabilizer group of $x$ in $G$. In other words, that the cardinality of the orbit of an element $x\in X$ is equal to the index of its stabilizer subgroup in $G$. I've seen two different texts present this, both of which explicitly say that this captures a very intuitive idea. I'm sorry if it's obvious, but I don't see the intuition behind this. I've asked a few questions looking for intuition now, and have received outstanding advice. As such, again I'm looking to the community to share some of their insights on this idea, and how they think of this theorem. As always, any help is greatly appreciated. Thanks!","The Orbit-Stabilizer says that, given a group $G$ which acts on a set $X$, then there exists a bijection between the orbit of an element $x\in X$ and the set of left cosets of the stabilizer group of $x$ in $G$. In other words, that the cardinality of the orbit of an element $x\in X$ is equal to the index of its stabilizer subgroup in $G$. I've seen two different texts present this, both of which explicitly say that this captures a very intuitive idea. I'm sorry if it's obvious, but I don't see the intuition behind this. I've asked a few questions looking for intuition now, and have received outstanding advice. As such, again I'm looking to the community to share some of their insights on this idea, and how they think of this theorem. As always, any help is greatly appreciated. Thanks!",,"['abstract-algebra', 'group-theory']"
53,Why do we look at morphisms?,Why do we look at morphisms?,,"I am reading some lecture notes and in one paragraph there is the following motivation: ""The best way to study spaces with a structure is usually to look at the maps between them preserving structure (linear maps, continuous maps differentiable maps). An important special case is usually the functions to the ground field."" Why is it a good idea to study a space with structure by looking at  maps that preserve this structure? It seems to me as if one achieves not much by going from one ""copy"" of a structured space to another copy.","I am reading some lecture notes and in one paragraph there is the following motivation: ""The best way to study spaces with a structure is usually to look at the maps between them preserving structure (linear maps, continuous maps differentiable maps). An important special case is usually the functions to the ground field."" Why is it a good idea to study a space with structure by looking at  maps that preserve this structure? It seems to me as if one achieves not much by going from one ""copy"" of a structured space to another copy.",,"['abstract-algebra', 'soft-question', 'category-theory', 'intuition']"
54,Is there active research in Galois Theory?,Is there active research in Galois Theory?,,"I recently decided to introduce myself to the field of Modern Algebra - in particular, Galois theory - and I found it absolutely beautiful! Thus I would really like to study something in Galois theory, which leads me to ask - do people still develop Galois theory? What else is there to learn in the subject? I am inspired by questions like these: What kind of work do modern day algebraists do? and What do modern-day analysts actually do? and would love to learn your opinions, stories, etc! Thanks in advance!","I recently decided to introduce myself to the field of Modern Algebra - in particular, Galois theory - and I found it absolutely beautiful! Thus I would really like to study something in Galois theory, which leads me to ask - do people still develop Galois theory? What else is there to learn in the subject? I am inspired by questions like these: What kind of work do modern day algebraists do? and What do modern-day analysts actually do? and would love to learn your opinions, stories, etc! Thanks in advance!",,"['abstract-algebra', 'soft-question', 'galois-theory']"
55,"Why are groups ""abelian"" but rings ""commutative""?","Why are groups ""abelian"" but rings ""commutative""?",,"I have never seen, in any text, a ring whose multiplication is commutative being called an ""abelian ring"", even though this would make perfect sense, because this term would necessarily refer to multiplication (addition is commutative by definition, of course). Is there some historical reason for that? Did Abel, maybe, only study ""additive"" structures?","I have never seen, in any text, a ring whose multiplication is commutative being called an ""abelian ring"", even though this would make perfect sense, because this term would necessarily refer to multiplication (addition is commutative by definition, of course). Is there some historical reason for that? Did Abel, maybe, only study ""additive"" structures?",,"['abstract-algebra', 'group-theory', 'ring-theory', 'soft-question', 'terminology']"
56,"Prove that the polynomial $(x-1)(x-2)\cdots(x-n) + 1$, $ n\ge1 $, $ n\ne4 $ is irreducible over $\mathbb Z$","Prove that the polynomial , ,  is irreducible over",(x-1)(x-2)\cdots(x-n) + 1  n\ge1   n\ne4  \mathbb Z,"I try to solve this problem. I seems to come close to the end but I can't get the conclusion. Can someone help me complete my proof. Thanks Show that the polynomial $h(x) = (x-1)(x-2)\cdots(x-n) + 1$ is irreducible over $\mathbb Z$ for all $n\ge1$ and $ n\ne4$. Suppose $h(x) = f(x) g(x)$, then we must have $f(i)g(i) = 1$ for all $i = 1,2,...n$. So both $f(i)$ and $g(i)$ are $1$ or $-1$. In either case, $m(x) = f(x) - g(x)$ has degree smaller than $n$ and have $n$ different roots ($1,2,...,n$). So we must have $m(x) = 0$. Then $h(x) = f(x)^{2}$. So $n$ must be even. Let $n = 2k$. Because $f(x)$ has degree $k$, there are $k$ values from $\{1,2,...,2k\}$ at which $f(x)$ is $1$ and $k$ values at which $f(x)$ is $-1$. This is where I got stuck. Hope some one can help me solve this. Thanks.","I try to solve this problem. I seems to come close to the end but I can't get the conclusion. Can someone help me complete my proof. Thanks Show that the polynomial $h(x) = (x-1)(x-2)\cdots(x-n) + 1$ is irreducible over $\mathbb Z$ for all $n\ge1$ and $ n\ne4$. Suppose $h(x) = f(x) g(x)$, then we must have $f(i)g(i) = 1$ for all $i = 1,2,...n$. So both $f(i)$ and $g(i)$ are $1$ or $-1$. In either case, $m(x) = f(x) - g(x)$ has degree smaller than $n$ and have $n$ different roots ($1,2,...,n$). So we must have $m(x) = 0$. Then $h(x) = f(x)^{2}$. So $n$ must be even. Let $n = 2k$. Because $f(x)$ has degree $k$, there are $k$ values from $\{1,2,...,2k\}$ at which $f(x)$ is $1$ and $k$ values at which $f(x)$ is $-1$. This is where I got stuck. Hope some one can help me solve this. Thanks.",,"['abstract-algebra', 'elementary-number-theory', 'polynomials', 'contest-math', 'irreducible-polynomials']"
57,"Can you make the triviality of $\langle a,b,c \mid aba^{-1} = b^2, bcb^{-1} = c^2, cac^{-1} = a^2 \rangle$ more trivial?",Can you make the triviality of  more trivial?,"\langle a,b,c \mid aba^{-1} = b^2, bcb^{-1} = c^2, cac^{-1} = a^2 \rangle","I recently learned the following pleasant fact. (It was in the proof of Proposition 3.1 of this paper - but don't worry, there's no model theory in this question.) Let $G$ be a group, and let $a,b,c\in G$ . If $aba^{-1} = b^2$ , $bcb^{-1} = c^2$ , and $cac^{-1} = a^2$ , then $a = b = c = e$ . Put another way, the group defined by generators and relations $\langle a,b,c \mid aba^{-1} = b^2, bcb^{-1} = c^2, cac^{-1} = a^2 \rangle$ is the trivial group. I came up with the following elementary, but ugly, proof: The relations can be rewritten as (1) $ab = b^2a$ , (2) $bc = c^2b$ , (3) $ca = a^2c$ . Using (1), (2), and (3), we can rewrite $a^4bc = a^4c^2b = c^2ab = c^2b^2a$ . But we can also rewrite $a^4bc = b^{16}a^4c = b^{16}ca^2 = c^{2^{16}}b^{16}a^2$ . So $c^{2^{16}}b^{16}a^2 = c^2b^2a$ . This implies $a = b^{-16}c^{2-2^{16}}b^2$ . Substituting for $a$ in (1) above, $b^{-16}c^{2(1-2^{15})}b^3 = b^{-14}c^{2(1-2^{15})}b^2$ , and cancelling from both sides, $c^{2(1-2^{15})}b = b^{2}c^{2(1-2^{15})}$ . But now by (2), we have $bc^{1-2^{15}} = b^{2}c^{2(1-2^{15})}$ , and $b = c^{2^{15}-1}$ . But then $b$ and $c$ commute, so $bcb^{-1} = c^2$ implies $c = c^2$ , and $c = e$ . It then follows easily that $a = b = c = e$ . Question: Is there a better way to see this? i.e. a more abstract proof, or at least one that doesn't involve manipulating words of length $2^{16}$ ?","I recently learned the following pleasant fact. (It was in the proof of Proposition 3.1 of this paper - but don't worry, there's no model theory in this question.) Let be a group, and let . If , , and , then . Put another way, the group defined by generators and relations is the trivial group. I came up with the following elementary, but ugly, proof: The relations can be rewritten as (1) , (2) , (3) . Using (1), (2), and (3), we can rewrite . But we can also rewrite . So . This implies . Substituting for in (1) above, , and cancelling from both sides, . But now by (2), we have , and . But then and commute, so implies , and . It then follows easily that . Question: Is there a better way to see this? i.e. a more abstract proof, or at least one that doesn't involve manipulating words of length ?","G a,b,c\in G aba^{-1} = b^2 bcb^{-1} = c^2 cac^{-1} = a^2 a = b = c = e \langle a,b,c \mid aba^{-1} = b^2, bcb^{-1} = c^2, cac^{-1} = a^2 \rangle ab = b^2a bc = c^2b ca = a^2c a^4bc = a^4c^2b = c^2ab = c^2b^2a a^4bc = b^{16}a^4c = b^{16}ca^2 = c^{2^{16}}b^{16}a^2 c^{2^{16}}b^{16}a^2 = c^2b^2a a = b^{-16}c^{2-2^{16}}b^2 a b^{-16}c^{2(1-2^{15})}b^3 = b^{-14}c^{2(1-2^{15})}b^2 c^{2(1-2^{15})}b = b^{2}c^{2(1-2^{15})} bc^{1-2^{15}} = b^{2}c^{2(1-2^{15})} b = c^{2^{15}-1} b c bcb^{-1} = c^2 c = c^2 c = e a = b = c = e 2^{16}","['abstract-algebra', 'group-theory', 'alternative-proof', 'group-presentation']"
58,Examples of prime ideals that are not maximal,Examples of prime ideals that are not maximal,,I would like to know of some examples of a prime ideal that is not maximal in some commutative ring with unity.,I would like to know of some examples of a prime ideal that is not maximal in some commutative ring with unity.,,"['abstract-algebra', 'ring-theory', 'examples-counterexamples', 'maximal-and-prime-ideals']"
59,A group $G$ with a subgroup $H$ of index $n$ has a normal subgroup $K\subset H$ whose index in $G$ divides $n!$,A group  with a subgroup  of index  has a normal subgroup  whose index in  divides,G H n K\subset H G n!,"I would be very thankful if someone could give me a hint with proving this. It's a very common exercise in abstract algebra textbooks. If $G$ is a group with a subgroup $H$ of finite index $n$, then $G$ has a normal subgroup $K$ contained in $H$ whose index in $G$ is finite and divides $n!$. I found the proof on this Wikipedia page at some point (although the proof appears to be no longer there), but I got lost in one of the details.","I would be very thankful if someone could give me a hint with proving this. It's a very common exercise in abstract algebra textbooks. If $G$ is a group with a subgroup $H$ of finite index $n$, then $G$ has a normal subgroup $K$ contained in $H$ whose index in $G$ is finite and divides $n!$. I found the proof on this Wikipedia page at some point (although the proof appears to be no longer there), but I got lost in one of the details.",,"['abstract-algebra', 'group-theory', 'normal-subgroups']"
60,Automorphism of the Field of rational functions,Automorphism of the Field of rational functions,,"Let $K$ be a field and let $K(x)$ be the field of rational functions in $x$ whose coefficients are in $K$. Let $\theta(x)$ $\in \operatorname{Aut}(K(x))$ such that $\theta|_K = \operatorname{id}_K$.  Show that $\theta(x) =\frac{ax+b}{cx+d}$, with $ad\neq bc$. Here is my attempt. Let $\theta(x) = \frac{f}{g}$, $f,g \in K[x]$, with $\gcd(f, g)=1$. let $h \in K(x)$. Then  $h(\frac{f}{g}) = x$ and $\frac{f(x)}{g(x)}\neq\frac{f(y)}{g(y)}$ if $x\neq y$ . Suppose $\deg f \gt 1$ or $\deg g \gt 1$, then the equation $g(x)b=f(x)$, $b\in K$,  will have more than one solution for $x$. Hence $f$ and $g$ have degrees at most $1$ and thus $\theta(x)=\frac{ax+b}{cx+d}$ with inverse function $\theta(x) =\frac{dx-b}{a-cx}$. What I'd like to know is if Ive approached the problem in the right manner. Please any input will be very much appreciated. Thanks.","Let $K$ be a field and let $K(x)$ be the field of rational functions in $x$ whose coefficients are in $K$. Let $\theta(x)$ $\in \operatorname{Aut}(K(x))$ such that $\theta|_K = \operatorname{id}_K$.  Show that $\theta(x) =\frac{ax+b}{cx+d}$, with $ad\neq bc$. Here is my attempt. Let $\theta(x) = \frac{f}{g}$, $f,g \in K[x]$, with $\gcd(f, g)=1$. let $h \in K(x)$. Then  $h(\frac{f}{g}) = x$ and $\frac{f(x)}{g(x)}\neq\frac{f(y)}{g(y)}$ if $x\neq y$ . Suppose $\deg f \gt 1$ or $\deg g \gt 1$, then the equation $g(x)b=f(x)$, $b\in K$,  will have more than one solution for $x$. Hence $f$ and $g$ have degrees at most $1$ and thus $\theta(x)=\frac{ax+b}{cx+d}$ with inverse function $\theta(x) =\frac{dx-b}{a-cx}$. What I'd like to know is if Ive approached the problem in the right manner. Please any input will be very much appreciated. Thanks.",,['abstract-algebra']
61,Is War necessarily finite?,Is War necessarily finite?,,"War is an cardgame played by children and drunk college students which involves no strategic choices on either side.  The outcome is determined by the dealing of the cards.  These are the rules. A standard $52$ card deck is shuffled and dealt evenly to two players face down.  Every turn, both players lay down their top cards (a ""battle"").  The higher card wins, and the winning player gets both cards and places them at the bottom of his deck.  If the battle is a tie, a ""war"" is declared, and each player lays down his next three cards.  Of these six cards, the owner of the card with the highest value wins the turn, and all cards played that turn are placed at the bottom of his deck.  If a player does not have $3$ cards to play in a war, he wins the turn by default.  If the war is a tie, another war is held (""double war""), and so on until one of the players wins.  The game ends when a player runs out of cards. If the case occurs that the deals are entirely symmetric, e.g. both players decks are two two's, two threes, two fours, etc. in that order, we say that both players lose, as does everyone watching.  (In other words, the game ""fails."")  This would be considered a finite game. Ordering Conventions. To consider the game from a mathematical standpoint, the order that cards are placed into a player's hand after he wins a turn must be well-defined.  There are many choices to be made here, especially considering that we can vary the conventions for double wars, triple wars, etc., which is where the most potential for variation lies.  For the sake of simplicity, and because it coincides with how I pick the cards up when I am actually playing, I suggest that when a player wins a turn, he picks up all of the losing cards in the order that they were laid down (highest to lowest), followed all of the winning cards. For example, say a turn begins with a battle in which players $\sf X$ and $\sf Y$ both lay down aces.  Thus a war begins.  Say that player $\sf X$ lays down a Jack, a King, and a $3$ , in that order, and player $\sf Y$ lays down a Queen, a $4$ , and a King, in that order.  So a double war is held.  Player $\sf X$ lays down $5$ , $6$ , $7$ and player $\sf Y$ lays down $8$ , $9$ , $10$ .  Now, player $\sf Y$ wins, so all of these cards are placed in his deck, ordered as such: $$\sf (\ldots\text{previous deck}\ldots, \underbrace{\overbrace{\text{A},}^{\text{battle}}\overbrace{\text{J},\text{K},3,}^{\text{first war}}\overbrace{5,6,7,}^{\text{second war}}}_{\text{losing cards}}\underbrace{\overbrace{\text{A},}^{\text{battle}}\overbrace{\text{Q},4,\text{K},}^{\text{first war}}\overbrace{8,9,10}^{\text{second war}}}_{\text{winning cards}}).$$ A possible variation would be to pick up the cards in each war separately, in the order the wars were played, in which case the above example would yield $$\sf (\ldots\text{previous deck}\ldots,\overbrace{\underbrace{A,}_{\text{losing}}\underbrace{A,}_{\text{winning}}}^{battle}\overbrace{\underbrace{J,K,3,}_{\text{losing}}\underbrace{Q,4,K,}_{\text{winning}}}^{\text{first war}}\overbrace{\underbrace{5,6,7,}_{\text{losing}}\underbrace{8,9,10}_{\text{winning}}}^{\text{second war}}).$$ Questions. First and foremost, Is War necessarily finite? Perhaps there exists some arrangement of starting decks which would produce an everlasting game, forever permuting cards between the two players.  If this is not possible, can we prove it?  Which ordering conventions always give rise to finite games (if any exist)? and, for bonus points, There are $\rm {52 \choose 26}(26!)^2\approx 10^{41}$ possible ways to deal the decks at the beginning of the game.  Since suits are irrelevant, however, some arrangements will result in identical games. What is the expression for the number non-identical games? Thanks joriki. A (substantially) tougher question: if we say that two games are similar if every turn has the same outcome, what is the expression for the number of non-similar games?","War is an cardgame played by children and drunk college students which involves no strategic choices on either side.  The outcome is determined by the dealing of the cards.  These are the rules. A standard card deck is shuffled and dealt evenly to two players face down.  Every turn, both players lay down their top cards (a ""battle"").  The higher card wins, and the winning player gets both cards and places them at the bottom of his deck.  If the battle is a tie, a ""war"" is declared, and each player lays down his next three cards.  Of these six cards, the owner of the card with the highest value wins the turn, and all cards played that turn are placed at the bottom of his deck.  If a player does not have cards to play in a war, he wins the turn by default.  If the war is a tie, another war is held (""double war""), and so on until one of the players wins.  The game ends when a player runs out of cards. If the case occurs that the deals are entirely symmetric, e.g. both players decks are two two's, two threes, two fours, etc. in that order, we say that both players lose, as does everyone watching.  (In other words, the game ""fails."")  This would be considered a finite game. Ordering Conventions. To consider the game from a mathematical standpoint, the order that cards are placed into a player's hand after he wins a turn must be well-defined.  There are many choices to be made here, especially considering that we can vary the conventions for double wars, triple wars, etc., which is where the most potential for variation lies.  For the sake of simplicity, and because it coincides with how I pick the cards up when I am actually playing, I suggest that when a player wins a turn, he picks up all of the losing cards in the order that they were laid down (highest to lowest), followed all of the winning cards. For example, say a turn begins with a battle in which players and both lay down aces.  Thus a war begins.  Say that player lays down a Jack, a King, and a , in that order, and player lays down a Queen, a , and a King, in that order.  So a double war is held.  Player lays down , , and player lays down , , .  Now, player wins, so all of these cards are placed in his deck, ordered as such: A possible variation would be to pick up the cards in each war separately, in the order the wars were played, in which case the above example would yield Questions. First and foremost, Is War necessarily finite? Perhaps there exists some arrangement of starting decks which would produce an everlasting game, forever permuting cards between the two players.  If this is not possible, can we prove it?  Which ordering conventions always give rise to finite games (if any exist)? and, for bonus points, There are possible ways to deal the decks at the beginning of the game.  Since suits are irrelevant, however, some arrangements will result in identical games. What is the expression for the number non-identical games? Thanks joriki. A (substantially) tougher question: if we say that two games are similar if every turn has the same outcome, what is the expression for the number of non-similar games?","52 3 \sf X \sf Y \sf X 3 \sf Y 4 \sf X 5 6 7 \sf Y 8 9 10 \sf Y \sf (\ldots\text{previous deck}\ldots, \underbrace{\overbrace{\text{A},}^{\text{battle}}\overbrace{\text{J},\text{K},3,}^{\text{first war}}\overbrace{5,6,7,}^{\text{second war}}}_{\text{losing cards}}\underbrace{\overbrace{\text{A},}^{\text{battle}}\overbrace{\text{Q},4,\text{K},}^{\text{first war}}\overbrace{8,9,10}^{\text{second war}}}_{\text{winning cards}}). \sf (\ldots\text{previous deck}\ldots,\overbrace{\underbrace{A,}_{\text{losing}}\underbrace{A,}_{\text{winning}}}^{battle}\overbrace{\underbrace{J,K,3,}_{\text{losing}}\underbrace{Q,4,K,}_{\text{winning}}}^{\text{first war}}\overbrace{\underbrace{5,6,7,}_{\text{losing}}\underbrace{8,9,10}_{\text{winning}}}^{\text{second war}}). \rm {52 \choose 26}(26!)^2\approx 10^{41}","['abstract-algebra', 'combinatorics', 'algorithms', 'permutations', 'game-theory']"
62,Is there an idempotent element in a finite semigroup?,Is there an idempotent element in a finite semigroup?,,"Let $(G,\cdot)$ be a non-empty finite semigroup . Is there any $a\in G$ such that: $$a^2=a$$ It seems to be true in view of theorem 2.2.1 page 97 of this book (I'm not sure).  But is there an elementary proof? Theorem 2.2.1. [R. Ellis] Let $S$ be a compact right topological semigroup. Then there exists an idempotent in it. This theorem is also known as Ellis–Numakura lemma .",Let be a non-empty finite semigroup . Is there any such that: It seems to be true in view of theorem 2.2.1 page 97 of this book (I'm not sure).  But is there an elementary proof? Theorem 2.2.1. [R. Ellis] Let be a compact right topological semigroup. Then there exists an idempotent in it. This theorem is also known as Ellis–Numakura lemma .,"(G,\cdot) a\in G a^2=a S","['abstract-algebra', 'alternative-proof']"
63,"I don't understand what a ""free group"" is!","I don't understand what a ""free group"" is!",,"My lecture note glosses over it really, introduces it and says ""well it intuitively makes sense"" but I say, nope it doesn't. Free groups on generators $x_1,...,x_m,x_1^{-1},...,x_m^{-1}$ is a group whose elements are words in the symbols $x_1,...,x_m,x_1^{-1},...,x_m^{-1}$ subject to the group axioms. The group operation is concatenation. What do I not understand? Well, to star with, where's the identity? The operation, say I denote it $*$, is $x_1 * x_2=x_1x_2$ yes? How is the identity defined? I mean, $e*x_1=ex_1$ because it's ""concatenation""  so I cannot conveniently say $e*x_1=x_1$ and ignore the fact I need to ""concatenate"" it. These are apparently words, symbols not numbers. The inverse doesn't make sense too, $x_1*x_1^{-1}=x_1x_1^{-1}$ and period. Not $x_1*x_1^{-1}=e$. I mean, I don't even know what $e$ is supposed to be in this supposedly group object so I am left puzzled. I don't see any mathematics here, concatenation, in other words, is just ""lining up the symbols in order."" It's not like $1 \times 2 \times 10=20$ but $1 \times 2 \times 20=1220$. And another problem. Doesn't the free group have order infinity? It can't be finite can it? Because, say I start with $x_1,...,x_m,x_1^{-1},...,x_m^{-1}$ but it must be closed under concatenation. Well, $x_1*x_2=x_1x_2$ already causes an issue because clearly we just created a new element. A new word $x_1x_2$. Continuing this way, we keep adding the newly created words and reach infinity. And before someone directs me to it, no, wikipedia's page on free groups didn't help me understand this either. This bizarre notion is confusing and incomprehensible than ever. Does anyone know the answers to my questions?","My lecture note glosses over it really, introduces it and says ""well it intuitively makes sense"" but I say, nope it doesn't. Free groups on generators $x_1,...,x_m,x_1^{-1},...,x_m^{-1}$ is a group whose elements are words in the symbols $x_1,...,x_m,x_1^{-1},...,x_m^{-1}$ subject to the group axioms. The group operation is concatenation. What do I not understand? Well, to star with, where's the identity? The operation, say I denote it $*$, is $x_1 * x_2=x_1x_2$ yes? How is the identity defined? I mean, $e*x_1=ex_1$ because it's ""concatenation""  so I cannot conveniently say $e*x_1=x_1$ and ignore the fact I need to ""concatenate"" it. These are apparently words, symbols not numbers. The inverse doesn't make sense too, $x_1*x_1^{-1}=x_1x_1^{-1}$ and period. Not $x_1*x_1^{-1}=e$. I mean, I don't even know what $e$ is supposed to be in this supposedly group object so I am left puzzled. I don't see any mathematics here, concatenation, in other words, is just ""lining up the symbols in order."" It's not like $1 \times 2 \times 10=20$ but $1 \times 2 \times 20=1220$. And another problem. Doesn't the free group have order infinity? It can't be finite can it? Because, say I start with $x_1,...,x_m,x_1^{-1},...,x_m^{-1}$ but it must be closed under concatenation. Well, $x_1*x_2=x_1x_2$ already causes an issue because clearly we just created a new element. A new word $x_1x_2$. Continuing this way, we keep adding the newly created words and reach infinity. And before someone directs me to it, no, wikipedia's page on free groups didn't help me understand this either. This bizarre notion is confusing and incomprehensible than ever. Does anyone know the answers to my questions?",,"['abstract-algebra', 'group-theory', 'free-groups']"
64,Why should I care about fields of positive characteristic?,Why should I care about fields of positive characteristic?,,"This is what I know about why someone might care about fields of positive characteristic: they are useful for number theory in algebraic geometry, a theory of ""geometry"" can be developed over them, and it's fun to see how this geometry works out Some people might read this and think, ""What more could you need?"" But I've never been able to make myself care about number theory, so (1) doesn't help me. (2) is nice for what it is, but I'm hoping there's something more. My understanding of (2) is that this is only geometry in a rather abstract sense and, for instance, there's no generally useful way to directly visually represent these fields or varieties over them the way we can over the reals or complex numbers. (Drawing a curve in R^2 and saying it's the curve over some other field may be helpful for some purposes, but it's not what I'm after here.) Is there anything else? The ideal (surely impossible) answer for me would be ""Yes, such fields are very good models for these common and easy to understand physical systems: A, B, C. Also, we can visualize them and varieties over them quite easily by method D. Finally, here's a bunch of surprising and helpful applications to 500 other areas of mathematics."" UPDATE: to answer Qiaochu's comment about what I do care about. Let's say I care about: algebraic & geometric topology differential geometry & topology applications to physics and I certainly care about algebraic geometry over C (this is to say I understand the motivations behind these subjects and the general idea, not necessarily that I know them in depth)","This is what I know about why someone might care about fields of positive characteristic: they are useful for number theory in algebraic geometry, a theory of ""geometry"" can be developed over them, and it's fun to see how this geometry works out Some people might read this and think, ""What more could you need?"" But I've never been able to make myself care about number theory, so (1) doesn't help me. (2) is nice for what it is, but I'm hoping there's something more. My understanding of (2) is that this is only geometry in a rather abstract sense and, for instance, there's no generally useful way to directly visually represent these fields or varieties over them the way we can over the reals or complex numbers. (Drawing a curve in R^2 and saying it's the curve over some other field may be helpful for some purposes, but it's not what I'm after here.) Is there anything else? The ideal (surely impossible) answer for me would be ""Yes, such fields are very good models for these common and easy to understand physical systems: A, B, C. Also, we can visualize them and varieties over them quite easily by method D. Finally, here's a bunch of surprising and helpful applications to 500 other areas of mathematics."" UPDATE: to answer Qiaochu's comment about what I do care about. Let's say I care about: algebraic & geometric topology differential geometry & topology applications to physics and I certainly care about algebraic geometry over C (this is to say I understand the motivations behind these subjects and the general idea, not necessarily that I know them in depth)",,"['abstract-algebra', 'field-theory']"
65,Product of two cyclic groups is cyclic iff their orders are co-prime,Product of two cyclic groups is cyclic iff their orders are co-prime,,"Say you have two groups $G = \langle g \rangle$ with order $n$ and $H = \langle h \rangle$ with order $m$. Then the product $G \times H$ is a cyclic group if and only if $\gcd(n,m)=1$. I can't seem to figure out how to start proving this. I have tried with some examples, where I pick $(g,h)$ as a candidate generator of $G \times H$. I see that what we want is for the cycles of $g$ and $h$, as we take powers of $(g,h)$, to interleave such that we do not get $(1,1)$ until the $(mn)$-th power. However, I am having a hard time formalizing this and relating it to the greatest common divisor. Any hints are much appreciated!","Say you have two groups $G = \langle g \rangle$ with order $n$ and $H = \langle h \rangle$ with order $m$. Then the product $G \times H$ is a cyclic group if and only if $\gcd(n,m)=1$. I can't seem to figure out how to start proving this. I have tried with some examples, where I pick $(g,h)$ as a candidate generator of $G \times H$. I see that what we want is for the cycles of $g$ and $h$, as we take powers of $(g,h)$, to interleave such that we do not get $(1,1)$ until the $(mn)$-th power. However, I am having a hard time formalizing this and relating it to the greatest common divisor. Any hints are much appreciated!",,"['abstract-algebra', 'group-theory', 'elementary-number-theory', 'cyclic-groups', 'direct-product']"
66,Why $\sqrt[3]{3}\not\in \mathbb{Q}(\sqrt[3]{2})$?,Why ?,\sqrt[3]{3}\not\in \mathbb{Q}(\sqrt[3]{2}),"We all know that $\sqrt[3]{3}\not\in \mathbb{Q}({\sqrt[3]{2}})$ intuitively. We even know that $\sqrt[3]{p}\not\in \mathbb{Q}(\sqrt[3]{q})$ for two distinct primes $p, q$. However, I don't know how to prove these things rigorously. In case of $\sqrt{p}\not\in \mathbb{Q}(\sqrt{q})$, we can just set assume $\sqrt{p}=a+b\sqrt{q}$ for some $a, b\in\mathbb{Q}$ and deduce a contradiction. However, in cubic case, expansion of $(a+b\sqrt[3]{2}+c\sqrt[3]{4})^{3}$ is very complicated and even I expand this I couldn't get any contradiction by this.  To be specific, it gives us 3 diophantine equations $$a^{3}+2b^{3}+4c^{3}+12abc=3$$ $$a^{2}b+2b^{2}c+2c^{2}a=0$$ $$ab^{2}+2bc^{2}+ca^{2}=0$$ and I don't know how to check solvability of this kind of diophantine equations (over $\mathbb{Q}$). Even if there exists a way to check solvability, I cannot convince that it can be used to prove $\sqrt[3]{p}\not\in \mathbb{Q}(\sqrt[3]{q})$. So I tried to use Galois theory, but actually I don't know where to go. I assumed $\mathbb{Q}(\sqrt[3]{3})=\mathbb{Q}(\sqrt[3]{2})$ and take a Galois closure, i.e. $K=\mathbb{Q}(\sqrt[3]{3}, w)=\mathbb{Q}(\sqrt[3]{2}, w)$ where $w^{3}=1$ then tried to use field norm maps. We can check that $\beta=\sqrt[3]{3}\cdot\sigma(\sqrt[3]{3})\cdot\sigma^{2}(\sqrt[3]{3})\in \mathbb{Q}$ when $\sigma\in Gal(K/\mathbb{Q}), \sigma(\sqrt[3]{2})=\sqrt[3]{2}, \sigma(w)=w^{2}$. But I cannot get any information from this. Edit : It would be great if there is a simple algorithm to check whether $\alpha\in K$ or not for a given algebraic number $\alpha$ and a number field $K$.","We all know that $\sqrt[3]{3}\not\in \mathbb{Q}({\sqrt[3]{2}})$ intuitively. We even know that $\sqrt[3]{p}\not\in \mathbb{Q}(\sqrt[3]{q})$ for two distinct primes $p, q$. However, I don't know how to prove these things rigorously. In case of $\sqrt{p}\not\in \mathbb{Q}(\sqrt{q})$, we can just set assume $\sqrt{p}=a+b\sqrt{q}$ for some $a, b\in\mathbb{Q}$ and deduce a contradiction. However, in cubic case, expansion of $(a+b\sqrt[3]{2}+c\sqrt[3]{4})^{3}$ is very complicated and even I expand this I couldn't get any contradiction by this.  To be specific, it gives us 3 diophantine equations $$a^{3}+2b^{3}+4c^{3}+12abc=3$$ $$a^{2}b+2b^{2}c+2c^{2}a=0$$ $$ab^{2}+2bc^{2}+ca^{2}=0$$ and I don't know how to check solvability of this kind of diophantine equations (over $\mathbb{Q}$). Even if there exists a way to check solvability, I cannot convince that it can be used to prove $\sqrt[3]{p}\not\in \mathbb{Q}(\sqrt[3]{q})$. So I tried to use Galois theory, but actually I don't know where to go. I assumed $\mathbb{Q}(\sqrt[3]{3})=\mathbb{Q}(\sqrt[3]{2})$ and take a Galois closure, i.e. $K=\mathbb{Q}(\sqrt[3]{3}, w)=\mathbb{Q}(\sqrt[3]{2}, w)$ where $w^{3}=1$ then tried to use field norm maps. We can check that $\beta=\sqrt[3]{3}\cdot\sigma(\sqrt[3]{3})\cdot\sigma^{2}(\sqrt[3]{3})\in \mathbb{Q}$ when $\sigma\in Gal(K/\mathbb{Q}), \sigma(\sqrt[3]{2})=\sqrt[3]{2}, \sigma(w)=w^{2}$. But I cannot get any information from this. Edit : It would be great if there is a simple algorithm to check whether $\alpha\in K$ or not for a given algebraic number $\alpha$ and a number field $K$.",,"['abstract-algebra', 'field-theory', 'galois-theory']"
67,Why don't we study algebraic objects with more than two operations?,Why don't we study algebraic objects with more than two operations?,,"Undergraduates learn about algebraic objects with one operation, namely groups, and we learn about algebraic objects with two ""compatible"" operations, namely rings and fields. It seems natural to then look at algebraic objects with three or more operations that are compatible, but we don't learn about them. I asked one of my professors why this is so and she posed this question in response: we have left-modules and right-modules, but there are no top-modules or bottom-modules, or any other ways of combining two elements to produce a third. I can't think of any satisfactory answer to either of these questions. Can anyone shed any light on them? Edit: Now that I know these objects are studied, what I meant by ""we"" is essentially ""Why are these objects not introduced to undergraduates (at least in a standard curriculum) given how natural they seem?""","Undergraduates learn about algebraic objects with one operation, namely groups, and we learn about algebraic objects with two ""compatible"" operations, namely rings and fields. It seems natural to then look at algebraic objects with three or more operations that are compatible, but we don't learn about them. I asked one of my professors why this is so and she posed this question in response: we have left-modules and right-modules, but there are no top-modules or bottom-modules, or any other ways of combining two elements to produce a third. I can't think of any satisfactory answer to either of these questions. Can anyone shed any light on them? Edit: Now that I know these objects are studied, what I meant by ""we"" is essentially ""Why are these objects not introduced to undergraduates (at least in a standard curriculum) given how natural they seem?""",,['abstract-algebra']
68,Chinese Remainder theorem with non-pairwise coprime moduli,Chinese Remainder theorem with non-pairwise coprime moduli,,"Let $n_1,...,n_k \in \mathbb{N}$ and let $a_1,...,a_k \in \mathbb{Z}$.  How to prove the following version of the Chinese remainder theorem ( see here ): There exists a $x \in \mathbb{Z}$ satisfying system of equations: $$x=a_1 \pmod {n_1}$$ $$x=a_2  \pmod {n_2}$$ $$\ldots$$ $$x=a_k \pmod{n_k}$$ if and only if $a_i=a_j \pmod{\gcd(n_i,n_j)}$ for all $i,j=1,...,k$? If numbers $n_i$, for $i=1,...,k$, are pairwise coprime, it is a classical version of Chinese remainder theorem. Thanks.","Let $n_1,...,n_k \in \mathbb{N}$ and let $a_1,...,a_k \in \mathbb{Z}$.  How to prove the following version of the Chinese remainder theorem ( see here ): There exists a $x \in \mathbb{Z}$ satisfying system of equations: $$x=a_1 \pmod {n_1}$$ $$x=a_2  \pmod {n_2}$$ $$\ldots$$ $$x=a_k \pmod{n_k}$$ if and only if $a_i=a_j \pmod{\gcd(n_i,n_j)}$ for all $i,j=1,...,k$? If numbers $n_i$, for $i=1,...,k$, are pairwise coprime, it is a classical version of Chinese remainder theorem. Thanks.",,"['abstract-algebra', 'number-theory', 'arithmetic']"
69,"How few $(42^\circ,60^\circ,78^\circ)$ triangles can an equilateral triangle be divided into?",How few  triangles can an equilateral triangle be divided into?,"(42^\circ,60^\circ,78^\circ)","This is the parallel question to this other post with many answers already , in the sense that the $(42^\circ,60^\circ,78^\circ)$ -similar triangles form the only non-trivial rational-angle tiling of the equilateral triangle (and the regular hexagon), modulo a real conjugation of the coordinate field (a subfield of $\mathbf{Q}(\zeta_{60})$ ) which transforms between $(42^\circ,60^\circ,78^\circ)$ -similar triangles and $(6^\circ,60^\circ,114^\circ)$ -similar triangles. (Reference: M. Laczkovich's Tilings of triangles .) My attempt has been the following: Since $\sin(42^\circ)$ and $\sin(78^\circ)$ have nested radicals, I tried to get rid of them by restricting my basic tiling units to only the $60^\circ$ -angled isosceles trapezoids and parallelograms that are a single row of the triangular tiles. They have shorter-base-to-leg ratios of the form $$m\cdot\frac{9-3\sqrt{5}}{2}+n\cdot\frac{11-3\sqrt{5}}{2}\quad\left(m,n\ge 0\right)$$ which are automatically algebraic integers. Any potential tiling of the equilateral triangle from these quadrilateral units corresponds to some integer polynomial relation of the above algebraics, whose polynomial degree correlates with the number of quadrilateral pieces in the tiling. Unfortunately all the above algebraics have large norms, so a blind search for the desired polynomial is out of the question, and I had to reduce the pieces' proportions again to the rationals. I was able to find a $60^\circ$ -angled isosceles trapezoid with shorter-base-to-leg ratio of $10$ using $79$ tiles, and a $60^\circ$ -angled parallelogram with neighboring sides' ratio of $11$ using $80$ tiles. Thus a few more tiles produce a $60^\circ$ -angled rhombus, and another few more tiles produce a $60^\circ$ -angled isosceles trapezoid with shorter-base-to-leg ratio $1$ , three of which tile an equilateral triangle, using a total of $121\,170$ triangular tiles. While I was at it, I found this less related post that might reduce my number of tiles to a bit below a hundred thousand. Meanwhile, I also did a quick computer search through some conceptually simple configurations that attempt to tile the equilateral triangle using less than about $50$ tiles, and I found nothing at all. I get this feeling that about a hundred thousand tiles is not the optimal amount for such a tiling, so I'm asking to see if people have better ideas. I'm unable to provide cash incentive as the parallel post did, but anyone who tries this puzzle will sure have my gratitude. Edit suggested by RavenclawPrefect: To get to the quadrilateral tiling units that I used, the first thing is to de-nest the radicals as I mentioned above. As $\mathbf{Q}(\zeta_{60})$ is Galois over $\mathbf{Q}(\sqrt{3})$ (the base field here should not be $\mathbf{Q}$ but instead the coordinate field of the equilateral triangle), if we can geometrically construct any length $\ell$ (or techinically, ratio $\ell$ ), such that when we perform the same geometric construction but with all the $42^\circ$ angles and $78^\circ$ angles swapped with each other, we still provably construct the same $\ell$ , then it must hold that $\ell\in\mathbf{Q}(\sqrt{5})$ , so that $\ell$ doesn't contain any nested radicals. There were a couple of ideas on what $\ell$ should specifically be, most of them parallel ideas that can all be found in the parallel question for the square. I settled on the above $\mathbf{Q}(\sqrt{5})$ -quadrilaterals (the ones that are a single row of triangular tiles) because they had the smallest numerator norms among others. As a non-example, there was a double-decker idea using $9$ tiles that resulted in a trapezoid with ratio a rational multiple of $889-321\sqrt{5}$ , yuck. There were also some non-triviality in which way the triangles should be oriented when being put into a single row, but some more calculation showed that the above $(m,n)$ form are all we really get. More precisely, a trapezoid also can't have $m=0$ , and a parallelogram also can't have $n=0$ . After all that work, the rest has really been a matter of trial-and-error. Among all the $(m,n)$ form, I picked a parallelogram with the smallest norm, which is an $(m,n)=(0,1)$ parallelogram with $4$ tiles, and rotated it so that it becomes a $\frac{11+3\sqrt{5}}{38}$ -parallelogram. Then $19$ of those make a $\frac{11+3\sqrt{5}}{2}$ -parallelogram with $76$ tiles, and obviously I combined it to a $(1,0)$ -trapezoid and a $(0,1)$ -parallelogram to get to the rational quadrilaterals. So the process was more like ""I frankly don't know what else to do"" rather than ""I see potential simplifications but I don't know the optimum"". It's also why I'm seeking for completely new ideas (see above) that aren't found in the parallel question about the square. RavenclawPrefect also asked a well-motivated question for if the same tiling could be performed but with congruent tiles. M. Laczkovich proved this is impossible in a subsequent paper Tilings of Convex Polygons with Congruent Triangles .","This is the parallel question to this other post with many answers already , in the sense that the -similar triangles form the only non-trivial rational-angle tiling of the equilateral triangle (and the regular hexagon), modulo a real conjugation of the coordinate field (a subfield of ) which transforms between -similar triangles and -similar triangles. (Reference: M. Laczkovich's Tilings of triangles .) My attempt has been the following: Since and have nested radicals, I tried to get rid of them by restricting my basic tiling units to only the -angled isosceles trapezoids and parallelograms that are a single row of the triangular tiles. They have shorter-base-to-leg ratios of the form which are automatically algebraic integers. Any potential tiling of the equilateral triangle from these quadrilateral units corresponds to some integer polynomial relation of the above algebraics, whose polynomial degree correlates with the number of quadrilateral pieces in the tiling. Unfortunately all the above algebraics have large norms, so a blind search for the desired polynomial is out of the question, and I had to reduce the pieces' proportions again to the rationals. I was able to find a -angled isosceles trapezoid with shorter-base-to-leg ratio of using tiles, and a -angled parallelogram with neighboring sides' ratio of using tiles. Thus a few more tiles produce a -angled rhombus, and another few more tiles produce a -angled isosceles trapezoid with shorter-base-to-leg ratio , three of which tile an equilateral triangle, using a total of triangular tiles. While I was at it, I found this less related post that might reduce my number of tiles to a bit below a hundred thousand. Meanwhile, I also did a quick computer search through some conceptually simple configurations that attempt to tile the equilateral triangle using less than about tiles, and I found nothing at all. I get this feeling that about a hundred thousand tiles is not the optimal amount for such a tiling, so I'm asking to see if people have better ideas. I'm unable to provide cash incentive as the parallel post did, but anyone who tries this puzzle will sure have my gratitude. Edit suggested by RavenclawPrefect: To get to the quadrilateral tiling units that I used, the first thing is to de-nest the radicals as I mentioned above. As is Galois over (the base field here should not be but instead the coordinate field of the equilateral triangle), if we can geometrically construct any length (or techinically, ratio ), such that when we perform the same geometric construction but with all the angles and angles swapped with each other, we still provably construct the same , then it must hold that , so that doesn't contain any nested radicals. There were a couple of ideas on what should specifically be, most of them parallel ideas that can all be found in the parallel question for the square. I settled on the above -quadrilaterals (the ones that are a single row of triangular tiles) because they had the smallest numerator norms among others. As a non-example, there was a double-decker idea using tiles that resulted in a trapezoid with ratio a rational multiple of , yuck. There were also some non-triviality in which way the triangles should be oriented when being put into a single row, but some more calculation showed that the above form are all we really get. More precisely, a trapezoid also can't have , and a parallelogram also can't have . After all that work, the rest has really been a matter of trial-and-error. Among all the form, I picked a parallelogram with the smallest norm, which is an parallelogram with tiles, and rotated it so that it becomes a -parallelogram. Then of those make a -parallelogram with tiles, and obviously I combined it to a -trapezoid and a -parallelogram to get to the rational quadrilaterals. So the process was more like ""I frankly don't know what else to do"" rather than ""I see potential simplifications but I don't know the optimum"". It's also why I'm seeking for completely new ideas (see above) that aren't found in the parallel question about the square. RavenclawPrefect also asked a well-motivated question for if the same tiling could be performed but with congruent tiles. M. Laczkovich proved this is impossible in a subsequent paper Tilings of Convex Polygons with Congruent Triangles .","(42^\circ,60^\circ,78^\circ) \mathbf{Q}(\zeta_{60}) (42^\circ,60^\circ,78^\circ) (6^\circ,60^\circ,114^\circ) \sin(42^\circ) \sin(78^\circ) 60^\circ m\cdot\frac{9-3\sqrt{5}}{2}+n\cdot\frac{11-3\sqrt{5}}{2}\quad\left(m,n\ge 0\right) 60^\circ 10 79 60^\circ 11 80 60^\circ 60^\circ 1 121\,170 50 \mathbf{Q}(\zeta_{60}) \mathbf{Q}(\sqrt{3}) \mathbf{Q} \ell \ell 42^\circ 78^\circ \ell \ell\in\mathbf{Q}(\sqrt{5}) \ell \ell \mathbf{Q}(\sqrt{5}) 9 889-321\sqrt{5} (m,n) m=0 n=0 (m,n) (m,n)=(0,1) 4 \frac{11+3\sqrt{5}}{38} 19 \frac{11+3\sqrt{5}}{2} 76 (1,0) (0,1)","['abstract-algebra', 'geometry']"
70,Can someone explain the Y Combinator?,Can someone explain the Y Combinator?,,"The Y combinator is a concept in functional programming, borrowed from the lambda calculus. It is a fixed-point combinator. A fixed point combinator $G$ is a higher-order function (a functional, in mathematical language) that, given a function $f$, returns a fixed point of $f$. In mathematical language, $$f(G(f)) = G(f)$$ This can be considered the defining equation of a fixed-point combinator. Note that $f$ might be a function whose range and domain are themselves function spaces -- in fact this is the most common use of a fixed-point combinator: you can define a function $\alpha$ by specifying that it is the fixed point of another function $f$, and then compute $\alpha$ as $G(f)$. As mathematicians we're used to functions having names, eg $f:x\mapsto x^2$ is the function called $f$ that maps $x$ to $x^2$. But there's no reason why you can't have anonymous function. Since the lambda calculus deals with these a lot, there's a special notation for them: $$\lambda x.x^2$$ is the function that takes $x$ to $x^2$, so that e.g. $(\lambda x.x^2)(2) = 4$. When there's no ambiguity, we can write function application by concatenation: $(\lambda x.x^2) 2 = 4$, and if we defined $f = \lambda x.x^2$ then $f\; 2 = 4$. Okay, now we get to the meat of the question. The Y combinator is a higher-order function (functional) defined as $$Y = \lambda f. (\lambda x. f (x\;x)) \; (\lambda x. f (x\;x))$$ I can follow through the algebra and see that this is indeed a fixed-point combinator: $$\begin{align} Y\; g & = (\lambda f. (\lambda x. f (x\;x)) \; (\lambda x. f (x\;x))) \; g \\ & = (\lambda x. g (x\;x)) \; (\lambda x. g (x\;x)) \\ & = (\lambda y. g (y\;y)) \; (\lambda x. g (x\;x)) \\ & = g \; (\lambda x. g (x\;x)) (\lambda x. g (x\;x)) \\ & = g\; (Y\; g) \end{align}$$ but I have no intuition as to why it works, or how someone might have come up with it. More to the point, I don't see how it can be practically used to compute functions as fixed-points of functionals. Anyone got a good 'intuitive' explanation?","The Y combinator is a concept in functional programming, borrowed from the lambda calculus. It is a fixed-point combinator. A fixed point combinator $G$ is a higher-order function (a functional, in mathematical language) that, given a function $f$, returns a fixed point of $f$. In mathematical language, $$f(G(f)) = G(f)$$ This can be considered the defining equation of a fixed-point combinator. Note that $f$ might be a function whose range and domain are themselves function spaces -- in fact this is the most common use of a fixed-point combinator: you can define a function $\alpha$ by specifying that it is the fixed point of another function $f$, and then compute $\alpha$ as $G(f)$. As mathematicians we're used to functions having names, eg $f:x\mapsto x^2$ is the function called $f$ that maps $x$ to $x^2$. But there's no reason why you can't have anonymous function. Since the lambda calculus deals with these a lot, there's a special notation for them: $$\lambda x.x^2$$ is the function that takes $x$ to $x^2$, so that e.g. $(\lambda x.x^2)(2) = 4$. When there's no ambiguity, we can write function application by concatenation: $(\lambda x.x^2) 2 = 4$, and if we defined $f = \lambda x.x^2$ then $f\; 2 = 4$. Okay, now we get to the meat of the question. The Y combinator is a higher-order function (functional) defined as $$Y = \lambda f. (\lambda x. f (x\;x)) \; (\lambda x. f (x\;x))$$ I can follow through the algebra and see that this is indeed a fixed-point combinator: $$\begin{align} Y\; g & = (\lambda f. (\lambda x. f (x\;x)) \; (\lambda x. f (x\;x))) \; g \\ & = (\lambda x. g (x\;x)) \; (\lambda x. g (x\;x)) \\ & = (\lambda y. g (y\;y)) \; (\lambda x. g (x\;x)) \\ & = g \; (\lambda x. g (x\;x)) (\lambda x. g (x\;x)) \\ & = g\; (Y\; g) \end{align}$$ but I have no intuition as to why it works, or how someone might have come up with it. More to the point, I don't see how it can be practically used to compute functions as fixed-points of functionals. Anyone got a good 'intuitive' explanation?",,"['abstract-algebra', 'computer-science', 'computability', 'lambda-calculus', 'combinatory-logic']"
71,Intuitive explanation of Nakayama's Lemma,Intuitive explanation of Nakayama's Lemma,,"Nakayama's lemma states that given a finitely generated $A$-module $M$, and $J(A)$ the Jacobson radical of $A$, with $I\subseteq J(A)$ some ideal, then if $IM=M$, we have $M=0$. I've read the proof, and while being relatively simple, it doesn't give much insight on why this lemma should be true, for example - is there some way to see how the fact that $J(A)$ is the intersection of all maximal ideals related to the result? Any intuition on the conditions and the result would be of great help.","Nakayama's lemma states that given a finitely generated $A$-module $M$, and $J(A)$ the Jacobson radical of $A$, with $I\subseteq J(A)$ some ideal, then if $IM=M$, we have $M=0$. I've read the proof, and while being relatively simple, it doesn't give much insight on why this lemma should be true, for example - is there some way to see how the fact that $J(A)$ is the intersection of all maximal ideals related to the result? Any intuition on the conditions and the result would be of great help.",,"['abstract-algebra', 'intuition', 'commutative-algebra', 'modules']"
72,One-to-one correspondence of ideals in the quotient also extends to prime ideals?,One-to-one correspondence of ideals in the quotient also extends to prime ideals?,,"I'm beginning to learn some Grothendieck's algebraic geometry and I have a doubt about a property of commutative algebra. For a comm. ring $A$ and an ideal $I$ of $A$, does the one-to-one correspondence between ideals of the quotient $A/I$ and ideals of $A$ containing $I$ extends to a correspondence of prime ideals ? My guess would be no, because I never learned it and couldn't find it on internet. However, where is the mistake in : Let $I \leq J \leq A$ be an ideal. We have $J$ prime iff $A/J \cong (A/I) / (J/I)$ integral domain iff $\bar{J} \leq A/I$ prime. Also can someone provide a counter example if this is not true ? Thanks !","I'm beginning to learn some Grothendieck's algebraic geometry and I have a doubt about a property of commutative algebra. For a comm. ring $A$ and an ideal $I$ of $A$, does the one-to-one correspondence between ideals of the quotient $A/I$ and ideals of $A$ containing $I$ extends to a correspondence of prime ideals ? My guess would be no, because I never learned it and couldn't find it on internet. However, where is the mistake in : Let $I \leq J \leq A$ be an ideal. We have $J$ prime iff $A/J \cong (A/I) / (J/I)$ integral domain iff $\bar{J} \leq A/I$ prime. Also can someone provide a counter example if this is not true ? Thanks !",,"['abstract-algebra', 'ideals', 'maximal-and-prime-ideals']"
73,A nonsplit short exact sequence of abelian groups with $B \cong A \oplus C$,A nonsplit short exact sequence of abelian groups with,B \cong A \oplus C,"A homework problem asked to find a short exact sequence of abelian groups $$0 \rightarrow A \longrightarrow B \longrightarrow C \rightarrow 0$$ such that $B \cong A \oplus C$ although the sequence does not split. My solution to this is the sequence $$0 \rightarrow \mathbb{Z} \overset{i}{\longrightarrow} \mathbb{Z} \oplus (\mathbb{Z}/2\mathbb{Z})^{\mathbb{N}} \overset{p}{\longrightarrow} (\mathbb{Z}/2\mathbb{Z})^{\mathbb{N}} \rightarrow 0$$ with $i(x)=(2x,0,0,\dotsc)$ and $p(x,y_1,y_2,\dotsc)=(x+2\mathbb{Z},y_1,y_2,\dotsc)$ . My new questions: Is there an example with finite/finitely generated abelian groups? If the answer to (1) is negative, will it help to pass to general $R$ -modules for some ring $R$ ?","A homework problem asked to find a short exact sequence of abelian groups such that although the sequence does not split. My solution to this is the sequence with and . My new questions: Is there an example with finite/finitely generated abelian groups? If the answer to (1) is negative, will it help to pass to general -modules for some ring ?","0 \rightarrow A \longrightarrow B \longrightarrow C \rightarrow 0 B \cong A \oplus C 0 \rightarrow \mathbb{Z} \overset{i}{\longrightarrow}
\mathbb{Z} \oplus (\mathbb{Z}/2\mathbb{Z})^{\mathbb{N}} \overset{p}{\longrightarrow}
(\mathbb{Z}/2\mathbb{Z})^{\mathbb{N}} \rightarrow 0 i(x)=(2x,0,0,\dotsc) p(x,y_1,y_2,\dotsc)=(x+2\mathbb{Z},y_1,y_2,\dotsc) R R","['abstract-algebra', 'abelian-groups', 'exact-sequence']"
74,A ring isomorphic to its finite polynomial rings but not to its infinite one.,A ring isomorphic to its finite polynomial rings but not to its infinite one.,,"I was messing with the ring $k[x_1,\dots,x_n,\dots]$ of polynomials in numerable many variables in order to solve an exercise of Atiyah, and the following question came to me and made me curious: Is there a commutative unitary ring $A$ isomorphic to $A[X]$ but not isomorphic to $A[x_1,\dots,x_n,\dots]$ the ring of polynomials with coefficients in $A$ and numerable many variables?","I was messing with the ring $k[x_1,\dots,x_n,\dots]$ of polynomials in numerable many variables in order to solve an exercise of Atiyah, and the following question came to me and made me curious: Is there a commutative unitary ring $A$ isomorphic to $A[X]$ but not isomorphic to $A[x_1,\dots,x_n,\dots]$ the ring of polynomials with coefficients in $A$ and numerable many variables?",,"['abstract-algebra', 'commutative-algebra', 'ring-theory']"
75,Where Fermat's last theorem fails,Where Fermat's last theorem fails,,"It's fairly well known that Fermat's last theorem fails in $\mathbb{Z}/p\mathbb{Z}$. Schur discovered this while he was trying to prove the conjecture on $\mathbb{N}$, and the proof is an application of one of his results in Ramsey theory, now known as Schur's theorem. I'm wondering whether there are any other places (let's say, unique factorisation domains) where the statement is known to be false?","It's fairly well known that Fermat's last theorem fails in $\mathbb{Z}/p\mathbb{Z}$. Schur discovered this while he was trying to prove the conjecture on $\mathbb{N}$, and the proof is an application of one of his results in Ramsey theory, now known as Schur's theorem. I'm wondering whether there are any other places (let's say, unique factorisation domains) where the statement is known to be false?",,"['abstract-algebra', 'number-theory', 'elementary-number-theory', 'integral-domain', 'unique-factorization-domains']"
76,Analogue of the Cantor-Bernstein-Schroeder theorem for general algebraic structures,Analogue of the Cantor-Bernstein-Schroeder theorem for general algebraic structures,,"The Cantor-Bernstein-Schroeder theorem states that if there are two sets $A$ and $B$ such that there exist injective (alternatively, surjective, assuming choice I think) maps $A \to B$ and $B \to A$, then $A$ and $B$ are in bijection.  It then seems natural to try to strengthen the result to other structures.  If $A$ and $B$ are, say, groups (or rings, modules, etc) such that there are injective homomorphisms from each to the other, then $A$ and $B$ are isomorphic. I'm wondering if there are any results on this, or if there are known counterexamples.  The theorem for sets alone is nontrivial, so I feel like any results about how it holds for other structures would be quite interesting.","The Cantor-Bernstein-Schroeder theorem states that if there are two sets $A$ and $B$ such that there exist injective (alternatively, surjective, assuming choice I think) maps $A \to B$ and $B \to A$, then $A$ and $B$ are in bijection.  It then seems natural to try to strengthen the result to other structures.  If $A$ and $B$ are, say, groups (or rings, modules, etc) such that there are injective homomorphisms from each to the other, then $A$ and $B$ are isomorphic. I'm wondering if there are any results on this, or if there are known counterexamples.  The theorem for sets alone is nontrivial, so I feel like any results about how it holds for other structures would be quite interesting.",,"['abstract-algebra', 'category-theory']"
77,Normal subgroups of $S_4$,Normal subgroups of,S_4,"Can anyone tell me how to find all normal subgroups of the symmetric group $S_4$? In particular are $H=\{e,(1 2)(3 4)\}$ and $K=\{e,(1 2)(3 4), (1 3)(2 4),(1 4)(2 3)\}$ normal subgroups?","Can anyone tell me how to find all normal subgroups of the symmetric group $S_4$? In particular are $H=\{e,(1 2)(3 4)\}$ and $K=\{e,(1 2)(3 4), (1 3)(2 4),(1 4)(2 3)\}$ normal subgroups?",,"['abstract-algebra', 'group-theory', 'symmetric-groups', 'normal-subgroups']"
78,Example of a finitely generated module with submodules that are not finitely generated,Example of a finitely generated module with submodules that are not finitely generated,,"I'm looking for an example of a finitely generated module with submodules that are not finitely generated. I've found a similar question dealing with group (i.e. an example of a finitely generated group with subgroups that are not finitely generated). But I can't figure out whether that question do help to this one. And I actually want to find a more ""module-like"" example rather than an example derived from a 'strange' group. Can you please help? Thank you!","I'm looking for an example of a finitely generated module with submodules that are not finitely generated. I've found a similar question dealing with group (i.e. an example of a finitely generated group with subgroups that are not finitely generated). But I can't figure out whether that question do help to this one. And I actually want to find a more ""module-like"" example rather than an example derived from a 'strange' group. Can you please help? Thank you!",,"['abstract-algebra', 'modules']"
79,Conjectures for finite groups that fail with large counterexamples,Conjectures for finite groups that fail with large counterexamples,,"I'm asking this question in the same spirit as this other question: Conjectures that have been disproved with extremely large counterexamples? . What's a nice conjecture relating to finite groups, that first fails for a group of order $N$ , with $N$ large? By ""nice"" I mean a conjecture with a balance between having a simple statement, one that first fails for a large $N$ , and something that is not immediately obvious that it is going to fail. An example might be the conjecture ""if $G$ is a finite simple group, then it is the unique simple group of its order"", which first fails for $N=20160$ . ""Large"" in this context is of course undefined, perhaps we'll say $N$ is ""large"" if the number of groups of order at most $N$ up to isomorphism is ""large"" in a more generic context. Perhaps I might suggest $N \geq 32$ as large, since $N= 32$ is the smallest number such that there are at least $100$ groups of order at most $N$ . The bigger $N$ you can come up with, though, the better!","I'm asking this question in the same spirit as this other question: Conjectures that have been disproved with extremely large counterexamples? . What's a nice conjecture relating to finite groups, that first fails for a group of order , with large? By ""nice"" I mean a conjecture with a balance between having a simple statement, one that first fails for a large , and something that is not immediately obvious that it is going to fail. An example might be the conjecture ""if is a finite simple group, then it is the unique simple group of its order"", which first fails for . ""Large"" in this context is of course undefined, perhaps we'll say is ""large"" if the number of groups of order at most up to isomorphism is ""large"" in a more generic context. Perhaps I might suggest as large, since is the smallest number such that there are at least groups of order at most . The bigger you can come up with, though, the better!",N N N G N=20160 N N N \geq 32 N= 32 100 N N,"['abstract-algebra', 'group-theory', 'finite-groups', 'examples-counterexamples', 'recreational-mathematics']"
80,"Does there exist rational $a,b,c$, such that $\sqrt[3]{1}+\sqrt[3]{2}+\sqrt[3]{4}=\sqrt[3]{a}+\sqrt[3]{b}+\sqrt[3]{c}$","Does there exist rational , such that","a,b,c \sqrt[3]{1}+\sqrt[3]{2}+\sqrt[3]{4}=\sqrt[3]{a}+\sqrt[3]{b}+\sqrt[3]{c}","Let $w = \sqrt[3]{1}+\sqrt[3]{2}+\sqrt[3]{4}$. How to prove that there are no triples $(a,b,c)$, such that $a,b,c \in \mathbb{Q}$; $a \leqslant b \leqslant c$; $(a,b,c)\ne (1,2,4)$; $w = \sqrt[3]{a}+\sqrt[3]{b}+\sqrt[3]{c}$. Or maybe there exists one?","Let $w = \sqrt[3]{1}+\sqrt[3]{2}+\sqrt[3]{4}$. How to prove that there are no triples $(a,b,c)$, such that $a,b,c \in \mathbb{Q}$; $a \leqslant b \leqslant c$; $(a,b,c)\ne (1,2,4)$; $w = \sqrt[3]{a}+\sqrt[3]{b}+\sqrt[3]{c}$. Or maybe there exists one?",,"['abstract-algebra', 'algebra-precalculus', 'number-theory']"
81,Showing that a finite abelian group has a subgroup of order $m$ for each divisor $m$ of $n$,Showing that a finite abelian group has a subgroup of order  for each divisor  of,m m n,"I have made an attempt to prove that a finite abelian group of order $n$ has a subgroup of order $m$ for every divisor $m$ of $n$ . Specifically, I am asked to use a quotient group-induction argument to show this.  I'd appreciate comments on the validity or lack thereof of my attempted proof below. Let $G$ be a finite abelian group of order $n$ and let $m$ be a divisor of $n$ .  The proposition is true for $n=1$ , so we'll proceed by induction and assume $n \ge 2$ .  Let $p$ be a prime dividing $m$ and let $x$ be an element of order $p$ in $G$ (which exists by Cauchy's Theorem for Abelian Groups).  By the induction hypothesis, $G/\langle x \rangle$ has a subgroup of order $\frac{m}{p}$ .  This subgroup is of the form $H/\langle x \rangle$ for some $H \le G$ .  Since $|H/\langle x \rangle| = \frac{m}{p}$ , it follows that $H \le G$ has order $m$ . I chose to use a prime divisor of $m$ , but I don't see why it wouldn't work to use any proper divisor of $m$ .  Am I correct on this point? Thanks, I appreciate the help.","I have made an attempt to prove that a finite abelian group of order has a subgroup of order for every divisor of . Specifically, I am asked to use a quotient group-induction argument to show this.  I'd appreciate comments on the validity or lack thereof of my attempted proof below. Let be a finite abelian group of order and let be a divisor of .  The proposition is true for , so we'll proceed by induction and assume .  Let be a prime dividing and let be an element of order in (which exists by Cauchy's Theorem for Abelian Groups).  By the induction hypothesis, has a subgroup of order .  This subgroup is of the form for some .  Since , it follows that has order . I chose to use a prime divisor of , but I don't see why it wouldn't work to use any proper divisor of .  Am I correct on this point? Thanks, I appreciate the help.",n m m n G n m n n=1 n \ge 2 p m x p G G/\langle x \rangle \frac{m}{p} H/\langle x \rangle H \le G |H/\langle x \rangle| = \frac{m}{p} H \le G m m m,"['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups']"
82,Does group of permutations of natural numbers contain subgroup isomorphic to $SO(2)$?,Does group of permutations of natural numbers contain subgroup isomorphic to ?,SO(2),"Consider following two infinite groups: group of all permutations of natural numbers (i.e. group of all bijections $f: \mathbb{N} \to \mathbb{N}$ ) and group of all rotations of a plane. Does group of permutations contain subgroup isomorphic to the group of rotations? Both groups have cardinality of the continuum, so simple cardinality considerations do not work.","Consider following two infinite groups: group of all permutations of natural numbers (i.e. group of all bijections ) and group of all rotations of a plane. Does group of permutations contain subgroup isomorphic to the group of rotations? Both groups have cardinality of the continuum, so simple cardinality considerations do not work.",f: \mathbb{N} \to \mathbb{N},"['abstract-algebra', 'group-theory', 'infinite-groups']"
83,Does the equation $x^4+y^4+1 = z^2$ have a non-trivial solution?,Does the equation  have a non-trivial solution?,x^4+y^4+1 = z^2,"The background of this question is this: Fermat proved that the equation, $$x^4+y^4 = z^2$$ has no solution in the positive integers. If we consider the near-miss, $$x^4+y^4-1 = z^2$$ then this has plenty (in fact, an infinity, as it can be solved by a Pell equation). But J. Cullen, by exhaustive search, found that the other near-miss, $$x^4+y^4+1 = z^2$$ has none with $0 < x,y < 10^6$. Does the third equation really have none at all, or are the solutions just enormous?","The background of this question is this: Fermat proved that the equation, $$x^4+y^4 = z^2$$ has no solution in the positive integers. If we consider the near-miss, $$x^4+y^4-1 = z^2$$ then this has plenty (in fact, an infinity, as it can be solved by a Pell equation). But J. Cullen, by exhaustive search, found that the other near-miss, $$x^4+y^4+1 = z^2$$ has none with $0 < x,y < 10^6$. Does the third equation really have none at all, or are the solutions just enormous?",,"['abstract-algebra', 'number-theory', 'diophantine-equations']"
84,How to calculate GCD of Gaussian integers? [closed],How to calculate GCD of Gaussian integers? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question What is the gcd of $\,a = 18-i\,$ and $\,b = 11+7i\,$ in $\,\Bbb Z [i] =\{m+ni: m,n \in \mathbb Z\}$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question What is the gcd of and in ?","\,a = 18-i\, \,b = 11+7i\, \,\Bbb Z [i] =\{m+ni: m,n \in \mathbb Z\}","['abstract-algebra', 'elementary-number-theory', 'ring-theory', 'complex-numbers', 'gcd-and-lcm']"
85,About the localization of a UFD,About the localization of a UFD,,"I was wondering, is the localization of a UFD also a UFD? How would one go about proving this? It seems like it would be kind of messy to prove if it is true. If it is not true, what about localizing at a prime? Or what if the UFD is Noetherian?","I was wondering, is the localization of a UFD also a UFD? How would one go about proving this? It seems like it would be kind of messy to prove if it is true. If it is not true, what about localizing at a prime? Or what if the UFD is Noetherian?",,"['abstract-algebra', 'commutative-algebra', 'unique-factorization-domains']"
86,Prove that 1+1=2 [duplicate],Prove that 1+1=2 [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: How do I convince someone that $1+1=2$ may not necessarily be true? I once read that some mathematicians provided a very length proof of $1+1=2$. Can you think of some way to extend mathematical rigor to present a long proof of that equation? I'm not asking for a proof, but rather for some outline what one would consider to make the derivation as long as possible. EDIT: It seems the proof I heard about is a standard reference given here multiple times :) I stated that the proof itself is less useful than an outline for me as I know only ""physics level maths"". Can someone provide a short outline what's going on in the proof? Some outline I can look up section by section in Wikipedia to at least get a feel of what could be needed to make such a proof?","This question already has answers here : Closed 11 years ago . Possible Duplicate: How do I convince someone that $1+1=2$ may not necessarily be true? I once read that some mathematicians provided a very length proof of $1+1=2$. Can you think of some way to extend mathematical rigor to present a long proof of that equation? I'm not asking for a proof, but rather for some outline what one would consider to make the derivation as long as possible. EDIT: It seems the proof I heard about is a standard reference given here multiple times :) I stated that the proof itself is less useful than an outline for me as I know only ""physics level maths"". Can someone provide a short outline what's going on in the proof? Some outline I can look up section by section in Wikipedia to at least get a feel of what could be needed to make such a proof?",,['abstract-algebra']
87,"What is $\gcd(0,a)$, where $a$ is a positive integer?","What is , where  is a positive integer?","\gcd(0,a) a","I have tried $\gcd(0,8)$ in a lot of online  gcd (or hcf) calculators, but some say $\gcd(0,8)=0$, some other gives $\gcd(0,8)=8$ and some others give $\gcd(0,8)=1$. So really which one of these is correct and why there are different conventions?","I have tried $\gcd(0,8)$ in a lot of online  gcd (or hcf) calculators, but some say $\gcd(0,8)=0$, some other gives $\gcd(0,8)=8$ and some others give $\gcd(0,8)=1$. So really which one of these is correct and why there are different conventions?",,"['abstract-algebra', 'group-theory', 'elementary-number-theory', 'gcd-and-lcm']"
88,Find the center of the symmetry group $S_n$.,Find the center of the symmetry group .,S_n,"Find the center of the symmetry group $S_n$. Attempt: By definition, the center is $Z(S_n) = \{ a \in S_n : ag = ga \forall\  g \in S_n\}$. Then we know the identity $e$ is in $S_n$ since there is always the trivial permutation. Suppose $a$ is in $S_n$, but not equal to identity. Now we can imagine the permutation as bijective function that maps from $\{1,2,\dotsc,n\}$ to $\{1,2,\dotsc,n\}$. So suppose $p$ is a permutation map. Then $p$ maps from a location $i$ to a location $j$. Take $p(i) = j$ where $i\neq j$. Let $k$ be in $\{1,2,\dotsc,n\}$, where $k$, $i$ and $j$ are all different elements. The cycle $r = (jk)$, then we will see if this commutes.  $rp(i) = rj$ Can someone please help me, I am stuck?  Thank you.","Find the center of the symmetry group $S_n$. Attempt: By definition, the center is $Z(S_n) = \{ a \in S_n : ag = ga \forall\  g \in S_n\}$. Then we know the identity $e$ is in $S_n$ since there is always the trivial permutation. Suppose $a$ is in $S_n$, but not equal to identity. Now we can imagine the permutation as bijective function that maps from $\{1,2,\dotsc,n\}$ to $\{1,2,\dotsc,n\}$. So suppose $p$ is a permutation map. Then $p$ maps from a location $i$ to a location $j$. Take $p(i) = j$ where $i\neq j$. Let $k$ be in $\{1,2,\dotsc,n\}$, where $k$, $i$ and $j$ are all different elements. The cycle $r = (jk)$, then we will see if this commutes.  $rp(i) = rj$ Can someone please help me, I am stuck?  Thank you.",,"['abstract-algebra', 'group-theory', 'permutations', 'symmetric-groups']"
89,"My proof of $I \otimes N \cong IN$ is clearly wrong, but where have I gone wrong?","My proof of  is clearly wrong, but where have I gone wrong?",I \otimes N \cong IN,"Ok, I'm reading some thesis of some former students, and come up with this proof, but it doesn't really look good to me. So I guess it should be wrong somewhere. So, here it goes: Let $R$ be a unitary commutative ring, and $I$ be an ideal, and $N$ is an $R$-module. I'll now show that $$I \otimes_R N \cong IN\,.$$ First, since $1 \in R$, and $I$ is an ideal of $R$, we must have that $I = RI$. So then $$I \otimes_R N = RI \otimes_R N = R \otimes_R IN \cong IN\,.$$ But this must be so wrong, since if this proof is true, I could prove that any $R$-module is flat. So how come this proof is wrong?","Ok, I'm reading some thesis of some former students, and come up with this proof, but it doesn't really look good to me. So I guess it should be wrong somewhere. So, here it goes: Let $R$ be a unitary commutative ring, and $I$ be an ideal, and $N$ is an $R$-module. I'll now show that $$I \otimes_R N \cong IN\,.$$ First, since $1 \in R$, and $I$ is an ideal of $R$, we must have that $I = RI$. So then $$I \otimes_R N = RI \otimes_R N = R \otimes_R IN \cong IN\,.$$ But this must be so wrong, since if this proof is true, I could prove that any $R$-module is flat. So how come this proof is wrong?",,"['abstract-algebra', 'commutative-algebra', 'modules', 'tensor-products', 'flatness']"
90,Galois Group of $\sqrt{2+\sqrt{2}}$ over $\mathbb{Q}$,Galois Group of  over,\sqrt{2+\sqrt{2}} \mathbb{Q},"So I want to show that $\mathbb{Q}(\sqrt{2+\sqrt{2}})$ is Galois over $\mathbb{Q}$ and determine its Galois group. My thoughts are as follows: Define $\alpha := \sqrt{2+\sqrt{2}}$. Then it is easily shown that $\alpha$ satisfies $\alpha^4-4\alpha^2+2=0$. Define $f(x) := x^4-4x^2+2$. Then $f$ is irreducible over $\mathbb{Q}$ by Eisenstein with $p=2$. So we have that $f$ is the irreducible polynomial for $\alpha$ over $\mathbb{Q}$. Further $|\mathbb{Q}(\alpha):\mathbb{Q}|=4$. For $\mathbb{Q}({\alpha})$ to be Galois, it must contain all roots of $f$. Define $K=\mathbb{Q}(\alpha)$ for convenience. Define $\alpha := \alpha_1$. Since $f$ has only even powers, we know that $-\alpha := \alpha_2$ is a root, and therefore contained in $K$ since $K$ is a field. We note that the other two roots are $\alpha_3=\sqrt{2-\sqrt{2}}$ and $\alpha_4=-\sqrt{2-\sqrt{2}}$. So in order to show $K$ is Galois, it must be shown that $\alpha_3$ and $\alpha_4$ lie in $K$. Now $\alpha_1^2=2+\sqrt2$ and so $\sqrt2 \in K$. Thus $-\sqrt2 \in K$ since $K$ is a field. Can somebody explain why $\alpha_3$ and $\alpha_4$ lie in $K$? Next we are to determine the Galois group of $K$. Assuming $K$ is Galois, since it has degree $4$ over $\mathbb{Q}$ (shown earlier), we know that its Galois group has size $4$. There are only two groups of size $4$, namely $V_4$ and $C_4$, the Klein four group and the cyclic group of order $4$. How do we determine which of these choice is in fact the Galois Group?","So I want to show that $\mathbb{Q}(\sqrt{2+\sqrt{2}})$ is Galois over $\mathbb{Q}$ and determine its Galois group. My thoughts are as follows: Define $\alpha := \sqrt{2+\sqrt{2}}$. Then it is easily shown that $\alpha$ satisfies $\alpha^4-4\alpha^2+2=0$. Define $f(x) := x^4-4x^2+2$. Then $f$ is irreducible over $\mathbb{Q}$ by Eisenstein with $p=2$. So we have that $f$ is the irreducible polynomial for $\alpha$ over $\mathbb{Q}$. Further $|\mathbb{Q}(\alpha):\mathbb{Q}|=4$. For $\mathbb{Q}({\alpha})$ to be Galois, it must contain all roots of $f$. Define $K=\mathbb{Q}(\alpha)$ for convenience. Define $\alpha := \alpha_1$. Since $f$ has only even powers, we know that $-\alpha := \alpha_2$ is a root, and therefore contained in $K$ since $K$ is a field. We note that the other two roots are $\alpha_3=\sqrt{2-\sqrt{2}}$ and $\alpha_4=-\sqrt{2-\sqrt{2}}$. So in order to show $K$ is Galois, it must be shown that $\alpha_3$ and $\alpha_4$ lie in $K$. Now $\alpha_1^2=2+\sqrt2$ and so $\sqrt2 \in K$. Thus $-\sqrt2 \in K$ since $K$ is a field. Can somebody explain why $\alpha_3$ and $\alpha_4$ lie in $K$? Next we are to determine the Galois group of $K$. Assuming $K$ is Galois, since it has degree $4$ over $\mathbb{Q}$ (shown earlier), we know that its Galois group has size $4$. There are only two groups of size $4$, namely $V_4$ and $C_4$, the Klein four group and the cyclic group of order $4$. How do we determine which of these choice is in fact the Galois Group?",,"['abstract-algebra', 'group-theory', 'polynomials', 'field-theory', 'galois-theory']"
91,Geometric intuition of tensor product,Geometric intuition of tensor product,,"Let $V$ and $W$ be two algebraic structures, $v\in V$, $w\in W$ be two arbitrary elements. Then, what is the geometric intuition of $v\otimes w$, and more complex $V\otimes W$ ? Please explain for me in the most concrete way (for example, $v, w$ are two vectors in 2 dimensional vector spaces $V, W$) Thanks","Let $V$ and $W$ be two algebraic structures, $v\in V$, $w\in W$ be two arbitrary elements. Then, what is the geometric intuition of $v\otimes w$, and more complex $V\otimes W$ ? Please explain for me in the most concrete way (for example, $v, w$ are two vectors in 2 dimensional vector spaces $V, W$) Thanks",,"['abstract-algebra', 'intuition', 'tensor-products']"
92,Kaplansky's theorem of infinitely many right inverses in monoids?,Kaplansky's theorem of infinitely many right inverses in monoids?,,"There's a theorem of Kaplansky that states that if an element $u$ of a ring has more than one right inverse, then it in fact has infinitely many. I could prove this by assuming $v$ is a right inverse, and then showing that the elements $v+(1-vu)u^n$ are right inverses for all $n$ and distinct. To see they're distinct, I suppose $v+(1-vu)u^n=v+(1-vu)u^m$ for distinct $n$ and $m$. I suppose $n>m$. Since $u$ is cancellable on the right, this implies $(1-vu)u^{n-m}=1-vu$. Then $(1-vu)u^{n-m-1}u+vu=((1-vu)u^{n-m-1}+v)u=1$, so $u$ has a left inverse, but then $u$ would be a unit, and hence have only one right inverse. Does the same theorem hold in monoids, or is there some counterexample?","There's a theorem of Kaplansky that states that if an element $u$ of a ring has more than one right inverse, then it in fact has infinitely many. I could prove this by assuming $v$ is a right inverse, and then showing that the elements $v+(1-vu)u^n$ are right inverses for all $n$ and distinct. To see they're distinct, I suppose $v+(1-vu)u^n=v+(1-vu)u^m$ for distinct $n$ and $m$. I suppose $n>m$. Since $u$ is cancellable on the right, this implies $(1-vu)u^{n-m}=1-vu$. Then $(1-vu)u^{n-m-1}u+vu=((1-vu)u^{n-m-1}+v)u=1$, so $u$ has a left inverse, but then $u$ would be a unit, and hence have only one right inverse. Does the same theorem hold in monoids, or is there some counterexample?",,"['abstract-algebra', 'ring-theory', 'monoid']"
93,Good introductory books on homological algebra,Good introductory books on homological algebra,,"Which books would you recommend, for self-studying homological algebra, to a beginning graduate (or advanced undergraduate) student who has background in ring theory, modules, basic commutative algebra (some of Atiyah & Macdonald's book) and some (basic) field theory? I would especially like to hear your opinions on the following books: A Course On Homological Algebra / P. J Hilton and U. Stambach Introduction to Homological Algebra / Szen-Tsen Hu Notes on Homological Algebra / Rotman But other recommendations will also be appreciated.","Which books would you recommend, for self-studying homological algebra, to a beginning graduate (or advanced undergraduate) student who has background in ring theory, modules, basic commutative algebra (some of Atiyah & Macdonald's book) and some (basic) field theory? I would especially like to hear your opinions on the following books: A Course On Homological Algebra / P. J Hilton and U. Stambach Introduction to Homological Algebra / Szen-Tsen Hu Notes on Homological Algebra / Rotman But other recommendations will also be appreciated.",,"['abstract-algebra', 'reference-request', 'homological-algebra', 'big-list', 'learning']"
94,Are cyclic groups always abelian?,Are cyclic groups always abelian?,,"If a group $C$ is cyclic, is it also abelian (commutative)? If so, is it possible to give an “easy” explanation of why this is? Thanks in advance!","If a group $C$ is cyclic, is it also abelian (commutative)? If so, is it possible to give an “easy” explanation of why this is? Thanks in advance!",,"['abstract-algebra', 'group-theory', 'intuition', 'abelian-groups', 'cyclic-groups']"
95,Categorical introduction to Algebra and Topology,Categorical introduction to Algebra and Topology,,"At the moment I am reading books on Algebra and on Category theory. More exactly, I started working through the book Algebra by Serge Lang. I have read the chapters on groups and rings, but then my motivation somehow disappeared and I turned to category theory. More exactly, I started reading Categories for the Working Mathematician by Saunders MacLane. I now feel comfortable with all the concepts discussed in the first five Chapters, i.e. categories and functors and the usual formulations of universal properties. I would really like to go on reading about algebra, but once I understood the strucutrual approaches to Mathematics, I can hardly imagine to continue doing all the awful calculations, basic Algebra books like Lang's are filled with, instead of using universal properties and so on. So basically, my question is, if there are books on Algebra, not assuming any algebraic knowledge, but extensively using category-theoretic methods. Of course, it is very non-standard to cover all the basic category theory before turning to applications in Algebra, but I hope someone knows a book or some lecture notes satisfying my needs. Furthermore, I would like to learn some topology. In this field I have even less knowledge than in Algebra, i.e. I don't even know the definition of a topological space. My question is the same as with Algebra: Is there a categorical/conceptional introduction to general topology?","At the moment I am reading books on Algebra and on Category theory. More exactly, I started working through the book Algebra by Serge Lang. I have read the chapters on groups and rings, but then my motivation somehow disappeared and I turned to category theory. More exactly, I started reading Categories for the Working Mathematician by Saunders MacLane. I now feel comfortable with all the concepts discussed in the first five Chapters, i.e. categories and functors and the usual formulations of universal properties. I would really like to go on reading about algebra, but once I understood the strucutrual approaches to Mathematics, I can hardly imagine to continue doing all the awful calculations, basic Algebra books like Lang's are filled with, instead of using universal properties and so on. So basically, my question is, if there are books on Algebra, not assuming any algebraic knowledge, but extensively using category-theoretic methods. Of course, it is very non-standard to cover all the basic category theory before turning to applications in Algebra, but I hope someone knows a book or some lecture notes satisfying my needs. Furthermore, I would like to learn some topology. In this field I have even less knowledge than in Algebra, i.e. I don't even know the definition of a topological space. My question is the same as with Algebra: Is there a categorical/conceptional introduction to general topology?",,"['abstract-algebra', 'general-topology']"
96,Why is the multiplicative group of a finite field cyclic? [duplicate],Why is the multiplicative group of a finite field cyclic? [duplicate],,"This question already has answers here : Finite subgroups of the multiplicative group of a field are cyclic (6 answers) Closed 10 years ago . Why is the multiplicative group $(K\smallsetminus\{0\},\cdot)$ of a finite field $(K,+,\cdot)$ always cyclic?","This question already has answers here : Finite subgroups of the multiplicative group of a field are cyclic (6 answers) Closed 10 years ago . Why is the multiplicative group $(K\smallsetminus\{0\},\cdot)$ of a finite field $(K,+,\cdot)$ always cyclic?",,"['abstract-algebra', 'finite-fields']"
97,What are examples of irreducible but not prime elements?,What are examples of irreducible but not prime elements?,,"I am looking for a ring element which is irreducible but not prime. So necessarily the ring can't be a PID. My idea was to consider $R=K[x,y]$ and $x+y\in R$. This is irreducible because in any product $x+y=fg$ only one factor, say f, can have a $x$ in it (otherwise we get $x^2$ in the product). And actually then there can be no $y$ in $g$ either because $x+y$ has no mixed terms. Thus $g$ is just an element from $K$, i.e. a unit. I got stuck at proving that $x+y$ is not prime. First off, is this even true? If so, how can I see it?","I am looking for a ring element which is irreducible but not prime. So necessarily the ring can't be a PID. My idea was to consider $R=K[x,y]$ and $x+y\in R$. This is irreducible because in any product $x+y=fg$ only one factor, say f, can have a $x$ in it (otherwise we get $x^2$ in the product). And actually then there can be no $y$ in $g$ either because $x+y$ has no mixed terms. Thus $g$ is just an element from $K$, i.e. a unit. I got stuck at proving that $x+y$ is not prime. First off, is this even true? If so, how can I see it?",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'irreducible-polynomials', 'prime-factorization']"
98,"Nontrivial idempotents in $\mathbb Z_n$ (roots of $x^2 = x$ and $\,x\neq 0,1)$",Nontrivial idempotents in  (roots of  and,"\mathbb Z_n x^2 = x \,x\neq 0,1)","An element $a$ of the ring $(P,+,\cdot)$ is called idempotent if $a^2=a$ . An idempotent $a$ is called nontrivial if $a \neq 0$ and $a \neq 1$ . My question concerns idempotents in rings $\mathbb Z_n$ , with addition and multiplication modulo $n$ , where $n$ is natural number. Obviously when $n$ is a prime number then there is no nontrivial idempotent. If $n$ is nonprime it may happen, for example $n=4, n=9$ , that also there is no. Is it known, in general, for what $n$ there are nontrivial idempotents and what is a form of such idempotents?","An element of the ring is called idempotent if . An idempotent is called nontrivial if and . My question concerns idempotents in rings , with addition and multiplication modulo , where is natural number. Obviously when is a prime number then there is no nontrivial idempotent. If is nonprime it may happen, for example , that also there is no. Is it known, in general, for what there are nontrivial idempotents and what is a form of such idempotents?","a (P,+,\cdot) a^2=a a a \neq 0 a \neq 1 \mathbb Z_n n n n n n=4, n=9 n","['abstract-algebra', 'idempotents']"
99,How does one actually show from associativity that one can drop parentheses?,How does one actually show from associativity that one can drop parentheses?,,"I've always heard this reasoning, and it makes obvious sense, but how do you actually show it for some arbitrary product? Would it be something like this? $$(a(b(cd)))e=((ab)(cd))e=(((ab)c)d)e=abcde?$$ Do you just say that the grouping of the parentheses now corresponds to just multiplying straight through? Thanks.","I've always heard this reasoning, and it makes obvious sense, but how do you actually show it for some arbitrary product? Would it be something like this? $$(a(b(cd)))e=((ab)(cd))e=(((ab)c)d)e=abcde?$$ Do you just say that the grouping of the parentheses now corresponds to just multiplying straight through? Thanks.",,['abstract-algebra']
