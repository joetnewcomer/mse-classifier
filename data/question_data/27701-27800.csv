,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Dimension of subspace of all upper triangular matrices,Dimension of subspace of all upper triangular matrices,,"If $S$ is the subspace of $M_7(R)$  consisting of all upper triangular matrices, then $dim(S)$ = ? So if I have an upper triangular matrix $$ \begin{bmatrix} a_{11} & a_{12} & . & . & a_{17}\\ . & a_{22} & . & . & a_{27}\\ .  & . & . & . & .\\ 0 & . & . & . & a_{77}\\ \end{bmatrix} $$ It looks to me that this matrix can potentially have 7 pivots, therefore it is linearly independent and so it will take all 7 column vectors to span it. But that answer is marked as incorrect when I enter it so what am I missing here?","If $S$ is the subspace of $M_7(R)$  consisting of all upper triangular matrices, then $dim(S)$ = ? So if I have an upper triangular matrix $$ \begin{bmatrix} a_{11} & a_{12} & . & . & a_{17}\\ . & a_{22} & . & . & a_{27}\\ .  & . & . & . & .\\ 0 & . & . & . & a_{77}\\ \end{bmatrix} $$ It looks to me that this matrix can potentially have 7 pivots, therefore it is linearly independent and so it will take all 7 column vectors to span it. But that answer is marked as incorrect when I enter it so what am I missing here?",,"['linear-algebra', 'matrices']"
1,Why are $A=\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ and $B=\begin{bmatrix} 0 & 1 \\ -\det(A) & \operatorname{tr}(A) \end{bmatrix}$ similar?,Why are  and  similar?,A=\begin{bmatrix} a & b \\ c & d \end{bmatrix} B=\begin{bmatrix} 0 & 1 \\ -\det(A) & \operatorname{tr}(A) \end{bmatrix},"We have two $2\times 2$ matrices. The first matrix is $A=\begin{bmatrix} a & b \\ c & d \end{bmatrix}$, and the second matrix is obtained from the first: $B=\begin{bmatrix} 0 & 1 \\ -\det(A) & \operatorname{tr}(A) \end{bmatrix}$, or in other words $B=\begin{bmatrix}  0 & 1 \\ bc-ad & a+d  \end{bmatrix}$. The characteristic polynomial of $A$ is $\lambda^2 - \operatorname{tr}(A) \lambda + \det(A)$. The characteristic polynomial of $B$ is the same, i.e.  $\det(A-\lambda E) = \det(B-\lambda E)$. Since $\det(A-\lambda E) = \det(B-\lambda E)$, then $A$ is similar to $B$, if $A$ isn't equal to $\lambda E$. Prove why, if $\det(A-\lambda E) = \det(B-\lambda E)$, then $A$ is similar to $B$..? Thank you.","We have two $2\times 2$ matrices. The first matrix is $A=\begin{bmatrix} a & b \\ c & d \end{bmatrix}$, and the second matrix is obtained from the first: $B=\begin{bmatrix} 0 & 1 \\ -\det(A) & \operatorname{tr}(A) \end{bmatrix}$, or in other words $B=\begin{bmatrix}  0 & 1 \\ bc-ad & a+d  \end{bmatrix}$. The characteristic polynomial of $A$ is $\lambda^2 - \operatorname{tr}(A) \lambda + \det(A)$. The characteristic polynomial of $B$ is the same, i.e.  $\det(A-\lambda E) = \det(B-\lambda E)$. Since $\det(A-\lambda E) = \det(B-\lambda E)$, then $A$ is similar to $B$, if $A$ isn't equal to $\lambda E$. Prove why, if $\det(A-\lambda E) = \det(B-\lambda E)$, then $A$ is similar to $B$..? Thank you.",,"['linear-algebra', 'matrices']"
2,Linear homogeneous difference equation with constant coefficients,Linear homogeneous difference equation with constant coefficients,,If I have for instance the equation: $0 = 2x_{n} - 3x_{n+1}+x_{n+2}$ Then the solution space is a linear vector space of dimension 2. Someone who can explain why this is true? My teacher has written something about a dimension argument.,If I have for instance the equation: $0 = 2x_{n} - 3x_{n+1}+x_{n+2}$ Then the solution space is a linear vector space of dimension 2. Someone who can explain why this is true? My teacher has written something about a dimension argument.,,"['linear-algebra', 'recurrence-relations']"
3,Help a newbie understand Linear Algebraic terms,Help a newbie understand Linear Algebraic terms,,"I am taking a class in Algebra but I am having a problem grasping exactly what it is I am being asked to do -- I think I am having a problem with the vocabulary being used. I have a couple of questions below and I would appreciate it, if you could take time to explain(in layman's terms or maybe programmer terms, exactly what is being asked. thanks in advance. 1) Let $[e_1; e_2; e_3]$ be the standard basis vectors in $\mathbb{R}^{3}$ and consider the ordered basis: $[e_2; e_1; e_3 + e_1]$ Verify that this is actually a basis and find the coordinates of the vector $(1; 1; 1)^T$ with respect to that basis. 2)  Let $T$ be the linear map from $\mathbb{R}^{2}$ to $\mathbb{R}$ defined by: $T ((x, y)^T ) = x-y$ Find its matrix (with respect to the standard bases) and a basis for its kernel 3)  Find a spanning vector set for the image of the linear map from $\mathbb{R}^{2}$ to $\mathbb{R}^{3}$ defined by: $T ((x, y)^T) = (2x-y, x + y, y)$ 4)  Consider the subspace U of $\mathbb{R}^3$ defined by: $ U = \{(x; y; z)^T: 2x- y + z = 0; x + y = 0 \} $. Express U as the kernel of an appropriately defined linear map and the matrix of that map with respect to the standard bases of the corresponding $\mathbb{R}^n$'s","I am taking a class in Algebra but I am having a problem grasping exactly what it is I am being asked to do -- I think I am having a problem with the vocabulary being used. I have a couple of questions below and I would appreciate it, if you could take time to explain(in layman's terms or maybe programmer terms, exactly what is being asked. thanks in advance. 1) Let $[e_1; e_2; e_3]$ be the standard basis vectors in $\mathbb{R}^{3}$ and consider the ordered basis: $[e_2; e_1; e_3 + e_1]$ Verify that this is actually a basis and find the coordinates of the vector $(1; 1; 1)^T$ with respect to that basis. 2)  Let $T$ be the linear map from $\mathbb{R}^{2}$ to $\mathbb{R}$ defined by: $T ((x, y)^T ) = x-y$ Find its matrix (with respect to the standard bases) and a basis for its kernel 3)  Find a spanning vector set for the image of the linear map from $\mathbb{R}^{2}$ to $\mathbb{R}^{3}$ defined by: $T ((x, y)^T) = (2x-y, x + y, y)$ 4)  Consider the subspace U of $\mathbb{R}^3$ defined by: $ U = \{(x; y; z)^T: 2x- y + z = 0; x + y = 0 \} $. Express U as the kernel of an appropriately defined linear map and the matrix of that map with respect to the standard bases of the corresponding $\mathbb{R}^n$'s",,"['linear-algebra', 'matrices', 'vector-spaces', 'transformation']"
4,rank of a matrix,rank of a matrix,,"First of all I am sorry because I have asked similar kind of question a few days ago.But I still have problem with row reductions when there are letters in a matrix.The question is asking the value of 'a' when the rank of matrix  is 1 , 2 , 3 and 4. I am not good at row reductions.In each row operations the matrix became more confusing.If someone help me with the row reductions , I will be very happy.          $$ \left( \begin{array}{ccc} 1&1&2&0\\ 2&a+1&3&a-1\\ -3&a-2&a-5&a+1\\ a+2&2&a+4&-2a \end{array} \right) $$","First of all I am sorry because I have asked similar kind of question a few days ago.But I still have problem with row reductions when there are letters in a matrix.The question is asking the value of 'a' when the rank of matrix  is 1 , 2 , 3 and 4. I am not good at row reductions.In each row operations the matrix became more confusing.If someone help me with the row reductions , I will be very happy.          $$ \left( \begin{array}{ccc} 1&1&2&0\\ 2&a+1&3&a-1\\ -3&a-2&a-5&a+1\\ a+2&2&a+4&-2a \end{array} \right) $$",,['linear-algebra']
5,What is the meaning of $A^TA$?,What is the meaning of ?,A^TA,"What does it mean if a matrix is multiplied by its transpose? Informally, it seems like $A^TA$ boils a matrix down to its essentials, but can this operation somehow be understood ""intuitively"" (e.g. through a geometric interpretation)?","What does it mean if a matrix is multiplied by its transpose? Informally, it seems like $A^TA$ boils a matrix down to its essentials, but can this operation somehow be understood ""intuitively"" (e.g. through a geometric interpretation)?",,['linear-algebra']
6,How to check if a subset is a generator of a vector space,How to check if a subset is a generator of a vector space,,"I have a very noob questions about generators: what algorithm do I have to follow so I can prove that a finite subset is a generator? Here is the background story (I'll tell it all because I suck at maths and I might have understood the whole concept wrongly): I want to know how to compute the image of a vector space morphism. I'll continue on a concrete example: $f\colon \mathbb{R}^2 \to \mathbb{R}^2$, $f(x,y) = (x-y, 2x + y)$ is the morphism. To find its image, I observe that $f(x,y)$ can be rewritten as $x(1,2) + y(-1,1)$. If I am correct, that means that $\mathrm{Im}(f) = \langle(1,2), (-1,1)\rangle$ (that's the notation we use for set generators). Now the question is: does the $\{(1,2), (-1,1)\}$ set generate $\mathbb{R}^2$ or not?","I have a very noob questions about generators: what algorithm do I have to follow so I can prove that a finite subset is a generator? Here is the background story (I'll tell it all because I suck at maths and I might have understood the whole concept wrongly): I want to know how to compute the image of a vector space morphism. I'll continue on a concrete example: $f\colon \mathbb{R}^2 \to \mathbb{R}^2$, $f(x,y) = (x-y, 2x + y)$ is the morphism. To find its image, I observe that $f(x,y)$ can be rewritten as $x(1,2) + y(-1,1)$. If I am correct, that means that $\mathrm{Im}(f) = \langle(1,2), (-1,1)\rangle$ (that's the notation we use for set generators). Now the question is: does the $\{(1,2), (-1,1)\}$ set generate $\mathbb{R}^2$ or not?",,"['linear-algebra', 'vector-spaces']"
7,Finding kernel and image of a linear transformation (polynomial),Finding kernel and image of a linear transformation (polynomial),,"I have the following linear transformation: $D : V(\mathbb{R})\rightarrow V(\mathbb{R}) $ defined by: $$ D:p(x)=ax^3 +bx^2 + cx + d \mapsto Dp(x) = 3ax^2 + 2bx + c$$ And I want to find the kernel and the image of the transformation. Regarding the kernel, I have done the following $$\operatorname{ker}(D) =  \{ Dp(x) = 0 \mid x \in V\} $$ $$ p(x)=ax^3 +bx^2 + cx + d = Dp(x) = 3ax^2 + 2bx + c = 0 $$ Which can be solved through the quadratic formula. Is the answer then: $$\operatorname{ker}(D) = \left\{\frac{-b\pm\sqrt{b^2-3ac}}{3a} \right\}$$ And what about the image? I know that $\operatorname{Im}(D) = \{ Dp(x) \mid x \in V \}$ , but how can I show it?","I have the following linear transformation: defined by: And I want to find the kernel and the image of the transformation. Regarding the kernel, I have done the following Which can be solved through the quadratic formula. Is the answer then: And what about the image? I know that , but how can I show it?",D : V(\mathbb{R})\rightarrow V(\mathbb{R})   D:p(x)=ax^3 +bx^2 + cx + d \mapsto Dp(x) = 3ax^2 + 2bx + c \operatorname{ker}(D) =  \{ Dp(x) = 0 \mid x \in V\}   p(x)=ax^3 +bx^2 + cx + d = Dp(x) = 3ax^2 + 2bx + c = 0  \operatorname{ker}(D) = \left\{\frac{-b\pm\sqrt{b^2-3ac}}{3a} \right\} \operatorname{Im}(D) = \{ Dp(x) \mid x \in V \},"['linear-algebra', 'linear-transformations']"
8,Can't use Stolz - Cesaro theorem because the condition that the limit exists can't be proven,Can't use Stolz - Cesaro theorem because the condition that the limit exists can't be proven,,"So I found this question: If $\lim_{n \to \infty} \frac{c_{n+1}}{n \cdot c_n} = a$ then prove that $\lim_{n \to \infty} \sqrt[n+1]{c_{n+1}} - \sqrt[n]{c_n} = \frac{a}{e}$ Now I used the Stolz - Cesaro theorem to prove this by the following method: Let $a_{n+1} = \sqrt[n+1]{c_{n+1}}$ and $b_{n+1} = n+1$ Thus $\lim_{n \to \infty} \sqrt[n+1]{c_{n+1}} - \sqrt[n]{c_n} = \frac{a_{n+1}-a_n}{b_{n+1}-b_n} = \frac{a_n}{b_n} = \frac{\sqrt[n]{c_n}}{n} = \sqrt[n]{\frac{c_n}{n^n}}$ Let $x_n = \frac{c_n}{n^n}$ then using the converse of the geometric mean case of Stolz - Cesaro theorem $(\lim_{n \to \infty} \sqrt[n]y_n = L \implies \lim_{n \to \infty}\frac{y_{n+1}}{y_n} = L$ ) we get $\sqrt[n]{\frac{c_n}{n^n}} = \frac {c_{n+1} \cdot n^n}{c_n \cdot (n+1)^{n+1}} = \frac {c_{n+1}}{n \cdot c_n} \cdot \frac{1}{(1 + \frac{1}{n})^{n+1}}$ I don't know if the converse of the multiplicative identity of the Stolz - Cesaro theorem can be used or not but I could not find another way to solve this question. Can you also tell me if the converse of the multiplicative identity of the Stolz - Cesaro theorem exists here ? we know $(1 + \frac{1}{n})^n = (1 + \frac{1}{n})^{n+1} = e\;\&\;(\lim_{n \to \infty} (1+\frac{1}{n}) â†’ 1)$ and it is given that $\frac {c_{n+1}}{n \cdot c_n} = a$ So we get that $\lim_{n \to \infty} \sqrt[n+1]{c_{n+1}} - \sqrt[n]{c_n} = \frac{a}{e}$ Obviously the problem (ignoring the usage of the converse of the multiplicative identity of the Stolz - Cesaro theorem) is we have to prove that $\frac{a_{n+1}-a_n}{b_{n+1}-b_n}$ exists to use Stolz - Cesaro theorem but I cannot find a way to prove how to do it, I just can't get where to start to prove that it exists. It intuitively looks like it should exist. So my question is : can we prove that it exists and if not, then how would we legitimately solve this question? P.S. : I saw this question but that solution has a different approach and also my doubt is a little different because I want to use the method stated above. To prove that, we had to prove that $\lim_{n \to \infty} \sqrt[n+1]{c_{n+1}} - \sqrt[n]{c_n}$ exists first which I am not able to do.","So I found this question: If then prove that Now I used the Stolz - Cesaro theorem to prove this by the following method: Let and Thus Let then using the converse of the geometric mean case of Stolz - Cesaro theorem ) we get I don't know if the converse of the multiplicative identity of the Stolz - Cesaro theorem can be used or not but I could not find another way to solve this question. Can you also tell me if the converse of the multiplicative identity of the Stolz - Cesaro theorem exists here ? we know and it is given that So we get that Obviously the problem (ignoring the usage of the converse of the multiplicative identity of the Stolz - Cesaro theorem) is we have to prove that exists to use Stolz - Cesaro theorem but I cannot find a way to prove how to do it, I just can't get where to start to prove that it exists. It intuitively looks like it should exist. So my question is : can we prove that it exists and if not, then how would we legitimately solve this question? P.S. : I saw this question but that solution has a different approach and also my doubt is a little different because I want to use the method stated above. To prove that, we had to prove that exists first which I am not able to do.",\lim_{n \to \infty} \frac{c_{n+1}}{n \cdot c_n} = a \lim_{n \to \infty} \sqrt[n+1]{c_{n+1}} - \sqrt[n]{c_n} = \frac{a}{e} a_{n+1} = \sqrt[n+1]{c_{n+1}} b_{n+1} = n+1 \lim_{n \to \infty} \sqrt[n+1]{c_{n+1}} - \sqrt[n]{c_n} = \frac{a_{n+1}-a_n}{b_{n+1}-b_n} = \frac{a_n}{b_n} = \frac{\sqrt[n]{c_n}}{n} = \sqrt[n]{\frac{c_n}{n^n}} x_n = \frac{c_n}{n^n} (\lim_{n \to \infty} \sqrt[n]y_n = L \implies \lim_{n \to \infty}\frac{y_{n+1}}{y_n} = L \sqrt[n]{\frac{c_n}{n^n}} = \frac {c_{n+1} \cdot n^n}{c_n \cdot (n+1)^{n+1}} = \frac {c_{n+1}}{n \cdot c_n} \cdot \frac{1}{(1 + \frac{1}{n})^{n+1}} (1 + \frac{1}{n})^n = (1 + \frac{1}{n})^{n+1} = e\;\&\;(\lim_{n \to \infty} (1+\frac{1}{n}) â†’ 1) \frac {c_{n+1}}{n \cdot c_n} = a \lim_{n \to \infty} \sqrt[n+1]{c_{n+1}} - \sqrt[n]{c_n} = \frac{a}{e} \frac{a_{n+1}-a_n}{b_{n+1}-b_n} \lim_{n \to \infty} \sqrt[n+1]{c_{n+1}} - \sqrt[n]{c_n},"['real-analysis', 'calculus', 'linear-algebra', 'limits']"
9,Does matrix multiplication require an inner product space?,Does matrix multiplication require an inner product space?,,"Does matrix multiplication require an inner product space? It would seem to me that since multiplying a vector by a matrix is simply a linear map, and multiplying two matrices the composition of a linear maps, these must be defined in any vector space, and should not depend on an inner product. Yet, any definition of matrix multiplication I can consider immediately induces an inner product.  I understand that you can multiply a vector by a matrix without using an inner product, but, once you do so, you immediately define an inner product. So: Can matrix multiplication exist in vector spaces without inner products? What is the relationship between the two?","Does matrix multiplication require an inner product space? It would seem to me that since multiplying a vector by a matrix is simply a linear map, and multiplying two matrices the composition of a linear maps, these must be defined in any vector space, and should not depend on an inner product. Yet, any definition of matrix multiplication I can consider immediately induces an inner product.  I understand that you can multiply a vector by a matrix without using an inner product, but, once you do so, you immediately define an inner product. So: Can matrix multiplication exist in vector spaces without inner products? What is the relationship between the two?",,"['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations', 'inner-products']"
10,When is the product of two matrices diagonalizable?,When is the product of two matrices diagonalizable?,,"Say, I have two square matrices, $A$ and $B$ , not necessarily Hermitian, whose eigenvalues and eigenvectors are known, and I also know if they are diagonalizable. Is there a way to figure out if their product $AB$ is diagonalizable without explicitly calculating the eigenvalues and eigenvectors of $AB$ ?","Say, I have two square matrices, and , not necessarily Hermitian, whose eigenvalues and eigenvectors are known, and I also know if they are diagonalizable. Is there a way to figure out if their product is diagonalizable without explicitly calculating the eigenvalues and eigenvectors of ?",A B AB AB,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
11,Prove that $\dim \ker (A-I_{n}) = \frac{1}{d} \sum\limits_{i= 1}^{d}\mbox{Tr} \left(A^{i}\right)$,Prove that,\dim \ker (A-I_{n}) = \frac{1}{d} \sum\limits_{i= 1}^{d}\mbox{Tr} \left(A^{i}\right),Let $A \in M_{n}(\mathbb{C})$ be a matrix such that $A^{d} = I_{n}$ for some positive integer $d$ . Prove that $$\dim \ker (A-I_{n}) = \frac{1}{d} \sum\limits_{i= 1}^{d}\mbox{Tr} \left(A^{i}\right)$$ My Attempt: $A$ is diagonalizable as minimal polynomial of $A$ is product of distinct linear factor. I think diagonalizability of $A$ can help to prove this identity. Can anyone help me for further approach?,Let be a matrix such that for some positive integer . Prove that My Attempt: is diagonalizable as minimal polynomial of is product of distinct linear factor. I think diagonalizability of can help to prove this identity. Can anyone help me for further approach?,A \in M_{n}(\mathbb{C}) A^{d} = I_{n} d \dim \ker (A-I_{n}) = \frac{1}{d} \sum\limits_{i= 1}^{d}\mbox{Tr} \left(A^{i}\right) A A A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
12,"Given $A,B$ are $n \times n$ matrices, $(A+B)^2=A+B,r(A+B)=r(A)+r(B)$. Prove: $A^2=A,B^2=B$","Given  are  matrices, . Prove:","A,B n \times n (A+B)^2=A+B,r(A+B)=r(A)+r(B) A^2=A,B^2=B","Given $A,B$ are $n \times n$ matrices, $(A+B)^2=A+B,r(A+B)=r(A)+r(B)$ . Prove: $A^2=A,B^2=B$ I don't know how to use $r(A+B)=r(A)+r(B)$","Given are matrices, . Prove: I don't know how to use","A,B n \times n (A+B)^2=A+B,r(A+B)=r(A)+r(B) A^2=A,B^2=B r(A+B)=r(A)+r(B)",['linear-algebra']
13,Determinant of $A+A^T$ is an odd integer if $\text{det}(A-A^T)=1$.,Determinant of  is an odd integer if .,A+A^T \text{det}(A-A^T)=1,"Let $A\in\text{Mat}(2n\times 2n;\mathbb{Z})$ be an integer matrix such that $\text{det}(A-A^T)=1$ . I want to show that $\text{det}(A+A^T)$ is an odd integer. Murasugi claims in his book ""Knot Theory and its Applications"" that this is trivial and it probably follows immeditiately from some determinant property of skew-symmetric resp. symmetric matrices. But I just can not proof it. Any help would be greatly appreciated!","Let be an integer matrix such that . I want to show that is an odd integer. Murasugi claims in his book ""Knot Theory and its Applications"" that this is trivial and it probably follows immeditiately from some determinant property of skew-symmetric resp. symmetric matrices. But I just can not proof it. Any help would be greatly appreciated!",A\in\text{Mat}(2n\times 2n;\mathbb{Z}) \text{det}(A-A^T)=1 \text{det}(A+A^T),"['linear-algebra', 'matrices', 'matrix-rank']"
14,How to show that linear map is surjective?,How to show that linear map is surjective?,,"I have the following linear map: $$T:\mathcal{P}^2 \to \Bbb{R}^2 $$ where $\mathcal{P}^2$ denotes the vector space of polynomials  with real coefficients having degree at most $2$ $ T $ is defined  by $$T(ax^2+bx+c)=[a+b , bâˆ’c]^t$$ I do not know how to prove that $T$ is surjective. I know its not injective. Yet,i do not now how to formally show that is a surjection. I tried following the answer at How to show that a linear map is surjective? Using the formula described: $\dim(V) = 2$ , $\dim(\operatorname{range}T)=1$ According to the formula ""will be surjective if $\dim V= \dim \operatorname{range} T$ "" this map would not be surjective. Yet, in the website i took this exercise from, it says this map is a surjective. So, i am wondering what i did wrong, and if there are better ways to show that this map is a surjective. I came across on this exercise on this website.","I have the following linear map: where denotes the vector space of polynomials  with real coefficients having degree at most is defined  by I do not know how to prove that is surjective. I know its not injective. Yet,i do not now how to formally show that is a surjection. I tried following the answer at How to show that a linear map is surjective? Using the formula described: , According to the formula ""will be surjective if "" this map would not be surjective. Yet, in the website i took this exercise from, it says this map is a surjective. So, i am wondering what i did wrong, and if there are better ways to show that this map is a surjective. I came across on this exercise on this website.","T:\mathcal{P}^2 \to \Bbb{R}^2  \mathcal{P}^2 2  T  T(ax^2+bx+c)=[a+b , bâˆ’c]^t T \dim(V) = 2 \dim(\operatorname{range}T)=1 \dim V= \dim \operatorname{range} T","['linear-algebra', 'vector-spaces', 'linear-transformations', 'systems-of-equations']"
15,Derivatives of a quadratic form.,Derivatives of a quadratic form.,,I have this quadratic function $$\bar x^T Q \bar x$$ and I need to find the first and second derivative. My question is how can I do that by just using the matrix from? I've been multiplying $Q$ by $\bar x$ and then taking it back to matrix form but this is too impractical. Thanks for the help.,I have this quadratic function and I need to find the first and second derivative. My question is how can I do that by just using the matrix from? I've been multiplying by and then taking it back to matrix form but this is too impractical. Thanks for the help.,\bar x^T Q \bar x Q \bar x,"['calculus', 'linear-algebra', 'derivatives']"
16,Is there an intuitive way to understand why $\det(AB)=\det(A)\det(B)\;$?,Is there an intuitive way to understand why ?,\det(AB)=\det(A)\det(B)\;,"Let $A, B \in L(V)$ (or equivalently $n \times n$ matrix), where $V$ is a $n$ -dimensional vector space. There are a multiple of proofs why $\det(A)\det(B) = \det(AB)$ , but I couldn't find a satisfactory that is intuition-appealing. First, the most imminent motivation for determinant is volume: $\det(A)$ is oriented volume of parallelepiped consisting $n$ -column vectors of $A$ . Given three matrices $A, B, AB$ , we have three distinct parallelepiped each of which has oriented volume $\det(A)$ , $\det(B)$ , and $\det(AB)$ , respectively. Second, the given $A, B \in L(V)$ as in linear transformation, $AB$ is simply a composition of linear transformation. I am trying to relate these two concepts, but it feels that the gap between two concepts are too broad. Is there a nice interpretation that fills the gap between these two concepts ?","Let (or equivalently matrix), where is a -dimensional vector space. There are a multiple of proofs why , but I couldn't find a satisfactory that is intuition-appealing. First, the most imminent motivation for determinant is volume: is oriented volume of parallelepiped consisting -column vectors of . Given three matrices , we have three distinct parallelepiped each of which has oriented volume , , and , respectively. Second, the given as in linear transformation, is simply a composition of linear transformation. I am trying to relate these two concepts, but it feels that the gap between two concepts are too broad. Is there a nice interpretation that fills the gap between these two concepts ?","A, B \in L(V) n \times n V n \det(A)\det(B) = \det(AB) \det(A) n A A, B, AB \det(A) \det(B) \det(AB) A, B \in L(V) AB","['linear-algebra', 'matrices']"
17,What does it mean for an inner product to induce a norm?,What does it mean for an inner product to induce a norm?,,"I'm working on a problem set and am trying to understand the concept of ""induced norms"": Let's say I have a space of $M \times N$ matrices (I'm interpreting this as the collection of all real-valued $M \times N$ matrices). Let's say I want to convert this space into an inner-product space using some inner product $\langle A, B\rangle$ . I now have some inner-product vector space where each matrix pair has an associated value produced by the inner product. For those interested, the provided inner product is $\operatorname{trace}(A^{T}B)$ . The problem then goes on to state that the provided inner product induces a norm. What does it mean to induce a norm? And is this an induced norm on the original $M \times N$ matrix space? Or the inner-product space?","I'm working on a problem set and am trying to understand the concept of ""induced norms"": Let's say I have a space of matrices (I'm interpreting this as the collection of all real-valued matrices). Let's say I want to convert this space into an inner-product space using some inner product . I now have some inner-product vector space where each matrix pair has an associated value produced by the inner product. For those interested, the provided inner product is . The problem then goes on to state that the provided inner product induces a norm. What does it mean to induce a norm? And is this an induced norm on the original matrix space? Or the inner-product space?","M \times N M \times N \langle A, B\rangle \operatorname{trace}(A^{T}B) M \times N","['linear-algebra', 'matrices', 'inner-products', 'matrix-norms']"
18,Basis of $\{0\}$ set,Basis of  set,\{0\},"I am solving Linear Algebra and having a trivial doubt . Is W ={âˆ…} i.e. an empty set a basis of ð‘‰={0} ? I have read some solutions regarding the above and they imply that since W contains no vector , W  by definition is linearly independent . I am not sure how W spans V. Can anyone explain this to me ? ""Every vector space has a basis."" Is the above statement true ?","I am solving Linear Algebra and having a trivial doubt . Is W ={âˆ…} i.e. an empty set a basis of ð‘‰={0} ? I have read some solutions regarding the above and they imply that since W contains no vector , W  by definition is linearly independent . I am not sure how W spans V. Can anyone explain this to me ? ""Every vector space has a basis."" Is the above statement true ?",,"['linear-algebra', 'vector-spaces']"
19,"If $A^m = 0$, then $\mbox{rank}(A) \leq \frac{m-1}{m}{n}$ [duplicate]","If , then  [duplicate]",A^m = 0 \mbox{rank}(A) \leq \frac{m-1}{m}{n},"This question already has answers here : Upper bound for the rank of a nilpotent matrix , if $A^2 \ne 0$ (4 answers) Closed 3 years ago . Let $A$ be a $n \times n$ real matrix. Show that if $A^m = 0$ , then $\mbox{rank}(A) \leq \frac{m-1}{m}{n}$ My attempt: If $m=1$ , then $A=0$ so $\mbox{rank}(A)=0$ . If $m=2$ , we have $\mbox{im}(A) \subset \ker(A)$ so $2\operatorname{rank}(A) \leq \dim \mbox{im}(A) + \dim \ker(A)=n$ For arbitrary $m$ , I want to use induction. $B=A|_{\mbox{im}(A)}$ satisfies $B^{m-1}=0$ so $\mbox{rank}(B) \leq \frac{m-2}{m-1}\mbox{rank}(A)$ . Thus $\dim \ker B \geq \mbox{rank}(A)- \frac{m-2}{m-1}\mbox{rank}(A) =\frac{1}{m-1}\mbox{rank}(A)$ by rank-nullity theorem. Thus $n=\dim \ker A + \mbox{rank}(A) \geq  \dim \ker B +\mbox{rank}(A)\geq \frac m {m-1}\mbox{rank}(A)$ Is this ok?","This question already has answers here : Upper bound for the rank of a nilpotent matrix , if $A^2 \ne 0$ (4 answers) Closed 3 years ago . Let be a real matrix. Show that if , then My attempt: If , then so . If , we have so For arbitrary , I want to use induction. satisfies so . Thus by rank-nullity theorem. Thus Is this ok?",A n \times n A^m = 0 \mbox{rank}(A) \leq \frac{m-1}{m}{n} m=1 A=0 \mbox{rank}(A)=0 m=2 \mbox{im}(A) \subset \ker(A) 2\operatorname{rank}(A) \leq \dim \mbox{im}(A) + \dim \ker(A)=n m B=A|_{\mbox{im}(A)} B^{m-1}=0 \mbox{rank}(B) \leq \frac{m-2}{m-1}\mbox{rank}(A) \dim \ker B \geq \mbox{rank}(A)- \frac{m-2}{m-1}\mbox{rank}(A) =\frac{1}{m-1}\mbox{rank}(A) n=\dim \ker A + \mbox{rank}(A) \geq  \dim \ker B +\mbox{rank}(A)\geq \frac m {m-1}\mbox{rank}(A),"['linear-algebra', 'inequality', 'solution-verification', 'matrix-rank']"
20,Inverse of block anti-diagonal matrix,Inverse of block anti-diagonal matrix,,"Let $A \in \mathbb R^{n\times n}$ be an invertible block anti-diagonal matrix (with $d$ blocks), i.e. $$ A = \begin{pmatrix} & & & A_1 \\ & & A_2 & \\ & \cdot^{\textstyle \cdot^{\textstyle \cdot}} &  & \\ A_d\end{pmatrix}, $$ with all square blocks $A_1, \ldots, A_d$ invertible. Is there a formula for its inverse? In the diagonal case, it is just the diagonal block matrix with the inverses of the blocks, is there an equivalent for the anti-diagonal case?","Let be an invertible block anti-diagonal matrix (with blocks), i.e. with all square blocks invertible. Is there a formula for its inverse? In the diagonal case, it is just the diagonal block matrix with the inverses of the blocks, is there an equivalent for the anti-diagonal case?","A \in \mathbb R^{n\times n} d 
A = \begin{pmatrix} & & & A_1 \\ & & A_2 & \\ & \cdot^{\textstyle \cdot^{\textstyle \cdot}} &  & \\ A_d\end{pmatrix},
 A_1, \ldots, A_d","['linear-algebra', 'matrices', 'inverse', 'block-matrices']"
21,Show that $A^4=A^2$ for an adjacency matrix with spectral radius $\leq1$,Show that  for an adjacency matrix with spectral radius,A^4=A^2 \leq1,"Let $\Gamma$ be a finite, undirected graph. Let $A=(a_{ij})$ be its adjacency matrix (that is $a_{ij}=$ number of edges from vertex $v_i$ to vertex $v_j$ ) and thus $A$ is symmetric. Show that if the spectral radius of $A$ is less than or equal to $1$ , then $A^4=A^2$ . My try: Since $A$ is symmetric, eigenvectors $\{v_i\}_{i=1}^n$ of $A$ is a basis. To show $A^4=A^2$ , we only need to show that for any $i,j$ , $$ (A^4v_i,v_j)=(A^2v_i,v_j). $$ This is equivalent to $$ \lambda_i^4(v_i,v_j)=\lambda_i^2(v_i,v_j). $$ If $i=j$ , then we should have $$\lambda_i^4=\lambda_i^2.$$ But why this holds?","Let be a finite, undirected graph. Let be its adjacency matrix (that is number of edges from vertex to vertex ) and thus is symmetric. Show that if the spectral radius of is less than or equal to , then . My try: Since is symmetric, eigenvectors of is a basis. To show , we only need to show that for any , This is equivalent to If , then we should have But why this holds?","\Gamma A=(a_{ij}) a_{ij}= v_i v_j A A 1 A^4=A^2 A \{v_i\}_{i=1}^n A A^4=A^2 i,j 
(A^4v_i,v_j)=(A^2v_i,v_j).
 
\lambda_i^4(v_i,v_j)=\lambda_i^2(v_i,v_j).
 i=j \lambda_i^4=\lambda_i^2.","['linear-algebra', 'matrices', 'spectral-graph-theory', 'adjacency-matrix', 'spectral-radius']"
22,On the existence a symmetric positive definite matrix,On the existence a symmetric positive definite matrix,,"Let $u,v\in\mathbb{R}^n$ be such that $\langle u,v\rangle>0.$ My question is whether or not there exists a symmetric positive definite matrix $Q$ such that $v=Qu$ . If such a matrix $Q$ exists, how to construct $Q$ from $u,v$ ? Thank you for all solutions.","Let be such that My question is whether or not there exists a symmetric positive definite matrix such that . If such a matrix exists, how to construct from ? Thank you for all solutions.","u,v\in\mathbb{R}^n \langle u,v\rangle>0. Q v=Qu Q Q u,v",['linear-algebra']
23,How to show that $f:V\to V$ is linear?,How to show that  is linear?,f:V\to V,"Let $(V,|.|)$ be a normed finite dimensional vector space and $f:V\to V$ a map with the following property: $|f(y)|=|f(x+y)-f(x)|,\quad \forall x, y\in V.$ Then how to prove that $f$ is linear? Update: what can be say if $V$ is a real vector space?",Let be a normed finite dimensional vector space and a map with the following property: Then how to prove that is linear? Update: what can be say if is a real vector space?,"(V,|.|) f:V\to V |f(y)|=|f(x+y)-f(x)|,\quad \forall x, y\in V. f V","['linear-algebra', 'vector-spaces', 'linear-transformations', 'normed-spaces']"
24,A question of non-singularity,A question of non-singularity,,"Let $A$ and $B$ be matrices such that $B^2+ AB + 2I = 0$ , where I denotes the identity matrix. Which of the following matrices must be nonsingular? (A) $A + 2I$ (B) $B$ (C) $B + 2I$ (D) $A$ I tried using a few tricks assuming each option to be nonsingular and then coming to the given form but to no avail. Any hint is appreciated.","Let and be matrices such that , where I denotes the identity matrix. Which of the following matrices must be nonsingular? (A) (B) (C) (D) I tried using a few tricks assuming each option to be nonsingular and then coming to the given form but to no avail. Any hint is appreciated.",A B B^2+ AB + 2I = 0 A + 2I B B + 2I A,"['linear-algebra', 'matrices', 'determinant']"
25,Change of basis of a linear map defined by non-square matrix,Change of basis of a linear map defined by non-square matrix,,"The given: Let a linear map $L : V â†’ U$ be given in the basis $(e_1, e_2, e_3)$ of $V$ and in the basis $(f_1, f_2)$ of $U$ by $\begin{pmatrix} 0 & 1 & 2 \\  3 & 4 & 5 \end{pmatrix}$ .  Find the matrix of $L$ with respect to the bases $(e_1, e_1 + e_2, e_1 + e_2 + e_3)$ and $(f_1, f_1 + f_2)$ . Now I know I am being stupid in some way, but I can't make this work. I want to say: we have new bases $(e_1,e_1+e_2,e_1+e_2+e_3)$ and $(f_1,f_1+f_2)$ which correspond to $C = \begin{pmatrix}   1 & 1 & 1  \\   0 & 1 & 1  \\   0 & 0 & 1 \end{pmatrix}$ and $D = \begin{pmatrix}   1 & 1  \\   0 & 1    \end{pmatrix}$ respectively. And Using our change of bases formula, one version of $L_1' = C^TAC$ and the other is $L_2' = D^TAD$ . But here I run into a problem of dimension, the matrix multiplication does not work. I had thought perhaps to try $L'=DAC$ , since this expression has workable dimension, but that's my only reason for trying it. The basis change examples I've seen before involve expressions with one matrix and either its transpose or inverse. Apologies again for my stupidity here, and thanks very much in advance for any assistance. Edit : Initial problem had typo on $D$ , one-zero were swapped incorrectly.","The given: Let a linear map be given in the basis of and in the basis of by .  Find the matrix of with respect to the bases and . Now I know I am being stupid in some way, but I can't make this work. I want to say: we have new bases and which correspond to and respectively. And Using our change of bases formula, one version of and the other is . But here I run into a problem of dimension, the matrix multiplication does not work. I had thought perhaps to try , since this expression has workable dimension, but that's my only reason for trying it. The basis change examples I've seen before involve expressions with one matrix and either its transpose or inverse. Apologies again for my stupidity here, and thanks very much in advance for any assistance. Edit : Initial problem had typo on , one-zero were swapped incorrectly.","L : V â†’ U (e_1, e_2, e_3) V (f_1, f_2) U \begin{pmatrix} 0 & 1 & 2 \\ 
3 & 4 & 5 \end{pmatrix} L (e_1, e_1 + e_2, e_1 + e_2 + e_3) (f_1, f_1 + f_2) (e_1,e_1+e_2,e_1+e_2+e_3) (f_1,f_1+f_2) C = \begin{pmatrix}
  1 & 1 & 1  \\
  0 & 1 & 1  \\
  0 & 0 & 1
\end{pmatrix} D = \begin{pmatrix}
  1 & 1  \\
  0 & 1   
\end{pmatrix} L_1' = C^TAC L_2' = D^TAD L'=DAC D","['linear-algebra', 'matrices', 'linear-transformations', 'change-of-basis']"
26,A finite group is isomorphic to a subgroup of $GL_n(\mathbb Z)$,A finite group is isomorphic to a subgroup of,GL_n(\mathbb Z),"I'm trying to prove that any finite group $G$ of order $n\ge 2$ is isomorphic to a subgroup of $GL_n(\mathbb Z)$. My thoughts: By Cayley, $G$ is isomorphic to a subgroup of $S_n$. Now I need to establish somehow a connection between $S_n$ and $GL_n(\mathbb Z)$. In the case of $n$ small, say $n=2$, $S_3$ acts on $\{e_1,e_2,e_1+e_2\}$, and this action is faithful, which gives an injective homomorphism $GL_2(\mathbb Z)\to S_3$. But a) I guess I need a homomorphism to the other direction (in this case this is an isomorphism, but for larger $n$ it is not I believe) and b) I don't know how to generalize this for larger $n$.","I'm trying to prove that any finite group $G$ of order $n\ge 2$ is isomorphic to a subgroup of $GL_n(\mathbb Z)$. My thoughts: By Cayley, $G$ is isomorphic to a subgroup of $S_n$. Now I need to establish somehow a connection between $S_n$ and $GL_n(\mathbb Z)$. In the case of $n$ small, say $n=2$, $S_3$ acts on $\{e_1,e_2,e_1+e_2\}$, and this action is faithful, which gives an injective homomorphism $GL_2(\mathbb Z)\to S_3$. But a) I guess I need a homomorphism to the other direction (in this case this is an isomorphism, but for larger $n$ it is not I believe) and b) I don't know how to generalize this for larger $n$.",,"['linear-algebra', 'abstract-algebra', 'group-theory']"
27,Show that $B$ $\in$ $GL_{n-1}($K$)$ $\Rightarrow$ $A$ $\in$ $GL_n$($K$),Show that   K    (),B \in GL_{n-1}( ) \Rightarrow A \in GL_n K,"Let $K$ be a field and A = $   \begin{bmatrix} -1 & A_{12} \\  0 & B \\  \end{bmatrix} $  $\in$ $K^{n,n}$ and $B$ $\in$  $K^{n-1,n-1}$. Show that $A$ $\in$ $GL_n$($K$) applies if $B$ $\in$ $GL_{n-1}($K$)$. My thoughts and questions : $B$ is obviously  a (n - 1) $\times$ (n - 1) matrix in the field $\mathbb{K}$. For example: For n = 4 is $B$ a $\underbrace{3}_{4-1}$ $\times$ $\underbrace{3}_{4-1}$ matrix in the field $\mathbb{K}$. But generally I don't understand how $A$ $\in$ $GL_n$($K$) applies if $B$ $\in$ $GL_{n-1}($K$)$. I thought about using this lemma: Let $A$ $\in$ $K^{n,m}$ with $A$ = $ \begin{bmatrix} A_{1} \\ A_{2} \\ \end{bmatrix}$ which means that $A_{1}$ $\in$ $K^{l,m}$, $A_{2}$ $\in$ $K^{n-l,m}$. Therefore we have that Rank(A) $\le$ Rank($A_{1}$) + Rank($A_{2}$). But it is made for  $A$ $\in$ $K^{n,m}$ and not for $A$ $\in$ $K^{n,n}$. How do I beginn this proof? and how does  $A$ $\in$ $GL_n$($K$) applies if $B$ $\in$ $GL_{n-1}($K$)$? (I really don't why this correct). Any hints guiding me to the right direction I much appreciate.","Let $K$ be a field and A = $   \begin{bmatrix} -1 & A_{12} \\  0 & B \\  \end{bmatrix} $  $\in$ $K^{n,n}$ and $B$ $\in$  $K^{n-1,n-1}$. Show that $A$ $\in$ $GL_n$($K$) applies if $B$ $\in$ $GL_{n-1}($K$)$. My thoughts and questions : $B$ is obviously  a (n - 1) $\times$ (n - 1) matrix in the field $\mathbb{K}$. For example: For n = 4 is $B$ a $\underbrace{3}_{4-1}$ $\times$ $\underbrace{3}_{4-1}$ matrix in the field $\mathbb{K}$. But generally I don't understand how $A$ $\in$ $GL_n$($K$) applies if $B$ $\in$ $GL_{n-1}($K$)$. I thought about using this lemma: Let $A$ $\in$ $K^{n,m}$ with $A$ = $ \begin{bmatrix} A_{1} \\ A_{2} \\ \end{bmatrix}$ which means that $A_{1}$ $\in$ $K^{l,m}$, $A_{2}$ $\in$ $K^{n-l,m}$. Therefore we have that Rank(A) $\le$ Rank($A_{1}$) + Rank($A_{2}$). But it is made for  $A$ $\in$ $K^{n,m}$ and not for $A$ $\in$ $K^{n,n}$. How do I beginn this proof? and how does  $A$ $\in$ $GL_n$($K$) applies if $B$ $\in$ $GL_{n-1}($K$)$? (I really don't why this correct). Any hints guiding me to the right direction I much appreciate.",,"['linear-algebra', 'matrices', 'matrix-rank']"
28,"Determinant of a matrix of ones, whose anti diagonal elements are zero","Determinant of a matrix of ones, whose anti diagonal elements are zero",,"I'm trying to prove a formula I have constructed for the determinant of a general $n\times n $ real matrix $A$, given here in the case $n=5$: $$ A = \begin{bmatrix} 1 & 1 & 1 & 1 & 0 \\ 1 & 1 & 1 & 0 & 1 \\ 1 & 1 & 0 & 1 & 1 \\ 1 & 0 & 1 & 1 & 1 \\ 0 & 1 & 1 & 1 & 1 \end{bmatrix}. $$ That is, the matrix containing all 1's apart from the anti-diagonal which consists of zeros. Using a simple matlab code I've computed the determinants for the first few values of $n$, and have come to the formula $$ \det A = \begin{cases} -(n-1) \hspace{1em}\mbox{ if }\,\,\, n \equiv -1,0\mod 4, \\ n-1 \hspace{2.2em}\mbox{ otherwise,} \end{cases} $$ but I'm unsure where to start to prove this.","I'm trying to prove a formula I have constructed for the determinant of a general $n\times n $ real matrix $A$, given here in the case $n=5$: $$ A = \begin{bmatrix} 1 & 1 & 1 & 1 & 0 \\ 1 & 1 & 1 & 0 & 1 \\ 1 & 1 & 0 & 1 & 1 \\ 1 & 0 & 1 & 1 & 1 \\ 0 & 1 & 1 & 1 & 1 \end{bmatrix}. $$ That is, the matrix containing all 1's apart from the anti-diagonal which consists of zeros. Using a simple matlab code I've computed the determinants for the first few values of $n$, and have come to the formula $$ \det A = \begin{cases} -(n-1) \hspace{1em}\mbox{ if }\,\,\, n \equiv -1,0\mod 4, \\ n-1 \hspace{2.2em}\mbox{ otherwise,} \end{cases} $$ but I'm unsure where to start to prove this.",,"['linear-algebra', 'matrices', 'determinant']"
29,Prove the following determinant,Prove the following determinant,,Prove the following: $$\left| \begin{matrix} (b+c)&a&a \\ b&(c+a)&b \\ c&c&(a+b) \\ \end{matrix}\right|=4abc$$ My Attempt: $$\left| \begin{matrix} (b+c)&a&a \\ b&(c+a)&b \\ c&c&(a+b) \\ \end{matrix}\right|$$ Using $R_1\to R_1+R_2+R_3$ $$\left | \begin{matrix} 2(b+c)&2(a+c)&2(a+b) \\ b&(c+a)&b \\ c&c&(a+b) \\ \end{matrix}\right|$$ Taking common $2$ from $R_1$  $$2\left| \begin{matrix} (b+c)&(a+c)&(a+b) \\ b&(c+a)&b \\ c&c&(a+b) \\ \end{matrix}\right|$$ How do I proceed further?,Prove the following: $$\left| \begin{matrix} (b+c)&a&a \\ b&(c+a)&b \\ c&c&(a+b) \\ \end{matrix}\right|=4abc$$ My Attempt: $$\left| \begin{matrix} (b+c)&a&a \\ b&(c+a)&b \\ c&c&(a+b) \\ \end{matrix}\right|$$ Using $R_1\to R_1+R_2+R_3$ $$\left | \begin{matrix} 2(b+c)&2(a+c)&2(a+b) \\ b&(c+a)&b \\ c&c&(a+b) \\ \end{matrix}\right|$$ Taking common $2$ from $R_1$  $$2\left| \begin{matrix} (b+c)&(a+c)&(a+b) \\ b&(c+a)&b \\ c&c&(a+b) \\ \end{matrix}\right|$$ How do I proceed further?,,"['linear-algebra', 'matrices', 'algebra-precalculus', 'determinant']"
30,Short way for upper triangularization,Short way for upper triangularization,,"We are given a matrix $$A =  \begin{bmatrix} 3 & 0 & 1 \\ -1 & 4 & -3 \\ -1 & 0 & 5 \\ \end{bmatrix}$$   and we are asked to find a matrix $P$ such that $P^{-1}AP$ is upper triangular. Here, we first find one eigenvalue as $\lambda= 4$. Then the matrix $$ A-4I =  \begin{bmatrix} -1 & 0 & 1 \\ -1 & 0 & -3 \\ -1 & 0 & 1 \\ \end{bmatrix}$$ has basis formed from $f_1 = (-1,-1,-1)^T$, $f_2 = (1,-3,1)^T$. We extend this to a basis of the whole space by adjoining $f_3 = (1,0,0)^T$, and so we have base-change matrix $$ P =  \begin{bmatrix} -1 & 1 & 1 \\ -1 & -3 & 0 \\ -1 & 1 & 0 \\ \end{bmatrix}.$$ Then, by using some computational tools, we find that $$ P^{-1}AP =  \begin{bmatrix} 3 & 1 & 1 \\ -1 & 5 & 0 \\ 0 & 0 & 4 \\ \end{bmatrix}$$ Now, we need to look at $$B = \begin{bmatrix} 3 & 1 \\ -1 & 5 \\ \end{bmatrix},$$ which has eigenvalue $\lambda = 4$. So we have $$B-4I = \begin{bmatrix} -1 & 1 \\ -1 & 1 \\ \end{bmatrix}.$$ So basis for the image of this is $(1,1)^T$. We extend this to the basis $(1,1)^T$, $(1,0)^T$ of $\mathbb{R}^2$. Now, going back to $\mathbb{R}^3$, we have the matrix $$Q = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \\ \end{bmatrix}.$$ Then, by using calculation tools, we get $$Q^{-1}P^{-1}APQ = \begin{bmatrix} 4 & -1 & 0 \\ 0 & 4 & 1 \\ 0 & 0 & 4 \\ \end{bmatrix},$$ which is in upper triangular form. Now, what I wanted to ask is that is there a way to directly find the matrix $R = PQ$ such that $R^{-1}AR$ is upper triangular, without going through these steps?","We are given a matrix $$A =  \begin{bmatrix} 3 & 0 & 1 \\ -1 & 4 & -3 \\ -1 & 0 & 5 \\ \end{bmatrix}$$   and we are asked to find a matrix $P$ such that $P^{-1}AP$ is upper triangular. Here, we first find one eigenvalue as $\lambda= 4$. Then the matrix $$ A-4I =  \begin{bmatrix} -1 & 0 & 1 \\ -1 & 0 & -3 \\ -1 & 0 & 1 \\ \end{bmatrix}$$ has basis formed from $f_1 = (-1,-1,-1)^T$, $f_2 = (1,-3,1)^T$. We extend this to a basis of the whole space by adjoining $f_3 = (1,0,0)^T$, and so we have base-change matrix $$ P =  \begin{bmatrix} -1 & 1 & 1 \\ -1 & -3 & 0 \\ -1 & 1 & 0 \\ \end{bmatrix}.$$ Then, by using some computational tools, we find that $$ P^{-1}AP =  \begin{bmatrix} 3 & 1 & 1 \\ -1 & 5 & 0 \\ 0 & 0 & 4 \\ \end{bmatrix}$$ Now, we need to look at $$B = \begin{bmatrix} 3 & 1 \\ -1 & 5 \\ \end{bmatrix},$$ which has eigenvalue $\lambda = 4$. So we have $$B-4I = \begin{bmatrix} -1 & 1 \\ -1 & 1 \\ \end{bmatrix}.$$ So basis for the image of this is $(1,1)^T$. We extend this to the basis $(1,1)^T$, $(1,0)^T$ of $\mathbb{R}^2$. Now, going back to $\mathbb{R}^3$, we have the matrix $$Q = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \\ \end{bmatrix}.$$ Then, by using calculation tools, we get $$Q^{-1}P^{-1}APQ = \begin{bmatrix} 4 & -1 & 0 \\ 0 & 4 & 1 \\ 0 & 0 & 4 \\ \end{bmatrix},$$ which is in upper triangular form. Now, what I wanted to ask is that is there a way to directly find the matrix $R = PQ$ such that $R^{-1}AR$ is upper triangular, without going through these steps?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'triangularization']"
31,Formula for determinant of sum of matrices,Formula for determinant of sum of matrices,,"Some time ago I came across this apparently quite obscure formula that expands the determinant of a sum of two matrices that I had put on my notes (assuming that I made no errors in my writing): $$\det(A+B)=\det(A)+\det(B)+\text{Tr}(\text{adj}(A)B)$$ Where $\text{adj}()$ denotes the adjugate of the matrix. I cannot seem to find any mention of this formula online. Does anyone know of the name (and maybe a proof) of it? Furthermore, is there any more info on it, like conditions that $A$ and $B$ must obey for it to hold?","Some time ago I came across this apparently quite obscure formula that expands the determinant of a sum of two matrices that I had put on my notes (assuming that I made no errors in my writing): $$\det(A+B)=\det(A)+\det(B)+\text{Tr}(\text{adj}(A)B)$$ Where $\text{adj}()$ denotes the adjugate of the matrix. I cannot seem to find any mention of this formula online. Does anyone know of the name (and maybe a proof) of it? Furthermore, is there any more info on it, like conditions that $A$ and $B$ must obey for it to hold?",,"['linear-algebra', 'matrices', 'determinant']"
32,Positive-definiteness of the Schur Complement,Positive-definiteness of the Schur Complement,,"Let $M$ to be a real-valued symmetric and positive-definite (PD) matrix (also sparse and banded if it helps) $$ M= \begin{bmatrix} A & B\\ B^T & D \end{bmatrix} $$ Under what conditions the Schur complement of $M$ ( $S=D-B^T A^{-1} B$) is PD? As far as I found, it holds if $M$ and $A$ are both PD. If this is true, how can say if $A$ is PD?","Let $M$ to be a real-valued symmetric and positive-definite (PD) matrix (also sparse and banded if it helps) $$ M= \begin{bmatrix} A & B\\ B^T & D \end{bmatrix} $$ Under what conditions the Schur complement of $M$ ( $S=D-B^T A^{-1} B$) is PD? As far as I found, it holds if $M$ and $A$ are both PD. If this is true, how can say if $A$ is PD?",,"['linear-algebra', 'matrices', 'positive-definite', 'schur-complement']"
33,For which real $x$ is the matrix $ A_{ij} = x^{|i-j|}$ invertible?,For which real  is the matrix  invertible?,x  A_{ij} = x^{|i-j|},"For which real $x$ is the $n\times n$ matrix with $ A_{ij} = x^{|i-j|}$  invertible? Omitting the trivial cases $x=1,0$ I thought since it was real symmetric matrix I could diagonalize and look at its eigenvalues, but it quickly got messy and I couldn't come up with a closed solution","For which real $x$ is the $n\times n$ matrix with $ A_{ij} = x^{|i-j|}$  invertible? Omitting the trivial cases $x=1,0$ I thought since it was real symmetric matrix I could diagonalize and look at its eigenvalues, but it quickly got messy and I couldn't come up with a closed solution",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'inverse']"
34,"Prove that the eigenvectors of $(A - pI)^{-1}$ are the same as the eigenvectors of $A$ for real, symmetric $A$","Prove that the eigenvectors of  are the same as the eigenvectors of  for real, symmetric",(A - pI)^{-1} A A,"From the book ""Numerical Linear Algebra"" p. 206: $A$ is a real, symmetric matrix. For any $p\in R$ that is not an eigenvalue of $A$, the eigenvectors of   $(A - pI)^{-1}$ are the same as the eigenvectors of $A$, and the   corresponding eigenvalues  are$ \{(q_j - p)^{-1}\}$, where $\{q_j\}$   are the eigenvalues of $A$ How is this result derived?","From the book ""Numerical Linear Algebra"" p. 206: $A$ is a real, symmetric matrix. For any $p\in R$ that is not an eigenvalue of $A$, the eigenvectors of   $(A - pI)^{-1}$ are the same as the eigenvectors of $A$, and the   corresponding eigenvalues  are$ \{(q_j - p)^{-1}\}$, where $\{q_j\}$   are the eigenvalues of $A$ How is this result derived?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
35,"Given a $2 \times 2$ matrix $B$ that satisfies $B^2=3B-2I$, find the eigenvalues of $B$","Given a  matrix  that satisfies , find the eigenvalues of",2 \times 2 B B^2=3B-2I B,"Given a $2 \times 2$ matrix $B$ that satisfies $B^2=3B-2I$, find the eigenvalues of $B$. My attempt: Let $v$ be an eigenvector for B, and $\lambda$ it's corresponding eigenvalue. Also, let $T$ be the linear transformation (not that this is exactly necessary for the question, but just added it in for my understanding.) Therefore, $$T(v) = Bv = \lambda v$$ Now I'm unsure how to incorporate this information into the quadratic equation given above since by matrix / vector arithmetic isn't extremely solid. Thanks!","Given a $2 \times 2$ matrix $B$ that satisfies $B^2=3B-2I$, find the eigenvalues of $B$. My attempt: Let $v$ be an eigenvector for B, and $\lambda$ it's corresponding eigenvalue. Also, let $T$ be the linear transformation (not that this is exactly necessary for the question, but just added it in for my understanding.) Therefore, $$T(v) = Bv = \lambda v$$ Now I'm unsure how to incorporate this information into the quadratic equation given above since by matrix / vector arithmetic isn't extremely solid. Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations', 'matrix-equations']"
36,Show that the operator has a chain of invariant subspaces.,Show that the operator has a chain of invariant subspaces.,,Let $V$ be a $n$-dimensional vector space over $\Bbb C$ and let $T:V\rightarrow V$ be any linear operator. Show that  $T$ has a chain $V_0\subseteq V_1\subseteq\ldots \subseteq V_n=V$ of invariant subspaces such that $\dim V_i=i$ for $0\le i\le n$. Here a subspace $U$ of $V$ is an invariant subspace of $V$ if $T(U)âŠ†U$ and in this case $T|_U:Uâ†’U$ is a linear operator on $U$. Please help me to solve this.,Let $V$ be a $n$-dimensional vector space over $\Bbb C$ and let $T:V\rightarrow V$ be any linear operator. Show that  $T$ has a chain $V_0\subseteq V_1\subseteq\ldots \subseteq V_n=V$ of invariant subspaces such that $\dim V_i=i$ for $0\le i\le n$. Here a subspace $U$ of $V$ is an invariant subspace of $V$ if $T(U)âŠ†U$ and in this case $T|_U:Uâ†’U$ is a linear operator on $U$. Please help me to solve this.,,"['linear-algebra', 'vector-spaces', 'invariant-subspace']"
37,Dimension of $U+V$,Dimension of,U+V,"Let $U$ and $V$ be the null spaces of $ A=\begin{bmatrix} 1&1&0&0\\0& 0&1&1 \end{bmatrix} $ and $ B=\begin{bmatrix} 1&2&3&2\\0&1&2&1 \end{bmatrix} $. Then what will be the dimension of $U+V.$ I calculated the null space of $U$ and $V$ as follows:  \begin{align*} U=\{x\in \mathbb{R}^4: Ax=0 \}\implies U=\text{span}\{(1,-1,0,0), \ (0,0,1,-1) \}\\ V=\{x\in \mathbb{R}^4: Bx=0 \}\implies V=\text{span}\{(1,-2,1,0), \ (0,-1,0,1) \}\\ \end{align*}  For calculating the dimension of $U+V$, we have to see what is the dimension of $U\cap V$. For this I consider the matrix  $$ \begin{bmatrix} 1&-1&0&0\\0&0&1&-1\\1&-2&1&0\\0&-1&0&1 \end{bmatrix}\sim   \begin{bmatrix} 1&-1&0&0\\0&-1&1&0\\0&0&1&-1\\0&0&0&0 \end{bmatrix} ,$$ which tells that the dimension of $U\cap V=1.$ So, $\dim(U+V)=3$. Now I want to ask is there any shorter method to do this? At least for the dimension of $U\cap V$.","Let $U$ and $V$ be the null spaces of $ A=\begin{bmatrix} 1&1&0&0\\0& 0&1&1 \end{bmatrix} $ and $ B=\begin{bmatrix} 1&2&3&2\\0&1&2&1 \end{bmatrix} $. Then what will be the dimension of $U+V.$ I calculated the null space of $U$ and $V$ as follows:  \begin{align*} U=\{x\in \mathbb{R}^4: Ax=0 \}\implies U=\text{span}\{(1,-1,0,0), \ (0,0,1,-1) \}\\ V=\{x\in \mathbb{R}^4: Bx=0 \}\implies V=\text{span}\{(1,-2,1,0), \ (0,-1,0,1) \}\\ \end{align*}  For calculating the dimension of $U+V$, we have to see what is the dimension of $U\cap V$. For this I consider the matrix  $$ \begin{bmatrix} 1&-1&0&0\\0&0&1&-1\\1&-2&1&0\\0&-1&0&1 \end{bmatrix}\sim   \begin{bmatrix} 1&-1&0&0\\0&-1&1&0\\0&0&1&-1\\0&0&0&0 \end{bmatrix} ,$$ which tells that the dimension of $U\cap V=1.$ So, $\dim(U+V)=3$. Now I want to ask is there any shorter method to do this? At least for the dimension of $U\cap V$.",,['linear-algebra']
38,Linear Independence,Linear Independence,,"I've come across a question in Linear Algebra that I can't quite figure out. I've tried a multitude of things that either don't work or aren't sufficient enough to convince me I understand linear independence well enough. I know a set of vectors, S, in vector space V are linearly independent if their linear combination, that is, $\lambda_1 \mathbf{v}_1 + ... + \lambda_n \mathbf{v}_n = \mathbf{0}$ means all scalars are equal to each other and 0, $\lambda_1 = ... = \lambda_n = 0.$ I can also show a set of vectors S is linearly independent if I'm given a set of vectors with numerical values - by creating a matrix and reducing it to row echelon form. However, my understanding isn't great enough that I can expand on this and answer questions such as the following: Assume the vectors u, v and w are linearly independent elements of a vector space V.  For each of the following sets decide whether it is linearly independent. A. { u + v + w , v - 2 w ,  2 u + 3 w } B. { u + 2 w , v + 2 w , 2 w } C. { x, y, z } where, x = u + 2 v - w , y = 2 x + u + 2 v - w , z = 3 x - 2 y . If anyone can explain to me the connection between this type of question and the definition of linear independence by answering A or providing a guideline of how to answer A then hopefully I can tackle B and C and any related questions. Thanks.","I've come across a question in Linear Algebra that I can't quite figure out. I've tried a multitude of things that either don't work or aren't sufficient enough to convince me I understand linear independence well enough. I know a set of vectors, S, in vector space V are linearly independent if their linear combination, that is, $\lambda_1 \mathbf{v}_1 + ... + \lambda_n \mathbf{v}_n = \mathbf{0}$ means all scalars are equal to each other and 0, $\lambda_1 = ... = \lambda_n = 0.$ I can also show a set of vectors S is linearly independent if I'm given a set of vectors with numerical values - by creating a matrix and reducing it to row echelon form. However, my understanding isn't great enough that I can expand on this and answer questions such as the following: Assume the vectors u, v and w are linearly independent elements of a vector space V.  For each of the following sets decide whether it is linearly independent. A. { u + v + w , v - 2 w ,  2 u + 3 w } B. { u + 2 w , v + 2 w , 2 w } C. { x, y, z } where, x = u + 2 v - w , y = 2 x + u + 2 v - w , z = 3 x - 2 y . If anyone can explain to me the connection between this type of question and the definition of linear independence by answering A or providing a guideline of how to answer A then hopefully I can tackle B and C and any related questions. Thanks.",,[]
39,Set of Polynomials divided by single linear factor is linearly independent,Set of Polynomials divided by single linear factor is linearly independent,,"Let $f(t)$ be a polynomial composed of linear factors $(t-a_i)$ for $i \in [n]$, i.e $f(t) = (t-a_1)\cdots (t-a_n) $. Let $g_k(t)$ be given by: $$g_k(t) = \frac{f(t)}{(t-a_k)}$$  for $k \in[n]$. Prove that the set of polynomials $\{g_k(t) \mid 1 \leq k \leq n \}$ is linearly independent. Forgot to specify, but each $a_i$ is distinct. I am not sure how to proceed here. I was thinking of using induction but am not entirely sure how to work in the inductive hypothesis.","Let $f(t)$ be a polynomial composed of linear factors $(t-a_i)$ for $i \in [n]$, i.e $f(t) = (t-a_1)\cdots (t-a_n) $. Let $g_k(t)$ be given by: $$g_k(t) = \frac{f(t)}{(t-a_k)}$$  for $k \in[n]$. Prove that the set of polynomials $\{g_k(t) \mid 1 \leq k \leq n \}$ is linearly independent. Forgot to specify, but each $a_i$ is distinct. I am not sure how to proceed here. I was thinking of using induction but am not entirely sure how to work in the inductive hypothesis.",,"['linear-algebra', 'polynomials', 'eigenvalues-eigenvectors', 'spectral-theory']"
40,Question about the complex inner product axioms,Question about the complex inner product axioms,,"My textbook claims that from the axioms for the complex inner product: $$\left<y,x\right>=\overline{\left<x,y\right>}\tag{1}$$  $$c\left<x,y\right> = \left<cx,y\right>\tag{2}$$ we can derive: \begin{align} \left<x,cy\right> &= \overline{\left<cy,x\right>}\\ &= \overline{c}\overline{\left<y,x\right>}\\ &= \overline{c}\left<x,y\right> \end{align} I understand the first and last steps of the derivation, but in the middle step, I don't understand what justifies bringing the $c$ out of the inner product and taking its complex conjugate.","My textbook claims that from the axioms for the complex inner product: $$\left<y,x\right>=\overline{\left<x,y\right>}\tag{1}$$  $$c\left<x,y\right> = \left<cx,y\right>\tag{2}$$ we can derive: \begin{align} \left<x,cy\right> &= \overline{\left<cy,x\right>}\\ &= \overline{c}\overline{\left<y,x\right>}\\ &= \overline{c}\left<x,y\right> \end{align} I understand the first and last steps of the derivation, but in the middle step, I don't understand what justifies bringing the $c$ out of the inner product and taking its complex conjugate.",,"['linear-algebra', 'complex-numbers']"
41,Show the exponential map $\mathfrak{u}(n) \to U(n)$ is surjective,Show the exponential map  is surjective,\mathfrak{u}(n) \to U(n),"I know this or similar questions have been asked numerous times. But the answers seem quite advanced. This problem is an exercise in a set of notes in a course where the main text is Stillwell's ""Naive Lie Theory,"" so I would appreciate help with a solution at that level. Show the exponential map $\mathfrak{u}(n)\rightarrow U(n)$ is surjective where $\mathfrak{u}(n)$ is the tangent space at the identity (in the next section to be referred to as the Lie algebra) of $U(n)$, the space of unitary matrices. Earlier it was proved that this tangent space for $U(n)$ is the set of matrices $\lbrace X\in M_n(\mathbb{C}):X+X^{*}=0\rbrace$ There is a hint that if $A$ is a unitary matrix, then there is a unitary matrix $U$ and a diagonal matrix $D$ such that $A=UDU^{*}$. So far, I would say if $A\in \mathfrak{u}(n)$ then $e^A=e^{UDU^{*}}\in U(n)$. And $e^{UDU^{*}}=Ue^{D}U^{*}$. Thanks","I know this or similar questions have been asked numerous times. But the answers seem quite advanced. This problem is an exercise in a set of notes in a course where the main text is Stillwell's ""Naive Lie Theory,"" so I would appreciate help with a solution at that level. Show the exponential map $\mathfrak{u}(n)\rightarrow U(n)$ is surjective where $\mathfrak{u}(n)$ is the tangent space at the identity (in the next section to be referred to as the Lie algebra) of $U(n)$, the space of unitary matrices. Earlier it was proved that this tangent space for $U(n)$ is the set of matrices $\lbrace X\in M_n(\mathbb{C}):X+X^{*}=0\rbrace$ There is a hint that if $A$ is a unitary matrix, then there is a unitary matrix $U$ and a diagonal matrix $D$ such that $A=UDU^{*}$. So far, I would say if $A\in \mathfrak{u}(n)$ then $e^A=e^{UDU^{*}}\in U(n)$. And $e^{UDU^{*}}=Ue^{D}U^{*}$. Thanks",,"['linear-algebra', 'matrices']"
42,"Doubt on proofs of ""If $A, B$ are $n\times n$ matrices such that $AB = I$ then $BA = I$""","Doubt on proofs of ""If  are  matrices such that  then ""","A, B n\times n AB = I BA = I","The proofs are already presented at length here . And I found that most of the solutions used the fact that if the linear map $B$ is one-one then it must be onto. But this fact appears trivial because of $AB = I$. Consider the equation $Bx = y$ and if there are two solutions $x, x'$ then $Bx = Bx' = y$ by multiplying by $A$ on left we get $x = x'$. This is what is referred to as $B$ is one-one. But the same argument also shows that $x = x' = Ay$ so that $B$ is onto (for each $y$ we have found $x = Ay$). I wonder why do the proofs in the linked question used some linear-algebra to show that $B$ is onto. It appears that most proofs use the left multiplication by $A$ to ensure that $x = x'$ but somehow this does not seem to guarantee $x = Ay$. Why is that the case? Or am I missing something? Update : Thanks to all those who replied. I got hold of my mistake. The argument proves that if $Bx = y$ has a solution it must be $x = Ay$. It does not show that there is a solution. Sorry to bother you all for such trivial matter (and if all agree I may delete this silly question). Existence of a left inverse of $B$ does not guarantee the solution of $Bx  = y$. Rather it is the existence of right inverse of $B$ which is needed here to get a solution.","The proofs are already presented at length here . And I found that most of the solutions used the fact that if the linear map $B$ is one-one then it must be onto. But this fact appears trivial because of $AB = I$. Consider the equation $Bx = y$ and if there are two solutions $x, x'$ then $Bx = Bx' = y$ by multiplying by $A$ on left we get $x = x'$. This is what is referred to as $B$ is one-one. But the same argument also shows that $x = x' = Ay$ so that $B$ is onto (for each $y$ we have found $x = Ay$). I wonder why do the proofs in the linked question used some linear-algebra to show that $B$ is onto. It appears that most proofs use the left multiplication by $A$ to ensure that $x = x'$ but somehow this does not seem to guarantee $x = Ay$. Why is that the case? Or am I missing something? Update : Thanks to all those who replied. I got hold of my mistake. The argument proves that if $Bx = y$ has a solution it must be $x = Ay$. It does not show that there is a solution. Sorry to bother you all for such trivial matter (and if all agree I may delete this silly question). Existence of a left inverse of $B$ does not guarantee the solution of $Bx  = y$. Rather it is the existence of right inverse of $B$ which is needed here to get a solution.",,"['linear-algebra', 'matrices', 'proof-explanation']"
43,"Composition of linear functions, but backwards?","Composition of linear functions, but backwards?",,"Is it true that if a composition of functions is linear, then the two functions it is composed of are also linear? ie. $f_i(x) = g_i(T(x))$, where $f_i$ is linear. Does this imply $T$ is linear?  For this specific question, we also have $g_i$ is linear.","Is it true that if a composition of functions is linear, then the two functions it is composed of are also linear? ie. $f_i(x) = g_i(T(x))$, where $f_i$ is linear. Does this imply $T$ is linear?  For this specific question, we also have $g_i$ is linear.",,['linear-algebra']
44,Can anyone explain how the complex matrix representation of a quaternions is constructed?,Can anyone explain how the complex matrix representation of a quaternions is constructed?,,I am reading some properties of quaternionic matrices and I am unable to understand how can we got such matrix representation. please help in this regards.,I am reading some properties of quaternionic matrices and I am unable to understand how can we got such matrix representation. please help in this regards.,,"['linear-algebra', 'complex-analysis', 'quaternions']"
45,90 degree counter-clockwise rotation around a point,90 degree counter-clockwise rotation around a point,,"How do you do a 90 degree counter-clockwise rotation around a point? I know around the origin it's $(-y,x)$, but what would it be around a point? $$(-y - a,x - b)$$ Where $(a,b)$ is the rotation point.","How do you do a 90 degree counter-clockwise rotation around a point? I know around the origin it's $(-y,x)$, but what would it be around a point? $$(-y - a,x - b)$$ Where $(a,b)$ is the rotation point.",,['linear-algebra']
46,How to express $\sqrt{2-\sqrt{2}}$ in terms of the basis for $\mathbb{Q}(\sqrt{2+\sqrt{2}})$,How to express  in terms of the basis for,\sqrt{2-\sqrt{2}} \mathbb{Q}(\sqrt{2+\sqrt{2}}),"I've already shown that $\sqrt{2-\sqrt{2}}\in\mathbb{Q}(\sqrt{2+\sqrt{2}})$ but I want to write $\sqrt{2-\sqrt{2}}=a+b\alpha+c\alpha^2+d\alpha^3$ where $\alpha=\sqrt{2+\sqrt{2}}$. By squaring $\sqrt{2-\sqrt{2}}$ I get a system of four equations with four variables, but everything I try (Sage, for instance) takes too long to solve it. Is there a simpler way to write it in terms of the basis?","I've already shown that $\sqrt{2-\sqrt{2}}\in\mathbb{Q}(\sqrt{2+\sqrt{2}})$ but I want to write $\sqrt{2-\sqrt{2}}=a+b\alpha+c\alpha^2+d\alpha^3$ where $\alpha=\sqrt{2+\sqrt{2}}$. By squaring $\sqrt{2-\sqrt{2}}$ I get a system of four equations with four variables, but everything I try (Sage, for instance) takes too long to solve it. Is there a simpler way to write it in terms of the basis?",,"['linear-algebra', 'abstract-algebra', 'field-theory', 'galois-theory']"
47,"Prove that $e^x, xe^x,$ and $x^2e^x$ are linearly independent over $\mathbb{R}$",Prove that  and  are linearly independent over,"e^x, xe^x, x^2e^x \mathbb{R}","Question: Prove that $e^x, xe^x,$ and $x^2e^x$ are linearly independent over $\mathbb{R}$. Generally we proceed by setting up the equation $$a_1e^x + a_2xe^x+a_3x^2e^x=0_f,$$  which simplifies to $$e^x(a_1+a_2x+a_3x^2)=0_f,$$ and furthermore to $$a_1+a_2x+a_3x^2=0_f.$$ From here I think it's obvious that the only choice to make the sum the zero function is to let each scalar equal 0, but this is very weak reasoning. As an undergraduate we learned to test for independence by determining whether the Wronskian is not identically equal to 0. But I can only use this method if the functions are solutions to the same linear homogeneous differential equation of order 3. In other words, I cannot use this method for an arbitrary set of functions. I was not given a differential equation, so I determined it on my own and got that they satisfy $$y'''-3y''+3y'-y = 0.$$ I found the Wronskian, $2e^{3x}\neq0$ for any real number. Thus the set is linearly independent. But it took me some time to find the differential equation and even longer finding the Wronskian so I'm wondering if there is a stronger way to prove this without using the Wronskian Test for Independence.","Question: Prove that $e^x, xe^x,$ and $x^2e^x$ are linearly independent over $\mathbb{R}$. Generally we proceed by setting up the equation $$a_1e^x + a_2xe^x+a_3x^2e^x=0_f,$$  which simplifies to $$e^x(a_1+a_2x+a_3x^2)=0_f,$$ and furthermore to $$a_1+a_2x+a_3x^2=0_f.$$ From here I think it's obvious that the only choice to make the sum the zero function is to let each scalar equal 0, but this is very weak reasoning. As an undergraduate we learned to test for independence by determining whether the Wronskian is not identically equal to 0. But I can only use this method if the functions are solutions to the same linear homogeneous differential equation of order 3. In other words, I cannot use this method for an arbitrary set of functions. I was not given a differential equation, so I determined it on my own and got that they satisfy $$y'''-3y''+3y'-y = 0.$$ I found the Wronskian, $2e^{3x}\neq0$ for any real number. Thus the set is linearly independent. But it took me some time to find the differential equation and even longer finding the Wronskian so I'm wondering if there is a stronger way to prove this without using the Wronskian Test for Independence.",,['linear-algebra']
48,For each $f \in P_{n-1}$ there exists $g \in P_n$ such that $f(x)=g(x+1)-g(x)$,For each  there exists  such that,f \in P_{n-1} g \in P_n f(x)=g(x+1)-g(x),"Assume that $P_n$ & $P_{n-1}$ are defined as the vector spaces of the polynomials of degree $n$ & $n-1$ over field $\mathbb R$, respectively. Prove that for each $f \in P_{n-1}$ there exists a polynomial $g \in P_n$ such that $f(x)=g(x+1)-g(x)$. My try : I defined a linear-map $T: P_{n} \to P_{n-1}$ such that $T(g)=g(x+1)-g(x)$. I need to prove that $T$ is surjective. That's what i don't know how to show ... Thanks in advance.","Assume that $P_n$ & $P_{n-1}$ are defined as the vector spaces of the polynomials of degree $n$ & $n-1$ over field $\mathbb R$, respectively. Prove that for each $f \in P_{n-1}$ there exists a polynomial $g \in P_n$ such that $f(x)=g(x+1)-g(x)$. My try : I defined a linear-map $T: P_{n} \to P_{n-1}$ such that $T(g)=g(x+1)-g(x)$. I need to prove that $T$ is surjective. That's what i don't know how to show ... Thanks in advance.",,"['linear-algebra', 'vector-spaces']"
49,Find values of $a$ and $b$ that make matrix orthogonal,Find values of  and  that make matrix orthogonal,a b,Given the matrix $$A=\begin{bmatrix}1/2&a\\b&1/2\\ \end{bmatrix}$$ find the values of $a$ and $b$ that make it orthogonal. So far I have tried using dot product $$(1/2)a+(1/2)b=0$$ and we can conclude that $a=-b$ and $b=-a$. I also tried the following theorem $$A^T=A^{-1}$$ so $$\begin{bmatrix}1/2&b\\a&1/2\\ \end{bmatrix}= \begin{bmatrix} (2+\frac{4ab}{1-4ab})&\frac{-4a}{1-4ab}\\ \frac{-4b}{1-4ab}&\frac{2}{1-4ab}\\ \end{bmatrix}$$ Can someone tell if I'am on the right track and point me in the right direction? Thanks!,Given the matrix $$A=\begin{bmatrix}1/2&a\\b&1/2\\ \end{bmatrix}$$ find the values of $a$ and $b$ that make it orthogonal. So far I have tried using dot product $$(1/2)a+(1/2)b=0$$ and we can conclude that $a=-b$ and $b=-a$. I also tried the following theorem $$A^T=A^{-1}$$ so $$\begin{bmatrix}1/2&b\\a&1/2\\ \end{bmatrix}= \begin{bmatrix} (2+\frac{4ab}{1-4ab})&\frac{-4a}{1-4ab}\\ \frac{-4b}{1-4ab}&\frac{2}{1-4ab}\\ \end{bmatrix}$$ Can someone tell if I'am on the right track and point me in the right direction? Thanks!,,"['linear-algebra', 'matrices', 'orthogonality', 'orthogonal-matrices']"
50,"If A and B are both invertible, then $[(A^{-1})B]^T$ is invertible.","If A and B are both invertible, then  is invertible.",[(A^{-1})B]^T,"If $A$ and $B$ are both invertible, then $[A^{-1}B]^T$ is invertible. Is this true or false? I have tried coming up with a proof but I don't really know where to start.","If $A$ and $B$ are both invertible, then $[A^{-1}B]^T$ is invertible. Is this true or false? I have tried coming up with a proof but I don't really know where to start.",,"['linear-algebra', 'matrices', 'inverse', 'transpose']"
51,"Stuff which squares to $-1$ in the quaternions, thinking geometrically.","Stuff which squares to  in the quaternions, thinking geometrically.",-1,"How can we think of the set$$\{x \in \mathbb{H} : x^2 = -1\}$$geometrically? Is this set finite or infinite? Are there some more geometric ways of thinking about than meets the eye? Here, $\mathbb{H}$ denotes the quaternions.","How can we think of the set$$\{x \in \mathbb{H} : x^2 = -1\}$$geometrically? Is this set finite or infinite? Are there some more geometric ways of thinking about than meets the eye? Here, $\mathbb{H}$ denotes the quaternions.",,"['linear-algebra', 'abstract-algebra']"
52,Showing that if $AB=BA$ then $A$ and $B$ are simultaneously diagonalizable [duplicate],Showing that if  then  and  are simultaneously diagonalizable [duplicate],AB=BA A B,"This question already has an answer here : Prove that simultaneously diagonalizable matrices commute (1 answer) Closed 4 years ago . Suppose $A \in M_n$ has distinct eigenvalues $a_1,\dots,a_n$ and that $A$ commutes with a given matrix $B \in M_n$ so that $AB=BA$ . a. Prove A and B are simultaneously diagonalizable I was able to show that $B$ is diagonalizable but I cannot figure out how to show that $A$ and $B$ are simultaneously diagonalizable.",This question already has an answer here : Prove that simultaneously diagonalizable matrices commute (1 answer) Closed 4 years ago . Suppose has distinct eigenvalues and that commutes with a given matrix so that . a. Prove A and B are simultaneously diagonalizable I was able to show that is diagonalizable but I cannot figure out how to show that and are simultaneously diagonalizable.,"A \in M_n a_1,\dots,a_n A B \in M_n AB=BA B A B",['linear-algebra']
53,Solving matrix equation $AB = C$ with $B (n\times 1)$ and $C (n\times 1)$,Solving matrix equation  with  and,AB = C B (n\times 1) C (n\times 1),I am trying to solve a matrix equation such as $AB = C$. The $A$ is the unknown matrix and I must find it. I know that $B$ and  $C$ are $n \times 1$ matrices and so $A$ must be $n \times n$. I can't use the inverse of B since it doesn't exist.,I am trying to solve a matrix equation such as $AB = C$. The $A$ is the unknown matrix and I must find it. I know that $B$ and  $C$ are $n \times 1$ matrices and so $A$ must be $n \times n$. I can't use the inverse of B since it doesn't exist.,,"['linear-algebra', 'matrices', 'linear-programming', 'matrix-equations', 'matrix-calculus']"
54,Unconstrained quadratic programming problem with positive semidefinite matrix,Unconstrained quadratic programming problem with positive semidefinite matrix,,"I want to find $x\in\mathbb{R^n}$, where $x$ minimizes $$f(x) = \frac{1}{2}x^TA x + b^Tx$$ There are no constraints. I do know that $A$ is symmetric positive semidefinite, and $f(x) \ge 0$ for all $x$. If $A$ is invertible, then the problem is easy, but I am interested in a general solution that handles the case where $A$ is singular. If the solution is not unique, that's ok. I just need any solution, but preferably a sparse one. Through some reading, it looks like the pseudoinverse obtained via SVD would give a solution, but I haven't seen much theory about that. Would that work? And if it does, can I get a proof? Or is there another way to do this? Edit: forgot to mention that $A$ is symmetric","I want to find $x\in\mathbb{R^n}$, where $x$ minimizes $$f(x) = \frac{1}{2}x^TA x + b^Tx$$ There are no constraints. I do know that $A$ is symmetric positive semidefinite, and $f(x) \ge 0$ for all $x$. If $A$ is invertible, then the problem is easy, but I am interested in a general solution that handles the case where $A$ is singular. If the solution is not unique, that's ok. I just need any solution, but preferably a sparse one. Through some reading, it looks like the pseudoinverse obtained via SVD would give a solution, but I haven't seen much theory about that. Would that work? And if it does, can I get a proof? Or is there another way to do this? Edit: forgot to mention that $A$ is symmetric",,"['linear-algebra', 'optimization', 'convex-optimization', 'positive-semidefinite', 'quadratic-programming']"
55,Determine the eigenvalue of a real matrix,Determine the eigenvalue of a real matrix,,I think to this question for two days : Let $A$ be a $3\times3$ real matrix such that $\det(A) = 1$ and $A^{-1}= A^T$. Prove that  one of the eigenvalues is equal to $1$. I used the fact that determinant of $A$ is the product of its eigenvalues and then I wrote many equations that I couldn't  solve the question.,I think to this question for two days : Let $A$ be a $3\times3$ real matrix such that $\det(A) = 1$ and $A^{-1}= A^T$. Prove that  one of the eigenvalues is equal to $1$. I used the fact that determinant of $A$ is the product of its eigenvalues and then I wrote many equations that I couldn't  solve the question.,,"['linear-algebra', 'eigenvalues-eigenvectors', 'determinant']"
56,$P^{-1}A^{-1}B P$ diagonal implies $P^\top A P$ and $P^\top B P$ diagonal?,diagonal implies  and  diagonal?,P^{-1}A^{-1}B P P^\top A P P^\top B P,"Consider two symmetric positive-definite real matrices $A,B$. $A$ is hence invertible. Denote by $P$ the matrix of the column-eigenvectors of $A^{-1}B$ such that $P^{-1}A^{-1}BP=D$ where $D$ is a diagonal matrix of the eigenvalues of $A^{-1}B$. $A^{-1}B$ is symmetric and real, hence can be diagonalized in an orthogonal basis, but this does not mean that $P$ is necessarily orthogonal. It seems that $P^\top A P$ and $P^\top B P$ are diagonal. Is it always true, and if so, why? What is not clear to me is the meaning of $P^\top$ instead of $P^{-1}$. I wonder whether this is related to a change of basis (for example, the change of basis of the bilinear forms defined by $A$ and $B$). Edit I think this is true only if $A^{-1}B$ has distinct eigenvalues. Indeed, let $v_i$ denote an eigenvector of $A^{-1}B$ for $\lambda_i>0$ (because it is PD), then $A^{-1}B v_i=\lambda_i v_i$ or $Bv_i=\lambda A v_i$ so that $v_j^\top B v_i=\lambda_i v_j^\top A v_i$ and similarly $v_i^\top B v_j=\lambda_j v_i^\top A v_j$. Substracting one equality to the other: $$ (\lambda_j-\lambda_i)v_i^\top A v_j=(\lambda_j-\lambda_i)v_i^\top \dfrac{1}{\lambda_j}  B v_j.$$ Hence, if $\lambda_i\neq\lambda_j$, necessarily $v_i^\top A v_j=v_i^\top  B v_j=0$ or $V^\top A V$ and $V^\top B V$ are diagonal, where $V$ is a matrix of columns the $v_i$. Does it seem correct?","Consider two symmetric positive-definite real matrices $A,B$. $A$ is hence invertible. Denote by $P$ the matrix of the column-eigenvectors of $A^{-1}B$ such that $P^{-1}A^{-1}BP=D$ where $D$ is a diagonal matrix of the eigenvalues of $A^{-1}B$. $A^{-1}B$ is symmetric and real, hence can be diagonalized in an orthogonal basis, but this does not mean that $P$ is necessarily orthogonal. It seems that $P^\top A P$ and $P^\top B P$ are diagonal. Is it always true, and if so, why? What is not clear to me is the meaning of $P^\top$ instead of $P^{-1}$. I wonder whether this is related to a change of basis (for example, the change of basis of the bilinear forms defined by $A$ and $B$). Edit I think this is true only if $A^{-1}B$ has distinct eigenvalues. Indeed, let $v_i$ denote an eigenvector of $A^{-1}B$ for $\lambda_i>0$ (because it is PD), then $A^{-1}B v_i=\lambda_i v_i$ or $Bv_i=\lambda A v_i$ so that $v_j^\top B v_i=\lambda_i v_j^\top A v_i$ and similarly $v_i^\top B v_j=\lambda_j v_i^\top A v_j$. Substracting one equality to the other: $$ (\lambda_j-\lambda_i)v_i^\top A v_j=(\lambda_j-\lambda_i)v_i^\top \dfrac{1}{\lambda_j}  B v_j.$$ Hence, if $\lambda_i\neq\lambda_j$, necessarily $v_i^\top A v_j=v_i^\top  B v_j=0$ or $V^\top A V$ and $V^\top B V$ are diagonal, where $V$ is a matrix of columns the $v_i$. Does it seem correct?",,"['linear-algebra', 'matrices', 'bilinear-form', 'symmetric-matrices']"
57,The trace functional and its scalar multiples [duplicate],The trace functional and its scalar multiples [duplicate],,"This question already has answers here : Show that trace is a unique linear functional (2 answers) Closed 7 years ago . I am trying to solve the following problem: Show that the trace functional on $n \times n$ matrices is unique in the following sense. If $W$ is the space of $n \times n$ matrices over the field $F$ and if $f$ is a linear functional on $W$ such that $f(AB)=f(BA)$ for each $A$ and $B$ in $W$, then $f$ is a scalar multiple of the trace function. I know that one way to prove this is to show that the null space of $f$ equals the null space of the trace function, but I found this solution rather complicated; any other suggestions?","This question already has answers here : Show that trace is a unique linear functional (2 answers) Closed 7 years ago . I am trying to solve the following problem: Show that the trace functional on $n \times n$ matrices is unique in the following sense. If $W$ is the space of $n \times n$ matrices over the field $F$ and if $f$ is a linear functional on $W$ such that $f(AB)=f(BA)$ for each $A$ and $B$ in $W$, then $f$ is a scalar multiple of the trace function. I know that one way to prove this is to show that the null space of $f$ equals the null space of the trace function, but I found this solution rather complicated; any other suggestions?",,"['linear-algebra', 'matrices', 'linear-transformations']"
58,Is a diagonalization of a matrix unique?,Is a diagonalization of a matrix unique?,,I was solving problems of diagonalization of matrices and I wanted to know if a diagonalization of a matrix is always unique? but there's nothing about it in the books nor the net. I was trying to look for counter examples but I found none. Any hint would be much appreciated Thanks!,I was solving problems of diagonalization of matrices and I wanted to know if a diagonalization of a matrix is always unique? but there's nothing about it in the books nor the net. I was trying to look for counter examples but I found none. Any hint would be much appreciated Thanks!,,"['linear-algebra', 'matrices']"
59,What does it mean to write a linear operator in a particular basis?,What does it mean to write a linear operator in a particular basis?,,"I am working on a past Algebra exam paper and have come across a problem which requires me to write the linear operator associated to a given matrix $M$ in the standard basis of $\mathbb{R}^4$. What does it mean to 'write a linear operator IN a given basis'? Thank you. NB: I have looked at this link , but I am not sure if the concept of writing a matrix in a given basis is synonymous to the concept of writing a linear operator in a given basis. EDIT: I think I should be more specific. The statement of the problem I am working on is as follows: Let $M$ be the matrix:  $$\begin{bmatrix}  -2 & 3 & 7 & -3 \\ -6 & 1 & 16 & 1 \\ -2 & 1 & 6 & -1 \\ -2 & -1 & 6 & 3 \\ \end{bmatrix}$$ Write the linear operator associated to $M$ in the standard basis of $\mathbb{R}^4$. The model solution is as follows: The linear operator associated to $M$ is the linear map given by $$M \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ \end{bmatrix} =  \begin{bmatrix}  -2x_1 + 3x_2 + 7x_3 -3x_4 \\  -6x_1 + x_2 + 16x_3 + x_4 \\  -2x_1 + x_2 + 6x_3 - x_4 \\  -2x_1 - x_2 + 6x_3 + 3x_4  \end{bmatrix} $$ My current issue is that I don't fully understand how the model solution is an example of the linear operator written in the standard basis of $\mathbb{R}^4$. In particular, I'm still trying to grasp the concept of writing a linear operator IN a given basis.  What does this really mean?","I am working on a past Algebra exam paper and have come across a problem which requires me to write the linear operator associated to a given matrix $M$ in the standard basis of $\mathbb{R}^4$. What does it mean to 'write a linear operator IN a given basis'? Thank you. NB: I have looked at this link , but I am not sure if the concept of writing a matrix in a given basis is synonymous to the concept of writing a linear operator in a given basis. EDIT: I think I should be more specific. The statement of the problem I am working on is as follows: Let $M$ be the matrix:  $$\begin{bmatrix}  -2 & 3 & 7 & -3 \\ -6 & 1 & 16 & 1 \\ -2 & 1 & 6 & -1 \\ -2 & -1 & 6 & 3 \\ \end{bmatrix}$$ Write the linear operator associated to $M$ in the standard basis of $\mathbb{R}^4$. The model solution is as follows: The linear operator associated to $M$ is the linear map given by $$M \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ \end{bmatrix} =  \begin{bmatrix}  -2x_1 + 3x_2 + 7x_3 -3x_4 \\  -6x_1 + x_2 + 16x_3 + x_4 \\  -2x_1 + x_2 + 6x_3 - x_4 \\  -2x_1 - x_2 + 6x_3 + 3x_4  \end{bmatrix} $$ My current issue is that I don't fully understand how the model solution is an example of the linear operator written in the standard basis of $\mathbb{R}^4$. In particular, I'm still trying to grasp the concept of writing a linear operator IN a given basis.  What does this really mean?",,"['linear-algebra', 'matrices', 'linear-transformations']"
60,Why is row swapping not allowed when finding the LU decomposition?,Why is row swapping not allowed when finding the LU decomposition?,,"I have been taught that if there is a $0$ where a pivot should be then no LU decomposition exists, even if you row swap. Why is this true? If you can row swap to get all non zero pivots then why is there no LU decomposition?  Here is an example where we need to row swap in order to have a non zero pivot: $$ \begin{bmatrix} 0 & 1 \\ 2 & 3 \\ \end{bmatrix} $$","I have been taught that if there is a $0$ where a pivot should be then no LU decomposition exists, even if you row swap. Why is this true? If you can row swap to get all non zero pivots then why is there no LU decomposition?  Here is an example where we need to row swap in order to have a non zero pivot: $$ \begin{bmatrix} 0 & 1 \\ 2 & 3 \\ \end{bmatrix} $$",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
61,Prove that identity matrix is the only idempotent $n x n$ matrix that is invertible.,Prove that identity matrix is the only idempotent  matrix that is invertible.,n x n,"I do get that let's say  $A=(A*A)$ $A^{-1} * A = A^{-1} *(A*A)$ $(A^{-1}*A)=(A^{-1}*A)A$ Then $I=I*A$, therefore $I=A$ Are those correct? Is there another or proper way to prove this?","I do get that let's say  $A=(A*A)$ $A^{-1} * A = A^{-1} *(A*A)$ $(A^{-1}*A)=(A^{-1}*A)A$ Then $I=I*A$, therefore $I=A$ Are those correct? Is there another or proper way to prove this?",,['linear-algebra']
62,"Let A be a square matrix of order n. Prove that if $A^2 = A$, then $\operatorname{rank}(A) + \operatorname{rank}(I - A) = n$. [duplicate]","Let A be a square matrix of order n. Prove that if , then . [duplicate]",A^2 = A \operatorname{rank}(A) + \operatorname{rank}(I - A) = n,This question already has answers here : $\mathrm{rank}(A)+\mathrm{rank}(I-A)=n$ for $A$ idempotent matrix (2 answers) Closed 8 years ago . $A(I-A) = 0\implies\operatorname{rank}(A) + \operatorname{rank}(I-A)\le n$. I managed to get this but wasn't able to go further. Any help would be appreciated.,This question already has answers here : $\mathrm{rank}(A)+\mathrm{rank}(I-A)=n$ for $A$ idempotent matrix (2 answers) Closed 8 years ago . $A(I-A) = 0\implies\operatorname{rank}(A) + \operatorname{rank}(I-A)\le n$. I managed to get this but wasn't able to go further. Any help would be appreciated.,,"['linear-algebra', 'matrices', 'matrix-rank', 'idempotents']"
63,"$A^k=B^k$ and $A,B$ are positive semidefinite $ \Rightarrow A = B$",and  are positive semidefinite,"A^k=B^k A,B  \Rightarrow A = B","Let $A,B \in {\mathbb{C}^{n \times n}}$ and $A,B$ are positive semidefinite. If there is $k \in \mathbb{Z}$ such that $A^k=B^k$, why does $A=B$?","Let $A,B \in {\mathbb{C}^{n \times n}}$ and $A,B$ are positive semidefinite. If there is $k \in \mathbb{Z}$ such that $A^k=B^k$, why does $A=B$?",,"['linear-algebra', 'matrices']"
64,Why a linearly independent set of vectors must not contain the zero vector?,Why a linearly independent set of vectors must not contain the zero vector?,,Why is it necessary for a linearly independent set of vectors to not contain the zero vector? I am looking with the definition perspective i.e. why do we define linear independence in this way?,Why is it necessary for a linearly independent set of vectors to not contain the zero vector? I am looking with the definition perspective i.e. why do we define linear independence in this way?,,"['linear-algebra', 'definition']"
65,Why $\mathrm{adj}(A)\cdot A = A\cdot\mathrm{adj}(A)$?,Why ?,\mathrm{adj}(A)\cdot A = A\cdot\mathrm{adj}(A),"I know that $A\cdot\mathrm{adj}(A) = \det(A) \cdot I$, but why $\mathrm{adj}(A)\cdot A = A\cdot\mathrm{adj}(A)$?","I know that $A\cdot\mathrm{adj}(A) = \det(A) \cdot I$, but why $\mathrm{adj}(A)\cdot A = A\cdot\mathrm{adj}(A)$?",,"['linear-algebra', 'matrices', 'determinant']"
66,"Is it right to say that if two vectors, $A$ and $B$, have same $L^p$ norms, for all $p$, then $A = B$?","Is it right to say that if two vectors,  and , have same  norms, for all , then ?",A B L^p p A = B,"Is it right to say that if two vectors, $A$ and $B$ (all elements of $A$ and $B$ are positive), have same $L^p$ norms, for all p, then $A = B$ ?. Thanks.","Is it right to say that if two vectors, $A$ and $B$ (all elements of $A$ and $B$ are positive), have same $L^p$ norms, for all p, then $A = B$ ?. Thanks.",,['linear-algebra']
67,A question on commutation of matrices,A question on commutation of matrices,,"Given a diagonal matrix $D$, and a nilpotent matrix $N$, do we always have $DN=ND$? If not so, what further conditions do we need to have it? This question came form an ODE/Linear Algebra problem: Give $A\in \mathcal{M}_{n\times n}(\mathbb{R})$, there exists an invertible matrix $P$, and a matrix $B=D+N$, $D$ diagonal and $N$ nilpotent,  such that $A=PBP^{-1}$. Moreover, it is stated that $DN=ND$. Why?","Given a diagonal matrix $D$, and a nilpotent matrix $N$, do we always have $DN=ND$? If not so, what further conditions do we need to have it? This question came form an ODE/Linear Algebra problem: Give $A\in \mathcal{M}_{n\times n}(\mathbb{R})$, there exists an invertible matrix $P$, and a matrix $B=D+N$, $D$ diagonal and $N$ nilpotent,  such that $A=PBP^{-1}$. Moreover, it is stated that $DN=ND$. Why?",,"['linear-algebra', 'matrices']"
68,Trying to understand physical interpretation of outer product,Trying to understand physical interpretation of outer product,,"I am reading a book on Quantum computation and found the following expression: |Ïˆ><Ïˆ||Ï•> â†’ |Ïˆ>   <Ïˆ|Ï•> =    <Ïˆ|Ï•>|Ïˆ> The book says that it means: That is, the operator |Ïˆ><Ïˆ| projects a vector |Ï•> in H to the   1-dimensional subspace of H spanned by |Ïˆ>. But I am not able to understand this meaning of the above expression. Please help me understand this. (I know inner product is projection)","I am reading a book on Quantum computation and found the following expression: |Ïˆ><Ïˆ||Ï•> â†’ |Ïˆ>   <Ïˆ|Ï•> =    <Ïˆ|Ï•>|Ïˆ> The book says that it means: That is, the operator |Ïˆ><Ïˆ| projects a vector |Ï•> in H to the   1-dimensional subspace of H spanned by |Ïˆ>. But I am not able to understand this meaning of the above expression. Please help me understand this. (I know inner product is projection)",,['linear-algebra']
69,Trace of a product of two PSD symmetric matrices being zero means this product being a zero matrix?,Trace of a product of two PSD symmetric matrices being zero means this product being a zero matrix?,,"Some one can help me with this problem? I have two real positive-semidefinite matrices $P$ and $Z$, $P \succeq 0$, $Z \succeq 0$, and they are both symmetric ($P^T = P$ and $Z^T = Z$). Also trace$ (P\cdot Z) = 0$. Does that mean $P \cdot Z = 0$ ? Assume they are both $n \times n$ square matrices. Thanksï¼","Some one can help me with this problem? I have two real positive-semidefinite matrices $P$ and $Z$, $P \succeq 0$, $Z \succeq 0$, and they are both symmetric ($P^T = P$ and $Z^T = Z$). Also trace$ (P\cdot Z) = 0$. Does that mean $P \cdot Z = 0$ ? Assume they are both $n \times n$ square matrices. Thanksï¼",,"['linear-algebra', 'matrices', 'semidefinite-programming']"
70,Why Rotations (in CG) are not linear?,Why Rotations (in CG) are not linear?,,"One of my teachers told me that a fundamental problem in Computer Graphics is that the rotations are not linear. The transations and scalar are linear, but not the rotations. He tried to explain to me why rotations were not linear, but I didn't understand at all. Not to mention my undertanding about the linearity concept is weak. Does anyone has a simple for-dummies explanation about why rotations are not linear? My math is very simple, so I've looking for an explanation that's basic and simple. Thanks a lot :)","One of my teachers told me that a fundamental problem in Computer Graphics is that the rotations are not linear. The transations and scalar are linear, but not the rotations. He tried to explain to me why rotations were not linear, but I didn't understand at all. Not to mention my undertanding about the linearity concept is weak. Does anyone has a simple for-dummies explanation about why rotations are not linear? My math is very simple, so I've looking for an explanation that's basic and simple. Thanks a lot :)",,"['linear-algebra', 'matrices', 'nonlinear-system']"
71,All Two by Two Matrices Satisfy a Certain Property Problem,All Two by Two Matrices Satisfy a Certain Property Problem,,"Show that if $A$, $B$ are $2 \times 2$ matrices over $\mathbb{R}$ then there exists a real number $\lambda$ so that $$ (AB-BA)^2 = \lambda I $$ I can do this problem using brute force (i.e. looking at $A$ and $B$ element-wise), but I know there has to be a different, clever way. It was on an old qualifier, and I know they would not be looking for a solution using element wise entries.","Show that if $A$, $B$ are $2 \times 2$ matrices over $\mathbb{R}$ then there exists a real number $\lambda$ so that $$ (AB-BA)^2 = \lambda I $$ I can do this problem using brute force (i.e. looking at $A$ and $B$ element-wise), but I know there has to be a different, clever way. It was on an old qualifier, and I know they would not be looking for a solution using element wise entries.",,"['linear-algebra', 'matrices']"
72,eigenvalues of an involution,eigenvalues of an involution,,"Let $V \neq  \{0\}$ be a K-vector space and let $P : V \rightarrow V $ be linear. Furthermore, let P be an involution, i.e. $P (P(x)) = x $ for every $x \in V.$ Show that if $P \neq Â±id,$ then $V = Eig (P,-1) \oplus Eig (P,1)$ , i.e. one can write ever element $v$ of $V$ as the sum $v_1 + v_2$ of elements $v_1 \in Eig (P, -1)$ and $v_2 \in Eig(P, 1)$ and that $Eig (P, -1) \cap Eig (P,1) = \{0\}$","Let be a K-vector space and let be linear. Furthermore, let P be an involution, i.e. for every Show that if then , i.e. one can write ever element of as the sum of elements and and that","V \neq  \{0\} P : V \rightarrow V  P (P(x)) = x  x \in V. P \neq Â±id, V = Eig (P,-1) \oplus Eig (P,1) v V v_1 + v_2 v_1 \in Eig (P, -1) v_2 \in Eig(P, 1) Eig (P, -1) \cap Eig (P,1) = \{0\}","['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations', 'involutions']"
73,Is the trace set of orthogonal matrix compact?,Is the trace set of orthogonal matrix compact?,,Show that $$T = \{ \mbox{tr} (A) : A \in O_n (\mathbb{R}) \}$$ is compact. I tried to show this set is compact. I could not. Any hint would suffice. Thanks in advance.,Show that $$T = \{ \mbox{tr} (A) : A \in O_n (\mathbb{R}) \}$$ is compact. I tried to show this set is compact. I could not. Any hint would suffice. Thanks in advance.,,"['linear-algebra', 'matrices', 'compactness', 'trace', 'orthogonal-matrices']"
74,Proof involving subspaces,Proof involving subspaces,,"I encountered this question in a document I found on a  google search, it bugged me because my perception keeps telling me I'm wrong no matter what I do. Let $U$, $W$ and $Z$ be subspaces of a vector space $V$ and the following conditions are satisfied: $U + Z = W +Z$ $U \cap Z = W\cap Z$ $U \subset W$ Prove that $U=W$ So I tried proving by contradiction that $U$ is not equal to $W$,  by saying there exists some vector $w \in W \setminus U$ and  if $w \in W \cap Z$ then $w \in U \cap Z$ which means $w \in  U$. At this momment my mind is shouting  ""This isn't it"", and I think I should be using a different approach.","I encountered this question in a document I found on a  google search, it bugged me because my perception keeps telling me I'm wrong no matter what I do. Let $U$, $W$ and $Z$ be subspaces of a vector space $V$ and the following conditions are satisfied: $U + Z = W +Z$ $U \cap Z = W\cap Z$ $U \subset W$ Prove that $U=W$ So I tried proving by contradiction that $U$ is not equal to $W$,  by saying there exists some vector $w \in W \setminus U$ and  if $w \in W \cap Z$ then $w \in U \cap Z$ which means $w \in  U$. At this momment my mind is shouting  ""This isn't it"", and I think I should be using a different approach.",,"['linear-algebra', 'vector-spaces']"
75,Kernel Explanation,Kernel Explanation,,sorry for asking so many questions lately but our lecturer is doing a terrible job explaining things. Calculate $ker(A)$ given that: $f:\{\mathbb{R}^3â†’\mathbb{R}^3; râ†’ A\vec{r}\}$ $A= \bigl(\begin{smallmatrix}  1&2  &4 \\   0&1  &2 \\   3&1  &2  \end{smallmatrix}\bigr)$ I have been browsing the web for some answers but I don't really get it. Maybe someone can explain what the kernel is and how I calculate it. I hope my question makes sense. I had to translate it since I am studying at a german university. Thanks in advance,sorry for asking so many questions lately but our lecturer is doing a terrible job explaining things. Calculate $ker(A)$ given that: $f:\{\mathbb{R}^3â†’\mathbb{R}^3; râ†’ A\vec{r}\}$ $A= \bigl(\begin{smallmatrix}  1&2  &4 \\   0&1  &2 \\   3&1  &2  \end{smallmatrix}\bigr)$ I have been browsing the web for some answers but I don't really get it. Maybe someone can explain what the kernel is and how I calculate it. I hope my question makes sense. I had to translate it since I am studying at a german university. Thanks in advance,,"['linear-algebra', 'matrices', 'vectors']"
76,Multiple-choice question about the probability of a random answer to itself being correct,Multiple-choice question about the probability of a random answer to itself being correct,,"I found this math ""problem"" on the internet, and I'm wondering if it has an answer: Question: If you choose an answer to this question at random, what is the probability that you will be correct? a. $25\%$ b. $50\%$ c. $0\%$ d. $25\%$ Does this question have a correct answer?","I found this math ""problem"" on the internet, and I'm wondering if it has an answer: Question: If you choose an answer to this question at random, what is the probability that you will be correct? a. b. c. d. Does this question have a correct answer?",25\% 50\% 0\% 25\%,[]
77,"In Russian roulette, is it best to go first?","In Russian roulette, is it best to go first?",,"Assume that we are playing a game of Russian roulette (6 chambers) and that there is no shuffling after the shot is fired. I was wondering if you have an advantage in going first? If so, how big of an advantage? I was just debating this with friends, and I wouldn't know what probability to use to prove it. I'm thinking binomial distribution or something like that. If $n=2$ , then there's no advantage. Just $50/50$ if the person survives or dies. If $n=3$ , then maybe the other guy has an advantage. The person who goes second should have an advantage. Or maybe I'm wrong.","Assume that we are playing a game of Russian roulette (6 chambers) and that there is no shuffling after the shot is fired. I was wondering if you have an advantage in going first? If so, how big of an advantage? I was just debating this with friends, and I wouldn't know what probability to use to prove it. I'm thinking binomial distribution or something like that. If , then there's no advantage. Just if the person survives or dies. If , then maybe the other guy has an advantage. The person who goes second should have an advantage. Or maybe I'm wrong.",n=2 50/50 n=3,['probability']
78,"Given an infinite number of monkeys and an infinite amount of time, would one of them write Hamlet?","Given an infinite number of monkeys and an infinite amount of time, would one of them write Hamlet?",,"Of course, we've all heard the colloquialism ""If a bunch of monkeys pound on a typewriter, eventually one of them will write Hamlet."" I have a (not very mathematically intelligent) friend who presented it as if it were a mathematical fact, which got me thinking... Is this really true?  Of course, I've learned that dealing with infinity can be tricky, but my intuition says that time is countably infinite while the number of works the monkeys could produce is uncountably infinite.  Therefore, it isn't necessarily given that the monkeys would write Hamlet. Could someone who's better at this kind of math than me tell me if this is correct?  Or is there more to it than I'm thinking?","Of course, we've all heard the colloquialism ""If a bunch of monkeys pound on a typewriter, eventually one of them will write Hamlet."" I have a (not very mathematically intelligent) friend who presented it as if it were a mathematical fact, which got me thinking... Is this really true?  Of course, I've learned that dealing with infinity can be tricky, but my intuition says that time is countably infinite while the number of works the monkeys could produce is uncountably infinite.  Therefore, it isn't necessarily given that the monkeys would write Hamlet. Could someone who's better at this kind of math than me tell me if this is correct?  Or is there more to it than I'm thinking?",,"['probability', 'infinity']"
79,Do men or women have more brothers?,Do men or women have more brothers?,,"Do men or women have more brothers? I think women have more as no man can be his own brother. But how one can prove it rigorously? I am going to suggest some reasonable background assumptions: There are a large number of individuals, of whom half are men and half are women. The individuals are partitioned into nonempty families. The distribution of the sizes of the families is deliberately not specified. However, in each family, the sex of each member is independent of the sexes of the other members. I believe these assumptions are roughly correct for the world we actually live in. Even in the absence of any information about point 3, what can one say about relative expectation of the random variables â€œNumber of brothers of individual $I$, given that $I$ is femaleâ€ and â€œNumber of brothers of individual $I$, given that $I$ is maleâ€? And how can one directly refute the argument that claims that the second expectation should almost certainly be smaller than the first, based on the observation that in any single family, say with two girls and one boy, the girls have at least as many brothers as do the boys, and usually more.","Do men or women have more brothers? I think women have more as no man can be his own brother. But how one can prove it rigorously? I am going to suggest some reasonable background assumptions: There are a large number of individuals, of whom half are men and half are women. The individuals are partitioned into nonempty families. The distribution of the sizes of the families is deliberately not specified. However, in each family, the sex of each member is independent of the sexes of the other members. I believe these assumptions are roughly correct for the world we actually live in. Even in the absence of any information about point 3, what can one say about relative expectation of the random variables â€œNumber of brothers of individual $I$, given that $I$ is femaleâ€ and â€œNumber of brothers of individual $I$, given that $I$ is maleâ€? And how can one directly refute the argument that claims that the second expectation should almost certainly be smaller than the first, based on the observation that in any single family, say with two girls and one boy, the girls have at least as many brothers as do the boys, and usually more.",,"['probability', 'combinatorics']"
80,Counterintuitive examples in probability,Counterintuitive examples in probability,,"I want to teach a short course in probability and I am looking for some counter-intuitive examples in probability. I am mainly interested in the problems whose results seem to be obviously false while they are not. I already found some things. For example these two videos: Penney's game How to win a guessing game In addition, I have found some weird examples of random walks. For example this amazing theorem: For a simple random walk, the mean number of visits to point $b$ before returning to the origin is equal to $1$ for every $b \neq 0$ . I have also found some advanced examples such as Do longer games favor the stronger player ? Could you please do me a favor and share some other examples of such problems? It's very exciting to read yours...","I want to teach a short course in probability and I am looking for some counter-intuitive examples in probability. I am mainly interested in the problems whose results seem to be obviously false while they are not. I already found some things. For example these two videos: Penney's game How to win a guessing game In addition, I have found some weird examples of random walks. For example this amazing theorem: For a simple random walk, the mean number of visits to point before returning to the origin is equal to for every . I have also found some advanced examples such as Do longer games favor the stronger player ? Could you please do me a favor and share some other examples of such problems? It's very exciting to read yours...",b 1 b \neq 0,"['probability', 'soft-question', 'examples-counterexamples', 'intuition', 'big-list']"
81,Taking Seats on a Plane,Taking Seats on a Plane,,"This is a neat little problem that I was discussing today with my lab group out at lunch. Not particularly difficult but interesting implications nonetheless Imagine there are a 100 people in line to board a plane that seats 100. The first person in line, Alice, realizes she lost her boarding pass, so when she boards she decides to take a random seat instead. Every person that boards the plane after her will either take their ""proper"" seat, or if that seat is taken, a random seat instead. Question: What is the probability that the last person that boards will end up in their proper seat? Moreover, and this is the part I'm still pondering about. Can you think of a physical system that would follow this combinatorial statistics? Maybe a spin wave function in a crystal etc...","This is a neat little problem that I was discussing today with my lab group out at lunch. Not particularly difficult but interesting implications nonetheless Imagine there are a 100 people in line to board a plane that seats 100. The first person in line, Alice, realizes she lost her boarding pass, so when she boards she decides to take a random seat instead. Every person that boards the plane after her will either take their ""proper"" seat, or if that seat is taken, a random seat instead. Question: What is the probability that the last person that boards will end up in their proper seat? Moreover, and this is the part I'm still pondering about. Can you think of a physical system that would follow this combinatorial statistics? Maybe a spin wave function in a crystal etc...",,"['probability', 'combinatorics', 'puzzle']"
82,Can a coin with an unknown bias be treated as fair?,Can a coin with an unknown bias be treated as fair?,,"This morning, I wanted to flip a coin to make a decision but only had an SD card: Given that I don't know the bias of this SD card, would flipping it be considered a ""fair toss""? I thought if I'm just as likely to assign an outcome to one side as to the other, then it must be a fair. But this also seems like a recasting of the original question; instead of asking whether the unknowing of the SD card's construction defines fairness, I'm asking if the unknowing of my own psychology ( e.g. which side I'd choose for which outcome) defines fairness. Either way, I think I'm asking: What's the exact relationship between not knowing and ""fairness""? Additional thought: An SD card might be ""fair"" to me , but not at all fair to, say, a design engineer looking at the SD card's blueprint, who immediately sees that the chip is off-center from the flat plane. So it seems fairness even depends on the subjects to whom fairness matters . In a football game then, does an SD card remain ""fair"" as long as no design engineer is there to discern the object being tossed?","This morning, I wanted to flip a coin to make a decision but only had an SD card: Given that I don't know the bias of this SD card, would flipping it be considered a ""fair toss""? I thought if I'm just as likely to assign an outcome to one side as to the other, then it must be a fair. But this also seems like a recasting of the original question; instead of asking whether the unknowing of the SD card's construction defines fairness, I'm asking if the unknowing of my own psychology ( e.g. which side I'd choose for which outcome) defines fairness. Either way, I think I'm asking: What's the exact relationship between not knowing and ""fairness""? Additional thought: An SD card might be ""fair"" to me , but not at all fair to, say, a design engineer looking at the SD card's blueprint, who immediately sees that the chip is off-center from the flat plane. So it seems fairness even depends on the subjects to whom fairness matters . In a football game then, does an SD card remain ""fair"" as long as no design engineer is there to discern the object being tossed?",,"['probability', 'random', 'philosophy']"
83,Is the product of two Gaussian random variables also a Gaussian?,Is the product of two Gaussian random variables also a Gaussian?,,"Say I have $X \sim \mathcal N(a, b)$ and $Y\sim \mathcal N(c, d)$.  Is $XY$ also normally distributed? Is the answer any different if we know that $X$ and $Y$ are independent?","Say I have $X \sim \mathcal N(a, b)$ and $Y\sim \mathcal N(c, d)$.  Is $XY$ also normally distributed? Is the answer any different if we know that $X$ and $Y$ are independent?",,"['probability', 'probability-theory', 'random-variables', 'normal-distribution']"
84,What is the best book to learn probability?,What is the best book to learn probability?,,Question is quite straight... I'm not very good in this subject but need to understand at a good level.,Question is quite straight... I'm not very good in this subject but need to understand at a good level.,,"['probability', 'probability-theory', 'reference-request']"
85,Monty hall problem extended.,Monty hall problem extended.,,"I just learned about the Monty Hall problem and found it quite amazing. So I thought about extending the problem a bit to understand more about it. In this modification of the Monty Hall Problem, instead of three doors, we have four (or maybe $n$) doors, one with a car and the other three (or $n-1$) with a goat each (I want the car). We need to choose any one of the doors. After we have chosen the door, Monty deliberately reveals one of the doors that has a goat and asks us if we wish to change our choice. So should we switch the door we have chosen, or does it not matter if we switch or stay with our choice? It would be even better if we knew the probability of winning upon switching given that Monty opens $k$ doors.","I just learned about the Monty Hall problem and found it quite amazing. So I thought about extending the problem a bit to understand more about it. In this modification of the Monty Hall Problem, instead of three doors, we have four (or maybe $n$) doors, one with a car and the other three (or $n-1$) with a goat each (I want the car). We need to choose any one of the doors. After we have chosen the door, Monty deliberately reveals one of the doors that has a goat and asks us if we wish to change our choice. So should we switch the door we have chosen, or does it not matter if we switch or stay with our choice? It would be even better if we knew the probability of winning upon switching given that Monty opens $k$ doors.",,"['probability', 'monty-hall']"
86,What's 4 times more likely than 80%?,What's 4 times more likely than 80%?,,"There's an 80% probability of a certain outcome, we get some new information that means that outcome is 4 times more likely to occur. What's the new probability as a percentage and how do you work it out? As I remember it the question was posed like so: Suppose there's a student, Tom W, if you were asked to estimate the   probability that Tom is a student of computer science. Without any   other information you would only have the base rate to go by   (percentage of total students enrolled on computer science) suppose   this base rate is 80%. Then you are given a description of Tom W's personality, suppose from   this description you estimate that Tom W is 4 times more likely to be   enrolled on computer science. What is the new probability that Tom W is enrolled on computer   science. The answer given in the book is 94.1% but I couldn't work out how to calculate it! Another example in the book is with a base rate of 3%, 4 times more likely than this is stated as 11%.","There's an 80% probability of a certain outcome, we get some new information that means that outcome is 4 times more likely to occur. What's the new probability as a percentage and how do you work it out? As I remember it the question was posed like so: Suppose there's a student, Tom W, if you were asked to estimate the   probability that Tom is a student of computer science. Without any   other information you would only have the base rate to go by   (percentage of total students enrolled on computer science) suppose   this base rate is 80%. Then you are given a description of Tom W's personality, suppose from   this description you estimate that Tom W is 4 times more likely to be   enrolled on computer science. What is the new probability that Tom W is enrolled on computer   science. The answer given in the book is 94.1% but I couldn't work out how to calculate it! Another example in the book is with a base rate of 3%, 4 times more likely than this is stated as 11%.",,['probability']
87,Intuition behind Conditional Expectation,Intuition behind Conditional Expectation,,"I'm struggling with the concept of conditional expectation. First of all, if you have a link to any explanation that goes beyond showing that it is a generalization of elementary intuitive concepts, please let me know. Let me get more specific. Let $\left(\Omega,\mathcal{A},P\right)$ be a probability space and $X$ an integrable real random variable defined on $(\Omega,\mathcal{A},P)$ . Let $\mathcal{F}$ be a sub- $\sigma$ -algebra of $\mathcal{A}$ . Then $E[X|\mathcal{F}]$ is the a.s. unique random variable $Y$ such that $Y$ is $\mathcal{F}$ -measurable and for any $A\in\mathcal{F}$ , $E\left[X1_A\right]=E\left[Y1_A\right]$ . The common interpretation seems to be: "" $E[X|\mathcal{F}]$ is the expectation of $X$ given the information of $\mathcal{F}$ ."" I'm finding it hard to get any meaning from this sentence. In elementary probability theory, expectation is a real number. So the sentence above makes me think of a real number instead of a random variable. This is reinforced by $E[X|\mathcal{F}]$ sometimes being called ""conditional expected value"". Is there some canonical way of getting real numbers out of $E[X|\mathcal{F}]$ that can be interpreted as elementary expected values of something? In what way does $\mathcal{F}$ provide information? To know that some event occurred, is something I would call information, and I have a clear picture of conditional expectation in this case. To me $\mathcal{F}$ is not a piece of information, but rather a ""complete"" set of pieces of information one could possibly acquire in some way. Maybe you say there is no real intuition behind this, $E[X|\mathcal{F}]$ is just what the definition says it is. But then, how does one see that a martingale is a model of a fair game? Surely, there must be some intuition behind that! I hope you have got some impression of my misconceptions and can rectify them.","I'm struggling with the concept of conditional expectation. First of all, if you have a link to any explanation that goes beyond showing that it is a generalization of elementary intuitive concepts, please let me know. Let me get more specific. Let be a probability space and an integrable real random variable defined on . Let be a sub- -algebra of . Then is the a.s. unique random variable such that is -measurable and for any , . The common interpretation seems to be: "" is the expectation of given the information of ."" I'm finding it hard to get any meaning from this sentence. In elementary probability theory, expectation is a real number. So the sentence above makes me think of a real number instead of a random variable. This is reinforced by sometimes being called ""conditional expected value"". Is there some canonical way of getting real numbers out of that can be interpreted as elementary expected values of something? In what way does provide information? To know that some event occurred, is something I would call information, and I have a clear picture of conditional expectation in this case. To me is not a piece of information, but rather a ""complete"" set of pieces of information one could possibly acquire in some way. Maybe you say there is no real intuition behind this, is just what the definition says it is. But then, how does one see that a martingale is a model of a fair game? Surely, there must be some intuition behind that! I hope you have got some impression of my misconceptions and can rectify them.","\left(\Omega,\mathcal{A},P\right) X (\Omega,\mathcal{A},P) \mathcal{F} \sigma \mathcal{A} E[X|\mathcal{F}] Y Y \mathcal{F} A\in\mathcal{F} E\left[X1_A\right]=E\left[Y1_A\right] E[X|\mathcal{F}] X \mathcal{F} E[X|\mathcal{F}] E[X|\mathcal{F}] \mathcal{F} \mathcal{F} E[X|\mathcal{F}]","['probability', 'probability-theory', 'intuition', 'conditional-expectation', 'faq']"
88,Expected time to roll all $1$ through $6$ on a die,Expected time to roll all  through  on a die,1 6,"What is the average number of times it would it take to roll a fair $6$ -sided die and get all numbers on the die?  The order in which the numbers appear does not matter. I had this questions explained to me by a professor (not math professor), but it was not clear in the explanation.  We were given the answer $(1-(\frac56)^n)^6 = .5$ or $n = 12.152$ Can someone please explain this to me, possibly with a link to a general topic?","What is the average number of times it would it take to roll a fair -sided die and get all numbers on the die?  The order in which the numbers appear does not matter. I had this questions explained to me by a professor (not math professor), but it was not clear in the explanation.  We were given the answer or Can someone please explain this to me, possibly with a link to a general topic?",6 (1-(\frac56)^n)^6 = .5 n = 12.152,"['probability', 'dice', 'coupon-collector']"
89,What is the difference between independent and mutually exclusive events?,What is the difference between independent and mutually exclusive events?,,"Two events are mutually exclusive if they can't both happen. Independent events are events where knowledge of the probability of one doesn't change the probability of the other. Are these definitions correct? If possible, please give more than one example and counterexample.","Two events are mutually exclusive if they can't both happen. Independent events are events where knowledge of the probability of one doesn't change the probability of the other. Are these definitions correct? If possible, please give more than one example and counterexample.",,['probability']
90,Could someone explain conditional independence?,Could someone explain conditional independence?,,"My understanding right now is that an example of conditional independence would be: If two people live in the same city, the probability that person A gets home in time for dinner, and the probability that person B gets home in time for dinner are independent; that is, we wouldn't expect one to have an effect on the other. But if a snow storm hits the city and introduces a probability C that traffic will be at a stand still, you would expect that the probability of both A getting home in time for dinner and B getting home in time for dinner, would change. If this is a correct understanding, I guess I still don't understand what exactly conditional independence is , or what it does for us (why does it have a separate name, as opposed to just compounded probabilities), and if this isn't a correct understanding, could someone please provide an example with an explanation?","My understanding right now is that an example of conditional independence would be: If two people live in the same city, the probability that person A gets home in time for dinner, and the probability that person B gets home in time for dinner are independent; that is, we wouldn't expect one to have an effect on the other. But if a snow storm hits the city and introduces a probability C that traffic will be at a stand still, you would expect that the probability of both A getting home in time for dinner and B getting home in time for dinner, would change. If this is a correct understanding, I guess I still don't understand what exactly conditional independence is , or what it does for us (why does it have a separate name, as opposed to just compounded probabilities), and if this isn't a correct understanding, could someone please provide an example with an explanation?",,"['probability', 'independence']"
91,probability $2/4$ vs $3/6$,probability  vs,2/4 3/6,"Recently I was asked the following in an interview: If you are a pretty good basketball player, and were betting on whether you could make $2$ out of $4$ or $3$ out of $6$ baskets, which would you take? I said anyone since ratio is same. Any insights?","Recently I was asked the following in an interview: If you are a pretty good basketball player, and were betting on whether you could make $2$ out of $4$ or $3$ out of $6$ baskets, which would you take? I said anyone since ratio is same. Any insights?",,['probability']
92,Probability that a stick randomly broken in five places can form a tetrahedron,Probability that a stick randomly broken in five places can form a tetrahedron,,"Edit (June. 2015) This question has been moved to MathOverflow, where a recent write-up finds a similar approximation as leonbloy's post below; see here . Randomly break a stick in five places. Question: What is the probability that the resulting six pieces can form a tetrahedron? Clearly satisfying the triangle inequality on each face is a necessary but not sufficient condition; an example is provided below. Furthermore, another commenter kindly points to a reference that may be of help in resolving this problem. In particular, it relates the question of when six numbers can be edges of a tetrahedron to a certain $5 \times 5$ determinant. Finally, a third commenter points out that since one such construction is possible, there is an admissible neighborhood around this arrangement, so that the probability is in fact positive. In any event, this problem is far harder than the classic $2D$ ""form a triangle"" one. Several numerical attacks can be found below; I will be grateful if anyone can provide an exact solution.","Edit (June. 2015) This question has been moved to MathOverflow, where a recent write-up finds a similar approximation as leonbloy's post below; see here . Randomly break a stick in five places. Question: What is the probability that the resulting six pieces can form a tetrahedron? Clearly satisfying the triangle inequality on each face is a necessary but not sufficient condition; an example is provided below. Furthermore, another commenter kindly points to a reference that may be of help in resolving this problem. In particular, it relates the question of when six numbers can be edges of a tetrahedron to a certain $5 \times 5$ determinant. Finally, a third commenter points out that since one such construction is possible, there is an admissible neighborhood around this arrangement, so that the probability is in fact positive. In any event, this problem is far harder than the classic $2D$ ""form a triangle"" one. Several numerical attacks can be found below; I will be grateful if anyone can provide an exact solution.",,"['probability', 'geometry', 'euclidean-geometry', 'recreational-mathematics', 'problem-solving']"
93,What is the difference and relationship between the binomial and Bernoulli distributions?,What is the difference and relationship between the binomial and Bernoulli distributions?,,How should I understand the difference or relationship between binomial and Bernoulli distribution?,How should I understand the difference or relationship between binomial and Bernoulli distribution?,,"['probability', 'statistics', 'random-variables']"
94,"In a family with two children, what are the chances, if one of the children is a girl, that both children are girls?","In a family with two children, what are the chances, if one of the children is a girl, that both children are girls?",,"In a family with two children, what are the chances, if one of the children is a girl, that both children are girls? I just dipped into a book, The Drunkard's Walk - How Randomness Rules Our Lives , by Leonard Mlodinow, Vintage Books, 2008. On p.107 Mlodinow says the chances are 1 in 3. It seems obvious to me that the chances are 1 in 2. Am I correct? Is this not exactly analogous to having a bowl with an infinite number of marbles, half black and half red? Without looking I draw out a black marble. The probability of the second marble I draw being black is 1/2.","In a family with two children, what are the chances, if one of the children is a girl, that both children are girls? I just dipped into a book, The Drunkard's Walk - How Randomness Rules Our Lives , by Leonard Mlodinow, Vintage Books, 2008. On p.107 Mlodinow says the chances are 1 in 3. It seems obvious to me that the chances are 1 in 2. Am I correct? Is this not exactly analogous to having a bowl with an infinite number of marbles, half black and half red? Without looking I draw out a black marble. The probability of the second marble I draw being black is 1/2.",,"['probability', 'faq']"
95,Expected Number of Coin Tosses to Get Five Consecutive Heads,Expected Number of Coin Tosses to Get Five Consecutive Heads,,A fair coin is tossed repeatedly until 5 consecutive heads occurs. What is the expected number of coin tosses?,A fair coin is tossed repeatedly until 5 consecutive heads occurs. What is the expected number of coin tosses?,,"['probability', 'contest-math', 'expected-value']"
96,Expectation of the maximum of gaussian random variables,Expectation of the maximum of gaussian random variables,,"Is there an exact or good approximate expression for the expectation, variance or other moments of the maximum of $n$ independent, identically distributed gaussian random variables where $n$ is large? If $F$ is the cumulative distribution function for a standard gaussian and $f$ is the probability density function, then the CDF for the maximum is (from the study of order statistics) given by $$F_{\rm max}(x) = F(x)^n$$ and the PDF is $$f_{\rm max}(x) = n F(x)^{n-1} f(x)$$ so it's certainly possible to write down integrals which evaluate to the expectation and other moments, but it's not pretty. My intuition tells me that the expectation of the maximum would be proportional to $\log n$, although I don't see how to go about proving this.","Is there an exact or good approximate expression for the expectation, variance or other moments of the maximum of $n$ independent, identically distributed gaussian random variables where $n$ is large? If $F$ is the cumulative distribution function for a standard gaussian and $f$ is the probability density function, then the CDF for the maximum is (from the study of order statistics) given by $$F_{\rm max}(x) = F(x)^n$$ and the PDF is $$f_{\rm max}(x) = n F(x)^{n-1} f(x)$$ so it's certainly possible to write down integrals which evaluate to the expectation and other moments, but it's not pretty. My intuition tells me that the expectation of the maximum would be proportional to $\log n$, although I don't see how to go about proving this.",,"['probability', 'normal-distribution', 'order-statistics']"
97,"If I flip a coin 1000 times in a row and it lands on heads all 1000 times, what is the probability that it's an unfair coin?","If I flip a coin 1000 times in a row and it lands on heads all 1000 times, what is the probability that it's an unfair coin?",,"Consider a two-sided coin. If I flip it $1000$ times and it lands heads up for each flip, what is the probability that the coin is unfair, and how do we quantify that if it is unfair? Furthermore, would it still be considered unfair for $50$ straight heads? $20$? $7$?","Consider a two-sided coin. If I flip it $1000$ times and it lands heads up for each flip, what is the probability that the coin is unfair, and how do we quantify that if it is unfair? Furthermore, would it still be considered unfair for $50$ straight heads? $20$? $7$?",,"['probability', 'statistics', 'experimental-mathematics']"
98,Is Bayes' Theorem really that interesting?,Is Bayes' Theorem really that interesting?,,"I have trouble understanding the massive importance that is afforded to Bayes' theorem in undergraduate courses in probability and popular science. From the purely mathematical point of view, I think it would be uncontroversial to say that Bayes' theorem does not amount to a particularly sophisticated result. Indeed, the relation $$P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{P(B\cap A)P(A)}{P(B)P(A)}=\frac{P(B|A)P(A)}{P(B)}$$ is a one line proof that follows from expanding both sides directly from the definition of conditional probability. Thus, I expect that what people find interesting about Bayes' theorem has to do with its practical applications or implications. However, even in those cases I find the typical examples being used as a justification of this to be a bit artificial. To illustrate this, the classical application of Bayes' theorem usually goes something like this: Suppose that 1% of women have breast cancer; 80% of mammograms are positive when breast cancer is present; and 10% of mammograms are positive when breast cancer is not present. If a woman has a positive mammogram, then what is the probability that she has breast cancer? I understand that Bayes' theorem allows to compute the desired probability with the given information, and that this probability is counterintuitively low. However, I can't help but feel that the premise of this question is wholly artificial. The only reason why we need to use Bayes' theorem here is that the full information with which the other probabilities (i.e., 1% have cancer, 80% true positive, etc.) have been computed is not provided to us. If we have access to the sample data with which these probabilities were computed, then we can directly find $$P(\text{cancer}|\text{positive test})=\frac{\text{number of women with cancer and positive test}}{\text{number of women with positive test}}.$$ In mathematical terms, if you know how to compute $P(B|A)$ , $P(A)$ , and $P(B)$ , then this means that you know how to compute $P(A\cap B)$ and $P(B)$ , in which case you already have your answer. From the above arguments, it seems to me that Bayes' theorem is essentially only useful for the following reasons: In an adversarial context, i.e., someone who has access to the data only tells you about $P(B|A)$ when $P(A|B)$ is actually the quantity that is relevant to your interests, hoping that you will get confused and will not notice. An opportunity to dispel the confusion between $P(A|B)$ and $P(B|A)$ with concrete examples, and to explain that these are very different when the ratio between $P(A)$ and $P(B)$ deviates significantly from one. Am I missing something big about the usefulness of Bayes' theorem? In light of point 2., especially, I don't understand why Bayes' theorem stands out so much compared to, say, the Borel-Kolmogorov paradox, or the ""paradox"" that $P[X=x]=0$ when $X$ is a continuous random variable, etc.","I have trouble understanding the massive importance that is afforded to Bayes' theorem in undergraduate courses in probability and popular science. From the purely mathematical point of view, I think it would be uncontroversial to say that Bayes' theorem does not amount to a particularly sophisticated result. Indeed, the relation is a one line proof that follows from expanding both sides directly from the definition of conditional probability. Thus, I expect that what people find interesting about Bayes' theorem has to do with its practical applications or implications. However, even in those cases I find the typical examples being used as a justification of this to be a bit artificial. To illustrate this, the classical application of Bayes' theorem usually goes something like this: Suppose that 1% of women have breast cancer; 80% of mammograms are positive when breast cancer is present; and 10% of mammograms are positive when breast cancer is not present. If a woman has a positive mammogram, then what is the probability that she has breast cancer? I understand that Bayes' theorem allows to compute the desired probability with the given information, and that this probability is counterintuitively low. However, I can't help but feel that the premise of this question is wholly artificial. The only reason why we need to use Bayes' theorem here is that the full information with which the other probabilities (i.e., 1% have cancer, 80% true positive, etc.) have been computed is not provided to us. If we have access to the sample data with which these probabilities were computed, then we can directly find In mathematical terms, if you know how to compute , , and , then this means that you know how to compute and , in which case you already have your answer. From the above arguments, it seems to me that Bayes' theorem is essentially only useful for the following reasons: In an adversarial context, i.e., someone who has access to the data only tells you about when is actually the quantity that is relevant to your interests, hoping that you will get confused and will not notice. An opportunity to dispel the confusion between and with concrete examples, and to explain that these are very different when the ratio between and deviates significantly from one. Am I missing something big about the usefulness of Bayes' theorem? In light of point 2., especially, I don't understand why Bayes' theorem stands out so much compared to, say, the Borel-Kolmogorov paradox, or the ""paradox"" that when is a continuous random variable, etc.",P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{P(B\cap A)P(A)}{P(B)P(A)}=\frac{P(B|A)P(A)}{P(B)} P(\text{cancer}|\text{positive test})=\frac{\text{number of women with cancer and positive test}}{\text{number of women with positive test}}. P(B|A) P(A) P(B) P(A\cap B) P(B) P(B|A) P(A|B) P(A|B) P(B|A) P(A) P(B) P[X=x]=0 X,"['probability', 'statistics', 'bayes-theorem', 'motivation']"
99,Probability density function vs. probability mass function,Probability density function vs. probability mass function,,"I've a confession to make. I've been using PDF's and PMF's without actually knowing what they are. My understanding is that density equals area under the curve, but if I look at it that way, then it doesn't make sense to refer to the ""mass"" of a random variable in discrete distributions. How can I interpret this? Why do we call use ""mass"" and ""density"" to describe these functions rather than something else? P.S. Please feel free to change the question itself in a more understandable way if you feel this is a logically wrong question.","I've a confession to make. I've been using PDF's and PMF's without actually knowing what they are. My understanding is that density equals area under the curve, but if I look at it that way, then it doesn't make sense to refer to the ""mass"" of a random variable in discrete distributions. How can I interpret this? Why do we call use ""mass"" and ""density"" to describe these functions rather than something else? P.S. Please feel free to change the question itself in a more understandable way if you feel this is a logically wrong question.",,"['probability', 'probability-theory', 'probability-distributions', 'terminology']"
