,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,If $(A-\lambda{I})$ is $\lambda$-equivalent to $(B-\lambda{I})$ then $A$ is similar to $B$,If  is -equivalent to  then  is similar to,(A-\lambda{I}) \lambda (B-\lambda{I}) A B,"When reading the topic about primary and rational canonical form of matrices I stuck myself on this theorem: The matrices $A,B\in K^{n\times n}$ are similar if and only if their characteristic $\lambda$-matrices $(A-\lambda{I})$ and $(B-\lambda{I})$ are $\lambda$-equivalent; more precisely if $P(\lambda), Q(\lambda) \in K[\lambda]^{n\times n}$ are invertible such that $(B-\lambda{I})=Q(\lambda)(A-\lambda{I})P(\lambda)$ then $P(\lambda)=P$ and $Q(\lambda)=Q$ are constant matrices satisfying $Q=P^{-1}$ and $B=Q A P$. Theorem 21.5.1 on page 448 (in Slovak). The main point in the proof of this theorem seems to be the assertion: If $P(\lambda), Q(\lambda)$ are invertible and $(B-\lambda{I})=Q(\lambda)(A-\lambda{I})P(\lambda)$ then $P(\lambda)=P$ and $Q(\lambda)=Q$ are constant matrices.  What is the proof of this assertion?","When reading the topic about primary and rational canonical form of matrices I stuck myself on this theorem: The matrices $A,B\in K^{n\times n}$ are similar if and only if their characteristic $\lambda$-matrices $(A-\lambda{I})$ and $(B-\lambda{I})$ are $\lambda$-equivalent; more precisely if $P(\lambda), Q(\lambda) \in K[\lambda]^{n\times n}$ are invertible such that $(B-\lambda{I})=Q(\lambda)(A-\lambda{I})P(\lambda)$ then $P(\lambda)=P$ and $Q(\lambda)=Q$ are constant matrices satisfying $Q=P^{-1}$ and $B=Q A P$. Theorem 21.5.1 on page 448 (in Slovak). The main point in the proof of this theorem seems to be the assertion: If $P(\lambda), Q(\lambda)$ are invertible and $(B-\lambda{I})=Q(\lambda)(A-\lambda{I})P(\lambda)$ then $P(\lambda)=P$ and $Q(\lambda)=Q$ are constant matrices.  What is the proof of this assertion?",,"['linear-algebra', 'matrices', 'polynomials']"
1,Let $A$ be a $2 × 2$ matrix with real entries which is not a diagonal matrix and which satisﬁes $A^3 = I$. Pick out the true statements:,Let  be a  matrix with real entries which is not a diagonal matrix and which satisﬁes . Pick out the true statements:,A 2 × 2 A^3 = I,Let $A$ be a $2 \times 2$ matrix with real entries which is not a diagonal matrix and which satisﬁes $A^3 = \mathcal{I}_2$. Pick out the true statements: $\operatorname{tr}(A) = −1$ $A$ is diagonalizable over $\mathbb{R}$ $λ = 1$ is an eigenvalue of $A$ The characteristic polynomial of $A$ will divide the equation $x^3-1$ and it has degree $2$. So the characteristic polynomial will be $x^2+x+1$ which has two non real complex roots and sum of them is $-1$. so only (a) is true. Am I right?,Let $A$ be a $2 \times 2$ matrix with real entries which is not a diagonal matrix and which satisﬁes $A^3 = \mathcal{I}_2$. Pick out the true statements: $\operatorname{tr}(A) = −1$ $A$ is diagonalizable over $\mathbb{R}$ $λ = 1$ is an eigenvalue of $A$ The characteristic polynomial of $A$ will divide the equation $x^3-1$ and it has degree $2$. So the characteristic polynomial will be $x^2+x+1$ which has two non real complex roots and sum of them is $-1$. so only (a) is true. Am I right?,,['linear-algebra']
2,Connection between even/odd and symmetric/skew symmetric,Connection between even/odd and symmetric/skew symmetric,,"I read awhile back that the set of continuous real valued functions from $\mathbb{R} \to \mathbb{R} $ has a direct sum decomposition into subspaces of strictly even and odd functions. Any such function $f$ could then be uniquely written in terms of even and odd parts by the formulas $\text{even}(x)=\dfrac{f(x)+f(-x)}{2}$ and $\text{odd}(x)=\dfrac{f(x)-f(-x)}{2}$ where $\text{even}(x)+\text{odd}(x)=f(x)$. Recently I stumbled into a similar claim for $n\times n$ matrices, but instead of even and odd, the set of square matrices has a direct sum representation in terms of symmetric and skew symmetric parts. So given any square matrix $A$ we can write $B=\dfrac{A+A^T}{2}$ and $C=\dfrac{A-A^T}{2}$ as the symmetric and skew symmetric representations where $A=B+C$. Does this mean that the notion of symmetric and skew symmetric have a connection to the notion of even and odd? Do they represent a generalization of even/odd, or is there another idea that generalizes these two ideas?","I read awhile back that the set of continuous real valued functions from $\mathbb{R} \to \mathbb{R} $ has a direct sum decomposition into subspaces of strictly even and odd functions. Any such function $f$ could then be uniquely written in terms of even and odd parts by the formulas $\text{even}(x)=\dfrac{f(x)+f(-x)}{2}$ and $\text{odd}(x)=\dfrac{f(x)-f(-x)}{2}$ where $\text{even}(x)+\text{odd}(x)=f(x)$. Recently I stumbled into a similar claim for $n\times n$ matrices, but instead of even and odd, the set of square matrices has a direct sum representation in terms of symmetric and skew symmetric parts. So given any square matrix $A$ we can write $B=\dfrac{A+A^T}{2}$ and $C=\dfrac{A-A^T}{2}$ as the symmetric and skew symmetric representations where $A=B+C$. Does this mean that the notion of symmetric and skew symmetric have a connection to the notion of even and odd? Do they represent a generalization of even/odd, or is there another idea that generalizes these two ideas?",,"['linear-algebra', 'matrices', 'intuition']"
3,Is there always an injective map from a space in its dual space?,Is there always an injective map from a space in its dual space?,,"Today our teacher said that dual spaces are ""big"" and told us that this is a consequence by Hahn-Banach's theorem. So I was wondering whether the dual space of a space is always ""bigger"" or equal compared with the space itself? I thought a good way to check the adjective ""big"" by mathematical methods is to ask whether there is an injective map in the dual space. I am very interested in any comment on this.","Today our teacher said that dual spaces are ""big"" and told us that this is a consequence by Hahn-Banach's theorem. So I was wondering whether the dual space of a space is always ""bigger"" or equal compared with the space itself? I thought a good way to check the adjective ""big"" by mathematical methods is to ask whether there is an injective map in the dual space. I am very interested in any comment on this.",,"['real-analysis', 'linear-algebra']"
4,Linear map $f:V\rightarrow V$ injective $\Longleftrightarrow$ surjective,Linear map  injective  surjective,f:V\rightarrow V \Longleftrightarrow,"Maybe I am not good at looking for the right questions but I haven't seen this task anywhere so I hope it is no duplicate. I have to prove the following statement: Let $V$ be a finite dimensional vector space and $f:V \rightarrow V$ a linear map. Show that $f$ is injective $\Longleftrightarrow$ $f$ is surjective. The following is already known: $(i)$ $\ker f$ and $\def\Im{\operatorname{Im}}\Im f$ are linear subspaces $(ii)$ There exists an isomorphism $V/\ker f\rightarrow \Im f$ $(iii)$ If $U\subset V$ is a linear subspace then $\dim V/U=\dim V-\dim U$ $(iv)$ $f$ is injective $\Longleftrightarrow$ $\ker f=\{0\}$ So my approach is the following: $""\Longrightarrow ""$ Let $f:V\rightarrow V$ be injective. Then due to $(iv)$ $\dim\ker f=0$ . Hence due to $(ii)$ and $(iii)$ $\dim V/\ker f=\dim V=\dim\Im f$ . Since $\dim V=\dim\Im f$ it implies that $f$ is surjective. $""\Longleftarrow ""$ Let $f$ be surjective. Then $\dim\Im f=\dim V$ . Additionally, because of $(ii)$ it is $\dim\Im f=\dim V/\ker f$ . Thus $\dim V=\dim V/\ker f=\dim V-\dim\ker f$ . This means that $\dim\ker f=0$ and hence $\ker f=\{0\}$ and thus $f:V\rightarrow V$ is injective. For me it all makes sense but maybe I missed something. Thank you in advance.","Maybe I am not good at looking for the right questions but I haven't seen this task anywhere so I hope it is no duplicate. I have to prove the following statement: Let be a finite dimensional vector space and a linear map. Show that is injective is surjective. The following is already known: and are linear subspaces There exists an isomorphism If is a linear subspace then is injective So my approach is the following: Let be injective. Then due to . Hence due to and . Since it implies that is surjective. Let be surjective. Then . Additionally, because of it is . Thus . This means that and hence and thus is injective. For me it all makes sense but maybe I missed something. Thank you in advance.","V f:V \rightarrow V f \Longleftrightarrow f (i) \ker f \def\Im{\operatorname{Im}}\Im f (ii) V/\ker f\rightarrow \Im f (iii) U\subset V \dim V/U=\dim V-\dim U (iv) f \Longleftrightarrow \ker f=\{0\} ""\Longrightarrow "" f:V\rightarrow V (iv) \dim\ker f=0 (ii) (iii) \dim V/\ker f=\dim V=\dim\Im f \dim V=\dim\Im f f ""\Longleftarrow "" f \dim\Im f=\dim V (ii) \dim\Im f=\dim V/\ker f \dim V=\dim V/\ker f=\dim V-\dim\ker f \dim\ker f=0 \ker f=\{0\} f:V\rightarrow V",[]
5,Determine that a vector is in the column space of a matrix [duplicate],Determine that a vector is in the column space of a matrix [duplicate],,"This question already has answers here : Existence of solution for a linear system mod 2 (3 answers) Closed 3 years ago . I need hep with the following problem. We will work only in $\mathbb{F}_2$. Let say $A$ is a symmetric matrix with its main diagonal consisting of only $1$s. I need to prove that $1$ (the vector with all ones) is in the column space (or row-space as is symmetric) of $A$. Any help is appreciated, thanks.","This question already has answers here : Existence of solution for a linear system mod 2 (3 answers) Closed 3 years ago . I need hep with the following problem. We will work only in $\mathbb{F}_2$. Let say $A$ is a symmetric matrix with its main diagonal consisting of only $1$s. I need to prove that $1$ (the vector with all ones) is in the column space (or row-space as is symmetric) of $A$. Any help is appreciated, thanks.",,"['linear-algebra', 'matrices']"
6,How to show that exp is a diffeomorphism between symmetric reals and positive definite matrices?,How to show that exp is a diffeomorphism between symmetric reals and positive definite matrices?,,"I am looking for an easy proof of the fact that the exponential function is a diffeomorphism between the finite dimensional vector space of symmetric real nxn-matrices and the open subset of positive definite symmetric real matrices. I know that the exp-map is a bijection and that it is smooth. So, there are two ways to proceed: One way would be to use the Inverse Function Theorem. But this would require to calculate the derivative of exp at a matrix A in the direction of another direction B and then we would have to show that this linear map (seen as a linear map in terms of B) is ivertible. I have no clue how to do that. I know that it suffices to show this for a real diagonal matrix A since we can always diagonalize real symmetric matrices, but since it is not possible to diagonalize A and B simultaniously, it is no fun to calculate the matrix exponential... A totally different way would be not to use the Inverse Function Theorem, but to construct the inverse (the ""logarithm"") directly and show that it is smooth. One could use the series expansion of the real logarithm and then plug in positive definite matrices, but this seems to be complicated again... Are there any ideas how to show this without introducing too much complicated machinery? Thanks very much in advance, Tom","I am looking for an easy proof of the fact that the exponential function is a diffeomorphism between the finite dimensional vector space of symmetric real nxn-matrices and the open subset of positive definite symmetric real matrices. I know that the exp-map is a bijection and that it is smooth. So, there are two ways to proceed: One way would be to use the Inverse Function Theorem. But this would require to calculate the derivative of exp at a matrix A in the direction of another direction B and then we would have to show that this linear map (seen as a linear map in terms of B) is ivertible. I have no clue how to do that. I know that it suffices to show this for a real diagonal matrix A since we can always diagonalize real symmetric matrices, but since it is not possible to diagonalize A and B simultaniously, it is no fun to calculate the matrix exponential... A totally different way would be not to use the Inverse Function Theorem, but to construct the inverse (the ""logarithm"") directly and show that it is smooth. One could use the series expansion of the real logarithm and then plug in positive definite matrices, but this seems to be complicated again... Are there any ideas how to show this without introducing too much complicated machinery? Thanks very much in advance, Tom",,"['linear-algebra', 'lie-groups', 'exponential-function']"
7,Map to symmetric matrices is surjective.,Map to symmetric matrices is surjective.,,"Let $M_{k,n}$ be the set of all $k\times n$ matrices, $S_k$ be the set of all symmetric $k\times k$ matrices, and $I_k$ the identity $k\times k$ matrix. Suppose $A\in M_{k,n}$ is such that $AA^t=I_k$. Let $f:M_{k,n}\rightarrow S_k$ be the map $f(B)=BA^t+AB^t$. Prove that $f$ is onto (surjective). (Note: all matrices have entries in $\mathbb{R}$.) Clearly the matrix $BA^t+AB^t$ is symmetric, since $$(BA^t+AB^t)^t=(BA^t)^t+(AB^t)^t=AB^t+BA^t.$$ We want to show that for every $C\in S_k$, there exists $B\in M_{k,n}$ such that $$C=BA^t+AB^t.$$ How can we do that?","Let $M_{k,n}$ be the set of all $k\times n$ matrices, $S_k$ be the set of all symmetric $k\times k$ matrices, and $I_k$ the identity $k\times k$ matrix. Suppose $A\in M_{k,n}$ is such that $AA^t=I_k$. Let $f:M_{k,n}\rightarrow S_k$ be the map $f(B)=BA^t+AB^t$. Prove that $f$ is onto (surjective). (Note: all matrices have entries in $\mathbb{R}$.) Clearly the matrix $BA^t+AB^t$ is symmetric, since $$(BA^t+AB^t)^t=(BA^t)^t+(AB^t)^t=AB^t+BA^t.$$ We want to show that for every $C\in S_k$, there exists $B\in M_{k,n}$ such that $$C=BA^t+AB^t.$$ How can we do that?",,"['linear-algebra', 'matrices']"
8,Deducing that a matrix is indefinite using only its leading principal minors,Deducing that a matrix is indefinite using only its leading principal minors,,"$A$ is indefinite iff $A$ fits none of the above criteria. Equivalently, $A$ has both positive and negative eigenvalues. Also equivalently, $x^TAx$ is positive for at least one $x$ and negative for at least another $x$ . Note that the leading principal minors refer to the determinants of the northwest-corner submatrices, and are merely a subset of all the principal minors. Now, suppose that a symmetric $n\times n$ matrix $M$ is neither positive definite nor negative definite. Then, can we deduce the following statement (2) or at least statement (1)? If $M$ 's leading principal minors are all nonzero , then $M$ is indefinite; If $M$ has some nonzero leading principal minor, then $M$ is indefinite. I have seen both assertions separately in different texts (e.g. http://people.ds.cam.ac.uk/iar1/teaching/Hessians-DefinitenessTutorial.pdf and http://www.econ.ucsb.edu/~tedb/Courses/GraduateTheoryUCSB/BlumeSimonCh16.PDF ), but am unable to prove either. EDIT If a symmetric $n\times n$ matrix $M$ is neither positive definite nor negative definite and det $M$ is nonzero, then $M$ is indefinite. Proof: If det M is nonzero then we can immediately deduce that $M$ has no zero eigenvalues, and since $M$ is neither positive definite nor negative definite, $M$ can only be indefinite.","is indefinite iff fits none of the above criteria. Equivalently, has both positive and negative eigenvalues. Also equivalently, is positive for at least one and negative for at least another . Note that the leading principal minors refer to the determinants of the northwest-corner submatrices, and are merely a subset of all the principal minors. Now, suppose that a symmetric matrix is neither positive definite nor negative definite. Then, can we deduce the following statement (2) or at least statement (1)? If 's leading principal minors are all nonzero , then is indefinite; If has some nonzero leading principal minor, then is indefinite. I have seen both assertions separately in different texts (e.g. http://people.ds.cam.ac.uk/iar1/teaching/Hessians-DefinitenessTutorial.pdf and http://www.econ.ucsb.edu/~tedb/Courses/GraduateTheoryUCSB/BlumeSimonCh16.PDF ), but am unable to prove either. EDIT If a symmetric matrix is neither positive definite nor negative definite and det is nonzero, then is indefinite. Proof: If det M is nonzero then we can immediately deduce that has no zero eigenvalues, and since is neither positive definite nor negative definite, can only be indefinite.",A A A x^TAx x x n\times n M M M M M n\times n M M M M M M,"['linear-algebra', 'analysis', 'matrices', 'multivariable-calculus', 'optimization']"
9,Questioning a Basis for $\mathbb{Q}[\sqrt[3]{2}]$ over $\mathbb{Q}$,Questioning a Basis for  over,\mathbb{Q}[\sqrt[3]{2}] \mathbb{Q},"Let $\omega = e^{2 \pi i /3}$ and $\alpha = \sqrt[3]{2}$.  I'm seeing it claimed that $\mathcal{B} = \{\alpha, \alpha^2, \omega \alpha, \omega \alpha^2, \omega^2 \alpha, \omega^2 \alpha^2\}$ forms a basis for the vector space $\mathbb{Q}[\alpha, \omega]$ over $\mathbb{Q}$. But the vector space $\mathbb{Q}[\alpha, \omega]$ over $\mathbb{Q}$ should have all of the elements of $\mathbb{Q}$ as members, and it's not clear to me how one could express some rational $q$ as a $\mathbb{Q}$ linear combination of the elements of $\mathcal{B}$.  This is because all of the members of $\mathcal{B}$ are either strictly irrational or strictly complex so that any linear combination of them will result again in a strictly irrational or strictly complex number (and hence not a member of $\mathbb{Q}$). Am I missing something, or is $\mathcal{B}$ in fact not a basis for $\mathbb{Q}[\omega, \alpha]$? EDIT: I miswrote the claimed basis $\mathcal{B}$.  I know that adding $1$ would make it a true basis.  But as now written, the supposed basis is $\mathcal{B} = \{\alpha, \alpha^2, \omega \alpha, \omega \alpha^2, \omega^2 \alpha, \omega^2 \alpha^2\}$.","Let $\omega = e^{2 \pi i /3}$ and $\alpha = \sqrt[3]{2}$.  I'm seeing it claimed that $\mathcal{B} = \{\alpha, \alpha^2, \omega \alpha, \omega \alpha^2, \omega^2 \alpha, \omega^2 \alpha^2\}$ forms a basis for the vector space $\mathbb{Q}[\alpha, \omega]$ over $\mathbb{Q}$. But the vector space $\mathbb{Q}[\alpha, \omega]$ over $\mathbb{Q}$ should have all of the elements of $\mathbb{Q}$ as members, and it's not clear to me how one could express some rational $q$ as a $\mathbb{Q}$ linear combination of the elements of $\mathcal{B}$.  This is because all of the members of $\mathcal{B}$ are either strictly irrational or strictly complex so that any linear combination of them will result again in a strictly irrational or strictly complex number (and hence not a member of $\mathbb{Q}$). Am I missing something, or is $\mathcal{B}$ in fact not a basis for $\mathbb{Q}[\omega, \alpha]$? EDIT: I miswrote the claimed basis $\mathcal{B}$.  I know that adding $1$ would make it a true basis.  But as now written, the supposed basis is $\mathcal{B} = \{\alpha, \alpha^2, \omega \alpha, \omega \alpha^2, \omega^2 \alpha, \omega^2 \alpha^2\}$.",,"['linear-algebra', 'field-theory', 'galois-theory']"
10,"Given a reduced row exhelon form of a $4 \times 4$ matrix and two columns, how do you find the other two columns?","Given a reduced row exhelon form of a  matrix and two columns, how do you find the other two columns?",4 \times 4,"I am given the following : Let $A$ be a $4 \times 4$ matrix with RREF given by:    $$ U = \begin{bmatrix} 1 & 0 & 2 & 1 \\ 0 & 1 & 1 & 4 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ \end{bmatrix}. $$   If    $$ a_1 = \begin{bmatrix} -3 \\ 5 \\ 2 \\ 1 \end{bmatrix} \text{ and } a_2 = \begin{bmatrix} 4 \\ -3 \\ 7 \\ -1 \end{bmatrix},   $$   find $a_3$ and $a_4$. [Note: $a_1,a_2,a_3,a_4$ are the columns of $A$.] This is in the section with column, row, and null space. I don't know how those apply no this problem though... I have absolutely no idea how to do this. Can anyone help point me in the right direction?","I am given the following : Let $A$ be a $4 \times 4$ matrix with RREF given by:    $$ U = \begin{bmatrix} 1 & 0 & 2 & 1 \\ 0 & 1 & 1 & 4 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ \end{bmatrix}. $$   If    $$ a_1 = \begin{bmatrix} -3 \\ 5 \\ 2 \\ 1 \end{bmatrix} \text{ and } a_2 = \begin{bmatrix} 4 \\ -3 \\ 7 \\ -1 \end{bmatrix},   $$   find $a_3$ and $a_4$. [Note: $a_1,a_2,a_3,a_4$ are the columns of $A$.] This is in the section with column, row, and null space. I don't know how those apply no this problem though... I have absolutely no idea how to do this. Can anyone help point me in the right direction?",,['linear-algebra']
11,"If $A$ Hurwitz, $(A+A^*)$ is Hurwitz?","If  Hurwitz,  is Hurwitz?",A (A+A^*),"If I have $A$ Hurwitz matrix, is $(A+A^*)$, with $A^*$ the transpose of $A$, still Hurwitz? Any reference or proof? Because if $(A+A^*)$ is still Hurwitz I can say that it is even negative definite being symmetric and with real eigenvalues negative.","If I have $A$ Hurwitz matrix, is $(A+A^*)$, with $A^*$ the transpose of $A$, still Hurwitz? Any reference or proof? Because if $(A+A^*)$ is still Hurwitz I can say that it is even negative definite being symmetric and with real eigenvalues negative.",,"['linear-algebra', 'matrices', 'hurwitz-matrices']"
12,What is the upper bound on the error of a matrix multiplication,What is the upper bound on the error of a matrix multiplication,,"When both A and B are n x n upper-triangular matrices, the entries of C = AB are defined as follows: $$ c_{ij} = \begin{cases} \sum _{k=i}^ja_{ik}b_{kj} & 1\leq i\leq j\leq n \\0 & 1\leq j\lt i\leq n \end{cases} $$ For n = 2 show that  $$ fl(AB) = \hat A \hat B $$  where  $$ \hat A = A + E_A\\\hat B = B + E_B $$  Derive bounds for  $$||E_A||, ||E_B||$$ showing that they are small relative to ||A|| and ||B||, respectively. In other words, show that the computed product is the exact product of slightly perturbed A and B.","When both A and B are n x n upper-triangular matrices, the entries of C = AB are defined as follows: $$ c_{ij} = \begin{cases} \sum _{k=i}^ja_{ik}b_{kj} & 1\leq i\leq j\leq n \\0 & 1\leq j\lt i\leq n \end{cases} $$ For n = 2 show that  $$ fl(AB) = \hat A \hat B $$  where  $$ \hat A = A + E_A\\\hat B = B + E_B $$  Derive bounds for  $$||E_A||, ||E_B||$$ showing that they are small relative to ||A|| and ||B||, respectively. In other words, show that the computed product is the exact product of slightly perturbed A and B.",,"['linear-algebra', 'numerical-methods', 'computational-mathematics']"
13,Line-preserving transformations,Line-preserving transformations,,"Is there a name for the class of transformations on the Euclidean plane (or projective plane) that preserves lines? They are not all affine transformations; consider a perspective projection $p$ in 3D Euclidean space (center of projection at $(1,1,0)$) from the $xz$-plane to the $yz$-plane. This map is not affine. Could it be that all transformations on the plane that preserve lines are results of such projections?","Is there a name for the class of transformations on the Euclidean plane (or projective plane) that preserves lines? They are not all affine transformations; consider a perspective projection $p$ in 3D Euclidean space (center of projection at $(1,1,0)$) from the $xz$-plane to the $yz$-plane. This map is not affine. Could it be that all transformations on the plane that preserve lines are results of such projections?",,"['linear-algebra', 'vector-spaces', 'euclidean-geometry', 'projective-geometry', 'affine-geometry']"
14,Uniqueness of LDU Factorisation [Strang P105 2.6.18],Uniqueness of LDU Factorisation [Strang P105 2.6.18],,"Let $L$ be a lower triangular matrix, $D$ diagonal, and $U$ upper triangular. If $A = LDU$ and also $A = L_1D_1U_1$ with all factors invertible, then $L = L_1$ and   $D = D_1$ and $U = U_1$. 'The three factors are unique!' Hint: Derive the equation $L_1^{-1}LD = D_1 U_1 U^{-l}.$ Are the two sides triangular or diagonal?   Deduce $L = L^{-1}$ and $U = U^{-1}$ (they all have diagonal $1$'s). Then $D = D_1$. Lemma: The inverse of a (lower/upper) triangular matrix is a (lower/upper) triangular matrix. Uniqueness Proof: Assume $LDU = L_1D_1U_1$. Objective: Prove $X = X_1$ for each $X = L, D, U$. In keeping with the hint, $\color{green}{L_1^{-1}}LDU\color{#D555D1}{U^{-1}} = \color{green}{L_1^{-1}}L_1D_1U_1\color{#D555D1}{U^{-1}} \iff \color{green}{L_1^{-1}}LD = D_1U_1\color{#D555D1}{U^{-1}}$ $\Longrightarrow (\text{Lower triangular})D = D_1(\text{Upper triangular}).$ $\color{red}{\bigstar} $ So both sides are diagonal. $ \; L,U,L_1,U_1$ have diagonal $1$’s so $D = D_1$. Then $\color{green}{L_1^{-1}}L = I$ and $U_1\color{#D555D1}{U^{-1}} = I. \qquad \blacksquare$ $\Large{1.}$ I don't apprehend the (gruff) sentences after $\color{red}{\bigstar}$. Would someone please enlarge upon them? $\Large{2.}$ Without the hint, how would one divine/previse to work with $L_1^{-1}LD = D_1 U_1 U^{-l}$? This equation looks like the critical one in this proof.","Let $L$ be a lower triangular matrix, $D$ diagonal, and $U$ upper triangular. If $A = LDU$ and also $A = L_1D_1U_1$ with all factors invertible, then $L = L_1$ and   $D = D_1$ and $U = U_1$. 'The three factors are unique!' Hint: Derive the equation $L_1^{-1}LD = D_1 U_1 U^{-l}.$ Are the two sides triangular or diagonal?   Deduce $L = L^{-1}$ and $U = U^{-1}$ (they all have diagonal $1$'s). Then $D = D_1$. Lemma: The inverse of a (lower/upper) triangular matrix is a (lower/upper) triangular matrix. Uniqueness Proof: Assume $LDU = L_1D_1U_1$. Objective: Prove $X = X_1$ for each $X = L, D, U$. In keeping with the hint, $\color{green}{L_1^{-1}}LDU\color{#D555D1}{U^{-1}} = \color{green}{L_1^{-1}}L_1D_1U_1\color{#D555D1}{U^{-1}} \iff \color{green}{L_1^{-1}}LD = D_1U_1\color{#D555D1}{U^{-1}}$ $\Longrightarrow (\text{Lower triangular})D = D_1(\text{Upper triangular}).$ $\color{red}{\bigstar} $ So both sides are diagonal. $ \; L,U,L_1,U_1$ have diagonal $1$’s so $D = D_1$. Then $\color{green}{L_1^{-1}}L = I$ and $U_1\color{#D555D1}{U^{-1}} = I. \qquad \blacksquare$ $\Large{1.}$ I don't apprehend the (gruff) sentences after $\color{red}{\bigstar}$. Would someone please enlarge upon them? $\Large{2.}$ Without the hint, how would one divine/previse to work with $L_1^{-1}LD = D_1 U_1 U^{-l}$? This equation looks like the critical one in this proof.",,[]
15,How to get the upper triangular form for any linear map?,How to get the upper triangular form for any linear map?,,"Let $T:V \to V$ be linear. $V$ is a complex vector space of dimension $k$. Then there exists a basis so that the matrix generated by $T$ under that basis is upper triangular. The proof is by induction on $k$. But how to generate such a basis. The first step is that because $v$ is a complex vector space, so there exists a non-zero vector $v$ such that $Tv=\lambda v$ for some $\lambda \in \Bbb F$. So, take $e_1=v$ because then $T(e_1)=\lambda e_1$. So, in the first column of the matrix the rows 2 to n will be zero. Now if I extend $e_1$ to any basis of $v$ that basis may not be a basis for which the matrix is upper triangular. Then how to construct it ?","Let $T:V \to V$ be linear. $V$ is a complex vector space of dimension $k$. Then there exists a basis so that the matrix generated by $T$ under that basis is upper triangular. The proof is by induction on $k$. But how to generate such a basis. The first step is that because $v$ is a complex vector space, so there exists a non-zero vector $v$ such that $Tv=\lambda v$ for some $\lambda \in \Bbb F$. So, take $e_1=v$ because then $T(e_1)=\lambda e_1$. So, in the first column of the matrix the rows 2 to n will be zero. Now if I extend $e_1$ to any basis of $v$ that basis may not be a basis for which the matrix is upper triangular. Then how to construct it ?",,"['linear-algebra', 'vector-spaces']"
16,"What does the symbol: $\mathcal{F}(S,F)$ in linear algebra mean?",What does the symbol:  in linear algebra mean?,"\mathcal{F}(S,F)","I have a problem in linear algebra course, and I'm looking to solve it by myself, but I'm confused with notation since my teacher never mention it in class. It says: Let $S$ be a nonempty set and $F$     is a field. Prove that for any $s_{0}\in S$    , $\left\{ f\in\mathcal{F}\left(S,F\right)\colon f\left(s_{0}\right)=0\right\}$      is a subspace of $\mathcal{F}\left(S,F\right)$ I'm confused with the $\mathcal{F} \left(S,F\right)$ notation. It certainly a vector space (or so I thought), but which vector space? Is that a standard notation? If this information gives a better clue, it comes from Linear Algebra text by Stephen H. Friedberg, Arnold J. Insel, and Lawrence E. Space. (I can't check the book myself because I copy the questions from my friend's note)","I have a problem in linear algebra course, and I'm looking to solve it by myself, but I'm confused with notation since my teacher never mention it in class. It says: Let $S$ be a nonempty set and $F$     is a field. Prove that for any $s_{0}\in S$    , $\left\{ f\in\mathcal{F}\left(S,F\right)\colon f\left(s_{0}\right)=0\right\}$      is a subspace of $\mathcal{F}\left(S,F\right)$ I'm confused with the $\mathcal{F} \left(S,F\right)$ notation. It certainly a vector space (or so I thought), but which vector space? Is that a standard notation? If this information gives a better clue, it comes from Linear Algebra text by Stephen H. Friedberg, Arnold J. Insel, and Lawrence E. Space. (I can't check the book myself because I copy the questions from my friend's note)",,"['linear-algebra', 'vector-spaces']"
17,"Understanding a proof from Serge Lang's ""Linear Algebra"" p. 15","Understanding a proof from Serge Lang's ""Linear Algebra"" p. 15",,"I am working through Serge Lang's undergraduate text: ""Linear Algebra"" and I've gotten hung up on a particular claim in one of his proofs on p. 15. The result is: Theorem 3.1: Let $V$ be a vector space over the field $K$. Let $\left\{ v_{1},\ldots,v_{m}\right\} $ be a basis of $V$ over $K$. Let $w_{1},\ldots,w_{n}$ be elements of $V$, and assume that $n>m$. Then $w_{1},\ldots,w_{n}$ are linearly dependent. His proof starts out in the following way. Proof: Assume that $w_{1},\ldots,w_{n}$ are linearly independent. Since $\left\{ v_{1},\ldots,v_{m}\right\} $ is a basis, there exists elements $a_{1},\ldots,a_{m}\in K$ such that $w_{1}=a_{1}v_{1}+\cdots+a_{m}v_{m} $. And here is the part that I am having trouble with: By assumption, we know that $w_{1}\neq O$ and hence some $a_{i}\ne0$. I do not understand how he deduces that $w_{1}\neq O$. He has assumed that $w_{1},\ldots,w_{n}$ are linearly independent, which means that if we have a a linear combination of the form $a_{1}w_{1}+\cdots+a_{n}w_{n}=O$ then all of the coefficients $a_{1},\ldots,a_{m}$ must be zero. But I do not understand how to deduce from this that a particular $w_{i}\neq O$. It seems like there should be a simple explanation, but I have not been able to puzzle it out. Thanks in advance for the help!","I am working through Serge Lang's undergraduate text: ""Linear Algebra"" and I've gotten hung up on a particular claim in one of his proofs on p. 15. The result is: Theorem 3.1: Let $V$ be a vector space over the field $K$. Let $\left\{ v_{1},\ldots,v_{m}\right\} $ be a basis of $V$ over $K$. Let $w_{1},\ldots,w_{n}$ be elements of $V$, and assume that $n>m$. Then $w_{1},\ldots,w_{n}$ are linearly dependent. His proof starts out in the following way. Proof: Assume that $w_{1},\ldots,w_{n}$ are linearly independent. Since $\left\{ v_{1},\ldots,v_{m}\right\} $ is a basis, there exists elements $a_{1},\ldots,a_{m}\in K$ such that $w_{1}=a_{1}v_{1}+\cdots+a_{m}v_{m} $. And here is the part that I am having trouble with: By assumption, we know that $w_{1}\neq O$ and hence some $a_{i}\ne0$. I do not understand how he deduces that $w_{1}\neq O$. He has assumed that $w_{1},\ldots,w_{n}$ are linearly independent, which means that if we have a a linear combination of the form $a_{1}w_{1}+\cdots+a_{n}w_{n}=O$ then all of the coefficients $a_{1},\ldots,a_{m}$ must be zero. But I do not understand how to deduce from this that a particular $w_{i}\neq O$. It seems like there should be a simple explanation, but I have not been able to puzzle it out. Thanks in advance for the help!",,['linear-algebra']
18,Derivation linear map examples,Derivation linear map examples,,"From Humphreys' Introduction to Lie Algebras and Representation Theory : By an $F$-algebra (not necessarily associative) we simply mean a vector space $U$ over $F$ endowed with a bilinear operation $U\times U\rightarrow U$, usually denoted by juxtaposition (unless $U$ is a Lie algebra, in which case we always use the bracket). By a derivation of $U$ we mean a linear map $\delta:U\rightarrow U$ satisfying the familiar product rule $\delta(ab)=a\delta(b)+\delta(a)b$. I'm wondering what is an example of a derivation. Suppose I take $U=\mathbb{R}^n$. Clearly the map that takes everything to $0$ is a derivation. The map $\delta(x)=kx$ is not a derivation for $k\neq 0$, because then $kab\neq kab+kab$. What are some other linear maps satisfying that product rule?","From Humphreys' Introduction to Lie Algebras and Representation Theory : By an $F$-algebra (not necessarily associative) we simply mean a vector space $U$ over $F$ endowed with a bilinear operation $U\times U\rightarrow U$, usually denoted by juxtaposition (unless $U$ is a Lie algebra, in which case we always use the bracket). By a derivation of $U$ we mean a linear map $\delta:U\rightarrow U$ satisfying the familiar product rule $\delta(ab)=a\delta(b)+\delta(a)b$. I'm wondering what is an example of a derivation. Suppose I take $U=\mathbb{R}^n$. Clearly the map that takes everything to $0$ is a derivation. The map $\delta(x)=kx$ is not a derivation for $k\neq 0$, because then $kab\neq kab+kab$. What are some other linear maps satisfying that product rule?",,['linear-algebra']
19,Find a basis of $V$ containing $v$ and $w$,Find a basis of  containing  and,V v w,"Find a basis of $V$ containing $v$ and $w$, where $V=\mathbb{R}^4, v=(0,0,1,1), w=(1,1,1,1)$. I am not sure how to begin so a hint would be appreciated. I suspect I must use the following fact: If $V$ is a finite dimensional vector space and if $U$ is a subspace of $V$, then any independent subset of $U$ can be enlarged to a finite basis of $U$. Alright, here's my stab at an attempt for a solution: $V_{\text{basis}}=\{(0,0,1,1),(1,1,1,1),(0,0,0,1),(1,0,0,0) \}$ Would this be correct? The two vectors I added to the basis were the standard basis vectors for $\mathbb{R}^4.$","Find a basis of $V$ containing $v$ and $w$, where $V=\mathbb{R}^4, v=(0,0,1,1), w=(1,1,1,1)$. I am not sure how to begin so a hint would be appreciated. I suspect I must use the following fact: If $V$ is a finite dimensional vector space and if $U$ is a subspace of $V$, then any independent subset of $U$ can be enlarged to a finite basis of $U$. Alright, here's my stab at an attempt for a solution: $V_{\text{basis}}=\{(0,0,1,1),(1,1,1,1),(0,0,0,1),(1,0,0,0) \}$ Would this be correct? The two vectors I added to the basis were the standard basis vectors for $\mathbb{R}^4.$",,"['linear-algebra', 'vector-spaces', 'solution-verification']"
20,matrix representation of polynomial,matrix representation of polynomial,,"Here is a polynomial $p(x,y) = (ax + by)^2$, it can be written like this $$p(x,y) = \left(\left[ \begin{array}{cc} a & b \\ \end{array} \right] \left[ \begin{array}{c} x\\ y\\ \end{array} \right]\right)^2$$ and I know that it can also be written as something like $v^TMv$, here $v = [x,y]^T$, and $$M = \left[ \begin{array}{cc} a^2 & ab \\ ab & b^2\\ \end{array} \right]$$. But how could I find out $M$, any technique? Furthermore, here $p(x,y)$ has degree $2$, and it can be represented with the multiplication of matrix and vector, what if the degree is $3,4...$?","Here is a polynomial $p(x,y) = (ax + by)^2$, it can be written like this $$p(x,y) = \left(\left[ \begin{array}{cc} a & b \\ \end{array} \right] \left[ \begin{array}{c} x\\ y\\ \end{array} \right]\right)^2$$ and I know that it can also be written as something like $v^TMv$, here $v = [x,y]^T$, and $$M = \left[ \begin{array}{cc} a^2 & ab \\ ab & b^2\\ \end{array} \right]$$. But how could I find out $M$, any technique? Furthermore, here $p(x,y)$ has degree $2$, and it can be represented with the multiplication of matrix and vector, what if the degree is $3,4...$?",,"['linear-algebra', 'matrices', 'polynomials']"
21,How does this vector addition work in geometry?,How does this vector addition work in geometry?,,"I saw the accepted answer to the question: Finding a point along a line a certain distance away from another point! I am not getting how to use it actually to find the coordinates of the new point at a given distance. This is because I am confused between how to translate to/from the Cartesian system and the vector system. So please explain me the following by walking through the solution suggested in that answer with the following example data. Suppose I have two points $(0,0)$ and $(1,1)$ and I want to find a point at a distance which is 3/5th of the total distance between the points (i.e. $\frac{3}{5}\sqrt{2})$ from the point $(0,0)$ and lies on the segment. How do I use the vectors mentioned in the solution given there to find the required coordinates? Edit: Precisely speaking, What I do expect is the explanation of: What is vector $\mathbf v$ there if $(x_1,y_1) = (1,1)$ and $(x_0,y_0) = (0,0)$ What is the normalized vector $d\mathbf u$? How do I do the addition $(x_0,y_0) + d\mathbf u$?","I saw the accepted answer to the question: Finding a point along a line a certain distance away from another point! I am not getting how to use it actually to find the coordinates of the new point at a given distance. This is because I am confused between how to translate to/from the Cartesian system and the vector system. So please explain me the following by walking through the solution suggested in that answer with the following example data. Suppose I have two points $(0,0)$ and $(1,1)$ and I want to find a point at a distance which is 3/5th of the total distance between the points (i.e. $\frac{3}{5}\sqrt{2})$ from the point $(0,0)$ and lies on the segment. How do I use the vectors mentioned in the solution given there to find the required coordinates? Edit: Precisely speaking, What I do expect is the explanation of: What is vector $\mathbf v$ there if $(x_1,y_1) = (1,1)$ and $(x_0,y_0) = (0,0)$ What is the normalized vector $d\mathbf u$? How do I do the addition $(x_0,y_0) + d\mathbf u$?",,"['linear-algebra', 'geometry', 'analytic-geometry']"
22,Invertibility in a finite-dimensional inner product space,Invertibility in a finite-dimensional inner product space,,"Let $T$ be an invertible linear operator on a finite-dimensional inner product space. I just want a hint as to how I should prove that $T^{*}$ is also invertible and $( T^{-1} )^{*} = ( T^{*} )^{-1}$. $$ \circ \circ \circ ~ Answer ~ from ~ Below ~ \circ \circ \circ $$ $$ \langle(T^{-1})^*(T^*(v))\mid w\rangle\overset{1}{=} \langle T^*(v)\mid T^{-1}(w)\rangle\overset{2}{=} \langle v\mid T(T^{-1}(w))\rangle\overset{3}{=} \langle v\mid w\rangle $$ Could somebody explain steps $1$ through $3$, please? Actually, I think @egreg is using  this property: $\langle Ax,y \rangle = \langle x,A^*y \rangle$","Let $T$ be an invertible linear operator on a finite-dimensional inner product space. I just want a hint as to how I should prove that $T^{*}$ is also invertible and $( T^{-1} )^{*} = ( T^{*} )^{-1}$. $$ \circ \circ \circ ~ Answer ~ from ~ Below ~ \circ \circ \circ $$ $$ \langle(T^{-1})^*(T^*(v))\mid w\rangle\overset{1}{=} \langle T^*(v)\mid T^{-1}(w)\rangle\overset{2}{=} \langle v\mid T(T^{-1}(w))\rangle\overset{3}{=} \langle v\mid w\rangle $$ Could somebody explain steps $1$ through $3$, please? Actually, I think @egreg is using  this property: $\langle Ax,y \rangle = \langle x,A^*y \rangle$",,"['linear-algebra', 'vector-spaces', 'inner-products', 'adjoint-operators']"
23,Finding a unitary matrix that diagonalizes a given matrix,Finding a unitary matrix that diagonalizes a given matrix,,"Let $$T=\begin{pmatrix}5 & 0 & 0 \\ 0 & 2 & i\\ 0 & -i & 2 \end{pmatrix}$$ be a Hermitian matrix. I found the eigenvalues and eigenvectors already and they are $1,3,5$ and $\begin{pmatrix}0\\-i\\1\end{pmatrix}$,$\begin{pmatrix}0\\i\\1\end{pmatrix}$, and $\begin{pmatrix}1\\0\\0\end{pmatrix}$. Normalizing each vector, I get $<1,0,0>$, $<0,\frac{-i}{\sqrt{2}},\frac{1}{\sqrt{2}}>$, and $<0,\frac{i}{\sqrt{2}},\frac{1}{\sqrt{2}}>$. I need to find a matrix $P$ such that $P^*AP$ is diagonal.  My first idea was $P=\begin{pmatrix}1 & 0 & 0\\ 0 & -i & i\\ 0 & 1 & 1 \end{pmatrix}$.  $P^*=P^{-1}$, but $P^*P\neq I$. I'm having a some problems trying to find a $P$ that will allow me to diagonalize $T$.","Let $$T=\begin{pmatrix}5 & 0 & 0 \\ 0 & 2 & i\\ 0 & -i & 2 \end{pmatrix}$$ be a Hermitian matrix. I found the eigenvalues and eigenvectors already and they are $1,3,5$ and $\begin{pmatrix}0\\-i\\1\end{pmatrix}$,$\begin{pmatrix}0\\i\\1\end{pmatrix}$, and $\begin{pmatrix}1\\0\\0\end{pmatrix}$. Normalizing each vector, I get $<1,0,0>$, $<0,\frac{-i}{\sqrt{2}},\frac{1}{\sqrt{2}}>$, and $<0,\frac{i}{\sqrt{2}},\frac{1}{\sqrt{2}}>$. I need to find a matrix $P$ such that $P^*AP$ is diagonal.  My first idea was $P=\begin{pmatrix}1 & 0 & 0\\ 0 & -i & i\\ 0 & 1 & 1 \end{pmatrix}$.  $P^*=P^{-1}$, but $P^*P\neq I$. I'm having a some problems trying to find a $P$ that will allow me to diagonalize $T$.",,['linear-algebra']
24,Is the inverse function smooth?,Is the inverse function smooth?,,"Imagine that we have a function $Inv$ that maps $A \rightarrow A^{-1}$, where A is an invertible square matrix. now my questions is: how do i see that this function is arbitrarily often differentiable?","Imagine that we have a function $Inv$ that maps $A \rightarrow A^{-1}$, where A is an invertible square matrix. now my questions is: how do i see that this function is arbitrarily often differentiable?",,['real-analysis']
25,"Quadratic Equation with ""0"" coefficients","Quadratic Equation with ""0"" coefficients",,"Let's say I have two objects $x$ and $y$ whose position at time $t$ is given by: $$ x = a_xt^2+b_xt+c_x \\ y= a_yt^2+b_yt+c_y $$ And I want to find which (if any) values of $t$ cause $x$ to equal $y$. That is $$ (a_x-a_y)t^2+(b_x-b_y)t+(c_x-c_y) = 0 $$ This can easily be solved with quadratic equation. But what about the case when $a_x = a_y$? Now obviously this makes the problem much simpler to solve by hand! But if I were writing, lets say a simulation, where $a_x$ didn't necessarily equal $a_y$, but it could, how would I go about solving the equation? Preferably I'm looking for some sort of algorithm/equation that I can use to avoid having to write separate logic for the cases where $a_x = a_y$ and $a_x \neq a_y$. Quick background: I have knowledge of math up to Linear Algebra (I feel a very simple answer lies here, but I can't quite work it out) and ~1/2 a course in Differential Equations (though I think that doesn't really apply here), but I'm more than willing to learn something new if it provides an easy way of solving my problem. Thanks!","Let's say I have two objects $x$ and $y$ whose position at time $t$ is given by: $$ x = a_xt^2+b_xt+c_x \\ y= a_yt^2+b_yt+c_y $$ And I want to find which (if any) values of $t$ cause $x$ to equal $y$. That is $$ (a_x-a_y)t^2+(b_x-b_y)t+(c_x-c_y) = 0 $$ This can easily be solved with quadratic equation. But what about the case when $a_x = a_y$? Now obviously this makes the problem much simpler to solve by hand! But if I were writing, lets say a simulation, where $a_x$ didn't necessarily equal $a_y$, but it could, how would I go about solving the equation? Preferably I'm looking for some sort of algorithm/equation that I can use to avoid having to write separate logic for the cases where $a_x = a_y$ and $a_x \neq a_y$. Quick background: I have knowledge of math up to Linear Algebra (I feel a very simple answer lies here, but I can't quite work it out) and ~1/2 a course in Differential Equations (though I think that doesn't really apply here), but I'm more than willing to learn something new if it provides an easy way of solving my problem. Thanks!",,"['linear-algebra', 'physics', 'quadratic-forms']"
26,"""Deformation"" of the kernel of a linear map","""Deformation"" of the kernel of a linear map",,"It is known that the roots of a monic polynomial of fixed degree vary continuously (smoothly?) with its coefficients, at least over $\mathbb{C}$.  My question is whether there is such a result for linear maps.  To be precise: Let $U \subseteq \mathbb{R}^k$ be an open set and $A: U \to M_{m \times n}(\mathbb{R}) \cong \mathbb{R}^{mn}$ a smooth map.  Suppose that for all $t \in U$, $\ker(A(t))$ is $l$-dimensional.  Fix $t_0 \in U$.  Is there an open set $\tilde{U} \subseteq U$ containing $t_0$ and smooth function $f: \tilde{U} \times \mathbb{R}^l \to \mathbb{R}^n$ such that $f(t,\mathbb{R}^l) = \ker(A(t))$ for all $t \in \tilde{U}$? I think this is true.  Here is an example.  Let $A(t) = \begin{pmatrix} \cos t & -\sin t \end{pmatrix}$. Then for $t \in (-\frac{\pi}{2},\frac{\pi}{2})$, we have $\ker A(t) = \langle (\tan t, 1) \rangle$, so we can take $f(t,\lambda) = \lambda (\tan t, 1)$.  Intuitively, it seems that if the rank of a linear map is fixed, nothing else too catastrophic can happen to its level sets - similar to how nothing strange happens with the roots of a polynomial except for their number as the coefficients are varied. Can anyone come up with a counterexample, or provide a proof of this fact?","It is known that the roots of a monic polynomial of fixed degree vary continuously (smoothly?) with its coefficients, at least over $\mathbb{C}$.  My question is whether there is such a result for linear maps.  To be precise: Let $U \subseteq \mathbb{R}^k$ be an open set and $A: U \to M_{m \times n}(\mathbb{R}) \cong \mathbb{R}^{mn}$ a smooth map.  Suppose that for all $t \in U$, $\ker(A(t))$ is $l$-dimensional.  Fix $t_0 \in U$.  Is there an open set $\tilde{U} \subseteq U$ containing $t_0$ and smooth function $f: \tilde{U} \times \mathbb{R}^l \to \mathbb{R}^n$ such that $f(t,\mathbb{R}^l) = \ker(A(t))$ for all $t \in \tilde{U}$? I think this is true.  Here is an example.  Let $A(t) = \begin{pmatrix} \cos t & -\sin t \end{pmatrix}$. Then for $t \in (-\frac{\pi}{2},\frac{\pi}{2})$, we have $\ker A(t) = \langle (\tan t, 1) \rangle$, so we can take $f(t,\lambda) = \lambda (\tan t, 1)$.  Intuitively, it seems that if the rank of a linear map is fixed, nothing else too catastrophic can happen to its level sets - similar to how nothing strange happens with the roots of a polynomial except for their number as the coefficients are varied. Can anyone come up with a counterexample, or provide a proof of this fact?",,"['linear-algebra', 'continuity']"
27,Does Real Eigenvalues mean it is an hermitian Matrix,Does Real Eigenvalues mean it is an hermitian Matrix,,"Let us say I know that a given $N\times N$ matrix has all its eigenvalues as real, does it mean, it is hermitian. How do I prove (or disprove) that?","Let us say I know that a given $N\times N$ matrix has all its eigenvalues as real, does it mean, it is hermitian. How do I prove (or disprove) that?",,['linear-algebra']
28,Determine the cokernel of a linear transformation between $\mathbb Q$ vector spaces,Determine the cokernel of a linear transformation between  vector spaces,\mathbb Q,"Let $f:E\longmapsto V$ be a linear map between finite dimensional $\mathbb Q$-vector spaces with bases $\{e_1,\cdots,e_n\}$ and $\{v_1,\cdots,v_m\}$  Define $coker(f)$ to be the quotient vector space $V/Im(f)$. This is a $\mathbb Q$-vector space of dimension $m-rank(f)$. Knowing that a basis for $Im(f)$ is already determined and it is composed of the $rank(f)$ vectors among the $n$ vectors $\{f(e_1), \cdots,f(e_n)\}$ that are linearly independant,  I want to give an explicit basis for $coker(f)$, possibly in terms of the basis of $V$ and the basis of $Im(f)$. thank you for your help!","Let $f:E\longmapsto V$ be a linear map between finite dimensional $\mathbb Q$-vector spaces with bases $\{e_1,\cdots,e_n\}$ and $\{v_1,\cdots,v_m\}$  Define $coker(f)$ to be the quotient vector space $V/Im(f)$. This is a $\mathbb Q$-vector space of dimension $m-rank(f)$. Knowing that a basis for $Im(f)$ is already determined and it is composed of the $rank(f)$ vectors among the $n$ vectors $\{f(e_1), \cdots,f(e_n)\}$ that are linearly independant,  I want to give an explicit basis for $coker(f)$, possibly in terms of the basis of $V$ and the basis of $Im(f)$. thank you for your help!",,"['linear-algebra', 'abstract-algebra']"
29,Singular Value Decomposition of a block diagonal matrix,Singular Value Decomposition of a block diagonal matrix,,"For a block diagonal matrix , we have an identity for its cholesky decomposition i.e. $chol(Z) = chol(blockdiag(A,B,...)) = blockdiag(chol(A),chol(B),...)$ (Here, $Z = blockdiag(A,B,...)$) I want to know whether there any such similar identities for SVD of a block diagonal matrix. Thanks.","For a block diagonal matrix , we have an identity for its cholesky decomposition i.e. $chol(Z) = chol(blockdiag(A,B,...)) = blockdiag(chol(A),chol(B),...)$ (Here, $Z = blockdiag(A,B,...)$) I want to know whether there any such similar identities for SVD of a block diagonal matrix. Thanks.",,"['linear-algebra', 'matrices', 'block-matrices', 'svd']"
30,Equivalence classes of similar $2\times 2$ matrices,Equivalence classes of similar  matrices,2\times 2,"How can we describe the equivalence classes under the similarity relation for $2 \times 2$ matrices with respect to the field of real numbers, $\mathbb{R}$? How would the equivalence classes change if the field is $\mathbb{C}$? I know that for ${Mat} _{1\times1}(\mathbb{R})$ each matrix has its own equivalence class, and I know that for $2\times 2$ matrices, the identity and zero matrices have their own equivalence class. But how can we describe the rest of the equivalence classes with respect to transformations and bases?","How can we describe the equivalence classes under the similarity relation for $2 \times 2$ matrices with respect to the field of real numbers, $\mathbb{R}$? How would the equivalence classes change if the field is $\mathbb{C}$? I know that for ${Mat} _{1\times1}(\mathbb{R})$ each matrix has its own equivalence class, and I know that for $2\times 2$ matrices, the identity and zero matrices have their own equivalence class. But how can we describe the rest of the equivalence classes with respect to transformations and bases?",,['linear-algebra']
31,There isn't a product operation that is commmutative on $ \mathbb{R}^{n} $ that satisfies all the field axioms for $ n \geq 3 $.,There isn't a product operation that is commmutative on  that satisfies all the field axioms for ., \mathbb{R}^{n}   n \geq 3 ,"This proof is broken down into simple easy algebra and vector questions. I would like to discuss different answers and approaches. Please see pg 162-163 on books.google.ca/books?isbn=0387290524 There are 5 questions from 7.6.3 - 7.6.7. You can read the paragraph above 7.6.3. You can also read the first part on quarternions. Exclude the ""rotations of ijk space section. Here is what I have tried. Q1: I used the Pythagorean Theorem to get the norm equal to $ \sqrt{2} $. Then I used the property $ \text{Norm}(uv) = \text{Norm}(u) \text{Norm}(v) $ to show that that $ 2 = \text{Norm}(1 - i^{2}) $. Am I right? Q2: I used the property $ \text{Norm}(uv) = \text{Norm}(u) \text{Norm}(v) $ since $ \text{Norm}(i) = 1 $. But I don’t know how to show $ i^{2} = -1 $. One says to use the Triangle Inequality. I guess the equality implies that $ i^{2} $ and $ 1 $ are collinear. Q3: And thus I don’t know how to do this. Q4: The map $ p \longmapsto pi $ multiplies all distances in $ \mathbb{R}^{n} $ by $ |i| = 1 $, since $ |pi| = |p||i| $. For any points $ p_{1} $ and $ p_{2} $ in $ \mathbb{R}^{n} $, $ |p_{1} * i - p_{2} * i| = |(p_{1} - p_{2})i| = |p_{1} - p_{2}||i| $. Therefore, the distance $ |p_{1} - p_{2}| $ between any two points is multiplied by $ |i| = 1 $. Therefore, the map is an isometry of $ \mathbb{R}^{n} $. Therefore, since $ i $ and $ j $ are perpendicular directions, $ i * i $ and $ i * j $ are still perpendicular by the isometry. (An isometry preserves the distance between points.) Still not sure why $ \mathbf{1} $ and $ ij $ are perpendicular. Q5: From $ jiij= j i^{2} j = jj i^{2} = j^{2} i^{2} = - \mathbf{1} * - \mathbf{1} = \mathbf{1} $, therefore $ 1 = -1 $, which is a contradiction.","This proof is broken down into simple easy algebra and vector questions. I would like to discuss different answers and approaches. Please see pg 162-163 on books.google.ca/books?isbn=0387290524 There are 5 questions from 7.6.3 - 7.6.7. You can read the paragraph above 7.6.3. You can also read the first part on quarternions. Exclude the ""rotations of ijk space section. Here is what I have tried. Q1: I used the Pythagorean Theorem to get the norm equal to $ \sqrt{2} $. Then I used the property $ \text{Norm}(uv) = \text{Norm}(u) \text{Norm}(v) $ to show that that $ 2 = \text{Norm}(1 - i^{2}) $. Am I right? Q2: I used the property $ \text{Norm}(uv) = \text{Norm}(u) \text{Norm}(v) $ since $ \text{Norm}(i) = 1 $. But I don’t know how to show $ i^{2} = -1 $. One says to use the Triangle Inequality. I guess the equality implies that $ i^{2} $ and $ 1 $ are collinear. Q3: And thus I don’t know how to do this. Q4: The map $ p \longmapsto pi $ multiplies all distances in $ \mathbb{R}^{n} $ by $ |i| = 1 $, since $ |pi| = |p||i| $. For any points $ p_{1} $ and $ p_{2} $ in $ \mathbb{R}^{n} $, $ |p_{1} * i - p_{2} * i| = |(p_{1} - p_{2})i| = |p_{1} - p_{2}||i| $. Therefore, the distance $ |p_{1} - p_{2}| $ between any two points is multiplied by $ |i| = 1 $. Therefore, the map is an isometry of $ \mathbb{R}^{n} $. Therefore, since $ i $ and $ j $ are perpendicular directions, $ i * i $ and $ i * j $ are still perpendicular by the isometry. (An isometry preserves the distance between points.) Still not sure why $ \mathbf{1} $ and $ ij $ are perpendicular. Q5: From $ jiij= j i^{2} j = jj i^{2} = j^{2} i^{2} = - \mathbf{1} * - \mathbf{1} = \mathbf{1} $, therefore $ 1 = -1 $, which is a contradiction.",,"['linear-algebra', 'abstract-algebra', 'geometry', 'algebra-precalculus', 'quaternions']"
32,How to find a transformation matrix having several original points and their respective transformed results?,How to find a transformation matrix having several original points and their respective transformed results?,,"I have three original points $pt_1, pt_2, pt_3$ which if transformed by an unknown matrix $M$ turn into points $gd_1, gd_2, gd_3$ respectively. How can I find the matrix $M$ (all points are in 3-dimensional space)? I understand that for original points holds $M\cdot pt_i = gd_i$, so combining all $pt_i$ into matrix $PT$ and all $gd_i$ into $GD$ I'd get a matrix equation $M\cdot PT=GD$ with unknown $M$. However, many math packages solve matrix equations in form of $A\cdot x=B$, where $x$ is unknown. Is my idea of combining points into matrices correct and if so how can I solve my matrix equation?","I have three original points $pt_1, pt_2, pt_3$ which if transformed by an unknown matrix $M$ turn into points $gd_1, gd_2, gd_3$ respectively. How can I find the matrix $M$ (all points are in 3-dimensional space)? I understand that for original points holds $M\cdot pt_i = gd_i$, so combining all $pt_i$ into matrix $PT$ and all $gd_i$ into $GD$ I'd get a matrix equation $M\cdot PT=GD$ with unknown $M$. However, many math packages solve matrix equations in form of $A\cdot x=B$, where $x$ is unknown. Is my idea of combining points into matrices correct and if so how can I solve my matrix equation?",,"['linear-algebra', 'matrices', 'transformation']"
33,Orthogonal matrix with eigenvalues $\in \mathbb{R}$ is symmetric,Orthogonal matrix with eigenvalues  is symmetric,\in \mathbb{R},I need help showing that an orthogonal matrix with all real eigenvalues is symmetric.  The condition for Orthogonality is. $$O^T = O^{-1} \implies O^TO = I$$ But if O is also symmetric: $$O^T = O = O^{-1}$$ I have tried using the similarity transform relationship: $$S^{-1}OS = D = diag(\lambda_i) \implies (S^{-1}OS)^T = S^TO^{-1}S^{-1^T} = D$$ I don't really understand where the idea of real eigenvalues comes in other than the fact that symmetric matrices have all real eigen values. I'm not quite sure how to prove what I want to show without explicitly using it.,I need help showing that an orthogonal matrix with all real eigenvalues is symmetric.  The condition for Orthogonality is. $$O^T = O^{-1} \implies O^TO = I$$ But if O is also symmetric: $$O^T = O = O^{-1}$$ I have tried using the similarity transform relationship: $$S^{-1}OS = D = diag(\lambda_i) \implies (S^{-1}OS)^T = S^TO^{-1}S^{-1^T} = D$$ I don't really understand where the idea of real eigenvalues comes in other than the fact that symmetric matrices have all real eigen values. I'm not quite sure how to prove what I want to show without explicitly using it.,,"['linear-algebra', 'matrices', 'symmetric-matrices', 'orthogonal-matrices']"
34,real eigenvalue,real eigenvalue,,Let matrix $A$ be   $$\begin{bmatrix}  -5& 1& 0& 0\\   a &2& 1 &0\\   0& 1 &1 &1\\   0 &0&1& 0 \end{bmatrix}$$    where $a$ is a constant between 1 and 3. Show that the dominant eigenvalue is real. Thanks a lot!!,Let matrix $A$ be   $$\begin{bmatrix}  -5& 1& 0& 0\\   a &2& 1 &0\\   0& 1 &1 &1\\   0 &0&1& 0 \end{bmatrix}$$    where $a$ is a constant between 1 and 3. Show that the dominant eigenvalue is real. Thanks a lot!!,,"['linear-algebra', 'numerical-linear-algebra']"
35,determinants of matrices of minors,determinants of matrices of minors,,"Let $A$ be an $n \times n$ matrix and fix an integer $k$ with $1 \leq k \leq n$. Define a new matrix $\text{minor}_k(A)$ whose entries are the $k \times k$ minors of $A$. This new matrix will be $\binom{n}{k} \times \binom{n}{k}$. Theorem? Let $D$ be the determinant of $A$. The determinant of $\text{minor}_k(A)$ is $D^\binom{n-1}{k-1}$. Is this right? Can anyone provide a reference or proof? (If you want to be more precise in the definition of $\text{minor}_k(A)$, put an ordering on the cardinality $k$ subsets of $\{1, 2, \dots, n\}$, and index the rows and columns of $\text{minor}_k(A)$ using that ordering. The $(i,j)$ entry is then the determinant of the matrix obtained by keeping only the rows of $A$ indexed by the $i$th subset, and the columns of $A$ indexed by the $j$ subset. Changing the ordering shouldn't affect the determinant of the matrix of minors.)","Let $A$ be an $n \times n$ matrix and fix an integer $k$ with $1 \leq k \leq n$. Define a new matrix $\text{minor}_k(A)$ whose entries are the $k \times k$ minors of $A$. This new matrix will be $\binom{n}{k} \times \binom{n}{k}$. Theorem? Let $D$ be the determinant of $A$. The determinant of $\text{minor}_k(A)$ is $D^\binom{n-1}{k-1}$. Is this right? Can anyone provide a reference or proof? (If you want to be more precise in the definition of $\text{minor}_k(A)$, put an ordering on the cardinality $k$ subsets of $\{1, 2, \dots, n\}$, and index the rows and columns of $\text{minor}_k(A)$ using that ordering. The $(i,j)$ entry is then the determinant of the matrix obtained by keeping only the rows of $A$ indexed by the $i$th subset, and the columns of $A$ indexed by the $j$ subset. Changing the ordering shouldn't affect the determinant of the matrix of minors.)",,['linear-algebra']
36,Equivalent condition for a vector space to be finite dimensional,Equivalent condition for a vector space to be finite dimensional,,"Let $k$ be a field and let $V$ be a vector space over $k$. Then $V$ is finite dimensional if and only if for every $\phi\in End_k(V)$, there are $a_0,\dots,a_{m-1}\in k$ such that    $$\phi^m+a_{m-1}\phi^{m-1}+\cdots+a_1\phi+a_0id_V=0.$$ I have no idea on how to prove this statement. I was trying to use the fact that $V$ is finite dimensional if and only if $End_k(V)$ is finite dimensional... Could you help me with this? Thanks!","Let $k$ be a field and let $V$ be a vector space over $k$. Then $V$ is finite dimensional if and only if for every $\phi\in End_k(V)$, there are $a_0,\dots,a_{m-1}\in k$ such that    $$\phi^m+a_{m-1}\phi^{m-1}+\cdots+a_1\phi+a_0id_V=0.$$ I have no idea on how to prove this statement. I was trying to use the fact that $V$ is finite dimensional if and only if $End_k(V)$ is finite dimensional... Could you help me with this? Thanks!",,"['linear-algebra', 'abstract-algebra']"
37,$V$ is a vector space over $\mathbb Q$ of dimension $3$,is a vector space over  of dimension,V \mathbb Q 3,"$V$ is a vector space over $\mathbb Q$ of dimension $3$, and $T: V \to V$ is linear with $Tx = y$, $Ty = z$, $Tz=(x+y)$ where $x$ is non-zero. Show that $x, y, z$ are linearly independent.","$V$ is a vector space over $\mathbb Q$ of dimension $3$, and $T: V \to V$ is linear with $Tx = y$, $Ty = z$, $Tz=(x+y)$ where $x$ is non-zero. Show that $x, y, z$ are linearly independent.",,['linear-algebra']
38,Differences between Real Matrices and Complex matrices.,Differences between Real Matrices and Complex matrices.,,"I am going through a course in linear algebra. Most of the time I learn that ""this concept can be generalized to complex matrices without loss of generality"" or ""since it holds for complex matrices, it holds for real matrices also"". I was curious if there are any concepts that holds for real matrices and doesn't hold for complex matrices and vice-versa. Some trivial ones are Determinant and trace of a real matrix is real Eigen values occurs in complex conjugate pairs. Fundamental spaces associated with a real matrix are all real. thats it!!, that's all I could remember now. Any help would be appreciated.","I am going through a course in linear algebra. Most of the time I learn that ""this concept can be generalized to complex matrices without loss of generality"" or ""since it holds for complex matrices, it holds for real matrices also"". I was curious if there are any concepts that holds for real matrices and doesn't hold for complex matrices and vice-versa. Some trivial ones are Determinant and trace of a real matrix is real Eigen values occurs in complex conjugate pairs. Fundamental spaces associated with a real matrix are all real. thats it!!, that's all I could remember now. Any help would be appreciated.",,"['linear-algebra', 'matrices']"
39,Connection between the sum of a matrix's columns and the largest eigenvalues,Connection between the sum of a matrix's columns and the largest eigenvalues,,Let's say I have a matrix $A \in \mathbb{R}^{n \times n}$ . Matrix $A$ satisfies the condition that the absolute value sum of its columns is always less than or equal to $1$ . Show that all eigenvalues are less than or equal to $1$ . What's the connection between the columns and the eigenvalues?,Let's say I have a matrix . Matrix satisfies the condition that the absolute value sum of its columns is always less than or equal to . Show that all eigenvalues are less than or equal to . What's the connection between the columns and the eigenvalues?,A \in \mathbb{R}^{n \times n} A 1 1,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
40,Diagonal Matrices with Zero on Diagonal,Diagonal Matrices with Zero on Diagonal,,"As far as I understand, a diagonal matrix is one whose non-zero elements are on the main diagonal. Am I correct in assuming that the diagonal can contain zeros as well? ie: $$ \begin{bmatrix} 0&0 \\ 0&1 \\ \end{bmatrix} $$ and $$ \begin{bmatrix} 1&0 \\ 0&0 \\ \end{bmatrix} $$ are also diagonal matrices.","As far as I understand, a diagonal matrix is one whose non-zero elements are on the main diagonal. Am I correct in assuming that the diagonal can contain zeros as well? ie: $$ \begin{bmatrix} 0&0 \\ 0&1 \\ \end{bmatrix} $$ and $$ \begin{bmatrix} 1&0 \\ 0&0 \\ \end{bmatrix} $$ are also diagonal matrices.",,"['linear-algebra', 'matrices']"
41,Complexification of a real Algebra,Complexification of a real Algebra,,Let $\mathbb R$ be the field of real numbers and $\mathbb C$ be the field of complex numbers. Consider the complexification of the real matrix algebra $M_n(\mathbb R)$ that is $\mathbb C\otimes_{\mathbb R}M_n(\mathbb R)$. It is known that  $$\mathbb C\otimes_{\mathbb R}M_n(\mathbb R)\cong M_n(\mathbb C).$$ What is an example of an isomorphism from $\mathbb C\otimes_{\mathbb R}M_n(\mathbb R)$ to  $M_n(\mathbb C)?$,Let $\mathbb R$ be the field of real numbers and $\mathbb C$ be the field of complex numbers. Consider the complexification of the real matrix algebra $M_n(\mathbb R)$ that is $\mathbb C\otimes_{\mathbb R}M_n(\mathbb R)$. It is known that  $$\mathbb C\otimes_{\mathbb R}M_n(\mathbb R)\cong M_n(\mathbb C).$$ What is an example of an isomorphism from $\mathbb C\otimes_{\mathbb R}M_n(\mathbb R)$ to  $M_n(\mathbb C)?$,,['linear-algebra']
42,How to transform one nonsquare matrix into another,How to transform one nonsquare matrix into another,,"I am modeling the effect of neural activity on synaptic strength. My question, though, is mathematical. I have the following differential equation: $ \tau_{W}  \frac{d\mathbf{W}}{dt}=\mathbf{KWQ}-\alpha\mathbf{R_{\infty}}\mathbf{W}   \quad\mathbf{Q}=\left\langle \vec{u}\vec{u}\right\rangle ,\,\mathbf{R_{\infty}}=\left\langle \vec{v_{\infty}}\vec{v_{\infty}}\right\rangle $ $\tau_{W}$ and $\alpha$ are constants. $\mathbf{W}$ is $n \times m$.  $\mathbf{R_{\infty}} $ is  $n \times n$. $\mathbf{Q}$ is $m \times m$. $\mathbf{K}$ is $ n \times n$. If $\mathbf{Q}$ and $\mathbf{R_{\infty}}$ come from the outer products of vectors, how do I know that they are invertible? How would I find the value for $\mathbf{W}$ for which the derivative vanishes? Especially if neither $\mathbf{Q}$ nor $\mathbf{R_{\infty}}$ are invertible. How would I find $\mathbf{X}$ such that $ \mathbf{X}\mathbf{W} = \mathbf{Q}$ if neither $\mathbf{Q}$ nor $\mathbf{W}$ are invertible?","I am modeling the effect of neural activity on synaptic strength. My question, though, is mathematical. I have the following differential equation: $ \tau_{W}  \frac{d\mathbf{W}}{dt}=\mathbf{KWQ}-\alpha\mathbf{R_{\infty}}\mathbf{W}   \quad\mathbf{Q}=\left\langle \vec{u}\vec{u}\right\rangle ,\,\mathbf{R_{\infty}}=\left\langle \vec{v_{\infty}}\vec{v_{\infty}}\right\rangle $ $\tau_{W}$ and $\alpha$ are constants. $\mathbf{W}$ is $n \times m$.  $\mathbf{R_{\infty}} $ is  $n \times n$. $\mathbf{Q}$ is $m \times m$. $\mathbf{K}$ is $ n \times n$. If $\mathbf{Q}$ and $\mathbf{R_{\infty}}$ come from the outer products of vectors, how do I know that they are invertible? How would I find the value for $\mathbf{W}$ for which the derivative vanishes? Especially if neither $\mathbf{Q}$ nor $\mathbf{R_{\infty}}$ are invertible. How would I find $\mathbf{X}$ such that $ \mathbf{X}\mathbf{W} = \mathbf{Q}$ if neither $\mathbf{Q}$ nor $\mathbf{W}$ are invertible?",,"['linear-algebra', 'ordinary-differential-equations', 'tensors']"
43,Eigenvalue of a polynomial evaluated in a operator,Eigenvalue of a polynomial evaluated in a operator,,"Suppose $T:V\to V$, $p\in \mathcal{P}(\mathbb{C})$ (polynomials with complex coefficients), and $a\in \mathbb{C}$. Prove that $a$ is an eigenvalue of $p(T)$ if and only if $a=p(\lambda)$ for some eigenvalue $\lambda$ of $T$. I can prove: if $a=p(\lambda)$ then $a$ is a eigenvalue of $p(T)$ because: $$Tv=\lambda v$$ $$T^kv=\lambda^k v$$ $$p(T)v=p(\lambda)v=av$$ But, how can I justify the other direction? Thanks for your help.","Suppose $T:V\to V$, $p\in \mathcal{P}(\mathbb{C})$ (polynomials with complex coefficients), and $a\in \mathbb{C}$. Prove that $a$ is an eigenvalue of $p(T)$ if and only if $a=p(\lambda)$ for some eigenvalue $\lambda$ of $T$. I can prove: if $a=p(\lambda)$ then $a$ is a eigenvalue of $p(T)$ because: $$Tv=\lambda v$$ $$T^kv=\lambda^k v$$ $$p(T)v=p(\lambda)v=av$$ But, how can I justify the other direction? Thanks for your help.",,['linear-algebra']
44,Let $A$ be real symmetric $n\times n$ matrix whose only eigenvalues are 0 and 1. Pick out the true statements.,Let  be real symmetric  matrix whose only eigenvalues are 0 and 1. Pick out the true statements.,A n\times n,Let $A$ be real symmetric $n\times n$ matrix whose only eigenvalues are $0$ and $1$. Let the dimension of the null space of $A-I$ be $m$. Pick out the true statements. The characteristic  polynomial of $A$ is $(\lambda-1)^m(\lambda)^{m-n}$. $A^k = A^{k+1}$ The rank of $A$ is $m$. This is what I did: I found geometric multiplicity corresponding to eigenvalue value $0$ to be $n-m$(using symmetric matrix is diagonalizable ) while geometric multiplicity of  eigenvalue value 1 is $m$(that is given) .so the characteristic polynomial of $A$ should be $(\lambda-1)^m(\lambda)^{n-m}$. While I am not sure about other two statements. Any kind of help is highly appreciated. Thanks.,Let $A$ be real symmetric $n\times n$ matrix whose only eigenvalues are $0$ and $1$. Let the dimension of the null space of $A-I$ be $m$. Pick out the true statements. The characteristic  polynomial of $A$ is $(\lambda-1)^m(\lambda)^{m-n}$. $A^k = A^{k+1}$ The rank of $A$ is $m$. This is what I did: I found geometric multiplicity corresponding to eigenvalue value $0$ to be $n-m$(using symmetric matrix is diagonalizable ) while geometric multiplicity of  eigenvalue value 1 is $m$(that is given) .so the characteristic polynomial of $A$ should be $(\lambda-1)^m(\lambda)^{n-m}$. While I am not sure about other two statements. Any kind of help is highly appreciated. Thanks.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
45,Calculating the inertia of a real symmetric (or tridiagonal) matrix,Calculating the inertia of a real symmetric (or tridiagonal) matrix,,"I'm trying to find a quick method for evaluating the inertia of a real symmetric matrix, though I don't need to evaluate eigenvalues directly. The inertia of a matrix is a triple of the number of positive eigenvalues, negative eigenvalues and eigenvalues equal to zero. Thus far I have implemented a method of using Householder matrices to reduce a real symmetric matrix to tridiagonal form (whilst preserving inertia). A textbook by Gilbert Stewart I found through a simple Google search suggests that this is in the right direction. Does anyone have any ideas for the last step?","I'm trying to find a quick method for evaluating the inertia of a real symmetric matrix, though I don't need to evaluate eigenvalues directly. The inertia of a matrix is a triple of the number of positive eigenvalues, negative eigenvalues and eigenvalues equal to zero. Thus far I have implemented a method of using Householder matrices to reduce a real symmetric matrix to tridiagonal form (whilst preserving inertia). A textbook by Gilbert Stewart I found through a simple Google search suggests that this is in the right direction. Does anyone have any ideas for the last step?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
46,Is there a formula that can scale to find linear combinations that equal a sum?,Is there a formula that can scale to find linear combinations that equal a sum?,,"I'm not the best at math(but eager to learn) so please excuse me if I'm not explaining this problem correctly, I will try to add as much info to make it clear.  I basically receive 2 pieces of data,  one is a list of integers and the other is a target_sum, and I want to figure out all the ways I can use the list to equal the target sum. So for a list of [1,2,4] to a target_sum of 10 , I would get: 2 * 4 +  1 * 2 +  0 * 1 2 * 4 +  0 * 2 +  2 * 1 1 * 4 +  3 * 2 +  0 * 1 1 * 4 +  2 * 2 +  2 * 1 1 * 4 +  1 * 2 +  4 * 1 1 * 4 +  0 * 2 +  6 * 1 0 * 4 +  5 * 2 +  0 * 1 0 * 4 +  4 * 2 +  2 * 1 0 * 4 +  3 * 2 +  4 * 1 0 * 4 +  2 * 2 +  6 * 1 0 * 4 +  1 * 2 +  8 * 1 0 * 4 +  0 * 2 + 10 * 1 The current algorithm I'm using is two parts, one builds a look up table of what combinations are possible and the other builds the actual table: Table building: for i = 1 to k     for z = 0 to sum:         for c = 1 to z / x_i:             if T[z - c * x_i][i - 1] is true:                 set T[z][i] to true Possibility construction: function RecursivelyListAllThatWork(k, sum) // Using last k variables, make sum     /* Base case: If we've assigned all the variables correctly, list this      * solution.      */     if k == 0:         print what we have so far         return      /* Recursive step: Try all coefficients, but only if they work. */     for c = 0 to sum / x_k:        if T[sum - c * x_k][k - 1] is true:            mark the coefficient of x_k to be c            call RecursivelyListAllThatWork(k - 1, sum - c * x_k)            unmark the coefficient of x_k This is the basic idea, my actual code is a slightly different because I am using bounds to remove the possibility of infinite values(I say a single value cannot exceed the value of the sum). The problem is, the table building part does not scale. It is flawed in, at least two ways, one is its dependent on the previous number to be completed(thus I cannnot break it and run it individually for each number) and the second problem is it requires to read a table before it writes(I am learning about how to get around this technically but currently it makes the program very slow). Is there a more efficient way to do this that scales? Here's an approach I tried to take but failed(so far): create a large table full of all possible values.z to target_sum.. create another large table of T[z - c * x_i][i - 1] and compare if the values exist. If they do exists, add  T[z][i] to a third table that contains the correct master I don't need code just the logic(if this is possible).  If it helps you(as it often helps me understand) here is some python code with my approach/examples: #data = [-2,10,5,50,20,25,40] #target_sum = 100 data = [1,2,3,4,5,6,7,8,9,10] target_sum = 10  # T[x, i] is True if 'x' can be solved # by a linear combination of data[:i+1] T = []           # all values are False by default T.append([0, 0])                # base case  R=200 # Maximum size of any partial sum max_percent=0.3 # Maximum weight of any term  for i, x in enumerate(data):    # i is index, x is data[i]     for s in range(-R,R+1): #set the range of one higher than sum to include sum itself         max_value = int(abs((target_sum * max_percent)/x))         for c in range(max_value + 1):               if [s - c * x, i] in T:                 T.append([s, i+1])  coeff = [0]*len(data) def RecursivelyListAllThatWork(k, sum): # Using last k variables, make sum     # /* Base case: If we've assigned all the variables correctly, list this     # * solution.     # */     if k == 0:         # print what we have so far         print(' + '.join(""%2s*%s"" % t for t in zip(coeff, data)))         return     x_k = data[k-1]     # /* Recursive step: Try all coefficients, but only if they work. */     max_value = int(abs((target_sum * max_percent)/x_k))     for c in range(max_value + 1):        if [sum - c * x_k, k - 1] in T:            # mark the coefficient of x_k to be c            coeff[k-1] = c            RecursivelyListAllThatWork(k - 1, sum - c * x_k)            # unmark the coefficient of x_k            coeff[k-1] = 0  RecursivelyListAllThatWork(len(data), target_sum) Any help or suggestions would be appreciated. I have worked on this for a long time and all my experiments have failed.  I'm hoping to get the correct answer but even ideas of different approaches would be great so I can experiment with them. Thank you. p.s. I have asked a question on stackoverflow 2 days ago about improving my existing algo, but I got answers from posters who admitted to not fully understanding what I was asking for and because they have answered the question, I am unable to delete it to post here. I have flagged it for deletion. Update: Regarding some of the comments,I'm not looking for a fast way of doing this(although it would be nice), I'm looking for a scalable way..my method works but each loop is dependent on the last loop which causes it to be bound to a single process. The math is in such a way that it builds upon previous results. If I can somehow break the process up into independent parts then I can use more cpu/computers to handle the work. I know it'll take a long time, but if it takes 600 hours on one cpu, then two should cut it down a bit and so on..right now I can't use other computers so I'm forced to wait 600 hours(while everything else on the system is ideal). please help! Also the results are large but not infinite as I have bounds set so the number cannot exceed a certain percent of target_sum.","I'm not the best at math(but eager to learn) so please excuse me if I'm not explaining this problem correctly, I will try to add as much info to make it clear.  I basically receive 2 pieces of data,  one is a list of integers and the other is a target_sum, and I want to figure out all the ways I can use the list to equal the target sum. So for a list of [1,2,4] to a target_sum of 10 , I would get: 2 * 4 +  1 * 2 +  0 * 1 2 * 4 +  0 * 2 +  2 * 1 1 * 4 +  3 * 2 +  0 * 1 1 * 4 +  2 * 2 +  2 * 1 1 * 4 +  1 * 2 +  4 * 1 1 * 4 +  0 * 2 +  6 * 1 0 * 4 +  5 * 2 +  0 * 1 0 * 4 +  4 * 2 +  2 * 1 0 * 4 +  3 * 2 +  4 * 1 0 * 4 +  2 * 2 +  6 * 1 0 * 4 +  1 * 2 +  8 * 1 0 * 4 +  0 * 2 + 10 * 1 The current algorithm I'm using is two parts, one builds a look up table of what combinations are possible and the other builds the actual table: Table building: for i = 1 to k     for z = 0 to sum:         for c = 1 to z / x_i:             if T[z - c * x_i][i - 1] is true:                 set T[z][i] to true Possibility construction: function RecursivelyListAllThatWork(k, sum) // Using last k variables, make sum     /* Base case: If we've assigned all the variables correctly, list this      * solution.      */     if k == 0:         print what we have so far         return      /* Recursive step: Try all coefficients, but only if they work. */     for c = 0 to sum / x_k:        if T[sum - c * x_k][k - 1] is true:            mark the coefficient of x_k to be c            call RecursivelyListAllThatWork(k - 1, sum - c * x_k)            unmark the coefficient of x_k This is the basic idea, my actual code is a slightly different because I am using bounds to remove the possibility of infinite values(I say a single value cannot exceed the value of the sum). The problem is, the table building part does not scale. It is flawed in, at least two ways, one is its dependent on the previous number to be completed(thus I cannnot break it and run it individually for each number) and the second problem is it requires to read a table before it writes(I am learning about how to get around this technically but currently it makes the program very slow). Is there a more efficient way to do this that scales? Here's an approach I tried to take but failed(so far): create a large table full of all possible values.z to target_sum.. create another large table of T[z - c * x_i][i - 1] and compare if the values exist. If they do exists, add  T[z][i] to a third table that contains the correct master I don't need code just the logic(if this is possible).  If it helps you(as it often helps me understand) here is some python code with my approach/examples: #data = [-2,10,5,50,20,25,40] #target_sum = 100 data = [1,2,3,4,5,6,7,8,9,10] target_sum = 10  # T[x, i] is True if 'x' can be solved # by a linear combination of data[:i+1] T = []           # all values are False by default T.append([0, 0])                # base case  R=200 # Maximum size of any partial sum max_percent=0.3 # Maximum weight of any term  for i, x in enumerate(data):    # i is index, x is data[i]     for s in range(-R,R+1): #set the range of one higher than sum to include sum itself         max_value = int(abs((target_sum * max_percent)/x))         for c in range(max_value + 1):               if [s - c * x, i] in T:                 T.append([s, i+1])  coeff = [0]*len(data) def RecursivelyListAllThatWork(k, sum): # Using last k variables, make sum     # /* Base case: If we've assigned all the variables correctly, list this     # * solution.     # */     if k == 0:         # print what we have so far         print(' + '.join(""%2s*%s"" % t for t in zip(coeff, data)))         return     x_k = data[k-1]     # /* Recursive step: Try all coefficients, but only if they work. */     max_value = int(abs((target_sum * max_percent)/x_k))     for c in range(max_value + 1):        if [sum - c * x_k, k - 1] in T:            # mark the coefficient of x_k to be c            coeff[k-1] = c            RecursivelyListAllThatWork(k - 1, sum - c * x_k)            # unmark the coefficient of x_k            coeff[k-1] = 0  RecursivelyListAllThatWork(len(data), target_sum) Any help or suggestions would be appreciated. I have worked on this for a long time and all my experiments have failed.  I'm hoping to get the correct answer but even ideas of different approaches would be great so I can experiment with them. Thank you. p.s. I have asked a question on stackoverflow 2 days ago about improving my existing algo, but I got answers from posters who admitted to not fully understanding what I was asking for and because they have answered the question, I am unable to delete it to post here. I have flagged it for deletion. Update: Regarding some of the comments,I'm not looking for a fast way of doing this(although it would be nice), I'm looking for a scalable way..my method works but each loop is dependent on the last loop which causes it to be bound to a single process. The math is in such a way that it builds upon previous results. If I can somehow break the process up into independent parts then I can use more cpu/computers to handle the work. I know it'll take a long time, but if it takes 600 hours on one cpu, then two should cut it down a bit and so on..right now I can't use other computers so I'm forced to wait 600 hours(while everything else on the system is ideal). please help! Also the results are large but not infinite as I have bounds set so the number cannot exceed a certain percent of target_sum.",,"['linear-algebra', 'abstract-algebra', 'number-theory']"
47,number of differents vector space structures over the same field $\mathbb{F}$ on an abelian group,number of differents vector space structures over the same field  on an abelian group,\mathbb{F},"My question here raised another one. How many differents vector space structures over a field $\mathbb{F}$ we may have on an abelian group?  I know that there are abelian groups that we can not endow it with a structure of vector space over any field, for example $\mathbb{Z}_{6}$. But if an abelian group has a structure of a vector space over a field $\mathbb{F}$, is there an upper bound for ways we can define a different structure? Such as the number of automorphisms of the field $\mathbb{F}$. I am conjecturing according to the answer given before.","My question here raised another one. How many differents vector space structures over a field $\mathbb{F}$ we may have on an abelian group?  I know that there are abelian groups that we can not endow it with a structure of vector space over any field, for example $\mathbb{Z}_{6}$. But if an abelian group has a structure of a vector space over a field $\mathbb{F}$, is there an upper bound for ways we can define a different structure? Such as the number of automorphisms of the field $\mathbb{F}$. I am conjecturing according to the answer given before.",,['linear-algebra']
48,clarification on the definition of direct product of vector spaces,clarification on the definition of direct product of vector spaces,,In the Roman's book (Advanced Linear Algebra) he defines the direct product of a family of vector spaves over $\mathbb{F}$ as follows: Definition: Let $\mathcal{F}=\{V_{i}| i\in K\}$ be any family of vector spaces over $\mathbb{F}$. The direct product of $\mathcal{F}$ is the vector space $$\prod_{i\in K}V_{i}=\{f:K\rightarrow\cup_{{i\in K}} V_{i}|f(i)\in V_{i}\}$$ thought of as a subspace of the vector space of all functions from $K$ to $\cup_{{i\in K}}V_{i}$. ( Here $K$ is a set of indexes). I don't understand how $V=\{f:K\rightarrow \cup_{{i\in K}}V_{i}\}$ is a vector space over the field $\mathbb{F}$. Is the set $\cup_{{i\in K}}V_{i}$ a vector space over $\mathbb{F}$? I can't see how. Thanks for your help.,In the Roman's book (Advanced Linear Algebra) he defines the direct product of a family of vector spaves over $\mathbb{F}$ as follows: Definition: Let $\mathcal{F}=\{V_{i}| i\in K\}$ be any family of vector spaces over $\mathbb{F}$. The direct product of $\mathcal{F}$ is the vector space $$\prod_{i\in K}V_{i}=\{f:K\rightarrow\cup_{{i\in K}} V_{i}|f(i)\in V_{i}\}$$ thought of as a subspace of the vector space of all functions from $K$ to $\cup_{{i\in K}}V_{i}$. ( Here $K$ is a set of indexes). I don't understand how $V=\{f:K\rightarrow \cup_{{i\in K}}V_{i}\}$ is a vector space over the field $\mathbb{F}$. Is the set $\cup_{{i\in K}}V_{i}$ a vector space over $\mathbb{F}$? I can't see how. Thanks for your help.,,['linear-algebra']
49,Don't understand what this question about solutions of linear equations is asking.,Don't understand what this question about solutions of linear equations is asking.,,"If there exists a solution for the linear equation $A x = \left( \begin{array}{lll} 1 \\ 1 \\ 1 \end{array} \right)$, then there also exists a solution for $Ax = \left( \begin{array}{lll} 1 \\ 2 \\ 1 \end{array} \right)$. I have a list of statements, and I need to prove or disprove them. Here is an example of such a statement. I don't understand the format of the question. I know a contradictory example to the statement I'm attaching is a square matrix $3 \times 3$ of all $1$'s. Why?","If there exists a solution for the linear equation $A x = \left( \begin{array}{lll} 1 \\ 1 \\ 1 \end{array} \right)$, then there also exists a solution for $Ax = \left( \begin{array}{lll} 1 \\ 2 \\ 1 \end{array} \right)$. I have a list of statements, and I need to prove or disprove them. Here is an example of such a statement. I don't understand the format of the question. I know a contradictory example to the statement I'm attaching is a square matrix $3 \times 3$ of all $1$'s. Why?",,['linear-algebra']
50,Algebra: Orthogonal Complement,Algebra: Orthogonal Complement,,"Problem Let $V$ be a real inner product space and $U \subset V$ . Show that $(U^\perp)^\perp=U$ . Progress Clearly for $x\in U$ we have that $\langle x,v \rangle=0$ for all $v \in U^\perp$ . This immediately yields that $x \in (U^\perp)^\perp$ and so $U \subset (U^\perp)^\perp$ . Taking $x \in (U^\perp)^\perp$ , we have that $\langle x,v \rangle=0$ for all $v \in U^\perp$ . Not sure how to move it on from here though; any assistance would be much appreciated. Regards.","Problem Let be a real inner product space and . Show that . Progress Clearly for we have that for all . This immediately yields that and so . Taking , we have that for all . Not sure how to move it on from here though; any assistance would be much appreciated. Regards.","V U \subset V (U^\perp)^\perp=U x\in U \langle x,v \rangle=0 v \in U^\perp x \in (U^\perp)^\perp U \subset (U^\perp)^\perp x \in (U^\perp)^\perp \langle x,v \rangle=0 v \in U^\perp","['linear-algebra', 'vector-spaces', 'inner-products']"
51,How to create a formula for calculating armor effect in a rpg game?,How to create a formula for calculating armor effect in a rpg game?,,"So I'm making a game and want to create an equation for calculating the effect of a stat called armor. The effect is in percent and determines how much damage reduction one has against attacks. The parameters are, 1, Player level and, 2, Armor points. Here's my initial approach: I created a matrix $A$ that looks like this:  $$\left[\array{ 1 &  5 & 0.05\\ 30 & 170 & 0.5 }\right]$$ I want the effect to be 5% when the player is level 1 and has 5 armor points. And when the player is level 30 and has 170 armor points, the effect should be 50%. This all works fine, except for two problems: The solution to that Matrix result in something unsuitable: When a player increases his armor points (without gaining in level) the effect decreases . If you have ever played a rpg, it should increase . The changes to the effect are happening too fast. If $B$ is the effect when the player is level 25 and has 150 armor points and $C$ is the effect when the player is level 25 and has 151 armor points, $|B-C|$ is too big. Any suggestions on how to get by problem 1 and 2? To clarify: I rowreduce matrix A. That gives me $$\left[\array{ 1 &  0 & 0.3\\ 0 & 1 & -0.05 }\right]$$ So the equation I use is $$\text{effect} = 0.3\text{ level}-0.05\text{ armor}\;.$$ Here's what I would like to happen: 1. I would like the effect to increase as armor increases(and level standing still). 2. I would like the effect to decrease as level increases(and armor standing still).","So I'm making a game and want to create an equation for calculating the effect of a stat called armor. The effect is in percent and determines how much damage reduction one has against attacks. The parameters are, 1, Player level and, 2, Armor points. Here's my initial approach: I created a matrix $A$ that looks like this:  $$\left[\array{ 1 &  5 & 0.05\\ 30 & 170 & 0.5 }\right]$$ I want the effect to be 5% when the player is level 1 and has 5 armor points. And when the player is level 30 and has 170 armor points, the effect should be 50%. This all works fine, except for two problems: The solution to that Matrix result in something unsuitable: When a player increases his armor points (without gaining in level) the effect decreases . If you have ever played a rpg, it should increase . The changes to the effect are happening too fast. If $B$ is the effect when the player is level 25 and has 150 armor points and $C$ is the effect when the player is level 25 and has 151 armor points, $|B-C|$ is too big. Any suggestions on how to get by problem 1 and 2? To clarify: I rowreduce matrix A. That gives me $$\left[\array{ 1 &  0 & 0.3\\ 0 & 1 & -0.05 }\right]$$ So the equation I use is $$\text{effect} = 0.3\text{ level}-0.05\text{ armor}\;.$$ Here's what I would like to happen: 1. I would like the effect to increase as armor increases(and level standing still). 2. I would like the effect to decrease as level increases(and armor standing still).",,"['linear-algebra', 'matrices']"
52,Testing Linear Least Squares with Linear Inequality Constrains for Optimality,Testing Linear Least Squares with Linear Inequality Constrains for Optimality,,"I've written a C# solver for linear least squares problems with inequality constraints.  That is, given $A$, $b$, $G$, $h$ $$\min\|Ax-b\|^2\text{ s.t. }Gx\ge h$$ I have a few hand crafted test problems that my solver gives the correct answer for, and now I'd like to throw a gauntlet of randomly generated problems of various ranks at it to make sure there aren't any edge cases I'm missing. So what I need is a way to determine that a given $b$ vector calculated satisfies the constraints $Gx \ge h$ (which is easy to check for) and that the solution vector can't be improved by perturbing it in a given non-constrained direction.  The second part is what I'm at a loss for.","I've written a C# solver for linear least squares problems with inequality constraints.  That is, given $A$, $b$, $G$, $h$ $$\min\|Ax-b\|^2\text{ s.t. }Gx\ge h$$ I have a few hand crafted test problems that my solver gives the correct answer for, and now I'd like to throw a gauntlet of randomly generated problems of various ranks at it to make sure there aren't any edge cases I'm missing. So what I need is a way to determine that a given $b$ vector calculated satisfies the constraints $Gx \ge h$ (which is easy to check for) and that the solution vector can't be improved by perturbing it in a given non-constrained direction.  The second part is what I'm at a loss for.",,"['linear-algebra', 'optimization', 'numerical-linear-algebra']"
53,Where is the contradiction?,Where is the contradiction?,,"My linear algebra book states the following: Let $$a_0 + a_1 z + \dots + a_m z^m = 0.$$ (Where $z$'s signify polynomials). If at least one of the coefficients was non-zero, then there would be at most $m$ distinct values of $z$ that would satisfy this equation. Hence, by contradiction, all coefficients are equal to zero. I do not understand the reasoning here, I know the coefficients must be equal to zero, but can someone explain this specific proof?","My linear algebra book states the following: Let $$a_0 + a_1 z + \dots + a_m z^m = 0.$$ (Where $z$'s signify polynomials). If at least one of the coefficients was non-zero, then there would be at most $m$ distinct values of $z$ that would satisfy this equation. Hence, by contradiction, all coefficients are equal to zero. I do not understand the reasoning here, I know the coefficients must be equal to zero, but can someone explain this specific proof?",,['linear-algebra']
54,Special Orthogonal Group and Cayley-Hamilton theorem,Special Orthogonal Group and Cayley-Hamilton theorem,,"One of the questions on my University's algebra qual had us prove that given an arbitrary $\mathbf{A}\in SO_{3}(\mathbb{R})$, there is some constant, $-1\leq\alpha\leq3$, such that $$\mathbf{A}^{3}-\alpha\mathbf{A}^{2}+\alpha\mathbf{A}-\mathbf{I}_{3}=0.$$ Appealing to the Cayley-Hamilton theorem, and the fact that $\det\mathbf{A}=1$, I was able to (through good old fashioned number crunching) show that $$\mathbf{A}^3-(\text{tr }\mathbf{A})\mathbf{A}^2+\beta\mathbf{A}-\mathbf{I}_{3}=0,$$ where $\beta=-a_{12}a_{21} + a_{11}a_{22} - a_{13}a_{31} - a_{23}a_{32} + a_{11}a_{33} + a_{22}a_{33}$. So naturally I assumed that the $\alpha$ they were looking for was $\alpha=\text{tr }\mathbf{A}$, and that (by the restrictions placed on $\mathbf{A}$ by its orthogonality) $\beta=\alpha$. I also suspected that since $\text{tr }\mathbf{I}_{3}=3$ and $\text{tr }\begin{bmatrix}   1 & 0 & 0\\   0 & -1 & 0\\   0 & 0 & -1 \end{bmatrix}=-1$ (both of which are in $SO_{3}(\mathbb{R})$) then these might contribute to the proposed bounds on $\alpha$, however I was not able to formally show anything. Any help in proving the bounds on $\alpha$ or on showing that $\alpha=\beta$ (of which I'm pretty certain is indeed true, after trying a number of specific examples) is much appreciated.","One of the questions on my University's algebra qual had us prove that given an arbitrary $\mathbf{A}\in SO_{3}(\mathbb{R})$, there is some constant, $-1\leq\alpha\leq3$, such that $$\mathbf{A}^{3}-\alpha\mathbf{A}^{2}+\alpha\mathbf{A}-\mathbf{I}_{3}=0.$$ Appealing to the Cayley-Hamilton theorem, and the fact that $\det\mathbf{A}=1$, I was able to (through good old fashioned number crunching) show that $$\mathbf{A}^3-(\text{tr }\mathbf{A})\mathbf{A}^2+\beta\mathbf{A}-\mathbf{I}_{3}=0,$$ where $\beta=-a_{12}a_{21} + a_{11}a_{22} - a_{13}a_{31} - a_{23}a_{32} + a_{11}a_{33} + a_{22}a_{33}$. So naturally I assumed that the $\alpha$ they were looking for was $\alpha=\text{tr }\mathbf{A}$, and that (by the restrictions placed on $\mathbf{A}$ by its orthogonality) $\beta=\alpha$. I also suspected that since $\text{tr }\mathbf{I}_{3}=3$ and $\text{tr }\begin{bmatrix}   1 & 0 & 0\\   0 & -1 & 0\\   0 & 0 & -1 \end{bmatrix}=-1$ (both of which are in $SO_{3}(\mathbb{R})$) then these might contribute to the proposed bounds on $\alpha$, however I was not able to formally show anything. Any help in proving the bounds on $\alpha$ or on showing that $\alpha=\beta$ (of which I'm pretty certain is indeed true, after trying a number of specific examples) is much appreciated.",,"['linear-algebra', 'matrices']"
55,Casimir element invariance,Casimir element invariance,,"I'm sorry to bother but i'm having some problems in proving that, given a simple Lie Algebra L of finite dimension $n$ (equipped with the Killing form) and its enveloping universal algebra U(L), then the element (Casimir): c = $\sum x_iy_i$ where $(x_i)_i$ is a basis and $(y_i)_i$ is its dual basis (with respect to the Killing form) doesn't depend on the choice of a particular basis. The argument should be just related to linear algebra i guess. I tried to take another basis and the change-of-coordinates-matrix but i can't get to the solution. Can someone help me? Am I missing something? Thank you","I'm sorry to bother but i'm having some problems in proving that, given a simple Lie Algebra L of finite dimension $n$ (equipped with the Killing form) and its enveloping universal algebra U(L), then the element (Casimir): c = $\sum x_iy_i$ where $(x_i)_i$ is a basis and $(y_i)_i$ is its dual basis (with respect to the Killing form) doesn't depend on the choice of a particular basis. The argument should be just related to linear algebra i guess. I tried to take another basis and the change-of-coordinates-matrix but i can't get to the solution. Can someone help me? Am I missing something? Thank you",,['linear-algebra']
56,Matrices (Hermitian and Unitary) [closed],Matrices (Hermitian and Unitary) [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . Improve this question I have some short proofs I’m quite stuck on below. I know the definitions but get stuck on how to use them to prove what’s required. If possible, can you please explain how to apply the definition to get these proofs so that I can try again? Thanks! Q1. Show that if there exists a unitary matrix P such that $P^*AP = D$ where $P^*$ is the conjugate transpose of $P$ and $D$ is a diagonal matrix then $A$ is a normal matrix. Show that the columns of $P$ form an orthornormal basis of $\Bbb C^n$ if and only if $P$ is unitary. A1. If $P$ is unitary, $PP^*=P^*P=I$ . A is normal if $AA^*=A^*A$ . I'm not sure where to go from there. Q2. Show that eigenvectors of a Hermitian matrix corresponding to distinct eigenvalues are orthogonal. A2. Hermitian if $A=A^*$ . Eigenvalues of $A$ are given by the equation: $Av=(\lambda)v$ . Again, what do I do with this? Q3. Show that the eigenvalues of a Hermitian matrix are real, and that the eigenvectors corresponding to different eigenvalues are orthogonal. A2. Similar to the above. Q4. What is meant to say that a matrix is unitarily diagonalizable? Prove that if $P$ is a unitary matrix then all of the eigenvalues of $P$ have modulus equal to one. Further, prove that the column vectors of $P$ form an orthornormal set (with respect to the Euclidean inner product). A3. Is it unitarily diagonalizable when it can be expressed as a matrix with orthornormal vectors? I’m not sure about what the rest of the question even means! :(","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . Improve this question I have some short proofs I’m quite stuck on below. I know the definitions but get stuck on how to use them to prove what’s required. If possible, can you please explain how to apply the definition to get these proofs so that I can try again? Thanks! Q1. Show that if there exists a unitary matrix P such that where is the conjugate transpose of and is a diagonal matrix then is a normal matrix. Show that the columns of form an orthornormal basis of if and only if is unitary. A1. If is unitary, . A is normal if . I'm not sure where to go from there. Q2. Show that eigenvectors of a Hermitian matrix corresponding to distinct eigenvalues are orthogonal. A2. Hermitian if . Eigenvalues of are given by the equation: . Again, what do I do with this? Q3. Show that the eigenvalues of a Hermitian matrix are real, and that the eigenvectors corresponding to different eigenvalues are orthogonal. A2. Similar to the above. Q4. What is meant to say that a matrix is unitarily diagonalizable? Prove that if is a unitary matrix then all of the eigenvalues of have modulus equal to one. Further, prove that the column vectors of form an orthornormal set (with respect to the Euclidean inner product). A3. Is it unitarily diagonalizable when it can be expressed as a matrix with orthornormal vectors? I’m not sure about what the rest of the question even means! :(",P^*AP = D P^* P D A P \Bbb C^n P P PP^*=P^*P=I AA^*=A^*A A=A^* A Av=(\lambda)v P P P,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'unitary-matrices']"
57,Invariant subspaces,Invariant subspaces,,"Good morning, Let T be a linear transformation, which acts on a vector space V, over a field F. Let W be a sub-space  which  invariants of T, and f,g polynomials from the same field F. I need to prove that W is invariant of g(T) and f(T)(W) invariant of g(T). obviously, g(T) is also a linear mapping and therefore is also invariants of W as T does, I just don't know how to prove it correctly. Have a good day.","Good morning, Let T be a linear transformation, which acts on a vector space V, over a field F. Let W be a sub-space  which  invariants of T, and f,g polynomials from the same field F. I need to prove that W is invariant of g(T) and f(T)(W) invariant of g(T). obviously, g(T) is also a linear mapping and therefore is also invariants of W as T does, I just don't know how to prove it correctly. Have a good day.",,[]
58,Braid groups and representations,Braid groups and representations,,"I was wondering if $\mathbb{Z} \wr S_n$, where $\mathbb{Z}$ is the usual group of integers, $S_n$ the symmetric group on n elements and $\wr$ the wreath product of two groups, contains the braid group $B_n$. I was also wondering if $n+1$-dimensional matrices of the form : $$\begin{bmatrix} 1&0&0 \\\\ 1&0&1 \\\\ 1&1&0 \end{bmatrix}$$ for B2 $$\begin{bmatrix} 1&0&0&0 \\\\ 1&0&1&0 \\\\ 1&1&0&0 \\\\ 0&0&0&1 \end{bmatrix}$$ and $$\begin{bmatrix} 1&0&0&0 \\\\ 0&1&0&0 \\\\ 1&0&0&1 \\\\ 1&0&1&0 \end{bmatrix}$$  for B3 and so on... form a representation of the braid group $B_n$. Thank you for your help A. Popoff","I was wondering if $\mathbb{Z} \wr S_n$, where $\mathbb{Z}$ is the usual group of integers, $S_n$ the symmetric group on n elements and $\wr$ the wreath product of two groups, contains the braid group $B_n$. I was also wondering if $n+1$-dimensional matrices of the form : $$\begin{bmatrix} 1&0&0 \\\\ 1&0&1 \\\\ 1&1&0 \end{bmatrix}$$ for B2 $$\begin{bmatrix} 1&0&0&0 \\\\ 1&0&1&0 \\\\ 1&1&0&0 \\\\ 0&0&0&1 \end{bmatrix}$$ and $$\begin{bmatrix} 1&0&0&0 \\\\ 0&1&0&0 \\\\ 1&0&0&1 \\\\ 1&0&1&0 \end{bmatrix}$$  for B3 and so on... form a representation of the braid group $B_n$. Thank you for your help A. Popoff",,"['linear-algebra', 'abstract-algebra']"
59,Determine which of the following mappings F are linear,Determine which of the following mappings F are linear,,"I'm having a really hard time understanding how to figure out if a mapping is linear or not. Here is my homework question: Determine which of the following mappings F are linear. (a) $F: \mathbb{R}^3 \to \mathbb{R}^2$ defined by $F(x,y,z) = (x, z)$ (b) $F: \mathbb{R}^4 \to \mathbb{R}^4$ defined by $F(X) = -X$ (c) $F: \mathbb{R}^3 \to \mathbb{R}^3$ defined by $F(X) = X + (0, -1, 0)$ Sorry about my formatting. I'm not sure how to write exponents and the arrow showing that the mapping is from R^n to R^m. Any help is greatly appreciated!!","I'm having a really hard time understanding how to figure out if a mapping is linear or not. Here is my homework question: Determine which of the following mappings F are linear. (a) $F: \mathbb{R}^3 \to \mathbb{R}^2$ defined by $F(x,y,z) = (x, z)$ (b) $F: \mathbb{R}^4 \to \mathbb{R}^4$ defined by $F(X) = -X$ (c) $F: \mathbb{R}^3 \to \mathbb{R}^3$ defined by $F(X) = X + (0, -1, 0)$ Sorry about my formatting. I'm not sure how to write exponents and the arrow showing that the mapping is from R^n to R^m. Any help is greatly appreciated!!",,[]
60,"What are the linear isometries on $R^n$, equipped with the $l_1$ norm?","What are the linear isometries on , equipped with the  norm?",R^n l_1,"Which conditions must the matrix entries satisfy, and what would be an interpretation of the row and column sums of the matrix?","Which conditions must the matrix entries satisfy, and what would be an interpretation of the row and column sums of the matrix?",,['linear-algebra']
61,Indefiniteness of KKT system,Indefiniteness of KKT system,,"I know this is a trivial problem but am stuck with this thing. $A \in \mathbb{R}^{m \times m}$ is a symmetric positive definite matrix and $C \in \mathbb{R}^{m \times p}$ How do I show that the matrix $$B = \begin{bmatrix} A & C \\ C^T & 0 \end{bmatrix}$$ is indefinite? I can show that there exists a vector $v$ such that $v^TBv > 0$. (This is trivial. For instance, $v = [x , 0]^T$ where $x \in \mathbb{R}^{1 \times m}$ and $0 \in \mathbb{R}^{1 \times p}$) Now how do I find a vector such that $v^TBv < 0$. I do not need the answer. A clue/hint is welcome. You can assume that $p \leq m$ and the matrix $C$ is full rank that is to say that all the constraints are linearly independent. Thanks","I know this is a trivial problem but am stuck with this thing. $A \in \mathbb{R}^{m \times m}$ is a symmetric positive definite matrix and $C \in \mathbb{R}^{m \times p}$ How do I show that the matrix $$B = \begin{bmatrix} A & C \\ C^T & 0 \end{bmatrix}$$ is indefinite? I can show that there exists a vector $v$ such that $v^TBv > 0$. (This is trivial. For instance, $v = [x , 0]^T$ where $x \in \mathbb{R}^{1 \times m}$ and $0 \in \mathbb{R}^{1 \times p}$) Now how do I find a vector such that $v^TBv < 0$. I do not need the answer. A clue/hint is welcome. You can assume that $p \leq m$ and the matrix $C$ is full rank that is to say that all the constraints are linearly independent. Thanks",,['linear-algebra']
62,Finding determinant of a $n\times n$ matrix.,Finding determinant of a  matrix.,n\times n,"Let $A_{n\times n}$ = $((a_{ij}))$ $n\geq {3}$ , where $a_{ij}=(b_i^2-b_j^2)$ , $i,j=1,2,\ldots ,n$ for some distinct real numbers $b_1,b_2,\ldots ,b_n$ . Then what is $\det(A)$ ? Clearly the matrix $A$ is skew-symmetric and if $n$ is odd then the determinant is zero. I want to prove that for any $n\ge4$ , $\det(A)$ is $0$ . For the case when $n=4$ , I have calculated the determinant by ordinary method. I want to generalize it, please someone help. Thank you.","Let = , where , for some distinct real numbers . Then what is ? Clearly the matrix is skew-symmetric and if is odd then the determinant is zero. I want to prove that for any , is . For the case when , I have calculated the determinant by ordinary method. I want to generalize it, please someone help. Thank you.","A_{n\times n} ((a_{ij})) n\geq {3} a_{ij}=(b_i^2-b_j^2) i,j=1,2,\ldots ,n b_1,b_2,\ldots ,b_n \det(A) A n n\ge4 \det(A) 0 n=4","['linear-algebra', 'matrices', 'determinant']"
63,Quadratic form with an absolute lower bound on integer vectors: Conditions for semidefinite matrices,Quadratic form with an absolute lower bound on integer vectors: Conditions for semidefinite matrices,,"Assume that $X\in \mathrm{PSD}(n)$ is a symmetric, positive semi-definite matrix with real entries and $C>0$ is a positive real number. If $X$ satisfies $$ \forall \alpha\in\mathbb{Z}^n\setminus\{0\},\qquad \alpha^T X \alpha \geq C, $$ then is it true that $X$ is positive definite? That is, if $X$ is lower bounded on integer vectors, is it true that it is lower bounded on whole $\mathbb{R}^n$ ? Can we give a concrete lower bound for $\lambda_{\min}(X)$ in terms of $C,n$ and $\lambda_{\max}(X)$ ? The way I understand, the problem is closely related to approximations of real vectors with rational numbers: Simultaneous version of the Dirichlet's approximation theorem implies that if $v\in\mathbb{R}^n$ is a real vector and $N>0$ is a natural number, then there exists a rational vector $\beta\in\mathbb{Q}^n$ such that $$ \beta=\Big(\,\frac{p_1}{q},\, \frac{p_2}{q},\,\dots,\,\frac{p_n}{q}\,\Big)\; \text{ with }\;1\leq q\leq N\qquad\text{and}\qquad \Vert v-\beta\Vert <\frac{\sqrt{n}}{q N^{1/n}}. $$ Note that $\beta^T X \beta \geq \frac{C}{q^2}\geq \frac{C}{N^2}$ . Since $\Vert v-\beta\Vert < \frac{\sqrt{n}}{q N^{1/n}}$ and $\beta^T X\beta\geq \frac{C}{N^2}$ , I guess that choosing a large enough $N$ (relative to $\lambda_{\max}(X)$ ?) should guarantee that $v^T Xv> 0$ . Above, I am not using the sharpest bound so maybe Dirichlet's theorem might be an overkill.","Assume that is a symmetric, positive semi-definite matrix with real entries and is a positive real number. If satisfies then is it true that is positive definite? That is, if is lower bounded on integer vectors, is it true that it is lower bounded on whole ? Can we give a concrete lower bound for in terms of and ? The way I understand, the problem is closely related to approximations of real vectors with rational numbers: Simultaneous version of the Dirichlet's approximation theorem implies that if is a real vector and is a natural number, then there exists a rational vector such that Note that . Since and , I guess that choosing a large enough (relative to ?) should guarantee that . Above, I am not using the sharpest bound so maybe Dirichlet's theorem might be an overkill.","X\in \mathrm{PSD}(n) C>0 X 
\forall \alpha\in\mathbb{Z}^n\setminus\{0\},\qquad \alpha^T X \alpha \geq C,
 X X \mathbb{R}^n \lambda_{\min}(X) C,n \lambda_{\max}(X) v\in\mathbb{R}^n N>0 \beta\in\mathbb{Q}^n 
\beta=\Big(\,\frac{p_1}{q},\, \frac{p_2}{q},\,\dots,\,\frac{p_n}{q}\,\Big)\; \text{ with }\;1\leq q\leq N\qquad\text{and}\qquad \Vert v-\beta\Vert <\frac{\sqrt{n}}{q N^{1/n}}.
 \beta^T X \beta \geq \frac{C}{q^2}\geq \frac{C}{N^2} \Vert v-\beta\Vert < \frac{\sqrt{n}}{q N^{1/n}} \beta^T X\beta\geq \frac{C}{N^2} N \lambda_{\max}(X) v^T Xv> 0","['linear-algebra', 'number-theory', 'quadratic-forms']"
64,Eigenvalues of a weighted mean where the weights are positive definite matrices,Eigenvalues of a weighted mean where the weights are positive definite matrices,,"Suppose I have some positive definite matrices $A_1, A_2, \dots A_k \in \mathbb{R}^{n \times n}$ and some values $s_1 \leq s_2\dots \leq s_k \in \mathbb{R}$ . Consider the matrix $A = (\sum_{k=1}A_k s_k)(\sum_{k=1} A_k)^{-1}$ . For $n=1$ (the scalar case), $A$ becomes a weighted mean of the $s_i$ 's, and thus necessarily satisfies $s_1 \leq A \leq s_k$ . For higher $n$ , can we say that the eigenvalues of $A$ must lie between $s_1$ and $s_k$ ? I'm particularly interested if this can be shown (or disproven) in the special case where each $A_k$ is of the form $A_k = x_k x_k^\top$ .","Suppose I have some positive definite matrices and some values . Consider the matrix . For (the scalar case), becomes a weighted mean of the 's, and thus necessarily satisfies . For higher , can we say that the eigenvalues of must lie between and ? I'm particularly interested if this can be shown (or disproven) in the special case where each is of the form .","A_1, A_2, \dots A_k \in \mathbb{R}^{n \times n} s_1 \leq s_2\dots \leq s_k \in \mathbb{R} A = (\sum_{k=1}A_k s_k)(\sum_{k=1} A_k)^{-1} n=1 A s_i s_1 \leq A \leq s_k n A s_1 s_k A_k A_k = x_k x_k^\top","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-definite']"
65,Operatornorm of powers of Matricies with integer coefficient,Operatornorm of powers of Matricies with integer coefficient,,"Let $A\in GL_n(\mathbb{Z})$ have infinite order, so $A^k\neq Id_n$ for all $k>0$ . The operator norm is defined by $\lVert A \rVert=\max\{\lVert Av\rVert \mid v\in\mathbb{R}^n: \lVert v\rVert=1\}$ . It´s easy to see, that $\lVert A\rVert > 1$ . My question is: Is $\lVert A^k \rVert\geq k$ for all (big enough) $k$ ? If not, is there a constant $C>0$ with $\lVert A^k \rVert\geq Ck$ for all (big enough) $k$ ? It feels like there should be some easy solution, since there are only finitely many matrices with integer coefficients with operator norm at most $k$ (or $Ck$ ), and since $A$ has infinite order, I should eventually outgrow them, but I have no nice formal argument.","Let have infinite order, so for all . The operator norm is defined by . It´s easy to see, that . My question is: Is for all (big enough) ? If not, is there a constant with for all (big enough) ? It feels like there should be some easy solution, since there are only finitely many matrices with integer coefficients with operator norm at most (or ), and since has infinite order, I should eventually outgrow them, but I have no nice formal argument.",A\in GL_n(\mathbb{Z}) A^k\neq Id_n k>0 \lVert A \rVert=\max\{\lVert Av\rVert \mid v\in\mathbb{R}^n: \lVert v\rVert=1\} \lVert A\rVert > 1 \lVert A^k \rVert\geq k k C>0 \lVert A^k \rVert\geq Ck k k Ck A,"['linear-algebra', 'matrices', 'normed-spaces', 'general-linear-group']"
66,Is the hypothesis that $V$ be finite dimensional needed in this exercise?,Is the hypothesis that  be finite dimensional needed in this exercise?,V,"I am confused if the hypothesis that $V$ be finite dimensional is required in this exercise as I never use that hypothesis in my proof. I have typed the exercise and my own attempted proof below. Exercise: Suppose $V$ is finite-dimensional, $T \in L(V)$ , and $v \in V$ with $v \ne 0$ . Let $p$ be a nonzero polynomial of smallest degree such that $p(T)v = 0$ . Prove that every zero of $p$ is an eigenvalue of $T$ . Proof: Let $\lambda$ be a zero of $p$ . Then there exists a polynomial $q$ such that $$p(z)=(z-\lambda)q(z)$$ Evaluating $p(T)$ for $v$ we get $$p(T)v=((T-\lambda I)q(T))v=0$$ Given that $p$ is the polynomial of the smallest degree that satisfies $p(T)v=0$ , we have that $q(T)v\ne 0$ as $\deg q <\deg p$ . Then the above equation implies that $$(T-\lambda I)(q(T)v)=0$$ Because $q(T)v\ne 0$ , the above equation implies that $T-\lambda I$ is not injective. Which is equivalent to $\lambda$ being an eigenvalue of $T$ . I am not sure where I use the hypothesis that $V$ is finite dimensional. The condition that $$\text{$\lambda$ is an eigenvalue of $T\iff T-\lambda I$ is not injective}$$ is true on any vector space and not just for finite dimensional vector spaces. Am I correct in believing that the hypothesis that $V$ be finite dimensional is not needed?","I am confused if the hypothesis that be finite dimensional is required in this exercise as I never use that hypothesis in my proof. I have typed the exercise and my own attempted proof below. Exercise: Suppose is finite-dimensional, , and with . Let be a nonzero polynomial of smallest degree such that . Prove that every zero of is an eigenvalue of . Proof: Let be a zero of . Then there exists a polynomial such that Evaluating for we get Given that is the polynomial of the smallest degree that satisfies , we have that as . Then the above equation implies that Because , the above equation implies that is not injective. Which is equivalent to being an eigenvalue of . I am not sure where I use the hypothesis that is finite dimensional. The condition that is true on any vector space and not just for finite dimensional vector spaces. Am I correct in believing that the hypothesis that be finite dimensional is not needed?",V V T \in L(V) v \in V v \ne 0 p p(T)v = 0 p T \lambda p q p(z)=(z-\lambda)q(z) p(T) v p(T)v=((T-\lambda I)q(T))v=0 p p(T)v=0 q(T)v\ne 0 \deg q <\deg p (T-\lambda I)(q(T)v)=0 q(T)v\ne 0 T-\lambda I \lambda T V \text{\lambda is an eigenvalue of T\iff T-\lambda I is not injective} V,"['linear-algebra', 'polynomials', 'eigenvalues-eigenvectors']"
67,What are the singular values of Jordan blocks?,What are the singular values of Jordan blocks?,,"As per the title: consider an $n\times n$ matrix of the form $$J_n = a I_n + E_n,$$ where $E_n$ is defined componentwise as $(E_n)_{i,i+1}=1$ for all $i$ , and with zero components everywhere else. This is a single Jordan block of size $n$ . Is there a general expression for its singular values? This is simple enough to work out in the $n=2$ case, where we get $$s_\pm(J_2) = \frac{1}{\sqrt2}\sqrt{1 + 2|a|^2 \pm \sqrt{1 + 4 |a|^2}}.$$ Already for $n=3$ I get nontrivial cubic polynomial equations to solve, however. While of course it is natural that working this out for generic $n$ involves solving polynomial equations of degree $n$ , is there any way to simplify the problem, or get some understanding about the structure of the solutions more in general? Looking at it numerically, there seems to be a relatively straightforward structure to the solutions. For example, for $n=4$ and $a\in\mathbb{R}$ , we have where I'm plotting the singular values of $J_4$ as a function of $a\in[-3,3]$ . It also seems like the only thing that ever matters is $|a|$ , so the right half-plane in this plot is sufficient to cover the general case with $n=4$ , also when $a\in\mathbb{C}$ . There is clearly a lot of structure here, so how can we see it analytically by computing or somehow estimating the singular values?","As per the title: consider an matrix of the form where is defined componentwise as for all , and with zero components everywhere else. This is a single Jordan block of size . Is there a general expression for its singular values? This is simple enough to work out in the case, where we get Already for I get nontrivial cubic polynomial equations to solve, however. While of course it is natural that working this out for generic involves solving polynomial equations of degree , is there any way to simplify the problem, or get some understanding about the structure of the solutions more in general? Looking at it numerically, there seems to be a relatively straightforward structure to the solutions. For example, for and , we have where I'm plotting the singular values of as a function of . It also seems like the only thing that ever matters is , so the right half-plane in this plot is sufficient to cover the general case with , also when . There is clearly a lot of structure here, so how can we see it analytically by computing or somehow estimating the singular values?","n\times n J_n = a I_n + E_n, E_n (E_n)_{i,i+1}=1 i n n=2 s_\pm(J_2) = \frac{1}{\sqrt2}\sqrt{1 + 2|a|^2 \pm \sqrt{1 + 4 |a|^2}}. n=3 n n n=4 a\in\mathbb{R} J_4 a\in[-3,3] |a| n=4 a\in\mathbb{C}","['linear-algebra', 'eigenvalues-eigenvectors', 'svd', 'jordan-normal-form']"
68,Prove a matrix is positive semi-definite,Prove a matrix is positive semi-definite,,"Suppose we have a finite set $\Omega$ and a collection of subsets of $\Omega$ , denoted by $\{A_1,...,A_n\}$ . Define $k(X,Y)=2^{|X\cap Y|}, \forall X,Y\subset \Omega$ . Define a $n\times n$ matrix $G$ by letting $G_{i,j}=k(A_i,A_j)$ , i.e. $2^{|A_i\cap A_j|}.$ I need to show that $G$ is positive semi-definite. To this end, I fix an $x\in \mathbb R^n$ and try to show that $x^T Gx \ge 0$ . When $n=2$ , we can show that $x^T Gx$ is greater than a complete square which is nonnegative. But when $n\ge 3$ , it seems like we can no longer use trivial inequalities to get a lower bound which is complete square. I get stuck here. Thanks for any help.","Suppose we have a finite set and a collection of subsets of , denoted by . Define . Define a matrix by letting , i.e. I need to show that is positive semi-definite. To this end, I fix an and try to show that . When , we can show that is greater than a complete square which is nonnegative. But when , it seems like we can no longer use trivial inequalities to get a lower bound which is complete square. I get stuck here. Thanks for any help.","\Omega \Omega \{A_1,...,A_n\} k(X,Y)=2^{|X\cap Y|}, \forall X,Y\subset \Omega n\times n G G_{i,j}=k(A_i,A_j) 2^{|A_i\cap A_j|}. G x\in \mathbb R^n x^T Gx \ge 0 n=2 x^T Gx n\ge 3","['linear-algebra', 'matrices', 'positive-semidefinite']"
69,trying to prove a property of vector space,trying to prove a property of vector space,,"I want to prove this property: If $V$ is a vector space, $X$ a vector in $V$ , then $0\odot X = 0$ . My proof: From axiom $1\odot X = X$ , \begin{align} 1 &\odot X = X \\ &\implies& (1+0) \odot X &= X \\ &\implies& 1 \odot X \oplus 0 \odot X &= X  &&\text{(from axioms of the vector space)} \\ &\implies& X \oplus 0 \odot X &= X \\ &\implies& -X \oplus X \oplus0 \odot X &= X \oplus -X \\ &\implies& (-X \oplus X) \oplus 0 \odot X &= X \oplus -X  &&\text{(associative property)} \\ &\implies& 0 \oplus 0 \odot X &= 0 \\ &\implies& 0 \odot X &= 0 \end{align} Done. Is my proof ok? Or there is another simple way?","I want to prove this property: If is a vector space, a vector in , then . My proof: From axiom , Done. Is my proof ok? Or there is another simple way?","V X V 0\odot X = 0 1\odot X = X \begin{align}
1 &\odot X = X \\
&\implies& (1+0) \odot X &= X \\
&\implies& 1 \odot X \oplus 0 \odot X &= X 
&&\text{(from axioms of the vector space)} \\
&\implies& X \oplus 0 \odot X &= X \\
&\implies& -X \oplus X \oplus0 \odot X &= X \oplus -X \\
&\implies& (-X \oplus X) \oplus 0 \odot X &= X \oplus -X 
&&\text{(associative property)} \\
&\implies& 0 \oplus 0 \odot X &= 0 \\
&\implies& 0 \odot X &= 0
\end{align}","['linear-algebra', 'solution-verification']"
70,Eigenvalue and spectral condition,Eigenvalue and spectral condition,,"Let $A=\begin{pmatrix} 1& 1 \\ a^2 &1 \end{pmatrix} \text{ with } a\in (0,\frac{1}{2}]$ . Show $$cond_2(A)=||A||_2 \cdot ||A^{-1}||_2\leq 4(1-a^2)^{-1}$$ by first showing $||A||^2_2\leq||A||_1||A||_{\infty}$ . $||A||^2_2$ is the maximal eigenvalue of $A^TA$ and $||A||_1||A||_{\infty}=2\cdot2 = 4$ . Prove $||A||^2_2\leq||A||_1||A||_{\infty}=4$ by contradiction: Suppose $\lambda_{max} >4$ , then $$4<\lambda_{\ast}=\frac{\sqrt{a^8+2a^4+4a^2+1}+a^4+3}{2} \iff 5<\sqrt{a^8+\underbrace{2a^4}_{\leq \frac{1}{2}}+\underbrace{4a^2}_{\leq 1}+1}+\underbrace{a^4}_{\leq 1} \\ \leq \sqrt{4}+1 = 3$$ contradiction! So I know $||A||^2_2\leq 4 \Rightarrow ||A||_2\leq 2$ but I need to show $cond_2(A) = \underbrace{||A||_2}_{\leq 2}||A^{-1}||_2\leq 4(1-a^2)^{-1}$ . I don't know why I needed to show $||A||^2_2\leq||A||_1||A||_{\infty}$ first? How does it help? I still need the maximal eigenvalue of $(A^{-1})^TA^{-1}$ . The eigenvalues are $$\lambda_1=\frac{\sqrt{a^4-2a^2+5}-a^2+1}{2\sqrt{a^4-2a^2+5}-4} \\ \lambda_2=\frac{\sqrt{a^4-2a^2+5}-a^2-1}{2\sqrt{a^4-2a^2+5}+4}$$ What am I supposed to do now? Thanks for any help!","Let . Show by first showing . is the maximal eigenvalue of and . Prove by contradiction: Suppose , then contradiction! So I know but I need to show . I don't know why I needed to show first? How does it help? I still need the maximal eigenvalue of . The eigenvalues are What am I supposed to do now? Thanks for any help!","A=\begin{pmatrix} 1& 1 \\ a^2 &1 \end{pmatrix} \text{ with } a\in (0,\frac{1}{2}] cond_2(A)=||A||_2 \cdot ||A^{-1}||_2\leq 4(1-a^2)^{-1} ||A||^2_2\leq||A||_1||A||_{\infty} ||A||^2_2 A^TA ||A||_1||A||_{\infty}=2\cdot2 = 4 ||A||^2_2\leq||A||_1||A||_{\infty}=4 \lambda_{max} >4 4<\lambda_{\ast}=\frac{\sqrt{a^8+2a^4+4a^2+1}+a^4+3}{2} \iff 5<\sqrt{a^8+\underbrace{2a^4}_{\leq \frac{1}{2}}+\underbrace{4a^2}_{\leq 1}+1}+\underbrace{a^4}_{\leq 1} \\ \leq \sqrt{4}+1 = 3 ||A||^2_2\leq 4 \Rightarrow ||A||_2\leq 2 cond_2(A) = \underbrace{||A||_2}_{\leq 2}||A^{-1}||_2\leq 4(1-a^2)^{-1} ||A||^2_2\leq||A||_1||A||_{\infty} (A^{-1})^TA^{-1} \lambda_1=\frac{\sqrt{a^4-2a^2+5}-a^2+1}{2\sqrt{a^4-2a^2+5}-4} \\ \lambda_2=\frac{\sqrt{a^4-2a^2+5}-a^2-1}{2\sqrt{a^4-2a^2+5}+4}","['linear-algebra', 'matrices', 'inequality', 'eigenvalues-eigenvectors', 'condition-number']"
71,Can eigenvalues be interpreted as coordinates in a vector space of projection operators?,Can eigenvalues be interpreted as coordinates in a vector space of projection operators?,,"Consider the vector space $\mathbb R^n$ . Let $T : \mathbb R^n \to \mathbb R^n$ be a linear operator, and let $\mathbf T$ be its matrix representation with respect to the standard basis in $\mathbb R^n$ . Suppose that the eigendecomposition of $\mathbf T$ is \begin{align} \mathbf T &= \mathbf Q \mathbf \Lambda \mathbf Q^{-1} \\ &= \lambda_1 \mathbf v_1 \mathbf u_1^T + \lambda_2 \mathbf v_2 \mathbf u_2^T + \cdots + \lambda_n \mathbf v_n \mathbf u_n^T \end{align} where $\lambda_i$ is the $i$ th eigenvalue of $\mathbf{T}$ , $\mathbf{v}_i$ is the $i$ th column in $\mathbf Q$ , and $\mathbf{u}_i^T$ is the $i$ th row in $\mathbf Q^{-1}$ . The outer products $\mathbf v_i \mathbf u_i^T$ represent ""scaled"" projection operators, such that if each of them was replaced with $\mathbf v_i \mathbf u_i^T / \mathbf u_i^T \mathbf v_i$ , then they would be true projection operators. I am not sure if the set of all projection operators on $\mathbb R^n$ forms a vector space, since this set is not a subspace of the vector space of all linear operators on $\mathbb R^n$ (not closed under scalar multiplication). However, assuming that the set of all projection operators is indeed a vector space, wouldn't the outer products $\mathbf v_1 \mathbf u_1^T,\mathbf v_2 \mathbf u_2^T,\dots,\mathbf v_n \mathbf u_n^T$ form a basis for this vector space? Furthermore, since $\mathbf T$ is expressed as a linear combination of basis vectors, then wouldn't the eigenvalues be coordinates in this vector space? Essentially, what I am asking is: do linear operators on $\mathbb R^n$ live in a vector space of projections, and are they ""located"" by their eigenvalues in this vector space?","Consider the vector space . Let be a linear operator, and let be its matrix representation with respect to the standard basis in . Suppose that the eigendecomposition of is where is the th eigenvalue of , is the th column in , and is the th row in . The outer products represent ""scaled"" projection operators, such that if each of them was replaced with , then they would be true projection operators. I am not sure if the set of all projection operators on forms a vector space, since this set is not a subspace of the vector space of all linear operators on (not closed under scalar multiplication). However, assuming that the set of all projection operators is indeed a vector space, wouldn't the outer products form a basis for this vector space? Furthermore, since is expressed as a linear combination of basis vectors, then wouldn't the eigenvalues be coordinates in this vector space? Essentially, what I am asking is: do linear operators on live in a vector space of projections, and are they ""located"" by their eigenvalues in this vector space?","\mathbb R^n T : \mathbb R^n \to \mathbb R^n \mathbf T \mathbb R^n \mathbf T \begin{align}
\mathbf T &= \mathbf Q \mathbf \Lambda \mathbf Q^{-1} \\
&= \lambda_1 \mathbf v_1 \mathbf u_1^T + \lambda_2 \mathbf v_2 \mathbf u_2^T + \cdots + \lambda_n \mathbf v_n \mathbf u_n^T
\end{align} \lambda_i i \mathbf{T} \mathbf{v}_i i \mathbf Q \mathbf{u}_i^T i \mathbf Q^{-1} \mathbf v_i \mathbf u_i^T \mathbf v_i \mathbf u_i^T / \mathbf u_i^T \mathbf v_i \mathbb R^n \mathbb R^n \mathbf v_1 \mathbf u_1^T,\mathbf v_2 \mathbf u_2^T,\dots,\mathbf v_n \mathbf u_n^T \mathbf T \mathbb R^n",['linear-algebra']
72,Geometric interpretation of $A^TA$,Geometric interpretation of,A^TA,"For a transformation $A \in \mathbb{R}^{n\times m}$ what exactly is the geometric interpretation of the transformation $A^TA$ . If I understand it correctly the entries of $A^TA$ are the inner products or the columns of $A$ but how exactly should I interpret this geometrically as a linear transformation? And why is $A^TA$ often loosely called squaring the matrix, how does having the pairwise inner products (which normally are interpreted as projecting one vector on the other) yield us something close to a matrix squared? One thing I've noticed is that using the SVD for a real matrix $A$ we get $A = UDV^T \leftrightarrow A^TA=VD^TU^TUDV^T=VD^TDV^T$ where $U$ and $V$ are orthogonal, but how does changing basis to $V$ and scaling by the eigenvalues squared relate to the concept above?","For a transformation what exactly is the geometric interpretation of the transformation . If I understand it correctly the entries of are the inner products or the columns of but how exactly should I interpret this geometrically as a linear transformation? And why is often loosely called squaring the matrix, how does having the pairwise inner products (which normally are interpreted as projecting one vector on the other) yield us something close to a matrix squared? One thing I've noticed is that using the SVD for a real matrix we get where and are orthogonal, but how does changing basis to and scaling by the eigenvalues squared relate to the concept above?",A \in \mathbb{R}^{n\times m} A^TA A^TA A A^TA A A = UDV^T \leftrightarrow A^TA=VD^TU^TUDV^T=VD^TDV^T U V V,"['linear-algebra', 'matrices', 'linear-transformations', 'svd', 'geometric-interpretation']"
73,What is wrong with my reasoning regarding tensor products?,What is wrong with my reasoning regarding tensor products?,,"$\def\Rbb{\mathbf{R}}$ Let $F$ be a subfield of the field $K$ and let $V$ be an $n$ -dimensional vector space over $F$ . Then $K\otimes_FV\cong K^n$ . Considering the case when $F=K=\Rbb$ and $V=\Rbb^1$ , I have $$ \Rbb\otimes_\Rbb\Rbb^1=\Rbb^1(=\Rbb)\;. $$ On the other hand, let $V$ and $W$ be two (finite-dimensional) vector spaces over the field $F$ . Then $$ V\otimes_F W\cong L(V,W;F)\;, $$ where $L(V,W;F)$ denotes the set of all the bilinear maps from $V\times W$ to $F$ . If I let $V=W=\Rbb^1$ , and $F=\Rbb$ , then $$ \Rbb^1\otimes_\Rbb\Rbb^1 \cong L(\Rbb^1,\Rbb^1;\Rbb) \cong \Rbb^2 $$ But $\Rbb^1$ cannot be isomorphic to $\Rbb^2$ . What is going wrong here? I guess one cannot ""identify"" $\Rbb^1$ and $\Rbb$ when talking about tensor products since one is a ""vector space"" while the other is a ""field"".","Let be a subfield of the field and let be an -dimensional vector space over . Then . Considering the case when and , I have On the other hand, let and be two (finite-dimensional) vector spaces over the field . Then where denotes the set of all the bilinear maps from to . If I let , and , then But cannot be isomorphic to . What is going wrong here? I guess one cannot ""identify"" and when talking about tensor products since one is a ""vector space"" while the other is a ""field"".","\def\Rbb{\mathbf{R}} F K V n F K\otimes_FV\cong K^n F=K=\Rbb V=\Rbb^1 
\Rbb\otimes_\Rbb\Rbb^1=\Rbb^1(=\Rbb)\;.
 V W F 
V\otimes_F W\cong L(V,W;F)\;,
 L(V,W;F) V\times W F V=W=\Rbb^1 F=\Rbb 
\Rbb^1\otimes_\Rbb\Rbb^1
\cong L(\Rbb^1,\Rbb^1;\Rbb)
\cong \Rbb^2
 \Rbb^1 \Rbb^2 \Rbb^1 \Rbb",['linear-algebra']
74,Prove a quotient space and a subspace are isomorphic,Prove a quotient space and a subspace are isomorphic,,"Let $V_1$ and $V_2$ be subspaces of a vector space $V$ such that $V=V_1+V_2$ . Let $\alpha$ be a basic for $V_1 \cap V_2$ . Extend $\alpha$ to a basic $\alpha \cup \alpha_1$ for $V_1$ and $\alpha \cup \alpha_2$ for $V_2$ . (where $\alpha$ , $\alpha_1$ and $\alpha_2$ are pairwise disjoint). Define $V'=span\,\alpha_2$ . I need to prove that $V/V_1$ is isomorphic to $V'$ . I tried to prove that $V=V_1 \oplus V'$ but failed. Also I tried to construct an isomorphism but found out that it isn't well-defined. Any thoughts about this question?","Let and be subspaces of a vector space such that . Let be a basic for . Extend to a basic for and for . (where , and are pairwise disjoint). Define . I need to prove that is isomorphic to . I tried to prove that but failed. Also I tried to construct an isomorphism but found out that it isn't well-defined. Any thoughts about this question?","V_1 V_2 V V=V_1+V_2 \alpha V_1 \cap V_2 \alpha \alpha \cup \alpha_1 V_1 \alpha \cup \alpha_2 V_2 \alpha \alpha_1 \alpha_2 V'=span\,\alpha_2 V/V_1 V' V=V_1 \oplus V'","['linear-algebra', 'vector-spaces', 'linear-transformations']"
75,Is the set of matrices with constrained condition numbers a convex set?,Is the set of matrices with constrained condition numbers a convex set?,,"Let $\mathbb{S}_{\tau}^{+,p}$ indicates the $p\times p$ real symmetric positive-semidefinite matrix, whose condition number, defined as the ratio of maximum eigenvalue and minimum eigenvalue, is less or equal to $\tau$ . Is $\mathbb{S}_{\tau}^{+,p}$ convex? I attempted to use the definition of convexity to prove it. Suppose $M_1,M_2\in \mathbb{S}_{\tau}^{+,p}$ , and $\forall v\in(0,1)$ , I would like to show $vM_1+(1-v)M_2\in\mathbb{S}_{\tau}^{+,p}$ . I understand that $$\lambda_{\text{min}}\left[\nu M_1+(1-\nu)M_2\right]\geq\lambda_{\text{min}}[\nu M_1]+\lambda_{\text{min}}[(1-\nu) M_2]\geq\min\{\lambda_{\text{min}}[M_1], \lambda_{\text{min}}[M_2]\}$$ and $$\lambda_{\text{max}}\left[\nu M_1+(1-\nu)M_2\right]\leq\lambda_{\text{max}}[\nu M_1]+\lambda_{\text{max}}[(1-\nu) M_2]\leq\max\{\lambda_{\text{max}}[M_1], \lambda_{\text{max}}[M_2]\},$$ but they do not guarantee that $$\frac{\lambda_{\text{max}}\left[\nu M_1+(1-\nu)M_2\right]}{\lambda_{\text{min}}\left[\nu M_1+(1-\nu)M_2\right]}\leq \tau$$ If the statement is not true, can you give a simple counterexample? Thanks!","Let indicates the real symmetric positive-semidefinite matrix, whose condition number, defined as the ratio of maximum eigenvalue and minimum eigenvalue, is less or equal to . Is convex? I attempted to use the definition of convexity to prove it. Suppose , and , I would like to show . I understand that and but they do not guarantee that If the statement is not true, can you give a simple counterexample? Thanks!","\mathbb{S}_{\tau}^{+,p} p\times p \tau \mathbb{S}_{\tau}^{+,p} M_1,M_2\in \mathbb{S}_{\tau}^{+,p} \forall v\in(0,1) vM_1+(1-v)M_2\in\mathbb{S}_{\tau}^{+,p} \lambda_{\text{min}}\left[\nu M_1+(1-\nu)M_2\right]\geq\lambda_{\text{min}}[\nu M_1]+\lambda_{\text{min}}[(1-\nu) M_2]\geq\min\{\lambda_{\text{min}}[M_1], \lambda_{\text{min}}[M_2]\} \lambda_{\text{max}}\left[\nu M_1+(1-\nu)M_2\right]\leq\lambda_{\text{max}}[\nu M_1]+\lambda_{\text{max}}[(1-\nu) M_2]\leq\max\{\lambda_{\text{max}}[M_1], \lambda_{\text{max}}[M_2]\}, \frac{\lambda_{\text{max}}\left[\nu M_1+(1-\nu)M_2\right]}{\lambda_{\text{min}}\left[\nu M_1+(1-\nu)M_2\right]}\leq \tau","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'convex-analysis', 'condition-number']"
76,Prove that every ray is a polyhedron,Prove that every ray is a polyhedron,,"I am reading a book on linear optimization and I am stuck with the following problem: Prove that every ray in $\mathbb{R}^n$ is a polyhedron. The book defines a ray as follows: For a point $x_0\in\mathbb{R}^n$ and a vector $d\in\mathbb{R}^n$ , a ray is the set $$\{x_0+\lambda d \mid \forall \lambda \in \mathbb{R} \, \text{such that} \, \lambda \ge 0\}$$ It also defines a polyhedron as a set of solutions to a system of linear inequalities. My idea is to find a linear transformation $A$ , whose kernel is the subspace spanned by $d$ . Consequently, the set of solutions to the equation $Ax=Ax_0$ will be a line containing the ray. But there is two problems. First I do not know how to find such a $A$ . Second, I do not know what to do next. Any help is appreciated.","I am reading a book on linear optimization and I am stuck with the following problem: Prove that every ray in is a polyhedron. The book defines a ray as follows: For a point and a vector , a ray is the set It also defines a polyhedron as a set of solutions to a system of linear inequalities. My idea is to find a linear transformation , whose kernel is the subspace spanned by . Consequently, the set of solutions to the equation will be a line containing the ray. But there is two problems. First I do not know how to find such a . Second, I do not know what to do next. Any help is appreciated.","\mathbb{R}^n x_0\in\mathbb{R}^n d\in\mathbb{R}^n \{x_0+\lambda d \mid \forall \lambda \in \mathbb{R} \, \text{such that} \, \lambda \ge 0\} A d Ax=Ax_0 A","['linear-algebra', 'convex-analysis', 'linear-programming', 'polyhedra']"
77,Sparse Approximation in the Mahalanobis Distance,Sparse Approximation in the Mahalanobis Distance,,"Given a vector $z \in \mathbb{R}^n$ and $k < n$ , finding the best $k$ -sparse approximation to $z$ in terms of the Euclidean distance means solving $$\min_{\{x \in \mathbb{R}^n : ||x||_0 \le k\}} ||z - x||_2$$ This can easily be done by choosing $x$ such that it consists of the $k$ largest components of $z$ in terms of $| \cdot |$ and zero in every other component. I was now thinking about what happens if we slightly modify the question to allow for other metrics on $\mathbb{R}^n$ . For example, what would happen if we instead try to solve $$\min_{\{x \in \mathbb{R}^n : ||x||_0 \le k\}} (x-z)^TA(x-z)$$ for a symmetric positive definite matrix $A$ ? I guess this is much harder, but are there good algorithms that do this?","Given a vector and , finding the best -sparse approximation to in terms of the Euclidean distance means solving This can easily be done by choosing such that it consists of the largest components of in terms of and zero in every other component. I was now thinking about what happens if we slightly modify the question to allow for other metrics on . For example, what would happen if we instead try to solve for a symmetric positive definite matrix ? I guess this is much harder, but are there good algorithms that do this?",z \in \mathbb{R}^n k < n k z \min_{\{x \in \mathbb{R}^n : ||x||_0 \le k\}} ||z - x||_2 x k z | \cdot | \mathbb{R}^n \min_{\{x \in \mathbb{R}^n : ||x||_0 \le k\}} (x-z)^TA(x-z) A,"['real-analysis', 'linear-algebra', 'optimization', 'sparsity', 'mahalanobis-distance']"
78,"""rotating"" a matrix instead of transposing it","""rotating"" a matrix instead of transposing it",,"This is a silly question asked just out of curiosity. The question has nothing to do with rotation matrices as far as I know, but I didn't know how else to refer to the following operation. In linear algebra, suppose I have an $m\times n$ matrix $$ A = \begin{pmatrix} a_{11} & \dots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{m1} & \dots & a_{mn} \end{pmatrix} $$ Then suppose I ""rotate"" the elements of this matrix by 90 degrees to get an $n\times m$ matrix $$ A^\circlearrowright = \begin{pmatrix} a_{m1} & \dots & a_{11} \\ \vdots & \ddots & \vdots \\ a_{mn} & \dots & a_{1n} \end{pmatrix}. $$ This is similar to the transpose, in that the transpose $A^T$ reflects the matrix $A$ along its diagonal, while $A^\circlearrowright$ rotates it by 90 degrees instead. The question is just whether this ""rotation"" operation has any sensible meaning when $A$ is interpreted as a linear transformation. I am guessing the answer is no, but it seemed worth asking anyway, just in case it's something that's been considered for some reason. A related operation is what we could call the ""anti-transpose"", which reflects the matrix along the 'other' diagonal and also results in an $n\times m$ matrix: $$ A^\bot \begin{pmatrix} a_{mn} & \dots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{m1} & \dots & a_{11} \end{pmatrix} $$ An application of this anti-transpose operation is mentioned in this answer on mathoverflow. (Thanks to Torsten Schoeneberg for pointing that out.)","This is a silly question asked just out of curiosity. The question has nothing to do with rotation matrices as far as I know, but I didn't know how else to refer to the following operation. In linear algebra, suppose I have an matrix Then suppose I ""rotate"" the elements of this matrix by 90 degrees to get an matrix This is similar to the transpose, in that the transpose reflects the matrix along its diagonal, while rotates it by 90 degrees instead. The question is just whether this ""rotation"" operation has any sensible meaning when is interpreted as a linear transformation. I am guessing the answer is no, but it seemed worth asking anyway, just in case it's something that's been considered for some reason. A related operation is what we could call the ""anti-transpose"", which reflects the matrix along the 'other' diagonal and also results in an matrix: An application of this anti-transpose operation is mentioned in this answer on mathoverflow. (Thanks to Torsten Schoeneberg for pointing that out.)","m\times n 
A = \begin{pmatrix}
a_{11} & \dots & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{m1} & \dots & a_{mn}
\end{pmatrix}
 n\times m 
A^\circlearrowright = \begin{pmatrix}
a_{m1} & \dots & a_{11} \\
\vdots & \ddots & \vdots \\
a_{mn} & \dots & a_{1n}
\end{pmatrix}.
 A^T A A^\circlearrowright A n\times m 
A^\bot \begin{pmatrix}
a_{mn} & \dots & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{m1} & \dots & a_{11}
\end{pmatrix}
","['linear-algebra', 'matrices']"
79,How many matrices do you need to generate a dense subset of $U(n)$?,How many matrices do you need to generate a dense subset of ?,U(n),"In quantum computing, people talk about universal gates , which are a finite collection of matrices $\{U_1,\dots,U_k\}$ that generate a dense subset of $U(n)$ . The wiki gives various examples of collections of $\sim \log(n)$ operators that generate $U(n)$ . In the physics literature however, there is a focus on considering unitary matrices that only act on a finite (usually one, two, or three) qubits, so I wouldn't be surprised if you could actually generate $U(n)$ with less (maybe even $O(1)$ ?) number of operators. Is there an example of this, or a proof that this is impossible?","In quantum computing, people talk about universal gates , which are a finite collection of matrices that generate a dense subset of . The wiki gives various examples of collections of operators that generate . In the physics literature however, there is a focus on considering unitary matrices that only act on a finite (usually one, two, or three) qubits, so I wouldn't be surprised if you could actually generate with less (maybe even ?) number of operators. Is there an example of this, or a proof that this is impossible?","\{U_1,\dots,U_k\} U(n) \sim \log(n) U(n) U(n) O(1)","['linear-algebra', 'matrices', 'lie-groups', 'quantum-computation']"
80,How to determine a possible solution when an equation has multiple unknowns,How to determine a possible solution when an equation has multiple unknowns,,"Suppose we have an equation $$15 = 5i + 3j + 2k$$ where $i, j, k$ are non-negative integers. It is easy to find some values that make this equation true: $$i=3, j=0, k=0$$ $$i=0, j=5, k=0$$ $$i=1, j=2, k=2$$ $$...$$ But if we change the equation to be something quite a bit more complex, it becomes much harder to determine if a solution exists. For example, consider the equation $$2038 = 17i + 8j + 7k$$ again where $i, j, k$ are non-negative integers. Is there a way to determine if a solution exists other than by trial and error?","Suppose we have an equation where are non-negative integers. It is easy to find some values that make this equation true: But if we change the equation to be something quite a bit more complex, it becomes much harder to determine if a solution exists. For example, consider the equation again where are non-negative integers. Is there a way to determine if a solution exists other than by trial and error?","15 = 5i + 3j + 2k i, j, k i=3, j=0, k=0 i=0, j=5, k=0 i=1, j=2, k=2 ... 2038 = 17i + 8j + 7k i, j, k","['linear-algebra', 'linear-diophantine-equations']"
81,Let $A = (a_{ij})$ be an an $n \times n$ matrix such that $\max\lvert a_{ij} \rvert < \cfrac{1}{n}$. Show that $I-A$ is invertible,Let  be an an  matrix such that . Show that  is invertible,A = (a_{ij}) n \times n \max\lvert a_{ij} \rvert < \cfrac{1}{n} I-A,"Just wondering if the following solution I had works: (Note here im using the norm $\lVert A\rVert_{1} = \max_{1\leq j \leq n} \sum_{i=1}^n \rvert a_{ij} \lvert$ ) Suppose BWOC that $I-A$ is not-invertible. Then, $I-A$ has a non-trivial kernel $\implies$ $1$ is an eigenvalue of $A \implies \exists \mathbf{v} \neq 0$ such that $A\mathbf{v} = \mathbf{v}$ . Now since $\lVert A\mathbf{v} \rVert_1 \leq \lVert A \rVert_1 \lVert \mathbf{v} \rVert_1$ (proved in previous part of the question) we have $\lVert \mathbf{v} \rVert_1 \leq \lVert A \rVert_1 \lVert \mathbf{v} \rVert_1 \implies \lVert A \rVert_1 \geq 1$ . But, (using definition of norm above and that $\max\lvert a_{ij} \rvert < \cfrac{1}{n}$ ) $\lVert A \rVert_1 < n\cfrac{1}{n} = 1$ . Contradiction.","Just wondering if the following solution I had works: (Note here im using the norm ) Suppose BWOC that is not-invertible. Then, has a non-trivial kernel is an eigenvalue of such that . Now since (proved in previous part of the question) we have . But, (using definition of norm above and that ) . Contradiction.",\lVert A\rVert_{1} = \max_{1\leq j \leq n} \sum_{i=1}^n \rvert a_{ij} \lvert I-A I-A \implies 1 A \implies \exists \mathbf{v} \neq 0 A\mathbf{v} = \mathbf{v} \lVert A\mathbf{v} \rVert_1 \leq \lVert A \rVert_1 \lVert \mathbf{v} \rVert_1 \lVert \mathbf{v} \rVert_1 \leq \lVert A \rVert_1 \lVert \mathbf{v} \rVert_1 \implies \lVert A \rVert_1 \geq 1 \max\lvert a_{ij} \rvert < \cfrac{1}{n} \lVert A \rVert_1 < n\cfrac{1}{n} = 1,"['linear-algebra', 'matrices', 'normed-spaces']"
82,A property of matrix $(I-A)^{-1}$ for $A$ strictly substochastic.,A property of matrix  for  strictly substochastic.,(I-A)^{-1} A,"Let $A$ be a strictly substochastic matrix (i.e., nonnegative elements and row sums strictly less than one) and let $M = (I-A)^{-1}.$ Since $I-A$ is an M-matrix, I know that matrix $M$ has all nonnegative entries. Simulations also show that $m_{ii} \geq m_{ji}$ for all $i,j$ . Is that true in general? This is true in the 2x2 case, since letting $\Delta$ be the determinant of $I-A$ (which is positive), then $m_{11} = (1-a_{22}) / \Delta$ while $m_{21} = a_{21} / \Delta$ , and so $a_{21} + a_{22} < 1$ implies that $m_{11} > m_{21}$ , and similarly $m_{22} > m_{12}$ . It is also easy to show it directly in the 3x3 case. Perhaps one can show by induction, but I imagine that this is a well known result?","Let be a strictly substochastic matrix (i.e., nonnegative elements and row sums strictly less than one) and let Since is an M-matrix, I know that matrix has all nonnegative entries. Simulations also show that for all . Is that true in general? This is true in the 2x2 case, since letting be the determinant of (which is positive), then while , and so implies that , and similarly . It is also easy to show it directly in the 3x3 case. Perhaps one can show by induction, but I imagine that this is a well known result?","A M = (I-A)^{-1}. I-A M m_{ii} \geq m_{ji} i,j \Delta I-A m_{11} = (1-a_{22}) / \Delta m_{21} = a_{21} / \Delta a_{21} + a_{22} < 1 m_{11} > m_{21} m_{22} > m_{12}","['linear-algebra', 'matrices']"
83,The eigenvectors of a general upper triangular matrix,The eigenvectors of a general upper triangular matrix,,"I am searching for the eigenvectors of an upper $n\times n$ triangular matrix $U$ with distinct and non-zero entries. What I know is that the diagonal entries are the eigenvalues of $U$ and to determine $v_{\lambda_{i}}$ I took the simplest case that is for $n=2$ , we can see that an upper triangular matrix has the first column of the canonical basis of $\mathbb{R}^{n}$ as an eigenvector to $\lambda_{1}$ of $U$ . Moreover, I believe $v_{\lambda_{2}}$ can be founded as a linear combination of $e_{2}$ and $e_{1}$ . Using the basic form of the eigenvalue problem, I am able to derive the second eigenvector of $U$ as follow : $$ U(e_{2}+\alpha e_{1})=u_{2,2}(e_{2}+\alpha e_{1}) $$ $$ u_{2,2}+u_{1,2}e_{1}+\gamma u_{1,1}e_{1}=u_{2,2}e_{2}+\gamma u_{2,2}e_{1} $$ $$ u_{1,2}=\gamma(u_{2,2}-u_{1,1}) $$ assuming distinct eigenvalues $(u_{1,1}\neq u_{2,2})$ we get : $$ v_{\lambda_{2}}=e_{2}+\frac{u_{1,2}}{u_{2,2}-u_{1,1}}e_{1} $$ For the $j-1$ eigenvector corresponding to $\lambda_{j-1}$ , what I can see is that it must have its first $j-1$ entries as non-zero so I expect $v_{\lambda_{j-1}}$ to be formed as a linear combination of the past $j-2$ eigenvectors such that : $$ v_{j-1}=e_{j-1}+\alpha_{1} v_{1}+\ldots+\alpha_{j-2}v_{j-2} $$ I am not able to see what pattern the coeffecients $\{\alpha_{1},\alpha_{2},\ldots,\alpha_{j-1}\}$ have except for the simple case of $n=2$ therefore I hope someone can assist me in generalizing $U$ for its full size to determine the formula for the $j^{th}$ eigenvector","I am searching for the eigenvectors of an upper triangular matrix with distinct and non-zero entries. What I know is that the diagonal entries are the eigenvalues of and to determine I took the simplest case that is for , we can see that an upper triangular matrix has the first column of the canonical basis of as an eigenvector to of . Moreover, I believe can be founded as a linear combination of and . Using the basic form of the eigenvalue problem, I am able to derive the second eigenvector of as follow : assuming distinct eigenvalues we get : For the eigenvector corresponding to , what I can see is that it must have its first entries as non-zero so I expect to be formed as a linear combination of the past eigenvectors such that : I am not able to see what pattern the coeffecients have except for the simple case of therefore I hope someone can assist me in generalizing for its full size to determine the formula for the eigenvector","n\times n U U v_{\lambda_{i}} n=2 \mathbb{R}^{n} \lambda_{1} U v_{\lambda_{2}} e_{2} e_{1} U 
U(e_{2}+\alpha e_{1})=u_{2,2}(e_{2}+\alpha e_{1})
 
u_{2,2}+u_{1,2}e_{1}+\gamma u_{1,1}e_{1}=u_{2,2}e_{2}+\gamma u_{2,2}e_{1}
 
u_{1,2}=\gamma(u_{2,2}-u_{1,1})
 (u_{1,1}\neq u_{2,2}) 
v_{\lambda_{2}}=e_{2}+\frac{u_{1,2}}{u_{2,2}-u_{1,1}}e_{1}
 j-1 \lambda_{j-1} j-1 v_{\lambda_{j-1}} j-2 
v_{j-1}=e_{j-1}+\alpha_{1} v_{1}+\ldots+\alpha_{j-2}v_{j-2}
 \{\alpha_{1},\alpha_{2},\ldots,\alpha_{j-1}\} n=2 U j^{th}","['linear-algebra', 'eigenvalues-eigenvectors']"
84,Faster method for solving the inequality $\sqrt{λ_1^2+ λ_2^2+ λ_3^2}$ $\leq$ $\sqrt{1949}$ for a 3x3 matrix,Faster method for solving the inequality    for a 3x3 matrix,\sqrt{λ_1^2+ λ_2^2+ λ_3^2} \leq \sqrt{1949},"λ 1 , λ 2 , λ 3 are the eigen values of the matrix \begin{bmatrix}26&-2&2\\2&21&4\\4&2&28\end{bmatrix} Show that $\sqrt{λ_1^2+ λ_2^2+ λ_3^2}$ $\leq$ $\sqrt{1949}$ Here's my approach at solving this problem - Performing Single Value Decomposition of $A$ , we get $$A^TA = \begin{bmatrix}696&-2&172\\-2&449&136\\172&136&804\end{bmatrix}$$ $$\Sigma^T\Sigma = \begin{bmatrix}389.78&0&0\\0&604.83&0\\0&0&954.38\end{bmatrix}$$ Therefore, $$\Sigma = \begin{bmatrix}\sqrt{389.78}&0&0\\0&\sqrt{604.83}&0\\0&0&\sqrt{954.38}\end{bmatrix}$$ Now, $$\sum_{i=1}^n|\lambda_i| \leq \sum_{i=1}^n|\sigma_{ii}|$$ Squaring on both sides, $$\sum_{i=1}^n|\lambda_i|^2 \leq \sum_{i=1}^n|\sigma_{ii}|^2$$ We get $$\lambda_1^2+\lambda_3^2+\lambda_3^2 \leq 1948.99\approx1949$$ Taking square root, we get $$\sqrt{\lambda_1^2+\lambda_3^2+\lambda_3^2} \leq \sqrt{1949}$$ Is the step where I square the summation inequality correct? I have never come across taking squares of summation so I have my doubts whether it's correct or not. Also, is there any faster way of solving this problem. Computing $A^TA$ and then finding it's eigenvalues takes too much time. EDIT : $$(\sum_{i=1}^n|\lambda_i|)^2 \leq (\sum_{i=1}^n|\sigma_{ii}|)^2$$ $$\sum_{i=1}^n|\lambda_i|^2 + 2\sum_{j=1}^{n-1}\sum_{i=j+1}^{n}|\lambda_i||\lambda_j| \leq tr(A^TA)$$ Since $2\sum_{j=1}^{n-1}\sum_{i=j+1}^{n}|\lambda_i||\lambda_j| > 0$ , $$|\lambda_1|^2+|\lambda_3|^2+|\lambda_3|^2 \leq 1949$$ $$\sqrt{|\lambda_1|^2+|\lambda_3|^2+|\lambda_3|^2} \leq \sqrt{1949}$$","λ 1 , λ 2 , λ 3 are the eigen values of the matrix Show that Here's my approach at solving this problem - Performing Single Value Decomposition of , we get Therefore, Now, Squaring on both sides, We get Taking square root, we get Is the step where I square the summation inequality correct? I have never come across taking squares of summation so I have my doubts whether it's correct or not. Also, is there any faster way of solving this problem. Computing and then finding it's eigenvalues takes too much time. EDIT : Since ,",\begin{bmatrix}26&-2&2\\2&21&4\\4&2&28\end{bmatrix} \sqrt{λ_1^2+ λ_2^2+ λ_3^2} \leq \sqrt{1949} A A^TA = \begin{bmatrix}696&-2&172\\-2&449&136\\172&136&804\end{bmatrix} \Sigma^T\Sigma = \begin{bmatrix}389.78&0&0\\0&604.83&0\\0&0&954.38\end{bmatrix} \Sigma = \begin{bmatrix}\sqrt{389.78}&0&0\\0&\sqrt{604.83}&0\\0&0&\sqrt{954.38}\end{bmatrix} \sum_{i=1}^n|\lambda_i| \leq \sum_{i=1}^n|\sigma_{ii}| \sum_{i=1}^n|\lambda_i|^2 \leq \sum_{i=1}^n|\sigma_{ii}|^2 \lambda_1^2+\lambda_3^2+\lambda_3^2 \leq 1948.99\approx1949 \sqrt{\lambda_1^2+\lambda_3^2+\lambda_3^2} \leq \sqrt{1949} A^TA (\sum_{i=1}^n|\lambda_i|)^2 \leq (\sum_{i=1}^n|\sigma_{ii}|)^2 \sum_{i=1}^n|\lambda_i|^2 + 2\sum_{j=1}^{n-1}\sum_{i=j+1}^{n}|\lambda_i||\lambda_j| \leq tr(A^TA) 2\sum_{j=1}^{n-1}\sum_{i=j+1}^{n}|\lambda_i||\lambda_j| > 0 |\lambda_1|^2+|\lambda_3|^2+|\lambda_3|^2 \leq 1949 \sqrt{|\lambda_1|^2+|\lambda_3|^2+|\lambda_3|^2} \leq \sqrt{1949},['linear-algebra']
85,Is there a theory that describes eigenspaces of matrix functions near degeneracies?,Is there a theory that describes eigenspaces of matrix functions near degeneracies?,,"Let $X$ be a smooth manifold and let $h$ be a smooth map from $X$ to hermitian $N \times N$ matrices. Under favorable circumstances (one sufficient condition: all eigenvalues of $h(x)$ are simple for every $x \in X$ ) this datum determines a number of vector bundles over $X$ whose fibers are eigenspaces of $h(x)$ . These bundles are all subbundles of a fixed trivial bundle with fiber $\mathbb C^N$ , whose direct sum is the whole trivial bundle. More generally one could consider the situation in which the above is true generically but fails on some locus $Y \subset X$ (e.g. the subset of $X$ on which $h(x)$ does not have simple spectrum). One nice example is $X= \mathbb R^3$ , $h(x) = \begin{bmatrix} x_3 & x_1 - \mathrm{i} x_2 \\ x_1 + \mathrm{i} x_2 & -x_3 \end{bmatrix}$ for which the ""bad locus"" is $\{ 0 \}$ . I would like to know if there is some general theory which describes the behaviour of eigenspaces at ""bad points"". I suspect that this problem should have interesting local and global aspects; I am interested in both.","Let be a smooth manifold and let be a smooth map from to hermitian matrices. Under favorable circumstances (one sufficient condition: all eigenvalues of are simple for every ) this datum determines a number of vector bundles over whose fibers are eigenspaces of . These bundles are all subbundles of a fixed trivial bundle with fiber , whose direct sum is the whole trivial bundle. More generally one could consider the situation in which the above is true generically but fails on some locus (e.g. the subset of on which does not have simple spectrum). One nice example is , for which the ""bad locus"" is . I would like to know if there is some general theory which describes the behaviour of eigenspaces at ""bad points"". I suspect that this problem should have interesting local and global aspects; I am interested in both.",X h X N \times N h(x) x \in X X h(x) \mathbb C^N Y \subset X X h(x) X= \mathbb R^3 h(x) = \begin{bmatrix} x_3 & x_1 - \mathrm{i} x_2 \\ x_1 + \mathrm{i} x_2 & -x_3 \end{bmatrix} \{ 0 \},"['linear-algebra', 'algebraic-geometry', 'algebraic-topology', 'eigenvalues-eigenvectors', 'vector-bundles']"
86,What is the use of Frobenuis inner product?,What is the use of Frobenuis inner product?,,"I was studying Linear Algebra from the famous textbook Linear Algebra by Stephen H. Friedberg, Arnold J. Insel, and Lawrence E. Spence and reached the inner product of a vector space, I understand the intuition behind the standard dot product in $\mathbb{R}^{n}$ , but he also mentioned Frobenuis inner product over the space of the square matrices. Is there any intuition I can understand, given that I only have the basic knowledge of algebra?","I was studying Linear Algebra from the famous textbook Linear Algebra by Stephen H. Friedberg, Arnold J. Insel, and Lawrence E. Spence and reached the inner product of a vector space, I understand the intuition behind the standard dot product in , but he also mentioned Frobenuis inner product over the space of the square matrices. Is there any intuition I can understand, given that I only have the basic knowledge of algebra?",\mathbb{R}^{n},"['linear-algebra', 'inner-products']"
87,Determinant Identity: Elegant Solution Please.,Determinant Identity: Elegant Solution Please.,,"Can anyone provide a proof of this identity: $$\begin{vmatrix} (b+c)^2 &a^2&a^2\\ b^2&(a+c)^2&b^2\\ c^2&c^2&(a+b)^2\\ \end{vmatrix}=2abc(a+b+c)^3 ?$$ Ideally I would like a series of row column operations resulting in the formula. No brute force calculations. Note that letting $a=0$ we get two rows equal. So $abc$ is a factor. But I have unable to produce this by a simple series of operations, I mean to get one row or column all of whose entries are divisible by $a$ . Getting divisibility by $(a+b+c)^2$ is also not hard just subtract one column from the other two. After that however I have been unable to make any further progress. Hopefully someone here is smarter than I.","Can anyone provide a proof of this identity: Ideally I would like a series of row column operations resulting in the formula. No brute force calculations. Note that letting we get two rows equal. So is a factor. But I have unable to produce this by a simple series of operations, I mean to get one row or column all of whose entries are divisible by . Getting divisibility by is also not hard just subtract one column from the other two. After that however I have been unable to make any further progress. Hopefully someone here is smarter than I.","\begin{vmatrix}
(b+c)^2 &a^2&a^2\\
b^2&(a+c)^2&b^2\\
c^2&c^2&(a+b)^2\\
\end{vmatrix}=2abc(a+b+c)^3 ? a=0 abc a (a+b+c)^2","['linear-algebra', 'determinant']"
88,Algebraic and definable closure of a vector space is a span of its subset,Algebraic and definable closure of a vector space is a span of its subset,,"I am trying to show that for every infinite $K$ -vector space $V$ (we use the first order language of vector spaces over $K$ ) and each $A \subseteq V$ , $acl(A)$ is the $K$ -linear subspace of $V$ spanned by $A$ . The general notion I got is that it follows from strong minimality, yet I cannot see a connection between the two. How should I start thinking about it? Is it also true for a definable closure? EDIT [as suggested by @Berci in the comments]: Definitions The first-order language of vector spaces is: a constant $0$ , a binary function "" $+$ "", and a unary function symbol $F_a$ for each $a\in K$ . Given a $K$ -vector space $V$ , $0$ is interpreted by the identity element of $V$ , "" $+$ "" is interpreted by the addition of $V$ , and each $F_a$ is interpreted by the operation of scalar multiplication by $a$ . $acl(A)$ is the set of elements of $M$ that are algebraic over $A$ in an $L$ -structure $\mathcal{M}$ . An element $a$ of $M$ is algebraic over $A$ in $\mathcal{M}$ if there is an $L$ -formula $\phi(x,y_1,\ldots,y_n)$ and elements $e_1,\ldots , e_n$ of $A$ such that: (i) $\mathcal{M}\models \phi [a,e_1,\ldots ,e_n]$ AND (ii) $\{c\in M|\mathcal{M}\models[c, e_1, \ldots ,e_n]\}$ is finite.","I am trying to show that for every infinite -vector space (we use the first order language of vector spaces over ) and each , is the -linear subspace of spanned by . The general notion I got is that it follows from strong minimality, yet I cannot see a connection between the two. How should I start thinking about it? Is it also true for a definable closure? EDIT [as suggested by @Berci in the comments]: Definitions The first-order language of vector spaces is: a constant , a binary function "" "", and a unary function symbol for each . Given a -vector space , is interpreted by the identity element of , "" "" is interpreted by the addition of , and each is interpreted by the operation of scalar multiplication by . is the set of elements of that are algebraic over in an -structure . An element of is algebraic over in if there is an -formula and elements of such that: (i) AND (ii) is finite.","K V K A \subseteq V acl(A) K V A 0 + F_a a\in K K V 0 V + V F_a a acl(A) M A L \mathcal{M} a M A \mathcal{M} L \phi(x,y_1,\ldots,y_n) e_1,\ldots , e_n A \mathcal{M}\models \phi [a,e_1,\ldots ,e_n] \{c\in M|\mathcal{M}\models[c, e_1, \ldots ,e_n]\}","['linear-algebra', 'logic', 'vector-spaces', 'first-order-logic', 'model-theory']"
89,How to find the eigenvalues of this matrix in a simple way?,How to find the eigenvalues of this matrix in a simple way?,,"Can anyone help how to find the eigenvalues of the following matrix in a simple way? I expand the characteristic polynomial being, $$     \lambda(\lambda-3)(\lambda - 2k) = 0 $$ and get the answer but intuition is that there must be a simple way to find it. $$     \begin{bmatrix} 1 + k & 1 & 1 - k \\ 1 & 1 & 1 \\ 1 - k & 1 & 1 + k \\ \end{bmatrix} $$","Can anyone help how to find the eigenvalues of the following matrix in a simple way? I expand the characteristic polynomial being, and get the answer but intuition is that there must be a simple way to find it.","
    \lambda(\lambda-3)(\lambda - 2k) = 0
 
    \begin{bmatrix} 1 + k & 1 & 1 - k \\ 1 & 1 & 1 \\ 1 - k & 1 & 1 + k \\ \end{bmatrix}
","['linear-algebra', 'eigenvalues-eigenvectors']"
90,Matrix representation of trace,Matrix representation of trace,,"I got trace is $4$ for a $2\times2$ matrix. I know trace is a linear operation which transforms a matrix to a field. So the matrix representation of a trace should be like $1×n$ , where $n$ is the dimension of that square matrix. How I can represent number $4$ as a $1\times4$ matrix? Thanks in advance!","I got trace is for a matrix. I know trace is a linear operation which transforms a matrix to a field. So the matrix representation of a trace should be like , where is the dimension of that square matrix. How I can represent number as a matrix? Thanks in advance!",4 2\times2 1×n n 4 1\times4,"['linear-algebra', 'matrices', 'trace']"
91,"two reduced row echelon matrices have the same nullspace, prove they are identical","two reduced row echelon matrices have the same nullspace, prove they are identical",,"I am trying to prove if R and R' are two reduced row echelon matrices, and have the same nullspaces, they are identical. I have observed this when testing them, but I have trouble finding a formal proof. I tried reading this question and extracting a general solution, but this question is focused on a particular 2 by 3 matrix whereas I am trying to find a formal proof for an m by n matrix. I believe I should start by writing two general reduced row echelon matrices, writing Rx=R'x=0 and finding a general answer for each row, then proving they are equal, but I have trouble proving it generally and without presumptions. Thank you in advance.","I am trying to prove if R and R' are two reduced row echelon matrices, and have the same nullspaces, they are identical. I have observed this when testing them, but I have trouble finding a formal proof. I tried reading this question and extracting a general solution, but this question is focused on a particular 2 by 3 matrix whereas I am trying to find a formal proof for an m by n matrix. I believe I should start by writing two general reduced row echelon matrices, writing Rx=R'x=0 and finding a general answer for each row, then proving they are equal, but I have trouble proving it generally and without presumptions. Thank you in advance.",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'matrix-rank']"
92,Is every plane in $\mathbb{R}^4$ a line in $\mathbb{C}^2$?,Is every plane in  a line in ?,\mathbb{R}^4 \mathbb{C}^2,"Every complex line, that is, one-dimensional complex affine space, in $\mathbb{C}^2$ is a real plane in $\mathbb{R}^4$ . Is the converse true? That is, is every real plane in $\mathbb{R}^4$ a complex line in $\mathbb{C}^2$ ?","Every complex line, that is, one-dimensional complex affine space, in is a real plane in . Is the converse true? That is, is every real plane in a complex line in ?",\mathbb{C}^2 \mathbb{R}^4 \mathbb{R}^4 \mathbb{C}^2,"['linear-algebra', 'vector-spaces', 'almost-complex']"
93,Prove that $Dim(W) \leq k$,Prove that,Dim(W) \leq k,"Let be $V$ a vector space over a field $F$ with finit dimension. Let be $T$ a lineal operator in $V$ . Supose that the characteristic polynomial of $T$ , $p(x)$ , is of the form $p(x)=(x-c)^{k}g(x)$ with $k > \in \mathbb{N}^{+}$ , $c \in F$ and $g(c) \neq 0$ , and consider $W$ the space of the eigenvectors associated with $c$ . Prove that: $Dim(W) \leq k$ If $Dim(W)<k$ , then $T$ is not diagonizable I'm not sure of how to solve the problem. How can I prove it? I would really appreciate your help!","Let be a vector space over a field with finit dimension. Let be a lineal operator in . Supose that the characteristic polynomial of , , is of the form with , and , and consider the space of the eigenvectors associated with . Prove that: If , then is not diagonizable I'm not sure of how to solve the problem. How can I prove it? I would really appreciate your help!","V F T V T p(x) p(x)=(x-c)^{k}g(x) k
> \in \mathbb{N}^{+} c \in F g(c) \neq 0 W c Dim(W) \leq k Dim(W)<k T",['linear-algebra']
94,Argument of Feynman for equivalence of dot product definitions,Argument of Feynman for equivalence of dot product definitions,,"The following argument from the Feynman Lectures on Physics ( Vol I, Lecture 11 ), which relates to the equivalence of the algebraic and geometric definitions, does not particularly convince me. Also, there is a simple geometrical way to calculate $\vec{a} \cdot \vec{b}$ , without having to calculate the components of $\vec{a}$ and $\vec{b}$ : $\vec{a} \cdot \vec{b}$ is the product of the length of $\vec{a}$ and the length of $\vec{b}$ times the cosine of the angle between them. Why? Suppose that we choose a special coordinate system in which the x-axis lies along $\vec{a}$ ; in those circumstances, the only component of $\vec{a}$ that will be there is $a_x$ , which is of course the whole length of $\vec{a}$ . Thus Eq. (11.19) reduces to $a \cdot b = a_x b_x$ for this case, and this is the length of $\vec{a}$ times the component of $\vec{b}$ in the direction of $\vec{a}$ , that is, $b \cos \theta$ : $a \cdot b = a b \cos \theta$ . Therefore, in that special coordinate system, we have proved that $\vec{a} \cdot \vec{b}$ is the length of $\vec{a}$ times the length of $\vec{b}$ times $\cos \theta$ . But if it is true in one coordinate system, it is true in all, because $\vec{a} \cdot \vec{b}$ is independent of the coordinate system ; that is our argument. In fact, most of this argument seems just fine, but it seems like Feynman is casually asserting a priori that the dot product should be independent of the coordinate system. This is something I do not like, since I can't see an obvious justification for it. (Indeed, if by ""coordinate system"" he means basis, then there are clearly bases for which this is not true, e.g., ${2\hat{i}, 2\hat{j}, 2\hat{k}}$ .) Could someone who is better at reading between the lines of Feynman please clarify this for me?","The following argument from the Feynman Lectures on Physics ( Vol I, Lecture 11 ), which relates to the equivalence of the algebraic and geometric definitions, does not particularly convince me. Also, there is a simple geometrical way to calculate , without having to calculate the components of and : is the product of the length of and the length of times the cosine of the angle between them. Why? Suppose that we choose a special coordinate system in which the x-axis lies along ; in those circumstances, the only component of that will be there is , which is of course the whole length of . Thus Eq. (11.19) reduces to for this case, and this is the length of times the component of in the direction of , that is, : . Therefore, in that special coordinate system, we have proved that is the length of times the length of times . But if it is true in one coordinate system, it is true in all, because is independent of the coordinate system ; that is our argument. In fact, most of this argument seems just fine, but it seems like Feynman is casually asserting a priori that the dot product should be independent of the coordinate system. This is something I do not like, since I can't see an obvious justification for it. (Indeed, if by ""coordinate system"" he means basis, then there are clearly bases for which this is not true, e.g., .) Could someone who is better at reading between the lines of Feynman please clarify this for me?","\vec{a} \cdot \vec{b} \vec{a} \vec{b} \vec{a} \cdot \vec{b} \vec{a} \vec{b} \vec{a} \vec{a} a_x \vec{a} a \cdot b = a_x b_x \vec{a} \vec{b} \vec{a} b \cos \theta a \cdot b = a b \cos \theta \vec{a} \cdot \vec{b} \vec{a} \vec{b} \cos \theta \vec{a} \cdot \vec{b} {2\hat{i}, 2\hat{j}, 2\hat{k}}","['linear-algebra', 'vector-spaces']"
95,Is it true that there is an interesting entry in each row of a matrix with nonzero determinant? [duplicate],Is it true that there is an interesting entry in each row of a matrix with nonzero determinant? [duplicate],,"This question already has answers here : Can the determinant of a matrix can be made $0$? (4 answers) Closed 3 years ago . We call an entry of an $ n × n $ matrix with nonzero determinant interesting, if by changing this entry (and only this) the determinant of the matrix can be made $0$ . Is it true that there is an interesting entry in each row of a matrix with nonzero determinant? I know that each entry of every matrix with nonzero determinant is not interesting. However I could not find proof if this is a case in each row.","This question already has answers here : Can the determinant of a matrix can be made $0$? (4 answers) Closed 3 years ago . We call an entry of an matrix with nonzero determinant interesting, if by changing this entry (and only this) the determinant of the matrix can be made . Is it true that there is an interesting entry in each row of a matrix with nonzero determinant? I know that each entry of every matrix with nonzero determinant is not interesting. However I could not find proof if this is a case in each row.", n × n  0,['linear-algebra']
96,Solve many linear equations of similar structure,Solve many linear equations of similar structure,,"Given G : real and symmetric square matrix v : real column vector I need to solve n linear systems of the form \begin{align} A = \begin{pmatrix} G & v \\\ v^T & 0 \end{pmatrix}\end{align} \begin{align} Ax = b\end{align} Where n is large G : real and symmetric square matrix, constant for all n systems v : real column vector, changes for each system (Combination vector where at most 2 values are nonzero) b : is zero column vector with exception of the last element I want to know if there is a fast method to solve these many systems via exploiting this structure and suspect that there is a way to do this via eigenvalue decomposition of sums of hermitian matrices. However, I am unsure of how to combine the results. I currently solve n systems via a hermitian solver which doesn't scale well. For convenience, I provide the following equivalent python code import numpy as np import scipy.linalg as sp_linalg  np.set_printoptions(threshold=np.inf, linewidth=100000, precision=3, suppress=True)  N = 10 # Size of A-1  G = np.random.random(size=(N, N)) G += G.T G *= 2  v = np.zeros((N, 1)) v[np.random.choice(N, 2)] = 1.0  A = np.block([[G, v], [v.T, 0.0]]) A_G = np.block([[G, np.zeros((N, 1))], [np.zeros((1, N+1))]]) A_v = np.block([[np.zeros((N, N)), v], [v.T, 0.0]])  b = np.concatenate((np.zeros((N, 1)), np.random.random((1,1))))  ###  x = sp_linalg.solve(A, b, assume_a='sym') # General solution to compare against  ###  # for eigenvalue decomposition # lambda_G, Q_G = np.linalg.eigh(A_G) # lambda_v, Q_v = np.linalg.eigh(A_v) Thanks! Solution: I've taken the solution mentioned by eepperly16 and further generalized the problem. Now G : NxN random symetric matrix constant for all n systems v : NxM matrix of random variables The big idea is since v is now a matrix, an inverse of $-v^\top G^{-1} v$ rather than doing a simple divide. These changes include... $x_2 = -y_2 / (v^\top G^{-1}v)$ Becomes $x_2 = (v^\top G^{-1}v)^{-1} -y_2$ $x_1 = y_1 - x_2G^{-1}v$ Becomes $x_1 = y_1 - G^{-1}vx_2$ Since the result of this is always symmetric, that can be exploited with similar factorization. Note, however, that now the time complexity of the second stage expands proportionately to $O(M^2)$ . And finally the code with benchmark import numpy as np import scipy.linalg as sp_linalg import timeit  np.random.seed(40) np.set_printoptions(threshold=8, linewidth=1000, precision=3, suppress=True)  N = 100 # Size of square matrix G M = 10 # Number of columns in v  # Setup problem and randomize def setup_and_randomize():      # Create random symmetric matrix G on range (-1.0, 1.0)     G = 2.0 * np.random.random(size=(N, N)) - 1.0     G += G.T     G *= 0.5      # Create random rectangular matrix v on range (-1.0, 1.0)     v = 2.0 * np.random.random(size=(N, M)) - 1.0      A = np.block([[G, v], [v.T, np.zeros((M, M))]])      b_1 = np.zeros((N, 1))     b_2 = np.ones((M, 1))     b = np.concatenate((b_1, b_2), axis=0)      return A, G, v, b, b_1, b_2   # General solution to compare against def naive_method(A, b):     return sp_linalg.solve(A, b, assume_a='sym')   # Generalized solution created from eepperly16's solution Part 1 def answer_method_precompute(G, b_1, b_2):     P, L, U = sp_linalg.lu(G, overwrite_a=True, check_finite=False)     L_inv = sp_linalg.solve_triangular(L, np.eye(N), lower=True, trans='N', overwrite_b=True)     U_inv = sp_linalg.solve_triangular(U, np.eye(N), lower=False, trans='N', overwrite_b=True)     G_inv = U_inv @ L_inv @ P.T      y_1 = G_inv @ b_1     y_2 = b_2 - v.T @ y_1     return y_1, y_2, G_inv  # Generalized solution crated from eepperly16's solution Part 2 def answer_method_main(v, y_1, y_2, G_inv):     G_inv_dot_v = G_inv @ v      # IF M >= 1 -----------------------------------------------------     B = v.T @ G_inv_dot_v     P, L, U = sp_linalg.lu(B, overwrite_a=True, check_finite=False)     L_inv = sp_linalg.solve_triangular(L, np.eye(M), lower=True, trans='N', overwrite_b=True)     U_inv = sp_linalg.solve_triangular(U, np.eye(M), lower=False, trans='N', overwrite_b=True)     B_inv = U_inv @ L_inv @ P.T      x_2 = B_inv @ -y_2     x_1 = y_1 - G_inv_dot_v @ x_2      # IF M == 1 -----------------------------------------------------     # x_2 = -y_2 / (v.T @ G_inv_dot_v)     # x_1 = y_1 - (x_2 * G_inv_dot_v)      return np.concatenate((x_1, x_2), axis=0)  if __name__ == ""__main__"":      # Verify Same Solution ------------------------------------------     A, G, v, b, b_1, b_2 = setup_and_randomize()      x_naive = naive_method(A, b)      y_1, y_2, G_inv = answer_method_precompute(G, b_1, b_2)     x_answer = answer_method_main(v, y_1, y_2, G_inv)      print('Naive Solution:\t', x_naive.T)     print('Final Solution:\t', x_answer.T)      # Benchmark Performance ----------------------------------------------     n_tests = 1000      A, G, v, b, b_1, b_2 = setup_and_randomize()     print('\nTimeit on naive_method', timeit.timeit('naive_method(A, b)', globals=globals(), number=n_tests))     print('Timeit on answer_precompute', timeit.timeit('answer_method_precompute(G, b_1, b_2)', globals=globals(), number=n_tests))     print('Timeit on answer_main', timeit.timeit('answer_method_main(v, y_1, y_2, G_inv)', globals=globals(), number=n_tests)) Which yields the following on my machine for 1000 iterations of N=100, M=10 Naive Solution:  [[ 0.33  -1.518  0.434 ... -0.394 -0.569  0.824]] Final Solution:  [[ 0.33  -1.518  0.434 ... -0.394 -0.569  0.824]]  Timeit on naive_method 0.39002 Timeit on answer_precompute 0.46521499999999993 Timeit on answer_main 0.14545809999999992 Final Edit: I understand that with scipy, there are better ways to compute the inverse that better tie into one of many BLAS style libraries. Below are 2 ways to compute the inverse of G that work better than the initial solution. Also, enabling more flags on the naive solver also makes that timing calculation fairer. G_inv = sp_linalg.lu_solve(             sp_linalg.lu_factor(G, overwrite_a=True, check_finite=False),             np.eye(N), overwrite_b=True, check_finite=False)  L, D, perm = sp_linalg.ldl(G, overwrite_a=True, hermitian=True, check_finite=False)     L_inv = sp_linalg.solve_triangular(L[perm, :], np.eye(N), lower=True, trans='N', overwrite_b=True, check_finite=False)[:, perm]     G_inv = (L_inv.T / D.diagonal()) @ L_inv","Given G : real and symmetric square matrix v : real column vector I need to solve n linear systems of the form Where n is large G : real and symmetric square matrix, constant for all n systems v : real column vector, changes for each system (Combination vector where at most 2 values are nonzero) b : is zero column vector with exception of the last element I want to know if there is a fast method to solve these many systems via exploiting this structure and suspect that there is a way to do this via eigenvalue decomposition of sums of hermitian matrices. However, I am unsure of how to combine the results. I currently solve n systems via a hermitian solver which doesn't scale well. For convenience, I provide the following equivalent python code import numpy as np import scipy.linalg as sp_linalg  np.set_printoptions(threshold=np.inf, linewidth=100000, precision=3, suppress=True)  N = 10 # Size of A-1  G = np.random.random(size=(N, N)) G += G.T G *= 2  v = np.zeros((N, 1)) v[np.random.choice(N, 2)] = 1.0  A = np.block([[G, v], [v.T, 0.0]]) A_G = np.block([[G, np.zeros((N, 1))], [np.zeros((1, N+1))]]) A_v = np.block([[np.zeros((N, N)), v], [v.T, 0.0]])  b = np.concatenate((np.zeros((N, 1)), np.random.random((1,1))))  ###  x = sp_linalg.solve(A, b, assume_a='sym') # General solution to compare against  ###  # for eigenvalue decomposition # lambda_G, Q_G = np.linalg.eigh(A_G) # lambda_v, Q_v = np.linalg.eigh(A_v) Thanks! Solution: I've taken the solution mentioned by eepperly16 and further generalized the problem. Now G : NxN random symetric matrix constant for all n systems v : NxM matrix of random variables The big idea is since v is now a matrix, an inverse of rather than doing a simple divide. These changes include... Becomes Becomes Since the result of this is always symmetric, that can be exploited with similar factorization. Note, however, that now the time complexity of the second stage expands proportionately to . And finally the code with benchmark import numpy as np import scipy.linalg as sp_linalg import timeit  np.random.seed(40) np.set_printoptions(threshold=8, linewidth=1000, precision=3, suppress=True)  N = 100 # Size of square matrix G M = 10 # Number of columns in v  # Setup problem and randomize def setup_and_randomize():      # Create random symmetric matrix G on range (-1.0, 1.0)     G = 2.0 * np.random.random(size=(N, N)) - 1.0     G += G.T     G *= 0.5      # Create random rectangular matrix v on range (-1.0, 1.0)     v = 2.0 * np.random.random(size=(N, M)) - 1.0      A = np.block([[G, v], [v.T, np.zeros((M, M))]])      b_1 = np.zeros((N, 1))     b_2 = np.ones((M, 1))     b = np.concatenate((b_1, b_2), axis=0)      return A, G, v, b, b_1, b_2   # General solution to compare against def naive_method(A, b):     return sp_linalg.solve(A, b, assume_a='sym')   # Generalized solution created from eepperly16's solution Part 1 def answer_method_precompute(G, b_1, b_2):     P, L, U = sp_linalg.lu(G, overwrite_a=True, check_finite=False)     L_inv = sp_linalg.solve_triangular(L, np.eye(N), lower=True, trans='N', overwrite_b=True)     U_inv = sp_linalg.solve_triangular(U, np.eye(N), lower=False, trans='N', overwrite_b=True)     G_inv = U_inv @ L_inv @ P.T      y_1 = G_inv @ b_1     y_2 = b_2 - v.T @ y_1     return y_1, y_2, G_inv  # Generalized solution crated from eepperly16's solution Part 2 def answer_method_main(v, y_1, y_2, G_inv):     G_inv_dot_v = G_inv @ v      # IF M >= 1 -----------------------------------------------------     B = v.T @ G_inv_dot_v     P, L, U = sp_linalg.lu(B, overwrite_a=True, check_finite=False)     L_inv = sp_linalg.solve_triangular(L, np.eye(M), lower=True, trans='N', overwrite_b=True)     U_inv = sp_linalg.solve_triangular(U, np.eye(M), lower=False, trans='N', overwrite_b=True)     B_inv = U_inv @ L_inv @ P.T      x_2 = B_inv @ -y_2     x_1 = y_1 - G_inv_dot_v @ x_2      # IF M == 1 -----------------------------------------------------     # x_2 = -y_2 / (v.T @ G_inv_dot_v)     # x_1 = y_1 - (x_2 * G_inv_dot_v)      return np.concatenate((x_1, x_2), axis=0)  if __name__ == ""__main__"":      # Verify Same Solution ------------------------------------------     A, G, v, b, b_1, b_2 = setup_and_randomize()      x_naive = naive_method(A, b)      y_1, y_2, G_inv = answer_method_precompute(G, b_1, b_2)     x_answer = answer_method_main(v, y_1, y_2, G_inv)      print('Naive Solution:\t', x_naive.T)     print('Final Solution:\t', x_answer.T)      # Benchmark Performance ----------------------------------------------     n_tests = 1000      A, G, v, b, b_1, b_2 = setup_and_randomize()     print('\nTimeit on naive_method', timeit.timeit('naive_method(A, b)', globals=globals(), number=n_tests))     print('Timeit on answer_precompute', timeit.timeit('answer_method_precompute(G, b_1, b_2)', globals=globals(), number=n_tests))     print('Timeit on answer_main', timeit.timeit('answer_method_main(v, y_1, y_2, G_inv)', globals=globals(), number=n_tests)) Which yields the following on my machine for 1000 iterations of N=100, M=10 Naive Solution:  [[ 0.33  -1.518  0.434 ... -0.394 -0.569  0.824]] Final Solution:  [[ 0.33  -1.518  0.434 ... -0.394 -0.569  0.824]]  Timeit on naive_method 0.39002 Timeit on answer_precompute 0.46521499999999993 Timeit on answer_main 0.14545809999999992 Final Edit: I understand that with scipy, there are better ways to compute the inverse that better tie into one of many BLAS style libraries. Below are 2 ways to compute the inverse of G that work better than the initial solution. Also, enabling more flags on the naive solver also makes that timing calculation fairer. G_inv = sp_linalg.lu_solve(             sp_linalg.lu_factor(G, overwrite_a=True, check_finite=False),             np.eye(N), overwrite_b=True, check_finite=False)  L, D, perm = sp_linalg.ldl(G, overwrite_a=True, hermitian=True, check_finite=False)     L_inv = sp_linalg.solve_triangular(L[perm, :], np.eye(N), lower=True, trans='N', overwrite_b=True, check_finite=False)[:, perm]     G_inv = (L_inv.T / D.diagonal()) @ L_inv",\begin{align} A = \begin{pmatrix} G & v \\\ v^T & 0 \end{pmatrix}\end{align} \begin{align} Ax = b\end{align} -v^\top G^{-1} v x_2 = -y_2 / (v^\top G^{-1}v) x_2 = (v^\top G^{-1}v)^{-1} -y_2 x_1 = y_1 - x_2G^{-1}v x_1 = y_1 - G^{-1}vx_2 O(M^2),"['linear-algebra', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'matrix-decomposition', 'lu-decomposition']"
97,"Let $T,U:V\to W$ be linear transformations. Prove that if $W$ is finite-dimensional, then $\text{rank}(T+U)\leq\text{rank}(T) + \text{rank}(U)$.","Let  be linear transformations. Prove that if  is finite-dimensional, then .","T,U:V\to W W \text{rank}(T+U)\leq\text{rank}(T) + \text{rank}(U)","Let $T,U:V\to W$ be linear transformations. (a) Prove that $R(T+U)\subseteq R(T) + R(U)$ . (b) Prove that if $W$ is finite-dimensional, then $\text{rank}(T+U)\leq\text{rank}(T) + \text{rank}(U)$ . MY ATTEMPT (a) If $w\in R(T+U)$ , there exists $v\in V$ such that $w = (T+U)v = T(v) + U(v)$ . But $T(v)\in R(T)$ and $U(v)\in R(U)$ . Thus $w\in R(T) + R(U)$ , from whence we conclude that $R(T+U)\subseteq R(T)+R(U)$ . (b) Since $R(T + U)$ is a linear subspace of $R(T)+R(U)$ , we have that \begin{align*} \dim R(T+U) & \leq \dim(R(T) + R(U)) = \dim R(T) + \dim R(U) - \dim(R(T)\cap R(U))\\\\ & \leq \dim R(T) + \dim R(U) \end{align*} Could someone verify if I am not doing any conceptual mistake?","Let be linear transformations. (a) Prove that . (b) Prove that if is finite-dimensional, then . MY ATTEMPT (a) If , there exists such that . But and . Thus , from whence we conclude that . (b) Since is a linear subspace of , we have that Could someone verify if I am not doing any conceptual mistake?","T,U:V\to W R(T+U)\subseteq R(T) + R(U) W \text{rank}(T+U)\leq\text{rank}(T) + \text{rank}(U) w\in R(T+U) v\in V w = (T+U)v = T(v) + U(v) T(v)\in R(T) U(v)\in R(U) w\in R(T) + R(U) R(T+U)\subseteq R(T)+R(U) R(T + U) R(T)+R(U) \begin{align*}
\dim R(T+U) & \leq \dim(R(T) + R(U)) = \dim R(T) + \dim R(U) - \dim(R(T)\cap R(U))\\\\
& \leq \dim R(T) + \dim R(U)
\end{align*}","['linear-algebra', 'solution-verification', 'matrix-rank']"
98,What is the matrix derivative of a symmetric bilinear form $\mathbf{a}^T X \mathbf{b}$ wrt $X$?,What is the matrix derivative of a symmetric bilinear form  wrt ?,\mathbf{a}^T X \mathbf{b} X,"What is the derivative of the symmetric bilinear form $$ f_X(\mathbf{a},\mathbf{b}) = \mathbf{a}^T X \mathbf{b} $$ with respect to the (symmetric) matrix $X$ ? Following Wikipedia , and using the denominator layout, I would say $$ \frac{\partial f_X}{\partial X}(\mathbf{a},\mathbf{b}) = \mathbf{a}\mathbf{b}^T $$ But since $f_X$ is symmetric, $f_X(\mathbf{a},\mathbf{b}) = f_X(\mathbf{b},\mathbf{a})$ .  If I derive this equality I obtain $$ \frac{\partial f_X}{\partial X}(\mathbf{a},\mathbf{b}) = \frac{\partial f_X}{\partial X}(\mathbf{b},\mathbf{a}) \qquad\Rightarrow\qquad \mathbf{a}\mathbf{b}^T = \mathbf{b}\mathbf{a}^T $$ which is wrong because $\mathbf{a}\mathbf{b}^T \neq \mathbf{b}\mathbf{a}^T$ Where am I wrong?","What is the derivative of the symmetric bilinear form with respect to the (symmetric) matrix ? Following Wikipedia , and using the denominator layout, I would say But since is symmetric, .  If I derive this equality I obtain which is wrong because Where am I wrong?","
f_X(\mathbf{a},\mathbf{b}) = \mathbf{a}^T X \mathbf{b}
 X 
\frac{\partial f_X}{\partial X}(\mathbf{a},\mathbf{b}) = \mathbf{a}\mathbf{b}^T
 f_X f_X(\mathbf{a},\mathbf{b}) = f_X(\mathbf{b},\mathbf{a}) 
\frac{\partial f_X}{\partial X}(\mathbf{a},\mathbf{b}) = \frac{\partial f_X}{\partial X}(\mathbf{b},\mathbf{a}) \qquad\Rightarrow\qquad \mathbf{a}\mathbf{b}^T = \mathbf{b}\mathbf{a}^T
 \mathbf{a}\mathbf{b}^T \neq \mathbf{b}\mathbf{a}^T","['linear-algebra', 'derivatives', 'matrix-calculus', 'symmetric-matrices', 'bilinear-form']"
99,exp(log + log) for positive semidefinite matrices,exp(log + log) for positive semidefinite matrices,,"Let $A$ and $B$ be positive definite matrices.  What is known about $f(A,B)=\exp(\log A + \log B)$ ?  Does this function have a name?  This is interesting because $f(A,B) = AB$ for commuting matrices and $f(A,B)=f(B,A)$ even for non-commuting matrices.",Let and be positive definite matrices.  What is known about ?  Does this function have a name?  This is interesting because for commuting matrices and even for non-commuting matrices.,"A B f(A,B)=\exp(\log A + \log B) f(A,B) = AB f(A,B)=f(B,A)","['linear-algebra', 'matrices', 'logarithms', 'terminology', 'matrix-equations']"
