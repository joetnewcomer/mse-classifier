,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Evaluating $\lim_{n \to \infty}\prod_{r=1}^{n}[1+(r/n)^2]^{1/r}$ by expressing it as a definite integral,Evaluating  by expressing it as a definite integral,\lim_{n \to \infty}\prod_{r=1}^{n}[1+(r/n)^2]^{1/r},"I have been asked to find the limit of the following series by expressing it as definite integral: If $na=1$ always and $n$ tends to infinity, find the limiting value $$\lim_{n \to \infty}\left(\prod_{r=1}^{n}[1+(ra)^2]^{1/r}\right)$$ The answer given is $\exp\left(\dfrac{\pi^2}{24}\right)$ . I took the log of both sides and set up my integral as $$\int_{0}^{1}\log(1+x^2)dx$$ But my answer came out wrong. Thanks in advance","I have been asked to find the limit of the following series by expressing it as definite integral: If always and tends to infinity, find the limiting value The answer given is . I took the log of both sides and set up my integral as But my answer came out wrong. Thanks in advance",na=1 n \lim_{n \to \infty}\left(\prod_{r=1}^{n}[1+(ra)^2]^{1/r}\right) \exp\left(\dfrac{\pi^2}{24}\right) \int_{0}^{1}\log(1+x^2)dx,"['calculus', 'limits', 'definite-integrals']"
1,Calculate the Limit of Double Sum,Calculate the Limit of Double Sum,,"Compute \begin{equation} L=\lim _{n \rightarrow \infty}\frac{1}{n} \sum_{a=1}^n \sum_{b=1}^n \frac{a}{a^2+b^2 }. \end{equation} My attempt : Define \begin{equation} f(n,m)= \frac{1}{n} \sum_{a=1}^n \frac{1}{m}\sum_{b=1}^m \frac{\frac{a}{n}}{(\frac{a}{n})^2+(\frac{b}{m})^2 } = \frac{1}{m}\sum_{b=1}^m \frac{1}{n} \sum_{a=1}^n \frac{\frac{a}{n}}{(\frac{a}{n})^2+(\frac{b}{m})^2 } \end{equation} Now for any $\epsilon >0$ there exists some $B>0$ such that for any $n,m \in \mathbb{N}$ and $n,m\geq B$ : \begin{equation} |f(n,m)-f(n,n)|<\epsilon \end{equation} Thus, we have \begin{equation} L=\lim _{n \rightarrow \infty} f(n,n)=\lim _{m \rightarrow \infty} \lim _{n \rightarrow \infty} f(n,m)= \lim _{m \rightarrow \infty} \frac{1}{m}\sum_{b=1}^m \int_{0}^{1}\frac{x}{x^2+(\frac{b}{m})^2}dx=\\ \frac{1}{2}\lim _{m \rightarrow \infty} \frac{1}{m}\sum_{b=1}^m \ln\frac{1+(\frac{b}{m})^2}{(\frac{b}{m})^2}=\frac{1}{2}\int_{0}^{1}\ln\frac{1+x^2}{x^2}dx=\frac{2\ln2 +\pi}{4} \end{equation}","Compute My attempt : Define Now for any there exists some such that for any and : Thus, we have","\begin{equation}
L=\lim _{n \rightarrow \infty}\frac{1}{n} \sum_{a=1}^n \sum_{b=1}^n \frac{a}{a^2+b^2 }.
\end{equation} \begin{equation} f(n,m)= \frac{1}{n} \sum_{a=1}^n \frac{1}{m}\sum_{b=1}^m \frac{\frac{a}{n}}{(\frac{a}{n})^2+(\frac{b}{m})^2 } = \frac{1}{m}\sum_{b=1}^m \frac{1}{n} \sum_{a=1}^n \frac{\frac{a}{n}}{(\frac{a}{n})^2+(\frac{b}{m})^2 }
\end{equation} \epsilon >0 B>0 n,m \in \mathbb{N} n,m\geq B \begin{equation}
|f(n,m)-f(n,n)|<\epsilon
\end{equation} \begin{equation}
L=\lim _{n \rightarrow \infty} f(n,n)=\lim _{m \rightarrow \infty} \lim _{n \rightarrow \infty} f(n,m)= \lim _{m \rightarrow \infty} \frac{1}{m}\sum_{b=1}^m \int_{0}^{1}\frac{x}{x^2+(\frac{b}{m})^2}dx=\\ \frac{1}{2}\lim _{m \rightarrow \infty} \frac{1}{m}\sum_{b=1}^m \ln\frac{1+(\frac{b}{m})^2}{(\frac{b}{m})^2}=\frac{1}{2}\int_{0}^{1}\ln\frac{1+x^2}{x^2}dx=\frac{2\ln2 +\pi}{4}
\end{equation}","['limits', 'riemann-sum']"
2,How can I evaluate $\lim_{n\to\infty}\Big[ \cot(\pi\sqrt{100n^2+n+1}\Big]$,How can I evaluate,\lim_{n\to\infty}\Big[ \cot(\pi\sqrt{100n^2+n+1}\Big],"While scrolling through questions in Brilliant I saw the following question on limit $$\lim_{n\to\infty}\Big[ \cot(\pi\sqrt{100n^2+n+1}\Big]$$ If we take $n^2$ outside square root we are left with ${100+\frac {1} {n} +\frac {1}{n^2}}$ inside square root and we can't use expansion of $\sqrt{(1+\frac{1}{n})}$ because of three terms inside square root. If we use $$\pi\cot(\pi x)=\frac{1}{x}+\sum_{k=1}^{\infty}\Big(\frac{1}{x-k}+\frac{1}{x+k}\Big)$$ with $x=\sqrt{100n^2+n+1}$ ,then our limit becomes $$\lim_{n\to\infty}\frac{1}{\pi}\Big[\frac{1}{ \sqrt{100n^2+n+1} }+\sum_{k=1}^{\infty}\Big(\frac{1}{\sqrt{100n^2+n+1} -k}+\frac{1}{\sqrt{100n^2+n+1} +k}\Big)\Big]$$ Now I got stuck on summation with this method. How can we show that this limit equals those messy square roots?","While scrolling through questions in Brilliant I saw the following question on limit If we take outside square root we are left with inside square root and we can't use expansion of because of three terms inside square root. If we use with ,then our limit becomes Now I got stuck on summation with this method. How can we show that this limit equals those messy square roots?",\lim_{n\to\infty}\Big[ \cot(\pi\sqrt{100n^2+n+1}\Big] n^2 {100+\frac {1} {n} +\frac {1}{n^2}} \sqrt{(1+\frac{1}{n})} \pi\cot(\pi x)=\frac{1}{x}+\sum_{k=1}^{\infty}\Big(\frac{1}{x-k}+\frac{1}{x+k}\Big) x=\sqrt{100n^2+n+1} \lim_{n\to\infty}\frac{1}{\pi}\Big[\frac{1}{ \sqrt{100n^2+n+1} }+\sum_{k=1}^{\infty}\Big(\frac{1}{\sqrt{100n^2+n+1} -k}+\frac{1}{\sqrt{100n^2+n+1} +k}\Big)\Big],"['calculus', 'sequences-and-series', 'limits', 'trigonometry', 'summation']"
3,Triangle inequality infinite terms $|a-b|\leq \sum_{i=1}^\infty|x_{i}-x_{i-1}|$,Triangle inequality infinite terms,|a-b|\leq \sum_{i=1}^\infty|x_{i}-x_{i-1}|,"While solving a problem, I thought of kind of ""Triangle inequality infinite terms"". Clearly for finite terms if $a=x_0$ and $x_n=b$ we have $$|a-b|\leq |a-x_1+x_1-x_2+\cdots+x_{n-1}-b|\leq \sum_{i=1}^n|x_{i-1}-x_{i}|  $$ It seems like, under the conditions $\lim_{n\to \infty}x_n=b$ and $x_0=a$ , this should be extended to $$|a-b|\leq \sum_{i=1}^\infty|x_{i-1}-x_{i}|$$ Is it right? Is not a surprising idea, but I think I could not find this usually on textbooks.","While solving a problem, I thought of kind of ""Triangle inequality infinite terms"". Clearly for finite terms if and we have It seems like, under the conditions and , this should be extended to Is it right? Is not a surprising idea, but I think I could not find this usually on textbooks.",a=x_0 x_n=b |a-b|\leq |a-x_1+x_1-x_2+\cdots+x_{n-1}-b|\leq \sum_{i=1}^n|x_{i-1}-x_{i}|   \lim_{n\to \infty}x_n=b x_0=a |a-b|\leq \sum_{i=1}^\infty|x_{i-1}-x_{i}|,"['sequences-and-series', 'limits']"
4,Is changing limits in this fashion okay? What is the justification for it?,Is changing limits in this fashion okay? What is the justification for it?,,"I am currently working through chapter $5$ on limits of Spivaks calculus book. I have come to exercise $15$ ix. I feel like I can answer this question correctly, but there is one small step I know to be true but can't seem to show it concisely. I would like to note, I am aware similar questions have been asked, but I am querying a specific part of this question. The question: Assume that: $$\lim_{x\to{0}}{}\frac{\sin(x)}{x}=\alpha.$$ Find, in terms of $\alpha$ , the following limit: $$\lim_{x\to1}\frac{\sin(x^2-1)}{x-1}.$$ My attempt: Firstly, multiplying bottom and top by $x+1$ yields: $$\lim_{x\to1}\frac{\sin(x^2-1)(x+1)}{(x-1)(x+1)}=\lim_{x\to1}\frac{\sin(x^2-1)(x+1)}{x^2-1}$$ Now, IF $(*):=\lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1}$ exists, we can split the above into: $$\lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1}\times{}\lim_{x\to1}{x+1},$$ using algebra of limits. Which gives us: $$\lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1}\times{}2.$$ So I'm essentially left to show $(*)$ exists and is equal to $\alpha$ . Which means the original limit is equivalent to $2\alpha$ . Now clearly as $x\to{1}$ we have that $(x^2-1)\to{0},$ so can we write this limit as: $$\lim_{x\to0}\frac{\sin(x)}{x}\text{ or }\lim_{x^2-1\to0}\frac{\sin(x^2-1)}{x^2-1}?$$ If so, why exactly? It just feels a bit imprecise. I guess what I'm asking exactly is: $$\lim_{x\to0}\frac{\sin(x)}{x}\overset{?}{=}\lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1}$$","I am currently working through chapter on limits of Spivaks calculus book. I have come to exercise ix. I feel like I can answer this question correctly, but there is one small step I know to be true but can't seem to show it concisely. I would like to note, I am aware similar questions have been asked, but I am querying a specific part of this question. The question: Assume that: Find, in terms of , the following limit: My attempt: Firstly, multiplying bottom and top by yields: Now, IF exists, we can split the above into: using algebra of limits. Which gives us: So I'm essentially left to show exists and is equal to . Which means the original limit is equivalent to . Now clearly as we have that so can we write this limit as: If so, why exactly? It just feels a bit imprecise. I guess what I'm asking exactly is:","5 15 \lim_{x\to{0}}{}\frac{\sin(x)}{x}=\alpha. \alpha \lim_{x\to1}\frac{\sin(x^2-1)}{x-1}. x+1 \lim_{x\to1}\frac{\sin(x^2-1)(x+1)}{(x-1)(x+1)}=\lim_{x\to1}\frac{\sin(x^2-1)(x+1)}{x^2-1} (*):=\lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1} \lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1}\times{}\lim_{x\to1}{x+1}, \lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1}\times{}2. (*) \alpha 2\alpha x\to{1} (x^2-1)\to{0}, \lim_{x\to0}\frac{\sin(x)}{x}\text{ or }\lim_{x^2-1\to0}\frac{\sin(x^2-1)}{x^2-1}? \lim_{x\to0}\frac{\sin(x)}{x}\overset{?}{=}\lim_{x\to1}\frac{\sin(x^2-1)}{x^2-1}","['real-analysis', 'calculus', 'limits']"
5,I solved the problem. But I have some doubts on the solution.,I solved the problem. But I have some doubts on the solution.,,"Given the sequence $a_n = \sin(n)$ , where $n$ is an integer. Prove that limit superior is $1$ . My solution: by previous theorems we know that if $x,y \in \Bbb R$ and the ratio of $x$ and y is irrational, then there exists the integers $a$ and $b$ such that $$\forall \epsilon, \quad  0<ax+by<\epsilon$$ So we can put integrs $m$ ands $n$ such that $$0<\frac{n}{m}-\frac{\pi}{2}<\epsilon$$ And we are done. But here m should be in form of $m=4k+1,\, k\in \Bbb Z$ . My question is, how we can be sure that we can find such $m$ ?","Given the sequence , where is an integer. Prove that limit superior is . My solution: by previous theorems we know that if and the ratio of and y is irrational, then there exists the integers and such that So we can put integrs ands such that And we are done. But here m should be in form of . My question is, how we can be sure that we can find such ?","a_n = \sin(n) n 1 x,y \in \Bbb R x a b \forall \epsilon, \quad  0<ax+by<\epsilon m n 0<\frac{n}{m}-\frac{\pi}{2}<\epsilon m=4k+1,\, k\in \Bbb Z m","['real-analysis', 'sequences-and-series', 'limits']"
6,What is the formal proof that the reciprocal of a number approaches 0 as the number increases without bound?,What is the formal proof that the reciprocal of a number approaches 0 as the number increases without bound?,,"In Calculus textbooks it seems assumed that $$ \lim_{n\to\infty} \frac{1}{n} = 0 ,$$ meaning that ""the limit of the reciprocal of $n$ as $n$ increases without bound is zero"". This makes intuitive sense: if $a/b$ gives the (potentially fractional) number of times $b$ fits into $a$ , then as $b$ increases without bound, $a/b$ should get increasingly small. What would be the formal proof that $$\lim_{n\to\infty} \frac{1}{n} = 0?$$ More generally, this is usually expressed as $$ \lim_{n\to\infty} \frac{a}{n} = 0 ,$$ though if $a$ does not contain $n$ it gives the same result, $$ \lim_{n\to\infty} \frac{a}{n} = a \lim_{n\to\infty} \frac{1}{n} = a(0) = 0 .$$","In Calculus textbooks it seems assumed that meaning that ""the limit of the reciprocal of as increases without bound is zero"". This makes intuitive sense: if gives the (potentially fractional) number of times fits into , then as increases without bound, should get increasingly small. What would be the formal proof that More generally, this is usually expressed as though if does not contain it gives the same result,"," \lim_{n\to\infty} \frac{1}{n} = 0 , n n a/b b a b a/b \lim_{n\to\infty} \frac{1}{n} = 0?  \lim_{n\to\infty} \frac{a}{n} = 0 , a n  \lim_{n\to\infty} \frac{a}{n} = a \lim_{n\to\infty} \frac{1}{n} = a(0) = 0 .","['calculus', 'limits']"
7,'Proof' that $f''(x)=\frac{f'(x)}{x}$,'Proof' that,f''(x)=\frac{f'(x)}{x},Consider the following: $$f''(x)=\lim_{h\to0}\frac{f'(x+h)-f'(x)}{h}$$ Now using L'Hopital's rule (as this is a case of an inderterminate) we have $$f''(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{hx}$$ But this is $$\frac{1}{x}\lim_{h\to0}\frac{f(x+h)-f(x)}{h}$$ which is $\frac{1}{x}f'(x)$ . So it would seem that $$f''(x)=\frac{f'(x)}{x}$$ which is quite obviously false. Where is my error? Thanks in advance.,Consider the following: Now using L'Hopital's rule (as this is a case of an inderterminate) we have But this is which is . So it would seem that which is quite obviously false. Where is my error? Thanks in advance.,f''(x)=\lim_{h\to0}\frac{f'(x+h)-f'(x)}{h} f''(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{hx} \frac{1}{x}\lim_{h\to0}\frac{f(x+h)-f(x)}{h} \frac{1}{x}f'(x) f''(x)=\frac{f'(x)}{x},"['calculus', 'limits', 'derivatives', 'fake-proofs']"
8,Trigonometric Integral Limit,Trigonometric Integral Limit,,"My problem is to evaluate the following limit: $$\lim_{n \to \infty}{\int^{n}_{0}{|\sin(x)|^n dx}}$$ I have no idea where to begin, but as you can tell the area under $\sin(x)^n$ decreases as $n$ approaches $\infty$ . WolframAlpha can't evaluate this for $n>134$ , where it equals about $9.29$ . If I had to guess I would say it's probably $\infty$ . One way to tackle this could be to find a representation of $$\int^{2\pi}_{0}{|\sin(x)|^n dx}$$ and multiply by $\frac{n}{2\pi}$ . This should approximate the integral for large $n$ .","My problem is to evaluate the following limit: I have no idea where to begin, but as you can tell the area under decreases as approaches . WolframAlpha can't evaluate this for , where it equals about . If I had to guess I would say it's probably . One way to tackle this could be to find a representation of and multiply by . This should approximate the integral for large .",\lim_{n \to \infty}{\int^{n}_{0}{|\sin(x)|^n dx}} \sin(x)^n n \infty n>134 9.29 \infty \int^{2\pi}_{0}{|\sin(x)|^n dx} \frac{n}{2\pi} n,"['integration', 'limits']"
9,Use of definition of limit to prove $\lim_{x→2} \dfrac{x}{x^2-2}=1$,Use of definition of limit to prove,\lim_{x→2} \dfrac{x}{x^2-2}=1,"I know that by definition I have to prove that $$\lim_{x→2}\dfrac{x}{x^2−2}=1⟺∀ϵ>0,∃δ>0,\, 0<|x−2|<δ⟹\left|\frac{x}{x^2−2}−1\right|<ϵ.$$ I have: $\left|\dfrac{-x^2+x+2}{x^2-2}\right|= \left|\dfrac{-(x^2-x-2)}{x^2-2}\right|= \left|\dfrac{-(x^2-x+1-3)}{x^2-2}\right|=$ $\left|\dfrac{-(x-1)^2+3}{x^2-2}\right|$ . So, I don't know how to continue.","I know that by definition I have to prove that I have: . So, I don't know how to continue.","\lim_{x→2}\dfrac{x}{x^2−2}=1⟺∀ϵ>0,∃δ>0,\, 0<|x−2|<δ⟹\left|\frac{x}{x^2−2}−1\right|<ϵ. \left|\dfrac{-x^2+x+2}{x^2-2}\right|= \left|\dfrac{-(x^2-x-2)}{x^2-2}\right|= \left|\dfrac{-(x^2-x+1-3)}{x^2-2}\right|= \left|\dfrac{-(x-1)^2+3}{x^2-2}\right|","['real-analysis', 'calculus', 'limits', 'epsilon-delta']"
10,find limit of $\frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n}$ with squeeze theorem [duplicate],find limit of  with squeeze theorem [duplicate],\frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n},"This question already has answers here : Evaluating Limit Question $\lim\limits_{n\to \infty}\ \frac{1+\sqrt[2]{2}+\sqrt[3]{3}+\cdots+\sqrt[n]{n}}{n}=1$? (4 answers) Closed 3 years ago . I'm trying to prove with squeeze theorem that the limit of the following series equals 1: $$\frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n}$$ For the left side of the inequality I did: $$\frac{1+\sqrt{1}+\sqrt[3]{1}+...+\sqrt[n]{1}}{n} <  \frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n}$$ For the right side, at first I did the following: $$\frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n} <  \frac{n\sqrt[n]{n}}{n}$$ But then I realized it wasn't true and that the direction of this inequality is the opposite. Do you have any idea which series with limit 1 is bigger from the original series? Thanks!","This question already has answers here : Evaluating Limit Question $\lim\limits_{n\to \infty}\ \frac{1+\sqrt[2]{2}+\sqrt[3]{3}+\cdots+\sqrt[n]{n}}{n}=1$? (4 answers) Closed 3 years ago . I'm trying to prove with squeeze theorem that the limit of the following series equals 1: For the left side of the inequality I did: For the right side, at first I did the following: But then I realized it wasn't true and that the direction of this inequality is the opposite. Do you have any idea which series with limit 1 is bigger from the original series? Thanks!",\frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n} \frac{1+\sqrt{1}+\sqrt[3]{1}+...+\sqrt[n]{1}}{n} <  \frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n} \frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n} <  \frac{n\sqrt[n]{n}}{n},"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
11,Limit in two variables of $x e^{-y^2/x}$,Limit in two variables of,x e^{-y^2/x},"I'm trying to compute the limit $$\lim_{(x,y)\rightarrow (0,0)} x e^{-y^2/x}$$ In order to compute it, I would just use the following bound: $$e^{-\frac{-y^2}{x}} \leq 1$$ and hence $$|x e^{-y^2/x}|$$ and hence the limit is $0$ . Is it right? Using polar coordinates I obtain $\lim_{r \rightarrow 0} r cos(\theta) e^{- r \frac{sin^2(\theta)}{cos(\theta)}}$ why isn't this $0$ ?","I'm trying to compute the limit In order to compute it, I would just use the following bound: and hence and hence the limit is . Is it right? Using polar coordinates I obtain why isn't this ?","\lim_{(x,y)\rightarrow (0,0)} x e^{-y^2/x} e^{-\frac{-y^2}{x}} \leq 1 |x e^{-y^2/x}| 0 \lim_{r \rightarrow 0} r cos(\theta) e^{- r \frac{sin^2(\theta)}{cos(\theta)}} 0","['limits', 'multivariable-calculus', 'solution-verification']"
12,Help solving the limit of the sequence: $\left( \frac{2n^2 - 1}{2n^2 + 1} \right) ^ { \left( \frac{2n^3 - n}{n + 3} \right) }$,Help solving the limit of the sequence:,\left( \frac{2n^2 - 1}{2n^2 + 1} \right) ^ { \left( \frac{2n^3 - n}{n + 3} \right) },"Given the following sequence, $$\left( \frac{2n^2 - 1}{2n^2 + 1} \right) ^ { \left( \frac{2n^3 - n}{n + 3} \right) }$$ I am asked to determine to which $l \in \mathbb{R}$ the sequence converges. This is what I have tried so far: $$ \lim_{n} \left( \frac{2n^2 - 1}{2n^2 + 1} \right) ^ { \left( \frac{2n^3 - n}{n + 3} \right) } \implies \left( \frac{2n^2 + 1}{2n^2 - 1} \right) ^ { - \left( \frac{2n^3 - n}{n + 3} \right) }  \implies  \left( \frac{2n^2}{2n^2 - 1} + \frac{1}{2n^2 - 1} \right) ^ { - \left( \frac{2n^3 - n}{n + 3} \right) } \implies  \left( 1 + \frac{1}{2n^2 - 1} \right) ^ { - \left( \frac{2n^3 - n}{n + 3} \right) } \implies ? $$ My objective was to algebraically modify the sequence such that I could be able to reduce it to the form: $$\lim _{x\to +\infty }\left(1+{\frac {1}{x}}\right)^{x}=e$$ but seem to be stuck at the ? point. Any suggestion? Thank you.","Given the following sequence, I am asked to determine to which the sequence converges. This is what I have tried so far: My objective was to algebraically modify the sequence such that I could be able to reduce it to the form: but seem to be stuck at the ? point. Any suggestion? Thank you.","\left( \frac{2n^2 - 1}{2n^2 + 1} \right) ^ { \left( \frac{2n^3 - n}{n + 3} \right) } l \in \mathbb{R} 
\lim_{n} \left( \frac{2n^2 - 1}{2n^2 + 1} \right) ^ { \left( \frac{2n^3 - n}{n + 3} \right) } \implies
\left( \frac{2n^2 + 1}{2n^2 - 1} \right) ^ { - \left( \frac{2n^3 - n}{n + 3} \right) }  \implies 
\left( \frac{2n^2}{2n^2 - 1} + \frac{1}{2n^2 - 1} \right) ^ { - \left( \frac{2n^3 - n}{n + 3} \right) } \implies 
\left( 1 + \frac{1}{2n^2 - 1} \right) ^ { - \left( \frac{2n^3 - n}{n + 3} \right) } \implies ?  \lim _{x\to +\infty }\left(1+{\frac {1}{x}}\right)^{x}=e","['sequences-and-series', 'limits', 'convergence-divergence']"
13,"Let $f:[a, b]\rightarrow\mathbb{R}$ be differentiable. If $f'(a)=f'(b)$, then exists a $c \in (a, b)$, such that $f'(c) = \frac{f(c) - f(a)}{c - a}$","Let  be differentiable. If , then exists a , such that","f:[a, b]\rightarrow\mathbb{R} f'(a)=f'(b) c \in (a, b) f'(c) = \frac{f(c) - f(a)}{c - a}","In the book ( Curso de Análise, volume 1 ,Elon Lages), there is a suggestion that helps a lot. First, consider that $$f'(a) =f'(b)=0$$ Then, consider the function $g:[a,b] \rightarrow \mathbb{R}$ , where $g(x) = \frac{f(x) - f(a)}{x - a}$ and $g(a) = 0$ . Show that $g$ reaches it's maximum or minimum in a point $c \in (a,b)$ . For the general case, consider $$g(x) = f(x) - xf'(a)$$ I can see why the first case: if a take the derivative of g, I end up with something like: $$g'(x) = \frac{1}{x - a} \left( f'(x) - \frac{f(x) - f(a)}{x - a} \right)$$ So, by being continuous (from the differentiable hypothesis) on a compact set, by Weierstrass's Theorem, we have that $g$ must have it's maximum/minimum on $c \in [a,b]$ . By being a critical point, we must have $g'(c) = 0$ , and assuming $c \neq a$ , we have our first conclusion. But (1) I can't see why it must be a interior point (seriously, I've been on this question for 4 days), and (2) the second suggestion isn't that clear for me. Any other ideas for solutions will be of great help to me.","In the book ( Curso de Análise, volume 1 ,Elon Lages), there is a suggestion that helps a lot. First, consider that Then, consider the function , where and . Show that reaches it's maximum or minimum in a point . For the general case, consider I can see why the first case: if a take the derivative of g, I end up with something like: So, by being continuous (from the differentiable hypothesis) on a compact set, by Weierstrass's Theorem, we have that must have it's maximum/minimum on . By being a critical point, we must have , and assuming , we have our first conclusion. But (1) I can't see why it must be a interior point (seriously, I've been on this question for 4 days), and (2) the second suggestion isn't that clear for me. Any other ideas for solutions will be of great help to me.","f'(a) =f'(b)=0 g:[a,b] \rightarrow \mathbb{R} g(x) = \frac{f(x) - f(a)}{x - a} g(a) = 0 g c \in (a,b) g(x) = f(x) - xf'(a) g'(x) = \frac{1}{x - a} \left( f'(x) - \frac{f(x) - f(a)}{x - a} \right) g c \in [a,b] g'(c) = 0 c \neq a","['real-analysis', 'limits', 'derivatives']"
14,$\lim_{x\to 0}\frac{1}{x}−\frac{2}{e^{2x}−1}$ doesn't get solved as expected,doesn't get solved as expected,\lim_{x\to 0}\frac{1}{x}−\frac{2}{e^{2x}−1},"A question says $f:\Bbb R\setminus\{0\} \to \Bbb R$ , $f(x)=\dfrac{1}{x}−\dfrac{2}{e^{2x}−1}$ . The limit as the function approaches $x=0$ is logically $0$ , as in; $\lim \limits_{x \to 0}\dfrac{e^{2x}-1}{2x} = 1$ This function then becomes $\dfrac{1}{x}−\dfrac{2}{2x} =0$ But when I graphed it online, the function approached $1$ .","A question says , . The limit as the function approaches is logically , as in; This function then becomes But when I graphed it online, the function approached .",f:\Bbb R\setminus\{0\} \to \Bbb R f(x)=\dfrac{1}{x}−\dfrac{2}{e^{2x}−1} x=0 0 \lim \limits_{x \to 0}\dfrac{e^{2x}-1}{2x} = 1 \dfrac{1}{x}−\dfrac{2}{2x} =0 1,"['calculus', 'limits']"
15,Partial derivative of a two argument function,Partial derivative of a two argument function,,"What I have to do is to get a value of partial derivative at a $(0 , 0)$ point in a following function $\\ f(x,y) = \sqrt[3]{xy} \\ \\$ What I struggle with is that I get different results while using different methods. When I use definition I get $\frac{\partial f}{\partial x}(0, 0) =  \lim_{x \to 0} \frac{f(x, 0) - f(0, 0)}{x} = 0\\ \\$ , however I can clearly see that $\frac{\partial f}{\partial x} = \frac{y^{\frac{1}{3}}}{3x^\frac{2}{3}}$ but then x can't be equal to $0$ so the derivative shouldn't have any value at a $(0, 0)$ point. Can someone point out my mistake? Which method is right?","What I have to do is to get a value of partial derivative at a point in a following function What I struggle with is that I get different results while using different methods. When I use definition I get , however I can clearly see that but then x can't be equal to so the derivative shouldn't have any value at a point. Can someone point out my mistake? Which method is right?","(0 , 0) \\ f(x,y) = \sqrt[3]{xy} \\ \\ \frac{\partial f}{\partial x}(0, 0) =  \lim_{x \to 0} \frac{f(x, 0) - f(0, 0)}{x} = 0\\ \\ \frac{\partial f}{\partial x} = \frac{y^{\frac{1}{3}}}{3x^\frac{2}{3}} 0 (0, 0)","['limits', 'derivatives', 'partial-derivative']"
16,Find limits of 2 sequences,Find limits of 2 sequences,,"I have 2 sequences I have to find the limit of. But I'm not sure if I've proven this correct enough or if there's a better way to do so. (a) $\lim_{n\to \infty}$ $\frac{n^2+(-1)^n}{5n^2+3n+1}$ (b) $\lim_{n\to \infty}$ $\frac{a^n+2b^n}{3a^n+5b^n}$ , for $a,b \gt 0$ So for (a) I have that since $(-1)^n$ is bounded, it is irrelevant for large n. Also, (I don't know the correct term for this) since $n^2$ is dominant compared to $n$ , for very large $n$ , the limit is basically $\lim_{n\to \infty}$ $\frac{n^2}{5n^2}$ $= 1/5$ . I don't know how to write this correctly since this seems quite informal and not proven. for (b) I'm not quite sure. First I tried factoring something out but I did not get very far. Now I used $c$ $=$ $a^n+2b^n$ . This results in $\frac{c}{3c-b^n}$ . But that's where I'm stuck. How do i proceed from this?","I have 2 sequences I have to find the limit of. But I'm not sure if I've proven this correct enough or if there's a better way to do so. (a) (b) , for So for (a) I have that since is bounded, it is irrelevant for large n. Also, (I don't know the correct term for this) since is dominant compared to , for very large , the limit is basically . I don't know how to write this correctly since this seems quite informal and not proven. for (b) I'm not quite sure. First I tried factoring something out but I did not get very far. Now I used . This results in . But that's where I'm stuck. How do i proceed from this?","\lim_{n\to \infty} \frac{n^2+(-1)^n}{5n^2+3n+1} \lim_{n\to \infty} \frac{a^n+2b^n}{3a^n+5b^n} a,b \gt 0 (-1)^n n^2 n n \lim_{n\to \infty} \frac{n^2}{5n^2} = 1/5 c = a^n+2b^n \frac{c}{3c-b^n}","['sequences-and-series', 'limits']"
17,"How to show that the sequence $\int_{0}^{\infty} \frac{e^{-nx}}{\sqrt{x}} \,dx$ converges to $0$?",How to show that the sequence  converges to ?,"\int_{0}^{\infty} \frac{e^{-nx}}{\sqrt{x}} \,dx 0","I need to show that for any $\epsilon>0$ exist a $N \in \mathbb{N}$ s.t $n \geq N\in \mathbb{N}$$ \implies$ $\left|\int_{0}^{\infty} \frac{e^{-nx}}{\sqrt{x}} \,dx \right|<\epsilon$ . I know that: \begin{align*} \left|\int_{0}^{\infty} \frac{e^{-nx}}{\sqrt{x}} \,dx \right|\leq \int_{0}^{\infty} \left| \frac{e^{-nx}}{\sqrt{x}} \right| \,dx=\int_{0}^{\infty} \frac{e^{-nx}}{\sqrt{x}} \,dx \end{align*} But i can't find a function $g(x,n)$ such that: \begin{align*} \int_{0}^{\infty} \frac{e^{-nx}}{\sqrt{x}} \,dx \leq \int_{0}^{\infty} g(x,n) \,dx \end{align*} I apreciate your help.",I need to show that for any exist a s.t . I know that: But i can't find a function such that: I apreciate your help.,"\epsilon>0 N \in \mathbb{N} n \geq N\in \mathbb{N} \implies \left|\int_{0}^{\infty} \frac{e^{-nx}}{\sqrt{x}} \,dx \right|<\epsilon \begin{align*}
\left|\int_{0}^{\infty} \frac{e^{-nx}}{\sqrt{x}} \,dx \right|\leq \int_{0}^{\infty} \left| \frac{e^{-nx}}{\sqrt{x}} \right| \,dx=\int_{0}^{\infty} \frac{e^{-nx}}{\sqrt{x}} \,dx
\end{align*} g(x,n) \begin{align*}
\int_{0}^{\infty} \frac{e^{-nx}}{\sqrt{x}} \,dx \leq \int_{0}^{\infty} g(x,n) \,dx
\end{align*}","['limits', 'improper-integrals']"
18,Does $\int_0^x \tan\left(\frac\pi4e^{-t}\right) dt $ have a horizontal asymptote?,Does  have a horizontal asymptote?,\int_0^x \tan\left(\frac\pi4e^{-t}\right) dt ,"Let $$f(x) = \int_0^x  \tan\left(\frac\pi4e^{-t}\right) dt.$$ Does $f(x)$ have a horizontal asymptote? If so, what value does it tend to? Also, what are the necessary and sufficient conditions on which a function has horizontal asymptotes for a function that cannot be defined by elementary function, i.e., like above?","Let Does have a horizontal asymptote? If so, what value does it tend to? Also, what are the necessary and sufficient conditions on which a function has horizontal asymptotes for a function that cannot be defined by elementary function, i.e., like above?",f(x) = \int_0^x  \tan\left(\frac\pi4e^{-t}\right) dt. f(x),"['calculus', 'limits', 'asymptotics']"
19,Does this limit hold?,Does this limit hold?,,"Suppose that $f$ and $g$ are continuously differentiable that converge to $l_1$ and $l_2$ , when $x\to\infty$ . Does it hold that $$\lim_{x\to\infty}\left(\frac{f(x)}{g(x)}\right)^2=\left(\lim_{x\to\infty}\frac{f(x)}{g(x)}\right)^2=\left(\frac{l_1}{l_2}\right)^2$$","Suppose that and are continuously differentiable that converge to and , when . Does it hold that",f g l_1 l_2 x\to\infty \lim_{x\to\infty}\left(\frac{f(x)}{g(x)}\right)^2=\left(\lim_{x\to\infty}\frac{f(x)}{g(x)}\right)^2=\left(\frac{l_1}{l_2}\right)^2,"['limits', 'derivatives', 'continuity']"
20,"How to find the supremum of the sequence $(x_n)$, where $x_n = \frac{2n}{6n+3}$ without using any calculus.","How to find the supremum of the sequence , where  without using any calculus.",(x_n) x_n = \frac{2n}{6n+3},"My professor gets upset when we use Calculus to solve problems in his Introduction to Abstract Math course. I have taken Linear Algebra, Discrete Math, and Calculus I. I have no idea how to find the supremum of the sequence $(x_n)$ with $x_n = \frac{2n}{6n+3}$ for $n \geq 1$ and $n \in \mathbb{Z}^+$ without using a limit. I know that the limit of $x_n$ as $n$ approaches infinity is $\frac{1}{3}$ , which would be the supremum, but I do not know how to show that without doing a limit. How would I go about finding that?","My professor gets upset when we use Calculus to solve problems in his Introduction to Abstract Math course. I have taken Linear Algebra, Discrete Math, and Calculus I. I have no idea how to find the supremum of the sequence with for and without using a limit. I know that the limit of as approaches infinity is , which would be the supremum, but I do not know how to show that without doing a limit. How would I go about finding that?",(x_n) x_n = \frac{2n}{6n+3} n \geq 1 n \in \mathbb{Z}^+ x_n n \frac{1}{3},"['real-analysis', 'sequences-and-series', 'limits']"
21,Question on Fundamental Theorem of Calculus,Question on Fundamental Theorem of Calculus,,"In my answer to this question ( When we evaluate an indefinite integral of one variable, what area does this yield? ) I wrote the following: Now, if we divide both sides by $h$ we obtain the following: $$\lim_{h\to0}\frac{F(x+h)-F(x)}{h}=\lim_{h\to0}\frac{f(x+h)+f(x)}{2}$$ but we can see that the expression on the left hand side is the definition of the derivative, $F'(x)$ , for $F(x)$ , our area accumulator function. So we can write $$F'(x)=\lim_{h\to0}\frac{f(x+h)+f(x)}{2}=\frac{f(x)+f(x)}{2}=\frac{2f(x)}{2}=f(x)$$ My question is, on the right hand side of the equality we basically say that $$\lim_{h\to0}f(x+h)=f(x)$$ However, why don't we say the same also on the left hand side; ie why don't we write (instead of $F'(x)$ ) $$\lim_{h\to0}\frac{F(x+h)-F(x)}{h}=\lim_{h\to0}\frac{F(x)-F(x)}{h}=\lim_{h\to0}\frac{0}{h}=\infty$$ ie why doesn't the left hand side become meaningless if we are willing to write on the right hand side $\lim_{h\to0}f(x+h)=f(x)$ ? Thank you for your help. If my derivation of the Fundamental Theorem of calculus is mistaken please tell me and help me correct it :)","In my answer to this question ( When we evaluate an indefinite integral of one variable, what area does this yield? ) I wrote the following: Now, if we divide both sides by we obtain the following: but we can see that the expression on the left hand side is the definition of the derivative, , for , our area accumulator function. So we can write My question is, on the right hand side of the equality we basically say that However, why don't we say the same also on the left hand side; ie why don't we write (instead of ) ie why doesn't the left hand side become meaningless if we are willing to write on the right hand side ? Thank you for your help. If my derivation of the Fundamental Theorem of calculus is mistaken please tell me and help me correct it :)",h \lim_{h\to0}\frac{F(x+h)-F(x)}{h}=\lim_{h\to0}\frac{f(x+h)+f(x)}{2} F'(x) F(x) F'(x)=\lim_{h\to0}\frac{f(x+h)+f(x)}{2}=\frac{f(x)+f(x)}{2}=\frac{2f(x)}{2}=f(x) \lim_{h\to0}f(x+h)=f(x) F'(x) \lim_{h\to0}\frac{F(x+h)-F(x)}{h}=\lim_{h\to0}\frac{F(x)-F(x)}{h}=\lim_{h\to0}\frac{0}{h}=\infty \lim_{h\to0}f(x+h)=f(x),"['calculus', 'integration', 'limits', 'derivatives', 'fake-proofs']"
22,Inexistence or limit that does not exist,Inexistence or limit that does not exist,,"We suppose that have this limit: $$\lim _{x\to +\infty }\frac{(x-1)^{\sqrt x}}{x-2}$$ Are there theorems in Mathematical Analysis, corollaries that use successions, particular strategies, that help me to demonstrate that a limit exists or does not exist? Related question : Limits that do not exist: search of general techniques","We suppose that have this limit: Are there theorems in Mathematical Analysis, corollaries that use successions, particular strategies, that help me to demonstrate that a limit exists or does not exist? Related question : Limits that do not exist: search of general techniques",\lim _{x\to +\infty }\frac{(x-1)^{\sqrt x}}{x-2},"['calculus', 'limits', 'proof-explanation', 'alternative-proof']"
23,Find limit of $f(x)$ as $x$ tends to $0$,Find limit of  as  tends to,f(x) x 0,I need some help answering this question: $$f(x) = \frac{\cosh(x)}{\sinh(x)} - \frac{1}{x}$$ find the limit of $f(x)$ as $x$ tends to $0$ by writing $f(x)$ as a quotient of two powers series. I have so far: $$\frac{(x(1+\frac{x^2}{2!}+\frac{x^4}{4!}+\cdots))-(x + \frac{x^3}{3!} + \cdots)}{x(x + \frac{x^3}{3!}+\cdots)}= \frac{(x+\frac{x^3}{2!}+\frac{x^5}{4!}+\cdots))-(x + \frac{x^3}{3!} + \cdots)}{(x^2 + \frac{x^4}{3!}+\cdots)}$$ but I don't know how to reduce this further.,I need some help answering this question: find the limit of as tends to by writing as a quotient of two powers series. I have so far: but I don't know how to reduce this further.,f(x) = \frac{\cosh(x)}{\sinh(x)} - \frac{1}{x} f(x) x 0 f(x) \frac{(x(1+\frac{x^2}{2!}+\frac{x^4}{4!}+\cdots))-(x + \frac{x^3}{3!} + \cdots)}{x(x + \frac{x^3}{3!}+\cdots)}= \frac{(x+\frac{x^3}{2!}+\frac{x^5}{4!}+\cdots))-(x + \frac{x^3}{3!} + \cdots)}{(x^2 + \frac{x^4}{3!}+\cdots)},"['sequences-and-series', 'limits', 'trigonometry', 'power-series']"
24,"If $f(x)=\frac{e^{\tan x} -e^x+\ln (\sec x +\tan x)-x}{\tan x -x}$ is a continuous function at $x=0$, find $f(0)$","If  is a continuous function at , find",f(x)=\frac{e^{\tan x} -e^x+\ln (\sec x +\tan x)-x}{\tan x -x} x=0 f(0),"Using L’Hospital will provide the answer, the process is long and tedious, so I generally like to avoid it in such questions. But I am not able to find an alternative. Can I get a hint for this?","Using L’Hospital will provide the answer, the process is long and tedious, so I generally like to avoid it in such questions. But I am not able to find an alternative. Can I get a hint for this?",,"['limits', 'functions', 'continuity', 'limits-without-lhopital']"
25,How to show that $2\sum_{j=1}^{N}\cot(\frac{\pi j}{2N+1})$ is related to $(\frac{4 \text{N}+2}{\pi}) (H_{\text{N}}+\log (\frac{2}{\pi }))$?,How to show that  is related to ?,2\sum_{j=1}^{N}\cot(\frac{\pi j}{2N+1}) (\frac{4 \text{N}+2}{\pi}) (H_{\text{N}}+\log (\frac{2}{\pi })),"I have started with the following relation: $ 2\sum_{j=1}^{N}\frac{1}{\tan\left(\frac{\pi j}{2N+1}\right)}=\frac{4N+2}{\pi}H_N+2\sum_{j=1}^{N}\left[\frac{1}{\tan\left(\frac{\pi j}{2N+1}\right)}-\frac{1}{(\frac{\pi j}{2N+1})}\right]\tag1$ According to WolframAlpha, $\frac{1}{\tan(x)}-\frac{1}{x}$ can be integrated over interval $\left(0,\frac{\pi}{2}\right)$ to equal $\log \left(\frac{2}{\pi }\right)$ . So using $\log \left(\frac{2}{\pi }\right)$ in the right hand side of (1) gives: $(\frac{4 \text{N}+2}{\pi}) (H_{\text{N}}+\log (\frac{2}{\pi }))\tag2$ Empirically (1) seems to be related to (2). My questions are How do I show (1) is related to (2)? How can I prove this? How do I find bounds for any error? or does the harmonic number and nearer the $\tan$ part approximates simply reduce this error the larger N gets? If so then how do I prove this?","I have started with the following relation: According to WolframAlpha, can be integrated over interval to equal . So using in the right hand side of (1) gives: Empirically (1) seems to be related to (2). My questions are How do I show (1) is related to (2)? How can I prove this? How do I find bounds for any error? or does the harmonic number and nearer the part approximates simply reduce this error the larger N gets? If so then how do I prove this?"," 2\sum_{j=1}^{N}\frac{1}{\tan\left(\frac{\pi j}{2N+1}\right)}=\frac{4N+2}{\pi}H_N+2\sum_{j=1}^{N}\left[\frac{1}{\tan\left(\frac{\pi j}{2N+1}\right)}-\frac{1}{(\frac{\pi j}{2N+1})}\right]\tag1 \frac{1}{\tan(x)}-\frac{1}{x} \left(0,\frac{\pi}{2}\right) \log \left(\frac{2}{\pi }\right) \log \left(\frac{2}{\pi }\right) (\frac{4 \text{N}+2}{\pi}) (H_{\text{N}}+\log (\frac{2}{\pi }))\tag2 \tan","['integration', 'sequences-and-series', 'limits', 'summation', 'trigonometric-series']"
26,"If a 'distance function' does not possess triangle inequality property, would the limit of a converging sequence still be unique?","If a 'distance function' does not possess triangle inequality property, would the limit of a converging sequence still be unique?",,"Let $X$ be a set and $d$ be a function such that $d:X\times X\to \mathbb{R}$ such that it satisfies positivity, that is, $d(x,y)\geq 0$ and $d(x,y)=0 \iff x=y.$ Moreover suppose it satisfies symmetry property, that is, $d(x,y)=d(y,x).$ However it does not satisfy triangle inequality. Obviously if triangle inequality was to be satisfied then this will make $(X,d)$ a metric space and subsequently every converging sequence will have a unique limit. Hence I am just curious if this property is taken away, can there still be examples such that every converging sequence has a unique limit with respect to this function $d$ ? I hope I explained my question sufficiently clear, many thanks in advance!","Let be a set and be a function such that such that it satisfies positivity, that is, and Moreover suppose it satisfies symmetry property, that is, However it does not satisfy triangle inequality. Obviously if triangle inequality was to be satisfied then this will make a metric space and subsequently every converging sequence will have a unique limit. Hence I am just curious if this property is taken away, can there still be examples such that every converging sequence has a unique limit with respect to this function ? I hope I explained my question sufficiently clear, many thanks in advance!","X d d:X\times X\to \mathbb{R} d(x,y)\geq 0 d(x,y)=0 \iff x=y. d(x,y)=d(y,x). (X,d) d","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'metric-spaces']"
27,"Can $\int_0^\infty f (x) \, dx$ exist if $\lim_{x \to \infty} f(x)$ does not exist?",Can  exist if  does not exist?,"\int_0^\infty f (x) \, dx \lim_{x \to \infty} f(x)","Is is possible to have a function for which $\lim_{x \to \infty} f(x)$ does not exist, but $\int_0^\infty f(x) \, dx$ exists and is finite? I think I've found an example actually, but I'm not sure it works. Let $H_n$ be the $n$ th harmonic number. Consider $f$ such that $f(x) = 1$ for $x \in [0,1)$ and $f(x) = (-1)^{n}$ for $x \in [H_n , H_{n + 1})$ . It seems that $$ \int_0^\infty f(x) \, dx = \sum_{n = 1}^\infty \frac{(-1)^{n + 1}}{n} = \log 2 $$ even though $\lim_{x \to \infty} f(x)$ doesn't exist. Does this work?","Is is possible to have a function for which does not exist, but exists and is finite? I think I've found an example actually, but I'm not sure it works. Let be the th harmonic number. Consider such that for and for . It seems that even though doesn't exist. Does this work?","\lim_{x \to \infty} f(x) \int_0^\infty f(x) \, dx H_n n f f(x) = 1 x \in [0,1) f(x) = (-1)^{n} x \in [H_n , H_{n + 1}) 
\int_0^\infty f(x) \, dx = \sum_{n = 1}^\infty \frac{(-1)^{n + 1}}{n} = \log 2
 \lim_{x \to \infty} f(x)","['sequences-and-series', 'limits', 'improper-integrals', 'harmonic-numbers']"
28,Convergence of $\sum_{n=1}^{\infty} \frac{{(-1)}^n {2n \choose n}}{3^n}$,Convergence of,\sum_{n=1}^{\infty} \frac{{(-1)}^n {2n \choose n}}{3^n},"Does the series converge absolute, converge conditionally, or diverge. $$\sum_{n=1}^{\infty} \frac{{(-1)}^n {2n \choose n}}{3^n}$$ I am confuse because doesnt $$\lim_{n \to \infty} \frac{{2n \choose n}}{3^n}=0$$ ?  Why doesnt this converge condiitonally at least and maybe absolutely?  Key says it diverges.","Does the series converge absolute, converge conditionally, or diverge. I am confuse because doesnt ?  Why doesnt this converge condiitonally at least and maybe absolutely?  Key says it diverges.",\sum_{n=1}^{\infty} \frac{{(-1)}^n {2n \choose n}}{3^n} \lim_{n \to \infty} \frac{{2n \choose n}}{3^n}=0,"['real-analysis', 'calculus']"
29,Solution verification: $ \lim _ { x \to 3 } \frac { 2 x - 6 } { \sqrt x - \sqrt 3 } = ? $,Solution verification:, \lim _ { x \to 3 } \frac { 2 x - 6 } { \sqrt x - \sqrt 3 } = ? ,This limit is not too difficult but I was just wondering if my work/solution looked good? Thanks so much for your input!! $$ \lim _ { x \to 3 } \frac { 2 x - 6 } { \sqrt x - \sqrt 3 } = ? $$ $$ 2 x - 6 = 2 x \left( 1 - \frac 6 { 2 x } \right) $$ $$ \lim _ { x \to 3 } \frac { 2 x - 6 } { \sqrt x - \sqrt 3 } = \lim _ { x \to 3 } \frac { 2 x \left( 1 - \frac 6 { 2 x } \right) } { \sqrt x - \sqrt 3 } = 2 \cdot \lim _ { x \to 3 } \frac { x \left( 1 - \frac 6 { 2 x } \right) } { \sqrt x - \sqrt 3 } = 2 \cdot \lim _ { x \to 3 } \frac { x - 3 } { \sqrt x - \sqrt 3 } $$ By rationalizing the denominator: $$ \frac { x - 3 } { \sqrt x - \sqrt 3 } = \sqrt x + \sqrt 3 $$ $$ 2 \cdot \lim _ { x \to 3 } \frac { x - 3 } { \sqrt x - \sqrt 3 } = 2 \cdot \lim _ { x \to 3 } \left( \sqrt x + \sqrt 3 \right) $$ By plugging in $ x = 3 $ : $$ 2 \cdot \lim _ { x \to 3 } \left( \sqrt x + \sqrt 3 \right) = 2 \left( \sqrt 3 + \sqrt 3 \right) = 4 \sqrt 3 $$,This limit is not too difficult but I was just wondering if my work/solution looked good? Thanks so much for your input!! By rationalizing the denominator: By plugging in :," \lim _ { x \to 3 } \frac { 2 x - 6 } { \sqrt x - \sqrt 3 } = ?   2 x - 6 = 2 x \left( 1 - \frac 6 { 2 x } \right)   \lim _ { x \to 3 } \frac { 2 x - 6 } { \sqrt x - \sqrt 3 } =
\lim _ { x \to 3 } \frac { 2 x \left( 1 - \frac 6 { 2 x } \right) } { \sqrt x - \sqrt 3 } =
2 \cdot \lim _ { x \to 3 } \frac { x \left( 1 - \frac 6 { 2 x } \right) } { \sqrt x - \sqrt 3 } =
2 \cdot \lim _ { x \to 3 } \frac { x - 3 } { \sqrt x - \sqrt 3 }   \frac { x - 3 } { \sqrt x - \sqrt 3 } = \sqrt x + \sqrt 3   2 \cdot \lim _ { x \to 3 } \frac { x - 3 } { \sqrt x - \sqrt 3 } =
2 \cdot \lim _ { x \to 3 } \left( \sqrt x + \sqrt 3 \right)   x = 3   2 \cdot \lim _ { x \to 3 } \left( \sqrt x + \sqrt 3 \right) =
2 \left( \sqrt 3 + \sqrt 3 \right) = 4 \sqrt 3 ","['calculus', 'limits', 'solution-verification']"
30,For $f(x) = e^x + x^3 - x^2 + x$ find the limit $\lim\limits_{x\to \infty} \frac{f^{-1}(x)}{\ln x}$.,For  find the limit .,f(x) = e^x + x^3 - x^2 + x \lim\limits_{x\to \infty} \frac{f^{-1}(x)}{\ln x},"I have the function: $$f : \mathbb{R} \rightarrow \mathbb{R} \hspace{2cm} f(x) = e^x + x^3 -x^2 + x$$ and I have to find the limit: $$\lim\limits_{x \to \infty} \frac{f^{-1}(x)}{\ln x}$$ (In the first part of the problem, I had to show that the function is strictly increasing and invertible. I don't know if that's relevant to this, since I could show that the function is invertible, but I can't find the inverse.) So this is what I tried: I showed $$\lim_{x \to \infty} f(x) = \infty$$ and so I concluded that $\lim\limits_{x \to \infty} f^{-1}(x) = \infty$ . I'm not sure if this is correct, it might be wrong. But if would be right, then we could use l'Hospital, knowing that: $$(f^{-1})'(x) = \frac{1}{f'(f^{-1}(x))}$$ but after trying to use all of this on paper, I got nowhere. It just complicated thing a lot more. So how should I solve this limit?","I have the function: and I have to find the limit: (In the first part of the problem, I had to show that the function is strictly increasing and invertible. I don't know if that's relevant to this, since I could show that the function is invertible, but I can't find the inverse.) So this is what I tried: I showed and so I concluded that . I'm not sure if this is correct, it might be wrong. But if would be right, then we could use l'Hospital, knowing that: but after trying to use all of this on paper, I got nowhere. It just complicated thing a lot more. So how should I solve this limit?",f : \mathbb{R} \rightarrow \mathbb{R} \hspace{2cm} f(x) = e^x + x^3 -x^2 + x \lim\limits_{x \to \infty} \frac{f^{-1}(x)}{\ln x} \lim_{x \to \infty} f(x) = \infty \lim\limits_{x \to \infty} f^{-1}(x) = \infty (f^{-1})'(x) = \frac{1}{f'(f^{-1}(x))},"['calculus', 'limits']"
31,Spivak Calculus Limit Intuition Clarification,Spivak Calculus Limit Intuition Clarification,,"This is from page 94 of Spivak's ""Calculus"" 4th edition. He builds up the definition of a limit from examples, but I am confused about this paragraph: Let $f(x) = \frac{1}{x}$ (for $x \neq 0$ ) To show in general that $f$ approaches $1/a$ near $a$ for any $a$ we proceed in basically the same way, except that, again, we have to be a little more careful in formulating our initial stipulation. It's not good enough simply to require that $|x-a|$ should be less than 1, or any other particular number, because if $a$ is close to 0 this would allow values of $x$ that are negative (not to mention the embarrassing possibility that $x=0$ , so that $f(x)$ isn't even defined!). Why would negative values of $x$ be bad in this example? Couldn't the value of $a$ be negative, since $f(x)$ is defined for $x < 0$ ?","This is from page 94 of Spivak's ""Calculus"" 4th edition. He builds up the definition of a limit from examples, but I am confused about this paragraph: Let (for ) To show in general that approaches near for any we proceed in basically the same way, except that, again, we have to be a little more careful in formulating our initial stipulation. It's not good enough simply to require that should be less than 1, or any other particular number, because if is close to 0 this would allow values of that are negative (not to mention the embarrassing possibility that , so that isn't even defined!). Why would negative values of be bad in this example? Couldn't the value of be negative, since is defined for ?",f(x) = \frac{1}{x} x \neq 0 f 1/a a a |x-a| a x x=0 f(x) x a f(x) x < 0,"['calculus', 'limits']"
32,How to prove that $\lim_{y \rightarrow \infty} \frac{\sin^2(xy)}{yx^2}=\pi \delta(x)$,How to prove that,\lim_{y \rightarrow \infty} \frac{\sin^2(xy)}{yx^2}=\pi \delta(x),"I want to understand how to prove that $$\lim_{y \rightarrow \infty} \frac{\sin^2(xy)}{yx^2}=\pi \delta(x)$$ The proof I am studying relies on doing the following Fourier Transform $$\int dx \frac{\sin^2(xy)}{yx^2} \exp(-ipx) = \frac{\pi}{2} \Theta(2y - |p|) \Big(2-\frac{|p|}{y}\Big)$$ Which is shown by using the inverse of this formula. Then for $y \rightarrow \infty$ the right hand side goes to $\pi$ which gets you the limit proven. However, I do not understand this proof. Could you please shed more details on it? If you have another kind of proof in mind please feel free to share it. Thank you.","I want to understand how to prove that The proof I am studying relies on doing the following Fourier Transform Which is shown by using the inverse of this formula. Then for the right hand side goes to which gets you the limit proven. However, I do not understand this proof. Could you please shed more details on it? If you have another kind of proof in mind please feel free to share it. Thank you.",\lim_{y \rightarrow \infty} \frac{\sin^2(xy)}{yx^2}=\pi \delta(x) \int dx \frac{\sin^2(xy)}{yx^2} \exp(-ipx) = \frac{\pi}{2} \Theta(2y - |p|) \Big(2-\frac{|p|}{y}\Big) y \rightarrow \infty \pi,"['limits', 'fourier-transform']"
33,Prove $\lim\limits_{x \to +\infty}\int_0^{\pi} xe^{-x\sin t}{\rm d}t=2$.,Prove .,\lim\limits_{x \to +\infty}\int_0^{\pi} xe^{-x\sin t}{\rm d}t=2,"Someone gives a proof as follows： Above all, notice that \begin{align*} I(x):&=\int_0^{\pi} x{\rm  e}^{-x\sin t}{\rm d}t=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm  d}t+\overbrace{\int_{\frac{\pi}{2}}^{\pi} x{\rm e}^{-x\sin t}{\rm  d}t}^{t~ \mapsto ~t+\frac{\pi}{2}}\\ &=\int_0^{\frac{\pi}{2}} x{\rm  e}^{-x\sin t}{\rm d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin  \left(t+\frac{\pi}{2}\right)}{\rm d}t\\ &=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\cos t}{\rm  d}t\\ &=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm  d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\cos\left(\frac{\pi}{2}-  t\right)}{\rm d}t\\ &=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm d}t\\ &=2\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm d}t. \end{align*} Consider making a substitution that $\theta:=tx.$ Then $x=\theta/t,  {\rm d}\theta=x{\rm d}t.$ Thus $$I(x):=2\int_0^{\frac{\pi x}{2}}  \exp\left(-x\sin\frac{\theta}{x}\right){\rm d}\theta.$$ Since $\theta/x \in[0,\pi/2],$ and $f(x):=\dfrac{\sin x}{x}$ decreases over $(0,\pi/2]$ , hence $ \sin \dfrac{\theta}{x}\ge \dfrac{2\theta}{\pi x}.$ Therefore $ -x\sin \dfrac{\theta}{x}\le -\dfrac{2\theta}{\pi },$ and    further we obtain $$\left| \exp\left(-x\sin\frac{\theta}{x}\right)\right|=  \exp\left(-x\sin\frac{\theta}{x}\right)\le \rm  e^{-\frac{2\theta}{\pi}},$$ the right hand side of which is integrable    over $ [0,+\infty) .$ Moreover $$\lim_{x \to  +\infty}\exp\left(-x\sin\frac{\theta}{x}\right)=\exp\lim_{x \to +\infty}\left(\frac{\sin \frac{\theta}{x}}{\frac{\theta}{x}}\cdot -\theta\right)=e^{-\theta}.$$ Now, we can exchange the orders of the limit and the integral by Lebesgue dominated convergence theorem and    obtain $$\lim_{x \to +\infty}I(x)=2\int_0^{+\infty}{\rm  e}^{-\theta}{\rm d}\theta=2.$$ Is this correct? I don't know Lebesgue dominated convergence theorem well. Is there another proof more elementary?","Someone gives a proof as follows： Above all, notice that Consider making a substitution that Then Thus Since and decreases over , hence Therefore and    further we obtain the right hand side of which is integrable    over Moreover Now, we can exchange the orders of the limit and the integral by Lebesgue dominated convergence theorem and    obtain Is this correct? I don't know Lebesgue dominated convergence theorem well. Is there another proof more elementary?","\begin{align*} I(x):&=\int_0^{\pi} x{\rm  e}^{-x\sin t}{\rm d}t=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm  d}t+\overbrace{\int_{\frac{\pi}{2}}^{\pi} x{\rm e}^{-x\sin t}{\rm  d}t}^{t~ \mapsto ~t+\frac{\pi}{2}}\\ &=\int_0^{\frac{\pi}{2}} x{\rm  e}^{-x\sin t}{\rm d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin  \left(t+\frac{\pi}{2}\right)}{\rm d}t\\ &=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\cos t}{\rm  d}t\\ &=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm  d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\cos\left(\frac{\pi}{2}-  t\right)}{\rm d}t\\ &=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm
d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm d}t\\
&=2\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm d}t. \end{align*} \theta:=tx. x=\theta/t,
 {\rm d}\theta=x{\rm d}t. I(x):=2\int_0^{\frac{\pi x}{2}}
 \exp\left(-x\sin\frac{\theta}{x}\right){\rm d}\theta. \theta/x \in[0,\pi/2], f(x):=\dfrac{\sin x}{x} (0,\pi/2]  \sin \dfrac{\theta}{x}\ge \dfrac{2\theta}{\pi x}.  -x\sin \dfrac{\theta}{x}\le -\dfrac{2\theta}{\pi }, \left|
\exp\left(-x\sin\frac{\theta}{x}\right)\right|=
 \exp\left(-x\sin\frac{\theta}{x}\right)\le \rm
 e^{-\frac{2\theta}{\pi}},  [0,+\infty) . \lim_{x \to
 +\infty}\exp\left(-x\sin\frac{\theta}{x}\right)=\exp\lim_{x \to +\infty}\left(\frac{\sin \frac{\theta}{x}}{\frac{\theta}{x}}\cdot -\theta\right)=e^{-\theta}. \lim_{x \to +\infty}I(x)=2\int_0^{+\infty}{\rm
 e}^{-\theta}{\rm d}\theta=2.","['real-analysis', 'calculus', 'integration', 'limits']"
34,Evaluating: $\lim_{x\to\infty} \frac{\sum_{k=0}^{x/2}\binom{x}{2k}2k(x-2k)}{\sum_{k=0}^{x/2}\binom{x}{2k}(x-2k)^{2}}$,Evaluating:,\lim_{x\to\infty} \frac{\sum_{k=0}^{x/2}\binom{x}{2k}2k(x-2k)}{\sum_{k=0}^{x/2}\binom{x}{2k}(x-2k)^{2}},"I'm trying to solve the following limit $$\lim_{x\to\infty} \frac{\sum_{k=0}^{x/2}\binom{x}{2k}2k(x-2k)}{\sum_{k=0}^{x/2}\binom{x}{2k}(x-2k)^{2}}$$ My thoughts: I've tried to use Stirling's formula for the binomial coefficient $\binom{x}{2k}\approx\frac{x^{2k}}{2k!}$ . Using that, the limit takes the value $-1$ which is wrong since it must be positive. Thanks in advance.","I'm trying to solve the following limit My thoughts: I've tried to use Stirling's formula for the binomial coefficient . Using that, the limit takes the value which is wrong since it must be positive. Thanks in advance.",\lim_{x\to\infty} \frac{\sum_{k=0}^{x/2}\binom{x}{2k}2k(x-2k)}{\sum_{k=0}^{x/2}\binom{x}{2k}(x-2k)^{2}} \binom{x}{2k}\approx\frac{x^{2k}}{2k!} -1,"['sequences-and-series', 'limits', 'binomial-coefficients']"
35,N Circle touching x-axis and fixed point,N Circle touching x-axis and fixed point,,"If a circle $C_0$ , with radius $1$ unit touches both the axes and as well as line ( $ L_1 $ ) through $P(0,4)$ , $L_1$ cut the $x$ -axis at $(x_1 ,0)$ . Again a circle $C_1$ is drawn touching $x$ -axis, line $L_1$ and another line $L_2$ through point $P$ . $L_2$ intersects $x$ -axis at $(  x_2,0)$ and this process is repeated $n$ times. Then the value of : $$\lim_{n\to \infty }\frac{x_n}{2^n} $$ is I am only able to find $C_1$ which is $(x-1)^2+(y-1)^2=1$ and $L_1$ which is $3x+4y=12$ but not able to proceed forward","If a circle , with radius unit touches both the axes and as well as line ( ) through , cut the -axis at . Again a circle is drawn touching -axis, line and another line through point . intersects -axis at and this process is repeated times. Then the value of : is I am only able to find which is and which is but not able to proceed forward","C_0 1  L_1
 P(0,4) L_1 x (x_1 ,0) C_1 x L_1 L_2 P L_2 x (  x_2,0) n \lim_{n\to \infty }\frac{x_n}{2^n}  C_1 (x-1)^2+(y-1)^2=1 L_1 3x+4y=12","['limits', 'conic-sections']"
36,Basic question about limit,Basic question about limit,,"I am new to limit. I have a function $f(x)=x^2$ for $x \neq 2$ and $f(2)=1$ . I want to prove that $\lim_{x \to 2} f(x) = 4$ . I am unsure about my work: We want to show $\forall \epsilon > 0, \exists \delta >0$ such that $0<|x-2|<\delta \implies |f(x)-4|<\epsilon$ . $|f(x)-4| < \epsilon$ $|x^2-4| < \epsilon$ $-\epsilon < x^2 -4 < \epsilon$ $4-\epsilon < x^2 < 4+\epsilon$ $\sqrt{4-\epsilon} < x < \sqrt{4+\epsilon}$ for $\epsilon \leq 4$ $\sqrt{4-\epsilon} -2 < x-2 < \sqrt{4+\epsilon} -2$ We can choose $\delta = min( 2-\sqrt{4-\epsilon}, \sqrt{4+\epsilon}-2)$ for $\epsilon \leq 4$ . For $\epsilon > 4$ , we can choose a sufficiently  small value of $\delta$ (say $\delta=0.0001$ ) Edit 1: Removed $|\sqrt{4-\epsilon}-2|< |x-2| < |\sqrt{4+\epsilon}-2|$ Is it correct? Are there any thing to improve?","I am new to limit. I have a function for and . I want to prove that . I am unsure about my work: We want to show such that . for We can choose for . For , we can choose a sufficiently  small value of (say ) Edit 1: Removed Is it correct? Are there any thing to improve?","f(x)=x^2 x \neq 2 f(2)=1 \lim_{x \to 2} f(x) = 4 \forall \epsilon > 0, \exists \delta >0 0<|x-2|<\delta \implies |f(x)-4|<\epsilon |f(x)-4| < \epsilon |x^2-4| < \epsilon -\epsilon < x^2 -4 < \epsilon 4-\epsilon < x^2 < 4+\epsilon \sqrt{4-\epsilon} < x < \sqrt{4+\epsilon} \epsilon \leq 4 \sqrt{4-\epsilon} -2 < x-2 < \sqrt{4+\epsilon} -2 \delta = min( 2-\sqrt{4-\epsilon}, \sqrt{4+\epsilon}-2) \epsilon \leq 4 \epsilon > 4 \delta \delta=0.0001 |\sqrt{4-\epsilon}-2|< |x-2| < |\sqrt{4+\epsilon}-2|","['calculus', 'limits', 'solution-verification']"
37,Limit of the form $0*\infty$,Limit of the form,0*\infty,"Consider the limit L= $\lim_{x\to\infty}$ $x^2*((\frac{x+1}{x-1})^x-e^2)$ . As $x$ approaches $\infty$ , $x^2$ approaches $\infty$ and $(\dfrac{x+1}{x-1})^x$ approaches $e^2$ , So we have a $0*\infty$ situation. I tried resolving this with L'hopital, by writing $x^2$ as $1/(1/x^2))$ , But it wasnt quite clear with how to proceed further. Another Idea I had was to write out the expansion of $(\dfrac{x+1}{x-1})^x$ by writing it as ( $1+\dfrac{2x}{x-1}$ )^x, and using the binomial theroem, But that didnt help either. This was from a high school maths exam and the syllabus DID-NOT include maclaurin/taylor series, so there has to be an easier way. The answer given is $2/3*e^2$","Consider the limit L= . As approaches , approaches and approaches , So we have a situation. I tried resolving this with L'hopital, by writing as , But it wasnt quite clear with how to proceed further. Another Idea I had was to write out the expansion of by writing it as ( )^x, and using the binomial theroem, But that didnt help either. This was from a high school maths exam and the syllabus DID-NOT include maclaurin/taylor series, so there has to be an easier way. The answer given is",\lim_{x\to\infty} x^2*((\frac{x+1}{x-1})^x-e^2) x \infty x^2 \infty (\dfrac{x+1}{x-1})^x e^2 0*\infty x^2 1/(1/x^2)) (\dfrac{x+1}{x-1})^x 1+\dfrac{2x}{x-1} 2/3*e^2,"['real-analysis', 'calculus', 'limits', 'limits-without-lhopital']"
38,"Prove that if $x_n \rightarrow x$ and $x_n + y_n \rightarrow z$, then $y_n \rightarrow z - x$","Prove that if  and , then",x_n \rightarrow x x_n + y_n \rightarrow z y_n \rightarrow z - x,"I've tried a proof of the above question, but it seems too simple to be right. If anyone has some pointers or tips that would be much appreciated as I am rather new to all this. We can deduce by $x_n + y_n \rightarrow z$ that $y_n$ must converge. So let $y_n \rightarrow y$ . $$x_n + y_n \rightarrow x +y $$ $$x_n + y_n \rightarrow z $$ $$z= x+y $$ $$y= z-x $$ $$y_n \rightarrow y $$ $$y_n \rightarrow z-x $$ Thanks for your time!","I've tried a proof of the above question, but it seems too simple to be right. If anyone has some pointers or tips that would be much appreciated as I am rather new to all this. We can deduce by that must converge. So let . Thanks for your time!",x_n + y_n \rightarrow z y_n y_n \rightarrow y x_n + y_n \rightarrow x +y  x_n + y_n \rightarrow z  z= x+y  y= z-x  y_n \rightarrow y  y_n \rightarrow z-x ,"['real-analysis', 'sequences-and-series', 'limits', 'solution-verification']"
39,$\lim_{n\to\infty} \int_{-\infty}^\infty \cos(x^{2n}) \:dx$ and $\lim_{n\to\infty} 2n \int_{-\infty}^\infty \sin(x^{2n}) \:dx$,and,\lim_{n\to\infty} \int_{-\infty}^\infty \cos(x^{2n}) \:dx \lim_{n\to\infty} 2n \int_{-\infty}^\infty \sin(x^{2n}) \:dx,"Like the title says, I'm curious if anyone has any insight on trying to compute these limits. Numerical investigations seem to indicate that $$\lim_{n\to\infty} \int_{-\infty}^\infty \cos(x^{2n}) \:dx = 2$$ and $$\lim_{n\to\infty} 2n \int_{-\infty}^\infty \sin(x^{2n}) \:dx = \pi$$ For the first one, it seems to almost follow from dominated convergence since $$\lim_{n\to\infty} \int_{-1}^1 \cos(x^{2n}) \:dx = \int_{-1}^1 \cos(0) \:dx = 2$$ All there is left to prove is that $$\lim_{n\to\infty} \int_1^\infty \cos(x^{2n})\:dx = 0$$ I've tried integrating by parts and a Fourier transform argument, but nothing seems to definitively pin this limit as being zero in a rigorous way. For the other one I am completely at a loss as to where the $\pi$ would come from in a dominated convergence style argument since the usual trick would give some multiple of $\sin(1)$ . Granted, the limit may not be $\pi$ , but I am having even less luck with this limit than the other. Any tips are appreciated.","Like the title says, I'm curious if anyone has any insight on trying to compute these limits. Numerical investigations seem to indicate that and For the first one, it seems to almost follow from dominated convergence since All there is left to prove is that I've tried integrating by parts and a Fourier transform argument, but nothing seems to definitively pin this limit as being zero in a rigorous way. For the other one I am completely at a loss as to where the would come from in a dominated convergence style argument since the usual trick would give some multiple of . Granted, the limit may not be , but I am having even less luck with this limit than the other. Any tips are appreciated.",\lim_{n\to\infty} \int_{-\infty}^\infty \cos(x^{2n}) \:dx = 2 \lim_{n\to\infty} 2n \int_{-\infty}^\infty \sin(x^{2n}) \:dx = \pi \lim_{n\to\infty} \int_{-1}^1 \cos(x^{2n}) \:dx = \int_{-1}^1 \cos(0) \:dx = 2 \lim_{n\to\infty} \int_1^\infty \cos(x^{2n})\:dx = 0 \pi \sin(1) \pi,"['calculus', 'integration', 'limits', 'improper-integrals']"
40,"Using the definition of sequence convergence, prove that if $\lim{y_n}=2$, then $\lim{3(y_n)^2−2}=10$","Using the definition of sequence convergence, prove that if , then",\lim{y_n}=2 \lim{3(y_n)^2−2}=10,"Problem: Using the definition of sequence convergence, prove that if $\lim{y_n}=2$ , then $\lim{3(y_n)^2−2}=10$ . Note: not allowed to use the Algebraic Limit Theorem, ONLY allowed to use the definition of sequence convergence. So I understand how to work with the epsilon proof structure when I'm just trying to show one sequence converges, but I'm confused here. $\lim{3(y_n)^2−2}=10$ in my proof becomes $|{(3(y_n)^2−2)-10|=|3(y_n+2)(y_n-2)|}$ .  What I am struggling with is how to go from that to the ending, $< \epsilon$ . I obviously use $\lim{y_n}=2$ so $|y_n - 2|$ < something, although I'm not quite sure what to make it less than. Can anybody help me with my proof structure here?","Problem: Using the definition of sequence convergence, prove that if , then . Note: not allowed to use the Algebraic Limit Theorem, ONLY allowed to use the definition of sequence convergence. So I understand how to work with the epsilon proof structure when I'm just trying to show one sequence converges, but I'm confused here. in my proof becomes .  What I am struggling with is how to go from that to the ending, . I obviously use so < something, although I'm not quite sure what to make it less than. Can anybody help me with my proof structure here?",\lim{y_n}=2 \lim{3(y_n)^2−2}=10 \lim{3(y_n)^2−2}=10 |{(3(y_n)^2−2)-10|=|3(y_n+2)(y_n-2)|} < \epsilon \lim{y_n}=2 |y_n - 2|,"['real-analysis', 'limits']"
41,Solve $\frac{x+\dots+x^K}{K} = \frac{1}{2}$ for large values of $K$,Solve  for large values of,\frac{x+\dots+x^K}{K} = \frac{1}{2} K,"I am interested in the unique solution $x$ for the equation : $$ p_K(x)=\frac{x+\dots+x^K}{K}=\frac{1}{2}, $$ for large values of $K$ . When $K$ is small ( $K=1$ and $K=2$ ) we can solve this equation explicitly and find : $$ x=\frac{1}{2}, \frac{\sqrt{5}-1}{2}. $$ For $K=3$ we still get an explicit solution which is more complicated and from $K=4$ on I do not find an explicit solution. As $K$ tends to infinity, we find that the unique solution $x_K$ of $p_K(x_K)=0$ tends to one, i.e. $\lim_{K \rightarrow \infty} x_K = 1$ . I would like to find an asymptotic approximation of $p_K(x)$ denoted by $\tilde p_K(x)$ for which the solutions $\tilde x_K$ satisfy: $$ \lim_{K\rightarrow \infty} K \cdot \log(x_K) = \lim_{K\rightarrow \infty} K \cdot \log(\tilde x_K). $$ My idea was to use the approximation : $$ \frac{x+\dots+x^K}{K} \approx x^{\frac{\sum_{j=1}^K j }{K}} = x^{\frac{K+1}{2}}. $$ Using this approximation, we find $\tilde x_K = \left( \frac{1}{2} \right) ^{\frac{2}{K+1}}$ and we find for the limit : $$ \lim_{K\rightarrow \infty} K \cdot \log(\tilde x_K) = -2 \log(2) \approx -1.38 $$ but by numerical approximation,  find : $$ \lim_{K\rightarrow \infty} K \cdot \log(x_K) \approx - 1.592 $$","I am interested in the unique solution for the equation : for large values of . When is small ( and ) we can solve this equation explicitly and find : For we still get an explicit solution which is more complicated and from on I do not find an explicit solution. As tends to infinity, we find that the unique solution of tends to one, i.e. . I would like to find an asymptotic approximation of denoted by for which the solutions satisfy: My idea was to use the approximation : Using this approximation, we find and we find for the limit : but by numerical approximation,  find :","x 
p_K(x)=\frac{x+\dots+x^K}{K}=\frac{1}{2},
 K K K=1 K=2 
x=\frac{1}{2}, \frac{\sqrt{5}-1}{2}.
 K=3 K=4 K x_K p_K(x_K)=0 \lim_{K \rightarrow \infty} x_K = 1 p_K(x) \tilde p_K(x) \tilde x_K 
\lim_{K\rightarrow \infty} K \cdot \log(x_K)
=
\lim_{K\rightarrow \infty} K \cdot \log(\tilde x_K).
 
\frac{x+\dots+x^K}{K} \approx x^{\frac{\sum_{j=1}^K j }{K}} = x^{\frac{K+1}{2}}.
 \tilde x_K = \left( \frac{1}{2} \right) ^{\frac{2}{K+1}} 
\lim_{K\rightarrow \infty} K \cdot \log(\tilde x_K) = -2 \log(2) \approx -1.38
 
\lim_{K\rightarrow \infty} K \cdot \log(x_K) \approx - 1.592
","['real-analysis', 'calculus', 'limits', 'polynomials', 'convex-analysis']"
42,How do I evaluate this limit without using L'Hospital or Series expansion?,How do I evaluate this limit without using L'Hospital or Series expansion?,,"I don't know how to do this limit. $$\lim_{x\to\infty}\left(\frac1{x^2\sin^2\frac 1x}\right)^\frac 1{x\sin\frac 1x-1}$$ And here it is as an image, with bigger font: I tried substituting $t=1/x$ and I got something better looking but it still didn't work. What trick do I need to use here?","I don't know how to do this limit. And here it is as an image, with bigger font: I tried substituting and I got something better looking but it still didn't work. What trick do I need to use here?",\lim_{x\to\infty}\left(\frac1{x^2\sin^2\frac 1x}\right)^\frac 1{x\sin\frac 1x-1} t=1/x,"['limits', 'limits-without-lhopital']"
43,Evaluate $\lim_{x \to \infty} \left( \left( \frac{x+1}{x-1} \right)^x - e^2\right) x^2$,Evaluate,\lim_{x \to \infty} \left( \left( \frac{x+1}{x-1} \right)^x - e^2\right) x^2,"$$\underset{x\to \infty}{\lim} \left( \left( \frac{x+1}{x-1} \right)^x - e^2\right) x^2$$ My Attempt: $$L = \underset{t\to 0}{\lim} \frac{\left( \left( \frac{t+1}{t-1} \right)^{\frac 1t} - e^2\right)} {t^2}$$ I Now have a $\frac 00$ form that I could use L'Hopital rule with, but I don't want to differentiate the ugly looking function in the numerator. Is there an easier way to solve these kinds of problems? Maybe a taylor series expansion for $(1+t)^{\frac 1t}, t \to 0$ forms would come in handy here and I could just subtract the $e^2$ from the resulting expansion.","My Attempt: I Now have a form that I could use L'Hopital rule with, but I don't want to differentiate the ugly looking function in the numerator. Is there an easier way to solve these kinds of problems? Maybe a taylor series expansion for forms would come in handy here and I could just subtract the from the resulting expansion.","\underset{x\to \infty}{\lim} \left( \left( \frac{x+1}{x-1} \right)^x - e^2\right) x^2 L = \underset{t\to 0}{\lim} \frac{\left( \left( \frac{t+1}{t-1} \right)^{\frac 1t} - e^2\right)} {t^2} \frac 00 (1+t)^{\frac 1t}, t \to 0 e^2",['limits']
44,When is it acceptable to stop adding terms to a Taylor approximation if my goal it to find a limit?,When is it acceptable to stop adding terms to a Taylor approximation if my goal it to find a limit?,,"I'm new to the concept of finding a limit using a Taylor polynomial. I am curious when is it ok to stop adding terms to the polynomial if I am trying to find a limit? I've seen examples where people stopped at the $x^2$ term and then added $O(x^3)$ to the expression, examples where people went as far as $x^7$ and then added $O(x^8)$ to the expression and everything in between. If what I want is to find a limit, how should I know when to stop? What would be acceptable in say, an exam situation? EDIT For example, say I am trying to find the limit $$\lim_{x \to 0} \dfrac{\sin x}{x}$$ without using L'Hospital. So I am trying to use the Taylor Expansion. By ""When is it acceptable to stop adding terms"" I'm really asking about what is the difference between doing something like: $$\lim_{x \to 0} \dfrac{x - \frac{x^3}{3!} + O(x^5)}{x}$$ or doing something like: $$\lim_{x \to 0} \dfrac{x - \frac{x^3}{3!} + \frac{x^5}{5!}+ O(x^7)}{x}$$ or something like: $$\lim_{x \to 0} \dfrac{x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!}   + O(x^9)}{x}$$ and so on. Which one should I use? That's what I mean when I say ""When is it acceptable to add terms"".","I'm new to the concept of finding a limit using a Taylor polynomial. I am curious when is it ok to stop adding terms to the polynomial if I am trying to find a limit? I've seen examples where people stopped at the term and then added to the expression, examples where people went as far as and then added to the expression and everything in between. If what I want is to find a limit, how should I know when to stop? What would be acceptable in say, an exam situation? EDIT For example, say I am trying to find the limit without using L'Hospital. So I am trying to use the Taylor Expansion. By ""When is it acceptable to stop adding terms"" I'm really asking about what is the difference between doing something like: or doing something like: or something like: and so on. Which one should I use? That's what I mean when I say ""When is it acceptable to add terms"".","x^2 O(x^3) x^7 O(x^8) \lim_{x \to 0} \dfrac{\sin x}{x} \lim_{x \to 0} \dfrac{x - \frac{x^3}{3!} + O(x^5)}{x} \lim_{x \to 0} \dfrac{x - \frac{x^3}{3!} + \frac{x^5}{5!}+ O(x^7)}{x} \lim_{x \to 0} \dfrac{x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} 
 + O(x^9)}{x}",['calculus']
45,Evaluate $\lim_{n\to \infty}\sqrt[n]{\frac{(17n)!}{(n!)^{17}}}$,Evaluate,\lim_{n\to \infty}\sqrt[n]{\frac{(17n)!}{(n!)^{17}}},Evaluate $$\lim_{n\to \infty}\sqrt[n]{\frac{(17n)!}{(n!)^{17}}}$$ I know that if we can show that $\lim_{n\to \infty}\frac{a_{n+1}}{a_n}=L$ then $\lim_{n\to\infty}\sqrt[n]{a_n}=L$ So we have to look at $$\lim_{n\to \infty}\frac{a_{n+1}}{a_n}=\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{[17(n+1)]!}{[(n+1)!]^{17}}}}{\sqrt[n]{\frac{(17n)!}{(n!)^{17}}}}$$ I managed to continue to: $$\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)[17n]!}{(n+1)^{17}[(n!)]^{17}}}}{\sqrt[n]{\frac{(17n)!}{(n!)^{17}}}}$$ So it is of the form: $$\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)a}{(n+1)^{17}b}}}{\sqrt[n]{\frac{a}{b}}}=\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)}{(n+1)^{17}}}\sqrt[n+1]{\frac{a}{b}}}{\sqrt[n]{\frac{a}{b}}}=\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)}{(n+1)^{17}}}\sqrt[n+1]{\frac{a}{b}}}{\sqrt[n]{\frac{a}{b}}}=\lim_{n\to \infty}{\sqrt[n+1]{\frac{(n+1)}{(n+1)^{17}}}\sqrt[n(n+1)]{\frac{a}{b}}}$$ But I can see how to proceed,Evaluate I know that if we can show that then So we have to look at I managed to continue to: So it is of the form: But I can see how to proceed,\lim_{n\to \infty}\sqrt[n]{\frac{(17n)!}{(n!)^{17}}} \lim_{n\to \infty}\frac{a_{n+1}}{a_n}=L \lim_{n\to\infty}\sqrt[n]{a_n}=L \lim_{n\to \infty}\frac{a_{n+1}}{a_n}=\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{[17(n+1)]!}{[(n+1)!]^{17}}}}{\sqrt[n]{\frac{(17n)!}{(n!)^{17}}}} \lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)[17n]!}{(n+1)^{17}[(n!)]^{17}}}}{\sqrt[n]{\frac{(17n)!}{(n!)^{17}}}} \lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)a}{(n+1)^{17}b}}}{\sqrt[n]{\frac{a}{b}}}=\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)}{(n+1)^{17}}}\sqrt[n+1]{\frac{a}{b}}}{\sqrt[n]{\frac{a}{b}}}=\lim_{n\to \infty}\frac{\sqrt[n+1]{\frac{(n+1)}{(n+1)^{17}}}\sqrt[n+1]{\frac{a}{b}}}{\sqrt[n]{\frac{a}{b}}}=\lim_{n\to \infty}{\sqrt[n+1]{\frac{(n+1)}{(n+1)^{17}}}\sqrt[n(n+1)]{\frac{a}{b}}},"['calculus', 'sequences-and-series', 'limits', 'factorial']"
46,Does $\lim_{n \to \infty}\frac{n}{n + \sum_{k=1}^{n}k}$ converge to $0$ or $1$?,Does  converge to  or ?,\lim_{n \to \infty}\frac{n}{n + \sum_{k=1}^{n}k} 0 1,"This converges. I am asking whether it converges to $0$ or $1$ because both seem to make sense. Using LH, the sum becomes $$\displaystyle\lim_{n \to \infty}\frac{n}{n + \displaystyle\sum_{k=1}^{n}k}\Rightarrow _{LH}\lim_{n \to \infty}\frac{1}{1+0}=1$$ However, I believe I am making a mistake because my idea is that since $\sum_{k=1}^{n}k$ is always a constant, I can differentiate term by term in terms of $d/dn$ to get $0$ , but $\sum_{k=1}^{n}k$ is in terms of $k$ . The case that this converges to $0$ is strong since $\sum_{k=1}^{n}k > n$ for all $n>1$ , as in the denominator grows much faster than the numerator. Nonetheless, I want a solid understanding of why I cannot differentiate the sum term by term in terms of $d/dn$ and, thus, not use LH. Furthermore, I suspect that we can just convert $\sum_{k=1}^{n}k$ to its sequence of partial sums, which would be in terms of $n$ , and then evaluate the limit from there, but I got lost trying to do this.","This converges. I am asking whether it converges to or because both seem to make sense. Using LH, the sum becomes However, I believe I am making a mistake because my idea is that since is always a constant, I can differentiate term by term in terms of to get , but is in terms of . The case that this converges to is strong since for all , as in the denominator grows much faster than the numerator. Nonetheless, I want a solid understanding of why I cannot differentiate the sum term by term in terms of and, thus, not use LH. Furthermore, I suspect that we can just convert to its sequence of partial sums, which would be in terms of , and then evaluate the limit from there, but I got lost trying to do this.",0 1 \displaystyle\lim_{n \to \infty}\frac{n}{n + \displaystyle\sum_{k=1}^{n}k}\Rightarrow _{LH}\lim_{n \to \infty}\frac{1}{1+0}=1 \sum_{k=1}^{n}k d/dn 0 \sum_{k=1}^{n}k k 0 \sum_{k=1}^{n}k > n n>1 d/dn \sum_{k=1}^{n}k n,"['calculus', 'limits', 'summation']"
47,Recursive sequence depending on the parameter,Recursive sequence depending on the parameter,,"For the given parameter $\mathbb R\ni t\geq 1$ , the sequence is   defined recursively: $$a_1=t,\;\;a_{n+1}a_n=3a_n-2$$ $(a)$ Let $t=4$ .   Prove the sequence $(a_n)$ converges and find its limit. $(b)$ Which parameters $t\geq 1$ is the sequence $(a_n)$ increasing   for? My attempt: Bolzano-Weierstrass:A sequence converges if it is monotonous and    bounded $$a_{n+1}a_n=3a_n-2\implies a_{n+1}=3-\frac{2}{a_n}$$ $(a)$ First few terms: $a_1=4,a_2=\frac{5}{2},a_3=\frac{11}{5}$ Assumption: the sequence is decreasing Proof by induction: the basis (n=1) is trivial: $\frac{5}{2}<4$ Assumption: $a_n<a_{n-1},\;\forall n\in\mathbb N$ Step: $$a_n<a_{n-1}\implies\frac{1}{a_n}\geq\frac{1}{a_{n-1}}\Bigg/\cdot(-2)$$ $$\iff-\frac{2}{a_n}\leq-\frac{2}{a_{n-1}}\iff \underbrace{3-\frac{2}{a_n}}_{a_{n+1}}\leq\underbrace{3-\frac{2}{a_{n-1}}}_{a_n}$$ The limit: $$L=3-\frac{2}{L}\implies L^2-3L+2=0$$ I take into account only $2$ because the parabola is convex and $$a_n\to L^-.$$ Then I have to prove: $a_n\geq 2\;\forall n\in\mathbb N$ after the formal computing: $a_{n+1}\geq 3-\frac{2}{2}=2$ $\underset{\implies}{\text{Bolzano-Weierstrass theorem}}(a_n)\to 2$ $(b)$ Since the sequence doesn't have to be convergent, only increasing: $$a_2=3-\frac{2}{t}\geq t\implies t\in[1,2]$$ Then, it should follow inductively,analogously to $(a)$ , this time it is increasing. Is this correct?","For the given parameter , the sequence is   defined recursively: Let .   Prove the sequence converges and find its limit. Which parameters is the sequence increasing   for? My attempt: Bolzano-Weierstrass:A sequence converges if it is monotonous and    bounded First few terms: Assumption: the sequence is decreasing Proof by induction: the basis (n=1) is trivial: Assumption: Step: The limit: I take into account only because the parabola is convex and Then I have to prove: after the formal computing: Since the sequence doesn't have to be convergent, only increasing: Then, it should follow inductively,analogously to , this time it is increasing. Is this correct?","\mathbb R\ni t\geq 1 a_1=t,\;\;a_{n+1}a_n=3a_n-2 (a) t=4 (a_n) (b) t\geq 1 (a_n) a_{n+1}a_n=3a_n-2\implies a_{n+1}=3-\frac{2}{a_n} (a) a_1=4,a_2=\frac{5}{2},a_3=\frac{11}{5} \frac{5}{2}<4 a_n<a_{n-1},\;\forall n\in\mathbb N a_n<a_{n-1}\implies\frac{1}{a_n}\geq\frac{1}{a_{n-1}}\Bigg/\cdot(-2) \iff-\frac{2}{a_n}\leq-\frac{2}{a_{n-1}}\iff \underbrace{3-\frac{2}{a_n}}_{a_{n+1}}\leq\underbrace{3-\frac{2}{a_{n-1}}}_{a_n} L=3-\frac{2}{L}\implies L^2-3L+2=0 2 a_n\to L^-. a_n\geq 2\;\forall n\in\mathbb N a_{n+1}\geq 3-\frac{2}{2}=2 \underset{\implies}{\text{Bolzano-Weierstrass theorem}}(a_n)\to 2 (b) a_2=3-\frac{2}{t}\geq t\implies t\in[1,2] (a)","['real-analysis', 'sequences-and-series', 'limits', 'solution-verification']"
48,How to find:$\lim_{x\to 0}\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{x\arctan x}$,How to find:,\lim_{x\to 0}\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{x\arctan x},"Evalute: $$\lim_{x\to  0}\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{x\arctan x}$$ My attempt: I used the standard limits from the table: $$\lim_{x\to 0}\frac{\sin x}{x}=1,\;\;\lim_{x\to 0}\frac{1-\cos x}{x^2}=\frac{1}{2},\;\;\lim_{x\to 0}\frac{\tan x}{x}=1,\;\;\lim_{x\to 0}\frac{e^x-1}{x}=1$$ $$$$ $L=\displaystyle\lim_{x\to 0}\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{x\arctan x}=$ $$$$ $\displaystyle\lim_{x\to 0}\left[\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{e^{1-\cos^3x}-e^{1-\cos^4x}}\cdot\left(\frac{e^{1-\cos^3x}-1}{1-\cos^3x}\cdot\frac{1-\cos^3x}{x^2}-\frac{e^{1-\cos^4x}-1}{1-\cos^4x}\cdot\frac{1-\cos^4x}{x^2}\right)\cdot\frac{x}{\arctan x}\right]$ Substitution: $$[t=\arctan x\implies x=\tan t\;\;\&\;\; x\to 0\implies t\to 0]$$ $$\lim_{x\to 0}\frac{x}{\arctan x}\iff\lim_{t\to 0}\frac{\tan t}{t}=1$$ The next step: $$1-\cos^3x=(1-\cos x)(1+\cos x+\cos^2x)$$ $$1-\cos^4x=(1-\cos x)(1+\cos x)(1+\cos^2x)$$ Now I obtained: $$L=1\cdot\left(1\cdot\frac{3}{2}-1\cdot 2\right)\cdot 1=-\frac{1}{2}$$ Is this correct?",Evalute: My attempt: I used the standard limits from the table: Substitution: The next step: Now I obtained: Is this correct?,"\lim_{x\to
 0}\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{x\arctan x} \lim_{x\to 0}\frac{\sin x}{x}=1,\;\;\lim_{x\to 0}\frac{1-\cos x}{x^2}=\frac{1}{2},\;\;\lim_{x\to 0}\frac{\tan x}{x}=1,\;\;\lim_{x\to 0}\frac{e^x-1}{x}=1  L=\displaystyle\lim_{x\to 0}\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{x\arctan x}=  \displaystyle\lim_{x\to 0}\left[\frac{\sin\left(e^{1-\cos^3x}-e^{1-\cos^4x}\right)}{e^{1-\cos^3x}-e^{1-\cos^4x}}\cdot\left(\frac{e^{1-\cos^3x}-1}{1-\cos^3x}\cdot\frac{1-\cos^3x}{x^2}-\frac{e^{1-\cos^4x}-1}{1-\cos^4x}\cdot\frac{1-\cos^4x}{x^2}\right)\cdot\frac{x}{\arctan x}\right] [t=\arctan x\implies x=\tan t\;\;\&\;\; x\to 0\implies t\to 0] \lim_{x\to 0}\frac{x}{\arctan x}\iff\lim_{t\to 0}\frac{\tan t}{t}=1 1-\cos^3x=(1-\cos x)(1+\cos x+\cos^2x) 1-\cos^4x=(1-\cos x)(1+\cos x)(1+\cos^2x) L=1\cdot\left(1\cdot\frac{3}{2}-1\cdot 2\right)\cdot 1=-\frac{1}{2}","['calculus', 'limits', 'limits-without-lhopital', 'solution-verification']"
49,Limit of sum $x^3+x^5+x^7+x^9+...)$,Limit of sum,x^3+x^5+x^7+x^9+...),"I am asked to give the limit of: $$ x^3+x^5+x^7+x^9+... \quad x\in(-1,1)$$ So I do the following: The sum of the first $n$ terms will be equal to: $$x^3+x^5+x^7+...+x^{3+2(n-1)}$$ I factor out $x^3$ , I get: $$x^3(1+x^2+x^4+..+x^{2(n-1)})$$ I also factor out $x^2$ , I get: $$x^3 x^2(1/x^2+1+x^2+x^3+...+x^{n-1}=x^5(1/x^2+1+x^2+x^3+...+x^{n-1})$$ So I have: $$x^5/x^2+x^5(1+x^2+x^3+...+x^{n-1})=x^3+\frac{x^5 (1-x^n)}{1-x}$$ So I take the limit when $n$ goes to infinity and I get: $$x^3+\frac{x^5}{1-x}=\frac{x^3(1-x^2)}{1-x^2}+\frac{(1+x)x^5}{1-x^2}=\frac{x^3-x^5+x^5+x^6}{1-x^2}=\frac{x^3+x^6}{1-x^2}$$ But the right answer is $\frac{x^3}{1-x^2}$ Where did I go wrong?","I am asked to give the limit of: So I do the following: The sum of the first terms will be equal to: I factor out , I get: I also factor out , I get: So I have: So I take the limit when goes to infinity and I get: But the right answer is Where did I go wrong?"," x^3+x^5+x^7+x^9+... \quad x\in(-1,1) n x^3+x^5+x^7+...+x^{3+2(n-1)} x^3 x^3(1+x^2+x^4+..+x^{2(n-1)}) x^2 x^3 x^2(1/x^2+1+x^2+x^3+...+x^{n-1}=x^5(1/x^2+1+x^2+x^3+...+x^{n-1}) x^5/x^2+x^5(1+x^2+x^3+...+x^{n-1})=x^3+\frac{x^5 (1-x^n)}{1-x} n x^3+\frac{x^5}{1-x}=\frac{x^3(1-x^2)}{1-x^2}+\frac{(1+x)x^5}{1-x^2}=\frac{x^3-x^5+x^5+x^6}{1-x^2}=\frac{x^3+x^6}{1-x^2} \frac{x^3}{1-x^2}","['limits', 'summation']"
50,Prove that $\lim_{x\to0}f(x)=\lim_{x\to0}f(x^3)$,Prove that,\lim_{x\to0}f(x)=\lim_{x\to0}f(x^3),"I'm not really sure how to do this. Is $(1)$ the way to go? $$\lim_{x\to0}f(x)-f(x^3)=0 \tag 1$$ What I tried to prove was $(2)$ with the stipulation that $x:=t^3$ . $$\lim_{x\to0}f(x) = L \iff \lim_{t\to0}f(t^3)=L \tag 1$$ Left to right : we know $|f(x)-L|<\epsilon$ when $0<|x|<\delta$ . Let $\delta':=\min\{1/2, \delta^3\}$ , so that if $|x|<\delta'$ , then $|t|<\delta$ . Then $|f(t^3)-L|<\epsilon$ . Right to left : we know $|f(t^3)-L|<\epsilon$ when $0<|t|<\delta$ . Let $\delta':=\min\{1/2, \delta\}$ , so that if $|t|<\delta'$ , then $|x|<\delta$ . Then $|f(x)-L|<\epsilon$ . Is this correct? Did I overcomplicate things?","I'm not really sure how to do this. Is the way to go? What I tried to prove was with the stipulation that . Left to right : we know when . Let , so that if , then . Then . Right to left : we know when . Let , so that if , then . Then . Is this correct? Did I overcomplicate things?","(1) \lim_{x\to0}f(x)-f(x^3)=0 \tag 1 (2) x:=t^3 \lim_{x\to0}f(x) = L \iff \lim_{t\to0}f(t^3)=L \tag 1 |f(x)-L|<\epsilon 0<|x|<\delta \delta':=\min\{1/2, \delta^3\} |x|<\delta' |t|<\delta |f(t^3)-L|<\epsilon |f(t^3)-L|<\epsilon 0<|t|<\delta \delta':=\min\{1/2, \delta\} |t|<\delta' |x|<\delta |f(x)-L|<\epsilon","['real-analysis', 'limits']"
51,Solving a limit using only special limits and algebric manipulations,Solving a limit using only special limits and algebric manipulations,,"I'm wondering how to solve this limit: $$\lim_{x \to 0^+} \frac{\tan^3((1+x^{\frac 23})^\frac13-1)+\ln(1+\sin^2(x))}{\arctan^2(3x)+5^{x^4}-1}(\sqrt{\frac{1+x+x^2}{x^2}}-\frac 1x)$$ With my actual notions that are: -Special limits -A limit of a sum/product/quotient of functions is the sum/product/quotient of limits of those functions if the functions converge(and also if the denominator function doesn't converge to 0 in the case of quotient) -Basic notions like $+\infty\cdot a=+\infty, a>0$ etc -Comparison theorem -Algebric manipulations Often my teacher does this ""trick"": ""If we have to calculate: $\lim_\limits{x \to x_0} s(x)c(x)$ . Where $s$ is a simple function that we know to be convergent to a non-zero value and $c$ is a complicated functions whom limit is unknown. We can write this: $$ \lim_\limits{x \to x_0} s(x)c(x)=\lim_\limits{x \to x_0} s(x)\lim_\limits{x \to x_0} c(x)$$ If we discover then that: $$\lim_\limits{x \to x_0} c(x)\in \mathbb{R}$$ Then our previous passage is justified. If we discover that: $$\lim_\limits{x \to x_0} c(x)\in \pm \infty$$ Then our previous passage is not justified formally, but it doesn't affect the limit(it's a kind of notation abuse). If we discover that: $$\not\exists \lim_\limits{x \to x_0} c(x)$$ Then our passage is not justified and it may have affected the limit result"" I kinda understood why this works(it's a kind of retrospective justificatin) but i was wondering if there a was a more formal way to describe this, because when i try to do limits I always try to justify all the steps I do and to be formal. However let's go back to the initial limit and to my attempt: $$\lim_{x \to 0^+} \frac{\tan^3((1+x^{\frac 23})^\frac13-1)+\ln(1+\sin^2(x))}{\arctan^2(3x)+5^{x^4}-1}(\sqrt{\frac{1+x+x^2}{x^2}}-\frac 1x)$$ Let's try to calculate first: $$\lim_{x \to 0^+} \sqrt{\frac{1+x+x^2}{x^2}}-\frac 1x=\lim_{x \to 0^+} \frac{\sqrt{1+x+x^2}-1}{x}=\lim_{x \to 0^+} \frac{\sqrt{1+x+x^2}-1}{x+x^2}(x+1)$$ Now i use a known special limit: $$\lim_{x \to 0^+} \frac{x+1}{2}=\frac 12$$ Now let's use the trick of my teacher and let's hope that the remaining limit exists otherwise we are at the starting point(this is also why sometimes i'm a bit unsure doing this it feels like a bet): $$\frac 12\lim_{x \to 0^+} \frac{\tan^3((1+x^{\frac 23})^\frac13-1)+\ln(1+\sin^2(x))}{\arctan^2(3x)+5^{x^4}-1}$$ And now i'm stuck because I see many useful special limits that i could apply but it always come to a $$0 \cdot \infty$$ form where i can't apply the ""trick"". Sometimes I feel i'm overcomplicating everything by being too formal but I really want to understand why I can apply something and I don't want to make it become an automatism before i totally understood it.","I'm wondering how to solve this limit: With my actual notions that are: -Special limits -A limit of a sum/product/quotient of functions is the sum/product/quotient of limits of those functions if the functions converge(and also if the denominator function doesn't converge to 0 in the case of quotient) -Basic notions like etc -Comparison theorem -Algebric manipulations Often my teacher does this ""trick"": ""If we have to calculate: . Where is a simple function that we know to be convergent to a non-zero value and is a complicated functions whom limit is unknown. We can write this: If we discover then that: Then our previous passage is justified. If we discover that: Then our previous passage is not justified formally, but it doesn't affect the limit(it's a kind of notation abuse). If we discover that: Then our passage is not justified and it may have affected the limit result"" I kinda understood why this works(it's a kind of retrospective justificatin) but i was wondering if there a was a more formal way to describe this, because when i try to do limits I always try to justify all the steps I do and to be formal. However let's go back to the initial limit and to my attempt: Let's try to calculate first: Now i use a known special limit: Now let's use the trick of my teacher and let's hope that the remaining limit exists otherwise we are at the starting point(this is also why sometimes i'm a bit unsure doing this it feels like a bet): And now i'm stuck because I see many useful special limits that i could apply but it always come to a form where i can't apply the ""trick"". Sometimes I feel i'm overcomplicating everything by being too formal but I really want to understand why I can apply something and I don't want to make it become an automatism before i totally understood it.","\lim_{x \to 0^+} \frac{\tan^3((1+x^{\frac 23})^\frac13-1)+\ln(1+\sin^2(x))}{\arctan^2(3x)+5^{x^4}-1}(\sqrt{\frac{1+x+x^2}{x^2}}-\frac 1x) +\infty\cdot a=+\infty, a>0 \lim_\limits{x \to x_0} s(x)c(x) s c  \lim_\limits{x \to x_0} s(x)c(x)=\lim_\limits{x \to x_0} s(x)\lim_\limits{x \to x_0} c(x) \lim_\limits{x \to x_0} c(x)\in \mathbb{R} \lim_\limits{x \to x_0} c(x)\in \pm \infty \not\exists \lim_\limits{x \to x_0} c(x) \lim_{x \to 0^+} \frac{\tan^3((1+x^{\frac 23})^\frac13-1)+\ln(1+\sin^2(x))}{\arctan^2(3x)+5^{x^4}-1}(\sqrt{\frac{1+x+x^2}{x^2}}-\frac 1x) \lim_{x \to 0^+} \sqrt{\frac{1+x+x^2}{x^2}}-\frac 1x=\lim_{x \to 0^+} \frac{\sqrt{1+x+x^2}-1}{x}=\lim_{x \to 0^+} \frac{\sqrt{1+x+x^2}-1}{x+x^2}(x+1) \lim_{x \to 0^+} \frac{x+1}{2}=\frac 12 \frac 12\lim_{x \to 0^+} \frac{\tan^3((1+x^{\frac 23})^\frac13-1)+\ln(1+\sin^2(x))}{\arctan^2(3x)+5^{x^4}-1} 0 \cdot \infty","['limits', 'analysis', 'proof-writing', 'limits-without-lhopital']"
52,Solve $\lim_{x\to 0^{+}}x^{x^x-1}$ [duplicate],Solve  [duplicate],\lim_{x\to 0^{+}}x^{x^x-1},"This question already has answers here : Limits using L'Hopital's rule $\lim_{x\to0^+} (x^{x^x-1})$ (4 answers) Closed 4 years ago . $$\lim_{x\to 0^{+}}x^{x^x-1}$$ Note:- I have solved this problem and below is the solution but I am searching for the better approach My attempt is as follows:- $x^x$ is indeterminate form $(0^0)$ as $x$ tends to $0^{+}$ $$\lim_{x\to 0^{+}}x^{x^x-1}=e^{\lim_{x\to 0^{+}}\left(x^x-1\right)ln(x)}\tag{1}$$ Let's assume $\lim_{x\to 0^{+}}\left(x^x-1\right)ln(x)=y$ $$y=\lim_{x\to 0^{+}}\left(e^{x\ln x}-1\right)\ln x\tag{2}$$ As $x$ tends to $0^{+}$ , $x\ln x$ is the indeterminate form $(0\cdot\infty)$ So first let's see what this indeterminate form actually tends to $$z=\lim_{x\to 0^{+}}x\ln x$$ Assume $t=\ln x$ $$z=\lim_{t\to -\infty}e^tt$$ $$z=\lim_{t\to -\infty}\dfrac{t}{e^{-t}}$$ So we are getting $\dfrac{-\infty}{\infty}$ and we can apply L's Hospital rule here $$z=\lim_{t\to -\infty}\dfrac{1}{-e^{-t}}$$ $$z=0$$ So going back to equation $(1)$ , now we can say as $x$ tends to $0^{+}$ , $x\ln x$ tends to $0$ $$y=\lim_{x\to 0^{+}}\left(e^{x\ln x}-1\right)\ln x\tag{3}$$ $$y=\lim_{x\to 0^{+}}\dfrac{\left(e^{x\ln x}-1\right)}{x\ln x}\cdot x\ln^2 x$$ $$y=\lim_{x\to 0^{+}}\dfrac{\left(e^{x\ln x}-1\right)}{x\ln x}\cdot x\ln^2x$$ $$y=\lim_{x\to 0^{+}}x\ln^2x$$ $$y=\lim_{x\to 0^{+}}\dfrac{ln^2x}{\dfrac{1}{x}}$$ So we have $\dfrac{\infty}{\infty}$ form here and so we can apply L's hospital rule here $$y=\lim_{x\to 0^{+}}\dfrac{2\ln x\cdot\dfrac{1}{x}}{\dfrac{-1}{x^2}}$$ $$y=\lim_{x\to 0^{+}}\dfrac{2\ln x}{\dfrac{-1}{x}}$$ Again applying L's hospital rule here as we have $\dfrac{\infty}{\infty}$ form $$y=\lim_{x\to 0^{+}}\dfrac{\dfrac{2}{x}}{\dfrac{1}{x^2}}$$ $$y=\lim_{x\to 0^{+}}2x$$ $$y=0$$ Putting value of $y$ back in equation $(1)$ $$\lim_{x\to 0^{+}}x^{x^x-1}=1$$ So lots of ups and down in this problem, can it solved by any simpler means?","This question already has answers here : Limits using L'Hopital's rule $\lim_{x\to0^+} (x^{x^x-1})$ (4 answers) Closed 4 years ago . Note:- I have solved this problem and below is the solution but I am searching for the better approach My attempt is as follows:- is indeterminate form as tends to Let's assume As tends to , is the indeterminate form So first let's see what this indeterminate form actually tends to Assume So we are getting and we can apply L's Hospital rule here So going back to equation , now we can say as tends to , tends to So we have form here and so we can apply L's hospital rule here Again applying L's hospital rule here as we have form Putting value of back in equation So lots of ups and down in this problem, can it solved by any simpler means?",\lim_{x\to 0^{+}}x^{x^x-1} x^x (0^0) x 0^{+} \lim_{x\to 0^{+}}x^{x^x-1}=e^{\lim_{x\to 0^{+}}\left(x^x-1\right)ln(x)}\tag{1} \lim_{x\to 0^{+}}\left(x^x-1\right)ln(x)=y y=\lim_{x\to 0^{+}}\left(e^{x\ln x}-1\right)\ln x\tag{2} x 0^{+} x\ln x (0\cdot\infty) z=\lim_{x\to 0^{+}}x\ln x t=\ln x z=\lim_{t\to -\infty}e^tt z=\lim_{t\to -\infty}\dfrac{t}{e^{-t}} \dfrac{-\infty}{\infty} z=\lim_{t\to -\infty}\dfrac{1}{-e^{-t}} z=0 (1) x 0^{+} x\ln x 0 y=\lim_{x\to 0^{+}}\left(e^{x\ln x}-1\right)\ln x\tag{3} y=\lim_{x\to 0^{+}}\dfrac{\left(e^{x\ln x}-1\right)}{x\ln x}\cdot x\ln^2 x y=\lim_{x\to 0^{+}}\dfrac{\left(e^{x\ln x}-1\right)}{x\ln x}\cdot x\ln^2x y=\lim_{x\to 0^{+}}x\ln^2x y=\lim_{x\to 0^{+}}\dfrac{ln^2x}{\dfrac{1}{x}} \dfrac{\infty}{\infty} y=\lim_{x\to 0^{+}}\dfrac{2\ln x\cdot\dfrac{1}{x}}{\dfrac{-1}{x^2}} y=\lim_{x\to 0^{+}}\dfrac{2\ln x}{\dfrac{-1}{x}} \dfrac{\infty}{\infty} y=\lim_{x\to 0^{+}}\dfrac{\dfrac{2}{x}}{\dfrac{1}{x^2}} y=\lim_{x\to 0^{+}}2x y=0 y (1) \lim_{x\to 0^{+}}x^{x^x-1}=1,"['calculus', 'limits', 'exponential-function']"
53,Is it necessarily true that $\lim\limits_{x \rightarrow \infty} f(x) = 0$?,Is it necessarily true that ?,\lim\limits_{x \rightarrow \infty} f(x) = 0,Suppose $f : \Bbb R \longrightarrow \Bbb R$ be a continuous function such that $\int\limits_{0}^{\infty} f(x)\ dx$ exists finitely. If $\lim\limits_{x \rightarrow \infty} f(x)$ also exists finitely then is it necessarily true that $\lim\limits_{x \rightarrow \infty} f(x) = 0$ ? Any suggestion regarding this will be highly appreciated. Thank you very much for your valuable time.,Suppose be a continuous function such that exists finitely. If also exists finitely then is it necessarily true that ? Any suggestion regarding this will be highly appreciated. Thank you very much for your valuable time.,f : \Bbb R \longrightarrow \Bbb R \int\limits_{0}^{\infty} f(x)\ dx \lim\limits_{x \rightarrow \infty} f(x) \lim\limits_{x \rightarrow \infty} f(x) = 0,"['real-analysis', 'limits']"
54,Limit comparison Test for series 1/(n^2 * log n ) converge or diverge,Limit comparison Test for series 1/(n^2 * log n ) converge or diverge,,"Doubt  - used inequality, $1/(n^2 \log n) < 1/ n^2$ will be true only for $n > 10$ as $\log n < 1$ for $1 < n < 10 $ . but here summation is running from $n=  2$ to infinite  please check whether the solution which is arrived is correct with the correct procedure.","Doubt  - used inequality, will be true only for as for . but here summation is running from to infinite  please check whether the solution which is arrived is correct with the correct procedure.",1/(n^2 \log n) < 1/ n^2 n > 10 \log n < 1 1 < n < 10  n=  2,['limits']
55,Application of L'Hospital's Rule on the definition of a derivative.,Application of L'Hospital's Rule on the definition of a derivative.,,"I'm currently taking an introduction to Calculus course and I've come across the following identity: How would one come up with this? My best guess is using L'Hospital's Rule on $$\lim_{x\rightarrow a}{\frac{f(x)-f(a)}{x-a}}$$ but I'm not very sure how, since differentiating both the numerator and denominator merely yields $$\lim_{x\rightarrow a}{f'(x)} = f'(a)$$","I'm currently taking an introduction to Calculus course and I've come across the following identity: How would one come up with this? My best guess is using L'Hospital's Rule on but I'm not very sure how, since differentiating both the numerator and denominator merely yields",\lim_{x\rightarrow a}{\frac{f(x)-f(a)}{x-a}} \lim_{x\rightarrow a}{f'(x)} = f'(a),"['real-analysis', 'limits']"
56,Give $f:\mathbb{R} \rightarrow \mathbb{R}$ such that $|f'(x)|<1$ $ f(x) \neq x$ for all $x \in \mathbb{R}$,Give  such that   for all,f:\mathbb{R} \rightarrow \mathbb{R} |f'(x)|<1  f(x) \neq x x \in \mathbb{R},"Problem Give a function $f:\mathbb{R} \rightarrow \mathbb{R}$ , $C^\infty$ such that $1) |f'(x)|<1$ $2) f(x) \neq x$ for all $x \in \mathbb{R}$ My ideia The idea is to get a function that tends asymptotically to $y=x$ . If you reduce the condition to $|f'(x)|\le 1$ , it's easy but I don't know how to proceed with the condition $|f'(x)|<1$ .","Problem Give a function , such that for all My ideia The idea is to get a function that tends asymptotically to . If you reduce the condition to , it's easy but I don't know how to proceed with the condition .",f:\mathbb{R} \rightarrow \mathbb{R} C^\infty 1) |f'(x)|<1 2) f(x) \neq x x \in \mathbb{R} y=x |f'(x)|\le 1 |f'(x)|<1,"['real-analysis', 'calculus', 'limits', 'analysis', 'functions']"
57,Consider $\sum_{n=1}^\infty(n^{1/n}-1)^a$. Find out all the values of $a$ for which this converges.,Consider . Find out all the values of  for which this converges.,\sum_{n=1}^\infty(n^{1/n}-1)^a a,"Consider $$\sum_{n=1}^\infty(n^{1/n}-1)^a$$ Find out all the values of $a$ for which this converges. Let me include my try: First observation, we know that $\operatorname{log}x \leq x-1$ . Now putting $x=n^{\frac1n}$ we get $\operatorname{log}n^{\frac1n} \leq n^{\frac1n}-1 \Rightarrow \frac1n \operatorname{log} n \leq n^{\frac1n}-1\Rightarrow \operatorname{log}n \leq n(n^{\frac1n}-1)\Rightarrow 1 \leq \operatorname{log}n \leq n(n^{\frac1n}-1)\text{ for $n>3$}\Rightarrow \frac1n \leq \frac {\operatorname{log}n} n\leq (n^{\frac1n}-1)\text{ for $n>3$}\Rightarrow (\frac1n)^a \leq (\frac {\operatorname{log}n} n)^a\leq (n^{\frac1n}-1)^a\text{ for $n>3$}$ Now $\sum_{n=1}^\infty (\frac1n)^a$ diverges for all $a \leq 1$ so we get $\sum_{n=1}^\infty(n^{\frac1n}-1)^a$ diverges for all $a \leq 1$ . Second observation, $\lim_{x \to 1} \frac{\operatorname{log}x}{ x-1}=1$ . So putting $x=n^{\frac1n}$ and observing that $x \to 1$ if $n \to \infty$ , we get $\lim_{n \to \infty} \frac{\operatorname{log}n^{\frac1n}}{ n^{\frac1n}-1}=1 \Rightarrow \lim_{n \to \infty} (\frac{\operatorname{log}n^{\frac1n}}{ n^{\frac1n}-1})^a=1 $ . So, $\sum_{n=1}^\infty(n^{1/n}-1)^a$ and $\sum_{n=1}^\infty(\operatorname{log}n^{\frac1n})^a$ converge and diverge simultaneously. So we might find a condition for what values of $a$ , $\sum_{n=1}^\infty(\operatorname{log}n^{\frac1n})^a$ converges!","Consider Find out all the values of for which this converges. Let me include my try: First observation, we know that . Now putting we get Now diverges for all so we get diverges for all . Second observation, . So putting and observing that if , we get . So, and converge and diverge simultaneously. So we might find a condition for what values of , converges!",\sum_{n=1}^\infty(n^{1/n}-1)^a a \operatorname{log}x \leq x-1 x=n^{\frac1n} \operatorname{log}n^{\frac1n} \leq n^{\frac1n}-1 \Rightarrow \frac1n \operatorname{log} n \leq n^{\frac1n}-1\Rightarrow \operatorname{log}n \leq n(n^{\frac1n}-1)\Rightarrow 1 \leq \operatorname{log}n \leq n(n^{\frac1n}-1)\text{ for n>3}\Rightarrow \frac1n \leq \frac {\operatorname{log}n} n\leq (n^{\frac1n}-1)\text{ for n>3}\Rightarrow (\frac1n)^a \leq (\frac {\operatorname{log}n} n)^a\leq (n^{\frac1n}-1)^a\text{ for n>3} \sum_{n=1}^\infty (\frac1n)^a a \leq 1 \sum_{n=1}^\infty(n^{\frac1n}-1)^a a \leq 1 \lim_{x \to 1} \frac{\operatorname{log}x}{ x-1}=1 x=n^{\frac1n} x \to 1 n \to \infty \lim_{n \to \infty} \frac{\operatorname{log}n^{\frac1n}}{ n^{\frac1n}-1}=1 \Rightarrow \lim_{n \to \infty} (\frac{\operatorname{log}n^{\frac1n}}{ n^{\frac1n}-1})^a=1  \sum_{n=1}^\infty(n^{1/n}-1)^a \sum_{n=1}^\infty(\operatorname{log}n^{\frac1n})^a a \sum_{n=1}^\infty(\operatorname{log}n^{\frac1n})^a,"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'contest-math']"
58,Explore whether or not the sequence converges uniformly,Explore whether or not the sequence converges uniformly,,"Explore whether or not the sequence converges uniformly on $E = [0, 2] $ . $$f_n(x) = \sqrt[n]{1+x^n} $$ I tried to find $\displaystyle f(x) = \lim_{n \to \infty} \sqrt[n]{1+x^n} = \lim_{n \to \infty} e^{\frac1n \cdot \ln(1+x^n)}$ and stuck here. At first glance I thought the limit may depend on whether $x < 1$ or $x > 1$ but I'm not sure about that. I would appreciate any help",Explore whether or not the sequence converges uniformly on . I tried to find and stuck here. At first glance I thought the limit may depend on whether or but I'm not sure about that. I would appreciate any help,"E = [0, 2]  f_n(x) = \sqrt[n]{1+x^n}  \displaystyle f(x) = \lim_{n \to \infty} \sqrt[n]{1+x^n} = \lim_{n \to \infty} e^{\frac1n \cdot \ln(1+x^n)} x < 1 x > 1","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'uniform-convergence']"
59,How to prove $\lim\limits_{n \to \infty}\sin 2^n \neq 0$?,How to prove ?,\lim\limits_{n \to \infty}\sin 2^n \neq 0,"We can prove $\{\sin n\}$ diverge. We can also prove $\{\sin n^2\}$ diverge. But can we prove $\{\sin 2^n\}$ diverge? I tried the same methods like the former two, but failed. Can anyone help?","We can prove diverge. We can also prove diverge. But can we prove diverge? I tried the same methods like the former two, but failed. Can anyone help?",\{\sin n\} \{\sin n^2\} \{\sin 2^n\},"['calculus', 'limits']"
60,"Studying continuity of $f(x,y)=\frac{x+y}{x-y}$",Studying continuity of,"f(x,y)=\frac{x+y}{x-y}","Let $$f(x,y)=\left\{ \begin{array}{ll}       \frac{x+y}{x-y} & , x \neq y \\       1 & , x=y \\ \end{array}  \right.$$ I want to see whether $f(x,y)$ is continuous at points $\{(x,y)\in \Bbb{R}^2:x=y\}$ . In understand I have to compute a limit, but I am not sure how to do it (not sure about notation either). I see that we need $x \to y$ somehow, but these are both variables here. How to deal with this? Is something like \begin{equation} \lim_{(x,y) \to (x,x)} \frac{x+y}{x-y} \end{equation} correct? Does this restrict how we approach the function in any way? What about defining $x=y+\varepsilon$ and then studying $\varepsilon \to 0$ ?","Let I want to see whether is continuous at points . In understand I have to compute a limit, but I am not sure how to do it (not sure about notation either). I see that we need somehow, but these are both variables here. How to deal with this? Is something like correct? Does this restrict how we approach the function in any way? What about defining and then studying ?","f(x,y)=\left\{
\begin{array}{ll}
      \frac{x+y}{x-y} & , x \neq y \\
      1 & , x=y \\
\end{array} 
\right. f(x,y) \{(x,y)\in \Bbb{R}^2:x=y\} x \to y \begin{equation}
\lim_{(x,y) \to (x,x)} \frac{x+y}{x-y}
\end{equation} x=y+\varepsilon \varepsilon \to 0","['calculus', 'limits', 'multivariable-calculus', 'continuity']"
61,Prove $\lim_{n \to \infty} |\cos (nx)|^{\frac{1}{n} }= 1$ for almost every $x \in \mathbb{R}$,Prove  for almost every,\lim_{n \to \infty} |\cos (nx)|^{\frac{1}{n} }= 1 x \in \mathbb{R},"don't know how to proceed with this.  my thought: if there exist some $n_0, k_0 \in \mathbb{Z}, x_0 \in \mathbb{R}$ such that $n_0 x_0= \frac{\pi}{2}+k_0\pi $ , then there exist countable sequence $\{(n_i,k_i)\}$ such that $n_i x_0= \frac{\pi}{2}+k_i\pi $ , so the limit doesn't exist at $x_0$","don't know how to proceed with this.  my thought: if there exist some such that , then there exist countable sequence such that , so the limit doesn't exist at","n_0, k_0 \in \mathbb{Z}, x_0 \in \mathbb{R} n_0 x_0= \frac{\pi}{2}+k_0\pi  \{(n_i,k_i)\} n_i x_0= \frac{\pi}{2}+k_i\pi  x_0","['real-analysis', 'limits', 'measure-theory']"
62,Conditions to exploit Polar coordinates in limits.,Conditions to exploit Polar coordinates in limits.,,"Evaluate, $$\lim_{(x,y)\rightarrow(0,0)}f(x,y)=\lim_{(x,y)\rightarrow(0,0)}\dfrac{2x^2y}{x^4+y^2}$$ When I used polar coordinates with $x=r\cos\theta, y=r\sin\theta$ , $$\lim_{r\rightarrow0}\dfrac{r\cos\theta\sin2\theta}{r^2\cos^4\theta+\sin^2\theta}=0$$ But when I use path $y=x^2$ , $$\lim_{(x,y)\rightarrow(0,0)}\dfrac{2x^4}{2x^4}=1$$ Also from path $x=0$ or $y=0$ both gives, $$\lim_{(x,y)\rightarrow(0,0)}\dfrac{2x^2y}{x^4+y^2}=0$$ From path knowledge, I can say Limit does not exist. Why this occurred that I got two different values of limits from Polar and the path makes me put a question that when to employ polar coordinates method to compute limits? When can I ascertain that it gives the correct value? Why is it giving out the value $0$ even when limit DNE? Please help!","Evaluate, When I used polar coordinates with , But when I use path , Also from path or both gives, From path knowledge, I can say Limit does not exist. Why this occurred that I got two different values of limits from Polar and the path makes me put a question that when to employ polar coordinates method to compute limits? When can I ascertain that it gives the correct value? Why is it giving out the value even when limit DNE? Please help!","\lim_{(x,y)\rightarrow(0,0)}f(x,y)=\lim_{(x,y)\rightarrow(0,0)}\dfrac{2x^2y}{x^4+y^2} x=r\cos\theta, y=r\sin\theta \lim_{r\rightarrow0}\dfrac{r\cos\theta\sin2\theta}{r^2\cos^4\theta+\sin^2\theta}=0 y=x^2 \lim_{(x,y)\rightarrow(0,0)}\dfrac{2x^4}{2x^4}=1 x=0 y=0 \lim_{(x,y)\rightarrow(0,0)}\dfrac{2x^2y}{x^4+y^2}=0 0","['limits', 'multivariable-calculus', 'polar-coordinates']"
63,I don't understand the logical leap made in the analogy of $e$,I don't understand the logical leap made in the analogy of,e,"$e$ is often explained in terms of compound interest. If I found a bank that gave me 100% annual compound interest, then if I put in £1.00, at the end of the year, I would have £2.00. If I were more savvy, and instead asked for 50% interest paid biannually, then I would end up with more – £2.25 to be exact. (This is because 50% of £1.50 > 50% of £1 – simple interest, rather than compound interest, would still only give me £2.00.) $e$ appears to be the logical extreme of this idea: of taking $\frac{100%}{n}$ % interest $n$ times per year. I understand it as the limit of $(1+1/n)$ as $n$ tends to infinity. When the analogy starts to break down for me is when it is therefore concluded that you can take the interest infinitely/continually often. Obviously, this is conceptually harder already, because of the introduction of infinity. However, it is also seems to beg the question ""what is the interest rate?"". If it is 0%, then the £1.00 will never increase, but any more than 0%, and then the individual interest rates would no longer add up to 100%. Is it some kind of infinitesimal? To illustrate my wariness, I have this example from the wikipedia article on limits ( https://en.wikipedia.org/wiki/Limit_(mathematics) ): $$f(x)=\frac{x^2-1}{x-1}$$ As $x$ gets arbitrarily close to 1, $f(x)$ approaches 2, no matter which side you approach 1 from. However, $f(1)$ is undefined as involves division by zero. Similarly, as $n$ tends to infinity in the $e$ analogy, the growth rate becomes arbitrarily close to $e$ . But I don't see how this means that when $n=\infty$ , the growth rate is necessarily $e$ . After all, if you plug $n=\infty$ into the normal formula $(1+1/n)^n$ , it seems that it breaks down (forgive me if you cannot use infinity in this way).","is often explained in terms of compound interest. If I found a bank that gave me 100% annual compound interest, then if I put in £1.00, at the end of the year, I would have £2.00. If I were more savvy, and instead asked for 50% interest paid biannually, then I would end up with more – £2.25 to be exact. (This is because 50% of £1.50 > 50% of £1 – simple interest, rather than compound interest, would still only give me £2.00.) appears to be the logical extreme of this idea: of taking % interest times per year. I understand it as the limit of as tends to infinity. When the analogy starts to break down for me is when it is therefore concluded that you can take the interest infinitely/continually often. Obviously, this is conceptually harder already, because of the introduction of infinity. However, it is also seems to beg the question ""what is the interest rate?"". If it is 0%, then the £1.00 will never increase, but any more than 0%, and then the individual interest rates would no longer add up to 100%. Is it some kind of infinitesimal? To illustrate my wariness, I have this example from the wikipedia article on limits ( https://en.wikipedia.org/wiki/Limit_(mathematics) ): As gets arbitrarily close to 1, approaches 2, no matter which side you approach 1 from. However, is undefined as involves division by zero. Similarly, as tends to infinity in the analogy, the growth rate becomes arbitrarily close to . But I don't see how this means that when , the growth rate is necessarily . After all, if you plug into the normal formula , it seems that it breaks down (forgive me if you cannot use infinity in this way).",e e \frac{100%}{n} n (1+1/n) n f(x)=\frac{x^2-1}{x-1} x f(x) f(1) n e e n=\infty e n=\infty (1+1/n)^n,"['limits', 'exponential-function', 'infinity']"
64,How do I evaluate $\lim _{x\to 0}\left(\frac{x-\sin x}{x\sin x}\right)$ without using L'Hopital or series?,How do I evaluate  without using L'Hopital or series?,\lim _{x\to 0}\left(\frac{x-\sin x}{x\sin x}\right),"How do I evaluate $\lim _{x\to 0}\left(\frac{x-\sin x}{x\sin x}\right)$ without using L'Hopital or series? I've tried expanding the variable such as $x = 2y$ or $x = 3y$ , but seemed to still get stuck.","How do I evaluate without using L'Hopital or series? I've tried expanding the variable such as or , but seemed to still get stuck.",\lim _{x\to 0}\left(\frac{x-\sin x}{x\sin x}\right) x = 2y x = 3y,"['calculus', 'limits', 'limits-without-lhopital']"
65,Evaluate $\lim_{n\to\infty} [{1\over kn}+{1\over k(n+1)}+{1\over k(n+2)}+\cdots+{1\over k(n+p-k)}]$ where $k<p$,Evaluate  where,\lim_{n\to\infty} [{1\over kn}+{1\over k(n+1)}+{1\over k(n+2)}+\cdots+{1\over k(n+p-k)}] k<p,"There are four options. Which of them is correct?- (a) ${p\over k}$ (b) ${k\over p}$ (c) $\log({p\over k})$ (d) ${\log p\over k}$ I somehow want to use the rule for evaluating this kind of sum using integration, i.e. $\int_{0}^{1} f(x) \ dx=\lim_{n\to\infty} {1\over n}\sum_{r=1}^{n}f({r\over n})$ But the given expression $\lim_{n\to\infty} [{1\over kn}+{1\over k(n+1)}+{1\over k(n+2)}+\cdots+{1\over k(n+p-k)}]={1\over k}\lim_{n\to\infty}{1\over n}\sum_{r=0}^{p-k}\frac{1}{1+{r\over n}}$ , sum running from $0$ to $p-k$ instead of $n$ . So, how to evaluate this limit? Thanks for assistance in advance.","There are four options. Which of them is correct?- (a) (b) (c) (d) I somehow want to use the rule for evaluating this kind of sum using integration, i.e. But the given expression , sum running from to instead of . So, how to evaluate this limit? Thanks for assistance in advance.",{p\over k} {k\over p} \log({p\over k}) {\log p\over k} \int_{0}^{1} f(x) \ dx=\lim_{n\to\infty} {1\over n}\sum_{r=1}^{n}f({r\over n}) \lim_{n\to\infty} [{1\over kn}+{1\over k(n+1)}+{1\over k(n+2)}+\cdots+{1\over k(n+p-k)}]={1\over k}\lim_{n\to\infty}{1\over n}\sum_{r=0}^{p-k}\frac{1}{1+{r\over n}} 0 p-k n,"['real-analysis', 'sequences-and-series', 'limits', 'definite-integrals']"
66,Mistake in (french) wikipedia on definition of limit?,Mistake in (french) wikipedia on definition of limit?,,"In the french wikipedia they say that (for $f:U\to \mathbb R$ where $U$ open) that $\displaystyle \lim_{x\to a}f(x)= \ell$ (where $a\in U$ ) if $$\forall \varepsilon >0, \exists \delta >0: \forall x\in U, |x-a|<\delta \implies |f(x)-L|<\varepsilon .$$ Isn't it wrong ? For example, take $\displaystyle f(x)=\boldsymbol 1_{\{0\}}(x)$ . Then, $\lim_{x\to 0}f(x)=0$ , but according to the french wikipedia definition, it doesn't converges to $0$ since if $\varepsilon <1$ , if $\delta >0$ , then for $x=0$ , we have that $|x-0|<\delta $ , but $|f(x)-0|=1>\varepsilon $ . Is this a mistakes or is it a more general definition that is more restrictive ?","In the french wikipedia they say that (for where open) that (where ) if Isn't it wrong ? For example, take . Then, , but according to the french wikipedia definition, it doesn't converges to since if , if , then for , we have that , but . Is this a mistakes or is it a more general definition that is more restrictive ?","f:U\to \mathbb R U \displaystyle \lim_{x\to a}f(x)= \ell a\in U \forall \varepsilon >0, \exists \delta >0: \forall x\in U, |x-a|<\delta \implies |f(x)-L|<\varepsilon . \displaystyle f(x)=\boldsymbol 1_{\{0\}}(x) \lim_{x\to 0}f(x)=0 0 \varepsilon <1 \delta >0 x=0 |x-0|<\delta  |f(x)-0|=1>\varepsilon ","['real-analysis', 'limits', 'definition']"
67,Solve $ \lim_{x\to 0}\ (\sqrt {2x+1}\ -\ \sqrt[3]{1-3x})^{x}$ without using L'Hospital,Solve  without using L'Hospital, \lim_{x\to 0}\ (\sqrt {2x+1}\ -\ \sqrt[3]{1-3x})^{x},"I need to solve $$ \lim_{x\to 0}\ (\sqrt {2x+1}\ -\ \sqrt[3]{1-3x})^{x}$$ Please note that I'm first year student and that this can be solved much simpler than in the answers. I tried doing $$\lim_{x\to 0} \ e^{x \cdot \ln\Bigl(\sqrt{2x+1}-1-\left(\sqrt[3]{1-3x}-1\right)\Bigr)}$$ then going with the limit inside the function like this $$\exp\left\{\lim_{x\to0}x \cdot \ln\left[\lim_{x \to 0}\Bigl(\sqrt{2x+1}-1\Bigr) \cdot \lim_{x \to 0} \left(1- \frac{ \sqrt[3]{1-3x}-1\over x }{ \sqrt{2x+1}-1 \over x }\right)\right] \right\}$$ But problem is that although I can solve third limit this way, I get that second limit is 0, which makes that 0 is inside of $\ln$ and thus is incorrect attempt.  Please help, I'm new here, I wan't to contribute back and this is from my university math exam.","I need to solve Please note that I'm first year student and that this can be solved much simpler than in the answers. I tried doing then going with the limit inside the function like this But problem is that although I can solve third limit this way, I get that second limit is 0, which makes that 0 is inside of and thus is incorrect attempt.  Please help, I'm new here, I wan't to contribute back and this is from my university math exam."," \lim_{x\to 0}\ (\sqrt {2x+1}\ -\ \sqrt[3]{1-3x})^{x} \lim_{x\to 0} \ e^{x \cdot \ln\Bigl(\sqrt{2x+1}-1-\left(\sqrt[3]{1-3x}-1\right)\Bigr)} \exp\left\{\lim_{x\to0}x \cdot
\ln\left[\lim_{x \to 0}\Bigl(\sqrt{2x+1}-1\Bigr) \cdot
\lim_{x \to 0} \left(1-
\frac{ \sqrt[3]{1-3x}-1\over x }{ \sqrt{2x+1}-1 \over x }\right)\right] \right\} \ln","['limits', 'limits-without-lhopital']"
68,"Show that $f(x,y) = \frac{x^3+y^3}{x-y}$ when $x \neq y$ is discontinuous at the origin",Show that  when  is discontinuous at the origin,"f(x,y) = \frac{x^3+y^3}{x-y} x \neq y","Show that the following function is discontinuous at the origin $(x,y)=(0,0)$ $$ \begin{equation} f(x,y) =  \begin{cases} \frac{x^3+y^3}{x-y} &x\neq y\\ 0  &x = y \end{cases} \end{equation} $$ Now I am taking $x=my\ (m \neq 1)$ and getting $f(x,y)$ as $$\lim_{(x,y) \to (0,0)}f(x,y)=\lim_{x \to 0}\frac{x^3+m^3x^3}{x-mx} = \lim_{x \to 0}\frac{x^2+m^3x^2}{1-m}$$ which tends to zero showing that the function is continuous. But we need to show discontinuance, I am not getting any other substitution for $y$ that will prove discontinuity.","Show that the following function is discontinuous at the origin Now I am taking and getting as which tends to zero showing that the function is continuous. But we need to show discontinuance, I am not getting any other substitution for that will prove discontinuity.","(x,y)=(0,0) 
\begin{equation}
f(x,y) = 
\begin{cases}
\frac{x^3+y^3}{x-y} &x\neq y\\
0  &x = y
\end{cases}
\end{equation}
 x=my\ (m \neq 1) f(x,y) \lim_{(x,y) \to (0,0)}f(x,y)=\lim_{x \to 0}\frac{x^3+m^3x^3}{x-mx} = \lim_{x \to 0}\frac{x^2+m^3x^2}{1-m} y","['limits', 'multivariable-calculus', 'continuity']"
69,How to show that this multivariable limit exists or not?,How to show that this multivariable limit exists or not?,,"I was a given a problem to solve (if the limit exists): $$ \lim_{(x,y)\to(0,0)} \cos\left(\frac{x^3-y^3}{x^2+y^2}\right) .$$ My Approach: Approaching along the path $y=0$ , yield the limit to be $1$ . Similarly, going along the path $y=mx$ yields the limit to be $1$ . Thus, it seems that if the limit does exist, the limiting value has to be $1$ . To show that it exists, I decided to use the $\epsilon - \delta$ approach (I am unable to think of an algebraic approach). It will be sufficient to check whether the limit: $$ \lim_{(x,y)\to(0,0)} \frac{x^3-y^3}{x^2+y^2}  $$ exists or not and if it does exist it should be equal to $0$ . We know that for some $\epsilon \in \mathbb{R}$ , $$ 0< \left|\frac{x^3-y^3}{x^2+y^2}\right| < \epsilon,$$ we must be able to choose a $\delta$ such that $0<\sqrt{x^2+y^2} < \delta$ for the limit to exist. Now, $$ \left|\frac{x^3-y^3}{x^2+y^2}\right| = \left| \frac{(x-y)(x^2+y^2+xy)}{x^2+y^2} \right| < \frac{|(x-y)||xy|}{x^2+y^2} \leq \frac{|(x-y)|}{2} < |x-y| = \sqrt{x^2+y^2-2xy}.$$ We have 2 cases: Case I: $xy>0$ This will result in , $$ \left|\frac{x^3-y^3}{x^2+y^2}\right| < \sqrt{x^2+y^2}< \epsilon$$ Here $\delta$ can be chosen as $\epsilon$ . Case II: $xy<0$ . This is where I am not able to proceed further. Is there any alternative approach? Or can the case II part be proven?","I was a given a problem to solve (if the limit exists): My Approach: Approaching along the path , yield the limit to be . Similarly, going along the path yields the limit to be . Thus, it seems that if the limit does exist, the limiting value has to be . To show that it exists, I decided to use the approach (I am unable to think of an algebraic approach). It will be sufficient to check whether the limit: exists or not and if it does exist it should be equal to . We know that for some , we must be able to choose a such that for the limit to exist. Now, We have 2 cases: Case I: This will result in , Here can be chosen as . Case II: . This is where I am not able to proceed further. Is there any alternative approach? Or can the case II part be proven?"," \lim_{(x,y)\to(0,0)} \cos\left(\frac{x^3-y^3}{x^2+y^2}\right) . y=0 1 y=mx 1 1 \epsilon - \delta  \lim_{(x,y)\to(0,0)} \frac{x^3-y^3}{x^2+y^2}   0 \epsilon \in \mathbb{R}  0< \left|\frac{x^3-y^3}{x^2+y^2}\right| < \epsilon, \delta 0<\sqrt{x^2+y^2} < \delta  \left|\frac{x^3-y^3}{x^2+y^2}\right| = \left| \frac{(x-y)(x^2+y^2+xy)}{x^2+y^2} \right| < \frac{|(x-y)||xy|}{x^2+y^2} \leq \frac{|(x-y)|}{2} < |x-y| = \sqrt{x^2+y^2-2xy}. xy>0  \left|\frac{x^3-y^3}{x^2+y^2}\right| < \sqrt{x^2+y^2}< \epsilon \delta \epsilon xy<0","['limits', 'multivariable-calculus']"
70,Derivation of Gumbel Distribution,Derivation of Gumbel Distribution,,The standard generalised extreme value (GEV) distribution is given by $H_{\xi}$ which is $exp(-(1+\xi x)^{-1/\xi}$ if $\xi<>0$ and $exp(-e^{-x})$ if $\xi=0$ In the lecture notes it is stated $1-H_\xi (x)$ approximatle equals $e^{-x}$ for $\xi=0$ for $x_{H_\xi}$ going to infinity which is the Gumbel distribution. $1-H_\xi (x)$ approximatle equals $(\xi x)^{-1/\xi}$ for $\xi=0$ for $x_{H_\xi}$ going to infinity which is the Fréchet distribution. I would like to do the math to derive the Gumbel and the Fréchet from the GEV to understand deriving limits better (I seem to have some deficits). I would be grateful for a solution or a textbook hint. Many thanks.,The standard generalised extreme value (GEV) distribution is given by which is if and if In the lecture notes it is stated approximatle equals for for going to infinity which is the Gumbel distribution. approximatle equals for for going to infinity which is the Fréchet distribution. I would like to do the math to derive the Gumbel and the Fréchet from the GEV to understand deriving limits better (I seem to have some deficits). I would be grateful for a solution or a textbook hint. Many thanks.,H_{\xi} exp(-(1+\xi x)^{-1/\xi} \xi<>0 exp(-e^{-x}) \xi=0 1-H_\xi (x) e^{-x} \xi=0 x_{H_\xi} 1-H_\xi (x) (\xi x)^{-1/\xi} \xi=0 x_{H_\xi},"['probability', 'limits', 'extreme-value-theorem', 'extreme-value-analysis']"
71,Calculating $\lim_{n\rightarrow\infty} e^{-t\sqrt{n}}\left(1-\frac{t}{\sqrt{n}}\right)^{-n}$,Calculating,\lim_{n\rightarrow\infty} e^{-t\sqrt{n}}\left(1-\frac{t}{\sqrt{n}}\right)^{-n},For my probability homework I have to show that a certain limit exists and equals $e^{\frac{1}{2}t^2}$ . The limit in question is $\lim_{n\rightarrow\infty} e^{-t\sqrt{n}}\left(1-\frac{t}{\sqrt{n}}\right)^{-n}$ . I have tried the following simplifications: \begin{align*} 	&\quad\ \text{substitute $m = \sqrt{n}$}\\ 	&= \lim_{m\rightarrow\infty} e^{-tm}\left(1+\frac{-t}{m}\right)^{-m^2}\\ 	&= \lim_{m\rightarrow\infty} e^{-tm}\left(\left(1+\frac{-t}{m}\right)^m\right)^{-m}\\ 	&= \lim_{m\rightarrow\infty} e^{-tm}\left(e^{-t}\right)^{-m}\\ 	&= \lim_{m\rightarrow\infty} e^{-tm+tm}\\ 	&= 1 \end{align*} But according to wolfram alpha during the third equality the outcome changes. Can anyone help me on how to properly calculate this limit?,For my probability homework I have to show that a certain limit exists and equals . The limit in question is . I have tried the following simplifications: But according to wolfram alpha during the third equality the outcome changes. Can anyone help me on how to properly calculate this limit?,"e^{\frac{1}{2}t^2} \lim_{n\rightarrow\infty} e^{-t\sqrt{n}}\left(1-\frac{t}{\sqrt{n}}\right)^{-n} \begin{align*}
	&\quad\ \text{substitute m = \sqrt{n}}\\
	&= \lim_{m\rightarrow\infty} e^{-tm}\left(1+\frac{-t}{m}\right)^{-m^2}\\
	&= \lim_{m\rightarrow\infty} e^{-tm}\left(\left(1+\frac{-t}{m}\right)^m\right)^{-m}\\
	&= \lim_{m\rightarrow\infty} e^{-tm}\left(e^{-t}\right)^{-m}\\
	&= \lim_{m\rightarrow\infty} e^{-tm+tm}\\
	&= 1
\end{align*}",['limits']
72,Understanding Baby Rudin Theorem 3.37,Understanding Baby Rudin Theorem 3.37,,"I am looking for some help with making sense of the proof in Baby Rudin (i.e. ""Principles of mathematical analysis"") for Theorem 3.37, shown below. 3.37 Theorem For any sequence $\{c_n\}$ of positive numbers, $$\liminf_{n \to \infty} \frac{c_{n+1}}{c_n} \leq \liminf_{n \to \infty} \sqrt[n]{c_n} \\ \limsup_{n \to \infty} \sqrt[n]{c_n} \leq \limsup_{n \to \infty} \frac{c_{n+1}}{c_n}.$$ Proof We shall prove the second inequality; the proof of the first is quite similar. Put $$\alpha = \limsup_{n \to \infty} \frac{c_{n+1}}{c_n}.$$ If $\alpha = + \infty$ , there is nothing to prove. If $\alpha$ is finite, choose $\beta > \alpha$ . There is an integer $N$ such that $$\frac{c_{n+1}}{c_n} \leq \beta$$ for $n \geq N$ . In particular, for any $p > 0$ , $$c_{N+k+1} \leq \beta c_{N+k}.$$ Multiplying these inequalities, we obtain $$c_{N+p} \leq \beta^p c_N,$$ or $$c_n \leq c_N \beta^{-N} \cdot \beta^n \quad (n \geq N).$$ Hence $$\sqrt[n]{c_n} \leq \sqrt[n]{c_N \beta^{-N}} \cdot \beta,$$ so that $$\limsup_{n \to \infty} \sqrt[n]{c_n} \leq \beta, \quad \quad (18)$$ by Theorem 3.20(b). Since (18) is true for every $\beta > \alpha$ , we have $$\limsup_{n \to \infty} \sqrt[n]{c_n} \leq \alpha.$$ I am struggling to understand the steps that are taken in the following part of the proof: Multiplying these inequalities, we obtain $$c_{N+p} \leq \beta^p c_N,$$ or $$c_n \leq c_N \beta^{-N} \cdot \beta^n \quad (n \geq N).$$ Specifically, I'm wondering: Where does the inequality $c_{N+p} \leq \beta^p c_N$ come from? Is the author saying that this is the result of multiplying the two preceding inequalities together, like $(\frac{c_{n+1}}{c_n} \cdot c_{N+k+1}) \leq (\beta \cdot \beta c_{N+k})$ ? What is the connection between $c_{N+p} \leq \beta^p c_N$ and the next inequality $c_n \leq c_N \beta^{-N} \beta^n$ ? Any help would be very much appreciated! Side note: I found this question to be really helpful with proving the first inequality.","I am looking for some help with making sense of the proof in Baby Rudin (i.e. ""Principles of mathematical analysis"") for Theorem 3.37, shown below. 3.37 Theorem For any sequence of positive numbers, Proof We shall prove the second inequality; the proof of the first is quite similar. Put If , there is nothing to prove. If is finite, choose . There is an integer such that for . In particular, for any , Multiplying these inequalities, we obtain or Hence so that by Theorem 3.20(b). Since (18) is true for every , we have I am struggling to understand the steps that are taken in the following part of the proof: Multiplying these inequalities, we obtain or Specifically, I'm wondering: Where does the inequality come from? Is the author saying that this is the result of multiplying the two preceding inequalities together, like ? What is the connection between and the next inequality ? Any help would be very much appreciated! Side note: I found this question to be really helpful with proving the first inequality.","\{c_n\} \liminf_{n \to \infty} \frac{c_{n+1}}{c_n} \leq \liminf_{n \to \infty} \sqrt[n]{c_n} \\ \limsup_{n \to \infty} \sqrt[n]{c_n} \leq \limsup_{n \to \infty} \frac{c_{n+1}}{c_n}. \alpha = \limsup_{n \to \infty} \frac{c_{n+1}}{c_n}. \alpha = + \infty \alpha \beta > \alpha N \frac{c_{n+1}}{c_n} \leq \beta n \geq N p > 0 c_{N+k+1} \leq \beta c_{N+k}. c_{N+p} \leq \beta^p c_N, c_n \leq c_N \beta^{-N} \cdot \beta^n \quad (n \geq N). \sqrt[n]{c_n} \leq \sqrt[n]{c_N \beta^{-N}} \cdot \beta, \limsup_{n \to \infty} \sqrt[n]{c_n} \leq \beta, \quad \quad (18) \beta > \alpha \limsup_{n \to \infty} \sqrt[n]{c_n} \leq \alpha. c_{N+p} \leq \beta^p c_N, c_n \leq c_N \beta^{-N} \cdot \beta^n \quad (n \geq N). c_{N+p} \leq \beta^p c_N (\frac{c_{n+1}}{c_n} \cdot c_{N+k+1}) \leq (\beta \cdot \beta c_{N+k}) c_{N+p} \leq \beta^p c_N c_n \leq c_N \beta^{-N} \beta^n","['real-analysis', 'limits']"
73,"Guessing delta value, in epsilon-delta proof","Guessing delta value, in epsilon-delta proof",,"So as I've solved various problems regarding epsilon-delta proof, I have faced several questions where I had to kind of implement this process: So here's one of them: $\lim_{x\rightarrow 1} (x^3+x+1) = 3 $ So I started off saying, Let $\epsilon$ > 0 be given. We want to find $\delta$ > 0 s.t. if 0 < $|x-1|$ < $\delta$ , then $|x^3+x+2| < \epsilon$ . $|x^3+x+2| = |(x-1)(x-1)(x+2)|$ ..... (a) (Here comes the part! ) Let $\delta < 1, $ ... (b) $ |x-1| < 1$ $-1 < x-1 < 1$ $1 < x + 2 < 3$ $x + 2 < 3$ Apply to the part (a), $<(x-1)\times 1 \times 3  < \delta \times 3  = \epsilon$ $\delta = \epsilon/3$ $\delta = min(1, \epsilon/3)$ . and the proof follows... So my question is what is the standard of choosing n in part (b) Let $\delta < n$ ? Sometimes they choose $\frac{1}{2}$ or $\frac{1}{4}$ for n, and sometimes 1. For this case, my answer sheet specifically chose $\frac{1}{2}$ for n and I have no idea why.","So as I've solved various problems regarding epsilon-delta proof, I have faced several questions where I had to kind of implement this process: So here's one of them: So I started off saying, Let > 0 be given. We want to find > 0 s.t. if 0 < < , then . ..... (a) (Here comes the part! ) Let ... (b) Apply to the part (a), . and the proof follows... So my question is what is the standard of choosing n in part (b) Let ? Sometimes they choose or for n, and sometimes 1. For this case, my answer sheet specifically chose for n and I have no idea why.","\lim_{x\rightarrow 1} (x^3+x+1) = 3  \epsilon \delta |x-1| \delta |x^3+x+2| < \epsilon |x^3+x+2| = |(x-1)(x-1)(x+2)| \delta < 1,   |x-1| < 1 -1 < x-1 < 1 1 < x + 2 < 3 x + 2 < 3 <(x-1)\times 1 \times 3  < \delta \times 3
 = \epsilon \delta = \epsilon/3 \delta = min(1, \epsilon/3) \delta < n \frac{1}{2} \frac{1}{4} \frac{1}{2}","['calculus', 'limits', 'epsilon-delta']"
74,multivariable calculus - Proving that this limit exists with polar coordinates,multivariable calculus - Proving that this limit exists with polar coordinates,,"I have the following limit: $\lim \limits_{(x, y) \to (0,0)} \cfrac{x^2y^2}{x^2+y^4}$ According to my book the limit is supposed to exist and be equal to 0, but I'm not sure what I'm doing wrong. First, I try to approach it from two different directions, I'll skip this and say that I tried 3 different directions and got 0 everytime. Now onto polar coordinates: $\lim \limits_{\rho \to 0} \cfrac{\rho cos^2 \theta \rho^2sin^2\theta}{\rho^2cos^2\theta+\rho^4sin^4\theta} = \lim \limits_{\rho \to 0} \cfrac{\rho^4cos^2\theta sin^2\theta}{\rho^2(cos^2\theta+\rho^2sin^4\theta)}$ Simplifying the above expression: $\lim \limits_{\rho \to 0} \cfrac{\rho^2cos^2\theta sin^2\theta}{cos^2\theta+r^2sin^4\theta}$ Now, when $\rho \to 0$ the above function is dependant on theta, and even though the numerator tends to $0$ there's the chance of $\theta = k\pi-\cfrac{\pi}{2}$ meaning that we'd end up with $cos^2\theta = 0$ therefor we'd be in a $0/0$ situation. Because of that, the above limit is dependant on $\theta$ and not $\rho$ , and that means that the function will assume different values on different directions, meaning that the limit does not exist, but according to my textbook the limit is 0. I really can't understand what I'm doing wrong.","I have the following limit: According to my book the limit is supposed to exist and be equal to 0, but I'm not sure what I'm doing wrong. First, I try to approach it from two different directions, I'll skip this and say that I tried 3 different directions and got 0 everytime. Now onto polar coordinates: Simplifying the above expression: Now, when the above function is dependant on theta, and even though the numerator tends to there's the chance of meaning that we'd end up with therefor we'd be in a situation. Because of that, the above limit is dependant on and not , and that means that the function will assume different values on different directions, meaning that the limit does not exist, but according to my textbook the limit is 0. I really can't understand what I'm doing wrong.","\lim \limits_{(x, y) \to (0,0)} \cfrac{x^2y^2}{x^2+y^4} \lim \limits_{\rho \to 0} \cfrac{\rho cos^2 \theta \rho^2sin^2\theta}{\rho^2cos^2\theta+\rho^4sin^4\theta} = \lim \limits_{\rho \to 0} \cfrac{\rho^4cos^2\theta sin^2\theta}{\rho^2(cos^2\theta+\rho^2sin^4\theta)} \lim \limits_{\rho \to 0} \cfrac{\rho^2cos^2\theta sin^2\theta}{cos^2\theta+r^2sin^4\theta} \rho \to 0 0 \theta = k\pi-\cfrac{\pi}{2} cos^2\theta = 0 0/0 \theta \rho","['calculus', 'limits']"
75,$\lim\limits_{n\to\infty}{[x]+[2^2x]+\dots +[n^2x]\over n^3}$,,\lim\limits_{n\to\infty}{[x]+[2^2x]+\dots +[n^2x]\over n^3},"Find the limit, $$\lim_{n\to\infty}{[x]+[2^2x]+\dots +[n^2x]\over n^3}$$ Where $[.]$ denotes the integral part of $.$ ? Efforts: If $x$ is integer, $$\lim_{n\to\infty}{[x]+[2^2x]+\dots +[n^2x]\over n^3}$$ $$\lim_{n\to\infty}{x(1+2^2+\dots +n^2)\over n^3}$$ We know $\sum\limits_{i=1}^ni^2=(n+1)(2n+1)n/6$ Therefore we get that limit is equal to $x/3$ . How to solve it for non integral values. Thanks in advance.","Find the limit, Where denotes the integral part of ? Efforts: If is integer, We know Therefore we get that limit is equal to . How to solve it for non integral values. Thanks in advance.",\lim_{n\to\infty}{[x]+[2^2x]+\dots +[n^2x]\over n^3} [.] . x \lim_{n\to\infty}{[x]+[2^2x]+\dots +[n^2x]\over n^3} \lim_{n\to\infty}{x(1+2^2+\dots +n^2)\over n^3} \sum\limits_{i=1}^ni^2=(n+1)(2n+1)n/6 x/3,"['real-analysis', 'limits']"
76,Evaluate the following limit: $\lim_{x\downarrow 0}\dfrac{1}{x^c(1 - e^x)}$,Evaluate the following limit:,\lim_{x\downarrow 0}\dfrac{1}{x^c(1 - e^x)},"I need to evaluate the following limit: $$ \lim_{x\downarrow 0} \dfrac{(1 - e^x)^{-1}}{x^c} $$ for different values of the constant $c$ . What I've tried thus far: We have that $$ \lim_{x\downarrow 0} \dfrac{(1 - e^x)^{-1}}{x^c} = \lim_{x\downarrow 0}\dfrac{1}{x^c(1 - e^x)} $$ Now I know that $1 - e^x \to 0$ as $x\to 0$ . If $c\geq 0$ then $x^c\to 0$ as $x\to 0$ so we have that $$ \lim_{x\downarrow 0}\dfrac{1}{x^c(1 - e^x)} = \infty $$ What I'm not so sure about is what happens when $c<0$ . If $c< 0$ then we can write $x^c$ as $x^{-\gamma}$ where $\gamma = \left|c\right|$ . I know that $\lim_{x\downarrow 0}x^{-\gamma}\to \infty$ , but I'm not sure whether $x^{-\gamma}\to \infty$ quicker than $1 - e^x\to 0$ as $x\to 0$ . I need to know because right now I'm not sure what the term $x^c(1 - e^x)$ does as $x\downarrow 0$ when $c <0$ . Question: How do I evaluate $$ \lim_{x\downarrow 0}\dfrac{1}{x^c(1 - e^x)} $$ when $c< 0$ ?","I need to evaluate the following limit: for different values of the constant . What I've tried thus far: We have that Now I know that as . If then as so we have that What I'm not so sure about is what happens when . If then we can write as where . I know that , but I'm not sure whether quicker than as . I need to know because right now I'm not sure what the term does as when . Question: How do I evaluate when ?","
\lim_{x\downarrow 0} \dfrac{(1 - e^x)^{-1}}{x^c}
 c 
\lim_{x\downarrow 0} \dfrac{(1 - e^x)^{-1}}{x^c} = \lim_{x\downarrow 0}\dfrac{1}{x^c(1 - e^x)}
 1 - e^x \to 0 x\to 0 c\geq 0 x^c\to 0 x\to 0 
\lim_{x\downarrow 0}\dfrac{1}{x^c(1 - e^x)} = \infty
 c<0 c< 0 x^c x^{-\gamma} \gamma = \left|c\right| \lim_{x\downarrow 0}x^{-\gamma}\to \infty x^{-\gamma}\to \infty 1 - e^x\to 0 x\to 0 x^c(1 - e^x) x\downarrow 0 c <0 
\lim_{x\downarrow 0}\dfrac{1}{x^c(1 - e^x)}
 c< 0","['calculus', 'limits']"
77,L'Hospital's Rule and indeterminate form $\frac{\infty}{-\infty}$,L'Hospital's Rule and indeterminate form,\frac{\infty}{-\infty},"Suppose I have a limit of the form \begin{align*} \lim\limits_{x \to -\infty} \frac{x}{e^{x^2}}. \end{align*} As $x \to -\infty$ , $x \to -\infty$ and $e^{x^2} \to \infty$ . Now, if we were subtracting this limit (suppose, for example, we're evaluating some term in the integration by parts formula from $\infty$ to $-\infty$ ), it doesn't quite matter. We can move constants outside a limit, and can surely move them back in as well. $-x \to \infty$ as $x \to -\infty$ , so that is our $\frac{\infty}{\infty}$ indeterminate form which allows us to apply L'Hospital's rule. But, what if this weren't the case? Is $\frac{\infty}{-\infty}$ an indeterminate form? I suppose I could multiply the limit by $1 = \frac{-1}{-1}$ and pull one of $-1$ 's outside the limit to turn this limit into the form $\frac{\infty}{\infty}$ , though this feels like cheating. I'm really concerned with whether this is valid. I'd appreciate any insights on this.","Suppose I have a limit of the form As , and . Now, if we were subtracting this limit (suppose, for example, we're evaluating some term in the integration by parts formula from to ), it doesn't quite matter. We can move constants outside a limit, and can surely move them back in as well. as , so that is our indeterminate form which allows us to apply L'Hospital's rule. But, what if this weren't the case? Is an indeterminate form? I suppose I could multiply the limit by and pull one of 's outside the limit to turn this limit into the form , though this feels like cheating. I'm really concerned with whether this is valid. I'd appreciate any insights on this.","\begin{align*}
\lim\limits_{x \to -\infty} \frac{x}{e^{x^2}}.
\end{align*} x \to -\infty x \to -\infty e^{x^2} \to \infty \infty -\infty -x \to \infty x \to -\infty \frac{\infty}{\infty} \frac{\infty}{-\infty} 1 = \frac{-1}{-1} -1 \frac{\infty}{\infty}",['calculus']
78,$\lim\limits_{n\to\infty} \prod\limits_{k=1}^{n} \left( 1 + \tan{\frac{k}{n^2}} \right) $,,\lim\limits_{n\to\infty} \prod\limits_{k=1}^{n} \left( 1 + \tan{\frac{k}{n^2}} \right) ,"I want to calculate $$\lim\limits_{n\to\infty} \prod_{k=1}^{n} \left( 1 + \tan{\frac{k}{n^2}} \right) $$ Taking logarithms, it's enough to find $$\lim\limits_{n\to\infty} \sum_{k=1}^{n} \log\left( 1 + \tan{\frac{k}{n^2}} \right).$$ Since $\lim\limits_{n\to\infty} \tan{\frac{x}{n^2}} = 0$ , we can combine the Taylor series near $0$ of $\log(1+x)$ with the taylor series of $\tan{x}$ near $0$ to obtain the limit $e^\frac{1}{2}$ . My question is: is there any nicer way of evaluating this limit?","I want to calculate Taking logarithms, it's enough to find Since , we can combine the Taylor series near of with the taylor series of near to obtain the limit . My question is: is there any nicer way of evaluating this limit?",\lim\limits_{n\to\infty} \prod_{k=1}^{n} \left( 1 + \tan{\frac{k}{n^2}} \right)  \lim\limits_{n\to\infty} \sum_{k=1}^{n} \log\left( 1 + \tan{\frac{k}{n^2}} \right). \lim\limits_{n\to\infty} \tan{\frac{x}{n^2}} = 0 0 \log(1+x) \tan{x} 0 e^\frac{1}{2},"['calculus', 'limits']"
79,"With derivatives, why doesn't '$h$' in the denominator not invalidate the whole function as diving by zero?","With derivatives, why doesn't '' in the denominator not invalidate the whole function as diving by zero?",h,"The ' $h$ ' in the $\frac{df}{dx}$ formula, i.e., $$f'(x) = \lim_{h\to0}\frac{f(x + h) - f(x)}{h}$$ stands for an infinitesimal change, correct? Why isn't this ' $h$ ' equal $0$ ? If $h\neq 1$ either, why can it be overlooked as inconsequential in the denominator? I'm self-learning, stop me if I'm mistaken or if the question has already been asked.","The ' ' in the formula, i.e., stands for an infinitesimal change, correct? Why isn't this ' ' equal ? If either, why can it be overlooked as inconsequential in the denominator? I'm self-learning, stop me if I'm mistaken or if the question has already been asked.",h \frac{df}{dx} f'(x) = \lim_{h\to0}\frac{f(x + h) - f(x)}{h} h 0 h\neq 1,"['calculus', 'limits', 'derivatives']"
80,Proving a function is always nonnegative,Proving a function is always nonnegative,,"Suppose $f : \mathbb{R}^{n} \rightarrow \mathbb{R}$ is continuous and $f(u) > 0$ if the point $u \in \mathbb{R}^{n}$ has at least one   rational component. Prove that $f(u) \geq 0 $ for all $u \in \mathbb{R}^{n}$ . Partial Solution: What remains to be proven is that $f(u) \geq 0$ for every $u \in \mathbb{R}^{n}$ with at all irrational components. Let $v$ be such a vector. Since every real number is the limit point of a rational sequence, we can define sequences $\{r_{1}\}, \{r_{2}\}, \ldots, \{r_{n}\},$ where $\{r_{k}\}$ converges to the $k^{\text{th}}$ component of $v$ . I don't really know how to finish from here. Can someone please help me?","Suppose is continuous and if the point has at least one   rational component. Prove that for all . Partial Solution: What remains to be proven is that for every with at all irrational components. Let be such a vector. Since every real number is the limit point of a rational sequence, we can define sequences where converges to the component of . I don't really know how to finish from here. Can someone please help me?","f : \mathbb{R}^{n} \rightarrow \mathbb{R} f(u) > 0 u \in \mathbb{R}^{n} f(u) \geq 0  u \in
\mathbb{R}^{n} f(u) \geq 0 u \in \mathbb{R}^{n} v \{r_{1}\}, \{r_{2}\}, \ldots, \{r_{n}\}, \{r_{k}\} k^{\text{th}} v","['real-analysis', 'general-topology']"
81,Find $ \lim\limits_{x \rightarrow 0} \frac{\cos(\sqrt{\cot^{-1}(x)})-1}{x} $,Find, \lim\limits_{x \rightarrow 0} \frac{\cos(\sqrt{\cot^{-1}(x)})-1}{x} ,"I want to find that limit $$ \lim_{x \rightarrow 0} \frac{\cos(\sqrt{\cot^{-1}(x)})-1}{x} $$ I can use there L'Hospital's rule, so let's calculate $$ (\cos(\sqrt{\cot^{-1}(x)})-1)' = (\cos(\sqrt{\cot^{-1}(x)})' = \\ (\sqrt{\cot^{-1}(x)})'\cdot \sin(\sqrt{\cot^{-1}(x)}) = \\ \sin(\sqrt{\cot^{-1}(x)} \cdot \frac{1}{1+x^2}\cdot \frac{1}{2\sqrt{\cot^{-1}(x)}} $$ So $$ \lim_{x \rightarrow 0} \frac{\cos(\sqrt{\cot^{-1}(x)})-1}{x} = \\ \lim_{x \rightarrow 0}\sin(\sqrt{\cot^{-1}(x)} \cdot \frac{1}{1+x^2}\cdot \frac{1}{2\sqrt{\cot^{-1}(x)}} = \sin(\sqrt{\pi / 2}) \cdot \frac{1}{2 \sqrt{\pi / 2}}  $$ but wolfram tells that it is $-\infty$","I want to find that limit I can use there L'Hospital's rule, so let's calculate So but wolfram tells that it is"," \lim_{x \rightarrow 0} \frac{\cos(\sqrt{\cot^{-1}(x)})-1}{x}   (\cos(\sqrt{\cot^{-1}(x)})-1)' = (\cos(\sqrt{\cot^{-1}(x)})' = \\
(\sqrt{\cot^{-1}(x)})'\cdot \sin(\sqrt{\cot^{-1}(x)}) = \\
\sin(\sqrt{\cot^{-1}(x)} \cdot \frac{1}{1+x^2}\cdot \frac{1}{2\sqrt{\cot^{-1}(x)}}   \lim_{x \rightarrow 0} \frac{\cos(\sqrt{\cot^{-1}(x)})-1}{x} = \\ \lim_{x \rightarrow 0}\sin(\sqrt{\cot^{-1}(x)} \cdot \frac{1}{1+x^2}\cdot \frac{1}{2\sqrt{\cot^{-1}(x)}} = \sin(\sqrt{\pi / 2}) \cdot \frac{1}{2 \sqrt{\pi / 2}}   -\infty","['real-analysis', 'limits']"
82,Prove that $\lim\limits_{n\rightarrow\infty}\int_{1}^{a}\frac{1}{1+x^{n}}dx=\ln2$,Prove that,\lim\limits_{n\rightarrow\infty}\int_{1}^{a}\frac{1}{1+x^{n}}dx=\ln2,"Prove that $$\lim_{n\rightarrow\infty}\int_{1}^{a}\frac{1}{1+x^{n}}\,\mathrm{d}x=\ln2.$$ We can write $$\frac{1}{1+x^{n}}=1-\frac{x^{n}}{1+x^{n}},$$ so $$\int_{1}^{a}\frac{1}{1+x^{n}}\,\mathrm{d}x=a-1-\int_{1}^{a}\frac{x^n}{1+x^n}\,\mathrm{d}x$$ and $$\int_{1}^{a}\frac{x^n}{1+x^n}\,\mathrm{d}x=\frac{1}{n}x\ln(1+x^n)|_{1}^{a}+\int_{1}^{a}\ln(1+x^n)\,\mathrm{d}x$$ The last can be manipulated to providing a limit using $e^x\geq x+1$ , in our case $x \geq \ln(x+1)$ , $x \geq0$ . However, I need help from this point on or, if the method is faulty, on the problem itself. EDIT: Thanks for clarifications. I see the mistake. However, by changing it into $\lim_{n\rightarrow\infty}n\int_{1}^{a}\frac{1}{1+x^{n}}\,\mathrm{d}x=\ln2.$ I followed Cosmin's duplicate, but I am unable to grasp the proof, as the level is high. Could you give an easier proof for this than one using improper integrals, please?","Prove that We can write so and The last can be manipulated to providing a limit using , in our case , . However, I need help from this point on or, if the method is faulty, on the problem itself. EDIT: Thanks for clarifications. I see the mistake. However, by changing it into I followed Cosmin's duplicate, but I am unable to grasp the proof, as the level is high. Could you give an easier proof for this than one using improper integrals, please?","\lim_{n\rightarrow\infty}\int_{1}^{a}\frac{1}{1+x^{n}}\,\mathrm{d}x=\ln2. \frac{1}{1+x^{n}}=1-\frac{x^{n}}{1+x^{n}}, \int_{1}^{a}\frac{1}{1+x^{n}}\,\mathrm{d}x=a-1-\int_{1}^{a}\frac{x^n}{1+x^n}\,\mathrm{d}x \int_{1}^{a}\frac{x^n}{1+x^n}\,\mathrm{d}x=\frac{1}{n}x\ln(1+x^n)|_{1}^{a}+\int_{1}^{a}\ln(1+x^n)\,\mathrm{d}x e^x\geq x+1 x \geq \ln(x+1) x \geq0 \lim_{n\rightarrow\infty}n\int_{1}^{a}\frac{1}{1+x^{n}}\,\mathrm{d}x=\ln2.","['real-analysis', 'calculus']"
83,"$\lim_{x\to 2}f(x)$ (if it exists), where $f(x)=\begin{cases}x-[x],&x<2\\4,&x=2\\3x-5,&x>2\end{cases}$","(if it exists), where","\lim_{x\to 2}f(x) f(x)=\begin{cases}x-[x],&x<2\\4,&x=2\\3x-5,&x>2\end{cases}","Question : Evaluate $\lim_{x\to 2}f(x)$ (if it exists), where $f(x)=\begin{cases}x-[x],&x<2\\[2ex]4,&x=2\\[2ex]3x-5,&x>2\end{cases}$ [.] denotes the greatest integer function . My attempt: $$\lim_{x \to 2^+} f(x) = 3(2)-5=1$$ $$\lim_{x \to 2^-} f(x) = x - [x] = 2-[2]$$ From this step onwards I am stuck and do not know how to continue. Please help. Thank you!","Question : Evaluate (if it exists), where [.] denotes the greatest integer function . My attempt: From this step onwards I am stuck and do not know how to continue. Please help. Thank you!","\lim_{x\to 2}f(x) f(x)=\begin{cases}x-[x],&x<2\\[2ex]4,&x=2\\[2ex]3x-5,&x>2\end{cases} \lim_{x \to 2^+} f(x) = 3(2)-5=1 \lim_{x \to 2^-} f(x) = x - [x] = 2-[2]","['calculus', 'limits']"
84,"Continuous function and limit $ f(x,y)=\left\{\begin{matrix} (x-y)\sin\frac{1}{x}\sin\frac{1}{y} & , xy\neq 0 \\ 0 &, x=y=0 \end{matrix}\right. $",Continuous function and limit," f(x,y)=\left\{\begin{matrix} (x-y)\sin\frac{1}{x}\sin\frac{1}{y} & , xy\neq 0 \\ 0 &, x=y=0 \end{matrix}\right. ","I have this function: $$ f(x,y)=\left\{\begin{matrix} (x-y)\sin\frac{1}{x}\sin\frac{1}{y} & ,  xy\neq 0 \\ 0  &, x=y=0 \end{matrix}\right. $$ a) Show that $ \lim_{x\rightarrow 0}[\lim_{y\rightarrow 0} f(x,y)] $ does not exist. b) Show that $ f(x,y) $ is continuous at $ (0,0) $ . For (a) I took $ \lim_{y\rightarrow 0} f(x,y) $ and I got that it's equal to $$ x\sin\frac{1}{x}\lim_{y\rightarrow 0}\sin\frac{1}{y} - \sin\frac{1}{x}\lim_{y\rightarrow 0}y \sin\frac{1}{y} $$ , but $$ \lim_{y\rightarrow 0}\sin\frac{1}{y} $$ does not exist, so $ \lim_{y\rightarrow 0} f(x,y) $ does not exist and $ \lim_{x\rightarrow 0}[\lim_{y\rightarrow 0} f(x,y)] $ does not exist. Am I right? For (b) I guess that I have to show that $$ \lim_{(x,y)\rightarrow (0,0)} f(x,y) = 0 $$ How can I do this ?","I have this function: a) Show that does not exist. b) Show that is continuous at . For (a) I took and I got that it's equal to , but does not exist, so does not exist and does not exist. Am I right? For (b) I guess that I have to show that How can I do this ?"," f(x,y)=\left\{\begin{matrix}
(x-y)\sin\frac{1}{x}\sin\frac{1}{y} & ,  xy\neq 0 \\ 0
 &, x=y=0
\end{matrix}\right.   \lim_{x\rightarrow 0}[\lim_{y\rightarrow 0} f(x,y)]   f(x,y)   (0,0)   \lim_{y\rightarrow 0} f(x,y)   x\sin\frac{1}{x}\lim_{y\rightarrow 0}\sin\frac{1}{y} - \sin\frac{1}{x}\lim_{y\rightarrow 0}y \sin\frac{1}{y}   \lim_{y\rightarrow 0}\sin\frac{1}{y}   \lim_{y\rightarrow 0} f(x,y)   \lim_{x\rightarrow 0}[\lim_{y\rightarrow 0} f(x,y)]   \lim_{(x,y)\rightarrow (0,0)} f(x,y) = 0 ","['limits', 'functions', 'trigonometry', 'continued-fractions']"
85,Limit $\lim\limits_{n \rightarrow +\infty} \frac{\sum\limits_{k=1}^{n} \sqrt[k] {k} }{n}= 1$,Limit,\lim\limits_{n \rightarrow +\infty} \frac{\sum\limits_{k=1}^{n} \sqrt[k] {k} }{n}= 1,I would like to prove that: $$\lim\limits_{n \rightarrow +\infty} \frac{\sum\limits_{k=1}^{n} \sqrt[k] {k} }{n}= 1$$ I thought to write $\sqrt[k] {k} = e^{\frac{\ln({k})}{k}}$ but I don't know how to continue.,I would like to prove that: I thought to write but I don't know how to continue.,\lim\limits_{n \rightarrow +\infty} \frac{\sum\limits_{k=1}^{n} \sqrt[k] {k} }{n}= 1 \sqrt[k] {k} = e^{\frac{\ln({k})}{k}},"['limits', 'radicals']"
86,convergence of a recursive sequence with parameter a,convergence of a recursive sequence with parameter a,,How can you determine if the following recursive sequence converges: $$x_{n+1}=\frac{1}{2}(a+x_n^2)$$ where $0\le a \le 1$ and $x_1=0$ I know that the limit x (if it exists) satisfies the following equation: $$x=\frac{1}{2}(a+x^2)$$ since $\lim_{x\rightarrow \infty} x_n = \lim_{x\rightarrow \infty} x_{n+1}$ . Therefore $x=1\pm \sqrt{1-a}$ Thank you in advance :),How can you determine if the following recursive sequence converges: where and I know that the limit x (if it exists) satisfies the following equation: since . Therefore Thank you in advance :),x_{n+1}=\frac{1}{2}(a+x_n^2) 0\le a \le 1 x_1=0 x=\frac{1}{2}(a+x^2) \lim_{x\rightarrow \infty} x_n = \lim_{x\rightarrow \infty} x_{n+1} x=1\pm \sqrt{1-a},"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'recursion']"
87,Find the limit $\lim_{n\to\infty}\frac{x_n + x_n^2 + \cdots + x_n^k - k}{x_n - 1}$ given $x_n \ne 1$ and $\lim x_n = 1$,Find the limit  given  and,\lim_{n\to\infty}\frac{x_n + x_n^2 + \cdots + x_n^k - k}{x_n - 1} x_n \ne 1 \lim x_n = 1,"Let $x_n$ denote a sequence, $n\in\Bbb N$ . Evaluate the limit: $$ \lim_{n\to\infty} \frac{x_n + x_n^2 + \cdots + x_n^k - k}{x_n - 1},\ k\in\Bbb N $$ given $$ \lim_{n\to\infty} x_n = 1 \\  x_n \ne 1 $$ I'm interested in verifying the following results. Denote: $$ y_n = \frac{x_n + x_n^2 + \cdots + x_n^k - k}{x_n - 1} $$ After some trials long division seem to produce a pattern here. Not sure how to put it here in a fancy way, so I will post the result only. It appears that: $$ y_n = x_n^{k-1} + 2x_n^{k-2}+\cdots + (k-1)x_n + k $$ It is given that $\lim x_n = 1$ therefore we may use the following properties: $$ \lim(a_n + b_n) = \lim a_n + \lim b_n\\ \lim(a_n \cdot b_n) = \lim a_n \cdot \lim b_n $$ In particular: $$ y_n = x_n^{k-1} + 2x_n^{k-2}+\cdots + (k-1)x_n + k\\ \lim_{n\to\infty}y_n =\lim_{n\to\infty}\left(x_n^{k-1} + 2x_n^{k-2}+\cdots + (k-1)x_n + k\right) $$ Then since $\lim x_n = 1$ and by the sum of first $k$ integers: $$ \lim_{n\to\infty} y_n = \frac{k(k+1)}{2} $$ Could you please verify whether the reasoning above is correct or not and point to the mistakes in case of any? Thank you!","Let denote a sequence, . Evaluate the limit: given I'm interested in verifying the following results. Denote: After some trials long division seem to produce a pattern here. Not sure how to put it here in a fancy way, so I will post the result only. It appears that: It is given that therefore we may use the following properties: In particular: Then since and by the sum of first integers: Could you please verify whether the reasoning above is correct or not and point to the mistakes in case of any? Thank you!","x_n n\in\Bbb N 
\lim_{n\to\infty} \frac{x_n + x_n^2 + \cdots + x_n^k - k}{x_n - 1},\ k\in\Bbb N
 
\lim_{n\to\infty} x_n = 1 \\ 
x_n \ne 1
 
y_n = \frac{x_n + x_n^2 + \cdots + x_n^k - k}{x_n - 1}
 
y_n = x_n^{k-1} + 2x_n^{k-2}+\cdots + (k-1)x_n + k
 \lim x_n = 1 
\lim(a_n + b_n) = \lim a_n + \lim b_n\\
\lim(a_n \cdot b_n) = \lim a_n \cdot \lim b_n
 
y_n = x_n^{k-1} + 2x_n^{k-2}+\cdots + (k-1)x_n + k\\
\lim_{n\to\infty}y_n =\lim_{n\to\infty}\left(x_n^{k-1} + 2x_n^{k-2}+\cdots + (k-1)x_n + k\right)
 \lim x_n = 1 k 
\lim_{n\to\infty} y_n = \frac{k(k+1)}{2}
","['calculus', 'sequences-and-series', 'limits', 'proof-verification']"
88,Convergence of $\sum_{n = 1}^\infty \frac{(-1)^n}{n + n^2(1 + (-1)^n)}$,Convergence of,\sum_{n = 1}^\infty \frac{(-1)^n}{n + n^2(1 + (-1)^n)},Is $$\sum_{n = 1}^\infty \frac{(-1)^n}{n + n^2(1 + (-1)^n)}$$ convergent? It is not absolutely convergent and Leibniz test is inconclusive.,Is convergent? It is not absolutely convergent and Leibniz test is inconclusive.,\sum_{n = 1}^\infty \frac{(-1)^n}{n + n^2(1 + (-1)^n)},"['sequences-and-series', 'limits', 'convergence-divergence']"
89,"Prove any number $c \in [a, b]$ is a subsequential limit if $\lim\inf x_n = a$, $\lim \sup x_n = b$, $a\ne b$, $\lim(x_n -x_{n+1})=0$","Prove any number  is a subsequential limit if , , ,","c \in [a, b] \lim\inf x_n = a \lim \sup x_n = b a\ne b \lim(x_n -x_{n+1})=0","I'm trying to solve the following problem: Let $\{x_n\}$ denote a bounded sequence. Prove that any number $c \in [a, b]$ is a subsequential limit of $\{x_n\}$ if: $$ \begin{cases} \lim_{n\to\infty} (x_n - x_{n+1})=0\\ \lim\inf x_n = a\\ \lim \sup x_n = b\\ a\ne b \end{cases} $$ Here are some of my thoughts. We know that $x_n$ is bounded. Then by Bolzano-Weierstrass we may choose some subsequence such that it has a finite limit: $$ \exists c \in [a, b] : \lim x_{n_k} = c \iff \forall \epsilon_1 > 0 \exists N_1\in\Bbb N: \forall n_k > N_1 \implies |x_{n_k} - c| < \epsilon_1 $$ We are also given that limsup and liminf exist and therefore: $$ \exists N_2 \in \Bbb N : \forall n_k > N_2 \implies x_{n_k} \ge a \\ \exists N_3 \in \Bbb N : \forall n_k > N_3 \implies x_{n_k} \le b $$ If we now choose $N$ to be $\max\{N_2, N_3\}$ we obtain: $$ \exists N = \max\{N_2, N_3\}: \forall n_k > N \implies a \le x_{n_k} \le b \tag1 $$ Also we are given the fact that $\lim (x_n - x_{n+1}) = 0$ : $$ \forall \epsilon_2 > 0, \exists N_4 \in \Bbb N: \forall n_k > N_4\implies |x_n - x_{n+1}| < \epsilon_2 $$ But if $\lim (x_n - x_{n+1}) = 0$ , then it is also true for the subsequences: $$ \forall \epsilon_3 > 0, \exists N_5 \in \Bbb N: \forall n_k > N_5\implies |x_{n_k} - x_{n_k+1}| < \epsilon_3 \tag 2 $$ Now I'm struggling to combine that facts in order to show that any $c \in [a, b]$ is a subsequential limit of $\{x_n\}$ , how do I proceed? Feels like i have to consider $(1)$ and $(2)$ in tandem in order to finish the proof.","I'm trying to solve the following problem: Let denote a bounded sequence. Prove that any number is a subsequential limit of if: Here are some of my thoughts. We know that is bounded. Then by Bolzano-Weierstrass we may choose some subsequence such that it has a finite limit: We are also given that limsup and liminf exist and therefore: If we now choose to be we obtain: Also we are given the fact that : But if , then it is also true for the subsequences: Now I'm struggling to combine that facts in order to show that any is a subsequential limit of , how do I proceed? Feels like i have to consider and in tandem in order to finish the proof.","\{x_n\} c \in [a, b] \{x_n\} 
\begin{cases}
\lim_{n\to\infty} (x_n - x_{n+1})=0\\
\lim\inf x_n = a\\
\lim \sup x_n = b\\
a\ne b
\end{cases}
 x_n 
\exists c \in [a, b] : \lim x_{n_k} = c \iff \forall \epsilon_1 > 0 \exists N_1\in\Bbb N: \forall n_k > N_1 \implies |x_{n_k} - c| < \epsilon_1
 
\exists N_2 \in \Bbb N : \forall n_k > N_2 \implies x_{n_k} \ge a \\
\exists N_3 \in \Bbb N : \forall n_k > N_3 \implies x_{n_k} \le b
 N \max\{N_2, N_3\} 
\exists N = \max\{N_2, N_3\}: \forall n_k > N \implies a \le x_{n_k} \le b \tag1
 \lim (x_n - x_{n+1}) = 0 
\forall \epsilon_2 > 0, \exists N_4 \in \Bbb N: \forall n_k > N_4\implies |x_n - x_{n+1}| < \epsilon_2
 \lim (x_n - x_{n+1}) = 0 
\forall \epsilon_3 > 0, \exists N_5 \in \Bbb N: \forall n_k > N_5\implies |x_{n_k} - x_{n_k+1}| < \epsilon_3 \tag 2
 c \in [a, b] \{x_n\} (1) (2)","['calculus', 'limits', 'limsup-and-liminf']"
90,Limit Question: a better way to solve this limit? where $x \to -\infty$,Limit Question: a better way to solve this limit? where,x \to -\infty,"Hey guys so I have this limit: $$\lim_{x \to -∞} f(x) = {(x+\sqrt{x^2+2x})}$$ I solved it by multiplying numerator and denominator by $$x-\sqrt{x^2+2x}$$ and got $-1$ as my answer, but I really don't like how I solved it; any better way to solve it?","Hey guys so I have this limit: I solved it by multiplying numerator and denominator by and got as my answer, but I really don't like how I solved it; any better way to solve it?",\lim_{x \to -∞} f(x) = {(x+\sqrt{x^2+2x})} x-\sqrt{x^2+2x} -1,['limits']
91,"Show that $f$ is differentiable on $(0,\infty)$ if and only if $f$ is differentiable at $1$",Show that  is differentiable on  if and only if  is differentiable at,"f (0,\infty) f 1","Suppose $f:(0,\infty) \to \mathbb{R}$ satisfies $f(x/y) = f(x) - f(y)$ for all $x,y>0$ and that $f(1) = 0$ . Question: Show that $f$ is differentiable on $(0,\infty)$ if and only if $f$ is differentiable at $1$ . My attempt on this problem: Let $a \in (0,\infty)$ . Then $f'(x) = \lim_{h\to 0} \frac{f(a+h) - f(a)}{h} = \lim_{h\to 0} \frac{f(\frac{a+h}{a})}{h} = \lim_{h\to 0} \frac{f(1 +\frac{h}{a})}{h}.$ All I know is that the top will go to $f(1) = 0$ and the bottom will also go to $0$ but I don't know how to show that this is differentiable on $(0,\infty)$ . Any help would be appreciated.",Suppose satisfies for all and that . Question: Show that is differentiable on if and only if is differentiable at . My attempt on this problem: Let . Then All I know is that the top will go to and the bottom will also go to but I don't know how to show that this is differentiable on . Any help would be appreciated.,"f:(0,\infty) \to \mathbb{R} f(x/y) = f(x) - f(y) x,y>0 f(1) = 0 f (0,\infty) f 1 a \in (0,\infty) f'(x) = \lim_{h\to 0} \frac{f(a+h) - f(a)}{h} = \lim_{h\to 0} \frac{f(\frac{a+h}{a})}{h} = \lim_{h\to 0} \frac{f(1 +\frac{h}{a})}{h}. f(1) = 0 0 (0,\infty)","['real-analysis', 'limits', 'derivatives']"
92,Another Limit Conundrum,Another Limit Conundrum,,"For what values of $a$ and $b$ is the following limit true? $$\lim_{x\to0}\left(\frac{\tan2x}{x^3}+\frac{a}{x^2}+\frac{\sin bx}{x}\right)=0$$ This question is really confusing me. I know that $\tan(2x)/x^3$ approaches infinity as $x$ goes to $0$ (L'Hôpital's). I also understand that $\sin(bx)/x$ goes to $b$ as $x$ approaches $0$ . However, I am not sure how to get rid of this infinity with the middle term. Any ideas? Thanks!","For what values of and is the following limit true? This question is really confusing me. I know that approaches infinity as goes to (L'Hôpital's). I also understand that goes to as approaches . However, I am not sure how to get rid of this infinity with the middle term. Any ideas? Thanks!",a b \lim_{x\to0}\left(\frac{\tan2x}{x^3}+\frac{a}{x^2}+\frac{\sin bx}{x}\right)=0 \tan(2x)/x^3 x 0 \sin(bx)/x b x 0,"['calculus', 'limits']"
93,Showing that the Dirichlet function has no limit,Showing that the Dirichlet function has no limit,,"Let $f(x) = 0$ if $x$ is rational and $f(x)=1$ if $x$ is irrational. Prove that $\lim_{x\to a} f(x)$ does not exist for any $a$ . Proof attempt: Let $a\in\mathbb{R}$ . The only two possible values for a limit of $\lim_{x\to a}f(x)$ are $0,1$ , since otherwise there will always be a constant gap between the valules of $f$ and the limit, and hence $f$ would not get arbitrarily small. However, $\lim_{x\to a}f(x) \not = 1$ for the following reason: Set $\epsilon = 1/2$ and let $\delta > 0$ . Then there exists a rational number $x_0$ in $0<|x_0-a|<\delta$ by the density of the rationals. However, $|f(x_0)-1| = |0-1| = 1 \not < 1/2$ , and so $\lim_{x\to a}f(x) \not = 1$ . Also, $\lim_{x\to a}f(x) \not = 0$ for the following reason: Set $\epsilon = 1/2$ and let $\delta > 0$ . Then there exists a irrational number $x_0$ in $0<|x_0-a|<\delta$ by the density of the irrationals. However, $|f(x_0)-0| = |1-0| = 1 \not < 1/2$ , and so $\lim_{x\to a}f(x) \not = 0$ .","Let if is rational and if is irrational. Prove that does not exist for any . Proof attempt: Let . The only two possible values for a limit of are , since otherwise there will always be a constant gap between the valules of and the limit, and hence would not get arbitrarily small. However, for the following reason: Set and let . Then there exists a rational number in by the density of the rationals. However, , and so . Also, for the following reason: Set and let . Then there exists a irrational number in by the density of the irrationals. However, , and so .","f(x) = 0 x f(x)=1 x \lim_{x\to a} f(x) a a\in\mathbb{R} \lim_{x\to a}f(x) 0,1 f f \lim_{x\to a}f(x) \not = 1 \epsilon = 1/2 \delta > 0 x_0 0<|x_0-a|<\delta |f(x_0)-1| = |0-1| = 1 \not < 1/2 \lim_{x\to a}f(x) \not = 1 \lim_{x\to a}f(x) \not = 0 \epsilon = 1/2 \delta > 0 x_0 0<|x_0-a|<\delta |f(x_0)-0| = |1-0| = 1 \not < 1/2 \lim_{x\to a}f(x) \not = 0","['real-analysis', 'limits', 'proof-verification']"
94,Computing $\;\lim\limits_{x\to 4}\left (\frac{\frac{\pi}{6} - \arcsin\left(\frac{\sqrt{x}}{4}\right)}{\sqrt[3]{2x-7}-1}\right) $ without L'Hôpital?,Computing  without L'Hôpital?,\;\lim\limits_{x\to 4}\left (\frac{\frac{\pi}{6} - \arcsin\left(\frac{\sqrt{x}}{4}\right)}{\sqrt[3]{2x-7}-1}\right) ,"$$ \lim_{x\to 4}\left(\frac{\frac{\pi}{6} - \arcsin\left(\frac{\sqrt{x}}{4}\right)}{\sqrt[3]{2x-7}-1}\right) $$ Hello! I need to solve this limit. I had solved it with the rule of L'Hôpital, but i can't without it. I tried multiplication by conjugate expression and using Special Limits. Please help me, I must solve it using only Special Limits and simple transformations. I can't use derivatives.","Hello! I need to solve this limit. I had solved it with the rule of L'Hôpital, but i can't without it. I tried multiplication by conjugate expression and using Special Limits. Please help me, I must solve it using only Special Limits and simple transformations. I can't use derivatives.","
\lim_{x\to 4}\left(\frac{\frac{\pi}{6} - \arcsin\left(\frac{\sqrt{x}}{4}\right)}{\sqrt[3]{2x-7}-1}\right)
","['calculus', 'sequences-and-series', 'limits', 'limits-without-lhopital']"
95,How to evaluate $\lim_{n\to\infty} \dfrac{1^p+2^p+...+n^p}{n^p}-\frac{n}{p+1}$? [duplicate],How to evaluate ? [duplicate],\lim_{n\to\infty} \dfrac{1^p+2^p+...+n^p}{n^p}-\frac{n}{p+1},"This question already has answers here : Evaluating $\lim\limits_{n\to\infty} \left(\frac{1^p+2^p+3^p + \cdots + n^p}{n^p} - \frac{n}{p+1}\right)$ (7 answers) Closed 5 years ago . $$\lim_{n\to\infty} \dfrac{1^p+2^p+...+n^p}{n^p}-\frac{n}{p+1}\text{ with } p\in\mathbb N$$ I don't really know how to start, I mean... I could try substituting by $\Big(\dfrac{n(n+1)}{2}\Big)^p$ but that would do me anything? Any hints?","This question already has answers here : Evaluating $\lim\limits_{n\to\infty} \left(\frac{1^p+2^p+3^p + \cdots + n^p}{n^p} - \frac{n}{p+1}\right)$ (7 answers) Closed 5 years ago . I don't really know how to start, I mean... I could try substituting by but that would do me anything? Any hints?",\lim_{n\to\infty} \dfrac{1^p+2^p+...+n^p}{n^p}-\frac{n}{p+1}\text{ with } p\in\mathbb N \Big(\dfrac{n(n+1)}{2}\Big)^p,"['limits', 'analysis', 'limits-without-lhopital']"
96,How do I disprove using $\epsilon - \delta $ definition of limits?,How do I disprove using  definition of limits?,\epsilon - \delta ,"I have only learned (in the morning), that how to use $\epsilon- \delta$ definition to ""prove"" a limit, thanks to the people here, that helped me learn how to pick my delta, and after 6+ hours of practising, I think I have gotten fairly good at that. I'd like to learn how to ""disprove"" using the same method. My book, which I'm self studying from, Bartle, doesn't have a lot on disproving. So I'd like some to help me out with this. First of all, what I know: I understand to disprove, It'd be sufficient to pick a particular $\epsilon$ , and show that for every $\delta >0$ there exists some $x$ belonging to the deleted delta nbd of $c$ (point at which I'm finding the limit) for which, $|f(x)-L|≥ \epsilon$ this is more or less, negation of the definition. So taking random example, let $f(x)= 2x+3$ , disprove the $\lim\limits_{x \to 2} f(x)= 10$ So $|f(x)-10| = |2x-7|$ Now before taking random $\epsilon$ I'd like someone to point me out, how $\epsilon$ is chosen? Because clearly, the condition tells us, there exists ""at least one"" $\epsilon$ , not ""for all"" $\epsilon$ , so (I'm guessing) there are some specific $\epsilon$ 's only out of which we have to pick one. Next up, how is that ""particular"" $x$ taken from the deleted nbd of $2$ . I did see some examples on the internet, but as I'm a beginner, given my limited understanding of real analysis, it becomes very hard to follow the arguments without explanation. Hence this question. I know I'm asking a lot, so any help is appreciated","I have only learned (in the morning), that how to use definition to ""prove"" a limit, thanks to the people here, that helped me learn how to pick my delta, and after 6+ hours of practising, I think I have gotten fairly good at that. I'd like to learn how to ""disprove"" using the same method. My book, which I'm self studying from, Bartle, doesn't have a lot on disproving. So I'd like some to help me out with this. First of all, what I know: I understand to disprove, It'd be sufficient to pick a particular , and show that for every there exists some belonging to the deleted delta nbd of (point at which I'm finding the limit) for which, this is more or less, negation of the definition. So taking random example, let , disprove the So Now before taking random I'd like someone to point me out, how is chosen? Because clearly, the condition tells us, there exists ""at least one"" , not ""for all"" , so (I'm guessing) there are some specific 's only out of which we have to pick one. Next up, how is that ""particular"" taken from the deleted nbd of . I did see some examples on the internet, but as I'm a beginner, given my limited understanding of real analysis, it becomes very hard to follow the arguments without explanation. Hence this question. I know I'm asking a lot, so any help is appreciated",\epsilon- \delta \epsilon \delta >0 x c |f(x)-L|≥ \epsilon f(x)= 2x+3 \lim\limits_{x \to 2} f(x)= 10 |f(x)-10| = |2x-7| \epsilon \epsilon \epsilon \epsilon \epsilon x 2,"['real-analysis', 'limits']"
97,How to find $\lim_{x \to \infty} x(e^{1/x}-1)^x$ without L'Hopital's rule or Taylor expansion?,How to find  without L'Hopital's rule or Taylor expansion?,\lim_{x \to \infty} x(e^{1/x}-1)^x,"I'm trying to find the following limit $$ \lim_{x \to \infty} x(e^{1/x}-1)^x,$$ without L'Hopital or Taylor. So far I got \begin{align*} \lim_{x \to \infty} x(e^{1/x}-1)^x &= \lim_{x \to \infty} x(e^{1/x}-1)^x \\ &= \lim_{u \to 0} \frac{1}{\ln(u+1)}(u)^{\frac{1}{\ln(u+1)}} \\ &= \lim_{u \to 0} \frac{1}{\ln(u+1)} \left((u)^{\frac{u}{ \ln(u+1)}}\right)^{\frac{1}{u} } \\ &= \lim_{u \to 0} \frac{1}{\ln(u+1)} \left((u)^{\frac{1}{ \ln(u+1)^{1/u}}}\right)^{\frac{1}{u} } \\ \end{align*} Any hint for going on?",I'm trying to find the following limit without L'Hopital or Taylor. So far I got Any hint for going on?," \lim_{x \to \infty} x(e^{1/x}-1)^x, \begin{align*}
\lim_{x \to \infty} x(e^{1/x}-1)^x &= \lim_{x \to \infty} x(e^{1/x}-1)^x \\
&= \lim_{u \to 0} \frac{1}{\ln(u+1)}(u)^{\frac{1}{\ln(u+1)}} \\
&= \lim_{u \to 0} \frac{1}{\ln(u+1)} \left((u)^{\frac{u}{
\ln(u+1)}}\right)^{\frac{1}{u} } \\
&= \lim_{u \to 0} \frac{1}{\ln(u+1)} \left((u)^{\frac{1}{
\ln(u+1)^{1/u}}}\right)^{\frac{1}{u} } \\
\end{align*}","['calculus', 'sequences-and-series', 'limits', 'limits-without-lhopital']"
98,Application of squeeze theorem,Application of squeeze theorem,,I don't really know how the Squeeze theorem works and I tried applying it to solve this limit: $$\lim_{n\to\infty}\text{ } \frac{1}{n^2}+\frac{1}{(n+1)^2}+\dots+\frac{1}{(n+n)^2}$$ So $$\frac{n}{(n+n)^2}\leq \frac{1}{n^2}+\frac{1}{(n+1)^2}+\dots+\frac{1}{(n+n)^2} \leq \frac{n}{n^2} $$ Then: $$\lim_{n\to\infty}\text{ } \frac{n}{n^2}=0$$ $$\lim_{n\to\infty}\text{ } \frac{n}{(n+n)^2}=0$$ Therefore: $$\lim_{n\to\infty}\text{ } \frac{1}{n^2}+\frac{1}{(n+1)^2}+\dots+\frac{1}{(n+n)^2}=0$$ Is this the correct way?,I don't really know how the Squeeze theorem works and I tried applying it to solve this limit: So Then: Therefore: Is this the correct way?,\lim_{n\to\infty}\text{ } \frac{1}{n^2}+\frac{1}{(n+1)^2}+\dots+\frac{1}{(n+n)^2} \frac{n}{(n+n)^2}\leq \frac{1}{n^2}+\frac{1}{(n+1)^2}+\dots+\frac{1}{(n+n)^2} \leq \frac{n}{n^2}  \lim_{n\to\infty}\text{ } \frac{n}{n^2}=0 \lim_{n\to\infty}\text{ } \frac{n}{(n+n)^2}=0 \lim_{n\to\infty}\text{ } \frac{1}{n^2}+\frac{1}{(n+1)^2}+\dots+\frac{1}{(n+n)^2}=0,"['limits', 'analysis']"
99,Is $\frac{x^2y^2}{\sqrt{x^2+y^2}}$ continuous?,Is  continuous?,\frac{x^2y^2}{\sqrt{x^2+y^2}},"Let $f: \mathbb{R}^2 $ -> $\mathbb{R}$ I was wondering if this function is a continuous function. Can I just say that $\frac{x^2y^2}{\sqrt{x^2+y^2}}$ is continuous everywhere except perhaps at $x=0$ , because $\lim_{x\rightarrow \infty}$ $\frac{x^2y^2}{\sqrt{x^2+y^2}}$ = $\infty$ , and the definition is that if $\lim_{x\rightarrow c}$ $f(x)$ = $f(c)$ then $f(x)$ is continuous at $c$ .","Let -> I was wondering if this function is a continuous function. Can I just say that is continuous everywhere except perhaps at , because = , and the definition is that if = then is continuous at .",f: \mathbb{R}^2  \mathbb{R} \frac{x^2y^2}{\sqrt{x^2+y^2}} x=0 \lim_{x\rightarrow \infty} \frac{x^2y^2}{\sqrt{x^2+y^2}} \infty \lim_{x\rightarrow c} f(x) f(c) f(x) c,['limits']
