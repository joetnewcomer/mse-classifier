,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Singular covariance matrix identity,Singular covariance matrix identity,,"Let $Z=(Z_1,...,Z_n)^T$ be a random vector with mean 0, and define its covariance $\Sigma=\mathbb{E}[ZZ^T]$ . Assume that $\Sigma$ is singular. Then I need to show something (non-related to the question), but I was told to start with the identity, that $\alpha^T \Sigma \alpha=0$ (I guess its supposed to be that there exists an $\alpha$ such that it is true), but I fail to understand how that is true, any help is appreciated.","Let be a random vector with mean 0, and define its covariance . Assume that is singular. Then I need to show something (non-related to the question), but I was told to start with the identity, that (I guess its supposed to be that there exists an such that it is true), but I fail to understand how that is true, any help is appreciated.","Z=(Z_1,...,Z_n)^T \Sigma=\mathbb{E}[ZZ^T] \Sigma \alpha^T \Sigma \alpha=0 \alpha","['linear-algebra', 'stochastic-processes']"
1,Show that the orthogonal projection onto $Range T$ is equal to $T(T^\ast T)^{-1}T^\ast$ given that $T: V \to W$ is injective,Show that the orthogonal projection onto  is equal to  given that  is injective,Range T T(T^\ast T)^{-1}T^\ast T: V \to W,"Given $V$ and $W$ as finite-dimensional inner product spaces and an injective linear map $T: V \to W$ , how can we show that $$P_{Range T} = T(T^\ast T)^{-1}T^* \in \mathcal{L}(W)$$ where $P_{Range T}$ is the orthogonal projection onto $Range T$ and $T^\ast$ is the adjoint of $T$ ? Supposedly this result can be used to derive a useful matrix formula for orthogonal projection, but  I'm not sure how to figure out how we can show the equality above. Does $(T^\ast T)^{-1}$ even make sense? How do we show that? And what is the intuition of the above formulation? Among my ideas are trying to show that the right hand side is in the range of $T$ (which it seems to be) and trying to utilize the fact that $P_{Range T}^2 = P_{Range T}$ . But I'm not sure how to proceed from there.","Given and as finite-dimensional inner product spaces and an injective linear map , how can we show that where is the orthogonal projection onto and is the adjoint of ? Supposedly this result can be used to derive a useful matrix formula for orthogonal projection, but  I'm not sure how to figure out how we can show the equality above. Does even make sense? How do we show that? And what is the intuition of the above formulation? Among my ideas are trying to show that the right hand side is in the range of (which it seems to be) and trying to utilize the fact that . But I'm not sure how to proceed from there.",V W T: V \to W P_{Range T} = T(T^\ast T)^{-1}T^* \in \mathcal{L}(W) P_{Range T} Range T T^\ast T (T^\ast T)^{-1} T P_{Range T}^2 = P_{Range T},['linear-algebra']
2,"Properties of the matrix $A = uv^T$, where $u, v \in \mathbb R^{m}$","Properties of the matrix , where","A = uv^T u, v \in \mathbb R^{m}","Clarification: this is a review problem, not a homework problem or anything—I'm not getting graded on this. That being said, I cannot seem to figure out how to do the last part. Consider the matrix $A = u v^T$ , where $u, v \in \mathbb R^{m}$ . (a) What is the rank of $A$ ? Find a basis for the range of $A$ . This part is easy: we know that Range( $uv^T$ ) is just span({ $u$ }). (b) List all eigenvalue of $A$ . What are their geometric and algebraic multiplicities? Since the columns of $A$ are just linear combinations of $u$ , it should be easy to say that 0 is an eigenvalue with geometric and algebraic multiplicity $m - 1$ since the eigenspace associated with $\lambda = 0$ is just Null( $A$ ). The other eigenvalue I feel can only be found through observation: $uv^T*u$ = $u<u, v>$ = $<u, v> u$ , so the eigenvalue is $<u,v>$ with geometric and algebraic multiplicities 1. If there's any other way to find this, please let me know. (c) Find the eigenvector for the nonzero eigenvalue of $A$ . From above: $u$ (d) Find an orthogonal projector onto the range of $A$ . This is pretty obvious again just by looking at the definition of a projector and because Range( $A$ ) = span({ $u$ }): $\frac{1}{\|u\|^2}uu^T$ . (e) Find an orthogonal projector onto the nullspace of $A$ . No idea. I'm having a sort of disconnect here and really can't figure it out.","Clarification: this is a review problem, not a homework problem or anything—I'm not getting graded on this. That being said, I cannot seem to figure out how to do the last part. Consider the matrix , where . (a) What is the rank of ? Find a basis for the range of . This part is easy: we know that Range( ) is just span({ }). (b) List all eigenvalue of . What are their geometric and algebraic multiplicities? Since the columns of are just linear combinations of , it should be easy to say that 0 is an eigenvalue with geometric and algebraic multiplicity since the eigenspace associated with is just Null( ). The other eigenvalue I feel can only be found through observation: = = , so the eigenvalue is with geometric and algebraic multiplicities 1. If there's any other way to find this, please let me know. (c) Find the eigenvector for the nonzero eigenvalue of . From above: (d) Find an orthogonal projector onto the range of . This is pretty obvious again just by looking at the definition of a projector and because Range( ) = span({ }): . (e) Find an orthogonal projector onto the nullspace of . No idea. I'm having a sort of disconnect here and really can't figure it out.","A = u v^T u, v \in \mathbb R^{m} A A uv^T u A A u m - 1 \lambda = 0 A uv^T*u u<u, v> <u, v> u <u,v> A u A A u \frac{1}{\|u\|^2}uu^T A","['linear-algebra', 'matrices', 'projection-matrices']"
3,QR decomposition with lower triangular matrix using Householder reflection,QR decomposition with lower triangular matrix using Householder reflection,,"Problem Find householder matrices $H_1,H_2,\cdots,H_n$ such that $$ H_n\cdots H_1 A = L $$ where $A$ : $n \times n$ matrix and $L$ : $n \times n$ lower triangular matrix . Try By defining $v_k:= [\cdots, sgn(x_k) |x_k|, \cdots]$ and $H_k := I - 2v_kv_k^T/v_k^Tv_k$ , we can make $$ H_n\cdots H_1 A = U $$ where $U$ : $n \times n$ upper triangular matrix But I'm currently stuck at how to define $v_k$ to make RHS lower triangular.","Problem Find householder matrices such that where : matrix and : lower triangular matrix . Try By defining and , we can make where : upper triangular matrix But I'm currently stuck at how to define to make RHS lower triangular.","H_1,H_2,\cdots,H_n 
H_n\cdots H_1 A = L
 A n \times n L n \times n v_k:= [\cdots, sgn(x_k) |x_k|, \cdots] H_k := I - 2v_kv_k^T/v_k^Tv_k 
H_n\cdots H_1 A = U
 U n \times n v_k","['linear-algebra', 'numerical-linear-algebra', 'matrix-decomposition']"
4,Orthogonal bases of the vector space $\mathbb{Z}_2^4$,Orthogonal bases of the vector space,\mathbb{Z}_2^4,"Let $\mathbb{Z}_2$ be the two element field $\mathbb{Z}/2\mathbb{Z}$ . The vectors $e_0 = \langle1,1,1,1\rangle$ , $e_1=\langle1,1,0,0\rangle$ , $e_2 = \langle1,0,0,1\rangle$ , $e_3 = \langle1,0,1,0\rangle$ in the $\mathbb{Z}_2$ -vector space $\mathbb{Z}_2^4$ don't form a basis of $\mathbb{Z}_2^4$ . Compare this to $\langle1,0,0,0\rangle$ , $\langle 0,1,0,0\rangle$ , $\langle 0,0,1,0\rangle$ , $\langle 0,0,0,1\rangle$ which do form a orthonormal basis of $\mathbb{Z}_2^4$ and to $\langle 1,1,1,1\rangle$ , $\langle 1,1,-1,-1\rangle$ , $\langle 1,-1,-1,1\rangle$ , $\langle 1,-1,1,-1\rangle$ which form a orthogonal basis of the $\mathbb{R}$ -vector space $\mathbb{R}^4$ (the Hadamard-Walsh basis). What does this mean? Does it mean, that there is essentially only one basis of $\mathbb{Z}_2^4$ ? If not so: What are the orthogonal bases of $\mathbb{Z}_2^4$ ? What are the orthogonal bases of $\mathbb{Z}_2^8$ ? (Complete lists would be welcome.)","Let be the two element field . The vectors , , , in the -vector space don't form a basis of . Compare this to , , , which do form a orthonormal basis of and to , , , which form a orthogonal basis of the -vector space (the Hadamard-Walsh basis). What does this mean? Does it mean, that there is essentially only one basis of ? If not so: What are the orthogonal bases of ? What are the orthogonal bases of ? (Complete lists would be welcome.)","\mathbb{Z}_2 \mathbb{Z}/2\mathbb{Z} e_0 = \langle1,1,1,1\rangle e_1=\langle1,1,0,0\rangle e_2 = \langle1,0,0,1\rangle e_3 = \langle1,0,1,0\rangle \mathbb{Z}_2 \mathbb{Z}_2^4 \mathbb{Z}_2^4 \langle1,0,0,0\rangle \langle 0,1,0,0\rangle \langle 0,0,1,0\rangle \langle 0,0,0,1\rangle \mathbb{Z}_2^4 \langle 1,1,1,1\rangle \langle 1,1,-1,-1\rangle \langle 1,-1,-1,1\rangle \langle 1,-1,1,-1\rangle \mathbb{R} \mathbb{R}^4 \mathbb{Z}_2^4 \mathbb{Z}_2^4 \mathbb{Z}_2^8","['linear-algebra', 'orthogonality', 'orthonormal', 'change-of-basis']"
5,How to evaluate this three diagonal determinant?2019,How to evaluate this three diagonal determinant?2019,,"Can someone give me a hint how to solve \begin{align*} |A|=\begin{vmatrix} x & 1 & 0 & 0 & \cdots & 0 & 0 \\ n-1 & x & 2 & 0 & \cdots & 0 & 0 \\ 0 & n-2 & x & 3 & \cdots & 0 & 0 \\ 0 & 0 & n-3 & x & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & & \vdots & \vdots \\ 0 & 0 & 0 & 0 & \cdots & 1 & x \\ \end{vmatrix}？ \end{align*} By adding to row 1 the rows from 2 to n ,I can see that $|A|$ has $x+n-1$ as a factor. And by trying $n=2,3,4$ , I can deduce that $$|A|=(x^2-(n-1)^2)(x^2-(n-3)^2)\cdots (x^2-1^2)$$ if $n$ is even, and $$|A|=(x^2-(n-1)^2)(x^2-(n-3)^2)\cdots (x^2-2^2)x$$ if $n$ is odd. I tried hard to give a proof, but without any progress.","Can someone give me a hint how to solve By adding to row 1 the rows from 2 to n ,I can see that has as a factor. And by trying , I can deduce that if is even, and if is odd. I tried hard to give a proof, but without any progress.","\begin{align*}
|A|=\begin{vmatrix} x & 1 & 0 & 0 & \cdots & 0 & 0 \\ n-1 & x & 2 & 0 & \cdots & 0 & 0 \\ 0 & n-2 & x & 3 & \cdots & 0 & 0 \\ 0 & 0 & n-3 & x & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & & \vdots & \vdots \\ 0 & 0 & 0 & 0 & \cdots & 1 & x \\ \end{vmatrix}？
\end{align*} |A| x+n-1 n=2,3,4 |A|=(x^2-(n-1)^2)(x^2-(n-3)^2)\cdots (x^2-1^2) n |A|=(x^2-(n-1)^2)(x^2-(n-3)^2)\cdots (x^2-2^2)x n","['linear-algebra', 'determinant']"
6,"Given probability vectors $x$ and $y$, how can I compute probability vector of $z = x + y$ using linear algebra operations","Given probability vectors  and , how can I compute probability vector of  using linear algebra operations",x y z = x + y,"I have probability vector $t_1$ (hours to get ' $A$ ' done), $p({t^{a}_{1})} = [0.25, 0.25, 0.25, 0.25]$ (here: $0.25$ probability for $A$ done in $1, 2, 3$ or $4$ hours). Another probability vector $t_2$ (hours to get ' $B$ ' done), $p({t^{b}_{2})} = [0.33, 0.33, 0.33]$ . How can I get to probability vector $p({t^{a}_{1}+t^{b}_{2}))}$ (hours to get both $A$ and $B$ done assuming they can only be done one after the other and distributions are independent) using linear algebra operations ? p.s.: I am looking for linear algebraic solution and suspect that there may be a solution via markov chain etc.","I have probability vector (hours to get ' ' done), (here: probability for done in or hours). Another probability vector (hours to get ' ' done), . How can I get to probability vector (hours to get both and done assuming they can only be done one after the other and distributions are independent) using linear algebra operations ? p.s.: I am looking for linear algebraic solution and suspect that there may be a solution via markov chain etc.","t_1 A p({t^{a}_{1})} = [0.25, 0.25, 0.25, 0.25] 0.25 A 1, 2, 3 4 t_2 B p({t^{b}_{2})} = [0.33, 0.33, 0.33] p({t^{a}_{1}+t^{b}_{2}))} A B","['linear-algebra', 'probability', 'markov-chains']"
7,Symmetrizability of shallow water equations,Symmetrizability of shallow water equations,,"Consider the shallow water equation \begin{equation}h_t+(hu)_x=0\\ (hu)_t+\left(hu^2+\frac{g}{2}h^2 \right)_x=0 \end{equation} I want to know the entropy of this system? I understood that if their exists a change of variable which symmetrizes the system, then system admits strictly convex entropy.. But I am unable to proceed... Please help","Consider the shallow water equation I want to know the entropy of this system? I understood that if their exists a change of variable which symmetrizes the system, then system admits strictly convex entropy.. But I am unable to proceed... Please help","\begin{equation}h_t+(hu)_x=0\\
(hu)_t+\left(hu^2+\frac{g}{2}h^2 \right)_x=0
\end{equation}","['linear-algebra', 'analysis', 'partial-differential-equations', 'hyperbolic-equations']"
8,"Show that if all solutions of $\frac{dy}{dx}=A(x)y$ are periodic with period $T$, than $\int_0^T \text{Trace }A(t) dt=0$","Show that if all solutions of  are periodic with period , than",\frac{dy}{dx}=A(x)y T \int_0^T \text{Trace }A(t) dt=0,"Let $A:\mathbb R \rightarrow L(\mathbb R ^n, \mathbb R^n)$ be   continuous and periodic  with period $T$ . Show that if all solutions   of $\frac{dy}{dx}=A(x)y$ are periodic with period $T$ , than $\int_0^T  \text{Trace }A(t) dt=0$ Intuitively I can see why this is true, but I'm not really sure if I'm on the right track with showing it formally. If we know all solutions are periodic, we have that for all $x$ , $y(x)=y(x+T)$ and thus $$\frac{dy(x)}{dx}=\frac{dy(x+T)}{dx}$$ From this it follows that $$A(x)y(x)=A(x+T)y(x+T)$$ So we also have that $A(x)=A(x+T)$ . So now I know $\int_0^T  \text{Trace }A(t) dt=\int_0^T  \text{Trace }A(t+T) dt$ and I want to show that this is $0$ . I don't really know where to go from here. This chapter we did learn things about the fundamental matrix, so I think I might need to use something like $det \psi(x)= e ^{\text{trace } A(x) x}$ , but so far I don't know how to.","Let be   continuous and periodic  with period . Show that if all solutions   of are periodic with period , than Intuitively I can see why this is true, but I'm not really sure if I'm on the right track with showing it formally. If we know all solutions are periodic, we have that for all , and thus From this it follows that So we also have that . So now I know and I want to show that this is . I don't really know where to go from here. This chapter we did learn things about the fundamental matrix, so I think I might need to use something like , but so far I don't know how to.","A:\mathbb R \rightarrow L(\mathbb R ^n, \mathbb R^n) T \frac{dy}{dx}=A(x)y T \int_0^T
 \text{Trace }A(t) dt=0 x y(x)=y(x+T) \frac{dy(x)}{dx}=\frac{dy(x+T)}{dx} A(x)y(x)=A(x+T)y(x+T) A(x)=A(x+T) \int_0^T
 \text{Trace }A(t) dt=\int_0^T
 \text{Trace }A(t+T) dt 0 det \psi(x)= e ^{\text{trace } A(x) x}","['linear-algebra', 'integration', 'ordinary-differential-equations']"
9,"Find $A, B \in M_2(\mathbb{Q})$",Find,"A, B \in M_2(\mathbb{Q})","Find $A, B \in M_2(\mathbb {Q}) $ so that $A^2+2B^2=\begin{pmatrix} 4& - 4\\ -2 & 2 \end{pmatrix}$ and $AB+BA=\begin{pmatrix}3 & - 3\\ -1 & 1 \end{pmatrix}$ . My work so far : $(A +\sqrt 2 B) ^2=A^2+2B^2+\sqrt 2 (AB+BA) $ . After taking determinants we get that $\det (A +\sqrt 2 B)=0$ and this implies that $Tr A\cdot Tr B=Tr(AB) $ and $\det A=-2\det B$ . Here I am stuck. Edit: We can similarly get that $\det(A-\sqrt 2 B) =0$ and this implies that $\det A=\det B=0$ and I think now we can easily find the traces and then just substitute back into the equations.",Find so that and . My work so far : . After taking determinants we get that and this implies that and . Here I am stuck. Edit: We can similarly get that and this implies that and I think now we can easily find the traces and then just substitute back into the equations.,"A, B \in M_2(\mathbb {Q})  A^2+2B^2=\begin{pmatrix} 4& - 4\\
-2 & 2
\end{pmatrix} AB+BA=\begin{pmatrix}3 & - 3\\
-1 & 1
\end{pmatrix} (A +\sqrt 2 B) ^2=A^2+2B^2+\sqrt 2 (AB+BA)  \det (A +\sqrt 2 B)=0 Tr A\cdot Tr B=Tr(AB)  \det A=-2\det B \det(A-\sqrt 2 B) =0 \det A=\det B=0","['linear-algebra', 'matrices']"
10,Why the sum of all coefficients is 1 in Affine Combination?,Why the sum of all coefficients is 1 in Affine Combination?,,"In Vector Space $V$ , any vector $v$ can be written in $\textbf{linear combination}$ of a basis $\{e_1, e_2, \dots e_n\}$ such as $$ v = \sum_{i=1}^{n} \alpha_i e_i  $$ In Affine Space, any point $p$ can be written in $\textbf{affine combination}$ of $\{p_1, p_1, \dots p_n\}$ such as $$ p = \sum_{i=1}^{n} \beta_i p_i \quad \mbox{ where } 1 = \sum_{i=1}^{n} \beta_i $$ My Questions: Why we need the condition in $\textbf{Affine combination}$ such as $$ 1 = \sum_{i=1}^{n} \beta_i $$ What is wrong with that if we DO NOT have the constraint","In Vector Space , any vector can be written in of a basis such as In Affine Space, any point can be written in of such as My Questions: Why we need the condition in such as What is wrong with that if we DO NOT have the constraint","V v \textbf{linear combination} \{e_1, e_2, \dots e_n\} 
v = \sum_{i=1}^{n} \alpha_i e_i 
 p \textbf{affine combination} \{p_1, p_1, \dots p_n\} 
p = \sum_{i=1}^{n} \beta_i p_i \quad \mbox{ where } 1 = \sum_{i=1}^{n} \beta_i
 \textbf{Affine combination} 
1 = \sum_{i=1}^{n} \beta_i
",['linear-algebra']
11,What is the Hessian of the spectral norm?,What is the Hessian of the spectral norm?,,The spectral norm of a symmetric matrix is the absolute value of the top eigenvalue. The gradient of this norm is $uu^T$ where $u$ is the eigenvector associated with that top eigenvalue. Assume that $A$ has an isolated top eigenvalue. Then it is twice differentiable. What is its Hessian? Any comments about how one generally computes Hessians of matrix norms are also appreciated!,The spectral norm of a symmetric matrix is the absolute value of the top eigenvalue. The gradient of this norm is where is the eigenvector associated with that top eigenvalue. Assume that has an isolated top eigenvalue. Then it is twice differentiable. What is its Hessian? Any comments about how one generally computes Hessians of matrix norms are also appreciated!,uu^T u A,"['linear-algebra', 'matrices', 'positive-semidefinite', 'hessian-matrix', 'spectral-norm']"
12,"Representation of negative Quantum entropy in terms of eigenvalues, i.e., $\text{Tr}(M\log M -M)=\sum_{i=1}^{n}(\lambda_i\log(\lambda_i)-\lambda_i)$?","Representation of negative Quantum entropy in terms of eigenvalues, i.e., ?",\text{Tr}(M\log M -M)=\sum_{i=1}^{n}(\lambda_i\log(\lambda_i)-\lambda_i),"Negative Quantum entropy or Negative Von Nuemann entropy is defined as $f(M)=\text{Tr}(M\log M -M)$ . Where $M$ is a positive definite matrix in $\mathbb{S}_+^n$ , $\log$ is natural matrix logarithm for which $\log(M)$ is defined as $\log(M)=\sum_{i=1}^{n}\log(\lambda_i)v_iv_i^T$ where $(\lambda_i,v_i)$ are eigenpairs of $M$ . Show $f(M)=\text{Tr}(M\log M -M)=\sum_{i=1}^{n}(\lambda_i\log(\lambda_i)-\lambda_i)$ .","Negative Quantum entropy or Negative Von Nuemann entropy is defined as . Where is a positive definite matrix in , is natural matrix logarithm for which is defined as where are eigenpairs of . Show .","f(M)=\text{Tr}(M\log M -M) M \mathbb{S}_+^n \log \log(M) \log(M)=\sum_{i=1}^{n}\log(\lambda_i)v_iv_i^T (\lambda_i,v_i) M f(M)=\text{Tr}(M\log M -M)=\sum_{i=1}^{n}(\lambda_i\log(\lambda_i)-\lambda_i)","['linear-algebra', 'matrices']"
13,"$A \in \mathbb{C}^{m\times n}$,$A=FG^*$ and $r(A)=r(F)=r(G)$. Prove $A^\dagger = G(F^*AG)^{-1}F^*$ and $A^\dagger = (G^\dagger)^*F^\dagger$",", and . Prove  and",A \in \mathbb{C}^{m\times n} A=FG^* r(A)=r(F)=r(G) A^\dagger = G(F^*AG)^{-1}F^* A^\dagger = (G^\dagger)^*F^\dagger,"Let $A^\dagger$ be a Moore-Penrose inverse of a matrix $A$ . If $A \in \mathbb{C}^{m\times n}$ and $A=FG^*$ , for some $F,G$ and $r(A)=r(F)=r(G)$ , prove that $$A^\dagger = G(F^*AG)^{-1}F^*$$ and $$A^\dagger = (G^\dagger)^*F^\dagger.$$ I need to show this using SVD decomposition and maybe some other properties of a Moore-Penrose inverse. I tried to show the statement by writing SVD decomposition of all the matrices included, but it just gets messy and I didn't succeed. Any hints would be really helpful! Thanks in advance!","Let be a Moore-Penrose inverse of a matrix . If and , for some and , prove that and I need to show this using SVD decomposition and maybe some other properties of a Moore-Penrose inverse. I tried to show the statement by writing SVD decomposition of all the matrices included, but it just gets messy and I didn't succeed. Any hints would be really helpful! Thanks in advance!","A^\dagger A A \in \mathbb{C}^{m\times n} A=FG^* F,G r(A)=r(F)=r(G) A^\dagger = G(F^*AG)^{-1}F^* A^\dagger = (G^\dagger)^*F^\dagger.","['linear-algebra', 'svd', 'generalized-inverse']"
14,How to find orthogonal eigenvectors if some of the eigenvalues are the same?,How to find orthogonal eigenvectors if some of the eigenvalues are the same?,,"I have an example: $$A=\begin{pmatrix} 2 & 2 & 4 \\ 2 & 5 & 8 \\ 4 & 8 & 17 \end{pmatrix}$$ The eigenvalue I found is $\lambda_1=\lambda_2=1$ and $\lambda_3=22$ . For $\lambda=1$ , $$\begin{pmatrix} x\\ y \\ z \end{pmatrix}=\begin{pmatrix} -2\\ 1 \\ 0 \end{pmatrix}y+\begin{pmatrix} -4\\ 0 \\ 1 \end{pmatrix}z$$ For $\lambda=22$ , $$\begin{pmatrix} x\\ y \\ z \end{pmatrix}=\begin{pmatrix} 1/4\\ 1/2 \\ 1 \end{pmatrix}z$$ However, those eigenvectors I found are not orthogonal to each other. The goal is to find an orthogonal matrix P and diagonal matrix Q so that $A=PQP^T$ .","I have an example: The eigenvalue I found is and . For , For , However, those eigenvectors I found are not orthogonal to each other. The goal is to find an orthogonal matrix P and diagonal matrix Q so that .",A=\begin{pmatrix} 2 & 2 & 4 \\ 2 & 5 & 8 \\ 4 & 8 & 17 \end{pmatrix} \lambda_1=\lambda_2=1 \lambda_3=22 \lambda=1 \begin{pmatrix} x\\ y \\ z \end{pmatrix}=\begin{pmatrix} -2\\ 1 \\ 0 \end{pmatrix}y+\begin{pmatrix} -4\\ 0 \\ 1 \end{pmatrix}z \lambda=22 \begin{pmatrix} x\\ y \\ z \end{pmatrix}=\begin{pmatrix} 1/4\\ 1/2 \\ 1 \end{pmatrix}z A=PQP^T,"['linear-algebra', 'eigenvalues-eigenvectors']"
15,"For which values of $\ a, b $ the matrix is diagonalizable",For which values of  the matrix is diagonalizable,"\ a, b ","$$\ A = \begin{bmatrix} a & 0 & 0 \\ b & 0 & 0 \\ 1 & 2 & 1 \end{bmatrix} $$ for which values of a , b the matrix is diagonizable? in each case that $\ A $ is diagonizable, write similar diagonal matrix. My attempt: I split the solution into six cases. The characteristic polynomial of such matrix will be $\ p(x) = (a - \lambda)(1- \lambda) \lambda $ and therefore the eigen values should be $\ a , 1, 0 $ . 1) If $\ a = b $ and $\ a \not = 0,1  \Rightarrow $ 3 eigen values: $\lambda = 0,1,a $ $\ \lambda =  a, \ \ \ |I_a - A| = \begin{vmatrix} 0 & 0 & 0 \\ -a & a & 0 \\ -1 & -2 & a-1 \end{vmatrix}  \Rightarrow$ matrix rank is $\ 2 $ and therefore one eigen vector. $\ \lambda = 1 , \ \ \ |I - A | = \begin{vmatrix} -a & 0 & 0 \\ -a & 1 & 0 \\ -1 & - 2 & 0\end{vmatrix} \Rightarrow $ matrix rank is 2 again and therefore only one eigen vector $\ \lambda = 0, |0-A| = \begin{vmatrix} -a & 0 & 0 \\ -a & 0  & 0 \\ -1 & -2 & - 1  \end{vmatrix} \Rightarrow  $ matrix rank is 2 again and therefore only one eigen vector Therefore if $\ a = b $ and $\ a \not = 0 ,1 $ then the matrix is diagonizable and similar to the matrix $\ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & a \end{bmatrix} $ 2) $\ a = b $ and $\ a = 0 $ then $\ |0-A| = \begin{vmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ -1 & -2 & -1 \end{vmatrix} \Rightarrow $ Matrix rank is one and therefore eigen value $\ 0$ has two eigen vectors and the eigen value $\ 1 $ must have at least one eigen vector because geometric multiplicity is always equal or greater then algebraic multiplicity. Therefore the matrix is diagonizable and similar to $\ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} $ 3) If $\ a= b $ and $ a= 1 $ then $\ |I - A | = \begin{vmatrix} 0 & 0 & 0 \\ -1 & 1 & 0 \\ -1 & -2 & 0 \end{vmatrix} \Rightarrow $ algebraic multiplicity is $\ 2 $ while geometric is only one and therefore the matrix is not diagnosable. 4) $\ a \not = b $ and $\ a \not = 0,1 $ then $\ A  $ has three different eigenvalues and each one must have one eigenvector and because geometric multiplicity must be at least one and cannot exceed algebraic multiplicity and there the matrix is diagonizable and similar to $\ \begin{bmatrix} a & 0 & 0 \\ 0 & 0  & 0 \\ 0 & 0 & 1 \end{bmatrix} $ 5) $ a \not = b $ and $\ a = 0 $ then $\ 0 $ is eigen value with algebraic multiplicity of $\ 2 $ and so $\ |0 - A| = \begin{vmatrix} 0 &0 & 0 \\ b & 0 & 0 \\ -1 & -2 & -1 \end{vmatrix} \Rightarrow$ matrix rank is two and therefore algebraic multiplicity > geometric multiplicity and the matrix cannot be diagonalised. 6) $\ a \not = b $ and $\ a = 1 $ then $\ |I-A| = \begin{vmatrix} 0 & 0 & 0 \\ b & 1 & 0 \\ -1 & -2 & 0 \end{vmatrix} \Rightarrow $ matrix will be diagonizable only if $\ b = 1/2 $ and then geometric multiplicity of eigen value $\ 1 $ will be $\ 2 $ and then it will be similar to $\ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix} $ Thank you, for those of you who made it so far. It is a question I had on a test and I got for this answer 2/14 points. After talking to friends who also studied the subject they told me that even though the answer may be unnecessarily long, they couldn't find any flaws.","for which values of a , b the matrix is diagonizable? in each case that is diagonizable, write similar diagonal matrix. My attempt: I split the solution into six cases. The characteristic polynomial of such matrix will be and therefore the eigen values should be . 1) If and 3 eigen values: matrix rank is and therefore one eigen vector. matrix rank is 2 again and therefore only one eigen vector matrix rank is 2 again and therefore only one eigen vector Therefore if and then the matrix is diagonizable and similar to the matrix 2) and then Matrix rank is one and therefore eigen value has two eigen vectors and the eigen value must have at least one eigen vector because geometric multiplicity is always equal or greater then algebraic multiplicity. Therefore the matrix is diagonizable and similar to 3) If and then algebraic multiplicity is while geometric is only one and therefore the matrix is not diagnosable. 4) and then has three different eigenvalues and each one must have one eigenvector and because geometric multiplicity must be at least one and cannot exceed algebraic multiplicity and there the matrix is diagonizable and similar to 5) and then is eigen value with algebraic multiplicity of and so matrix rank is two and therefore algebraic multiplicity > geometric multiplicity and the matrix cannot be diagonalised. 6) and then matrix will be diagonizable only if and then geometric multiplicity of eigen value will be and then it will be similar to Thank you, for those of you who made it so far. It is a question I had on a test and I got for this answer 2/14 points. After talking to friends who also studied the subject they told me that even though the answer may be unnecessarily long, they couldn't find any flaws.","\ A = \begin{bmatrix} a & 0 & 0 \\ b & 0 & 0 \\ 1 & 2 & 1 \end{bmatrix}  \ A  \ p(x) = (a - \lambda)(1- \lambda) \lambda  \ a , 1, 0  \ a = b  \ a \not = 0,1  \Rightarrow  \lambda = 0,1,a  \ \lambda =  a, \ \ \ |I_a - A| = \begin{vmatrix} 0 & 0 & 0 \\ -a & a & 0 \\ -1 & -2 & a-1 \end{vmatrix}  \Rightarrow \ 2  \ \lambda = 1 , \ \ \ |I - A | = \begin{vmatrix} -a & 0 & 0 \\ -a & 1 & 0 \\ -1 & - 2 & 0\end{vmatrix} \Rightarrow  \ \lambda = 0, |0-A| = \begin{vmatrix} -a & 0 & 0 \\ -a & 0  & 0 \\ -1 & -2 & - 1  \end{vmatrix} \Rightarrow   \ a = b  \ a \not = 0 ,1  \ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & a \end{bmatrix}  \ a = b  \ a = 0  \ |0-A| = \begin{vmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ -1 & -2 & -1 \end{vmatrix} \Rightarrow  \ 0 \ 1  \ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}  \ a= b   a= 1  \ |I - A | = \begin{vmatrix} 0 & 0 & 0 \\ -1 & 1 & 0 \\ -1 & -2 & 0 \end{vmatrix} \Rightarrow  \ 2  \ a \not = b  \ a \not = 0,1  \ A   \ \begin{bmatrix} a & 0 & 0 \\ 0 & 0  & 0 \\ 0 & 0 & 1 \end{bmatrix}   a \not = b  \ a = 0  \ 0  \ 2  \ |0 - A| = \begin{vmatrix} 0 &0 & 0 \\ b & 0 & 0 \\ -1 & -2 & -1 \end{vmatrix} \Rightarrow \ a \not = b  \ a = 1  \ |I-A| = \begin{vmatrix} 0 & 0 & 0 \\ b & 1 & 0 \\ -1 & -2 & 0 \end{vmatrix} \Rightarrow  \ b = 1/2  \ 1  \ 2  \ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix} ","['linear-algebra', 'proof-verification']"
16,"Find the determinant of the $n\times n$ matrix $A_n$ with $(A_n)_{i,j}={n\choose |i-j|}$.",Find the determinant of the  matrix  with .,"n\times n A_n (A_n)_{i,j}={n\choose |i-j|}","I'd like to find the determinant of the matrix $A_n$ given by $(A_n)_{i,j}={n\choose |i-j|}$ for all $n\in\mathbb{Z}_{\ge 1}$ and $i,j\in\{1,2,\ldots,n\}$ . Here is what I know so far: $\det(A_n)=0$ if and only if $6\mid n$ . $2^n-1$ is an eigenvalue of all $A_n$ , with eigenvector $(1,1,\ldots,1)$ If $n$ is prime, then $\det(A_n)\equiv 1\pmod n$ If $n+1$ is prime and $n>2$ , then $\det(A_n)\equiv 0\pmod {n+1}$","I'd like to find the determinant of the matrix given by for all and . Here is what I know so far: if and only if . is an eigenvalue of all , with eigenvector If is prime, then If is prime and , then","A_n (A_n)_{i,j}={n\choose |i-j|} n\in\mathbb{Z}_{\ge 1} i,j\in\{1,2,\ldots,n\} \det(A_n)=0 6\mid n 2^n-1 A_n (1,1,\ldots,1) n \det(A_n)\equiv 1\pmod n n+1 n>2 \det(A_n)\equiv 0\pmod {n+1}","['linear-algebra', 'matrices', 'number-theory', 'binomial-coefficients', 'determinant']"
17,Find $A$ given $A^2$ and $B$ and $(A^2)^{-1} = A^{-1}B$,Find  given  and  and,A A^2 B (A^2)^{-1} = A^{-1}B,"This excersice took place in class I had today. The exercise was the following: Let the regular matrix $A$ , $B$ : $$A^2 = \left[ \begin{matrix} 2 &-2 & 2\\ -2 & 2 & 2 \\ 4&4&-4 \end{matrix} \right]  \hspace{2cm}  B = \left[ \begin{matrix} 0 &-1 & 2\\ 0 & 2 & 0 \\ 1&3&-1 \end{matrix} \right]$$ Knowing that $(A^2)^{-1} = A^{-1}B$ , find A. My Attempt During the exam I tried the following: $(A^2)^{-1} = A^{-1}B \Leftrightarrow A^2(A^2)^{-1} = A^2A^{-1}B \Leftrightarrow I_n = AB \Leftrightarrow A = B^{-1}$ And procedeed to find the inverse of B, which is: $A = B^{-1} = \left[ \begin{matrix} \frac{1}{2} &\frac{-5}{4} &  1\\ 0 & \frac{1}{2} & 0 \\ \frac{1}{2}& \frac{1}{4}&0 \end{matrix} \right]$ But, as I tend to make silly mistakes, I figured out that it would be a better way to do find $A$ just by using matrix products. We had $I_n = AB \Leftrightarrow A = A^2B$ . Using this method I got that $A = A^2B = \left[ \begin{matrix} 2 & 0 &  2\\ 2 & 12 & -6 \\ -4& -8&12 \end{matrix} \right]$ But as you can see the $2$ matrix are different. Where is the mistake in my logic? Another thing I noticed is that $I_n = AB \Leftrightarrow A*I_n*B = A*(AB)*B \Leftrightarrow I_n = AB = A^2B^2$ , but $A^2B^2$ isn't the identity matrix, so maybe the exercise is wrong, but I rather not rush into that thinking.","This excersice took place in class I had today. The exercise was the following: Let the regular matrix , : Knowing that , find A. My Attempt During the exam I tried the following: And procedeed to find the inverse of B, which is: But, as I tend to make silly mistakes, I figured out that it would be a better way to do find just by using matrix products. We had . Using this method I got that But as you can see the matrix are different. Where is the mistake in my logic? Another thing I noticed is that , but isn't the identity matrix, so maybe the exercise is wrong, but I rather not rush into that thinking.",A B A^2 = \left[ \begin{matrix} 2 &-2 & 2\\ -2 & 2 & 2 \\ 4&4&-4 \end{matrix} \right]  \hspace{2cm}  B = \left[ \begin{matrix} 0 &-1 & 2\\ 0 & 2 & 0 \\ 1&3&-1 \end{matrix} \right] (A^2)^{-1} = A^{-1}B (A^2)^{-1} = A^{-1}B \Leftrightarrow A^2(A^2)^{-1} = A^2A^{-1}B \Leftrightarrow I_n = AB \Leftrightarrow A = B^{-1} A = B^{-1} = \left[ \begin{matrix} \frac{1}{2} &\frac{-5}{4} &  1\\ 0 & \frac{1}{2} & 0 \\ \frac{1}{2}& \frac{1}{4}&0 \end{matrix} \right] A I_n = AB \Leftrightarrow A = A^2B A = A^2B = \left[ \begin{matrix} 2 & 0 &  2\\ 2 & 12 & -6 \\ -4& -8&12 \end{matrix} \right] 2 I_n = AB \Leftrightarrow A*I_n*B = A*(AB)*B \Leftrightarrow I_n = AB = A^2B^2 A^2B^2,"['linear-algebra', 'matrices', 'matrix-equations']"
18,A basis of a null space of a binary matrix,A basis of a null space of a binary matrix,,"An $m \times n$ matrix $A$ is called a binary matrix if all entries of $A$ are either $0$ or $1$ . Also, a vector $\mathbf{x}$ in $\mathbb{R}^n$ is called a trinary vector if all entries of $\mathbf{x}$ are either $-1$ , $0$ , or $1$ . I want to prove/disprove the following assertion. Given an $m \times n$ binary matrix $A$ , it is always possible to construct a basis for a null space of $A$ with trinary vectors only. I played with matlab, and didn't find any counter example at least for comparatively small $m,\, n$ . Any idean would be appreciated.","An matrix is called a binary matrix if all entries of are either or . Also, a vector in is called a trinary vector if all entries of are either , , or . I want to prove/disprove the following assertion. Given an binary matrix , it is always possible to construct a basis for a null space of with trinary vectors only. I played with matlab, and didn't find any counter example at least for comparatively small . Any idean would be appreciated.","m \times n A A 0 1 \mathbf{x} \mathbb{R}^n \mathbf{x} -1 0 1 m \times n A A m,\, n","['linear-algebra', 'matrices']"
19,"If $X \subseteq R^n$ generates the null space of $A \in M_{m,n}(R)$, does it still generate the null space when we pass to an extension ring?","If  generates the null space of , does it still generate the null space when we pass to an extension ring?","X \subseteq R^n A \in M_{m,n}(R)","Let $R$ be a commutative ring and $A$ an $m \times n$ matrix with entries in $R$ . In other, words, an $R$ -linear map $R^n \to R^m$ . Let $K =\ker(A) \subseteq R^n$ and suppose that $X$ is a finite $R$ -generating set for $K$ . Now suppose that $S$ is a commutative ring containing $R$ as a subring (I'm assuming $R$ and $S$ are unital with the same unit). We may also regard $A$ as a matrix with entries in $S$ i.e. a map $S^n \to S^m$ . So now, we have another (larger) kernel to consider. The kernel $K'$ of $A$ considered as a map $S^n \to S^m$ . Question: Is $X$ still an $S$ -generating set for $K'$ ? Or, equivalently, is $K'$ generated by $K = K' \cap R^n$ ? I only really care about the case where $X$ is finite, i.e. $K$ is finitely-generated over $R$ . In fact, $R$ can be Noetherian. I'm not sure how relevant that would be here though. In the linear algebra setting, i.e. when $R$ and $S$ are fields, this is pretty obvious. You can think of it as a consequence of the Gauss-Jordan elimination algorithm. Indeed, if you bring $A$ to echelon form over $R$ , then that must also be the echelon form over $S$ (by uniqueness) so the basis for the null space which you read off from the echelon form works for either $R$ or $S$ . I don't know much about commutative algebra, but I think this also has to hold in the case where $A$ is surjective. Then, applying $\otimes_R S$ to $$ 0 \to K \to R^n \overset{A}{\to} R^m \to 0 $$ gives $$ \cdots \to \mathrm{Tor}_1^R(R^m,S) \to \ker_R(A) \otimes_R S \to S^n \overset{A}{\to} S^m \to 0$$ and the Tor group above is zero, because $R^m$ is projective, so we see that $\ker_R(A) \otimes_R S$ coincides with $K'$ , as needed. So what do you think, does this hold in general? And, do we really need any commutative algebra tools to prove it if so? I feel like, because the statement is so transparent in the field case, if it is true in general it should hold for some fairly transparent reason.","Let be a commutative ring and an matrix with entries in . In other, words, an -linear map . Let and suppose that is a finite -generating set for . Now suppose that is a commutative ring containing as a subring (I'm assuming and are unital with the same unit). We may also regard as a matrix with entries in i.e. a map . So now, we have another (larger) kernel to consider. The kernel of considered as a map . Question: Is still an -generating set for ? Or, equivalently, is generated by ? I only really care about the case where is finite, i.e. is finitely-generated over . In fact, can be Noetherian. I'm not sure how relevant that would be here though. In the linear algebra setting, i.e. when and are fields, this is pretty obvious. You can think of it as a consequence of the Gauss-Jordan elimination algorithm. Indeed, if you bring to echelon form over , then that must also be the echelon form over (by uniqueness) so the basis for the null space which you read off from the echelon form works for either or . I don't know much about commutative algebra, but I think this also has to hold in the case where is surjective. Then, applying to gives and the Tor group above is zero, because is projective, so we see that coincides with , as needed. So what do you think, does this hold in general? And, do we really need any commutative algebra tools to prove it if so? I feel like, because the statement is so transparent in the field case, if it is true in general it should hold for some fairly transparent reason.","R A m \times n R R R^n \to R^m K =\ker(A) \subseteq R^n X R K S R R S A S S^n \to S^m K' A S^n \to S^m X S K' K' K = K' \cap R^n X K R R R S A R S R S A \otimes_R S  0 \to K \to R^n \overset{A}{\to} R^m \to 0   \cdots \to \mathrm{Tor}_1^R(R^m,S) \to \ker_R(A) \otimes_R S \to S^n \overset{A}{\to} S^m \to 0 R^m \ker_R(A) \otimes_R S K'","['linear-algebra', 'abstract-algebra', 'ring-theory', 'commutative-algebra', 'modules']"
20,Difficulty Finding $A^k$,Difficulty Finding,A^k,"Let $A= \begin{bmatrix} 1& -1 & 1\\  0 & 1 & 1 \\  0 & 0 &  1\\ \end{bmatrix}$ . Compute $A^k$ . My attempt I'm trying to compute $A^k$ using this approach as follows: $$  A=I+N= \begin{bmatrix} 1& 0 & 0\\  0 & 1 & 0 \\  0 & 0 &  1 \end{bmatrix}+ \begin{bmatrix} 0& -1 & 1\\  0 & 0 & 1 \\  0 & 0 &  0\\ \end{bmatrix} $$ with $$ N^2= \begin{bmatrix} 0& 0 & -1\\  0 & 0 & 0 \\  0 & 0 &  0\\ \end{bmatrix}, \, \text{and} \, \, N^3= \begin{bmatrix} 0& 0 & 0\\  0 & 0 & 0 \\  0 & 0 &  0\\ \end{bmatrix} $$ Then, $$ A^2=(I+N)^2=I+2N+N^2, \\ A^3=(I+N)^3=I+3N+3N^2, \\ A^4=(I+N)^4=I+4N+6N^2, \\ A^5=(I+N)^5=I+5N+10N^2, \\ A^6=(I+N)^5=I+6N+15N^2, $$ By induction, we can see $A^k=(I+N)^k=I+kN+f[k]N^2$ . But, I couldn't figure out what $f[k]$ is. Any help?","Let . Compute . My attempt I'm trying to compute using this approach as follows: with Then, By induction, we can see . But, I couldn't figure out what is. Any help?","A=
\begin{bmatrix}
1& -1 & 1\\ 
0 & 1 & 1 \\ 
0 & 0 &  1\\
\end{bmatrix} A^k A^k  
A=I+N=
\begin{bmatrix}
1& 0 & 0\\ 
0 & 1 & 0 \\ 
0 & 0 &  1
\end{bmatrix}+
\begin{bmatrix}
0& -1 & 1\\ 
0 & 0 & 1 \\ 
0 & 0 &  0\\
\end{bmatrix}
 
N^2=
\begin{bmatrix}
0& 0 & -1\\ 
0 & 0 & 0 \\ 
0 & 0 &  0\\
\end{bmatrix}, \, \text{and} \, \,
N^3=
\begin{bmatrix}
0& 0 & 0\\ 
0 & 0 & 0 \\ 
0 & 0 &  0\\
\end{bmatrix}
 
A^2=(I+N)^2=I+2N+N^2, \\
A^3=(I+N)^3=I+3N+3N^2, \\
A^4=(I+N)^4=I+4N+6N^2, \\
A^5=(I+N)^5=I+5N+10N^2, \\
A^6=(I+N)^5=I+6N+15N^2,
 A^k=(I+N)^k=I+kN+f[k]N^2 f[k]","['linear-algebra', 'matrices']"
21,Extending Cauchy-Schwarz,Extending Cauchy-Schwarz,,Is it valid to extend Cauchy-Schwarz to the 3 variable case with the following proof: $$ \sum_{i=1}^n(a_ib_i) c_i \leq \sqrt{(\sum_{i=1}^{n}a_i^2b_i^2)(\sum_{i=1}^{n}c_i^2) } \leq \sqrt{\sum_{i=1}^{n}a_i^2} \sqrt{\sum_{i=1}^{n}b_i^2} \sqrt{\sum_{i=1}^{n}c_i^2} $$ The first inequality simply applies Cauchy-Schwarz to the two variable case. The last inequality assumes $\sum_{i=1}^{n}a_i^2b_i^2\leq (\sum_{i=1}^{n}a_i^2)(\sum_{i=1}^{n}b_i^2)$ . Is this a valid assumption?,Is it valid to extend Cauchy-Schwarz to the 3 variable case with the following proof: The first inequality simply applies Cauchy-Schwarz to the two variable case. The last inequality assumes . Is this a valid assumption?,"
\sum_{i=1}^n(a_ib_i) c_i \leq \sqrt{(\sum_{i=1}^{n}a_i^2b_i^2)(\sum_{i=1}^{n}c_i^2) } \leq \sqrt{\sum_{i=1}^{n}a_i^2} \sqrt{\sum_{i=1}^{n}b_i^2} \sqrt{\sum_{i=1}^{n}c_i^2}
 \sum_{i=1}^{n}a_i^2b_i^2\leq (\sum_{i=1}^{n}a_i^2)(\sum_{i=1}^{n}b_i^2)","['linear-algebra', 'proof-verification', 'inequality']"
22,Can we characterize the equivalence classes of matrices up to left multiplication by an orthogonal matrix?,Can we characterize the equivalence classes of matrices up to left multiplication by an orthogonal matrix?,,"Let $M_n$ be the space of $n \times n$ real matrices, and consider the following equivalence relation on $M_n$: $A \sim B$ if there exist $Q \in O(n)$ such that $A=QB$. Can we characterise nicely the equivalence classes of this relation? By comparison, the usual ""orthogonal equivalence"", i.e. $A \sim B \iff A=QBU$ for some $Q,U \in O(n)$ gives rise to the notion of SVD-i.e. each equivalence class corresponds to a specific finite sequence of singular values . Thus, the singular values are the ""invariants"" which classify the equivalence classes. Is there any sensible way to associate a list of classifying invariants to the new relation in a similar way? Of course, we shall need ""more invariants"" to distinguish between different classes: It will be something like ""the same singular values+something else"". In particular, we have $A \sim B \implies \ker A=\ker B$, but for invertible matrices this does not really add any information. Even finding some non-trivial sufficient conditions for when $A \sim B$ would be interesting: Having the same singular values is certainly necessary , but is far from sufficient, which can be checked directly even at dimension $2$. ($\Sigma$ and $\Sigma Q$ are not equivalent generically).","Let $M_n$ be the space of $n \times n$ real matrices, and consider the following equivalence relation on $M_n$: $A \sim B$ if there exist $Q \in O(n)$ such that $A=QB$. Can we characterise nicely the equivalence classes of this relation? By comparison, the usual ""orthogonal equivalence"", i.e. $A \sim B \iff A=QBU$ for some $Q,U \in O(n)$ gives rise to the notion of SVD-i.e. each equivalence class corresponds to a specific finite sequence of singular values . Thus, the singular values are the ""invariants"" which classify the equivalence classes. Is there any sensible way to associate a list of classifying invariants to the new relation in a similar way? Of course, we shall need ""more invariants"" to distinguish between different classes: It will be something like ""the same singular values+something else"". In particular, we have $A \sim B \implies \ker A=\ker B$, but for invertible matrices this does not really add any information. Even finding some non-trivial sufficient conditions for when $A \sim B$ would be interesting: Having the same singular values is certainly necessary , but is far from sufficient, which can be checked directly even at dimension $2$. ($\Sigma$ and $\Sigma Q$ are not equivalent generically).",,"['linear-algebra', 'equivalence-relations', 'orthogonal-matrices', 'invariant-theory', 'singular-values']"
23,Iwasawa Decomposition Uniqueness Proof,Iwasawa Decomposition Uniqueness Proof,,"Iwasawa Decomposition (special case): Let $G=SL_n(\Bbb{R})$, $K=$ real unitary matrices, $U=$ upper triangular matrices with $1$'s on the diagonal (called unipotent ), and $A=$ diagonal matrices with positive elements ($0$ everywhere else). Then, the product map $U\times{A}\times{K}\rightarrow{G}$ given by $(u,a,k)\mapsto{uak}$ is a bijection. I have already proved the surjection of this map myself, however, when viewing Lang's proof (Undergraduate Algebra Section 6 Chapter 4 pg246) for the uniqueness of this mapping I became rather stuck and would like to understand it. Lang states the following: ""For uniqueness of the decomposition, if $g=uak=u'a'k'$, let $u_1=u^{-1}u'$, so using $g^tg$ (what does he mean by 'using'?) you get $a^{2t}u_1^{-1}=u_1a'^2$. These matrices are lower and upper triangular respectively, with diagonals $a^2,a'^2$, so $a=a'$, and finally $u_1=I$, proving uniqueness"". What does Lang mean by 'using' $g^tg$ (I presume he means applying $g^tg$ to both sides, but he is unclear...), and so therefore how does he 'get' $a^{2t}u_1^{-1}=u_1a'^2$. Thank you","Iwasawa Decomposition (special case): Let $G=SL_n(\Bbb{R})$, $K=$ real unitary matrices, $U=$ upper triangular matrices with $1$'s on the diagonal (called unipotent ), and $A=$ diagonal matrices with positive elements ($0$ everywhere else). Then, the product map $U\times{A}\times{K}\rightarrow{G}$ given by $(u,a,k)\mapsto{uak}$ is a bijection. I have already proved the surjection of this map myself, however, when viewing Lang's proof (Undergraduate Algebra Section 6 Chapter 4 pg246) for the uniqueness of this mapping I became rather stuck and would like to understand it. Lang states the following: ""For uniqueness of the decomposition, if $g=uak=u'a'k'$, let $u_1=u^{-1}u'$, so using $g^tg$ (what does he mean by 'using'?) you get $a^{2t}u_1^{-1}=u_1a'^2$. These matrices are lower and upper triangular respectively, with diagonals $a^2,a'^2$, so $a=a'$, and finally $u_1=I$, proving uniqueness"". What does Lang mean by 'using' $g^tg$ (I presume he means applying $g^tg$ to both sides, but he is unclear...), and so therefore how does he 'get' $a^{2t}u_1^{-1}=u_1a'^2$. Thank you",,"['linear-algebra', 'abstract-algebra', 'matrices', 'matrix-decomposition']"
24,Help with axler's definition of a polynomial (linear algebra done right),Help with axler's definition of a polynomial (linear algebra done right),,"I'm having trouble following Axler's polynomial definition. I don't understand the meaning of $$p\colon {\bf F}\to {\bf F}$$ on page 10 of Axler Further, how would I go about rigorously verifying that ${\cal P}({\bf F})$ is a vector space?","I'm having trouble following Axler's polynomial definition. I don't understand the meaning of $$p\colon {\bf F}\to {\bf F}$$ on page 10 of Axler Further, how would I go about rigorously verifying that ${\cal P}({\bf F})$ is a vector space?",,"['linear-algebra', 'notation']"
25,Can the gravitational force of a many-body system be calculated via matrix operations?,Can the gravitational force of a many-body system be calculated via matrix operations?,,"I'm working on a software project at work that calculates the ""gravitational attraction"" between points in 1, 2, or 3 dimensions. This is an $O(n^2)$ runtime efficiency problem, however, if I can write it as a sequence of matrix operations, then I might be able to leverage the GPU to run my simulation which would be substantially faster. My linear algebra chops aren't that great however. Is it possible to represent the mass and position as a matrix and compute the net gravitational force for each element in the matrix as a vector? Edit: The problem I'm trying to solve is this: Given an array of positioned-masses, compute the net Gravitational force that each mass experiences from the other elements in the array. I can easily do this using for-loops, but if it can be done using matrices then I can leverage the GPU for faster calculation. Non-OP edit: given masses $m_{1..n}$, positions $\underline{r}_{1..n}$. Calculate the matrix $F$, for which $F_{ij}=m_im_j\frac{\underline{r}_i-\underline{r}_j}{|\underline{r}_i-\underline{r}_j|^3}$. The goal is to do this with ""simple"" vector/matrix operations.","I'm working on a software project at work that calculates the ""gravitational attraction"" between points in 1, 2, or 3 dimensions. This is an $O(n^2)$ runtime efficiency problem, however, if I can write it as a sequence of matrix operations, then I might be able to leverage the GPU to run my simulation which would be substantially faster. My linear algebra chops aren't that great however. Is it possible to represent the mass and position as a matrix and compute the net gravitational force for each element in the matrix as a vector? Edit: The problem I'm trying to solve is this: Given an array of positioned-masses, compute the net Gravitational force that each mass experiences from the other elements in the array. I can easily do this using for-loops, but if it can be done using matrices then I can leverage the GPU for faster calculation. Non-OP edit: given masses $m_{1..n}$, positions $\underline{r}_{1..n}$. Calculate the matrix $F$, for which $F_{ij}=m_im_j\frac{\underline{r}_i-\underline{r}_j}{|\underline{r}_i-\underline{r}_j|^3}$. The goal is to do this with ""simple"" vector/matrix operations.",,"['linear-algebra', 'matrices']"
26,Commutative subalgebra of matrix-algebra,Commutative subalgebra of matrix-algebra,,"Let $M_3$ be the $\mathbb{C}$- vector space of matrices of order 3 with complex entries. What is the dimension of subspace $V$ of $M_3$ satisfying the following both conditions? (a) $AB=BA$ if $A,B\in V$. (b) If $W$ is a subspace of $M_3$ satisfying $W\supset V$ and $W \neq V$, then there exist $A,B\in W$ such that $AB\neq BA$.","Let $M_3$ be the $\mathbb{C}$- vector space of matrices of order 3 with complex entries. What is the dimension of subspace $V$ of $M_3$ satisfying the following both conditions? (a) $AB=BA$ if $A,B\in V$. (b) If $W$ is a subspace of $M_3$ satisfying $W\supset V$ and $W \neq V$, then there exist $A,B\in W$ such that $AB\neq BA$.",,"['linear-algebra', 'matrices']"
27,Applying Sherman-Morrison-Woodbury to obtain rank 2 update,Applying Sherman-Morrison-Woodbury to obtain rank 2 update,,"I was reading Nocedal and Wright, and it is stated that one may use the Sherman-Morrison-Woodbury formula  $$(A+ YGZ^*)^{-1} = A^{-1} - A^{-1}Y(G^{-1}+Z^*A^{-1}Y)^{-1}Z^*A^{-1}$$ On the hessian $H_{k+1}$ inverse approximation from the BFGS:  $$ H_{k+1}=  (I - \gamma s_k y_k ^T)H_k(I-\gamma_k y_k s_k^T) + \gamma_k s_k s_k^T$$ To obtain an update formula for the Hessian $B_{k+1}$ for the BFGS: $$B_{k+1} = B_k - \frac{B_ks_ks_k^T B_k}{s_k^TB_k s_k } + \gamma_k y_ky_k^T $$ Where the setting here is $\gamma_k = \frac{1}{y_k^Ts_k}$ and $H_k = B_k^{-1}$ and $H_k$ and $B_k$ are positive definite symmetric matrices. My issue is that I do not see how to apply the Sherman-Morrison-Woodbury formula. What I see is that I want  $H_{k+1} = A+ YGZ^*$ and that would give me $B_{k+1}$ Then the question is what is $A$, $Y$, $G$, and $Z$. My thoughts are that $A \not = \gamma_k y_ky_k^T$ as that is singular. Thus that leaves $A = (I - \gamma s_k y_k ^T)H_k(I-\gamma_k y_k s_k^T)$, but that is a mess to invert and when I tried it I did not get anything resembling the desired expression and also then what do I define $G,Y,Z$ to be?","I was reading Nocedal and Wright, and it is stated that one may use the Sherman-Morrison-Woodbury formula  $$(A+ YGZ^*)^{-1} = A^{-1} - A^{-1}Y(G^{-1}+Z^*A^{-1}Y)^{-1}Z^*A^{-1}$$ On the hessian $H_{k+1}$ inverse approximation from the BFGS:  $$ H_{k+1}=  (I - \gamma s_k y_k ^T)H_k(I-\gamma_k y_k s_k^T) + \gamma_k s_k s_k^T$$ To obtain an update formula for the Hessian $B_{k+1}$ for the BFGS: $$B_{k+1} = B_k - \frac{B_ks_ks_k^T B_k}{s_k^TB_k s_k } + \gamma_k y_ky_k^T $$ Where the setting here is $\gamma_k = \frac{1}{y_k^Ts_k}$ and $H_k = B_k^{-1}$ and $H_k$ and $B_k$ are positive definite symmetric matrices. My issue is that I do not see how to apply the Sherman-Morrison-Woodbury formula. What I see is that I want  $H_{k+1} = A+ YGZ^*$ and that would give me $B_{k+1}$ Then the question is what is $A$, $Y$, $G$, and $Z$. My thoughts are that $A \not = \gamma_k y_ky_k^T$ as that is singular. Thus that leaves $A = (I - \gamma s_k y_k ^T)H_k(I-\gamma_k y_k s_k^T)$, but that is a mess to invert and when I tried it I did not get anything resembling the desired expression and also then what do I define $G,Y,Z$ to be?",,"['linear-algebra', 'matrices', 'optimization']"
28,Inertia of the matrix $\frac{1}{(i+j)!}$,Inertia of the matrix,\frac{1}{(i+j)!},"Let $A$ be a matrix defined as $a_{ij} = \frac{1}{(i+j)!}$ of order $n\times n$, then find the inertia of $A$ for every $n\geq 1$. Can you find the sign of determinant of $A$ for every $n$? We define inertia of a square matrix $A$ as $(\mu(A),\delta(A),\nu(A))$, where $\mu(A),\delta(A),\nu(A)$ denote number of positive, zero and negative eigenvalues of $A$. By using Mathematica we have a conjecture for it as follows :    Inertia of $A_{n\times n}$ = $(\frac{n}{2},0,\frac{n}{2})$, if n is even and $(\frac{n+1}{2}$,0,$\frac{n-1}{2}$) \,  if n is is odd. I tried to solve this and reached a conclusion that if I could find sign of $det(A_{n\times n})$ for every n then I am done. So, Can anyone help me for finding sign of $detA$ or inertia of $A$ directly? Thanks in advance.","Let $A$ be a matrix defined as $a_{ij} = \frac{1}{(i+j)!}$ of order $n\times n$, then find the inertia of $A$ for every $n\geq 1$. Can you find the sign of determinant of $A$ for every $n$? We define inertia of a square matrix $A$ as $(\mu(A),\delta(A),\nu(A))$, where $\mu(A),\delta(A),\nu(A)$ denote number of positive, zero and negative eigenvalues of $A$. By using Mathematica we have a conjecture for it as follows :    Inertia of $A_{n\times n}$ = $(\frac{n}{2},0,\frac{n}{2})$, if n is even and $(\frac{n+1}{2}$,0,$\frac{n-1}{2}$) \,  if n is is odd. I tried to solve this and reached a conclusion that if I could find sign of $det(A_{n\times n})$ for every n then I am done. So, Can anyone help me for finding sign of $detA$ or inertia of $A$ directly? Thanks in advance.",,"['linear-algebra', 'matrices', 'functional-analysis', 'matrix-calculus', 'matrix-decomposition']"
29,Inverse of doubly stochastic matrix $M$ is doubly stochastic iff $M$ is a permution matrix,Inverse of doubly stochastic matrix  is doubly stochastic iff  is a permution matrix,M M,"In a very short remark, my syllabus states that if $M\in\Omega_n$ (i.e. the set of doubly stochastic matrices) and $M$ is invertible, that $M^{-1}\in\Omega_n$ if and only if $M$ is a permutation matrix. I'm having trouble to see why this is so obvious. By definition of a doubly stochastic matrix, $Me=e$ and $e^TM=e^T$, and thus $M^{-1}Me=M^{-1}e=e$, and similarly $e^TM^{-1}=e^T$. But of course, we do not know that all matrix elements are non-negative, and by trying a few examples it indeed turns out that (for all matrices I tried), doubly stochastic matrices which are not permutation matrices do not have non-negative inverses. For the given remark, I'm trying to prove the equivalence. The reverse implication is easy: $S_n$ is a group, and $h:S_n\to GL_n(\mathbb{R}):\sigma\mapsto\Pi_{\sigma}$ is a group homomorphism, and it follows directly that the inverse of a permutation matrix will also be a permutation matrix. However, I don't see how to prove the forward implication. Any help is much appreciated!","In a very short remark, my syllabus states that if $M\in\Omega_n$ (i.e. the set of doubly stochastic matrices) and $M$ is invertible, that $M^{-1}\in\Omega_n$ if and only if $M$ is a permutation matrix. I'm having trouble to see why this is so obvious. By definition of a doubly stochastic matrix, $Me=e$ and $e^TM=e^T$, and thus $M^{-1}Me=M^{-1}e=e$, and similarly $e^TM^{-1}=e^T$. But of course, we do not know that all matrix elements are non-negative, and by trying a few examples it indeed turns out that (for all matrices I tried), doubly stochastic matrices which are not permutation matrices do not have non-negative inverses. For the given remark, I'm trying to prove the equivalence. The reverse implication is easy: $S_n$ is a group, and $h:S_n\to GL_n(\mathbb{R}):\sigma\mapsto\Pi_{\sigma}$ is a group homomorphism, and it follows directly that the inverse of a permutation matrix will also be a permutation matrix. However, I don't see how to prove the forward implication. Any help is much appreciated!",,"['linear-algebra', 'abstract-algebra']"
30,Proving $BAx = x$ for $B$ pseudo inverse of $A$,Proving  for  pseudo inverse of,BAx = x B A,"Suppose $A$ is square but not necessarily invertible and has SVD $\displaystyle A=\sum_{i=1}^r \sigma_i u_i v_i^T$. Let$$ B=\sum_{i=1}^r \frac{1}{\sigma_i}v_i u_i^T.$$   Show that $BA\,x = x$ for all $x$ in the span of the right-singular vectors of $A$. For this reason $B$ is sometimes called the pseudo-inverse of $A$ and can play the role of $A^{-1}$ in many applications. What I have done so far: \begin{align*} ABx &= \left(\sum_{i=1}^r \sigma_i u_i v_i^T\right)\left(\sum_{i=1}^r \frac{1}{\sigma_i}v_i u_i^T\right)x\\  &= (\sigma_1u_1v_1^T + \sigma_2u_2v_2^T + \cdots + \sigma_ru_rv_r^T)\left(\frac{1}{\sigma_1}v_iu_i^T + \cdots + \frac{1}{\sigma_r}v_ru_r^T\right)x\\ &= (u_1u_1^T + u_2u_2^T + \cdots + u_ru_r^T)x,\end{align*} and that is where I am stuck. I tried turning all the $u_iu_i^T$ to $\dfrac{Av_iv_i^TA^T}{\|Av_i\|_2^2}$, but it does not lead me anywhere. Help would be appreciated.","Suppose $A$ is square but not necessarily invertible and has SVD $\displaystyle A=\sum_{i=1}^r \sigma_i u_i v_i^T$. Let$$ B=\sum_{i=1}^r \frac{1}{\sigma_i}v_i u_i^T.$$   Show that $BA\,x = x$ for all $x$ in the span of the right-singular vectors of $A$. For this reason $B$ is sometimes called the pseudo-inverse of $A$ and can play the role of $A^{-1}$ in many applications. What I have done so far: \begin{align*} ABx &= \left(\sum_{i=1}^r \sigma_i u_i v_i^T\right)\left(\sum_{i=1}^r \frac{1}{\sigma_i}v_i u_i^T\right)x\\  &= (\sigma_1u_1v_1^T + \sigma_2u_2v_2^T + \cdots + \sigma_ru_rv_r^T)\left(\frac{1}{\sigma_1}v_iu_i^T + \cdots + \frac{1}{\sigma_r}v_ru_r^T\right)x\\ &= (u_1u_1^T + u_2u_2^T + \cdots + u_ru_r^T)x,\end{align*} and that is where I am stuck. I tried turning all the $u_iu_i^T$ to $\dfrac{Av_iv_i^TA^T}{\|Av_i\|_2^2}$, but it does not lead me anywhere. Help would be appreciated.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'svd']"
31,"If $AX=0 \Rightarrow BX=0$, prove that there is $C$ such that $B=CA$","If , prove that there is  such that",AX=0 \Rightarrow BX=0 C B=CA,"Let $A,B$ be $3 \times 3$ real matrices such that, for any vector $v$, if $Av=0$ then we also have $Bv=0$. Prove that there is $C$ such that $B=CA$. If $A$ is invertible, we can get $C=BA^{-1}$. Now let's say that $A$ is not invertible so $\det A=0$. This means that the system of linear equations $Ax=0$ has a nonzero solution. But this means that also $Bx=0$ has a nonzero solution so $\det B=0$. Let $v \neq 0$ such that $Av=Bv=0$. Then $(A+B)v=(A-B)v=0$ which gives $\det(A+B)=\det(A-B)=0$. Using $$\det(A+\lambda B)=\det A +p\lambda +q \lambda^2+(\det B)\cdot \lambda^3=p\lambda+q\lambda^2$$ and these results gives $p=q=0$, so $\det(A+\lambda B)=0, \forall \lambda \in \mathbb{C}$. This is where I got stuck. Is there any way to finish from here?","Let $A,B$ be $3 \times 3$ real matrices such that, for any vector $v$, if $Av=0$ then we also have $Bv=0$. Prove that there is $C$ such that $B=CA$. If $A$ is invertible, we can get $C=BA^{-1}$. Now let's say that $A$ is not invertible so $\det A=0$. This means that the system of linear equations $Ax=0$ has a nonzero solution. But this means that also $Bx=0$ has a nonzero solution so $\det B=0$. Let $v \neq 0$ such that $Av=Bv=0$. Then $(A+B)v=(A-B)v=0$ which gives $\det(A+B)=\det(A-B)=0$. Using $$\det(A+\lambda B)=\det A +p\lambda +q \lambda^2+(\det B)\cdot \lambda^3=p\lambda+q\lambda^2$$ and these results gives $p=q=0$, so $\det(A+\lambda B)=0, \forall \lambda \in \mathbb{C}$. This is where I got stuck. Is there any way to finish from here?",,"['linear-algebra', 'matrices', 'determinant']"
32,Determinant using factor theorem,Determinant using factor theorem,,"Prove   $$\Delta=\begin{vmatrix} (y+z)^2 & x^2 & x^2 \\ y^2 & (z+x)^2 & y^2 \\ z^2 & z^2 & (x+y)^2 \\ \end{vmatrix} = 2xyz(x+y+z)^3$$   using factor theorem. This is solved in Demonstrate using determinant properties that the determinant of A is equal to $2abc(a+b+c)^3$ using factor theorem. My Attempt: $$ x=0\text{ or }y=0\text{ or }z=0\implies\Delta=0\text{ , So $x,y,z$ are factors of }\Delta.\\ (x+y+z)=0\implies \Delta=\begin{vmatrix}x^2&x^2&x^2\\y^2&y^2&y^2\\z^2&z^2&z^2\end{vmatrix}=0\text{ , So $(x+y+z)$ is a factor of $\Delta$.} $$ $\color{black}{\text{But how do i extract the remaining term $(x+y+z)^2$ to prove $\Delta=2xyz(x+y+z)^3$ }\color{red}{ ?}}$ Similar Example: Please check answer of @user348749 in How to solve this determinant , $$ \Delta'=\begin{vmatrix} (b+c)^2&ab&ca\\ ab&(a+c)^2&bc\\ ac&bc&(a+b)^2 \end{vmatrix}=2abc(a+b+c)^3 $$ it is said that $$ (a+b+c)=0\implies\Delta'=\begin{align*} \begin{vmatrix} c^2 & ca & bc \\  ca & a^2 & ab \\ bc & ab & b^2 \\  \end{vmatrix} =abc\begin{vmatrix} c & a & b \\  c & a & b \\ c & a & b \\  \end{vmatrix}  \end{align*}=0 $$ ""Since all rows are identical, $(a+b+c)^2$ is a factor. The determinant is a polynomial of degree 6 and hence the remaining factor is linear and since it is symmetric, the factor must be $k(a+b+c)$."" $\color{black}{\text{How can we say this }\color{red}{ ?}}$ My Understanding: If the problem was similar to this, answer of @Saibal in Factorise a matrix using the factor theorem , $$\Delta''= \begin{vmatrix} x&y&z\\ x^2&y^2&z^2\\ x^3&y^3&z^3\\ \end{vmatrix}$$ I could without doubt do as below: $$ x=0\text{ or }y=0\text{ or }z=0\implies\Delta''=0\\ x=y\text{ or }y=z\text{ or }z=x\implies\Delta''=0 $$ Thus, $x,y,z,(x-y),(y-z),(z-x)$ are factors of $\Delta''$. ie. $\Delta''=kxyz(x-y)(y-z)(z-x)$","Prove   $$\Delta=\begin{vmatrix} (y+z)^2 & x^2 & x^2 \\ y^2 & (z+x)^2 & y^2 \\ z^2 & z^2 & (x+y)^2 \\ \end{vmatrix} = 2xyz(x+y+z)^3$$   using factor theorem. This is solved in Demonstrate using determinant properties that the determinant of A is equal to $2abc(a+b+c)^3$ using factor theorem. My Attempt: $$ x=0\text{ or }y=0\text{ or }z=0\implies\Delta=0\text{ , So $x,y,z$ are factors of }\Delta.\\ (x+y+z)=0\implies \Delta=\begin{vmatrix}x^2&x^2&x^2\\y^2&y^2&y^2\\z^2&z^2&z^2\end{vmatrix}=0\text{ , So $(x+y+z)$ is a factor of $\Delta$.} $$ $\color{black}{\text{But how do i extract the remaining term $(x+y+z)^2$ to prove $\Delta=2xyz(x+y+z)^3$ }\color{red}{ ?}}$ Similar Example: Please check answer of @user348749 in How to solve this determinant , $$ \Delta'=\begin{vmatrix} (b+c)^2&ab&ca\\ ab&(a+c)^2&bc\\ ac&bc&(a+b)^2 \end{vmatrix}=2abc(a+b+c)^3 $$ it is said that $$ (a+b+c)=0\implies\Delta'=\begin{align*} \begin{vmatrix} c^2 & ca & bc \\  ca & a^2 & ab \\ bc & ab & b^2 \\  \end{vmatrix} =abc\begin{vmatrix} c & a & b \\  c & a & b \\ c & a & b \\  \end{vmatrix}  \end{align*}=0 $$ ""Since all rows are identical, $(a+b+c)^2$ is a factor. The determinant is a polynomial of degree 6 and hence the remaining factor is linear and since it is symmetric, the factor must be $k(a+b+c)$."" $\color{black}{\text{How can we say this }\color{red}{ ?}}$ My Understanding: If the problem was similar to this, answer of @Saibal in Factorise a matrix using the factor theorem , $$\Delta''= \begin{vmatrix} x&y&z\\ x^2&y^2&z^2\\ x^3&y^3&z^3\\ \end{vmatrix}$$ I could without doubt do as below: $$ x=0\text{ or }y=0\text{ or }z=0\implies\Delta''=0\\ x=y\text{ or }y=z\text{ or }z=x\implies\Delta''=0 $$ Thus, $x,y,z,(x-y),(y-z),(z-x)$ are factors of $\Delta''$. ie. $\Delta''=kxyz(x-y)(y-z)(z-x)$",,"['linear-algebra', 'matrices', 'determinant', 'factoring']"
33,Prove that this $3 \times 3$ determinant vanishes,Prove that this  determinant vanishes,3 \times 3,"Let $a,b,c,d,e,f \in \mathbb{R}$. Prove that    $$ \begin{vmatrix} (a+b)de-(d+e)ab & de-ab & a+b-d-e \\ (b+c)ef-(e+f)bc & ef-bc & b+c-e-f \\ (c+d)fa-(f+a)cd & fa-cd & c+d-f-a \end{vmatrix}=0 $$ I tried to write the determinant as  $$ \begin{vmatrix} x_1x_3-x_2x_4 & x_3-x_4 & x_1-x_2 \\ y_1y_3-y_2y_4 & y_3-y_4 & y_1-y_2 \\ z_1z_3-z_2z_4 & z_3-z_4 & z_1-z_2 \end{vmatrix} $$ where $x_1=a+b, x_2=d+e,x_3=de, x_4=ab$ and the other cyclical ones are defined similarly. But now I have $12$ variables instead of $6$, so they depend on one another somehow. I tried to expand it, but it doesn't look too good.","Let $a,b,c,d,e,f \in \mathbb{R}$. Prove that    $$ \begin{vmatrix} (a+b)de-(d+e)ab & de-ab & a+b-d-e \\ (b+c)ef-(e+f)bc & ef-bc & b+c-e-f \\ (c+d)fa-(f+a)cd & fa-cd & c+d-f-a \end{vmatrix}=0 $$ I tried to write the determinant as  $$ \begin{vmatrix} x_1x_3-x_2x_4 & x_3-x_4 & x_1-x_2 \\ y_1y_3-y_2y_4 & y_3-y_4 & y_1-y_2 \\ z_1z_3-z_2z_4 & z_3-z_4 & z_1-z_2 \end{vmatrix} $$ where $x_1=a+b, x_2=d+e,x_3=de, x_4=ab$ and the other cyclical ones are defined similarly. But now I have $12$ variables instead of $6$, so they depend on one another somehow. I tried to expand it, but it doesn't look too good.",,"['linear-algebra', 'matrices', 'determinant']"
34,Injection from dual space into double dual space,Injection from dual space into double dual space,,"I know for a vector space $V$ over the real or complex numbers, there exists a canonical embedding into its double dual $V^{**}$, and if $V$ is given an inner product, then there is a canonical embedding from $V$ into $V^*$. However, I was not sure if there is any canonical embedding from $V^*$ into $V^{**}$ that is compatible with the above emebeddings, i.e. if $$\sigma:V \rightarrow V^{**}, \sigma(v)(f)=f(v)$$ $$\iota:V \rightarrow V^*, \iota(v)(w)=\langle w,v \rangle$$ are injective linear map. So my question is: Is there a canonical (does not depend on basis) injective linear map  $\mu: V^* \rightarrow V^{**}$ such that $$\sigma = \mu \circ \iota$$ I try to build an inner product on $V^*$ based on the inner product on $V$ but to no avail.","I know for a vector space $V$ over the real or complex numbers, there exists a canonical embedding into its double dual $V^{**}$, and if $V$ is given an inner product, then there is a canonical embedding from $V$ into $V^*$. However, I was not sure if there is any canonical embedding from $V^*$ into $V^{**}$ that is compatible with the above emebeddings, i.e. if $$\sigma:V \rightarrow V^{**}, \sigma(v)(f)=f(v)$$ $$\iota:V \rightarrow V^*, \iota(v)(w)=\langle w,v \rangle$$ are injective linear map. So my question is: Is there a canonical (does not depend on basis) injective linear map  $\mu: V^* \rightarrow V^{**}$ such that $$\sigma = \mu \circ \iota$$ I try to build an inner product on $V^*$ based on the inner product on $V$ but to no avail.",,"['linear-algebra', 'linear-transformations']"
35,"Representation theory of $\mathrm{GL}(n,\mathbb{K})$ where $\mathbb{K}$ is a (not necessarily algebraically closed) field of characteristic zero",Representation theory of  where  is a (not necessarily algebraically closed) field of characteristic zero,"\mathrm{GL}(n,\mathbb{K}) \mathbb{K}","I would like to know about irreducible representations of $\mathrm{GL}(n,\mathbb{K})$ where $\mathbb{K}$ is a field of characteristic zero, and topics such as irreducible representations of  $\mathrm{GL}(n,\mathbb{K})$, Young tableaux, Schur functors, Schur-Weyl duality etc. Unfortunately, essentially every representation theory textbook I have looked at only discusses representations over $\mathbb{C}$, and does not give any indication whether any of the results generalize to other fields, and if they do, how to extract such information from the complex rep theory. Is there anywhere I could learn about representation theory of the general linear group over other fields of characteristic zero, which are not necessarily algebraically closed? Even just $\mathbb{K}=\mathbb{R}$ would suffice since that is the case I am most interested in.","I would like to know about irreducible representations of $\mathrm{GL}(n,\mathbb{K})$ where $\mathbb{K}$ is a field of characteristic zero, and topics such as irreducible representations of  $\mathrm{GL}(n,\mathbb{K})$, Young tableaux, Schur functors, Schur-Weyl duality etc. Unfortunately, essentially every representation theory textbook I have looked at only discusses representations over $\mathbb{C}$, and does not give any indication whether any of the results generalize to other fields, and if they do, how to extract such information from the complex rep theory. Is there anywhere I could learn about representation theory of the general linear group over other fields of characteristic zero, which are not necessarily algebraically closed? Even just $\mathbb{K}=\mathbb{R}$ would suffice since that is the case I am most interested in.",,"['linear-algebra', 'group-theory', 'reference-request', 'representation-theory', 'lie-groups']"
36,"Is Hermitian matrix a self-adjoint operator w.r.t. just the standard inner product, or any inner product?","Is Hermitian matrix a self-adjoint operator w.r.t. just the standard inner product, or any inner product?",,"Given a linear map $L:V→V$ over an inner product space $(V,〈⟩)$, the adjoint of $L$, denoted by $L^*$, is a linear map $L^*:V→V$ s.t. $〈Lx,y⟩=〈x,L^* y⟩$ for any $x,y\in V$. $L$ is self adjoint if $L=L^*$ and then $〈Lx,y⟩=〈x,Ly⟩$. Just consider $V=\Bbb C^n$. I am confused by Hermtian matrix. It is easy to prove a Hermitian matrix $H$ is self-adjoint under the standard inner product $〈x,y⟩=y^*x$, by $〈Hx,y⟩=y^*Hx=(H^*y)^*x=(Hy)^*x=〈x,Hy⟩$, but I am not sure if it is actually self adjoint for any inner product. If not, it would be great if someone could help give a counterexample.","Given a linear map $L:V→V$ over an inner product space $(V,〈⟩)$, the adjoint of $L$, denoted by $L^*$, is a linear map $L^*:V→V$ s.t. $〈Lx,y⟩=〈x,L^* y⟩$ for any $x,y\in V$. $L$ is self adjoint if $L=L^*$ and then $〈Lx,y⟩=〈x,Ly⟩$. Just consider $V=\Bbb C^n$. I am confused by Hermtian matrix. It is easy to prove a Hermitian matrix $H$ is self-adjoint under the standard inner product $〈x,y⟩=y^*x$, by $〈Hx,y⟩=y^*Hx=(H^*y)^*x=(Hy)^*x=〈x,Hy⟩$, but I am not sure if it is actually self adjoint for any inner product. If not, it would be great if someone could help give a counterexample.",,"['linear-algebra', 'matrices', 'operator-theory']"
37,Prove that $J^+AJ$ has positive eigenvalues,Prove that  has positive eigenvalues,J^+AJ,"Let $J \in \mathbb{R}$ be a $m \times n$ matrix of rank $n$ and a $A \in \mathbb{R}$ be a $m \times m$ diagonal positive definite matrix. Denote by $J^+$ a pseudoinverse of $J$. As was shown in this question $J^+AJ$ is not positive definite in general. However, I was wondering whether one can prove or disprove that $J^+AJ$ has real and positive eigenvalues.","Let $J \in \mathbb{R}$ be a $m \times n$ matrix of rank $n$ and a $A \in \mathbb{R}$ be a $m \times m$ diagonal positive definite matrix. Denote by $J^+$ a pseudoinverse of $J$. As was shown in this question $J^+AJ$ is not positive definite in general. However, I was wondering whether one can prove or disprove that $J^+AJ$ has real and positive eigenvalues.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'pseudoinverse']"
38,Find number of non-singular matrices,Find number of non-singular matrices,,"If $w$ is complex cube root of unity $w \ne 1$ then find number of non-singular matrices of the form   $$A=\begin{bmatrix} 1 & a & b\\   w&1  &c \\  w^2 &w  & 1 \end{bmatrix}$$ such that $a,b,c$ takes values from the set $S=\left\{w,w^2\right\}$ My Try: we have $$\det(A)=acw^2-(a+c)w+1$$. $\det(A)=0$ only if $ac=1$ and $a+c=-1$ since $1+w+w^2=0$. So $a=w $ ad $c=w^2$ OR $a=w^2$ and $c=w$. But since $\det(A)$ is independent of $b$ The final possible triplets $(a,b,c)$ are $1.$ $(w,w^2,w^2)$ $2.$ $(w^2,w,w)$ $3.$ $(w,w,w^2)$ $4.$ $(w^2,w^2,w)$ So total number of singular matrices is $4$. But total number of matrices $A$ is $8$. Hence total number of non-singular matrices is $8-4=4$. But my book answer is $2$. May I know where is the mistake?","If $w$ is complex cube root of unity $w \ne 1$ then find number of non-singular matrices of the form   $$A=\begin{bmatrix} 1 & a & b\\   w&1  &c \\  w^2 &w  & 1 \end{bmatrix}$$ such that $a,b,c$ takes values from the set $S=\left\{w,w^2\right\}$ My Try: we have $$\det(A)=acw^2-(a+c)w+1$$. $\det(A)=0$ only if $ac=1$ and $a+c=-1$ since $1+w+w^2=0$. So $a=w $ ad $c=w^2$ OR $a=w^2$ and $c=w$. But since $\det(A)$ is independent of $b$ The final possible triplets $(a,b,c)$ are $1.$ $(w,w^2,w^2)$ $2.$ $(w^2,w,w)$ $3.$ $(w,w,w^2)$ $4.$ $(w^2,w^2,w)$ So total number of singular matrices is $4$. But total number of matrices $A$ is $8$. Hence total number of non-singular matrices is $8-4=4$. But my book answer is $2$. May I know where is the mistake?",,"['linear-algebra', 'matrices', 'determinant']"
39,Is $T(p(x))=p(x+1)$ a linear map if $T:\mathbb{R}[x]\to\mathbb{R}[x]$?,Is  a linear map if ?,T(p(x))=p(x+1) T:\mathbb{R}[x]\to\mathbb{R}[x],Given $T:\mathbb{R}[x]\to\mathbb{R}[x]$ is $T(p(x))=p(x+1)$ a linear map? $\Bbb{R}[x] $ is a space of all polynomials with coefficients over $\mathbb{R}$. It seems too easy so I suspect I'm doing something wrong but let: $$ T(\alpha p(x) + \beta q(x))=(\alpha p + \beta q)(x+1)=\\ =\alpha p(x+1)+\beta q(x+1) = \alpha T(p(x))+\beta T(q(x)) $$ therefore it's a linear map. Is it correct?,Given $T:\mathbb{R}[x]\to\mathbb{R}[x]$ is $T(p(x))=p(x+1)$ a linear map? $\Bbb{R}[x] $ is a space of all polynomials with coefficients over $\mathbb{R}$. It seems too easy so I suspect I'm doing something wrong but let: $$ T(\alpha p(x) + \beta q(x))=(\alpha p + \beta q)(x+1)=\\ =\alpha p(x+1)+\beta q(x+1) = \alpha T(p(x))+\beta T(q(x)) $$ therefore it's a linear map. Is it correct?,,"['linear-algebra', 'proof-verification', 'linear-transformations']"
40,Inner product of dual basis,Inner product of dual basis,,"Assume $V$ is an inner product space over $\mathbb{R}$ with inner product $\left<\cdot,\cdot\right>$. Let $u_1,\cdots,u_n$ be a basis of $V$ and $v_1,\cdots,v_n$ be the dual basis, i.e., $\left<u_i,v_j\right> = \delta_{ij}$. Prove that if $\left<u_i,u_j\right>\leq 0$ for all $1\leq i < j \leq n$, then $\left<v_i,v_j\right>\geq 0$ for all $1\leq i < j \leq n$. If we let $B=(b_{ij})$ be the matrix from basis $\{v_i\}$ to $\{u_i\}$, that is, $(u_1,\cdots,u_n) = (v_1,\cdots,v_n)B$, then $u_j = \sum_{i=1}^n b_{ij}v_i$, hence $\left< u_j, u_k \right> \leq 0$ and the duality imply $\left< u_j, u_k \right> = \left< \sum_{i=1}^n b_{ij}v_i, u_k \right> = \left<  b_{kj}v_k, u_k \right> = b_{kj} \leq 0$ for $k\neq j$. Now $(v_1,\cdots,v_n) = (u_1,\cdots,u_n) B^{-1}$, if we let $B^{-1} = (c_{ij})$ then $v_j = \sum_{k=1}^n c_{kj}u_k$, so $\left< v_i, v_j \right> = \left< v_i, \sum_{k=1}^n c_{kj}u_k \right> = \left< v_i, c_{ij}u_i \right> = c_{ij}$ for $i\neq j$. Since $BB^{-1} = I_n$, we have $\sum_{k=1}^n b_{ik}c_{kj} = \delta_{ij}$. But it seems difficult to determine whether $c_{ij}\geq 0$ from this condition. How should I do next?","Assume $V$ is an inner product space over $\mathbb{R}$ with inner product $\left<\cdot,\cdot\right>$. Let $u_1,\cdots,u_n$ be a basis of $V$ and $v_1,\cdots,v_n$ be the dual basis, i.e., $\left<u_i,v_j\right> = \delta_{ij}$. Prove that if $\left<u_i,u_j\right>\leq 0$ for all $1\leq i < j \leq n$, then $\left<v_i,v_j\right>\geq 0$ for all $1\leq i < j \leq n$. If we let $B=(b_{ij})$ be the matrix from basis $\{v_i\}$ to $\{u_i\}$, that is, $(u_1,\cdots,u_n) = (v_1,\cdots,v_n)B$, then $u_j = \sum_{i=1}^n b_{ij}v_i$, hence $\left< u_j, u_k \right> \leq 0$ and the duality imply $\left< u_j, u_k \right> = \left< \sum_{i=1}^n b_{ij}v_i, u_k \right> = \left<  b_{kj}v_k, u_k \right> = b_{kj} \leq 0$ for $k\neq j$. Now $(v_1,\cdots,v_n) = (u_1,\cdots,u_n) B^{-1}$, if we let $B^{-1} = (c_{ij})$ then $v_j = \sum_{k=1}^n c_{kj}u_k$, so $\left< v_i, v_j \right> = \left< v_i, \sum_{k=1}^n c_{kj}u_k \right> = \left< v_i, c_{ij}u_i \right> = c_{ij}$ for $i\neq j$. Since $BB^{-1} = I_n$, we have $\sum_{k=1}^n b_{ik}c_{kj} = \delta_{ij}$. But it seems difficult to determine whether $c_{ij}\geq 0$ from this condition. How should I do next?",,"['linear-algebra', 'matrices']"
41,Lattice embeddings of a polygon,Lattice embeddings of a polygon,,"Consider the four lattice polygons below.  Each shape is over the coordinates. If reflected or flipped on the major axes and diagonals, these four polygons remain distinct.  However, this is the same polygon each time. The last three are rotations of the first, with the rotation matrix built from the arctan of different pythagorean triple based fractions: -12/5, 63/16, and 4/3. One way to determine if a lattice polygon has a different rotational embedding is to apply all of the arctan pythagorean fraction rotation matrices and to see if the points stay on the lattice.  Is there an easier method?","Consider the four lattice polygons below.  Each shape is over the coordinates. If reflected or flipped on the major axes and diagonals, these four polygons remain distinct.  However, this is the same polygon each time. The last three are rotations of the first, with the rotation matrix built from the arctan of different pythagorean triple based fractions: -12/5, 63/16, and 4/3. One way to determine if a lattice polygon has a different rotational embedding is to apply all of the arctan pythagorean fraction rotation matrices and to see if the points stay on the lattice.  Is there an easier method?",,"['linear-algebra', 'combinatorics', 'geometry', 'polygons']"
42,Simplifying a dot and cross product expression,Simplifying a dot and cross product expression,,"I am trying to solve or simplify $$ \left[ \,f\cdot \frac{(u\times v)}{||u\times v||_2} \right]\frac{(u\times v)}{||u\times v||_2} = 0 $$ where $f$ is a unit vector and $u,v$ are vectors. Is there a way to simplify this? I have been trying to use some of the triple product identities e.g. ${\displaystyle (\mathbf {a} \cdot (\mathbf {b} \times \mathbf {c} ))\,\mathbf {a} =(\mathbf {a} \times \mathbf {b} )\times (\mathbf {a} \times \mathbf {c} )}$, but have only really made things more complicated.","I am trying to solve or simplify $$ \left[ \,f\cdot \frac{(u\times v)}{||u\times v||_2} \right]\frac{(u\times v)}{||u\times v||_2} = 0 $$ where $f$ is a unit vector and $u,v$ are vectors. Is there a way to simplify this? I have been trying to use some of the triple product identities e.g. ${\displaystyle (\mathbf {a} \cdot (\mathbf {b} \times \mathbf {c} ))\,\mathbf {a} =(\mathbf {a} \times \mathbf {b} )\times (\mathbf {a} \times \mathbf {c} )}$, but have only really made things more complicated.",,"['linear-algebra', 'vectors', 'orthogonality', 'vector-fields', 'cross-product']"
43,Determinant in inner product space,Determinant in inner product space,,"Looking at the vector space $\Bbb C^n$ with the standart inner product, Let $v_1,...v_n \in \Bbb C^n$ and $A \in M_n(\Bbb C)$ a matrix with columns $v_1,...,v_n$. Prove that: $$ \lvert detA\rvert \leq \prod_{j=1}^n ||v_j||$$ Furthermore, prove that equality holds if and only if $v_1,...,v_n$ is an orthogonal sequence.","Looking at the vector space $\Bbb C^n$ with the standart inner product, Let $v_1,...v_n \in \Bbb C^n$ and $A \in M_n(\Bbb C)$ a matrix with columns $v_1,...,v_n$. Prove that: $$ \lvert detA\rvert \leq \prod_{j=1}^n ||v_j||$$ Furthermore, prove that equality holds if and only if $v_1,...,v_n$ is an orthogonal sequence.",,"['linear-algebra', 'determinant', 'inner-products', 'orthogonality']"
44,Echelon Form and Reduced Row Echelon Form differences and when to use,Echelon Form and Reduced Row Echelon Form differences and when to use,,"I have a quick question regarding the difference between echelon form and reduced row echelon form (rref). According to my googling these seem to be the same, but to me it seems that the difference between the two is that echelon form only requires the first value of the first row to be 1. The first non-zero value of each row after the first one can be any value as long as it's not 0. As for reduced row echelon form every first non-zero value of a row has to be 1. Am I correct with this or am I completely mixing things up? If I'm correct, when would the echelon form be fine and when do I have to use RREF? Thanks :)","I have a quick question regarding the difference between echelon form and reduced row echelon form (rref). According to my googling these seem to be the same, but to me it seems that the difference between the two is that echelon form only requires the first value of the first row to be 1. The first non-zero value of each row after the first one can be any value as long as it's not 0. As for reduced row echelon form every first non-zero value of a row has to be 1. Am I correct with this or am I completely mixing things up? If I'm correct, when would the echelon form be fine and when do I have to use RREF? Thanks :)",,"['linear-algebra', 'matrices']"
45,Finding rotation of 3 given lines in 3D until intersection with 3 other given lines,Finding rotation of 3 given lines in 3D until intersection with 3 other given lines,,"I already posted a similar question. but since the problem is still unanswered and the details are a bit changed, I'm posting it again. I'm trying to solve the following mathematical problem: Given 2 sets of 3 lines each in 3D space (where each line is given by a $3D$ point and a $3D$ vector): $$ set1=\{ [(px1,py1,pz1), (vx1,vy1,vz1)], [(px2,py2,pz2),(vx2,vy2,vz2)], [(px3,py3,pz3), (vx3,vy3,vz3)] \} $$ $$ set2=\{ [(qx1,qy1,qz1), (ux1,uy1,uz1)], [(qx2,qy2,qz2),(ux2,uy2,uz2)],  [(qx3,qy3,qz3), (ux3,uy3,uz3)] \} $$ find the angles of rotation $x,y,z$ (in radians) such that: $$R=rotate\_x*rotate\_y*rotate\_z$$ $$ {rotate\_x} = \left( {\matrix{    {1} & {0} & {0}  \cr     {0} & {cos(x)} & {-sin(x)}  \cr     {0} & {sin(x)} & {cos(x)}  \cr   } } \right) $$ $$ {rotate\_y} = \left( {\matrix{    {cos(y)} & {0} & {sin(y)}  \cr     {0} & {1} & {0}  \cr     {-sin(y)} & {0} & {cos(y)}  \cr   } } \right) $$ $$ {rotate\_z} = \left( {\matrix{    {cos(z)} & {-sin(z)} & {0}  \cr     {sin(z)} & {cos(z)} & {0}  \cr     {0} & {0} & {1}  \cr   } } \right) $$ and if we rotate $set1$ by $R$ then all the lines in $set1$ will intersect their corresponding lines in $set2$. Meaning, let's define $set1\_r$ as $set1$ after the above rotation. Then line $i$ $(i=1,2,3)$ in $set1\_r$ will intersect line $i$  in $set2$. I already built the 3 equations with 3 variables that needed to be solved. but they seem to be unsolvable. Here are the equations (only the angles $x,y,z$ are unknown and the rest are known): eq1 = qx1*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + qy1*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + ((ux1*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + uy1*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + uz1*cos(x)*cos(y))*(px1 - qz1*sin(y) - qx1*cos(y)*cos(z) + qy1*cos(y)*sin(z) + (vx1*(qx1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py1 + qy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz1*cos(y)*sin(x) + ((ux1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz1*cos(y)*sin(x))*(px1 - qz1*sin(y) - qx1*cos(y)*cos(z) + qy1*cos(y)*sin(z)))/(uz1*sin(y) + ux1*cos(y)*cos(z) - uy1*cos(y)*sin(z))))/(vy1 - (vx1*(ux1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz1*cos(y)*sin(x)))/(uz1*sin(y) + ux1*cos(y)*cos(z) - uy1*cos(y)*sin(z)))))/(uz1*sin(y) + ux1*cos(y)*cos(z) - uy1*cos(y)*sin(z)) + qz1*cos(x)*cos(y) == pz1 + (vz1*(qx1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py1 + qy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz1*cos(y)*sin(x) + ((ux1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz1*cos(y)*sin(x))*(px1 - qz1*sin(y) - qx1*cos(y)*cos(z) + qy1*cos(y)*sin(z)))/(uz1*sin(y) + ux1*cos(y)*cos(z) - uy1*cos(y)*sin(z))))/(vy1 - (vx1*(ux1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz1*cos(y)*sin(x)))/(uz1*sin(y) + ux1*cos(y)*cos(z) - uy1*cos(y)*sin(z))) eq2 = qx2*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + qy2*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + ((ux2*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + uy2*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + uz2*cos(x)*cos(y))*(px2 - qz2*sin(y) - qx2*cos(y)*cos(z) + qy2*cos(y)*sin(z) + (vx2*(qx2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py2 + qy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz2*cos(y)*sin(x) + ((ux2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz2*cos(y)*sin(x))*(px2 - qz2*sin(y) - qx2*cos(y)*cos(z) + qy2*cos(y)*sin(z)))/(uz2*sin(y) + ux2*cos(y)*cos(z) - uy2*cos(y)*sin(z))))/(vy2 - (vx2*(ux2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz2*cos(y)*sin(x)))/(uz2*sin(y) + ux2*cos(y)*cos(z) - uy2*cos(y)*sin(z)))))/(uz2*sin(y) + ux2*cos(y)*cos(z) - uy2*cos(y)*sin(z)) + qz2*cos(x)*cos(y) == pz2 + (vz2*(qx2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py2 + qy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz2*cos(y)*sin(x) + ((ux2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz2*cos(y)*sin(x))*(px2 - qz2*sin(y) - qx2*cos(y)*cos(z) + qy2*cos(y)*sin(z)))/(uz2*sin(y) + ux2*cos(y)*cos(z) - uy2*cos(y)*sin(z))))/(vy2 - (vx2*(ux2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz2*cos(y)*sin(x)))/(uz2*sin(y) + ux2*cos(y)*cos(z) - uy2*cos(y)*sin(z))) eq3 = qx3*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + qy3*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + ((ux3*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + uy3*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + uz3*cos(x)*cos(y))*(px3 - qz3*sin(y) - qx3*cos(y)*cos(z) + qy3*cos(y)*sin(z) + (vx3*(qx3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py3 + qy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz3*cos(y)*sin(x) + ((ux3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz3*cos(y)*sin(x))*(px3 - qz3*sin(y) - qx3*cos(y)*cos(z) + qy3*cos(y)*sin(z)))/(uz3*sin(y) + ux3*cos(y)*cos(z) - uy3*cos(y)*sin(z))))/(vy3 - (vx3*(ux3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz3*cos(y)*sin(x)))/(uz3*sin(y) + ux3*cos(y)*cos(z) - uy3*cos(y)*sin(z)))))/(uz3*sin(y) + ux3*cos(y)*cos(z) - uy3*cos(y)*sin(z)) + qz3*cos(x)*cos(y) == pz3 + (vz3*(qx3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py3 + qy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz3*cos(y)*sin(x) + ((ux3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz3*cos(y)*sin(x))*(px3 - qz3*sin(y) - qx3*cos(y)*cos(z) + qy3*cos(y)*sin(z)))/(uz3*sin(y) + ux3*cos(y)*cos(z) - uy3*cos(y)*sin(z))))/(vy3 - (vx3*(ux3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz3*cos(y)*sin(x)))/(uz3*sin(y) + ux3*cos(y)*cos(z) - uy3*cos(y)*sin(z))) It can be seen that it's almost impossible (hopefully not impossible) to extract $x,y,z$ from the equations since there are many multiplications between $sin$ and $cos$ of the different angles. I couldn't solve it. and I also tried it in matlab using the 'solve' function but it didn't work. Can anyone suggest a solution for finding the angles $x,y,z$? or any other way of finding the matrix $R$? If it helps, here's a specific example of the 2 sets of lines in which I know the desired angles $x,y,z$ (there are probably many more triplets of angles that also solve the problem). %set1 px1=0; py1=0; pz1=-30;  px2=0; py2=0; pz2=-30;  px3=0;  py3=0; pz3=-30;  vx1 = -0.083717247687439;  vy1 = -0.107930827800543;  vz1 = 0.990627255252918;  vx2 = 0.076364294519742; vy2 = 0.060269029165473;  vz2 = 0.995256820446840;  vx3 = -0.081460429387834;  vy3 = 0.105021268850622;  vz3 = 0.991128009660183;   %set2 qx1=0;  qy1=0; qz1=-30;  qx2=0;  qy2=0; qz2=-30;  qx3=0;  qy3=0;  qz3=-30;  ux1 = -0.079382581863774; uy1 = -0.095259098236529;  uz1 = 0.992282273297173;  ux2 = 0.079382581863774;  uy2 = 0.095259098236529;  uz2 = 0.992282273297173;  ux3 = -0.086165283952334;  uy3 = 0.103398340742801;  uz3 = 0.990900765451843; For the above example, for $x=0.5236$ ($x$ is 30 degrees), $y=0, z=0$,  meaning, $$ {R} = \left( {\matrix{    {1} & {0} & {0}  \cr     {0} & {0.866} & {-0.5}  \cr     {0} & {0.5} & {0.866}  \cr   } } \right) $$ $line$ $i$ ($i=1,2,3$) in $set1$ will intersect $line$ $i$ in $set2$. Also, $x=0, y=0, z=0$ ($R$ is the identity matrix) is also a solution since all the $6$ lines already intersect at the point $(0,0,-30)$. And I'm sure there are many more solutions. But, of course I need the general solution. Hope anyone can solve it since I've been trying to solve it for weeks :(. Thanks","I already posted a similar question. but since the problem is still unanswered and the details are a bit changed, I'm posting it again. I'm trying to solve the following mathematical problem: Given 2 sets of 3 lines each in 3D space (where each line is given by a $3D$ point and a $3D$ vector): $$ set1=\{ [(px1,py1,pz1), (vx1,vy1,vz1)], [(px2,py2,pz2),(vx2,vy2,vz2)], [(px3,py3,pz3), (vx3,vy3,vz3)] \} $$ $$ set2=\{ [(qx1,qy1,qz1), (ux1,uy1,uz1)], [(qx2,qy2,qz2),(ux2,uy2,uz2)],  [(qx3,qy3,qz3), (ux3,uy3,uz3)] \} $$ find the angles of rotation $x,y,z$ (in radians) such that: $$R=rotate\_x*rotate\_y*rotate\_z$$ $$ {rotate\_x} = \left( {\matrix{    {1} & {0} & {0}  \cr     {0} & {cos(x)} & {-sin(x)}  \cr     {0} & {sin(x)} & {cos(x)}  \cr   } } \right) $$ $$ {rotate\_y} = \left( {\matrix{    {cos(y)} & {0} & {sin(y)}  \cr     {0} & {1} & {0}  \cr     {-sin(y)} & {0} & {cos(y)}  \cr   } } \right) $$ $$ {rotate\_z} = \left( {\matrix{    {cos(z)} & {-sin(z)} & {0}  \cr     {sin(z)} & {cos(z)} & {0}  \cr     {0} & {0} & {1}  \cr   } } \right) $$ and if we rotate $set1$ by $R$ then all the lines in $set1$ will intersect their corresponding lines in $set2$. Meaning, let's define $set1\_r$ as $set1$ after the above rotation. Then line $i$ $(i=1,2,3)$ in $set1\_r$ will intersect line $i$  in $set2$. I already built the 3 equations with 3 variables that needed to be solved. but they seem to be unsolvable. Here are the equations (only the angles $x,y,z$ are unknown and the rest are known): eq1 = qx1*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + qy1*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + ((ux1*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + uy1*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + uz1*cos(x)*cos(y))*(px1 - qz1*sin(y) - qx1*cos(y)*cos(z) + qy1*cos(y)*sin(z) + (vx1*(qx1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py1 + qy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz1*cos(y)*sin(x) + ((ux1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz1*cos(y)*sin(x))*(px1 - qz1*sin(y) - qx1*cos(y)*cos(z) + qy1*cos(y)*sin(z)))/(uz1*sin(y) + ux1*cos(y)*cos(z) - uy1*cos(y)*sin(z))))/(vy1 - (vx1*(ux1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz1*cos(y)*sin(x)))/(uz1*sin(y) + ux1*cos(y)*cos(z) - uy1*cos(y)*sin(z)))))/(uz1*sin(y) + ux1*cos(y)*cos(z) - uy1*cos(y)*sin(z)) + qz1*cos(x)*cos(y) == pz1 + (vz1*(qx1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py1 + qy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz1*cos(y)*sin(x) + ((ux1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz1*cos(y)*sin(x))*(px1 - qz1*sin(y) - qx1*cos(y)*cos(z) + qy1*cos(y)*sin(z)))/(uz1*sin(y) + ux1*cos(y)*cos(z) - uy1*cos(y)*sin(z))))/(vy1 - (vx1*(ux1*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy1*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz1*cos(y)*sin(x)))/(uz1*sin(y) + ux1*cos(y)*cos(z) - uy1*cos(y)*sin(z))) eq2 = qx2*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + qy2*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + ((ux2*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + uy2*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + uz2*cos(x)*cos(y))*(px2 - qz2*sin(y) - qx2*cos(y)*cos(z) + qy2*cos(y)*sin(z) + (vx2*(qx2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py2 + qy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz2*cos(y)*sin(x) + ((ux2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz2*cos(y)*sin(x))*(px2 - qz2*sin(y) - qx2*cos(y)*cos(z) + qy2*cos(y)*sin(z)))/(uz2*sin(y) + ux2*cos(y)*cos(z) - uy2*cos(y)*sin(z))))/(vy2 - (vx2*(ux2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz2*cos(y)*sin(x)))/(uz2*sin(y) + ux2*cos(y)*cos(z) - uy2*cos(y)*sin(z)))))/(uz2*sin(y) + ux2*cos(y)*cos(z) - uy2*cos(y)*sin(z)) + qz2*cos(x)*cos(y) == pz2 + (vz2*(qx2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py2 + qy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz2*cos(y)*sin(x) + ((ux2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz2*cos(y)*sin(x))*(px2 - qz2*sin(y) - qx2*cos(y)*cos(z) + qy2*cos(y)*sin(z)))/(uz2*sin(y) + ux2*cos(y)*cos(z) - uy2*cos(y)*sin(z))))/(vy2 - (vx2*(ux2*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy2*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz2*cos(y)*sin(x)))/(uz2*sin(y) + ux2*cos(y)*cos(z) - uy2*cos(y)*sin(z))) eq3 = qx3*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + qy3*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + ((ux3*(sin(x)*sin(z) - cos(x)*cos(z)*sin(y)) + uy3*(cos(z)*sin(x) + cos(x)*sin(y)*sin(z)) + uz3*cos(x)*cos(y))*(px3 - qz3*sin(y) - qx3*cos(y)*cos(z) + qy3*cos(y)*sin(z) + (vx3*(qx3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py3 + qy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz3*cos(y)*sin(x) + ((ux3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz3*cos(y)*sin(x))*(px3 - qz3*sin(y) - qx3*cos(y)*cos(z) + qy3*cos(y)*sin(z)))/(uz3*sin(y) + ux3*cos(y)*cos(z) - uy3*cos(y)*sin(z))))/(vy3 - (vx3*(ux3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz3*cos(y)*sin(x)))/(uz3*sin(y) + ux3*cos(y)*cos(z) - uy3*cos(y)*sin(z)))))/(uz3*sin(y) + ux3*cos(y)*cos(z) - uy3*cos(y)*sin(z)) + qz3*cos(x)*cos(y) == pz3 + (vz3*(qx3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) - py3 + qy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - qz3*cos(y)*sin(x) + ((ux3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz3*cos(y)*sin(x))*(px3 - qz3*sin(y) - qx3*cos(y)*cos(z) + qy3*cos(y)*sin(z)))/(uz3*sin(y) + ux3*cos(y)*cos(z) - uy3*cos(y)*sin(z))))/(vy3 - (vx3*(ux3*(cos(x)*sin(z) + cos(z)*sin(x)*sin(y)) + uy3*(cos(x)*cos(z) - sin(x)*sin(y)*sin(z)) - uz3*cos(y)*sin(x)))/(uz3*sin(y) + ux3*cos(y)*cos(z) - uy3*cos(y)*sin(z))) It can be seen that it's almost impossible (hopefully not impossible) to extract $x,y,z$ from the equations since there are many multiplications between $sin$ and $cos$ of the different angles. I couldn't solve it. and I also tried it in matlab using the 'solve' function but it didn't work. Can anyone suggest a solution for finding the angles $x,y,z$? or any other way of finding the matrix $R$? If it helps, here's a specific example of the 2 sets of lines in which I know the desired angles $x,y,z$ (there are probably many more triplets of angles that also solve the problem). %set1 px1=0; py1=0; pz1=-30;  px2=0; py2=0; pz2=-30;  px3=0;  py3=0; pz3=-30;  vx1 = -0.083717247687439;  vy1 = -0.107930827800543;  vz1 = 0.990627255252918;  vx2 = 0.076364294519742; vy2 = 0.060269029165473;  vz2 = 0.995256820446840;  vx3 = -0.081460429387834;  vy3 = 0.105021268850622;  vz3 = 0.991128009660183;   %set2 qx1=0;  qy1=0; qz1=-30;  qx2=0;  qy2=0; qz2=-30;  qx3=0;  qy3=0;  qz3=-30;  ux1 = -0.079382581863774; uy1 = -0.095259098236529;  uz1 = 0.992282273297173;  ux2 = 0.079382581863774;  uy2 = 0.095259098236529;  uz2 = 0.992282273297173;  ux3 = -0.086165283952334;  uy3 = 0.103398340742801;  uz3 = 0.990900765451843; For the above example, for $x=0.5236$ ($x$ is 30 degrees), $y=0, z=0$,  meaning, $$ {R} = \left( {\matrix{    {1} & {0} & {0}  \cr     {0} & {0.866} & {-0.5}  \cr     {0} & {0.5} & {0.866}  \cr   } } \right) $$ $line$ $i$ ($i=1,2,3$) in $set1$ will intersect $line$ $i$ in $set2$. Also, $x=0, y=0, z=0$ ($R$ is the identity matrix) is also a solution since all the $6$ lines already intersect at the point $(0,0,-30)$. And I'm sure there are many more solutions. But, of course I need the general solution. Hope anyone can solve it since I've been trying to solve it for weeks :(. Thanks",,"['calculus', 'linear-algebra', 'matrices', 'geometry', 'rotations']"
46,"In the Collatz conjecture, why are $\max(\textrm{collatz}(n))$ and $\textrm{var}(\textrm{collatz}(n))$ so closely related?","In the Collatz conjecture, why are  and  so closely related?",\max(\textrm{collatz}(n)) \textrm{var}(\textrm{collatz}(n)),"Like the question title reads, in the Collatz conjecture, why are $\max(\textrm{collatz}(n))$ and $\textrm{var}(\textrm{collatz}(n))$ so closely related? See the Figure below for a log-log plot. I refer with $\textrm{collatz}(n)$ to a sequence starting from $n$ (so $\max(\textrm{collatz}(n))$ is the maximum value of the sequence etc). EDIT: oops, I just noticed thet x- and y-labels are in wrong order. Maximum value is on the x-axis. The plot shows the data for sequences starting with $n = 1, ..., 100000$.","Like the question title reads, in the Collatz conjecture, why are $\max(\textrm{collatz}(n))$ and $\textrm{var}(\textrm{collatz}(n))$ so closely related? See the Figure below for a log-log plot. I refer with $\textrm{collatz}(n)$ to a sequence starting from $n$ (so $\max(\textrm{collatz}(n))$ is the maximum value of the sequence etc). EDIT: oops, I just noticed thet x- and y-labels are in wrong order. Maximum value is on the x-axis. The plot shows the data for sequences starting with $n = 1, ..., 100000$.",,"['linear-algebra', 'sequences-and-series', 'number-theory', 'variance', 'collatz-conjecture']"
47,Finding the value of the determinant $|M^2+MN^2|$ and determining whether $U$ is a zero matrix or not,Finding the value of the determinant  and determining whether  is a zero matrix or not,|M^2+MN^2| U,"Let $M$ and $N$ be two $3\times 3$ matrices such that $MN=NM$. Further   if $M\neq N^2$ and $M^2=N^4$ then what is/are the possible value(s) of   the determinant $|M^2+MN^2|$? Also if there is a $3\times 3$ matrix   $U$ such that $(M^2+MN^2)U=O$ can we say whether $U$ is a zero matrix or   a non-zero matrix ? My Attempt: $|M|=\pm|N|^2$ (from $M^2=N^4$) $(M-N)(N-M)=M^2-N^2$ (as $MN=NM$) $M^2=N^4$ $\implies MMN=N^5$ $\implies NMMN=N^6$ $\implies MNMN=N^6$ $\implies (MN)^2=N^6$ Similarly, $M^6=(MN)^4$ These are what I could deduce from the given information. How should I proceed from here to solve the question?","Let $M$ and $N$ be two $3\times 3$ matrices such that $MN=NM$. Further   if $M\neq N^2$ and $M^2=N^4$ then what is/are the possible value(s) of   the determinant $|M^2+MN^2|$? Also if there is a $3\times 3$ matrix   $U$ such that $(M^2+MN^2)U=O$ can we say whether $U$ is a zero matrix or   a non-zero matrix ? My Attempt: $|M|=\pm|N|^2$ (from $M^2=N^4$) $(M-N)(N-M)=M^2-N^2$ (as $MN=NM$) $M^2=N^4$ $\implies MMN=N^5$ $\implies NMMN=N^6$ $\implies MNMN=N^6$ $\implies (MN)^2=N^6$ Similarly, $M^6=(MN)^4$ These are what I could deduce from the given information. How should I proceed from here to solve the question?",,['linear-algebra']
48,Find the rational canonical form of a matrix from its minimal and characteristic polynomials,Find the rational canonical form of a matrix from its minimal and characteristic polynomials,,"What is the rational canonical form of $A$?   $$A=\begin{bmatrix}     1       & 1 & 0 & 0 \\     0       & 1 & 0 & 0 \\     2      & 3 & -1 & 4\\    1     & 1 & -1 & 3\\ \end{bmatrix}$$ I found that the minimal polynomial $m_A(x)=(x-1)^2$ and the characteristic polynomial $c_A(x)=(x-1)^4$. Therefore the invariant factors can be  $$x-1,x-1,(x-1)^2$$ or $$(x-1)^2,(x-1)^2$$ Therefore the rational canonical form may be $$\begin{bmatrix}     1      & 0 & 0 & 0 \\     0       & 1 & 0 & 0 \\     0     & 0 & 0 & -1\\   0    & 0 & 1 & 2\\ \end{bmatrix}$$ or $$\begin{bmatrix}     0      & -1 & 0 & 0 \\     1       & 2 & 0 & 0 \\     0     & 0 & 0 & -1\\   0    & 0 & 1 & 2\\ \end{bmatrix}$$ How do I quickly figure out which one is the correct one?","What is the rational canonical form of $A$?   $$A=\begin{bmatrix}     1       & 1 & 0 & 0 \\     0       & 1 & 0 & 0 \\     2      & 3 & -1 & 4\\    1     & 1 & -1 & 3\\ \end{bmatrix}$$ I found that the minimal polynomial $m_A(x)=(x-1)^2$ and the characteristic polynomial $c_A(x)=(x-1)^4$. Therefore the invariant factors can be  $$x-1,x-1,(x-1)^2$$ or $$(x-1)^2,(x-1)^2$$ Therefore the rational canonical form may be $$\begin{bmatrix}     1      & 0 & 0 & 0 \\     0       & 1 & 0 & 0 \\     0     & 0 & 0 & -1\\   0    & 0 & 1 & 2\\ \end{bmatrix}$$ or $$\begin{bmatrix}     0      & -1 & 0 & 0 \\     1       & 2 & 0 & 0 \\     0     & 0 & 0 & -1\\   0    & 0 & 1 & 2\\ \end{bmatrix}$$ How do I quickly figure out which one is the correct one?",,"['linear-algebra', 'abstract-algebra']"
49,The rotations of bidiagonal matrices,The rotations of bidiagonal matrices,,"Let $B$ is upper bidiagonal matrix. Consider the algorithm: $ B_0 = B, C_k = B_{k-1}Q_k, B_k = Z_kC_k $, where $B_k$ is upper bidiagonal, $C_k$ is lower bidiagonal and $Q_k$ and $Z_k$ are unitary. So at each iteration, we change the upper bidiagonal matrix to the lower and vice versa. Prove that matrices $B_k$ and $C_k$ converges to the same diagonal matrix. Really I have no ideas how to prove it. I suppose that matrices $Q_k$ and $Z_k$ can be implemented as a Givens rotations. But maybe there are another variants of transforms $Q_k$ and $Z_k$. Also I noticed that after each iteration the absolute value of $\{B_k\}_{11}$ and $\{C_k\}_{11}$ is growing because the unitary transform saves the second norm. And because of the same reasons the absolute value of $\{B_k\}_{nn}$ and $\{C_k\}_{nn}$ is decreasing. But I can not say anything about the intermediate diagonal elements. Maybe this statements is a part of some algorithm of constructing SVD, but I do not know this algorithm. Thanks for any help of ideas!","Let $B$ is upper bidiagonal matrix. Consider the algorithm: $ B_0 = B, C_k = B_{k-1}Q_k, B_k = Z_kC_k $, where $B_k$ is upper bidiagonal, $C_k$ is lower bidiagonal and $Q_k$ and $Z_k$ are unitary. So at each iteration, we change the upper bidiagonal matrix to the lower and vice versa. Prove that matrices $B_k$ and $C_k$ converges to the same diagonal matrix. Really I have no ideas how to prove it. I suppose that matrices $Q_k$ and $Z_k$ can be implemented as a Givens rotations. But maybe there are another variants of transforms $Q_k$ and $Z_k$. Also I noticed that after each iteration the absolute value of $\{B_k\}_{11}$ and $\{C_k\}_{11}$ is growing because the unitary transform saves the second norm. And because of the same reasons the absolute value of $\{B_k\}_{nn}$ and $\{C_k\}_{nn}$ is decreasing. But I can not say anything about the intermediate diagonal elements. Maybe this statements is a part of some algorithm of constructing SVD, but I do not know this algorithm. Thanks for any help of ideas!",,"['linear-algebra', 'matrices', 'convergence-divergence', 'numerical-methods', 'svd']"
50,This algorithm will find a basis for the span of some vectors. How/why does it work?,This algorithm will find a basis for the span of some vectors. How/why does it work?,,"Say I want to find a basis for $span((1,2,5),(2,4,10),(-3,-5,-13),(2,1,4),(-4,-6,-16))$. Google tells me that to get the answer, I'm supposed to write down the vectors as columns of a matrix: $$         \begin{pmatrix}         1 &  2 &  -3 & 2 & -4 \\         2 &  4 &  -5 & 1 & -6 \\         5 & 10 & -13 & 4 & -16 \\         \end{pmatrix} $$ ... then bring said matrix to row echelon form... $$         \begin{pmatrix}         1 &  2 & 0 & -7 & 2 \\         0 &  0 & 1 & -3 & 2 \\         0 &  0 & 0 &  0 & 0 \\         \end{pmatrix} $$ ... then look at the pivots; in this case the first and third rows contain pivots, therefore the first and third columns of the original matrix are linearly independent while the others are linearly dependent, and therefore those two vectors are a basis of the span. Okay, that's simple enough to remember and use in the exam but it still feels like magic to me. How does this work? Why do the pivots show which vectors are linearly independent?","Say I want to find a basis for $span((1,2,5),(2,4,10),(-3,-5,-13),(2,1,4),(-4,-6,-16))$. Google tells me that to get the answer, I'm supposed to write down the vectors as columns of a matrix: $$         \begin{pmatrix}         1 &  2 &  -3 & 2 & -4 \\         2 &  4 &  -5 & 1 & -6 \\         5 & 10 & -13 & 4 & -16 \\         \end{pmatrix} $$ ... then bring said matrix to row echelon form... $$         \begin{pmatrix}         1 &  2 & 0 & -7 & 2 \\         0 &  0 & 1 & -3 & 2 \\         0 &  0 & 0 &  0 & 0 \\         \end{pmatrix} $$ ... then look at the pivots; in this case the first and third rows contain pivots, therefore the first and third columns of the original matrix are linearly independent while the others are linearly dependent, and therefore those two vectors are a basis of the span. Okay, that's simple enough to remember and use in the exam but it still feels like magic to me. How does this work? Why do the pivots show which vectors are linearly independent?",,['linear-algebra']
51,Real Matrices with Complex Eigenvalues and Eigenvectors: Clockwise or Counterclockwise?,Real Matrices with Complex Eigenvalues and Eigenvectors: Clockwise or Counterclockwise?,,"Say I am interested in a linear operator $A: \mathbb{R}^2 \to \mathbb{R}^2$ who's matrix in a standard orthonormal basis is $\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$. $A$ acts on the real plane as a rigid rotation by 90 degrees counter clockwise. The two pairs of eigenvalues and eigenvectors are $\left(i, \begin{bmatrix} 1 \\ -i \end{bmatrix} \right)$ and $\left(-i, \begin{bmatrix} 1 \\ i \end{bmatrix} \right)$. I can pick either eigenvector and take the real and imaginary parts to define a basis for $\mathbb{R}^2$. Now, the eigenvalue $i$ represents rotation by $90$ degrees counterclockwise, but $-i$ represents rotation by $90$ degrees clockwise. So how am I supposed to determine, given two eigenvalue-eigenvector pairs, what direction the rotation is in for a real operator on a real vector space? Thank you.","Say I am interested in a linear operator $A: \mathbb{R}^2 \to \mathbb{R}^2$ who's matrix in a standard orthonormal basis is $\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$. $A$ acts on the real plane as a rigid rotation by 90 degrees counter clockwise. The two pairs of eigenvalues and eigenvectors are $\left(i, \begin{bmatrix} 1 \\ -i \end{bmatrix} \right)$ and $\left(-i, \begin{bmatrix} 1 \\ i \end{bmatrix} \right)$. I can pick either eigenvector and take the real and imaginary parts to define a basis for $\mathbb{R}^2$. Now, the eigenvalue $i$ represents rotation by $90$ degrees counterclockwise, but $-i$ represents rotation by $90$ degrees clockwise. So how am I supposed to determine, given two eigenvalue-eigenvector pairs, what direction the rotation is in for a real operator on a real vector space? Thank you.",,"['linear-algebra', 'complex-analysis']"
52,Show that $P = A(A^TA)^{-1}A^T$ is a projection matrix.,Show that  is a projection matrix.,P = A(A^TA)^{-1}A^T,"Let $A\in \mathbb{R^{m\times n}}$ have full column rank, $b\in \mathbb{R^m}$ with $b \neq 0$ Show that $P = A(A^TA)^{-1}A^T$ is a projection matrix. Onto what space does $P$ project? I tried looking at videos,at textbooks, and lecture notes. Which version best answers the question? version 1: Since $ATA$ is an invertible matrix, take the normal equation $A^TAx=A^Tb$ and solve for $x$. We get, $x=(A^TA)^{-1}A^Tb$. For $Ax=Pb$, we have $Ax$ as a projection, and $P$ is an orthogonal projector onto the range of $A$. It follows, $A(A^TA)^{-1}A^Tb = Pb$, $\forall b$. Generally, for any matrices $M$ and $N$ s.t. $Mx=Nx$,$\forall x \Rightarrow M=N$, thus $P=A(A^TA)^{-1}A^T$. $P$ projects a vector $b$ onto the spaced spanned by the columns of $A$. version 2 Given a matrix $A$, the projection of $b$ onto $C(A)$ is $P=Ax$, where $x$ solves $A^TAx=A^Tb$. Since A has full column rank, $A^TA$ is invertible. We write, the projection matrix is $P=A(A^TA)^{-1}A^T$. version 3 The vector $Ax$ is always in the column space of $A$. We project b onto a vector $P$ in the column space of $A$ and solve $Ax=P$. In matrix form, $A^T(b-Ax)=0 \Rightarrow A^TAx=A^Tb$. Since $A^TA$ is a square matrix, we multiply by $(A^TA)^{-1}$ In $n$ dimensions, $x=(A^TA)^{-1}A^Tb \Rightarrow P=Ax=A(A^TA)^{-1}A^Tb \Rightarrow P=A(A^TA)^{-1}A^T$","Let $A\in \mathbb{R^{m\times n}}$ have full column rank, $b\in \mathbb{R^m}$ with $b \neq 0$ Show that $P = A(A^TA)^{-1}A^T$ is a projection matrix. Onto what space does $P$ project? I tried looking at videos,at textbooks, and lecture notes. Which version best answers the question? version 1: Since $ATA$ is an invertible matrix, take the normal equation $A^TAx=A^Tb$ and solve for $x$. We get, $x=(A^TA)^{-1}A^Tb$. For $Ax=Pb$, we have $Ax$ as a projection, and $P$ is an orthogonal projector onto the range of $A$. It follows, $A(A^TA)^{-1}A^Tb = Pb$, $\forall b$. Generally, for any matrices $M$ and $N$ s.t. $Mx=Nx$,$\forall x \Rightarrow M=N$, thus $P=A(A^TA)^{-1}A^T$. $P$ projects a vector $b$ onto the spaced spanned by the columns of $A$. version 2 Given a matrix $A$, the projection of $b$ onto $C(A)$ is $P=Ax$, where $x$ solves $A^TAx=A^Tb$. Since A has full column rank, $A^TA$ is invertible. We write, the projection matrix is $P=A(A^TA)^{-1}A^T$. version 3 The vector $Ax$ is always in the column space of $A$. We project b onto a vector $P$ in the column space of $A$ and solve $Ax=P$. In matrix form, $A^T(b-Ax)=0 \Rightarrow A^TAx=A^Tb$. Since $A^TA$ is a square matrix, we multiply by $(A^TA)^{-1}$ In $n$ dimensions, $x=(A^TA)^{-1}A^Tb \Rightarrow P=Ax=A(A^TA)^{-1}A^Tb \Rightarrow P=A(A^TA)^{-1}A^T$",,['linear-algebra']
53,"Find the determinant and possible sizes of a matrix $A$, which satisfies the equation $A^2 - 2A + 4E = 0$","Find the determinant and possible sizes of a matrix , which satisfies the equation",A A^2 - 2A + 4E = 0,"Let $A \in M_n(\mathbb{R})$ satisfy the equation: $$A^2 - 2A + 4I = 0$$ (a) What is $\det A$? (3 points) (b) What are possible sizes of a matrix $A$? (6 points) My solution We can derive 2 useful equations from the given one: 1) $A^2 = -2(2I - A)\Rightarrow (\det A)^2 = \det (-2(2I - A)) = (-2)^n \cdot \det (2I - A)$ 2) $A(2I - A) = 4E \Rightarrow \det A \cdot \det (2I - A) = 4^n = 2^{2n}$ From the first we see that the size $n$ must be even. This is the answer to the question (b). Now, let's build the system of equations, where $x = \det A$, $y = \det (2I - A)$. Also, we can replace $(-2)^n$ with $2^n$, because we know, that $n$ is even: $$\begin{cases} x^2 = 2^n \cdot y, \\ xy = 2^{2n} \end{cases} \Rightarrow \begin{cases} y = x^2\cdot2^{-n}, \\ x\cdot(x^2\cdot2^{-n}) = 2^{2n} \end{cases} \Rightarrow x = (2^{2n}\cdot2^n)^{1/3} = 2^n$$ So, $\det A = 2^n$ and it depends on the size of the matrix. This is the answer to the question (a). The question Is this solution correct? I have big doubts about it, because the question (b) is worth twice more points than (a), but happened to be much easier and also order of tasks suggests, that we should solve (a) first.","Let $A \in M_n(\mathbb{R})$ satisfy the equation: $$A^2 - 2A + 4I = 0$$ (a) What is $\det A$? (3 points) (b) What are possible sizes of a matrix $A$? (6 points) My solution We can derive 2 useful equations from the given one: 1) $A^2 = -2(2I - A)\Rightarrow (\det A)^2 = \det (-2(2I - A)) = (-2)^n \cdot \det (2I - A)$ 2) $A(2I - A) = 4E \Rightarrow \det A \cdot \det (2I - A) = 4^n = 2^{2n}$ From the first we see that the size $n$ must be even. This is the answer to the question (b). Now, let's build the system of equations, where $x = \det A$, $y = \det (2I - A)$. Also, we can replace $(-2)^n$ with $2^n$, because we know, that $n$ is even: $$\begin{cases} x^2 = 2^n \cdot y, \\ xy = 2^{2n} \end{cases} \Rightarrow \begin{cases} y = x^2\cdot2^{-n}, \\ x\cdot(x^2\cdot2^{-n}) = 2^{2n} \end{cases} \Rightarrow x = (2^{2n}\cdot2^n)^{1/3} = 2^n$$ So, $\det A = 2^n$ and it depends on the size of the matrix. This is the answer to the question (a). The question Is this solution correct? I have big doubts about it, because the question (b) is worth twice more points than (a), but happened to be much easier and also order of tasks suggests, that we should solve (a) first.",,"['linear-algebra', 'matrices', 'matrix-equations']"
54,"What is the geometrical interpretation of $f(x,y)=\begin{pmatrix}9&12\\12&16\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}$",What is the geometrical interpretation of,"f(x,y)=\begin{pmatrix}9&12\\12&16\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}","Consider the matrix $A=\begin{pmatrix}9&12\\12&16\end{pmatrix}$. I show that $\begin{pmatrix}-4\\ 3\end{pmatrix}$ is a eigenvector or the eigen value $0$ and that $\begin{pmatrix}3\\ 4\end{pmatrix}$ is an eigenvector for the eigenvalue $25$. Therefore, in the basis $\left\{\begin{pmatrix}-4\\ 3\end{pmatrix},\begin{pmatrix}3\\ 4\end{pmatrix}\right\}$, is given by $$\begin{pmatrix}0&0\\0&25\end{pmatrix}.$$ Now, they ask me which type of application is $f$, the application given by the matrix $A$, but I don't know. I would say something like a projection on $Span\{(3,4)\}$ and an homothetic transformation in the direction of center $(0,0)$ and of parameter $\lambda=25$, is it correct ? Can we do better ?","Consider the matrix $A=\begin{pmatrix}9&12\\12&16\end{pmatrix}$. I show that $\begin{pmatrix}-4\\ 3\end{pmatrix}$ is a eigenvector or the eigen value $0$ and that $\begin{pmatrix}3\\ 4\end{pmatrix}$ is an eigenvector for the eigenvalue $25$. Therefore, in the basis $\left\{\begin{pmatrix}-4\\ 3\end{pmatrix},\begin{pmatrix}3\\ 4\end{pmatrix}\right\}$, is given by $$\begin{pmatrix}0&0\\0&25\end{pmatrix}.$$ Now, they ask me which type of application is $f$, the application given by the matrix $A$, but I don't know. I would say something like a projection on $Span\{(3,4)\}$ and an homothetic transformation in the direction of center $(0,0)$ and of parameter $\lambda=25$, is it correct ? Can we do better ?",,"['linear-algebra', 'geometry']"
55,An equality in matrix theory (also interesting in analysis),An equality in matrix theory (also interesting in analysis),,"Let $M_{n\times n}$ be a real, symmetric matrix and  $$ S=\left\{ \left(u_{1},\ldots,u_{k}\right)\Bigl|0\leq k\leq n,\left\{ u_{1},\ldots,u_{k}\right\} \textrm{ is an orthonormal subset in } \mathbb{R}^{n}\right\}.  $$  Prove that $$ \sum_{\lambda\textrm{ are positive eigenvalues of }M}\lambda=\max_{S}\left\{ u_{1}Mu_{1}^{T}+\ldots+u_{k}Mu_{k}^{T}\right\} . $$ A very special case is also interesting: Let $\lambda_{1}$ and $\lambda_{2}$ are positive, prove that  $$ \lambda_{1}\left(a_{11}^{2}+a_{21}^{2}\right)+\lambda_{2}\left(a_{12}^{2}+a_{22}^{2}\right)\leq\lambda_{1}+\lambda_{2}, $$ where $a_{11}a_{21}+a_{12}a_{22}=0$ and  $a_{11}^{2}+a_{12}^{2}=a_{21}^{2}+a_{22}^{2}=1.$","Let $M_{n\times n}$ be a real, symmetric matrix and  $$ S=\left\{ \left(u_{1},\ldots,u_{k}\right)\Bigl|0\leq k\leq n,\left\{ u_{1},\ldots,u_{k}\right\} \textrm{ is an orthonormal subset in } \mathbb{R}^{n}\right\}.  $$  Prove that $$ \sum_{\lambda\textrm{ are positive eigenvalues of }M}\lambda=\max_{S}\left\{ u_{1}Mu_{1}^{T}+\ldots+u_{k}Mu_{k}^{T}\right\} . $$ A very special case is also interesting: Let $\lambda_{1}$ and $\lambda_{2}$ are positive, prove that  $$ \lambda_{1}\left(a_{11}^{2}+a_{21}^{2}\right)+\lambda_{2}\left(a_{12}^{2}+a_{22}^{2}\right)\leq\lambda_{1}+\lambda_{2}, $$ where $a_{11}a_{21}+a_{12}a_{22}=0$ and  $a_{11}^{2}+a_{12}^{2}=a_{21}^{2}+a_{22}^{2}=1.$",,"['linear-algebra', 'matrices', 'functional-analysis', 'inequality']"
56,Unique decomposition of a positive definite matrix into a sum of outer products ${\bf x}_k{\bf x}_k^{\rm T}$ and a diagonal matrix?,Unique decomposition of a positive definite matrix into a sum of outer products  and a diagonal matrix?,{\bf x}_k{\bf x}_k^{\rm T},"Given a (symmetric) positive definite matrix ${\bf A}\in\mathbb{R}^{N\times N}$, I know that it can always be expressed as a sum of $N$ rank-one matrices using the singular value decomposition ${\bf U\Sigma V^\ast}$ of the corresponding Cholesky matrix $\bf L$: $${\bf A} = {\bf L}{\bf L}^{\rm T} = {\bf U\Sigma V^\ast}({\bf U\Sigma V^\ast})^{\rm T} = \sum_{i,j=1}^N \sigma_i \sigma_j {\bf u}_i {\bf v}^{\rm T}_i {\bf v}_j {\bf u}^{\rm T}_i= \sum_{i=1}^N \sigma_i^2 {\bf u}_i {\bf u}^{\rm T}_i,$$ with ${\bf u}_i$ being the $i$-th column of $\bf U$. So the matrix terms here are all outer products (dyads) of a vector with itself. Can such a matrix also be expressed as the sum of a diagonal positive definite matrix $\bf D$ of full rank with $D_{ii} = d_i^2 > 0$ and a series of $M$ outer products ${\bf x}{\bf x}^{\rm T}$ (${\bf x}\in \mathbb{R}^N$)? That is: $${\bf A} = {\bf D} + \sum_{k=1}^M {\bf x}_k{\bf x}_k^{\rm T}$$ Basically, this is just one large system of $N(N+1)/2$ quadratic equations with $N(M+1)$ unknowns $\{d_i, x_{k,i}\}$. Using small test values for $N$ and $M$, I was able to find numerical solutions to this equation system for specific example matrices, but it would be nice to have an analytic solution. I suspect this should always be possible, at least for large enough values of $M$. If so, what is the minimal value of $M$ for which such a decomposition exists? What can be said about the relationship of ${\bf d}={\rm diag}({\bf D})$ and the different ${\bf x}_k$? Is there some $M$ for which the decomposition is unique? If not, how can a counterexample be constructed? Some thoughts on this: for $N=1$, this is trivial, as one can always choose $\bf D=A$ and $M=0$ for $N\geq2$, I thought of choosing $D_{ii} = A_{ii}$ as a starting point and applying the Cholesky/SVD strategy on the difference ${\bf A}-{\bf D}$ to get the ${\bf x}_k$. However, for $N=2$ at least, this doesn't work, since the difference matrix is not positive definite. So I guess a related question is: For which $N\times N$ diagonal matrices ${\bf D}$ does ${\bf A}-{\bf D}$ remain positive definite? from a geometric perspective, a positive definite matrix ${\bf A}$ defines an $N$-dimensional ellipsoid $\{{\bf z}\in\mathbb{R}:{\bf z}^{\rm T}{\bf A}{\bf z}={\rm const.}\}$. If the matrix is diagonal, the semi-axes of this ellipsoid are aligned with the coordinate axes.  In constrast, an outer-product-like term ${\bf x}{\bf x}^{\rm T}$ defines a pair of hyperplanes $\{{\bf z}\in\mathbb{R}:{\bf z}^{\rm T}{\bf x}{\bf x}^{\rm T}{\bf z}=\|{\bf z}^{\rm T}{\bf x}\|^2={\rm const.}\}$. So the above problem is equivalent to asking whether any $N$-dimensional ellipsoid can be understood as a ""superposition"" of an axis-aligned ellipsoid and a series of hyperplane pairs, although the meaning of ""superposition"" is somewhat vague in this context... the above system of equations can be written using the tensor product $\otimes$: $ \left(\sum_{i,j} a_{i,j}\,{\bf e}_i\otimes{\bf e}_j\right) = \left(\sum_i d_i^2\,{\bf e}_i\otimes{\bf e}_i\right) + \sum_k   \left(\sum_i x_{k,i}{\bf e}_i\right) \otimes   \left(\sum_j x_{k,j}{\bf e}_j\right) $. Maybe some tensor algebra can come in handy?","Given a (symmetric) positive definite matrix ${\bf A}\in\mathbb{R}^{N\times N}$, I know that it can always be expressed as a sum of $N$ rank-one matrices using the singular value decomposition ${\bf U\Sigma V^\ast}$ of the corresponding Cholesky matrix $\bf L$: $${\bf A} = {\bf L}{\bf L}^{\rm T} = {\bf U\Sigma V^\ast}({\bf U\Sigma V^\ast})^{\rm T} = \sum_{i,j=1}^N \sigma_i \sigma_j {\bf u}_i {\bf v}^{\rm T}_i {\bf v}_j {\bf u}^{\rm T}_i= \sum_{i=1}^N \sigma_i^2 {\bf u}_i {\bf u}^{\rm T}_i,$$ with ${\bf u}_i$ being the $i$-th column of $\bf U$. So the matrix terms here are all outer products (dyads) of a vector with itself. Can such a matrix also be expressed as the sum of a diagonal positive definite matrix $\bf D$ of full rank with $D_{ii} = d_i^2 > 0$ and a series of $M$ outer products ${\bf x}{\bf x}^{\rm T}$ (${\bf x}\in \mathbb{R}^N$)? That is: $${\bf A} = {\bf D} + \sum_{k=1}^M {\bf x}_k{\bf x}_k^{\rm T}$$ Basically, this is just one large system of $N(N+1)/2$ quadratic equations with $N(M+1)$ unknowns $\{d_i, x_{k,i}\}$. Using small test values for $N$ and $M$, I was able to find numerical solutions to this equation system for specific example matrices, but it would be nice to have an analytic solution. I suspect this should always be possible, at least for large enough values of $M$. If so, what is the minimal value of $M$ for which such a decomposition exists? What can be said about the relationship of ${\bf d}={\rm diag}({\bf D})$ and the different ${\bf x}_k$? Is there some $M$ for which the decomposition is unique? If not, how can a counterexample be constructed? Some thoughts on this: for $N=1$, this is trivial, as one can always choose $\bf D=A$ and $M=0$ for $N\geq2$, I thought of choosing $D_{ii} = A_{ii}$ as a starting point and applying the Cholesky/SVD strategy on the difference ${\bf A}-{\bf D}$ to get the ${\bf x}_k$. However, for $N=2$ at least, this doesn't work, since the difference matrix is not positive definite. So I guess a related question is: For which $N\times N$ diagonal matrices ${\bf D}$ does ${\bf A}-{\bf D}$ remain positive definite? from a geometric perspective, a positive definite matrix ${\bf A}$ defines an $N$-dimensional ellipsoid $\{{\bf z}\in\mathbb{R}:{\bf z}^{\rm T}{\bf A}{\bf z}={\rm const.}\}$. If the matrix is diagonal, the semi-axes of this ellipsoid are aligned with the coordinate axes.  In constrast, an outer-product-like term ${\bf x}{\bf x}^{\rm T}$ defines a pair of hyperplanes $\{{\bf z}\in\mathbb{R}:{\bf z}^{\rm T}{\bf x}{\bf x}^{\rm T}{\bf z}=\|{\bf z}^{\rm T}{\bf x}\|^2={\rm const.}\}$. So the above problem is equivalent to asking whether any $N$-dimensional ellipsoid can be understood as a ""superposition"" of an axis-aligned ellipsoid and a series of hyperplane pairs, although the meaning of ""superposition"" is somewhat vague in this context... the above system of equations can be written using the tensor product $\otimes$: $ \left(\sum_{i,j} a_{i,j}\,{\bf e}_i\otimes{\bf e}_j\right) = \left(\sum_i d_i^2\,{\bf e}_i\otimes{\bf e}_i\right) + \sum_k   \left(\sum_i x_{k,i}{\bf e}_i\right) \otimes   \left(\sum_j x_{k,j}{\bf e}_j\right) $. Maybe some tensor algebra can come in handy?",,"['linear-algebra', 'systems-of-equations', 'matrix-decomposition', 'positive-definite', 'symmetric-matrices']"
57,$AA^t=BB^t \implies A=B$,,AA^t=BB^t \implies A=B,"Let $A$ and $B$ are complex valued $n\times n$ matrices and let $\det(A)=1$ or $-1$. Then what necessary and sufficient conditions can be given for $A$ and $B$ to satisfy this proposition? ""If $AA^t=BB^t$, then $A=B$ or $A=-B$."" where $A^t$ is the transpose of $A$.","Let $A$ and $B$ are complex valued $n\times n$ matrices and let $\det(A)=1$ or $-1$. Then what necessary and sufficient conditions can be given for $A$ and $B$ to satisfy this proposition? ""If $AA^t=BB^t$, then $A=B$ or $A=-B$."" where $A^t$ is the transpose of $A$.",,"['linear-algebra', 'transpose']"
58,Index of positivity of a symmetric form,Index of positivity of a symmetric form,,"Old qual question: Let $V$ be a finite dimensional positive definite inner product space over $\mathbb{R}$ under $\langle,\rangle$ and let $A:V\to V$ be a linear map. Define $B=A^tA$. Now define a new bilinear form $\{,\}$ by $$\{v,w\}=\langle Bv,w\rangle.$$ Assume that $\ker A=m$. Prove that $\{,\}$ is symmetric. Calculate the index of positivity, negativity, nullity of the bilinear form $\{,\}$. (Recall that the index of positivity (resp. negativity, nullity of $\{,\}$ means the number of the positive (resp. negative, zero) eigenvalues of the corresponding symmetric matrix.) OK so part 1 is easy, and I did this by putting $V$ in an ONB and then the adjoint of $A$ is just $A^t$. However, I'm not sure how to get going on part 2. My friend suggested showing that $\ker A=\ker A^tA$, and I can do that, but I'm still not sure where to go. Thank you.","Old qual question: Let $V$ be a finite dimensional positive definite inner product space over $\mathbb{R}$ under $\langle,\rangle$ and let $A:V\to V$ be a linear map. Define $B=A^tA$. Now define a new bilinear form $\{,\}$ by $$\{v,w\}=\langle Bv,w\rangle.$$ Assume that $\ker A=m$. Prove that $\{,\}$ is symmetric. Calculate the index of positivity, negativity, nullity of the bilinear form $\{,\}$. (Recall that the index of positivity (resp. negativity, nullity of $\{,\}$ means the number of the positive (resp. negative, zero) eigenvalues of the corresponding symmetric matrix.) OK so part 1 is easy, and I did this by putting $V$ in an ONB and then the adjoint of $A$ is just $A^t$. However, I'm not sure how to get going on part 2. My friend suggested showing that $\ker A=\ker A^tA$, and I can do that, but I'm still not sure where to go. Thank you.",,"['linear-algebra', 'abstract-algebra', 'inner-products']"
59,"Does there exist a normal matrix which is not diagonal, nor Hermitian nor Unitary nor Skew Hermitian?","Does there exist a normal matrix which is not diagonal, nor Hermitian nor Unitary nor Skew Hermitian?",,"Well, we know that if a matrix is diagonal, Hermitian, Unitary or Skew-hermitian, then the matrix is Normal. But is the converse true? If not, can anyone give an example?","Well, we know that if a matrix is diagonal, Hermitian, Unitary or Skew-hermitian, then the matrix is Normal. But is the converse true? If not, can anyone give an example?",,[]
60,When do Eigenvalues of $Q^TAQ$ and $A$ coincide?,When do Eigenvalues of  and  coincide?,Q^TAQ A,"Lets say I have some $n\times n$ matrix  $A$ and a $n\times l$ matrix $Q$, whose columns for a basis for some subspace $\mathcal{L} \subset \mathbb{R^n}$. My intuition tells me that I would need $Q$ to be a full orthogonal basis of $\mathbb{R^n}$ ( i.e. $l=n$), such that all eigenvalues $\lambda_i$ of  $Q^TAQ$ and $A$ coincide. However, I don't see where in the proof for the latter assertion the same dimensionality is needed: Be $v\in\mathbb{R}^n\setminus\{0\}$ an eigenvector of A, i.e. $Av=\lambda v$. Then $\lambda$ is an eigenvalue of $Q^TAQ$ with eigenvector $Q^Tv$: $$ (Q^TAQ)(Q^Tv)=Q'A(QQ^T)v=Q^TAv=\lambda(Q^Tv). $$ Be $v\in\mathcal{L}\setminus\{0\}$ an eigenvector of (Q^TAQ), i.e. $(Q^TAQ)v=\lambda v$. Then $\lambda$ is an eigenvalue of $A$ with eigenvector $Qv$: $$ A(Qv)=IAQv=QQ^TAQv=Q(Q^TAQv)=\lambda(Qv). $$ Maybe you can help me out here. Thanks a lot in advance!","Lets say I have some $n\times n$ matrix  $A$ and a $n\times l$ matrix $Q$, whose columns for a basis for some subspace $\mathcal{L} \subset \mathbb{R^n}$. My intuition tells me that I would need $Q$ to be a full orthogonal basis of $\mathbb{R^n}$ ( i.e. $l=n$), such that all eigenvalues $\lambda_i$ of  $Q^TAQ$ and $A$ coincide. However, I don't see where in the proof for the latter assertion the same dimensionality is needed: Be $v\in\mathbb{R}^n\setminus\{0\}$ an eigenvector of A, i.e. $Av=\lambda v$. Then $\lambda$ is an eigenvalue of $Q^TAQ$ with eigenvector $Q^Tv$: $$ (Q^TAQ)(Q^Tv)=Q'A(QQ^T)v=Q^TAv=\lambda(Q^Tv). $$ Be $v\in\mathcal{L}\setminus\{0\}$ an eigenvector of (Q^TAQ), i.e. $(Q^TAQ)v=\lambda v$. Then $\lambda$ is an eigenvalue of $A$ with eigenvector $Qv$: $$ A(Qv)=IAQv=QQ^TAQv=Q(Q^TAQv)=\lambda(Qv). $$ Maybe you can help me out here. Thanks a lot in advance!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'orthogonal-matrices']"
61,Two homogenous system are equivalent if they have the same answer,Two homogenous system are equivalent if they have the same answer,,Prove that two homogeneous systems of linear equations are equivalent iff they have the same solution set. My definition of equivalent of linear equations is that each equation of system $A$ is a linear combination of the equations of system $B$ and converse. So i have a bit trouble proving this statement in general as $m\times n$ form. I worked with their coefficient matrices and if i show that these two are row equivalent the problem is solved. Is there any method to use for this? If not what's the general prove.,Prove that two homogeneous systems of linear equations are equivalent iff they have the same solution set. My definition of equivalent of linear equations is that each equation of system $A$ is a linear combination of the equations of system $B$ and converse. So i have a bit trouble proving this statement in general as $m\times n$ form. I worked with their coefficient matrices and if i show that these two are row equivalent the problem is solved. Is there any method to use for this? If not what's the general prove.,,"['linear-algebra', 'systems-of-equations']"
62,Required conditions for eigenvalues $\lambda_{\min}(A) >\lambda_{\min}(B)$ and $\lambda_{\max}(A) >\lambda_{\max}(B) $ etc?,Required conditions for eigenvalues  and  etc?,\lambda_{\min}(A) >\lambda_{\min}(B) \lambda_{\max}(A) >\lambda_{\max}(B) ,"Given $\operatorname{tr}(X^TAX) > \operatorname{tr}(X^TBX)$ and $A$ and $B$ are p.s.d then under what conditions will we have $\lambda_{\max}(A)>\lambda_{\max}(B)$ to be guaranteed ? What are required conditions for  $\lambda_{\min}(A)>\lambda_{\min}(B)$ ? All matrix entries are real valued and $X$ is a rectangular matrix. Thirdly, what are the conditions for second smallest eigenvalue (algebraic connectivity) of $A$ to be greater than the same for $B$? Also fourthly, what are the conditions for the eigenvalues of $A$ to majorize the eigenvalues of $B$ from above, below and so forth? And by majorization I mean this mathematical property: https://en.wikipedia.org/wiki/Majorization . And finally and most important of all for me..What are the conditions w.r.t $A(X)$ and $B(X)$ for $\operatorname{tr}(X^TA(X)X) > \operatorname{tr}(X^TB(X)X)$ to be true if $A(X)$ and $B(X)$ are functions of X and matrix valued as well?","Given $\operatorname{tr}(X^TAX) > \operatorname{tr}(X^TBX)$ and $A$ and $B$ are p.s.d then under what conditions will we have $\lambda_{\max}(A)>\lambda_{\max}(B)$ to be guaranteed ? What are required conditions for  $\lambda_{\min}(A)>\lambda_{\min}(B)$ ? All matrix entries are real valued and $X$ is a rectangular matrix. Thirdly, what are the conditions for second smallest eigenvalue (algebraic connectivity) of $A$ to be greater than the same for $B$? Also fourthly, what are the conditions for the eigenvalues of $A$ to majorize the eigenvalues of $B$ from above, below and so forth? And by majorization I mean this mathematical property: https://en.wikipedia.org/wiki/Majorization . And finally and most important of all for me..What are the conditions w.r.t $A(X)$ and $B(X)$ for $\operatorname{tr}(X^TA(X)X) > \operatorname{tr}(X^TB(X)X)$ to be true if $A(X)$ and $B(X)$ are functions of X and matrix valued as well?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-semidefinite']"
63,Looking for a reference to a proof of $(I - A)^{-1} = I + A + A^2 + A^3 + \ldots$,Looking for a reference to a proof of,(I - A)^{-1} = I + A + A^2 + A^3 + \ldots,"On some online forum, there is the claim: Given some square matrix: $$(I - A)^{-1} = I + A + A^2 + A^3 + \ldots$$ This is true if the right side converges, which is true if and only if   all of the eigenvalues of A have absolute value smaller than $1$. Reference https://www.physicsforums.com/threads/matrix-inverse-equals-power-series.423897/ I really like this result, because it relies on the more intuitive spectral radius rather than matrix norm which is defined as: \begin{align} \|A\| &= \sup\{\|Ax\| : x\in K^n \mbox{ with }\|x\|= 1\} \\ &= \sup\left\{\frac{\|Ax\|}{\|x\|} : x\in K^n \mbox{ with }x\ne 0\right\}. \end{align} Can someone provide a reference to the proof of this claim?","On some online forum, there is the claim: Given some square matrix: $$(I - A)^{-1} = I + A + A^2 + A^3 + \ldots$$ This is true if the right side converges, which is true if and only if   all of the eigenvalues of A have absolute value smaller than $1$. Reference https://www.physicsforums.com/threads/matrix-inverse-equals-power-series.423897/ I really like this result, because it relies on the more intuitive spectral radius rather than matrix norm which is defined as: \begin{align} \|A\| &= \sup\{\|Ax\| : x\in K^n \mbox{ with }\|x\|= 1\} \\ &= \sup\left\{\frac{\|Ax\|}{\|x\|} : x\in K^n \mbox{ with }x\ne 0\right\}. \end{align} Can someone provide a reference to the proof of this claim?",,"['linear-algebra', 'sequences-and-series', 'matrices', 'reference-request', 'matrix-calculus']"
64,Invertibility of $U+I$ where $U$ comes from a SVD $M=USV'$,Invertibility of  where  comes from a SVD,U+I U M=USV',"Assume that $U$ is a real $2 \times 2$ matrix arising from a singluar value decomposition $$ M = USV' $$ As a part of a bigger calculation, it suggested in this paper (see text right beneath equation (44)) that we can in this case use the properties of $U$ to create a so-called orthomorphic transformation, i.e. a matrix $X$ such that $$ X = (I_2 + U)^{-1}(I_2-U) $$ and $$ U = (I+X)^{-1}(I-X) $$ Now, here is my problem: It seems to me that the matrix $I_2 + U$ is almost never invertible. If $U$ is a real matrix, then by nature of the SVD it is typically a rotation matrix with $U_{11}=-U_{22}$. Then we have $$ |I_2+U| = |U|+U_{11}+U_{22}+1 $$ with $|U| = \pm1$ since $U$ is orthogonal. So in that case, any time $|U|=-1$, the said matrix is singular. Am I right in that it is a somewhat exceptional case that $(I_2+U)$ is in fact invertible? Or am I missing something? I would be truly grateful if anyone could shed some light on this problem.","Assume that $U$ is a real $2 \times 2$ matrix arising from a singluar value decomposition $$ M = USV' $$ As a part of a bigger calculation, it suggested in this paper (see text right beneath equation (44)) that we can in this case use the properties of $U$ to create a so-called orthomorphic transformation, i.e. a matrix $X$ such that $$ X = (I_2 + U)^{-1}(I_2-U) $$ and $$ U = (I+X)^{-1}(I-X) $$ Now, here is my problem: It seems to me that the matrix $I_2 + U$ is almost never invertible. If $U$ is a real matrix, then by nature of the SVD it is typically a rotation matrix with $U_{11}=-U_{22}$. Then we have $$ |I_2+U| = |U|+U_{11}+U_{22}+1 $$ with $|U| = \pm1$ since $U$ is orthogonal. So in that case, any time $|U|=-1$, the said matrix is singular. Am I right in that it is a somewhat exceptional case that $(I_2+U)$ is in fact invertible? Or am I missing something? I would be truly grateful if anyone could shed some light on this problem.",,['linear-algebra']
65,Is there a theory of generalized eigenvectors over commutative rings?,Is there a theory of generalized eigenvectors over commutative rings?,,"Brown's Matrices over Commutative Rings book discusses the theory of eigenvalues, eigenvectors, and diagonalizing matrices over commutative rings, but unless I've missed something, nothing like generalized eigenvectors is even mentioned. Has the notion of generalized eigenvectors ever been studied over commutative rings in any context more general than working over a field? My interest stems from trying to find a closed form expression for the matrix exponential $e^{zA}$, where $z$ is an element of a finite dimensional real associative commutative algebra, and $A$ is a matrix built of elements of said algebra, which in the real case relies on the theory of generalized eigenvectors.","Brown's Matrices over Commutative Rings book discusses the theory of eigenvalues, eigenvectors, and diagonalizing matrices over commutative rings, but unless I've missed something, nothing like generalized eigenvectors is even mentioned. Has the notion of generalized eigenvectors ever been studied over commutative rings in any context more general than working over a field? My interest stems from trying to find a closed form expression for the matrix exponential $e^{zA}$, where $z$ is an element of a finite dimensional real associative commutative algebra, and $A$ is a matrix built of elements of said algebra, which in the real case relies on the theory of generalized eigenvectors.",,"['linear-algebra', 'ring-theory', 'commutative-algebra', 'modules']"
66,"Basis-free formula for $\mathrm{Hom}_k(V,V)\rightarrow V^*\otimes V$",Basis-free formula for,"\mathrm{Hom}_k(V,V)\rightarrow V^*\otimes V","Let $V$ be a finite dimensional vector space over a field $k$.  Then  there is a natural map $\phi:V^*\otimes V\rightarrow \mathrm{Hom}_k(V,V)$ given by $$\phi:f\otimes v\mapsto \Big(x\mapsto f(x)v\Big)$$ (extended by linearity). It's not hard to check that this is well-defined and injective, hence surjective by a dimension count. The above suffices to define a (canonical) map $\phi^{-1}\colon\mathrm{Hom}_k(V,V)\rightarrow V^*\otimes V$. Explicitly, $\phi^{-1}(g)$ is the unique element of $V^*\otimes V$ that maps to $g$ under $\phi$. This definition of $\phi$ is basis-free, but it does not seem to give a formula for $\phi^{-1}$ in the same sense that the displayed equation gives a formula for $\phi$. Question 1: How should I make precise the notion that the displayed equation for $\phi$ counts as a ""formula"", but the definition of $\phi^{-1}$ does not? Question 2: Is there some alternate basis-free definitionof $\phi^{-1}$ that clearly would count as a formula? Comment:  It is easy to write down a basis-dependent formula for $\phi^{-1}$ and then prove that the result of this formula is independent of the choice of basis.  But I'm looking for a formula that never requires picking a basis in the first place.","Let $V$ be a finite dimensional vector space over a field $k$.  Then  there is a natural map $\phi:V^*\otimes V\rightarrow \mathrm{Hom}_k(V,V)$ given by $$\phi:f\otimes v\mapsto \Big(x\mapsto f(x)v\Big)$$ (extended by linearity). It's not hard to check that this is well-defined and injective, hence surjective by a dimension count. The above suffices to define a (canonical) map $\phi^{-1}\colon\mathrm{Hom}_k(V,V)\rightarrow V^*\otimes V$. Explicitly, $\phi^{-1}(g)$ is the unique element of $V^*\otimes V$ that maps to $g$ under $\phi$. This definition of $\phi$ is basis-free, but it does not seem to give a formula for $\phi^{-1}$ in the same sense that the displayed equation gives a formula for $\phi$. Question 1: How should I make precise the notion that the displayed equation for $\phi$ counts as a ""formula"", but the definition of $\phi^{-1}$ does not? Question 2: Is there some alternate basis-free definitionof $\phi^{-1}$ that clearly would count as a formula? Comment:  It is easy to write down a basis-dependent formula for $\phi^{-1}$ and then prove that the result of this formula is independent of the choice of basis.  But I'm looking for a formula that never requires picking a basis in the first place.",,"['linear-algebra', 'category-theory', 'canonical-transformation']"
67,Complex representation of a quaternionic matrix,Complex representation of a quaternionic matrix,,"It is evident that right module $\mathbb{H}^n$ is $\mathbb{C}$-linearly isomorphic to $\mathbb{C^{2n}}$ with corresponding isomorphism   $\nu :  \mathbb{C^{2n}} \to\mathbb{H}^n   $ given by $ \nu(a,b) = a + b\mathrm{j}$. This naturaly gives  representation for any quaternionic matrix $M \in \mathcal{M}^{n \times m}(\mathbb{H}) $ with two complex matrices $A,B \in \mathcal{M}^{n \times m}(\mathbb{C})$ as $M = A + B\mathrm{j}$. It's assumed that  complex matrix representing $\nu^{-1}M\nu$ in parallel with complex representation of quaternion numbers can be written in  the form $$ \theta_{n,m}(M) = \theta_{n,m}(A+B\mathrm{j}) =  \left[\begin{matrix} A & B \\ -\overline B & \overline{A} \end{matrix}\right]$$ where $\overline{A}$ is a complex conjugate. However, what i don't understand is there this conjugation came from and I need your help. When I write $$ \nu^{-1}M\nu(a,b) = \nu^{-1}(A +B\mathrm{j})(a + b\mathrm j) =\nu^{-1}\left(Aa + Ab\mathrm{j} + B\overline{a}\mathrm{j} -B\overline b\right) = \left( Aa - B\overline{b}, Ab + B\overline a \right) $$ I don't have any idea what to do with conjugates to show that this map even linear.","It is evident that right module $\mathbb{H}^n$ is $\mathbb{C}$-linearly isomorphic to $\mathbb{C^{2n}}$ with corresponding isomorphism   $\nu :  \mathbb{C^{2n}} \to\mathbb{H}^n   $ given by $ \nu(a,b) = a + b\mathrm{j}$. This naturaly gives  representation for any quaternionic matrix $M \in \mathcal{M}^{n \times m}(\mathbb{H}) $ with two complex matrices $A,B \in \mathcal{M}^{n \times m}(\mathbb{C})$ as $M = A + B\mathrm{j}$. It's assumed that  complex matrix representing $\nu^{-1}M\nu$ in parallel with complex representation of quaternion numbers can be written in  the form $$ \theta_{n,m}(M) = \theta_{n,m}(A+B\mathrm{j}) =  \left[\begin{matrix} A & B \\ -\overline B & \overline{A} \end{matrix}\right]$$ where $\overline{A}$ is a complex conjugate. However, what i don't understand is there this conjugation came from and I need your help. When I write $$ \nu^{-1}M\nu(a,b) = \nu^{-1}(A +B\mathrm{j})(a + b\mathrm j) =\nu^{-1}\left(Aa + Ab\mathrm{j} + B\overline{a}\mathrm{j} -B\overline b\right) = \left( Aa - B\overline{b}, Ab + B\overline a \right) $$ I don't have any idea what to do with conjugates to show that this map even linear.",,"['linear-algebra', 'modules', 'lie-groups', 'quaternions']"
68,Demystifying the tensor product,Demystifying the tensor product,,"It seems to me, through my mathematical immaturity, that the tensor product seems to beg for more well-definition. I am working in vector spaces (so we always have a free module) and here is what my professor has shown me thus far. We can define the tensor product of two maps (multi-linear) as follows. Let $S \in \mathcal{L}(V_1, \dots, V_n; \mathcal{L}(W;,Z))$ and $T \in \mathcal{L}(V_{n+1}, \dots , V_{n+m};W)$ , We define $S \otimes T \in \mathcal{L}(V_1, \dots , V_{n+m};Z)$ by setting $$S \otimes T(v_1, \dots ,v_{n+m})=S(v_1, \dots, v_n)[T(v_{n+1}, \dots , v_{n+m})]$$ Now, we do have $\mathcal{L}(V_1, \dots , V_{n+m};Z) \cong V^*_1 \otimes \dots \otimes V^*_{n+m} \otimes Z$ I believe. So it is, up to isomorphism, a tensor but not, itself, a tensor. Further, suppose that $V_1, \dots , V_n$ are vector spaces. We define the tensor product $$V_1 \otimes \dots \otimes V_n = \mathcal{L}(V^*_1, \dots V^*_n; \mathbb{F})$$ Since we regard $V$ and $V^{**}$ to be identified we have $$v_1 \otimes \dots \otimes v_n \in V_1 \otimes \dots \otimes V_n$$ defined $$(v_1 \otimes \dots \otimes v_n)(L_1, \dots L_n)=L_1(v_1)\dots L_n(v_n)$$ Finally, we have defined a tensor of type $m,n$ to be a multi-linear map from $\underbrace{V^* \times \dots \times V^*}_{m \text{ times}}\times \underbrace{V \times \dots \times V}_{n \text{ times}} \to \mathbb{F}$ . problem So it seems to me that tensor products do not always produce tensors? That a tensor product sometimes is and sometimes is not a map to the field? Which makes me wonder how we can consider the idea to be well-defined? I have to be told by some to think about it in terms of the universal property, i.e., it takes multi-linear maps to linear ones but that isn't as illuminating as some may think. How is one to think about this product and these objects? Thanks for your help!","It seems to me, through my mathematical immaturity, that the tensor product seems to beg for more well-definition. I am working in vector spaces (so we always have a free module) and here is what my professor has shown me thus far. We can define the tensor product of two maps (multi-linear) as follows. Let and , We define by setting Now, we do have I believe. So it is, up to isomorphism, a tensor but not, itself, a tensor. Further, suppose that are vector spaces. We define the tensor product Since we regard and to be identified we have defined Finally, we have defined a tensor of type to be a multi-linear map from . problem So it seems to me that tensor products do not always produce tensors? That a tensor product sometimes is and sometimes is not a map to the field? Which makes me wonder how we can consider the idea to be well-defined? I have to be told by some to think about it in terms of the universal property, i.e., it takes multi-linear maps to linear ones but that isn't as illuminating as some may think. How is one to think about this product and these objects? Thanks for your help!","S \in \mathcal{L}(V_1, \dots, V_n; \mathcal{L}(W;,Z)) T \in \mathcal{L}(V_{n+1}, \dots , V_{n+m};W) S \otimes T \in \mathcal{L}(V_1, \dots , V_{n+m};Z) S \otimes T(v_1, \dots ,v_{n+m})=S(v_1, \dots, v_n)[T(v_{n+1}, \dots , v_{n+m})] \mathcal{L}(V_1, \dots , V_{n+m};Z) \cong V^*_1 \otimes \dots \otimes V^*_{n+m} \otimes Z V_1, \dots , V_n V_1 \otimes \dots \otimes V_n = \mathcal{L}(V^*_1, \dots V^*_n; \mathbb{F}) V V^{**} v_1 \otimes \dots \otimes v_n \in V_1 \otimes \dots \otimes V_n (v_1 \otimes \dots \otimes v_n)(L_1, \dots L_n)=L_1(v_1)\dots L_n(v_n) m,n \underbrace{V^* \times \dots \times V^*}_{m \text{ times}}\times \underbrace{V \times \dots \times V}_{n \text{ times}} \to \mathbb{F}","['linear-algebra', 'soft-question', 'multilinear-algebra']"
69,Random Binary matrix,Random Binary matrix,,"This is a question from Strang's ""Linear Algebra and its Applications"", right in the first chapter (I'm studying it by myself). I couldn't solve it, it isn't in the Solutions Manual, and my research suggests that there shouldn't be a simple solution for it . However, its presence in the very first chapter suggests me that I'm missing something. Here it goes: 1.6: a) There are sixteen 2x2 matrices whose entries are 1's and 0's. How many are invertible? b) (Much harder!) If you put 1's and 0's at random into the entries of a 10 by 10 matrix, is it more likely to be invertible or singular? From what I can tell, this can't be solved by elementary linear algebra so... it shouldn't be there? I'm guessing there is a clever computational way to exhaust all cases?","This is a question from Strang's ""Linear Algebra and its Applications"", right in the first chapter (I'm studying it by myself). I couldn't solve it, it isn't in the Solutions Manual, and my research suggests that there shouldn't be a simple solution for it . However, its presence in the very first chapter suggests me that I'm missing something. Here it goes: 1.6: a) There are sixteen 2x2 matrices whose entries are 1's and 0's. How many are invertible? b) (Much harder!) If you put 1's and 0's at random into the entries of a 10 by 10 matrix, is it more likely to be invertible or singular? From what I can tell, this can't be solved by elementary linear algebra so... it shouldn't be there? I'm guessing there is a clever computational way to exhaust all cases?",,['linear-algebra']
70,"Vectors sometimes used as arrays, sometimes as concept of ""change""","Vectors sometimes used as arrays, sometimes as concept of ""change""",,"As a first-year student at a small town college, I've been receiving conflicting information about what vectors (and matrices/tensors) are. Sometimes, it seems that they are simply used as containers/arrays for multiple numbers in an ordered form. Other times, I've seen them being used to represent a concept of change in space, differentiating themselves from the concept of a point in space. Is there a precise definition of what a vector (matrix/tensor) is that applies across all mathematical fields? Or am I correct in understanding that they are sometimes used to represent abstract concepts of change and other times they are used simply as arrays of numbers? Is this just how things are? And instead of seeking a universal definition, should I accept that its use and concept may vary slightly depending on the context?","As a first-year student at a small town college, I've been receiving conflicting information about what vectors (and matrices/tensors) are. Sometimes, it seems that they are simply used as containers/arrays for multiple numbers in an ordered form. Other times, I've seen them being used to represent a concept of change in space, differentiating themselves from the concept of a point in space. Is there a precise definition of what a vector (matrix/tensor) is that applies across all mathematical fields? Or am I correct in understanding that they are sometimes used to represent abstract concepts of change and other times they are used simply as arrays of numbers? Is this just how things are? And instead of seeking a universal definition, should I accept that its use and concept may vary slightly depending on the context?",,"['linear-algebra', 'soft-question', 'terminology', 'vectors', 'definition']"
71,Etymology of transpose of morphisms in an adjunction,Etymology of transpose of morphisms in an adjunction,,"Let $F: \mathbf{C} \to \mathbf{D}$ be left adjoint to $G : \mathbf{D} \to \mathbf{C}$, witnessed by the family of bijections between hom-sets, natural in objects $X, Y$: $$\operatorname{Hom}_{\mathbf{C}}(X,GY) \overset{\psi_{X,Y}}{\longrightarrow} \operatorname{Hom}_{\mathbf{D}}(FX,Y).$$ I've noticed that among some authors, the image $\psi_{X,Y}(f)$ of a map $X \overset{f}{\to} GY$ is called the transpose of $f$. Does this have anything to do with a categorical realization of transpositions in linear algebra? Is there a canonical motivating example for adjoint functors involving transposes of linear operators, involving (say) adjoint representations of Lie groups?","Let $F: \mathbf{C} \to \mathbf{D}$ be left adjoint to $G : \mathbf{D} \to \mathbf{C}$, witnessed by the family of bijections between hom-sets, natural in objects $X, Y$: $$\operatorname{Hom}_{\mathbf{C}}(X,GY) \overset{\psi_{X,Y}}{\longrightarrow} \operatorname{Hom}_{\mathbf{D}}(FX,Y).$$ I've noticed that among some authors, the image $\psi_{X,Y}(f)$ of a map $X \overset{f}{\to} GY$ is called the transpose of $f$. Does this have anything to do with a categorical realization of transpositions in linear algebra? Is there a canonical motivating example for adjoint functors involving transposes of linear operators, involving (say) adjoint representations of Lie groups?",,"['linear-algebra', 'soft-question', 'category-theory']"
72,"If $A,B\in M_2(\mathbb{R})$, show that $(AB-BA)(AB-BA)$ is a scalar matrix.","If , show that  is a scalar matrix.","A,B\in M_2(\mathbb{R}) (AB-BA)(AB-BA)","$M_2(\mathbb{R})$ is the set of $2\times 2$ matrices. If $A,B\in M_2(\mathbb{R})$, show that $(AB-BA)(AB-BA)$ is a scalar matrix. I'm a bit stuck with this. Until now I know : Matrix multiplication is associative but in general, not commutative. And other basic properties of matrices. I've tried to do it via the perhaps dumb way, that is, working with the matrix elements themselves: And it works, but is there a less cumbersome way? I've tried to write: $$(AB-BA)(AB-BA)\stackrel{?}{=}\lambda I$$ And multiply both sides by $\frac{1}{\lambda}$ to see if something interesting/helpful happens but got nothing. And I've tried to expand: $$(AB-BA)(AB-BA)$$ But also yielded nothing. So is that the only way to do it or there is some more interesting way?","$M_2(\mathbb{R})$ is the set of $2\times 2$ matrices. If $A,B\in M_2(\mathbb{R})$, show that $(AB-BA)(AB-BA)$ is a scalar matrix. I'm a bit stuck with this. Until now I know : Matrix multiplication is associative but in general, not commutative. And other basic properties of matrices. I've tried to do it via the perhaps dumb way, that is, working with the matrix elements themselves: And it works, but is there a less cumbersome way? I've tried to write: $$(AB-BA)(AB-BA)\stackrel{?}{=}\lambda I$$ And multiply both sides by $\frac{1}{\lambda}$ to see if something interesting/helpful happens but got nothing. And I've tried to expand: $$(AB-BA)(AB-BA)$$ But also yielded nothing. So is that the only way to do it or there is some more interesting way?",,"['linear-algebra', 'matrices']"
73,"If $A$ is diagonal and positive and $B$ is skew-hermitian, does $AB$ have only pure imaginary eigenvalues?","If  is diagonal and positive and  is skew-hermitian, does  have only pure imaginary eigenvalues?",A B AB,"Let $A$ be diagonal with strictly positive (real) entries, and let $B$ be skew hermitian.  Can it be shown that the eigenvalues of $AB$ are pure imaginary? I suspect this also holds in the more general case that $A$ is symmetric positive definite.","Let $A$ be diagonal with strictly positive (real) entries, and let $B$ be skew hermitian.  Can it be shown that the eigenvalues of $AB$ are pure imaginary? I suspect this also holds in the more general case that $A$ is symmetric positive definite.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
74,"Does $A$ being positive definite imply $\ker A = 0\,$?",Does  being positive definite imply ?,"A \ker A = 0\,","If $A$ is a positive definite matrix can it be concluded that the kernel of $A$ is $\{0\}$ ? pf: R.T.P $\ker A = 0$ . Suppose not, i.e., there exists some $x\in\ker A$ s.t $x\neq 0$ , then $$Ax = 0\;\Longrightarrow x^T Ax = 0$$ which is a contradiction by definition of positive definite. Therefore $\ker A=\{0\}$ .","If is a positive definite matrix can it be concluded that the kernel of is ? pf: R.T.P . Suppose not, i.e., there exists some s.t , then which is a contradiction by definition of positive definite. Therefore .",A A \{0\} \ker A = 0 x\in\ker A x\neq 0 Ax = 0\;\Longrightarrow x^T Ax = 0 \ker A=\{0\},"['linear-algebra', 'proof-verification', 'inner-products', 'positive-definite']"
75,Proving the transpose / dual map is well defined.,Proving the transpose / dual map is well defined.,,"The definition for a dual map is as follows: The dual map, or transpose of linear $f:V \rightarrow W $ is given by   $f^t(g)(v) = g(f(v)) $ for $\forall g \in W^* , v \in V $. In my lecture notes, I have the following proof to show that the definition is well defined: $f^t (g)(a_1v_1 + a_2v_2) = g(f(a_1v_1 + a_2v_2)) $ $  = ag(f(v_1)) + a_2 g(f(v_2))$ $= a_1 f^t(g)(v_1) + a_2f^t(g)(v_2)$ and so $f^t(g) \in V^* $. How does this last step show $f^t(g) \in V^* $? How can I get my head around this proof? My understanding is that $g(f(v))$ takes in an element of $V$, applies $f$ to obtain an element of $W$ and then the functional $g$ to produce an element of the field ($K$), ultimately going from $V \rightarrow K$. Thus the function itself is an element of $V^*$. How is this intuition shown in the proof above?","The definition for a dual map is as follows: The dual map, or transpose of linear $f:V \rightarrow W $ is given by   $f^t(g)(v) = g(f(v)) $ for $\forall g \in W^* , v \in V $. In my lecture notes, I have the following proof to show that the definition is well defined: $f^t (g)(a_1v_1 + a_2v_2) = g(f(a_1v_1 + a_2v_2)) $ $  = ag(f(v_1)) + a_2 g(f(v_2))$ $= a_1 f^t(g)(v_1) + a_2f^t(g)(v_2)$ and so $f^t(g) \in V^* $. How does this last step show $f^t(g) \in V^* $? How can I get my head around this proof? My understanding is that $g(f(v))$ takes in an element of $V$, applies $f$ to obtain an element of $W$ and then the functional $g$ to produce an element of the field ($K$), ultimately going from $V \rightarrow K$. Thus the function itself is an element of $V^*$. How is this intuition shown in the proof above?",,"['linear-algebra', 'vector-spaces', 'duality-theorems']"
76,Lights Out variant: what matrix to use when whole row+column gets flipped?,Lights Out variant: what matrix to use when whole row+column gets flipped?,,"I've done my research and found a few similar questions here and on other sites, but none of them have what I'm looking for. Lights out is a simple game that has interesting math-based solutions (info here ). Using the typical rules in a 5x5 board, clicking anywhere in the grid causes the clicked spot and its 4 immediate neighbours to get flipped.  But I want to implement a solution where the entire row and column get flipped. For example, in this board 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Clicking on the point at coordinate (4, 4) will result in 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1  0 0 0 1 0 Given a 5x5 board, I know how to solve it using Gaussian elimination when using the standard rules.  I read this short pdf that explained the linear algebra theory very well and showed how a solution can be obtained by solving Ax = b , where b is the column-vector of the current board and A is the following 25x25 matrix C I 0 0 0    where    I = identity(5) and C = 1 1 0 0 0 I C I 0 0                                     1 1 1 0 0 0 I C I 0                                     0 1 1 1 0  0 0 I C I                                     0 0 1 1 1 0 0 0 I C                                     0 0 0 1 1 I figured out how to do Gaussian elimination in modulus 2 in order to solve the matrix equation and was able to solve 5x5 boards. I also found out that by just changing the C matrix slightly, I can easily use the same code to solve 3x3. I'm pretty sure that in order to solve the variant that I want (entire row+column gets flipped), all I need to do is figure out the C matrix.  But I tried sitting down with pencil and paper for a while and do something similar to what the PDF I linked to was doing, but I couldn't figure it out. Does anyone know how to derive the matrix required for this?","I've done my research and found a few similar questions here and on other sites, but none of them have what I'm looking for. Lights out is a simple game that has interesting math-based solutions (info here ). Using the typical rules in a 5x5 board, clicking anywhere in the grid causes the clicked spot and its 4 immediate neighbours to get flipped.  But I want to implement a solution where the entire row and column get flipped. For example, in this board 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Clicking on the point at coordinate (4, 4) will result in 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1  0 0 0 1 0 Given a 5x5 board, I know how to solve it using Gaussian elimination when using the standard rules.  I read this short pdf that explained the linear algebra theory very well and showed how a solution can be obtained by solving Ax = b , where b is the column-vector of the current board and A is the following 25x25 matrix C I 0 0 0    where    I = identity(5) and C = 1 1 0 0 0 I C I 0 0                                     1 1 1 0 0 0 I C I 0                                     0 1 1 1 0  0 0 I C I                                     0 0 1 1 1 0 0 0 I C                                     0 0 0 1 1 I figured out how to do Gaussian elimination in modulus 2 in order to solve the matrix equation and was able to solve 5x5 boards. I also found out that by just changing the C matrix slightly, I can easily use the same code to solve 3x3. I'm pretty sure that in order to solve the variant that I want (entire row+column gets flipped), all I need to do is figure out the C matrix.  But I tried sitting down with pencil and paper for a while and do something similar to what the PDF I linked to was doing, but I couldn't figure it out. Does anyone know how to derive the matrix required for this?",,"['linear-algebra', 'matrices']"
77,"If $A>0$ , $B>0$ , $A-B>0$ then $\rho (A) - \rho (B) > 0$?","If  ,  ,  then ?",A>0 B>0 A-B>0 \rho (A) - \rho (B) > 0,"let $A,B\in M_n$, suppose $A>0$ (i.e, all $a_{ij}>0$) $\rho (A) = \max \{ \left| \lambda  \right|:\lambda $ is eigenvalue of $A$   $\}$ $B>0$ $A-B>0$ Why does $\rho (A) - \rho (B) > 0$?","let $A,B\in M_n$, suppose $A>0$ (i.e, all $a_{ij}>0$) $\rho (A) = \max \{ \left| \lambda  \right|:\lambda $ is eigenvalue of $A$   $\}$ $B>0$ $A-B>0$ Why does $\rho (A) - \rho (B) > 0$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
78,Determinant of a finite order matrix,Determinant of a finite order matrix,,"Let $M$ be a $5\times 5$ with real entries. Suppose $M$ has finite   order and $\det(M-I_5)\neq 0$. Find $\det(M)$. I am trying to do this old algebra qual problem. So far I know that since $M$ is of finite order, say $|M|=n$, then $1=\det(I_5)=\det(M^n)=[\det(M)]^n$ and so $\det(M)$ is a root of unity. $M$ has real entries and so the determinant is real, but the only real roots of unity are $1$ and $-1$. I don't know how to use the fact that $M$ is $5\times 5$ or that $1$ is not an eigenvalue. Can we perhaps say something about the value of $n$?","Let $M$ be a $5\times 5$ with real entries. Suppose $M$ has finite   order and $\det(M-I_5)\neq 0$. Find $\det(M)$. I am trying to do this old algebra qual problem. So far I know that since $M$ is of finite order, say $|M|=n$, then $1=\det(I_5)=\det(M^n)=[\det(M)]^n$ and so $\det(M)$ is a root of unity. $M$ has real entries and so the determinant is real, but the only real roots of unity are $1$ and $-1$. I don't know how to use the fact that $M$ is $5\times 5$ or that $1$ is not an eigenvalue. Can we perhaps say something about the value of $n$?",,['linear-algebra']
79,Determinant of a $2n$ square block matrix in which all blocks commute,Determinant of a  square block matrix in which all blocks commute,2n,"Problem: Let $A , B , C , D$ be commuting $n$-square matrices. Consider the $2n$-square block matrix  $$M=\begin{pmatrix}  A & B  \\  C & D\end{pmatrix}$$  Prove that $|M|= |A||D| - |B||C|$, where $|M|$ means the determinant. I should also state that this from a beginning Linear Algebra book, so I have not studied any fancy determinant formulas yet. My problem here is that everything I can try involves multiplication but there is a minus sign on the right hand side which I cannot presently handle. Note: (this is not the same question as has been asked before here on this site as the formula here is quite different.)","Problem: Let $A , B , C , D$ be commuting $n$-square matrices. Consider the $2n$-square block matrix  $$M=\begin{pmatrix}  A & B  \\  C & D\end{pmatrix}$$  Prove that $|M|= |A||D| - |B||C|$, where $|M|$ means the determinant. I should also state that this from a beginning Linear Algebra book, so I have not studied any fancy determinant formulas yet. My problem here is that everything I can try involves multiplication but there is a minus sign on the right hand side which I cannot presently handle. Note: (this is not the same question as has been asked before here on this site as the formula here is quite different.)",,"['linear-algebra', 'matrices', 'determinant']"
80,On volume forms and norms on exterior powers,On volume forms and norms on exterior powers,,"Let $V$ be a $n$-dimensional vector space. Given an inner product on $V$ one may define an inner product on the simple $k$-vectors of $\Lambda^k(V)$ by  $$\langle v_1 \wedge \cdots \wedge v_k, w_1 \wedge \cdots \wedge w_k\rangle_{\Lambda^k(V)}   := \operatorname{det}\left(\langle v_i, w_j \rangle_V\right)$$ and extend it bilinearly. As usual this induces a norm on $\Lambda^k(V)$. Burago/Ivanov claim in [Lemma 2.4, p. 6] that an oriented volume form $\omega\in \Lambda^2(V^\ast) \cong \left(\Lambda^2(V)\right)^\ast$ on $V$ determines a linear isometry $J: V \to V^\ast$ in ""a standard way"". I don't understand the ""isometry""-part. Here is what I have so far: Define the mapping $J:V \to V^\ast$ by $J(u)(v) := \omega(u \wedge v), v\in V$. I can show that this is an isomorphism. I can define a somewhat ""dual volume form"" $\omega^\ast \in \Lambda^2(V) \cong \Lambda^2(V^{\ast\ast}) \cong \left(\Lambda^2(V^\ast)\right)^\ast$ by means of  $$\omega^\ast(l\wedge g) := \omega\left(J^{-1}(l)\wedge J^{-1}(g)\right)$$ Thus,  $$\omega^\ast\left(J(u)\wedge J(u')\right) = \omega(u\wedge u').$$ This looks quite promising already.  (I am able to generalise this to $\Lambda^n(V)$ via $\widetilde J: \Lambda^{n-1}(V) \to V^\ast, \sigma \mapsto \omega(\sigma \wedge \cdot)$ and Hodge dual) The way Burago/Ivanov use the ""isometry""-part in their paper is $\left|J(v) \wedge J(v')\right| = \left|v \wedge v'\right|$ though (where the norms are on the respective exterior powers). Is there a relationship between the induced norm on the exterior power and a corresponding volume form? Maybe by choosing an orthonormal basis for $V$ and taking the standard volume form $\varepsilon^1 \wedge \cdots \wedge \varepsilon^n$ determined by the dual basis $\{\varepsilon^i\}$ ?","Let $V$ be a $n$-dimensional vector space. Given an inner product on $V$ one may define an inner product on the simple $k$-vectors of $\Lambda^k(V)$ by  $$\langle v_1 \wedge \cdots \wedge v_k, w_1 \wedge \cdots \wedge w_k\rangle_{\Lambda^k(V)}   := \operatorname{det}\left(\langle v_i, w_j \rangle_V\right)$$ and extend it bilinearly. As usual this induces a norm on $\Lambda^k(V)$. Burago/Ivanov claim in [Lemma 2.4, p. 6] that an oriented volume form $\omega\in \Lambda^2(V^\ast) \cong \left(\Lambda^2(V)\right)^\ast$ on $V$ determines a linear isometry $J: V \to V^\ast$ in ""a standard way"". I don't understand the ""isometry""-part. Here is what I have so far: Define the mapping $J:V \to V^\ast$ by $J(u)(v) := \omega(u \wedge v), v\in V$. I can show that this is an isomorphism. I can define a somewhat ""dual volume form"" $\omega^\ast \in \Lambda^2(V) \cong \Lambda^2(V^{\ast\ast}) \cong \left(\Lambda^2(V^\ast)\right)^\ast$ by means of  $$\omega^\ast(l\wedge g) := \omega\left(J^{-1}(l)\wedge J^{-1}(g)\right)$$ Thus,  $$\omega^\ast\left(J(u)\wedge J(u')\right) = \omega(u\wedge u').$$ This looks quite promising already.  (I am able to generalise this to $\Lambda^n(V)$ via $\widetilde J: \Lambda^{n-1}(V) \to V^\ast, \sigma \mapsto \omega(\sigma \wedge \cdot)$ and Hodge dual) The way Burago/Ivanov use the ""isometry""-part in their paper is $\left|J(v) \wedge J(v')\right| = \left|v \wedge v'\right|$ though (where the norms are on the respective exterior powers). Is there a relationship between the induced norm on the exterior power and a corresponding volume form? Maybe by choosing an orthonormal basis for $V$ and taking the standard volume form $\varepsilon^1 \wedge \cdots \wedge \varepsilon^n$ determined by the dual basis $\{\varepsilon^i\}$ ?",,"['linear-algebra', 'normed-spaces', 'volume', 'multilinear-algebra', 'exterior-algebra']"
81,Is there a non-singular matrix with $k$ ones on each row?,Is there a non-singular matrix with  ones on each row?,k,"Let $n$ be a fixed positive integer. I would like to know for what values of $k$ there exists an $n$ by $n$ $0/1$ matrix that is non-singular with exactly $k$ ones per row. Clearly if $k=1$ then the identity matrix is non-singular. Also if $k=n$ there are no non-singular matrices. What can one say about $1 < k  < n$? If $n$ is large, is it true for almost all $k$ in the range?","Let $n$ be a fixed positive integer. I would like to know for what values of $k$ there exists an $n$ by $n$ $0/1$ matrix that is non-singular with exactly $k$ ones per row. Clearly if $k=1$ then the identity matrix is non-singular. Also if $k=n$ there are no non-singular matrices. What can one say about $1 < k  < n$? If $n$ is large, is it true for almost all $k$ in the range?",,[]
82,What is the geometric implication of subtracting two Matrices representing linear transformations?,What is the geometric implication of subtracting two Matrices representing linear transformations?,,"If we have two linear transformations denoted by matrices $A, B$ operating on an arbitrary vector $v \in \mathbb R^n$, then how does $Av$ and $Bv$ differ geometrically from $(A-B)v$ ? Does the difference inherit properties from the two linear transformations, or is there no pattern at all? The reason why I ask pertains to eigen vectors, for they are sent to the null vector when operated upon by $\lambda I - A$ and I am trying to geometrically understand why. If you can explain this second question and not the first one, that would also be fine.","If we have two linear transformations denoted by matrices $A, B$ operating on an arbitrary vector $v \in \mathbb R^n$, then how does $Av$ and $Bv$ differ geometrically from $(A-B)v$ ? Does the difference inherit properties from the two linear transformations, or is there no pattern at all? The reason why I ask pertains to eigen vectors, for they are sent to the null vector when operated upon by $\lambda I - A$ and I am trying to geometrically understand why. If you can explain this second question and not the first one, that would also be fine.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations', 'geometric-interpretation']"
83,"If $T$ injective or $T$ surjective, what is the composition $T^\ast T$? (where $T^\ast$ denotes adjoint of linear map $T$)","If  injective or  surjective, what is the composition ? (where  denotes adjoint of linear map )",T T T^\ast T T^\ast T,"Let $T:V \to W$ be a linear transformation between inner product spaces. Then $T^\ast: W \to V$ denotes the linear transformation with the property that for every $v \in V$ and $w \in W$, $$\langle T(v),w \rangle = \langle v, T^\ast(w) \rangle.$$ We call $T^\ast$ the adjoint of $T:V \to W$. $\cdot$ If $T$ is injective is $T^\ast T$ injective (or possibly a bijection)? $\cdot$ If $T$ is surjective is $T T^\ast$ surjective (or a bijection)? How can we prove this? Any pointers in the right direction appreciated.","Let $T:V \to W$ be a linear transformation between inner product spaces. Then $T^\ast: W \to V$ denotes the linear transformation with the property that for every $v \in V$ and $w \in W$, $$\langle T(v),w \rangle = \langle v, T^\ast(w) \rangle.$$ We call $T^\ast$ the adjoint of $T:V \to W$. $\cdot$ If $T$ is injective is $T^\ast T$ injective (or possibly a bijection)? $\cdot$ If $T$ is surjective is $T T^\ast$ surjective (or a bijection)? How can we prove this? Any pointers in the right direction appreciated.",,['linear-algebra']
84,Does the tensor product with a finite dimensional vector space gives you some kind of semi-basis?,Does the tensor product with a finite dimensional vector space gives you some kind of semi-basis?,,"Let $V$ be a (possibly infinite dimensional) vector space, and $W$ a finite dimensional one. Then, one can define the tensor product $V\otimes W$ as the free vector space on $V\times W$ modulo some relations. Now, if $V$ is finite dimensional, and if $\{e_1,\ldots,e_m\}$ is a basis for $V$ and $\{e'_1,\ldots,e'_n\}$ for $W$, we get a basis $$\{e_i\otimes e'_j:1\le i\le m,q\le j\le n\}$$ for $V\otimes W$. In particular, every element $u\in V\otimes W$ can be expressed as   $$u=v_1\otimes e'_1+\cdots+v_n\otimes e'_n$$   for unique $v_1,\ldots,v_n\in V$. Question: Does this statement also holds when $V$ is infinite dimensional? I can easily show that the elements $v_i\in V$ exist, but I am not sure why they would be unique. Existence: An element $u\in V\times W$ is the projection of a formal sum of finitely many $(v_i,w_i)\in V\times W$, i.e. $u=v_1\otimes w_1+\cdots v_n\otimes w_n$. Then, we can express each $w_i$ in terms of the basis.","Let $V$ be a (possibly infinite dimensional) vector space, and $W$ a finite dimensional one. Then, one can define the tensor product $V\otimes W$ as the free vector space on $V\times W$ modulo some relations. Now, if $V$ is finite dimensional, and if $\{e_1,\ldots,e_m\}$ is a basis for $V$ and $\{e'_1,\ldots,e'_n\}$ for $W$, we get a basis $$\{e_i\otimes e'_j:1\le i\le m,q\le j\le n\}$$ for $V\otimes W$. In particular, every element $u\in V\otimes W$ can be expressed as   $$u=v_1\otimes e'_1+\cdots+v_n\otimes e'_n$$   for unique $v_1,\ldots,v_n\in V$. Question: Does this statement also holds when $V$ is infinite dimensional? I can easily show that the elements $v_i\in V$ exist, but I am not sure why they would be unique. Existence: An element $u\in V\times W$ is the projection of a formal sum of finitely many $(v_i,w_i)\in V\times W$, i.e. $u=v_1\otimes w_1+\cdots v_n\otimes w_n$. Then, we can express each $w_i$ in terms of the basis.",,"['linear-algebra', 'abstract-algebra', 'differential-geometry', 'tensor-products', 'multilinear-algebra']"
85,Prove that if $A^2+I=0$ then $A$ is similar over $\mathbb{R}$ to this matrix,Prove that if  then  is similar over  to this matrix,A^2+I=0 A \mathbb{R},$A$ is a matrix which satisfies $A^2 + I = 0$. Prove that $A$ is of even order $2k$ and $A$ is similar over $\mathbb{R}$ to    $$\begin{bmatrix}   0 & I  \\   -I & 0 \\   \end{bmatrix}$$   where $I$ is the identity matrix of order $k$. I could do the even thing as characteristic polyomial would be some power of minimal polynomial which is $x^2 + 1$ over $\mathbb{R}$. But how to do the next one ?,$A$ is a matrix which satisfies $A^2 + I = 0$. Prove that $A$ is of even order $2k$ and $A$ is similar over $\mathbb{R}$ to    $$\begin{bmatrix}   0 & I  \\   -I & 0 \\   \end{bmatrix}$$   where $I$ is the identity matrix of order $k$. I could do the even thing as characteristic polyomial would be some power of minimal polynomial which is $x^2 + 1$ over $\mathbb{R}$. But how to do the next one ?,,"['linear-algebra', 'matrices']"
86,Why does matrix multiplication represent linear transformation compositions?,Why does matrix multiplication represent linear transformation compositions?,,"I know it's probably a silly question, but I'm trying to figure out why was matrix multiplication (the standard one) defined the way it was defined. I know that it was defined like that so we would gain invariance under change of basis: $PAP^{-1}+PBP^{-1}=P(A+B)P^{-1}$ and $(PAP^{-1})(PBP^{-1})=PABP^{-1}$ and that ofcourse the case. But another explanation that was suggested is: ""We defined matrix multiplication this way so that if $A$ is the matrix of a linear transformation $T_1$ with respect to some basis $s$ and $B$ is the matrix of a linear transformation $T_2$ with respect to the same basis $s$ then $AB$ is the matrix of $T_1$ composition with $T_2$ (I don't know the command for composition operator) with respect to basis $s$. Again, this is a completely legitimate aspiration, but I fail to see why it follows. Why is linear transformation composition equivalent to matrix multiplication?","I know it's probably a silly question, but I'm trying to figure out why was matrix multiplication (the standard one) defined the way it was defined. I know that it was defined like that so we would gain invariance under change of basis: $PAP^{-1}+PBP^{-1}=P(A+B)P^{-1}$ and $(PAP^{-1})(PBP^{-1})=PABP^{-1}$ and that ofcourse the case. But another explanation that was suggested is: ""We defined matrix multiplication this way so that if $A$ is the matrix of a linear transformation $T_1$ with respect to some basis $s$ and $B$ is the matrix of a linear transformation $T_2$ with respect to the same basis $s$ then $AB$ is the matrix of $T_1$ composition with $T_2$ (I don't know the command for composition operator) with respect to basis $s$. Again, this is a completely legitimate aspiration, but I fail to see why it follows. Why is linear transformation composition equivalent to matrix multiplication?",,"['linear-algebra', 'matrices', 'linear-transformations']"
87,Bounds on norm of matrix exponentials,Bounds on norm of matrix exponentials,,"Given $A = n\times n$ matrix with the real parts of its eigenvalues are contained in $[\alpha, \beta]$ where $-\infty < \alpha \leq \beta <\infty$. For any $\epsilon > 0$ and any norm $||.||$ (since any norm is equivalent on an $n$-dimensional finite space), show that there exists a constant $M > 0$ such that the two inequalities hold: (a) $||e^{At}|| \leq Me^{(\alpha - \epsilon)t}$ for every $t\leq 0$ (b) $||e^{At}|| \leq Me^{(\beta + \epsilon)t}$ for every $t\geq 0$ If all the real parts of eigenvalues are semisimple (i.e, geometric multiplicity of $\lambda$ $=$ algebraitc multiplicity of $\lambda$), then we can choose $\epsilon = 0$ in part (a). My attempt: I'm currently trying to modify the proof in Theorem 2.61 in Chicone's book (ODEs with applications), but I'm still stucked on modifying it successfully, as I couldn't use the two conditions $t\leq 0$ and real parts of its eigenvalues are contained in $[\alpha, \beta]$. For the semisimple part, I couldn't see where to start:P Can someone please help with this problem?","Given $A = n\times n$ matrix with the real parts of its eigenvalues are contained in $[\alpha, \beta]$ where $-\infty < \alpha \leq \beta <\infty$. For any $\epsilon > 0$ and any norm $||.||$ (since any norm is equivalent on an $n$-dimensional finite space), show that there exists a constant $M > 0$ such that the two inequalities hold: (a) $||e^{At}|| \leq Me^{(\alpha - \epsilon)t}$ for every $t\leq 0$ (b) $||e^{At}|| \leq Me^{(\beta + \epsilon)t}$ for every $t\geq 0$ If all the real parts of eigenvalues are semisimple (i.e, geometric multiplicity of $\lambda$ $=$ algebraitc multiplicity of $\lambda$), then we can choose $\epsilon = 0$ in part (a). My attempt: I'm currently trying to modify the proof in Theorem 2.61 in Chicone's book (ODEs with applications), but I'm still stucked on modifying it successfully, as I couldn't use the two conditions $t\leq 0$ and real parts of its eigenvalues are contained in $[\alpha, \beta]$. For the semisimple part, I couldn't see where to start:P Can someone please help with this problem?",,"['linear-algebra', 'ordinary-differential-equations']"
88,The set of all points $\in \mathbb{R}^3$ twice as far from $\mathbf{a}$ as they are from $\mathbf{b}$?,The set of all points  twice as far from  as they are from ?,\in \mathbb{R}^3 \mathbf{a} \mathbf{b},"I am told that, given two points $\mathbf{a}, \mathbf{b} \in \mathbb{R}^3$, the set of all points which are twice as far from $\mathbf{a}$ as they are from $\mathbf{b}$: $$\{\mathbf{p} : |\mathbf{p} - \mathbf{a}| = 2(|\mathbf{p} - \mathbf{b}|)\}$$ is a sphere. I'm wondering why this is true intuitively. Given two points $\mathbf{a} = (a_x, a_y, a_z)$, $\mathbf{b} = (b_x, b_y, b_z)$, I can write down the equation: $$\sqrt{(a_x - p_x)^2 + (a_y - p_y)^2 + (a_z - p_z)^2} = 2\sqrt{(b_x - p_x)^2 + (b_y - p_y)^2 + (b_z - p_z)^2}$$ go through the algebra, square both sides, collect like terms, and get the equation of a sphere. But I can't manage to pierce the fog of algebra with my intuition. What I'm trying to understand intuitively or geometrically is: What determines the radius and center of the resulting sphere? If I ask instead for the set of all points which are equidistant from $\mathbf{A}$ and $\mathbf{B}$, I get a plane. If I ask for the same in $\mathbb{R}^2$, I get a line. If I ask for all points equidistant from just one point in $\mathbb{R}^3$, though, I get a sphere. Is it true in general that the set of all points in $\mathbb{R}^n$ equidistant from $k$ selected points is a subspace of dimension $n - k$? If I don't ask for equidistant points but instead ask for something like the above, i.e some distances are proportional to some other distances, what is the analogous result? Is there a geometric way of understanding the above? EDIT: It strikes me that a sphere isn't a subspace of $\mathbb{R}^3$ because it's not a vector space. Can I still talk about it having ""dimension"" 2?","I am told that, given two points $\mathbf{a}, \mathbf{b} \in \mathbb{R}^3$, the set of all points which are twice as far from $\mathbf{a}$ as they are from $\mathbf{b}$: $$\{\mathbf{p} : |\mathbf{p} - \mathbf{a}| = 2(|\mathbf{p} - \mathbf{b}|)\}$$ is a sphere. I'm wondering why this is true intuitively. Given two points $\mathbf{a} = (a_x, a_y, a_z)$, $\mathbf{b} = (b_x, b_y, b_z)$, I can write down the equation: $$\sqrt{(a_x - p_x)^2 + (a_y - p_y)^2 + (a_z - p_z)^2} = 2\sqrt{(b_x - p_x)^2 + (b_y - p_y)^2 + (b_z - p_z)^2}$$ go through the algebra, square both sides, collect like terms, and get the equation of a sphere. But I can't manage to pierce the fog of algebra with my intuition. What I'm trying to understand intuitively or geometrically is: What determines the radius and center of the resulting sphere? If I ask instead for the set of all points which are equidistant from $\mathbf{A}$ and $\mathbf{B}$, I get a plane. If I ask for the same in $\mathbb{R}^2$, I get a line. If I ask for all points equidistant from just one point in $\mathbb{R}^3$, though, I get a sphere. Is it true in general that the set of all points in $\mathbb{R}^n$ equidistant from $k$ selected points is a subspace of dimension $n - k$? If I don't ask for equidistant points but instead ask for something like the above, i.e some distances are proportional to some other distances, what is the analogous result? Is there a geometric way of understanding the above? EDIT: It strikes me that a sphere isn't a subspace of $\mathbb{R}^3$ because it's not a vector space. Can I still talk about it having ""dimension"" 2?",,"['linear-algebra', 'geometry', 'vector-spaces']"
89,Measuring rotation and translation differences between two matrices,Measuring rotation and translation differences between two matrices,,"I am developing a docking application in which I want to have for every step the difference between the target transformation matrix and the user's transformation matrix. Now I don't have any problem with the coding part but rather with the linear algebra part. So let's say T is my target transformation matrix, and U is the user's transformation matrix. To get the difference between the two matrices I do: (U^-1) * (T) = Difference Now my matrices are formed with translation, rotation and scaling. My question would be how I should measure the total translation difference as well as the total rotation difference (not interested in the scaling part). Thanks in advance,","I am developing a docking application in which I want to have for every step the difference between the target transformation matrix and the user's transformation matrix. Now I don't have any problem with the coding part but rather with the linear algebra part. So let's say T is my target transformation matrix, and U is the user's transformation matrix. To get the difference between the two matrices I do: (U^-1) * (T) = Difference Now my matrices are formed with translation, rotation and scaling. My question would be how I should measure the total translation difference as well as the total rotation difference (not interested in the scaling part). Thanks in advance,",,"['linear-algebra', 'linear-programming']"
90,Must a basis for an $n$-dimensional vector space have $n$ vectors?,Must a basis for an -dimensional vector space have  vectors?,n n,"Does a basis for an $n$-dimensional vector space have to have $n$ vectors? For example, if I form a basis for $\mathbb{R}^n$, do I need at least $n$ vectors in my basis set? In other words, can I form a basis for $\mathbb{R}^n$ by using only $n-1$, or less, vectors? Note that, in this question, we only consider the whole vector space not creating a basis for a subspace.","Does a basis for an $n$-dimensional vector space have to have $n$ vectors? For example, if I form a basis for $\mathbb{R}^n$, do I need at least $n$ vectors in my basis set? In other words, can I form a basis for $\mathbb{R}^n$ by using only $n-1$, or less, vectors? Note that, in this question, we only consider the whole vector space not creating a basis for a subspace.",,"['linear-algebra', 'vector-spaces']"
91,Prove that $\dim(V)$ is even,Prove that  is even,\dim(V),"Let $V$ be a finite dimensional vector space. Let $A_1,A_2: V\rightarrow V$ be commuting linear operators such that $A_1+A_2=-I$ where $I$ is the identity operator. Also $A_1,A_2$ have no negative eigenvalues. Prove that $\dim(V)$ is even. How to approach this problem?","Let $V$ be a finite dimensional vector space. Let $A_1,A_2: V\rightarrow V$ be commuting linear operators such that $A_1+A_2=-I$ where $I$ is the identity operator. Also $A_1,A_2$ have no negative eigenvalues. Prove that $\dim(V)$ is even. How to approach this problem?",,"['linear-algebra', 'matrices', 'vector-spaces']"
92,When is it true that $\dim(U \cap (V+W))=\dim(U \cap V + U \cap W)$?,When is it true that ?,\dim(U \cap (V+W))=\dim(U \cap V + U \cap W),"I apologize if this is a silly question( which may have been asked before), I was wondering after seeing a post on this list on math-overflow When is it true that $\dim(U \cap (V+W))=\dim(U \cap V + U \cap W$)? This is not true in general, since we can set $U,V,W$ to be three distinct lines in $\mathbb{R}^{2}$. For example, if if I set $U=\langle (1,0)\rangle$, $V=\langle (0,1)\rangle$, and $W=\langle (1,1) \rangle$. We then have that $\dim(U \cap (V+W))=\dim(U)=1$ $\dim(U \cap V + U \cap W)=0$ I was just curious more than anything.","I apologize if this is a silly question( which may have been asked before), I was wondering after seeing a post on this list on math-overflow When is it true that $\dim(U \cap (V+W))=\dim(U \cap V + U \cap W$)? This is not true in general, since we can set $U,V,W$ to be three distinct lines in $\mathbb{R}^{2}$. For example, if if I set $U=\langle (1,0)\rangle$, $V=\langle (0,1)\rangle$, and $W=\langle (1,1) \rangle$. We then have that $\dim(U \cap (V+W))=\dim(U)=1$ $\dim(U \cap V + U \cap W)=0$ I was just curious more than anything.",,"['linear-algebra', 'vector-spaces']"
93,Finding the Jordan Canonical form of a $6 \times 6$ matrix,Finding the Jordan Canonical form of a  matrix,6 \times 6,"Find the Jordan Canonical Form of the following matrix $$\begin{bmatrix}  1 & 0 & 0 & 0 & 0 & 0\\ 1 & 1 & 0 & 0 & 0 & 0\\ 1 & 0 & 1 & 0 & 0 & 0\\ 1 & 0 & 0 & 1 & 0 & 0\\ 1 & 0 & 0 & 0 & 1 & 0\\ 1 & 1 & 1 & 1 & 1 & 1\\ \end{bmatrix}$$ My try: I go about finding the Jordan Basis for this matrix. It is clear that $1$ is the only eigenvalue of this matrix. So $$A-I=\begin{bmatrix}0 & 0 & 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0 & 0 & 0\\ 1 & 1 & 1 & 1 & 1 & 0\\ \end{bmatrix}$$ Moreover Rank$(A-I)^2=1$. Moreover Rank$(A-I)^3=0$ . So We don't need to go further on evaluating the powers of matrices in our search for generalized eigenvectors.  $$(A-I)^2=\begin{bmatrix}0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0\\ 4 & 0 & 0 & 0 & 0 & 0\\ \end{bmatrix}$$ Also $(A-I)^3=0$. It is seen that the generalized eigenspace (say $U$) consists of $U$=span$\{v_1=(0,1,0,0,0,0)^t,v_2=(0,0,1,0,0,0)^t,v_3=(0,0,0,1,0,0)^t,v_4=(0,0,0,0,1,0)^t,v_5=(0,0,0,0,0,1)^t\}$ Here I run into a little problem. Since $(A-I)v_i=v_5$ for each $i=1,2,3,4$, I have only five vectors with me for the Jordan Canonical Basis. Moreover I can't choose any other arbitrary vector linearly independent to these four simply because if $v$ were such a vector , then there is no $i \gt 0$ (and integer) such that $(A-I)v^{i}=0$ I have also another question here. Since $(A-I)^3=0$, why should I not choose general eigen vectors corresponding to $(A-I)^3$?? I am a little stuck here. Thanks for the help!!!","Find the Jordan Canonical Form of the following matrix $$\begin{bmatrix}  1 & 0 & 0 & 0 & 0 & 0\\ 1 & 1 & 0 & 0 & 0 & 0\\ 1 & 0 & 1 & 0 & 0 & 0\\ 1 & 0 & 0 & 1 & 0 & 0\\ 1 & 0 & 0 & 0 & 1 & 0\\ 1 & 1 & 1 & 1 & 1 & 1\\ \end{bmatrix}$$ My try: I go about finding the Jordan Basis for this matrix. It is clear that $1$ is the only eigenvalue of this matrix. So $$A-I=\begin{bmatrix}0 & 0 & 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0 & 0 & 0\\ 1 & 1 & 1 & 1 & 1 & 0\\ \end{bmatrix}$$ Moreover Rank$(A-I)^2=1$. Moreover Rank$(A-I)^3=0$ . So We don't need to go further on evaluating the powers of matrices in our search for generalized eigenvectors.  $$(A-I)^2=\begin{bmatrix}0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0\\ 4 & 0 & 0 & 0 & 0 & 0\\ \end{bmatrix}$$ Also $(A-I)^3=0$. It is seen that the generalized eigenspace (say $U$) consists of $U$=span$\{v_1=(0,1,0,0,0,0)^t,v_2=(0,0,1,0,0,0)^t,v_3=(0,0,0,1,0,0)^t,v_4=(0,0,0,0,1,0)^t,v_5=(0,0,0,0,0,1)^t\}$ Here I run into a little problem. Since $(A-I)v_i=v_5$ for each $i=1,2,3,4$, I have only five vectors with me for the Jordan Canonical Basis. Moreover I can't choose any other arbitrary vector linearly independent to these four simply because if $v$ were such a vector , then there is no $i \gt 0$ (and integer) such that $(A-I)v^{i}=0$ I have also another question here. Since $(A-I)^3=0$, why should I not choose general eigen vectors corresponding to $(A-I)^3$?? I am a little stuck here. Thanks for the help!!!",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'jordan-normal-form']"
94,Why does ${\lambda _i}(A) \ge {\lambda _i}(B)$?,Why does ?,{\lambda _i}(A) \ge {\lambda _i}(B),"Let $A,B \in {M_n}$ are Hermitian and $A-B$ has only nonnegative eigenvalues.Why does ${\lambda _i}(A) \ge {\lambda _i}(B)$  (for $i=1,2,\ldots,n$) ?","Let $A,B \in {M_n}$ are Hermitian and $A-B$ has only nonnegative eigenvalues.Why does ${\lambda _i}(A) \ge {\lambda _i}(B)$  (for $i=1,2,\ldots,n$) ?",,"['linear-algebra', 'matrices']"
95,Is $\det(U_1\Lambda_1 U_1^t +U_2\Lambda_2 U_2^t +I)\le \det(\Lambda_1 +\Lambda_2 +I)$ correct?,Is  correct?,\det(U_1\Lambda_1 U_1^t +U_2\Lambda_2 U_2^t +I)\le \det(\Lambda_1 +\Lambda_2 +I),"I want to simplify or find an upper bound for the  determinant  $|K_1+K_2+I|$ where $I$ is identity matrix, $K_1$ and $K_2$ are positive semi-definite matrices of size $n$ and thus can be written as $K_1=U_1\Lambda_1 U_1^t$, $K_2=U_2\Lambda_2 U_2^t$,  in which  $U_1$ and $U_2$ are unitary matrices and $\Lambda_1, \Lambda_2$ are diagonal matrices. Can one say  $\det(U_1\Lambda_1 U_1^t +U_2\Lambda_2 U_2^t +I)\le \det(\Lambda_1  +\Lambda_2  +I)$? Hint: when we have only $K_1$ (or $K_2$) we can write: $|K_1+I|=|U_1\Lambda_1 U_1^t  +I|=|\Lambda_1 U_1^tU_1 +I|=|\Lambda_1   +I|$ where the second equality is due to the identity $\det(AB+I)=\det(BA+I)$.","I want to simplify or find an upper bound for the  determinant  $|K_1+K_2+I|$ where $I$ is identity matrix, $K_1$ and $K_2$ are positive semi-definite matrices of size $n$ and thus can be written as $K_1=U_1\Lambda_1 U_1^t$, $K_2=U_2\Lambda_2 U_2^t$,  in which  $U_1$ and $U_2$ are unitary matrices and $\Lambda_1, \Lambda_2$ are diagonal matrices. Can one say  $\det(U_1\Lambda_1 U_1^t +U_2\Lambda_2 U_2^t +I)\le \det(\Lambda_1  +\Lambda_2  +I)$? Hint: when we have only $K_1$ (or $K_2$) we can write: $|K_1+I|=|U_1\Lambda_1 U_1^t  +I|=|\Lambda_1 U_1^tU_1 +I|=|\Lambda_1   +I|$ where the second equality is due to the identity $\det(AB+I)=\det(BA+I)$.",,"['linear-algebra', 'matrices', 'optimization', 'eigenvalues-eigenvectors', 'determinant']"
96,Is there a problem in assuming that a point is the same thing of a vector?,Is there a problem in assuming that a point is the same thing of a vector?,,"I've read Apostol's Calculus , in the section on analytic geometry. He says that he's going to use 'vector' and 'point' interchangeably. But in Beardon's Algebra and Geometry , he argues that there is not agreement on what vectors are, some say that they are points on $\mathbb{R}^n$, some say that they are directed line segments and for others they are classes of line segments in which they are the same if they represent the same displacement. I'm curious if problems can appear by assuming that each of the definitions given by Beardon is equivalent.","I've read Apostol's Calculus , in the section on analytic geometry. He says that he's going to use 'vector' and 'point' interchangeably. But in Beardon's Algebra and Geometry , he argues that there is not agreement on what vectors are, some say that they are points on $\mathbb{R}^n$, some say that they are directed line segments and for others they are classes of line segments in which they are the same if they represent the same displacement. I'm curious if problems can appear by assuming that each of the definitions given by Beardon is equivalent.",,"['linear-algebra', 'analytic-geometry']"
97,Show that $Y$ is invertible,Show that  is invertible,Y,Let X be a $40\times40$ matrix such that $X^3 = 2I$. I want to show that $Y= X^2 -2X + 2I$ is invertible as well. I tried working with the equations to see if I can get Y as a product of matrices that I thought would be invertible such as $X$ or $X-I$ or $X^2+X+I$ but I couldn't get anything. I was also wondering if I can use properties of row and column spaces to approach this question. I know that $X$ is invertible so $det(X) \ne 0$ so the column vectors of $X$ are linearly independent and hence its rank is 40. Still I don't know how I can make use of this.,Let X be a $40\times40$ matrix such that $X^3 = 2I$. I want to show that $Y= X^2 -2X + 2I$ is invertible as well. I tried working with the equations to see if I can get Y as a product of matrices that I thought would be invertible such as $X$ or $X-I$ or $X^2+X+I$ but I couldn't get anything. I was also wondering if I can use properties of row and column spaces to approach this question. I know that $X$ is invertible so $det(X) \ne 0$ so the column vectors of $X$ are linearly independent and hence its rank is 40. Still I don't know how I can make use of this.,,"['linear-algebra', 'matrices', 'inverse']"
98,"Prove that every elementary matrix is invertible, and the inverse is again an elementary matrix.","Prove that every elementary matrix is invertible, and the inverse is again an elementary matrix.",,"I know that there are many proofs regarding this. However, the book i'm using seems to suggest another way to do it without giving an answer. What i mean by the another way is some other proofs that do not use the fact that elementary row operation can be expressed by multiplying elementary matrices. The book says that the lemma need to be proved only when the size of identity matrix is 2 by 2 . Does it have something to do with mathematical induction? That is, did the author want us to find out both induction hypothesis and induction step?","I know that there are many proofs regarding this. However, the book i'm using seems to suggest another way to do it without giving an answer. What i mean by the another way is some other proofs that do not use the fact that elementary row operation can be expressed by multiplying elementary matrices. The book says that the lemma need to be proved only when the size of identity matrix is 2 by 2 . Does it have something to do with mathematical induction? That is, did the author want us to find out both induction hypothesis and induction step?",,['linear-algebra']
99,Proving that $\det(A)R^n \subset\mathrm{Im}(\Phi)$,Proving that,\det(A)R^n \subset\mathrm{Im}(\Phi),"Let $R$ be a commutative ring with unity and consider the free $R$-module $R^n$. Given a matrix $A$ with coefficients in $R$, define a homomorphism $\Phi: R^n\to R^n$ by $\Phi(u) = Au$. My question is how to prove that $\det(A)R^n \subset \mathrm{Im}(\Phi)$? (We have that $\det(A)R^n = \{\det(A)u: \ u\in R^n\}$ is a submodule.) I started doing the obvious, ie, trying to show that for any $u\in R^n$, there is some $\tilde{u}\in R^n$ such that $\det(A)u = A\tilde{u}$. Then I fixed the canonical base and tried to show that this equation (which is a linear system) always have a solution, but this approach seems more linear algebra and less abstract algebra, also it's a little messy. Is there a nice way to prove this? Thanks.","Let $R$ be a commutative ring with unity and consider the free $R$-module $R^n$. Given a matrix $A$ with coefficients in $R$, define a homomorphism $\Phi: R^n\to R^n$ by $\Phi(u) = Au$. My question is how to prove that $\det(A)R^n \subset \mathrm{Im}(\Phi)$? (We have that $\det(A)R^n = \{\det(A)u: \ u\in R^n\}$ is a submodule.) I started doing the obvious, ie, trying to show that for any $u\in R^n$, there is some $\tilde{u}\in R^n$ such that $\det(A)u = A\tilde{u}$. Then I fixed the canonical base and tried to show that this equation (which is a linear system) always have a solution, but this approach seems more linear algebra and less abstract algebra, also it's a little messy. Is there a nice way to prove this? Thanks.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'modules', 'determinant']"
