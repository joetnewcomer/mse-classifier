,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Which matrix $P$ makes two matrices $A$ and $B$ similar,Which matrix  makes two matrices  and  similar,P A B,"Two square matrices $A,\ B$ are similar if $\exists P : PAP^{-1} = B$. In order to find $P$ for two given $A,\ B$ one can try to solve $PA-BP = 0$. I worked out this equation for abstract $3 \times 3$ matrices: $A = \left(\begin{matrix}a_{00} & a_{01} & a_{02}\\a_{10} & a_{11} & a_{12}\\a_{20} & a_{21} & a_{22}\end{matrix}\right),\  B = \left(\begin{matrix}b_{00} & b_{01} & b_{02}\\b_{10} & b_{11} & b_{12}\\b_{20} & b_{21} & b_{22}\end{matrix}\right),\  P = \left(\begin{matrix}p_{00} & p_{01} & p_{02}\\p_{10} & p_{11} & p_{12}\\p_{20} & p_{21} & p_{22}\end{matrix}\right)$ If one considers $3 \times 3$ matrices as vectors of a $9$ dimensional vector space, using the following matrix expressions: $PA \mapsto \left(\begin{matrix}a_{00} & a_{10} & a_{20} & 0 & 0 & 0 & 0 & 0 & 0\\a_{01} & a_{11} & a_{21} & 0 & 0 & 0 & 0 & 0 & 0\\a_{02} & a_{12} & a_{22} & 0 & 0 & 0 & 0 & 0 & 0\\0 & 0 & 0 & a_{00} & a_{10} & a_{20} & 0 & 0 & 0\\0 & 0 & 0 & a_{01} & a_{11} & a_{21} & 0 & 0 & 0\\0 & 0 & 0 & a_{02} & a_{12} & a_{22} & 0 & 0 & 0\\0 & 0 & 0 & 0 & 0 & 0 & a_{00} & a_{10} & a_{20}\\0 & 0 & 0 & 0 & 0 & 0 & a_{01} & a_{11} & a_{21}\\0 & 0 & 0 & 0 & 0 & 0 & a_{02} & a_{12} & a_{22}\end{matrix}\right)\left(\begin{matrix}p_{00}\\p_{01}\\p_{02}\\p_{10}\\p_{11}\\p_{12}\\p_{20}\\p_{21}\\p_{22}\end{matrix}\right)$ $BP \mapsto \left(\begin{matrix}b_{00} & 0 & 0 & b_{01} & 0 & 0 & b_{02} & 0 & 0\\0 & b_{00} & 0 & 0 & b_{01} & 0 & 0 & b_{02} & 0\\0 & 0 & b_{00} & 0 & 0 & b_{01} & 0 & 0 & b_{02}\\b_{10} & 0 & 0 & b_{11} & 0 & 0 & b_{12} & 0 & 0\\0 & b_{10} & 0 & 0 & b_{11} & 0 & 0 & b_{12} & 0\\0 & 0 & b_{10} & 0 & 0 & b_{11} & 0 & 0 & b_{12}\\b_{20} & 0 & 0 & b_{21} & 0 & 0 & b_{22} & 0 & 0\\0 & b_{20} & 0 & 0 & b_{21} & 0 & 0 & b_{22} & 0\\0 & 0 & b_{20} & 0 & 0 & b_{21} & 0 & 0 & b_{22}\end{matrix}\right)\left(\begin{matrix}p_{00}\\p_{01}\\p_{02}\\p_{10}\\p_{11}\\p_{12}\\p_{20}\\p_{21}\\p_{22}\end{matrix}\right)$ the equation to solve can be expressed as:  $$  (I \otimes A^t - B \otimes I)p = 0 $$ Where $\otimes$ stands for the Kronecker product of square matrices and $I$ is the identity matrix. The question is if this can be proven for arbitrary dimensions?","Two square matrices $A,\ B$ are similar if $\exists P : PAP^{-1} = B$. In order to find $P$ for two given $A,\ B$ one can try to solve $PA-BP = 0$. I worked out this equation for abstract $3 \times 3$ matrices: $A = \left(\begin{matrix}a_{00} & a_{01} & a_{02}\\a_{10} & a_{11} & a_{12}\\a_{20} & a_{21} & a_{22}\end{matrix}\right),\  B = \left(\begin{matrix}b_{00} & b_{01} & b_{02}\\b_{10} & b_{11} & b_{12}\\b_{20} & b_{21} & b_{22}\end{matrix}\right),\  P = \left(\begin{matrix}p_{00} & p_{01} & p_{02}\\p_{10} & p_{11} & p_{12}\\p_{20} & p_{21} & p_{22}\end{matrix}\right)$ If one considers $3 \times 3$ matrices as vectors of a $9$ dimensional vector space, using the following matrix expressions: $PA \mapsto \left(\begin{matrix}a_{00} & a_{10} & a_{20} & 0 & 0 & 0 & 0 & 0 & 0\\a_{01} & a_{11} & a_{21} & 0 & 0 & 0 & 0 & 0 & 0\\a_{02} & a_{12} & a_{22} & 0 & 0 & 0 & 0 & 0 & 0\\0 & 0 & 0 & a_{00} & a_{10} & a_{20} & 0 & 0 & 0\\0 & 0 & 0 & a_{01} & a_{11} & a_{21} & 0 & 0 & 0\\0 & 0 & 0 & a_{02} & a_{12} & a_{22} & 0 & 0 & 0\\0 & 0 & 0 & 0 & 0 & 0 & a_{00} & a_{10} & a_{20}\\0 & 0 & 0 & 0 & 0 & 0 & a_{01} & a_{11} & a_{21}\\0 & 0 & 0 & 0 & 0 & 0 & a_{02} & a_{12} & a_{22}\end{matrix}\right)\left(\begin{matrix}p_{00}\\p_{01}\\p_{02}\\p_{10}\\p_{11}\\p_{12}\\p_{20}\\p_{21}\\p_{22}\end{matrix}\right)$ $BP \mapsto \left(\begin{matrix}b_{00} & 0 & 0 & b_{01} & 0 & 0 & b_{02} & 0 & 0\\0 & b_{00} & 0 & 0 & b_{01} & 0 & 0 & b_{02} & 0\\0 & 0 & b_{00} & 0 & 0 & b_{01} & 0 & 0 & b_{02}\\b_{10} & 0 & 0 & b_{11} & 0 & 0 & b_{12} & 0 & 0\\0 & b_{10} & 0 & 0 & b_{11} & 0 & 0 & b_{12} & 0\\0 & 0 & b_{10} & 0 & 0 & b_{11} & 0 & 0 & b_{12}\\b_{20} & 0 & 0 & b_{21} & 0 & 0 & b_{22} & 0 & 0\\0 & b_{20} & 0 & 0 & b_{21} & 0 & 0 & b_{22} & 0\\0 & 0 & b_{20} & 0 & 0 & b_{21} & 0 & 0 & b_{22}\end{matrix}\right)\left(\begin{matrix}p_{00}\\p_{01}\\p_{02}\\p_{10}\\p_{11}\\p_{12}\\p_{20}\\p_{21}\\p_{22}\end{matrix}\right)$ the equation to solve can be expressed as:  $$  (I \otimes A^t - B \otimes I)p = 0 $$ Where $\otimes$ stands for the Kronecker product of square matrices and $I$ is the identity matrix. The question is if this can be proven for arbitrary dimensions?",,"['linear-algebra', 'matrices']"
1,There exists $v_0 \in V$ such that $\forall v \in V \space\space T(v)=\lambda v+ \alpha(v) v_0$,There exists  such that,v_0 \in V \forall v \in V \space\space T(v)=\lambda v+ \alpha(v) v_0,"Let $V$ be a vector space and $\lambda$ a constant real number. Suppose that $T: V \to V$ is a linear map and $\alpha: V \to \mathbb R$ is a linear functional on $V$. Suppose that for every $v \in Ker(\alpha)$ we have $T(v)=\lambda v$. Prove that:   There exists $v_0 \in V$ such that  $\forall v \in V  \space\space T(v)=\lambda v+ \alpha(v) v_0$ I'm completely blind! I don't know where to start! The question says if $v \in Ker(\alpha)$, $T(v)=\lambda v$. So, If $\alpha(v)=0$,  then $T(v)=\lambda v$. But how is this related to the existence of $v_0$?","Let $V$ be a vector space and $\lambda$ a constant real number. Suppose that $T: V \to V$ is a linear map and $\alpha: V \to \mathbb R$ is a linear functional on $V$. Suppose that for every $v \in Ker(\alpha)$ we have $T(v)=\lambda v$. Prove that:   There exists $v_0 \in V$ such that  $\forall v \in V  \space\space T(v)=\lambda v+ \alpha(v) v_0$ I'm completely blind! I don't know where to start! The question says if $v \in Ker(\alpha)$, $T(v)=\lambda v$. So, If $\alpha(v)=0$,  then $T(v)=\lambda v$. But how is this related to the existence of $v_0$?",,"['linear-algebra', 'vector-spaces', 'linear-transformations']"
2,Conjugate classes for 2x2 matrices,Conjugate classes for 2x2 matrices,,"I've read an unproved claim that every real 2 by 2 matrix is similar to a matrix in exactly one of the following categories: $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix}$ $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} + \beta \begin{bmatrix} 0 & 1 \\ 1 & 0\end{bmatrix}$ $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} + \beta \begin{bmatrix} 0 & -1 \\ 1 & 0\end{bmatrix}$ $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} + \beta \begin{bmatrix} 0 & 1 \\ 0 & 0\end{bmatrix}$ where $\alpha, \beta \in \mathbb{R}$ and $\beta \neq 0$. What is a general procedure for finding the appropriate class and the appropriate values of $\alpha$ and $\beta$ for an arbitrary real 2 by 2 matrix? I presume that one starts by computing the Jordan normal form of the matrix, which looks like $P\begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2\end{bmatrix}P^{-1} \quad \quad $ or $\quad \quad P\begin{bmatrix} \lambda & 1 \\ 0 & \lambda\end{bmatrix}P^{-1}$. In the latter case, it's pretty clear how to express (a conjugate of) the matrix in the 4th form above. In the former case (the diagonalizable case) when $\lambda_1 == \lambda_2$, it's even easier to express the matrix in the 1st form above. The hard part (the part I need help with) is how to express a diagonalizable matrix with distinct eigenvalues in either the 2nd or 3rd form above. In addition, I'd like an explanation of how one could generalize this result to larger matrices. Thanks","I've read an unproved claim that every real 2 by 2 matrix is similar to a matrix in exactly one of the following categories: $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix}$ $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} + \beta \begin{bmatrix} 0 & 1 \\ 1 & 0\end{bmatrix}$ $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} + \beta \begin{bmatrix} 0 & -1 \\ 1 & 0\end{bmatrix}$ $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} + \beta \begin{bmatrix} 0 & 1 \\ 0 & 0\end{bmatrix}$ where $\alpha, \beta \in \mathbb{R}$ and $\beta \neq 0$. What is a general procedure for finding the appropriate class and the appropriate values of $\alpha$ and $\beta$ for an arbitrary real 2 by 2 matrix? I presume that one starts by computing the Jordan normal form of the matrix, which looks like $P\begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2\end{bmatrix}P^{-1} \quad \quad $ or $\quad \quad P\begin{bmatrix} \lambda & 1 \\ 0 & \lambda\end{bmatrix}P^{-1}$. In the latter case, it's pretty clear how to express (a conjugate of) the matrix in the 4th form above. In the former case (the diagonalizable case) when $\lambda_1 == \lambda_2$, it's even easier to express the matrix in the 1st form above. The hard part (the part I need help with) is how to express a diagonalizable matrix with distinct eigenvalues in either the 2nd or 3rd form above. In addition, I'd like an explanation of how one could generalize this result to larger matrices. Thanks",,"['linear-algebra', 'matrices', 'lie-algebras']"
3,Volume of the intersection of two $n$-dimensional cubes,Volume of the intersection of two -dimensional cubes,n,"Let $U \in \mathbb{R}^n$ be a unitary matrix and $K \subset \mathbb{R}^n$ and $n$-dimensional cube centered at zero.   Let \begin{align} K_1=U \cdot K= \{ y: y=U x, x\in K\}. \end{align} That is $K_1$ is a rotation of $K$.  Next, let ${\rm Vol}(\cdot)$ be a volume operator (Lebesgue measure). Can we give lower bounds on  \begin{align} {\rm Vol}( K_1 \cap K), \end{align} The upper bounds are simple \begin{align} {\rm Vol}( K_1 \cap K) \le \min ({\rm Vol}( K_1 ),{\rm Vol}( K)), \end{align} and tight if $U$ is an identiy matrix. Lower Bound Based on inclusion Let $B$ be a ball of radius $r$ such that \begin{align} B \subset K_1\cap K, \end{align} then  \begin{align} \frac{\pi^{n/2}}{\Gamma(n/2+1)} r^n =Vol(B) \le  {\rm Vol}( K_1 \cap K)= 2^nr^n . \end{align} But,   since  $\frac{\pi^{n/2}}{\Gamma(n/2+1)} \to 0$ as  $n \to \infty$  and $2^n \to \infty$ as $n \to \infty$, the lower and the upper bounds do not match. Question: Can we come up with a lower bound and upper bounds of the same order as $n \to \infty$.","Let $U \in \mathbb{R}^n$ be a unitary matrix and $K \subset \mathbb{R}^n$ and $n$-dimensional cube centered at zero.   Let \begin{align} K_1=U \cdot K= \{ y: y=U x, x\in K\}. \end{align} That is $K_1$ is a rotation of $K$.  Next, let ${\rm Vol}(\cdot)$ be a volume operator (Lebesgue measure). Can we give lower bounds on  \begin{align} {\rm Vol}( K_1 \cap K), \end{align} The upper bounds are simple \begin{align} {\rm Vol}( K_1 \cap K) \le \min ({\rm Vol}( K_1 ),{\rm Vol}( K)), \end{align} and tight if $U$ is an identiy matrix. Lower Bound Based on inclusion Let $B$ be a ball of radius $r$ such that \begin{align} B \subset K_1\cap K, \end{align} then  \begin{align} \frac{\pi^{n/2}}{\Gamma(n/2+1)} r^n =Vol(B) \le  {\rm Vol}( K_1 \cap K)= 2^nr^n . \end{align} But,   since  $\frac{\pi^{n/2}}{\Gamma(n/2+1)} \to 0$ as  $n \to \infty$  and $2^n \to \infty$ as $n \to \infty$, the lower and the upper bounds do not match. Question: Can we come up with a lower bound and upper bounds of the same order as $n \to \infty$.",,"['linear-algebra', 'geometry', 'linear-transformations', 'packing-problem']"
4,Derivation of normal equations for maximum likelihood and least squares,Derivation of normal equations for maximum likelihood and least squares,,"The question stems from the text Pattern Recognition and Machine Learning by Christopher Bishop, Chapter 3.1.1. To say my maths is rusty is an understatement so I would appreciate any nudge in the right direction regarding the below. The logarithm of the likelihood functions of a standard univariate Gaussian is given below: $$(1)\quad \frac{N}{2} \ln \beta - \frac{N}{2}\ln 2 \pi - \frac{\beta}{2}\sum_{n=1}^N\{t_n - \mathbf w^T \mathbf \phi(\mathbf x_n) \}^2$$ Taking the gradient(derivative) with respect to $\mathbf w$ and setting this equal to zero we get: $$(2)\quad0 = \sum_{n=1}^N\{t_n - \mathbf w^T \mathbf \phi(\mathbf x_n) \}\phi(\mathbf x_n)^T $$ $$(3)\quad0 = \sum_{n=1}^Nt_n \phi(\mathbf x_n)^T - \mathbf w^T(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T)$$ Solving for $\mathbf w$ we get: $$(4)\quad\mathbf w_{ML}=(\mathbf \Phi^T \mathbf \Phi)^{-1} \mathbf \Phi^T \mathbf {\mathtt t}$$ where $$(5)\quad\mathbf \Phi = \phi_j(\mathbf x_n)$$ Now for the questions. (a) In (2) when using the chain rule why do we get $\phi(\mathbf x_n)^T$ instead of $\phi(\mathbf x_n)?$ (b) Is the following valid to get from (3) to (4)? Continuing from (3) $$\mathbf w^T(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T) = \sum_{n=1}^Nt_n \phi(\mathbf x_n)^T$$ $$[\mathbf w^T(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T)]^T = [\sum_{n=1}^Nt_n \phi(\mathbf x_n)^T]^T$$ $$\mathbf w_{ML}(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T) = \sum_{n=1}^N \phi(\mathbf x_n) t_n $$ Where $\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T$ = $(\mathbf \Phi^T \mathbf \Phi)$ and $\sum_{n=1}^N \phi(\mathbf x_n) t_n = \mathbf \Phi^T \mathbf {\mathtt t}$ $$\mathbf w_{ML}(\mathbf \Phi^T \mathbf \Phi)(\mathbf \Phi^T \mathbf \Phi)^{-1}=(\mathbf \Phi^T \mathbf \Phi)^{-1} \mathbf \Phi^T \mathbf {\mathtt t}$$ $$(4)\quad\mathbf w_{ML}=(\mathbf \Phi^T \mathbf \Phi)^{-1} \mathbf \Phi^T \mathbf {\mathtt t}$$ If it is not valid can you please point out where.","The question stems from the text Pattern Recognition and Machine Learning by Christopher Bishop, Chapter 3.1.1. To say my maths is rusty is an understatement so I would appreciate any nudge in the right direction regarding the below. The logarithm of the likelihood functions of a standard univariate Gaussian is given below: $$(1)\quad \frac{N}{2} \ln \beta - \frac{N}{2}\ln 2 \pi - \frac{\beta}{2}\sum_{n=1}^N\{t_n - \mathbf w^T \mathbf \phi(\mathbf x_n) \}^2$$ Taking the gradient(derivative) with respect to $\mathbf w$ and setting this equal to zero we get: $$(2)\quad0 = \sum_{n=1}^N\{t_n - \mathbf w^T \mathbf \phi(\mathbf x_n) \}\phi(\mathbf x_n)^T $$ $$(3)\quad0 = \sum_{n=1}^Nt_n \phi(\mathbf x_n)^T - \mathbf w^T(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T)$$ Solving for $\mathbf w$ we get: $$(4)\quad\mathbf w_{ML}=(\mathbf \Phi^T \mathbf \Phi)^{-1} \mathbf \Phi^T \mathbf {\mathtt t}$$ where $$(5)\quad\mathbf \Phi = \phi_j(\mathbf x_n)$$ Now for the questions. (a) In (2) when using the chain rule why do we get $\phi(\mathbf x_n)^T$ instead of $\phi(\mathbf x_n)?$ (b) Is the following valid to get from (3) to (4)? Continuing from (3) $$\mathbf w^T(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T) = \sum_{n=1}^Nt_n \phi(\mathbf x_n)^T$$ $$[\mathbf w^T(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T)]^T = [\sum_{n=1}^Nt_n \phi(\mathbf x_n)^T]^T$$ $$\mathbf w_{ML}(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T) = \sum_{n=1}^N \phi(\mathbf x_n) t_n $$ Where $\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T$ = $(\mathbf \Phi^T \mathbf \Phi)$ and $\sum_{n=1}^N \phi(\mathbf x_n) t_n = \mathbf \Phi^T \mathbf {\mathtt t}$ $$\mathbf w_{ML}(\mathbf \Phi^T \mathbf \Phi)(\mathbf \Phi^T \mathbf \Phi)^{-1}=(\mathbf \Phi^T \mathbf \Phi)^{-1} \mathbf \Phi^T \mathbf {\mathtt t}$$ $$(4)\quad\mathbf w_{ML}=(\mathbf \Phi^T \mathbf \Phi)^{-1} \mathbf \Phi^T \mathbf {\mathtt t}$$ If it is not valid can you please point out where.",,"['linear-algebra', 'machine-learning', 'least-squares', 'maximum-likelihood']"
5,Connection between dual maps and bilinear forms?,Connection between dual maps and bilinear forms?,,"Say $\xi$ is a linear map from $V$ to $W$ . Then its dual map is the map $\xi^\star:W^\star \to V^\star$ such that $g\mapsto \xi^\star(g) = g\circ \xi \doteq f$ . If $A_\mathcal V^\mathcal W$ is the matrix associated to $\xi$ in the bases $\mathcal V$ of $V$ and $\mathcal W$ of $W$ , then its transpose $(A_\mathcal V ^ \mathcal W)^T$ is the matrix associated to $\xi^\star$ in the dual bases $\mathcal W^\star$ and $\mathcal V^\star$ . This also means that if $V$ is one-dimensional, the matrix associated to $\xi^\star$ is a row vector (covector), the transpose of a column vector in $\mathbb K^m$ . Now suppose we have a bilinear form $\phi : V\times W \to \mathbb K$ . Then I can represent it through the matrix $$\Phi =  \begin{bmatrix}\phi(v_1,v_1) & \cdots & \phi(v_1,w_m) \\ \vdots & \ddots & \vdots \\ \phi(v_n,w_1) & \cdots& \phi (v_n,w_m) \end{bmatrix}$$ where the $v_i$ 's are the elements of the basis $\mathcal V$ and the $w_j$ 's are the elements of the basis $\mathcal W$ . If $x=(x_1,\dots,x_n)$ and $y=(y_1,\dots,y_n)$ are the coordinates of $v$ and $w$ respectively in the bases $\mathcal V$ and $\mathcal W$ , then I can write $\phi(v,w) = x^T\Phi y$ . This last step is justified in my notes by means of the proposition: Proposition. The bilinear form $\phi$ is completely determined by the $nm$ scalars $a_{ij} \doteq \phi(v_i,w_j)$ . Proof. We can write $v = \sum_{i=1}^n x_iv_i$ and $w = \sum_{j=1}^m y_jw_j$ , so, by iterating bilinearity, we have $$\phi(v,w) = \phi\left(\sum_{i=1}^n x_iv_i,\sum_{j=1}^my_jw_j\right) = \sum_{i,j} a_{ij} x_iy_j $$ The last term corresponds to the expansion of $x^T\Phi y$ , but this seems to fall relatively out of the blue. Indeed, seeing that a row vector is involved would have one think that some dual map is involved – and when learning about matrix congruence, this involvement seems even more evident. Yet nowhere in my notes is the connection directly mentioned and clarified (the two concepts belong to separate chapters). So, is there a way to justify the transpose formula in terms of dual spaces? In other words, does this way of representing bilinear forms (once the bases are chosen) conceal a deeper connection to the concept of dual spaces and dual maps?","Say is a linear map from to . Then its dual map is the map such that . If is the matrix associated to in the bases of and of , then its transpose is the matrix associated to in the dual bases and . This also means that if is one-dimensional, the matrix associated to is a row vector (covector), the transpose of a column vector in . Now suppose we have a bilinear form . Then I can represent it through the matrix where the 's are the elements of the basis and the 's are the elements of the basis . If and are the coordinates of and respectively in the bases and , then I can write . This last step is justified in my notes by means of the proposition: Proposition. The bilinear form is completely determined by the scalars . Proof. We can write and , so, by iterating bilinearity, we have The last term corresponds to the expansion of , but this seems to fall relatively out of the blue. Indeed, seeing that a row vector is involved would have one think that some dual map is involved – and when learning about matrix congruence, this involvement seems even more evident. Yet nowhere in my notes is the connection directly mentioned and clarified (the two concepts belong to separate chapters). So, is there a way to justify the transpose formula in terms of dual spaces? In other words, does this way of representing bilinear forms (once the bases are chosen) conceal a deeper connection to the concept of dual spaces and dual maps?","\xi V W \xi^\star:W^\star \to V^\star g\mapsto \xi^\star(g) = g\circ \xi \doteq f A_\mathcal V^\mathcal W \xi \mathcal V V \mathcal W W (A_\mathcal V ^ \mathcal W)^T \xi^\star \mathcal W^\star \mathcal V^\star V \xi^\star \mathbb K^m \phi : V\times W \to \mathbb K \Phi =  \begin{bmatrix}\phi(v_1,v_1) & \cdots & \phi(v_1,w_m) \\ \vdots & \ddots & \vdots \\ \phi(v_n,w_1) & \cdots& \phi (v_n,w_m) \end{bmatrix} v_i \mathcal V w_j \mathcal W x=(x_1,\dots,x_n) y=(y_1,\dots,y_n) v w \mathcal V \mathcal W \phi(v,w) = x^T\Phi y \phi nm a_{ij} \doteq \phi(v_i,w_j) v = \sum_{i=1}^n x_iv_i w = \sum_{j=1}^m y_jw_j \phi(v,w) = \phi\left(\sum_{i=1}^n x_iv_i,\sum_{j=1}^my_jw_j\right) = \sum_{i,j} a_{ij} x_iy_j  x^T\Phi y","['linear-algebra', 'duality-theorems', 'bilinear-form']"
6,How to find sides of parallelogram given centroid and two vertices?,How to find sides of parallelogram given centroid and two vertices?,,"I've been given a parallelogram with two of its vertices, $A$ and $B$, being equal to (-5, -8, 3) and (4, 7, -5) respectively, and a centroid $S$ at (-10, 4, 6). How do I go around finding remaining coordinates of points $D$ and $C$?","I've been given a parallelogram with two of its vertices, $A$ and $B$, being equal to (-5, -8, 3) and (4, 7, -5) respectively, and a centroid $S$ at (-10, 4, 6). How do I go around finding remaining coordinates of points $D$ and $C$?",,['linear-algebra']
7,Linear algebra proof with linear operator,Linear algebra proof with linear operator,,"Let $V$ be a finite dimensional vector space, $T \in L(V)$ a linear map whose matrix $M(T)$ is the same for every basis of $V$. Show that $T$ is a scalar multiple of the identity map $I$. I know it has to do with something about the vectors being linearly independent in a basis but I don't know where to go with that when trying to find a contradiction","Let $V$ be a finite dimensional vector space, $T \in L(V)$ a linear map whose matrix $M(T)$ is the same for every basis of $V$. Show that $T$ is a scalar multiple of the identity map $I$. I know it has to do with something about the vectors being linearly independent in a basis but I don't know where to go with that when trying to find a contradiction",,['linear-algebra']
8,Matrices with Three Nonzero Diagonals,Matrices with Three Nonzero Diagonals,,"How would you compute eigenvectors of matrices of the form \begin{equation} \nonumber M = \left( \begin{array}{cccccccccc}  a_1 & 0  & b_1&&&&&&&\\  0 &  a_2 & 0& b_2&&&&&&& \\ c_1 &0 &  a_3 & 0&b_3&&&&&& \\ &c_2&0&  a_4& 0&&&&&& \\ &&c_3&0& &&&&&& \\ &&&&&\ddots\\ \\ &&&&&& a_{n-3} & 0 & b_{n-3}& \\ &&&&& &0 & a_{n-2} & 0 & b_{n-2}\\ &&&&&&c_{n-3}& 0& a_{n-1}&  0\\ &&&&&&&c_{n-2}& 0 &a_n \\ \end{array} \right)  \end{equation} where all the omitted entries are null? The particular matrix I'm dealing with has the additional property of being stochastic, so the principal eigenvalue is one.","How would you compute eigenvectors of matrices of the form \begin{equation} \nonumber M = \left( \begin{array}{cccccccccc}  a_1 & 0  & b_1&&&&&&&\\  0 &  a_2 & 0& b_2&&&&&&& \\ c_1 &0 &  a_3 & 0&b_3&&&&&& \\ &c_2&0&  a_4& 0&&&&&& \\ &&c_3&0& &&&&&& \\ &&&&&\ddots\\ \\ &&&&&& a_{n-3} & 0 & b_{n-3}& \\ &&&&& &0 & a_{n-2} & 0 & b_{n-2}\\ &&&&&&c_{n-3}& 0& a_{n-1}&  0\\ &&&&&&&c_{n-2}& 0 &a_n \\ \end{array} \right)  \end{equation} where all the omitted entries are null? The particular matrix I'm dealing with has the additional property of being stochastic, so the principal eigenvalue is one.",,"['linear-algebra', 'matrices', 'stochastic-processes', 'eigenvalues-eigenvectors', 'matrix-equations']"
9,Anyone knows a transformation that can lump non-zero values in a matrix together?,Anyone knows a transformation that can lump non-zero values in a matrix together?,,"Suppose you have a matrix that contains a lot of zeros (like spare matrix). Is there any known transformation that can gather non-zero values (e.g., into a corner of the matrix)? I am not sure but I think such a transformation is very likely to be non-linear. There is a naïve transformation can be used as an example of the intention (however it does not well achieve the above objective). You can count the number of zeros of each row and rearrange rows by those counts in an ascending order. This can be done by multiplying a corresponding permutation matrix on the left. Then you can count the number of zeros of each column and do the same thing -- rearrange the columns by those counts in an ascending order. After rearrangement of both rows and columns, a sparse matrix would approximate a diagonal block matrix, as shown below, with blue color meaning 0. However, I am asking if there is any transformation that can lump most non-zero values into approximately one block , rather than a diagonal block. It would be greatly appreciated if someone could help provide some hint or reference.","Suppose you have a matrix that contains a lot of zeros (like spare matrix). Is there any known transformation that can gather non-zero values (e.g., into a corner of the matrix)? I am not sure but I think such a transformation is very likely to be non-linear. There is a naïve transformation can be used as an example of the intention (however it does not well achieve the above objective). You can count the number of zeros of each row and rearrange rows by those counts in an ascending order. This can be done by multiplying a corresponding permutation matrix on the left. Then you can count the number of zeros of each column and do the same thing -- rearrange the columns by those counts in an ascending order. After rearrangement of both rows and columns, a sparse matrix would approximate a diagonal block matrix, as shown below, with blue color meaning 0. However, I am asking if there is any transformation that can lump most non-zero values into approximately one block , rather than a diagonal block. It would be greatly appreciated if someone could help provide some hint or reference.",,"['linear-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra', 'sparse-matrices']"
10,Question about kernel of the restriction of a linear operator,Question about kernel of the restriction of a linear operator,,"I'm studying linear algebra and I am trying to answer a question I asked myself. Suppose $T:V\rightarrow V$ is a linear operator on a finite dimensional vector space $V$ over an algebraically closed field $K$. Let $\lambda_1, \dots, \lambda_n$ be the eigenvalues of $T$. Let $$m_T(x)=(x-\lambda_1)^{r_1}\dots(x-\lambda_n)^{r_n}$$ be the minimal polynomial of $T$. By the Primary Decomposition Theorem: $$V=\ker(T-\lambda_1 I_V)^{r_1}\oplus\dots\oplus\ker(T-\lambda_nI_V)^{r_n}$$ Let $V_i=\ker(T-\lambda_i I_V)^{r_i}$. Then $T$ is $V_i$ invariant. Let $T|V_i=T_i$.Then $T_i$ is nilpotent and it's minimal polynomial is $(x-\lambda_i)^{r_i}$. It's easy to show that for every $n\leq r_i$, $\ker (T_i-\lambda_i I_{V_i})^n=\ker (T-\lambda_iI_V)^n$. My question is: Is it also true for $n>r_i$?","I'm studying linear algebra and I am trying to answer a question I asked myself. Suppose $T:V\rightarrow V$ is a linear operator on a finite dimensional vector space $V$ over an algebraically closed field $K$. Let $\lambda_1, \dots, \lambda_n$ be the eigenvalues of $T$. Let $$m_T(x)=(x-\lambda_1)^{r_1}\dots(x-\lambda_n)^{r_n}$$ be the minimal polynomial of $T$. By the Primary Decomposition Theorem: $$V=\ker(T-\lambda_1 I_V)^{r_1}\oplus\dots\oplus\ker(T-\lambda_nI_V)^{r_n}$$ Let $V_i=\ker(T-\lambda_i I_V)^{r_i}$. Then $T$ is $V_i$ invariant. Let $T|V_i=T_i$.Then $T_i$ is nilpotent and it's minimal polynomial is $(x-\lambda_i)^{r_i}$. It's easy to show that for every $n\leq r_i$, $\ker (T_i-\lambda_i I_{V_i})^n=\ker (T-\lambda_iI_V)^n$. My question is: Is it also true for $n>r_i$?",,[]
11,Explicit example of non constant linear functional $f: \mathbb R \to \mathbb Q$?,Explicit example of non constant linear functional ?,f: \mathbb R \to \mathbb Q,Recall that $V=\mathbb R$ is a uncountably dimension vector space over $\mathbb Q$ as countable dimension vector space over $\mathbb Q$ is itself countable. Is there any explicit example of a non constant linear functional $f: \mathbb R \to \mathbb Q$ ? Existence of such linear functional is almost trivial but can we give the explicit example of such $1$-form? Also it is clear that under usual topology such a map $f$ cannot be continuous as $\mathbb Q$ is totally disconnected.,Recall that $V=\mathbb R$ is a uncountably dimension vector space over $\mathbb Q$ as countable dimension vector space over $\mathbb Q$ is itself countable. Is there any explicit example of a non constant linear functional $f: \mathbb R \to \mathbb Q$ ? Existence of such linear functional is almost trivial but can we give the explicit example of such $1$-form? Also it is clear that under usual topology such a map $f$ cannot be continuous as $\mathbb Q$ is totally disconnected.,,"['linear-algebra', 'vector-spaces', 'linear-transformations']"
12,Is there a relation between trace of congruent matrices?,Is there a relation between trace of congruent matrices?,,"If $A$ and $\bar{A}$ are two congruent positive definite matrices such that $A=V\bar{A}V^T$ (det$(V) \neq 0$), then is there an expression for trace$(A)$ in terms of trace$(\bar{A})$? I know trace$(\bar{A})$ and I want to find trace$(A)$. Thanks.","If $A$ and $\bar{A}$ are two congruent positive definite matrices such that $A=V\bar{A}V^T$ (det$(V) \neq 0$), then is there an expression for trace$(A)$ in terms of trace$(\bar{A})$? I know trace$(\bar{A})$ and I want to find trace$(A)$. Thanks.",,"['linear-algebra', 'matrices', 'quadratic-forms', 'trace', 'positive-definite']"
13,Show $A^T$ has an eigenvector with all components rational [closed],Show  has an eigenvector with all components rational [closed],A^T,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Matrix $A$ is a $5 \times 5$ matrix with rational entries such that $(1, \sqrt{2}, \sqrt{3}, \sqrt{4}, \sqrt{5})^T$ is an eigenvector of A. Show that $A^T$ has eigenvector with all components rational. My idea is: let the eigenvalue associated with the above eigenvector be $λ$. Since all matrix entries are rational numbers so an irrational number will be linearly independent. Use this $2(a_{11} + 2a_{14}) = a_{41} + 2a_{44}$ $a_{21}+2a_{24} = 0$ $a_{31}+2a_{34} = 0$ $a_{51}+2a_{54} = 0$ but I can't find transposed matrix's eigenvector.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Matrix $A$ is a $5 \times 5$ matrix with rational entries such that $(1, \sqrt{2}, \sqrt{3}, \sqrt{4}, \sqrt{5})^T$ is an eigenvector of A. Show that $A^T$ has eigenvector with all components rational. My idea is: let the eigenvalue associated with the above eigenvector be $λ$. Since all matrix entries are rational numbers so an irrational number will be linearly independent. Use this $2(a_{11} + 2a_{14}) = a_{41} + 2a_{44}$ $a_{21}+2a_{24} = 0$ $a_{31}+2a_{34} = 0$ $a_{51}+2a_{54} = 0$ but I can't find transposed matrix's eigenvector.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
14,Why two possibles Jordan Canonical forms of a matrix cannot be similar?,Why two possibles Jordan Canonical forms of a matrix cannot be similar?,,"Consider an matrix $A_{5x5}$ with only one eigenvalue $\lambda$. If the dimension of the eigenspace $\lambda$ is two, then we can have two possibilities of the Jordan blocs here: I didnt understand the statement marked by yellow so I have two questions about it: 1) If $(J^{(2)}-\lambda I)^3$ and $(J^{(3)}-\lambda I)^3$ were equal zero, then $J^{(2)}$ and $J^{(3)}$ would be similar? Why ? 2) Like in the reference, why the fact that $(J^{(2)}-\lambda I)^3=0$ and $(J^{(3)}-\lambda I)^3 \ne 0$ guarantees that $J^{(2)}$ and $J^{(3)}$ are not similar? I dont need a proof, just an understandment. One can find the cited reference here: http://math.postech.ac.kr/~sungpyo/LinearAlge-2007/Chap8.pdf , and the statement marked by yellow in the bottom of the second page of the reference. obs: I dont know if the tag ""jordan normal form"" applies here, since I am studying jordan canonical form (dont really know if there's any difference).","Consider an matrix $A_{5x5}$ with only one eigenvalue $\lambda$. If the dimension of the eigenspace $\lambda$ is two, then we can have two possibilities of the Jordan blocs here: I didnt understand the statement marked by yellow so I have two questions about it: 1) If $(J^{(2)}-\lambda I)^3$ and $(J^{(3)}-\lambda I)^3$ were equal zero, then $J^{(2)}$ and $J^{(3)}$ would be similar? Why ? 2) Like in the reference, why the fact that $(J^{(2)}-\lambda I)^3=0$ and $(J^{(3)}-\lambda I)^3 \ne 0$ guarantees that $J^{(2)}$ and $J^{(3)}$ are not similar? I dont need a proof, just an understandment. One can find the cited reference here: http://math.postech.ac.kr/~sungpyo/LinearAlge-2007/Chap8.pdf , and the statement marked by yellow in the bottom of the second page of the reference. obs: I dont know if the tag ""jordan normal form"" applies here, since I am studying jordan canonical form (dont really know if there's any difference).",,"['linear-algebra', 'jordan-normal-form']"
15,Polynomial roots related question,Polynomial roots related question,,If a polynomial $P(x)$ with integer coefficients has degree $d\ge 0$ Then prove that $P^2(x)-1$ can have atmost $(d+2)$ integer roots. I am not able to solve this. Basically I was trying to show that if $P(x)=1$ has $d$ distinct integer roots then $P(x)=-1$ can have maximum two distinct integer roots,If a polynomial $P(x)$ with integer coefficients has degree $d\ge 0$ Then prove that $P^2(x)-1$ can have atmost $(d+2)$ integer roots. I am not able to solve this. Basically I was trying to show that if $P(x)=1$ has $d$ distinct integer roots then $P(x)=-1$ can have maximum two distinct integer roots,,"['linear-algebra', 'polynomials']"
16,On sum of determinants,On sum of determinants,,$(1)$ Are there any special non-trivial classes of $n\times n$ square matrices where $$\det(A)=\sum_{i=1}^m\det(A_i)$$ at some (not necessarily any ) $m$ satisfying $2\leq m\leq n$ where $A=\sum_{i=1}^mA_i$ holds? $(2)$ Supposing if $A_i$ are symmetric and positive definite is it true that $$\det(A)\geq\sum_{i=1}^m\det(A_i)$$ holds at any $n\geq1$ if $A=\sum_{i=1}^mA_i$ holds (if true or not are there any other classes of matrices for which this holds)? $(3)$ Are there any classes of non-trivial matrices for which we can have $$\det(A)>\sum_{i=1}^m\det(A_i)$$ holding true if $A=\sum_{i=1}^mA_i$ holds?,$(1)$ Are there any special non-trivial classes of $n\times n$ square matrices where $$\det(A)=\sum_{i=1}^m\det(A_i)$$ at some (not necessarily any ) $m$ satisfying $2\leq m\leq n$ where $A=\sum_{i=1}^mA_i$ holds? $(2)$ Supposing if $A_i$ are symmetric and positive definite is it true that $$\det(A)\geq\sum_{i=1}^m\det(A_i)$$ holds at any $n\geq1$ if $A=\sum_{i=1}^mA_i$ holds (if true or not are there any other classes of matrices for which this holds)? $(3)$ Are there any classes of non-trivial matrices for which we can have $$\det(A)>\sum_{i=1}^m\det(A_i)$$ holding true if $A=\sum_{i=1}^mA_i$ holds?,,"['linear-algebra', 'matrices', 'determinant', 'positive-definite', 'symmetric-matrices']"
17,a property of infinite matrices,a property of infinite matrices,,"An infinite matrix $[a_{ij}]_{i,j\in\mathbb{N}}$ is called invertible , if for any convergent sequence $(y_m)$ there exists exactly one sequence $(x_m)$ such that $y_m=\sum_{n\ge 1}a_{mn}x_n$ for all $m\in\mathbb{N}$. Find two infinite invertible matrices $A,B$ such that $AB$ is not invertible.","An infinite matrix $[a_{ij}]_{i,j\in\mathbb{N}}$ is called invertible , if for any convergent sequence $(y_m)$ there exists exactly one sequence $(x_m)$ such that $y_m=\sum_{n\ge 1}a_{mn}x_n$ for all $m\in\mathbb{N}$. Find two infinite invertible matrices $A,B$ such that $AB$ is not invertible.",,"['linear-algebra', 'matrices', 'matrix-equations']"
18,Compute the main diagonal of $(K + D)^{-1}$ in less than $O(n^3)$ operations,Compute the main diagonal of  in less than  operations,(K + D)^{-1} O(n^3),"Compute the main diagonal of $(K + D)^{-1}$ in less than $O(n^3)$ operations given full-rank, dense and symmetric matrices $K$ and $K^{-1}$, and a diagonal matrix $D$ with positive elements on its main diagonal. I have tried this for a month and I can't manage to perform this operation in less than $O(n^3)$. I would also be happy knowing a proof or a clear indication that it is actually impossible. Note: I assume we are using $O(n^3)$ for matrix product operations.","Compute the main diagonal of $(K + D)^{-1}$ in less than $O(n^3)$ operations given full-rank, dense and symmetric matrices $K$ and $K^{-1}$, and a diagonal matrix $D$ with positive elements on its main diagonal. I have tried this for a month and I can't manage to perform this operation in less than $O(n^3)$. I would also be happy knowing a proof or a clear indication that it is actually impossible. Note: I assume we are using $O(n^3)$ for matrix product operations.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
19,Finite subgroup of $\text{SO}(3)$ acts on set of points on unit sphere in $\mathbb{R}^3$ which are fixed via some nontrivial rotation in $G$,Finite subgroup of  acts on set of points on unit sphere in  which are fixed via some nontrivial rotation in,\text{SO}(3) \mathbb{R}^3 G,Let $G$ be a finite nontrivial subgroup of $\text{SO}(3)$. Let $X$ be the set of points on the unit sphere in $\mathbb{R}^3$ which are fixed by some nontrivial rotation in $G$. I have two questions. How do I see that $G$ acts on $X$ and that the number of orbits is either $2$ or $3$? What is $G$ is there are only $2$ orbits?,Let $G$ be a finite nontrivial subgroup of $\text{SO}(3)$. Let $X$ be the set of points on the unit sphere in $\mathbb{R}^3$ which are fixed by some nontrivial rotation in $G$. I have two questions. How do I see that $G$ acts on $X$ and that the number of orbits is either $2$ or $3$? What is $G$ is there are only $2$ orbits?,,"['linear-algebra', 'abstract-algebra']"
20,Proof of the Addition and Scalar multiplication for linear maps,Proof of the Addition and Scalar multiplication for linear maps,,"Let $f, g : U\rightarrow V$ be linear maps and $\lambda\in  F$ . Then the maps $f + g : U\rightarrow V$ and $\lambda f : U \rightarrow V$ are linear. My attempt at the proof for the first statement is as follows: Let $u,z\in U$ and $a\in F$ , using a linearity check by deﬁnition of $f + g$ $$(f + g)(au + z) = f(au + z) + g(au + z)$$ by linearity of $f$ and $g$ $$= (af(u) + f(z)) + (ag(u) + g(z)) $$ by basic properties of vector spaces $$= af(u) + ag(u) + f(z) + g(z)$$ by an axiom of vector spaces $$= a(f(u) + g(u)) + (f(z) + g(z))$$ by deﬁnition of $f + g$ . $$= a(f + g)(u) + (f + g)(z)$$ Hence, $f + g$ is linear Is this the correct approach. What is the proof of $\lambda f : U \rightarrow V$ to be linear?","Let be linear maps and . Then the maps and are linear. My attempt at the proof for the first statement is as follows: Let and , using a linearity check by deﬁnition of by linearity of and by basic properties of vector spaces by an axiom of vector spaces by deﬁnition of . Hence, is linear Is this the correct approach. What is the proof of to be linear?","f, g : U\rightarrow V \lambda\in  F f + g : U\rightarrow V \lambda f : U \rightarrow V u,z\in U a\in F f + g (f + g)(au + z) = f(au + z) + g(au + z) f g = (af(u) + f(z)) + (ag(u) + g(z))  = af(u) + ag(u) + f(z) + g(z) = a(f(u) + g(u)) + (f(z) + g(z)) f + g = a(f + g)(u) + (f + g)(z) f + g \lambda f : U \rightarrow V",['linear-algebra']
21,Amount of zero in a range,Amount of zero in a range,,"I'm stuck with a calculation.  If a person would write down, Everytime he sees a zero and counts +1. 10 counts as 1 zero. 100 counts as 2 zeros.  1000 has 3 zeros. 1005 counts as 2 zeros. Is there a formula how one is able to calculate how many zeros pass by? 0 - 100 has 12 zeros 100 - 202 has 25 zeros. 0 - 501 has 93 zeros. At first I was thinking of 100/10 = 10.but the 100 gives 1 extra zero and the zero itself too.  With bigger numbers like 100 - 1010 is 10. And 1011 - 120 is only one. I hope my post is understandable since I'm not a mathematician myself. edit: numbers can go from any number untill 2^63 as the maximum example: 0 - 100 = 12 (include the 0) 100 - 201 = 23 0 - 500 = 92 0 - 4294967296 = 3825876150 1234567890 - 2345678901 = 987654304","I'm stuck with a calculation.  If a person would write down, Everytime he sees a zero and counts +1. 10 counts as 1 zero. 100 counts as 2 zeros.  1000 has 3 zeros. 1005 counts as 2 zeros. Is there a formula how one is able to calculate how many zeros pass by? 0 - 100 has 12 zeros 100 - 202 has 25 zeros. 0 - 501 has 93 zeros. At first I was thinking of 100/10 = 10.but the 100 gives 1 extra zero and the zero itself too.  With bigger numbers like 100 - 1010 is 10. And 1011 - 120 is only one. I hope my post is understandable since I'm not a mathematician myself. edit: numbers can go from any number untill 2^63 as the maximum example: 0 - 100 = 12 (include the 0) 100 - 201 = 23 0 - 500 = 92 0 - 4294967296 = 3825876150 1234567890 - 2345678901 = 987654304",,"['linear-algebra', 'algorithms', 'math-software']"
22,Transformation of matrices in 3 dimensions,Transformation of matrices in 3 dimensions,,"The transformation T is represented by the matrix $$         \begin{pmatrix}         2 & -1 & 3 \\         -1 & 4 & -2 \\         3 & 2 & 4 \\         \end{pmatrix} $$ The plane has equation $2x-y+3z=-6$ Show that the image of the plane under T is a line and find Cartesian equations of this line. Let $x=s$ and $z=t$, then $y=6+2s+3t$ I know that the general point on the plane is: $$         \begin{pmatrix}           s   \\           6+2s+3t   \\           t   \\         \end{pmatrix} $$ Under transformation T , it becomes: $$         \begin{pmatrix}         -6 \\         24+7s+10t \\         12+7s+10t \\         \end{pmatrix} $$ $x=-6$, $y=z+12$ But I am not sure how to show that it is a line.","The transformation T is represented by the matrix $$         \begin{pmatrix}         2 & -1 & 3 \\         -1 & 4 & -2 \\         3 & 2 & 4 \\         \end{pmatrix} $$ The plane has equation $2x-y+3z=-6$ Show that the image of the plane under T is a line and find Cartesian equations of this line. Let $x=s$ and $z=t$, then $y=6+2s+3t$ I know that the general point on the plane is: $$         \begin{pmatrix}           s   \\           6+2s+3t   \\           t   \\         \end{pmatrix} $$ Under transformation T , it becomes: $$         \begin{pmatrix}         -6 \\         24+7s+10t \\         12+7s+10t \\         \end{pmatrix} $$ $x=-6$, $y=z+12$ But I am not sure how to show that it is a line.",,"['linear-algebra', 'matrices', 'linear-transformations']"
23,What's the condition for a matrix $A$ ($2N\times 2N$ dimension) to have eigenvalues in pairs $\pm\lambda$?,What's the condition for a matrix  ( dimension) to have eigenvalues in pairs ?,A 2N\times 2N \pm\lambda,"For a given even dimension square complex matrix $A$ ($2N\times 2N$ dimension), what's the sufficient and necessary condition for the matrix $A$ such that: if $\lambda_{1}$ is an eigenvalue, then $\lambda_{2}=-\lambda_{1}$ is also an eigenvalue? I have found one sufficient condition: skew-symmetric matrix, but obviously there is a lot of matrices having eigenvalues in pairs are not skew-symmetric. So I wonder is there a sufficient and necessary condition? Or is there a name for this family of matrices (skew-symmetric matrix is a member)?","For a given even dimension square complex matrix $A$ ($2N\times 2N$ dimension), what's the sufficient and necessary condition for the matrix $A$ such that: if $\lambda_{1}$ is an eigenvalue, then $\lambda_{2}=-\lambda_{1}$ is also an eigenvalue? I have found one sufficient condition: skew-symmetric matrix, but obviously there is a lot of matrices having eigenvalues in pairs are not skew-symmetric. So I wonder is there a sufficient and necessary condition? Or is there a name for this family of matrices (skew-symmetric matrix is a member)?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
24,Associating a $m-1$-tensor on $\mathbb R^m$ to an element of $\mathbb R^m$.,Associating a -tensor on  to an element of .,m-1 \mathbb R^m \mathbb R^m,"Show that for every alternating $m-1$-tensor on $\mathbb R^m$ there   exists a unique $v \in \mathbb R^m$ such that for every linear   function from $\mathbb R^m$ into $\mathbb R$ and for every $v_1,  \dots, v_m \in \mathbb R^m$, $(\omega \wedge f)(v_1, \dots,  v_m)=f(v)\det(v_1, \dots, v_m)$. I have reduced the problem to the following: Show that for every alternating $m-1$-tensor on $\mathbb R^m$ there   exists a unique $v \in \mathbb R^m$ such that for every linear   function from $\mathbb R^m$ into $\mathbb R$ , $(\omega \wedge f)(e_1, \dots, e_m)=f(v)$. I have accomplished this, but my proof was very ugly (I calculated the left side using the definition of the wedge product, using the Alt function). I'd like to know if there is a better way to accomplish it. Edit: It's very simple to prove this result if there is an easy way to prove that $(\phi_1\wedge\dots\wedge \phi_{k-1}\wedge \phi_{k+1}\wedge \dots \wedge\phi_m\wedge f)(e_1, \dots, e_m)=f(e_k)$, where the $\phi_i(e_j)=\delta_{ij}$. Is there an elegant way to show this?","Show that for every alternating $m-1$-tensor on $\mathbb R^m$ there   exists a unique $v \in \mathbb R^m$ such that for every linear   function from $\mathbb R^m$ into $\mathbb R$ and for every $v_1,  \dots, v_m \in \mathbb R^m$, $(\omega \wedge f)(v_1, \dots,  v_m)=f(v)\det(v_1, \dots, v_m)$. I have reduced the problem to the following: Show that for every alternating $m-1$-tensor on $\mathbb R^m$ there   exists a unique $v \in \mathbb R^m$ such that for every linear   function from $\mathbb R^m$ into $\mathbb R$ , $(\omega \wedge f)(e_1, \dots, e_m)=f(v)$. I have accomplished this, but my proof was very ugly (I calculated the left side using the definition of the wedge product, using the Alt function). I'd like to know if there is a better way to accomplish it. Edit: It's very simple to prove this result if there is an easy way to prove that $(\phi_1\wedge\dots\wedge \phi_{k-1}\wedge \phi_{k+1}\wedge \dots \wedge\phi_m\wedge f)(e_1, \dots, e_m)=f(e_k)$, where the $\phi_i(e_j)=\delta_{ij}$. Is there an elegant way to show this?",,['linear-algebra']
25,Equivalence of matrix norms,Equivalence of matrix norms,,"The equivalence of vector norms on finite dimensional spaces  immediately implies that all induced matrix norms are equivalent. However, for matrix norms (like Frobenius norm, Nuclear norm) that are not induced by a vector norm, one proves equivalence on a case by case basis. Hence my question is, can I say all matrix norms, induced or not, are equivalent ? If so how do I justify it in general ?","The equivalence of vector norms on finite dimensional spaces  immediately implies that all induced matrix norms are equivalent. However, for matrix norms (like Frobenius norm, Nuclear norm) that are not induced by a vector norm, one proves equivalence on a case by case basis. Hence my question is, can I say all matrix norms, induced or not, are equivalent ? If so how do I justify it in general ?",,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms']"
26,Linear independence over $\mathbb Q$ and $\mathbb R$ of subsets of $2^{\mathbb N}$,Linear independence over  and  of subsets of,\mathbb Q \mathbb R 2^{\mathbb N},"I have the following doubt: Suppose $f_1,\ldots,f_n\in 2^{\mathbb N}$ are such that $\{f_1,\ldots,f_n\}$ is linearly independent in the $\mathbb Q$-vector space $\mathbb{Q^N}$. Does this set remain linearly independent in the $\mathbb R$-vector space $\mathbb{R^N}$? Here $2=\{0,1\}$. I would like hints, not full answers. Thanks Edit: I have shown that if there is some $I\subseteq\Bbb N$ such that $f_1\upharpoonright I,\ldots,f_n\upharpoonright I$ is linearly independent over $\Bbb Q$ with $|I|\geq n$,then we are done, however I can't see why such $I$ should exist.","I have the following doubt: Suppose $f_1,\ldots,f_n\in 2^{\mathbb N}$ are such that $\{f_1,\ldots,f_n\}$ is linearly independent in the $\mathbb Q$-vector space $\mathbb{Q^N}$. Does this set remain linearly independent in the $\mathbb R$-vector space $\mathbb{R^N}$? Here $2=\{0,1\}$. I would like hints, not full answers. Thanks Edit: I have shown that if there is some $I\subseteq\Bbb N$ such that $f_1\upharpoonright I,\ldots,f_n\upharpoonright I$ is linearly independent over $\Bbb Q$ with $|I|\geq n$,then we are done, however I can't see why such $I$ should exist.",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
27,Wedge Products with the Symplectic Form,Wedge Products with the Symplectic Form,,"Let $ \omega $ denote the symplectic form on $ \mathbb{R}^{2n} $, namely $ \omega = dx_1 \wedge dy_1 + \cdots + dx_n \wedge dy_n $.  Then let $ T $ be the linear map from $ (n-1) $- forms to $ (n+1) $-forms given by $ T(\tau) = \omega \wedge \tau $.  I believe that $ T $ is a linear isomorphism, but I need help finding the proof, please.","Let $ \omega $ denote the symplectic form on $ \mathbb{R}^{2n} $, namely $ \omega = dx_1 \wedge dy_1 + \cdots + dx_n \wedge dy_n $.  Then let $ T $ be the linear map from $ (n-1) $- forms to $ (n+1) $-forms given by $ T(\tau) = \omega \wedge \tau $.  I believe that $ T $ is a linear isomorphism, but I need help finding the proof, please.",,"['linear-algebra', 'symplectic-geometry', 'vector-space-isomorphism']"
28,Method of determining symmetries in an irregular polygon (2D or 3D)?,Method of determining symmetries in an irregular polygon (2D or 3D)?,,"Thank you in advance for helping. Given a polygon with $n$ vertices, $$P = \begin{bmatrix} x_{1} & x_{2} & ... & x_{n} \\ y_{1} & y_{2} & ... & y_{n} \end{bmatrix}$$ how does one determine: 1) The axes of symmetry, if there are any 2) Radial (rotational) symmetry, if it exists 3) Any other type of symmetry (or something $close$ to a symmetry, if such a thing exists) Methods involving linear algebra and/or group theory are more than welcome. And if you can suggest, any thoughts for finding symmetries in three-dimensional polygons? (Not sure what they're called...) $$P = \begin{bmatrix} x_{1} & x_{2} & ... & x_{n} \\ y_{1} & y_{2} & ... & y_{n} \\ z_{1} & z_{2} & ... & z_{n} \end{bmatrix}$$","Thank you in advance for helping. Given a polygon with $n$ vertices, $$P = \begin{bmatrix} x_{1} & x_{2} & ... & x_{n} \\ y_{1} & y_{2} & ... & y_{n} \end{bmatrix}$$ how does one determine: 1) The axes of symmetry, if there are any 2) Radial (rotational) symmetry, if it exists 3) Any other type of symmetry (or something $close$ to a symmetry, if such a thing exists) Methods involving linear algebra and/or group theory are more than welcome. And if you can suggest, any thoughts for finding symmetries in three-dimensional polygons? (Not sure what they're called...) $$P = \begin{bmatrix} x_{1} & x_{2} & ... & x_{n} \\ y_{1} & y_{2} & ... & y_{n} \\ z_{1} & z_{2} & ... & z_{n} \end{bmatrix}$$",,"['linear-algebra', 'matrices', 'group-theory', 'symmetry', 'reflection']"
29,Powerful applications of linear algebra?,Powerful applications of linear algebra?,,"I'd like to see some neat, elegant applications of linear algebra. I'm a undergraduate student but I don't want to prevent people from posting things just because I won't understand them, but if it's undergraduate level even better. Examples: Cayley–Bacharach theorem Every element in a finite extension of a field is algebric Radon's theorem","I'd like to see some neat, elegant applications of linear algebra. I'm a undergraduate student but I don't want to prevent people from posting things just because I won't understand them, but if it's undergraduate level even better. Examples: Cayley–Bacharach theorem Every element in a finite extension of a field is algebric Radon's theorem",,['linear-algebra']
30,Exercise 2.27 from Bazaraa (LP),Exercise 2.27 from Bazaraa (LP),,"Consider the system $Ax=b$ where $A=[a_1,a_2,...,a_n]$ is an $m \times n$ matrix of rank $m$. Let $x$ be any solution of this system. Starting with $x$, construct a basic solution. There is a hint also. Suppose $x_1,...,x_p \neq 0$ and $x_p+1,...,x_n=0$. If $p>m$ represent one of the columns $a_j$ for $j=1,...,p$ as a linear combination of the remaining vectors. This results in a new solution having a smaller number of nonzero variables. Repeat the process. Ok. I will repeat the process until $p=m$. But in each iteration, how will the remaining non-zero variables change? I do not know how to figure out that. Could you please help.","Consider the system $Ax=b$ where $A=[a_1,a_2,...,a_n]$ is an $m \times n$ matrix of rank $m$. Let $x$ be any solution of this system. Starting with $x$, construct a basic solution. There is a hint also. Suppose $x_1,...,x_p \neq 0$ and $x_p+1,...,x_n=0$. If $p>m$ represent one of the columns $a_j$ for $j=1,...,p$ as a linear combination of the remaining vectors. This results in a new solution having a smaller number of nonzero variables. Repeat the process. Ok. I will repeat the process until $p=m$. But in each iteration, how will the remaining non-zero variables change? I do not know how to figure out that. Could you please help.",,"['linear-algebra', 'optimization', 'linear-programming', 'operations-research']"
31,"What is the span of $(1, 1, 1), (0, -1, 1), (0, 0, -1) \in \mathbb R^3$?",What is the span of ?,"(1, 1, 1), (0, -1, 1), (0, 0, -1) \in \mathbb R^3","What is the span of $(1, 1, 1), (0, -1, 1), (0, 0, -1) \in \mathbb R^3$? Supposing we haven't covered linear in/dependence, can we solve the problem as done below? The span is a set of all systems: $$ \left\{  \begin{array}{c} a+b\cdot 0 + c\cdot 0 \\  a-b + c \cdot 0 \\  a+b -c \end{array} \right.  $$ where $a, b, c \in \mathbb R$. Suppose $(x, y, z) \in \mathbb R^3$ and $$ \left\{  \begin{array}{c} a+b\cdot 0 + c\cdot 0 = x \\  a-b + c \cdot 0 = y\\  a+b -c = z \end{array} \right.  $$ Then $b = x – y$ and $c = x + x – y – z = 2x -y – z$. So, $x(1, 1, 1) + (x – y)(0, -1, 1) + (2x -y – z)(0, 0, -1)$ $= (x, x, x) + (0, y – x, x – y) + (0, 0, -2x + y + z)$ $= (x, y,  z)$ Thus the given set of vectors spans $\mathbb R^3$.","What is the span of $(1, 1, 1), (0, -1, 1), (0, 0, -1) \in \mathbb R^3$? Supposing we haven't covered linear in/dependence, can we solve the problem as done below? The span is a set of all systems: $$ \left\{  \begin{array}{c} a+b\cdot 0 + c\cdot 0 \\  a-b + c \cdot 0 \\  a+b -c \end{array} \right.  $$ where $a, b, c \in \mathbb R$. Suppose $(x, y, z) \in \mathbb R^3$ and $$ \left\{  \begin{array}{c} a+b\cdot 0 + c\cdot 0 = x \\  a-b + c \cdot 0 = y\\  a+b -c = z \end{array} \right.  $$ Then $b = x – y$ and $c = x + x – y – z = 2x -y – z$. So, $x(1, 1, 1) + (x – y)(0, -1, 1) + (2x -y – z)(0, 0, -1)$ $= (x, x, x) + (0, y – x, x – y) + (0, 0, -2x + y + z)$ $= (x, y,  z)$ Thus the given set of vectors spans $\mathbb R^3$.",,['linear-algebra']
32,Help me to prove the determinant of given matrix.,Help me to prove the determinant of given matrix.,,"Suppose, $ M=\begin{bmatrix}\begin{array}{ccccccc}      	-x & a_2&a_3&a_4&\cdots &a_n\\      	a_{1}+x & -x-a_2 & 0&0&\cdots &0\\      	a_1+x&0 & -x-a_3 &0&\cdots &0\\      	\vdots&\vdots&\vdots&\vdots &\ddots&\vdots\\      	a_1+x&0 & 0&0&\cdots & -x-a_n\\      	\end{array}\end{bmatrix}$, then how to find the $\det (M)$? The empirical formula I got from considering $n=2,3,4$ in Wolfram Mathematica is $$ (-1)^n(x^n-\sum_{k=2}^n (k-1)\sigma_k x^{n-k}) $$ where $\sigma_k$ is the $k$-th elementary symmetric polynomial in $a_1,\dots,a_n$.","Suppose, $ M=\begin{bmatrix}\begin{array}{ccccccc}      	-x & a_2&a_3&a_4&\cdots &a_n\\      	a_{1}+x & -x-a_2 & 0&0&\cdots &0\\      	a_1+x&0 & -x-a_3 &0&\cdots &0\\      	\vdots&\vdots&\vdots&\vdots &\ddots&\vdots\\      	a_1+x&0 & 0&0&\cdots & -x-a_n\\      	\end{array}\end{bmatrix}$, then how to find the $\det (M)$? The empirical formula I got from considering $n=2,3,4$ in Wolfram Mathematica is $$ (-1)^n(x^n-\sum_{k=2}^n (k-1)\sigma_k x^{n-k}) $$ where $\sigma_k$ is the $k$-th elementary symmetric polynomial in $a_1,\dots,a_n$.",,"['linear-algebra', 'matrices', 'determinant']"
33,$X:\Vert X\Vert_2<1 \iff\text{ matrix }\begin{bmatrix} I&X^*\\X&I\\\end{bmatrix} $ is positive,is positive,X:\Vert X\Vert_2<1 \iff\text{ matrix }\begin{bmatrix} I&X^*\\X&I\\\end{bmatrix} ,"Following question seems so simple, yet I could not come up with a solution. I started to think that there might be sth wrong with the question. Could you please take a look? For a matrix $X:\Vert X\Vert_2<1 \iff \text{ matrix }\begin{bmatrix} I&X^*\\X&I\\\end{bmatrix} $ is positive","Following question seems so simple, yet I could not come up with a solution. I started to think that there might be sth wrong with the question. Could you please take a look? For a matrix $X:\Vert X\Vert_2<1 \iff \text{ matrix }\begin{bmatrix} I&X^*\\X&I\\\end{bmatrix} $ is positive",,"['linear-algebra', 'normed-spaces']"
34,Finding nonsimilar solutions to the matrix equation $ Y^2 = Y $.,Finding nonsimilar solutions to the matrix equation ., Y^2 = Y ,"Let $Y \in M_n $. We have $ Y^2 = Y \Longleftrightarrow Y(Y-I) = 0 $. This means that the vectors in $ Y-I $ are in the nullspace of $ Y$. This means that $ Y $ is a projection (also because the characteristic polynomial is $x^2-x=0$, which has the only roots 0 and 1). Non-similar solutions are then given by non-similar projection matrices, which should be those with unequal ranks. There are thus $n$ non-similar solutions (from rank 1 to rank $n$ projections, where a rank $n$ projection would be defined to just be a positive-definite unitary matrix; if one counts the zero matrix, then that would be $n+1$ solutions, I guess). Since projections are diagonalizable, it is pretty trivial that projections of equal rank are similar to each other. Is there more to this problem than meets the eye? Edit: the rank-$n$ projection should just be the identity matrix, I believe.","Let $Y \in M_n $. We have $ Y^2 = Y \Longleftrightarrow Y(Y-I) = 0 $. This means that the vectors in $ Y-I $ are in the nullspace of $ Y$. This means that $ Y $ is a projection (also because the characteristic polynomial is $x^2-x=0$, which has the only roots 0 and 1). Non-similar solutions are then given by non-similar projection matrices, which should be those with unequal ranks. There are thus $n$ non-similar solutions (from rank 1 to rank $n$ projections, where a rank $n$ projection would be defined to just be a positive-definite unitary matrix; if one counts the zero matrix, then that would be $n+1$ solutions, I guess). Since projections are diagonalizable, it is pretty trivial that projections of equal rank are similar to each other. Is there more to this problem than meets the eye? Edit: the rank-$n$ projection should just be the identity matrix, I believe.",,"['linear-algebra', 'matrices']"
35,Phase portait of 3 by 3 matrix.,Phase portait of 3 by 3 matrix.,,"Consider the following linear system $x = Ax$, where $x\in\mathbb R^3$ and  $$\begin{bmatrix} -3 & 0 & 0 \\ 0 & 3 & -2 \\ 0 & 1 & 1 \end{bmatrix}.$$ Draw the phase portrait nearby the origin as precisely, without computing the eigenvectors of the matrix. I found $-3,2+i$ and $2-i$ are the eigenvalues. How could I figure out the  phase portrait nearby the origin. Please help me out to solve it.","Consider the following linear system $x = Ax$, where $x\in\mathbb R^3$ and  $$\begin{bmatrix} -3 & 0 & 0 \\ 0 & 3 & -2 \\ 0 & 1 & 1 \end{bmatrix}.$$ Draw the phase portrait nearby the origin as precisely, without computing the eigenvectors of the matrix. I found $-3,2+i$ and $2-i$ are the eigenvalues. How could I figure out the  phase portrait nearby the origin. Please help me out to solve it.",,"['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
36,"What do mathematicians mean by if two vector spaces are isomorphic, then ""they are the same""","What do mathematicians mean by if two vector spaces are isomorphic, then ""they are the same""",,"I have been reading some of the answers here that if two vector fields are isomorphic, then they are ""essentially the same"". For example, an answer  here: What does ""isomorphic"" mean in linear algebra? Because isomorphic vector spaces are the same size and have the same algebraic properties, mathematicians think of them as ""the same, for all intents and purposes."" Now when you read it the first time, it is pretty impressive. But when you are sitting down and doing a problem. You are wondering just exactly what it means. What algebraic properties are the same? What do you mean by size? What does it mean by ""the same, for all intents and purposes"". Can someone please concretify this notion using the following example? Claim: An isomorphism between symmetric matrices $S^{n \times n}$ and $R^{n^2}$ is given by map $F = vectorize(A)$ Where given $A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}$, vectorize(A) gives $\begin{bmatrix} a_{11} \\ a_{12} \\ a_{21}\\ a_{22} \end{bmatrix}$ Ok, this is a nice math trick. But so what? Ok so there is an invertible map. What does that imply about the algebraic properties of the two vector spaces?","I have been reading some of the answers here that if two vector fields are isomorphic, then they are ""essentially the same"". For example, an answer  here: What does ""isomorphic"" mean in linear algebra? Because isomorphic vector spaces are the same size and have the same algebraic properties, mathematicians think of them as ""the same, for all intents and purposes."" Now when you read it the first time, it is pretty impressive. But when you are sitting down and doing a problem. You are wondering just exactly what it means. What algebraic properties are the same? What do you mean by size? What does it mean by ""the same, for all intents and purposes"". Can someone please concretify this notion using the following example? Claim: An isomorphism between symmetric matrices $S^{n \times n}$ and $R^{n^2}$ is given by map $F = vectorize(A)$ Where given $A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}$, vectorize(A) gives $\begin{bmatrix} a_{11} \\ a_{12} \\ a_{21}\\ a_{22} \end{bmatrix}$ Ok, this is a nice math trick. But so what? Ok so there is an invertible map. What does that imply about the algebraic properties of the two vector spaces?",,"['linear-algebra', 'vector-spaces', 'definition', 'vectors', 'linear-transformations']"
37,A $3\times 3$ matrix is positive semidefinite if and only if all of its $2\times 2$ submatrices are positive semidefinite$?$,A  matrix is positive semidefinite if and only if all of its  submatrices are positive semidefinite,3\times 3 2\times 2 ?,"Is it true that a $3\times 3$ Hermitian matrix, $A$, is positive semidefinite if and only if all of its $2\times 2$ submatrices are positive semidefinite$?$ First consider the ordered set of projectors that project out a single axis  $$P=\left(I-e_1e_1^T,I-e_2e_2^T,I-e_3e_3^T \right)$$ I can write the identity matrix as the following: $$I = \frac{1}{2} \left(P_1+P_2+P_3 \right) $$ Now the condition in order for my 3x3 matrix, A, to be semidefinite is: $$\forall x \in \mathbb{R}^3: x^TAx \ge0$$ Inserting the identity matrix on both sides: $$\forall x \in \mathbb{R}^3: x^TAx=x^TIAIx \ge0$$ and expanding out my definition of the identity. $$\forall x \in \mathbb{R}^3: x^TAx=\frac{1}{4}x^T\left(P_1+P_2+P_3 \right)A\left(P_1+P_2+P_3 \right)x \ge0$$ Expanding the product we get: $$\forall x \in \mathbb{R}^3: x^TP_1AP_1x+x^TP_1AP_2x+x^TP_1AP_3x+x^TP_2AP_1x+x^TP_2AP_2x+x^TP_2AP_3x+x^TP_3AP_1x+x^TP_3AP_2x+x^TP_3AP_3x\ge0$$ Exploiting the hermiticity of A and the projection matrices we can rewrite the above as: $$\forall x \in \mathbb{R}^3: x^TP_1AP_1x+2x^TP_1AP_2x+2x^TP_1AP_3x+x^TP_2AP_2x+2x^TP_2AP_3x+x^TP_3AP_3x\ge0$$ Now I believe that through the various projections each term in this sum can be independently varied with particular choices in the variation of X.  Since each term in the sum is independent of the others, the inequality distributes over each term in the sum. $$\leftrightarrow \forall x \in \mathbb{R}^3:x^TP_1AP_1x \ge0 \cap x^TP_1AP_2x \ge0 \cap x^TP_1AP_3x \ge0 \cap x^TP_2AP_2x \ge0 \cap x^TP_2AP_3x \ge0 \cap  x^TP_3AP_3x \ge0$$ Since each term above is a 3x3 matrix with a 2x2 submatrix and the rest of the entries zero, this implies that every 2x2 submatrix of A is positive semidefinite if and only if A is positive semidefinite. Is my reasoning sound or is the sleep deprivation kicking in?","Is it true that a $3\times 3$ Hermitian matrix, $A$, is positive semidefinite if and only if all of its $2\times 2$ submatrices are positive semidefinite$?$ First consider the ordered set of projectors that project out a single axis  $$P=\left(I-e_1e_1^T,I-e_2e_2^T,I-e_3e_3^T \right)$$ I can write the identity matrix as the following: $$I = \frac{1}{2} \left(P_1+P_2+P_3 \right) $$ Now the condition in order for my 3x3 matrix, A, to be semidefinite is: $$\forall x \in \mathbb{R}^3: x^TAx \ge0$$ Inserting the identity matrix on both sides: $$\forall x \in \mathbb{R}^3: x^TAx=x^TIAIx \ge0$$ and expanding out my definition of the identity. $$\forall x \in \mathbb{R}^3: x^TAx=\frac{1}{4}x^T\left(P_1+P_2+P_3 \right)A\left(P_1+P_2+P_3 \right)x \ge0$$ Expanding the product we get: $$\forall x \in \mathbb{R}^3: x^TP_1AP_1x+x^TP_1AP_2x+x^TP_1AP_3x+x^TP_2AP_1x+x^TP_2AP_2x+x^TP_2AP_3x+x^TP_3AP_1x+x^TP_3AP_2x+x^TP_3AP_3x\ge0$$ Exploiting the hermiticity of A and the projection matrices we can rewrite the above as: $$\forall x \in \mathbb{R}^3: x^TP_1AP_1x+2x^TP_1AP_2x+2x^TP_1AP_3x+x^TP_2AP_2x+2x^TP_2AP_3x+x^TP_3AP_3x\ge0$$ Now I believe that through the various projections each term in this sum can be independently varied with particular choices in the variation of X.  Since each term in the sum is independent of the others, the inequality distributes over each term in the sum. $$\leftrightarrow \forall x \in \mathbb{R}^3:x^TP_1AP_1x \ge0 \cap x^TP_1AP_2x \ge0 \cap x^TP_1AP_3x \ge0 \cap x^TP_2AP_2x \ge0 \cap x^TP_2AP_3x \ge0 \cap  x^TP_3AP_3x \ge0$$ Since each term above is a 3x3 matrix with a 2x2 submatrix and the rest of the entries zero, this implies that every 2x2 submatrix of A is positive semidefinite if and only if A is positive semidefinite. Is my reasoning sound or is the sleep deprivation kicking in?",,"['linear-algebra', 'matrices', 'semidefinite-programming']"
38,Intersection of hyperplanes,Intersection of hyperplanes,,"We have two vectors, $x = (1,4, -1)$ and $y = (-1,0,1)$ in $\mathbb{R}^3$ . We have the following hyperplanes: $P_1 = \{ v \in \mathbb{R}^3 : \langle x, v \rangle =2\} $ $P_2 = \{ v \in \mathbb{R}^3 : \langle y, v \rangle =0\} $ Show the the intersection of these hyperplanes is a line. So I think that $P_1$ and $P_2$ are $x_1 + 4x_2 - x_3  = 2$ and $x_1 = x_3$ respectively. So to find the intersection you fill in $x_1 = x_3$ in $P_1$ and you get $x_2=\dfrac{1}{2}$ , which is a line. So now my question: Is this a satisfactory answer to the question? What relation does $x$ have with $P_1$ (algebraically or geometrically), or $y$ with $P_2$ ? It seems as if you're just taking the coordinates of the vector and then multiplying them with the corresponding variable.","We have two vectors, and in . We have the following hyperplanes: Show the the intersection of these hyperplanes is a line. So I think that and are and respectively. So to find the intersection you fill in in and you get , which is a line. So now my question: Is this a satisfactory answer to the question? What relation does have with (algebraically or geometrically), or with ? It seems as if you're just taking the coordinates of the vector and then multiplying them with the corresponding variable.","x = (1,4, -1) y = (-1,0,1) \mathbb{R}^3 P_1 = \{ v \in \mathbb{R}^3 : \langle x, v \rangle =2\}  P_2 = \{ v \in \mathbb{R}^3 : \langle y, v \rangle =0\}  P_1 P_2 x_1 + 4x_2 - x_3  = 2 x_1 = x_3 x_1 = x_3 P_1 x_2=\dfrac{1}{2} x P_1 y P_2","['linear-algebra', 'analytic-geometry']"
39,Connecting a vector space to its dual - why?,Connecting a vector space to its dual - why?,,"Can someone explain to me - intuitively - why embedding a vector space into its dual should naturally fix its geometry? I mean, I can run the usual statements through my mind - ""The injection into the dual gives an non-degenerate bilinear form (inner product), which allows us to define length, angle, etc, while conversely the bilinear form has an embedding into the dual as a by-product"", but I feel that there is some sort of understanding that is fluttering just over my head as I ponder these arguments. Why is this the right way, or the natural way, to go about putting a geometry on the vector space? Once we have fixed such a geometry, what do we gain from interpreting a vector as a functional and vice versa? It just seems very strange that once a correspondence with the dual has been fixed, then so should whatever geometrical properties that the space may have, or why the natural geometry of the space should arise from this connection.","Can someone explain to me - intuitively - why embedding a vector space into its dual should naturally fix its geometry? I mean, I can run the usual statements through my mind - ""The injection into the dual gives an non-degenerate bilinear form (inner product), which allows us to define length, angle, etc, while conversely the bilinear form has an embedding into the dual as a by-product"", but I feel that there is some sort of understanding that is fluttering just over my head as I ponder these arguments. Why is this the right way, or the natural way, to go about putting a geometry on the vector space? Once we have fixed such a geometry, what do we gain from interpreting a vector as a functional and vice versa? It just seems very strange that once a correspondence with the dual has been fixed, then so should whatever geometrical properties that the space may have, or why the natural geometry of the space should arise from this connection.",,"['linear-algebra', 'vector-spaces', 'intuition']"
40,"Sending unitary group to reals, second differential at critical points.","Sending unitary group to reals, second differential at critical points.",,"Let$$X = U(n) = \{B \in \text{GL}_n(\mathbb{C}): BB^* = I\}$$be the group of $n \times n$ unitary matrices over $\mathbb{C}$, viewed as a real manifold. Fix a diagonal $n \times n$ matrix $A$ with real entries, whose diagonal entries, call them $a_1, \dots, a_n$, are pairwise distinct. (Of course, usually, $A \notin U(n)$.) Define a (smooth) function$$f: U(n) \to \mathbb{R},\,B \mapsto \text{Re}(\text{Tr}(AB)),$$where $\text{Re}(z)$ is the real part of a complex number $z$. What are the critical points of $f$? What is the second differential of $f$ at each of these points?","Let$$X = U(n) = \{B \in \text{GL}_n(\mathbb{C}): BB^* = I\}$$be the group of $n \times n$ unitary matrices over $\mathbb{C}$, viewed as a real manifold. Fix a diagonal $n \times n$ matrix $A$ with real entries, whose diagonal entries, call them $a_1, \dots, a_n$, are pairwise distinct. (Of course, usually, $A \notin U(n)$.) Define a (smooth) function$$f: U(n) \to \mathbb{R},\,B \mapsto \text{Re}(\text{Tr}(AB)),$$where $\text{Re}(z)$ is the real part of a complex number $z$. What are the critical points of $f$? What is the second differential of $f$ at each of these points?",,"['linear-algebra', 'differential-geometry']"
41,Linear independence and the Wronskian,Linear independence and the Wronskian,,"Suppose I have two linearly independent solution vectors \begin{bmatrix}x_1,_1(t)\\x_1,_2(t)\end{bmatrix} and \begin{bmatrix}x_2,_1(t)\\x_2,_2(t)\end{bmatrix} If I take the Wronskian of these 2 solution vectors, it comes out to a nonzero number since they are stated to be Linearly Independent. My question is, if you take the Wronskian of the same solution vectors but their derivative: \begin{bmatrix}x'_{1,1(t)}&x'_{2,1(t)}\\x''_{1,1(t)}&x''_{2,1(t)}\end{bmatrix} Would it still be linearly independent? (Would the Wronskian still be a nonzero number?","Suppose I have two linearly independent solution vectors \begin{bmatrix}x_1,_1(t)\\x_1,_2(t)\end{bmatrix} and \begin{bmatrix}x_2,_1(t)\\x_2,_2(t)\end{bmatrix} If I take the Wronskian of these 2 solution vectors, it comes out to a nonzero number since they are stated to be Linearly Independent. My question is, if you take the Wronskian of the same solution vectors but their derivative: \begin{bmatrix}x'_{1,1(t)}&x'_{2,1(t)}\\x''_{1,1(t)}&x''_{2,1(t)}\end{bmatrix} Would it still be linearly independent? (Would the Wronskian still be a nonzero number?",,"['linear-algebra', 'ordinary-differential-equations', 'wronskian']"
42,Determine the null space of a linear map,Determine the null space of a linear map,,"Let $P_k(x)$ denote the space of polynomials of at most degree $k$. Let $D$ denote differentiation with respect to $x$. Regard the differential operator $L: P_k\rightarrow P_k$ such that $L=\frac{1}{n!}D^n+\frac{1}{(n-1)!}D^{n-1}+...+D+I$ . If $k\leq n$, find the dimension of the kernel of $L-T$ where $T:P_k\rightarrow P_k$ is given by $T(p(x))=p(x+1)$. To minimize the amount of calculation, I start with finding the matrix representation of $D$ w.r.t $\{1,x,x^2,...,\}$ basis, which is a matrix with $1,2,3,..,n$ on the super diagonal and 0 everywhere else. Then should I find $D^k$ for each $k$? The computation seems to be insane. Are there any easier way? Any shortcuts?","Let $P_k(x)$ denote the space of polynomials of at most degree $k$. Let $D$ denote differentiation with respect to $x$. Regard the differential operator $L: P_k\rightarrow P_k$ such that $L=\frac{1}{n!}D^n+\frac{1}{(n-1)!}D^{n-1}+...+D+I$ . If $k\leq n$, find the dimension of the kernel of $L-T$ where $T:P_k\rightarrow P_k$ is given by $T(p(x))=p(x+1)$. To minimize the amount of calculation, I start with finding the matrix representation of $D$ w.r.t $\{1,x,x^2,...,\}$ basis, which is a matrix with $1,2,3,..,n$ on the super diagonal and 0 everywhere else. Then should I find $D^k$ for each $k$? The computation seems to be insane. Are there any easier way? Any shortcuts?",,['linear-algebra']
43,How to perturb an adjacency matrix in order to have the highest increase in spectral radius?,How to perturb an adjacency matrix in order to have the highest increase in spectral radius?,,"Let's suppose I have a generic directed graph $G$ and it's adjacency matrix $A$ . I can add an arc wherever I want in the graph. (i.e. perturb the matrix $A$ changing a single $0$ into a $1$ ).  Where should I put that one to have the highest increase in the biggest eigenvector as possible? I suppose that the answer is ""where you can connect the two largest strongly connected components"".","Let's suppose I have a generic directed graph and it's adjacency matrix . I can add an arc wherever I want in the graph. (i.e. perturb the matrix changing a single into a ).  Where should I put that one to have the highest increase in the biggest eigenvector as possible? I suppose that the answer is ""where you can connect the two largest strongly connected components"".",G A A 0 1,"['linear-algebra', 'graph-theory', 'spectral-graph-theory']"
44,question on self adjoint operator [duplicate],question on self adjoint operator [duplicate],,This question already has answers here : $A^2=A^*A$. Why is matrix $A$ Hermitian? [duplicate] (2 answers) Closed 8 years ago . Suppose $A$ is a $n\times n$ matrix with complex entries and $A^*A=A^2$. Does it imply $A=A^*.$,This question already has answers here : $A^2=A^*A$. Why is matrix $A$ Hermitian? [duplicate] (2 answers) Closed 8 years ago . Suppose $A$ is a $n\times n$ matrix with complex entries and $A^*A=A^2$. Does it imply $A=A^*.$,,['linear-algebra']
45,Show that there exists a vector $v$ such that $Av\neq 0$ but $A^2v=0$,Show that there exists a vector  such that  but,v Av\neq 0 A^2v=0,"Let $A$ be a $4\times 4$ matrix over $\mathbb C$ such that $rank A=2$ and $A^3=A^2\neq 0$.Suppose that $A$ is not diagonalisable. Then Show that there exists a vector $v$ such that $Av\neq 0$ but $A^2v=0$ My try :$\dim \operatorname{Im}(A)+\dim \ker (A)=4$ so $\dim(\ker A)=\dim (\operatorname{Im}A)=2$. Now $A$ satisfies $x^3=x^2\implies x^2(x-1)=0$ thus $0,1$ are the only eigen values of $A$. Since $A$ has rank $2$ so geometric multiplicity of $0$ is $2$ but $A$ is not diagonalizable thus algebraic multiplicity of $0$ is $3$. So the characteristic polynomial will be $x^4-x^3=0$ Obviously $A$ will have a Jordan block of size $2$ corresponding to 0 $$\   A  \text{ will have Jordan form as } \pmatrix{1&0&0&0\\0&0&1&0\\0&0&0&0\\0&0&0&0}   $$ How should I use these  information to conclude my result??","Let $A$ be a $4\times 4$ matrix over $\mathbb C$ such that $rank A=2$ and $A^3=A^2\neq 0$.Suppose that $A$ is not diagonalisable. Then Show that there exists a vector $v$ such that $Av\neq 0$ but $A^2v=0$ My try :$\dim \operatorname{Im}(A)+\dim \ker (A)=4$ so $\dim(\ker A)=\dim (\operatorname{Im}A)=2$. Now $A$ satisfies $x^3=x^2\implies x^2(x-1)=0$ thus $0,1$ are the only eigen values of $A$. Since $A$ has rank $2$ so geometric multiplicity of $0$ is $2$ but $A$ is not diagonalizable thus algebraic multiplicity of $0$ is $3$. So the characteristic polynomial will be $x^4-x^3=0$ Obviously $A$ will have a Jordan block of size $2$ corresponding to 0 $$\   A  \text{ will have Jordan form as } \pmatrix{1&0&0&0\\0&0&1&0\\0&0&0&0\\0&0&0&0}   $$ How should I use these  information to conclude my result??",,"['linear-algebra', 'matrices']"
46,The closure of a subspace of a normed vector space is a subspace,The closure of a subspace of a normed vector space is a subspace,,"This is a self-study problem (Folland Real Analysis exercise 5.5). If $\mathcal{X}$ is a normed vector space, the closure of any subspace of $\mathcal{X}$ is a subspace. My attempt: It is evident that $0 \in \overline{\mathcal{M}}$. Let $\mathcal{M} \subset \mathcal{X}$ and consider $v,w \in \overline{\mathcal{M}}$. As $v$ is a limit point, there is some sequence $\{v_n\} \to v$ in the metric induced by the norm; namely for some $N$, $\|v_n - v\| \le \frac{\epsilon}{2}$ for $n \ge N$, and likewise for $w$. Then, for scalars $a,b$, $$\|av_n+bw_n - (av + bw)\| = \|av_n - av + bw_n - bw\| \le |a|\|v_n - v\| + |b|\|w_n - w\| = \epsilon(|a|+|b|).$$ Therefore, $\mathcal{M} \supset \{av_n+bw_n\} \to av+bw$, so $av+bw \in \overline{\mathcal{M}}$. My question: Is my proof correct?","This is a self-study problem (Folland Real Analysis exercise 5.5). If $\mathcal{X}$ is a normed vector space, the closure of any subspace of $\mathcal{X}$ is a subspace. My attempt: It is evident that $0 \in \overline{\mathcal{M}}$. Let $\mathcal{M} \subset \mathcal{X}$ and consider $v,w \in \overline{\mathcal{M}}$. As $v$ is a limit point, there is some sequence $\{v_n\} \to v$ in the metric induced by the norm; namely for some $N$, $\|v_n - v\| \le \frac{\epsilon}{2}$ for $n \ge N$, and likewise for $w$. Then, for scalars $a,b$, $$\|av_n+bw_n - (av + bw)\| = \|av_n - av + bw_n - bw\| \le |a|\|v_n - v\| + |b|\|w_n - w\| = \epsilon(|a|+|b|).$$ Therefore, $\mathcal{M} \supset \{av_n+bw_n\} \to av+bw$, so $av+bw \in \overline{\mathcal{M}}$. My question: Is my proof correct?",,"['linear-algebra', 'general-topology', 'vector-spaces', 'normed-spaces']"
47,Optimal Matching Distance,Optimal Matching Distance,,"I'm stuck on problem II.5.9 from Bhatia's Matrix Analysis.  The problem is as follows: Let $\left(\lambda_1,\dots,\lambda_n\right)$ and $\left(\mu_1,\dots,\mu_n\right)$ be two $n$-tuples of complex numbers. Let   $$ d(\lambda,\mu) = \min_\sigma \max_{1 \leq j \leq n}|\lambda_j - \mu_{\sigma(j)}| $$   where the minimum is taken over all permutations on $n$ symbols.  This is called the optimal matching distance.... Show that we also have   $$ d(\lambda,\mu) = \max_{I,J \subset \{1,\dots,n\};|I| + |J| = n+1} \min _{i \in I, j \in J} |\lambda_i - \mu_j| $$ I'm not really sure how to approach this. $n=1,n=2$ doesn't yield much general insight, and $n=3$ is a bit too big to analyze effectively. I also can't find a good intuition for why minimizing one should maximize the other.  I'm thinking that a clever application of the pigeonhole principle fits in somewhere . Any nudges in the right direction would be highly appreciated.","I'm stuck on problem II.5.9 from Bhatia's Matrix Analysis.  The problem is as follows: Let $\left(\lambda_1,\dots,\lambda_n\right)$ and $\left(\mu_1,\dots,\mu_n\right)$ be two $n$-tuples of complex numbers. Let   $$ d(\lambda,\mu) = \min_\sigma \max_{1 \leq j \leq n}|\lambda_j - \mu_{\sigma(j)}| $$   where the minimum is taken over all permutations on $n$ symbols.  This is called the optimal matching distance.... Show that we also have   $$ d(\lambda,\mu) = \max_{I,J \subset \{1,\dots,n\};|I| + |J| = n+1} \min _{i \in I, j \in J} |\lambda_i - \mu_j| $$ I'm not really sure how to approach this. $n=1,n=2$ doesn't yield much general insight, and $n=3$ is a bit too big to analyze effectively. I also can't find a good intuition for why minimizing one should maximize the other.  I'm thinking that a clever application of the pigeonhole principle fits in somewhere . Any nudges in the right direction would be highly appreciated.",,"['linear-algebra', 'combinatorics', 'matrices', 'optimization', 'matching-theory']"
48,Limit of the projection of a matrix when the projection is not continuous,Limit of the projection of a matrix when the projection is not continuous,,"Consider two real matrices: the $n\times n$ matrix $A$ the $n\times m$ matrix $B$ of rank $m$, with $m<n$. Let, for $a\in\mathbb{R}$,  $$S_a=A-aI_n,$$ and denote by $P_a$ the orthogonal projection onto the orthogonal complement of $\operatorname{col}(S_a B)$, that is, $P_a=I_n-S_aB(S_aB)^\dagger$, where $(\cdot)^\dagger$ denotes the Moore-Penrose pseudoinverse. I'm interested in whether the following is correct. Conjecture : For an arbitrary real eigenvalue $\lambda$ of $A$,  $$P_\lambda S_\lambda =0$$ implies $$\lim_{a\to \lambda}P_a S_a =0.$$ My attempts : Note that $\operatorname{rank}(S_a B)=k$ for any $a$ different from an eigenvalue of $A$. The conjecture is correct when $\operatorname{rank}(S_\lambda B)=k$, because in that case $$\lim_{a\to \lambda}P_a=P_\lambda.$$ (since the Moore-Penrose pseudoinverse of a matrix is continuous at some point if the rank of the matrix does not change at that point). From my numerical examples so far it seems to me that the conjecture may be correct even when $\operatorname{rank}(S_\lambda B)=k$, but I cannot see why. I'll also state some facts that might be relevant, or at least might help to clarify my question. I can see that $\operatorname{rank}(S_\lambda B)=k$ iff $\operatorname{null}(S_\lambda)\cap \operatorname{col}(B)=\{0\}$, that is, iff $\operatorname{col}(B)$ does not contain any eigenvectors of $A$ associated to $\lambda$; $P_\lambda S_\lambda =0 \text{ iff }\operatorname{col}(S_\lambda)=\operatorname{col}(S_\lambda B),$ because $\operatorname{col}(S_\lambda B)$ is trivially a subset of $\operatorname{col}(S_\lambda)$ and $P_\lambda S_\lambda =0$ iff $\operatorname{col}(S_\lambda)\subseteq\operatorname{col}(S_\lambda B)$; $P_a S_a \neq0$ for any $a$ different from an eigenvalue $\lambda$ of $A$.","Consider two real matrices: the $n\times n$ matrix $A$ the $n\times m$ matrix $B$ of rank $m$, with $m<n$. Let, for $a\in\mathbb{R}$,  $$S_a=A-aI_n,$$ and denote by $P_a$ the orthogonal projection onto the orthogonal complement of $\operatorname{col}(S_a B)$, that is, $P_a=I_n-S_aB(S_aB)^\dagger$, where $(\cdot)^\dagger$ denotes the Moore-Penrose pseudoinverse. I'm interested in whether the following is correct. Conjecture : For an arbitrary real eigenvalue $\lambda$ of $A$,  $$P_\lambda S_\lambda =0$$ implies $$\lim_{a\to \lambda}P_a S_a =0.$$ My attempts : Note that $\operatorname{rank}(S_a B)=k$ for any $a$ different from an eigenvalue of $A$. The conjecture is correct when $\operatorname{rank}(S_\lambda B)=k$, because in that case $$\lim_{a\to \lambda}P_a=P_\lambda.$$ (since the Moore-Penrose pseudoinverse of a matrix is continuous at some point if the rank of the matrix does not change at that point). From my numerical examples so far it seems to me that the conjecture may be correct even when $\operatorname{rank}(S_\lambda B)=k$, but I cannot see why. I'll also state some facts that might be relevant, or at least might help to clarify my question. I can see that $\operatorname{rank}(S_\lambda B)=k$ iff $\operatorname{null}(S_\lambda)\cap \operatorname{col}(B)=\{0\}$, that is, iff $\operatorname{col}(B)$ does not contain any eigenvectors of $A$ associated to $\lambda$; $P_\lambda S_\lambda =0 \text{ iff }\operatorname{col}(S_\lambda)=\operatorname{col}(S_\lambda B),$ because $\operatorname{col}(S_\lambda B)$ is trivially a subset of $\operatorname{col}(S_\lambda)$ and $P_\lambda S_\lambda =0$ iff $\operatorname{col}(S_\lambda)\subseteq\operatorname{col}(S_\lambda B)$; $P_a S_a \neq0$ for any $a$ different from an eigenvalue $\lambda$ of $A$.",,"['real-analysis', 'linear-algebra', 'limits', 'vector-spaces', 'eigenvalues-eigenvectors']"
49,How to extended a unitary operator to a larger space?,How to extended a unitary operator to a larger space?,,"Problem (the following is the exercise problem from Neilson and Chuang) Suppose $V$ is a Hilbert space with a subspace $W$. Suppose $U: W\rightarrow V$ is a linear operator which preserves inner products, that is, for any $\left|w_1\right>$ and $\left|w_2\right>$ in $W$, $$\left <w_1|U^ {\dagger} U|w_2\right> = \left<w_1|w_2\right> $$ Prove that there exists a unitary operator $U':V\rightarrow V$ which extends $U$. That is, $U'\left|w\right>=U\left|w\right>$ for all $\left|w\right>$ in $W$, but $U'$ is defined on the entire space $V$. Usually we omit the prime symbol $'$ and just write $U$ to denote the extension. Solution Following is the solution I found online from here Solution . It says that the desired operator can be defined as   $U'=U\otimes I$, where the identity is defined only on $V \perp W$. My approach and doubt In the solution I found above the operator $U'=U\otimes I$, will act on a space with dimension $m*n$ where $m$ is dimension of space $W$ and $n$ is dimension of space $V \perp W$, but we wanted an operator which acted on space $V$ which has dimension $m+n$. I know that the operator $U$ will have range as some space $W^{'}$ ( some subspace of space $V$ ) of the same dimension as space $W$. What I came up as a solution was let $\{i\}$ and $\{j\}$ for $1<=i<=m$, be the orthonormal basis for spaces $W$ and $W^{'}$ respectively such that $U=\sum | j \rangle \langle i| $, then we can extend the orthonormal basis of space $W$ to an orthonormal basis of space $V$ ie. $\{i\}$ for $1<=i<=m+n$ and the define $U^{'}=\sum | j \rangle \langle i| $. But I was still doubtful. Am I correct that the solution I found has a flaw and is my approach correct ?","Problem (the following is the exercise problem from Neilson and Chuang) Suppose $V$ is a Hilbert space with a subspace $W$. Suppose $U: W\rightarrow V$ is a linear operator which preserves inner products, that is, for any $\left|w_1\right>$ and $\left|w_2\right>$ in $W$, $$\left <w_1|U^ {\dagger} U|w_2\right> = \left<w_1|w_2\right> $$ Prove that there exists a unitary operator $U':V\rightarrow V$ which extends $U$. That is, $U'\left|w\right>=U\left|w\right>$ for all $\left|w\right>$ in $W$, but $U'$ is defined on the entire space $V$. Usually we omit the prime symbol $'$ and just write $U$ to denote the extension. Solution Following is the solution I found online from here Solution . It says that the desired operator can be defined as   $U'=U\otimes I$, where the identity is defined only on $V \perp W$. My approach and doubt In the solution I found above the operator $U'=U\otimes I$, will act on a space with dimension $m*n$ where $m$ is dimension of space $W$ and $n$ is dimension of space $V \perp W$, but we wanted an operator which acted on space $V$ which has dimension $m+n$. I know that the operator $U$ will have range as some space $W^{'}$ ( some subspace of space $V$ ) of the same dimension as space $W$. What I came up as a solution was let $\{i\}$ and $\{j\}$ for $1<=i<=m$, be the orthonormal basis for spaces $W$ and $W^{'}$ respectively such that $U=\sum | j \rangle \langle i| $, then we can extend the orthonormal basis of space $W$ to an orthonormal basis of space $V$ ie. $\{i\}$ for $1<=i<=m+n$ and the define $U^{'}=\sum | j \rangle \langle i| $. But I was still doubtful. Am I correct that the solution I found has a flaw and is my approach correct ?",,"['linear-algebra', 'vector-spaces']"
50,How do stochastic matrices really converge?,How do stochastic matrices really converge?,,"We are given the matrix $A=\begin{bmatrix}0.9&0.5\\0.1&0.5\end{bmatrix}$ and any initial vector $X^{(0)}=\begin{bmatrix}a\\b\end{bmatrix}$. The matrix $A$ has the following eigensystem: $\qquad\text{Eigenvalues}: 1,\,0.4\\\qquad\text{Eigenvectors: }E_1=\begin{bmatrix}5&1\end{bmatrix}^T, \,E_2=\begin{bmatrix}-1&1\end{bmatrix}^T$ Given in order of occurring eigenvalue. We can write $A^nE_1=1^nE_1=E$, and also $A^nE_2=(0.4)^nE_2$. Since $E_1$ and $E_2$ form a basis in $\mathbb{R}^2$ we can now write our initial value vector $X^{(0)}$ as $x_1'E_1+x_2'E_2$. However, since $X^{(n)}=A^nX^{(0)}$ and because of the linear property of our matrix we get the following equation: $$A^nX^{(0)}=x_1'A^nE_1+x_2'E_2 \iff X^{(n)}=x_1'E_1+x_2'(0.4)^nE_2$$ Because $0.4^n \rightarrow 0$ when $n\rightarrow\infty \iff X^{(n)}\rightarrow x_1'E_1$ when $n\rightarrow\infty$. As such, $X^{(n)}$ always converges to the component of $X^{(0)}$ in the direction of $E_1$. Trying this out, say we have the initial vector $X$ where $a=30,\,b=20$. Note that $a+b=50$ $\text{Proj}_{E_1}X^{(0)} \approx (32.6, 6.5)$ And now, the proportions are right, as $\frac{32.6}{32.6+6.5}\approx\frac{5}{6}$. Note that $32.6+6.5$ is not near $50$. HOWEVER, when I let Wolfram calculate this manually, i.e. $$X^{(n)}=A^nX^{(0)}\rightarrow (41.6667,\,8.3333) \text{ when } n\rightarrow \infty$$ Note that $41.6667\ldots+8.3333\ldots = 50$ Why does this differ from the projection? Where is my theory wrong?","We are given the matrix $A=\begin{bmatrix}0.9&0.5\\0.1&0.5\end{bmatrix}$ and any initial vector $X^{(0)}=\begin{bmatrix}a\\b\end{bmatrix}$. The matrix $A$ has the following eigensystem: $\qquad\text{Eigenvalues}: 1,\,0.4\\\qquad\text{Eigenvectors: }E_1=\begin{bmatrix}5&1\end{bmatrix}^T, \,E_2=\begin{bmatrix}-1&1\end{bmatrix}^T$ Given in order of occurring eigenvalue. We can write $A^nE_1=1^nE_1=E$, and also $A^nE_2=(0.4)^nE_2$. Since $E_1$ and $E_2$ form a basis in $\mathbb{R}^2$ we can now write our initial value vector $X^{(0)}$ as $x_1'E_1+x_2'E_2$. However, since $X^{(n)}=A^nX^{(0)}$ and because of the linear property of our matrix we get the following equation: $$A^nX^{(0)}=x_1'A^nE_1+x_2'E_2 \iff X^{(n)}=x_1'E_1+x_2'(0.4)^nE_2$$ Because $0.4^n \rightarrow 0$ when $n\rightarrow\infty \iff X^{(n)}\rightarrow x_1'E_1$ when $n\rightarrow\infty$. As such, $X^{(n)}$ always converges to the component of $X^{(0)}$ in the direction of $E_1$. Trying this out, say we have the initial vector $X$ where $a=30,\,b=20$. Note that $a+b=50$ $\text{Proj}_{E_1}X^{(0)} \approx (32.6, 6.5)$ And now, the proportions are right, as $\frac{32.6}{32.6+6.5}\approx\frac{5}{6}$. Note that $32.6+6.5$ is not near $50$. HOWEVER, when I let Wolfram calculate this manually, i.e. $$X^{(n)}=A^nX^{(0)}\rightarrow (41.6667,\,8.3333) \text{ when } n\rightarrow \infty$$ Note that $41.6667\ldots+8.3333\ldots = 50$ Why does this differ from the projection? Where is my theory wrong?",,"['linear-algebra', 'matrices', 'markov-chains']"
51,"Prove that $\{\sin x, \sin 2x, ... , \sin nx\}$ is a linearly independent set",Prove that  is a linearly independent set,"\{\sin x, \sin 2x, ... , \sin nx\}","Prove that $\{\sin x, \sin 2x, ... , \sin nx\}$ is linearly independent. The short solution that I do not understand is as follow: For p and q are positive integer, we have  $$ \int\limits_{0}^{\pi}\sin{px}\sin{qx} dx=\left\{\begin{array}{l}0\qquad  \textrm{if } \, p\ne q\\ \dfrac{\pi}{2}\qquad \textrm{if }\, p=q\ne0\\ \end{array} \right. $$ Applying this result to show that if $\sum\limits_{k=1}^{n}\alpha_k\sin kx=0$ then $\alpha_k=0,\, k=1,\dots,n$ The solution is too short for me too understand. I would be grateful if you could explain this problem more in detail for me.","Prove that $\{\sin x, \sin 2x, ... , \sin nx\}$ is linearly independent. The short solution that I do not understand is as follow: For p and q are positive integer, we have  $$ \int\limits_{0}^{\pi}\sin{px}\sin{qx} dx=\left\{\begin{array}{l}0\qquad  \textrm{if } \, p\ne q\\ \dfrac{\pi}{2}\qquad \textrm{if }\, p=q\ne0\\ \end{array} \right. $$ Applying this result to show that if $\sum\limits_{k=1}^{n}\alpha_k\sin kx=0$ then $\alpha_k=0,\, k=1,\dots,n$ The solution is too short for me too understand. I would be grateful if you could explain this problem more in detail for me.",,"['linear-algebra', 'fourier-series', 'lp-spaces']"
52,Jointly nilpotent matrices,Jointly nilpotent matrices,,"We say that a matrix $J \in \mathbb{R}^{n \times n}$ is nilpotent if $J^n = 0$. This is equivalent to the statement that $\forall x \in \mathbb{R}^{n} \quad \exists k \in \mathbb{Z}^+$ such that $J^kx = 0$. What I would like to do is to extend this notion to pairs of matrices in the following way. $J_1, J_2 \in \mathbb{R}^{n \times n}$ is a nilpotent pair if $$\forall x \in \mathbb{R}^{n} \quad \exists N \in \mathbb{Z}^+, \{i_1,k_1,\ldots,i_N,k_N\} \in {\mathbb{Z}^+}^{2N}$$ such that $$J_1^{i_N}J_2^{k_N}\ldots J_1^{i_1}J_2^{k_1}x = 0$$ I am not sure if this definition makes sense. What I would like to ask is how one would attempt to characterize such a notion, i.e. what kind of properties the matrices $J_1,J_2$ should satisfy to be called a nilpotent pair. My apologies if this trivially follows from some existing result.","We say that a matrix $J \in \mathbb{R}^{n \times n}$ is nilpotent if $J^n = 0$. This is equivalent to the statement that $\forall x \in \mathbb{R}^{n} \quad \exists k \in \mathbb{Z}^+$ such that $J^kx = 0$. What I would like to do is to extend this notion to pairs of matrices in the following way. $J_1, J_2 \in \mathbb{R}^{n \times n}$ is a nilpotent pair if $$\forall x \in \mathbb{R}^{n} \quad \exists N \in \mathbb{Z}^+, \{i_1,k_1,\ldots,i_N,k_N\} \in {\mathbb{Z}^+}^{2N}$$ such that $$J_1^{i_N}J_2^{k_N}\ldots J_1^{i_1}J_2^{k_1}x = 0$$ I am not sure if this definition makes sense. What I would like to ask is how one would attempt to characterize such a notion, i.e. what kind of properties the matrices $J_1,J_2$ should satisfy to be called a nilpotent pair. My apologies if this trivially follows from some existing result.",,"['linear-algebra', 'nilpotence']"
53,"Tensor product notation for $(x,y)\mapsto f(x)g(y)$",Tensor product notation for,"(x,y)\mapsto f(x)g(y)","I often stumbled across (variations of) the notation $f\otimes g$ for $(x,y)\mapsto f(x)g(x)$. If $f\in V^*$, $g\in W^*$ for vector spaces $V,W$ over ${\bf K}$ then $f\otimes g:x\otimes y\mapsto f(x)g(y)$ is the standard definition of the tensor product of linear maps where we associate ${\bf K}\otimes{\bf K}$ with ${\bf K}$ in the canonical way. But sometimes I read this in contexts where $f,g$ are not linear functionals, e.g. $f,g\in{\cal C}^\infty(\Omega)$ for $\Omega\subseteq{\bf R}^n$, here I always thought $f\otimes g$ denotes an element in ${\cal C}^\infty(\Omega)\otimes{\cal C}^\infty(\Omega)$, but I could not prove that ${\cal C}^\infty(\Omega)\otimes {\cal C}^\infty(\Omega)$ is a subset of ${\cal C}^\infty(\Omega\times\Omega)$, i.e. the space where $(x,y)\mapsto f(x)g(y)$ lives in. So is this just a notational convention derived from the case of linear functionals, or is there something more subtle going on which I'm not able to see yet?","I often stumbled across (variations of) the notation $f\otimes g$ for $(x,y)\mapsto f(x)g(x)$. If $f\in V^*$, $g\in W^*$ for vector spaces $V,W$ over ${\bf K}$ then $f\otimes g:x\otimes y\mapsto f(x)g(y)$ is the standard definition of the tensor product of linear maps where we associate ${\bf K}\otimes{\bf K}$ with ${\bf K}$ in the canonical way. But sometimes I read this in contexts where $f,g$ are not linear functionals, e.g. $f,g\in{\cal C}^\infty(\Omega)$ for $\Omega\subseteq{\bf R}^n$, here I always thought $f\otimes g$ denotes an element in ${\cal C}^\infty(\Omega)\otimes{\cal C}^\infty(\Omega)$, but I could not prove that ${\cal C}^\infty(\Omega)\otimes {\cal C}^\infty(\Omega)$ is a subset of ${\cal C}^\infty(\Omega\times\Omega)$, i.e. the space where $(x,y)\mapsto f(x)g(y)$ lives in. So is this just a notational convention derived from the case of linear functionals, or is there something more subtle going on which I'm not able to see yet?",,"['linear-algebra', 'notation', 'tensor-products']"
54,On normal matrices and norms,On normal matrices and norms,,"Let $A^*$ denote the complex conjugate transpose of the matrix $A$ and $\|\cdot\|=\|\cdot\|_2$ the norm induced by the Euclidean vector norm. If $$ \|A^*A+AA^*\|-\|A^*A-AA^*\|=\|A^*A\| $$ what can be said about $A$? Note that $A$ does not have to be normal, in fact if $A$ is normal and satisfies the above condition, then $A=0$, as noted in the comments below. My first hypothesis, after lots of numerical experiments was that $\det(A)=0$, since almost all random matrices with the above property fulfilled had zero determinant, but then I found $$ A=\begin{pmatrix}-2&2&0\\0&0&4\\-2&-2&0\end{pmatrix} $$ for which $\det(A)=-32$. This shattered my hypothesis, but the question remains. What can be said about such matrices? EDIT: I'm putting a small bounty on this question. Let's define a class $\mathcal{A}$ of complex-valued matrices such that $$ \mathcal{A}=\{A\in\mathbb{C}^{n\times n}\;:\; \|A^*A+AA^*\|-\|A^*A-AA^*\|=\|A^*A\|\}\;. $$ Can you find any necessary or sufficient conditions for a matrix to be in this class? Special cases, $A\in\mathbb{R}^{n\times n}$ for a fixed $n$ are welcome.","Let $A^*$ denote the complex conjugate transpose of the matrix $A$ and $\|\cdot\|=\|\cdot\|_2$ the norm induced by the Euclidean vector norm. If $$ \|A^*A+AA^*\|-\|A^*A-AA^*\|=\|A^*A\| $$ what can be said about $A$? Note that $A$ does not have to be normal, in fact if $A$ is normal and satisfies the above condition, then $A=0$, as noted in the comments below. My first hypothesis, after lots of numerical experiments was that $\det(A)=0$, since almost all random matrices with the above property fulfilled had zero determinant, but then I found $$ A=\begin{pmatrix}-2&2&0\\0&0&4\\-2&-2&0\end{pmatrix} $$ for which $\det(A)=-32$. This shattered my hypothesis, but the question remains. What can be said about such matrices? EDIT: I'm putting a small bounty on this question. Let's define a class $\mathcal{A}$ of complex-valued matrices such that $$ \mathcal{A}=\{A\in\mathbb{C}^{n\times n}\;:\; \|A^*A+AA^*\|-\|A^*A-AA^*\|=\|A^*A\|\}\;. $$ Can you find any necessary or sufficient conditions for a matrix to be in this class? Special cases, $A\in\mathbb{R}^{n\times n}$ for a fixed $n$ are welcome.",,"['linear-algebra', 'matrices', 'functional-analysis']"
55,"What is a ""supplementary subspace""?","What is a ""supplementary subspace""?",,"Let $Q$ be a quadratic form of vector space $V$ over a field $k$ with characteristic $\neq 2 $, $V^{0}$ be its orthogonal complement. If $U$ is a supplementary subspace of $V^0$ in $V$, then $V = U \oplus V^0$ . What does supplementaty subspace mean in above proposition? This is s a proposition in Serre's A Course in Arithmetic , and it only says that it's clear to prove, but never defines what a supplementary subspace is.","Let $Q$ be a quadratic form of vector space $V$ over a field $k$ with characteristic $\neq 2 $, $V^{0}$ be its orthogonal complement. If $U$ is a supplementary subspace of $V^0$ in $V$, then $V = U \oplus V^0$ . What does supplementaty subspace mean in above proposition? This is s a proposition in Serre's A Course in Arithmetic , and it only says that it's clear to prove, but never defines what a supplementary subspace is.",,"['linear-algebra', 'quadratic-forms']"
56,"Geometrical or Physical significance (interpretation) of the inner-product $\langle A,B \rangle := Trace (AB^t)$ over $M_n(\mathbb R)$",Geometrical or Physical significance (interpretation) of the inner-product  over,"\langle A,B \rangle := Trace (AB^t) M_n(\mathbb R)","$\langle A,B \rangle := Trace (AB^t)$ is an inner product over the vector space $M_n(\mathbb R)$ of all real matrices of size $n$ , I would like to know whether this inner-product has any Geometrical or Physical significance (interpretation) or not ? Please shed some light . Thanks in advance","$\langle A,B \rangle := Trace (AB^t)$ is an inner product over the vector space $M_n(\mathbb R)$ of all real matrices of size $n$ , I would like to know whether this inner-product has any Geometrical or Physical significance (interpretation) or not ? Please shed some light . Thanks in advance",,"['linear-algebra', 'geometry']"
57,Induction proof about entries of powers of strictly upper triangular matrix,Induction proof about entries of powers of strictly upper triangular matrix,,"Let $A$ be a $n \times n$ strictly upper triangular matrix. Prove that, for $k \ge1$, the matrix $A^k$ has the property that $(A^k)_{i,j} = 0$ for all $(i,j)$ with $j-i < k$.  Also, show that $A^n = 0$ for any $n \times n$ strictly upper triangular matrix. I tried to split the sum into two parts for the $k + 1$th term, but now I'm stuck with the induction proof. Anything would be helpful, thanks.","Let $A$ be a $n \times n$ strictly upper triangular matrix. Prove that, for $k \ge1$, the matrix $A^k$ has the property that $(A^k)_{i,j} = 0$ for all $(i,j)$ with $j-i < k$.  Also, show that $A^n = 0$ for any $n \times n$ strictly upper triangular matrix. I tried to split the sum into two parts for the $k + 1$th term, but now I'm stuck with the induction proof. Anything would be helpful, thanks.",,"['linear-algebra', 'matrices', 'summation', 'induction']"
58,Is $\text{Trace}(e^{XA+A^TX})$ a convex function of $X$?,Is  a convex function of ?,\text{Trace}(e^{XA+A^TX}) X,"Is $\text{Trace}(e^{XA+A^TX})$ a convex function of $X$? $X$ is diagonal and positive definite, $A$ is symmetric negative definite definite. And by the way, what is the best way to solve a problem of the form: $\text{min}.$ $\text{Trace}(e^{XA+A^TX})$ s.t. $X$ diagonal, $X_{ii}>0$ and $1^T X 1<x$ where $X$ is the variable and $A$ is symmetric negative definite, $x>0$, $x \in R$","Is $\text{Trace}(e^{XA+A^TX})$ a convex function of $X$? $X$ is diagonal and positive definite, $A$ is symmetric negative definite definite. And by the way, what is the best way to solve a problem of the form: $\text{min}.$ $\text{Trace}(e^{XA+A^TX})$ s.t. $X$ diagonal, $X_{ii}>0$ and $1^T X 1<x$ where $X$ is the variable and $A$ is symmetric negative definite, $x>0$, $x \in R$",,"['linear-algebra', 'matrices', 'optimization', 'convex-optimization']"
59,"Let $M(n,\mathbb R)$ denote set of all $n\times n$ matrices over $\mathbb R$.Which are true:",Let  denote set of all  matrices over .Which are true:,"M(n,\mathbb R) n\times n \mathbb R","Let $M(n,\mathbb R)$ denote set of all $n\times n$ matrices over $\mathbb R$.Which are true: 1.If $A\in M(2,\mathbb R)$ is nilpotent and non-zero ,then there exists a matrix $B\in M(2,\mathbb R)$ such that $B^2=A$ 2.If $A\in M(n,\mathbb R)$ is symmetric  and positive definite  ,then there exists a symmetric matrix $B\in M(n,\mathbb R)$ such that $B^2=A$ 3.If $A\in M(n,\mathbb R)$ is symmetric  ,then there exists a symmetric matrix $B\in M(n,\mathbb R)$ such that $B^3=A$ I dont know how to approach these?","Let $M(n,\mathbb R)$ denote set of all $n\times n$ matrices over $\mathbb R$.Which are true: 1.If $A\in M(2,\mathbb R)$ is nilpotent and non-zero ,then there exists a matrix $B\in M(2,\mathbb R)$ such that $B^2=A$ 2.If $A\in M(n,\mathbb R)$ is symmetric  and positive definite  ,then there exists a symmetric matrix $B\in M(n,\mathbb R)$ such that $B^2=A$ 3.If $A\in M(n,\mathbb R)$ is symmetric  ,then there exists a symmetric matrix $B\in M(n,\mathbb R)$ such that $B^3=A$ I dont know how to approach these?",,['linear-algebra']
60,Prove spatial velocity identity - screw theory,Prove spatial velocity identity - screw theory,,"This question involves a proof regarding coordinate transformations of velocities of screw motions. This comes from ""A Mathematical Introduction to Robotic Manipulation"" (the text is available for free here: http://www.cds.caltech.edu/~murray/books/MLS/pdf/mls94-complete.pdf ) The identity is shown below, and is on pg. 59 of the above link. Here's the image: I know how to get $V_{a,c}^c$ (with respect to frame c). That comes directly from the chain rule, switching the order of $\dot{g}$ and $g^{-1}$. But I don't know how to get $V_{a,c}^b$ (with respect to frame b), i.e. the left-hand side. Any help much appreciated.","This question involves a proof regarding coordinate transformations of velocities of screw motions. This comes from ""A Mathematical Introduction to Robotic Manipulation"" (the text is available for free here: http://www.cds.caltech.edu/~murray/books/MLS/pdf/mls94-complete.pdf ) The identity is shown below, and is on pg. 59 of the above link. Here's the image: I know how to get $V_{a,c}^c$ (with respect to frame c). That comes directly from the chain rule, switching the order of $\dot{g}$ and $g^{-1}$. But I don't know how to get $V_{a,c}^b$ (with respect to frame b), i.e. the left-hand side. Any help much appreciated.",,"['linear-algebra', 'proof-writing', 'transformation']"
61,For what $X$ do we have $\|XA\|_{1} \leq 1$ for a given $\|A\|_{1} \leq 1$.,For what  do we have  for a given .,X \|XA\|_{1} \leq 1 \|A\|_{1} \leq 1,"All matrices are real. By $\| \cdot \|_1$ I mean a matrix norm induced by the vector norm $L_1$, i.e. the max of the column sums of absolute values. The matrix $A$ is given and we have $\|A\|_{1} \leq 1$. Is there a nice characterization of the set of matrices $X$ for which we have $\|XA\|_{1} \leq 1$? I see two sufficient conditions: $\|X\|_{1} \leq 1$, (edit) which can be sharpened to $\|X\|_{1} \leq \frac{1}{\|A\|_{1}}$ X is an oblique projection on some space that includes the column space of $A$. But neither of them is necessary. Is there a nice characterization of the set of matrices that meet this condition (like they have common form or something like that)?","All matrices are real. By $\| \cdot \|_1$ I mean a matrix norm induced by the vector norm $L_1$, i.e. the max of the column sums of absolute values. The matrix $A$ is given and we have $\|A\|_{1} \leq 1$. Is there a nice characterization of the set of matrices $X$ for which we have $\|XA\|_{1} \leq 1$? I see two sufficient conditions: $\|X\|_{1} \leq 1$, (edit) which can be sharpened to $\|X\|_{1} \leq \frac{1}{\|A\|_{1}}$ X is an oblique projection on some space that includes the column space of $A$. But neither of them is necessary. Is there a nice characterization of the set of matrices that meet this condition (like they have common form or something like that)?",,"['linear-algebra', 'matrices', 'convex-analysis', 'normed-spaces']"
62,How to find $(Ker(A^{*}))^{\perp}$,How to find,(Ker(A^{*}))^{\perp},"Let $$A = \begin{pmatrix} 1 & 1 & 1 & 1 \\ 1 & 0 & -2 & -1 \\ 1 & 2 & 4 & 3 \end{pmatrix}$$ Find a basis for $(Ker(A^{*}))^{\perp}$. Find vectors $b_i$ such that $ y \perp b$ implies $Ax = y$ is solvable. I know that $A^{*}$ is simply $A^T$ since $A$ is a real matrix.  I can find a basis for $Ker(A^{*})$ easily.  How does the $\perp$ operator change things?  Also, how do I find vectors $b_i$ such that $ y \perp b$ implies $Ax = y$ is solvable?","Let $$A = \begin{pmatrix} 1 & 1 & 1 & 1 \\ 1 & 0 & -2 & -1 \\ 1 & 2 & 4 & 3 \end{pmatrix}$$ Find a basis for $(Ker(A^{*}))^{\perp}$. Find vectors $b_i$ such that $ y \perp b$ implies $Ax = y$ is solvable. I know that $A^{*}$ is simply $A^T$ since $A$ is a real matrix.  I can find a basis for $Ker(A^{*})$ easily.  How does the $\perp$ operator change things?  Also, how do I find vectors $b_i$ such that $ y \perp b$ implies $Ax = y$ is solvable?",,"['linear-algebra', 'matrices']"
63,Recursion relation of fourth order Runge-Kutta method applied on system,Recursion relation of fourth order Runge-Kutta method applied on system,,"I'm trying to apply the Gauss-Legendre method of fourth order (as Runge-Kutta method) on the following system of equations $$\left\{ \begin{matrix}               \dot{a} =& -b \\                \dot{b} =& a \\          \end{matrix}  \right.$$ and I want the result to be a recurrence relation, i.e. of the form $$\begin{pmatrix} a_{n+1} \\ b_{n+1} \end{pmatrix}= \begin{pmatrix} d_1 & d_2 \\ d_3 & d_4 \end{pmatrix} \cdot \begin{pmatrix} a_{n} \\ b_{n} \end{pmatrix}$$ where I still have to find the $d$ elements. The problem I'm struggling with is that the $c$-elements of the butcher tableau (from the Gauss-Legendre method of order 4), are $c_1 = \frac{1}{2}-\frac{\sqrt{3}}{6}$ and $c_2=\frac{1}{2}+\frac{\sqrt{3}}{6}$, which will result in a different discretization step from the $a_{n+1} = a_n + h$ and $b_{n+1} = b_n + h$ of the recurrence relation. Any tips and help are welcome! Edit: the butcher tableau for the 2-steps Gauss-Legendre method (fourth order) is given by \begin{array}{c|ccc} \frac{1}{2}-\frac{\sqrt{3}}{6} & \frac{1}{4} & \frac{1}{4}-\frac{\sqrt{3}}{6}\\ \frac{1}{2}+\frac{\sqrt{3}}{6} & \frac{1}{4}+\frac{\sqrt{3}}{6} & \frac{1}{4} \\ \hline & \frac{1}{2} & \frac{1}{2} \end{array} Added: these are the equations I've got at the moment $$ a_{n+1} = a_n + \frac{1}{2}hk_1 + \frac{1}{2}hk_2 \\ \, \, k_1 = f(t_n + (\frac{1}{2} - \frac{\sqrt{3}}{6})h, a_n + \frac{1}{4}k_1 + (\frac{1}{4}-\frac{\sqrt{3}}{6})k_2) \\ \, \, k_2 = f(t_n + (\frac{1}{2} + \frac{\sqrt{3}}{6})h, a_n + (\frac{1}{4}+\frac{\sqrt{3}}{6})k_1 + \frac{1}{4}k_2) $$ The same holds for the $b$ part (with $g$ instead of $f$).","I'm trying to apply the Gauss-Legendre method of fourth order (as Runge-Kutta method) on the following system of equations $$\left\{ \begin{matrix}               \dot{a} =& -b \\                \dot{b} =& a \\          \end{matrix}  \right.$$ and I want the result to be a recurrence relation, i.e. of the form $$\begin{pmatrix} a_{n+1} \\ b_{n+1} \end{pmatrix}= \begin{pmatrix} d_1 & d_2 \\ d_3 & d_4 \end{pmatrix} \cdot \begin{pmatrix} a_{n} \\ b_{n} \end{pmatrix}$$ where I still have to find the $d$ elements. The problem I'm struggling with is that the $c$-elements of the butcher tableau (from the Gauss-Legendre method of order 4), are $c_1 = \frac{1}{2}-\frac{\sqrt{3}}{6}$ and $c_2=\frac{1}{2}+\frac{\sqrt{3}}{6}$, which will result in a different discretization step from the $a_{n+1} = a_n + h$ and $b_{n+1} = b_n + h$ of the recurrence relation. Any tips and help are welcome! Edit: the butcher tableau for the 2-steps Gauss-Legendre method (fourth order) is given by \begin{array}{c|ccc} \frac{1}{2}-\frac{\sqrt{3}}{6} & \frac{1}{4} & \frac{1}{4}-\frac{\sqrt{3}}{6}\\ \frac{1}{2}+\frac{\sqrt{3}}{6} & \frac{1}{4}+\frac{\sqrt{3}}{6} & \frac{1}{4} \\ \hline & \frac{1}{2} & \frac{1}{2} \end{array} Added: these are the equations I've got at the moment $$ a_{n+1} = a_n + \frac{1}{2}hk_1 + \frac{1}{2}hk_2 \\ \, \, k_1 = f(t_n + (\frac{1}{2} - \frac{\sqrt{3}}{6})h, a_n + \frac{1}{4}k_1 + (\frac{1}{4}-\frac{\sqrt{3}}{6})k_2) \\ \, \, k_2 = f(t_n + (\frac{1}{2} + \frac{\sqrt{3}}{6})h, a_n + (\frac{1}{4}+\frac{\sqrt{3}}{6})k_1 + \frac{1}{4}k_2) $$ The same holds for the $b$ part (with $g$ instead of $f$).",,"['linear-algebra', 'ordinary-differential-equations', 'discrete-mathematics', 'numerical-methods', 'recurrence-relations']"
64,Finding the eigenvalues of a given Markov matrix,Finding the eigenvalues of a given Markov matrix,,"Let $$A = \begin{pmatrix} 0.6 & 0.1 & 0.1\\  0.1 & 0.8 & 0.2\\ 0.3 & 0.1 & 0.7  \end{pmatrix}$$ I want to find the eigenvalues of this matrix. Because this is a markov matrix, I know that $\lambda_1 = 1$. Furthermore, I know that the remaining eigenvalues $\lambda_2$ and $\lambda_3$ should be less than one (their magnitudes should be less than one, at least) because all the entries in $A$ are positive. I found the characteristic polynomial, but it is an ugly polynomial. Is there a better/easier way to determine the remaining two eigenvalues? Maybe by using the information I already have that $\lambda_1 = 1$?","Let $$A = \begin{pmatrix} 0.6 & 0.1 & 0.1\\  0.1 & 0.8 & 0.2\\ 0.3 & 0.1 & 0.7  \end{pmatrix}$$ I want to find the eigenvalues of this matrix. Because this is a markov matrix, I know that $\lambda_1 = 1$. Furthermore, I know that the remaining eigenvalues $\lambda_2$ and $\lambda_3$ should be less than one (their magnitudes should be less than one, at least) because all the entries in $A$ are positive. I found the characteristic polynomial, but it is an ugly polynomial. Is there a better/easier way to determine the remaining two eigenvalues? Maybe by using the information I already have that $\lambda_1 = 1$?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
65,Inequality involving traces and matrix inversions,Inequality involving traces and matrix inversions,,"The following question kept me wondering for some weeks: Given the symmetric matrices $A,B,C\in\mathbb{R}^{n\times n}$ where $A$ and $C$ are positive definite (hence invertible), and $B$ is positive semidefinite (hence not necessarily invertible) with $\operatorname{trace}(B)\neq 0$, prove that $$\operatorname{trace}\left\{C^{-1/2}BC^{-1/2}(A^{-1}+C^{-1/2}BC^{-1/2})^{-1}(A+\frac{n}{\operatorname{trace}(B)}C)^{-1}\right\}\geq \operatorname{trace}\left\{(A+\frac{n}{\operatorname{trace}(B)}C)^{-2}A    \right\}.$$ If it would help, one can also consider the simpler version with $C=I$: prove that $$\operatorname{trace}\left\{B(A^{-1}+B)^{-1}(A+\frac{n}{\operatorname{trace}(B)}I)^{-1}\right\}\geq \operatorname{trace}\left\{(A+\frac{n}{\operatorname{trace}(B)}I)^{-2}A    \right\}.$$ Please note that the matrix inversion lemma is not applicable at first as $B$ is positive semidefinite. Although I'm not sure, it seems like $\operatorname{trace}(B)=\operatorname{trace}\{(\operatorname{trace}(B)/n)I\}$ should be utilized in some way, and it may also help to interpret the trace operator in terms of the Frobenius norm. I would highly appreciate if anyone can provide some help or suggestions on this.","The following question kept me wondering for some weeks: Given the symmetric matrices $A,B,C\in\mathbb{R}^{n\times n}$ where $A$ and $C$ are positive definite (hence invertible), and $B$ is positive semidefinite (hence not necessarily invertible) with $\operatorname{trace}(B)\neq 0$, prove that $$\operatorname{trace}\left\{C^{-1/2}BC^{-1/2}(A^{-1}+C^{-1/2}BC^{-1/2})^{-1}(A+\frac{n}{\operatorname{trace}(B)}C)^{-1}\right\}\geq \operatorname{trace}\left\{(A+\frac{n}{\operatorname{trace}(B)}C)^{-2}A    \right\}.$$ If it would help, one can also consider the simpler version with $C=I$: prove that $$\operatorname{trace}\left\{B(A^{-1}+B)^{-1}(A+\frac{n}{\operatorname{trace}(B)}I)^{-1}\right\}\geq \operatorname{trace}\left\{(A+\frac{n}{\operatorname{trace}(B)}I)^{-2}A    \right\}.$$ Please note that the matrix inversion lemma is not applicable at first as $B$ is positive semidefinite. Although I'm not sure, it seems like $\operatorname{trace}(B)=\operatorname{trace}\{(\operatorname{trace}(B)/n)I\}$ should be utilized in some way, and it may also help to interpret the trace operator in terms of the Frobenius norm. I would highly appreciate if anyone can provide some help or suggestions on this.",,"['linear-algebra', 'matrices', 'inequality']"
66,R is uncountable as a Q-vector space,R is uncountable as a Q-vector space,,"I just wanted to ask whether my proof is correct: Suppose instead that $\mathbb{R}$ had a countable $\mathbb{Q}$-basis, say $v_1,v_2,v_3,\ldots$ (possibly finite). Since $\mathbb{Q}$ is countable, $\,\text{span}(v_1,\ldots,v_k)$ is countable for each $k$ (possibly finitely many). We have $\mathbb{R}=\bigcup_{k}\text{span}(v_1,\ldots,v_k)$ which is a countable union of countable sets. It follows that $\mathbb{R}$ is countable. Contradiction. I would be very grateful for any feedback. Best wishes!","I just wanted to ask whether my proof is correct: Suppose instead that $\mathbb{R}$ had a countable $\mathbb{Q}$-basis, say $v_1,v_2,v_3,\ldots$ (possibly finite). Since $\mathbb{Q}$ is countable, $\,\text{span}(v_1,\ldots,v_k)$ is countable for each $k$ (possibly finitely many). We have $\mathbb{R}=\bigcup_{k}\text{span}(v_1,\ldots,v_k)$ which is a countable union of countable sets. It follows that $\mathbb{R}$ is countable. Contradiction. I would be very grateful for any feedback. Best wishes!",,['linear-algebra']
67,Solve a system of linear equations,Solve a system of linear equations,,"$\newcommand{\Sp}{\phantom{0}}$There is a system of linear equations: \begin{alignat*}{4}  &x -  &&y - 2&&z = &&1, \\ 2&x + 3&&y -  &&z =-&&2. \end{alignat*} I create the matrix of the system: $$   \left[\begin{array}{rrr|r}     1 & -1 & -2 & 1 \\     2 & 3 & -1 & -2   \end{array}\right] $$  then with GEM, $$ \left[\begin{array}{rrr|r}     1 & -1 & -2 & 1 \\     0 & 5 & 3 & -4   \end{array}\right] $$ I don't know how to proceed after that? I have found the correction of this exercise but I still don't understand the way to solve it. Can someone help me please?","$\newcommand{\Sp}{\phantom{0}}$There is a system of linear equations: \begin{alignat*}{4}  &x -  &&y - 2&&z = &&1, \\ 2&x + 3&&y -  &&z =-&&2. \end{alignat*} I create the matrix of the system: $$   \left[\begin{array}{rrr|r}     1 & -1 & -2 & 1 \\     2 & 3 & -1 & -2   \end{array}\right] $$  then with GEM, $$ \left[\begin{array}{rrr|r}     1 & -1 & -2 & 1 \\     0 & 5 & 3 & -4   \end{array}\right] $$ I don't know how to proceed after that? I have found the correction of this exercise but I still don't understand the way to solve it. Can someone help me please?",,"['linear-algebra', 'matrices', 'systems-of-equations']"
68,Determine an explicit expression for $f$.,Determine an explicit expression for .,f,"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ a continuous function, bounded such that the space $\mathrm{lin}\{f_k(x)=f(x+k)∣k ∈\mathbb{N}\}$ is finite-dimensional. Determine an explicit expression for $f$ . The sequence $(f_k)$ generates a vector space of finite dimension means that the sequence $(f_k)$ admits a non-zero minimal polynomial. Let $P_f(X) = \sum_{k=0}^n p_k X^k$ this polynomial. We can then construct $f$ by induction: first we construct $f$ continuous (and therefore bounded) on $[0, k]$ such that $\sum_{k = 0}^d p_k f(k)=0$ , then $f$ extends to $\mathbb{R}$ through the relation $$\forall x \in \mathbb{R} \sum_{k=0}^d p_kf (x+k)=0.$$ How can I continue ? NB: Writing this exercise I've realized that he was here . Unfortunately the answer doesn't give us a explicit expression for $f$ . Then please do not close my question as a duplicate. EDIT: This function $$f_\theta : n + x \mapsto (\sin(n\theta)-\sin((n-1)\theta)) x - \sin(n\theta)$$ for $n \in \mathbb{Z}, 0 \leq x < 1$ works.","Let a continuous function, bounded such that the space is finite-dimensional. Determine an explicit expression for . The sequence generates a vector space of finite dimension means that the sequence admits a non-zero minimal polynomial. Let this polynomial. We can then construct by induction: first we construct continuous (and therefore bounded) on such that , then extends to through the relation How can I continue ? NB: Writing this exercise I've realized that he was here . Unfortunately the answer doesn't give us a explicit expression for . Then please do not close my question as a duplicate. EDIT: This function for works.","f:\mathbb{R}\rightarrow\mathbb{R} \mathrm{lin}\{f_k(x)=f(x+k)∣k ∈\mathbb{N}\} f (f_k) (f_k) P_f(X) = \sum_{k=0}^n p_k X^k f f [0, k] \sum_{k = 0}^d p_k f(k)=0 f \mathbb{R} \forall x \in \mathbb{R} \sum_{k=0}^d p_kf (x+k)=0. f f_\theta : n + x \mapsto (\sin(n\theta)-\sin((n-1)\theta)) x - \sin(n\theta) n \in \mathbb{Z}, 0 \leq x < 1",['linear-algebra']
69,Coordinate-free proof of determinant of transpose,Coordinate-free proof of determinant of transpose,,"I'm interested in a coordinate-free proof of the statement $\mathrm{det}(A) = \mathrm{det}(A^T).$ Let $V$ be a finite-dimensional vector space over a field $K$, and let $f : V \rightarrow V$ be an endomorphism. I define $\mathrm{det}(f)$ by $\mathrm{det}(f) \cdot \varphi = \varphi \circ f$, where $\varphi : V \times ... \times V \rightarrow K$ is any alternating function and $(\varphi \circ f)(v_1,...,v_n) := \varphi(f(v_1),...,f(v_n)).$ I define the transpose by $$f^* : V^* \rightarrow V^*, \; \psi \mapsto \psi \circ f.$$ I would like to prove that $\mathrm{det}(f) = \mathrm{det}(f^*)$ using these definitions but haven't been able to find a proof that doesn't involve choosing a basis. Thanks","I'm interested in a coordinate-free proof of the statement $\mathrm{det}(A) = \mathrm{det}(A^T).$ Let $V$ be a finite-dimensional vector space over a field $K$, and let $f : V \rightarrow V$ be an endomorphism. I define $\mathrm{det}(f)$ by $\mathrm{det}(f) \cdot \varphi = \varphi \circ f$, where $\varphi : V \times ... \times V \rightarrow K$ is any alternating function and $(\varphi \circ f)(v_1,...,v_n) := \varphi(f(v_1),...,f(v_n)).$ I define the transpose by $$f^* : V^* \rightarrow V^*, \; \psi \mapsto \psi \circ f.$$ I would like to prove that $\mathrm{det}(f) = \mathrm{det}(f^*)$ using these definitions but haven't been able to find a proof that doesn't involve choosing a basis. Thanks",,"['linear-algebra', 'determinant']"
70,"Show that if polynomials $f,g$ have no common factor in $K[x,y]$, then they have only finitely many common roots [duplicate]","Show that if polynomials  have no common factor in , then they have only finitely many common roots [duplicate]","f,g K[x,y]","This question already has an answer here : Irreducible polynomials and affine variety (1 answer) Closed last year . I have a homework problem where I have to show that if $f$ and $g$ are polynomials with no common factor in $K[x, y]$, where $K$ is a field, there are only finitely many elements $(a, b)\in K^2$ such that $f(a, b)=g(a, b)=0$. My approach involves looking at $f$ and $g$ as polynomials in $y$ whose coefficients are polynomials in $x$. Then I can write the Sylvester matrix of $f$ and $g$, which is a matrix whose entries are polynomials in $x$. I've shown that if $f$ and $g$ have a common root $(a, b)$, then this matrix's determinant evaluated at $x=a$ would be $0$. Thus, if this determinant, viewed as a polynomial over $x$ is not the zero polynomial, it only has finitely many roots, and thus there are only finitely many values of $x$ that can be in a common root, and I'd be done. Now I have to deal with the case where this determinant is identically the zero polynomial. If so, that means, that for every $a\in K$, $f(a, y)$ and $g(a, y)\in K[y]$ have a common root. I'm not quite sure how to get from this to my desired contradiction... Something that may or may not be helpful is that I was able to show, from Gauss' Lemma and the fact that $K(x)[y]$ is a Principal Ideal Domain ($K(x)$ is the field of rational functions of $x$), that if there's a $b\in K$ such that both $f(x, b)$ and $g(x, b)\in K[x]$ are both the zero polynomial, then $f$ and $g$ have a common factor in the Unique Factorization Domain $K[x, y]$, a contradiction. Does this result help me prove that if the Sylvester determinant is zero, then $f$ and $g$ have a common factor? I know one way of doing this is to prove that the Sylvester Determinant is equal to the resultant, but that proof seems rather long, requiring me to consider the algebraic closure of a field of fractions, and I don't think I need the full force of the result...is there an easier way to prove the weaker result that if the determinant is $0$, then $f$ and $g$ have a common root/factor. Please don't give me full solutions; I just want some hints in the right direction so I can finish the problem on my own.","This question already has an answer here : Irreducible polynomials and affine variety (1 answer) Closed last year . I have a homework problem where I have to show that if $f$ and $g$ are polynomials with no common factor in $K[x, y]$, where $K$ is a field, there are only finitely many elements $(a, b)\in K^2$ such that $f(a, b)=g(a, b)=0$. My approach involves looking at $f$ and $g$ as polynomials in $y$ whose coefficients are polynomials in $x$. Then I can write the Sylvester matrix of $f$ and $g$, which is a matrix whose entries are polynomials in $x$. I've shown that if $f$ and $g$ have a common root $(a, b)$, then this matrix's determinant evaluated at $x=a$ would be $0$. Thus, if this determinant, viewed as a polynomial over $x$ is not the zero polynomial, it only has finitely many roots, and thus there are only finitely many values of $x$ that can be in a common root, and I'd be done. Now I have to deal with the case where this determinant is identically the zero polynomial. If so, that means, that for every $a\in K$, $f(a, y)$ and $g(a, y)\in K[y]$ have a common root. I'm not quite sure how to get from this to my desired contradiction... Something that may or may not be helpful is that I was able to show, from Gauss' Lemma and the fact that $K(x)[y]$ is a Principal Ideal Domain ($K(x)$ is the field of rational functions of $x$), that if there's a $b\in K$ such that both $f(x, b)$ and $g(x, b)\in K[x]$ are both the zero polynomial, then $f$ and $g$ have a common factor in the Unique Factorization Domain $K[x, y]$, a contradiction. Does this result help me prove that if the Sylvester determinant is zero, then $f$ and $g$ have a common factor? I know one way of doing this is to prove that the Sylvester Determinant is equal to the resultant, but that proof seems rather long, requiring me to consider the algebraic closure of a field of fractions, and I don't think I need the full force of the result...is there an easier way to prove the weaker result that if the determinant is $0$, then $f$ and $g$ have a common root/factor. Please don't give me full solutions; I just want some hints in the right direction so I can finish the problem on my own.",,"['linear-algebra', 'abstract-algebra']"
71,Closed conjugacy classes in $M_n(k)$,Closed conjugacy classes in,M_n(k),"Let $k$ be an algebraically closed field, $n$ a positive integer, and consider the action of $\mathrm{GL}_n(k)$ on $M_n(k)$ by conjugation. My professor tells me that semisimple conjugacy classes are closed, and his argument is as follows.  Fix $f$ a monic polynomial of degree $n$, and consider the set of matrices $A_f$ whose characteristic polynomial is $f$.  Then $A_f$ is closed, a finite union of orbits (by considering Jordan forms), and the orbit of minimal dimension is the unique semisimple conjugacy class with characteristic polynomial $f$.  Since orbits of minimal dimension are closed, this semisimple orbit is closed. A few questions I have to understand this proof: Why do we need to note that $A_f$ is closed?  It is true, but doesn't the argument work without this fact? How can I see that the semisimple orbit in $A_f$ has minimal dimension? Consider the semisimple class $S_f$ of $2\times 2$ matrices defined by the characteristic polynomial $f(x)=(x-1)(x-2)$.  Since $I_2$ satisfies this polynomial, wouldn't it be in the closure of $S_f$?  That is, doesn't $I_2$ satisfy any polynomial equation that the elements of $S_f$ satisfy, and hence $I_2$ is in the Zariski closure?  However, this can't be right because then $S_f$ is not closed (because $I_2\notin S_f$).  What is wrong with my reasoning here?","Let $k$ be an algebraically closed field, $n$ a positive integer, and consider the action of $\mathrm{GL}_n(k)$ on $M_n(k)$ by conjugation. My professor tells me that semisimple conjugacy classes are closed, and his argument is as follows.  Fix $f$ a monic polynomial of degree $n$, and consider the set of matrices $A_f$ whose characteristic polynomial is $f$.  Then $A_f$ is closed, a finite union of orbits (by considering Jordan forms), and the orbit of minimal dimension is the unique semisimple conjugacy class with characteristic polynomial $f$.  Since orbits of minimal dimension are closed, this semisimple orbit is closed. A few questions I have to understand this proof: Why do we need to note that $A_f$ is closed?  It is true, but doesn't the argument work without this fact? How can I see that the semisimple orbit in $A_f$ has minimal dimension? Consider the semisimple class $S_f$ of $2\times 2$ matrices defined by the characteristic polynomial $f(x)=(x-1)(x-2)$.  Since $I_2$ satisfies this polynomial, wouldn't it be in the closure of $S_f$?  That is, doesn't $I_2$ satisfy any polynomial equation that the elements of $S_f$ satisfy, and hence $I_2$ is in the Zariski closure?  However, this can't be right because then $S_f$ is not closed (because $I_2\notin S_f$).  What is wrong with my reasoning here?",,"['linear-algebra', 'group-theory', 'matrices', 'algebraic-geometry', 'algebraic-groups']"
72,Is $\sqrt{1+x^2}$ matrix monotone?,Is  matrix monotone?,\sqrt{1+x^2},"A function $f(x)$ is matrix monotone if $f(A)-f(B)$ is positive semidefinite whenever $A-B$ is positive semidefinite for positive semidefinite matrices $A, B$. Is $\sqrt{1+x^2}$ matrix monotone?","A function $f(x)$ is matrix monotone if $f(A)-f(B)$ is positive semidefinite whenever $A-B$ is positive semidefinite for positive semidefinite matrices $A, B$. Is $\sqrt{1+x^2}$ matrix monotone?",,"['linear-algebra', 'matrices']"
73,Exterior power of a space of maps $(\mathbb{K}^T)$,Exterior power of a space of maps,(\mathbb{K}^T),"We are given a set $T \neq \emptyset, \ \ p \ge 1, \ \ p_i : T \rightarrow \mathbb{K}$ Could you help me prove that if $ \phi: (\mathbb{K}^T)^p \ni (f_1, ..., f_p) \rightarrow \rho \in \mathbb{K}^{T^p}$ where $\rho: T^p \ni (x_1, ..., x_p) \rightarrow det [f_i(x_j)]_{i,j = 1, ... p} \in \mathbb{K}$ then $(\mathbb{K}^{T^p}, \phi)$ is the $p$-th exterior power of $\mathbb{K}^T$? I know that $\phi$ is $p$-linear and anti-symmetric, because $\det$ is $p$-linear and anti-symmetric, but I have problems finding the unique linear map which makes the proper diagram commute. Could you help me with that? Thank you.","We are given a set $T \neq \emptyset, \ \ p \ge 1, \ \ p_i : T \rightarrow \mathbb{K}$ Could you help me prove that if $ \phi: (\mathbb{K}^T)^p \ni (f_1, ..., f_p) \rightarrow \rho \in \mathbb{K}^{T^p}$ where $\rho: T^p \ni (x_1, ..., x_p) \rightarrow det [f_i(x_j)]_{i,j = 1, ... p} \in \mathbb{K}$ then $(\mathbb{K}^{T^p}, \phi)$ is the $p$-th exterior power of $\mathbb{K}^T$? I know that $\phi$ is $p$-linear and anti-symmetric, because $\det$ is $p$-linear and anti-symmetric, but I have problems finding the unique linear map which makes the proper diagram commute. Could you help me with that? Thank you.",,"['linear-algebra', 'analysis', 'exterior-algebra']"
74,Span and Dimension: A subspace,Span and Dimension: A subspace,,"If $A$ is finite set of linearly independent vectors then the dimension of the subspace spanned by $A$  is equal to the number of vectors in $A$. This is obviously true. Since $A$ is a finite set of linearly independent vectors and spans a subspace, $A$ is a basis for that subspace spanned by $A$ and thus by definition the dimension of a vector space is equal to the cardinality of any basis. I would help with writing the above argument in a concise, precise manner with mathematical notation and other shorthand Secondly in general what tips and/or advice you could give in general to make my arguments and proofs as efficient (time-wise) as possible.","If $A$ is finite set of linearly independent vectors then the dimension of the subspace spanned by $A$  is equal to the number of vectors in $A$. This is obviously true. Since $A$ is a finite set of linearly independent vectors and spans a subspace, $A$ is a basis for that subspace spanned by $A$ and thus by definition the dimension of a vector space is equal to the cardinality of any basis. I would help with writing the above argument in a concise, precise manner with mathematical notation and other shorthand Secondly in general what tips and/or advice you could give in general to make my arguments and proofs as efficient (time-wise) as possible.",,"['linear-algebra', 'vector-spaces', 'proof-writing']"
75,Special Gram's inequality,Special Gram's inequality,,"For $1 \le s < k$ and $v_1$, $v_2,\dots,v_k$ vectors in $\mathbb{R}^n$, show that $$\det G(v_1, v_2,\dots,v_k) \le \det G(v_1,v_2,\dots,v_s)\det G(v_{s+1}, v_{s+2},\dots,v_k).$$ Here, $G(v_1, v_2,\dots,v_k)$ is a Gram matrix of vectors $v_1, v_2,\dots,v_k$ with the standard inner product. This is my homework problem, but I have no idea how to approach it. I found that this is one of the Gram's inequalities, but I can't figure out how it could be derived either from this: $$\det G(v_1, v_2,\dots,v_k) \ge 0,$$ or Hadamard's inequality: $$\det G(v_1, v_2,\dots,v_k) \le \prod_{i=1}^{k}\|x_i\|^2.$$ So any help would be much appreciated.","For $1 \le s < k$ and $v_1$, $v_2,\dots,v_k$ vectors in $\mathbb{R}^n$, show that $$\det G(v_1, v_2,\dots,v_k) \le \det G(v_1,v_2,\dots,v_s)\det G(v_{s+1}, v_{s+2},\dots,v_k).$$ Here, $G(v_1, v_2,\dots,v_k)$ is a Gram matrix of vectors $v_1, v_2,\dots,v_k$ with the standard inner product. This is my homework problem, but I have no idea how to approach it. I found that this is one of the Gram's inequalities, but I can't figure out how it could be derived either from this: $$\det G(v_1, v_2,\dots,v_k) \ge 0,$$ or Hadamard's inequality: $$\det G(v_1, v_2,\dots,v_k) \le \prod_{i=1}^{k}\|x_i\|^2.$$ So any help would be much appreciated.",,"['linear-algebra', 'matrices', 'determinant']"
76,matrices - traces and square root,matrices - traces and square root,,"I'm trying to show that Tr$\left(\sqrt{(\mathbf{PXP}^\top)}\right) \le \text{c Tr}\left(\mathbf{P}\sqrt{\mathbf{X}}\mathbf{P}^\top  \right)$ where Tr is the trace operator, $\mathbf{X}$ is symmetric positive semi-definite matrix, $\mathbf{P}$ is a non-symmetric low rank matrix, and $c$ is a positive constant. Is this statement always true and if so how can it be shown?","I'm trying to show that Tr$\left(\sqrt{(\mathbf{PXP}^\top)}\right) \le \text{c Tr}\left(\mathbf{P}\sqrt{\mathbf{X}}\mathbf{P}^\top  \right)$ where Tr is the trace operator, $\mathbf{X}$ is symmetric positive semi-definite matrix, $\mathbf{P}$ is a non-symmetric low rank matrix, and $c$ is a positive constant. Is this statement always true and if so how can it be shown?",,['linear-algebra']
77,Rank-$1$ update for Cholesky factor,Rank- update for Cholesky factor,1,"I have covariance matrix known to be $$K = \sum_{i=1}^Nx_ix_i^T$$ where the dimension of $x$ is big (like $50000$ ) so I don't want to really compute any outer-product to expand it as a full matrix. Also, I know this covariance matrix is sparse Since K is guaranteed to be positive-definite, there is a unique Cholesky decomposition : $$K = L^TL$$ Two questions: is there a way to update $L$ sequentially (update Cholesky factor after seeing each data point). is there approximation of Cholesky that keeps $L$ sparse or low-rank to save memory ?","I have covariance matrix known to be where the dimension of is big (like ) so I don't want to really compute any outer-product to expand it as a full matrix. Also, I know this covariance matrix is sparse Since K is guaranteed to be positive-definite, there is a unique Cholesky decomposition : Two questions: is there a way to update sequentially (update Cholesky factor after seeing each data point). is there approximation of Cholesky that keeps sparse or low-rank to save memory ?",K = \sum_{i=1}^Nx_ix_i^T x 50000 K = L^TL L L,"['linear-algebra', 'matrices', 'machine-learning', 'cholesky-decomposition']"
78,"About the Special Solutions Method in general, for solving Ax = 0 [GStrang, P140 3.2.1]","About the Special Solutions Method in general, for solving Ax = 0 [GStrang, P140 3.2.1]",,"● MIT Lec 7 Course Notes : Letting a different free variable equal 1 and setting the other free variables equal to zero gives us other vectors in the nullspace. ● P133: The nullspace consists of all combinations of the special solutions. ● 38:25 of Lecture 7 :  ""If I set the free variable to 0 and solve for the pivot variables, I'll get all 0s. No progress."" - Prof Strang I accept the following general method for finding the nullspace and will exemplify with Ex 3.2.1: $A = \begin{bmatrix} 1 & 2 & 2 & 4 & 6 \\ 1 & 2 & 3 & 6 & 9 \\ 0 & 0 & 1 & 2 & 3 \end{bmatrix} \implies RREF(A) = \begin{bmatrix} 1 & 2 & 0 & 0 & 0 \\  &  & 1 & 2 & 3 \\  &  & \mathbf{0} &  &  \end{bmatrix}$ Thus, $\mathbf{Ax = 0} \implies$ $x_1 = -2a_2 \\  x_2 = a_2 \\ x_3 = -2a_4 -3a_5 \\  x_4 = a_4 \\                 x_5 = a_5  $ $\implies \mathbf{x} = a_2\begin{bmatrix} -2  \\ 1 \\ 0 \\  0\\ 0 \\ \end{bmatrix} + a_4\begin{bmatrix} 0  \\ 0 \\ -2 \\  1 \\ 0 \\ \end{bmatrix} + a_5\begin{bmatrix} 0  \\ 0 \\ -3 \\  0 \\ 1 \\ \end{bmatrix}. $ $1.$ I can't pinpoint why, but I'm tentative about this method (in the grey box above): for each free variable (there're 3 here), this method sets $1$ for it and sets the other free variables to $0$. How and why does each free variable have one of these special solns? How and why does this function? $2.$ Since $a_2, a_4, a_5$ are free, each can be any scalar. Say I select $a_2 = a_4 = a_5 = 1$. Then  $x = (-2, 1, -5, 1, 1)$, which is one vector. But it's wrong to infer that $ \ker(A) = \{$ all scalar multiples of this one vector $\}$. How and Why? This question precedes rank, REF, $\mathbf{Ax = b}$, linear independence, span, basis, dimension, dimensions/theorems of the 4 subspaces,  Orthogonality, Determinants, eigenvalues and eigenvectors, and linear transformations. Please omit them from answers.","● MIT Lec 7 Course Notes : Letting a different free variable equal 1 and setting the other free variables equal to zero gives us other vectors in the nullspace. ● P133: The nullspace consists of all combinations of the special solutions. ● 38:25 of Lecture 7 :  ""If I set the free variable to 0 and solve for the pivot variables, I'll get all 0s. No progress."" - Prof Strang I accept the following general method for finding the nullspace and will exemplify with Ex 3.2.1: $A = \begin{bmatrix} 1 & 2 & 2 & 4 & 6 \\ 1 & 2 & 3 & 6 & 9 \\ 0 & 0 & 1 & 2 & 3 \end{bmatrix} \implies RREF(A) = \begin{bmatrix} 1 & 2 & 0 & 0 & 0 \\  &  & 1 & 2 & 3 \\  &  & \mathbf{0} &  &  \end{bmatrix}$ Thus, $\mathbf{Ax = 0} \implies$ $x_1 = -2a_2 \\  x_2 = a_2 \\ x_3 = -2a_4 -3a_5 \\  x_4 = a_4 \\                 x_5 = a_5  $ $\implies \mathbf{x} = a_2\begin{bmatrix} -2  \\ 1 \\ 0 \\  0\\ 0 \\ \end{bmatrix} + a_4\begin{bmatrix} 0  \\ 0 \\ -2 \\  1 \\ 0 \\ \end{bmatrix} + a_5\begin{bmatrix} 0  \\ 0 \\ -3 \\  0 \\ 1 \\ \end{bmatrix}. $ $1.$ I can't pinpoint why, but I'm tentative about this method (in the grey box above): for each free variable (there're 3 here), this method sets $1$ for it and sets the other free variables to $0$. How and why does each free variable have one of these special solns? How and why does this function? $2.$ Since $a_2, a_4, a_5$ are free, each can be any scalar. Say I select $a_2 = a_4 = a_5 = 1$. Then  $x = (-2, 1, -5, 1, 1)$, which is one vector. But it's wrong to infer that $ \ker(A) = \{$ all scalar multiples of this one vector $\}$. How and Why? This question precedes rank, REF, $\mathbf{Ax = b}$, linear independence, span, basis, dimension, dimensions/theorems of the 4 subspaces,  Orthogonality, Determinants, eigenvalues and eigenvectors, and linear transformations. Please omit them from answers.",,['linear-algebra']
79,Linear Algebra in curved space,Linear Algebra in curved space,,"We know that Euclidean geometry and Newtonian Physics are special cases that only work in a flat space-time. Got to thinking about linear algebra and matrices. Is linear-algebra a special subset of some math? And does LA only work because our local space-time is flat enough to discard any discrepancies that would result from plotting a ""line"" or a ""plane"" in a curved space-time?","We know that Euclidean geometry and Newtonian Physics are special cases that only work in a flat space-time. Got to thinking about linear algebra and matrices. Is linear-algebra a special subset of some math? And does LA only work because our local space-time is flat enough to discard any discrepancies that would result from plotting a ""line"" or a ""plane"" in a curved space-time?",,"['linear-algebra', 'recreational-mathematics', 'mathematical-physics']"
80,Generating a $n$-th dimensional vector orthogonal to $n-1$ linearly-independent vectors,Generating a -th dimensional vector orthogonal to  linearly-independent vectors,n n-1,"Let us have $n-1$ linearly independent vectors $\vec{v}_{1},\dots,\vec{v}_{n-1}\in\mathbb{R}^{n}$, define the vector $\vec{w}$ as follows: $$\vec{w}=\begin{pmatrix}\det(\vec{v}_{1},\dots,\vec{v}_{n-1},\vec{e}_{1}) \\ \vdots \\ \det(\vec{v}_{1},\dots,\vec{v}_{n-1},\vec{e}_{n})\end{pmatrix}$$ This vector is apparantly orthogonal to all vectors $v_{i}$, $i\in\{1,\dots,n-1\}$; however I am not able to prove that this is the case and I don't understand the intuition behind it either, so I would be grateful if someone was able to prove this fact and perhaps allow me to grasp it more intuitively.","Let us have $n-1$ linearly independent vectors $\vec{v}_{1},\dots,\vec{v}_{n-1}\in\mathbb{R}^{n}$, define the vector $\vec{w}$ as follows: $$\vec{w}=\begin{pmatrix}\det(\vec{v}_{1},\dots,\vec{v}_{n-1},\vec{e}_{1}) \\ \vdots \\ \det(\vec{v}_{1},\dots,\vec{v}_{n-1},\vec{e}_{n})\end{pmatrix}$$ This vector is apparantly orthogonal to all vectors $v_{i}$, $i\in\{1,\dots,n-1\}$; however I am not able to prove that this is the case and I don't understand the intuition behind it either, so I would be grateful if someone was able to prove this fact and perhaps allow me to grasp it more intuitively.",,"['linear-algebra', 'vector-spaces', 'determinant']"
81,Summation of matrix multiplications of three matrices,Summation of matrix multiplications of three matrices,,"I have a formula as below $$\vec{H}=A_1\Omega \vec{V}_1+A_2\Omega \vec{V}_2+A_3\Omega \vec{V}_3$$ where $A_1$, $A_2$ and $A_3$ are matrices of size $3 \times 3$; $\vec{V}_1$, $\vec{V}_2$, $\vec{V}_3$ and $\vec{H}$ are vectors of size $3 \times 1$; $\Omega$ is a symmetric matrix of size $3 \times 3$ . More specifically, $\Omega=\vec{k} \ \vec{k}^T$ where $\vec{k}$ is a  vector of size $3 \times 1$. My question is: Is there any way to combine those 3 terms into 1 term (simplifying the formula of H)?  For example, something like $\vec{H}=P \Omega \vec{Q}$. Other forms are OK, as long as only one term remains on the right side. What I have tried: I have tried to expand all the elements of the terms in the right side both manually and using Matlab symbolic calculation. But I couldn't return it as a matrix multiplication of $\Omega$ and other matrices Since my knowledge about matrix calculation is poor. I have been reading about it quite a while, but again, I couldn't figure out if the simplification in this case would be possible and how. Therefore, I have decided to come here to ask. Your help would be greatly appreciated.","I have a formula as below $$\vec{H}=A_1\Omega \vec{V}_1+A_2\Omega \vec{V}_2+A_3\Omega \vec{V}_3$$ where $A_1$, $A_2$ and $A_3$ are matrices of size $3 \times 3$; $\vec{V}_1$, $\vec{V}_2$, $\vec{V}_3$ and $\vec{H}$ are vectors of size $3 \times 1$; $\Omega$ is a symmetric matrix of size $3 \times 3$ . More specifically, $\Omega=\vec{k} \ \vec{k}^T$ where $\vec{k}$ is a  vector of size $3 \times 1$. My question is: Is there any way to combine those 3 terms into 1 term (simplifying the formula of H)?  For example, something like $\vec{H}=P \Omega \vec{Q}$. Other forms are OK, as long as only one term remains on the right side. What I have tried: I have tried to expand all the elements of the terms in the right side both manually and using Matlab symbolic calculation. But I couldn't return it as a matrix multiplication of $\Omega$ and other matrices Since my knowledge about matrix calculation is poor. I have been reading about it quite a while, but again, I couldn't figure out if the simplification in this case would be possible and how. Therefore, I have decided to come here to ask. Your help would be greatly appreciated.",,"['linear-algebra', 'matrices']"
82,Operator in the commutant with certain property,Operator in the commutant with certain property,,"If $T$ is a bounded operator with nontrivial kernel (in my case it is actually finite dimensional kernel and the operator is quasinilpotent) acting on an infinite dimensional Banach space, can one always find an operator $S$ with non-trivial range that commutes with $T$ and such that $Ker{(T)}\cap\overline{Range(S)}=\{0\}$? Edit: Is this true for matrices? If $A$ is a $n\times n$ non-zero matrix with non-trivial null space, can we find a matrix $B$ that commutes with $A$ and whose column space has trivial intersection with the null space of $A$?","If $T$ is a bounded operator with nontrivial kernel (in my case it is actually finite dimensional kernel and the operator is quasinilpotent) acting on an infinite dimensional Banach space, can one always find an operator $S$ with non-trivial range that commutes with $T$ and such that $Ker{(T)}\cap\overline{Range(S)}=\{0\}$? Edit: Is this true for matrices? If $A$ is a $n\times n$ non-zero matrix with non-trivial null space, can we find a matrix $B$ that commutes with $A$ and whose column space has trivial intersection with the null space of $A$?",,"['linear-algebra', 'functional-analysis', 'operator-theory']"
83,A vector that is orthogonal to the null space must be in the row space,A vector that is orthogonal to the null space must be in the row space,,"Simple question. We know from the fundamental theorem of linear algebra that the nullspace of a matrix is the orthogonal complement of its row space. I can write this as: Let $M$ be a matrix. The following two conditions are equivalent: (i)  $u$ is orthogonal to the null space of $M$. (ii) $u$ is in the row space of $M$. That (ii) implies (i) is trivial, pretty much by definition. Question: But why is it also true (or even obvious) that (i) implies (ii)?","Simple question. We know from the fundamental theorem of linear algebra that the nullspace of a matrix is the orthogonal complement of its row space. I can write this as: Let $M$ be a matrix. The following two conditions are equivalent: (i)  $u$ is orthogonal to the null space of $M$. (ii) $u$ is in the row space of $M$. That (ii) implies (i) is trivial, pretty much by definition. Question: But why is it also true (or even obvious) that (i) implies (ii)?",,[]
84,Determinant of a circulant matrix as Chebyshev-like recurrence,Determinant of a circulant matrix as Chebyshev-like recurrence,,"It is while studying the Hückel Method of Physical Chemistry that I came across the following recurrence relation: \begin{align*} U_n(x)=xU_{n-1}(x)-U_{n-2}(x)+(-1)^{n-1}(4+2x) \end{align*} Where  \begin{align*} U_n(x):=\underbrace{\left|\begin{matrix} x& 1 &&&&1\\ 1 & x& 1 &&& \\ & 1 & x &1&& \\ && 1 &x&& \\ &&&& \ddots &1\\  1 &&&&1&x \end{matrix}\right|}_n \end{align*} For the related determinant $\displaystyle D_n(x):=\underbrace{\left|\begin{matrix} x& 1 &&&&\\ 1 & x& 1 &&& \\ & 1 & x &1&& \\ && 1 &x&& \\ &&&& \ddots &1\\   &&&&1&x \end{matrix}\right|}_n$, we have the nicer recurrence relation: \begin{align*} D_n(x)=xD_{n-1}(x)-D_{n-2}(x) \end{align*} which is a Chebyshev polynomial of the first kind. However, I would like to somehow find a closed form for $U_n(x)$. Is this at all possible?","It is while studying the Hückel Method of Physical Chemistry that I came across the following recurrence relation: \begin{align*} U_n(x)=xU_{n-1}(x)-U_{n-2}(x)+(-1)^{n-1}(4+2x) \end{align*} Where  \begin{align*} U_n(x):=\underbrace{\left|\begin{matrix} x& 1 &&&&1\\ 1 & x& 1 &&& \\ & 1 & x &1&& \\ && 1 &x&& \\ &&&& \ddots &1\\  1 &&&&1&x \end{matrix}\right|}_n \end{align*} For the related determinant $\displaystyle D_n(x):=\underbrace{\left|\begin{matrix} x& 1 &&&&\\ 1 & x& 1 &&& \\ & 1 & x &1&& \\ && 1 &x&& \\ &&&& \ddots &1\\   &&&&1&x \end{matrix}\right|}_n$, we have the nicer recurrence relation: \begin{align*} D_n(x)=xD_{n-1}(x)-D_{n-2}(x) \end{align*} which is a Chebyshev polynomial of the first kind. However, I would like to somehow find a closed form for $U_n(x)$. Is this at all possible?",,"['linear-algebra', 'polynomials', 'recurrence-relations', 'determinant', 'orthogonal-polynomials']"
85,Proving a well-known formula regarding adj(A),Proving a well-known formula regarding adj(A),,"The adjugate of a matrix $A$ is defined as $$ (\mathrm{adj}(A))_{ij} = (-1)^{i+j}M_{ji}(A) $$ where $M_{ji}(A)$ is the determinant of the matrix $A$ after row $j$ and column $i$ have been removed. It is well-known that $$ A\,\mathrm{adj}(A) = \mathrm{adj}(A)A = \det(A) I $$ Therefore, if $A$ is invertible, we get the famous formula $A^{-1} = \det(A)^{-1}\mathrm{adj}(A)$. There is also a well-known expansion of $A$ using the Cayley-Hamilton theorem, that looks like this: $$ p_0 I + p_1 A + p_2 A^2 + \dots + p_n A^n = 0 $$ where the $p_i$ are the coefficients of the characteristic polynomial of $A$. I have seen it written in many places (wikipedia, planetmath, textbooks) that the following also holds: $$ \mathrm{adj}(A) = -(p_1 I + p_2 A + \dots + p_n A^{n-1}) $$ where the $p_i$ are the same as in the previous equation. It is easy to prove this formula in the case where $A$ is invertible by applying the identities given above together with the fact that $p_0 = \det(A)$. However, I think the adjugate expansion still holds when $A$ is not invertible... how do I prove it for such a case?","The adjugate of a matrix $A$ is defined as $$ (\mathrm{adj}(A))_{ij} = (-1)^{i+j}M_{ji}(A) $$ where $M_{ji}(A)$ is the determinant of the matrix $A$ after row $j$ and column $i$ have been removed. It is well-known that $$ A\,\mathrm{adj}(A) = \mathrm{adj}(A)A = \det(A) I $$ Therefore, if $A$ is invertible, we get the famous formula $A^{-1} = \det(A)^{-1}\mathrm{adj}(A)$. There is also a well-known expansion of $A$ using the Cayley-Hamilton theorem, that looks like this: $$ p_0 I + p_1 A + p_2 A^2 + \dots + p_n A^n = 0 $$ where the $p_i$ are the coefficients of the characteristic polynomial of $A$. I have seen it written in many places (wikipedia, planetmath, textbooks) that the following also holds: $$ \mathrm{adj}(A) = -(p_1 I + p_2 A + \dots + p_n A^{n-1}) $$ where the $p_i$ are the same as in the previous equation. It is easy to prove this formula in the case where $A$ is invertible by applying the identities given above together with the fact that $p_0 = \det(A)$. However, I think the adjugate expansion still holds when $A$ is not invertible... how do I prove it for such a case?",,"['linear-algebra', 'matrices']"
86,"If a vector space has a proper subspace isomorphic to itself, it has infinite dimension?","If a vector space has a proper subspace isomorphic to itself, it has infinite dimension?",,"If a set $S$ has a proper subset $A$ such that there is a bijection from $A$ to $S$ we know that $S$ cannot be finite. Now, if a vector space $V$ has a proper subspace $W$ isomorphic to itself, then the dimension of $V$ need to be infinite? I think the answer is yes and I've supplied the following proof: let $\alpha \subset W$ be a basis of the subspace. Since there's an isomorphism $\phi : W \to V$ and since isomorphisms transforms basis into basis, we know that $\phi(\alpha)\subset V$ is a basis of $V$ and that $\phi | \alpha : \alpha \to \phi(\alpha)$ is a bijection between those bases as sets. Now, we know from the proof that every vector space has a basis that we can produce a basis for $V$ from the set $\alpha$. In that case, let $\beta$ be that basis. We know that any two basis of $V$ has the same cardinality and we know that $\alpha \subset \beta$ and that $|\alpha| = |\phi(\alpha)|$ (because $\phi$ is a bijection between those) and we know that $|\phi(\alpha)| = |\beta|$ because those two are basis of $V$. Hence, the set $\beta$ has the same cardinality as it's proper subset $\alpha$ (which we know to be proper because $W$ is a proper subspace). In that case, since these are just sets, we must conclude that $\beta$ is infinite. Now, I'm a little unsure of this proof. Is this proof correct? Is the proposition true? Thanks very much in advance!","If a set $S$ has a proper subset $A$ such that there is a bijection from $A$ to $S$ we know that $S$ cannot be finite. Now, if a vector space $V$ has a proper subspace $W$ isomorphic to itself, then the dimension of $V$ need to be infinite? I think the answer is yes and I've supplied the following proof: let $\alpha \subset W$ be a basis of the subspace. Since there's an isomorphism $\phi : W \to V$ and since isomorphisms transforms basis into basis, we know that $\phi(\alpha)\subset V$ is a basis of $V$ and that $\phi | \alpha : \alpha \to \phi(\alpha)$ is a bijection between those bases as sets. Now, we know from the proof that every vector space has a basis that we can produce a basis for $V$ from the set $\alpha$. In that case, let $\beta$ be that basis. We know that any two basis of $V$ has the same cardinality and we know that $\alpha \subset \beta$ and that $|\alpha| = |\phi(\alpha)|$ (because $\phi$ is a bijection between those) and we know that $|\phi(\alpha)| = |\beta|$ because those two are basis of $V$. Hence, the set $\beta$ has the same cardinality as it's proper subset $\alpha$ (which we know to be proper because $W$ is a proper subspace). In that case, since these are just sets, we must conclude that $\beta$ is infinite. Now, I'm a little unsure of this proof. Is this proof correct? Is the proposition true? Thanks very much in advance!",,"['linear-algebra', 'proof-verification']"
87,Simultaneously (generalized) diagonalizable matrices,Simultaneously (generalized) diagonalizable matrices,,"I heard the following theorem from our textbook: Given $A,B$ are two commuting ($AB=BA$) real normal matrices. There's some real orthogonal matrix $P$ such that $P^{-1}AP$, $P^{-1}BP$ are canonical forms. The canonical form of a real normal matrix is a block diagonal whose diagonal blocks are either $\lambda\in\mathbb R$ or \begin{bmatrix}a&-b\\b&a\end{bmatrix} It could be proved by induction on the size of the matrices. It's well-known that there's a common (complex) eigenvector $v$. If $v$ is a real vector (or a scalar of a real one), then the reduction is obvious. If $v=x+iy$, where $x,y$ are linear independent, since $A$ is orthogonally similar to a canonical form, it's not so hard to show that $x^tx=y^ty$ and $x^ty=y^tx=0$, and $\operatorname{span}\{x,y\}$ is $A$(and $B$)-invariant. However, I'm not satisfied with the proof above. It seems that the paradigm could be generalized as follow: Suppose $A,B$ are semi-simple matrices over a number field $K$ which could be simultaneously diagonalized over $\mathbb C$, then there's some matrix $P$ over $K$ such that $P^{-1}AP$, $P^{-1}BP$ are block diagonal matrices, the characteristic polynomial of whose diagonal blocks are irreducible. Even more, I wonder whether we could stipulate these blocks to be Frobenius normal forms. I don't know, however. Any proof (or disproof) of the preceding statement or any generalization? Thanks! EDIT: Semisimple matrices are matrices for semisimple operators . For example, if the number field $K=\mathbb C$ or another algebraically closed field, semisimple matrices are just diagonalizable matrices, therefore the statement is true when $K$ is algebraically closed .","I heard the following theorem from our textbook: Given $A,B$ are two commuting ($AB=BA$) real normal matrices. There's some real orthogonal matrix $P$ such that $P^{-1}AP$, $P^{-1}BP$ are canonical forms. The canonical form of a real normal matrix is a block diagonal whose diagonal blocks are either $\lambda\in\mathbb R$ or \begin{bmatrix}a&-b\\b&a\end{bmatrix} It could be proved by induction on the size of the matrices. It's well-known that there's a common (complex) eigenvector $v$. If $v$ is a real vector (or a scalar of a real one), then the reduction is obvious. If $v=x+iy$, where $x,y$ are linear independent, since $A$ is orthogonally similar to a canonical form, it's not so hard to show that $x^tx=y^ty$ and $x^ty=y^tx=0$, and $\operatorname{span}\{x,y\}$ is $A$(and $B$)-invariant. However, I'm not satisfied with the proof above. It seems that the paradigm could be generalized as follow: Suppose $A,B$ are semi-simple matrices over a number field $K$ which could be simultaneously diagonalized over $\mathbb C$, then there's some matrix $P$ over $K$ such that $P^{-1}AP$, $P^{-1}BP$ are block diagonal matrices, the characteristic polynomial of whose diagonal blocks are irreducible. Even more, I wonder whether we could stipulate these blocks to be Frobenius normal forms. I don't know, however. Any proof (or disproof) of the preceding statement or any generalization? Thanks! EDIT: Semisimple matrices are matrices for semisimple operators . For example, if the number field $K=\mathbb C$ or another algebraically closed field, semisimple matrices are just diagonalizable matrices, therefore the statement is true when $K$ is algebraically closed .",,"['linear-algebra', 'matrices', 'representation-theory']"
88,Showing that a transformation $T:\mathbb R^3 \to \mathbb R^2$ is linear,Showing that a transformation  is linear,T:\mathbb R^3 \to \mathbb R^2,"OK, I am trying to prove the following transformation is linear, and find the basis for $\ker(T)$ and Im$(T)$ (also denoted in our textbook by $N(T)$ and $R(T)$ ). Then we're suposed to find the nullity and rank of $T$. $T: \Bbb{R}^3 \rightarrow \Bbb{R}^2$ defined by  $T(a_1, a_2, a_3) = (a_1-a_2, 2a_3)$ We want to see that the transformation preserved addition and scalar multiplication. So I define vector $a$ as $(a_1, a_2, a_3)$ and $b$ as $(b_1, b_2, b_3)$. So the first question is whether $T((a_1+b_1, a_2+b_2, a_3+b_3)$ = $T((a_1-a_2), 2a_3) + T(b_1-b_2, 2b_3)$ and when I plug in vectors $a+b$ to the transformation I get: $((a_1+b_1)-(a_2+b_2), 2(a_3+b_3))$ which works. So addition is preserved. The next question is whether it preserves scalar multiplication, or if $T(ca+b) = cT(a) + T(b)$ and as it happens:  $T(ca_1+b_1, ca_2+b_2, ca_3+b_3) = ((ca_1+b_1-ca_2+b_2), 2(ca_3+b_3))$ and then if we break up the vectors we find that we get $(ca_1-ca_2, 2ca_3)+(b_1-b_2, 2ba_3)$ so the transformation is linear. To find the kernel we look for the set of vectors for which $T(a_1,a_2,a_3) = 0$.  That happens whenever $a_1 = a_2$ and $a_3 = 0$ But that is where I get stuck because the definition of a kernel doesn't seem to fit. What is the basis for the kernel in this case? If a kernel is a set of vectors then this is making little or no sense to me from the get-go. Because I am not sure what the basis would be if the set of vectors are all those where $a_1 = a_2$ unless it's something like $(a_1, a_2, 0)$. And the dimension of the kernel is 2, I wold think intuitively, but I want to better understand why that is so I can get through the rest of the problem.","OK, I am trying to prove the following transformation is linear, and find the basis for $\ker(T)$ and Im$(T)$ (also denoted in our textbook by $N(T)$ and $R(T)$ ). Then we're suposed to find the nullity and rank of $T$. $T: \Bbb{R}^3 \rightarrow \Bbb{R}^2$ defined by  $T(a_1, a_2, a_3) = (a_1-a_2, 2a_3)$ We want to see that the transformation preserved addition and scalar multiplication. So I define vector $a$ as $(a_1, a_2, a_3)$ and $b$ as $(b_1, b_2, b_3)$. So the first question is whether $T((a_1+b_1, a_2+b_2, a_3+b_3)$ = $T((a_1-a_2), 2a_3) + T(b_1-b_2, 2b_3)$ and when I plug in vectors $a+b$ to the transformation I get: $((a_1+b_1)-(a_2+b_2), 2(a_3+b_3))$ which works. So addition is preserved. The next question is whether it preserves scalar multiplication, or if $T(ca+b) = cT(a) + T(b)$ and as it happens:  $T(ca_1+b_1, ca_2+b_2, ca_3+b_3) = ((ca_1+b_1-ca_2+b_2), 2(ca_3+b_3))$ and then if we break up the vectors we find that we get $(ca_1-ca_2, 2ca_3)+(b_1-b_2, 2ba_3)$ so the transformation is linear. To find the kernel we look for the set of vectors for which $T(a_1,a_2,a_3) = 0$.  That happens whenever $a_1 = a_2$ and $a_3 = 0$ But that is where I get stuck because the definition of a kernel doesn't seem to fit. What is the basis for the kernel in this case? If a kernel is a set of vectors then this is making little or no sense to me from the get-go. Because I am not sure what the basis would be if the set of vectors are all those where $a_1 = a_2$ unless it's something like $(a_1, a_2, 0)$. And the dimension of the kernel is 2, I wold think intuitively, but I want to better understand why that is so I can get through the rest of the problem.",,"['linear-algebra', 'vector-spaces', 'transformation']"
89,what is degree of minimal polynomial?,what is degree of minimal polynomial?,,"Let $V$ and $ W$ be finite dimensional vector space over $\mathbb R $ and let $T_1 : V \rightarrow V$ and $T_2 : W \rightarrow W$ be linear transformation whose minimal polynomial are $f_1 (x)= x^3+x^2+x+1$ and$f_2 (x)= x^4 - x^2-2$. let $T :  V\oplus W \rightarrow V \oplus W$ be linear transformation s.t. $$T(v,w) =(T_1(v),T_2 (w)) $$ minimal polynomial of T is  $f(x)$, then deg $f(x)$ =? and nulity T =? I can't find such $T_1$, $T_2$and  $T$ please guide me.. I don't know where to begin... I am stuck on this problem. Can anyone help me please?","Let $V$ and $ W$ be finite dimensional vector space over $\mathbb R $ and let $T_1 : V \rightarrow V$ and $T_2 : W \rightarrow W$ be linear transformation whose minimal polynomial are $f_1 (x)= x^3+x^2+x+1$ and$f_2 (x)= x^4 - x^2-2$. let $T :  V\oplus W \rightarrow V \oplus W$ be linear transformation s.t. $$T(v,w) =(T_1(v),T_2 (w)) $$ minimal polynomial of T is  $f(x)$, then deg $f(x)$ =? and nulity T =? I can't find such $T_1$, $T_2$and  $T$ please guide me.. I don't know where to begin... I am stuck on this problem. Can anyone help me please?",,['linear-algebra']
90,Inner product polynomials,Inner product polynomials,,"Let $V$ be the vector space of real polynomial $\mathbb{R}[x]$ endowed   with the inner product $\langle f,g \rangle = \displaystyle\int_{-\infty}^{\infty}  e^{-|x|}f(x)g(x) \ dx$ By considering the sequence of subspaces $\{V_n\}$ where $V_n = \{f(x) \in \mathbb{R}[x] : \deg f \leq n \}$ or otherwise, show that there exist unique monic polynomials   $\phi_n(x)$ for $n \geq 0$ such that $\displaystyle\int_{-\infty}^{\infty} e^{-|x|}\phi_n(x)g(x) \ dx = 0$ whenever $\deg g < n$, and find $\phi_n(x)$ for $n = 0,1,2.$ What is the coefficient of $x^{2000}$ in $\phi_{2007}(x)$? I'm having trouble even knowing where to begin with this question, any help appreciated!","Let $V$ be the vector space of real polynomial $\mathbb{R}[x]$ endowed   with the inner product $\langle f,g \rangle = \displaystyle\int_{-\infty}^{\infty}  e^{-|x|}f(x)g(x) \ dx$ By considering the sequence of subspaces $\{V_n\}$ where $V_n = \{f(x) \in \mathbb{R}[x] : \deg f \leq n \}$ or otherwise, show that there exist unique monic polynomials   $\phi_n(x)$ for $n \geq 0$ such that $\displaystyle\int_{-\infty}^{\infty} e^{-|x|}\phi_n(x)g(x) \ dx = 0$ whenever $\deg g < n$, and find $\phi_n(x)$ for $n = 0,1,2.$ What is the coefficient of $x^{2000}$ in $\phi_{2007}(x)$? I'm having trouble even knowing where to begin with this question, any help appreciated!",,"['linear-algebra', 'inner-products']"
91,Tensors as mutlilinear maps,Tensors as mutlilinear maps,,"I am aware that many books on differential geometry define tensors as multilinear maps. Namely $$ V\otimes W := L_2(V^*\times W^*,\Bbb F) $$ I am also aware that this space is isomorphic to the tensor product in the finite dimensional case, but I am wondering if it is a good idea to think of tensor products as multilinear maps. Is there any reason why one would like to make this identification in the finite dimensional case? Or does this definition come from the idea that students new to the subject may have an easier time with this less abstract definition? Thanks","I am aware that many books on differential geometry define tensors as multilinear maps. Namely $$ V\otimes W := L_2(V^*\times W^*,\Bbb F) $$ I am also aware that this space is isomorphic to the tensor product in the finite dimensional case, but I am wondering if it is a good idea to think of tensor products as multilinear maps. Is there any reason why one would like to make this identification in the finite dimensional case? Or does this definition come from the idea that students new to the subject may have an easier time with this less abstract definition? Thanks",,"['linear-algebra', 'differential-geometry', 'tensor-products', 'tensors', 'multilinear-algebra']"
92,Book on quadric surfaces with linear algebra,Book on quadric surfaces with linear algebra,,"Most information that I can find about quadric surfaces is written from a calculus perspective - without using any matrices or vectors. However, I would like to have a reference that tells me the different representations for a quadric surface and also some background on why these representations are used. So, e.g., I want to see both the vector representation(s) and the scalar representation(s) of a cone. Are there any books that cover this?","Most information that I can find about quadric surfaces is written from a calculus perspective - without using any matrices or vectors. However, I would like to have a reference that tells me the different representations for a quadric surface and also some background on why these representations are used. So, e.g., I want to see both the vector representation(s) and the scalar representation(s) of a cone. Are there any books that cover this?",,"['linear-algebra', 'geometry', 'reference-request', 'book-recommendation']"
93,Eigenvectors and Principal component,Eigenvectors and Principal component,,"What is the difference between eigenvectors and principal component. I got confused about this point because some researches reported that the principal components are the same eigenvectors of covariance matrix whereas, the others say that the principal components are the yield of eigenvectors (weights) of covariance matrix multiplied with the original data. Please if some one can help me in this point.","What is the difference between eigenvectors and principal component. I got confused about this point because some researches reported that the principal components are the same eigenvectors of covariance matrix whereas, the others say that the principal components are the yield of eigenvectors (weights) of covariance matrix multiplied with the original data. Please if some one can help me in this point.",,"['linear-algebra', 'abstract-algebra', 'eigenvalues-eigenvectors', 'eigenfunctions']"
94,Solution of Matrix Equation---Positive Definite Matrix,Solution of Matrix Equation---Positive Definite Matrix,,"Let $A,C$ be $n$-dimensional symmetric matrix, $A$ is negative definite, while $C$ is positive definite. Assume that $AX+XA+2C=0$ has a unique solution $X=B$, prove then $B$ is real, symmetric, and positive definite. How to prove it? I have no idea. Thank you very much.","Let $A,C$ be $n$-dimensional symmetric matrix, $A$ is negative definite, while $C$ is positive definite. Assume that $AX+XA+2C=0$ has a unique solution $X=B$, prove then $B$ is real, symmetric, and positive definite. How to prove it? I have no idea. Thank you very much.",,['linear-algebra']
95,"Root Locus Diagrams - ""Breakaway Point""","Root Locus Diagrams - ""Breakaway Point""",,"Say that we have a root locus diagram with n poles and m zeroes. And we determine that the root locus on the real axis lies between two of these poles and breaks away from the real axis and tends to infinity somewhere in between these points. There are formulae to determine what angle the asymptote makes with the real axis as well as what point the root locus ""breaks away"" from the real axis. I have noticed from some examples that the point at which the root locus breaks is often half-way in between the two poles I was talking about. But sometimes its not. Is there a rule of thumb to determine when this is the case? This would make drawing root locus diagrams much quicker and give me more intuition on the topic. This might sound a bit confusing, http://www.facstaff.bucknell.edu/mastascu/econtrolhtml/RootLocus/RLocus1ARCADE.html has some examples on root locus diagrams. The ""Three Real Poles"" and ""Three Poles, One Zero"" examples show what I meant about the ""breakaway point"" being exactly halfway between two poles whilst my textbook uses long formulae to determine a ""breaksway point"" which is not halfway between the two poles.","Say that we have a root locus diagram with n poles and m zeroes. And we determine that the root locus on the real axis lies between two of these poles and breaks away from the real axis and tends to infinity somewhere in between these points. There are formulae to determine what angle the asymptote makes with the real axis as well as what point the root locus ""breaks away"" from the real axis. I have noticed from some examples that the point at which the root locus breaks is often half-way in between the two poles I was talking about. But sometimes its not. Is there a rule of thumb to determine when this is the case? This would make drawing root locus diagrams much quicker and give me more intuition on the topic. This might sound a bit confusing, http://www.facstaff.bucknell.edu/mastascu/econtrolhtml/RootLocus/RLocus1ARCADE.html has some examples on root locus diagrams. The ""Three Real Poles"" and ""Three Poles, One Zero"" examples show what I meant about the ""breakaway point"" being exactly halfway between two poles whilst my textbook uses long formulae to determine a ""breaksway point"" which is not halfway between the two poles.",,"['linear-algebra', 'dynamical-systems', 'control-theory']"
96,"Existence of a solution with entries $1,-1$ or $0$",Existence of a solution with entries  or,"1,-1 0","Suppose $A$ is an $m\times n$ matrix and $b$ is an $m\times 1$ vector (where $m,n\geq 3$) such that each of the column vectors of $A$ and the vector $b$ has one entry equal to $1$, another entry equal to $-1$ and rest of the entries are zero. Now if the system $Ax=b$ has a solution ( where $x$ is $n\times 1$) then does there always exist a solution $x$ such that each of the entries of $x$ are $1,-1$ or $0$ ? If not in general, is it true when rank of $A$ is $m-1$ ?","Suppose $A$ is an $m\times n$ matrix and $b$ is an $m\times 1$ vector (where $m,n\geq 3$) such that each of the column vectors of $A$ and the vector $b$ has one entry equal to $1$, another entry equal to $-1$ and rest of the entries are zero. Now if the system $Ax=b$ has a solution ( where $x$ is $n\times 1$) then does there always exist a solution $x$ such that each of the entries of $x$ are $1,-1$ or $0$ ? If not in general, is it true when rank of $A$ is $m-1$ ?",,['linear-algebra']
97,Composition of orthogonal projections,Composition of orthogonal projections,,"I need to prove the following result: Suppose $P_1$ and $P_2$ are orthogonal projections onto closed subspaces $V_1$ and $V_2$, then $P_1P_2x = x$ if and only if $x\in V_1\cap V_2$. But it seems to me that if you take two parallel lines $V_1$, $V_2$ in $\mathbb R^2$, and $x\in V_1$, then $P_1P_2x =x$, but $x\not\in V_1\cap V_2$. Can you please tell me what is wrong with my intuition? Thanks!","I need to prove the following result: Suppose $P_1$ and $P_2$ are orthogonal projections onto closed subspaces $V_1$ and $V_2$, then $P_1P_2x = x$ if and only if $x\in V_1\cap V_2$. But it seems to me that if you take two parallel lines $V_1$, $V_2$ in $\mathbb R^2$, and $x\in V_1$, then $P_1P_2x =x$, but $x\not\in V_1\cap V_2$. Can you please tell me what is wrong with my intuition? Thanks!",,['linear-algebra']
98,List of various vector (linear) spaces [closed],List of various vector (linear) spaces [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question The vector (linear) space is defined as a non-empty set L over a field F , where two relations (binary operations) are defined: Addition $ \oplus: L \times L \longrightarrow L $ Scalar multiplication $ \odot: F \times L \longrightarrow L $ Although we call these relations as addition and scalar multiplication, both of these relations can have arbitrary forms, which do not need to have anything in common with the traditional apprehension of the addition and the multiplication (for example the addition and the multiplication of real numbers). From the set-theoretical point of view are both operations just mappings between two sets, closed under the relations, which need to meet following conditions (axioms of linear space): Associativity of addition Commutativity of addition Identity element of addition Inverse elements of addition Distributivity of scalar multiplication with respect to vector addition Distributivity of scalar multiplication with respect to field addition Compatibility of scalar multiplication with field multiplication Identity element of scalar multiplication We are usually working with the linear space of complex numbers $\mathbb{C}^n$, because all linear spaces of dimension of $n$ are isomorphic. In other words, we can use linear transformation between two different linear spaces and equivalently solve the problem in some well known linear space (usually $\mathbb{C}^n$) and then transform it back. My question is: What kinds of linear spaces do you know? What fields and binary operations compose the linear space. If you can, please, also note the physical application of such a linear space. I will just summarize the ones I know: Vector space $\mathbb{C}^n$, field of complex numbers $\mathbb{C}$ Let $\mathbf{x},\mathbf{y} \in \mathbb{C}^n$ and $\alpha \in \mathbb{C}$, where $\mathbf{x} = (x_1, x_2, ..., x_n)$ and $\mathbf{y} = (y_1, y_2, ..., y_n)$. Operation of addition $\oplus$ and scalar multiplication $\odot$ are defined as $\mathbf{x} \oplus \mathbf{y} \triangleq (x_1 + y_1, x_2 + y_2, ..., x_n + y_n)$ $\alpha \odot \mathbf{x} \triangleq (\alpha \cdot x_1, \alpha \cdot x_2, ..., \alpha \cdot x_n)$ (Note: Plus symbol and dot symbol in the brackets denote operations of addition and multiplication of complex numbers.) Function space, field of complex numbers $\mathbb{C}$ $ f \oplus g \triangleq f(x) + g(x) $ $ \alpha \odot f \triangleq \alpha f(x)$ Vector space of possitive real numbers $\mathbb{R}^+$, field of real numbers $\mathbb{R}$ $\mathbf{x} \oplus \mathbf{y} \triangleq x \cdot y$ $\alpha \odot \mathbf{x} \triangleq x^{\alpha}$ Vector space of matrices $\mathbb{F}^{m \times n}$ over field $\mathbb{F}$ $ (\mathbf{X} \oplus \mathbf{Y})_{i,j} \triangleq (\mathbf{X})_{i,j} + (\mathbf{Y})_{i,j}$ $ (\alpha \odot \mathbf{X})_{i,j} \triangleq \alpha(\mathbf{X})_{i,j} $","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question The vector (linear) space is defined as a non-empty set L over a field F , where two relations (binary operations) are defined: Addition $ \oplus: L \times L \longrightarrow L $ Scalar multiplication $ \odot: F \times L \longrightarrow L $ Although we call these relations as addition and scalar multiplication, both of these relations can have arbitrary forms, which do not need to have anything in common with the traditional apprehension of the addition and the multiplication (for example the addition and the multiplication of real numbers). From the set-theoretical point of view are both operations just mappings between two sets, closed under the relations, which need to meet following conditions (axioms of linear space): Associativity of addition Commutativity of addition Identity element of addition Inverse elements of addition Distributivity of scalar multiplication with respect to vector addition Distributivity of scalar multiplication with respect to field addition Compatibility of scalar multiplication with field multiplication Identity element of scalar multiplication We are usually working with the linear space of complex numbers $\mathbb{C}^n$, because all linear spaces of dimension of $n$ are isomorphic. In other words, we can use linear transformation between two different linear spaces and equivalently solve the problem in some well known linear space (usually $\mathbb{C}^n$) and then transform it back. My question is: What kinds of linear spaces do you know? What fields and binary operations compose the linear space. If you can, please, also note the physical application of such a linear space. I will just summarize the ones I know: Vector space $\mathbb{C}^n$, field of complex numbers $\mathbb{C}$ Let $\mathbf{x},\mathbf{y} \in \mathbb{C}^n$ and $\alpha \in \mathbb{C}$, where $\mathbf{x} = (x_1, x_2, ..., x_n)$ and $\mathbf{y} = (y_1, y_2, ..., y_n)$. Operation of addition $\oplus$ and scalar multiplication $\odot$ are defined as $\mathbf{x} \oplus \mathbf{y} \triangleq (x_1 + y_1, x_2 + y_2, ..., x_n + y_n)$ $\alpha \odot \mathbf{x} \triangleq (\alpha \cdot x_1, \alpha \cdot x_2, ..., \alpha \cdot x_n)$ (Note: Plus symbol and dot symbol in the brackets denote operations of addition and multiplication of complex numbers.) Function space, field of complex numbers $\mathbb{C}$ $ f \oplus g \triangleq f(x) + g(x) $ $ \alpha \odot f \triangleq \alpha f(x)$ Vector space of possitive real numbers $\mathbb{R}^+$, field of real numbers $\mathbb{R}$ $\mathbf{x} \oplus \mathbf{y} \triangleq x \cdot y$ $\alpha \odot \mathbf{x} \triangleq x^{\alpha}$ Vector space of matrices $\mathbb{F}^{m \times n}$ over field $\mathbb{F}$ $ (\mathbf{X} \oplus \mathbf{Y})_{i,j} \triangleq (\mathbf{X})_{i,j} + (\mathbf{Y})_{i,j}$ $ (\alpha \odot \mathbf{X})_{i,j} \triangleq \alpha(\mathbf{X})_{i,j} $",,"['linear-algebra', 'vector-spaces']"
99,Properties of a Multilinear Function,Properties of a Multilinear Function,,"I have a question regarding the properties of a multilinear function. This is for a linear algebra class. I know that for a multilinear function, $$f(c\vec{v}_1, \vec{v}_2,\ldots,\vec{v}_n)=c \cdot f(\vec{v}_1, \vec{v}_2,\ldots,\vec{v}_n)$$ Does this imply $$f(c\vec{v}_1, d\vec{v}_2,\ldots,\vec{v}_n)=c\cdot d \cdot f(\vec{v}_1, \vec{v}_2,\ldots,\vec{v}_n)?$$ It is for a question involving a multilinear function $f:\mathbb{R}^2 \times \mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}$. I am given eight values of $f$, each of which is composed of a combination three unit vectors. For instance, $$ f\left (  \begin{bmatrix} 1\\  0 \end{bmatrix}  , \begin{bmatrix} 1\\  0 \end{bmatrix}  , \begin{bmatrix} 1\\  0 \end{bmatrix}  \right ) =e $$ and $$ f\left (  \begin{bmatrix} 0\\  1 \end{bmatrix}  , \begin{bmatrix} 0\\  1 \end{bmatrix}  , \begin{bmatrix} 0\\  1 \end{bmatrix}  \right ) =3 $$ or, $f(\vec{e}_1,\vec{e}_1,\vec{e}_1)=e$. Then I am asked to compute for different values of $f$. For instance, $$ f\left (  \begin{bmatrix} 1\\  2 \end{bmatrix}  , \begin{bmatrix} 1\\  3 \end{bmatrix}  , \begin{bmatrix} 1\\  5 \end{bmatrix}  \right ) $$ In this case I used $$f(\vec{e}_1 + 2\vec{e}_2,\vec{e}_1 + 3\vec{e}_2,\vec{e}_1 + 5\vec{e}_2) = f(\vec{e}_1 ,\vec{e}_1,\vec{e}_1) + f(2\vec{e}_2 ,3\vec{e}_2,5\vec{e}_2)= f(\vec{e}_1 ,\vec{e}_1,\vec{e}_1) + 2 \cdot 3 \cdot 5 \cdot f(\vec{e}_2 ,\vec{e}_2,\vec{e}_2)$$ Which, using the given values, equals $(2)(3)(5)(3) + e = 90+e$. Is this okay?","I have a question regarding the properties of a multilinear function. This is for a linear algebra class. I know that for a multilinear function, $$f(c\vec{v}_1, \vec{v}_2,\ldots,\vec{v}_n)=c \cdot f(\vec{v}_1, \vec{v}_2,\ldots,\vec{v}_n)$$ Does this imply $$f(c\vec{v}_1, d\vec{v}_2,\ldots,\vec{v}_n)=c\cdot d \cdot f(\vec{v}_1, \vec{v}_2,\ldots,\vec{v}_n)?$$ It is for a question involving a multilinear function $f:\mathbb{R}^2 \times \mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}$. I am given eight values of $f$, each of which is composed of a combination three unit vectors. For instance, $$ f\left (  \begin{bmatrix} 1\\  0 \end{bmatrix}  , \begin{bmatrix} 1\\  0 \end{bmatrix}  , \begin{bmatrix} 1\\  0 \end{bmatrix}  \right ) =e $$ and $$ f\left (  \begin{bmatrix} 0\\  1 \end{bmatrix}  , \begin{bmatrix} 0\\  1 \end{bmatrix}  , \begin{bmatrix} 0\\  1 \end{bmatrix}  \right ) =3 $$ or, $f(\vec{e}_1,\vec{e}_1,\vec{e}_1)=e$. Then I am asked to compute for different values of $f$. For instance, $$ f\left (  \begin{bmatrix} 1\\  2 \end{bmatrix}  , \begin{bmatrix} 1\\  3 \end{bmatrix}  , \begin{bmatrix} 1\\  5 \end{bmatrix}  \right ) $$ In this case I used $$f(\vec{e}_1 + 2\vec{e}_2,\vec{e}_1 + 3\vec{e}_2,\vec{e}_1 + 5\vec{e}_2) = f(\vec{e}_1 ,\vec{e}_1,\vec{e}_1) + f(2\vec{e}_2 ,3\vec{e}_2,5\vec{e}_2)= f(\vec{e}_1 ,\vec{e}_1,\vec{e}_1) + 2 \cdot 3 \cdot 5 \cdot f(\vec{e}_2 ,\vec{e}_2,\vec{e}_2)$$ Which, using the given values, equals $(2)(3)(5)(3) + e = 90+e$. Is this okay?",,['linear-algebra']
