,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Statistics: Maximising expected value of a function of a random variable,Statistics: Maximising expected value of a function of a random variable,,"An agent wishes to solve his optimisation problem: $ \mbox{max}_{\theta} \ \ \mathbb{E}U(\theta S_1 + (w - \theta) + Y)$, where $S_1$ is a random variable, $Y$ a contingent claim and $U(x) = x - \frac{1}{2}\epsilon x^2$. My problem is - how to I 'get rid' of '$\mathbb{E}$', to get something I can work with? Thanks","An agent wishes to solve his optimisation problem: $ \mbox{max}_{\theta} \ \ \mathbb{E}U(\theta S_1 + (w - \theta) + Y)$, where $S_1$ is a random variable, $Y$ a contingent claim and $U(x) = x - \frac{1}{2}\epsilon x^2$. My problem is - how to I 'get rid' of '$\mathbb{E}$', to get something I can work with? Thanks",,"['statistics', 'optimization', 'random-variables', 'expected-value']"
1,Convergence in probability of Sn / n,Convergence in probability of Sn / n,,"Can you please help verify if what I have done is correct for the question below? I applied Chebyshev's theorem in the first step, but I am worried if there are any mathematical errors or misapplied theorems in my solution. Thanks for the help.","Can you please help verify if what I have done is correct for the question below? I applied Chebyshev's theorem in the first step, but I am worried if there are any mathematical errors or misapplied theorems in my solution. Thanks for the help.",,"['probability', 'statistics']"
2,"Incomplete ""round trip"" of taking a minimum, then a maximum, from a positively skewed distribution","Incomplete ""round trip"" of taking a minimum, then a maximum, from a positively skewed distribution",,"Let's say you have a distribution that is either symmetric or positively skewed (and defined over 0-1).  Call it F. Then, you find the distribution of the minimum of n>1 draws from F.  Call it Fmin. Clearly the mean of Fmin is less than the mean of F. Now, find the distribution of the maximum of the same number n draws from Fmin.  Call it Fminimax. The mean of Fminimax is of course greater than the mean of Fmin.  However, the increase from Fmin to Fminimax is less in magnitude than the original decrease from F to Fmin.  Meaning, that the ""round trip"" is not complete, and the minimax operation ends up lowering the mean. What I meant to ask, was, does this single operation lower the mean for any non-negatively skewed distribution? I deleted a bunch of stuff from the question that I think I made a mess out of.  But, I can always revert the changes if anyone objects.","Let's say you have a distribution that is either symmetric or positively skewed (and defined over 0-1).  Call it F. Then, you find the distribution of the minimum of n>1 draws from F.  Call it Fmin. Clearly the mean of Fmin is less than the mean of F. Now, find the distribution of the maximum of the same number n draws from Fmin.  Call it Fminimax. The mean of Fminimax is of course greater than the mean of Fmin.  However, the increase from Fmin to Fminimax is less in magnitude than the original decrease from F to Fmin.  Meaning, that the ""round trip"" is not complete, and the minimax operation ends up lowering the mean. What I meant to ask, was, does this single operation lower the mean for any non-negatively skewed distribution? I deleted a bunch of stuff from the question that I think I made a mess out of.  But, I can always revert the changes if anyone objects.",,"['sequences-and-series', 'statistics', 'inequality', 'economics', 'recursive-algorithms']"
3,Confidence Interval within min and max values,Confidence Interval within min and max values,,"I have set of numbers and know it's size, mean, standard deviation, minimum and maximum. When I calculate regular confidence interval for mean I get something like this (-20;50). But the source value can only be positive, so it would look bad on a graph. I'd like to find confidence interval within given minimum and maximum values. It's just max values have random peaks and I want to smooth it with CI. Is there some way to do it?","I have set of numbers and know it's size, mean, standard deviation, minimum and maximum. When I calculate regular confidence interval for mean I get something like this (-20;50). But the source value can only be positive, so it would look bad on a graph. I'd like to find confidence interval within given minimum and maximum values. It's just max values have random peaks and I want to smooth it with CI. Is there some way to do it?",,['statistics']
4,What is empirical accuracy?,What is empirical accuracy?,,"Can anyone give a definition on empirical accuracy ? I Googled this keyword but cannot find a satisfactory definition. It seems to be the accuracy rate of a specific sample. Any link or typed text will be appreciated. I saw it in an data mining book. The context is evaluating the accuracy of decision tree by inputing the model test data. The original test is like this: Given the test set that contains N records, let X be the number of records predicted by a model and p be the true accuracy of the model. By modeling the prediction task as a binomial experiment, X has a binomial distribution with mean Np and variance Np(1-p) . And the empirical accuracy , acc= X/N , also has a binomial distribution with mean p and variance p(1-p)/N.","Can anyone give a definition on empirical accuracy ? I Googled this keyword but cannot find a satisfactory definition. It seems to be the accuracy rate of a specific sample. Any link or typed text will be appreciated. I saw it in an data mining book. The context is evaluating the accuracy of decision tree by inputing the model test data. The original test is like this: Given the test set that contains N records, let X be the number of records predicted by a model and p be the true accuracy of the model. By modeling the prediction task as a binomial experiment, X has a binomial distribution with mean Np and variance Np(1-p) . And the empirical accuracy , acc= X/N , also has a binomial distribution with mean p and variance p(1-p)/N.",,"['statistics', 'definition']"
5,Choosing set of best estimators for linear least squares,Choosing set of best estimators for linear least squares,,"I have a measured experimental dataset which is well approximated by the sum of several basis functions in linear combinations. Linear least squares of course gives me the optimal weight for each basis function.  These basis functions are all unrelated and may or may not be correlated (or even repeated). That still doesn't cause any problem when fitting.. least squares is well defined and I always get optimal weights. My question is about choosing the best subset of these basis functions to represent my data. If I'm allowed to use say only up to 7 of the 20 basis functions I have, how should I pick the 7?   I realize I can just use the covariance matrix and cut out rows and columns to solve the fit for any set of 7 I like, but is there an optimal strategy for choosing the best 7 to fit my data? Enumerating all sets of 7 would be expensive (20 choose 7 = 77520) and what happens if I have 100 or 1000 input base functions? My first thought is to find the one basis function (out of 20) that best approximates the data, and greedily ""accept"" that one. I then look at the remaining 19 contributors, and take the one that best helps. Repeat 7 times.  This seems like a reasonable strategy, but I don't know if it finds the best set of choices, or even if it's efficient. I haven't found discussion of choosing subsets in such a fit in my linear algebra or multivariate statistics texts, nor Google. This must be a common question, I can think of many examples where you'd use it. (Pick the best 3 weather stations to query to mix to find your local conditions. Or have a table you want to interpolate with several built in functions, and you want to pick at most 5 of them to keep evaluation speed fast.  Or have multiple different sources of stock recommendations, and you only have the budget to query a limited set of them...) Thanks!","I have a measured experimental dataset which is well approximated by the sum of several basis functions in linear combinations. Linear least squares of course gives me the optimal weight for each basis function.  These basis functions are all unrelated and may or may not be correlated (or even repeated). That still doesn't cause any problem when fitting.. least squares is well defined and I always get optimal weights. My question is about choosing the best subset of these basis functions to represent my data. If I'm allowed to use say only up to 7 of the 20 basis functions I have, how should I pick the 7?   I realize I can just use the covariance matrix and cut out rows and columns to solve the fit for any set of 7 I like, but is there an optimal strategy for choosing the best 7 to fit my data? Enumerating all sets of 7 would be expensive (20 choose 7 = 77520) and what happens if I have 100 or 1000 input base functions? My first thought is to find the one basis function (out of 20) that best approximates the data, and greedily ""accept"" that one. I then look at the remaining 19 contributors, and take the one that best helps. Repeat 7 times.  This seems like a reasonable strategy, but I don't know if it finds the best set of choices, or even if it's efficient. I haven't found discussion of choosing subsets in such a fit in my linear algebra or multivariate statistics texts, nor Google. This must be a common question, I can think of many examples where you'd use it. (Pick the best 3 weather stations to query to mix to find your local conditions. Or have a table you want to interpolate with several built in functions, and you want to pick at most 5 of them to keep evaluation speed fast.  Or have multiple different sources of stock recommendations, and you only have the budget to query a limited set of them...) Thanks!",,"['matrices', 'statistics', 'mathematical-modeling', 'regression']"
6,Forecasting based on advises from multiple advisers with various experiences(sample sizes),Forecasting based on advises from multiple advisers with various experiences(sample sizes),,"I have a signal generator, which each second generates one of the letters 'a', 'b' or 'c'. I don't know anything else about this signal generator, but I suspect that there are some patterns to it. The signal generator is started at time 0. Given output of the signal from time 0 to time n, I need to forecast its output at time n+1. I suspect that there are some patterns to the signal, so I create a frequency table: For each sub-string(up to length 7) of the first n symbols of the signal generated data, I calculate three values: the number of occurrences of 'a','b', and 'c' after that sub-string. So, for example for sub-string ""abc""(if it exists in the data), I store: the number of cases when symbol 'a' comes after string ""abc"" the number of cases when symbol 'b' comes after string ""abc"" the number of cases when symbol 'c' comes after string ""abc"" Now that I have all these data, I have 7 predictions as to what the next symbol could be. If for example the last 7 characters of the signal are ""acbccba"", then: If I look just at the one-character frequency table, then I will have a certain prediction, which would look like: ""Since the last character of the string is 'a', and since coming directly after character 'a' there were 40 cases of letter 'a', 25 cases of letter 'b', and 130 cases of letter 'c', I predict that the next character will be letter 'c'"" Similarly for the last 2-letter(""ba""), 3-letter(""cba""), ... , 7-letter(""acbccba""). So in the end I have 7 predictions. The question is, how do I find which next character is actually the most probable for this signal generator? Different predictions are based on different sample sizes, so how do I combine them effectively?","I have a signal generator, which each second generates one of the letters 'a', 'b' or 'c'. I don't know anything else about this signal generator, but I suspect that there are some patterns to it. The signal generator is started at time 0. Given output of the signal from time 0 to time n, I need to forecast its output at time n+1. I suspect that there are some patterns to the signal, so I create a frequency table: For each sub-string(up to length 7) of the first n symbols of the signal generated data, I calculate three values: the number of occurrences of 'a','b', and 'c' after that sub-string. So, for example for sub-string ""abc""(if it exists in the data), I store: the number of cases when symbol 'a' comes after string ""abc"" the number of cases when symbol 'b' comes after string ""abc"" the number of cases when symbol 'c' comes after string ""abc"" Now that I have all these data, I have 7 predictions as to what the next symbol could be. If for example the last 7 characters of the signal are ""acbccba"", then: If I look just at the one-character frequency table, then I will have a certain prediction, which would look like: ""Since the last character of the string is 'a', and since coming directly after character 'a' there were 40 cases of letter 'a', 25 cases of letter 'b', and 130 cases of letter 'c', I predict that the next character will be letter 'c'"" Similarly for the last 2-letter(""ba""), 3-letter(""cba""), ... , 7-letter(""acbccba""). So in the end I have 7 predictions. The question is, how do I find which next character is actually the most probable for this signal generator? Different predictions are based on different sample sizes, so how do I combine them effectively?",,"['probability', 'statistics']"
7,General Definition of Likelihood Function,General Definition of Likelihood Function,,"I am wondering if there is a way of generalizing the likelihood function of some parameters $L(\theta | \mathbf x), \theta \in \Theta$ given some data $\mathbf x$ that has been observed coming from a distribution on $\mathbb R^n$, say $\mu$, so that it is defined for distributions that admit neither a density nor mass function. I guess something rooted in measure theory that preserves the relevant properties of likelihood functions would be desired (e.g. under suitable regularity conditions it would be desirable for asymptotic normality and consistency of maximum likelihood estimators to hold). My only thought is that we may be able to generalize the notion so long as the distribution of the data admits some density with respect to some measure other than Lebesgue or counting measure.","I am wondering if there is a way of generalizing the likelihood function of some parameters $L(\theta | \mathbf x), \theta \in \Theta$ given some data $\mathbf x$ that has been observed coming from a distribution on $\mathbb R^n$, say $\mu$, so that it is defined for distributions that admit neither a density nor mass function. I guess something rooted in measure theory that preserves the relevant properties of likelihood functions would be desired (e.g. under suitable regularity conditions it would be desirable for asymptotic normality and consistency of maximum likelihood estimators to hold). My only thought is that we may be able to generalize the notion so long as the distribution of the data admits some density with respect to some measure other than Lebesgue or counting measure.",,['statistics']
8,"Percentage of risk, Is my calculation correct?","Percentage of risk, Is my calculation correct?",,"X is the worst scenario. In order for X to occur, two things must happen, in sequence. An event with an occurrence chance on 1/120 must occur. This is EVENT A. if EVENT A occurs, another event with a 2/100 occurrence chance must occur. This is EVENT B. EVENT A + EVENT B occurring are X = the worst possible scenario. So, at the starting point. What are the chances of ""X"" to occur? I calculated it to be 0.01% percent. Am I correct? Based on Comments: following bloodwork, a statistical model used by OBGYN's determines that a fetus has 1/120 chance of having a certain disorder. If this is indeed the case, a certain test can detect it. But that test fails in 2% of the cases (failure=existing disorder not detected). What are the chances of that fetus to be born sick, if the test is taken and upon detection of the disorder he's aborted.","X is the worst scenario. In order for X to occur, two things must happen, in sequence. An event with an occurrence chance on 1/120 must occur. This is EVENT A. if EVENT A occurs, another event with a 2/100 occurrence chance must occur. This is EVENT B. EVENT A + EVENT B occurring are X = the worst possible scenario. So, at the starting point. What are the chances of ""X"" to occur? I calculated it to be 0.01% percent. Am I correct? Based on Comments: following bloodwork, a statistical model used by OBGYN's determines that a fetus has 1/120 chance of having a certain disorder. If this is indeed the case, a certain test can detect it. But that test fails in 2% of the cases (failure=existing disorder not detected). What are the chances of that fetus to be born sick, if the test is taken and upon detection of the disorder he's aborted.",,['statistics']
9,MCMC Metropolis Hastings,MCMC Metropolis Hastings,,"Does anyone know a webpage or a document where I can find a practical example of implementation of the Metropolis-Hastings algorithm, with some thoughts about burn-in time and how to construct the transition matrix?","Does anyone know a webpage or a document where I can find a practical example of implementation of the Metropolis-Hastings algorithm, with some thoughts about burn-in time and how to construct the transition matrix?",,"['statistics', 'stochastic-processes', 'probability-distributions']"
10,bayesian networks for regression,bayesian networks for regression,,"Would it be possible to use bayesian network for regression and/or prediction? I understand that it is a tool one can use to compute probabilities, but I haven't found much material about possible applications for forecasting.","Would it be possible to use bayesian network for regression and/or prediction? I understand that it is a tool one can use to compute probabilities, but I haven't found much material about possible applications for forecasting.",,"['statistics', 'regression', 'bayesian-network']"
11,posterior distribution after having partial information on some linear combinations of unknown variables (Revised),posterior distribution after having partial information on some linear combinations of unknown variables (Revised),,"$x_1$, $x_2$, and $x_3$ are i.i.d. normal random variables with distribution $N(0, \sigma_x^{2})$ $\epsilon_1$, $\epsilon_2$, and $\epsilon_3$ are i.i.d. normal random variables with distribution $N(0, \sigma_n^{2})$ lets define : $y_1 = x_1 + \epsilon_1$ $y_2 = x_2 + \epsilon_2$ $y_3 = x_3 + \epsilon_3$ given the values for : $s_1 = y_1 + y_2$ $s_2 = y_1 + y_3$ what is the posterior distribution $f(x_1,x_2,x_3|s_1,s_2)$ ? what is the ML estimate for $x_1$, $x_2$, and $x_3$ ? as an example, assume  $\sigma_x=2, \sigma_n=1, s_1=2, s_2=1$ , we are interested in an estimate for $x_1$, $x_2$, and $x_3$. Thanks, MG","$x_1$, $x_2$, and $x_3$ are i.i.d. normal random variables with distribution $N(0, \sigma_x^{2})$ $\epsilon_1$, $\epsilon_2$, and $\epsilon_3$ are i.i.d. normal random variables with distribution $N(0, \sigma_n^{2})$ lets define : $y_1 = x_1 + \epsilon_1$ $y_2 = x_2 + \epsilon_2$ $y_3 = x_3 + \epsilon_3$ given the values for : $s_1 = y_1 + y_2$ $s_2 = y_1 + y_3$ what is the posterior distribution $f(x_1,x_2,x_3|s_1,s_2)$ ? what is the ML estimate for $x_1$, $x_2$, and $x_3$ ? as an example, assume  $\sigma_x=2, \sigma_n=1, s_1=2, s_2=1$ , we are interested in an estimate for $x_1$, $x_2$, and $x_3$. Thanks, MG",,"['probability', 'statistics', 'probability-theory', 'stochastic-processes']"
12,What is the statistically natural way to center the logit-scale around a given value?,What is the statistically natural way to center the logit-scale around a given value?,,"I would like to find out the formula for CandidateAbility used in the European PISA-test, which tests 9th grade pupil's abilities. Unfortunately the agency which publishes the results does not provide many mathematical facts. They say they use a logit-function to determine pupils' abilities in terms of percentage of correctly solved problems from a fixed problem set and the average problem difficulty for that set (never mind the definition of that). Googling for ""logit"" revealed the following formula: $$\mathrm{CandidateAbility} = \log \left( \frac{x}{1-x} \right) + \mathrm{AverageDifficulty}$$ where $x$ denotes the fraction of correctly solved problems. Assuming $\mathrm{AverageDifficulty}=0$ for now, this is centered around 0.5, i.e. a pupil solving half of the problems gets assigned ability zero. However, the PISA-agency says that they center the scale around 0.625, i.e. a pupil solving 62.5 percent of the problems gets assigned 0. Now I can imagine many ways of modifying the above formula to achieve this. The first that come to my mind are: $$\mathrm{CandidateAbility} = \log \left( \frac{x}{1-x} \right) + \mathrm{AverageDifficulty} - \log \left( \frac{0.625}{1-0.625} \right),$$ just shifting the outcome of the formula, and $$\mathrm{CandidateAbility} = \log \left( \frac{x-c}{1-(x-c)} \right) + \mathrm{AverageDifficulty}$$ where c=0.625-0.5 (the difference between the new and the old center), modifying the input into the log-term. My question is: Is there any modification of the formula, either one of the above or something entirely different, which is most natural from a statistician's point of view? Any suggestion would be welcome and could be used to counter-check against the data that is provided by the PISA-agency. Thanks!","I would like to find out the formula for CandidateAbility used in the European PISA-test, which tests 9th grade pupil's abilities. Unfortunately the agency which publishes the results does not provide many mathematical facts. They say they use a logit-function to determine pupils' abilities in terms of percentage of correctly solved problems from a fixed problem set and the average problem difficulty for that set (never mind the definition of that). Googling for ""logit"" revealed the following formula: $$\mathrm{CandidateAbility} = \log \left( \frac{x}{1-x} \right) + \mathrm{AverageDifficulty}$$ where $x$ denotes the fraction of correctly solved problems. Assuming $\mathrm{AverageDifficulty}=0$ for now, this is centered around 0.5, i.e. a pupil solving half of the problems gets assigned ability zero. However, the PISA-agency says that they center the scale around 0.625, i.e. a pupil solving 62.5 percent of the problems gets assigned 0. Now I can imagine many ways of modifying the above formula to achieve this. The first that come to my mind are: $$\mathrm{CandidateAbility} = \log \left( \frac{x}{1-x} \right) + \mathrm{AverageDifficulty} - \log \left( \frac{0.625}{1-0.625} \right),$$ just shifting the outcome of the formula, and $$\mathrm{CandidateAbility} = \log \left( \frac{x-c}{1-(x-c)} \right) + \mathrm{AverageDifficulty}$$ where c=0.625-0.5 (the difference between the new and the old center), modifying the input into the log-term. My question is: Is there any modification of the formula, either one of the above or something entirely different, which is most natural from a statistician's point of view? Any suggestion would be welcome and could be used to counter-check against the data that is provided by the PISA-agency. Thanks!",,['statistics']
13,How to fit fixed data from two linear functions,How to fit fixed data from two linear functions,,"I have a set of points $(x, y)$ where each one comes from either one of two linear functions: \begin{align*}     y &= m_1 x + b_1\\     y &= m_2 x + b_2 \end{align*} Is there a fitting method to find such functions, without knowing from which function each of the points come from? PS. can somebody add fit (or fitting) to the existing tags","I have a set of points $(x, y)$ where each one comes from either one of two linear functions: \begin{align*}     y &= m_1 x + b_1\\     y &= m_2 x + b_2 \end{align*} Is there a fitting method to find such functions, without knowing from which function each of the points come from? PS. can somebody add fit (or fitting) to the existing tags",,['statistics']
14,Correlation between Beta distributions,Correlation between Beta distributions,,"I have a Computer Science background and not very knowledgeable in Probability and Statistics. So excuse me if my question,notation, or language is flawed. Anyways, the problems is that we have two Bernoulli variables $X_1,X_2$ that generate sequences of values for $n$ consecutive Bernoulli trials. Now we have $m$ independent observations of those trials in discrete independent time-slots. For example: let:  $n=4, m=3$ The result of the aforementioned trials is: for $X_1: ((1,0,0,1),(0,0,1,1),(0,0,0,1))$ for $X_2: ((0,0,1,0),(0,1,1,0),(0,1,0,0))$ where each of ""inner-sequences"" is one of $m$ independent sequence of Bernoulli trials. Now we aggregate each of those ""inner-sequences"" into a Beta distribution function to represent the posterior probability of success/failure of each variable in different observation. For instance the above sequence transforms to the following $X_1:(B_{11},B_{12},B_{13})$ $X_2:(B_{21},B_{22},B_{23})$ where each $B_{ij}$ is a Beta distribution function associated with the corresponding sequence of trials in the previous part of the example. Now we a have two sequences Beta Distribution where we want to use in order to find the correlation between $X_1$ and $X_2$ preferably producing a final beta curve that shows the degree of correlation between $X_1$ and $X_2$ incorporating the factor of uncertainty, or a Gaussian curve. A very simple approach is to find the correlation based on the mean of the curves and using the Pearson's correlation method. However, this is not precise enough. Is there any method to find the correlation between two Beta distribution functions. An easier question is where can I find useful information about the detection of correlation based on two distribution functions of any kind (easiest should be Gaussian functions). Thank you so much in advance. Amir","I have a Computer Science background and not very knowledgeable in Probability and Statistics. So excuse me if my question,notation, or language is flawed. Anyways, the problems is that we have two Bernoulli variables $X_1,X_2$ that generate sequences of values for $n$ consecutive Bernoulli trials. Now we have $m$ independent observations of those trials in discrete independent time-slots. For example: let:  $n=4, m=3$ The result of the aforementioned trials is: for $X_1: ((1,0,0,1),(0,0,1,1),(0,0,0,1))$ for $X_2: ((0,0,1,0),(0,1,1,0),(0,1,0,0))$ where each of ""inner-sequences"" is one of $m$ independent sequence of Bernoulli trials. Now we aggregate each of those ""inner-sequences"" into a Beta distribution function to represent the posterior probability of success/failure of each variable in different observation. For instance the above sequence transforms to the following $X_1:(B_{11},B_{12},B_{13})$ $X_2:(B_{21},B_{22},B_{23})$ where each $B_{ij}$ is a Beta distribution function associated with the corresponding sequence of trials in the previous part of the example. Now we a have two sequences Beta Distribution where we want to use in order to find the correlation between $X_1$ and $X_2$ preferably producing a final beta curve that shows the degree of correlation between $X_1$ and $X_2$ incorporating the factor of uncertainty, or a Gaussian curve. A very simple approach is to find the correlation based on the mean of the curves and using the Pearson's correlation method. However, this is not precise enough. Is there any method to find the correlation between two Beta distribution functions. An easier question is where can I find useful information about the detection of correlation based on two distribution functions of any kind (easiest should be Gaussian functions). Thank you so much in advance. Amir",,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'correlation']"
15,Notation question with Dirichlet processes,Notation question with Dirichlet processes,,"This is a question on how to make sense of an equation (specifically, a description of a distribution) that, no matter how I slice it and dice it, always has some left over pieces that don't fit. The equation in question describes a random variable $G$ drawn from a Dirichlet process $\textrm{DP}(\alpha_0,G_0)$: \begin{align} G = \sum_{k=1}^\infty \beta_k \delta_{\theta_k} \end{align} where $\alpha_0 > 0$ and $G_0$ is some base measure. The $\beta_k$ are stick breaking weights that depend on $\alpha_0$ and $\theta_k$ are atoms drawn from $G_0$. The full description can be found at the top of page 4 of this paper . My first question is whether I can read $G$ like a function. Say, if I have some ""atom"" (which term I don't fully understand) $\theta_j$, can I plug it in like: \begin{align} G(\theta_j) &= \sum_{k=1}^\infty \beta_k \delta_{\theta_k} (\theta_j) \newline &= \sum_{k \:\textrm{where} \: \theta_k = \theta_j} \beta_k \end{align} meaning if I happened to draw an atom that was identical to more than one $\theta_k$ I am just adding up the relevant weights $\beta_k$. And that the random variable $G$ itself can be loosely understood as very long vector of weights $(\ldots,G_j = \sum_{k \:\textrm{where} \: \theta_k = \theta_j} \beta_k,\ldots)$ such that it is just a repartitioning of the infinite dimension vector $(\ldots,\beta_k,\ldots)$. But if I interpret it this way, I get into trouble because apparently, you're supposed to use the $G$ to draw the so-called ""atoms"" again. So I revise my understanding to think of $G$ as a multinomial distribution. But then the equation that sums over the infinitely many values of $\beta_k$ doesn't make any sense. A sum is a sum. How do you draw things from a sum, which has just a fixed scalar value? Is my interpretation wrong? Whether it's wrong at the foundations or wrong in subtle ways, please correct me regardless. I'm trying to make the leap from the prob/statistics in DeGroot's book to understanding measure theory and nonparametric processes and it's things like these that completely throw me for a loop.","This is a question on how to make sense of an equation (specifically, a description of a distribution) that, no matter how I slice it and dice it, always has some left over pieces that don't fit. The equation in question describes a random variable $G$ drawn from a Dirichlet process $\textrm{DP}(\alpha_0,G_0)$: \begin{align} G = \sum_{k=1}^\infty \beta_k \delta_{\theta_k} \end{align} where $\alpha_0 > 0$ and $G_0$ is some base measure. The $\beta_k$ are stick breaking weights that depend on $\alpha_0$ and $\theta_k$ are atoms drawn from $G_0$. The full description can be found at the top of page 4 of this paper . My first question is whether I can read $G$ like a function. Say, if I have some ""atom"" (which term I don't fully understand) $\theta_j$, can I plug it in like: \begin{align} G(\theta_j) &= \sum_{k=1}^\infty \beta_k \delta_{\theta_k} (\theta_j) \newline &= \sum_{k \:\textrm{where} \: \theta_k = \theta_j} \beta_k \end{align} meaning if I happened to draw an atom that was identical to more than one $\theta_k$ I am just adding up the relevant weights $\beta_k$. And that the random variable $G$ itself can be loosely understood as very long vector of weights $(\ldots,G_j = \sum_{k \:\textrm{where} \: \theta_k = \theta_j} \beta_k,\ldots)$ such that it is just a repartitioning of the infinite dimension vector $(\ldots,\beta_k,\ldots)$. But if I interpret it this way, I get into trouble because apparently, you're supposed to use the $G$ to draw the so-called ""atoms"" again. So I revise my understanding to think of $G$ as a multinomial distribution. But then the equation that sums over the infinitely many values of $\beta_k$ doesn't make any sense. A sum is a sum. How do you draw things from a sum, which has just a fixed scalar value? Is my interpretation wrong? Whether it's wrong at the foundations or wrong in subtle ways, please correct me regardless. I'm trying to make the leap from the prob/statistics in DeGroot's book to understanding measure theory and nonparametric processes and it's things like these that completely throw me for a loop.",,"['probability', 'statistics', 'probability-distributions']"
16,Confidence in classifying data,Confidence in classifying data,,"If I have 5 people classify an image as offensive or not, how can I calculate my confidence that the answer that 3 or more people agree on is the correct answer? I have past accuracy rates, but am not sure how to incorporate my confidence in those accuracy rates. For example, Person A may be 90% accurate, but with only 10 sample points. Person B may be 85% accurate with 1000 sample points. I've gotten as far as knowing the probability that all 5 people will choose the correct answer is, assuming P(X) is the probability that Person X will choose the correct answer: P(A)*P(B)*P(C)*P(D)*P(E) I've been looking at confidence intervals and multinomial models, but haven't been able to piece everything together.","If I have 5 people classify an image as offensive or not, how can I calculate my confidence that the answer that 3 or more people agree on is the correct answer? I have past accuracy rates, but am not sure how to incorporate my confidence in those accuracy rates. For example, Person A may be 90% accurate, but with only 10 sample points. Person B may be 85% accurate with 1000 sample points. I've gotten as far as knowing the probability that all 5 people will choose the correct answer is, assuming P(X) is the probability that Person X will choose the correct answer: P(A)*P(B)*P(C)*P(D)*P(E) I've been looking at confidence intervals and multinomial models, but haven't been able to piece everything together.",,['probability']
17,is statistical hypothesis testing more reliable when there are less possible tests?,is statistical hypothesis testing more reliable when there are less possible tests?,,"I have a collections of ""systems"" that I want to test their performance, $1,\ldots,k$. I have a small set of samples, for which I can test whether each condition does better or not than the other, so I can get a binary result $I(i,j)$ for each $1 \le i,j \le k$ that tests whether $i$ or $j$ do better on this small sample. This $I(i,j)$ is calculated by the following way: each $i$ is associated with a ""system"", like I said. We have a collection of samples $X_1,...,X_n$, and we measure $C(i) = 1/n \sum_{l=1}^n l(X_l,i)$ where $l(X_l,i)$ is a measure of how well system $i$ does on sample $l$. Then we just measure whether $C(i) > C(j)$ or vice versa (to get $I(i,j)$). Can I pick from these collections of binary indicators the system which is mostly likely to do best on the expected value of the whole distribution? Are there any ways to have theoretical guarantees for that, and do they depend on $k$? By ""do best on the expected value"" I am referring to $D(i) = \mathbb{E}[l(X),i]$ - this should be highest for the system I pick.","I have a collections of ""systems"" that I want to test their performance, $1,\ldots,k$. I have a small set of samples, for which I can test whether each condition does better or not than the other, so I can get a binary result $I(i,j)$ for each $1 \le i,j \le k$ that tests whether $i$ or $j$ do better on this small sample. This $I(i,j)$ is calculated by the following way: each $i$ is associated with a ""system"", like I said. We have a collection of samples $X_1,...,X_n$, and we measure $C(i) = 1/n \sum_{l=1}^n l(X_l,i)$ where $l(X_l,i)$ is a measure of how well system $i$ does on sample $l$. Then we just measure whether $C(i) > C(j)$ or vice versa (to get $I(i,j)$). Can I pick from these collections of binary indicators the system which is mostly likely to do best on the expected value of the whole distribution? Are there any ways to have theoretical guarantees for that, and do they depend on $k$? By ""do best on the expected value"" I am referring to $D(i) = \mathbb{E}[l(X),i]$ - this should be highest for the system I pick.",,"['probability', 'statistics']"
18,Find formula to changing cyclic behavior,Find formula to changing cyclic behavior,,"Could you please help me in this problem? I have 3 independent variables, $(T,H,t)$, as inputs and one output $P$  ( I have all data for these inputs and the output, done experimentally measured every hour during one year). I want to find a formula of this form: $$P = f ( T , H , t )$$ where $t$ is the time in hours and it is always in the $x$-axis (index), $T$ is temperature, $H$ is humidity, and $P$ is power. I have all the data, and when I draw them in the same graph during one year, meaning that $P$, $T$,  and $H$ vs. hours. I found that the behavior of $P$ is oscillating, making a sinusoidal shape over the entire, year as you can see from the following ($P$ and time only): So, if I make a zoom view to this figure, for example from the $2000$th hour of the year to the $3000$th hour, it is clear that it almost has the same shape but it is oscillating. So, it keeps oscillating and increasing up to the peak point and then it starts decreasing till the end of the year. But this is only for only one independent variable which is time in hours. Now, how if we include the effect of temperature and humidity and draw  these vectors along with the power vs. time, to see how power is changing with respect to $T$, $H$, and $t$  rather than $t$ only: So, how can I predict the structure of the formula that relates $P$ with $T$, $H$, and $t$? Is there any approach that you advise me to follow? Sorry for this long question and any help from you is highly appreciated. I read many papers but I could not know how to solve the problem. thanks","Could you please help me in this problem? I have 3 independent variables, $(T,H,t)$, as inputs and one output $P$  ( I have all data for these inputs and the output, done experimentally measured every hour during one year). I want to find a formula of this form: $$P = f ( T , H , t )$$ where $t$ is the time in hours and it is always in the $x$-axis (index), $T$ is temperature, $H$ is humidity, and $P$ is power. I have all the data, and when I draw them in the same graph during one year, meaning that $P$, $T$,  and $H$ vs. hours. I found that the behavior of $P$ is oscillating, making a sinusoidal shape over the entire, year as you can see from the following ($P$ and time only): So, if I make a zoom view to this figure, for example from the $2000$th hour of the year to the $3000$th hour, it is clear that it almost has the same shape but it is oscillating. So, it keeps oscillating and increasing up to the peak point and then it starts decreasing till the end of the year. But this is only for only one independent variable which is time in hours. Now, how if we include the effect of temperature and humidity and draw  these vectors along with the power vs. time, to see how power is changing with respect to $T$, $H$, and $t$  rather than $t$ only: So, how can I predict the structure of the formula that relates $P$ with $T$, $H$, and $t$? Is there any approach that you advise me to follow? Sorry for this long question and any help from you is highly appreciated. I read many papers but I could not know how to solve the problem. thanks",,['statistics']
19,stationary non-isotropic spatial stochastic processes,stationary non-isotropic spatial stochastic processes,,"Are there any interesting examples of second order stationary processes on ${\mathcal R}^2$ or ${\mathcal R}^3$ that are not isotropic? The book I am looking at has no examples. Update : I asked this question in mathoverflow, apparently such examples are not easy to come by. Update: Processes with anisotropic variograms are examples of non-isotropic stationary processes.","Are there any interesting examples of second order stationary processes on ${\mathcal R}^2$ or ${\mathcal R}^3$ that are not isotropic? The book I am looking at has no examples. Update : I asked this question in mathoverflow, apparently such examples are not easy to come by. Update: Processes with anisotropic variograms are examples of non-isotropic stationary processes.",,"['probability-theory', 'statistics']"
20,Which geometric distribution to use?,Which geometric distribution to use?,,"I am learning about discrete probability distributions and found 2 definitions for Geometric Distributions from wikipedia : 1. The probability distribution of the number X of Bernoulli trials needed to get one success, supported on the set { 1, 2, 3, ...} 2. The probability distribution of the number Y = X − 1 of failures before the first success, supported on the set { 0, 1, 2, 3, ... } It seems like a subtle difference, but i'm having trouble wrapping my head around when i would use which? Any insights appreciated.","I am learning about discrete probability distributions and found 2 definitions for Geometric Distributions from wikipedia : 1. The probability distribution of the number X of Bernoulli trials needed to get one success, supported on the set { 1, 2, 3, ...} 2. The probability distribution of the number Y = X − 1 of failures before the first success, supported on the set { 0, 1, 2, 3, ... } It seems like a subtle difference, but i'm having trouble wrapping my head around when i would use which? Any insights appreciated.",,"['probability', 'statistics']"
21,Maximum combined 1st through nth highest combinations of ai + bi,Maximum combined 1st through nth highest combinations of ai + bi,,"I have two sets (a,b) of data and I am restricted to using the  values in the sets once and only once when they are combined. For example, the 2nd highest value possible from   a data set of a{3,2} and b{2,0} is a1+b2=3.  There are two possible ways of combining these two data sets:   (1) a1+b1 and a2+b2 which results in values of 5 and 2, and   (2) a1+b2 and a2+b1 which results in values of 3 and 4.  The (2) combination results in the highest (or maximum) 2nd-high value  possible of 3. Ergo, the 3rd highest value possible from   a data set of a{3,2,1} and b{4,3,0} is a1+b3=3.  There are six possible ways of combining these two data sets:  (1) a1+b1 followed by a2+b2  and a3+b3, which results in values of 7, 5 and 1, and  (2) a1+b1 followed by a2+b3  and a3+b2, which results in values of 7, 2 and 4, and  (3) a1+b2 followed by a2+b1  and a3+b3, which results in values of 6, 6 and 1, and  (4) a1+b2 followed by a2+b3  and a3+b1, which results in values of 5, 2 and 5, and  (5) a1+b3 followed by a2+b1  and a3+b2, which results in values of 3, 6 and 4, and  (6) a1+b3 followed by a2+b2  and a3+b1, which results in values of 3, 5 and 5.  The (5) and (6) combinations both result in a maximum 3rd-highest value of 3,    from a1+b3. By brute force and analogy, I have determined the maximum 1st through nth highest combination of ai+bi in two data sets can be determined as follows: Maximum             Formula  1st-high            a1+b1  2nd-high            min(a1+b2, a2+b1)  3rd-high            min(a1+b3, a2+b2, a3+b1)  4th-high            min(a1+b4, a2+b3, a3+b2, a4+b1)  ...  8th-high            min(a1+b8, a2+b7, a3+b6, a4+b5, a5+b4, a6+b3, a7+b2, a8+b1)  ...  nth-high            min(a1+bn, a2+b(n-1), ... , a(n-1)+b2, an+b1) I have a handle on the formula, what I want to know is  (a) where the answer to this question has been published, and (b) what it is formally described by in mathematics. I am sure this is not the first time this has come up. Is it addressed in a text book or in the literature? Thanks","I have two sets (a,b) of data and I am restricted to using the  values in the sets once and only once when they are combined. For example, the 2nd highest value possible from   a data set of a{3,2} and b{2,0} is a1+b2=3.  There are two possible ways of combining these two data sets:   (1) a1+b1 and a2+b2 which results in values of 5 and 2, and   (2) a1+b2 and a2+b1 which results in values of 3 and 4.  The (2) combination results in the highest (or maximum) 2nd-high value  possible of 3. Ergo, the 3rd highest value possible from   a data set of a{3,2,1} and b{4,3,0} is a1+b3=3.  There are six possible ways of combining these two data sets:  (1) a1+b1 followed by a2+b2  and a3+b3, which results in values of 7, 5 and 1, and  (2) a1+b1 followed by a2+b3  and a3+b2, which results in values of 7, 2 and 4, and  (3) a1+b2 followed by a2+b1  and a3+b3, which results in values of 6, 6 and 1, and  (4) a1+b2 followed by a2+b3  and a3+b1, which results in values of 5, 2 and 5, and  (5) a1+b3 followed by a2+b1  and a3+b2, which results in values of 3, 6 and 4, and  (6) a1+b3 followed by a2+b2  and a3+b1, which results in values of 3, 5 and 5.  The (5) and (6) combinations both result in a maximum 3rd-highest value of 3,    from a1+b3. By brute force and analogy, I have determined the maximum 1st through nth highest combination of ai+bi in two data sets can be determined as follows: Maximum             Formula  1st-high            a1+b1  2nd-high            min(a1+b2, a2+b1)  3rd-high            min(a1+b3, a2+b2, a3+b1)  4th-high            min(a1+b4, a2+b3, a3+b2, a4+b1)  ...  8th-high            min(a1+b8, a2+b7, a3+b6, a4+b5, a5+b4, a6+b3, a7+b2, a8+b1)  ...  nth-high            min(a1+bn, a2+b(n-1), ... , a(n-1)+b2, an+b1) I have a handle on the formula, what I want to know is  (a) where the answer to this question has been published, and (b) what it is formally described by in mathematics. I am sure this is not the first time this has come up. Is it addressed in a text book or in the literature? Thanks",,['algebra-precalculus']
22,How can you calculate the accuracy rate of the following set of statements?,How can you calculate the accuracy rate of the following set of statements?,,"If we can say that we're able to determine the weather at 10ms intervals, and if we observe that over a whole second (1000 samples) that it's raining at every sample, what is the accuracy rate within which we've calculated the weather for the second? I'm sure this sounds like homework to some, but it's not. I was reading a statement about sampling rates that claimed a sample every 10ms, and that the result was within 10% accuracy. I am not sure how they came up with this guarantee.","If we can say that we're able to determine the weather at 10ms intervals, and if we observe that over a whole second (1000 samples) that it's raining at every sample, what is the accuracy rate within which we've calculated the weather for the second? I'm sure this sounds like homework to some, but it's not. I was reading a statement about sampling rates that claimed a sample every 10ms, and that the result was within 10% accuracy. I am not sure how they came up with this guarantee.",,['statistics']
23,How to force a product of two i.i.d. random variable to be gaussian,How to force a product of two i.i.d. random variable to be gaussian,,"This question is related to this other question of mine : I realized that my original question was maybe too abitious, and I would like to discuss a much more limited version of it. Consider two real random variables indipendent and identically distributed ( i.i.d. ) $X,Y$ . Now consider the random variable $Z$ given by: $$Z=XY, \tag{1}$$ how should X and Y be (identically) distributed to have $Z$ be destributed as a gaussian (normal distribution) with mean equal to $0$ and variance equal to $\sigma ^2$ ? I found no theory on the topic: some bibliographical references, if they exist, will be also much apreciated.","This question is related to this other question of mine : I realized that my original question was maybe too abitious, and I would like to discuss a much more limited version of it. Consider two real random variables indipendent and identically distributed ( i.i.d. ) . Now consider the random variable given by: how should X and Y be (identically) distributed to have be destributed as a gaussian (normal distribution) with mean equal to and variance equal to ? I found no theory on the topic: some bibliographical references, if they exist, will be also much apreciated.","X,Y Z Z=XY, \tag{1} Z 0 \sigma ^2","['probability', 'statistics', 'random-variables', 'normal-distribution', 'independence']"
24,Would like to validate whether the AUC equation is correct or not,Would like to validate whether the AUC equation is correct or not,,"I found a paper ""Chapi, Kamran, et al. ""A novel hybrid artificial intelligence approach for flood susceptibility assessment."" Environmental modelling & software 95 (2017): 229-245"". The paper mentioned the equation of area under the ROC curve (AUROC) is: $$ AUROC = \frac{ \sum TP+\sum TN }{P+N} $$ Here, I don't want to argue anything since it is published in a good journal but I would like to have more discussion if the AUC can be easily calculated with the equation. What I know to calculate AUC is by using integral for area calculation. I tried to find the source of the equation, but I could not find the original one who proposed the equation and well-widely recognized. I would like to validate if the equation is correct to calculate AUC? I would be glad to appreciate any comments and thoughts, big thanks.","I found a paper ""Chapi, Kamran, et al. ""A novel hybrid artificial intelligence approach for flood susceptibility assessment."" Environmental modelling & software 95 (2017): 229-245"". The paper mentioned the equation of area under the ROC curve (AUROC) is: Here, I don't want to argue anything since it is published in a good journal but I would like to have more discussion if the AUC can be easily calculated with the equation. What I know to calculate AUC is by using integral for area calculation. I tried to find the source of the equation, but I could not find the original one who proposed the equation and well-widely recognized. I would like to validate if the equation is correct to calculate AUC? I would be glad to appreciate any comments and thoughts, big thanks.","
AUROC = \frac{ \sum TP+\sum TN }{P+N}
","['statistics', 'machine-learning', 'signal-processing', 'neural-networks']"
25,Difference between compensator of point process under real parameter an its MLE estimator,Difference between compensator of point process under real parameter an its MLE estimator,,"Suppose we have some point process $N=N_{\theta_0}$ on the real line, driven by a conditional intensity $\lambda_{\theta_0}$ dependent on some finite-dimensional parameter $\theta_0\in\Theta\subset\mathbb R^d$ . Given an observation of this point process $N$ on $[0,T]$ , we intend to estimate $\theta_0$ by its maximum likelihood estimator (MLE) $\hat\theta_T$ , which is the solution to $$0=\int_0^T\frac{\dot\lambda_\theta(t)}{\lambda_\theta(t)}[\mathrm dN(t)-\lambda_\theta(t)\mathrm dt].$$ Details about such procedures can be found in The asymptotic behaviour of maximum likelihood estimators for stationary point processes by Ogata, 1978. I'm interested in the compensator of point processes, i.e. $\Lambda_\theta(t)=\int_0^t\lambda_\theta(s)\ \mathrm ds$ . In particular, I was wondering whether something can be said about the difference $\Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T)$ . Does this satisfy some limiting equation? Is anything known about this in the literature? E.g. is $T^{-1/2}(\Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T))$ Gaussian, or does it satisfy some (functional) CLT? It is known that $\hat\theta_T\to\theta_0$ in probability as $T\to\infty$ (see e.g. Theorem 2 from the paper by Ogata; which, however, does not say anything about the speed of convergence). Under regularity conditions from the same paper, $\lambda_\theta$ is locally Lipschitz in $\theta$ , so $\lambda_{\hat\theta_T}\to\lambda_{\theta_0}$ . Then it should not be too difficult to say something about $T^{-1}(\Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T))$ , but I'm particularly interested in some nondegenerate limit for (something like) $$T^{-1/2}(\Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T)).$$ Any help or reference is much appreciated.","Suppose we have some point process on the real line, driven by a conditional intensity dependent on some finite-dimensional parameter . Given an observation of this point process on , we intend to estimate by its maximum likelihood estimator (MLE) , which is the solution to Details about such procedures can be found in The asymptotic behaviour of maximum likelihood estimators for stationary point processes by Ogata, 1978. I'm interested in the compensator of point processes, i.e. . In particular, I was wondering whether something can be said about the difference . Does this satisfy some limiting equation? Is anything known about this in the literature? E.g. is Gaussian, or does it satisfy some (functional) CLT? It is known that in probability as (see e.g. Theorem 2 from the paper by Ogata; which, however, does not say anything about the speed of convergence). Under regularity conditions from the same paper, is locally Lipschitz in , so . Then it should not be too difficult to say something about , but I'm particularly interested in some nondegenerate limit for (something like) Any help or reference is much appreciated.","N=N_{\theta_0} \lambda_{\theta_0} \theta_0\in\Theta\subset\mathbb R^d N [0,T] \theta_0 \hat\theta_T 0=\int_0^T\frac{\dot\lambda_\theta(t)}{\lambda_\theta(t)}[\mathrm dN(t)-\lambda_\theta(t)\mathrm dt]. \Lambda_\theta(t)=\int_0^t\lambda_\theta(s)\ \mathrm ds \Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T) T^{-1/2}(\Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T)) \hat\theta_T\to\theta_0 T\to\infty \lambda_\theta \theta \lambda_{\hat\theta_T}\to\lambda_{\theta_0} T^{-1}(\Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T)) T^{-1/2}(\Lambda_{\hat\theta_T}(T)-\Lambda_{\theta_0}(T)).","['statistics', 'stochastic-processes', 'central-limit-theorem', 'maximum-likelihood', 'point-processes']"
26,"FOB Poker: Probability of a sequence of numbers (with the existence of ""wild-card"" number)","FOB Poker: Probability of a sequence of numbers (with the existence of ""wild-card"" number)",,"At work we have 2 factor authentication using a fob that generates a sequence of 6 numbers 0-9. We started playing ""poker"" by having everyone generate a number at the same time and see who has poker type hands with the number sequence. There are some differences: Poker has a fixed deck so ""numbers"" do not repeat, but the fob repeats numbers obviously. We make the zeros ""wild."" There are 6 instead of 5 values. Therefore, we do not know how to rank some of the ""hands."" 2 questions: (1) Do these differences effect the probability of the standard (5 value) ""poker"" hands? (2) How do I calculate the probability of ""hands"" utilizing the 6th digit ( 3 pair, two triples, straight of 6 numbers, 6 of a kind,... etc.)? I would like to generate a ""hand"" ranking table to quell some arguments. Any help is greatly appreciated!","At work we have 2 factor authentication using a fob that generates a sequence of 6 numbers 0-9. We started playing ""poker"" by having everyone generate a number at the same time and see who has poker type hands with the number sequence. There are some differences: Poker has a fixed deck so ""numbers"" do not repeat, but the fob repeats numbers obviously. We make the zeros ""wild."" There are 6 instead of 5 values. Therefore, we do not know how to rank some of the ""hands."" 2 questions: (1) Do these differences effect the probability of the standard (5 value) ""poker"" hands? (2) How do I calculate the probability of ""hands"" utilizing the 6th digit ( 3 pair, two triples, straight of 6 numbers, 6 of a kind,... etc.)? I would like to generate a ""hand"" ranking table to quell some arguments. Any help is greatly appreciated!",,"['probability', 'statistics']"
27,Expectation of a piecewise const approximation based on Beta distribution,Expectation of a piecewise const approximation based on Beta distribution,,"Let $X_1, X_2 \stackrel{\text{iid}}{\sim}\mathrm{Uniform}(0,1)$ and then sort $X_1,X_2$ to get $X_{(1)} < X_{(2)}$ . Based on the pdfs of $X_{(i)}$ , we know $X_{(1)} \sim \mathrm{Beta}(1,2)$ and $X_{(2)} \sim \mathrm{Beta}(2,1)$ , with $\mathbb{E}(X_{(1)}) = \frac{1}{3}$ and $\mathbb{E}(X_{(2)}) = \frac{2}{3}$ . Consider the following piecewise constant approximation for the function $f(x) = x$ . Sample two points $x_1, x_2$ uniformly on $(0, 1)$ and them sort them to get $x_{(1)}$ and $x_{(2)}$ , $x_{(1)} <  x_{(2)}$ . Denote the expectation as $\mathbb{E}_1(\|f - c\|_2^2) = \mathbb{E}_1(\displaystyle\int_{x_{(1)}}^{x_{(2)}} (f(t)-c)^2 \; dt)$ , where $c = \frac{1}{x_{(2)}-x_{(1)}}\displaystyle\int_{x_{(1)}}^{x_{(2)}} f(t)\; dt$ is a constant. Sample $y_{(1)}$ from $\mathrm{Beta}(1,2)$ and $y_{(2)}$ from $\mathrm{Beta}(2,1)$ . Denote the Expectation as $\mathbb{E}_2(\|f - c\|_2^2) = \mathbb{E}_2(\displaystyle\int_{y_{(1)}}^{y_{(2)}} (f(t)-c)^2 \; dt)$ , where $c = \frac{1}{y_{(2)}-y_{(1)}}\displaystyle\int_{y_{(1)}}^{y_{(2)}} f(t)\; dt$ is a constant. (Note: the definitions of $\mathbb{E}_1$ and $\mathbb{E}_2$ are identical; the only difference is the method of obtaining the points $x_{(i)}, y_{(i)}$ .) Compare $\mathbb{E}_1(\|f - c\|_2^2)$ and $\mathbb{E}_2(\|f - c\|_2^2)$ . It's observed from the numerical experiments that $\mathbb{E}_1(\|f - c\|_2^2) < \mathbb{E}_2(\|f - c\|_2^2)$ . This result confuses me a lot. First, I expect them to be equal because both $x_{(i)}$ and $y_{(i)}$ from the same Beta distribution as discussed above. Second, even if $\; \mathbb{E}_1 \neq \mathbb{E}_2$ , since $y_{(1)}$ could be greater than $y_{(2)}$ , $\displaystyle\int_{y_{(1)}}^{y_{(2)}} (f(t)-c)^2 \; dt$ could be negative for some $y_{(i)}$ . However, we know $\int_{x_{(1)}}^{x_{(2)}} (f(t)-c)^2 \; dt > 0$ . Therefore, shouldn't it be $\mathbb{E}_1(\|f - c\|_2^2) > \mathbb{E}_2(\|f - c\|_2^2)$ ? I am confused by this result. Did I miss something? My experiments were done using Matlab with the built-in functions $\textit{rand}$ and $\textit{betarnd}$ .","Let and then sort to get . Based on the pdfs of , we know and , with and . Consider the following piecewise constant approximation for the function . Sample two points uniformly on and them sort them to get and , . Denote the expectation as , where is a constant. Sample from and from . Denote the Expectation as , where is a constant. (Note: the definitions of and are identical; the only difference is the method of obtaining the points .) Compare and . It's observed from the numerical experiments that . This result confuses me a lot. First, I expect them to be equal because both and from the same Beta distribution as discussed above. Second, even if , since could be greater than , could be negative for some . However, we know . Therefore, shouldn't it be ? I am confused by this result. Did I miss something? My experiments were done using Matlab with the built-in functions and .","X_1, X_2 \stackrel{\text{iid}}{\sim}\mathrm{Uniform}(0,1) X_1,X_2 X_{(1)} < X_{(2)} X_{(i)} X_{(1)} \sim \mathrm{Beta}(1,2) X_{(2)} \sim \mathrm{Beta}(2,1) \mathbb{E}(X_{(1)}) = \frac{1}{3} \mathbb{E}(X_{(2)}) = \frac{2}{3} f(x) = x x_1, x_2 (0, 1) x_{(1)} x_{(2)} x_{(1)} <  x_{(2)} \mathbb{E}_1(\|f - c\|_2^2) = \mathbb{E}_1(\displaystyle\int_{x_{(1)}}^{x_{(2)}} (f(t)-c)^2 \; dt) c = \frac{1}{x_{(2)}-x_{(1)}}\displaystyle\int_{x_{(1)}}^{x_{(2)}} f(t)\; dt y_{(1)} \mathrm{Beta}(1,2) y_{(2)} \mathrm{Beta}(2,1) \mathbb{E}_2(\|f - c\|_2^2) = \mathbb{E}_2(\displaystyle\int_{y_{(1)}}^{y_{(2)}} (f(t)-c)^2 \; dt) c = \frac{1}{y_{(2)}-y_{(1)}}\displaystyle\int_{y_{(1)}}^{y_{(2)}} f(t)\; dt \mathbb{E}_1 \mathbb{E}_2 x_{(i)}, y_{(i)} \mathbb{E}_1(\|f - c\|_2^2) \mathbb{E}_2(\|f - c\|_2^2) \mathbb{E}_1(\|f - c\|_2^2) < \mathbb{E}_2(\|f - c\|_2^2) x_{(i)} y_{(i)} \; \mathbb{E}_1 \neq \mathbb{E}_2 y_{(1)} y_{(2)} \displaystyle\int_{y_{(1)}}^{y_{(2)}} (f(t)-c)^2 \; dt y_{(i)} \int_{x_{(1)}}^{x_{(2)}} (f(t)-c)^2 \; dt > 0 \mathbb{E}_1(\|f - c\|_2^2) > \mathbb{E}_2(\|f - c\|_2^2) \textit{rand} \textit{betarnd}","['statistics', 'probability-distributions', 'approximation', 'uniform-distribution', 'order-statistics']"
28,Convergence of weighted sum to Brownian Motion,Convergence of weighted sum to Brownian Motion,,"Let $\{\varepsilon_t\}_{t = 1}^T$ be a sequence of iid random variables such that $\varepsilon_t \sim N(0, \sigma^2)$ and $\sigma^2 > 0$ . Then it is known that (see 17.3.6 in James Hamilton's Time Series Analysis) $T^{-1/2}\sum_{t = 1}^{[sT]}\varepsilon_t \to \sigma B(s)$ , where $s \in [0, 1]$ , $[sT]$ is the largest integer less than or equal to $sT$ , and $B(s)$ is a standard brownian motion. The convergence is weak and follows by the functional central limit theorem. What would the corresponding statement look like for $T^{-1/2}\sum_{t = 1}^{[sT]}w_t\varepsilon_t$ , where $\{w_t\}_{t = 1}^T$ is some sequence. What conditions need to be placed on this sequence? My conjecture is that $T^{-1/2}\sum_{t = 1}^{[sT]}w_t\varepsilon_t \to \lambda B(s)$ , where $\lambda = \sigma \sqrt{\sum_{t = 1}^\infty w_t^2}$ , so that the required condition would be $\sum_{t = 1}^\infty w_t^2 < \infty$ . However, I am not familiar enough with the concepts to show it. I know this discussion is not fully formal. I am only looking for references, and likewise informal answers. Thanks in advance!","Let be a sequence of iid random variables such that and . Then it is known that (see 17.3.6 in James Hamilton's Time Series Analysis) , where , is the largest integer less than or equal to , and is a standard brownian motion. The convergence is weak and follows by the functional central limit theorem. What would the corresponding statement look like for , where is some sequence. What conditions need to be placed on this sequence? My conjecture is that , where , so that the required condition would be . However, I am not familiar enough with the concepts to show it. I know this discussion is not fully formal. I am only looking for references, and likewise informal answers. Thanks in advance!","\{\varepsilon_t\}_{t = 1}^T \varepsilon_t \sim N(0, \sigma^2) \sigma^2 > 0 T^{-1/2}\sum_{t = 1}^{[sT]}\varepsilon_t \to \sigma B(s) s \in [0, 1] [sT] sT B(s) T^{-1/2}\sum_{t = 1}^{[sT]}w_t\varepsilon_t \{w_t\}_{t = 1}^T T^{-1/2}\sum_{t = 1}^{[sT]}w_t\varepsilon_t \to \lambda B(s) \lambda = \sigma \sqrt{\sum_{t = 1}^\infty w_t^2} \sum_{t = 1}^\infty w_t^2 < \infty","['probability', 'statistics', 'stochastic-processes', 'brownian-motion', 'time-series']"
29,Borel Cantelli Lemma and Almost Sure Convergence,Borel Cantelli Lemma and Almost Sure Convergence,,Let $X_n$ be a sequence of random variables where $n$ is a natural number such that $X_n \geq 0$ for all $n$ . Suppose that Var( $X_n) \leq n^{\frac{1}{2}}$ and E[ $X_n] = n$ Show that P( $X_n=0$ infinitely often) $=0$ . I am struggling to prove this. I think I have to use Markov's inequality because I have been told the random variables are non-negative,Let be a sequence of random variables where is a natural number such that for all . Suppose that Var( and E[ Show that P( infinitely often) . I am struggling to prove this. I think I have to use Markov's inequality because I have been told the random variables are non-negative,X_n n X_n \geq 0 n X_n) \leq n^{\frac{1}{2}} X_n] = n X_n=0 =0,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
30,How to show this discrete quadratic equation converges?,How to show this discrete quadratic equation converges?,,"So I have a discrete process $V_{k}=AV_{k-1}A^T+C$ where $C$ is a constant and $V$ is symmetric (this is supposed to be the update for the state covariance of a discrete stochastic process). I read that the above converges as $k$ goes to $\infty$ if $A$ has its eigenvalues in the unit disk. How can I show this? I know that $A^k$ will go to zero as $k$ goes to $\infty$ if its eigenvalues are in the unit disk so I think I can show that the $V$ dependent term goes to zero, but the constant $C$ terms are giving me trouble because there seems to be a summation of an unbounded amount of different $A^n C [A^T]^n$ terms as I consider larger and larger $k$ .","So I have a discrete process where is a constant and is symmetric (this is supposed to be the update for the state covariance of a discrete stochastic process). I read that the above converges as goes to if has its eigenvalues in the unit disk. How can I show this? I know that will go to zero as goes to if its eigenvalues are in the unit disk so I think I can show that the dependent term goes to zero, but the constant terms are giving me trouble because there seems to be a summation of an unbounded amount of different terms as I consider larger and larger .",V_{k}=AV_{k-1}A^T+C C V k \infty A A^k k \infty V C A^n C [A^T]^n k,"['linear-algebra', 'statistics', 'power-series']"
31,How to bet on individual games in best of seven series so that no matter how my team wins I make same profit? (Assume games are coin flips),How to bet on individual games in best of seven series so that no matter how my team wins I make same profit? (Assume games are coin flips),,"Suppose I have a 100 dollar starting bankroll. I want to bet this on my team to win a best of 7 series, however assume I am restricted to only being able to bet individual games one at a time as they occur, and money accumulates or subtracts from by bankroll after every game. Also assume that the chances of winning each game is 50% and each individual game bet pays 1:1. So for example, if I bet 20 dollars on game 1 and my team wins, I now have 120 dollars available to bet on the second game. If I lose I will only have 80 dollars available to bet on the second game. I want to know what betting strategy I should adopt (e.g. what percentage or amount from my bankroll should I bet on each game based on all previous games) so that no matter what happens, if my team wins the best of 7 series I will end up with the exact same amount of money (and if my team does not win the series, my bankroll will end up being 0). I've tried to solve this question but the tricky parts for me are 1) the fact that the series might terminate after fewer than 7 games and 2) the fact that the amount of money I have available to wager in each game fluctuates based on the outcome of the previous game. I assume in real life you can probably just bet on the outcomes of entire series, but this is just a purely theoretical problem I came up with in my head.","Suppose I have a 100 dollar starting bankroll. I want to bet this on my team to win a best of 7 series, however assume I am restricted to only being able to bet individual games one at a time as they occur, and money accumulates or subtracts from by bankroll after every game. Also assume that the chances of winning each game is 50% and each individual game bet pays 1:1. So for example, if I bet 20 dollars on game 1 and my team wins, I now have 120 dollars available to bet on the second game. If I lose I will only have 80 dollars available to bet on the second game. I want to know what betting strategy I should adopt (e.g. what percentage or amount from my bankroll should I bet on each game based on all previous games) so that no matter what happens, if my team wins the best of 7 series I will end up with the exact same amount of money (and if my team does not win the series, my bankroll will end up being 0). I've tried to solve this question but the tricky parts for me are 1) the fact that the series might terminate after fewer than 7 games and 2) the fact that the amount of money I have available to wager in each game fluctuates based on the outcome of the previous game. I assume in real life you can probably just bet on the outcomes of entire series, but this is just a purely theoretical problem I came up with in my head.",,"['probability', 'statistics', 'binomial-distribution', 'gambling']"
32,Binomial distribution in reliability theory,Binomial distribution in reliability theory,,"Do binomial distributions $Bin(n,p)$ always have increasing hazard rate?",Do binomial distributions always have increasing hazard rate?,"Bin(n,p)","['probability-theory', 'statistics', 'probability-distributions', 'binomial-distribution', 'reliability']"
33,"If $T$ is sufficient, then there's a probability $\mathbb{P}_0$ such [...] $\mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T)$ a.e. for all $P\in\mathfrak{M}$","If  is sufficient, then there's a probability  such [...]  a.e. for all",T \mathbb{P}_0 \mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T) P\in\mathfrak{M},"Let $(\Omega,\Sigma)$ be a measurable space and $\mathfrak{M}$ a family of probability measures. Suppose that exists a $\sigma$ -finite measure $\mu:\Sigma \to\overline{\mathbb{R}}$ such $P\ll \mu$ for all $P\in\mathfrak{M}$ . If $T$ is a sufficient statistic w.r.t. to $\mathfrak{M}$ , is at least one of the following propositions true? There's a probability measure $\mathbb{P}_0:\Sigma\to \mathbb{R}$ such that for all $X\in\cap _{P\in\mathfrak{M}}\mathcal{L}^1(P)$ we have that $X\in\mathcal{L}^1(\mathbb{P}_0)$ and $\mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T)$ $P$ -a.e. for all $P\in\mathfrak{M}$ . There's a probability measure $\mathbb{P}_0:\Sigma\to \mathbb{R}$ such that for all $X\in\mathcal{L}^1(\mathbb{P}_0)$ we have $X\in\mathcal{L}^1(P)$ and $\mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T)$ $P$ -a.e. for all $P\in\mathfrak{M}$ . Using the Theorem 2.4 which can be found at the page 38 of the book ""A Graduate Course on Statistical Inference"" written by Li and Babu (see this link ), I was able to prove that there's a probability measure $\mathbb{P}_0:\Sigma\to \mathbb{R}$ such for all $X\in(\cap _{P\in\mathfrak{M}}\mathcal{L}^1(P))\cap\mathcal{L}^1(\mathbb{P}_0)$ we have $\mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T)$ $P$ -a.e. for all $P\in\mathfrak{M}$ I also know that $\mathbb{P}_0 $ can be defined as $\mathbb{P}_0 :=\sum_{P\in\mathfrak{N}}c_PP$ in which $\mathfrak{N}$ is an enumerable subfamily of $\mathfrak{M}$ and $\{c_P\}_{P\in\mathfrak{N}}\subseteq (0,\infty )$ satisfies $\sum_{P\in\mathfrak{N}}c_P=1$ . Besides, $\mathfrak{N}$ has the following property: if $P(E)=0$ for all $P\in\mathfrak{N}$ , then $P(E)=0$ for all $P\in\mathfrak{M}$ . At least one of the proposition appears in some arguments in statistics (as you can see in this link , for instance) and in the page 41 of the book I mentioned (see the proof of the Theorem 2.5).","Let be a measurable space and a family of probability measures. Suppose that exists a -finite measure such for all . If is a sufficient statistic w.r.t. to , is at least one of the following propositions true? There's a probability measure such that for all we have that and -a.e. for all . There's a probability measure such that for all we have and -a.e. for all . Using the Theorem 2.4 which can be found at the page 38 of the book ""A Graduate Course on Statistical Inference"" written by Li and Babu (see this link ), I was able to prove that there's a probability measure such for all we have -a.e. for all I also know that can be defined as in which is an enumerable subfamily of and satisfies . Besides, has the following property: if for all , then for all . At least one of the proposition appears in some arguments in statistics (as you can see in this link , for instance) and in the page 41 of the book I mentioned (see the proof of the Theorem 2.5).","(\Omega,\Sigma) \mathfrak{M} \sigma \mu:\Sigma \to\overline{\mathbb{R}} P\ll \mu P\in\mathfrak{M} T \mathfrak{M} \mathbb{P}_0:\Sigma\to \mathbb{R} X\in\cap _{P\in\mathfrak{M}}\mathcal{L}^1(P) X\in\mathcal{L}^1(\mathbb{P}_0) \mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T) P P\in\mathfrak{M} \mathbb{P}_0:\Sigma\to \mathbb{R} X\in\mathcal{L}^1(\mathbb{P}_0) X\in\mathcal{L}^1(P) \mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T) P P\in\mathfrak{M} \mathbb{P}_0:\Sigma\to \mathbb{R} X\in(\cap _{P\in\mathfrak{M}}\mathcal{L}^1(P))\cap\mathcal{L}^1(\mathbb{P}_0) \mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T) P P\in\mathfrak{M} \mathbb{P}_0  \mathbb{P}_0 :=\sum_{P\in\mathfrak{N}}c_PP \mathfrak{N} \mathfrak{M} \{c_P\}_{P\in\mathfrak{N}}\subseteq (0,\infty ) \sum_{P\in\mathfrak{N}}c_P=1 \mathfrak{N} P(E)=0 P\in\mathfrak{N} P(E)=0 P\in\mathfrak{M}","['probability-theory', 'measure-theory', 'statistics', 'conditional-expectation']"
34,Lower bound of $\frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np}$,Lower bound of,\frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np},"According to Theorem 7.16 of High-Dimensional Statistics: A Non-Asymptotic Viewpoint (M. Wainwright, 2019), we know that for $\mathbf X\in\mathbb R^{n\times p}, X_{ij}\overset{iid}{\sim}N(0,1),$ there are universal positive constants $c_1 < 1 < c_2$ such that $$ \frac{\|\mathbf X\theta\|_2^2}{n}\geq c_1 \|\theta\|_2^2 - c_2 \frac{\log p}{n} \|\theta\|_1^2,\theta\in\mathbb R^p $$ with high probability. I want to derive the lower bound of the following, $$ \frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np}, \theta\in\mathbb R^{np} $$ The proof of Theorem 7.16 makes use of the Gordon-Slepian inequality and the variable $\dfrac{\langle u, \mathbf X v\rangle}{\sqrt n}$ is zero-mean Gaussian with variance $n^{-1}$ , where $u,v$ are unit vertors. While for $Z_{u,v} = \dfrac{\langle u, (\mathbf X \otimes \mathbf X^\top) v\rangle}{\sqrt n}$ , $Z_{u,v}$ is not a Gaussian variable and its mean is not zero, the techniques of Gaussian process cannot be used. If we consider $\theta = \theta _1 \otimes \theta_2, \theta _1 \in\mathbb R^{p}, \theta _2 \in\mathbb R^{n}$ , we can obtain that $$ \frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np}\geq c_1 c_1^\prime \|\theta\|_2^2 - c_2c_1^\prime \frac{\log p}{n} \|\theta_1\|_1^2 \|\theta_2\|_2^2 - c_2^\prime c_1 \frac{\log n}{p} \|\theta_2\|_1^2 \|\theta_1\|_2^2 $$ Since $\|\theta\|_1\geq\|\theta\|_2,$ we can get $$ \frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np}\geq \tilde c_1 \|\theta\|_2^2 - \tilde c_2 (\frac{\log p}{n} + \frac{\log n}{p}) \|\theta\|_1^2 $$ So I guess that the right-hand side of the inequality is the lower bound. But I don't know how to cope with $\theta$ without the Kronecker structure. Thanks for your help!!!","According to Theorem 7.16 of High-Dimensional Statistics: A Non-Asymptotic Viewpoint (M. Wainwright, 2019), we know that for there are universal positive constants such that with high probability. I want to derive the lower bound of the following, The proof of Theorem 7.16 makes use of the Gordon-Slepian inequality and the variable is zero-mean Gaussian with variance , where are unit vertors. While for , is not a Gaussian variable and its mean is not zero, the techniques of Gaussian process cannot be used. If we consider , we can obtain that Since we can get So I guess that the right-hand side of the inequality is the lower bound. But I don't know how to cope with without the Kronecker structure. Thanks for your help!!!","\mathbf X\in\mathbb R^{n\times p}, X_{ij}\overset{iid}{\sim}N(0,1), c_1 < 1 < c_2 
\frac{\|\mathbf X\theta\|_2^2}{n}\geq c_1 \|\theta\|_2^2 - c_2 \frac{\log p}{n} \|\theta\|_1^2,\theta\in\mathbb R^p
 
\frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np}, \theta\in\mathbb R^{np}
 \dfrac{\langle u, \mathbf X v\rangle}{\sqrt n} n^{-1} u,v Z_{u,v} = \dfrac{\langle u, (\mathbf X \otimes \mathbf X^\top) v\rangle}{\sqrt n} Z_{u,v} \theta = \theta _1 \otimes \theta_2, \theta _1 \in\mathbb R^{p}, \theta _2 \in\mathbb R^{n} 
\frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np}\geq c_1 c_1^\prime \|\theta\|_2^2 - c_2c_1^\prime \frac{\log p}{n} \|\theta_1\|_1^2 \|\theta_2\|_2^2 - c_2^\prime c_1 \frac{\log n}{p} \|\theta_2\|_1^2 \|\theta_1\|_2^2
 \|\theta\|_1\geq\|\theta\|_2, 
\frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np}\geq \tilde c_1 \|\theta\|_2^2 - \tilde c_2 (\frac{\log p}{n} + \frac{\log n}{p}) \|\theta\|_1^2
 \theta","['probability-theory', 'statistics', 'normal-distribution', 'upper-lower-bounds', 'random-matrices']"
35,What is Sørensen–Dice coefficient formula for 3 sets?,What is Sørensen–Dice coefficient formula for 3 sets?,,"What is actually Sørensen–Dice coefficient formula for 3 sets? for 2 sets, it is: $$\frac{2 \cdot |A \cap B|}{|A| + |B|}$$ But for 3 sets? is this right? : $$\frac{3 \cdot |A \cap B \cap C|}{|A| + |B| + |C|}$$ I searched but I cannot find an exact answer","What is actually Sørensen–Dice coefficient formula for 3 sets? for 2 sets, it is: But for 3 sets? is this right? : I searched but I cannot find an exact answer",\frac{2 \cdot |A \cap B|}{|A| + |B|} \frac{3 \cdot |A \cap B \cap C|}{|A| + |B| + |C|},"['statistics', 'reference-request']"
36,Uniform Convergence of Empirical Means: Universal Separability Implies Measurability,Uniform Convergence of Empirical Means: Universal Separability Implies Measurability,,"On page 38 of Pollard's book ""Convergence of Stochastic Processes"" one finds the following exercise (Problem 3 in Chapter II): Call a class of functions $\mathcal{F}$ universally separable if there exists a countable subclass $\mathcal{F}_0$ such that each $f$ in $\mathcal{F}$ can be written as a pointwise limit of a sequence in $\mathcal{F}_0$ . If $\mathcal{F}$ has an envelope $F$ for which $PF<\infty$ , prove that universal separability implies measurability of $\lVert P_n-P\rVert$ . Notes: $P$ is a probability measure on a set $S$ resp. on a $\sigma$ -algebra $\Sigma\subseteq\mathcal{P}(S)$ . $\mathcal{F}$ is a set of (measurable) functions mapping from $S$ into $\mathbb{R}$ . $P_n$ is the empirical probability measure that puts equal mass at each of the $n$ observations $\xi_1,\dots,\xi_n$ . More precisely: $$P_nf=\frac{1}{n}\sum\limits_{i=1}^n f(\xi_i)$$ for $f\in\mathcal{F}$ . $Pf$ denotes the expected value of $P_nf$ (if $Pf<\infty$ ). $\lVert P_n-P\rVert:=\sup\limits_{f\in\mathcal{F}}|P_nf-Pf|$ . An envelope $F$ for $\mathcal{F}$ is a measurable map $F\colon S\to\mathbb{R}$ such that $|f|\leq F$ for any $f\in\mathcal{F}$ . I personally am mostly interested in the case when $\mathcal{F}$ is a set of characteristic functions, i.e. functions whose image is a subset of {0,1}. In this case, $\mathcal{F}$ is universally separable, as witnessed by the countable subclass $\mathcal{F}_0$ , if for any $f\in\mathcal{F}$ there exists a sequence $(f_n)_n$ in $\mathcal{F}_0$ such that for any $s\in S$ there exists $n_s\in\mathbb{N}$ such that $f(s)=f_n(s)$ for any $n\geq n_s$ . Moreover, in my setting, I usually assume that $S$ is a topological space and $\Sigma$ is the corresponding Borel $\sigma$ -algebra generated by the topology. My attempts so far: First note that $P_n$ depends on $\underline{\xi}=(\xi_1,\dots,\xi_n)$ . Therefore, I denote by $P_n(\underline{\xi})f$ the empirical mean $$\frac{1}{n}\sum\limits_{i=1}^n f(\xi_i).$$ Further, we have $$Pf=\mathbb{E}_{\underline{\xi}\sim P^n}[P_n(\underline{\xi})f]=\mathbb{E}_{\xi\sim P}[f(\xi)],$$ where $P^n$ denotes the product measure $P\otimes\dots\otimes P$ . The map $$\lVert P_n-P\rVert\colon S^n\to\mathbb{R},\ \underline{\xi}\mapsto\sup\limits_{f\in\mathcal{F}}|P_n(\underline{\xi})f-Pf|$$ is measurable if for any $\varepsilon>0$ the set { $\underline{\xi}=(\xi_1,\dots,\xi_n)\in S^n\mid \lVert P_n(\underline{\xi})-P\rVert\leq\varepsilon$ } is a member of the product $\sigma$ -algebra $\Sigma^n=\Sigma\otimes\dots\otimes\Sigma$ . I suppose that the idea is to show that $$\sup\limits_{f\in\mathcal{F}}|P_n(\underline{\xi})(f)-Pf|=\sup\limits_{f\in\mathcal{F}_0}|P_n(\underline{\xi})(f)-Pf|.$$ However, I don't see how one can show this. Given $f\in\mathcal{F}$ and $\underline{\xi}\in S^n$ , the universal separability of $\mathcal{F}$ implies that there exists $f_0\in\mathcal{F}_0$ such that $P_n(\underline{\xi})f=P_n(\underline{\xi})f_0$ . However, I don't see how the subclass $\mathcal{F}_0$ can be used with regard to $Pf$ . I'm looking forward to comments and answers! Please let me know if I shall clarify some point.","On page 38 of Pollard's book ""Convergence of Stochastic Processes"" one finds the following exercise (Problem 3 in Chapter II): Call a class of functions universally separable if there exists a countable subclass such that each in can be written as a pointwise limit of a sequence in . If has an envelope for which , prove that universal separability implies measurability of . Notes: is a probability measure on a set resp. on a -algebra . is a set of (measurable) functions mapping from into . is the empirical probability measure that puts equal mass at each of the observations . More precisely: for . denotes the expected value of (if ). . An envelope for is a measurable map such that for any . I personally am mostly interested in the case when is a set of characteristic functions, i.e. functions whose image is a subset of {0,1}. In this case, is universally separable, as witnessed by the countable subclass , if for any there exists a sequence in such that for any there exists such that for any . Moreover, in my setting, I usually assume that is a topological space and is the corresponding Borel -algebra generated by the topology. My attempts so far: First note that depends on . Therefore, I denote by the empirical mean Further, we have where denotes the product measure . The map is measurable if for any the set { } is a member of the product -algebra . I suppose that the idea is to show that However, I don't see how one can show this. Given and , the universal separability of implies that there exists such that . However, I don't see how the subclass can be used with regard to . I'm looking forward to comments and answers! Please let me know if I shall clarify some point.","\mathcal{F} \mathcal{F}_0 f \mathcal{F} \mathcal{F}_0 \mathcal{F} F PF<\infty \lVert P_n-P\rVert P S \sigma \Sigma\subseteq\mathcal{P}(S) \mathcal{F} S \mathbb{R} P_n n \xi_1,\dots,\xi_n P_nf=\frac{1}{n}\sum\limits_{i=1}^n f(\xi_i) f\in\mathcal{F} Pf P_nf Pf<\infty \lVert P_n-P\rVert:=\sup\limits_{f\in\mathcal{F}}|P_nf-Pf| F \mathcal{F} F\colon S\to\mathbb{R} |f|\leq F f\in\mathcal{F} \mathcal{F} \mathcal{F} \mathcal{F}_0 f\in\mathcal{F} (f_n)_n \mathcal{F}_0 s\in S n_s\in\mathbb{N} f(s)=f_n(s) n\geq n_s S \Sigma \sigma P_n \underline{\xi}=(\xi_1,\dots,\xi_n) P_n(\underline{\xi})f \frac{1}{n}\sum\limits_{i=1}^n f(\xi_i). Pf=\mathbb{E}_{\underline{\xi}\sim P^n}[P_n(\underline{\xi})f]=\mathbb{E}_{\xi\sim P}[f(\xi)], P^n P\otimes\dots\otimes P \lVert P_n-P\rVert\colon S^n\to\mathbb{R},\ \underline{\xi}\mapsto\sup\limits_{f\in\mathcal{F}}|P_n(\underline{\xi})f-Pf| \varepsilon>0 \underline{\xi}=(\xi_1,\dots,\xi_n)\in S^n\mid \lVert P_n(\underline{\xi})-P\rVert\leq\varepsilon \sigma \Sigma^n=\Sigma\otimes\dots\otimes\Sigma \sup\limits_{f\in\mathcal{F}}|P_n(\underline{\xi})(f)-Pf|=\sup\limits_{f\in\mathcal{F}_0}|P_n(\underline{\xi})(f)-Pf|. f\in\mathcal{F} \underline{\xi}\in S^n \mathcal{F} f_0\in\mathcal{F}_0 P_n(\underline{\xi})f=P_n(\underline{\xi})f_0 \mathcal{F}_0 Pf","['measure-theory', 'statistics', 'uniform-convergence', 'borel-sets', 'empirical-processes']"
37,Deriving the CAPM pricing kernel from the general SDF and consumption-based kernel,Deriving the CAPM pricing kernel from the general SDF and consumption-based kernel,,"I'm reading the paper "" Quality minus junk "" by Asness et al. (2019) and trying to understand the pricing kernel definition they provide on page 6. The authors present the following pricing kernel: $$ \frac{M_{t+1}}{M_t} = \frac{1}{1+r^f} \left(1 + e^M_{t+1}\right) $$ where $r^f$ is the risk-free rate and $e^M_{t+1}$ is the zero-mean innovation to the pricing kernel. They then state that if the Capital Asset Pricing Model (CAPM) holds, the pricing kernel is: $$ e^M_{t+1} = -\lambda_t \left(\frac{r^{MKT}{t+1}-E_t(r^{MKT}{t+1})}{\sigma^2_t(r^{MKT}_{t+1})}\right) $$ where $\lambda_t=E_t(r^{MKT}_{t+1})-r^f$ is the market risk premium. I'm familiar with two other common forms of the pricing kernel: The general stochastic discount factor (SDF) equation from asset pricing theory can be seen here: Stochastic discount factor The consumption-based pricing kernel from the consumption CAPM: $$ m_{t+1} = \beta \frac{u'(c_{t+1})}{u'(c_t)} $$ My question is: How can I derive the two euqations used in the paper from either the general SDF equation or the consumption-based pricing kernel? I'd appreciate a step-by-step explanation of the derivation process. Additionally, I'm curious about the economic intuition behind the CAPM pricing kernel formula. How does it relate to the pricing kernel and to and to the basic CAPM model? Please let me know if you need further context from the paper or have any other questions.","I'm reading the paper "" Quality minus junk "" by Asness et al. (2019) and trying to understand the pricing kernel definition they provide on page 6. The authors present the following pricing kernel: where is the risk-free rate and is the zero-mean innovation to the pricing kernel. They then state that if the Capital Asset Pricing Model (CAPM) holds, the pricing kernel is: where is the market risk premium. I'm familiar with two other common forms of the pricing kernel: The general stochastic discount factor (SDF) equation from asset pricing theory can be seen here: Stochastic discount factor The consumption-based pricing kernel from the consumption CAPM: My question is: How can I derive the two euqations used in the paper from either the general SDF equation or the consumption-based pricing kernel? I'd appreciate a step-by-step explanation of the derivation process. Additionally, I'm curious about the economic intuition behind the CAPM pricing kernel formula. How does it relate to the pricing kernel and to and to the basic CAPM model? Please let me know if you need further context from the paper or have any other questions.","
\frac{M_{t+1}}{M_t} = \frac{1}{1+r^f} \left(1 + e^M_{t+1}\right)
 r^f e^M_{t+1} 
e^M_{t+1} = -\lambda_t \left(\frac{r^{MKT}{t+1}-E_t(r^{MKT}{t+1})}{\sigma^2_t(r^{MKT}_{t+1})}\right)
 \lambda_t=E_t(r^{MKT}_{t+1})-r^f 
m_{t+1} = \beta \frac{u'(c_{t+1})}{u'(c_t)}
","['probability', 'statistics', 'finance']"
38,Why does the standard deviation have all the properties it does? [closed],Why does the standard deviation have all the properties it does? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 months ago . Improve this question Currently in a probability class and am having a hard time wrapping my head around the standard deviation. In many ways it feels ""made up"" to me; let me explain: $$\sqrt{a^2 + b^2} \neq a + b$$ So unless it's used relative to other standard deviations, I feel like it wouldn't inherently mean anything. By that I mean, measurements like meters/inches have some grounding in ""reality."" Knowing something has a length of 3 m is useful information even if I don't have any other lengths in meters to compare it to. But when I see that a distribution has a SD of 2, that information only translates to my understanding of how ""spread"" the distribution is when I compare it to other distributions with different SDs. I think, ""Oh distribution 1 is pretty spread out and it has an SD of ___ so this other SD must mean the corresponding distribution 2 is less/more spread out."" Is this just how it is? Are some units of measurement just less intuitive and derive meaning from comparison? To me, it would make more sense to take the mean of the absolute values of all the deviations from the mean, but my professor told me that SD just happens to be more useful. I don't doubt this, and I am aware of the many properties that makes SD more useful, but my question is why? What causes SD to have those properties? For example, what causes most of the points in a normal distribution to land within 2 SD of the mean? It can't be just a coincidence or happenstance; there must be a reason. Any help would be appreciated. Thanks in advance!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 months ago . Improve this question Currently in a probability class and am having a hard time wrapping my head around the standard deviation. In many ways it feels ""made up"" to me; let me explain: So unless it's used relative to other standard deviations, I feel like it wouldn't inherently mean anything. By that I mean, measurements like meters/inches have some grounding in ""reality."" Knowing something has a length of 3 m is useful information even if I don't have any other lengths in meters to compare it to. But when I see that a distribution has a SD of 2, that information only translates to my understanding of how ""spread"" the distribution is when I compare it to other distributions with different SDs. I think, ""Oh distribution 1 is pretty spread out and it has an SD of ___ so this other SD must mean the corresponding distribution 2 is less/more spread out."" Is this just how it is? Are some units of measurement just less intuitive and derive meaning from comparison? To me, it would make more sense to take the mean of the absolute values of all the deviations from the mean, but my professor told me that SD just happens to be more useful. I don't doubt this, and I am aware of the many properties that makes SD more useful, but my question is why? What causes SD to have those properties? For example, what causes most of the points in a normal distribution to land within 2 SD of the mean? It can't be just a coincidence or happenstance; there must be a reason. Any help would be appreciated. Thanks in advance!",\sqrt{a^2 + b^2} \neq a + b,"['probability', 'statistics', 'intuition', 'standard-deviation']"
39,Minimum variance unbiased estimator for $\mu$ in Normal location model with known but random variance,Minimum variance unbiased estimator for  in Normal location model with known but random variance,\mu,"Consider observing $X \mid \sigma \sim N(\mu, \sigma^2)$ and $\sigma \sim F$ for some known distribution $F$ supported on the positive reals. We observe a single draw $(X, \sigma)$ . An estimator $T(X, \sigma)$ is unbiased for $\mu$ if $E[T(X,\sigma)] = \mu$ . $T(X, \sigma) = X$ is one such choice. Another choice is $T(X,\sigma) = w(\sigma) X$ for any $w(\sigma)$ where $E[w(\sigma)] = 1$ . Similarly, one could construct some unbiased estimator that uses $X^2, \ldots$ . There may be other more exotic choices as well. Among these estimators, what is the minimum variance estimator? The Cramer-Rao bound for $\mu$ with likelihood $f(x,\sigma) = f(\sigma) \frac{1}{\sigma} \varphi((x-\mu)/ \sigma)$ is $1/E[1/\sigma^2]$ . This is a lower bound on the minimum variance. Is there an estimator that achieves it?","Consider observing and for some known distribution supported on the positive reals. We observe a single draw . An estimator is unbiased for if . is one such choice. Another choice is for any where . Similarly, one could construct some unbiased estimator that uses . There may be other more exotic choices as well. Among these estimators, what is the minimum variance estimator? The Cramer-Rao bound for with likelihood is . This is a lower bound on the minimum variance. Is there an estimator that achieves it?","X \mid \sigma \sim N(\mu, \sigma^2) \sigma \sim F F (X, \sigma) T(X, \sigma) \mu E[T(X,\sigma)] = \mu T(X, \sigma) = X T(X,\sigma) = w(\sigma) X w(\sigma) E[w(\sigma)] = 1 X^2, \ldots \mu f(x,\sigma) = f(\sigma) \frac{1}{\sigma} \varphi((x-\mu)/ \sigma) 1/E[1/\sigma^2]","['probability', 'statistics', 'normal-distribution', 'parameter-estimation', 'sufficient-statistics']"
40,Are cumulants the only additive functions of independent random variables?,Are cumulants the only additive functions of independent random variables?,,"For a random variable $X$ , the cumulant generating function $CGF_X$ is defined as $CGF_X(t)=\log Ee^{tX}$ , and the nth cumulant $k_n(X)$ is defined as the coefficient of $t^n/n!$ in the corresponding power series. The cumulant $k_n$ has the following properties: $k_n(X+Y)=k_n(X)+k_n(Y)$ if X and Y are independent (additivity) $k_n(cX)=c^nk_n(X)$ for any scalar $c$ (homogeneity) $k_n(X)=p_n(EX, EX^2,\dots, EX^n)$ where $p_n$ is a universal polynomial (i.e. does not depend on X) Now suppose I have some other function $k'$ that satisfies properties 1-3. Is k' necessarily a scalar multiple of $k_n$ ? Motivation: The higher order cumulants can be somewhat mysterious-having a characterization like the above would make them seem much more natural. Alternatively, it would be interesting if there are other invariant polynomials besides the cumulants.","For a random variable , the cumulant generating function is defined as , and the nth cumulant is defined as the coefficient of in the corresponding power series. The cumulant has the following properties: if X and Y are independent (additivity) for any scalar (homogeneity) where is a universal polynomial (i.e. does not depend on X) Now suppose I have some other function that satisfies properties 1-3. Is k' necessarily a scalar multiple of ? Motivation: The higher order cumulants can be somewhat mysterious-having a characterization like the above would make them seem much more natural. Alternatively, it would be interesting if there are other invariant polynomials besides the cumulants.","X CGF_X CGF_X(t)=\log Ee^{tX} k_n(X) t^n/n! k_n k_n(X+Y)=k_n(X)+k_n(Y) k_n(cX)=c^nk_n(X) c k_n(X)=p_n(EX, EX^2,\dots, EX^n) p_n k' k_n","['probability', 'statistics', 'moment-generating-functions', 'invariant-theory', 'cumulants']"
41,Deriving the Likelihood Ratio for a Normal Distribution Hypothesis Testing,Deriving the Likelihood Ratio for a Normal Distribution Hypothesis Testing,,"I'm exploring the likelihood ratio principle in hypothesis testing, specifically within the context of normal distributions, and I've encountered a challenge in deriving a specific likelihood ratio. The principle is typically used to select a suitable statistic for testing hypotheses, where we compare the likelihood of the data under the null hypothesis against an alternative hypothesis. Consider a scenario where we have a set of samples $x_1, \ldots, x_n$ that are independently and identically distributed (i.i.d.) from a normal distribution $N(\mu, \sigma^2)$ with unknown parameters $\mu$ and $\sigma^2$ . We're interested in testing the null hypothesis $H_0: \mu = \mu_0$ against the alternative $H_a: \mu \neq \mu_0$ , where $\mu_0$ is a specified value. Let $L(\widehat{\Omega}_0)$ denote the maximum likelihood of observing the samples given $\mu = \mu_0$ , and let $L(\widehat{\Omega})$ denote the maximum likelihood over all possible values of $\mu$ and $\sigma^2$ . According to the likelihood ratio principle, the rejection region for $H_0$ is determined by the ratio $\frac{L(\widehat{\Omega}_0)}{L(\widehat{\Omega})}$ being less than or equal to a critical value $c$ , which is chosen based on the desired level of statistical significance $\alpha$ . I am trying to show that, for this particular setup, the likelihood ratio simplifies to $\left(1 + \frac{t^2}{n-1}\right)^{-\frac{n}{2}}$ , where $t$ is the test statistic defined as $t = \frac{\overline{x} - \mu_0}{s / \sqrt{n}}$ , with $\overline{x}$ being the sample mean and $s^2$ the unbiased sample variance. I've made several attempts to derive this expression from the definition of the likelihood ratio, considering the probability density function of the normal distribution, but I'm not sure how to proceed. Could someone guide me through the derivation or point out any resources that could help with understanding this specific case of the likelihood ratio in hypothesis testing for normal distributions? $\overline{x}= \frac{1}{n}\Sigma_{i=1}^{n} x_i $ and $s^2 = \frac{1}{n-1}\Sigma_{i=1}^{n}\left(x_i-\overline{x} \right)^2 $","I'm exploring the likelihood ratio principle in hypothesis testing, specifically within the context of normal distributions, and I've encountered a challenge in deriving a specific likelihood ratio. The principle is typically used to select a suitable statistic for testing hypotheses, where we compare the likelihood of the data under the null hypothesis against an alternative hypothesis. Consider a scenario where we have a set of samples that are independently and identically distributed (i.i.d.) from a normal distribution with unknown parameters and . We're interested in testing the null hypothesis against the alternative , where is a specified value. Let denote the maximum likelihood of observing the samples given , and let denote the maximum likelihood over all possible values of and . According to the likelihood ratio principle, the rejection region for is determined by the ratio being less than or equal to a critical value , which is chosen based on the desired level of statistical significance . I am trying to show that, for this particular setup, the likelihood ratio simplifies to , where is the test statistic defined as , with being the sample mean and the unbiased sample variance. I've made several attempts to derive this expression from the definition of the likelihood ratio, considering the probability density function of the normal distribution, but I'm not sure how to proceed. Could someone guide me through the derivation or point out any resources that could help with understanding this specific case of the likelihood ratio in hypothesis testing for normal distributions? and","x_1, \ldots, x_n N(\mu, \sigma^2) \mu \sigma^2 H_0: \mu = \mu_0 H_a: \mu \neq \mu_0 \mu_0 L(\widehat{\Omega}_0) \mu = \mu_0 L(\widehat{\Omega}) \mu \sigma^2 H_0 \frac{L(\widehat{\Omega}_0)}{L(\widehat{\Omega})} c \alpha \left(1 + \frac{t^2}{n-1}\right)^{-\frac{n}{2}} t t = \frac{\overline{x} - \mu_0}{s / \sqrt{n}} \overline{x} s^2 \overline{x}= \frac{1}{n}\Sigma_{i=1}^{n} x_i  s^2 = \frac{1}{n-1}\Sigma_{i=1}^{n}\left(x_i-\overline{x} \right)^2 ","['statistics', 'statistical-inference', 'hypothesis-testing']"
42,Question on literature for contraction rates,Question on literature for contraction rates,,"I read in some lecture notes the following definition of contraction rate: Definition (Posterior rate of contraction) The posterior distribution $\Pi_n\left(\cdot \mid X^{(n)}\right)$ is said to contract at rate $\epsilon_n \rightarrow 0$ at $\theta_0 \in \Theta$ if $\Pi_n\left(\theta: d\left(\theta, \theta_0\right)>M \epsilon_n \mid X^{(n)}\right) \rightarrow 0$ in $P_{\theta_0}^{(n)}$ probability, for a sufficiently large constant $M$ as $n \rightarrow \infty$ . Q : Is there literature that treats results of the type: For any $\eta >0$ , $$\Pi_n\left(\theta: d\left(\theta, \theta_0\right)> \eta \epsilon_n \mid X^{(n)}\right) \rightarrow 0$$ in $P_{\theta_0}^{(n)}$ probability,  as $n \rightarrow \infty$ . This is a slightly stronger type of contraction, but I could not find anything in the literature. Any reference is highly appreciated. What I am really interested in is: under which conditions, under the posterior: $$ \epsilon_n^{-1} d\left(\theta, \theta_0\right) \rightarrow 0 $$","I read in some lecture notes the following definition of contraction rate: Definition (Posterior rate of contraction) The posterior distribution is said to contract at rate at if in probability, for a sufficiently large constant as . Q : Is there literature that treats results of the type: For any , in probability,  as . This is a slightly stronger type of contraction, but I could not find anything in the literature. Any reference is highly appreciated. What I am really interested in is: under which conditions, under the posterior:","\Pi_n\left(\cdot \mid X^{(n)}\right) \epsilon_n \rightarrow 0 \theta_0 \in \Theta \Pi_n\left(\theta: d\left(\theta, \theta_0\right)>M \epsilon_n \mid X^{(n)}\right) \rightarrow 0 P_{\theta_0}^{(n)} M n \rightarrow \infty \eta >0 \Pi_n\left(\theta: d\left(\theta, \theta_0\right)> \eta \epsilon_n \mid X^{(n)}\right) \rightarrow 0 P_{\theta_0}^{(n)} n \rightarrow \infty  \epsilon_n^{-1} d\left(\theta, \theta_0\right) \rightarrow 0 ","['probability-theory', 'measure-theory', 'statistics', 'bayesian', 'nonparametric-statistics']"
43,Does this show $\bar{Y}$ is a sufficient statistic?,Does this show  is a sufficient statistic?,\bar{Y},"Question If $Y_1,\dots, Y_n \sim \; \textrm{iid geometric(p)}$ , show that $\bar{Y}$ is a sufficient statistic for p. My work Factorization Theorem If $Y_1, \dots, Y_n \sim \; \textrm{iid}$ then U is a sufficient statistic if $$ L(\theta) = g(U,\theta)h(Y_1,\dots,Y_n) $$ My actual work The likelihood function for $Y_1, \dots, Y_n$ is \begin{align} L(p) &= \prod_{i=1}^n{f_{Y_i}(y)} = \begin{bmatrix} p(1-p)^{y_i-1} \end{bmatrix}^n \notag \\ &= p^{n}(1-p)^{\sum_{i=1}^n{(y_i - 1)}} \notag \\ &= p^{n}(1-p)^{(n\bar{Y} - n)} = p^n(1-p)^{n(\bar{Y} - 1)} \notag \end{align} Applying the above factorization theorem gives us \begin{align} &g(\bar{Y},p) = p^n(1-p)^{n(\bar{Y} - 1)} \notag \\ &h(Y_1, \dots, Y_n) = 1 \notag \end{align} Therefore, we conclude that $\bar{Y}$ is a sufficient statistic for p.","Question If , show that is a sufficient statistic for p. My work Factorization Theorem If then U is a sufficient statistic if My actual work The likelihood function for is Applying the above factorization theorem gives us Therefore, we conclude that is a sufficient statistic for p.","Y_1,\dots, Y_n \sim \; \textrm{iid geometric(p)} \bar{Y} Y_1, \dots, Y_n \sim \; \textrm{iid} 
L(\theta) = g(U,\theta)h(Y_1,\dots,Y_n)
 Y_1, \dots, Y_n \begin{align}
L(p) &= \prod_{i=1}^n{f_{Y_i}(y)} = \begin{bmatrix} p(1-p)^{y_i-1} \end{bmatrix}^n \notag \\
&= p^{n}(1-p)^{\sum_{i=1}^n{(y_i - 1)}} \notag \\
&= p^{n}(1-p)^{(n\bar{Y} - n)} = p^n(1-p)^{n(\bar{Y} - 1)} \notag
\end{align} \begin{align}
&g(\bar{Y},p) = p^n(1-p)^{n(\bar{Y} - 1)} \notag \\
&h(Y_1, \dots, Y_n) = 1 \notag
\end{align} \bar{Y}","['probability', 'statistics', 'probability-distributions', 'sufficient-statistics']"
44,N perfect Logicians box opening game,N perfect Logicians box opening game,,"Question: There is a game that involves $n$ ordered boxes each with a hidden value associated with it. The value is sampled from a probability distribution density function $P(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2}$ . You observe each box’s value in order from box 1 to $n$ . After observing any given box’s value, you must decide to pick it or leave it, forever discarding the option to choose its value again. A player wins this game if and only if they pick the box with the highest sampled value. There is a group of perfect Logicians $L_1, L_2, \ldots, L_m$ competing in the game. The only information they know is that they are all perfect logicians, the rules of the game, and if any other logicians have lost or won the game. They play the game in order starting with $L_1$ . The game resets for each logician, but the random sampling is the same. a) Prove that if $m > n$ , a win would always occur b) We find out that only $L_m$ wins the game. What is the winning box? Give your answer in terms of $n, m$ . Thoughts so far: Initially, I was confused about how any given logician is influenced by the results of previous logicians since they can't see the outcome of their decisions, only whether they won or lost. My best guess is that, for instance, if Logician 2 knows Logician 1 lost, then Logician 2 can infer that the optimal choice Logician 1 would have made led to failure. Therefore, by the time Logician 2 encounters the box that Logician 1 would have chosen using the optimal strategy, he would know that this particular box was not the winning choice. Additionally, logicians can observe the number of failures preceding their turn, although I’m not sure how this would impact their decision-making process. For part a, my intuition tells me that logicians can always deduce the choices made by their predecessors because the outcome is deterministic, regardless of the random seed. Since each logician knows the choices of the previous logicians, once the number of logicians $m$ equals the number of boxes $n$ , they would, by the pigeonhole principle, be guaranteed to find the solution. For part b, I'm uncertain how the distribution of random sampling affects the optimal stopping problem. Is there a more optimal strategy when the distribution is known? Furthermore, once an optimal strategy is discovered, how does this information benefit subsequent logicians in their turn order? Edit: The logicians are not working together nor have they discussed a strategy beforehand together. They wish only to maximize their own chances of winning","Question: There is a game that involves ordered boxes each with a hidden value associated with it. The value is sampled from a probability distribution density function . You observe each box’s value in order from box 1 to . After observing any given box’s value, you must decide to pick it or leave it, forever discarding the option to choose its value again. A player wins this game if and only if they pick the box with the highest sampled value. There is a group of perfect Logicians competing in the game. The only information they know is that they are all perfect logicians, the rules of the game, and if any other logicians have lost or won the game. They play the game in order starting with . The game resets for each logician, but the random sampling is the same. a) Prove that if , a win would always occur b) We find out that only wins the game. What is the winning box? Give your answer in terms of . Thoughts so far: Initially, I was confused about how any given logician is influenced by the results of previous logicians since they can't see the outcome of their decisions, only whether they won or lost. My best guess is that, for instance, if Logician 2 knows Logician 1 lost, then Logician 2 can infer that the optimal choice Logician 1 would have made led to failure. Therefore, by the time Logician 2 encounters the box that Logician 1 would have chosen using the optimal strategy, he would know that this particular box was not the winning choice. Additionally, logicians can observe the number of failures preceding their turn, although I’m not sure how this would impact their decision-making process. For part a, my intuition tells me that logicians can always deduce the choices made by their predecessors because the outcome is deterministic, regardless of the random seed. Since each logician knows the choices of the previous logicians, once the number of logicians equals the number of boxes , they would, by the pigeonhole principle, be guaranteed to find the solution. For part b, I'm uncertain how the distribution of random sampling affects the optimal stopping problem. Is there a more optimal strategy when the distribution is known? Furthermore, once an optimal strategy is discovered, how does this information benefit subsequent logicians in their turn order? Edit: The logicians are not working together nor have they discussed a strategy beforehand together. They wish only to maximize their own chances of winning","n P(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2} n L_1, L_2, \ldots, L_m L_1 m > n L_m n, m m n","['statistics', 'logic', 'game-theory']"
45,Almost Sure Convergence of Order Statistics,Almost Sure Convergence of Order Statistics,,"Let $F$ be a strictly increasing distribution function. For a given $\tau \in (0,1)$ , suppose there exists $\epsilon_{\tau}$ such that $F(\epsilon_{\tau}) = \tau$ . Considering a set of independent and identically distributed (i.i.d.) random variables $X_{1}, \cdots, X_{n}$ , with their order statistics denoted by $X_{(1)}, \cdots, X_{(n)}$ , and let $k = [n\tau]$ (where $[ \cdot ]$ denotes the floor function). I got the following statement from the post https://stats.stackexchange.com/a/373530/409031 . Specifically, it is stated that $$X_{(k)} \to \epsilon_\tau$$ almost surely as $n$ approaches infinity. Could someone provide a detailed explanation or proof of this form of convergence? I have some basic understanding of order statistics and the concept of almost sure convergence. However, I am struggling to connect these concepts to establish the proof or detailed explanation of this particular scenario. Any insights or step-by-step explanations would be greatly appreciated.","Let be a strictly increasing distribution function. For a given , suppose there exists such that . Considering a set of independent and identically distributed (i.i.d.) random variables , with their order statistics denoted by , and let (where denotes the floor function). I got the following statement from the post https://stats.stackexchange.com/a/373530/409031 . Specifically, it is stated that almost surely as approaches infinity. Could someone provide a detailed explanation or proof of this form of convergence? I have some basic understanding of order statistics and the concept of almost sure convergence. However, I am struggling to connect these concepts to establish the proof or detailed explanation of this particular scenario. Any insights or step-by-step explanations would be greatly appreciated.","F \tau \in (0,1) \epsilon_{\tau} F(\epsilon_{\tau}) = \tau X_{1}, \cdots, X_{n} X_{(1)}, \cdots, X_{(n)} k = [n\tau] [ \cdot ] X_{(k)} \to \epsilon_\tau n","['probability-theory', 'statistics', 'convergence-divergence', 'order-statistics']"
46,How to estimate the best variance-proxy of a sub-Gaussian distribution from data?,How to estimate the best variance-proxy of a sub-Gaussian distribution from data?,,"Suppose we have $N$ independently identically distributed (i.i.d.) samples $X_1,\cdots,X_N$ generated from a sub-Gaussian random variable $X \sim \mathbb{P}$ . Then by definition there exists the smallest $\sigma >0$ such that $$\mathbb{E}[\exp(\theta (X - \mathbb{E}[X]))] \le \exp(\theta^2 \sigma^2/2) ~~ \forall \theta \in \mathbb{R}.$$ My question is that how can we estimate such $\sigma$ from the $N$ i.i.d. samples? What statistical properties can we expect for the estimator? My attempt (a natural idea) is to use the definition with the empirical distribution. Let $\bar X$ be the sample mean and solve the following optimization problems $$\hat \sigma_1 := \sup_{\theta >0} \frac{\sqrt{2\ln (\frac{1}{N} \sum_{i=1}^N e^{\theta(X_i - \bar X)})}}{\theta}$$ and $$\hat \sigma_2 := \sup_{\theta >0} \frac{\sqrt{2\ln (\frac{1}{N} \sum_{i=1}^N e^{\theta(\bar X - X_i)})}}{\theta}$$ and then take the maximum of the two, i.e., let $\hat \sigma := \max(\hat \sigma_1, \hat \sigma_2)$ . In this way, we obtain the smallest variance-proxy for the empirical distribution. However, this optimization problems have no closed-form solution. Hence, it is difficult to prove unbiasedness, etc. Is there any other method for estimating the variance-proxy from the data with nice statistical properties? It would be appreciated if there is any reference paper or note. Thank you!","Suppose we have independently identically distributed (i.i.d.) samples generated from a sub-Gaussian random variable . Then by definition there exists the smallest such that My question is that how can we estimate such from the i.i.d. samples? What statistical properties can we expect for the estimator? My attempt (a natural idea) is to use the definition with the empirical distribution. Let be the sample mean and solve the following optimization problems and and then take the maximum of the two, i.e., let . In this way, we obtain the smallest variance-proxy for the empirical distribution. However, this optimization problems have no closed-form solution. Hence, it is difficult to prove unbiasedness, etc. Is there any other method for estimating the variance-proxy from the data with nice statistical properties? It would be appreciated if there is any reference paper or note. Thank you!","N X_1,\cdots,X_N X \sim \mathbb{P} \sigma >0 \mathbb{E}[\exp(\theta (X - \mathbb{E}[X]))] \le \exp(\theta^2 \sigma^2/2) ~~ \forall \theta \in \mathbb{R}. \sigma N \bar X \hat \sigma_1 := \sup_{\theta >0} \frac{\sqrt{2\ln (\frac{1}{N} \sum_{i=1}^N e^{\theta(X_i - \bar X)})}}{\theta} \hat \sigma_2 := \sup_{\theta >0} \frac{\sqrt{2\ln (\frac{1}{N} \sum_{i=1}^N e^{\theta(\bar X - X_i)})}}{\theta} \hat \sigma := \max(\hat \sigma_1, \hat \sigma_2)","['statistics', 'statistical-inference', 'parameter-estimation', 'descriptive-statistics']"
47,Convergence in Distribution of a Particular Sample Average,Convergence in Distribution of a Particular Sample Average,,"Suppose $g_{n}(\cdot)$ defined on $[0,1]$ converges in distribution to a continuous Gaussian process. Let $U_{1},...,U_{n}$ be i.i.d. random variables following $\text{Unif}[0,1]$ .  Allow $g_{n}$ to be correlated with the $U_{i}$ s; for instance, $g_{n}(x)=\frac{1}{\sqrt{n}}\sum_{i}1(U_{i}\leq x)$ . Is the following statement correct? $$\frac{1}{n}\sum_{i}g_{n}(U_{i}) \text{ has the same asymptotic distribution as } \int_{[0,1]} g_{n}(u)du.$$ I feel the statement is true but I'm not sure if my proof is correct: Let $\mu_{n}$ be the empirical c.d.f. for $U_{i}$ . We can rewrite the average $\frac{1}{n}\sum_{i}g_{n}(U_{i})$ as $\int_{[0,1]}g_{n}(u)\mu_{n}(du)$ .Hence, the difference of the considered two terms is $$\int_{[0,1]} g_{n}(u)(\mu_{n}-\mu)(du)\equiv f(g_{n},\mu_{n}-\mu).$$ By Glivenko-Cantelli, $\mu_{n}\to \mu$ almost surely where $\mu$ is the c.d.f. of $\text{Unif}[0,1]$ . By convergence in distribution of $g_{n}(u)$ and by continuity of the functional $f$ , the continuous mapping theorem implies that $f(g_{n},\mu_{n}-\mu)$ converges to 0 in probability. Is there any error in my proof? Thank you very much.","Suppose defined on converges in distribution to a continuous Gaussian process. Let be i.i.d. random variables following .  Allow to be correlated with the s; for instance, . Is the following statement correct? I feel the statement is true but I'm not sure if my proof is correct: Let be the empirical c.d.f. for . We can rewrite the average as .Hence, the difference of the considered two terms is By Glivenko-Cantelli, almost surely where is the c.d.f. of . By convergence in distribution of and by continuity of the functional , the continuous mapping theorem implies that converges to 0 in probability. Is there any error in my proof? Thank you very much.","g_{n}(\cdot) [0,1] U_{1},...,U_{n} \text{Unif}[0,1] g_{n} U_{i} g_{n}(x)=\frac{1}{\sqrt{n}}\sum_{i}1(U_{i}\leq x) \frac{1}{n}\sum_{i}g_{n}(U_{i}) \text{ has the same asymptotic distribution as } \int_{[0,1]} g_{n}(u)du. \mu_{n} U_{i} \frac{1}{n}\sum_{i}g_{n}(U_{i}) \int_{[0,1]}g_{n}(u)\mu_{n}(du) \int_{[0,1]} g_{n}(u)(\mu_{n}-\mu)(du)\equiv f(g_{n},\mu_{n}-\mu). \mu_{n}\to \mu \mu \text{Unif}[0,1] g_{n}(u) f f(g_{n},\mu_{n}-\mu)","['probability-theory', 'statistics', 'asymptotics', 'weak-convergence']"
48,Likelihood an employee gets a failing evaluation by chance,Likelihood an employee gets a failing evaluation by chance,,"Suppose I have a group of $n$ employees. Each one is expected to perform a certain task, and the result of this task is either Pass or Fail. As a part of their reviews, I have to tally up their totals, and each employee should have > 80% passing results. But suppose there is an unknown (but constant) chance that the task fails completely out of the employee's control. Based on the number of passing and failing results of each employee, how can I assess the likelihood that the outcome was just by chance, and not because a certain employee is underperforming? For example, I have 12 employees, each with 40 attempts. One has a 100% passing rate, while the lowest is an 80% passing rate.","Suppose I have a group of employees. Each one is expected to perform a certain task, and the result of this task is either Pass or Fail. As a part of their reviews, I have to tally up their totals, and each employee should have > 80% passing results. But suppose there is an unknown (but constant) chance that the task fails completely out of the employee's control. Based on the number of passing and failing results of each employee, how can I assess the likelihood that the outcome was just by chance, and not because a certain employee is underperforming? For example, I have 12 employees, each with 40 attempts. One has a 100% passing rate, while the lowest is an 80% passing rate.",n,"['probability', 'statistics']"
49,Concentration inequalities on the covariance between observations of the samples,Concentration inequalities on the covariance between observations of the samples,,"I have the following problem : I have $X_1, \dots, X_n$ , $n$ i.i.d. random variables in $\mathbb{R}^p$ . They are bounded, but I think that it is not important. I also have an embedding map $f : \mathbb{R}^p \rightarrow \mathbb{R}^q$ that has a bounded image. I think that this hypothesis is much more important. I am looking at concentration inequalities of the matrix $((f(X_i) - E(f(X_i))^T(f(X_j) - E(f(X_j)))_{i, j}$ around its expectation, if possible in terms of operator norm. In particular, it is easily seen that this expectation is the diagonal matrix with the variances of the $f(X_i)$ 's on the diagonal. I no not know where to start. Regular concentration inequalities that typically require independence do not apply here. Thanks in advance.","I have the following problem : I have , i.i.d. random variables in . They are bounded, but I think that it is not important. I also have an embedding map that has a bounded image. I think that this hypothesis is much more important. I am looking at concentration inequalities of the matrix around its expectation, if possible in terms of operator norm. In particular, it is easily seen that this expectation is the diagonal matrix with the variances of the 's on the diagonal. I no not know where to start. Regular concentration inequalities that typically require independence do not apply here. Thanks in advance.","X_1, \dots, X_n n \mathbb{R}^p f : \mathbb{R}^p \rightarrow \mathbb{R}^q ((f(X_i) - E(f(X_i))^T(f(X_j) - E(f(X_j)))_{i, j} f(X_i)","['probability-theory', 'statistics', 'concentration-of-measure']"
50,Sum of arrival times of Chinese Restaurant Process (CRP),Sum of arrival times of Chinese Restaurant Process (CRP),,"Suppose that a random sample $X_1, X_2, \ldots$ is drawn from a continuous spectrum of colors, or species, following a Chinese Restaurant Process distribution with parameter $|\alpha|$ (or equivalently $X_1,\ldots,X_n \mid P \sim P$ where $P$ is a Dirichlet process $\operatorname{DP}(\alpha)$ with atomless measure $\alpha$ , because the CRP is the unique urn process associated with the DP). Therefore we assume that $X_i$ takes values in some measurable space $(S, \mathcal{S})$ ; for simplicity take $S = \mathbb{R}^{+}$ and $\mathcal{S}$ equal to the Borel sigma algebra on $\mathbb{R}^{+}$ . Assume that we keep track of the unique species $X_j^*$ . We may do so by noting the arrival time of the $j$ th new color. Thus define $T_1=1$ and $$T_j=\min \left\{n: \sum_{i=1}^n D_i=j \: \: \text{for} \: \: D_i=1 \:\: \text{if} \: \: X_i \notin\left\{X_1, \ldots, X_{i-1}\right\}\right\}$$ and setting $X_j^*=X_{T_j}$ . A sequence $X_1, \ldots, X_n$ will contain a random number of unique species, usually denoted by $K_n$ . My question is: are there know results to compute: $$ \mathbb{E}\left[\sum_{j=1}^{K_n} T_j \right]$$ The same question is also posted on MathOverflow given its difficulty: here . PS: also upper bounds of the above quantity are of interest to me. PS2: the probability of a ""new draw"" for the Dirichlet process, i.e. $X_i \notin\left\{X_1, \ldots, X_{i-1}\right\}$ , is given by $\frac{|\alpha|}{|\alpha|+i}$ . Moreover the distinct values in a random sequence $X_1, X_2, \ldots$ sampled from a distribution generated from a $\mathrm{DP}(\alpha)$ -prior with atomless base measure form an i.i.d. sequence from $\bar{\alpha}$ .","Suppose that a random sample is drawn from a continuous spectrum of colors, or species, following a Chinese Restaurant Process distribution with parameter (or equivalently where is a Dirichlet process with atomless measure , because the CRP is the unique urn process associated with the DP). Therefore we assume that takes values in some measurable space ; for simplicity take and equal to the Borel sigma algebra on . Assume that we keep track of the unique species . We may do so by noting the arrival time of the th new color. Thus define and and setting . A sequence will contain a random number of unique species, usually denoted by . My question is: are there know results to compute: The same question is also posted on MathOverflow given its difficulty: here . PS: also upper bounds of the above quantity are of interest to me. PS2: the probability of a ""new draw"" for the Dirichlet process, i.e. , is given by . Moreover the distinct values in a random sequence sampled from a distribution generated from a -prior with atomless base measure form an i.i.d. sequence from .","X_1, X_2, \ldots |\alpha| X_1,\ldots,X_n \mid P \sim P P \operatorname{DP}(\alpha) \alpha X_i (S, \mathcal{S}) S = \mathbb{R}^{+} \mathcal{S} \mathbb{R}^{+} X_j^* j T_1=1 T_j=\min \left\{n: \sum_{i=1}^n D_i=j \: \: \text{for} \: \: D_i=1 \:\: \text{if} \: \: X_i \notin\left\{X_1, \ldots, X_{i-1}\right\}\right\} X_j^*=X_{T_j} X_1, \ldots, X_n K_n  \mathbb{E}\left[\sum_{j=1}^{K_n} T_j \right] X_i \notin\left\{X_1, \ldots, X_{i-1}\right\} \frac{|\alpha|}{|\alpha|+i} X_1, X_2, \ldots \mathrm{DP}(\alpha) \bar{\alpha}","['combinatorics', 'probability-theory', 'statistics', 'bayesian', 'nonparametric-statistics']"
51,How can I properly show that the Residual Sum of Squares can be written in matrix form?,How can I properly show that the Residual Sum of Squares can be written in matrix form?,,"First, I apologize if I am violating any community laws or have improper tags I am a newbie. However, I am tasked with showing, $$RSS(\boldsymbol{\beta}) = \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j =1}^{n}\beta_{j}x_{ij})^2$$ to be written in matrix form as $$RSS(\boldsymbol{\beta})  = (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^T (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})$$ I just want to ensure my thought process is accurate or if you can point me in the right direction. My solution: Let $\boldsymbol{y}$ be a column vector of size $N \times 1$ . Let $\boldsymbol{\beta}$ be a column vector of size $(n+1) \times 1$ . Finally, let $X$ be a matrix of size $N \times (n+1)$ then, $$X \boldsymbol{\beta}=\begin{bmatrix}     1 & x_{11} & x_{12} & \dots  & x_{1n} \\     1 & x_{21} & x_{22} & \dots  & x_{2n} \\     \vdots & \vdots & \vdots & \vdots & \vdots \\     1 & x_{N1} & x_{N2} & \dots  & x_{Nn} \end{bmatrix} \cdot \begin{bmatrix}     \beta_0 \\     \beta_1 \\     \vdots \\     \beta_n  \end{bmatrix} = \begin{bmatrix}     \beta_0 + x_{11}\beta_1+ x_{12}\beta_2+...+x_{1n}\beta_n \\     \beta_0 + x_{21}\beta_1+ x_{22}\beta_2+...+x_{2n}\beta_n \\     \vdots \\     \beta_0 + x_{N1}\beta_1 + x_{N2}\beta_2+...+x_{Nn}\beta_n \end{bmatrix}$$ Resulting in a $N \times 1$ column vector containing our predicted values. Then we can find the residuals as such, $$ u = \boldsymbol{y} - X\boldsymbol{\beta}$$ where $u$ is an $ N \times  1$ column vector.Therefore, we can rewrite the Residual Sum of Squares in matrix form as follows, $$RSS(\boldsymbol{\beta}) = (\boldsymbol{y}-X\boldsymbol{\beta})^T(\boldsymbol{y} - X\boldsymbol{\beta}) = u^Tu$$ . Since $(\boldsymbol{y}-X\boldsymbol{\beta})$ is a column vector we need to transpose it to multiply it by itself.","First, I apologize if I am violating any community laws or have improper tags I am a newbie. However, I am tasked with showing, to be written in matrix form as I just want to ensure my thought process is accurate or if you can point me in the right direction. My solution: Let be a column vector of size . Let be a column vector of size . Finally, let be a matrix of size then, Resulting in a column vector containing our predicted values. Then we can find the residuals as such, where is an column vector.Therefore, we can rewrite the Residual Sum of Squares in matrix form as follows, . Since is a column vector we need to transpose it to multiply it by itself.","RSS(\boldsymbol{\beta}) = \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j =1}^{n}\beta_{j}x_{ij})^2 RSS(\boldsymbol{\beta})  = (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^T (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}) \boldsymbol{y} N \times 1 \boldsymbol{\beta} (n+1) \times 1 X N \times (n+1) X \boldsymbol{\beta}=\begin{bmatrix}
    1 & x_{11} & x_{12} & \dots  & x_{1n} \\
    1 & x_{21} & x_{22} & \dots  & x_{2n} \\
    \vdots & \vdots & \vdots & \vdots & \vdots \\
    1 & x_{N1} & x_{N2} & \dots  & x_{Nn}
\end{bmatrix} \cdot \begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    \vdots \\
    \beta_n 
\end{bmatrix} = \begin{bmatrix}
    \beta_0 + x_{11}\beta_1+ x_{12}\beta_2+...+x_{1n}\beta_n \\
    \beta_0 + x_{21}\beta_1+ x_{22}\beta_2+...+x_{2n}\beta_n \\
    \vdots \\
    \beta_0 + x_{N1}\beta_1 + x_{N2}\beta_2+...+x_{Nn}\beta_n
\end{bmatrix} N \times 1  u = \boldsymbol{y} - X\boldsymbol{\beta} u  N \times  1 RSS(\boldsymbol{\beta}) = (\boldsymbol{y}-X\boldsymbol{\beta})^T(\boldsymbol{y} - X\boldsymbol{\beta}) = u^Tu (\boldsymbol{y}-X\boldsymbol{\beta})","['statistics', 'regression-analysis']"
52,Fisher information with known moments,Fisher information with known moments,,"I have a sequence $X^n$ of length $n$ , where each $X_i$ takes a value from a finite set with probability vector $\mathbf{p} = [p_1, \ldots, p_K]^T$ , i.e., $X_i \in [K]$ , where $p_{X_i}(k) = p_k, k = 1, \ldots, K$ . In this case, I can show that the Fisher Information matrix $I_{X^n}(\mathbf{p}) := \mathbb{E}[\nabla \log p(X^n)\nabla \log p(X^n)^T]= \text{diag}(\frac{n}{p_1}, \ldots, \frac{n}{p_K})$ . My question is: can we compute $I_{X^n}(\mathbf{p})$ when additional information is available regarding moments of the distribution $\mathbf{p}$ (e.g., mean and variance), i.e., $\mathbb{E}_\mathbf{p}[\phi_j(X)]= c_j, j = 1, \ldots, m$ . My objective is to quantify the enhancement in the Cramer-Rao bound when incorporating side information about the moments. In fact, a lower bound on the Fisher Information would also work. While I am aware that there is existing work on constrained Cramer-Rao bounds, it tends to be intricate and occasionally unclear to me. I am hopeful though that one can establish more specific insights for this discrete setting with moment constraints (even if for some special cases such as mean and variance). Any assistance or guidance on this would be greatly appreciated.","I have a sequence of length , where each takes a value from a finite set with probability vector , i.e., , where . In this case, I can show that the Fisher Information matrix . My question is: can we compute when additional information is available regarding moments of the distribution (e.g., mean and variance), i.e., . My objective is to quantify the enhancement in the Cramer-Rao bound when incorporating side information about the moments. In fact, a lower bound on the Fisher Information would also work. While I am aware that there is existing work on constrained Cramer-Rao bounds, it tends to be intricate and occasionally unclear to me. I am hopeful though that one can establish more specific insights for this discrete setting with moment constraints (even if for some special cases such as mean and variance). Any assistance or guidance on this would be greatly appreciated.","X^n n X_i \mathbf{p} = [p_1, \ldots, p_K]^T X_i \in [K] p_{X_i}(k) = p_k, k = 1, \ldots, K I_{X^n}(\mathbf{p}) := \mathbb{E}[\nabla \log p(X^n)\nabla \log p(X^n)^T]= \text{diag}(\frac{n}{p_1}, \ldots, \frac{n}{p_K}) I_{X^n}(\mathbf{p}) \mathbf{p} \mathbb{E}_\mathbf{p}[\phi_j(X)]= c_j, j = 1, \ldots, m","['probability', 'statistics', 'probability-distributions', 'statistical-inference', 'fisher-information']"
53,Probability of forming an N-symbol cluster in a MxM matrix,Probability of forming an N-symbol cluster in a MxM matrix,,"I have the following task: A 7x7 matrix with empty slots in each cell we must put a symbol randomly selected from 1000 symbols 25% of them are the symbol B(250 Bs). The matrix is filled with symbols from the 1000 symbol pool. What is the probability of forming a 5 or more symbols clusters with Bs? (A cluster is a segment where a symbol has adjacent same symbol. Diagonals are not included.) Here is an example of a cluster: Cluster example Using computer simulations I get a probability around 30% and a probability of exactly 5 symbol cluster around 14%. However, I cant manually reach these numbers with mathematics. My approach was to use the hypergeometric distribution but the numbers I got were not relevant. Because with hypergeometric distribution I get all the possible ways of getting exactly 5 Bs randomly. Then I divide them by all the possible ways to arrange the matrix and I get 0.005196 which is no way near 14%. Also I might have problem in the code itself because it is complex so the numbers I get from the computer simulations may not be relevant. For now I assume they are. I will be glad to hear suggestions how to compute the probability. With computer simulations I also got that all possible ways of forming a 5 symbol cluster in 7x7 grid are 1086 I don't really know if this number matters.","I have the following task: A 7x7 matrix with empty slots in each cell we must put a symbol randomly selected from 1000 symbols 25% of them are the symbol B(250 Bs). The matrix is filled with symbols from the 1000 symbol pool. What is the probability of forming a 5 or more symbols clusters with Bs? (A cluster is a segment where a symbol has adjacent same symbol. Diagonals are not included.) Here is an example of a cluster: Cluster example Using computer simulations I get a probability around 30% and a probability of exactly 5 symbol cluster around 14%. However, I cant manually reach these numbers with mathematics. My approach was to use the hypergeometric distribution but the numbers I got were not relevant. Because with hypergeometric distribution I get all the possible ways of getting exactly 5 Bs randomly. Then I divide them by all the possible ways to arrange the matrix and I get 0.005196 which is no way near 14%. Also I might have problem in the code itself because it is complex so the numbers I get from the computer simulations may not be relevant. For now I assume they are. I will be glad to hear suggestions how to compute the probability. With computer simulations I also got that all possible ways of forming a 5 symbol cluster in 7x7 grid are 1086 I don't really know if this number matters.",,"['combinatorics', 'statistics']"
54,Method of Moment and MLE of Bernoulli,Method of Moment and MLE of Bernoulli,,"Given n observations $X_1, X_2, \dots, X_n$ from a random sample with Bernoulli probability function $$ \operatorname{Pr}(X=k) = p^k(1-p)^{1-k}, \text{ for }k=0,1. $$ Denote the method of moment estimator of $p$ as $\tilde{p}$ and the MLE estimator of $p$ as $\hat{p}$ Here is what I got so far. For MOM, $$ \operatorname{E}(X) = \mu_{\text{sample}}  \;\longrightarrow\; \tilde{p} = \dfrac{\sum_{i=1}^n X_i}{n} $$ For MLE, $$ l(\hat{p};x)  = \sum_{i=1}^n X_i(\log\hat{p})  + \sum_{i=1}^n (1 - X_i)(\log(1-p)),  $$ take the derivative and set the equation to zero gave me $\hat{p} = \dfrac{\sum_{i=1}^n X_i}{n}$ It seems like the two method gave me the same expression for the estimators; however, I got a hint for this problem as $\tilde{p}$ and $\hat{p}$ are not the same. So I am confused, but unsure about what I did wrong. Any help would be greatly appreciated!","Given n observations from a random sample with Bernoulli probability function Denote the method of moment estimator of as and the MLE estimator of as Here is what I got so far. For MOM, For MLE, take the derivative and set the equation to zero gave me It seems like the two method gave me the same expression for the estimators; however, I got a hint for this problem as and are not the same. So I am confused, but unsure about what I did wrong. Any help would be greatly appreciated!","X_1, X_2, \dots, X_n 
\operatorname{Pr}(X=k) = p^k(1-p)^{1-k}, \text{ for }k=0,1.
 p \tilde{p} p \hat{p} 
\operatorname{E}(X) = \mu_{\text{sample}} 
\;\longrightarrow\; \tilde{p} = \dfrac{\sum_{i=1}^n X_i}{n}
 
l(\hat{p};x) 
= \sum_{i=1}^n X_i(\log\hat{p}) 
+ \sum_{i=1}^n (1 - X_i)(\log(1-p)), 
 \hat{p} = \dfrac{\sum_{i=1}^n X_i}{n} \tilde{p} \hat{p}","['probability', 'statistics', 'maximum-likelihood']"
55,Calculate the numbers obtained by a die,Calculate the numbers obtained by a die,,"Original question: ""A fair die is rolled 150 times. Let X denote the number of sixes obtained. Use an approximation to find: a)P(X>24) b)P(20≤X≤25) A.0.534 B.0.107 C.0.025 D.0.391"" I tried to use the binomial distribution transferring to normal distribution with 25 as mean and 125/6 as variance(steps shown below), but the answer didn't suit to the answer given by calculator which is about 0.534 and 0.441. I'm not pretty sure if I used the corret method. So it would be very helpful if there's any methoed I don't know can solve it. Edit 1:I got 0.534 and 0.441 by input these to calculator $$1-\sum_{x=0}^{24} \binom{150}{x} \left(\frac{1}{6}\right)^x \left(\frac{5}{6}\right)^{150-x}$$ and the output is 0.53396149. $$\sum_{x=20}^{25} \binom{150}{x} \left(\frac{1}{6}\right)^x \left(\frac{5}{6}\right)^{150-x}$$ and the ouput is 0.4415964366. However, there's no option about 0.587 or 0.441. But I have no idea what's the problem, especially for 0.441, because I'm using the same formula on both, but one of them is correct and another one is not.","Original question: ""A fair die is rolled 150 times. Let X denote the number of sixes obtained. Use an approximation to find: a)P(X>24) b)P(20≤X≤25) A.0.534 B.0.107 C.0.025 D.0.391"" I tried to use the binomial distribution transferring to normal distribution with 25 as mean and 125/6 as variance(steps shown below), but the answer didn't suit to the answer given by calculator which is about 0.534 and 0.441. I'm not pretty sure if I used the corret method. So it would be very helpful if there's any methoed I don't know can solve it. Edit 1:I got 0.534 and 0.441 by input these to calculator and the output is 0.53396149. and the ouput is 0.4415964366. However, there's no option about 0.587 or 0.441. But I have no idea what's the problem, especially for 0.441, because I'm using the same formula on both, but one of them is correct and another one is not.",1-\sum_{x=0}^{24} \binom{150}{x} \left(\frac{1}{6}\right)^x \left(\frac{5}{6}\right)^{150-x} \sum_{x=20}^{25} \binom{150}{x} \left(\frac{1}{6}\right)^x \left(\frac{5}{6}\right)^{150-x},"['probability', 'statistics']"
56,Left tail bound for stopping time of a gaussian random walk,Left tail bound for stopping time of a gaussian random walk,,"Given a sequence of i.i.d. random variables $X_1,X_2,\cdots,X_n\sim N(\mu,1)$ and define the sum (gaussian random walk) as $S_n(\mu) = \sum\limits_{i=1}^n X_i$ . We want to say something about the quantity $N(\mu)$ , which is defined as $$ N(\mu): =\inf\left\{n\in \mathbb N^+ : e^{-\frac{1}{n}S_n(\mu)^2}\leq \alpha\right\} = \inf\left\{n\in \mathbb N^+: |S_n|\geq \sqrt{-n\log\alpha}\right\} $$ where $\alpha = \Omega(1)\in (0,0.5)$ , say $0.025$ . The distribution of $N(\mu)$ itself is untractable, but the right tail is fairly easy to be bounded via any Hoeffding-type inequality. The question is about the left tail: $$ \mathbb P(N(\mu)\leq \beta)\text{ or any thing control N is small}, $$ where $\beta$ is presumably $\beta\sim -\log \alpha \cdot \mu^{-2}$ this order. I found this paper and reference therein study a similar case (in a fairly general framework with weaker assumptions), but they don't consider the left tail bound. The reason why I want to consider left tail bound is to upper bound $\text{Var}(\bar{X}_N)$ or equivalently $E[1/N]$ (or something like $E[1/N^2]$ ).","Given a sequence of i.i.d. random variables and define the sum (gaussian random walk) as . We want to say something about the quantity , which is defined as where , say . The distribution of itself is untractable, but the right tail is fairly easy to be bounded via any Hoeffding-type inequality. The question is about the left tail: where is presumably this order. I found this paper and reference therein study a similar case (in a fairly general framework with weaker assumptions), but they don't consider the left tail bound. The reason why I want to consider left tail bound is to upper bound or equivalently (or something like ).","X_1,X_2,\cdots,X_n\sim N(\mu,1) S_n(\mu) = \sum\limits_{i=1}^n X_i N(\mu) 
N(\mu): =\inf\left\{n\in \mathbb N^+ : e^{-\frac{1}{n}S_n(\mu)^2}\leq \alpha\right\} = \inf\left\{n\in \mathbb N^+: |S_n|\geq \sqrt{-n\log\alpha}\right\}
 \alpha = \Omega(1)\in (0,0.5) 0.025 N(\mu) 
\mathbb P(N(\mu)\leq \beta)\text{ or any thing control N is small},
 \beta \beta\sim -\log \alpha \cdot \mu^{-2} \text{Var}(\bar{X}_N) E[1/N] E[1/N^2]","['probability-theory', 'statistics', 'random-walk', 'stopping-times', 'concentration-of-measure']"
57,A result in multivariable statistics,A result in multivariable statistics,,I'm having problems proving the following result: $$\sum ^{n}_{i=1}(\underline{X_{i}} - \underline{\mu})(\underline{X_{i}} - \underline{\mu})^{T} = \sum^{n}_{i=1}(\underline{X_{i}}-\overline{\underline{X}})(\underline{X_{i}}-\overline{\underline{X}})^{T}+n(\overline{\underline{X}}-\underline{\mu})(\overline{\underline{X}}-\underline{\mu})^{T}$$ Where $\underline{X_{i}}$ is the notation for a vector. This is what I tried to do: $$\sum^{n}_{i=1}\left[(\underline{X_{i}})-\overline{\underline{X}})(\underline{X_{i}}-\overline{\underline{X}})^{T} + (\underline{X_{i}}-\overline{\underline{X}})(\overline{\underline{X}}-\underline{\mu})^{T}+(\overline{\underline{X}}-\underline{\mu})(\underline{X_{i}}-\overline{\underline{X}})^{T}+(\overline{\underline{X}}-\underline{\mu})(\overline{\underline{X}}-\underline{\mu})^{T}\right]$$ $$\sum^{n}_{i=1}\left[(\underline{X_{i}}-\overline{\underline{X}})(\underline{X_{i}}-\overline{\underline{X}})^{T} + (\underline{X_{i}}-\overline{\underline{X}})(\overline{\underline{X}}-\underline{\mu})^{T}+(\overline{\underline{X}}-\underline{\mu})(\underline{X_{i}}-\overline{\underline{X}})^{T}\right]+n(\overline{\underline{X}}-\underline{\mu})(\overline{\underline{X}}-\underline{\mu})^{T}$$ But I don't understand why $(\underline{X_{i}}-\overline{\underline{X}})(\overline{\underline{X}}-\underline{\mu})^{T}$ apparently equals 0. Any help would be appreciated thank you!,I'm having problems proving the following result: Where is the notation for a vector. This is what I tried to do: But I don't understand why apparently equals 0. Any help would be appreciated thank you!,\sum ^{n}_{i=1}(\underline{X_{i}} - \underline{\mu})(\underline{X_{i}} - \underline{\mu})^{T} = \sum^{n}_{i=1}(\underline{X_{i}}-\overline{\underline{X}})(\underline{X_{i}}-\overline{\underline{X}})^{T}+n(\overline{\underline{X}}-\underline{\mu})(\overline{\underline{X}}-\underline{\mu})^{T} \underline{X_{i}} \sum^{n}_{i=1}\left[(\underline{X_{i}})-\overline{\underline{X}})(\underline{X_{i}}-\overline{\underline{X}})^{T} + (\underline{X_{i}}-\overline{\underline{X}})(\overline{\underline{X}}-\underline{\mu})^{T}+(\overline{\underline{X}}-\underline{\mu})(\underline{X_{i}}-\overline{\underline{X}})^{T}+(\overline{\underline{X}}-\underline{\mu})(\overline{\underline{X}}-\underline{\mu})^{T}\right] \sum^{n}_{i=1}\left[(\underline{X_{i}}-\overline{\underline{X}})(\underline{X_{i}}-\overline{\underline{X}})^{T} + (\underline{X_{i}}-\overline{\underline{X}})(\overline{\underline{X}}-\underline{\mu})^{T}+(\overline{\underline{X}}-\underline{\mu})(\underline{X_{i}}-\overline{\underline{X}})^{T}\right]+n(\overline{\underline{X}}-\underline{\mu})(\overline{\underline{X}}-\underline{\mu})^{T} (\underline{X_{i}}-\overline{\underline{X}})(\overline{\underline{X}}-\underline{\mu})^{T},"['linear-algebra', 'statistics', 'multivariable-calculus']"
58,Verifying change of variable in published work,Verifying change of variable in published work,,"In the proof of Proposition A.11 on page 32 of this paper , the author takes the following step: $$ \int_U \exp(-\|D_Gu\|^2_2/2 - \omega \|D u\|^2/2)du = \text{det}(I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2} \int_U  \exp(-\|D_G w\|^2_2/2 )dw $$ which they explain is by substituting $(I+\omega D_G^{-1} D^2 D_G^{-1})^{1/2}u$ for $w$ . To me this seems incorrect but given that it is central to the paper I am seeking a sanity check and likely missing something obvious. Note that the definition of $U$ is irrelevant to the question, $D_G, D$ are symmetric and positive definite matrices, and $\omega$ is some positive constant. Note that the exponent in the LHS can be written as $$ -\|D_G u\|^2_2/2 - \omega \|D u\|^2/2  = -\frac{1}{2}u^T (D_G^2+ \omega D^2)u = -\frac{1}{2}\|(D_G^2 + \omega D^2)^{1/2} u\|^2_2, $$ and plugging in the proposed subsitution: $u = (I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2}w$ gives \begin{align*} -\frac{1}{2}\|(D_G^2 + \omega D^2)^{1/2} u\|^2_2 &= -\frac{1}{2}\|(D_G^2 + \omega D^2)^{1/2} (I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2} w\|^2_2\\ &= -\frac{1}{2}\|D_G^{1/2}(I + \omega D_G^{-1}D^2D_G^{-1})^{1/2}D_G^{1/2} (I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2} w\|^2_2\\ &\neq  -\frac{1}{2}\|D_G w\|^2_2. \end{align*} It seems to me that the correct substitution ought to have been: $D_G^{1/2}(I+\omega D_G^{-1} D^2 D_G^{-1})^{1/2} u=w$ .","In the proof of Proposition A.11 on page 32 of this paper , the author takes the following step: which they explain is by substituting for . To me this seems incorrect but given that it is central to the paper I am seeking a sanity check and likely missing something obvious. Note that the definition of is irrelevant to the question, are symmetric and positive definite matrices, and is some positive constant. Note that the exponent in the LHS can be written as and plugging in the proposed subsitution: gives It seems to me that the correct substitution ought to have been: .","
\int_U \exp(-\|D_Gu\|^2_2/2 - \omega \|D u\|^2/2)du
=
\text{det}(I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2}
\int_U 
\exp(-\|D_G w\|^2_2/2 )dw
 (I+\omega D_G^{-1} D^2 D_G^{-1})^{1/2}u w U D_G, D \omega 
-\|D_G u\|^2_2/2 - \omega \|D u\|^2/2 
= -\frac{1}{2}u^T (D_G^2+ \omega D^2)u
= -\frac{1}{2}\|(D_G^2 + \omega D^2)^{1/2} u\|^2_2,
 u = (I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2}w \begin{align*}
-\frac{1}{2}\|(D_G^2 + \omega D^2)^{1/2} u\|^2_2
&=
-\frac{1}{2}\|(D_G^2 + \omega D^2)^{1/2} (I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2} w\|^2_2\\
&=
-\frac{1}{2}\|D_G^{1/2}(I + \omega D_G^{-1}D^2D_G^{-1})^{1/2}D_G^{1/2} (I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2} w\|^2_2\\
&\neq 
-\frac{1}{2}\|D_G w\|^2_2.
\end{align*} D_G^{1/2}(I+\omega D_G^{-1} D^2 D_G^{-1})^{1/2} u=w","['probability', 'integration', 'statistics', 'statistical-inference', 'change-of-variable']"
59,Distribution of $X(X^\top X)^{-1}X^\top$ when $X$ is a matrix of iid normals,Distribution of  when  is a matrix of iid normals,X(X^\top X)^{-1}X^\top X,"I'm looking at a sparse linear regression model. For the problem I'm working on, I would like to show that there are many possible choices of the independent variables that are wrong but achieve low residual error. In particular, I have a setup of $Y=X\beta^*+W$ , where $Y$ is $n\times 1$ , $X$ is $n\times p$ , with $X_{ij}$ distributed i.i.d. as $N(0,\sigma^2_x)$ , and $W$ is i.i.d $N(0,\sigma^2_w)$ . The OLS coefficient is $\beta_{OLS}=(X^\top X)^{-1}X^\top Y$ . This gives a predicted $\hat{Y}=X(X^\top X)^{-1}X^\top Y$ . I want to consider the case that the true $\beta^*$ is actually 0, and bound the correlation between $Y$ and $\hat{Y}$ , i.e. to see if $$\frac{Y\hat{Y}}{\lVert Y \rVert^2}$$ could be close to 1 even though $X$ is independent of $Y$ . I would therefore like to determine the distribution of $X(X^\top X)^{-1}X^\top$ when $X$ is an $n\times p$ matrix of i.i.d. normals, i.e. $X_{ij}$ are i.i.d. $N(0, \sigma^2)$ for all $i,j$ . I know that in this case $X^\top X$ is distributed as a Wishart $W_p(\sigma^2 I_p,n)$ but I haven't got any further than this.","I'm looking at a sparse linear regression model. For the problem I'm working on, I would like to show that there are many possible choices of the independent variables that are wrong but achieve low residual error. In particular, I have a setup of , where is , is , with distributed i.i.d. as , and is i.i.d . The OLS coefficient is . This gives a predicted . I want to consider the case that the true is actually 0, and bound the correlation between and , i.e. to see if could be close to 1 even though is independent of . I would therefore like to determine the distribution of when is an matrix of i.i.d. normals, i.e. are i.i.d. for all . I know that in this case is distributed as a Wishart but I haven't got any further than this.","Y=X\beta^*+W Y n\times 1 X n\times p X_{ij} N(0,\sigma^2_x) W N(0,\sigma^2_w) \beta_{OLS}=(X^\top X)^{-1}X^\top Y \hat{Y}=X(X^\top X)^{-1}X^\top Y \beta^* Y \hat{Y} \frac{Y\hat{Y}}{\lVert Y \rVert^2} X Y X(X^\top X)^{-1}X^\top X n\times p X_{ij} N(0, \sigma^2) i,j X^\top X W_p(\sigma^2 I_p,n)","['probability', 'statistics', 'random-variables', 'normal-distribution']"
60,Depending on a Probability Measure.,Depending on a Probability Measure.,,"In Jun Shao's Mathematical Statistics, the author tries to frame statistics in a more general setting than I'm used to. For example, by his definition, the Population is a probability measure $P$ . So far so good... Then in some definitions, he talks about ""depending on $P$ "". Here are some examples: In factorization theorem for a sufficient statistics $T(X)$ , he states $$ \frac{dP}{d\nu}=g_P(T(x)) h(x)$$ that $g_P$ depends on $P$ , but $h(x)$ does not. In the definition for ancillary statistic, he states that the distribution of that statistic does not depend on the population $P$ . I'm used to these definitions being given w.r.t. parameters. I'm assuming that given this level of generality, these definitions may also work for non-parametric distribution families. What does it mean to depend on the distribution, on the previous definitions?","In Jun Shao's Mathematical Statistics, the author tries to frame statistics in a more general setting than I'm used to. For example, by his definition, the Population is a probability measure . So far so good... Then in some definitions, he talks about ""depending on "". Here are some examples: In factorization theorem for a sufficient statistics , he states that depends on , but does not. In the definition for ancillary statistic, he states that the distribution of that statistic does not depend on the population . I'm used to these definitions being given w.r.t. parameters. I'm assuming that given this level of generality, these definitions may also work for non-parametric distribution families. What does it mean to depend on the distribution, on the previous definitions?",P P T(X)  \frac{dP}{d\nu}=g_P(T(x)) h(x) g_P P h(x) P,"['probability-theory', 'statistics']"
61,Trouble finding most powerful test given hypothesis test and density $\theta e^{x - \theta(e^{x} -1)}$,Trouble finding most powerful test given hypothesis test and density,\theta e^{x - \theta(e^{x} -1)},"Let $X_{1}, \dots X_{n}$ be a sample from the distribution with density given by: \begin{equation}p_{\theta}(x) = \theta e^{x - \theta(e^{x} - 1)},\end{equation} where $x > 0$ and $0$ otherwise. Let $\theta$ be an unknown parameter. Determine a most powerful test $H_{0} \colon \theta = \theta_{0} = 1$ against $H_{1}\colon \theta = \theta_{1} = 2$ at level $\alpha = 0.05$ . I intend to solve for $k$ in $\mathbb{P}(\frac{L_{1}}{L_{0}} \geq k \mid \theta = 1) = 0.05$ . I've determined the likelihood ratio, for $\theta_{0}$ and $\theta_{1}$ . This gave me: \begin{equation} \left(\frac{\theta_{1}}{\theta_{0}}\right)^{n} \cdot e^{\sum_{i=1}^{n} (-\theta_{1} + \theta_{0})e^{x_{i}} + (\theta_{1} - \theta_{0})} \end{equation} This function must be greater than or equal to some $k$ . Simplifying this as the function of a sufficient statistic and inserting the values of $\theta_{0}$ and $\theta_{1}$ , means that $\sum_{i=1}^{n} -e^{x_{i}} + 1 = n - \sum_{i=1}^{n} e^{x_{i}}$ must be greater than or equal to some constant $k_{1}$ . I assume I should arrive at a value from the Gamma distribution for $k$ / $k_{1}$ but I'm not sure how, or whether I've made a mistake on the way. Would someone be so kind as to help me. Thanks in advance.","Let be a sample from the distribution with density given by: where and otherwise. Let be an unknown parameter. Determine a most powerful test against at level . I intend to solve for in . I've determined the likelihood ratio, for and . This gave me: This function must be greater than or equal to some . Simplifying this as the function of a sufficient statistic and inserting the values of and , means that must be greater than or equal to some constant . I assume I should arrive at a value from the Gamma distribution for / but I'm not sure how, or whether I've made a mistake on the way. Would someone be so kind as to help me. Thanks in advance.","X_{1}, \dots X_{n} \begin{equation}p_{\theta}(x) = \theta e^{x - \theta(e^{x} - 1)},\end{equation} x > 0 0 \theta H_{0} \colon \theta = \theta_{0} = 1 H_{1}\colon \theta = \theta_{1} = 2 \alpha = 0.05 k \mathbb{P}(\frac{L_{1}}{L_{0}} \geq k \mid \theta = 1) = 0.05 \theta_{0} \theta_{1} \begin{equation}
\left(\frac{\theta_{1}}{\theta_{0}}\right)^{n} \cdot e^{\sum_{i=1}^{n} (-\theta_{1} + \theta_{0})e^{x_{i}} + (\theta_{1} - \theta_{0})}
\end{equation} k \theta_{0} \theta_{1} \sum_{i=1}^{n} -e^{x_{i}} + 1 = n - \sum_{i=1}^{n} e^{x_{i}} k_{1} k k_{1}","['statistics', 'hypothesis-testing']"
62,IF $f(x)= \frac{1}{\mu} e^{-\frac{x}{\mu}} $ Write the likelihood function to find those observations.,IF  Write the likelihood function to find those observations.,f(x)= \frac{1}{\mu} e^{-\frac{x}{\mu}} ,"Suppose a measurement process is applied to something whose actual value $\mu$ is given by the probability density function $f(x)= \frac{1}{\mu} e^{-\frac{x}{\mu}} $ Suppose we have observations $x_1, x_2 \ldots, x_n$ were taken with precision $\Delta$ . This means that each measurement can have a value of $x \pm \Delta$ . Write the likelihood function to find those observations. Find the value that maximizes the likelihood function I do $L(\mu)= \frac{1}{\mu} e^{-\frac{x_1}{\mu}} \cdots \frac{1}{\mu} e^{-\frac{x_n}{\mu}}=\frac{1}{\mu^n}e^{\frac{-x_1-x_2- \ldots -x_n}{\mu}}=\frac{1}{\mu^n}e^{\frac{-n\bar{x}}{\mu}}$ but i dont know if im right or  i have to consider the $ \pm \Delta$ and for the second i do $\frac{\partial ln(L(\mu))}{\partial \mu}=\frac{n(\bar{x}-\mu)}{\mu^2}=0$ then $\mu=\bar{x}$",Suppose a measurement process is applied to something whose actual value is given by the probability density function Suppose we have observations were taken with precision . This means that each measurement can have a value of . Write the likelihood function to find those observations. Find the value that maximizes the likelihood function I do but i dont know if im right or  i have to consider the and for the second i do then,"\mu f(x)= \frac{1}{\mu} e^{-\frac{x}{\mu}}  x_1, x_2 \ldots, x_n \Delta x \pm \Delta L(\mu)= \frac{1}{\mu} e^{-\frac{x_1}{\mu}} \cdots \frac{1}{\mu} e^{-\frac{x_n}{\mu}}=\frac{1}{\mu^n}e^{\frac{-x_1-x_2- \ldots -x_n}{\mu}}=\frac{1}{\mu^n}e^{\frac{-n\bar{x}}{\mu}}  \pm \Delta \frac{\partial ln(L(\mu))}{\partial \mu}=\frac{n(\bar{x}-\mu)}{\mu^2}=0 \mu=\bar{x}",['statistics']
63,Regression coefficient on a triangle using geometry [closed],Regression coefficient on a triangle using geometry [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 7 months ago . Improve this question I also post this question in CrossValidate . Consider the following problem: Let $X, Y$ be two independent uniform random variable on $(0,1)$ . We consider the regression model $Y = \beta_1 X + \beta_0$ , given the restriction that $X + Y > 1$ . Find $\beta_1$ . Let us call $U = X|X + Y > 1$ , $V = Y|X+Y > 1$ . Then we could compute the density of $U$ and $V$ by doing conditional probability, and $\beta_1 = \frac{Cov(U,V)}{Var(U)}$ , $\beta_0$ could be found by plugging in the mean of $U$ and $V$ . The final answer should be $\beta_1 = -\frac{1}{2}$ . My question is: how to compute $\beta_1$ based on geometry? If we draw the graph, we could see that we are doing a right-top triangle within the unit square $[0,1] \times [0,1]$ . We could also know the center of the triangle is $(\frac{2}{3}, \frac{2}{3}) = (E[U], E[V])$ . Is there a way that we could visualize the $\beta_1$ should be $-\frac{1}{2}$ ?","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 7 months ago . Improve this question I also post this question in CrossValidate . Consider the following problem: Let be two independent uniform random variable on . We consider the regression model , given the restriction that . Find . Let us call , . Then we could compute the density of and by doing conditional probability, and , could be found by plugging in the mean of and . The final answer should be . My question is: how to compute based on geometry? If we draw the graph, we could see that we are doing a right-top triangle within the unit square . We could also know the center of the triangle is . Is there a way that we could visualize the should be ?","X, Y (0,1) Y = \beta_1 X + \beta_0 X + Y > 1 \beta_1 U = X|X + Y > 1 V = Y|X+Y > 1 U V \beta_1 = \frac{Cov(U,V)}{Var(U)} \beta_0 U V \beta_1 = -\frac{1}{2} \beta_1 [0,1] \times [0,1] (\frac{2}{3}, \frac{2}{3}) = (E[U], E[V]) \beta_1 -\frac{1}{2}","['statistics', 'linear-regression']"
64,Bregman divergence from Wasserstein distance,Bregman divergence from Wasserstein distance,,"I was wondering whether one has studied the Bregman divergence arising from a squared Wasserstein distance. More precisely, let $\Omega\subset \mathbb{R}^d$ be a compact set and $c\in \Omega\times \Omega $ be continuous. Let $\nu\in P(\Omega)$ be a fixed probability measure, and define the map $P(\Omega) \ni \mu\mapsto T(\mu,\nu)\in \mathbb {R}$ such that $$ T(\mu,\nu)=\min_{\gamma \in \Pi(\mu,\nu)}\int_\Omega c(x,y)d \gamma, $$ where $\Pi(\mu,\nu)$ is the set of probability measures with $\mu$ and $\nu$ as their first and second marginals, respectively. As shown in Proposition 7.17 of the book , $\mu\mapsto T(\mu,\nu)$ is convex and under suitable conditions on $\nu$ and $c$ , for all $\mu,\mu'\in P(\Omega)$ , $$ \lim_{\epsilon \to 0} \frac{T(\mu+\epsilon (\mu'-\mu),\nu)}{\epsilon}=\int_\Omega \varphi_\mu (x) (\mu'-\mu)(dx), $$ where $\varphi_\mu$ is the unique Kantorovich potential from $\mu$ to $\nu$ .  This allows us to define the following Bregman divergence $D(\cdot|\cdot):P(\Omega)\times P(\Omega)\to \mathbb{R}$ by $$ D(\mu'|\mu)=T(\mu',\nu)-T(\mu,\nu)-\int_\Omega \varphi_\mu (x) (\mu'-\mu)(dx), $$ where $\varphi_\mu (x)$ is the first variation of $  T(\cdot,\nu)$ at $\mu$ . Has the above Bregman divergence been studied in the literature? For instance, with specific choices of $c$ , say $c(x,y)=|x-y|^2$ , can we simplify the expression of $D(\mu'|\mu)$ ? Can we obtain its relation with commonly used metrics of probability measures?","I was wondering whether one has studied the Bregman divergence arising from a squared Wasserstein distance. More precisely, let be a compact set and be continuous. Let be a fixed probability measure, and define the map such that where is the set of probability measures with and as their first and second marginals, respectively. As shown in Proposition 7.17 of the book , is convex and under suitable conditions on and , for all , where is the unique Kantorovich potential from to .  This allows us to define the following Bregman divergence by where is the first variation of at . Has the above Bregman divergence been studied in the literature? For instance, with specific choices of , say , can we simplify the expression of ? Can we obtain its relation with commonly used metrics of probability measures?","\Omega\subset \mathbb{R}^d c\in \Omega\times \Omega  \nu\in P(\Omega) P(\Omega) \ni \mu\mapsto T(\mu,\nu)\in \mathbb {R} 
T(\mu,\nu)=\min_{\gamma \in \Pi(\mu,\nu)}\int_\Omega c(x,y)d \gamma,
 \Pi(\mu,\nu) \mu \nu \mu\mapsto T(\mu,\nu) \nu c \mu,\mu'\in P(\Omega) 
\lim_{\epsilon \to 0} \frac{T(\mu+\epsilon (\mu'-\mu),\nu)}{\epsilon}=\int_\Omega \varphi_\mu (x) (\mu'-\mu)(dx),
 \varphi_\mu \mu \nu D(\cdot|\cdot):P(\Omega)\times P(\Omega)\to \mathbb{R} 
D(\mu'|\mu)=T(\mu',\nu)-T(\mu,\nu)-\int_\Omega \varphi_\mu (x) (\mu'-\mu)(dx),
 \varphi_\mu (x)   T(\cdot,\nu) \mu c c(x,y)=|x-y|^2 D(\mu'|\mu)","['probability-theory', 'statistics', 'information-theory', 'optimal-transport', 'wasserstein']"
65,Maximum variance of a discrete probability distribution over a set of N fixed numbers,Maximum variance of a discrete probability distribution over a set of N fixed numbers,,"Suppose $P_i$ is a probability distribution over a set $\\{a_i\\}$ i.e. $\\{a_1,a_2,...a_N\\}$ where the $a_i$ 's are fixed and $N$ is finite. If the mean is defined by $$ \langle a \rangle = \sum_{i=1}^{N} P_i a_i $$ Then what will be the maximum value of the variance $\langle a^2 \rangle - (\langle a \rangle)^2 $ ? Note: this is a variation of the post Maximum variance of a discrete probability distribution over the non-negative integers. Here the set $\\{a_i\\}$ was simply the set of first $N$ non-negative integers, but my question is for a more general finite set. Any guidance on how to approach this would be helpful!","Suppose is a probability distribution over a set i.e. where the 's are fixed and is finite. If the mean is defined by Then what will be the maximum value of the variance ? Note: this is a variation of the post Maximum variance of a discrete probability distribution over the non-negative integers. Here the set was simply the set of first non-negative integers, but my question is for a more general finite set. Any guidance on how to approach this would be helpful!","P_i \\{a_i\\} \\{a_1,a_2,...a_N\\} a_i N 
\langle a \rangle = \sum_{i=1}^{N} P_i a_i
 \langle a^2 \rangle - (\langle a \rangle)^2  \\{a_i\\} N","['probability', 'statistics', 'probability-distributions']"
66,Trouble defining hypothesis-testing problem,Trouble defining hypothesis-testing problem,,"Consider a person who claims to have favorable chances in a game in the sense that if you randomly draw one card from a set with as many red as black cards, said person has probability 0.6 of naming the correct color of the card instead of probability 0.5. To test this person's claim we have him guess 25 consecutive times, where the drawn card is put back in the set every time. We choose to believe him if he guesses correctly at least 17 times; otherwise we choose not to believe him. I've made an attempt at defining suitable null and alternative hypotheses for this test along with a suitable critical region where $T = X$ is the test statistic. I've considered $p \leq 0.5$ and $p > 0.5$ as the null and alternative hypothesis respectively, which corresponds to the critical region $\{17, 18, \dots, 25\}$ . But I'm not sure whether this is correct, as I'm still confused on what the hypotheses should look like given the context. Perhaps we would want to disprove the person's claim? Would someone be so kind as to provide me with a hint to what the null and alternative hypothesis should look like, or whether what I've proposed is correct. Thanks in advance.","Consider a person who claims to have favorable chances in a game in the sense that if you randomly draw one card from a set with as many red as black cards, said person has probability 0.6 of naming the correct color of the card instead of probability 0.5. To test this person's claim we have him guess 25 consecutive times, where the drawn card is put back in the set every time. We choose to believe him if he guesses correctly at least 17 times; otherwise we choose not to believe him. I've made an attempt at defining suitable null and alternative hypotheses for this test along with a suitable critical region where is the test statistic. I've considered and as the null and alternative hypothesis respectively, which corresponds to the critical region . But I'm not sure whether this is correct, as I'm still confused on what the hypotheses should look like given the context. Perhaps we would want to disprove the person's claim? Would someone be so kind as to provide me with a hint to what the null and alternative hypothesis should look like, or whether what I've proposed is correct. Thanks in advance.","T = X p \leq 0.5 p > 0.5 \{17, 18, \dots, 25\}","['statistics', 'hypothesis-testing']"
67,Finding CDF for simulation [closed],Finding CDF for simulation [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 7 months ago . Improve this question I have the following pdf: $f(x) = 2\alpha e^{-\alpha x} (1 - e^{-\alpha x})$ , $x>0, \alpha>0$ fixed. I need to find the cdf to simulate X. $$F(x)=  \int_{-\infty}^{x}(2\alpha\cdot e^{-\alpha\cdot t})dt= \left ( 1-e^{-\alpha\cdot t} \right )^{2}-\displaystyle \lim_{t \to -\infty} ( 1-e^{-\alpha\cdot t})^{2} = (1-e^{-\alpha\cdot t})^{2}\cdot (1-e^{\infty})^{2} = -\infty$$ Why isn't it finite?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 7 months ago . Improve this question I have the following pdf: , fixed. I need to find the cdf to simulate X. Why isn't it finite?","f(x) = 2\alpha e^{-\alpha x} (1 - e^{-\alpha x}) x>0, \alpha>0 F(x)=  \int_{-\infty}^{x}(2\alpha\cdot e^{-\alpha\cdot t})dt= \left ( 1-e^{-\alpha\cdot t} \right )^{2}-\displaystyle \lim_{t \to -\infty} ( 1-e^{-\alpha\cdot t})^{2} = (1-e^{-\alpha\cdot t})^{2}\cdot (1-e^{\infty})^{2} = -\infty",['probability']
68,Does zero-probability imply impossible event in discrete cases?,Does zero-probability imply impossible event in discrete cases?,,"I'm still confused about impossible events and events with zero probability. When the sample space is discrete and finite, does $P(A)=0$ ⇒ $A=∅$ ? Also, if B is an event which doesn't contain any elements from the sample space, is it therefore an impossible event $B=∅$ ? Like, rolling a $7$ on a $6$ -sided die. Thanks.","I'm still confused about impossible events and events with zero probability. When the sample space is discrete and finite, does ⇒ ? Also, if B is an event which doesn't contain any elements from the sample space, is it therefore an impossible event ? Like, rolling a on a -sided die. Thanks.",P(A)=0 A=∅ B=∅ 7 6,"['probability', 'probability-theory', 'statistics']"
69,Problems with the definition of $R^2$,Problems with the definition of,R^2,"Wikipedia , defines $R^2$ or R-squared as follow: In statistics, the coefficient of determination, denoted $R^2$ or $r^2$ and pronounced ""R squared"", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s). It is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. I couldn't completely understand the above definition about $R^2$ . In the first paragraph, I'm not sure how variation in the dependent variable can be predicted by independent variables. To be specific, I cannot figure out what does predicting variation of a variable mean, I know about predicting a variable but not predicting variation of a variable. Also in the last part of the second paragraph, what does proportion of total variation of outcomes explained by model mean? For example, for $R^2= 0.9$ I think it means that % $90$ of variation of dependent variable is explained by the model. I couldn't figure out the meaning of variation being explained by a model.","Wikipedia , defines or R-squared as follow: In statistics, the coefficient of determination, denoted or and pronounced ""R squared"", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s). It is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. I couldn't completely understand the above definition about . In the first paragraph, I'm not sure how variation in the dependent variable can be predicted by independent variables. To be specific, I cannot figure out what does predicting variation of a variable mean, I know about predicting a variable but not predicting variation of a variable. Also in the last part of the second paragraph, what does proportion of total variation of outcomes explained by model mean? For example, for I think it means that % of variation of dependent variable is explained by the model. I couldn't figure out the meaning of variation being explained by a model.",R^2 R^2 r^2 R^2 R^2= 0.9 90,['statistics']
70,How do I calculate the prediction interval for a data set in python?,How do I calculate the prediction interval for a data set in python?,,"I have a data set taken from real measurements that I have modeled with a simple univariate linear regression in python using scipy.stats.linregress , so the model is of the form $$y=\beta_0+\beta_1x$$ My goal is to be able to predict the response $\hat{y}$ with a new measured value $\hat{x}$ . Besides just plugging $\hat{x}$ into my model for a prediction, I would like to include the prediction interval to indicate the uncertainty in the prediction. I have found this equation but I’m not sure I’m implementing it correctly since I’m getting a value that seems larger than what I would expect from my understanding of the prediction interval. Particularly, the t-statistic is something that is confusing for me. If there is just a package that can do the calculation for me, please point me in the right direction because I didn’t find anything in the libraries I’m familiar with, though I am still a python novice. Code: import scipy.stats as sps import numpy as np  x = np.array([0.506, 0.55, 0.479, 0.637, 0.558, 0.685, 0.508, 0.573, 0.612, 0.263, 0.366, 0.437,  0.668, 0.506, 0.42, 0.341, 0.35, 0.544, 0.528, 0.513, 0.515, 0.399, 0.585, 0.499,  0.488, 0.415, 0.512, 0.514, 0.468, 0.464, 0.35, 0.516, 0.459, 0.443, 0.497, 0.506,  0.525, 0.408, 0.509, 0.285, 0.436, 0.509, 0.489]) y = np.array([134., 134.8, 128.8, 148.9, 140.7, 155.1, 132.5, 141.3, 146.5, 90.9, 117.7, 128.1,  152.6, 134.5, 119.6, 101.1, 108.9, 137.7, 134., 130.6, 130.9, 115.4, 140.9, 132.6,  129.1, 117.8, 137.4, 134.7, 130., 128.2, 116.3, 134.3, 127.1, 124.6, 129.2, 133.6,  136.9, 115.9, 136.2,  98.6, 123.5, 129., 130.6])  model = sps.linregress(x,y)  new_x = 0.55  y_predictions = model[0] * x + model[1] mse = np.mean((y - y_predictions)**2) tss = ((x - np.mean(x))**2).sum() x_mean = np.mean(x) n = len(x) t = (x_mean - new_x) / (np.std(x) / np.sqrt(n))  prediction_interval = t * np.sqrt(mse * (1 + 1.0 / n + (new_x - x_mean)**2 / tss))  print(prediction_interval) # = -14.1659827752476","I have a data set taken from real measurements that I have modeled with a simple univariate linear regression in python using scipy.stats.linregress , so the model is of the form My goal is to be able to predict the response with a new measured value . Besides just plugging into my model for a prediction, I would like to include the prediction interval to indicate the uncertainty in the prediction. I have found this equation but I’m not sure I’m implementing it correctly since I’m getting a value that seems larger than what I would expect from my understanding of the prediction interval. Particularly, the t-statistic is something that is confusing for me. If there is just a package that can do the calculation for me, please point me in the right direction because I didn’t find anything in the libraries I’m familiar with, though I am still a python novice. Code: import scipy.stats as sps import numpy as np  x = np.array([0.506, 0.55, 0.479, 0.637, 0.558, 0.685, 0.508, 0.573, 0.612, 0.263, 0.366, 0.437,  0.668, 0.506, 0.42, 0.341, 0.35, 0.544, 0.528, 0.513, 0.515, 0.399, 0.585, 0.499,  0.488, 0.415, 0.512, 0.514, 0.468, 0.464, 0.35, 0.516, 0.459, 0.443, 0.497, 0.506,  0.525, 0.408, 0.509, 0.285, 0.436, 0.509, 0.489]) y = np.array([134., 134.8, 128.8, 148.9, 140.7, 155.1, 132.5, 141.3, 146.5, 90.9, 117.7, 128.1,  152.6, 134.5, 119.6, 101.1, 108.9, 137.7, 134., 130.6, 130.9, 115.4, 140.9, 132.6,  129.1, 117.8, 137.4, 134.7, 130., 128.2, 116.3, 134.3, 127.1, 124.6, 129.2, 133.6,  136.9, 115.9, 136.2,  98.6, 123.5, 129., 130.6])  model = sps.linregress(x,y)  new_x = 0.55  y_predictions = model[0] * x + model[1] mse = np.mean((y - y_predictions)**2) tss = ((x - np.mean(x))**2).sum() x_mean = np.mean(x) n = len(x) t = (x_mean - new_x) / (np.std(x) / np.sqrt(n))  prediction_interval = t * np.sqrt(mse * (1 + 1.0 / n + (new_x - x_mean)**2 / tss))  print(prediction_interval) # = -14.1659827752476",y=\beta_0+\beta_1x \hat{y} \hat{x} \hat{x},"['statistics', 'python', 'sums-of-squares', 'mean-square-error']"
71,Find Expectation of Product of Three Random Variables,Find Expectation of Product of Three Random Variables,,"I have two normal random variables $X$ ~ $\mathcal{N}(\mu_X, \sigma^2_X)$ and $Y$ ~ $\mathcal{N}(\mu_Y, \sigma^2_Y)$ and one discrete uniform random variable $\theta$ ~Unif $(a,b)$ whose mean $\mu_\theta$ and variance $\sigma^2_\theta$ are known. To be clear, $\theta$ is uniformly distributed over a finite number of $b-a+1$ integers $a,a+1,a+2,...,b-1,b$ . I also have the following covariance matrix $$ \begin{bmatrix}     \sigma^2_X       & \sigma_{X,Y} & 0 \\     \sigma_{X,Y}       & \sigma^2_Y & 0 \\     0       & 0 & \sigma^2_\theta  \end{bmatrix} $$ I am trying to determine $\mathbb{E} \Big[ X Y \sin2\theta \Big]$ . In other words, I am trying to determine the expectation of the product of three random variables: $X$ , $Y$ , and $\sin2\theta$ , the latter of which is a function of $\theta$ . I tried leveraging the formula for covariance as follows $$ \mathbb{E}[V_1V_2] = \sigma_{V_1,V_2}  + \mathbb{E}[V_1]\mathbb{E}[V_2] $$ but this only returns the expectation of the product of two random variables. I could possibly define $A = XY$ and try to find $\mathbb{E}[A,\sin2\theta]$ , but then I would need the covariance between $XY$ and $\sin2\theta$ , which I don't have and unfortunately I don't believe I can assume independence between $\sin2\theta$ and the product $XY$ . Any thoughts on how to proceed? Or do I simply lack the info I need? NOTE: I'm asking this question to assist in answering another question here on the math SE. Feel free to tackle it directly if you wish.","I have two normal random variables ~ and ~ and one discrete uniform random variable ~Unif whose mean and variance are known. To be clear, is uniformly distributed over a finite number of integers . I also have the following covariance matrix I am trying to determine . In other words, I am trying to determine the expectation of the product of three random variables: , , and , the latter of which is a function of . I tried leveraging the formula for covariance as follows but this only returns the expectation of the product of two random variables. I could possibly define and try to find , but then I would need the covariance between and , which I don't have and unfortunately I don't believe I can assume independence between and the product . Any thoughts on how to proceed? Or do I simply lack the info I need? NOTE: I'm asking this question to assist in answering another question here on the math SE. Feel free to tackle it directly if you wish.","X \mathcal{N}(\mu_X, \sigma^2_X) Y \mathcal{N}(\mu_Y, \sigma^2_Y) \theta (a,b) \mu_\theta \sigma^2_\theta \theta b-a+1 a,a+1,a+2,...,b-1,b 
\begin{bmatrix}
    \sigma^2_X       & \sigma_{X,Y} & 0 \\
    \sigma_{X,Y}       & \sigma^2_Y & 0 \\
    0       & 0 & \sigma^2_\theta 
\end{bmatrix}
 \mathbb{E} \Big[ X Y \sin2\theta \Big] X Y \sin2\theta \theta  \mathbb{E}[V_1V_2] = \sigma_{V_1,V_2}  + \mathbb{E}[V_1]\mathbb{E}[V_2]  A = XY \mathbb{E}[A,\sin2\theta] XY \sin2\theta \sin2\theta XY","['probability', 'probability-theory', 'statistics', 'probability-distributions']"
72,"Suppose $g$ is a periodic function with period $k$. Let the stochastic process be defined as $X_t=g(t+T)$, where $T\sim U(0,k)$.","Suppose  is a periodic function with period . Let the stochastic process be defined as , where .","g k X_t=g(t+T) T\sim U(0,k)","Suppose $g$ is a periodic function with period $k$ . Let the stochastic process be defined as $X_t=g(t+T)$ , where $T\sim U(0,k)$ . I'm trying to prove that $\{X_t:t \geq0\}$ is weak-sense stationary. I already proved that $\mu_X(t)=constant$ , for all $t$ . Now I need to prove that $R_X(t_1,t_2)$ only depends of $t_1-t_2$ or $t_2-t_1$ . By definition: $R_X(t_1,t_2)=\mathbb{E}(X_{t_{1}}X_{t_{2}})=\mathbb{E}\left[g(t_1+T)\,g(t_2+T)\right]=\frac{1}{k}\int_{0}^kg(t_1+T)\,g(t_2+T)\,dT$ . I don't know how to continue from this integral, I have tried to make variable changes and use the periodicity of $g$ , but I can't get anything in particular. Any help would be appreciated.","Suppose is a periodic function with period . Let the stochastic process be defined as , where . I'm trying to prove that is weak-sense stationary. I already proved that , for all . Now I need to prove that only depends of or . By definition: . I don't know how to continue from this integral, I have tried to make variable changes and use the periodicity of , but I can't get anything in particular. Any help would be appreciated.","g k X_t=g(t+T) T\sim U(0,k) \{X_t:t \geq0\} \mu_X(t)=constant t R_X(t_1,t_2) t_1-t_2 t_2-t_1 R_X(t_1,t_2)=\mathbb{E}(X_{t_{1}}X_{t_{2}})=\mathbb{E}\left[g(t_1+T)\,g(t_2+T)\right]=\frac{1}{k}\int_{0}^kg(t_1+T)\,g(t_2+T)\,dT g","['probability', 'statistics', 'stochastic-processes']"
73,Mean speed in a network,Mean speed in a network,,"I am new to this community so I hope this is the rigth place for this question. I am working on traffic simulations on a certain area, and I need to know which is the average speed in the area during the simulated period (4 hours). I know, for each vehicle in the simulation, the distance covered and the time needed, thus I can compute the mean speed of each vehicle by dividing those two. Now that I have a list of mean speeds, what is the more correct thing to do: should I compute the average of those mean speeds, or their harmonic average? And why one is more suitable than the other? By searching online I found out that harmonic average is commonly used to compute the mean speed of a vehicle, but does this apply also to multiple vehicles on multiple different streets? Second question: I will do multiple simulations with different parameters, and at the end I need to know the mean speed over the N simulations. Can I simply compute the mean (or harmonic mean?) of the mean values obtained in each simulation? I hope I have been clear enough. Thank you in advance.","I am new to this community so I hope this is the rigth place for this question. I am working on traffic simulations on a certain area, and I need to know which is the average speed in the area during the simulated period (4 hours). I know, for each vehicle in the simulation, the distance covered and the time needed, thus I can compute the mean speed of each vehicle by dividing those two. Now that I have a list of mean speeds, what is the more correct thing to do: should I compute the average of those mean speeds, or their harmonic average? And why one is more suitable than the other? By searching online I found out that harmonic average is commonly used to compute the mean speed of a vehicle, but does this apply also to multiple vehicles on multiple different streets? Second question: I will do multiple simulations with different parameters, and at the end I need to know the mean speed over the N simulations. Can I simply compute the mean (or harmonic mean?) of the mean values obtained in each simulation? I hope I have been clear enough. Thank you in advance.",,"['statistics', 'means', 'simulation']"
74,Show continuity of the conditional expectation of a curved exponential family,Show continuity of the conditional expectation of a curved exponential family,,"I'm working with the paper of Wu - On the Convergence Properties of the EM Algorithm (1983). In a theorem he says that the EM-algorithm converges to a stationary point if the conditional expectation $Q(\phi^{'} | \phi)$ is both continuous in $\phi^{'}$ and $\phi$ . Furthermore he writes that this condition is always fulfilled for curved exponential families. So, given is the density of a curved exponential family $f(x| \phi) = b(x) \exp(\phi^{T} t(x))/a(\phi)$ , where the parameters $\phi$ lie in a compact submanifold $\Omega_0$ of the r-dimensional convex region $\Omega = \{\phi |\int b(x) \exp(\phi^{T} t(x)) \, dx < \infty \}.$ \ Then $Q(\phi^{'} | \phi) := E[\log(f(x|\phi^{'})) | y, \phi] = - \log(a(\phi^{'})) + E[\log(b(x)) | y, \phi] + \phi'^{\,T} E[t(x) | y, \phi].$ Wu claims that the continuity follows from the compactness of $\Omega_0$ and properties of the exponential family. I think I can say that $- \log(a(\phi^{'}))$ is continuous as $- \log$ is continuous and $\Omega_0$ is compact, so that $- \log(a(\phi^{'}))$ takes its maximum on this compact set which means that this part must be continuous. For the two other terms I should need the exponential family properties. But I don't know any properties, which give me useful information about $b(x)$ and $t(x)$ . Can I somehow show that they have to be continuous? The definition of exponential families say that they have to be only measureable... Thanks! \ (explanation of the EM-algorithm: E-Step: Determine $Q(\phi | \phi_p)$ , M-Step: Choose $\phi_{p+1}$ to be any value of the parameter space which maximizes $Q(\phi | \phi_p)$ . (Repeat these steps until $Q(\phi_{p+1} | \phi_p)$ convergence against a $Q(\phi^* | \phi^*)$ ))","I'm working with the paper of Wu - On the Convergence Properties of the EM Algorithm (1983). In a theorem he says that the EM-algorithm converges to a stationary point if the conditional expectation is both continuous in and . Furthermore he writes that this condition is always fulfilled for curved exponential families. So, given is the density of a curved exponential family , where the parameters lie in a compact submanifold of the r-dimensional convex region \ Then Wu claims that the continuity follows from the compactness of and properties of the exponential family. I think I can say that is continuous as is continuous and is compact, so that takes its maximum on this compact set which means that this part must be continuous. For the two other terms I should need the exponential family properties. But I don't know any properties, which give me useful information about and . Can I somehow show that they have to be continuous? The definition of exponential families say that they have to be only measureable... Thanks! \ (explanation of the EM-algorithm: E-Step: Determine , M-Step: Choose to be any value of the parameter space which maximizes . (Repeat these steps until convergence against a ))","Q(\phi^{'} | \phi) \phi^{'} \phi f(x| \phi) = b(x) \exp(\phi^{T} t(x))/a(\phi) \phi \Omega_0 \Omega = \{\phi |\int b(x) \exp(\phi^{T} t(x)) \, dx < \infty \}. Q(\phi^{'} | \phi) := E[\log(f(x|\phi^{'})) | y, \phi] = - \log(a(\phi^{'})) + E[\log(b(x)) | y, \phi] + \phi'^{\,T} E[t(x) | y, \phi]. \Omega_0 - \log(a(\phi^{'})) - \log \Omega_0 - \log(a(\phi^{'})) b(x) t(x) Q(\phi | \phi_p) \phi_{p+1} Q(\phi | \phi_p) Q(\phi_{p+1} | \phi_p) Q(\phi^* | \phi^*)","['probability-theory', 'statistics', 'algorithms', 'conditional-expectation']"
75,Number of flips required to detect a biased coin using Chebyshev's Inequality,Number of flips required to detect a biased coin using Chebyshev's Inequality,,"I'm trying to solve the following question: ""Given 2 coins where one is fair but the other comes up heads 3/4 of the time.  One coin is picked and tossed n times.  Use Chebyshev's Inequality to determine the smallest n which allows us to determine which coin is picked with 95% confidence."" I already previously calculated that the probability of having picked the unfair coin given that you flipped n heads in a row is $3^n / (2^n + 3^n)$ but I'm not sure if that is relevant at all here. I know Chebyshev's inequality is $P(|X - E[X]| \ge a) \le Var[X]/a^2$ , but I have no idea what $a$ would be in this problem.  Or perhaps this is not the correct way to be thinking about this at all...","I'm trying to solve the following question: ""Given 2 coins where one is fair but the other comes up heads 3/4 of the time.  One coin is picked and tossed n times.  Use Chebyshev's Inequality to determine the smallest n which allows us to determine which coin is picked with 95% confidence."" I already previously calculated that the probability of having picked the unfair coin given that you flipped n heads in a row is but I'm not sure if that is relevant at all here. I know Chebyshev's inequality is , but I have no idea what would be in this problem.  Or perhaps this is not the correct way to be thinking about this at all...",3^n / (2^n + 3^n) P(|X - E[X]| \ge a) \le Var[X]/a^2 a,"['probability', 'statistics']"
76,"Covariance between a vector $X$ and its norm $\lVert X \rVert$ for $X \sim \mathcal{N}(\mu, \Sigma)$",Covariance between a vector  and its norm  for,"X \lVert X \rVert X \sim \mathcal{N}(\mu, \Sigma)","As stated in the title, I need an expression for the covariance between the elements of a vector $X \in \mathbb{R}^n \sim \mathcal{N}(\mu, \Sigma)$ , and its norm $\lVert X \rVert$ . There is a formula for the covariance between $X$ and $\lVert X \rVert^2$ , as I wrote in the answer to another question, which seems to be an easier case due to the lack of the square root, but I can't find anything for $\lVert X \rVert$ . I'm not sure what approach I could take to try to derive such an expression myself. Any help would be greatly appreciated.","As stated in the title, I need an expression for the covariance between the elements of a vector , and its norm . There is a formula for the covariance between and , as I wrote in the answer to another question, which seems to be an easier case due to the lack of the square root, but I can't find anything for . I'm not sure what approach I could take to try to derive such an expression myself. Any help would be greatly appreciated.","X \in \mathbb{R}^n \sim \mathcal{N}(\mu, \Sigma) \lVert X \rVert X \lVert X \rVert^2 \lVert X \rVert","['probability', 'statistics', 'normal-distribution']"
77,How do I develop an understanding of how equations were created? [closed],How do I develop an understanding of how equations were created? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 months ago . Improve this question Firstly: having a lot of difficulty figuring out how to articulate this question due to lack of general math knowledge.  There are multiple questions posed below, but I feel like if I knew more they could be condensed into a single question and am hoping someone can suggest and edit to the effect of the below.  Thank you in advance for your consideration and assistance on this point! Background I'm taking a course which includes descriptive statistics. In that course they describe the method of calculating covariance and provide that equation.  I find myself wondering - why did they choose to define it as they did (using multiplication instead of addition between the terms in the numerator - not even sure if that description is accurate). My question? Are equations for things like covariance derived from looking at phenomenon and 'cracking the code' of how those phenomenon could be described mathematically via a proof?  Or does one, at some level of mathematical skill, say: I want to model this phenomenon and I want that model to have these features and qualities to its output, therefore, I choose this particular structure to achieve that goal and then I prove that functionality through a proof? If the latter case - what progression of mathematical learning develops that skillset? Is the same skillset used in both cases? Why I ask If I understand the motivation of the creator of the covariance equation, I could compare it to my own motivation and perhaps come up with a different approach to the same problem that better fits my own goals because maybe our goals are similar but not the same... Thank you again for any advice on how to simplify this..","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 months ago . Improve this question Firstly: having a lot of difficulty figuring out how to articulate this question due to lack of general math knowledge.  There are multiple questions posed below, but I feel like if I knew more they could be condensed into a single question and am hoping someone can suggest and edit to the effect of the below.  Thank you in advance for your consideration and assistance on this point! Background I'm taking a course which includes descriptive statistics. In that course they describe the method of calculating covariance and provide that equation.  I find myself wondering - why did they choose to define it as they did (using multiplication instead of addition between the terms in the numerator - not even sure if that description is accurate). My question? Are equations for things like covariance derived from looking at phenomenon and 'cracking the code' of how those phenomenon could be described mathematically via a proof?  Or does one, at some level of mathematical skill, say: I want to model this phenomenon and I want that model to have these features and qualities to its output, therefore, I choose this particular structure to achieve that goal and then I prove that functionality through a proof? If the latter case - what progression of mathematical learning develops that skillset? Is the same skillset used in both cases? Why I ask If I understand the motivation of the creator of the covariance equation, I could compare it to my own motivation and perhaps come up with a different approach to the same problem that better fits my own goals because maybe our goals are similar but not the same... Thank you again for any advice on how to simplify this..",,"['statistics', 'covariance', 'descriptive-statistics']"
78,Investigate whether $\widetilde\beta$ is an unbiased estimator for $\beta$,Investigate whether  is an unbiased estimator for,\widetilde\beta \beta,"Consider a linear model $Y=X\beta+\varepsilon$ , where $Y,\varepsilon \in \Bbb R^n,\beta\in \Bbb R^p$ and with model matrix $X \in \Bbb R^{n×p}$ of full rank, $n, p\in  \Bbb N$ with $1\lt p\le n$ .Consider further the singular value decomposition of the model matrix X, $$X=UDV^T$$ , where U is $n×p$ , D is a $p×p$ diagonal matrix and V is $p×p$ .(Note: The diagonal elements of $D=diag(\lambda_1,\lambda_2,\ldots,\lambda_p)$ are the positive square roots of the eigenvalues of $X^TX$ or $XX^T$ and U and V contain normalized eigenvectors of $XX^T$ and $X^TX$ respectively, $i.e.U^TU=I,V^TV=I.$ We assume $\lambda_1\ge\lambda_2\ge\ldots\ge\lambda_p.)$ Show that the least square estimator $\hat \beta$ for $\beta$ can be written as $\hat \beta=V D^{−1}U^TY$ . An alternative estimator for $\beta$ is given by $\widetilde\beta:=V D^{−1}_* U^TY$ , where $D^{−1}_*=diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)$ for some $1\le k \lt p$ . (Note:  Such an estimator can e.g. be useful if some covariates are highly correlated and $X^TX$ is close to singular.) Investigate  whether $\widetilde\beta$ is  an  unbiased  estimator  for $\beta$ .   If  applicable,  calculate  its bias. Answer:2)To investigate whether the alternative estimator $\beta$ equals $VD^{-1}U^TY$ is unbiased, we need to check whether its expected value equals the true parameter vector $ẞ$ .Taking the expectation of $\beta$ , we have $\Bbb E (\widetilde{\beta}) = \Bbb E(VD^{-1}_* U^TY)$ . Since $D^{-1}_* $ is a diagonal matrix with non-zero elements only on the first k diagonal positions, we can write $D=diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)$ Therefore, $\widetilde\beta =(VD^{-1}_* U^TY)$ can be written as $\widetilde\beta = Vdiag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY)$ . Expanding this expression, we have $\widetilde \beta = (V_1, V_2,\ldots,V_p)diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY)$ ,where $V_1, V_2,\ldots,V_p$ represent the columns of V. Taking the expectation, we have, $\Bbb E (\widetilde{\beta})=\Bbb E (V_1, V_2,\ldots,V_p)diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY)$ . Since the expectation operator is linear, we can move it inside the product: $\Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p)  \Bbb Ediag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY)$ .Since the diagonal elements of $D^{-1}_*$ only have non-zero values on the first k positions, $\Bbb E D^{-1}_* = diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)$ Therefore, $\Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p)  diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)\Bbb E U^TY)$ . By the properties of the expectation operator, $\Bbb E U^TY=U^T\Bbb EY$ . But $\Bbb E (Y) = X ẞ$ , so $\Bbb E U^TY = U^T X ẞ$ .Therefore, $\Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p)  diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)\Bbb E U^TX ẞ)$ . Since the matrix product diag equals zero for any matrix X, except for the first k positions, we have, $\Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p) 0 $ . In  other words, the alternative estimator $\widetilde{\beta}$ is unbiased and its expected value is equal to the true parameter vector ẞ. i just try it but i am not sure so need help for the solution verification.","Consider a linear model , where and with model matrix of full rank, with .Consider further the singular value decomposition of the model matrix X, , where U is , D is a diagonal matrix and V is .(Note: The diagonal elements of are the positive square roots of the eigenvalues of or and U and V contain normalized eigenvectors of and respectively, We assume Show that the least square estimator for can be written as . An alternative estimator for is given by , where for some . (Note:  Such an estimator can e.g. be useful if some covariates are highly correlated and is close to singular.) Investigate  whether is  an  unbiased  estimator  for .   If  applicable,  calculate  its bias. Answer:2)To investigate whether the alternative estimator equals is unbiased, we need to check whether its expected value equals the true parameter vector .Taking the expectation of , we have . Since is a diagonal matrix with non-zero elements only on the first k diagonal positions, we can write Therefore, can be written as . Expanding this expression, we have ,where represent the columns of V. Taking the expectation, we have, . Since the expectation operator is linear, we can move it inside the product: .Since the diagonal elements of only have non-zero values on the first k positions, Therefore, . By the properties of the expectation operator, . But , so .Therefore, . Since the matrix product diag equals zero for any matrix X, except for the first k positions, we have, . In  other words, the alternative estimator is unbiased and its expected value is equal to the true parameter vector ẞ. i just try it but i am not sure so need help for the solution verification.","Y=X\beta+\varepsilon Y,\varepsilon \in \Bbb R^n,\beta\in \Bbb R^p X \in \Bbb R^{n×p} n, p\in 
\Bbb N 1\lt p\le n X=UDV^T n×p p×p p×p D=diag(\lambda_1,\lambda_2,\ldots,\lambda_p) X^TX XX^T XX^T X^TX i.e.U^TU=I,V^TV=I. \lambda_1\ge\lambda_2\ge\ldots\ge\lambda_p.) \hat \beta \beta \hat \beta=V D^{−1}U^TY \beta \widetilde\beta:=V D^{−1}_* U^TY D^{−1}_*=diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0) 1\le k \lt p X^TX \widetilde\beta \beta \beta VD^{-1}U^TY ẞ \beta \Bbb E (\widetilde{\beta}) = \Bbb E(VD^{-1}_* U^TY) D^{-1}_*  D=diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0) \widetilde\beta =(VD^{-1}_* U^TY) \widetilde\beta = Vdiag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY) \widetilde \beta = (V_1, V_2,\ldots,V_p)diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY) V_1, V_2,\ldots,V_p \Bbb E (\widetilde{\beta})=\Bbb E (V_1, V_2,\ldots,V_p)diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY) \Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p) 
\Bbb Ediag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)U^TY) D^{-1}_* \Bbb E D^{-1}_* = diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0) \Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p) 
diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)\Bbb E U^TY) \Bbb E U^TY=U^T\Bbb EY \Bbb E (Y) = X ẞ \Bbb E U^TY = U^T X ẞ \Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p) 
diag(\lambda^{-1}_1,\lambda^{-1}_2,\ldots,\lambda^{-1}_k,0,\ldots,0)\Bbb E U^TX ẞ) \Bbb E (\widetilde{\beta})= (V_1, V_2,\ldots,V_p) 0  \widetilde{\beta}","['statistics', 'solution-verification', 'regression', 'linear-regression']"
79,optimisation problem that minimises the sum of absolute errors of the simple linear model $y = a + bx$.,optimisation problem that minimises the sum of absolute errors of the simple linear model .,y = a + bx,"Consider the following sample: $(x_1, y_1) = (1, 1)$ , $(x_2, y_2) = (2, 5)$ , $(x_3, y_3) = (3, 8)$ , $(x_4, y_4) = (4, 18)$ . How can I write down an optimisation problem that minimises the sum of absolute errors of the simple linear model $y = a + bx$ . Also, is this optimisation problem constrained? Is the objective function differentiable? (Optional: Is the objective function convex?) For 1) is it going to be just: $$min\sum_{i=1}^{4}|y_i-(a+bx_i)|$$","Consider the following sample: , , , . How can I write down an optimisation problem that minimises the sum of absolute errors of the simple linear model . Also, is this optimisation problem constrained? Is the objective function differentiable? (Optional: Is the objective function convex?) For 1) is it going to be just:","(x_1, y_1) = (1, 1) (x_2, y_2) = (2, 5) (x_3, y_3) = (3, 8) (x_4, y_4) = (4, 18) y = a + bx min\sum_{i=1}^{4}|y_i-(a+bx_i)|","['probability', 'statistics', 'optimization', 'data-analysis', 'data-mining']"
80,How to compute the conditional expectation of a jointly multivariate normal distribution?,How to compute the conditional expectation of a jointly multivariate normal distribution?,,"In the paper Functional Linear Discriminant Analysis , on page 7, they use the EM algorithm. The model involves random variables $\gamma\sim\mathcal{N}_m(0, \Gamma)$ and $\epsilon\sim\mathcal{N}_n(0, \sigma²I)$ and is of the form $$\mathbf{Y}=S(\beta + \gamma) + \epsilon$$ , where $S$ is a design matrix, $\beta$ is the parameter and $\mathbf{Y}$ is the regression target. They then try to compute the MLE of $\Gamma, \sigma²$ and $\beta$ via the EM algorithm, treating $\gamma$ as a latent variable. Clearly, the joint log likelihood is $$l(\sigma², \Gamma, \beta)\propto-\frac{||\mathbf{Y} - S(\beta+\gamma)||²}{\sigma²}-\gamma^T\Gamma^{-1}\gamma-\log(|\Gamma|)-n\log(\sigma²)$$ However, they then compute the expectation of $\gamma$ for the E-step as $$\mathbb{E}[\gamma|\mathbf{Y}, \beta, \Gamma, \sigma²]=(\sigma²\Gamma^{-1} + S^TS)^{-1}S^T(\mathbf{Y}-S\beta)$$ I cannot move ahead after computing $S\gamma = \mathbf{Y}-S\beta-\epsilon$ . I know that this means $S\mathbb{E}[\gamma|\mathbf{Y},\beta,\Gamma,\sigma²]=\mathbf{Y}-S\beta$ , but how do I transfer $S$ to the RHS?","In the paper Functional Linear Discriminant Analysis , on page 7, they use the EM algorithm. The model involves random variables and and is of the form , where is a design matrix, is the parameter and is the regression target. They then try to compute the MLE of and via the EM algorithm, treating as a latent variable. Clearly, the joint log likelihood is However, they then compute the expectation of for the E-step as I cannot move ahead after computing . I know that this means , but how do I transfer to the RHS?","\gamma\sim\mathcal{N}_m(0, \Gamma) \epsilon\sim\mathcal{N}_n(0, \sigma²I) \mathbf{Y}=S(\beta + \gamma) + \epsilon S \beta \mathbf{Y} \Gamma, \sigma² \beta \gamma l(\sigma², \Gamma, \beta)\propto-\frac{||\mathbf{Y} - S(\beta+\gamma)||²}{\sigma²}-\gamma^T\Gamma^{-1}\gamma-\log(|\Gamma|)-n\log(\sigma²) \gamma \mathbb{E}[\gamma|\mathbf{Y}, \beta, \Gamma, \sigma²]=(\sigma²\Gamma^{-1} + S^TS)^{-1}S^T(\mathbf{Y}-S\beta) S\gamma = \mathbf{Y}-S\beta-\epsilon S\mathbb{E}[\gamma|\mathbf{Y},\beta,\Gamma,\sigma²]=\mathbf{Y}-S\beta S","['probability', 'matrices', 'statistics', 'optimization', 'statistical-inference']"
81,Optimal Betting Strategy in Coin Toss,Optimal Betting Strategy in Coin Toss,,"Goal: Participate in a coin-tossing game with the aim to maximize your earnings. A fundamental part of the game is formulating a strategy that optimizes your earnings based on the observation of the coin toss outcomes. Rules: The Coin : A coin is tossed, landing on ""HEADS"" with an unknown probability  p . Observation and Betting : You have the option to either observe the outcome of the toss without betting or place a fixed bet on an interval where you believe  p  lies. Placing Bets : Betting involves selecting a confidence interval where you believe the probability  p exists. The bet is fixed, meaning you bet the same amount each time. Earnings : If you choose an interval that includes p , you earn a point. If you narrow down the interval size compared to the previous round while  p  is still within the interval, your earnings are quadratically increased based on the reduction in the interval size. Strategy : The goal of the game is to find the best strategy that maximizes your total earnings. The strategy involves when to observe, when to bet, and how to adjust the betting intervals based on the observed outcomes. Gameplay: In each round, the game simulates a coin toss, revealing whether it lands on ""HEADS"" or ""TAILS"". Based on the outcomes, you make a decision—either to observe or to bet by choosing a probability interval. Implement your strategy based on the observations to optimize the betting intervals and maximize earnings. Conclusion of the Game: The game proceeds for a set number of rounds (e.g., 100 rounds). At the end of the game, the actual value of  p is disclosed, and your total earnings are calculated, showcasing the effectiveness of your strategy. Example: For instance, you decide to observe for the first five rounds, noting the outcomes. In the subsequent round, you may choose to bet with a certain interval based on your observations and strategy. Continually adjust your strategy based on the outcomes and the objective to find the most profitable betting intervals. Remember, the fundamental aim is to develop and apply a strategy that maximally increases your earnings throughout the game, using observation and intelligent betting based on the revealed outcomes.","Goal: Participate in a coin-tossing game with the aim to maximize your earnings. A fundamental part of the game is formulating a strategy that optimizes your earnings based on the observation of the coin toss outcomes. Rules: The Coin : A coin is tossed, landing on ""HEADS"" with an unknown probability  p . Observation and Betting : You have the option to either observe the outcome of the toss without betting or place a fixed bet on an interval where you believe  p  lies. Placing Bets : Betting involves selecting a confidence interval where you believe the probability  p exists. The bet is fixed, meaning you bet the same amount each time. Earnings : If you choose an interval that includes p , you earn a point. If you narrow down the interval size compared to the previous round while  p  is still within the interval, your earnings are quadratically increased based on the reduction in the interval size. Strategy : The goal of the game is to find the best strategy that maximizes your total earnings. The strategy involves when to observe, when to bet, and how to adjust the betting intervals based on the observed outcomes. Gameplay: In each round, the game simulates a coin toss, revealing whether it lands on ""HEADS"" or ""TAILS"". Based on the outcomes, you make a decision—either to observe or to bet by choosing a probability interval. Implement your strategy based on the observations to optimize the betting intervals and maximize earnings. Conclusion of the Game: The game proceeds for a set number of rounds (e.g., 100 rounds). At the end of the game, the actual value of  p is disclosed, and your total earnings are calculated, showcasing the effectiveness of your strategy. Example: For instance, you decide to observe for the first five rounds, noting the outcomes. In the subsequent round, you may choose to bet with a certain interval based on your observations and strategy. Continually adjust your strategy based on the outcomes and the objective to find the most profitable betting intervals. Remember, the fundamental aim is to develop and apply a strategy that maximally increases your earnings throughout the game, using observation and intelligent betting based on the revealed outcomes.",,"['probability', 'statistics', 'optimization', 'gambling']"
82,Prove that XY is a chi-squared random variable if X and Y are independent normal random variables.,Prove that XY is a chi-squared random variable if X and Y are independent normal random variables.,,"According to this answer , $X+Y$ and $X−Y$ are Gaussian random variables, so that $(X+Y)^2$ and $(X−Y)^2$ are Chi-square distributed with 1 degree of freedom, where $X\sim N(a,b)$ and $Y\sim N(c,d)$ I'm confused since while I understand that $(X+Y)$ ~Normal $(a + c, b + d)$ and $(X-Y)$ ~Normal $(a + c, b + d)$ , I do not understand why $(X+Y)^2$ and $(X-Y)^2$ are chi-squared random variables. As far as I know, only squares of standard normal random variables can be chi-squared distributed, and $(X+Y)^2$ and $(X-Y)^2$ are, in general, not standard normal random variables. Am I misunderstanding / missing something here?","According to this answer , and are Gaussian random variables, so that and are Chi-square distributed with 1 degree of freedom, where and I'm confused since while I understand that ~Normal and ~Normal , I do not understand why and are chi-squared random variables. As far as I know, only squares of standard normal random variables can be chi-squared distributed, and and are, in general, not standard normal random variables. Am I misunderstanding / missing something here?","X+Y X−Y (X+Y)^2 (X−Y)^2 X\sim N(a,b) Y\sim N(c,d) (X+Y) (a + c, b + d) (X-Y) (a + c, b + d) (X+Y)^2 (X-Y)^2 (X+Y)^2 (X-Y)^2","['statistics', 'probability-distributions', 'normal-distribution', 'chi-squared']"
83,How to calculate the excpected Number if ec primes were random?,How to calculate the excpected Number if ec primes were random?,,"Ec primes are primes of the form: $(2^n-1)\cdot 10^d+2^{n-1}-1$ , where d Is the Number of decimal digits of $2^{n-1}$ . The Vector containing all the numbers n leading to a prime or probable prime Is: [2,3,4,7,8,12,19,22,36,46,51,67,79,215,359,394,451,1323,2131,3336,3371,6231,19179,39699,51456,56238,69660,75894,79798,92020,174968,176006,181015,285019,331259,360787,366770,541456] I dont know if there are other ec probable primes between n=400000 and n=541456 maybe there could be one or two. The thing that strikes me Is the abundance of multiples of $43$ in the vector,infact there are four n's (215,69660,92020,541456) out of 38 which are multiple of $43$ . If the primes of this type were random, how many n's leading to a prime would be excpected multiple of $43$ out of 38 n's leading to a prime? How to calculate the excpected Number of multiple of $43$ ?","Ec primes are primes of the form: , where d Is the Number of decimal digits of . The Vector containing all the numbers n leading to a prime or probable prime Is: [2,3,4,7,8,12,19,22,36,46,51,67,79,215,359,394,451,1323,2131,3336,3371,6231,19179,39699,51456,56238,69660,75894,79798,92020,174968,176006,181015,285019,331259,360787,366770,541456] I dont know if there are other ec probable primes between n=400000 and n=541456 maybe there could be one or two. The thing that strikes me Is the abundance of multiples of in the vector,infact there are four n's (215,69660,92020,541456) out of 38 which are multiple of . If the primes of this type were random, how many n's leading to a prime would be excpected multiple of out of 38 n's leading to a prime? How to calculate the excpected Number of multiple of ?",(2^n-1)\cdot 10^d+2^{n-1}-1 2^{n-1} 43 43 43 43,['number-theory']
84,Generate sample values of a distribution with known Characteristic Function in R,Generate sample values of a distribution with known Characteristic Function in R,,"I have given the characteristic function $$\mathcal F(p)(x)=\frac{\exp(-|x|^s)}{(1 + x^2)^5}$$ where $s=0.5, s=1, s=1.5$ or $s=2$ . Now I want to get sample values of the distribution belonging to the density $p$ . Since the inverse Fourier transform does not exist in closed form one way to get samples would be using numerical methods. A well-known method is ""Inverse transform sampling"" but for this I would need the inverse CDF and it is a long way to get there (CF->PDF->CDF->InvCDF). Does anyone know a better algorithm to generate a sample? My paper refers to the properties of $\alpha$ -stable random variables.","I have given the characteristic function where or . Now I want to get sample values of the distribution belonging to the density . Since the inverse Fourier transform does not exist in closed form one way to get samples would be using numerical methods. A well-known method is ""Inverse transform sampling"" but for this I would need the inverse CDF and it is a long way to get there (CF->PDF->CDF->InvCDF). Does anyone know a better algorithm to generate a sample? My paper refers to the properties of -stable random variables.","\mathcal F(p)(x)=\frac{\exp(-|x|^s)}{(1 + x^2)^5} s=0.5, s=1, s=1.5 s=2 p \alpha","['statistics', 'numerical-methods', 'fourier-transform', 'characteristic-functions']"
85,Asymptotic normality of Kernel Density Estimator,Asymptotic normality of Kernel Density Estimator,,"Set up We consider kernel density estimation. $X_1, \ldots, X_n \overset{\mathrm{i.i.d.}}{\sim} F$ (c.d.f., unknown). $F$ has a density $f$ and then the object it wants to estimate pointwise. Kernel Density Estimator: $$ \hat{f_n} := \frac{1}{n}\sum_{j=1}^n \frac{1}{h}K \left(\frac{y-X_j}{h} \right). $$ $h$ means bandwidth. What I know If $h$ satisfied $h \to 0, nh \to \infty$ , then \begin{align} \mathrm{bias\ of\ }\hat{f_n}(y) &= \frac{1}{2}h^2 f''(y)\tau^2 + o(h^2),\\ \mathrm{Var}[\hat{f_n}(y)] &= \frac{1}{nh}f(y) \int K^2(y)dy + o(\frac{1}{nh}), \end{align} where $$ \tau^2 := \int y^2 K(y)dy. $$ Question Why do the following limitation hold? $$ \frac{\mathrm{bias\ of\ }\hat{f_n}(y)}{\sqrt{\mathrm{Var}[\hat{f_n}(y)]}}\to \xi, $$ where $\xi$ is a constant.","Set up We consider kernel density estimation. (c.d.f., unknown). has a density and then the object it wants to estimate pointwise. Kernel Density Estimator: means bandwidth. What I know If satisfied , then where Question Why do the following limitation hold? where is a constant.","X_1, \ldots, X_n \overset{\mathrm{i.i.d.}}{\sim} F F f 
\hat{f_n} := \frac{1}{n}\sum_{j=1}^n \frac{1}{h}K \left(\frac{y-X_j}{h} \right).
 h h h \to 0, nh \to \infty \begin{align}
\mathrm{bias\ of\ }\hat{f_n}(y) &= \frac{1}{2}h^2 f''(y)\tau^2 + o(h^2),\\
\mathrm{Var}[\hat{f_n}(y)] &= \frac{1}{nh}f(y) \int K^2(y)dy + o(\frac{1}{nh}),
\end{align} 
\tau^2 := \int y^2 K(y)dy.
 
\frac{\mathrm{bias\ of\ }\hat{f_n}(y)}{\sqrt{\mathrm{Var}[\hat{f_n}(y)]}}\to \xi,
 \xi","['probability-theory', 'statistics', 'asymptotics']"
86,(Method of transformation for discrete random variable) Compute the probability mass function of $W=\cos{\frac{2\pi X}{N}}$ where $X$ is geometric,(Method of transformation for discrete random variable) Compute the probability mass function of  where  is geometric,W=\cos{\frac{2\pi X}{N}} X,"Let $X$ be a geometric random variable with the following distribution: \begin{align} \mathbb P\{X=x\}=p^x(1-p)  \end{align} where $$x=0,1,2,...$$ The random variable $$W=\cos{\frac{2\pi X}{N}}$$ for some integer $N$ . Compute the probability mass function of $W$ . You should consider the case when $N$ is odd or $N$ is even separately. My attempt: Let $$Y=\frac{2\pi X}{N}$$ Although $W=\cos{\frac{2\pi X}{N}}=\cos{Y}$ is not a monotone function, it can be divided into a finite number of regions in which it is monotone. Due to the symmetry of the cosine function, it is sufficient to consider the case where $y\in[0,\pi]$ and then scale the density with factor $n$ , where $n$ denotes the number of regions. If $y\in[0,\pi]$ , or equivalently $x\in[0,\frac{N}{2}]$ then $W=\cos{\frac{2\pi X}{N}}=\cos{Y}$ is a one-to-one function, and we can now compute the inverse transformation: \begin{align}     x=\frac{N}{2\pi}\arccos{w} \end{align} The probability mass function of $W$ in this special case is: \begin{align}     f_W(w)&=f_X\bigg(\frac{N}{2\pi}\arccos{w}\bigg)\\     &=p^{\frac{N}{2\pi}\arccos{w}}(1-p) \end{align} In the general case, we have: \begin{align}     f_W(w)=np^{\frac{N}{2\pi}\arccos{w}}(1-p) \end{align} If $N$ is even, we count the point $w=-1$ twice each time we add regions, hence we need to subtract it: \begin{align}     f_W(w)&=n\bigg[p^{\frac{N}{2\pi}\arccos{w}}(1-p)-p^{\frac{N}{2}}(1-p)\bigg]\\     &=n(1-p)\bigg(p^{\frac{N}{2\pi}\arccos{w}}-p^{\frac{N}{2}}\bigg) \end{align} My questions: My answer looks quite messy so I am not sure if it is correct or not. If someone can comment on that or suggest other simpler methods for solving it would be greatly appreciated. Thank you!","Let be a geometric random variable with the following distribution: where The random variable for some integer . Compute the probability mass function of . You should consider the case when is odd or is even separately. My attempt: Let Although is not a monotone function, it can be divided into a finite number of regions in which it is monotone. Due to the symmetry of the cosine function, it is sufficient to consider the case where and then scale the density with factor , where denotes the number of regions. If , or equivalently then is a one-to-one function, and we can now compute the inverse transformation: The probability mass function of in this special case is: In the general case, we have: If is even, we count the point twice each time we add regions, hence we need to subtract it: My questions: My answer looks quite messy so I am not sure if it is correct or not. If someone can comment on that or suggest other simpler methods for solving it would be greatly appreciated. Thank you!","X \begin{align}
\mathbb P\{X=x\}=p^x(1-p) 
\end{align} x=0,1,2,... W=\cos{\frac{2\pi X}{N}} N W N N Y=\frac{2\pi X}{N} W=\cos{\frac{2\pi X}{N}}=\cos{Y} y\in[0,\pi] n n y\in[0,\pi] x\in[0,\frac{N}{2}] W=\cos{\frac{2\pi X}{N}}=\cos{Y} \begin{align}
    x=\frac{N}{2\pi}\arccos{w}
\end{align} W \begin{align}
    f_W(w)&=f_X\bigg(\frac{N}{2\pi}\arccos{w}\bigg)\\
    &=p^{\frac{N}{2\pi}\arccos{w}}(1-p)
\end{align} \begin{align}
    f_W(w)=np^{\frac{N}{2\pi}\arccos{w}}(1-p)
\end{align} N w=-1 \begin{align}
    f_W(w)&=n\bigg[p^{\frac{N}{2\pi}\arccos{w}}(1-p)-p^{\frac{N}{2}}(1-p)\bigg]\\
    &=n(1-p)\bigg(p^{\frac{N}{2\pi}\arccos{w}}-p^{\frac{N}{2}}\bigg)
\end{align}","['probability', 'statistics', 'probability-distributions', 'transformation']"
87,When is sufficiency and completeness of a statistic preserved?,When is sufficiency and completeness of a statistic preserved?,,"I have been given these definitions in my statistical inference class: Let $(X_1,...,X_n)$ be a simple random sampling of $X\rightarrow\{P_\theta:\theta \in \Theta\}$ and $T\equiv T(X_1,...,X_n)$ a statistic. We say that $T$ is sufficient for $\theta$ if the conditional distribution of $(X_1,...,X_n)$ to each value of $T$ is independent of $\theta$ ( $\theta$ does not appear). $T$ is complete if for every unidimensional measurable function $g$ $$ E_\theta[g(T)]=0 \ \forall \theta \in \Theta \Rightarrow P_\theta[g(T)=0]=1 \ \forall \theta \in \Theta $$ I'm interested in knowing under what functions sufficiency and completeness are preserved. I already know that if $T$ is sufficient for $\theta$ and $T=f(U)$ then U is sufficient for $\theta$ , and therefore, if $f$ is biyective, $T'=f(T)$ is also sufficient. However, we haven't seen any properties for completeness and I have done the following: Let $T$ be a complete statistic, and $T'=f(T)$ with $f$ measurable. Then $T'$ is complete. I proceed as follows: Let $g$ be an unidimensional and measurable function such that $E_\theta[g(T')]=0 \ \forall \theta \in \Theta$ . Then $E_\theta[g(f(T))]=0 \ \forall \theta \in \Theta$ and by completeness of $T$ , $P_\theta[g(f(T))=0]=1 \ \forall \theta \in \Theta$ , so $P_\theta[g(T')=0]=1 \ \forall \theta \in \Theta$ and $T'$ is complete. I wasn't told what ""measurable"" means in the definitions, but I guess it is borel measurable (so that $g\circ f$ is borel measurable and the previous line makes sense). Is this last result correct? If $T$ is sufficient and complete and $f$ is biyective and measurable, can I conclude that $T'=f(T)$ is also sufficient and complete?","I have been given these definitions in my statistical inference class: Let be a simple random sampling of and a statistic. We say that is sufficient for if the conditional distribution of to each value of is independent of ( does not appear). is complete if for every unidimensional measurable function I'm interested in knowing under what functions sufficiency and completeness are preserved. I already know that if is sufficient for and then U is sufficient for , and therefore, if is biyective, is also sufficient. However, we haven't seen any properties for completeness and I have done the following: Let be a complete statistic, and with measurable. Then is complete. I proceed as follows: Let be an unidimensional and measurable function such that . Then and by completeness of , , so and is complete. I wasn't told what ""measurable"" means in the definitions, but I guess it is borel measurable (so that is borel measurable and the previous line makes sense). Is this last result correct? If is sufficient and complete and is biyective and measurable, can I conclude that is also sufficient and complete?","(X_1,...,X_n) X\rightarrow\{P_\theta:\theta \in \Theta\} T\equiv T(X_1,...,X_n) T \theta (X_1,...,X_n) T \theta \theta T g 
E_\theta[g(T)]=0 \ \forall \theta \in \Theta \Rightarrow P_\theta[g(T)=0]=1 \ \forall \theta \in \Theta
 T \theta T=f(U) \theta f T'=f(T) T T'=f(T) f T' g E_\theta[g(T')]=0 \ \forall \theta \in \Theta E_\theta[g(f(T))]=0 \ \forall \theta \in \Theta T P_\theta[g(f(T))=0]=1 \ \forall \theta \in \Theta P_\theta[g(T')=0]=1 \ \forall \theta \in \Theta T' g\circ f T f T'=f(T)","['statistics', 'solution-verification', 'statistical-inference', 'sufficient-statistics']"
88,The Collector’s Problem,The Collector’s Problem,,"I have the following Question in my text book , where the Answer is given. I have got my own alternative Answer. I want to know whether that alternative Answer is Correct & Equivalent to the text book Answer. Suppose that each package of bubble gum contains the picture of a baseball player, that the pictures of $r$ different players are used, that the picture of each player is equally likely to be placed in any given package of gum, and that pictures are placed in different packages independently of each other. The problem now is to determine the probability $p$ that a person who buys $n$ packages of gum ( $n$ ≥ $r$ ) will obtain a complete set of $r$ different pictures. The textbook answers as follows: For i = 1, ..., r, let $A_{i}$ denote the event that the picture of player i is missing from all n packages. Then $ \bigcup \limits_{i=1}^{r}A_{i} $ is the event that the picture of at least one player is missing. We shall find $Pr \left( \bigcup \limits_{i=1}^{r}A_{i}\right) $ Since the picture of each of the r players is equally likely to be placed in any particular package, the probability that the picture of player i will not be obtained in any particular package is $\frac{r-1}{r}$ . Since the packages are filled independently, the probability that the picture of player i will not be obtained in any of the n packages is $\left [ \frac{r-1}{r}  \right ] ^n$ , hence $$ Pr \left( A_i \right) =  \left ( \frac{r-1}{r}  \right ) ^n $$ Now consider any two players i and j . The probability that neither the picture of player i nor the picture of player j will be obtained in any particular package is $\frac{r-2}{r}$ . Therefore, the probability that neither picture will be obtained in any of the n packages is $\left [ \frac{r-2}{r}  \right ] ^n$ . Thus, $$ Pr \left( A_i\bigcap A_j \right) =  \left ( \frac{r-2}{r}  \right ) ^n $$ If we next consider any three players i , j , and k , we find that $$ Pr \left( A_i\bigcap A_j \bigcap A_k \right) =  \left ( \frac{r-3}{r}  \right ) ^n $$ By continuing in this way, we finally arrive at the probability $Pr \left( A_i\bigcap A_j \bigcap... \bigcap A_r \right)$ that the pictures of all r players are missing from the n packages. Of course, this probability is 0. $$ Pr \left( \bigcap \limits_{i=1}^{r} A_i \right) = r\left( \frac{r-1}{r}  \right)^n - \binom{r}{2} \left( \frac{r-2}{r}  \right)^n + ...  + \left(-1 \right)^r \binom{r}{r-1}\left(\frac{1}{r}  \right)^n = \sum_{j=1}^{r-1}\left( -1\right) ^{j+1} \binom{r}{j}\left(1- \frac{j}{r}  \right)^n  $$ Since the probability p of obtaining a complete set of r different pictures is equal to $$ 1 − Pr \left( \bigcup \limits_{i=1}^{r}A_{i}\right) $$ it follows from the foregoing derivation that p can be written in the form: $$ p=\sum_{j=0}^{r-1}\left( -1\right) ^{j}\binom{r}{j}\left(1- \frac{j}{r}  \right)^n  $$ I just used another method to solve this. Each package has r choices, so the total outcomes would be $r^n$ and then we first select r packages out of n to have a complete set of r pictures, there are $\binom{n}{r}$ combinations, and then arrange those r packages with r! choices. The remaining n-r packages each have r different arrivals. so my calculation is : $$ \frac{ \binom{n}{r}r!r^{n-r}}{r^n} $$ I don't know whether it's correct or not. I can understand the procedure provided by the textbook, so are these two answers the same?","I have the following Question in my text book , where the Answer is given. I have got my own alternative Answer. I want to know whether that alternative Answer is Correct & Equivalent to the text book Answer. Suppose that each package of bubble gum contains the picture of a baseball player, that the pictures of different players are used, that the picture of each player is equally likely to be placed in any given package of gum, and that pictures are placed in different packages independently of each other. The problem now is to determine the probability that a person who buys packages of gum ( ≥ ) will obtain a complete set of different pictures. The textbook answers as follows: For i = 1, ..., r, let denote the event that the picture of player i is missing from all n packages. Then is the event that the picture of at least one player is missing. We shall find Since the picture of each of the r players is equally likely to be placed in any particular package, the probability that the picture of player i will not be obtained in any particular package is . Since the packages are filled independently, the probability that the picture of player i will not be obtained in any of the n packages is , hence Now consider any two players i and j . The probability that neither the picture of player i nor the picture of player j will be obtained in any particular package is . Therefore, the probability that neither picture will be obtained in any of the n packages is . Thus, If we next consider any three players i , j , and k , we find that By continuing in this way, we finally arrive at the probability that the pictures of all r players are missing from the n packages. Of course, this probability is 0. Since the probability p of obtaining a complete set of r different pictures is equal to it follows from the foregoing derivation that p can be written in the form: I just used another method to solve this. Each package has r choices, so the total outcomes would be and then we first select r packages out of n to have a complete set of r pictures, there are combinations, and then arrange those r packages with r! choices. The remaining n-r packages each have r different arrivals. so my calculation is : I don't know whether it's correct or not. I can understand the procedure provided by the textbook, so are these two answers the same?","r p n n r r A_{i}  \bigcup \limits_{i=1}^{r}A_{i}  Pr \left( \bigcup \limits_{i=1}^{r}A_{i}\right)  \frac{r-1}{r} \left [ \frac{r-1}{r}  \right ] ^n 
Pr \left( A_i \right) =  \left ( \frac{r-1}{r}  \right ) ^n
 \frac{r-2}{r} \left [ \frac{r-2}{r}  \right ] ^n 
Pr \left( A_i\bigcap A_j \right) =  \left ( \frac{r-2}{r}  \right ) ^n
 
Pr \left( A_i\bigcap A_j \bigcap A_k \right) =  \left ( \frac{r-3}{r}  \right ) ^n
 Pr \left( A_i\bigcap A_j \bigcap... \bigcap A_r \right) 
Pr \left( \bigcap \limits_{i=1}^{r} A_i \right) = r\left( \frac{r-1}{r}  \right)^n - \binom{r}{2} \left( \frac{r-2}{r}  \right)^n + ...
 + \left(-1 \right)^r \binom{r}{r-1}\left(\frac{1}{r}  \right)^n = \sum_{j=1}^{r-1}\left( -1\right) ^{j+1}
\binom{r}{j}\left(1- \frac{j}{r}  \right)^n 
 
1 − Pr \left( \bigcup \limits_{i=1}^{r}A_{i}\right)
 
p=\sum_{j=0}^{r-1}\left( -1\right) ^{j}\binom{r}{j}\left(1- \frac{j}{r}  \right)^n 
 r^n \binom{n}{r} 
\frac{ \binom{n}{r}r!r^{n-r}}{r^n}
","['probability', 'statistics', 'combinations', 'independence', 'coupon-collector']"
89,Dealing with log(1) in computational linguistics,Dealing with log(1) in computational linguistics,,"I am not sure this is the right venue for this question: if you think it is not, I will move it to another StackExchange board. Explanation I am trying to calculate phonotactic probability on a corpus of consonant-vowel-consonant (CVC) words. Phonotactic probability can be defined as follows: Phonotactic probability refers to the frequency with which a phonological segment, such as /s/, and a sequence of phonological segments, such as /s^/, occur in a given position in a word ( Kansas University ) And can be calculated on segments of various length, such as monograms, bigrams, trigrams, etc. In simple terms: For a word like blick in English, the unigram average would include the probability of /b/ occurring in the first position of a word, the probability of /l/ in the second position, the probability of /ɪ/ occuring in the third position, and the probability of /k/ occurring in the fourth position of a word. Each positional probability is calculated by summing the log token frequency of words containing that segment in that position divided by the sum of the log token frequency of all words that have that position in their transcription. The bigram average is calculated in an equivalent way, except that sequences of two segments and their positions are used instead of single segments. So for blick that would be /bl/, /lɪ/, /ɪk/ as the included positional probabilities In matemathical terms, the phonotactic probability of the word blick would be calculated as follows: Given an example corpus (e.g., the one from CorpusTools documentation ): The phonotactic probability of blick would be calculated as follows: Question Unfortunately, in my corpus, some bigrams (e.g., /Aw/, 'Aw': [1, 1, 1, 1] ) appear in only words with token frequency one. As such, when calculating the sum of log() frequencies of words starting with /Aw/, the sum ammounts to log(1) + log(1) + log(1) + log(1) Which in turn returns 0 . Indeed, this is not very representative, since the bigram appears in at least four words. Is there a way to deal with this from a mathematical perspective?","I am not sure this is the right venue for this question: if you think it is not, I will move it to another StackExchange board. Explanation I am trying to calculate phonotactic probability on a corpus of consonant-vowel-consonant (CVC) words. Phonotactic probability can be defined as follows: Phonotactic probability refers to the frequency with which a phonological segment, such as /s/, and a sequence of phonological segments, such as /s^/, occur in a given position in a word ( Kansas University ) And can be calculated on segments of various length, such as monograms, bigrams, trigrams, etc. In simple terms: For a word like blick in English, the unigram average would include the probability of /b/ occurring in the first position of a word, the probability of /l/ in the second position, the probability of /ɪ/ occuring in the third position, and the probability of /k/ occurring in the fourth position of a word. Each positional probability is calculated by summing the log token frequency of words containing that segment in that position divided by the sum of the log token frequency of all words that have that position in their transcription. The bigram average is calculated in an equivalent way, except that sequences of two segments and their positions are used instead of single segments. So for blick that would be /bl/, /lɪ/, /ɪk/ as the included positional probabilities In matemathical terms, the phonotactic probability of the word blick would be calculated as follows: Given an example corpus (e.g., the one from CorpusTools documentation ): The phonotactic probability of blick would be calculated as follows: Question Unfortunately, in my corpus, some bigrams (e.g., /Aw/, 'Aw': [1, 1, 1, 1] ) appear in only words with token frequency one. As such, when calculating the sum of log() frequencies of words starting with /Aw/, the sum ammounts to log(1) + log(1) + log(1) + log(1) Which in turn returns 0 . Indeed, this is not very representative, since the bigram appears in at least four words. Is there a way to deal with this from a mathematical perspective?",,"['statistics', 'logarithms']"
90,Chernoff bound and Polynomial Markov,Chernoff bound and Polynomial Markov,,"I'm currently struggling the following statement; Show that upper bound of polynomial Markov is better than the Chernoff upper bound; that is, for given $\delta > 0,$ $$\inf_{k = 0,1,2,...} \frac{\mathbb E [X^k]}{\delta^k} \le \inf_{\lambda \ge 0} \frac{\mathbb E [e^{\lambda X}]}{e^{\lambda \delta}}$$ I tried to use the Taylor's expansion in $e^{\lambda X}$ for both exponentials; $$ \mathbb E [e^{\lambda X}] = \mathbb E [ 1 + \frac{\lambda X}{1!} + \frac{(\lambda X)^2}{2!} + ...]$$ $$e^{\lambda \delta} = 1 + \frac{\lambda \delta}{1!} + \frac{(\lambda \delta)^2}{2!} + ...$$ , but failed to show more things. It would be grateful for any hint regarding this inequality (or any errors or uncertain points in the above statement).","I'm currently struggling the following statement; Show that upper bound of polynomial Markov is better than the Chernoff upper bound; that is, for given I tried to use the Taylor's expansion in for both exponentials; , but failed to show more things. It would be grateful for any hint regarding this inequality (or any errors or uncertain points in the above statement).","\delta > 0, \inf_{k = 0,1,2,...} \frac{\mathbb E [X^k]}{\delta^k} \le \inf_{\lambda \ge 0} \frac{\mathbb E [e^{\lambda X}]}{e^{\lambda \delta}} e^{\lambda X}  \mathbb E [e^{\lambda X}] = \mathbb E [ 1 + \frac{\lambda X}{1!} + \frac{(\lambda X)^2}{2!} + ...] e^{\lambda \delta} = 1 + \frac{\lambda \delta}{1!} + \frac{(\lambda \delta)^2}{2!} + ...","['probability-theory', 'statistics', 'solution-verification']"
91,Decoupling $n$ Gaussian random variables using linear algebra,Decoupling  Gaussian random variables using linear algebra,n,"For a bivariate normal distribution $\begin{pmatrix} X \\ Y \end{pmatrix}$ with mean $\begin{pmatrix} \mu_X \\ \mu_Y\end{pmatrix}$ and variance $\begin{pmatrix} \sigma_X^2 & \rho \sigma_X \sigma_Y \\ \rho \sigma_X \sigma_Y & \sigma_Y^2\end{pmatrix}$ , we can turn these into a pair of independent random variables by rewriting (for example) $Y$ as a linear function of $X$ so that the covariance will be $0$ . So letting $Y = aX + Z$ , we require $\text{cov}(X, Y - aX) = 0 \Rightarrow a = \frac{\sigma_X}{\sigma_Y}\rho$ and then $Z = Y - \frac{\sigma_X}{\sigma_Y}\rho X$ and $Y$ are uncorrelated. It's now easier to carry out calculations. To make things even nicer we can also standardise these variables. I want to now extend this reasoning to higher dimensions, for example three Gaussian random variables in a multivariate Gaussian distribution. How can I rephrase my above argument in terms of matrices and linear algebra? Including the standardisation step. How can I use that rephrasing to extend this to general multivariate Gaussian?","For a bivariate normal distribution with mean and variance , we can turn these into a pair of independent random variables by rewriting (for example) as a linear function of so that the covariance will be . So letting , we require and then and are uncorrelated. It's now easier to carry out calculations. To make things even nicer we can also standardise these variables. I want to now extend this reasoning to higher dimensions, for example three Gaussian random variables in a multivariate Gaussian distribution. How can I rephrase my above argument in terms of matrices and linear algebra? Including the standardisation step. How can I use that rephrasing to extend this to general multivariate Gaussian?","\begin{pmatrix} X \\ Y \end{pmatrix} \begin{pmatrix} \mu_X \\ \mu_Y\end{pmatrix} \begin{pmatrix} \sigma_X^2 & \rho \sigma_X \sigma_Y \\ \rho \sigma_X \sigma_Y & \sigma_Y^2\end{pmatrix} Y X 0 Y = aX + Z \text{cov}(X, Y - aX) = 0 \Rightarrow a = \frac{\sigma_X}{\sigma_Y}\rho Z = Y - \frac{\sigma_X}{\sigma_Y}\rho X Y","['linear-algebra', 'probability', 'statistics', 'linear-transformations', 'normal-distribution']"
92,Suggestions to start Statistical Manifolds,Suggestions to start Statistical Manifolds,,"I am a computer science PhD student, and I need statistical manifolds theory for my work. I am currently reading Differential Geometry of Curves and Surface by Kristopher Tapp and Carmo. I plan to study Lee's smooth Manifold next. (My supervisor's recommendations.) Unfortunately I do not have the topology background to the depth I would like to have to; I studied Topology without Tears in my second year (not completely) but I do not remember all of what I read either. But I am keeping it alongside. Can anyone suggest a self-readable set of books to get to Stat. Manifold? I might end up needing quite a bit of Discrete Differential Geometry as well. Other suggestions about alternate study plans are also welcome.","I am a computer science PhD student, and I need statistical manifolds theory for my work. I am currently reading Differential Geometry of Curves and Surface by Kristopher Tapp and Carmo. I plan to study Lee's smooth Manifold next. (My supervisor's recommendations.) Unfortunately I do not have the topology background to the depth I would like to have to; I studied Topology without Tears in my second year (not completely) but I do not remember all of what I read either. But I am keeping it alongside. Can anyone suggest a self-readable set of books to get to Stat. Manifold? I might end up needing quite a bit of Discrete Differential Geometry as well. Other suggestions about alternate study plans are also welcome.",,"['general-topology', 'statistics', 'differential-geometry', 'book-recommendation', 'stochastic-geometry']"
93,Is this a proof that ALL generalized inverses of a hermitian matrix is also hermitian?,Is this a proof that ALL generalized inverses of a hermitian matrix is also hermitian?,,"I know that the inverse of a nonsingular hermitian matrix is also hermitian (symmetric if matrix is real). I also know that a singular hermitian matrix has a hermitian generalized inverse. But are ALL generalized inverses of a hermitian matrix also hermitian? I could not find an answer anywhere, and the following proof suggests so. Please help me find any mistakes if it is not so. Definition 1 : A matrix $A^- \in \mathbb{C}^{n \times m}$ is a generalized inverse of a matrix $A \in \mathbb{C}^{m \times n}$ if $A A^- A = A$ . Lemma 2 : Every hermitian matrix $A \in \mathbb{C}^{n \times n}$ can be decomposed into a product of a matrix and its conjugate transpose: $A = X^\dagger X$ , where $X \in \mathbb{C}^{r \times n}$ ( proof ). Now, I will take this $X$ and construct a perpendicular (orthogonal) projection matrix onto its column space $C(X)$ . Definition 2 : $M \in \mathbb{C}^{n \times n}$ is a perpendicular projection matrix onto $X$ if and only if: $v \in C(X) \Rightarrow Mv = v$ $w \perp C(X) \Rightarrow Mw = 0$ Lemma 3 : $M$ is a perpendicular projection matrix onto $C(M) = C(X)$ if and only if: $MM = M$ $M^\dagger = M$ The proof for lemma 3 is taken from Plane Answers to Complex Questions Proposition B.32 and Theorem B.33 and slightly modified to account for complex matrices. Lemma 4 : Perpendicular projection matrices are unique (Proposition B.34 from Plane Answers to Complex Questions ). Lemma 5 : If $G$ is a generalized inverse for $X^\dagger X$ , then $X G X^\dagger X = X$ (Proposition B.43 from Plane Answers to Complex Questions ). Lemma 6 : The (unique) perpendicular projection matrix onto $C(X)$ is $X (X^\dagger X)^- X^\dagger$ . Proof: Show that this matrix holds properties in definition 2: For $v \in C(X)$ , $v = Xb$ for some $b \in \mathbb{C}^n$ . So, using lemma 5: $$X (X^\dagger X)^- X^\dagger v = X (X^\dagger X)^- X^\dagger Xb = Xb = v$$ For $w \perp C(X)$ , $X^\dagger w = 0$ . So: $$X (X^\dagger X)^- X^\dagger w = 0$$ Since lemma 4 says that perpendicular projection operators are unique, this is the one and only perpendicular projection operator onto $C(X)$ . Since lemma 3 tells us that a perpendicular projection matrix is hermitian, the matrix from lemma 6 must be hermitian: $$X (X^\dagger X)^- X^\dagger = (X (X^\dagger X)^- X^\dagger )^\dagger = X ((X^\dagger X)^-)^\dagger X^\dagger $$ Since $X^\dagger X = A$ , we have: $$A^- = (A^-)^\dagger $$ This works for any arbitrary hermitian matrix $A$ . Does this prove that any generalized inverse of a non-zero hermitian matrix is hermitian?","I know that the inverse of a nonsingular hermitian matrix is also hermitian (symmetric if matrix is real). I also know that a singular hermitian matrix has a hermitian generalized inverse. But are ALL generalized inverses of a hermitian matrix also hermitian? I could not find an answer anywhere, and the following proof suggests so. Please help me find any mistakes if it is not so. Definition 1 : A matrix is a generalized inverse of a matrix if . Lemma 2 : Every hermitian matrix can be decomposed into a product of a matrix and its conjugate transpose: , where ( proof ). Now, I will take this and construct a perpendicular (orthogonal) projection matrix onto its column space . Definition 2 : is a perpendicular projection matrix onto if and only if: Lemma 3 : is a perpendicular projection matrix onto if and only if: The proof for lemma 3 is taken from Plane Answers to Complex Questions Proposition B.32 and Theorem B.33 and slightly modified to account for complex matrices. Lemma 4 : Perpendicular projection matrices are unique (Proposition B.34 from Plane Answers to Complex Questions ). Lemma 5 : If is a generalized inverse for , then (Proposition B.43 from Plane Answers to Complex Questions ). Lemma 6 : The (unique) perpendicular projection matrix onto is . Proof: Show that this matrix holds properties in definition 2: For , for some . So, using lemma 5: For , . So: Since lemma 4 says that perpendicular projection operators are unique, this is the one and only perpendicular projection operator onto . Since lemma 3 tells us that a perpendicular projection matrix is hermitian, the matrix from lemma 6 must be hermitian: Since , we have: This works for any arbitrary hermitian matrix . Does this prove that any generalized inverse of a non-zero hermitian matrix is hermitian?",A^- \in \mathbb{C}^{n \times m} A \in \mathbb{C}^{m \times n} A A^- A = A A \in \mathbb{C}^{n \times n} A = X^\dagger X X \in \mathbb{C}^{r \times n} X C(X) M \in \mathbb{C}^{n \times n} X v \in C(X) \Rightarrow Mv = v w \perp C(X) \Rightarrow Mw = 0 M C(M) = C(X) MM = M M^\dagger = M G X^\dagger X X G X^\dagger X = X C(X) X (X^\dagger X)^- X^\dagger v \in C(X) v = Xb b \in \mathbb{C}^n X (X^\dagger X)^- X^\dagger v = X (X^\dagger X)^- X^\dagger Xb = Xb = v w \perp C(X) X^\dagger w = 0 X (X^\dagger X)^- X^\dagger w = 0 C(X) X (X^\dagger X)^- X^\dagger = (X (X^\dagger X)^- X^\dagger )^\dagger = X ((X^\dagger X)^-)^\dagger X^\dagger  X^\dagger X = A A^- = (A^-)^\dagger  A,"['linear-algebra', 'matrices', 'statistics', 'solution-verification', 'inverse']"
94,What is the expectation of switch activation?,What is the expectation of switch activation?,,"Consider $n$ switches, which activate at rate $a_i$ , $1\leq i\leq n$ . The time it takes for a specific switch to activate, $T_i$ , satisfies $T_i\sim \text{Exp}(a_i)$ . In particular, $E[T_i]=1/a_i$ . Now, imagine there are $m$ activating factors (available at any time for the $n$ switches) so that each switch only activates once it is bound to exactly $1$ activating factor (say, at a rate $b$ ). Once a switch is activated, the activating factor is recycled at rate $r$ . For example, if $b$ is high and $r$ is low, we expect early activation of $m$ switches (if $m<n$ ), followed by a period of no activation. What distribution does $T_i$ now follows? Is it possible to work out its expectation? Edit: Following a comment, I would just add that both binding and recycling times are exponentially distributed with the given rates, and the switch an activating factor binds to is chosen uniformly at random among those not yet activated.","Consider switches, which activate at rate , . The time it takes for a specific switch to activate, , satisfies . In particular, . Now, imagine there are activating factors (available at any time for the switches) so that each switch only activates once it is bound to exactly activating factor (say, at a rate ). Once a switch is activated, the activating factor is recycled at rate . For example, if is high and is low, we expect early activation of switches (if ), followed by a period of no activation. What distribution does now follows? Is it possible to work out its expectation? Edit: Following a comment, I would just add that both binding and recycling times are exponentially distributed with the given rates, and the switch an activating factor binds to is chosen uniformly at random among those not yet activated.",n a_i 1\leq i\leq n T_i T_i\sim \text{Exp}(a_i) E[T_i]=1/a_i m n 1 b r b r m m<n T_i,"['statistics', 'random-variables', 'expected-value', 'exponential-distribution']"
95,Using Histograms for Discrete Data?,Using Histograms for Discrete Data?,,"Consider the following From my previous understanding, Histograms were used only to model continuous data. However, here the histogram is used to model the number of incorrect notes, which is clearly discrete. Hence, I suspect that Histograms can be used to model discrete data. Let's say we can model discrete data with a Histogram. I see many immediate problems. To ensure, the bars are touching we must extend the classes such that $1-5$ becomes $0.5-5.5$ . Hence it seems that when we do model discrete data, we more or less treat the data as being continuous in nature. It obviously doesn't make sense to have half a note. So what is going on here?","Consider the following From my previous understanding, Histograms were used only to model continuous data. However, here the histogram is used to model the number of incorrect notes, which is clearly discrete. Hence, I suspect that Histograms can be used to model discrete data. Let's say we can model discrete data with a Histogram. I see many immediate problems. To ensure, the bars are touching we must extend the classes such that becomes . Hence it seems that when we do model discrete data, we more or less treat the data as being continuous in nature. It obviously doesn't make sense to have half a note. So what is going on here?",1-5 0.5-5.5,"['probability', 'statistics']"
96,Ill-Conditioned Model Error,Ill-Conditioned Model Error,,"Problem : Consider a linear regression problem where we observe noisy observations, $$y=Xw^* + \epsilon,$$ where $X \in \mathbb{R}^{n\times d}$ represents the data matrix ( $n$ points and $d$ features), $w^*\in\mathbb{R}^n$ represents the ground truth, and $\epsilon \in \mathbb{R}^n$ represents some noise. Suppose $X$ is ill-conditioned (i.e. $\kappa(X)\gg 1$ ) and we take $\hat{w}=X^{-1}y$ as an estimate for $w^*$ . What can I say about this quantity? $$ \lVert \hat{w}-w^*\rVert_2$$ Thoughts : Well, following our nose we see $$ \lVert \hat{w}  - w^*\rVert_{2} = \lVert X^{-1}y - w^* \rVert_{2} = \lVert X^{-1}(Xw^* + \varepsilon) - w^*\rVert_{2}=\lVert X^{-1}\varepsilon \rVert_{2}.$$ With Cauchy-Schwarz, I can see (letting $\lambda_{\min}$ denote the smallest characteristic value of $X$ ) $$ \lVert \hat{w}  - w^*\rVert_{2}\leq \frac{1}{\lambda_{\min}}\lVert \varepsilon\rVert_2.$$ Question : I guess I'm lamenting the lameness of this upper bound. If $\lambda_{\min}\geq 1$ , then this actually isn't a bad estimate for $w^*$ right? Just because the condition number is high doesn't mean $\lambda_{\min}$ has to be tiny - it's just that it's tiny in comparison to $\lambda_{\max}$ . I guess the idea here is that this error could be large provided $\varepsilon$ is large in norm and/or $\lambda_{\min}$ is tiny? Is there a lower bound for this that's more interesting?","Problem : Consider a linear regression problem where we observe noisy observations, where represents the data matrix ( points and features), represents the ground truth, and represents some noise. Suppose is ill-conditioned (i.e. ) and we take as an estimate for . What can I say about this quantity? Thoughts : Well, following our nose we see With Cauchy-Schwarz, I can see (letting denote the smallest characteristic value of ) Question : I guess I'm lamenting the lameness of this upper bound. If , then this actually isn't a bad estimate for right? Just because the condition number is high doesn't mean has to be tiny - it's just that it's tiny in comparison to . I guess the idea here is that this error could be large provided is large in norm and/or is tiny? Is there a lower bound for this that's more interesting?","y=Xw^* + \epsilon, X \in \mathbb{R}^{n\times d} n d w^*\in\mathbb{R}^n \epsilon \in \mathbb{R}^n X \kappa(X)\gg 1 \hat{w}=X^{-1}y w^*  \lVert \hat{w}-w^*\rVert_2  \lVert \hat{w}  - w^*\rVert_{2} = \lVert X^{-1}y - w^* \rVert_{2} = \lVert X^{-1}(Xw^* + \varepsilon) - w^*\rVert_{2}=\lVert X^{-1}\varepsilon \rVert_{2}. \lambda_{\min} X  \lVert \hat{w}  - w^*\rVert_{2}\leq \frac{1}{\lambda_{\min}}\lVert \varepsilon\rVert_2. \lambda_{\min}\geq 1 w^* \lambda_{\min} \lambda_{\max} \varepsilon \lambda_{\min}",['linear-algebra']
97,Does $\mathbb{E}[g(X)Y]=0$ for all functions $g$ imply $\mathbb{E}[Y|X]=0$?,Does  for all functions  imply ?,\mathbb{E}[g(X)Y]=0 g \mathbb{E}[Y|X]=0,"Is there any result saying something like $$\mathbb{E}[g(X)Y]=0 \,\,\forall g\in\mathcal{G}\,\,\,\implies\mathbb{E}[Y\mid X]=0$$ where $\mathcal{G}$ is some function class? Have been searching online but didn't find anything useful. Context: I came across a paper where the objective function is of the form $\mathbb{E}[(X-Y)\mid X]=0$ but the authors instead use $\mathbb{E}[g(X)(X-Y)]=0$ for a class of $g\in\mathcal{G}$ in their actual implementation. I understand the reason is to avoid the conditional moment and replace it by the unconditional one, but are the two moments equivalent (the if direction is easy to show so I'm wondering the other direction...)?","Is there any result saying something like where is some function class? Have been searching online but didn't find anything useful. Context: I came across a paper where the objective function is of the form but the authors instead use for a class of in their actual implementation. I understand the reason is to avoid the conditional moment and replace it by the unconditional one, but are the two moments equivalent (the if direction is easy to show so I'm wondering the other direction...)?","\mathbb{E}[g(X)Y]=0 \,\,\forall g\in\mathcal{G}\,\,\,\implies\mathbb{E}[Y\mid X]=0 \mathcal{G} \mathbb{E}[(X-Y)\mid X]=0 \mathbb{E}[g(X)(X-Y)]=0 g\in\mathcal{G}","['probability-theory', 'measure-theory', 'statistics', 'lp-spaces', 'conditional-expectation']"
98,Distribution of concominant order statistics,Distribution of concominant order statistics,,"Motivating problem: We have $n$ students writing a mock test, and a day after, they write a final test. Let $X_i$ represent the grade (continuous from $0$ to $\infty$ ) of $i$ -th student from the first test, and $Y_i$ from the second test. Let $S$ be the set of $k<n$ best students from the mock test, i.e. $S:=\{i\leq n: X_i> X_{(n-k)}\}$ . Here, $X_{(i)}$ denotes the i-th order statistic, i.e. $X_{(n)} = max(X_1, \dots, X_n)$ . The question is about the distribution of the final grade from the second test between the students in $S$ . In particular, I would like to know the (asymptotic) distribution of $$ \Psi = \frac{1}{k}\sum_{i\in S}F(Y_i),  $$ where $F$ is the distribution of $Y_1$ . More detailed mathematical notation: Let $(X_1, Y_1), \dots, (X_n, Y_n)$ be iid random vectors. Assume some statistical model, for example let $$Y_i = X_i +\varepsilon_i, $$ where $\varepsilon_i$ are iid independent of $X_i$ . Let $F$ be a df of $Y_1$ and let $k<n$ . My question : What is the distribution (or mean) of $$ \Psi = \frac{1}{k}\sum_{i=1}^nF(Y_i)1[X_i> X_{(n-k)}]? $$ Here, $X_{(i)}$ denotes the i-th order statistic, i.e. $X_{(n)} = max(X_1, \dots, X_n)$ . My approach : Consider the case when $\varepsilon_i = 0$ for all $i$ . Then, $\Psi = \frac{1}{k}\sum_{i=1}^kF(Y_{(n-i+1)})=$ average of $k$ largest order statistics. Since $F(Y_i)\sim U(0,1)$ , this is a sum of beta distributions (since it is well-known that order statistics of uniform variables have beta distribution). However, if $\varepsilon_i \neq 0$ , then we are averaging not nessesarily $k$ largest statistics. Do you have any idea what might help with solving this? I heard about concominants of order statistics (section 6.8 in David, H. A.; Nagaraja, H. N. (2003). Order Statistics. Wiley Series in Probability and Statistics), but I am not sure if there is a theorem that can help with this.","Motivating problem: We have students writing a mock test, and a day after, they write a final test. Let represent the grade (continuous from to ) of -th student from the first test, and from the second test. Let be the set of best students from the mock test, i.e. . Here, denotes the i-th order statistic, i.e. . The question is about the distribution of the final grade from the second test between the students in . In particular, I would like to know the (asymptotic) distribution of where is the distribution of . More detailed mathematical notation: Let be iid random vectors. Assume some statistical model, for example let where are iid independent of . Let be a df of and let . My question : What is the distribution (or mean) of Here, denotes the i-th order statistic, i.e. . My approach : Consider the case when for all . Then, average of largest order statistics. Since , this is a sum of beta distributions (since it is well-known that order statistics of uniform variables have beta distribution). However, if , then we are averaging not nessesarily largest statistics. Do you have any idea what might help with solving this? I heard about concominants of order statistics (section 6.8 in David, H. A.; Nagaraja, H. N. (2003). Order Statistics. Wiley Series in Probability and Statistics), but I am not sure if there is a theorem that can help with this.","n X_i 0 \infty i Y_i S k<n S:=\{i\leq n: X_i> X_{(n-k)}\} X_{(i)} X_{(n)} = max(X_1, \dots, X_n) S 
\Psi = \frac{1}{k}\sum_{i\in S}F(Y_i), 
 F Y_1 (X_1, Y_1), \dots, (X_n, Y_n) Y_i = X_i +\varepsilon_i,  \varepsilon_i X_i F Y_1 k<n 
\Psi = \frac{1}{k}\sum_{i=1}^nF(Y_i)1[X_i> X_{(n-k)}]?
 X_{(i)} X_{(n)} = max(X_1, \dots, X_n) \varepsilon_i = 0 i \Psi = \frac{1}{k}\sum_{i=1}^kF(Y_{(n-i+1)})= k F(Y_i)\sim U(0,1) \varepsilon_i \neq 0 k","['statistics', 'uniform-distribution', 'maximum-likelihood', 'order-statistics']"
99,Can we generalize the idea of spatially coupled gaussian matrices to rotationally invariant matrices?,Can we generalize the idea of spatially coupled gaussian matrices to rotationally invariant matrices?,,"Setup: An $(\omega,\Lambda)$ base matrix $W\in\mathbb{R}^{R\times C}$ is described by the coupling width $\omega\geq1$ and the coupling length $\Lambda\geq2\omega-1$ . The matrix has $R=\Lambda+\omega-1$ rows and $C=\Lambda$ columns, with the $(r,c)$ -th entry of the matrix, where $r\in\{1,\dots,R\}$ and $c\in\{1,\dots,C\}$ , given by $$ W_{rc}= \begin{cases} 1/\omega &\text{if $c\leq r\leq c+\omega-1$,} \\ 0 &\text{otherwise.} \end{cases} $$ It is known that we can create a Gaussian spatially coupled matrix $A\in\mathbb{R}^{n\times p}$ (visually, a band diagonal matrix), in the following way: First specify a base matrix $W$ of dimensions $R\times C$ . Replace each entry of the base matrix $W_{rc}$ with an $\frac{n}{R}\times\frac{p}{C}$ matrix with entries drawn i.i.d. $\sim N\big(0,\frac{W_{rc}}{n/R}\big)$ . I would like to extend this idea to rotationally invariant matrices $A$ where $$ A=O^\top\Gamma Q \in\mathbb{R}^{n\times p}, $$ where $\Gamma=\text{diag}(\lambda)$ where $\lambda$ are the singular values, and $O$ and $Q$ are independent Haar orthogonal matrices. Motivation: This would allow us to design more general codes for communication etc. Question: Can we use the same approach of replacing entries of the base matrix $W$ to produce a band diagonal (spatially coupled) matrix $A$ where each sub-matrix of non-zero entries $\widetilde{A}$ in $A$ can be written in the form of $\widetilde{A}=\widetilde{O}^\top\widetilde{\Gamma}\widetilde{Q}$ (where $\widetilde{O}$ , $\widetilde{\Gamma}$ , and $\widetilde{Q}$ are some matrices)? Refer to the image below for a visual representation of my question. If so, then how can I do it? Thanks.","Setup: An base matrix is described by the coupling width and the coupling length . The matrix has rows and columns, with the -th entry of the matrix, where and , given by It is known that we can create a Gaussian spatially coupled matrix (visually, a band diagonal matrix), in the following way: First specify a base matrix of dimensions . Replace each entry of the base matrix with an matrix with entries drawn i.i.d. . I would like to extend this idea to rotationally invariant matrices where where where are the singular values, and and are independent Haar orthogonal matrices. Motivation: This would allow us to design more general codes for communication etc. Question: Can we use the same approach of replacing entries of the base matrix to produce a band diagonal (spatially coupled) matrix where each sub-matrix of non-zero entries in can be written in the form of (where , , and are some matrices)? Refer to the image below for a visual representation of my question. If so, then how can I do it? Thanks.","(\omega,\Lambda) W\in\mathbb{R}^{R\times C} \omega\geq1 \Lambda\geq2\omega-1 R=\Lambda+\omega-1 C=\Lambda (r,c) r\in\{1,\dots,R\} c\in\{1,\dots,C\} 
W_{rc}=
\begin{cases}
1/\omega &\text{if c\leq r\leq c+\omega-1,} \\
0 &\text{otherwise.}
\end{cases}
 A\in\mathbb{R}^{n\times p} W R\times C W_{rc} \frac{n}{R}\times\frac{p}{C} \sim N\big(0,\frac{W_{rc}}{n/R}\big) A 
A=O^\top\Gamma Q \in\mathbb{R}^{n\times p},
 \Gamma=\text{diag}(\lambda) \lambda O Q W A \widetilde{A} A \widetilde{A}=\widetilde{O}^\top\widetilde{\Gamma}\widetilde{Q} \widetilde{O} \widetilde{\Gamma} \widetilde{Q}","['matrices', 'statistics', 'information-theory', 'coding-theory', 'random-matrices']"
