,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is $(tr(A))^n\geq n^n \det(A)$ for a symmetric positive definite matrix $A\in M_{n\times n} (\mathbb{R})$,Is  for a symmetric positive definite matrix,(tr(A))^n\geq n^n \det(A) A\in M_{n\times n} (\mathbb{R}),"If $A\in M_{n\times n} (\mathbb{R})$ a positive definite symmetric matrix, Question is to check if : $$(tr(A))^n\geq n^n \det(A)$$ What i have tried is : As $A\in M_{n\times n} (\mathbb{R})$ a positive definite symmetric matrix, all its eigen values would be positive. let  $a_i>0$ be eigen values of $A$ then i would have : $tr(A)=a_1+a_2+\dots +a_n$ and $\det(A)=a_1a_2\dots a_n$ for given inequality to be true, I should have $(tr(A))^n\geq n^n \det(A)$ i.e., $\big( \frac{tr(A)}{n}\big)^n \geq \det(A)$ i.e., $\big( \frac{a_1+a_2+\dots+a_n}{n}\big)^n \geq a_1a_2\dots a_n$ I guess this should be true as a more general form of A.M-G.M inequality saying $(\frac{a+b}{2})^{\frac{1}{2}}\geq ab$ where $a,b >0$ So, I believe $(tr(A))^n\geq n^n \det(A)$ should be true.. please let me know if i am correct or try providing some hints if i am wrong. EDIT : As every one say that i am correct now, i would like to ""prove"" the result which i have used just like that namely generalization of A.M-G.M inequality.. I tried but could not see this result in detail. SO, i would be thankful if some one can help me in this case.","If $A\in M_{n\times n} (\mathbb{R})$ a positive definite symmetric matrix, Question is to check if : $$(tr(A))^n\geq n^n \det(A)$$ What i have tried is : As $A\in M_{n\times n} (\mathbb{R})$ a positive definite symmetric matrix, all its eigen values would be positive. let  $a_i>0$ be eigen values of $A$ then i would have : $tr(A)=a_1+a_2+\dots +a_n$ and $\det(A)=a_1a_2\dots a_n$ for given inequality to be true, I should have $(tr(A))^n\geq n^n \det(A)$ i.e., $\big( \frac{tr(A)}{n}\big)^n \geq \det(A)$ i.e., $\big( \frac{a_1+a_2+\dots+a_n}{n}\big)^n \geq a_1a_2\dots a_n$ I guess this should be true as a more general form of A.M-G.M inequality saying $(\frac{a+b}{2})^{\frac{1}{2}}\geq ab$ where $a,b >0$ So, I believe $(tr(A))^n\geq n^n \det(A)$ should be true.. please let me know if i am correct or try providing some hints if i am wrong. EDIT : As every one say that i am correct now, i would like to ""prove"" the result which i have used just like that namely generalization of A.M-G.M inequality.. I tried but could not see this result in detail. SO, i would be thankful if some one can help me in this case.",,['linear-algebra']
1,Almost projections in matrix algebras.,Almost projections in matrix algebras.,,"I have a question about projection in matrix algebras over the complex number, that I can not solve.. Let p be a matrix in some  $M_n(\mathbb C)$ and suppose that p is almost self-adjoint (i.e. $||p-p^*||<\epsilon$) and almost idempotent (i.e. $||p-p^2||<\epsilon$). Is there a bound on the distance between $p$ and a projection? In particular is there $k\in\mathbb N$ and $q=q^*=q^2\in M_n(\mathbb C)$ such that $||q-p||<k\epsilon$ where $k$ does NOT depend on $\epsilon$? Thanks a lot!","I have a question about projection in matrix algebras over the complex number, that I can not solve.. Let p be a matrix in some  $M_n(\mathbb C)$ and suppose that p is almost self-adjoint (i.e. $||p-p^*||<\epsilon$) and almost idempotent (i.e. $||p-p^2||<\epsilon$). Is there a bound on the distance between $p$ and a projection? In particular is there $k\in\mathbb N$ and $q=q^*=q^2\in M_n(\mathbb C)$ such that $||q-p||<k\epsilon$ where $k$ does NOT depend on $\epsilon$? Thanks a lot!",,['linear-algebra']
2,The integer $c_n$ in $(1+4\sqrt[3]2-4\sqrt[3]4)^n=a_n+b_n\sqrt[3]2+c_n\sqrt[3]4$,The integer  in,c_n (1+4\sqrt[3]2-4\sqrt[3]4)^n=a_n+b_n\sqrt[3]2+c_n\sqrt[3]4,"For non-negative integer $n$, write $$(1+4\sqrt[3]2-4\sqrt[3]4)^n=a_n+b_n\sqrt[3]2+c_n\sqrt[3]4$$ where $a_n,b_n,c_n$ are integers. For any non-negative integer $m$, prove or disprove $$2^{m+2}|c_n\iff2^m|n$$ So far I have  $$\left[\begin{array}{c}a_n\\b_n\\c_n\end{array}\right]=\left[\begin{array}{ccc}1&-8&8\\4&1&-8\\-4&4&1\end{array}\right]\left[\begin{array}{c}a_{n-1}\\b_{n-1}\\c_{n-1}\end{array}\right].$$","For non-negative integer $n$, write $$(1+4\sqrt[3]2-4\sqrt[3]4)^n=a_n+b_n\sqrt[3]2+c_n\sqrt[3]4$$ where $a_n,b_n,c_n$ are integers. For any non-negative integer $m$, prove or disprove $$2^{m+2}|c_n\iff2^m|n$$ So far I have  $$\left[\begin{array}{c}a_n\\b_n\\c_n\end{array}\right]=\left[\begin{array}{ccc}1&-8&8\\4&1&-8\\-4&4&1\end{array}\right]\left[\begin{array}{c}a_{n-1}\\b_{n-1}\\c_{n-1}\end{array}\right].$$",,"['linear-algebra', 'number-theory', 'elementary-number-theory', 'recurrence-relations', 'contest-math']"
3,What is the significance of the matrix in the LAPACK logo?,What is the significance of the matrix in the LAPACK logo?,,This is the LAPACK linear algebra library logo: What is the significance of this matrix?,This is the LAPACK linear algebra library logo: What is the significance of this matrix?,,"['linear-algebra', 'numerical-linear-algebra']"
4,The maximum eigenvalue of the sum of two matrix,The maximum eigenvalue of the sum of two matrix,,"Suppose there are two matrix $A$ and $B$. The components of each matrix is non-negative. And  $$Ax_1=\lambda_1 x_1  $$ where $\lambda_1$ is the maximum eigenvalue of $A$. Similarly $$Bx_2=\lambda_2 x_2  $$ where $\lambda_2$ is the maximum eigenvalue of $B$. Let $C = A+B$ And  $$Cx=\lambda x  $$ where $\lambda$ is the maximum eigenvalue of $C$. From wiki( https://en.wikipedia.org/wiki/Matrix_norm ), it shows that $$\left\| A+B \right\|\le \left\| A \right\|+\left\| B \right\|$$ The maximum eigenvalue is 2 norm. So no matter the components are negative or not, $$\lambda\le \lambda_1+\lambda_2$$","Suppose there are two matrix $A$ and $B$. The components of each matrix is non-negative. And  $$Ax_1=\lambda_1 x_1  $$ where $\lambda_1$ is the maximum eigenvalue of $A$. Similarly $$Bx_2=\lambda_2 x_2  $$ where $\lambda_2$ is the maximum eigenvalue of $B$. Let $C = A+B$ And  $$Cx=\lambda x  $$ where $\lambda$ is the maximum eigenvalue of $C$. From wiki( https://en.wikipedia.org/wiki/Matrix_norm ), it shows that $$\left\| A+B \right\|\le \left\| A \right\|+\left\| B \right\|$$ The maximum eigenvalue is 2 norm. So no matter the components are negative or not, $$\lambda\le \lambda_1+\lambda_2$$",,['linear-algebra']
5,When are two commuting linear operators functions of each other,When are two commuting linear operators functions of each other,,"I've computed that the following is valid for certain functions but I've hit a slight bump in my proof. I was wondering if someone could clear it up. If we formally consider the integral operators $$E f(s) = \frac{1}{\Gamma(-s)} \int_{0}^{\infty} f(-y) y^{-s-1} dy$$ and $$Y f(s) = \int_{-\infty}^{\infty} f(y) \frac{s^y}{\Gamma(y+1)} dy$$ I've shown that if $Q f(s) = s f(s)$ then $$Q Y E f = Y E Q f$$ and $$\frac{d}{ds} Y E f = Y E \frac{df}{ds}$$ Q : What step should I use to show that, because they commute, $YE$ is a   function of $Q$ and $ \frac{d}{ds}$ and therefore the constant linear   operator; $\alpha = \alpha Q^0 = \alpha \frac{d^0}{ds^0}$? I mean that $Y = E^{-1}$, which I have verified for a few functions. Considering only such functions that converge I won't go into that here.","I've computed that the following is valid for certain functions but I've hit a slight bump in my proof. I was wondering if someone could clear it up. If we formally consider the integral operators $$E f(s) = \frac{1}{\Gamma(-s)} \int_{0}^{\infty} f(-y) y^{-s-1} dy$$ and $$Y f(s) = \int_{-\infty}^{\infty} f(y) \frac{s^y}{\Gamma(y+1)} dy$$ I've shown that if $Q f(s) = s f(s)$ then $$Q Y E f = Y E Q f$$ and $$\frac{d}{ds} Y E f = Y E \frac{df}{ds}$$ Q : What step should I use to show that, because they commute, $YE$ is a   function of $Q$ and $ \frac{d}{ds}$ and therefore the constant linear   operator; $\alpha = \alpha Q^0 = \alpha \frac{d^0}{ds^0}$? I mean that $Y = E^{-1}$, which I have verified for a few functions. Considering only such functions that converge I won't go into that here.",,"['linear-algebra', 'complex-analysis', 'analysis', 'functional-analysis', 'operator-theory']"
6,Do complex eigenvalues of a real matrix imply a rotation-dilation?,Do complex eigenvalues of a real matrix imply a rotation-dilation?,,"This is part of a bigger proof that if there is a compact set, $K \subset \mathbb R^n$ such that the linear transformation $L$ maps $K$ into its interior, the eigenvalues $\lambda_i$ are all of absolute value less than 1. Clearly, if $L(K)\subset \operatorname{int}(K)$ then $L^n(K)\subset \operatorname{int}(K)$. The easiest case seems to be: If the eigenvalue were an $n$th root of unity, then $L^n$ must have some fixed point on the boundary, a contradiction. I can't even seem to prove it for complex eigenvalues of absolute value greater than $1$ or for eigenvalues of absolute value $1$ which are not roots of unity. My attempts: 1) Say $|\lambda|>1$ then it seems like $L$ would take the vectors off to infinity. However, I am worried there is a case where $L$ takes some vector, lengthens it, and maps it to some other vector, and on the next iteration it undoes, or something like that. 2) Say $|\lambda|=1$. If a linear transformation on $\mathbb R^n$ has a pair of complex eigenvalues of absolute value 1 (but not roots of unity), is it true that this always corresponds to a rotation on a two dimensional subspace? If so, then it seems like this is an irrational rotation. So start with some point on the boundary of $K$ that is on the subspace of rotation, then look at the infinite union of $L^i(K)$ for $i>1$. The original point is a limit point of this union, and since $K$ was compact and since $L$ is continuous since it is a finite dimensional linear operator, $L^i(K)$ are all compact....but this isn't good because now the point is the limit point of an infinite union of compact sets which may not be compact and thus don't need to contain their limit points. But if it did contain this point this would be a contradiction. Please and thank you for helping :D","This is part of a bigger proof that if there is a compact set, $K \subset \mathbb R^n$ such that the linear transformation $L$ maps $K$ into its interior, the eigenvalues $\lambda_i$ are all of absolute value less than 1. Clearly, if $L(K)\subset \operatorname{int}(K)$ then $L^n(K)\subset \operatorname{int}(K)$. The easiest case seems to be: If the eigenvalue were an $n$th root of unity, then $L^n$ must have some fixed point on the boundary, a contradiction. I can't even seem to prove it for complex eigenvalues of absolute value greater than $1$ or for eigenvalues of absolute value $1$ which are not roots of unity. My attempts: 1) Say $|\lambda|>1$ then it seems like $L$ would take the vectors off to infinity. However, I am worried there is a case where $L$ takes some vector, lengthens it, and maps it to some other vector, and on the next iteration it undoes, or something like that. 2) Say $|\lambda|=1$. If a linear transformation on $\mathbb R^n$ has a pair of complex eigenvalues of absolute value 1 (but not roots of unity), is it true that this always corresponds to a rotation on a two dimensional subspace? If so, then it seems like this is an irrational rotation. So start with some point on the boundary of $K$ that is on the subspace of rotation, then look at the infinite union of $L^i(K)$ for $i>1$. The original point is a limit point of this union, and since $K$ was compact and since $L$ is continuous since it is a finite dimensional linear operator, $L^i(K)$ are all compact....but this isn't good because now the point is the limit point of an infinite union of compact sets which may not be compact and thus don't need to contain their limit points. But if it did contain this point this would be a contradiction. Please and thank you for helping :D",,['linear-algebra']
7,Cool/Useful Examples of Characteristic and Minimal Polynomials?,Cool/Useful Examples of Characteristic and Minimal Polynomials?,,"I'm teaching a Linear Algebra II undergrad course and for the section on characteristic & minimal polynomials, I really don't want to just give the students a bunch of matrices that have no meaning and ask them to find the char/min poly. I'm looking for cool/useful examples. Got any favourites? So far, the only cool/useful examples I can think of are the characteristic poly of a companion matrix (since companion matrices will come up in other math courses the students might take) and the char&min polys of matrices of the form a's on the diagonal and b's everywhere else (yes, this is ""cool"" in my opinion, because once you do the general case, you can just read off the answer for a specific matrix of this form).","I'm teaching a Linear Algebra II undergrad course and for the section on characteristic & minimal polynomials, I really don't want to just give the students a bunch of matrices that have no meaning and ask them to find the char/min poly. I'm looking for cool/useful examples. Got any favourites? So far, the only cool/useful examples I can think of are the characteristic poly of a companion matrix (since companion matrices will come up in other math courses the students might take) and the char&min polys of matrices of the form a's on the diagonal and b's everywhere else (yes, this is ""cool"" in my opinion, because once you do the general case, you can just read off the answer for a specific matrix of this form).",,"['linear-algebra', 'matrices', 'soft-question', 'big-list', 'characteristic-polynomial']"
8,Can the product of some matrices equal the identity matrix?,Can the product of some matrices equal the identity matrix?,,"Let $A=\begin{pmatrix} 1 & 2 \\ 0 & 1\end{pmatrix}$ and $B=\begin{pmatrix} 1 & 0 \\ -2 & 1\end{pmatrix}$ . Can a product $X_1 X_2...X_n$ be equal to the identity matrix if every factor $X_i$ equals either $A$ or $B$ ? I believe that the answer is negative, but I don't really know how to prove it. I thought of two approaches: We could try doing this by induction. The base case is trivial since neither of $A$ nor $B$ are the identity matrix. However, I don't know how to go from $n$ to $n+1$ . Maybe we should use the fact that if $U, V$ are two square matrices such that $UV$ is the identity, then $VU$ is also the identity. I guess that we should somehow shuffle the order of the factors using this observation, but then again, I don't know how to use this.","Let and . Can a product be equal to the identity matrix if every factor equals either or ? I believe that the answer is negative, but I don't really know how to prove it. I thought of two approaches: We could try doing this by induction. The base case is trivial since neither of nor are the identity matrix. However, I don't know how to go from to . Maybe we should use the fact that if are two square matrices such that is the identity, then is also the identity. I guess that we should somehow shuffle the order of the factors using this observation, but then again, I don't know how to use this.","A=\begin{pmatrix} 1 & 2 \\ 0 & 1\end{pmatrix} B=\begin{pmatrix} 1 & 0 \\ -2 & 1\end{pmatrix} X_1 X_2...X_n X_i A B A B n n+1 U, V UV VU","['linear-algebra', 'matrices', 'contest-math']"
9,Construction of a connection on a vector bundle $E\to M$ with trivial determinant bundle,Construction of a connection on a vector bundle  with trivial determinant bundle,E\to M,"I read the following statement, and I am trying to see why it is true. Let $E\to M$ be a rank 2 complex vector bundle over a smooth manifold $M$ , such that the determinant bundle $\det(E)=\Lambda^2(E)$ is the trivial line bundle over $M$ , with trivialization $\phi:\det(E)\to M\times \mathbb{C}$ . Then there exists a connection $\nabla^E$ on $E$ which induces the trivial connection on $\det(E)$ , under $\phi$ . I wanted to construct $\nabla^E$ by defining local connections, which I then glue together by a partition of unity. Let $\{U_\alpha\}$ be a trivializing open cover of $M$ . Over each $U_\alpha$ , I have a local frame $\{e_1^\alpha,e_2^\alpha\}$ such that $e_1^\alpha\wedge e_2^\alpha$ corresponds to 1 under the diffeomorphism $\phi$ . On each $U_\alpha$ , I can define a flat connection $\nabla^\alpha$ (so by declaring the sections $e^\alpha_i$ to be flat). If $(\rho_\alpha)$ is a partition of unity subordinated to the trivializing open cover, I define $\nabla^E(s)=\sum\limits_\alpha \nabla^\alpha(\rho_\alpha s)$ , which is indeed a connection. Next, I want to check that it induces the trivial connection on $\det(E)$ under $\phi$ . Note that $\phi^{-1}(1)=\sum\limits_\alpha \rho_\alpha e_1^\alpha\wedge e_2^\alpha.$ Then \begin{align*} \nabla^{\det(E)}\big(\sum_\alpha \rho_\alpha e_1^\alpha\wedge e_2^\alpha\big)&=\sum_\alpha \nabla^{\det(E)}\big(\rho_\alpha e_1^\alpha\wedge e_2^\alpha)\\&=\sum_\alpha \nabla^E(\rho_\alpha e^\alpha_1)\wedge e_2^\alpha+\rho_\alpha e_1^\alpha\wedge \nabla^E(e_2^\alpha)\\&=\sum_\alpha \rho_\alpha\big(\nabla^E(e_1^\alpha)\wedge e_2^\alpha+e_1^\alpha\wedge \nabla^E(e_2^\alpha)\big). \end{align*} I want this to be zero, but the problem I encounter is that $\nabla^E(e_i^\alpha)\neq\nabla^\alpha(e_i^\alpha)=0$ . Nevertheless, is this the correct approach to take and how can I fix it? Does summing over all $\alpha$ make sure it is correct again? EDIT: This question seems to have been partially asked here: Connections on Bundles with Trivial Determinant But it seems that an answer is missing and I am very curious, it seems to take a different approach, by introducing a Hermitean metric on $E$ .","I read the following statement, and I am trying to see why it is true. Let be a rank 2 complex vector bundle over a smooth manifold , such that the determinant bundle is the trivial line bundle over , with trivialization . Then there exists a connection on which induces the trivial connection on , under . I wanted to construct by defining local connections, which I then glue together by a partition of unity. Let be a trivializing open cover of . Over each , I have a local frame such that corresponds to 1 under the diffeomorphism . On each , I can define a flat connection (so by declaring the sections to be flat). If is a partition of unity subordinated to the trivializing open cover, I define , which is indeed a connection. Next, I want to check that it induces the trivial connection on under . Note that Then I want this to be zero, but the problem I encounter is that . Nevertheless, is this the correct approach to take and how can I fix it? Does summing over all make sure it is correct again? EDIT: This question seems to have been partially asked here: Connections on Bundles with Trivial Determinant But it seems that an answer is missing and I am very curious, it seems to take a different approach, by introducing a Hermitean metric on .","E\to M M \det(E)=\Lambda^2(E) M \phi:\det(E)\to M\times \mathbb{C} \nabla^E E \det(E) \phi \nabla^E \{U_\alpha\} M U_\alpha \{e_1^\alpha,e_2^\alpha\} e_1^\alpha\wedge e_2^\alpha \phi U_\alpha \nabla^\alpha e^\alpha_i (\rho_\alpha) \nabla^E(s)=\sum\limits_\alpha \nabla^\alpha(\rho_\alpha s) \det(E) \phi \phi^{-1}(1)=\sum\limits_\alpha \rho_\alpha e_1^\alpha\wedge e_2^\alpha. \begin{align*}
\nabla^{\det(E)}\big(\sum_\alpha \rho_\alpha e_1^\alpha\wedge e_2^\alpha\big)&=\sum_\alpha \nabla^{\det(E)}\big(\rho_\alpha e_1^\alpha\wedge e_2^\alpha)\\&=\sum_\alpha \nabla^E(\rho_\alpha e^\alpha_1)\wedge e_2^\alpha+\rho_\alpha e_1^\alpha\wedge \nabla^E(e_2^\alpha)\\&=\sum_\alpha \rho_\alpha\big(\nabla^E(e_1^\alpha)\wedge e_2^\alpha+e_1^\alpha\wedge \nabla^E(e_2^\alpha)\big).
\end{align*} \nabla^E(e_i^\alpha)\neq\nabla^\alpha(e_i^\alpha)=0 \alpha E","['linear-algebra', 'differential-geometry']"
10,"Section 3.1.1. ""Complete Integrals"" from Evans PDE book","Section 3.1.1. ""Complete Integrals"" from Evans PDE book",,"Question about an observation taken from p. 92-93 of the book {1} in the title. With $$ F(Du,u,x) = 0 \tag{1} $$ We denote a non linear first order PDE. Here $u$ is the scalar function of vector variable $x$ and $Du = (u_{x_1}, \ldots, u_{x_n})$ is the gradient. From the book NOTATION. We write $$(D_a u, D^2_{xa}u) =  \begin{pmatrix} u_{a_1} & u_{x_1a_1} &\dots & u_{x_n a_1} \\ \vdots &   \vdots & \ddots & \vdots \\ u_{a_n} & u_{x_1a_n} &\dots & u_{x_n a_n} \end{pmatrix}_{n \times (n+1)} \tag{2} $$ where $u(x;a)$ is a solution for $F$ parametrized by $a \in A \subset \mathbb{R}^n$ . DEFINITION. A $C^2$ function $u = u(x;a)$ is called complete integral in $U\times A$ provided (i) $\quad u(x;a)$ solves $(1)$ for each $a \in A$ (ii) $\quad \text{rank}(D_a u, D^2_{xa}u) = n \quad (x \in U, a \in A)$ After this definition we have Interpretation. Condition (ii) ensures $u(x;a)$ ""depends on all the $n$ independent parameters $a_1,\ldots,a_n$ "". To see this suppose $B \subset \mathbb{R}^{n-1}$ is open, and for each $b \in B$ assume $v = v(x;b)$ , $(x \in U)$ is a solution of $(1)$ . Suppose also there exists a $C^1$ mapping $\psi : A \to B$ , $\psi = (\psi^1,\ldots,\psi^{n-1})$ such that $$ u(x;a) = v(x;\psi(a)) \quad (x \in U, a \in A) \tag{3} $$ That is, we are supposing the function $u(x;a)$ ""really depends only on the $n-1$ parameters $b_1,\ldots, b_{n-1}$ "". But then $$ u_{x_i a_j}(x;a) = \sum_{k=1}^{n-1} v_{x_i b_k}(x;\psi(a)) \psi_{a_j}^k(a) \quad (i,j = 1,\ldots, n) \tag{*} $$ Consequently $$ \det(D_{xa}^2 u) = \sum_{k_1,\ldots, k_n = 1}^{n-1} v_{x_1 b_{k_1}} \ldots v_{x_n b_{k_n}} \det \begin{pmatrix} \psi_{a_1}^{k_1} & \ldots & \psi_{a_n}^{k_1} \\  & \ddots &  \\ \psi_{a_1}^{k_n} & \ldots & \psi_{a_n}^{k_n} \end{pmatrix} \tag{**} $$ I don't get how $(**)$ follows from $(*)$ . To me $(*)$ is the $(i,j)$ entry of matrix defined as product between two matrices namely $$ V(x;a) =  \begin{pmatrix} v_{x_1,b_1}(x;a) & \ldots & v_{x_1,b_{n-1}}(x;a) \\ \vdots & \ddots & \vdots \\ v_{x_n,b_1}(x;a) & \ldots & v_{x_n,b_{n-1}}(x;a) \end{pmatrix} $$ and $$ \Psi(a) =  \begin{pmatrix} \psi_{a_1}^1(a) & \ldots & \psi_{a_n}^{1}(a) \\ \vdots & \ddots & \vdots \\ \psi_{a_1}^{n-1}(a) & \ldots & \psi_{a_n}^{n-1}(a) \end{pmatrix} $$ So $$ D^2_{xa} u = V(x;a) \Psi(a) $$ But I cannot manage from this to derive the determinant, because the matrices are not square, but rectangular so I cannot apply the Binet rule for the product of determinants. Any suggestion? Also continuing with the chapter it seems to me that in order to use this method I'd need to find a complete integral first, so I can generate other solutions. However at this point the question is should I use the characteristic method described later to find a complete integral first? {1} L.C. Evans, Partial Differential Equations , 2nd ed., American Mathematical Soc., 2010. Update : I'd still like to understand where (**) comes from, however I've found a workaround that doesn't use any explicit computation. I can use the dimension theorem to reach the same conclusion, since I can regards $V(x;a)$ and $\Psi(a)$ as linear maps. Since $V(x;a) : \mathbb{R}^{n-1} \to \mathbb{R}^n$ and $\Psi(a) : \mathbb{R}^n \to \mathbb{R}^{n-1}$ then we must have $rank(V(x;a)) \leq n - 1$ and $rank(\Psi(a)) \leq n - 1$ , more specifically we have $dim(Ker(\Psi(a))) \geq 1$ (which means an entire subspace of dimension at least $1$ is mapped to $0$ ). This yields to state $dim(ker(V(x;a)\Psi(a))) \geq 1 \Rightarrow rank(V(x;a)\Psi(a)) \leq n - 1$ which in turnes yields $rank(D^2_{xa}u) \leq n - 1$ , since $D^2_{xa}u : \mathbb{R}^n \to \mathbb{R}^n$ then the determinant must be 0. I think the arguments works fine, but I think Evans uses computation like (**) in the book so understanding where it comes from might make my life easier in the future.","Question about an observation taken from p. 92-93 of the book {1} in the title. With We denote a non linear first order PDE. Here is the scalar function of vector variable and is the gradient. From the book NOTATION. We write where is a solution for parametrized by . DEFINITION. A function is called complete integral in provided (i) solves for each (ii) After this definition we have Interpretation. Condition (ii) ensures ""depends on all the independent parameters "". To see this suppose is open, and for each assume , is a solution of . Suppose also there exists a mapping , such that That is, we are supposing the function ""really depends only on the parameters "". But then Consequently I don't get how follows from . To me is the entry of matrix defined as product between two matrices namely and So But I cannot manage from this to derive the determinant, because the matrices are not square, but rectangular so I cannot apply the Binet rule for the product of determinants. Any suggestion? Also continuing with the chapter it seems to me that in order to use this method I'd need to find a complete integral first, so I can generate other solutions. However at this point the question is should I use the characteristic method described later to find a complete integral first? {1} L.C. Evans, Partial Differential Equations , 2nd ed., American Mathematical Soc., 2010. Update : I'd still like to understand where (**) comes from, however I've found a workaround that doesn't use any explicit computation. I can use the dimension theorem to reach the same conclusion, since I can regards and as linear maps. Since and then we must have and , more specifically we have (which means an entire subspace of dimension at least is mapped to ). This yields to state which in turnes yields , since then the determinant must be 0. I think the arguments works fine, but I think Evans uses computation like (**) in the book so understanding where it comes from might make my life easier in the future.","
F(Du,u,x) = 0 \tag{1}
 u x Du = (u_{x_1}, \ldots, u_{x_n}) (D_a u, D^2_{xa}u) = 
\begin{pmatrix}
u_{a_1} & u_{x_1a_1} &\dots & u_{x_n a_1} \\
\vdots &   \vdots & \ddots & \vdots \\
u_{a_n} & u_{x_1a_n} &\dots & u_{x_n a_n}
\end{pmatrix}_{n \times (n+1)} \tag{2}
 u(x;a) F a \in A \subset \mathbb{R}^n C^2 u = u(x;a) U\times A \quad u(x;a) (1) a \in A \quad \text{rank}(D_a u, D^2_{xa}u) = n \quad (x \in U, a \in A) u(x;a) n a_1,\ldots,a_n B \subset \mathbb{R}^{n-1} b \in B v = v(x;b) (x \in U) (1) C^1 \psi : A \to B \psi = (\psi^1,\ldots,\psi^{n-1}) 
u(x;a) = v(x;\psi(a)) \quad (x \in U, a \in A) \tag{3}
 u(x;a) n-1 b_1,\ldots, b_{n-1} 
u_{x_i a_j}(x;a) = \sum_{k=1}^{n-1} v_{x_i b_k}(x;\psi(a)) \psi_{a_j}^k(a) \quad (i,j = 1,\ldots, n) \tag{*}
 
\det(D_{xa}^2 u) = \sum_{k_1,\ldots, k_n = 1}^{n-1} v_{x_1 b_{k_1}} \ldots v_{x_n b_{k_n}} \det \begin{pmatrix}
\psi_{a_1}^{k_1} & \ldots & \psi_{a_n}^{k_1} \\
 & \ddots &  \\
\psi_{a_1}^{k_n} & \ldots & \psi_{a_n}^{k_n}
\end{pmatrix} \tag{**}
 (**) (*) (*) (i,j) 
V(x;a) = 
\begin{pmatrix}
v_{x_1,b_1}(x;a) & \ldots & v_{x_1,b_{n-1}}(x;a) \\
\vdots & \ddots & \vdots \\
v_{x_n,b_1}(x;a) & \ldots & v_{x_n,b_{n-1}}(x;a)
\end{pmatrix}
 
\Psi(a) = 
\begin{pmatrix}
\psi_{a_1}^1(a) & \ldots & \psi_{a_n}^{1}(a) \\
\vdots & \ddots & \vdots \\
\psi_{a_1}^{n-1}(a) & \ldots & \psi_{a_n}^{n-1}(a)
\end{pmatrix}
 
D^2_{xa} u = V(x;a) \Psi(a)
 V(x;a) \Psi(a) V(x;a) : \mathbb{R}^{n-1} \to \mathbb{R}^n \Psi(a) : \mathbb{R}^n \to \mathbb{R}^{n-1} rank(V(x;a)) \leq n - 1 rank(\Psi(a)) \leq n - 1 dim(Ker(\Psi(a))) \geq 1 1 0 dim(ker(V(x;a)\Psi(a))) \geq 1 \Rightarrow rank(V(x;a)\Psi(a)) \leq n - 1 rank(D^2_{xa}u) \leq n - 1 D^2_{xa}u : \mathbb{R}^n \to \mathbb{R}^n","['linear-algebra', 'proof-verification', 'partial-differential-equations', 'proof-explanation']"
11,Holomorphic Lefschetz formula and basic linear algebra.,Holomorphic Lefschetz formula and basic linear algebra.,,"There is a well-known way to prove the fact that the field of complex numbers is algebraically closed using Lefschetz fixed point theorem. Let me recall the idea: The existence of a root for any polynomial with complex coefficients is equivalent to the existence of an eigenvector for any endomoprhism of a finite-dimensional vector space over $\mathbb{C}$. Indeed, any polynomial is a characteristic polynomial of certain matrix. Assume, that $V$ is a finite-dimensional complex vector space and $A \colon V \to V$ is an endomorphism without eigenvectors. In particular, $A$ has trivial kernel, so it induces an automorphism of the projectivization: $$\overline{A} \colon \mathbb{P}(V) \to \mathbb{P}(V).$$ The group $\mathrm{PGL}(n, \mathbb{C})$ is connected (one doesn't need the main theorem of algebra to prove this), so $\overline{A}$ is homotopic to identity as a diffeomorphism of $\mathbb{P}(V)$. Thus its Lefschetz number equals Euler characteristic of complex projective space, which is non-zero. By Lefschetz fixed point theorem, this implies that $\overline{A}$ has a fixed point. If $l$ is a line fixed by $\overline{A}$, then any vector $v \in l$ is eigen for $A$. Now, my question is the following: what if we apply the holomorphic Lefschetz theorem to a linear automorphism of a projective space? In this case it is easy to compute the right-hand-side: since the only non-tirivial Dolbeaut cohomology group of projective space is $H^{0,0}(\mathbb{CP}^n)$,and $\overline{A}$ acts on it trivial, the holomorphic Lefschetz number equals $1$. What is the left-hand-side in this case? It seems to be certain algebraic expression in coefficients of $A$. However, after thinking about it for some time I am not able to write it down explicitly.","There is a well-known way to prove the fact that the field of complex numbers is algebraically closed using Lefschetz fixed point theorem. Let me recall the idea: The existence of a root for any polynomial with complex coefficients is equivalent to the existence of an eigenvector for any endomoprhism of a finite-dimensional vector space over $\mathbb{C}$. Indeed, any polynomial is a characteristic polynomial of certain matrix. Assume, that $V$ is a finite-dimensional complex vector space and $A \colon V \to V$ is an endomorphism without eigenvectors. In particular, $A$ has trivial kernel, so it induces an automorphism of the projectivization: $$\overline{A} \colon \mathbb{P}(V) \to \mathbb{P}(V).$$ The group $\mathrm{PGL}(n, \mathbb{C})$ is connected (one doesn't need the main theorem of algebra to prove this), so $\overline{A}$ is homotopic to identity as a diffeomorphism of $\mathbb{P}(V)$. Thus its Lefschetz number equals Euler characteristic of complex projective space, which is non-zero. By Lefschetz fixed point theorem, this implies that $\overline{A}$ has a fixed point. If $l$ is a line fixed by $\overline{A}$, then any vector $v \in l$ is eigen for $A$. Now, my question is the following: what if we apply the holomorphic Lefschetz theorem to a linear automorphism of a projective space? In this case it is easy to compute the right-hand-side: since the only non-tirivial Dolbeaut cohomology group of projective space is $H^{0,0}(\mathbb{CP}^n)$,and $\overline{A}$ acts on it trivial, the holomorphic Lefschetz number equals $1$. What is the left-hand-side in this case? It seems to be certain algebraic expression in coefficients of $A$. However, after thinking about it for some time I am not able to write it down explicitly.",,"['linear-algebra', 'algebraic-geometry', 'complex-geometry']"
12,Markov's Mouse and the Maze,Markov's Mouse and the Maze,,"I recently started to learn about Markov Chains and had a problem regarding the expected time to absorption: Problem: Markov has an untrained mouse that he place in a maze. The mouse may move between adjoining rooms, or stay in the same room at any time-step. At the exit of the maze is an enormous piece of unscented cheese where the mouse may leave from and never return. Given the following transition matrix: $$\begin{pmatrix}\frac 1{10}&\frac 3{10}&\frac 35&0&0&0\\\frac 12&\frac 12 &0&0&0&0\\\frac 3{10}&0&\frac 1{10}&\frac 35&0&0\\0&0&\frac 3{10}&\frac 1{10}&\frac 3{10}&\frac 3{10}\\0&0&0&\frac 12&\frac 12&0\\0&0&0&0&0&1\\ \end{pmatrix}$$ (a) What is the probability it leaves the maze, given it started in State $i \quad \forall \ i \in[0,5]$ ?? (b) How long before it leaves the maze, given it started in State $i \quad \forall \ i \in[0,5]$ ? My Attempt: Well first I made a diagram to get the understanding of the problem visually. I replace the number States to letters, i.e. State $1$ as State $A$ etc. and wrote down the transition probabilities from the matrix. (a) I think because there exists an absorbing state at the exit, then the probability of exiting must be $1$ a.s. . (b) I used First-Step-Analysis to find the expected time for each $i$. Let $v_i$ be the expected time to exit from starting position $i$, i.e. $v_i =\Bbb E_i[T_{\{5\}}]$. Then solving the system of equations: $$ \left\{ \begin{array}{ll} v_A &=1+\frac 1{10} v_A+\frac 3{10} v_B+\frac 35 v_C \\ v_B &=1+\frac 12 v_A +\frac 12 v_B\\  v_C &=1+\frac 3{10} v_A+\frac 1{10} v_C+\frac 35 v_D \\ v_D &=1+\frac 3{10} v_C+\frac 1{10} v_D+\frac 3{10} v_E+\frac 3{10} v_F \\ v_E &=1+\frac 12 v_D+\frac 12 v_E\\ v_F &=0 \end{array}  \right. $$ So I get the following solution: $$\Bbb E_A[T_{\{5\}}]=14; \quad \Bbb E_B[T_{\{5\}}]=16 \quad \Bbb E_C[T_{\{5\}}]=11 \frac 13 \quad \Bbb E_D[T_{\{5\}}]=8 \frac 13 \quad \Bbb E_E[T_{\{5\}}]= 10 \frac 13$$ I'm not $100\%$ sure about the part (b). My Question(s): I was wondering, is there another way to look at this problem without using Markov Chains or First-Step-Analysis? I feel like there are geometric random variables involved here. Also what would happen if there were 2 exits, could I apply the exact same principles or is something changing? Thank you in advance!","I recently started to learn about Markov Chains and had a problem regarding the expected time to absorption: Problem: Markov has an untrained mouse that he place in a maze. The mouse may move between adjoining rooms, or stay in the same room at any time-step. At the exit of the maze is an enormous piece of unscented cheese where the mouse may leave from and never return. Given the following transition matrix: $$\begin{pmatrix}\frac 1{10}&\frac 3{10}&\frac 35&0&0&0\\\frac 12&\frac 12 &0&0&0&0\\\frac 3{10}&0&\frac 1{10}&\frac 35&0&0\\0&0&\frac 3{10}&\frac 1{10}&\frac 3{10}&\frac 3{10}\\0&0&0&\frac 12&\frac 12&0\\0&0&0&0&0&1\\ \end{pmatrix}$$ (a) What is the probability it leaves the maze, given it started in State $i \quad \forall \ i \in[0,5]$ ?? (b) How long before it leaves the maze, given it started in State $i \quad \forall \ i \in[0,5]$ ? My Attempt: Well first I made a diagram to get the understanding of the problem visually. I replace the number States to letters, i.e. State $1$ as State $A$ etc. and wrote down the transition probabilities from the matrix. (a) I think because there exists an absorbing state at the exit, then the probability of exiting must be $1$ a.s. . (b) I used First-Step-Analysis to find the expected time for each $i$. Let $v_i$ be the expected time to exit from starting position $i$, i.e. $v_i =\Bbb E_i[T_{\{5\}}]$. Then solving the system of equations: $$ \left\{ \begin{array}{ll} v_A &=1+\frac 1{10} v_A+\frac 3{10} v_B+\frac 35 v_C \\ v_B &=1+\frac 12 v_A +\frac 12 v_B\\  v_C &=1+\frac 3{10} v_A+\frac 1{10} v_C+\frac 35 v_D \\ v_D &=1+\frac 3{10} v_C+\frac 1{10} v_D+\frac 3{10} v_E+\frac 3{10} v_F \\ v_E &=1+\frac 12 v_D+\frac 12 v_E\\ v_F &=0 \end{array}  \right. $$ So I get the following solution: $$\Bbb E_A[T_{\{5\}}]=14; \quad \Bbb E_B[T_{\{5\}}]=16 \quad \Bbb E_C[T_{\{5\}}]=11 \frac 13 \quad \Bbb E_D[T_{\{5\}}]=8 \frac 13 \quad \Bbb E_E[T_{\{5\}}]= 10 \frac 13$$ I'm not $100\%$ sure about the part (b). My Question(s): I was wondering, is there another way to look at this problem without using Markov Chains or First-Step-Analysis? I feel like there are geometric random variables involved here. Also what would happen if there were 2 exits, could I apply the exact same principles or is something changing? Thank you in advance!",,"['linear-algebra', 'probability', 'random-variables', 'markov-chains']"
13,"Given finite field projections of a rational, can I find the original rational?","Given finite field projections of a rational, can I find the original rational?",,"If you don't know a particular integer $a$, but you know several residues ($r_i$) of $a$ modulo several mutually prime integers ($m_i$), you can use the Chinese Remainder Theorem to find $r$ such that $a = r + k\prod{m_i}$, varying over $k$, with $0 \le r < \prod{m_i}$.  If you know $0 \le a < \prod{m_i}$, then you have $a = r$. Every rational number $a/b$ can be projected onto  a prime finite field $\mathbb{Z}/\mathbb{Z}_p$ (given that $b$ is invertible, i.e. $\gcd(p,b) = 1)$, for many $p$. Given a collection of these projections, $i$ pairs $(r_i, p_i)$ where $a/b \equiv r_i \mod p_i$, is there some process similar to the Chinese Remainder Theorem by which you can take that collection of pairs and work backwards to find $a/b$? For example, given the list of pairs $[(2,3), (3,5), (6,11), (500000004, 1000000007)]$, can you work backwards to find $1/2$? I realize that there will always be an infinity of integer and rational solutions for every collection of residues, but there will always be a smallest solution corresponding to the $0 \le r < \prod{m_i}$ above, perhaps a $a/b$ minimizing $|ab|$ or $a^2b^2$, and I would like to find that. If you need to, add in the ability to generate new residues for arbitrary given primes instead of working from a fixed input collection of pairs.  These rational numbers I'm trying to solve for are solutions to some large linear equations, and these equations are convenient to solve over ""small"" finite fields ($p \le 2^{32}$), but inconvenient to solve over infinite-precision rationals.","If you don't know a particular integer $a$, but you know several residues ($r_i$) of $a$ modulo several mutually prime integers ($m_i$), you can use the Chinese Remainder Theorem to find $r$ such that $a = r + k\prod{m_i}$, varying over $k$, with $0 \le r < \prod{m_i}$.  If you know $0 \le a < \prod{m_i}$, then you have $a = r$. Every rational number $a/b$ can be projected onto  a prime finite field $\mathbb{Z}/\mathbb{Z}_p$ (given that $b$ is invertible, i.e. $\gcd(p,b) = 1)$, for many $p$. Given a collection of these projections, $i$ pairs $(r_i, p_i)$ where $a/b \equiv r_i \mod p_i$, is there some process similar to the Chinese Remainder Theorem by which you can take that collection of pairs and work backwards to find $a/b$? For example, given the list of pairs $[(2,3), (3,5), (6,11), (500000004, 1000000007)]$, can you work backwards to find $1/2$? I realize that there will always be an infinity of integer and rational solutions for every collection of residues, but there will always be a smallest solution corresponding to the $0 \le r < \prod{m_i}$ above, perhaps a $a/b$ minimizing $|ab|$ or $a^2b^2$, and I would like to find that. If you need to, add in the ability to generate new residues for arbitrary given primes instead of working from a fixed input collection of pairs.  These rational numbers I'm trying to solve for are solutions to some large linear equations, and these equations are convenient to solve over ""small"" finite fields ($p \le 2^{32}$), but inconvenient to solve over infinite-precision rationals.",,"['linear-algebra', 'diophantine-equations', 'finite-fields', 'chinese-remainder-theorem']"
14,Which metrics on exterior power are induced from metrics on the base?,Which metrics on exterior power are induced from metrics on the base?,,"$\newcommand{\id}{\text{id}}$ $\newcommand{\Hom}{\text{Hom}}$ Let $V$ be a $d$-dimensional real vector space, and let $2 \le k \le d-1$. Every inner product on $V$ induces an inner product on $\Lambda^k V$, in the following way $$ \langle v_1 \wedge \dots \wedge v_k , w_1 \wedge \dots \wedge w_k \rangle:=\det (\langle v_i ,w_j \rangle). \tag{1}$$ Question: What are necessary and sufficient conditions on an inner product on $\Lambda^k V$ to to be induced from a product on $V$ as in $(1)$? (I restricted $k \neq 1,d$ because then of course, any metric is induced by a metric on the base). I think there should be some ""compatibility"" or ""symmetry"" conditions, but I am not sure how to formulate them. Edit: If there exist an inducing product at the base, this product is unique. Perhaps we can construct an ""inverse map"" which is defined on all products on $\Lambda^k V$, and see when the result is an honest inner product on $V$. Proof of uniqueness: Suppose $g_1,g_2$ are inner products on $V$, which induce the same product on $\Lambda^k V$. Let $A:(V,g_1) \to (V,g_2)$ be an isometry. Then the induced exterior map $$\bigwedge ^k A:(\Lambda^k V,\Lambda^k g_1) \to (\Lambda^k V,\Lambda^k g_2)$$ is an isometry, where $\Lambda^k g_i$ is the metric on $ \Lambda^k V$ induced by $g_i$ via $(1)$. Since by assumption $\Lambda^k g_1=\Lambda^k g_2$, we get that $$\bigwedge ^k A:(\Lambda^k V,\Lambda^k g_1) \to (\Lambda^k V,\Lambda^k g_1)$$ is an isometry, i.e. $$ \id_{\Lambda_k(V)}=(\bigwedge ^k A)^T \circ \bigwedge ^k A= \bigwedge ^k A^T \circ \bigwedge ^k A=\bigwedge ^k A^T A,$$ where the transpose in $A^T$ is taken with respect to the metric $g_1$. Denote $S=A^TA$. Then $S \in \Hom(V,V)$ is symmetric (w.r.t. $g_1$) and positive-definite , and satisfies $  \id_{\Lambda_k(V)}  =\bigwedge^k S $. This implies $S=\id_V$. (For a short proof, see this answer ). So, we have obtained $A^TA=\id_V$; Remembering the transpose was taken w.r.t $g_1$, this shows $A$ is an automorphism of $(V,g_1)$. Recalling that we assumed $A:(V,g_1) \to (V,g_2)$ was an isometry, this shows $g_1=g_2$.","$\newcommand{\id}{\text{id}}$ $\newcommand{\Hom}{\text{Hom}}$ Let $V$ be a $d$-dimensional real vector space, and let $2 \le k \le d-1$. Every inner product on $V$ induces an inner product on $\Lambda^k V$, in the following way $$ \langle v_1 \wedge \dots \wedge v_k , w_1 \wedge \dots \wedge w_k \rangle:=\det (\langle v_i ,w_j \rangle). \tag{1}$$ Question: What are necessary and sufficient conditions on an inner product on $\Lambda^k V$ to to be induced from a product on $V$ as in $(1)$? (I restricted $k \neq 1,d$ because then of course, any metric is induced by a metric on the base). I think there should be some ""compatibility"" or ""symmetry"" conditions, but I am not sure how to formulate them. Edit: If there exist an inducing product at the base, this product is unique. Perhaps we can construct an ""inverse map"" which is defined on all products on $\Lambda^k V$, and see when the result is an honest inner product on $V$. Proof of uniqueness: Suppose $g_1,g_2$ are inner products on $V$, which induce the same product on $\Lambda^k V$. Let $A:(V,g_1) \to (V,g_2)$ be an isometry. Then the induced exterior map $$\bigwedge ^k A:(\Lambda^k V,\Lambda^k g_1) \to (\Lambda^k V,\Lambda^k g_2)$$ is an isometry, where $\Lambda^k g_i$ is the metric on $ \Lambda^k V$ induced by $g_i$ via $(1)$. Since by assumption $\Lambda^k g_1=\Lambda^k g_2$, we get that $$\bigwedge ^k A:(\Lambda^k V,\Lambda^k g_1) \to (\Lambda^k V,\Lambda^k g_1)$$ is an isometry, i.e. $$ \id_{\Lambda_k(V)}=(\bigwedge ^k A)^T \circ \bigwedge ^k A= \bigwedge ^k A^T \circ \bigwedge ^k A=\bigwedge ^k A^T A,$$ where the transpose in $A^T$ is taken with respect to the metric $g_1$. Denote $S=A^TA$. Then $S \in \Hom(V,V)$ is symmetric (w.r.t. $g_1$) and positive-definite , and satisfies $  \id_{\Lambda_k(V)}  =\bigwedge^k S $. This implies $S=\id_V$. (For a short proof, see this answer ). So, we have obtained $A^TA=\id_V$; Remembering the transpose was taken w.r.t $g_1$, this shows $A$ is an automorphism of $(V,g_1)$. Recalling that we assumed $A:(V,g_1) \to (V,g_2)$ was an isometry, this shows $g_1=g_2$.",,"['linear-algebra', 'differential-geometry', 'exterior-algebra']"
15,Sum of entries of a matrix,Sum of entries of a matrix,,"For a matrix $A \in \mathbb{R}^{n \times n}$, it is clear that the sum of all the entries of $A$ can be expressed as $$\vec{1}^{T} A \vec{1} = \sum \limits_{i,j} A_{i,j}$$ Now suppose $A,B \in \mathbb{R}^{n \times n}$ are symmetric matrices. Then by the above expression, it is clear that the sum of the entries of the product $AB$ is the same as that of $BA$, even though the two are distinct as matrices. So  $$\vec{1}^{T} (AB+BA) \vec{1}=2\vec{1}^{T} AB \vec{1}$$ Do we have any such expression for higher degrees? That is, suppose we form the sum of all possible permutations of a product of $n$ repetitions of $A$ and $m$ repetitions of $B$, and let $Symm(A^nB^m)$ denote this sum. For example, when $n=3$ and $m=2$, the expression has $\binom{5}{2}$ terms as follows $$Symm(A^3B^2)=A^3B^2 + A^2BAB + A^2B^2A+ABA^2B+ABABA+AB^2A^2+BA^3B+BA^2BA +BABA^2+B^2A^3$$ Can we say anything useful about $$\vec{1}^{T} Symm(A^nB^m) \vec{1}$$ in terms of $A,B$? This came up while working on a larger problem, so I've skipped the context here as of now. I apologize if the question is a bit vague and open-ended, and will update it promptly based on any feedback. Thanks.","For a matrix $A \in \mathbb{R}^{n \times n}$, it is clear that the sum of all the entries of $A$ can be expressed as $$\vec{1}^{T} A \vec{1} = \sum \limits_{i,j} A_{i,j}$$ Now suppose $A,B \in \mathbb{R}^{n \times n}$ are symmetric matrices. Then by the above expression, it is clear that the sum of the entries of the product $AB$ is the same as that of $BA$, even though the two are distinct as matrices. So  $$\vec{1}^{T} (AB+BA) \vec{1}=2\vec{1}^{T} AB \vec{1}$$ Do we have any such expression for higher degrees? That is, suppose we form the sum of all possible permutations of a product of $n$ repetitions of $A$ and $m$ repetitions of $B$, and let $Symm(A^nB^m)$ denote this sum. For example, when $n=3$ and $m=2$, the expression has $\binom{5}{2}$ terms as follows $$Symm(A^3B^2)=A^3B^2 + A^2BAB + A^2B^2A+ABA^2B+ABABA+AB^2A^2+BA^3B+BA^2BA +BABA^2+B^2A^3$$ Can we say anything useful about $$\vec{1}^{T} Symm(A^nB^m) \vec{1}$$ in terms of $A,B$? This came up while working on a larger problem, so I've skipped the context here as of now. I apologize if the question is a bit vague and open-ended, and will update it promptly based on any feedback. Thanks.",,"['linear-algebra', 'matrices', 'noncommutative-algebra']"
16,Intuition of Wronskian determinant and linear independence,Intuition of Wronskian determinant and linear independence,,"I am wondering the intuition in regard to the following; (let $w$ represent the wronskian function). Please correct me If I am mistaken, but I will write what I do know and what I am confused about. Suppose we consider a set of $n$ differentiable functions, say,  $\{f_1,…f_n\}$ on some open interval $I=(\alpha,\beta)$ Then why is it that if $$w(f_1,…,f_n)(x) \neq 0$$ for $\mathbf{some}$ $x \in I$, then they are linearly independent on the interval. But, if $$w(f_1,…,f_n)(x)=0$$ for even one $x \in I$, then they are linearly dependent in I? Is this because if it is zero for one x that we can find, this implies that it will be zero for all $x \in I$? how can we conclude this? in regard to its relationship with differential equations, ( which is why I am currently learning this), I understand that we would require $w \neq 0$ to have a unique solution, and I also understand why have a row that is a multiple of another would gives $w=0$ from simple determinant rules. But I am having trouble tying it all together. Thank you","I am wondering the intuition in regard to the following; (let $w$ represent the wronskian function). Please correct me If I am mistaken, but I will write what I do know and what I am confused about. Suppose we consider a set of $n$ differentiable functions, say,  $\{f_1,…f_n\}$ on some open interval $I=(\alpha,\beta)$ Then why is it that if $$w(f_1,…,f_n)(x) \neq 0$$ for $\mathbf{some}$ $x \in I$, then they are linearly independent on the interval. But, if $$w(f_1,…,f_n)(x)=0$$ for even one $x \in I$, then they are linearly dependent in I? Is this because if it is zero for one x that we can find, this implies that it will be zero for all $x \in I$? how can we conclude this? in regard to its relationship with differential equations, ( which is why I am currently learning this), I understand that we would require $w \neq 0$ to have a unique solution, and I also understand why have a row that is a multiple of another would gives $w=0$ from simple determinant rules. But I am having trouble tying it all together. Thank you",,"['linear-algebra', 'determinant']"
17,Eigenvalues of symmetric matrix with skew-symmetric matrix perturbation,Eigenvalues of symmetric matrix with skew-symmetric matrix perturbation,,"If $A$ is diagonalizable, using the Bauer-Fike theorem , for any eigenvalue $λ$ of $A$ , there exists an eigenvalue $μ$ of $A+E$ such that $|\lambda-\mu|\leq\|E\|_2$ (the vector induced norm). Here I have the doubt that if we have more properties on $A$ and $E$ : $A$ is symmetric and $E$ is skew-symmetric, The eigenvalues of $A$ are completely separated from each other, i.e.: there is a certain distance between each two eigenvalues so that with sufficiently small $\|E\|_2$ , the perturbed eigenvalues are still different from each other. Do we have $|\lambda-\mu|\leq c\|E\|_2^2$ for some constant $c$ ? Is there any strict proof or a counterexample? I have only a rough idea that it may be true, because if we denote $\lambda(\epsilon)$ as the eigenvalue of $A+\epsilon E$ , then: $$ \lambda(-\epsilon)=\lambda(A-\epsilon E)=\lambda(A^T-\epsilon E^T)=\lambda(A+\epsilon E)=\lambda(\epsilon) $$ This means the function $\lambda(\epsilon)$ is an even function, so the Taylor expansion may be $$ \lambda(\epsilon)=\lambda(0)+\lambda''(0)\epsilon^2/2+O(\epsilon^4) $$ Or equivalently, $$ \lambda(A+\epsilon E)-\lambda(A)=\lambda''(0)\epsilon^2/2+O(\epsilon^4) $$","If is diagonalizable, using the Bauer-Fike theorem , for any eigenvalue of , there exists an eigenvalue of such that (the vector induced norm). Here I have the doubt that if we have more properties on and : is symmetric and is skew-symmetric, The eigenvalues of are completely separated from each other, i.e.: there is a certain distance between each two eigenvalues so that with sufficiently small , the perturbed eigenvalues are still different from each other. Do we have for some constant ? Is there any strict proof or a counterexample? I have only a rough idea that it may be true, because if we denote as the eigenvalue of , then: This means the function is an even function, so the Taylor expansion may be Or equivalently,",A λ A μ A+E |\lambda-\mu|\leq\|E\|_2 A E A E A \|E\|_2 |\lambda-\mu|\leq c\|E\|_2^2 c \lambda(\epsilon) A+\epsilon E  \lambda(-\epsilon)=\lambda(A-\epsilon E)=\lambda(A^T-\epsilon E^T)=\lambda(A+\epsilon E)=\lambda(\epsilon)  \lambda(\epsilon)  \lambda(\epsilon)=\lambda(0)+\lambda''(0)\epsilon^2/2+O(\epsilon^4)   \lambda(A+\epsilon E)-\lambda(A)=\lambda''(0)\epsilon^2/2+O(\epsilon^4) ,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'perturbation-theory']"
18,Prove a generator of $\mathrm{SL}_2(\mathbb{R})$,Prove a generator of,\mathrm{SL}_2(\mathbb{R}),"Definitions Let $\mathrm{SL}_2(\mathbb{R}) := \left \{M := \begin{pmatrix} a & b\\ c & d\end{pmatrix} \in \mathbb{R}^{2 \times 2}: \det{M} = 1 \right \}$ with matrix multiplication be a group of $2 \times 2$ matrices. Let \begin{align}   A_\lambda :&= \begin{pmatrix} \lambda & 0\\ 0 & \lambda^{-1}\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R}) \text{ with }\lambda \in \mathbb{R} \setminus \{0\}\\   B_t :&= \begin{pmatrix} 1 & t\\ 0 & 1\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R}) \text{ with } t \in \mathbb{R}\\   C :&= \begin{pmatrix} 0 & 1\\ -1 & 0\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R}) \end{align} be elements of $\mathrm{SL}_2(\mathbb{R})$. Question Do $A_\lambda, B_t$ and $C$ generate $\mathrm{SL}_2(\mathbb{R})$? My try I think they do. When I could show that $$\begin{pmatrix} a & b\\ c & d\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R})$$ can be multiplied with a combination of the three matrices $A, B, C$ in such a way that the result is the neutral element of this group $$\begin{pmatrix} 1 & 0\\ 0 & 1\end{pmatrix}$$ then I could generate the matrix by multiplying the inverse matrices.  The inverse matrices can be generated because: \begin{align}   A_\lambda^{-1} &= A_{\frac{1}{\lambda}}\\   B_t^{-1}       &= B_{-t}\\   C^{-1}         &= C^3 \end{align} Now the only thing left is to get from  $$M = \begin{pmatrix} a & b\\ c & d\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R})$$ to $$\begin{pmatrix} 1 & 0\\ 0 & 1\end{pmatrix}$$ Ok. So let's try it. Case 1: $a = 0$ As $\det(M)=1 = ad - bc = 0d-bc$ we know that $bc=-1$. Especially is $c \neq 0$. $$\begin{pmatrix} 0 & 1\\ -1 & 0\end{pmatrix} \cdot \begin{pmatrix} a & b\\ c & d\end{pmatrix} = \begin{pmatrix} c & d\\ -a & -b\end{pmatrix}$$ Continue with Case 2. Case 2: $a \neq 0$ Normalize ($M \cdot A_{\frac{1}{a}}$): $$\begin{pmatrix} a & b\\ c & d\end{pmatrix} \cdot \begin{pmatrix} \frac{1}{a} & 0\\ 0 & a\end{pmatrix} = \begin{pmatrix} 1 & ab\\ \frac{c}{a} & ad\end{pmatrix}$$ Continue with Case 3. Case 3: $a=1$ Get a $0$ with $M \cdot B_{-b}$: $$\begin{pmatrix} 1 & b\\ c & d\end{pmatrix} \cdot \begin{pmatrix} 1 & -b\\ 0 & 1\end{pmatrix} = \begin{pmatrix} 1 & 0\\ c & d-bc\end{pmatrix}$$ We know that $\det{M} = 1 = ad - bc = d - bc$. Continue with Case 4. Case 4: $a=1$, $b=0, d=1$ At this stage we have matrices that look like $$\begin{pmatrix} 1 & 0\\ c & 1\end{pmatrix}$$ This is where I'm stuck. Can you help me?","Definitions Let $\mathrm{SL}_2(\mathbb{R}) := \left \{M := \begin{pmatrix} a & b\\ c & d\end{pmatrix} \in \mathbb{R}^{2 \times 2}: \det{M} = 1 \right \}$ with matrix multiplication be a group of $2 \times 2$ matrices. Let \begin{align}   A_\lambda :&= \begin{pmatrix} \lambda & 0\\ 0 & \lambda^{-1}\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R}) \text{ with }\lambda \in \mathbb{R} \setminus \{0\}\\   B_t :&= \begin{pmatrix} 1 & t\\ 0 & 1\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R}) \text{ with } t \in \mathbb{R}\\   C :&= \begin{pmatrix} 0 & 1\\ -1 & 0\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R}) \end{align} be elements of $\mathrm{SL}_2(\mathbb{R})$. Question Do $A_\lambda, B_t$ and $C$ generate $\mathrm{SL}_2(\mathbb{R})$? My try I think they do. When I could show that $$\begin{pmatrix} a & b\\ c & d\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R})$$ can be multiplied with a combination of the three matrices $A, B, C$ in such a way that the result is the neutral element of this group $$\begin{pmatrix} 1 & 0\\ 0 & 1\end{pmatrix}$$ then I could generate the matrix by multiplying the inverse matrices.  The inverse matrices can be generated because: \begin{align}   A_\lambda^{-1} &= A_{\frac{1}{\lambda}}\\   B_t^{-1}       &= B_{-t}\\   C^{-1}         &= C^3 \end{align} Now the only thing left is to get from  $$M = \begin{pmatrix} a & b\\ c & d\end{pmatrix} \in \mathrm{SL}_2(\mathbb{R})$$ to $$\begin{pmatrix} 1 & 0\\ 0 & 1\end{pmatrix}$$ Ok. So let's try it. Case 1: $a = 0$ As $\det(M)=1 = ad - bc = 0d-bc$ we know that $bc=-1$. Especially is $c \neq 0$. $$\begin{pmatrix} 0 & 1\\ -1 & 0\end{pmatrix} \cdot \begin{pmatrix} a & b\\ c & d\end{pmatrix} = \begin{pmatrix} c & d\\ -a & -b\end{pmatrix}$$ Continue with Case 2. Case 2: $a \neq 0$ Normalize ($M \cdot A_{\frac{1}{a}}$): $$\begin{pmatrix} a & b\\ c & d\end{pmatrix} \cdot \begin{pmatrix} \frac{1}{a} & 0\\ 0 & a\end{pmatrix} = \begin{pmatrix} 1 & ab\\ \frac{c}{a} & ad\end{pmatrix}$$ Continue with Case 3. Case 3: $a=1$ Get a $0$ with $M \cdot B_{-b}$: $$\begin{pmatrix} 1 & b\\ c & d\end{pmatrix} \cdot \begin{pmatrix} 1 & -b\\ 0 & 1\end{pmatrix} = \begin{pmatrix} 1 & 0\\ c & d-bc\end{pmatrix}$$ We know that $\det{M} = 1 = ad - bc = d - bc$. Continue with Case 4. Case 4: $a=1$, $b=0, d=1$ At this stage we have matrices that look like $$\begin{pmatrix} 1 & 0\\ c & 1\end{pmatrix}$$ This is where I'm stuck. Can you help me?",,"['linear-algebra', 'matrices']"
19,Validity of my weird proof that $AB$ and $BA$ have the same eigenvalues?,Validity of my weird proof that  and  have the same eigenvalues?,AB BA,"On a recent linear algebra exam, I was required to prove that ""for every $n \times k$ matrix $A$ and $k \times n$ matrix $B$ over the same field, it holds that $AB$ and $BA$ have the same eigenvalues except for 0."" This is a classic result and I really should have known better, but for some reason the only proof I could come up with is the following. It's kind of uncanny (the examiner rejected it), but I think it might be valid anyway. Am I correct? Proof: Let $M_{AB}(t)$ and $M_{BA}(t)$ be the minimal polynomials of $AB$ and $BA$ respectively. It holds that: $M_{AB}(AB)=0$. Therefore: $B \cdot M_{AB}(AB)=0$ $M_{AB}(BA) \cdot B=0$ $M_{AB}(BA) \cdot BA=0$ So we see that $BA$ is a zero of the polynomial $M_{AB}(t) \cdot t=0$. Therefore, $M_{BA}(t)\: |\: M_{AB}(t)\,t$, which means that $M_{BA}(t)$ and $M_{AB}(t)$ share the same irreducible factors except possibly for $t$, and the claim follows. $\square$ Please note: I know that this result has been addressed many times on this site, but I didn't find my proof anywhere, so I think this isn't a duplicate.","On a recent linear algebra exam, I was required to prove that ""for every $n \times k$ matrix $A$ and $k \times n$ matrix $B$ over the same field, it holds that $AB$ and $BA$ have the same eigenvalues except for 0."" This is a classic result and I really should have known better, but for some reason the only proof I could come up with is the following. It's kind of uncanny (the examiner rejected it), but I think it might be valid anyway. Am I correct? Proof: Let $M_{AB}(t)$ and $M_{BA}(t)$ be the minimal polynomials of $AB$ and $BA$ respectively. It holds that: $M_{AB}(AB)=0$. Therefore: $B \cdot M_{AB}(AB)=0$ $M_{AB}(BA) \cdot B=0$ $M_{AB}(BA) \cdot BA=0$ So we see that $BA$ is a zero of the polynomial $M_{AB}(t) \cdot t=0$. Therefore, $M_{BA}(t)\: |\: M_{AB}(t)\,t$, which means that $M_{BA}(t)$ and $M_{AB}(t)$ share the same irreducible factors except possibly for $t$, and the claim follows. $\square$ Please note: I know that this result has been addressed many times on this site, but I didn't find my proof anywhere, so I think this isn't a duplicate.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'proof-verification']"
20,Existence of a certain subspace in a vector space,Existence of a certain subspace in a vector space,,"Let $V$ be a vector space, and $W_1, W_2, \ldots, W_m \subset V$ its $k$-dimensional subspaces. Every their pairwise intersection is $k-1$-dimensional (for all $i \neq j$, $\dim (W_i \cap W_j) = k - 1$). Show that there is either: a $k-1$-dimensional subspace $U \subset W_i$ in all $W_i$, or a $k+1$-dimensional subspace $Z \supset W_i$ containing all $W_i$. My try: First, notice that $\dim W_i < \dim V$, since otherwise $W_i = V$. Thus there is always room for a $Z$. $m=1$ is trivial. For $m=2$ there is definitely a set $U = W_1 \cap W_2$, $\dim U = k - 1$. There is also a set $Z = W_1 + W_2$, since we can consider the basis $u_1, u_2, \ldots, u_{k-1}$ in $U$ add to it two vectors (one to complement the basis in $W_1$ and the other for $W_2$) and obtain a basis of $Z$. So for $m=2$ there are both $U$ and $Z$. For $m=3$ things get interesting. We have k-dimensional $W_1, W_2, W_3$, and their pairwise intersections are all $k-1$-dimensional. $U_1 = W_2 \cap W_3$, $\dim U_1 = k - 1$ $U_2 = W_3 \cap W_1$, $\dim U_2 = k - 1$ $U_3 = W_1 \cap W_2$, $\dim U_3 = k - 1$ $U = W_1 \cap W_2 \cap W_3$, $\dim U = k - 1 - x$ The same can be said about the sums instead of intersections: $Z_1 = W_2 + W_3$, $\dim Z_1 = k + 1$ $Z_2 = W_3 + W_1$, $\dim Z_2 = k + 1$ $Z_3 = W_1 + W_2$, $\dim Z_3 = k + 1$ $Z = W_1 + W_2 + W_3$, $\dim Z = k + 1 + y$ The problem is, essentially, to show that either $x=0$ or $y=0$. It is probably also true that $x \leq 1$ and $y \leq 1$, but I have no idea how to prove that either. So, that's where I got stuck. The picture I have in mind is the sequence of increasing subspaces $(U, U_i, W_i, Z_i, Z)$ with expanding basises, but their basises expand in some complicated manner I do not fully comprehend. I suppose that the method for solving the case $m=3$ could be applied to all other $m$s, probably resulting in a proof by induction. Any help would be appreciated.","Let $V$ be a vector space, and $W_1, W_2, \ldots, W_m \subset V$ its $k$-dimensional subspaces. Every their pairwise intersection is $k-1$-dimensional (for all $i \neq j$, $\dim (W_i \cap W_j) = k - 1$). Show that there is either: a $k-1$-dimensional subspace $U \subset W_i$ in all $W_i$, or a $k+1$-dimensional subspace $Z \supset W_i$ containing all $W_i$. My try: First, notice that $\dim W_i < \dim V$, since otherwise $W_i = V$. Thus there is always room for a $Z$. $m=1$ is trivial. For $m=2$ there is definitely a set $U = W_1 \cap W_2$, $\dim U = k - 1$. There is also a set $Z = W_1 + W_2$, since we can consider the basis $u_1, u_2, \ldots, u_{k-1}$ in $U$ add to it two vectors (one to complement the basis in $W_1$ and the other for $W_2$) and obtain a basis of $Z$. So for $m=2$ there are both $U$ and $Z$. For $m=3$ things get interesting. We have k-dimensional $W_1, W_2, W_3$, and their pairwise intersections are all $k-1$-dimensional. $U_1 = W_2 \cap W_3$, $\dim U_1 = k - 1$ $U_2 = W_3 \cap W_1$, $\dim U_2 = k - 1$ $U_3 = W_1 \cap W_2$, $\dim U_3 = k - 1$ $U = W_1 \cap W_2 \cap W_3$, $\dim U = k - 1 - x$ The same can be said about the sums instead of intersections: $Z_1 = W_2 + W_3$, $\dim Z_1 = k + 1$ $Z_2 = W_3 + W_1$, $\dim Z_2 = k + 1$ $Z_3 = W_1 + W_2$, $\dim Z_3 = k + 1$ $Z = W_1 + W_2 + W_3$, $\dim Z = k + 1 + y$ The problem is, essentially, to show that either $x=0$ or $y=0$. It is probably also true that $x \leq 1$ and $y \leq 1$, but I have no idea how to prove that either. So, that's where I got stuck. The picture I have in mind is the sequence of increasing subspaces $(U, U_i, W_i, Z_i, Z)$ with expanding basises, but their basises expand in some complicated manner I do not fully comprehend. I suppose that the method for solving the case $m=3$ could be applied to all other $m$s, probably resulting in a proof by induction. Any help would be appreciated.",,"['linear-algebra', 'vector-spaces']"
21,Proving that two vector spaces of equal dimension are isomorphic,Proving that two vector spaces of equal dimension are isomorphic,,"Given the following problem Let $U$ and $V$ be finite dimensional vector spaces, such that $\dim(U) = \dim(V)$. Prove that $U$ and $V$ are isomorphic. I was just wondering if somebody could critique and validate my proof. Proof Two vector spaces $U$ and $V$ are isomorphic if and only if there exists a bijection $\phi: U \to V$. That is, an invertible linear map between the two. Let $\dim(U) = \dim(V) = n$ and let $\left\{u_1, \dots, u_n\right\}$ and $\left\{v_1, \dots, v_n\right\}$ be bases for $U$ and $V$, respectively. It follows that any vector $u \in U$ can be written as $$ u = c_1u_1 + \cdots + c_nu_n $$ We define $\phi: U \to V$ as $$ \phi(c_1u_1 + \cdots + c_nu_n) = c_1v_1 + \cdots + c_nv_n $$ Observe that $\phi$ is invertible. That is, given a vector $$ v = d_1v_1 + \cdots + d_nv_n $$ in $V$, we define $\phi^{-1}: V \to U$ as $$ \phi^{-1}(d_1v_1 + \cdots + d_nv_n) = d_1u_1 + \cdots + d_nu_n $$ Since there exists a bijection between $U$ and $V$, they are isomorphic.","Given the following problem Let $U$ and $V$ be finite dimensional vector spaces, such that $\dim(U) = \dim(V)$. Prove that $U$ and $V$ are isomorphic. I was just wondering if somebody could critique and validate my proof. Proof Two vector spaces $U$ and $V$ are isomorphic if and only if there exists a bijection $\phi: U \to V$. That is, an invertible linear map between the two. Let $\dim(U) = \dim(V) = n$ and let $\left\{u_1, \dots, u_n\right\}$ and $\left\{v_1, \dots, v_n\right\}$ be bases for $U$ and $V$, respectively. It follows that any vector $u \in U$ can be written as $$ u = c_1u_1 + \cdots + c_nu_n $$ We define $\phi: U \to V$ as $$ \phi(c_1u_1 + \cdots + c_nu_n) = c_1v_1 + \cdots + c_nv_n $$ Observe that $\phi$ is invertible. That is, given a vector $$ v = d_1v_1 + \cdots + d_nv_n $$ in $V$, we define $\phi^{-1}: V \to U$ as $$ \phi^{-1}(d_1v_1 + \cdots + d_nv_n) = d_1u_1 + \cdots + d_nu_n $$ Since there exists a bijection between $U$ and $V$, they are isomorphic.",,"['linear-algebra', 'proof-verification', 'vector-space-isomorphism']"
22,From matrices to bipartite graphs,From matrices to bipartite graphs,,"Assume $G(A,B)$ is a bipartite graph and assume $L(G)$ is the adjacency matrix of its line graph. define $$B=[3\text{I}+L(G)]^{-1}$$. Is it always the case that for each edge $e=(a,b)\in G$, we have: $$B_{e,e}>\sum_{e'\in G, e'=(a,b'),e'\neq e}B_{e,e'}$$ (I have asked the same question in Linear Algebra form here: Simple to state yet tricky question","Assume $G(A,B)$ is a bipartite graph and assume $L(G)$ is the adjacency matrix of its line graph. define $$B=[3\text{I}+L(G)]^{-1}$$. Is it always the case that for each edge $e=(a,b)\in G$, we have: $$B_{e,e}>\sum_{e'\in G, e'=(a,b'),e'\neq e}B_{e,e'}$$ (I have asked the same question in Linear Algebra form here: Simple to state yet tricky question",,"['linear-algebra', 'matrices', 'graph-theory', 'inverse']"
23,Does this cross-product norm inequality hold?,Does this cross-product norm inequality hold?,,"Let $\times$ denote the cross-product. $\;$ Is it the case that For all unit vectors $\:\mathbf{x}\hspace{.01 in},\hspace{-0.03 in}\mathbf{y}\hspace{-0.03 in},\hspace{-0.02 in}\mathbf{z}\:$ in $\mathbf{R}^3$, $\;\;\;\; \left|\left|\hspace{.03 in}\mathbf{x} \hspace{-0.03 in}\times \hspace{-0.03 in}\mathbf{z}\hspace{.02 in}\right|\right| \:\: \leq \:\: \left|\left|\hspace{.03 in}\mathbf{x} \hspace{-0.03 in}\times \hspace{-0.03 in}\mathbf{y}\hspace{.02 in}\right|\right| \hspace{.02 in}+\hspace{.02 in}\left|\left|\hspace{.03 in}\mathbf{y} \hspace{-0.03 in}\times \hspace{-0.03 in}\mathbf{z}\hspace{.02 in}\right|\right| \;\;\;\;\;$. ? (If yes, then $\;\;\; \langle [\mathbf{x}]\hspace{.01 in},\hspace{-0.03 in}[\hspace{.02 in}\mathbf{y}] \rangle \: \mapsto \: \left|\left|\hspace{.03 in}\mathbf{x} \hspace{-0.03 in}\times \hspace{-0.03 in}\mathbf{y}\hspace{.02 in}\right|\right| \;\;\;$ defines a nice metric on the projective plane.)","Let $\times$ denote the cross-product. $\;$ Is it the case that For all unit vectors $\:\mathbf{x}\hspace{.01 in},\hspace{-0.03 in}\mathbf{y}\hspace{-0.03 in},\hspace{-0.02 in}\mathbf{z}\:$ in $\mathbf{R}^3$, $\;\;\;\; \left|\left|\hspace{.03 in}\mathbf{x} \hspace{-0.03 in}\times \hspace{-0.03 in}\mathbf{z}\hspace{.02 in}\right|\right| \:\: \leq \:\: \left|\left|\hspace{.03 in}\mathbf{x} \hspace{-0.03 in}\times \hspace{-0.03 in}\mathbf{y}\hspace{.02 in}\right|\right| \hspace{.02 in}+\hspace{.02 in}\left|\left|\hspace{.03 in}\mathbf{y} \hspace{-0.03 in}\times \hspace{-0.03 in}\mathbf{z}\hspace{.02 in}\right|\right| \;\;\;\;\;$. ? (If yes, then $\;\;\; \langle [\mathbf{x}]\hspace{.01 in},\hspace{-0.03 in}[\hspace{.02 in}\mathbf{y}] \rangle \: \mapsto \: \left|\left|\hspace{.03 in}\mathbf{x} \hspace{-0.03 in}\times \hspace{-0.03 in}\mathbf{y}\hspace{.02 in}\right|\right| \;\;\;$ defines a nice metric on the projective plane.)",,"['linear-algebra', 'inequality']"
24,Simple matrix help,Simple matrix help,,"Let 'M' be an $(a-1)\times(a-1)$ coeifficent matrix, $\large\ m_{r,c}=\left\{\Large\frac{r\cdot c}{a}\right\}$ Where $\{ x \}$ is the fractional part of $x$. And let vectors 'G' and 'H' be column vectors, $\vec{G}=\begin{bmatrix} j_1 \\ j_2 \\ j_3 \\: \\ : \\ j_{a-1}\end{bmatrix}$,  $  \vec{H}=\begin{bmatrix} y_1 \\ y_2 \\y_3 \\ : \\ : \\ y_{a-1} \end{bmatrix}$, Does M$\vec{G}=\vec{H}$, always have a unique solution for any integer a>3, if so prove it please","Let 'M' be an $(a-1)\times(a-1)$ coeifficent matrix, $\large\ m_{r,c}=\left\{\Large\frac{r\cdot c}{a}\right\}$ Where $\{ x \}$ is the fractional part of $x$. And let vectors 'G' and 'H' be column vectors, $\vec{G}=\begin{bmatrix} j_1 \\ j_2 \\ j_3 \\: \\ : \\ j_{a-1}\end{bmatrix}$,  $  \vec{H}=\begin{bmatrix} y_1 \\ y_2 \\y_3 \\ : \\ : \\ y_{a-1} \end{bmatrix}$, Does M$\vec{G}=\vec{H}$, always have a unique solution for any integer a>3, if so prove it please",,"['linear-algebra', 'number-theory', 'elementary-number-theory']"
25,Can you find a minimal polynomial of $A^n$ if you know the minimal polynomial of $A$?,Can you find a minimal polynomial of  if you know the minimal polynomial of ?,A^n A,"Can you find a minimal polynomial of $A^n$ if you know the minimal polynomial of $A$? I'm talking about minimal polynomials of matrices. I'm asking in the general sort of way, I know that in some cases you can use algebraic tricks or some other cleverness. $n$ is just some integer, it's not connected to the matrix in any way.","Can you find a minimal polynomial of $A^n$ if you know the minimal polynomial of $A$? I'm talking about minimal polynomials of matrices. I'm asking in the general sort of way, I know that in some cases you can use algebraic tricks or some other cleverness. $n$ is just some integer, it's not connected to the matrix in any way.",,"['linear-algebra', 'matrices']"
26,Reconciling 'intersecting planes' and 'linear transformation' interpretations of matrices,Reconciling 'intersecting planes' and 'linear transformation' interpretations of matrices,,"I've learned in linear algebra class that an $n \times m$ augmented matrix can be thought of as a collection of n planes in $\mathbb {R}^m$ . If the matrix is invertible, the planes all intersect at a single point. If it has infinite solutions, two or more planes coincide and so their intersection is a line rather than a point. If the matrix is inconsistent then there is no one single point of intersection. When you do Gauss-Jordan elimination, you are adding scalar multiples of the planes to each other, which has the effect of rotating them over the line where they intersect. When they attain RREF, they are at right angles to each other in the dimensions corresponding to columns that contain leading ones. But then we go on to another interpretation of matrices as linear transformations for altering the magnitude and direction of vectors. I would like to have a geometric interpretation of matrix multiplication that is compatible with the intersecting planes interpretation. Since matrix multiplication is built from row by column dot-products, I guess the first step would be to visualize those. First question: Am I correct in interpreting the dot-product of two vectors as the cosine of the angle between them scaled to the magnitude of both vectors, with a value somewhere between the length of the shadow vector A casts on vector B and the length of vector B. Is there anything more specific I should associate with this quantity? Second question: Now let's say we multiply a $3 \times 3$ matrix by a vector $\in \mathbb R^3$ and get a different vector $\in \mathbb R^3$. How do I interpret it graphically relative to the three intersecting planes and one line through the origin that gave rise to it? Third question: Can anybody recommend some software I can use for visualizing matrix operations, especially on Linux? At the moment I'm using wxMaxima with the draw package, but it's really awkward to use because draw cannot take matrices as arguments and I don't see any way of updating and existing plot with new information. Thank you all kindly.","I've learned in linear algebra class that an $n \times m$ augmented matrix can be thought of as a collection of n planes in $\mathbb {R}^m$ . If the matrix is invertible, the planes all intersect at a single point. If it has infinite solutions, two or more planes coincide and so their intersection is a line rather than a point. If the matrix is inconsistent then there is no one single point of intersection. When you do Gauss-Jordan elimination, you are adding scalar multiples of the planes to each other, which has the effect of rotating them over the line where they intersect. When they attain RREF, they are at right angles to each other in the dimensions corresponding to columns that contain leading ones. But then we go on to another interpretation of matrices as linear transformations for altering the magnitude and direction of vectors. I would like to have a geometric interpretation of matrix multiplication that is compatible with the intersecting planes interpretation. Since matrix multiplication is built from row by column dot-products, I guess the first step would be to visualize those. First question: Am I correct in interpreting the dot-product of two vectors as the cosine of the angle between them scaled to the magnitude of both vectors, with a value somewhere between the length of the shadow vector A casts on vector B and the length of vector B. Is there anything more specific I should associate with this quantity? Second question: Now let's say we multiply a $3 \times 3$ matrix by a vector $\in \mathbb R^3$ and get a different vector $\in \mathbb R^3$. How do I interpret it graphically relative to the three intersecting planes and one line through the origin that gave rise to it? Third question: Can anybody recommend some software I can use for visualizing matrix operations, especially on Linux? At the moment I'm using wxMaxima with the draw package, but it's really awkward to use because draw cannot take matrices as arguments and I don't see any way of updating and existing plot with new information. Thank you all kindly.",,"['linear-algebra', 'geometry', 'matrices', 'vector-spaces', 'visualization']"
27,Linear Algebra exercises available online?,Linear Algebra exercises available online?,,Is there a good source of undergraduates linear algebra problems available on line? Do you have any better idea of how to resharpen my linear algebra skills?,Is there a good source of undergraduates linear algebra problems available on line? Do you have any better idea of how to resharpen my linear algebra skills?,,"['linear-algebra', 'online-resources']"
28,Updating eigen-decomposition of symmetric matrix $A$ to eigendecomposition of $A+D$ where $D$ is low-rank diagonal,Updating eigen-decomposition of symmetric matrix  to eigendecomposition of  where  is low-rank diagonal,A A+D D,"Given a symmetric positive definite matrix $A$ and a mostly-zeros non-negative diagonal matrix $D$, is there a way to cheaply update the eigenvalues and/or eigenvectors of $A$ to that of $A+D$? Ideally I'm looking for something akin to the Woodbury matrix identity.","Given a symmetric positive definite matrix $A$ and a mostly-zeros non-negative diagonal matrix $D$, is there a way to cheaply update the eigenvalues and/or eigenvectors of $A$ to that of $A+D$? Ideally I'm looking for something akin to the Woodbury matrix identity.",,['linear-algebra']
29,How many connected components could the intersection of $\{A \in M_n(\mathbb R): \rho(A) < 1\}$ and an affine subspace in $M_n(\mathbb R)$ have?,How many connected components could the intersection of  and an affine subspace in  have?,\{A \in M_n(\mathbb R): \rho(A) < 1\} M_n(\mathbb R),"Let $\mathcal E = \{A \in M_n(\mathbb R): \rho(A) < 1\}$ where $\rho(\cdot)$ is the spectral radius and $\mathcal U$ be an affine space in $M_n(\mathbb R)$ . If we assume $\mathcal E \cap \mathcal U \neq \emptyset$ , how many connected components could the intersection have? In proving $\mathcal E$ is connected, I know we can use a path $(1-t)A + t 0$ but if $B$ is in the intersection, $(1-t)B$ could not be guaranteed in the intersection.","Let where is the spectral radius and be an affine space in . If we assume , how many connected components could the intersection have? In proving is connected, I know we can use a path but if is in the intersection, could not be guaranteed in the intersection.",\mathcal E = \{A \in M_n(\mathbb R): \rho(A) < 1\} \rho(\cdot) \mathcal U M_n(\mathbb R) \mathcal E \cap \mathcal U \neq \emptyset \mathcal E (1-t)A + t 0 B (1-t)B,"['linear-algebra', 'general-topology', 'functional-analysis', 'operator-theory', 'path-connected']"
30,Is there a homeomorphism between the sets of Schur stable and Hurwitz stable matrices in companion forms?,Is there a homeomorphism between the sets of Schur stable and Hurwitz stable matrices in companion forms?,,"The set of Schur stable matrices is \begin{align*} \mathcal S = \{A \in M_n(\mathbb R): \rho(A) < 1\}, \end{align*} where $\rho(\cdot)$ denotes the spectral radius of a matrix and the set of Hurwitz stable matrices is \begin{align*} \mathcal H = \{A \in M_n(\mathbb R): \max_{i=1, \dots, n} \text{Re}(\lambda_i(A)) < 0\}, \end{align*} i.e., matrices with eigenvalues lying on the open left half plane. Let $\mathcal C$ denote all matrices in companion form . Let $\hat{\mathcal S} = \mathcal S \cap \mathcal C$ and $\hat{\mathcal H} = \mathcal H \cap \mathcal C$ . Now I would like to determine whether there is a homeomorphism between the sets $\hat {\mathcal S}$ and $\hat{\mathcal H}$ . Let us exclude the trivial case $n=1$ . If we consider the sets $\mathcal S$ and $\mathcal H$ only, there is a diffeomorphism $f: \mathcal S \to \mathcal H$ given by \begin{align*} A \mapsto (A-I)^{-1}(A+I). \end{align*} But apparently this doesn't work for $\hat {\mathcal S}$ and $\hat{\mathcal H}$ since the inversion and multiplication will not necessarily yield a matrix in companion form.","The set of Schur stable matrices is where denotes the spectral radius of a matrix and the set of Hurwitz stable matrices is i.e., matrices with eigenvalues lying on the open left half plane. Let denote all matrices in companion form . Let and . Now I would like to determine whether there is a homeomorphism between the sets and . Let us exclude the trivial case . If we consider the sets and only, there is a diffeomorphism given by But apparently this doesn't work for and since the inversion and multiplication will not necessarily yield a matrix in companion form.","\begin{align*}
\mathcal S = \{A \in M_n(\mathbb R): \rho(A) < 1\},
\end{align*} \rho(\cdot) \begin{align*}
\mathcal H = \{A \in M_n(\mathbb R): \max_{i=1, \dots, n} \text{Re}(\lambda_i(A)) < 0\},
\end{align*} \mathcal C \hat{\mathcal S} = \mathcal S \cap \mathcal C \hat{\mathcal H} = \mathcal H \cap \mathcal C \hat {\mathcal S} \hat{\mathcal H} n=1 \mathcal S \mathcal H f: \mathcal S \to \mathcal H \begin{align*}
A \mapsto (A-I)^{-1}(A+I).
\end{align*} \hat {\mathcal S} \hat{\mathcal H}","['linear-algebra', 'general-topology', 'matrices', 'hurwitz-matrices']"
31,When can two matrices have zero diagonal in the same basis?,When can two matrices have zero diagonal in the same basis?,,"It's common to ask if two Hermitian matrices $A$ and $B$ can be diagonalized in the same basis. Is there an efficient way to check if they can be made hollow in the same basis? By hollow, I mean that the diagonal elements are all $0$. As esoteric as this sounds, I've had it come up in an experimentally relevant physics problem. Potentially useful aspects I have understood so far (I am only concerned with Hermitian matrices): $A$ can be transformed into a hollow matrix iff $\operatorname{Tr}[A]=0$ (see this post ) To find a basis in which $A$ is hollow, find a non-degenerate, full rank Hermitian matrix $Q$ that satisfies $\operatorname{Tr}[A Q^N] = 0$ for $N=0,\ldots,\dim(A)-1$. $A$ is hollow in the eigenbasis of $Q$. In principle, the second fact could be used to solve the problem, however I would like a method to check if a solution exists that is simpler than finding the solution, analogous to how $AB-BA = 0$ tests if $A$ and $B$ can be simultaneously diagonalized.","It's common to ask if two Hermitian matrices $A$ and $B$ can be diagonalized in the same basis. Is there an efficient way to check if they can be made hollow in the same basis? By hollow, I mean that the diagonal elements are all $0$. As esoteric as this sounds, I've had it come up in an experimentally relevant physics problem. Potentially useful aspects I have understood so far (I am only concerned with Hermitian matrices): $A$ can be transformed into a hollow matrix iff $\operatorname{Tr}[A]=0$ (see this post ) To find a basis in which $A$ is hollow, find a non-degenerate, full rank Hermitian matrix $Q$ that satisfies $\operatorname{Tr}[A Q^N] = 0$ for $N=0,\ldots,\dim(A)-1$. $A$ is hollow in the eigenbasis of $Q$. In principle, the second fact could be used to solve the problem, however I would like a method to check if a solution exists that is simpler than finding the solution, analogous to how $AB-BA = 0$ tests if $A$ and $B$ can be simultaneously diagonalized.",,"['linear-algebra', 'matrices', 'diagonalization']"
32,Upper Bound of Eigenvalues of Symmetric Real Matrix $A$,Upper Bound of Eigenvalues of Symmetric Real Matrix,A,"Let $A_n=\begin{pmatrix} 1&\frac{1}{2}&\frac{1}{3}&\cdots&\frac{1}{n}\\ \frac{1}{2}&\frac{1}{2}&\frac{1}{3}&\cdots&\frac{1}{n}\\ \frac{1}{3}&\frac{1}{3}&\frac{1}{3}&\cdots&\frac{1}{n}\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ \frac{1}{n}&\frac{1}{n}&\frac{1}{n}&\cdots&\frac{1}{n} \end{pmatrix}$. For each $n\in\mathbb{N}$, let $\lambda_0$ be one eigenvalue of $A_n$ ($\lambda_0$ is arbitrarily chosen) Prove that $0<\lambda_0<3+2\sqrt{2}$. A problem in a previous final exam. The number $3+2\sqrt{2}$ seems strange enough... If $\alpha=(x_1, x_2,\cdots,x_n)^\mathrm{T}$ is an eigenvector of $A_n$ belonging to $\lambda_0$, we get $n$ equations: $$x_1+\frac{x_2}{2}+\cdots+\frac{x_n}{n}=\lambda_0x_1$$ $$\frac{x_1}{2}+\frac{x_2}{2}+\cdots+\frac{x_n}{n}=\lambda_0x_2$$ $$\vdots$$ $$\frac{x_1}{n}+\frac{x_2}{n}+\cdots+\frac{x_n}{n}=\lambda_0x_n$$ I tried to use inequalities to get a proof, but it doesn't work...There should be better approaches. Any advice or help appreciated.","Let $A_n=\begin{pmatrix} 1&\frac{1}{2}&\frac{1}{3}&\cdots&\frac{1}{n}\\ \frac{1}{2}&\frac{1}{2}&\frac{1}{3}&\cdots&\frac{1}{n}\\ \frac{1}{3}&\frac{1}{3}&\frac{1}{3}&\cdots&\frac{1}{n}\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ \frac{1}{n}&\frac{1}{n}&\frac{1}{n}&\cdots&\frac{1}{n} \end{pmatrix}$. For each $n\in\mathbb{N}$, let $\lambda_0$ be one eigenvalue of $A_n$ ($\lambda_0$ is arbitrarily chosen) Prove that $0<\lambda_0<3+2\sqrt{2}$. A problem in a previous final exam. The number $3+2\sqrt{2}$ seems strange enough... If $\alpha=(x_1, x_2,\cdots,x_n)^\mathrm{T}$ is an eigenvector of $A_n$ belonging to $\lambda_0$, we get $n$ equations: $$x_1+\frac{x_2}{2}+\cdots+\frac{x_n}{n}=\lambda_0x_1$$ $$\frac{x_1}{2}+\frac{x_2}{2}+\cdots+\frac{x_n}{n}=\lambda_0x_2$$ $$\vdots$$ $$\frac{x_1}{n}+\frac{x_2}{n}+\cdots+\frac{x_n}{n}=\lambda_0x_n$$ I tried to use inequalities to get a proof, but it doesn't work...There should be better approaches. Any advice or help appreciated.",,"['linear-algebra', 'matrices']"
33,Gram-Schmidt process in Minkowski space $\Bbb L^n$.,Gram-Schmidt process in Minkowski space .,\Bbb L^n,"I'm trying to prove a version of Gram-Schmidt orthogonalization process in Minkowski space $\Bbb L^n$ (for concreteness, I'll put the sign last). I am not interested in the existence of orthonormal bases, but instead in the algorithm. Namely, suppose that $\{v_1,\cdots,v_k\}\subseteq\Bbb L^n$ is a linearly independent set which does not contain any lightlike vectors, and whose span is non-degenerate. I'd try to mimic the usual proof by induction. If $k=1$, take $u_1 = v_1$, done. And if I assume $\{u_1,\cdots,u_k\}$ constructed, I'd define $$u_{k+1} = v_{k+1} - \sum_{j=1}^n\frac{\langle v_{k+1},u_j\rangle}{\langle u_j,u_j\rangle}u_j = v_{k+1} - \sum_{j=1}^n\epsilon_j\frac{\langle v_{k+1},u_j\rangle}{\|u_j\|^2}u_j,$$which is orthogonal to the previous $u_i$'s. But: One of the $u_i$'s could be lightlike and the construction would stop there. I'm not using (as far as I can see) non-degenerability of the span of the initial vectors. Also, I tried applying the GS process to the plane $y=z$ in $\Bbb L^3$, starting with a basis with no lightlike vectors... it produced a freaking lightlike vector (and gave me an orthogonal basis, hooray!). I mean... it's no surprise a lightlike vector came up, assuming the GS process works here... but why should it? I'm terribly lost. Can someone help me state the result correctly and maybe give me a little push on the proof? Thanks.","I'm trying to prove a version of Gram-Schmidt orthogonalization process in Minkowski space $\Bbb L^n$ (for concreteness, I'll put the sign last). I am not interested in the existence of orthonormal bases, but instead in the algorithm. Namely, suppose that $\{v_1,\cdots,v_k\}\subseteq\Bbb L^n$ is a linearly independent set which does not contain any lightlike vectors, and whose span is non-degenerate. I'd try to mimic the usual proof by induction. If $k=1$, take $u_1 = v_1$, done. And if I assume $\{u_1,\cdots,u_k\}$ constructed, I'd define $$u_{k+1} = v_{k+1} - \sum_{j=1}^n\frac{\langle v_{k+1},u_j\rangle}{\langle u_j,u_j\rangle}u_j = v_{k+1} - \sum_{j=1}^n\epsilon_j\frac{\langle v_{k+1},u_j\rangle}{\|u_j\|^2}u_j,$$which is orthogonal to the previous $u_i$'s. But: One of the $u_i$'s could be lightlike and the construction would stop there. I'm not using (as far as I can see) non-degenerability of the span of the initial vectors. Also, I tried applying the GS process to the plane $y=z$ in $\Bbb L^3$, starting with a basis with no lightlike vectors... it produced a freaking lightlike vector (and gave me an orthogonal basis, hooray!). I mean... it's no surprise a lightlike vector came up, assuming the GS process works here... but why should it? I'm terribly lost. Can someone help me state the result correctly and maybe give me a little push on the proof? Thanks.",,"['linear-algebra', 'orthogonality', 'bilinear-form', 'semi-riemannian-geometry', 'gram-schmidt']"
34,"Invariant vectors of $A^n B^m$ with $A,B$ orthogonal matrices",Invariant vectors of  with  orthogonal matrices,"A^n B^m A,B","Let $A$ be the following matrix:$$A=\dfrac{1}{2}\ \left( \begin{array}{cccccccccc}  -1 & -1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\  -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\  0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 \\  0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\  0 & 0 & 0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 \\  0 & 0 & 0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 \\  0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 & 1 & -1 \\  0 & 0 & 0 & 0 & 0 & 0 & -1 & 1 & 1 & -1 \\ \end{array} \right)$$ and $B=A+C$ with $C$ a matrix full of zeros, except the first $(2,2)$ block equal to $\begin{bmatrix} 1&1 \\ -1&-1\end{bmatrix}$: $$B=\dfrac{1}{2}\ \left( \begin{array}{cccccccccc}  \mathbf{1} &  \mathbf{1} & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\   \mathbf{-1} &  \mathbf{-1} & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\  -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\  0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 \\  0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\  0 & 0 & 0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 \\  0 & 0 & 0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 \\  0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 & 1 & -1 \\  0 & 0 & 0 & 0 & 0 & 0 & -1 & 1 & 1 & -1 \\ \end{array} \right)$$ Both $A,B$ are both orthogonal matrices with elements in $\{-\frac{1}{2},0,\frac{1}{2}\}$, hence each row and each column has exactly 4 non-zeros entries (the sum of the square of the entries of each row or column is 1). The characteristic polynomial of $A$ is $1+x^N$ and that of $B$ is $-1+x^N$ (here, $N=10$). $A,B,C$ do not commute. $A,B$ are isometries, so is $A^n B^m$. I am trying to characterize $\ker(A^n B^m -I)$ for $(n,m)\in\mathbb{N}^2$. From their characteristic polynomial, $B^N=I$ and $A^{2N}=I$ so it suffices to consider $n\leq 2N$ and $m\leq N$. From experimental computations, it seems that ""something happens"": $A,B$ are made of $(2,2)$ blocks which are multiplied (there seems to be a structure of monoid) and translates to neighbour blocks. The question is how to characterize the kernel of  $A^nB^m-I$ (at least it's dimension) as a function of $n$ and $m$. This could be based on a basis in which $A$ and $B$ have a simple expression, but I did not succeed in identifying such a basis, despite the fact that $C=B-A$ is full of zeros (except 4 entries). Edit Based on Armadillo Jim's answer, the question can also be written as finding the maximal invariant subspace of  $A^n (DA)^m$ ($D$ is a block diagonal matrix of blocks $\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$ and $I_{N-2}$).","Let $A$ be the following matrix:$$A=\dfrac{1}{2}\ \left( \begin{array}{cccccccccc}  -1 & -1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\  -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\  0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 \\  0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\  0 & 0 & 0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 \\  0 & 0 & 0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 \\  0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 & 1 & -1 \\  0 & 0 & 0 & 0 & 0 & 0 & -1 & 1 & 1 & -1 \\ \end{array} \right)$$ and $B=A+C$ with $C$ a matrix full of zeros, except the first $(2,2)$ block equal to $\begin{bmatrix} 1&1 \\ -1&-1\end{bmatrix}$: $$B=\dfrac{1}{2}\ \left( \begin{array}{cccccccccc}  \mathbf{1} &  \mathbf{1} & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\   \mathbf{-1} &  \mathbf{-1} & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\  -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\  0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 \\  0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\  0 & 0 & 0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 \\  0 & 0 & 0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 \\  0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 & 1 & -1 \\  0 & 0 & 0 & 0 & 0 & 0 & -1 & 1 & 1 & -1 \\ \end{array} \right)$$ Both $A,B$ are both orthogonal matrices with elements in $\{-\frac{1}{2},0,\frac{1}{2}\}$, hence each row and each column has exactly 4 non-zeros entries (the sum of the square of the entries of each row or column is 1). The characteristic polynomial of $A$ is $1+x^N$ and that of $B$ is $-1+x^N$ (here, $N=10$). $A,B,C$ do not commute. $A,B$ are isometries, so is $A^n B^m$. I am trying to characterize $\ker(A^n B^m -I)$ for $(n,m)\in\mathbb{N}^2$. From their characteristic polynomial, $B^N=I$ and $A^{2N}=I$ so it suffices to consider $n\leq 2N$ and $m\leq N$. From experimental computations, it seems that ""something happens"": $A,B$ are made of $(2,2)$ blocks which are multiplied (there seems to be a structure of monoid) and translates to neighbour blocks. The question is how to characterize the kernel of  $A^nB^m-I$ (at least it's dimension) as a function of $n$ and $m$. This could be based on a basis in which $A$ and $B$ have a simple expression, but I did not succeed in identifying such a basis, despite the fact that $C=B-A$ is full of zeros (except 4 entries). Edit Based on Armadillo Jim's answer, the question can also be written as finding the maximal invariant subspace of  $A^n (DA)^m$ ($D$ is a block diagonal matrix of blocks $\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$ and $I_{N-2}$).",,"['linear-algebra', 'matrices', 'vector-spaces', 'monoid']"
35,Eigenvalues of a quasi-circulant matrix,Eigenvalues of a quasi-circulant matrix,,"The following matrix cropped up in a model I am building of a dynamical system: $$A= \begin{bmatrix} 1 - \alpha & \alpha/2 & 0 & 0 &\cdots & 0 & 0 & \alpha/2\\ \alpha/2 & 1-\alpha & \alpha/2 & 0 &\cdots & 0 & 0 & 0\\ 0 & \alpha/2 & 1-\alpha & \alpha/2 &\cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\ 0 & 0 & 0 & 0 &\cdots & \alpha/2 & 1-\alpha & \alpha/2\\ \alpha/2 & 0 & 0 & 0 &\cdots & 0 & \alpha/2 & 1-\alpha\\ \end{bmatrix}$$ It is a stochastic matrix and a circulant matrix , and has equal values in the diagonal. I am interested in the eigenvalues of this matrix, and it was easy to derive them from the properties listed here . It turns out that for size $n$, $$ \lambda_k = 1 - \alpha \left(1 - \cos\frac{\pi k (n-2)}{n}\right), \qquad k\in\{0,1,\dots,n-1\}, $$ and in the limiting case, $$ \lim_{n\rightarrow\infty} \lambda_k = 1 - \alpha(1 + (-1)^k) = \begin{cases}1-2\alpha & k \textrm{ even}\\ 1 & k \textrm{ odd}\end{cases} $$ This is interesting for my study, because an eigenvalue of $1$ that is independent of $\alpha$ implies a marginally stable system that cannot be fully stabilized. Now, I am interested in a slightly modified system, represented by the matrix below. This matrix is exactly like the one above save for the first and last rows, and is still a stochastic matrix with equal values in the diagonal. I am wondering whether it is possible to derive the eigenvalues of this matrix, even if only for the limiting case. $$A^\prime= \begin{bmatrix} 1 - \alpha & \color{red} \alpha & 0 & 0 &\cdots & 0 & 0 & \color{red} 0\\ \alpha/2 & 1-\alpha & \alpha/2 & 0 &\cdots & 0 & 0 & 0\\ 0 & \alpha/2 & 1-\alpha & \alpha/2 &\cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\ 0 & 0 & 0 & 0 &\cdots & \alpha/2 & 1-\alpha & \alpha/2\\ \color{red} 0 & 0 & 0 & 0 &\cdots & 0 & \color{red} \alpha & 1-\alpha\\ \end{bmatrix}$$","The following matrix cropped up in a model I am building of a dynamical system: $$A= \begin{bmatrix} 1 - \alpha & \alpha/2 & 0 & 0 &\cdots & 0 & 0 & \alpha/2\\ \alpha/2 & 1-\alpha & \alpha/2 & 0 &\cdots & 0 & 0 & 0\\ 0 & \alpha/2 & 1-\alpha & \alpha/2 &\cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\ 0 & 0 & 0 & 0 &\cdots & \alpha/2 & 1-\alpha & \alpha/2\\ \alpha/2 & 0 & 0 & 0 &\cdots & 0 & \alpha/2 & 1-\alpha\\ \end{bmatrix}$$ It is a stochastic matrix and a circulant matrix , and has equal values in the diagonal. I am interested in the eigenvalues of this matrix, and it was easy to derive them from the properties listed here . It turns out that for size $n$, $$ \lambda_k = 1 - \alpha \left(1 - \cos\frac{\pi k (n-2)}{n}\right), \qquad k\in\{0,1,\dots,n-1\}, $$ and in the limiting case, $$ \lim_{n\rightarrow\infty} \lambda_k = 1 - \alpha(1 + (-1)^k) = \begin{cases}1-2\alpha & k \textrm{ even}\\ 1 & k \textrm{ odd}\end{cases} $$ This is interesting for my study, because an eigenvalue of $1$ that is independent of $\alpha$ implies a marginally stable system that cannot be fully stabilized. Now, I am interested in a slightly modified system, represented by the matrix below. This matrix is exactly like the one above save for the first and last rows, and is still a stochastic matrix with equal values in the diagonal. I am wondering whether it is possible to derive the eigenvalues of this matrix, even if only for the limiting case. $$A^\prime= \begin{bmatrix} 1 - \alpha & \color{red} \alpha & 0 & 0 &\cdots & 0 & 0 & \color{red} 0\\ \alpha/2 & 1-\alpha & \alpha/2 & 0 &\cdots & 0 & 0 & 0\\ 0 & \alpha/2 & 1-\alpha & \alpha/2 &\cdots & 0 & 0 & 0\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\ 0 & 0 & 0 & 0 &\cdots & \alpha/2 & 1-\alpha & \alpha/2\\ \color{red} 0 & 0 & 0 & 0 &\cdots & 0 & \color{red} \alpha & 1-\alpha\\ \end{bmatrix}$$",,"['linear-algebra', 'abstract-algebra', 'limits', 'eigenvalues-eigenvectors', 'dynamical-systems']"
36,Find the matrices $A\in M_{n}(\textbf{Z}/n\textbf{Z})$ such that $A^3=I_{n}$,Find the matrices  such that,A\in M_{n}(\textbf{Z}/n\textbf{Z}) A^3=I_{n},"If $n\in\textbf{N}$, what are the matrices $A\in M_{n}(\textbf{Z}/n\textbf{Z})$ such that $A^3=I_{n}$ ? An exercise I found and have no clue how to do. Even if I assume $n$ is prime and we are talking about matrices whose coefficients belong to a field, I don't really see how to proceed. Help? edit: both the $n$ in the size of the matrices and the $n$ in $\textbf{Z}/n\textbf{Z}$ are the same by the way","If $n\in\textbf{N}$, what are the matrices $A\in M_{n}(\textbf{Z}/n\textbf{Z})$ such that $A^3=I_{n}$ ? An exercise I found and have no clue how to do. Even if I assume $n$ is prime and we are talking about matrices whose coefficients belong to a field, I don't really see how to proceed. Help? edit: both the $n$ in the size of the matrices and the $n$ in $\textbf{Z}/n\textbf{Z}$ are the same by the way",,"['linear-algebra', 'matrices', 'matrix-equations']"
37,Here is a riddle that I have no idea how to solve.,Here is a riddle that I have no idea how to solve.,,"Okay, so I was trying to solve this riddle found here . It is a diagram of a star with 16 points. Each point corresponds uniquely to a number between 1 and 16. The letters on each point represent a letter of some saying, where if the unique corresponding to a point is $n$, then the letter on that point is the $n$th letter of the saying. We are also given the condition that the sum of the 4 numbers on a given line segment is the same. I noticed that $\sum^{16}_{i=1} i = 136$, and each point is counted exactly twice, so each line should add up to $2\cdot136/8 = 34$. So we can find 8 equations in this way. $$x_1 + x_2 + x_3 + x_4 = 34 $$ $$x_4 + x_5 + x_6 + x_7 = 34 $$ $$x_2 + x_7 + x_8 + x_9 = 34 $$ $$x_5 + x_9 + x_{10} + x_{11} = 34 $$ $$x_8 + x_{11} + x_{12} + x_{13} = 34 $$ $$x_{10} + x_{13} + x_{14} + x_{15} = 34 $$ $$x_3 + x_{12} + x_{15} + x_{16} = 34 $$ $$x_1 + x_6 + x_{14} + x_{16} = 34 $$ So, I have 8 equations and 16 unknowns, and finding the coefficient matrix and putting it in rref didn't shed that much light on the matter, because there are still too many unknowns. Now, I know there will be more than one solution. We can rotate it 7 times and there are 8 lines of symmetry; however, I think we should be able to narrow down our solutions farther than what I have already done. Does anyone have other ideas on how to approach this problem?","Okay, so I was trying to solve this riddle found here . It is a diagram of a star with 16 points. Each point corresponds uniquely to a number between 1 and 16. The letters on each point represent a letter of some saying, where if the unique corresponding to a point is $n$, then the letter on that point is the $n$th letter of the saying. We are also given the condition that the sum of the 4 numbers on a given line segment is the same. I noticed that $\sum^{16}_{i=1} i = 136$, and each point is counted exactly twice, so each line should add up to $2\cdot136/8 = 34$. So we can find 8 equations in this way. $$x_1 + x_2 + x_3 + x_4 = 34 $$ $$x_4 + x_5 + x_6 + x_7 = 34 $$ $$x_2 + x_7 + x_8 + x_9 = 34 $$ $$x_5 + x_9 + x_{10} + x_{11} = 34 $$ $$x_8 + x_{11} + x_{12} + x_{13} = 34 $$ $$x_{10} + x_{13} + x_{14} + x_{15} = 34 $$ $$x_3 + x_{12} + x_{15} + x_{16} = 34 $$ $$x_1 + x_6 + x_{14} + x_{16} = 34 $$ So, I have 8 equations and 16 unknowns, and finding the coefficient matrix and putting it in rref didn't shed that much light on the matter, because there are still too many unknowns. Now, I know there will be more than one solution. We can rotate it 7 times and there are 8 lines of symmetry; however, I think we should be able to narrow down our solutions farther than what I have already done. Does anyone have other ideas on how to approach this problem?",,"['linear-algebra', 'recreational-mathematics', 'puzzle']"
38,A very difficult problem about the existence of following $SU(2)$ matrices?,A very difficult problem about the existence of following  matrices?,SU(2),"Let $G_i$ be a sequence of $SU(2)$ matrices, where $i=1,2,...,n$; and $P$ represents a permutation of $\left \{ 1,2,...,n \right \}$. The question is: Does there exist a sequence of $SU(2)$ matrices $W_i$ such that $$W_i^\dagger G_iW_{P(i)}=\varepsilon_ig,$$ where the sign $\varepsilon_i=1$ or $-1$ depending on $i$ while $g\in SU(2)$ does not depend on $i$. This question is based on some physics problems and I believe the existence of $W_i$, but it seems very difficult to prove it. Thank you very much.","Let $G_i$ be a sequence of $SU(2)$ matrices, where $i=1,2,...,n$; and $P$ represents a permutation of $\left \{ 1,2,...,n \right \}$. The question is: Does there exist a sequence of $SU(2)$ matrices $W_i$ such that $$W_i^\dagger G_iW_{P(i)}=\varepsilon_ig,$$ where the sign $\varepsilon_i=1$ or $-1$ depending on $i$ while $g\in SU(2)$ does not depend on $i$. This question is based on some physics problems and I believe the existence of $W_i$, but it seems very difficult to prove it. Thank you very much.",,"['linear-algebra', 'matrices', 'lie-groups', 'matrix-equations', 'matrix-decomposition']"
39,How to find LU factors of a matrix when diagonals are changed,How to find LU factors of a matrix when diagonals are changed,,"Say I have $A=LU$ already factored into lower and upper triangular matrices $L$ and $U$. Now I want to work on the eigenvalue problem $A-\lambda I=A'=L'U'$ where prime indicates new matrices. Given $A$, $L$, $U$, and $\lambda$, is there a way to get $L'$ and $U'$ without refactoring $A'$ from the beginning? Thanks!","Say I have $A=LU$ already factored into lower and upper triangular matrices $L$ and $U$. Now I want to work on the eigenvalue problem $A-\lambda I=A'=L'U'$ where prime indicates new matrices. Given $A$, $L$, $U$, and $\lambda$, is there a way to get $L'$ and $U'$ without refactoring $A'$ from the beginning? Thanks!",,"['linear-algebra', 'matrices']"
40,"Strength of ""Every finite dimensional subspace of a vector space has a complement""","Strength of ""Every finite dimensional subspace of a vector space has a complement""",,"Does the following choice principle have a name? Every finite dimensional subspace of a vector space has a complement. Equivalently, every line inside a vector space has a complementary hyperplane. How strong/weak is it compared to other choice principles? It certainly follows from the existence of a basis, and as such is a consequence of the axiom of choice. Feel free to edit the Tags, I wasn't sure which tag is appropriate.","Does the following choice principle have a name? Every finite dimensional subspace of a vector space has a complement. Equivalently, every line inside a vector space has a complementary hyperplane. How strong/weak is it compared to other choice principles? It certainly follows from the existence of a basis, and as such is a consequence of the axiom of choice. Feel free to edit the Tags, I wasn't sure which tag is appropriate.",,"['linear-algebra', 'reference-request', 'vector-spaces', 'axiom-of-choice', 'axioms']"
41,Conceptual proof relating linear fractional transformations to matrices,Conceptual proof relating linear fractional transformations to matrices,,"Define a map from $2 \times 2$ invertible matrices to linear fractional transformations $$ f:\left( \begin{array}{ccc} a & b  \\ c & d \\\end{array} \right) \mapsto \frac{az + b}{cz + d}.$$  It is well known that $f(AB) = f(A) \circ f(B)$.  This is easy to prove:  just simplify both sides of the equation.  But is there a more conceptual proof that does not involve this computation?  Is there a generalization to $3 \times3$ matrices, etc.?","Define a map from $2 \times 2$ invertible matrices to linear fractional transformations $$ f:\left( \begin{array}{ccc} a & b  \\ c & d \\\end{array} \right) \mapsto \frac{az + b}{cz + d}.$$  It is well known that $f(AB) = f(A) \circ f(B)$.  This is easy to prove:  just simplify both sides of the equation.  But is there a more conceptual proof that does not involve this computation?  Is there a generalization to $3 \times3$ matrices, etc.?",,"['linear-algebra', 'complex-analysis', 'matrices']"
42,Isometries of $\mathbb{R}^2$,Isometries of,\mathbb{R}^2,"Show that if $A:\mathbb{R}^2\to \mathbb{R}^2$ is a proper rotation,   then it may be represented by a matrix of the form $$\pmatrix{  \cos(\theta)& -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \\}.$$ Further, any   improper rotation is given by $$\pmatrix{ 1 & 0 \\ 0 & -1 \\} \dot\  \pmatrix{ \cos(\theta)& -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \\}.$$   Conclude then that any isometry of $\mathbb{R}^2$ is a composition of   a translation, a proper rotation and possibly a reflection with   respect to the y -axis. I do not know how to do this problem. Any help with be greatly appreciated. Note: If $f:\mathbb{R}^n\to \mathbb{R}^n$ is an isometry, then $$f(p)=f(o)+A(p),$$ where $o$ is the origin of $\mathbb{R}^n$ and $A$ is an orthogonal transformation. So if $f:\mathbb{R}^n\to \mathbb{R}^n$ is an isometry with $f(o)=o$ we say that it is a rotation , and if $A=f-f(o)$ is identity we say that $f$ is a translation .","Show that if $A:\mathbb{R}^2\to \mathbb{R}^2$ is a proper rotation,   then it may be represented by a matrix of the form $$\pmatrix{  \cos(\theta)& -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \\}.$$ Further, any   improper rotation is given by $$\pmatrix{ 1 & 0 \\ 0 & -1 \\} \dot\  \pmatrix{ \cos(\theta)& -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \\}.$$   Conclude then that any isometry of $\mathbb{R}^2$ is a composition of   a translation, a proper rotation and possibly a reflection with   respect to the y -axis. I do not know how to do this problem. Any help with be greatly appreciated. Note: If $f:\mathbb{R}^n\to \mathbb{R}^n$ is an isometry, then $$f(p)=f(o)+A(p),$$ where $o$ is the origin of $\mathbb{R}^n$ and $A$ is an orthogonal transformation. So if $f:\mathbb{R}^n\to \mathbb{R}^n$ is an isometry with $f(o)=o$ we say that it is a rotation , and if $A=f-f(o)$ is identity we say that $f$ is a translation .",,['linear-algebra']
43,A conjecture about vector space,A conjecture about vector space,,"Let $V$ be a $(r+1)$-dimensional vector space, and $p$ be a positive integer and $1\leq p\leq r-1$. Let $$X=\{v_1,\cdots,v_{2r+1-p}\}\subseteq V$$ be a finite set containing $(2r+1-p)$ different vectors, and all these vectors are linearly-independent to each other $\{u,v\}$ is a linearly-independent set for any $u,v\in X$ such that $u\neq v$. Moreover, any (2s+2−p) vectors in X are not in some (s+1)-dimensional subspaces for any set of $(2s+2−p)$ vectors in $X$ there exists no $(s+1)$-dimensional subspace that contains said set, where $s=p,p+1,\cdots,r-1$. Prove or disprove the following conjecture :  $X$ can be divided into two non-intersecting non-empty subsets $$X=X_1\cup X_2$$ such that $X_1$ consists of $(r+1)$ linearly-independent vectors and $X_2$ consists of $(r-p)$ linearly-independent vectors. P.S. I am a college lecturer in Macau, and this conjecture is based on some discussions with my colleagues. We think this problem can be set for some math competitions for college students, however we have not reached conclusion regarding this conjecture. Therefore I post it out and invite your attention. My description of the conjecture using English may not look professional, and I welcome your editing to make it more sound. Thank you very much.","Let $V$ be a $(r+1)$-dimensional vector space, and $p$ be a positive integer and $1\leq p\leq r-1$. Let $$X=\{v_1,\cdots,v_{2r+1-p}\}\subseteq V$$ be a finite set containing $(2r+1-p)$ different vectors, and all these vectors are linearly-independent to each other $\{u,v\}$ is a linearly-independent set for any $u,v\in X$ such that $u\neq v$. Moreover, any (2s+2−p) vectors in X are not in some (s+1)-dimensional subspaces for any set of $(2s+2−p)$ vectors in $X$ there exists no $(s+1)$-dimensional subspace that contains said set, where $s=p,p+1,\cdots,r-1$. Prove or disprove the following conjecture :  $X$ can be divided into two non-intersecting non-empty subsets $$X=X_1\cup X_2$$ such that $X_1$ consists of $(r+1)$ linearly-independent vectors and $X_2$ consists of $(r-p)$ linearly-independent vectors. P.S. I am a college lecturer in Macau, and this conjecture is based on some discussions with my colleagues. We think this problem can be set for some math competitions for college students, however we have not reached conclusion regarding this conjecture. Therefore I post it out and invite your attention. My description of the conjecture using English may not look professional, and I welcome your editing to make it more sound. Thank you very much.",,"['linear-algebra', 'vector-spaces']"
44,"""Convex"" polynomials","""Convex"" polynomials",,"Let me define ""convex"" polynomials, as the smallest class $\mathcal{C}$ of functions $p:\mathbb{R}\rightarrow \mathbb{R}$ defined (inductively) as: UPDATED (case 0 was missing): 0) $p(x)=x$, i.e., the identity, is in $\mathcal{C}$. 1) $p(x)= 1$ is in $\mathcal{C}$ 2) $p(x)= 1- q(x)$, for $q\in \mathcal{C}$, is in $\mathcal{C}$. 3) $p(x) = c \cdot q(x) + (1-c) r(x)$, for some $c\in [0,1]$ and $q,r\in\mathcal{C}$, is in $\mathcal{C}$. 4) $p(x) = q(x) \cdot r(x)$, with $q,r\in\mathcal{C}$, is in $\mathcal{C}$. So basically $\mathcal{C}$ include all constant $c\in[0,1]$ functions, and is closed under the operations of convex combination, $1-$, and multiplication. Clearly for every $p\in \mathcal{C}$ and $x\in [0,1]$, $p(x)\in [0,1]$ too. Question: Is this class of functions known? Can anything interesting be said about this class? For example I think that every continuous function $f:[0,1]\rightarrow[0,1]$ can be approximated by functions in $\mathcal{C}$. Would this be some (trivial?) consequence of the Stone–Weierstrass theorem? Thanks!","Let me define ""convex"" polynomials, as the smallest class $\mathcal{C}$ of functions $p:\mathbb{R}\rightarrow \mathbb{R}$ defined (inductively) as: UPDATED (case 0 was missing): 0) $p(x)=x$, i.e., the identity, is in $\mathcal{C}$. 1) $p(x)= 1$ is in $\mathcal{C}$ 2) $p(x)= 1- q(x)$, for $q\in \mathcal{C}$, is in $\mathcal{C}$. 3) $p(x) = c \cdot q(x) + (1-c) r(x)$, for some $c\in [0,1]$ and $q,r\in\mathcal{C}$, is in $\mathcal{C}$. 4) $p(x) = q(x) \cdot r(x)$, with $q,r\in\mathcal{C}$, is in $\mathcal{C}$. So basically $\mathcal{C}$ include all constant $c\in[0,1]$ functions, and is closed under the operations of convex combination, $1-$, and multiplication. Clearly for every $p\in \mathcal{C}$ and $x\in [0,1]$, $p(x)\in [0,1]$ too. Question: Is this class of functions known? Can anything interesting be said about this class? For example I think that every continuous function $f:[0,1]\rightarrow[0,1]$ can be approximated by functions in $\mathcal{C}$. Would this be some (trivial?) consequence of the Stone–Weierstrass theorem? Thanks!",,"['linear-algebra', 'abstract-algebra', 'polynomials']"
45,Possible Jordan Canonical Forms Given Minimal Polynomial,Possible Jordan Canonical Forms Given Minimal Polynomial,,"I was supposed to find all possible Jordan canonical forms of a $5\times 5$ complex matrix with minimal polynomial $(x-2)^2(x-1)$ on a qualifying exam last semester.  I took the polynomial to mean that there were at least two 2's and one 1 on the main diagonal, and that the largest Jordan block with eigenvalue 2 is $2\times 2$ while the largest Jordan block with eigenvalue 1 is $1\times 1$.  Did I miss any matrices or interrupt the minimal polynomial incorrectly? \begin{pmatrix}   2 &1  &0  &0  &0\\   0 &2  &0  &0  &0\\   0 &0  &2  &1  &0\\   0 &0  &0  &2  &0\\   0 &0  &0  &0  &1 \end{pmatrix} \begin{pmatrix}   2 &1  &0  &0  &0\\   0 &2  &0  &0  &0\\   0 &0  &2  &0  &0\\   0 &0  &0  &2  &0\\   0 &0  &0  &0  &1 \end{pmatrix} \begin{pmatrix}   2 &1  &0  &0  &0\\   0 &2  &0  &0  &0\\   0 &0  &2  &0  &0\\   0 &0  &0  &1  &0\\   0 &0  &0  &0  &1 \end{pmatrix} \begin{pmatrix}   2 &1  &0  &0  &0\\   0 &2  &0  &0  &0\\   0 &0  &1  &0  &0\\   0 &0  &0  &1  &0\\   0 &0  &0  &0  &1 \end{pmatrix} \begin{pmatrix}   2 &0  &0  &0  &0\\   0 &2  &0  &0  &0\\   0 &0  &1  &0  &0\\   0 &0  &0  &1  &0\\   0 &0  &0  &0  &1 \end{pmatrix}","I was supposed to find all possible Jordan canonical forms of a $5\times 5$ complex matrix with minimal polynomial $(x-2)^2(x-1)$ on a qualifying exam last semester.  I took the polynomial to mean that there were at least two 2's and one 1 on the main diagonal, and that the largest Jordan block with eigenvalue 2 is $2\times 2$ while the largest Jordan block with eigenvalue 1 is $1\times 1$.  Did I miss any matrices or interrupt the minimal polynomial incorrectly? \begin{pmatrix}   2 &1  &0  &0  &0\\   0 &2  &0  &0  &0\\   0 &0  &2  &1  &0\\   0 &0  &0  &2  &0\\   0 &0  &0  &0  &1 \end{pmatrix} \begin{pmatrix}   2 &1  &0  &0  &0\\   0 &2  &0  &0  &0\\   0 &0  &2  &0  &0\\   0 &0  &0  &2  &0\\   0 &0  &0  &0  &1 \end{pmatrix} \begin{pmatrix}   2 &1  &0  &0  &0\\   0 &2  &0  &0  &0\\   0 &0  &2  &0  &0\\   0 &0  &0  &1  &0\\   0 &0  &0  &0  &1 \end{pmatrix} \begin{pmatrix}   2 &1  &0  &0  &0\\   0 &2  &0  &0  &0\\   0 &0  &1  &0  &0\\   0 &0  &0  &1  &0\\   0 &0  &0  &0  &1 \end{pmatrix} \begin{pmatrix}   2 &0  &0  &0  &0\\   0 &2  &0  &0  &0\\   0 &0  &1  &0  &0\\   0 &0  &0  &1  &0\\   0 &0  &0  &0  &1 \end{pmatrix}",,"['linear-algebra', 'abstract-algebra', 'matrices', 'jordan-normal-form']"
46,How exactly does Hahn-Banach theorem explain duality of vector spaces?,How exactly does Hahn-Banach theorem explain duality of vector spaces?,,"Serge Lang's Linear Algebra textbook just introduced me to the concept of dual space in very formal terms: space of all functional transformations having co-domain as $1$ -dimensional vector space over the field $\mathbb{K}$ (since in essence, field $\mathbb{K}$ is a vector space over itself). But the textbook did not explain the exact purpose of the term ""duality"", thus I decided to go little further and dive into some basic functional analysis. The Uncertainty Principle by Terrence Tao ( reference ): Terrence Tao wrote a really nice article on the concept of duality, that is explained in terms of local and global perspectives. In his first example: Vector space duality A vector space ${V}$ over a field ${F}$ can be described either by the set of vectors inside ${V}$ , or dually by the set of linear functionals ${\lambda: V \rightarrow F}$ from ${V}$ to the field ${F}$ (or equivalently, the set of vectors inside the dual space ${V^*}$ ). (If one is working in the category of topological vector spaces, one would work instead with continuous linear functionals; and so forth.) A fundamental connection between the two is given by the Hahn-Banach theorem (and its relatives). As you see in the last sentence (in italic font), Tao mentions that Hahn-Banach theorem displays the fundamental connection between some vector space $V$ and its dual $V^*$ . Therefore I've decided to investigate this concept a little further. Hahn-Banach Theorem and Dual space : There is a question regarding some similar connection on Math SE , but I'm not certain whether or not it is the answer to my question. From my understanding of answers below the referenced question, Hahn-Banach theorem states that for any arbitrary vector $v \in V$ , there exists a functional $L \in V^*$ such that $|L(v)|=||v||_{V}$ and $||L||_{V^*}=1$ . The definition of norm on the dual space is: $$||L||_{V^*}=\textrm{sup}\{|L(v)|: v \in V, |v| \leq 1 \}$$ where $\textrm{sup}$ denotes the supremum of set. I also know that every $L \in V^*$ is a linear transformation with norm $1$ that is bounded (i.e $\exists C \in \mathbb{K}, ||T(v)||_{V^*} < C||v||_{V}, \forall v \in V$ , where $C$ is called operator norm). This (along with definition of dual norm) shows another interesting relation: $$||v||_{V}=\textrm{sup}\{|L(v)|: v \in V, ||L||_{V^*}=1 \}$$ Riesz-representation theorem (Extension) : According to comments made by Berci below this post, complete inner product spaces (or Hilbert spaces) have special relationship with their dual spaces. Let $H$ be a Hilbert Space on the field $\mathbb{R}$ , this relationship can be seen by Riesz-representation theorem which asserts that $H$ and $H^*$ are isometrically isomorphic (whereas in complex field case, they are anti-isomorphic). In more specific details, it shows that there exists $g \in H$ such that for any functional $L \in H^*$ and any $x \in H$ : $L(x) = \langle{} f, g \rangle{}$ . Moreover, as a consequence to isometric connection: $||x||_{H} = ||L(x)||_{H^*}$ . This theorem establishes interesting connection between inner product and functionals. In fact, I believe it can be utilized as extension for Hahn-Banach theorem to see the deeper connection from geometric perspective, since the isomorphic isometric connection that Riesz Representation gives is equivalent of hyper-plane corresponding to its normal unit vector (and this seems to be a consequence of Hahn-Banach theorem, proving the existence of unit functionals). This can be more intuitively understood by specific cases of $L^P$ spaces, since they have interesting properties such as natural isomorphism of their duals, but I don't believe I have sufficient experience to group this information yet. Question : How exactly does the Hahn-Banach theorem show the fundamental connection between a vector space and its dual, as mentioned by Terrence Tao? Is it just that every vector $v$ has a corresponding functional which has the norm $||v||$ ? Is there more abstract explanation involving the idea of dual norm? Thank you!","Serge Lang's Linear Algebra textbook just introduced me to the concept of dual space in very formal terms: space of all functional transformations having co-domain as -dimensional vector space over the field (since in essence, field is a vector space over itself). But the textbook did not explain the exact purpose of the term ""duality"", thus I decided to go little further and dive into some basic functional analysis. The Uncertainty Principle by Terrence Tao ( reference ): Terrence Tao wrote a really nice article on the concept of duality, that is explained in terms of local and global perspectives. In his first example: Vector space duality A vector space over a field can be described either by the set of vectors inside , or dually by the set of linear functionals from to the field (or equivalently, the set of vectors inside the dual space ). (If one is working in the category of topological vector spaces, one would work instead with continuous linear functionals; and so forth.) A fundamental connection between the two is given by the Hahn-Banach theorem (and its relatives). As you see in the last sentence (in italic font), Tao mentions that Hahn-Banach theorem displays the fundamental connection between some vector space and its dual . Therefore I've decided to investigate this concept a little further. Hahn-Banach Theorem and Dual space : There is a question regarding some similar connection on Math SE , but I'm not certain whether or not it is the answer to my question. From my understanding of answers below the referenced question, Hahn-Banach theorem states that for any arbitrary vector , there exists a functional such that and . The definition of norm on the dual space is: where denotes the supremum of set. I also know that every is a linear transformation with norm that is bounded (i.e , where is called operator norm). This (along with definition of dual norm) shows another interesting relation: Riesz-representation theorem (Extension) : According to comments made by Berci below this post, complete inner product spaces (or Hilbert spaces) have special relationship with their dual spaces. Let be a Hilbert Space on the field , this relationship can be seen by Riesz-representation theorem which asserts that and are isometrically isomorphic (whereas in complex field case, they are anti-isomorphic). In more specific details, it shows that there exists such that for any functional and any : . Moreover, as a consequence to isometric connection: . This theorem establishes interesting connection between inner product and functionals. In fact, I believe it can be utilized as extension for Hahn-Banach theorem to see the deeper connection from geometric perspective, since the isomorphic isometric connection that Riesz Representation gives is equivalent of hyper-plane corresponding to its normal unit vector (and this seems to be a consequence of Hahn-Banach theorem, proving the existence of unit functionals). This can be more intuitively understood by specific cases of spaces, since they have interesting properties such as natural isomorphism of their duals, but I don't believe I have sufficient experience to group this information yet. Question : How exactly does the Hahn-Banach theorem show the fundamental connection between a vector space and its dual, as mentioned by Terrence Tao? Is it just that every vector has a corresponding functional which has the norm ? Is there more abstract explanation involving the idea of dual norm? Thank you!","1 \mathbb{K} \mathbb{K} {V} {F} {V} {\lambda: V \rightarrow F} {V} {F} {V^*} V V^* v \in V L \in V^* |L(v)|=||v||_{V} ||L||_{V^*}=1 ||L||_{V^*}=\textrm{sup}\{|L(v)|: v \in V, |v| \leq 1 \} \textrm{sup} L \in V^* 1 \exists C \in \mathbb{K}, ||T(v)||_{V^*} < C||v||_{V}, \forall v \in V C ||v||_{V}=\textrm{sup}\{|L(v)|: v \in V, ||L||_{V^*}=1 \} H \mathbb{R} H H^* g \in H L \in H^* x \in H L(x) = \langle{} f, g \rangle{} ||x||_{H} = ||L(x)||_{H^*} L^P v ||v||","['linear-algebra', 'functional-analysis', 'linear-transformations', 'duality-theorems']"
47,Reference request: arranging distinct numbers into a full rank matrix,Reference request: arranging distinct numbers into a full rank matrix,,"I thought of the following problem: Let $n\ge 2$ .  Suppose you have $n^2$ distinct numbers in some field.    Is it necessarily possible to arrange the numbers into an $n\times n$ matrix of full rank (ie, nonsingular or invertible)? (I am able to solve the problem, for example using the combinatorial nullstellensatz .) I was wondering whether this problem was previous stated elsewhere, perhaps even on this site? My original motivation for the problem was in fact quite similar to this question , but I was rearranging the primes.","I thought of the following problem: Let .  Suppose you have distinct numbers in some field.    Is it necessarily possible to arrange the numbers into an matrix of full rank (ie, nonsingular or invertible)? (I am able to solve the problem, for example using the combinatorial nullstellensatz .) I was wondering whether this problem was previous stated elsewhere, perhaps even on this site? My original motivation for the problem was in fact quite similar to this question , but I was rearranging the primes.",n\ge 2 n^2 n\times n,"['linear-algebra', 'matrices', 'reference-request']"
48,Show matrix is nilpotent,Show matrix is nilpotent,,"I have matrices $A,B$ of dimension $n$ with real coefficients which satisfy the following: $A^2-B^2=c(AB-BA)$ where $c$ is a real number. If $c\neq0$ , prove that $(AB-BA)^n = 0$. So far, I've been able to show that $AB-BA$ is singular. Can someone help?","I have matrices $A,B$ of dimension $n$ with real coefficients which satisfy the following: $A^2-B^2=c(AB-BA)$ where $c$ is a real number. If $c\neq0$ , prove that $(AB-BA)^n = 0$. So far, I've been able to show that $AB-BA$ is singular. Can someone help?",,"['linear-algebra', 'matrices']"
49,How to derive tangential divergence (and thus Laplace-Beltrami operator) from tangential gradient?,How to derive tangential divergence (and thus Laplace-Beltrami operator) from tangential gradient?,,"If we define the tangential gradient for some appropriate function $f$ on a hypersurface $\Gamma$ \begin{align} \nabla_\Gamma f = P \nabla f = (I - n n^T)\nabla f,  \end{align} then we can derive its tangential divergence \begin{align} \text{div}_\Gamma f = \nabla_\Gamma^T f = \nabla^T(f - n^T f n) = \mathrm{tr}(\nabla_\Gamma f) \end{align} and thus the Laplace-Beltrami operator \begin{align} \Delta_\Gamma f = \text{div}_\Gamma(\nabla_\Gamma f) \end{align} A lecture note says we can derive but doesn't say how to derive .  Can you show me step by step why it holds true? In addition, for some $g \in C^1$ the tangential divergence satisfies \begin{align} \text{div}_\Gamma (gf) = g \, \text{div}_\Gamma f.  \end{align} Why?","If we define the tangential gradient for some appropriate function on a hypersurface then we can derive its tangential divergence and thus the Laplace-Beltrami operator A lecture note says we can derive but doesn't say how to derive .  Can you show me step by step why it holds true? In addition, for some the tangential divergence satisfies Why?","f \Gamma \begin{align}
\nabla_\Gamma f = P \nabla f = (I - n n^T)\nabla f, 
\end{align} \begin{align}
\text{div}_\Gamma f = \nabla_\Gamma^T f = \nabla^T(f - n^T f n) = \mathrm{tr}(\nabla_\Gamma f)
\end{align} \begin{align}
\Delta_\Gamma f = \text{div}_\Gamma(\nabla_\Gamma f)
\end{align} g \in C^1 \begin{align}
\text{div}_\Gamma (gf) = g \, \text{div}_\Gamma f. 
\end{align}","['linear-algebra', 'differential-geometry', 'field-theory', 'manifolds']"
50,Bases in vector spaces without $AC$,Bases in vector spaces without,AC,"It is known that without the axiom of choice, not every vector space has a basis. But I was wondering, if I don't assume the axiom of choice, and I choose a vector space $V$ which does have a basis (assume it is infinite, otherwise the following question is trivial - it might also be trivial if it is infinite but I can't see why just yet). Does every subspace of $V$ have a basis ?","It is known that without the axiom of choice, not every vector space has a basis. But I was wondering, if I don't assume the axiom of choice, and I choose a vector space $V$ which does have a basis (assume it is infinite, otherwise the following question is trivial - it might also be trivial if it is infinite but I can't see why just yet). Does every subspace of $V$ have a basis ?",,"['linear-algebra', 'vector-spaces', 'set-theory', 'axiom-of-choice', 'axioms']"
51,Does $\forall v ( T_1 v = 0 \lor T_2 v = 0 \lor \dots \lor T_n v =0 )$ imply $T_1 = 0 \lor T_2 = 0 \lor \dots \lor T_n = 0$?,Does  imply ?,\forall v ( T_1 v = 0 \lor T_2 v = 0 \lor \dots \lor T_n v =0 ) T_1 = 0 \lor T_2 = 0 \lor \dots \lor T_n = 0,"Let $V$ and $W$ be vector spaces and $T_1$, $T_2$, $\dots$, $T_n$ be linear transformations from $V$ to $W$, such that for every $v$ in $V$, either $T_1 v = 0$, $T_2 v = 0$, $\dots$ or $T_n v = 0$. Can we conclude that $T_1 = 0$, $T_2 = 0$, $\dots$ or $T_n = 0$? My attempt: I could prove the statement for $n = 2$ but I couldn't generalize it, and I also couldn't find any counterexample for greater $n$. For $n = 2$, suppose $T_1 \ne 0$ and $T_2 \ne 0$. Hence there are $u$ and $v$ in $V$ such that $T_1 u \ne 0$ and $T_2 v \ne 0$. By hypothesis, we must have $T_1 v = 0$ and $T_2 u = 0$ which yield $T_1 ( u + v ) \ne 0$ and $T_2 ( u + v ) \ne 0$, contradicting the hypothesis. I also noted that my argument for $n = 2$ can be reformulated if $V$ and $W$ were just groups and $T_1$ and $T_2$ were group homomorphisms. So I got interested in the more general problem about groups. (This is in fact another question about which I'm less concerned in this particular post.)","Let $V$ and $W$ be vector spaces and $T_1$, $T_2$, $\dots$, $T_n$ be linear transformations from $V$ to $W$, such that for every $v$ in $V$, either $T_1 v = 0$, $T_2 v = 0$, $\dots$ or $T_n v = 0$. Can we conclude that $T_1 = 0$, $T_2 = 0$, $\dots$ or $T_n = 0$? My attempt: I could prove the statement for $n = 2$ but I couldn't generalize it, and I also couldn't find any counterexample for greater $n$. For $n = 2$, suppose $T_1 \ne 0$ and $T_2 \ne 0$. Hence there are $u$ and $v$ in $V$ such that $T_1 u \ne 0$ and $T_2 v \ne 0$. By hypothesis, we must have $T_1 v = 0$ and $T_2 u = 0$ which yield $T_1 ( u + v ) \ne 0$ and $T_2 ( u + v ) \ne 0$, contradicting the hypothesis. I also noted that my argument for $n = 2$ can be reformulated if $V$ and $W$ were just groups and $T_1$ and $T_2$ were group homomorphisms. So I got interested in the more general problem about groups. (This is in fact another question about which I'm less concerned in this particular post.)",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'linear-transformations', 'group-homomorphism']"
52,How to estimate a specific infinite sum,How to estimate a specific infinite sum,,"Let $M$ be an $n$ by $n$ matrix with each diagonal element equal to $k$ and each non-diagonal element equal to $k-1$ where $n$ and $k$ are positive integers. Let $k < n$ and we can assume both $k$ and $n$ are large. What is $$S_{M,k} = \sum_{x \in \mathbb{Z}^n} e^{-x^T M x}\;?$$ Is there some way to estimate this sum? Update April 21 2016 Hajo argues below that $S_{M,k} \leq S_{M,1} = (\sum_{x=-\infty}^{\infty} e^{-x^2})^n$.  This bound may however be loose when $k \gg 1$.","Let $M$ be an $n$ by $n$ matrix with each diagonal element equal to $k$ and each non-diagonal element equal to $k-1$ where $n$ and $k$ are positive integers. Let $k < n$ and we can assume both $k$ and $n$ are large. What is $$S_{M,k} = \sum_{x \in \mathbb{Z}^n} e^{-x^T M x}\;?$$ Is there some way to estimate this sum? Update April 21 2016 Hajo argues below that $S_{M,k} \leq S_{M,1} = (\sum_{x=-\infty}^{\infty} e^{-x^2})^n$.  This bound may however be loose when $k \gg 1$.",,['linear-algebra']
53,"Show that $ \exp \left(\mathfrak{sl}(2,R)\right)$ is the set of all matrices with positive trace $\geq -2$",Show that  is the set of all matrices with positive trace," \exp \left(\mathfrak{sl}(2,R)\right) \geq -2","Using the fact that every matrix in $SL(2,\mathbb{R})$ is conjugate in $SL(2,\mathbb{R})$ to one of the following matrices: $$ \left(\begin{array}{rr} a & 0\\ 0 & \frac{1}{a} \end{array}\right), \quad  \left(\begin{array}{rr} 1 & t\\ 0 & 1 \end{array}\right), \quad  \left(\begin{array}{rr} -1 & t\\ 0 & -1 \end{array}\right) \quad \mbox{y} \quad \left(\begin{array}{rr} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{array}\right),   $$ Show that the image of the exponential map $$\exp\: : \: \left\{ \left(\begin{array}{rr} x & y \\ z & -x \end{array}\right) \: : \: x,y,z \in \mathbb{R}\right\} \longrightarrow SL(2,\mathbb{R})$$ is \begin{equation}\left\{M\in SL(2,\mathbb{R}) \: : \: tr(M)>-2\right\} \cup \left\{-I\right\}. \qquad  (*)\end{equation} Remark: I have tried many things, for example, we know that $$\exp(PJP^{-1}) =P\exp(J)P^{-1}$$ We also know that the trace is invariant under cujugación, this is $$tr\left(\exp(PJP^{-1}) \right)=tr\left(\exp(J)\right).$$ Therefore, if $M \in \exp \left(SL(2,R)\right)$, then there is $A\in SL(2,R)$ such that $M=\exp(A)$, then, as $A$ is conjugate to one of above matrices, then  $$tr(M)= \left\{ \begin{array}{} tr\left(\exp\left(\left(\begin{array}{rr} a & 0\\ 0 & \frac{1}{a} \end{array}\right) \right) \right) & \mbox{if } A \mbox{ is conjugate to } \left(\begin{array}{rr} a & 0\\ 0 & \frac{1}{a} \end{array}\right)\\ tr\left(\exp\left( \left(\begin{array}{rr} 1 & t\\ 0 & 1 \end{array}\right)\right) \right) & \mbox{if } A \mbox{ is conjugate to } \left(\begin{array}{rr} 1 & t\\ 0 & 1 \end{array}\right)\\ tr\left(\exp\left(\left(\begin{array}{rr} -1 & t\\ 0 & -1 \end{array}\right) \right) \right) & \mbox{if } A \mbox{ is conjugate to } \left(\begin{array}{rr} -1 & t\\ 0 & -1 \end{array}\right)\\ tr\left(\exp\left( \left(\begin{array}{rr} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{array}\right) \right) \right) & \mbox{if } A \mbox{ is conjugate to } \left(\begin{array}{rr} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{array}\right)\\ \end{array} \right.$$ From the latter, the problem that I have is to show that all matris in $ \exp \left(SL(2,R)\right)$ is in the set $(*)$. The biggest problem I have when I want to show that all matris in set $(*)$ is in $ \exp \left(SL(2,R)\right)$.","Using the fact that every matrix in $SL(2,\mathbb{R})$ is conjugate in $SL(2,\mathbb{R})$ to one of the following matrices: $$ \left(\begin{array}{rr} a & 0\\ 0 & \frac{1}{a} \end{array}\right), \quad  \left(\begin{array}{rr} 1 & t\\ 0 & 1 \end{array}\right), \quad  \left(\begin{array}{rr} -1 & t\\ 0 & -1 \end{array}\right) \quad \mbox{y} \quad \left(\begin{array}{rr} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{array}\right),   $$ Show that the image of the exponential map $$\exp\: : \: \left\{ \left(\begin{array}{rr} x & y \\ z & -x \end{array}\right) \: : \: x,y,z \in \mathbb{R}\right\} \longrightarrow SL(2,\mathbb{R})$$ is \begin{equation}\left\{M\in SL(2,\mathbb{R}) \: : \: tr(M)>-2\right\} \cup \left\{-I\right\}. \qquad  (*)\end{equation} Remark: I have tried many things, for example, we know that $$\exp(PJP^{-1}) =P\exp(J)P^{-1}$$ We also know that the trace is invariant under cujugación, this is $$tr\left(\exp(PJP^{-1}) \right)=tr\left(\exp(J)\right).$$ Therefore, if $M \in \exp \left(SL(2,R)\right)$, then there is $A\in SL(2,R)$ such that $M=\exp(A)$, then, as $A$ is conjugate to one of above matrices, then  $$tr(M)= \left\{ \begin{array}{} tr\left(\exp\left(\left(\begin{array}{rr} a & 0\\ 0 & \frac{1}{a} \end{array}\right) \right) \right) & \mbox{if } A \mbox{ is conjugate to } \left(\begin{array}{rr} a & 0\\ 0 & \frac{1}{a} \end{array}\right)\\ tr\left(\exp\left( \left(\begin{array}{rr} 1 & t\\ 0 & 1 \end{array}\right)\right) \right) & \mbox{if } A \mbox{ is conjugate to } \left(\begin{array}{rr} 1 & t\\ 0 & 1 \end{array}\right)\\ tr\left(\exp\left(\left(\begin{array}{rr} -1 & t\\ 0 & -1 \end{array}\right) \right) \right) & \mbox{if } A \mbox{ is conjugate to } \left(\begin{array}{rr} -1 & t\\ 0 & -1 \end{array}\right)\\ tr\left(\exp\left( \left(\begin{array}{rr} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{array}\right) \right) \right) & \mbox{if } A \mbox{ is conjugate to } \left(\begin{array}{rr} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{array}\right)\\ \end{array} \right.$$ From the latter, the problem that I have is to show that all matris in $ \exp \left(SL(2,R)\right)$ is in the set $(*)$. The biggest problem I have when I want to show that all matris in set $(*)$ is in $ \exp \left(SL(2,R)\right)$.",,"['linear-algebra', 'algebraic-geometry', 'differential-geometry', 'exponential-function', 'lie-groups']"
54,Sylvester's criterion for negative semidefinite matrices,Sylvester's criterion for negative semidefinite matrices,,Is there a Sylvester's criterion for negative semidefinite matrices?  I suspect such a criterion to be: All principal minors with odd dimension are non-positive. All principal minors with even dimension are non-negative. Please include a reference if it exists.,Is there a Sylvester's criterion for negative semidefinite matrices?  I suspect such a criterion to be: All principal minors with odd dimension are non-positive. All principal minors with even dimension are non-negative. Please include a reference if it exists.,,"['linear-algebra', 'matrices', 'reference-request']"
55,"Two definitions of uniformly observable, are they equivalent?","Two definitions of uniformly observable, are they equivalent?",,"When I do my research, I found that there are two definitions of uniformly observable, I can't help thinking are they equivalent? These two definitions are listed as follows. For a linear stochastic system with the description $$x_{k+1} = F_{k}x_{k}+w_{k}$$ and $$z_k = H_k x_k+v_k$$ where $F_{k}$ and $H_k$ are system matrix and measurement matrix, respectively, $x_k$ is the system state, $z_k$ is the output, while $w_k$ and $v_k$ are Gaussian while noise sequences, $R_k$ is the covariance of $v_k$. The first definition of uniformly observable is coming form J. C. Spall and K. D. Wall, “Asymptotic distribution theory for the Kalman filter state estimator,” Communications in Statistics - Theory and Methods, vol. 13, no. 16, pp. 1981–2003, Jan. 1984. it says The system  is said to be uniformly completely observable if there exists an integer $1 \leq m< \infty$ and constants $0<\beta_1 \leq \beta_2 <\infty$ such that    \begin{align} \beta_1 I \leq \mathcal{O}(l,k)  \triangleq  \sum_{l=k-m}^{k} \phi^T(l,k) H_l^T R_l^{-1} H_l  \phi(l,k) \leq \beta_2 I \end{align}    for all $k\geq m$, where $\phi(k,k) =I$ and   \begin{align}  \phi(l,k) = (F_l)^{-1} (F_{l+1})^{-1} \cdots (F_{k-1})^{-1}.  \end{align} The second one is found in K. Reif, S. Gunther, E. Yaz, and R. Unbehauen, “Stochastic stability of the discrete-time extended Kalman filter,” IEEE Transactions on Automatic Control, vol. 44, no. 4, pp. 714–728, Apr. 1999. it says Consider time-varying matrices $F_k$, $H_k$, $k \geq 0$,  and let the observability gramian be given by   \begin{align} M_{k+m,k}=\sum_{l=k}^{k+m}  \phi^T(l,k) H_l^T H_l  \phi(l,k)  \end{align}   for some integer $m\geq 0$ with $\phi(k,k)=I$ for    \begin{align}  \phi(l,k) = (F_{l-1}) (F_{l-2}) \cdots (F_{k}),  \end{align}   for $i>n$.  The matrices $F_k$, $H_k$, $k \geq 0$ are said to satisfy   the uniform observability condition, if there are real numbers $\alpha_1$, $\alpha_2$ and an integer $m>0$, such that the following inequality holds:    \begin{align} \alpha_1 I \leq M_{k+m,k} \leq \alpha_2 I. \end{align} My question is that are those two definition equivalent?","When I do my research, I found that there are two definitions of uniformly observable, I can't help thinking are they equivalent? These two definitions are listed as follows. For a linear stochastic system with the description $$x_{k+1} = F_{k}x_{k}+w_{k}$$ and $$z_k = H_k x_k+v_k$$ where $F_{k}$ and $H_k$ are system matrix and measurement matrix, respectively, $x_k$ is the system state, $z_k$ is the output, while $w_k$ and $v_k$ are Gaussian while noise sequences, $R_k$ is the covariance of $v_k$. The first definition of uniformly observable is coming form J. C. Spall and K. D. Wall, “Asymptotic distribution theory for the Kalman filter state estimator,” Communications in Statistics - Theory and Methods, vol. 13, no. 16, pp. 1981–2003, Jan. 1984. it says The system  is said to be uniformly completely observable if there exists an integer $1 \leq m< \infty$ and constants $0<\beta_1 \leq \beta_2 <\infty$ such that    \begin{align} \beta_1 I \leq \mathcal{O}(l,k)  \triangleq  \sum_{l=k-m}^{k} \phi^T(l,k) H_l^T R_l^{-1} H_l  \phi(l,k) \leq \beta_2 I \end{align}    for all $k\geq m$, where $\phi(k,k) =I$ and   \begin{align}  \phi(l,k) = (F_l)^{-1} (F_{l+1})^{-1} \cdots (F_{k-1})^{-1}.  \end{align} The second one is found in K. Reif, S. Gunther, E. Yaz, and R. Unbehauen, “Stochastic stability of the discrete-time extended Kalman filter,” IEEE Transactions on Automatic Control, vol. 44, no. 4, pp. 714–728, Apr. 1999. it says Consider time-varying matrices $F_k$, $H_k$, $k \geq 0$,  and let the observability gramian be given by   \begin{align} M_{k+m,k}=\sum_{l=k}^{k+m}  \phi^T(l,k) H_l^T H_l  \phi(l,k)  \end{align}   for some integer $m\geq 0$ with $\phi(k,k)=I$ for    \begin{align}  \phi(l,k) = (F_{l-1}) (F_{l-2}) \cdots (F_{k}),  \end{align}   for $i>n$.  The matrices $F_k$, $H_k$, $k \geq 0$ are said to satisfy   the uniform observability condition, if there are real numbers $\alpha_1$, $\alpha_2$ and an integer $m>0$, such that the following inequality holds:    \begin{align} \alpha_1 I \leq M_{k+m,k} \leq \alpha_2 I. \end{align} My question is that are those two definition equivalent?",,"['linear-algebra', 'control-theory', 'linear-control']"
56,Understanding the tensor product,Understanding the tensor product,,"I know the definition of the tensor product, and I can somehow understand its importance, but among several constructions in mathematics, somehow I just never grasped the meaning of the tensor product. I don't understand them in the same sense as I understand the concepts of say products, coproducts, semidirect product, fibre product, pushout, kernel, cokernel and their universal properties. When proving things I have a feeling for when I should take for instance the fibre product, and I have a feeling for when it is appropriate to consider certain structures, but the tensor product just never came naturally to me. Just to make it clear. I understand the algebra behind the construction of the tensor product. I can operate with it. I can also verify and prove the Hom-Tensor adjunction, although I do not fully grasp what it really is telling me. I also have seen that the right-exactness of tensoring provides useful, but I never really understood what is going on. Someone told me that I should look at tensor products as linearizing , but I am not sure in what context this was intended as a visualization. I am wondering if anyone knows a reference or could explain to me what the tensor product really means. I am seeking examples where it is ""obvious"" that tensoring will help solve a problem, and how it solves the problem. This is probably a very wide question, and I have not specified the context in which the tensor product is considered, but I feel like there must be some general explanation for what the tensor product really does.","I know the definition of the tensor product, and I can somehow understand its importance, but among several constructions in mathematics, somehow I just never grasped the meaning of the tensor product. I don't understand them in the same sense as I understand the concepts of say products, coproducts, semidirect product, fibre product, pushout, kernel, cokernel and their universal properties. When proving things I have a feeling for when I should take for instance the fibre product, and I have a feeling for when it is appropriate to consider certain structures, but the tensor product just never came naturally to me. Just to make it clear. I understand the algebra behind the construction of the tensor product. I can operate with it. I can also verify and prove the Hom-Tensor adjunction, although I do not fully grasp what it really is telling me. I also have seen that the right-exactness of tensoring provides useful, but I never really understood what is going on. Someone told me that I should look at tensor products as linearizing , but I am not sure in what context this was intended as a visualization. I am wondering if anyone knows a reference or could explain to me what the tensor product really means. I am seeking examples where it is ""obvious"" that tensoring will help solve a problem, and how it solves the problem. This is probably a very wide question, and I have not specified the context in which the tensor product is considered, but I feel like there must be some general explanation for what the tensor product really does.",,"['linear-algebra', 'abstract-algebra', 'category-theory', 'tensor-products']"
57,"Can I go from the LU factorization of a symmetric matrix to its Cholesky factorization, without starting over?","Can I go from the LU factorization of a symmetric matrix to its Cholesky factorization, without starting over?",,"I mistakenly computed the LU factorization and then realized that the question is asking for a Cholesky factorization, i.e., finding a lower triangular matrix L such that the symmetric matrix A has factorization $LL^T$. Can I modify this factorization to achieve the Cholesky factorization? Thanks,","I mistakenly computed the LU factorization and then realized that the question is asking for a Cholesky factorization, i.e., finding a lower triangular matrix L such that the symmetric matrix A has factorization $LL^T$. Can I modify this factorization to achieve the Cholesky factorization? Thanks,",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'lu-decomposition', 'cholesky-decomposition']"
58,Show that $A^k$ has eigenvalues $\lambda^k$ and eigenvectors $v$.,Show that  has eigenvalues  and eigenvectors .,A^k \lambda^k v,"I want to prove the following statement: Let $A \in \Bbb R^{n\times n}$ with eigenvalues $\lambda$ and   eigenvectors $v$. Show that $A^k$ has eigenvalues $\lambda^k$ and   eigenvectors $v$. There are two ways I tried to prove this but I am not sure if either of them is accurate or complete. If $Ax=\lambda x$ then multiplying by $A$ from the left yields $$AAx=A \lambda x \iff A^2x=\lambda Ax \iff A^2x=\lambda (\lambda x)\iff A^2x=\lambda^2x$$ In fact, for every $A$ thats multiplied to both sides, the right side ""gains"" a factor $\lambda$ (since $Ax$ can be substituted by $\lambda x$) while the eigenvectors remain the same. It follows that multiplying both sides by $A^{k-1}$ yields: $$A^{k-1}Ax=A^{k-1}\lambda x \iff A^kx=\lambda (\lambda^{k-1}x)\iff A^kx=\lambda^kx$$ This is a proof that I partly got from Gilbert Strang's Lin. Algebra lecture. Suppose $A$ has $n$ linearly independent eigenvectors. Let $S$ be the matrix that has the eigenvectors of $A$ as its columns. Then, $$AS=A\begin{bmatrix}x_1…x_n\end{bmatrix}=\begin{bmatrix}\lambda_1x_1...\lambda_nx_n\end{bmatrix}=\begin{bmatrix}x_1...x_n\end{bmatrix}\begin{bmatrix}\lambda_1&0&.&0\\0&\lambda_2&.&0\\.&.&.&.\\0&0&.& \lambda_n\end{bmatrix}$$ Let $\Lambda=\begin{bmatrix}\lambda_1&0&.&0\\0&\lambda_2&.&0\\.&.&.&.\\0&0&.& \lambda_n\end{bmatrix}$ then $AS=S\Lambda$ Multiplying by $S^{-1}$ from the right: $$\implies S^{-1}AS=\Lambda \space \space \text{or} \space \space A=S \Lambda S^{-1}$$ $$\implies A^k=(S \Lambda S^{-1})^k=S \Lambda^kS^{-1}$$ It follows that $A^k$ has eigenvalues $\lambda^k$ and eigenvectors $x$. I am not sure if either of them are correct. For the second one, I suspect that I need to guarantee that $S^{-1}$ exists or that $S$ is invertible but I am not sure how to do that. Also, how can I be sure that there are $n$ linearly indep. eigenvectors and not $n-1$ for example? Are there any problems with the first one?","I want to prove the following statement: Let $A \in \Bbb R^{n\times n}$ with eigenvalues $\lambda$ and   eigenvectors $v$. Show that $A^k$ has eigenvalues $\lambda^k$ and   eigenvectors $v$. There are two ways I tried to prove this but I am not sure if either of them is accurate or complete. If $Ax=\lambda x$ then multiplying by $A$ from the left yields $$AAx=A \lambda x \iff A^2x=\lambda Ax \iff A^2x=\lambda (\lambda x)\iff A^2x=\lambda^2x$$ In fact, for every $A$ thats multiplied to both sides, the right side ""gains"" a factor $\lambda$ (since $Ax$ can be substituted by $\lambda x$) while the eigenvectors remain the same. It follows that multiplying both sides by $A^{k-1}$ yields: $$A^{k-1}Ax=A^{k-1}\lambda x \iff A^kx=\lambda (\lambda^{k-1}x)\iff A^kx=\lambda^kx$$ This is a proof that I partly got from Gilbert Strang's Lin. Algebra lecture. Suppose $A$ has $n$ linearly independent eigenvectors. Let $S$ be the matrix that has the eigenvectors of $A$ as its columns. Then, $$AS=A\begin{bmatrix}x_1…x_n\end{bmatrix}=\begin{bmatrix}\lambda_1x_1...\lambda_nx_n\end{bmatrix}=\begin{bmatrix}x_1...x_n\end{bmatrix}\begin{bmatrix}\lambda_1&0&.&0\\0&\lambda_2&.&0\\.&.&.&.\\0&0&.& \lambda_n\end{bmatrix}$$ Let $\Lambda=\begin{bmatrix}\lambda_1&0&.&0\\0&\lambda_2&.&0\\.&.&.&.\\0&0&.& \lambda_n\end{bmatrix}$ then $AS=S\Lambda$ Multiplying by $S^{-1}$ from the right: $$\implies S^{-1}AS=\Lambda \space \space \text{or} \space \space A=S \Lambda S^{-1}$$ $$\implies A^k=(S \Lambda S^{-1})^k=S \Lambda^kS^{-1}$$ It follows that $A^k$ has eigenvalues $\lambda^k$ and eigenvectors $x$. I am not sure if either of them are correct. For the second one, I suspect that I need to guarantee that $S^{-1}$ exists or that $S$ is invertible but I am not sure how to do that. Also, how can I be sure that there are $n$ linearly indep. eigenvectors and not $n-1$ for example? Are there any problems with the first one?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
59,Hadamard matrices and sub-matrices (Converse of Sylvester Construction),Hadamard matrices and sub-matrices (Converse of Sylvester Construction),,"Let $H$ be a $d$ by $d$ real Hadamard matrix , namely: $$HH^{T}=d I$$  where $I$ is the identity matrix and $d=2^{k}$ for some natural number $k\geq 2$. The entries of $H$ are either $1$ or $-1$ and it has orthogonal rows. Can we say that this matrix has four $\frac{d}{2}$ by $\frac{d}{2}$ sub-matrices each of which  have orthogonal rows? If not, what conditions do we need on the matrix so that it would have such (Hadamard like) sub-matrices?","Let $H$ be a $d$ by $d$ real Hadamard matrix , namely: $$HH^{T}=d I$$  where $I$ is the identity matrix and $d=2^{k}$ for some natural number $k\geq 2$. The entries of $H$ are either $1$ or $-1$ and it has orthogonal rows. Can we say that this matrix has four $\frac{d}{2}$ by $\frac{d}{2}$ sub-matrices each of which  have orthogonal rows? If not, what conditions do we need on the matrix so that it would have such (Hadamard like) sub-matrices?",,"['linear-algebra', 'matrices']"
60,Eigenvalues of Group Elements and Quaternions,Eigenvalues of Group Elements and Quaternions,,"The quaternion algebra is given by $\mathbb{H}$ = $\{a+bi+cj+dk \mid a, b, c, d \in \mathbb{R}, i^2 = j^2 = k^2 = -1, ij = k = -ji\} := \{z_1+z_2j \mid z_1, z_2 \in \mathbb{C}\}.$ I consider the 3-sphere as the set of unit quaternions (the quaternions of length 1) as follows: $\mathbb{S}^3 = \{a + bi + cj + dk \mid a^2 + b^2 + c^2 + d^2 = 1\} = \{z_1 + z_2j \mid |z_1|^2 + |z_2|^2 = 1\}$ The product in $\mathbb{H}$ induces a group structure on $\mathbb{S}^3$. For each pair $(p, q)$ of elements of $\mathbb{S}^3$, the function  $\Phi_{p,q} : \mathbb{H} \rightarrow \mathbb{H}$  with $\Phi_{p,q}(h) = phq^{-1}$ leaves invariant the length of quaternions. We can, therefore, define a homomorphism of groups: $\Phi : \mathbb{S}^3 \times \mathbb{S}^3 \rightarrow \mathrm{SO}(4)$ such that $\Phi(p, q) = \Phi_{p,q}$. I am looking at subgroup $C_4 \times D^*_{24}$ of  $\mathbb{S}^3 \times \mathbb{S}^3$, where $C_4$ is the cyclic group of order 4 and $D^*_{12}$ is the binary dihedral group of order 12. I have looked at 3 different ways of representing this group and in each case I get resulting matrices in $\mathrm{SO}(4)$ with different eigenvalues. Here are the 3 ways: $C_4 = \{\langle g \rangle$, where $g^4 = 1\}$, $D^*_{12} = \{\langle A, B \rangle$, where $A^{6} = 1, B^4 = 1$ and $BAB^{-1} = A^{-1}\}$. I represent $g$ as the matrix \begin{pmatrix}  i & 0 \\  0 & -i  \end{pmatrix} I represent $A$ as the matrix  \begin{pmatrix}   e^{i\pi/3} & 0 \\                     0 & e^{-i\pi/3} \end{pmatrix}. I represent $B$ as the matrix \begin{pmatrix}   0 & i \\                   i & 0 \end{pmatrix}. With this, $BA$ is the matrix  \begin{pmatrix}  0 & e^{i\pi/6} \\                     -e^{-i\pi/6} & 0 \end{pmatrix}. Then $\Phi(g, BA)(h)$ is the $4 \times 4$ matrix \begin{pmatrix}  0 & 0 & \sin \frac{\pi}{6} & \cos \frac{\pi}{6} \\\\                     0 & 0 & -\cos \frac{\pi}{6} & \sin \frac{\pi}{6} \\\\ \sin \frac{\pi}{6} & \cos \frac{\pi}{6} & 0 & 0 \\\\                     -\cos \frac{\pi}{6} & \sin \frac{\pi}{6} & 0 & 0 \end{pmatrix}. This has eigenvalues $\pm e^{\pm i\pi/3}$ I  can also view $D^*_{12} = C_{6} \cup C_{6}j$ With $g$ and $A$ as before, I represent $j$ as the matrix  \begin{pmatrix}   0 & 1 \\                     -1 & 0 \end{pmatrix}. With this, $Aj$ is the matrix \begin{pmatrix}  \ 0 & e^{i\pi/3} \\                    -e^{-i\pi/3} & 0 \end{pmatrix}. Now, $\Phi(g, Aj)(h)$ is the $4 \times 4$ matrix \begin{pmatrix}  0 & 0 & \sin \frac{\pi}{3} & \cos \frac{\pi}{3} \\\\                     0 & 0 & -\cos \frac{\pi}{3} & \sin \frac{\pi}{3} \\\\ \sin \frac{\pi}{3} & \cos \frac{\pi}{3} & 0 & 0 \\\\                     -\cos \frac{\pi}{3} & \sin \frac{\pi}{3} & 0 & 0 \end{pmatrix}. This has eigenvalues $\pm e^{\pm i\pi/6}$ I represent $g = i$, $A = e^{i\pi/3}$ and $j = j$. With this  $\Phi(i, Aj)(h)$ = $i (a + bi + cj + dk) (-jcos(\pi/3) - ksin(\pi/3))$ = $c\sin(\pi/3)-d\cos(\pi/3) + i(c\cos(\pi/3) + d\sin(\pi/3)) + j(a\sin(\pi/3)+ b\cos(\pi/3)) + k(-a\cos(\pi/3) + b\sin(\pi/3))$ which gives the $4 \times 4$ matrix for $\Phi(i, Aj)$ to be \begin{pmatrix}  0 & 0 & \sin \frac{\pi}{3} & -\cos \frac{\pi}{3} \\\\                     0 & 0 & \cos \frac{\pi}{3} & \sin \frac{\pi}{3} \\\\ \sin \frac{\pi}{3} & \cos \frac{\pi}{3} & 0 & 0 \\\\                     -\cos \frac{\pi}{3} & \sin \frac{\pi}{3} & 0 & 0 \end{pmatrix}. This has eigenvalues $\pm 1, \pm 1$ So I have 3 different ways to do this and in each case, I get a different matrix in $\mathrm{SO}(4)$ with different eigenvalues. I thought that if the three matrices represent the same element in $\mathrm{SO}(4)$, their eigenvalues should have been the same. What mistake am I making here?","The quaternion algebra is given by $\mathbb{H}$ = $\{a+bi+cj+dk \mid a, b, c, d \in \mathbb{R}, i^2 = j^2 = k^2 = -1, ij = k = -ji\} := \{z_1+z_2j \mid z_1, z_2 \in \mathbb{C}\}.$ I consider the 3-sphere as the set of unit quaternions (the quaternions of length 1) as follows: $\mathbb{S}^3 = \{a + bi + cj + dk \mid a^2 + b^2 + c^2 + d^2 = 1\} = \{z_1 + z_2j \mid |z_1|^2 + |z_2|^2 = 1\}$ The product in $\mathbb{H}$ induces a group structure on $\mathbb{S}^3$. For each pair $(p, q)$ of elements of $\mathbb{S}^3$, the function  $\Phi_{p,q} : \mathbb{H} \rightarrow \mathbb{H}$  with $\Phi_{p,q}(h) = phq^{-1}$ leaves invariant the length of quaternions. We can, therefore, define a homomorphism of groups: $\Phi : \mathbb{S}^3 \times \mathbb{S}^3 \rightarrow \mathrm{SO}(4)$ such that $\Phi(p, q) = \Phi_{p,q}$. I am looking at subgroup $C_4 \times D^*_{24}$ of  $\mathbb{S}^3 \times \mathbb{S}^3$, where $C_4$ is the cyclic group of order 4 and $D^*_{12}$ is the binary dihedral group of order 12. I have looked at 3 different ways of representing this group and in each case I get resulting matrices in $\mathrm{SO}(4)$ with different eigenvalues. Here are the 3 ways: $C_4 = \{\langle g \rangle$, where $g^4 = 1\}$, $D^*_{12} = \{\langle A, B \rangle$, where $A^{6} = 1, B^4 = 1$ and $BAB^{-1} = A^{-1}\}$. I represent $g$ as the matrix \begin{pmatrix}  i & 0 \\  0 & -i  \end{pmatrix} I represent $A$ as the matrix  \begin{pmatrix}   e^{i\pi/3} & 0 \\                     0 & e^{-i\pi/3} \end{pmatrix}. I represent $B$ as the matrix \begin{pmatrix}   0 & i \\                   i & 0 \end{pmatrix}. With this, $BA$ is the matrix  \begin{pmatrix}  0 & e^{i\pi/6} \\                     -e^{-i\pi/6} & 0 \end{pmatrix}. Then $\Phi(g, BA)(h)$ is the $4 \times 4$ matrix \begin{pmatrix}  0 & 0 & \sin \frac{\pi}{6} & \cos \frac{\pi}{6} \\\\                     0 & 0 & -\cos \frac{\pi}{6} & \sin \frac{\pi}{6} \\\\ \sin \frac{\pi}{6} & \cos \frac{\pi}{6} & 0 & 0 \\\\                     -\cos \frac{\pi}{6} & \sin \frac{\pi}{6} & 0 & 0 \end{pmatrix}. This has eigenvalues $\pm e^{\pm i\pi/3}$ I  can also view $D^*_{12} = C_{6} \cup C_{6}j$ With $g$ and $A$ as before, I represent $j$ as the matrix  \begin{pmatrix}   0 & 1 \\                     -1 & 0 \end{pmatrix}. With this, $Aj$ is the matrix \begin{pmatrix}  \ 0 & e^{i\pi/3} \\                    -e^{-i\pi/3} & 0 \end{pmatrix}. Now, $\Phi(g, Aj)(h)$ is the $4 \times 4$ matrix \begin{pmatrix}  0 & 0 & \sin \frac{\pi}{3} & \cos \frac{\pi}{3} \\\\                     0 & 0 & -\cos \frac{\pi}{3} & \sin \frac{\pi}{3} \\\\ \sin \frac{\pi}{3} & \cos \frac{\pi}{3} & 0 & 0 \\\\                     -\cos \frac{\pi}{3} & \sin \frac{\pi}{3} & 0 & 0 \end{pmatrix}. This has eigenvalues $\pm e^{\pm i\pi/6}$ I represent $g = i$, $A = e^{i\pi/3}$ and $j = j$. With this  $\Phi(i, Aj)(h)$ = $i (a + bi + cj + dk) (-jcos(\pi/3) - ksin(\pi/3))$ = $c\sin(\pi/3)-d\cos(\pi/3) + i(c\cos(\pi/3) + d\sin(\pi/3)) + j(a\sin(\pi/3)+ b\cos(\pi/3)) + k(-a\cos(\pi/3) + b\sin(\pi/3))$ which gives the $4 \times 4$ matrix for $\Phi(i, Aj)$ to be \begin{pmatrix}  0 & 0 & \sin \frac{\pi}{3} & -\cos \frac{\pi}{3} \\\\                     0 & 0 & \cos \frac{\pi}{3} & \sin \frac{\pi}{3} \\\\ \sin \frac{\pi}{3} & \cos \frac{\pi}{3} & 0 & 0 \\\\                     -\cos \frac{\pi}{3} & \sin \frac{\pi}{3} & 0 & 0 \end{pmatrix}. This has eigenvalues $\pm 1, \pm 1$ So I have 3 different ways to do this and in each case, I get a different matrix in $\mathrm{SO}(4)$ with different eigenvalues. I thought that if the three matrices represent the same element in $\mathrm{SO}(4)$, their eigenvalues should have been the same. What mistake am I making here?",,"['linear-algebra', 'group-theory', 'eigenvalues-eigenvectors', 'quaternions']"
61,Is a normal matrix satisfying $A^TA=...$ circulant?,Is a normal matrix satisfying  circulant?,A^TA=...,"Let $A=\{a_{ij}\}$ be a normal matrix such that $a_{ij}\geq 0$ with equality iff $i=j$. Suppose that $$ A^TA=\begin{pmatrix} a & b & \cdots & b\\ b & a & \ddots & \vdots\\ \vdots & \ddots & a & b\\ b & \cdots & b & a\\ \end{pmatrix},\ where\ b>0. $$ Does it follow that $A$ is a circulant matrix? Note: There is a partial classification of non-negative normal matrices posted here , which seems like it can be used to attack this problem. There is a geometric interpretation as well: both the set of rows and the set of columns of $A$ form equidistant sets of vectors on a sphere, and basic geometry appears to severely restrict the possibilities.","Let $A=\{a_{ij}\}$ be a normal matrix such that $a_{ij}\geq 0$ with equality iff $i=j$. Suppose that $$ A^TA=\begin{pmatrix} a & b & \cdots & b\\ b & a & \ddots & \vdots\\ \vdots & \ddots & a & b\\ b & \cdots & b & a\\ \end{pmatrix},\ where\ b>0. $$ Does it follow that $A$ is a circulant matrix? Note: There is a partial classification of non-negative normal matrices posted here , which seems like it can be used to attack this problem. There is a geometric interpretation as well: both the set of rows and the set of columns of $A$ form equidistant sets of vectors on a sphere, and basic geometry appears to severely restrict the possibilities.",,"['linear-algebra', 'matrices', 'geometry', 'eigenvalues-eigenvectors', 'diagonalization']"
62,Generating a stochastic matrix with a given second dominant eigenvalue,Generating a stochastic matrix with a given second dominant eigenvalue,,"I need a procedure (iterative or otherwise) that, given a positive integer $N$ and a (possibly complex) number $\lambda$ such that $0 < \vert \lambda \vert < 1$, will be able to generate an $N \times N$ stochastic matrix $\mathbf{M}$ (in which every column sums to $1$) whose second-largest (in absolute value) eigenvalue is $\lambda$. The constraint is only on $\vert \lambda \vert$ and it does not matter if $\lambda$ is complex. The first (largest-magnitude) eigenvalue is always $1$. The rest of the eigenvalues are arbitrary. I need the procedure (if one exists) to always converge with a correct answer with probability $1$ (if it is iterative). I need this in Markov chain simulations and I want to control the convergence rate from the initial distributions to the equilibrium one. EDIT : It is also important to know that randomly generating a stochastic matrix in Matlab (shown in the figure below) yields a relatively small second-largest eigenvalue, indicating that the probability of having large second-largest eigenvalue is small and it needs to be crafted. You can also see the eigenvalue $1$ that always exists in isolation on the far right. The figure below is the spectra of $10000$ randomly generated $16 \times 16$ stochastic matrices. As pointed out in the comments, I found a paper that constructs a doubly stochastic matrix explicitly from a given positive spectrum. However, the constructed matrices are, in fact, very concentrated around the diagonal (these can be called lazy Markov chains since they tend to stay where they are). This result explains why it is extremely unlikely that a randomly generated stochastic matrix would have eigenvalues closer to $1$ than shown in the above figure. This means there is only local transitions between Markov chain states with very weak global transitions. Is that indeed implied by asking for large eigenvalues of the transition matrix ? Here is a surface plot of a $100 \times 100$ stochastic matrix constructed from a randomly generated positive spectrum as explained in this paper . I also found a result here saying that lazy, reversible Markov chains have positive spectra, which is in line with the above result.","I need a procedure (iterative or otherwise) that, given a positive integer $N$ and a (possibly complex) number $\lambda$ such that $0 < \vert \lambda \vert < 1$, will be able to generate an $N \times N$ stochastic matrix $\mathbf{M}$ (in which every column sums to $1$) whose second-largest (in absolute value) eigenvalue is $\lambda$. The constraint is only on $\vert \lambda \vert$ and it does not matter if $\lambda$ is complex. The first (largest-magnitude) eigenvalue is always $1$. The rest of the eigenvalues are arbitrary. I need the procedure (if one exists) to always converge with a correct answer with probability $1$ (if it is iterative). I need this in Markov chain simulations and I want to control the convergence rate from the initial distributions to the equilibrium one. EDIT : It is also important to know that randomly generating a stochastic matrix in Matlab (shown in the figure below) yields a relatively small second-largest eigenvalue, indicating that the probability of having large second-largest eigenvalue is small and it needs to be crafted. You can also see the eigenvalue $1$ that always exists in isolation on the far right. The figure below is the spectra of $10000$ randomly generated $16 \times 16$ stochastic matrices. As pointed out in the comments, I found a paper that constructs a doubly stochastic matrix explicitly from a given positive spectrum. However, the constructed matrices are, in fact, very concentrated around the diagonal (these can be called lazy Markov chains since they tend to stay where they are). This result explains why it is extremely unlikely that a randomly generated stochastic matrix would have eigenvalues closer to $1$ than shown in the above figure. This means there is only local transitions between Markov chain states with very weak global transitions. Is that indeed implied by asking for large eigenvalues of the transition matrix ? Here is a surface plot of a $100 \times 100$ stochastic matrix constructed from a randomly generated positive spectrum as explained in this paper . I also found a result here saying that lazy, reversible Markov chains have positive spectra, which is in line with the above result.",,"['linear-algebra', 'algorithms', 'stochastic-processes', 'markov-chains', 'spectral-graph-theory']"
63,"Duality of $Z(G)$ and $[G,G]$ in representation?",Duality of  and  in representation?,"Z(G) [G,G]","This question and its many wonderful answers illustrate many faces of the duality of $Z(G)$ and $[G,G]$, the centre/ commutator duality of a group. I was thinking about its manifestation in group representations. We might as well look at a finite group $G$ and its classes of irreducible representations $\hat{G}$, and some possible candidates might be \begin{equation} d_{\alpha}=\operatorname{degree}(\alpha) \end{equation}  and \begin{equation} n_{d}=\#\{\alpha\in\hat{G}:d_\alpha=d\}. \end{equation}Some evidence might be contained in $d_{\alpha}$ divides $\#G/Z(G)$ for all $\alpha\in\hat{G}$. So the larger $Z(G)$ is, the more commutative $G$ is, the smaller $d_\alpha$ is. On the other hand, $n_1=\#G/[G,G]$. So the larger $[G,G]$ is, the less commutative $G$ is, the smaller $n_1$ is. So, I am wondering whether these can be made precise. And are there other manifestations of the $Z(G)/[G,G]$-duality in group representations? Thanks very much!","This question and its many wonderful answers illustrate many faces of the duality of $Z(G)$ and $[G,G]$, the centre/ commutator duality of a group. I was thinking about its manifestation in group representations. We might as well look at a finite group $G$ and its classes of irreducible representations $\hat{G}$, and some possible candidates might be \begin{equation} d_{\alpha}=\operatorname{degree}(\alpha) \end{equation}  and \begin{equation} n_{d}=\#\{\alpha\in\hat{G}:d_\alpha=d\}. \end{equation}Some evidence might be contained in $d_{\alpha}$ divides $\#G/Z(G)$ for all $\alpha\in\hat{G}$. So the larger $Z(G)$ is, the more commutative $G$ is, the smaller $d_\alpha$ is. On the other hand, $n_1=\#G/[G,G]$. So the larger $[G,G]$ is, the less commutative $G$ is, the smaller $n_1$ is. So, I am wondering whether these can be made precise. And are there other manifestations of the $Z(G)/[G,G]$-duality in group representations? Thanks very much!",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'representation-theory', 'duality-theorems']"
64,Does anyone know a reference to best-fitting lines with integral coefficients?,Does anyone know a reference to best-fitting lines with integral coefficients?,,"I'm writing up a manual on how to generate ""nice"" Linear Algebra problems; that is, where the solutions tend to be integral. I ""discovered"" the following fact about the best-fitting line: Theorem. Let $x_1,x_2,\ldots,x_n$ be integers such that (a) $\displaystyle\sum_{i=1}^n x_i=0$, and (b) $x_i\not=x_j$ for some $i,j$. Then the best-fitting line $y=ax+b$ for the data points $(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)$ has integral coefficients iff  $$\sum_{k=1}^n x_i y_i \equiv 0 \pmod {\sum_{i=1}^n {x_i}^2} \quad\quad\quad\quad{\rm and}\quad\quad\quad\quad \sum_{k=1}^n y_i \equiv 0 \pmod {n}.$$ This is the sort of result which is nice, was probably discovered before, and which I cannot find any reference to. Has anyone seen this in a paper anywhere?","I'm writing up a manual on how to generate ""nice"" Linear Algebra problems; that is, where the solutions tend to be integral. I ""discovered"" the following fact about the best-fitting line: Theorem. Let $x_1,x_2,\ldots,x_n$ be integers such that (a) $\displaystyle\sum_{i=1}^n x_i=0$, and (b) $x_i\not=x_j$ for some $i,j$. Then the best-fitting line $y=ax+b$ for the data points $(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)$ has integral coefficients iff  $$\sum_{k=1}^n x_i y_i \equiv 0 \pmod {\sum_{i=1}^n {x_i}^2} \quad\quad\quad\quad{\rm and}\quad\quad\quad\quad \sum_{k=1}^n y_i \equiv 0 \pmod {n}.$$ This is the sort of result which is nice, was probably discovered before, and which I cannot find any reference to. Has anyone seen this in a paper anywhere?",,"['linear-algebra', 'elementary-number-theory', 'reference-request', 'least-squares']"
65,Homomorphisms between fields are injective.,Homomorphisms between fields are injective.,,"How would I prove this? I know that I must show $f(a)=f(b) \Rightarrow a = b$ I also know I must use the definition of homomorphism, ie: $f(a+b)=f(a)+f(b)$ $f(ab)=f(a)f(b)$ $f(1)=1$ I am assuming that a contradiction would be a good approach to solve this, but not quite sure on specifics.","How would I prove this? I know that I must show I also know I must use the definition of homomorphism, ie: I am assuming that a contradiction would be a good approach to solve this, but not quite sure on specifics.",f(a)=f(b) \Rightarrow a = b f(a+b)=f(a)+f(b) f(ab)=f(a)f(b) f(1)=1,"['linear-algebra', 'field-theory', 'finite-fields']"
66,Prove that if $A^2=0$ then $A$ is not invertible,Prove that if  then  is not invertible,A^2=0 A,Let $A$ be $n\times n$ matrix. Prove that if $A^2=\mathbf{0}$ then $A$ is not invertible.,Let $A$ be $n\times n$ matrix. Prove that if $A^2=\mathbf{0}$ then $A$ is not invertible.,,['linear-algebra']
67,"Why do mathematicians say that ""let an operator be represented by a matrix"" instead of operator is the matrix?","Why do mathematicians say that ""let an operator be represented by a matrix"" instead of operator is the matrix?",,"For example, look at this sentence from Perko's text on dynamical system ""It follows from Cauchy Schwarz inequality that if $T \in L(R^n)$ is represented by the matrix $A$ with respect to the standard basis for $R^n$ $_\cdots$"" pg 11 What does it mean for a $T: R^n \to R^n$ to be represented by a matrix? Isn't it by definition that $T: R^n \to R^n$ is equivalent to an n by n matrix? Can someone translate exactly what it means by ""represented"" versus ""not represented""?","For example, look at this sentence from Perko's text on dynamical system ""It follows from Cauchy Schwarz inequality that if $T \in L(R^n)$ is represented by the matrix $A$ with respect to the standard basis for $R^n$ $_\cdots$"" pg 11 What does it mean for a $T: R^n \to R^n$ to be represented by a matrix? Isn't it by definition that $T: R^n \to R^n$ is equivalent to an n by n matrix? Can someone translate exactly what it means by ""represented"" versus ""not represented""?",,"['linear-algebra', 'operator-theory']"
68,"If $A$ is rank-$1$, show that $A^2=cA$ for some scalar $c$","If  is rank-, show that  for some scalar",A 1 A^2=cA c,Let an $n \times n$ complex matrix $A$ having rank $1$ .  Prove that $A^2 = cA$ for some scalar $c$ . What I know is that all matrices having rank 1 have rows based on a scalar multiple of the other. This should probably be enough to figure out a very simple proof but I've been out of academic pursuits for 15 years and am having a hard time shaking the dust off.,Let an complex matrix having rank .  Prove that for some scalar . What I know is that all matrices having rank 1 have rows based on a scalar multiple of the other. This should probably be enough to figure out a very simple proof but I've been out of academic pursuits for 15 years and am having a hard time shaking the dust off.,n \times n A 1 A^2 = cA c,"['linear-algebra', 'matrices']"
69,Showing determinants using trace in a 2x2 matrix,Showing determinants using trace in a 2x2 matrix,,"I am confused about this homework question. It says ""Show that : $\det(A) = \frac 12 \begin{vmatrix}\operatorname{tr}(A)&1\\\operatorname{tr}(A^2)& \operatorname{tr}(A)\end{vmatrix}$ for every $2\times 2$ matrix."" I am not sure how to ""show"" this. I'm guessing it means a proof of sorts. But I am also confused as to why the $\det(A) = 1/2$ times a matrix. I'm pretty sure determinants are just a number, not a matrix. Any tips would be great. Thanks in advance.","I am confused about this homework question. It says ""Show that : $\det(A) = \frac 12 \begin{vmatrix}\operatorname{tr}(A)&1\\\operatorname{tr}(A^2)& \operatorname{tr}(A)\end{vmatrix}$ for every $2\times 2$ matrix."" I am not sure how to ""show"" this. I'm guessing it means a proof of sorts. But I am also confused as to why the $\det(A) = 1/2$ times a matrix. I'm pretty sure determinants are just a number, not a matrix. Any tips would be great. Thanks in advance.",,"['linear-algebra', 'matrices', 'determinant']"
70,A certain kind of non-linear mapping from $\mathbb{R}^2$ to $\mathbb{R}^2$,A certain kind of non-linear mapping from  to,\mathbb{R}^2 \mathbb{R}^2,"Give an example of a map $T:\mathbb{R}^2\rightarrow\mathbb{R}^2$ with both of the following properties: a. $T(kx)=kT(x)$ for all $x\in \mathbb{R}^2, k\in \mathbb{R}$ b. $T$ fails to be a linear transformation I am really stuck on this. I can only find matrices that are not linear transformations and don't fulfill the first condition, or they are linear transformations and they do fulfill the first condition. I am not sure how to go about doing this.","Give an example of a map $T:\mathbb{R}^2\rightarrow\mathbb{R}^2$ with both of the following properties: a. $T(kx)=kT(x)$ for all $x\in \mathbb{R}^2, k\in \mathbb{R}$ b. $T$ fails to be a linear transformation I am really stuck on this. I can only find matrices that are not linear transformations and don't fulfill the first condition, or they are linear transformations and they do fulfill the first condition. I am not sure how to go about doing this.",,"['linear-algebra', 'examples-counterexamples']"
71,Is the sum of singular and nonsingular matrix always a nonsingular matrix?,Is the sum of singular and nonsingular matrix always a nonsingular matrix?,,"If $A$ and $B$ are singular and nonsingular respectively, where both are square, is $A+B$ always nonsingular? Suppose that $A$ is a singular matrix and that $B$ is nonsingular, where both are square of the same dimension. It is not hard to see that $AB$ and $BA$ are both singular. It seems natural to ask whether the same is true for addition of matrices instead of product. For $1\times1$ matrices (i.e., numbers), the only singular matrix is $0$ ; so if we add it to any nonsingular (invertible) matrix, it remains nonsingular. So to find a counterexample, we have to look at bigger matrices.","If and are singular and nonsingular respectively, where both are square, is always nonsingular? Suppose that is a singular matrix and that is nonsingular, where both are square of the same dimension. It is not hard to see that and are both singular. It seems natural to ask whether the same is true for addition of matrices instead of product. For matrices (i.e., numbers), the only singular matrix is ; so if we add it to any nonsingular (invertible) matrix, it remains nonsingular. So to find a counterexample, we have to look at bigger matrices.",A B A+B A B AB BA 1\times1 0,"['linear-algebra', 'matrices', 'examples-counterexamples']"
72,"Intuition behind Isomorphic spaces ""Being the Same""","Intuition behind Isomorphic spaces ""Being the Same""",,"I know that isomorphic spaces are treated as the same.  But why is it so.... Like $R^2$ and the set of all ${(x, y, 0) }$ are isomorphic but the ""same"" vectors in the two spaces are actually different vectors. Some isomorphic spaces might be having even different rules of vector addition and scalar multiplication, then why the corresponding vectors in both will be the same. Also any N dimensional vector space $V$ is isomorphic to $F^n$ . Buth that n dimensional vector space can be a space of matrices or of polynomials or of any other abstract vectors. How does saying corresponding vectors in each such n dimensional vector spaces are ""the same"" as the the n tuple in $F^n$ . All these vectors have different rules for multiplication and addition, then what is the intuitive reasoning behind them being treated as same. Will it not defeat the purpose of treating abstract objects as vectors. Edit: Precisely this An n dimensional polynomial space is isomorphic to $F^n$ . An n dimensional space of matrices ( n= ab) is isomorphic to $F^n$ . Now how is Differentiation in n-dimensional polynomial space mirrored in $F^n$ ( n- tuple are constants) and How is a transpose operation in n dimensional matrix space mirrored to $F^n$ . Also since the n dimensional space and n dimensional matrix space are isomorphic to $F^n$ , then they should be isomorphic to each other too ( is this correct). But then how is differentiation in n dimensional polynomial space mirrored to an n dimensional matrix space.","I know that isomorphic spaces are treated as the same.  But why is it so.... Like and the set of all are isomorphic but the ""same"" vectors in the two spaces are actually different vectors. Some isomorphic spaces might be having even different rules of vector addition and scalar multiplication, then why the corresponding vectors in both will be the same. Also any N dimensional vector space is isomorphic to . Buth that n dimensional vector space can be a space of matrices or of polynomials or of any other abstract vectors. How does saying corresponding vectors in each such n dimensional vector spaces are ""the same"" as the the n tuple in . All these vectors have different rules for multiplication and addition, then what is the intuitive reasoning behind them being treated as same. Will it not defeat the purpose of treating abstract objects as vectors. Edit: Precisely this An n dimensional polynomial space is isomorphic to . An n dimensional space of matrices ( n= ab) is isomorphic to . Now how is Differentiation in n-dimensional polynomial space mirrored in ( n- tuple are constants) and How is a transpose operation in n dimensional matrix space mirrored to . Also since the n dimensional space and n dimensional matrix space are isomorphic to , then they should be isomorphic to each other too ( is this correct). But then how is differentiation in n dimensional polynomial space mirrored to an n dimensional matrix space.","R^2 {(x, y, 0) } V F^n F^n F^n F^n F^n F^n F^n","['linear-algebra', 'linear-transformations', 'intuition', 'vector-space-isomorphism']"
73,Proof If $AB-I$ Invertible then $BA-I$ invertible. [duplicate],Proof If  Invertible then  invertible. [duplicate],AB-I BA-I,"This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 7 years ago . I have these problems : Proof If $AB-I$ invertible then $BA-I$ invertible. Proof If $I-AB$ invertible then $I-BA$ invertible. I think I solve it correctly, But I'm not so sure, I'll be glad to receive feedback. If $AB-I$ invertible then :  $$\det|AB-I| \neq 0 \implies \\ \det|A-I||B| \neq 0 \implies \\ \det|B||A-I| \neq 0  \implies\\ \det|BA-I| \neq 0$$ Therefore $BA-I$ invertible. If $I-AB$ invertible then : $$\det|I-AB| \neq 0 \implies \\ \det|I-B||A| \neq 0 \implies \\ \det|I-BA| \neq 0$$ Therefore $I-BA$ invertible.","This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 7 years ago . I have these problems : Proof If $AB-I$ invertible then $BA-I$ invertible. Proof If $I-AB$ invertible then $I-BA$ invertible. I think I solve it correctly, But I'm not so sure, I'll be glad to receive feedback. If $AB-I$ invertible then :  $$\det|AB-I| \neq 0 \implies \\ \det|A-I||B| \neq 0 \implies \\ \det|B||A-I| \neq 0  \implies\\ \det|BA-I| \neq 0$$ Therefore $BA-I$ invertible. If $I-AB$ invertible then : $$\det|I-AB| \neq 0 \implies \\ \det|I-B||A| \neq 0 \implies \\ \det|I-BA| \neq 0$$ Therefore $I-BA$ invertible.",,"['linear-algebra', 'matrices', 'determinant']"
74,"Proving that for when AB = 2BA, then B is not invertible if A is invertible.","Proving that for when AB = 2BA, then B is not invertible if A is invertible.",,"Suppose that A and B are 2x2 matrices, A is invertible, and AB = 2BA. Prove that B is not invertible. My first thought is that I need to find what B is in terms of the terms of A, and proving B's invertibility from there, but I don't know how to start that.. Im really stumped. Any help/tips/advice would be much appreciated!","Suppose that A and B are 2x2 matrices, A is invertible, and AB = 2BA. Prove that B is not invertible. My first thought is that I need to find what B is in terms of the terms of A, and proving B's invertibility from there, but I don't know how to start that.. Im really stumped. Any help/tips/advice would be much appreciated!",,['linear-algebra']
75,"$A^2=I,\det A>0$ implies $A+I$ is non-singular",implies  is non-singular,"A^2=I,\det A>0 A+I","Question: If a square matrix $A$ satisfies $A^2=I$ and $\det A>0$, show that $A+I$ is non-singular. I have tried to suppose a non-zero vector $x$ s.t. $Ax=x$ but fail to make a contradiction. And I tried to find the inverse matrix of $A+I$ directly, suppose $(A+I)^{-1}=\alpha I +\beta A$, but it still doesn't work. (Update: According to the two answers, this question itself is incorrect.)","Question: If a square matrix $A$ satisfies $A^2=I$ and $\det A>0$, show that $A+I$ is non-singular. I have tried to suppose a non-zero vector $x$ s.t. $Ax=x$ but fail to make a contradiction. And I tried to find the inverse matrix of $A+I$ directly, suppose $(A+I)^{-1}=\alpha I +\beta A$, but it still doesn't work. (Update: According to the two answers, this question itself is incorrect.)",,['linear-algebra']
76,Are conjugate vectors unique?,Are conjugate vectors unique?,,"A set of nonzero vectors $\{p_0,p_1,\ldots,p_{n-1}\}$ is said to be conjugate with respect to a symmetric positive definite matrix $A$ if $$ p_i^{\mathrm T}Ap_j=0 $$ for all $i\ne j$ . Such vectors are used in the conjugate gradient method . It follows that conjugate vectors are also linearly independent. An example of conjugate vectors are the eigenvectors of the matrix $A$ . Are the conjugate vectors unique up to a scalar multiple? In other words, if there are two sets of conjugate vectors with respect to the same symmetric positive definite matrix $A$ , are the vectors going to be the same up to a scalar multiple? I would guess that that they are unique up to a scalar multiple, but I am not sure. Any help is much appreciated!","A set of nonzero vectors is said to be conjugate with respect to a symmetric positive definite matrix if for all . Such vectors are used in the conjugate gradient method . It follows that conjugate vectors are also linearly independent. An example of conjugate vectors are the eigenvectors of the matrix . Are the conjugate vectors unique up to a scalar multiple? In other words, if there are two sets of conjugate vectors with respect to the same symmetric positive definite matrix , are the vectors going to be the same up to a scalar multiple? I would guess that that they are unique up to a scalar multiple, but I am not sure. Any help is much appreciated!","\{p_0,p_1,\ldots,p_{n-1}\} A 
p_i^{\mathrm T}Ap_j=0
 i\ne j A A","['linear-algebra', 'vectors', 'positive-definite', 'symmetric-matrices']"
77,Do $AB$ and $BA$ have the same eigenvalues? [duplicate],Do  and  have the same eigenvalues? [duplicate],AB BA,This question already has answers here : Do matrices $ AB $ and $ BA $ have the same minimal and characteristic polynomials? (13 answers) Closed 7 years ago . Suppose $A$ and $B$ are $n\times n$ matrices and they are both invertible. Can we say that $AB$ and $BA$ have the same eigenvalues?,This question already has answers here : Do matrices $ AB $ and $ BA $ have the same minimal and characteristic polynomials? (13 answers) Closed 7 years ago . Suppose $A$ and $B$ are $n\times n$ matrices and they are both invertible. Can we say that $AB$ and $BA$ have the same eigenvalues?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
78,Is the set of rational numbers a vector space?,Is the set of rational numbers a vector space?,,"Is the set of all rational numbers $\mathbb{Q}$ a vectorspace? I assumed no, because if $$\vec{x} \in \mathbb{Q}$$  $$\pi \vec{x} \notin \mathbb{Q}$$ failing scalar mult closure This was a question out of my linear algebra book, looking at the solutions says that it is a vectorspace. Am I making an incorrect assumption?","Is the set of all rational numbers $\mathbb{Q}$ a vectorspace? I assumed no, because if $$\vec{x} \in \mathbb{Q}$$  $$\pi \vec{x} \notin \mathbb{Q}$$ failing scalar mult closure This was a question out of my linear algebra book, looking at the solutions says that it is a vectorspace. Am I making an incorrect assumption?",,"['linear-algebra', 'vector-spaces']"
79,Trace of $A$ if $A =A^{-1}$,Trace of  if,A A =A^{-1},"Suppose $I\neq A\neq -I$, where $I$ is the identity matrix and $A$ is a real $2\times2$ matrix. If $A=A^{-1}$ then what is the trace of $A$? I was thinking of writing $A$ as $\{a,b;c; d\}$ then using $A=A^{-1}$ to equate the positions but the equations I get suggest there is an easier way.","Suppose $I\neq A\neq -I$, where $I$ is the identity matrix and $A$ is a real $2\times2$ matrix. If $A=A^{-1}$ then what is the trace of $A$? I was thinking of writing $A$ as $\{a,b;c; d\}$ then using $A=A^{-1}$ to equate the positions but the equations I get suggest there is an easier way.",,"['linear-algebra', 'matrices']"
80,invertible matrices connected or not,invertible matrices connected or not,,"The question asks ""Is the set of all $3\times3$ real invertible matrices connected?"" My intuitive idea is that we can establish a separation consisting of matrices with positive and negative determinant respectively, whose union will be the whole set but with no intersections. However I am not sure how to show intersection of one set with the closure of the other set is empty, according to the definition of being disconnected. (My guess is that the closure for real matrices with negative determinant is just itself union matrices with 0 determinant) So what will be a rigorous proof of this, using only tools in point-set topology and knowledge in linear algebra? I know that $3\times3$ matrices can be viewed as homeomorphic to $\mathbb{R}^9$, but how do we define the topology on this subspace (i.e. set of all $3\times3$ matrices)? What will an open set look like in this topology?","The question asks ""Is the set of all $3\times3$ real invertible matrices connected?"" My intuitive idea is that we can establish a separation consisting of matrices with positive and negative determinant respectively, whose union will be the whole set but with no intersections. However I am not sure how to show intersection of one set with the closure of the other set is empty, according to the definition of being disconnected. (My guess is that the closure for real matrices with negative determinant is just itself union matrices with 0 determinant) So what will be a rigorous proof of this, using only tools in point-set topology and knowledge in linear algebra? I know that $3\times3$ matrices can be viewed as homeomorphic to $\mathbb{R}^9$, but how do we define the topology on this subspace (i.e. set of all $3\times3$ matrices)? What will an open set look like in this topology?",,"['linear-algebra', 'general-topology']"
81,"Prove $e^x, e^{2x}..., e^{nx}$ is linear independent on the vector space of $\mathbb{R} \to \mathbb{R}$",Prove  is linear independent on the vector space of,"e^x, e^{2x}..., e^{nx} \mathbb{R} \to \mathbb{R}","Prove $e^x, e^{2x}..., e^{nx}$ is linear independent on the vector space of $\mathbb{R} \to \mathbb{R}$ isn't it suffice to say that $e^y$ for any $y \in \mathbb{R}$ is in $\mathbb{R}^+$ Therefore, there aren't $\gamma_1, ...\gamma_n$ such that $\gamma_1e^x+\gamma_2e^{2x}...+\gamma_ne^{nx}=0$. Therefore, they're not linear dependent. I've seen a proof goes as follow: take $(n-1)$ derivatives of the equation. then, you got $n$ equations with $n$ variables. Arranging it in a matrix (which is found out to be Van-Der-Monde matrix). calculate the determinant which is $\ne 0$. Therefore, only the trivial solution exist. Therefore, no linear dependency. Is all that necessary?","Prove $e^x, e^{2x}..., e^{nx}$ is linear independent on the vector space of $\mathbb{R} \to \mathbb{R}$ isn't it suffice to say that $e^y$ for any $y \in \mathbb{R}$ is in $\mathbb{R}^+$ Therefore, there aren't $\gamma_1, ...\gamma_n$ such that $\gamma_1e^x+\gamma_2e^{2x}...+\gamma_ne^{nx}=0$. Therefore, they're not linear dependent. I've seen a proof goes as follow: take $(n-1)$ derivatives of the equation. then, you got $n$ equations with $n$ variables. Arranging it in a matrix (which is found out to be Van-Der-Monde matrix). calculate the determinant which is $\ne 0$. Therefore, only the trivial solution exist. Therefore, no linear dependency. Is all that necessary?",,"['linear-algebra', 'matrices']"
82,Eigenvectors of similar matrices,Eigenvectors of similar matrices,,"If $A$ and $B$ are similar matrices then every eigenvector of $A$ is an eigenvector of $B$ . Is the above statement is true? I know that similar matrices have same eigenvalue, but I'm not sure about the eigenvectors.","If and are similar matrices then every eigenvector of is an eigenvector of . Is the above statement is true? I know that similar matrices have same eigenvalue, but I'm not sure about the eigenvectors.",A B A B,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
83,Definition of Polynomials in Linear Algebra Done Right,Definition of Polynomials in Linear Algebra Done Right,,"Consider the following definitions where $\bf{F}$ is the real or complex number field and $\mathcal{L}(V)$ is the vector space of all linear operators on $V$. I think that the notation $p(T)$ does not make sense as $T$ is not in the domain of $p$. Am I right? If YES , So what is the right notation for this?","Consider the following definitions where $\bf{F}$ is the real or complex number field and $\mathcal{L}(V)$ is the vector space of all linear operators on $V$. I think that the notation $p(T)$ does not make sense as $T$ is not in the domain of $p$. Am I right? If YES , So what is the right notation for this?",,"['linear-algebra', 'polynomials', 'notation']"
84,Prove Why $B^2 = A$ exists?,Prove Why  exists?,B^2 = A,"Define   $$A = \begin{pmatrix} 8 & −4 & 3/2 & 2 & −11/4 & −4 & −4 & 1 \\ 2 & 2 & 1 & 0 & 1 & 0 & 0 & 0 \\ −9 & 8 & 1/2 & −4 & 31/4 & 8 & 8 & −2 \\ 4 & −6 & 2 & 5 & −7 & −6 & −6 & 0 \\ −2 & 0 & −1 & 0 & 1/2 & 0 & 0 & 0 \\ −1 & 0 & −1/2 & 0 & −3/4 & 3 & 1 & 0 \\ 1 & 0 & 1/2 & 0 & 3/4 & −1 & 1 & 0 \\ 2 & 0 & 1 & 0 & 0 & 0 & 0 & 5 \\ \end{pmatrix} \in M_8(\mathbb R)$$ Justify why there is a matrix $B$ such that $B^2 = A$. I think it has something to do with the Jordan normal form so I found it. \begin{pmatrix} 2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 1 & 2 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 2 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 2 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 2 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 5 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 5 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 5 \\ \end{pmatrix} (I know the ones are under the diagonal, but I think it doesn't make a difference.) I have no idea what should I do next, a help will be appreciated! Thanks!","Define   $$A = \begin{pmatrix} 8 & −4 & 3/2 & 2 & −11/4 & −4 & −4 & 1 \\ 2 & 2 & 1 & 0 & 1 & 0 & 0 & 0 \\ −9 & 8 & 1/2 & −4 & 31/4 & 8 & 8 & −2 \\ 4 & −6 & 2 & 5 & −7 & −6 & −6 & 0 \\ −2 & 0 & −1 & 0 & 1/2 & 0 & 0 & 0 \\ −1 & 0 & −1/2 & 0 & −3/4 & 3 & 1 & 0 \\ 1 & 0 & 1/2 & 0 & 3/4 & −1 & 1 & 0 \\ 2 & 0 & 1 & 0 & 0 & 0 & 0 & 5 \\ \end{pmatrix} \in M_8(\mathbb R)$$ Justify why there is a matrix $B$ such that $B^2 = A$. I think it has something to do with the Jordan normal form so I found it. \begin{pmatrix} 2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 1 & 2 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 2 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 2 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 2 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 5 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 5 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 5 \\ \end{pmatrix} (I know the ones are under the diagonal, but I think it doesn't make a difference.) I have no idea what should I do next, a help will be appreciated! Thanks!",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
85,Question definition: If I take any three vectors...,Question definition: If I take any three vectors...,,"I am currently studying linear algebra and one of the things I am having trouble with concerns understanding the questions being asked to me. The following question I am having trouble defining. The following question is in R2 space. If I take any three vectors, u, v, w in the plane, will there always   be two different combinations that produce... (and the question   continues, I understand the rest of the question) What I don't understand is the ""If I take any three vectors, u, v, w in the plane"". Does this mean I can take for example u, u and u (effectively three u vectors, as it states: any three vectors and figure out different combinations with them etc.). Or perhaps u, u and v? Or does it mean u, v, w specifically. I know this might seem like a stupid question.","I am currently studying linear algebra and one of the things I am having trouble with concerns understanding the questions being asked to me. The following question I am having trouble defining. The following question is in R2 space. If I take any three vectors, u, v, w in the plane, will there always   be two different combinations that produce... (and the question   continues, I understand the rest of the question) What I don't understand is the ""If I take any three vectors, u, v, w in the plane"". Does this mean I can take for example u, u and u (effectively three u vectors, as it states: any three vectors and figure out different combinations with them etc.). Or perhaps u, u and v? Or does it mean u, v, w specifically. I know this might seem like a stupid question.",,['linear-algebra']
86,"How to show if $ \lambda$ is an eigenvalue of $AB^{-1}$, then $ \lambda$ is an eigenvalue of $ B^{-1}A$?","How to show if  is an eigenvalue of , then  is an eigenvalue of ?", \lambda AB^{-1}  \lambda  B^{-1}A,"Statement: If $ \lambda$ is an eigenvalue of $AB^{-1}$, then $ \lambda$ is an eigenvalue of $ B^{-1}A$ and vice versa. One way of the proof. We have  $B(B^{-1}A ) B^{-1} = AB^{-1}. $ Assuming $ \lambda$ is an eigenvalue of $AB^{-1}$ then we have,  $$\begin{align*} \det(\lambda I - AB^{-1})  &= \det( \lambda I - B( B^{-1}A ) B^{-1} )\\  &=  \det( B(\lambda I - B^{-1}A ) B^{-1})\\  &= \det(B) \det\bigl( \lambda I - B^{-1}A  \bigr) \det(B^{-1})\\  &= \det(B) \det\bigl( \lambda I - (B^{-1}A )\bigr)   \frac{1}{ \det(B) }\\ \ &= \det( \lambda I - B^{-1}A ).  \end{align*}$$  It follows that $ \lambda$ is an eigenvalue of $ B^{-1}A.$ The other side of the lemma can also be proved similarly. Is there another way how to prove the statement?","Statement: If $ \lambda$ is an eigenvalue of $AB^{-1}$, then $ \lambda$ is an eigenvalue of $ B^{-1}A$ and vice versa. One way of the proof. We have  $B(B^{-1}A ) B^{-1} = AB^{-1}. $ Assuming $ \lambda$ is an eigenvalue of $AB^{-1}$ then we have,  $$\begin{align*} \det(\lambda I - AB^{-1})  &= \det( \lambda I - B( B^{-1}A ) B^{-1} )\\  &=  \det( B(\lambda I - B^{-1}A ) B^{-1})\\  &= \det(B) \det\bigl( \lambda I - B^{-1}A  \bigr) \det(B^{-1})\\  &= \det(B) \det\bigl( \lambda I - (B^{-1}A )\bigr)   \frac{1}{ \det(B) }\\ \ &= \det( \lambda I - B^{-1}A ).  \end{align*}$$  It follows that $ \lambda$ is an eigenvalue of $ B^{-1}A.$ The other side of the lemma can also be proved similarly. Is there another way how to prove the statement?",,"['linear-algebra', 'matrices', 'functional-analysis']"
87,Units of $M_2(Z)$,Units of,M_2(Z),"In one of my classes we discussed the ring of 2x2 matrices $M_{2}(\mathbb{Z})$. We said that its group of units was $SL_{2}(\mathbb{Z})$ which means that it is the set of 2x2 with determinant equal to $\pm$1. Why can't we have a 2x2 matrix with entries a,b,c, and d such that $\frac{a}{ad-bc}$,$\frac{-b}{ad-bc}$,$\frac{-c}{ad-bc}$, and $\frac{d}{ad-bc}$ are all integers? I'm sure its a simple contradiction argument, but I couldn't see it. So if anyone knows a quick elementary argument, it'd be greatly appreciated","In one of my classes we discussed the ring of 2x2 matrices $M_{2}(\mathbb{Z})$. We said that its group of units was $SL_{2}(\mathbb{Z})$ which means that it is the set of 2x2 with determinant equal to $\pm$1. Why can't we have a 2x2 matrix with entries a,b,c, and d such that $\frac{a}{ad-bc}$,$\frac{-b}{ad-bc}$,$\frac{-c}{ad-bc}$, and $\frac{d}{ad-bc}$ are all integers? I'm sure its a simple contradiction argument, but I couldn't see it. So if anyone knows a quick elementary argument, it'd be greatly appreciated",,"['linear-algebra', 'ring-theory']"
88,Giving a basis for the column space of A,Giving a basis for the column space of A,,"Let $A = \begin{bmatrix}3&3&3\\3&5&1\\-2&4&-8\\-2&-4&0\\4&9&-1\end{bmatrix}$ Give a basis for the column space of A So what I've done so far is put it in RREF (which was a task itself) and got $\begin{bmatrix}1&0&2\\0&1&-1\\0&0&0\\0&0&0\\0&0&0\end{bmatrix}$ , but I'm not sure what to do next to give the ""basis"" of the column space of A","Let Give a basis for the column space of A So what I've done so far is put it in RREF (which was a task itself) and got , but I'm not sure what to do next to give the ""basis"" of the column space of A",A = \begin{bmatrix}3&3&3\\3&5&1\\-2&4&-8\\-2&-4&0\\4&9&-1\end{bmatrix} \begin{bmatrix}1&0&2\\0&1&-1\\0&0&0\\0&0&0\\0&0&0\end{bmatrix},['linear-algebra']
89,Why matrices commuting with $\small\begin{bmatrix} 0&1\\-1&0\end{bmatrix}$ represent complex numbers?,Why matrices commuting with  represent complex numbers?,\small\begin{bmatrix} 0&1\\-1&0\end{bmatrix},"I am trying to understand which $2$ by $2$ real matrices represent complex numbers in following way. Let $J=\begin{bmatrix} 0&1\\-1&0\end{bmatrix}$ and $A=\begin{bmatrix} a&b\\c&d\end{bmatrix}$ be any real matrix. If $A$ represents a complex matrix (by standard embedding of complex field into matrix ring) then $A$ should commute with the matrix $J$ , which image of complex number $i$ . Q. I want to understand why the matrices commuting with $J$ are precisely the matrices representing complex numbers?","I am trying to understand which by real matrices represent complex numbers in following way. Let and be any real matrix. If represents a complex matrix (by standard embedding of complex field into matrix ring) then should commute with the matrix , which image of complex number . Q. I want to understand why the matrices commuting with are precisely the matrices representing complex numbers?",2 2 J=\begin{bmatrix} 0&1\\-1&0\end{bmatrix} A=\begin{bmatrix} a&b\\c&d\end{bmatrix} A A J i J,"['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory', 'complex-numbers']"
90,"What is the difference between a zero operator, zero function, zero scalar, and zero vector?","What is the difference between a zero operator, zero function, zero scalar, and zero vector?",,"I'm pretty sure that a zero vector is just a vector of length zero with direction, zero scalar is just the number zero, and that a zero function is any function that maps to zero. Not entirely sure what exactly a zero operator is however.","I'm pretty sure that a zero vector is just a vector of length zero with direction, zero scalar is just the number zero, and that a zero function is any function that maps to zero. Not entirely sure what exactly a zero operator is however.",,"['linear-algebra', 'soft-question', 'terminology']"
91,example of a nonempty subset is closed under scalar multiplication but not a subspace,example of a nonempty subset is closed under scalar multiplication but not a subspace,,"Could anyone provide an example of a nonempty subset $U$ of $R^2$ such that $U$ is closed under scalar multiplication, but $U$ is not a subspace of $R^2$","Could anyone provide an example of a nonempty subset $U$ of $R^2$ such that $U$ is closed under scalar multiplication, but $U$ is not a subspace of $R^2$",,['linear-algebra']
92,$\mathbb{C}$ is a one-dimensional complex vector space. What is its dimension when regarded as a vector space over $\mathbb{R}$?,is a one-dimensional complex vector space. What is its dimension when regarded as a vector space over ?,\mathbb{C} \mathbb{R},"$\mathbb{C}$ is a one-dimensional complex vector space. What is its dimension when regarded as a vector space over $\mathbb{R}$? I don't understand how $\mathbb{C}$ is one-dimensional. Please help me understand that. Also, I'm pretty sure that when the field is reals we have $\dim(\mathbb{C})=2$. Since when $\alpha$ is real and $z=a+bi$ is complex we have $\alpha z=\alpha a+\alpha bi=\alpha a(1,0)+\alpha b(0,i)$. How does this look? Any solutions or help is greatly appreciated.","$\mathbb{C}$ is a one-dimensional complex vector space. What is its dimension when regarded as a vector space over $\mathbb{R}$? I don't understand how $\mathbb{C}$ is one-dimensional. Please help me understand that. Also, I'm pretty sure that when the field is reals we have $\dim(\mathbb{C})=2$. Since when $\alpha$ is real and $z=a+bi$ is complex we have $\alpha z=\alpha a+\alpha bi=\alpha a(1,0)+\alpha b(0,i)$. How does this look? Any solutions or help is greatly appreciated.",,"['linear-algebra', 'complex-numbers']"
93,"If $(A + I)^4 = 0$, $A$ is non-singular","If ,  is non-singular",(A + I)^4 = 0 A,"If $A$ be an n × n real matrix such that $(A + I)^4 = 0$, where $I$ denotes the identity matrix, how to prove that $A$ is non-singular?","If $A$ be an n × n real matrix such that $(A + I)^4 = 0$, where $I$ denotes the identity matrix, how to prove that $A$ is non-singular?",,"['linear-algebra', 'matrices']"
94,How do I prove that a matrix is a rotation-matrix?,How do I prove that a matrix is a rotation-matrix?,,"I have to prove that this matrix is a rotation-matrix $$\begin{pmatrix} \frac12 & 0 & \frac{\sqrt{3}}{2} \\ 0 & 1 & 0 \\ \frac{\sqrt{3}}{2} & 0 & \frac12 \end{pmatrix}$$ How do I do this?  My idea is to multiplicate it with $\begin{pmatrix} x \\ y \\ z\end{pmatrix}$ and show that one component will remain unchanged . Is this enough? Do non-rotational transformations exist, which leave one component unchanged ?","I have to prove that this matrix is a rotation-matrix $$\begin{pmatrix} \frac12 & 0 & \frac{\sqrt{3}}{2} \\ 0 & 1 & 0 \\ \frac{\sqrt{3}}{2} & 0 & \frac12 \end{pmatrix}$$ How do I do this?  My idea is to multiplicate it with $\begin{pmatrix} x \\ y \\ z\end{pmatrix}$ and show that one component will remain unchanged . Is this enough? Do non-rotational transformations exist, which leave one component unchanged ?",,['linear-algebra']
95,Upper and Lower Triangular Matrices,Upper and Lower Triangular Matrices,,"Given the matrix A=$ \left( \begin{array}{ccc} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8\\ 1 & -1 & 2 & 3 \\ 2 & 1 & 1 &2\end{array} \right) $, write it in the $L_{4 \times 4}U_{4 \times 4}$, where L is the lower triangular matrix and U is the upper triangular matrix. To be honest, I don't even understand what the question is asking of me, however I do know what upper and lower triangular matrices are. Thanks in advance for your help.","Given the matrix A=$ \left( \begin{array}{ccc} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8\\ 1 & -1 & 2 & 3 \\ 2 & 1 & 1 &2\end{array} \right) $, write it in the $L_{4 \times 4}U_{4 \times 4}$, where L is the lower triangular matrix and U is the upper triangular matrix. To be honest, I don't even understand what the question is asking of me, however I do know what upper and lower triangular matrices are. Thanks in advance for your help.",,"['linear-algebra', 'matrices']"
96,Can a matrix in $\mathbb{R}$ have a minimal polynomial that has coefficients in $\mathbb{C}$?,Can a matrix in  have a minimal polynomial that has coefficients in ?,\mathbb{R} \mathbb{C},"Like the title says, can a matrix in $\mathbb{R}$ have a minimal polynomial that has coefficients in $\mathbb{C}$? I have a feeling the answer is yes because the characteristic polynomial can have coefficients in $\mathbb{C}$ and so I don't see any reason why not, I was merely asking in case there was some sneaky proof that proves that this cannot happen.","Like the title says, can a matrix in $\mathbb{R}$ have a minimal polynomial that has coefficients in $\mathbb{C}$? I have a feeling the answer is yes because the characteristic polynomial can have coefficients in $\mathbb{C}$ and so I don't see any reason why not, I was merely asking in case there was some sneaky proof that proves that this cannot happen.",,['linear-algebra']
97,Do two symmetric matrix of the same size commute? [closed],Do two symmetric matrix of the same size commute? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question If i have two symmetric matrix A and B of the same size, do A and B commute? If not, what is a counter example? This is related to a problem in my intro to linear alg class. Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question If i have two symmetric matrix A and B of the same size, do A and B commute? If not, what is a counter example? This is related to a problem in my intro to linear alg class. Thanks.",,"['linear-algebra', 'matrices']"
98,what are pivot numbers in LU decomposition? please explain me in an example,what are pivot numbers in LU decomposition? please explain me in an example,,"studying many pages like wikipedia , wolfram , Mathworks , Math Stack Exchange , lu-pivot , LU_Decomposition I couldn't find exactly whart are the pivot numbers in a specific example: for example what are the pivot numbers in this example: (I've extracted the LU-decomposition above based on an example in this pdf. ) or this one: Can you explain me how to extract the pivot numbers in both the examples above? Also if it is possible, can you explain me about these questions alittle? How can we extract pivot numbers in various forms of pivoting. e.g. LU factorization with Partial Pivoting ( PA = LU ) , LU factorization with full pivoting ( PAQ = LU ) , LDU decomposition ( A = LDU ) ? How can we understand what permutation matrix ( P ) should be multiplied by ( A ) to be able to extract a stable LU-decomposition before starting to decompose?","studying many pages like wikipedia , wolfram , Mathworks , Math Stack Exchange , lu-pivot , LU_Decomposition I couldn't find exactly whart are the pivot numbers in a specific example: for example what are the pivot numbers in this example: (I've extracted the LU-decomposition above based on an example in this pdf. ) or this one: Can you explain me how to extract the pivot numbers in both the examples above? Also if it is possible, can you explain me about these questions alittle? How can we extract pivot numbers in various forms of pivoting. e.g. LU factorization with Partial Pivoting ( PA = LU ) , LU factorization with full pivoting ( PAQ = LU ) , LDU decomposition ( A = LDU ) ? How can we understand what permutation matrix ( P ) should be multiplied by ( A ) to be able to extract a stable LU-decomposition before starting to decompose?",,"['linear-algebra', 'matrices']"
99,Is the absolute value function a linear function?,Is the absolute value function a linear function?,,"I'm inclined to say yes, as it doesn't involve exponentiation, roots, logarithmic or trigonometric functions, but I watched a video where the teacher said that the absolute value function is ""clearly non-linear"". Why would he say that? Is he wrong? Wikipedia's graph for abs:","I'm inclined to say yes, as it doesn't involve exponentiation, roots, logarithmic or trigonometric functions, but I watched a video where the teacher said that the absolute value function is ""clearly non-linear"". Why would he say that? Is he wrong? Wikipedia's graph for abs:",,"['linear-algebra', 'functions', 'absolute-value']"
